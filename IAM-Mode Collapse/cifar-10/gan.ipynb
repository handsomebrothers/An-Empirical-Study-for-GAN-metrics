{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.client.session.Session at 0x7fd5d8540160>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "# #指定使用那块GUP训练\n",
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "config = tf.ConfigProto()\n",
    "# 设置最大占有GPU不超过显存的70%\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.7 \n",
    "# # 重点：设置动态分配GPU\n",
    "config.gpu_options.allow_growth = True\n",
    "# 创建session时\n",
    "tf.Session(config=config) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "Sequential(\n",
      "  (0): Conv2d(3, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "  (2): ReLU()\n",
      "  (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "  (5): ReLU()\n",
      "  (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (7): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (8): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "  (9): ReLU()\n",
      "  (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (11): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "  (12): ReLU()\n",
      "  (13): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (14): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (15): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "  (16): ReLU()\n",
      "  (17): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (18): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "  (19): ReLU()\n",
      "  (20): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (21): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (22): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "  (23): ReLU()\n",
      "  (24): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      ")\n",
      "Sequential(\n",
      "  (0): Linear(in_features=1024, out_features=10, bias=True)\n",
      ")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_1 (Flatten)          (None, 3072)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 512)               1573376   \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 1,704,961\n",
      "Trainable params: 1,704,961\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_4 (Dense)              (None, 256)               25856     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 512)               131584    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 1024)              525312    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)    (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 1024)              4096      \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 3072)              3148800   \n",
      "_________________________________________________________________\n",
      "reshape_1 (Reshape)          (None, 32, 32, 3)         0         \n",
      "=================================================================\n",
      "Total params: 3,838,720\n",
      "Trainable params: 3,835,136\n",
      "Non-trainable params: 3,584\n",
      "_________________________________________________________________\n",
      "(50000, 32, 32, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/imi432_006/anaconda3/envs/tf/lib/python3.5/site-packages/keras/engine/training.py:490: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0 step:1 [D loss: 0.722421, acc.: 54.69%] [G loss: 0.634639]\n",
      "epoch:0 step:2 [D loss: 0.625889, acc.: 57.03%] [G loss: 0.607043]\n",
      "epoch:0 step:3 [D loss: 0.565688, acc.: 58.59%] [G loss: 0.656417]\n",
      "epoch:0 step:4 [D loss: 0.539331, acc.: 64.06%] [G loss: 0.724627]\n",
      "epoch:0 step:5 [D loss: 0.489601, acc.: 69.53%] [G loss: 0.742720]\n",
      "epoch:0 step:6 [D loss: 0.496257, acc.: 71.88%] [G loss: 0.784894]\n",
      "epoch:0 step:7 [D loss: 0.521062, acc.: 71.88%] [G loss: 0.888948]\n",
      "epoch:0 step:8 [D loss: 0.478658, acc.: 71.09%] [G loss: 0.898402]\n",
      "epoch:0 step:9 [D loss: 0.477769, acc.: 73.44%] [G loss: 0.919063]\n",
      "epoch:0 step:10 [D loss: 0.479139, acc.: 71.09%] [G loss: 0.949653]\n",
      "epoch:0 step:11 [D loss: 0.421680, acc.: 83.59%] [G loss: 1.013812]\n",
      "epoch:0 step:12 [D loss: 0.447405, acc.: 78.12%] [G loss: 0.990016]\n",
      "epoch:0 step:13 [D loss: 0.442277, acc.: 81.25%] [G loss: 1.084875]\n",
      "epoch:0 step:14 [D loss: 0.454726, acc.: 79.69%] [G loss: 1.098620]\n",
      "epoch:0 step:15 [D loss: 0.415784, acc.: 82.81%] [G loss: 1.181590]\n",
      "epoch:0 step:16 [D loss: 0.416671, acc.: 83.59%] [G loss: 1.227419]\n",
      "epoch:0 step:17 [D loss: 0.351838, acc.: 89.84%] [G loss: 1.259456]\n",
      "epoch:0 step:18 [D loss: 0.335843, acc.: 85.94%] [G loss: 1.238941]\n",
      "epoch:0 step:19 [D loss: 0.399060, acc.: 83.59%] [G loss: 1.368556]\n",
      "epoch:0 step:20 [D loss: 0.387383, acc.: 85.94%] [G loss: 1.368423]\n",
      "epoch:0 step:21 [D loss: 0.402045, acc.: 85.16%] [G loss: 1.329760]\n",
      "epoch:0 step:22 [D loss: 0.434144, acc.: 80.47%] [G loss: 1.431401]\n",
      "epoch:0 step:23 [D loss: 0.382208, acc.: 85.94%] [G loss: 1.498102]\n",
      "epoch:0 step:24 [D loss: 0.520448, acc.: 78.12%] [G loss: 1.504337]\n",
      "epoch:0 step:25 [D loss: 0.438623, acc.: 80.47%] [G loss: 1.435444]\n",
      "epoch:0 step:26 [D loss: 0.353701, acc.: 84.38%] [G loss: 1.579663]\n",
      "epoch:0 step:27 [D loss: 0.323776, acc.: 91.41%] [G loss: 1.654146]\n",
      "epoch:0 step:28 [D loss: 0.524100, acc.: 75.00%] [G loss: 1.557479]\n",
      "epoch:0 step:29 [D loss: 0.344457, acc.: 87.50%] [G loss: 1.663925]\n",
      "epoch:0 step:30 [D loss: 0.589190, acc.: 74.22%] [G loss: 1.608321]\n",
      "epoch:0 step:31 [D loss: 0.477925, acc.: 80.47%] [G loss: 1.657442]\n",
      "epoch:0 step:32 [D loss: 0.523034, acc.: 77.34%] [G loss: 1.762811]\n",
      "epoch:0 step:33 [D loss: 0.567603, acc.: 71.09%] [G loss: 1.614267]\n",
      "epoch:0 step:34 [D loss: 0.496237, acc.: 82.03%] [G loss: 1.793553]\n",
      "epoch:0 step:35 [D loss: 0.406612, acc.: 84.38%] [G loss: 1.745227]\n",
      "epoch:0 step:36 [D loss: 0.710648, acc.: 66.41%] [G loss: 1.882143]\n",
      "epoch:0 step:37 [D loss: 0.643585, acc.: 68.75%] [G loss: 1.814045]\n",
      "epoch:0 step:38 [D loss: 0.562321, acc.: 71.09%] [G loss: 1.705802]\n",
      "epoch:0 step:39 [D loss: 0.487074, acc.: 78.91%] [G loss: 1.686181]\n",
      "epoch:0 step:40 [D loss: 0.638591, acc.: 64.84%] [G loss: 2.240625]\n",
      "epoch:0 step:41 [D loss: 0.776528, acc.: 63.28%] [G loss: 1.909231]\n",
      "epoch:0 step:42 [D loss: 0.467322, acc.: 80.47%] [G loss: 1.676501]\n",
      "epoch:0 step:43 [D loss: 0.490792, acc.: 78.12%] [G loss: 1.571431]\n",
      "epoch:0 step:44 [D loss: 0.551932, acc.: 73.44%] [G loss: 1.918250]\n",
      "epoch:0 step:45 [D loss: 0.466485, acc.: 85.16%] [G loss: 1.964402]\n",
      "epoch:0 step:46 [D loss: 0.520725, acc.: 72.66%] [G loss: 1.751083]\n",
      "epoch:0 step:47 [D loss: 0.629196, acc.: 68.75%] [G loss: 1.898222]\n",
      "epoch:0 step:48 [D loss: 0.687680, acc.: 68.75%] [G loss: 1.616969]\n",
      "epoch:0 step:49 [D loss: 0.602564, acc.: 68.75%] [G loss: 1.593526]\n",
      "epoch:0 step:50 [D loss: 0.592023, acc.: 66.41%] [G loss: 1.684520]\n",
      "epoch:0 step:51 [D loss: 0.541500, acc.: 71.88%] [G loss: 1.730602]\n",
      "epoch:0 step:52 [D loss: 0.693951, acc.: 67.19%] [G loss: 1.947238]\n",
      "epoch:0 step:53 [D loss: 0.700754, acc.: 61.72%] [G loss: 2.169515]\n",
      "epoch:0 step:54 [D loss: 0.643601, acc.: 70.31%] [G loss: 1.789754]\n",
      "epoch:0 step:55 [D loss: 0.650914, acc.: 64.06%] [G loss: 1.696926]\n",
      "epoch:0 step:56 [D loss: 0.484838, acc.: 80.47%] [G loss: 1.733162]\n",
      "epoch:0 step:57 [D loss: 0.566159, acc.: 72.66%] [G loss: 1.701710]\n",
      "epoch:0 step:58 [D loss: 0.624806, acc.: 71.09%] [G loss: 1.518980]\n",
      "epoch:0 step:59 [D loss: 0.541164, acc.: 78.12%] [G loss: 1.778411]\n",
      "epoch:0 step:60 [D loss: 0.731642, acc.: 64.84%] [G loss: 1.621320]\n",
      "epoch:0 step:61 [D loss: 0.762388, acc.: 64.84%] [G loss: 1.773155]\n",
      "epoch:0 step:62 [D loss: 0.603580, acc.: 67.97%] [G loss: 1.718459]\n",
      "epoch:0 step:63 [D loss: 0.677291, acc.: 62.50%] [G loss: 1.823256]\n",
      "epoch:0 step:64 [D loss: 0.787492, acc.: 57.81%] [G loss: 1.580512]\n",
      "epoch:0 step:65 [D loss: 0.538455, acc.: 70.31%] [G loss: 1.673705]\n",
      "epoch:0 step:66 [D loss: 0.539169, acc.: 75.78%] [G loss: 1.809869]\n",
      "epoch:0 step:67 [D loss: 0.817310, acc.: 54.69%] [G loss: 1.706719]\n",
      "epoch:0 step:68 [D loss: 0.622387, acc.: 66.41%] [G loss: 1.716140]\n",
      "epoch:0 step:69 [D loss: 0.768630, acc.: 52.34%] [G loss: 1.678373]\n",
      "epoch:0 step:70 [D loss: 0.588935, acc.: 75.00%] [G loss: 1.751851]\n",
      "epoch:0 step:71 [D loss: 0.908598, acc.: 50.78%] [G loss: 1.689429]\n",
      "epoch:0 step:72 [D loss: 0.670278, acc.: 58.59%] [G loss: 1.681857]\n",
      "epoch:0 step:73 [D loss: 0.836641, acc.: 57.03%] [G loss: 1.740853]\n",
      "epoch:0 step:74 [D loss: 0.673436, acc.: 63.28%] [G loss: 1.708779]\n",
      "epoch:0 step:75 [D loss: 0.630791, acc.: 64.84%] [G loss: 1.699179]\n",
      "epoch:0 step:76 [D loss: 0.636360, acc.: 64.06%] [G loss: 1.498925]\n",
      "epoch:0 step:77 [D loss: 0.708847, acc.: 60.16%] [G loss: 1.821427]\n",
      "epoch:0 step:78 [D loss: 0.674907, acc.: 60.94%] [G loss: 1.669577]\n",
      "epoch:0 step:79 [D loss: 0.693649, acc.: 67.19%] [G loss: 1.629114]\n",
      "epoch:0 step:80 [D loss: 0.709252, acc.: 58.59%] [G loss: 1.661069]\n",
      "epoch:0 step:81 [D loss: 0.710420, acc.: 67.19%] [G loss: 1.618280]\n",
      "epoch:0 step:82 [D loss: 0.786320, acc.: 53.91%] [G loss: 1.428903]\n",
      "epoch:0 step:83 [D loss: 0.713120, acc.: 62.50%] [G loss: 1.721583]\n",
      "epoch:0 step:84 [D loss: 0.771665, acc.: 57.03%] [G loss: 1.534128]\n",
      "epoch:0 step:85 [D loss: 0.875082, acc.: 50.78%] [G loss: 1.573040]\n",
      "epoch:0 step:86 [D loss: 0.721677, acc.: 57.03%] [G loss: 1.486454]\n",
      "epoch:0 step:87 [D loss: 0.762571, acc.: 53.12%] [G loss: 1.486530]\n",
      "epoch:0 step:88 [D loss: 0.692811, acc.: 59.38%] [G loss: 1.600389]\n",
      "epoch:0 step:89 [D loss: 0.663544, acc.: 64.84%] [G loss: 1.498374]\n",
      "epoch:0 step:90 [D loss: 0.729281, acc.: 59.38%] [G loss: 1.934033]\n",
      "epoch:0 step:91 [D loss: 0.736794, acc.: 53.12%] [G loss: 1.776191]\n",
      "epoch:0 step:92 [D loss: 0.822666, acc.: 46.88%] [G loss: 1.595225]\n",
      "epoch:0 step:93 [D loss: 0.841855, acc.: 48.44%] [G loss: 1.756361]\n",
      "epoch:0 step:94 [D loss: 0.621330, acc.: 68.75%] [G loss: 1.636856]\n",
      "epoch:0 step:95 [D loss: 0.876408, acc.: 51.56%] [G loss: 1.422476]\n",
      "epoch:0 step:96 [D loss: 0.672492, acc.: 60.16%] [G loss: 1.404215]\n",
      "epoch:0 step:97 [D loss: 0.621759, acc.: 65.62%] [G loss: 1.601067]\n",
      "epoch:0 step:98 [D loss: 0.832245, acc.: 47.66%] [G loss: 1.678619]\n",
      "epoch:0 step:99 [D loss: 0.749929, acc.: 54.69%] [G loss: 1.665781]\n",
      "epoch:0 step:100 [D loss: 0.802628, acc.: 60.16%] [G loss: 1.437794]\n",
      "epoch:0 step:101 [D loss: 0.728798, acc.: 58.59%] [G loss: 1.463633]\n",
      "epoch:0 step:102 [D loss: 0.707504, acc.: 67.19%] [G loss: 1.355526]\n",
      "epoch:0 step:103 [D loss: 0.669769, acc.: 60.94%] [G loss: 1.534002]\n",
      "epoch:0 step:104 [D loss: 0.693170, acc.: 59.38%] [G loss: 1.341461]\n",
      "epoch:0 step:105 [D loss: 0.774777, acc.: 51.56%] [G loss: 1.451018]\n",
      "epoch:0 step:106 [D loss: 0.755423, acc.: 57.81%] [G loss: 1.594468]\n",
      "epoch:0 step:107 [D loss: 0.761481, acc.: 54.69%] [G loss: 1.663230]\n",
      "epoch:0 step:108 [D loss: 0.740043, acc.: 58.59%] [G loss: 1.553386]\n",
      "epoch:0 step:109 [D loss: 0.811811, acc.: 48.44%] [G loss: 1.458994]\n",
      "epoch:0 step:110 [D loss: 0.635789, acc.: 64.06%] [G loss: 1.565263]\n",
      "epoch:0 step:111 [D loss: 0.826405, acc.: 55.47%] [G loss: 1.445745]\n",
      "epoch:0 step:112 [D loss: 0.736346, acc.: 57.81%] [G loss: 1.424417]\n",
      "epoch:0 step:113 [D loss: 0.716288, acc.: 63.28%] [G loss: 1.463619]\n",
      "epoch:0 step:114 [D loss: 0.797893, acc.: 52.34%] [G loss: 1.358278]\n",
      "epoch:0 step:115 [D loss: 0.759705, acc.: 53.91%] [G loss: 1.513158]\n",
      "epoch:0 step:116 [D loss: 0.634591, acc.: 59.38%] [G loss: 1.678672]\n",
      "epoch:0 step:117 [D loss: 0.750801, acc.: 55.47%] [G loss: 1.641795]\n",
      "epoch:0 step:118 [D loss: 0.705357, acc.: 62.50%] [G loss: 1.422348]\n",
      "epoch:0 step:119 [D loss: 0.639388, acc.: 69.53%] [G loss: 1.542755]\n",
      "epoch:0 step:120 [D loss: 0.716351, acc.: 64.06%] [G loss: 1.706917]\n",
      "epoch:0 step:121 [D loss: 0.657243, acc.: 67.19%] [G loss: 1.533381]\n",
      "epoch:0 step:122 [D loss: 0.692658, acc.: 58.59%] [G loss: 1.526345]\n",
      "epoch:0 step:123 [D loss: 0.690293, acc.: 62.50%] [G loss: 1.647448]\n",
      "epoch:0 step:124 [D loss: 0.758570, acc.: 57.03%] [G loss: 1.622030]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0 step:125 [D loss: 0.721289, acc.: 61.72%] [G loss: 1.489766]\n",
      "epoch:0 step:126 [D loss: 0.749689, acc.: 59.38%] [G loss: 1.456690]\n",
      "epoch:0 step:127 [D loss: 0.741318, acc.: 54.69%] [G loss: 1.425542]\n",
      "epoch:0 step:128 [D loss: 0.701079, acc.: 54.69%] [G loss: 1.584983]\n",
      "epoch:0 step:129 [D loss: 0.796261, acc.: 50.78%] [G loss: 1.404122]\n",
      "epoch:0 step:130 [D loss: 0.718984, acc.: 56.25%] [G loss: 1.542346]\n",
      "epoch:0 step:131 [D loss: 0.760926, acc.: 52.34%] [G loss: 1.425337]\n",
      "epoch:0 step:132 [D loss: 0.625272, acc.: 60.16%] [G loss: 1.548638]\n",
      "epoch:0 step:133 [D loss: 0.640235, acc.: 62.50%] [G loss: 1.519421]\n",
      "epoch:0 step:134 [D loss: 0.602225, acc.: 67.19%] [G loss: 1.392839]\n",
      "epoch:0 step:135 [D loss: 0.753010, acc.: 54.69%] [G loss: 1.436329]\n",
      "epoch:0 step:136 [D loss: 0.615150, acc.: 63.28%] [G loss: 1.538992]\n",
      "epoch:0 step:137 [D loss: 0.770045, acc.: 42.97%] [G loss: 1.461833]\n",
      "epoch:0 step:138 [D loss: 0.681338, acc.: 61.72%] [G loss: 1.448668]\n",
      "epoch:0 step:139 [D loss: 0.680076, acc.: 64.84%] [G loss: 1.493355]\n",
      "epoch:0 step:140 [D loss: 0.643036, acc.: 63.28%] [G loss: 1.414411]\n",
      "epoch:0 step:141 [D loss: 0.681847, acc.: 59.38%] [G loss: 1.413447]\n",
      "epoch:0 step:142 [D loss: 0.628069, acc.: 66.41%] [G loss: 1.441998]\n",
      "epoch:0 step:143 [D loss: 0.637588, acc.: 66.41%] [G loss: 1.527132]\n",
      "epoch:0 step:144 [D loss: 0.693784, acc.: 61.72%] [G loss: 1.480819]\n",
      "epoch:0 step:145 [D loss: 0.697463, acc.: 57.81%] [G loss: 1.474021]\n",
      "epoch:0 step:146 [D loss: 0.632929, acc.: 60.94%] [G loss: 1.474584]\n",
      "epoch:0 step:147 [D loss: 0.705219, acc.: 54.69%] [G loss: 1.540459]\n",
      "epoch:0 step:148 [D loss: 0.693705, acc.: 60.94%] [G loss: 1.526699]\n",
      "epoch:0 step:149 [D loss: 0.741608, acc.: 53.91%] [G loss: 1.554655]\n",
      "epoch:0 step:150 [D loss: 0.663084, acc.: 59.38%] [G loss: 1.478710]\n",
      "epoch:0 step:151 [D loss: 0.683545, acc.: 60.94%] [G loss: 1.489405]\n",
      "epoch:0 step:152 [D loss: 0.778719, acc.: 46.88%] [G loss: 1.452936]\n",
      "epoch:0 step:153 [D loss: 0.664639, acc.: 57.81%] [G loss: 1.488365]\n",
      "epoch:0 step:154 [D loss: 0.651120, acc.: 60.94%] [G loss: 1.419533]\n",
      "epoch:0 step:155 [D loss: 0.691840, acc.: 56.25%] [G loss: 1.476038]\n",
      "epoch:0 step:156 [D loss: 0.693239, acc.: 59.38%] [G loss: 1.599731]\n",
      "epoch:0 step:157 [D loss: 0.696463, acc.: 59.38%] [G loss: 1.436498]\n",
      "epoch:0 step:158 [D loss: 0.706459, acc.: 54.69%] [G loss: 1.464163]\n",
      "epoch:0 step:159 [D loss: 0.703305, acc.: 51.56%] [G loss: 1.634610]\n",
      "epoch:0 step:160 [D loss: 0.636084, acc.: 60.94%] [G loss: 1.525830]\n",
      "epoch:0 step:161 [D loss: 0.676903, acc.: 62.50%] [G loss: 1.424347]\n",
      "epoch:0 step:162 [D loss: 0.647099, acc.: 63.28%] [G loss: 1.460916]\n",
      "epoch:0 step:163 [D loss: 0.663229, acc.: 62.50%] [G loss: 1.537482]\n",
      "epoch:0 step:164 [D loss: 0.731643, acc.: 55.47%] [G loss: 1.356990]\n",
      "epoch:0 step:165 [D loss: 0.681375, acc.: 57.81%] [G loss: 1.374301]\n",
      "epoch:0 step:166 [D loss: 0.677064, acc.: 56.25%] [G loss: 1.509098]\n",
      "epoch:0 step:167 [D loss: 0.710881, acc.: 54.69%] [G loss: 1.466742]\n",
      "epoch:0 step:168 [D loss: 0.671252, acc.: 61.72%] [G loss: 1.473142]\n",
      "epoch:0 step:169 [D loss: 0.690371, acc.: 53.12%] [G loss: 1.521878]\n",
      "epoch:0 step:170 [D loss: 0.684370, acc.: 53.91%] [G loss: 1.431030]\n",
      "epoch:0 step:171 [D loss: 0.662412, acc.: 60.94%] [G loss: 1.511599]\n",
      "epoch:0 step:172 [D loss: 0.672513, acc.: 67.19%] [G loss: 1.432482]\n",
      "epoch:0 step:173 [D loss: 0.696422, acc.: 57.03%] [G loss: 1.402576]\n",
      "epoch:0 step:174 [D loss: 0.680728, acc.: 59.38%] [G loss: 1.355850]\n",
      "epoch:0 step:175 [D loss: 0.785837, acc.: 50.78%] [G loss: 1.378631]\n",
      "epoch:0 step:176 [D loss: 0.704709, acc.: 60.94%] [G loss: 1.371477]\n",
      "epoch:0 step:177 [D loss: 0.716030, acc.: 55.47%] [G loss: 1.378384]\n",
      "epoch:0 step:178 [D loss: 0.742466, acc.: 54.69%] [G loss: 1.441476]\n",
      "epoch:0 step:179 [D loss: 0.694420, acc.: 52.34%] [G loss: 1.482892]\n",
      "epoch:0 step:180 [D loss: 0.667300, acc.: 59.38%] [G loss: 1.420446]\n",
      "epoch:0 step:181 [D loss: 0.660886, acc.: 60.94%] [G loss: 1.478009]\n",
      "epoch:0 step:182 [D loss: 0.676108, acc.: 55.47%] [G loss: 1.343447]\n",
      "epoch:0 step:183 [D loss: 0.743254, acc.: 56.25%] [G loss: 1.489618]\n",
      "epoch:0 step:184 [D loss: 0.710646, acc.: 53.12%] [G loss: 1.588313]\n",
      "epoch:0 step:185 [D loss: 0.725967, acc.: 49.22%] [G loss: 1.409685]\n",
      "epoch:0 step:186 [D loss: 0.678117, acc.: 56.25%] [G loss: 1.605981]\n",
      "epoch:0 step:187 [D loss: 0.654129, acc.: 60.16%] [G loss: 1.602175]\n",
      "epoch:0 step:188 [D loss: 0.678325, acc.: 61.72%] [G loss: 1.449661]\n",
      "epoch:0 step:189 [D loss: 0.577672, acc.: 70.31%] [G loss: 1.324248]\n",
      "epoch:0 step:190 [D loss: 0.675361, acc.: 62.50%] [G loss: 1.427535]\n",
      "epoch:0 step:191 [D loss: 0.601846, acc.: 68.75%] [G loss: 1.612268]\n",
      "epoch:0 step:192 [D loss: 0.691740, acc.: 62.50%] [G loss: 1.417476]\n",
      "epoch:0 step:193 [D loss: 0.643747, acc.: 60.94%] [G loss: 1.277630]\n",
      "epoch:0 step:194 [D loss: 0.648188, acc.: 67.19%] [G loss: 1.546156]\n",
      "epoch:0 step:195 [D loss: 0.702251, acc.: 62.50%] [G loss: 1.416366]\n",
      "epoch:0 step:196 [D loss: 0.677376, acc.: 60.16%] [G loss: 1.524436]\n",
      "epoch:0 step:197 [D loss: 0.803630, acc.: 43.75%] [G loss: 1.401443]\n",
      "epoch:0 step:198 [D loss: 0.596260, acc.: 67.97%] [G loss: 1.506514]\n",
      "epoch:0 step:199 [D loss: 0.679543, acc.: 57.03%] [G loss: 1.564555]\n",
      "epoch:0 step:200 [D loss: 0.647774, acc.: 64.06%] [G loss: 1.404468]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/imi432_006/anaconda3/envs/tf/lib/python3.5/site-packages/ipykernel_launcher.py:57: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "/home/imi432_006/anaconda3/envs/tf/lib/python3.5/site-packages/ipykernel_launcher.py:59: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "/home/imi432_006/anaconda3/envs/tf/lib/python3.5/site-packages/ipykernel_launcher.py:86: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "/home/imi432_006/anaconda3/envs/tf/lib/python3.5/site-packages/ipykernel_launcher.py:90: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##############\n",
      "[0.85994204 0.79163246 0.83068061 0.81380153 0.76719214 0.83036596\n",
      " 0.89158812 0.82006953 0.83481217 0.82921865]\n",
      "##########\n",
      "epoch:0 step:201 [D loss: 0.684458, acc.: 64.84%] [G loss: 1.346900]\n",
      "epoch:0 step:202 [D loss: 0.748635, acc.: 58.59%] [G loss: 1.384509]\n",
      "epoch:0 step:203 [D loss: 0.629140, acc.: 64.84%] [G loss: 1.379705]\n",
      "epoch:0 step:204 [D loss: 0.634096, acc.: 60.16%] [G loss: 1.443540]\n",
      "epoch:0 step:205 [D loss: 0.718274, acc.: 54.69%] [G loss: 1.357024]\n",
      "epoch:0 step:206 [D loss: 0.729745, acc.: 48.44%] [G loss: 1.357837]\n",
      "epoch:0 step:207 [D loss: 0.659969, acc.: 59.38%] [G loss: 1.366038]\n",
      "epoch:0 step:208 [D loss: 0.694252, acc.: 55.47%] [G loss: 1.344412]\n",
      "epoch:0 step:209 [D loss: 0.630773, acc.: 58.59%] [G loss: 1.382995]\n",
      "epoch:0 step:210 [D loss: 0.651820, acc.: 60.16%] [G loss: 1.487133]\n",
      "epoch:0 step:211 [D loss: 0.658237, acc.: 63.28%] [G loss: 1.337182]\n",
      "epoch:0 step:212 [D loss: 0.624054, acc.: 64.84%] [G loss: 1.271956]\n",
      "epoch:0 step:213 [D loss: 0.681356, acc.: 53.91%] [G loss: 1.369923]\n",
      "epoch:0 step:214 [D loss: 0.712538, acc.: 60.16%] [G loss: 1.402593]\n",
      "epoch:0 step:215 [D loss: 0.625568, acc.: 64.06%] [G loss: 1.279233]\n",
      "epoch:0 step:216 [D loss: 0.625896, acc.: 69.53%] [G loss: 1.461171]\n",
      "epoch:0 step:217 [D loss: 0.677243, acc.: 57.81%] [G loss: 1.312501]\n",
      "epoch:0 step:218 [D loss: 0.617821, acc.: 60.94%] [G loss: 1.313161]\n",
      "epoch:0 step:219 [D loss: 0.620854, acc.: 71.09%] [G loss: 1.383851]\n",
      "epoch:0 step:220 [D loss: 0.658990, acc.: 67.97%] [G loss: 1.311846]\n",
      "epoch:0 step:221 [D loss: 0.658618, acc.: 64.06%] [G loss: 1.250251]\n",
      "epoch:0 step:222 [D loss: 0.720569, acc.: 51.56%] [G loss: 1.259835]\n",
      "epoch:0 step:223 [D loss: 0.658007, acc.: 65.62%] [G loss: 1.415459]\n",
      "epoch:0 step:224 [D loss: 0.652532, acc.: 58.59%] [G loss: 1.416685]\n",
      "epoch:0 step:225 [D loss: 0.653812, acc.: 62.50%] [G loss: 1.421011]\n",
      "epoch:0 step:226 [D loss: 0.642951, acc.: 64.06%] [G loss: 1.350572]\n",
      "epoch:0 step:227 [D loss: 0.713438, acc.: 55.47%] [G loss: 1.307925]\n",
      "epoch:0 step:228 [D loss: 0.710048, acc.: 60.16%] [G loss: 1.417329]\n",
      "epoch:0 step:229 [D loss: 0.700904, acc.: 56.25%] [G loss: 1.409837]\n",
      "epoch:0 step:230 [D loss: 0.660210, acc.: 59.38%] [G loss: 1.395783]\n",
      "epoch:0 step:231 [D loss: 0.652167, acc.: 68.75%] [G loss: 1.403788]\n",
      "epoch:0 step:232 [D loss: 0.723162, acc.: 53.12%] [G loss: 1.357647]\n",
      "epoch:0 step:233 [D loss: 0.725913, acc.: 54.69%] [G loss: 1.293410]\n",
      "epoch:0 step:234 [D loss: 0.665533, acc.: 60.16%] [G loss: 1.362167]\n",
      "epoch:0 step:235 [D loss: 0.646652, acc.: 58.59%] [G loss: 1.270996]\n",
      "epoch:0 step:236 [D loss: 0.618500, acc.: 67.97%] [G loss: 1.333028]\n",
      "epoch:0 step:237 [D loss: 0.691859, acc.: 60.16%] [G loss: 1.386081]\n",
      "epoch:0 step:238 [D loss: 0.642705, acc.: 65.62%] [G loss: 1.459681]\n",
      "epoch:0 step:239 [D loss: 0.700073, acc.: 64.06%] [G loss: 1.315503]\n",
      "epoch:0 step:240 [D loss: 0.711799, acc.: 53.91%] [G loss: 1.283445]\n",
      "epoch:0 step:241 [D loss: 0.650173, acc.: 60.16%] [G loss: 1.447540]\n",
      "epoch:0 step:242 [D loss: 0.642246, acc.: 64.84%] [G loss: 1.372421]\n",
      "epoch:0 step:243 [D loss: 0.665668, acc.: 59.38%] [G loss: 1.388108]\n",
      "epoch:0 step:244 [D loss: 0.649358, acc.: 59.38%] [G loss: 1.513633]\n",
      "epoch:0 step:245 [D loss: 0.748432, acc.: 50.00%] [G loss: 1.332555]\n",
      "epoch:0 step:246 [D loss: 0.709080, acc.: 56.25%] [G loss: 1.401526]\n",
      "epoch:0 step:247 [D loss: 0.643972, acc.: 66.41%] [G loss: 1.386296]\n",
      "epoch:0 step:248 [D loss: 0.686587, acc.: 59.38%] [G loss: 1.392270]\n",
      "epoch:0 step:249 [D loss: 0.666335, acc.: 61.72%] [G loss: 1.332645]\n",
      "epoch:0 step:250 [D loss: 0.671990, acc.: 64.06%] [G loss: 1.431754]\n",
      "epoch:0 step:251 [D loss: 0.668496, acc.: 60.94%] [G loss: 1.412268]\n",
      "epoch:0 step:252 [D loss: 0.757804, acc.: 58.59%] [G loss: 1.347891]\n",
      "epoch:0 step:253 [D loss: 0.706252, acc.: 51.56%] [G loss: 1.409448]\n",
      "epoch:0 step:254 [D loss: 0.741971, acc.: 57.03%] [G loss: 1.329714]\n",
      "epoch:0 step:255 [D loss: 0.847252, acc.: 39.84%] [G loss: 1.366361]\n",
      "epoch:0 step:256 [D loss: 0.775609, acc.: 55.47%] [G loss: 1.229191]\n",
      "epoch:0 step:257 [D loss: 0.705814, acc.: 52.34%] [G loss: 1.192946]\n",
      "epoch:0 step:258 [D loss: 0.713447, acc.: 56.25%] [G loss: 1.396131]\n",
      "epoch:0 step:259 [D loss: 0.813688, acc.: 50.00%] [G loss: 1.253141]\n",
      "epoch:0 step:260 [D loss: 0.802024, acc.: 45.31%] [G loss: 1.245573]\n",
      "epoch:0 step:261 [D loss: 0.692415, acc.: 58.59%] [G loss: 1.394891]\n",
      "epoch:0 step:262 [D loss: 0.735002, acc.: 56.25%] [G loss: 1.365100]\n",
      "epoch:0 step:263 [D loss: 0.764095, acc.: 41.41%] [G loss: 1.203664]\n",
      "epoch:0 step:264 [D loss: 0.712020, acc.: 53.91%] [G loss: 1.321451]\n",
      "epoch:0 step:265 [D loss: 0.743489, acc.: 50.00%] [G loss: 1.309913]\n",
      "epoch:0 step:266 [D loss: 0.775845, acc.: 57.03%] [G loss: 1.287543]\n",
      "epoch:0 step:267 [D loss: 0.766647, acc.: 44.53%] [G loss: 1.293844]\n",
      "epoch:0 step:268 [D loss: 0.701861, acc.: 56.25%] [G loss: 1.288719]\n",
      "epoch:0 step:269 [D loss: 0.765716, acc.: 50.00%] [G loss: 1.145052]\n",
      "epoch:0 step:270 [D loss: 0.641049, acc.: 57.81%] [G loss: 1.225401]\n",
      "epoch:0 step:271 [D loss: 0.720471, acc.: 55.47%] [G loss: 1.309020]\n",
      "epoch:0 step:272 [D loss: 0.668069, acc.: 63.28%] [G loss: 1.379476]\n",
      "epoch:0 step:273 [D loss: 0.728420, acc.: 52.34%] [G loss: 1.329026]\n",
      "epoch:0 step:274 [D loss: 0.714815, acc.: 55.47%] [G loss: 1.248822]\n",
      "epoch:0 step:275 [D loss: 0.687406, acc.: 59.38%] [G loss: 1.239114]\n",
      "epoch:0 step:276 [D loss: 0.713453, acc.: 49.22%] [G loss: 1.409161]\n",
      "epoch:0 step:277 [D loss: 0.696327, acc.: 57.03%] [G loss: 1.305352]\n",
      "epoch:0 step:278 [D loss: 0.692217, acc.: 53.91%] [G loss: 1.307086]\n",
      "epoch:0 step:279 [D loss: 0.686126, acc.: 57.03%] [G loss: 1.205286]\n",
      "epoch:0 step:280 [D loss: 0.688797, acc.: 57.81%] [G loss: 1.265796]\n",
      "epoch:0 step:281 [D loss: 0.673899, acc.: 56.25%] [G loss: 1.268320]\n",
      "epoch:0 step:282 [D loss: 0.636885, acc.: 64.06%] [G loss: 1.304312]\n",
      "epoch:0 step:283 [D loss: 0.735700, acc.: 55.47%] [G loss: 1.307622]\n",
      "epoch:0 step:284 [D loss: 0.697738, acc.: 59.38%] [G loss: 1.272791]\n",
      "epoch:0 step:285 [D loss: 0.743981, acc.: 53.91%] [G loss: 1.252055]\n",
      "epoch:0 step:286 [D loss: 0.657162, acc.: 58.59%] [G loss: 1.264747]\n",
      "epoch:0 step:287 [D loss: 0.717794, acc.: 56.25%] [G loss: 1.268611]\n",
      "epoch:0 step:288 [D loss: 0.687005, acc.: 59.38%] [G loss: 1.191715]\n",
      "epoch:0 step:289 [D loss: 0.654905, acc.: 56.25%] [G loss: 1.275098]\n",
      "epoch:0 step:290 [D loss: 0.690871, acc.: 59.38%] [G loss: 1.335382]\n",
      "epoch:0 step:291 [D loss: 0.710469, acc.: 53.12%] [G loss: 1.258229]\n",
      "epoch:0 step:292 [D loss: 0.758695, acc.: 50.00%] [G loss: 1.296499]\n",
      "epoch:0 step:293 [D loss: 0.695290, acc.: 58.59%] [G loss: 1.168600]\n",
      "epoch:0 step:294 [D loss: 0.655816, acc.: 60.16%] [G loss: 1.262069]\n",
      "epoch:0 step:295 [D loss: 0.773153, acc.: 45.31%] [G loss: 1.288275]\n",
      "epoch:0 step:296 [D loss: 0.687819, acc.: 54.69%] [G loss: 1.322340]\n",
      "epoch:0 step:297 [D loss: 0.718800, acc.: 47.66%] [G loss: 1.298192]\n",
      "epoch:0 step:298 [D loss: 0.697625, acc.: 50.00%] [G loss: 1.235883]\n",
      "epoch:0 step:299 [D loss: 0.719157, acc.: 54.69%] [G loss: 1.169515]\n",
      "epoch:0 step:300 [D loss: 0.751140, acc.: 46.88%] [G loss: 1.261795]\n",
      "epoch:0 step:301 [D loss: 0.718734, acc.: 53.12%] [G loss: 1.313981]\n",
      "epoch:0 step:302 [D loss: 0.712205, acc.: 48.44%] [G loss: 1.272726]\n",
      "epoch:0 step:303 [D loss: 0.743547, acc.: 48.44%] [G loss: 1.257351]\n",
      "epoch:0 step:304 [D loss: 0.716040, acc.: 53.91%] [G loss: 1.229899]\n",
      "epoch:0 step:305 [D loss: 0.719050, acc.: 52.34%] [G loss: 1.287746]\n",
      "epoch:0 step:306 [D loss: 0.729796, acc.: 51.56%] [G loss: 1.322069]\n",
      "epoch:0 step:307 [D loss: 0.689772, acc.: 62.50%] [G loss: 1.242221]\n",
      "epoch:0 step:308 [D loss: 0.662621, acc.: 62.50%] [G loss: 1.235051]\n",
      "epoch:0 step:309 [D loss: 0.673409, acc.: 58.59%] [G loss: 1.234527]\n",
      "epoch:0 step:310 [D loss: 0.629124, acc.: 64.06%] [G loss: 1.208101]\n",
      "epoch:0 step:311 [D loss: 0.658540, acc.: 66.41%] [G loss: 1.223062]\n",
      "epoch:0 step:312 [D loss: 0.736120, acc.: 57.03%] [G loss: 1.319302]\n",
      "epoch:0 step:313 [D loss: 0.595346, acc.: 70.31%] [G loss: 1.264596]\n",
      "epoch:0 step:314 [D loss: 0.703763, acc.: 60.94%] [G loss: 1.145497]\n",
      "epoch:0 step:315 [D loss: 0.688855, acc.: 57.03%] [G loss: 1.276178]\n",
      "epoch:0 step:316 [D loss: 0.741988, acc.: 54.69%] [G loss: 1.340498]\n",
      "epoch:0 step:317 [D loss: 0.717163, acc.: 51.56%] [G loss: 1.216951]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0 step:318 [D loss: 0.726557, acc.: 50.00%] [G loss: 1.208888]\n",
      "epoch:0 step:319 [D loss: 0.722064, acc.: 52.34%] [G loss: 1.346618]\n",
      "epoch:0 step:320 [D loss: 0.759954, acc.: 46.88%] [G loss: 1.335862]\n",
      "epoch:0 step:321 [D loss: 0.706488, acc.: 51.56%] [G loss: 1.242213]\n",
      "epoch:0 step:322 [D loss: 0.728899, acc.: 54.69%] [G loss: 1.195866]\n",
      "epoch:0 step:323 [D loss: 0.682647, acc.: 61.72%] [G loss: 1.244828]\n",
      "epoch:0 step:324 [D loss: 0.701742, acc.: 52.34%] [G loss: 1.289971]\n",
      "epoch:0 step:325 [D loss: 0.681836, acc.: 60.94%] [G loss: 1.294576]\n",
      "epoch:0 step:326 [D loss: 0.701909, acc.: 54.69%] [G loss: 1.262037]\n",
      "epoch:0 step:327 [D loss: 0.719833, acc.: 55.47%] [G loss: 1.234591]\n",
      "epoch:0 step:328 [D loss: 0.649682, acc.: 60.16%] [G loss: 1.219252]\n",
      "epoch:0 step:329 [D loss: 0.700222, acc.: 53.12%] [G loss: 1.191862]\n",
      "epoch:0 step:330 [D loss: 0.639710, acc.: 62.50%] [G loss: 1.198260]\n",
      "epoch:0 step:331 [D loss: 0.692894, acc.: 60.94%] [G loss: 1.244631]\n",
      "epoch:0 step:332 [D loss: 0.680871, acc.: 61.72%] [G loss: 1.317271]\n",
      "epoch:0 step:333 [D loss: 0.661219, acc.: 62.50%] [G loss: 1.323322]\n",
      "epoch:0 step:334 [D loss: 0.645283, acc.: 64.06%] [G loss: 1.299083]\n",
      "epoch:0 step:335 [D loss: 0.719354, acc.: 50.00%] [G loss: 1.242497]\n",
      "epoch:0 step:336 [D loss: 0.688093, acc.: 57.03%] [G loss: 1.353822]\n",
      "epoch:0 step:337 [D loss: 0.691573, acc.: 57.81%] [G loss: 1.379950]\n",
      "epoch:0 step:338 [D loss: 0.623272, acc.: 68.75%] [G loss: 1.404814]\n",
      "epoch:0 step:339 [D loss: 0.645080, acc.: 64.06%] [G loss: 1.347695]\n",
      "epoch:0 step:340 [D loss: 0.735616, acc.: 53.91%] [G loss: 1.422424]\n",
      "epoch:0 step:341 [D loss: 0.748571, acc.: 56.25%] [G loss: 1.276272]\n",
      "epoch:0 step:342 [D loss: 0.625542, acc.: 62.50%] [G loss: 1.276426]\n",
      "epoch:0 step:343 [D loss: 0.674585, acc.: 57.03%] [G loss: 1.302723]\n",
      "epoch:0 step:344 [D loss: 0.674040, acc.: 56.25%] [G loss: 1.223111]\n",
      "epoch:0 step:345 [D loss: 0.670091, acc.: 58.59%] [G loss: 1.475518]\n",
      "epoch:0 step:346 [D loss: 0.696174, acc.: 57.81%] [G loss: 1.332833]\n",
      "epoch:0 step:347 [D loss: 0.667110, acc.: 53.12%] [G loss: 1.252000]\n",
      "epoch:0 step:348 [D loss: 0.675281, acc.: 63.28%] [G loss: 1.183941]\n",
      "epoch:0 step:349 [D loss: 0.561847, acc.: 72.66%] [G loss: 1.365655]\n",
      "epoch:0 step:350 [D loss: 0.653921, acc.: 61.72%] [G loss: 1.399311]\n",
      "epoch:0 step:351 [D loss: 0.763217, acc.: 64.06%] [G loss: 1.332055]\n",
      "epoch:0 step:352 [D loss: 0.639697, acc.: 65.62%] [G loss: 1.304820]\n",
      "epoch:0 step:353 [D loss: 0.574771, acc.: 69.53%] [G loss: 1.371057]\n",
      "epoch:0 step:354 [D loss: 0.657951, acc.: 64.06%] [G loss: 1.385579]\n",
      "epoch:0 step:355 [D loss: 0.651456, acc.: 59.38%] [G loss: 1.362707]\n",
      "epoch:0 step:356 [D loss: 0.681165, acc.: 63.28%] [G loss: 1.266548]\n",
      "epoch:0 step:357 [D loss: 0.665905, acc.: 54.69%] [G loss: 1.313910]\n",
      "epoch:0 step:358 [D loss: 0.690460, acc.: 56.25%] [G loss: 1.293277]\n",
      "epoch:0 step:359 [D loss: 0.671903, acc.: 62.50%] [G loss: 1.171237]\n",
      "epoch:0 step:360 [D loss: 0.676385, acc.: 54.69%] [G loss: 1.212845]\n",
      "epoch:0 step:361 [D loss: 0.607977, acc.: 61.72%] [G loss: 1.264892]\n",
      "epoch:0 step:362 [D loss: 0.664205, acc.: 62.50%] [G loss: 1.308471]\n",
      "epoch:0 step:363 [D loss: 0.648998, acc.: 58.59%] [G loss: 1.234267]\n",
      "epoch:0 step:364 [D loss: 0.657745, acc.: 60.94%] [G loss: 1.273247]\n",
      "epoch:0 step:365 [D loss: 0.596926, acc.: 71.09%] [G loss: 1.284297]\n",
      "epoch:0 step:366 [D loss: 0.636072, acc.: 66.41%] [G loss: 1.255511]\n",
      "epoch:0 step:367 [D loss: 0.582660, acc.: 71.88%] [G loss: 1.246485]\n",
      "epoch:0 step:368 [D loss: 0.628434, acc.: 64.84%] [G loss: 1.280589]\n",
      "epoch:0 step:369 [D loss: 0.563758, acc.: 71.09%] [G loss: 1.335495]\n",
      "epoch:0 step:370 [D loss: 0.660645, acc.: 65.62%] [G loss: 1.464329]\n",
      "epoch:0 step:371 [D loss: 0.648408, acc.: 64.06%] [G loss: 1.293324]\n",
      "epoch:0 step:372 [D loss: 0.633123, acc.: 60.94%] [G loss: 1.236737]\n",
      "epoch:0 step:373 [D loss: 0.698934, acc.: 52.34%] [G loss: 1.325789]\n",
      "epoch:0 step:374 [D loss: 0.668517, acc.: 61.72%] [G loss: 1.246253]\n",
      "epoch:0 step:375 [D loss: 0.636111, acc.: 65.62%] [G loss: 1.277311]\n",
      "epoch:0 step:376 [D loss: 0.631832, acc.: 67.97%] [G loss: 1.394743]\n",
      "epoch:0 step:377 [D loss: 0.630111, acc.: 70.31%] [G loss: 1.366774]\n",
      "epoch:0 step:378 [D loss: 0.700333, acc.: 53.12%] [G loss: 1.435720]\n",
      "epoch:0 step:379 [D loss: 0.616735, acc.: 67.19%] [G loss: 1.402627]\n",
      "epoch:0 step:380 [D loss: 0.656686, acc.: 60.16%] [G loss: 1.283852]\n",
      "epoch:0 step:381 [D loss: 0.697941, acc.: 57.03%] [G loss: 1.354893]\n",
      "epoch:0 step:382 [D loss: 0.644997, acc.: 60.94%] [G loss: 1.507509]\n",
      "epoch:0 step:383 [D loss: 0.730669, acc.: 58.59%] [G loss: 1.267590]\n",
      "epoch:0 step:384 [D loss: 0.678511, acc.: 64.06%] [G loss: 1.185478]\n",
      "epoch:0 step:385 [D loss: 0.695755, acc.: 53.91%] [G loss: 1.200785]\n",
      "epoch:0 step:386 [D loss: 0.700052, acc.: 57.03%] [G loss: 1.187465]\n",
      "epoch:0 step:387 [D loss: 0.679151, acc.: 51.56%] [G loss: 1.200528]\n",
      "epoch:0 step:388 [D loss: 0.672880, acc.: 62.50%] [G loss: 1.243797]\n",
      "epoch:0 step:389 [D loss: 0.670168, acc.: 57.81%] [G loss: 1.374148]\n",
      "epoch:0 step:390 [D loss: 0.675858, acc.: 54.69%] [G loss: 1.360138]\n",
      "epoch:0 step:391 [D loss: 0.721559, acc.: 50.00%] [G loss: 1.262479]\n",
      "epoch:0 step:392 [D loss: 0.685943, acc.: 60.94%] [G loss: 1.282849]\n",
      "epoch:0 step:393 [D loss: 0.646070, acc.: 63.28%] [G loss: 1.415573]\n",
      "epoch:0 step:394 [D loss: 0.703380, acc.: 56.25%] [G loss: 1.279221]\n",
      "epoch:0 step:395 [D loss: 0.676149, acc.: 57.03%] [G loss: 1.339408]\n",
      "epoch:0 step:396 [D loss: 0.685089, acc.: 57.03%] [G loss: 1.325535]\n",
      "epoch:0 step:397 [D loss: 0.675461, acc.: 54.69%] [G loss: 1.331485]\n",
      "epoch:0 step:398 [D loss: 0.711954, acc.: 57.03%] [G loss: 1.238831]\n",
      "epoch:0 step:399 [D loss: 0.634204, acc.: 63.28%] [G loss: 1.308070]\n",
      "epoch:0 step:400 [D loss: 0.719618, acc.: 53.91%] [G loss: 1.282108]\n",
      "##############\n",
      "[0.83564422 0.81016428 0.78488432 0.80712335 0.77224375 0.819529\n",
      " 0.87934558 0.83336217 0.80156255 0.81321755]\n",
      "##########\n",
      "epoch:0 step:401 [D loss: 0.731439, acc.: 51.56%] [G loss: 1.278418]\n",
      "epoch:0 step:402 [D loss: 0.733105, acc.: 52.34%] [G loss: 1.246669]\n",
      "epoch:0 step:403 [D loss: 0.681664, acc.: 62.50%] [G loss: 1.256584]\n",
      "epoch:0 step:404 [D loss: 0.661864, acc.: 54.69%] [G loss: 1.341087]\n",
      "epoch:0 step:405 [D loss: 0.704080, acc.: 59.38%] [G loss: 1.383354]\n",
      "epoch:0 step:406 [D loss: 0.727233, acc.: 52.34%] [G loss: 1.311719]\n",
      "epoch:0 step:407 [D loss: 0.676732, acc.: 60.16%] [G loss: 1.262607]\n",
      "epoch:0 step:408 [D loss: 0.715373, acc.: 56.25%] [G loss: 1.252029]\n",
      "epoch:0 step:409 [D loss: 0.662000, acc.: 60.16%] [G loss: 1.212875]\n",
      "epoch:0 step:410 [D loss: 0.648261, acc.: 58.59%] [G loss: 1.231112]\n",
      "epoch:0 step:411 [D loss: 0.675368, acc.: 57.03%] [G loss: 1.271071]\n",
      "epoch:0 step:412 [D loss: 0.660333, acc.: 61.72%] [G loss: 1.278999]\n",
      "epoch:0 step:413 [D loss: 0.662831, acc.: 61.72%] [G loss: 1.316926]\n",
      "epoch:0 step:414 [D loss: 0.645351, acc.: 64.06%] [G loss: 1.274754]\n",
      "epoch:0 step:415 [D loss: 0.658601, acc.: 67.97%] [G loss: 1.356716]\n",
      "epoch:0 step:416 [D loss: 0.640081, acc.: 60.94%] [G loss: 1.247880]\n",
      "epoch:0 step:417 [D loss: 0.655349, acc.: 59.38%] [G loss: 1.131783]\n",
      "epoch:0 step:418 [D loss: 0.694737, acc.: 64.06%] [G loss: 1.308394]\n",
      "epoch:0 step:419 [D loss: 0.683681, acc.: 68.75%] [G loss: 1.320761]\n",
      "epoch:0 step:420 [D loss: 0.691674, acc.: 54.69%] [G loss: 1.281568]\n",
      "epoch:0 step:421 [D loss: 0.668706, acc.: 56.25%] [G loss: 1.343705]\n",
      "epoch:0 step:422 [D loss: 0.687443, acc.: 59.38%] [G loss: 1.327801]\n",
      "epoch:0 step:423 [D loss: 0.632999, acc.: 67.19%] [G loss: 1.298535]\n",
      "epoch:0 step:424 [D loss: 0.711565, acc.: 54.69%] [G loss: 1.346774]\n",
      "epoch:0 step:425 [D loss: 0.700165, acc.: 56.25%] [G loss: 1.183386]\n",
      "epoch:0 step:426 [D loss: 0.702769, acc.: 58.59%] [G loss: 1.316732]\n",
      "epoch:0 step:427 [D loss: 0.675925, acc.: 65.62%] [G loss: 1.330951]\n",
      "epoch:0 step:428 [D loss: 0.692522, acc.: 51.56%] [G loss: 1.194911]\n",
      "epoch:0 step:429 [D loss: 0.681014, acc.: 57.03%] [G loss: 1.196830]\n",
      "epoch:0 step:430 [D loss: 0.660675, acc.: 57.81%] [G loss: 1.320585]\n",
      "epoch:0 step:431 [D loss: 0.649411, acc.: 61.72%] [G loss: 1.378428]\n",
      "epoch:0 step:432 [D loss: 0.663567, acc.: 59.38%] [G loss: 1.358656]\n",
      "epoch:0 step:433 [D loss: 0.760994, acc.: 52.34%] [G loss: 1.362276]\n",
      "epoch:0 step:434 [D loss: 0.710460, acc.: 53.91%] [G loss: 1.338194]\n",
      "epoch:0 step:435 [D loss: 0.692473, acc.: 54.69%] [G loss: 1.080695]\n",
      "epoch:0 step:436 [D loss: 0.677157, acc.: 58.59%] [G loss: 1.199605]\n",
      "epoch:0 step:437 [D loss: 0.684116, acc.: 53.12%] [G loss: 1.286535]\n",
      "epoch:0 step:438 [D loss: 0.731137, acc.: 51.56%] [G loss: 1.326180]\n",
      "epoch:0 step:439 [D loss: 0.713598, acc.: 59.38%] [G loss: 1.187539]\n",
      "epoch:0 step:440 [D loss: 0.664719, acc.: 57.81%] [G loss: 1.338667]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0 step:441 [D loss: 0.789274, acc.: 51.56%] [G loss: 1.205037]\n",
      "epoch:0 step:442 [D loss: 0.718625, acc.: 50.00%] [G loss: 1.163618]\n",
      "epoch:0 step:443 [D loss: 0.694296, acc.: 54.69%] [G loss: 1.157874]\n",
      "epoch:0 step:444 [D loss: 0.717634, acc.: 45.31%] [G loss: 1.166036]\n",
      "epoch:0 step:445 [D loss: 0.683062, acc.: 53.91%] [G loss: 1.195903]\n",
      "epoch:0 step:446 [D loss: 0.669148, acc.: 58.59%] [G loss: 1.232900]\n",
      "epoch:0 step:447 [D loss: 0.708106, acc.: 51.56%] [G loss: 1.216952]\n",
      "epoch:0 step:448 [D loss: 0.687824, acc.: 57.81%] [G loss: 1.155425]\n",
      "epoch:0 step:449 [D loss: 0.777019, acc.: 53.12%] [G loss: 1.121838]\n",
      "epoch:0 step:450 [D loss: 0.637829, acc.: 55.47%] [G loss: 1.178100]\n",
      "epoch:0 step:451 [D loss: 0.690623, acc.: 53.91%] [G loss: 1.161043]\n",
      "epoch:0 step:452 [D loss: 0.642465, acc.: 59.38%] [G loss: 1.207883]\n",
      "epoch:0 step:453 [D loss: 0.694868, acc.: 56.25%] [G loss: 1.213819]\n",
      "epoch:0 step:454 [D loss: 0.683821, acc.: 57.03%] [G loss: 1.235193]\n",
      "epoch:0 step:455 [D loss: 0.657334, acc.: 63.28%] [G loss: 1.355562]\n",
      "epoch:0 step:456 [D loss: 0.701499, acc.: 53.91%] [G loss: 1.249665]\n",
      "epoch:0 step:457 [D loss: 0.646390, acc.: 63.28%] [G loss: 1.112519]\n",
      "epoch:0 step:458 [D loss: 0.677562, acc.: 54.69%] [G loss: 1.095081]\n",
      "epoch:0 step:459 [D loss: 0.715946, acc.: 50.78%] [G loss: 1.248011]\n",
      "epoch:0 step:460 [D loss: 0.679713, acc.: 52.34%] [G loss: 1.212592]\n",
      "epoch:0 step:461 [D loss: 0.708891, acc.: 51.56%] [G loss: 1.194068]\n",
      "epoch:0 step:462 [D loss: 0.703634, acc.: 50.78%] [G loss: 1.264113]\n",
      "epoch:0 step:463 [D loss: 0.710325, acc.: 50.78%] [G loss: 1.272768]\n",
      "epoch:0 step:464 [D loss: 0.675252, acc.: 61.72%] [G loss: 1.215430]\n",
      "epoch:0 step:465 [D loss: 0.686820, acc.: 53.91%] [G loss: 1.195015]\n",
      "epoch:0 step:466 [D loss: 0.659422, acc.: 62.50%] [G loss: 1.131388]\n",
      "epoch:0 step:467 [D loss: 0.677578, acc.: 56.25%] [G loss: 1.316925]\n",
      "epoch:0 step:468 [D loss: 0.666166, acc.: 57.81%] [G loss: 1.371515]\n",
      "epoch:0 step:469 [D loss: 0.752742, acc.: 48.44%] [G loss: 1.341097]\n",
      "epoch:0 step:470 [D loss: 0.743418, acc.: 51.56%] [G loss: 1.307083]\n",
      "epoch:0 step:471 [D loss: 0.709184, acc.: 50.00%] [G loss: 1.238249]\n",
      "epoch:0 step:472 [D loss: 0.721227, acc.: 53.12%] [G loss: 1.186759]\n",
      "epoch:0 step:473 [D loss: 0.710528, acc.: 57.81%] [G loss: 1.151731]\n",
      "epoch:0 step:474 [D loss: 0.687663, acc.: 62.50%] [G loss: 1.161575]\n",
      "epoch:0 step:475 [D loss: 0.686148, acc.: 55.47%] [G loss: 1.136536]\n",
      "epoch:0 step:476 [D loss: 0.696208, acc.: 50.00%] [G loss: 1.100423]\n",
      "epoch:0 step:477 [D loss: 0.657277, acc.: 57.03%] [G loss: 1.153680]\n",
      "epoch:0 step:478 [D loss: 0.689926, acc.: 52.34%] [G loss: 1.280259]\n",
      "epoch:0 step:479 [D loss: 0.727198, acc.: 53.91%] [G loss: 1.174096]\n",
      "epoch:0 step:480 [D loss: 0.718658, acc.: 51.56%] [G loss: 1.177713]\n",
      "epoch:0 step:481 [D loss: 0.699980, acc.: 50.78%] [G loss: 1.301664]\n",
      "epoch:0 step:482 [D loss: 0.695959, acc.: 53.12%] [G loss: 1.250767]\n",
      "epoch:0 step:483 [D loss: 0.664543, acc.: 57.03%] [G loss: 1.336429]\n",
      "epoch:0 step:484 [D loss: 0.718942, acc.: 53.12%] [G loss: 1.271615]\n",
      "epoch:0 step:485 [D loss: 0.694151, acc.: 57.03%] [G loss: 1.243675]\n",
      "epoch:0 step:486 [D loss: 0.742510, acc.: 41.41%] [G loss: 1.251512]\n",
      "epoch:0 step:487 [D loss: 0.685172, acc.: 53.91%] [G loss: 1.185136]\n",
      "epoch:0 step:488 [D loss: 0.669973, acc.: 53.91%] [G loss: 1.309726]\n",
      "epoch:0 step:489 [D loss: 0.766388, acc.: 46.88%] [G loss: 1.289982]\n",
      "epoch:0 step:490 [D loss: 0.729925, acc.: 42.19%] [G loss: 1.247900]\n",
      "epoch:0 step:491 [D loss: 0.670100, acc.: 55.47%] [G loss: 1.138048]\n",
      "epoch:0 step:492 [D loss: 0.753748, acc.: 51.56%] [G loss: 1.241807]\n",
      "epoch:0 step:493 [D loss: 0.666295, acc.: 51.56%] [G loss: 1.251633]\n",
      "epoch:0 step:494 [D loss: 0.681694, acc.: 60.94%] [G loss: 1.284379]\n",
      "epoch:0 step:495 [D loss: 0.671519, acc.: 59.38%] [G loss: 1.182399]\n",
      "epoch:0 step:496 [D loss: 0.685313, acc.: 57.03%] [G loss: 1.102052]\n",
      "epoch:0 step:497 [D loss: 0.690314, acc.: 64.06%] [G loss: 1.177827]\n",
      "epoch:0 step:498 [D loss: 0.631469, acc.: 69.53%] [G loss: 1.296163]\n",
      "epoch:0 step:499 [D loss: 0.694122, acc.: 54.69%] [G loss: 1.267796]\n",
      "epoch:0 step:500 [D loss: 0.759543, acc.: 58.59%] [G loss: 1.246206]\n",
      "epoch:0 step:501 [D loss: 0.670700, acc.: 54.69%] [G loss: 1.278900]\n",
      "epoch:0 step:502 [D loss: 0.689813, acc.: 54.69%] [G loss: 1.254513]\n",
      "epoch:0 step:503 [D loss: 0.686477, acc.: 61.72%] [G loss: 1.253959]\n",
      "epoch:0 step:504 [D loss: 0.719840, acc.: 52.34%] [G loss: 1.189480]\n",
      "epoch:0 step:505 [D loss: 0.712550, acc.: 55.47%] [G loss: 1.244596]\n",
      "epoch:0 step:506 [D loss: 0.696705, acc.: 50.78%] [G loss: 1.300718]\n",
      "epoch:0 step:507 [D loss: 0.710852, acc.: 48.44%] [G loss: 1.218443]\n",
      "epoch:0 step:508 [D loss: 0.674470, acc.: 60.94%] [G loss: 1.150468]\n",
      "epoch:0 step:509 [D loss: 0.665032, acc.: 59.38%] [G loss: 1.174121]\n",
      "epoch:0 step:510 [D loss: 0.637231, acc.: 60.16%] [G loss: 1.251760]\n",
      "epoch:0 step:511 [D loss: 0.683740, acc.: 55.47%] [G loss: 1.288704]\n",
      "epoch:0 step:512 [D loss: 0.630926, acc.: 58.59%] [G loss: 1.443933]\n",
      "epoch:0 step:513 [D loss: 0.695207, acc.: 52.34%] [G loss: 1.275681]\n",
      "epoch:0 step:514 [D loss: 0.676684, acc.: 58.59%] [G loss: 1.227257]\n",
      "epoch:0 step:515 [D loss: 0.690033, acc.: 55.47%] [G loss: 1.181723]\n",
      "epoch:0 step:516 [D loss: 0.643024, acc.: 64.84%] [G loss: 1.175885]\n",
      "epoch:0 step:517 [D loss: 0.664160, acc.: 60.16%] [G loss: 1.147162]\n",
      "epoch:0 step:518 [D loss: 0.627811, acc.: 67.19%] [G loss: 1.189867]\n",
      "epoch:0 step:519 [D loss: 0.620446, acc.: 64.06%] [G loss: 1.237666]\n",
      "epoch:0 step:520 [D loss: 0.697717, acc.: 53.91%] [G loss: 1.186462]\n",
      "epoch:0 step:521 [D loss: 0.645010, acc.: 64.84%] [G loss: 1.125885]\n",
      "epoch:0 step:522 [D loss: 0.723334, acc.: 53.12%] [G loss: 1.188605]\n",
      "epoch:0 step:523 [D loss: 0.674687, acc.: 56.25%] [G loss: 1.235752]\n",
      "epoch:0 step:524 [D loss: 0.660971, acc.: 60.16%] [G loss: 1.153935]\n",
      "epoch:0 step:525 [D loss: 0.704806, acc.: 58.59%] [G loss: 1.149534]\n",
      "epoch:0 step:526 [D loss: 0.683740, acc.: 56.25%] [G loss: 1.213683]\n",
      "epoch:0 step:527 [D loss: 0.679070, acc.: 60.16%] [G loss: 1.228539]\n",
      "epoch:0 step:528 [D loss: 0.685439, acc.: 60.16%] [G loss: 1.156909]\n",
      "epoch:0 step:529 [D loss: 0.673082, acc.: 56.25%] [G loss: 1.121128]\n",
      "epoch:0 step:530 [D loss: 0.664174, acc.: 60.16%] [G loss: 1.113495]\n",
      "epoch:0 step:531 [D loss: 0.647306, acc.: 61.72%] [G loss: 1.164903]\n",
      "epoch:0 step:532 [D loss: 0.652755, acc.: 61.72%] [G loss: 1.204170]\n",
      "epoch:0 step:533 [D loss: 0.616244, acc.: 67.19%] [G loss: 1.227823]\n",
      "epoch:0 step:534 [D loss: 0.711056, acc.: 52.34%] [G loss: 1.143818]\n",
      "epoch:0 step:535 [D loss: 0.702753, acc.: 64.06%] [G loss: 1.173239]\n",
      "epoch:0 step:536 [D loss: 0.649398, acc.: 61.72%] [G loss: 1.219218]\n",
      "epoch:0 step:537 [D loss: 0.693542, acc.: 57.81%] [G loss: 1.239601]\n",
      "epoch:0 step:538 [D loss: 0.637686, acc.: 75.78%] [G loss: 1.218911]\n",
      "epoch:0 step:539 [D loss: 0.684062, acc.: 60.16%] [G loss: 1.247400]\n",
      "epoch:0 step:540 [D loss: 0.673401, acc.: 62.50%] [G loss: 1.209748]\n",
      "epoch:0 step:541 [D loss: 0.659760, acc.: 61.72%] [G loss: 1.294320]\n",
      "epoch:0 step:542 [D loss: 0.681692, acc.: 52.34%] [G loss: 1.182490]\n",
      "epoch:0 step:543 [D loss: 0.658796, acc.: 59.38%] [G loss: 1.198925]\n",
      "epoch:0 step:544 [D loss: 0.642892, acc.: 67.19%] [G loss: 1.254350]\n",
      "epoch:0 step:545 [D loss: 0.680451, acc.: 57.81%] [G loss: 1.207198]\n",
      "epoch:0 step:546 [D loss: 0.663274, acc.: 62.50%] [G loss: 1.158991]\n",
      "epoch:0 step:547 [D loss: 0.671997, acc.: 61.72%] [G loss: 1.215033]\n",
      "epoch:0 step:548 [D loss: 0.689627, acc.: 61.72%] [G loss: 1.137555]\n",
      "epoch:0 step:549 [D loss: 0.649253, acc.: 58.59%] [G loss: 1.234876]\n",
      "epoch:0 step:550 [D loss: 0.605159, acc.: 69.53%] [G loss: 1.344219]\n",
      "epoch:0 step:551 [D loss: 0.646667, acc.: 65.62%] [G loss: 1.234648]\n",
      "epoch:0 step:552 [D loss: 0.737656, acc.: 57.03%] [G loss: 1.400966]\n",
      "epoch:0 step:553 [D loss: 0.668260, acc.: 61.72%] [G loss: 1.157690]\n",
      "epoch:0 step:554 [D loss: 0.674734, acc.: 59.38%] [G loss: 1.139844]\n",
      "epoch:0 step:555 [D loss: 0.669046, acc.: 62.50%] [G loss: 1.183950]\n",
      "epoch:0 step:556 [D loss: 0.662254, acc.: 57.81%] [G loss: 1.191701]\n",
      "epoch:0 step:557 [D loss: 0.680955, acc.: 63.28%] [G loss: 1.160843]\n",
      "epoch:0 step:558 [D loss: 0.690168, acc.: 51.56%] [G loss: 1.163367]\n",
      "epoch:0 step:559 [D loss: 0.680451, acc.: 62.50%] [G loss: 1.139090]\n",
      "epoch:0 step:560 [D loss: 0.683457, acc.: 55.47%] [G loss: 1.179350]\n",
      "epoch:0 step:561 [D loss: 0.675834, acc.: 57.03%] [G loss: 1.212003]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0 step:562 [D loss: 0.693775, acc.: 61.72%] [G loss: 1.195033]\n",
      "epoch:0 step:563 [D loss: 0.765812, acc.: 41.41%] [G loss: 1.168511]\n",
      "epoch:0 step:564 [D loss: 0.709874, acc.: 46.88%] [G loss: 1.063621]\n",
      "epoch:0 step:565 [D loss: 0.649539, acc.: 61.72%] [G loss: 1.077760]\n",
      "epoch:0 step:566 [D loss: 0.653767, acc.: 59.38%] [G loss: 1.139408]\n",
      "epoch:0 step:567 [D loss: 0.653962, acc.: 59.38%] [G loss: 1.150622]\n",
      "epoch:0 step:568 [D loss: 0.646553, acc.: 61.72%] [G loss: 1.207091]\n",
      "epoch:0 step:569 [D loss: 0.652148, acc.: 67.19%] [G loss: 1.264532]\n",
      "epoch:0 step:570 [D loss: 0.681242, acc.: 58.59%] [G loss: 1.133715]\n",
      "epoch:0 step:571 [D loss: 0.610130, acc.: 71.88%] [G loss: 1.170511]\n",
      "epoch:0 step:572 [D loss: 0.655556, acc.: 61.72%] [G loss: 1.169307]\n",
      "epoch:0 step:573 [D loss: 0.691677, acc.: 54.69%] [G loss: 1.236251]\n",
      "epoch:0 step:574 [D loss: 0.668614, acc.: 57.81%] [G loss: 1.211458]\n",
      "epoch:0 step:575 [D loss: 0.614508, acc.: 67.19%] [G loss: 1.189825]\n",
      "epoch:0 step:576 [D loss: 0.646434, acc.: 56.25%] [G loss: 1.139366]\n",
      "epoch:0 step:577 [D loss: 0.682920, acc.: 53.91%] [G loss: 1.271911]\n",
      "epoch:0 step:578 [D loss: 0.705462, acc.: 57.03%] [G loss: 1.160488]\n",
      "epoch:0 step:579 [D loss: 0.659552, acc.: 64.06%] [G loss: 1.272582]\n",
      "epoch:0 step:580 [D loss: 0.670578, acc.: 65.62%] [G loss: 1.226277]\n",
      "epoch:0 step:581 [D loss: 0.664650, acc.: 57.81%] [G loss: 1.207167]\n",
      "epoch:0 step:582 [D loss: 0.688221, acc.: 68.75%] [G loss: 1.222466]\n",
      "epoch:0 step:583 [D loss: 0.650938, acc.: 66.41%] [G loss: 1.157093]\n",
      "epoch:0 step:584 [D loss: 0.633790, acc.: 63.28%] [G loss: 1.221798]\n",
      "epoch:0 step:585 [D loss: 0.657859, acc.: 60.94%] [G loss: 1.225123]\n",
      "epoch:0 step:586 [D loss: 0.687181, acc.: 54.69%] [G loss: 1.168903]\n",
      "epoch:0 step:587 [D loss: 0.639819, acc.: 62.50%] [G loss: 1.166087]\n",
      "epoch:0 step:588 [D loss: 0.698953, acc.: 56.25%] [G loss: 1.167759]\n",
      "epoch:0 step:589 [D loss: 0.676847, acc.: 57.03%] [G loss: 1.226720]\n",
      "epoch:0 step:590 [D loss: 0.654811, acc.: 69.53%] [G loss: 1.195307]\n",
      "epoch:0 step:591 [D loss: 0.678230, acc.: 59.38%] [G loss: 1.189988]\n",
      "epoch:0 step:592 [D loss: 0.681493, acc.: 62.50%] [G loss: 1.272374]\n",
      "epoch:0 step:593 [D loss: 0.698014, acc.: 55.47%] [G loss: 1.170690]\n",
      "epoch:0 step:594 [D loss: 0.750704, acc.: 49.22%] [G loss: 1.122559]\n",
      "epoch:0 step:595 [D loss: 0.667310, acc.: 55.47%] [G loss: 1.154826]\n",
      "epoch:0 step:596 [D loss: 0.666785, acc.: 56.25%] [G loss: 1.187128]\n",
      "epoch:0 step:597 [D loss: 0.692723, acc.: 50.00%] [G loss: 1.217237]\n",
      "epoch:0 step:598 [D loss: 0.660523, acc.: 62.50%] [G loss: 1.141569]\n",
      "epoch:0 step:599 [D loss: 0.651393, acc.: 58.59%] [G loss: 1.248656]\n",
      "epoch:0 step:600 [D loss: 0.650662, acc.: 64.06%] [G loss: 1.221783]\n",
      "##############\n",
      "[0.87850062 0.79729229 0.8232427  0.8451471  0.81541001 0.81056437\n",
      " 0.87672969 0.8254986  0.80843339 0.8120886 ]\n",
      "##########\n",
      "epoch:0 step:601 [D loss: 0.718692, acc.: 50.00%] [G loss: 1.111828]\n",
      "epoch:0 step:602 [D loss: 0.656669, acc.: 63.28%] [G loss: 1.114213]\n",
      "epoch:0 step:603 [D loss: 0.670534, acc.: 56.25%] [G loss: 1.135566]\n",
      "epoch:0 step:604 [D loss: 0.639151, acc.: 65.62%] [G loss: 1.204289]\n",
      "epoch:0 step:605 [D loss: 0.724186, acc.: 48.44%] [G loss: 1.153545]\n",
      "epoch:0 step:606 [D loss: 0.659575, acc.: 56.25%] [G loss: 1.215267]\n",
      "epoch:0 step:607 [D loss: 0.690018, acc.: 52.34%] [G loss: 1.151283]\n",
      "epoch:0 step:608 [D loss: 0.644500, acc.: 55.47%] [G loss: 1.142884]\n",
      "epoch:0 step:609 [D loss: 0.706052, acc.: 50.78%] [G loss: 1.068581]\n",
      "epoch:0 step:610 [D loss: 0.658440, acc.: 61.72%] [G loss: 1.141418]\n",
      "epoch:0 step:611 [D loss: 0.693395, acc.: 55.47%] [G loss: 1.140258]\n",
      "epoch:0 step:612 [D loss: 0.668183, acc.: 51.56%] [G loss: 1.137872]\n",
      "epoch:0 step:613 [D loss: 0.682123, acc.: 53.12%] [G loss: 1.093139]\n",
      "epoch:0 step:614 [D loss: 0.684916, acc.: 57.81%] [G loss: 1.168152]\n",
      "epoch:0 step:615 [D loss: 0.666509, acc.: 60.94%] [G loss: 1.087533]\n",
      "epoch:0 step:616 [D loss: 0.631907, acc.: 66.41%] [G loss: 1.091898]\n",
      "epoch:0 step:617 [D loss: 0.701273, acc.: 52.34%] [G loss: 1.100291]\n",
      "epoch:0 step:618 [D loss: 0.694217, acc.: 54.69%] [G loss: 1.123620]\n",
      "epoch:0 step:619 [D loss: 0.678588, acc.: 60.94%] [G loss: 1.215958]\n",
      "epoch:0 step:620 [D loss: 0.639880, acc.: 59.38%] [G loss: 1.223113]\n",
      "epoch:0 step:621 [D loss: 0.738875, acc.: 44.53%] [G loss: 1.094187]\n",
      "epoch:0 step:622 [D loss: 0.650770, acc.: 63.28%] [G loss: 1.151723]\n",
      "epoch:0 step:623 [D loss: 0.690632, acc.: 52.34%] [G loss: 1.201331]\n",
      "epoch:0 step:624 [D loss: 0.666487, acc.: 58.59%] [G loss: 1.180442]\n",
      "epoch:0 step:625 [D loss: 0.651785, acc.: 60.16%] [G loss: 1.168533]\n",
      "epoch:0 step:626 [D loss: 0.629213, acc.: 67.19%] [G loss: 1.147602]\n",
      "epoch:0 step:627 [D loss: 0.672847, acc.: 59.38%] [G loss: 1.082524]\n",
      "epoch:0 step:628 [D loss: 0.641420, acc.: 63.28%] [G loss: 1.111266]\n",
      "epoch:0 step:629 [D loss: 0.653882, acc.: 67.97%] [G loss: 1.198857]\n",
      "epoch:0 step:630 [D loss: 0.634238, acc.: 63.28%] [G loss: 1.117593]\n",
      "epoch:0 step:631 [D loss: 0.635380, acc.: 63.28%] [G loss: 1.106901]\n",
      "epoch:0 step:632 [D loss: 0.645715, acc.: 62.50%] [G loss: 1.194571]\n",
      "epoch:0 step:633 [D loss: 0.654407, acc.: 61.72%] [G loss: 1.118690]\n",
      "epoch:0 step:634 [D loss: 0.671845, acc.: 57.03%] [G loss: 1.153960]\n",
      "epoch:0 step:635 [D loss: 0.658480, acc.: 57.03%] [G loss: 1.240544]\n",
      "epoch:0 step:636 [D loss: 0.710531, acc.: 58.59%] [G loss: 1.249137]\n",
      "epoch:0 step:637 [D loss: 0.685498, acc.: 52.34%] [G loss: 1.251197]\n",
      "epoch:0 step:638 [D loss: 0.693739, acc.: 52.34%] [G loss: 1.281920]\n",
      "epoch:0 step:639 [D loss: 0.719697, acc.: 53.91%] [G loss: 1.153263]\n",
      "epoch:0 step:640 [D loss: 0.679373, acc.: 55.47%] [G loss: 1.090813]\n",
      "epoch:0 step:641 [D loss: 0.688620, acc.: 55.47%] [G loss: 1.049687]\n",
      "epoch:0 step:642 [D loss: 0.683301, acc.: 55.47%] [G loss: 1.090702]\n",
      "epoch:0 step:643 [D loss: 0.702165, acc.: 49.22%] [G loss: 1.182042]\n",
      "epoch:0 step:644 [D loss: 0.644532, acc.: 66.41%] [G loss: 1.172035]\n",
      "epoch:0 step:645 [D loss: 0.677894, acc.: 57.81%] [G loss: 1.117585]\n",
      "epoch:0 step:646 [D loss: 0.649088, acc.: 53.91%] [G loss: 1.189931]\n",
      "epoch:0 step:647 [D loss: 0.681821, acc.: 55.47%] [G loss: 1.145114]\n",
      "epoch:0 step:648 [D loss: 0.628477, acc.: 63.28%] [G loss: 1.249897]\n",
      "epoch:0 step:649 [D loss: 0.661430, acc.: 60.16%] [G loss: 1.181915]\n",
      "epoch:0 step:650 [D loss: 0.681439, acc.: 55.47%] [G loss: 1.218180]\n",
      "epoch:0 step:651 [D loss: 0.671705, acc.: 55.47%] [G loss: 1.141655]\n",
      "epoch:0 step:652 [D loss: 0.648937, acc.: 60.16%] [G loss: 1.244711]\n",
      "epoch:0 step:653 [D loss: 0.644340, acc.: 59.38%] [G loss: 1.231845]\n",
      "epoch:0 step:654 [D loss: 0.680128, acc.: 54.69%] [G loss: 1.256858]\n",
      "epoch:0 step:655 [D loss: 0.688372, acc.: 60.94%] [G loss: 1.225204]\n",
      "epoch:0 step:656 [D loss: 0.702451, acc.: 57.03%] [G loss: 1.101581]\n",
      "epoch:0 step:657 [D loss: 0.662016, acc.: 58.59%] [G loss: 1.069295]\n",
      "epoch:0 step:658 [D loss: 0.630831, acc.: 60.94%] [G loss: 1.096097]\n",
      "epoch:0 step:659 [D loss: 0.712023, acc.: 51.56%] [G loss: 1.345506]\n",
      "epoch:0 step:660 [D loss: 0.707105, acc.: 55.47%] [G loss: 1.199634]\n",
      "epoch:0 step:661 [D loss: 0.739830, acc.: 52.34%] [G loss: 1.277931]\n",
      "epoch:0 step:662 [D loss: 0.628270, acc.: 62.50%] [G loss: 1.249198]\n",
      "epoch:0 step:663 [D loss: 0.658621, acc.: 60.94%] [G loss: 1.212578]\n",
      "epoch:0 step:664 [D loss: 0.669142, acc.: 56.25%] [G loss: 1.119060]\n",
      "epoch:0 step:665 [D loss: 0.635315, acc.: 69.53%] [G loss: 1.189131]\n",
      "epoch:0 step:666 [D loss: 0.664023, acc.: 60.94%] [G loss: 1.163837]\n",
      "epoch:0 step:667 [D loss: 0.637157, acc.: 63.28%] [G loss: 1.150822]\n",
      "epoch:0 step:668 [D loss: 0.696308, acc.: 53.91%] [G loss: 1.139117]\n",
      "epoch:0 step:669 [D loss: 0.634831, acc.: 67.19%] [G loss: 1.194453]\n",
      "epoch:0 step:670 [D loss: 0.696953, acc.: 57.81%] [G loss: 1.129355]\n",
      "epoch:0 step:671 [D loss: 0.653176, acc.: 64.06%] [G loss: 1.215426]\n",
      "epoch:0 step:672 [D loss: 0.687380, acc.: 53.91%] [G loss: 1.176955]\n",
      "epoch:0 step:673 [D loss: 0.711844, acc.: 50.00%] [G loss: 1.152590]\n",
      "epoch:0 step:674 [D loss: 0.672481, acc.: 56.25%] [G loss: 1.167107]\n",
      "epoch:0 step:675 [D loss: 0.690790, acc.: 53.12%] [G loss: 1.071426]\n",
      "epoch:0 step:676 [D loss: 0.652393, acc.: 64.06%] [G loss: 1.106506]\n",
      "epoch:0 step:677 [D loss: 0.678648, acc.: 59.38%] [G loss: 1.154266]\n",
      "epoch:0 step:678 [D loss: 0.643119, acc.: 66.41%] [G loss: 1.317585]\n",
      "epoch:0 step:679 [D loss: 0.704306, acc.: 58.59%] [G loss: 1.162583]\n",
      "epoch:0 step:680 [D loss: 0.660776, acc.: 60.94%] [G loss: 1.189073]\n",
      "epoch:0 step:681 [D loss: 0.713468, acc.: 55.47%] [G loss: 1.178479]\n",
      "epoch:0 step:682 [D loss: 0.780614, acc.: 41.41%] [G loss: 1.110416]\n",
      "epoch:0 step:683 [D loss: 0.640257, acc.: 64.06%] [G loss: 1.282969]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0 step:684 [D loss: 0.696364, acc.: 58.59%] [G loss: 1.077111]\n",
      "epoch:0 step:685 [D loss: 0.653048, acc.: 64.06%] [G loss: 1.126024]\n",
      "epoch:0 step:686 [D loss: 0.671550, acc.: 57.03%] [G loss: 1.144972]\n",
      "epoch:0 step:687 [D loss: 0.684648, acc.: 55.47%] [G loss: 1.188668]\n",
      "epoch:0 step:688 [D loss: 0.643223, acc.: 62.50%] [G loss: 1.275224]\n",
      "epoch:0 step:689 [D loss: 0.714292, acc.: 57.03%] [G loss: 1.153435]\n",
      "epoch:0 step:690 [D loss: 0.646721, acc.: 61.72%] [G loss: 1.124996]\n",
      "epoch:0 step:691 [D loss: 0.650020, acc.: 66.41%] [G loss: 1.197060]\n",
      "epoch:0 step:692 [D loss: 0.746060, acc.: 53.12%] [G loss: 1.092999]\n",
      "epoch:0 step:693 [D loss: 0.654577, acc.: 62.50%] [G loss: 1.041999]\n",
      "epoch:0 step:694 [D loss: 0.655810, acc.: 55.47%] [G loss: 1.052165]\n",
      "epoch:0 step:695 [D loss: 0.650738, acc.: 66.41%] [G loss: 1.156926]\n",
      "epoch:0 step:696 [D loss: 0.632340, acc.: 66.41%] [G loss: 1.148681]\n",
      "epoch:0 step:697 [D loss: 0.645967, acc.: 63.28%] [G loss: 1.132703]\n",
      "epoch:0 step:698 [D loss: 0.670751, acc.: 58.59%] [G loss: 1.143483]\n",
      "epoch:0 step:699 [D loss: 0.668752, acc.: 57.81%] [G loss: 1.128909]\n",
      "epoch:0 step:700 [D loss: 0.647909, acc.: 63.28%] [G loss: 1.134056]\n",
      "epoch:0 step:701 [D loss: 0.698774, acc.: 54.69%] [G loss: 1.187102]\n",
      "epoch:0 step:702 [D loss: 0.677007, acc.: 56.25%] [G loss: 1.149467]\n",
      "epoch:0 step:703 [D loss: 0.645456, acc.: 67.97%] [G loss: 1.098913]\n",
      "epoch:0 step:704 [D loss: 0.677249, acc.: 61.72%] [G loss: 1.224593]\n",
      "epoch:0 step:705 [D loss: 0.723477, acc.: 55.47%] [G loss: 1.298688]\n",
      "epoch:0 step:706 [D loss: 0.692248, acc.: 60.16%] [G loss: 1.139687]\n",
      "epoch:0 step:707 [D loss: 0.674240, acc.: 60.16%] [G loss: 1.139023]\n",
      "epoch:0 step:708 [D loss: 0.674449, acc.: 58.59%] [G loss: 1.165859]\n",
      "epoch:0 step:709 [D loss: 0.668814, acc.: 61.72%] [G loss: 1.166704]\n",
      "epoch:0 step:710 [D loss: 0.674157, acc.: 60.94%] [G loss: 1.168352]\n",
      "epoch:0 step:711 [D loss: 0.664870, acc.: 58.59%] [G loss: 1.096516]\n",
      "epoch:0 step:712 [D loss: 0.624883, acc.: 66.41%] [G loss: 1.136555]\n",
      "epoch:0 step:713 [D loss: 0.729831, acc.: 53.12%] [G loss: 1.195906]\n",
      "epoch:0 step:714 [D loss: 0.654568, acc.: 61.72%] [G loss: 1.249197]\n",
      "epoch:0 step:715 [D loss: 0.704018, acc.: 57.03%] [G loss: 1.220989]\n",
      "epoch:0 step:716 [D loss: 0.668573, acc.: 67.19%] [G loss: 1.126875]\n",
      "epoch:0 step:717 [D loss: 0.670333, acc.: 60.16%] [G loss: 1.107570]\n",
      "epoch:0 step:718 [D loss: 0.640496, acc.: 61.72%] [G loss: 1.165736]\n",
      "epoch:0 step:719 [D loss: 0.646287, acc.: 67.97%] [G loss: 1.168136]\n",
      "epoch:0 step:720 [D loss: 0.645008, acc.: 60.94%] [G loss: 1.158291]\n",
      "epoch:0 step:721 [D loss: 0.661765, acc.: 58.59%] [G loss: 1.130938]\n",
      "epoch:0 step:722 [D loss: 0.705552, acc.: 54.69%] [G loss: 1.041315]\n",
      "epoch:0 step:723 [D loss: 0.635677, acc.: 65.62%] [G loss: 1.060035]\n",
      "epoch:0 step:724 [D loss: 0.621716, acc.: 65.62%] [G loss: 1.060403]\n",
      "epoch:0 step:725 [D loss: 0.635293, acc.: 64.06%] [G loss: 1.095881]\n",
      "epoch:0 step:726 [D loss: 0.619854, acc.: 66.41%] [G loss: 1.146507]\n",
      "epoch:0 step:727 [D loss: 0.651870, acc.: 60.94%] [G loss: 1.186226]\n",
      "epoch:0 step:728 [D loss: 0.649093, acc.: 62.50%] [G loss: 1.075624]\n",
      "epoch:0 step:729 [D loss: 0.637544, acc.: 63.28%] [G loss: 1.158672]\n",
      "epoch:0 step:730 [D loss: 0.727449, acc.: 53.91%] [G loss: 1.117142]\n",
      "epoch:0 step:731 [D loss: 0.632350, acc.: 68.75%] [G loss: 1.192773]\n",
      "epoch:0 step:732 [D loss: 0.682081, acc.: 58.59%] [G loss: 1.084798]\n",
      "epoch:0 step:733 [D loss: 0.619801, acc.: 64.84%] [G loss: 1.125376]\n",
      "epoch:0 step:734 [D loss: 0.674181, acc.: 53.12%] [G loss: 1.126867]\n",
      "epoch:0 step:735 [D loss: 0.646884, acc.: 67.97%] [G loss: 1.116253]\n",
      "epoch:0 step:736 [D loss: 0.691730, acc.: 57.03%] [G loss: 1.063855]\n",
      "epoch:0 step:737 [D loss: 0.656181, acc.: 60.16%] [G loss: 1.015329]\n",
      "epoch:0 step:738 [D loss: 0.647953, acc.: 65.62%] [G loss: 1.039892]\n",
      "epoch:0 step:739 [D loss: 0.623806, acc.: 67.19%] [G loss: 1.115326]\n",
      "epoch:0 step:740 [D loss: 0.618020, acc.: 67.19%] [G loss: 1.177025]\n",
      "epoch:0 step:741 [D loss: 0.640247, acc.: 64.84%] [G loss: 1.195578]\n",
      "epoch:0 step:742 [D loss: 0.600720, acc.: 68.75%] [G loss: 1.177265]\n",
      "epoch:0 step:743 [D loss: 0.637025, acc.: 67.19%] [G loss: 1.148523]\n",
      "epoch:0 step:744 [D loss: 0.643986, acc.: 64.06%] [G loss: 1.127595]\n",
      "epoch:0 step:745 [D loss: 0.665157, acc.: 57.03%] [G loss: 1.118438]\n",
      "epoch:0 step:746 [D loss: 0.642197, acc.: 64.84%] [G loss: 1.072827]\n",
      "epoch:0 step:747 [D loss: 0.637757, acc.: 63.28%] [G loss: 1.078020]\n",
      "epoch:0 step:748 [D loss: 0.620936, acc.: 62.50%] [G loss: 1.035801]\n",
      "epoch:0 step:749 [D loss: 0.647401, acc.: 65.62%] [G loss: 1.203734]\n",
      "epoch:0 step:750 [D loss: 0.610890, acc.: 73.44%] [G loss: 1.204950]\n",
      "epoch:0 step:751 [D loss: 0.699121, acc.: 60.16%] [G loss: 1.177481]\n",
      "epoch:0 step:752 [D loss: 0.674296, acc.: 63.28%] [G loss: 1.147588]\n",
      "epoch:0 step:753 [D loss: 0.684689, acc.: 53.12%] [G loss: 1.131754]\n",
      "epoch:0 step:754 [D loss: 0.639849, acc.: 64.84%] [G loss: 1.124020]\n",
      "epoch:0 step:755 [D loss: 0.659662, acc.: 62.50%] [G loss: 1.115111]\n",
      "epoch:0 step:756 [D loss: 0.655681, acc.: 61.72%] [G loss: 1.160629]\n",
      "epoch:0 step:757 [D loss: 0.647029, acc.: 64.84%] [G loss: 1.135465]\n",
      "epoch:0 step:758 [D loss: 0.658185, acc.: 60.16%] [G loss: 1.135177]\n",
      "epoch:0 step:759 [D loss: 0.637477, acc.: 64.06%] [G loss: 1.091606]\n",
      "epoch:0 step:760 [D loss: 0.711932, acc.: 62.50%] [G loss: 1.119916]\n",
      "epoch:0 step:761 [D loss: 0.637766, acc.: 64.84%] [G loss: 1.042315]\n",
      "epoch:0 step:762 [D loss: 0.664539, acc.: 60.16%] [G loss: 1.050634]\n",
      "epoch:0 step:763 [D loss: 0.624357, acc.: 64.84%] [G loss: 1.057283]\n",
      "epoch:0 step:764 [D loss: 0.614062, acc.: 67.97%] [G loss: 1.126432]\n",
      "epoch:0 step:765 [D loss: 0.664553, acc.: 64.06%] [G loss: 1.137302]\n",
      "epoch:0 step:766 [D loss: 0.619492, acc.: 67.19%] [G loss: 1.192728]\n",
      "epoch:0 step:767 [D loss: 0.676035, acc.: 57.81%] [G loss: 1.173268]\n",
      "epoch:0 step:768 [D loss: 0.635895, acc.: 64.84%] [G loss: 1.172822]\n",
      "epoch:0 step:769 [D loss: 0.658597, acc.: 62.50%] [G loss: 1.146614]\n",
      "epoch:0 step:770 [D loss: 0.639797, acc.: 64.84%] [G loss: 1.089175]\n",
      "epoch:0 step:771 [D loss: 0.669413, acc.: 60.16%] [G loss: 1.174602]\n",
      "epoch:0 step:772 [D loss: 0.649437, acc.: 62.50%] [G loss: 1.096691]\n",
      "epoch:0 step:773 [D loss: 0.655044, acc.: 57.03%] [G loss: 1.083355]\n",
      "epoch:0 step:774 [D loss: 0.635898, acc.: 68.75%] [G loss: 1.084965]\n",
      "epoch:0 step:775 [D loss: 0.638823, acc.: 64.84%] [G loss: 1.084155]\n",
      "epoch:0 step:776 [D loss: 0.621357, acc.: 67.19%] [G loss: 1.072086]\n",
      "epoch:0 step:777 [D loss: 0.627408, acc.: 64.84%] [G loss: 1.164447]\n",
      "epoch:0 step:778 [D loss: 0.683747, acc.: 57.03%] [G loss: 1.189704]\n",
      "epoch:0 step:779 [D loss: 0.677115, acc.: 55.47%] [G loss: 1.185694]\n",
      "epoch:0 step:780 [D loss: 0.652759, acc.: 64.06%] [G loss: 1.176473]\n",
      "epoch:0 step:781 [D loss: 0.640389, acc.: 63.28%] [G loss: 1.154678]\n",
      "epoch:1 step:782 [D loss: 0.692096, acc.: 56.25%] [G loss: 1.090180]\n",
      "epoch:1 step:783 [D loss: 0.662980, acc.: 62.50%] [G loss: 1.062219]\n",
      "epoch:1 step:784 [D loss: 0.630844, acc.: 65.62%] [G loss: 1.091785]\n",
      "epoch:1 step:785 [D loss: 0.630099, acc.: 64.84%] [G loss: 1.131688]\n",
      "epoch:1 step:786 [D loss: 0.650673, acc.: 65.62%] [G loss: 1.103241]\n",
      "epoch:1 step:787 [D loss: 0.672523, acc.: 56.25%] [G loss: 1.168026]\n",
      "epoch:1 step:788 [D loss: 0.676973, acc.: 62.50%] [G loss: 1.088134]\n",
      "epoch:1 step:789 [D loss: 0.682582, acc.: 58.59%] [G loss: 1.111280]\n",
      "epoch:1 step:790 [D loss: 0.657799, acc.: 59.38%] [G loss: 1.136871]\n",
      "epoch:1 step:791 [D loss: 0.714835, acc.: 47.66%] [G loss: 1.098585]\n",
      "epoch:1 step:792 [D loss: 0.641097, acc.: 65.62%] [G loss: 1.152929]\n",
      "epoch:1 step:793 [D loss: 0.617694, acc.: 63.28%] [G loss: 1.175776]\n",
      "epoch:1 step:794 [D loss: 0.740790, acc.: 53.91%] [G loss: 1.114113]\n",
      "epoch:1 step:795 [D loss: 0.706171, acc.: 55.47%] [G loss: 1.027127]\n",
      "epoch:1 step:796 [D loss: 0.647782, acc.: 61.72%] [G loss: 1.056009]\n",
      "epoch:1 step:797 [D loss: 0.634274, acc.: 65.62%] [G loss: 1.114279]\n",
      "epoch:1 step:798 [D loss: 0.636728, acc.: 64.06%] [G loss: 1.106579]\n",
      "epoch:1 step:799 [D loss: 0.634966, acc.: 64.84%] [G loss: 1.034634]\n",
      "epoch:1 step:800 [D loss: 0.631951, acc.: 70.31%] [G loss: 1.120672]\n",
      "##############\n",
      "[0.81060413 0.79613656 0.79967274 0.80989338 0.79289924 0.78408381\n",
      " 0.89214628 0.82033952 0.83508269 0.83075201]\n",
      "##########\n",
      "epoch:1 step:801 [D loss: 0.631092, acc.: 70.31%] [G loss: 1.089116]\n",
      "epoch:1 step:802 [D loss: 0.627083, acc.: 68.75%] [G loss: 1.158473]\n",
      "epoch:1 step:803 [D loss: 0.648325, acc.: 64.84%] [G loss: 1.154535]\n",
      "epoch:1 step:804 [D loss: 0.640982, acc.: 65.62%] [G loss: 1.095469]\n",
      "epoch:1 step:805 [D loss: 0.672114, acc.: 57.03%] [G loss: 1.234523]\n",
      "epoch:1 step:806 [D loss: 0.674097, acc.: 58.59%] [G loss: 1.191808]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:1 step:807 [D loss: 0.627973, acc.: 64.84%] [G loss: 1.171817]\n",
      "epoch:1 step:808 [D loss: 0.641129, acc.: 60.94%] [G loss: 1.082622]\n",
      "epoch:1 step:809 [D loss: 0.635431, acc.: 60.16%] [G loss: 1.168102]\n",
      "epoch:1 step:810 [D loss: 0.662984, acc.: 58.59%] [G loss: 1.067634]\n",
      "epoch:1 step:811 [D loss: 0.693282, acc.: 57.81%] [G loss: 1.105356]\n",
      "epoch:1 step:812 [D loss: 0.601614, acc.: 67.19%] [G loss: 1.108616]\n",
      "epoch:1 step:813 [D loss: 0.620856, acc.: 67.19%] [G loss: 1.078576]\n",
      "epoch:1 step:814 [D loss: 0.672658, acc.: 58.59%] [G loss: 1.121186]\n",
      "epoch:1 step:815 [D loss: 0.614049, acc.: 72.66%] [G loss: 1.115443]\n",
      "epoch:1 step:816 [D loss: 0.672803, acc.: 64.06%] [G loss: 1.084836]\n",
      "epoch:1 step:817 [D loss: 0.636067, acc.: 64.84%] [G loss: 1.174675]\n",
      "epoch:1 step:818 [D loss: 0.701943, acc.: 58.59%] [G loss: 1.043729]\n",
      "epoch:1 step:819 [D loss: 0.624397, acc.: 68.75%] [G loss: 1.129200]\n",
      "epoch:1 step:820 [D loss: 0.637321, acc.: 64.84%] [G loss: 1.183187]\n",
      "epoch:1 step:821 [D loss: 0.657809, acc.: 60.94%] [G loss: 1.077064]\n",
      "epoch:1 step:822 [D loss: 0.664057, acc.: 62.50%] [G loss: 1.108697]\n",
      "epoch:1 step:823 [D loss: 0.616093, acc.: 66.41%] [G loss: 1.078125]\n",
      "epoch:1 step:824 [D loss: 0.673307, acc.: 57.81%] [G loss: 1.099913]\n",
      "epoch:1 step:825 [D loss: 0.657592, acc.: 63.28%] [G loss: 1.059276]\n",
      "epoch:1 step:826 [D loss: 0.591790, acc.: 73.44%] [G loss: 1.122576]\n",
      "epoch:1 step:827 [D loss: 0.606154, acc.: 71.09%] [G loss: 1.034111]\n",
      "epoch:1 step:828 [D loss: 0.593552, acc.: 70.31%] [G loss: 1.064115]\n",
      "epoch:1 step:829 [D loss: 0.603577, acc.: 67.19%] [G loss: 1.112841]\n",
      "epoch:1 step:830 [D loss: 0.646244, acc.: 63.28%] [G loss: 1.301739]\n",
      "epoch:1 step:831 [D loss: 0.792742, acc.: 53.91%] [G loss: 1.122030]\n",
      "epoch:1 step:832 [D loss: 0.696008, acc.: 60.16%] [G loss: 1.164199]\n",
      "epoch:1 step:833 [D loss: 0.697941, acc.: 59.38%] [G loss: 1.043429]\n",
      "epoch:1 step:834 [D loss: 0.667369, acc.: 56.25%] [G loss: 1.043330]\n",
      "epoch:1 step:835 [D loss: 0.621697, acc.: 64.84%] [G loss: 1.065777]\n",
      "epoch:1 step:836 [D loss: 0.638434, acc.: 61.72%] [G loss: 1.032362]\n",
      "epoch:1 step:837 [D loss: 0.699355, acc.: 52.34%] [G loss: 0.995569]\n",
      "epoch:1 step:838 [D loss: 0.660934, acc.: 60.94%] [G loss: 1.113553]\n",
      "epoch:1 step:839 [D loss: 0.661656, acc.: 57.81%] [G loss: 1.064987]\n",
      "epoch:1 step:840 [D loss: 0.633762, acc.: 64.06%] [G loss: 1.083174]\n",
      "epoch:1 step:841 [D loss: 0.626470, acc.: 65.62%] [G loss: 1.072183]\n",
      "epoch:1 step:842 [D loss: 0.617260, acc.: 71.09%] [G loss: 1.157402]\n",
      "epoch:1 step:843 [D loss: 0.634946, acc.: 66.41%] [G loss: 1.057942]\n",
      "epoch:1 step:844 [D loss: 0.644433, acc.: 64.84%] [G loss: 1.123797]\n",
      "epoch:1 step:845 [D loss: 0.656306, acc.: 62.50%] [G loss: 1.115682]\n",
      "epoch:1 step:846 [D loss: 0.627040, acc.: 60.16%] [G loss: 1.137574]\n",
      "epoch:1 step:847 [D loss: 0.641315, acc.: 64.06%] [G loss: 1.139951]\n",
      "epoch:1 step:848 [D loss: 0.667262, acc.: 57.81%] [G loss: 1.198105]\n",
      "epoch:1 step:849 [D loss: 0.655746, acc.: 59.38%] [G loss: 1.170022]\n",
      "epoch:1 step:850 [D loss: 0.732261, acc.: 51.56%] [G loss: 1.056920]\n",
      "epoch:1 step:851 [D loss: 0.591838, acc.: 74.22%] [G loss: 1.168540]\n",
      "epoch:1 step:852 [D loss: 0.736166, acc.: 50.78%] [G loss: 1.092368]\n",
      "epoch:1 step:853 [D loss: 0.688238, acc.: 59.38%] [G loss: 1.137578]\n",
      "epoch:1 step:854 [D loss: 0.721470, acc.: 50.78%] [G loss: 1.052155]\n",
      "epoch:1 step:855 [D loss: 0.642029, acc.: 58.59%] [G loss: 1.068625]\n",
      "epoch:1 step:856 [D loss: 0.663771, acc.: 57.03%] [G loss: 1.087220]\n",
      "epoch:1 step:857 [D loss: 0.683160, acc.: 51.56%] [G loss: 1.091191]\n",
      "epoch:1 step:858 [D loss: 0.633062, acc.: 67.97%] [G loss: 1.060513]\n",
      "epoch:1 step:859 [D loss: 0.640414, acc.: 64.06%] [G loss: 1.016651]\n",
      "epoch:1 step:860 [D loss: 0.678008, acc.: 59.38%] [G loss: 1.112395]\n",
      "epoch:1 step:861 [D loss: 0.641739, acc.: 57.03%] [G loss: 1.178669]\n",
      "epoch:1 step:862 [D loss: 0.697370, acc.: 55.47%] [G loss: 1.140908]\n",
      "epoch:1 step:863 [D loss: 0.633779, acc.: 65.62%] [G loss: 1.154268]\n",
      "epoch:1 step:864 [D loss: 0.656395, acc.: 67.97%] [G loss: 1.131938]\n",
      "epoch:1 step:865 [D loss: 0.720687, acc.: 52.34%] [G loss: 1.200943]\n",
      "epoch:1 step:866 [D loss: 0.668763, acc.: 53.91%] [G loss: 1.220155]\n",
      "epoch:1 step:867 [D loss: 0.695288, acc.: 51.56%] [G loss: 1.125296]\n",
      "epoch:1 step:868 [D loss: 0.611793, acc.: 67.97%] [G loss: 1.100286]\n",
      "epoch:1 step:869 [D loss: 0.616901, acc.: 66.41%] [G loss: 1.178948]\n",
      "epoch:1 step:870 [D loss: 0.669248, acc.: 59.38%] [G loss: 1.048210]\n",
      "epoch:1 step:871 [D loss: 0.653117, acc.: 65.62%] [G loss: 1.113404]\n",
      "epoch:1 step:872 [D loss: 0.636115, acc.: 67.97%] [G loss: 1.203554]\n",
      "epoch:1 step:873 [D loss: 0.690983, acc.: 62.50%] [G loss: 1.065194]\n",
      "epoch:1 step:874 [D loss: 0.668191, acc.: 55.47%] [G loss: 1.203406]\n",
      "epoch:1 step:875 [D loss: 0.627544, acc.: 60.16%] [G loss: 1.125216]\n",
      "epoch:1 step:876 [D loss: 0.704068, acc.: 56.25%] [G loss: 1.193950]\n",
      "epoch:1 step:877 [D loss: 0.655912, acc.: 61.72%] [G loss: 1.224077]\n",
      "epoch:1 step:878 [D loss: 0.693768, acc.: 55.47%] [G loss: 1.117547]\n",
      "epoch:1 step:879 [D loss: 0.692290, acc.: 57.03%] [G loss: 1.108287]\n",
      "epoch:1 step:880 [D loss: 0.630483, acc.: 68.75%] [G loss: 1.117635]\n",
      "epoch:1 step:881 [D loss: 0.693099, acc.: 55.47%] [G loss: 1.126192]\n",
      "epoch:1 step:882 [D loss: 0.670572, acc.: 58.59%] [G loss: 1.030061]\n",
      "epoch:1 step:883 [D loss: 0.661511, acc.: 64.84%] [G loss: 1.096640]\n",
      "epoch:1 step:884 [D loss: 0.680677, acc.: 59.38%] [G loss: 1.102912]\n",
      "epoch:1 step:885 [D loss: 0.670506, acc.: 61.72%] [G loss: 1.145146]\n",
      "epoch:1 step:886 [D loss: 0.685549, acc.: 56.25%] [G loss: 1.002105]\n",
      "epoch:1 step:887 [D loss: 0.675926, acc.: 58.59%] [G loss: 1.125931]\n",
      "epoch:1 step:888 [D loss: 0.601364, acc.: 74.22%] [G loss: 1.158071]\n",
      "epoch:1 step:889 [D loss: 0.688037, acc.: 57.03%] [G loss: 1.168266]\n",
      "epoch:1 step:890 [D loss: 0.727805, acc.: 51.56%] [G loss: 1.159654]\n",
      "epoch:1 step:891 [D loss: 0.662956, acc.: 58.59%] [G loss: 1.141459]\n",
      "epoch:1 step:892 [D loss: 0.771541, acc.: 42.97%] [G loss: 1.150296]\n",
      "epoch:1 step:893 [D loss: 0.694506, acc.: 65.62%] [G loss: 1.261590]\n",
      "epoch:1 step:894 [D loss: 0.717489, acc.: 56.25%] [G loss: 1.200880]\n",
      "epoch:1 step:895 [D loss: 0.769580, acc.: 46.09%] [G loss: 1.029559]\n",
      "epoch:1 step:896 [D loss: 0.677812, acc.: 58.59%] [G loss: 1.028593]\n",
      "epoch:1 step:897 [D loss: 0.656640, acc.: 61.72%] [G loss: 1.111134]\n",
      "epoch:1 step:898 [D loss: 0.739906, acc.: 56.25%] [G loss: 1.133171]\n",
      "epoch:1 step:899 [D loss: 0.669236, acc.: 59.38%] [G loss: 1.061852]\n",
      "epoch:1 step:900 [D loss: 0.643857, acc.: 69.53%] [G loss: 1.123621]\n",
      "epoch:1 step:901 [D loss: 0.671701, acc.: 60.94%] [G loss: 1.120883]\n",
      "epoch:1 step:902 [D loss: 0.657883, acc.: 57.81%] [G loss: 1.057799]\n",
      "epoch:1 step:903 [D loss: 0.639336, acc.: 64.06%] [G loss: 1.011647]\n",
      "epoch:1 step:904 [D loss: 0.648596, acc.: 60.94%] [G loss: 1.070016]\n",
      "epoch:1 step:905 [D loss: 0.685023, acc.: 67.19%] [G loss: 1.171908]\n",
      "epoch:1 step:906 [D loss: 0.673693, acc.: 60.94%] [G loss: 1.173760]\n",
      "epoch:1 step:907 [D loss: 0.698910, acc.: 57.03%] [G loss: 1.123050]\n",
      "epoch:1 step:908 [D loss: 0.684707, acc.: 53.91%] [G loss: 1.092186]\n",
      "epoch:1 step:909 [D loss: 0.671578, acc.: 56.25%] [G loss: 1.136592]\n",
      "epoch:1 step:910 [D loss: 0.682194, acc.: 56.25%] [G loss: 1.146952]\n",
      "epoch:1 step:911 [D loss: 0.668771, acc.: 61.72%] [G loss: 1.175338]\n",
      "epoch:1 step:912 [D loss: 0.726186, acc.: 58.59%] [G loss: 0.998408]\n",
      "epoch:1 step:913 [D loss: 0.673140, acc.: 65.62%] [G loss: 1.018541]\n",
      "epoch:1 step:914 [D loss: 0.667297, acc.: 62.50%] [G loss: 1.028112]\n",
      "epoch:1 step:915 [D loss: 0.642693, acc.: 68.75%] [G loss: 1.066617]\n",
      "epoch:1 step:916 [D loss: 0.659461, acc.: 67.19%] [G loss: 1.145498]\n",
      "epoch:1 step:917 [D loss: 0.686707, acc.: 58.59%] [G loss: 1.053783]\n",
      "epoch:1 step:918 [D loss: 0.668307, acc.: 57.81%] [G loss: 1.062905]\n",
      "epoch:1 step:919 [D loss: 0.631528, acc.: 70.31%] [G loss: 1.088958]\n",
      "epoch:1 step:920 [D loss: 0.651099, acc.: 67.19%] [G loss: 1.045002]\n",
      "epoch:1 step:921 [D loss: 0.658696, acc.: 61.72%] [G loss: 1.041299]\n",
      "epoch:1 step:922 [D loss: 0.669925, acc.: 59.38%] [G loss: 1.077590]\n",
      "epoch:1 step:923 [D loss: 0.627958, acc.: 64.84%] [G loss: 1.095128]\n",
      "epoch:1 step:924 [D loss: 0.635845, acc.: 64.84%] [G loss: 1.108163]\n",
      "epoch:1 step:925 [D loss: 0.665574, acc.: 64.84%] [G loss: 1.199028]\n",
      "epoch:1 step:926 [D loss: 0.648186, acc.: 64.06%] [G loss: 1.061287]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:1 step:927 [D loss: 0.638033, acc.: 67.97%] [G loss: 1.083657]\n",
      "epoch:1 step:928 [D loss: 0.621490, acc.: 67.97%] [G loss: 1.105492]\n",
      "epoch:1 step:929 [D loss: 0.665531, acc.: 59.38%] [G loss: 1.058959]\n",
      "epoch:1 step:930 [D loss: 0.667441, acc.: 60.16%] [G loss: 1.052799]\n",
      "epoch:1 step:931 [D loss: 0.632839, acc.: 67.19%] [G loss: 1.114568]\n",
      "epoch:1 step:932 [D loss: 0.623557, acc.: 72.66%] [G loss: 1.105520]\n",
      "epoch:1 step:933 [D loss: 0.655259, acc.: 63.28%] [G loss: 1.085721]\n",
      "epoch:1 step:934 [D loss: 0.645079, acc.: 57.81%] [G loss: 1.122947]\n",
      "epoch:1 step:935 [D loss: 0.679998, acc.: 63.28%] [G loss: 1.124514]\n",
      "epoch:1 step:936 [D loss: 0.639645, acc.: 58.59%] [G loss: 1.082998]\n",
      "epoch:1 step:937 [D loss: 0.689005, acc.: 56.25%] [G loss: 1.094490]\n",
      "epoch:1 step:938 [D loss: 0.682832, acc.: 53.12%] [G loss: 1.099513]\n",
      "epoch:1 step:939 [D loss: 0.646862, acc.: 58.59%] [G loss: 1.079770]\n",
      "epoch:1 step:940 [D loss: 0.704073, acc.: 57.81%] [G loss: 1.118765]\n",
      "epoch:1 step:941 [D loss: 0.761290, acc.: 52.34%] [G loss: 1.149348]\n",
      "epoch:1 step:942 [D loss: 0.690583, acc.: 62.50%] [G loss: 1.032590]\n",
      "epoch:1 step:943 [D loss: 0.658585, acc.: 66.41%] [G loss: 1.080036]\n",
      "epoch:1 step:944 [D loss: 0.633607, acc.: 65.62%] [G loss: 1.143538]\n",
      "epoch:1 step:945 [D loss: 0.702293, acc.: 53.91%] [G loss: 1.024769]\n",
      "epoch:1 step:946 [D loss: 0.643967, acc.: 70.31%] [G loss: 1.084127]\n",
      "epoch:1 step:947 [D loss: 0.682732, acc.: 53.91%] [G loss: 0.986126]\n",
      "epoch:1 step:948 [D loss: 0.659165, acc.: 63.28%] [G loss: 0.971683]\n",
      "epoch:1 step:949 [D loss: 0.596411, acc.: 74.22%] [G loss: 1.078638]\n",
      "epoch:1 step:950 [D loss: 0.689861, acc.: 54.69%] [G loss: 1.020276]\n",
      "epoch:1 step:951 [D loss: 0.625483, acc.: 70.31%] [G loss: 1.007548]\n",
      "epoch:1 step:952 [D loss: 0.650703, acc.: 68.75%] [G loss: 1.106486]\n",
      "epoch:1 step:953 [D loss: 0.619894, acc.: 71.88%] [G loss: 1.130089]\n",
      "epoch:1 step:954 [D loss: 0.618325, acc.: 67.97%] [G loss: 1.236338]\n",
      "epoch:1 step:955 [D loss: 0.666690, acc.: 60.94%] [G loss: 1.179510]\n",
      "epoch:1 step:956 [D loss: 0.650058, acc.: 60.16%] [G loss: 1.154918]\n",
      "epoch:1 step:957 [D loss: 0.629485, acc.: 66.41%] [G loss: 1.200245]\n",
      "epoch:1 step:958 [D loss: 0.685981, acc.: 53.12%] [G loss: 1.099938]\n",
      "epoch:1 step:959 [D loss: 0.640763, acc.: 64.06%] [G loss: 1.182536]\n",
      "epoch:1 step:960 [D loss: 0.620272, acc.: 61.72%] [G loss: 1.148640]\n",
      "epoch:1 step:961 [D loss: 0.629957, acc.: 67.19%] [G loss: 1.145864]\n",
      "epoch:1 step:962 [D loss: 0.651195, acc.: 59.38%] [G loss: 1.070970]\n",
      "epoch:1 step:963 [D loss: 0.651480, acc.: 63.28%] [G loss: 1.000308]\n",
      "epoch:1 step:964 [D loss: 0.626967, acc.: 63.28%] [G loss: 1.016531]\n",
      "epoch:1 step:965 [D loss: 0.646073, acc.: 59.38%] [G loss: 1.044136]\n",
      "epoch:1 step:966 [D loss: 0.676173, acc.: 67.19%] [G loss: 1.098291]\n",
      "epoch:1 step:967 [D loss: 0.663293, acc.: 59.38%] [G loss: 1.018647]\n",
      "epoch:1 step:968 [D loss: 0.680835, acc.: 57.81%] [G loss: 1.127456]\n",
      "epoch:1 step:969 [D loss: 0.630290, acc.: 74.22%] [G loss: 1.094076]\n",
      "epoch:1 step:970 [D loss: 0.665841, acc.: 55.47%] [G loss: 0.992911]\n",
      "epoch:1 step:971 [D loss: 0.674469, acc.: 59.38%] [G loss: 1.033714]\n",
      "epoch:1 step:972 [D loss: 0.642420, acc.: 66.41%] [G loss: 1.157329]\n",
      "epoch:1 step:973 [D loss: 0.679547, acc.: 60.94%] [G loss: 1.065164]\n",
      "epoch:1 step:974 [D loss: 0.683200, acc.: 58.59%] [G loss: 1.161552]\n",
      "epoch:1 step:975 [D loss: 0.687403, acc.: 58.59%] [G loss: 1.147562]\n",
      "epoch:1 step:976 [D loss: 0.672050, acc.: 57.81%] [G loss: 1.102107]\n",
      "epoch:1 step:977 [D loss: 0.599900, acc.: 70.31%] [G loss: 1.061732]\n",
      "epoch:1 step:978 [D loss: 0.677275, acc.: 57.03%] [G loss: 1.095543]\n",
      "epoch:1 step:979 [D loss: 0.640409, acc.: 60.16%] [G loss: 1.062969]\n",
      "epoch:1 step:980 [D loss: 0.634671, acc.: 64.06%] [G loss: 1.057495]\n",
      "epoch:1 step:981 [D loss: 0.654592, acc.: 64.06%] [G loss: 1.116302]\n",
      "epoch:1 step:982 [D loss: 0.687871, acc.: 60.94%] [G loss: 1.065155]\n",
      "epoch:1 step:983 [D loss: 0.644437, acc.: 64.06%] [G loss: 1.120622]\n",
      "epoch:1 step:984 [D loss: 0.648137, acc.: 60.16%] [G loss: 1.088421]\n",
      "epoch:1 step:985 [D loss: 0.699061, acc.: 54.69%] [G loss: 1.050369]\n",
      "epoch:1 step:986 [D loss: 0.633029, acc.: 67.19%] [G loss: 1.076618]\n",
      "epoch:1 step:987 [D loss: 0.689559, acc.: 50.00%] [G loss: 1.038208]\n",
      "epoch:1 step:988 [D loss: 0.679454, acc.: 58.59%] [G loss: 1.032679]\n",
      "epoch:1 step:989 [D loss: 0.680674, acc.: 55.47%] [G loss: 1.005629]\n",
      "epoch:1 step:990 [D loss: 0.638472, acc.: 65.62%] [G loss: 1.067762]\n",
      "epoch:1 step:991 [D loss: 0.619850, acc.: 66.41%] [G loss: 1.088107]\n",
      "epoch:1 step:992 [D loss: 0.668962, acc.: 66.41%] [G loss: 1.151456]\n",
      "epoch:1 step:993 [D loss: 0.666302, acc.: 54.69%] [G loss: 1.012662]\n",
      "epoch:1 step:994 [D loss: 0.688364, acc.: 56.25%] [G loss: 1.032301]\n",
      "epoch:1 step:995 [D loss: 0.617167, acc.: 67.19%] [G loss: 1.129922]\n",
      "epoch:1 step:996 [D loss: 0.684330, acc.: 57.03%] [G loss: 1.039968]\n",
      "epoch:1 step:997 [D loss: 0.661524, acc.: 67.97%] [G loss: 1.023439]\n",
      "epoch:1 step:998 [D loss: 0.620462, acc.: 69.53%] [G loss: 1.023318]\n",
      "epoch:1 step:999 [D loss: 0.674061, acc.: 53.91%] [G loss: 1.024861]\n",
      "epoch:1 step:1000 [D loss: 0.609884, acc.: 70.31%] [G loss: 1.040692]\n",
      "##############\n",
      "[0.87168402 0.83150152 0.77551517 0.84956917 0.76301303 0.82113634\n",
      " 0.87257343 0.84450979 0.79750074 0.80810842]\n",
      "##########\n",
      "epoch:1 step:1001 [D loss: 0.643121, acc.: 62.50%] [G loss: 1.045293]\n",
      "epoch:1 step:1002 [D loss: 0.641712, acc.: 67.19%] [G loss: 1.057965]\n",
      "epoch:1 step:1003 [D loss: 0.687743, acc.: 62.50%] [G loss: 1.077732]\n",
      "epoch:1 step:1004 [D loss: 0.649708, acc.: 64.84%] [G loss: 1.123615]\n",
      "epoch:1 step:1005 [D loss: 0.656552, acc.: 67.19%] [G loss: 1.047858]\n",
      "epoch:1 step:1006 [D loss: 0.634391, acc.: 64.06%] [G loss: 1.049710]\n",
      "epoch:1 step:1007 [D loss: 0.644026, acc.: 65.62%] [G loss: 0.965147]\n",
      "epoch:1 step:1008 [D loss: 0.703245, acc.: 49.22%] [G loss: 1.074243]\n",
      "epoch:1 step:1009 [D loss: 0.637044, acc.: 62.50%] [G loss: 1.025857]\n",
      "epoch:1 step:1010 [D loss: 0.643545, acc.: 61.72%] [G loss: 1.033417]\n",
      "epoch:1 step:1011 [D loss: 0.641521, acc.: 66.41%] [G loss: 1.014004]\n",
      "epoch:1 step:1012 [D loss: 0.609794, acc.: 68.75%] [G loss: 1.072398]\n",
      "epoch:1 step:1013 [D loss: 0.653497, acc.: 63.28%] [G loss: 1.095803]\n",
      "epoch:1 step:1014 [D loss: 0.620604, acc.: 68.75%] [G loss: 1.132925]\n",
      "epoch:1 step:1015 [D loss: 0.622654, acc.: 67.97%] [G loss: 1.087625]\n",
      "epoch:1 step:1016 [D loss: 0.650652, acc.: 65.62%] [G loss: 1.061159]\n",
      "epoch:1 step:1017 [D loss: 0.683488, acc.: 56.25%] [G loss: 1.014884]\n",
      "epoch:1 step:1018 [D loss: 0.639749, acc.: 60.16%] [G loss: 1.130503]\n",
      "epoch:1 step:1019 [D loss: 0.624250, acc.: 70.31%] [G loss: 1.076777]\n",
      "epoch:1 step:1020 [D loss: 0.624483, acc.: 68.75%] [G loss: 1.095246]\n",
      "epoch:1 step:1021 [D loss: 0.624779, acc.: 67.97%] [G loss: 1.100719]\n",
      "epoch:1 step:1022 [D loss: 0.661948, acc.: 61.72%] [G loss: 1.055165]\n",
      "epoch:1 step:1023 [D loss: 0.630124, acc.: 64.06%] [G loss: 1.069434]\n",
      "epoch:1 step:1024 [D loss: 0.633570, acc.: 64.06%] [G loss: 1.104500]\n",
      "epoch:1 step:1025 [D loss: 0.653339, acc.: 61.72%] [G loss: 1.106905]\n",
      "epoch:1 step:1026 [D loss: 0.621664, acc.: 68.75%] [G loss: 1.109036]\n",
      "epoch:1 step:1027 [D loss: 0.715254, acc.: 56.25%] [G loss: 1.066675]\n",
      "epoch:1 step:1028 [D loss: 0.638367, acc.: 61.72%] [G loss: 1.085492]\n",
      "epoch:1 step:1029 [D loss: 0.652530, acc.: 63.28%] [G loss: 1.159366]\n",
      "epoch:1 step:1030 [D loss: 0.689578, acc.: 54.69%] [G loss: 1.088857]\n",
      "epoch:1 step:1031 [D loss: 0.649645, acc.: 57.81%] [G loss: 1.042068]\n",
      "epoch:1 step:1032 [D loss: 0.642625, acc.: 57.81%] [G loss: 1.088296]\n",
      "epoch:1 step:1033 [D loss: 0.648524, acc.: 67.97%] [G loss: 1.080596]\n",
      "epoch:1 step:1034 [D loss: 0.593880, acc.: 71.09%] [G loss: 1.124568]\n",
      "epoch:1 step:1035 [D loss: 0.681269, acc.: 53.12%] [G loss: 1.097905]\n",
      "epoch:1 step:1036 [D loss: 0.662420, acc.: 56.25%] [G loss: 1.004945]\n",
      "epoch:1 step:1037 [D loss: 0.684496, acc.: 59.38%] [G loss: 1.039837]\n",
      "epoch:1 step:1038 [D loss: 0.613956, acc.: 72.66%] [G loss: 1.031700]\n",
      "epoch:1 step:1039 [D loss: 0.650029, acc.: 59.38%] [G loss: 0.998659]\n",
      "epoch:1 step:1040 [D loss: 0.655550, acc.: 64.84%] [G loss: 1.014232]\n",
      "epoch:1 step:1041 [D loss: 0.664148, acc.: 61.72%] [G loss: 0.990510]\n",
      "epoch:1 step:1042 [D loss: 0.642882, acc.: 59.38%] [G loss: 1.031500]\n",
      "epoch:1 step:1043 [D loss: 0.654184, acc.: 58.59%] [G loss: 1.012861]\n",
      "epoch:1 step:1044 [D loss: 0.666342, acc.: 62.50%] [G loss: 1.100537]\n",
      "epoch:1 step:1045 [D loss: 0.641241, acc.: 67.97%] [G loss: 1.011954]\n",
      "epoch:1 step:1046 [D loss: 0.616074, acc.: 64.84%] [G loss: 1.076766]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:1 step:1047 [D loss: 0.687020, acc.: 57.03%] [G loss: 1.125904]\n",
      "epoch:1 step:1048 [D loss: 0.665655, acc.: 60.94%] [G loss: 1.109527]\n",
      "epoch:1 step:1049 [D loss: 0.705447, acc.: 55.47%] [G loss: 1.140977]\n",
      "epoch:1 step:1050 [D loss: 0.693606, acc.: 59.38%] [G loss: 1.073450]\n",
      "epoch:1 step:1051 [D loss: 0.690441, acc.: 50.00%] [G loss: 1.025985]\n",
      "epoch:1 step:1052 [D loss: 0.662023, acc.: 60.16%] [G loss: 1.016870]\n",
      "epoch:1 step:1053 [D loss: 0.605553, acc.: 67.19%] [G loss: 1.075452]\n",
      "epoch:1 step:1054 [D loss: 0.707785, acc.: 51.56%] [G loss: 1.038514]\n",
      "epoch:1 step:1055 [D loss: 0.640557, acc.: 61.72%] [G loss: 1.123005]\n",
      "epoch:1 step:1056 [D loss: 0.729382, acc.: 53.91%] [G loss: 1.211716]\n",
      "epoch:1 step:1057 [D loss: 0.665243, acc.: 63.28%] [G loss: 1.122307]\n",
      "epoch:1 step:1058 [D loss: 0.733989, acc.: 46.88%] [G loss: 0.968850]\n",
      "epoch:1 step:1059 [D loss: 0.638496, acc.: 65.62%] [G loss: 1.073831]\n",
      "epoch:1 step:1060 [D loss: 0.671398, acc.: 60.16%] [G loss: 1.002350]\n",
      "epoch:1 step:1061 [D loss: 0.675918, acc.: 60.16%] [G loss: 0.966538]\n",
      "epoch:1 step:1062 [D loss: 0.656295, acc.: 62.50%] [G loss: 0.988931]\n",
      "epoch:1 step:1063 [D loss: 0.701531, acc.: 52.34%] [G loss: 0.961042]\n",
      "epoch:1 step:1064 [D loss: 0.637067, acc.: 65.62%] [G loss: 1.006739]\n",
      "epoch:1 step:1065 [D loss: 0.617803, acc.: 71.09%] [G loss: 1.152465]\n",
      "epoch:1 step:1066 [D loss: 0.740655, acc.: 59.38%] [G loss: 1.099301]\n",
      "epoch:1 step:1067 [D loss: 0.733252, acc.: 55.47%] [G loss: 1.015987]\n",
      "epoch:1 step:1068 [D loss: 0.646525, acc.: 61.72%] [G loss: 1.040167]\n",
      "epoch:1 step:1069 [D loss: 0.676211, acc.: 60.94%] [G loss: 0.941857]\n",
      "epoch:1 step:1070 [D loss: 0.671218, acc.: 62.50%] [G loss: 0.984486]\n",
      "epoch:1 step:1071 [D loss: 0.625866, acc.: 66.41%] [G loss: 0.962215]\n",
      "epoch:1 step:1072 [D loss: 0.657177, acc.: 63.28%] [G loss: 1.062108]\n",
      "epoch:1 step:1073 [D loss: 0.716080, acc.: 55.47%] [G loss: 0.982631]\n",
      "epoch:1 step:1074 [D loss: 0.635838, acc.: 61.72%] [G loss: 1.026210]\n",
      "epoch:1 step:1075 [D loss: 0.694212, acc.: 49.22%] [G loss: 0.962184]\n",
      "epoch:1 step:1076 [D loss: 0.663143, acc.: 61.72%] [G loss: 1.050623]\n",
      "epoch:1 step:1077 [D loss: 0.704780, acc.: 57.81%] [G loss: 1.176721]\n",
      "epoch:1 step:1078 [D loss: 0.736427, acc.: 51.56%] [G loss: 0.960665]\n",
      "epoch:1 step:1079 [D loss: 0.650768, acc.: 60.94%] [G loss: 0.924349]\n",
      "epoch:1 step:1080 [D loss: 0.639457, acc.: 69.53%] [G loss: 0.954208]\n",
      "epoch:1 step:1081 [D loss: 0.660055, acc.: 60.16%] [G loss: 0.950132]\n",
      "epoch:1 step:1082 [D loss: 0.646342, acc.: 64.06%] [G loss: 0.936065]\n",
      "epoch:1 step:1083 [D loss: 0.663716, acc.: 61.72%] [G loss: 0.852686]\n",
      "epoch:1 step:1084 [D loss: 0.632514, acc.: 59.38%] [G loss: 0.919582]\n",
      "epoch:1 step:1085 [D loss: 0.668921, acc.: 64.84%] [G loss: 0.989452]\n",
      "epoch:1 step:1086 [D loss: 0.655193, acc.: 67.19%] [G loss: 0.971098]\n",
      "epoch:1 step:1087 [D loss: 0.620640, acc.: 68.75%] [G loss: 1.007559]\n",
      "epoch:1 step:1088 [D loss: 0.606609, acc.: 70.31%] [G loss: 1.027740]\n",
      "epoch:1 step:1089 [D loss: 0.613190, acc.: 67.97%] [G loss: 0.968169]\n",
      "epoch:1 step:1090 [D loss: 0.661296, acc.: 59.38%] [G loss: 1.079931]\n",
      "epoch:1 step:1091 [D loss: 0.688456, acc.: 62.50%] [G loss: 1.025880]\n",
      "epoch:1 step:1092 [D loss: 0.634250, acc.: 64.84%] [G loss: 1.042438]\n",
      "epoch:1 step:1093 [D loss: 0.680335, acc.: 62.50%] [G loss: 1.071658]\n",
      "epoch:1 step:1094 [D loss: 0.629774, acc.: 66.41%] [G loss: 1.025964]\n",
      "epoch:1 step:1095 [D loss: 0.620755, acc.: 68.75%] [G loss: 1.044526]\n",
      "epoch:1 step:1096 [D loss: 0.649883, acc.: 68.75%] [G loss: 1.050272]\n",
      "epoch:1 step:1097 [D loss: 0.677138, acc.: 67.97%] [G loss: 1.000662]\n",
      "epoch:1 step:1098 [D loss: 0.628431, acc.: 62.50%] [G loss: 0.995932]\n",
      "epoch:1 step:1099 [D loss: 0.641527, acc.: 65.62%] [G loss: 0.985458]\n",
      "epoch:1 step:1100 [D loss: 0.655267, acc.: 60.94%] [G loss: 1.002126]\n",
      "epoch:1 step:1101 [D loss: 0.644726, acc.: 67.97%] [G loss: 1.119802]\n",
      "epoch:1 step:1102 [D loss: 0.638520, acc.: 63.28%] [G loss: 1.069674]\n",
      "epoch:1 step:1103 [D loss: 0.643828, acc.: 62.50%] [G loss: 1.134316]\n",
      "epoch:1 step:1104 [D loss: 0.667300, acc.: 60.94%] [G loss: 1.017158]\n",
      "epoch:1 step:1105 [D loss: 0.668490, acc.: 56.25%] [G loss: 0.960691]\n",
      "epoch:1 step:1106 [D loss: 0.657305, acc.: 62.50%] [G loss: 0.956715]\n",
      "epoch:1 step:1107 [D loss: 0.645312, acc.: 64.06%] [G loss: 1.101396]\n",
      "epoch:1 step:1108 [D loss: 0.638763, acc.: 64.06%] [G loss: 1.016359]\n",
      "epoch:1 step:1109 [D loss: 0.645607, acc.: 67.19%] [G loss: 0.981622]\n",
      "epoch:1 step:1110 [D loss: 0.637244, acc.: 61.72%] [G loss: 0.982482]\n",
      "epoch:1 step:1111 [D loss: 0.643676, acc.: 62.50%] [G loss: 0.978474]\n",
      "epoch:1 step:1112 [D loss: 0.646689, acc.: 60.16%] [G loss: 1.017072]\n",
      "epoch:1 step:1113 [D loss: 0.635842, acc.: 61.72%] [G loss: 1.001420]\n",
      "epoch:1 step:1114 [D loss: 0.623905, acc.: 71.88%] [G loss: 1.111527]\n",
      "epoch:1 step:1115 [D loss: 0.660040, acc.: 61.72%] [G loss: 1.045146]\n",
      "epoch:1 step:1116 [D loss: 0.651219, acc.: 63.28%] [G loss: 1.019922]\n",
      "epoch:1 step:1117 [D loss: 0.639845, acc.: 64.84%] [G loss: 1.022440]\n",
      "epoch:1 step:1118 [D loss: 0.628830, acc.: 70.31%] [G loss: 1.026992]\n",
      "epoch:1 step:1119 [D loss: 0.659252, acc.: 61.72%] [G loss: 0.996891]\n",
      "epoch:1 step:1120 [D loss: 0.649712, acc.: 69.53%] [G loss: 1.071011]\n",
      "epoch:1 step:1121 [D loss: 0.707990, acc.: 57.81%] [G loss: 1.093360]\n",
      "epoch:1 step:1122 [D loss: 0.702893, acc.: 59.38%] [G loss: 0.991183]\n",
      "epoch:1 step:1123 [D loss: 0.615015, acc.: 71.88%] [G loss: 0.977636]\n",
      "epoch:1 step:1124 [D loss: 0.629593, acc.: 64.84%] [G loss: 0.954298]\n",
      "epoch:1 step:1125 [D loss: 0.654661, acc.: 60.16%] [G loss: 0.969469]\n",
      "epoch:1 step:1126 [D loss: 0.693302, acc.: 53.91%] [G loss: 0.955686]\n",
      "epoch:1 step:1127 [D loss: 0.658759, acc.: 62.50%] [G loss: 0.980937]\n",
      "epoch:1 step:1128 [D loss: 0.661048, acc.: 59.38%] [G loss: 1.010111]\n",
      "epoch:1 step:1129 [D loss: 0.669872, acc.: 59.38%] [G loss: 1.022861]\n",
      "epoch:1 step:1130 [D loss: 0.669427, acc.: 57.81%] [G loss: 1.003576]\n",
      "epoch:1 step:1131 [D loss: 0.665986, acc.: 54.69%] [G loss: 1.046399]\n",
      "epoch:1 step:1132 [D loss: 0.694280, acc.: 63.28%] [G loss: 1.089051]\n",
      "epoch:1 step:1133 [D loss: 0.758037, acc.: 48.44%] [G loss: 0.973434]\n",
      "epoch:1 step:1134 [D loss: 0.621631, acc.: 66.41%] [G loss: 0.965140]\n",
      "epoch:1 step:1135 [D loss: 0.674211, acc.: 57.81%] [G loss: 0.955191]\n",
      "epoch:1 step:1136 [D loss: 0.629437, acc.: 61.72%] [G loss: 0.974448]\n",
      "epoch:1 step:1137 [D loss: 0.698898, acc.: 54.69%] [G loss: 0.943730]\n",
      "epoch:1 step:1138 [D loss: 0.621409, acc.: 70.31%] [G loss: 1.028762]\n",
      "epoch:1 step:1139 [D loss: 0.640654, acc.: 62.50%] [G loss: 1.063609]\n",
      "epoch:1 step:1140 [D loss: 0.657013, acc.: 62.50%] [G loss: 1.092476]\n",
      "epoch:1 step:1141 [D loss: 0.657091, acc.: 58.59%] [G loss: 0.995832]\n",
      "epoch:1 step:1142 [D loss: 0.667525, acc.: 63.28%] [G loss: 0.960540]\n",
      "epoch:1 step:1143 [D loss: 0.678955, acc.: 57.03%] [G loss: 1.016982]\n",
      "epoch:1 step:1144 [D loss: 0.722892, acc.: 48.44%] [G loss: 0.899514]\n",
      "epoch:1 step:1145 [D loss: 0.632986, acc.: 67.19%] [G loss: 0.990403]\n",
      "epoch:1 step:1146 [D loss: 0.693225, acc.: 53.12%] [G loss: 0.882541]\n",
      "epoch:1 step:1147 [D loss: 0.658345, acc.: 60.16%] [G loss: 0.980564]\n",
      "epoch:1 step:1148 [D loss: 0.632759, acc.: 66.41%] [G loss: 0.915111]\n",
      "epoch:1 step:1149 [D loss: 0.678074, acc.: 57.81%] [G loss: 0.979543]\n",
      "epoch:1 step:1150 [D loss: 0.670356, acc.: 61.72%] [G loss: 0.934370]\n",
      "epoch:1 step:1151 [D loss: 0.669431, acc.: 59.38%] [G loss: 0.996685]\n",
      "epoch:1 step:1152 [D loss: 0.679106, acc.: 60.16%] [G loss: 1.006158]\n",
      "epoch:1 step:1153 [D loss: 0.665517, acc.: 58.59%] [G loss: 0.993844]\n",
      "epoch:1 step:1154 [D loss: 0.627474, acc.: 65.62%] [G loss: 1.054911]\n",
      "epoch:1 step:1155 [D loss: 0.713778, acc.: 51.56%] [G loss: 1.067839]\n",
      "epoch:1 step:1156 [D loss: 0.661303, acc.: 67.19%] [G loss: 0.925778]\n",
      "epoch:1 step:1157 [D loss: 0.614427, acc.: 68.75%] [G loss: 0.980400]\n",
      "epoch:1 step:1158 [D loss: 0.649526, acc.: 62.50%] [G loss: 1.008593]\n",
      "epoch:1 step:1159 [D loss: 0.679897, acc.: 62.50%] [G loss: 1.000181]\n",
      "epoch:1 step:1160 [D loss: 0.633304, acc.: 62.50%] [G loss: 0.997335]\n",
      "epoch:1 step:1161 [D loss: 0.629044, acc.: 70.31%] [G loss: 0.947947]\n",
      "epoch:1 step:1162 [D loss: 0.681265, acc.: 58.59%] [G loss: 1.043160]\n",
      "epoch:1 step:1163 [D loss: 0.648990, acc.: 63.28%] [G loss: 1.090134]\n",
      "epoch:1 step:1164 [D loss: 0.678958, acc.: 63.28%] [G loss: 1.019309]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:1 step:1165 [D loss: 0.651834, acc.: 64.06%] [G loss: 0.989370]\n",
      "epoch:1 step:1166 [D loss: 0.668832, acc.: 58.59%] [G loss: 1.013286]\n",
      "epoch:1 step:1167 [D loss: 0.706742, acc.: 51.56%] [G loss: 0.977864]\n",
      "epoch:1 step:1168 [D loss: 0.724805, acc.: 53.12%] [G loss: 0.976537]\n",
      "epoch:1 step:1169 [D loss: 0.675938, acc.: 55.47%] [G loss: 0.959961]\n",
      "epoch:1 step:1170 [D loss: 0.638874, acc.: 64.06%] [G loss: 0.996492]\n",
      "epoch:1 step:1171 [D loss: 0.628588, acc.: 64.84%] [G loss: 1.128881]\n",
      "epoch:1 step:1172 [D loss: 0.759131, acc.: 47.66%] [G loss: 0.974792]\n",
      "epoch:1 step:1173 [D loss: 0.644663, acc.: 64.84%] [G loss: 1.050465]\n",
      "epoch:1 step:1174 [D loss: 0.672862, acc.: 59.38%] [G loss: 1.020055]\n",
      "epoch:1 step:1175 [D loss: 0.681898, acc.: 59.38%] [G loss: 1.025595]\n",
      "epoch:1 step:1176 [D loss: 0.694836, acc.: 60.94%] [G loss: 1.060166]\n",
      "epoch:1 step:1177 [D loss: 0.666658, acc.: 64.06%] [G loss: 1.069613]\n",
      "epoch:1 step:1178 [D loss: 0.670506, acc.: 55.47%] [G loss: 0.971525]\n",
      "epoch:1 step:1179 [D loss: 0.636540, acc.: 65.62%] [G loss: 1.064749]\n",
      "epoch:1 step:1180 [D loss: 0.677388, acc.: 55.47%] [G loss: 1.020361]\n",
      "epoch:1 step:1181 [D loss: 0.666435, acc.: 61.72%] [G loss: 0.971841]\n",
      "epoch:1 step:1182 [D loss: 0.690542, acc.: 51.56%] [G loss: 0.981568]\n",
      "epoch:1 step:1183 [D loss: 0.668938, acc.: 59.38%] [G loss: 0.993324]\n",
      "epoch:1 step:1184 [D loss: 0.666331, acc.: 64.06%] [G loss: 0.994534]\n",
      "epoch:1 step:1185 [D loss: 0.685598, acc.: 57.03%] [G loss: 1.019558]\n",
      "epoch:1 step:1186 [D loss: 0.653871, acc.: 62.50%] [G loss: 1.028955]\n",
      "epoch:1 step:1187 [D loss: 0.617766, acc.: 69.53%] [G loss: 1.012839]\n",
      "epoch:1 step:1188 [D loss: 0.656260, acc.: 62.50%] [G loss: 1.034987]\n",
      "epoch:1 step:1189 [D loss: 0.674231, acc.: 56.25%] [G loss: 0.939668]\n",
      "epoch:1 step:1190 [D loss: 0.641120, acc.: 59.38%] [G loss: 0.931748]\n",
      "epoch:1 step:1191 [D loss: 0.652372, acc.: 61.72%] [G loss: 0.997823]\n",
      "epoch:1 step:1192 [D loss: 0.648858, acc.: 67.97%] [G loss: 1.002885]\n",
      "epoch:1 step:1193 [D loss: 0.646919, acc.: 61.72%] [G loss: 1.020182]\n",
      "epoch:1 step:1194 [D loss: 0.632466, acc.: 66.41%] [G loss: 1.014619]\n",
      "epoch:1 step:1195 [D loss: 0.664130, acc.: 56.25%] [G loss: 0.944579]\n",
      "epoch:1 step:1196 [D loss: 0.638446, acc.: 61.72%] [G loss: 1.056000]\n",
      "epoch:1 step:1197 [D loss: 0.652046, acc.: 63.28%] [G loss: 0.981608]\n",
      "epoch:1 step:1198 [D loss: 0.649843, acc.: 64.06%] [G loss: 0.995306]\n",
      "epoch:1 step:1199 [D loss: 0.646807, acc.: 58.59%] [G loss: 1.094162]\n",
      "epoch:1 step:1200 [D loss: 0.656970, acc.: 66.41%] [G loss: 1.114187]\n",
      "##############\n",
      "[0.86170104 0.81366689 0.82440687 0.83496959 0.80296907 0.82001556\n",
      " 0.89538734 0.78929801 0.79584247 0.8126626 ]\n",
      "##########\n",
      "epoch:1 step:1201 [D loss: 0.682400, acc.: 64.06%] [G loss: 1.029475]\n",
      "epoch:1 step:1202 [D loss: 0.655212, acc.: 64.84%] [G loss: 1.116533]\n",
      "epoch:1 step:1203 [D loss: 0.663521, acc.: 62.50%] [G loss: 1.098220]\n",
      "epoch:1 step:1204 [D loss: 0.636485, acc.: 68.75%] [G loss: 1.142836]\n",
      "epoch:1 step:1205 [D loss: 0.692684, acc.: 56.25%] [G loss: 1.228534]\n",
      "epoch:1 step:1206 [D loss: 0.752187, acc.: 46.88%] [G loss: 0.980828]\n",
      "epoch:1 step:1207 [D loss: 0.681255, acc.: 63.28%] [G loss: 0.995440]\n",
      "epoch:1 step:1208 [D loss: 0.671359, acc.: 60.94%] [G loss: 1.007212]\n",
      "epoch:1 step:1209 [D loss: 0.695949, acc.: 54.69%] [G loss: 0.997000]\n",
      "epoch:1 step:1210 [D loss: 0.679214, acc.: 60.94%] [G loss: 0.911044]\n",
      "epoch:1 step:1211 [D loss: 0.673176, acc.: 59.38%] [G loss: 0.978119]\n",
      "epoch:1 step:1212 [D loss: 0.680134, acc.: 57.03%] [G loss: 0.958468]\n",
      "epoch:1 step:1213 [D loss: 0.702332, acc.: 57.03%] [G loss: 0.966881]\n",
      "epoch:1 step:1214 [D loss: 0.701161, acc.: 64.06%] [G loss: 1.045606]\n",
      "epoch:1 step:1215 [D loss: 0.655320, acc.: 66.41%] [G loss: 0.987470]\n",
      "epoch:1 step:1216 [D loss: 0.687706, acc.: 54.69%] [G loss: 0.934122]\n",
      "epoch:1 step:1217 [D loss: 0.687824, acc.: 53.12%] [G loss: 0.948067]\n",
      "epoch:1 step:1218 [D loss: 0.641295, acc.: 66.41%] [G loss: 0.996730]\n",
      "epoch:1 step:1219 [D loss: 0.724707, acc.: 49.22%] [G loss: 1.106103]\n",
      "epoch:1 step:1220 [D loss: 0.713683, acc.: 57.03%] [G loss: 0.978846]\n",
      "epoch:1 step:1221 [D loss: 0.658232, acc.: 60.16%] [G loss: 0.999742]\n",
      "epoch:1 step:1222 [D loss: 0.677434, acc.: 53.12%] [G loss: 0.971458]\n",
      "epoch:1 step:1223 [D loss: 0.658700, acc.: 60.16%] [G loss: 1.052557]\n",
      "epoch:1 step:1224 [D loss: 0.702799, acc.: 57.03%] [G loss: 0.919101]\n",
      "epoch:1 step:1225 [D loss: 0.660434, acc.: 63.28%] [G loss: 0.899010]\n",
      "epoch:1 step:1226 [D loss: 0.669472, acc.: 57.81%] [G loss: 0.955557]\n",
      "epoch:1 step:1227 [D loss: 0.695203, acc.: 55.47%] [G loss: 0.981882]\n",
      "epoch:1 step:1228 [D loss: 0.678461, acc.: 58.59%] [G loss: 0.938453]\n",
      "epoch:1 step:1229 [D loss: 0.671361, acc.: 57.81%] [G loss: 0.937623]\n",
      "epoch:1 step:1230 [D loss: 0.727630, acc.: 51.56%] [G loss: 0.961640]\n",
      "epoch:1 step:1231 [D loss: 0.702591, acc.: 49.22%] [G loss: 0.966386]\n",
      "epoch:1 step:1232 [D loss: 0.672571, acc.: 57.03%] [G loss: 0.935765]\n",
      "epoch:1 step:1233 [D loss: 0.732804, acc.: 53.91%] [G loss: 0.983641]\n",
      "epoch:1 step:1234 [D loss: 0.713740, acc.: 52.34%] [G loss: 0.936276]\n",
      "epoch:1 step:1235 [D loss: 0.684952, acc.: 56.25%] [G loss: 0.990010]\n",
      "epoch:1 step:1236 [D loss: 0.714086, acc.: 51.56%] [G loss: 1.035251]\n",
      "epoch:1 step:1237 [D loss: 0.688944, acc.: 56.25%] [G loss: 0.913071]\n",
      "epoch:1 step:1238 [D loss: 0.687724, acc.: 62.50%] [G loss: 0.854607]\n",
      "epoch:1 step:1239 [D loss: 0.658753, acc.: 55.47%] [G loss: 0.898968]\n",
      "epoch:1 step:1240 [D loss: 0.637766, acc.: 65.62%] [G loss: 1.012367]\n",
      "epoch:1 step:1241 [D loss: 0.629621, acc.: 68.75%] [G loss: 0.977062]\n",
      "epoch:1 step:1242 [D loss: 0.677882, acc.: 59.38%] [G loss: 0.984053]\n",
      "epoch:1 step:1243 [D loss: 0.700625, acc.: 54.69%] [G loss: 0.957656]\n",
      "epoch:1 step:1244 [D loss: 0.629766, acc.: 64.84%] [G loss: 0.920442]\n",
      "epoch:1 step:1245 [D loss: 0.652599, acc.: 55.47%] [G loss: 0.930209]\n",
      "epoch:1 step:1246 [D loss: 0.656603, acc.: 61.72%] [G loss: 1.001398]\n",
      "epoch:1 step:1247 [D loss: 0.675895, acc.: 53.91%] [G loss: 1.001451]\n",
      "epoch:1 step:1248 [D loss: 0.649736, acc.: 64.84%] [G loss: 1.045338]\n",
      "epoch:1 step:1249 [D loss: 0.694314, acc.: 53.91%] [G loss: 1.040164]\n",
      "epoch:1 step:1250 [D loss: 0.668511, acc.: 55.47%] [G loss: 1.033525]\n",
      "epoch:1 step:1251 [D loss: 0.648922, acc.: 67.19%] [G loss: 1.049823]\n",
      "epoch:1 step:1252 [D loss: 0.615256, acc.: 67.97%] [G loss: 1.092805]\n",
      "epoch:1 step:1253 [D loss: 0.707496, acc.: 52.34%] [G loss: 0.967604]\n",
      "epoch:1 step:1254 [D loss: 0.601136, acc.: 69.53%] [G loss: 0.931545]\n",
      "epoch:1 step:1255 [D loss: 0.664480, acc.: 60.16%] [G loss: 0.953633]\n",
      "epoch:1 step:1256 [D loss: 0.657500, acc.: 65.62%] [G loss: 0.936282]\n",
      "epoch:1 step:1257 [D loss: 0.619318, acc.: 69.53%] [G loss: 0.931427]\n",
      "epoch:1 step:1258 [D loss: 0.632637, acc.: 64.84%] [G loss: 0.946054]\n",
      "epoch:1 step:1259 [D loss: 0.647102, acc.: 65.62%] [G loss: 1.020403]\n",
      "epoch:1 step:1260 [D loss: 0.657396, acc.: 60.16%] [G loss: 1.038144]\n",
      "epoch:1 step:1261 [D loss: 0.696446, acc.: 57.03%] [G loss: 0.916862]\n",
      "epoch:1 step:1262 [D loss: 0.641393, acc.: 60.94%] [G loss: 1.017310]\n",
      "epoch:1 step:1263 [D loss: 0.679879, acc.: 57.81%] [G loss: 0.938880]\n",
      "epoch:1 step:1264 [D loss: 0.652383, acc.: 64.06%] [G loss: 1.050132]\n",
      "epoch:1 step:1265 [D loss: 0.689611, acc.: 55.47%] [G loss: 0.986777]\n",
      "epoch:1 step:1266 [D loss: 0.657977, acc.: 60.94%] [G loss: 0.956261]\n",
      "epoch:1 step:1267 [D loss: 0.678259, acc.: 56.25%] [G loss: 0.955749]\n",
      "epoch:1 step:1268 [D loss: 0.645650, acc.: 64.06%] [G loss: 0.990718]\n",
      "epoch:1 step:1269 [D loss: 0.632441, acc.: 64.84%] [G loss: 1.037573]\n",
      "epoch:1 step:1270 [D loss: 0.698504, acc.: 57.03%] [G loss: 0.974372]\n",
      "epoch:1 step:1271 [D loss: 0.698445, acc.: 56.25%] [G loss: 0.913790]\n",
      "epoch:1 step:1272 [D loss: 0.651500, acc.: 63.28%] [G loss: 0.982046]\n",
      "epoch:1 step:1273 [D loss: 0.704952, acc.: 50.00%] [G loss: 1.097859]\n",
      "epoch:1 step:1274 [D loss: 0.666943, acc.: 61.72%] [G loss: 1.054185]\n",
      "epoch:1 step:1275 [D loss: 0.648726, acc.: 65.62%] [G loss: 1.068226]\n",
      "epoch:1 step:1276 [D loss: 0.658547, acc.: 63.28%] [G loss: 1.002115]\n",
      "epoch:1 step:1277 [D loss: 0.690075, acc.: 52.34%] [G loss: 0.947613]\n",
      "epoch:1 step:1278 [D loss: 0.666738, acc.: 60.94%] [G loss: 0.965794]\n",
      "epoch:1 step:1279 [D loss: 0.666866, acc.: 57.81%] [G loss: 0.899751]\n",
      "epoch:1 step:1280 [D loss: 0.670560, acc.: 57.81%] [G loss: 0.939456]\n",
      "epoch:1 step:1281 [D loss: 0.712655, acc.: 56.25%] [G loss: 1.067302]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:1 step:1282 [D loss: 0.712772, acc.: 55.47%] [G loss: 0.999044]\n",
      "epoch:1 step:1283 [D loss: 0.650899, acc.: 66.41%] [G loss: 1.017614]\n",
      "epoch:1 step:1284 [D loss: 0.695856, acc.: 56.25%] [G loss: 1.039490]\n",
      "epoch:1 step:1285 [D loss: 0.800206, acc.: 49.22%] [G loss: 0.947261]\n",
      "epoch:1 step:1286 [D loss: 0.724579, acc.: 48.44%] [G loss: 0.858602]\n",
      "epoch:1 step:1287 [D loss: 0.644714, acc.: 64.84%] [G loss: 0.929154]\n",
      "epoch:1 step:1288 [D loss: 0.673698, acc.: 57.81%] [G loss: 0.963335]\n",
      "epoch:1 step:1289 [D loss: 0.663810, acc.: 62.50%] [G loss: 0.959407]\n",
      "epoch:1 step:1290 [D loss: 0.646317, acc.: 67.19%] [G loss: 1.021365]\n",
      "epoch:1 step:1291 [D loss: 0.650470, acc.: 61.72%] [G loss: 0.990418]\n",
      "epoch:1 step:1292 [D loss: 0.670576, acc.: 55.47%] [G loss: 0.956853]\n",
      "epoch:1 step:1293 [D loss: 0.645413, acc.: 60.94%] [G loss: 1.035949]\n",
      "epoch:1 step:1294 [D loss: 0.624921, acc.: 64.06%] [G loss: 1.030846]\n",
      "epoch:1 step:1295 [D loss: 0.687599, acc.: 52.34%] [G loss: 0.961861]\n",
      "epoch:1 step:1296 [D loss: 0.703886, acc.: 56.25%] [G loss: 0.996653]\n",
      "epoch:1 step:1297 [D loss: 0.670751, acc.: 56.25%] [G loss: 0.977070]\n",
      "epoch:1 step:1298 [D loss: 0.656936, acc.: 58.59%] [G loss: 1.003713]\n",
      "epoch:1 step:1299 [D loss: 0.707290, acc.: 57.03%] [G loss: 1.012972]\n",
      "epoch:1 step:1300 [D loss: 0.669023, acc.: 64.06%] [G loss: 1.146552]\n",
      "epoch:1 step:1301 [D loss: 0.711694, acc.: 52.34%] [G loss: 1.052778]\n",
      "epoch:1 step:1302 [D loss: 0.685258, acc.: 59.38%] [G loss: 1.030920]\n",
      "epoch:1 step:1303 [D loss: 0.655746, acc.: 65.62%] [G loss: 0.963857]\n",
      "epoch:1 step:1304 [D loss: 0.634336, acc.: 60.16%] [G loss: 1.027839]\n",
      "epoch:1 step:1305 [D loss: 0.656763, acc.: 58.59%] [G loss: 1.010197]\n",
      "epoch:1 step:1306 [D loss: 0.648747, acc.: 61.72%] [G loss: 1.066808]\n",
      "epoch:1 step:1307 [D loss: 0.693783, acc.: 57.81%] [G loss: 1.032543]\n",
      "epoch:1 step:1308 [D loss: 0.615843, acc.: 75.78%] [G loss: 1.126323]\n",
      "epoch:1 step:1309 [D loss: 0.705394, acc.: 54.69%] [G loss: 0.957601]\n",
      "epoch:1 step:1310 [D loss: 0.669262, acc.: 54.69%] [G loss: 0.984760]\n",
      "epoch:1 step:1311 [D loss: 0.665965, acc.: 59.38%] [G loss: 0.962714]\n",
      "epoch:1 step:1312 [D loss: 0.671009, acc.: 57.81%] [G loss: 0.978229]\n",
      "epoch:1 step:1313 [D loss: 0.666003, acc.: 58.59%] [G loss: 0.929022]\n",
      "epoch:1 step:1314 [D loss: 0.674856, acc.: 58.59%] [G loss: 0.990340]\n",
      "epoch:1 step:1315 [D loss: 0.692419, acc.: 54.69%] [G loss: 0.998638]\n",
      "epoch:1 step:1316 [D loss: 0.655433, acc.: 60.94%] [G loss: 0.963718]\n",
      "epoch:1 step:1317 [D loss: 0.656399, acc.: 59.38%] [G loss: 0.956021]\n",
      "epoch:1 step:1318 [D loss: 0.682333, acc.: 57.81%] [G loss: 1.140738]\n",
      "epoch:1 step:1319 [D loss: 0.740525, acc.: 57.03%] [G loss: 1.099537]\n",
      "epoch:1 step:1320 [D loss: 0.707073, acc.: 56.25%] [G loss: 1.054849]\n",
      "epoch:1 step:1321 [D loss: 0.674198, acc.: 60.94%] [G loss: 0.960538]\n",
      "epoch:1 step:1322 [D loss: 0.655399, acc.: 64.06%] [G loss: 0.998093]\n",
      "epoch:1 step:1323 [D loss: 0.619527, acc.: 69.53%] [G loss: 0.974337]\n",
      "epoch:1 step:1324 [D loss: 0.640847, acc.: 64.84%] [G loss: 0.991100]\n",
      "epoch:1 step:1325 [D loss: 0.630050, acc.: 68.75%] [G loss: 1.055135]\n",
      "epoch:1 step:1326 [D loss: 0.676924, acc.: 56.25%] [G loss: 1.032464]\n",
      "epoch:1 step:1327 [D loss: 0.668515, acc.: 67.19%] [G loss: 1.044307]\n",
      "epoch:1 step:1328 [D loss: 0.694682, acc.: 50.78%] [G loss: 0.972463]\n",
      "epoch:1 step:1329 [D loss: 0.658576, acc.: 58.59%] [G loss: 0.930999]\n",
      "epoch:1 step:1330 [D loss: 0.651221, acc.: 63.28%] [G loss: 0.949567]\n",
      "epoch:1 step:1331 [D loss: 0.631317, acc.: 66.41%] [G loss: 1.022348]\n",
      "epoch:1 step:1332 [D loss: 0.671240, acc.: 56.25%] [G loss: 1.031823]\n",
      "epoch:1 step:1333 [D loss: 0.671864, acc.: 54.69%] [G loss: 1.043190]\n",
      "epoch:1 step:1334 [D loss: 0.679186, acc.: 54.69%] [G loss: 0.938729]\n",
      "epoch:1 step:1335 [D loss: 0.618848, acc.: 66.41%] [G loss: 1.003711]\n",
      "epoch:1 step:1336 [D loss: 0.666019, acc.: 54.69%] [G loss: 1.014474]\n",
      "epoch:1 step:1337 [D loss: 0.606864, acc.: 76.56%] [G loss: 0.984587]\n",
      "epoch:1 step:1338 [D loss: 0.651912, acc.: 57.03%] [G loss: 0.985153]\n",
      "epoch:1 step:1339 [D loss: 0.677361, acc.: 55.47%] [G loss: 1.000180]\n",
      "epoch:1 step:1340 [D loss: 0.664808, acc.: 56.25%] [G loss: 1.070539]\n",
      "epoch:1 step:1341 [D loss: 0.668532, acc.: 61.72%] [G loss: 1.074594]\n",
      "epoch:1 step:1342 [D loss: 0.667796, acc.: 53.91%] [G loss: 1.075069]\n",
      "epoch:1 step:1343 [D loss: 0.643952, acc.: 62.50%] [G loss: 1.047128]\n",
      "epoch:1 step:1344 [D loss: 0.723199, acc.: 54.69%] [G loss: 1.162304]\n",
      "epoch:1 step:1345 [D loss: 0.774514, acc.: 51.56%] [G loss: 0.993303]\n",
      "epoch:1 step:1346 [D loss: 0.685371, acc.: 56.25%] [G loss: 0.982995]\n",
      "epoch:1 step:1347 [D loss: 0.634032, acc.: 74.22%] [G loss: 0.950814]\n",
      "epoch:1 step:1348 [D loss: 0.684436, acc.: 56.25%] [G loss: 0.974715]\n",
      "epoch:1 step:1349 [D loss: 0.671405, acc.: 58.59%] [G loss: 1.044460]\n",
      "epoch:1 step:1350 [D loss: 0.686945, acc.: 58.59%] [G loss: 1.104731]\n",
      "epoch:1 step:1351 [D loss: 0.691643, acc.: 53.91%] [G loss: 0.995838]\n",
      "epoch:1 step:1352 [D loss: 0.694601, acc.: 56.25%] [G loss: 1.017037]\n",
      "epoch:1 step:1353 [D loss: 0.619392, acc.: 67.19%] [G loss: 0.997337]\n",
      "epoch:1 step:1354 [D loss: 0.676983, acc.: 57.03%] [G loss: 0.980424]\n",
      "epoch:1 step:1355 [D loss: 0.661261, acc.: 57.03%] [G loss: 1.013570]\n",
      "epoch:1 step:1356 [D loss: 0.654793, acc.: 64.06%] [G loss: 1.025006]\n",
      "epoch:1 step:1357 [D loss: 0.640056, acc.: 63.28%] [G loss: 1.021869]\n",
      "epoch:1 step:1358 [D loss: 0.687690, acc.: 57.03%] [G loss: 1.058990]\n",
      "epoch:1 step:1359 [D loss: 0.690423, acc.: 57.03%] [G loss: 1.090436]\n",
      "epoch:1 step:1360 [D loss: 0.650324, acc.: 64.84%] [G loss: 1.029811]\n",
      "epoch:1 step:1361 [D loss: 0.630936, acc.: 63.28%] [G loss: 0.992019]\n",
      "epoch:1 step:1362 [D loss: 0.669525, acc.: 59.38%] [G loss: 1.001732]\n",
      "epoch:1 step:1363 [D loss: 0.643611, acc.: 66.41%] [G loss: 1.002700]\n",
      "epoch:1 step:1364 [D loss: 0.683477, acc.: 58.59%] [G loss: 1.004678]\n",
      "epoch:1 step:1365 [D loss: 0.672718, acc.: 53.12%] [G loss: 1.038604]\n",
      "epoch:1 step:1366 [D loss: 0.656560, acc.: 63.28%] [G loss: 1.096470]\n",
      "epoch:1 step:1367 [D loss: 0.637232, acc.: 58.59%] [G loss: 1.125739]\n",
      "epoch:1 step:1368 [D loss: 0.618402, acc.: 64.84%] [G loss: 0.956023]\n",
      "epoch:1 step:1369 [D loss: 0.663544, acc.: 59.38%] [G loss: 0.987603]\n",
      "epoch:1 step:1370 [D loss: 0.611520, acc.: 71.88%] [G loss: 0.990486]\n",
      "epoch:1 step:1371 [D loss: 0.624475, acc.: 64.84%] [G loss: 0.993637]\n",
      "epoch:1 step:1372 [D loss: 0.627058, acc.: 64.06%] [G loss: 1.002838]\n",
      "epoch:1 step:1373 [D loss: 0.628355, acc.: 67.97%] [G loss: 0.997915]\n",
      "epoch:1 step:1374 [D loss: 0.623765, acc.: 71.09%] [G loss: 0.991557]\n",
      "epoch:1 step:1375 [D loss: 0.634256, acc.: 64.84%] [G loss: 0.965208]\n",
      "epoch:1 step:1376 [D loss: 0.644163, acc.: 62.50%] [G loss: 1.006868]\n",
      "epoch:1 step:1377 [D loss: 0.627533, acc.: 71.88%] [G loss: 1.054242]\n",
      "epoch:1 step:1378 [D loss: 0.687387, acc.: 54.69%] [G loss: 1.005232]\n",
      "epoch:1 step:1379 [D loss: 0.661937, acc.: 64.06%] [G loss: 1.062894]\n",
      "epoch:1 step:1380 [D loss: 0.644071, acc.: 66.41%] [G loss: 1.081651]\n",
      "epoch:1 step:1381 [D loss: 0.615567, acc.: 70.31%] [G loss: 1.169893]\n",
      "epoch:1 step:1382 [D loss: 0.598357, acc.: 68.75%] [G loss: 1.252349]\n",
      "epoch:1 step:1383 [D loss: 0.596657, acc.: 67.19%] [G loss: 1.134667]\n",
      "epoch:1 step:1384 [D loss: 0.644873, acc.: 63.28%] [G loss: 1.099612]\n",
      "epoch:1 step:1385 [D loss: 0.616715, acc.: 66.41%] [G loss: 1.110856]\n",
      "epoch:1 step:1386 [D loss: 0.677218, acc.: 53.91%] [G loss: 1.003482]\n",
      "epoch:1 step:1387 [D loss: 0.639535, acc.: 60.16%] [G loss: 0.982916]\n",
      "epoch:1 step:1388 [D loss: 0.637583, acc.: 61.72%] [G loss: 1.047355]\n",
      "epoch:1 step:1389 [D loss: 0.618235, acc.: 67.97%] [G loss: 1.138580]\n",
      "epoch:1 step:1390 [D loss: 0.660679, acc.: 60.16%] [G loss: 1.056369]\n",
      "epoch:1 step:1391 [D loss: 0.629252, acc.: 65.62%] [G loss: 1.092966]\n",
      "epoch:1 step:1392 [D loss: 0.646707, acc.: 58.59%] [G loss: 1.285846]\n",
      "epoch:1 step:1393 [D loss: 0.665274, acc.: 60.94%] [G loss: 1.090647]\n",
      "epoch:1 step:1394 [D loss: 0.646093, acc.: 59.38%] [G loss: 1.104519]\n",
      "epoch:1 step:1395 [D loss: 0.695793, acc.: 57.03%] [G loss: 1.069310]\n",
      "epoch:1 step:1396 [D loss: 0.564709, acc.: 67.97%] [G loss: 1.239801]\n",
      "epoch:1 step:1397 [D loss: 0.642640, acc.: 55.47%] [G loss: 1.121740]\n",
      "epoch:1 step:1398 [D loss: 0.710857, acc.: 53.12%] [G loss: 1.182456]\n",
      "epoch:1 step:1399 [D loss: 0.690300, acc.: 54.69%] [G loss: 1.063249]\n",
      "epoch:1 step:1400 [D loss: 0.630470, acc.: 67.19%] [G loss: 1.153757]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##############\n",
      "[0.84958093 0.82736597 0.79469253 0.82373995 0.76989621 0.79426577\n",
      " 0.90393801 0.82029249 0.83268893 0.82203759]\n",
      "##########\n",
      "epoch:1 step:1401 [D loss: 0.638422, acc.: 62.50%] [G loss: 1.054476]\n",
      "epoch:1 step:1402 [D loss: 0.664303, acc.: 60.94%] [G loss: 0.958404]\n",
      "epoch:1 step:1403 [D loss: 0.634859, acc.: 61.72%] [G loss: 1.025550]\n",
      "epoch:1 step:1404 [D loss: 0.646283, acc.: 60.94%] [G loss: 1.147355]\n",
      "epoch:1 step:1405 [D loss: 0.668195, acc.: 57.81%] [G loss: 1.103273]\n",
      "epoch:1 step:1406 [D loss: 0.669512, acc.: 54.69%] [G loss: 0.953539]\n",
      "epoch:1 step:1407 [D loss: 0.691944, acc.: 53.91%] [G loss: 0.938494]\n",
      "epoch:1 step:1408 [D loss: 0.635400, acc.: 65.62%] [G loss: 0.993890]\n",
      "epoch:1 step:1409 [D loss: 0.681891, acc.: 50.00%] [G loss: 1.003726]\n",
      "epoch:1 step:1410 [D loss: 0.647431, acc.: 68.75%] [G loss: 1.097301]\n",
      "epoch:1 step:1411 [D loss: 0.621165, acc.: 68.75%] [G loss: 0.988816]\n",
      "epoch:1 step:1412 [D loss: 0.609195, acc.: 67.97%] [G loss: 1.066558]\n",
      "epoch:1 step:1413 [D loss: 0.564994, acc.: 77.34%] [G loss: 1.297460]\n",
      "epoch:1 step:1414 [D loss: 0.593730, acc.: 68.75%] [G loss: 1.258097]\n",
      "epoch:1 step:1415 [D loss: 0.588744, acc.: 70.31%] [G loss: 1.363541]\n",
      "epoch:1 step:1416 [D loss: 0.629839, acc.: 67.97%] [G loss: 1.083570]\n",
      "epoch:1 step:1417 [D loss: 0.584217, acc.: 74.22%] [G loss: 1.231343]\n",
      "epoch:1 step:1418 [D loss: 0.584527, acc.: 70.31%] [G loss: 1.291361]\n",
      "epoch:1 step:1419 [D loss: 0.624301, acc.: 66.41%] [G loss: 1.144805]\n",
      "epoch:1 step:1420 [D loss: 0.618371, acc.: 61.72%] [G loss: 1.125812]\n",
      "epoch:1 step:1421 [D loss: 0.596964, acc.: 71.88%] [G loss: 1.077403]\n",
      "epoch:1 step:1422 [D loss: 0.629488, acc.: 71.88%] [G loss: 1.117946]\n",
      "epoch:1 step:1423 [D loss: 0.671010, acc.: 55.47%] [G loss: 1.004269]\n",
      "epoch:1 step:1424 [D loss: 0.668996, acc.: 55.47%] [G loss: 1.017560]\n",
      "epoch:1 step:1425 [D loss: 0.649387, acc.: 65.62%] [G loss: 1.097580]\n",
      "epoch:1 step:1426 [D loss: 0.676736, acc.: 52.34%] [G loss: 1.272690]\n",
      "epoch:1 step:1427 [D loss: 0.634711, acc.: 60.94%] [G loss: 1.392746]\n",
      "epoch:1 step:1428 [D loss: 0.691807, acc.: 58.59%] [G loss: 0.948706]\n",
      "epoch:1 step:1429 [D loss: 0.629077, acc.: 67.19%] [G loss: 1.096616]\n",
      "epoch:1 step:1430 [D loss: 0.593667, acc.: 74.22%] [G loss: 1.161738]\n",
      "epoch:1 step:1431 [D loss: 0.651367, acc.: 60.94%] [G loss: 1.018879]\n",
      "epoch:1 step:1432 [D loss: 0.658642, acc.: 63.28%] [G loss: 1.241184]\n",
      "epoch:1 step:1433 [D loss: 0.583121, acc.: 73.44%] [G loss: 1.208385]\n",
      "epoch:1 step:1434 [D loss: 0.580169, acc.: 67.97%] [G loss: 1.218395]\n",
      "epoch:1 step:1435 [D loss: 0.532327, acc.: 75.78%] [G loss: 1.436677]\n",
      "epoch:1 step:1436 [D loss: 0.598508, acc.: 69.53%] [G loss: 1.612039]\n",
      "epoch:1 step:1437 [D loss: 0.549511, acc.: 77.34%] [G loss: 1.588526]\n",
      "epoch:1 step:1438 [D loss: 0.564931, acc.: 64.84%] [G loss: 1.485145]\n",
      "epoch:1 step:1439 [D loss: 0.533687, acc.: 74.22%] [G loss: 1.429366]\n",
      "epoch:1 step:1440 [D loss: 0.602916, acc.: 67.19%] [G loss: 1.077082]\n",
      "epoch:1 step:1441 [D loss: 0.623269, acc.: 71.88%] [G loss: 1.161193]\n",
      "epoch:1 step:1442 [D loss: 0.586142, acc.: 76.56%] [G loss: 1.368685]\n",
      "epoch:1 step:1443 [D loss: 0.593825, acc.: 71.09%] [G loss: 1.226695]\n",
      "epoch:1 step:1444 [D loss: 0.567143, acc.: 75.00%] [G loss: 1.184914]\n",
      "epoch:1 step:1445 [D loss: 0.521144, acc.: 84.38%] [G loss: 1.258114]\n",
      "epoch:1 step:1446 [D loss: 0.562144, acc.: 72.66%] [G loss: 1.379128]\n",
      "epoch:1 step:1447 [D loss: 0.625715, acc.: 65.62%] [G loss: 0.942819]\n",
      "epoch:1 step:1448 [D loss: 0.619658, acc.: 65.62%] [G loss: 1.276882]\n",
      "epoch:1 step:1449 [D loss: 0.628559, acc.: 60.16%] [G loss: 1.211923]\n",
      "epoch:1 step:1450 [D loss: 0.629521, acc.: 67.19%] [G loss: 1.214225]\n",
      "epoch:1 step:1451 [D loss: 0.610880, acc.: 61.72%] [G loss: 1.314243]\n",
      "epoch:1 step:1452 [D loss: 0.593991, acc.: 70.31%] [G loss: 1.015695]\n",
      "epoch:1 step:1453 [D loss: 0.614068, acc.: 67.97%] [G loss: 1.229333]\n",
      "epoch:1 step:1454 [D loss: 0.653757, acc.: 58.59%] [G loss: 1.351254]\n",
      "epoch:1 step:1455 [D loss: 0.598606, acc.: 62.50%] [G loss: 1.318835]\n",
      "epoch:1 step:1456 [D loss: 0.585277, acc.: 71.88%] [G loss: 1.576240]\n",
      "epoch:1 step:1457 [D loss: 0.656380, acc.: 57.03%] [G loss: 1.480943]\n",
      "epoch:1 step:1458 [D loss: 0.693113, acc.: 54.69%] [G loss: 1.036550]\n",
      "epoch:1 step:1459 [D loss: 0.684927, acc.: 58.59%] [G loss: 1.071430]\n",
      "epoch:1 step:1460 [D loss: 0.536627, acc.: 73.44%] [G loss: 1.508504]\n",
      "epoch:1 step:1461 [D loss: 0.572554, acc.: 74.22%] [G loss: 1.195014]\n",
      "epoch:1 step:1462 [D loss: 0.641359, acc.: 61.72%] [G loss: 1.109395]\n",
      "epoch:1 step:1463 [D loss: 0.701570, acc.: 48.44%] [G loss: 1.227520]\n",
      "epoch:1 step:1464 [D loss: 0.600403, acc.: 60.94%] [G loss: 1.561790]\n",
      "epoch:1 step:1465 [D loss: 0.640581, acc.: 67.97%] [G loss: 1.337559]\n",
      "epoch:1 step:1466 [D loss: 0.584261, acc.: 67.97%] [G loss: 1.430068]\n",
      "epoch:1 step:1467 [D loss: 0.655587, acc.: 64.84%] [G loss: 1.510520]\n",
      "epoch:1 step:1468 [D loss: 0.620786, acc.: 60.94%] [G loss: 1.472538]\n",
      "epoch:1 step:1469 [D loss: 0.645239, acc.: 64.06%] [G loss: 1.184676]\n",
      "epoch:1 step:1470 [D loss: 0.608165, acc.: 63.28%] [G loss: 1.331821]\n",
      "epoch:1 step:1471 [D loss: 0.483116, acc.: 78.91%] [G loss: 1.959505]\n",
      "epoch:1 step:1472 [D loss: 0.548423, acc.: 72.66%] [G loss: 1.470716]\n",
      "epoch:1 step:1473 [D loss: 0.598930, acc.: 68.75%] [G loss: 1.477776]\n",
      "epoch:1 step:1474 [D loss: 0.559941, acc.: 71.09%] [G loss: 1.310589]\n",
      "epoch:1 step:1475 [D loss: 0.518148, acc.: 75.00%] [G loss: 1.770040]\n",
      "epoch:1 step:1476 [D loss: 0.565082, acc.: 67.97%] [G loss: 1.212721]\n",
      "epoch:1 step:1477 [D loss: 0.534926, acc.: 74.22%] [G loss: 1.748961]\n",
      "epoch:1 step:1478 [D loss: 0.595921, acc.: 61.72%] [G loss: 1.830739]\n",
      "epoch:1 step:1479 [D loss: 0.589882, acc.: 70.31%] [G loss: 1.269681]\n",
      "epoch:1 step:1480 [D loss: 0.641421, acc.: 64.06%] [G loss: 1.288602]\n",
      "epoch:1 step:1481 [D loss: 0.554738, acc.: 75.78%] [G loss: 1.424590]\n",
      "epoch:1 step:1482 [D loss: 0.518844, acc.: 76.56%] [G loss: 1.821707]\n",
      "epoch:1 step:1483 [D loss: 0.583393, acc.: 72.66%] [G loss: 1.427564]\n",
      "epoch:1 step:1484 [D loss: 0.504037, acc.: 81.25%] [G loss: 1.577534]\n",
      "epoch:1 step:1485 [D loss: 0.579706, acc.: 71.88%] [G loss: 1.466687]\n",
      "epoch:1 step:1486 [D loss: 0.613379, acc.: 69.53%] [G loss: 1.975514]\n",
      "epoch:1 step:1487 [D loss: 0.646951, acc.: 64.06%] [G loss: 1.819818]\n",
      "epoch:1 step:1488 [D loss: 0.705779, acc.: 50.00%] [G loss: 1.634562]\n",
      "epoch:1 step:1489 [D loss: 0.589700, acc.: 70.31%] [G loss: 1.526549]\n",
      "epoch:1 step:1490 [D loss: 0.526839, acc.: 77.34%] [G loss: 1.752844]\n",
      "epoch:1 step:1491 [D loss: 0.520020, acc.: 78.91%] [G loss: 1.441335]\n",
      "epoch:1 step:1492 [D loss: 0.562564, acc.: 76.56%] [G loss: 1.484733]\n",
      "epoch:1 step:1493 [D loss: 0.533346, acc.: 75.78%] [G loss: 1.496565]\n",
      "epoch:1 step:1494 [D loss: 0.610289, acc.: 68.75%] [G loss: 1.344347]\n",
      "epoch:1 step:1495 [D loss: 0.512411, acc.: 78.12%] [G loss: 1.982290]\n",
      "epoch:1 step:1496 [D loss: 0.621678, acc.: 60.94%] [G loss: 1.447278]\n",
      "epoch:1 step:1497 [D loss: 0.540074, acc.: 73.44%] [G loss: 1.489218]\n",
      "epoch:1 step:1498 [D loss: 0.568515, acc.: 71.88%] [G loss: 1.363102]\n",
      "epoch:1 step:1499 [D loss: 0.556842, acc.: 74.22%] [G loss: 1.589198]\n",
      "epoch:1 step:1500 [D loss: 0.602730, acc.: 72.66%] [G loss: 1.266730]\n",
      "epoch:1 step:1501 [D loss: 0.563584, acc.: 78.91%] [G loss: 1.515929]\n",
      "epoch:1 step:1502 [D loss: 0.670248, acc.: 61.72%] [G loss: 1.721643]\n",
      "epoch:1 step:1503 [D loss: 0.624263, acc.: 57.81%] [G loss: 1.951153]\n",
      "epoch:1 step:1504 [D loss: 0.687108, acc.: 58.59%] [G loss: 1.306035]\n",
      "epoch:1 step:1505 [D loss: 0.493923, acc.: 82.81%] [G loss: 1.655137]\n",
      "epoch:1 step:1506 [D loss: 0.579446, acc.: 67.97%] [G loss: 1.214633]\n",
      "epoch:1 step:1507 [D loss: 0.517768, acc.: 77.34%] [G loss: 1.585396]\n",
      "epoch:1 step:1508 [D loss: 0.587512, acc.: 70.31%] [G loss: 1.332155]\n",
      "epoch:1 step:1509 [D loss: 0.584927, acc.: 74.22%] [G loss: 1.423804]\n",
      "epoch:1 step:1510 [D loss: 0.523376, acc.: 76.56%] [G loss: 1.699802]\n",
      "epoch:1 step:1511 [D loss: 0.572903, acc.: 68.75%] [G loss: 2.008605]\n",
      "epoch:1 step:1512 [D loss: 0.556189, acc.: 67.97%] [G loss: 1.858404]\n",
      "epoch:1 step:1513 [D loss: 0.526862, acc.: 76.56%] [G loss: 1.583754]\n",
      "epoch:1 step:1514 [D loss: 0.532071, acc.: 78.91%] [G loss: 1.818233]\n",
      "epoch:1 step:1515 [D loss: 0.510087, acc.: 78.12%] [G loss: 1.969128]\n",
      "epoch:1 step:1516 [D loss: 0.668893, acc.: 50.78%] [G loss: 1.725752]\n",
      "epoch:1 step:1517 [D loss: 0.643367, acc.: 62.50%] [G loss: 1.423003]\n",
      "epoch:1 step:1518 [D loss: 0.508519, acc.: 82.81%] [G loss: 1.814071]\n",
      "epoch:1 step:1519 [D loss: 0.593841, acc.: 65.62%] [G loss: 2.143051]\n",
      "epoch:1 step:1520 [D loss: 0.673353, acc.: 57.03%] [G loss: 1.488603]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:1 step:1521 [D loss: 0.463777, acc.: 82.03%] [G loss: 1.612555]\n",
      "epoch:1 step:1522 [D loss: 0.566302, acc.: 76.56%] [G loss: 1.435734]\n",
      "epoch:1 step:1523 [D loss: 0.519087, acc.: 70.31%] [G loss: 1.609490]\n",
      "epoch:1 step:1524 [D loss: 0.532457, acc.: 77.34%] [G loss: 1.356647]\n",
      "epoch:1 step:1525 [D loss: 0.520072, acc.: 76.56%] [G loss: 1.591662]\n",
      "epoch:1 step:1526 [D loss: 0.502864, acc.: 77.34%] [G loss: 1.812870]\n",
      "epoch:1 step:1527 [D loss: 0.558205, acc.: 75.00%] [G loss: 1.697464]\n",
      "epoch:1 step:1528 [D loss: 0.556548, acc.: 75.78%] [G loss: 1.724455]\n",
      "epoch:1 step:1529 [D loss: 0.440883, acc.: 85.16%] [G loss: 1.860466]\n",
      "epoch:1 step:1530 [D loss: 0.504936, acc.: 79.69%] [G loss: 1.410256]\n",
      "epoch:1 step:1531 [D loss: 0.570730, acc.: 72.66%] [G loss: 1.362355]\n",
      "epoch:1 step:1532 [D loss: 0.571343, acc.: 64.06%] [G loss: 2.405459]\n",
      "epoch:1 step:1533 [D loss: 0.738950, acc.: 57.03%] [G loss: 2.588340]\n",
      "epoch:1 step:1534 [D loss: 0.771668, acc.: 51.56%] [G loss: 2.500391]\n",
      "epoch:1 step:1535 [D loss: 0.509752, acc.: 76.56%] [G loss: 1.645654]\n",
      "epoch:1 step:1536 [D loss: 0.527691, acc.: 71.88%] [G loss: 1.752279]\n",
      "epoch:1 step:1537 [D loss: 0.471478, acc.: 82.03%] [G loss: 1.666172]\n",
      "epoch:1 step:1538 [D loss: 0.571147, acc.: 69.53%] [G loss: 1.294226]\n",
      "epoch:1 step:1539 [D loss: 0.503602, acc.: 82.03%] [G loss: 1.701414]\n",
      "epoch:1 step:1540 [D loss: 0.584324, acc.: 67.19%] [G loss: 1.623078]\n",
      "epoch:1 step:1541 [D loss: 0.628242, acc.: 67.97%] [G loss: 1.432480]\n",
      "epoch:1 step:1542 [D loss: 0.535475, acc.: 72.66%] [G loss: 1.705926]\n",
      "epoch:1 step:1543 [D loss: 0.557287, acc.: 76.56%] [G loss: 1.447935]\n",
      "epoch:1 step:1544 [D loss: 0.581626, acc.: 69.53%] [G loss: 1.460517]\n",
      "epoch:1 step:1545 [D loss: 0.501864, acc.: 84.38%] [G loss: 1.580305]\n",
      "epoch:1 step:1546 [D loss: 0.609134, acc.: 62.50%] [G loss: 1.565986]\n",
      "epoch:1 step:1547 [D loss: 0.496750, acc.: 75.78%] [G loss: 1.925924]\n",
      "epoch:1 step:1548 [D loss: 0.478981, acc.: 83.59%] [G loss: 1.595529]\n",
      "epoch:1 step:1549 [D loss: 0.539045, acc.: 74.22%] [G loss: 1.435281]\n",
      "epoch:1 step:1550 [D loss: 0.546767, acc.: 75.00%] [G loss: 1.700290]\n",
      "epoch:1 step:1551 [D loss: 0.596213, acc.: 68.75%] [G loss: 1.470957]\n",
      "epoch:1 step:1552 [D loss: 0.557988, acc.: 73.44%] [G loss: 1.383563]\n",
      "epoch:1 step:1553 [D loss: 0.592216, acc.: 68.75%] [G loss: 1.200941]\n",
      "epoch:1 step:1554 [D loss: 0.527586, acc.: 79.69%] [G loss: 1.369898]\n",
      "epoch:1 step:1555 [D loss: 0.531888, acc.: 76.56%] [G loss: 1.776833]\n",
      "epoch:1 step:1556 [D loss: 0.551580, acc.: 70.31%] [G loss: 2.228235]\n",
      "epoch:1 step:1557 [D loss: 0.555920, acc.: 78.91%] [G loss: 2.100552]\n",
      "epoch:1 step:1558 [D loss: 0.523741, acc.: 75.00%] [G loss: 1.598985]\n",
      "epoch:1 step:1559 [D loss: 0.501989, acc.: 74.22%] [G loss: 1.694288]\n",
      "epoch:1 step:1560 [D loss: 0.463922, acc.: 82.81%] [G loss: 1.503561]\n",
      "epoch:1 step:1561 [D loss: 0.520175, acc.: 74.22%] [G loss: 1.534439]\n",
      "epoch:1 step:1562 [D loss: 0.512577, acc.: 78.91%] [G loss: 1.818567]\n",
      "epoch:2 step:1563 [D loss: 0.503720, acc.: 76.56%] [G loss: 1.741599]\n",
      "epoch:2 step:1564 [D loss: 0.540387, acc.: 72.66%] [G loss: 1.600775]\n",
      "epoch:2 step:1565 [D loss: 0.504764, acc.: 73.44%] [G loss: 1.736773]\n",
      "epoch:2 step:1566 [D loss: 0.523524, acc.: 80.47%] [G loss: 1.704156]\n",
      "epoch:2 step:1567 [D loss: 0.604319, acc.: 70.31%] [G loss: 1.378501]\n",
      "epoch:2 step:1568 [D loss: 0.586346, acc.: 71.88%] [G loss: 1.407609]\n",
      "epoch:2 step:1569 [D loss: 0.554969, acc.: 75.78%] [G loss: 1.785250]\n",
      "epoch:2 step:1570 [D loss: 0.565631, acc.: 67.19%] [G loss: 2.640139]\n",
      "epoch:2 step:1571 [D loss: 0.559422, acc.: 64.84%] [G loss: 1.984107]\n",
      "epoch:2 step:1572 [D loss: 0.434265, acc.: 79.69%] [G loss: 2.243645]\n",
      "epoch:2 step:1573 [D loss: 0.546937, acc.: 71.88%] [G loss: 1.968560]\n",
      "epoch:2 step:1574 [D loss: 0.493398, acc.: 79.69%] [G loss: 1.426790]\n",
      "epoch:2 step:1575 [D loss: 0.556776, acc.: 68.75%] [G loss: 1.567253]\n",
      "epoch:2 step:1576 [D loss: 0.531476, acc.: 72.66%] [G loss: 2.380776]\n",
      "epoch:2 step:1577 [D loss: 0.634673, acc.: 64.06%] [G loss: 2.127419]\n",
      "epoch:2 step:1578 [D loss: 0.537469, acc.: 73.44%] [G loss: 2.549831]\n",
      "epoch:2 step:1579 [D loss: 0.501112, acc.: 78.12%] [G loss: 1.642942]\n",
      "epoch:2 step:1580 [D loss: 0.530895, acc.: 76.56%] [G loss: 1.602124]\n",
      "epoch:2 step:1581 [D loss: 0.485434, acc.: 82.03%] [G loss: 1.920054]\n",
      "epoch:2 step:1582 [D loss: 0.502430, acc.: 76.56%] [G loss: 1.559613]\n",
      "epoch:2 step:1583 [D loss: 0.512964, acc.: 79.69%] [G loss: 1.501307]\n",
      "epoch:2 step:1584 [D loss: 0.557397, acc.: 76.56%] [G loss: 1.575469]\n",
      "epoch:2 step:1585 [D loss: 0.564167, acc.: 74.22%] [G loss: 1.774342]\n",
      "epoch:2 step:1586 [D loss: 0.538731, acc.: 71.09%] [G loss: 2.156245]\n",
      "epoch:2 step:1587 [D loss: 0.565341, acc.: 73.44%] [G loss: 1.577785]\n",
      "epoch:2 step:1588 [D loss: 0.551002, acc.: 69.53%] [G loss: 1.916410]\n",
      "epoch:2 step:1589 [D loss: 0.524836, acc.: 76.56%] [G loss: 1.817457]\n",
      "epoch:2 step:1590 [D loss: 0.591997, acc.: 67.97%] [G loss: 2.367061]\n",
      "epoch:2 step:1591 [D loss: 0.605077, acc.: 65.62%] [G loss: 2.682661]\n",
      "epoch:2 step:1592 [D loss: 0.616917, acc.: 62.50%] [G loss: 1.811980]\n",
      "epoch:2 step:1593 [D loss: 0.446285, acc.: 84.38%] [G loss: 2.140943]\n",
      "epoch:2 step:1594 [D loss: 0.621870, acc.: 60.16%] [G loss: 1.726396]\n",
      "epoch:2 step:1595 [D loss: 0.507709, acc.: 82.81%] [G loss: 1.507170]\n",
      "epoch:2 step:1596 [D loss: 0.554954, acc.: 76.56%] [G loss: 1.785450]\n",
      "epoch:2 step:1597 [D loss: 0.625776, acc.: 63.28%] [G loss: 1.477960]\n",
      "epoch:2 step:1598 [D loss: 0.582296, acc.: 64.06%] [G loss: 1.535954]\n",
      "epoch:2 step:1599 [D loss: 0.519813, acc.: 81.25%] [G loss: 2.170741]\n",
      "epoch:2 step:1600 [D loss: 0.542262, acc.: 74.22%] [G loss: 1.999508]\n",
      "##############\n",
      "[0.85581253 0.80125659 0.79816122 0.84531845 0.77501114 0.84081413\n",
      " 0.88775296 0.79825823 0.80825781 0.81894828]\n",
      "##########\n",
      "epoch:2 step:1601 [D loss: 0.660275, acc.: 67.19%] [G loss: 2.331199]\n",
      "epoch:2 step:1602 [D loss: 0.536795, acc.: 71.09%] [G loss: 1.507991]\n",
      "epoch:2 step:1603 [D loss: 0.446933, acc.: 82.81%] [G loss: 1.704579]\n",
      "epoch:2 step:1604 [D loss: 0.523488, acc.: 76.56%] [G loss: 1.958467]\n",
      "epoch:2 step:1605 [D loss: 0.509259, acc.: 82.81%] [G loss: 1.567696]\n",
      "epoch:2 step:1606 [D loss: 0.534892, acc.: 78.91%] [G loss: 1.504930]\n",
      "epoch:2 step:1607 [D loss: 0.454524, acc.: 86.72%] [G loss: 2.103904]\n",
      "epoch:2 step:1608 [D loss: 0.489024, acc.: 82.03%] [G loss: 1.786477]\n",
      "epoch:2 step:1609 [D loss: 0.448329, acc.: 82.81%] [G loss: 2.262829]\n",
      "epoch:2 step:1610 [D loss: 0.472517, acc.: 82.81%] [G loss: 1.680365]\n",
      "epoch:2 step:1611 [D loss: 0.527348, acc.: 75.78%] [G loss: 1.730551]\n",
      "epoch:2 step:1612 [D loss: 0.507375, acc.: 76.56%] [G loss: 2.030464]\n",
      "epoch:2 step:1613 [D loss: 0.475522, acc.: 82.03%] [G loss: 2.085918]\n",
      "epoch:2 step:1614 [D loss: 0.483106, acc.: 82.03%] [G loss: 2.026469]\n",
      "epoch:2 step:1615 [D loss: 0.611074, acc.: 70.31%] [G loss: 1.478580]\n",
      "epoch:2 step:1616 [D loss: 0.520553, acc.: 77.34%] [G loss: 2.027744]\n",
      "epoch:2 step:1617 [D loss: 0.593942, acc.: 71.88%] [G loss: 2.534479]\n",
      "epoch:2 step:1618 [D loss: 0.705856, acc.: 50.00%] [G loss: 2.305231]\n",
      "epoch:2 step:1619 [D loss: 0.706607, acc.: 61.72%] [G loss: 1.542513]\n",
      "epoch:2 step:1620 [D loss: 0.492491, acc.: 78.12%] [G loss: 1.906372]\n",
      "epoch:2 step:1621 [D loss: 0.570478, acc.: 73.44%] [G loss: 1.480198]\n",
      "epoch:2 step:1622 [D loss: 0.585427, acc.: 69.53%] [G loss: 1.771920]\n",
      "epoch:2 step:1623 [D loss: 0.466186, acc.: 80.47%] [G loss: 2.447650]\n",
      "epoch:2 step:1624 [D loss: 0.391531, acc.: 85.16%] [G loss: 2.237631]\n",
      "epoch:2 step:1625 [D loss: 0.546818, acc.: 72.66%] [G loss: 1.750318]\n",
      "epoch:2 step:1626 [D loss: 0.473923, acc.: 82.03%] [G loss: 2.095788]\n",
      "epoch:2 step:1627 [D loss: 0.466340, acc.: 82.81%] [G loss: 2.218308]\n",
      "epoch:2 step:1628 [D loss: 0.556952, acc.: 77.34%] [G loss: 1.722228]\n",
      "epoch:2 step:1629 [D loss: 0.506496, acc.: 76.56%] [G loss: 2.043824]\n",
      "epoch:2 step:1630 [D loss: 0.546424, acc.: 68.75%] [G loss: 1.784578]\n",
      "epoch:2 step:1631 [D loss: 0.580116, acc.: 76.56%] [G loss: 1.727116]\n",
      "epoch:2 step:1632 [D loss: 0.526029, acc.: 78.12%] [G loss: 1.901527]\n",
      "epoch:2 step:1633 [D loss: 0.490577, acc.: 71.88%] [G loss: 2.004543]\n",
      "epoch:2 step:1634 [D loss: 0.610277, acc.: 77.34%] [G loss: 2.627603]\n",
      "epoch:2 step:1635 [D loss: 0.676155, acc.: 64.06%] [G loss: 2.996893]\n",
      "epoch:2 step:1636 [D loss: 0.664775, acc.: 60.16%] [G loss: 2.227763]\n",
      "epoch:2 step:1637 [D loss: 0.594086, acc.: 65.62%] [G loss: 1.592534]\n",
      "epoch:2 step:1638 [D loss: 0.561137, acc.: 75.78%] [G loss: 1.933775]\n",
      "epoch:2 step:1639 [D loss: 0.631861, acc.: 64.06%] [G loss: 1.562575]\n",
      "epoch:2 step:1640 [D loss: 0.440005, acc.: 85.94%] [G loss: 1.633389]\n",
      "epoch:2 step:1641 [D loss: 0.592868, acc.: 59.38%] [G loss: 1.805982]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:2 step:1642 [D loss: 0.570239, acc.: 67.97%] [G loss: 1.644340]\n",
      "epoch:2 step:1643 [D loss: 0.471587, acc.: 83.59%] [G loss: 2.189779]\n",
      "epoch:2 step:1644 [D loss: 0.521180, acc.: 78.12%] [G loss: 2.084416]\n",
      "epoch:2 step:1645 [D loss: 0.501877, acc.: 79.69%] [G loss: 1.589940]\n",
      "epoch:2 step:1646 [D loss: 0.490753, acc.: 82.03%] [G loss: 1.856864]\n",
      "epoch:2 step:1647 [D loss: 0.539411, acc.: 73.44%] [G loss: 1.696028]\n",
      "epoch:2 step:1648 [D loss: 0.477833, acc.: 85.16%] [G loss: 1.760284]\n",
      "epoch:2 step:1649 [D loss: 0.505888, acc.: 78.12%] [G loss: 1.811492]\n",
      "epoch:2 step:1650 [D loss: 0.504652, acc.: 76.56%] [G loss: 1.537632]\n",
      "epoch:2 step:1651 [D loss: 0.477349, acc.: 82.03%] [G loss: 1.924586]\n",
      "epoch:2 step:1652 [D loss: 0.518888, acc.: 72.66%] [G loss: 2.579980]\n",
      "epoch:2 step:1653 [D loss: 0.567202, acc.: 73.44%] [G loss: 1.963009]\n",
      "epoch:2 step:1654 [D loss: 0.556979, acc.: 72.66%] [G loss: 1.583765]\n",
      "epoch:2 step:1655 [D loss: 0.475669, acc.: 82.81%] [G loss: 1.950122]\n",
      "epoch:2 step:1656 [D loss: 0.501109, acc.: 85.16%] [G loss: 1.760587]\n",
      "epoch:2 step:1657 [D loss: 0.483586, acc.: 78.91%] [G loss: 2.475700]\n",
      "epoch:2 step:1658 [D loss: 0.614542, acc.: 59.38%] [G loss: 2.179352]\n",
      "epoch:2 step:1659 [D loss: 0.618699, acc.: 60.94%] [G loss: 2.096045]\n",
      "epoch:2 step:1660 [D loss: 0.605664, acc.: 67.19%] [G loss: 1.688803]\n",
      "epoch:2 step:1661 [D loss: 0.497788, acc.: 79.69%] [G loss: 1.722524]\n",
      "epoch:2 step:1662 [D loss: 0.601742, acc.: 66.41%] [G loss: 1.844534]\n",
      "epoch:2 step:1663 [D loss: 0.594441, acc.: 68.75%] [G loss: 1.799889]\n",
      "epoch:2 step:1664 [D loss: 0.561419, acc.: 64.06%] [G loss: 2.117179]\n",
      "epoch:2 step:1665 [D loss: 0.514183, acc.: 80.47%] [G loss: 1.716447]\n",
      "epoch:2 step:1666 [D loss: 0.454986, acc.: 82.81%] [G loss: 2.181475]\n",
      "epoch:2 step:1667 [D loss: 0.562850, acc.: 73.44%] [G loss: 1.944594]\n",
      "epoch:2 step:1668 [D loss: 0.480591, acc.: 74.22%] [G loss: 2.414453]\n",
      "epoch:2 step:1669 [D loss: 0.521643, acc.: 80.47%] [G loss: 1.698363]\n",
      "epoch:2 step:1670 [D loss: 0.463090, acc.: 89.84%] [G loss: 2.098389]\n",
      "epoch:2 step:1671 [D loss: 0.481436, acc.: 74.22%] [G loss: 1.833758]\n",
      "epoch:2 step:1672 [D loss: 0.522036, acc.: 78.91%] [G loss: 1.829389]\n",
      "epoch:2 step:1673 [D loss: 0.554596, acc.: 71.09%] [G loss: 1.765661]\n",
      "epoch:2 step:1674 [D loss: 0.527138, acc.: 71.88%] [G loss: 2.090238]\n",
      "epoch:2 step:1675 [D loss: 0.520654, acc.: 72.66%] [G loss: 1.835595]\n",
      "epoch:2 step:1676 [D loss: 0.486378, acc.: 78.91%] [G loss: 2.047164]\n",
      "epoch:2 step:1677 [D loss: 0.508423, acc.: 77.34%] [G loss: 1.776049]\n",
      "epoch:2 step:1678 [D loss: 0.516591, acc.: 78.91%] [G loss: 1.743058]\n",
      "epoch:2 step:1679 [D loss: 0.461723, acc.: 80.47%] [G loss: 1.899280]\n",
      "epoch:2 step:1680 [D loss: 0.513459, acc.: 75.00%] [G loss: 1.573315]\n",
      "epoch:2 step:1681 [D loss: 0.500978, acc.: 78.12%] [G loss: 2.747511]\n",
      "epoch:2 step:1682 [D loss: 0.491740, acc.: 82.03%] [G loss: 2.768338]\n",
      "epoch:2 step:1683 [D loss: 0.444647, acc.: 84.38%] [G loss: 1.987918]\n",
      "epoch:2 step:1684 [D loss: 0.428834, acc.: 85.16%] [G loss: 1.978117]\n",
      "epoch:2 step:1685 [D loss: 0.432966, acc.: 84.38%] [G loss: 2.360938]\n",
      "epoch:2 step:1686 [D loss: 0.497924, acc.: 83.59%] [G loss: 2.265248]\n",
      "epoch:2 step:1687 [D loss: 0.736459, acc.: 65.62%] [G loss: 3.003300]\n",
      "epoch:2 step:1688 [D loss: 0.781679, acc.: 53.91%] [G loss: 3.422731]\n",
      "epoch:2 step:1689 [D loss: 0.538628, acc.: 72.66%] [G loss: 1.988312]\n",
      "epoch:2 step:1690 [D loss: 0.503106, acc.: 79.69%] [G loss: 2.162147]\n",
      "epoch:2 step:1691 [D loss: 0.522078, acc.: 78.91%] [G loss: 2.291659]\n",
      "epoch:2 step:1692 [D loss: 0.410974, acc.: 87.50%] [G loss: 2.409760]\n",
      "epoch:2 step:1693 [D loss: 0.448988, acc.: 80.47%] [G loss: 1.875289]\n",
      "epoch:2 step:1694 [D loss: 0.508513, acc.: 80.47%] [G loss: 2.052336]\n",
      "epoch:2 step:1695 [D loss: 0.489170, acc.: 80.47%] [G loss: 1.410787]\n",
      "epoch:2 step:1696 [D loss: 0.496254, acc.: 81.25%] [G loss: 2.254978]\n",
      "epoch:2 step:1697 [D loss: 0.447808, acc.: 81.25%] [G loss: 2.308175]\n",
      "epoch:2 step:1698 [D loss: 0.441245, acc.: 81.25%] [G loss: 1.967323]\n",
      "epoch:2 step:1699 [D loss: 0.377048, acc.: 91.41%] [G loss: 2.419076]\n",
      "epoch:2 step:1700 [D loss: 0.457628, acc.: 80.47%] [G loss: 2.290339]\n",
      "epoch:2 step:1701 [D loss: 0.430486, acc.: 82.03%] [G loss: 2.257786]\n",
      "epoch:2 step:1702 [D loss: 0.449265, acc.: 84.38%] [G loss: 2.100630]\n",
      "epoch:2 step:1703 [D loss: 0.427189, acc.: 89.06%] [G loss: 1.729041]\n",
      "epoch:2 step:1704 [D loss: 0.444578, acc.: 80.47%] [G loss: 2.111024]\n",
      "epoch:2 step:1705 [D loss: 0.422936, acc.: 84.38%] [G loss: 2.451406]\n",
      "epoch:2 step:1706 [D loss: 0.420520, acc.: 83.59%] [G loss: 2.280397]\n",
      "epoch:2 step:1707 [D loss: 0.411361, acc.: 88.28%] [G loss: 2.238013]\n",
      "epoch:2 step:1708 [D loss: 0.399315, acc.: 89.84%] [G loss: 2.660486]\n",
      "epoch:2 step:1709 [D loss: 0.417074, acc.: 85.94%] [G loss: 2.428956]\n",
      "epoch:2 step:1710 [D loss: 0.432589, acc.: 88.28%] [G loss: 2.179049]\n",
      "epoch:2 step:1711 [D loss: 0.411665, acc.: 83.59%] [G loss: 2.172148]\n",
      "epoch:2 step:1712 [D loss: 0.458432, acc.: 75.78%] [G loss: 3.106534]\n",
      "epoch:2 step:1713 [D loss: 0.414086, acc.: 82.81%] [G loss: 3.359101]\n",
      "epoch:2 step:1714 [D loss: 0.451139, acc.: 79.69%] [G loss: 2.303671]\n",
      "epoch:2 step:1715 [D loss: 0.415370, acc.: 81.25%] [G loss: 2.429391]\n",
      "epoch:2 step:1716 [D loss: 0.422706, acc.: 84.38%] [G loss: 2.740291]\n",
      "epoch:2 step:1717 [D loss: 0.507082, acc.: 80.47%] [G loss: 1.790101]\n",
      "epoch:2 step:1718 [D loss: 0.553453, acc.: 77.34%] [G loss: 1.656124]\n",
      "epoch:2 step:1719 [D loss: 0.499595, acc.: 75.78%] [G loss: 2.890814]\n",
      "epoch:2 step:1720 [D loss: 0.520583, acc.: 75.78%] [G loss: 3.678972]\n",
      "epoch:2 step:1721 [D loss: 0.572161, acc.: 74.22%] [G loss: 2.212266]\n",
      "epoch:2 step:1722 [D loss: 0.656819, acc.: 64.06%] [G loss: 2.106237]\n",
      "epoch:2 step:1723 [D loss: 0.590243, acc.: 65.62%] [G loss: 2.317748]\n",
      "epoch:2 step:1724 [D loss: 0.453878, acc.: 84.38%] [G loss: 1.945264]\n",
      "epoch:2 step:1725 [D loss: 0.458743, acc.: 77.34%] [G loss: 1.830330]\n",
      "epoch:2 step:1726 [D loss: 0.505395, acc.: 75.78%] [G loss: 2.149621]\n",
      "epoch:2 step:1727 [D loss: 0.440575, acc.: 87.50%] [G loss: 2.561817]\n",
      "epoch:2 step:1728 [D loss: 0.468028, acc.: 77.34%] [G loss: 2.733006]\n",
      "epoch:2 step:1729 [D loss: 0.456142, acc.: 82.03%] [G loss: 2.394724]\n",
      "epoch:2 step:1730 [D loss: 0.462605, acc.: 81.25%] [G loss: 1.812201]\n",
      "epoch:2 step:1731 [D loss: 0.428783, acc.: 85.16%] [G loss: 2.177043]\n",
      "epoch:2 step:1732 [D loss: 0.554271, acc.: 74.22%] [G loss: 1.833772]\n",
      "epoch:2 step:1733 [D loss: 0.490104, acc.: 80.47%] [G loss: 1.708234]\n",
      "epoch:2 step:1734 [D loss: 0.413239, acc.: 85.94%] [G loss: 1.828269]\n",
      "epoch:2 step:1735 [D loss: 0.462791, acc.: 83.59%] [G loss: 2.072647]\n",
      "epoch:2 step:1736 [D loss: 0.484672, acc.: 73.44%] [G loss: 2.164827]\n",
      "epoch:2 step:1737 [D loss: 0.465128, acc.: 79.69%] [G loss: 1.834136]\n",
      "epoch:2 step:1738 [D loss: 0.420505, acc.: 83.59%] [G loss: 2.106525]\n",
      "epoch:2 step:1739 [D loss: 0.413263, acc.: 83.59%] [G loss: 2.453351]\n",
      "epoch:2 step:1740 [D loss: 0.519303, acc.: 74.22%] [G loss: 2.157205]\n",
      "epoch:2 step:1741 [D loss: 0.477305, acc.: 76.56%] [G loss: 2.758235]\n",
      "epoch:2 step:1742 [D loss: 0.534080, acc.: 75.00%] [G loss: 2.210876]\n",
      "epoch:2 step:1743 [D loss: 0.497694, acc.: 80.47%] [G loss: 2.120912]\n",
      "epoch:2 step:1744 [D loss: 0.459735, acc.: 79.69%] [G loss: 2.525385]\n",
      "epoch:2 step:1745 [D loss: 0.499605, acc.: 74.22%] [G loss: 2.424489]\n",
      "epoch:2 step:1746 [D loss: 0.441013, acc.: 78.12%] [G loss: 2.560892]\n",
      "epoch:2 step:1747 [D loss: 0.388932, acc.: 86.72%] [G loss: 2.223009]\n",
      "epoch:2 step:1748 [D loss: 0.336797, acc.: 92.19%] [G loss: 2.493945]\n",
      "epoch:2 step:1749 [D loss: 0.483830, acc.: 77.34%] [G loss: 1.721678]\n",
      "epoch:2 step:1750 [D loss: 0.396382, acc.: 84.38%] [G loss: 2.682726]\n",
      "epoch:2 step:1751 [D loss: 0.519522, acc.: 72.66%] [G loss: 1.859440]\n",
      "epoch:2 step:1752 [D loss: 0.476593, acc.: 82.81%] [G loss: 1.996599]\n",
      "epoch:2 step:1753 [D loss: 0.490098, acc.: 75.78%] [G loss: 2.160733]\n",
      "epoch:2 step:1754 [D loss: 0.432215, acc.: 87.50%] [G loss: 2.099420]\n",
      "epoch:2 step:1755 [D loss: 0.456139, acc.: 83.59%] [G loss: 1.945094]\n",
      "epoch:2 step:1756 [D loss: 0.474742, acc.: 78.12%] [G loss: 2.187430]\n",
      "epoch:2 step:1757 [D loss: 0.503104, acc.: 78.12%] [G loss: 1.925653]\n",
      "epoch:2 step:1758 [D loss: 0.434008, acc.: 83.59%] [G loss: 2.032634]\n",
      "epoch:2 step:1759 [D loss: 0.459493, acc.: 82.03%] [G loss: 1.908034]\n",
      "epoch:2 step:1760 [D loss: 0.502832, acc.: 76.56%] [G loss: 2.291671]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:2 step:1761 [D loss: 0.452967, acc.: 82.03%] [G loss: 1.778628]\n",
      "epoch:2 step:1762 [D loss: 0.529574, acc.: 72.66%] [G loss: 2.557997]\n",
      "epoch:2 step:1763 [D loss: 0.522162, acc.: 71.09%] [G loss: 2.152860]\n",
      "epoch:2 step:1764 [D loss: 0.404349, acc.: 80.47%] [G loss: 2.672889]\n",
      "epoch:2 step:1765 [D loss: 0.447809, acc.: 83.59%] [G loss: 1.864318]\n",
      "epoch:2 step:1766 [D loss: 0.436571, acc.: 83.59%] [G loss: 2.500278]\n",
      "epoch:2 step:1767 [D loss: 0.424104, acc.: 88.28%] [G loss: 2.307196]\n",
      "epoch:2 step:1768 [D loss: 0.438251, acc.: 82.81%] [G loss: 2.327948]\n",
      "epoch:2 step:1769 [D loss: 0.368766, acc.: 90.62%] [G loss: 2.660659]\n",
      "epoch:2 step:1770 [D loss: 0.470263, acc.: 79.69%] [G loss: 1.773604]\n",
      "epoch:2 step:1771 [D loss: 0.541491, acc.: 75.78%] [G loss: 1.738683]\n",
      "epoch:2 step:1772 [D loss: 0.417632, acc.: 82.81%] [G loss: 1.904053]\n",
      "epoch:2 step:1773 [D loss: 0.479083, acc.: 82.81%] [G loss: 2.124027]\n",
      "epoch:2 step:1774 [D loss: 0.455606, acc.: 79.69%] [G loss: 2.925550]\n",
      "epoch:2 step:1775 [D loss: 0.659856, acc.: 66.41%] [G loss: 2.755090]\n",
      "epoch:2 step:1776 [D loss: 0.621874, acc.: 64.84%] [G loss: 3.523952]\n",
      "epoch:2 step:1777 [D loss: 0.777971, acc.: 57.81%] [G loss: 2.977227]\n",
      "epoch:2 step:1778 [D loss: 0.515364, acc.: 75.78%] [G loss: 2.127720]\n",
      "epoch:2 step:1779 [D loss: 0.520335, acc.: 69.53%] [G loss: 1.847262]\n",
      "epoch:2 step:1780 [D loss: 0.487331, acc.: 78.91%] [G loss: 2.112499]\n",
      "epoch:2 step:1781 [D loss: 0.427559, acc.: 86.72%] [G loss: 2.052933]\n",
      "epoch:2 step:1782 [D loss: 0.523536, acc.: 77.34%] [G loss: 1.570438]\n",
      "epoch:2 step:1783 [D loss: 0.471890, acc.: 83.59%] [G loss: 1.984075]\n",
      "epoch:2 step:1784 [D loss: 0.478667, acc.: 78.91%] [G loss: 2.706235]\n",
      "epoch:2 step:1785 [D loss: 0.490371, acc.: 82.03%] [G loss: 1.904892]\n",
      "epoch:2 step:1786 [D loss: 0.480751, acc.: 82.81%] [G loss: 1.958327]\n",
      "epoch:2 step:1787 [D loss: 0.463787, acc.: 81.25%] [G loss: 2.229756]\n",
      "epoch:2 step:1788 [D loss: 0.487284, acc.: 82.03%] [G loss: 2.100240]\n",
      "epoch:2 step:1789 [D loss: 0.413033, acc.: 84.38%] [G loss: 1.976438]\n",
      "epoch:2 step:1790 [D loss: 0.412939, acc.: 86.72%] [G loss: 2.167146]\n",
      "epoch:2 step:1791 [D loss: 0.444513, acc.: 85.16%] [G loss: 2.030725]\n",
      "epoch:2 step:1792 [D loss: 0.443695, acc.: 83.59%] [G loss: 1.975616]\n",
      "epoch:2 step:1793 [D loss: 0.411159, acc.: 85.94%] [G loss: 2.374794]\n",
      "epoch:2 step:1794 [D loss: 0.437965, acc.: 83.59%] [G loss: 1.549135]\n",
      "epoch:2 step:1795 [D loss: 0.445335, acc.: 82.81%] [G loss: 2.248261]\n",
      "epoch:2 step:1796 [D loss: 0.392154, acc.: 85.94%] [G loss: 2.693136]\n",
      "epoch:2 step:1797 [D loss: 0.425832, acc.: 84.38%] [G loss: 2.297246]\n",
      "epoch:2 step:1798 [D loss: 0.528477, acc.: 75.00%] [G loss: 3.147010]\n",
      "epoch:2 step:1799 [D loss: 0.654812, acc.: 62.50%] [G loss: 2.477669]\n",
      "epoch:2 step:1800 [D loss: 0.557932, acc.: 75.78%] [G loss: 1.961418]\n",
      "##############\n",
      "[0.88212531 0.81125697 0.79008751 0.8192224  0.80845734 0.81246322\n",
      " 0.88134623 0.8004991  0.81587694 0.80442873]\n",
      "##########\n",
      "epoch:2 step:1801 [D loss: 0.473766, acc.: 79.69%] [G loss: 3.571756]\n",
      "epoch:2 step:1802 [D loss: 0.566960, acc.: 64.84%] [G loss: 2.205685]\n",
      "epoch:2 step:1803 [D loss: 0.445085, acc.: 79.69%] [G loss: 2.248414]\n",
      "epoch:2 step:1804 [D loss: 0.468704, acc.: 83.59%] [G loss: 2.258833]\n",
      "epoch:2 step:1805 [D loss: 0.387347, acc.: 85.94%] [G loss: 2.466503]\n",
      "epoch:2 step:1806 [D loss: 0.478568, acc.: 76.56%] [G loss: 2.207209]\n",
      "epoch:2 step:1807 [D loss: 0.513541, acc.: 78.12%] [G loss: 2.031663]\n",
      "epoch:2 step:1808 [D loss: 0.541139, acc.: 75.78%] [G loss: 2.404520]\n",
      "epoch:2 step:1809 [D loss: 0.440462, acc.: 81.25%] [G loss: 2.245568]\n",
      "epoch:2 step:1810 [D loss: 0.458323, acc.: 78.12%] [G loss: 1.939748]\n",
      "epoch:2 step:1811 [D loss: 0.427309, acc.: 85.16%] [G loss: 1.928024]\n",
      "epoch:2 step:1812 [D loss: 0.457517, acc.: 84.38%] [G loss: 1.979228]\n",
      "epoch:2 step:1813 [D loss: 0.380933, acc.: 89.06%] [G loss: 2.420464]\n",
      "epoch:2 step:1814 [D loss: 0.445727, acc.: 82.03%] [G loss: 2.199880]\n",
      "epoch:2 step:1815 [D loss: 0.470262, acc.: 79.69%] [G loss: 2.555606]\n",
      "epoch:2 step:1816 [D loss: 0.438983, acc.: 78.91%] [G loss: 2.927628]\n",
      "epoch:2 step:1817 [D loss: 0.404810, acc.: 86.72%] [G loss: 2.221445]\n",
      "epoch:2 step:1818 [D loss: 0.392620, acc.: 87.50%] [G loss: 2.341932]\n",
      "epoch:2 step:1819 [D loss: 0.405654, acc.: 83.59%] [G loss: 2.503509]\n",
      "epoch:2 step:1820 [D loss: 0.471931, acc.: 79.69%] [G loss: 1.640420]\n",
      "epoch:2 step:1821 [D loss: 0.376303, acc.: 92.97%] [G loss: 2.188453]\n",
      "epoch:2 step:1822 [D loss: 0.424756, acc.: 89.06%] [G loss: 2.129029]\n",
      "epoch:2 step:1823 [D loss: 0.449153, acc.: 78.91%] [G loss: 2.148382]\n",
      "epoch:2 step:1824 [D loss: 0.448172, acc.: 79.69%] [G loss: 2.410812]\n",
      "epoch:2 step:1825 [D loss: 0.493555, acc.: 78.12%] [G loss: 2.398358]\n",
      "epoch:2 step:1826 [D loss: 0.449223, acc.: 79.69%] [G loss: 2.051948]\n",
      "epoch:2 step:1827 [D loss: 0.401391, acc.: 89.84%] [G loss: 2.303929]\n",
      "epoch:2 step:1828 [D loss: 0.420571, acc.: 82.81%] [G loss: 3.194281]\n",
      "epoch:2 step:1829 [D loss: 0.572944, acc.: 70.31%] [G loss: 4.206064]\n",
      "epoch:2 step:1830 [D loss: 0.540206, acc.: 71.09%] [G loss: 2.740718]\n",
      "epoch:2 step:1831 [D loss: 0.601675, acc.: 62.50%] [G loss: 1.995458]\n",
      "epoch:2 step:1832 [D loss: 0.464675, acc.: 80.47%] [G loss: 2.772261]\n",
      "epoch:2 step:1833 [D loss: 0.523971, acc.: 75.78%] [G loss: 2.566012]\n",
      "epoch:2 step:1834 [D loss: 0.333166, acc.: 94.53%] [G loss: 2.542479]\n",
      "epoch:2 step:1835 [D loss: 0.496070, acc.: 78.12%] [G loss: 2.161706]\n",
      "epoch:2 step:1836 [D loss: 0.397693, acc.: 92.19%] [G loss: 1.740675]\n",
      "epoch:2 step:1837 [D loss: 0.572825, acc.: 71.88%] [G loss: 2.563854]\n",
      "epoch:2 step:1838 [D loss: 0.630928, acc.: 63.28%] [G loss: 3.056215]\n",
      "epoch:2 step:1839 [D loss: 0.479875, acc.: 78.91%] [G loss: 2.132889]\n",
      "epoch:2 step:1840 [D loss: 0.356482, acc.: 89.06%] [G loss: 2.396039]\n",
      "epoch:2 step:1841 [D loss: 0.426768, acc.: 84.38%] [G loss: 2.073253]\n",
      "epoch:2 step:1842 [D loss: 0.475520, acc.: 78.12%] [G loss: 1.915332]\n",
      "epoch:2 step:1843 [D loss: 0.480456, acc.: 81.25%] [G loss: 1.565798]\n",
      "epoch:2 step:1844 [D loss: 0.465710, acc.: 85.16%] [G loss: 2.326738]\n",
      "epoch:2 step:1845 [D loss: 0.444645, acc.: 86.72%] [G loss: 1.722799]\n",
      "epoch:2 step:1846 [D loss: 0.418106, acc.: 87.50%] [G loss: 2.353681]\n",
      "epoch:2 step:1847 [D loss: 0.377485, acc.: 84.38%] [G loss: 2.577814]\n",
      "epoch:2 step:1848 [D loss: 0.447960, acc.: 78.91%] [G loss: 2.071420]\n",
      "epoch:2 step:1849 [D loss: 0.332835, acc.: 89.84%] [G loss: 2.323891]\n",
      "epoch:2 step:1850 [D loss: 0.407724, acc.: 84.38%] [G loss: 1.823219]\n",
      "epoch:2 step:1851 [D loss: 0.408075, acc.: 85.94%] [G loss: 1.899144]\n",
      "epoch:2 step:1852 [D loss: 0.369111, acc.: 90.62%] [G loss: 2.558781]\n",
      "epoch:2 step:1853 [D loss: 0.445081, acc.: 89.06%] [G loss: 2.431743]\n",
      "epoch:2 step:1854 [D loss: 0.427719, acc.: 81.25%] [G loss: 2.553928]\n",
      "epoch:2 step:1855 [D loss: 0.457060, acc.: 82.81%] [G loss: 2.275516]\n",
      "epoch:2 step:1856 [D loss: 0.498614, acc.: 76.56%] [G loss: 1.577858]\n",
      "epoch:2 step:1857 [D loss: 0.430141, acc.: 84.38%] [G loss: 1.999344]\n",
      "epoch:2 step:1858 [D loss: 0.444788, acc.: 85.94%] [G loss: 1.821812]\n",
      "epoch:2 step:1859 [D loss: 0.348769, acc.: 90.62%] [G loss: 2.714567]\n",
      "epoch:2 step:1860 [D loss: 0.485666, acc.: 79.69%] [G loss: 1.580183]\n",
      "epoch:2 step:1861 [D loss: 0.400800, acc.: 85.94%] [G loss: 2.268850]\n",
      "epoch:2 step:1862 [D loss: 0.492983, acc.: 77.34%] [G loss: 2.783066]\n",
      "epoch:2 step:1863 [D loss: 0.586841, acc.: 67.19%] [G loss: 4.636733]\n",
      "epoch:2 step:1864 [D loss: 0.771561, acc.: 65.62%] [G loss: 4.041108]\n",
      "epoch:2 step:1865 [D loss: 0.841365, acc.: 60.16%] [G loss: 2.040847]\n",
      "epoch:2 step:1866 [D loss: 0.466062, acc.: 79.69%] [G loss: 1.861239]\n",
      "epoch:2 step:1867 [D loss: 0.481869, acc.: 80.47%] [G loss: 1.978448]\n",
      "epoch:2 step:1868 [D loss: 0.468887, acc.: 79.69%] [G loss: 1.607654]\n",
      "epoch:2 step:1869 [D loss: 0.382632, acc.: 88.28%] [G loss: 2.175087]\n",
      "epoch:2 step:1870 [D loss: 0.465855, acc.: 75.00%] [G loss: 2.092759]\n",
      "epoch:2 step:1871 [D loss: 0.455307, acc.: 79.69%] [G loss: 1.871707]\n",
      "epoch:2 step:1872 [D loss: 0.418585, acc.: 88.28%] [G loss: 2.105344]\n",
      "epoch:2 step:1873 [D loss: 0.416403, acc.: 83.59%] [G loss: 1.754460]\n",
      "epoch:2 step:1874 [D loss: 0.414542, acc.: 84.38%] [G loss: 2.428705]\n",
      "epoch:2 step:1875 [D loss: 0.436040, acc.: 85.94%] [G loss: 1.939163]\n",
      "epoch:2 step:1876 [D loss: 0.391627, acc.: 86.72%] [G loss: 2.202323]\n",
      "epoch:2 step:1877 [D loss: 0.595992, acc.: 67.19%] [G loss: 2.313146]\n",
      "epoch:2 step:1878 [D loss: 0.548592, acc.: 73.44%] [G loss: 3.667887]\n",
      "epoch:2 step:1879 [D loss: 0.501424, acc.: 80.47%] [G loss: 1.680071]\n",
      "epoch:2 step:1880 [D loss: 0.457582, acc.: 79.69%] [G loss: 1.594920]\n",
      "epoch:2 step:1881 [D loss: 0.521841, acc.: 75.78%] [G loss: 1.617256]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:2 step:1882 [D loss: 0.504873, acc.: 75.00%] [G loss: 1.999324]\n",
      "epoch:2 step:1883 [D loss: 0.513446, acc.: 79.69%] [G loss: 2.244904]\n",
      "epoch:2 step:1884 [D loss: 0.511717, acc.: 75.78%] [G loss: 2.124813]\n",
      "epoch:2 step:1885 [D loss: 0.458182, acc.: 83.59%] [G loss: 1.802075]\n",
      "epoch:2 step:1886 [D loss: 0.454822, acc.: 82.81%] [G loss: 2.197502]\n",
      "epoch:2 step:1887 [D loss: 0.438660, acc.: 82.81%] [G loss: 2.156052]\n",
      "epoch:2 step:1888 [D loss: 0.534456, acc.: 78.91%] [G loss: 2.428953]\n",
      "epoch:2 step:1889 [D loss: 0.457050, acc.: 79.69%] [G loss: 2.493789]\n",
      "epoch:2 step:1890 [D loss: 0.483214, acc.: 75.78%] [G loss: 1.812407]\n",
      "epoch:2 step:1891 [D loss: 0.525966, acc.: 81.25%] [G loss: 1.492198]\n",
      "epoch:2 step:1892 [D loss: 0.398463, acc.: 85.94%] [G loss: 2.639588]\n",
      "epoch:2 step:1893 [D loss: 0.525750, acc.: 71.09%] [G loss: 2.810363]\n",
      "epoch:2 step:1894 [D loss: 0.492966, acc.: 79.69%] [G loss: 2.076513]\n",
      "epoch:2 step:1895 [D loss: 0.418444, acc.: 84.38%] [G loss: 2.405258]\n",
      "epoch:2 step:1896 [D loss: 0.408140, acc.: 82.81%] [G loss: 2.460561]\n",
      "epoch:2 step:1897 [D loss: 0.440390, acc.: 81.25%] [G loss: 2.162549]\n",
      "epoch:2 step:1898 [D loss: 0.390675, acc.: 86.72%] [G loss: 2.820507]\n",
      "epoch:2 step:1899 [D loss: 0.428778, acc.: 85.16%] [G loss: 2.046218]\n",
      "epoch:2 step:1900 [D loss: 0.455999, acc.: 82.03%] [G loss: 1.872043]\n",
      "epoch:2 step:1901 [D loss: 0.506166, acc.: 80.47%] [G loss: 2.302641]\n",
      "epoch:2 step:1902 [D loss: 0.559650, acc.: 67.97%] [G loss: 2.639922]\n",
      "epoch:2 step:1903 [D loss: 0.648280, acc.: 67.19%] [G loss: 2.083075]\n",
      "epoch:2 step:1904 [D loss: 0.397917, acc.: 84.38%] [G loss: 2.367543]\n",
      "epoch:2 step:1905 [D loss: 0.534617, acc.: 72.66%] [G loss: 2.243432]\n",
      "epoch:2 step:1906 [D loss: 0.481488, acc.: 82.03%] [G loss: 1.732600]\n",
      "epoch:2 step:1907 [D loss: 0.418995, acc.: 82.81%] [G loss: 1.916745]\n",
      "epoch:2 step:1908 [D loss: 0.425829, acc.: 80.47%] [G loss: 2.032043]\n",
      "epoch:2 step:1909 [D loss: 0.398426, acc.: 85.16%] [G loss: 2.298810]\n",
      "epoch:2 step:1910 [D loss: 0.391843, acc.: 86.72%] [G loss: 2.461568]\n",
      "epoch:2 step:1911 [D loss: 0.490228, acc.: 71.88%] [G loss: 2.129293]\n",
      "epoch:2 step:1912 [D loss: 0.420228, acc.: 83.59%] [G loss: 2.419082]\n",
      "epoch:2 step:1913 [D loss: 0.542087, acc.: 72.66%] [G loss: 2.324852]\n",
      "epoch:2 step:1914 [D loss: 0.438683, acc.: 84.38%] [G loss: 2.496965]\n",
      "epoch:2 step:1915 [D loss: 0.475013, acc.: 78.12%] [G loss: 3.537932]\n",
      "epoch:2 step:1916 [D loss: 0.473090, acc.: 78.12%] [G loss: 2.784232]\n",
      "epoch:2 step:1917 [D loss: 0.498395, acc.: 77.34%] [G loss: 1.825516]\n",
      "epoch:2 step:1918 [D loss: 0.530592, acc.: 69.53%] [G loss: 1.570669]\n",
      "epoch:2 step:1919 [D loss: 0.407240, acc.: 89.84%] [G loss: 2.872696]\n",
      "epoch:2 step:1920 [D loss: 0.468217, acc.: 78.12%] [G loss: 2.107316]\n",
      "epoch:2 step:1921 [D loss: 0.446047, acc.: 82.03%] [G loss: 2.156858]\n",
      "epoch:2 step:1922 [D loss: 0.449263, acc.: 82.81%] [G loss: 1.988485]\n",
      "epoch:2 step:1923 [D loss: 0.440309, acc.: 79.69%] [G loss: 2.397795]\n",
      "epoch:2 step:1924 [D loss: 0.555175, acc.: 73.44%] [G loss: 2.490434]\n",
      "epoch:2 step:1925 [D loss: 0.498984, acc.: 77.34%] [G loss: 2.341700]\n",
      "epoch:2 step:1926 [D loss: 0.554427, acc.: 71.88%] [G loss: 1.845581]\n",
      "epoch:2 step:1927 [D loss: 0.424100, acc.: 89.06%] [G loss: 1.903932]\n",
      "epoch:2 step:1928 [D loss: 0.451978, acc.: 84.38%] [G loss: 2.154751]\n",
      "epoch:2 step:1929 [D loss: 0.456853, acc.: 82.03%] [G loss: 2.480603]\n",
      "epoch:2 step:1930 [D loss: 0.452157, acc.: 79.69%] [G loss: 2.334208]\n",
      "epoch:2 step:1931 [D loss: 0.441493, acc.: 85.16%] [G loss: 2.298408]\n",
      "epoch:2 step:1932 [D loss: 0.452418, acc.: 76.56%] [G loss: 2.027111]\n",
      "epoch:2 step:1933 [D loss: 0.438752, acc.: 82.81%] [G loss: 2.026499]\n",
      "epoch:2 step:1934 [D loss: 0.437694, acc.: 83.59%] [G loss: 2.568010]\n",
      "epoch:2 step:1935 [D loss: 0.464774, acc.: 79.69%] [G loss: 3.228713]\n",
      "epoch:2 step:1936 [D loss: 0.468138, acc.: 78.12%] [G loss: 2.497708]\n",
      "epoch:2 step:1937 [D loss: 0.490274, acc.: 79.69%] [G loss: 2.877829]\n",
      "epoch:2 step:1938 [D loss: 0.536563, acc.: 71.88%] [G loss: 2.399666]\n",
      "epoch:2 step:1939 [D loss: 0.466257, acc.: 78.91%] [G loss: 2.260778]\n",
      "epoch:2 step:1940 [D loss: 0.409094, acc.: 83.59%] [G loss: 2.917108]\n",
      "epoch:2 step:1941 [D loss: 0.482968, acc.: 74.22%] [G loss: 3.068200]\n",
      "epoch:2 step:1942 [D loss: 0.501102, acc.: 76.56%] [G loss: 2.531055]\n",
      "epoch:2 step:1943 [D loss: 0.485211, acc.: 78.12%] [G loss: 1.639592]\n",
      "epoch:2 step:1944 [D loss: 0.451719, acc.: 82.81%] [G loss: 2.331157]\n",
      "epoch:2 step:1945 [D loss: 0.451037, acc.: 77.34%] [G loss: 2.283320]\n",
      "epoch:2 step:1946 [D loss: 0.453414, acc.: 78.91%] [G loss: 2.083136]\n",
      "epoch:2 step:1947 [D loss: 0.459591, acc.: 83.59%] [G loss: 2.194701]\n",
      "epoch:2 step:1948 [D loss: 0.446484, acc.: 82.03%] [G loss: 2.163307]\n",
      "epoch:2 step:1949 [D loss: 0.497050, acc.: 76.56%] [G loss: 3.030449]\n",
      "epoch:2 step:1950 [D loss: 0.831012, acc.: 57.03%] [G loss: 3.740374]\n",
      "epoch:2 step:1951 [D loss: 1.142300, acc.: 50.00%] [G loss: 2.749918]\n",
      "epoch:2 step:1952 [D loss: 0.529641, acc.: 78.91%] [G loss: 2.035010]\n",
      "epoch:2 step:1953 [D loss: 0.606260, acc.: 67.19%] [G loss: 1.638932]\n",
      "epoch:2 step:1954 [D loss: 0.515063, acc.: 75.78%] [G loss: 2.296882]\n",
      "epoch:2 step:1955 [D loss: 0.580734, acc.: 71.88%] [G loss: 1.809715]\n",
      "epoch:2 step:1956 [D loss: 0.492079, acc.: 81.25%] [G loss: 2.326133]\n",
      "epoch:2 step:1957 [D loss: 0.509335, acc.: 78.12%] [G loss: 2.972626]\n",
      "epoch:2 step:1958 [D loss: 0.430105, acc.: 81.25%] [G loss: 2.286720]\n",
      "epoch:2 step:1959 [D loss: 0.471929, acc.: 78.91%] [G loss: 2.315005]\n",
      "epoch:2 step:1960 [D loss: 0.438718, acc.: 86.72%] [G loss: 1.993018]\n",
      "epoch:2 step:1961 [D loss: 0.486056, acc.: 78.91%] [G loss: 1.765115]\n",
      "epoch:2 step:1962 [D loss: 0.391279, acc.: 88.28%] [G loss: 2.178572]\n",
      "epoch:2 step:1963 [D loss: 0.436267, acc.: 83.59%] [G loss: 2.201486]\n",
      "epoch:2 step:1964 [D loss: 0.414480, acc.: 85.94%] [G loss: 1.847455]\n",
      "epoch:2 step:1965 [D loss: 0.386633, acc.: 89.06%] [G loss: 1.794456]\n",
      "epoch:2 step:1966 [D loss: 0.443235, acc.: 82.03%] [G loss: 1.951278]\n",
      "epoch:2 step:1967 [D loss: 0.387100, acc.: 90.62%] [G loss: 1.940690]\n",
      "epoch:2 step:1968 [D loss: 0.402024, acc.: 88.28%] [G loss: 2.507977]\n",
      "epoch:2 step:1969 [D loss: 0.409070, acc.: 85.16%] [G loss: 2.322745]\n",
      "epoch:2 step:1970 [D loss: 0.447446, acc.: 83.59%] [G loss: 2.349994]\n",
      "epoch:2 step:1971 [D loss: 0.383227, acc.: 85.94%] [G loss: 2.375387]\n",
      "epoch:2 step:1972 [D loss: 0.445608, acc.: 83.59%] [G loss: 1.915078]\n",
      "epoch:2 step:1973 [D loss: 0.454399, acc.: 82.03%] [G loss: 2.047252]\n",
      "epoch:2 step:1974 [D loss: 0.492426, acc.: 76.56%] [G loss: 2.074535]\n",
      "epoch:2 step:1975 [D loss: 0.393108, acc.: 87.50%] [G loss: 2.012527]\n",
      "epoch:2 step:1976 [D loss: 0.448350, acc.: 81.25%] [G loss: 2.160402]\n",
      "epoch:2 step:1977 [D loss: 0.415383, acc.: 88.28%] [G loss: 2.120792]\n",
      "epoch:2 step:1978 [D loss: 0.360049, acc.: 90.62%] [G loss: 2.080004]\n",
      "epoch:2 step:1979 [D loss: 0.443886, acc.: 75.00%] [G loss: 2.263452]\n",
      "epoch:2 step:1980 [D loss: 0.425349, acc.: 78.91%] [G loss: 2.839821]\n",
      "epoch:2 step:1981 [D loss: 0.396066, acc.: 87.50%] [G loss: 2.101807]\n",
      "epoch:2 step:1982 [D loss: 0.422268, acc.: 85.94%] [G loss: 2.203475]\n",
      "epoch:2 step:1983 [D loss: 0.446443, acc.: 84.38%] [G loss: 2.764726]\n",
      "epoch:2 step:1984 [D loss: 0.462708, acc.: 81.25%] [G loss: 1.998665]\n",
      "epoch:2 step:1985 [D loss: 0.400669, acc.: 85.94%] [G loss: 3.238073]\n",
      "epoch:2 step:1986 [D loss: 0.418555, acc.: 85.94%] [G loss: 2.880740]\n",
      "epoch:2 step:1987 [D loss: 0.420448, acc.: 82.81%] [G loss: 2.212580]\n",
      "epoch:2 step:1988 [D loss: 0.361529, acc.: 89.06%] [G loss: 2.332465]\n",
      "epoch:2 step:1989 [D loss: 0.454710, acc.: 78.91%] [G loss: 1.879769]\n",
      "epoch:2 step:1990 [D loss: 0.397755, acc.: 84.38%] [G loss: 2.579852]\n",
      "epoch:2 step:1991 [D loss: 0.415425, acc.: 82.03%] [G loss: 2.499797]\n",
      "epoch:2 step:1992 [D loss: 0.461293, acc.: 82.03%] [G loss: 2.196089]\n",
      "epoch:2 step:1993 [D loss: 0.495413, acc.: 78.91%] [G loss: 2.146956]\n",
      "epoch:2 step:1994 [D loss: 0.563323, acc.: 74.22%] [G loss: 3.743222]\n",
      "epoch:2 step:1995 [D loss: 0.547208, acc.: 76.56%] [G loss: 3.037719]\n",
      "epoch:2 step:1996 [D loss: 0.545874, acc.: 71.88%] [G loss: 2.404992]\n",
      "epoch:2 step:1997 [D loss: 0.552876, acc.: 77.34%] [G loss: 4.212132]\n",
      "epoch:2 step:1998 [D loss: 0.702969, acc.: 68.75%] [G loss: 2.964899]\n",
      "epoch:2 step:1999 [D loss: 0.762349, acc.: 67.19%] [G loss: 1.859324]\n",
      "epoch:2 step:2000 [D loss: 0.600335, acc.: 64.06%] [G loss: 2.002043]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##############\n",
      "[0.85042216 0.83815787 0.78648825 0.82129631 0.79022058 0.82732307\n",
      " 0.89253737 0.84521269 0.82101969 0.79418739]\n",
      "##########\n",
      "epoch:2 step:2001 [D loss: 0.417857, acc.: 85.16%] [G loss: 2.496232]\n",
      "epoch:2 step:2002 [D loss: 0.530533, acc.: 73.44%] [G loss: 2.200870]\n",
      "epoch:2 step:2003 [D loss: 0.427995, acc.: 82.03%] [G loss: 1.952668]\n",
      "epoch:2 step:2004 [D loss: 0.390156, acc.: 85.94%] [G loss: 2.082748]\n",
      "epoch:2 step:2005 [D loss: 0.463256, acc.: 79.69%] [G loss: 2.283607]\n",
      "epoch:2 step:2006 [D loss: 0.458404, acc.: 81.25%] [G loss: 1.747630]\n",
      "epoch:2 step:2007 [D loss: 0.419621, acc.: 84.38%] [G loss: 2.220278]\n",
      "epoch:2 step:2008 [D loss: 0.382024, acc.: 90.62%] [G loss: 2.056725]\n",
      "epoch:2 step:2009 [D loss: 0.393685, acc.: 85.94%] [G loss: 2.201908]\n",
      "epoch:2 step:2010 [D loss: 0.439715, acc.: 84.38%] [G loss: 1.762852]\n",
      "epoch:2 step:2011 [D loss: 0.465294, acc.: 81.25%] [G loss: 1.879603]\n",
      "epoch:2 step:2012 [D loss: 0.491505, acc.: 78.91%] [G loss: 1.923390]\n",
      "epoch:2 step:2013 [D loss: 0.434042, acc.: 88.28%] [G loss: 1.997277]\n",
      "epoch:2 step:2014 [D loss: 0.455288, acc.: 84.38%] [G loss: 1.923991]\n",
      "epoch:2 step:2015 [D loss: 0.441773, acc.: 82.81%] [G loss: 1.941493]\n",
      "epoch:2 step:2016 [D loss: 0.385380, acc.: 89.06%] [G loss: 2.632729]\n",
      "epoch:2 step:2017 [D loss: 0.343751, acc.: 89.06%] [G loss: 2.441490]\n",
      "epoch:2 step:2018 [D loss: 0.426217, acc.: 84.38%] [G loss: 1.930693]\n",
      "epoch:2 step:2019 [D loss: 0.377165, acc.: 83.59%] [G loss: 2.033123]\n",
      "epoch:2 step:2020 [D loss: 0.394838, acc.: 83.59%] [G loss: 2.304762]\n",
      "epoch:2 step:2021 [D loss: 0.389969, acc.: 84.38%] [G loss: 2.571542]\n",
      "epoch:2 step:2022 [D loss: 0.376916, acc.: 88.28%] [G loss: 2.530080]\n",
      "epoch:2 step:2023 [D loss: 0.420146, acc.: 82.03%] [G loss: 2.284555]\n",
      "epoch:2 step:2024 [D loss: 0.404236, acc.: 82.81%] [G loss: 2.592220]\n",
      "epoch:2 step:2025 [D loss: 0.507501, acc.: 75.78%] [G loss: 2.306461]\n",
      "epoch:2 step:2026 [D loss: 0.397116, acc.: 86.72%] [G loss: 2.634975]\n",
      "epoch:2 step:2027 [D loss: 0.478698, acc.: 75.78%] [G loss: 2.686777]\n",
      "epoch:2 step:2028 [D loss: 0.369993, acc.: 82.81%] [G loss: 2.364429]\n",
      "epoch:2 step:2029 [D loss: 0.459998, acc.: 80.47%] [G loss: 1.574968]\n",
      "epoch:2 step:2030 [D loss: 0.410682, acc.: 86.72%] [G loss: 2.171770]\n",
      "epoch:2 step:2031 [D loss: 0.490912, acc.: 70.31%] [G loss: 2.037540]\n",
      "epoch:2 step:2032 [D loss: 0.395089, acc.: 89.84%] [G loss: 1.637533]\n",
      "epoch:2 step:2033 [D loss: 0.490285, acc.: 75.78%] [G loss: 2.177272]\n",
      "epoch:2 step:2034 [D loss: 0.495282, acc.: 77.34%] [G loss: 2.450364]\n",
      "epoch:2 step:2035 [D loss: 0.429493, acc.: 83.59%] [G loss: 2.152972]\n",
      "epoch:2 step:2036 [D loss: 0.431941, acc.: 86.72%] [G loss: 2.593956]\n",
      "epoch:2 step:2037 [D loss: 0.434057, acc.: 87.50%] [G loss: 1.876704]\n",
      "epoch:2 step:2038 [D loss: 0.490610, acc.: 82.81%] [G loss: 1.522311]\n",
      "epoch:2 step:2039 [D loss: 0.476149, acc.: 79.69%] [G loss: 2.258080]\n",
      "epoch:2 step:2040 [D loss: 0.386743, acc.: 81.25%] [G loss: 2.611228]\n",
      "epoch:2 step:2041 [D loss: 0.357824, acc.: 89.84%] [G loss: 2.848805]\n",
      "epoch:2 step:2042 [D loss: 0.464460, acc.: 81.25%] [G loss: 2.035208]\n",
      "epoch:2 step:2043 [D loss: 0.486193, acc.: 77.34%] [G loss: 2.487444]\n",
      "epoch:2 step:2044 [D loss: 0.408806, acc.: 81.25%] [G loss: 2.403603]\n",
      "epoch:2 step:2045 [D loss: 0.421288, acc.: 81.25%] [G loss: 1.695517]\n",
      "epoch:2 step:2046 [D loss: 0.402638, acc.: 86.72%] [G loss: 2.205380]\n",
      "epoch:2 step:2047 [D loss: 0.356434, acc.: 85.16%] [G loss: 2.242474]\n",
      "epoch:2 step:2048 [D loss: 0.410963, acc.: 82.03%] [G loss: 2.258371]\n",
      "epoch:2 step:2049 [D loss: 0.510956, acc.: 75.00%] [G loss: 2.014800]\n",
      "epoch:2 step:2050 [D loss: 0.377378, acc.: 82.03%] [G loss: 2.715909]\n",
      "epoch:2 step:2051 [D loss: 0.464694, acc.: 77.34%] [G loss: 2.636855]\n",
      "epoch:2 step:2052 [D loss: 0.485699, acc.: 73.44%] [G loss: 3.528840]\n",
      "epoch:2 step:2053 [D loss: 0.543618, acc.: 73.44%] [G loss: 3.356140]\n",
      "epoch:2 step:2054 [D loss: 0.595246, acc.: 74.22%] [G loss: 2.810865]\n",
      "epoch:2 step:2055 [D loss: 0.519124, acc.: 71.88%] [G loss: 2.038961]\n",
      "epoch:2 step:2056 [D loss: 0.437444, acc.: 85.16%] [G loss: 3.467676]\n",
      "epoch:2 step:2057 [D loss: 0.654372, acc.: 69.53%] [G loss: 3.471666]\n",
      "epoch:2 step:2058 [D loss: 0.619802, acc.: 76.56%] [G loss: 2.346875]\n",
      "epoch:2 step:2059 [D loss: 0.504237, acc.: 75.78%] [G loss: 2.180352]\n",
      "epoch:2 step:2060 [D loss: 0.402384, acc.: 82.81%] [G loss: 2.183193]\n",
      "epoch:2 step:2061 [D loss: 0.547872, acc.: 67.19%] [G loss: 1.977414]\n",
      "epoch:2 step:2062 [D loss: 0.495989, acc.: 75.00%] [G loss: 2.196095]\n",
      "epoch:2 step:2063 [D loss: 0.385530, acc.: 87.50%] [G loss: 2.067560]\n",
      "epoch:2 step:2064 [D loss: 0.362528, acc.: 88.28%] [G loss: 1.845337]\n",
      "epoch:2 step:2065 [D loss: 0.461147, acc.: 82.81%] [G loss: 2.084518]\n",
      "epoch:2 step:2066 [D loss: 0.467248, acc.: 79.69%] [G loss: 2.676654]\n",
      "epoch:2 step:2067 [D loss: 0.599758, acc.: 75.00%] [G loss: 1.738917]\n",
      "epoch:2 step:2068 [D loss: 0.481170, acc.: 82.03%] [G loss: 1.696758]\n",
      "epoch:2 step:2069 [D loss: 0.548974, acc.: 75.00%] [G loss: 2.612458]\n",
      "epoch:2 step:2070 [D loss: 0.372517, acc.: 90.62%] [G loss: 2.569209]\n",
      "epoch:2 step:2071 [D loss: 0.433021, acc.: 78.91%] [G loss: 2.002059]\n",
      "epoch:2 step:2072 [D loss: 0.428652, acc.: 85.16%] [G loss: 1.831191]\n",
      "epoch:2 step:2073 [D loss: 0.376834, acc.: 89.84%] [G loss: 2.018030]\n",
      "epoch:2 step:2074 [D loss: 0.355720, acc.: 85.16%] [G loss: 2.442096]\n",
      "epoch:2 step:2075 [D loss: 0.370185, acc.: 86.72%] [G loss: 2.932060]\n",
      "epoch:2 step:2076 [D loss: 0.519061, acc.: 71.88%] [G loss: 3.560776]\n",
      "epoch:2 step:2077 [D loss: 0.661621, acc.: 68.75%] [G loss: 2.279699]\n",
      "epoch:2 step:2078 [D loss: 0.640778, acc.: 63.28%] [G loss: 1.967776]\n",
      "epoch:2 step:2079 [D loss: 0.501801, acc.: 78.91%] [G loss: 2.569737]\n",
      "epoch:2 step:2080 [D loss: 0.400123, acc.: 90.62%] [G loss: 2.286320]\n",
      "epoch:2 step:2081 [D loss: 0.348236, acc.: 89.84%] [G loss: 3.377197]\n",
      "epoch:2 step:2082 [D loss: 0.430925, acc.: 79.69%] [G loss: 1.839552]\n",
      "epoch:2 step:2083 [D loss: 0.349943, acc.: 85.16%] [G loss: 2.665111]\n",
      "epoch:2 step:2084 [D loss: 0.435778, acc.: 83.59%] [G loss: 2.355794]\n",
      "epoch:2 step:2085 [D loss: 0.359066, acc.: 82.81%] [G loss: 2.852629]\n",
      "epoch:2 step:2086 [D loss: 0.332304, acc.: 90.62%] [G loss: 3.332875]\n",
      "epoch:2 step:2087 [D loss: 0.371230, acc.: 86.72%] [G loss: 2.472540]\n",
      "epoch:2 step:2088 [D loss: 0.490727, acc.: 74.22%] [G loss: 2.171399]\n",
      "epoch:2 step:2089 [D loss: 0.424812, acc.: 84.38%] [G loss: 2.315016]\n",
      "epoch:2 step:2090 [D loss: 0.416516, acc.: 78.91%] [G loss: 2.812310]\n",
      "epoch:2 step:2091 [D loss: 0.429612, acc.: 82.81%] [G loss: 2.062987]\n",
      "epoch:2 step:2092 [D loss: 0.434488, acc.: 82.81%] [G loss: 2.043514]\n",
      "epoch:2 step:2093 [D loss: 0.391971, acc.: 86.72%] [G loss: 2.075282]\n",
      "epoch:2 step:2094 [D loss: 0.451619, acc.: 79.69%] [G loss: 2.197361]\n",
      "epoch:2 step:2095 [D loss: 0.415244, acc.: 85.16%] [G loss: 2.063822]\n",
      "epoch:2 step:2096 [D loss: 0.495537, acc.: 79.69%] [G loss: 1.732090]\n",
      "epoch:2 step:2097 [D loss: 0.413910, acc.: 83.59%] [G loss: 2.570337]\n",
      "epoch:2 step:2098 [D loss: 0.416137, acc.: 80.47%] [G loss: 2.609941]\n",
      "epoch:2 step:2099 [D loss: 0.318026, acc.: 92.19%] [G loss: 2.498501]\n",
      "epoch:2 step:2100 [D loss: 0.399325, acc.: 84.38%] [G loss: 2.276478]\n",
      "epoch:2 step:2101 [D loss: 0.421427, acc.: 83.59%] [G loss: 1.993994]\n",
      "epoch:2 step:2102 [D loss: 0.351930, acc.: 89.06%] [G loss: 3.283486]\n",
      "epoch:2 step:2103 [D loss: 0.346964, acc.: 90.62%] [G loss: 3.487762]\n",
      "epoch:2 step:2104 [D loss: 0.420308, acc.: 82.03%] [G loss: 3.213466]\n",
      "epoch:2 step:2105 [D loss: 0.466359, acc.: 80.47%] [G loss: 3.320918]\n",
      "epoch:2 step:2106 [D loss: 0.489766, acc.: 78.12%] [G loss: 5.197997]\n",
      "epoch:2 step:2107 [D loss: 0.861574, acc.: 52.34%] [G loss: 2.752503]\n",
      "epoch:2 step:2108 [D loss: 0.573217, acc.: 71.09%] [G loss: 2.282912]\n",
      "epoch:2 step:2109 [D loss: 0.429500, acc.: 80.47%] [G loss: 2.256093]\n",
      "epoch:2 step:2110 [D loss: 0.536856, acc.: 75.78%] [G loss: 3.602997]\n",
      "epoch:2 step:2111 [D loss: 0.507671, acc.: 77.34%] [G loss: 2.708428]\n",
      "epoch:2 step:2112 [D loss: 0.509808, acc.: 75.78%] [G loss: 2.012911]\n",
      "epoch:2 step:2113 [D loss: 0.455565, acc.: 82.81%] [G loss: 2.099546]\n",
      "epoch:2 step:2114 [D loss: 0.486378, acc.: 80.47%] [G loss: 2.095922]\n",
      "epoch:2 step:2115 [D loss: 0.501146, acc.: 74.22%] [G loss: 2.696297]\n",
      "epoch:2 step:2116 [D loss: 0.478590, acc.: 77.34%] [G loss: 2.226278]\n",
      "epoch:2 step:2117 [D loss: 0.448649, acc.: 83.59%] [G loss: 2.248629]\n",
      "epoch:2 step:2118 [D loss: 0.377060, acc.: 81.25%] [G loss: 2.338041]\n",
      "epoch:2 step:2119 [D loss: 0.437023, acc.: 80.47%] [G loss: 1.963748]\n",
      "epoch:2 step:2120 [D loss: 0.446168, acc.: 78.12%] [G loss: 2.977506]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:2 step:2121 [D loss: 0.381271, acc.: 89.06%] [G loss: 2.006261]\n",
      "epoch:2 step:2122 [D loss: 0.362272, acc.: 88.28%] [G loss: 2.952776]\n",
      "epoch:2 step:2123 [D loss: 0.389087, acc.: 82.81%] [G loss: 2.343295]\n",
      "epoch:2 step:2124 [D loss: 0.393987, acc.: 85.16%] [G loss: 2.257903]\n",
      "epoch:2 step:2125 [D loss: 0.425458, acc.: 83.59%] [G loss: 2.100517]\n",
      "epoch:2 step:2126 [D loss: 0.430743, acc.: 81.25%] [G loss: 2.389226]\n",
      "epoch:2 step:2127 [D loss: 0.404149, acc.: 83.59%] [G loss: 2.351237]\n",
      "epoch:2 step:2128 [D loss: 0.452188, acc.: 75.00%] [G loss: 2.690268]\n",
      "epoch:2 step:2129 [D loss: 0.508261, acc.: 74.22%] [G loss: 3.079595]\n",
      "epoch:2 step:2130 [D loss: 0.421177, acc.: 81.25%] [G loss: 2.464663]\n",
      "epoch:2 step:2131 [D loss: 0.399539, acc.: 81.25%] [G loss: 2.848931]\n",
      "epoch:2 step:2132 [D loss: 0.424233, acc.: 77.34%] [G loss: 3.160789]\n",
      "epoch:2 step:2133 [D loss: 0.465779, acc.: 73.44%] [G loss: 2.757566]\n",
      "epoch:2 step:2134 [D loss: 0.376430, acc.: 90.62%] [G loss: 2.778060]\n",
      "epoch:2 step:2135 [D loss: 0.453906, acc.: 76.56%] [G loss: 1.961607]\n",
      "epoch:2 step:2136 [D loss: 0.458035, acc.: 78.12%] [G loss: 3.076875]\n",
      "epoch:2 step:2137 [D loss: 0.467539, acc.: 76.56%] [G loss: 2.306421]\n",
      "epoch:2 step:2138 [D loss: 0.448252, acc.: 84.38%] [G loss: 2.125849]\n",
      "epoch:2 step:2139 [D loss: 0.396592, acc.: 85.94%] [G loss: 2.602261]\n",
      "epoch:2 step:2140 [D loss: 0.381028, acc.: 88.28%] [G loss: 2.835779]\n",
      "epoch:2 step:2141 [D loss: 0.369200, acc.: 85.16%] [G loss: 3.196358]\n",
      "epoch:2 step:2142 [D loss: 0.365466, acc.: 85.16%] [G loss: 2.871098]\n",
      "epoch:2 step:2143 [D loss: 0.389660, acc.: 87.50%] [G loss: 2.215280]\n",
      "epoch:2 step:2144 [D loss: 0.538579, acc.: 70.31%] [G loss: 4.332476]\n",
      "epoch:2 step:2145 [D loss: 0.705781, acc.: 67.19%] [G loss: 3.794490]\n",
      "epoch:2 step:2146 [D loss: 0.838769, acc.: 62.50%] [G loss: 2.470626]\n",
      "epoch:2 step:2147 [D loss: 0.526013, acc.: 82.03%] [G loss: 2.469461]\n",
      "epoch:2 step:2148 [D loss: 0.484388, acc.: 74.22%] [G loss: 2.834988]\n",
      "epoch:2 step:2149 [D loss: 0.533385, acc.: 75.00%] [G loss: 2.334915]\n",
      "epoch:2 step:2150 [D loss: 0.541002, acc.: 71.88%] [G loss: 1.911231]\n",
      "epoch:2 step:2151 [D loss: 0.440355, acc.: 78.91%] [G loss: 2.109216]\n",
      "epoch:2 step:2152 [D loss: 0.429263, acc.: 83.59%] [G loss: 2.048222]\n",
      "epoch:2 step:2153 [D loss: 0.459048, acc.: 81.25%] [G loss: 2.053727]\n",
      "epoch:2 step:2154 [D loss: 0.432192, acc.: 82.81%] [G loss: 1.925600]\n",
      "epoch:2 step:2155 [D loss: 0.481168, acc.: 71.88%] [G loss: 2.436373]\n",
      "epoch:2 step:2156 [D loss: 0.433780, acc.: 78.91%] [G loss: 2.637753]\n",
      "epoch:2 step:2157 [D loss: 0.425790, acc.: 80.47%] [G loss: 2.710322]\n",
      "epoch:2 step:2158 [D loss: 0.390075, acc.: 89.84%] [G loss: 2.290197]\n",
      "epoch:2 step:2159 [D loss: 0.451698, acc.: 78.12%] [G loss: 2.381474]\n",
      "epoch:2 step:2160 [D loss: 0.389408, acc.: 83.59%] [G loss: 3.184541]\n",
      "epoch:2 step:2161 [D loss: 0.369617, acc.: 85.94%] [G loss: 2.568124]\n",
      "epoch:2 step:2162 [D loss: 0.440401, acc.: 83.59%] [G loss: 1.878377]\n",
      "epoch:2 step:2163 [D loss: 0.381618, acc.: 85.16%] [G loss: 2.852229]\n",
      "epoch:2 step:2164 [D loss: 0.453493, acc.: 81.25%] [G loss: 2.352018]\n",
      "epoch:2 step:2165 [D loss: 0.475738, acc.: 78.12%] [G loss: 2.317035]\n",
      "epoch:2 step:2166 [D loss: 0.457268, acc.: 77.34%] [G loss: 1.982502]\n",
      "epoch:2 step:2167 [D loss: 0.379823, acc.: 82.81%] [G loss: 2.460549]\n",
      "epoch:2 step:2168 [D loss: 0.380624, acc.: 89.84%] [G loss: 1.734664]\n",
      "epoch:2 step:2169 [D loss: 0.433168, acc.: 85.16%] [G loss: 1.782969]\n",
      "epoch:2 step:2170 [D loss: 0.415773, acc.: 83.59%] [G loss: 2.699665]\n",
      "epoch:2 step:2171 [D loss: 0.473967, acc.: 78.12%] [G loss: 4.184402]\n",
      "epoch:2 step:2172 [D loss: 0.780822, acc.: 64.06%] [G loss: 3.427972]\n",
      "epoch:2 step:2173 [D loss: 0.540077, acc.: 76.56%] [G loss: 2.796168]\n",
      "epoch:2 step:2174 [D loss: 0.453025, acc.: 79.69%] [G loss: 2.138287]\n",
      "epoch:2 step:2175 [D loss: 0.419579, acc.: 85.16%] [G loss: 2.361625]\n",
      "epoch:2 step:2176 [D loss: 0.421218, acc.: 82.81%] [G loss: 2.005264]\n",
      "epoch:2 step:2177 [D loss: 0.325115, acc.: 86.72%] [G loss: 2.784039]\n",
      "epoch:2 step:2178 [D loss: 0.400764, acc.: 87.50%] [G loss: 2.382483]\n",
      "epoch:2 step:2179 [D loss: 0.477537, acc.: 73.44%] [G loss: 4.894721]\n",
      "epoch:2 step:2180 [D loss: 0.383304, acc.: 82.03%] [G loss: 3.556835]\n",
      "epoch:2 step:2181 [D loss: 0.409572, acc.: 80.47%] [G loss: 3.018187]\n",
      "epoch:2 step:2182 [D loss: 0.407675, acc.: 82.81%] [G loss: 2.520807]\n",
      "epoch:2 step:2183 [D loss: 0.471728, acc.: 79.69%] [G loss: 1.945680]\n",
      "epoch:2 step:2184 [D loss: 0.393320, acc.: 85.16%] [G loss: 2.726549]\n",
      "epoch:2 step:2185 [D loss: 0.408303, acc.: 83.59%] [G loss: 2.353560]\n",
      "epoch:2 step:2186 [D loss: 0.411587, acc.: 85.16%] [G loss: 2.479748]\n",
      "epoch:2 step:2187 [D loss: 0.390783, acc.: 83.59%] [G loss: 2.558626]\n",
      "epoch:2 step:2188 [D loss: 0.417790, acc.: 79.69%] [G loss: 3.019577]\n",
      "epoch:2 step:2189 [D loss: 0.510572, acc.: 73.44%] [G loss: 2.188642]\n",
      "epoch:2 step:2190 [D loss: 0.470939, acc.: 75.78%] [G loss: 2.946301]\n",
      "epoch:2 step:2191 [D loss: 0.539825, acc.: 76.56%] [G loss: 2.653891]\n",
      "epoch:2 step:2192 [D loss: 0.362221, acc.: 82.81%] [G loss: 2.321199]\n",
      "epoch:2 step:2193 [D loss: 0.439188, acc.: 80.47%] [G loss: 2.614151]\n",
      "epoch:2 step:2194 [D loss: 0.447743, acc.: 78.91%] [G loss: 2.243768]\n",
      "epoch:2 step:2195 [D loss: 0.397755, acc.: 84.38%] [G loss: 2.044325]\n",
      "epoch:2 step:2196 [D loss: 0.402055, acc.: 83.59%] [G loss: 2.192064]\n",
      "epoch:2 step:2197 [D loss: 0.436576, acc.: 82.03%] [G loss: 1.952591]\n",
      "epoch:2 step:2198 [D loss: 0.437480, acc.: 84.38%] [G loss: 1.932233]\n",
      "epoch:2 step:2199 [D loss: 0.446591, acc.: 80.47%] [G loss: 2.208309]\n",
      "epoch:2 step:2200 [D loss: 0.379199, acc.: 90.62%] [G loss: 2.028148]\n",
      "##############\n",
      "[0.85090349 0.82407801 0.79218122 0.8191775  0.79035061 0.81857691\n",
      " 0.90057308 0.83370811 0.81026872 0.82370603]\n",
      "##########\n",
      "epoch:2 step:2201 [D loss: 0.535833, acc.: 74.22%] [G loss: 3.342108]\n",
      "epoch:2 step:2202 [D loss: 0.520251, acc.: 76.56%] [G loss: 3.601740]\n",
      "epoch:2 step:2203 [D loss: 0.672190, acc.: 64.84%] [G loss: 2.852607]\n",
      "epoch:2 step:2204 [D loss: 0.527744, acc.: 75.00%] [G loss: 2.170328]\n",
      "epoch:2 step:2205 [D loss: 0.391931, acc.: 85.16%] [G loss: 2.394222]\n",
      "epoch:2 step:2206 [D loss: 0.424418, acc.: 81.25%] [G loss: 3.053020]\n",
      "epoch:2 step:2207 [D loss: 0.376098, acc.: 85.94%] [G loss: 2.639546]\n",
      "epoch:2 step:2208 [D loss: 0.353677, acc.: 89.06%] [G loss: 2.640635]\n",
      "epoch:2 step:2209 [D loss: 0.369474, acc.: 87.50%] [G loss: 2.256025]\n",
      "epoch:2 step:2210 [D loss: 0.375576, acc.: 88.28%] [G loss: 3.103096]\n",
      "epoch:2 step:2211 [D loss: 0.336975, acc.: 91.41%] [G loss: 2.441761]\n",
      "epoch:2 step:2212 [D loss: 0.346073, acc.: 85.16%] [G loss: 3.466618]\n",
      "epoch:2 step:2213 [D loss: 0.470209, acc.: 80.47%] [G loss: 2.460675]\n",
      "epoch:2 step:2214 [D loss: 0.416001, acc.: 80.47%] [G loss: 2.573906]\n",
      "epoch:2 step:2215 [D loss: 0.454182, acc.: 78.91%] [G loss: 3.388439]\n",
      "epoch:2 step:2216 [D loss: 0.417253, acc.: 83.59%] [G loss: 2.921636]\n",
      "epoch:2 step:2217 [D loss: 0.415447, acc.: 83.59%] [G loss: 2.436803]\n",
      "epoch:2 step:2218 [D loss: 0.349261, acc.: 86.72%] [G loss: 3.469421]\n",
      "epoch:2 step:2219 [D loss: 0.490145, acc.: 74.22%] [G loss: 2.508717]\n",
      "epoch:2 step:2220 [D loss: 0.554663, acc.: 69.53%] [G loss: 4.003000]\n",
      "epoch:2 step:2221 [D loss: 0.415884, acc.: 86.72%] [G loss: 3.092393]\n",
      "epoch:2 step:2222 [D loss: 0.377709, acc.: 82.81%] [G loss: 4.256098]\n",
      "epoch:2 step:2223 [D loss: 0.490341, acc.: 71.09%] [G loss: 2.638982]\n",
      "epoch:2 step:2224 [D loss: 0.337247, acc.: 89.06%] [G loss: 3.130128]\n",
      "epoch:2 step:2225 [D loss: 0.358996, acc.: 88.28%] [G loss: 3.161769]\n",
      "epoch:2 step:2226 [D loss: 0.386160, acc.: 85.16%] [G loss: 2.172671]\n",
      "epoch:2 step:2227 [D loss: 0.467545, acc.: 83.59%] [G loss: 2.008816]\n",
      "epoch:2 step:2228 [D loss: 0.371269, acc.: 86.72%] [G loss: 2.450221]\n",
      "epoch:2 step:2229 [D loss: 0.380116, acc.: 90.62%] [G loss: 2.296879]\n",
      "epoch:2 step:2230 [D loss: 0.483864, acc.: 73.44%] [G loss: 2.295617]\n",
      "epoch:2 step:2231 [D loss: 0.442838, acc.: 86.72%] [G loss: 2.139532]\n",
      "epoch:2 step:2232 [D loss: 0.488106, acc.: 79.69%] [G loss: 2.564826]\n",
      "epoch:2 step:2233 [D loss: 0.516029, acc.: 73.44%] [G loss: 2.168044]\n",
      "epoch:2 step:2234 [D loss: 0.366349, acc.: 90.62%] [G loss: 2.889396]\n",
      "epoch:2 step:2235 [D loss: 0.510286, acc.: 71.09%] [G loss: 2.242164]\n",
      "epoch:2 step:2236 [D loss: 0.414471, acc.: 84.38%] [G loss: 2.212652]\n",
      "epoch:2 step:2237 [D loss: 0.369568, acc.: 85.94%] [G loss: 2.226114]\n",
      "epoch:2 step:2238 [D loss: 0.392756, acc.: 85.16%] [G loss: 2.264721]\n",
      "epoch:2 step:2239 [D loss: 0.428714, acc.: 79.69%] [G loss: 2.482554]\n",
      "epoch:2 step:2240 [D loss: 0.433922, acc.: 79.69%] [G loss: 2.576381]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:2 step:2241 [D loss: 0.389814, acc.: 83.59%] [G loss: 1.963407]\n",
      "epoch:2 step:2242 [D loss: 0.357885, acc.: 83.59%] [G loss: 2.331022]\n",
      "epoch:2 step:2243 [D loss: 0.366846, acc.: 86.72%] [G loss: 2.542077]\n",
      "epoch:2 step:2244 [D loss: 0.453613, acc.: 77.34%] [G loss: 2.169988]\n",
      "epoch:2 step:2245 [D loss: 0.355112, acc.: 85.16%] [G loss: 2.585171]\n",
      "epoch:2 step:2246 [D loss: 0.432479, acc.: 84.38%] [G loss: 2.641387]\n",
      "epoch:2 step:2247 [D loss: 0.385697, acc.: 85.16%] [G loss: 3.077726]\n",
      "epoch:2 step:2248 [D loss: 0.412062, acc.: 82.03%] [G loss: 3.039480]\n",
      "epoch:2 step:2249 [D loss: 0.503346, acc.: 77.34%] [G loss: 2.542438]\n",
      "epoch:2 step:2250 [D loss: 0.477979, acc.: 77.34%] [G loss: 3.087031]\n",
      "epoch:2 step:2251 [D loss: 0.531719, acc.: 73.44%] [G loss: 2.441055]\n",
      "epoch:2 step:2252 [D loss: 0.377185, acc.: 85.94%] [G loss: 2.256866]\n",
      "epoch:2 step:2253 [D loss: 0.419530, acc.: 85.16%] [G loss: 2.798047]\n",
      "epoch:2 step:2254 [D loss: 0.556738, acc.: 66.41%] [G loss: 2.000211]\n",
      "epoch:2 step:2255 [D loss: 0.425552, acc.: 82.81%] [G loss: 2.391685]\n",
      "epoch:2 step:2256 [D loss: 0.454948, acc.: 79.69%] [G loss: 3.149335]\n",
      "epoch:2 step:2257 [D loss: 0.418573, acc.: 83.59%] [G loss: 2.286514]\n",
      "epoch:2 step:2258 [D loss: 0.400028, acc.: 85.94%] [G loss: 2.471756]\n",
      "epoch:2 step:2259 [D loss: 0.365810, acc.: 88.28%] [G loss: 3.033670]\n",
      "epoch:2 step:2260 [D loss: 0.343517, acc.: 85.16%] [G loss: 2.412274]\n",
      "epoch:2 step:2261 [D loss: 0.465741, acc.: 75.00%] [G loss: 1.985517]\n",
      "epoch:2 step:2262 [D loss: 0.382691, acc.: 86.72%] [G loss: 2.294770]\n",
      "epoch:2 step:2263 [D loss: 0.397922, acc.: 85.94%] [G loss: 2.282416]\n",
      "epoch:2 step:2264 [D loss: 0.343994, acc.: 89.06%] [G loss: 2.472957]\n",
      "epoch:2 step:2265 [D loss: 0.389533, acc.: 86.72%] [G loss: 2.394303]\n",
      "epoch:2 step:2266 [D loss: 0.356082, acc.: 85.94%] [G loss: 2.151161]\n",
      "epoch:2 step:2267 [D loss: 0.498816, acc.: 74.22%] [G loss: 2.525835]\n",
      "epoch:2 step:2268 [D loss: 0.429142, acc.: 83.59%] [G loss: 3.482137]\n",
      "epoch:2 step:2269 [D loss: 0.481525, acc.: 81.25%] [G loss: 4.487150]\n",
      "epoch:2 step:2270 [D loss: 0.604478, acc.: 64.06%] [G loss: 2.639928]\n",
      "epoch:2 step:2271 [D loss: 0.507229, acc.: 78.91%] [G loss: 3.137671]\n",
      "epoch:2 step:2272 [D loss: 0.505991, acc.: 72.66%] [G loss: 2.793894]\n",
      "epoch:2 step:2273 [D loss: 0.423496, acc.: 83.59%] [G loss: 2.673406]\n",
      "epoch:2 step:2274 [D loss: 0.439472, acc.: 82.03%] [G loss: 2.362562]\n",
      "epoch:2 step:2275 [D loss: 0.544953, acc.: 68.75%] [G loss: 2.901861]\n",
      "epoch:2 step:2276 [D loss: 0.408657, acc.: 85.94%] [G loss: 3.405307]\n",
      "epoch:2 step:2277 [D loss: 0.508748, acc.: 70.31%] [G loss: 2.588729]\n",
      "epoch:2 step:2278 [D loss: 0.404644, acc.: 85.16%] [G loss: 3.665744]\n",
      "epoch:2 step:2279 [D loss: 0.463196, acc.: 80.47%] [G loss: 2.785494]\n",
      "epoch:2 step:2280 [D loss: 0.402145, acc.: 84.38%] [G loss: 2.204670]\n",
      "epoch:2 step:2281 [D loss: 0.432435, acc.: 82.03%] [G loss: 2.126065]\n",
      "epoch:2 step:2282 [D loss: 0.359770, acc.: 85.94%] [G loss: 2.541663]\n",
      "epoch:2 step:2283 [D loss: 0.354140, acc.: 86.72%] [G loss: 2.531174]\n",
      "epoch:2 step:2284 [D loss: 0.498773, acc.: 78.12%] [G loss: 3.378416]\n",
      "epoch:2 step:2285 [D loss: 0.528300, acc.: 75.00%] [G loss: 3.993134]\n",
      "epoch:2 step:2286 [D loss: 0.615222, acc.: 65.62%] [G loss: 2.886827]\n",
      "epoch:2 step:2287 [D loss: 0.475901, acc.: 76.56%] [G loss: 2.597860]\n",
      "epoch:2 step:2288 [D loss: 0.413305, acc.: 81.25%] [G loss: 3.517399]\n",
      "epoch:2 step:2289 [D loss: 0.500194, acc.: 76.56%] [G loss: 2.684437]\n",
      "epoch:2 step:2290 [D loss: 0.329896, acc.: 92.97%] [G loss: 3.023182]\n",
      "epoch:2 step:2291 [D loss: 0.477028, acc.: 78.12%] [G loss: 2.066325]\n",
      "epoch:2 step:2292 [D loss: 0.378495, acc.: 88.28%] [G loss: 2.632356]\n",
      "epoch:2 step:2293 [D loss: 0.385493, acc.: 86.72%] [G loss: 2.141537]\n",
      "epoch:2 step:2294 [D loss: 0.354850, acc.: 90.62%] [G loss: 1.720182]\n",
      "epoch:2 step:2295 [D loss: 0.441492, acc.: 81.25%] [G loss: 2.601179]\n",
      "epoch:2 step:2296 [D loss: 0.428012, acc.: 75.00%] [G loss: 2.451821]\n",
      "epoch:2 step:2297 [D loss: 0.526186, acc.: 70.31%] [G loss: 2.643190]\n",
      "epoch:2 step:2298 [D loss: 0.499402, acc.: 73.44%] [G loss: 2.743598]\n",
      "epoch:2 step:2299 [D loss: 0.401625, acc.: 86.72%] [G loss: 2.461481]\n",
      "epoch:2 step:2300 [D loss: 0.442962, acc.: 81.25%] [G loss: 2.028935]\n",
      "epoch:2 step:2301 [D loss: 0.401908, acc.: 80.47%] [G loss: 2.273002]\n",
      "epoch:2 step:2302 [D loss: 0.383608, acc.: 81.25%] [G loss: 2.915353]\n",
      "epoch:2 step:2303 [D loss: 0.388078, acc.: 84.38%] [G loss: 2.691005]\n",
      "epoch:2 step:2304 [D loss: 0.417184, acc.: 84.38%] [G loss: 3.940321]\n",
      "epoch:2 step:2305 [D loss: 0.411908, acc.: 78.91%] [G loss: 3.893234]\n",
      "epoch:2 step:2306 [D loss: 0.391611, acc.: 85.94%] [G loss: 2.836420]\n",
      "epoch:2 step:2307 [D loss: 0.294452, acc.: 90.62%] [G loss: 3.008768]\n",
      "epoch:2 step:2308 [D loss: 0.389042, acc.: 83.59%] [G loss: 2.257526]\n",
      "epoch:2 step:2309 [D loss: 0.350514, acc.: 84.38%] [G loss: 2.297515]\n",
      "epoch:2 step:2310 [D loss: 0.299892, acc.: 91.41%] [G loss: 2.420561]\n",
      "epoch:2 step:2311 [D loss: 0.332700, acc.: 87.50%] [G loss: 3.107489]\n",
      "epoch:2 step:2312 [D loss: 0.338728, acc.: 86.72%] [G loss: 3.674796]\n",
      "epoch:2 step:2313 [D loss: 0.436752, acc.: 76.56%] [G loss: 2.344662]\n",
      "epoch:2 step:2314 [D loss: 0.371219, acc.: 85.16%] [G loss: 3.108351]\n",
      "epoch:2 step:2315 [D loss: 0.494542, acc.: 77.34%] [G loss: 3.493345]\n",
      "epoch:2 step:2316 [D loss: 0.556325, acc.: 67.19%] [G loss: 3.177980]\n",
      "epoch:2 step:2317 [D loss: 0.503536, acc.: 74.22%] [G loss: 2.513583]\n",
      "epoch:2 step:2318 [D loss: 0.579471, acc.: 68.75%] [G loss: 3.182170]\n",
      "epoch:2 step:2319 [D loss: 0.462497, acc.: 78.12%] [G loss: 2.840582]\n",
      "epoch:2 step:2320 [D loss: 0.381995, acc.: 81.25%] [G loss: 2.227103]\n",
      "epoch:2 step:2321 [D loss: 0.361072, acc.: 85.16%] [G loss: 1.984999]\n",
      "epoch:2 step:2322 [D loss: 0.438026, acc.: 81.25%] [G loss: 2.806732]\n",
      "epoch:2 step:2323 [D loss: 0.357271, acc.: 86.72%] [G loss: 3.323110]\n",
      "epoch:2 step:2324 [D loss: 0.437778, acc.: 82.03%] [G loss: 2.294036]\n",
      "epoch:2 step:2325 [D loss: 0.392959, acc.: 85.16%] [G loss: 2.706853]\n",
      "epoch:2 step:2326 [D loss: 0.512362, acc.: 80.47%] [G loss: 2.610194]\n",
      "epoch:2 step:2327 [D loss: 0.637267, acc.: 72.66%] [G loss: 3.329255]\n",
      "epoch:2 step:2328 [D loss: 0.418925, acc.: 79.69%] [G loss: 2.811028]\n",
      "epoch:2 step:2329 [D loss: 0.465546, acc.: 77.34%] [G loss: 2.067029]\n",
      "epoch:2 step:2330 [D loss: 0.370795, acc.: 85.94%] [G loss: 2.396210]\n",
      "epoch:2 step:2331 [D loss: 0.411843, acc.: 82.81%] [G loss: 2.632438]\n",
      "epoch:2 step:2332 [D loss: 0.381056, acc.: 86.72%] [G loss: 3.085831]\n",
      "epoch:2 step:2333 [D loss: 0.364085, acc.: 88.28%] [G loss: 2.890407]\n",
      "epoch:2 step:2334 [D loss: 0.428384, acc.: 82.03%] [G loss: 2.056206]\n",
      "epoch:2 step:2335 [D loss: 0.410297, acc.: 80.47%] [G loss: 1.918457]\n",
      "epoch:2 step:2336 [D loss: 0.306919, acc.: 92.19%] [G loss: 2.558205]\n",
      "epoch:2 step:2337 [D loss: 0.347031, acc.: 86.72%] [G loss: 2.255260]\n",
      "epoch:2 step:2338 [D loss: 0.394884, acc.: 82.03%] [G loss: 2.015750]\n",
      "epoch:2 step:2339 [D loss: 0.334579, acc.: 90.62%] [G loss: 2.335671]\n",
      "epoch:2 step:2340 [D loss: 0.455868, acc.: 80.47%] [G loss: 2.345859]\n",
      "epoch:2 step:2341 [D loss: 0.477369, acc.: 79.69%] [G loss: 2.217253]\n",
      "epoch:2 step:2342 [D loss: 0.350923, acc.: 89.84%] [G loss: 2.492915]\n",
      "epoch:2 step:2343 [D loss: 0.378301, acc.: 85.94%] [G loss: 1.994776]\n",
      "epoch:3 step:2344 [D loss: 0.429157, acc.: 81.25%] [G loss: 3.124066]\n",
      "epoch:3 step:2345 [D loss: 0.373803, acc.: 85.94%] [G loss: 2.114698]\n",
      "epoch:3 step:2346 [D loss: 0.370298, acc.: 85.16%] [G loss: 2.765416]\n",
      "epoch:3 step:2347 [D loss: 0.495748, acc.: 73.44%] [G loss: 3.216730]\n",
      "epoch:3 step:2348 [D loss: 0.613214, acc.: 75.00%] [G loss: 3.200502]\n",
      "epoch:3 step:2349 [D loss: 0.670943, acc.: 68.75%] [G loss: 4.745620]\n",
      "epoch:3 step:2350 [D loss: 0.887820, acc.: 59.38%] [G loss: 2.729682]\n",
      "epoch:3 step:2351 [D loss: 0.699754, acc.: 64.06%] [G loss: 1.846221]\n",
      "epoch:3 step:2352 [D loss: 0.482324, acc.: 78.12%] [G loss: 2.776832]\n",
      "epoch:3 step:2353 [D loss: 0.542862, acc.: 70.31%] [G loss: 2.305958]\n",
      "epoch:3 step:2354 [D loss: 0.410351, acc.: 85.16%] [G loss: 2.057888]\n",
      "epoch:3 step:2355 [D loss: 0.361051, acc.: 84.38%] [G loss: 2.650806]\n",
      "epoch:3 step:2356 [D loss: 0.415303, acc.: 80.47%] [G loss: 3.894893]\n",
      "epoch:3 step:2357 [D loss: 0.449908, acc.: 81.25%] [G loss: 3.140110]\n",
      "epoch:3 step:2358 [D loss: 0.342661, acc.: 89.06%] [G loss: 2.284323]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:3 step:2359 [D loss: 0.392522, acc.: 81.25%] [G loss: 2.185582]\n",
      "epoch:3 step:2360 [D loss: 0.351455, acc.: 85.94%] [G loss: 2.896687]\n",
      "epoch:3 step:2361 [D loss: 0.305812, acc.: 89.06%] [G loss: 2.351191]\n",
      "epoch:3 step:2362 [D loss: 0.372827, acc.: 84.38%] [G loss: 3.837107]\n",
      "epoch:3 step:2363 [D loss: 0.430791, acc.: 79.69%] [G loss: 2.661316]\n",
      "epoch:3 step:2364 [D loss: 0.435156, acc.: 80.47%] [G loss: 2.266973]\n",
      "epoch:3 step:2365 [D loss: 0.386995, acc.: 86.72%] [G loss: 2.083971]\n",
      "epoch:3 step:2366 [D loss: 0.404935, acc.: 82.03%] [G loss: 2.692656]\n",
      "epoch:3 step:2367 [D loss: 0.429717, acc.: 80.47%] [G loss: 2.358082]\n",
      "epoch:3 step:2368 [D loss: 0.408457, acc.: 84.38%] [G loss: 2.312078]\n",
      "epoch:3 step:2369 [D loss: 0.434349, acc.: 80.47%] [G loss: 2.696482]\n",
      "epoch:3 step:2370 [D loss: 0.354062, acc.: 85.16%] [G loss: 2.593158]\n",
      "epoch:3 step:2371 [D loss: 0.406039, acc.: 82.03%] [G loss: 2.681446]\n",
      "epoch:3 step:2372 [D loss: 0.425873, acc.: 83.59%] [G loss: 2.635798]\n",
      "epoch:3 step:2373 [D loss: 0.350911, acc.: 89.84%] [G loss: 2.463384]\n",
      "epoch:3 step:2374 [D loss: 0.394167, acc.: 85.16%] [G loss: 2.795037]\n",
      "epoch:3 step:2375 [D loss: 0.434348, acc.: 81.25%] [G loss: 2.758598]\n",
      "epoch:3 step:2376 [D loss: 0.335353, acc.: 89.84%] [G loss: 3.173628]\n",
      "epoch:3 step:2377 [D loss: 0.368475, acc.: 85.94%] [G loss: 2.672146]\n",
      "epoch:3 step:2378 [D loss: 0.389000, acc.: 82.03%] [G loss: 2.268356]\n",
      "epoch:3 step:2379 [D loss: 0.476316, acc.: 76.56%] [G loss: 3.629238]\n",
      "epoch:3 step:2380 [D loss: 0.420662, acc.: 82.03%] [G loss: 2.912912]\n",
      "epoch:3 step:2381 [D loss: 0.566143, acc.: 69.53%] [G loss: 4.847650]\n",
      "epoch:3 step:2382 [D loss: 1.002656, acc.: 67.97%] [G loss: 4.028469]\n",
      "epoch:3 step:2383 [D loss: 0.659071, acc.: 64.06%] [G loss: 1.879258]\n",
      "epoch:3 step:2384 [D loss: 0.506384, acc.: 78.91%] [G loss: 3.297087]\n",
      "epoch:3 step:2385 [D loss: 0.447549, acc.: 77.34%] [G loss: 2.437443]\n",
      "epoch:3 step:2386 [D loss: 0.442188, acc.: 78.91%] [G loss: 1.945080]\n",
      "epoch:3 step:2387 [D loss: 0.394471, acc.: 84.38%] [G loss: 3.803352]\n",
      "epoch:3 step:2388 [D loss: 0.334208, acc.: 89.06%] [G loss: 2.314805]\n",
      "epoch:3 step:2389 [D loss: 0.370768, acc.: 82.81%] [G loss: 2.363604]\n",
      "epoch:3 step:2390 [D loss: 0.375474, acc.: 85.16%] [G loss: 2.772982]\n",
      "epoch:3 step:2391 [D loss: 0.373232, acc.: 88.28%] [G loss: 2.167111]\n",
      "epoch:3 step:2392 [D loss: 0.575060, acc.: 75.00%] [G loss: 2.337615]\n",
      "epoch:3 step:2393 [D loss: 0.552197, acc.: 77.34%] [G loss: 2.282185]\n",
      "epoch:3 step:2394 [D loss: 0.427878, acc.: 81.25%] [G loss: 2.315708]\n",
      "epoch:3 step:2395 [D loss: 0.441751, acc.: 78.91%] [G loss: 2.514269]\n",
      "epoch:3 step:2396 [D loss: 0.388482, acc.: 88.28%] [G loss: 2.173107]\n",
      "epoch:3 step:2397 [D loss: 0.378460, acc.: 89.06%] [G loss: 2.268993]\n",
      "epoch:3 step:2398 [D loss: 0.457550, acc.: 82.81%] [G loss: 2.113492]\n",
      "epoch:3 step:2399 [D loss: 0.466528, acc.: 78.12%] [G loss: 2.239442]\n",
      "epoch:3 step:2400 [D loss: 0.416015, acc.: 82.81%] [G loss: 2.280353]\n",
      "##############\n",
      "[0.84473633 0.83182305 0.80729103 0.82895811 0.76379589 0.8233947\n",
      " 0.90258543 0.81526219 0.81425935 0.80612662]\n",
      "##########\n",
      "epoch:3 step:2401 [D loss: 0.326843, acc.: 89.06%] [G loss: 2.592103]\n",
      "epoch:3 step:2402 [D loss: 0.338917, acc.: 87.50%] [G loss: 2.739229]\n",
      "epoch:3 step:2403 [D loss: 0.389925, acc.: 88.28%] [G loss: 2.161476]\n",
      "epoch:3 step:2404 [D loss: 0.374849, acc.: 87.50%] [G loss: 2.572845]\n",
      "epoch:3 step:2405 [D loss: 0.372629, acc.: 87.50%] [G loss: 3.143553]\n",
      "epoch:3 step:2406 [D loss: 0.400267, acc.: 82.03%] [G loss: 2.522242]\n",
      "epoch:3 step:2407 [D loss: 0.472423, acc.: 75.78%] [G loss: 3.165296]\n",
      "epoch:3 step:2408 [D loss: 0.446976, acc.: 78.91%] [G loss: 3.849718]\n",
      "epoch:3 step:2409 [D loss: 0.410007, acc.: 84.38%] [G loss: 2.922889]\n",
      "epoch:3 step:2410 [D loss: 0.391074, acc.: 85.16%] [G loss: 2.867094]\n",
      "epoch:3 step:2411 [D loss: 0.392515, acc.: 82.03%] [G loss: 2.518905]\n",
      "epoch:3 step:2412 [D loss: 0.461548, acc.: 78.91%] [G loss: 2.223631]\n",
      "epoch:3 step:2413 [D loss: 0.403173, acc.: 79.69%] [G loss: 2.537509]\n",
      "epoch:3 step:2414 [D loss: 0.494780, acc.: 75.78%] [G loss: 2.117631]\n",
      "epoch:3 step:2415 [D loss: 0.483541, acc.: 78.91%] [G loss: 2.220164]\n",
      "epoch:3 step:2416 [D loss: 0.598741, acc.: 71.09%] [G loss: 3.215405]\n",
      "epoch:3 step:2417 [D loss: 0.561627, acc.: 71.88%] [G loss: 3.322154]\n",
      "epoch:3 step:2418 [D loss: 0.752865, acc.: 68.75%] [G loss: 3.013888]\n",
      "epoch:3 step:2419 [D loss: 0.346671, acc.: 88.28%] [G loss: 3.154440]\n",
      "epoch:3 step:2420 [D loss: 0.361454, acc.: 84.38%] [G loss: 3.108029]\n",
      "epoch:3 step:2421 [D loss: 0.409400, acc.: 83.59%] [G loss: 2.002606]\n",
      "epoch:3 step:2422 [D loss: 0.395091, acc.: 85.16%] [G loss: 2.570030]\n",
      "epoch:3 step:2423 [D loss: 0.416758, acc.: 80.47%] [G loss: 2.858779]\n",
      "epoch:3 step:2424 [D loss: 0.429116, acc.: 82.81%] [G loss: 1.856109]\n",
      "epoch:3 step:2425 [D loss: 0.476270, acc.: 74.22%] [G loss: 1.676252]\n",
      "epoch:3 step:2426 [D loss: 0.488741, acc.: 76.56%] [G loss: 1.935560]\n",
      "epoch:3 step:2427 [D loss: 0.510278, acc.: 78.91%] [G loss: 2.220662]\n",
      "epoch:3 step:2428 [D loss: 0.389889, acc.: 85.94%] [G loss: 3.109768]\n",
      "epoch:3 step:2429 [D loss: 0.496069, acc.: 77.34%] [G loss: 2.185614]\n",
      "epoch:3 step:2430 [D loss: 0.454291, acc.: 84.38%] [G loss: 2.506539]\n",
      "epoch:3 step:2431 [D loss: 0.448747, acc.: 75.78%] [G loss: 2.647616]\n",
      "epoch:3 step:2432 [D loss: 0.387215, acc.: 87.50%] [G loss: 2.643879]\n",
      "epoch:3 step:2433 [D loss: 0.362964, acc.: 85.16%] [G loss: 2.148456]\n",
      "epoch:3 step:2434 [D loss: 0.367634, acc.: 88.28%] [G loss: 2.066977]\n",
      "epoch:3 step:2435 [D loss: 0.404320, acc.: 82.81%] [G loss: 1.849195]\n",
      "epoch:3 step:2436 [D loss: 0.359431, acc.: 89.06%] [G loss: 1.836138]\n",
      "epoch:3 step:2437 [D loss: 0.386281, acc.: 85.94%] [G loss: 2.448947]\n",
      "epoch:3 step:2438 [D loss: 0.531045, acc.: 73.44%] [G loss: 2.294106]\n",
      "epoch:3 step:2439 [D loss: 0.553954, acc.: 75.00%] [G loss: 3.740096]\n",
      "epoch:3 step:2440 [D loss: 0.670548, acc.: 65.62%] [G loss: 4.109591]\n",
      "epoch:3 step:2441 [D loss: 0.643831, acc.: 74.22%] [G loss: 1.864309]\n",
      "epoch:3 step:2442 [D loss: 0.369220, acc.: 85.94%] [G loss: 2.636087]\n",
      "epoch:3 step:2443 [D loss: 0.672726, acc.: 60.94%] [G loss: 1.862729]\n",
      "epoch:3 step:2444 [D loss: 0.435602, acc.: 80.47%] [G loss: 2.225122]\n",
      "epoch:3 step:2445 [D loss: 0.465043, acc.: 79.69%] [G loss: 2.004983]\n",
      "epoch:3 step:2446 [D loss: 0.403107, acc.: 84.38%] [G loss: 2.098975]\n",
      "epoch:3 step:2447 [D loss: 0.447183, acc.: 79.69%] [G loss: 2.139582]\n",
      "epoch:3 step:2448 [D loss: 0.411027, acc.: 82.03%] [G loss: 2.742630]\n",
      "epoch:3 step:2449 [D loss: 0.385859, acc.: 86.72%] [G loss: 2.325860]\n",
      "epoch:3 step:2450 [D loss: 0.358591, acc.: 89.84%] [G loss: 2.240061]\n",
      "epoch:3 step:2451 [D loss: 0.388968, acc.: 80.47%] [G loss: 2.318211]\n",
      "epoch:3 step:2452 [D loss: 0.427784, acc.: 81.25%] [G loss: 2.091477]\n",
      "epoch:3 step:2453 [D loss: 0.346262, acc.: 87.50%] [G loss: 3.463107]\n",
      "epoch:3 step:2454 [D loss: 0.419765, acc.: 78.12%] [G loss: 2.970379]\n",
      "epoch:3 step:2455 [D loss: 0.372240, acc.: 84.38%] [G loss: 3.230037]\n",
      "epoch:3 step:2456 [D loss: 0.368308, acc.: 82.03%] [G loss: 3.042644]\n",
      "epoch:3 step:2457 [D loss: 0.439524, acc.: 80.47%] [G loss: 2.216154]\n",
      "epoch:3 step:2458 [D loss: 0.373271, acc.: 92.19%] [G loss: 2.255174]\n",
      "epoch:3 step:2459 [D loss: 0.410507, acc.: 82.81%] [G loss: 2.364275]\n",
      "epoch:3 step:2460 [D loss: 0.367811, acc.: 88.28%] [G loss: 2.553477]\n",
      "epoch:3 step:2461 [D loss: 0.460859, acc.: 75.78%] [G loss: 2.391827]\n",
      "epoch:3 step:2462 [D loss: 0.392759, acc.: 84.38%] [G loss: 2.255034]\n",
      "epoch:3 step:2463 [D loss: 0.376244, acc.: 83.59%] [G loss: 2.865763]\n",
      "epoch:3 step:2464 [D loss: 0.331539, acc.: 85.94%] [G loss: 2.450431]\n",
      "epoch:3 step:2465 [D loss: 0.431423, acc.: 83.59%] [G loss: 2.965915]\n",
      "epoch:3 step:2466 [D loss: 0.440012, acc.: 79.69%] [G loss: 4.124581]\n",
      "epoch:3 step:2467 [D loss: 0.577969, acc.: 73.44%] [G loss: 4.571887]\n",
      "epoch:3 step:2468 [D loss: 1.296678, acc.: 51.56%] [G loss: 4.040324]\n",
      "epoch:3 step:2469 [D loss: 0.713178, acc.: 68.75%] [G loss: 3.304746]\n",
      "epoch:3 step:2470 [D loss: 0.501553, acc.: 79.69%] [G loss: 2.620680]\n",
      "epoch:3 step:2471 [D loss: 0.531699, acc.: 72.66%] [G loss: 2.216414]\n",
      "epoch:3 step:2472 [D loss: 0.475519, acc.: 74.22%] [G loss: 2.789014]\n",
      "epoch:3 step:2473 [D loss: 0.551545, acc.: 74.22%] [G loss: 2.853547]\n",
      "epoch:3 step:2474 [D loss: 0.425666, acc.: 79.69%] [G loss: 2.124958]\n",
      "epoch:3 step:2475 [D loss: 0.466608, acc.: 71.88%] [G loss: 2.011532]\n",
      "epoch:3 step:2476 [D loss: 0.336747, acc.: 86.72%] [G loss: 2.641132]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:3 step:2477 [D loss: 0.438102, acc.: 81.25%] [G loss: 3.146355]\n",
      "epoch:3 step:2478 [D loss: 0.372600, acc.: 82.81%] [G loss: 2.457173]\n",
      "epoch:3 step:2479 [D loss: 0.475846, acc.: 75.78%] [G loss: 3.311866]\n",
      "epoch:3 step:2480 [D loss: 0.486326, acc.: 71.88%] [G loss: 2.523708]\n",
      "epoch:3 step:2481 [D loss: 0.492511, acc.: 75.78%] [G loss: 2.661773]\n",
      "epoch:3 step:2482 [D loss: 0.441915, acc.: 75.78%] [G loss: 2.488373]\n",
      "epoch:3 step:2483 [D loss: 0.402618, acc.: 80.47%] [G loss: 3.280067]\n",
      "epoch:3 step:2484 [D loss: 0.378722, acc.: 86.72%] [G loss: 2.425802]\n",
      "epoch:3 step:2485 [D loss: 0.411714, acc.: 83.59%] [G loss: 2.502174]\n",
      "epoch:3 step:2486 [D loss: 0.368671, acc.: 85.94%] [G loss: 2.669624]\n",
      "epoch:3 step:2487 [D loss: 0.388837, acc.: 82.81%] [G loss: 3.162919]\n",
      "epoch:3 step:2488 [D loss: 0.388397, acc.: 85.16%] [G loss: 2.912577]\n",
      "epoch:3 step:2489 [D loss: 0.430417, acc.: 83.59%] [G loss: 2.070280]\n",
      "epoch:3 step:2490 [D loss: 0.395030, acc.: 84.38%] [G loss: 2.331577]\n",
      "epoch:3 step:2491 [D loss: 0.442709, acc.: 81.25%] [G loss: 2.537076]\n",
      "epoch:3 step:2492 [D loss: 0.453691, acc.: 82.03%] [G loss: 1.854423]\n",
      "epoch:3 step:2493 [D loss: 0.370592, acc.: 85.94%] [G loss: 2.030180]\n",
      "epoch:3 step:2494 [D loss: 0.301676, acc.: 89.06%] [G loss: 2.389351]\n",
      "epoch:3 step:2495 [D loss: 0.446013, acc.: 78.12%] [G loss: 2.909024]\n",
      "epoch:3 step:2496 [D loss: 0.401717, acc.: 83.59%] [G loss: 2.361435]\n",
      "epoch:3 step:2497 [D loss: 0.525001, acc.: 76.56%] [G loss: 2.240245]\n",
      "epoch:3 step:2498 [D loss: 0.453179, acc.: 78.12%] [G loss: 2.217258]\n",
      "epoch:3 step:2499 [D loss: 0.409640, acc.: 85.94%] [G loss: 2.362786]\n",
      "epoch:3 step:2500 [D loss: 0.474602, acc.: 77.34%] [G loss: 2.563799]\n",
      "epoch:3 step:2501 [D loss: 0.538753, acc.: 76.56%] [G loss: 2.323710]\n",
      "epoch:3 step:2502 [D loss: 0.625256, acc.: 71.09%] [G loss: 2.501477]\n",
      "epoch:3 step:2503 [D loss: 0.726402, acc.: 66.41%] [G loss: 4.294711]\n",
      "epoch:3 step:2504 [D loss: 0.817998, acc.: 58.59%] [G loss: 3.232340]\n",
      "epoch:3 step:2505 [D loss: 0.320768, acc.: 90.62%] [G loss: 2.921286]\n",
      "epoch:3 step:2506 [D loss: 0.330705, acc.: 88.28%] [G loss: 2.674933]\n",
      "epoch:3 step:2507 [D loss: 0.420126, acc.: 81.25%] [G loss: 2.388239]\n",
      "epoch:3 step:2508 [D loss: 0.366621, acc.: 81.25%] [G loss: 2.760158]\n",
      "epoch:3 step:2509 [D loss: 0.357382, acc.: 86.72%] [G loss: 2.558845]\n",
      "epoch:3 step:2510 [D loss: 0.390437, acc.: 82.81%] [G loss: 2.560152]\n",
      "epoch:3 step:2511 [D loss: 0.377606, acc.: 85.16%] [G loss: 2.355344]\n",
      "epoch:3 step:2512 [D loss: 0.447657, acc.: 75.78%] [G loss: 2.038935]\n",
      "epoch:3 step:2513 [D loss: 0.402072, acc.: 85.16%] [G loss: 2.154556]\n",
      "epoch:3 step:2514 [D loss: 0.420104, acc.: 80.47%] [G loss: 2.278552]\n",
      "epoch:3 step:2515 [D loss: 0.371195, acc.: 87.50%] [G loss: 2.163868]\n",
      "epoch:3 step:2516 [D loss: 0.409331, acc.: 81.25%] [G loss: 2.248260]\n",
      "epoch:3 step:2517 [D loss: 0.378802, acc.: 82.03%] [G loss: 2.706087]\n",
      "epoch:3 step:2518 [D loss: 0.415748, acc.: 84.38%] [G loss: 2.179190]\n",
      "epoch:3 step:2519 [D loss: 0.361705, acc.: 84.38%] [G loss: 2.833431]\n",
      "epoch:3 step:2520 [D loss: 0.336881, acc.: 86.72%] [G loss: 2.528181]\n",
      "epoch:3 step:2521 [D loss: 0.394298, acc.: 85.16%] [G loss: 2.152536]\n",
      "epoch:3 step:2522 [D loss: 0.386903, acc.: 85.94%] [G loss: 2.185008]\n",
      "epoch:3 step:2523 [D loss: 0.361958, acc.: 90.62%] [G loss: 2.323626]\n",
      "epoch:3 step:2524 [D loss: 0.368250, acc.: 85.16%] [G loss: 2.497718]\n",
      "epoch:3 step:2525 [D loss: 0.373758, acc.: 85.16%] [G loss: 2.394549]\n",
      "epoch:3 step:2526 [D loss: 0.389095, acc.: 84.38%] [G loss: 2.088058]\n",
      "epoch:3 step:2527 [D loss: 0.431132, acc.: 78.91%] [G loss: 2.355363]\n",
      "epoch:3 step:2528 [D loss: 0.477090, acc.: 78.91%] [G loss: 2.374681]\n",
      "epoch:3 step:2529 [D loss: 0.412039, acc.: 85.16%] [G loss: 2.440916]\n",
      "epoch:3 step:2530 [D loss: 0.391116, acc.: 76.56%] [G loss: 2.800606]\n",
      "epoch:3 step:2531 [D loss: 0.465402, acc.: 78.12%] [G loss: 1.909855]\n",
      "epoch:3 step:2532 [D loss: 0.376601, acc.: 81.25%] [G loss: 2.221302]\n",
      "epoch:3 step:2533 [D loss: 0.437280, acc.: 81.25%] [G loss: 2.736505]\n",
      "epoch:3 step:2534 [D loss: 0.348721, acc.: 91.41%] [G loss: 2.078226]\n",
      "epoch:3 step:2535 [D loss: 0.373416, acc.: 85.16%] [G loss: 2.298285]\n",
      "epoch:3 step:2536 [D loss: 0.471126, acc.: 78.12%] [G loss: 2.954305]\n",
      "epoch:3 step:2537 [D loss: 0.407229, acc.: 84.38%] [G loss: 2.647304]\n",
      "epoch:3 step:2538 [D loss: 0.421723, acc.: 80.47%] [G loss: 2.868312]\n",
      "epoch:3 step:2539 [D loss: 0.407095, acc.: 83.59%] [G loss: 3.193628]\n",
      "epoch:3 step:2540 [D loss: 0.478549, acc.: 74.22%] [G loss: 3.767947]\n",
      "epoch:3 step:2541 [D loss: 0.454048, acc.: 78.12%] [G loss: 2.210000]\n",
      "epoch:3 step:2542 [D loss: 0.432444, acc.: 79.69%] [G loss: 3.152200]\n",
      "epoch:3 step:2543 [D loss: 0.393186, acc.: 83.59%] [G loss: 2.234722]\n",
      "epoch:3 step:2544 [D loss: 0.421988, acc.: 81.25%] [G loss: 2.757268]\n",
      "epoch:3 step:2545 [D loss: 0.373027, acc.: 85.16%] [G loss: 2.588591]\n",
      "epoch:3 step:2546 [D loss: 0.351408, acc.: 83.59%] [G loss: 3.199301]\n",
      "epoch:3 step:2547 [D loss: 0.393204, acc.: 83.59%] [G loss: 2.843548]\n",
      "epoch:3 step:2548 [D loss: 0.343568, acc.: 88.28%] [G loss: 2.178274]\n",
      "epoch:3 step:2549 [D loss: 0.397330, acc.: 80.47%] [G loss: 3.186091]\n",
      "epoch:3 step:2550 [D loss: 0.516914, acc.: 68.75%] [G loss: 3.106674]\n",
      "epoch:3 step:2551 [D loss: 0.463865, acc.: 76.56%] [G loss: 2.275243]\n",
      "epoch:3 step:2552 [D loss: 0.487232, acc.: 74.22%] [G loss: 2.735570]\n",
      "epoch:3 step:2553 [D loss: 0.335756, acc.: 85.16%] [G loss: 2.690778]\n",
      "epoch:3 step:2554 [D loss: 0.639780, acc.: 67.19%] [G loss: 3.942772]\n",
      "epoch:3 step:2555 [D loss: 0.687117, acc.: 73.44%] [G loss: 4.586115]\n",
      "epoch:3 step:2556 [D loss: 0.559491, acc.: 75.78%] [G loss: 3.745548]\n",
      "epoch:3 step:2557 [D loss: 0.438151, acc.: 77.34%] [G loss: 3.152406]\n",
      "epoch:3 step:2558 [D loss: 0.330087, acc.: 87.50%] [G loss: 3.158573]\n",
      "epoch:3 step:2559 [D loss: 0.407491, acc.: 80.47%] [G loss: 2.276356]\n",
      "epoch:3 step:2560 [D loss: 0.446576, acc.: 80.47%] [G loss: 2.347958]\n",
      "epoch:3 step:2561 [D loss: 0.373185, acc.: 88.28%] [G loss: 2.267482]\n",
      "epoch:3 step:2562 [D loss: 0.356513, acc.: 85.16%] [G loss: 3.095605]\n",
      "epoch:3 step:2563 [D loss: 0.431188, acc.: 82.81%] [G loss: 2.754880]\n",
      "epoch:3 step:2564 [D loss: 0.378949, acc.: 84.38%] [G loss: 3.091251]\n",
      "epoch:3 step:2565 [D loss: 0.289867, acc.: 91.41%] [G loss: 2.890419]\n",
      "epoch:3 step:2566 [D loss: 0.376811, acc.: 83.59%] [G loss: 2.709265]\n",
      "epoch:3 step:2567 [D loss: 0.387687, acc.: 80.47%] [G loss: 2.337144]\n",
      "epoch:3 step:2568 [D loss: 0.349670, acc.: 87.50%] [G loss: 2.553102]\n",
      "epoch:3 step:2569 [D loss: 0.314283, acc.: 90.62%] [G loss: 2.209703]\n",
      "epoch:3 step:2570 [D loss: 0.413910, acc.: 84.38%] [G loss: 2.583380]\n",
      "epoch:3 step:2571 [D loss: 0.390966, acc.: 81.25%] [G loss: 2.544485]\n",
      "epoch:3 step:2572 [D loss: 0.315105, acc.: 88.28%] [G loss: 3.199544]\n",
      "epoch:3 step:2573 [D loss: 0.342687, acc.: 85.94%] [G loss: 2.446410]\n",
      "epoch:3 step:2574 [D loss: 0.316609, acc.: 89.84%] [G loss: 3.212039]\n",
      "epoch:3 step:2575 [D loss: 0.496350, acc.: 73.44%] [G loss: 2.359713]\n",
      "epoch:3 step:2576 [D loss: 0.377206, acc.: 85.16%] [G loss: 3.371232]\n",
      "epoch:3 step:2577 [D loss: 0.343236, acc.: 90.62%] [G loss: 2.674188]\n",
      "epoch:3 step:2578 [D loss: 0.369402, acc.: 85.16%] [G loss: 2.541246]\n",
      "epoch:3 step:2579 [D loss: 0.620516, acc.: 69.53%] [G loss: 2.413890]\n",
      "epoch:3 step:2580 [D loss: 0.525596, acc.: 73.44%] [G loss: 2.645832]\n",
      "epoch:3 step:2581 [D loss: 0.476140, acc.: 73.44%] [G loss: 2.500553]\n",
      "epoch:3 step:2582 [D loss: 0.535525, acc.: 78.91%] [G loss: 2.812995]\n",
      "epoch:3 step:2583 [D loss: 0.612300, acc.: 64.06%] [G loss: 2.975115]\n",
      "epoch:3 step:2584 [D loss: 0.487822, acc.: 78.91%] [G loss: 1.674483]\n",
      "epoch:3 step:2585 [D loss: 0.410111, acc.: 82.81%] [G loss: 2.997295]\n",
      "epoch:3 step:2586 [D loss: 0.464535, acc.: 75.78%] [G loss: 2.264536]\n",
      "epoch:3 step:2587 [D loss: 0.425124, acc.: 78.12%] [G loss: 2.838314]\n",
      "epoch:3 step:2588 [D loss: 0.337243, acc.: 84.38%] [G loss: 2.696369]\n",
      "epoch:3 step:2589 [D loss: 0.329149, acc.: 88.28%] [G loss: 2.292149]\n",
      "epoch:3 step:2590 [D loss: 0.432057, acc.: 79.69%] [G loss: 2.273047]\n",
      "epoch:3 step:2591 [D loss: 0.388101, acc.: 85.94%] [G loss: 2.314734]\n",
      "epoch:3 step:2592 [D loss: 0.368078, acc.: 89.06%] [G loss: 2.043305]\n",
      "epoch:3 step:2593 [D loss: 0.370234, acc.: 85.16%] [G loss: 2.084951]\n",
      "epoch:3 step:2594 [D loss: 0.483570, acc.: 72.66%] [G loss: 3.064913]\n",
      "epoch:3 step:2595 [D loss: 0.329912, acc.: 87.50%] [G loss: 4.254928]\n",
      "epoch:3 step:2596 [D loss: 0.373982, acc.: 83.59%] [G loss: 2.685068]\n",
      "epoch:3 step:2597 [D loss: 0.346867, acc.: 87.50%] [G loss: 2.304869]\n",
      "epoch:3 step:2598 [D loss: 0.400082, acc.: 82.03%] [G loss: 2.352481]\n",
      "epoch:3 step:2599 [D loss: 0.376924, acc.: 85.94%] [G loss: 2.927583]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:3 step:2600 [D loss: 0.403880, acc.: 78.12%] [G loss: 3.470566]\n",
      "##############\n",
      "[0.85009906 0.84301764 0.80398189 0.79730079 0.7673162  0.84029845\n",
      " 0.88604949 0.81121731 0.83365007 0.79776242]\n",
      "##########\n",
      "epoch:3 step:2601 [D loss: 0.426825, acc.: 79.69%] [G loss: 1.879596]\n",
      "epoch:3 step:2602 [D loss: 0.414788, acc.: 82.81%] [G loss: 2.237364]\n",
      "epoch:3 step:2603 [D loss: 0.386733, acc.: 84.38%] [G loss: 2.713680]\n",
      "epoch:3 step:2604 [D loss: 0.431011, acc.: 80.47%] [G loss: 2.286763]\n",
      "epoch:3 step:2605 [D loss: 0.405891, acc.: 84.38%] [G loss: 2.123046]\n",
      "epoch:3 step:2606 [D loss: 0.318527, acc.: 93.75%] [G loss: 2.669030]\n",
      "epoch:3 step:2607 [D loss: 0.362854, acc.: 82.03%] [G loss: 3.701318]\n",
      "epoch:3 step:2608 [D loss: 0.432513, acc.: 81.25%] [G loss: 2.722105]\n",
      "epoch:3 step:2609 [D loss: 0.372433, acc.: 82.03%] [G loss: 4.095130]\n",
      "epoch:3 step:2610 [D loss: 0.504269, acc.: 79.69%] [G loss: 2.497522]\n",
      "epoch:3 step:2611 [D loss: 0.491905, acc.: 77.34%] [G loss: 3.855131]\n",
      "epoch:3 step:2612 [D loss: 0.566037, acc.: 72.66%] [G loss: 2.507983]\n",
      "epoch:3 step:2613 [D loss: 0.439035, acc.: 79.69%] [G loss: 3.063885]\n",
      "epoch:3 step:2614 [D loss: 0.432890, acc.: 80.47%] [G loss: 1.989084]\n",
      "epoch:3 step:2615 [D loss: 0.359895, acc.: 88.28%] [G loss: 1.937092]\n",
      "epoch:3 step:2616 [D loss: 0.428854, acc.: 79.69%] [G loss: 2.218422]\n",
      "epoch:3 step:2617 [D loss: 0.327224, acc.: 89.06%] [G loss: 2.803672]\n",
      "epoch:3 step:2618 [D loss: 0.417381, acc.: 83.59%] [G loss: 2.218099]\n",
      "epoch:3 step:2619 [D loss: 0.356907, acc.: 86.72%] [G loss: 2.509254]\n",
      "epoch:3 step:2620 [D loss: 0.396369, acc.: 79.69%] [G loss: 3.550456]\n",
      "epoch:3 step:2621 [D loss: 0.397259, acc.: 82.03%] [G loss: 3.179682]\n",
      "epoch:3 step:2622 [D loss: 0.458792, acc.: 79.69%] [G loss: 3.068700]\n",
      "epoch:3 step:2623 [D loss: 0.360840, acc.: 87.50%] [G loss: 3.067968]\n",
      "epoch:3 step:2624 [D loss: 0.356547, acc.: 83.59%] [G loss: 2.524970]\n",
      "epoch:3 step:2625 [D loss: 0.414882, acc.: 83.59%] [G loss: 2.444704]\n",
      "epoch:3 step:2626 [D loss: 0.355186, acc.: 89.06%] [G loss: 2.234178]\n",
      "epoch:3 step:2627 [D loss: 0.455399, acc.: 79.69%] [G loss: 1.900158]\n",
      "epoch:3 step:2628 [D loss: 0.387487, acc.: 85.16%] [G loss: 3.259447]\n",
      "epoch:3 step:2629 [D loss: 0.391537, acc.: 81.25%] [G loss: 2.448861]\n",
      "epoch:3 step:2630 [D loss: 0.401059, acc.: 82.03%] [G loss: 3.706698]\n",
      "epoch:3 step:2631 [D loss: 0.467323, acc.: 75.78%] [G loss: 2.435122]\n",
      "epoch:3 step:2632 [D loss: 0.382881, acc.: 82.03%] [G loss: 2.585197]\n",
      "epoch:3 step:2633 [D loss: 0.404139, acc.: 81.25%] [G loss: 2.102402]\n",
      "epoch:3 step:2634 [D loss: 0.381457, acc.: 85.16%] [G loss: 2.632591]\n",
      "epoch:3 step:2635 [D loss: 0.467911, acc.: 74.22%] [G loss: 1.935318]\n",
      "epoch:3 step:2636 [D loss: 0.329267, acc.: 87.50%] [G loss: 2.862162]\n",
      "epoch:3 step:2637 [D loss: 0.262009, acc.: 92.19%] [G loss: 3.983531]\n",
      "epoch:3 step:2638 [D loss: 0.342934, acc.: 85.16%] [G loss: 3.498914]\n",
      "epoch:3 step:2639 [D loss: 0.328133, acc.: 89.06%] [G loss: 2.851079]\n",
      "epoch:3 step:2640 [D loss: 0.297604, acc.: 90.62%] [G loss: 3.398462]\n",
      "epoch:3 step:2641 [D loss: 0.329611, acc.: 87.50%] [G loss: 3.061853]\n",
      "epoch:3 step:2642 [D loss: 0.382545, acc.: 80.47%] [G loss: 2.024883]\n",
      "epoch:3 step:2643 [D loss: 0.467482, acc.: 81.25%] [G loss: 2.941867]\n",
      "epoch:3 step:2644 [D loss: 0.434574, acc.: 80.47%] [G loss: 3.009000]\n",
      "epoch:3 step:2645 [D loss: 0.492181, acc.: 74.22%] [G loss: 4.078649]\n",
      "epoch:3 step:2646 [D loss: 0.414572, acc.: 82.81%] [G loss: 3.753820]\n",
      "epoch:3 step:2647 [D loss: 0.518918, acc.: 74.22%] [G loss: 2.460203]\n",
      "epoch:3 step:2648 [D loss: 0.459416, acc.: 74.22%] [G loss: 1.975288]\n",
      "epoch:3 step:2649 [D loss: 0.422218, acc.: 82.81%] [G loss: 2.178090]\n",
      "epoch:3 step:2650 [D loss: 0.351964, acc.: 88.28%] [G loss: 2.151100]\n",
      "epoch:3 step:2651 [D loss: 0.390516, acc.: 79.69%] [G loss: 2.905964]\n",
      "epoch:3 step:2652 [D loss: 0.441226, acc.: 76.56%] [G loss: 2.689517]\n",
      "epoch:3 step:2653 [D loss: 0.344123, acc.: 89.84%] [G loss: 2.114974]\n",
      "epoch:3 step:2654 [D loss: 0.396677, acc.: 84.38%] [G loss: 2.535071]\n",
      "epoch:3 step:2655 [D loss: 0.367725, acc.: 80.47%] [G loss: 3.309033]\n",
      "epoch:3 step:2656 [D loss: 0.363902, acc.: 86.72%] [G loss: 2.131418]\n",
      "epoch:3 step:2657 [D loss: 0.377100, acc.: 82.03%] [G loss: 2.646438]\n",
      "epoch:3 step:2658 [D loss: 0.421150, acc.: 79.69%] [G loss: 3.878348]\n",
      "epoch:3 step:2659 [D loss: 0.482914, acc.: 78.91%] [G loss: 4.450082]\n",
      "epoch:3 step:2660 [D loss: 0.963954, acc.: 67.97%] [G loss: 5.823420]\n",
      "epoch:3 step:2661 [D loss: 1.493831, acc.: 58.59%] [G loss: 3.179446]\n",
      "epoch:3 step:2662 [D loss: 0.474066, acc.: 77.34%] [G loss: 3.595545]\n",
      "epoch:3 step:2663 [D loss: 0.483611, acc.: 72.66%] [G loss: 2.205686]\n",
      "epoch:3 step:2664 [D loss: 0.273081, acc.: 89.06%] [G loss: 2.928094]\n",
      "epoch:3 step:2665 [D loss: 0.392216, acc.: 84.38%] [G loss: 2.459745]\n",
      "epoch:3 step:2666 [D loss: 0.388356, acc.: 80.47%] [G loss: 2.538770]\n",
      "epoch:3 step:2667 [D loss: 0.366001, acc.: 85.94%] [G loss: 1.818505]\n",
      "epoch:3 step:2668 [D loss: 0.444620, acc.: 78.91%] [G loss: 2.281651]\n",
      "epoch:3 step:2669 [D loss: 0.350005, acc.: 90.62%] [G loss: 2.827004]\n",
      "epoch:3 step:2670 [D loss: 0.445346, acc.: 80.47%] [G loss: 2.276710]\n",
      "epoch:3 step:2671 [D loss: 0.382401, acc.: 83.59%] [G loss: 2.686909]\n",
      "epoch:3 step:2672 [D loss: 0.413645, acc.: 85.16%] [G loss: 2.009222]\n",
      "epoch:3 step:2673 [D loss: 0.347270, acc.: 88.28%] [G loss: 2.135622]\n",
      "epoch:3 step:2674 [D loss: 0.404837, acc.: 81.25%] [G loss: 1.826635]\n",
      "epoch:3 step:2675 [D loss: 0.350036, acc.: 87.50%] [G loss: 2.961579]\n",
      "epoch:3 step:2676 [D loss: 0.360700, acc.: 89.06%] [G loss: 2.606097]\n",
      "epoch:3 step:2677 [D loss: 0.417464, acc.: 83.59%] [G loss: 2.649496]\n",
      "epoch:3 step:2678 [D loss: 0.377136, acc.: 85.94%] [G loss: 2.035588]\n",
      "epoch:3 step:2679 [D loss: 0.389799, acc.: 85.94%] [G loss: 2.313840]\n",
      "epoch:3 step:2680 [D loss: 0.384263, acc.: 84.38%] [G loss: 2.533185]\n",
      "epoch:3 step:2681 [D loss: 0.409452, acc.: 81.25%] [G loss: 2.388495]\n",
      "epoch:3 step:2682 [D loss: 0.432201, acc.: 84.38%] [G loss: 2.211364]\n",
      "epoch:3 step:2683 [D loss: 0.471126, acc.: 73.44%] [G loss: 3.302017]\n",
      "epoch:3 step:2684 [D loss: 0.453508, acc.: 83.59%] [G loss: 2.774308]\n",
      "epoch:3 step:2685 [D loss: 0.340542, acc.: 87.50%] [G loss: 2.986253]\n",
      "epoch:3 step:2686 [D loss: 0.390331, acc.: 87.50%] [G loss: 2.522571]\n",
      "epoch:3 step:2687 [D loss: 0.452901, acc.: 75.78%] [G loss: 3.221948]\n",
      "epoch:3 step:2688 [D loss: 0.416379, acc.: 80.47%] [G loss: 2.835576]\n",
      "epoch:3 step:2689 [D loss: 0.383987, acc.: 85.94%] [G loss: 2.916789]\n",
      "epoch:3 step:2690 [D loss: 0.351606, acc.: 86.72%] [G loss: 2.525435]\n",
      "epoch:3 step:2691 [D loss: 0.353348, acc.: 85.94%] [G loss: 3.374645]\n",
      "epoch:3 step:2692 [D loss: 0.391270, acc.: 84.38%] [G loss: 3.200974]\n",
      "epoch:3 step:2693 [D loss: 0.421753, acc.: 80.47%] [G loss: 3.036759]\n",
      "epoch:3 step:2694 [D loss: 0.403347, acc.: 82.03%] [G loss: 3.590952]\n",
      "epoch:3 step:2695 [D loss: 0.527452, acc.: 70.31%] [G loss: 3.347612]\n",
      "epoch:3 step:2696 [D loss: 0.330549, acc.: 89.06%] [G loss: 2.250302]\n",
      "epoch:3 step:2697 [D loss: 0.401217, acc.: 80.47%] [G loss: 3.103385]\n",
      "epoch:3 step:2698 [D loss: 0.336982, acc.: 87.50%] [G loss: 2.461031]\n",
      "epoch:3 step:2699 [D loss: 0.375774, acc.: 82.81%] [G loss: 2.068142]\n",
      "epoch:3 step:2700 [D loss: 0.336412, acc.: 89.84%] [G loss: 2.827427]\n",
      "epoch:3 step:2701 [D loss: 0.398348, acc.: 82.03%] [G loss: 2.751532]\n",
      "epoch:3 step:2702 [D loss: 0.379240, acc.: 88.28%] [G loss: 3.036787]\n",
      "epoch:3 step:2703 [D loss: 0.502796, acc.: 71.88%] [G loss: 3.485788]\n",
      "epoch:3 step:2704 [D loss: 0.574975, acc.: 71.09%] [G loss: 3.593894]\n",
      "epoch:3 step:2705 [D loss: 0.674871, acc.: 71.09%] [G loss: 3.225157]\n",
      "epoch:3 step:2706 [D loss: 0.392105, acc.: 81.25%] [G loss: 2.882247]\n",
      "epoch:3 step:2707 [D loss: 0.379014, acc.: 83.59%] [G loss: 2.674950]\n",
      "epoch:3 step:2708 [D loss: 0.430927, acc.: 78.91%] [G loss: 3.423357]\n",
      "epoch:3 step:2709 [D loss: 0.383715, acc.: 84.38%] [G loss: 2.403845]\n",
      "epoch:3 step:2710 [D loss: 0.347116, acc.: 86.72%] [G loss: 3.417124]\n",
      "epoch:3 step:2711 [D loss: 0.398532, acc.: 81.25%] [G loss: 3.008142]\n",
      "epoch:3 step:2712 [D loss: 0.460088, acc.: 77.34%] [G loss: 3.444134]\n",
      "epoch:3 step:2713 [D loss: 0.379629, acc.: 82.03%] [G loss: 3.281682]\n",
      "epoch:3 step:2714 [D loss: 0.395342, acc.: 81.25%] [G loss: 2.576359]\n",
      "epoch:3 step:2715 [D loss: 0.440358, acc.: 81.25%] [G loss: 2.349486]\n",
      "epoch:3 step:2716 [D loss: 0.356916, acc.: 85.94%] [G loss: 2.796185]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:3 step:2717 [D loss: 0.423193, acc.: 78.91%] [G loss: 2.164953]\n",
      "epoch:3 step:2718 [D loss: 0.495202, acc.: 74.22%] [G loss: 2.786851]\n",
      "epoch:3 step:2719 [D loss: 0.377390, acc.: 82.03%] [G loss: 2.340896]\n",
      "epoch:3 step:2720 [D loss: 0.420820, acc.: 85.94%] [G loss: 2.285403]\n",
      "epoch:3 step:2721 [D loss: 0.388803, acc.: 83.59%] [G loss: 2.814957]\n",
      "epoch:3 step:2722 [D loss: 0.531252, acc.: 78.91%] [G loss: 3.840129]\n",
      "epoch:3 step:2723 [D loss: 0.756031, acc.: 61.72%] [G loss: 3.487935]\n",
      "epoch:3 step:2724 [D loss: 0.537594, acc.: 69.53%] [G loss: 1.758934]\n",
      "epoch:3 step:2725 [D loss: 0.410830, acc.: 85.94%] [G loss: 3.043436]\n",
      "epoch:3 step:2726 [D loss: 0.470438, acc.: 80.47%] [G loss: 2.284962]\n",
      "epoch:3 step:2727 [D loss: 0.451793, acc.: 75.00%] [G loss: 3.138819]\n",
      "epoch:3 step:2728 [D loss: 0.480266, acc.: 72.66%] [G loss: 1.562226]\n",
      "epoch:3 step:2729 [D loss: 0.362414, acc.: 89.84%] [G loss: 2.065815]\n",
      "epoch:3 step:2730 [D loss: 0.405976, acc.: 79.69%] [G loss: 2.613565]\n",
      "epoch:3 step:2731 [D loss: 0.443394, acc.: 81.25%] [G loss: 2.308173]\n",
      "epoch:3 step:2732 [D loss: 0.406099, acc.: 85.16%] [G loss: 2.488804]\n",
      "epoch:3 step:2733 [D loss: 0.403135, acc.: 85.94%] [G loss: 2.124355]\n",
      "epoch:3 step:2734 [D loss: 0.429405, acc.: 85.94%] [G loss: 2.145335]\n",
      "epoch:3 step:2735 [D loss: 0.347904, acc.: 89.84%] [G loss: 2.640401]\n",
      "epoch:3 step:2736 [D loss: 0.382678, acc.: 84.38%] [G loss: 2.261995]\n",
      "epoch:3 step:2737 [D loss: 0.439757, acc.: 81.25%] [G loss: 2.130737]\n",
      "epoch:3 step:2738 [D loss: 0.419255, acc.: 79.69%] [G loss: 2.429606]\n",
      "epoch:3 step:2739 [D loss: 0.467224, acc.: 77.34%] [G loss: 2.199673]\n",
      "epoch:3 step:2740 [D loss: 0.365808, acc.: 85.16%] [G loss: 2.923715]\n",
      "epoch:3 step:2741 [D loss: 0.368879, acc.: 84.38%] [G loss: 2.934000]\n",
      "epoch:3 step:2742 [D loss: 0.508623, acc.: 73.44%] [G loss: 1.912770]\n",
      "epoch:3 step:2743 [D loss: 0.301079, acc.: 89.84%] [G loss: 3.063608]\n",
      "epoch:3 step:2744 [D loss: 0.496820, acc.: 74.22%] [G loss: 1.953434]\n",
      "epoch:3 step:2745 [D loss: 0.402564, acc.: 81.25%] [G loss: 1.918509]\n",
      "epoch:3 step:2746 [D loss: 0.384271, acc.: 85.16%] [G loss: 3.089605]\n",
      "epoch:3 step:2747 [D loss: 0.405567, acc.: 84.38%] [G loss: 2.008418]\n",
      "epoch:3 step:2748 [D loss: 0.412555, acc.: 85.16%] [G loss: 2.516073]\n",
      "epoch:3 step:2749 [D loss: 0.495521, acc.: 72.66%] [G loss: 3.623811]\n",
      "epoch:3 step:2750 [D loss: 0.534673, acc.: 70.31%] [G loss: 3.150562]\n",
      "epoch:3 step:2751 [D loss: 0.489386, acc.: 75.78%] [G loss: 2.540964]\n",
      "epoch:3 step:2752 [D loss: 0.375877, acc.: 82.03%] [G loss: 2.284675]\n",
      "epoch:3 step:2753 [D loss: 0.482538, acc.: 78.12%] [G loss: 2.659892]\n",
      "epoch:3 step:2754 [D loss: 0.577852, acc.: 72.66%] [G loss: 3.980967]\n",
      "epoch:3 step:2755 [D loss: 0.958284, acc.: 62.50%] [G loss: 5.252312]\n",
      "epoch:3 step:2756 [D loss: 0.672813, acc.: 63.28%] [G loss: 2.515201]\n",
      "epoch:3 step:2757 [D loss: 0.409024, acc.: 81.25%] [G loss: 2.427803]\n",
      "epoch:3 step:2758 [D loss: 0.550978, acc.: 74.22%] [G loss: 1.926623]\n",
      "epoch:3 step:2759 [D loss: 0.340690, acc.: 86.72%] [G loss: 2.898470]\n",
      "epoch:3 step:2760 [D loss: 0.335083, acc.: 88.28%] [G loss: 3.393987]\n",
      "epoch:3 step:2761 [D loss: 0.451408, acc.: 78.91%] [G loss: 2.171146]\n",
      "epoch:3 step:2762 [D loss: 0.301109, acc.: 89.84%] [G loss: 2.134517]\n",
      "epoch:3 step:2763 [D loss: 0.322582, acc.: 87.50%] [G loss: 3.006767]\n",
      "epoch:3 step:2764 [D loss: 0.493573, acc.: 78.12%] [G loss: 1.983766]\n",
      "epoch:3 step:2765 [D loss: 0.417371, acc.: 80.47%] [G loss: 2.195446]\n",
      "epoch:3 step:2766 [D loss: 0.382840, acc.: 87.50%] [G loss: 2.583368]\n",
      "epoch:3 step:2767 [D loss: 0.396693, acc.: 87.50%] [G loss: 2.803573]\n",
      "epoch:3 step:2768 [D loss: 0.426399, acc.: 82.03%] [G loss: 2.507991]\n",
      "epoch:3 step:2769 [D loss: 0.378163, acc.: 85.16%] [G loss: 1.926249]\n",
      "epoch:3 step:2770 [D loss: 0.454811, acc.: 76.56%] [G loss: 1.981929]\n",
      "epoch:3 step:2771 [D loss: 0.453029, acc.: 78.12%] [G loss: 1.871292]\n",
      "epoch:3 step:2772 [D loss: 0.468197, acc.: 78.91%] [G loss: 1.823750]\n",
      "epoch:3 step:2773 [D loss: 0.534713, acc.: 71.09%] [G loss: 2.417255]\n",
      "epoch:3 step:2774 [D loss: 0.501609, acc.: 74.22%] [G loss: 3.560250]\n",
      "epoch:3 step:2775 [D loss: 0.402723, acc.: 81.25%] [G loss: 2.779915]\n",
      "epoch:3 step:2776 [D loss: 0.373843, acc.: 82.81%] [G loss: 3.650795]\n",
      "epoch:3 step:2777 [D loss: 0.347169, acc.: 86.72%] [G loss: 3.646269]\n",
      "epoch:3 step:2778 [D loss: 0.482959, acc.: 78.12%] [G loss: 3.740897]\n",
      "epoch:3 step:2779 [D loss: 0.759726, acc.: 57.81%] [G loss: 3.112078]\n",
      "epoch:3 step:2780 [D loss: 0.794824, acc.: 72.66%] [G loss: 4.197973]\n",
      "epoch:3 step:2781 [D loss: 0.746214, acc.: 64.06%] [G loss: 1.865211]\n",
      "epoch:3 step:2782 [D loss: 0.403264, acc.: 80.47%] [G loss: 2.976184]\n",
      "epoch:3 step:2783 [D loss: 0.562907, acc.: 75.78%] [G loss: 2.208894]\n",
      "epoch:3 step:2784 [D loss: 0.396150, acc.: 81.25%] [G loss: 2.456655]\n",
      "epoch:3 step:2785 [D loss: 0.544173, acc.: 72.66%] [G loss: 2.776192]\n",
      "epoch:3 step:2786 [D loss: 0.444578, acc.: 76.56%] [G loss: 2.454691]\n",
      "epoch:3 step:2787 [D loss: 0.460439, acc.: 77.34%] [G loss: 2.096524]\n",
      "epoch:3 step:2788 [D loss: 0.456400, acc.: 76.56%] [G loss: 1.941992]\n",
      "epoch:3 step:2789 [D loss: 0.395919, acc.: 79.69%] [G loss: 2.400419]\n",
      "epoch:3 step:2790 [D loss: 0.396116, acc.: 85.16%] [G loss: 1.987567]\n",
      "epoch:3 step:2791 [D loss: 0.390109, acc.: 84.38%] [G loss: 2.481980]\n",
      "epoch:3 step:2792 [D loss: 0.435060, acc.: 82.03%] [G loss: 2.197838]\n",
      "epoch:3 step:2793 [D loss: 0.377154, acc.: 85.16%] [G loss: 2.216147]\n",
      "epoch:3 step:2794 [D loss: 0.416992, acc.: 80.47%] [G loss: 2.168179]\n",
      "epoch:3 step:2795 [D loss: 0.457797, acc.: 75.78%] [G loss: 2.297024]\n",
      "epoch:3 step:2796 [D loss: 0.314523, acc.: 86.72%] [G loss: 4.001341]\n",
      "epoch:3 step:2797 [D loss: 0.351301, acc.: 86.72%] [G loss: 2.684515]\n",
      "epoch:3 step:2798 [D loss: 0.467690, acc.: 78.91%] [G loss: 1.765660]\n",
      "epoch:3 step:2799 [D loss: 0.357051, acc.: 82.81%] [G loss: 3.580494]\n",
      "epoch:3 step:2800 [D loss: 0.357386, acc.: 82.81%] [G loss: 3.473663]\n",
      "##############\n",
      "[0.84789346 0.85106171 0.81964309 0.80188263 0.78762912 0.82683954\n",
      " 0.90411063 0.81804753 0.82296779 0.79930359]\n",
      "##########\n",
      "epoch:3 step:2801 [D loss: 0.389613, acc.: 82.03%] [G loss: 2.318639]\n",
      "epoch:3 step:2802 [D loss: 0.478454, acc.: 76.56%] [G loss: 1.821591]\n",
      "epoch:3 step:2803 [D loss: 0.469047, acc.: 75.78%] [G loss: 2.144367]\n",
      "epoch:3 step:2804 [D loss: 0.517839, acc.: 77.34%] [G loss: 3.119473]\n",
      "epoch:3 step:2805 [D loss: 0.461012, acc.: 75.00%] [G loss: 2.577962]\n",
      "epoch:3 step:2806 [D loss: 0.407555, acc.: 79.69%] [G loss: 2.055626]\n",
      "epoch:3 step:2807 [D loss: 0.358602, acc.: 85.16%] [G loss: 2.238280]\n",
      "epoch:3 step:2808 [D loss: 0.399285, acc.: 83.59%] [G loss: 2.546948]\n",
      "epoch:3 step:2809 [D loss: 0.346902, acc.: 83.59%] [G loss: 2.697348]\n",
      "epoch:3 step:2810 [D loss: 0.409021, acc.: 83.59%] [G loss: 2.537388]\n",
      "epoch:3 step:2811 [D loss: 0.367334, acc.: 85.16%] [G loss: 2.505725]\n",
      "epoch:3 step:2812 [D loss: 0.336411, acc.: 88.28%] [G loss: 3.067145]\n",
      "epoch:3 step:2813 [D loss: 0.428017, acc.: 78.12%] [G loss: 2.850266]\n",
      "epoch:3 step:2814 [D loss: 0.314189, acc.: 89.06%] [G loss: 2.817927]\n",
      "epoch:3 step:2815 [D loss: 0.406170, acc.: 83.59%] [G loss: 2.605377]\n",
      "epoch:3 step:2816 [D loss: 0.346341, acc.: 88.28%] [G loss: 2.330436]\n",
      "epoch:3 step:2817 [D loss: 0.325096, acc.: 91.41%] [G loss: 2.012429]\n",
      "epoch:3 step:2818 [D loss: 0.406626, acc.: 81.25%] [G loss: 2.720166]\n",
      "epoch:3 step:2819 [D loss: 0.403176, acc.: 82.81%] [G loss: 2.282621]\n",
      "epoch:3 step:2820 [D loss: 0.375223, acc.: 86.72%] [G loss: 2.291959]\n",
      "epoch:3 step:2821 [D loss: 0.421969, acc.: 77.34%] [G loss: 2.111621]\n",
      "epoch:3 step:2822 [D loss: 0.436060, acc.: 78.91%] [G loss: 2.569812]\n",
      "epoch:3 step:2823 [D loss: 0.469954, acc.: 76.56%] [G loss: 2.098608]\n",
      "epoch:3 step:2824 [D loss: 0.667236, acc.: 64.84%] [G loss: 2.740877]\n",
      "epoch:3 step:2825 [D loss: 0.377051, acc.: 82.81%] [G loss: 2.359516]\n",
      "epoch:3 step:2826 [D loss: 0.450423, acc.: 78.12%] [G loss: 2.908114]\n",
      "epoch:3 step:2827 [D loss: 0.435082, acc.: 84.38%] [G loss: 2.593606]\n",
      "epoch:3 step:2828 [D loss: 0.570412, acc.: 73.44%] [G loss: 3.037977]\n",
      "epoch:3 step:2829 [D loss: 0.477378, acc.: 75.00%] [G loss: 2.304495]\n",
      "epoch:3 step:2830 [D loss: 0.408761, acc.: 82.03%] [G loss: 2.945379]\n",
      "epoch:3 step:2831 [D loss: 0.348174, acc.: 86.72%] [G loss: 2.543912]\n",
      "epoch:3 step:2832 [D loss: 0.358097, acc.: 82.81%] [G loss: 2.939515]\n",
      "epoch:3 step:2833 [D loss: 0.456796, acc.: 77.34%] [G loss: 2.677065]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:3 step:2834 [D loss: 0.365696, acc.: 83.59%] [G loss: 2.700817]\n",
      "epoch:3 step:2835 [D loss: 0.361593, acc.: 85.16%] [G loss: 1.838507]\n",
      "epoch:3 step:2836 [D loss: 0.353687, acc.: 86.72%] [G loss: 1.892764]\n",
      "epoch:3 step:2837 [D loss: 0.347729, acc.: 83.59%] [G loss: 2.655005]\n",
      "epoch:3 step:2838 [D loss: 0.303512, acc.: 85.94%] [G loss: 3.213525]\n",
      "epoch:3 step:2839 [D loss: 0.354800, acc.: 87.50%] [G loss: 2.741131]\n",
      "epoch:3 step:2840 [D loss: 0.403202, acc.: 80.47%] [G loss: 2.539704]\n",
      "epoch:3 step:2841 [D loss: 0.340546, acc.: 85.16%] [G loss: 3.049276]\n",
      "epoch:3 step:2842 [D loss: 0.460271, acc.: 72.66%] [G loss: 2.367303]\n",
      "epoch:3 step:2843 [D loss: 0.372321, acc.: 83.59%] [G loss: 2.372994]\n",
      "epoch:3 step:2844 [D loss: 0.355873, acc.: 86.72%] [G loss: 2.460366]\n",
      "epoch:3 step:2845 [D loss: 0.373676, acc.: 83.59%] [G loss: 2.731330]\n",
      "epoch:3 step:2846 [D loss: 0.451698, acc.: 80.47%] [G loss: 3.115799]\n",
      "epoch:3 step:2847 [D loss: 0.581600, acc.: 71.09%] [G loss: 5.962131]\n",
      "epoch:3 step:2848 [D loss: 1.829416, acc.: 46.09%] [G loss: 4.211020]\n",
      "epoch:3 step:2849 [D loss: 0.956192, acc.: 64.84%] [G loss: 3.281065]\n",
      "epoch:3 step:2850 [D loss: 0.512610, acc.: 73.44%] [G loss: 3.040037]\n",
      "epoch:3 step:2851 [D loss: 0.493003, acc.: 79.69%] [G loss: 2.923729]\n",
      "epoch:3 step:2852 [D loss: 0.373804, acc.: 83.59%] [G loss: 3.193051]\n",
      "epoch:3 step:2853 [D loss: 0.310760, acc.: 88.28%] [G loss: 3.186923]\n",
      "epoch:3 step:2854 [D loss: 0.416232, acc.: 79.69%] [G loss: 2.400851]\n",
      "epoch:3 step:2855 [D loss: 0.308644, acc.: 89.06%] [G loss: 2.622779]\n",
      "epoch:3 step:2856 [D loss: 0.353277, acc.: 82.03%] [G loss: 2.549550]\n",
      "epoch:3 step:2857 [D loss: 0.468998, acc.: 75.78%] [G loss: 2.241718]\n",
      "epoch:3 step:2858 [D loss: 0.395353, acc.: 78.91%] [G loss: 2.045115]\n",
      "epoch:3 step:2859 [D loss: 0.413363, acc.: 80.47%] [G loss: 2.131179]\n",
      "epoch:3 step:2860 [D loss: 0.483535, acc.: 75.00%] [G loss: 2.235511]\n",
      "epoch:3 step:2861 [D loss: 0.425583, acc.: 84.38%] [G loss: 2.413948]\n",
      "epoch:3 step:2862 [D loss: 0.365226, acc.: 84.38%] [G loss: 2.168324]\n",
      "epoch:3 step:2863 [D loss: 0.364769, acc.: 85.16%] [G loss: 2.392995]\n",
      "epoch:3 step:2864 [D loss: 0.414331, acc.: 83.59%] [G loss: 2.731342]\n",
      "epoch:3 step:2865 [D loss: 0.375195, acc.: 82.03%] [G loss: 3.038181]\n",
      "epoch:3 step:2866 [D loss: 0.377604, acc.: 85.16%] [G loss: 1.882534]\n",
      "epoch:3 step:2867 [D loss: 0.403801, acc.: 86.72%] [G loss: 1.989402]\n",
      "epoch:3 step:2868 [D loss: 0.413499, acc.: 79.69%] [G loss: 2.326849]\n",
      "epoch:3 step:2869 [D loss: 0.402440, acc.: 78.12%] [G loss: 2.259724]\n",
      "epoch:3 step:2870 [D loss: 0.421388, acc.: 82.81%] [G loss: 2.529604]\n",
      "epoch:3 step:2871 [D loss: 0.391987, acc.: 84.38%] [G loss: 2.354270]\n",
      "epoch:3 step:2872 [D loss: 0.351655, acc.: 85.16%] [G loss: 2.600533]\n",
      "epoch:3 step:2873 [D loss: 0.423888, acc.: 84.38%] [G loss: 1.678624]\n",
      "epoch:3 step:2874 [D loss: 0.401533, acc.: 85.94%] [G loss: 2.344196]\n",
      "epoch:3 step:2875 [D loss: 0.441950, acc.: 77.34%] [G loss: 2.656599]\n",
      "epoch:3 step:2876 [D loss: 0.431059, acc.: 82.81%] [G loss: 2.359730]\n",
      "epoch:3 step:2877 [D loss: 0.386956, acc.: 81.25%] [G loss: 2.807829]\n",
      "epoch:3 step:2878 [D loss: 0.345705, acc.: 82.81%] [G loss: 3.078752]\n",
      "epoch:3 step:2879 [D loss: 0.356845, acc.: 86.72%] [G loss: 2.739301]\n",
      "epoch:3 step:2880 [D loss: 0.417091, acc.: 81.25%] [G loss: 2.311280]\n",
      "epoch:3 step:2881 [D loss: 0.375496, acc.: 85.16%] [G loss: 3.840661]\n",
      "epoch:3 step:2882 [D loss: 0.439736, acc.: 75.00%] [G loss: 3.546239]\n",
      "epoch:3 step:2883 [D loss: 0.440033, acc.: 82.81%] [G loss: 2.128640]\n",
      "epoch:3 step:2884 [D loss: 0.332661, acc.: 88.28%] [G loss: 2.838927]\n",
      "epoch:3 step:2885 [D loss: 0.445424, acc.: 82.03%] [G loss: 2.391011]\n",
      "epoch:3 step:2886 [D loss: 0.388939, acc.: 82.03%] [G loss: 2.394181]\n",
      "epoch:3 step:2887 [D loss: 0.413109, acc.: 84.38%] [G loss: 1.973173]\n",
      "epoch:3 step:2888 [D loss: 0.381738, acc.: 82.81%] [G loss: 2.752441]\n",
      "epoch:3 step:2889 [D loss: 0.456618, acc.: 76.56%] [G loss: 3.853939]\n",
      "epoch:3 step:2890 [D loss: 0.610012, acc.: 67.97%] [G loss: 2.883636]\n",
      "epoch:3 step:2891 [D loss: 0.435974, acc.: 78.12%] [G loss: 2.621078]\n",
      "epoch:3 step:2892 [D loss: 0.395064, acc.: 84.38%] [G loss: 4.836320]\n",
      "epoch:3 step:2893 [D loss: 0.421098, acc.: 78.91%] [G loss: 3.439209]\n",
      "epoch:3 step:2894 [D loss: 0.290278, acc.: 85.94%] [G loss: 2.739558]\n",
      "epoch:3 step:2895 [D loss: 0.415807, acc.: 81.25%] [G loss: 4.143746]\n",
      "epoch:3 step:2896 [D loss: 0.819790, acc.: 71.88%] [G loss: 3.906183]\n",
      "epoch:3 step:2897 [D loss: 0.690369, acc.: 71.09%] [G loss: 3.708336]\n",
      "epoch:3 step:2898 [D loss: 0.459613, acc.: 79.69%] [G loss: 2.032950]\n",
      "epoch:3 step:2899 [D loss: 0.342461, acc.: 86.72%] [G loss: 2.351702]\n",
      "epoch:3 step:2900 [D loss: 0.408048, acc.: 81.25%] [G loss: 2.479144]\n",
      "epoch:3 step:2901 [D loss: 0.351604, acc.: 85.94%] [G loss: 2.027242]\n",
      "epoch:3 step:2902 [D loss: 0.423239, acc.: 80.47%] [G loss: 2.427307]\n",
      "epoch:3 step:2903 [D loss: 0.386058, acc.: 80.47%] [G loss: 2.724130]\n",
      "epoch:3 step:2904 [D loss: 0.364280, acc.: 85.16%] [G loss: 2.666263]\n",
      "epoch:3 step:2905 [D loss: 0.321911, acc.: 89.06%] [G loss: 3.039891]\n",
      "epoch:3 step:2906 [D loss: 0.373818, acc.: 83.59%] [G loss: 2.254495]\n",
      "epoch:3 step:2907 [D loss: 0.369325, acc.: 87.50%] [G loss: 3.505275]\n",
      "epoch:3 step:2908 [D loss: 0.423682, acc.: 79.69%] [G loss: 2.116223]\n",
      "epoch:3 step:2909 [D loss: 0.367933, acc.: 87.50%] [G loss: 2.370584]\n",
      "epoch:3 step:2910 [D loss: 0.431314, acc.: 78.91%] [G loss: 2.328903]\n",
      "epoch:3 step:2911 [D loss: 0.436441, acc.: 84.38%] [G loss: 3.019430]\n",
      "epoch:3 step:2912 [D loss: 0.388622, acc.: 85.94%] [G loss: 2.498186]\n",
      "epoch:3 step:2913 [D loss: 0.372895, acc.: 79.69%] [G loss: 2.668183]\n",
      "epoch:3 step:2914 [D loss: 0.460135, acc.: 80.47%] [G loss: 2.191299]\n",
      "epoch:3 step:2915 [D loss: 0.453145, acc.: 76.56%] [G loss: 2.404601]\n",
      "epoch:3 step:2916 [D loss: 0.484616, acc.: 78.12%] [G loss: 2.617378]\n",
      "epoch:3 step:2917 [D loss: 0.531307, acc.: 72.66%] [G loss: 3.411672]\n",
      "epoch:3 step:2918 [D loss: 0.717689, acc.: 71.09%] [G loss: 2.706452]\n",
      "epoch:3 step:2919 [D loss: 0.501682, acc.: 77.34%] [G loss: 2.644622]\n",
      "epoch:3 step:2920 [D loss: 0.421782, acc.: 78.12%] [G loss: 2.661504]\n",
      "epoch:3 step:2921 [D loss: 0.436582, acc.: 78.12%] [G loss: 2.519726]\n",
      "epoch:3 step:2922 [D loss: 0.352724, acc.: 85.16%] [G loss: 3.126961]\n",
      "epoch:3 step:2923 [D loss: 0.390087, acc.: 79.69%] [G loss: 2.031500]\n",
      "epoch:3 step:2924 [D loss: 0.370760, acc.: 85.16%] [G loss: 2.601445]\n",
      "epoch:3 step:2925 [D loss: 0.389326, acc.: 82.03%] [G loss: 2.243657]\n",
      "epoch:3 step:2926 [D loss: 0.412154, acc.: 81.25%] [G loss: 1.761636]\n",
      "epoch:3 step:2927 [D loss: 0.369369, acc.: 81.25%] [G loss: 2.680705]\n",
      "epoch:3 step:2928 [D loss: 0.416421, acc.: 82.81%] [G loss: 2.484253]\n",
      "epoch:3 step:2929 [D loss: 0.391999, acc.: 83.59%] [G loss: 2.432582]\n",
      "epoch:3 step:2930 [D loss: 0.453105, acc.: 77.34%] [G loss: 2.125462]\n",
      "epoch:3 step:2931 [D loss: 0.383515, acc.: 85.94%] [G loss: 3.122446]\n",
      "epoch:3 step:2932 [D loss: 0.542122, acc.: 73.44%] [G loss: 2.523461]\n",
      "epoch:3 step:2933 [D loss: 0.388853, acc.: 82.81%] [G loss: 3.626475]\n",
      "epoch:3 step:2934 [D loss: 0.397380, acc.: 82.81%] [G loss: 3.617676]\n",
      "epoch:3 step:2935 [D loss: 0.363806, acc.: 82.81%] [G loss: 2.436828]\n",
      "epoch:3 step:2936 [D loss: 0.331730, acc.: 88.28%] [G loss: 3.121037]\n",
      "epoch:3 step:2937 [D loss: 0.393628, acc.: 81.25%] [G loss: 2.426164]\n",
      "epoch:3 step:2938 [D loss: 0.433962, acc.: 82.03%] [G loss: 2.749714]\n",
      "epoch:3 step:2939 [D loss: 0.510744, acc.: 76.56%] [G loss: 2.662603]\n",
      "epoch:3 step:2940 [D loss: 0.462974, acc.: 75.78%] [G loss: 2.694607]\n",
      "epoch:3 step:2941 [D loss: 0.367883, acc.: 83.59%] [G loss: 2.491199]\n",
      "epoch:3 step:2942 [D loss: 0.381079, acc.: 81.25%] [G loss: 2.706678]\n",
      "epoch:3 step:2943 [D loss: 0.424580, acc.: 77.34%] [G loss: 2.216302]\n",
      "epoch:3 step:2944 [D loss: 0.372559, acc.: 82.81%] [G loss: 2.677699]\n",
      "epoch:3 step:2945 [D loss: 0.323945, acc.: 88.28%] [G loss: 2.933633]\n",
      "epoch:3 step:2946 [D loss: 0.344049, acc.: 87.50%] [G loss: 2.529558]\n",
      "epoch:3 step:2947 [D loss: 0.298065, acc.: 85.16%] [G loss: 2.913336]\n",
      "epoch:3 step:2948 [D loss: 0.353319, acc.: 83.59%] [G loss: 3.271272]\n",
      "epoch:3 step:2949 [D loss: 0.282704, acc.: 90.62%] [G loss: 2.360931]\n",
      "epoch:3 step:2950 [D loss: 0.368138, acc.: 82.03%] [G loss: 2.544201]\n",
      "epoch:3 step:2951 [D loss: 0.366076, acc.: 82.81%] [G loss: 2.391169]\n",
      "epoch:3 step:2952 [D loss: 0.350669, acc.: 86.72%] [G loss: 2.638981]\n",
      "epoch:3 step:2953 [D loss: 0.372584, acc.: 80.47%] [G loss: 3.121150]\n",
      "epoch:3 step:2954 [D loss: 0.328658, acc.: 85.16%] [G loss: 3.110718]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:3 step:2955 [D loss: 0.355979, acc.: 84.38%] [G loss: 3.480919]\n",
      "epoch:3 step:2956 [D loss: 0.414280, acc.: 83.59%] [G loss: 3.277208]\n",
      "epoch:3 step:2957 [D loss: 0.376886, acc.: 82.03%] [G loss: 3.110276]\n",
      "epoch:3 step:2958 [D loss: 0.437870, acc.: 80.47%] [G loss: 2.607919]\n",
      "epoch:3 step:2959 [D loss: 0.372120, acc.: 85.94%] [G loss: 3.843740]\n",
      "epoch:3 step:2960 [D loss: 0.404793, acc.: 81.25%] [G loss: 2.378902]\n",
      "epoch:3 step:2961 [D loss: 0.380500, acc.: 80.47%] [G loss: 2.780873]\n",
      "epoch:3 step:2962 [D loss: 0.361418, acc.: 83.59%] [G loss: 2.816186]\n",
      "epoch:3 step:2963 [D loss: 0.397385, acc.: 81.25%] [G loss: 2.293881]\n",
      "epoch:3 step:2964 [D loss: 0.400115, acc.: 84.38%] [G loss: 2.473330]\n",
      "epoch:3 step:2965 [D loss: 0.340915, acc.: 87.50%] [G loss: 2.635440]\n",
      "epoch:3 step:2966 [D loss: 0.360783, acc.: 82.81%] [G loss: 2.515154]\n",
      "epoch:3 step:2967 [D loss: 0.355698, acc.: 87.50%] [G loss: 2.689606]\n",
      "epoch:3 step:2968 [D loss: 0.320521, acc.: 86.72%] [G loss: 2.542894]\n",
      "epoch:3 step:2969 [D loss: 0.346549, acc.: 85.94%] [G loss: 2.575123]\n",
      "epoch:3 step:2970 [D loss: 0.367918, acc.: 85.94%] [G loss: 3.071182]\n",
      "epoch:3 step:2971 [D loss: 0.378485, acc.: 82.81%] [G loss: 2.605879]\n",
      "epoch:3 step:2972 [D loss: 0.361985, acc.: 87.50%] [G loss: 2.413037]\n",
      "epoch:3 step:2973 [D loss: 0.509191, acc.: 74.22%] [G loss: 3.887866]\n",
      "epoch:3 step:2974 [D loss: 0.612036, acc.: 72.66%] [G loss: 5.757593]\n",
      "epoch:3 step:2975 [D loss: 1.348578, acc.: 60.16%] [G loss: 4.214617]\n",
      "epoch:3 step:2976 [D loss: 1.099144, acc.: 54.69%] [G loss: 2.994316]\n",
      "epoch:3 step:2977 [D loss: 0.432791, acc.: 78.12%] [G loss: 2.811694]\n",
      "epoch:3 step:2978 [D loss: 0.567583, acc.: 75.00%] [G loss: 2.628661]\n",
      "epoch:3 step:2979 [D loss: 0.423986, acc.: 82.03%] [G loss: 2.791898]\n",
      "epoch:3 step:2980 [D loss: 0.523537, acc.: 71.09%] [G loss: 2.883754]\n",
      "epoch:3 step:2981 [D loss: 0.461783, acc.: 76.56%] [G loss: 3.333899]\n",
      "epoch:3 step:2982 [D loss: 0.465159, acc.: 75.00%] [G loss: 2.203893]\n",
      "epoch:3 step:2983 [D loss: 0.343597, acc.: 86.72%] [G loss: 2.124699]\n",
      "epoch:3 step:2984 [D loss: 0.362997, acc.: 86.72%] [G loss: 2.517269]\n",
      "epoch:3 step:2985 [D loss: 0.400457, acc.: 85.16%] [G loss: 2.516552]\n",
      "epoch:3 step:2986 [D loss: 0.345984, acc.: 84.38%] [G loss: 2.931746]\n",
      "epoch:3 step:2987 [D loss: 0.342312, acc.: 86.72%] [G loss: 2.664663]\n",
      "epoch:3 step:2988 [D loss: 0.388813, acc.: 85.94%] [G loss: 2.274616]\n",
      "epoch:3 step:2989 [D loss: 0.366402, acc.: 85.94%] [G loss: 2.138025]\n",
      "epoch:3 step:2990 [D loss: 0.388190, acc.: 86.72%] [G loss: 2.794208]\n",
      "epoch:3 step:2991 [D loss: 0.423508, acc.: 76.56%] [G loss: 2.220492]\n",
      "epoch:3 step:2992 [D loss: 0.383751, acc.: 80.47%] [G loss: 3.121301]\n",
      "epoch:3 step:2993 [D loss: 0.376920, acc.: 84.38%] [G loss: 2.376212]\n",
      "epoch:3 step:2994 [D loss: 0.395584, acc.: 82.03%] [G loss: 3.578611]\n",
      "epoch:3 step:2995 [D loss: 0.446171, acc.: 81.25%] [G loss: 2.902904]\n",
      "epoch:3 step:2996 [D loss: 0.408897, acc.: 79.69%] [G loss: 2.643427]\n",
      "epoch:3 step:2997 [D loss: 0.348067, acc.: 88.28%] [G loss: 2.249237]\n",
      "epoch:3 step:2998 [D loss: 0.409719, acc.: 85.94%] [G loss: 2.926426]\n",
      "epoch:3 step:2999 [D loss: 0.325057, acc.: 85.94%] [G loss: 2.571600]\n",
      "epoch:3 step:3000 [D loss: 0.440295, acc.: 74.22%] [G loss: 3.219822]\n",
      "##############\n",
      "[0.86084292 0.84494354 0.79832308 0.82830683 0.76874667 0.81054213\n",
      " 0.87473089 0.82067441 0.85197162 0.8236053 ]\n",
      "##########\n",
      "epoch:3 step:3001 [D loss: 0.564485, acc.: 72.66%] [G loss: 2.472782]\n",
      "epoch:3 step:3002 [D loss: 0.402868, acc.: 82.81%] [G loss: 2.493500]\n",
      "epoch:3 step:3003 [D loss: 0.429209, acc.: 85.16%] [G loss: 4.140384]\n",
      "epoch:3 step:3004 [D loss: 0.571190, acc.: 69.53%] [G loss: 2.370065]\n",
      "epoch:3 step:3005 [D loss: 0.330382, acc.: 85.16%] [G loss: 2.248696]\n",
      "epoch:3 step:3006 [D loss: 0.397914, acc.: 77.34%] [G loss: 2.185009]\n",
      "epoch:3 step:3007 [D loss: 0.354866, acc.: 87.50%] [G loss: 1.865253]\n",
      "epoch:3 step:3008 [D loss: 0.375070, acc.: 87.50%] [G loss: 2.202204]\n",
      "epoch:3 step:3009 [D loss: 0.323048, acc.: 85.16%] [G loss: 2.929514]\n",
      "epoch:3 step:3010 [D loss: 0.349855, acc.: 87.50%] [G loss: 2.820033]\n",
      "epoch:3 step:3011 [D loss: 0.406782, acc.: 79.69%] [G loss: 2.337554]\n",
      "epoch:3 step:3012 [D loss: 0.384267, acc.: 83.59%] [G loss: 1.971106]\n",
      "epoch:3 step:3013 [D loss: 0.387101, acc.: 84.38%] [G loss: 2.426542]\n",
      "epoch:3 step:3014 [D loss: 0.331328, acc.: 89.06%] [G loss: 2.475795]\n",
      "epoch:3 step:3015 [D loss: 0.280075, acc.: 91.41%] [G loss: 2.724370]\n",
      "epoch:3 step:3016 [D loss: 0.426417, acc.: 78.12%] [G loss: 2.364507]\n",
      "epoch:3 step:3017 [D loss: 0.354917, acc.: 83.59%] [G loss: 3.085929]\n",
      "epoch:3 step:3018 [D loss: 0.372555, acc.: 82.81%] [G loss: 2.399825]\n",
      "epoch:3 step:3019 [D loss: 0.374781, acc.: 84.38%] [G loss: 2.142393]\n",
      "epoch:3 step:3020 [D loss: 0.396392, acc.: 82.81%] [G loss: 2.230313]\n",
      "epoch:3 step:3021 [D loss: 0.361640, acc.: 84.38%] [G loss: 3.393388]\n",
      "epoch:3 step:3022 [D loss: 0.439675, acc.: 79.69%] [G loss: 1.881698]\n",
      "epoch:3 step:3023 [D loss: 0.319191, acc.: 86.72%] [G loss: 3.353463]\n",
      "epoch:3 step:3024 [D loss: 0.391640, acc.: 82.03%] [G loss: 2.214036]\n",
      "epoch:3 step:3025 [D loss: 0.399013, acc.: 80.47%] [G loss: 2.170463]\n",
      "epoch:3 step:3026 [D loss: 0.424920, acc.: 82.03%] [G loss: 2.056033]\n",
      "epoch:3 step:3027 [D loss: 0.331666, acc.: 84.38%] [G loss: 2.986971]\n",
      "epoch:3 step:3028 [D loss: 0.359489, acc.: 81.25%] [G loss: 3.932817]\n",
      "epoch:3 step:3029 [D loss: 0.330523, acc.: 85.94%] [G loss: 3.245127]\n",
      "epoch:3 step:3030 [D loss: 0.367901, acc.: 86.72%] [G loss: 2.784925]\n",
      "epoch:3 step:3031 [D loss: 0.431467, acc.: 76.56%] [G loss: 3.946530]\n",
      "epoch:3 step:3032 [D loss: 0.486023, acc.: 75.00%] [G loss: 2.582689]\n",
      "epoch:3 step:3033 [D loss: 0.273971, acc.: 90.62%] [G loss: 2.756994]\n",
      "epoch:3 step:3034 [D loss: 0.351149, acc.: 89.06%] [G loss: 2.648256]\n",
      "epoch:3 step:3035 [D loss: 0.458216, acc.: 78.12%] [G loss: 2.394696]\n",
      "epoch:3 step:3036 [D loss: 0.445380, acc.: 78.12%] [G loss: 2.811835]\n",
      "epoch:3 step:3037 [D loss: 0.404347, acc.: 81.25%] [G loss: 2.179032]\n",
      "epoch:3 step:3038 [D loss: 0.443889, acc.: 78.12%] [G loss: 2.441704]\n",
      "epoch:3 step:3039 [D loss: 0.448689, acc.: 77.34%] [G loss: 2.855421]\n",
      "epoch:3 step:3040 [D loss: 0.562955, acc.: 73.44%] [G loss: 3.847595]\n",
      "epoch:3 step:3041 [D loss: 0.603007, acc.: 74.22%] [G loss: 4.177756]\n",
      "epoch:3 step:3042 [D loss: 0.558326, acc.: 71.09%] [G loss: 2.506605]\n",
      "epoch:3 step:3043 [D loss: 0.433409, acc.: 80.47%] [G loss: 2.022913]\n",
      "epoch:3 step:3044 [D loss: 0.368711, acc.: 83.59%] [G loss: 1.978863]\n",
      "epoch:3 step:3045 [D loss: 0.330714, acc.: 85.16%] [G loss: 2.875606]\n",
      "epoch:3 step:3046 [D loss: 0.405333, acc.: 78.12%] [G loss: 2.717715]\n",
      "epoch:3 step:3047 [D loss: 0.345156, acc.: 87.50%] [G loss: 2.831388]\n",
      "epoch:3 step:3048 [D loss: 0.424551, acc.: 80.47%] [G loss: 2.516870]\n",
      "epoch:3 step:3049 [D loss: 0.360846, acc.: 84.38%] [G loss: 2.664802]\n",
      "epoch:3 step:3050 [D loss: 0.337678, acc.: 86.72%] [G loss: 2.751512]\n",
      "epoch:3 step:3051 [D loss: 0.380468, acc.: 81.25%] [G loss: 2.433467]\n",
      "epoch:3 step:3052 [D loss: 0.311314, acc.: 85.94%] [G loss: 3.593898]\n",
      "epoch:3 step:3053 [D loss: 0.403887, acc.: 79.69%] [G loss: 2.259705]\n",
      "epoch:3 step:3054 [D loss: 0.324185, acc.: 87.50%] [G loss: 3.162398]\n",
      "epoch:3 step:3055 [D loss: 0.327210, acc.: 86.72%] [G loss: 2.627617]\n",
      "epoch:3 step:3056 [D loss: 0.468459, acc.: 78.12%] [G loss: 1.963590]\n",
      "epoch:3 step:3057 [D loss: 0.351147, acc.: 85.94%] [G loss: 2.088459]\n",
      "epoch:3 step:3058 [D loss: 0.506291, acc.: 74.22%] [G loss: 2.590806]\n",
      "epoch:3 step:3059 [D loss: 0.409168, acc.: 79.69%] [G loss: 2.929104]\n",
      "epoch:3 step:3060 [D loss: 0.398433, acc.: 82.81%] [G loss: 2.254261]\n",
      "epoch:3 step:3061 [D loss: 0.385225, acc.: 79.69%] [G loss: 2.763083]\n",
      "epoch:3 step:3062 [D loss: 0.371346, acc.: 82.81%] [G loss: 3.356575]\n",
      "epoch:3 step:3063 [D loss: 0.469313, acc.: 78.12%] [G loss: 2.692426]\n",
      "epoch:3 step:3064 [D loss: 0.499808, acc.: 73.44%] [G loss: 3.164826]\n",
      "epoch:3 step:3065 [D loss: 0.383700, acc.: 78.91%] [G loss: 2.252124]\n",
      "epoch:3 step:3066 [D loss: 0.366942, acc.: 85.94%] [G loss: 3.118647]\n",
      "epoch:3 step:3067 [D loss: 0.323083, acc.: 85.16%] [G loss: 2.235416]\n",
      "epoch:3 step:3068 [D loss: 0.399136, acc.: 82.81%] [G loss: 2.869478]\n",
      "epoch:3 step:3069 [D loss: 0.336098, acc.: 86.72%] [G loss: 3.406077]\n",
      "epoch:3 step:3070 [D loss: 0.483151, acc.: 77.34%] [G loss: 2.391938]\n",
      "epoch:3 step:3071 [D loss: 0.373429, acc.: 84.38%] [G loss: 2.332516]\n",
      "epoch:3 step:3072 [D loss: 0.432496, acc.: 75.00%] [G loss: 2.747259]\n",
      "epoch:3 step:3073 [D loss: 0.519534, acc.: 69.53%] [G loss: 3.625822]\n",
      "epoch:3 step:3074 [D loss: 0.678250, acc.: 71.09%] [G loss: 4.760210]\n",
      "epoch:3 step:3075 [D loss: 0.698950, acc.: 71.09%] [G loss: 4.168995]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:3 step:3076 [D loss: 0.799455, acc.: 62.50%] [G loss: 2.724352]\n",
      "epoch:3 step:3077 [D loss: 0.390357, acc.: 79.69%] [G loss: 2.994136]\n",
      "epoch:3 step:3078 [D loss: 0.364666, acc.: 81.25%] [G loss: 2.986359]\n",
      "epoch:3 step:3079 [D loss: 0.303271, acc.: 88.28%] [G loss: 3.193233]\n",
      "epoch:3 step:3080 [D loss: 0.348628, acc.: 89.06%] [G loss: 2.880246]\n",
      "epoch:3 step:3081 [D loss: 0.415011, acc.: 80.47%] [G loss: 2.881866]\n",
      "epoch:3 step:3082 [D loss: 0.482589, acc.: 74.22%] [G loss: 2.091984]\n",
      "epoch:3 step:3083 [D loss: 0.376770, acc.: 80.47%] [G loss: 2.808943]\n",
      "epoch:3 step:3084 [D loss: 0.397125, acc.: 84.38%] [G loss: 2.213335]\n",
      "epoch:3 step:3085 [D loss: 0.297333, acc.: 92.19%] [G loss: 2.779943]\n",
      "epoch:3 step:3086 [D loss: 0.348531, acc.: 86.72%] [G loss: 1.938451]\n",
      "epoch:3 step:3087 [D loss: 0.343326, acc.: 87.50%] [G loss: 2.421001]\n",
      "epoch:3 step:3088 [D loss: 0.450883, acc.: 76.56%] [G loss: 2.267512]\n",
      "epoch:3 step:3089 [D loss: 0.311691, acc.: 91.41%] [G loss: 2.446676]\n",
      "epoch:3 step:3090 [D loss: 0.384269, acc.: 86.72%] [G loss: 2.511255]\n",
      "epoch:3 step:3091 [D loss: 0.339005, acc.: 85.94%] [G loss: 2.009248]\n",
      "epoch:3 step:3092 [D loss: 0.436953, acc.: 76.56%] [G loss: 2.283881]\n",
      "epoch:3 step:3093 [D loss: 0.356958, acc.: 85.94%] [G loss: 2.374640]\n",
      "epoch:3 step:3094 [D loss: 0.385818, acc.: 85.16%] [G loss: 2.648269]\n",
      "epoch:3 step:3095 [D loss: 0.425663, acc.: 79.69%] [G loss: 3.591210]\n",
      "epoch:3 step:3096 [D loss: 0.600646, acc.: 74.22%] [G loss: 3.726351]\n",
      "epoch:3 step:3097 [D loss: 0.554960, acc.: 72.66%] [G loss: 3.747608]\n",
      "epoch:3 step:3098 [D loss: 0.522520, acc.: 69.53%] [G loss: 1.741650]\n",
      "epoch:3 step:3099 [D loss: 0.293090, acc.: 91.41%] [G loss: 3.923766]\n",
      "epoch:3 step:3100 [D loss: 0.431559, acc.: 78.12%] [G loss: 1.646994]\n",
      "epoch:3 step:3101 [D loss: 0.330140, acc.: 85.16%] [G loss: 2.509167]\n",
      "epoch:3 step:3102 [D loss: 0.377683, acc.: 81.25%] [G loss: 2.343235]\n",
      "epoch:3 step:3103 [D loss: 0.361233, acc.: 86.72%] [G loss: 2.890144]\n",
      "epoch:3 step:3104 [D loss: 0.354348, acc.: 85.94%] [G loss: 2.084305]\n",
      "epoch:3 step:3105 [D loss: 0.341596, acc.: 85.16%] [G loss: 2.298711]\n",
      "epoch:3 step:3106 [D loss: 0.370506, acc.: 78.12%] [G loss: 2.609306]\n",
      "epoch:3 step:3107 [D loss: 0.286682, acc.: 92.97%] [G loss: 3.013155]\n",
      "epoch:3 step:3108 [D loss: 0.363478, acc.: 85.16%] [G loss: 3.202600]\n",
      "epoch:3 step:3109 [D loss: 0.321698, acc.: 83.59%] [G loss: 3.814914]\n",
      "epoch:3 step:3110 [D loss: 0.300924, acc.: 90.62%] [G loss: 3.244199]\n",
      "epoch:3 step:3111 [D loss: 0.301459, acc.: 89.84%] [G loss: 2.308300]\n",
      "epoch:3 step:3112 [D loss: 0.309805, acc.: 88.28%] [G loss: 2.357625]\n",
      "epoch:3 step:3113 [D loss: 0.335761, acc.: 85.94%] [G loss: 3.354833]\n",
      "epoch:3 step:3114 [D loss: 0.391740, acc.: 80.47%] [G loss: 3.205020]\n",
      "epoch:3 step:3115 [D loss: 0.420777, acc.: 78.91%] [G loss: 2.444852]\n",
      "epoch:3 step:3116 [D loss: 0.441369, acc.: 75.78%] [G loss: 2.957456]\n",
      "epoch:3 step:3117 [D loss: 0.661346, acc.: 72.66%] [G loss: 4.338344]\n",
      "epoch:3 step:3118 [D loss: 1.331434, acc.: 57.03%] [G loss: 3.070390]\n",
      "epoch:3 step:3119 [D loss: 0.604506, acc.: 70.31%] [G loss: 2.772581]\n",
      "epoch:3 step:3120 [D loss: 0.337094, acc.: 86.72%] [G loss: 3.115185]\n",
      "epoch:3 step:3121 [D loss: 0.356170, acc.: 83.59%] [G loss: 2.912255]\n",
      "epoch:3 step:3122 [D loss: 0.371227, acc.: 85.94%] [G loss: 2.852643]\n",
      "epoch:3 step:3123 [D loss: 0.383231, acc.: 82.03%] [G loss: 2.269485]\n",
      "epoch:3 step:3124 [D loss: 0.314428, acc.: 85.94%] [G loss: 2.844506]\n",
      "epoch:4 step:3125 [D loss: 0.356017, acc.: 86.72%] [G loss: 2.919373]\n",
      "epoch:4 step:3126 [D loss: 0.294409, acc.: 88.28%] [G loss: 2.216793]\n",
      "epoch:4 step:3127 [D loss: 0.319725, acc.: 85.16%] [G loss: 2.506049]\n",
      "epoch:4 step:3128 [D loss: 0.330417, acc.: 87.50%] [G loss: 2.476156]\n",
      "epoch:4 step:3129 [D loss: 0.418741, acc.: 82.03%] [G loss: 2.471747]\n",
      "epoch:4 step:3130 [D loss: 0.418659, acc.: 80.47%] [G loss: 2.214692]\n",
      "epoch:4 step:3131 [D loss: 0.367903, acc.: 85.94%] [G loss: 2.115314]\n",
      "epoch:4 step:3132 [D loss: 0.502563, acc.: 71.09%] [G loss: 2.251214]\n",
      "epoch:4 step:3133 [D loss: 0.388163, acc.: 85.94%] [G loss: 2.314543]\n",
      "epoch:4 step:3134 [D loss: 0.404298, acc.: 78.91%] [G loss: 2.925142]\n",
      "epoch:4 step:3135 [D loss: 0.464657, acc.: 82.03%] [G loss: 3.036148]\n",
      "epoch:4 step:3136 [D loss: 0.472258, acc.: 78.12%] [G loss: 1.876216]\n",
      "epoch:4 step:3137 [D loss: 0.415081, acc.: 79.69%] [G loss: 4.281785]\n",
      "epoch:4 step:3138 [D loss: 0.459852, acc.: 84.38%] [G loss: 3.003258]\n",
      "epoch:4 step:3139 [D loss: 0.426037, acc.: 78.12%] [G loss: 2.631152]\n",
      "epoch:4 step:3140 [D loss: 0.364829, acc.: 84.38%] [G loss: 2.391127]\n",
      "epoch:4 step:3141 [D loss: 0.366531, acc.: 83.59%] [G loss: 3.055008]\n",
      "epoch:4 step:3142 [D loss: 0.350587, acc.: 84.38%] [G loss: 2.941928]\n",
      "epoch:4 step:3143 [D loss: 0.399768, acc.: 81.25%] [G loss: 2.633344]\n",
      "epoch:4 step:3144 [D loss: 0.399218, acc.: 81.25%] [G loss: 2.628371]\n",
      "epoch:4 step:3145 [D loss: 0.365950, acc.: 82.81%] [G loss: 2.672014]\n",
      "epoch:4 step:3146 [D loss: 0.411037, acc.: 82.03%] [G loss: 1.962043]\n",
      "epoch:4 step:3147 [D loss: 0.375623, acc.: 85.94%] [G loss: 2.675803]\n",
      "epoch:4 step:3148 [D loss: 0.432383, acc.: 78.91%] [G loss: 2.343724]\n",
      "epoch:4 step:3149 [D loss: 0.351834, acc.: 86.72%] [G loss: 2.965895]\n",
      "epoch:4 step:3150 [D loss: 0.347242, acc.: 83.59%] [G loss: 2.838197]\n",
      "epoch:4 step:3151 [D loss: 0.365990, acc.: 78.91%] [G loss: 2.319763]\n",
      "epoch:4 step:3152 [D loss: 0.432225, acc.: 82.81%] [G loss: 2.696544]\n",
      "epoch:4 step:3153 [D loss: 0.449772, acc.: 76.56%] [G loss: 3.604393]\n",
      "epoch:4 step:3154 [D loss: 0.523911, acc.: 75.78%] [G loss: 1.968559]\n",
      "epoch:4 step:3155 [D loss: 0.473101, acc.: 79.69%] [G loss: 3.287936]\n",
      "epoch:4 step:3156 [D loss: 0.541704, acc.: 71.09%] [G loss: 2.731001]\n",
      "epoch:4 step:3157 [D loss: 0.356139, acc.: 86.72%] [G loss: 3.612386]\n",
      "epoch:4 step:3158 [D loss: 0.408749, acc.: 78.91%] [G loss: 2.162484]\n",
      "epoch:4 step:3159 [D loss: 0.376500, acc.: 81.25%] [G loss: 2.278866]\n",
      "epoch:4 step:3160 [D loss: 0.401630, acc.: 81.25%] [G loss: 2.275724]\n",
      "epoch:4 step:3161 [D loss: 0.377820, acc.: 83.59%] [G loss: 2.837669]\n",
      "epoch:4 step:3162 [D loss: 0.489927, acc.: 73.44%] [G loss: 2.161350]\n",
      "epoch:4 step:3163 [D loss: 0.385926, acc.: 85.94%] [G loss: 2.306704]\n",
      "epoch:4 step:3164 [D loss: 0.339510, acc.: 85.16%] [G loss: 2.815123]\n",
      "epoch:4 step:3165 [D loss: 0.328833, acc.: 86.72%] [G loss: 2.829887]\n",
      "epoch:4 step:3166 [D loss: 0.323033, acc.: 88.28%] [G loss: 2.689434]\n",
      "epoch:4 step:3167 [D loss: 0.397250, acc.: 79.69%] [G loss: 2.049642]\n",
      "epoch:4 step:3168 [D loss: 0.357261, acc.: 85.16%] [G loss: 2.221399]\n",
      "epoch:4 step:3169 [D loss: 0.297366, acc.: 87.50%] [G loss: 2.861500]\n",
      "epoch:4 step:3170 [D loss: 0.334397, acc.: 87.50%] [G loss: 2.354406]\n",
      "epoch:4 step:3171 [D loss: 0.345445, acc.: 86.72%] [G loss: 3.092027]\n",
      "epoch:4 step:3172 [D loss: 0.262890, acc.: 92.97%] [G loss: 3.632398]\n",
      "epoch:4 step:3173 [D loss: 0.300404, acc.: 87.50%] [G loss: 2.630227]\n",
      "epoch:4 step:3174 [D loss: 0.406700, acc.: 78.91%] [G loss: 2.890663]\n",
      "epoch:4 step:3175 [D loss: 0.380268, acc.: 78.91%] [G loss: 2.463335]\n",
      "epoch:4 step:3176 [D loss: 0.427730, acc.: 77.34%] [G loss: 2.426983]\n",
      "epoch:4 step:3177 [D loss: 0.334342, acc.: 85.16%] [G loss: 3.028554]\n",
      "epoch:4 step:3178 [D loss: 0.312128, acc.: 86.72%] [G loss: 3.037884]\n",
      "epoch:4 step:3179 [D loss: 0.284709, acc.: 91.41%] [G loss: 2.690782]\n",
      "epoch:4 step:3180 [D loss: 0.376219, acc.: 83.59%] [G loss: 2.704750]\n",
      "epoch:4 step:3181 [D loss: 0.386973, acc.: 82.81%] [G loss: 2.294073]\n",
      "epoch:4 step:3182 [D loss: 0.369411, acc.: 82.81%] [G loss: 2.028854]\n",
      "epoch:4 step:3183 [D loss: 0.354130, acc.: 83.59%] [G loss: 2.626203]\n",
      "epoch:4 step:3184 [D loss: 0.362814, acc.: 83.59%] [G loss: 2.431860]\n",
      "epoch:4 step:3185 [D loss: 0.396563, acc.: 83.59%] [G loss: 2.683176]\n",
      "epoch:4 step:3186 [D loss: 0.383344, acc.: 82.03%] [G loss: 3.146855]\n",
      "epoch:4 step:3187 [D loss: 0.286286, acc.: 88.28%] [G loss: 4.231318]\n",
      "epoch:4 step:3188 [D loss: 0.349774, acc.: 82.81%] [G loss: 3.031256]\n",
      "epoch:4 step:3189 [D loss: 0.337219, acc.: 85.94%] [G loss: 2.443618]\n",
      "epoch:4 step:3190 [D loss: 0.421118, acc.: 78.12%] [G loss: 2.477202]\n",
      "epoch:4 step:3191 [D loss: 0.324284, acc.: 86.72%] [G loss: 2.997116]\n",
      "epoch:4 step:3192 [D loss: 0.406358, acc.: 82.81%] [G loss: 2.413411]\n",
      "epoch:4 step:3193 [D loss: 0.413891, acc.: 82.03%] [G loss: 3.301590]\n",
      "epoch:4 step:3194 [D loss: 0.742447, acc.: 71.09%] [G loss: 5.083077]\n",
      "epoch:4 step:3195 [D loss: 0.773267, acc.: 58.59%] [G loss: 4.646186]\n",
      "epoch:4 step:3196 [D loss: 1.044147, acc.: 61.72%] [G loss: 4.515457]\n",
      "epoch:4 step:3197 [D loss: 1.327102, acc.: 53.12%] [G loss: 3.381239]\n",
      "epoch:4 step:3198 [D loss: 0.852731, acc.: 60.16%] [G loss: 2.033451]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:4 step:3199 [D loss: 0.371316, acc.: 82.81%] [G loss: 3.125617]\n",
      "epoch:4 step:3200 [D loss: 0.462809, acc.: 77.34%] [G loss: 2.125454]\n",
      "##############\n",
      "[0.8560695  0.84128627 0.79832965 0.79571601 0.78758228 0.81184092\n",
      " 0.90146938 0.81989088 0.8271927  0.80957949]\n",
      "##########\n",
      "epoch:4 step:3201 [D loss: 0.395748, acc.: 82.81%] [G loss: 2.011218]\n",
      "epoch:4 step:3202 [D loss: 0.305394, acc.: 88.28%] [G loss: 2.403054]\n",
      "epoch:4 step:3203 [D loss: 0.446731, acc.: 81.25%] [G loss: 2.521083]\n",
      "epoch:4 step:3204 [D loss: 0.399116, acc.: 80.47%] [G loss: 2.575303]\n",
      "epoch:4 step:3205 [D loss: 0.392048, acc.: 81.25%] [G loss: 2.526447]\n",
      "epoch:4 step:3206 [D loss: 0.357449, acc.: 83.59%] [G loss: 2.462520]\n",
      "epoch:4 step:3207 [D loss: 0.346379, acc.: 87.50%] [G loss: 1.882750]\n",
      "epoch:4 step:3208 [D loss: 0.406655, acc.: 80.47%] [G loss: 2.002318]\n",
      "epoch:4 step:3209 [D loss: 0.502215, acc.: 78.91%] [G loss: 1.933785]\n",
      "epoch:4 step:3210 [D loss: 0.443242, acc.: 75.78%] [G loss: 2.055023]\n",
      "epoch:4 step:3211 [D loss: 0.410727, acc.: 79.69%] [G loss: 2.629908]\n",
      "epoch:4 step:3212 [D loss: 0.288910, acc.: 91.41%] [G loss: 3.425480]\n",
      "epoch:4 step:3213 [D loss: 0.403544, acc.: 78.12%] [G loss: 1.962011]\n",
      "epoch:4 step:3214 [D loss: 0.356668, acc.: 85.94%] [G loss: 2.791883]\n",
      "epoch:4 step:3215 [D loss: 0.394063, acc.: 78.12%] [G loss: 2.670758]\n",
      "epoch:4 step:3216 [D loss: 0.414984, acc.: 81.25%] [G loss: 2.649211]\n",
      "epoch:4 step:3217 [D loss: 0.495010, acc.: 78.91%] [G loss: 2.851093]\n",
      "epoch:4 step:3218 [D loss: 0.421790, acc.: 82.03%] [G loss: 2.610530]\n",
      "epoch:4 step:3219 [D loss: 0.362256, acc.: 82.03%] [G loss: 3.706697]\n",
      "epoch:4 step:3220 [D loss: 0.443473, acc.: 78.12%] [G loss: 2.859215]\n",
      "epoch:4 step:3221 [D loss: 0.289551, acc.: 85.16%] [G loss: 4.185072]\n",
      "epoch:4 step:3222 [D loss: 0.373584, acc.: 83.59%] [G loss: 3.938214]\n",
      "epoch:4 step:3223 [D loss: 0.359524, acc.: 86.72%] [G loss: 2.991498]\n",
      "epoch:4 step:3224 [D loss: 0.419175, acc.: 77.34%] [G loss: 2.773176]\n",
      "epoch:4 step:3225 [D loss: 0.456354, acc.: 81.25%] [G loss: 3.488863]\n",
      "epoch:4 step:3226 [D loss: 0.632427, acc.: 65.62%] [G loss: 2.756719]\n",
      "epoch:4 step:3227 [D loss: 0.600109, acc.: 68.75%] [G loss: 2.612301]\n",
      "epoch:4 step:3228 [D loss: 0.423968, acc.: 78.12%] [G loss: 2.980152]\n",
      "epoch:4 step:3229 [D loss: 0.550795, acc.: 71.09%] [G loss: 2.420102]\n",
      "epoch:4 step:3230 [D loss: 0.381334, acc.: 80.47%] [G loss: 2.539036]\n",
      "epoch:4 step:3231 [D loss: 0.303531, acc.: 89.06%] [G loss: 2.791625]\n",
      "epoch:4 step:3232 [D loss: 0.394316, acc.: 85.16%] [G loss: 2.116215]\n",
      "epoch:4 step:3233 [D loss: 0.402807, acc.: 83.59%] [G loss: 2.118719]\n",
      "epoch:4 step:3234 [D loss: 0.379815, acc.: 81.25%] [G loss: 1.946151]\n",
      "epoch:4 step:3235 [D loss: 0.466799, acc.: 73.44%] [G loss: 2.369310]\n",
      "epoch:4 step:3236 [D loss: 0.608244, acc.: 70.31%] [G loss: 2.418051]\n",
      "epoch:4 step:3237 [D loss: 0.448684, acc.: 77.34%] [G loss: 2.535411]\n",
      "epoch:4 step:3238 [D loss: 0.567086, acc.: 74.22%] [G loss: 4.244166]\n",
      "epoch:4 step:3239 [D loss: 0.669490, acc.: 66.41%] [G loss: 2.014090]\n",
      "epoch:4 step:3240 [D loss: 0.438812, acc.: 80.47%] [G loss: 2.244945]\n",
      "epoch:4 step:3241 [D loss: 0.556556, acc.: 69.53%] [G loss: 2.186960]\n",
      "epoch:4 step:3242 [D loss: 0.433164, acc.: 78.12%] [G loss: 2.352511]\n",
      "epoch:4 step:3243 [D loss: 0.403994, acc.: 84.38%] [G loss: 3.033398]\n",
      "epoch:4 step:3244 [D loss: 0.400228, acc.: 81.25%] [G loss: 2.217222]\n",
      "epoch:4 step:3245 [D loss: 0.354169, acc.: 88.28%] [G loss: 2.203448]\n",
      "epoch:4 step:3246 [D loss: 0.364590, acc.: 84.38%] [G loss: 2.563919]\n",
      "epoch:4 step:3247 [D loss: 0.365249, acc.: 82.81%] [G loss: 2.528667]\n",
      "epoch:4 step:3248 [D loss: 0.337251, acc.: 87.50%] [G loss: 3.219644]\n",
      "epoch:4 step:3249 [D loss: 0.365685, acc.: 82.81%] [G loss: 3.027851]\n",
      "epoch:4 step:3250 [D loss: 0.305129, acc.: 89.06%] [G loss: 3.105607]\n",
      "epoch:4 step:3251 [D loss: 0.313669, acc.: 89.06%] [G loss: 3.183298]\n",
      "epoch:4 step:3252 [D loss: 0.346812, acc.: 82.03%] [G loss: 2.828910]\n",
      "epoch:4 step:3253 [D loss: 0.333297, acc.: 85.94%] [G loss: 2.207896]\n",
      "epoch:4 step:3254 [D loss: 0.329221, acc.: 89.06%] [G loss: 2.514374]\n",
      "epoch:4 step:3255 [D loss: 0.461813, acc.: 75.78%] [G loss: 2.295107]\n",
      "epoch:4 step:3256 [D loss: 0.424406, acc.: 77.34%] [G loss: 2.324076]\n",
      "epoch:4 step:3257 [D loss: 0.333039, acc.: 85.16%] [G loss: 3.369893]\n",
      "epoch:4 step:3258 [D loss: 0.396699, acc.: 82.03%] [G loss: 2.422905]\n",
      "epoch:4 step:3259 [D loss: 0.415435, acc.: 82.81%] [G loss: 2.325831]\n",
      "epoch:4 step:3260 [D loss: 0.318590, acc.: 89.06%] [G loss: 3.739607]\n",
      "epoch:4 step:3261 [D loss: 0.473737, acc.: 77.34%] [G loss: 2.295667]\n",
      "epoch:4 step:3262 [D loss: 0.486385, acc.: 77.34%] [G loss: 2.927845]\n",
      "epoch:4 step:3263 [D loss: 0.535971, acc.: 75.00%] [G loss: 2.641091]\n",
      "epoch:4 step:3264 [D loss: 0.444053, acc.: 76.56%] [G loss: 2.707003]\n",
      "epoch:4 step:3265 [D loss: 0.412517, acc.: 78.91%] [G loss: 2.683018]\n",
      "epoch:4 step:3266 [D loss: 0.319860, acc.: 86.72%] [G loss: 3.345961]\n",
      "epoch:4 step:3267 [D loss: 0.335772, acc.: 85.16%] [G loss: 2.226954]\n",
      "epoch:4 step:3268 [D loss: 0.332076, acc.: 86.72%] [G loss: 2.644588]\n",
      "epoch:4 step:3269 [D loss: 0.427993, acc.: 81.25%] [G loss: 2.536386]\n",
      "epoch:4 step:3270 [D loss: 0.375596, acc.: 82.81%] [G loss: 2.817772]\n",
      "epoch:4 step:3271 [D loss: 0.415102, acc.: 81.25%] [G loss: 2.227395]\n",
      "epoch:4 step:3272 [D loss: 0.423235, acc.: 78.91%] [G loss: 2.660300]\n",
      "epoch:4 step:3273 [D loss: 0.334897, acc.: 88.28%] [G loss: 2.750363]\n",
      "epoch:4 step:3274 [D loss: 0.350960, acc.: 81.25%] [G loss: 2.841935]\n",
      "epoch:4 step:3275 [D loss: 0.369124, acc.: 85.16%] [G loss: 2.887639]\n",
      "epoch:4 step:3276 [D loss: 0.306984, acc.: 87.50%] [G loss: 3.744085]\n",
      "epoch:4 step:3277 [D loss: 0.442294, acc.: 75.00%] [G loss: 1.952371]\n",
      "epoch:4 step:3278 [D loss: 0.347315, acc.: 84.38%] [G loss: 2.200654]\n",
      "epoch:4 step:3279 [D loss: 0.359580, acc.: 85.16%] [G loss: 2.249434]\n",
      "epoch:4 step:3280 [D loss: 0.417364, acc.: 83.59%] [G loss: 2.483009]\n",
      "epoch:4 step:3281 [D loss: 0.310358, acc.: 86.72%] [G loss: 3.099775]\n",
      "epoch:4 step:3282 [D loss: 0.303162, acc.: 90.62%] [G loss: 3.225798]\n",
      "epoch:4 step:3283 [D loss: 0.305867, acc.: 86.72%] [G loss: 4.286095]\n",
      "epoch:4 step:3284 [D loss: 0.431012, acc.: 74.22%] [G loss: 3.604486]\n",
      "epoch:4 step:3285 [D loss: 0.395163, acc.: 82.03%] [G loss: 3.960343]\n",
      "epoch:4 step:3286 [D loss: 0.457120, acc.: 77.34%] [G loss: 2.465083]\n",
      "epoch:4 step:3287 [D loss: 0.372275, acc.: 82.03%] [G loss: 2.353357]\n",
      "epoch:4 step:3288 [D loss: 0.446725, acc.: 78.12%] [G loss: 3.019905]\n",
      "epoch:4 step:3289 [D loss: 0.373875, acc.: 81.25%] [G loss: 2.780951]\n",
      "epoch:4 step:3290 [D loss: 0.393173, acc.: 85.16%] [G loss: 3.060528]\n",
      "epoch:4 step:3291 [D loss: 0.336345, acc.: 87.50%] [G loss: 2.834406]\n",
      "epoch:4 step:3292 [D loss: 0.266304, acc.: 90.62%] [G loss: 2.445813]\n",
      "epoch:4 step:3293 [D loss: 0.369706, acc.: 82.81%] [G loss: 2.592259]\n",
      "epoch:4 step:3294 [D loss: 0.348475, acc.: 84.38%] [G loss: 2.224008]\n",
      "epoch:4 step:3295 [D loss: 0.286914, acc.: 89.84%] [G loss: 3.098939]\n",
      "epoch:4 step:3296 [D loss: 0.365544, acc.: 82.03%] [G loss: 2.920359]\n",
      "epoch:4 step:3297 [D loss: 0.277622, acc.: 88.28%] [G loss: 3.310745]\n",
      "epoch:4 step:3298 [D loss: 0.416946, acc.: 82.03%] [G loss: 2.563637]\n",
      "epoch:4 step:3299 [D loss: 0.499885, acc.: 71.88%] [G loss: 2.344061]\n",
      "epoch:4 step:3300 [D loss: 0.400383, acc.: 79.69%] [G loss: 3.133683]\n",
      "epoch:4 step:3301 [D loss: 0.371412, acc.: 78.12%] [G loss: 3.570914]\n",
      "epoch:4 step:3302 [D loss: 0.483121, acc.: 77.34%] [G loss: 2.270146]\n",
      "epoch:4 step:3303 [D loss: 0.437631, acc.: 78.12%] [G loss: 2.585953]\n",
      "epoch:4 step:3304 [D loss: 0.374981, acc.: 84.38%] [G loss: 2.239769]\n",
      "epoch:4 step:3305 [D loss: 0.297150, acc.: 89.06%] [G loss: 3.116431]\n",
      "epoch:4 step:3306 [D loss: 0.398679, acc.: 76.56%] [G loss: 2.989394]\n",
      "epoch:4 step:3307 [D loss: 0.426536, acc.: 78.12%] [G loss: 3.468669]\n",
      "epoch:4 step:3308 [D loss: 0.308407, acc.: 87.50%] [G loss: 3.026953]\n",
      "epoch:4 step:3309 [D loss: 0.404097, acc.: 82.03%] [G loss: 2.719362]\n",
      "epoch:4 step:3310 [D loss: 0.321785, acc.: 86.72%] [G loss: 3.082994]\n",
      "epoch:4 step:3311 [D loss: 0.308845, acc.: 87.50%] [G loss: 2.574902]\n",
      "epoch:4 step:3312 [D loss: 0.388657, acc.: 81.25%] [G loss: 2.353379]\n",
      "epoch:4 step:3313 [D loss: 0.419836, acc.: 82.03%] [G loss: 2.440799]\n",
      "epoch:4 step:3314 [D loss: 0.372628, acc.: 83.59%] [G loss: 2.082785]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:4 step:3315 [D loss: 0.340931, acc.: 84.38%] [G loss: 2.115560]\n",
      "epoch:4 step:3316 [D loss: 0.351364, acc.: 85.16%] [G loss: 2.388240]\n",
      "epoch:4 step:3317 [D loss: 0.534697, acc.: 71.88%] [G loss: 3.311581]\n",
      "epoch:4 step:3318 [D loss: 0.580630, acc.: 69.53%] [G loss: 5.370050]\n",
      "epoch:4 step:3319 [D loss: 1.037742, acc.: 60.94%] [G loss: 4.652484]\n",
      "epoch:4 step:3320 [D loss: 1.280851, acc.: 61.72%] [G loss: 3.470552]\n",
      "epoch:4 step:3321 [D loss: 0.576451, acc.: 72.66%] [G loss: 1.916379]\n",
      "epoch:4 step:3322 [D loss: 0.421664, acc.: 77.34%] [G loss: 3.478275]\n",
      "epoch:4 step:3323 [D loss: 0.466168, acc.: 78.12%] [G loss: 2.431025]\n",
      "epoch:4 step:3324 [D loss: 0.337206, acc.: 86.72%] [G loss: 2.664084]\n",
      "epoch:4 step:3325 [D loss: 0.400232, acc.: 82.03%] [G loss: 2.300783]\n",
      "epoch:4 step:3326 [D loss: 0.359074, acc.: 84.38%] [G loss: 2.718179]\n",
      "epoch:4 step:3327 [D loss: 0.305637, acc.: 86.72%] [G loss: 3.139249]\n",
      "epoch:4 step:3328 [D loss: 0.327259, acc.: 85.94%] [G loss: 2.363015]\n",
      "epoch:4 step:3329 [D loss: 0.476548, acc.: 76.56%] [G loss: 2.147023]\n",
      "epoch:4 step:3330 [D loss: 0.494806, acc.: 75.00%] [G loss: 2.256161]\n",
      "epoch:4 step:3331 [D loss: 0.451640, acc.: 75.78%] [G loss: 3.238100]\n",
      "epoch:4 step:3332 [D loss: 0.536412, acc.: 74.22%] [G loss: 3.247033]\n",
      "epoch:4 step:3333 [D loss: 0.542216, acc.: 71.09%] [G loss: 3.421540]\n",
      "epoch:4 step:3334 [D loss: 0.506890, acc.: 70.31%] [G loss: 2.912578]\n",
      "epoch:4 step:3335 [D loss: 0.376173, acc.: 83.59%] [G loss: 4.268419]\n",
      "epoch:4 step:3336 [D loss: 0.506164, acc.: 72.66%] [G loss: 3.506355]\n",
      "epoch:4 step:3337 [D loss: 0.302906, acc.: 85.94%] [G loss: 2.777055]\n",
      "epoch:4 step:3338 [D loss: 0.406630, acc.: 80.47%] [G loss: 2.906114]\n",
      "epoch:4 step:3339 [D loss: 0.447614, acc.: 82.81%] [G loss: 2.054715]\n",
      "epoch:4 step:3340 [D loss: 0.385277, acc.: 82.81%] [G loss: 2.095174]\n",
      "epoch:4 step:3341 [D loss: 0.383035, acc.: 84.38%] [G loss: 2.397203]\n",
      "epoch:4 step:3342 [D loss: 0.454098, acc.: 79.69%] [G loss: 2.042969]\n",
      "epoch:4 step:3343 [D loss: 0.397881, acc.: 84.38%] [G loss: 2.198051]\n",
      "epoch:4 step:3344 [D loss: 0.336336, acc.: 85.94%] [G loss: 2.864732]\n",
      "epoch:4 step:3345 [D loss: 0.317406, acc.: 89.06%] [G loss: 2.870793]\n",
      "epoch:4 step:3346 [D loss: 0.367121, acc.: 82.03%] [G loss: 2.152383]\n",
      "epoch:4 step:3347 [D loss: 0.413321, acc.: 83.59%] [G loss: 2.174945]\n",
      "epoch:4 step:3348 [D loss: 0.385637, acc.: 82.81%] [G loss: 2.076224]\n",
      "epoch:4 step:3349 [D loss: 0.384233, acc.: 84.38%] [G loss: 2.559050]\n",
      "epoch:4 step:3350 [D loss: 0.419849, acc.: 77.34%] [G loss: 3.276540]\n",
      "epoch:4 step:3351 [D loss: 0.339996, acc.: 87.50%] [G loss: 2.100744]\n",
      "epoch:4 step:3352 [D loss: 0.396669, acc.: 84.38%] [G loss: 2.474280]\n",
      "epoch:4 step:3353 [D loss: 0.386369, acc.: 80.47%] [G loss: 2.291522]\n",
      "epoch:4 step:3354 [D loss: 0.404722, acc.: 84.38%] [G loss: 3.895862]\n",
      "epoch:4 step:3355 [D loss: 0.409370, acc.: 83.59%] [G loss: 3.614624]\n",
      "epoch:4 step:3356 [D loss: 0.602069, acc.: 73.44%] [G loss: 2.435581]\n",
      "epoch:4 step:3357 [D loss: 0.425652, acc.: 77.34%] [G loss: 3.007852]\n",
      "epoch:4 step:3358 [D loss: 0.386032, acc.: 82.03%] [G loss: 2.937596]\n",
      "epoch:4 step:3359 [D loss: 0.367626, acc.: 82.81%] [G loss: 2.552581]\n",
      "epoch:4 step:3360 [D loss: 0.331657, acc.: 88.28%] [G loss: 3.350284]\n",
      "epoch:4 step:3361 [D loss: 0.436218, acc.: 79.69%] [G loss: 3.177814]\n",
      "epoch:4 step:3362 [D loss: 0.407270, acc.: 80.47%] [G loss: 2.431241]\n",
      "epoch:4 step:3363 [D loss: 0.321611, acc.: 89.84%] [G loss: 3.014383]\n",
      "epoch:4 step:3364 [D loss: 0.391916, acc.: 79.69%] [G loss: 2.713239]\n",
      "epoch:4 step:3365 [D loss: 0.379597, acc.: 82.81%] [G loss: 2.949533]\n",
      "epoch:4 step:3366 [D loss: 0.433126, acc.: 78.12%] [G loss: 2.657845]\n",
      "epoch:4 step:3367 [D loss: 0.378877, acc.: 82.03%] [G loss: 4.529771]\n",
      "epoch:4 step:3368 [D loss: 0.421315, acc.: 79.69%] [G loss: 2.578798]\n",
      "epoch:4 step:3369 [D loss: 0.360569, acc.: 84.38%] [G loss: 2.241423]\n",
      "epoch:4 step:3370 [D loss: 0.277804, acc.: 92.97%] [G loss: 3.861274]\n",
      "epoch:4 step:3371 [D loss: 0.411960, acc.: 79.69%] [G loss: 2.641907]\n",
      "epoch:4 step:3372 [D loss: 0.407214, acc.: 83.59%] [G loss: 2.734512]\n",
      "epoch:4 step:3373 [D loss: 0.395666, acc.: 79.69%] [G loss: 3.058257]\n",
      "epoch:4 step:3374 [D loss: 0.516263, acc.: 75.78%] [G loss: 3.564943]\n",
      "epoch:4 step:3375 [D loss: 0.635469, acc.: 69.53%] [G loss: 3.800897]\n",
      "epoch:4 step:3376 [D loss: 0.711554, acc.: 68.75%] [G loss: 2.091458]\n",
      "epoch:4 step:3377 [D loss: 0.385855, acc.: 84.38%] [G loss: 3.180246]\n",
      "epoch:4 step:3378 [D loss: 0.401854, acc.: 81.25%] [G loss: 2.967579]\n",
      "epoch:4 step:3379 [D loss: 0.382562, acc.: 81.25%] [G loss: 2.943363]\n",
      "epoch:4 step:3380 [D loss: 0.360935, acc.: 82.81%] [G loss: 2.593156]\n",
      "epoch:4 step:3381 [D loss: 0.336883, acc.: 84.38%] [G loss: 2.724149]\n",
      "epoch:4 step:3382 [D loss: 0.374186, acc.: 87.50%] [G loss: 2.549973]\n",
      "epoch:4 step:3383 [D loss: 0.335464, acc.: 86.72%] [G loss: 2.508014]\n",
      "epoch:4 step:3384 [D loss: 0.374564, acc.: 82.03%] [G loss: 2.777505]\n",
      "epoch:4 step:3385 [D loss: 0.330032, acc.: 89.84%] [G loss: 3.130717]\n",
      "epoch:4 step:3386 [D loss: 0.390941, acc.: 82.03%] [G loss: 3.106628]\n",
      "epoch:4 step:3387 [D loss: 0.343959, acc.: 87.50%] [G loss: 2.948608]\n",
      "epoch:4 step:3388 [D loss: 0.319666, acc.: 90.62%] [G loss: 2.513956]\n",
      "epoch:4 step:3389 [D loss: 0.405966, acc.: 82.81%] [G loss: 3.273221]\n",
      "epoch:4 step:3390 [D loss: 0.513609, acc.: 72.66%] [G loss: 4.729942]\n",
      "epoch:4 step:3391 [D loss: 0.632325, acc.: 75.00%] [G loss: 5.783552]\n",
      "epoch:4 step:3392 [D loss: 1.179943, acc.: 64.84%] [G loss: 4.056827]\n",
      "epoch:4 step:3393 [D loss: 0.815997, acc.: 62.50%] [G loss: 1.767378]\n",
      "epoch:4 step:3394 [D loss: 0.482219, acc.: 75.78%] [G loss: 2.524248]\n",
      "epoch:4 step:3395 [D loss: 0.381301, acc.: 80.47%] [G loss: 2.658302]\n",
      "epoch:4 step:3396 [D loss: 0.326334, acc.: 87.50%] [G loss: 4.082068]\n",
      "epoch:4 step:3397 [D loss: 0.287689, acc.: 89.06%] [G loss: 2.625525]\n",
      "epoch:4 step:3398 [D loss: 0.366126, acc.: 82.81%] [G loss: 2.495948]\n",
      "epoch:4 step:3399 [D loss: 0.429942, acc.: 80.47%] [G loss: 2.735427]\n",
      "epoch:4 step:3400 [D loss: 0.314761, acc.: 85.94%] [G loss: 2.802241]\n",
      "##############\n",
      "[0.8423997  0.85821557 0.80455395 0.80410079 0.79295512 0.82997253\n",
      " 0.88333566 0.81835551 0.81056003 0.81187623]\n",
      "##########\n",
      "epoch:4 step:3401 [D loss: 0.344252, acc.: 85.16%] [G loss: 2.236891]\n",
      "epoch:4 step:3402 [D loss: 0.422985, acc.: 82.03%] [G loss: 2.061398]\n",
      "epoch:4 step:3403 [D loss: 0.479424, acc.: 77.34%] [G loss: 2.815928]\n",
      "epoch:4 step:3404 [D loss: 0.412611, acc.: 81.25%] [G loss: 2.700848]\n",
      "epoch:4 step:3405 [D loss: 0.300215, acc.: 89.84%] [G loss: 3.518706]\n",
      "epoch:4 step:3406 [D loss: 0.371060, acc.: 88.28%] [G loss: 2.577062]\n",
      "epoch:4 step:3407 [D loss: 0.354035, acc.: 85.16%] [G loss: 2.694952]\n",
      "epoch:4 step:3408 [D loss: 0.369337, acc.: 81.25%] [G loss: 2.186625]\n",
      "epoch:4 step:3409 [D loss: 0.339933, acc.: 88.28%] [G loss: 1.912911]\n",
      "epoch:4 step:3410 [D loss: 0.371856, acc.: 86.72%] [G loss: 2.509257]\n",
      "epoch:4 step:3411 [D loss: 0.361049, acc.: 87.50%] [G loss: 2.124298]\n",
      "epoch:4 step:3412 [D loss: 0.371514, acc.: 83.59%] [G loss: 2.207815]\n",
      "epoch:4 step:3413 [D loss: 0.379360, acc.: 83.59%] [G loss: 1.993880]\n",
      "epoch:4 step:3414 [D loss: 0.443627, acc.: 78.91%] [G loss: 2.331333]\n",
      "epoch:4 step:3415 [D loss: 0.366102, acc.: 84.38%] [G loss: 2.536735]\n",
      "epoch:4 step:3416 [D loss: 0.397504, acc.: 81.25%] [G loss: 3.247820]\n",
      "epoch:4 step:3417 [D loss: 0.392067, acc.: 82.81%] [G loss: 2.769241]\n",
      "epoch:4 step:3418 [D loss: 0.384845, acc.: 83.59%] [G loss: 3.309655]\n",
      "epoch:4 step:3419 [D loss: 0.297811, acc.: 89.06%] [G loss: 4.141498]\n",
      "epoch:4 step:3420 [D loss: 0.337313, acc.: 85.94%] [G loss: 3.985799]\n",
      "epoch:4 step:3421 [D loss: 0.329803, acc.: 83.59%] [G loss: 3.957969]\n",
      "epoch:4 step:3422 [D loss: 0.316046, acc.: 87.50%] [G loss: 2.928066]\n",
      "epoch:4 step:3423 [D loss: 0.318147, acc.: 86.72%] [G loss: 3.514672]\n",
      "epoch:4 step:3424 [D loss: 0.418564, acc.: 79.69%] [G loss: 3.227139]\n",
      "epoch:4 step:3425 [D loss: 0.417219, acc.: 80.47%] [G loss: 3.164639]\n",
      "epoch:4 step:3426 [D loss: 0.330685, acc.: 88.28%] [G loss: 3.222992]\n",
      "epoch:4 step:3427 [D loss: 0.401102, acc.: 82.03%] [G loss: 2.503142]\n",
      "epoch:4 step:3428 [D loss: 0.395036, acc.: 79.69%] [G loss: 2.351101]\n",
      "epoch:4 step:3429 [D loss: 0.370440, acc.: 85.94%] [G loss: 2.121954]\n",
      "epoch:4 step:3430 [D loss: 0.316586, acc.: 88.28%] [G loss: 2.377569]\n",
      "epoch:4 step:3431 [D loss: 0.413399, acc.: 82.81%] [G loss: 2.446797]\n",
      "epoch:4 step:3432 [D loss: 0.445347, acc.: 75.78%] [G loss: 2.581610]\n",
      "epoch:4 step:3433 [D loss: 0.465199, acc.: 73.44%] [G loss: 3.872578]\n",
      "epoch:4 step:3434 [D loss: 0.418322, acc.: 82.03%] [G loss: 3.029766]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:4 step:3435 [D loss: 0.377149, acc.: 84.38%] [G loss: 3.313554]\n",
      "epoch:4 step:3436 [D loss: 0.399513, acc.: 80.47%] [G loss: 3.227199]\n",
      "epoch:4 step:3437 [D loss: 0.327473, acc.: 89.06%] [G loss: 2.521245]\n",
      "epoch:4 step:3438 [D loss: 0.339548, acc.: 90.62%] [G loss: 3.387464]\n",
      "epoch:4 step:3439 [D loss: 0.407112, acc.: 78.12%] [G loss: 2.733027]\n",
      "epoch:4 step:3440 [D loss: 0.488081, acc.: 77.34%] [G loss: 6.062285]\n",
      "epoch:4 step:3441 [D loss: 1.044604, acc.: 67.19%] [G loss: 5.795742]\n",
      "epoch:4 step:3442 [D loss: 1.919387, acc.: 50.00%] [G loss: 3.012468]\n",
      "epoch:4 step:3443 [D loss: 0.592649, acc.: 71.88%] [G loss: 3.100059]\n",
      "epoch:4 step:3444 [D loss: 0.834489, acc.: 58.59%] [G loss: 1.993195]\n",
      "epoch:4 step:3445 [D loss: 0.424415, acc.: 78.12%] [G loss: 2.281185]\n",
      "epoch:4 step:3446 [D loss: 0.479716, acc.: 75.00%] [G loss: 2.595836]\n",
      "epoch:4 step:3447 [D loss: 0.453585, acc.: 75.78%] [G loss: 2.322187]\n",
      "epoch:4 step:3448 [D loss: 0.608185, acc.: 71.09%] [G loss: 2.809880]\n",
      "epoch:4 step:3449 [D loss: 0.536229, acc.: 71.88%] [G loss: 1.739964]\n",
      "epoch:4 step:3450 [D loss: 0.338572, acc.: 85.16%] [G loss: 2.534482]\n",
      "epoch:4 step:3451 [D loss: 0.401399, acc.: 86.72%] [G loss: 3.314945]\n",
      "epoch:4 step:3452 [D loss: 0.311011, acc.: 92.97%] [G loss: 2.123677]\n",
      "epoch:4 step:3453 [D loss: 0.354462, acc.: 85.94%] [G loss: 2.410250]\n",
      "epoch:4 step:3454 [D loss: 0.322574, acc.: 86.72%] [G loss: 2.337650]\n",
      "epoch:4 step:3455 [D loss: 0.430421, acc.: 76.56%] [G loss: 2.785319]\n",
      "epoch:4 step:3456 [D loss: 0.391825, acc.: 85.16%] [G loss: 1.812916]\n",
      "epoch:4 step:3457 [D loss: 0.341815, acc.: 89.06%] [G loss: 2.170675]\n",
      "epoch:4 step:3458 [D loss: 0.360779, acc.: 88.28%] [G loss: 2.235726]\n",
      "epoch:4 step:3459 [D loss: 0.328816, acc.: 90.62%] [G loss: 3.040522]\n",
      "epoch:4 step:3460 [D loss: 0.347264, acc.: 85.94%] [G loss: 2.874710]\n",
      "epoch:4 step:3461 [D loss: 0.375706, acc.: 85.16%] [G loss: 2.607473]\n",
      "epoch:4 step:3462 [D loss: 0.332951, acc.: 83.59%] [G loss: 2.388265]\n",
      "epoch:4 step:3463 [D loss: 0.318582, acc.: 89.06%] [G loss: 2.719469]\n",
      "epoch:4 step:3464 [D loss: 0.385071, acc.: 82.81%] [G loss: 2.780280]\n",
      "epoch:4 step:3465 [D loss: 0.420992, acc.: 80.47%] [G loss: 2.633062]\n",
      "epoch:4 step:3466 [D loss: 0.367865, acc.: 85.16%] [G loss: 2.961040]\n",
      "epoch:4 step:3467 [D loss: 0.408910, acc.: 82.81%] [G loss: 2.916958]\n",
      "epoch:4 step:3468 [D loss: 0.469057, acc.: 75.78%] [G loss: 3.421188]\n",
      "epoch:4 step:3469 [D loss: 0.451244, acc.: 78.91%] [G loss: 2.273008]\n",
      "epoch:4 step:3470 [D loss: 0.395717, acc.: 80.47%] [G loss: 3.265083]\n",
      "epoch:4 step:3471 [D loss: 0.397260, acc.: 82.81%] [G loss: 2.110228]\n",
      "epoch:4 step:3472 [D loss: 0.298995, acc.: 89.06%] [G loss: 2.442187]\n",
      "epoch:4 step:3473 [D loss: 0.395209, acc.: 82.81%] [G loss: 2.385559]\n",
      "epoch:4 step:3474 [D loss: 0.443726, acc.: 76.56%] [G loss: 2.478599]\n",
      "epoch:4 step:3475 [D loss: 0.377420, acc.: 84.38%] [G loss: 2.829396]\n",
      "epoch:4 step:3476 [D loss: 0.496098, acc.: 74.22%] [G loss: 2.941081]\n",
      "epoch:4 step:3477 [D loss: 0.524903, acc.: 72.66%] [G loss: 2.485434]\n",
      "epoch:4 step:3478 [D loss: 0.404282, acc.: 76.56%] [G loss: 2.496424]\n",
      "epoch:4 step:3479 [D loss: 0.368275, acc.: 84.38%] [G loss: 2.284993]\n",
      "epoch:4 step:3480 [D loss: 0.386862, acc.: 82.81%] [G loss: 3.040972]\n",
      "epoch:4 step:3481 [D loss: 0.324913, acc.: 89.06%] [G loss: 3.785366]\n",
      "epoch:4 step:3482 [D loss: 0.356744, acc.: 87.50%] [G loss: 3.462355]\n",
      "epoch:4 step:3483 [D loss: 0.407420, acc.: 80.47%] [G loss: 2.415719]\n",
      "epoch:4 step:3484 [D loss: 0.470426, acc.: 76.56%] [G loss: 2.562529]\n",
      "epoch:4 step:3485 [D loss: 0.474120, acc.: 75.78%] [G loss: 2.792682]\n",
      "epoch:4 step:3486 [D loss: 0.395892, acc.: 82.03%] [G loss: 3.375988]\n",
      "epoch:4 step:3487 [D loss: 0.408142, acc.: 80.47%] [G loss: 2.450921]\n",
      "epoch:4 step:3488 [D loss: 0.355786, acc.: 83.59%] [G loss: 2.694772]\n",
      "epoch:4 step:3489 [D loss: 0.430240, acc.: 82.81%] [G loss: 2.773269]\n",
      "epoch:4 step:3490 [D loss: 0.460743, acc.: 77.34%] [G loss: 1.995635]\n",
      "epoch:4 step:3491 [D loss: 0.319887, acc.: 88.28%] [G loss: 3.018688]\n",
      "epoch:4 step:3492 [D loss: 0.386036, acc.: 83.59%] [G loss: 2.299304]\n",
      "epoch:4 step:3493 [D loss: 0.444304, acc.: 77.34%] [G loss: 2.110906]\n",
      "epoch:4 step:3494 [D loss: 0.306085, acc.: 85.94%] [G loss: 3.009744]\n",
      "epoch:4 step:3495 [D loss: 0.279587, acc.: 89.06%] [G loss: 2.748515]\n",
      "epoch:4 step:3496 [D loss: 0.429623, acc.: 79.69%] [G loss: 3.357538]\n",
      "epoch:4 step:3497 [D loss: 0.373716, acc.: 85.16%] [G loss: 2.620127]\n",
      "epoch:4 step:3498 [D loss: 0.394173, acc.: 85.16%] [G loss: 2.574857]\n",
      "epoch:4 step:3499 [D loss: 0.350595, acc.: 82.81%] [G loss: 2.574628]\n",
      "epoch:4 step:3500 [D loss: 0.351805, acc.: 85.16%] [G loss: 3.058999]\n",
      "epoch:4 step:3501 [D loss: 0.349127, acc.: 85.16%] [G loss: 2.143758]\n",
      "epoch:4 step:3502 [D loss: 0.337624, acc.: 87.50%] [G loss: 3.367641]\n",
      "epoch:4 step:3503 [D loss: 0.332356, acc.: 86.72%] [G loss: 2.561318]\n",
      "epoch:4 step:3504 [D loss: 0.392128, acc.: 82.03%] [G loss: 2.400424]\n",
      "epoch:4 step:3505 [D loss: 0.360563, acc.: 85.16%] [G loss: 3.142152]\n",
      "epoch:4 step:3506 [D loss: 0.341093, acc.: 87.50%] [G loss: 3.447105]\n",
      "epoch:4 step:3507 [D loss: 0.308475, acc.: 88.28%] [G loss: 3.711460]\n",
      "epoch:4 step:3508 [D loss: 0.401562, acc.: 81.25%] [G loss: 2.489645]\n",
      "epoch:4 step:3509 [D loss: 0.309115, acc.: 91.41%] [G loss: 3.295622]\n",
      "epoch:4 step:3510 [D loss: 0.347384, acc.: 88.28%] [G loss: 3.588643]\n",
      "epoch:4 step:3511 [D loss: 0.405142, acc.: 85.16%] [G loss: 2.928154]\n",
      "epoch:4 step:3512 [D loss: 0.401692, acc.: 78.12%] [G loss: 2.376466]\n",
      "epoch:4 step:3513 [D loss: 0.308900, acc.: 87.50%] [G loss: 2.776201]\n",
      "epoch:4 step:3514 [D loss: 0.338968, acc.: 87.50%] [G loss: 2.865075]\n",
      "epoch:4 step:3515 [D loss: 0.361953, acc.: 82.81%] [G loss: 2.500822]\n",
      "epoch:4 step:3516 [D loss: 0.380050, acc.: 82.81%] [G loss: 3.875533]\n",
      "epoch:4 step:3517 [D loss: 0.426578, acc.: 78.12%] [G loss: 2.534841]\n",
      "epoch:4 step:3518 [D loss: 0.432812, acc.: 78.12%] [G loss: 2.661988]\n",
      "epoch:4 step:3519 [D loss: 0.398570, acc.: 79.69%] [G loss: 3.001224]\n",
      "epoch:4 step:3520 [D loss: 0.428230, acc.: 81.25%] [G loss: 2.691689]\n",
      "epoch:4 step:3521 [D loss: 0.355564, acc.: 85.94%] [G loss: 3.570470]\n",
      "epoch:4 step:3522 [D loss: 0.342302, acc.: 85.16%] [G loss: 2.535873]\n",
      "epoch:4 step:3523 [D loss: 0.349742, acc.: 85.94%] [G loss: 3.154471]\n",
      "epoch:4 step:3524 [D loss: 0.287352, acc.: 89.06%] [G loss: 3.091744]\n",
      "epoch:4 step:3525 [D loss: 0.405291, acc.: 81.25%] [G loss: 2.903538]\n",
      "epoch:4 step:3526 [D loss: 0.430666, acc.: 77.34%] [G loss: 1.866796]\n",
      "epoch:4 step:3527 [D loss: 0.322187, acc.: 88.28%] [G loss: 2.935970]\n",
      "epoch:4 step:3528 [D loss: 0.361008, acc.: 82.81%] [G loss: 3.190425]\n",
      "epoch:4 step:3529 [D loss: 0.315158, acc.: 89.06%] [G loss: 2.691941]\n",
      "epoch:4 step:3530 [D loss: 0.369204, acc.: 86.72%] [G loss: 3.979609]\n",
      "epoch:4 step:3531 [D loss: 0.584551, acc.: 75.78%] [G loss: 4.689192]\n",
      "epoch:4 step:3532 [D loss: 0.779223, acc.: 66.41%] [G loss: 4.116361]\n",
      "epoch:4 step:3533 [D loss: 0.555242, acc.: 71.88%] [G loss: 3.195765]\n",
      "epoch:4 step:3534 [D loss: 0.529186, acc.: 70.31%] [G loss: 2.024618]\n",
      "epoch:4 step:3535 [D loss: 0.306208, acc.: 89.84%] [G loss: 2.367903]\n",
      "epoch:4 step:3536 [D loss: 0.461816, acc.: 75.00%] [G loss: 2.739841]\n",
      "epoch:4 step:3537 [D loss: 0.443029, acc.: 75.78%] [G loss: 2.398610]\n",
      "epoch:4 step:3538 [D loss: 0.497483, acc.: 71.09%] [G loss: 2.248631]\n",
      "epoch:4 step:3539 [D loss: 0.351076, acc.: 82.03%] [G loss: 2.666416]\n",
      "epoch:4 step:3540 [D loss: 0.370965, acc.: 83.59%] [G loss: 2.495418]\n",
      "epoch:4 step:3541 [D loss: 0.315369, acc.: 89.06%] [G loss: 3.616736]\n",
      "epoch:4 step:3542 [D loss: 0.363434, acc.: 84.38%] [G loss: 2.816756]\n",
      "epoch:4 step:3543 [D loss: 0.311897, acc.: 85.16%] [G loss: 3.478549]\n",
      "epoch:4 step:3544 [D loss: 0.340448, acc.: 82.03%] [G loss: 2.573787]\n",
      "epoch:4 step:3545 [D loss: 0.383220, acc.: 83.59%] [G loss: 3.092803]\n",
      "epoch:4 step:3546 [D loss: 0.372485, acc.: 85.16%] [G loss: 2.698647]\n",
      "epoch:4 step:3547 [D loss: 0.344959, acc.: 88.28%] [G loss: 3.608083]\n",
      "epoch:4 step:3548 [D loss: 0.353856, acc.: 83.59%] [G loss: 2.571553]\n",
      "epoch:4 step:3549 [D loss: 0.359787, acc.: 85.16%] [G loss: 3.488204]\n",
      "epoch:4 step:3550 [D loss: 0.384789, acc.: 81.25%] [G loss: 3.117501]\n",
      "epoch:4 step:3551 [D loss: 0.369031, acc.: 80.47%] [G loss: 2.070609]\n",
      "epoch:4 step:3552 [D loss: 0.373521, acc.: 80.47%] [G loss: 5.465251]\n",
      "epoch:4 step:3553 [D loss: 0.661625, acc.: 67.97%] [G loss: 4.494601]\n",
      "epoch:4 step:3554 [D loss: 0.980159, acc.: 57.03%] [G loss: 3.193663]\n",
      "epoch:4 step:3555 [D loss: 0.377328, acc.: 85.16%] [G loss: 3.099165]\n",
      "epoch:4 step:3556 [D loss: 0.809544, acc.: 71.88%] [G loss: 4.723812]\n",
      "epoch:4 step:3557 [D loss: 1.063976, acc.: 70.31%] [G loss: 3.039176]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:4 step:3558 [D loss: 0.561097, acc.: 73.44%] [G loss: 3.430415]\n",
      "epoch:4 step:3559 [D loss: 0.555139, acc.: 71.09%] [G loss: 2.294636]\n",
      "epoch:4 step:3560 [D loss: 0.465513, acc.: 78.12%] [G loss: 2.150118]\n",
      "epoch:4 step:3561 [D loss: 0.375959, acc.: 86.72%] [G loss: 2.872827]\n",
      "epoch:4 step:3562 [D loss: 0.575801, acc.: 67.97%] [G loss: 2.120479]\n",
      "epoch:4 step:3563 [D loss: 0.484091, acc.: 74.22%] [G loss: 2.050898]\n",
      "epoch:4 step:3564 [D loss: 0.393260, acc.: 83.59%] [G loss: 2.586127]\n",
      "epoch:4 step:3565 [D loss: 0.366762, acc.: 85.94%] [G loss: 2.351564]\n",
      "epoch:4 step:3566 [D loss: 0.384842, acc.: 81.25%] [G loss: 2.746479]\n",
      "epoch:4 step:3567 [D loss: 0.421791, acc.: 82.81%] [G loss: 2.117735]\n",
      "epoch:4 step:3568 [D loss: 0.378245, acc.: 87.50%] [G loss: 2.941656]\n",
      "epoch:4 step:3569 [D loss: 0.296641, acc.: 88.28%] [G loss: 4.443259]\n",
      "epoch:4 step:3570 [D loss: 0.309510, acc.: 87.50%] [G loss: 2.744706]\n",
      "epoch:4 step:3571 [D loss: 0.353457, acc.: 85.94%] [G loss: 2.103195]\n",
      "epoch:4 step:3572 [D loss: 0.391065, acc.: 83.59%] [G loss: 2.049251]\n",
      "epoch:4 step:3573 [D loss: 0.290850, acc.: 90.62%] [G loss: 2.432797]\n",
      "epoch:4 step:3574 [D loss: 0.444795, acc.: 79.69%] [G loss: 2.516141]\n",
      "epoch:4 step:3575 [D loss: 0.329584, acc.: 87.50%] [G loss: 2.981400]\n",
      "epoch:4 step:3576 [D loss: 0.447577, acc.: 78.12%] [G loss: 3.054745]\n",
      "epoch:4 step:3577 [D loss: 0.370011, acc.: 82.81%] [G loss: 2.878279]\n",
      "epoch:4 step:3578 [D loss: 0.324674, acc.: 87.50%] [G loss: 3.296089]\n",
      "epoch:4 step:3579 [D loss: 0.303459, acc.: 92.97%] [G loss: 3.573848]\n",
      "epoch:4 step:3580 [D loss: 0.444566, acc.: 76.56%] [G loss: 3.850834]\n",
      "epoch:4 step:3581 [D loss: 0.480668, acc.: 74.22%] [G loss: 4.140772]\n",
      "epoch:4 step:3582 [D loss: 0.988193, acc.: 57.81%] [G loss: 4.281894]\n",
      "epoch:4 step:3583 [D loss: 0.660786, acc.: 67.19%] [G loss: 2.628524]\n",
      "epoch:4 step:3584 [D loss: 0.485188, acc.: 78.91%] [G loss: 2.752419]\n",
      "epoch:4 step:3585 [D loss: 0.408592, acc.: 80.47%] [G loss: 3.202983]\n",
      "epoch:4 step:3586 [D loss: 0.407240, acc.: 79.69%] [G loss: 2.710023]\n",
      "epoch:4 step:3587 [D loss: 0.377061, acc.: 81.25%] [G loss: 2.941557]\n",
      "epoch:4 step:3588 [D loss: 0.394418, acc.: 82.81%] [G loss: 2.239495]\n",
      "epoch:4 step:3589 [D loss: 0.333087, acc.: 85.16%] [G loss: 3.292158]\n",
      "epoch:4 step:3590 [D loss: 0.287652, acc.: 85.94%] [G loss: 3.447917]\n",
      "epoch:4 step:3591 [D loss: 0.339253, acc.: 83.59%] [G loss: 3.001019]\n",
      "epoch:4 step:3592 [D loss: 0.431829, acc.: 78.12%] [G loss: 2.084419]\n",
      "epoch:4 step:3593 [D loss: 0.424471, acc.: 78.91%] [G loss: 2.157627]\n",
      "epoch:4 step:3594 [D loss: 0.382860, acc.: 84.38%] [G loss: 2.750191]\n",
      "epoch:4 step:3595 [D loss: 0.269651, acc.: 89.84%] [G loss: 2.480098]\n",
      "epoch:4 step:3596 [D loss: 0.497020, acc.: 76.56%] [G loss: 1.919504]\n",
      "epoch:4 step:3597 [D loss: 0.419309, acc.: 78.91%] [G loss: 2.067306]\n",
      "epoch:4 step:3598 [D loss: 0.391739, acc.: 78.91%] [G loss: 2.281952]\n",
      "epoch:4 step:3599 [D loss: 0.370983, acc.: 81.25%] [G loss: 2.330866]\n",
      "epoch:4 step:3600 [D loss: 0.420280, acc.: 78.91%] [G loss: 2.493381]\n",
      "##############\n",
      "[0.85215762 0.84047456 0.79140073 0.8271129  0.79648766 0.8133456\n",
      " 0.88610928 0.83815809 0.82509175 0.81375942]\n",
      "##########\n",
      "epoch:4 step:3601 [D loss: 0.390825, acc.: 82.03%] [G loss: 3.041041]\n",
      "epoch:4 step:3602 [D loss: 0.311416, acc.: 89.06%] [G loss: 4.430923]\n",
      "epoch:4 step:3603 [D loss: 0.344400, acc.: 83.59%] [G loss: 3.990262]\n",
      "epoch:4 step:3604 [D loss: 0.401465, acc.: 78.12%] [G loss: 3.163425]\n",
      "epoch:4 step:3605 [D loss: 0.405593, acc.: 80.47%] [G loss: 2.316333]\n",
      "epoch:4 step:3606 [D loss: 0.411840, acc.: 78.12%] [G loss: 4.169170]\n",
      "epoch:4 step:3607 [D loss: 0.330334, acc.: 85.94%] [G loss: 3.182299]\n",
      "epoch:4 step:3608 [D loss: 0.343546, acc.: 85.16%] [G loss: 2.891175]\n",
      "epoch:4 step:3609 [D loss: 0.285525, acc.: 88.28%] [G loss: 3.201468]\n",
      "epoch:4 step:3610 [D loss: 0.391564, acc.: 79.69%] [G loss: 3.007448]\n",
      "epoch:4 step:3611 [D loss: 0.423408, acc.: 79.69%] [G loss: 2.539570]\n",
      "epoch:4 step:3612 [D loss: 0.383924, acc.: 78.12%] [G loss: 2.987674]\n",
      "epoch:4 step:3613 [D loss: 0.353510, acc.: 83.59%] [G loss: 4.191918]\n",
      "epoch:4 step:3614 [D loss: 0.269604, acc.: 89.84%] [G loss: 3.909040]\n",
      "epoch:4 step:3615 [D loss: 0.420054, acc.: 81.25%] [G loss: 2.701844]\n",
      "epoch:4 step:3616 [D loss: 0.396180, acc.: 83.59%] [G loss: 2.383799]\n",
      "epoch:4 step:3617 [D loss: 0.413425, acc.: 76.56%] [G loss: 2.695285]\n",
      "epoch:4 step:3618 [D loss: 0.386204, acc.: 79.69%] [G loss: 3.133874]\n",
      "epoch:4 step:3619 [D loss: 0.422548, acc.: 79.69%] [G loss: 3.104442]\n",
      "epoch:4 step:3620 [D loss: 0.433336, acc.: 86.72%] [G loss: 2.966160]\n",
      "epoch:4 step:3621 [D loss: 0.426667, acc.: 78.12%] [G loss: 3.272357]\n",
      "epoch:4 step:3622 [D loss: 0.376840, acc.: 78.91%] [G loss: 2.291603]\n",
      "epoch:4 step:3623 [D loss: 0.341110, acc.: 87.50%] [G loss: 3.029164]\n",
      "epoch:4 step:3624 [D loss: 0.458199, acc.: 78.12%] [G loss: 2.300512]\n",
      "epoch:4 step:3625 [D loss: 0.465776, acc.: 75.78%] [G loss: 2.076442]\n",
      "epoch:4 step:3626 [D loss: 0.344312, acc.: 83.59%] [G loss: 2.543694]\n",
      "epoch:4 step:3627 [D loss: 0.381269, acc.: 81.25%] [G loss: 2.603047]\n",
      "epoch:4 step:3628 [D loss: 0.395816, acc.: 80.47%] [G loss: 2.216484]\n",
      "epoch:4 step:3629 [D loss: 0.498483, acc.: 76.56%] [G loss: 1.926672]\n",
      "epoch:4 step:3630 [D loss: 0.419633, acc.: 80.47%] [G loss: 2.426307]\n",
      "epoch:4 step:3631 [D loss: 0.382827, acc.: 84.38%] [G loss: 2.333672]\n",
      "epoch:4 step:3632 [D loss: 0.331919, acc.: 84.38%] [G loss: 2.570131]\n",
      "epoch:4 step:3633 [D loss: 0.349165, acc.: 85.16%] [G loss: 1.737270]\n",
      "epoch:4 step:3634 [D loss: 0.407975, acc.: 81.25%] [G loss: 2.047708]\n",
      "epoch:4 step:3635 [D loss: 0.411531, acc.: 81.25%] [G loss: 2.338530]\n",
      "epoch:4 step:3636 [D loss: 0.321118, acc.: 89.84%] [G loss: 2.965892]\n",
      "epoch:4 step:3637 [D loss: 0.402511, acc.: 82.81%] [G loss: 2.905805]\n",
      "epoch:4 step:3638 [D loss: 0.383482, acc.: 78.12%] [G loss: 2.620425]\n",
      "epoch:4 step:3639 [D loss: 0.342135, acc.: 82.81%] [G loss: 2.742280]\n",
      "epoch:4 step:3640 [D loss: 0.369345, acc.: 83.59%] [G loss: 2.798076]\n",
      "epoch:4 step:3641 [D loss: 0.454805, acc.: 77.34%] [G loss: 1.987999]\n",
      "epoch:4 step:3642 [D loss: 0.352571, acc.: 86.72%] [G loss: 2.421372]\n",
      "epoch:4 step:3643 [D loss: 0.388651, acc.: 82.81%] [G loss: 3.364559]\n",
      "epoch:4 step:3644 [D loss: 0.452787, acc.: 78.91%] [G loss: 4.812611]\n",
      "epoch:4 step:3645 [D loss: 0.627539, acc.: 71.88%] [G loss: 3.208179]\n",
      "epoch:4 step:3646 [D loss: 0.359526, acc.: 84.38%] [G loss: 2.860863]\n",
      "epoch:4 step:3647 [D loss: 0.344229, acc.: 84.38%] [G loss: 3.010984]\n",
      "epoch:4 step:3648 [D loss: 0.266075, acc.: 89.84%] [G loss: 3.003517]\n",
      "epoch:4 step:3649 [D loss: 0.322277, acc.: 87.50%] [G loss: 2.312894]\n",
      "epoch:4 step:3650 [D loss: 0.438371, acc.: 77.34%] [G loss: 2.939031]\n",
      "epoch:4 step:3651 [D loss: 0.357860, acc.: 86.72%] [G loss: 2.983342]\n",
      "epoch:4 step:3652 [D loss: 0.421123, acc.: 79.69%] [G loss: 2.954696]\n",
      "epoch:4 step:3653 [D loss: 0.339272, acc.: 82.03%] [G loss: 2.791294]\n",
      "epoch:4 step:3654 [D loss: 0.393694, acc.: 83.59%] [G loss: 2.991534]\n",
      "epoch:4 step:3655 [D loss: 0.390647, acc.: 82.81%] [G loss: 2.291395]\n",
      "epoch:4 step:3656 [D loss: 0.358078, acc.: 81.25%] [G loss: 2.602099]\n",
      "epoch:4 step:3657 [D loss: 0.362451, acc.: 82.81%] [G loss: 3.093740]\n",
      "epoch:4 step:3658 [D loss: 0.318270, acc.: 86.72%] [G loss: 3.444720]\n",
      "epoch:4 step:3659 [D loss: 0.333835, acc.: 85.94%] [G loss: 3.042375]\n",
      "epoch:4 step:3660 [D loss: 0.411503, acc.: 81.25%] [G loss: 2.758704]\n",
      "epoch:4 step:3661 [D loss: 0.323042, acc.: 85.16%] [G loss: 2.014025]\n",
      "epoch:4 step:3662 [D loss: 0.399672, acc.: 78.91%] [G loss: 2.288565]\n",
      "epoch:4 step:3663 [D loss: 0.385881, acc.: 81.25%] [G loss: 2.732077]\n",
      "epoch:4 step:3664 [D loss: 0.404099, acc.: 78.12%] [G loss: 2.407820]\n",
      "epoch:4 step:3665 [D loss: 0.359869, acc.: 86.72%] [G loss: 3.080750]\n",
      "epoch:4 step:3666 [D loss: 0.432395, acc.: 80.47%] [G loss: 2.937140]\n",
      "epoch:4 step:3667 [D loss: 0.374531, acc.: 85.94%] [G loss: 2.365773]\n",
      "epoch:4 step:3668 [D loss: 0.329338, acc.: 87.50%] [G loss: 2.431152]\n",
      "epoch:4 step:3669 [D loss: 0.327278, acc.: 86.72%] [G loss: 2.819038]\n",
      "epoch:4 step:3670 [D loss: 0.308430, acc.: 85.16%] [G loss: 3.396001]\n",
      "epoch:4 step:3671 [D loss: 0.315587, acc.: 89.06%] [G loss: 2.939691]\n",
      "epoch:4 step:3672 [D loss: 0.348223, acc.: 82.81%] [G loss: 2.778904]\n",
      "epoch:4 step:3673 [D loss: 0.324673, acc.: 92.19%] [G loss: 3.069046]\n",
      "epoch:4 step:3674 [D loss: 0.299385, acc.: 89.06%] [G loss: 3.364252]\n",
      "epoch:4 step:3675 [D loss: 0.312849, acc.: 83.59%] [G loss: 3.049004]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:4 step:3676 [D loss: 0.441413, acc.: 82.03%] [G loss: 5.768971]\n",
      "epoch:4 step:3677 [D loss: 0.955655, acc.: 57.81%] [G loss: 7.381300]\n",
      "epoch:4 step:3678 [D loss: 1.780802, acc.: 49.22%] [G loss: 4.654793]\n",
      "epoch:4 step:3679 [D loss: 0.858311, acc.: 62.50%] [G loss: 3.437871]\n",
      "epoch:4 step:3680 [D loss: 0.575348, acc.: 75.78%] [G loss: 2.904431]\n",
      "epoch:4 step:3681 [D loss: 0.434357, acc.: 81.25%] [G loss: 1.979259]\n",
      "epoch:4 step:3682 [D loss: 0.475817, acc.: 72.66%] [G loss: 2.941064]\n",
      "epoch:4 step:3683 [D loss: 0.306655, acc.: 84.38%] [G loss: 3.038330]\n",
      "epoch:4 step:3684 [D loss: 0.413445, acc.: 78.91%] [G loss: 2.798726]\n",
      "epoch:4 step:3685 [D loss: 0.317468, acc.: 87.50%] [G loss: 3.725772]\n",
      "epoch:4 step:3686 [D loss: 0.357967, acc.: 82.03%] [G loss: 2.877616]\n",
      "epoch:4 step:3687 [D loss: 0.365304, acc.: 83.59%] [G loss: 2.475573]\n",
      "epoch:4 step:3688 [D loss: 0.413085, acc.: 79.69%] [G loss: 1.930142]\n",
      "epoch:4 step:3689 [D loss: 0.393014, acc.: 82.81%] [G loss: 2.909476]\n",
      "epoch:4 step:3690 [D loss: 0.442847, acc.: 78.12%] [G loss: 2.677828]\n",
      "epoch:4 step:3691 [D loss: 0.410313, acc.: 82.81%] [G loss: 1.974514]\n",
      "epoch:4 step:3692 [D loss: 0.327840, acc.: 90.62%] [G loss: 3.586498]\n",
      "epoch:4 step:3693 [D loss: 0.361666, acc.: 83.59%] [G loss: 2.869771]\n",
      "epoch:4 step:3694 [D loss: 0.230913, acc.: 92.97%] [G loss: 4.623474]\n",
      "epoch:4 step:3695 [D loss: 0.276332, acc.: 89.84%] [G loss: 2.526083]\n",
      "epoch:4 step:3696 [D loss: 0.313346, acc.: 85.16%] [G loss: 2.785773]\n",
      "epoch:4 step:3697 [D loss: 0.324834, acc.: 89.84%] [G loss: 2.788742]\n",
      "epoch:4 step:3698 [D loss: 0.413773, acc.: 82.03%] [G loss: 3.774993]\n",
      "epoch:4 step:3699 [D loss: 0.759118, acc.: 67.19%] [G loss: 2.847655]\n",
      "epoch:4 step:3700 [D loss: 0.710525, acc.: 71.09%] [G loss: 2.395288]\n",
      "epoch:4 step:3701 [D loss: 0.308825, acc.: 86.72%] [G loss: 3.246796]\n",
      "epoch:4 step:3702 [D loss: 0.439118, acc.: 78.91%] [G loss: 2.407008]\n",
      "epoch:4 step:3703 [D loss: 0.353222, acc.: 85.94%] [G loss: 2.728061]\n",
      "epoch:4 step:3704 [D loss: 0.433564, acc.: 78.12%] [G loss: 2.949712]\n",
      "epoch:4 step:3705 [D loss: 0.295914, acc.: 89.06%] [G loss: 3.223793]\n",
      "epoch:4 step:3706 [D loss: 0.387115, acc.: 81.25%] [G loss: 3.062707]\n",
      "epoch:4 step:3707 [D loss: 0.451815, acc.: 75.78%] [G loss: 3.577876]\n",
      "epoch:4 step:3708 [D loss: 0.393670, acc.: 80.47%] [G loss: 2.793719]\n",
      "epoch:4 step:3709 [D loss: 0.282942, acc.: 88.28%] [G loss: 3.337487]\n",
      "epoch:4 step:3710 [D loss: 0.271807, acc.: 89.06%] [G loss: 4.224438]\n",
      "epoch:4 step:3711 [D loss: 0.314288, acc.: 85.94%] [G loss: 2.512386]\n",
      "epoch:4 step:3712 [D loss: 0.281420, acc.: 89.06%] [G loss: 3.387422]\n",
      "epoch:4 step:3713 [D loss: 0.323127, acc.: 84.38%] [G loss: 2.290699]\n",
      "epoch:4 step:3714 [D loss: 0.338957, acc.: 86.72%] [G loss: 2.176845]\n",
      "epoch:4 step:3715 [D loss: 0.381578, acc.: 82.03%] [G loss: 2.605328]\n",
      "epoch:4 step:3716 [D loss: 0.399791, acc.: 82.81%] [G loss: 2.073898]\n",
      "epoch:4 step:3717 [D loss: 0.450129, acc.: 78.12%] [G loss: 3.337960]\n",
      "epoch:4 step:3718 [D loss: 0.567152, acc.: 75.78%] [G loss: 3.707574]\n",
      "epoch:4 step:3719 [D loss: 0.615266, acc.: 70.31%] [G loss: 3.679145]\n",
      "epoch:4 step:3720 [D loss: 1.010981, acc.: 57.03%] [G loss: 2.833951]\n",
      "epoch:4 step:3721 [D loss: 0.684125, acc.: 68.75%] [G loss: 1.812918]\n",
      "epoch:4 step:3722 [D loss: 0.391277, acc.: 80.47%] [G loss: 2.863528]\n",
      "epoch:4 step:3723 [D loss: 0.434726, acc.: 76.56%] [G loss: 2.843123]\n",
      "epoch:4 step:3724 [D loss: 0.346820, acc.: 85.16%] [G loss: 3.082804]\n",
      "epoch:4 step:3725 [D loss: 0.291361, acc.: 86.72%] [G loss: 2.702260]\n",
      "epoch:4 step:3726 [D loss: 0.305864, acc.: 86.72%] [G loss: 2.739011]\n",
      "epoch:4 step:3727 [D loss: 0.263304, acc.: 92.19%] [G loss: 2.926871]\n",
      "epoch:4 step:3728 [D loss: 0.347914, acc.: 88.28%] [G loss: 2.674355]\n",
      "epoch:4 step:3729 [D loss: 0.359894, acc.: 86.72%] [G loss: 1.727978]\n",
      "epoch:4 step:3730 [D loss: 0.348132, acc.: 84.38%] [G loss: 2.328492]\n",
      "epoch:4 step:3731 [D loss: 0.353854, acc.: 85.94%] [G loss: 2.714894]\n",
      "epoch:4 step:3732 [D loss: 0.397048, acc.: 79.69%] [G loss: 3.053797]\n",
      "epoch:4 step:3733 [D loss: 0.338926, acc.: 89.84%] [G loss: 2.247282]\n",
      "epoch:4 step:3734 [D loss: 0.342250, acc.: 82.81%] [G loss: 2.454798]\n",
      "epoch:4 step:3735 [D loss: 0.362198, acc.: 82.81%] [G loss: 2.248341]\n",
      "epoch:4 step:3736 [D loss: 0.413888, acc.: 78.12%] [G loss: 2.596618]\n",
      "epoch:4 step:3737 [D loss: 0.386620, acc.: 85.16%] [G loss: 2.609125]\n",
      "epoch:4 step:3738 [D loss: 0.346497, acc.: 88.28%] [G loss: 2.698946]\n",
      "epoch:4 step:3739 [D loss: 0.260823, acc.: 92.97%] [G loss: 2.543321]\n",
      "epoch:4 step:3740 [D loss: 0.390217, acc.: 85.94%] [G loss: 2.359870]\n",
      "epoch:4 step:3741 [D loss: 0.437469, acc.: 79.69%] [G loss: 2.187585]\n",
      "epoch:4 step:3742 [D loss: 0.369171, acc.: 88.28%] [G loss: 2.609428]\n",
      "epoch:4 step:3743 [D loss: 0.379809, acc.: 82.81%] [G loss: 2.100864]\n",
      "epoch:4 step:3744 [D loss: 0.394001, acc.: 80.47%] [G loss: 2.207701]\n",
      "epoch:4 step:3745 [D loss: 0.386219, acc.: 81.25%] [G loss: 2.050837]\n",
      "epoch:4 step:3746 [D loss: 0.304692, acc.: 85.94%] [G loss: 2.239800]\n",
      "epoch:4 step:3747 [D loss: 0.405245, acc.: 82.03%] [G loss: 2.748536]\n",
      "epoch:4 step:3748 [D loss: 0.416122, acc.: 83.59%] [G loss: 2.618501]\n",
      "epoch:4 step:3749 [D loss: 0.400445, acc.: 79.69%] [G loss: 2.810393]\n",
      "epoch:4 step:3750 [D loss: 0.370844, acc.: 83.59%] [G loss: 2.693875]\n",
      "epoch:4 step:3751 [D loss: 0.320168, acc.: 89.84%] [G loss: 2.545615]\n",
      "epoch:4 step:3752 [D loss: 0.323308, acc.: 82.81%] [G loss: 2.807747]\n",
      "epoch:4 step:3753 [D loss: 0.446633, acc.: 78.12%] [G loss: 2.115669]\n",
      "epoch:4 step:3754 [D loss: 0.417239, acc.: 80.47%] [G loss: 2.319652]\n",
      "epoch:4 step:3755 [D loss: 0.362037, acc.: 83.59%] [G loss: 3.207233]\n",
      "epoch:4 step:3756 [D loss: 0.498756, acc.: 78.91%] [G loss: 4.332550]\n",
      "epoch:4 step:3757 [D loss: 0.687272, acc.: 71.09%] [G loss: 4.510074]\n",
      "epoch:4 step:3758 [D loss: 0.917603, acc.: 58.59%] [G loss: 4.369624]\n",
      "epoch:4 step:3759 [D loss: 1.073260, acc.: 56.25%] [G loss: 2.538985]\n",
      "epoch:4 step:3760 [D loss: 0.432195, acc.: 82.03%] [G loss: 2.538619]\n",
      "epoch:4 step:3761 [D loss: 0.402643, acc.: 79.69%] [G loss: 4.114120]\n",
      "epoch:4 step:3762 [D loss: 0.455662, acc.: 72.66%] [G loss: 2.626764]\n",
      "epoch:4 step:3763 [D loss: 0.395453, acc.: 81.25%] [G loss: 2.510620]\n",
      "epoch:4 step:3764 [D loss: 0.344315, acc.: 84.38%] [G loss: 2.214243]\n",
      "epoch:4 step:3765 [D loss: 0.365808, acc.: 82.03%] [G loss: 3.056591]\n",
      "epoch:4 step:3766 [D loss: 0.381841, acc.: 82.03%] [G loss: 2.748657]\n",
      "epoch:4 step:3767 [D loss: 0.354160, acc.: 82.03%] [G loss: 1.810565]\n",
      "epoch:4 step:3768 [D loss: 0.372485, acc.: 78.91%] [G loss: 2.499175]\n",
      "epoch:4 step:3769 [D loss: 0.379638, acc.: 84.38%] [G loss: 2.441529]\n",
      "epoch:4 step:3770 [D loss: 0.435955, acc.: 78.91%] [G loss: 2.165197]\n",
      "epoch:4 step:3771 [D loss: 0.307037, acc.: 87.50%] [G loss: 2.732532]\n",
      "epoch:4 step:3772 [D loss: 0.286852, acc.: 89.84%] [G loss: 3.338869]\n",
      "epoch:4 step:3773 [D loss: 0.331849, acc.: 85.94%] [G loss: 2.497527]\n",
      "epoch:4 step:3774 [D loss: 0.310017, acc.: 88.28%] [G loss: 2.753943]\n",
      "epoch:4 step:3775 [D loss: 0.350314, acc.: 86.72%] [G loss: 3.866889]\n",
      "epoch:4 step:3776 [D loss: 0.302562, acc.: 89.06%] [G loss: 2.774938]\n",
      "epoch:4 step:3777 [D loss: 0.354094, acc.: 84.38%] [G loss: 3.134806]\n",
      "epoch:4 step:3778 [D loss: 0.378479, acc.: 80.47%] [G loss: 2.315540]\n",
      "epoch:4 step:3779 [D loss: 0.331633, acc.: 85.16%] [G loss: 2.421288]\n",
      "epoch:4 step:3780 [D loss: 0.574567, acc.: 71.88%] [G loss: 2.539087]\n",
      "epoch:4 step:3781 [D loss: 0.444261, acc.: 80.47%] [G loss: 4.928113]\n",
      "epoch:4 step:3782 [D loss: 0.618970, acc.: 67.97%] [G loss: 3.245621]\n",
      "epoch:4 step:3783 [D loss: 0.528068, acc.: 78.12%] [G loss: 3.374318]\n",
      "epoch:4 step:3784 [D loss: 0.581670, acc.: 76.56%] [G loss: 4.015888]\n",
      "epoch:4 step:3785 [D loss: 0.664007, acc.: 61.72%] [G loss: 2.499722]\n",
      "epoch:4 step:3786 [D loss: 0.303146, acc.: 86.72%] [G loss: 3.652935]\n",
      "epoch:4 step:3787 [D loss: 0.340480, acc.: 85.16%] [G loss: 2.672907]\n",
      "epoch:4 step:3788 [D loss: 0.371731, acc.: 82.81%] [G loss: 2.170195]\n",
      "epoch:4 step:3789 [D loss: 0.329380, acc.: 85.94%] [G loss: 2.760291]\n",
      "epoch:4 step:3790 [D loss: 0.294965, acc.: 85.16%] [G loss: 2.830473]\n",
      "epoch:4 step:3791 [D loss: 0.339071, acc.: 86.72%] [G loss: 2.415921]\n",
      "epoch:4 step:3792 [D loss: 0.385578, acc.: 82.81%] [G loss: 2.440782]\n",
      "epoch:4 step:3793 [D loss: 0.419055, acc.: 77.34%] [G loss: 2.628892]\n",
      "epoch:4 step:3794 [D loss: 0.379954, acc.: 79.69%] [G loss: 3.502415]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:4 step:3795 [D loss: 0.346558, acc.: 83.59%] [G loss: 2.204778]\n",
      "epoch:4 step:3796 [D loss: 0.469790, acc.: 76.56%] [G loss: 3.014939]\n",
      "epoch:4 step:3797 [D loss: 0.650651, acc.: 62.50%] [G loss: 4.282280]\n",
      "epoch:4 step:3798 [D loss: 0.621587, acc.: 71.88%] [G loss: 3.226243]\n",
      "epoch:4 step:3799 [D loss: 0.503071, acc.: 69.53%] [G loss: 3.125017]\n",
      "epoch:4 step:3800 [D loss: 0.492661, acc.: 74.22%] [G loss: 2.564581]\n",
      "##############\n",
      "[0.83345864 0.86759744 0.80453644 0.80879715 0.75536727 0.8198478\n",
      " 0.89427509 0.81837678 0.8145799  0.80532436]\n",
      "##########\n",
      "epoch:4 step:3801 [D loss: 0.378323, acc.: 82.81%] [G loss: 2.839037]\n",
      "epoch:4 step:3802 [D loss: 0.399806, acc.: 80.47%] [G loss: 2.852018]\n",
      "epoch:4 step:3803 [D loss: 0.325566, acc.: 85.94%] [G loss: 2.463497]\n",
      "epoch:4 step:3804 [D loss: 0.343394, acc.: 83.59%] [G loss: 2.669224]\n",
      "epoch:4 step:3805 [D loss: 0.357335, acc.: 85.16%] [G loss: 2.568827]\n",
      "epoch:4 step:3806 [D loss: 0.404467, acc.: 81.25%] [G loss: 2.557066]\n",
      "epoch:4 step:3807 [D loss: 0.465789, acc.: 74.22%] [G loss: 2.134496]\n",
      "epoch:4 step:3808 [D loss: 0.378287, acc.: 80.47%] [G loss: 2.022562]\n",
      "epoch:4 step:3809 [D loss: 0.342723, acc.: 85.94%] [G loss: 2.168326]\n",
      "epoch:4 step:3810 [D loss: 0.327940, acc.: 82.03%] [G loss: 2.090043]\n",
      "epoch:4 step:3811 [D loss: 0.471584, acc.: 75.78%] [G loss: 2.311327]\n",
      "epoch:4 step:3812 [D loss: 0.391168, acc.: 82.03%] [G loss: 2.515962]\n",
      "epoch:4 step:3813 [D loss: 0.420300, acc.: 82.03%] [G loss: 2.133660]\n",
      "epoch:4 step:3814 [D loss: 0.331692, acc.: 89.06%] [G loss: 2.589807]\n",
      "epoch:4 step:3815 [D loss: 0.377447, acc.: 82.03%] [G loss: 2.585876]\n",
      "epoch:4 step:3816 [D loss: 0.337386, acc.: 86.72%] [G loss: 2.768828]\n",
      "epoch:4 step:3817 [D loss: 0.308959, acc.: 89.84%] [G loss: 3.181680]\n",
      "epoch:4 step:3818 [D loss: 0.357566, acc.: 82.81%] [G loss: 2.316926]\n",
      "epoch:4 step:3819 [D loss: 0.391767, acc.: 82.81%] [G loss: 2.842115]\n",
      "epoch:4 step:3820 [D loss: 0.394504, acc.: 82.03%] [G loss: 2.428925]\n",
      "epoch:4 step:3821 [D loss: 0.362999, acc.: 86.72%] [G loss: 3.493542]\n",
      "epoch:4 step:3822 [D loss: 0.278258, acc.: 86.72%] [G loss: 4.182364]\n",
      "epoch:4 step:3823 [D loss: 0.355103, acc.: 83.59%] [G loss: 2.737500]\n",
      "epoch:4 step:3824 [D loss: 0.385592, acc.: 83.59%] [G loss: 3.108836]\n",
      "epoch:4 step:3825 [D loss: 0.328191, acc.: 88.28%] [G loss: 2.488279]\n",
      "epoch:4 step:3826 [D loss: 0.377578, acc.: 81.25%] [G loss: 2.683199]\n",
      "epoch:4 step:3827 [D loss: 0.291057, acc.: 90.62%] [G loss: 3.258449]\n",
      "epoch:4 step:3828 [D loss: 0.332723, acc.: 84.38%] [G loss: 2.857834]\n",
      "epoch:4 step:3829 [D loss: 0.404559, acc.: 83.59%] [G loss: 3.142474]\n",
      "epoch:4 step:3830 [D loss: 0.306806, acc.: 89.84%] [G loss: 2.754060]\n",
      "epoch:4 step:3831 [D loss: 0.446922, acc.: 79.69%] [G loss: 3.570279]\n",
      "epoch:4 step:3832 [D loss: 0.424927, acc.: 77.34%] [G loss: 3.425531]\n",
      "epoch:4 step:3833 [D loss: 0.444339, acc.: 79.69%] [G loss: 3.905145]\n",
      "epoch:4 step:3834 [D loss: 0.527112, acc.: 74.22%] [G loss: 4.126756]\n",
      "epoch:4 step:3835 [D loss: 0.654048, acc.: 64.84%] [G loss: 3.511023]\n",
      "epoch:4 step:3836 [D loss: 0.551055, acc.: 77.34%] [G loss: 2.088063]\n",
      "epoch:4 step:3837 [D loss: 0.550927, acc.: 71.09%] [G loss: 2.017202]\n",
      "epoch:4 step:3838 [D loss: 0.448514, acc.: 79.69%] [G loss: 2.641335]\n",
      "epoch:4 step:3839 [D loss: 0.530318, acc.: 70.31%] [G loss: 2.427490]\n",
      "epoch:4 step:3840 [D loss: 0.402323, acc.: 80.47%] [G loss: 2.609756]\n",
      "epoch:4 step:3841 [D loss: 0.383678, acc.: 80.47%] [G loss: 2.666625]\n",
      "epoch:4 step:3842 [D loss: 0.413938, acc.: 82.81%] [G loss: 1.994128]\n",
      "epoch:4 step:3843 [D loss: 0.413406, acc.: 82.03%] [G loss: 1.966873]\n",
      "epoch:4 step:3844 [D loss: 0.381922, acc.: 82.81%] [G loss: 2.163939]\n",
      "epoch:4 step:3845 [D loss: 0.394406, acc.: 82.81%] [G loss: 2.641217]\n",
      "epoch:4 step:3846 [D loss: 0.443963, acc.: 77.34%] [G loss: 3.082055]\n",
      "epoch:4 step:3847 [D loss: 0.404673, acc.: 82.81%] [G loss: 3.532776]\n",
      "epoch:4 step:3848 [D loss: 0.399741, acc.: 80.47%] [G loss: 2.887932]\n",
      "epoch:4 step:3849 [D loss: 0.405285, acc.: 78.91%] [G loss: 2.706864]\n",
      "epoch:4 step:3850 [D loss: 0.378982, acc.: 81.25%] [G loss: 3.493734]\n",
      "epoch:4 step:3851 [D loss: 0.408144, acc.: 78.12%] [G loss: 2.951863]\n",
      "epoch:4 step:3852 [D loss: 0.354700, acc.: 82.81%] [G loss: 2.139359]\n",
      "epoch:4 step:3853 [D loss: 0.354646, acc.: 83.59%] [G loss: 2.217576]\n",
      "epoch:4 step:3854 [D loss: 0.360640, acc.: 84.38%] [G loss: 2.591327]\n",
      "epoch:4 step:3855 [D loss: 0.344841, acc.: 85.94%] [G loss: 2.503663]\n",
      "epoch:4 step:3856 [D loss: 0.380379, acc.: 86.72%] [G loss: 2.056913]\n",
      "epoch:4 step:3857 [D loss: 0.399369, acc.: 79.69%] [G loss: 2.609894]\n",
      "epoch:4 step:3858 [D loss: 0.391224, acc.: 82.03%] [G loss: 2.354811]\n",
      "epoch:4 step:3859 [D loss: 0.382032, acc.: 83.59%] [G loss: 2.891904]\n",
      "epoch:4 step:3860 [D loss: 0.291239, acc.: 91.41%] [G loss: 2.933103]\n",
      "epoch:4 step:3861 [D loss: 0.267563, acc.: 92.97%] [G loss: 3.720097]\n",
      "epoch:4 step:3862 [D loss: 0.438443, acc.: 78.91%] [G loss: 2.179915]\n",
      "epoch:4 step:3863 [D loss: 0.376718, acc.: 82.03%] [G loss: 2.321364]\n",
      "epoch:4 step:3864 [D loss: 0.305206, acc.: 89.84%] [G loss: 3.359301]\n",
      "epoch:4 step:3865 [D loss: 0.271343, acc.: 90.62%] [G loss: 3.943484]\n",
      "epoch:4 step:3866 [D loss: 0.355444, acc.: 86.72%] [G loss: 2.893638]\n",
      "epoch:4 step:3867 [D loss: 0.380323, acc.: 79.69%] [G loss: 2.257722]\n",
      "epoch:4 step:3868 [D loss: 0.326556, acc.: 86.72%] [G loss: 2.799101]\n",
      "epoch:4 step:3869 [D loss: 0.305188, acc.: 85.94%] [G loss: 2.236917]\n",
      "epoch:4 step:3870 [D loss: 0.316771, acc.: 87.50%] [G loss: 2.881969]\n",
      "epoch:4 step:3871 [D loss: 0.360709, acc.: 84.38%] [G loss: 2.017342]\n",
      "epoch:4 step:3872 [D loss: 0.295957, acc.: 89.84%] [G loss: 2.387261]\n",
      "epoch:4 step:3873 [D loss: 0.520231, acc.: 72.66%] [G loss: 2.949852]\n",
      "epoch:4 step:3874 [D loss: 0.407449, acc.: 78.91%] [G loss: 3.778370]\n",
      "epoch:4 step:3875 [D loss: 0.669681, acc.: 72.66%] [G loss: 6.204115]\n",
      "epoch:4 step:3876 [D loss: 1.741250, acc.: 51.56%] [G loss: 6.999728]\n",
      "epoch:4 step:3877 [D loss: 1.842889, acc.: 48.44%] [G loss: 2.472569]\n",
      "epoch:4 step:3878 [D loss: 0.460542, acc.: 75.78%] [G loss: 4.793974]\n",
      "epoch:4 step:3879 [D loss: 0.530799, acc.: 76.56%] [G loss: 2.472271]\n",
      "epoch:4 step:3880 [D loss: 0.360263, acc.: 81.25%] [G loss: 2.353141]\n",
      "epoch:4 step:3881 [D loss: 0.411838, acc.: 77.34%] [G loss: 2.837325]\n",
      "epoch:4 step:3882 [D loss: 0.387200, acc.: 81.25%] [G loss: 2.982871]\n",
      "epoch:4 step:3883 [D loss: 0.331938, acc.: 83.59%] [G loss: 2.828841]\n",
      "epoch:4 step:3884 [D loss: 0.428516, acc.: 77.34%] [G loss: 2.703605]\n",
      "epoch:4 step:3885 [D loss: 0.315124, acc.: 84.38%] [G loss: 2.334177]\n",
      "epoch:4 step:3886 [D loss: 0.379905, acc.: 85.16%] [G loss: 2.503519]\n",
      "epoch:4 step:3887 [D loss: 0.379643, acc.: 85.94%] [G loss: 2.806831]\n",
      "epoch:4 step:3888 [D loss: 0.377201, acc.: 87.50%] [G loss: 2.320094]\n",
      "epoch:4 step:3889 [D loss: 0.560741, acc.: 72.66%] [G loss: 2.822394]\n",
      "epoch:4 step:3890 [D loss: 0.358033, acc.: 86.72%] [G loss: 2.348040]\n",
      "epoch:4 step:3891 [D loss: 0.363209, acc.: 86.72%] [G loss: 2.471355]\n",
      "epoch:4 step:3892 [D loss: 0.339785, acc.: 87.50%] [G loss: 2.705945]\n",
      "epoch:4 step:3893 [D loss: 0.369709, acc.: 85.94%] [G loss: 2.587207]\n",
      "epoch:4 step:3894 [D loss: 0.235294, acc.: 91.41%] [G loss: 3.878792]\n",
      "epoch:4 step:3895 [D loss: 0.323305, acc.: 83.59%] [G loss: 2.540869]\n",
      "epoch:4 step:3896 [D loss: 0.362777, acc.: 81.25%] [G loss: 2.453583]\n",
      "epoch:4 step:3897 [D loss: 0.313017, acc.: 87.50%] [G loss: 2.497117]\n",
      "epoch:4 step:3898 [D loss: 0.410813, acc.: 80.47%] [G loss: 1.841023]\n",
      "epoch:4 step:3899 [D loss: 0.431420, acc.: 79.69%] [G loss: 2.548495]\n",
      "epoch:4 step:3900 [D loss: 0.305758, acc.: 87.50%] [G loss: 2.142526]\n",
      "epoch:4 step:3901 [D loss: 0.354059, acc.: 84.38%] [G loss: 3.168475]\n",
      "epoch:4 step:3902 [D loss: 0.432221, acc.: 79.69%] [G loss: 2.101435]\n",
      "epoch:4 step:3903 [D loss: 0.339462, acc.: 87.50%] [G loss: 3.362979]\n",
      "epoch:4 step:3904 [D loss: 0.492744, acc.: 73.44%] [G loss: 2.903528]\n",
      "epoch:4 step:3905 [D loss: 0.417227, acc.: 82.81%] [G loss: 2.095722]\n",
      "epoch:5 step:3906 [D loss: 0.505723, acc.: 76.56%] [G loss: 1.975364]\n",
      "epoch:5 step:3907 [D loss: 0.394915, acc.: 81.25%] [G loss: 2.114773]\n",
      "epoch:5 step:3908 [D loss: 0.369125, acc.: 85.16%] [G loss: 2.474461]\n",
      "epoch:5 step:3909 [D loss: 0.404419, acc.: 80.47%] [G loss: 2.097148]\n",
      "epoch:5 step:3910 [D loss: 0.356904, acc.: 88.28%] [G loss: 2.387089]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:5 step:3911 [D loss: 0.399463, acc.: 81.25%] [G loss: 2.659769]\n",
      "epoch:5 step:3912 [D loss: 0.396592, acc.: 82.03%] [G loss: 2.474195]\n",
      "epoch:5 step:3913 [D loss: 0.454562, acc.: 75.78%] [G loss: 2.323835]\n",
      "epoch:5 step:3914 [D loss: 0.370321, acc.: 85.16%] [G loss: 1.839043]\n",
      "epoch:5 step:3915 [D loss: 0.379896, acc.: 80.47%] [G loss: 2.660754]\n",
      "epoch:5 step:3916 [D loss: 0.423385, acc.: 80.47%] [G loss: 2.449284]\n",
      "epoch:5 step:3917 [D loss: 0.353396, acc.: 81.25%] [G loss: 2.803040]\n",
      "epoch:5 step:3918 [D loss: 0.419237, acc.: 79.69%] [G loss: 2.604765]\n",
      "epoch:5 step:3919 [D loss: 0.481638, acc.: 75.78%] [G loss: 3.348016]\n",
      "epoch:5 step:3920 [D loss: 0.550386, acc.: 69.53%] [G loss: 4.001936]\n",
      "epoch:5 step:3921 [D loss: 0.465097, acc.: 76.56%] [G loss: 2.881413]\n",
      "epoch:5 step:3922 [D loss: 0.310441, acc.: 87.50%] [G loss: 3.120168]\n",
      "epoch:5 step:3923 [D loss: 0.458787, acc.: 76.56%] [G loss: 3.083383]\n",
      "epoch:5 step:3924 [D loss: 0.438806, acc.: 81.25%] [G loss: 4.738169]\n",
      "epoch:5 step:3925 [D loss: 0.623755, acc.: 67.97%] [G loss: 2.263813]\n",
      "epoch:5 step:3926 [D loss: 0.386532, acc.: 80.47%] [G loss: 2.950537]\n",
      "epoch:5 step:3927 [D loss: 0.379234, acc.: 84.38%] [G loss: 2.773432]\n",
      "epoch:5 step:3928 [D loss: 0.447825, acc.: 75.78%] [G loss: 2.659328]\n",
      "epoch:5 step:3929 [D loss: 0.498195, acc.: 74.22%] [G loss: 2.334065]\n",
      "epoch:5 step:3930 [D loss: 0.496349, acc.: 69.53%] [G loss: 2.803486]\n",
      "epoch:5 step:3931 [D loss: 0.398040, acc.: 80.47%] [G loss: 1.977993]\n",
      "epoch:5 step:3932 [D loss: 0.318948, acc.: 85.94%] [G loss: 2.977246]\n",
      "epoch:5 step:3933 [D loss: 0.347364, acc.: 85.16%] [G loss: 2.560286]\n",
      "epoch:5 step:3934 [D loss: 0.305789, acc.: 89.06%] [G loss: 3.150473]\n",
      "epoch:5 step:3935 [D loss: 0.335453, acc.: 82.03%] [G loss: 2.008489]\n",
      "epoch:5 step:3936 [D loss: 0.376082, acc.: 82.81%] [G loss: 2.876750]\n",
      "epoch:5 step:3937 [D loss: 0.365734, acc.: 84.38%] [G loss: 2.282998]\n",
      "epoch:5 step:3938 [D loss: 0.396231, acc.: 83.59%] [G loss: 1.972471]\n",
      "epoch:5 step:3939 [D loss: 0.377179, acc.: 84.38%] [G loss: 2.291039]\n",
      "epoch:5 step:3940 [D loss: 0.381994, acc.: 82.81%] [G loss: 2.023946]\n",
      "epoch:5 step:3941 [D loss: 0.389193, acc.: 82.81%] [G loss: 3.030062]\n",
      "epoch:5 step:3942 [D loss: 0.439264, acc.: 81.25%] [G loss: 4.369635]\n",
      "epoch:5 step:3943 [D loss: 0.419505, acc.: 79.69%] [G loss: 2.991507]\n",
      "epoch:5 step:3944 [D loss: 0.369530, acc.: 85.16%] [G loss: 3.369992]\n",
      "epoch:5 step:3945 [D loss: 0.412856, acc.: 78.91%] [G loss: 3.203487]\n",
      "epoch:5 step:3946 [D loss: 0.406477, acc.: 78.91%] [G loss: 2.518138]\n",
      "epoch:5 step:3947 [D loss: 0.336368, acc.: 86.72%] [G loss: 3.268565]\n",
      "epoch:5 step:3948 [D loss: 0.345439, acc.: 85.94%] [G loss: 2.394108]\n",
      "epoch:5 step:3949 [D loss: 0.391031, acc.: 82.81%] [G loss: 2.745399]\n",
      "epoch:5 step:3950 [D loss: 0.366401, acc.: 85.94%] [G loss: 2.200731]\n",
      "epoch:5 step:3951 [D loss: 0.390431, acc.: 81.25%] [G loss: 2.478598]\n",
      "epoch:5 step:3952 [D loss: 0.366143, acc.: 85.94%] [G loss: 3.158191]\n",
      "epoch:5 step:3953 [D loss: 0.359009, acc.: 83.59%] [G loss: 3.912448]\n",
      "epoch:5 step:3954 [D loss: 0.498149, acc.: 72.66%] [G loss: 1.939736]\n",
      "epoch:5 step:3955 [D loss: 0.453641, acc.: 80.47%] [G loss: 1.787218]\n",
      "epoch:5 step:3956 [D loss: 0.412579, acc.: 78.12%] [G loss: 2.090795]\n",
      "epoch:5 step:3957 [D loss: 0.352522, acc.: 83.59%] [G loss: 2.584312]\n",
      "epoch:5 step:3958 [D loss: 0.370899, acc.: 81.25%] [G loss: 2.265505]\n",
      "epoch:5 step:3959 [D loss: 0.322189, acc.: 85.94%] [G loss: 2.793285]\n",
      "epoch:5 step:3960 [D loss: 0.325036, acc.: 87.50%] [G loss: 2.475515]\n",
      "epoch:5 step:3961 [D loss: 0.275446, acc.: 91.41%] [G loss: 2.947876]\n",
      "epoch:5 step:3962 [D loss: 0.347410, acc.: 82.81%] [G loss: 2.704183]\n",
      "epoch:5 step:3963 [D loss: 0.278165, acc.: 86.72%] [G loss: 4.186622]\n",
      "epoch:5 step:3964 [D loss: 0.264805, acc.: 88.28%] [G loss: 3.352530]\n",
      "epoch:5 step:3965 [D loss: 0.411207, acc.: 82.03%] [G loss: 2.311500]\n",
      "epoch:5 step:3966 [D loss: 0.360352, acc.: 85.16%] [G loss: 2.932115]\n",
      "epoch:5 step:3967 [D loss: 0.379001, acc.: 82.03%] [G loss: 2.389052]\n",
      "epoch:5 step:3968 [D loss: 0.341791, acc.: 84.38%] [G loss: 2.628012]\n",
      "epoch:5 step:3969 [D loss: 0.471822, acc.: 75.00%] [G loss: 4.273929]\n",
      "epoch:5 step:3970 [D loss: 0.809731, acc.: 66.41%] [G loss: 5.562591]\n",
      "epoch:5 step:3971 [D loss: 1.488672, acc.: 58.59%] [G loss: 4.212403]\n",
      "epoch:5 step:3972 [D loss: 0.735542, acc.: 71.88%] [G loss: 3.116192]\n",
      "epoch:5 step:3973 [D loss: 0.491709, acc.: 72.66%] [G loss: 2.105059]\n",
      "epoch:5 step:3974 [D loss: 0.463042, acc.: 78.12%] [G loss: 2.629541]\n",
      "epoch:5 step:3975 [D loss: 0.423608, acc.: 77.34%] [G loss: 2.895582]\n",
      "epoch:5 step:3976 [D loss: 0.365905, acc.: 82.03%] [G loss: 3.003745]\n",
      "epoch:5 step:3977 [D loss: 0.420019, acc.: 76.56%] [G loss: 2.584499]\n",
      "epoch:5 step:3978 [D loss: 0.367118, acc.: 85.94%] [G loss: 2.582947]\n",
      "epoch:5 step:3979 [D loss: 0.267549, acc.: 92.97%] [G loss: 2.813211]\n",
      "epoch:5 step:3980 [D loss: 0.351817, acc.: 85.16%] [G loss: 2.541023]\n",
      "epoch:5 step:3981 [D loss: 0.307875, acc.: 85.16%] [G loss: 2.224852]\n",
      "epoch:5 step:3982 [D loss: 0.359657, acc.: 83.59%] [G loss: 1.885008]\n",
      "epoch:5 step:3983 [D loss: 0.323157, acc.: 83.59%] [G loss: 2.765154]\n",
      "epoch:5 step:3984 [D loss: 0.355010, acc.: 85.16%] [G loss: 2.614525]\n",
      "epoch:5 step:3985 [D loss: 0.353690, acc.: 85.94%] [G loss: 2.662505]\n",
      "epoch:5 step:3986 [D loss: 0.466580, acc.: 76.56%] [G loss: 2.878679]\n",
      "epoch:5 step:3987 [D loss: 0.592579, acc.: 70.31%] [G loss: 5.003075]\n",
      "epoch:5 step:3988 [D loss: 0.661618, acc.: 64.84%] [G loss: 3.404560]\n",
      "epoch:5 step:3989 [D loss: 0.296230, acc.: 85.94%] [G loss: 3.510645]\n",
      "epoch:5 step:3990 [D loss: 0.395754, acc.: 79.69%] [G loss: 2.345171]\n",
      "epoch:5 step:3991 [D loss: 0.312163, acc.: 82.81%] [G loss: 2.673523]\n",
      "epoch:5 step:3992 [D loss: 0.358082, acc.: 81.25%] [G loss: 2.115495]\n",
      "epoch:5 step:3993 [D loss: 0.376397, acc.: 82.03%] [G loss: 2.382859]\n",
      "epoch:5 step:3994 [D loss: 0.397417, acc.: 79.69%] [G loss: 2.242047]\n",
      "epoch:5 step:3995 [D loss: 0.468185, acc.: 78.12%] [G loss: 1.833798]\n",
      "epoch:5 step:3996 [D loss: 0.484682, acc.: 71.88%] [G loss: 2.244381]\n",
      "epoch:5 step:3997 [D loss: 0.404911, acc.: 81.25%] [G loss: 2.508602]\n",
      "epoch:5 step:3998 [D loss: 0.449838, acc.: 72.66%] [G loss: 2.003572]\n",
      "epoch:5 step:3999 [D loss: 0.284329, acc.: 92.97%] [G loss: 3.050409]\n",
      "epoch:5 step:4000 [D loss: 0.368399, acc.: 85.16%] [G loss: 2.513249]\n",
      "##############\n",
      "[0.84342876 0.85555116 0.79150424 0.80564637 0.78047852 0.80292565\n",
      " 0.9030381  0.81385088 0.83253187 0.81347729]\n",
      "##########\n",
      "epoch:5 step:4001 [D loss: 0.376105, acc.: 82.81%] [G loss: 1.972937]\n",
      "epoch:5 step:4002 [D loss: 0.265807, acc.: 91.41%] [G loss: 4.848649]\n",
      "epoch:5 step:4003 [D loss: 0.357122, acc.: 85.94%] [G loss: 2.672920]\n",
      "epoch:5 step:4004 [D loss: 0.369736, acc.: 84.38%] [G loss: 2.205552]\n",
      "epoch:5 step:4005 [D loss: 0.324643, acc.: 85.94%] [G loss: 3.111797]\n",
      "epoch:5 step:4006 [D loss: 0.514269, acc.: 71.88%] [G loss: 2.228496]\n",
      "epoch:5 step:4007 [D loss: 0.532995, acc.: 71.09%] [G loss: 3.894654]\n",
      "epoch:5 step:4008 [D loss: 0.513437, acc.: 74.22%] [G loss: 2.808282]\n",
      "epoch:5 step:4009 [D loss: 0.533428, acc.: 71.09%] [G loss: 2.412380]\n",
      "epoch:5 step:4010 [D loss: 0.368662, acc.: 81.25%] [G loss: 2.381296]\n",
      "epoch:5 step:4011 [D loss: 0.393412, acc.: 83.59%] [G loss: 2.123000]\n",
      "epoch:5 step:4012 [D loss: 0.383852, acc.: 82.03%] [G loss: 2.388267]\n",
      "epoch:5 step:4013 [D loss: 0.396583, acc.: 82.03%] [G loss: 2.865720]\n",
      "epoch:5 step:4014 [D loss: 0.503086, acc.: 75.78%] [G loss: 2.757044]\n",
      "epoch:5 step:4015 [D loss: 0.404438, acc.: 81.25%] [G loss: 2.035357]\n",
      "epoch:5 step:4016 [D loss: 0.438712, acc.: 78.91%] [G loss: 3.271769]\n",
      "epoch:5 step:4017 [D loss: 0.562247, acc.: 73.44%] [G loss: 1.870752]\n",
      "epoch:5 step:4018 [D loss: 0.376132, acc.: 83.59%] [G loss: 2.837780]\n",
      "epoch:5 step:4019 [D loss: 0.449932, acc.: 75.78%] [G loss: 2.600331]\n",
      "epoch:5 step:4020 [D loss: 0.326478, acc.: 87.50%] [G loss: 3.111279]\n",
      "epoch:5 step:4021 [D loss: 0.327111, acc.: 82.03%] [G loss: 2.632499]\n",
      "epoch:5 step:4022 [D loss: 0.386130, acc.: 82.03%] [G loss: 1.922852]\n",
      "epoch:5 step:4023 [D loss: 0.474366, acc.: 76.56%] [G loss: 2.030400]\n",
      "epoch:5 step:4024 [D loss: 0.339456, acc.: 88.28%] [G loss: 2.163535]\n",
      "epoch:5 step:4025 [D loss: 0.362819, acc.: 87.50%] [G loss: 2.124269]\n",
      "epoch:5 step:4026 [D loss: 0.405183, acc.: 82.81%] [G loss: 3.130508]\n",
      "epoch:5 step:4027 [D loss: 0.410574, acc.: 80.47%] [G loss: 2.804326]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:5 step:4028 [D loss: 0.490830, acc.: 71.09%] [G loss: 2.913467]\n",
      "epoch:5 step:4029 [D loss: 0.401126, acc.: 82.81%] [G loss: 3.793835]\n",
      "epoch:5 step:4030 [D loss: 0.446891, acc.: 79.69%] [G loss: 3.916066]\n",
      "epoch:5 step:4031 [D loss: 0.468420, acc.: 75.78%] [G loss: 2.120360]\n",
      "epoch:5 step:4032 [D loss: 0.293266, acc.: 90.62%] [G loss: 2.604260]\n",
      "epoch:5 step:4033 [D loss: 0.408450, acc.: 78.12%] [G loss: 2.459282]\n",
      "epoch:5 step:4034 [D loss: 0.367327, acc.: 85.94%] [G loss: 2.953875]\n",
      "epoch:5 step:4035 [D loss: 0.347817, acc.: 86.72%] [G loss: 1.974083]\n",
      "epoch:5 step:4036 [D loss: 0.377365, acc.: 85.16%] [G loss: 2.084134]\n",
      "epoch:5 step:4037 [D loss: 0.354299, acc.: 84.38%] [G loss: 2.182921]\n",
      "epoch:5 step:4038 [D loss: 0.327636, acc.: 85.94%] [G loss: 2.265895]\n",
      "epoch:5 step:4039 [D loss: 0.432302, acc.: 78.12%] [G loss: 2.389035]\n",
      "epoch:5 step:4040 [D loss: 0.415327, acc.: 80.47%] [G loss: 1.985391]\n",
      "epoch:5 step:4041 [D loss: 0.342906, acc.: 85.16%] [G loss: 2.240496]\n",
      "epoch:5 step:4042 [D loss: 0.360207, acc.: 85.16%] [G loss: 1.962244]\n",
      "epoch:5 step:4043 [D loss: 0.377166, acc.: 81.25%] [G loss: 1.997247]\n",
      "epoch:5 step:4044 [D loss: 0.443027, acc.: 77.34%] [G loss: 1.995214]\n",
      "epoch:5 step:4045 [D loss: 0.430383, acc.: 75.78%] [G loss: 2.269749]\n",
      "epoch:5 step:4046 [D loss: 0.380892, acc.: 80.47%] [G loss: 2.473343]\n",
      "epoch:5 step:4047 [D loss: 0.305571, acc.: 87.50%] [G loss: 3.514364]\n",
      "epoch:5 step:4048 [D loss: 0.488143, acc.: 75.00%] [G loss: 1.794428]\n",
      "epoch:5 step:4049 [D loss: 0.258042, acc.: 89.84%] [G loss: 2.898916]\n",
      "epoch:5 step:4050 [D loss: 0.302783, acc.: 85.16%] [G loss: 4.602341]\n",
      "epoch:5 step:4051 [D loss: 0.336048, acc.: 84.38%] [G loss: 3.374018]\n",
      "epoch:5 step:4052 [D loss: 0.313882, acc.: 84.38%] [G loss: 2.688641]\n",
      "epoch:5 step:4053 [D loss: 0.384600, acc.: 80.47%] [G loss: 2.560297]\n",
      "epoch:5 step:4054 [D loss: 0.361465, acc.: 83.59%] [G loss: 2.815657]\n",
      "epoch:5 step:4055 [D loss: 0.412062, acc.: 78.12%] [G loss: 2.618584]\n",
      "epoch:5 step:4056 [D loss: 0.311607, acc.: 88.28%] [G loss: 2.735782]\n",
      "epoch:5 step:4057 [D loss: 0.364437, acc.: 82.03%] [G loss: 2.735465]\n",
      "epoch:5 step:4058 [D loss: 0.338323, acc.: 83.59%] [G loss: 2.530114]\n",
      "epoch:5 step:4059 [D loss: 0.551920, acc.: 71.09%] [G loss: 2.602488]\n",
      "epoch:5 step:4060 [D loss: 0.381247, acc.: 78.91%] [G loss: 3.396476]\n",
      "epoch:5 step:4061 [D loss: 0.353031, acc.: 81.25%] [G loss: 3.776825]\n",
      "epoch:5 step:4062 [D loss: 0.427088, acc.: 78.12%] [G loss: 3.452687]\n",
      "epoch:5 step:4063 [D loss: 0.434778, acc.: 78.91%] [G loss: 4.667385]\n",
      "epoch:5 step:4064 [D loss: 0.675679, acc.: 69.53%] [G loss: 4.145738]\n",
      "epoch:5 step:4065 [D loss: 1.532256, acc.: 57.03%] [G loss: 6.190582]\n",
      "epoch:5 step:4066 [D loss: 1.568917, acc.: 50.78%] [G loss: 2.631041]\n",
      "epoch:5 step:4067 [D loss: 0.565789, acc.: 74.22%] [G loss: 2.697880]\n",
      "epoch:5 step:4068 [D loss: 0.429337, acc.: 78.91%] [G loss: 3.590768]\n",
      "epoch:5 step:4069 [D loss: 0.685350, acc.: 67.19%] [G loss: 3.403865]\n",
      "epoch:5 step:4070 [D loss: 0.444211, acc.: 78.91%] [G loss: 3.328949]\n",
      "epoch:5 step:4071 [D loss: 0.427110, acc.: 78.91%] [G loss: 2.483179]\n",
      "epoch:5 step:4072 [D loss: 0.391897, acc.: 83.59%] [G loss: 2.212015]\n",
      "epoch:5 step:4073 [D loss: 0.315187, acc.: 87.50%] [G loss: 2.585869]\n",
      "epoch:5 step:4074 [D loss: 0.373546, acc.: 81.25%] [G loss: 2.594589]\n",
      "epoch:5 step:4075 [D loss: 0.426739, acc.: 76.56%] [G loss: 1.953041]\n",
      "epoch:5 step:4076 [D loss: 0.419088, acc.: 78.91%] [G loss: 2.389611]\n",
      "epoch:5 step:4077 [D loss: 0.345255, acc.: 89.06%] [G loss: 2.281890]\n",
      "epoch:5 step:4078 [D loss: 0.349045, acc.: 87.50%] [G loss: 2.695342]\n",
      "epoch:5 step:4079 [D loss: 0.345393, acc.: 82.03%] [G loss: 3.009497]\n",
      "epoch:5 step:4080 [D loss: 0.416925, acc.: 79.69%] [G loss: 2.415632]\n",
      "epoch:5 step:4081 [D loss: 0.449417, acc.: 74.22%] [G loss: 2.015646]\n",
      "epoch:5 step:4082 [D loss: 0.386351, acc.: 83.59%] [G loss: 2.562655]\n",
      "epoch:5 step:4083 [D loss: 0.303728, acc.: 89.06%] [G loss: 2.836010]\n",
      "epoch:5 step:4084 [D loss: 0.366422, acc.: 83.59%] [G loss: 2.876490]\n",
      "epoch:5 step:4085 [D loss: 0.351126, acc.: 89.84%] [G loss: 2.086615]\n",
      "epoch:5 step:4086 [D loss: 0.439596, acc.: 77.34%] [G loss: 1.714296]\n",
      "epoch:5 step:4087 [D loss: 0.374193, acc.: 87.50%] [G loss: 2.049837]\n",
      "epoch:5 step:4088 [D loss: 0.481011, acc.: 75.78%] [G loss: 1.682232]\n",
      "epoch:5 step:4089 [D loss: 0.416883, acc.: 75.78%] [G loss: 2.065324]\n",
      "epoch:5 step:4090 [D loss: 0.301239, acc.: 89.06%] [G loss: 3.129332]\n",
      "epoch:5 step:4091 [D loss: 0.360900, acc.: 85.16%] [G loss: 2.372772]\n",
      "epoch:5 step:4092 [D loss: 0.373711, acc.: 82.81%] [G loss: 2.262902]\n",
      "epoch:5 step:4093 [D loss: 0.418294, acc.: 79.69%] [G loss: 2.769481]\n",
      "epoch:5 step:4094 [D loss: 0.412002, acc.: 80.47%] [G loss: 2.168537]\n",
      "epoch:5 step:4095 [D loss: 0.313472, acc.: 89.84%] [G loss: 2.298643]\n",
      "epoch:5 step:4096 [D loss: 0.411968, acc.: 82.81%] [G loss: 2.268359]\n",
      "epoch:5 step:4097 [D loss: 0.489755, acc.: 75.00%] [G loss: 2.112015]\n",
      "epoch:5 step:4098 [D loss: 0.335361, acc.: 88.28%] [G loss: 3.648995]\n",
      "epoch:5 step:4099 [D loss: 0.358714, acc.: 82.81%] [G loss: 3.221572]\n",
      "epoch:5 step:4100 [D loss: 0.411204, acc.: 79.69%] [G loss: 2.364680]\n",
      "epoch:5 step:4101 [D loss: 0.397105, acc.: 78.91%] [G loss: 2.949340]\n",
      "epoch:5 step:4102 [D loss: 0.405487, acc.: 81.25%] [G loss: 2.286425]\n",
      "epoch:5 step:4103 [D loss: 0.340354, acc.: 83.59%] [G loss: 1.838653]\n",
      "epoch:5 step:4104 [D loss: 0.418870, acc.: 75.78%] [G loss: 2.278109]\n",
      "epoch:5 step:4105 [D loss: 0.322123, acc.: 85.94%] [G loss: 2.249950]\n",
      "epoch:5 step:4106 [D loss: 0.381799, acc.: 85.16%] [G loss: 2.364824]\n",
      "epoch:5 step:4107 [D loss: 0.427208, acc.: 83.59%] [G loss: 2.514307]\n",
      "epoch:5 step:4108 [D loss: 0.445204, acc.: 78.12%] [G loss: 1.924626]\n",
      "epoch:5 step:4109 [D loss: 0.489216, acc.: 73.44%] [G loss: 2.397623]\n",
      "epoch:5 step:4110 [D loss: 0.448423, acc.: 75.78%] [G loss: 2.432113]\n",
      "epoch:5 step:4111 [D loss: 0.432757, acc.: 78.12%] [G loss: 3.212284]\n",
      "epoch:5 step:4112 [D loss: 0.467756, acc.: 77.34%] [G loss: 2.152993]\n",
      "epoch:5 step:4113 [D loss: 0.402266, acc.: 82.03%] [G loss: 2.177963]\n",
      "epoch:5 step:4114 [D loss: 0.471211, acc.: 74.22%] [G loss: 3.045915]\n",
      "epoch:5 step:4115 [D loss: 0.406557, acc.: 79.69%] [G loss: 2.111794]\n",
      "epoch:5 step:4116 [D loss: 0.417031, acc.: 81.25%] [G loss: 2.649655]\n",
      "epoch:5 step:4117 [D loss: 0.425635, acc.: 73.44%] [G loss: 4.190104]\n",
      "epoch:5 step:4118 [D loss: 0.501330, acc.: 73.44%] [G loss: 2.230641]\n",
      "epoch:5 step:4119 [D loss: 0.462605, acc.: 77.34%] [G loss: 2.502410]\n",
      "epoch:5 step:4120 [D loss: 0.410578, acc.: 82.03%] [G loss: 2.575464]\n",
      "epoch:5 step:4121 [D loss: 0.632514, acc.: 67.19%] [G loss: 2.271867]\n",
      "epoch:5 step:4122 [D loss: 0.421922, acc.: 79.69%] [G loss: 2.609282]\n",
      "epoch:5 step:4123 [D loss: 0.519681, acc.: 70.31%] [G loss: 2.375028]\n",
      "epoch:5 step:4124 [D loss: 0.452854, acc.: 72.66%] [G loss: 2.431939]\n",
      "epoch:5 step:4125 [D loss: 0.394390, acc.: 85.94%] [G loss: 2.184886]\n",
      "epoch:5 step:4126 [D loss: 0.319644, acc.: 89.84%] [G loss: 2.436781]\n",
      "epoch:5 step:4127 [D loss: 0.384417, acc.: 83.59%] [G loss: 2.145188]\n",
      "epoch:5 step:4128 [D loss: 0.322699, acc.: 89.06%] [G loss: 2.911660]\n",
      "epoch:5 step:4129 [D loss: 0.339372, acc.: 87.50%] [G loss: 2.075114]\n",
      "epoch:5 step:4130 [D loss: 0.363901, acc.: 81.25%] [G loss: 2.246418]\n",
      "epoch:5 step:4131 [D loss: 0.344135, acc.: 85.94%] [G loss: 2.092456]\n",
      "epoch:5 step:4132 [D loss: 0.398713, acc.: 81.25%] [G loss: 2.820331]\n",
      "epoch:5 step:4133 [D loss: 0.382065, acc.: 83.59%] [G loss: 2.655031]\n",
      "epoch:5 step:4134 [D loss: 0.391830, acc.: 78.12%] [G loss: 3.103019]\n",
      "epoch:5 step:4135 [D loss: 0.372006, acc.: 83.59%] [G loss: 2.696727]\n",
      "epoch:5 step:4136 [D loss: 0.348169, acc.: 87.50%] [G loss: 2.658513]\n",
      "epoch:5 step:4137 [D loss: 0.317422, acc.: 89.06%] [G loss: 2.940161]\n",
      "epoch:5 step:4138 [D loss: 0.382264, acc.: 84.38%] [G loss: 2.194068]\n",
      "epoch:5 step:4139 [D loss: 0.399119, acc.: 80.47%] [G loss: 2.587548]\n",
      "epoch:5 step:4140 [D loss: 0.302493, acc.: 88.28%] [G loss: 4.351783]\n",
      "epoch:5 step:4141 [D loss: 0.420641, acc.: 80.47%] [G loss: 5.586357]\n",
      "epoch:5 step:4142 [D loss: 1.005506, acc.: 64.84%] [G loss: 2.692659]\n",
      "epoch:5 step:4143 [D loss: 0.649862, acc.: 70.31%] [G loss: 3.131968]\n",
      "epoch:5 step:4144 [D loss: 0.511807, acc.: 71.88%] [G loss: 3.340816]\n",
      "epoch:5 step:4145 [D loss: 0.468540, acc.: 76.56%] [G loss: 2.705305]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:5 step:4146 [D loss: 0.332100, acc.: 88.28%] [G loss: 2.292812]\n",
      "epoch:5 step:4147 [D loss: 0.407016, acc.: 79.69%] [G loss: 3.068307]\n",
      "epoch:5 step:4148 [D loss: 0.443401, acc.: 79.69%] [G loss: 2.618932]\n",
      "epoch:5 step:4149 [D loss: 0.359729, acc.: 78.91%] [G loss: 2.682344]\n",
      "epoch:5 step:4150 [D loss: 0.299995, acc.: 85.94%] [G loss: 2.961698]\n",
      "epoch:5 step:4151 [D loss: 0.411367, acc.: 79.69%] [G loss: 2.207948]\n",
      "epoch:5 step:4152 [D loss: 0.326410, acc.: 85.94%] [G loss: 3.048864]\n",
      "epoch:5 step:4153 [D loss: 0.351104, acc.: 85.94%] [G loss: 2.108411]\n",
      "epoch:5 step:4154 [D loss: 0.430893, acc.: 78.91%] [G loss: 2.311639]\n",
      "epoch:5 step:4155 [D loss: 0.409231, acc.: 82.81%] [G loss: 2.223782]\n",
      "epoch:5 step:4156 [D loss: 0.324900, acc.: 85.94%] [G loss: 2.089915]\n",
      "epoch:5 step:4157 [D loss: 0.312041, acc.: 85.94%] [G loss: 2.729374]\n",
      "epoch:5 step:4158 [D loss: 0.334033, acc.: 84.38%] [G loss: 2.877635]\n",
      "epoch:5 step:4159 [D loss: 0.361390, acc.: 85.16%] [G loss: 2.279423]\n",
      "epoch:5 step:4160 [D loss: 0.336435, acc.: 82.03%] [G loss: 3.180845]\n",
      "epoch:5 step:4161 [D loss: 0.335027, acc.: 83.59%] [G loss: 3.299541]\n",
      "epoch:5 step:4162 [D loss: 0.412087, acc.: 78.91%] [G loss: 2.645234]\n",
      "epoch:5 step:4163 [D loss: 0.346200, acc.: 87.50%] [G loss: 2.275791]\n",
      "epoch:5 step:4164 [D loss: 0.378860, acc.: 83.59%] [G loss: 3.277812]\n",
      "epoch:5 step:4165 [D loss: 0.461623, acc.: 76.56%] [G loss: 2.563941]\n",
      "epoch:5 step:4166 [D loss: 0.369735, acc.: 80.47%] [G loss: 2.451622]\n",
      "epoch:5 step:4167 [D loss: 0.362791, acc.: 81.25%] [G loss: 2.212325]\n",
      "epoch:5 step:4168 [D loss: 0.284018, acc.: 85.94%] [G loss: 2.897166]\n",
      "epoch:5 step:4169 [D loss: 0.351391, acc.: 81.25%] [G loss: 2.394129]\n",
      "epoch:5 step:4170 [D loss: 0.363261, acc.: 82.03%] [G loss: 2.338555]\n",
      "epoch:5 step:4171 [D loss: 0.335209, acc.: 89.06%] [G loss: 2.308792]\n",
      "epoch:5 step:4172 [D loss: 0.256013, acc.: 91.41%] [G loss: 3.468380]\n",
      "epoch:5 step:4173 [D loss: 0.410371, acc.: 78.91%] [G loss: 2.673086]\n",
      "epoch:5 step:4174 [D loss: 0.441565, acc.: 77.34%] [G loss: 2.217799]\n",
      "epoch:5 step:4175 [D loss: 0.449683, acc.: 78.12%] [G loss: 2.637782]\n",
      "epoch:5 step:4176 [D loss: 0.369021, acc.: 85.16%] [G loss: 2.147763]\n",
      "epoch:5 step:4177 [D loss: 0.312065, acc.: 89.06%] [G loss: 3.216547]\n",
      "epoch:5 step:4178 [D loss: 0.433158, acc.: 78.91%] [G loss: 2.551245]\n",
      "epoch:5 step:4179 [D loss: 0.502789, acc.: 76.56%] [G loss: 2.109235]\n",
      "epoch:5 step:4180 [D loss: 0.488934, acc.: 71.09%] [G loss: 3.412827]\n",
      "epoch:5 step:4181 [D loss: 0.661133, acc.: 66.41%] [G loss: 3.546599]\n",
      "epoch:5 step:4182 [D loss: 0.982325, acc.: 67.97%] [G loss: 5.939398]\n",
      "epoch:5 step:4183 [D loss: 1.728082, acc.: 55.47%] [G loss: 2.869888]\n",
      "epoch:5 step:4184 [D loss: 0.437771, acc.: 77.34%] [G loss: 2.442533]\n",
      "epoch:5 step:4185 [D loss: 0.628516, acc.: 71.88%] [G loss: 1.830000]\n",
      "epoch:5 step:4186 [D loss: 0.474661, acc.: 73.44%] [G loss: 2.673147]\n",
      "epoch:5 step:4187 [D loss: 0.388040, acc.: 79.69%] [G loss: 2.665703]\n",
      "epoch:5 step:4188 [D loss: 0.426744, acc.: 82.81%] [G loss: 2.669555]\n",
      "epoch:5 step:4189 [D loss: 0.468287, acc.: 74.22%] [G loss: 2.205735]\n",
      "epoch:5 step:4190 [D loss: 0.337751, acc.: 84.38%] [G loss: 2.471314]\n",
      "epoch:5 step:4191 [D loss: 0.426872, acc.: 75.78%] [G loss: 2.519588]\n",
      "epoch:5 step:4192 [D loss: 0.246700, acc.: 92.19%] [G loss: 3.577816]\n",
      "epoch:5 step:4193 [D loss: 0.465527, acc.: 73.44%] [G loss: 1.997232]\n",
      "epoch:5 step:4194 [D loss: 0.392402, acc.: 82.03%] [G loss: 3.161690]\n",
      "epoch:5 step:4195 [D loss: 0.414794, acc.: 78.91%] [G loss: 2.773645]\n",
      "epoch:5 step:4196 [D loss: 0.299106, acc.: 90.62%] [G loss: 2.515706]\n",
      "epoch:5 step:4197 [D loss: 0.424708, acc.: 79.69%] [G loss: 2.148590]\n",
      "epoch:5 step:4198 [D loss: 0.366019, acc.: 83.59%] [G loss: 2.284135]\n",
      "epoch:5 step:4199 [D loss: 0.369927, acc.: 82.03%] [G loss: 2.158182]\n",
      "epoch:5 step:4200 [D loss: 0.357503, acc.: 85.16%] [G loss: 3.153485]\n",
      "##############\n",
      "[0.82444507 0.8551733  0.78401108 0.8123731  0.79340998 0.82256071\n",
      " 0.86052347 0.82045357 0.81308181 0.83295022]\n",
      "##########\n",
      "epoch:5 step:4201 [D loss: 0.337244, acc.: 88.28%] [G loss: 2.150777]\n",
      "epoch:5 step:4202 [D loss: 0.436373, acc.: 75.00%] [G loss: 2.371377]\n",
      "epoch:5 step:4203 [D loss: 0.360451, acc.: 82.81%] [G loss: 2.298046]\n",
      "epoch:5 step:4204 [D loss: 0.381381, acc.: 82.81%] [G loss: 2.829755]\n",
      "epoch:5 step:4205 [D loss: 0.517054, acc.: 71.09%] [G loss: 2.032155]\n",
      "epoch:5 step:4206 [D loss: 0.491675, acc.: 75.00%] [G loss: 1.947835]\n",
      "epoch:5 step:4207 [D loss: 0.310524, acc.: 88.28%] [G loss: 3.108405]\n",
      "epoch:5 step:4208 [D loss: 0.280990, acc.: 90.62%] [G loss: 3.075520]\n",
      "epoch:5 step:4209 [D loss: 0.427452, acc.: 77.34%] [G loss: 2.476553]\n",
      "epoch:5 step:4210 [D loss: 0.322006, acc.: 85.94%] [G loss: 3.304905]\n",
      "epoch:5 step:4211 [D loss: 0.438485, acc.: 77.34%] [G loss: 2.153853]\n",
      "epoch:5 step:4212 [D loss: 0.426192, acc.: 77.34%] [G loss: 2.004520]\n",
      "epoch:5 step:4213 [D loss: 0.368312, acc.: 84.38%] [G loss: 2.687035]\n",
      "epoch:5 step:4214 [D loss: 0.386515, acc.: 80.47%] [G loss: 3.442814]\n",
      "epoch:5 step:4215 [D loss: 0.460238, acc.: 75.78%] [G loss: 2.311941]\n",
      "epoch:5 step:4216 [D loss: 0.391542, acc.: 81.25%] [G loss: 3.171688]\n",
      "epoch:5 step:4217 [D loss: 0.551198, acc.: 66.41%] [G loss: 3.799047]\n",
      "epoch:5 step:4218 [D loss: 0.692348, acc.: 59.38%] [G loss: 4.161521]\n",
      "epoch:5 step:4219 [D loss: 0.815825, acc.: 64.06%] [G loss: 4.284954]\n",
      "epoch:5 step:4220 [D loss: 0.616995, acc.: 66.41%] [G loss: 2.761184]\n",
      "epoch:5 step:4221 [D loss: 0.733304, acc.: 66.41%] [G loss: 1.886335]\n",
      "epoch:5 step:4222 [D loss: 0.507093, acc.: 81.25%] [G loss: 2.628191]\n",
      "epoch:5 step:4223 [D loss: 0.582600, acc.: 70.31%] [G loss: 3.011401]\n",
      "epoch:5 step:4224 [D loss: 0.423551, acc.: 78.12%] [G loss: 2.294962]\n",
      "epoch:5 step:4225 [D loss: 0.480794, acc.: 71.88%] [G loss: 2.429148]\n",
      "epoch:5 step:4226 [D loss: 0.469543, acc.: 72.66%] [G loss: 2.064816]\n",
      "epoch:5 step:4227 [D loss: 0.368839, acc.: 85.16%] [G loss: 3.252781]\n",
      "epoch:5 step:4228 [D loss: 0.364278, acc.: 85.94%] [G loss: 2.952982]\n",
      "epoch:5 step:4229 [D loss: 0.388934, acc.: 79.69%] [G loss: 2.420130]\n",
      "epoch:5 step:4230 [D loss: 0.379746, acc.: 83.59%] [G loss: 2.202879]\n",
      "epoch:5 step:4231 [D loss: 0.373763, acc.: 82.03%] [G loss: 2.117095]\n",
      "epoch:5 step:4232 [D loss: 0.370272, acc.: 85.94%] [G loss: 2.398669]\n",
      "epoch:5 step:4233 [D loss: 0.371817, acc.: 89.06%] [G loss: 2.574143]\n",
      "epoch:5 step:4234 [D loss: 0.428462, acc.: 82.81%] [G loss: 2.757125]\n",
      "epoch:5 step:4235 [D loss: 0.403543, acc.: 78.91%] [G loss: 2.885426]\n",
      "epoch:5 step:4236 [D loss: 0.298456, acc.: 89.06%] [G loss: 2.316448]\n",
      "epoch:5 step:4237 [D loss: 0.407723, acc.: 79.69%] [G loss: 1.960962]\n",
      "epoch:5 step:4238 [D loss: 0.383065, acc.: 85.94%] [G loss: 2.003655]\n",
      "epoch:5 step:4239 [D loss: 0.390655, acc.: 83.59%] [G loss: 2.151372]\n",
      "epoch:5 step:4240 [D loss: 0.346093, acc.: 82.81%] [G loss: 2.733144]\n",
      "epoch:5 step:4241 [D loss: 0.319104, acc.: 89.84%] [G loss: 3.199947]\n",
      "epoch:5 step:4242 [D loss: 0.345037, acc.: 82.81%] [G loss: 3.670557]\n",
      "epoch:5 step:4243 [D loss: 0.355977, acc.: 82.81%] [G loss: 2.922048]\n",
      "epoch:5 step:4244 [D loss: 0.357601, acc.: 84.38%] [G loss: 2.792214]\n",
      "epoch:5 step:4245 [D loss: 0.288312, acc.: 87.50%] [G loss: 6.343478]\n",
      "epoch:5 step:4246 [D loss: 0.368920, acc.: 78.91%] [G loss: 1.794102]\n",
      "epoch:5 step:4247 [D loss: 0.358509, acc.: 83.59%] [G loss: 2.517002]\n",
      "epoch:5 step:4248 [D loss: 0.274980, acc.: 89.84%] [G loss: 2.735000]\n",
      "epoch:5 step:4249 [D loss: 0.365747, acc.: 83.59%] [G loss: 2.367966]\n",
      "epoch:5 step:4250 [D loss: 0.376241, acc.: 79.69%] [G loss: 2.168117]\n",
      "epoch:5 step:4251 [D loss: 0.317813, acc.: 86.72%] [G loss: 3.104871]\n",
      "epoch:5 step:4252 [D loss: 0.341948, acc.: 83.59%] [G loss: 2.792248]\n",
      "epoch:5 step:4253 [D loss: 0.390400, acc.: 78.91%] [G loss: 4.673786]\n",
      "epoch:5 step:4254 [D loss: 0.496168, acc.: 71.09%] [G loss: 2.927556]\n",
      "epoch:5 step:4255 [D loss: 0.388958, acc.: 79.69%] [G loss: 3.095326]\n",
      "epoch:5 step:4256 [D loss: 0.342636, acc.: 78.91%] [G loss: 2.745078]\n",
      "epoch:5 step:4257 [D loss: 0.377233, acc.: 81.25%] [G loss: 2.110656]\n",
      "epoch:5 step:4258 [D loss: 0.388672, acc.: 80.47%] [G loss: 2.150292]\n",
      "epoch:5 step:4259 [D loss: 0.469457, acc.: 75.78%] [G loss: 2.314854]\n",
      "epoch:5 step:4260 [D loss: 0.341329, acc.: 82.81%] [G loss: 2.832434]\n",
      "epoch:5 step:4261 [D loss: 0.369820, acc.: 85.16%] [G loss: 2.146980]\n",
      "epoch:5 step:4262 [D loss: 0.419851, acc.: 76.56%] [G loss: 2.609788]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:5 step:4263 [D loss: 0.378790, acc.: 82.81%] [G loss: 2.593714]\n",
      "epoch:5 step:4264 [D loss: 0.229253, acc.: 92.97%] [G loss: 3.860598]\n",
      "epoch:5 step:4265 [D loss: 0.268485, acc.: 88.28%] [G loss: 4.340734]\n",
      "epoch:5 step:4266 [D loss: 0.350374, acc.: 87.50%] [G loss: 2.429775]\n",
      "epoch:5 step:4267 [D loss: 0.378024, acc.: 81.25%] [G loss: 3.254217]\n",
      "epoch:5 step:4268 [D loss: 0.378348, acc.: 78.12%] [G loss: 3.447595]\n",
      "epoch:5 step:4269 [D loss: 0.411425, acc.: 85.16%] [G loss: 2.749108]\n",
      "epoch:5 step:4270 [D loss: 0.435278, acc.: 76.56%] [G loss: 2.794639]\n",
      "epoch:5 step:4271 [D loss: 0.410114, acc.: 78.91%] [G loss: 3.164658]\n",
      "epoch:5 step:4272 [D loss: 0.479138, acc.: 78.91%] [G loss: 2.079761]\n",
      "epoch:5 step:4273 [D loss: 0.375579, acc.: 85.94%] [G loss: 2.445948]\n",
      "epoch:5 step:4274 [D loss: 0.385998, acc.: 82.03%] [G loss: 2.565677]\n",
      "epoch:5 step:4275 [D loss: 0.400357, acc.: 81.25%] [G loss: 2.350859]\n",
      "epoch:5 step:4276 [D loss: 0.396557, acc.: 79.69%] [G loss: 2.442700]\n",
      "epoch:5 step:4277 [D loss: 0.424877, acc.: 81.25%] [G loss: 3.951007]\n",
      "epoch:5 step:4278 [D loss: 0.514943, acc.: 71.88%] [G loss: 4.185563]\n",
      "epoch:5 step:4279 [D loss: 1.124354, acc.: 54.69%] [G loss: 3.250484]\n",
      "epoch:5 step:4280 [D loss: 1.568561, acc.: 49.22%] [G loss: 6.333949]\n",
      "epoch:5 step:4281 [D loss: 0.851728, acc.: 62.50%] [G loss: 3.603534]\n",
      "epoch:5 step:4282 [D loss: 0.532250, acc.: 79.69%] [G loss: 3.891387]\n",
      "epoch:5 step:4283 [D loss: 0.338000, acc.: 82.81%] [G loss: 3.327239]\n",
      "epoch:5 step:4284 [D loss: 0.504315, acc.: 78.91%] [G loss: 2.057637]\n",
      "epoch:5 step:4285 [D loss: 0.356450, acc.: 82.03%] [G loss: 2.085938]\n",
      "epoch:5 step:4286 [D loss: 0.462584, acc.: 76.56%] [G loss: 2.508214]\n",
      "epoch:5 step:4287 [D loss: 0.421751, acc.: 78.91%] [G loss: 2.247081]\n",
      "epoch:5 step:4288 [D loss: 0.358134, acc.: 85.16%] [G loss: 2.462718]\n",
      "epoch:5 step:4289 [D loss: 0.380860, acc.: 83.59%] [G loss: 2.712880]\n",
      "epoch:5 step:4290 [D loss: 0.330623, acc.: 85.94%] [G loss: 3.096512]\n",
      "epoch:5 step:4291 [D loss: 0.446012, acc.: 78.12%] [G loss: 2.174043]\n",
      "epoch:5 step:4292 [D loss: 0.348408, acc.: 87.50%] [G loss: 2.527853]\n",
      "epoch:5 step:4293 [D loss: 0.408420, acc.: 78.91%] [G loss: 2.739332]\n",
      "epoch:5 step:4294 [D loss: 0.367180, acc.: 82.81%] [G loss: 2.257879]\n",
      "epoch:5 step:4295 [D loss: 0.247783, acc.: 90.62%] [G loss: 3.429345]\n",
      "epoch:5 step:4296 [D loss: 0.295719, acc.: 86.72%] [G loss: 3.111082]\n",
      "epoch:5 step:4297 [D loss: 0.411178, acc.: 79.69%] [G loss: 2.508619]\n",
      "epoch:5 step:4298 [D loss: 0.302406, acc.: 89.84%] [G loss: 2.697622]\n",
      "epoch:5 step:4299 [D loss: 0.454293, acc.: 76.56%] [G loss: 2.783237]\n",
      "epoch:5 step:4300 [D loss: 0.397388, acc.: 84.38%] [G loss: 2.459651]\n",
      "epoch:5 step:4301 [D loss: 0.439324, acc.: 78.12%] [G loss: 2.884449]\n",
      "epoch:5 step:4302 [D loss: 0.321885, acc.: 89.84%] [G loss: 2.683643]\n",
      "epoch:5 step:4303 [D loss: 0.318321, acc.: 84.38%] [G loss: 3.127467]\n",
      "epoch:5 step:4304 [D loss: 0.374576, acc.: 82.81%] [G loss: 2.049761]\n",
      "epoch:5 step:4305 [D loss: 0.336532, acc.: 85.94%] [G loss: 2.507919]\n",
      "epoch:5 step:4306 [D loss: 0.389929, acc.: 79.69%] [G loss: 3.080701]\n",
      "epoch:5 step:4307 [D loss: 0.380271, acc.: 79.69%] [G loss: 3.311662]\n",
      "epoch:5 step:4308 [D loss: 0.491801, acc.: 70.31%] [G loss: 2.438697]\n",
      "epoch:5 step:4309 [D loss: 0.515572, acc.: 75.00%] [G loss: 2.337793]\n",
      "epoch:5 step:4310 [D loss: 0.426878, acc.: 78.12%] [G loss: 2.918748]\n",
      "epoch:5 step:4311 [D loss: 0.487316, acc.: 75.78%] [G loss: 2.838044]\n",
      "epoch:5 step:4312 [D loss: 0.489462, acc.: 73.44%] [G loss: 3.501430]\n",
      "epoch:5 step:4313 [D loss: 0.460936, acc.: 75.00%] [G loss: 2.250742]\n",
      "epoch:5 step:4314 [D loss: 0.311520, acc.: 84.38%] [G loss: 3.324189]\n",
      "epoch:5 step:4315 [D loss: 0.294432, acc.: 87.50%] [G loss: 2.840406]\n",
      "epoch:5 step:4316 [D loss: 0.461245, acc.: 76.56%] [G loss: 2.554257]\n",
      "epoch:5 step:4317 [D loss: 0.307828, acc.: 88.28%] [G loss: 2.891453]\n",
      "epoch:5 step:4318 [D loss: 0.250174, acc.: 89.84%] [G loss: 3.169343]\n",
      "epoch:5 step:4319 [D loss: 0.367283, acc.: 87.50%] [G loss: 3.809808]\n",
      "epoch:5 step:4320 [D loss: 0.341156, acc.: 82.03%] [G loss: 3.554392]\n",
      "epoch:5 step:4321 [D loss: 0.398874, acc.: 82.03%] [G loss: 2.801548]\n",
      "epoch:5 step:4322 [D loss: 0.414035, acc.: 82.03%] [G loss: 2.316841]\n",
      "epoch:5 step:4323 [D loss: 0.458682, acc.: 74.22%] [G loss: 2.380655]\n",
      "epoch:5 step:4324 [D loss: 0.387216, acc.: 81.25%] [G loss: 2.802718]\n",
      "epoch:5 step:4325 [D loss: 0.394083, acc.: 79.69%] [G loss: 2.371679]\n",
      "epoch:5 step:4326 [D loss: 0.348591, acc.: 89.06%] [G loss: 2.587567]\n",
      "epoch:5 step:4327 [D loss: 0.424934, acc.: 75.78%] [G loss: 2.409712]\n",
      "epoch:5 step:4328 [D loss: 0.386662, acc.: 84.38%] [G loss: 2.698599]\n",
      "epoch:5 step:4329 [D loss: 0.358946, acc.: 82.81%] [G loss: 4.338786]\n",
      "epoch:5 step:4330 [D loss: 0.307694, acc.: 90.62%] [G loss: 3.599950]\n",
      "epoch:5 step:4331 [D loss: 0.386526, acc.: 80.47%] [G loss: 2.452445]\n",
      "epoch:5 step:4332 [D loss: 0.373024, acc.: 82.81%] [G loss: 2.605319]\n",
      "epoch:5 step:4333 [D loss: 0.383529, acc.: 85.16%] [G loss: 2.158916]\n",
      "epoch:5 step:4334 [D loss: 0.427636, acc.: 78.12%] [G loss: 2.219074]\n",
      "epoch:5 step:4335 [D loss: 0.465483, acc.: 74.22%] [G loss: 3.053023]\n",
      "epoch:5 step:4336 [D loss: 0.548445, acc.: 73.44%] [G loss: 4.183400]\n",
      "epoch:5 step:4337 [D loss: 0.773285, acc.: 71.09%] [G loss: 3.696357]\n",
      "epoch:5 step:4338 [D loss: 0.660399, acc.: 62.50%] [G loss: 4.630096]\n",
      "epoch:5 step:4339 [D loss: 0.643100, acc.: 72.66%] [G loss: 2.364286]\n",
      "epoch:5 step:4340 [D loss: 0.593351, acc.: 71.09%] [G loss: 2.697526]\n",
      "epoch:5 step:4341 [D loss: 0.734197, acc.: 66.41%] [G loss: 4.430631]\n",
      "epoch:5 step:4342 [D loss: 0.969578, acc.: 63.28%] [G loss: 2.818860]\n",
      "epoch:5 step:4343 [D loss: 0.635168, acc.: 67.97%] [G loss: 2.252279]\n",
      "epoch:5 step:4344 [D loss: 0.430089, acc.: 78.91%] [G loss: 2.807929]\n",
      "epoch:5 step:4345 [D loss: 0.638983, acc.: 65.62%] [G loss: 2.068363]\n",
      "epoch:5 step:4346 [D loss: 0.456111, acc.: 76.56%] [G loss: 2.024736]\n",
      "epoch:5 step:4347 [D loss: 0.404593, acc.: 79.69%] [G loss: 2.170403]\n",
      "epoch:5 step:4348 [D loss: 0.422101, acc.: 80.47%] [G loss: 2.300578]\n",
      "epoch:5 step:4349 [D loss: 0.363468, acc.: 81.25%] [G loss: 3.017948]\n",
      "epoch:5 step:4350 [D loss: 0.391661, acc.: 82.03%] [G loss: 3.210811]\n",
      "epoch:5 step:4351 [D loss: 0.369928, acc.: 78.91%] [G loss: 2.622980]\n",
      "epoch:5 step:4352 [D loss: 0.407920, acc.: 82.81%] [G loss: 2.323493]\n",
      "epoch:5 step:4353 [D loss: 0.407893, acc.: 82.03%] [G loss: 1.873890]\n",
      "epoch:5 step:4354 [D loss: 0.367125, acc.: 82.03%] [G loss: 1.944352]\n",
      "epoch:5 step:4355 [D loss: 0.413638, acc.: 81.25%] [G loss: 1.994596]\n",
      "epoch:5 step:4356 [D loss: 0.389209, acc.: 86.72%] [G loss: 2.516730]\n",
      "epoch:5 step:4357 [D loss: 0.422768, acc.: 80.47%] [G loss: 2.733864]\n",
      "epoch:5 step:4358 [D loss: 0.334891, acc.: 85.94%] [G loss: 2.427411]\n",
      "epoch:5 step:4359 [D loss: 0.304677, acc.: 89.84%] [G loss: 2.686156]\n",
      "epoch:5 step:4360 [D loss: 0.363637, acc.: 82.81%] [G loss: 3.016417]\n",
      "epoch:5 step:4361 [D loss: 0.418087, acc.: 80.47%] [G loss: 2.887246]\n",
      "epoch:5 step:4362 [D loss: 0.333384, acc.: 82.81%] [G loss: 3.676501]\n",
      "epoch:5 step:4363 [D loss: 0.368996, acc.: 82.81%] [G loss: 2.630692]\n",
      "epoch:5 step:4364 [D loss: 0.383011, acc.: 82.03%] [G loss: 2.418765]\n",
      "epoch:5 step:4365 [D loss: 0.334286, acc.: 85.16%] [G loss: 2.606654]\n",
      "epoch:5 step:4366 [D loss: 0.383459, acc.: 80.47%] [G loss: 2.444902]\n",
      "epoch:5 step:4367 [D loss: 0.419433, acc.: 75.00%] [G loss: 2.132631]\n",
      "epoch:5 step:4368 [D loss: 0.330780, acc.: 87.50%] [G loss: 2.592535]\n",
      "epoch:5 step:4369 [D loss: 0.402443, acc.: 82.81%] [G loss: 2.535495]\n",
      "epoch:5 step:4370 [D loss: 0.385924, acc.: 84.38%] [G loss: 3.494881]\n",
      "epoch:5 step:4371 [D loss: 0.453092, acc.: 77.34%] [G loss: 2.688470]\n",
      "epoch:5 step:4372 [D loss: 0.505265, acc.: 71.09%] [G loss: 2.728247]\n",
      "epoch:5 step:4373 [D loss: 0.383623, acc.: 81.25%] [G loss: 2.293135]\n",
      "epoch:5 step:4374 [D loss: 0.333251, acc.: 85.16%] [G loss: 2.190067]\n",
      "epoch:5 step:4375 [D loss: 0.392806, acc.: 80.47%] [G loss: 1.700255]\n",
      "epoch:5 step:4376 [D loss: 0.350570, acc.: 89.06%] [G loss: 1.841229]\n",
      "epoch:5 step:4377 [D loss: 0.351960, acc.: 86.72%] [G loss: 3.133671]\n",
      "epoch:5 step:4378 [D loss: 0.243177, acc.: 92.97%] [G loss: 4.447738]\n",
      "epoch:5 step:4379 [D loss: 0.403461, acc.: 77.34%] [G loss: 2.048593]\n",
      "epoch:5 step:4380 [D loss: 0.388959, acc.: 83.59%] [G loss: 2.190927]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:5 step:4381 [D loss: 0.423306, acc.: 78.91%] [G loss: 2.082719]\n",
      "epoch:5 step:4382 [D loss: 0.324223, acc.: 86.72%] [G loss: 2.412450]\n",
      "epoch:5 step:4383 [D loss: 0.348937, acc.: 84.38%] [G loss: 2.866943]\n",
      "epoch:5 step:4384 [D loss: 0.379904, acc.: 84.38%] [G loss: 3.341713]\n",
      "epoch:5 step:4385 [D loss: 0.298510, acc.: 89.06%] [G loss: 2.637573]\n",
      "epoch:5 step:4386 [D loss: 0.360818, acc.: 84.38%] [G loss: 2.115240]\n",
      "epoch:5 step:4387 [D loss: 0.406996, acc.: 80.47%] [G loss: 2.297873]\n",
      "epoch:5 step:4388 [D loss: 0.373838, acc.: 82.81%] [G loss: 3.113374]\n",
      "epoch:5 step:4389 [D loss: 0.479386, acc.: 72.66%] [G loss: 3.034369]\n",
      "epoch:5 step:4390 [D loss: 0.384875, acc.: 81.25%] [G loss: 3.375318]\n",
      "epoch:5 step:4391 [D loss: 0.405928, acc.: 84.38%] [G loss: 1.634873]\n",
      "epoch:5 step:4392 [D loss: 0.414987, acc.: 82.03%] [G loss: 2.678037]\n",
      "epoch:5 step:4393 [D loss: 0.493706, acc.: 74.22%] [G loss: 2.780234]\n",
      "epoch:5 step:4394 [D loss: 0.503159, acc.: 71.88%] [G loss: 2.494702]\n",
      "epoch:5 step:4395 [D loss: 0.545312, acc.: 72.66%] [G loss: 4.249516]\n",
      "epoch:5 step:4396 [D loss: 0.749279, acc.: 61.72%] [G loss: 5.345885]\n",
      "epoch:5 step:4397 [D loss: 0.795699, acc.: 67.19%] [G loss: 3.120992]\n",
      "epoch:5 step:4398 [D loss: 0.348320, acc.: 78.91%] [G loss: 3.688637]\n",
      "epoch:5 step:4399 [D loss: 0.497258, acc.: 78.12%] [G loss: 4.286681]\n",
      "epoch:5 step:4400 [D loss: 0.587530, acc.: 72.66%] [G loss: 3.065711]\n",
      "##############\n",
      "[0.82845983 0.85712801 0.80479918 0.80364231 0.80917362 0.8013808\n",
      " 0.86404058 0.7879794  0.81516101 0.83715436]\n",
      "##########\n",
      "epoch:5 step:4401 [D loss: 0.317277, acc.: 79.69%] [G loss: 2.777027]\n",
      "epoch:5 step:4402 [D loss: 0.526946, acc.: 70.31%] [G loss: 1.979068]\n",
      "epoch:5 step:4403 [D loss: 0.378589, acc.: 83.59%] [G loss: 2.118707]\n",
      "epoch:5 step:4404 [D loss: 0.378084, acc.: 75.78%] [G loss: 2.139274]\n",
      "epoch:5 step:4405 [D loss: 0.373199, acc.: 82.03%] [G loss: 2.537892]\n",
      "epoch:5 step:4406 [D loss: 0.427868, acc.: 80.47%] [G loss: 2.464220]\n",
      "epoch:5 step:4407 [D loss: 0.407586, acc.: 80.47%] [G loss: 2.231007]\n",
      "epoch:5 step:4408 [D loss: 0.397218, acc.: 81.25%] [G loss: 2.259777]\n",
      "epoch:5 step:4409 [D loss: 0.339192, acc.: 87.50%] [G loss: 3.057407]\n",
      "epoch:5 step:4410 [D loss: 0.434053, acc.: 79.69%] [G loss: 2.758117]\n",
      "epoch:5 step:4411 [D loss: 0.448543, acc.: 75.00%] [G loss: 1.948930]\n",
      "epoch:5 step:4412 [D loss: 0.413469, acc.: 82.03%] [G loss: 2.781303]\n",
      "epoch:5 step:4413 [D loss: 0.382604, acc.: 82.03%] [G loss: 2.139282]\n",
      "epoch:5 step:4414 [D loss: 0.330764, acc.: 85.16%] [G loss: 3.399511]\n",
      "epoch:5 step:4415 [D loss: 0.384524, acc.: 79.69%] [G loss: 1.935558]\n",
      "epoch:5 step:4416 [D loss: 0.391009, acc.: 78.91%] [G loss: 1.902022]\n",
      "epoch:5 step:4417 [D loss: 0.346811, acc.: 89.06%] [G loss: 1.942503]\n",
      "epoch:5 step:4418 [D loss: 0.338588, acc.: 89.84%] [G loss: 1.952803]\n",
      "epoch:5 step:4419 [D loss: 0.348491, acc.: 82.03%] [G loss: 2.538516]\n",
      "epoch:5 step:4420 [D loss: 0.525858, acc.: 71.88%] [G loss: 2.344485]\n",
      "epoch:5 step:4421 [D loss: 0.578637, acc.: 72.66%] [G loss: 3.339053]\n",
      "epoch:5 step:4422 [D loss: 0.771215, acc.: 65.62%] [G loss: 3.629628]\n",
      "epoch:5 step:4423 [D loss: 0.958307, acc.: 56.25%] [G loss: 2.798614]\n",
      "epoch:5 step:4424 [D loss: 0.469817, acc.: 76.56%] [G loss: 1.921055]\n",
      "epoch:5 step:4425 [D loss: 0.471770, acc.: 79.69%] [G loss: 2.582086]\n",
      "epoch:5 step:4426 [D loss: 0.328435, acc.: 89.06%] [G loss: 2.437403]\n",
      "epoch:5 step:4427 [D loss: 0.388434, acc.: 77.34%] [G loss: 2.901648]\n",
      "epoch:5 step:4428 [D loss: 0.333905, acc.: 83.59%] [G loss: 2.891243]\n",
      "epoch:5 step:4429 [D loss: 0.343342, acc.: 82.81%] [G loss: 2.751742]\n",
      "epoch:5 step:4430 [D loss: 0.248578, acc.: 95.31%] [G loss: 2.797104]\n",
      "epoch:5 step:4431 [D loss: 0.392685, acc.: 84.38%] [G loss: 2.041344]\n",
      "epoch:5 step:4432 [D loss: 0.353333, acc.: 85.16%] [G loss: 2.894742]\n",
      "epoch:5 step:4433 [D loss: 0.377822, acc.: 83.59%] [G loss: 2.365346]\n",
      "epoch:5 step:4434 [D loss: 0.346470, acc.: 85.16%] [G loss: 1.896362]\n",
      "epoch:5 step:4435 [D loss: 0.333492, acc.: 84.38%] [G loss: 2.428972]\n",
      "epoch:5 step:4436 [D loss: 0.408260, acc.: 81.25%] [G loss: 1.954863]\n",
      "epoch:5 step:4437 [D loss: 0.424898, acc.: 81.25%] [G loss: 2.353918]\n",
      "epoch:5 step:4438 [D loss: 0.341477, acc.: 83.59%] [G loss: 2.980836]\n",
      "epoch:5 step:4439 [D loss: 0.280513, acc.: 91.41%] [G loss: 2.523128]\n",
      "epoch:5 step:4440 [D loss: 0.317218, acc.: 89.84%] [G loss: 1.923418]\n",
      "epoch:5 step:4441 [D loss: 0.362879, acc.: 82.03%] [G loss: 1.875399]\n",
      "epoch:5 step:4442 [D loss: 0.338861, acc.: 84.38%] [G loss: 2.295328]\n",
      "epoch:5 step:4443 [D loss: 0.354585, acc.: 86.72%] [G loss: 2.221178]\n",
      "epoch:5 step:4444 [D loss: 0.394121, acc.: 83.59%] [G loss: 2.466426]\n",
      "epoch:5 step:4445 [D loss: 0.416633, acc.: 82.03%] [G loss: 3.270221]\n",
      "epoch:5 step:4446 [D loss: 0.279571, acc.: 93.75%] [G loss: 3.337495]\n",
      "epoch:5 step:4447 [D loss: 0.399275, acc.: 78.91%] [G loss: 2.226353]\n",
      "epoch:5 step:4448 [D loss: 0.349280, acc.: 85.94%] [G loss: 3.142395]\n",
      "epoch:5 step:4449 [D loss: 0.275611, acc.: 87.50%] [G loss: 3.106603]\n",
      "epoch:5 step:4450 [D loss: 0.443087, acc.: 75.78%] [G loss: 2.108611]\n",
      "epoch:5 step:4451 [D loss: 0.317313, acc.: 89.06%] [G loss: 2.789942]\n",
      "epoch:5 step:4452 [D loss: 0.487002, acc.: 73.44%] [G loss: 2.469365]\n",
      "epoch:5 step:4453 [D loss: 0.411764, acc.: 85.94%] [G loss: 2.542898]\n",
      "epoch:5 step:4454 [D loss: 0.353365, acc.: 83.59%] [G loss: 3.387406]\n",
      "epoch:5 step:4455 [D loss: 0.335742, acc.: 85.94%] [G loss: 2.174592]\n",
      "epoch:5 step:4456 [D loss: 0.273836, acc.: 88.28%] [G loss: 2.821734]\n",
      "epoch:5 step:4457 [D loss: 0.409458, acc.: 83.59%] [G loss: 2.690282]\n",
      "epoch:5 step:4458 [D loss: 0.535184, acc.: 71.09%] [G loss: 2.544641]\n",
      "epoch:5 step:4459 [D loss: 0.459139, acc.: 77.34%] [G loss: 3.054858]\n",
      "epoch:5 step:4460 [D loss: 0.508330, acc.: 70.31%] [G loss: 2.273533]\n",
      "epoch:5 step:4461 [D loss: 0.540342, acc.: 71.88%] [G loss: 2.815012]\n",
      "epoch:5 step:4462 [D loss: 0.544344, acc.: 72.66%] [G loss: 3.352458]\n",
      "epoch:5 step:4463 [D loss: 0.533073, acc.: 73.44%] [G loss: 2.231820]\n",
      "epoch:5 step:4464 [D loss: 0.311719, acc.: 86.72%] [G loss: 2.979211]\n",
      "epoch:5 step:4465 [D loss: 0.554296, acc.: 69.53%] [G loss: 2.950031]\n",
      "epoch:5 step:4466 [D loss: 0.500318, acc.: 76.56%] [G loss: 2.127815]\n",
      "epoch:5 step:4467 [D loss: 0.365614, acc.: 84.38%] [G loss: 2.579885]\n",
      "epoch:5 step:4468 [D loss: 0.352264, acc.: 87.50%] [G loss: 4.061286]\n",
      "epoch:5 step:4469 [D loss: 0.256053, acc.: 90.62%] [G loss: 4.404735]\n",
      "epoch:5 step:4470 [D loss: 0.455042, acc.: 77.34%] [G loss: 2.876684]\n",
      "epoch:5 step:4471 [D loss: 0.359621, acc.: 82.03%] [G loss: 2.545780]\n",
      "epoch:5 step:4472 [D loss: 0.346337, acc.: 88.28%] [G loss: 2.038255]\n",
      "epoch:5 step:4473 [D loss: 0.339770, acc.: 88.28%] [G loss: 2.638818]\n",
      "epoch:5 step:4474 [D loss: 0.482172, acc.: 77.34%] [G loss: 2.477583]\n",
      "epoch:5 step:4475 [D loss: 0.389721, acc.: 79.69%] [G loss: 1.863399]\n",
      "epoch:5 step:4476 [D loss: 0.361948, acc.: 82.03%] [G loss: 1.970832]\n",
      "epoch:5 step:4477 [D loss: 0.393234, acc.: 85.94%] [G loss: 2.794074]\n",
      "epoch:5 step:4478 [D loss: 0.345570, acc.: 83.59%] [G loss: 2.458762]\n",
      "epoch:5 step:4479 [D loss: 0.423553, acc.: 81.25%] [G loss: 2.149441]\n",
      "epoch:5 step:4480 [D loss: 0.480730, acc.: 73.44%] [G loss: 3.163261]\n",
      "epoch:5 step:4481 [D loss: 0.419515, acc.: 79.69%] [G loss: 3.034643]\n",
      "epoch:5 step:4482 [D loss: 0.499106, acc.: 74.22%] [G loss: 2.232319]\n",
      "epoch:5 step:4483 [D loss: 0.446146, acc.: 80.47%] [G loss: 3.540237]\n",
      "epoch:5 step:4484 [D loss: 0.444809, acc.: 80.47%] [G loss: 4.019532]\n",
      "epoch:5 step:4485 [D loss: 0.312493, acc.: 84.38%] [G loss: 4.308743]\n",
      "epoch:5 step:4486 [D loss: 0.284091, acc.: 89.06%] [G loss: 3.337403]\n",
      "epoch:5 step:4487 [D loss: 0.374736, acc.: 78.12%] [G loss: 2.925021]\n",
      "epoch:5 step:4488 [D loss: 0.326595, acc.: 84.38%] [G loss: 3.385981]\n",
      "epoch:5 step:4489 [D loss: 0.457090, acc.: 73.44%] [G loss: 2.975703]\n",
      "epoch:5 step:4490 [D loss: 0.393563, acc.: 78.91%] [G loss: 2.963969]\n",
      "epoch:5 step:4491 [D loss: 0.378583, acc.: 83.59%] [G loss: 2.824346]\n",
      "epoch:5 step:4492 [D loss: 0.345903, acc.: 82.81%] [G loss: 3.245359]\n",
      "epoch:5 step:4493 [D loss: 0.310163, acc.: 84.38%] [G loss: 3.781718]\n",
      "epoch:5 step:4494 [D loss: 0.303203, acc.: 87.50%] [G loss: 2.953390]\n",
      "epoch:5 step:4495 [D loss: 0.320885, acc.: 84.38%] [G loss: 2.621205]\n",
      "epoch:5 step:4496 [D loss: 0.465376, acc.: 76.56%] [G loss: 2.159617]\n",
      "epoch:5 step:4497 [D loss: 0.394987, acc.: 79.69%] [G loss: 2.376908]\n",
      "epoch:5 step:4498 [D loss: 0.391318, acc.: 79.69%] [G loss: 4.364354]\n",
      "epoch:5 step:4499 [D loss: 0.348399, acc.: 83.59%] [G loss: 3.250679]\n",
      "epoch:5 step:4500 [D loss: 0.391795, acc.: 78.91%] [G loss: 3.664000]\n",
      "epoch:5 step:4501 [D loss: 0.356077, acc.: 85.94%] [G loss: 2.337505]\n",
      "epoch:5 step:4502 [D loss: 0.373624, acc.: 82.03%] [G loss: 2.533018]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:5 step:4503 [D loss: 0.352530, acc.: 82.81%] [G loss: 2.813671]\n",
      "epoch:5 step:4504 [D loss: 0.316021, acc.: 85.94%] [G loss: 2.811851]\n",
      "epoch:5 step:4505 [D loss: 0.387293, acc.: 82.03%] [G loss: 3.170305]\n",
      "epoch:5 step:4506 [D loss: 0.485623, acc.: 74.22%] [G loss: 3.647607]\n",
      "epoch:5 step:4507 [D loss: 0.549938, acc.: 73.44%] [G loss: 4.271840]\n",
      "epoch:5 step:4508 [D loss: 0.799209, acc.: 69.53%] [G loss: 4.216863]\n",
      "epoch:5 step:4509 [D loss: 0.616477, acc.: 66.41%] [G loss: 3.457295]\n",
      "epoch:5 step:4510 [D loss: 0.357242, acc.: 82.03%] [G loss: 2.038228]\n",
      "epoch:5 step:4511 [D loss: 0.472520, acc.: 71.09%] [G loss: 2.304055]\n",
      "epoch:5 step:4512 [D loss: 0.389639, acc.: 78.12%] [G loss: 2.952888]\n",
      "epoch:5 step:4513 [D loss: 0.373508, acc.: 82.03%] [G loss: 2.472438]\n",
      "epoch:5 step:4514 [D loss: 0.406998, acc.: 82.03%] [G loss: 3.616689]\n",
      "epoch:5 step:4515 [D loss: 0.407842, acc.: 75.78%] [G loss: 2.954765]\n",
      "epoch:5 step:4516 [D loss: 0.339279, acc.: 87.50%] [G loss: 3.085709]\n",
      "epoch:5 step:4517 [D loss: 0.315222, acc.: 85.94%] [G loss: 2.708678]\n",
      "epoch:5 step:4518 [D loss: 0.472156, acc.: 76.56%] [G loss: 2.090190]\n",
      "epoch:5 step:4519 [D loss: 0.331314, acc.: 85.16%] [G loss: 3.065695]\n",
      "epoch:5 step:4520 [D loss: 0.353382, acc.: 84.38%] [G loss: 3.098523]\n",
      "epoch:5 step:4521 [D loss: 0.324602, acc.: 84.38%] [G loss: 3.850328]\n",
      "epoch:5 step:4522 [D loss: 0.364881, acc.: 83.59%] [G loss: 3.108432]\n",
      "epoch:5 step:4523 [D loss: 0.288976, acc.: 87.50%] [G loss: 3.510074]\n",
      "epoch:5 step:4524 [D loss: 0.301549, acc.: 86.72%] [G loss: 4.240260]\n",
      "epoch:5 step:4525 [D loss: 0.344139, acc.: 85.94%] [G loss: 2.419444]\n",
      "epoch:5 step:4526 [D loss: 0.303806, acc.: 86.72%] [G loss: 2.747589]\n",
      "epoch:5 step:4527 [D loss: 0.305689, acc.: 86.72%] [G loss: 2.706350]\n",
      "epoch:5 step:4528 [D loss: 0.354603, acc.: 84.38%] [G loss: 2.427060]\n",
      "epoch:5 step:4529 [D loss: 0.348787, acc.: 83.59%] [G loss: 2.496706]\n",
      "epoch:5 step:4530 [D loss: 0.353670, acc.: 84.38%] [G loss: 2.843268]\n",
      "epoch:5 step:4531 [D loss: 0.318774, acc.: 84.38%] [G loss: 2.610118]\n",
      "epoch:5 step:4532 [D loss: 0.428338, acc.: 80.47%] [G loss: 2.335691]\n",
      "epoch:5 step:4533 [D loss: 0.439880, acc.: 75.78%] [G loss: 3.797982]\n",
      "epoch:5 step:4534 [D loss: 0.562954, acc.: 68.75%] [G loss: 3.669740]\n",
      "epoch:5 step:4535 [D loss: 0.568774, acc.: 68.75%] [G loss: 2.734578]\n",
      "epoch:5 step:4536 [D loss: 0.535637, acc.: 70.31%] [G loss: 2.815737]\n",
      "epoch:5 step:4537 [D loss: 0.580329, acc.: 78.12%] [G loss: 4.721657]\n",
      "epoch:5 step:4538 [D loss: 1.086913, acc.: 63.28%] [G loss: 3.934450]\n",
      "epoch:5 step:4539 [D loss: 1.114110, acc.: 57.81%] [G loss: 2.413359]\n",
      "epoch:5 step:4540 [D loss: 0.720606, acc.: 71.09%] [G loss: 2.541632]\n",
      "epoch:5 step:4541 [D loss: 0.425995, acc.: 77.34%] [G loss: 2.100724]\n",
      "epoch:5 step:4542 [D loss: 0.440152, acc.: 71.09%] [G loss: 2.301010]\n",
      "epoch:5 step:4543 [D loss: 0.344766, acc.: 85.94%] [G loss: 2.374554]\n",
      "epoch:5 step:4544 [D loss: 0.447991, acc.: 77.34%] [G loss: 1.947123]\n",
      "epoch:5 step:4545 [D loss: 0.357109, acc.: 82.03%] [G loss: 2.030726]\n",
      "epoch:5 step:4546 [D loss: 0.376608, acc.: 83.59%] [G loss: 2.522351]\n",
      "epoch:5 step:4547 [D loss: 0.366710, acc.: 83.59%] [G loss: 2.718402]\n",
      "epoch:5 step:4548 [D loss: 0.305473, acc.: 89.84%] [G loss: 2.579654]\n",
      "epoch:5 step:4549 [D loss: 0.425075, acc.: 78.91%] [G loss: 1.909730]\n",
      "epoch:5 step:4550 [D loss: 0.354619, acc.: 85.94%] [G loss: 2.071888]\n",
      "epoch:5 step:4551 [D loss: 0.388640, acc.: 82.03%] [G loss: 2.240002]\n",
      "epoch:5 step:4552 [D loss: 0.418248, acc.: 78.91%] [G loss: 2.710227]\n",
      "epoch:5 step:4553 [D loss: 0.397110, acc.: 81.25%] [G loss: 2.456038]\n",
      "epoch:5 step:4554 [D loss: 0.388455, acc.: 80.47%] [G loss: 2.701385]\n",
      "epoch:5 step:4555 [D loss: 0.381252, acc.: 85.16%] [G loss: 2.298751]\n",
      "epoch:5 step:4556 [D loss: 0.434392, acc.: 76.56%] [G loss: 2.533132]\n",
      "epoch:5 step:4557 [D loss: 0.359867, acc.: 84.38%] [G loss: 2.182441]\n",
      "epoch:5 step:4558 [D loss: 0.383013, acc.: 80.47%] [G loss: 2.175084]\n",
      "epoch:5 step:4559 [D loss: 0.340864, acc.: 88.28%] [G loss: 2.073848]\n",
      "epoch:5 step:4560 [D loss: 0.340384, acc.: 82.81%] [G loss: 3.163373]\n",
      "epoch:5 step:4561 [D loss: 0.338642, acc.: 88.28%] [G loss: 3.037028]\n",
      "epoch:5 step:4562 [D loss: 0.410655, acc.: 78.91%] [G loss: 3.127569]\n",
      "epoch:5 step:4563 [D loss: 0.387124, acc.: 80.47%] [G loss: 2.340170]\n",
      "epoch:5 step:4564 [D loss: 0.305722, acc.: 86.72%] [G loss: 2.388015]\n",
      "epoch:5 step:4565 [D loss: 0.359432, acc.: 82.81%] [G loss: 2.322183]\n",
      "epoch:5 step:4566 [D loss: 0.369716, acc.: 84.38%] [G loss: 3.265389]\n",
      "epoch:5 step:4567 [D loss: 0.342960, acc.: 83.59%] [G loss: 2.704055]\n",
      "epoch:5 step:4568 [D loss: 0.366692, acc.: 79.69%] [G loss: 2.118623]\n",
      "epoch:5 step:4569 [D loss: 0.390633, acc.: 85.94%] [G loss: 2.089875]\n",
      "epoch:5 step:4570 [D loss: 0.327346, acc.: 87.50%] [G loss: 2.277742]\n",
      "epoch:5 step:4571 [D loss: 0.360994, acc.: 87.50%] [G loss: 2.123592]\n",
      "epoch:5 step:4572 [D loss: 0.344009, acc.: 83.59%] [G loss: 2.336603]\n",
      "epoch:5 step:4573 [D loss: 0.400770, acc.: 78.12%] [G loss: 1.844376]\n",
      "epoch:5 step:4574 [D loss: 0.420868, acc.: 84.38%] [G loss: 2.867819]\n",
      "epoch:5 step:4575 [D loss: 0.723644, acc.: 64.84%] [G loss: 5.433186]\n",
      "epoch:5 step:4576 [D loss: 1.100622, acc.: 57.81%] [G loss: 3.230386]\n",
      "epoch:5 step:4577 [D loss: 0.684342, acc.: 72.66%] [G loss: 3.128102]\n",
      "epoch:5 step:4578 [D loss: 0.538278, acc.: 67.97%] [G loss: 2.446656]\n",
      "epoch:5 step:4579 [D loss: 0.437197, acc.: 75.78%] [G loss: 2.190902]\n",
      "epoch:5 step:4580 [D loss: 0.460821, acc.: 75.78%] [G loss: 3.071887]\n",
      "epoch:5 step:4581 [D loss: 0.320639, acc.: 85.16%] [G loss: 3.188424]\n",
      "epoch:5 step:4582 [D loss: 0.333479, acc.: 85.94%] [G loss: 2.817332]\n",
      "epoch:5 step:4583 [D loss: 0.448850, acc.: 75.00%] [G loss: 1.963743]\n",
      "epoch:5 step:4584 [D loss: 0.355190, acc.: 82.81%] [G loss: 1.785432]\n",
      "epoch:5 step:4585 [D loss: 0.394520, acc.: 80.47%] [G loss: 1.521173]\n",
      "epoch:5 step:4586 [D loss: 0.344120, acc.: 86.72%] [G loss: 2.199757]\n",
      "epoch:5 step:4587 [D loss: 0.429075, acc.: 82.03%] [G loss: 2.205265]\n",
      "epoch:5 step:4588 [D loss: 0.353791, acc.: 86.72%] [G loss: 2.327161]\n",
      "epoch:5 step:4589 [D loss: 0.251808, acc.: 91.41%] [G loss: 5.493498]\n",
      "epoch:5 step:4590 [D loss: 0.351040, acc.: 85.94%] [G loss: 1.975330]\n",
      "epoch:5 step:4591 [D loss: 0.252465, acc.: 91.41%] [G loss: 2.466616]\n",
      "epoch:5 step:4592 [D loss: 0.323741, acc.: 85.16%] [G loss: 3.000266]\n",
      "epoch:5 step:4593 [D loss: 0.340026, acc.: 85.16%] [G loss: 2.920324]\n",
      "epoch:5 step:4594 [D loss: 0.380255, acc.: 82.03%] [G loss: 2.492435]\n",
      "epoch:5 step:4595 [D loss: 0.314376, acc.: 87.50%] [G loss: 2.906280]\n",
      "epoch:5 step:4596 [D loss: 0.354521, acc.: 84.38%] [G loss: 2.891880]\n",
      "epoch:5 step:4597 [D loss: 0.475418, acc.: 76.56%] [G loss: 3.894223]\n",
      "epoch:5 step:4598 [D loss: 0.513326, acc.: 76.56%] [G loss: 3.872332]\n",
      "epoch:5 step:4599 [D loss: 0.679480, acc.: 64.84%] [G loss: 1.956403]\n",
      "epoch:5 step:4600 [D loss: 0.429079, acc.: 78.12%] [G loss: 2.647357]\n",
      "##############\n",
      "[0.83984882 0.88646326 0.78774247 0.76895278 0.79147126 0.80014498\n",
      " 0.88850559 0.80797688 0.82669186 0.81954054]\n",
      "##########\n",
      "epoch:5 step:4601 [D loss: 0.407564, acc.: 78.91%] [G loss: 2.855526]\n",
      "epoch:5 step:4602 [D loss: 0.333428, acc.: 88.28%] [G loss: 3.180187]\n",
      "epoch:5 step:4603 [D loss: 0.328957, acc.: 85.94%] [G loss: 2.807158]\n",
      "epoch:5 step:4604 [D loss: 0.439589, acc.: 78.12%] [G loss: 2.770479]\n",
      "epoch:5 step:4605 [D loss: 0.367306, acc.: 84.38%] [G loss: 2.832057]\n",
      "epoch:5 step:4606 [D loss: 0.240498, acc.: 92.97%] [G loss: 3.375948]\n",
      "epoch:5 step:4607 [D loss: 0.342705, acc.: 84.38%] [G loss: 2.377645]\n",
      "epoch:5 step:4608 [D loss: 0.372492, acc.: 81.25%] [G loss: 1.729002]\n",
      "epoch:5 step:4609 [D loss: 0.366139, acc.: 87.50%] [G loss: 1.922164]\n",
      "epoch:5 step:4610 [D loss: 0.399765, acc.: 81.25%] [G loss: 2.195065]\n",
      "epoch:5 step:4611 [D loss: 0.315151, acc.: 89.84%] [G loss: 2.276172]\n",
      "epoch:5 step:4612 [D loss: 0.350637, acc.: 86.72%] [G loss: 2.329061]\n",
      "epoch:5 step:4613 [D loss: 0.348416, acc.: 83.59%] [G loss: 2.134026]\n",
      "epoch:5 step:4614 [D loss: 0.331858, acc.: 89.06%] [G loss: 2.453964]\n",
      "epoch:5 step:4615 [D loss: 0.318333, acc.: 89.06%] [G loss: 2.243939]\n",
      "epoch:5 step:4616 [D loss: 0.332054, acc.: 84.38%] [G loss: 2.675581]\n",
      "epoch:5 step:4617 [D loss: 0.361414, acc.: 81.25%] [G loss: 2.258482]\n",
      "epoch:5 step:4618 [D loss: 0.321932, acc.: 88.28%] [G loss: 2.834846]\n",
      "epoch:5 step:4619 [D loss: 0.371586, acc.: 79.69%] [G loss: 2.198985]\n",
      "epoch:5 step:4620 [D loss: 0.391845, acc.: 79.69%] [G loss: 3.319699]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:5 step:4621 [D loss: 0.336289, acc.: 85.16%] [G loss: 2.616772]\n",
      "epoch:5 step:4622 [D loss: 0.336629, acc.: 86.72%] [G loss: 2.212910]\n",
      "epoch:5 step:4623 [D loss: 0.316054, acc.: 86.72%] [G loss: 3.545267]\n",
      "epoch:5 step:4624 [D loss: 0.310959, acc.: 89.84%] [G loss: 2.784481]\n",
      "epoch:5 step:4625 [D loss: 0.331561, acc.: 88.28%] [G loss: 2.906632]\n",
      "epoch:5 step:4626 [D loss: 0.337945, acc.: 83.59%] [G loss: 2.412313]\n",
      "epoch:5 step:4627 [D loss: 0.330122, acc.: 85.16%] [G loss: 2.854627]\n",
      "epoch:5 step:4628 [D loss: 0.389174, acc.: 80.47%] [G loss: 2.553593]\n",
      "epoch:5 step:4629 [D loss: 0.319065, acc.: 86.72%] [G loss: 3.022190]\n",
      "epoch:5 step:4630 [D loss: 0.254673, acc.: 94.53%] [G loss: 2.621568]\n",
      "epoch:5 step:4631 [D loss: 0.360387, acc.: 85.16%] [G loss: 2.419595]\n",
      "epoch:5 step:4632 [D loss: 0.443205, acc.: 74.22%] [G loss: 2.358136]\n",
      "epoch:5 step:4633 [D loss: 0.391814, acc.: 77.34%] [G loss: 1.960704]\n",
      "epoch:5 step:4634 [D loss: 0.334677, acc.: 83.59%] [G loss: 2.412156]\n",
      "epoch:5 step:4635 [D loss: 0.420266, acc.: 79.69%] [G loss: 1.856327]\n",
      "epoch:5 step:4636 [D loss: 0.323429, acc.: 87.50%] [G loss: 1.878940]\n",
      "epoch:5 step:4637 [D loss: 0.373497, acc.: 82.03%] [G loss: 2.214688]\n",
      "epoch:5 step:4638 [D loss: 0.420182, acc.: 85.94%] [G loss: 2.050769]\n",
      "epoch:5 step:4639 [D loss: 0.393044, acc.: 82.03%] [G loss: 2.897902]\n",
      "epoch:5 step:4640 [D loss: 0.402371, acc.: 82.81%] [G loss: 3.321553]\n",
      "epoch:5 step:4641 [D loss: 0.251710, acc.: 89.06%] [G loss: 2.965073]\n",
      "epoch:5 step:4642 [D loss: 0.457012, acc.: 74.22%] [G loss: 2.043267]\n",
      "epoch:5 step:4643 [D loss: 0.431716, acc.: 77.34%] [G loss: 2.891441]\n",
      "epoch:5 step:4644 [D loss: 0.377077, acc.: 75.78%] [G loss: 2.965333]\n",
      "epoch:5 step:4645 [D loss: 0.323029, acc.: 90.62%] [G loss: 2.311188]\n",
      "epoch:5 step:4646 [D loss: 0.282502, acc.: 90.62%] [G loss: 2.278602]\n",
      "epoch:5 step:4647 [D loss: 0.315888, acc.: 89.84%] [G loss: 2.562227]\n",
      "epoch:5 step:4648 [D loss: 0.265816, acc.: 90.62%] [G loss: 2.401244]\n",
      "epoch:5 step:4649 [D loss: 0.265972, acc.: 90.62%] [G loss: 3.191330]\n",
      "epoch:5 step:4650 [D loss: 0.239511, acc.: 89.06%] [G loss: 3.446861]\n",
      "epoch:5 step:4651 [D loss: 0.320466, acc.: 85.94%] [G loss: 3.042454]\n",
      "epoch:5 step:4652 [D loss: 0.398881, acc.: 82.03%] [G loss: 2.386850]\n",
      "epoch:5 step:4653 [D loss: 0.340980, acc.: 85.16%] [G loss: 2.641132]\n",
      "epoch:5 step:4654 [D loss: 0.450815, acc.: 75.78%] [G loss: 2.331068]\n",
      "epoch:5 step:4655 [D loss: 0.387137, acc.: 83.59%] [G loss: 2.183980]\n",
      "epoch:5 step:4656 [D loss: 0.361407, acc.: 81.25%] [G loss: 2.198317]\n",
      "epoch:5 step:4657 [D loss: 0.243940, acc.: 92.97%] [G loss: 4.195692]\n",
      "epoch:5 step:4658 [D loss: 0.388487, acc.: 77.34%] [G loss: 2.987502]\n",
      "epoch:5 step:4659 [D loss: 0.450601, acc.: 78.12%] [G loss: 2.442498]\n",
      "epoch:5 step:4660 [D loss: 0.333071, acc.: 85.94%] [G loss: 3.306960]\n",
      "epoch:5 step:4661 [D loss: 0.440334, acc.: 74.22%] [G loss: 2.865697]\n",
      "epoch:5 step:4662 [D loss: 0.450089, acc.: 80.47%] [G loss: 3.814976]\n",
      "epoch:5 step:4663 [D loss: 0.486982, acc.: 75.78%] [G loss: 2.955320]\n",
      "epoch:5 step:4664 [D loss: 0.709399, acc.: 63.28%] [G loss: 3.304433]\n",
      "epoch:5 step:4665 [D loss: 0.616798, acc.: 65.62%] [G loss: 3.734407]\n",
      "epoch:5 step:4666 [D loss: 0.619471, acc.: 73.44%] [G loss: 2.088195]\n",
      "epoch:5 step:4667 [D loss: 0.474781, acc.: 75.78%] [G loss: 2.875226]\n",
      "epoch:5 step:4668 [D loss: 0.441641, acc.: 77.34%] [G loss: 2.231562]\n",
      "epoch:5 step:4669 [D loss: 0.361829, acc.: 83.59%] [G loss: 2.516413]\n",
      "epoch:5 step:4670 [D loss: 0.383382, acc.: 79.69%] [G loss: 2.871183]\n",
      "epoch:5 step:4671 [D loss: 0.392096, acc.: 78.91%] [G loss: 1.987678]\n",
      "epoch:5 step:4672 [D loss: 0.360818, acc.: 80.47%] [G loss: 1.840986]\n",
      "epoch:5 step:4673 [D loss: 0.375950, acc.: 78.91%] [G loss: 2.698640]\n",
      "epoch:5 step:4674 [D loss: 0.326000, acc.: 86.72%] [G loss: 1.934442]\n",
      "epoch:5 step:4675 [D loss: 0.294865, acc.: 87.50%] [G loss: 2.618690]\n",
      "epoch:5 step:4676 [D loss: 0.410764, acc.: 76.56%] [G loss: 2.398986]\n",
      "epoch:5 step:4677 [D loss: 0.342784, acc.: 82.03%] [G loss: 3.241275]\n",
      "epoch:5 step:4678 [D loss: 0.363102, acc.: 83.59%] [G loss: 2.970506]\n",
      "epoch:5 step:4679 [D loss: 0.323180, acc.: 85.16%] [G loss: 3.219214]\n",
      "epoch:5 step:4680 [D loss: 0.312178, acc.: 88.28%] [G loss: 4.031376]\n",
      "epoch:5 step:4681 [D loss: 0.250327, acc.: 91.41%] [G loss: 4.024580]\n",
      "epoch:5 step:4682 [D loss: 0.282168, acc.: 87.50%] [G loss: 3.078018]\n",
      "epoch:5 step:4683 [D loss: 0.366392, acc.: 81.25%] [G loss: 2.914421]\n",
      "epoch:5 step:4684 [D loss: 0.397907, acc.: 82.03%] [G loss: 2.361577]\n",
      "epoch:5 step:4685 [D loss: 0.372381, acc.: 86.72%] [G loss: 2.394048]\n",
      "epoch:5 step:4686 [D loss: 0.313118, acc.: 88.28%] [G loss: 3.186762]\n",
      "epoch:6 step:4687 [D loss: 0.402394, acc.: 83.59%] [G loss: 2.387681]\n",
      "epoch:6 step:4688 [D loss: 0.382680, acc.: 82.81%] [G loss: 2.453132]\n",
      "epoch:6 step:4689 [D loss: 0.259817, acc.: 89.84%] [G loss: 4.319638]\n",
      "epoch:6 step:4690 [D loss: 0.331438, acc.: 85.94%] [G loss: 2.629375]\n",
      "epoch:6 step:4691 [D loss: 0.320088, acc.: 83.59%] [G loss: 3.355688]\n",
      "epoch:6 step:4692 [D loss: 0.228350, acc.: 92.19%] [G loss: 4.823510]\n",
      "epoch:6 step:4693 [D loss: 0.265830, acc.: 89.84%] [G loss: 2.955629]\n",
      "epoch:6 step:4694 [D loss: 0.417330, acc.: 78.12%] [G loss: 2.626616]\n",
      "epoch:6 step:4695 [D loss: 0.367837, acc.: 82.81%] [G loss: 2.755298]\n",
      "epoch:6 step:4696 [D loss: 0.353677, acc.: 83.59%] [G loss: 2.976461]\n",
      "epoch:6 step:4697 [D loss: 0.368747, acc.: 82.81%] [G loss: 2.049967]\n",
      "epoch:6 step:4698 [D loss: 0.371718, acc.: 78.91%] [G loss: 2.576907]\n",
      "epoch:6 step:4699 [D loss: 0.355190, acc.: 81.25%] [G loss: 3.616063]\n",
      "epoch:6 step:4700 [D loss: 0.466469, acc.: 84.38%] [G loss: 4.724192]\n",
      "epoch:6 step:4701 [D loss: 0.815913, acc.: 64.06%] [G loss: 5.567242]\n",
      "epoch:6 step:4702 [D loss: 0.938882, acc.: 59.38%] [G loss: 3.160261]\n",
      "epoch:6 step:4703 [D loss: 0.374149, acc.: 83.59%] [G loss: 3.716974]\n",
      "epoch:6 step:4704 [D loss: 0.638000, acc.: 75.00%] [G loss: 4.107737]\n",
      "epoch:6 step:4705 [D loss: 0.969437, acc.: 64.06%] [G loss: 3.912971]\n",
      "epoch:6 step:4706 [D loss: 0.429533, acc.: 78.12%] [G loss: 4.856489]\n",
      "epoch:6 step:4707 [D loss: 0.469982, acc.: 78.91%] [G loss: 2.375488]\n",
      "epoch:6 step:4708 [D loss: 0.368175, acc.: 80.47%] [G loss: 2.499278]\n",
      "epoch:6 step:4709 [D loss: 0.443289, acc.: 73.44%] [G loss: 2.789052]\n",
      "epoch:6 step:4710 [D loss: 0.404810, acc.: 75.78%] [G loss: 2.417402]\n",
      "epoch:6 step:4711 [D loss: 0.468497, acc.: 73.44%] [G loss: 2.496445]\n",
      "epoch:6 step:4712 [D loss: 0.368320, acc.: 83.59%] [G loss: 2.099228]\n",
      "epoch:6 step:4713 [D loss: 0.401863, acc.: 80.47%] [G loss: 2.349938]\n",
      "epoch:6 step:4714 [D loss: 0.283005, acc.: 85.94%] [G loss: 3.082898]\n",
      "epoch:6 step:4715 [D loss: 0.304200, acc.: 89.06%] [G loss: 3.483502]\n",
      "epoch:6 step:4716 [D loss: 0.282867, acc.: 87.50%] [G loss: 2.460476]\n",
      "epoch:6 step:4717 [D loss: 0.323757, acc.: 85.94%] [G loss: 2.562600]\n",
      "epoch:6 step:4718 [D loss: 0.304255, acc.: 89.84%] [G loss: 3.227625]\n",
      "epoch:6 step:4719 [D loss: 0.238820, acc.: 92.97%] [G loss: 3.974676]\n",
      "epoch:6 step:4720 [D loss: 0.267820, acc.: 89.84%] [G loss: 3.466941]\n",
      "epoch:6 step:4721 [D loss: 0.417763, acc.: 78.12%] [G loss: 2.549038]\n",
      "epoch:6 step:4722 [D loss: 0.235477, acc.: 92.19%] [G loss: 2.754484]\n",
      "epoch:6 step:4723 [D loss: 0.384188, acc.: 78.12%] [G loss: 2.440876]\n",
      "epoch:6 step:4724 [D loss: 0.407297, acc.: 74.22%] [G loss: 2.904215]\n",
      "epoch:6 step:4725 [D loss: 0.285205, acc.: 90.62%] [G loss: 2.520833]\n",
      "epoch:6 step:4726 [D loss: 0.385758, acc.: 84.38%] [G loss: 3.705663]\n",
      "epoch:6 step:4727 [D loss: 0.437215, acc.: 77.34%] [G loss: 2.192232]\n",
      "epoch:6 step:4728 [D loss: 0.281826, acc.: 89.84%] [G loss: 2.411640]\n",
      "epoch:6 step:4729 [D loss: 0.455061, acc.: 75.78%] [G loss: 2.362891]\n",
      "epoch:6 step:4730 [D loss: 0.406245, acc.: 75.00%] [G loss: 2.741633]\n",
      "epoch:6 step:4731 [D loss: 0.342122, acc.: 83.59%] [G loss: 2.421935]\n",
      "epoch:6 step:4732 [D loss: 0.375591, acc.: 81.25%] [G loss: 2.743588]\n",
      "epoch:6 step:4733 [D loss: 0.356580, acc.: 85.16%] [G loss: 3.201678]\n",
      "epoch:6 step:4734 [D loss: 0.343140, acc.: 86.72%] [G loss: 2.120804]\n",
      "epoch:6 step:4735 [D loss: 0.396001, acc.: 77.34%] [G loss: 2.070043]\n",
      "epoch:6 step:4736 [D loss: 0.404269, acc.: 79.69%] [G loss: 2.781163]\n",
      "epoch:6 step:4737 [D loss: 0.340897, acc.: 84.38%] [G loss: 2.476057]\n",
      "epoch:6 step:4738 [D loss: 0.222638, acc.: 95.31%] [G loss: 3.790861]\n",
      "epoch:6 step:4739 [D loss: 0.338344, acc.: 84.38%] [G loss: 3.112056]\n",
      "epoch:6 step:4740 [D loss: 0.313181, acc.: 86.72%] [G loss: 2.599724]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:6 step:4741 [D loss: 0.368959, acc.: 81.25%] [G loss: 1.962038]\n",
      "epoch:6 step:4742 [D loss: 0.441264, acc.: 73.44%] [G loss: 2.723563]\n",
      "epoch:6 step:4743 [D loss: 0.361713, acc.: 83.59%] [G loss: 2.687821]\n",
      "epoch:6 step:4744 [D loss: 0.261999, acc.: 89.84%] [G loss: 3.654731]\n",
      "epoch:6 step:4745 [D loss: 0.390897, acc.: 85.16%] [G loss: 2.853322]\n",
      "epoch:6 step:4746 [D loss: 0.385160, acc.: 79.69%] [G loss: 2.431356]\n",
      "epoch:6 step:4747 [D loss: 0.324467, acc.: 84.38%] [G loss: 2.254853]\n",
      "epoch:6 step:4748 [D loss: 0.273943, acc.: 89.06%] [G loss: 2.392428]\n",
      "epoch:6 step:4749 [D loss: 0.353111, acc.: 82.81%] [G loss: 2.963464]\n",
      "epoch:6 step:4750 [D loss: 0.374879, acc.: 80.47%] [G loss: 2.922371]\n",
      "epoch:6 step:4751 [D loss: 0.406742, acc.: 81.25%] [G loss: 2.878517]\n",
      "epoch:6 step:4752 [D loss: 0.454243, acc.: 75.00%] [G loss: 3.162658]\n",
      "epoch:6 step:4753 [D loss: 0.430954, acc.: 78.12%] [G loss: 4.327110]\n",
      "epoch:6 step:4754 [D loss: 0.419595, acc.: 78.12%] [G loss: 4.467474]\n",
      "epoch:6 step:4755 [D loss: 0.613438, acc.: 67.19%] [G loss: 3.649498]\n",
      "epoch:6 step:4756 [D loss: 1.038752, acc.: 60.16%] [G loss: 5.731092]\n",
      "epoch:6 step:4757 [D loss: 1.190841, acc.: 54.69%] [G loss: 4.878112]\n",
      "epoch:6 step:4758 [D loss: 0.903799, acc.: 64.06%] [G loss: 1.932054]\n",
      "epoch:6 step:4759 [D loss: 0.437101, acc.: 75.00%] [G loss: 3.823736]\n",
      "epoch:6 step:4760 [D loss: 0.632779, acc.: 72.66%] [G loss: 2.972867]\n",
      "epoch:6 step:4761 [D loss: 0.446545, acc.: 77.34%] [G loss: 2.221009]\n",
      "epoch:6 step:4762 [D loss: 0.487274, acc.: 74.22%] [G loss: 2.064009]\n",
      "epoch:6 step:4763 [D loss: 0.327989, acc.: 85.16%] [G loss: 2.386936]\n",
      "epoch:6 step:4764 [D loss: 0.470031, acc.: 77.34%] [G loss: 1.977395]\n",
      "epoch:6 step:4765 [D loss: 0.419409, acc.: 75.00%] [G loss: 2.069598]\n",
      "epoch:6 step:4766 [D loss: 0.363436, acc.: 82.81%] [G loss: 2.375720]\n",
      "epoch:6 step:4767 [D loss: 0.400027, acc.: 85.16%] [G loss: 2.369528]\n",
      "epoch:6 step:4768 [D loss: 0.354804, acc.: 82.81%] [G loss: 3.145146]\n",
      "epoch:6 step:4769 [D loss: 0.367611, acc.: 83.59%] [G loss: 2.295938]\n",
      "epoch:6 step:4770 [D loss: 0.434814, acc.: 78.91%] [G loss: 2.832359]\n",
      "epoch:6 step:4771 [D loss: 0.479541, acc.: 75.00%] [G loss: 2.116958]\n",
      "epoch:6 step:4772 [D loss: 0.391134, acc.: 82.03%] [G loss: 2.053991]\n",
      "epoch:6 step:4773 [D loss: 0.429844, acc.: 78.12%] [G loss: 1.751616]\n",
      "epoch:6 step:4774 [D loss: 0.289706, acc.: 92.97%] [G loss: 3.147351]\n",
      "epoch:6 step:4775 [D loss: 0.281517, acc.: 85.94%] [G loss: 3.907541]\n",
      "epoch:6 step:4776 [D loss: 0.354560, acc.: 84.38%] [G loss: 2.718130]\n",
      "epoch:6 step:4777 [D loss: 0.372424, acc.: 86.72%] [G loss: 2.101692]\n",
      "epoch:6 step:4778 [D loss: 0.344981, acc.: 86.72%] [G loss: 2.056283]\n",
      "epoch:6 step:4779 [D loss: 0.267847, acc.: 92.19%] [G loss: 2.418867]\n",
      "epoch:6 step:4780 [D loss: 0.212003, acc.: 93.75%] [G loss: 4.171214]\n",
      "epoch:6 step:4781 [D loss: 0.317547, acc.: 88.28%] [G loss: 4.225885]\n",
      "epoch:6 step:4782 [D loss: 0.316564, acc.: 83.59%] [G loss: 3.448155]\n",
      "epoch:6 step:4783 [D loss: 0.378040, acc.: 78.12%] [G loss: 3.261755]\n",
      "epoch:6 step:4784 [D loss: 0.307327, acc.: 84.38%] [G loss: 4.112934]\n",
      "epoch:6 step:4785 [D loss: 0.338358, acc.: 86.72%] [G loss: 2.586102]\n",
      "epoch:6 step:4786 [D loss: 0.432175, acc.: 77.34%] [G loss: 2.704556]\n",
      "epoch:6 step:4787 [D loss: 0.485549, acc.: 71.09%] [G loss: 1.741439]\n",
      "epoch:6 step:4788 [D loss: 0.569438, acc.: 66.41%] [G loss: 3.196280]\n",
      "epoch:6 step:4789 [D loss: 0.644669, acc.: 64.06%] [G loss: 2.649170]\n",
      "epoch:6 step:4790 [D loss: 0.680978, acc.: 65.62%] [G loss: 2.070816]\n",
      "epoch:6 step:4791 [D loss: 0.551115, acc.: 75.00%] [G loss: 1.911529]\n",
      "epoch:6 step:4792 [D loss: 0.482460, acc.: 74.22%] [G loss: 2.314224]\n",
      "epoch:6 step:4793 [D loss: 0.484448, acc.: 74.22%] [G loss: 2.297130]\n",
      "epoch:6 step:4794 [D loss: 0.355562, acc.: 83.59%] [G loss: 2.633137]\n",
      "epoch:6 step:4795 [D loss: 0.444498, acc.: 77.34%] [G loss: 2.349817]\n",
      "epoch:6 step:4796 [D loss: 0.404269, acc.: 83.59%] [G loss: 2.349450]\n",
      "epoch:6 step:4797 [D loss: 0.453237, acc.: 78.12%] [G loss: 1.529894]\n",
      "epoch:6 step:4798 [D loss: 0.396446, acc.: 81.25%] [G loss: 2.322731]\n",
      "epoch:6 step:4799 [D loss: 0.351645, acc.: 85.94%] [G loss: 2.264892]\n",
      "epoch:6 step:4800 [D loss: 0.409103, acc.: 81.25%] [G loss: 2.269079]\n",
      "##############\n",
      "[0.84399471 0.87789023 0.78561763 0.79369102 0.77339606 0.79249586\n",
      " 0.84927958 0.82733757 0.83461063 0.81962309]\n",
      "##########\n",
      "epoch:6 step:4801 [D loss: 0.447204, acc.: 74.22%] [G loss: 2.261778]\n",
      "epoch:6 step:4802 [D loss: 0.288460, acc.: 87.50%] [G loss: 3.023602]\n",
      "epoch:6 step:4803 [D loss: 0.327326, acc.: 88.28%] [G loss: 2.478465]\n",
      "epoch:6 step:4804 [D loss: 0.362298, acc.: 81.25%] [G loss: 2.913747]\n",
      "epoch:6 step:4805 [D loss: 0.321277, acc.: 84.38%] [G loss: 2.955981]\n",
      "epoch:6 step:4806 [D loss: 0.280361, acc.: 89.06%] [G loss: 2.861217]\n",
      "epoch:6 step:4807 [D loss: 0.354193, acc.: 83.59%] [G loss: 2.250845]\n",
      "epoch:6 step:4808 [D loss: 0.463254, acc.: 77.34%] [G loss: 1.958392]\n",
      "epoch:6 step:4809 [D loss: 0.278277, acc.: 91.41%] [G loss: 2.795043]\n",
      "epoch:6 step:4810 [D loss: 0.293674, acc.: 90.62%] [G loss: 3.839139]\n",
      "epoch:6 step:4811 [D loss: 0.375996, acc.: 83.59%] [G loss: 2.816307]\n",
      "epoch:6 step:4812 [D loss: 0.488216, acc.: 75.00%] [G loss: 2.208528]\n",
      "epoch:6 step:4813 [D loss: 0.341362, acc.: 81.25%] [G loss: 3.474762]\n",
      "epoch:6 step:4814 [D loss: 0.321760, acc.: 83.59%] [G loss: 3.398901]\n",
      "epoch:6 step:4815 [D loss: 0.347630, acc.: 78.12%] [G loss: 3.545960]\n",
      "epoch:6 step:4816 [D loss: 0.369671, acc.: 81.25%] [G loss: 2.766163]\n",
      "epoch:6 step:4817 [D loss: 0.309658, acc.: 87.50%] [G loss: 2.738812]\n",
      "epoch:6 step:4818 [D loss: 0.345619, acc.: 86.72%] [G loss: 3.862439]\n",
      "epoch:6 step:4819 [D loss: 0.395060, acc.: 81.25%] [G loss: 1.832933]\n",
      "epoch:6 step:4820 [D loss: 0.378067, acc.: 85.16%] [G loss: 2.963017]\n",
      "epoch:6 step:4821 [D loss: 0.438697, acc.: 76.56%] [G loss: 2.534367]\n",
      "epoch:6 step:4822 [D loss: 0.474746, acc.: 70.31%] [G loss: 1.878450]\n",
      "epoch:6 step:4823 [D loss: 0.385970, acc.: 83.59%] [G loss: 1.773947]\n",
      "epoch:6 step:4824 [D loss: 0.373452, acc.: 86.72%] [G loss: 2.270745]\n",
      "epoch:6 step:4825 [D loss: 0.447589, acc.: 77.34%] [G loss: 2.374387]\n",
      "epoch:6 step:4826 [D loss: 0.382257, acc.: 82.03%] [G loss: 2.615470]\n",
      "epoch:6 step:4827 [D loss: 0.400499, acc.: 82.03%] [G loss: 2.421463]\n",
      "epoch:6 step:4828 [D loss: 0.393341, acc.: 80.47%] [G loss: 2.230102]\n",
      "epoch:6 step:4829 [D loss: 0.277741, acc.: 89.06%] [G loss: 3.122237]\n",
      "epoch:6 step:4830 [D loss: 0.353293, acc.: 82.03%] [G loss: 2.408067]\n",
      "epoch:6 step:4831 [D loss: 0.469580, acc.: 74.22%] [G loss: 3.006737]\n",
      "epoch:6 step:4832 [D loss: 0.460497, acc.: 77.34%] [G loss: 5.034982]\n",
      "epoch:6 step:4833 [D loss: 0.485328, acc.: 77.34%] [G loss: 3.428366]\n",
      "epoch:6 step:4834 [D loss: 0.410252, acc.: 76.56%] [G loss: 2.477015]\n",
      "epoch:6 step:4835 [D loss: 0.427914, acc.: 76.56%] [G loss: 2.010457]\n",
      "epoch:6 step:4836 [D loss: 0.406521, acc.: 85.16%] [G loss: 2.152703]\n",
      "epoch:6 step:4837 [D loss: 0.295287, acc.: 86.72%] [G loss: 4.052598]\n",
      "epoch:6 step:4838 [D loss: 0.265382, acc.: 91.41%] [G loss: 4.174041]\n",
      "epoch:6 step:4839 [D loss: 0.314018, acc.: 88.28%] [G loss: 2.644967]\n",
      "epoch:6 step:4840 [D loss: 0.344483, acc.: 79.69%] [G loss: 1.895295]\n",
      "epoch:6 step:4841 [D loss: 0.443858, acc.: 77.34%] [G loss: 2.995851]\n",
      "epoch:6 step:4842 [D loss: 0.303717, acc.: 89.06%] [G loss: 2.608966]\n",
      "epoch:6 step:4843 [D loss: 0.413744, acc.: 78.91%] [G loss: 2.846591]\n",
      "epoch:6 step:4844 [D loss: 0.372445, acc.: 82.81%] [G loss: 4.311833]\n",
      "epoch:6 step:4845 [D loss: 0.493169, acc.: 76.56%] [G loss: 4.669893]\n",
      "epoch:6 step:4846 [D loss: 0.797359, acc.: 71.09%] [G loss: 6.388987]\n",
      "epoch:6 step:4847 [D loss: 1.546846, acc.: 50.78%] [G loss: 2.207760]\n",
      "epoch:6 step:4848 [D loss: 0.568140, acc.: 73.44%] [G loss: 2.420115]\n",
      "epoch:6 step:4849 [D loss: 0.360854, acc.: 86.72%] [G loss: 2.452642]\n",
      "epoch:6 step:4850 [D loss: 0.502253, acc.: 75.00%] [G loss: 2.807090]\n",
      "epoch:6 step:4851 [D loss: 0.370488, acc.: 80.47%] [G loss: 2.388995]\n",
      "epoch:6 step:4852 [D loss: 0.450622, acc.: 78.12%] [G loss: 3.121207]\n",
      "epoch:6 step:4853 [D loss: 0.465054, acc.: 76.56%] [G loss: 2.326007]\n",
      "epoch:6 step:4854 [D loss: 0.340590, acc.: 86.72%] [G loss: 2.560641]\n",
      "epoch:6 step:4855 [D loss: 0.399115, acc.: 82.81%] [G loss: 2.340063]\n",
      "epoch:6 step:4856 [D loss: 0.395642, acc.: 79.69%] [G loss: 3.233142]\n",
      "epoch:6 step:4857 [D loss: 0.394088, acc.: 80.47%] [G loss: 2.418927]\n",
      "epoch:6 step:4858 [D loss: 0.309904, acc.: 91.41%] [G loss: 3.009448]\n",
      "epoch:6 step:4859 [D loss: 0.313881, acc.: 85.16%] [G loss: 3.967241]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:6 step:4860 [D loss: 0.398543, acc.: 82.03%] [G loss: 2.827378]\n",
      "epoch:6 step:4861 [D loss: 0.376152, acc.: 79.69%] [G loss: 1.934747]\n",
      "epoch:6 step:4862 [D loss: 0.379932, acc.: 85.16%] [G loss: 2.473918]\n",
      "epoch:6 step:4863 [D loss: 0.346559, acc.: 78.91%] [G loss: 2.442611]\n",
      "epoch:6 step:4864 [D loss: 0.399335, acc.: 78.91%] [G loss: 2.295435]\n",
      "epoch:6 step:4865 [D loss: 0.407430, acc.: 81.25%] [G loss: 1.833953]\n",
      "epoch:6 step:4866 [D loss: 0.344106, acc.: 85.94%] [G loss: 3.029327]\n",
      "epoch:6 step:4867 [D loss: 0.295978, acc.: 87.50%] [G loss: 3.533112]\n",
      "epoch:6 step:4868 [D loss: 0.283656, acc.: 86.72%] [G loss: 3.126808]\n",
      "epoch:6 step:4869 [D loss: 0.381900, acc.: 82.81%] [G loss: 2.786678]\n",
      "epoch:6 step:4870 [D loss: 0.412090, acc.: 80.47%] [G loss: 2.904373]\n",
      "epoch:6 step:4871 [D loss: 0.438170, acc.: 81.25%] [G loss: 2.014984]\n",
      "epoch:6 step:4872 [D loss: 0.348305, acc.: 84.38%] [G loss: 2.694528]\n",
      "epoch:6 step:4873 [D loss: 0.335443, acc.: 88.28%] [G loss: 2.836421]\n",
      "epoch:6 step:4874 [D loss: 0.348642, acc.: 84.38%] [G loss: 2.765547]\n",
      "epoch:6 step:4875 [D loss: 0.400016, acc.: 79.69%] [G loss: 1.718943]\n",
      "epoch:6 step:4876 [D loss: 0.354870, acc.: 82.03%] [G loss: 1.660213]\n",
      "epoch:6 step:4877 [D loss: 0.362602, acc.: 79.69%] [G loss: 1.948998]\n",
      "epoch:6 step:4878 [D loss: 0.271889, acc.: 90.62%] [G loss: 3.043354]\n",
      "epoch:6 step:4879 [D loss: 0.358529, acc.: 82.81%] [G loss: 2.204875]\n",
      "epoch:6 step:4880 [D loss: 0.394607, acc.: 78.91%] [G loss: 2.172393]\n",
      "epoch:6 step:4881 [D loss: 0.437715, acc.: 82.03%] [G loss: 2.028950]\n",
      "epoch:6 step:4882 [D loss: 0.305450, acc.: 89.84%] [G loss: 2.312945]\n",
      "epoch:6 step:4883 [D loss: 0.329612, acc.: 83.59%] [G loss: 2.905425]\n",
      "epoch:6 step:4884 [D loss: 0.327999, acc.: 83.59%] [G loss: 3.270819]\n",
      "epoch:6 step:4885 [D loss: 0.292903, acc.: 89.06%] [G loss: 2.517389]\n",
      "epoch:6 step:4886 [D loss: 0.321091, acc.: 89.06%] [G loss: 2.743787]\n",
      "epoch:6 step:4887 [D loss: 0.370594, acc.: 83.59%] [G loss: 2.458825]\n",
      "epoch:6 step:4888 [D loss: 0.466607, acc.: 81.25%] [G loss: 1.792467]\n",
      "epoch:6 step:4889 [D loss: 0.396082, acc.: 79.69%] [G loss: 2.041959]\n",
      "epoch:6 step:4890 [D loss: 0.368769, acc.: 85.94%] [G loss: 2.378762]\n",
      "epoch:6 step:4891 [D loss: 0.314784, acc.: 88.28%] [G loss: 2.447026]\n",
      "epoch:6 step:4892 [D loss: 0.413487, acc.: 80.47%] [G loss: 2.503881]\n",
      "epoch:6 step:4893 [D loss: 0.377727, acc.: 82.03%] [G loss: 2.588677]\n",
      "epoch:6 step:4894 [D loss: 0.419272, acc.: 79.69%] [G loss: 2.489660]\n",
      "epoch:6 step:4895 [D loss: 0.449181, acc.: 75.78%] [G loss: 3.282882]\n",
      "epoch:6 step:4896 [D loss: 0.556194, acc.: 73.44%] [G loss: 3.872083]\n",
      "epoch:6 step:4897 [D loss: 0.302468, acc.: 88.28%] [G loss: 3.873818]\n",
      "epoch:6 step:4898 [D loss: 0.409945, acc.: 77.34%] [G loss: 2.404465]\n",
      "epoch:6 step:4899 [D loss: 0.346075, acc.: 81.25%] [G loss: 4.075137]\n",
      "epoch:6 step:4900 [D loss: 0.402107, acc.: 82.03%] [G loss: 2.910527]\n",
      "epoch:6 step:4901 [D loss: 0.310210, acc.: 85.16%] [G loss: 2.714394]\n",
      "epoch:6 step:4902 [D loss: 0.486637, acc.: 75.00%] [G loss: 2.108604]\n",
      "epoch:6 step:4903 [D loss: 0.262647, acc.: 89.06%] [G loss: 3.225831]\n",
      "epoch:6 step:4904 [D loss: 0.297383, acc.: 87.50%] [G loss: 3.247092]\n",
      "epoch:6 step:4905 [D loss: 0.294040, acc.: 85.94%] [G loss: 2.834629]\n",
      "epoch:6 step:4906 [D loss: 0.450055, acc.: 73.44%] [G loss: 1.976971]\n",
      "epoch:6 step:4907 [D loss: 0.438634, acc.: 77.34%] [G loss: 2.341832]\n",
      "epoch:6 step:4908 [D loss: 0.424690, acc.: 76.56%] [G loss: 2.604893]\n",
      "epoch:6 step:4909 [D loss: 0.404121, acc.: 81.25%] [G loss: 2.848127]\n",
      "epoch:6 step:4910 [D loss: 0.345978, acc.: 84.38%] [G loss: 3.507822]\n",
      "epoch:6 step:4911 [D loss: 0.279091, acc.: 85.94%] [G loss: 5.310213]\n",
      "epoch:6 step:4912 [D loss: 0.335058, acc.: 88.28%] [G loss: 3.344647]\n",
      "epoch:6 step:4913 [D loss: 0.266395, acc.: 89.06%] [G loss: 3.475838]\n",
      "epoch:6 step:4914 [D loss: 0.362558, acc.: 80.47%] [G loss: 4.038042]\n",
      "epoch:6 step:4915 [D loss: 0.355347, acc.: 82.81%] [G loss: 2.570981]\n",
      "epoch:6 step:4916 [D loss: 0.487360, acc.: 70.31%] [G loss: 2.219924]\n",
      "epoch:6 step:4917 [D loss: 0.333252, acc.: 84.38%] [G loss: 2.470446]\n",
      "epoch:6 step:4918 [D loss: 0.332237, acc.: 86.72%] [G loss: 2.985005]\n",
      "epoch:6 step:4919 [D loss: 0.303819, acc.: 85.16%] [G loss: 3.881352]\n",
      "epoch:6 step:4920 [D loss: 0.321800, acc.: 82.81%] [G loss: 3.524562]\n",
      "epoch:6 step:4921 [D loss: 0.369156, acc.: 83.59%] [G loss: 2.216982]\n",
      "epoch:6 step:4922 [D loss: 0.333678, acc.: 86.72%] [G loss: 3.414131]\n",
      "epoch:6 step:4923 [D loss: 0.263950, acc.: 85.94%] [G loss: 4.424902]\n",
      "epoch:6 step:4924 [D loss: 0.291965, acc.: 87.50%] [G loss: 3.734290]\n",
      "epoch:6 step:4925 [D loss: 0.380309, acc.: 79.69%] [G loss: 3.201971]\n",
      "epoch:6 step:4926 [D loss: 0.332896, acc.: 84.38%] [G loss: 3.896343]\n",
      "epoch:6 step:4927 [D loss: 0.359627, acc.: 83.59%] [G loss: 3.294516]\n",
      "epoch:6 step:4928 [D loss: 0.418532, acc.: 77.34%] [G loss: 2.221310]\n",
      "epoch:6 step:4929 [D loss: 0.295558, acc.: 91.41%] [G loss: 2.736750]\n",
      "epoch:6 step:4930 [D loss: 0.329914, acc.: 84.38%] [G loss: 2.472003]\n",
      "epoch:6 step:4931 [D loss: 0.277286, acc.: 90.62%] [G loss: 2.029499]\n",
      "epoch:6 step:4932 [D loss: 0.438155, acc.: 78.12%] [G loss: 4.452802]\n",
      "epoch:6 step:4933 [D loss: 0.656788, acc.: 67.19%] [G loss: 4.450558]\n",
      "epoch:6 step:4934 [D loss: 0.824438, acc.: 61.72%] [G loss: 5.103568]\n",
      "epoch:6 step:4935 [D loss: 1.352963, acc.: 58.59%] [G loss: 5.079781]\n",
      "epoch:6 step:4936 [D loss: 1.973225, acc.: 49.22%] [G loss: 2.844665]\n",
      "epoch:6 step:4937 [D loss: 0.520786, acc.: 80.47%] [G loss: 2.235828]\n",
      "epoch:6 step:4938 [D loss: 0.539568, acc.: 73.44%] [G loss: 2.593271]\n",
      "epoch:6 step:4939 [D loss: 0.470572, acc.: 77.34%] [G loss: 3.037730]\n",
      "epoch:6 step:4940 [D loss: 0.341633, acc.: 82.81%] [G loss: 2.615601]\n",
      "epoch:6 step:4941 [D loss: 0.492126, acc.: 71.88%] [G loss: 2.420774]\n",
      "epoch:6 step:4942 [D loss: 0.356112, acc.: 84.38%] [G loss: 2.286684]\n",
      "epoch:6 step:4943 [D loss: 0.424081, acc.: 75.78%] [G loss: 2.153358]\n",
      "epoch:6 step:4944 [D loss: 0.296560, acc.: 89.06%] [G loss: 2.964362]\n",
      "epoch:6 step:4945 [D loss: 0.415069, acc.: 80.47%] [G loss: 2.728353]\n",
      "epoch:6 step:4946 [D loss: 0.357596, acc.: 84.38%] [G loss: 2.183454]\n",
      "epoch:6 step:4947 [D loss: 0.402246, acc.: 81.25%] [G loss: 2.454683]\n",
      "epoch:6 step:4948 [D loss: 0.332615, acc.: 85.16%] [G loss: 2.158968]\n",
      "epoch:6 step:4949 [D loss: 0.307307, acc.: 86.72%] [G loss: 1.973252]\n",
      "epoch:6 step:4950 [D loss: 0.393731, acc.: 80.47%] [G loss: 2.073623]\n",
      "epoch:6 step:4951 [D loss: 0.411903, acc.: 84.38%] [G loss: 2.034379]\n",
      "epoch:6 step:4952 [D loss: 0.298077, acc.: 88.28%] [G loss: 2.426201]\n",
      "epoch:6 step:4953 [D loss: 0.303666, acc.: 89.06%] [G loss: 3.218159]\n",
      "epoch:6 step:4954 [D loss: 0.284026, acc.: 86.72%] [G loss: 4.247160]\n",
      "epoch:6 step:4955 [D loss: 0.415196, acc.: 77.34%] [G loss: 3.049442]\n",
      "epoch:6 step:4956 [D loss: 0.288150, acc.: 86.72%] [G loss: 2.815316]\n",
      "epoch:6 step:4957 [D loss: 0.472768, acc.: 75.00%] [G loss: 2.183629]\n",
      "epoch:6 step:4958 [D loss: 0.429101, acc.: 76.56%] [G loss: 2.385113]\n",
      "epoch:6 step:4959 [D loss: 0.350292, acc.: 84.38%] [G loss: 2.394936]\n",
      "epoch:6 step:4960 [D loss: 0.468793, acc.: 77.34%] [G loss: 2.514766]\n",
      "epoch:6 step:4961 [D loss: 0.524667, acc.: 73.44%] [G loss: 1.960052]\n",
      "epoch:6 step:4962 [D loss: 0.553870, acc.: 75.00%] [G loss: 1.628787]\n",
      "epoch:6 step:4963 [D loss: 0.379085, acc.: 86.72%] [G loss: 2.390235]\n",
      "epoch:6 step:4964 [D loss: 0.386067, acc.: 82.81%] [G loss: 2.337960]\n",
      "epoch:6 step:4965 [D loss: 0.361736, acc.: 85.16%] [G loss: 2.557059]\n",
      "epoch:6 step:4966 [D loss: 0.271551, acc.: 89.84%] [G loss: 3.126712]\n",
      "epoch:6 step:4967 [D loss: 0.414724, acc.: 80.47%] [G loss: 3.341838]\n",
      "epoch:6 step:4968 [D loss: 0.423689, acc.: 77.34%] [G loss: 2.087971]\n",
      "epoch:6 step:4969 [D loss: 0.452099, acc.: 76.56%] [G loss: 2.421815]\n",
      "epoch:6 step:4970 [D loss: 0.327912, acc.: 85.94%] [G loss: 3.474696]\n",
      "epoch:6 step:4971 [D loss: 0.302191, acc.: 87.50%] [G loss: 4.455468]\n",
      "epoch:6 step:4972 [D loss: 0.477918, acc.: 71.88%] [G loss: 1.985639]\n",
      "epoch:6 step:4973 [D loss: 0.384311, acc.: 81.25%] [G loss: 2.952471]\n",
      "epoch:6 step:4974 [D loss: 0.279271, acc.: 92.19%] [G loss: 4.159317]\n",
      "epoch:6 step:4975 [D loss: 0.375682, acc.: 78.91%] [G loss: 2.378004]\n",
      "epoch:6 step:4976 [D loss: 0.413939, acc.: 76.56%] [G loss: 1.968713]\n",
      "epoch:6 step:4977 [D loss: 0.440483, acc.: 78.91%] [G loss: 2.259549]\n",
      "epoch:6 step:4978 [D loss: 0.360583, acc.: 83.59%] [G loss: 2.689750]\n",
      "epoch:6 step:4979 [D loss: 0.357583, acc.: 83.59%] [G loss: 2.215270]\n",
      "epoch:6 step:4980 [D loss: 0.346579, acc.: 85.16%] [G loss: 2.449730]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:6 step:4981 [D loss: 0.380116, acc.: 83.59%] [G loss: 2.264327]\n",
      "epoch:6 step:4982 [D loss: 0.449906, acc.: 81.25%] [G loss: 2.160028]\n",
      "epoch:6 step:4983 [D loss: 0.355609, acc.: 89.06%] [G loss: 1.571662]\n",
      "epoch:6 step:4984 [D loss: 0.385878, acc.: 83.59%] [G loss: 2.039667]\n",
      "epoch:6 step:4985 [D loss: 0.410090, acc.: 83.59%] [G loss: 2.178782]\n",
      "epoch:6 step:4986 [D loss: 0.469063, acc.: 72.66%] [G loss: 3.069913]\n",
      "epoch:6 step:4987 [D loss: 0.512308, acc.: 76.56%] [G loss: 3.977451]\n",
      "epoch:6 step:4988 [D loss: 0.701201, acc.: 67.97%] [G loss: 4.549198]\n",
      "epoch:6 step:4989 [D loss: 1.146917, acc.: 54.69%] [G loss: 2.828142]\n",
      "epoch:6 step:4990 [D loss: 0.508061, acc.: 67.97%] [G loss: 1.716170]\n",
      "epoch:6 step:4991 [D loss: 0.438014, acc.: 77.34%] [G loss: 1.533546]\n",
      "epoch:6 step:4992 [D loss: 0.387426, acc.: 78.91%] [G loss: 2.120835]\n",
      "epoch:6 step:4993 [D loss: 0.334513, acc.: 85.16%] [G loss: 2.953346]\n",
      "epoch:6 step:4994 [D loss: 0.368332, acc.: 78.12%] [G loss: 2.177239]\n",
      "epoch:6 step:4995 [D loss: 0.436388, acc.: 73.44%] [G loss: 2.272215]\n",
      "epoch:6 step:4996 [D loss: 0.309650, acc.: 83.59%] [G loss: 2.702839]\n",
      "epoch:6 step:4997 [D loss: 0.454978, acc.: 80.47%] [G loss: 2.020699]\n",
      "epoch:6 step:4998 [D loss: 0.425513, acc.: 75.78%] [G loss: 2.600989]\n",
      "epoch:6 step:4999 [D loss: 0.390823, acc.: 82.81%] [G loss: 2.028778]\n",
      "epoch:6 step:5000 [D loss: 0.452818, acc.: 78.91%] [G loss: 2.308290]\n",
      "##############\n",
      "[0.83202433 0.87793158 0.78391351 0.78441189 0.76344617 0.80620296\n",
      " 0.85282083 0.81932105 0.81269339 0.8146082 ]\n",
      "##########\n",
      "epoch:6 step:5001 [D loss: 0.558373, acc.: 65.62%] [G loss: 2.147326]\n",
      "epoch:6 step:5002 [D loss: 0.360217, acc.: 82.03%] [G loss: 2.440204]\n",
      "epoch:6 step:5003 [D loss: 0.387883, acc.: 83.59%] [G loss: 2.616962]\n",
      "epoch:6 step:5004 [D loss: 0.302874, acc.: 89.84%] [G loss: 2.517302]\n",
      "epoch:6 step:5005 [D loss: 0.480523, acc.: 75.78%] [G loss: 1.940491]\n",
      "epoch:6 step:5006 [D loss: 0.530345, acc.: 71.88%] [G loss: 2.370359]\n",
      "epoch:6 step:5007 [D loss: 0.338866, acc.: 88.28%] [G loss: 3.209468]\n",
      "epoch:6 step:5008 [D loss: 0.333853, acc.: 85.16%] [G loss: 3.577227]\n",
      "epoch:6 step:5009 [D loss: 0.385559, acc.: 81.25%] [G loss: 2.806720]\n",
      "epoch:6 step:5010 [D loss: 0.553741, acc.: 66.41%] [G loss: 2.095220]\n",
      "epoch:6 step:5011 [D loss: 0.253473, acc.: 90.62%] [G loss: 3.446846]\n",
      "epoch:6 step:5012 [D loss: 0.356589, acc.: 82.81%] [G loss: 2.401406]\n",
      "epoch:6 step:5013 [D loss: 0.424710, acc.: 77.34%] [G loss: 1.597426]\n",
      "epoch:6 step:5014 [D loss: 0.381119, acc.: 78.91%] [G loss: 1.752103]\n",
      "epoch:6 step:5015 [D loss: 0.332728, acc.: 85.16%] [G loss: 2.537883]\n",
      "epoch:6 step:5016 [D loss: 0.346127, acc.: 83.59%] [G loss: 2.822157]\n",
      "epoch:6 step:5017 [D loss: 0.368876, acc.: 85.94%] [G loss: 1.992543]\n",
      "epoch:6 step:5018 [D loss: 0.423019, acc.: 81.25%] [G loss: 2.109556]\n",
      "epoch:6 step:5019 [D loss: 0.351797, acc.: 84.38%] [G loss: 2.627248]\n",
      "epoch:6 step:5020 [D loss: 0.413794, acc.: 81.25%] [G loss: 2.295142]\n",
      "epoch:6 step:5021 [D loss: 0.438309, acc.: 77.34%] [G loss: 1.606570]\n",
      "epoch:6 step:5022 [D loss: 0.323767, acc.: 85.94%] [G loss: 2.776533]\n",
      "epoch:6 step:5023 [D loss: 0.312233, acc.: 87.50%] [G loss: 3.266285]\n",
      "epoch:6 step:5024 [D loss: 0.356497, acc.: 80.47%] [G loss: 2.871853]\n",
      "epoch:6 step:5025 [D loss: 0.379974, acc.: 81.25%] [G loss: 2.277073]\n",
      "epoch:6 step:5026 [D loss: 0.363311, acc.: 84.38%] [G loss: 2.640103]\n",
      "epoch:6 step:5027 [D loss: 0.540410, acc.: 75.00%] [G loss: 2.850009]\n",
      "epoch:6 step:5028 [D loss: 0.318242, acc.: 86.72%] [G loss: 2.568375]\n",
      "epoch:6 step:5029 [D loss: 0.301561, acc.: 86.72%] [G loss: 2.556367]\n",
      "epoch:6 step:5030 [D loss: 0.290141, acc.: 84.38%] [G loss: 2.789560]\n",
      "epoch:6 step:5031 [D loss: 0.358846, acc.: 83.59%] [G loss: 2.562289]\n",
      "epoch:6 step:5032 [D loss: 0.419749, acc.: 75.78%] [G loss: 2.924801]\n",
      "epoch:6 step:5033 [D loss: 0.322098, acc.: 85.94%] [G loss: 2.265240]\n",
      "epoch:6 step:5034 [D loss: 0.405839, acc.: 80.47%] [G loss: 2.243781]\n",
      "epoch:6 step:5035 [D loss: 0.378919, acc.: 82.81%] [G loss: 3.058366]\n",
      "epoch:6 step:5036 [D loss: 0.428106, acc.: 78.91%] [G loss: 2.892510]\n",
      "epoch:6 step:5037 [D loss: 0.411515, acc.: 79.69%] [G loss: 5.133484]\n",
      "epoch:6 step:5038 [D loss: 0.772400, acc.: 69.53%] [G loss: 3.579642]\n",
      "epoch:6 step:5039 [D loss: 0.843128, acc.: 60.94%] [G loss: 3.062571]\n",
      "epoch:6 step:5040 [D loss: 0.419207, acc.: 79.69%] [G loss: 2.725426]\n",
      "epoch:6 step:5041 [D loss: 0.414823, acc.: 78.91%] [G loss: 1.903457]\n",
      "epoch:6 step:5042 [D loss: 0.346621, acc.: 85.16%] [G loss: 2.646863]\n",
      "epoch:6 step:5043 [D loss: 0.366881, acc.: 85.16%] [G loss: 3.442486]\n",
      "epoch:6 step:5044 [D loss: 0.408353, acc.: 80.47%] [G loss: 2.794498]\n",
      "epoch:6 step:5045 [D loss: 0.379144, acc.: 81.25%] [G loss: 2.487311]\n",
      "epoch:6 step:5046 [D loss: 0.377638, acc.: 81.25%] [G loss: 2.655036]\n",
      "epoch:6 step:5047 [D loss: 0.450657, acc.: 74.22%] [G loss: 1.627706]\n",
      "epoch:6 step:5048 [D loss: 0.403975, acc.: 80.47%] [G loss: 1.866624]\n",
      "epoch:6 step:5049 [D loss: 0.481535, acc.: 68.75%] [G loss: 2.396361]\n",
      "epoch:6 step:5050 [D loss: 0.380064, acc.: 80.47%] [G loss: 2.598488]\n",
      "epoch:6 step:5051 [D loss: 0.286656, acc.: 89.06%] [G loss: 2.711040]\n",
      "epoch:6 step:5052 [D loss: 0.344450, acc.: 85.16%] [G loss: 2.219026]\n",
      "epoch:6 step:5053 [D loss: 0.319537, acc.: 84.38%] [G loss: 1.831024]\n",
      "epoch:6 step:5054 [D loss: 0.348871, acc.: 82.81%] [G loss: 2.362702]\n",
      "epoch:6 step:5055 [D loss: 0.389284, acc.: 81.25%] [G loss: 1.889810]\n",
      "epoch:6 step:5056 [D loss: 0.352002, acc.: 85.94%] [G loss: 1.741752]\n",
      "epoch:6 step:5057 [D loss: 0.396785, acc.: 80.47%] [G loss: 1.845051]\n",
      "epoch:6 step:5058 [D loss: 0.496974, acc.: 74.22%] [G loss: 2.890323]\n",
      "epoch:6 step:5059 [D loss: 0.586488, acc.: 73.44%] [G loss: 4.867381]\n",
      "epoch:6 step:5060 [D loss: 1.206724, acc.: 58.59%] [G loss: 3.040422]\n",
      "epoch:6 step:5061 [D loss: 1.574056, acc.: 44.53%] [G loss: 2.482285]\n",
      "epoch:6 step:5062 [D loss: 0.629139, acc.: 57.81%] [G loss: 2.502480]\n",
      "epoch:6 step:5063 [D loss: 0.646241, acc.: 71.88%] [G loss: 3.438328]\n",
      "epoch:6 step:5064 [D loss: 0.344084, acc.: 84.38%] [G loss: 2.728459]\n",
      "epoch:6 step:5065 [D loss: 0.314353, acc.: 86.72%] [G loss: 2.628759]\n",
      "epoch:6 step:5066 [D loss: 0.473147, acc.: 75.00%] [G loss: 2.691017]\n",
      "epoch:6 step:5067 [D loss: 0.326953, acc.: 88.28%] [G loss: 2.339139]\n",
      "epoch:6 step:5068 [D loss: 0.373717, acc.: 81.25%] [G loss: 3.145881]\n",
      "epoch:6 step:5069 [D loss: 0.387151, acc.: 87.50%] [G loss: 2.567502]\n",
      "epoch:6 step:5070 [D loss: 0.251867, acc.: 89.84%] [G loss: 2.461238]\n",
      "epoch:6 step:5071 [D loss: 0.332409, acc.: 84.38%] [G loss: 2.613198]\n",
      "epoch:6 step:5072 [D loss: 0.511696, acc.: 71.09%] [G loss: 2.148016]\n",
      "epoch:6 step:5073 [D loss: 0.418764, acc.: 78.91%] [G loss: 2.163167]\n",
      "epoch:6 step:5074 [D loss: 0.363969, acc.: 88.28%] [G loss: 1.882438]\n",
      "epoch:6 step:5075 [D loss: 0.388117, acc.: 85.16%] [G loss: 2.045740]\n",
      "epoch:6 step:5076 [D loss: 0.335211, acc.: 89.84%] [G loss: 1.842235]\n",
      "epoch:6 step:5077 [D loss: 0.391787, acc.: 82.81%] [G loss: 2.057042]\n",
      "epoch:6 step:5078 [D loss: 0.257525, acc.: 92.97%] [G loss: 3.076054]\n",
      "epoch:6 step:5079 [D loss: 0.286597, acc.: 90.62%] [G loss: 2.464364]\n",
      "epoch:6 step:5080 [D loss: 0.346776, acc.: 85.16%] [G loss: 2.038320]\n",
      "epoch:6 step:5081 [D loss: 0.298544, acc.: 89.84%] [G loss: 2.227209]\n",
      "epoch:6 step:5082 [D loss: 0.353787, acc.: 86.72%] [G loss: 1.912628]\n",
      "epoch:6 step:5083 [D loss: 0.225630, acc.: 91.41%] [G loss: 3.784637]\n",
      "epoch:6 step:5084 [D loss: 0.301438, acc.: 88.28%] [G loss: 2.761322]\n",
      "epoch:6 step:5085 [D loss: 0.423479, acc.: 78.12%] [G loss: 1.423362]\n",
      "epoch:6 step:5086 [D loss: 0.363404, acc.: 85.94%] [G loss: 2.266756]\n",
      "epoch:6 step:5087 [D loss: 0.317541, acc.: 90.62%] [G loss: 2.210502]\n",
      "epoch:6 step:5088 [D loss: 0.375429, acc.: 85.94%] [G loss: 2.175056]\n",
      "epoch:6 step:5089 [D loss: 0.478654, acc.: 78.12%] [G loss: 1.960299]\n",
      "epoch:6 step:5090 [D loss: 0.399295, acc.: 81.25%] [G loss: 2.140826]\n",
      "epoch:6 step:5091 [D loss: 0.348826, acc.: 88.28%] [G loss: 1.920500]\n",
      "epoch:6 step:5092 [D loss: 0.416541, acc.: 79.69%] [G loss: 2.340893]\n",
      "epoch:6 step:5093 [D loss: 0.381609, acc.: 83.59%] [G loss: 2.202428]\n",
      "epoch:6 step:5094 [D loss: 0.394898, acc.: 82.03%] [G loss: 2.246915]\n",
      "epoch:6 step:5095 [D loss: 0.332195, acc.: 85.94%] [G loss: 2.094264]\n",
      "epoch:6 step:5096 [D loss: 0.345671, acc.: 83.59%] [G loss: 3.914039]\n",
      "epoch:6 step:5097 [D loss: 0.454131, acc.: 82.03%] [G loss: 3.345616]\n",
      "epoch:6 step:5098 [D loss: 0.520416, acc.: 75.78%] [G loss: 2.433882]\n",
      "epoch:6 step:5099 [D loss: 0.341351, acc.: 82.03%] [G loss: 1.816240]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:6 step:5100 [D loss: 0.334854, acc.: 90.62%] [G loss: 3.366721]\n",
      "epoch:6 step:5101 [D loss: 0.306868, acc.: 87.50%] [G loss: 2.300381]\n",
      "epoch:6 step:5102 [D loss: 0.244658, acc.: 95.31%] [G loss: 2.553354]\n",
      "epoch:6 step:5103 [D loss: 0.384258, acc.: 84.38%] [G loss: 1.999146]\n",
      "epoch:6 step:5104 [D loss: 0.399400, acc.: 82.03%] [G loss: 1.715626]\n",
      "epoch:6 step:5105 [D loss: 0.369013, acc.: 85.94%] [G loss: 2.000524]\n",
      "epoch:6 step:5106 [D loss: 0.313971, acc.: 89.84%] [G loss: 2.939063]\n",
      "epoch:6 step:5107 [D loss: 0.312597, acc.: 85.94%] [G loss: 3.461881]\n",
      "epoch:6 step:5108 [D loss: 0.264363, acc.: 92.19%] [G loss: 2.906476]\n",
      "epoch:6 step:5109 [D loss: 0.367704, acc.: 84.38%] [G loss: 2.081145]\n",
      "epoch:6 step:5110 [D loss: 0.388071, acc.: 77.34%] [G loss: 1.631957]\n",
      "epoch:6 step:5111 [D loss: 0.452727, acc.: 71.88%] [G loss: 1.966108]\n",
      "epoch:6 step:5112 [D loss: 0.367283, acc.: 86.72%] [G loss: 2.414405]\n",
      "epoch:6 step:5113 [D loss: 0.297784, acc.: 85.94%] [G loss: 3.014963]\n",
      "epoch:6 step:5114 [D loss: 0.337468, acc.: 86.72%] [G loss: 3.005526]\n",
      "epoch:6 step:5115 [D loss: 0.463862, acc.: 75.00%] [G loss: 1.773780]\n",
      "epoch:6 step:5116 [D loss: 0.440471, acc.: 78.91%] [G loss: 3.488319]\n",
      "epoch:6 step:5117 [D loss: 0.552570, acc.: 75.00%] [G loss: 2.567879]\n",
      "epoch:6 step:5118 [D loss: 0.276405, acc.: 87.50%] [G loss: 2.970484]\n",
      "epoch:6 step:5119 [D loss: 0.355539, acc.: 83.59%] [G loss: 2.511919]\n",
      "epoch:6 step:5120 [D loss: 0.341134, acc.: 84.38%] [G loss: 4.007059]\n",
      "epoch:6 step:5121 [D loss: 0.507335, acc.: 71.09%] [G loss: 3.083668]\n",
      "epoch:6 step:5122 [D loss: 0.589075, acc.: 68.75%] [G loss: 5.170960]\n",
      "epoch:6 step:5123 [D loss: 1.172832, acc.: 67.19%] [G loss: 3.332568]\n",
      "epoch:6 step:5124 [D loss: 0.326192, acc.: 82.81%] [G loss: 3.127700]\n",
      "epoch:6 step:5125 [D loss: 0.616984, acc.: 62.50%] [G loss: 2.079934]\n",
      "epoch:6 step:5126 [D loss: 0.335110, acc.: 83.59%] [G loss: 2.148784]\n",
      "epoch:6 step:5127 [D loss: 0.331481, acc.: 85.94%] [G loss: 1.966831]\n",
      "epoch:6 step:5128 [D loss: 0.408107, acc.: 79.69%] [G loss: 2.041882]\n",
      "epoch:6 step:5129 [D loss: 0.442863, acc.: 76.56%] [G loss: 1.865649]\n",
      "epoch:6 step:5130 [D loss: 0.391283, acc.: 77.34%] [G loss: 2.330705]\n",
      "epoch:6 step:5131 [D loss: 0.257930, acc.: 90.62%] [G loss: 2.823414]\n",
      "epoch:6 step:5132 [D loss: 0.298178, acc.: 85.16%] [G loss: 2.427819]\n",
      "epoch:6 step:5133 [D loss: 0.329529, acc.: 84.38%] [G loss: 1.826874]\n",
      "epoch:6 step:5134 [D loss: 0.428717, acc.: 78.12%] [G loss: 2.178878]\n",
      "epoch:6 step:5135 [D loss: 0.286062, acc.: 88.28%] [G loss: 2.691734]\n",
      "epoch:6 step:5136 [D loss: 0.277443, acc.: 91.41%] [G loss: 2.512766]\n",
      "epoch:6 step:5137 [D loss: 0.431174, acc.: 78.12%] [G loss: 1.975521]\n",
      "epoch:6 step:5138 [D loss: 0.281548, acc.: 91.41%] [G loss: 3.335819]\n",
      "epoch:6 step:5139 [D loss: 0.270333, acc.: 89.06%] [G loss: 2.967295]\n",
      "epoch:6 step:5140 [D loss: 0.313971, acc.: 85.94%] [G loss: 2.480632]\n",
      "epoch:6 step:5141 [D loss: 0.422842, acc.: 80.47%] [G loss: 2.451825]\n",
      "epoch:6 step:5142 [D loss: 0.431356, acc.: 79.69%] [G loss: 2.187769]\n",
      "epoch:6 step:5143 [D loss: 0.450469, acc.: 74.22%] [G loss: 1.888378]\n",
      "epoch:6 step:5144 [D loss: 0.396869, acc.: 78.91%] [G loss: 1.939901]\n",
      "epoch:6 step:5145 [D loss: 0.280331, acc.: 89.84%] [G loss: 2.152176]\n",
      "epoch:6 step:5146 [D loss: 0.437708, acc.: 77.34%] [G loss: 1.728632]\n",
      "epoch:6 step:5147 [D loss: 0.435965, acc.: 76.56%] [G loss: 2.425127]\n",
      "epoch:6 step:5148 [D loss: 0.324843, acc.: 85.16%] [G loss: 2.358073]\n",
      "epoch:6 step:5149 [D loss: 0.247614, acc.: 90.62%] [G loss: 2.422240]\n",
      "epoch:6 step:5150 [D loss: 0.345708, acc.: 85.16%] [G loss: 1.843876]\n",
      "epoch:6 step:5151 [D loss: 0.384737, acc.: 82.03%] [G loss: 1.848362]\n",
      "epoch:6 step:5152 [D loss: 0.352386, acc.: 85.16%] [G loss: 1.936495]\n",
      "epoch:6 step:5153 [D loss: 0.307259, acc.: 88.28%] [G loss: 2.600264]\n",
      "epoch:6 step:5154 [D loss: 0.336934, acc.: 87.50%] [G loss: 2.661350]\n",
      "epoch:6 step:5155 [D loss: 0.409612, acc.: 76.56%] [G loss: 1.857878]\n",
      "epoch:6 step:5156 [D loss: 0.372110, acc.: 83.59%] [G loss: 2.127097]\n",
      "epoch:6 step:5157 [D loss: 0.489172, acc.: 75.00%] [G loss: 2.139227]\n",
      "epoch:6 step:5158 [D loss: 0.458545, acc.: 73.44%] [G loss: 2.611240]\n",
      "epoch:6 step:5159 [D loss: 0.476472, acc.: 71.09%] [G loss: 3.267709]\n",
      "epoch:6 step:5160 [D loss: 0.563569, acc.: 67.19%] [G loss: 2.047148]\n",
      "epoch:6 step:5161 [D loss: 0.625412, acc.: 71.09%] [G loss: 3.618079]\n",
      "epoch:6 step:5162 [D loss: 0.564628, acc.: 73.44%] [G loss: 2.881412]\n",
      "epoch:6 step:5163 [D loss: 0.350613, acc.: 83.59%] [G loss: 2.614031]\n",
      "epoch:6 step:5164 [D loss: 0.476804, acc.: 77.34%] [G loss: 1.817008]\n",
      "epoch:6 step:5165 [D loss: 0.370751, acc.: 77.34%] [G loss: 2.023027]\n",
      "epoch:6 step:5166 [D loss: 0.363238, acc.: 82.03%] [G loss: 1.907460]\n",
      "epoch:6 step:5167 [D loss: 0.365882, acc.: 84.38%] [G loss: 2.142012]\n",
      "epoch:6 step:5168 [D loss: 0.348501, acc.: 84.38%] [G loss: 2.754974]\n",
      "epoch:6 step:5169 [D loss: 0.356727, acc.: 89.06%] [G loss: 2.399906]\n",
      "epoch:6 step:5170 [D loss: 0.364535, acc.: 82.03%] [G loss: 1.852321]\n",
      "epoch:6 step:5171 [D loss: 0.338930, acc.: 86.72%] [G loss: 2.362593]\n",
      "epoch:6 step:5172 [D loss: 0.304551, acc.: 87.50%] [G loss: 3.631900]\n",
      "epoch:6 step:5173 [D loss: 0.272964, acc.: 90.62%] [G loss: 5.026536]\n",
      "epoch:6 step:5174 [D loss: 0.332750, acc.: 84.38%] [G loss: 3.354577]\n",
      "epoch:6 step:5175 [D loss: 0.401354, acc.: 78.12%] [G loss: 2.023478]\n",
      "epoch:6 step:5176 [D loss: 0.360473, acc.: 82.81%] [G loss: 2.239468]\n",
      "epoch:6 step:5177 [D loss: 0.365615, acc.: 84.38%] [G loss: 2.477437]\n",
      "epoch:6 step:5178 [D loss: 0.403535, acc.: 82.81%] [G loss: 2.166948]\n",
      "epoch:6 step:5179 [D loss: 0.252325, acc.: 92.19%] [G loss: 3.392713]\n",
      "epoch:6 step:5180 [D loss: 0.291010, acc.: 86.72%] [G loss: 2.992548]\n",
      "epoch:6 step:5181 [D loss: 0.396417, acc.: 82.81%] [G loss: 2.293346]\n",
      "epoch:6 step:5182 [D loss: 0.292788, acc.: 90.62%] [G loss: 2.336945]\n",
      "epoch:6 step:5183 [D loss: 0.333577, acc.: 83.59%] [G loss: 3.321571]\n",
      "epoch:6 step:5184 [D loss: 0.296221, acc.: 87.50%] [G loss: 2.774837]\n",
      "epoch:6 step:5185 [D loss: 0.312882, acc.: 87.50%] [G loss: 2.683705]\n",
      "epoch:6 step:5186 [D loss: 0.340736, acc.: 85.16%] [G loss: 3.183596]\n",
      "epoch:6 step:5187 [D loss: 0.364317, acc.: 84.38%] [G loss: 2.038105]\n",
      "epoch:6 step:5188 [D loss: 0.404961, acc.: 77.34%] [G loss: 2.463778]\n",
      "epoch:6 step:5189 [D loss: 0.400833, acc.: 82.81%] [G loss: 1.846608]\n",
      "epoch:6 step:5190 [D loss: 0.293703, acc.: 87.50%] [G loss: 2.322475]\n",
      "epoch:6 step:5191 [D loss: 0.443609, acc.: 75.00%] [G loss: 2.084864]\n",
      "epoch:6 step:5192 [D loss: 0.371258, acc.: 83.59%] [G loss: 2.293804]\n",
      "epoch:6 step:5193 [D loss: 0.313857, acc.: 87.50%] [G loss: 2.502759]\n",
      "epoch:6 step:5194 [D loss: 0.313930, acc.: 85.16%] [G loss: 3.605073]\n",
      "epoch:6 step:5195 [D loss: 0.288638, acc.: 88.28%] [G loss: 3.503988]\n",
      "epoch:6 step:5196 [D loss: 0.342356, acc.: 84.38%] [G loss: 3.251758]\n",
      "epoch:6 step:5197 [D loss: 0.267281, acc.: 89.84%] [G loss: 4.341512]\n",
      "epoch:6 step:5198 [D loss: 0.200073, acc.: 93.75%] [G loss: 4.266305]\n",
      "epoch:6 step:5199 [D loss: 0.299867, acc.: 83.59%] [G loss: 3.609572]\n",
      "epoch:6 step:5200 [D loss: 0.325776, acc.: 85.94%] [G loss: 2.135427]\n",
      "##############\n",
      "[0.82104928 0.86286111 0.78798243 0.78875653 0.80685572 0.8029065\n",
      " 0.85304612 0.79479572 0.81109286 0.81755136]\n",
      "##########\n",
      "epoch:6 step:5201 [D loss: 0.377686, acc.: 80.47%] [G loss: 2.252586]\n",
      "epoch:6 step:5202 [D loss: 0.448282, acc.: 75.78%] [G loss: 2.078714]\n",
      "epoch:6 step:5203 [D loss: 0.459148, acc.: 74.22%] [G loss: 2.136774]\n",
      "epoch:6 step:5204 [D loss: 0.303709, acc.: 87.50%] [G loss: 2.872346]\n",
      "epoch:6 step:5205 [D loss: 0.269379, acc.: 93.75%] [G loss: 3.831881]\n",
      "epoch:6 step:5206 [D loss: 0.235990, acc.: 92.97%] [G loss: 3.970812]\n",
      "epoch:6 step:5207 [D loss: 0.389203, acc.: 78.91%] [G loss: 2.643201]\n",
      "epoch:6 step:5208 [D loss: 0.273690, acc.: 86.72%] [G loss: 3.129536]\n",
      "epoch:6 step:5209 [D loss: 0.226393, acc.: 91.41%] [G loss: 4.110631]\n",
      "epoch:6 step:5210 [D loss: 0.351593, acc.: 82.81%] [G loss: 1.967890]\n",
      "epoch:6 step:5211 [D loss: 0.311206, acc.: 89.84%] [G loss: 2.435667]\n",
      "epoch:6 step:5212 [D loss: 0.466051, acc.: 73.44%] [G loss: 2.090452]\n",
      "epoch:6 step:5213 [D loss: 0.285880, acc.: 89.84%] [G loss: 3.239761]\n",
      "epoch:6 step:5214 [D loss: 0.310201, acc.: 88.28%] [G loss: 3.728688]\n",
      "epoch:6 step:5215 [D loss: 0.352114, acc.: 80.47%] [G loss: 2.641417]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:6 step:5216 [D loss: 0.362645, acc.: 83.59%] [G loss: 3.644860]\n",
      "epoch:6 step:5217 [D loss: 0.247043, acc.: 92.97%] [G loss: 3.377203]\n",
      "epoch:6 step:5218 [D loss: 0.256876, acc.: 92.19%] [G loss: 3.974718]\n",
      "epoch:6 step:5219 [D loss: 0.334975, acc.: 81.25%] [G loss: 2.441765]\n",
      "epoch:6 step:5220 [D loss: 0.207923, acc.: 92.19%] [G loss: 2.879025]\n",
      "epoch:6 step:5221 [D loss: 0.369204, acc.: 80.47%] [G loss: 2.333982]\n",
      "epoch:6 step:5222 [D loss: 0.326192, acc.: 85.94%] [G loss: 2.649126]\n",
      "epoch:6 step:5223 [D loss: 0.512019, acc.: 75.78%] [G loss: 2.544606]\n",
      "epoch:6 step:5224 [D loss: 0.399371, acc.: 80.47%] [G loss: 2.616851]\n",
      "epoch:6 step:5225 [D loss: 0.426514, acc.: 79.69%] [G loss: 2.619551]\n",
      "epoch:6 step:5226 [D loss: 0.467069, acc.: 75.78%] [G loss: 3.103963]\n",
      "epoch:6 step:5227 [D loss: 0.407438, acc.: 78.12%] [G loss: 3.199965]\n",
      "epoch:6 step:5228 [D loss: 0.368923, acc.: 82.03%] [G loss: 2.722591]\n",
      "epoch:6 step:5229 [D loss: 0.352976, acc.: 80.47%] [G loss: 2.695363]\n",
      "epoch:6 step:5230 [D loss: 0.298654, acc.: 85.94%] [G loss: 3.245183]\n",
      "epoch:6 step:5231 [D loss: 0.303856, acc.: 88.28%] [G loss: 2.462209]\n",
      "epoch:6 step:5232 [D loss: 0.333454, acc.: 86.72%] [G loss: 2.856015]\n",
      "epoch:6 step:5233 [D loss: 0.289970, acc.: 86.72%] [G loss: 2.970795]\n",
      "epoch:6 step:5234 [D loss: 0.328533, acc.: 85.94%] [G loss: 3.116989]\n",
      "epoch:6 step:5235 [D loss: 0.356139, acc.: 85.16%] [G loss: 2.784744]\n",
      "epoch:6 step:5236 [D loss: 0.305027, acc.: 89.06%] [G loss: 3.461483]\n",
      "epoch:6 step:5237 [D loss: 0.303097, acc.: 87.50%] [G loss: 3.153666]\n",
      "epoch:6 step:5238 [D loss: 0.412688, acc.: 81.25%] [G loss: 2.491382]\n",
      "epoch:6 step:5239 [D loss: 0.357572, acc.: 86.72%] [G loss: 2.170150]\n",
      "epoch:6 step:5240 [D loss: 0.294706, acc.: 88.28%] [G loss: 3.161043]\n",
      "epoch:6 step:5241 [D loss: 0.216761, acc.: 92.19%] [G loss: 5.857184]\n",
      "epoch:6 step:5242 [D loss: 0.244151, acc.: 89.84%] [G loss: 3.393251]\n",
      "epoch:6 step:5243 [D loss: 0.374875, acc.: 83.59%] [G loss: 2.837658]\n",
      "epoch:6 step:5244 [D loss: 0.334705, acc.: 85.94%] [G loss: 4.831468]\n",
      "epoch:6 step:5245 [D loss: 0.751920, acc.: 58.59%] [G loss: 3.073976]\n",
      "epoch:6 step:5246 [D loss: 0.584655, acc.: 71.88%] [G loss: 4.578662]\n",
      "epoch:6 step:5247 [D loss: 0.952028, acc.: 50.78%] [G loss: 5.076730]\n",
      "epoch:6 step:5248 [D loss: 0.691636, acc.: 65.62%] [G loss: 2.480674]\n",
      "epoch:6 step:5249 [D loss: 0.570035, acc.: 73.44%] [G loss: 3.318515]\n",
      "epoch:6 step:5250 [D loss: 0.638972, acc.: 67.97%] [G loss: 1.979661]\n",
      "epoch:6 step:5251 [D loss: 0.508867, acc.: 75.00%] [G loss: 2.406653]\n",
      "epoch:6 step:5252 [D loss: 0.509987, acc.: 71.09%] [G loss: 2.265866]\n",
      "epoch:6 step:5253 [D loss: 0.264509, acc.: 86.72%] [G loss: 3.078717]\n",
      "epoch:6 step:5254 [D loss: 0.271969, acc.: 89.84%] [G loss: 3.651321]\n",
      "epoch:6 step:5255 [D loss: 0.342838, acc.: 82.03%] [G loss: 2.624173]\n",
      "epoch:6 step:5256 [D loss: 0.304161, acc.: 85.94%] [G loss: 2.773638]\n",
      "epoch:6 step:5257 [D loss: 0.278653, acc.: 90.62%] [G loss: 4.203465]\n",
      "epoch:6 step:5258 [D loss: 0.399907, acc.: 81.25%] [G loss: 2.620222]\n",
      "epoch:6 step:5259 [D loss: 0.377729, acc.: 80.47%] [G loss: 1.967012]\n",
      "epoch:6 step:5260 [D loss: 0.339686, acc.: 83.59%] [G loss: 1.977764]\n",
      "epoch:6 step:5261 [D loss: 0.334286, acc.: 85.16%] [G loss: 2.744497]\n",
      "epoch:6 step:5262 [D loss: 0.279687, acc.: 91.41%] [G loss: 2.663673]\n",
      "epoch:6 step:5263 [D loss: 0.385522, acc.: 82.81%] [G loss: 2.508012]\n",
      "epoch:6 step:5264 [D loss: 0.290650, acc.: 88.28%] [G loss: 2.888090]\n",
      "epoch:6 step:5265 [D loss: 0.355887, acc.: 83.59%] [G loss: 2.048952]\n",
      "epoch:6 step:5266 [D loss: 0.374830, acc.: 80.47%] [G loss: 2.864981]\n",
      "epoch:6 step:5267 [D loss: 0.228298, acc.: 90.62%] [G loss: 4.875531]\n",
      "epoch:6 step:5268 [D loss: 0.259422, acc.: 90.62%] [G loss: 3.281442]\n",
      "epoch:6 step:5269 [D loss: 0.445846, acc.: 71.09%] [G loss: 1.746608]\n",
      "epoch:6 step:5270 [D loss: 0.403117, acc.: 83.59%] [G loss: 2.410223]\n",
      "epoch:6 step:5271 [D loss: 0.313698, acc.: 86.72%] [G loss: 2.371679]\n",
      "epoch:6 step:5272 [D loss: 0.329019, acc.: 81.25%] [G loss: 2.644372]\n",
      "epoch:6 step:5273 [D loss: 0.394793, acc.: 75.78%] [G loss: 2.751980]\n",
      "epoch:6 step:5274 [D loss: 0.335017, acc.: 85.16%] [G loss: 3.593386]\n",
      "epoch:6 step:5275 [D loss: 0.260072, acc.: 87.50%] [G loss: 4.037239]\n",
      "epoch:6 step:5276 [D loss: 0.347806, acc.: 84.38%] [G loss: 3.129309]\n",
      "epoch:6 step:5277 [D loss: 0.350321, acc.: 86.72%] [G loss: 2.853574]\n",
      "epoch:6 step:5278 [D loss: 0.227295, acc.: 91.41%] [G loss: 4.908152]\n",
      "epoch:6 step:5279 [D loss: 0.223246, acc.: 95.31%] [G loss: 3.633392]\n",
      "epoch:6 step:5280 [D loss: 0.305076, acc.: 86.72%] [G loss: 2.928498]\n",
      "epoch:6 step:5281 [D loss: 0.409033, acc.: 78.12%] [G loss: 2.684178]\n",
      "epoch:6 step:5282 [D loss: 0.423802, acc.: 73.44%] [G loss: 2.697337]\n",
      "epoch:6 step:5283 [D loss: 0.269143, acc.: 89.06%] [G loss: 3.191057]\n",
      "epoch:6 step:5284 [D loss: 0.298799, acc.: 85.94%] [G loss: 3.215042]\n",
      "epoch:6 step:5285 [D loss: 0.324193, acc.: 82.03%] [G loss: 2.044177]\n",
      "epoch:6 step:5286 [D loss: 0.390190, acc.: 82.81%] [G loss: 2.198732]\n",
      "epoch:6 step:5287 [D loss: 0.287299, acc.: 91.41%] [G loss: 3.240215]\n",
      "epoch:6 step:5288 [D loss: 0.267801, acc.: 87.50%] [G loss: 2.779074]\n",
      "epoch:6 step:5289 [D loss: 0.269338, acc.: 89.06%] [G loss: 2.382957]\n",
      "epoch:6 step:5290 [D loss: 0.360470, acc.: 83.59%] [G loss: 2.984969]\n",
      "epoch:6 step:5291 [D loss: 0.302621, acc.: 88.28%] [G loss: 2.526488]\n",
      "epoch:6 step:5292 [D loss: 0.295640, acc.: 86.72%] [G loss: 3.419313]\n",
      "epoch:6 step:5293 [D loss: 0.282590, acc.: 88.28%] [G loss: 2.741103]\n",
      "epoch:6 step:5294 [D loss: 0.413554, acc.: 85.94%] [G loss: 2.391706]\n",
      "epoch:6 step:5295 [D loss: 0.356217, acc.: 82.03%] [G loss: 2.784832]\n",
      "epoch:6 step:5296 [D loss: 0.428491, acc.: 77.34%] [G loss: 2.954345]\n",
      "epoch:6 step:5297 [D loss: 0.517869, acc.: 75.00%] [G loss: 2.599741]\n",
      "epoch:6 step:5298 [D loss: 0.559323, acc.: 73.44%] [G loss: 3.143540]\n",
      "epoch:6 step:5299 [D loss: 0.581240, acc.: 75.00%] [G loss: 6.116158]\n",
      "epoch:6 step:5300 [D loss: 0.932387, acc.: 60.94%] [G loss: 1.987812]\n",
      "epoch:6 step:5301 [D loss: 0.515912, acc.: 78.12%] [G loss: 2.540367]\n",
      "epoch:6 step:5302 [D loss: 0.470249, acc.: 74.22%] [G loss: 2.350093]\n",
      "epoch:6 step:5303 [D loss: 0.360436, acc.: 79.69%] [G loss: 4.709754]\n",
      "epoch:6 step:5304 [D loss: 0.357690, acc.: 82.81%] [G loss: 3.883153]\n",
      "epoch:6 step:5305 [D loss: 0.412332, acc.: 78.12%] [G loss: 2.760716]\n",
      "epoch:6 step:5306 [D loss: 0.275017, acc.: 85.94%] [G loss: 2.121536]\n",
      "epoch:6 step:5307 [D loss: 0.320289, acc.: 85.16%] [G loss: 2.545769]\n",
      "epoch:6 step:5308 [D loss: 0.357485, acc.: 83.59%] [G loss: 4.042887]\n",
      "epoch:6 step:5309 [D loss: 0.231224, acc.: 92.97%] [G loss: 3.888535]\n",
      "epoch:6 step:5310 [D loss: 0.346049, acc.: 83.59%] [G loss: 2.949253]\n",
      "epoch:6 step:5311 [D loss: 0.486436, acc.: 74.22%] [G loss: 2.888819]\n",
      "epoch:6 step:5312 [D loss: 0.421408, acc.: 78.12%] [G loss: 2.051665]\n",
      "epoch:6 step:5313 [D loss: 0.362093, acc.: 85.16%] [G loss: 2.352509]\n",
      "epoch:6 step:5314 [D loss: 0.414376, acc.: 80.47%] [G loss: 3.314336]\n",
      "epoch:6 step:5315 [D loss: 0.501583, acc.: 80.47%] [G loss: 4.113710]\n",
      "epoch:6 step:5316 [D loss: 0.637859, acc.: 68.75%] [G loss: 2.128623]\n",
      "epoch:6 step:5317 [D loss: 0.364274, acc.: 84.38%] [G loss: 2.100135]\n",
      "epoch:6 step:5318 [D loss: 0.329663, acc.: 86.72%] [G loss: 3.450272]\n",
      "epoch:6 step:5319 [D loss: 0.461439, acc.: 75.78%] [G loss: 1.858196]\n",
      "epoch:6 step:5320 [D loss: 0.431380, acc.: 78.91%] [G loss: 2.301854]\n",
      "epoch:6 step:5321 [D loss: 0.285316, acc.: 89.84%] [G loss: 3.142342]\n",
      "epoch:6 step:5322 [D loss: 0.361514, acc.: 82.81%] [G loss: 2.982914]\n",
      "epoch:6 step:5323 [D loss: 0.270809, acc.: 85.94%] [G loss: 2.417145]\n",
      "epoch:6 step:5324 [D loss: 0.392318, acc.: 78.12%] [G loss: 2.278270]\n",
      "epoch:6 step:5325 [D loss: 0.357335, acc.: 80.47%] [G loss: 3.005183]\n",
      "epoch:6 step:5326 [D loss: 0.266675, acc.: 89.84%] [G loss: 3.326255]\n",
      "epoch:6 step:5327 [D loss: 0.429394, acc.: 79.69%] [G loss: 2.378761]\n",
      "epoch:6 step:5328 [D loss: 0.478377, acc.: 73.44%] [G loss: 2.968976]\n",
      "epoch:6 step:5329 [D loss: 0.301358, acc.: 88.28%] [G loss: 4.184625]\n",
      "epoch:6 step:5330 [D loss: 0.270627, acc.: 89.06%] [G loss: 4.159562]\n",
      "epoch:6 step:5331 [D loss: 0.390943, acc.: 78.12%] [G loss: 3.119602]\n",
      "epoch:6 step:5332 [D loss: 0.307409, acc.: 86.72%] [G loss: 2.736590]\n",
      "epoch:6 step:5333 [D loss: 0.447892, acc.: 75.78%] [G loss: 2.750185]\n",
      "epoch:6 step:5334 [D loss: 0.407105, acc.: 82.03%] [G loss: 1.992526]\n",
      "epoch:6 step:5335 [D loss: 0.383021, acc.: 79.69%] [G loss: 2.059368]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:6 step:5336 [D loss: 0.342924, acc.: 87.50%] [G loss: 2.522618]\n",
      "epoch:6 step:5337 [D loss: 0.314769, acc.: 87.50%] [G loss: 4.409188]\n",
      "epoch:6 step:5338 [D loss: 0.339045, acc.: 87.50%] [G loss: 4.528619]\n",
      "epoch:6 step:5339 [D loss: 0.316521, acc.: 85.94%] [G loss: 2.614972]\n",
      "epoch:6 step:5340 [D loss: 0.459655, acc.: 74.22%] [G loss: 3.123885]\n",
      "epoch:6 step:5341 [D loss: 0.340478, acc.: 87.50%] [G loss: 2.280993]\n",
      "epoch:6 step:5342 [D loss: 0.425877, acc.: 81.25%] [G loss: 2.287804]\n",
      "epoch:6 step:5343 [D loss: 0.419242, acc.: 80.47%] [G loss: 3.524469]\n",
      "epoch:6 step:5344 [D loss: 0.179009, acc.: 93.75%] [G loss: 4.271353]\n",
      "epoch:6 step:5345 [D loss: 0.208492, acc.: 92.97%] [G loss: 4.933612]\n",
      "epoch:6 step:5346 [D loss: 0.225163, acc.: 92.97%] [G loss: 4.101796]\n",
      "epoch:6 step:5347 [D loss: 0.242385, acc.: 90.62%] [G loss: 4.753722]\n",
      "epoch:6 step:5348 [D loss: 0.515982, acc.: 72.66%] [G loss: 2.813984]\n",
      "epoch:6 step:5349 [D loss: 0.472779, acc.: 75.00%] [G loss: 3.661593]\n",
      "epoch:6 step:5350 [D loss: 0.493497, acc.: 73.44%] [G loss: 5.517361]\n",
      "epoch:6 step:5351 [D loss: 0.823148, acc.: 64.84%] [G loss: 2.800809]\n",
      "epoch:6 step:5352 [D loss: 0.372613, acc.: 82.81%] [G loss: 2.612331]\n",
      "epoch:6 step:5353 [D loss: 0.354675, acc.: 83.59%] [G loss: 3.575388]\n",
      "epoch:6 step:5354 [D loss: 0.275266, acc.: 89.84%] [G loss: 3.724182]\n",
      "epoch:6 step:5355 [D loss: 0.333907, acc.: 82.81%] [G loss: 3.376986]\n",
      "epoch:6 step:5356 [D loss: 0.259349, acc.: 89.06%] [G loss: 5.403743]\n",
      "epoch:6 step:5357 [D loss: 0.300727, acc.: 83.59%] [G loss: 3.172006]\n",
      "epoch:6 step:5358 [D loss: 0.277588, acc.: 87.50%] [G loss: 5.007325]\n",
      "epoch:6 step:5359 [D loss: 0.397608, acc.: 77.34%] [G loss: 2.607011]\n",
      "epoch:6 step:5360 [D loss: 0.262824, acc.: 89.84%] [G loss: 2.678855]\n",
      "epoch:6 step:5361 [D loss: 0.253510, acc.: 87.50%] [G loss: 3.545691]\n",
      "epoch:6 step:5362 [D loss: 0.235078, acc.: 90.62%] [G loss: 4.539015]\n",
      "epoch:6 step:5363 [D loss: 0.282310, acc.: 88.28%] [G loss: 2.875563]\n",
      "epoch:6 step:5364 [D loss: 0.411288, acc.: 78.12%] [G loss: 2.396530]\n",
      "epoch:6 step:5365 [D loss: 0.374536, acc.: 83.59%] [G loss: 2.542074]\n",
      "epoch:6 step:5366 [D loss: 0.364149, acc.: 85.16%] [G loss: 1.917253]\n",
      "epoch:6 step:5367 [D loss: 0.378060, acc.: 85.16%] [G loss: 2.348886]\n",
      "epoch:6 step:5368 [D loss: 0.512252, acc.: 73.44%] [G loss: 3.909356]\n",
      "epoch:6 step:5369 [D loss: 0.676401, acc.: 72.66%] [G loss: 4.579581]\n",
      "epoch:6 step:5370 [D loss: 1.057514, acc.: 61.72%] [G loss: 4.505087]\n",
      "epoch:6 step:5371 [D loss: 0.888013, acc.: 71.88%] [G loss: 4.906534]\n",
      "epoch:6 step:5372 [D loss: 0.541566, acc.: 75.78%] [G loss: 3.189322]\n",
      "epoch:6 step:5373 [D loss: 0.291440, acc.: 82.03%] [G loss: 4.419551]\n",
      "epoch:6 step:5374 [D loss: 0.313946, acc.: 83.59%] [G loss: 3.427486]\n",
      "epoch:6 step:5375 [D loss: 0.275019, acc.: 86.72%] [G loss: 4.148596]\n",
      "epoch:6 step:5376 [D loss: 0.279504, acc.: 88.28%] [G loss: 2.636132]\n",
      "epoch:6 step:5377 [D loss: 0.435108, acc.: 71.09%] [G loss: 2.097706]\n",
      "epoch:6 step:5378 [D loss: 0.611149, acc.: 68.75%] [G loss: 1.574234]\n",
      "epoch:6 step:5379 [D loss: 0.331400, acc.: 88.28%] [G loss: 2.444602]\n",
      "epoch:6 step:5380 [D loss: 0.281519, acc.: 90.62%] [G loss: 2.900772]\n",
      "epoch:6 step:5381 [D loss: 0.340262, acc.: 82.81%] [G loss: 4.205436]\n",
      "epoch:6 step:5382 [D loss: 0.301626, acc.: 84.38%] [G loss: 2.436964]\n",
      "epoch:6 step:5383 [D loss: 0.336918, acc.: 80.47%] [G loss: 2.709892]\n",
      "epoch:6 step:5384 [D loss: 0.301865, acc.: 91.41%] [G loss: 2.182346]\n",
      "epoch:6 step:5385 [D loss: 0.409879, acc.: 79.69%] [G loss: 1.649533]\n",
      "epoch:6 step:5386 [D loss: 0.350948, acc.: 85.94%] [G loss: 2.182528]\n",
      "epoch:6 step:5387 [D loss: 0.361629, acc.: 82.81%] [G loss: 2.999238]\n",
      "epoch:6 step:5388 [D loss: 0.276701, acc.: 89.84%] [G loss: 3.473235]\n",
      "epoch:6 step:5389 [D loss: 0.326567, acc.: 84.38%] [G loss: 2.623812]\n",
      "epoch:6 step:5390 [D loss: 0.312270, acc.: 83.59%] [G loss: 2.437794]\n",
      "epoch:6 step:5391 [D loss: 0.357273, acc.: 83.59%] [G loss: 1.767829]\n",
      "epoch:6 step:5392 [D loss: 0.335241, acc.: 79.69%] [G loss: 2.386719]\n",
      "epoch:6 step:5393 [D loss: 0.294496, acc.: 88.28%] [G loss: 3.118070]\n",
      "epoch:6 step:5394 [D loss: 0.380274, acc.: 78.91%] [G loss: 1.982791]\n",
      "epoch:6 step:5395 [D loss: 0.381684, acc.: 83.59%] [G loss: 2.087189]\n",
      "epoch:6 step:5396 [D loss: 0.335536, acc.: 89.84%] [G loss: 2.329087]\n",
      "epoch:6 step:5397 [D loss: 0.249127, acc.: 92.19%] [G loss: 2.845476]\n",
      "epoch:6 step:5398 [D loss: 0.316384, acc.: 91.41%] [G loss: 2.253064]\n",
      "epoch:6 step:5399 [D loss: 0.389410, acc.: 81.25%] [G loss: 1.823410]\n",
      "epoch:6 step:5400 [D loss: 0.341110, acc.: 88.28%] [G loss: 1.863026]\n",
      "##############\n",
      "[0.80294164 0.89571059 0.78826233 0.79827385 0.77430318 0.78886604\n",
      " 0.85894986 0.80210716 0.80302437 0.8164537 ]\n",
      "##########\n",
      "epoch:6 step:5401 [D loss: 0.341683, acc.: 86.72%] [G loss: 2.022176]\n",
      "epoch:6 step:5402 [D loss: 0.373694, acc.: 83.59%] [G loss: 2.187081]\n",
      "epoch:6 step:5403 [D loss: 0.289853, acc.: 90.62%] [G loss: 2.508127]\n",
      "epoch:6 step:5404 [D loss: 0.326093, acc.: 86.72%] [G loss: 2.874456]\n",
      "epoch:6 step:5405 [D loss: 0.339307, acc.: 85.94%] [G loss: 2.603663]\n",
      "epoch:6 step:5406 [D loss: 0.350347, acc.: 91.41%] [G loss: 1.866693]\n",
      "epoch:6 step:5407 [D loss: 0.450669, acc.: 77.34%] [G loss: 2.102394]\n",
      "epoch:6 step:5408 [D loss: 0.259092, acc.: 90.62%] [G loss: 2.766633]\n",
      "epoch:6 step:5409 [D loss: 0.260696, acc.: 92.19%] [G loss: 2.609650]\n",
      "epoch:6 step:5410 [D loss: 0.326750, acc.: 87.50%] [G loss: 2.908748]\n",
      "epoch:6 step:5411 [D loss: 0.316547, acc.: 90.62%] [G loss: 2.690126]\n",
      "epoch:6 step:5412 [D loss: 0.261789, acc.: 93.75%] [G loss: 3.813190]\n",
      "epoch:6 step:5413 [D loss: 0.287129, acc.: 87.50%] [G loss: 2.719830]\n",
      "epoch:6 step:5414 [D loss: 0.383406, acc.: 79.69%] [G loss: 2.233479]\n",
      "epoch:6 step:5415 [D loss: 0.283000, acc.: 92.19%] [G loss: 2.115854]\n",
      "epoch:6 step:5416 [D loss: 0.272882, acc.: 88.28%] [G loss: 2.966277]\n",
      "epoch:6 step:5417 [D loss: 0.316716, acc.: 87.50%] [G loss: 3.055900]\n",
      "epoch:6 step:5418 [D loss: 0.361554, acc.: 86.72%] [G loss: 3.528433]\n",
      "epoch:6 step:5419 [D loss: 0.406369, acc.: 85.16%] [G loss: 3.329592]\n",
      "epoch:6 step:5420 [D loss: 0.341404, acc.: 87.50%] [G loss: 2.949641]\n",
      "epoch:6 step:5421 [D loss: 0.252759, acc.: 89.84%] [G loss: 3.591706]\n",
      "epoch:6 step:5422 [D loss: 0.418638, acc.: 79.69%] [G loss: 2.106802]\n",
      "epoch:6 step:5423 [D loss: 0.410432, acc.: 78.91%] [G loss: 2.173904]\n",
      "epoch:6 step:5424 [D loss: 0.433486, acc.: 75.78%] [G loss: 2.165078]\n",
      "epoch:6 step:5425 [D loss: 0.284118, acc.: 92.97%] [G loss: 2.579896]\n",
      "epoch:6 step:5426 [D loss: 0.233856, acc.: 89.84%] [G loss: 4.206491]\n",
      "epoch:6 step:5427 [D loss: 0.212734, acc.: 90.62%] [G loss: 4.714624]\n",
      "epoch:6 step:5428 [D loss: 0.360977, acc.: 81.25%] [G loss: 3.048646]\n",
      "epoch:6 step:5429 [D loss: 0.317704, acc.: 86.72%] [G loss: 1.868521]\n",
      "epoch:6 step:5430 [D loss: 0.331155, acc.: 83.59%] [G loss: 2.534720]\n",
      "epoch:6 step:5431 [D loss: 0.322126, acc.: 89.84%] [G loss: 1.908025]\n",
      "epoch:6 step:5432 [D loss: 0.375841, acc.: 84.38%] [G loss: 3.769651]\n",
      "epoch:6 step:5433 [D loss: 0.365507, acc.: 82.81%] [G loss: 2.334572]\n",
      "epoch:6 step:5434 [D loss: 0.348158, acc.: 82.81%] [G loss: 3.156480]\n",
      "epoch:6 step:5435 [D loss: 0.410677, acc.: 81.25%] [G loss: 2.315786]\n",
      "epoch:6 step:5436 [D loss: 0.344296, acc.: 83.59%] [G loss: 2.421041]\n",
      "epoch:6 step:5437 [D loss: 0.283826, acc.: 86.72%] [G loss: 2.766948]\n",
      "epoch:6 step:5438 [D loss: 0.335953, acc.: 86.72%] [G loss: 3.383090]\n",
      "epoch:6 step:5439 [D loss: 0.248428, acc.: 92.19%] [G loss: 2.764804]\n",
      "epoch:6 step:5440 [D loss: 0.396026, acc.: 78.91%] [G loss: 2.199249]\n",
      "epoch:6 step:5441 [D loss: 0.344732, acc.: 85.16%] [G loss: 2.417813]\n",
      "epoch:6 step:5442 [D loss: 0.331933, acc.: 85.16%] [G loss: 2.602021]\n",
      "epoch:6 step:5443 [D loss: 0.351223, acc.: 83.59%] [G loss: 2.136264]\n",
      "epoch:6 step:5444 [D loss: 0.364402, acc.: 83.59%] [G loss: 2.316243]\n",
      "epoch:6 step:5445 [D loss: 0.328587, acc.: 87.50%] [G loss: 1.905515]\n",
      "epoch:6 step:5446 [D loss: 0.318714, acc.: 85.16%] [G loss: 2.216937]\n",
      "epoch:6 step:5447 [D loss: 0.277781, acc.: 89.84%] [G loss: 2.686179]\n",
      "epoch:6 step:5448 [D loss: 0.258454, acc.: 91.41%] [G loss: 2.927935]\n",
      "epoch:6 step:5449 [D loss: 0.288436, acc.: 92.19%] [G loss: 2.734139]\n",
      "epoch:6 step:5450 [D loss: 0.409085, acc.: 82.03%] [G loss: 2.192341]\n",
      "epoch:6 step:5451 [D loss: 0.506239, acc.: 72.66%] [G loss: 4.455075]\n",
      "epoch:6 step:5452 [D loss: 0.839018, acc.: 62.50%] [G loss: 4.868189]\n",
      "epoch:6 step:5453 [D loss: 1.016183, acc.: 59.38%] [G loss: 2.312437]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:6 step:5454 [D loss: 0.589276, acc.: 76.56%] [G loss: 3.891971]\n",
      "epoch:6 step:5455 [D loss: 0.337212, acc.: 82.81%] [G loss: 2.476914]\n",
      "epoch:6 step:5456 [D loss: 0.394083, acc.: 79.69%] [G loss: 3.109043]\n",
      "epoch:6 step:5457 [D loss: 0.405692, acc.: 76.56%] [G loss: 2.271969]\n",
      "epoch:6 step:5458 [D loss: 0.356110, acc.: 86.72%] [G loss: 2.001778]\n",
      "epoch:6 step:5459 [D loss: 0.414697, acc.: 79.69%] [G loss: 3.182338]\n",
      "epoch:6 step:5460 [D loss: 0.444488, acc.: 77.34%] [G loss: 2.264972]\n",
      "epoch:6 step:5461 [D loss: 0.340646, acc.: 78.91%] [G loss: 3.208692]\n",
      "epoch:6 step:5462 [D loss: 0.214968, acc.: 92.19%] [G loss: 4.686279]\n",
      "epoch:6 step:5463 [D loss: 0.318446, acc.: 85.16%] [G loss: 2.590963]\n",
      "epoch:6 step:5464 [D loss: 0.295344, acc.: 86.72%] [G loss: 3.889521]\n",
      "epoch:6 step:5465 [D loss: 0.251603, acc.: 87.50%] [G loss: 3.811958]\n",
      "epoch:6 step:5466 [D loss: 0.286613, acc.: 89.84%] [G loss: 4.521957]\n",
      "epoch:6 step:5467 [D loss: 0.266560, acc.: 88.28%] [G loss: 4.808077]\n",
      "epoch:7 step:5468 [D loss: 0.269192, acc.: 87.50%] [G loss: 3.910262]\n",
      "epoch:7 step:5469 [D loss: 0.301730, acc.: 85.16%] [G loss: 3.134420]\n",
      "epoch:7 step:5470 [D loss: 0.297822, acc.: 86.72%] [G loss: 3.162873]\n",
      "epoch:7 step:5471 [D loss: 0.331950, acc.: 87.50%] [G loss: 2.262065]\n",
      "epoch:7 step:5472 [D loss: 0.284241, acc.: 89.84%] [G loss: 2.673369]\n",
      "epoch:7 step:5473 [D loss: 0.335763, acc.: 82.03%] [G loss: 4.246928]\n",
      "epoch:7 step:5474 [D loss: 0.436357, acc.: 76.56%] [G loss: 2.037863]\n",
      "epoch:7 step:5475 [D loss: 0.301582, acc.: 89.06%] [G loss: 2.677807]\n",
      "epoch:7 step:5476 [D loss: 0.480921, acc.: 74.22%] [G loss: 2.668684]\n",
      "epoch:7 step:5477 [D loss: 0.496536, acc.: 75.78%] [G loss: 2.656351]\n",
      "epoch:7 step:5478 [D loss: 0.518037, acc.: 73.44%] [G loss: 3.485467]\n",
      "epoch:7 step:5479 [D loss: 0.238068, acc.: 90.62%] [G loss: 3.998402]\n",
      "epoch:7 step:5480 [D loss: 0.360021, acc.: 82.81%] [G loss: 4.524774]\n",
      "epoch:7 step:5481 [D loss: 0.461724, acc.: 72.66%] [G loss: 3.689249]\n",
      "epoch:7 step:5482 [D loss: 0.347571, acc.: 85.94%] [G loss: 4.336557]\n",
      "epoch:7 step:5483 [D loss: 0.442919, acc.: 78.12%] [G loss: 3.477542]\n",
      "epoch:7 step:5484 [D loss: 0.330302, acc.: 83.59%] [G loss: 1.912131]\n",
      "epoch:7 step:5485 [D loss: 0.375608, acc.: 80.47%] [G loss: 2.280375]\n",
      "epoch:7 step:5486 [D loss: 0.414829, acc.: 79.69%] [G loss: 2.006952]\n",
      "epoch:7 step:5487 [D loss: 0.388268, acc.: 81.25%] [G loss: 2.229155]\n",
      "epoch:7 step:5488 [D loss: 0.305262, acc.: 89.06%] [G loss: 2.326565]\n",
      "epoch:7 step:5489 [D loss: 0.293182, acc.: 87.50%] [G loss: 4.481350]\n",
      "epoch:7 step:5490 [D loss: 0.312555, acc.: 86.72%] [G loss: 2.841846]\n",
      "epoch:7 step:5491 [D loss: 0.237948, acc.: 92.19%] [G loss: 4.201948]\n",
      "epoch:7 step:5492 [D loss: 0.236920, acc.: 89.06%] [G loss: 3.239704]\n",
      "epoch:7 step:5493 [D loss: 0.255179, acc.: 92.19%] [G loss: 2.890672]\n",
      "epoch:7 step:5494 [D loss: 0.343854, acc.: 85.94%] [G loss: 2.881864]\n",
      "epoch:7 step:5495 [D loss: 0.407800, acc.: 78.12%] [G loss: 2.545079]\n",
      "epoch:7 step:5496 [D loss: 0.430905, acc.: 78.12%] [G loss: 2.860884]\n",
      "epoch:7 step:5497 [D loss: 0.342194, acc.: 84.38%] [G loss: 2.448477]\n",
      "epoch:7 step:5498 [D loss: 0.394067, acc.: 79.69%] [G loss: 2.395298]\n",
      "epoch:7 step:5499 [D loss: 0.332341, acc.: 88.28%] [G loss: 3.033495]\n",
      "epoch:7 step:5500 [D loss: 0.332306, acc.: 85.94%] [G loss: 2.093157]\n",
      "epoch:7 step:5501 [D loss: 0.376309, acc.: 83.59%] [G loss: 3.509600]\n",
      "epoch:7 step:5502 [D loss: 0.383828, acc.: 82.81%] [G loss: 3.277446]\n",
      "epoch:7 step:5503 [D loss: 0.304329, acc.: 88.28%] [G loss: 2.001075]\n",
      "epoch:7 step:5504 [D loss: 0.472137, acc.: 75.78%] [G loss: 2.392344]\n",
      "epoch:7 step:5505 [D loss: 0.338633, acc.: 86.72%] [G loss: 1.851141]\n",
      "epoch:7 step:5506 [D loss: 0.382683, acc.: 82.03%] [G loss: 2.240305]\n",
      "epoch:7 step:5507 [D loss: 0.297025, acc.: 87.50%] [G loss: 3.726148]\n",
      "epoch:7 step:5508 [D loss: 0.239427, acc.: 91.41%] [G loss: 4.540868]\n",
      "epoch:7 step:5509 [D loss: 0.303533, acc.: 89.06%] [G loss: 3.291897]\n",
      "epoch:7 step:5510 [D loss: 0.352946, acc.: 82.03%] [G loss: 2.232176]\n",
      "epoch:7 step:5511 [D loss: 0.343372, acc.: 85.16%] [G loss: 2.399524]\n",
      "epoch:7 step:5512 [D loss: 0.355986, acc.: 84.38%] [G loss: 2.658275]\n",
      "epoch:7 step:5513 [D loss: 0.207338, acc.: 91.41%] [G loss: 4.581849]\n",
      "epoch:7 step:5514 [D loss: 0.388978, acc.: 80.47%] [G loss: 2.993326]\n",
      "epoch:7 step:5515 [D loss: 0.257452, acc.: 90.62%] [G loss: 2.894154]\n",
      "epoch:7 step:5516 [D loss: 0.384554, acc.: 81.25%] [G loss: 4.014124]\n",
      "epoch:7 step:5517 [D loss: 0.383173, acc.: 85.16%] [G loss: 3.272813]\n",
      "epoch:7 step:5518 [D loss: 0.445333, acc.: 74.22%] [G loss: 2.482219]\n",
      "epoch:7 step:5519 [D loss: 0.339660, acc.: 80.47%] [G loss: 2.070893]\n",
      "epoch:7 step:5520 [D loss: 0.251311, acc.: 91.41%] [G loss: 3.640733]\n",
      "epoch:7 step:5521 [D loss: 0.284299, acc.: 85.94%] [G loss: 2.257221]\n",
      "epoch:7 step:5522 [D loss: 0.394297, acc.: 80.47%] [G loss: 3.359008]\n",
      "epoch:7 step:5523 [D loss: 0.471209, acc.: 73.44%] [G loss: 2.354284]\n",
      "epoch:7 step:5524 [D loss: 0.380930, acc.: 79.69%] [G loss: 2.668061]\n",
      "epoch:7 step:5525 [D loss: 0.419622, acc.: 81.25%] [G loss: 2.307564]\n",
      "epoch:7 step:5526 [D loss: 0.417014, acc.: 85.94%] [G loss: 3.366494]\n",
      "epoch:7 step:5527 [D loss: 0.667660, acc.: 75.00%] [G loss: 5.785932]\n",
      "epoch:7 step:5528 [D loss: 2.023129, acc.: 56.25%] [G loss: 2.598960]\n",
      "epoch:7 step:5529 [D loss: 0.770847, acc.: 75.78%] [G loss: 2.100866]\n",
      "epoch:7 step:5530 [D loss: 0.345363, acc.: 86.72%] [G loss: 2.295476]\n",
      "epoch:7 step:5531 [D loss: 0.387761, acc.: 82.81%] [G loss: 2.988059]\n",
      "epoch:7 step:5532 [D loss: 0.355691, acc.: 83.59%] [G loss: 3.312450]\n",
      "epoch:7 step:5533 [D loss: 0.314915, acc.: 83.59%] [G loss: 3.721804]\n",
      "epoch:7 step:5534 [D loss: 0.261918, acc.: 89.84%] [G loss: 4.396727]\n",
      "epoch:7 step:5535 [D loss: 0.346859, acc.: 83.59%] [G loss: 2.841717]\n",
      "epoch:7 step:5536 [D loss: 0.417726, acc.: 75.78%] [G loss: 2.916413]\n",
      "epoch:7 step:5537 [D loss: 0.321286, acc.: 87.50%] [G loss: 2.377673]\n",
      "epoch:7 step:5538 [D loss: 0.369890, acc.: 82.03%] [G loss: 2.481386]\n",
      "epoch:7 step:5539 [D loss: 0.357386, acc.: 85.16%] [G loss: 3.603601]\n",
      "epoch:7 step:5540 [D loss: 0.425151, acc.: 82.81%] [G loss: 3.188038]\n",
      "epoch:7 step:5541 [D loss: 0.364257, acc.: 84.38%] [G loss: 2.284105]\n",
      "epoch:7 step:5542 [D loss: 0.280128, acc.: 89.06%] [G loss: 3.059968]\n",
      "epoch:7 step:5543 [D loss: 0.457946, acc.: 73.44%] [G loss: 1.645348]\n",
      "epoch:7 step:5544 [D loss: 0.320918, acc.: 86.72%] [G loss: 2.578537]\n",
      "epoch:7 step:5545 [D loss: 0.396656, acc.: 78.91%] [G loss: 2.831681]\n",
      "epoch:7 step:5546 [D loss: 0.444589, acc.: 80.47%] [G loss: 6.472629]\n",
      "epoch:7 step:5547 [D loss: 1.125707, acc.: 62.50%] [G loss: 4.135431]\n",
      "epoch:7 step:5548 [D loss: 0.721490, acc.: 71.09%] [G loss: 2.681998]\n",
      "epoch:7 step:5549 [D loss: 0.574635, acc.: 74.22%] [G loss: 3.204070]\n",
      "epoch:7 step:5550 [D loss: 0.405388, acc.: 79.69%] [G loss: 4.217597]\n",
      "epoch:7 step:5551 [D loss: 0.315324, acc.: 86.72%] [G loss: 3.121907]\n",
      "epoch:7 step:5552 [D loss: 0.341356, acc.: 82.03%] [G loss: 2.837015]\n",
      "epoch:7 step:5553 [D loss: 0.360478, acc.: 85.94%] [G loss: 2.254295]\n",
      "epoch:7 step:5554 [D loss: 0.496974, acc.: 76.56%] [G loss: 3.119861]\n",
      "epoch:7 step:5555 [D loss: 0.432842, acc.: 79.69%] [G loss: 2.646457]\n",
      "epoch:7 step:5556 [D loss: 0.398066, acc.: 78.12%] [G loss: 3.368456]\n",
      "epoch:7 step:5557 [D loss: 0.250926, acc.: 89.06%] [G loss: 3.206964]\n",
      "epoch:7 step:5558 [D loss: 0.426964, acc.: 75.78%] [G loss: 2.467978]\n",
      "epoch:7 step:5559 [D loss: 0.365788, acc.: 85.94%] [G loss: 2.526061]\n",
      "epoch:7 step:5560 [D loss: 0.308452, acc.: 87.50%] [G loss: 3.146697]\n",
      "epoch:7 step:5561 [D loss: 0.311806, acc.: 88.28%] [G loss: 3.578350]\n",
      "epoch:7 step:5562 [D loss: 0.349998, acc.: 79.69%] [G loss: 3.229880]\n",
      "epoch:7 step:5563 [D loss: 0.321245, acc.: 84.38%] [G loss: 2.395632]\n",
      "epoch:7 step:5564 [D loss: 0.335170, acc.: 78.91%] [G loss: 4.147223]\n",
      "epoch:7 step:5565 [D loss: 0.418444, acc.: 78.12%] [G loss: 2.228421]\n",
      "epoch:7 step:5566 [D loss: 0.396687, acc.: 75.00%] [G loss: 2.550875]\n",
      "epoch:7 step:5567 [D loss: 0.388622, acc.: 84.38%] [G loss: 2.034444]\n",
      "epoch:7 step:5568 [D loss: 0.438077, acc.: 79.69%] [G loss: 2.624581]\n",
      "epoch:7 step:5569 [D loss: 0.459260, acc.: 80.47%] [G loss: 3.137254]\n",
      "epoch:7 step:5570 [D loss: 0.399033, acc.: 77.34%] [G loss: 3.898099]\n",
      "epoch:7 step:5571 [D loss: 0.300165, acc.: 89.06%] [G loss: 6.641214]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:7 step:5572 [D loss: 0.216349, acc.: 90.62%] [G loss: 6.311406]\n",
      "epoch:7 step:5573 [D loss: 0.398275, acc.: 82.03%] [G loss: 2.362478]\n",
      "epoch:7 step:5574 [D loss: 0.320640, acc.: 89.84%] [G loss: 2.477479]\n",
      "epoch:7 step:5575 [D loss: 0.368226, acc.: 83.59%] [G loss: 3.986921]\n",
      "epoch:7 step:5576 [D loss: 0.347175, acc.: 83.59%] [G loss: 2.816180]\n",
      "epoch:7 step:5577 [D loss: 0.333251, acc.: 85.94%] [G loss: 2.655699]\n",
      "epoch:7 step:5578 [D loss: 0.313862, acc.: 85.94%] [G loss: 4.437243]\n",
      "epoch:7 step:5579 [D loss: 0.228961, acc.: 92.19%] [G loss: 6.013293]\n",
      "epoch:7 step:5580 [D loss: 0.352330, acc.: 87.50%] [G loss: 3.573548]\n",
      "epoch:7 step:5581 [D loss: 0.383885, acc.: 85.16%] [G loss: 2.677289]\n",
      "epoch:7 step:5582 [D loss: 0.494618, acc.: 69.53%] [G loss: 3.472570]\n",
      "epoch:7 step:5583 [D loss: 0.548157, acc.: 71.09%] [G loss: 2.647187]\n",
      "epoch:7 step:5584 [D loss: 0.387137, acc.: 82.03%] [G loss: 2.686314]\n",
      "epoch:7 step:5585 [D loss: 0.312638, acc.: 85.16%] [G loss: 3.862936]\n",
      "epoch:7 step:5586 [D loss: 0.328995, acc.: 86.72%] [G loss: 2.835356]\n",
      "epoch:7 step:5587 [D loss: 0.335357, acc.: 84.38%] [G loss: 2.349979]\n",
      "epoch:7 step:5588 [D loss: 0.330736, acc.: 92.19%] [G loss: 2.340572]\n",
      "epoch:7 step:5589 [D loss: 0.304924, acc.: 88.28%] [G loss: 3.053063]\n",
      "epoch:7 step:5590 [D loss: 0.282246, acc.: 87.50%] [G loss: 5.630012]\n",
      "epoch:7 step:5591 [D loss: 0.375308, acc.: 84.38%] [G loss: 3.410876]\n",
      "epoch:7 step:5592 [D loss: 0.391900, acc.: 79.69%] [G loss: 5.103184]\n",
      "epoch:7 step:5593 [D loss: 0.677606, acc.: 66.41%] [G loss: 2.867726]\n",
      "epoch:7 step:5594 [D loss: 0.624548, acc.: 72.66%] [G loss: 3.614746]\n",
      "epoch:7 step:5595 [D loss: 0.197198, acc.: 92.19%] [G loss: 4.515283]\n",
      "epoch:7 step:5596 [D loss: 0.276026, acc.: 90.62%] [G loss: 3.228179]\n",
      "epoch:7 step:5597 [D loss: 0.316156, acc.: 83.59%] [G loss: 3.034662]\n",
      "epoch:7 step:5598 [D loss: 0.351704, acc.: 83.59%] [G loss: 2.505946]\n",
      "epoch:7 step:5599 [D loss: 0.305447, acc.: 85.94%] [G loss: 3.435346]\n",
      "epoch:7 step:5600 [D loss: 0.315419, acc.: 84.38%] [G loss: 4.118073]\n",
      "##############\n",
      "[0.81884626 0.89191766 0.79197318 0.78558394 0.78463999 0.79133235\n",
      " 0.84798553 0.84220779 0.79953646 0.8218274 ]\n",
      "##########\n",
      "epoch:7 step:5601 [D loss: 0.282368, acc.: 88.28%] [G loss: 2.762761]\n",
      "epoch:7 step:5602 [D loss: 0.277897, acc.: 87.50%] [G loss: 3.462541]\n",
      "epoch:7 step:5603 [D loss: 0.265012, acc.: 90.62%] [G loss: 2.272743]\n",
      "epoch:7 step:5604 [D loss: 0.283932, acc.: 89.06%] [G loss: 2.627491]\n",
      "epoch:7 step:5605 [D loss: 0.244187, acc.: 92.97%] [G loss: 3.433613]\n",
      "epoch:7 step:5606 [D loss: 0.389156, acc.: 80.47%] [G loss: 2.537275]\n",
      "epoch:7 step:5607 [D loss: 0.170222, acc.: 95.31%] [G loss: 4.148032]\n",
      "epoch:7 step:5608 [D loss: 0.206811, acc.: 94.53%] [G loss: 5.826087]\n",
      "epoch:7 step:5609 [D loss: 0.331172, acc.: 85.16%] [G loss: 2.505461]\n",
      "epoch:7 step:5610 [D loss: 0.205314, acc.: 90.62%] [G loss: 3.039853]\n",
      "epoch:7 step:5611 [D loss: 0.271417, acc.: 91.41%] [G loss: 3.229928]\n",
      "epoch:7 step:5612 [D loss: 0.353201, acc.: 83.59%] [G loss: 1.866134]\n",
      "epoch:7 step:5613 [D loss: 0.308691, acc.: 88.28%] [G loss: 2.497253]\n",
      "epoch:7 step:5614 [D loss: 0.374868, acc.: 85.94%] [G loss: 1.929617]\n",
      "epoch:7 step:5615 [D loss: 0.410724, acc.: 81.25%] [G loss: 2.634301]\n",
      "epoch:7 step:5616 [D loss: 0.353120, acc.: 86.72%] [G loss: 2.400003]\n",
      "epoch:7 step:5617 [D loss: 0.299178, acc.: 92.97%] [G loss: 2.383113]\n",
      "epoch:7 step:5618 [D loss: 0.307831, acc.: 91.41%] [G loss: 2.267769]\n",
      "epoch:7 step:5619 [D loss: 0.338580, acc.: 91.41%] [G loss: 2.988950]\n",
      "epoch:7 step:5620 [D loss: 0.244871, acc.: 90.62%] [G loss: 3.949141]\n",
      "epoch:7 step:5621 [D loss: 0.338501, acc.: 84.38%] [G loss: 2.245902]\n",
      "epoch:7 step:5622 [D loss: 0.293123, acc.: 92.19%] [G loss: 2.345492]\n",
      "epoch:7 step:5623 [D loss: 0.300012, acc.: 89.06%] [G loss: 2.174208]\n",
      "epoch:7 step:5624 [D loss: 0.413967, acc.: 77.34%] [G loss: 2.558122]\n",
      "epoch:7 step:5625 [D loss: 0.284007, acc.: 89.06%] [G loss: 4.192883]\n",
      "epoch:7 step:5626 [D loss: 0.297072, acc.: 86.72%] [G loss: 2.229200]\n",
      "epoch:7 step:5627 [D loss: 0.354782, acc.: 85.16%] [G loss: 2.755427]\n",
      "epoch:7 step:5628 [D loss: 0.371205, acc.: 84.38%] [G loss: 6.523114]\n",
      "epoch:7 step:5629 [D loss: 0.534320, acc.: 74.22%] [G loss: 2.898564]\n",
      "epoch:7 step:5630 [D loss: 0.461255, acc.: 76.56%] [G loss: 4.268023]\n",
      "epoch:7 step:5631 [D loss: 0.254611, acc.: 90.62%] [G loss: 3.941578]\n",
      "epoch:7 step:5632 [D loss: 0.373321, acc.: 81.25%] [G loss: 7.816124]\n",
      "epoch:7 step:5633 [D loss: 0.791325, acc.: 66.41%] [G loss: 3.811571]\n",
      "epoch:7 step:5634 [D loss: 0.853167, acc.: 65.62%] [G loss: 4.828093]\n",
      "epoch:7 step:5635 [D loss: 0.594354, acc.: 75.78%] [G loss: 1.855565]\n",
      "epoch:7 step:5636 [D loss: 0.405580, acc.: 78.12%] [G loss: 4.718484]\n",
      "epoch:7 step:5637 [D loss: 0.353534, acc.: 80.47%] [G loss: 3.647958]\n",
      "epoch:7 step:5638 [D loss: 0.368370, acc.: 82.03%] [G loss: 4.493217]\n",
      "epoch:7 step:5639 [D loss: 0.352024, acc.: 83.59%] [G loss: 2.863498]\n",
      "epoch:7 step:5640 [D loss: 0.477097, acc.: 74.22%] [G loss: 2.396423]\n",
      "epoch:7 step:5641 [D loss: 0.295401, acc.: 89.06%] [G loss: 2.868014]\n",
      "epoch:7 step:5642 [D loss: 0.419652, acc.: 82.81%] [G loss: 2.592513]\n",
      "epoch:7 step:5643 [D loss: 0.296415, acc.: 88.28%] [G loss: 2.584254]\n",
      "epoch:7 step:5644 [D loss: 0.277353, acc.: 89.06%] [G loss: 2.465126]\n",
      "epoch:7 step:5645 [D loss: 0.279366, acc.: 92.19%] [G loss: 2.054506]\n",
      "epoch:7 step:5646 [D loss: 0.264935, acc.: 92.97%] [G loss: 2.994105]\n",
      "epoch:7 step:5647 [D loss: 0.237872, acc.: 90.62%] [G loss: 3.268216]\n",
      "epoch:7 step:5648 [D loss: 0.296516, acc.: 92.19%] [G loss: 2.159833]\n",
      "epoch:7 step:5649 [D loss: 0.278415, acc.: 94.53%] [G loss: 3.492377]\n",
      "epoch:7 step:5650 [D loss: 0.238466, acc.: 91.41%] [G loss: 3.410450]\n",
      "epoch:7 step:5651 [D loss: 0.308173, acc.: 88.28%] [G loss: 1.901674]\n",
      "epoch:7 step:5652 [D loss: 0.235412, acc.: 89.06%] [G loss: 2.771566]\n",
      "epoch:7 step:5653 [D loss: 0.313061, acc.: 89.84%] [G loss: 2.323894]\n",
      "epoch:7 step:5654 [D loss: 0.293129, acc.: 90.62%] [G loss: 2.109162]\n",
      "epoch:7 step:5655 [D loss: 0.312547, acc.: 89.06%] [G loss: 2.202494]\n",
      "epoch:7 step:5656 [D loss: 0.345508, acc.: 86.72%] [G loss: 1.880853]\n",
      "epoch:7 step:5657 [D loss: 0.264137, acc.: 90.62%] [G loss: 3.318870]\n",
      "epoch:7 step:5658 [D loss: 0.321512, acc.: 85.16%] [G loss: 2.870882]\n",
      "epoch:7 step:5659 [D loss: 0.336192, acc.: 84.38%] [G loss: 2.429637]\n",
      "epoch:7 step:5660 [D loss: 0.327067, acc.: 86.72%] [G loss: 2.566190]\n",
      "epoch:7 step:5661 [D loss: 0.325788, acc.: 87.50%] [G loss: 2.214433]\n",
      "epoch:7 step:5662 [D loss: 0.262088, acc.: 90.62%] [G loss: 3.342600]\n",
      "epoch:7 step:5663 [D loss: 0.307939, acc.: 91.41%] [G loss: 2.418688]\n",
      "epoch:7 step:5664 [D loss: 0.300319, acc.: 90.62%] [G loss: 2.894453]\n",
      "epoch:7 step:5665 [D loss: 0.245858, acc.: 90.62%] [G loss: 2.611110]\n",
      "epoch:7 step:5666 [D loss: 0.276992, acc.: 84.38%] [G loss: 5.348598]\n",
      "epoch:7 step:5667 [D loss: 0.205995, acc.: 96.09%] [G loss: 2.958549]\n",
      "epoch:7 step:5668 [D loss: 0.228780, acc.: 93.75%] [G loss: 2.787810]\n",
      "epoch:7 step:5669 [D loss: 0.298642, acc.: 92.19%] [G loss: 2.840993]\n",
      "epoch:7 step:5670 [D loss: 0.199853, acc.: 95.31%] [G loss: 2.302899]\n",
      "epoch:7 step:5671 [D loss: 0.271196, acc.: 89.84%] [G loss: 2.580666]\n",
      "epoch:7 step:5672 [D loss: 0.272330, acc.: 91.41%] [G loss: 2.665778]\n",
      "epoch:7 step:5673 [D loss: 0.267798, acc.: 91.41%] [G loss: 2.764198]\n",
      "epoch:7 step:5674 [D loss: 0.342535, acc.: 85.16%] [G loss: 2.984085]\n",
      "epoch:7 step:5675 [D loss: 0.346729, acc.: 85.94%] [G loss: 2.308836]\n",
      "epoch:7 step:5676 [D loss: 0.252443, acc.: 92.97%] [G loss: 2.487804]\n",
      "epoch:7 step:5677 [D loss: 0.271352, acc.: 91.41%] [G loss: 3.623825]\n",
      "epoch:7 step:5678 [D loss: 0.277906, acc.: 92.19%] [G loss: 2.427560]\n",
      "epoch:7 step:5679 [D loss: 0.295879, acc.: 90.62%] [G loss: 2.806390]\n",
      "epoch:7 step:5680 [D loss: 0.394724, acc.: 82.81%] [G loss: 2.047017]\n",
      "epoch:7 step:5681 [D loss: 0.345952, acc.: 84.38%] [G loss: 1.890491]\n",
      "epoch:7 step:5682 [D loss: 0.366837, acc.: 86.72%] [G loss: 2.769819]\n",
      "epoch:7 step:5683 [D loss: 0.265057, acc.: 91.41%] [G loss: 3.969339]\n",
      "epoch:7 step:5684 [D loss: 0.226713, acc.: 92.97%] [G loss: 3.466278]\n",
      "epoch:7 step:5685 [D loss: 0.251237, acc.: 90.62%] [G loss: 3.517885]\n",
      "epoch:7 step:5686 [D loss: 0.307157, acc.: 89.06%] [G loss: 2.548374]\n",
      "epoch:7 step:5687 [D loss: 0.307098, acc.: 88.28%] [G loss: 2.314752]\n",
      "epoch:7 step:5688 [D loss: 0.257607, acc.: 94.53%] [G loss: 2.525579]\n",
      "epoch:7 step:5689 [D loss: 0.274647, acc.: 87.50%] [G loss: 2.479925]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:7 step:5690 [D loss: 0.308290, acc.: 87.50%] [G loss: 2.038660]\n",
      "epoch:7 step:5691 [D loss: 0.332619, acc.: 89.06%] [G loss: 2.284827]\n",
      "epoch:7 step:5692 [D loss: 0.366364, acc.: 80.47%] [G loss: 2.767394]\n",
      "epoch:7 step:5693 [D loss: 0.247398, acc.: 92.19%] [G loss: 2.717627]\n",
      "epoch:7 step:5694 [D loss: 0.261743, acc.: 90.62%] [G loss: 2.844939]\n",
      "epoch:7 step:5695 [D loss: 0.345029, acc.: 87.50%] [G loss: 2.910417]\n",
      "epoch:7 step:5696 [D loss: 0.222283, acc.: 92.97%] [G loss: 4.115395]\n",
      "epoch:7 step:5697 [D loss: 0.265139, acc.: 89.06%] [G loss: 2.948892]\n",
      "epoch:7 step:5698 [D loss: 0.229371, acc.: 89.84%] [G loss: 3.107226]\n",
      "epoch:7 step:5699 [D loss: 0.217395, acc.: 90.62%] [G loss: 4.480868]\n",
      "epoch:7 step:5700 [D loss: 0.319144, acc.: 88.28%] [G loss: 3.081779]\n",
      "epoch:7 step:5701 [D loss: 0.195710, acc.: 95.31%] [G loss: 3.828025]\n",
      "epoch:7 step:5702 [D loss: 0.298988, acc.: 88.28%] [G loss: 2.476084]\n",
      "epoch:7 step:5703 [D loss: 0.352162, acc.: 80.47%] [G loss: 4.011198]\n",
      "epoch:7 step:5704 [D loss: 0.295435, acc.: 88.28%] [G loss: 2.397572]\n",
      "epoch:7 step:5705 [D loss: 0.389170, acc.: 81.25%] [G loss: 2.570986]\n",
      "epoch:7 step:5706 [D loss: 0.371298, acc.: 83.59%] [G loss: 2.432235]\n",
      "epoch:7 step:5707 [D loss: 0.355189, acc.: 86.72%] [G loss: 2.670984]\n",
      "epoch:7 step:5708 [D loss: 0.310717, acc.: 87.50%] [G loss: 2.426241]\n",
      "epoch:7 step:5709 [D loss: 0.371323, acc.: 81.25%] [G loss: 2.095260]\n",
      "epoch:7 step:5710 [D loss: 0.320358, acc.: 90.62%] [G loss: 3.640910]\n",
      "epoch:7 step:5711 [D loss: 0.473914, acc.: 77.34%] [G loss: 2.086682]\n",
      "epoch:7 step:5712 [D loss: 0.323660, acc.: 88.28%] [G loss: 2.223658]\n",
      "epoch:7 step:5713 [D loss: 0.303202, acc.: 86.72%] [G loss: 2.418790]\n",
      "epoch:7 step:5714 [D loss: 0.336466, acc.: 82.81%] [G loss: 2.757880]\n",
      "epoch:7 step:5715 [D loss: 0.300468, acc.: 85.94%] [G loss: 3.809175]\n",
      "epoch:7 step:5716 [D loss: 0.301286, acc.: 88.28%] [G loss: 3.793806]\n",
      "epoch:7 step:5717 [D loss: 0.255699, acc.: 85.94%] [G loss: 4.736664]\n",
      "epoch:7 step:5718 [D loss: 0.180436, acc.: 95.31%] [G loss: 3.926980]\n",
      "epoch:7 step:5719 [D loss: 0.277822, acc.: 87.50%] [G loss: 4.594880]\n",
      "epoch:7 step:5720 [D loss: 0.436684, acc.: 82.03%] [G loss: 2.570349]\n",
      "epoch:7 step:5721 [D loss: 0.282776, acc.: 86.72%] [G loss: 3.195200]\n",
      "epoch:7 step:5722 [D loss: 0.312513, acc.: 85.94%] [G loss: 2.581053]\n",
      "epoch:7 step:5723 [D loss: 0.274604, acc.: 92.19%] [G loss: 2.754424]\n",
      "epoch:7 step:5724 [D loss: 0.445399, acc.: 76.56%] [G loss: 2.075910]\n",
      "epoch:7 step:5725 [D loss: 0.368895, acc.: 82.03%] [G loss: 1.646670]\n",
      "epoch:7 step:5726 [D loss: 0.419076, acc.: 81.25%] [G loss: 2.207780]\n",
      "epoch:7 step:5727 [D loss: 0.386329, acc.: 80.47%] [G loss: 3.885509]\n",
      "epoch:7 step:5728 [D loss: 0.873012, acc.: 68.75%] [G loss: 6.693419]\n",
      "epoch:7 step:5729 [D loss: 1.562348, acc.: 66.41%] [G loss: 3.819220]\n",
      "epoch:7 step:5730 [D loss: 0.339641, acc.: 85.16%] [G loss: 3.069868]\n",
      "epoch:7 step:5731 [D loss: 0.383668, acc.: 75.78%] [G loss: 2.305434]\n",
      "epoch:7 step:5732 [D loss: 0.352115, acc.: 83.59%] [G loss: 2.788519]\n",
      "epoch:7 step:5733 [D loss: 0.508477, acc.: 72.66%] [G loss: 2.144647]\n",
      "epoch:7 step:5734 [D loss: 0.325591, acc.: 92.19%] [G loss: 2.215696]\n",
      "epoch:7 step:5735 [D loss: 0.279754, acc.: 89.06%] [G loss: 2.320141]\n",
      "epoch:7 step:5736 [D loss: 0.313455, acc.: 88.28%] [G loss: 2.429078]\n",
      "epoch:7 step:5737 [D loss: 0.227011, acc.: 91.41%] [G loss: 3.187047]\n",
      "epoch:7 step:5738 [D loss: 0.147131, acc.: 95.31%] [G loss: 5.438221]\n",
      "epoch:7 step:5739 [D loss: 0.174411, acc.: 96.09%] [G loss: 5.353901]\n",
      "epoch:7 step:5740 [D loss: 0.191788, acc.: 96.09%] [G loss: 2.824848]\n",
      "epoch:7 step:5741 [D loss: 0.320167, acc.: 88.28%] [G loss: 3.461788]\n",
      "epoch:7 step:5742 [D loss: 0.383319, acc.: 82.03%] [G loss: 2.228269]\n",
      "epoch:7 step:5743 [D loss: 0.366621, acc.: 87.50%] [G loss: 2.341138]\n",
      "epoch:7 step:5744 [D loss: 0.287217, acc.: 85.94%] [G loss: 3.662861]\n",
      "epoch:7 step:5745 [D loss: 0.356282, acc.: 86.72%] [G loss: 2.815971]\n",
      "epoch:7 step:5746 [D loss: 0.247654, acc.: 87.50%] [G loss: 3.228042]\n",
      "epoch:7 step:5747 [D loss: 0.333965, acc.: 85.94%] [G loss: 2.147081]\n",
      "epoch:7 step:5748 [D loss: 0.316675, acc.: 87.50%] [G loss: 2.167186]\n",
      "epoch:7 step:5749 [D loss: 0.403528, acc.: 80.47%] [G loss: 2.535260]\n",
      "epoch:7 step:5750 [D loss: 0.262914, acc.: 91.41%] [G loss: 3.177587]\n",
      "epoch:7 step:5751 [D loss: 0.268218, acc.: 90.62%] [G loss: 4.754270]\n",
      "epoch:7 step:5752 [D loss: 0.376192, acc.: 80.47%] [G loss: 1.726251]\n",
      "epoch:7 step:5753 [D loss: 0.424810, acc.: 81.25%] [G loss: 2.320440]\n",
      "epoch:7 step:5754 [D loss: 0.389472, acc.: 81.25%] [G loss: 3.019536]\n",
      "epoch:7 step:5755 [D loss: 0.423102, acc.: 78.91%] [G loss: 4.883913]\n",
      "epoch:7 step:5756 [D loss: 0.362017, acc.: 80.47%] [G loss: 4.320290]\n",
      "epoch:7 step:5757 [D loss: 0.370006, acc.: 85.16%] [G loss: 2.389803]\n",
      "epoch:7 step:5758 [D loss: 0.282005, acc.: 91.41%] [G loss: 3.599300]\n",
      "epoch:7 step:5759 [D loss: 0.240163, acc.: 93.75%] [G loss: 3.779205]\n",
      "epoch:7 step:5760 [D loss: 0.303720, acc.: 89.06%] [G loss: 2.680066]\n",
      "epoch:7 step:5761 [D loss: 0.411951, acc.: 82.81%] [G loss: 2.353435]\n",
      "epoch:7 step:5762 [D loss: 0.301508, acc.: 85.94%] [G loss: 1.896219]\n",
      "epoch:7 step:5763 [D loss: 0.324717, acc.: 89.06%] [G loss: 3.683455]\n",
      "epoch:7 step:5764 [D loss: 0.253707, acc.: 94.53%] [G loss: 4.065512]\n",
      "epoch:7 step:5765 [D loss: 0.221570, acc.: 92.97%] [G loss: 3.536217]\n",
      "epoch:7 step:5766 [D loss: 0.259767, acc.: 91.41%] [G loss: 3.099390]\n",
      "epoch:7 step:5767 [D loss: 0.312863, acc.: 87.50%] [G loss: 2.278240]\n",
      "epoch:7 step:5768 [D loss: 0.362729, acc.: 75.78%] [G loss: 2.157936]\n",
      "epoch:7 step:5769 [D loss: 0.294491, acc.: 87.50%] [G loss: 2.452228]\n",
      "epoch:7 step:5770 [D loss: 0.239595, acc.: 90.62%] [G loss: 4.613480]\n",
      "epoch:7 step:5771 [D loss: 0.224891, acc.: 92.97%] [G loss: 4.875005]\n",
      "epoch:7 step:5772 [D loss: 0.323095, acc.: 84.38%] [G loss: 3.074216]\n",
      "epoch:7 step:5773 [D loss: 0.271182, acc.: 89.84%] [G loss: 5.383739]\n",
      "epoch:7 step:5774 [D loss: 0.356478, acc.: 84.38%] [G loss: 3.264296]\n",
      "epoch:7 step:5775 [D loss: 0.348890, acc.: 85.16%] [G loss: 2.544633]\n",
      "epoch:7 step:5776 [D loss: 0.364863, acc.: 83.59%] [G loss: 2.389102]\n",
      "epoch:7 step:5777 [D loss: 0.307604, acc.: 87.50%] [G loss: 3.314550]\n",
      "epoch:7 step:5778 [D loss: 0.326961, acc.: 88.28%] [G loss: 3.902511]\n",
      "epoch:7 step:5779 [D loss: 0.386518, acc.: 83.59%] [G loss: 2.207408]\n",
      "epoch:7 step:5780 [D loss: 0.381936, acc.: 82.81%] [G loss: 2.708743]\n",
      "epoch:7 step:5781 [D loss: 0.310922, acc.: 89.06%] [G loss: 3.205698]\n",
      "epoch:7 step:5782 [D loss: 0.463057, acc.: 78.12%] [G loss: 5.016487]\n",
      "epoch:7 step:5783 [D loss: 1.125594, acc.: 67.97%] [G loss: 6.268803]\n",
      "epoch:7 step:5784 [D loss: 2.097190, acc.: 58.59%] [G loss: 3.253730]\n",
      "epoch:7 step:5785 [D loss: 1.729591, acc.: 44.53%] [G loss: 2.869174]\n",
      "epoch:7 step:5786 [D loss: 0.295208, acc.: 85.94%] [G loss: 3.715643]\n",
      "epoch:7 step:5787 [D loss: 0.726757, acc.: 68.75%] [G loss: 2.853807]\n",
      "epoch:7 step:5788 [D loss: 0.335509, acc.: 87.50%] [G loss: 3.991157]\n",
      "epoch:7 step:5789 [D loss: 0.311239, acc.: 88.28%] [G loss: 2.922805]\n",
      "epoch:7 step:5790 [D loss: 0.503633, acc.: 75.78%] [G loss: 2.771133]\n",
      "epoch:7 step:5791 [D loss: 0.303179, acc.: 89.06%] [G loss: 3.313457]\n",
      "epoch:7 step:5792 [D loss: 0.358656, acc.: 86.72%] [G loss: 3.115632]\n",
      "epoch:7 step:5793 [D loss: 0.242641, acc.: 94.53%] [G loss: 2.876795]\n",
      "epoch:7 step:5794 [D loss: 0.318075, acc.: 90.62%] [G loss: 2.731194]\n",
      "epoch:7 step:5795 [D loss: 0.303266, acc.: 88.28%] [G loss: 2.483960]\n",
      "epoch:7 step:5796 [D loss: 0.279419, acc.: 89.06%] [G loss: 3.240963]\n",
      "epoch:7 step:5797 [D loss: 0.359068, acc.: 84.38%] [G loss: 4.324815]\n",
      "epoch:7 step:5798 [D loss: 0.252639, acc.: 89.84%] [G loss: 4.790127]\n",
      "epoch:7 step:5799 [D loss: 0.300254, acc.: 88.28%] [G loss: 3.272474]\n",
      "epoch:7 step:5800 [D loss: 0.251143, acc.: 88.28%] [G loss: 4.436995]\n",
      "##############\n",
      "[0.79890034 0.91039865 0.78221347 0.78581582 0.78109734 0.81687913\n",
      " 0.85055213 0.8210637  0.82205289 0.80601961]\n",
      "##########\n",
      "epoch:7 step:5801 [D loss: 0.369276, acc.: 81.25%] [G loss: 3.484032]\n",
      "epoch:7 step:5802 [D loss: 0.305707, acc.: 85.16%] [G loss: 3.880374]\n",
      "epoch:7 step:5803 [D loss: 0.325766, acc.: 87.50%] [G loss: 2.686766]\n",
      "epoch:7 step:5804 [D loss: 0.333638, acc.: 83.59%] [G loss: 2.542464]\n",
      "epoch:7 step:5805 [D loss: 0.356050, acc.: 84.38%] [G loss: 2.363722]\n",
      "epoch:7 step:5806 [D loss: 0.235952, acc.: 89.84%] [G loss: 3.249713]\n",
      "epoch:7 step:5807 [D loss: 0.304987, acc.: 89.84%] [G loss: 2.494736]\n",
      "epoch:7 step:5808 [D loss: 0.355992, acc.: 89.06%] [G loss: 3.050917]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:7 step:5809 [D loss: 0.210842, acc.: 90.62%] [G loss: 6.571204]\n",
      "epoch:7 step:5810 [D loss: 0.285047, acc.: 84.38%] [G loss: 3.015320]\n",
      "epoch:7 step:5811 [D loss: 0.402492, acc.: 80.47%] [G loss: 2.380305]\n",
      "epoch:7 step:5812 [D loss: 0.202477, acc.: 91.41%] [G loss: 2.492288]\n",
      "epoch:7 step:5813 [D loss: 0.354130, acc.: 83.59%] [G loss: 2.251784]\n",
      "epoch:7 step:5814 [D loss: 0.320865, acc.: 88.28%] [G loss: 2.473376]\n",
      "epoch:7 step:5815 [D loss: 0.256463, acc.: 91.41%] [G loss: 4.621351]\n",
      "epoch:7 step:5816 [D loss: 0.309103, acc.: 87.50%] [G loss: 2.040691]\n",
      "epoch:7 step:5817 [D loss: 0.306049, acc.: 85.94%] [G loss: 2.996405]\n",
      "epoch:7 step:5818 [D loss: 0.380442, acc.: 85.94%] [G loss: 2.767455]\n",
      "epoch:7 step:5819 [D loss: 0.445160, acc.: 83.59%] [G loss: 2.146500]\n",
      "epoch:7 step:5820 [D loss: 0.304429, acc.: 89.84%] [G loss: 2.596807]\n",
      "epoch:7 step:5821 [D loss: 0.233741, acc.: 89.06%] [G loss: 3.637236]\n",
      "epoch:7 step:5822 [D loss: 0.221970, acc.: 90.62%] [G loss: 4.108565]\n",
      "epoch:7 step:5823 [D loss: 0.355219, acc.: 83.59%] [G loss: 3.028062]\n",
      "epoch:7 step:5824 [D loss: 0.329454, acc.: 87.50%] [G loss: 2.059591]\n",
      "epoch:7 step:5825 [D loss: 0.305800, acc.: 88.28%] [G loss: 2.456329]\n",
      "epoch:7 step:5826 [D loss: 0.259653, acc.: 89.84%] [G loss: 3.269372]\n",
      "epoch:7 step:5827 [D loss: 0.336938, acc.: 82.03%] [G loss: 3.312198]\n",
      "epoch:7 step:5828 [D loss: 0.346019, acc.: 88.28%] [G loss: 2.109419]\n",
      "epoch:7 step:5829 [D loss: 0.421054, acc.: 83.59%] [G loss: 2.309327]\n",
      "epoch:7 step:5830 [D loss: 0.439643, acc.: 75.78%] [G loss: 3.091568]\n",
      "epoch:7 step:5831 [D loss: 0.349398, acc.: 83.59%] [G loss: 2.417443]\n",
      "epoch:7 step:5832 [D loss: 0.303055, acc.: 89.06%] [G loss: 2.875017]\n",
      "epoch:7 step:5833 [D loss: 0.385577, acc.: 82.03%] [G loss: 2.547985]\n",
      "epoch:7 step:5834 [D loss: 0.311113, acc.: 87.50%] [G loss: 4.805242]\n",
      "epoch:7 step:5835 [D loss: 0.331005, acc.: 82.03%] [G loss: 2.353716]\n",
      "epoch:7 step:5836 [D loss: 0.270679, acc.: 90.62%] [G loss: 3.390536]\n",
      "epoch:7 step:5837 [D loss: 0.226139, acc.: 89.06%] [G loss: 3.929239]\n",
      "epoch:7 step:5838 [D loss: 0.340715, acc.: 85.94%] [G loss: 2.801309]\n",
      "epoch:7 step:5839 [D loss: 0.337716, acc.: 85.94%] [G loss: 2.432237]\n",
      "epoch:7 step:5840 [D loss: 0.285858, acc.: 88.28%] [G loss: 3.334964]\n",
      "epoch:7 step:5841 [D loss: 0.270899, acc.: 89.84%] [G loss: 3.297644]\n",
      "epoch:7 step:5842 [D loss: 0.222438, acc.: 93.75%] [G loss: 2.912438]\n",
      "epoch:7 step:5843 [D loss: 0.533041, acc.: 72.66%] [G loss: 2.064625]\n",
      "epoch:7 step:5844 [D loss: 0.341422, acc.: 85.94%] [G loss: 2.099084]\n",
      "epoch:7 step:5845 [D loss: 0.381796, acc.: 79.69%] [G loss: 2.823206]\n",
      "epoch:7 step:5846 [D loss: 0.417899, acc.: 78.12%] [G loss: 2.416112]\n",
      "epoch:7 step:5847 [D loss: 0.441893, acc.: 78.12%] [G loss: 2.284555]\n",
      "epoch:7 step:5848 [D loss: 0.318747, acc.: 85.16%] [G loss: 2.060811]\n",
      "epoch:7 step:5849 [D loss: 0.442112, acc.: 78.91%] [G loss: 2.750091]\n",
      "epoch:7 step:5850 [D loss: 0.419155, acc.: 82.81%] [G loss: 2.921253]\n",
      "epoch:7 step:5851 [D loss: 0.382241, acc.: 81.25%] [G loss: 2.627589]\n",
      "epoch:7 step:5852 [D loss: 0.354979, acc.: 82.81%] [G loss: 3.182039]\n",
      "epoch:7 step:5853 [D loss: 0.511750, acc.: 72.66%] [G loss: 3.362625]\n",
      "epoch:7 step:5854 [D loss: 0.415849, acc.: 80.47%] [G loss: 3.250010]\n",
      "epoch:7 step:5855 [D loss: 0.269707, acc.: 86.72%] [G loss: 4.069867]\n",
      "epoch:7 step:5856 [D loss: 0.187969, acc.: 96.09%] [G loss: 6.618934]\n",
      "epoch:7 step:5857 [D loss: 0.272112, acc.: 91.41%] [G loss: 3.515892]\n",
      "epoch:7 step:5858 [D loss: 0.219669, acc.: 91.41%] [G loss: 3.003903]\n",
      "epoch:7 step:5859 [D loss: 0.423825, acc.: 75.00%] [G loss: 2.581815]\n",
      "epoch:7 step:5860 [D loss: 0.394389, acc.: 81.25%] [G loss: 3.077828]\n",
      "epoch:7 step:5861 [D loss: 0.304208, acc.: 87.50%] [G loss: 3.717769]\n",
      "epoch:7 step:5862 [D loss: 0.322257, acc.: 85.94%] [G loss: 3.144839]\n",
      "epoch:7 step:5863 [D loss: 0.293197, acc.: 88.28%] [G loss: 2.714129]\n",
      "epoch:7 step:5864 [D loss: 0.356570, acc.: 82.81%] [G loss: 2.283838]\n",
      "epoch:7 step:5865 [D loss: 0.369341, acc.: 82.03%] [G loss: 2.458643]\n",
      "epoch:7 step:5866 [D loss: 0.504120, acc.: 78.12%] [G loss: 2.814750]\n",
      "epoch:7 step:5867 [D loss: 0.540475, acc.: 78.91%] [G loss: 3.390625]\n",
      "epoch:7 step:5868 [D loss: 0.599848, acc.: 69.53%] [G loss: 3.239708]\n",
      "epoch:7 step:5869 [D loss: 0.457612, acc.: 81.25%] [G loss: 3.941016]\n",
      "epoch:7 step:5870 [D loss: 0.416255, acc.: 85.16%] [G loss: 2.854985]\n",
      "epoch:7 step:5871 [D loss: 0.341968, acc.: 85.16%] [G loss: 2.006961]\n",
      "epoch:7 step:5872 [D loss: 0.312486, acc.: 85.16%] [G loss: 2.665255]\n",
      "epoch:7 step:5873 [D loss: 0.396518, acc.: 84.38%] [G loss: 2.256367]\n",
      "epoch:7 step:5874 [D loss: 0.326672, acc.: 88.28%] [G loss: 2.398656]\n",
      "epoch:7 step:5875 [D loss: 0.272493, acc.: 86.72%] [G loss: 3.325623]\n",
      "epoch:7 step:5876 [D loss: 0.220293, acc.: 90.62%] [G loss: 4.202234]\n",
      "epoch:7 step:5877 [D loss: 0.231372, acc.: 90.62%] [G loss: 3.388214]\n",
      "epoch:7 step:5878 [D loss: 0.263024, acc.: 90.62%] [G loss: 2.990032]\n",
      "epoch:7 step:5879 [D loss: 0.234547, acc.: 87.50%] [G loss: 3.601703]\n",
      "epoch:7 step:5880 [D loss: 0.286997, acc.: 85.16%] [G loss: 4.174868]\n",
      "epoch:7 step:5881 [D loss: 0.372440, acc.: 83.59%] [G loss: 3.864973]\n",
      "epoch:7 step:5882 [D loss: 0.303106, acc.: 86.72%] [G loss: 4.774947]\n",
      "epoch:7 step:5883 [D loss: 0.284345, acc.: 82.03%] [G loss: 5.212770]\n",
      "epoch:7 step:5884 [D loss: 0.324034, acc.: 88.28%] [G loss: 2.681099]\n",
      "epoch:7 step:5885 [D loss: 0.313879, acc.: 86.72%] [G loss: 2.008820]\n",
      "epoch:7 step:5886 [D loss: 0.483680, acc.: 75.78%] [G loss: 1.994055]\n",
      "epoch:7 step:5887 [D loss: 0.222731, acc.: 92.97%] [G loss: 3.445347]\n",
      "epoch:7 step:5888 [D loss: 0.281502, acc.: 89.06%] [G loss: 3.509264]\n",
      "epoch:7 step:5889 [D loss: 0.329318, acc.: 85.94%] [G loss: 2.975994]\n",
      "epoch:7 step:5890 [D loss: 0.360712, acc.: 86.72%] [G loss: 1.845139]\n",
      "epoch:7 step:5891 [D loss: 0.275349, acc.: 92.19%] [G loss: 2.435158]\n",
      "epoch:7 step:5892 [D loss: 0.381176, acc.: 80.47%] [G loss: 3.926476]\n",
      "epoch:7 step:5893 [D loss: 0.517965, acc.: 74.22%] [G loss: 3.037751]\n",
      "epoch:7 step:5894 [D loss: 0.328720, acc.: 89.84%] [G loss: 2.351391]\n",
      "epoch:7 step:5895 [D loss: 0.442253, acc.: 80.47%] [G loss: 3.490300]\n",
      "epoch:7 step:5896 [D loss: 0.770588, acc.: 60.94%] [G loss: 4.748347]\n",
      "epoch:7 step:5897 [D loss: 1.387426, acc.: 58.59%] [G loss: 5.318703]\n",
      "epoch:7 step:5898 [D loss: 0.947917, acc.: 66.41%] [G loss: 3.347331]\n",
      "epoch:7 step:5899 [D loss: 0.464833, acc.: 77.34%] [G loss: 3.983672]\n",
      "epoch:7 step:5900 [D loss: 0.362479, acc.: 83.59%] [G loss: 2.834232]\n",
      "epoch:7 step:5901 [D loss: 0.642997, acc.: 75.00%] [G loss: 2.525549]\n",
      "epoch:7 step:5902 [D loss: 0.448550, acc.: 82.81%] [G loss: 3.501281]\n",
      "epoch:7 step:5903 [D loss: 0.380500, acc.: 83.59%] [G loss: 3.823334]\n",
      "epoch:7 step:5904 [D loss: 0.455649, acc.: 79.69%] [G loss: 2.278416]\n",
      "epoch:7 step:5905 [D loss: 0.366329, acc.: 86.72%] [G loss: 2.053308]\n",
      "epoch:7 step:5906 [D loss: 0.341727, acc.: 86.72%] [G loss: 2.489125]\n",
      "epoch:7 step:5907 [D loss: 0.320482, acc.: 90.62%] [G loss: 2.100189]\n",
      "epoch:7 step:5908 [D loss: 0.268935, acc.: 89.84%] [G loss: 2.509230]\n",
      "epoch:7 step:5909 [D loss: 0.270669, acc.: 91.41%] [G loss: 2.740302]\n",
      "epoch:7 step:5910 [D loss: 0.289432, acc.: 90.62%] [G loss: 3.199697]\n",
      "epoch:7 step:5911 [D loss: 0.237426, acc.: 91.41%] [G loss: 2.248217]\n",
      "epoch:7 step:5912 [D loss: 0.395417, acc.: 84.38%] [G loss: 2.141964]\n",
      "epoch:7 step:5913 [D loss: 0.328882, acc.: 85.94%] [G loss: 2.608462]\n",
      "epoch:7 step:5914 [D loss: 0.357886, acc.: 85.94%] [G loss: 3.191864]\n",
      "epoch:7 step:5915 [D loss: 0.319315, acc.: 86.72%] [G loss: 3.026162]\n",
      "epoch:7 step:5916 [D loss: 0.273108, acc.: 92.97%] [G loss: 3.215905]\n",
      "epoch:7 step:5917 [D loss: 0.198941, acc.: 93.75%] [G loss: 4.803564]\n",
      "epoch:7 step:5918 [D loss: 0.292196, acc.: 86.72%] [G loss: 3.709890]\n",
      "epoch:7 step:5919 [D loss: 0.382694, acc.: 80.47%] [G loss: 1.772740]\n",
      "epoch:7 step:5920 [D loss: 0.283540, acc.: 89.06%] [G loss: 3.289888]\n",
      "epoch:7 step:5921 [D loss: 0.233020, acc.: 92.19%] [G loss: 3.009498]\n",
      "epoch:7 step:5922 [D loss: 0.358231, acc.: 80.47%] [G loss: 2.454367]\n",
      "epoch:7 step:5923 [D loss: 0.356002, acc.: 84.38%] [G loss: 2.488083]\n",
      "epoch:7 step:5924 [D loss: 0.339546, acc.: 85.94%] [G loss: 2.243540]\n",
      "epoch:7 step:5925 [D loss: 0.328628, acc.: 91.41%] [G loss: 2.218496]\n",
      "epoch:7 step:5926 [D loss: 0.242971, acc.: 91.41%] [G loss: 2.742838]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:7 step:5927 [D loss: 0.350751, acc.: 88.28%] [G loss: 2.561843]\n",
      "epoch:7 step:5928 [D loss: 0.253263, acc.: 94.53%] [G loss: 2.331722]\n",
      "epoch:7 step:5929 [D loss: 0.266077, acc.: 91.41%] [G loss: 2.590151]\n",
      "epoch:7 step:5930 [D loss: 0.281143, acc.: 87.50%] [G loss: 2.996086]\n",
      "epoch:7 step:5931 [D loss: 0.344618, acc.: 83.59%] [G loss: 2.308873]\n",
      "epoch:7 step:5932 [D loss: 0.388731, acc.: 80.47%] [G loss: 2.481909]\n",
      "epoch:7 step:5933 [D loss: 0.326267, acc.: 88.28%] [G loss: 2.094646]\n",
      "epoch:7 step:5934 [D loss: 0.439391, acc.: 77.34%] [G loss: 2.273429]\n",
      "epoch:7 step:5935 [D loss: 0.312957, acc.: 88.28%] [G loss: 2.413321]\n",
      "epoch:7 step:5936 [D loss: 0.324830, acc.: 82.81%] [G loss: 2.079467]\n",
      "epoch:7 step:5937 [D loss: 0.370389, acc.: 82.81%] [G loss: 2.023021]\n",
      "epoch:7 step:5938 [D loss: 0.347530, acc.: 88.28%] [G loss: 3.352766]\n",
      "epoch:7 step:5939 [D loss: 0.372032, acc.: 81.25%] [G loss: 2.307333]\n",
      "epoch:7 step:5940 [D loss: 0.338002, acc.: 87.50%] [G loss: 3.623257]\n",
      "epoch:7 step:5941 [D loss: 0.344468, acc.: 85.94%] [G loss: 3.793132]\n",
      "epoch:7 step:5942 [D loss: 0.377201, acc.: 86.72%] [G loss: 2.831987]\n",
      "epoch:7 step:5943 [D loss: 0.238948, acc.: 89.84%] [G loss: 5.827284]\n",
      "epoch:7 step:5944 [D loss: 0.321219, acc.: 85.16%] [G loss: 3.013105]\n",
      "epoch:7 step:5945 [D loss: 0.283724, acc.: 91.41%] [G loss: 2.710147]\n",
      "epoch:7 step:5946 [D loss: 0.305226, acc.: 92.19%] [G loss: 2.820180]\n",
      "epoch:7 step:5947 [D loss: 0.313029, acc.: 89.06%] [G loss: 2.983975]\n",
      "epoch:7 step:5948 [D loss: 0.247688, acc.: 89.84%] [G loss: 3.537560]\n",
      "epoch:7 step:5949 [D loss: 0.260854, acc.: 89.06%] [G loss: 3.263916]\n",
      "epoch:7 step:5950 [D loss: 0.307692, acc.: 86.72%] [G loss: 3.049351]\n",
      "epoch:7 step:5951 [D loss: 0.375368, acc.: 84.38%] [G loss: 2.108333]\n",
      "epoch:7 step:5952 [D loss: 0.342146, acc.: 83.59%] [G loss: 2.899672]\n",
      "epoch:7 step:5953 [D loss: 0.299442, acc.: 89.84%] [G loss: 2.730982]\n",
      "epoch:7 step:5954 [D loss: 0.338497, acc.: 85.16%] [G loss: 2.795125]\n",
      "epoch:7 step:5955 [D loss: 0.453834, acc.: 74.22%] [G loss: 3.229017]\n",
      "epoch:7 step:5956 [D loss: 0.588495, acc.: 74.22%] [G loss: 4.410194]\n",
      "epoch:7 step:5957 [D loss: 0.641966, acc.: 75.78%] [G loss: 2.984365]\n",
      "epoch:7 step:5958 [D loss: 0.633726, acc.: 71.88%] [G loss: 1.620963]\n",
      "epoch:7 step:5959 [D loss: 0.388345, acc.: 82.03%] [G loss: 2.966557]\n",
      "epoch:7 step:5960 [D loss: 0.232299, acc.: 92.19%] [G loss: 3.875033]\n",
      "epoch:7 step:5961 [D loss: 0.439533, acc.: 80.47%] [G loss: 2.395016]\n",
      "epoch:7 step:5962 [D loss: 0.292219, acc.: 88.28%] [G loss: 2.660751]\n",
      "epoch:7 step:5963 [D loss: 0.276309, acc.: 85.94%] [G loss: 2.942881]\n",
      "epoch:7 step:5964 [D loss: 0.287557, acc.: 89.06%] [G loss: 3.478854]\n",
      "epoch:7 step:5965 [D loss: 0.216149, acc.: 92.97%] [G loss: 4.226361]\n",
      "epoch:7 step:5966 [D loss: 0.269407, acc.: 90.62%] [G loss: 2.627007]\n",
      "epoch:7 step:5967 [D loss: 0.353049, acc.: 84.38%] [G loss: 1.886101]\n",
      "epoch:7 step:5968 [D loss: 0.246006, acc.: 90.62%] [G loss: 3.885080]\n",
      "epoch:7 step:5969 [D loss: 0.380077, acc.: 85.94%] [G loss: 2.300370]\n",
      "epoch:7 step:5970 [D loss: 0.254012, acc.: 93.75%] [G loss: 2.028969]\n",
      "epoch:7 step:5971 [D loss: 0.363009, acc.: 81.25%] [G loss: 2.444347]\n",
      "epoch:7 step:5972 [D loss: 0.291343, acc.: 85.94%] [G loss: 4.682066]\n",
      "epoch:7 step:5973 [D loss: 0.289177, acc.: 85.94%] [G loss: 4.302801]\n",
      "epoch:7 step:5974 [D loss: 0.323633, acc.: 82.81%] [G loss: 3.747993]\n",
      "epoch:7 step:5975 [D loss: 0.416465, acc.: 80.47%] [G loss: 2.632758]\n",
      "epoch:7 step:5976 [D loss: 0.379741, acc.: 82.03%] [G loss: 2.902528]\n",
      "epoch:7 step:5977 [D loss: 0.296731, acc.: 88.28%] [G loss: 3.229352]\n",
      "epoch:7 step:5978 [D loss: 0.253981, acc.: 89.06%] [G loss: 4.618914]\n",
      "epoch:7 step:5979 [D loss: 0.353642, acc.: 86.72%] [G loss: 2.672799]\n",
      "epoch:7 step:5980 [D loss: 0.395328, acc.: 81.25%] [G loss: 2.736724]\n",
      "epoch:7 step:5981 [D loss: 0.258227, acc.: 91.41%] [G loss: 3.062429]\n",
      "epoch:7 step:5982 [D loss: 0.197254, acc.: 93.75%] [G loss: 3.452781]\n",
      "epoch:7 step:5983 [D loss: 0.417431, acc.: 85.94%] [G loss: 2.205165]\n",
      "epoch:7 step:5984 [D loss: 0.299346, acc.: 92.19%] [G loss: 2.851286]\n",
      "epoch:7 step:5985 [D loss: 0.201860, acc.: 92.19%] [G loss: 4.129062]\n",
      "epoch:7 step:5986 [D loss: 0.332877, acc.: 86.72%] [G loss: 2.964597]\n",
      "epoch:7 step:5987 [D loss: 0.305363, acc.: 88.28%] [G loss: 3.239957]\n",
      "epoch:7 step:5988 [D loss: 0.283603, acc.: 90.62%] [G loss: 3.678206]\n",
      "epoch:7 step:5989 [D loss: 0.326097, acc.: 85.94%] [G loss: 3.373135]\n",
      "epoch:7 step:5990 [D loss: 0.280999, acc.: 91.41%] [G loss: 2.963496]\n",
      "epoch:7 step:5991 [D loss: 0.270911, acc.: 91.41%] [G loss: 3.651181]\n",
      "epoch:7 step:5992 [D loss: 0.272997, acc.: 89.06%] [G loss: 2.861330]\n",
      "epoch:7 step:5993 [D loss: 0.384404, acc.: 82.03%] [G loss: 2.446479]\n",
      "epoch:7 step:5994 [D loss: 0.360868, acc.: 86.72%] [G loss: 1.934666]\n",
      "epoch:7 step:5995 [D loss: 0.375526, acc.: 86.72%] [G loss: 2.686323]\n",
      "epoch:7 step:5996 [D loss: 0.328837, acc.: 85.94%] [G loss: 3.761935]\n",
      "epoch:7 step:5997 [D loss: 0.401621, acc.: 85.16%] [G loss: 5.020817]\n",
      "epoch:7 step:5998 [D loss: 0.625475, acc.: 71.09%] [G loss: 3.566758]\n",
      "epoch:7 step:5999 [D loss: 0.708394, acc.: 71.09%] [G loss: 5.683146]\n",
      "epoch:7 step:6000 [D loss: 0.965410, acc.: 71.09%] [G loss: 3.807779]\n",
      "##############\n",
      "[0.80064281 0.8940984  0.77986922 0.79297084 0.77305871 0.79949227\n",
      " 0.86269837 0.81657516 0.80132478 0.80706377]\n",
      "##########\n",
      "epoch:7 step:6001 [D loss: 0.445336, acc.: 78.12%] [G loss: 3.296421]\n",
      "epoch:7 step:6002 [D loss: 0.512449, acc.: 71.88%] [G loss: 2.458829]\n",
      "epoch:7 step:6003 [D loss: 0.395915, acc.: 77.34%] [G loss: 2.923154]\n",
      "epoch:7 step:6004 [D loss: 0.381001, acc.: 87.50%] [G loss: 2.444283]\n",
      "epoch:7 step:6005 [D loss: 0.320819, acc.: 88.28%] [G loss: 2.702034]\n",
      "epoch:7 step:6006 [D loss: 0.399267, acc.: 85.16%] [G loss: 2.128402]\n",
      "epoch:7 step:6007 [D loss: 0.430489, acc.: 74.22%] [G loss: 2.273877]\n",
      "epoch:7 step:6008 [D loss: 0.368870, acc.: 86.72%] [G loss: 3.619570]\n",
      "epoch:7 step:6009 [D loss: 0.369258, acc.: 84.38%] [G loss: 2.418625]\n",
      "epoch:7 step:6010 [D loss: 0.382928, acc.: 86.72%] [G loss: 3.184744]\n",
      "epoch:7 step:6011 [D loss: 0.278848, acc.: 89.84%] [G loss: 7.271048]\n",
      "epoch:7 step:6012 [D loss: 0.340931, acc.: 85.16%] [G loss: 2.449080]\n",
      "epoch:7 step:6013 [D loss: 0.234476, acc.: 87.50%] [G loss: 4.295217]\n",
      "epoch:7 step:6014 [D loss: 0.387004, acc.: 78.12%] [G loss: 2.235348]\n",
      "epoch:7 step:6015 [D loss: 0.302000, acc.: 89.06%] [G loss: 2.986958]\n",
      "epoch:7 step:6016 [D loss: 0.334352, acc.: 88.28%] [G loss: 2.161092]\n",
      "epoch:7 step:6017 [D loss: 0.300936, acc.: 89.06%] [G loss: 2.551291]\n",
      "epoch:7 step:6018 [D loss: 0.300045, acc.: 86.72%] [G loss: 2.502468]\n",
      "epoch:7 step:6019 [D loss: 0.408888, acc.: 84.38%] [G loss: 2.000841]\n",
      "epoch:7 step:6020 [D loss: 0.537568, acc.: 77.34%] [G loss: 2.301946]\n",
      "epoch:7 step:6021 [D loss: 0.334005, acc.: 87.50%] [G loss: 2.361260]\n",
      "epoch:7 step:6022 [D loss: 0.346963, acc.: 83.59%] [G loss: 3.305578]\n",
      "epoch:7 step:6023 [D loss: 0.330023, acc.: 86.72%] [G loss: 3.500030]\n",
      "epoch:7 step:6024 [D loss: 0.322535, acc.: 88.28%] [G loss: 3.624084]\n",
      "epoch:7 step:6025 [D loss: 0.298948, acc.: 88.28%] [G loss: 3.371503]\n",
      "epoch:7 step:6026 [D loss: 0.311525, acc.: 85.94%] [G loss: 2.274303]\n",
      "epoch:7 step:6027 [D loss: 0.197498, acc.: 93.75%] [G loss: 4.620922]\n",
      "epoch:7 step:6028 [D loss: 0.305944, acc.: 88.28%] [G loss: 2.940742]\n",
      "epoch:7 step:6029 [D loss: 0.422583, acc.: 84.38%] [G loss: 2.519574]\n",
      "epoch:7 step:6030 [D loss: 0.220453, acc.: 93.75%] [G loss: 6.280504]\n",
      "epoch:7 step:6031 [D loss: 0.207779, acc.: 92.97%] [G loss: 3.149263]\n",
      "epoch:7 step:6032 [D loss: 0.315527, acc.: 85.94%] [G loss: 2.334777]\n",
      "epoch:7 step:6033 [D loss: 0.343518, acc.: 91.41%] [G loss: 2.105378]\n",
      "epoch:7 step:6034 [D loss: 0.399472, acc.: 80.47%] [G loss: 2.111117]\n",
      "epoch:7 step:6035 [D loss: 0.331613, acc.: 88.28%] [G loss: 2.905670]\n",
      "epoch:7 step:6036 [D loss: 0.276930, acc.: 87.50%] [G loss: 2.863113]\n",
      "epoch:7 step:6037 [D loss: 0.359227, acc.: 88.28%] [G loss: 2.685087]\n",
      "epoch:7 step:6038 [D loss: 0.224272, acc.: 94.53%] [G loss: 3.600631]\n",
      "epoch:7 step:6039 [D loss: 0.265163, acc.: 89.84%] [G loss: 3.140869]\n",
      "epoch:7 step:6040 [D loss: 0.320198, acc.: 89.84%] [G loss: 1.887584]\n",
      "epoch:7 step:6041 [D loss: 0.307533, acc.: 85.16%] [G loss: 2.401006]\n",
      "epoch:7 step:6042 [D loss: 0.373552, acc.: 81.25%] [G loss: 2.248003]\n",
      "epoch:7 step:6043 [D loss: 0.365651, acc.: 90.62%] [G loss: 2.005719]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:7 step:6044 [D loss: 0.356924, acc.: 88.28%] [G loss: 2.206112]\n",
      "epoch:7 step:6045 [D loss: 0.279577, acc.: 89.06%] [G loss: 2.975029]\n",
      "epoch:7 step:6046 [D loss: 0.259248, acc.: 92.97%] [G loss: 3.809505]\n",
      "epoch:7 step:6047 [D loss: 0.311313, acc.: 88.28%] [G loss: 3.759493]\n",
      "epoch:7 step:6048 [D loss: 0.364899, acc.: 85.16%] [G loss: 3.566583]\n",
      "epoch:7 step:6049 [D loss: 0.345554, acc.: 88.28%] [G loss: 2.962299]\n",
      "epoch:7 step:6050 [D loss: 0.333389, acc.: 82.81%] [G loss: 3.769689]\n",
      "epoch:7 step:6051 [D loss: 0.322845, acc.: 85.94%] [G loss: 4.008302]\n",
      "epoch:7 step:6052 [D loss: 0.333598, acc.: 87.50%] [G loss: 4.453472]\n",
      "epoch:7 step:6053 [D loss: 0.270481, acc.: 89.84%] [G loss: 2.562011]\n",
      "epoch:7 step:6054 [D loss: 0.229107, acc.: 92.97%] [G loss: 2.415330]\n",
      "epoch:7 step:6055 [D loss: 0.227628, acc.: 92.19%] [G loss: 4.750875]\n",
      "epoch:7 step:6056 [D loss: 0.239020, acc.: 95.31%] [G loss: 4.475684]\n",
      "epoch:7 step:6057 [D loss: 0.266248, acc.: 88.28%] [G loss: 3.804073]\n",
      "epoch:7 step:6058 [D loss: 0.446245, acc.: 75.78%] [G loss: 2.247340]\n",
      "epoch:7 step:6059 [D loss: 0.294462, acc.: 89.84%] [G loss: 3.524984]\n",
      "epoch:7 step:6060 [D loss: 0.170024, acc.: 94.53%] [G loss: 3.777933]\n",
      "epoch:7 step:6061 [D loss: 0.275008, acc.: 92.19%] [G loss: 3.836626]\n",
      "epoch:7 step:6062 [D loss: 0.241665, acc.: 89.84%] [G loss: 5.691003]\n",
      "epoch:7 step:6063 [D loss: 0.261977, acc.: 92.19%] [G loss: 3.058237]\n",
      "epoch:7 step:6064 [D loss: 0.481540, acc.: 73.44%] [G loss: 2.382734]\n",
      "epoch:7 step:6065 [D loss: 0.297690, acc.: 89.84%] [G loss: 2.630270]\n",
      "epoch:7 step:6066 [D loss: 0.299363, acc.: 89.06%] [G loss: 4.919020]\n",
      "epoch:7 step:6067 [D loss: 0.300940, acc.: 88.28%] [G loss: 3.155319]\n",
      "epoch:7 step:6068 [D loss: 0.260685, acc.: 89.84%] [G loss: 2.852326]\n",
      "epoch:7 step:6069 [D loss: 0.260738, acc.: 94.53%] [G loss: 2.658355]\n",
      "epoch:7 step:6070 [D loss: 0.263860, acc.: 91.41%] [G loss: 3.267665]\n",
      "epoch:7 step:6071 [D loss: 0.336490, acc.: 82.81%] [G loss: 2.323246]\n",
      "epoch:7 step:6072 [D loss: 0.320396, acc.: 86.72%] [G loss: 2.251022]\n",
      "epoch:7 step:6073 [D loss: 0.358834, acc.: 83.59%] [G loss: 3.803306]\n",
      "epoch:7 step:6074 [D loss: 0.275472, acc.: 91.41%] [G loss: 3.816834]\n",
      "epoch:7 step:6075 [D loss: 0.347300, acc.: 83.59%] [G loss: 2.902729]\n",
      "epoch:7 step:6076 [D loss: 0.412595, acc.: 80.47%] [G loss: 2.630741]\n",
      "epoch:7 step:6077 [D loss: 0.365482, acc.: 85.16%] [G loss: 2.888482]\n",
      "epoch:7 step:6078 [D loss: 0.349187, acc.: 88.28%] [G loss: 3.432522]\n",
      "epoch:7 step:6079 [D loss: 0.356576, acc.: 85.16%] [G loss: 2.404320]\n",
      "epoch:7 step:6080 [D loss: 0.368341, acc.: 78.91%] [G loss: 3.786649]\n",
      "epoch:7 step:6081 [D loss: 0.253559, acc.: 89.84%] [G loss: 3.538055]\n",
      "epoch:7 step:6082 [D loss: 0.291213, acc.: 85.94%] [G loss: 4.064074]\n",
      "epoch:7 step:6083 [D loss: 0.309757, acc.: 86.72%] [G loss: 3.662824]\n",
      "epoch:7 step:6084 [D loss: 0.449980, acc.: 75.00%] [G loss: 2.814680]\n",
      "epoch:7 step:6085 [D loss: 0.488280, acc.: 76.56%] [G loss: 3.231458]\n",
      "epoch:7 step:6086 [D loss: 0.358478, acc.: 82.81%] [G loss: 3.205228]\n",
      "epoch:7 step:6087 [D loss: 0.260322, acc.: 90.62%] [G loss: 3.067869]\n",
      "epoch:7 step:6088 [D loss: 0.326299, acc.: 89.06%] [G loss: 2.455992]\n",
      "epoch:7 step:6089 [D loss: 0.183544, acc.: 92.97%] [G loss: 3.411283]\n",
      "epoch:7 step:6090 [D loss: 0.333813, acc.: 83.59%] [G loss: 2.946425]\n",
      "epoch:7 step:6091 [D loss: 0.313455, acc.: 92.19%] [G loss: 2.042593]\n",
      "epoch:7 step:6092 [D loss: 0.288778, acc.: 87.50%] [G loss: 2.373001]\n",
      "epoch:7 step:6093 [D loss: 0.296707, acc.: 92.19%] [G loss: 3.086565]\n",
      "epoch:7 step:6094 [D loss: 0.234038, acc.: 94.53%] [G loss: 3.286550]\n",
      "epoch:7 step:6095 [D loss: 0.385982, acc.: 79.69%] [G loss: 2.405947]\n",
      "epoch:7 step:6096 [D loss: 0.292014, acc.: 89.06%] [G loss: 2.654557]\n",
      "epoch:7 step:6097 [D loss: 0.287277, acc.: 92.19%] [G loss: 2.985894]\n",
      "epoch:7 step:6098 [D loss: 0.267197, acc.: 90.62%] [G loss: 2.880394]\n",
      "epoch:7 step:6099 [D loss: 0.238808, acc.: 92.19%] [G loss: 2.404124]\n",
      "epoch:7 step:6100 [D loss: 0.353510, acc.: 89.06%] [G loss: 2.406687]\n",
      "epoch:7 step:6101 [D loss: 0.386195, acc.: 82.81%] [G loss: 3.227277]\n",
      "epoch:7 step:6102 [D loss: 0.373571, acc.: 82.03%] [G loss: 2.463153]\n",
      "epoch:7 step:6103 [D loss: 0.281888, acc.: 90.62%] [G loss: 3.591044]\n",
      "epoch:7 step:6104 [D loss: 0.271650, acc.: 90.62%] [G loss: 2.777032]\n",
      "epoch:7 step:6105 [D loss: 0.393910, acc.: 82.81%] [G loss: 3.733774]\n",
      "epoch:7 step:6106 [D loss: 0.449536, acc.: 78.91%] [G loss: 4.910591]\n",
      "epoch:7 step:6107 [D loss: 0.632520, acc.: 71.09%] [G loss: 2.304452]\n",
      "epoch:7 step:6108 [D loss: 0.230161, acc.: 89.06%] [G loss: 4.749781]\n",
      "epoch:7 step:6109 [D loss: 0.332722, acc.: 82.81%] [G loss: 5.028606]\n",
      "epoch:7 step:6110 [D loss: 0.577486, acc.: 67.97%] [G loss: 2.072438]\n",
      "epoch:7 step:6111 [D loss: 0.387652, acc.: 82.03%] [G loss: 2.755100]\n",
      "epoch:7 step:6112 [D loss: 0.355975, acc.: 78.91%] [G loss: 2.514480]\n",
      "epoch:7 step:6113 [D loss: 0.475869, acc.: 77.34%] [G loss: 2.607422]\n",
      "epoch:7 step:6114 [D loss: 0.392385, acc.: 83.59%] [G loss: 2.573208]\n",
      "epoch:7 step:6115 [D loss: 0.264745, acc.: 92.19%] [G loss: 2.790686]\n",
      "epoch:7 step:6116 [D loss: 0.411376, acc.: 78.91%] [G loss: 2.485650]\n",
      "epoch:7 step:6117 [D loss: 0.409833, acc.: 84.38%] [G loss: 2.932748]\n",
      "epoch:7 step:6118 [D loss: 0.411195, acc.: 80.47%] [G loss: 2.423567]\n",
      "epoch:7 step:6119 [D loss: 0.286640, acc.: 85.16%] [G loss: 2.442047]\n",
      "epoch:7 step:6120 [D loss: 0.366881, acc.: 84.38%] [G loss: 2.013592]\n",
      "epoch:7 step:6121 [D loss: 0.397242, acc.: 81.25%] [G loss: 2.163031]\n",
      "epoch:7 step:6122 [D loss: 0.254309, acc.: 90.62%] [G loss: 2.388988]\n",
      "epoch:7 step:6123 [D loss: 0.344735, acc.: 83.59%] [G loss: 4.236166]\n",
      "epoch:7 step:6124 [D loss: 0.439023, acc.: 78.91%] [G loss: 3.761248]\n",
      "epoch:7 step:6125 [D loss: 0.579305, acc.: 71.09%] [G loss: 3.041434]\n",
      "epoch:7 step:6126 [D loss: 0.291831, acc.: 90.62%] [G loss: 3.445569]\n",
      "epoch:7 step:6127 [D loss: 0.408720, acc.: 87.50%] [G loss: 6.804118]\n",
      "epoch:7 step:6128 [D loss: 0.692450, acc.: 71.88%] [G loss: 2.591439]\n",
      "epoch:7 step:6129 [D loss: 0.273941, acc.: 88.28%] [G loss: 3.023978]\n",
      "epoch:7 step:6130 [D loss: 0.332438, acc.: 86.72%] [G loss: 3.522150]\n",
      "epoch:7 step:6131 [D loss: 0.283004, acc.: 89.06%] [G loss: 2.397954]\n",
      "epoch:7 step:6132 [D loss: 0.261595, acc.: 90.62%] [G loss: 3.087183]\n",
      "epoch:7 step:6133 [D loss: 0.289770, acc.: 92.19%] [G loss: 2.165108]\n",
      "epoch:7 step:6134 [D loss: 0.268577, acc.: 90.62%] [G loss: 2.591078]\n",
      "epoch:7 step:6135 [D loss: 0.252848, acc.: 91.41%] [G loss: 2.737821]\n",
      "epoch:7 step:6136 [D loss: 0.323197, acc.: 89.84%] [G loss: 2.160632]\n",
      "epoch:7 step:6137 [D loss: 0.299238, acc.: 87.50%] [G loss: 3.598493]\n",
      "epoch:7 step:6138 [D loss: 0.229972, acc.: 89.84%] [G loss: 3.147279]\n",
      "epoch:7 step:6139 [D loss: 0.215771, acc.: 92.97%] [G loss: 3.404306]\n",
      "epoch:7 step:6140 [D loss: 0.258654, acc.: 91.41%] [G loss: 2.143277]\n",
      "epoch:7 step:6141 [D loss: 0.320211, acc.: 82.03%] [G loss: 2.283209]\n",
      "epoch:7 step:6142 [D loss: 0.206875, acc.: 92.19%] [G loss: 3.794616]\n",
      "epoch:7 step:6143 [D loss: 0.296926, acc.: 89.84%] [G loss: 2.648725]\n",
      "epoch:7 step:6144 [D loss: 0.269605, acc.: 89.84%] [G loss: 2.847853]\n",
      "epoch:7 step:6145 [D loss: 0.209429, acc.: 90.62%] [G loss: 4.296920]\n",
      "epoch:7 step:6146 [D loss: 0.210946, acc.: 92.97%] [G loss: 3.559969]\n",
      "epoch:7 step:6147 [D loss: 0.304654, acc.: 87.50%] [G loss: 3.962582]\n",
      "epoch:7 step:6148 [D loss: 0.315592, acc.: 85.16%] [G loss: 3.309392]\n",
      "epoch:7 step:6149 [D loss: 0.259073, acc.: 89.84%] [G loss: 4.807590]\n",
      "epoch:7 step:6150 [D loss: 0.428447, acc.: 80.47%] [G loss: 2.977149]\n",
      "epoch:7 step:6151 [D loss: 0.314289, acc.: 86.72%] [G loss: 3.517087]\n",
      "epoch:7 step:6152 [D loss: 0.207139, acc.: 92.97%] [G loss: 3.623791]\n",
      "epoch:7 step:6153 [D loss: 0.245385, acc.: 92.97%] [G loss: 3.643483]\n",
      "epoch:7 step:6154 [D loss: 0.148635, acc.: 94.53%] [G loss: 6.459000]\n",
      "epoch:7 step:6155 [D loss: 0.275533, acc.: 89.84%] [G loss: 3.680042]\n",
      "epoch:7 step:6156 [D loss: 0.299686, acc.: 89.84%] [G loss: 2.813077]\n",
      "epoch:7 step:6157 [D loss: 0.338821, acc.: 85.94%] [G loss: 4.653249]\n",
      "epoch:7 step:6158 [D loss: 0.213361, acc.: 94.53%] [G loss: 3.529121]\n",
      "epoch:7 step:6159 [D loss: 0.265844, acc.: 91.41%] [G loss: 3.347069]\n",
      "epoch:7 step:6160 [D loss: 0.264767, acc.: 85.94%] [G loss: 4.613161]\n",
      "epoch:7 step:6161 [D loss: 0.207861, acc.: 95.31%] [G loss: 3.434756]\n",
      "epoch:7 step:6162 [D loss: 0.320131, acc.: 85.16%] [G loss: 2.473709]\n",
      "epoch:7 step:6163 [D loss: 0.256281, acc.: 92.97%] [G loss: 2.110342]\n",
      "epoch:7 step:6164 [D loss: 0.294591, acc.: 87.50%] [G loss: 3.633909]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:7 step:6165 [D loss: 0.292609, acc.: 86.72%] [G loss: 3.527141]\n",
      "epoch:7 step:6166 [D loss: 0.256448, acc.: 91.41%] [G loss: 3.809488]\n",
      "epoch:7 step:6167 [D loss: 0.313235, acc.: 86.72%] [G loss: 2.669199]\n",
      "epoch:7 step:6168 [D loss: 0.208189, acc.: 94.53%] [G loss: 3.188234]\n",
      "epoch:7 step:6169 [D loss: 0.285937, acc.: 86.72%] [G loss: 2.699188]\n",
      "epoch:7 step:6170 [D loss: 0.346494, acc.: 83.59%] [G loss: 2.370376]\n",
      "epoch:7 step:6171 [D loss: 0.319616, acc.: 89.06%] [G loss: 3.092074]\n",
      "epoch:7 step:6172 [D loss: 0.298615, acc.: 89.06%] [G loss: 2.663539]\n",
      "epoch:7 step:6173 [D loss: 0.287009, acc.: 94.53%] [G loss: 3.565614]\n",
      "epoch:7 step:6174 [D loss: 0.335721, acc.: 88.28%] [G loss: 3.540520]\n",
      "epoch:7 step:6175 [D loss: 0.479385, acc.: 76.56%] [G loss: 2.176395]\n",
      "epoch:7 step:6176 [D loss: 0.320761, acc.: 85.94%] [G loss: 2.374795]\n",
      "epoch:7 step:6177 [D loss: 0.282365, acc.: 90.62%] [G loss: 2.774942]\n",
      "epoch:7 step:6178 [D loss: 0.284219, acc.: 88.28%] [G loss: 2.870216]\n",
      "epoch:7 step:6179 [D loss: 0.382818, acc.: 85.94%] [G loss: 2.190987]\n",
      "epoch:7 step:6180 [D loss: 0.383341, acc.: 85.94%] [G loss: 2.462913]\n",
      "epoch:7 step:6181 [D loss: 0.372257, acc.: 85.94%] [G loss: 2.390542]\n",
      "epoch:7 step:6182 [D loss: 0.286693, acc.: 91.41%] [G loss: 3.007336]\n",
      "epoch:7 step:6183 [D loss: 0.363914, acc.: 87.50%] [G loss: 2.496449]\n",
      "epoch:7 step:6184 [D loss: 0.322579, acc.: 83.59%] [G loss: 3.246666]\n",
      "epoch:7 step:6185 [D loss: 0.426069, acc.: 82.03%] [G loss: 2.686070]\n",
      "epoch:7 step:6186 [D loss: 0.347328, acc.: 87.50%] [G loss: 1.960046]\n",
      "epoch:7 step:6187 [D loss: 0.320856, acc.: 86.72%] [G loss: 2.392265]\n",
      "epoch:7 step:6188 [D loss: 0.325254, acc.: 88.28%] [G loss: 3.709532]\n",
      "epoch:7 step:6189 [D loss: 0.271605, acc.: 89.84%] [G loss: 3.476846]\n",
      "epoch:7 step:6190 [D loss: 0.228609, acc.: 92.19%] [G loss: 6.012136]\n",
      "epoch:7 step:6191 [D loss: 0.241221, acc.: 90.62%] [G loss: 3.119770]\n",
      "epoch:7 step:6192 [D loss: 0.289902, acc.: 91.41%] [G loss: 3.366625]\n",
      "epoch:7 step:6193 [D loss: 0.257030, acc.: 88.28%] [G loss: 3.003257]\n",
      "epoch:7 step:6194 [D loss: 0.216146, acc.: 93.75%] [G loss: 4.032787]\n",
      "epoch:7 step:6195 [D loss: 0.278457, acc.: 87.50%] [G loss: 3.857400]\n",
      "epoch:7 step:6196 [D loss: 0.239375, acc.: 90.62%] [G loss: 2.812518]\n",
      "epoch:7 step:6197 [D loss: 0.237011, acc.: 88.28%] [G loss: 3.410657]\n",
      "epoch:7 step:6198 [D loss: 0.244187, acc.: 90.62%] [G loss: 3.101913]\n",
      "epoch:7 step:6199 [D loss: 0.334050, acc.: 83.59%] [G loss: 2.276188]\n",
      "epoch:7 step:6200 [D loss: 0.259616, acc.: 89.84%] [G loss: 2.787166]\n",
      "##############\n",
      "[0.83706908 0.9108605  0.76207715 0.78329262 0.77579046 0.78561515\n",
      " 0.85234721 0.80841654 0.79619387 0.80499165]\n",
      "##########\n",
      "epoch:7 step:6201 [D loss: 0.378061, acc.: 82.03%] [G loss: 2.699897]\n",
      "epoch:7 step:6202 [D loss: 0.249994, acc.: 93.75%] [G loss: 2.854310]\n",
      "epoch:7 step:6203 [D loss: 0.326922, acc.: 85.94%] [G loss: 2.361754]\n",
      "epoch:7 step:6204 [D loss: 0.346262, acc.: 85.94%] [G loss: 2.392910]\n",
      "epoch:7 step:6205 [D loss: 0.277008, acc.: 89.06%] [G loss: 3.058255]\n",
      "epoch:7 step:6206 [D loss: 0.317779, acc.: 85.94%] [G loss: 4.217990]\n",
      "epoch:7 step:6207 [D loss: 0.326423, acc.: 86.72%] [G loss: 3.701373]\n",
      "epoch:7 step:6208 [D loss: 0.402586, acc.: 81.25%] [G loss: 2.694438]\n",
      "epoch:7 step:6209 [D loss: 0.304566, acc.: 91.41%] [G loss: 3.048159]\n",
      "epoch:7 step:6210 [D loss: 0.258827, acc.: 89.06%] [G loss: 3.084438]\n",
      "epoch:7 step:6211 [D loss: 0.205087, acc.: 92.97%] [G loss: 4.373817]\n",
      "epoch:7 step:6212 [D loss: 0.257994, acc.: 92.19%] [G loss: 4.222548]\n",
      "epoch:7 step:6213 [D loss: 0.204278, acc.: 92.19%] [G loss: 5.062736]\n",
      "epoch:7 step:6214 [D loss: 0.262178, acc.: 89.84%] [G loss: 3.803958]\n",
      "epoch:7 step:6215 [D loss: 0.258530, acc.: 89.84%] [G loss: 3.312664]\n",
      "epoch:7 step:6216 [D loss: 0.259482, acc.: 89.06%] [G loss: 4.063871]\n",
      "epoch:7 step:6217 [D loss: 0.426033, acc.: 82.81%] [G loss: 2.816795]\n",
      "epoch:7 step:6218 [D loss: 0.361183, acc.: 82.03%] [G loss: 2.405574]\n",
      "epoch:7 step:6219 [D loss: 0.237241, acc.: 90.62%] [G loss: 3.162266]\n",
      "epoch:7 step:6220 [D loss: 0.263305, acc.: 90.62%] [G loss: 3.725459]\n",
      "epoch:7 step:6221 [D loss: 0.390447, acc.: 82.03%] [G loss: 2.094706]\n",
      "epoch:7 step:6222 [D loss: 0.389952, acc.: 82.81%] [G loss: 2.084156]\n",
      "epoch:7 step:6223 [D loss: 0.333984, acc.: 88.28%] [G loss: 2.745412]\n",
      "epoch:7 step:6224 [D loss: 0.270869, acc.: 92.97%] [G loss: 2.156092]\n",
      "epoch:7 step:6225 [D loss: 0.386583, acc.: 83.59%] [G loss: 2.360482]\n",
      "epoch:7 step:6226 [D loss: 0.286519, acc.: 89.84%] [G loss: 3.185939]\n",
      "epoch:7 step:6227 [D loss: 0.293886, acc.: 89.06%] [G loss: 2.541849]\n",
      "epoch:7 step:6228 [D loss: 0.319736, acc.: 84.38%] [G loss: 3.475418]\n",
      "epoch:7 step:6229 [D loss: 0.323123, acc.: 83.59%] [G loss: 5.535860]\n",
      "epoch:7 step:6230 [D loss: 0.352220, acc.: 81.25%] [G loss: 3.144703]\n",
      "epoch:7 step:6231 [D loss: 0.474662, acc.: 77.34%] [G loss: 2.946223]\n",
      "epoch:7 step:6232 [D loss: 0.280824, acc.: 87.50%] [G loss: 5.037818]\n",
      "epoch:7 step:6233 [D loss: 0.373963, acc.: 82.03%] [G loss: 5.903302]\n",
      "epoch:7 step:6234 [D loss: 0.308714, acc.: 85.16%] [G loss: 2.581988]\n",
      "epoch:7 step:6235 [D loss: 0.370967, acc.: 80.47%] [G loss: 2.508655]\n",
      "epoch:7 step:6236 [D loss: 0.262965, acc.: 91.41%] [G loss: 3.363428]\n",
      "epoch:7 step:6237 [D loss: 0.281758, acc.: 88.28%] [G loss: 3.197777]\n",
      "epoch:7 step:6238 [D loss: 0.321633, acc.: 88.28%] [G loss: 3.237522]\n",
      "epoch:7 step:6239 [D loss: 0.384095, acc.: 81.25%] [G loss: 3.074183]\n",
      "epoch:7 step:6240 [D loss: 0.449794, acc.: 82.03%] [G loss: 5.316440]\n",
      "epoch:7 step:6241 [D loss: 1.551912, acc.: 58.59%] [G loss: 8.469160]\n",
      "epoch:7 step:6242 [D loss: 3.016874, acc.: 60.94%] [G loss: 7.166163]\n",
      "epoch:7 step:6243 [D loss: 2.461960, acc.: 44.53%] [G loss: 2.776031]\n",
      "epoch:7 step:6244 [D loss: 0.448315, acc.: 82.03%] [G loss: 4.793633]\n",
      "epoch:7 step:6245 [D loss: 0.546935, acc.: 76.56%] [G loss: 4.221842]\n",
      "epoch:7 step:6246 [D loss: 0.449683, acc.: 83.59%] [G loss: 3.549018]\n",
      "epoch:7 step:6247 [D loss: 0.425985, acc.: 82.81%] [G loss: 2.535494]\n",
      "epoch:7 step:6248 [D loss: 0.383377, acc.: 82.81%] [G loss: 3.137016]\n",
      "epoch:8 step:6249 [D loss: 0.331471, acc.: 90.62%] [G loss: 2.885651]\n",
      "epoch:8 step:6250 [D loss: 0.368782, acc.: 85.16%] [G loss: 1.967334]\n",
      "epoch:8 step:6251 [D loss: 0.327714, acc.: 87.50%] [G loss: 2.041976]\n",
      "epoch:8 step:6252 [D loss: 0.358905, acc.: 84.38%] [G loss: 1.915613]\n",
      "epoch:8 step:6253 [D loss: 0.251966, acc.: 92.19%] [G loss: 2.695850]\n",
      "epoch:8 step:6254 [D loss: 0.266365, acc.: 88.28%] [G loss: 4.061522]\n",
      "epoch:8 step:6255 [D loss: 0.178976, acc.: 96.09%] [G loss: 3.094226]\n",
      "epoch:8 step:6256 [D loss: 0.271496, acc.: 89.84%] [G loss: 2.516210]\n",
      "epoch:8 step:6257 [D loss: 0.283743, acc.: 91.41%] [G loss: 2.450667]\n",
      "epoch:8 step:6258 [D loss: 0.272174, acc.: 89.84%] [G loss: 3.508825]\n",
      "epoch:8 step:6259 [D loss: 0.249238, acc.: 89.84%] [G loss: 4.061974]\n",
      "epoch:8 step:6260 [D loss: 0.266723, acc.: 90.62%] [G loss: 2.325714]\n",
      "epoch:8 step:6261 [D loss: 0.320589, acc.: 87.50%] [G loss: 2.272318]\n",
      "epoch:8 step:6262 [D loss: 0.288327, acc.: 90.62%] [G loss: 2.461992]\n",
      "epoch:8 step:6263 [D loss: 0.243233, acc.: 93.75%] [G loss: 2.330966]\n",
      "epoch:8 step:6264 [D loss: 0.360750, acc.: 85.94%] [G loss: 2.589670]\n",
      "epoch:8 step:6265 [D loss: 0.247275, acc.: 92.19%] [G loss: 2.322190]\n",
      "epoch:8 step:6266 [D loss: 0.347894, acc.: 88.28%] [G loss: 2.266052]\n",
      "epoch:8 step:6267 [D loss: 0.366836, acc.: 85.16%] [G loss: 2.126018]\n",
      "epoch:8 step:6268 [D loss: 0.291421, acc.: 91.41%] [G loss: 2.134158]\n",
      "epoch:8 step:6269 [D loss: 0.314123, acc.: 86.72%] [G loss: 2.490291]\n",
      "epoch:8 step:6270 [D loss: 0.372681, acc.: 82.81%] [G loss: 2.376387]\n",
      "epoch:8 step:6271 [D loss: 0.217738, acc.: 92.97%] [G loss: 3.613184]\n",
      "epoch:8 step:6272 [D loss: 0.214590, acc.: 95.31%] [G loss: 2.019500]\n",
      "epoch:8 step:6273 [D loss: 0.309255, acc.: 91.41%] [G loss: 1.765420]\n",
      "epoch:8 step:6274 [D loss: 0.418556, acc.: 82.03%] [G loss: 2.723291]\n",
      "epoch:8 step:6275 [D loss: 0.305531, acc.: 89.06%] [G loss: 3.441058]\n",
      "epoch:8 step:6276 [D loss: 0.332356, acc.: 84.38%] [G loss: 2.765272]\n",
      "epoch:8 step:6277 [D loss: 0.295348, acc.: 91.41%] [G loss: 2.103846]\n",
      "epoch:8 step:6278 [D loss: 0.316540, acc.: 89.06%] [G loss: 2.601681]\n",
      "epoch:8 step:6279 [D loss: 0.278913, acc.: 88.28%] [G loss: 3.315140]\n",
      "epoch:8 step:6280 [D loss: 0.257060, acc.: 89.06%] [G loss: 5.087638]\n",
      "epoch:8 step:6281 [D loss: 0.283389, acc.: 89.84%] [G loss: 2.669154]\n",
      "epoch:8 step:6282 [D loss: 0.369994, acc.: 84.38%] [G loss: 2.448776]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:8 step:6283 [D loss: 0.294454, acc.: 89.06%] [G loss: 2.159024]\n",
      "epoch:8 step:6284 [D loss: 0.259089, acc.: 89.06%] [G loss: 4.243200]\n",
      "epoch:8 step:6285 [D loss: 0.204990, acc.: 90.62%] [G loss: 6.140211]\n",
      "epoch:8 step:6286 [D loss: 0.344078, acc.: 80.47%] [G loss: 2.308838]\n",
      "epoch:8 step:6287 [D loss: 0.253085, acc.: 89.84%] [G loss: 2.556497]\n",
      "epoch:8 step:6288 [D loss: 0.341060, acc.: 86.72%] [G loss: 2.765834]\n",
      "epoch:8 step:6289 [D loss: 0.382259, acc.: 88.28%] [G loss: 2.335214]\n",
      "epoch:8 step:6290 [D loss: 0.305826, acc.: 89.06%] [G loss: 2.281651]\n",
      "epoch:8 step:6291 [D loss: 0.254590, acc.: 89.84%] [G loss: 4.160345]\n",
      "epoch:8 step:6292 [D loss: 0.195506, acc.: 92.19%] [G loss: 3.737425]\n",
      "epoch:8 step:6293 [D loss: 0.332711, acc.: 85.94%] [G loss: 2.748130]\n",
      "epoch:8 step:6294 [D loss: 0.352718, acc.: 82.03%] [G loss: 1.891828]\n",
      "epoch:8 step:6295 [D loss: 0.380467, acc.: 85.94%] [G loss: 2.415378]\n",
      "epoch:8 step:6296 [D loss: 0.342507, acc.: 83.59%] [G loss: 3.120630]\n",
      "epoch:8 step:6297 [D loss: 0.254646, acc.: 90.62%] [G loss: 3.633169]\n",
      "epoch:8 step:6298 [D loss: 0.404277, acc.: 78.12%] [G loss: 2.432902]\n",
      "epoch:8 step:6299 [D loss: 0.313199, acc.: 85.94%] [G loss: 2.681357]\n",
      "epoch:8 step:6300 [D loss: 0.359235, acc.: 81.25%] [G loss: 2.682052]\n",
      "epoch:8 step:6301 [D loss: 0.329939, acc.: 86.72%] [G loss: 4.568828]\n",
      "epoch:8 step:6302 [D loss: 0.274265, acc.: 90.62%] [G loss: 4.540429]\n",
      "epoch:8 step:6303 [D loss: 0.311402, acc.: 86.72%] [G loss: 2.708449]\n",
      "epoch:8 step:6304 [D loss: 0.329431, acc.: 85.16%] [G loss: 2.136758]\n",
      "epoch:8 step:6305 [D loss: 0.417713, acc.: 82.03%] [G loss: 2.210533]\n",
      "epoch:8 step:6306 [D loss: 0.218484, acc.: 92.19%] [G loss: 3.348917]\n",
      "epoch:8 step:6307 [D loss: 0.299538, acc.: 89.06%] [G loss: 3.832209]\n",
      "epoch:8 step:6308 [D loss: 0.389921, acc.: 84.38%] [G loss: 3.805882]\n",
      "epoch:8 step:6309 [D loss: 0.374534, acc.: 82.81%] [G loss: 3.739581]\n",
      "epoch:8 step:6310 [D loss: 0.336110, acc.: 82.81%] [G loss: 2.842221]\n",
      "epoch:8 step:6311 [D loss: 0.281942, acc.: 85.16%] [G loss: 2.975926]\n",
      "epoch:8 step:6312 [D loss: 0.374190, acc.: 78.91%] [G loss: 1.956310]\n",
      "epoch:8 step:6313 [D loss: 0.512302, acc.: 73.44%] [G loss: 1.568890]\n",
      "epoch:8 step:6314 [D loss: 0.370106, acc.: 81.25%] [G loss: 2.737165]\n",
      "epoch:8 step:6315 [D loss: 0.295650, acc.: 89.06%] [G loss: 3.549865]\n",
      "epoch:8 step:6316 [D loss: 0.394825, acc.: 79.69%] [G loss: 2.866087]\n",
      "epoch:8 step:6317 [D loss: 0.385274, acc.: 79.69%] [G loss: 1.932613]\n",
      "epoch:8 step:6318 [D loss: 0.466820, acc.: 80.47%] [G loss: 2.160581]\n",
      "epoch:8 step:6319 [D loss: 0.258918, acc.: 91.41%] [G loss: 2.571733]\n",
      "epoch:8 step:6320 [D loss: 0.371434, acc.: 84.38%] [G loss: 4.448641]\n",
      "epoch:8 step:6321 [D loss: 0.331640, acc.: 85.16%] [G loss: 3.500252]\n",
      "epoch:8 step:6322 [D loss: 0.350870, acc.: 84.38%] [G loss: 3.834080]\n",
      "epoch:8 step:6323 [D loss: 0.513182, acc.: 77.34%] [G loss: 4.688128]\n",
      "epoch:8 step:6324 [D loss: 0.639248, acc.: 72.66%] [G loss: 5.236642]\n",
      "epoch:8 step:6325 [D loss: 1.053026, acc.: 67.97%] [G loss: 5.412853]\n",
      "epoch:8 step:6326 [D loss: 1.891790, acc.: 48.44%] [G loss: 3.615234]\n",
      "epoch:8 step:6327 [D loss: 1.375766, acc.: 46.09%] [G loss: 2.348812]\n",
      "epoch:8 step:6328 [D loss: 0.643795, acc.: 72.66%] [G loss: 2.608469]\n",
      "epoch:8 step:6329 [D loss: 0.818468, acc.: 69.53%] [G loss: 4.055918]\n",
      "epoch:8 step:6330 [D loss: 0.645455, acc.: 70.31%] [G loss: 4.666225]\n",
      "epoch:8 step:6331 [D loss: 0.743095, acc.: 81.25%] [G loss: 5.159605]\n",
      "epoch:8 step:6332 [D loss: 0.596950, acc.: 82.81%] [G loss: 7.484052]\n",
      "epoch:8 step:6333 [D loss: 0.680336, acc.: 72.66%] [G loss: 3.198720]\n",
      "epoch:8 step:6334 [D loss: 0.462613, acc.: 79.69%] [G loss: 2.994227]\n",
      "epoch:8 step:6335 [D loss: 0.372716, acc.: 83.59%] [G loss: 1.996466]\n",
      "epoch:8 step:6336 [D loss: 0.303093, acc.: 88.28%] [G loss: 2.489517]\n",
      "epoch:8 step:6337 [D loss: 0.414804, acc.: 79.69%] [G loss: 2.620513]\n",
      "epoch:8 step:6338 [D loss: 0.487353, acc.: 75.00%] [G loss: 2.566289]\n",
      "epoch:8 step:6339 [D loss: 0.156185, acc.: 95.31%] [G loss: 4.483760]\n",
      "epoch:8 step:6340 [D loss: 0.355106, acc.: 82.81%] [G loss: 3.407466]\n",
      "epoch:8 step:6341 [D loss: 0.521162, acc.: 78.91%] [G loss: 1.558445]\n",
      "epoch:8 step:6342 [D loss: 0.483230, acc.: 81.25%] [G loss: 3.108063]\n",
      "epoch:8 step:6343 [D loss: 0.408918, acc.: 82.03%] [G loss: 3.172153]\n",
      "epoch:8 step:6344 [D loss: 0.419106, acc.: 85.16%] [G loss: 2.271002]\n",
      "epoch:8 step:6345 [D loss: 0.379720, acc.: 85.16%] [G loss: 1.984875]\n",
      "epoch:8 step:6346 [D loss: 0.321931, acc.: 87.50%] [G loss: 2.421307]\n",
      "epoch:8 step:6347 [D loss: 0.398651, acc.: 84.38%] [G loss: 2.803231]\n",
      "epoch:8 step:6348 [D loss: 0.396884, acc.: 79.69%] [G loss: 2.351147]\n",
      "epoch:8 step:6349 [D loss: 0.373867, acc.: 82.81%] [G loss: 2.667387]\n",
      "epoch:8 step:6350 [D loss: 0.389591, acc.: 82.81%] [G loss: 2.010667]\n",
      "epoch:8 step:6351 [D loss: 0.361286, acc.: 86.72%] [G loss: 2.100852]\n",
      "epoch:8 step:6352 [D loss: 0.405475, acc.: 82.81%] [G loss: 2.290509]\n",
      "epoch:8 step:6353 [D loss: 0.314946, acc.: 90.62%] [G loss: 2.373528]\n",
      "epoch:8 step:6354 [D loss: 0.380778, acc.: 82.03%] [G loss: 2.998006]\n",
      "epoch:8 step:6355 [D loss: 0.293285, acc.: 88.28%] [G loss: 2.638419]\n",
      "epoch:8 step:6356 [D loss: 0.258159, acc.: 90.62%] [G loss: 3.440954]\n",
      "epoch:8 step:6357 [D loss: 0.376074, acc.: 85.16%] [G loss: 2.028232]\n",
      "epoch:8 step:6358 [D loss: 0.374212, acc.: 89.84%] [G loss: 2.254049]\n",
      "epoch:8 step:6359 [D loss: 0.358345, acc.: 78.91%] [G loss: 2.883310]\n",
      "epoch:8 step:6360 [D loss: 0.227223, acc.: 89.84%] [G loss: 4.100355]\n",
      "epoch:8 step:6361 [D loss: 0.326275, acc.: 87.50%] [G loss: 1.943702]\n",
      "epoch:8 step:6362 [D loss: 0.313642, acc.: 86.72%] [G loss: 2.488245]\n",
      "epoch:8 step:6363 [D loss: 0.340039, acc.: 88.28%] [G loss: 2.147712]\n",
      "epoch:8 step:6364 [D loss: 0.351821, acc.: 89.06%] [G loss: 2.120571]\n",
      "epoch:8 step:6365 [D loss: 0.319967, acc.: 85.94%] [G loss: 2.274424]\n",
      "epoch:8 step:6366 [D loss: 0.400752, acc.: 83.59%] [G loss: 1.645607]\n",
      "epoch:8 step:6367 [D loss: 0.289597, acc.: 89.06%] [G loss: 2.861421]\n",
      "epoch:8 step:6368 [D loss: 0.292033, acc.: 86.72%] [G loss: 3.879460]\n",
      "epoch:8 step:6369 [D loss: 0.314241, acc.: 87.50%] [G loss: 2.634401]\n",
      "epoch:8 step:6370 [D loss: 0.279853, acc.: 88.28%] [G loss: 2.298040]\n",
      "epoch:8 step:6371 [D loss: 0.389962, acc.: 85.16%] [G loss: 2.043875]\n",
      "epoch:8 step:6372 [D loss: 0.341426, acc.: 85.16%] [G loss: 2.326051]\n",
      "epoch:8 step:6373 [D loss: 0.340304, acc.: 85.94%] [G loss: 2.746396]\n",
      "epoch:8 step:6374 [D loss: 0.300636, acc.: 85.16%] [G loss: 2.503204]\n",
      "epoch:8 step:6375 [D loss: 0.244124, acc.: 91.41%] [G loss: 3.179216]\n",
      "epoch:8 step:6376 [D loss: 0.304730, acc.: 88.28%] [G loss: 3.020255]\n",
      "epoch:8 step:6377 [D loss: 0.314657, acc.: 89.06%] [G loss: 2.139417]\n",
      "epoch:8 step:6378 [D loss: 0.422847, acc.: 80.47%] [G loss: 1.978475]\n",
      "epoch:8 step:6379 [D loss: 0.493393, acc.: 75.78%] [G loss: 2.958223]\n",
      "epoch:8 step:6380 [D loss: 0.375256, acc.: 82.81%] [G loss: 2.625883]\n",
      "epoch:8 step:6381 [D loss: 0.363876, acc.: 80.47%] [G loss: 3.998766]\n",
      "epoch:8 step:6382 [D loss: 0.293200, acc.: 89.06%] [G loss: 5.121859]\n",
      "epoch:8 step:6383 [D loss: 0.277509, acc.: 90.62%] [G loss: 3.680518]\n",
      "epoch:8 step:6384 [D loss: 0.374474, acc.: 86.72%] [G loss: 2.507448]\n",
      "epoch:8 step:6385 [D loss: 0.369839, acc.: 84.38%] [G loss: 3.730417]\n",
      "epoch:8 step:6386 [D loss: 0.418861, acc.: 82.81%] [G loss: 3.702217]\n",
      "epoch:8 step:6387 [D loss: 0.325790, acc.: 85.94%] [G loss: 2.540180]\n",
      "epoch:8 step:6388 [D loss: 0.339566, acc.: 85.16%] [G loss: 2.278073]\n",
      "epoch:8 step:6389 [D loss: 0.254005, acc.: 90.62%] [G loss: 3.624474]\n",
      "epoch:8 step:6390 [D loss: 0.352402, acc.: 81.25%] [G loss: 2.862611]\n",
      "epoch:8 step:6391 [D loss: 0.269813, acc.: 92.97%] [G loss: 5.118613]\n",
      "epoch:8 step:6392 [D loss: 0.331192, acc.: 84.38%] [G loss: 4.182381]\n",
      "epoch:8 step:6393 [D loss: 0.350016, acc.: 83.59%] [G loss: 2.120759]\n",
      "epoch:8 step:6394 [D loss: 0.322119, acc.: 83.59%] [G loss: 2.813036]\n",
      "epoch:8 step:6395 [D loss: 0.289166, acc.: 91.41%] [G loss: 2.546769]\n",
      "epoch:8 step:6396 [D loss: 0.379706, acc.: 82.03%] [G loss: 2.127665]\n",
      "epoch:8 step:6397 [D loss: 0.313814, acc.: 86.72%] [G loss: 2.905212]\n",
      "epoch:8 step:6398 [D loss: 0.296452, acc.: 90.62%] [G loss: 2.480046]\n",
      "epoch:8 step:6399 [D loss: 0.244653, acc.: 90.62%] [G loss: 3.104338]\n",
      "epoch:8 step:6400 [D loss: 0.282772, acc.: 89.06%] [G loss: 2.605200]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##############\n",
      "[0.82786616 0.90189574 0.76300857 0.79679243 0.79075651 0.81554837\n",
      " 0.87674491 0.84375948 0.81163214 0.80330207]\n",
      "##########\n",
      "epoch:8 step:6401 [D loss: 0.374213, acc.: 79.69%] [G loss: 2.328196]\n",
      "epoch:8 step:6402 [D loss: 0.413122, acc.: 85.94%] [G loss: 3.633368]\n",
      "epoch:8 step:6403 [D loss: 0.438364, acc.: 84.38%] [G loss: 2.352677]\n",
      "epoch:8 step:6404 [D loss: 0.311221, acc.: 86.72%] [G loss: 3.577667]\n",
      "epoch:8 step:6405 [D loss: 0.306277, acc.: 89.84%] [G loss: 3.416927]\n",
      "epoch:8 step:6406 [D loss: 0.317902, acc.: 88.28%] [G loss: 3.685739]\n",
      "epoch:8 step:6407 [D loss: 0.313034, acc.: 89.06%] [G loss: 3.642157]\n",
      "epoch:8 step:6408 [D loss: 0.338489, acc.: 86.72%] [G loss: 4.400126]\n",
      "epoch:8 step:6409 [D loss: 0.319741, acc.: 84.38%] [G loss: 3.690133]\n",
      "epoch:8 step:6410 [D loss: 0.283583, acc.: 89.06%] [G loss: 2.658265]\n",
      "epoch:8 step:6411 [D loss: 0.327368, acc.: 85.16%] [G loss: 2.883352]\n",
      "epoch:8 step:6412 [D loss: 0.266145, acc.: 89.84%] [G loss: 3.220162]\n",
      "epoch:8 step:6413 [D loss: 0.412286, acc.: 82.03%] [G loss: 3.598079]\n",
      "epoch:8 step:6414 [D loss: 0.366519, acc.: 87.50%] [G loss: 2.775118]\n",
      "epoch:8 step:6415 [D loss: 0.399144, acc.: 79.69%] [G loss: 3.527161]\n",
      "epoch:8 step:6416 [D loss: 0.421565, acc.: 82.81%] [G loss: 3.075976]\n",
      "epoch:8 step:6417 [D loss: 0.300376, acc.: 88.28%] [G loss: 4.724540]\n",
      "epoch:8 step:6418 [D loss: 0.343739, acc.: 85.16%] [G loss: 3.051322]\n",
      "epoch:8 step:6419 [D loss: 0.341787, acc.: 82.81%] [G loss: 2.898172]\n",
      "epoch:8 step:6420 [D loss: 0.346705, acc.: 87.50%] [G loss: 2.446668]\n",
      "epoch:8 step:6421 [D loss: 0.366020, acc.: 85.16%] [G loss: 2.205978]\n",
      "epoch:8 step:6422 [D loss: 0.334842, acc.: 86.72%] [G loss: 3.243250]\n",
      "epoch:8 step:6423 [D loss: 0.313208, acc.: 89.06%] [G loss: 3.010231]\n",
      "epoch:8 step:6424 [D loss: 0.223624, acc.: 92.97%] [G loss: 3.190956]\n",
      "epoch:8 step:6425 [D loss: 0.268453, acc.: 88.28%] [G loss: 3.133013]\n",
      "epoch:8 step:6426 [D loss: 0.219256, acc.: 91.41%] [G loss: 3.631362]\n",
      "epoch:8 step:6427 [D loss: 0.313455, acc.: 89.84%] [G loss: 3.273799]\n",
      "epoch:8 step:6428 [D loss: 0.480673, acc.: 76.56%] [G loss: 2.092934]\n",
      "epoch:8 step:6429 [D loss: 0.320021, acc.: 89.06%] [G loss: 2.381863]\n",
      "epoch:8 step:6430 [D loss: 0.298208, acc.: 87.50%] [G loss: 3.109175]\n",
      "epoch:8 step:6431 [D loss: 0.300697, acc.: 89.06%] [G loss: 3.039810]\n",
      "epoch:8 step:6432 [D loss: 0.359629, acc.: 82.81%] [G loss: 2.024816]\n",
      "epoch:8 step:6433 [D loss: 0.241919, acc.: 91.41%] [G loss: 2.985435]\n",
      "epoch:8 step:6434 [D loss: 0.251016, acc.: 93.75%] [G loss: 2.527180]\n",
      "epoch:8 step:6435 [D loss: 0.358793, acc.: 87.50%] [G loss: 2.548943]\n",
      "epoch:8 step:6436 [D loss: 0.249211, acc.: 90.62%] [G loss: 3.409392]\n",
      "epoch:8 step:6437 [D loss: 0.233594, acc.: 94.53%] [G loss: 2.692118]\n",
      "epoch:8 step:6438 [D loss: 0.303530, acc.: 89.84%] [G loss: 2.841633]\n",
      "epoch:8 step:6439 [D loss: 0.354002, acc.: 86.72%] [G loss: 2.097250]\n",
      "epoch:8 step:6440 [D loss: 0.275326, acc.: 90.62%] [G loss: 2.405542]\n",
      "epoch:8 step:6441 [D loss: 0.296064, acc.: 88.28%] [G loss: 2.561854]\n",
      "epoch:8 step:6442 [D loss: 0.307199, acc.: 89.06%] [G loss: 2.488861]\n",
      "epoch:8 step:6443 [D loss: 0.372200, acc.: 85.16%] [G loss: 1.982872]\n",
      "epoch:8 step:6444 [D loss: 0.317907, acc.: 86.72%] [G loss: 2.375183]\n",
      "epoch:8 step:6445 [D loss: 0.324322, acc.: 85.94%] [G loss: 3.364860]\n",
      "epoch:8 step:6446 [D loss: 0.213372, acc.: 92.97%] [G loss: 3.966939]\n",
      "epoch:8 step:6447 [D loss: 0.318996, acc.: 85.94%] [G loss: 2.598970]\n",
      "epoch:8 step:6448 [D loss: 0.307456, acc.: 89.84%] [G loss: 2.003930]\n",
      "epoch:8 step:6449 [D loss: 0.298043, acc.: 92.19%] [G loss: 3.056140]\n",
      "epoch:8 step:6450 [D loss: 0.231440, acc.: 92.97%] [G loss: 2.928369]\n",
      "epoch:8 step:6451 [D loss: 0.406726, acc.: 82.03%] [G loss: 2.172208]\n",
      "epoch:8 step:6452 [D loss: 0.309314, acc.: 88.28%] [G loss: 2.988381]\n",
      "epoch:8 step:6453 [D loss: 0.291171, acc.: 87.50%] [G loss: 3.833478]\n",
      "epoch:8 step:6454 [D loss: 0.296952, acc.: 89.06%] [G loss: 3.379280]\n",
      "epoch:8 step:6455 [D loss: 0.298018, acc.: 88.28%] [G loss: 2.211477]\n",
      "epoch:8 step:6456 [D loss: 0.362480, acc.: 85.94%] [G loss: 2.211097]\n",
      "epoch:8 step:6457 [D loss: 0.238412, acc.: 91.41%] [G loss: 2.809856]\n",
      "epoch:8 step:6458 [D loss: 0.252031, acc.: 86.72%] [G loss: 3.410501]\n",
      "epoch:8 step:6459 [D loss: 0.368469, acc.: 85.94%] [G loss: 3.743097]\n",
      "epoch:8 step:6460 [D loss: 0.324337, acc.: 86.72%] [G loss: 5.175947]\n",
      "epoch:8 step:6461 [D loss: 0.267693, acc.: 87.50%] [G loss: 3.174084]\n",
      "epoch:8 step:6462 [D loss: 0.344767, acc.: 86.72%] [G loss: 2.905785]\n",
      "epoch:8 step:6463 [D loss: 0.317228, acc.: 85.94%] [G loss: 2.791854]\n",
      "epoch:8 step:6464 [D loss: 0.301913, acc.: 89.84%] [G loss: 2.705640]\n",
      "epoch:8 step:6465 [D loss: 0.325197, acc.: 86.72%] [G loss: 1.947866]\n",
      "epoch:8 step:6466 [D loss: 0.336931, acc.: 90.62%] [G loss: 2.465161]\n",
      "epoch:8 step:6467 [D loss: 0.325391, acc.: 85.94%] [G loss: 3.432614]\n",
      "epoch:8 step:6468 [D loss: 0.235378, acc.: 87.50%] [G loss: 4.141310]\n",
      "epoch:8 step:6469 [D loss: 0.349166, acc.: 88.28%] [G loss: 2.666022]\n",
      "epoch:8 step:6470 [D loss: 0.255343, acc.: 89.84%] [G loss: 2.953344]\n",
      "epoch:8 step:6471 [D loss: 0.356875, acc.: 83.59%] [G loss: 2.562144]\n",
      "epoch:8 step:6472 [D loss: 0.437648, acc.: 84.38%] [G loss: 3.813308]\n",
      "epoch:8 step:6473 [D loss: 0.456950, acc.: 75.78%] [G loss: 4.497534]\n",
      "epoch:8 step:6474 [D loss: 0.713461, acc.: 70.31%] [G loss: 4.631391]\n",
      "epoch:8 step:6475 [D loss: 0.665351, acc.: 70.31%] [G loss: 3.342937]\n",
      "epoch:8 step:6476 [D loss: 0.459737, acc.: 79.69%] [G loss: 2.300776]\n",
      "epoch:8 step:6477 [D loss: 0.509752, acc.: 71.88%] [G loss: 2.136616]\n",
      "epoch:8 step:6478 [D loss: 0.419074, acc.: 80.47%] [G loss: 2.806695]\n",
      "epoch:8 step:6479 [D loss: 0.233833, acc.: 92.19%] [G loss: 3.865753]\n",
      "epoch:8 step:6480 [D loss: 0.267794, acc.: 89.84%] [G loss: 3.248971]\n",
      "epoch:8 step:6481 [D loss: 0.327471, acc.: 85.94%] [G loss: 2.582939]\n",
      "epoch:8 step:6482 [D loss: 0.207588, acc.: 94.53%] [G loss: 4.284801]\n",
      "epoch:8 step:6483 [D loss: 0.249801, acc.: 89.84%] [G loss: 2.862885]\n",
      "epoch:8 step:6484 [D loss: 0.355064, acc.: 85.16%] [G loss: 2.332953]\n",
      "epoch:8 step:6485 [D loss: 0.275827, acc.: 89.06%] [G loss: 2.565397]\n",
      "epoch:8 step:6486 [D loss: 0.328367, acc.: 88.28%] [G loss: 2.752029]\n",
      "epoch:8 step:6487 [D loss: 0.345119, acc.: 87.50%] [G loss: 4.084215]\n",
      "epoch:8 step:6488 [D loss: 0.432980, acc.: 75.78%] [G loss: 3.635776]\n",
      "epoch:8 step:6489 [D loss: 0.302018, acc.: 91.41%] [G loss: 2.649084]\n",
      "epoch:8 step:6490 [D loss: 0.289757, acc.: 88.28%] [G loss: 3.037310]\n",
      "epoch:8 step:6491 [D loss: 0.392466, acc.: 81.25%] [G loss: 2.282449]\n",
      "epoch:8 step:6492 [D loss: 0.351674, acc.: 89.84%] [G loss: 2.317009]\n",
      "epoch:8 step:6493 [D loss: 0.397383, acc.: 85.16%] [G loss: 2.427630]\n",
      "epoch:8 step:6494 [D loss: 0.361540, acc.: 85.94%] [G loss: 2.302805]\n",
      "epoch:8 step:6495 [D loss: 0.352267, acc.: 86.72%] [G loss: 2.082921]\n",
      "epoch:8 step:6496 [D loss: 0.311138, acc.: 89.84%] [G loss: 2.107466]\n",
      "epoch:8 step:6497 [D loss: 0.453573, acc.: 82.81%] [G loss: 2.794097]\n",
      "epoch:8 step:6498 [D loss: 0.325140, acc.: 86.72%] [G loss: 3.012110]\n",
      "epoch:8 step:6499 [D loss: 0.246977, acc.: 91.41%] [G loss: 3.158169]\n",
      "epoch:8 step:6500 [D loss: 0.353959, acc.: 82.81%] [G loss: 5.466686]\n",
      "epoch:8 step:6501 [D loss: 0.391625, acc.: 82.03%] [G loss: 2.150213]\n",
      "epoch:8 step:6502 [D loss: 0.373285, acc.: 82.81%] [G loss: 2.441702]\n",
      "epoch:8 step:6503 [D loss: 0.406752, acc.: 84.38%] [G loss: 2.058604]\n",
      "epoch:8 step:6504 [D loss: 0.251697, acc.: 88.28%] [G loss: 2.740771]\n",
      "epoch:8 step:6505 [D loss: 0.404585, acc.: 83.59%] [G loss: 2.882325]\n",
      "epoch:8 step:6506 [D loss: 0.330160, acc.: 85.16%] [G loss: 2.951515]\n",
      "epoch:8 step:6507 [D loss: 0.259533, acc.: 91.41%] [G loss: 3.098405]\n",
      "epoch:8 step:6508 [D loss: 0.355228, acc.: 88.28%] [G loss: 2.802372]\n",
      "epoch:8 step:6509 [D loss: 0.231593, acc.: 90.62%] [G loss: 4.422283]\n",
      "epoch:8 step:6510 [D loss: 0.252298, acc.: 87.50%] [G loss: 5.547852]\n",
      "epoch:8 step:6511 [D loss: 0.342528, acc.: 87.50%] [G loss: 3.122348]\n",
      "epoch:8 step:6512 [D loss: 0.415851, acc.: 82.03%] [G loss: 2.089837]\n",
      "epoch:8 step:6513 [D loss: 0.325190, acc.: 88.28%] [G loss: 2.113559]\n",
      "epoch:8 step:6514 [D loss: 0.339178, acc.: 89.06%] [G loss: 2.329718]\n",
      "epoch:8 step:6515 [D loss: 0.267442, acc.: 88.28%] [G loss: 2.371679]\n",
      "epoch:8 step:6516 [D loss: 0.378856, acc.: 85.16%] [G loss: 2.131562]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:8 step:6517 [D loss: 0.336405, acc.: 83.59%] [G loss: 2.531580]\n",
      "epoch:8 step:6518 [D loss: 0.287256, acc.: 89.06%] [G loss: 2.876971]\n",
      "epoch:8 step:6519 [D loss: 0.331092, acc.: 85.16%] [G loss: 4.120641]\n",
      "epoch:8 step:6520 [D loss: 0.313416, acc.: 87.50%] [G loss: 2.372154]\n",
      "epoch:8 step:6521 [D loss: 0.233512, acc.: 93.75%] [G loss: 3.403223]\n",
      "epoch:8 step:6522 [D loss: 0.238279, acc.: 96.09%] [G loss: 2.125277]\n",
      "epoch:8 step:6523 [D loss: 0.438142, acc.: 85.16%] [G loss: 2.903762]\n",
      "epoch:8 step:6524 [D loss: 0.276234, acc.: 89.06%] [G loss: 3.480247]\n",
      "epoch:8 step:6525 [D loss: 0.262835, acc.: 88.28%] [G loss: 2.655918]\n",
      "epoch:8 step:6526 [D loss: 0.423451, acc.: 81.25%] [G loss: 2.618393]\n",
      "epoch:8 step:6527 [D loss: 0.336462, acc.: 84.38%] [G loss: 2.398500]\n",
      "epoch:8 step:6528 [D loss: 0.307195, acc.: 89.84%] [G loss: 2.791412]\n",
      "epoch:8 step:6529 [D loss: 0.341601, acc.: 83.59%] [G loss: 2.339123]\n",
      "epoch:8 step:6530 [D loss: 0.224169, acc.: 93.75%] [G loss: 5.011215]\n",
      "epoch:8 step:6531 [D loss: 0.292375, acc.: 83.59%] [G loss: 4.460378]\n",
      "epoch:8 step:6532 [D loss: 0.344505, acc.: 85.94%] [G loss: 3.102026]\n",
      "epoch:8 step:6533 [D loss: 0.451338, acc.: 78.12%] [G loss: 3.121866]\n",
      "epoch:8 step:6534 [D loss: 0.369196, acc.: 81.25%] [G loss: 3.618684]\n",
      "epoch:8 step:6535 [D loss: 0.344592, acc.: 83.59%] [G loss: 2.248669]\n",
      "epoch:8 step:6536 [D loss: 0.319008, acc.: 85.16%] [G loss: 2.620301]\n",
      "epoch:8 step:6537 [D loss: 0.317914, acc.: 87.50%] [G loss: 2.438347]\n",
      "epoch:8 step:6538 [D loss: 0.270836, acc.: 89.84%] [G loss: 2.874408]\n",
      "epoch:8 step:6539 [D loss: 0.285562, acc.: 87.50%] [G loss: 3.394051]\n",
      "epoch:8 step:6540 [D loss: 0.268290, acc.: 91.41%] [G loss: 2.927616]\n",
      "epoch:8 step:6541 [D loss: 0.256581, acc.: 89.06%] [G loss: 3.498198]\n",
      "epoch:8 step:6542 [D loss: 0.336791, acc.: 85.94%] [G loss: 3.069001]\n",
      "epoch:8 step:6543 [D loss: 0.292103, acc.: 88.28%] [G loss: 2.381383]\n",
      "epoch:8 step:6544 [D loss: 0.331154, acc.: 82.81%] [G loss: 2.112534]\n",
      "epoch:8 step:6545 [D loss: 0.417559, acc.: 78.91%] [G loss: 2.150104]\n",
      "epoch:8 step:6546 [D loss: 0.396880, acc.: 82.03%] [G loss: 2.000169]\n",
      "epoch:8 step:6547 [D loss: 0.418620, acc.: 82.03%] [G loss: 1.931972]\n",
      "epoch:8 step:6548 [D loss: 0.402343, acc.: 84.38%] [G loss: 3.363888]\n",
      "epoch:8 step:6549 [D loss: 0.348995, acc.: 82.81%] [G loss: 3.245867]\n",
      "epoch:8 step:6550 [D loss: 0.254231, acc.: 89.84%] [G loss: 3.637105]\n",
      "epoch:8 step:6551 [D loss: 0.243722, acc.: 90.62%] [G loss: 4.708514]\n",
      "epoch:8 step:6552 [D loss: 0.293465, acc.: 89.06%] [G loss: 4.076024]\n",
      "epoch:8 step:6553 [D loss: 0.426215, acc.: 78.91%] [G loss: 2.720182]\n",
      "epoch:8 step:6554 [D loss: 0.417459, acc.: 82.81%] [G loss: 3.452964]\n",
      "epoch:8 step:6555 [D loss: 0.470084, acc.: 78.12%] [G loss: 2.786377]\n",
      "epoch:8 step:6556 [D loss: 0.383272, acc.: 82.03%] [G loss: 2.203614]\n",
      "epoch:8 step:6557 [D loss: 0.325242, acc.: 84.38%] [G loss: 2.424168]\n",
      "epoch:8 step:6558 [D loss: 0.242015, acc.: 90.62%] [G loss: 2.862334]\n",
      "epoch:8 step:6559 [D loss: 0.300910, acc.: 89.06%] [G loss: 3.375795]\n",
      "epoch:8 step:6560 [D loss: 0.285039, acc.: 89.84%] [G loss: 3.373505]\n",
      "epoch:8 step:6561 [D loss: 0.285521, acc.: 88.28%] [G loss: 5.703401]\n",
      "epoch:8 step:6562 [D loss: 0.395261, acc.: 78.12%] [G loss: 3.965281]\n",
      "epoch:8 step:6563 [D loss: 0.510918, acc.: 75.78%] [G loss: 2.916012]\n",
      "epoch:8 step:6564 [D loss: 0.297201, acc.: 88.28%] [G loss: 2.308175]\n",
      "epoch:8 step:6565 [D loss: 0.271533, acc.: 89.06%] [G loss: 3.285646]\n",
      "epoch:8 step:6566 [D loss: 0.307331, acc.: 88.28%] [G loss: 3.973188]\n",
      "epoch:8 step:6567 [D loss: 0.243256, acc.: 91.41%] [G loss: 2.671521]\n",
      "epoch:8 step:6568 [D loss: 0.380495, acc.: 85.94%] [G loss: 3.317574]\n",
      "epoch:8 step:6569 [D loss: 0.497603, acc.: 72.66%] [G loss: 2.344391]\n",
      "epoch:8 step:6570 [D loss: 0.352097, acc.: 85.16%] [G loss: 2.463534]\n",
      "epoch:8 step:6571 [D loss: 0.313811, acc.: 86.72%] [G loss: 3.190950]\n",
      "epoch:8 step:6572 [D loss: 0.296095, acc.: 85.16%] [G loss: 4.409332]\n",
      "epoch:8 step:6573 [D loss: 0.383497, acc.: 79.69%] [G loss: 6.156038]\n",
      "epoch:8 step:6574 [D loss: 0.452938, acc.: 79.69%] [G loss: 2.843617]\n",
      "epoch:8 step:6575 [D loss: 0.327586, acc.: 88.28%] [G loss: 3.439301]\n",
      "epoch:8 step:6576 [D loss: 0.297929, acc.: 89.06%] [G loss: 3.004138]\n",
      "epoch:8 step:6577 [D loss: 0.341840, acc.: 83.59%] [G loss: 3.546829]\n",
      "epoch:8 step:6578 [D loss: 0.330569, acc.: 88.28%] [G loss: 2.423608]\n",
      "epoch:8 step:6579 [D loss: 0.334041, acc.: 83.59%] [G loss: 3.052585]\n",
      "epoch:8 step:6580 [D loss: 0.349671, acc.: 88.28%] [G loss: 3.150648]\n",
      "epoch:8 step:6581 [D loss: 0.269446, acc.: 86.72%] [G loss: 3.597580]\n",
      "epoch:8 step:6582 [D loss: 0.299662, acc.: 86.72%] [G loss: 4.228288]\n",
      "epoch:8 step:6583 [D loss: 0.449568, acc.: 75.78%] [G loss: 2.836248]\n",
      "epoch:8 step:6584 [D loss: 0.278847, acc.: 88.28%] [G loss: 2.796171]\n",
      "epoch:8 step:6585 [D loss: 0.254164, acc.: 85.16%] [G loss: 4.857031]\n",
      "epoch:8 step:6586 [D loss: 0.296055, acc.: 85.16%] [G loss: 3.880029]\n",
      "epoch:8 step:6587 [D loss: 0.363901, acc.: 82.03%] [G loss: 3.280652]\n",
      "epoch:8 step:6588 [D loss: 0.353342, acc.: 82.03%] [G loss: 6.460646]\n",
      "epoch:8 step:6589 [D loss: 0.580384, acc.: 75.78%] [G loss: 4.905299]\n",
      "epoch:8 step:6590 [D loss: 0.801923, acc.: 64.06%] [G loss: 6.796718]\n",
      "epoch:8 step:6591 [D loss: 1.207286, acc.: 57.81%] [G loss: 3.242710]\n",
      "epoch:8 step:6592 [D loss: 1.050796, acc.: 70.31%] [G loss: 2.481311]\n",
      "epoch:8 step:6593 [D loss: 0.734684, acc.: 79.69%] [G loss: 3.512387]\n",
      "epoch:8 step:6594 [D loss: 0.419223, acc.: 81.25%] [G loss: 2.780207]\n",
      "epoch:8 step:6595 [D loss: 0.347763, acc.: 86.72%] [G loss: 2.486463]\n",
      "epoch:8 step:6596 [D loss: 0.330004, acc.: 86.72%] [G loss: 3.447530]\n",
      "epoch:8 step:6597 [D loss: 0.301423, acc.: 88.28%] [G loss: 3.123708]\n",
      "epoch:8 step:6598 [D loss: 0.433956, acc.: 77.34%] [G loss: 2.523518]\n",
      "epoch:8 step:6599 [D loss: 0.439465, acc.: 82.03%] [G loss: 2.487308]\n",
      "epoch:8 step:6600 [D loss: 0.306183, acc.: 89.06%] [G loss: 3.869372]\n",
      "##############\n",
      "[0.82456981 0.87979458 0.77594932 0.76786555 0.77994089 0.80017749\n",
      " 0.86511107 0.79879429 0.82879918 0.80236218]\n",
      "##########\n",
      "epoch:8 step:6601 [D loss: 0.339208, acc.: 84.38%] [G loss: 6.017147]\n",
      "epoch:8 step:6602 [D loss: 0.389025, acc.: 84.38%] [G loss: 3.595404]\n",
      "epoch:8 step:6603 [D loss: 0.370991, acc.: 85.16%] [G loss: 2.994879]\n",
      "epoch:8 step:6604 [D loss: 0.392726, acc.: 78.91%] [G loss: 2.141616]\n",
      "epoch:8 step:6605 [D loss: 0.380231, acc.: 80.47%] [G loss: 1.945023]\n",
      "epoch:8 step:6606 [D loss: 0.438348, acc.: 78.91%] [G loss: 1.814703]\n",
      "epoch:8 step:6607 [D loss: 0.326606, acc.: 85.16%] [G loss: 2.587734]\n",
      "epoch:8 step:6608 [D loss: 0.411198, acc.: 81.25%] [G loss: 2.645503]\n",
      "epoch:8 step:6609 [D loss: 0.307889, acc.: 92.97%] [G loss: 2.381372]\n",
      "epoch:8 step:6610 [D loss: 0.447638, acc.: 80.47%] [G loss: 2.695110]\n",
      "epoch:8 step:6611 [D loss: 0.364442, acc.: 81.25%] [G loss: 2.762217]\n",
      "epoch:8 step:6612 [D loss: 0.416383, acc.: 85.16%] [G loss: 2.074171]\n",
      "epoch:8 step:6613 [D loss: 0.322589, acc.: 89.84%] [G loss: 2.683646]\n",
      "epoch:8 step:6614 [D loss: 0.336589, acc.: 85.16%] [G loss: 2.920110]\n",
      "epoch:8 step:6615 [D loss: 0.326581, acc.: 89.06%] [G loss: 2.743112]\n",
      "epoch:8 step:6616 [D loss: 0.289546, acc.: 84.38%] [G loss: 4.193686]\n",
      "epoch:8 step:6617 [D loss: 0.367465, acc.: 84.38%] [G loss: 4.493400]\n",
      "epoch:8 step:6618 [D loss: 0.359524, acc.: 85.16%] [G loss: 3.353714]\n",
      "epoch:8 step:6619 [D loss: 0.371458, acc.: 85.94%] [G loss: 2.910599]\n",
      "epoch:8 step:6620 [D loss: 0.388666, acc.: 87.50%] [G loss: 2.042315]\n",
      "epoch:8 step:6621 [D loss: 0.334569, acc.: 85.94%] [G loss: 2.036817]\n",
      "epoch:8 step:6622 [D loss: 0.391052, acc.: 79.69%] [G loss: 2.170865]\n",
      "epoch:8 step:6623 [D loss: 0.359314, acc.: 86.72%] [G loss: 2.526655]\n",
      "epoch:8 step:6624 [D loss: 0.329755, acc.: 86.72%] [G loss: 2.132457]\n",
      "epoch:8 step:6625 [D loss: 0.365256, acc.: 83.59%] [G loss: 2.380492]\n",
      "epoch:8 step:6626 [D loss: 0.281465, acc.: 87.50%] [G loss: 4.014926]\n",
      "epoch:8 step:6627 [D loss: 0.242405, acc.: 89.06%] [G loss: 5.624185]\n",
      "epoch:8 step:6628 [D loss: 0.403173, acc.: 81.25%] [G loss: 1.766505]\n",
      "epoch:8 step:6629 [D loss: 0.286667, acc.: 88.28%] [G loss: 3.245358]\n",
      "epoch:8 step:6630 [D loss: 0.347243, acc.: 85.94%] [G loss: 2.841892]\n",
      "epoch:8 step:6631 [D loss: 0.269092, acc.: 86.72%] [G loss: 3.735998]\n",
      "epoch:8 step:6632 [D loss: 0.333359, acc.: 86.72%] [G loss: 5.739112]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:8 step:6633 [D loss: 0.586664, acc.: 75.78%] [G loss: 3.582171]\n",
      "epoch:8 step:6634 [D loss: 0.459439, acc.: 79.69%] [G loss: 3.015206]\n",
      "epoch:8 step:6635 [D loss: 0.398680, acc.: 78.91%] [G loss: 2.677005]\n",
      "epoch:8 step:6636 [D loss: 0.363432, acc.: 85.16%] [G loss: 2.357718]\n",
      "epoch:8 step:6637 [D loss: 0.408963, acc.: 85.16%] [G loss: 2.035461]\n",
      "epoch:8 step:6638 [D loss: 0.301184, acc.: 86.72%] [G loss: 3.225525]\n",
      "epoch:8 step:6639 [D loss: 0.238812, acc.: 88.28%] [G loss: 4.168608]\n",
      "epoch:8 step:6640 [D loss: 0.355344, acc.: 81.25%] [G loss: 4.550141]\n",
      "epoch:8 step:6641 [D loss: 0.337443, acc.: 85.94%] [G loss: 3.279457]\n",
      "epoch:8 step:6642 [D loss: 0.327440, acc.: 83.59%] [G loss: 2.938939]\n",
      "epoch:8 step:6643 [D loss: 0.392340, acc.: 82.81%] [G loss: 3.821044]\n",
      "epoch:8 step:6644 [D loss: 0.305831, acc.: 83.59%] [G loss: 4.269633]\n",
      "epoch:8 step:6645 [D loss: 0.341561, acc.: 85.16%] [G loss: 2.069012]\n",
      "epoch:8 step:6646 [D loss: 0.375004, acc.: 84.38%] [G loss: 3.244563]\n",
      "epoch:8 step:6647 [D loss: 0.403074, acc.: 81.25%] [G loss: 3.025343]\n",
      "epoch:8 step:6648 [D loss: 0.331978, acc.: 84.38%] [G loss: 3.092046]\n",
      "epoch:8 step:6649 [D loss: 0.274673, acc.: 89.06%] [G loss: 3.124855]\n",
      "epoch:8 step:6650 [D loss: 0.421450, acc.: 82.81%] [G loss: 1.876855]\n",
      "epoch:8 step:6651 [D loss: 0.384273, acc.: 87.50%] [G loss: 2.634140]\n",
      "epoch:8 step:6652 [D loss: 0.247007, acc.: 90.62%] [G loss: 6.016870]\n",
      "epoch:8 step:6653 [D loss: 0.284047, acc.: 88.28%] [G loss: 3.323952]\n",
      "epoch:8 step:6654 [D loss: 0.276140, acc.: 89.06%] [G loss: 4.123351]\n",
      "epoch:8 step:6655 [D loss: 0.344270, acc.: 85.94%] [G loss: 2.284693]\n",
      "epoch:8 step:6656 [D loss: 0.290513, acc.: 87.50%] [G loss: 2.559631]\n",
      "epoch:8 step:6657 [D loss: 0.310960, acc.: 82.03%] [G loss: 3.248864]\n",
      "epoch:8 step:6658 [D loss: 0.334338, acc.: 88.28%] [G loss: 2.340170]\n",
      "epoch:8 step:6659 [D loss: 0.358559, acc.: 88.28%] [G loss: 1.929285]\n",
      "epoch:8 step:6660 [D loss: 0.402481, acc.: 85.16%] [G loss: 2.900299]\n",
      "epoch:8 step:6661 [D loss: 0.377329, acc.: 85.16%] [G loss: 2.262205]\n",
      "epoch:8 step:6662 [D loss: 0.385605, acc.: 86.72%] [G loss: 3.981897]\n",
      "epoch:8 step:6663 [D loss: 0.312781, acc.: 85.94%] [G loss: 4.995955]\n",
      "epoch:8 step:6664 [D loss: 0.404339, acc.: 84.38%] [G loss: 2.994389]\n",
      "epoch:8 step:6665 [D loss: 0.467625, acc.: 83.59%] [G loss: 3.065622]\n",
      "epoch:8 step:6666 [D loss: 0.414407, acc.: 82.03%] [G loss: 3.533309]\n",
      "epoch:8 step:6667 [D loss: 0.417340, acc.: 78.12%] [G loss: 3.142583]\n",
      "epoch:8 step:6668 [D loss: 0.349607, acc.: 83.59%] [G loss: 2.691225]\n",
      "epoch:8 step:6669 [D loss: 0.371806, acc.: 81.25%] [G loss: 2.492312]\n",
      "epoch:8 step:6670 [D loss: 0.358532, acc.: 85.94%] [G loss: 3.983411]\n",
      "epoch:8 step:6671 [D loss: 0.331315, acc.: 85.16%] [G loss: 2.449650]\n",
      "epoch:8 step:6672 [D loss: 0.427686, acc.: 83.59%] [G loss: 2.270043]\n",
      "epoch:8 step:6673 [D loss: 0.445999, acc.: 83.59%] [G loss: 2.627449]\n",
      "epoch:8 step:6674 [D loss: 0.354758, acc.: 79.69%] [G loss: 2.639337]\n",
      "epoch:8 step:6675 [D loss: 0.253971, acc.: 92.97%] [G loss: 3.305883]\n",
      "epoch:8 step:6676 [D loss: 0.401266, acc.: 82.81%] [G loss: 2.735312]\n",
      "epoch:8 step:6677 [D loss: 0.323108, acc.: 84.38%] [G loss: 2.799250]\n",
      "epoch:8 step:6678 [D loss: 0.282118, acc.: 90.62%] [G loss: 2.095558]\n",
      "epoch:8 step:6679 [D loss: 0.377232, acc.: 86.72%] [G loss: 2.569946]\n",
      "epoch:8 step:6680 [D loss: 0.342036, acc.: 83.59%] [G loss: 3.312522]\n",
      "epoch:8 step:6681 [D loss: 0.346186, acc.: 87.50%] [G loss: 2.477466]\n",
      "epoch:8 step:6682 [D loss: 0.337374, acc.: 85.16%] [G loss: 2.480517]\n",
      "epoch:8 step:6683 [D loss: 0.289929, acc.: 88.28%] [G loss: 2.281383]\n",
      "epoch:8 step:6684 [D loss: 0.368595, acc.: 84.38%] [G loss: 2.946066]\n",
      "epoch:8 step:6685 [D loss: 0.380714, acc.: 85.94%] [G loss: 7.172390]\n",
      "epoch:8 step:6686 [D loss: 0.722809, acc.: 64.06%] [G loss: 3.385994]\n",
      "epoch:8 step:6687 [D loss: 0.797806, acc.: 76.56%] [G loss: 7.717790]\n",
      "epoch:8 step:6688 [D loss: 1.350267, acc.: 60.94%] [G loss: 2.185659]\n",
      "epoch:8 step:6689 [D loss: 0.422469, acc.: 80.47%] [G loss: 2.652017]\n",
      "epoch:8 step:6690 [D loss: 0.269917, acc.: 88.28%] [G loss: 2.804893]\n",
      "epoch:8 step:6691 [D loss: 0.452231, acc.: 81.25%] [G loss: 2.607104]\n",
      "epoch:8 step:6692 [D loss: 0.495007, acc.: 76.56%] [G loss: 2.120373]\n",
      "epoch:8 step:6693 [D loss: 0.300429, acc.: 89.84%] [G loss: 2.487036]\n",
      "epoch:8 step:6694 [D loss: 0.406489, acc.: 82.81%] [G loss: 2.715266]\n",
      "epoch:8 step:6695 [D loss: 0.321571, acc.: 85.94%] [G loss: 2.519955]\n",
      "epoch:8 step:6696 [D loss: 0.318639, acc.: 87.50%] [G loss: 3.521539]\n",
      "epoch:8 step:6697 [D loss: 0.317553, acc.: 85.94%] [G loss: 2.752006]\n",
      "epoch:8 step:6698 [D loss: 0.414793, acc.: 83.59%] [G loss: 2.678819]\n",
      "epoch:8 step:6699 [D loss: 0.253216, acc.: 89.84%] [G loss: 3.644356]\n",
      "epoch:8 step:6700 [D loss: 0.232170, acc.: 88.28%] [G loss: 4.642758]\n",
      "epoch:8 step:6701 [D loss: 0.215249, acc.: 90.62%] [G loss: 4.550356]\n",
      "epoch:8 step:6702 [D loss: 0.299218, acc.: 87.50%] [G loss: 3.325043]\n",
      "epoch:8 step:6703 [D loss: 0.320133, acc.: 87.50%] [G loss: 3.027413]\n",
      "epoch:8 step:6704 [D loss: 0.339038, acc.: 88.28%] [G loss: 2.595473]\n",
      "epoch:8 step:6705 [D loss: 0.296767, acc.: 88.28%] [G loss: 2.353129]\n",
      "epoch:8 step:6706 [D loss: 0.237465, acc.: 95.31%] [G loss: 2.644595]\n",
      "epoch:8 step:6707 [D loss: 0.253119, acc.: 92.19%] [G loss: 3.052137]\n",
      "epoch:8 step:6708 [D loss: 0.252386, acc.: 90.62%] [G loss: 3.185381]\n",
      "epoch:8 step:6709 [D loss: 0.270256, acc.: 89.06%] [G loss: 2.722327]\n",
      "epoch:8 step:6710 [D loss: 0.203673, acc.: 95.31%] [G loss: 2.214345]\n",
      "epoch:8 step:6711 [D loss: 0.314805, acc.: 89.84%] [G loss: 2.227036]\n",
      "epoch:8 step:6712 [D loss: 0.336268, acc.: 88.28%] [G loss: 1.946792]\n",
      "epoch:8 step:6713 [D loss: 0.305591, acc.: 89.84%] [G loss: 2.298031]\n",
      "epoch:8 step:6714 [D loss: 0.326149, acc.: 91.41%] [G loss: 3.067778]\n",
      "epoch:8 step:6715 [D loss: 0.331294, acc.: 87.50%] [G loss: 2.593808]\n",
      "epoch:8 step:6716 [D loss: 0.413900, acc.: 83.59%] [G loss: 1.995918]\n",
      "epoch:8 step:6717 [D loss: 0.312838, acc.: 87.50%] [G loss: 2.344657]\n",
      "epoch:8 step:6718 [D loss: 0.352513, acc.: 85.16%] [G loss: 1.861563]\n",
      "epoch:8 step:6719 [D loss: 0.314019, acc.: 89.84%] [G loss: 2.486193]\n",
      "epoch:8 step:6720 [D loss: 0.417459, acc.: 81.25%] [G loss: 2.270449]\n",
      "epoch:8 step:6721 [D loss: 0.313209, acc.: 88.28%] [G loss: 2.196574]\n",
      "epoch:8 step:6722 [D loss: 0.283479, acc.: 89.06%] [G loss: 2.947473]\n",
      "epoch:8 step:6723 [D loss: 0.338653, acc.: 87.50%] [G loss: 3.209804]\n",
      "epoch:8 step:6724 [D loss: 0.354430, acc.: 85.94%] [G loss: 1.915185]\n",
      "epoch:8 step:6725 [D loss: 0.295838, acc.: 89.84%] [G loss: 2.737828]\n",
      "epoch:8 step:6726 [D loss: 0.276643, acc.: 93.75%] [G loss: 1.867887]\n",
      "epoch:8 step:6727 [D loss: 0.315815, acc.: 87.50%] [G loss: 2.741862]\n",
      "epoch:8 step:6728 [D loss: 0.326670, acc.: 86.72%] [G loss: 2.752527]\n",
      "epoch:8 step:6729 [D loss: 0.313262, acc.: 90.62%] [G loss: 2.093202]\n",
      "epoch:8 step:6730 [D loss: 0.294944, acc.: 89.06%] [G loss: 2.744800]\n",
      "epoch:8 step:6731 [D loss: 0.277244, acc.: 89.84%] [G loss: 2.730936]\n",
      "epoch:8 step:6732 [D loss: 0.309320, acc.: 89.84%] [G loss: 2.375030]\n",
      "epoch:8 step:6733 [D loss: 0.241824, acc.: 93.75%] [G loss: 2.614781]\n",
      "epoch:8 step:6734 [D loss: 0.352274, acc.: 82.81%] [G loss: 1.823900]\n",
      "epoch:8 step:6735 [D loss: 0.285251, acc.: 90.62%] [G loss: 2.506566]\n",
      "epoch:8 step:6736 [D loss: 0.342000, acc.: 86.72%] [G loss: 2.412374]\n",
      "epoch:8 step:6737 [D loss: 0.312415, acc.: 83.59%] [G loss: 3.356926]\n",
      "epoch:8 step:6738 [D loss: 0.288831, acc.: 86.72%] [G loss: 3.242381]\n",
      "epoch:8 step:6739 [D loss: 0.311063, acc.: 87.50%] [G loss: 3.010835]\n",
      "epoch:8 step:6740 [D loss: 0.275542, acc.: 88.28%] [G loss: 3.557289]\n",
      "epoch:8 step:6741 [D loss: 0.313199, acc.: 89.06%] [G loss: 2.264396]\n",
      "epoch:8 step:6742 [D loss: 0.287963, acc.: 86.72%] [G loss: 2.874049]\n",
      "epoch:8 step:6743 [D loss: 0.289872, acc.: 92.97%] [G loss: 3.066728]\n",
      "epoch:8 step:6744 [D loss: 0.326514, acc.: 89.06%] [G loss: 2.418642]\n",
      "epoch:8 step:6745 [D loss: 0.328623, acc.: 85.16%] [G loss: 2.275459]\n",
      "epoch:8 step:6746 [D loss: 0.262908, acc.: 92.97%] [G loss: 2.705305]\n",
      "epoch:8 step:6747 [D loss: 0.230723, acc.: 91.41%] [G loss: 3.694972]\n",
      "epoch:8 step:6748 [D loss: 0.364705, acc.: 79.69%] [G loss: 2.535576]\n",
      "epoch:8 step:6749 [D loss: 0.341633, acc.: 84.38%] [G loss: 2.987746]\n",
      "epoch:8 step:6750 [D loss: 0.471973, acc.: 81.25%] [G loss: 2.103343]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:8 step:6751 [D loss: 0.323378, acc.: 86.72%] [G loss: 2.177487]\n",
      "epoch:8 step:6752 [D loss: 0.313440, acc.: 90.62%] [G loss: 2.153874]\n",
      "epoch:8 step:6753 [D loss: 0.339971, acc.: 89.06%] [G loss: 1.925070]\n",
      "epoch:8 step:6754 [D loss: 0.353568, acc.: 89.06%] [G loss: 2.127235]\n",
      "epoch:8 step:6755 [D loss: 0.324305, acc.: 89.06%] [G loss: 2.569649]\n",
      "epoch:8 step:6756 [D loss: 0.331708, acc.: 89.06%] [G loss: 2.158378]\n",
      "epoch:8 step:6757 [D loss: 0.353352, acc.: 85.94%] [G loss: 2.320185]\n",
      "epoch:8 step:6758 [D loss: 0.286823, acc.: 91.41%] [G loss: 2.973937]\n",
      "epoch:8 step:6759 [D loss: 0.204183, acc.: 90.62%] [G loss: 4.383086]\n",
      "epoch:8 step:6760 [D loss: 0.368279, acc.: 82.81%] [G loss: 4.458461]\n",
      "epoch:8 step:6761 [D loss: 0.424496, acc.: 81.25%] [G loss: 2.825432]\n",
      "epoch:8 step:6762 [D loss: 0.247735, acc.: 86.72%] [G loss: 5.029148]\n",
      "epoch:8 step:6763 [D loss: 0.288086, acc.: 91.41%] [G loss: 2.575241]\n",
      "epoch:8 step:6764 [D loss: 0.265339, acc.: 93.75%] [G loss: 2.087876]\n",
      "epoch:8 step:6765 [D loss: 0.311353, acc.: 86.72%] [G loss: 3.610281]\n",
      "epoch:8 step:6766 [D loss: 0.252382, acc.: 89.84%] [G loss: 2.969991]\n",
      "epoch:8 step:6767 [D loss: 0.292049, acc.: 89.06%] [G loss: 3.464214]\n",
      "epoch:8 step:6768 [D loss: 0.231380, acc.: 91.41%] [G loss: 5.262682]\n",
      "epoch:8 step:6769 [D loss: 0.343811, acc.: 85.16%] [G loss: 2.303341]\n",
      "epoch:8 step:6770 [D loss: 0.274032, acc.: 87.50%] [G loss: 3.466234]\n",
      "epoch:8 step:6771 [D loss: 0.309240, acc.: 87.50%] [G loss: 2.858678]\n",
      "epoch:8 step:6772 [D loss: 0.305744, acc.: 91.41%] [G loss: 2.638521]\n",
      "epoch:8 step:6773 [D loss: 0.458163, acc.: 80.47%] [G loss: 2.292012]\n",
      "epoch:8 step:6774 [D loss: 0.394104, acc.: 85.94%] [G loss: 2.483881]\n",
      "epoch:8 step:6775 [D loss: 0.474973, acc.: 85.94%] [G loss: 2.339025]\n",
      "epoch:8 step:6776 [D loss: 0.320445, acc.: 86.72%] [G loss: 3.538591]\n",
      "epoch:8 step:6777 [D loss: 0.289441, acc.: 87.50%] [G loss: 3.411949]\n",
      "epoch:8 step:6778 [D loss: 0.310283, acc.: 86.72%] [G loss: 2.862768]\n",
      "epoch:8 step:6779 [D loss: 0.443770, acc.: 79.69%] [G loss: 3.018864]\n",
      "epoch:8 step:6780 [D loss: 0.535821, acc.: 75.00%] [G loss: 3.975688]\n",
      "epoch:8 step:6781 [D loss: 0.854419, acc.: 66.41%] [G loss: 5.299295]\n",
      "epoch:8 step:6782 [D loss: 0.741486, acc.: 71.09%] [G loss: 2.762711]\n",
      "epoch:8 step:6783 [D loss: 0.443550, acc.: 83.59%] [G loss: 2.537495]\n",
      "epoch:8 step:6784 [D loss: 0.396680, acc.: 86.72%] [G loss: 3.527084]\n",
      "epoch:8 step:6785 [D loss: 0.431475, acc.: 85.94%] [G loss: 3.343941]\n",
      "epoch:8 step:6786 [D loss: 0.439714, acc.: 80.47%] [G loss: 2.423980]\n",
      "epoch:8 step:6787 [D loss: 0.392054, acc.: 79.69%] [G loss: 3.269171]\n",
      "epoch:8 step:6788 [D loss: 0.313169, acc.: 84.38%] [G loss: 3.765878]\n",
      "epoch:8 step:6789 [D loss: 0.371992, acc.: 86.72%] [G loss: 2.383026]\n",
      "epoch:8 step:6790 [D loss: 0.316984, acc.: 85.94%] [G loss: 2.581939]\n",
      "epoch:8 step:6791 [D loss: 0.339882, acc.: 87.50%] [G loss: 2.176571]\n",
      "epoch:8 step:6792 [D loss: 0.251581, acc.: 91.41%] [G loss: 2.184901]\n",
      "epoch:8 step:6793 [D loss: 0.322001, acc.: 89.84%] [G loss: 2.070485]\n",
      "epoch:8 step:6794 [D loss: 0.284314, acc.: 90.62%] [G loss: 2.272338]\n",
      "epoch:8 step:6795 [D loss: 0.341382, acc.: 82.81%] [G loss: 2.357767]\n",
      "epoch:8 step:6796 [D loss: 0.243228, acc.: 88.28%] [G loss: 2.488241]\n",
      "epoch:8 step:6797 [D loss: 0.439543, acc.: 82.03%] [G loss: 2.793784]\n",
      "epoch:8 step:6798 [D loss: 0.306512, acc.: 86.72%] [G loss: 3.336474]\n",
      "epoch:8 step:6799 [D loss: 0.304920, acc.: 87.50%] [G loss: 3.532099]\n",
      "epoch:8 step:6800 [D loss: 0.426228, acc.: 81.25%] [G loss: 2.669321]\n",
      "##############\n",
      "[0.84050687 0.8849659  0.7769931  0.8075494  0.79269623 0.81217276\n",
      " 0.84448145 0.80095514 0.81045313 0.81864324]\n",
      "##########\n",
      "epoch:8 step:6801 [D loss: 0.286649, acc.: 89.06%] [G loss: 3.063254]\n",
      "epoch:8 step:6802 [D loss: 0.293995, acc.: 89.84%] [G loss: 2.819477]\n",
      "epoch:8 step:6803 [D loss: 0.273275, acc.: 91.41%] [G loss: 2.840904]\n",
      "epoch:8 step:6804 [D loss: 0.349877, acc.: 89.06%] [G loss: 3.315214]\n",
      "epoch:8 step:6805 [D loss: 0.386704, acc.: 85.16%] [G loss: 3.141577]\n",
      "epoch:8 step:6806 [D loss: 0.227084, acc.: 92.97%] [G loss: 4.311202]\n",
      "epoch:8 step:6807 [D loss: 0.360012, acc.: 90.62%] [G loss: 2.723222]\n",
      "epoch:8 step:6808 [D loss: 0.354399, acc.: 85.16%] [G loss: 2.922728]\n",
      "epoch:8 step:6809 [D loss: 0.423047, acc.: 79.69%] [G loss: 2.808105]\n",
      "epoch:8 step:6810 [D loss: 0.361654, acc.: 80.47%] [G loss: 2.794956]\n",
      "epoch:8 step:6811 [D loss: 0.200886, acc.: 92.97%] [G loss: 5.365249]\n",
      "epoch:8 step:6812 [D loss: 0.256300, acc.: 89.06%] [G loss: 3.114218]\n",
      "epoch:8 step:6813 [D loss: 0.390436, acc.: 83.59%] [G loss: 2.091792]\n",
      "epoch:8 step:6814 [D loss: 0.242384, acc.: 92.97%] [G loss: 2.699602]\n",
      "epoch:8 step:6815 [D loss: 0.247197, acc.: 91.41%] [G loss: 3.329194]\n",
      "epoch:8 step:6816 [D loss: 0.390678, acc.: 85.16%] [G loss: 1.776468]\n",
      "epoch:8 step:6817 [D loss: 0.352934, acc.: 85.16%] [G loss: 3.511624]\n",
      "epoch:8 step:6818 [D loss: 0.337009, acc.: 89.06%] [G loss: 2.023449]\n",
      "epoch:8 step:6819 [D loss: 0.344160, acc.: 85.94%] [G loss: 2.208232]\n",
      "epoch:8 step:6820 [D loss: 0.260212, acc.: 91.41%] [G loss: 1.986786]\n",
      "epoch:8 step:6821 [D loss: 0.302932, acc.: 91.41%] [G loss: 2.612221]\n",
      "epoch:8 step:6822 [D loss: 0.378983, acc.: 85.94%] [G loss: 3.494962]\n",
      "epoch:8 step:6823 [D loss: 0.435452, acc.: 81.25%] [G loss: 4.568357]\n",
      "epoch:8 step:6824 [D loss: 0.189696, acc.: 92.19%] [G loss: 4.051763]\n",
      "epoch:8 step:6825 [D loss: 0.306137, acc.: 85.16%] [G loss: 3.959301]\n",
      "epoch:8 step:6826 [D loss: 0.300187, acc.: 89.06%] [G loss: 3.441261]\n",
      "epoch:8 step:6827 [D loss: 0.263136, acc.: 87.50%] [G loss: 2.801888]\n",
      "epoch:8 step:6828 [D loss: 0.325089, acc.: 89.06%] [G loss: 2.513913]\n",
      "epoch:8 step:6829 [D loss: 0.323099, acc.: 89.06%] [G loss: 2.392056]\n",
      "epoch:8 step:6830 [D loss: 0.293157, acc.: 88.28%] [G loss: 2.869806]\n",
      "epoch:8 step:6831 [D loss: 0.394793, acc.: 75.00%] [G loss: 3.034305]\n",
      "epoch:8 step:6832 [D loss: 0.311325, acc.: 87.50%] [G loss: 3.292819]\n",
      "epoch:8 step:6833 [D loss: 0.450946, acc.: 83.59%] [G loss: 2.352276]\n",
      "epoch:8 step:6834 [D loss: 0.268477, acc.: 88.28%] [G loss: 3.561333]\n",
      "epoch:8 step:6835 [D loss: 0.216486, acc.: 89.84%] [G loss: 4.817266]\n",
      "epoch:8 step:6836 [D loss: 0.339840, acc.: 87.50%] [G loss: 2.568926]\n",
      "epoch:8 step:6837 [D loss: 0.392174, acc.: 83.59%] [G loss: 2.282051]\n",
      "epoch:8 step:6838 [D loss: 0.385978, acc.: 84.38%] [G loss: 2.463182]\n",
      "epoch:8 step:6839 [D loss: 0.395512, acc.: 85.94%] [G loss: 2.106685]\n",
      "epoch:8 step:6840 [D loss: 0.313997, acc.: 88.28%] [G loss: 3.369923]\n",
      "epoch:8 step:6841 [D loss: 0.286714, acc.: 86.72%] [G loss: 4.757885]\n",
      "epoch:8 step:6842 [D loss: 0.303182, acc.: 84.38%] [G loss: 2.526166]\n",
      "epoch:8 step:6843 [D loss: 0.335876, acc.: 86.72%] [G loss: 2.275303]\n",
      "epoch:8 step:6844 [D loss: 0.349592, acc.: 82.81%] [G loss: 2.920273]\n",
      "epoch:8 step:6845 [D loss: 0.298842, acc.: 92.19%] [G loss: 2.527742]\n",
      "epoch:8 step:6846 [D loss: 0.414603, acc.: 83.59%] [G loss: 2.974247]\n",
      "epoch:8 step:6847 [D loss: 0.402920, acc.: 82.81%] [G loss: 2.191138]\n",
      "epoch:8 step:6848 [D loss: 0.364571, acc.: 82.03%] [G loss: 2.102474]\n",
      "epoch:8 step:6849 [D loss: 0.248437, acc.: 92.19%] [G loss: 3.868610]\n",
      "epoch:8 step:6850 [D loss: 0.247113, acc.: 93.75%] [G loss: 3.684311]\n",
      "epoch:8 step:6851 [D loss: 0.248435, acc.: 93.75%] [G loss: 2.973717]\n",
      "epoch:8 step:6852 [D loss: 0.447303, acc.: 79.69%] [G loss: 1.983704]\n",
      "epoch:8 step:6853 [D loss: 0.377428, acc.: 82.81%] [G loss: 2.402021]\n",
      "epoch:8 step:6854 [D loss: 0.404375, acc.: 82.03%] [G loss: 2.457862]\n",
      "epoch:8 step:6855 [D loss: 0.355183, acc.: 83.59%] [G loss: 4.423838]\n",
      "epoch:8 step:6856 [D loss: 0.332757, acc.: 85.16%] [G loss: 4.959308]\n",
      "epoch:8 step:6857 [D loss: 0.510168, acc.: 75.78%] [G loss: 4.590257]\n",
      "epoch:8 step:6858 [D loss: 1.083641, acc.: 61.72%] [G loss: 6.370018]\n",
      "epoch:8 step:6859 [D loss: 1.731577, acc.: 61.72%] [G loss: 6.329236]\n",
      "epoch:8 step:6860 [D loss: 0.499424, acc.: 77.34%] [G loss: 4.535038]\n",
      "epoch:8 step:6861 [D loss: 0.848478, acc.: 70.31%] [G loss: 3.798204]\n",
      "epoch:8 step:6862 [D loss: 0.285755, acc.: 86.72%] [G loss: 4.114434]\n",
      "epoch:8 step:6863 [D loss: 0.375007, acc.: 82.03%] [G loss: 3.134638]\n",
      "epoch:8 step:6864 [D loss: 0.406965, acc.: 83.59%] [G loss: 2.415052]\n",
      "epoch:8 step:6865 [D loss: 0.391874, acc.: 83.59%] [G loss: 3.105875]\n",
      "epoch:8 step:6866 [D loss: 0.499567, acc.: 78.12%] [G loss: 2.269416]\n",
      "epoch:8 step:6867 [D loss: 0.251276, acc.: 92.97%] [G loss: 3.169389]\n",
      "epoch:8 step:6868 [D loss: 0.334617, acc.: 85.16%] [G loss: 2.747436]\n",
      "epoch:8 step:6869 [D loss: 0.407061, acc.: 82.03%] [G loss: 2.624895]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:8 step:6870 [D loss: 0.358175, acc.: 82.03%] [G loss: 2.112095]\n",
      "epoch:8 step:6871 [D loss: 0.516426, acc.: 72.66%] [G loss: 2.183819]\n",
      "epoch:8 step:6872 [D loss: 0.397123, acc.: 81.25%] [G loss: 2.266968]\n",
      "epoch:8 step:6873 [D loss: 0.413820, acc.: 82.81%] [G loss: 2.244586]\n",
      "epoch:8 step:6874 [D loss: 0.350011, acc.: 85.94%] [G loss: 2.015765]\n",
      "epoch:8 step:6875 [D loss: 0.360436, acc.: 86.72%] [G loss: 2.667191]\n",
      "epoch:8 step:6876 [D loss: 0.345243, acc.: 83.59%] [G loss: 2.637055]\n",
      "epoch:8 step:6877 [D loss: 0.335376, acc.: 87.50%] [G loss: 3.784005]\n",
      "epoch:8 step:6878 [D loss: 0.251814, acc.: 91.41%] [G loss: 3.371583]\n",
      "epoch:8 step:6879 [D loss: 0.322603, acc.: 87.50%] [G loss: 2.712217]\n",
      "epoch:8 step:6880 [D loss: 0.196812, acc.: 92.97%] [G loss: 4.543619]\n",
      "epoch:8 step:6881 [D loss: 0.441574, acc.: 75.00%] [G loss: 2.265419]\n",
      "epoch:8 step:6882 [D loss: 0.384220, acc.: 82.03%] [G loss: 2.094818]\n",
      "epoch:8 step:6883 [D loss: 0.354643, acc.: 83.59%] [G loss: 2.017004]\n",
      "epoch:8 step:6884 [D loss: 0.319677, acc.: 85.16%] [G loss: 3.521396]\n",
      "epoch:8 step:6885 [D loss: 0.359942, acc.: 85.16%] [G loss: 2.104843]\n",
      "epoch:8 step:6886 [D loss: 0.325506, acc.: 87.50%] [G loss: 2.409376]\n",
      "epoch:8 step:6887 [D loss: 0.465253, acc.: 76.56%] [G loss: 2.355973]\n",
      "epoch:8 step:6888 [D loss: 0.388979, acc.: 85.16%] [G loss: 2.166028]\n",
      "epoch:8 step:6889 [D loss: 0.375379, acc.: 83.59%] [G loss: 2.910230]\n",
      "epoch:8 step:6890 [D loss: 0.366983, acc.: 85.16%] [G loss: 3.070267]\n",
      "epoch:8 step:6891 [D loss: 0.339935, acc.: 85.94%] [G loss: 3.424652]\n",
      "epoch:8 step:6892 [D loss: 0.404536, acc.: 78.91%] [G loss: 2.628415]\n",
      "epoch:8 step:6893 [D loss: 0.353714, acc.: 88.28%] [G loss: 2.723988]\n",
      "epoch:8 step:6894 [D loss: 0.507014, acc.: 77.34%] [G loss: 3.148251]\n",
      "epoch:8 step:6895 [D loss: 0.326339, acc.: 86.72%] [G loss: 4.933062]\n",
      "epoch:8 step:6896 [D loss: 0.385161, acc.: 75.00%] [G loss: 1.857205]\n",
      "epoch:8 step:6897 [D loss: 0.281526, acc.: 86.72%] [G loss: 4.174137]\n",
      "epoch:8 step:6898 [D loss: 0.297021, acc.: 82.81%] [G loss: 4.408879]\n",
      "epoch:8 step:6899 [D loss: 0.294413, acc.: 87.50%] [G loss: 2.965736]\n",
      "epoch:8 step:6900 [D loss: 0.479521, acc.: 78.91%] [G loss: 2.853164]\n",
      "epoch:8 step:6901 [D loss: 0.270170, acc.: 86.72%] [G loss: 3.598756]\n",
      "epoch:8 step:6902 [D loss: 0.299435, acc.: 83.59%] [G loss: 3.759283]\n",
      "epoch:8 step:6903 [D loss: 0.240703, acc.: 90.62%] [G loss: 2.252724]\n",
      "epoch:8 step:6904 [D loss: 0.408113, acc.: 77.34%] [G loss: 2.073514]\n",
      "epoch:8 step:6905 [D loss: 0.448276, acc.: 78.12%] [G loss: 2.832555]\n",
      "epoch:8 step:6906 [D loss: 0.417815, acc.: 85.16%] [G loss: 2.753079]\n",
      "epoch:8 step:6907 [D loss: 0.419457, acc.: 79.69%] [G loss: 2.287082]\n",
      "epoch:8 step:6908 [D loss: 0.411763, acc.: 82.03%] [G loss: 2.767877]\n",
      "epoch:8 step:6909 [D loss: 0.400364, acc.: 83.59%] [G loss: 4.369524]\n",
      "epoch:8 step:6910 [D loss: 0.377570, acc.: 83.59%] [G loss: 5.850446]\n",
      "epoch:8 step:6911 [D loss: 0.439314, acc.: 82.81%] [G loss: 3.045870]\n",
      "epoch:8 step:6912 [D loss: 0.263733, acc.: 89.06%] [G loss: 3.216404]\n",
      "epoch:8 step:6913 [D loss: 0.220566, acc.: 92.19%] [G loss: 4.534220]\n",
      "epoch:8 step:6914 [D loss: 0.262712, acc.: 90.62%] [G loss: 4.952787]\n",
      "epoch:8 step:6915 [D loss: 0.356913, acc.: 85.16%] [G loss: 2.115948]\n",
      "epoch:8 step:6916 [D loss: 0.266610, acc.: 87.50%] [G loss: 3.602242]\n",
      "epoch:8 step:6917 [D loss: 0.303789, acc.: 88.28%] [G loss: 4.367667]\n",
      "epoch:8 step:6918 [D loss: 0.433785, acc.: 76.56%] [G loss: 1.835557]\n",
      "epoch:8 step:6919 [D loss: 0.285164, acc.: 91.41%] [G loss: 2.896338]\n",
      "epoch:8 step:6920 [D loss: 0.438656, acc.: 78.91%] [G loss: 2.556003]\n",
      "epoch:8 step:6921 [D loss: 0.381885, acc.: 82.81%] [G loss: 2.771148]\n",
      "epoch:8 step:6922 [D loss: 0.317104, acc.: 88.28%] [G loss: 2.676631]\n",
      "epoch:8 step:6923 [D loss: 0.339666, acc.: 82.81%] [G loss: 2.270562]\n",
      "epoch:8 step:6924 [D loss: 0.363911, acc.: 85.94%] [G loss: 2.532535]\n",
      "epoch:8 step:6925 [D loss: 0.268716, acc.: 90.62%] [G loss: 3.113207]\n",
      "epoch:8 step:6926 [D loss: 0.328202, acc.: 85.94%] [G loss: 2.848237]\n",
      "epoch:8 step:6927 [D loss: 0.280162, acc.: 89.84%] [G loss: 2.592045]\n",
      "epoch:8 step:6928 [D loss: 0.335248, acc.: 87.50%] [G loss: 2.216900]\n",
      "epoch:8 step:6929 [D loss: 0.311476, acc.: 87.50%] [G loss: 2.318892]\n",
      "epoch:8 step:6930 [D loss: 0.327079, acc.: 84.38%] [G loss: 2.665748]\n",
      "epoch:8 step:6931 [D loss: 0.434436, acc.: 83.59%] [G loss: 3.728024]\n",
      "epoch:8 step:6932 [D loss: 0.399393, acc.: 80.47%] [G loss: 4.463286]\n",
      "epoch:8 step:6933 [D loss: 0.625566, acc.: 68.75%] [G loss: 3.402747]\n",
      "epoch:8 step:6934 [D loss: 0.602982, acc.: 71.09%] [G loss: 4.283842]\n",
      "epoch:8 step:6935 [D loss: 0.417082, acc.: 81.25%] [G loss: 3.278776]\n",
      "epoch:8 step:6936 [D loss: 0.271681, acc.: 89.06%] [G loss: 3.375656]\n",
      "epoch:8 step:6937 [D loss: 0.369577, acc.: 82.03%] [G loss: 3.026627]\n",
      "epoch:8 step:6938 [D loss: 0.496447, acc.: 81.25%] [G loss: 2.340268]\n",
      "epoch:8 step:6939 [D loss: 0.291864, acc.: 92.19%] [G loss: 2.562359]\n",
      "epoch:8 step:6940 [D loss: 0.362982, acc.: 85.16%] [G loss: 2.572362]\n",
      "epoch:8 step:6941 [D loss: 0.303630, acc.: 91.41%] [G loss: 2.537802]\n",
      "epoch:8 step:6942 [D loss: 0.269406, acc.: 88.28%] [G loss: 2.267043]\n",
      "epoch:8 step:6943 [D loss: 0.299259, acc.: 89.84%] [G loss: 2.929384]\n",
      "epoch:8 step:6944 [D loss: 0.257538, acc.: 92.97%] [G loss: 2.739717]\n",
      "epoch:8 step:6945 [D loss: 0.325234, acc.: 87.50%] [G loss: 2.927534]\n",
      "epoch:8 step:6946 [D loss: 0.221875, acc.: 91.41%] [G loss: 2.547328]\n",
      "epoch:8 step:6947 [D loss: 0.342226, acc.: 89.84%] [G loss: 2.467705]\n",
      "epoch:8 step:6948 [D loss: 0.309081, acc.: 84.38%] [G loss: 2.333469]\n",
      "epoch:8 step:6949 [D loss: 0.385557, acc.: 83.59%] [G loss: 2.069955]\n",
      "epoch:8 step:6950 [D loss: 0.297012, acc.: 88.28%] [G loss: 2.547126]\n",
      "epoch:8 step:6951 [D loss: 0.305155, acc.: 88.28%] [G loss: 2.947037]\n",
      "epoch:8 step:6952 [D loss: 0.203258, acc.: 92.19%] [G loss: 3.245426]\n",
      "epoch:8 step:6953 [D loss: 0.247140, acc.: 91.41%] [G loss: 3.120720]\n",
      "epoch:8 step:6954 [D loss: 0.368779, acc.: 85.16%] [G loss: 2.021776]\n",
      "epoch:8 step:6955 [D loss: 0.440197, acc.: 78.91%] [G loss: 1.720520]\n",
      "epoch:8 step:6956 [D loss: 0.400737, acc.: 83.59%] [G loss: 2.750767]\n",
      "epoch:8 step:6957 [D loss: 0.395400, acc.: 81.25%] [G loss: 3.488369]\n",
      "epoch:8 step:6958 [D loss: 0.355946, acc.: 82.81%] [G loss: 2.623096]\n",
      "epoch:8 step:6959 [D loss: 0.278841, acc.: 89.84%] [G loss: 3.352157]\n",
      "epoch:8 step:6960 [D loss: 0.362228, acc.: 84.38%] [G loss: 3.566144]\n",
      "epoch:8 step:6961 [D loss: 0.413536, acc.: 84.38%] [G loss: 2.634037]\n",
      "epoch:8 step:6962 [D loss: 0.277632, acc.: 89.84%] [G loss: 2.700185]\n",
      "epoch:8 step:6963 [D loss: 0.482122, acc.: 75.78%] [G loss: 2.413906]\n",
      "epoch:8 step:6964 [D loss: 0.278858, acc.: 90.62%] [G loss: 2.528923]\n",
      "epoch:8 step:6965 [D loss: 0.387626, acc.: 86.72%] [G loss: 2.215919]\n",
      "epoch:8 step:6966 [D loss: 0.221333, acc.: 92.97%] [G loss: 3.293861]\n",
      "epoch:8 step:6967 [D loss: 0.229112, acc.: 90.62%] [G loss: 6.247680]\n",
      "epoch:8 step:6968 [D loss: 0.231806, acc.: 92.19%] [G loss: 3.786351]\n",
      "epoch:8 step:6969 [D loss: 0.281433, acc.: 90.62%] [G loss: 3.010166]\n",
      "epoch:8 step:6970 [D loss: 0.218553, acc.: 94.53%] [G loss: 2.868824]\n",
      "epoch:8 step:6971 [D loss: 0.328668, acc.: 86.72%] [G loss: 2.832139]\n",
      "epoch:8 step:6972 [D loss: 0.335648, acc.: 86.72%] [G loss: 1.686526]\n",
      "epoch:8 step:6973 [D loss: 0.291671, acc.: 88.28%] [G loss: 2.906609]\n",
      "epoch:8 step:6974 [D loss: 0.390727, acc.: 83.59%] [G loss: 2.016562]\n",
      "epoch:8 step:6975 [D loss: 0.278676, acc.: 89.84%] [G loss: 2.249658]\n",
      "epoch:8 step:6976 [D loss: 0.404554, acc.: 85.16%] [G loss: 2.313898]\n",
      "epoch:8 step:6977 [D loss: 0.364134, acc.: 84.38%] [G loss: 1.970819]\n",
      "epoch:8 step:6978 [D loss: 0.338249, acc.: 83.59%] [G loss: 2.209367]\n",
      "epoch:8 step:6979 [D loss: 0.284113, acc.: 91.41%] [G loss: 2.385136]\n",
      "epoch:8 step:6980 [D loss: 0.318543, acc.: 85.94%] [G loss: 2.449841]\n",
      "epoch:8 step:6981 [D loss: 0.344985, acc.: 88.28%] [G loss: 3.058128]\n",
      "epoch:8 step:6982 [D loss: 0.324275, acc.: 85.16%] [G loss: 3.742332]\n",
      "epoch:8 step:6983 [D loss: 0.486666, acc.: 77.34%] [G loss: 2.599392]\n",
      "epoch:8 step:6984 [D loss: 0.569781, acc.: 78.91%] [G loss: 3.328972]\n",
      "epoch:8 step:6985 [D loss: 0.398745, acc.: 77.34%] [G loss: 2.894303]\n",
      "epoch:8 step:6986 [D loss: 0.385884, acc.: 84.38%] [G loss: 1.976251]\n",
      "epoch:8 step:6987 [D loss: 0.378070, acc.: 81.25%] [G loss: 2.080445]\n",
      "epoch:8 step:6988 [D loss: 0.333755, acc.: 85.16%] [G loss: 2.577299]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:8 step:6989 [D loss: 0.424797, acc.: 82.81%] [G loss: 2.689082]\n",
      "epoch:8 step:6990 [D loss: 0.298587, acc.: 91.41%] [G loss: 3.016224]\n",
      "epoch:8 step:6991 [D loss: 0.227093, acc.: 92.19%] [G loss: 3.043577]\n",
      "epoch:8 step:6992 [D loss: 0.269992, acc.: 89.06%] [G loss: 2.657549]\n",
      "epoch:8 step:6993 [D loss: 0.341837, acc.: 85.16%] [G loss: 2.339698]\n",
      "epoch:8 step:6994 [D loss: 0.250529, acc.: 91.41%] [G loss: 2.630629]\n",
      "epoch:8 step:6995 [D loss: 0.286848, acc.: 89.84%] [G loss: 3.119716]\n",
      "epoch:8 step:6996 [D loss: 0.378400, acc.: 82.81%] [G loss: 2.890896]\n",
      "epoch:8 step:6997 [D loss: 0.316592, acc.: 89.06%] [G loss: 2.377527]\n",
      "epoch:8 step:6998 [D loss: 0.431443, acc.: 80.47%] [G loss: 3.141530]\n",
      "epoch:8 step:6999 [D loss: 0.223161, acc.: 90.62%] [G loss: 5.445210]\n",
      "epoch:8 step:7000 [D loss: 0.299623, acc.: 89.84%] [G loss: 2.708380]\n",
      "##############\n",
      "[0.82022949 0.87507071 0.80015484 0.78899469 0.77039354 0.82111817\n",
      " 0.84368508 0.83991988 0.79884121 0.80708509]\n",
      "##########\n",
      "epoch:8 step:7001 [D loss: 0.270343, acc.: 89.84%] [G loss: 2.962873]\n",
      "epoch:8 step:7002 [D loss: 0.335939, acc.: 87.50%] [G loss: 2.850491]\n",
      "epoch:8 step:7003 [D loss: 0.329630, acc.: 88.28%] [G loss: 2.433057]\n",
      "epoch:8 step:7004 [D loss: 0.305231, acc.: 87.50%] [G loss: 2.692828]\n",
      "epoch:8 step:7005 [D loss: 0.294956, acc.: 90.62%] [G loss: 2.361124]\n",
      "epoch:8 step:7006 [D loss: 0.268797, acc.: 91.41%] [G loss: 3.318666]\n",
      "epoch:8 step:7007 [D loss: 0.318817, acc.: 88.28%] [G loss: 3.040227]\n",
      "epoch:8 step:7008 [D loss: 0.364094, acc.: 82.81%] [G loss: 3.976125]\n",
      "epoch:8 step:7009 [D loss: 0.248831, acc.: 89.84%] [G loss: 4.693521]\n",
      "epoch:8 step:7010 [D loss: 0.407962, acc.: 80.47%] [G loss: 2.883367]\n",
      "epoch:8 step:7011 [D loss: 0.418687, acc.: 78.91%] [G loss: 2.833902]\n",
      "epoch:8 step:7012 [D loss: 0.236345, acc.: 90.62%] [G loss: 2.689339]\n",
      "epoch:8 step:7013 [D loss: 0.334027, acc.: 83.59%] [G loss: 2.787601]\n",
      "epoch:8 step:7014 [D loss: 0.407381, acc.: 82.81%] [G loss: 3.043314]\n",
      "epoch:8 step:7015 [D loss: 0.332819, acc.: 84.38%] [G loss: 2.066506]\n",
      "epoch:8 step:7016 [D loss: 0.346474, acc.: 84.38%] [G loss: 3.003316]\n",
      "epoch:8 step:7017 [D loss: 0.374716, acc.: 85.16%] [G loss: 3.891999]\n",
      "epoch:8 step:7018 [D loss: 0.332282, acc.: 84.38%] [G loss: 3.315214]\n",
      "epoch:8 step:7019 [D loss: 0.432689, acc.: 76.56%] [G loss: 1.956532]\n",
      "epoch:8 step:7020 [D loss: 0.420686, acc.: 79.69%] [G loss: 2.555306]\n",
      "epoch:8 step:7021 [D loss: 0.474913, acc.: 75.00%] [G loss: 3.387771]\n",
      "epoch:8 step:7022 [D loss: 0.579690, acc.: 82.03%] [G loss: 7.106483]\n",
      "epoch:8 step:7023 [D loss: 1.184167, acc.: 66.41%] [G loss: 4.536294]\n",
      "epoch:8 step:7024 [D loss: 1.159415, acc.: 65.62%] [G loss: 5.479359]\n",
      "epoch:8 step:7025 [D loss: 1.094904, acc.: 51.56%] [G loss: 2.768113]\n",
      "epoch:8 step:7026 [D loss: 0.531278, acc.: 75.00%] [G loss: 2.410417]\n",
      "epoch:8 step:7027 [D loss: 0.655004, acc.: 69.53%] [G loss: 4.311638]\n",
      "epoch:8 step:7028 [D loss: 1.037024, acc.: 71.09%] [G loss: 2.634533]\n",
      "epoch:8 step:7029 [D loss: 0.547947, acc.: 76.56%] [G loss: 3.015857]\n",
      "epoch:9 step:7030 [D loss: 0.487009, acc.: 78.91%] [G loss: 2.326884]\n",
      "epoch:9 step:7031 [D loss: 0.355843, acc.: 86.72%] [G loss: 2.545019]\n",
      "epoch:9 step:7032 [D loss: 0.523011, acc.: 75.00%] [G loss: 2.578834]\n",
      "epoch:9 step:7033 [D loss: 0.333904, acc.: 86.72%] [G loss: 1.988279]\n",
      "epoch:9 step:7034 [D loss: 0.284225, acc.: 89.84%] [G loss: 2.222794]\n",
      "epoch:9 step:7035 [D loss: 0.444607, acc.: 81.25%] [G loss: 3.178444]\n",
      "epoch:9 step:7036 [D loss: 0.334819, acc.: 87.50%] [G loss: 2.424288]\n",
      "epoch:9 step:7037 [D loss: 0.424197, acc.: 80.47%] [G loss: 2.784456]\n",
      "epoch:9 step:7038 [D loss: 0.309896, acc.: 86.72%] [G loss: 2.845935]\n",
      "epoch:9 step:7039 [D loss: 0.351955, acc.: 83.59%] [G loss: 2.830690]\n",
      "epoch:9 step:7040 [D loss: 0.243597, acc.: 91.41%] [G loss: 3.720184]\n",
      "epoch:9 step:7041 [D loss: 0.233892, acc.: 89.06%] [G loss: 2.840496]\n",
      "epoch:9 step:7042 [D loss: 0.348593, acc.: 82.03%] [G loss: 2.938409]\n",
      "epoch:9 step:7043 [D loss: 0.371493, acc.: 83.59%] [G loss: 2.240583]\n",
      "epoch:9 step:7044 [D loss: 0.557621, acc.: 72.66%] [G loss: 2.027346]\n",
      "epoch:9 step:7045 [D loss: 0.469301, acc.: 82.81%] [G loss: 3.043616]\n",
      "epoch:9 step:7046 [D loss: 0.316040, acc.: 87.50%] [G loss: 2.711930]\n",
      "epoch:9 step:7047 [D loss: 0.351203, acc.: 85.16%] [G loss: 2.989595]\n",
      "epoch:9 step:7048 [D loss: 0.311694, acc.: 88.28%] [G loss: 4.947370]\n",
      "epoch:9 step:7049 [D loss: 0.313485, acc.: 82.81%] [G loss: 4.229136]\n",
      "epoch:9 step:7050 [D loss: 0.353219, acc.: 83.59%] [G loss: 2.468373]\n",
      "epoch:9 step:7051 [D loss: 0.371174, acc.: 87.50%] [G loss: 2.399950]\n",
      "epoch:9 step:7052 [D loss: 0.281215, acc.: 89.06%] [G loss: 3.241349]\n",
      "epoch:9 step:7053 [D loss: 0.263299, acc.: 87.50%] [G loss: 5.774077]\n",
      "epoch:9 step:7054 [D loss: 0.239894, acc.: 92.97%] [G loss: 5.965500]\n",
      "epoch:9 step:7055 [D loss: 0.405358, acc.: 84.38%] [G loss: 2.121233]\n",
      "epoch:9 step:7056 [D loss: 0.349613, acc.: 83.59%] [G loss: 3.667272]\n",
      "epoch:9 step:7057 [D loss: 0.194220, acc.: 93.75%] [G loss: 3.956546]\n",
      "epoch:9 step:7058 [D loss: 0.378491, acc.: 85.94%] [G loss: 2.397110]\n",
      "epoch:9 step:7059 [D loss: 0.201304, acc.: 92.97%] [G loss: 2.975371]\n",
      "epoch:9 step:7060 [D loss: 0.499532, acc.: 74.22%] [G loss: 2.540516]\n",
      "epoch:9 step:7061 [D loss: 0.410525, acc.: 86.72%] [G loss: 3.345177]\n",
      "epoch:9 step:7062 [D loss: 0.365734, acc.: 87.50%] [G loss: 2.482542]\n",
      "epoch:9 step:7063 [D loss: 0.420335, acc.: 74.22%] [G loss: 3.559328]\n",
      "epoch:9 step:7064 [D loss: 0.324352, acc.: 86.72%] [G loss: 3.590634]\n",
      "epoch:9 step:7065 [D loss: 0.339756, acc.: 85.94%] [G loss: 3.800673]\n",
      "epoch:9 step:7066 [D loss: 0.408825, acc.: 76.56%] [G loss: 3.518600]\n",
      "epoch:9 step:7067 [D loss: 0.439365, acc.: 78.91%] [G loss: 2.275770]\n",
      "epoch:9 step:7068 [D loss: 0.312976, acc.: 85.94%] [G loss: 2.989331]\n",
      "epoch:9 step:7069 [D loss: 0.336666, acc.: 84.38%] [G loss: 2.854760]\n",
      "epoch:9 step:7070 [D loss: 0.262662, acc.: 90.62%] [G loss: 4.030915]\n",
      "epoch:9 step:7071 [D loss: 0.241601, acc.: 93.75%] [G loss: 3.277055]\n",
      "epoch:9 step:7072 [D loss: 0.417790, acc.: 77.34%] [G loss: 2.152210]\n",
      "epoch:9 step:7073 [D loss: 0.275612, acc.: 89.06%] [G loss: 5.585581]\n",
      "epoch:9 step:7074 [D loss: 0.235725, acc.: 90.62%] [G loss: 5.034564]\n",
      "epoch:9 step:7075 [D loss: 0.285597, acc.: 87.50%] [G loss: 2.569277]\n",
      "epoch:9 step:7076 [D loss: 0.326255, acc.: 86.72%] [G loss: 2.004781]\n",
      "epoch:9 step:7077 [D loss: 0.359676, acc.: 85.16%] [G loss: 2.281561]\n",
      "epoch:9 step:7078 [D loss: 0.317895, acc.: 87.50%] [G loss: 2.773882]\n",
      "epoch:9 step:7079 [D loss: 0.402018, acc.: 79.69%] [G loss: 2.954112]\n",
      "epoch:9 step:7080 [D loss: 0.298760, acc.: 91.41%] [G loss: 3.237581]\n",
      "epoch:9 step:7081 [D loss: 0.267485, acc.: 89.84%] [G loss: 2.955452]\n",
      "epoch:9 step:7082 [D loss: 0.326399, acc.: 86.72%] [G loss: 2.445675]\n",
      "epoch:9 step:7083 [D loss: 0.307746, acc.: 85.94%] [G loss: 2.255558]\n",
      "epoch:9 step:7084 [D loss: 0.334155, acc.: 85.94%] [G loss: 2.689882]\n",
      "epoch:9 step:7085 [D loss: 0.349408, acc.: 89.84%] [G loss: 3.269949]\n",
      "epoch:9 step:7086 [D loss: 0.409770, acc.: 83.59%] [G loss: 4.163952]\n",
      "epoch:9 step:7087 [D loss: 0.426341, acc.: 82.03%] [G loss: 2.517388]\n",
      "epoch:9 step:7088 [D loss: 0.330516, acc.: 85.16%] [G loss: 2.826037]\n",
      "epoch:9 step:7089 [D loss: 0.273503, acc.: 90.62%] [G loss: 2.738031]\n",
      "epoch:9 step:7090 [D loss: 0.452016, acc.: 77.34%] [G loss: 2.528345]\n",
      "epoch:9 step:7091 [D loss: 0.340573, acc.: 91.41%] [G loss: 1.967932]\n",
      "epoch:9 step:7092 [D loss: 0.375581, acc.: 84.38%] [G loss: 2.037650]\n",
      "epoch:9 step:7093 [D loss: 0.314045, acc.: 87.50%] [G loss: 2.502323]\n",
      "epoch:9 step:7094 [D loss: 0.357373, acc.: 85.16%] [G loss: 3.419449]\n",
      "epoch:9 step:7095 [D loss: 0.282374, acc.: 85.94%] [G loss: 3.276650]\n",
      "epoch:9 step:7096 [D loss: 0.431764, acc.: 81.25%] [G loss: 2.495161]\n",
      "epoch:9 step:7097 [D loss: 0.379547, acc.: 87.50%] [G loss: 2.295468]\n",
      "epoch:9 step:7098 [D loss: 0.286889, acc.: 89.06%] [G loss: 3.001053]\n",
      "epoch:9 step:7099 [D loss: 0.338594, acc.: 87.50%] [G loss: 4.214137]\n",
      "epoch:9 step:7100 [D loss: 0.424118, acc.: 78.91%] [G loss: 4.246681]\n",
      "epoch:9 step:7101 [D loss: 0.300018, acc.: 86.72%] [G loss: 2.692526]\n",
      "epoch:9 step:7102 [D loss: 0.303107, acc.: 89.06%] [G loss: 2.357277]\n",
      "epoch:9 step:7103 [D loss: 0.273598, acc.: 92.19%] [G loss: 2.583887]\n",
      "epoch:9 step:7104 [D loss: 0.212841, acc.: 92.19%] [G loss: 3.818196]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:9 step:7105 [D loss: 0.361417, acc.: 83.59%] [G loss: 2.369180]\n",
      "epoch:9 step:7106 [D loss: 0.339208, acc.: 87.50%] [G loss: 2.469491]\n",
      "epoch:9 step:7107 [D loss: 0.329911, acc.: 88.28%] [G loss: 3.004258]\n",
      "epoch:9 step:7108 [D loss: 0.327263, acc.: 82.03%] [G loss: 4.188857]\n",
      "epoch:9 step:7109 [D loss: 0.190893, acc.: 94.53%] [G loss: 5.093330]\n",
      "epoch:9 step:7110 [D loss: 0.306351, acc.: 87.50%] [G loss: 3.929001]\n",
      "epoch:9 step:7111 [D loss: 0.207547, acc.: 92.19%] [G loss: 4.233027]\n",
      "epoch:9 step:7112 [D loss: 0.299548, acc.: 88.28%] [G loss: 2.337454]\n",
      "epoch:9 step:7113 [D loss: 0.254331, acc.: 91.41%] [G loss: 4.648818]\n",
      "epoch:9 step:7114 [D loss: 0.353755, acc.: 85.94%] [G loss: 2.874379]\n",
      "epoch:9 step:7115 [D loss: 0.387260, acc.: 84.38%] [G loss: 2.507069]\n",
      "epoch:9 step:7116 [D loss: 0.342210, acc.: 90.62%] [G loss: 2.508450]\n",
      "epoch:9 step:7117 [D loss: 0.385151, acc.: 84.38%] [G loss: 2.699901]\n",
      "epoch:9 step:7118 [D loss: 0.312171, acc.: 89.06%] [G loss: 2.771519]\n",
      "epoch:9 step:7119 [D loss: 0.399081, acc.: 87.50%] [G loss: 2.760305]\n",
      "epoch:9 step:7120 [D loss: 0.418084, acc.: 82.81%] [G loss: 3.526914]\n",
      "epoch:9 step:7121 [D loss: 0.505263, acc.: 78.91%] [G loss: 3.550438]\n",
      "epoch:9 step:7122 [D loss: 0.444767, acc.: 82.03%] [G loss: 2.666929]\n",
      "epoch:9 step:7123 [D loss: 0.347144, acc.: 84.38%] [G loss: 4.218200]\n",
      "epoch:9 step:7124 [D loss: 0.357781, acc.: 82.81%] [G loss: 2.337579]\n",
      "epoch:9 step:7125 [D loss: 0.319442, acc.: 88.28%] [G loss: 2.786839]\n",
      "epoch:9 step:7126 [D loss: 0.299360, acc.: 84.38%] [G loss: 3.607826]\n",
      "epoch:9 step:7127 [D loss: 0.368538, acc.: 82.03%] [G loss: 2.153132]\n",
      "epoch:9 step:7128 [D loss: 0.314745, acc.: 91.41%] [G loss: 2.139800]\n",
      "epoch:9 step:7129 [D loss: 0.310414, acc.: 86.72%] [G loss: 2.785923]\n",
      "epoch:9 step:7130 [D loss: 0.279984, acc.: 88.28%] [G loss: 2.526233]\n",
      "epoch:9 step:7131 [D loss: 0.341257, acc.: 87.50%] [G loss: 1.788959]\n",
      "epoch:9 step:7132 [D loss: 0.416044, acc.: 78.12%] [G loss: 2.036288]\n",
      "epoch:9 step:7133 [D loss: 0.326018, acc.: 89.84%] [G loss: 2.409414]\n",
      "epoch:9 step:7134 [D loss: 0.312031, acc.: 86.72%] [G loss: 2.577103]\n",
      "epoch:9 step:7135 [D loss: 0.455712, acc.: 82.03%] [G loss: 2.738643]\n",
      "epoch:9 step:7136 [D loss: 0.345769, acc.: 89.06%] [G loss: 3.236763]\n",
      "epoch:9 step:7137 [D loss: 0.385870, acc.: 82.81%] [G loss: 2.413175]\n",
      "epoch:9 step:7138 [D loss: 0.227805, acc.: 89.84%] [G loss: 4.777544]\n",
      "epoch:9 step:7139 [D loss: 0.356937, acc.: 88.28%] [G loss: 3.813730]\n",
      "epoch:9 step:7140 [D loss: 0.239520, acc.: 96.09%] [G loss: 3.157882]\n",
      "epoch:9 step:7141 [D loss: 0.353232, acc.: 89.06%] [G loss: 3.828848]\n",
      "epoch:9 step:7142 [D loss: 0.368644, acc.: 85.16%] [G loss: 3.315360]\n",
      "epoch:9 step:7143 [D loss: 0.317372, acc.: 85.16%] [G loss: 2.926212]\n",
      "epoch:9 step:7144 [D loss: 0.271219, acc.: 90.62%] [G loss: 2.972421]\n",
      "epoch:9 step:7145 [D loss: 0.343306, acc.: 89.84%] [G loss: 2.225862]\n",
      "epoch:9 step:7146 [D loss: 0.361643, acc.: 86.72%] [G loss: 2.638459]\n",
      "epoch:9 step:7147 [D loss: 0.295822, acc.: 87.50%] [G loss: 3.024911]\n",
      "epoch:9 step:7148 [D loss: 0.305567, acc.: 89.06%] [G loss: 3.316851]\n",
      "epoch:9 step:7149 [D loss: 0.306271, acc.: 88.28%] [G loss: 3.154141]\n",
      "epoch:9 step:7150 [D loss: 0.330363, acc.: 88.28%] [G loss: 3.260140]\n",
      "epoch:9 step:7151 [D loss: 0.219055, acc.: 91.41%] [G loss: 5.443077]\n",
      "epoch:9 step:7152 [D loss: 0.318039, acc.: 89.84%] [G loss: 2.439692]\n",
      "epoch:9 step:7153 [D loss: 0.310480, acc.: 85.94%] [G loss: 4.492549]\n",
      "epoch:9 step:7154 [D loss: 0.270912, acc.: 89.84%] [G loss: 3.448476]\n",
      "epoch:9 step:7155 [D loss: 0.266860, acc.: 89.84%] [G loss: 3.113515]\n",
      "epoch:9 step:7156 [D loss: 0.210520, acc.: 94.53%] [G loss: 2.684032]\n",
      "epoch:9 step:7157 [D loss: 0.353159, acc.: 85.94%] [G loss: 2.421685]\n",
      "epoch:9 step:7158 [D loss: 0.304286, acc.: 87.50%] [G loss: 3.621777]\n",
      "epoch:9 step:7159 [D loss: 0.249403, acc.: 92.19%] [G loss: 3.824194]\n",
      "epoch:9 step:7160 [D loss: 0.335877, acc.: 82.03%] [G loss: 3.958464]\n",
      "epoch:9 step:7161 [D loss: 0.371291, acc.: 86.72%] [G loss: 2.778406]\n",
      "epoch:9 step:7162 [D loss: 0.258537, acc.: 90.62%] [G loss: 3.045002]\n",
      "epoch:9 step:7163 [D loss: 0.279034, acc.: 86.72%] [G loss: 3.456079]\n",
      "epoch:9 step:7164 [D loss: 0.320938, acc.: 86.72%] [G loss: 4.102017]\n",
      "epoch:9 step:7165 [D loss: 0.308441, acc.: 89.84%] [G loss: 2.786560]\n",
      "epoch:9 step:7166 [D loss: 0.226070, acc.: 93.75%] [G loss: 4.556735]\n",
      "epoch:9 step:7167 [D loss: 0.354582, acc.: 89.06%] [G loss: 3.534272]\n",
      "epoch:9 step:7168 [D loss: 0.362293, acc.: 87.50%] [G loss: 2.725383]\n",
      "epoch:9 step:7169 [D loss: 0.257245, acc.: 92.19%] [G loss: 2.674251]\n",
      "epoch:9 step:7170 [D loss: 0.336828, acc.: 86.72%] [G loss: 3.613300]\n",
      "epoch:9 step:7171 [D loss: 0.200120, acc.: 92.97%] [G loss: 3.888269]\n",
      "epoch:9 step:7172 [D loss: 0.169056, acc.: 95.31%] [G loss: 4.323339]\n",
      "epoch:9 step:7173 [D loss: 0.335351, acc.: 89.84%] [G loss: 2.878884]\n",
      "epoch:9 step:7174 [D loss: 0.335791, acc.: 86.72%] [G loss: 1.833534]\n",
      "epoch:9 step:7175 [D loss: 0.358874, acc.: 87.50%] [G loss: 2.231499]\n",
      "epoch:9 step:7176 [D loss: 0.350779, acc.: 88.28%] [G loss: 2.558096]\n",
      "epoch:9 step:7177 [D loss: 0.299473, acc.: 87.50%] [G loss: 2.970417]\n",
      "epoch:9 step:7178 [D loss: 0.299097, acc.: 92.19%] [G loss: 2.177067]\n",
      "epoch:9 step:7179 [D loss: 0.273361, acc.: 90.62%] [G loss: 2.890970]\n",
      "epoch:9 step:7180 [D loss: 0.292459, acc.: 89.84%] [G loss: 2.517726]\n",
      "epoch:9 step:7181 [D loss: 0.392652, acc.: 86.72%] [G loss: 2.589640]\n",
      "epoch:9 step:7182 [D loss: 0.320113, acc.: 89.06%] [G loss: 3.505510]\n",
      "epoch:9 step:7183 [D loss: 0.325707, acc.: 86.72%] [G loss: 5.062462]\n",
      "epoch:9 step:7184 [D loss: 0.605342, acc.: 74.22%] [G loss: 4.669491]\n",
      "epoch:9 step:7185 [D loss: 0.680805, acc.: 76.56%] [G loss: 4.916255]\n",
      "epoch:9 step:7186 [D loss: 0.377581, acc.: 83.59%] [G loss: 3.813130]\n",
      "epoch:9 step:7187 [D loss: 0.454119, acc.: 82.03%] [G loss: 2.756522]\n",
      "epoch:9 step:7188 [D loss: 0.239199, acc.: 89.84%] [G loss: 3.393382]\n",
      "epoch:9 step:7189 [D loss: 0.326262, acc.: 90.62%] [G loss: 3.337628]\n",
      "epoch:9 step:7190 [D loss: 0.415202, acc.: 81.25%] [G loss: 2.739823]\n",
      "epoch:9 step:7191 [D loss: 0.255666, acc.: 89.06%] [G loss: 2.783613]\n",
      "epoch:9 step:7192 [D loss: 0.288125, acc.: 89.84%] [G loss: 3.858171]\n",
      "epoch:9 step:7193 [D loss: 0.232881, acc.: 92.19%] [G loss: 4.014342]\n",
      "epoch:9 step:7194 [D loss: 0.285728, acc.: 88.28%] [G loss: 4.241119]\n",
      "epoch:9 step:7195 [D loss: 0.302937, acc.: 90.62%] [G loss: 4.196550]\n",
      "epoch:9 step:7196 [D loss: 0.353696, acc.: 85.16%] [G loss: 3.055289]\n",
      "epoch:9 step:7197 [D loss: 0.350134, acc.: 83.59%] [G loss: 2.728479]\n",
      "epoch:9 step:7198 [D loss: 0.455834, acc.: 79.69%] [G loss: 2.263181]\n",
      "epoch:9 step:7199 [D loss: 0.285522, acc.: 92.97%] [G loss: 3.616500]\n",
      "epoch:9 step:7200 [D loss: 0.222111, acc.: 90.62%] [G loss: 5.900484]\n",
      "##############\n",
      "[0.82593776 0.89330152 0.76479503 0.78673836 0.75732435 0.80158229\n",
      " 0.88885506 0.83694266 0.8338798  0.7979699 ]\n",
      "##########\n",
      "epoch:9 step:7201 [D loss: 0.251392, acc.: 89.06%] [G loss: 3.256041]\n",
      "epoch:9 step:7202 [D loss: 0.179150, acc.: 92.97%] [G loss: 5.186190]\n",
      "epoch:9 step:7203 [D loss: 0.315759, acc.: 85.94%] [G loss: 3.366141]\n",
      "epoch:9 step:7204 [D loss: 0.221579, acc.: 92.97%] [G loss: 3.256297]\n",
      "epoch:9 step:7205 [D loss: 0.284627, acc.: 89.06%] [G loss: 3.233828]\n",
      "epoch:9 step:7206 [D loss: 0.352170, acc.: 81.25%] [G loss: 2.032050]\n",
      "epoch:9 step:7207 [D loss: 0.353564, acc.: 83.59%] [G loss: 2.350197]\n",
      "epoch:9 step:7208 [D loss: 0.310457, acc.: 85.16%] [G loss: 3.836595]\n",
      "epoch:9 step:7209 [D loss: 0.400080, acc.: 87.50%] [G loss: 2.652035]\n",
      "epoch:9 step:7210 [D loss: 0.345671, acc.: 88.28%] [G loss: 2.803650]\n",
      "epoch:9 step:7211 [D loss: 0.324882, acc.: 85.16%] [G loss: 3.044621]\n",
      "epoch:9 step:7212 [D loss: 0.291632, acc.: 86.72%] [G loss: 3.454233]\n",
      "epoch:9 step:7213 [D loss: 0.408523, acc.: 84.38%] [G loss: 3.250460]\n",
      "epoch:9 step:7214 [D loss: 0.276937, acc.: 89.84%] [G loss: 3.083739]\n",
      "epoch:9 step:7215 [D loss: 0.235566, acc.: 94.53%] [G loss: 2.269837]\n",
      "epoch:9 step:7216 [D loss: 0.345819, acc.: 85.16%] [G loss: 2.267043]\n",
      "epoch:9 step:7217 [D loss: 0.328353, acc.: 86.72%] [G loss: 2.641882]\n",
      "epoch:9 step:7218 [D loss: 0.348996, acc.: 88.28%] [G loss: 2.194301]\n",
      "epoch:9 step:7219 [D loss: 0.281566, acc.: 86.72%] [G loss: 3.498998]\n",
      "epoch:9 step:7220 [D loss: 0.276331, acc.: 88.28%] [G loss: 4.100371]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:9 step:7221 [D loss: 0.299529, acc.: 84.38%] [G loss: 2.949745]\n",
      "epoch:9 step:7222 [D loss: 0.272961, acc.: 92.19%] [G loss: 3.086635]\n",
      "epoch:9 step:7223 [D loss: 0.244269, acc.: 89.84%] [G loss: 2.776263]\n",
      "epoch:9 step:7224 [D loss: 0.369282, acc.: 85.94%] [G loss: 3.922302]\n",
      "epoch:9 step:7225 [D loss: 0.226264, acc.: 91.41%] [G loss: 3.616194]\n",
      "epoch:9 step:7226 [D loss: 0.424704, acc.: 78.12%] [G loss: 2.869287]\n",
      "epoch:9 step:7227 [D loss: 0.285892, acc.: 89.06%] [G loss: 3.708606]\n",
      "epoch:9 step:7228 [D loss: 0.353887, acc.: 85.16%] [G loss: 2.858751]\n",
      "epoch:9 step:7229 [D loss: 0.244896, acc.: 91.41%] [G loss: 3.761722]\n",
      "epoch:9 step:7230 [D loss: 0.343678, acc.: 85.94%] [G loss: 5.184785]\n",
      "epoch:9 step:7231 [D loss: 0.408562, acc.: 78.91%] [G loss: 2.659558]\n",
      "epoch:9 step:7232 [D loss: 0.282012, acc.: 90.62%] [G loss: 3.053739]\n",
      "epoch:9 step:7233 [D loss: 0.257973, acc.: 88.28%] [G loss: 4.244722]\n",
      "epoch:9 step:7234 [D loss: 0.290805, acc.: 89.84%] [G loss: 2.840394]\n",
      "epoch:9 step:7235 [D loss: 0.302348, acc.: 82.03%] [G loss: 3.224925]\n",
      "epoch:9 step:7236 [D loss: 0.302683, acc.: 85.16%] [G loss: 2.453502]\n",
      "epoch:9 step:7237 [D loss: 0.334859, acc.: 84.38%] [G loss: 2.300251]\n",
      "epoch:9 step:7238 [D loss: 0.314975, acc.: 88.28%] [G loss: 3.241256]\n",
      "epoch:9 step:7239 [D loss: 0.312759, acc.: 88.28%] [G loss: 3.966413]\n",
      "epoch:9 step:7240 [D loss: 0.442516, acc.: 76.56%] [G loss: 2.709821]\n",
      "epoch:9 step:7241 [D loss: 0.374562, acc.: 85.94%] [G loss: 3.033936]\n",
      "epoch:9 step:7242 [D loss: 0.309506, acc.: 83.59%] [G loss: 5.332655]\n",
      "epoch:9 step:7243 [D loss: 0.377417, acc.: 82.03%] [G loss: 2.806782]\n",
      "epoch:9 step:7244 [D loss: 0.307164, acc.: 85.94%] [G loss: 4.677433]\n",
      "epoch:9 step:7245 [D loss: 0.232752, acc.: 89.06%] [G loss: 8.119886]\n",
      "epoch:9 step:7246 [D loss: 0.332397, acc.: 87.50%] [G loss: 3.397938]\n",
      "epoch:9 step:7247 [D loss: 0.303512, acc.: 87.50%] [G loss: 3.267162]\n",
      "epoch:9 step:7248 [D loss: 0.365888, acc.: 82.81%] [G loss: 2.120695]\n",
      "epoch:9 step:7249 [D loss: 0.363047, acc.: 82.03%] [G loss: 2.296856]\n",
      "epoch:9 step:7250 [D loss: 0.325421, acc.: 89.06%] [G loss: 2.153464]\n",
      "epoch:9 step:7251 [D loss: 0.290315, acc.: 90.62%] [G loss: 2.769302]\n",
      "epoch:9 step:7252 [D loss: 0.387213, acc.: 85.16%] [G loss: 3.424358]\n",
      "epoch:9 step:7253 [D loss: 0.426209, acc.: 83.59%] [G loss: 2.485148]\n",
      "epoch:9 step:7254 [D loss: 0.366003, acc.: 84.38%] [G loss: 3.101136]\n",
      "epoch:9 step:7255 [D loss: 0.319581, acc.: 89.06%] [G loss: 3.227150]\n",
      "epoch:9 step:7256 [D loss: 0.347480, acc.: 82.03%] [G loss: 3.718745]\n",
      "epoch:9 step:7257 [D loss: 0.451522, acc.: 80.47%] [G loss: 4.474808]\n",
      "epoch:9 step:7258 [D loss: 1.393850, acc.: 51.56%] [G loss: 4.630864]\n",
      "epoch:9 step:7259 [D loss: 1.284295, acc.: 61.72%] [G loss: 6.260001]\n",
      "epoch:9 step:7260 [D loss: 1.161990, acc.: 56.25%] [G loss: 2.736922]\n",
      "epoch:9 step:7261 [D loss: 0.488352, acc.: 80.47%] [G loss: 2.456756]\n",
      "epoch:9 step:7262 [D loss: 0.449942, acc.: 79.69%] [G loss: 3.037480]\n",
      "epoch:9 step:7263 [D loss: 0.450578, acc.: 77.34%] [G loss: 2.514907]\n",
      "epoch:9 step:7264 [D loss: 0.343908, acc.: 83.59%] [G loss: 2.611483]\n",
      "epoch:9 step:7265 [D loss: 0.346486, acc.: 85.94%] [G loss: 3.007493]\n",
      "epoch:9 step:7266 [D loss: 0.255691, acc.: 92.19%] [G loss: 2.103722]\n",
      "epoch:9 step:7267 [D loss: 0.401153, acc.: 83.59%] [G loss: 2.426672]\n",
      "epoch:9 step:7268 [D loss: 0.336161, acc.: 88.28%] [G loss: 2.815650]\n",
      "epoch:9 step:7269 [D loss: 0.220672, acc.: 93.75%] [G loss: 3.677151]\n",
      "epoch:9 step:7270 [D loss: 0.323391, acc.: 87.50%] [G loss: 2.359316]\n",
      "epoch:9 step:7271 [D loss: 0.309839, acc.: 89.06%] [G loss: 2.464614]\n",
      "epoch:9 step:7272 [D loss: 0.308060, acc.: 88.28%] [G loss: 3.342241]\n",
      "epoch:9 step:7273 [D loss: 0.316305, acc.: 87.50%] [G loss: 2.626178]\n",
      "epoch:9 step:7274 [D loss: 0.365402, acc.: 84.38%] [G loss: 1.938390]\n",
      "epoch:9 step:7275 [D loss: 0.347955, acc.: 81.25%] [G loss: 2.613990]\n",
      "epoch:9 step:7276 [D loss: 0.331559, acc.: 87.50%] [G loss: 3.276302]\n",
      "epoch:9 step:7277 [D loss: 0.231742, acc.: 92.97%] [G loss: 5.513083]\n",
      "epoch:9 step:7278 [D loss: 0.322359, acc.: 88.28%] [G loss: 2.816977]\n",
      "epoch:9 step:7279 [D loss: 0.334770, acc.: 86.72%] [G loss: 2.875889]\n",
      "epoch:9 step:7280 [D loss: 0.316450, acc.: 89.06%] [G loss: 3.245519]\n",
      "epoch:9 step:7281 [D loss: 0.364983, acc.: 81.25%] [G loss: 2.976610]\n",
      "epoch:9 step:7282 [D loss: 0.283961, acc.: 88.28%] [G loss: 3.332489]\n",
      "epoch:9 step:7283 [D loss: 0.314721, acc.: 85.94%] [G loss: 2.159704]\n",
      "epoch:9 step:7284 [D loss: 0.294593, acc.: 86.72%] [G loss: 2.401456]\n",
      "epoch:9 step:7285 [D loss: 0.331050, acc.: 87.50%] [G loss: 3.223998]\n",
      "epoch:9 step:7286 [D loss: 0.256946, acc.: 88.28%] [G loss: 5.240409]\n",
      "epoch:9 step:7287 [D loss: 0.340772, acc.: 82.81%] [G loss: 3.715489]\n",
      "epoch:9 step:7288 [D loss: 0.362767, acc.: 85.94%] [G loss: 2.937034]\n",
      "epoch:9 step:7289 [D loss: 0.235449, acc.: 90.62%] [G loss: 5.596607]\n",
      "epoch:9 step:7290 [D loss: 0.502154, acc.: 78.12%] [G loss: 3.209728]\n",
      "epoch:9 step:7291 [D loss: 0.406289, acc.: 79.69%] [G loss: 2.209988]\n",
      "epoch:9 step:7292 [D loss: 0.301506, acc.: 88.28%] [G loss: 3.556942]\n",
      "epoch:9 step:7293 [D loss: 0.204039, acc.: 95.31%] [G loss: 4.694794]\n",
      "epoch:9 step:7294 [D loss: 0.310036, acc.: 89.84%] [G loss: 3.266419]\n",
      "epoch:9 step:7295 [D loss: 0.227244, acc.: 94.53%] [G loss: 3.122018]\n",
      "epoch:9 step:7296 [D loss: 0.270393, acc.: 90.62%] [G loss: 2.065865]\n",
      "epoch:9 step:7297 [D loss: 0.411065, acc.: 85.16%] [G loss: 2.647099]\n",
      "epoch:9 step:7298 [D loss: 0.382320, acc.: 84.38%] [G loss: 2.323841]\n",
      "epoch:9 step:7299 [D loss: 0.320211, acc.: 85.16%] [G loss: 3.456637]\n",
      "epoch:9 step:7300 [D loss: 0.344135, acc.: 82.81%] [G loss: 3.431306]\n",
      "epoch:9 step:7301 [D loss: 0.216617, acc.: 89.06%] [G loss: 7.040902]\n",
      "epoch:9 step:7302 [D loss: 0.209028, acc.: 94.53%] [G loss: 4.213110]\n",
      "epoch:9 step:7303 [D loss: 0.241470, acc.: 92.97%] [G loss: 3.904881]\n",
      "epoch:9 step:7304 [D loss: 0.505341, acc.: 71.88%] [G loss: 2.950794]\n",
      "epoch:9 step:7305 [D loss: 0.285447, acc.: 87.50%] [G loss: 7.030107]\n",
      "epoch:9 step:7306 [D loss: 0.293248, acc.: 86.72%] [G loss: 3.099889]\n",
      "epoch:9 step:7307 [D loss: 0.261279, acc.: 92.19%] [G loss: 2.952025]\n",
      "epoch:9 step:7308 [D loss: 0.288197, acc.: 89.84%] [G loss: 2.678808]\n",
      "epoch:9 step:7309 [D loss: 0.235965, acc.: 93.75%] [G loss: 2.753036]\n",
      "epoch:9 step:7310 [D loss: 0.354023, acc.: 85.16%] [G loss: 3.817911]\n",
      "epoch:9 step:7311 [D loss: 0.460172, acc.: 78.12%] [G loss: 2.946446]\n",
      "epoch:9 step:7312 [D loss: 0.339327, acc.: 85.16%] [G loss: 2.521941]\n",
      "epoch:9 step:7313 [D loss: 0.271498, acc.: 90.62%] [G loss: 2.925829]\n",
      "epoch:9 step:7314 [D loss: 0.405017, acc.: 80.47%] [G loss: 2.347912]\n",
      "epoch:9 step:7315 [D loss: 0.284732, acc.: 89.06%] [G loss: 3.087774]\n",
      "epoch:9 step:7316 [D loss: 0.256854, acc.: 89.84%] [G loss: 2.658184]\n",
      "epoch:9 step:7317 [D loss: 0.335122, acc.: 86.72%] [G loss: 2.672950]\n",
      "epoch:9 step:7318 [D loss: 0.280342, acc.: 89.84%] [G loss: 2.855211]\n",
      "epoch:9 step:7319 [D loss: 0.181913, acc.: 95.31%] [G loss: 3.287766]\n",
      "epoch:9 step:7320 [D loss: 0.241624, acc.: 92.19%] [G loss: 2.869196]\n",
      "epoch:9 step:7321 [D loss: 0.275797, acc.: 93.75%] [G loss: 3.196183]\n",
      "epoch:9 step:7322 [D loss: 0.248279, acc.: 93.75%] [G loss: 3.011082]\n",
      "epoch:9 step:7323 [D loss: 0.316224, acc.: 85.16%] [G loss: 3.122724]\n",
      "epoch:9 step:7324 [D loss: 0.251055, acc.: 91.41%] [G loss: 2.647710]\n",
      "epoch:9 step:7325 [D loss: 0.255643, acc.: 90.62%] [G loss: 2.980544]\n",
      "epoch:9 step:7326 [D loss: 0.301855, acc.: 88.28%] [G loss: 3.696481]\n",
      "epoch:9 step:7327 [D loss: 0.279589, acc.: 88.28%] [G loss: 2.462652]\n",
      "epoch:9 step:7328 [D loss: 0.316089, acc.: 85.94%] [G loss: 5.353539]\n",
      "epoch:9 step:7329 [D loss: 0.353856, acc.: 86.72%] [G loss: 3.490350]\n",
      "epoch:9 step:7330 [D loss: 0.324391, acc.: 84.38%] [G loss: 4.132190]\n",
      "epoch:9 step:7331 [D loss: 0.378560, acc.: 84.38%] [G loss: 3.903577]\n",
      "epoch:9 step:7332 [D loss: 0.365004, acc.: 82.81%] [G loss: 2.943572]\n",
      "epoch:9 step:7333 [D loss: 0.225773, acc.: 93.75%] [G loss: 2.901320]\n",
      "epoch:9 step:7334 [D loss: 0.311363, acc.: 86.72%] [G loss: 3.035885]\n",
      "epoch:9 step:7335 [D loss: 0.363616, acc.: 84.38%] [G loss: 2.648986]\n",
      "epoch:9 step:7336 [D loss: 0.347039, acc.: 85.94%] [G loss: 2.987175]\n",
      "epoch:9 step:7337 [D loss: 0.409519, acc.: 82.03%] [G loss: 2.558507]\n",
      "epoch:9 step:7338 [D loss: 0.386383, acc.: 83.59%] [G loss: 3.012047]\n",
      "epoch:9 step:7339 [D loss: 0.405083, acc.: 88.28%] [G loss: 4.389367]\n",
      "epoch:9 step:7340 [D loss: 0.353250, acc.: 84.38%] [G loss: 3.632307]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:9 step:7341 [D loss: 0.272878, acc.: 90.62%] [G loss: 2.990043]\n",
      "epoch:9 step:7342 [D loss: 0.279472, acc.: 86.72%] [G loss: 4.163526]\n",
      "epoch:9 step:7343 [D loss: 0.334379, acc.: 85.16%] [G loss: 3.481600]\n",
      "epoch:9 step:7344 [D loss: 0.290517, acc.: 88.28%] [G loss: 4.099638]\n",
      "epoch:9 step:7345 [D loss: 0.438766, acc.: 82.81%] [G loss: 5.747865]\n",
      "epoch:9 step:7346 [D loss: 1.184401, acc.: 63.28%] [G loss: 8.222311]\n",
      "epoch:9 step:7347 [D loss: 2.549419, acc.: 53.91%] [G loss: 3.065045]\n",
      "epoch:9 step:7348 [D loss: 0.857147, acc.: 73.44%] [G loss: 6.523764]\n",
      "epoch:9 step:7349 [D loss: 1.425109, acc.: 54.69%] [G loss: 3.212776]\n",
      "epoch:9 step:7350 [D loss: 0.500729, acc.: 78.12%] [G loss: 3.512931]\n",
      "epoch:9 step:7351 [D loss: 0.669878, acc.: 77.34%] [G loss: 2.761034]\n",
      "epoch:9 step:7352 [D loss: 0.568428, acc.: 75.78%] [G loss: 2.822395]\n",
      "epoch:9 step:7353 [D loss: 0.508344, acc.: 76.56%] [G loss: 2.118679]\n",
      "epoch:9 step:7354 [D loss: 0.415392, acc.: 78.12%] [G loss: 2.469230]\n",
      "epoch:9 step:7355 [D loss: 0.279790, acc.: 90.62%] [G loss: 2.695518]\n",
      "epoch:9 step:7356 [D loss: 0.331101, acc.: 86.72%] [G loss: 2.081788]\n",
      "epoch:9 step:7357 [D loss: 0.271865, acc.: 92.97%] [G loss: 2.526863]\n",
      "epoch:9 step:7358 [D loss: 0.259321, acc.: 91.41%] [G loss: 2.144306]\n",
      "epoch:9 step:7359 [D loss: 0.380443, acc.: 82.03%] [G loss: 2.307055]\n",
      "epoch:9 step:7360 [D loss: 0.342465, acc.: 89.06%] [G loss: 2.158254]\n",
      "epoch:9 step:7361 [D loss: 0.334651, acc.: 87.50%] [G loss: 2.987832]\n",
      "epoch:9 step:7362 [D loss: 0.239699, acc.: 88.28%] [G loss: 3.405893]\n",
      "epoch:9 step:7363 [D loss: 0.395250, acc.: 78.91%] [G loss: 2.658463]\n",
      "epoch:9 step:7364 [D loss: 0.309555, acc.: 88.28%] [G loss: 2.771623]\n",
      "epoch:9 step:7365 [D loss: 0.340124, acc.: 87.50%] [G loss: 3.222874]\n",
      "epoch:9 step:7366 [D loss: 0.284023, acc.: 88.28%] [G loss: 2.527566]\n",
      "epoch:9 step:7367 [D loss: 0.383105, acc.: 81.25%] [G loss: 2.082192]\n",
      "epoch:9 step:7368 [D loss: 0.345223, acc.: 87.50%] [G loss: 1.973048]\n",
      "epoch:9 step:7369 [D loss: 0.374198, acc.: 81.25%] [G loss: 1.978670]\n",
      "epoch:9 step:7370 [D loss: 0.320710, acc.: 85.94%] [G loss: 3.740890]\n",
      "epoch:9 step:7371 [D loss: 0.312643, acc.: 82.81%] [G loss: 5.337061]\n",
      "epoch:9 step:7372 [D loss: 0.399152, acc.: 81.25%] [G loss: 2.109700]\n",
      "epoch:9 step:7373 [D loss: 0.416185, acc.: 79.69%] [G loss: 2.944725]\n",
      "epoch:9 step:7374 [D loss: 0.288971, acc.: 84.38%] [G loss: 3.363738]\n",
      "epoch:9 step:7375 [D loss: 0.428260, acc.: 80.47%] [G loss: 3.046179]\n",
      "epoch:9 step:7376 [D loss: 0.339241, acc.: 87.50%] [G loss: 1.924785]\n",
      "epoch:9 step:7377 [D loss: 0.283092, acc.: 90.62%] [G loss: 2.733344]\n",
      "epoch:9 step:7378 [D loss: 0.275025, acc.: 85.16%] [G loss: 3.608970]\n",
      "epoch:9 step:7379 [D loss: 0.302709, acc.: 83.59%] [G loss: 3.907881]\n",
      "epoch:9 step:7380 [D loss: 0.454319, acc.: 79.69%] [G loss: 2.611911]\n",
      "epoch:9 step:7381 [D loss: 0.379971, acc.: 78.91%] [G loss: 3.660762]\n",
      "epoch:9 step:7382 [D loss: 0.216137, acc.: 92.19%] [G loss: 4.013599]\n",
      "epoch:9 step:7383 [D loss: 0.221353, acc.: 92.97%] [G loss: 2.755821]\n",
      "epoch:9 step:7384 [D loss: 0.351247, acc.: 82.81%] [G loss: 2.656082]\n",
      "epoch:9 step:7385 [D loss: 0.389815, acc.: 83.59%] [G loss: 1.860864]\n",
      "epoch:9 step:7386 [D loss: 0.366946, acc.: 83.59%] [G loss: 2.088740]\n",
      "epoch:9 step:7387 [D loss: 0.460071, acc.: 75.78%] [G loss: 2.496811]\n",
      "epoch:9 step:7388 [D loss: 0.370102, acc.: 82.81%] [G loss: 2.655649]\n",
      "epoch:9 step:7389 [D loss: 0.314743, acc.: 83.59%] [G loss: 2.960191]\n",
      "epoch:9 step:7390 [D loss: 0.311833, acc.: 85.94%] [G loss: 2.348488]\n",
      "epoch:9 step:7391 [D loss: 0.506036, acc.: 78.91%] [G loss: 2.005670]\n",
      "epoch:9 step:7392 [D loss: 0.446338, acc.: 79.69%] [G loss: 2.533030]\n",
      "epoch:9 step:7393 [D loss: 0.284791, acc.: 89.06%] [G loss: 3.573124]\n",
      "epoch:9 step:7394 [D loss: 0.287638, acc.: 87.50%] [G loss: 2.534481]\n",
      "epoch:9 step:7395 [D loss: 0.382639, acc.: 83.59%] [G loss: 2.258222]\n",
      "epoch:9 step:7396 [D loss: 0.435258, acc.: 82.81%] [G loss: 2.187165]\n",
      "epoch:9 step:7397 [D loss: 0.425874, acc.: 82.81%] [G loss: 2.704631]\n",
      "epoch:9 step:7398 [D loss: 0.326169, acc.: 85.16%] [G loss: 2.864924]\n",
      "epoch:9 step:7399 [D loss: 0.224749, acc.: 89.84%] [G loss: 3.803316]\n",
      "epoch:9 step:7400 [D loss: 0.377655, acc.: 82.81%] [G loss: 2.387304]\n",
      "##############\n",
      "[0.84013792 0.87949954 0.79807737 0.7796084  0.77524631 0.82957689\n",
      " 0.88239338 0.82671275 0.84552526 0.81142889]\n",
      "##########\n",
      "epoch:9 step:7401 [D loss: 0.284552, acc.: 86.72%] [G loss: 2.461890]\n",
      "epoch:9 step:7402 [D loss: 0.351781, acc.: 85.16%] [G loss: 2.909435]\n",
      "epoch:9 step:7403 [D loss: 0.411008, acc.: 79.69%] [G loss: 2.415151]\n",
      "epoch:9 step:7404 [D loss: 0.295783, acc.: 88.28%] [G loss: 3.642186]\n",
      "epoch:9 step:7405 [D loss: 0.320880, acc.: 86.72%] [G loss: 2.453820]\n",
      "epoch:9 step:7406 [D loss: 0.225875, acc.: 94.53%] [G loss: 3.046168]\n",
      "epoch:9 step:7407 [D loss: 0.252477, acc.: 89.84%] [G loss: 3.837502]\n",
      "epoch:9 step:7408 [D loss: 0.281244, acc.: 89.84%] [G loss: 2.852901]\n",
      "epoch:9 step:7409 [D loss: 0.228170, acc.: 92.97%] [G loss: 4.534451]\n",
      "epoch:9 step:7410 [D loss: 0.230511, acc.: 90.62%] [G loss: 3.923147]\n",
      "epoch:9 step:7411 [D loss: 0.381823, acc.: 88.28%] [G loss: 2.519302]\n",
      "epoch:9 step:7412 [D loss: 0.318148, acc.: 87.50%] [G loss: 3.129192]\n",
      "epoch:9 step:7413 [D loss: 0.232969, acc.: 90.62%] [G loss: 4.966803]\n",
      "epoch:9 step:7414 [D loss: 0.313103, acc.: 87.50%] [G loss: 3.374127]\n",
      "epoch:9 step:7415 [D loss: 0.407376, acc.: 83.59%] [G loss: 2.213529]\n",
      "epoch:9 step:7416 [D loss: 0.348962, acc.: 85.94%] [G loss: 2.137976]\n",
      "epoch:9 step:7417 [D loss: 0.387722, acc.: 82.03%] [G loss: 2.966533]\n",
      "epoch:9 step:7418 [D loss: 0.463804, acc.: 76.56%] [G loss: 2.791425]\n",
      "epoch:9 step:7419 [D loss: 0.509802, acc.: 74.22%] [G loss: 2.510535]\n",
      "epoch:9 step:7420 [D loss: 0.500127, acc.: 76.56%] [G loss: 2.627092]\n",
      "epoch:9 step:7421 [D loss: 0.251053, acc.: 90.62%] [G loss: 2.654780]\n",
      "epoch:9 step:7422 [D loss: 0.481825, acc.: 81.25%] [G loss: 2.846447]\n",
      "epoch:9 step:7423 [D loss: 0.331840, acc.: 86.72%] [G loss: 3.082976]\n",
      "epoch:9 step:7424 [D loss: 0.510938, acc.: 76.56%] [G loss: 3.213462]\n",
      "epoch:9 step:7425 [D loss: 0.399018, acc.: 79.69%] [G loss: 3.748480]\n",
      "epoch:9 step:7426 [D loss: 0.412861, acc.: 79.69%] [G loss: 2.331100]\n",
      "epoch:9 step:7427 [D loss: 0.330438, acc.: 85.94%] [G loss: 3.074502]\n",
      "epoch:9 step:7428 [D loss: 0.323393, acc.: 88.28%] [G loss: 3.308675]\n",
      "epoch:9 step:7429 [D loss: 0.339719, acc.: 83.59%] [G loss: 2.059454]\n",
      "epoch:9 step:7430 [D loss: 0.412298, acc.: 78.91%] [G loss: 2.430017]\n",
      "epoch:9 step:7431 [D loss: 0.379592, acc.: 82.03%] [G loss: 2.342763]\n",
      "epoch:9 step:7432 [D loss: 0.273478, acc.: 89.06%] [G loss: 2.532732]\n",
      "epoch:9 step:7433 [D loss: 0.379125, acc.: 86.72%] [G loss: 2.819627]\n",
      "epoch:9 step:7434 [D loss: 0.350558, acc.: 83.59%] [G loss: 2.574519]\n",
      "epoch:9 step:7435 [D loss: 0.306467, acc.: 83.59%] [G loss: 2.543599]\n",
      "epoch:9 step:7436 [D loss: 0.337656, acc.: 87.50%] [G loss: 2.279289]\n",
      "epoch:9 step:7437 [D loss: 0.386668, acc.: 82.03%] [G loss: 1.732630]\n",
      "epoch:9 step:7438 [D loss: 0.312006, acc.: 89.84%] [G loss: 2.708774]\n",
      "epoch:9 step:7439 [D loss: 0.311920, acc.: 89.06%] [G loss: 2.355961]\n",
      "epoch:9 step:7440 [D loss: 0.388466, acc.: 86.72%] [G loss: 2.714184]\n",
      "epoch:9 step:7441 [D loss: 0.305094, acc.: 85.94%] [G loss: 3.700651]\n",
      "epoch:9 step:7442 [D loss: 0.252448, acc.: 88.28%] [G loss: 3.076964]\n",
      "epoch:9 step:7443 [D loss: 0.406782, acc.: 81.25%] [G loss: 1.812392]\n",
      "epoch:9 step:7444 [D loss: 0.330246, acc.: 85.16%] [G loss: 2.785070]\n",
      "epoch:9 step:7445 [D loss: 0.333847, acc.: 86.72%] [G loss: 2.550820]\n",
      "epoch:9 step:7446 [D loss: 0.360130, acc.: 84.38%] [G loss: 3.363251]\n",
      "epoch:9 step:7447 [D loss: 0.328732, acc.: 87.50%] [G loss: 3.827983]\n",
      "epoch:9 step:7448 [D loss: 0.308303, acc.: 84.38%] [G loss: 3.914181]\n",
      "epoch:9 step:7449 [D loss: 0.425653, acc.: 77.34%] [G loss: 2.267463]\n",
      "epoch:9 step:7450 [D loss: 0.312231, acc.: 90.62%] [G loss: 3.058051]\n",
      "epoch:9 step:7451 [D loss: 0.419159, acc.: 77.34%] [G loss: 2.408401]\n",
      "epoch:9 step:7452 [D loss: 0.360928, acc.: 81.25%] [G loss: 4.122723]\n",
      "epoch:9 step:7453 [D loss: 0.390647, acc.: 84.38%] [G loss: 2.561034]\n",
      "epoch:9 step:7454 [D loss: 0.316009, acc.: 87.50%] [G loss: 3.143057]\n",
      "epoch:9 step:7455 [D loss: 0.341234, acc.: 84.38%] [G loss: 3.095561]\n",
      "epoch:9 step:7456 [D loss: 0.345257, acc.: 85.16%] [G loss: 2.697442]\n",
      "epoch:9 step:7457 [D loss: 0.390316, acc.: 81.25%] [G loss: 2.162102]\n",
      "epoch:9 step:7458 [D loss: 0.419215, acc.: 82.81%] [G loss: 1.825217]\n",
      "epoch:9 step:7459 [D loss: 0.333701, acc.: 88.28%] [G loss: 2.650225]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:9 step:7460 [D loss: 0.375580, acc.: 84.38%] [G loss: 3.751743]\n",
      "epoch:9 step:7461 [D loss: 0.319141, acc.: 87.50%] [G loss: 3.210138]\n",
      "epoch:9 step:7462 [D loss: 0.377737, acc.: 84.38%] [G loss: 2.960620]\n",
      "epoch:9 step:7463 [D loss: 0.402125, acc.: 82.81%] [G loss: 2.829586]\n",
      "epoch:9 step:7464 [D loss: 0.362937, acc.: 82.81%] [G loss: 2.639742]\n",
      "epoch:9 step:7465 [D loss: 0.447276, acc.: 78.12%] [G loss: 3.202797]\n",
      "epoch:9 step:7466 [D loss: 0.511785, acc.: 81.25%] [G loss: 2.836690]\n",
      "epoch:9 step:7467 [D loss: 0.341368, acc.: 85.94%] [G loss: 3.447299]\n",
      "epoch:9 step:7468 [D loss: 0.349256, acc.: 86.72%] [G loss: 3.054336]\n",
      "epoch:9 step:7469 [D loss: 0.256186, acc.: 91.41%] [G loss: 2.296962]\n",
      "epoch:9 step:7470 [D loss: 0.392225, acc.: 82.81%] [G loss: 4.685064]\n",
      "epoch:9 step:7471 [D loss: 0.292662, acc.: 85.16%] [G loss: 4.397529]\n",
      "epoch:9 step:7472 [D loss: 0.358974, acc.: 82.81%] [G loss: 4.846342]\n",
      "epoch:9 step:7473 [D loss: 0.454446, acc.: 73.44%] [G loss: 3.828263]\n",
      "epoch:9 step:7474 [D loss: 1.038253, acc.: 60.94%] [G loss: 6.373785]\n",
      "epoch:9 step:7475 [D loss: 1.743220, acc.: 57.81%] [G loss: 2.672836]\n",
      "epoch:9 step:7476 [D loss: 0.416323, acc.: 79.69%] [G loss: 2.721850]\n",
      "epoch:9 step:7477 [D loss: 0.329855, acc.: 87.50%] [G loss: 3.305083]\n",
      "epoch:9 step:7478 [D loss: 0.472824, acc.: 85.94%] [G loss: 2.595250]\n",
      "epoch:9 step:7479 [D loss: 0.364376, acc.: 85.94%] [G loss: 2.904363]\n",
      "epoch:9 step:7480 [D loss: 0.497658, acc.: 80.47%] [G loss: 2.427649]\n",
      "epoch:9 step:7481 [D loss: 0.473842, acc.: 78.91%] [G loss: 2.565024]\n",
      "epoch:9 step:7482 [D loss: 0.445875, acc.: 79.69%] [G loss: 2.275998]\n",
      "epoch:9 step:7483 [D loss: 0.292359, acc.: 90.62%] [G loss: 2.366322]\n",
      "epoch:9 step:7484 [D loss: 0.382038, acc.: 83.59%] [G loss: 2.830099]\n",
      "epoch:9 step:7485 [D loss: 0.509240, acc.: 75.00%] [G loss: 2.067567]\n",
      "epoch:9 step:7486 [D loss: 0.332782, acc.: 82.03%] [G loss: 3.637817]\n",
      "epoch:9 step:7487 [D loss: 0.247128, acc.: 88.28%] [G loss: 5.086350]\n",
      "epoch:9 step:7488 [D loss: 0.340022, acc.: 85.94%] [G loss: 2.516007]\n",
      "epoch:9 step:7489 [D loss: 0.216625, acc.: 87.50%] [G loss: 5.246475]\n",
      "epoch:9 step:7490 [D loss: 0.388685, acc.: 86.72%] [G loss: 2.622683]\n",
      "epoch:9 step:7491 [D loss: 0.356341, acc.: 89.06%] [G loss: 2.143857]\n",
      "epoch:9 step:7492 [D loss: 0.284928, acc.: 88.28%] [G loss: 3.437349]\n",
      "epoch:9 step:7493 [D loss: 0.291586, acc.: 89.06%] [G loss: 3.304868]\n",
      "epoch:9 step:7494 [D loss: 0.371143, acc.: 80.47%] [G loss: 2.641506]\n",
      "epoch:9 step:7495 [D loss: 0.331083, acc.: 85.94%] [G loss: 2.304328]\n",
      "epoch:9 step:7496 [D loss: 0.422029, acc.: 82.81%] [G loss: 2.574601]\n",
      "epoch:9 step:7497 [D loss: 0.365431, acc.: 83.59%] [G loss: 1.736952]\n",
      "epoch:9 step:7498 [D loss: 0.409619, acc.: 79.69%] [G loss: 2.029039]\n",
      "epoch:9 step:7499 [D loss: 0.349387, acc.: 85.94%] [G loss: 2.771968]\n",
      "epoch:9 step:7500 [D loss: 0.391211, acc.: 82.03%] [G loss: 2.333023]\n",
      "epoch:9 step:7501 [D loss: 0.379420, acc.: 82.03%] [G loss: 3.195463]\n",
      "epoch:9 step:7502 [D loss: 0.437832, acc.: 78.12%] [G loss: 2.519719]\n",
      "epoch:9 step:7503 [D loss: 0.304627, acc.: 85.94%] [G loss: 3.389516]\n",
      "epoch:9 step:7504 [D loss: 0.400514, acc.: 79.69%] [G loss: 2.265982]\n",
      "epoch:9 step:7505 [D loss: 0.570938, acc.: 71.88%] [G loss: 2.109992]\n",
      "epoch:9 step:7506 [D loss: 0.295839, acc.: 88.28%] [G loss: 3.963253]\n",
      "epoch:9 step:7507 [D loss: 0.207395, acc.: 92.19%] [G loss: 5.024472]\n",
      "epoch:9 step:7508 [D loss: 0.278602, acc.: 89.06%] [G loss: 3.850881]\n",
      "epoch:9 step:7509 [D loss: 0.347822, acc.: 88.28%] [G loss: 2.784138]\n",
      "epoch:9 step:7510 [D loss: 0.310366, acc.: 85.94%] [G loss: 4.503531]\n",
      "epoch:9 step:7511 [D loss: 0.418237, acc.: 75.78%] [G loss: 3.149081]\n",
      "epoch:9 step:7512 [D loss: 0.382063, acc.: 83.59%] [G loss: 2.553804]\n",
      "epoch:9 step:7513 [D loss: 0.434199, acc.: 78.91%] [G loss: 2.995056]\n",
      "epoch:9 step:7514 [D loss: 0.348878, acc.: 83.59%] [G loss: 3.782159]\n",
      "epoch:9 step:7515 [D loss: 0.437390, acc.: 76.56%] [G loss: 4.263583]\n",
      "epoch:9 step:7516 [D loss: 0.516560, acc.: 75.00%] [G loss: 2.181138]\n",
      "epoch:9 step:7517 [D loss: 0.361235, acc.: 83.59%] [G loss: 2.476668]\n",
      "epoch:9 step:7518 [D loss: 0.335298, acc.: 84.38%] [G loss: 2.564188]\n",
      "epoch:9 step:7519 [D loss: 0.485399, acc.: 80.47%] [G loss: 2.686365]\n",
      "epoch:9 step:7520 [D loss: 0.326397, acc.: 83.59%] [G loss: 3.573640]\n",
      "epoch:9 step:7521 [D loss: 0.449528, acc.: 82.03%] [G loss: 2.610027]\n",
      "epoch:9 step:7522 [D loss: 0.354262, acc.: 86.72%] [G loss: 3.175588]\n",
      "epoch:9 step:7523 [D loss: 0.528061, acc.: 73.44%] [G loss: 4.876331]\n",
      "epoch:9 step:7524 [D loss: 0.835556, acc.: 62.50%] [G loss: 4.432118]\n",
      "epoch:9 step:7525 [D loss: 0.821510, acc.: 75.00%] [G loss: 3.969160]\n",
      "epoch:9 step:7526 [D loss: 0.695307, acc.: 66.41%] [G loss: 2.572200]\n",
      "epoch:9 step:7527 [D loss: 0.349929, acc.: 85.16%] [G loss: 2.930105]\n",
      "epoch:9 step:7528 [D loss: 0.488085, acc.: 71.09%] [G loss: 2.864373]\n",
      "epoch:9 step:7529 [D loss: 0.399189, acc.: 83.59%] [G loss: 3.147756]\n",
      "epoch:9 step:7530 [D loss: 0.484026, acc.: 78.12%] [G loss: 1.825679]\n",
      "epoch:9 step:7531 [D loss: 0.373075, acc.: 82.81%] [G loss: 2.627806]\n",
      "epoch:9 step:7532 [D loss: 0.320556, acc.: 86.72%] [G loss: 2.687806]\n",
      "epoch:9 step:7533 [D loss: 0.389349, acc.: 82.03%] [G loss: 3.418354]\n",
      "epoch:9 step:7534 [D loss: 0.385521, acc.: 82.03%] [G loss: 2.209619]\n",
      "epoch:9 step:7535 [D loss: 0.479330, acc.: 78.12%] [G loss: 3.255940]\n",
      "epoch:9 step:7536 [D loss: 0.415029, acc.: 80.47%] [G loss: 2.399809]\n",
      "epoch:9 step:7537 [D loss: 0.387398, acc.: 81.25%] [G loss: 3.377865]\n",
      "epoch:9 step:7538 [D loss: 0.284785, acc.: 85.16%] [G loss: 3.333654]\n",
      "epoch:9 step:7539 [D loss: 0.372581, acc.: 85.94%] [G loss: 2.467372]\n",
      "epoch:9 step:7540 [D loss: 0.257420, acc.: 92.19%] [G loss: 2.924152]\n",
      "epoch:9 step:7541 [D loss: 0.298264, acc.: 89.84%] [G loss: 2.809363]\n",
      "epoch:9 step:7542 [D loss: 0.428848, acc.: 81.25%] [G loss: 3.371576]\n",
      "epoch:9 step:7543 [D loss: 0.319142, acc.: 85.16%] [G loss: 3.269383]\n",
      "epoch:9 step:7544 [D loss: 0.284907, acc.: 86.72%] [G loss: 4.276811]\n",
      "epoch:9 step:7545 [D loss: 0.374917, acc.: 81.25%] [G loss: 2.399732]\n",
      "epoch:9 step:7546 [D loss: 0.315876, acc.: 86.72%] [G loss: 2.344163]\n",
      "epoch:9 step:7547 [D loss: 0.264088, acc.: 89.84%] [G loss: 4.394464]\n",
      "epoch:9 step:7548 [D loss: 0.299296, acc.: 86.72%] [G loss: 3.095623]\n",
      "epoch:9 step:7549 [D loss: 0.258803, acc.: 90.62%] [G loss: 3.780529]\n",
      "epoch:9 step:7550 [D loss: 0.442318, acc.: 80.47%] [G loss: 2.646086]\n",
      "epoch:9 step:7551 [D loss: 0.462207, acc.: 76.56%] [G loss: 2.873262]\n",
      "epoch:9 step:7552 [D loss: 0.330299, acc.: 83.59%] [G loss: 2.722964]\n",
      "epoch:9 step:7553 [D loss: 0.304003, acc.: 82.81%] [G loss: 4.758060]\n",
      "epoch:9 step:7554 [D loss: 0.303232, acc.: 87.50%] [G loss: 3.785266]\n",
      "epoch:9 step:7555 [D loss: 0.398821, acc.: 88.28%] [G loss: 2.542037]\n",
      "epoch:9 step:7556 [D loss: 0.392697, acc.: 77.34%] [G loss: 2.853668]\n",
      "epoch:9 step:7557 [D loss: 0.330541, acc.: 87.50%] [G loss: 3.887342]\n",
      "epoch:9 step:7558 [D loss: 0.216539, acc.: 91.41%] [G loss: 4.018745]\n",
      "epoch:9 step:7559 [D loss: 0.535740, acc.: 74.22%] [G loss: 1.834310]\n",
      "epoch:9 step:7560 [D loss: 0.316177, acc.: 88.28%] [G loss: 2.171180]\n",
      "epoch:9 step:7561 [D loss: 0.415467, acc.: 80.47%] [G loss: 2.144232]\n",
      "epoch:9 step:7562 [D loss: 0.361029, acc.: 84.38%] [G loss: 2.773937]\n",
      "epoch:9 step:7563 [D loss: 0.417696, acc.: 85.16%] [G loss: 2.570383]\n",
      "epoch:9 step:7564 [D loss: 0.347362, acc.: 87.50%] [G loss: 3.050170]\n",
      "epoch:9 step:7565 [D loss: 0.332045, acc.: 87.50%] [G loss: 3.340524]\n",
      "epoch:9 step:7566 [D loss: 0.389029, acc.: 81.25%] [G loss: 2.391288]\n",
      "epoch:9 step:7567 [D loss: 0.382305, acc.: 85.94%] [G loss: 3.129136]\n",
      "epoch:9 step:7568 [D loss: 0.283572, acc.: 83.59%] [G loss: 4.257032]\n",
      "epoch:9 step:7569 [D loss: 0.334561, acc.: 88.28%] [G loss: 3.440252]\n",
      "epoch:9 step:7570 [D loss: 0.393273, acc.: 82.81%] [G loss: 2.705353]\n",
      "epoch:9 step:7571 [D loss: 0.393334, acc.: 85.16%] [G loss: 2.253396]\n",
      "epoch:9 step:7572 [D loss: 0.305187, acc.: 86.72%] [G loss: 3.202717]\n",
      "epoch:9 step:7573 [D loss: 0.469385, acc.: 75.78%] [G loss: 2.474167]\n",
      "epoch:9 step:7574 [D loss: 0.397147, acc.: 84.38%] [G loss: 2.486049]\n",
      "epoch:9 step:7575 [D loss: 0.236574, acc.: 91.41%] [G loss: 2.484681]\n",
      "epoch:9 step:7576 [D loss: 0.310548, acc.: 85.94%] [G loss: 4.561929]\n",
      "epoch:9 step:7577 [D loss: 0.205570, acc.: 94.53%] [G loss: 6.415735]\n",
      "epoch:9 step:7578 [D loss: 0.304287, acc.: 85.16%] [G loss: 2.642522]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:9 step:7579 [D loss: 0.273407, acc.: 86.72%] [G loss: 3.343103]\n",
      "epoch:9 step:7580 [D loss: 0.228105, acc.: 88.28%] [G loss: 5.301543]\n",
      "epoch:9 step:7581 [D loss: 0.369422, acc.: 83.59%] [G loss: 2.750349]\n",
      "epoch:9 step:7582 [D loss: 0.288381, acc.: 86.72%] [G loss: 3.958682]\n",
      "epoch:9 step:7583 [D loss: 0.232393, acc.: 91.41%] [G loss: 4.183201]\n",
      "epoch:9 step:7584 [D loss: 0.292885, acc.: 89.06%] [G loss: 2.926352]\n",
      "epoch:9 step:7585 [D loss: 0.319118, acc.: 89.06%] [G loss: 3.123657]\n",
      "epoch:9 step:7586 [D loss: 0.354433, acc.: 86.72%] [G loss: 3.113095]\n",
      "epoch:9 step:7587 [D loss: 0.278855, acc.: 89.06%] [G loss: 3.324711]\n",
      "epoch:9 step:7588 [D loss: 0.414334, acc.: 80.47%] [G loss: 2.164032]\n",
      "epoch:9 step:7589 [D loss: 0.312451, acc.: 89.84%] [G loss: 2.074414]\n",
      "epoch:9 step:7590 [D loss: 0.348210, acc.: 87.50%] [G loss: 1.984975]\n",
      "epoch:9 step:7591 [D loss: 0.400996, acc.: 81.25%] [G loss: 2.100202]\n",
      "epoch:9 step:7592 [D loss: 0.334145, acc.: 86.72%] [G loss: 3.267452]\n",
      "epoch:9 step:7593 [D loss: 0.313758, acc.: 90.62%] [G loss: 2.308043]\n",
      "epoch:9 step:7594 [D loss: 0.280784, acc.: 92.19%] [G loss: 3.621639]\n",
      "epoch:9 step:7595 [D loss: 0.332711, acc.: 83.59%] [G loss: 3.098425]\n",
      "epoch:9 step:7596 [D loss: 0.365797, acc.: 88.28%] [G loss: 3.782627]\n",
      "epoch:9 step:7597 [D loss: 0.345456, acc.: 85.16%] [G loss: 3.909786]\n",
      "epoch:9 step:7598 [D loss: 0.275930, acc.: 89.84%] [G loss: 3.440924]\n",
      "epoch:9 step:7599 [D loss: 0.407121, acc.: 83.59%] [G loss: 3.048006]\n",
      "epoch:9 step:7600 [D loss: 0.371490, acc.: 86.72%] [G loss: 2.831166]\n",
      "##############\n",
      "[0.84377651 0.88018434 0.770627   0.81398386 0.77074057 0.82506271\n",
      " 0.86590028 0.84810026 0.81464487 0.81760555]\n",
      "##########\n",
      "epoch:9 step:7601 [D loss: 0.357267, acc.: 83.59%] [G loss: 4.834419]\n",
      "epoch:9 step:7602 [D loss: 0.418752, acc.: 79.69%] [G loss: 2.784163]\n",
      "epoch:9 step:7603 [D loss: 0.409964, acc.: 78.12%] [G loss: 2.837569]\n",
      "epoch:9 step:7604 [D loss: 0.353016, acc.: 84.38%] [G loss: 1.809852]\n",
      "epoch:9 step:7605 [D loss: 0.336081, acc.: 87.50%] [G loss: 3.088982]\n",
      "epoch:9 step:7606 [D loss: 0.381977, acc.: 82.81%] [G loss: 3.047714]\n",
      "epoch:9 step:7607 [D loss: 0.301757, acc.: 87.50%] [G loss: 1.941797]\n",
      "epoch:9 step:7608 [D loss: 0.358273, acc.: 84.38%] [G loss: 1.964045]\n",
      "epoch:9 step:7609 [D loss: 0.321298, acc.: 88.28%] [G loss: 2.039183]\n",
      "epoch:9 step:7610 [D loss: 0.375063, acc.: 84.38%] [G loss: 2.489607]\n",
      "epoch:9 step:7611 [D loss: 0.311183, acc.: 88.28%] [G loss: 3.339614]\n",
      "epoch:9 step:7612 [D loss: 0.467929, acc.: 76.56%] [G loss: 4.918761]\n",
      "epoch:9 step:7613 [D loss: 0.515140, acc.: 79.69%] [G loss: 3.248884]\n",
      "epoch:9 step:7614 [D loss: 0.529029, acc.: 75.00%] [G loss: 3.124386]\n",
      "epoch:9 step:7615 [D loss: 0.357833, acc.: 84.38%] [G loss: 2.226399]\n",
      "epoch:9 step:7616 [D loss: 0.417905, acc.: 75.78%] [G loss: 3.593862]\n",
      "epoch:9 step:7617 [D loss: 0.394961, acc.: 82.03%] [G loss: 2.884789]\n",
      "epoch:9 step:7618 [D loss: 0.345907, acc.: 82.03%] [G loss: 4.349249]\n",
      "epoch:9 step:7619 [D loss: 0.377576, acc.: 82.81%] [G loss: 2.368317]\n",
      "epoch:9 step:7620 [D loss: 0.458182, acc.: 83.59%] [G loss: 2.143593]\n",
      "epoch:9 step:7621 [D loss: 0.360956, acc.: 89.84%] [G loss: 2.740289]\n",
      "epoch:9 step:7622 [D loss: 0.366636, acc.: 86.72%] [G loss: 3.087722]\n",
      "epoch:9 step:7623 [D loss: 0.235846, acc.: 92.97%] [G loss: 2.964857]\n",
      "epoch:9 step:7624 [D loss: 0.402184, acc.: 82.81%] [G loss: 4.745999]\n",
      "epoch:9 step:7625 [D loss: 0.339545, acc.: 85.94%] [G loss: 3.881242]\n",
      "epoch:9 step:7626 [D loss: 0.316904, acc.: 88.28%] [G loss: 3.644097]\n",
      "epoch:9 step:7627 [D loss: 0.247842, acc.: 92.19%] [G loss: 2.730553]\n",
      "epoch:9 step:7628 [D loss: 0.345879, acc.: 84.38%] [G loss: 4.314101]\n",
      "epoch:9 step:7629 [D loss: 0.328592, acc.: 88.28%] [G loss: 2.725263]\n",
      "epoch:9 step:7630 [D loss: 0.271093, acc.: 88.28%] [G loss: 3.246151]\n",
      "epoch:9 step:7631 [D loss: 0.278924, acc.: 90.62%] [G loss: 2.872031]\n",
      "epoch:9 step:7632 [D loss: 0.190209, acc.: 95.31%] [G loss: 4.655585]\n",
      "epoch:9 step:7633 [D loss: 0.299548, acc.: 88.28%] [G loss: 3.748359]\n",
      "epoch:9 step:7634 [D loss: 0.397596, acc.: 82.03%] [G loss: 3.604807]\n",
      "epoch:9 step:7635 [D loss: 0.350043, acc.: 85.16%] [G loss: 2.251275]\n",
      "epoch:9 step:7636 [D loss: 0.296258, acc.: 89.84%] [G loss: 3.039385]\n",
      "epoch:9 step:7637 [D loss: 0.364309, acc.: 84.38%] [G loss: 2.521213]\n",
      "epoch:9 step:7638 [D loss: 0.437653, acc.: 78.91%] [G loss: 2.865570]\n",
      "epoch:9 step:7639 [D loss: 0.369337, acc.: 81.25%] [G loss: 3.489081]\n",
      "epoch:9 step:7640 [D loss: 0.487829, acc.: 74.22%] [G loss: 2.994889]\n",
      "epoch:9 step:7641 [D loss: 0.404016, acc.: 84.38%] [G loss: 2.377107]\n",
      "epoch:9 step:7642 [D loss: 0.379274, acc.: 77.34%] [G loss: 2.865942]\n",
      "epoch:9 step:7643 [D loss: 0.324129, acc.: 85.94%] [G loss: 2.733860]\n",
      "epoch:9 step:7644 [D loss: 0.314564, acc.: 88.28%] [G loss: 4.605208]\n",
      "epoch:9 step:7645 [D loss: 0.313562, acc.: 89.84%] [G loss: 4.040339]\n",
      "epoch:9 step:7646 [D loss: 0.273957, acc.: 85.94%] [G loss: 3.480173]\n",
      "epoch:9 step:7647 [D loss: 0.311980, acc.: 87.50%] [G loss: 2.618263]\n",
      "epoch:9 step:7648 [D loss: 0.318130, acc.: 86.72%] [G loss: 2.792410]\n",
      "epoch:9 step:7649 [D loss: 0.371658, acc.: 81.25%] [G loss: 2.574386]\n",
      "epoch:9 step:7650 [D loss: 0.279880, acc.: 92.97%] [G loss: 2.516101]\n",
      "epoch:9 step:7651 [D loss: 0.416354, acc.: 85.16%] [G loss: 3.038653]\n",
      "epoch:9 step:7652 [D loss: 0.419287, acc.: 81.25%] [G loss: 2.567718]\n",
      "epoch:9 step:7653 [D loss: 0.428850, acc.: 80.47%] [G loss: 2.766859]\n",
      "epoch:9 step:7654 [D loss: 0.371525, acc.: 88.28%] [G loss: 2.597765]\n",
      "epoch:9 step:7655 [D loss: 0.226716, acc.: 92.97%] [G loss: 2.377996]\n",
      "epoch:9 step:7656 [D loss: 0.363487, acc.: 85.94%] [G loss: 2.761656]\n",
      "epoch:9 step:7657 [D loss: 0.384587, acc.: 82.81%] [G loss: 2.268253]\n",
      "epoch:9 step:7658 [D loss: 0.478576, acc.: 85.16%] [G loss: 2.559218]\n",
      "epoch:9 step:7659 [D loss: 0.340594, acc.: 85.16%] [G loss: 3.554313]\n",
      "epoch:9 step:7660 [D loss: 0.313470, acc.: 82.81%] [G loss: 4.994488]\n",
      "epoch:9 step:7661 [D loss: 0.392850, acc.: 87.50%] [G loss: 2.964645]\n",
      "epoch:9 step:7662 [D loss: 0.501180, acc.: 77.34%] [G loss: 2.723495]\n",
      "epoch:9 step:7663 [D loss: 0.320966, acc.: 85.94%] [G loss: 4.391410]\n",
      "epoch:9 step:7664 [D loss: 0.504604, acc.: 80.47%] [G loss: 4.655996]\n",
      "epoch:9 step:7665 [D loss: 0.354803, acc.: 82.03%] [G loss: 7.708404]\n",
      "epoch:9 step:7666 [D loss: 0.205935, acc.: 94.53%] [G loss: 5.953028]\n",
      "epoch:9 step:7667 [D loss: 0.332491, acc.: 87.50%] [G loss: 5.961057]\n",
      "epoch:9 step:7668 [D loss: 0.421314, acc.: 78.91%] [G loss: 2.920021]\n",
      "epoch:9 step:7669 [D loss: 0.307799, acc.: 86.72%] [G loss: 2.558119]\n",
      "epoch:9 step:7670 [D loss: 0.308494, acc.: 89.06%] [G loss: 3.175178]\n",
      "epoch:9 step:7671 [D loss: 0.330485, acc.: 86.72%] [G loss: 2.870959]\n",
      "epoch:9 step:7672 [D loss: 0.254210, acc.: 90.62%] [G loss: 3.492789]\n",
      "epoch:9 step:7673 [D loss: 0.294542, acc.: 89.06%] [G loss: 2.779776]\n",
      "epoch:9 step:7674 [D loss: 0.351369, acc.: 83.59%] [G loss: 3.953067]\n",
      "epoch:9 step:7675 [D loss: 0.273511, acc.: 90.62%] [G loss: 2.230417]\n",
      "epoch:9 step:7676 [D loss: 0.262233, acc.: 94.53%] [G loss: 3.226650]\n",
      "epoch:9 step:7677 [D loss: 0.326661, acc.: 85.16%] [G loss: 2.798251]\n",
      "epoch:9 step:7678 [D loss: 0.334548, acc.: 86.72%] [G loss: 2.726254]\n",
      "epoch:9 step:7679 [D loss: 0.367394, acc.: 85.16%] [G loss: 3.963217]\n",
      "epoch:9 step:7680 [D loss: 0.420251, acc.: 82.03%] [G loss: 3.785283]\n",
      "epoch:9 step:7681 [D loss: 0.392690, acc.: 85.16%] [G loss: 2.881823]\n",
      "epoch:9 step:7682 [D loss: 0.355364, acc.: 85.16%] [G loss: 3.052991]\n",
      "epoch:9 step:7683 [D loss: 0.369417, acc.: 88.28%] [G loss: 2.571308]\n",
      "epoch:9 step:7684 [D loss: 0.357443, acc.: 84.38%] [G loss: 2.104072]\n",
      "epoch:9 step:7685 [D loss: 0.256541, acc.: 90.62%] [G loss: 3.350318]\n",
      "epoch:9 step:7686 [D loss: 0.350316, acc.: 84.38%] [G loss: 3.684964]\n",
      "epoch:9 step:7687 [D loss: 0.250061, acc.: 89.84%] [G loss: 2.484179]\n",
      "epoch:9 step:7688 [D loss: 0.269949, acc.: 90.62%] [G loss: 4.428532]\n",
      "epoch:9 step:7689 [D loss: 0.263544, acc.: 89.06%] [G loss: 2.673987]\n",
      "epoch:9 step:7690 [D loss: 0.273597, acc.: 89.06%] [G loss: 3.300034]\n",
      "epoch:9 step:7691 [D loss: 0.309647, acc.: 87.50%] [G loss: 2.373051]\n",
      "epoch:9 step:7692 [D loss: 0.295573, acc.: 90.62%] [G loss: 2.657798]\n",
      "epoch:9 step:7693 [D loss: 0.356756, acc.: 86.72%] [G loss: 3.233496]\n",
      "epoch:9 step:7694 [D loss: 0.368587, acc.: 85.94%] [G loss: 1.767475]\n",
      "epoch:9 step:7695 [D loss: 0.244268, acc.: 91.41%] [G loss: 4.390377]\n",
      "epoch:9 step:7696 [D loss: 0.306867, acc.: 87.50%] [G loss: 2.975105]\n",
      "epoch:9 step:7697 [D loss: 0.309905, acc.: 87.50%] [G loss: 4.679268]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:9 step:7698 [D loss: 0.402324, acc.: 79.69%] [G loss: 5.375482]\n",
      "epoch:9 step:7699 [D loss: 0.715940, acc.: 72.66%] [G loss: 5.151838]\n",
      "epoch:9 step:7700 [D loss: 1.147754, acc.: 59.38%] [G loss: 5.622868]\n",
      "epoch:9 step:7701 [D loss: 1.150488, acc.: 69.53%] [G loss: 3.199687]\n",
      "epoch:9 step:7702 [D loss: 0.535710, acc.: 75.78%] [G loss: 3.096572]\n",
      "epoch:9 step:7703 [D loss: 0.371376, acc.: 84.38%] [G loss: 2.634019]\n",
      "epoch:9 step:7704 [D loss: 0.272360, acc.: 86.72%] [G loss: 2.971252]\n",
      "epoch:9 step:7705 [D loss: 0.355597, acc.: 82.03%] [G loss: 2.572191]\n",
      "epoch:9 step:7706 [D loss: 0.355417, acc.: 85.16%] [G loss: 3.392222]\n",
      "epoch:9 step:7707 [D loss: 0.338977, acc.: 84.38%] [G loss: 2.377628]\n",
      "epoch:9 step:7708 [D loss: 0.334569, acc.: 88.28%] [G loss: 2.375208]\n",
      "epoch:9 step:7709 [D loss: 0.361379, acc.: 84.38%] [G loss: 2.139017]\n",
      "epoch:9 step:7710 [D loss: 0.270842, acc.: 91.41%] [G loss: 2.506493]\n",
      "epoch:9 step:7711 [D loss: 0.428251, acc.: 80.47%] [G loss: 2.484425]\n",
      "epoch:9 step:7712 [D loss: 0.373310, acc.: 83.59%] [G loss: 1.976125]\n",
      "epoch:9 step:7713 [D loss: 0.446488, acc.: 78.12%] [G loss: 2.291156]\n",
      "epoch:9 step:7714 [D loss: 0.403256, acc.: 82.03%] [G loss: 2.610034]\n",
      "epoch:9 step:7715 [D loss: 0.315270, acc.: 88.28%] [G loss: 3.143609]\n",
      "epoch:9 step:7716 [D loss: 0.235715, acc.: 92.97%] [G loss: 2.317756]\n",
      "epoch:9 step:7717 [D loss: 0.315156, acc.: 84.38%] [G loss: 2.091481]\n",
      "epoch:9 step:7718 [D loss: 0.384889, acc.: 82.81%] [G loss: 2.238509]\n",
      "epoch:9 step:7719 [D loss: 0.347810, acc.: 86.72%] [G loss: 2.346498]\n",
      "epoch:9 step:7720 [D loss: 0.195679, acc.: 96.88%] [G loss: 2.982992]\n",
      "epoch:9 step:7721 [D loss: 0.397037, acc.: 83.59%] [G loss: 2.460064]\n",
      "epoch:9 step:7722 [D loss: 0.347068, acc.: 87.50%] [G loss: 2.545673]\n",
      "epoch:9 step:7723 [D loss: 0.336111, acc.: 89.06%] [G loss: 2.981051]\n",
      "epoch:9 step:7724 [D loss: 0.389943, acc.: 84.38%] [G loss: 2.708447]\n",
      "epoch:9 step:7725 [D loss: 0.324029, acc.: 88.28%] [G loss: 2.040740]\n",
      "epoch:9 step:7726 [D loss: 0.385955, acc.: 89.84%] [G loss: 2.122780]\n",
      "epoch:9 step:7727 [D loss: 0.231016, acc.: 93.75%] [G loss: 2.414842]\n",
      "epoch:9 step:7728 [D loss: 0.295893, acc.: 85.16%] [G loss: 2.278308]\n",
      "epoch:9 step:7729 [D loss: 0.302307, acc.: 86.72%] [G loss: 3.224933]\n",
      "epoch:9 step:7730 [D loss: 0.263771, acc.: 88.28%] [G loss: 2.492518]\n",
      "epoch:9 step:7731 [D loss: 0.339630, acc.: 87.50%] [G loss: 2.346427]\n",
      "epoch:9 step:7732 [D loss: 0.428784, acc.: 80.47%] [G loss: 1.911533]\n",
      "epoch:9 step:7733 [D loss: 0.353546, acc.: 82.03%] [G loss: 2.610540]\n",
      "epoch:9 step:7734 [D loss: 0.471245, acc.: 78.91%] [G loss: 3.153459]\n",
      "epoch:9 step:7735 [D loss: 0.272672, acc.: 91.41%] [G loss: 3.232197]\n",
      "epoch:9 step:7736 [D loss: 0.336381, acc.: 91.41%] [G loss: 2.947286]\n",
      "epoch:9 step:7737 [D loss: 0.305221, acc.: 86.72%] [G loss: 2.390013]\n",
      "epoch:9 step:7738 [D loss: 0.459275, acc.: 78.12%] [G loss: 6.242860]\n",
      "epoch:9 step:7739 [D loss: 0.563278, acc.: 75.78%] [G loss: 4.117509]\n",
      "epoch:9 step:7740 [D loss: 0.302865, acc.: 86.72%] [G loss: 2.928530]\n",
      "epoch:9 step:7741 [D loss: 0.363291, acc.: 85.94%] [G loss: 2.026675]\n",
      "epoch:9 step:7742 [D loss: 0.344158, acc.: 86.72%] [G loss: 2.023806]\n",
      "epoch:9 step:7743 [D loss: 0.316434, acc.: 87.50%] [G loss: 3.244156]\n",
      "epoch:9 step:7744 [D loss: 0.383928, acc.: 80.47%] [G loss: 3.174102]\n",
      "epoch:9 step:7745 [D loss: 0.337397, acc.: 86.72%] [G loss: 3.081739]\n",
      "epoch:9 step:7746 [D loss: 0.290409, acc.: 87.50%] [G loss: 2.861605]\n",
      "epoch:9 step:7747 [D loss: 0.307578, acc.: 86.72%] [G loss: 2.797818]\n",
      "epoch:9 step:7748 [D loss: 0.280582, acc.: 91.41%] [G loss: 2.560256]\n",
      "epoch:9 step:7749 [D loss: 0.296153, acc.: 88.28%] [G loss: 3.461818]\n",
      "epoch:9 step:7750 [D loss: 0.247011, acc.: 93.75%] [G loss: 2.405248]\n",
      "epoch:9 step:7751 [D loss: 0.325070, acc.: 87.50%] [G loss: 2.590421]\n",
      "epoch:9 step:7752 [D loss: 0.353435, acc.: 85.94%] [G loss: 2.158550]\n",
      "epoch:9 step:7753 [D loss: 0.231806, acc.: 91.41%] [G loss: 2.829201]\n",
      "epoch:9 step:7754 [D loss: 0.330120, acc.: 88.28%] [G loss: 3.079526]\n",
      "epoch:9 step:7755 [D loss: 0.247757, acc.: 89.84%] [G loss: 2.439372]\n",
      "epoch:9 step:7756 [D loss: 0.272740, acc.: 87.50%] [G loss: 2.951596]\n",
      "epoch:9 step:7757 [D loss: 0.341785, acc.: 89.84%] [G loss: 2.888136]\n",
      "epoch:9 step:7758 [D loss: 0.271214, acc.: 89.06%] [G loss: 2.801324]\n",
      "epoch:9 step:7759 [D loss: 0.269261, acc.: 92.97%] [G loss: 2.680119]\n",
      "epoch:9 step:7760 [D loss: 0.246336, acc.: 92.19%] [G loss: 3.564030]\n",
      "epoch:9 step:7761 [D loss: 0.293569, acc.: 88.28%] [G loss: 4.276392]\n",
      "epoch:9 step:7762 [D loss: 0.382849, acc.: 80.47%] [G loss: 2.303757]\n",
      "epoch:9 step:7763 [D loss: 0.302680, acc.: 88.28%] [G loss: 3.331845]\n",
      "epoch:9 step:7764 [D loss: 0.298676, acc.: 88.28%] [G loss: 3.245918]\n",
      "epoch:9 step:7765 [D loss: 0.318306, acc.: 88.28%] [G loss: 3.215378]\n",
      "epoch:9 step:7766 [D loss: 0.319900, acc.: 88.28%] [G loss: 2.840494]\n",
      "epoch:9 step:7767 [D loss: 0.279322, acc.: 90.62%] [G loss: 2.423105]\n",
      "epoch:9 step:7768 [D loss: 0.327720, acc.: 87.50%] [G loss: 2.095233]\n",
      "epoch:9 step:7769 [D loss: 0.293693, acc.: 85.94%] [G loss: 2.539963]\n",
      "epoch:9 step:7770 [D loss: 0.344165, acc.: 87.50%] [G loss: 2.776435]\n",
      "epoch:9 step:7771 [D loss: 0.266278, acc.: 91.41%] [G loss: 4.380331]\n",
      "epoch:9 step:7772 [D loss: 0.273267, acc.: 89.06%] [G loss: 2.946291]\n",
      "epoch:9 step:7773 [D loss: 0.257608, acc.: 89.84%] [G loss: 3.308364]\n",
      "epoch:9 step:7774 [D loss: 0.310824, acc.: 87.50%] [G loss: 2.440618]\n",
      "epoch:9 step:7775 [D loss: 0.270684, acc.: 87.50%] [G loss: 2.818740]\n",
      "epoch:9 step:7776 [D loss: 0.391057, acc.: 89.06%] [G loss: 2.747717]\n",
      "epoch:9 step:7777 [D loss: 0.254162, acc.: 91.41%] [G loss: 3.610168]\n",
      "epoch:9 step:7778 [D loss: 0.233482, acc.: 89.84%] [G loss: 4.182345]\n",
      "epoch:9 step:7779 [D loss: 0.325629, acc.: 84.38%] [G loss: 3.575535]\n",
      "epoch:9 step:7780 [D loss: 0.292329, acc.: 89.84%] [G loss: 2.360058]\n",
      "epoch:9 step:7781 [D loss: 0.295220, acc.: 85.16%] [G loss: 3.435989]\n",
      "epoch:9 step:7782 [D loss: 0.222040, acc.: 92.97%] [G loss: 4.282838]\n",
      "epoch:9 step:7783 [D loss: 0.308495, acc.: 85.16%] [G loss: 2.110213]\n",
      "epoch:9 step:7784 [D loss: 0.256634, acc.: 86.72%] [G loss: 5.254922]\n",
      "epoch:9 step:7785 [D loss: 0.377944, acc.: 81.25%] [G loss: 3.023837]\n",
      "epoch:9 step:7786 [D loss: 0.346170, acc.: 83.59%] [G loss: 4.963277]\n",
      "epoch:9 step:7787 [D loss: 0.280539, acc.: 89.06%] [G loss: 2.245627]\n",
      "epoch:9 step:7788 [D loss: 0.311342, acc.: 86.72%] [G loss: 3.742876]\n",
      "epoch:9 step:7789 [D loss: 0.283301, acc.: 88.28%] [G loss: 3.464186]\n",
      "epoch:9 step:7790 [D loss: 0.269043, acc.: 90.62%] [G loss: 2.291353]\n",
      "epoch:9 step:7791 [D loss: 0.376627, acc.: 81.25%] [G loss: 2.868237]\n",
      "epoch:9 step:7792 [D loss: 0.399538, acc.: 82.03%] [G loss: 2.446481]\n",
      "epoch:9 step:7793 [D loss: 0.309415, acc.: 88.28%] [G loss: 2.301315]\n",
      "epoch:9 step:7794 [D loss: 0.532820, acc.: 72.66%] [G loss: 4.337045]\n",
      "epoch:9 step:7795 [D loss: 0.727332, acc.: 64.84%] [G loss: 2.224018]\n",
      "epoch:9 step:7796 [D loss: 0.595013, acc.: 82.03%] [G loss: 2.956356]\n",
      "epoch:9 step:7797 [D loss: 0.359441, acc.: 87.50%] [G loss: 3.043987]\n",
      "epoch:9 step:7798 [D loss: 0.337880, acc.: 85.16%] [G loss: 3.446741]\n",
      "epoch:9 step:7799 [D loss: 0.317400, acc.: 85.94%] [G loss: 3.893930]\n",
      "epoch:9 step:7800 [D loss: 0.284298, acc.: 89.06%] [G loss: 3.822961]\n",
      "##############\n",
      "[0.83132893 0.87794453 0.79604982 0.79614872 0.77892363 0.80374403\n",
      " 0.85165549 0.81849778 0.8085538  0.80736771]\n",
      "##########\n",
      "epoch:9 step:7801 [D loss: 0.367654, acc.: 82.81%] [G loss: 3.242276]\n",
      "epoch:9 step:7802 [D loss: 0.169806, acc.: 94.53%] [G loss: 4.823807]\n",
      "epoch:9 step:7803 [D loss: 0.291667, acc.: 90.62%] [G loss: 3.818331]\n",
      "epoch:9 step:7804 [D loss: 0.209063, acc.: 93.75%] [G loss: 4.527071]\n",
      "epoch:9 step:7805 [D loss: 0.235986, acc.: 92.19%] [G loss: 5.664173]\n",
      "epoch:9 step:7806 [D loss: 0.359548, acc.: 86.72%] [G loss: 4.234092]\n",
      "epoch:9 step:7807 [D loss: 0.297653, acc.: 89.06%] [G loss: 3.000305]\n",
      "epoch:9 step:7808 [D loss: 0.270975, acc.: 89.06%] [G loss: 3.348189]\n",
      "epoch:9 step:7809 [D loss: 0.306170, acc.: 89.06%] [G loss: 2.764814]\n",
      "epoch:9 step:7810 [D loss: 0.304486, acc.: 85.94%] [G loss: 3.091992]\n",
      "epoch:10 step:7811 [D loss: 0.301382, acc.: 87.50%] [G loss: 2.810115]\n",
      "epoch:10 step:7812 [D loss: 0.281604, acc.: 88.28%] [G loss: 2.399966]\n",
      "epoch:10 step:7813 [D loss: 0.322111, acc.: 87.50%] [G loss: 3.948782]\n",
      "epoch:10 step:7814 [D loss: 0.370522, acc.: 82.81%] [G loss: 2.512313]\n",
      "epoch:10 step:7815 [D loss: 0.226466, acc.: 90.62%] [G loss: 3.087747]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:10 step:7816 [D loss: 0.376480, acc.: 83.59%] [G loss: 2.397513]\n",
      "epoch:10 step:7817 [D loss: 0.245434, acc.: 92.97%] [G loss: 2.606391]\n",
      "epoch:10 step:7818 [D loss: 0.322581, acc.: 89.06%] [G loss: 2.975431]\n",
      "epoch:10 step:7819 [D loss: 0.273437, acc.: 88.28%] [G loss: 3.077269]\n",
      "epoch:10 step:7820 [D loss: 0.306977, acc.: 89.84%] [G loss: 2.504617]\n",
      "epoch:10 step:7821 [D loss: 0.319805, acc.: 88.28%] [G loss: 2.645994]\n",
      "epoch:10 step:7822 [D loss: 0.241036, acc.: 94.53%] [G loss: 3.558458]\n",
      "epoch:10 step:7823 [D loss: 0.265859, acc.: 89.06%] [G loss: 2.686752]\n",
      "epoch:10 step:7824 [D loss: 0.335706, acc.: 90.62%] [G loss: 2.494461]\n",
      "epoch:10 step:7825 [D loss: 0.334595, acc.: 83.59%] [G loss: 2.858773]\n",
      "epoch:10 step:7826 [D loss: 0.266534, acc.: 89.06%] [G loss: 2.934391]\n",
      "epoch:10 step:7827 [D loss: 0.259851, acc.: 92.19%] [G loss: 2.664311]\n",
      "epoch:10 step:7828 [D loss: 0.385971, acc.: 84.38%] [G loss: 2.134410]\n",
      "epoch:10 step:7829 [D loss: 0.330595, acc.: 89.06%] [G loss: 2.644163]\n",
      "epoch:10 step:7830 [D loss: 0.320941, acc.: 85.94%] [G loss: 2.705099]\n",
      "epoch:10 step:7831 [D loss: 0.332756, acc.: 82.81%] [G loss: 2.780957]\n",
      "epoch:10 step:7832 [D loss: 0.289135, acc.: 86.72%] [G loss: 2.761337]\n",
      "epoch:10 step:7833 [D loss: 0.222365, acc.: 91.41%] [G loss: 4.095812]\n",
      "epoch:10 step:7834 [D loss: 0.293290, acc.: 88.28%] [G loss: 3.787333]\n",
      "epoch:10 step:7835 [D loss: 0.351616, acc.: 83.59%] [G loss: 2.816860]\n",
      "epoch:10 step:7836 [D loss: 0.277560, acc.: 86.72%] [G loss: 3.193702]\n",
      "epoch:10 step:7837 [D loss: 0.265953, acc.: 90.62%] [G loss: 2.834519]\n",
      "epoch:10 step:7838 [D loss: 0.302720, acc.: 88.28%] [G loss: 4.045647]\n",
      "epoch:10 step:7839 [D loss: 0.469830, acc.: 79.69%] [G loss: 4.197690]\n",
      "epoch:10 step:7840 [D loss: 0.366314, acc.: 86.72%] [G loss: 2.701440]\n",
      "epoch:10 step:7841 [D loss: 0.346041, acc.: 85.94%] [G loss: 3.931228]\n",
      "epoch:10 step:7842 [D loss: 0.472018, acc.: 82.81%] [G loss: 2.865290]\n",
      "epoch:10 step:7843 [D loss: 0.210739, acc.: 91.41%] [G loss: 4.635780]\n",
      "epoch:10 step:7844 [D loss: 0.256685, acc.: 88.28%] [G loss: 3.353845]\n",
      "epoch:10 step:7845 [D loss: 0.254901, acc.: 88.28%] [G loss: 3.070893]\n",
      "epoch:10 step:7846 [D loss: 0.366216, acc.: 84.38%] [G loss: 3.396981]\n",
      "epoch:10 step:7847 [D loss: 0.307361, acc.: 88.28%] [G loss: 3.184824]\n",
      "epoch:10 step:7848 [D loss: 0.353573, acc.: 87.50%] [G loss: 1.776291]\n",
      "epoch:10 step:7849 [D loss: 0.297692, acc.: 91.41%] [G loss: 2.214832]\n",
      "epoch:10 step:7850 [D loss: 0.266041, acc.: 91.41%] [G loss: 2.021429]\n",
      "epoch:10 step:7851 [D loss: 0.366752, acc.: 85.16%] [G loss: 1.884612]\n",
      "epoch:10 step:7852 [D loss: 0.255125, acc.: 91.41%] [G loss: 2.772826]\n",
      "epoch:10 step:7853 [D loss: 0.397113, acc.: 82.03%] [G loss: 2.310458]\n",
      "epoch:10 step:7854 [D loss: 0.233401, acc.: 93.75%] [G loss: 2.570676]\n",
      "epoch:10 step:7855 [D loss: 0.324639, acc.: 85.94%] [G loss: 3.143923]\n",
      "epoch:10 step:7856 [D loss: 0.347937, acc.: 86.72%] [G loss: 2.730192]\n",
      "epoch:10 step:7857 [D loss: 0.257941, acc.: 88.28%] [G loss: 3.403022]\n",
      "epoch:10 step:7858 [D loss: 0.297328, acc.: 85.16%] [G loss: 3.074039]\n",
      "epoch:10 step:7859 [D loss: 0.276361, acc.: 87.50%] [G loss: 3.132191]\n",
      "epoch:10 step:7860 [D loss: 0.340829, acc.: 82.03%] [G loss: 2.581321]\n",
      "epoch:10 step:7861 [D loss: 0.282406, acc.: 89.06%] [G loss: 3.333235]\n",
      "epoch:10 step:7862 [D loss: 0.352101, acc.: 82.81%] [G loss: 2.790348]\n",
      "epoch:10 step:7863 [D loss: 0.247724, acc.: 87.50%] [G loss: 3.848725]\n",
      "epoch:10 step:7864 [D loss: 0.254117, acc.: 90.62%] [G loss: 4.265711]\n",
      "epoch:10 step:7865 [D loss: 0.296205, acc.: 87.50%] [G loss: 4.858494]\n",
      "epoch:10 step:7866 [D loss: 0.299286, acc.: 89.84%] [G loss: 2.677965]\n",
      "epoch:10 step:7867 [D loss: 0.342345, acc.: 85.94%] [G loss: 2.360798]\n",
      "epoch:10 step:7868 [D loss: 0.256818, acc.: 88.28%] [G loss: 3.944527]\n",
      "epoch:10 step:7869 [D loss: 0.334892, acc.: 85.16%] [G loss: 3.486401]\n",
      "epoch:10 step:7870 [D loss: 0.373419, acc.: 81.25%] [G loss: 3.158224]\n",
      "epoch:10 step:7871 [D loss: 0.266762, acc.: 88.28%] [G loss: 3.267174]\n",
      "epoch:10 step:7872 [D loss: 0.326018, acc.: 87.50%] [G loss: 3.245563]\n",
      "epoch:10 step:7873 [D loss: 0.278803, acc.: 90.62%] [G loss: 2.413552]\n",
      "epoch:10 step:7874 [D loss: 0.394807, acc.: 82.03%] [G loss: 3.022259]\n",
      "epoch:10 step:7875 [D loss: 0.381090, acc.: 85.16%] [G loss: 2.767134]\n",
      "epoch:10 step:7876 [D loss: 0.269135, acc.: 89.84%] [G loss: 2.328413]\n",
      "epoch:10 step:7877 [D loss: 0.405013, acc.: 84.38%] [G loss: 2.310072]\n",
      "epoch:10 step:7878 [D loss: 0.331070, acc.: 85.16%] [G loss: 3.177187]\n",
      "epoch:10 step:7879 [D loss: 0.332611, acc.: 83.59%] [G loss: 4.060966]\n",
      "epoch:10 step:7880 [D loss: 0.436417, acc.: 80.47%] [G loss: 3.643565]\n",
      "epoch:10 step:7881 [D loss: 0.382602, acc.: 82.81%] [G loss: 2.808582]\n",
      "epoch:10 step:7882 [D loss: 0.296251, acc.: 90.62%] [G loss: 3.916555]\n",
      "epoch:10 step:7883 [D loss: 0.540077, acc.: 81.25%] [G loss: 2.782873]\n",
      "epoch:10 step:7884 [D loss: 0.430849, acc.: 84.38%] [G loss: 3.409980]\n",
      "epoch:10 step:7885 [D loss: 0.629458, acc.: 70.31%] [G loss: 3.228774]\n",
      "epoch:10 step:7886 [D loss: 0.580450, acc.: 78.12%] [G loss: 3.535202]\n",
      "epoch:10 step:7887 [D loss: 0.349566, acc.: 86.72%] [G loss: 6.513315]\n",
      "epoch:10 step:7888 [D loss: 0.561221, acc.: 78.12%] [G loss: 3.285832]\n",
      "epoch:10 step:7889 [D loss: 0.427780, acc.: 75.78%] [G loss: 2.582226]\n",
      "epoch:10 step:7890 [D loss: 0.310209, acc.: 87.50%] [G loss: 3.158181]\n",
      "epoch:10 step:7891 [D loss: 0.455924, acc.: 82.03%] [G loss: 3.725143]\n",
      "epoch:10 step:7892 [D loss: 0.444751, acc.: 81.25%] [G loss: 5.758983]\n",
      "epoch:10 step:7893 [D loss: 0.624030, acc.: 70.31%] [G loss: 2.933976]\n",
      "epoch:10 step:7894 [D loss: 0.373030, acc.: 86.72%] [G loss: 3.839828]\n",
      "epoch:10 step:7895 [D loss: 0.330707, acc.: 86.72%] [G loss: 2.587146]\n",
      "epoch:10 step:7896 [D loss: 0.292424, acc.: 89.84%] [G loss: 4.239287]\n",
      "epoch:10 step:7897 [D loss: 0.289427, acc.: 86.72%] [G loss: 2.964427]\n",
      "epoch:10 step:7898 [D loss: 0.272962, acc.: 92.97%] [G loss: 2.531490]\n",
      "epoch:10 step:7899 [D loss: 0.299097, acc.: 90.62%] [G loss: 3.468473]\n",
      "epoch:10 step:7900 [D loss: 0.315873, acc.: 87.50%] [G loss: 3.532543]\n",
      "epoch:10 step:7901 [D loss: 0.419981, acc.: 80.47%] [G loss: 2.596040]\n",
      "epoch:10 step:7902 [D loss: 0.452490, acc.: 82.81%] [G loss: 3.060421]\n",
      "epoch:10 step:7903 [D loss: 0.397326, acc.: 82.03%] [G loss: 3.755768]\n",
      "epoch:10 step:7904 [D loss: 0.418664, acc.: 82.03%] [G loss: 2.599706]\n",
      "epoch:10 step:7905 [D loss: 0.349641, acc.: 82.81%] [G loss: 2.601972]\n",
      "epoch:10 step:7906 [D loss: 0.362334, acc.: 86.72%] [G loss: 2.572752]\n",
      "epoch:10 step:7907 [D loss: 0.355379, acc.: 84.38%] [G loss: 2.788498]\n",
      "epoch:10 step:7908 [D loss: 0.293339, acc.: 91.41%] [G loss: 3.462322]\n",
      "epoch:10 step:7909 [D loss: 0.184255, acc.: 93.75%] [G loss: 2.974668]\n",
      "epoch:10 step:7910 [D loss: 0.317412, acc.: 85.94%] [G loss: 5.693563]\n",
      "epoch:10 step:7911 [D loss: 0.296269, acc.: 89.06%] [G loss: 3.499578]\n",
      "epoch:10 step:7912 [D loss: 0.413180, acc.: 78.91%] [G loss: 4.102915]\n",
      "epoch:10 step:7913 [D loss: 0.366971, acc.: 82.03%] [G loss: 2.756376]\n",
      "epoch:10 step:7914 [D loss: 0.324150, acc.: 85.94%] [G loss: 2.905957]\n",
      "epoch:10 step:7915 [D loss: 0.262270, acc.: 89.84%] [G loss: 3.586964]\n",
      "epoch:10 step:7916 [D loss: 0.354195, acc.: 82.03%] [G loss: 3.040852]\n",
      "epoch:10 step:7917 [D loss: 0.330253, acc.: 84.38%] [G loss: 3.115870]\n",
      "epoch:10 step:7918 [D loss: 0.323698, acc.: 88.28%] [G loss: 3.227407]\n",
      "epoch:10 step:7919 [D loss: 0.371558, acc.: 84.38%] [G loss: 3.185421]\n",
      "epoch:10 step:7920 [D loss: 0.322186, acc.: 85.16%] [G loss: 3.512708]\n",
      "epoch:10 step:7921 [D loss: 0.242707, acc.: 92.19%] [G loss: 4.238822]\n",
      "epoch:10 step:7922 [D loss: 0.405660, acc.: 84.38%] [G loss: 5.590938]\n",
      "epoch:10 step:7923 [D loss: 0.395533, acc.: 77.34%] [G loss: 2.874673]\n",
      "epoch:10 step:7924 [D loss: 0.389505, acc.: 80.47%] [G loss: 3.007535]\n",
      "epoch:10 step:7925 [D loss: 0.278474, acc.: 86.72%] [G loss: 3.435157]\n",
      "epoch:10 step:7926 [D loss: 0.344143, acc.: 87.50%] [G loss: 3.314271]\n",
      "epoch:10 step:7927 [D loss: 0.274029, acc.: 90.62%] [G loss: 2.750164]\n",
      "epoch:10 step:7928 [D loss: 0.335524, acc.: 82.03%] [G loss: 5.371950]\n",
      "epoch:10 step:7929 [D loss: 0.555316, acc.: 76.56%] [G loss: 3.164608]\n",
      "epoch:10 step:7930 [D loss: 0.521575, acc.: 78.91%] [G loss: 4.079723]\n",
      "epoch:10 step:7931 [D loss: 0.260001, acc.: 90.62%] [G loss: 3.374841]\n",
      "epoch:10 step:7932 [D loss: 0.380571, acc.: 86.72%] [G loss: 3.013950]\n",
      "epoch:10 step:7933 [D loss: 0.274675, acc.: 88.28%] [G loss: 4.065868]\n",
      "epoch:10 step:7934 [D loss: 0.310628, acc.: 87.50%] [G loss: 5.445894]\n",
      "epoch:10 step:7935 [D loss: 0.441905, acc.: 79.69%] [G loss: 2.729115]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:10 step:7936 [D loss: 0.333075, acc.: 84.38%] [G loss: 5.310566]\n",
      "epoch:10 step:7937 [D loss: 0.270453, acc.: 91.41%] [G loss: 3.545840]\n",
      "epoch:10 step:7938 [D loss: 0.244015, acc.: 90.62%] [G loss: 2.476096]\n",
      "epoch:10 step:7939 [D loss: 0.319545, acc.: 85.94%] [G loss: 3.467728]\n",
      "epoch:10 step:7940 [D loss: 0.284003, acc.: 85.16%] [G loss: 3.461676]\n",
      "epoch:10 step:7941 [D loss: 0.343485, acc.: 85.16%] [G loss: 2.625563]\n",
      "epoch:10 step:7942 [D loss: 0.364536, acc.: 82.81%] [G loss: 3.326185]\n",
      "epoch:10 step:7943 [D loss: 0.363323, acc.: 82.81%] [G loss: 2.951714]\n",
      "epoch:10 step:7944 [D loss: 0.365638, acc.: 86.72%] [G loss: 3.916057]\n",
      "epoch:10 step:7945 [D loss: 0.389013, acc.: 84.38%] [G loss: 3.762669]\n",
      "epoch:10 step:7946 [D loss: 0.299723, acc.: 86.72%] [G loss: 2.700625]\n",
      "epoch:10 step:7947 [D loss: 0.464276, acc.: 78.12%] [G loss: 3.613225]\n",
      "epoch:10 step:7948 [D loss: 0.337094, acc.: 85.94%] [G loss: 2.966223]\n",
      "epoch:10 step:7949 [D loss: 0.250006, acc.: 88.28%] [G loss: 3.621473]\n",
      "epoch:10 step:7950 [D loss: 0.195190, acc.: 93.75%] [G loss: 4.160273]\n",
      "epoch:10 step:7951 [D loss: 0.404578, acc.: 78.91%] [G loss: 3.438747]\n",
      "epoch:10 step:7952 [D loss: 0.323353, acc.: 85.16%] [G loss: 2.326300]\n",
      "epoch:10 step:7953 [D loss: 0.272933, acc.: 91.41%] [G loss: 2.708208]\n",
      "epoch:10 step:7954 [D loss: 0.303095, acc.: 89.06%] [G loss: 2.843841]\n",
      "epoch:10 step:7955 [D loss: 0.294699, acc.: 88.28%] [G loss: 2.365362]\n",
      "epoch:10 step:7956 [D loss: 0.348290, acc.: 87.50%] [G loss: 2.655871]\n",
      "epoch:10 step:7957 [D loss: 0.362340, acc.: 86.72%] [G loss: 2.420805]\n",
      "epoch:10 step:7958 [D loss: 0.425518, acc.: 80.47%] [G loss: 2.639871]\n",
      "epoch:10 step:7959 [D loss: 0.294575, acc.: 89.84%] [G loss: 3.063447]\n",
      "epoch:10 step:7960 [D loss: 0.305199, acc.: 88.28%] [G loss: 4.683403]\n",
      "epoch:10 step:7961 [D loss: 0.367757, acc.: 83.59%] [G loss: 5.098016]\n",
      "epoch:10 step:7962 [D loss: 0.255625, acc.: 85.94%] [G loss: 4.306548]\n",
      "epoch:10 step:7963 [D loss: 0.332980, acc.: 85.16%] [G loss: 2.596529]\n",
      "epoch:10 step:7964 [D loss: 0.322220, acc.: 86.72%] [G loss: 2.549176]\n",
      "epoch:10 step:7965 [D loss: 0.253166, acc.: 89.84%] [G loss: 3.923907]\n",
      "epoch:10 step:7966 [D loss: 0.264653, acc.: 86.72%] [G loss: 4.516699]\n",
      "epoch:10 step:7967 [D loss: 0.315609, acc.: 87.50%] [G loss: 4.563162]\n",
      "epoch:10 step:7968 [D loss: 0.501725, acc.: 75.00%] [G loss: 2.667099]\n",
      "epoch:10 step:7969 [D loss: 0.333123, acc.: 85.16%] [G loss: 2.699884]\n",
      "epoch:10 step:7970 [D loss: 0.423348, acc.: 78.91%] [G loss: 3.654092]\n",
      "epoch:10 step:7971 [D loss: 0.596243, acc.: 78.91%] [G loss: 5.050745]\n",
      "epoch:10 step:7972 [D loss: 0.745790, acc.: 73.44%] [G loss: 3.122969]\n",
      "epoch:10 step:7973 [D loss: 0.277070, acc.: 89.06%] [G loss: 4.382667]\n",
      "epoch:10 step:7974 [D loss: 0.277636, acc.: 87.50%] [G loss: 3.980181]\n",
      "epoch:10 step:7975 [D loss: 0.325344, acc.: 85.94%] [G loss: 2.469092]\n",
      "epoch:10 step:7976 [D loss: 0.291730, acc.: 86.72%] [G loss: 3.508434]\n",
      "epoch:10 step:7977 [D loss: 0.433244, acc.: 80.47%] [G loss: 2.467444]\n",
      "epoch:10 step:7978 [D loss: 0.275196, acc.: 92.19%] [G loss: 2.535290]\n",
      "epoch:10 step:7979 [D loss: 0.337446, acc.: 85.16%] [G loss: 2.574215]\n",
      "epoch:10 step:7980 [D loss: 0.271799, acc.: 89.84%] [G loss: 3.365638]\n",
      "epoch:10 step:7981 [D loss: 0.327882, acc.: 85.94%] [G loss: 3.617922]\n",
      "epoch:10 step:7982 [D loss: 0.408383, acc.: 82.03%] [G loss: 3.509509]\n",
      "epoch:10 step:7983 [D loss: 0.440181, acc.: 84.38%] [G loss: 2.450421]\n",
      "epoch:10 step:7984 [D loss: 0.361342, acc.: 82.03%] [G loss: 2.880966]\n",
      "epoch:10 step:7985 [D loss: 0.283159, acc.: 90.62%] [G loss: 2.642206]\n",
      "epoch:10 step:7986 [D loss: 0.207554, acc.: 92.97%] [G loss: 4.168634]\n",
      "epoch:10 step:7987 [D loss: 0.371386, acc.: 83.59%] [G loss: 2.774948]\n",
      "epoch:10 step:7988 [D loss: 0.275869, acc.: 86.72%] [G loss: 4.827173]\n",
      "epoch:10 step:7989 [D loss: 0.229952, acc.: 88.28%] [G loss: 6.519606]\n",
      "epoch:10 step:7990 [D loss: 0.329846, acc.: 85.16%] [G loss: 3.961114]\n",
      "epoch:10 step:7991 [D loss: 0.312282, acc.: 88.28%] [G loss: 2.781402]\n",
      "epoch:10 step:7992 [D loss: 0.341349, acc.: 83.59%] [G loss: 4.018579]\n",
      "epoch:10 step:7993 [D loss: 0.307159, acc.: 88.28%] [G loss: 2.995188]\n",
      "epoch:10 step:7994 [D loss: 0.332483, acc.: 85.16%] [G loss: 2.606673]\n",
      "epoch:10 step:7995 [D loss: 0.384751, acc.: 85.16%] [G loss: 2.399606]\n",
      "epoch:10 step:7996 [D loss: 0.297648, acc.: 86.72%] [G loss: 3.429520]\n",
      "epoch:10 step:7997 [D loss: 0.381792, acc.: 80.47%] [G loss: 2.981618]\n",
      "epoch:10 step:7998 [D loss: 0.251077, acc.: 89.84%] [G loss: 5.415118]\n",
      "epoch:10 step:7999 [D loss: 0.243833, acc.: 90.62%] [G loss: 4.504662]\n",
      "epoch:10 step:8000 [D loss: 0.284892, acc.: 90.62%] [G loss: 3.982135]\n",
      "##############\n",
      "[0.84054893 0.87877795 0.78926437 0.79675926 0.76799942 0.81140424\n",
      " 0.86434504 0.82626778 0.82086129 0.81279682]\n",
      "##########\n",
      "epoch:10 step:8001 [D loss: 0.326648, acc.: 82.81%] [G loss: 5.057963]\n",
      "epoch:10 step:8002 [D loss: 0.244830, acc.: 89.06%] [G loss: 3.850969]\n",
      "epoch:10 step:8003 [D loss: 0.399003, acc.: 78.91%] [G loss: 2.398622]\n",
      "epoch:10 step:8004 [D loss: 0.249610, acc.: 92.19%] [G loss: 3.163965]\n",
      "epoch:10 step:8005 [D loss: 0.404373, acc.: 86.72%] [G loss: 3.279948]\n",
      "epoch:10 step:8006 [D loss: 0.282382, acc.: 88.28%] [G loss: 3.370045]\n",
      "epoch:10 step:8007 [D loss: 0.277372, acc.: 86.72%] [G loss: 4.093085]\n",
      "epoch:10 step:8008 [D loss: 0.363160, acc.: 82.81%] [G loss: 2.726946]\n",
      "epoch:10 step:8009 [D loss: 0.317640, acc.: 88.28%] [G loss: 4.506375]\n",
      "epoch:10 step:8010 [D loss: 0.286173, acc.: 85.94%] [G loss: 4.246487]\n",
      "epoch:10 step:8011 [D loss: 0.336024, acc.: 81.25%] [G loss: 2.054879]\n",
      "epoch:10 step:8012 [D loss: 0.376209, acc.: 83.59%] [G loss: 2.757201]\n",
      "epoch:10 step:8013 [D loss: 0.302662, acc.: 87.50%] [G loss: 2.929574]\n",
      "epoch:10 step:8014 [D loss: 0.270619, acc.: 91.41%] [G loss: 5.045778]\n",
      "epoch:10 step:8015 [D loss: 0.355411, acc.: 82.81%] [G loss: 2.499178]\n",
      "epoch:10 step:8016 [D loss: 0.413924, acc.: 85.16%] [G loss: 2.373421]\n",
      "epoch:10 step:8017 [D loss: 0.385003, acc.: 81.25%] [G loss: 2.738811]\n",
      "epoch:10 step:8018 [D loss: 0.402081, acc.: 79.69%] [G loss: 2.835126]\n",
      "epoch:10 step:8019 [D loss: 0.333420, acc.: 85.16%] [G loss: 2.469964]\n",
      "epoch:10 step:8020 [D loss: 0.288938, acc.: 89.84%] [G loss: 3.203190]\n",
      "epoch:10 step:8021 [D loss: 0.365254, acc.: 82.81%] [G loss: 5.965678]\n",
      "epoch:10 step:8022 [D loss: 0.546462, acc.: 75.78%] [G loss: 2.496827]\n",
      "epoch:10 step:8023 [D loss: 0.298018, acc.: 89.06%] [G loss: 2.814047]\n",
      "epoch:10 step:8024 [D loss: 0.316520, acc.: 87.50%] [G loss: 3.379663]\n",
      "epoch:10 step:8025 [D loss: 0.396223, acc.: 83.59%] [G loss: 3.059978]\n",
      "epoch:10 step:8026 [D loss: 0.281138, acc.: 85.94%] [G loss: 3.716576]\n",
      "epoch:10 step:8027 [D loss: 0.464489, acc.: 78.91%] [G loss: 3.349015]\n",
      "epoch:10 step:8028 [D loss: 0.396471, acc.: 84.38%] [G loss: 2.403933]\n",
      "epoch:10 step:8029 [D loss: 0.267082, acc.: 89.84%] [G loss: 3.601004]\n",
      "epoch:10 step:8030 [D loss: 0.343520, acc.: 85.94%] [G loss: 3.429137]\n",
      "epoch:10 step:8031 [D loss: 0.294277, acc.: 91.41%] [G loss: 5.023523]\n",
      "epoch:10 step:8032 [D loss: 0.274621, acc.: 89.06%] [G loss: 4.194630]\n",
      "epoch:10 step:8033 [D loss: 0.223098, acc.: 91.41%] [G loss: 3.294669]\n",
      "epoch:10 step:8034 [D loss: 0.365889, acc.: 85.94%] [G loss: 3.184099]\n",
      "epoch:10 step:8035 [D loss: 0.436725, acc.: 78.12%] [G loss: 2.392258]\n",
      "epoch:10 step:8036 [D loss: 0.436446, acc.: 78.91%] [G loss: 2.966616]\n",
      "epoch:10 step:8037 [D loss: 0.284898, acc.: 88.28%] [G loss: 4.010151]\n",
      "epoch:10 step:8038 [D loss: 0.324516, acc.: 86.72%] [G loss: 4.866441]\n",
      "epoch:10 step:8039 [D loss: 0.490647, acc.: 79.69%] [G loss: 3.118783]\n",
      "epoch:10 step:8040 [D loss: 0.497288, acc.: 75.00%] [G loss: 2.764240]\n",
      "epoch:10 step:8041 [D loss: 0.266478, acc.: 90.62%] [G loss: 2.529764]\n",
      "epoch:10 step:8042 [D loss: 0.438676, acc.: 82.03%] [G loss: 2.605582]\n",
      "epoch:10 step:8043 [D loss: 0.364543, acc.: 84.38%] [G loss: 2.585935]\n",
      "epoch:10 step:8044 [D loss: 0.392759, acc.: 85.16%] [G loss: 2.676589]\n",
      "epoch:10 step:8045 [D loss: 0.350887, acc.: 86.72%] [G loss: 2.366764]\n",
      "epoch:10 step:8046 [D loss: 0.441143, acc.: 81.25%] [G loss: 2.918259]\n",
      "epoch:10 step:8047 [D loss: 0.352300, acc.: 85.94%] [G loss: 4.223687]\n",
      "epoch:10 step:8048 [D loss: 0.238906, acc.: 89.06%] [G loss: 4.067053]\n",
      "epoch:10 step:8049 [D loss: 0.297181, acc.: 84.38%] [G loss: 3.870048]\n",
      "epoch:10 step:8050 [D loss: 0.305264, acc.: 84.38%] [G loss: 6.356546]\n",
      "epoch:10 step:8051 [D loss: 0.338847, acc.: 80.47%] [G loss: 3.993232]\n",
      "epoch:10 step:8052 [D loss: 0.525348, acc.: 78.91%] [G loss: 2.628669]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:10 step:8053 [D loss: 0.243772, acc.: 89.06%] [G loss: 4.956296]\n",
      "epoch:10 step:8054 [D loss: 0.318293, acc.: 82.81%] [G loss: 4.909163]\n",
      "epoch:10 step:8055 [D loss: 0.420607, acc.: 78.12%] [G loss: 2.682562]\n",
      "epoch:10 step:8056 [D loss: 0.337607, acc.: 87.50%] [G loss: 3.099793]\n",
      "epoch:10 step:8057 [D loss: 0.404340, acc.: 82.81%] [G loss: 4.453207]\n",
      "epoch:10 step:8058 [D loss: 0.392027, acc.: 84.38%] [G loss: 3.201974]\n",
      "epoch:10 step:8059 [D loss: 0.332655, acc.: 86.72%] [G loss: 4.306758]\n",
      "epoch:10 step:8060 [D loss: 0.389248, acc.: 84.38%] [G loss: 6.199630]\n",
      "epoch:10 step:8061 [D loss: 0.824229, acc.: 68.75%] [G loss: 4.439280]\n",
      "epoch:10 step:8062 [D loss: 0.507347, acc.: 82.81%] [G loss: 7.952502]\n",
      "epoch:10 step:8063 [D loss: 0.747912, acc.: 67.19%] [G loss: 3.512284]\n",
      "epoch:10 step:8064 [D loss: 0.520165, acc.: 81.25%] [G loss: 5.514718]\n",
      "epoch:10 step:8065 [D loss: 0.346261, acc.: 84.38%] [G loss: 3.013375]\n",
      "epoch:10 step:8066 [D loss: 0.238472, acc.: 91.41%] [G loss: 4.074214]\n",
      "epoch:10 step:8067 [D loss: 0.383521, acc.: 78.12%] [G loss: 4.378165]\n",
      "epoch:10 step:8068 [D loss: 0.360531, acc.: 84.38%] [G loss: 3.516402]\n",
      "epoch:10 step:8069 [D loss: 0.410725, acc.: 79.69%] [G loss: 2.441994]\n",
      "epoch:10 step:8070 [D loss: 0.220554, acc.: 92.97%] [G loss: 3.052730]\n",
      "epoch:10 step:8071 [D loss: 0.319815, acc.: 89.06%] [G loss: 2.882171]\n",
      "epoch:10 step:8072 [D loss: 0.357972, acc.: 85.94%] [G loss: 2.921237]\n",
      "epoch:10 step:8073 [D loss: 0.389742, acc.: 82.03%] [G loss: 2.178115]\n",
      "epoch:10 step:8074 [D loss: 0.406013, acc.: 86.72%] [G loss: 2.800131]\n",
      "epoch:10 step:8075 [D loss: 0.397126, acc.: 84.38%] [G loss: 2.370842]\n",
      "epoch:10 step:8076 [D loss: 0.337099, acc.: 85.16%] [G loss: 2.885029]\n",
      "epoch:10 step:8077 [D loss: 0.229973, acc.: 89.84%] [G loss: 4.198884]\n",
      "epoch:10 step:8078 [D loss: 0.289047, acc.: 88.28%] [G loss: 4.654165]\n",
      "epoch:10 step:8079 [D loss: 0.263574, acc.: 90.62%] [G loss: 3.101238]\n",
      "epoch:10 step:8080 [D loss: 0.247711, acc.: 92.19%] [G loss: 2.333027]\n",
      "epoch:10 step:8081 [D loss: 0.263627, acc.: 90.62%] [G loss: 3.009733]\n",
      "epoch:10 step:8082 [D loss: 0.331606, acc.: 85.94%] [G loss: 2.369288]\n",
      "epoch:10 step:8083 [D loss: 0.277213, acc.: 90.62%] [G loss: 2.624183]\n",
      "epoch:10 step:8084 [D loss: 0.274114, acc.: 89.84%] [G loss: 2.186999]\n",
      "epoch:10 step:8085 [D loss: 0.337814, acc.: 84.38%] [G loss: 3.167060]\n",
      "epoch:10 step:8086 [D loss: 0.379528, acc.: 83.59%] [G loss: 2.091653]\n",
      "epoch:10 step:8087 [D loss: 0.373763, acc.: 84.38%] [G loss: 3.569907]\n",
      "epoch:10 step:8088 [D loss: 0.423580, acc.: 80.47%] [G loss: 3.525452]\n",
      "epoch:10 step:8089 [D loss: 0.559157, acc.: 74.22%] [G loss: 6.818534]\n",
      "epoch:10 step:8090 [D loss: 1.129041, acc.: 66.41%] [G loss: 5.772889]\n",
      "epoch:10 step:8091 [D loss: 0.320912, acc.: 89.06%] [G loss: 4.527629]\n",
      "epoch:10 step:8092 [D loss: 0.404544, acc.: 84.38%] [G loss: 4.519264]\n",
      "epoch:10 step:8093 [D loss: 0.242135, acc.: 87.50%] [G loss: 4.449068]\n",
      "epoch:10 step:8094 [D loss: 0.682670, acc.: 69.53%] [G loss: 1.867269]\n",
      "epoch:10 step:8095 [D loss: 0.448829, acc.: 79.69%] [G loss: 2.762825]\n",
      "epoch:10 step:8096 [D loss: 0.355668, acc.: 84.38%] [G loss: 2.725049]\n",
      "epoch:10 step:8097 [D loss: 0.280227, acc.: 89.06%] [G loss: 4.117859]\n",
      "epoch:10 step:8098 [D loss: 0.288688, acc.: 88.28%] [G loss: 2.762423]\n",
      "epoch:10 step:8099 [D loss: 0.357616, acc.: 85.94%] [G loss: 2.628661]\n",
      "epoch:10 step:8100 [D loss: 0.239981, acc.: 92.97%] [G loss: 2.796881]\n",
      "epoch:10 step:8101 [D loss: 0.304682, acc.: 87.50%] [G loss: 2.729607]\n",
      "epoch:10 step:8102 [D loss: 0.326096, acc.: 85.94%] [G loss: 2.853929]\n",
      "epoch:10 step:8103 [D loss: 0.264070, acc.: 89.06%] [G loss: 2.875583]\n",
      "epoch:10 step:8104 [D loss: 0.352867, acc.: 85.16%] [G loss: 2.783017]\n",
      "epoch:10 step:8105 [D loss: 0.333640, acc.: 88.28%] [G loss: 2.175317]\n",
      "epoch:10 step:8106 [D loss: 0.286040, acc.: 90.62%] [G loss: 2.198220]\n",
      "epoch:10 step:8107 [D loss: 0.249985, acc.: 88.28%] [G loss: 3.095379]\n",
      "epoch:10 step:8108 [D loss: 0.321421, acc.: 85.94%] [G loss: 3.155734]\n",
      "epoch:10 step:8109 [D loss: 0.251363, acc.: 89.06%] [G loss: 4.043987]\n",
      "epoch:10 step:8110 [D loss: 0.339536, acc.: 85.16%] [G loss: 3.690165]\n",
      "epoch:10 step:8111 [D loss: 0.322706, acc.: 89.06%] [G loss: 3.272617]\n",
      "epoch:10 step:8112 [D loss: 0.313149, acc.: 85.16%] [G loss: 2.082877]\n",
      "epoch:10 step:8113 [D loss: 0.306661, acc.: 89.06%] [G loss: 2.617404]\n",
      "epoch:10 step:8114 [D loss: 0.348053, acc.: 89.06%] [G loss: 2.359348]\n",
      "epoch:10 step:8115 [D loss: 0.346230, acc.: 85.94%] [G loss: 2.753620]\n",
      "epoch:10 step:8116 [D loss: 0.338543, acc.: 80.47%] [G loss: 3.291134]\n",
      "epoch:10 step:8117 [D loss: 0.252644, acc.: 88.28%] [G loss: 2.684785]\n",
      "epoch:10 step:8118 [D loss: 0.403898, acc.: 85.94%] [G loss: 2.945387]\n",
      "epoch:10 step:8119 [D loss: 0.279887, acc.: 85.16%] [G loss: 4.645120]\n",
      "epoch:10 step:8120 [D loss: 0.167048, acc.: 94.53%] [G loss: 3.301684]\n",
      "epoch:10 step:8121 [D loss: 0.248392, acc.: 89.06%] [G loss: 3.168194]\n",
      "epoch:10 step:8122 [D loss: 0.247188, acc.: 90.62%] [G loss: 2.451118]\n",
      "epoch:10 step:8123 [D loss: 0.234506, acc.: 92.97%] [G loss: 2.767323]\n",
      "epoch:10 step:8124 [D loss: 0.281499, acc.: 88.28%] [G loss: 2.797951]\n",
      "epoch:10 step:8125 [D loss: 0.444377, acc.: 79.69%] [G loss: 3.020344]\n",
      "epoch:10 step:8126 [D loss: 0.388710, acc.: 82.03%] [G loss: 5.149045]\n",
      "epoch:10 step:8127 [D loss: 0.577304, acc.: 78.12%] [G loss: 3.775396]\n",
      "epoch:10 step:8128 [D loss: 0.802370, acc.: 68.75%] [G loss: 5.466243]\n",
      "epoch:10 step:8129 [D loss: 0.699805, acc.: 71.88%] [G loss: 3.692814]\n",
      "epoch:10 step:8130 [D loss: 0.411773, acc.: 79.69%] [G loss: 2.438833]\n",
      "epoch:10 step:8131 [D loss: 0.350384, acc.: 85.16%] [G loss: 1.700612]\n",
      "epoch:10 step:8132 [D loss: 0.435981, acc.: 82.81%] [G loss: 4.470000]\n",
      "epoch:10 step:8133 [D loss: 0.298012, acc.: 88.28%] [G loss: 3.246160]\n",
      "epoch:10 step:8134 [D loss: 0.405850, acc.: 83.59%] [G loss: 4.056020]\n",
      "epoch:10 step:8135 [D loss: 0.311749, acc.: 88.28%] [G loss: 2.196115]\n",
      "epoch:10 step:8136 [D loss: 0.349528, acc.: 85.94%] [G loss: 2.533914]\n",
      "epoch:10 step:8137 [D loss: 0.309708, acc.: 85.94%] [G loss: 2.121611]\n",
      "epoch:10 step:8138 [D loss: 0.237166, acc.: 91.41%] [G loss: 3.592426]\n",
      "epoch:10 step:8139 [D loss: 0.244515, acc.: 87.50%] [G loss: 5.013046]\n",
      "epoch:10 step:8140 [D loss: 0.332840, acc.: 83.59%] [G loss: 2.216032]\n",
      "epoch:10 step:8141 [D loss: 0.294021, acc.: 85.94%] [G loss: 2.789068]\n",
      "epoch:10 step:8142 [D loss: 0.273317, acc.: 90.62%] [G loss: 3.312667]\n",
      "epoch:10 step:8143 [D loss: 0.295823, acc.: 90.62%] [G loss: 2.373854]\n",
      "epoch:10 step:8144 [D loss: 0.466928, acc.: 76.56%] [G loss: 1.914017]\n",
      "epoch:10 step:8145 [D loss: 0.232432, acc.: 94.53%] [G loss: 2.478982]\n",
      "epoch:10 step:8146 [D loss: 0.353673, acc.: 86.72%] [G loss: 3.207634]\n",
      "epoch:10 step:8147 [D loss: 0.266321, acc.: 88.28%] [G loss: 3.589052]\n",
      "epoch:10 step:8148 [D loss: 0.274601, acc.: 89.84%] [G loss: 2.168353]\n",
      "epoch:10 step:8149 [D loss: 0.292211, acc.: 89.06%] [G loss: 2.154202]\n",
      "epoch:10 step:8150 [D loss: 0.289958, acc.: 90.62%] [G loss: 2.008888]\n",
      "epoch:10 step:8151 [D loss: 0.466573, acc.: 82.03%] [G loss: 2.400427]\n",
      "epoch:10 step:8152 [D loss: 0.298313, acc.: 89.06%] [G loss: 3.050052]\n",
      "epoch:10 step:8153 [D loss: 0.277709, acc.: 85.94%] [G loss: 3.173832]\n",
      "epoch:10 step:8154 [D loss: 0.289865, acc.: 86.72%] [G loss: 3.239636]\n",
      "epoch:10 step:8155 [D loss: 0.260824, acc.: 91.41%] [G loss: 2.631272]\n",
      "epoch:10 step:8156 [D loss: 0.316316, acc.: 90.62%] [G loss: 2.890281]\n",
      "epoch:10 step:8157 [D loss: 0.269481, acc.: 90.62%] [G loss: 2.383599]\n",
      "epoch:10 step:8158 [D loss: 0.318701, acc.: 88.28%] [G loss: 3.578613]\n",
      "epoch:10 step:8159 [D loss: 0.309008, acc.: 90.62%] [G loss: 2.803900]\n",
      "epoch:10 step:8160 [D loss: 0.244571, acc.: 90.62%] [G loss: 2.566216]\n",
      "epoch:10 step:8161 [D loss: 0.354143, acc.: 89.06%] [G loss: 2.827737]\n",
      "epoch:10 step:8162 [D loss: 0.415894, acc.: 79.69%] [G loss: 2.894704]\n",
      "epoch:10 step:8163 [D loss: 0.327284, acc.: 89.06%] [G loss: 2.936733]\n",
      "epoch:10 step:8164 [D loss: 0.339137, acc.: 88.28%] [G loss: 2.842537]\n",
      "epoch:10 step:8165 [D loss: 0.360583, acc.: 83.59%] [G loss: 2.843071]\n",
      "epoch:10 step:8166 [D loss: 0.285139, acc.: 89.06%] [G loss: 3.458429]\n",
      "epoch:10 step:8167 [D loss: 0.295627, acc.: 85.94%] [G loss: 3.333921]\n",
      "epoch:10 step:8168 [D loss: 0.329493, acc.: 87.50%] [G loss: 2.461664]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:10 step:8169 [D loss: 0.236692, acc.: 89.06%] [G loss: 3.567892]\n",
      "epoch:10 step:8170 [D loss: 0.245629, acc.: 89.06%] [G loss: 2.849018]\n",
      "epoch:10 step:8171 [D loss: 0.305958, acc.: 86.72%] [G loss: 3.080003]\n",
      "epoch:10 step:8172 [D loss: 0.400798, acc.: 82.81%] [G loss: 2.129239]\n",
      "epoch:10 step:8173 [D loss: 0.313808, acc.: 87.50%] [G loss: 2.660913]\n",
      "epoch:10 step:8174 [D loss: 0.310199, acc.: 89.06%] [G loss: 2.837847]\n",
      "epoch:10 step:8175 [D loss: 0.319223, acc.: 85.16%] [G loss: 4.165074]\n",
      "epoch:10 step:8176 [D loss: 0.314284, acc.: 84.38%] [G loss: 4.228587]\n",
      "epoch:10 step:8177 [D loss: 0.238842, acc.: 86.72%] [G loss: 4.413875]\n",
      "epoch:10 step:8178 [D loss: 0.355549, acc.: 85.94%] [G loss: 2.377868]\n",
      "epoch:10 step:8179 [D loss: 0.247145, acc.: 91.41%] [G loss: 2.917109]\n",
      "epoch:10 step:8180 [D loss: 0.250017, acc.: 91.41%] [G loss: 2.526337]\n",
      "epoch:10 step:8181 [D loss: 0.303076, acc.: 89.06%] [G loss: 2.625314]\n",
      "epoch:10 step:8182 [D loss: 0.264639, acc.: 89.84%] [G loss: 3.698275]\n",
      "epoch:10 step:8183 [D loss: 0.249551, acc.: 89.06%] [G loss: 3.223444]\n",
      "epoch:10 step:8184 [D loss: 0.353075, acc.: 85.94%] [G loss: 3.160781]\n",
      "epoch:10 step:8185 [D loss: 0.254214, acc.: 90.62%] [G loss: 2.728118]\n",
      "epoch:10 step:8186 [D loss: 0.384582, acc.: 82.03%] [G loss: 2.762508]\n",
      "epoch:10 step:8187 [D loss: 0.386966, acc.: 82.03%] [G loss: 2.193875]\n",
      "epoch:10 step:8188 [D loss: 0.328829, acc.: 88.28%] [G loss: 3.055167]\n",
      "epoch:10 step:8189 [D loss: 0.349798, acc.: 85.16%] [G loss: 3.023192]\n",
      "epoch:10 step:8190 [D loss: 0.267234, acc.: 89.84%] [G loss: 3.332808]\n",
      "epoch:10 step:8191 [D loss: 0.257503, acc.: 92.19%] [G loss: 2.280994]\n",
      "epoch:10 step:8192 [D loss: 0.317011, acc.: 91.41%] [G loss: 2.458490]\n",
      "epoch:10 step:8193 [D loss: 0.228084, acc.: 94.53%] [G loss: 2.423755]\n",
      "epoch:10 step:8194 [D loss: 0.265881, acc.: 88.28%] [G loss: 3.056260]\n",
      "epoch:10 step:8195 [D loss: 0.298497, acc.: 85.94%] [G loss: 4.091160]\n",
      "epoch:10 step:8196 [D loss: 0.269507, acc.: 89.06%] [G loss: 3.732464]\n",
      "epoch:10 step:8197 [D loss: 0.336907, acc.: 85.94%] [G loss: 2.798293]\n",
      "epoch:10 step:8198 [D loss: 0.327924, acc.: 87.50%] [G loss: 2.570770]\n",
      "epoch:10 step:8199 [D loss: 0.321569, acc.: 90.62%] [G loss: 2.519021]\n",
      "epoch:10 step:8200 [D loss: 0.318735, acc.: 87.50%] [G loss: 2.367862]\n",
      "##############\n",
      "[0.85067806 0.8859896  0.78685304 0.78483746 0.78573277 0.83088057\n",
      " 0.84059769 0.85286499 0.82777616 0.80584435]\n",
      "##########\n",
      "epoch:10 step:8201 [D loss: 0.423930, acc.: 78.91%] [G loss: 2.529961]\n",
      "epoch:10 step:8202 [D loss: 0.317654, acc.: 88.28%] [G loss: 2.948931]\n",
      "epoch:10 step:8203 [D loss: 0.263526, acc.: 88.28%] [G loss: 2.865268]\n",
      "epoch:10 step:8204 [D loss: 0.235803, acc.: 92.19%] [G loss: 2.945398]\n",
      "epoch:10 step:8205 [D loss: 0.285543, acc.: 87.50%] [G loss: 3.356204]\n",
      "epoch:10 step:8206 [D loss: 0.310679, acc.: 86.72%] [G loss: 2.166632]\n",
      "epoch:10 step:8207 [D loss: 0.356782, acc.: 88.28%] [G loss: 3.015275]\n",
      "epoch:10 step:8208 [D loss: 0.282268, acc.: 89.84%] [G loss: 2.680478]\n",
      "epoch:10 step:8209 [D loss: 0.281235, acc.: 86.72%] [G loss: 3.312882]\n",
      "epoch:10 step:8210 [D loss: 0.277539, acc.: 89.84%] [G loss: 3.087310]\n",
      "epoch:10 step:8211 [D loss: 0.248094, acc.: 93.75%] [G loss: 2.347928]\n",
      "epoch:10 step:8212 [D loss: 0.245188, acc.: 92.97%] [G loss: 2.695715]\n",
      "epoch:10 step:8213 [D loss: 0.297262, acc.: 85.94%] [G loss: 2.568517]\n",
      "epoch:10 step:8214 [D loss: 0.253026, acc.: 89.06%] [G loss: 3.961258]\n",
      "epoch:10 step:8215 [D loss: 0.296241, acc.: 88.28%] [G loss: 4.438703]\n",
      "epoch:10 step:8216 [D loss: 0.273792, acc.: 84.38%] [G loss: 4.287712]\n",
      "epoch:10 step:8217 [D loss: 0.408457, acc.: 79.69%] [G loss: 3.337647]\n",
      "epoch:10 step:8218 [D loss: 0.319782, acc.: 87.50%] [G loss: 4.023261]\n",
      "epoch:10 step:8219 [D loss: 0.301955, acc.: 85.16%] [G loss: 3.453447]\n",
      "epoch:10 step:8220 [D loss: 0.269074, acc.: 91.41%] [G loss: 2.292363]\n",
      "epoch:10 step:8221 [D loss: 0.289561, acc.: 85.94%] [G loss: 3.801445]\n",
      "epoch:10 step:8222 [D loss: 0.296042, acc.: 89.06%] [G loss: 3.401954]\n",
      "epoch:10 step:8223 [D loss: 0.277691, acc.: 91.41%] [G loss: 3.926334]\n",
      "epoch:10 step:8224 [D loss: 0.236368, acc.: 92.19%] [G loss: 3.029685]\n",
      "epoch:10 step:8225 [D loss: 0.227733, acc.: 90.62%] [G loss: 4.311653]\n",
      "epoch:10 step:8226 [D loss: 0.190441, acc.: 91.41%] [G loss: 5.178980]\n",
      "epoch:10 step:8227 [D loss: 0.292981, acc.: 87.50%] [G loss: 2.332035]\n",
      "epoch:10 step:8228 [D loss: 0.398996, acc.: 82.81%] [G loss: 3.219778]\n",
      "epoch:10 step:8229 [D loss: 0.390244, acc.: 85.16%] [G loss: 2.555492]\n",
      "epoch:10 step:8230 [D loss: 0.289186, acc.: 86.72%] [G loss: 3.390521]\n",
      "epoch:10 step:8231 [D loss: 0.307160, acc.: 85.94%] [G loss: 3.947556]\n",
      "epoch:10 step:8232 [D loss: 0.233329, acc.: 92.19%] [G loss: 4.705722]\n",
      "epoch:10 step:8233 [D loss: 0.317322, acc.: 86.72%] [G loss: 3.356220]\n",
      "epoch:10 step:8234 [D loss: 0.284250, acc.: 88.28%] [G loss: 2.530179]\n",
      "epoch:10 step:8235 [D loss: 0.287939, acc.: 86.72%] [G loss: 4.661850]\n",
      "epoch:10 step:8236 [D loss: 0.340197, acc.: 86.72%] [G loss: 2.107852]\n",
      "epoch:10 step:8237 [D loss: 0.236772, acc.: 90.62%] [G loss: 3.417141]\n",
      "epoch:10 step:8238 [D loss: 0.307270, acc.: 88.28%] [G loss: 3.158934]\n",
      "epoch:10 step:8239 [D loss: 0.351721, acc.: 85.94%] [G loss: 3.045407]\n",
      "epoch:10 step:8240 [D loss: 0.289907, acc.: 89.84%] [G loss: 3.447337]\n",
      "epoch:10 step:8241 [D loss: 0.353748, acc.: 86.72%] [G loss: 3.119339]\n",
      "epoch:10 step:8242 [D loss: 0.401569, acc.: 84.38%] [G loss: 3.176871]\n",
      "epoch:10 step:8243 [D loss: 0.169701, acc.: 91.41%] [G loss: 5.978377]\n",
      "epoch:10 step:8244 [D loss: 0.423445, acc.: 81.25%] [G loss: 3.230918]\n",
      "epoch:10 step:8245 [D loss: 0.271927, acc.: 85.16%] [G loss: 7.980469]\n",
      "epoch:10 step:8246 [D loss: 0.273414, acc.: 86.72%] [G loss: 5.631970]\n",
      "epoch:10 step:8247 [D loss: 0.209655, acc.: 89.06%] [G loss: 7.450392]\n",
      "epoch:10 step:8248 [D loss: 0.193669, acc.: 89.06%] [G loss: 5.365723]\n",
      "epoch:10 step:8249 [D loss: 0.288765, acc.: 85.94%] [G loss: 4.799373]\n",
      "epoch:10 step:8250 [D loss: 0.226811, acc.: 92.97%] [G loss: 3.500756]\n",
      "epoch:10 step:8251 [D loss: 0.214016, acc.: 92.19%] [G loss: 3.216377]\n",
      "epoch:10 step:8252 [D loss: 0.362462, acc.: 86.72%] [G loss: 2.986344]\n",
      "epoch:10 step:8253 [D loss: 0.372747, acc.: 82.81%] [G loss: 2.494211]\n",
      "epoch:10 step:8254 [D loss: 0.282590, acc.: 90.62%] [G loss: 2.922891]\n",
      "epoch:10 step:8255 [D loss: 0.227899, acc.: 91.41%] [G loss: 3.567795]\n",
      "epoch:10 step:8256 [D loss: 0.351623, acc.: 86.72%] [G loss: 3.464336]\n",
      "epoch:10 step:8257 [D loss: 0.355170, acc.: 83.59%] [G loss: 2.952231]\n",
      "epoch:10 step:8258 [D loss: 0.264224, acc.: 89.06%] [G loss: 3.461618]\n",
      "epoch:10 step:8259 [D loss: 0.265253, acc.: 89.84%] [G loss: 3.428751]\n",
      "epoch:10 step:8260 [D loss: 0.208798, acc.: 92.97%] [G loss: 3.338292]\n",
      "epoch:10 step:8261 [D loss: 0.329535, acc.: 85.94%] [G loss: 3.382960]\n",
      "epoch:10 step:8262 [D loss: 0.368764, acc.: 88.28%] [G loss: 3.586930]\n",
      "epoch:10 step:8263 [D loss: 0.235677, acc.: 87.50%] [G loss: 3.336483]\n",
      "epoch:10 step:8264 [D loss: 0.362989, acc.: 85.16%] [G loss: 2.449975]\n",
      "epoch:10 step:8265 [D loss: 0.364428, acc.: 85.16%] [G loss: 4.097807]\n",
      "epoch:10 step:8266 [D loss: 0.434599, acc.: 77.34%] [G loss: 2.356445]\n",
      "epoch:10 step:8267 [D loss: 0.253789, acc.: 91.41%] [G loss: 3.349919]\n",
      "epoch:10 step:8268 [D loss: 0.226291, acc.: 94.53%] [G loss: 4.010730]\n",
      "epoch:10 step:8269 [D loss: 0.323978, acc.: 89.84%] [G loss: 3.908766]\n",
      "epoch:10 step:8270 [D loss: 0.307425, acc.: 86.72%] [G loss: 3.067838]\n",
      "epoch:10 step:8271 [D loss: 0.258575, acc.: 86.72%] [G loss: 2.875468]\n",
      "epoch:10 step:8272 [D loss: 0.288812, acc.: 89.84%] [G loss: 2.960435]\n",
      "epoch:10 step:8273 [D loss: 0.356898, acc.: 85.16%] [G loss: 2.960654]\n",
      "epoch:10 step:8274 [D loss: 0.323238, acc.: 84.38%] [G loss: 3.395792]\n",
      "epoch:10 step:8275 [D loss: 0.315903, acc.: 91.41%] [G loss: 2.461629]\n",
      "epoch:10 step:8276 [D loss: 0.306238, acc.: 89.06%] [G loss: 3.016699]\n",
      "epoch:10 step:8277 [D loss: 0.316332, acc.: 82.81%] [G loss: 3.834399]\n",
      "epoch:10 step:8278 [D loss: 0.371969, acc.: 83.59%] [G loss: 4.539111]\n",
      "epoch:10 step:8279 [D loss: 0.344290, acc.: 84.38%] [G loss: 4.144212]\n",
      "epoch:10 step:8280 [D loss: 0.248046, acc.: 90.62%] [G loss: 3.352551]\n",
      "epoch:10 step:8281 [D loss: 0.312618, acc.: 88.28%] [G loss: 2.511148]\n",
      "epoch:10 step:8282 [D loss: 0.292299, acc.: 89.84%] [G loss: 3.670151]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:10 step:8283 [D loss: 0.306237, acc.: 85.16%] [G loss: 6.112085]\n",
      "epoch:10 step:8284 [D loss: 0.328466, acc.: 85.16%] [G loss: 4.710551]\n",
      "epoch:10 step:8285 [D loss: 0.401739, acc.: 79.69%] [G loss: 7.786004]\n",
      "epoch:10 step:8286 [D loss: 0.453411, acc.: 82.03%] [G loss: 3.987662]\n",
      "epoch:10 step:8287 [D loss: 0.260804, acc.: 87.50%] [G loss: 5.449751]\n",
      "epoch:10 step:8288 [D loss: 0.318223, acc.: 85.94%] [G loss: 2.882859]\n",
      "epoch:10 step:8289 [D loss: 0.286385, acc.: 92.19%] [G loss: 2.533485]\n",
      "epoch:10 step:8290 [D loss: 0.204454, acc.: 90.62%] [G loss: 3.148270]\n",
      "epoch:10 step:8291 [D loss: 0.285612, acc.: 89.84%] [G loss: 3.462674]\n",
      "epoch:10 step:8292 [D loss: 0.253531, acc.: 89.84%] [G loss: 3.137863]\n",
      "epoch:10 step:8293 [D loss: 0.263590, acc.: 85.94%] [G loss: 3.798715]\n",
      "epoch:10 step:8294 [D loss: 0.339325, acc.: 85.94%] [G loss: 2.341870]\n",
      "epoch:10 step:8295 [D loss: 0.303204, acc.: 85.16%] [G loss: 2.966483]\n",
      "epoch:10 step:8296 [D loss: 0.284355, acc.: 89.06%] [G loss: 3.515166]\n",
      "epoch:10 step:8297 [D loss: 0.282750, acc.: 91.41%] [G loss: 3.274026]\n",
      "epoch:10 step:8298 [D loss: 0.451670, acc.: 78.12%] [G loss: 2.128957]\n",
      "epoch:10 step:8299 [D loss: 0.385853, acc.: 82.81%] [G loss: 2.202292]\n",
      "epoch:10 step:8300 [D loss: 0.280182, acc.: 93.75%] [G loss: 2.997992]\n",
      "epoch:10 step:8301 [D loss: 0.233987, acc.: 92.19%] [G loss: 3.602917]\n",
      "epoch:10 step:8302 [D loss: 0.428604, acc.: 82.81%] [G loss: 3.222033]\n",
      "epoch:10 step:8303 [D loss: 0.271459, acc.: 86.72%] [G loss: 4.229237]\n",
      "epoch:10 step:8304 [D loss: 0.358994, acc.: 87.50%] [G loss: 4.505135]\n",
      "epoch:10 step:8305 [D loss: 0.545162, acc.: 74.22%] [G loss: 4.271382]\n",
      "epoch:10 step:8306 [D loss: 0.678129, acc.: 72.66%] [G loss: 6.433699]\n",
      "epoch:10 step:8307 [D loss: 1.811489, acc.: 55.47%] [G loss: 5.294672]\n",
      "epoch:10 step:8308 [D loss: 0.493541, acc.: 82.81%] [G loss: 3.529336]\n",
      "epoch:10 step:8309 [D loss: 0.590234, acc.: 75.78%] [G loss: 3.748401]\n",
      "epoch:10 step:8310 [D loss: 0.717787, acc.: 71.09%] [G loss: 2.653595]\n",
      "epoch:10 step:8311 [D loss: 0.413857, acc.: 83.59%] [G loss: 2.651450]\n",
      "epoch:10 step:8312 [D loss: 0.445744, acc.: 76.56%] [G loss: 4.609999]\n",
      "epoch:10 step:8313 [D loss: 0.697625, acc.: 79.69%] [G loss: 4.483704]\n",
      "epoch:10 step:8314 [D loss: 0.344155, acc.: 85.94%] [G loss: 4.166323]\n",
      "epoch:10 step:8315 [D loss: 0.354538, acc.: 86.72%] [G loss: 2.923481]\n",
      "epoch:10 step:8316 [D loss: 0.383140, acc.: 82.81%] [G loss: 4.657847]\n",
      "epoch:10 step:8317 [D loss: 0.377396, acc.: 81.25%] [G loss: 2.299479]\n",
      "epoch:10 step:8318 [D loss: 0.271330, acc.: 88.28%] [G loss: 2.561430]\n",
      "epoch:10 step:8319 [D loss: 0.367993, acc.: 83.59%] [G loss: 2.374030]\n",
      "epoch:10 step:8320 [D loss: 0.297804, acc.: 88.28%] [G loss: 2.091046]\n",
      "epoch:10 step:8321 [D loss: 0.295706, acc.: 90.62%] [G loss: 2.558311]\n",
      "epoch:10 step:8322 [D loss: 0.299029, acc.: 89.84%] [G loss: 2.701550]\n",
      "epoch:10 step:8323 [D loss: 0.281890, acc.: 90.62%] [G loss: 2.614170]\n",
      "epoch:10 step:8324 [D loss: 0.323609, acc.: 88.28%] [G loss: 2.262179]\n",
      "epoch:10 step:8325 [D loss: 0.261301, acc.: 92.97%] [G loss: 2.640756]\n",
      "epoch:10 step:8326 [D loss: 0.276396, acc.: 90.62%] [G loss: 2.401739]\n",
      "epoch:10 step:8327 [D loss: 0.423152, acc.: 82.81%] [G loss: 2.528969]\n",
      "epoch:10 step:8328 [D loss: 0.288656, acc.: 92.19%] [G loss: 2.213041]\n",
      "epoch:10 step:8329 [D loss: 0.345010, acc.: 80.47%] [G loss: 2.803453]\n",
      "epoch:10 step:8330 [D loss: 0.397439, acc.: 84.38%] [G loss: 2.210260]\n",
      "epoch:10 step:8331 [D loss: 0.333455, acc.: 85.16%] [G loss: 2.531448]\n",
      "epoch:10 step:8332 [D loss: 0.272772, acc.: 87.50%] [G loss: 2.897340]\n",
      "epoch:10 step:8333 [D loss: 0.370876, acc.: 86.72%] [G loss: 3.069980]\n",
      "epoch:10 step:8334 [D loss: 0.307329, acc.: 84.38%] [G loss: 3.644562]\n",
      "epoch:10 step:8335 [D loss: 0.234453, acc.: 89.06%] [G loss: 4.111415]\n",
      "epoch:10 step:8336 [D loss: 0.297629, acc.: 87.50%] [G loss: 2.420614]\n",
      "epoch:10 step:8337 [D loss: 0.291179, acc.: 87.50%] [G loss: 2.513998]\n",
      "epoch:10 step:8338 [D loss: 0.292034, acc.: 87.50%] [G loss: 2.519397]\n",
      "epoch:10 step:8339 [D loss: 0.231749, acc.: 89.84%] [G loss: 3.292296]\n",
      "epoch:10 step:8340 [D loss: 0.258345, acc.: 90.62%] [G loss: 2.822795]\n",
      "epoch:10 step:8341 [D loss: 0.291848, acc.: 87.50%] [G loss: 3.243507]\n",
      "epoch:10 step:8342 [D loss: 0.299576, acc.: 90.62%] [G loss: 2.490228]\n",
      "epoch:10 step:8343 [D loss: 0.245698, acc.: 87.50%] [G loss: 3.925945]\n",
      "epoch:10 step:8344 [D loss: 0.226702, acc.: 91.41%] [G loss: 3.681521]\n",
      "epoch:10 step:8345 [D loss: 0.276863, acc.: 91.41%] [G loss: 5.662704]\n",
      "epoch:10 step:8346 [D loss: 0.345290, acc.: 89.84%] [G loss: 2.526534]\n",
      "epoch:10 step:8347 [D loss: 0.370382, acc.: 85.16%] [G loss: 3.841284]\n",
      "epoch:10 step:8348 [D loss: 0.206007, acc.: 92.97%] [G loss: 4.984839]\n",
      "epoch:10 step:8349 [D loss: 0.322643, acc.: 86.72%] [G loss: 4.275780]\n",
      "epoch:10 step:8350 [D loss: 0.229031, acc.: 93.75%] [G loss: 3.346458]\n",
      "epoch:10 step:8351 [D loss: 0.249186, acc.: 91.41%] [G loss: 3.240351]\n",
      "epoch:10 step:8352 [D loss: 0.305697, acc.: 89.06%] [G loss: 2.961628]\n",
      "epoch:10 step:8353 [D loss: 0.328048, acc.: 85.94%] [G loss: 2.352160]\n",
      "epoch:10 step:8354 [D loss: 0.303846, acc.: 85.94%] [G loss: 4.068681]\n",
      "epoch:10 step:8355 [D loss: 0.337871, acc.: 84.38%] [G loss: 3.538938]\n",
      "epoch:10 step:8356 [D loss: 0.246789, acc.: 89.06%] [G loss: 4.969139]\n",
      "epoch:10 step:8357 [D loss: 0.356160, acc.: 85.16%] [G loss: 3.481110]\n",
      "epoch:10 step:8358 [D loss: 0.250809, acc.: 89.06%] [G loss: 2.729937]\n",
      "epoch:10 step:8359 [D loss: 0.349176, acc.: 86.72%] [G loss: 4.283686]\n",
      "epoch:10 step:8360 [D loss: 0.427247, acc.: 81.25%] [G loss: 3.240797]\n",
      "epoch:10 step:8361 [D loss: 0.363775, acc.: 83.59%] [G loss: 3.805192]\n",
      "epoch:10 step:8362 [D loss: 0.496851, acc.: 78.91%] [G loss: 4.488039]\n",
      "epoch:10 step:8363 [D loss: 0.646365, acc.: 68.75%] [G loss: 4.280196]\n",
      "epoch:10 step:8364 [D loss: 1.038126, acc.: 64.84%] [G loss: 7.232200]\n",
      "epoch:10 step:8365 [D loss: 1.291930, acc.: 54.69%] [G loss: 1.534427]\n",
      "epoch:10 step:8366 [D loss: 0.406330, acc.: 84.38%] [G loss: 2.410228]\n",
      "epoch:10 step:8367 [D loss: 0.328043, acc.: 85.16%] [G loss: 3.170445]\n",
      "epoch:10 step:8368 [D loss: 0.342490, acc.: 86.72%] [G loss: 2.597088]\n",
      "epoch:10 step:8369 [D loss: 0.410126, acc.: 80.47%] [G loss: 3.783678]\n",
      "epoch:10 step:8370 [D loss: 0.226231, acc.: 91.41%] [G loss: 4.022817]\n",
      "epoch:10 step:8371 [D loss: 0.303175, acc.: 87.50%] [G loss: 2.650541]\n",
      "epoch:10 step:8372 [D loss: 0.276144, acc.: 88.28%] [G loss: 3.044266]\n",
      "epoch:10 step:8373 [D loss: 0.213694, acc.: 90.62%] [G loss: 3.487638]\n",
      "epoch:10 step:8374 [D loss: 0.360980, acc.: 85.16%] [G loss: 2.897262]\n",
      "epoch:10 step:8375 [D loss: 0.269179, acc.: 91.41%] [G loss: 3.218799]\n",
      "epoch:10 step:8376 [D loss: 0.293375, acc.: 89.84%] [G loss: 2.573939]\n",
      "epoch:10 step:8377 [D loss: 0.286494, acc.: 89.84%] [G loss: 3.283014]\n",
      "epoch:10 step:8378 [D loss: 0.254254, acc.: 88.28%] [G loss: 3.349709]\n",
      "epoch:10 step:8379 [D loss: 0.290430, acc.: 90.62%] [G loss: 3.640933]\n",
      "epoch:10 step:8380 [D loss: 0.261258, acc.: 90.62%] [G loss: 3.236403]\n",
      "epoch:10 step:8381 [D loss: 0.403140, acc.: 83.59%] [G loss: 2.775864]\n",
      "epoch:10 step:8382 [D loss: 0.225938, acc.: 92.19%] [G loss: 2.824619]\n",
      "epoch:10 step:8383 [D loss: 0.346879, acc.: 86.72%] [G loss: 2.624794]\n",
      "epoch:10 step:8384 [D loss: 0.248281, acc.: 89.84%] [G loss: 3.117074]\n",
      "epoch:10 step:8385 [D loss: 0.387761, acc.: 85.16%] [G loss: 2.731554]\n",
      "epoch:10 step:8386 [D loss: 0.426880, acc.: 82.81%] [G loss: 2.632084]\n",
      "epoch:10 step:8387 [D loss: 0.334924, acc.: 86.72%] [G loss: 2.815869]\n",
      "epoch:10 step:8388 [D loss: 0.421996, acc.: 82.03%] [G loss: 2.970250]\n",
      "epoch:10 step:8389 [D loss: 0.386839, acc.: 86.72%] [G loss: 2.513536]\n",
      "epoch:10 step:8390 [D loss: 0.306117, acc.: 89.84%] [G loss: 3.213878]\n",
      "epoch:10 step:8391 [D loss: 0.208017, acc.: 96.09%] [G loss: 2.812876]\n",
      "epoch:10 step:8392 [D loss: 0.296485, acc.: 85.94%] [G loss: 2.600563]\n",
      "epoch:10 step:8393 [D loss: 0.329246, acc.: 85.16%] [G loss: 2.868784]\n",
      "epoch:10 step:8394 [D loss: 0.329196, acc.: 86.72%] [G loss: 3.025630]\n",
      "epoch:10 step:8395 [D loss: 0.369378, acc.: 84.38%] [G loss: 3.044314]\n",
      "epoch:10 step:8396 [D loss: 0.274973, acc.: 90.62%] [G loss: 2.858425]\n",
      "epoch:10 step:8397 [D loss: 0.296884, acc.: 88.28%] [G loss: 2.315773]\n",
      "epoch:10 step:8398 [D loss: 0.307692, acc.: 87.50%] [G loss: 2.838446]\n",
      "epoch:10 step:8399 [D loss: 0.291507, acc.: 86.72%] [G loss: 3.012928]\n",
      "epoch:10 step:8400 [D loss: 0.313035, acc.: 87.50%] [G loss: 2.372014]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##############\n",
      "[0.84718321 0.86262744 0.79431337 0.79912731 0.79138365 0.81960429\n",
      " 0.89510718 0.83268441 0.80718563 0.82028083]\n",
      "##########\n",
      "epoch:10 step:8401 [D loss: 0.443070, acc.: 73.44%] [G loss: 2.682000]\n",
      "epoch:10 step:8402 [D loss: 0.219763, acc.: 89.84%] [G loss: 5.317612]\n",
      "epoch:10 step:8403 [D loss: 0.299607, acc.: 89.84%] [G loss: 3.089130]\n",
      "epoch:10 step:8404 [D loss: 0.387272, acc.: 78.91%] [G loss: 2.072956]\n",
      "epoch:10 step:8405 [D loss: 0.275917, acc.: 88.28%] [G loss: 4.552094]\n",
      "epoch:10 step:8406 [D loss: 0.354586, acc.: 86.72%] [G loss: 5.720577]\n",
      "epoch:10 step:8407 [D loss: 0.225298, acc.: 88.28%] [G loss: 3.955945]\n",
      "epoch:10 step:8408 [D loss: 0.268114, acc.: 89.06%] [G loss: 3.008095]\n",
      "epoch:10 step:8409 [D loss: 0.347353, acc.: 89.06%] [G loss: 2.911185]\n",
      "epoch:10 step:8410 [D loss: 0.319705, acc.: 89.84%] [G loss: 2.839666]\n",
      "epoch:10 step:8411 [D loss: 0.181950, acc.: 94.53%] [G loss: 5.078975]\n",
      "epoch:10 step:8412 [D loss: 0.228136, acc.: 91.41%] [G loss: 5.520240]\n",
      "epoch:10 step:8413 [D loss: 0.215514, acc.: 94.53%] [G loss: 5.006696]\n",
      "epoch:10 step:8414 [D loss: 0.282812, acc.: 88.28%] [G loss: 2.606141]\n",
      "epoch:10 step:8415 [D loss: 0.384797, acc.: 81.25%] [G loss: 1.926530]\n",
      "epoch:10 step:8416 [D loss: 0.361974, acc.: 82.03%] [G loss: 3.096782]\n",
      "epoch:10 step:8417 [D loss: 0.254921, acc.: 94.53%] [G loss: 3.513205]\n",
      "epoch:10 step:8418 [D loss: 0.276365, acc.: 87.50%] [G loss: 4.289880]\n",
      "epoch:10 step:8419 [D loss: 0.470239, acc.: 77.34%] [G loss: 3.893368]\n",
      "epoch:10 step:8420 [D loss: 0.604483, acc.: 72.66%] [G loss: 4.510955]\n",
      "epoch:10 step:8421 [D loss: 0.617391, acc.: 77.34%] [G loss: 4.793827]\n",
      "epoch:10 step:8422 [D loss: 0.396245, acc.: 84.38%] [G loss: 5.880097]\n",
      "epoch:10 step:8423 [D loss: 0.566881, acc.: 73.44%] [G loss: 3.184843]\n",
      "epoch:10 step:8424 [D loss: 0.367491, acc.: 83.59%] [G loss: 2.688107]\n",
      "epoch:10 step:8425 [D loss: 0.231639, acc.: 88.28%] [G loss: 2.761055]\n",
      "epoch:10 step:8426 [D loss: 0.295366, acc.: 89.06%] [G loss: 3.171924]\n",
      "epoch:10 step:8427 [D loss: 0.295177, acc.: 86.72%] [G loss: 2.720285]\n",
      "epoch:10 step:8428 [D loss: 0.342646, acc.: 87.50%] [G loss: 2.373465]\n",
      "epoch:10 step:8429 [D loss: 0.289619, acc.: 86.72%] [G loss: 3.095277]\n",
      "epoch:10 step:8430 [D loss: 0.334054, acc.: 86.72%] [G loss: 2.479716]\n",
      "epoch:10 step:8431 [D loss: 0.278084, acc.: 88.28%] [G loss: 3.737768]\n",
      "epoch:10 step:8432 [D loss: 0.283104, acc.: 88.28%] [G loss: 3.240319]\n",
      "epoch:10 step:8433 [D loss: 0.234668, acc.: 89.84%] [G loss: 3.587106]\n",
      "epoch:10 step:8434 [D loss: 0.347975, acc.: 87.50%] [G loss: 3.231608]\n",
      "epoch:10 step:8435 [D loss: 0.307613, acc.: 86.72%] [G loss: 3.898624]\n",
      "epoch:10 step:8436 [D loss: 0.273373, acc.: 87.50%] [G loss: 3.294663]\n",
      "epoch:10 step:8437 [D loss: 0.377261, acc.: 85.94%] [G loss: 2.696748]\n",
      "epoch:10 step:8438 [D loss: 0.320360, acc.: 85.16%] [G loss: 3.455566]\n",
      "epoch:10 step:8439 [D loss: 0.282558, acc.: 87.50%] [G loss: 4.768325]\n",
      "epoch:10 step:8440 [D loss: 0.281572, acc.: 85.94%] [G loss: 3.494713]\n",
      "epoch:10 step:8441 [D loss: 0.374967, acc.: 85.16%] [G loss: 2.877330]\n",
      "epoch:10 step:8442 [D loss: 0.318970, acc.: 88.28%] [G loss: 2.536210]\n",
      "epoch:10 step:8443 [D loss: 0.291132, acc.: 86.72%] [G loss: 2.816754]\n",
      "epoch:10 step:8444 [D loss: 0.352308, acc.: 87.50%] [G loss: 2.543618]\n",
      "epoch:10 step:8445 [D loss: 0.394853, acc.: 82.81%] [G loss: 2.252390]\n",
      "epoch:10 step:8446 [D loss: 0.305979, acc.: 90.62%] [G loss: 3.152514]\n",
      "epoch:10 step:8447 [D loss: 0.315361, acc.: 86.72%] [G loss: 3.270729]\n",
      "epoch:10 step:8448 [D loss: 0.328243, acc.: 85.16%] [G loss: 3.328027]\n",
      "epoch:10 step:8449 [D loss: 0.316680, acc.: 85.16%] [G loss: 3.677524]\n",
      "epoch:10 step:8450 [D loss: 0.373847, acc.: 85.16%] [G loss: 6.896201]\n",
      "epoch:10 step:8451 [D loss: 0.397863, acc.: 82.03%] [G loss: 2.850278]\n",
      "epoch:10 step:8452 [D loss: 0.446379, acc.: 77.34%] [G loss: 2.530823]\n",
      "epoch:10 step:8453 [D loss: 0.310470, acc.: 90.62%] [G loss: 2.497170]\n",
      "epoch:10 step:8454 [D loss: 0.259931, acc.: 86.72%] [G loss: 2.674268]\n",
      "epoch:10 step:8455 [D loss: 0.319373, acc.: 86.72%] [G loss: 2.494941]\n",
      "epoch:10 step:8456 [D loss: 0.323688, acc.: 87.50%] [G loss: 2.992593]\n",
      "epoch:10 step:8457 [D loss: 0.196584, acc.: 91.41%] [G loss: 5.107412]\n",
      "epoch:10 step:8458 [D loss: 0.261070, acc.: 85.94%] [G loss: 3.026226]\n",
      "epoch:10 step:8459 [D loss: 0.334017, acc.: 86.72%] [G loss: 2.559927]\n",
      "epoch:10 step:8460 [D loss: 0.341047, acc.: 85.16%] [G loss: 4.241983]\n",
      "epoch:10 step:8461 [D loss: 0.296210, acc.: 86.72%] [G loss: 4.187206]\n",
      "epoch:10 step:8462 [D loss: 0.291735, acc.: 87.50%] [G loss: 3.978420]\n",
      "epoch:10 step:8463 [D loss: 0.289630, acc.: 89.84%] [G loss: 3.809815]\n",
      "epoch:10 step:8464 [D loss: 0.338376, acc.: 88.28%] [G loss: 2.689891]\n",
      "epoch:10 step:8465 [D loss: 0.383800, acc.: 78.12%] [G loss: 3.223737]\n",
      "epoch:10 step:8466 [D loss: 0.222847, acc.: 89.84%] [G loss: 6.113802]\n",
      "epoch:10 step:8467 [D loss: 0.448398, acc.: 76.56%] [G loss: 2.297382]\n",
      "epoch:10 step:8468 [D loss: 0.340956, acc.: 86.72%] [G loss: 2.944826]\n",
      "epoch:10 step:8469 [D loss: 0.249528, acc.: 91.41%] [G loss: 2.187684]\n",
      "epoch:10 step:8470 [D loss: 0.351116, acc.: 84.38%] [G loss: 2.841253]\n",
      "epoch:10 step:8471 [D loss: 0.434216, acc.: 82.81%] [G loss: 2.250811]\n",
      "epoch:10 step:8472 [D loss: 0.417371, acc.: 80.47%] [G loss: 2.133074]\n",
      "epoch:10 step:8473 [D loss: 0.279894, acc.: 89.84%] [G loss: 2.177104]\n",
      "epoch:10 step:8474 [D loss: 0.286074, acc.: 88.28%] [G loss: 2.624790]\n",
      "epoch:10 step:8475 [D loss: 0.390609, acc.: 85.16%] [G loss: 2.799366]\n",
      "epoch:10 step:8476 [D loss: 0.305401, acc.: 87.50%] [G loss: 3.685156]\n",
      "epoch:10 step:8477 [D loss: 0.178698, acc.: 92.19%] [G loss: 4.528378]\n",
      "epoch:10 step:8478 [D loss: 0.295249, acc.: 85.94%] [G loss: 2.923613]\n",
      "epoch:10 step:8479 [D loss: 0.382698, acc.: 80.47%] [G loss: 2.120595]\n",
      "epoch:10 step:8480 [D loss: 0.339071, acc.: 85.94%] [G loss: 2.810464]\n",
      "epoch:10 step:8481 [D loss: 0.286948, acc.: 87.50%] [G loss: 2.709401]\n",
      "epoch:10 step:8482 [D loss: 0.275169, acc.: 89.84%] [G loss: 4.320218]\n",
      "epoch:10 step:8483 [D loss: 0.251536, acc.: 85.94%] [G loss: 4.342658]\n",
      "epoch:10 step:8484 [D loss: 0.342774, acc.: 86.72%] [G loss: 3.680777]\n",
      "epoch:10 step:8485 [D loss: 0.443999, acc.: 82.81%] [G loss: 2.648984]\n",
      "epoch:10 step:8486 [D loss: 0.393613, acc.: 79.69%] [G loss: 3.319677]\n",
      "epoch:10 step:8487 [D loss: 0.341610, acc.: 86.72%] [G loss: 3.461466]\n",
      "epoch:10 step:8488 [D loss: 0.356165, acc.: 82.03%] [G loss: 4.894335]\n",
      "epoch:10 step:8489 [D loss: 0.334099, acc.: 85.16%] [G loss: 3.330810]\n",
      "epoch:10 step:8490 [D loss: 0.338002, acc.: 86.72%] [G loss: 2.980063]\n",
      "epoch:10 step:8491 [D loss: 0.263064, acc.: 90.62%] [G loss: 3.684690]\n",
      "epoch:10 step:8492 [D loss: 0.315729, acc.: 88.28%] [G loss: 3.991657]\n",
      "epoch:10 step:8493 [D loss: 0.435446, acc.: 80.47%] [G loss: 2.901440]\n",
      "epoch:10 step:8494 [D loss: 0.477555, acc.: 78.91%] [G loss: 2.739056]\n",
      "epoch:10 step:8495 [D loss: 0.482900, acc.: 78.12%] [G loss: 2.409909]\n",
      "epoch:10 step:8496 [D loss: 0.322597, acc.: 89.06%] [G loss: 2.586617]\n",
      "epoch:10 step:8497 [D loss: 0.304709, acc.: 89.06%] [G loss: 2.508762]\n",
      "epoch:10 step:8498 [D loss: 0.293951, acc.: 92.19%] [G loss: 4.033709]\n",
      "epoch:10 step:8499 [D loss: 0.205990, acc.: 89.84%] [G loss: 5.303048]\n",
      "epoch:10 step:8500 [D loss: 0.432034, acc.: 84.38%] [G loss: 3.256080]\n",
      "epoch:10 step:8501 [D loss: 0.290683, acc.: 90.62%] [G loss: 3.735240]\n",
      "epoch:10 step:8502 [D loss: 0.352929, acc.: 80.47%] [G loss: 3.711717]\n",
      "epoch:10 step:8503 [D loss: 0.325019, acc.: 89.06%] [G loss: 2.373400]\n",
      "epoch:10 step:8504 [D loss: 0.248935, acc.: 89.84%] [G loss: 3.611414]\n",
      "epoch:10 step:8505 [D loss: 0.216958, acc.: 89.06%] [G loss: 4.690171]\n",
      "epoch:10 step:8506 [D loss: 0.362069, acc.: 83.59%] [G loss: 2.533160]\n",
      "epoch:10 step:8507 [D loss: 0.354736, acc.: 83.59%] [G loss: 3.517395]\n",
      "epoch:10 step:8508 [D loss: 0.190003, acc.: 94.53%] [G loss: 3.941920]\n",
      "epoch:10 step:8509 [D loss: 0.291529, acc.: 87.50%] [G loss: 2.727544]\n",
      "epoch:10 step:8510 [D loss: 0.355091, acc.: 84.38%] [G loss: 3.600855]\n",
      "epoch:10 step:8511 [D loss: 0.452833, acc.: 79.69%] [G loss: 3.330065]\n",
      "epoch:10 step:8512 [D loss: 0.272289, acc.: 90.62%] [G loss: 2.796898]\n",
      "epoch:10 step:8513 [D loss: 0.242995, acc.: 89.06%] [G loss: 4.551167]\n",
      "epoch:10 step:8514 [D loss: 0.247972, acc.: 93.75%] [G loss: 3.844049]\n",
      "epoch:10 step:8515 [D loss: 0.311795, acc.: 90.62%] [G loss: 3.324737]\n",
      "epoch:10 step:8516 [D loss: 0.284273, acc.: 87.50%] [G loss: 5.204068]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:10 step:8517 [D loss: 0.414899, acc.: 82.81%] [G loss: 4.140627]\n",
      "epoch:10 step:8518 [D loss: 0.577991, acc.: 73.44%] [G loss: 3.826245]\n",
      "epoch:10 step:8519 [D loss: 0.292696, acc.: 86.72%] [G loss: 4.151233]\n",
      "epoch:10 step:8520 [D loss: 0.356236, acc.: 83.59%] [G loss: 4.241715]\n",
      "epoch:10 step:8521 [D loss: 0.313836, acc.: 83.59%] [G loss: 5.552458]\n",
      "epoch:10 step:8522 [D loss: 0.327928, acc.: 83.59%] [G loss: 3.929255]\n",
      "epoch:10 step:8523 [D loss: 0.372127, acc.: 80.47%] [G loss: 3.546016]\n",
      "epoch:10 step:8524 [D loss: 0.365969, acc.: 85.16%] [G loss: 3.141142]\n",
      "epoch:10 step:8525 [D loss: 0.322654, acc.: 87.50%] [G loss: 5.087088]\n",
      "epoch:10 step:8526 [D loss: 0.288740, acc.: 89.06%] [G loss: 2.668581]\n",
      "epoch:10 step:8527 [D loss: 0.292838, acc.: 90.62%] [G loss: 4.842163]\n",
      "epoch:10 step:8528 [D loss: 0.234148, acc.: 89.06%] [G loss: 4.058053]\n",
      "epoch:10 step:8529 [D loss: 0.245591, acc.: 89.06%] [G loss: 4.775784]\n",
      "epoch:10 step:8530 [D loss: 0.359926, acc.: 83.59%] [G loss: 3.575000]\n",
      "epoch:10 step:8531 [D loss: 0.349914, acc.: 81.25%] [G loss: 2.739074]\n",
      "epoch:10 step:8532 [D loss: 0.245279, acc.: 91.41%] [G loss: 3.562485]\n",
      "epoch:10 step:8533 [D loss: 0.290466, acc.: 86.72%] [G loss: 4.389636]\n",
      "epoch:10 step:8534 [D loss: 0.272135, acc.: 91.41%] [G loss: 3.386462]\n",
      "epoch:10 step:8535 [D loss: 0.284363, acc.: 89.06%] [G loss: 2.876304]\n",
      "epoch:10 step:8536 [D loss: 0.293346, acc.: 87.50%] [G loss: 3.985532]\n",
      "epoch:10 step:8537 [D loss: 0.275337, acc.: 89.06%] [G loss: 4.692264]\n",
      "epoch:10 step:8538 [D loss: 0.393786, acc.: 82.81%] [G loss: 3.652665]\n",
      "epoch:10 step:8539 [D loss: 0.265384, acc.: 89.06%] [G loss: 3.065973]\n",
      "epoch:10 step:8540 [D loss: 0.388711, acc.: 82.81%] [G loss: 3.697719]\n",
      "epoch:10 step:8541 [D loss: 0.227992, acc.: 91.41%] [G loss: 3.778500]\n",
      "epoch:10 step:8542 [D loss: 0.206310, acc.: 91.41%] [G loss: 5.110035]\n",
      "epoch:10 step:8543 [D loss: 0.223886, acc.: 92.19%] [G loss: 4.038439]\n",
      "epoch:10 step:8544 [D loss: 0.230298, acc.: 92.97%] [G loss: 4.881657]\n",
      "epoch:10 step:8545 [D loss: 0.353243, acc.: 83.59%] [G loss: 3.526703]\n",
      "epoch:10 step:8546 [D loss: 0.338058, acc.: 86.72%] [G loss: 3.224724]\n",
      "epoch:10 step:8547 [D loss: 0.292340, acc.: 86.72%] [G loss: 4.447536]\n",
      "epoch:10 step:8548 [D loss: 0.392125, acc.: 86.72%] [G loss: 2.744006]\n",
      "epoch:10 step:8549 [D loss: 0.417158, acc.: 76.56%] [G loss: 2.387031]\n",
      "epoch:10 step:8550 [D loss: 0.323138, acc.: 85.16%] [G loss: 3.251403]\n",
      "epoch:10 step:8551 [D loss: 0.275245, acc.: 91.41%] [G loss: 2.312107]\n",
      "epoch:10 step:8552 [D loss: 0.333822, acc.: 86.72%] [G loss: 3.167732]\n",
      "epoch:10 step:8553 [D loss: 0.279478, acc.: 87.50%] [G loss: 2.774293]\n",
      "epoch:10 step:8554 [D loss: 0.214295, acc.: 94.53%] [G loss: 3.176230]\n",
      "epoch:10 step:8555 [D loss: 0.496420, acc.: 75.00%] [G loss: 2.225024]\n",
      "epoch:10 step:8556 [D loss: 0.261981, acc.: 86.72%] [G loss: 3.020356]\n",
      "epoch:10 step:8557 [D loss: 0.294008, acc.: 88.28%] [G loss: 3.432823]\n",
      "epoch:10 step:8558 [D loss: 0.271845, acc.: 86.72%] [G loss: 7.093298]\n",
      "epoch:10 step:8559 [D loss: 0.321375, acc.: 85.16%] [G loss: 3.151821]\n",
      "epoch:10 step:8560 [D loss: 0.446479, acc.: 83.59%] [G loss: 3.692722]\n",
      "epoch:10 step:8561 [D loss: 0.312450, acc.: 89.06%] [G loss: 3.575234]\n",
      "epoch:10 step:8562 [D loss: 0.337105, acc.: 84.38%] [G loss: 3.144481]\n",
      "epoch:10 step:8563 [D loss: 0.581614, acc.: 77.34%] [G loss: 3.631705]\n",
      "epoch:10 step:8564 [D loss: 0.417035, acc.: 80.47%] [G loss: 2.405560]\n",
      "epoch:10 step:8565 [D loss: 0.336932, acc.: 85.94%] [G loss: 4.137441]\n",
      "epoch:10 step:8566 [D loss: 0.302175, acc.: 86.72%] [G loss: 3.781508]\n",
      "epoch:10 step:8567 [D loss: 0.272186, acc.: 89.06%] [G loss: 3.100389]\n",
      "epoch:10 step:8568 [D loss: 0.414804, acc.: 75.78%] [G loss: 2.454417]\n",
      "epoch:10 step:8569 [D loss: 0.254914, acc.: 87.50%] [G loss: 3.131319]\n",
      "epoch:10 step:8570 [D loss: 0.373879, acc.: 88.28%] [G loss: 3.987300]\n",
      "epoch:10 step:8571 [D loss: 0.212002, acc.: 88.28%] [G loss: 3.566146]\n",
      "epoch:10 step:8572 [D loss: 0.383910, acc.: 82.81%] [G loss: 4.229932]\n",
      "epoch:10 step:8573 [D loss: 0.281714, acc.: 89.84%] [G loss: 5.453870]\n",
      "epoch:10 step:8574 [D loss: 0.219634, acc.: 90.62%] [G loss: 4.379628]\n",
      "epoch:10 step:8575 [D loss: 0.342727, acc.: 81.25%] [G loss: 3.853637]\n",
      "epoch:10 step:8576 [D loss: 0.244000, acc.: 91.41%] [G loss: 3.394301]\n",
      "epoch:10 step:8577 [D loss: 0.381317, acc.: 78.91%] [G loss: 3.286842]\n",
      "epoch:10 step:8578 [D loss: 0.208789, acc.: 90.62%] [G loss: 5.346675]\n",
      "epoch:10 step:8579 [D loss: 0.256868, acc.: 90.62%] [G loss: 3.154766]\n",
      "epoch:10 step:8580 [D loss: 0.244611, acc.: 89.84%] [G loss: 4.668974]\n",
      "epoch:10 step:8581 [D loss: 0.181412, acc.: 92.19%] [G loss: 5.900867]\n",
      "epoch:10 step:8582 [D loss: 0.291790, acc.: 86.72%] [G loss: 2.936705]\n",
      "epoch:10 step:8583 [D loss: 0.320395, acc.: 84.38%] [G loss: 6.829255]\n",
      "epoch:10 step:8584 [D loss: 0.592369, acc.: 76.56%] [G loss: 3.986313]\n",
      "epoch:10 step:8585 [D loss: 1.402742, acc.: 63.28%] [G loss: 8.527623]\n",
      "epoch:10 step:8586 [D loss: 0.820335, acc.: 72.66%] [G loss: 2.368273]\n",
      "epoch:10 step:8587 [D loss: 0.571929, acc.: 77.34%] [G loss: 4.572931]\n",
      "epoch:10 step:8588 [D loss: 0.603110, acc.: 74.22%] [G loss: 2.434075]\n",
      "epoch:10 step:8589 [D loss: 0.402383, acc.: 85.16%] [G loss: 3.033878]\n",
      "epoch:10 step:8590 [D loss: 0.386419, acc.: 83.59%] [G loss: 4.026610]\n",
      "epoch:10 step:8591 [D loss: 0.427675, acc.: 80.47%] [G loss: 4.640782]\n",
      "epoch:11 step:8592 [D loss: 0.287929, acc.: 92.19%] [G loss: 3.163877]\n",
      "epoch:11 step:8593 [D loss: 0.339426, acc.: 85.16%] [G loss: 3.798998]\n",
      "epoch:11 step:8594 [D loss: 0.278126, acc.: 89.06%] [G loss: 3.758463]\n",
      "epoch:11 step:8595 [D loss: 0.281937, acc.: 92.19%] [G loss: 2.808995]\n",
      "epoch:11 step:8596 [D loss: 0.374621, acc.: 85.94%] [G loss: 2.009977]\n",
      "epoch:11 step:8597 [D loss: 0.325697, acc.: 85.16%] [G loss: 2.720897]\n",
      "epoch:11 step:8598 [D loss: 0.292717, acc.: 89.06%] [G loss: 3.109230]\n",
      "epoch:11 step:8599 [D loss: 0.355651, acc.: 85.94%] [G loss: 3.038278]\n",
      "epoch:11 step:8600 [D loss: 0.322446, acc.: 87.50%] [G loss: 2.858266]\n",
      "##############\n",
      "[0.84662213 0.88544182 0.80249014 0.79537428 0.75341652 0.80316115\n",
      " 0.85968622 0.81723699 0.80123536 0.81254732]\n",
      "##########\n",
      "epoch:11 step:8601 [D loss: 0.370048, acc.: 82.03%] [G loss: 2.866122]\n",
      "epoch:11 step:8602 [D loss: 0.289746, acc.: 89.84%] [G loss: 2.486875]\n",
      "epoch:11 step:8603 [D loss: 0.307812, acc.: 87.50%] [G loss: 2.667613]\n",
      "epoch:11 step:8604 [D loss: 0.295222, acc.: 87.50%] [G loss: 2.675210]\n",
      "epoch:11 step:8605 [D loss: 0.336276, acc.: 85.16%] [G loss: 3.233007]\n",
      "epoch:11 step:8606 [D loss: 0.291935, acc.: 89.06%] [G loss: 2.465988]\n",
      "epoch:11 step:8607 [D loss: 0.271721, acc.: 89.84%] [G loss: 3.125359]\n",
      "epoch:11 step:8608 [D loss: 0.297728, acc.: 85.94%] [G loss: 3.191595]\n",
      "epoch:11 step:8609 [D loss: 0.288595, acc.: 86.72%] [G loss: 2.904528]\n",
      "epoch:11 step:8610 [D loss: 0.239448, acc.: 92.19%] [G loss: 3.199562]\n",
      "epoch:11 step:8611 [D loss: 0.313508, acc.: 89.84%] [G loss: 2.728303]\n",
      "epoch:11 step:8612 [D loss: 0.259020, acc.: 86.72%] [G loss: 2.978965]\n",
      "epoch:11 step:8613 [D loss: 0.320761, acc.: 85.94%] [G loss: 2.512277]\n",
      "epoch:11 step:8614 [D loss: 0.277807, acc.: 89.84%] [G loss: 2.408371]\n",
      "epoch:11 step:8615 [D loss: 0.349615, acc.: 85.16%] [G loss: 2.657013]\n",
      "epoch:11 step:8616 [D loss: 0.235830, acc.: 93.75%] [G loss: 2.849274]\n",
      "epoch:11 step:8617 [D loss: 0.255677, acc.: 92.97%] [G loss: 2.985435]\n",
      "epoch:11 step:8618 [D loss: 0.358238, acc.: 85.94%] [G loss: 2.712833]\n",
      "epoch:11 step:8619 [D loss: 0.329145, acc.: 88.28%] [G loss: 3.387541]\n",
      "epoch:11 step:8620 [D loss: 0.233041, acc.: 90.62%] [G loss: 4.273077]\n",
      "epoch:11 step:8621 [D loss: 0.282293, acc.: 90.62%] [G loss: 2.347740]\n",
      "epoch:11 step:8622 [D loss: 0.371051, acc.: 81.25%] [G loss: 3.210295]\n",
      "epoch:11 step:8623 [D loss: 0.254021, acc.: 90.62%] [G loss: 4.358142]\n",
      "epoch:11 step:8624 [D loss: 0.240921, acc.: 90.62%] [G loss: 4.494615]\n",
      "epoch:11 step:8625 [D loss: 0.276923, acc.: 87.50%] [G loss: 3.443998]\n",
      "epoch:11 step:8626 [D loss: 0.355681, acc.: 84.38%] [G loss: 2.783317]\n",
      "epoch:11 step:8627 [D loss: 0.221545, acc.: 94.53%] [G loss: 3.238707]\n",
      "epoch:11 step:8628 [D loss: 0.345783, acc.: 84.38%] [G loss: 3.539544]\n",
      "epoch:11 step:8629 [D loss: 0.269414, acc.: 89.06%] [G loss: 3.557923]\n",
      "epoch:11 step:8630 [D loss: 0.293663, acc.: 83.59%] [G loss: 3.479900]\n",
      "epoch:11 step:8631 [D loss: 0.263076, acc.: 87.50%] [G loss: 2.705862]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:11 step:8632 [D loss: 0.328400, acc.: 83.59%] [G loss: 2.334526]\n",
      "epoch:11 step:8633 [D loss: 0.334987, acc.: 85.94%] [G loss: 2.943169]\n",
      "epoch:11 step:8634 [D loss: 0.384105, acc.: 85.16%] [G loss: 3.087384]\n",
      "epoch:11 step:8635 [D loss: 0.331630, acc.: 83.59%] [G loss: 2.327299]\n",
      "epoch:11 step:8636 [D loss: 0.326829, acc.: 84.38%] [G loss: 2.352054]\n",
      "epoch:11 step:8637 [D loss: 0.322730, acc.: 86.72%] [G loss: 2.897988]\n",
      "epoch:11 step:8638 [D loss: 0.412417, acc.: 80.47%] [G loss: 3.237636]\n",
      "epoch:11 step:8639 [D loss: 0.434817, acc.: 78.91%] [G loss: 2.874662]\n",
      "epoch:11 step:8640 [D loss: 0.346229, acc.: 88.28%] [G loss: 2.253017]\n",
      "epoch:11 step:8641 [D loss: 0.264101, acc.: 91.41%] [G loss: 3.941633]\n",
      "epoch:11 step:8642 [D loss: 0.258451, acc.: 89.06%] [G loss: 2.822194]\n",
      "epoch:11 step:8643 [D loss: 0.322219, acc.: 82.03%] [G loss: 2.979038]\n",
      "epoch:11 step:8644 [D loss: 0.496735, acc.: 78.12%] [G loss: 3.525269]\n",
      "epoch:11 step:8645 [D loss: 0.411142, acc.: 81.25%] [G loss: 2.969222]\n",
      "epoch:11 step:8646 [D loss: 0.263095, acc.: 91.41%] [G loss: 3.196699]\n",
      "epoch:11 step:8647 [D loss: 0.454745, acc.: 80.47%] [G loss: 4.255055]\n",
      "epoch:11 step:8648 [D loss: 0.397495, acc.: 83.59%] [G loss: 3.116293]\n",
      "epoch:11 step:8649 [D loss: 0.359547, acc.: 83.59%] [G loss: 2.884438]\n",
      "epoch:11 step:8650 [D loss: 0.392095, acc.: 83.59%] [G loss: 3.508083]\n",
      "epoch:11 step:8651 [D loss: 0.451465, acc.: 77.34%] [G loss: 2.943978]\n",
      "epoch:11 step:8652 [D loss: 0.299946, acc.: 86.72%] [G loss: 3.378890]\n",
      "epoch:11 step:8653 [D loss: 0.313040, acc.: 87.50%] [G loss: 2.753176]\n",
      "epoch:11 step:8654 [D loss: 0.292607, acc.: 92.19%] [G loss: 2.338248]\n",
      "epoch:11 step:8655 [D loss: 0.350957, acc.: 85.94%] [G loss: 2.804982]\n",
      "epoch:11 step:8656 [D loss: 0.463636, acc.: 79.69%] [G loss: 2.462317]\n",
      "epoch:11 step:8657 [D loss: 0.337011, acc.: 87.50%] [G loss: 2.596001]\n",
      "epoch:11 step:8658 [D loss: 0.235167, acc.: 90.62%] [G loss: 3.936908]\n",
      "epoch:11 step:8659 [D loss: 0.274695, acc.: 87.50%] [G loss: 3.320325]\n",
      "epoch:11 step:8660 [D loss: 0.261753, acc.: 90.62%] [G loss: 3.343942]\n",
      "epoch:11 step:8661 [D loss: 0.387369, acc.: 81.25%] [G loss: 2.630237]\n",
      "epoch:11 step:8662 [D loss: 0.427276, acc.: 85.94%] [G loss: 3.220364]\n",
      "epoch:11 step:8663 [D loss: 0.359672, acc.: 85.94%] [G loss: 3.833057]\n",
      "epoch:11 step:8664 [D loss: 0.284149, acc.: 88.28%] [G loss: 3.076451]\n",
      "epoch:11 step:8665 [D loss: 0.259207, acc.: 92.19%] [G loss: 2.732543]\n",
      "epoch:11 step:8666 [D loss: 0.452982, acc.: 80.47%] [G loss: 2.818996]\n",
      "epoch:11 step:8667 [D loss: 0.446493, acc.: 84.38%] [G loss: 2.727018]\n",
      "epoch:11 step:8668 [D loss: 0.226437, acc.: 89.84%] [G loss: 4.165679]\n",
      "epoch:11 step:8669 [D loss: 0.412801, acc.: 79.69%] [G loss: 2.860917]\n",
      "epoch:11 step:8670 [D loss: 0.335328, acc.: 85.16%] [G loss: 2.978649]\n",
      "epoch:11 step:8671 [D loss: 0.308688, acc.: 89.06%] [G loss: 2.395164]\n",
      "epoch:11 step:8672 [D loss: 0.349490, acc.: 86.72%] [G loss: 3.330312]\n",
      "epoch:11 step:8673 [D loss: 0.326223, acc.: 85.16%] [G loss: 3.720965]\n",
      "epoch:11 step:8674 [D loss: 0.387967, acc.: 84.38%] [G loss: 2.348730]\n",
      "epoch:11 step:8675 [D loss: 0.264560, acc.: 91.41%] [G loss: 3.077055]\n",
      "epoch:11 step:8676 [D loss: 0.185406, acc.: 93.75%] [G loss: 6.269044]\n",
      "epoch:11 step:8677 [D loss: 0.380346, acc.: 81.25%] [G loss: 3.469058]\n",
      "epoch:11 step:8678 [D loss: 0.181805, acc.: 92.97%] [G loss: 7.936116]\n",
      "epoch:11 step:8679 [D loss: 0.222413, acc.: 89.84%] [G loss: 5.465174]\n",
      "epoch:11 step:8680 [D loss: 0.190196, acc.: 93.75%] [G loss: 8.799747]\n",
      "epoch:11 step:8681 [D loss: 0.199396, acc.: 91.41%] [G loss: 7.294310]\n",
      "epoch:11 step:8682 [D loss: 0.253721, acc.: 91.41%] [G loss: 5.029914]\n",
      "epoch:11 step:8683 [D loss: 0.235795, acc.: 89.84%] [G loss: 5.601201]\n",
      "epoch:11 step:8684 [D loss: 0.414913, acc.: 78.12%] [G loss: 3.207115]\n",
      "epoch:11 step:8685 [D loss: 0.266149, acc.: 88.28%] [G loss: 3.489383]\n",
      "epoch:11 step:8686 [D loss: 0.284544, acc.: 89.06%] [G loss: 2.745324]\n",
      "epoch:11 step:8687 [D loss: 0.421874, acc.: 85.94%] [G loss: 3.926735]\n",
      "epoch:11 step:8688 [D loss: 0.434112, acc.: 81.25%] [G loss: 3.320642]\n",
      "epoch:11 step:8689 [D loss: 0.292501, acc.: 88.28%] [G loss: 4.249636]\n",
      "epoch:11 step:8690 [D loss: 0.387497, acc.: 82.81%] [G loss: 3.559046]\n",
      "epoch:11 step:8691 [D loss: 0.354343, acc.: 85.16%] [G loss: 3.199983]\n",
      "epoch:11 step:8692 [D loss: 0.459396, acc.: 80.47%] [G loss: 6.631467]\n",
      "epoch:11 step:8693 [D loss: 1.738898, acc.: 58.59%] [G loss: 10.027142]\n",
      "epoch:11 step:8694 [D loss: 2.655525, acc.: 49.22%] [G loss: 3.678532]\n",
      "epoch:11 step:8695 [D loss: 0.454234, acc.: 77.34%] [G loss: 4.507946]\n",
      "epoch:11 step:8696 [D loss: 0.898154, acc.: 57.81%] [G loss: 3.685324]\n",
      "epoch:11 step:8697 [D loss: 0.391859, acc.: 85.16%] [G loss: 3.635963]\n",
      "epoch:11 step:8698 [D loss: 0.468002, acc.: 78.91%] [G loss: 2.662052]\n",
      "epoch:11 step:8699 [D loss: 0.452891, acc.: 82.81%] [G loss: 2.918108]\n",
      "epoch:11 step:8700 [D loss: 0.485145, acc.: 75.78%] [G loss: 2.406404]\n",
      "epoch:11 step:8701 [D loss: 0.422961, acc.: 79.69%] [G loss: 2.662429]\n",
      "epoch:11 step:8702 [D loss: 0.316038, acc.: 86.72%] [G loss: 2.752802]\n",
      "epoch:11 step:8703 [D loss: 0.408595, acc.: 81.25%] [G loss: 2.896603]\n",
      "epoch:11 step:8704 [D loss: 0.444046, acc.: 81.25%] [G loss: 3.146403]\n",
      "epoch:11 step:8705 [D loss: 0.236468, acc.: 92.97%] [G loss: 3.399834]\n",
      "epoch:11 step:8706 [D loss: 0.373988, acc.: 85.94%] [G loss: 2.219694]\n",
      "epoch:11 step:8707 [D loss: 0.325873, acc.: 82.81%] [G loss: 2.910554]\n",
      "epoch:11 step:8708 [D loss: 0.221389, acc.: 92.97%] [G loss: 3.248852]\n",
      "epoch:11 step:8709 [D loss: 0.288876, acc.: 85.94%] [G loss: 3.205095]\n",
      "epoch:11 step:8710 [D loss: 0.319283, acc.: 84.38%] [G loss: 3.123150]\n",
      "epoch:11 step:8711 [D loss: 0.382480, acc.: 84.38%] [G loss: 2.353237]\n",
      "epoch:11 step:8712 [D loss: 0.253937, acc.: 91.41%] [G loss: 2.130255]\n",
      "epoch:11 step:8713 [D loss: 0.235547, acc.: 93.75%] [G loss: 2.940416]\n",
      "epoch:11 step:8714 [D loss: 0.376462, acc.: 85.16%] [G loss: 2.280906]\n",
      "epoch:11 step:8715 [D loss: 0.407935, acc.: 83.59%] [G loss: 2.595720]\n",
      "epoch:11 step:8716 [D loss: 0.381157, acc.: 84.38%] [G loss: 2.776700]\n",
      "epoch:11 step:8717 [D loss: 0.282430, acc.: 88.28%] [G loss: 2.359361]\n",
      "epoch:11 step:8718 [D loss: 0.340883, acc.: 87.50%] [G loss: 2.507200]\n",
      "epoch:11 step:8719 [D loss: 0.416243, acc.: 84.38%] [G loss: 2.867769]\n",
      "epoch:11 step:8720 [D loss: 0.417174, acc.: 85.94%] [G loss: 2.879869]\n",
      "epoch:11 step:8721 [D loss: 0.379187, acc.: 85.94%] [G loss: 3.242336]\n",
      "epoch:11 step:8722 [D loss: 0.386620, acc.: 83.59%] [G loss: 2.517918]\n",
      "epoch:11 step:8723 [D loss: 0.502106, acc.: 82.03%] [G loss: 2.834365]\n",
      "epoch:11 step:8724 [D loss: 0.360058, acc.: 87.50%] [G loss: 3.451944]\n",
      "epoch:11 step:8725 [D loss: 0.252817, acc.: 91.41%] [G loss: 2.476105]\n",
      "epoch:11 step:8726 [D loss: 0.395026, acc.: 81.25%] [G loss: 2.944115]\n",
      "epoch:11 step:8727 [D loss: 0.321022, acc.: 85.16%] [G loss: 3.044396]\n",
      "epoch:11 step:8728 [D loss: 0.317369, acc.: 87.50%] [G loss: 2.651887]\n",
      "epoch:11 step:8729 [D loss: 0.379633, acc.: 82.03%] [G loss: 2.139715]\n",
      "epoch:11 step:8730 [D loss: 0.323525, acc.: 87.50%] [G loss: 2.448263]\n",
      "epoch:11 step:8731 [D loss: 0.286413, acc.: 89.06%] [G loss: 3.371538]\n",
      "epoch:11 step:8732 [D loss: 0.433899, acc.: 78.12%] [G loss: 2.670517]\n",
      "epoch:11 step:8733 [D loss: 0.420356, acc.: 82.03%] [G loss: 3.270009]\n",
      "epoch:11 step:8734 [D loss: 0.343596, acc.: 87.50%] [G loss: 2.965348]\n",
      "epoch:11 step:8735 [D loss: 0.286770, acc.: 85.94%] [G loss: 3.762989]\n",
      "epoch:11 step:8736 [D loss: 0.279219, acc.: 87.50%] [G loss: 4.561411]\n",
      "epoch:11 step:8737 [D loss: 0.260563, acc.: 89.84%] [G loss: 3.218441]\n",
      "epoch:11 step:8738 [D loss: 0.327351, acc.: 86.72%] [G loss: 2.113458]\n",
      "epoch:11 step:8739 [D loss: 0.440819, acc.: 78.91%] [G loss: 2.000655]\n",
      "epoch:11 step:8740 [D loss: 0.417982, acc.: 80.47%] [G loss: 2.256655]\n",
      "epoch:11 step:8741 [D loss: 0.322887, acc.: 86.72%] [G loss: 2.638279]\n",
      "epoch:11 step:8742 [D loss: 0.300439, acc.: 87.50%] [G loss: 2.253378]\n",
      "epoch:11 step:8743 [D loss: 0.442102, acc.: 79.69%] [G loss: 2.155326]\n",
      "epoch:11 step:8744 [D loss: 0.346412, acc.: 85.16%] [G loss: 2.770532]\n",
      "epoch:11 step:8745 [D loss: 0.300979, acc.: 86.72%] [G loss: 2.788676]\n",
      "epoch:11 step:8746 [D loss: 0.327082, acc.: 88.28%] [G loss: 2.788719]\n",
      "epoch:11 step:8747 [D loss: 0.311990, acc.: 90.62%] [G loss: 2.549792]\n",
      "epoch:11 step:8748 [D loss: 0.417669, acc.: 81.25%] [G loss: 3.913577]\n",
      "epoch:11 step:8749 [D loss: 0.520204, acc.: 75.78%] [G loss: 2.525581]\n",
      "epoch:11 step:8750 [D loss: 0.286459, acc.: 89.06%] [G loss: 3.075243]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:11 step:8751 [D loss: 0.413124, acc.: 78.91%] [G loss: 2.055510]\n",
      "epoch:11 step:8752 [D loss: 0.242860, acc.: 91.41%] [G loss: 2.875467]\n",
      "epoch:11 step:8753 [D loss: 0.336290, acc.: 86.72%] [G loss: 3.496498]\n",
      "epoch:11 step:8754 [D loss: 0.360014, acc.: 90.62%] [G loss: 3.246727]\n",
      "epoch:11 step:8755 [D loss: 0.162212, acc.: 96.09%] [G loss: 3.370490]\n",
      "epoch:11 step:8756 [D loss: 0.217728, acc.: 92.19%] [G loss: 5.861472]\n",
      "epoch:11 step:8757 [D loss: 0.321934, acc.: 88.28%] [G loss: 3.389545]\n",
      "epoch:11 step:8758 [D loss: 0.396745, acc.: 83.59%] [G loss: 2.628543]\n",
      "epoch:11 step:8759 [D loss: 0.341750, acc.: 88.28%] [G loss: 2.741630]\n",
      "epoch:11 step:8760 [D loss: 0.325328, acc.: 88.28%] [G loss: 2.190820]\n",
      "epoch:11 step:8761 [D loss: 0.315684, acc.: 86.72%] [G loss: 2.579832]\n",
      "epoch:11 step:8762 [D loss: 0.406101, acc.: 82.81%] [G loss: 2.589507]\n",
      "epoch:11 step:8763 [D loss: 0.261906, acc.: 91.41%] [G loss: 2.635002]\n",
      "epoch:11 step:8764 [D loss: 0.362849, acc.: 87.50%] [G loss: 3.366395]\n",
      "epoch:11 step:8765 [D loss: 0.465965, acc.: 82.81%] [G loss: 2.081643]\n",
      "epoch:11 step:8766 [D loss: 0.280125, acc.: 88.28%] [G loss: 2.802000]\n",
      "epoch:11 step:8767 [D loss: 0.406148, acc.: 82.81%] [G loss: 2.411563]\n",
      "epoch:11 step:8768 [D loss: 0.350332, acc.: 85.94%] [G loss: 3.672658]\n",
      "epoch:11 step:8769 [D loss: 0.235690, acc.: 93.75%] [G loss: 3.797842]\n",
      "epoch:11 step:8770 [D loss: 0.298543, acc.: 86.72%] [G loss: 3.373794]\n",
      "epoch:11 step:8771 [D loss: 0.305127, acc.: 85.94%] [G loss: 2.856586]\n",
      "epoch:11 step:8772 [D loss: 0.245844, acc.: 92.19%] [G loss: 3.534076]\n",
      "epoch:11 step:8773 [D loss: 0.282343, acc.: 87.50%] [G loss: 3.605684]\n",
      "epoch:11 step:8774 [D loss: 0.303782, acc.: 88.28%] [G loss: 2.850809]\n",
      "epoch:11 step:8775 [D loss: 0.360664, acc.: 81.25%] [G loss: 3.685898]\n",
      "epoch:11 step:8776 [D loss: 0.308751, acc.: 91.41%] [G loss: 2.915386]\n",
      "epoch:11 step:8777 [D loss: 0.312409, acc.: 86.72%] [G loss: 3.125995]\n",
      "epoch:11 step:8778 [D loss: 0.471320, acc.: 74.22%] [G loss: 2.149172]\n",
      "epoch:11 step:8779 [D loss: 0.427789, acc.: 78.91%] [G loss: 2.044636]\n",
      "epoch:11 step:8780 [D loss: 0.432070, acc.: 80.47%] [G loss: 2.668974]\n",
      "epoch:11 step:8781 [D loss: 0.370000, acc.: 85.94%] [G loss: 3.965087]\n",
      "epoch:11 step:8782 [D loss: 0.465407, acc.: 75.00%] [G loss: 2.650678]\n",
      "epoch:11 step:8783 [D loss: 0.417139, acc.: 84.38%] [G loss: 2.940242]\n",
      "epoch:11 step:8784 [D loss: 0.398020, acc.: 83.59%] [G loss: 4.724645]\n",
      "epoch:11 step:8785 [D loss: 0.361719, acc.: 89.06%] [G loss: 1.939526]\n",
      "epoch:11 step:8786 [D loss: 0.349570, acc.: 83.59%] [G loss: 3.070296]\n",
      "epoch:11 step:8787 [D loss: 0.306412, acc.: 85.94%] [G loss: 2.674310]\n",
      "epoch:11 step:8788 [D loss: 0.271792, acc.: 88.28%] [G loss: 3.302855]\n",
      "epoch:11 step:8789 [D loss: 0.325783, acc.: 85.94%] [G loss: 3.369725]\n",
      "epoch:11 step:8790 [D loss: 0.365506, acc.: 85.16%] [G loss: 3.724082]\n",
      "epoch:11 step:8791 [D loss: 0.300611, acc.: 86.72%] [G loss: 3.305447]\n",
      "epoch:11 step:8792 [D loss: 0.308022, acc.: 87.50%] [G loss: 4.247921]\n",
      "epoch:11 step:8793 [D loss: 0.385104, acc.: 85.16%] [G loss: 4.861941]\n",
      "epoch:11 step:8794 [D loss: 0.347250, acc.: 89.06%] [G loss: 3.497567]\n",
      "epoch:11 step:8795 [D loss: 0.403950, acc.: 85.94%] [G loss: 3.202922]\n",
      "epoch:11 step:8796 [D loss: 0.458840, acc.: 81.25%] [G loss: 2.719024]\n",
      "epoch:11 step:8797 [D loss: 0.367552, acc.: 81.25%] [G loss: 2.725708]\n",
      "epoch:11 step:8798 [D loss: 0.355969, acc.: 82.81%] [G loss: 4.225947]\n",
      "epoch:11 step:8799 [D loss: 0.392564, acc.: 79.69%] [G loss: 4.654162]\n",
      "epoch:11 step:8800 [D loss: 0.375144, acc.: 86.72%] [G loss: 3.385243]\n",
      "##############\n",
      "[0.85720551 0.86392867 0.79959002 0.79460245 0.76043253 0.82061762\n",
      " 0.89209151 0.81828254 0.79856132 0.82296089]\n",
      "##########\n",
      "epoch:11 step:8801 [D loss: 0.340322, acc.: 85.16%] [G loss: 2.452664]\n",
      "epoch:11 step:8802 [D loss: 0.402082, acc.: 78.91%] [G loss: 4.630020]\n",
      "epoch:11 step:8803 [D loss: 0.480371, acc.: 82.81%] [G loss: 3.647063]\n",
      "epoch:11 step:8804 [D loss: 0.290979, acc.: 87.50%] [G loss: 3.305653]\n",
      "epoch:11 step:8805 [D loss: 0.528540, acc.: 71.88%] [G loss: 3.853198]\n",
      "epoch:11 step:8806 [D loss: 0.454207, acc.: 83.59%] [G loss: 2.705752]\n",
      "epoch:11 step:8807 [D loss: 0.323511, acc.: 88.28%] [G loss: 3.332326]\n",
      "epoch:11 step:8808 [D loss: 0.363910, acc.: 86.72%] [G loss: 3.097505]\n",
      "epoch:11 step:8809 [D loss: 0.305493, acc.: 89.06%] [G loss: 2.876485]\n",
      "epoch:11 step:8810 [D loss: 0.324631, acc.: 86.72%] [G loss: 3.792436]\n",
      "epoch:11 step:8811 [D loss: 0.410648, acc.: 82.03%] [G loss: 3.752688]\n",
      "epoch:11 step:8812 [D loss: 0.303152, acc.: 85.94%] [G loss: 3.592192]\n",
      "epoch:11 step:8813 [D loss: 0.323328, acc.: 85.94%] [G loss: 2.698212]\n",
      "epoch:11 step:8814 [D loss: 0.278529, acc.: 87.50%] [G loss: 3.396402]\n",
      "epoch:11 step:8815 [D loss: 0.357468, acc.: 85.16%] [G loss: 3.104703]\n",
      "epoch:11 step:8816 [D loss: 0.209121, acc.: 94.53%] [G loss: 4.418011]\n",
      "epoch:11 step:8817 [D loss: 0.441372, acc.: 80.47%] [G loss: 2.080658]\n",
      "epoch:11 step:8818 [D loss: 0.299398, acc.: 86.72%] [G loss: 2.456075]\n",
      "epoch:11 step:8819 [D loss: 0.249500, acc.: 88.28%] [G loss: 3.760883]\n",
      "epoch:11 step:8820 [D loss: 0.265388, acc.: 88.28%] [G loss: 3.135760]\n",
      "epoch:11 step:8821 [D loss: 0.369556, acc.: 82.81%] [G loss: 3.308060]\n",
      "epoch:11 step:8822 [D loss: 0.321735, acc.: 83.59%] [G loss: 2.720167]\n",
      "epoch:11 step:8823 [D loss: 0.384787, acc.: 80.47%] [G loss: 2.465169]\n",
      "epoch:11 step:8824 [D loss: 0.395709, acc.: 84.38%] [G loss: 2.052995]\n",
      "epoch:11 step:8825 [D loss: 0.388376, acc.: 86.72%] [G loss: 2.876906]\n",
      "epoch:11 step:8826 [D loss: 0.349591, acc.: 85.16%] [G loss: 3.037200]\n",
      "epoch:11 step:8827 [D loss: 0.269965, acc.: 89.84%] [G loss: 3.677495]\n",
      "epoch:11 step:8828 [D loss: 0.286092, acc.: 86.72%] [G loss: 3.865282]\n",
      "epoch:11 step:8829 [D loss: 0.257832, acc.: 87.50%] [G loss: 2.174870]\n",
      "epoch:11 step:8830 [D loss: 0.325638, acc.: 85.16%] [G loss: 2.715268]\n",
      "epoch:11 step:8831 [D loss: 0.296297, acc.: 86.72%] [G loss: 3.157422]\n",
      "epoch:11 step:8832 [D loss: 0.203007, acc.: 90.62%] [G loss: 4.362016]\n",
      "epoch:11 step:8833 [D loss: 0.232598, acc.: 89.84%] [G loss: 3.800362]\n",
      "epoch:11 step:8834 [D loss: 0.267675, acc.: 89.84%] [G loss: 3.651269]\n",
      "epoch:11 step:8835 [D loss: 0.358348, acc.: 84.38%] [G loss: 3.105717]\n",
      "epoch:11 step:8836 [D loss: 0.386239, acc.: 82.81%] [G loss: 3.254451]\n",
      "epoch:11 step:8837 [D loss: 0.332147, acc.: 89.06%] [G loss: 3.087354]\n",
      "epoch:11 step:8838 [D loss: 0.325647, acc.: 85.16%] [G loss: 2.549704]\n",
      "epoch:11 step:8839 [D loss: 0.255979, acc.: 93.75%] [G loss: 2.370184]\n",
      "epoch:11 step:8840 [D loss: 0.326701, acc.: 88.28%] [G loss: 2.491261]\n",
      "epoch:11 step:8841 [D loss: 0.258236, acc.: 90.62%] [G loss: 4.495343]\n",
      "epoch:11 step:8842 [D loss: 0.325033, acc.: 87.50%] [G loss: 3.283811]\n",
      "epoch:11 step:8843 [D loss: 0.254379, acc.: 89.84%] [G loss: 4.384943]\n",
      "epoch:11 step:8844 [D loss: 0.329730, acc.: 84.38%] [G loss: 2.430950]\n",
      "epoch:11 step:8845 [D loss: 0.275069, acc.: 91.41%] [G loss: 4.166504]\n",
      "epoch:11 step:8846 [D loss: 0.354682, acc.: 85.94%] [G loss: 2.489700]\n",
      "epoch:11 step:8847 [D loss: 0.242549, acc.: 91.41%] [G loss: 2.815376]\n",
      "epoch:11 step:8848 [D loss: 0.308540, acc.: 89.06%] [G loss: 2.864061]\n",
      "epoch:11 step:8849 [D loss: 0.373226, acc.: 86.72%] [G loss: 2.453036]\n",
      "epoch:11 step:8850 [D loss: 0.296777, acc.: 86.72%] [G loss: 2.829100]\n",
      "epoch:11 step:8851 [D loss: 0.326418, acc.: 91.41%] [G loss: 3.196445]\n",
      "epoch:11 step:8852 [D loss: 0.325035, acc.: 87.50%] [G loss: 2.883160]\n",
      "epoch:11 step:8853 [D loss: 0.259675, acc.: 90.62%] [G loss: 3.821375]\n",
      "epoch:11 step:8854 [D loss: 0.295650, acc.: 83.59%] [G loss: 3.500081]\n",
      "epoch:11 step:8855 [D loss: 0.247787, acc.: 91.41%] [G loss: 3.444455]\n",
      "epoch:11 step:8856 [D loss: 0.310296, acc.: 88.28%] [G loss: 2.902737]\n",
      "epoch:11 step:8857 [D loss: 0.242562, acc.: 88.28%] [G loss: 3.383569]\n",
      "epoch:11 step:8858 [D loss: 0.315241, acc.: 86.72%] [G loss: 4.464504]\n",
      "epoch:11 step:8859 [D loss: 0.238801, acc.: 90.62%] [G loss: 4.587723]\n",
      "epoch:11 step:8860 [D loss: 0.318158, acc.: 84.38%] [G loss: 4.347702]\n",
      "epoch:11 step:8861 [D loss: 0.351010, acc.: 84.38%] [G loss: 3.881883]\n",
      "epoch:11 step:8862 [D loss: 0.272371, acc.: 89.06%] [G loss: 3.368816]\n",
      "epoch:11 step:8863 [D loss: 0.340132, acc.: 82.81%] [G loss: 2.745396]\n",
      "epoch:11 step:8864 [D loss: 0.437567, acc.: 77.34%] [G loss: 3.130137]\n",
      "epoch:11 step:8865 [D loss: 0.332193, acc.: 89.06%] [G loss: 2.957522]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:11 step:8866 [D loss: 0.501727, acc.: 80.47%] [G loss: 3.281336]\n",
      "epoch:11 step:8867 [D loss: 0.419059, acc.: 82.03%] [G loss: 4.055134]\n",
      "epoch:11 step:8868 [D loss: 0.305952, acc.: 85.94%] [G loss: 3.509298]\n",
      "epoch:11 step:8869 [D loss: 0.324026, acc.: 86.72%] [G loss: 2.870880]\n",
      "epoch:11 step:8870 [D loss: 0.315025, acc.: 85.94%] [G loss: 2.543448]\n",
      "epoch:11 step:8871 [D loss: 0.283362, acc.: 89.06%] [G loss: 2.426887]\n",
      "epoch:11 step:8872 [D loss: 0.372995, acc.: 86.72%] [G loss: 3.073562]\n",
      "epoch:11 step:8873 [D loss: 0.367768, acc.: 83.59%] [G loss: 3.556769]\n",
      "epoch:11 step:8874 [D loss: 0.416869, acc.: 81.25%] [G loss: 2.931132]\n",
      "epoch:11 step:8875 [D loss: 0.274271, acc.: 88.28%] [G loss: 4.123080]\n",
      "epoch:11 step:8876 [D loss: 0.356212, acc.: 82.81%] [G loss: 3.791553]\n",
      "epoch:11 step:8877 [D loss: 0.399367, acc.: 85.94%] [G loss: 3.009234]\n",
      "epoch:11 step:8878 [D loss: 0.305842, acc.: 86.72%] [G loss: 3.467563]\n",
      "epoch:11 step:8879 [D loss: 0.232035, acc.: 90.62%] [G loss: 3.732089]\n",
      "epoch:11 step:8880 [D loss: 0.221363, acc.: 90.62%] [G loss: 4.818110]\n",
      "epoch:11 step:8881 [D loss: 0.406254, acc.: 78.12%] [G loss: 2.517782]\n",
      "epoch:11 step:8882 [D loss: 0.331111, acc.: 84.38%] [G loss: 3.062776]\n",
      "epoch:11 step:8883 [D loss: 0.249693, acc.: 89.84%] [G loss: 4.042963]\n",
      "epoch:11 step:8884 [D loss: 0.283267, acc.: 91.41%] [G loss: 2.835557]\n",
      "epoch:11 step:8885 [D loss: 0.334248, acc.: 85.16%] [G loss: 2.954513]\n",
      "epoch:11 step:8886 [D loss: 0.355693, acc.: 82.81%] [G loss: 2.880597]\n",
      "epoch:11 step:8887 [D loss: 0.285932, acc.: 90.62%] [G loss: 3.860421]\n",
      "epoch:11 step:8888 [D loss: 0.210457, acc.: 91.41%] [G loss: 6.909662]\n",
      "epoch:11 step:8889 [D loss: 0.326602, acc.: 89.84%] [G loss: 2.977781]\n",
      "epoch:11 step:8890 [D loss: 0.232464, acc.: 90.62%] [G loss: 3.787458]\n",
      "epoch:11 step:8891 [D loss: 0.325383, acc.: 86.72%] [G loss: 4.304566]\n",
      "epoch:11 step:8892 [D loss: 0.354198, acc.: 85.94%] [G loss: 2.705650]\n",
      "epoch:11 step:8893 [D loss: 0.361820, acc.: 84.38%] [G loss: 3.050185]\n",
      "epoch:11 step:8894 [D loss: 0.460688, acc.: 84.38%] [G loss: 4.546703]\n",
      "epoch:11 step:8895 [D loss: 0.308312, acc.: 85.16%] [G loss: 4.921466]\n",
      "epoch:11 step:8896 [D loss: 0.294202, acc.: 85.94%] [G loss: 4.207193]\n",
      "epoch:11 step:8897 [D loss: 0.318965, acc.: 85.94%] [G loss: 2.802890]\n",
      "epoch:11 step:8898 [D loss: 0.250782, acc.: 93.75%] [G loss: 3.189269]\n",
      "epoch:11 step:8899 [D loss: 0.218673, acc.: 92.19%] [G loss: 4.431439]\n",
      "epoch:11 step:8900 [D loss: 0.311734, acc.: 89.06%] [G loss: 2.368863]\n",
      "epoch:11 step:8901 [D loss: 0.193969, acc.: 92.97%] [G loss: 2.723263]\n",
      "epoch:11 step:8902 [D loss: 0.322208, acc.: 85.94%] [G loss: 2.613744]\n",
      "epoch:11 step:8903 [D loss: 0.263253, acc.: 87.50%] [G loss: 3.489172]\n",
      "epoch:11 step:8904 [D loss: 0.244899, acc.: 92.97%] [G loss: 2.632866]\n",
      "epoch:11 step:8905 [D loss: 0.343730, acc.: 85.16%] [G loss: 4.170345]\n",
      "epoch:11 step:8906 [D loss: 0.478236, acc.: 80.47%] [G loss: 5.038311]\n",
      "epoch:11 step:8907 [D loss: 0.742504, acc.: 79.69%] [G loss: 8.339592]\n",
      "epoch:11 step:8908 [D loss: 1.695157, acc.: 59.38%] [G loss: 7.964888]\n",
      "epoch:11 step:8909 [D loss: 2.252508, acc.: 53.12%] [G loss: 3.174455]\n",
      "epoch:11 step:8910 [D loss: 0.440043, acc.: 81.25%] [G loss: 3.961991]\n",
      "epoch:11 step:8911 [D loss: 0.296009, acc.: 85.16%] [G loss: 4.177163]\n",
      "epoch:11 step:8912 [D loss: 0.388028, acc.: 82.03%] [G loss: 3.008747]\n",
      "epoch:11 step:8913 [D loss: 0.309177, acc.: 87.50%] [G loss: 3.856705]\n",
      "epoch:11 step:8914 [D loss: 0.302595, acc.: 88.28%] [G loss: 3.105957]\n",
      "epoch:11 step:8915 [D loss: 0.328715, acc.: 88.28%] [G loss: 2.994163]\n",
      "epoch:11 step:8916 [D loss: 0.267884, acc.: 90.62%] [G loss: 3.668870]\n",
      "epoch:11 step:8917 [D loss: 0.337994, acc.: 85.16%] [G loss: 2.721155]\n",
      "epoch:11 step:8918 [D loss: 0.368009, acc.: 82.03%] [G loss: 3.566530]\n",
      "epoch:11 step:8919 [D loss: 0.295379, acc.: 86.72%] [G loss: 2.288572]\n",
      "epoch:11 step:8920 [D loss: 0.266996, acc.: 89.84%] [G loss: 2.825282]\n",
      "epoch:11 step:8921 [D loss: 0.362700, acc.: 82.81%] [G loss: 2.988716]\n",
      "epoch:11 step:8922 [D loss: 0.306810, acc.: 89.84%] [G loss: 2.898938]\n",
      "epoch:11 step:8923 [D loss: 0.443557, acc.: 81.25%] [G loss: 3.346400]\n",
      "epoch:11 step:8924 [D loss: 0.342471, acc.: 84.38%] [G loss: 2.730339]\n",
      "epoch:11 step:8925 [D loss: 0.444446, acc.: 78.12%] [G loss: 2.952451]\n",
      "epoch:11 step:8926 [D loss: 0.279769, acc.: 89.84%] [G loss: 2.769206]\n",
      "epoch:11 step:8927 [D loss: 0.377479, acc.: 82.81%] [G loss: 2.496233]\n",
      "epoch:11 step:8928 [D loss: 0.284905, acc.: 86.72%] [G loss: 2.803038]\n",
      "epoch:11 step:8929 [D loss: 0.293901, acc.: 84.38%] [G loss: 3.978867]\n",
      "epoch:11 step:8930 [D loss: 0.315578, acc.: 90.62%] [G loss: 3.271647]\n",
      "epoch:11 step:8931 [D loss: 0.319188, acc.: 85.94%] [G loss: 3.282453]\n",
      "epoch:11 step:8932 [D loss: 0.307219, acc.: 90.62%] [G loss: 2.541893]\n",
      "epoch:11 step:8933 [D loss: 0.437403, acc.: 74.22%] [G loss: 2.112095]\n",
      "epoch:11 step:8934 [D loss: 0.376148, acc.: 87.50%] [G loss: 2.184922]\n",
      "epoch:11 step:8935 [D loss: 0.475092, acc.: 77.34%] [G loss: 2.321604]\n",
      "epoch:11 step:8936 [D loss: 0.309242, acc.: 85.94%] [G loss: 3.051199]\n",
      "epoch:11 step:8937 [D loss: 0.322113, acc.: 85.94%] [G loss: 2.603484]\n",
      "epoch:11 step:8938 [D loss: 0.379220, acc.: 82.81%] [G loss: 2.400805]\n",
      "epoch:11 step:8939 [D loss: 0.350445, acc.: 84.38%] [G loss: 2.003294]\n",
      "epoch:11 step:8940 [D loss: 0.311985, acc.: 85.94%] [G loss: 2.108835]\n",
      "epoch:11 step:8941 [D loss: 0.446151, acc.: 79.69%] [G loss: 2.296940]\n",
      "epoch:11 step:8942 [D loss: 0.394928, acc.: 82.03%] [G loss: 2.512293]\n",
      "epoch:11 step:8943 [D loss: 0.346205, acc.: 85.16%] [G loss: 3.727826]\n",
      "epoch:11 step:8944 [D loss: 0.274974, acc.: 87.50%] [G loss: 3.730818]\n",
      "epoch:11 step:8945 [D loss: 0.243423, acc.: 91.41%] [G loss: 3.235720]\n",
      "epoch:11 step:8946 [D loss: 0.261325, acc.: 88.28%] [G loss: 3.189142]\n",
      "epoch:11 step:8947 [D loss: 0.395667, acc.: 80.47%] [G loss: 2.924366]\n",
      "epoch:11 step:8948 [D loss: 0.533672, acc.: 75.00%] [G loss: 2.387085]\n",
      "epoch:11 step:8949 [D loss: 0.403197, acc.: 81.25%] [G loss: 2.893840]\n",
      "epoch:11 step:8950 [D loss: 0.321146, acc.: 88.28%] [G loss: 3.113152]\n",
      "epoch:11 step:8951 [D loss: 0.293706, acc.: 88.28%] [G loss: 3.209612]\n",
      "epoch:11 step:8952 [D loss: 0.371883, acc.: 85.16%] [G loss: 3.951465]\n",
      "epoch:11 step:8953 [D loss: 0.375539, acc.: 85.16%] [G loss: 3.229229]\n",
      "epoch:11 step:8954 [D loss: 0.446319, acc.: 77.34%] [G loss: 2.907931]\n",
      "epoch:11 step:8955 [D loss: 0.333830, acc.: 83.59%] [G loss: 3.736586]\n",
      "epoch:11 step:8956 [D loss: 0.475035, acc.: 78.12%] [G loss: 3.878481]\n",
      "epoch:11 step:8957 [D loss: 0.295776, acc.: 86.72%] [G loss: 4.477849]\n",
      "epoch:11 step:8958 [D loss: 0.289379, acc.: 84.38%] [G loss: 4.741330]\n",
      "epoch:11 step:8959 [D loss: 0.378684, acc.: 82.81%] [G loss: 3.680295]\n",
      "epoch:11 step:8960 [D loss: 0.264100, acc.: 86.72%] [G loss: 2.948463]\n",
      "epoch:11 step:8961 [D loss: 0.324429, acc.: 85.94%] [G loss: 3.153435]\n",
      "epoch:11 step:8962 [D loss: 0.327410, acc.: 88.28%] [G loss: 3.280567]\n",
      "epoch:11 step:8963 [D loss: 0.274145, acc.: 86.72%] [G loss: 2.474523]\n",
      "epoch:11 step:8964 [D loss: 0.359363, acc.: 85.94%] [G loss: 2.777983]\n",
      "epoch:11 step:8965 [D loss: 0.263327, acc.: 85.94%] [G loss: 3.548287]\n",
      "epoch:11 step:8966 [D loss: 0.329883, acc.: 83.59%] [G loss: 4.849690]\n",
      "epoch:11 step:8967 [D loss: 0.396394, acc.: 82.03%] [G loss: 2.228383]\n",
      "epoch:11 step:8968 [D loss: 0.392742, acc.: 84.38%] [G loss: 2.101806]\n",
      "epoch:11 step:8969 [D loss: 0.385743, acc.: 83.59%] [G loss: 4.116671]\n",
      "epoch:11 step:8970 [D loss: 0.256219, acc.: 88.28%] [G loss: 4.290298]\n",
      "epoch:11 step:8971 [D loss: 0.332665, acc.: 85.94%] [G loss: 2.835318]\n",
      "epoch:11 step:8972 [D loss: 0.159345, acc.: 93.75%] [G loss: 4.366206]\n",
      "epoch:11 step:8973 [D loss: 0.292235, acc.: 83.59%] [G loss: 3.729047]\n",
      "epoch:11 step:8974 [D loss: 0.323247, acc.: 86.72%] [G loss: 3.465574]\n",
      "epoch:11 step:8975 [D loss: 0.328360, acc.: 89.84%] [G loss: 2.511015]\n",
      "epoch:11 step:8976 [D loss: 0.366682, acc.: 88.28%] [G loss: 2.442367]\n",
      "epoch:11 step:8977 [D loss: 0.380875, acc.: 83.59%] [G loss: 2.266593]\n",
      "epoch:11 step:8978 [D loss: 0.344033, acc.: 83.59%] [G loss: 2.637091]\n",
      "epoch:11 step:8979 [D loss: 0.500894, acc.: 76.56%] [G loss: 2.680567]\n",
      "epoch:11 step:8980 [D loss: 0.404974, acc.: 87.50%] [G loss: 2.680398]\n",
      "epoch:11 step:8981 [D loss: 0.347228, acc.: 86.72%] [G loss: 2.698369]\n",
      "epoch:11 step:8982 [D loss: 0.421582, acc.: 79.69%] [G loss: 3.614640]\n",
      "epoch:11 step:8983 [D loss: 0.386438, acc.: 84.38%] [G loss: 4.059813]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:11 step:8984 [D loss: 0.280882, acc.: 88.28%] [G loss: 5.042476]\n",
      "epoch:11 step:8985 [D loss: 0.314058, acc.: 86.72%] [G loss: 3.003488]\n",
      "epoch:11 step:8986 [D loss: 0.348446, acc.: 85.16%] [G loss: 3.157477]\n",
      "epoch:11 step:8987 [D loss: 0.264411, acc.: 89.06%] [G loss: 3.000859]\n",
      "epoch:11 step:8988 [D loss: 0.345696, acc.: 85.16%] [G loss: 4.455653]\n",
      "epoch:11 step:8989 [D loss: 0.355320, acc.: 85.94%] [G loss: 2.538596]\n",
      "epoch:11 step:8990 [D loss: 0.358550, acc.: 85.16%] [G loss: 3.568232]\n",
      "epoch:11 step:8991 [D loss: 0.221210, acc.: 91.41%] [G loss: 5.061335]\n",
      "epoch:11 step:8992 [D loss: 0.275436, acc.: 87.50%] [G loss: 3.139143]\n",
      "epoch:11 step:8993 [D loss: 0.270248, acc.: 90.62%] [G loss: 5.313497]\n",
      "epoch:11 step:8994 [D loss: 0.315645, acc.: 87.50%] [G loss: 4.517893]\n",
      "epoch:11 step:8995 [D loss: 0.276566, acc.: 90.62%] [G loss: 3.839448]\n",
      "epoch:11 step:8996 [D loss: 0.194231, acc.: 92.97%] [G loss: 4.326616]\n",
      "epoch:11 step:8997 [D loss: 0.358198, acc.: 84.38%] [G loss: 2.570449]\n",
      "epoch:11 step:8998 [D loss: 0.231206, acc.: 89.84%] [G loss: 3.862318]\n",
      "epoch:11 step:8999 [D loss: 0.342982, acc.: 84.38%] [G loss: 3.699506]\n",
      "epoch:11 step:9000 [D loss: 0.433091, acc.: 83.59%] [G loss: 2.214726]\n",
      "##############\n",
      "[0.84700682 0.86870009 0.80098356 0.79328444 0.7717617  0.81299468\n",
      " 0.89449824 0.83266624 0.79456668 0.81160737]\n",
      "##########\n",
      "epoch:11 step:9001 [D loss: 0.294340, acc.: 86.72%] [G loss: 3.114795]\n",
      "epoch:11 step:9002 [D loss: 0.330695, acc.: 87.50%] [G loss: 3.043129]\n",
      "epoch:11 step:9003 [D loss: 0.307108, acc.: 87.50%] [G loss: 2.688391]\n",
      "epoch:11 step:9004 [D loss: 0.246761, acc.: 92.97%] [G loss: 3.485116]\n",
      "epoch:11 step:9005 [D loss: 0.245995, acc.: 89.84%] [G loss: 3.332606]\n",
      "epoch:11 step:9006 [D loss: 0.306207, acc.: 89.06%] [G loss: 3.163315]\n",
      "epoch:11 step:9007 [D loss: 0.341075, acc.: 85.94%] [G loss: 2.345377]\n",
      "epoch:11 step:9008 [D loss: 0.324074, acc.: 86.72%] [G loss: 2.973975]\n",
      "epoch:11 step:9009 [D loss: 0.291381, acc.: 88.28%] [G loss: 3.664002]\n",
      "epoch:11 step:9010 [D loss: 0.201832, acc.: 94.53%] [G loss: 3.479580]\n",
      "epoch:11 step:9011 [D loss: 0.256674, acc.: 89.84%] [G loss: 4.331478]\n",
      "epoch:11 step:9012 [D loss: 0.311122, acc.: 86.72%] [G loss: 2.784026]\n",
      "epoch:11 step:9013 [D loss: 0.297955, acc.: 89.06%] [G loss: 3.067740]\n",
      "epoch:11 step:9014 [D loss: 0.275662, acc.: 88.28%] [G loss: 3.205965]\n",
      "epoch:11 step:9015 [D loss: 0.281975, acc.: 91.41%] [G loss: 2.186674]\n",
      "epoch:11 step:9016 [D loss: 0.327693, acc.: 89.06%] [G loss: 2.433965]\n",
      "epoch:11 step:9017 [D loss: 0.365793, acc.: 85.94%] [G loss: 2.198144]\n",
      "epoch:11 step:9018 [D loss: 0.292986, acc.: 88.28%] [G loss: 2.516781]\n",
      "epoch:11 step:9019 [D loss: 0.294684, acc.: 89.06%] [G loss: 3.917009]\n",
      "epoch:11 step:9020 [D loss: 0.277870, acc.: 90.62%] [G loss: 2.777464]\n",
      "epoch:11 step:9021 [D loss: 0.253046, acc.: 87.50%] [G loss: 3.399676]\n",
      "epoch:11 step:9022 [D loss: 0.400083, acc.: 78.91%] [G loss: 2.489119]\n",
      "epoch:11 step:9023 [D loss: 0.364671, acc.: 82.81%] [G loss: 3.189629]\n",
      "epoch:11 step:9024 [D loss: 0.316408, acc.: 86.72%] [G loss: 3.018754]\n",
      "epoch:11 step:9025 [D loss: 0.307521, acc.: 86.72%] [G loss: 4.752333]\n",
      "epoch:11 step:9026 [D loss: 0.597429, acc.: 79.69%] [G loss: 5.636423]\n",
      "epoch:11 step:9027 [D loss: 1.523331, acc.: 60.16%] [G loss: 7.700042]\n",
      "epoch:11 step:9028 [D loss: 2.197982, acc.: 67.97%] [G loss: 2.484650]\n",
      "epoch:11 step:9029 [D loss: 1.879904, acc.: 57.81%] [G loss: 8.979152]\n",
      "epoch:11 step:9030 [D loss: 1.557739, acc.: 72.66%] [G loss: 3.070150]\n",
      "epoch:11 step:9031 [D loss: 1.167770, acc.: 71.09%] [G loss: 2.027786]\n",
      "epoch:11 step:9032 [D loss: 0.834612, acc.: 79.69%] [G loss: 3.530403]\n",
      "epoch:11 step:9033 [D loss: 0.517966, acc.: 74.22%] [G loss: 3.209719]\n",
      "epoch:11 step:9034 [D loss: 0.367854, acc.: 85.94%] [G loss: 3.061259]\n",
      "epoch:11 step:9035 [D loss: 0.409457, acc.: 81.25%] [G loss: 2.562671]\n",
      "epoch:11 step:9036 [D loss: 0.303292, acc.: 85.94%] [G loss: 2.749129]\n",
      "epoch:11 step:9037 [D loss: 0.396035, acc.: 82.03%] [G loss: 2.518880]\n",
      "epoch:11 step:9038 [D loss: 0.322942, acc.: 88.28%] [G loss: 3.124779]\n",
      "epoch:11 step:9039 [D loss: 0.356374, acc.: 86.72%] [G loss: 2.343481]\n",
      "epoch:11 step:9040 [D loss: 0.295867, acc.: 85.94%] [G loss: 2.513631]\n",
      "epoch:11 step:9041 [D loss: 0.284661, acc.: 90.62%] [G loss: 2.606065]\n",
      "epoch:11 step:9042 [D loss: 0.362984, acc.: 86.72%] [G loss: 2.383454]\n",
      "epoch:11 step:9043 [D loss: 0.361552, acc.: 82.81%] [G loss: 2.612558]\n",
      "epoch:11 step:9044 [D loss: 0.350517, acc.: 85.94%] [G loss: 2.873377]\n",
      "epoch:11 step:9045 [D loss: 0.348567, acc.: 83.59%] [G loss: 2.718662]\n",
      "epoch:11 step:9046 [D loss: 0.351353, acc.: 87.50%] [G loss: 2.638006]\n",
      "epoch:11 step:9047 [D loss: 0.543063, acc.: 72.66%] [G loss: 1.955627]\n",
      "epoch:11 step:9048 [D loss: 0.340237, acc.: 87.50%] [G loss: 2.822520]\n",
      "epoch:11 step:9049 [D loss: 0.284931, acc.: 89.84%] [G loss: 3.430501]\n",
      "epoch:11 step:9050 [D loss: 0.261832, acc.: 90.62%] [G loss: 2.474409]\n",
      "epoch:11 step:9051 [D loss: 0.296736, acc.: 92.97%] [G loss: 3.143577]\n",
      "epoch:11 step:9052 [D loss: 0.252909, acc.: 92.19%] [G loss: 3.808344]\n",
      "epoch:11 step:9053 [D loss: 0.282970, acc.: 91.41%] [G loss: 3.188544]\n",
      "epoch:11 step:9054 [D loss: 0.340852, acc.: 84.38%] [G loss: 2.312475]\n",
      "epoch:11 step:9055 [D loss: 0.366631, acc.: 82.03%] [G loss: 2.404606]\n",
      "epoch:11 step:9056 [D loss: 0.322623, acc.: 87.50%] [G loss: 2.582275]\n",
      "epoch:11 step:9057 [D loss: 0.349330, acc.: 85.16%] [G loss: 2.763112]\n",
      "epoch:11 step:9058 [D loss: 0.390309, acc.: 84.38%] [G loss: 2.932898]\n",
      "epoch:11 step:9059 [D loss: 0.359886, acc.: 88.28%] [G loss: 2.599180]\n",
      "epoch:11 step:9060 [D loss: 0.432753, acc.: 78.91%] [G loss: 2.979341]\n",
      "epoch:11 step:9061 [D loss: 0.389466, acc.: 83.59%] [G loss: 2.980818]\n",
      "epoch:11 step:9062 [D loss: 0.230976, acc.: 90.62%] [G loss: 2.745476]\n",
      "epoch:11 step:9063 [D loss: 0.348413, acc.: 86.72%] [G loss: 2.820420]\n",
      "epoch:11 step:9064 [D loss: 0.365971, acc.: 82.81%] [G loss: 2.317171]\n",
      "epoch:11 step:9065 [D loss: 0.341964, acc.: 84.38%] [G loss: 1.990981]\n",
      "epoch:11 step:9066 [D loss: 0.281420, acc.: 92.19%] [G loss: 2.547642]\n",
      "epoch:11 step:9067 [D loss: 0.369765, acc.: 84.38%] [G loss: 2.543919]\n",
      "epoch:11 step:9068 [D loss: 0.313066, acc.: 85.94%] [G loss: 3.081060]\n",
      "epoch:11 step:9069 [D loss: 0.353618, acc.: 85.94%] [G loss: 3.417374]\n",
      "epoch:11 step:9070 [D loss: 0.373694, acc.: 86.72%] [G loss: 2.772007]\n",
      "epoch:11 step:9071 [D loss: 0.315979, acc.: 84.38%] [G loss: 2.183990]\n",
      "epoch:11 step:9072 [D loss: 0.465507, acc.: 82.03%] [G loss: 2.711368]\n",
      "epoch:11 step:9073 [D loss: 0.347724, acc.: 81.25%] [G loss: 2.258630]\n",
      "epoch:11 step:9074 [D loss: 0.335902, acc.: 82.81%] [G loss: 2.435680]\n",
      "epoch:11 step:9075 [D loss: 0.362622, acc.: 88.28%] [G loss: 2.617598]\n",
      "epoch:11 step:9076 [D loss: 0.279449, acc.: 89.84%] [G loss: 2.550553]\n",
      "epoch:11 step:9077 [D loss: 0.297998, acc.: 86.72%] [G loss: 2.992457]\n",
      "epoch:11 step:9078 [D loss: 0.258386, acc.: 89.84%] [G loss: 3.551485]\n",
      "epoch:11 step:9079 [D loss: 0.314935, acc.: 85.94%] [G loss: 4.298039]\n",
      "epoch:11 step:9080 [D loss: 0.252776, acc.: 89.06%] [G loss: 3.186373]\n",
      "epoch:11 step:9081 [D loss: 0.379797, acc.: 78.91%] [G loss: 2.334422]\n",
      "epoch:11 step:9082 [D loss: 0.379014, acc.: 82.81%] [G loss: 2.470558]\n",
      "epoch:11 step:9083 [D loss: 0.347947, acc.: 81.25%] [G loss: 3.850112]\n",
      "epoch:11 step:9084 [D loss: 0.230714, acc.: 92.97%] [G loss: 4.481138]\n",
      "epoch:11 step:9085 [D loss: 0.418997, acc.: 81.25%] [G loss: 2.286298]\n",
      "epoch:11 step:9086 [D loss: 0.310930, acc.: 89.06%] [G loss: 2.750977]\n",
      "epoch:11 step:9087 [D loss: 0.264846, acc.: 85.94%] [G loss: 3.926572]\n",
      "epoch:11 step:9088 [D loss: 0.393037, acc.: 82.03%] [G loss: 2.666152]\n",
      "epoch:11 step:9089 [D loss: 0.507178, acc.: 76.56%] [G loss: 3.532157]\n",
      "epoch:11 step:9090 [D loss: 0.294304, acc.: 86.72%] [G loss: 3.420572]\n",
      "epoch:11 step:9091 [D loss: 0.329883, acc.: 86.72%] [G loss: 3.368059]\n",
      "epoch:11 step:9092 [D loss: 0.243870, acc.: 89.84%] [G loss: 5.073657]\n",
      "epoch:11 step:9093 [D loss: 0.315428, acc.: 85.94%] [G loss: 4.732093]\n",
      "epoch:11 step:9094 [D loss: 0.321213, acc.: 85.94%] [G loss: 2.137193]\n",
      "epoch:11 step:9095 [D loss: 0.291569, acc.: 86.72%] [G loss: 3.401643]\n",
      "epoch:11 step:9096 [D loss: 0.240541, acc.: 89.84%] [G loss: 5.088267]\n",
      "epoch:11 step:9097 [D loss: 0.305209, acc.: 86.72%] [G loss: 2.735321]\n",
      "epoch:11 step:9098 [D loss: 0.254211, acc.: 89.84%] [G loss: 4.559604]\n",
      "epoch:11 step:9099 [D loss: 0.205431, acc.: 92.97%] [G loss: 3.503243]\n",
      "epoch:11 step:9100 [D loss: 0.320005, acc.: 85.16%] [G loss: 2.451552]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:11 step:9101 [D loss: 0.340952, acc.: 85.16%] [G loss: 2.950531]\n",
      "epoch:11 step:9102 [D loss: 0.338702, acc.: 82.03%] [G loss: 2.647410]\n",
      "epoch:11 step:9103 [D loss: 0.360601, acc.: 85.94%] [G loss: 3.177131]\n",
      "epoch:11 step:9104 [D loss: 0.435429, acc.: 80.47%] [G loss: 3.300833]\n",
      "epoch:11 step:9105 [D loss: 0.367576, acc.: 83.59%] [G loss: 2.934959]\n",
      "epoch:11 step:9106 [D loss: 0.347999, acc.: 86.72%] [G loss: 2.905100]\n",
      "epoch:11 step:9107 [D loss: 0.438639, acc.: 81.25%] [G loss: 2.153700]\n",
      "epoch:11 step:9108 [D loss: 0.284837, acc.: 87.50%] [G loss: 2.576377]\n",
      "epoch:11 step:9109 [D loss: 0.243490, acc.: 89.06%] [G loss: 3.340878]\n",
      "epoch:11 step:9110 [D loss: 0.229514, acc.: 91.41%] [G loss: 3.264786]\n",
      "epoch:11 step:9111 [D loss: 0.371906, acc.: 80.47%] [G loss: 2.416832]\n",
      "epoch:11 step:9112 [D loss: 0.287658, acc.: 89.84%] [G loss: 4.111280]\n",
      "epoch:11 step:9113 [D loss: 0.297777, acc.: 82.03%] [G loss: 4.354127]\n",
      "epoch:11 step:9114 [D loss: 0.361870, acc.: 83.59%] [G loss: 2.677110]\n",
      "epoch:11 step:9115 [D loss: 0.347981, acc.: 80.47%] [G loss: 3.502011]\n",
      "epoch:11 step:9116 [D loss: 0.401457, acc.: 81.25%] [G loss: 2.995335]\n",
      "epoch:11 step:9117 [D loss: 0.288291, acc.: 86.72%] [G loss: 3.799998]\n",
      "epoch:11 step:9118 [D loss: 0.379632, acc.: 82.81%] [G loss: 3.150103]\n",
      "epoch:11 step:9119 [D loss: 0.390305, acc.: 82.81%] [G loss: 2.571149]\n",
      "epoch:11 step:9120 [D loss: 0.360476, acc.: 80.47%] [G loss: 3.374817]\n",
      "epoch:11 step:9121 [D loss: 0.275499, acc.: 87.50%] [G loss: 4.574120]\n",
      "epoch:11 step:9122 [D loss: 0.404169, acc.: 81.25%] [G loss: 3.487638]\n",
      "epoch:11 step:9123 [D loss: 0.321169, acc.: 85.16%] [G loss: 3.222860]\n",
      "epoch:11 step:9124 [D loss: 0.396439, acc.: 81.25%] [G loss: 2.904402]\n",
      "epoch:11 step:9125 [D loss: 0.460016, acc.: 77.34%] [G loss: 2.715875]\n",
      "epoch:11 step:9126 [D loss: 0.525388, acc.: 72.66%] [G loss: 4.408870]\n",
      "epoch:11 step:9127 [D loss: 0.517094, acc.: 75.78%] [G loss: 2.163812]\n",
      "epoch:11 step:9128 [D loss: 0.406054, acc.: 82.81%] [G loss: 3.109303]\n",
      "epoch:11 step:9129 [D loss: 0.293280, acc.: 86.72%] [G loss: 4.129076]\n",
      "epoch:11 step:9130 [D loss: 0.379308, acc.: 82.03%] [G loss: 4.555763]\n",
      "epoch:11 step:9131 [D loss: 0.331311, acc.: 85.94%] [G loss: 2.749580]\n",
      "epoch:11 step:9132 [D loss: 0.458672, acc.: 78.91%] [G loss: 2.143992]\n",
      "epoch:11 step:9133 [D loss: 0.351364, acc.: 86.72%] [G loss: 2.424082]\n",
      "epoch:11 step:9134 [D loss: 0.312663, acc.: 89.06%] [G loss: 2.451207]\n",
      "epoch:11 step:9135 [D loss: 0.366536, acc.: 81.25%] [G loss: 2.303143]\n",
      "epoch:11 step:9136 [D loss: 0.326922, acc.: 87.50%] [G loss: 2.499592]\n",
      "epoch:11 step:9137 [D loss: 0.260181, acc.: 90.62%] [G loss: 3.172933]\n",
      "epoch:11 step:9138 [D loss: 0.361373, acc.: 82.03%] [G loss: 3.442674]\n",
      "epoch:11 step:9139 [D loss: 0.265074, acc.: 87.50%] [G loss: 2.879214]\n",
      "epoch:11 step:9140 [D loss: 0.365786, acc.: 86.72%] [G loss: 2.405270]\n",
      "epoch:11 step:9141 [D loss: 0.254948, acc.: 92.19%] [G loss: 2.734883]\n",
      "epoch:11 step:9142 [D loss: 0.205550, acc.: 90.62%] [G loss: 5.243184]\n",
      "epoch:11 step:9143 [D loss: 0.364148, acc.: 85.16%] [G loss: 2.748152]\n",
      "epoch:11 step:9144 [D loss: 0.327883, acc.: 88.28%] [G loss: 2.784031]\n",
      "epoch:11 step:9145 [D loss: 0.374724, acc.: 84.38%] [G loss: 2.962560]\n",
      "epoch:11 step:9146 [D loss: 0.366145, acc.: 86.72%] [G loss: 2.599078]\n",
      "epoch:11 step:9147 [D loss: 0.412008, acc.: 81.25%] [G loss: 3.031954]\n",
      "epoch:11 step:9148 [D loss: 0.420035, acc.: 81.25%] [G loss: 3.216140]\n",
      "epoch:11 step:9149 [D loss: 0.337946, acc.: 85.16%] [G loss: 3.283622]\n",
      "epoch:11 step:9150 [D loss: 0.338127, acc.: 85.16%] [G loss: 2.820697]\n",
      "epoch:11 step:9151 [D loss: 0.344298, acc.: 82.03%] [G loss: 2.783764]\n",
      "epoch:11 step:9152 [D loss: 0.406964, acc.: 84.38%] [G loss: 3.319063]\n",
      "epoch:11 step:9153 [D loss: 0.294110, acc.: 87.50%] [G loss: 4.411587]\n",
      "epoch:11 step:9154 [D loss: 0.353678, acc.: 83.59%] [G loss: 2.963904]\n",
      "epoch:11 step:9155 [D loss: 0.203662, acc.: 93.75%] [G loss: 3.173070]\n",
      "epoch:11 step:9156 [D loss: 0.352919, acc.: 85.94%] [G loss: 2.776712]\n",
      "epoch:11 step:9157 [D loss: 0.327884, acc.: 88.28%] [G loss: 2.992482]\n",
      "epoch:11 step:9158 [D loss: 0.340986, acc.: 81.25%] [G loss: 2.343014]\n",
      "epoch:11 step:9159 [D loss: 0.312586, acc.: 85.94%] [G loss: 2.803213]\n",
      "epoch:11 step:9160 [D loss: 0.329524, acc.: 86.72%] [G loss: 3.336508]\n",
      "epoch:11 step:9161 [D loss: 0.204337, acc.: 92.19%] [G loss: 4.550870]\n",
      "epoch:11 step:9162 [D loss: 0.227262, acc.: 87.50%] [G loss: 5.053906]\n",
      "epoch:11 step:9163 [D loss: 0.205578, acc.: 92.97%] [G loss: 5.246001]\n",
      "epoch:11 step:9164 [D loss: 0.242616, acc.: 90.62%] [G loss: 3.655148]\n",
      "epoch:11 step:9165 [D loss: 0.364691, acc.: 82.03%] [G loss: 2.643521]\n",
      "epoch:11 step:9166 [D loss: 0.318500, acc.: 85.16%] [G loss: 3.728666]\n",
      "epoch:11 step:9167 [D loss: 0.236570, acc.: 90.62%] [G loss: 4.856764]\n",
      "epoch:11 step:9168 [D loss: 0.270864, acc.: 89.84%] [G loss: 2.864332]\n",
      "epoch:11 step:9169 [D loss: 0.328816, acc.: 89.84%] [G loss: 3.814454]\n",
      "epoch:11 step:9170 [D loss: 0.371778, acc.: 82.03%] [G loss: 2.975236]\n",
      "epoch:11 step:9171 [D loss: 0.444423, acc.: 81.25%] [G loss: 3.094502]\n",
      "epoch:11 step:9172 [D loss: 0.354376, acc.: 86.72%] [G loss: 3.572299]\n",
      "epoch:11 step:9173 [D loss: 0.266914, acc.: 91.41%] [G loss: 4.903762]\n",
      "epoch:11 step:9174 [D loss: 0.487621, acc.: 73.44%] [G loss: 2.256661]\n",
      "epoch:11 step:9175 [D loss: 0.376273, acc.: 79.69%] [G loss: 2.718906]\n",
      "epoch:11 step:9176 [D loss: 0.307908, acc.: 85.94%] [G loss: 3.097404]\n",
      "epoch:11 step:9177 [D loss: 0.323979, acc.: 85.16%] [G loss: 2.323792]\n",
      "epoch:11 step:9178 [D loss: 0.327303, acc.: 88.28%] [G loss: 2.205421]\n",
      "epoch:11 step:9179 [D loss: 0.289955, acc.: 89.84%] [G loss: 2.851415]\n",
      "epoch:11 step:9180 [D loss: 0.317913, acc.: 88.28%] [G loss: 2.925933]\n",
      "epoch:11 step:9181 [D loss: 0.347425, acc.: 88.28%] [G loss: 2.961352]\n",
      "epoch:11 step:9182 [D loss: 0.321643, acc.: 89.06%] [G loss: 2.724573]\n",
      "epoch:11 step:9183 [D loss: 0.325701, acc.: 87.50%] [G loss: 3.117685]\n",
      "epoch:11 step:9184 [D loss: 0.336663, acc.: 89.06%] [G loss: 2.834103]\n",
      "epoch:11 step:9185 [D loss: 0.266808, acc.: 92.19%] [G loss: 3.280255]\n",
      "epoch:11 step:9186 [D loss: 0.466039, acc.: 75.00%] [G loss: 3.993042]\n",
      "epoch:11 step:9187 [D loss: 0.486551, acc.: 76.56%] [G loss: 2.804334]\n",
      "epoch:11 step:9188 [D loss: 0.586154, acc.: 70.31%] [G loss: 5.424550]\n",
      "epoch:11 step:9189 [D loss: 0.477154, acc.: 78.91%] [G loss: 3.816939]\n",
      "epoch:11 step:9190 [D loss: 0.358178, acc.: 83.59%] [G loss: 6.146332]\n",
      "epoch:11 step:9191 [D loss: 0.392803, acc.: 82.03%] [G loss: 4.997522]\n",
      "epoch:11 step:9192 [D loss: 0.372063, acc.: 85.16%] [G loss: 3.210807]\n",
      "epoch:11 step:9193 [D loss: 0.286207, acc.: 88.28%] [G loss: 3.196639]\n",
      "epoch:11 step:9194 [D loss: 0.320287, acc.: 87.50%] [G loss: 4.356555]\n",
      "epoch:11 step:9195 [D loss: 0.340994, acc.: 84.38%] [G loss: 2.331305]\n",
      "epoch:11 step:9196 [D loss: 0.355235, acc.: 85.16%] [G loss: 2.474987]\n",
      "epoch:11 step:9197 [D loss: 0.289811, acc.: 89.06%] [G loss: 3.240224]\n",
      "epoch:11 step:9198 [D loss: 0.285895, acc.: 88.28%] [G loss: 4.166991]\n",
      "epoch:11 step:9199 [D loss: 0.257411, acc.: 91.41%] [G loss: 4.886585]\n",
      "epoch:11 step:9200 [D loss: 0.403395, acc.: 87.50%] [G loss: 2.147562]\n",
      "##############\n",
      "[0.8347688  0.85438692 0.80921899 0.79675881 0.78155995 0.81338413\n",
      " 0.87072877 0.82247341 0.81010886 0.82037496]\n",
      "##########\n",
      "epoch:11 step:9201 [D loss: 0.341590, acc.: 83.59%] [G loss: 2.586147]\n",
      "epoch:11 step:9202 [D loss: 0.299832, acc.: 87.50%] [G loss: 2.857273]\n",
      "epoch:11 step:9203 [D loss: 0.261796, acc.: 87.50%] [G loss: 3.525516]\n",
      "epoch:11 step:9204 [D loss: 0.368390, acc.: 85.94%] [G loss: 3.918710]\n",
      "epoch:11 step:9205 [D loss: 0.303824, acc.: 89.84%] [G loss: 3.028184]\n",
      "epoch:11 step:9206 [D loss: 0.321177, acc.: 85.16%] [G loss: 2.701623]\n",
      "epoch:11 step:9207 [D loss: 0.327002, acc.: 84.38%] [G loss: 2.719237]\n",
      "epoch:11 step:9208 [D loss: 0.354965, acc.: 84.38%] [G loss: 2.435663]\n",
      "epoch:11 step:9209 [D loss: 0.291492, acc.: 89.06%] [G loss: 2.769016]\n",
      "epoch:11 step:9210 [D loss: 0.347182, acc.: 84.38%] [G loss: 2.645442]\n",
      "epoch:11 step:9211 [D loss: 0.284994, acc.: 86.72%] [G loss: 2.641047]\n",
      "epoch:11 step:9212 [D loss: 0.266803, acc.: 91.41%] [G loss: 2.380919]\n",
      "epoch:11 step:9213 [D loss: 0.281073, acc.: 89.06%] [G loss: 2.376106]\n",
      "epoch:11 step:9214 [D loss: 0.456377, acc.: 81.25%] [G loss: 2.232456]\n",
      "epoch:11 step:9215 [D loss: 0.323667, acc.: 85.94%] [G loss: 2.434795]\n",
      "epoch:11 step:9216 [D loss: 0.371245, acc.: 82.81%] [G loss: 2.594687]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:11 step:9217 [D loss: 0.334540, acc.: 87.50%] [G loss: 2.574383]\n",
      "epoch:11 step:9218 [D loss: 0.308836, acc.: 88.28%] [G loss: 3.217356]\n",
      "epoch:11 step:9219 [D loss: 0.322253, acc.: 82.81%] [G loss: 4.320899]\n",
      "epoch:11 step:9220 [D loss: 0.410491, acc.: 79.69%] [G loss: 2.248728]\n",
      "epoch:11 step:9221 [D loss: 0.350538, acc.: 84.38%] [G loss: 2.961795]\n",
      "epoch:11 step:9222 [D loss: 0.298979, acc.: 86.72%] [G loss: 2.530301]\n",
      "epoch:11 step:9223 [D loss: 0.270823, acc.: 89.06%] [G loss: 3.251560]\n",
      "epoch:11 step:9224 [D loss: 0.368167, acc.: 83.59%] [G loss: 4.226483]\n",
      "epoch:11 step:9225 [D loss: 0.289409, acc.: 88.28%] [G loss: 3.300894]\n",
      "epoch:11 step:9226 [D loss: 0.386447, acc.: 84.38%] [G loss: 2.611682]\n",
      "epoch:11 step:9227 [D loss: 0.380422, acc.: 84.38%] [G loss: 2.617493]\n",
      "epoch:11 step:9228 [D loss: 0.377835, acc.: 85.16%] [G loss: 2.391626]\n",
      "epoch:11 step:9229 [D loss: 0.331261, acc.: 89.06%] [G loss: 2.747201]\n",
      "epoch:11 step:9230 [D loss: 0.228781, acc.: 92.19%] [G loss: 2.948822]\n",
      "epoch:11 step:9231 [D loss: 0.265460, acc.: 92.19%] [G loss: 2.990447]\n",
      "epoch:11 step:9232 [D loss: 0.312358, acc.: 86.72%] [G loss: 4.290376]\n",
      "epoch:11 step:9233 [D loss: 0.288249, acc.: 85.94%] [G loss: 3.120756]\n",
      "epoch:11 step:9234 [D loss: 0.306741, acc.: 89.06%] [G loss: 2.090514]\n",
      "epoch:11 step:9235 [D loss: 0.217357, acc.: 92.19%] [G loss: 2.697122]\n",
      "epoch:11 step:9236 [D loss: 0.313135, acc.: 88.28%] [G loss: 2.507330]\n",
      "epoch:11 step:9237 [D loss: 0.479495, acc.: 81.25%] [G loss: 3.067354]\n",
      "epoch:11 step:9238 [D loss: 0.233452, acc.: 89.06%] [G loss: 3.230319]\n",
      "epoch:11 step:9239 [D loss: 0.218381, acc.: 92.19%] [G loss: 2.897248]\n",
      "epoch:11 step:9240 [D loss: 0.312375, acc.: 88.28%] [G loss: 2.530396]\n",
      "epoch:11 step:9241 [D loss: 0.304571, acc.: 85.94%] [G loss: 2.674432]\n",
      "epoch:11 step:9242 [D loss: 0.273797, acc.: 89.84%] [G loss: 2.818601]\n",
      "epoch:11 step:9243 [D loss: 0.283376, acc.: 88.28%] [G loss: 2.636829]\n",
      "epoch:11 step:9244 [D loss: 0.338973, acc.: 82.81%] [G loss: 4.931450]\n",
      "epoch:11 step:9245 [D loss: 0.437843, acc.: 79.69%] [G loss: 2.503299]\n",
      "epoch:11 step:9246 [D loss: 0.260297, acc.: 89.06%] [G loss: 5.378247]\n",
      "epoch:11 step:9247 [D loss: 0.442166, acc.: 79.69%] [G loss: 3.453578]\n",
      "epoch:11 step:9248 [D loss: 0.680118, acc.: 77.34%] [G loss: 5.020965]\n",
      "epoch:11 step:9249 [D loss: 0.999137, acc.: 61.72%] [G loss: 6.450264]\n",
      "epoch:11 step:9250 [D loss: 0.507031, acc.: 81.25%] [G loss: 4.569888]\n",
      "epoch:11 step:9251 [D loss: 0.641273, acc.: 76.56%] [G loss: 2.431193]\n",
      "epoch:11 step:9252 [D loss: 0.504873, acc.: 78.12%] [G loss: 4.554091]\n",
      "epoch:11 step:9253 [D loss: 0.316568, acc.: 82.81%] [G loss: 5.040787]\n",
      "epoch:11 step:9254 [D loss: 0.206803, acc.: 90.62%] [G loss: 5.224268]\n",
      "epoch:11 step:9255 [D loss: 0.300022, acc.: 83.59%] [G loss: 3.325735]\n",
      "epoch:11 step:9256 [D loss: 0.251860, acc.: 85.94%] [G loss: 5.461742]\n",
      "epoch:11 step:9257 [D loss: 0.276009, acc.: 89.06%] [G loss: 5.175508]\n",
      "epoch:11 step:9258 [D loss: 0.284929, acc.: 87.50%] [G loss: 2.826738]\n",
      "epoch:11 step:9259 [D loss: 0.366108, acc.: 86.72%] [G loss: 3.413046]\n",
      "epoch:11 step:9260 [D loss: 0.311305, acc.: 88.28%] [G loss: 2.342302]\n",
      "epoch:11 step:9261 [D loss: 0.362945, acc.: 82.03%] [G loss: 3.245063]\n",
      "epoch:11 step:9262 [D loss: 0.379782, acc.: 82.03%] [G loss: 3.123592]\n",
      "epoch:11 step:9263 [D loss: 0.433885, acc.: 82.81%] [G loss: 3.501234]\n",
      "epoch:11 step:9264 [D loss: 0.301990, acc.: 86.72%] [G loss: 2.867242]\n",
      "epoch:11 step:9265 [D loss: 0.193436, acc.: 94.53%] [G loss: 4.335750]\n",
      "epoch:11 step:9266 [D loss: 0.270852, acc.: 88.28%] [G loss: 2.805297]\n",
      "epoch:11 step:9267 [D loss: 0.456431, acc.: 80.47%] [G loss: 2.490062]\n",
      "epoch:11 step:9268 [D loss: 0.277595, acc.: 89.06%] [G loss: 2.693180]\n",
      "epoch:11 step:9269 [D loss: 0.336095, acc.: 83.59%] [G loss: 2.682836]\n",
      "epoch:11 step:9270 [D loss: 0.466374, acc.: 78.12%] [G loss: 3.052497]\n",
      "epoch:11 step:9271 [D loss: 0.389789, acc.: 82.03%] [G loss: 2.643153]\n",
      "epoch:11 step:9272 [D loss: 0.252090, acc.: 93.75%] [G loss: 2.720929]\n",
      "epoch:11 step:9273 [D loss: 0.530372, acc.: 77.34%] [G loss: 2.713073]\n",
      "epoch:11 step:9274 [D loss: 0.365047, acc.: 81.25%] [G loss: 2.872625]\n",
      "epoch:11 step:9275 [D loss: 0.363884, acc.: 85.94%] [G loss: 2.295327]\n",
      "epoch:11 step:9276 [D loss: 0.343681, acc.: 85.16%] [G loss: 2.276051]\n",
      "epoch:11 step:9277 [D loss: 0.360255, acc.: 85.16%] [G loss: 2.525486]\n",
      "epoch:11 step:9278 [D loss: 0.370670, acc.: 87.50%] [G loss: 2.543749]\n",
      "epoch:11 step:9279 [D loss: 0.304181, acc.: 92.19%] [G loss: 3.199413]\n",
      "epoch:11 step:9280 [D loss: 0.401289, acc.: 82.81%] [G loss: 2.812031]\n",
      "epoch:11 step:9281 [D loss: 0.327790, acc.: 86.72%] [G loss: 2.685719]\n",
      "epoch:11 step:9282 [D loss: 0.351725, acc.: 86.72%] [G loss: 2.395168]\n",
      "epoch:11 step:9283 [D loss: 0.373882, acc.: 82.03%] [G loss: 2.388385]\n",
      "epoch:11 step:9284 [D loss: 0.288114, acc.: 87.50%] [G loss: 3.856025]\n",
      "epoch:11 step:9285 [D loss: 0.213294, acc.: 85.94%] [G loss: 6.185132]\n",
      "epoch:11 step:9286 [D loss: 0.263853, acc.: 86.72%] [G loss: 3.352251]\n",
      "epoch:11 step:9287 [D loss: 0.221867, acc.: 91.41%] [G loss: 6.701294]\n",
      "epoch:11 step:9288 [D loss: 0.248012, acc.: 88.28%] [G loss: 4.394705]\n",
      "epoch:11 step:9289 [D loss: 0.183666, acc.: 91.41%] [G loss: 3.775919]\n",
      "epoch:11 step:9290 [D loss: 0.289039, acc.: 89.84%] [G loss: 2.772720]\n",
      "epoch:11 step:9291 [D loss: 0.307239, acc.: 85.94%] [G loss: 3.650175]\n",
      "epoch:11 step:9292 [D loss: 0.433373, acc.: 80.47%] [G loss: 2.753458]\n",
      "epoch:11 step:9293 [D loss: 0.232623, acc.: 90.62%] [G loss: 3.627596]\n",
      "epoch:11 step:9294 [D loss: 0.595300, acc.: 74.22%] [G loss: 5.992482]\n",
      "epoch:11 step:9295 [D loss: 0.879100, acc.: 72.66%] [G loss: 4.189669]\n",
      "epoch:11 step:9296 [D loss: 0.863939, acc.: 70.31%] [G loss: 4.378255]\n",
      "epoch:11 step:9297 [D loss: 0.489159, acc.: 78.12%] [G loss: 4.040453]\n",
      "epoch:11 step:9298 [D loss: 0.618393, acc.: 75.00%] [G loss: 2.830785]\n",
      "epoch:11 step:9299 [D loss: 0.399138, acc.: 79.69%] [G loss: 3.508614]\n",
      "epoch:11 step:9300 [D loss: 0.588536, acc.: 78.91%] [G loss: 2.687629]\n",
      "epoch:11 step:9301 [D loss: 0.684418, acc.: 70.31%] [G loss: 3.120503]\n",
      "epoch:11 step:9302 [D loss: 0.315046, acc.: 85.16%] [G loss: 4.455846]\n",
      "epoch:11 step:9303 [D loss: 0.356508, acc.: 81.25%] [G loss: 3.292484]\n",
      "epoch:11 step:9304 [D loss: 0.281189, acc.: 89.06%] [G loss: 4.155955]\n",
      "epoch:11 step:9305 [D loss: 0.281558, acc.: 89.84%] [G loss: 3.757102]\n",
      "epoch:11 step:9306 [D loss: 0.464475, acc.: 78.91%] [G loss: 3.278808]\n",
      "epoch:11 step:9307 [D loss: 0.239468, acc.: 89.84%] [G loss: 2.962248]\n",
      "epoch:11 step:9308 [D loss: 0.281965, acc.: 89.06%] [G loss: 2.859559]\n",
      "epoch:11 step:9309 [D loss: 0.340467, acc.: 84.38%] [G loss: 2.930690]\n",
      "epoch:11 step:9310 [D loss: 0.265971, acc.: 89.84%] [G loss: 3.127636]\n",
      "epoch:11 step:9311 [D loss: 0.248979, acc.: 90.62%] [G loss: 3.598514]\n",
      "epoch:11 step:9312 [D loss: 0.336996, acc.: 87.50%] [G loss: 2.194497]\n",
      "epoch:11 step:9313 [D loss: 0.325773, acc.: 88.28%] [G loss: 2.582852]\n",
      "epoch:11 step:9314 [D loss: 0.229422, acc.: 89.84%] [G loss: 3.962520]\n",
      "epoch:11 step:9315 [D loss: 0.280039, acc.: 92.97%] [G loss: 3.180292]\n",
      "epoch:11 step:9316 [D loss: 0.264579, acc.: 88.28%] [G loss: 2.750503]\n",
      "epoch:11 step:9317 [D loss: 0.340305, acc.: 85.16%] [G loss: 2.873591]\n",
      "epoch:11 step:9318 [D loss: 0.335556, acc.: 87.50%] [G loss: 4.869932]\n",
      "epoch:11 step:9319 [D loss: 0.208919, acc.: 91.41%] [G loss: 4.698566]\n",
      "epoch:11 step:9320 [D loss: 0.404205, acc.: 84.38%] [G loss: 2.694530]\n",
      "epoch:11 step:9321 [D loss: 0.312203, acc.: 86.72%] [G loss: 2.541665]\n",
      "epoch:11 step:9322 [D loss: 0.288616, acc.: 85.94%] [G loss: 2.537197]\n",
      "epoch:11 step:9323 [D loss: 0.344266, acc.: 84.38%] [G loss: 3.073715]\n",
      "epoch:11 step:9324 [D loss: 0.276888, acc.: 85.94%] [G loss: 3.580559]\n",
      "epoch:11 step:9325 [D loss: 0.322423, acc.: 84.38%] [G loss: 4.185823]\n",
      "epoch:11 step:9326 [D loss: 0.360446, acc.: 84.38%] [G loss: 2.634677]\n",
      "epoch:11 step:9327 [D loss: 0.308345, acc.: 86.72%] [G loss: 2.323780]\n",
      "epoch:11 step:9328 [D loss: 0.324205, acc.: 86.72%] [G loss: 2.318996]\n",
      "epoch:11 step:9329 [D loss: 0.406695, acc.: 80.47%] [G loss: 2.690824]\n",
      "epoch:11 step:9330 [D loss: 0.275665, acc.: 85.94%] [G loss: 4.445670]\n",
      "epoch:11 step:9331 [D loss: 0.226451, acc.: 91.41%] [G loss: 4.422239]\n",
      "epoch:11 step:9332 [D loss: 0.359089, acc.: 83.59%] [G loss: 2.787094]\n",
      "epoch:11 step:9333 [D loss: 0.416263, acc.: 84.38%] [G loss: 3.030008]\n",
      "epoch:11 step:9334 [D loss: 0.226793, acc.: 92.97%] [G loss: 3.001306]\n",
      "epoch:11 step:9335 [D loss: 0.296171, acc.: 87.50%] [G loss: 3.078368]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:11 step:9336 [D loss: 0.316288, acc.: 86.72%] [G loss: 3.512581]\n",
      "epoch:11 step:9337 [D loss: 0.216408, acc.: 91.41%] [G loss: 3.772460]\n",
      "epoch:11 step:9338 [D loss: 0.306120, acc.: 86.72%] [G loss: 2.532765]\n",
      "epoch:11 step:9339 [D loss: 0.245622, acc.: 93.75%] [G loss: 2.955147]\n",
      "epoch:11 step:9340 [D loss: 0.343636, acc.: 84.38%] [G loss: 4.071998]\n",
      "epoch:11 step:9341 [D loss: 0.356786, acc.: 84.38%] [G loss: 2.270787]\n",
      "epoch:11 step:9342 [D loss: 0.297173, acc.: 87.50%] [G loss: 3.855638]\n",
      "epoch:11 step:9343 [D loss: 0.204162, acc.: 93.75%] [G loss: 3.216293]\n",
      "epoch:11 step:9344 [D loss: 0.306260, acc.: 87.50%] [G loss: 4.518672]\n",
      "epoch:11 step:9345 [D loss: 0.270009, acc.: 92.19%] [G loss: 4.778970]\n",
      "epoch:11 step:9346 [D loss: 0.291058, acc.: 86.72%] [G loss: 3.345494]\n",
      "epoch:11 step:9347 [D loss: 0.254705, acc.: 92.19%] [G loss: 2.933092]\n",
      "epoch:11 step:9348 [D loss: 0.246754, acc.: 92.97%] [G loss: 3.575098]\n",
      "epoch:11 step:9349 [D loss: 0.201959, acc.: 93.75%] [G loss: 5.894803]\n",
      "epoch:11 step:9350 [D loss: 0.248049, acc.: 85.94%] [G loss: 2.828436]\n",
      "epoch:11 step:9351 [D loss: 0.257183, acc.: 91.41%] [G loss: 3.274776]\n",
      "epoch:11 step:9352 [D loss: 0.191095, acc.: 92.19%] [G loss: 3.294637]\n",
      "epoch:11 step:9353 [D loss: 0.272692, acc.: 89.06%] [G loss: 3.086835]\n",
      "epoch:11 step:9354 [D loss: 0.309839, acc.: 85.94%] [G loss: 2.762527]\n",
      "epoch:11 step:9355 [D loss: 0.269543, acc.: 89.06%] [G loss: 2.480482]\n",
      "epoch:11 step:9356 [D loss: 0.368019, acc.: 82.81%] [G loss: 3.283360]\n",
      "epoch:11 step:9357 [D loss: 0.290002, acc.: 86.72%] [G loss: 3.635041]\n",
      "epoch:11 step:9358 [D loss: 0.360920, acc.: 82.03%] [G loss: 2.640491]\n",
      "epoch:11 step:9359 [D loss: 0.343361, acc.: 89.06%] [G loss: 2.402837]\n",
      "epoch:11 step:9360 [D loss: 0.233204, acc.: 89.84%] [G loss: 3.412795]\n",
      "epoch:11 step:9361 [D loss: 0.275937, acc.: 90.62%] [G loss: 3.044937]\n",
      "epoch:11 step:9362 [D loss: 0.362373, acc.: 84.38%] [G loss: 2.833477]\n",
      "epoch:11 step:9363 [D loss: 0.375732, acc.: 82.03%] [G loss: 2.834101]\n",
      "epoch:11 step:9364 [D loss: 0.335823, acc.: 86.72%] [G loss: 3.907804]\n",
      "epoch:11 step:9365 [D loss: 0.516095, acc.: 80.47%] [G loss: 5.461654]\n",
      "epoch:11 step:9366 [D loss: 1.189133, acc.: 64.06%] [G loss: 6.475642]\n",
      "epoch:11 step:9367 [D loss: 1.171121, acc.: 68.75%] [G loss: 2.540016]\n",
      "epoch:11 step:9368 [D loss: 0.579914, acc.: 75.78%] [G loss: 7.089672]\n",
      "epoch:11 step:9369 [D loss: 0.881592, acc.: 65.62%] [G loss: 2.555873]\n",
      "epoch:11 step:9370 [D loss: 0.565370, acc.: 78.12%] [G loss: 3.075621]\n",
      "epoch:11 step:9371 [D loss: 0.234908, acc.: 89.84%] [G loss: 3.866266]\n",
      "epoch:11 step:9372 [D loss: 0.314762, acc.: 88.28%] [G loss: 3.730315]\n",
      "epoch:12 step:9373 [D loss: 0.279081, acc.: 87.50%] [G loss: 3.938896]\n",
      "epoch:12 step:9374 [D loss: 0.313545, acc.: 87.50%] [G loss: 2.859813]\n",
      "epoch:12 step:9375 [D loss: 0.411678, acc.: 84.38%] [G loss: 2.639469]\n",
      "epoch:12 step:9376 [D loss: 0.300476, acc.: 88.28%] [G loss: 3.039689]\n",
      "epoch:12 step:9377 [D loss: 0.343902, acc.: 82.03%] [G loss: 2.962123]\n",
      "epoch:12 step:9378 [D loss: 0.243476, acc.: 92.19%] [G loss: 2.816380]\n",
      "epoch:12 step:9379 [D loss: 0.273720, acc.: 87.50%] [G loss: 3.036678]\n",
      "epoch:12 step:9380 [D loss: 0.226669, acc.: 95.31%] [G loss: 4.749207]\n",
      "epoch:12 step:9381 [D loss: 0.333473, acc.: 89.06%] [G loss: 3.428623]\n",
      "epoch:12 step:9382 [D loss: 0.259670, acc.: 89.06%] [G loss: 3.078683]\n",
      "epoch:12 step:9383 [D loss: 0.343868, acc.: 85.94%] [G loss: 2.870939]\n",
      "epoch:12 step:9384 [D loss: 0.272914, acc.: 86.72%] [G loss: 2.564788]\n",
      "epoch:12 step:9385 [D loss: 0.310326, acc.: 86.72%] [G loss: 2.211507]\n",
      "epoch:12 step:9386 [D loss: 0.218642, acc.: 92.19%] [G loss: 2.948038]\n",
      "epoch:12 step:9387 [D loss: 0.242620, acc.: 90.62%] [G loss: 5.499314]\n",
      "epoch:12 step:9388 [D loss: 0.253492, acc.: 92.19%] [G loss: 3.350812]\n",
      "epoch:12 step:9389 [D loss: 0.199577, acc.: 96.88%] [G loss: 2.815394]\n",
      "epoch:12 step:9390 [D loss: 0.243707, acc.: 93.75%] [G loss: 2.698319]\n",
      "epoch:12 step:9391 [D loss: 0.276813, acc.: 90.62%] [G loss: 2.308410]\n",
      "epoch:12 step:9392 [D loss: 0.276572, acc.: 91.41%] [G loss: 3.141382]\n",
      "epoch:12 step:9393 [D loss: 0.194995, acc.: 95.31%] [G loss: 3.940282]\n",
      "epoch:12 step:9394 [D loss: 0.340405, acc.: 89.84%] [G loss: 2.191096]\n",
      "epoch:12 step:9395 [D loss: 0.296619, acc.: 85.94%] [G loss: 2.318159]\n",
      "epoch:12 step:9396 [D loss: 0.314748, acc.: 85.94%] [G loss: 2.501585]\n",
      "epoch:12 step:9397 [D loss: 0.395757, acc.: 81.25%] [G loss: 2.304008]\n",
      "epoch:12 step:9398 [D loss: 0.272764, acc.: 92.19%] [G loss: 3.030166]\n",
      "epoch:12 step:9399 [D loss: 0.293958, acc.: 89.06%] [G loss: 3.216943]\n",
      "epoch:12 step:9400 [D loss: 0.383373, acc.: 85.16%] [G loss: 2.948385]\n",
      "##############\n",
      "[0.83525221 0.88470843 0.80884985 0.77759395 0.78396617 0.80916347\n",
      " 0.86139629 0.81644692 0.81625246 0.80787273]\n",
      "##########\n",
      "epoch:12 step:9401 [D loss: 0.308177, acc.: 87.50%] [G loss: 2.421959]\n",
      "epoch:12 step:9402 [D loss: 0.266424, acc.: 88.28%] [G loss: 2.522142]\n",
      "epoch:12 step:9403 [D loss: 0.340804, acc.: 85.16%] [G loss: 2.293033]\n",
      "epoch:12 step:9404 [D loss: 0.304047, acc.: 89.06%] [G loss: 1.876518]\n",
      "epoch:12 step:9405 [D loss: 0.358227, acc.: 84.38%] [G loss: 1.851185]\n",
      "epoch:12 step:9406 [D loss: 0.342315, acc.: 85.16%] [G loss: 2.347457]\n",
      "epoch:12 step:9407 [D loss: 0.277666, acc.: 89.84%] [G loss: 2.374240]\n",
      "epoch:12 step:9408 [D loss: 0.282210, acc.: 92.19%] [G loss: 2.553270]\n",
      "epoch:12 step:9409 [D loss: 0.325671, acc.: 86.72%] [G loss: 2.997190]\n",
      "epoch:12 step:9410 [D loss: 0.385765, acc.: 84.38%] [G loss: 2.847055]\n",
      "epoch:12 step:9411 [D loss: 0.259964, acc.: 90.62%] [G loss: 2.400038]\n",
      "epoch:12 step:9412 [D loss: 0.332119, acc.: 87.50%] [G loss: 2.151438]\n",
      "epoch:12 step:9413 [D loss: 0.418050, acc.: 82.81%] [G loss: 2.909395]\n",
      "epoch:12 step:9414 [D loss: 0.342827, acc.: 87.50%] [G loss: 2.375399]\n",
      "epoch:12 step:9415 [D loss: 0.304026, acc.: 88.28%] [G loss: 3.295650]\n",
      "epoch:12 step:9416 [D loss: 0.214713, acc.: 92.97%] [G loss: 4.210546]\n",
      "epoch:12 step:9417 [D loss: 0.287405, acc.: 85.94%] [G loss: 3.047674]\n",
      "epoch:12 step:9418 [D loss: 0.275854, acc.: 86.72%] [G loss: 2.907969]\n",
      "epoch:12 step:9419 [D loss: 0.369223, acc.: 81.25%] [G loss: 3.914070]\n",
      "epoch:12 step:9420 [D loss: 0.271812, acc.: 86.72%] [G loss: 2.577631]\n",
      "epoch:12 step:9421 [D loss: 0.204580, acc.: 92.97%] [G loss: 2.927508]\n",
      "epoch:12 step:9422 [D loss: 0.365906, acc.: 84.38%] [G loss: 2.401743]\n",
      "epoch:12 step:9423 [D loss: 0.254660, acc.: 86.72%] [G loss: 3.187642]\n",
      "epoch:12 step:9424 [D loss: 0.288371, acc.: 87.50%] [G loss: 2.900880]\n",
      "epoch:12 step:9425 [D loss: 0.355962, acc.: 85.16%] [G loss: 2.156696]\n",
      "epoch:12 step:9426 [D loss: 0.303721, acc.: 86.72%] [G loss: 2.480768]\n",
      "epoch:12 step:9427 [D loss: 0.300657, acc.: 87.50%] [G loss: 2.246665]\n",
      "epoch:12 step:9428 [D loss: 0.287516, acc.: 91.41%] [G loss: 2.960442]\n",
      "epoch:12 step:9429 [D loss: 0.325526, acc.: 82.81%] [G loss: 3.821654]\n",
      "epoch:12 step:9430 [D loss: 0.380987, acc.: 81.25%] [G loss: 2.227922]\n",
      "epoch:12 step:9431 [D loss: 0.298397, acc.: 87.50%] [G loss: 2.844772]\n",
      "epoch:12 step:9432 [D loss: 0.266799, acc.: 90.62%] [G loss: 3.386484]\n",
      "epoch:12 step:9433 [D loss: 0.278207, acc.: 90.62%] [G loss: 2.052767]\n",
      "epoch:12 step:9434 [D loss: 0.352086, acc.: 82.03%] [G loss: 2.524657]\n",
      "epoch:12 step:9435 [D loss: 0.261229, acc.: 89.06%] [G loss: 2.658795]\n",
      "epoch:12 step:9436 [D loss: 0.299695, acc.: 89.06%] [G loss: 2.836647]\n",
      "epoch:12 step:9437 [D loss: 0.306442, acc.: 87.50%] [G loss: 2.786486]\n",
      "epoch:12 step:9438 [D loss: 0.268056, acc.: 86.72%] [G loss: 3.425966]\n",
      "epoch:12 step:9439 [D loss: 0.351914, acc.: 86.72%] [G loss: 2.743289]\n",
      "epoch:12 step:9440 [D loss: 0.291259, acc.: 89.06%] [G loss: 2.605120]\n",
      "epoch:12 step:9441 [D loss: 0.266128, acc.: 88.28%] [G loss: 3.444065]\n",
      "epoch:12 step:9442 [D loss: 0.284562, acc.: 89.06%] [G loss: 3.000614]\n",
      "epoch:12 step:9443 [D loss: 0.390004, acc.: 85.16%] [G loss: 3.039120]\n",
      "epoch:12 step:9444 [D loss: 0.349348, acc.: 87.50%] [G loss: 3.208014]\n",
      "epoch:12 step:9445 [D loss: 0.379756, acc.: 84.38%] [G loss: 3.157608]\n",
      "epoch:12 step:9446 [D loss: 0.309800, acc.: 83.59%] [G loss: 2.174593]\n",
      "epoch:12 step:9447 [D loss: 0.316650, acc.: 85.16%] [G loss: 2.115891]\n",
      "epoch:12 step:9448 [D loss: 0.414203, acc.: 79.69%] [G loss: 2.196868]\n",
      "epoch:12 step:9449 [D loss: 0.466871, acc.: 78.12%] [G loss: 2.322992]\n",
      "epoch:12 step:9450 [D loss: 0.326891, acc.: 89.06%] [G loss: 2.724998]\n",
      "epoch:12 step:9451 [D loss: 0.425341, acc.: 81.25%] [G loss: 2.906939]\n",
      "epoch:12 step:9452 [D loss: 0.245381, acc.: 90.62%] [G loss: 4.030774]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:12 step:9453 [D loss: 0.301470, acc.: 89.06%] [G loss: 4.182206]\n",
      "epoch:12 step:9454 [D loss: 0.329836, acc.: 83.59%] [G loss: 2.966915]\n",
      "epoch:12 step:9455 [D loss: 0.264991, acc.: 85.94%] [G loss: 3.819970]\n",
      "epoch:12 step:9456 [D loss: 0.343150, acc.: 88.28%] [G loss: 2.313822]\n",
      "epoch:12 step:9457 [D loss: 0.303607, acc.: 87.50%] [G loss: 2.983125]\n",
      "epoch:12 step:9458 [D loss: 0.226880, acc.: 91.41%] [G loss: 3.988614]\n",
      "epoch:12 step:9459 [D loss: 0.243557, acc.: 90.62%] [G loss: 3.276117]\n",
      "epoch:12 step:9460 [D loss: 0.249557, acc.: 88.28%] [G loss: 2.909734]\n",
      "epoch:12 step:9461 [D loss: 0.330878, acc.: 87.50%] [G loss: 2.385646]\n",
      "epoch:12 step:9462 [D loss: 0.346903, acc.: 85.16%] [G loss: 2.049447]\n",
      "epoch:12 step:9463 [D loss: 0.379174, acc.: 84.38%] [G loss: 2.745261]\n",
      "epoch:12 step:9464 [D loss: 0.285088, acc.: 86.72%] [G loss: 2.839217]\n",
      "epoch:12 step:9465 [D loss: 0.392327, acc.: 86.72%] [G loss: 3.341546]\n",
      "epoch:12 step:9466 [D loss: 0.279043, acc.: 89.06%] [G loss: 4.242808]\n",
      "epoch:12 step:9467 [D loss: 0.342551, acc.: 84.38%] [G loss: 3.273217]\n",
      "epoch:12 step:9468 [D loss: 0.283614, acc.: 83.59%] [G loss: 2.664871]\n",
      "epoch:12 step:9469 [D loss: 0.373527, acc.: 81.25%] [G loss: 4.682758]\n",
      "epoch:12 step:9470 [D loss: 0.464384, acc.: 77.34%] [G loss: 4.780556]\n",
      "epoch:12 step:9471 [D loss: 0.296928, acc.: 86.72%] [G loss: 3.845992]\n",
      "epoch:12 step:9472 [D loss: 0.381615, acc.: 80.47%] [G loss: 2.527802]\n",
      "epoch:12 step:9473 [D loss: 0.338757, acc.: 82.03%] [G loss: 4.437281]\n",
      "epoch:12 step:9474 [D loss: 0.384357, acc.: 81.25%] [G loss: 4.064859]\n",
      "epoch:12 step:9475 [D loss: 0.493659, acc.: 77.34%] [G loss: 4.288478]\n",
      "epoch:12 step:9476 [D loss: 0.717296, acc.: 68.75%] [G loss: 4.827252]\n",
      "epoch:12 step:9477 [D loss: 0.894398, acc.: 72.66%] [G loss: 3.689872]\n",
      "epoch:12 step:9478 [D loss: 0.492885, acc.: 73.44%] [G loss: 3.224313]\n",
      "epoch:12 step:9479 [D loss: 0.279631, acc.: 89.84%] [G loss: 2.754794]\n",
      "epoch:12 step:9480 [D loss: 0.338014, acc.: 87.50%] [G loss: 3.343049]\n",
      "epoch:12 step:9481 [D loss: 0.325195, acc.: 82.81%] [G loss: 2.560769]\n",
      "epoch:12 step:9482 [D loss: 0.360432, acc.: 86.72%] [G loss: 2.397898]\n",
      "epoch:12 step:9483 [D loss: 0.203424, acc.: 91.41%] [G loss: 2.535097]\n",
      "epoch:12 step:9484 [D loss: 0.339311, acc.: 87.50%] [G loss: 3.199028]\n",
      "epoch:12 step:9485 [D loss: 0.300062, acc.: 86.72%] [G loss: 2.231267]\n",
      "epoch:12 step:9486 [D loss: 0.326930, acc.: 86.72%] [G loss: 2.490046]\n",
      "epoch:12 step:9487 [D loss: 0.338963, acc.: 87.50%] [G loss: 2.614752]\n",
      "epoch:12 step:9488 [D loss: 0.445473, acc.: 83.59%] [G loss: 2.351778]\n",
      "epoch:12 step:9489 [D loss: 0.223289, acc.: 93.75%] [G loss: 3.019411]\n",
      "epoch:12 step:9490 [D loss: 0.283725, acc.: 85.16%] [G loss: 3.320615]\n",
      "epoch:12 step:9491 [D loss: 0.225345, acc.: 91.41%] [G loss: 3.162427]\n",
      "epoch:12 step:9492 [D loss: 0.269928, acc.: 91.41%] [G loss: 2.243404]\n",
      "epoch:12 step:9493 [D loss: 0.312581, acc.: 87.50%] [G loss: 2.352094]\n",
      "epoch:12 step:9494 [D loss: 0.309760, acc.: 83.59%] [G loss: 2.323297]\n",
      "epoch:12 step:9495 [D loss: 0.303958, acc.: 87.50%] [G loss: 3.754981]\n",
      "epoch:12 step:9496 [D loss: 0.440388, acc.: 85.16%] [G loss: 2.871097]\n",
      "epoch:12 step:9497 [D loss: 0.384956, acc.: 80.47%] [G loss: 2.569175]\n",
      "epoch:12 step:9498 [D loss: 0.365457, acc.: 88.28%] [G loss: 3.111550]\n",
      "epoch:12 step:9499 [D loss: 0.207855, acc.: 92.19%] [G loss: 3.475012]\n",
      "epoch:12 step:9500 [D loss: 0.188808, acc.: 92.97%] [G loss: 4.683559]\n",
      "epoch:12 step:9501 [D loss: 0.331315, acc.: 84.38%] [G loss: 5.117189]\n",
      "epoch:12 step:9502 [D loss: 0.336218, acc.: 81.25%] [G loss: 3.374940]\n",
      "epoch:12 step:9503 [D loss: 0.267420, acc.: 91.41%] [G loss: 3.693972]\n",
      "epoch:12 step:9504 [D loss: 0.430102, acc.: 78.91%] [G loss: 2.737420]\n",
      "epoch:12 step:9505 [D loss: 0.259940, acc.: 91.41%] [G loss: 3.108257]\n",
      "epoch:12 step:9506 [D loss: 0.283182, acc.: 87.50%] [G loss: 3.104876]\n",
      "epoch:12 step:9507 [D loss: 0.318153, acc.: 85.16%] [G loss: 3.327870]\n",
      "epoch:12 step:9508 [D loss: 0.223915, acc.: 88.28%] [G loss: 2.973750]\n",
      "epoch:12 step:9509 [D loss: 0.340785, acc.: 89.84%] [G loss: 2.160545]\n",
      "epoch:12 step:9510 [D loss: 0.335436, acc.: 90.62%] [G loss: 2.104409]\n",
      "epoch:12 step:9511 [D loss: 0.332765, acc.: 85.16%] [G loss: 2.417531]\n",
      "epoch:12 step:9512 [D loss: 0.331234, acc.: 88.28%] [G loss: 2.893821]\n",
      "epoch:12 step:9513 [D loss: 0.269192, acc.: 89.84%] [G loss: 3.751537]\n",
      "epoch:12 step:9514 [D loss: 0.358521, acc.: 85.16%] [G loss: 3.262045]\n",
      "epoch:12 step:9515 [D loss: 0.238913, acc.: 92.19%] [G loss: 3.627240]\n",
      "epoch:12 step:9516 [D loss: 0.457441, acc.: 81.25%] [G loss: 4.173339]\n",
      "epoch:12 step:9517 [D loss: 0.264117, acc.: 89.06%] [G loss: 2.617293]\n",
      "epoch:12 step:9518 [D loss: 0.274781, acc.: 89.06%] [G loss: 4.645407]\n",
      "epoch:12 step:9519 [D loss: 0.353105, acc.: 82.81%] [G loss: 3.974569]\n",
      "epoch:12 step:9520 [D loss: 0.398767, acc.: 84.38%] [G loss: 4.451083]\n",
      "epoch:12 step:9521 [D loss: 0.407624, acc.: 81.25%] [G loss: 2.679687]\n",
      "epoch:12 step:9522 [D loss: 0.207836, acc.: 91.41%] [G loss: 4.176215]\n",
      "epoch:12 step:9523 [D loss: 0.328549, acc.: 86.72%] [G loss: 3.203054]\n",
      "epoch:12 step:9524 [D loss: 0.362118, acc.: 82.03%] [G loss: 2.567237]\n",
      "epoch:12 step:9525 [D loss: 0.299599, acc.: 89.06%] [G loss: 2.410630]\n",
      "epoch:12 step:9526 [D loss: 0.329465, acc.: 86.72%] [G loss: 2.818602]\n",
      "epoch:12 step:9527 [D loss: 0.374950, acc.: 86.72%] [G loss: 3.124103]\n",
      "epoch:12 step:9528 [D loss: 0.264948, acc.: 89.06%] [G loss: 3.799860]\n",
      "epoch:12 step:9529 [D loss: 0.309931, acc.: 84.38%] [G loss: 3.088915]\n",
      "epoch:12 step:9530 [D loss: 0.335337, acc.: 85.16%] [G loss: 2.736981]\n",
      "epoch:12 step:9531 [D loss: 0.307997, acc.: 88.28%] [G loss: 2.559044]\n",
      "epoch:12 step:9532 [D loss: 0.352574, acc.: 81.25%] [G loss: 3.326571]\n",
      "epoch:12 step:9533 [D loss: 0.227545, acc.: 89.84%] [G loss: 3.218760]\n",
      "epoch:12 step:9534 [D loss: 0.225023, acc.: 92.97%] [G loss: 4.359689]\n",
      "epoch:12 step:9535 [D loss: 0.415970, acc.: 82.03%] [G loss: 3.623597]\n",
      "epoch:12 step:9536 [D loss: 0.217289, acc.: 89.84%] [G loss: 4.471352]\n",
      "epoch:12 step:9537 [D loss: 0.308842, acc.: 86.72%] [G loss: 2.691696]\n",
      "epoch:12 step:9538 [D loss: 0.362964, acc.: 82.81%] [G loss: 3.133327]\n",
      "epoch:12 step:9539 [D loss: 0.351557, acc.: 82.81%] [G loss: 2.808666]\n",
      "epoch:12 step:9540 [D loss: 0.366489, acc.: 85.94%] [G loss: 3.228426]\n",
      "epoch:12 step:9541 [D loss: 0.309825, acc.: 86.72%] [G loss: 2.788446]\n",
      "epoch:12 step:9542 [D loss: 0.266096, acc.: 92.19%] [G loss: 2.880942]\n",
      "epoch:12 step:9543 [D loss: 0.316302, acc.: 83.59%] [G loss: 2.777783]\n",
      "epoch:12 step:9544 [D loss: 0.360890, acc.: 86.72%] [G loss: 2.729154]\n",
      "epoch:12 step:9545 [D loss: 0.239482, acc.: 88.28%] [G loss: 4.862124]\n",
      "epoch:12 step:9546 [D loss: 0.245445, acc.: 89.06%] [G loss: 4.923420]\n",
      "epoch:12 step:9547 [D loss: 0.279650, acc.: 89.06%] [G loss: 2.181423]\n",
      "epoch:12 step:9548 [D loss: 0.234207, acc.: 92.97%] [G loss: 2.916483]\n",
      "epoch:12 step:9549 [D loss: 0.223639, acc.: 90.62%] [G loss: 3.714774]\n",
      "epoch:12 step:9550 [D loss: 0.322436, acc.: 87.50%] [G loss: 2.956989]\n",
      "epoch:12 step:9551 [D loss: 0.371343, acc.: 85.16%] [G loss: 2.463984]\n",
      "epoch:12 step:9552 [D loss: 0.334858, acc.: 84.38%] [G loss: 2.867707]\n",
      "epoch:12 step:9553 [D loss: 0.331422, acc.: 85.94%] [G loss: 2.323592]\n",
      "epoch:12 step:9554 [D loss: 0.294780, acc.: 85.94%] [G loss: 3.669948]\n",
      "epoch:12 step:9555 [D loss: 0.281680, acc.: 89.06%] [G loss: 3.669599]\n",
      "epoch:12 step:9556 [D loss: 0.279154, acc.: 89.06%] [G loss: 3.927844]\n",
      "epoch:12 step:9557 [D loss: 0.286453, acc.: 89.06%] [G loss: 3.876878]\n",
      "epoch:12 step:9558 [D loss: 0.259495, acc.: 87.50%] [G loss: 4.254654]\n",
      "epoch:12 step:9559 [D loss: 0.260791, acc.: 90.62%] [G loss: 4.295735]\n",
      "epoch:12 step:9560 [D loss: 0.415942, acc.: 84.38%] [G loss: 2.441406]\n",
      "epoch:12 step:9561 [D loss: 0.299030, acc.: 86.72%] [G loss: 3.184746]\n",
      "epoch:12 step:9562 [D loss: 0.381697, acc.: 84.38%] [G loss: 3.408398]\n",
      "epoch:12 step:9563 [D loss: 0.466960, acc.: 82.81%] [G loss: 2.427431]\n",
      "epoch:12 step:9564 [D loss: 0.241604, acc.: 92.97%] [G loss: 3.161806]\n",
      "epoch:12 step:9565 [D loss: 0.357518, acc.: 87.50%] [G loss: 4.085664]\n",
      "epoch:12 step:9566 [D loss: 0.287091, acc.: 85.94%] [G loss: 4.702927]\n",
      "epoch:12 step:9567 [D loss: 0.300621, acc.: 86.72%] [G loss: 2.585135]\n",
      "epoch:12 step:9568 [D loss: 0.259367, acc.: 92.97%] [G loss: 3.324306]\n",
      "epoch:12 step:9569 [D loss: 0.238252, acc.: 90.62%] [G loss: 3.598794]\n",
      "epoch:12 step:9570 [D loss: 0.261362, acc.: 92.97%] [G loss: 3.310264]\n",
      "epoch:12 step:9571 [D loss: 0.263887, acc.: 92.19%] [G loss: 2.575901]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:12 step:9572 [D loss: 0.349874, acc.: 89.06%] [G loss: 3.596889]\n",
      "epoch:12 step:9573 [D loss: 0.280575, acc.: 87.50%] [G loss: 2.947247]\n",
      "epoch:12 step:9574 [D loss: 0.326909, acc.: 86.72%] [G loss: 3.109174]\n",
      "epoch:12 step:9575 [D loss: 0.259757, acc.: 89.84%] [G loss: 2.750957]\n",
      "epoch:12 step:9576 [D loss: 0.300537, acc.: 86.72%] [G loss: 2.743077]\n",
      "epoch:12 step:9577 [D loss: 0.355003, acc.: 87.50%] [G loss: 2.751714]\n",
      "epoch:12 step:9578 [D loss: 0.243015, acc.: 86.72%] [G loss: 3.836289]\n",
      "epoch:12 step:9579 [D loss: 0.267060, acc.: 88.28%] [G loss: 3.845531]\n",
      "epoch:12 step:9580 [D loss: 0.321666, acc.: 85.94%] [G loss: 4.005302]\n",
      "epoch:12 step:9581 [D loss: 0.362950, acc.: 85.16%] [G loss: 2.656140]\n",
      "epoch:12 step:9582 [D loss: 0.229326, acc.: 91.41%] [G loss: 4.454261]\n",
      "epoch:12 step:9583 [D loss: 0.283507, acc.: 86.72%] [G loss: 3.799073]\n",
      "epoch:12 step:9584 [D loss: 0.244396, acc.: 90.62%] [G loss: 3.511326]\n",
      "epoch:12 step:9585 [D loss: 0.307877, acc.: 89.06%] [G loss: 3.521444]\n",
      "epoch:12 step:9586 [D loss: 0.339534, acc.: 86.72%] [G loss: 3.979007]\n",
      "epoch:12 step:9587 [D loss: 0.287303, acc.: 88.28%] [G loss: 4.091156]\n",
      "epoch:12 step:9588 [D loss: 0.291923, acc.: 87.50%] [G loss: 3.470205]\n",
      "epoch:12 step:9589 [D loss: 0.407882, acc.: 77.34%] [G loss: 2.809989]\n",
      "epoch:12 step:9590 [D loss: 0.231791, acc.: 87.50%] [G loss: 4.170293]\n",
      "epoch:12 step:9591 [D loss: 0.342510, acc.: 84.38%] [G loss: 4.122818]\n",
      "epoch:12 step:9592 [D loss: 0.444890, acc.: 83.59%] [G loss: 4.073768]\n",
      "epoch:12 step:9593 [D loss: 0.235622, acc.: 89.84%] [G loss: 3.297531]\n",
      "epoch:12 step:9594 [D loss: 0.246481, acc.: 90.62%] [G loss: 3.256005]\n",
      "epoch:12 step:9595 [D loss: 0.334895, acc.: 88.28%] [G loss: 2.719268]\n",
      "epoch:12 step:9596 [D loss: 0.469224, acc.: 78.12%] [G loss: 3.495510]\n",
      "epoch:12 step:9597 [D loss: 0.335917, acc.: 85.94%] [G loss: 5.718012]\n",
      "epoch:12 step:9598 [D loss: 0.344594, acc.: 85.16%] [G loss: 3.734501]\n",
      "epoch:12 step:9599 [D loss: 0.303378, acc.: 84.38%] [G loss: 2.894401]\n",
      "epoch:12 step:9600 [D loss: 0.396183, acc.: 82.03%] [G loss: 4.411040]\n",
      "##############\n",
      "[0.86157071 0.86060185 0.7792502  0.79892274 0.80054697 0.81997685\n",
      " 0.87103291 0.8349327  0.80642666 0.8029439 ]\n",
      "##########\n",
      "epoch:12 step:9601 [D loss: 0.686280, acc.: 72.66%] [G loss: 5.176737]\n",
      "epoch:12 step:9602 [D loss: 0.743362, acc.: 67.19%] [G loss: 3.405538]\n",
      "epoch:12 step:9603 [D loss: 0.509529, acc.: 75.00%] [G loss: 2.926509]\n",
      "epoch:12 step:9604 [D loss: 0.358262, acc.: 82.81%] [G loss: 3.574927]\n",
      "epoch:12 step:9605 [D loss: 0.283920, acc.: 85.16%] [G loss: 3.079801]\n",
      "epoch:12 step:9606 [D loss: 0.276234, acc.: 86.72%] [G loss: 4.403126]\n",
      "epoch:12 step:9607 [D loss: 0.246073, acc.: 89.84%] [G loss: 3.410969]\n",
      "epoch:12 step:9608 [D loss: 0.255129, acc.: 85.94%] [G loss: 4.417396]\n",
      "epoch:12 step:9609 [D loss: 0.318052, acc.: 85.16%] [G loss: 1.922550]\n",
      "epoch:12 step:9610 [D loss: 0.301656, acc.: 85.16%] [G loss: 3.823197]\n",
      "epoch:12 step:9611 [D loss: 0.233622, acc.: 91.41%] [G loss: 4.145521]\n",
      "epoch:12 step:9612 [D loss: 0.245879, acc.: 89.84%] [G loss: 3.955247]\n",
      "epoch:12 step:9613 [D loss: 0.387745, acc.: 78.91%] [G loss: 3.759368]\n",
      "epoch:12 step:9614 [D loss: 0.252968, acc.: 89.06%] [G loss: 6.042905]\n",
      "epoch:12 step:9615 [D loss: 0.369868, acc.: 77.34%] [G loss: 4.154748]\n",
      "epoch:12 step:9616 [D loss: 0.378462, acc.: 82.03%] [G loss: 3.335803]\n",
      "epoch:12 step:9617 [D loss: 0.409993, acc.: 83.59%] [G loss: 3.032730]\n",
      "epoch:12 step:9618 [D loss: 0.299615, acc.: 89.84%] [G loss: 3.281489]\n",
      "epoch:12 step:9619 [D loss: 0.354285, acc.: 85.16%] [G loss: 3.884851]\n",
      "epoch:12 step:9620 [D loss: 0.318765, acc.: 84.38%] [G loss: 3.024277]\n",
      "epoch:12 step:9621 [D loss: 0.374957, acc.: 80.47%] [G loss: 4.127246]\n",
      "epoch:12 step:9622 [D loss: 0.410716, acc.: 81.25%] [G loss: 2.132530]\n",
      "epoch:12 step:9623 [D loss: 0.243546, acc.: 92.19%] [G loss: 2.735741]\n",
      "epoch:12 step:9624 [D loss: 0.325948, acc.: 85.16%] [G loss: 2.697529]\n",
      "epoch:12 step:9625 [D loss: 0.302689, acc.: 92.97%] [G loss: 2.988257]\n",
      "epoch:12 step:9626 [D loss: 0.343943, acc.: 84.38%] [G loss: 3.189710]\n",
      "epoch:12 step:9627 [D loss: 0.465984, acc.: 80.47%] [G loss: 3.209952]\n",
      "epoch:12 step:9628 [D loss: 0.343275, acc.: 82.81%] [G loss: 2.492892]\n",
      "epoch:12 step:9629 [D loss: 0.386099, acc.: 82.03%] [G loss: 4.598961]\n",
      "epoch:12 step:9630 [D loss: 0.442301, acc.: 81.25%] [G loss: 4.296261]\n",
      "epoch:12 step:9631 [D loss: 0.353471, acc.: 85.16%] [G loss: 2.212989]\n",
      "epoch:12 step:9632 [D loss: 0.398259, acc.: 84.38%] [G loss: 3.209783]\n",
      "epoch:12 step:9633 [D loss: 0.460679, acc.: 80.47%] [G loss: 3.674469]\n",
      "epoch:12 step:9634 [D loss: 0.396410, acc.: 82.81%] [G loss: 2.926407]\n",
      "epoch:12 step:9635 [D loss: 0.333491, acc.: 89.84%] [G loss: 2.783843]\n",
      "epoch:12 step:9636 [D loss: 0.317174, acc.: 87.50%] [G loss: 3.127668]\n",
      "epoch:12 step:9637 [D loss: 0.238300, acc.: 88.28%] [G loss: 3.999477]\n",
      "epoch:12 step:9638 [D loss: 0.286330, acc.: 89.06%] [G loss: 3.067356]\n",
      "epoch:12 step:9639 [D loss: 0.435659, acc.: 82.81%] [G loss: 4.511226]\n",
      "epoch:12 step:9640 [D loss: 0.436730, acc.: 75.00%] [G loss: 2.534008]\n",
      "epoch:12 step:9641 [D loss: 0.517174, acc.: 75.78%] [G loss: 4.164763]\n",
      "epoch:12 step:9642 [D loss: 0.530623, acc.: 72.66%] [G loss: 3.582625]\n",
      "epoch:12 step:9643 [D loss: 0.493720, acc.: 79.69%] [G loss: 2.373724]\n",
      "epoch:12 step:9644 [D loss: 0.330675, acc.: 86.72%] [G loss: 3.020332]\n",
      "epoch:12 step:9645 [D loss: 0.415861, acc.: 82.81%] [G loss: 2.914388]\n",
      "epoch:12 step:9646 [D loss: 0.429120, acc.: 80.47%] [G loss: 3.194640]\n",
      "epoch:12 step:9647 [D loss: 0.583343, acc.: 71.88%] [G loss: 2.981761]\n",
      "epoch:12 step:9648 [D loss: 0.397577, acc.: 84.38%] [G loss: 2.975294]\n",
      "epoch:12 step:9649 [D loss: 0.326075, acc.: 86.72%] [G loss: 4.039091]\n",
      "epoch:12 step:9650 [D loss: 0.233567, acc.: 95.31%] [G loss: 4.442499]\n",
      "epoch:12 step:9651 [D loss: 0.285674, acc.: 86.72%] [G loss: 2.720994]\n",
      "epoch:12 step:9652 [D loss: 0.227814, acc.: 90.62%] [G loss: 4.499839]\n",
      "epoch:12 step:9653 [D loss: 0.221074, acc.: 90.62%] [G loss: 3.806247]\n",
      "epoch:12 step:9654 [D loss: 0.431164, acc.: 79.69%] [G loss: 1.901459]\n",
      "epoch:12 step:9655 [D loss: 0.233826, acc.: 89.06%] [G loss: 3.332013]\n",
      "epoch:12 step:9656 [D loss: 0.344093, acc.: 82.03%] [G loss: 4.784481]\n",
      "epoch:12 step:9657 [D loss: 0.336451, acc.: 85.16%] [G loss: 2.573835]\n",
      "epoch:12 step:9658 [D loss: 0.362192, acc.: 82.81%] [G loss: 1.984123]\n",
      "epoch:12 step:9659 [D loss: 0.359577, acc.: 84.38%] [G loss: 1.973281]\n",
      "epoch:12 step:9660 [D loss: 0.455492, acc.: 76.56%] [G loss: 2.411617]\n",
      "epoch:12 step:9661 [D loss: 0.340714, acc.: 87.50%] [G loss: 2.455172]\n",
      "epoch:12 step:9662 [D loss: 0.333390, acc.: 86.72%] [G loss: 3.202661]\n",
      "epoch:12 step:9663 [D loss: 0.265879, acc.: 87.50%] [G loss: 4.750912]\n",
      "epoch:12 step:9664 [D loss: 0.322203, acc.: 86.72%] [G loss: 3.949625]\n",
      "epoch:12 step:9665 [D loss: 0.369016, acc.: 83.59%] [G loss: 2.677618]\n",
      "epoch:12 step:9666 [D loss: 0.358776, acc.: 88.28%] [G loss: 3.102554]\n",
      "epoch:12 step:9667 [D loss: 0.251432, acc.: 89.06%] [G loss: 3.405400]\n",
      "epoch:12 step:9668 [D loss: 0.391596, acc.: 78.91%] [G loss: 3.744480]\n",
      "epoch:12 step:9669 [D loss: 0.265683, acc.: 90.62%] [G loss: 4.847494]\n",
      "epoch:12 step:9670 [D loss: 0.349892, acc.: 84.38%] [G loss: 2.364640]\n",
      "epoch:12 step:9671 [D loss: 0.297955, acc.: 85.16%] [G loss: 2.903015]\n",
      "epoch:12 step:9672 [D loss: 0.365997, acc.: 84.38%] [G loss: 3.071308]\n",
      "epoch:12 step:9673 [D loss: 0.201644, acc.: 91.41%] [G loss: 3.577155]\n",
      "epoch:12 step:9674 [D loss: 0.276349, acc.: 89.06%] [G loss: 3.182376]\n",
      "epoch:12 step:9675 [D loss: 0.416023, acc.: 78.91%] [G loss: 2.441028]\n",
      "epoch:12 step:9676 [D loss: 0.239576, acc.: 90.62%] [G loss: 3.506402]\n",
      "epoch:12 step:9677 [D loss: 0.248218, acc.: 89.06%] [G loss: 3.425265]\n",
      "epoch:12 step:9678 [D loss: 0.313810, acc.: 81.25%] [G loss: 2.840762]\n",
      "epoch:12 step:9679 [D loss: 0.413865, acc.: 83.59%] [G loss: 2.889422]\n",
      "epoch:12 step:9680 [D loss: 0.269771, acc.: 89.06%] [G loss: 3.321145]\n",
      "epoch:12 step:9681 [D loss: 0.313285, acc.: 85.16%] [G loss: 2.915443]\n",
      "epoch:12 step:9682 [D loss: 0.325058, acc.: 89.84%] [G loss: 3.005774]\n",
      "epoch:12 step:9683 [D loss: 0.480975, acc.: 82.03%] [G loss: 3.772965]\n",
      "epoch:12 step:9684 [D loss: 0.437430, acc.: 82.81%] [G loss: 4.673554]\n",
      "epoch:12 step:9685 [D loss: 0.362183, acc.: 84.38%] [G loss: 3.673780]\n",
      "epoch:12 step:9686 [D loss: 0.341177, acc.: 84.38%] [G loss: 3.731425]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:12 step:9687 [D loss: 0.573120, acc.: 68.75%] [G loss: 2.154438]\n",
      "epoch:12 step:9688 [D loss: 0.250311, acc.: 92.97%] [G loss: 2.816073]\n",
      "epoch:12 step:9689 [D loss: 0.451115, acc.: 80.47%] [G loss: 3.359435]\n",
      "epoch:12 step:9690 [D loss: 0.615928, acc.: 76.56%] [G loss: 8.337406]\n",
      "epoch:12 step:9691 [D loss: 0.689840, acc.: 71.09%] [G loss: 6.059064]\n",
      "epoch:12 step:9692 [D loss: 0.826174, acc.: 72.66%] [G loss: 3.160854]\n",
      "epoch:12 step:9693 [D loss: 0.379428, acc.: 82.81%] [G loss: 3.431261]\n",
      "epoch:12 step:9694 [D loss: 0.538684, acc.: 75.78%] [G loss: 2.923965]\n",
      "epoch:12 step:9695 [D loss: 0.635921, acc.: 72.66%] [G loss: 5.169856]\n",
      "epoch:12 step:9696 [D loss: 0.445064, acc.: 82.81%] [G loss: 2.547168]\n",
      "epoch:12 step:9697 [D loss: 0.436937, acc.: 80.47%] [G loss: 2.897516]\n",
      "epoch:12 step:9698 [D loss: 0.472581, acc.: 78.12%] [G loss: 3.011903]\n",
      "epoch:12 step:9699 [D loss: 0.293386, acc.: 88.28%] [G loss: 3.380937]\n",
      "epoch:12 step:9700 [D loss: 0.336024, acc.: 87.50%] [G loss: 2.272822]\n",
      "epoch:12 step:9701 [D loss: 0.364368, acc.: 85.16%] [G loss: 3.465359]\n",
      "epoch:12 step:9702 [D loss: 0.355469, acc.: 82.81%] [G loss: 2.872157]\n",
      "epoch:12 step:9703 [D loss: 0.244056, acc.: 92.97%] [G loss: 3.384812]\n",
      "epoch:12 step:9704 [D loss: 0.377593, acc.: 86.72%] [G loss: 2.702135]\n",
      "epoch:12 step:9705 [D loss: 0.313010, acc.: 88.28%] [G loss: 2.519839]\n",
      "epoch:12 step:9706 [D loss: 0.378862, acc.: 82.03%] [G loss: 2.773674]\n",
      "epoch:12 step:9707 [D loss: 0.333077, acc.: 82.81%] [G loss: 4.783753]\n",
      "epoch:12 step:9708 [D loss: 0.260364, acc.: 89.06%] [G loss: 4.757978]\n",
      "epoch:12 step:9709 [D loss: 0.253081, acc.: 91.41%] [G loss: 2.732746]\n",
      "epoch:12 step:9710 [D loss: 0.375566, acc.: 82.81%] [G loss: 2.615774]\n",
      "epoch:12 step:9711 [D loss: 0.285362, acc.: 89.06%] [G loss: 2.996913]\n",
      "epoch:12 step:9712 [D loss: 0.286339, acc.: 86.72%] [G loss: 3.180107]\n",
      "epoch:12 step:9713 [D loss: 0.287032, acc.: 89.84%] [G loss: 3.599808]\n",
      "epoch:12 step:9714 [D loss: 0.433916, acc.: 78.12%] [G loss: 3.350399]\n",
      "epoch:12 step:9715 [D loss: 0.277440, acc.: 90.62%] [G loss: 4.223541]\n",
      "epoch:12 step:9716 [D loss: 0.247396, acc.: 91.41%] [G loss: 4.282892]\n",
      "epoch:12 step:9717 [D loss: 0.236436, acc.: 92.19%] [G loss: 4.265042]\n",
      "epoch:12 step:9718 [D loss: 0.301637, acc.: 85.94%] [G loss: 4.295352]\n",
      "epoch:12 step:9719 [D loss: 0.268740, acc.: 89.06%] [G loss: 2.641354]\n",
      "epoch:12 step:9720 [D loss: 0.361861, acc.: 82.81%] [G loss: 2.397621]\n",
      "epoch:12 step:9721 [D loss: 0.293929, acc.: 85.16%] [G loss: 3.143057]\n",
      "epoch:12 step:9722 [D loss: 0.263424, acc.: 86.72%] [G loss: 3.101007]\n",
      "epoch:12 step:9723 [D loss: 0.304815, acc.: 88.28%] [G loss: 3.914904]\n",
      "epoch:12 step:9724 [D loss: 0.373278, acc.: 83.59%] [G loss: 2.456666]\n",
      "epoch:12 step:9725 [D loss: 0.359442, acc.: 84.38%] [G loss: 3.269572]\n",
      "epoch:12 step:9726 [D loss: 0.220888, acc.: 92.19%] [G loss: 4.504066]\n",
      "epoch:12 step:9727 [D loss: 0.339730, acc.: 82.81%] [G loss: 2.673604]\n",
      "epoch:12 step:9728 [D loss: 0.308721, acc.: 87.50%] [G loss: 2.684955]\n",
      "epoch:12 step:9729 [D loss: 0.279016, acc.: 88.28%] [G loss: 3.900883]\n",
      "epoch:12 step:9730 [D loss: 0.292003, acc.: 86.72%] [G loss: 3.484844]\n",
      "epoch:12 step:9731 [D loss: 0.392256, acc.: 78.91%] [G loss: 2.563315]\n",
      "epoch:12 step:9732 [D loss: 0.342940, acc.: 83.59%] [G loss: 3.050927]\n",
      "epoch:12 step:9733 [D loss: 0.294482, acc.: 85.94%] [G loss: 4.589145]\n",
      "epoch:12 step:9734 [D loss: 0.270998, acc.: 89.06%] [G loss: 3.928619]\n",
      "epoch:12 step:9735 [D loss: 0.333539, acc.: 87.50%] [G loss: 2.856706]\n",
      "epoch:12 step:9736 [D loss: 0.186123, acc.: 91.41%] [G loss: 4.304955]\n",
      "epoch:12 step:9737 [D loss: 0.332562, acc.: 85.94%] [G loss: 2.860887]\n",
      "epoch:12 step:9738 [D loss: 0.374565, acc.: 80.47%] [G loss: 3.019207]\n",
      "epoch:12 step:9739 [D loss: 0.253183, acc.: 89.06%] [G loss: 3.559985]\n",
      "epoch:12 step:9740 [D loss: 0.350923, acc.: 86.72%] [G loss: 3.741206]\n",
      "epoch:12 step:9741 [D loss: 0.405278, acc.: 85.16%] [G loss: 2.701056]\n",
      "epoch:12 step:9742 [D loss: 0.342538, acc.: 85.16%] [G loss: 2.744657]\n",
      "epoch:12 step:9743 [D loss: 0.299636, acc.: 87.50%] [G loss: 3.428741]\n",
      "epoch:12 step:9744 [D loss: 0.273722, acc.: 86.72%] [G loss: 4.456294]\n",
      "epoch:12 step:9745 [D loss: 0.241612, acc.: 92.97%] [G loss: 4.119790]\n",
      "epoch:12 step:9746 [D loss: 0.312214, acc.: 88.28%] [G loss: 3.147579]\n",
      "epoch:12 step:9747 [D loss: 0.239204, acc.: 90.62%] [G loss: 3.811814]\n",
      "epoch:12 step:9748 [D loss: 0.282087, acc.: 83.59%] [G loss: 4.298950]\n",
      "epoch:12 step:9749 [D loss: 0.306113, acc.: 89.06%] [G loss: 3.412759]\n",
      "epoch:12 step:9750 [D loss: 0.212847, acc.: 92.19%] [G loss: 3.629980]\n",
      "epoch:12 step:9751 [D loss: 0.325345, acc.: 86.72%] [G loss: 2.677148]\n",
      "epoch:12 step:9752 [D loss: 0.273557, acc.: 86.72%] [G loss: 3.244383]\n",
      "epoch:12 step:9753 [D loss: 0.204220, acc.: 89.84%] [G loss: 4.603225]\n",
      "epoch:12 step:9754 [D loss: 0.443682, acc.: 80.47%] [G loss: 2.466136]\n",
      "epoch:12 step:9755 [D loss: 0.305484, acc.: 87.50%] [G loss: 3.928908]\n",
      "epoch:12 step:9756 [D loss: 0.264289, acc.: 87.50%] [G loss: 6.559820]\n",
      "epoch:12 step:9757 [D loss: 0.351188, acc.: 83.59%] [G loss: 2.939818]\n",
      "epoch:12 step:9758 [D loss: 0.312433, acc.: 82.03%] [G loss: 3.461208]\n",
      "epoch:12 step:9759 [D loss: 0.280316, acc.: 88.28%] [G loss: 3.158343]\n",
      "epoch:12 step:9760 [D loss: 0.313258, acc.: 85.94%] [G loss: 2.965992]\n",
      "epoch:12 step:9761 [D loss: 0.288489, acc.: 90.62%] [G loss: 2.668301]\n",
      "epoch:12 step:9762 [D loss: 0.361553, acc.: 82.81%] [G loss: 2.339616]\n",
      "epoch:12 step:9763 [D loss: 0.321061, acc.: 88.28%] [G loss: 2.720429]\n",
      "epoch:12 step:9764 [D loss: 0.303713, acc.: 86.72%] [G loss: 3.400047]\n",
      "epoch:12 step:9765 [D loss: 0.299701, acc.: 89.06%] [G loss: 2.756485]\n",
      "epoch:12 step:9766 [D loss: 0.283305, acc.: 85.16%] [G loss: 3.432612]\n",
      "epoch:12 step:9767 [D loss: 0.342205, acc.: 85.16%] [G loss: 2.406313]\n",
      "epoch:12 step:9768 [D loss: 0.322866, acc.: 87.50%] [G loss: 2.604738]\n",
      "epoch:12 step:9769 [D loss: 0.370652, acc.: 83.59%] [G loss: 3.306833]\n",
      "epoch:12 step:9770 [D loss: 0.336966, acc.: 85.94%] [G loss: 3.897642]\n",
      "epoch:12 step:9771 [D loss: 0.382094, acc.: 81.25%] [G loss: 2.660081]\n",
      "epoch:12 step:9772 [D loss: 0.325780, acc.: 86.72%] [G loss: 3.338064]\n",
      "epoch:12 step:9773 [D loss: 0.303164, acc.: 90.62%] [G loss: 2.712495]\n",
      "epoch:12 step:9774 [D loss: 0.296320, acc.: 88.28%] [G loss: 2.751212]\n",
      "epoch:12 step:9775 [D loss: 0.308047, acc.: 85.94%] [G loss: 4.151172]\n",
      "epoch:12 step:9776 [D loss: 0.391233, acc.: 83.59%] [G loss: 2.512773]\n",
      "epoch:12 step:9777 [D loss: 0.295056, acc.: 88.28%] [G loss: 4.305912]\n",
      "epoch:12 step:9778 [D loss: 0.603449, acc.: 70.31%] [G loss: 5.623378]\n",
      "epoch:12 step:9779 [D loss: 1.176984, acc.: 70.31%] [G loss: 6.214260]\n",
      "epoch:12 step:9780 [D loss: 0.906485, acc.: 71.88%] [G loss: 9.070526]\n",
      "epoch:12 step:9781 [D loss: 1.022311, acc.: 64.84%] [G loss: 3.986409]\n",
      "epoch:12 step:9782 [D loss: 0.432769, acc.: 85.16%] [G loss: 3.384837]\n",
      "epoch:12 step:9783 [D loss: 0.286773, acc.: 84.38%] [G loss: 2.581631]\n",
      "epoch:12 step:9784 [D loss: 0.483662, acc.: 78.12%] [G loss: 2.685402]\n",
      "epoch:12 step:9785 [D loss: 0.332211, acc.: 85.16%] [G loss: 4.289481]\n",
      "epoch:12 step:9786 [D loss: 0.245722, acc.: 92.97%] [G loss: 3.592948]\n",
      "epoch:12 step:9787 [D loss: 0.399384, acc.: 82.81%] [G loss: 2.759613]\n",
      "epoch:12 step:9788 [D loss: 0.304106, acc.: 88.28%] [G loss: 2.860168]\n",
      "epoch:12 step:9789 [D loss: 0.280682, acc.: 89.84%] [G loss: 4.973481]\n",
      "epoch:12 step:9790 [D loss: 0.267563, acc.: 90.62%] [G loss: 2.250691]\n",
      "epoch:12 step:9791 [D loss: 0.343810, acc.: 82.81%] [G loss: 3.105779]\n",
      "epoch:12 step:9792 [D loss: 0.356861, acc.: 82.03%] [G loss: 4.665693]\n",
      "epoch:12 step:9793 [D loss: 0.217984, acc.: 89.84%] [G loss: 3.380327]\n",
      "epoch:12 step:9794 [D loss: 0.373822, acc.: 82.81%] [G loss: 2.297721]\n",
      "epoch:12 step:9795 [D loss: 0.295640, acc.: 91.41%] [G loss: 2.735992]\n",
      "epoch:12 step:9796 [D loss: 0.308549, acc.: 85.94%] [G loss: 2.897672]\n",
      "epoch:12 step:9797 [D loss: 0.328353, acc.: 85.16%] [G loss: 3.282955]\n",
      "epoch:12 step:9798 [D loss: 0.250474, acc.: 92.97%] [G loss: 2.845452]\n",
      "epoch:12 step:9799 [D loss: 0.241972, acc.: 89.06%] [G loss: 2.945044]\n",
      "epoch:12 step:9800 [D loss: 0.315127, acc.: 86.72%] [G loss: 2.692876]\n",
      "##############\n",
      "[0.8498117  0.8477632  0.81346186 0.79902959 0.76364595 0.8200979\n",
      " 0.88651673 0.83974695 0.84809702 0.81483084]\n",
      "##########\n",
      "epoch:12 step:9801 [D loss: 0.338021, acc.: 86.72%] [G loss: 2.327353]\n",
      "epoch:12 step:9802 [D loss: 0.323854, acc.: 86.72%] [G loss: 2.658562]\n",
      "epoch:12 step:9803 [D loss: 0.348827, acc.: 84.38%] [G loss: 2.576793]\n",
      "epoch:12 step:9804 [D loss: 0.378265, acc.: 84.38%] [G loss: 2.707861]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:12 step:9805 [D loss: 0.250896, acc.: 91.41%] [G loss: 2.802090]\n",
      "epoch:12 step:9806 [D loss: 0.281909, acc.: 87.50%] [G loss: 2.870091]\n",
      "epoch:12 step:9807 [D loss: 0.225822, acc.: 90.62%] [G loss: 3.747094]\n",
      "epoch:12 step:9808 [D loss: 0.353087, acc.: 82.81%] [G loss: 2.478855]\n",
      "epoch:12 step:9809 [D loss: 0.386057, acc.: 83.59%] [G loss: 1.936833]\n",
      "epoch:12 step:9810 [D loss: 0.355442, acc.: 82.03%] [G loss: 2.739128]\n",
      "epoch:12 step:9811 [D loss: 0.354250, acc.: 85.94%] [G loss: 2.211525]\n",
      "epoch:12 step:9812 [D loss: 0.378305, acc.: 84.38%] [G loss: 2.707083]\n",
      "epoch:12 step:9813 [D loss: 0.358499, acc.: 83.59%] [G loss: 3.937369]\n",
      "epoch:12 step:9814 [D loss: 0.211553, acc.: 92.97%] [G loss: 3.480745]\n",
      "epoch:12 step:9815 [D loss: 0.352607, acc.: 81.25%] [G loss: 2.694521]\n",
      "epoch:12 step:9816 [D loss: 0.379785, acc.: 86.72%] [G loss: 2.428545]\n",
      "epoch:12 step:9817 [D loss: 0.323944, acc.: 91.41%] [G loss: 2.676870]\n",
      "epoch:12 step:9818 [D loss: 0.223644, acc.: 94.53%] [G loss: 3.821216]\n",
      "epoch:12 step:9819 [D loss: 0.320811, acc.: 86.72%] [G loss: 2.945262]\n",
      "epoch:12 step:9820 [D loss: 0.298504, acc.: 85.16%] [G loss: 3.145529]\n",
      "epoch:12 step:9821 [D loss: 0.360273, acc.: 86.72%] [G loss: 3.333961]\n",
      "epoch:12 step:9822 [D loss: 0.285318, acc.: 89.84%] [G loss: 2.474855]\n",
      "epoch:12 step:9823 [D loss: 0.317686, acc.: 85.94%] [G loss: 3.557199]\n",
      "epoch:12 step:9824 [D loss: 0.415122, acc.: 80.47%] [G loss: 3.303548]\n",
      "epoch:12 step:9825 [D loss: 0.191839, acc.: 94.53%] [G loss: 3.742452]\n",
      "epoch:12 step:9826 [D loss: 0.431137, acc.: 80.47%] [G loss: 4.439888]\n",
      "epoch:12 step:9827 [D loss: 0.376544, acc.: 85.94%] [G loss: 4.788980]\n",
      "epoch:12 step:9828 [D loss: 0.364035, acc.: 81.25%] [G loss: 3.469032]\n",
      "epoch:12 step:9829 [D loss: 0.541133, acc.: 74.22%] [G loss: 2.774013]\n",
      "epoch:12 step:9830 [D loss: 0.343234, acc.: 85.94%] [G loss: 3.864354]\n",
      "epoch:12 step:9831 [D loss: 0.393109, acc.: 82.81%] [G loss: 3.427621]\n",
      "epoch:12 step:9832 [D loss: 0.299508, acc.: 85.16%] [G loss: 3.152906]\n",
      "epoch:12 step:9833 [D loss: 0.197062, acc.: 93.75%] [G loss: 3.918059]\n",
      "epoch:12 step:9834 [D loss: 0.220460, acc.: 89.84%] [G loss: 3.140679]\n",
      "epoch:12 step:9835 [D loss: 0.304467, acc.: 82.81%] [G loss: 3.383074]\n",
      "epoch:12 step:9836 [D loss: 0.261167, acc.: 91.41%] [G loss: 3.472204]\n",
      "epoch:12 step:9837 [D loss: 0.271640, acc.: 87.50%] [G loss: 3.654151]\n",
      "epoch:12 step:9838 [D loss: 0.323626, acc.: 84.38%] [G loss: 2.566087]\n",
      "epoch:12 step:9839 [D loss: 0.426891, acc.: 77.34%] [G loss: 3.099712]\n",
      "epoch:12 step:9840 [D loss: 0.377264, acc.: 84.38%] [G loss: 2.632714]\n",
      "epoch:12 step:9841 [D loss: 0.257798, acc.: 89.06%] [G loss: 4.527477]\n",
      "epoch:12 step:9842 [D loss: 0.208422, acc.: 90.62%] [G loss: 4.849348]\n",
      "epoch:12 step:9843 [D loss: 0.338434, acc.: 85.16%] [G loss: 2.952171]\n",
      "epoch:12 step:9844 [D loss: 0.335951, acc.: 85.94%] [G loss: 4.268023]\n",
      "epoch:12 step:9845 [D loss: 0.353549, acc.: 81.25%] [G loss: 6.034914]\n",
      "epoch:12 step:9846 [D loss: 0.290554, acc.: 88.28%] [G loss: 4.726954]\n",
      "epoch:12 step:9847 [D loss: 0.452483, acc.: 82.03%] [G loss: 3.563055]\n",
      "epoch:12 step:9848 [D loss: 0.228018, acc.: 89.84%] [G loss: 5.011187]\n",
      "epoch:12 step:9849 [D loss: 0.246656, acc.: 89.84%] [G loss: 4.164900]\n",
      "epoch:12 step:9850 [D loss: 0.353214, acc.: 86.72%] [G loss: 2.846927]\n",
      "epoch:12 step:9851 [D loss: 0.356944, acc.: 81.25%] [G loss: 3.574748]\n",
      "epoch:12 step:9852 [D loss: 0.355431, acc.: 86.72%] [G loss: 4.882242]\n",
      "epoch:12 step:9853 [D loss: 0.313497, acc.: 87.50%] [G loss: 4.388083]\n",
      "epoch:12 step:9854 [D loss: 0.407426, acc.: 78.12%] [G loss: 2.550158]\n",
      "epoch:12 step:9855 [D loss: 0.325866, acc.: 86.72%] [G loss: 3.942161]\n",
      "epoch:12 step:9856 [D loss: 0.280189, acc.: 88.28%] [G loss: 2.682190]\n",
      "epoch:12 step:9857 [D loss: 0.312911, acc.: 85.94%] [G loss: 2.835849]\n",
      "epoch:12 step:9858 [D loss: 0.297337, acc.: 88.28%] [G loss: 3.001240]\n",
      "epoch:12 step:9859 [D loss: 0.285596, acc.: 88.28%] [G loss: 2.798714]\n",
      "epoch:12 step:9860 [D loss: 0.293722, acc.: 86.72%] [G loss: 2.862114]\n",
      "epoch:12 step:9861 [D loss: 0.340926, acc.: 82.03%] [G loss: 3.040469]\n",
      "epoch:12 step:9862 [D loss: 0.295226, acc.: 85.16%] [G loss: 4.032289]\n",
      "epoch:12 step:9863 [D loss: 0.420109, acc.: 87.50%] [G loss: 3.387218]\n",
      "epoch:12 step:9864 [D loss: 0.446736, acc.: 76.56%] [G loss: 5.624897]\n",
      "epoch:12 step:9865 [D loss: 0.236175, acc.: 89.06%] [G loss: 5.014454]\n",
      "epoch:12 step:9866 [D loss: 0.254887, acc.: 89.84%] [G loss: 3.451341]\n",
      "epoch:12 step:9867 [D loss: 0.320378, acc.: 85.16%] [G loss: 4.224683]\n",
      "epoch:12 step:9868 [D loss: 0.291800, acc.: 85.94%] [G loss: 3.301220]\n",
      "epoch:12 step:9869 [D loss: 0.471086, acc.: 80.47%] [G loss: 2.403650]\n",
      "epoch:12 step:9870 [D loss: 0.280683, acc.: 88.28%] [G loss: 3.015510]\n",
      "epoch:12 step:9871 [D loss: 0.426236, acc.: 82.03%] [G loss: 3.561552]\n",
      "epoch:12 step:9872 [D loss: 0.340926, acc.: 88.28%] [G loss: 2.636657]\n",
      "epoch:12 step:9873 [D loss: 0.299862, acc.: 86.72%] [G loss: 2.917315]\n",
      "epoch:12 step:9874 [D loss: 0.425903, acc.: 81.25%] [G loss: 2.633308]\n",
      "epoch:12 step:9875 [D loss: 0.291991, acc.: 88.28%] [G loss: 2.565118]\n",
      "epoch:12 step:9876 [D loss: 0.328148, acc.: 90.62%] [G loss: 2.379256]\n",
      "epoch:12 step:9877 [D loss: 0.328248, acc.: 85.94%] [G loss: 2.923787]\n",
      "epoch:12 step:9878 [D loss: 0.337908, acc.: 90.62%] [G loss: 2.852126]\n",
      "epoch:12 step:9879 [D loss: 0.394152, acc.: 82.03%] [G loss: 3.360439]\n",
      "epoch:12 step:9880 [D loss: 0.329146, acc.: 85.94%] [G loss: 4.071156]\n",
      "epoch:12 step:9881 [D loss: 0.320511, acc.: 85.94%] [G loss: 2.949651]\n",
      "epoch:12 step:9882 [D loss: 0.247277, acc.: 92.97%] [G loss: 2.639573]\n",
      "epoch:12 step:9883 [D loss: 0.388769, acc.: 82.81%] [G loss: 3.232858]\n",
      "epoch:12 step:9884 [D loss: 0.283788, acc.: 90.62%] [G loss: 2.959221]\n",
      "epoch:12 step:9885 [D loss: 0.296317, acc.: 86.72%] [G loss: 4.559355]\n",
      "epoch:12 step:9886 [D loss: 0.335420, acc.: 85.16%] [G loss: 3.179386]\n",
      "epoch:12 step:9887 [D loss: 0.188660, acc.: 92.97%] [G loss: 5.594496]\n",
      "epoch:12 step:9888 [D loss: 0.284298, acc.: 85.94%] [G loss: 3.899626]\n",
      "epoch:12 step:9889 [D loss: 0.322520, acc.: 87.50%] [G loss: 3.541713]\n",
      "epoch:12 step:9890 [D loss: 0.286729, acc.: 88.28%] [G loss: 2.428166]\n",
      "epoch:12 step:9891 [D loss: 0.227192, acc.: 90.62%] [G loss: 3.695273]\n",
      "epoch:12 step:9892 [D loss: 0.342813, acc.: 85.16%] [G loss: 4.541107]\n",
      "epoch:12 step:9893 [D loss: 0.206552, acc.: 92.97%] [G loss: 5.151098]\n",
      "epoch:12 step:9894 [D loss: 0.222355, acc.: 91.41%] [G loss: 4.717382]\n",
      "epoch:12 step:9895 [D loss: 0.272897, acc.: 87.50%] [G loss: 2.852538]\n",
      "epoch:12 step:9896 [D loss: 0.284470, acc.: 88.28%] [G loss: 4.568738]\n",
      "epoch:12 step:9897 [D loss: 0.272157, acc.: 86.72%] [G loss: 3.980709]\n",
      "epoch:12 step:9898 [D loss: 0.296991, acc.: 86.72%] [G loss: 2.752034]\n",
      "epoch:12 step:9899 [D loss: 0.418276, acc.: 80.47%] [G loss: 2.540433]\n",
      "epoch:12 step:9900 [D loss: 0.374014, acc.: 82.81%] [G loss: 2.761105]\n",
      "epoch:12 step:9901 [D loss: 0.458955, acc.: 81.25%] [G loss: 3.489589]\n",
      "epoch:12 step:9902 [D loss: 0.268268, acc.: 88.28%] [G loss: 4.516881]\n",
      "epoch:12 step:9903 [D loss: 0.302754, acc.: 87.50%] [G loss: 5.859432]\n",
      "epoch:12 step:9904 [D loss: 0.266881, acc.: 86.72%] [G loss: 5.333474]\n",
      "epoch:12 step:9905 [D loss: 0.308272, acc.: 85.94%] [G loss: 3.268559]\n",
      "epoch:12 step:9906 [D loss: 0.258923, acc.: 86.72%] [G loss: 4.833794]\n",
      "epoch:12 step:9907 [D loss: 0.290705, acc.: 86.72%] [G loss: 4.460066]\n",
      "epoch:12 step:9908 [D loss: 0.345217, acc.: 85.94%] [G loss: 2.759276]\n",
      "epoch:12 step:9909 [D loss: 0.264245, acc.: 89.84%] [G loss: 3.413940]\n",
      "epoch:12 step:9910 [D loss: 0.397104, acc.: 80.47%] [G loss: 2.769248]\n",
      "epoch:12 step:9911 [D loss: 0.389431, acc.: 84.38%] [G loss: 3.449237]\n",
      "epoch:12 step:9912 [D loss: 0.359436, acc.: 82.81%] [G loss: 2.905987]\n",
      "epoch:12 step:9913 [D loss: 0.290958, acc.: 88.28%] [G loss: 3.928010]\n",
      "epoch:12 step:9914 [D loss: 0.245159, acc.: 90.62%] [G loss: 2.805780]\n",
      "epoch:12 step:9915 [D loss: 0.273715, acc.: 86.72%] [G loss: 3.562911]\n",
      "epoch:12 step:9916 [D loss: 0.324258, acc.: 85.94%] [G loss: 3.327785]\n",
      "epoch:12 step:9917 [D loss: 0.302318, acc.: 87.50%] [G loss: 2.704062]\n",
      "epoch:12 step:9918 [D loss: 0.302971, acc.: 86.72%] [G loss: 3.541116]\n",
      "epoch:12 step:9919 [D loss: 0.315984, acc.: 85.94%] [G loss: 3.647433]\n",
      "epoch:12 step:9920 [D loss: 0.203211, acc.: 92.97%] [G loss: 4.677912]\n",
      "epoch:12 step:9921 [D loss: 0.211365, acc.: 92.97%] [G loss: 4.411559]\n",
      "epoch:12 step:9922 [D loss: 0.274981, acc.: 89.84%] [G loss: 3.387849]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:12 step:9923 [D loss: 0.191786, acc.: 93.75%] [G loss: 5.910951]\n",
      "epoch:12 step:9924 [D loss: 0.407257, acc.: 82.81%] [G loss: 2.308304]\n",
      "epoch:12 step:9925 [D loss: 0.220012, acc.: 91.41%] [G loss: 4.929447]\n",
      "epoch:12 step:9926 [D loss: 0.262992, acc.: 87.50%] [G loss: 2.993282]\n",
      "epoch:12 step:9927 [D loss: 0.373816, acc.: 82.81%] [G loss: 3.523253]\n",
      "epoch:12 step:9928 [D loss: 0.359357, acc.: 82.03%] [G loss: 4.850780]\n",
      "epoch:12 step:9929 [D loss: 0.322497, acc.: 83.59%] [G loss: 4.130466]\n",
      "epoch:12 step:9930 [D loss: 0.240993, acc.: 88.28%] [G loss: 3.344704]\n",
      "epoch:12 step:9931 [D loss: 0.362002, acc.: 84.38%] [G loss: 2.685979]\n",
      "epoch:12 step:9932 [D loss: 0.272261, acc.: 92.19%] [G loss: 2.310833]\n",
      "epoch:12 step:9933 [D loss: 0.345244, acc.: 86.72%] [G loss: 3.375031]\n",
      "epoch:12 step:9934 [D loss: 0.369447, acc.: 84.38%] [G loss: 4.149176]\n",
      "epoch:12 step:9935 [D loss: 0.385498, acc.: 89.06%] [G loss: 3.212930]\n",
      "epoch:12 step:9936 [D loss: 0.244549, acc.: 89.06%] [G loss: 4.516722]\n",
      "epoch:12 step:9937 [D loss: 0.221677, acc.: 90.62%] [G loss: 4.033902]\n",
      "epoch:12 step:9938 [D loss: 0.317241, acc.: 85.16%] [G loss: 2.231084]\n",
      "epoch:12 step:9939 [D loss: 0.247425, acc.: 90.62%] [G loss: 3.902987]\n",
      "epoch:12 step:9940 [D loss: 0.219608, acc.: 91.41%] [G loss: 4.792021]\n",
      "epoch:12 step:9941 [D loss: 0.226405, acc.: 92.19%] [G loss: 3.792076]\n",
      "epoch:12 step:9942 [D loss: 0.247958, acc.: 92.19%] [G loss: 4.273995]\n",
      "epoch:12 step:9943 [D loss: 0.346647, acc.: 82.81%] [G loss: 3.405289]\n",
      "epoch:12 step:9944 [D loss: 0.269674, acc.: 87.50%] [G loss: 2.288272]\n",
      "epoch:12 step:9945 [D loss: 0.339381, acc.: 85.16%] [G loss: 3.611593]\n",
      "epoch:12 step:9946 [D loss: 0.251027, acc.: 89.06%] [G loss: 3.892596]\n",
      "epoch:12 step:9947 [D loss: 0.397558, acc.: 85.94%] [G loss: 2.169970]\n",
      "epoch:12 step:9948 [D loss: 0.226253, acc.: 93.75%] [G loss: 3.354464]\n",
      "epoch:12 step:9949 [D loss: 0.339101, acc.: 85.16%] [G loss: 3.397353]\n",
      "epoch:12 step:9950 [D loss: 0.407015, acc.: 80.47%] [G loss: 3.011239]\n",
      "epoch:12 step:9951 [D loss: 0.380613, acc.: 81.25%] [G loss: 3.390069]\n",
      "epoch:12 step:9952 [D loss: 0.334751, acc.: 82.81%] [G loss: 3.650568]\n",
      "epoch:12 step:9953 [D loss: 0.274258, acc.: 87.50%] [G loss: 2.570583]\n",
      "epoch:12 step:9954 [D loss: 0.267557, acc.: 88.28%] [G loss: 3.006218]\n",
      "epoch:12 step:9955 [D loss: 0.340650, acc.: 84.38%] [G loss: 3.538598]\n",
      "epoch:12 step:9956 [D loss: 0.257343, acc.: 89.06%] [G loss: 4.048501]\n",
      "epoch:12 step:9957 [D loss: 0.211268, acc.: 93.75%] [G loss: 4.333285]\n",
      "epoch:12 step:9958 [D loss: 0.302958, acc.: 87.50%] [G loss: 3.112720]\n",
      "epoch:12 step:9959 [D loss: 0.298137, acc.: 88.28%] [G loss: 2.531362]\n",
      "epoch:12 step:9960 [D loss: 0.287608, acc.: 89.06%] [G loss: 2.878903]\n",
      "epoch:12 step:9961 [D loss: 0.279598, acc.: 85.94%] [G loss: 3.219652]\n",
      "epoch:12 step:9962 [D loss: 0.426365, acc.: 81.25%] [G loss: 4.728166]\n",
      "epoch:12 step:9963 [D loss: 0.596736, acc.: 81.25%] [G loss: 6.443452]\n",
      "epoch:12 step:9964 [D loss: 0.656043, acc.: 75.78%] [G loss: 4.499169]\n",
      "epoch:12 step:9965 [D loss: 0.507842, acc.: 75.78%] [G loss: 3.421129]\n",
      "epoch:12 step:9966 [D loss: 0.300009, acc.: 88.28%] [G loss: 3.638133]\n",
      "epoch:12 step:9967 [D loss: 0.332723, acc.: 85.16%] [G loss: 2.463999]\n",
      "epoch:12 step:9968 [D loss: 0.287878, acc.: 89.06%] [G loss: 2.873213]\n",
      "epoch:12 step:9969 [D loss: 0.422593, acc.: 82.81%] [G loss: 2.342580]\n",
      "epoch:12 step:9970 [D loss: 0.383948, acc.: 80.47%] [G loss: 3.280550]\n",
      "epoch:12 step:9971 [D loss: 0.276025, acc.: 89.84%] [G loss: 3.693072]\n",
      "epoch:12 step:9972 [D loss: 0.288944, acc.: 87.50%] [G loss: 2.894117]\n",
      "epoch:12 step:9973 [D loss: 0.330500, acc.: 88.28%] [G loss: 3.668820]\n",
      "epoch:12 step:9974 [D loss: 0.349300, acc.: 84.38%] [G loss: 2.692637]\n",
      "epoch:12 step:9975 [D loss: 0.220910, acc.: 93.75%] [G loss: 5.039852]\n",
      "epoch:12 step:9976 [D loss: 0.301189, acc.: 85.94%] [G loss: 3.855849]\n",
      "epoch:12 step:9977 [D loss: 0.316862, acc.: 87.50%] [G loss: 3.736966]\n",
      "epoch:12 step:9978 [D loss: 0.474864, acc.: 74.22%] [G loss: 3.693648]\n",
      "epoch:12 step:9979 [D loss: 0.365284, acc.: 85.94%] [G loss: 2.268130]\n",
      "epoch:12 step:9980 [D loss: 0.272668, acc.: 89.06%] [G loss: 4.848290]\n",
      "epoch:12 step:9981 [D loss: 0.417180, acc.: 84.38%] [G loss: 2.361696]\n",
      "epoch:12 step:9982 [D loss: 0.389939, acc.: 83.59%] [G loss: 2.591422]\n",
      "epoch:12 step:9983 [D loss: 0.271617, acc.: 89.84%] [G loss: 3.693691]\n",
      "epoch:12 step:9984 [D loss: 0.197016, acc.: 96.09%] [G loss: 5.692270]\n",
      "epoch:12 step:9985 [D loss: 0.412634, acc.: 82.81%] [G loss: 2.886292]\n",
      "epoch:12 step:9986 [D loss: 0.331410, acc.: 86.72%] [G loss: 2.623381]\n",
      "epoch:12 step:9987 [D loss: 0.275692, acc.: 89.06%] [G loss: 2.619199]\n",
      "epoch:12 step:9988 [D loss: 0.302393, acc.: 84.38%] [G loss: 2.652256]\n",
      "epoch:12 step:9989 [D loss: 0.283637, acc.: 85.16%] [G loss: 3.153702]\n",
      "epoch:12 step:9990 [D loss: 0.341043, acc.: 87.50%] [G loss: 2.757191]\n",
      "epoch:12 step:9991 [D loss: 0.254103, acc.: 88.28%] [G loss: 3.109695]\n",
      "epoch:12 step:9992 [D loss: 0.276030, acc.: 89.84%] [G loss: 2.277776]\n",
      "epoch:12 step:9993 [D loss: 0.212495, acc.: 92.19%] [G loss: 3.033430]\n",
      "epoch:12 step:9994 [D loss: 0.202081, acc.: 93.75%] [G loss: 2.775971]\n",
      "epoch:12 step:9995 [D loss: 0.247673, acc.: 91.41%] [G loss: 3.244318]\n",
      "epoch:12 step:9996 [D loss: 0.321442, acc.: 89.06%] [G loss: 3.125007]\n",
      "epoch:12 step:9997 [D loss: 0.253799, acc.: 87.50%] [G loss: 5.038570]\n",
      "epoch:12 step:9998 [D loss: 0.313107, acc.: 88.28%] [G loss: 4.247327]\n",
      "epoch:12 step:9999 [D loss: 0.263580, acc.: 90.62%] [G loss: 3.640992]\n",
      "epoch:12 step:10000 [D loss: 0.292515, acc.: 86.72%] [G loss: 3.679764]\n",
      "##############\n",
      "[0.84743975 0.86557643 0.81169332 0.79251589 0.79102376 0.78796518\n",
      " 0.87653566 0.83616038 0.82562069 0.79945213]\n",
      "##########\n",
      "epoch:12 step:10001 [D loss: 0.330372, acc.: 85.16%] [G loss: 3.228468]\n",
      "epoch:12 step:10002 [D loss: 0.244176, acc.: 90.62%] [G loss: 3.690972]\n",
      "epoch:12 step:10003 [D loss: 0.266210, acc.: 88.28%] [G loss: 6.192629]\n",
      "epoch:12 step:10004 [D loss: 0.374149, acc.: 83.59%] [G loss: 4.580050]\n",
      "epoch:12 step:10005 [D loss: 0.236132, acc.: 89.06%] [G loss: 6.820635]\n",
      "epoch:12 step:10006 [D loss: 0.280498, acc.: 86.72%] [G loss: 4.528228]\n",
      "epoch:12 step:10007 [D loss: 0.247632, acc.: 88.28%] [G loss: 7.223857]\n",
      "epoch:12 step:10008 [D loss: 0.268860, acc.: 88.28%] [G loss: 2.853873]\n",
      "epoch:12 step:10009 [D loss: 0.176052, acc.: 92.97%] [G loss: 5.491504]\n",
      "epoch:12 step:10010 [D loss: 0.222770, acc.: 92.19%] [G loss: 4.306056]\n",
      "epoch:12 step:10011 [D loss: 0.339139, acc.: 85.16%] [G loss: 2.684833]\n",
      "epoch:12 step:10012 [D loss: 0.239519, acc.: 91.41%] [G loss: 3.064998]\n",
      "epoch:12 step:10013 [D loss: 0.407444, acc.: 77.34%] [G loss: 3.032353]\n",
      "epoch:12 step:10014 [D loss: 0.326452, acc.: 86.72%] [G loss: 2.440485]\n",
      "epoch:12 step:10015 [D loss: 0.305951, acc.: 87.50%] [G loss: 2.885689]\n",
      "epoch:12 step:10016 [D loss: 0.336891, acc.: 88.28%] [G loss: 3.331091]\n",
      "epoch:12 step:10017 [D loss: 0.352513, acc.: 82.81%] [G loss: 3.202875]\n",
      "epoch:12 step:10018 [D loss: 0.306946, acc.: 85.94%] [G loss: 3.160968]\n",
      "epoch:12 step:10019 [D loss: 0.227209, acc.: 90.62%] [G loss: 4.258682]\n",
      "epoch:12 step:10020 [D loss: 0.295617, acc.: 87.50%] [G loss: 2.608465]\n",
      "epoch:12 step:10021 [D loss: 0.302069, acc.: 85.94%] [G loss: 3.143562]\n",
      "epoch:12 step:10022 [D loss: 0.351914, acc.: 85.16%] [G loss: 3.155523]\n",
      "epoch:12 step:10023 [D loss: 0.504418, acc.: 77.34%] [G loss: 4.051358]\n",
      "epoch:12 step:10024 [D loss: 0.403068, acc.: 83.59%] [G loss: 4.099473]\n",
      "epoch:12 step:10025 [D loss: 0.258643, acc.: 86.72%] [G loss: 2.969219]\n",
      "epoch:12 step:10026 [D loss: 0.303639, acc.: 83.59%] [G loss: 3.573462]\n",
      "epoch:12 step:10027 [D loss: 0.342035, acc.: 88.28%] [G loss: 2.155332]\n",
      "epoch:12 step:10028 [D loss: 0.286091, acc.: 87.50%] [G loss: 2.988143]\n",
      "epoch:12 step:10029 [D loss: 0.391088, acc.: 81.25%] [G loss: 2.780392]\n",
      "epoch:12 step:10030 [D loss: 0.236317, acc.: 92.97%] [G loss: 4.031705]\n",
      "epoch:12 step:10031 [D loss: 0.323569, acc.: 88.28%] [G loss: 3.193920]\n",
      "epoch:12 step:10032 [D loss: 0.246060, acc.: 91.41%] [G loss: 3.281927]\n",
      "epoch:12 step:10033 [D loss: 0.296953, acc.: 91.41%] [G loss: 3.476331]\n",
      "epoch:12 step:10034 [D loss: 0.322898, acc.: 85.16%] [G loss: 3.431291]\n",
      "epoch:12 step:10035 [D loss: 0.263860, acc.: 88.28%] [G loss: 3.481428]\n",
      "epoch:12 step:10036 [D loss: 0.238988, acc.: 89.84%] [G loss: 2.773566]\n",
      "epoch:12 step:10037 [D loss: 0.284121, acc.: 88.28%] [G loss: 3.203699]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:12 step:10038 [D loss: 0.326835, acc.: 86.72%] [G loss: 2.820556]\n",
      "epoch:12 step:10039 [D loss: 0.276533, acc.: 87.50%] [G loss: 2.959118]\n",
      "epoch:12 step:10040 [D loss: 0.276520, acc.: 89.84%] [G loss: 3.054348]\n",
      "epoch:12 step:10041 [D loss: 0.341073, acc.: 89.06%] [G loss: 3.055980]\n",
      "epoch:12 step:10042 [D loss: 0.333346, acc.: 84.38%] [G loss: 2.234158]\n",
      "epoch:12 step:10043 [D loss: 0.260153, acc.: 91.41%] [G loss: 2.957412]\n",
      "epoch:12 step:10044 [D loss: 0.279708, acc.: 88.28%] [G loss: 4.780529]\n",
      "epoch:12 step:10045 [D loss: 0.290867, acc.: 88.28%] [G loss: 2.470638]\n",
      "epoch:12 step:10046 [D loss: 0.334166, acc.: 85.16%] [G loss: 2.502847]\n",
      "epoch:12 step:10047 [D loss: 0.260869, acc.: 89.84%] [G loss: 3.838769]\n",
      "epoch:12 step:10048 [D loss: 0.270951, acc.: 87.50%] [G loss: 3.800219]\n",
      "epoch:12 step:10049 [D loss: 0.193092, acc.: 92.97%] [G loss: 2.455445]\n",
      "epoch:12 step:10050 [D loss: 0.291104, acc.: 88.28%] [G loss: 2.723663]\n",
      "epoch:12 step:10051 [D loss: 0.245796, acc.: 92.19%] [G loss: 2.624344]\n",
      "epoch:12 step:10052 [D loss: 0.315618, acc.: 87.50%] [G loss: 2.036816]\n",
      "epoch:12 step:10053 [D loss: 0.274440, acc.: 90.62%] [G loss: 2.968126]\n",
      "epoch:12 step:10054 [D loss: 0.247029, acc.: 89.06%] [G loss: 2.506942]\n",
      "epoch:12 step:10055 [D loss: 0.324434, acc.: 86.72%] [G loss: 3.353031]\n",
      "epoch:12 step:10056 [D loss: 0.361569, acc.: 83.59%] [G loss: 3.430110]\n",
      "epoch:12 step:10057 [D loss: 0.431295, acc.: 78.12%] [G loss: 3.236549]\n",
      "epoch:12 step:10058 [D loss: 0.329856, acc.: 87.50%] [G loss: 2.716444]\n",
      "epoch:12 step:10059 [D loss: 0.336246, acc.: 88.28%] [G loss: 2.855736]\n",
      "epoch:12 step:10060 [D loss: 0.358480, acc.: 79.69%] [G loss: 2.571860]\n",
      "epoch:12 step:10061 [D loss: 0.254576, acc.: 91.41%] [G loss: 3.423905]\n",
      "epoch:12 step:10062 [D loss: 0.226022, acc.: 91.41%] [G loss: 4.017197]\n",
      "epoch:12 step:10063 [D loss: 0.297826, acc.: 89.84%] [G loss: 2.655343]\n",
      "epoch:12 step:10064 [D loss: 0.327821, acc.: 85.16%] [G loss: 4.144928]\n",
      "epoch:12 step:10065 [D loss: 0.182721, acc.: 90.62%] [G loss: 4.003701]\n",
      "epoch:12 step:10066 [D loss: 0.183315, acc.: 92.97%] [G loss: 3.594298]\n",
      "epoch:12 step:10067 [D loss: 0.258600, acc.: 88.28%] [G loss: 2.885412]\n",
      "epoch:12 step:10068 [D loss: 0.242586, acc.: 92.19%] [G loss: 2.276752]\n",
      "epoch:12 step:10069 [D loss: 0.255071, acc.: 93.75%] [G loss: 2.347286]\n",
      "epoch:12 step:10070 [D loss: 0.298495, acc.: 84.38%] [G loss: 3.007210]\n",
      "epoch:12 step:10071 [D loss: 0.277387, acc.: 90.62%] [G loss: 5.079221]\n",
      "epoch:12 step:10072 [D loss: 0.214871, acc.: 90.62%] [G loss: 7.529341]\n",
      "epoch:12 step:10073 [D loss: 0.235723, acc.: 89.06%] [G loss: 5.384116]\n",
      "epoch:12 step:10074 [D loss: 0.249165, acc.: 92.97%] [G loss: 5.236880]\n",
      "epoch:12 step:10075 [D loss: 0.220873, acc.: 89.06%] [G loss: 4.657499]\n",
      "epoch:12 step:10076 [D loss: 0.304909, acc.: 86.72%] [G loss: 3.265256]\n",
      "epoch:12 step:10077 [D loss: 0.238635, acc.: 92.19%] [G loss: 3.380203]\n",
      "epoch:12 step:10078 [D loss: 0.291357, acc.: 86.72%] [G loss: 2.898511]\n",
      "epoch:12 step:10079 [D loss: 0.252540, acc.: 89.06%] [G loss: 3.689889]\n",
      "epoch:12 step:10080 [D loss: 0.337935, acc.: 85.94%] [G loss: 3.844362]\n",
      "epoch:12 step:10081 [D loss: 0.388069, acc.: 85.94%] [G loss: 3.151704]\n",
      "epoch:12 step:10082 [D loss: 0.359280, acc.: 78.91%] [G loss: 2.855666]\n",
      "epoch:12 step:10083 [D loss: 0.259364, acc.: 89.84%] [G loss: 3.463936]\n",
      "epoch:12 step:10084 [D loss: 0.298935, acc.: 87.50%] [G loss: 3.211332]\n",
      "epoch:12 step:10085 [D loss: 0.366923, acc.: 85.94%] [G loss: 3.979209]\n",
      "epoch:12 step:10086 [D loss: 0.298158, acc.: 87.50%] [G loss: 2.819765]\n",
      "epoch:12 step:10087 [D loss: 0.392434, acc.: 81.25%] [G loss: 4.232441]\n",
      "epoch:12 step:10088 [D loss: 0.318698, acc.: 88.28%] [G loss: 5.053541]\n",
      "epoch:12 step:10089 [D loss: 0.239814, acc.: 93.75%] [G loss: 3.386274]\n",
      "epoch:12 step:10090 [D loss: 0.294400, acc.: 86.72%] [G loss: 4.524467]\n",
      "epoch:12 step:10091 [D loss: 0.320049, acc.: 82.03%] [G loss: 3.185107]\n",
      "epoch:12 step:10092 [D loss: 0.274668, acc.: 92.19%] [G loss: 3.706938]\n",
      "epoch:12 step:10093 [D loss: 0.297917, acc.: 88.28%] [G loss: 4.698689]\n",
      "epoch:12 step:10094 [D loss: 0.385207, acc.: 86.72%] [G loss: 5.403231]\n",
      "epoch:12 step:10095 [D loss: 0.324215, acc.: 87.50%] [G loss: 3.732711]\n",
      "epoch:12 step:10096 [D loss: 0.333358, acc.: 84.38%] [G loss: 2.815539]\n",
      "epoch:12 step:10097 [D loss: 0.372883, acc.: 83.59%] [G loss: 3.754159]\n",
      "epoch:12 step:10098 [D loss: 0.331830, acc.: 86.72%] [G loss: 4.239166]\n",
      "epoch:12 step:10099 [D loss: 0.198069, acc.: 94.53%] [G loss: 3.966551]\n",
      "epoch:12 step:10100 [D loss: 0.330949, acc.: 84.38%] [G loss: 5.250638]\n",
      "epoch:12 step:10101 [D loss: 0.410683, acc.: 80.47%] [G loss: 4.081054]\n",
      "epoch:12 step:10102 [D loss: 0.302978, acc.: 86.72%] [G loss: 3.720663]\n",
      "epoch:12 step:10103 [D loss: 0.271020, acc.: 90.62%] [G loss: 4.020213]\n",
      "epoch:12 step:10104 [D loss: 0.296851, acc.: 85.16%] [G loss: 3.131161]\n",
      "epoch:12 step:10105 [D loss: 0.238715, acc.: 91.41%] [G loss: 3.560662]\n",
      "epoch:12 step:10106 [D loss: 0.326474, acc.: 85.94%] [G loss: 3.480932]\n",
      "epoch:12 step:10107 [D loss: 0.387990, acc.: 86.72%] [G loss: 4.683860]\n",
      "epoch:12 step:10108 [D loss: 0.335362, acc.: 88.28%] [G loss: 2.893694]\n",
      "epoch:12 step:10109 [D loss: 0.179697, acc.: 93.75%] [G loss: 5.098010]\n",
      "epoch:12 step:10110 [D loss: 0.312582, acc.: 86.72%] [G loss: 5.041150]\n",
      "epoch:12 step:10111 [D loss: 0.279066, acc.: 86.72%] [G loss: 2.622475]\n",
      "epoch:12 step:10112 [D loss: 0.285204, acc.: 85.94%] [G loss: 3.004426]\n",
      "epoch:12 step:10113 [D loss: 0.250369, acc.: 91.41%] [G loss: 5.118563]\n",
      "epoch:12 step:10114 [D loss: 0.268289, acc.: 89.84%] [G loss: 4.347780]\n",
      "epoch:12 step:10115 [D loss: 0.390096, acc.: 80.47%] [G loss: 3.554657]\n",
      "epoch:12 step:10116 [D loss: 0.311119, acc.: 82.81%] [G loss: 4.162980]\n",
      "epoch:12 step:10117 [D loss: 0.295136, acc.: 87.50%] [G loss: 3.447065]\n",
      "epoch:12 step:10118 [D loss: 0.218202, acc.: 91.41%] [G loss: 3.420010]\n",
      "epoch:12 step:10119 [D loss: 0.320529, acc.: 85.94%] [G loss: 3.448621]\n",
      "epoch:12 step:10120 [D loss: 0.338949, acc.: 82.81%] [G loss: 3.607365]\n",
      "epoch:12 step:10121 [D loss: 0.305445, acc.: 85.16%] [G loss: 3.960262]\n",
      "epoch:12 step:10122 [D loss: 0.304173, acc.: 87.50%] [G loss: 3.600774]\n",
      "epoch:12 step:10123 [D loss: 0.324587, acc.: 88.28%] [G loss: 3.224972]\n",
      "epoch:12 step:10124 [D loss: 0.344478, acc.: 86.72%] [G loss: 2.699497]\n",
      "epoch:12 step:10125 [D loss: 0.304664, acc.: 87.50%] [G loss: 3.979995]\n",
      "epoch:12 step:10126 [D loss: 0.344327, acc.: 85.16%] [G loss: 5.709876]\n",
      "epoch:12 step:10127 [D loss: 0.250940, acc.: 86.72%] [G loss: 6.855698]\n",
      "epoch:12 step:10128 [D loss: 0.267570, acc.: 85.94%] [G loss: 4.782200]\n",
      "epoch:12 step:10129 [D loss: 0.309456, acc.: 84.38%] [G loss: 3.706422]\n",
      "epoch:12 step:10130 [D loss: 0.310003, acc.: 88.28%] [G loss: 4.124885]\n",
      "epoch:12 step:10131 [D loss: 0.293664, acc.: 85.16%] [G loss: 3.432797]\n",
      "epoch:12 step:10132 [D loss: 0.181522, acc.: 91.41%] [G loss: 6.079277]\n",
      "epoch:12 step:10133 [D loss: 0.267555, acc.: 89.84%] [G loss: 3.182020]\n",
      "epoch:12 step:10134 [D loss: 0.274470, acc.: 86.72%] [G loss: 6.045192]\n",
      "epoch:12 step:10135 [D loss: 0.408365, acc.: 84.38%] [G loss: 3.072896]\n",
      "epoch:12 step:10136 [D loss: 0.308997, acc.: 92.19%] [G loss: 4.913116]\n",
      "epoch:12 step:10137 [D loss: 0.338358, acc.: 85.16%] [G loss: 3.996123]\n",
      "epoch:12 step:10138 [D loss: 0.441150, acc.: 85.16%] [G loss: 3.941543]\n",
      "epoch:12 step:10139 [D loss: 0.312142, acc.: 89.06%] [G loss: 3.360238]\n",
      "epoch:12 step:10140 [D loss: 0.303018, acc.: 85.94%] [G loss: 4.538383]\n",
      "epoch:12 step:10141 [D loss: 0.332998, acc.: 87.50%] [G loss: 3.108979]\n",
      "epoch:12 step:10142 [D loss: 0.194430, acc.: 92.19%] [G loss: 4.073983]\n",
      "epoch:12 step:10143 [D loss: 0.335733, acc.: 85.94%] [G loss: 4.164066]\n",
      "epoch:12 step:10144 [D loss: 0.346416, acc.: 82.81%] [G loss: 4.645855]\n",
      "epoch:12 step:10145 [D loss: 0.167315, acc.: 93.75%] [G loss: 6.774084]\n",
      "epoch:12 step:10146 [D loss: 0.349433, acc.: 79.69%] [G loss: 4.366765]\n",
      "epoch:12 step:10147 [D loss: 0.404196, acc.: 81.25%] [G loss: 3.089237]\n",
      "epoch:12 step:10148 [D loss: 0.273049, acc.: 87.50%] [G loss: 3.239201]\n",
      "epoch:12 step:10149 [D loss: 0.429554, acc.: 78.12%] [G loss: 7.438575]\n",
      "epoch:12 step:10150 [D loss: 1.092562, acc.: 66.41%] [G loss: 8.579293]\n",
      "epoch:12 step:10151 [D loss: 1.977902, acc.: 52.34%] [G loss: 1.892471]\n",
      "epoch:12 step:10152 [D loss: 0.346854, acc.: 87.50%] [G loss: 2.758259]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:12 step:10153 [D loss: 0.436375, acc.: 81.25%] [G loss: 2.447065]\n",
      "epoch:13 step:10154 [D loss: 0.344234, acc.: 82.03%] [G loss: 3.474016]\n",
      "epoch:13 step:10155 [D loss: 0.410964, acc.: 85.16%] [G loss: 3.633696]\n",
      "epoch:13 step:10156 [D loss: 0.256969, acc.: 90.62%] [G loss: 3.455847]\n",
      "epoch:13 step:10157 [D loss: 0.329238, acc.: 85.16%] [G loss: 4.548460]\n",
      "epoch:13 step:10158 [D loss: 0.397211, acc.: 83.59%] [G loss: 3.007677]\n",
      "epoch:13 step:10159 [D loss: 0.285338, acc.: 85.94%] [G loss: 3.187576]\n",
      "epoch:13 step:10160 [D loss: 0.284579, acc.: 87.50%] [G loss: 3.022888]\n",
      "epoch:13 step:10161 [D loss: 0.239188, acc.: 92.19%] [G loss: 3.902553]\n",
      "epoch:13 step:10162 [D loss: 0.311179, acc.: 87.50%] [G loss: 2.509490]\n",
      "epoch:13 step:10163 [D loss: 0.347109, acc.: 85.94%] [G loss: 2.742174]\n",
      "epoch:13 step:10164 [D loss: 0.267833, acc.: 87.50%] [G loss: 2.844426]\n",
      "epoch:13 step:10165 [D loss: 0.235791, acc.: 91.41%] [G loss: 3.567751]\n",
      "epoch:13 step:10166 [D loss: 0.473067, acc.: 82.81%] [G loss: 2.318278]\n",
      "epoch:13 step:10167 [D loss: 0.314266, acc.: 89.06%] [G loss: 3.650355]\n",
      "epoch:13 step:10168 [D loss: 0.355850, acc.: 85.94%] [G loss: 3.150113]\n",
      "epoch:13 step:10169 [D loss: 0.228008, acc.: 89.84%] [G loss: 2.944448]\n",
      "epoch:13 step:10170 [D loss: 0.324607, acc.: 85.16%] [G loss: 3.477888]\n",
      "epoch:13 step:10171 [D loss: 0.278958, acc.: 86.72%] [G loss: 4.355995]\n",
      "epoch:13 step:10172 [D loss: 0.237339, acc.: 91.41%] [G loss: 6.931815]\n",
      "epoch:13 step:10173 [D loss: 0.224527, acc.: 91.41%] [G loss: 3.463354]\n",
      "epoch:13 step:10174 [D loss: 0.333070, acc.: 82.03%] [G loss: 2.500459]\n",
      "epoch:13 step:10175 [D loss: 0.376099, acc.: 84.38%] [G loss: 3.728029]\n",
      "epoch:13 step:10176 [D loss: 0.425827, acc.: 84.38%] [G loss: 2.927026]\n",
      "epoch:13 step:10177 [D loss: 0.381749, acc.: 84.38%] [G loss: 3.505474]\n",
      "epoch:13 step:10178 [D loss: 0.342057, acc.: 85.94%] [G loss: 2.466239]\n",
      "epoch:13 step:10179 [D loss: 0.253313, acc.: 92.97%] [G loss: 2.416491]\n",
      "epoch:13 step:10180 [D loss: 0.277633, acc.: 85.94%] [G loss: 2.837330]\n",
      "epoch:13 step:10181 [D loss: 0.254321, acc.: 86.72%] [G loss: 3.374352]\n",
      "epoch:13 step:10182 [D loss: 0.221298, acc.: 91.41%] [G loss: 3.683023]\n",
      "epoch:13 step:10183 [D loss: 0.218636, acc.: 95.31%] [G loss: 3.441578]\n",
      "epoch:13 step:10184 [D loss: 0.168256, acc.: 93.75%] [G loss: 6.523291]\n",
      "epoch:13 step:10185 [D loss: 0.312518, acc.: 86.72%] [G loss: 3.549416]\n",
      "epoch:13 step:10186 [D loss: 0.400878, acc.: 80.47%] [G loss: 2.860050]\n",
      "epoch:13 step:10187 [D loss: 0.266351, acc.: 89.84%] [G loss: 2.358306]\n",
      "epoch:13 step:10188 [D loss: 0.335469, acc.: 84.38%] [G loss: 3.089881]\n",
      "epoch:13 step:10189 [D loss: 0.413940, acc.: 82.81%] [G loss: 2.388497]\n",
      "epoch:13 step:10190 [D loss: 0.352781, acc.: 85.16%] [G loss: 2.478623]\n",
      "epoch:13 step:10191 [D loss: 0.280533, acc.: 92.19%] [G loss: 2.835521]\n",
      "epoch:13 step:10192 [D loss: 0.350565, acc.: 84.38%] [G loss: 3.805212]\n",
      "epoch:13 step:10193 [D loss: 0.421866, acc.: 81.25%] [G loss: 3.227221]\n",
      "epoch:13 step:10194 [D loss: 0.276002, acc.: 88.28%] [G loss: 3.247564]\n",
      "epoch:13 step:10195 [D loss: 0.324324, acc.: 86.72%] [G loss: 2.925647]\n",
      "epoch:13 step:10196 [D loss: 0.321786, acc.: 88.28%] [G loss: 3.676505]\n",
      "epoch:13 step:10197 [D loss: 0.466853, acc.: 79.69%] [G loss: 4.174864]\n",
      "epoch:13 step:10198 [D loss: 0.555044, acc.: 78.12%] [G loss: 3.403679]\n",
      "epoch:13 step:10199 [D loss: 0.520050, acc.: 72.66%] [G loss: 2.524137]\n",
      "epoch:13 step:10200 [D loss: 0.452717, acc.: 81.25%] [G loss: 3.679486]\n",
      "##############\n",
      "[0.83243384 0.84522164 0.82201265 0.78220576 0.77701435 0.815366\n",
      " 0.87762766 0.82467127 0.81877096 0.81906456]\n",
      "##########\n",
      "epoch:13 step:10201 [D loss: 0.278951, acc.: 89.84%] [G loss: 4.106014]\n",
      "epoch:13 step:10202 [D loss: 0.267277, acc.: 89.06%] [G loss: 2.682085]\n",
      "epoch:13 step:10203 [D loss: 0.262064, acc.: 90.62%] [G loss: 3.640372]\n",
      "epoch:13 step:10204 [D loss: 0.334882, acc.: 86.72%] [G loss: 3.035452]\n",
      "epoch:13 step:10205 [D loss: 0.236308, acc.: 89.06%] [G loss: 3.460839]\n",
      "epoch:13 step:10206 [D loss: 0.291638, acc.: 89.06%] [G loss: 3.422613]\n",
      "epoch:13 step:10207 [D loss: 0.368655, acc.: 85.16%] [G loss: 2.450348]\n",
      "epoch:13 step:10208 [D loss: 0.412325, acc.: 82.81%] [G loss: 2.928776]\n",
      "epoch:13 step:10209 [D loss: 0.304503, acc.: 88.28%] [G loss: 2.915061]\n",
      "epoch:13 step:10210 [D loss: 0.414677, acc.: 82.03%] [G loss: 2.712979]\n",
      "epoch:13 step:10211 [D loss: 0.314078, acc.: 83.59%] [G loss: 3.552960]\n",
      "epoch:13 step:10212 [D loss: 0.396093, acc.: 81.25%] [G loss: 3.578179]\n",
      "epoch:13 step:10213 [D loss: 0.379674, acc.: 80.47%] [G loss: 3.572625]\n",
      "epoch:13 step:10214 [D loss: 0.343335, acc.: 87.50%] [G loss: 4.215737]\n",
      "epoch:13 step:10215 [D loss: 0.273310, acc.: 90.62%] [G loss: 3.116230]\n",
      "epoch:13 step:10216 [D loss: 0.364155, acc.: 84.38%] [G loss: 2.946957]\n",
      "epoch:13 step:10217 [D loss: 0.363333, acc.: 88.28%] [G loss: 3.133024]\n",
      "epoch:13 step:10218 [D loss: 0.305037, acc.: 85.94%] [G loss: 3.202955]\n",
      "epoch:13 step:10219 [D loss: 0.303460, acc.: 86.72%] [G loss: 3.256989]\n",
      "epoch:13 step:10220 [D loss: 0.254341, acc.: 86.72%] [G loss: 3.204160]\n",
      "epoch:13 step:10221 [D loss: 0.350671, acc.: 85.16%] [G loss: 4.713857]\n",
      "epoch:13 step:10222 [D loss: 0.333521, acc.: 86.72%] [G loss: 3.777636]\n",
      "epoch:13 step:10223 [D loss: 0.252169, acc.: 88.28%] [G loss: 6.809305]\n",
      "epoch:13 step:10224 [D loss: 0.299816, acc.: 85.16%] [G loss: 3.080991]\n",
      "epoch:13 step:10225 [D loss: 0.326240, acc.: 86.72%] [G loss: 2.543399]\n",
      "epoch:13 step:10226 [D loss: 0.254817, acc.: 88.28%] [G loss: 4.780405]\n",
      "epoch:13 step:10227 [D loss: 0.260602, acc.: 91.41%] [G loss: 3.645358]\n",
      "epoch:13 step:10228 [D loss: 0.331094, acc.: 85.16%] [G loss: 4.036401]\n",
      "epoch:13 step:10229 [D loss: 0.365956, acc.: 79.69%] [G loss: 3.663368]\n",
      "epoch:13 step:10230 [D loss: 0.466332, acc.: 73.44%] [G loss: 2.473121]\n",
      "epoch:13 step:10231 [D loss: 0.261540, acc.: 90.62%] [G loss: 3.096708]\n",
      "epoch:13 step:10232 [D loss: 0.341745, acc.: 82.81%] [G loss: 5.304897]\n",
      "epoch:13 step:10233 [D loss: 0.340514, acc.: 86.72%] [G loss: 5.240940]\n",
      "epoch:13 step:10234 [D loss: 0.255293, acc.: 90.62%] [G loss: 4.095846]\n",
      "epoch:13 step:10235 [D loss: 0.420357, acc.: 80.47%] [G loss: 3.295276]\n",
      "epoch:13 step:10236 [D loss: 0.265464, acc.: 88.28%] [G loss: 3.752090]\n",
      "epoch:13 step:10237 [D loss: 0.233651, acc.: 92.97%] [G loss: 3.326603]\n",
      "epoch:13 step:10238 [D loss: 0.388781, acc.: 81.25%] [G loss: 2.861704]\n",
      "epoch:13 step:10239 [D loss: 0.307692, acc.: 86.72%] [G loss: 4.091150]\n",
      "epoch:13 step:10240 [D loss: 0.313959, acc.: 85.16%] [G loss: 3.901596]\n",
      "epoch:13 step:10241 [D loss: 0.268265, acc.: 89.06%] [G loss: 3.249410]\n",
      "epoch:13 step:10242 [D loss: 0.240017, acc.: 90.62%] [G loss: 3.340203]\n",
      "epoch:13 step:10243 [D loss: 0.250152, acc.: 89.84%] [G loss: 4.245250]\n",
      "epoch:13 step:10244 [D loss: 0.260761, acc.: 87.50%] [G loss: 4.004207]\n",
      "epoch:13 step:10245 [D loss: 0.273476, acc.: 91.41%] [G loss: 4.668578]\n",
      "epoch:13 step:10246 [D loss: 0.294070, acc.: 83.59%] [G loss: 4.265381]\n",
      "epoch:13 step:10247 [D loss: 0.236542, acc.: 90.62%] [G loss: 5.160880]\n",
      "epoch:13 step:10248 [D loss: 0.282588, acc.: 85.16%] [G loss: 3.542495]\n",
      "epoch:13 step:10249 [D loss: 0.452216, acc.: 82.03%] [G loss: 3.140321]\n",
      "epoch:13 step:10250 [D loss: 0.373157, acc.: 84.38%] [G loss: 4.100790]\n",
      "epoch:13 step:10251 [D loss: 0.321028, acc.: 85.94%] [G loss: 3.757849]\n",
      "epoch:13 step:10252 [D loss: 0.310171, acc.: 87.50%] [G loss: 4.682571]\n",
      "epoch:13 step:10253 [D loss: 0.373391, acc.: 85.16%] [G loss: 4.789692]\n",
      "epoch:13 step:10254 [D loss: 0.642956, acc.: 76.56%] [G loss: 6.941969]\n",
      "epoch:13 step:10255 [D loss: 1.606565, acc.: 57.81%] [G loss: 6.537421]\n",
      "epoch:13 step:10256 [D loss: 1.576642, acc.: 61.72%] [G loss: 8.399216]\n",
      "epoch:13 step:10257 [D loss: 1.107069, acc.: 66.41%] [G loss: 3.150990]\n",
      "epoch:13 step:10258 [D loss: 0.608244, acc.: 78.12%] [G loss: 3.105309]\n",
      "epoch:13 step:10259 [D loss: 0.571554, acc.: 71.09%] [G loss: 3.165660]\n",
      "epoch:13 step:10260 [D loss: 0.244279, acc.: 90.62%] [G loss: 3.671590]\n",
      "epoch:13 step:10261 [D loss: 0.241104, acc.: 89.84%] [G loss: 4.330155]\n",
      "epoch:13 step:10262 [D loss: 0.480577, acc.: 80.47%] [G loss: 3.062031]\n",
      "epoch:13 step:10263 [D loss: 0.448187, acc.: 79.69%] [G loss: 2.452424]\n",
      "epoch:13 step:10264 [D loss: 0.568575, acc.: 75.00%] [G loss: 2.800436]\n",
      "epoch:13 step:10265 [D loss: 0.298367, acc.: 89.06%] [G loss: 3.028253]\n",
      "epoch:13 step:10266 [D loss: 0.410929, acc.: 81.25%] [G loss: 2.350828]\n",
      "epoch:13 step:10267 [D loss: 0.506297, acc.: 78.91%] [G loss: 3.245698]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:13 step:10268 [D loss: 0.256775, acc.: 92.19%] [G loss: 4.361513]\n",
      "epoch:13 step:10269 [D loss: 0.272466, acc.: 92.19%] [G loss: 3.877253]\n",
      "epoch:13 step:10270 [D loss: 0.320118, acc.: 86.72%] [G loss: 2.777502]\n",
      "epoch:13 step:10271 [D loss: 0.290286, acc.: 86.72%] [G loss: 3.385818]\n",
      "epoch:13 step:10272 [D loss: 0.320571, acc.: 89.06%] [G loss: 2.470975]\n",
      "epoch:13 step:10273 [D loss: 0.373479, acc.: 86.72%] [G loss: 3.323596]\n",
      "epoch:13 step:10274 [D loss: 0.213915, acc.: 91.41%] [G loss: 3.917557]\n",
      "epoch:13 step:10275 [D loss: 0.445576, acc.: 80.47%] [G loss: 3.176266]\n",
      "epoch:13 step:10276 [D loss: 0.351905, acc.: 83.59%] [G loss: 3.154944]\n",
      "epoch:13 step:10277 [D loss: 0.141636, acc.: 94.53%] [G loss: 4.588637]\n",
      "epoch:13 step:10278 [D loss: 0.304476, acc.: 86.72%] [G loss: 4.770507]\n",
      "epoch:13 step:10279 [D loss: 0.273267, acc.: 90.62%] [G loss: 2.980506]\n",
      "epoch:13 step:10280 [D loss: 0.344754, acc.: 82.03%] [G loss: 4.180965]\n",
      "epoch:13 step:10281 [D loss: 0.257452, acc.: 87.50%] [G loss: 3.996009]\n",
      "epoch:13 step:10282 [D loss: 0.359897, acc.: 83.59%] [G loss: 2.876194]\n",
      "epoch:13 step:10283 [D loss: 0.166102, acc.: 94.53%] [G loss: 3.923500]\n",
      "epoch:13 step:10284 [D loss: 0.375621, acc.: 85.16%] [G loss: 3.052680]\n",
      "epoch:13 step:10285 [D loss: 0.352179, acc.: 81.25%] [G loss: 2.808925]\n",
      "epoch:13 step:10286 [D loss: 0.375282, acc.: 87.50%] [G loss: 3.037445]\n",
      "epoch:13 step:10287 [D loss: 0.277676, acc.: 89.06%] [G loss: 3.720094]\n",
      "epoch:13 step:10288 [D loss: 0.383924, acc.: 83.59%] [G loss: 3.016804]\n",
      "epoch:13 step:10289 [D loss: 0.306434, acc.: 89.06%] [G loss: 2.530204]\n",
      "epoch:13 step:10290 [D loss: 0.383841, acc.: 84.38%] [G loss: 4.382832]\n",
      "epoch:13 step:10291 [D loss: 0.280008, acc.: 86.72%] [G loss: 4.706847]\n",
      "epoch:13 step:10292 [D loss: 0.370263, acc.: 84.38%] [G loss: 3.177554]\n",
      "epoch:13 step:10293 [D loss: 0.348321, acc.: 86.72%] [G loss: 3.267683]\n",
      "epoch:13 step:10294 [D loss: 0.402142, acc.: 82.03%] [G loss: 2.590220]\n",
      "epoch:13 step:10295 [D loss: 0.410143, acc.: 83.59%] [G loss: 2.279629]\n",
      "epoch:13 step:10296 [D loss: 0.460140, acc.: 77.34%] [G loss: 2.829071]\n",
      "epoch:13 step:10297 [D loss: 0.373582, acc.: 83.59%] [G loss: 2.744942]\n",
      "epoch:13 step:10298 [D loss: 0.435334, acc.: 77.34%] [G loss: 3.003092]\n",
      "epoch:13 step:10299 [D loss: 0.375820, acc.: 83.59%] [G loss: 2.116530]\n",
      "epoch:13 step:10300 [D loss: 0.319119, acc.: 88.28%] [G loss: 2.752346]\n",
      "epoch:13 step:10301 [D loss: 0.345232, acc.: 82.03%] [G loss: 2.742377]\n",
      "epoch:13 step:10302 [D loss: 0.286303, acc.: 86.72%] [G loss: 2.757617]\n",
      "epoch:13 step:10303 [D loss: 0.334970, acc.: 86.72%] [G loss: 1.960268]\n",
      "epoch:13 step:10304 [D loss: 0.337756, acc.: 87.50%] [G loss: 2.681066]\n",
      "epoch:13 step:10305 [D loss: 0.393177, acc.: 85.16%] [G loss: 2.584476]\n",
      "epoch:13 step:10306 [D loss: 0.363363, acc.: 85.94%] [G loss: 2.702129]\n",
      "epoch:13 step:10307 [D loss: 0.415851, acc.: 83.59%] [G loss: 2.295960]\n",
      "epoch:13 step:10308 [D loss: 0.345864, acc.: 83.59%] [G loss: 2.744656]\n",
      "epoch:13 step:10309 [D loss: 0.322391, acc.: 85.94%] [G loss: 3.098164]\n",
      "epoch:13 step:10310 [D loss: 0.377177, acc.: 85.94%] [G loss: 4.071928]\n",
      "epoch:13 step:10311 [D loss: 0.429721, acc.: 79.69%] [G loss: 2.288810]\n",
      "epoch:13 step:10312 [D loss: 0.320659, acc.: 86.72%] [G loss: 2.505187]\n",
      "epoch:13 step:10313 [D loss: 0.326965, acc.: 87.50%] [G loss: 2.910622]\n",
      "epoch:13 step:10314 [D loss: 0.330388, acc.: 85.94%] [G loss: 2.848044]\n",
      "epoch:13 step:10315 [D loss: 0.323583, acc.: 83.59%] [G loss: 2.409648]\n",
      "epoch:13 step:10316 [D loss: 0.362357, acc.: 86.72%] [G loss: 3.332308]\n",
      "epoch:13 step:10317 [D loss: 0.167996, acc.: 93.75%] [G loss: 5.708820]\n",
      "epoch:13 step:10318 [D loss: 0.282480, acc.: 87.50%] [G loss: 3.136960]\n",
      "epoch:13 step:10319 [D loss: 0.365036, acc.: 82.81%] [G loss: 3.908701]\n",
      "epoch:13 step:10320 [D loss: 0.264658, acc.: 83.59%] [G loss: 6.685646]\n",
      "epoch:13 step:10321 [D loss: 0.282517, acc.: 85.94%] [G loss: 3.355870]\n",
      "epoch:13 step:10322 [D loss: 0.302188, acc.: 87.50%] [G loss: 2.951487]\n",
      "epoch:13 step:10323 [D loss: 0.283375, acc.: 89.84%] [G loss: 3.376099]\n",
      "epoch:13 step:10324 [D loss: 0.316442, acc.: 86.72%] [G loss: 4.343070]\n",
      "epoch:13 step:10325 [D loss: 0.381666, acc.: 83.59%] [G loss: 2.796915]\n",
      "epoch:13 step:10326 [D loss: 0.346619, acc.: 85.94%] [G loss: 4.401294]\n",
      "epoch:13 step:10327 [D loss: 0.396507, acc.: 82.81%] [G loss: 5.226525]\n",
      "epoch:13 step:10328 [D loss: 0.383246, acc.: 87.50%] [G loss: 2.406608]\n",
      "epoch:13 step:10329 [D loss: 0.324061, acc.: 83.59%] [G loss: 4.368998]\n",
      "epoch:13 step:10330 [D loss: 0.244326, acc.: 87.50%] [G loss: 6.099828]\n",
      "epoch:13 step:10331 [D loss: 0.328628, acc.: 86.72%] [G loss: 2.793470]\n",
      "epoch:13 step:10332 [D loss: 0.259356, acc.: 87.50%] [G loss: 4.905883]\n",
      "epoch:13 step:10333 [D loss: 0.331976, acc.: 82.03%] [G loss: 4.961321]\n",
      "epoch:13 step:10334 [D loss: 0.345019, acc.: 85.16%] [G loss: 2.468462]\n",
      "epoch:13 step:10335 [D loss: 0.311507, acc.: 84.38%] [G loss: 3.714078]\n",
      "epoch:13 step:10336 [D loss: 0.285861, acc.: 89.06%] [G loss: 3.225901]\n",
      "epoch:13 step:10337 [D loss: 0.311843, acc.: 88.28%] [G loss: 2.604019]\n",
      "epoch:13 step:10338 [D loss: 0.324626, acc.: 85.94%] [G loss: 2.543657]\n",
      "epoch:13 step:10339 [D loss: 0.292411, acc.: 89.06%] [G loss: 2.622676]\n",
      "epoch:13 step:10340 [D loss: 0.287482, acc.: 88.28%] [G loss: 3.153020]\n",
      "epoch:13 step:10341 [D loss: 0.304755, acc.: 87.50%] [G loss: 4.068536]\n",
      "epoch:13 step:10342 [D loss: 0.210431, acc.: 92.97%] [G loss: 3.516103]\n",
      "epoch:13 step:10343 [D loss: 0.345071, acc.: 87.50%] [G loss: 2.585808]\n",
      "epoch:13 step:10344 [D loss: 0.319970, acc.: 83.59%] [G loss: 3.540199]\n",
      "epoch:13 step:10345 [D loss: 0.238063, acc.: 90.62%] [G loss: 3.840962]\n",
      "epoch:13 step:10346 [D loss: 0.314939, acc.: 86.72%] [G loss: 3.936984]\n",
      "epoch:13 step:10347 [D loss: 0.359245, acc.: 80.47%] [G loss: 3.062487]\n",
      "epoch:13 step:10348 [D loss: 0.428969, acc.: 80.47%] [G loss: 5.186213]\n",
      "epoch:13 step:10349 [D loss: 0.324972, acc.: 89.84%] [G loss: 5.936766]\n",
      "epoch:13 step:10350 [D loss: 0.268475, acc.: 89.06%] [G loss: 6.108099]\n",
      "epoch:13 step:10351 [D loss: 0.265019, acc.: 91.41%] [G loss: 3.541851]\n",
      "epoch:13 step:10352 [D loss: 0.271960, acc.: 88.28%] [G loss: 4.637369]\n",
      "epoch:13 step:10353 [D loss: 0.297116, acc.: 87.50%] [G loss: 4.211248]\n",
      "epoch:13 step:10354 [D loss: 0.220162, acc.: 91.41%] [G loss: 3.248261]\n",
      "epoch:13 step:10355 [D loss: 0.374278, acc.: 89.06%] [G loss: 4.036638]\n",
      "epoch:13 step:10356 [D loss: 0.236670, acc.: 89.84%] [G loss: 3.674937]\n",
      "epoch:13 step:10357 [D loss: 0.366569, acc.: 82.81%] [G loss: 2.704513]\n",
      "epoch:13 step:10358 [D loss: 0.312178, acc.: 84.38%] [G loss: 4.071014]\n",
      "epoch:13 step:10359 [D loss: 0.291931, acc.: 89.06%] [G loss: 3.434886]\n",
      "epoch:13 step:10360 [D loss: 0.354526, acc.: 84.38%] [G loss: 3.387946]\n",
      "epoch:13 step:10361 [D loss: 0.343943, acc.: 88.28%] [G loss: 2.723021]\n",
      "epoch:13 step:10362 [D loss: 0.388925, acc.: 82.03%] [G loss: 3.686075]\n",
      "epoch:13 step:10363 [D loss: 0.265166, acc.: 90.62%] [G loss: 3.419891]\n",
      "epoch:13 step:10364 [D loss: 0.349641, acc.: 87.50%] [G loss: 3.153421]\n",
      "epoch:13 step:10365 [D loss: 0.217717, acc.: 92.97%] [G loss: 3.124057]\n",
      "epoch:13 step:10366 [D loss: 0.311312, acc.: 88.28%] [G loss: 3.847582]\n",
      "epoch:13 step:10367 [D loss: 0.345701, acc.: 86.72%] [G loss: 2.919580]\n",
      "epoch:13 step:10368 [D loss: 0.334619, acc.: 84.38%] [G loss: 3.598555]\n",
      "epoch:13 step:10369 [D loss: 0.346658, acc.: 85.16%] [G loss: 3.125575]\n",
      "epoch:13 step:10370 [D loss: 0.479781, acc.: 82.81%] [G loss: 3.308817]\n",
      "epoch:13 step:10371 [D loss: 0.428869, acc.: 85.94%] [G loss: 3.482884]\n",
      "epoch:13 step:10372 [D loss: 0.355197, acc.: 87.50%] [G loss: 5.627911]\n",
      "epoch:13 step:10373 [D loss: 0.296917, acc.: 87.50%] [G loss: 4.426664]\n",
      "epoch:13 step:10374 [D loss: 0.302574, acc.: 88.28%] [G loss: 3.141889]\n",
      "epoch:13 step:10375 [D loss: 0.371384, acc.: 79.69%] [G loss: 2.710701]\n",
      "epoch:13 step:10376 [D loss: 0.275715, acc.: 89.84%] [G loss: 2.910422]\n",
      "epoch:13 step:10377 [D loss: 0.306725, acc.: 86.72%] [G loss: 2.938342]\n",
      "epoch:13 step:10378 [D loss: 0.299183, acc.: 85.16%] [G loss: 2.937377]\n",
      "epoch:13 step:10379 [D loss: 0.307715, acc.: 85.16%] [G loss: 2.460371]\n",
      "epoch:13 step:10380 [D loss: 0.324127, acc.: 86.72%] [G loss: 3.225524]\n",
      "epoch:13 step:10381 [D loss: 0.409905, acc.: 82.03%] [G loss: 3.031934]\n",
      "epoch:13 step:10382 [D loss: 0.313942, acc.: 85.16%] [G loss: 3.016198]\n",
      "epoch:13 step:10383 [D loss: 0.269425, acc.: 89.84%] [G loss: 2.581972]\n",
      "epoch:13 step:10384 [D loss: 0.336681, acc.: 86.72%] [G loss: 2.176786]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:13 step:10385 [D loss: 0.264592, acc.: 90.62%] [G loss: 2.411187]\n",
      "epoch:13 step:10386 [D loss: 0.366268, acc.: 85.94%] [G loss: 2.369023]\n",
      "epoch:13 step:10387 [D loss: 0.356363, acc.: 82.81%] [G loss: 2.946286]\n",
      "epoch:13 step:10388 [D loss: 0.281483, acc.: 89.84%] [G loss: 2.572721]\n",
      "epoch:13 step:10389 [D loss: 0.328827, acc.: 85.16%] [G loss: 4.031197]\n",
      "epoch:13 step:10390 [D loss: 0.326699, acc.: 84.38%] [G loss: 4.557929]\n",
      "epoch:13 step:10391 [D loss: 0.264414, acc.: 89.06%] [G loss: 4.373369]\n",
      "epoch:13 step:10392 [D loss: 0.444398, acc.: 78.91%] [G loss: 3.380000]\n",
      "epoch:13 step:10393 [D loss: 0.400559, acc.: 80.47%] [G loss: 3.163941]\n",
      "epoch:13 step:10394 [D loss: 0.227746, acc.: 91.41%] [G loss: 2.946069]\n",
      "epoch:13 step:10395 [D loss: 0.271265, acc.: 89.06%] [G loss: 3.073911]\n",
      "epoch:13 step:10396 [D loss: 0.231188, acc.: 92.19%] [G loss: 2.611377]\n",
      "epoch:13 step:10397 [D loss: 0.310466, acc.: 88.28%] [G loss: 2.875306]\n",
      "epoch:13 step:10398 [D loss: 0.388492, acc.: 82.81%] [G loss: 4.269978]\n",
      "epoch:13 step:10399 [D loss: 0.318842, acc.: 86.72%] [G loss: 2.731105]\n",
      "epoch:13 step:10400 [D loss: 0.219658, acc.: 94.53%] [G loss: 3.323217]\n",
      "##############\n",
      "[0.83584572 0.87476222 0.80192186 0.79965616 0.76932667 0.82318574\n",
      " 0.86410903 0.86068106 0.81125282 0.79266611]\n",
      "##########\n",
      "epoch:13 step:10401 [D loss: 0.283667, acc.: 86.72%] [G loss: 2.887933]\n",
      "epoch:13 step:10402 [D loss: 0.311544, acc.: 82.81%] [G loss: 3.970238]\n",
      "epoch:13 step:10403 [D loss: 0.253835, acc.: 85.94%] [G loss: 3.319634]\n",
      "epoch:13 step:10404 [D loss: 0.259301, acc.: 92.19%] [G loss: 3.444933]\n",
      "epoch:13 step:10405 [D loss: 0.330943, acc.: 87.50%] [G loss: 2.435503]\n",
      "epoch:13 step:10406 [D loss: 0.295860, acc.: 89.06%] [G loss: 3.513562]\n",
      "epoch:13 step:10407 [D loss: 0.236471, acc.: 89.84%] [G loss: 4.046895]\n",
      "epoch:13 step:10408 [D loss: 0.319752, acc.: 86.72%] [G loss: 3.593374]\n",
      "epoch:13 step:10409 [D loss: 0.275265, acc.: 86.72%] [G loss: 4.223511]\n",
      "epoch:13 step:10410 [D loss: 0.235644, acc.: 92.19%] [G loss: 4.229726]\n",
      "epoch:13 step:10411 [D loss: 0.330497, acc.: 85.16%] [G loss: 3.371741]\n",
      "epoch:13 step:10412 [D loss: 0.294221, acc.: 88.28%] [G loss: 3.271604]\n",
      "epoch:13 step:10413 [D loss: 0.337856, acc.: 84.38%] [G loss: 2.734029]\n",
      "epoch:13 step:10414 [D loss: 0.307181, acc.: 88.28%] [G loss: 2.584913]\n",
      "epoch:13 step:10415 [D loss: 0.308418, acc.: 88.28%] [G loss: 3.064283]\n",
      "epoch:13 step:10416 [D loss: 0.243689, acc.: 89.06%] [G loss: 3.686624]\n",
      "epoch:13 step:10417 [D loss: 0.219577, acc.: 91.41%] [G loss: 3.958064]\n",
      "epoch:13 step:10418 [D loss: 0.256391, acc.: 89.06%] [G loss: 3.914050]\n",
      "epoch:13 step:10419 [D loss: 0.321043, acc.: 85.16%] [G loss: 2.327806]\n",
      "epoch:13 step:10420 [D loss: 0.446197, acc.: 75.78%] [G loss: 3.174196]\n",
      "epoch:13 step:10421 [D loss: 0.254693, acc.: 92.19%] [G loss: 2.298911]\n",
      "epoch:13 step:10422 [D loss: 0.347079, acc.: 87.50%] [G loss: 2.960290]\n",
      "epoch:13 step:10423 [D loss: 0.347446, acc.: 85.16%] [G loss: 2.591744]\n",
      "epoch:13 step:10424 [D loss: 0.269474, acc.: 89.06%] [G loss: 2.860109]\n",
      "epoch:13 step:10425 [D loss: 0.285008, acc.: 89.06%] [G loss: 3.498319]\n",
      "epoch:13 step:10426 [D loss: 0.325024, acc.: 85.16%] [G loss: 3.178026]\n",
      "epoch:13 step:10427 [D loss: 0.350851, acc.: 84.38%] [G loss: 3.027874]\n",
      "epoch:13 step:10428 [D loss: 0.359014, acc.: 85.16%] [G loss: 2.237548]\n",
      "epoch:13 step:10429 [D loss: 0.284524, acc.: 86.72%] [G loss: 2.861014]\n",
      "epoch:13 step:10430 [D loss: 0.294671, acc.: 86.72%] [G loss: 4.909516]\n",
      "epoch:13 step:10431 [D loss: 0.317095, acc.: 87.50%] [G loss: 3.005502]\n",
      "epoch:13 step:10432 [D loss: 0.431861, acc.: 81.25%] [G loss: 3.327434]\n",
      "epoch:13 step:10433 [D loss: 0.205723, acc.: 92.97%] [G loss: 3.223909]\n",
      "epoch:13 step:10434 [D loss: 0.280446, acc.: 86.72%] [G loss: 3.327764]\n",
      "epoch:13 step:10435 [D loss: 0.277919, acc.: 89.06%] [G loss: 3.805462]\n",
      "epoch:13 step:10436 [D loss: 0.311401, acc.: 85.94%] [G loss: 4.399795]\n",
      "epoch:13 step:10437 [D loss: 0.276209, acc.: 87.50%] [G loss: 3.933598]\n",
      "epoch:13 step:10438 [D loss: 0.420343, acc.: 81.25%] [G loss: 2.618049]\n",
      "epoch:13 step:10439 [D loss: 0.368042, acc.: 83.59%] [G loss: 2.768142]\n",
      "epoch:13 step:10440 [D loss: 0.265957, acc.: 87.50%] [G loss: 3.599926]\n",
      "epoch:13 step:10441 [D loss: 0.228237, acc.: 90.62%] [G loss: 4.187436]\n",
      "epoch:13 step:10442 [D loss: 0.385732, acc.: 79.69%] [G loss: 2.873960]\n",
      "epoch:13 step:10443 [D loss: 0.377409, acc.: 84.38%] [G loss: 3.392619]\n",
      "epoch:13 step:10444 [D loss: 0.416341, acc.: 81.25%] [G loss: 2.834111]\n",
      "epoch:13 step:10445 [D loss: 0.279878, acc.: 87.50%] [G loss: 3.396466]\n",
      "epoch:13 step:10446 [D loss: 0.328137, acc.: 83.59%] [G loss: 2.409929]\n",
      "epoch:13 step:10447 [D loss: 0.400392, acc.: 82.81%] [G loss: 2.922702]\n",
      "epoch:13 step:10448 [D loss: 0.470634, acc.: 79.69%] [G loss: 3.752316]\n",
      "epoch:13 step:10449 [D loss: 0.451019, acc.: 79.69%] [G loss: 3.792189]\n",
      "epoch:13 step:10450 [D loss: 0.488613, acc.: 76.56%] [G loss: 3.683604]\n",
      "epoch:13 step:10451 [D loss: 0.337550, acc.: 84.38%] [G loss: 5.853620]\n",
      "epoch:13 step:10452 [D loss: 0.211110, acc.: 92.19%] [G loss: 4.940764]\n",
      "epoch:13 step:10453 [D loss: 0.373177, acc.: 82.03%] [G loss: 2.892453]\n",
      "epoch:13 step:10454 [D loss: 0.378786, acc.: 86.72%] [G loss: 2.712887]\n",
      "epoch:13 step:10455 [D loss: 0.169786, acc.: 96.09%] [G loss: 4.691144]\n",
      "epoch:13 step:10456 [D loss: 0.247375, acc.: 89.84%] [G loss: 3.437933]\n",
      "epoch:13 step:10457 [D loss: 0.330637, acc.: 84.38%] [G loss: 3.567973]\n",
      "epoch:13 step:10458 [D loss: 0.546241, acc.: 76.56%] [G loss: 3.069130]\n",
      "epoch:13 step:10459 [D loss: 0.308929, acc.: 86.72%] [G loss: 4.755515]\n",
      "epoch:13 step:10460 [D loss: 0.248014, acc.: 89.06%] [G loss: 4.246097]\n",
      "epoch:13 step:10461 [D loss: 0.373790, acc.: 82.81%] [G loss: 3.061957]\n",
      "epoch:13 step:10462 [D loss: 0.475110, acc.: 81.25%] [G loss: 4.002360]\n",
      "epoch:13 step:10463 [D loss: 0.472163, acc.: 76.56%] [G loss: 4.109907]\n",
      "epoch:13 step:10464 [D loss: 0.429584, acc.: 80.47%] [G loss: 2.775171]\n",
      "epoch:13 step:10465 [D loss: 0.326127, acc.: 86.72%] [G loss: 3.522677]\n",
      "epoch:13 step:10466 [D loss: 0.309277, acc.: 88.28%] [G loss: 3.465630]\n",
      "epoch:13 step:10467 [D loss: 0.359388, acc.: 85.16%] [G loss: 3.526066]\n",
      "epoch:13 step:10468 [D loss: 0.260552, acc.: 88.28%] [G loss: 3.568301]\n",
      "epoch:13 step:10469 [D loss: 0.420284, acc.: 84.38%] [G loss: 3.730267]\n",
      "epoch:13 step:10470 [D loss: 0.455853, acc.: 81.25%] [G loss: 2.701722]\n",
      "epoch:13 step:10471 [D loss: 0.376290, acc.: 84.38%] [G loss: 3.069513]\n",
      "epoch:13 step:10472 [D loss: 0.257570, acc.: 90.62%] [G loss: 3.220895]\n",
      "epoch:13 step:10473 [D loss: 0.316312, acc.: 84.38%] [G loss: 3.589572]\n",
      "epoch:13 step:10474 [D loss: 0.291378, acc.: 85.94%] [G loss: 3.770046]\n",
      "epoch:13 step:10475 [D loss: 0.213819, acc.: 92.97%] [G loss: 6.967074]\n",
      "epoch:13 step:10476 [D loss: 0.311875, acc.: 82.81%] [G loss: 3.186213]\n",
      "epoch:13 step:10477 [D loss: 0.374002, acc.: 83.59%] [G loss: 3.660347]\n",
      "epoch:13 step:10478 [D loss: 0.279789, acc.: 86.72%] [G loss: 3.317735]\n",
      "epoch:13 step:10479 [D loss: 0.251134, acc.: 92.97%] [G loss: 4.366134]\n",
      "epoch:13 step:10480 [D loss: 0.436761, acc.: 80.47%] [G loss: 4.912748]\n",
      "epoch:13 step:10481 [D loss: 0.636715, acc.: 73.44%] [G loss: 6.683088]\n",
      "epoch:13 step:10482 [D loss: 0.855068, acc.: 69.53%] [G loss: 5.760645]\n",
      "epoch:13 step:10483 [D loss: 1.433045, acc.: 62.50%] [G loss: 6.553530]\n",
      "epoch:13 step:10484 [D loss: 1.377142, acc.: 56.25%] [G loss: 5.770847]\n",
      "epoch:13 step:10485 [D loss: 0.855545, acc.: 77.34%] [G loss: 4.177435]\n",
      "epoch:13 step:10486 [D loss: 0.455790, acc.: 81.25%] [G loss: 3.336386]\n",
      "epoch:13 step:10487 [D loss: 0.570797, acc.: 72.66%] [G loss: 2.509537]\n",
      "epoch:13 step:10488 [D loss: 0.315630, acc.: 87.50%] [G loss: 3.073315]\n",
      "epoch:13 step:10489 [D loss: 0.243919, acc.: 88.28%] [G loss: 4.406380]\n",
      "epoch:13 step:10490 [D loss: 0.292014, acc.: 89.84%] [G loss: 2.185679]\n",
      "epoch:13 step:10491 [D loss: 0.515476, acc.: 78.91%] [G loss: 3.081959]\n",
      "epoch:13 step:10492 [D loss: 0.207475, acc.: 91.41%] [G loss: 4.036098]\n",
      "epoch:13 step:10493 [D loss: 0.421492, acc.: 76.56%] [G loss: 2.696847]\n",
      "epoch:13 step:10494 [D loss: 0.247932, acc.: 92.97%] [G loss: 2.487174]\n",
      "epoch:13 step:10495 [D loss: 0.537399, acc.: 75.00%] [G loss: 3.773838]\n",
      "epoch:13 step:10496 [D loss: 0.351243, acc.: 83.59%] [G loss: 3.229245]\n",
      "epoch:13 step:10497 [D loss: 0.353365, acc.: 81.25%] [G loss: 4.425056]\n",
      "epoch:13 step:10498 [D loss: 0.342137, acc.: 85.16%] [G loss: 2.572834]\n",
      "epoch:13 step:10499 [D loss: 0.339995, acc.: 87.50%] [G loss: 4.744147]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:13 step:10500 [D loss: 0.245034, acc.: 89.84%] [G loss: 3.287612]\n",
      "epoch:13 step:10501 [D loss: 0.383827, acc.: 83.59%] [G loss: 4.189604]\n",
      "epoch:13 step:10502 [D loss: 0.196643, acc.: 92.97%] [G loss: 5.614245]\n",
      "epoch:13 step:10503 [D loss: 0.383451, acc.: 78.91%] [G loss: 3.431805]\n",
      "epoch:13 step:10504 [D loss: 0.366359, acc.: 79.69%] [G loss: 3.823387]\n",
      "epoch:13 step:10505 [D loss: 0.352079, acc.: 84.38%] [G loss: 3.029285]\n",
      "epoch:13 step:10506 [D loss: 0.275209, acc.: 87.50%] [G loss: 3.287692]\n",
      "epoch:13 step:10507 [D loss: 0.327967, acc.: 89.06%] [G loss: 4.275867]\n",
      "epoch:13 step:10508 [D loss: 0.436222, acc.: 76.56%] [G loss: 2.574460]\n",
      "epoch:13 step:10509 [D loss: 0.289821, acc.: 88.28%] [G loss: 3.487505]\n",
      "epoch:13 step:10510 [D loss: 0.272348, acc.: 90.62%] [G loss: 3.307474]\n",
      "epoch:13 step:10511 [D loss: 0.279601, acc.: 87.50%] [G loss: 3.168415]\n",
      "epoch:13 step:10512 [D loss: 0.324913, acc.: 89.84%] [G loss: 3.001228]\n",
      "epoch:13 step:10513 [D loss: 0.311097, acc.: 82.81%] [G loss: 3.583215]\n",
      "epoch:13 step:10514 [D loss: 0.228916, acc.: 91.41%] [G loss: 3.142267]\n",
      "epoch:13 step:10515 [D loss: 0.455196, acc.: 76.56%] [G loss: 3.505996]\n",
      "epoch:13 step:10516 [D loss: 0.360413, acc.: 85.16%] [G loss: 3.248003]\n",
      "epoch:13 step:10517 [D loss: 0.367594, acc.: 82.03%] [G loss: 2.969362]\n",
      "epoch:13 step:10518 [D loss: 0.427380, acc.: 82.81%] [G loss: 4.561542]\n",
      "epoch:13 step:10519 [D loss: 0.322253, acc.: 86.72%] [G loss: 3.764277]\n",
      "epoch:13 step:10520 [D loss: 0.341922, acc.: 83.59%] [G loss: 2.555523]\n",
      "epoch:13 step:10521 [D loss: 0.354582, acc.: 83.59%] [G loss: 2.682763]\n",
      "epoch:13 step:10522 [D loss: 0.488705, acc.: 77.34%] [G loss: 4.281729]\n",
      "epoch:13 step:10523 [D loss: 0.394974, acc.: 84.38%] [G loss: 2.984477]\n",
      "epoch:13 step:10524 [D loss: 0.247560, acc.: 91.41%] [G loss: 3.434456]\n",
      "epoch:13 step:10525 [D loss: 0.246338, acc.: 92.19%] [G loss: 3.807485]\n",
      "epoch:13 step:10526 [D loss: 0.272590, acc.: 85.94%] [G loss: 5.324973]\n",
      "epoch:13 step:10527 [D loss: 0.443674, acc.: 82.03%] [G loss: 2.740218]\n",
      "epoch:13 step:10528 [D loss: 0.480740, acc.: 74.22%] [G loss: 2.794539]\n",
      "epoch:13 step:10529 [D loss: 0.362345, acc.: 83.59%] [G loss: 3.538014]\n",
      "epoch:13 step:10530 [D loss: 0.329949, acc.: 86.72%] [G loss: 3.770785]\n",
      "epoch:13 step:10531 [D loss: 0.202799, acc.: 92.97%] [G loss: 5.563261]\n",
      "epoch:13 step:10532 [D loss: 0.217006, acc.: 91.41%] [G loss: 3.915560]\n",
      "epoch:13 step:10533 [D loss: 0.355959, acc.: 84.38%] [G loss: 2.659905]\n",
      "epoch:13 step:10534 [D loss: 0.289499, acc.: 89.84%] [G loss: 3.346096]\n",
      "epoch:13 step:10535 [D loss: 0.322555, acc.: 84.38%] [G loss: 3.671002]\n",
      "epoch:13 step:10536 [D loss: 0.342375, acc.: 84.38%] [G loss: 3.449527]\n",
      "epoch:13 step:10537 [D loss: 0.280682, acc.: 91.41%] [G loss: 4.538358]\n",
      "epoch:13 step:10538 [D loss: 0.205624, acc.: 92.97%] [G loss: 4.275209]\n",
      "epoch:13 step:10539 [D loss: 0.393494, acc.: 81.25%] [G loss: 2.497334]\n",
      "epoch:13 step:10540 [D loss: 0.236001, acc.: 94.53%] [G loss: 3.978160]\n",
      "epoch:13 step:10541 [D loss: 0.345857, acc.: 83.59%] [G loss: 3.908747]\n",
      "epoch:13 step:10542 [D loss: 0.388396, acc.: 82.81%] [G loss: 3.327878]\n",
      "epoch:13 step:10543 [D loss: 0.318574, acc.: 87.50%] [G loss: 3.149119]\n",
      "epoch:13 step:10544 [D loss: 0.334743, acc.: 78.91%] [G loss: 3.498312]\n",
      "epoch:13 step:10545 [D loss: 0.354757, acc.: 85.94%] [G loss: 2.394775]\n",
      "epoch:13 step:10546 [D loss: 0.329241, acc.: 84.38%] [G loss: 2.816145]\n",
      "epoch:13 step:10547 [D loss: 0.271560, acc.: 90.62%] [G loss: 2.999058]\n",
      "epoch:13 step:10548 [D loss: 0.309358, acc.: 84.38%] [G loss: 3.825310]\n",
      "epoch:13 step:10549 [D loss: 0.330210, acc.: 85.16%] [G loss: 3.872775]\n",
      "epoch:13 step:10550 [D loss: 0.354878, acc.: 83.59%] [G loss: 2.854977]\n",
      "epoch:13 step:10551 [D loss: 0.325657, acc.: 83.59%] [G loss: 2.871686]\n",
      "epoch:13 step:10552 [D loss: 0.357624, acc.: 82.03%] [G loss: 2.794886]\n",
      "epoch:13 step:10553 [D loss: 0.381741, acc.: 85.16%] [G loss: 3.907172]\n",
      "epoch:13 step:10554 [D loss: 0.256342, acc.: 89.84%] [G loss: 4.312282]\n",
      "epoch:13 step:10555 [D loss: 0.239681, acc.: 89.84%] [G loss: 3.083923]\n",
      "epoch:13 step:10556 [D loss: 0.322756, acc.: 89.84%] [G loss: 2.649016]\n",
      "epoch:13 step:10557 [D loss: 0.319736, acc.: 86.72%] [G loss: 2.524769]\n",
      "epoch:13 step:10558 [D loss: 0.334168, acc.: 84.38%] [G loss: 3.273426]\n",
      "epoch:13 step:10559 [D loss: 0.407784, acc.: 81.25%] [G loss: 3.990616]\n",
      "epoch:13 step:10560 [D loss: 0.356145, acc.: 85.94%] [G loss: 2.244252]\n",
      "epoch:13 step:10561 [D loss: 0.270595, acc.: 87.50%] [G loss: 3.031316]\n",
      "epoch:13 step:10562 [D loss: 0.321708, acc.: 85.94%] [G loss: 2.734778]\n",
      "epoch:13 step:10563 [D loss: 0.264470, acc.: 88.28%] [G loss: 3.197061]\n",
      "epoch:13 step:10564 [D loss: 0.335136, acc.: 84.38%] [G loss: 2.512231]\n",
      "epoch:13 step:10565 [D loss: 0.397983, acc.: 82.03%] [G loss: 2.380121]\n",
      "epoch:13 step:10566 [D loss: 0.256779, acc.: 91.41%] [G loss: 2.536449]\n",
      "epoch:13 step:10567 [D loss: 0.264752, acc.: 89.06%] [G loss: 2.712836]\n",
      "epoch:13 step:10568 [D loss: 0.296971, acc.: 89.84%] [G loss: 3.499833]\n",
      "epoch:13 step:10569 [D loss: 0.296585, acc.: 85.16%] [G loss: 3.090589]\n",
      "epoch:13 step:10570 [D loss: 0.286967, acc.: 88.28%] [G loss: 2.815320]\n",
      "epoch:13 step:10571 [D loss: 0.351686, acc.: 80.47%] [G loss: 3.490028]\n",
      "epoch:13 step:10572 [D loss: 0.363642, acc.: 81.25%] [G loss: 3.392336]\n",
      "epoch:13 step:10573 [D loss: 0.294703, acc.: 85.94%] [G loss: 3.508563]\n",
      "epoch:13 step:10574 [D loss: 0.257083, acc.: 88.28%] [G loss: 3.862512]\n",
      "epoch:13 step:10575 [D loss: 0.213796, acc.: 90.62%] [G loss: 5.591264]\n",
      "epoch:13 step:10576 [D loss: 0.295761, acc.: 84.38%] [G loss: 3.199943]\n",
      "epoch:13 step:10577 [D loss: 0.298141, acc.: 88.28%] [G loss: 3.822596]\n",
      "epoch:13 step:10578 [D loss: 0.345237, acc.: 87.50%] [G loss: 3.402178]\n",
      "epoch:13 step:10579 [D loss: 0.320347, acc.: 85.94%] [G loss: 2.903790]\n",
      "epoch:13 step:10580 [D loss: 0.275639, acc.: 89.84%] [G loss: 3.917681]\n",
      "epoch:13 step:10581 [D loss: 0.301226, acc.: 84.38%] [G loss: 5.319434]\n",
      "epoch:13 step:10582 [D loss: 0.297530, acc.: 87.50%] [G loss: 3.021103]\n",
      "epoch:13 step:10583 [D loss: 0.293899, acc.: 89.06%] [G loss: 3.401228]\n",
      "epoch:13 step:10584 [D loss: 0.273714, acc.: 86.72%] [G loss: 3.674676]\n",
      "epoch:13 step:10585 [D loss: 0.322861, acc.: 87.50%] [G loss: 2.954172]\n",
      "epoch:13 step:10586 [D loss: 0.282927, acc.: 90.62%] [G loss: 2.502733]\n",
      "epoch:13 step:10587 [D loss: 0.378344, acc.: 83.59%] [G loss: 2.834886]\n",
      "epoch:13 step:10588 [D loss: 0.370017, acc.: 82.03%] [G loss: 3.115461]\n",
      "epoch:13 step:10589 [D loss: 0.413801, acc.: 83.59%] [G loss: 3.988957]\n",
      "epoch:13 step:10590 [D loss: 0.302460, acc.: 83.59%] [G loss: 4.867599]\n",
      "epoch:13 step:10591 [D loss: 0.360845, acc.: 79.69%] [G loss: 3.434010]\n",
      "epoch:13 step:10592 [D loss: 0.362424, acc.: 84.38%] [G loss: 2.449975]\n",
      "epoch:13 step:10593 [D loss: 0.393615, acc.: 84.38%] [G loss: 3.394390]\n",
      "epoch:13 step:10594 [D loss: 0.262947, acc.: 89.84%] [G loss: 3.591019]\n",
      "epoch:13 step:10595 [D loss: 0.288467, acc.: 89.84%] [G loss: 3.181529]\n",
      "epoch:13 step:10596 [D loss: 0.309151, acc.: 88.28%] [G loss: 3.669344]\n",
      "epoch:13 step:10597 [D loss: 0.362757, acc.: 82.03%] [G loss: 3.301473]\n",
      "epoch:13 step:10598 [D loss: 0.299966, acc.: 85.94%] [G loss: 2.933838]\n",
      "epoch:13 step:10599 [D loss: 0.268757, acc.: 90.62%] [G loss: 3.063165]\n",
      "epoch:13 step:10600 [D loss: 0.177408, acc.: 93.75%] [G loss: 3.558690]\n",
      "##############\n",
      "[0.85593551 0.85955085 0.78725096 0.796653   0.75563763 0.81315251\n",
      " 0.87418182 0.84687936 0.81076157 0.80485945]\n",
      "##########\n",
      "epoch:13 step:10601 [D loss: 0.304297, acc.: 88.28%] [G loss: 4.985337]\n",
      "epoch:13 step:10602 [D loss: 0.365194, acc.: 88.28%] [G loss: 3.046176]\n",
      "epoch:13 step:10603 [D loss: 0.294793, acc.: 90.62%] [G loss: 2.925484]\n",
      "epoch:13 step:10604 [D loss: 0.214146, acc.: 89.06%] [G loss: 4.378422]\n",
      "epoch:13 step:10605 [D loss: 0.278204, acc.: 86.72%] [G loss: 3.962668]\n",
      "epoch:13 step:10606 [D loss: 0.270867, acc.: 85.94%] [G loss: 3.013121]\n",
      "epoch:13 step:10607 [D loss: 0.301430, acc.: 89.84%] [G loss: 3.419880]\n",
      "epoch:13 step:10608 [D loss: 0.289036, acc.: 89.06%] [G loss: 3.027942]\n",
      "epoch:13 step:10609 [D loss: 0.300282, acc.: 88.28%] [G loss: 4.459061]\n",
      "epoch:13 step:10610 [D loss: 0.353238, acc.: 83.59%] [G loss: 3.693568]\n",
      "epoch:13 step:10611 [D loss: 0.276089, acc.: 88.28%] [G loss: 3.270348]\n",
      "epoch:13 step:10612 [D loss: 0.268498, acc.: 88.28%] [G loss: 3.339059]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:13 step:10613 [D loss: 0.302758, acc.: 85.94%] [G loss: 3.338191]\n",
      "epoch:13 step:10614 [D loss: 0.255599, acc.: 91.41%] [G loss: 2.742889]\n",
      "epoch:13 step:10615 [D loss: 0.338990, acc.: 85.16%] [G loss: 2.342479]\n",
      "epoch:13 step:10616 [D loss: 0.283883, acc.: 85.16%] [G loss: 2.707620]\n",
      "epoch:13 step:10617 [D loss: 0.303645, acc.: 85.94%] [G loss: 2.466781]\n",
      "epoch:13 step:10618 [D loss: 0.301926, acc.: 87.50%] [G loss: 2.345144]\n",
      "epoch:13 step:10619 [D loss: 0.306724, acc.: 83.59%] [G loss: 2.179397]\n",
      "epoch:13 step:10620 [D loss: 0.367221, acc.: 83.59%] [G loss: 2.919031]\n",
      "epoch:13 step:10621 [D loss: 0.332497, acc.: 85.16%] [G loss: 2.093848]\n",
      "epoch:13 step:10622 [D loss: 0.274849, acc.: 90.62%] [G loss: 3.166956]\n",
      "epoch:13 step:10623 [D loss: 0.378374, acc.: 85.94%] [G loss: 3.030789]\n",
      "epoch:13 step:10624 [D loss: 0.235280, acc.: 89.06%] [G loss: 3.514975]\n",
      "epoch:13 step:10625 [D loss: 0.298414, acc.: 85.94%] [G loss: 2.907700]\n",
      "epoch:13 step:10626 [D loss: 0.255801, acc.: 89.84%] [G loss: 3.727539]\n",
      "epoch:13 step:10627 [D loss: 0.264890, acc.: 88.28%] [G loss: 6.387562]\n",
      "epoch:13 step:10628 [D loss: 0.405533, acc.: 81.25%] [G loss: 3.699872]\n",
      "epoch:13 step:10629 [D loss: 0.299662, acc.: 86.72%] [G loss: 3.609468]\n",
      "epoch:13 step:10630 [D loss: 0.213099, acc.: 92.97%] [G loss: 4.660341]\n",
      "epoch:13 step:10631 [D loss: 0.292886, acc.: 87.50%] [G loss: 3.660004]\n",
      "epoch:13 step:10632 [D loss: 0.339059, acc.: 87.50%] [G loss: 4.331457]\n",
      "epoch:13 step:10633 [D loss: 0.239271, acc.: 89.06%] [G loss: 4.479334]\n",
      "epoch:13 step:10634 [D loss: 0.439894, acc.: 85.16%] [G loss: 6.973002]\n",
      "epoch:13 step:10635 [D loss: 0.745360, acc.: 71.88%] [G loss: 4.148357]\n",
      "epoch:13 step:10636 [D loss: 0.393937, acc.: 86.72%] [G loss: 3.754402]\n",
      "epoch:13 step:10637 [D loss: 0.261283, acc.: 89.06%] [G loss: 4.369770]\n",
      "epoch:13 step:10638 [D loss: 0.294857, acc.: 88.28%] [G loss: 4.169670]\n",
      "epoch:13 step:10639 [D loss: 0.283577, acc.: 87.50%] [G loss: 5.370799]\n",
      "epoch:13 step:10640 [D loss: 0.244441, acc.: 89.06%] [G loss: 4.831276]\n",
      "epoch:13 step:10641 [D loss: 0.336565, acc.: 87.50%] [G loss: 2.498195]\n",
      "epoch:13 step:10642 [D loss: 0.271731, acc.: 87.50%] [G loss: 3.735053]\n",
      "epoch:13 step:10643 [D loss: 0.236147, acc.: 89.84%] [G loss: 3.026965]\n",
      "epoch:13 step:10644 [D loss: 0.271647, acc.: 88.28%] [G loss: 3.251125]\n",
      "epoch:13 step:10645 [D loss: 0.367369, acc.: 82.03%] [G loss: 3.750534]\n",
      "epoch:13 step:10646 [D loss: 0.283864, acc.: 87.50%] [G loss: 4.678701]\n",
      "epoch:13 step:10647 [D loss: 0.310212, acc.: 88.28%] [G loss: 2.885382]\n",
      "epoch:13 step:10648 [D loss: 0.259781, acc.: 92.19%] [G loss: 3.745749]\n",
      "epoch:13 step:10649 [D loss: 0.342681, acc.: 87.50%] [G loss: 3.106356]\n",
      "epoch:13 step:10650 [D loss: 0.251848, acc.: 88.28%] [G loss: 2.767928]\n",
      "epoch:13 step:10651 [D loss: 0.253659, acc.: 92.19%] [G loss: 2.826133]\n",
      "epoch:13 step:10652 [D loss: 0.305367, acc.: 88.28%] [G loss: 2.429413]\n",
      "epoch:13 step:10653 [D loss: 0.321353, acc.: 85.16%] [G loss: 2.908508]\n",
      "epoch:13 step:10654 [D loss: 0.351234, acc.: 83.59%] [G loss: 2.768512]\n",
      "epoch:13 step:10655 [D loss: 0.379415, acc.: 84.38%] [G loss: 3.692910]\n",
      "epoch:13 step:10656 [D loss: 0.272117, acc.: 87.50%] [G loss: 3.073099]\n",
      "epoch:13 step:10657 [D loss: 0.365106, acc.: 86.72%] [G loss: 2.484065]\n",
      "epoch:13 step:10658 [D loss: 0.385288, acc.: 82.03%] [G loss: 2.555946]\n",
      "epoch:13 step:10659 [D loss: 0.294810, acc.: 89.84%] [G loss: 2.961716]\n",
      "epoch:13 step:10660 [D loss: 0.356159, acc.: 85.16%] [G loss: 2.823143]\n",
      "epoch:13 step:10661 [D loss: 0.318555, acc.: 85.16%] [G loss: 3.848762]\n",
      "epoch:13 step:10662 [D loss: 0.328555, acc.: 84.38%] [G loss: 2.833952]\n",
      "epoch:13 step:10663 [D loss: 0.200115, acc.: 92.19%] [G loss: 6.903058]\n",
      "epoch:13 step:10664 [D loss: 0.279753, acc.: 88.28%] [G loss: 2.671535]\n",
      "epoch:13 step:10665 [D loss: 0.301154, acc.: 87.50%] [G loss: 3.023798]\n",
      "epoch:13 step:10666 [D loss: 0.278797, acc.: 91.41%] [G loss: 3.679708]\n",
      "epoch:13 step:10667 [D loss: 0.333307, acc.: 84.38%] [G loss: 5.015309]\n",
      "epoch:13 step:10668 [D loss: 0.209286, acc.: 90.62%] [G loss: 4.885466]\n",
      "epoch:13 step:10669 [D loss: 0.304810, acc.: 85.16%] [G loss: 4.018486]\n",
      "epoch:13 step:10670 [D loss: 0.314278, acc.: 86.72%] [G loss: 3.345719]\n",
      "epoch:13 step:10671 [D loss: 0.288373, acc.: 89.06%] [G loss: 3.449513]\n",
      "epoch:13 step:10672 [D loss: 0.283891, acc.: 89.06%] [G loss: 4.104015]\n",
      "epoch:13 step:10673 [D loss: 0.304272, acc.: 86.72%] [G loss: 4.340077]\n",
      "epoch:13 step:10674 [D loss: 0.283116, acc.: 91.41%] [G loss: 4.026623]\n",
      "epoch:13 step:10675 [D loss: 0.287650, acc.: 86.72%] [G loss: 3.557024]\n",
      "epoch:13 step:10676 [D loss: 0.314955, acc.: 87.50%] [G loss: 3.076545]\n",
      "epoch:13 step:10677 [D loss: 0.329627, acc.: 83.59%] [G loss: 3.163136]\n",
      "epoch:13 step:10678 [D loss: 0.260848, acc.: 90.62%] [G loss: 3.601982]\n",
      "epoch:13 step:10679 [D loss: 0.368551, acc.: 84.38%] [G loss: 3.081242]\n",
      "epoch:13 step:10680 [D loss: 0.434080, acc.: 81.25%] [G loss: 2.839590]\n",
      "epoch:13 step:10681 [D loss: 0.276985, acc.: 88.28%] [G loss: 3.300298]\n",
      "epoch:13 step:10682 [D loss: 0.454327, acc.: 78.12%] [G loss: 2.948534]\n",
      "epoch:13 step:10683 [D loss: 0.437592, acc.: 77.34%] [G loss: 4.409611]\n",
      "epoch:13 step:10684 [D loss: 0.357351, acc.: 85.16%] [G loss: 4.832282]\n",
      "epoch:13 step:10685 [D loss: 0.462980, acc.: 75.78%] [G loss: 2.912886]\n",
      "epoch:13 step:10686 [D loss: 0.380235, acc.: 83.59%] [G loss: 3.376132]\n",
      "epoch:13 step:10687 [D loss: 0.480266, acc.: 78.12%] [G loss: 3.596802]\n",
      "epoch:13 step:10688 [D loss: 0.579150, acc.: 75.78%] [G loss: 5.119407]\n",
      "epoch:13 step:10689 [D loss: 0.529460, acc.: 79.69%] [G loss: 5.744824]\n",
      "epoch:13 step:10690 [D loss: 0.529427, acc.: 70.31%] [G loss: 2.957870]\n",
      "epoch:13 step:10691 [D loss: 0.262553, acc.: 89.06%] [G loss: 4.336162]\n",
      "epoch:13 step:10692 [D loss: 0.378870, acc.: 86.72%] [G loss: 3.251996]\n",
      "epoch:13 step:10693 [D loss: 0.345541, acc.: 83.59%] [G loss: 2.613670]\n",
      "epoch:13 step:10694 [D loss: 0.391921, acc.: 81.25%] [G loss: 2.628762]\n",
      "epoch:13 step:10695 [D loss: 0.317759, acc.: 89.84%] [G loss: 2.858562]\n",
      "epoch:13 step:10696 [D loss: 0.280448, acc.: 89.84%] [G loss: 3.136178]\n",
      "epoch:13 step:10697 [D loss: 0.293734, acc.: 86.72%] [G loss: 3.853742]\n",
      "epoch:13 step:10698 [D loss: 0.229116, acc.: 88.28%] [G loss: 3.398377]\n",
      "epoch:13 step:10699 [D loss: 0.339544, acc.: 86.72%] [G loss: 2.930054]\n",
      "epoch:13 step:10700 [D loss: 0.374381, acc.: 81.25%] [G loss: 3.183931]\n",
      "epoch:13 step:10701 [D loss: 0.238495, acc.: 89.06%] [G loss: 2.979135]\n",
      "epoch:13 step:10702 [D loss: 0.314376, acc.: 87.50%] [G loss: 3.856292]\n",
      "epoch:13 step:10703 [D loss: 0.267446, acc.: 90.62%] [G loss: 5.016201]\n",
      "epoch:13 step:10704 [D loss: 0.264882, acc.: 88.28%] [G loss: 3.749506]\n",
      "epoch:13 step:10705 [D loss: 0.356693, acc.: 85.16%] [G loss: 2.220804]\n",
      "epoch:13 step:10706 [D loss: 0.289783, acc.: 87.50%] [G loss: 3.038065]\n",
      "epoch:13 step:10707 [D loss: 0.300621, acc.: 87.50%] [G loss: 3.267879]\n",
      "epoch:13 step:10708 [D loss: 0.285632, acc.: 85.16%] [G loss: 4.083984]\n",
      "epoch:13 step:10709 [D loss: 0.253430, acc.: 90.62%] [G loss: 4.971782]\n",
      "epoch:13 step:10710 [D loss: 0.281658, acc.: 89.06%] [G loss: 3.234385]\n",
      "epoch:13 step:10711 [D loss: 0.279751, acc.: 87.50%] [G loss: 3.611029]\n",
      "epoch:13 step:10712 [D loss: 0.267899, acc.: 84.38%] [G loss: 3.427229]\n",
      "epoch:13 step:10713 [D loss: 0.308871, acc.: 86.72%] [G loss: 2.607270]\n",
      "epoch:13 step:10714 [D loss: 0.301420, acc.: 88.28%] [G loss: 3.382258]\n",
      "epoch:13 step:10715 [D loss: 0.390532, acc.: 85.94%] [G loss: 2.587234]\n",
      "epoch:13 step:10716 [D loss: 0.298399, acc.: 85.94%] [G loss: 2.826392]\n",
      "epoch:13 step:10717 [D loss: 0.266304, acc.: 88.28%] [G loss: 2.746688]\n",
      "epoch:13 step:10718 [D loss: 0.376208, acc.: 82.03%] [G loss: 2.586365]\n",
      "epoch:13 step:10719 [D loss: 0.265403, acc.: 89.06%] [G loss: 2.824607]\n",
      "epoch:13 step:10720 [D loss: 0.381374, acc.: 84.38%] [G loss: 3.125347]\n",
      "epoch:13 step:10721 [D loss: 0.272582, acc.: 91.41%] [G loss: 3.624421]\n",
      "epoch:13 step:10722 [D loss: 0.291919, acc.: 88.28%] [G loss: 3.272797]\n",
      "epoch:13 step:10723 [D loss: 0.349813, acc.: 84.38%] [G loss: 1.833048]\n",
      "epoch:13 step:10724 [D loss: 0.350180, acc.: 81.25%] [G loss: 2.987041]\n",
      "epoch:13 step:10725 [D loss: 0.288037, acc.: 86.72%] [G loss: 2.331552]\n",
      "epoch:13 step:10726 [D loss: 0.348716, acc.: 82.81%] [G loss: 3.062744]\n",
      "epoch:13 step:10727 [D loss: 0.349390, acc.: 90.62%] [G loss: 2.567132]\n",
      "epoch:13 step:10728 [D loss: 0.314031, acc.: 86.72%] [G loss: 3.639013]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:13 step:10729 [D loss: 0.231453, acc.: 94.53%] [G loss: 4.341280]\n",
      "epoch:13 step:10730 [D loss: 0.237915, acc.: 88.28%] [G loss: 4.328826]\n",
      "epoch:13 step:10731 [D loss: 0.298542, acc.: 87.50%] [G loss: 4.082730]\n",
      "epoch:13 step:10732 [D loss: 0.277903, acc.: 89.84%] [G loss: 3.374351]\n",
      "epoch:13 step:10733 [D loss: 0.286584, acc.: 89.06%] [G loss: 3.158141]\n",
      "epoch:13 step:10734 [D loss: 0.179521, acc.: 94.53%] [G loss: 5.925899]\n",
      "epoch:13 step:10735 [D loss: 0.314254, acc.: 86.72%] [G loss: 4.457542]\n",
      "epoch:13 step:10736 [D loss: 0.285879, acc.: 88.28%] [G loss: 4.891261]\n",
      "epoch:13 step:10737 [D loss: 0.303832, acc.: 86.72%] [G loss: 3.993033]\n",
      "epoch:13 step:10738 [D loss: 0.299192, acc.: 87.50%] [G loss: 3.531765]\n",
      "epoch:13 step:10739 [D loss: 0.339106, acc.: 85.94%] [G loss: 3.405548]\n",
      "epoch:13 step:10740 [D loss: 0.321610, acc.: 86.72%] [G loss: 3.994391]\n",
      "epoch:13 step:10741 [D loss: 0.369080, acc.: 82.81%] [G loss: 2.984871]\n",
      "epoch:13 step:10742 [D loss: 0.401912, acc.: 84.38%] [G loss: 3.706672]\n",
      "epoch:13 step:10743 [D loss: 0.381825, acc.: 81.25%] [G loss: 2.911078]\n",
      "epoch:13 step:10744 [D loss: 0.335202, acc.: 88.28%] [G loss: 3.236312]\n",
      "epoch:13 step:10745 [D loss: 0.221540, acc.: 94.53%] [G loss: 3.087314]\n",
      "epoch:13 step:10746 [D loss: 0.261240, acc.: 88.28%] [G loss: 3.023242]\n",
      "epoch:13 step:10747 [D loss: 0.398561, acc.: 82.03%] [G loss: 3.099140]\n",
      "epoch:13 step:10748 [D loss: 0.321637, acc.: 86.72%] [G loss: 2.532467]\n",
      "epoch:13 step:10749 [D loss: 0.329724, acc.: 79.69%] [G loss: 3.684299]\n",
      "epoch:13 step:10750 [D loss: 0.406082, acc.: 76.56%] [G loss: 3.225635]\n",
      "epoch:13 step:10751 [D loss: 0.418855, acc.: 81.25%] [G loss: 3.631155]\n",
      "epoch:13 step:10752 [D loss: 0.248136, acc.: 89.84%] [G loss: 4.772564]\n",
      "epoch:13 step:10753 [D loss: 0.237420, acc.: 91.41%] [G loss: 4.057077]\n",
      "epoch:13 step:10754 [D loss: 0.317136, acc.: 84.38%] [G loss: 2.887229]\n",
      "epoch:13 step:10755 [D loss: 0.397393, acc.: 79.69%] [G loss: 2.781097]\n",
      "epoch:13 step:10756 [D loss: 0.296381, acc.: 91.41%] [G loss: 3.554496]\n",
      "epoch:13 step:10757 [D loss: 0.418391, acc.: 79.69%] [G loss: 3.012092]\n",
      "epoch:13 step:10758 [D loss: 0.422417, acc.: 78.12%] [G loss: 2.879857]\n",
      "epoch:13 step:10759 [D loss: 0.399014, acc.: 81.25%] [G loss: 3.229762]\n",
      "epoch:13 step:10760 [D loss: 0.318595, acc.: 85.94%] [G loss: 3.626046]\n",
      "epoch:13 step:10761 [D loss: 0.259793, acc.: 88.28%] [G loss: 4.299199]\n",
      "epoch:13 step:10762 [D loss: 0.413876, acc.: 81.25%] [G loss: 6.612566]\n",
      "epoch:13 step:10763 [D loss: 0.665209, acc.: 71.09%] [G loss: 2.912998]\n",
      "epoch:13 step:10764 [D loss: 0.342587, acc.: 85.94%] [G loss: 3.879217]\n",
      "epoch:13 step:10765 [D loss: 0.427707, acc.: 78.12%] [G loss: 3.298537]\n",
      "epoch:13 step:10766 [D loss: 0.335185, acc.: 85.94%] [G loss: 4.310760]\n",
      "epoch:13 step:10767 [D loss: 0.236577, acc.: 89.06%] [G loss: 3.597289]\n",
      "epoch:13 step:10768 [D loss: 0.296799, acc.: 87.50%] [G loss: 2.889966]\n",
      "epoch:13 step:10769 [D loss: 0.288169, acc.: 89.84%] [G loss: 2.412778]\n",
      "epoch:13 step:10770 [D loss: 0.254620, acc.: 91.41%] [G loss: 2.857387]\n",
      "epoch:13 step:10771 [D loss: 0.338698, acc.: 85.94%] [G loss: 2.847401]\n",
      "epoch:13 step:10772 [D loss: 0.398617, acc.: 80.47%] [G loss: 2.880250]\n",
      "epoch:13 step:10773 [D loss: 0.220128, acc.: 90.62%] [G loss: 3.499437]\n",
      "epoch:13 step:10774 [D loss: 0.271598, acc.: 86.72%] [G loss: 3.616617]\n",
      "epoch:13 step:10775 [D loss: 0.294926, acc.: 89.84%] [G loss: 4.582461]\n",
      "epoch:13 step:10776 [D loss: 0.271422, acc.: 89.06%] [G loss: 3.635237]\n",
      "epoch:13 step:10777 [D loss: 0.219903, acc.: 89.84%] [G loss: 5.718632]\n",
      "epoch:13 step:10778 [D loss: 0.258650, acc.: 90.62%] [G loss: 4.462627]\n",
      "epoch:13 step:10779 [D loss: 0.246740, acc.: 90.62%] [G loss: 3.901194]\n",
      "epoch:13 step:10780 [D loss: 0.283422, acc.: 89.84%] [G loss: 4.356033]\n",
      "epoch:13 step:10781 [D loss: 0.329119, acc.: 85.16%] [G loss: 4.545053]\n",
      "epoch:13 step:10782 [D loss: 0.298954, acc.: 85.16%] [G loss: 3.207125]\n",
      "epoch:13 step:10783 [D loss: 0.207621, acc.: 92.19%] [G loss: 4.346665]\n",
      "epoch:13 step:10784 [D loss: 0.422706, acc.: 83.59%] [G loss: 3.018751]\n",
      "epoch:13 step:10785 [D loss: 0.223359, acc.: 90.62%] [G loss: 4.672877]\n",
      "epoch:13 step:10786 [D loss: 0.420625, acc.: 79.69%] [G loss: 4.308217]\n",
      "epoch:13 step:10787 [D loss: 0.368986, acc.: 82.81%] [G loss: 3.126029]\n",
      "epoch:13 step:10788 [D loss: 0.324564, acc.: 84.38%] [G loss: 2.394041]\n",
      "epoch:13 step:10789 [D loss: 0.266276, acc.: 89.06%] [G loss: 3.298723]\n",
      "epoch:13 step:10790 [D loss: 0.339762, acc.: 88.28%] [G loss: 4.386805]\n",
      "epoch:13 step:10791 [D loss: 0.443594, acc.: 85.16%] [G loss: 4.897819]\n",
      "epoch:13 step:10792 [D loss: 0.397653, acc.: 82.81%] [G loss: 2.537404]\n",
      "epoch:13 step:10793 [D loss: 0.338363, acc.: 87.50%] [G loss: 2.771426]\n",
      "epoch:13 step:10794 [D loss: 0.235711, acc.: 87.50%] [G loss: 3.041531]\n",
      "epoch:13 step:10795 [D loss: 0.287817, acc.: 88.28%] [G loss: 3.071615]\n",
      "epoch:13 step:10796 [D loss: 0.261572, acc.: 90.62%] [G loss: 2.826190]\n",
      "epoch:13 step:10797 [D loss: 0.344856, acc.: 86.72%] [G loss: 3.958982]\n",
      "epoch:13 step:10798 [D loss: 0.227498, acc.: 91.41%] [G loss: 3.768968]\n",
      "epoch:13 step:10799 [D loss: 0.322280, acc.: 84.38%] [G loss: 3.440564]\n",
      "epoch:13 step:10800 [D loss: 0.229885, acc.: 93.75%] [G loss: 3.029294]\n",
      "##############\n",
      "[0.84662553 0.88142866 0.79412637 0.79850563 0.77404369 0.81584388\n",
      " 0.84319218 0.82999869 0.84051545 0.82208687]\n",
      "##########\n",
      "epoch:13 step:10801 [D loss: 0.415194, acc.: 79.69%] [G loss: 2.791344]\n",
      "epoch:13 step:10802 [D loss: 0.330278, acc.: 85.16%] [G loss: 3.086835]\n",
      "epoch:13 step:10803 [D loss: 0.458657, acc.: 81.25%] [G loss: 2.990439]\n",
      "epoch:13 step:10804 [D loss: 0.370647, acc.: 82.03%] [G loss: 2.475443]\n",
      "epoch:13 step:10805 [D loss: 0.352774, acc.: 85.16%] [G loss: 2.866301]\n",
      "epoch:13 step:10806 [D loss: 0.310797, acc.: 85.16%] [G loss: 4.081366]\n",
      "epoch:13 step:10807 [D loss: 0.301356, acc.: 86.72%] [G loss: 2.835290]\n",
      "epoch:13 step:10808 [D loss: 0.387901, acc.: 82.03%] [G loss: 2.257868]\n",
      "epoch:13 step:10809 [D loss: 0.326353, acc.: 85.94%] [G loss: 2.995027]\n",
      "epoch:13 step:10810 [D loss: 0.329361, acc.: 85.94%] [G loss: 5.674849]\n",
      "epoch:13 step:10811 [D loss: 0.407048, acc.: 80.47%] [G loss: 4.387511]\n",
      "epoch:13 step:10812 [D loss: 0.395308, acc.: 85.16%] [G loss: 4.805521]\n",
      "epoch:13 step:10813 [D loss: 0.471505, acc.: 77.34%] [G loss: 5.631909]\n",
      "epoch:13 step:10814 [D loss: 0.363298, acc.: 83.59%] [G loss: 5.337534]\n",
      "epoch:13 step:10815 [D loss: 0.367891, acc.: 82.81%] [G loss: 3.489541]\n",
      "epoch:13 step:10816 [D loss: 0.425988, acc.: 83.59%] [G loss: 4.871106]\n",
      "epoch:13 step:10817 [D loss: 0.373957, acc.: 83.59%] [G loss: 2.993655]\n",
      "epoch:13 step:10818 [D loss: 0.410003, acc.: 85.16%] [G loss: 3.757741]\n",
      "epoch:13 step:10819 [D loss: 0.329867, acc.: 81.25%] [G loss: 4.465642]\n",
      "epoch:13 step:10820 [D loss: 0.309571, acc.: 85.16%] [G loss: 3.500791]\n",
      "epoch:13 step:10821 [D loss: 0.330762, acc.: 85.16%] [G loss: 4.229143]\n",
      "epoch:13 step:10822 [D loss: 0.493800, acc.: 82.03%] [G loss: 6.657150]\n",
      "epoch:13 step:10823 [D loss: 0.668835, acc.: 71.88%] [G loss: 5.118867]\n",
      "epoch:13 step:10824 [D loss: 1.030043, acc.: 53.91%] [G loss: 4.327518]\n",
      "epoch:13 step:10825 [D loss: 0.585558, acc.: 82.03%] [G loss: 3.215055]\n",
      "epoch:13 step:10826 [D loss: 0.419355, acc.: 79.69%] [G loss: 3.605121]\n",
      "epoch:13 step:10827 [D loss: 0.266000, acc.: 86.72%] [G loss: 5.519937]\n",
      "epoch:13 step:10828 [D loss: 0.441006, acc.: 78.91%] [G loss: 3.096609]\n",
      "epoch:13 step:10829 [D loss: 0.359978, acc.: 83.59%] [G loss: 2.410269]\n",
      "epoch:13 step:10830 [D loss: 0.313429, acc.: 84.38%] [G loss: 2.848812]\n",
      "epoch:13 step:10831 [D loss: 0.383579, acc.: 83.59%] [G loss: 2.952376]\n",
      "epoch:13 step:10832 [D loss: 0.247546, acc.: 92.19%] [G loss: 3.205761]\n",
      "epoch:13 step:10833 [D loss: 0.325472, acc.: 85.94%] [G loss: 2.794159]\n",
      "epoch:13 step:10834 [D loss: 0.333247, acc.: 89.06%] [G loss: 2.972562]\n",
      "epoch:13 step:10835 [D loss: 0.267746, acc.: 88.28%] [G loss: 4.287483]\n",
      "epoch:13 step:10836 [D loss: 0.254890, acc.: 89.84%] [G loss: 2.493933]\n",
      "epoch:13 step:10837 [D loss: 0.236302, acc.: 92.19%] [G loss: 4.132247]\n",
      "epoch:13 step:10838 [D loss: 0.314188, acc.: 83.59%] [G loss: 4.644255]\n",
      "epoch:13 step:10839 [D loss: 0.336603, acc.: 83.59%] [G loss: 3.038982]\n",
      "epoch:13 step:10840 [D loss: 0.260268, acc.: 88.28%] [G loss: 3.396947]\n",
      "epoch:13 step:10841 [D loss: 0.337638, acc.: 85.16%] [G loss: 4.373924]\n",
      "epoch:13 step:10842 [D loss: 0.282538, acc.: 85.94%] [G loss: 3.095682]\n",
      "epoch:13 step:10843 [D loss: 0.313232, acc.: 88.28%] [G loss: 2.474410]\n",
      "epoch:13 step:10844 [D loss: 0.280595, acc.: 89.84%] [G loss: 4.478920]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:13 step:10845 [D loss: 0.333394, acc.: 85.16%] [G loss: 5.057377]\n",
      "epoch:13 step:10846 [D loss: 0.346503, acc.: 83.59%] [G loss: 2.925744]\n",
      "epoch:13 step:10847 [D loss: 0.293959, acc.: 86.72%] [G loss: 3.087274]\n",
      "epoch:13 step:10848 [D loss: 0.339422, acc.: 85.16%] [G loss: 2.840903]\n",
      "epoch:13 step:10849 [D loss: 0.318102, acc.: 87.50%] [G loss: 2.222150]\n",
      "epoch:13 step:10850 [D loss: 0.274178, acc.: 89.06%] [G loss: 2.750750]\n",
      "epoch:13 step:10851 [D loss: 0.274546, acc.: 85.16%] [G loss: 2.728275]\n",
      "epoch:13 step:10852 [D loss: 0.299082, acc.: 87.50%] [G loss: 2.824668]\n",
      "epoch:13 step:10853 [D loss: 0.319979, acc.: 86.72%] [G loss: 3.160591]\n",
      "epoch:13 step:10854 [D loss: 0.414308, acc.: 82.81%] [G loss: 3.257591]\n",
      "epoch:13 step:10855 [D loss: 0.293003, acc.: 86.72%] [G loss: 3.960197]\n",
      "epoch:13 step:10856 [D loss: 0.379127, acc.: 78.12%] [G loss: 4.129553]\n",
      "epoch:13 step:10857 [D loss: 0.357167, acc.: 83.59%] [G loss: 3.258306]\n",
      "epoch:13 step:10858 [D loss: 0.233634, acc.: 90.62%] [G loss: 3.281229]\n",
      "epoch:13 step:10859 [D loss: 0.429167, acc.: 78.91%] [G loss: 2.633785]\n",
      "epoch:13 step:10860 [D loss: 0.357360, acc.: 85.16%] [G loss: 3.102544]\n",
      "epoch:13 step:10861 [D loss: 0.395683, acc.: 81.25%] [G loss: 2.991061]\n",
      "epoch:13 step:10862 [D loss: 0.296257, acc.: 87.50%] [G loss: 4.274734]\n",
      "epoch:13 step:10863 [D loss: 0.223002, acc.: 92.19%] [G loss: 4.124534]\n",
      "epoch:13 step:10864 [D loss: 0.390247, acc.: 80.47%] [G loss: 3.177686]\n",
      "epoch:13 step:10865 [D loss: 0.376927, acc.: 85.16%] [G loss: 3.041165]\n",
      "epoch:13 step:10866 [D loss: 0.395997, acc.: 78.91%] [G loss: 5.192584]\n",
      "epoch:13 step:10867 [D loss: 0.441045, acc.: 82.81%] [G loss: 2.941122]\n",
      "epoch:13 step:10868 [D loss: 0.373449, acc.: 82.81%] [G loss: 3.483961]\n",
      "epoch:13 step:10869 [D loss: 0.354360, acc.: 84.38%] [G loss: 3.265951]\n",
      "epoch:13 step:10870 [D loss: 0.308647, acc.: 85.16%] [G loss: 2.946414]\n",
      "epoch:13 step:10871 [D loss: 0.208811, acc.: 91.41%] [G loss: 4.358075]\n",
      "epoch:13 step:10872 [D loss: 0.281218, acc.: 87.50%] [G loss: 2.895549]\n",
      "epoch:13 step:10873 [D loss: 0.288864, acc.: 85.16%] [G loss: 3.614360]\n",
      "epoch:13 step:10874 [D loss: 0.243368, acc.: 91.41%] [G loss: 5.790076]\n",
      "epoch:13 step:10875 [D loss: 0.297790, acc.: 82.81%] [G loss: 3.565301]\n",
      "epoch:13 step:10876 [D loss: 0.162264, acc.: 94.53%] [G loss: 6.110274]\n",
      "epoch:13 step:10877 [D loss: 0.288211, acc.: 88.28%] [G loss: 3.871908]\n",
      "epoch:13 step:10878 [D loss: 0.350589, acc.: 82.81%] [G loss: 3.374596]\n",
      "epoch:13 step:10879 [D loss: 0.339841, acc.: 85.16%] [G loss: 3.272922]\n",
      "epoch:13 step:10880 [D loss: 0.284052, acc.: 88.28%] [G loss: 3.239193]\n",
      "epoch:13 step:10881 [D loss: 0.456224, acc.: 80.47%] [G loss: 2.755862]\n",
      "epoch:13 step:10882 [D loss: 0.321555, acc.: 82.03%] [G loss: 2.943773]\n",
      "epoch:13 step:10883 [D loss: 0.304498, acc.: 85.94%] [G loss: 4.076778]\n",
      "epoch:13 step:10884 [D loss: 0.289346, acc.: 89.84%] [G loss: 4.491053]\n",
      "epoch:13 step:10885 [D loss: 0.308403, acc.: 87.50%] [G loss: 2.672760]\n",
      "epoch:13 step:10886 [D loss: 0.248012, acc.: 90.62%] [G loss: 2.861237]\n",
      "epoch:13 step:10887 [D loss: 0.291462, acc.: 84.38%] [G loss: 2.690121]\n",
      "epoch:13 step:10888 [D loss: 0.344647, acc.: 86.72%] [G loss: 2.376462]\n",
      "epoch:13 step:10889 [D loss: 0.353103, acc.: 82.81%] [G loss: 3.194763]\n",
      "epoch:13 step:10890 [D loss: 0.326544, acc.: 81.25%] [G loss: 3.737121]\n",
      "epoch:13 step:10891 [D loss: 0.366631, acc.: 84.38%] [G loss: 5.016551]\n",
      "epoch:13 step:10892 [D loss: 0.391704, acc.: 77.34%] [G loss: 3.323220]\n",
      "epoch:13 step:10893 [D loss: 0.321386, acc.: 87.50%] [G loss: 3.891392]\n",
      "epoch:13 step:10894 [D loss: 0.271039, acc.: 87.50%] [G loss: 3.865836]\n",
      "epoch:13 step:10895 [D loss: 0.208836, acc.: 93.75%] [G loss: 4.084241]\n",
      "epoch:13 step:10896 [D loss: 0.409588, acc.: 83.59%] [G loss: 2.286042]\n",
      "epoch:13 step:10897 [D loss: 0.312704, acc.: 86.72%] [G loss: 3.510671]\n",
      "epoch:13 step:10898 [D loss: 0.271588, acc.: 88.28%] [G loss: 3.680859]\n",
      "epoch:13 step:10899 [D loss: 0.223469, acc.: 92.97%] [G loss: 4.049034]\n",
      "epoch:13 step:10900 [D loss: 0.221301, acc.: 92.19%] [G loss: 4.445762]\n",
      "epoch:13 step:10901 [D loss: 0.340637, acc.: 84.38%] [G loss: 2.759190]\n",
      "epoch:13 step:10902 [D loss: 0.282064, acc.: 88.28%] [G loss: 3.997516]\n",
      "epoch:13 step:10903 [D loss: 0.372761, acc.: 84.38%] [G loss: 4.491023]\n",
      "epoch:13 step:10904 [D loss: 0.579475, acc.: 76.56%] [G loss: 6.325939]\n",
      "epoch:13 step:10905 [D loss: 0.740968, acc.: 78.91%] [G loss: 9.176506]\n",
      "epoch:13 step:10906 [D loss: 2.281069, acc.: 49.22%] [G loss: 2.236596]\n",
      "epoch:13 step:10907 [D loss: 0.615790, acc.: 80.47%] [G loss: 4.081264]\n",
      "epoch:13 step:10908 [D loss: 0.599288, acc.: 77.34%] [G loss: 4.149341]\n",
      "epoch:13 step:10909 [D loss: 0.344941, acc.: 84.38%] [G loss: 3.351954]\n",
      "epoch:13 step:10910 [D loss: 0.236825, acc.: 91.41%] [G loss: 3.625466]\n",
      "epoch:13 step:10911 [D loss: 0.307648, acc.: 88.28%] [G loss: 3.612476]\n",
      "epoch:13 step:10912 [D loss: 0.296812, acc.: 90.62%] [G loss: 3.384261]\n",
      "epoch:13 step:10913 [D loss: 0.318286, acc.: 88.28%] [G loss: 3.688014]\n",
      "epoch:13 step:10914 [D loss: 0.321507, acc.: 89.84%] [G loss: 2.842091]\n",
      "epoch:13 step:10915 [D loss: 0.383674, acc.: 83.59%] [G loss: 2.841003]\n",
      "epoch:13 step:10916 [D loss: 0.210309, acc.: 92.19%] [G loss: 4.739328]\n",
      "epoch:13 step:10917 [D loss: 0.274840, acc.: 89.84%] [G loss: 3.183301]\n",
      "epoch:13 step:10918 [D loss: 0.492210, acc.: 82.03%] [G loss: 4.133034]\n",
      "epoch:13 step:10919 [D loss: 0.365122, acc.: 88.28%] [G loss: 3.466423]\n",
      "epoch:13 step:10920 [D loss: 0.288122, acc.: 86.72%] [G loss: 4.124338]\n",
      "epoch:13 step:10921 [D loss: 0.209885, acc.: 93.75%] [G loss: 6.357373]\n",
      "epoch:13 step:10922 [D loss: 0.289201, acc.: 86.72%] [G loss: 3.678988]\n",
      "epoch:13 step:10923 [D loss: 0.273430, acc.: 87.50%] [G loss: 5.511428]\n",
      "epoch:13 step:10924 [D loss: 0.358657, acc.: 79.69%] [G loss: 4.325534]\n",
      "epoch:13 step:10925 [D loss: 0.305256, acc.: 89.06%] [G loss: 4.920272]\n",
      "epoch:13 step:10926 [D loss: 0.342570, acc.: 82.81%] [G loss: 2.766177]\n",
      "epoch:13 step:10927 [D loss: 0.404906, acc.: 78.12%] [G loss: 2.813128]\n",
      "epoch:13 step:10928 [D loss: 0.307670, acc.: 82.81%] [G loss: 3.118756]\n",
      "epoch:13 step:10929 [D loss: 0.301992, acc.: 89.06%] [G loss: 3.313815]\n",
      "epoch:13 step:10930 [D loss: 0.396616, acc.: 82.03%] [G loss: 2.732761]\n",
      "epoch:13 step:10931 [D loss: 0.260711, acc.: 87.50%] [G loss: 2.747981]\n",
      "epoch:13 step:10932 [D loss: 0.367991, acc.: 83.59%] [G loss: 2.368188]\n",
      "epoch:13 step:10933 [D loss: 0.348431, acc.: 85.94%] [G loss: 3.555279]\n",
      "epoch:13 step:10934 [D loss: 0.273965, acc.: 90.62%] [G loss: 3.117305]\n",
      "epoch:14 step:10935 [D loss: 0.349638, acc.: 85.16%] [G loss: 2.745131]\n",
      "epoch:14 step:10936 [D loss: 0.319218, acc.: 89.06%] [G loss: 2.980262]\n",
      "epoch:14 step:10937 [D loss: 0.255412, acc.: 87.50%] [G loss: 4.112560]\n",
      "epoch:14 step:10938 [D loss: 0.241464, acc.: 92.19%] [G loss: 2.918379]\n",
      "epoch:14 step:10939 [D loss: 0.328473, acc.: 87.50%] [G loss: 2.216923]\n",
      "epoch:14 step:10940 [D loss: 0.386924, acc.: 85.94%] [G loss: 2.679602]\n",
      "epoch:14 step:10941 [D loss: 0.294655, acc.: 88.28%] [G loss: 2.364118]\n",
      "epoch:14 step:10942 [D loss: 0.316372, acc.: 88.28%] [G loss: 2.519541]\n",
      "epoch:14 step:10943 [D loss: 0.336633, acc.: 88.28%] [G loss: 2.586849]\n",
      "epoch:14 step:10944 [D loss: 0.347815, acc.: 85.94%] [G loss: 2.698539]\n",
      "epoch:14 step:10945 [D loss: 0.369425, acc.: 82.81%] [G loss: 2.338482]\n",
      "epoch:14 step:10946 [D loss: 0.270304, acc.: 88.28%] [G loss: 3.059664]\n",
      "epoch:14 step:10947 [D loss: 0.342717, acc.: 86.72%] [G loss: 2.132913]\n",
      "epoch:14 step:10948 [D loss: 0.320572, acc.: 86.72%] [G loss: 2.084757]\n",
      "epoch:14 step:10949 [D loss: 0.315307, acc.: 86.72%] [G loss: 3.352775]\n",
      "epoch:14 step:10950 [D loss: 0.338258, acc.: 85.94%] [G loss: 2.711658]\n",
      "epoch:14 step:10951 [D loss: 0.326352, acc.: 87.50%] [G loss: 2.640368]\n",
      "epoch:14 step:10952 [D loss: 0.386541, acc.: 81.25%] [G loss: 3.290191]\n",
      "epoch:14 step:10953 [D loss: 0.365911, acc.: 82.81%] [G loss: 2.785543]\n",
      "epoch:14 step:10954 [D loss: 0.294351, acc.: 86.72%] [G loss: 3.778084]\n",
      "epoch:14 step:10955 [D loss: 0.270905, acc.: 89.84%] [G loss: 4.903440]\n",
      "epoch:14 step:10956 [D loss: 0.280934, acc.: 87.50%] [G loss: 3.529554]\n",
      "epoch:14 step:10957 [D loss: 0.297099, acc.: 87.50%] [G loss: 3.071715]\n",
      "epoch:14 step:10958 [D loss: 0.405705, acc.: 75.78%] [G loss: 2.119190]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:14 step:10959 [D loss: 0.206544, acc.: 92.97%] [G loss: 3.949947]\n",
      "epoch:14 step:10960 [D loss: 0.289737, acc.: 86.72%] [G loss: 3.640391]\n",
      "epoch:14 step:10961 [D loss: 0.291523, acc.: 88.28%] [G loss: 2.234637]\n",
      "epoch:14 step:10962 [D loss: 0.299157, acc.: 86.72%] [G loss: 3.099769]\n",
      "epoch:14 step:10963 [D loss: 0.383578, acc.: 82.03%] [G loss: 2.258272]\n",
      "epoch:14 step:10964 [D loss: 0.375051, acc.: 89.84%] [G loss: 2.597389]\n",
      "epoch:14 step:10965 [D loss: 0.341092, acc.: 85.94%] [G loss: 3.553814]\n",
      "epoch:14 step:10966 [D loss: 0.403641, acc.: 84.38%] [G loss: 2.369036]\n",
      "epoch:14 step:10967 [D loss: 0.337785, acc.: 85.94%] [G loss: 2.825920]\n",
      "epoch:14 step:10968 [D loss: 0.267441, acc.: 90.62%] [G loss: 2.506962]\n",
      "epoch:14 step:10969 [D loss: 0.275045, acc.: 88.28%] [G loss: 3.166352]\n",
      "epoch:14 step:10970 [D loss: 0.398029, acc.: 83.59%] [G loss: 2.360814]\n",
      "epoch:14 step:10971 [D loss: 0.397607, acc.: 78.91%] [G loss: 3.054011]\n",
      "epoch:14 step:10972 [D loss: 0.467291, acc.: 78.12%] [G loss: 2.956412]\n",
      "epoch:14 step:10973 [D loss: 0.445711, acc.: 78.12%] [G loss: 4.252985]\n",
      "epoch:14 step:10974 [D loss: 0.363126, acc.: 85.94%] [G loss: 5.017186]\n",
      "epoch:14 step:10975 [D loss: 0.526110, acc.: 75.00%] [G loss: 5.686684]\n",
      "epoch:14 step:10976 [D loss: 0.715398, acc.: 73.44%] [G loss: 3.508844]\n",
      "epoch:14 step:10977 [D loss: 0.570178, acc.: 75.78%] [G loss: 2.616717]\n",
      "epoch:14 step:10978 [D loss: 0.373287, acc.: 82.81%] [G loss: 3.559221]\n",
      "epoch:14 step:10979 [D loss: 0.452552, acc.: 79.69%] [G loss: 3.827298]\n",
      "epoch:14 step:10980 [D loss: 0.315534, acc.: 84.38%] [G loss: 4.299020]\n",
      "epoch:14 step:10981 [D loss: 0.407252, acc.: 77.34%] [G loss: 2.932680]\n",
      "epoch:14 step:10982 [D loss: 0.333392, acc.: 88.28%] [G loss: 3.139825]\n",
      "epoch:14 step:10983 [D loss: 0.279755, acc.: 89.84%] [G loss: 2.585761]\n",
      "epoch:14 step:10984 [D loss: 0.365441, acc.: 85.16%] [G loss: 3.559730]\n",
      "epoch:14 step:10985 [D loss: 0.350866, acc.: 83.59%] [G loss: 2.691183]\n",
      "epoch:14 step:10986 [D loss: 0.285077, acc.: 88.28%] [G loss: 3.242710]\n",
      "epoch:14 step:10987 [D loss: 0.296414, acc.: 86.72%] [G loss: 4.036559]\n",
      "epoch:14 step:10988 [D loss: 0.458581, acc.: 84.38%] [G loss: 2.715071]\n",
      "epoch:14 step:10989 [D loss: 0.257180, acc.: 92.19%] [G loss: 4.293248]\n",
      "epoch:14 step:10990 [D loss: 0.325068, acc.: 84.38%] [G loss: 2.479532]\n",
      "epoch:14 step:10991 [D loss: 0.307140, acc.: 86.72%] [G loss: 2.177438]\n",
      "epoch:14 step:10992 [D loss: 0.277116, acc.: 89.06%] [G loss: 2.463624]\n",
      "epoch:14 step:10993 [D loss: 0.355105, acc.: 85.16%] [G loss: 2.621519]\n",
      "epoch:14 step:10994 [D loss: 0.355502, acc.: 85.94%] [G loss: 2.937430]\n",
      "epoch:14 step:10995 [D loss: 0.273994, acc.: 87.50%] [G loss: 4.282384]\n",
      "epoch:14 step:10996 [D loss: 0.261052, acc.: 89.84%] [G loss: 3.804165]\n",
      "epoch:14 step:10997 [D loss: 0.302137, acc.: 89.84%] [G loss: 2.740056]\n",
      "epoch:14 step:10998 [D loss: 0.294705, acc.: 87.50%] [G loss: 2.819372]\n",
      "epoch:14 step:10999 [D loss: 0.281989, acc.: 89.06%] [G loss: 3.329880]\n",
      "epoch:14 step:11000 [D loss: 0.371243, acc.: 85.16%] [G loss: 2.635520]\n",
      "##############\n",
      "[0.82777929 0.88834881 0.80801513 0.80443298 0.77563222 0.81166365\n",
      " 0.85099246 0.83280051 0.78363836 0.83390474]\n",
      "##########\n",
      "epoch:14 step:11001 [D loss: 0.272355, acc.: 92.97%] [G loss: 3.030488]\n",
      "epoch:14 step:11002 [D loss: 0.248665, acc.: 88.28%] [G loss: 4.159942]\n",
      "epoch:14 step:11003 [D loss: 0.309514, acc.: 83.59%] [G loss: 4.568816]\n",
      "epoch:14 step:11004 [D loss: 0.304086, acc.: 85.94%] [G loss: 3.357886]\n",
      "epoch:14 step:11005 [D loss: 0.560662, acc.: 73.44%] [G loss: 2.454556]\n",
      "epoch:14 step:11006 [D loss: 0.396392, acc.: 78.91%] [G loss: 3.635062]\n",
      "epoch:14 step:11007 [D loss: 0.400827, acc.: 83.59%] [G loss: 4.138826]\n",
      "epoch:14 step:11008 [D loss: 0.254830, acc.: 87.50%] [G loss: 5.013742]\n",
      "epoch:14 step:11009 [D loss: 0.312622, acc.: 85.94%] [G loss: 2.815807]\n",
      "epoch:14 step:11010 [D loss: 0.384238, acc.: 83.59%] [G loss: 2.653611]\n",
      "epoch:14 step:11011 [D loss: 0.387880, acc.: 82.81%] [G loss: 2.398044]\n",
      "epoch:14 step:11012 [D loss: 0.339961, acc.: 86.72%] [G loss: 3.906004]\n",
      "epoch:14 step:11013 [D loss: 0.323787, acc.: 84.38%] [G loss: 4.746434]\n",
      "epoch:14 step:11014 [D loss: 0.320724, acc.: 87.50%] [G loss: 4.377516]\n",
      "epoch:14 step:11015 [D loss: 0.262832, acc.: 88.28%] [G loss: 4.599055]\n",
      "epoch:14 step:11016 [D loss: 0.378217, acc.: 78.12%] [G loss: 4.326797]\n",
      "epoch:14 step:11017 [D loss: 0.182486, acc.: 92.97%] [G loss: 5.144940]\n",
      "epoch:14 step:11018 [D loss: 0.328950, acc.: 86.72%] [G loss: 2.826599]\n",
      "epoch:14 step:11019 [D loss: 0.320814, acc.: 83.59%] [G loss: 6.211753]\n",
      "epoch:14 step:11020 [D loss: 0.308927, acc.: 86.72%] [G loss: 3.378793]\n",
      "epoch:14 step:11021 [D loss: 0.350753, acc.: 84.38%] [G loss: 3.209213]\n",
      "epoch:14 step:11022 [D loss: 0.366952, acc.: 81.25%] [G loss: 3.282181]\n",
      "epoch:14 step:11023 [D loss: 0.343450, acc.: 82.03%] [G loss: 2.431000]\n",
      "epoch:14 step:11024 [D loss: 0.325760, acc.: 84.38%] [G loss: 3.968024]\n",
      "epoch:14 step:11025 [D loss: 0.318802, acc.: 89.06%] [G loss: 4.837059]\n",
      "epoch:14 step:11026 [D loss: 0.353769, acc.: 82.81%] [G loss: 3.673911]\n",
      "epoch:14 step:11027 [D loss: 0.540353, acc.: 77.34%] [G loss: 2.585971]\n",
      "epoch:14 step:11028 [D loss: 0.292755, acc.: 90.62%] [G loss: 4.149221]\n",
      "epoch:14 step:11029 [D loss: 0.345473, acc.: 84.38%] [G loss: 3.960215]\n",
      "epoch:14 step:11030 [D loss: 0.310315, acc.: 85.16%] [G loss: 3.080236]\n",
      "epoch:14 step:11031 [D loss: 0.365614, acc.: 82.81%] [G loss: 4.360765]\n",
      "epoch:14 step:11032 [D loss: 0.305142, acc.: 86.72%] [G loss: 3.337482]\n",
      "epoch:14 step:11033 [D loss: 0.282229, acc.: 89.06%] [G loss: 3.407617]\n",
      "epoch:14 step:11034 [D loss: 0.290254, acc.: 89.06%] [G loss: 2.985420]\n",
      "epoch:14 step:11035 [D loss: 0.404456, acc.: 79.69%] [G loss: 2.697403]\n",
      "epoch:14 step:11036 [D loss: 0.292720, acc.: 91.41%] [G loss: 3.016713]\n",
      "epoch:14 step:11037 [D loss: 0.318211, acc.: 88.28%] [G loss: 3.409181]\n",
      "epoch:14 step:11038 [D loss: 0.413356, acc.: 85.94%] [G loss: 4.693537]\n",
      "epoch:14 step:11039 [D loss: 0.617404, acc.: 76.56%] [G loss: 2.288339]\n",
      "epoch:14 step:11040 [D loss: 0.522809, acc.: 79.69%] [G loss: 3.582442]\n",
      "epoch:14 step:11041 [D loss: 0.557110, acc.: 73.44%] [G loss: 3.172323]\n",
      "epoch:14 step:11042 [D loss: 0.407929, acc.: 81.25%] [G loss: 6.122251]\n",
      "epoch:14 step:11043 [D loss: 0.406162, acc.: 82.03%] [G loss: 3.356548]\n",
      "epoch:14 step:11044 [D loss: 0.343777, acc.: 85.94%] [G loss: 3.665853]\n",
      "epoch:14 step:11045 [D loss: 0.259392, acc.: 89.06%] [G loss: 4.418789]\n",
      "epoch:14 step:11046 [D loss: 0.275757, acc.: 89.06%] [G loss: 4.065842]\n",
      "epoch:14 step:11047 [D loss: 0.342573, acc.: 85.94%] [G loss: 2.696895]\n",
      "epoch:14 step:11048 [D loss: 0.293725, acc.: 87.50%] [G loss: 2.938137]\n",
      "epoch:14 step:11049 [D loss: 0.364802, acc.: 85.16%] [G loss: 2.184328]\n",
      "epoch:14 step:11050 [D loss: 0.358521, acc.: 83.59%] [G loss: 2.527260]\n",
      "epoch:14 step:11051 [D loss: 0.351372, acc.: 85.16%] [G loss: 2.843480]\n",
      "epoch:14 step:11052 [D loss: 0.461538, acc.: 80.47%] [G loss: 3.274971]\n",
      "epoch:14 step:11053 [D loss: 0.285464, acc.: 89.06%] [G loss: 3.219904]\n",
      "epoch:14 step:11054 [D loss: 0.294734, acc.: 84.38%] [G loss: 3.892834]\n",
      "epoch:14 step:11055 [D loss: 0.230615, acc.: 90.62%] [G loss: 4.146094]\n",
      "epoch:14 step:11056 [D loss: 0.336116, acc.: 83.59%] [G loss: 3.013066]\n",
      "epoch:14 step:11057 [D loss: 0.226540, acc.: 91.41%] [G loss: 3.340115]\n",
      "epoch:14 step:11058 [D loss: 0.347676, acc.: 84.38%] [G loss: 2.463824]\n",
      "epoch:14 step:11059 [D loss: 0.318308, acc.: 85.94%] [G loss: 3.883799]\n",
      "epoch:14 step:11060 [D loss: 0.261006, acc.: 89.84%] [G loss: 3.175820]\n",
      "epoch:14 step:11061 [D loss: 0.367560, acc.: 83.59%] [G loss: 3.117437]\n",
      "epoch:14 step:11062 [D loss: 0.278993, acc.: 84.38%] [G loss: 2.807985]\n",
      "epoch:14 step:11063 [D loss: 0.317401, acc.: 85.16%] [G loss: 3.395757]\n",
      "epoch:14 step:11064 [D loss: 0.388674, acc.: 80.47%] [G loss: 4.605441]\n",
      "epoch:14 step:11065 [D loss: 0.206529, acc.: 93.75%] [G loss: 4.744307]\n",
      "epoch:14 step:11066 [D loss: 0.268253, acc.: 89.06%] [G loss: 3.177435]\n",
      "epoch:14 step:11067 [D loss: 0.257723, acc.: 89.84%] [G loss: 3.029731]\n",
      "epoch:14 step:11068 [D loss: 0.241518, acc.: 90.62%] [G loss: 3.396468]\n",
      "epoch:14 step:11069 [D loss: 0.351274, acc.: 84.38%] [G loss: 3.110111]\n",
      "epoch:14 step:11070 [D loss: 0.305204, acc.: 85.16%] [G loss: 2.968913]\n",
      "epoch:14 step:11071 [D loss: 0.247868, acc.: 90.62%] [G loss: 2.773337]\n",
      "epoch:14 step:11072 [D loss: 0.367330, acc.: 85.94%] [G loss: 2.550813]\n",
      "epoch:14 step:11073 [D loss: 0.209579, acc.: 93.75%] [G loss: 2.739032]\n",
      "epoch:14 step:11074 [D loss: 0.321732, acc.: 89.06%] [G loss: 2.991140]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:14 step:11075 [D loss: 0.451281, acc.: 78.91%] [G loss: 4.682468]\n",
      "epoch:14 step:11076 [D loss: 0.716225, acc.: 72.66%] [G loss: 3.804157]\n",
      "epoch:14 step:11077 [D loss: 0.236873, acc.: 91.41%] [G loss: 4.683177]\n",
      "epoch:14 step:11078 [D loss: 0.333743, acc.: 84.38%] [G loss: 5.139251]\n",
      "epoch:14 step:11079 [D loss: 0.261255, acc.: 88.28%] [G loss: 5.891999]\n",
      "epoch:14 step:11080 [D loss: 0.320245, acc.: 85.94%] [G loss: 5.299173]\n",
      "epoch:14 step:11081 [D loss: 0.333854, acc.: 83.59%] [G loss: 3.069309]\n",
      "epoch:14 step:11082 [D loss: 0.348425, acc.: 84.38%] [G loss: 4.209627]\n",
      "epoch:14 step:11083 [D loss: 0.391155, acc.: 78.91%] [G loss: 3.972820]\n",
      "epoch:14 step:11084 [D loss: 0.268201, acc.: 88.28%] [G loss: 3.626461]\n",
      "epoch:14 step:11085 [D loss: 0.399036, acc.: 84.38%] [G loss: 3.441832]\n",
      "epoch:14 step:11086 [D loss: 0.298385, acc.: 88.28%] [G loss: 6.796787]\n",
      "epoch:14 step:11087 [D loss: 0.312415, acc.: 85.94%] [G loss: 2.963609]\n",
      "epoch:14 step:11088 [D loss: 0.297728, acc.: 87.50%] [G loss: 2.917322]\n",
      "epoch:14 step:11089 [D loss: 0.406042, acc.: 82.81%] [G loss: 3.739941]\n",
      "epoch:14 step:11090 [D loss: 0.300934, acc.: 85.16%] [G loss: 2.473122]\n",
      "epoch:14 step:11091 [D loss: 0.543463, acc.: 75.00%] [G loss: 2.758573]\n",
      "epoch:14 step:11092 [D loss: 0.386089, acc.: 85.16%] [G loss: 3.449010]\n",
      "epoch:14 step:11093 [D loss: 0.383374, acc.: 85.16%] [G loss: 3.105367]\n",
      "epoch:14 step:11094 [D loss: 0.470672, acc.: 78.91%] [G loss: 2.622263]\n",
      "epoch:14 step:11095 [D loss: 0.373294, acc.: 82.03%] [G loss: 2.838140]\n",
      "epoch:14 step:11096 [D loss: 0.299473, acc.: 89.06%] [G loss: 3.383274]\n",
      "epoch:14 step:11097 [D loss: 0.318897, acc.: 85.16%] [G loss: 2.661669]\n",
      "epoch:14 step:11098 [D loss: 0.214287, acc.: 92.97%] [G loss: 3.695887]\n",
      "epoch:14 step:11099 [D loss: 0.280709, acc.: 89.06%] [G loss: 2.095768]\n",
      "epoch:14 step:11100 [D loss: 0.253014, acc.: 90.62%] [G loss: 2.580549]\n",
      "epoch:14 step:11101 [D loss: 0.282784, acc.: 88.28%] [G loss: 2.918485]\n",
      "epoch:14 step:11102 [D loss: 0.414946, acc.: 82.81%] [G loss: 3.039225]\n",
      "epoch:14 step:11103 [D loss: 0.297864, acc.: 90.62%] [G loss: 3.994333]\n",
      "epoch:14 step:11104 [D loss: 0.407644, acc.: 83.59%] [G loss: 4.717264]\n",
      "epoch:14 step:11105 [D loss: 0.330390, acc.: 86.72%] [G loss: 4.962118]\n",
      "epoch:14 step:11106 [D loss: 0.314764, acc.: 85.94%] [G loss: 2.797966]\n",
      "epoch:14 step:11107 [D loss: 0.305883, acc.: 86.72%] [G loss: 4.082870]\n",
      "epoch:14 step:11108 [D loss: 0.391643, acc.: 78.12%] [G loss: 3.785408]\n",
      "epoch:14 step:11109 [D loss: 0.352876, acc.: 82.81%] [G loss: 2.954947]\n",
      "epoch:14 step:11110 [D loss: 0.319649, acc.: 89.06%] [G loss: 3.734752]\n",
      "epoch:14 step:11111 [D loss: 0.281031, acc.: 89.06%] [G loss: 4.100013]\n",
      "epoch:14 step:11112 [D loss: 0.424701, acc.: 85.16%] [G loss: 4.437769]\n",
      "epoch:14 step:11113 [D loss: 0.322585, acc.: 84.38%] [G loss: 4.933103]\n",
      "epoch:14 step:11114 [D loss: 0.315062, acc.: 87.50%] [G loss: 2.887192]\n",
      "epoch:14 step:11115 [D loss: 0.393358, acc.: 82.81%] [G loss: 3.052629]\n",
      "epoch:14 step:11116 [D loss: 0.333777, acc.: 85.94%] [G loss: 2.754420]\n",
      "epoch:14 step:11117 [D loss: 0.378608, acc.: 79.69%] [G loss: 3.169107]\n",
      "epoch:14 step:11118 [D loss: 0.266159, acc.: 89.84%] [G loss: 3.826813]\n",
      "epoch:14 step:11119 [D loss: 0.305223, acc.: 83.59%] [G loss: 5.187788]\n",
      "epoch:14 step:11120 [D loss: 0.306977, acc.: 86.72%] [G loss: 4.371532]\n",
      "epoch:14 step:11121 [D loss: 0.382941, acc.: 82.81%] [G loss: 2.993833]\n",
      "epoch:14 step:11122 [D loss: 0.385242, acc.: 82.81%] [G loss: 3.316882]\n",
      "epoch:14 step:11123 [D loss: 0.297505, acc.: 85.16%] [G loss: 3.138734]\n",
      "epoch:14 step:11124 [D loss: 0.470865, acc.: 79.69%] [G loss: 2.961620]\n",
      "epoch:14 step:11125 [D loss: 0.428960, acc.: 82.81%] [G loss: 3.912866]\n",
      "epoch:14 step:11126 [D loss: 0.345204, acc.: 85.16%] [G loss: 3.513462]\n",
      "epoch:14 step:11127 [D loss: 0.253720, acc.: 90.62%] [G loss: 5.031135]\n",
      "epoch:14 step:11128 [D loss: 0.324279, acc.: 84.38%] [G loss: 4.491205]\n",
      "epoch:14 step:11129 [D loss: 0.312755, acc.: 86.72%] [G loss: 3.859605]\n",
      "epoch:14 step:11130 [D loss: 0.227975, acc.: 92.19%] [G loss: 3.610273]\n",
      "epoch:14 step:11131 [D loss: 0.360214, acc.: 86.72%] [G loss: 3.006709]\n",
      "epoch:14 step:11132 [D loss: 0.312959, acc.: 87.50%] [G loss: 3.419816]\n",
      "epoch:14 step:11133 [D loss: 0.307063, acc.: 88.28%] [G loss: 3.427038]\n",
      "epoch:14 step:11134 [D loss: 0.378012, acc.: 79.69%] [G loss: 2.937833]\n",
      "epoch:14 step:11135 [D loss: 0.382395, acc.: 82.03%] [G loss: 3.135014]\n",
      "epoch:14 step:11136 [D loss: 0.480558, acc.: 80.47%] [G loss: 4.508232]\n",
      "epoch:14 step:11137 [D loss: 0.271672, acc.: 89.84%] [G loss: 3.052110]\n",
      "epoch:14 step:11138 [D loss: 0.391522, acc.: 85.16%] [G loss: 5.488073]\n",
      "epoch:14 step:11139 [D loss: 0.435729, acc.: 83.59%] [G loss: 3.391988]\n",
      "epoch:14 step:11140 [D loss: 0.277635, acc.: 90.62%] [G loss: 2.490652]\n",
      "epoch:14 step:11141 [D loss: 0.340881, acc.: 86.72%] [G loss: 2.898058]\n",
      "epoch:14 step:11142 [D loss: 0.360624, acc.: 82.03%] [G loss: 3.397577]\n",
      "epoch:14 step:11143 [D loss: 0.194563, acc.: 93.75%] [G loss: 4.814986]\n",
      "epoch:14 step:11144 [D loss: 0.393531, acc.: 84.38%] [G loss: 3.450096]\n",
      "epoch:14 step:11145 [D loss: 0.375637, acc.: 82.03%] [G loss: 2.877894]\n",
      "epoch:14 step:11146 [D loss: 0.287385, acc.: 89.84%] [G loss: 2.591516]\n",
      "epoch:14 step:11147 [D loss: 0.277639, acc.: 86.72%] [G loss: 2.549713]\n",
      "epoch:14 step:11148 [D loss: 0.386240, acc.: 79.69%] [G loss: 2.919899]\n",
      "epoch:14 step:11149 [D loss: 0.221477, acc.: 92.97%] [G loss: 3.851631]\n",
      "epoch:14 step:11150 [D loss: 0.250542, acc.: 87.50%] [G loss: 4.623898]\n",
      "epoch:14 step:11151 [D loss: 0.248429, acc.: 92.19%] [G loss: 3.430543]\n",
      "epoch:14 step:11152 [D loss: 0.334777, acc.: 86.72%] [G loss: 2.594182]\n",
      "epoch:14 step:11153 [D loss: 0.253849, acc.: 89.84%] [G loss: 2.814813]\n",
      "epoch:14 step:11154 [D loss: 0.402485, acc.: 82.81%] [G loss: 3.509312]\n",
      "epoch:14 step:11155 [D loss: 0.327526, acc.: 85.16%] [G loss: 2.892107]\n",
      "epoch:14 step:11156 [D loss: 0.268452, acc.: 89.06%] [G loss: 3.522422]\n",
      "epoch:14 step:11157 [D loss: 0.288650, acc.: 91.41%] [G loss: 3.546941]\n",
      "epoch:14 step:11158 [D loss: 0.319061, acc.: 84.38%] [G loss: 4.497236]\n",
      "epoch:14 step:11159 [D loss: 0.335932, acc.: 85.16%] [G loss: 3.727532]\n",
      "epoch:14 step:11160 [D loss: 0.367175, acc.: 83.59%] [G loss: 3.210832]\n",
      "epoch:14 step:11161 [D loss: 0.307053, acc.: 89.06%] [G loss: 3.289346]\n",
      "epoch:14 step:11162 [D loss: 0.528950, acc.: 75.78%] [G loss: 3.865005]\n",
      "epoch:14 step:11163 [D loss: 0.356735, acc.: 84.38%] [G loss: 4.043142]\n",
      "epoch:14 step:11164 [D loss: 0.218241, acc.: 91.41%] [G loss: 5.425127]\n",
      "epoch:14 step:11165 [D loss: 0.229151, acc.: 92.19%] [G loss: 2.530107]\n",
      "epoch:14 step:11166 [D loss: 0.298441, acc.: 85.94%] [G loss: 2.739277]\n",
      "epoch:14 step:11167 [D loss: 0.370052, acc.: 82.81%] [G loss: 2.294981]\n",
      "epoch:14 step:11168 [D loss: 0.295360, acc.: 89.06%] [G loss: 3.092493]\n",
      "epoch:14 step:11169 [D loss: 0.295298, acc.: 85.16%] [G loss: 3.914433]\n",
      "epoch:14 step:11170 [D loss: 0.227385, acc.: 88.28%] [G loss: 4.729768]\n",
      "epoch:14 step:11171 [D loss: 0.345224, acc.: 84.38%] [G loss: 3.476918]\n",
      "epoch:14 step:11172 [D loss: 0.321858, acc.: 87.50%] [G loss: 2.963666]\n",
      "epoch:14 step:11173 [D loss: 0.300940, acc.: 89.06%] [G loss: 2.568036]\n",
      "epoch:14 step:11174 [D loss: 0.329491, acc.: 86.72%] [G loss: 2.684385]\n",
      "epoch:14 step:11175 [D loss: 0.265113, acc.: 88.28%] [G loss: 3.007015]\n",
      "epoch:14 step:11176 [D loss: 0.386265, acc.: 84.38%] [G loss: 3.748768]\n",
      "epoch:14 step:11177 [D loss: 0.350187, acc.: 85.16%] [G loss: 2.522333]\n",
      "epoch:14 step:11178 [D loss: 0.287823, acc.: 85.16%] [G loss: 2.758635]\n",
      "epoch:14 step:11179 [D loss: 0.323018, acc.: 86.72%] [G loss: 3.466795]\n",
      "epoch:14 step:11180 [D loss: 0.263358, acc.: 85.94%] [G loss: 4.885737]\n",
      "epoch:14 step:11181 [D loss: 0.231374, acc.: 88.28%] [G loss: 5.588928]\n",
      "epoch:14 step:11182 [D loss: 0.341973, acc.: 86.72%] [G loss: 3.517406]\n",
      "epoch:14 step:11183 [D loss: 0.382036, acc.: 79.69%] [G loss: 3.489323]\n",
      "epoch:14 step:11184 [D loss: 0.312456, acc.: 87.50%] [G loss: 2.905228]\n",
      "epoch:14 step:11185 [D loss: 0.384534, acc.: 83.59%] [G loss: 3.650712]\n",
      "epoch:14 step:11186 [D loss: 0.366694, acc.: 85.16%] [G loss: 7.691961]\n",
      "epoch:14 step:11187 [D loss: 0.750762, acc.: 67.97%] [G loss: 4.002577]\n",
      "epoch:14 step:11188 [D loss: 0.303002, acc.: 89.06%] [G loss: 3.485845]\n",
      "epoch:14 step:11189 [D loss: 0.473230, acc.: 77.34%] [G loss: 5.333739]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:14 step:11190 [D loss: 0.448965, acc.: 78.12%] [G loss: 2.448203]\n",
      "epoch:14 step:11191 [D loss: 0.383925, acc.: 79.69%] [G loss: 4.069162]\n",
      "epoch:14 step:11192 [D loss: 0.386225, acc.: 85.16%] [G loss: 3.697356]\n",
      "epoch:14 step:11193 [D loss: 0.464072, acc.: 82.81%] [G loss: 2.345605]\n",
      "epoch:14 step:11194 [D loss: 0.381458, acc.: 84.38%] [G loss: 2.662032]\n",
      "epoch:14 step:11195 [D loss: 0.347416, acc.: 83.59%] [G loss: 4.314136]\n",
      "epoch:14 step:11196 [D loss: 0.225242, acc.: 90.62%] [G loss: 5.303052]\n",
      "epoch:14 step:11197 [D loss: 0.301221, acc.: 86.72%] [G loss: 3.891140]\n",
      "epoch:14 step:11198 [D loss: 0.251375, acc.: 90.62%] [G loss: 5.238418]\n",
      "epoch:14 step:11199 [D loss: 0.329017, acc.: 86.72%] [G loss: 4.477542]\n",
      "epoch:14 step:11200 [D loss: 0.222303, acc.: 89.06%] [G loss: 7.850603]\n",
      "##############\n",
      "[0.84986656 0.87124055 0.81200335 0.79622939 0.79340458 0.81720114\n",
      " 0.85964868 0.83500606 0.7982641  0.81174952]\n",
      "##########\n",
      "epoch:14 step:11201 [D loss: 0.361367, acc.: 88.28%] [G loss: 5.078561]\n",
      "epoch:14 step:11202 [D loss: 0.168345, acc.: 93.75%] [G loss: 5.238293]\n",
      "epoch:14 step:11203 [D loss: 0.350135, acc.: 88.28%] [G loss: 3.390766]\n",
      "epoch:14 step:11204 [D loss: 0.236221, acc.: 88.28%] [G loss: 4.098503]\n",
      "epoch:14 step:11205 [D loss: 0.429474, acc.: 78.91%] [G loss: 2.409579]\n",
      "epoch:14 step:11206 [D loss: 0.282094, acc.: 90.62%] [G loss: 3.239274]\n",
      "epoch:14 step:11207 [D loss: 0.288974, acc.: 81.25%] [G loss: 3.125886]\n",
      "epoch:14 step:11208 [D loss: 0.328614, acc.: 89.06%] [G loss: 2.972949]\n",
      "epoch:14 step:11209 [D loss: 0.562045, acc.: 71.88%] [G loss: 3.308307]\n",
      "epoch:14 step:11210 [D loss: 0.337401, acc.: 84.38%] [G loss: 2.791011]\n",
      "epoch:14 step:11211 [D loss: 0.413543, acc.: 82.81%] [G loss: 2.345257]\n",
      "epoch:14 step:11212 [D loss: 0.355609, acc.: 89.06%] [G loss: 3.177289]\n",
      "epoch:14 step:11213 [D loss: 0.333709, acc.: 84.38%] [G loss: 2.528980]\n",
      "epoch:14 step:11214 [D loss: 0.299037, acc.: 89.84%] [G loss: 3.436062]\n",
      "epoch:14 step:11215 [D loss: 0.331306, acc.: 89.06%] [G loss: 4.205146]\n",
      "epoch:14 step:11216 [D loss: 0.326347, acc.: 86.72%] [G loss: 4.465262]\n",
      "epoch:14 step:11217 [D loss: 0.295774, acc.: 87.50%] [G loss: 4.925473]\n",
      "epoch:14 step:11218 [D loss: 0.348635, acc.: 80.47%] [G loss: 5.667456]\n",
      "epoch:14 step:11219 [D loss: 0.392232, acc.: 80.47%] [G loss: 3.240984]\n",
      "epoch:14 step:11220 [D loss: 0.261360, acc.: 87.50%] [G loss: 4.751250]\n",
      "epoch:14 step:11221 [D loss: 0.162434, acc.: 96.88%] [G loss: 4.533211]\n",
      "epoch:14 step:11222 [D loss: 0.264971, acc.: 85.16%] [G loss: 3.562984]\n",
      "epoch:14 step:11223 [D loss: 0.256051, acc.: 89.06%] [G loss: 5.091461]\n",
      "epoch:14 step:11224 [D loss: 0.285724, acc.: 83.59%] [G loss: 3.014400]\n",
      "epoch:14 step:11225 [D loss: 0.280310, acc.: 89.84%] [G loss: 3.060877]\n",
      "epoch:14 step:11226 [D loss: 0.272561, acc.: 91.41%] [G loss: 4.095811]\n",
      "epoch:14 step:11227 [D loss: 0.309502, acc.: 85.94%] [G loss: 3.412388]\n",
      "epoch:14 step:11228 [D loss: 0.441138, acc.: 82.81%] [G loss: 2.730165]\n",
      "epoch:14 step:11229 [D loss: 0.445867, acc.: 81.25%] [G loss: 3.931973]\n",
      "epoch:14 step:11230 [D loss: 0.505950, acc.: 77.34%] [G loss: 2.818844]\n",
      "epoch:14 step:11231 [D loss: 0.371698, acc.: 85.94%] [G loss: 2.898570]\n",
      "epoch:14 step:11232 [D loss: 0.314908, acc.: 87.50%] [G loss: 3.766075]\n",
      "epoch:14 step:11233 [D loss: 0.327238, acc.: 89.06%] [G loss: 3.913858]\n",
      "epoch:14 step:11234 [D loss: 0.342021, acc.: 84.38%] [G loss: 3.237142]\n",
      "epoch:14 step:11235 [D loss: 0.459390, acc.: 79.69%] [G loss: 2.291195]\n",
      "epoch:14 step:11236 [D loss: 0.257045, acc.: 86.72%] [G loss: 3.365924]\n",
      "epoch:14 step:11237 [D loss: 0.454175, acc.: 84.38%] [G loss: 2.701045]\n",
      "epoch:14 step:11238 [D loss: 0.321284, acc.: 84.38%] [G loss: 3.272609]\n",
      "epoch:14 step:11239 [D loss: 0.505302, acc.: 73.44%] [G loss: 3.680633]\n",
      "epoch:14 step:11240 [D loss: 0.313684, acc.: 84.38%] [G loss: 4.092429]\n",
      "epoch:14 step:11241 [D loss: 0.359812, acc.: 84.38%] [G loss: 2.818667]\n",
      "epoch:14 step:11242 [D loss: 0.231311, acc.: 88.28%] [G loss: 3.819807]\n",
      "epoch:14 step:11243 [D loss: 0.257760, acc.: 85.94%] [G loss: 3.532310]\n",
      "epoch:14 step:11244 [D loss: 0.338895, acc.: 87.50%] [G loss: 3.383561]\n",
      "epoch:14 step:11245 [D loss: 0.352491, acc.: 85.94%] [G loss: 2.836244]\n",
      "epoch:14 step:11246 [D loss: 0.420434, acc.: 80.47%] [G loss: 2.609744]\n",
      "epoch:14 step:11247 [D loss: 0.267421, acc.: 92.97%] [G loss: 4.631656]\n",
      "epoch:14 step:11248 [D loss: 0.275501, acc.: 86.72%] [G loss: 3.303327]\n",
      "epoch:14 step:11249 [D loss: 0.404813, acc.: 81.25%] [G loss: 3.324239]\n",
      "epoch:14 step:11250 [D loss: 0.323637, acc.: 89.06%] [G loss: 2.308183]\n",
      "epoch:14 step:11251 [D loss: 0.424957, acc.: 82.03%] [G loss: 2.649659]\n",
      "epoch:14 step:11252 [D loss: 0.392722, acc.: 81.25%] [G loss: 3.006431]\n",
      "epoch:14 step:11253 [D loss: 0.351348, acc.: 82.81%] [G loss: 4.825983]\n",
      "epoch:14 step:11254 [D loss: 0.231896, acc.: 90.62%] [G loss: 4.451513]\n",
      "epoch:14 step:11255 [D loss: 0.298229, acc.: 85.94%] [G loss: 4.530633]\n",
      "epoch:14 step:11256 [D loss: 0.396380, acc.: 85.16%] [G loss: 3.330715]\n",
      "epoch:14 step:11257 [D loss: 0.273953, acc.: 92.97%] [G loss: 2.788499]\n",
      "epoch:14 step:11258 [D loss: 0.345949, acc.: 86.72%] [G loss: 3.005857]\n",
      "epoch:14 step:11259 [D loss: 0.289824, acc.: 88.28%] [G loss: 2.818769]\n",
      "epoch:14 step:11260 [D loss: 0.453731, acc.: 82.03%] [G loss: 2.979836]\n",
      "epoch:14 step:11261 [D loss: 0.301950, acc.: 85.94%] [G loss: 3.692689]\n",
      "epoch:14 step:11262 [D loss: 0.482724, acc.: 81.25%] [G loss: 3.865172]\n",
      "epoch:14 step:11263 [D loss: 0.239511, acc.: 90.62%] [G loss: 4.099990]\n",
      "epoch:14 step:11264 [D loss: 0.251543, acc.: 88.28%] [G loss: 4.171349]\n",
      "epoch:14 step:11265 [D loss: 0.332839, acc.: 85.16%] [G loss: 2.720899]\n",
      "epoch:14 step:11266 [D loss: 0.337218, acc.: 84.38%] [G loss: 3.289627]\n",
      "epoch:14 step:11267 [D loss: 0.324635, acc.: 87.50%] [G loss: 2.736024]\n",
      "epoch:14 step:11268 [D loss: 0.201901, acc.: 92.19%] [G loss: 3.177489]\n",
      "epoch:14 step:11269 [D loss: 0.246346, acc.: 91.41%] [G loss: 5.109543]\n",
      "epoch:14 step:11270 [D loss: 0.383482, acc.: 80.47%] [G loss: 2.805724]\n",
      "epoch:14 step:11271 [D loss: 0.286228, acc.: 85.16%] [G loss: 4.139559]\n",
      "epoch:14 step:11272 [D loss: 0.488628, acc.: 80.47%] [G loss: 6.023503]\n",
      "epoch:14 step:11273 [D loss: 0.464583, acc.: 79.69%] [G loss: 3.819638]\n",
      "epoch:14 step:11274 [D loss: 0.352670, acc.: 82.03%] [G loss: 5.394238]\n",
      "epoch:14 step:11275 [D loss: 0.724455, acc.: 76.56%] [G loss: 3.541608]\n",
      "epoch:14 step:11276 [D loss: 0.298937, acc.: 86.72%] [G loss: 4.483501]\n",
      "epoch:14 step:11277 [D loss: 0.399754, acc.: 85.16%] [G loss: 2.709459]\n",
      "epoch:14 step:11278 [D loss: 0.429879, acc.: 79.69%] [G loss: 4.315659]\n",
      "epoch:14 step:11279 [D loss: 0.209882, acc.: 90.62%] [G loss: 6.384708]\n",
      "epoch:14 step:11280 [D loss: 0.415864, acc.: 80.47%] [G loss: 3.042693]\n",
      "epoch:14 step:11281 [D loss: 0.332258, acc.: 87.50%] [G loss: 3.819024]\n",
      "epoch:14 step:11282 [D loss: 0.425441, acc.: 80.47%] [G loss: 3.579022]\n",
      "epoch:14 step:11283 [D loss: 0.418365, acc.: 81.25%] [G loss: 3.738091]\n",
      "epoch:14 step:11284 [D loss: 0.324726, acc.: 89.06%] [G loss: 4.267767]\n",
      "epoch:14 step:11285 [D loss: 0.438000, acc.: 78.91%] [G loss: 6.053677]\n",
      "epoch:14 step:11286 [D loss: 0.463028, acc.: 76.56%] [G loss: 5.751787]\n",
      "epoch:14 step:11287 [D loss: 0.321338, acc.: 82.81%] [G loss: 3.124902]\n",
      "epoch:14 step:11288 [D loss: 0.297326, acc.: 84.38%] [G loss: 3.454962]\n",
      "epoch:14 step:11289 [D loss: 0.310326, acc.: 86.72%] [G loss: 3.380589]\n",
      "epoch:14 step:11290 [D loss: 0.243867, acc.: 89.06%] [G loss: 4.049990]\n",
      "epoch:14 step:11291 [D loss: 0.326071, acc.: 82.03%] [G loss: 4.625378]\n",
      "epoch:14 step:11292 [D loss: 0.301309, acc.: 86.72%] [G loss: 3.883445]\n",
      "epoch:14 step:11293 [D loss: 0.400880, acc.: 81.25%] [G loss: 3.082601]\n",
      "epoch:14 step:11294 [D loss: 0.301980, acc.: 85.16%] [G loss: 2.966084]\n",
      "epoch:14 step:11295 [D loss: 0.275168, acc.: 89.84%] [G loss: 4.010462]\n",
      "epoch:14 step:11296 [D loss: 0.285546, acc.: 87.50%] [G loss: 3.637091]\n",
      "epoch:14 step:11297 [D loss: 0.396045, acc.: 84.38%] [G loss: 2.488676]\n",
      "epoch:14 step:11298 [D loss: 0.326313, acc.: 85.16%] [G loss: 2.455139]\n",
      "epoch:14 step:11299 [D loss: 0.297299, acc.: 90.62%] [G loss: 3.086538]\n",
      "epoch:14 step:11300 [D loss: 0.407392, acc.: 85.16%] [G loss: 3.324457]\n",
      "epoch:14 step:11301 [D loss: 0.283947, acc.: 87.50%] [G loss: 3.701284]\n",
      "epoch:14 step:11302 [D loss: 0.480521, acc.: 74.22%] [G loss: 2.722824]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:14 step:11303 [D loss: 0.374557, acc.: 89.84%] [G loss: 2.513528]\n",
      "epoch:14 step:11304 [D loss: 0.289043, acc.: 83.59%] [G loss: 2.954775]\n",
      "epoch:14 step:11305 [D loss: 0.253501, acc.: 89.84%] [G loss: 4.084575]\n",
      "epoch:14 step:11306 [D loss: 0.327895, acc.: 85.16%] [G loss: 3.172389]\n",
      "epoch:14 step:11307 [D loss: 0.321658, acc.: 83.59%] [G loss: 2.659411]\n",
      "epoch:14 step:11308 [D loss: 0.344263, acc.: 83.59%] [G loss: 2.931160]\n",
      "epoch:14 step:11309 [D loss: 0.406303, acc.: 82.03%] [G loss: 2.316836]\n",
      "epoch:14 step:11310 [D loss: 0.183829, acc.: 96.09%] [G loss: 3.033986]\n",
      "epoch:14 step:11311 [D loss: 0.282201, acc.: 88.28%] [G loss: 3.351306]\n",
      "epoch:14 step:11312 [D loss: 0.369464, acc.: 83.59%] [G loss: 3.143999]\n",
      "epoch:14 step:11313 [D loss: 0.270770, acc.: 87.50%] [G loss: 3.062783]\n",
      "epoch:14 step:11314 [D loss: 0.286077, acc.: 88.28%] [G loss: 3.204530]\n",
      "epoch:14 step:11315 [D loss: 0.195879, acc.: 91.41%] [G loss: 4.061787]\n",
      "epoch:14 step:11316 [D loss: 0.300576, acc.: 90.62%] [G loss: 3.165062]\n",
      "epoch:14 step:11317 [D loss: 0.322464, acc.: 85.94%] [G loss: 3.158396]\n",
      "epoch:14 step:11318 [D loss: 0.390937, acc.: 82.03%] [G loss: 2.344865]\n",
      "epoch:14 step:11319 [D loss: 0.307059, acc.: 88.28%] [G loss: 3.367757]\n",
      "epoch:14 step:11320 [D loss: 0.289952, acc.: 89.84%] [G loss: 4.372775]\n",
      "epoch:14 step:11321 [D loss: 0.354174, acc.: 84.38%] [G loss: 3.941672]\n",
      "epoch:14 step:11322 [D loss: 0.296631, acc.: 85.94%] [G loss: 3.815188]\n",
      "epoch:14 step:11323 [D loss: 0.285820, acc.: 90.62%] [G loss: 3.802987]\n",
      "epoch:14 step:11324 [D loss: 0.330140, acc.: 85.94%] [G loss: 3.034661]\n",
      "epoch:14 step:11325 [D loss: 0.470540, acc.: 78.91%] [G loss: 3.473397]\n",
      "epoch:14 step:11326 [D loss: 0.242675, acc.: 91.41%] [G loss: 4.644547]\n",
      "epoch:14 step:11327 [D loss: 0.349650, acc.: 81.25%] [G loss: 4.096963]\n",
      "epoch:14 step:11328 [D loss: 0.417969, acc.: 78.91%] [G loss: 2.724674]\n",
      "epoch:14 step:11329 [D loss: 0.497088, acc.: 75.78%] [G loss: 3.106123]\n",
      "epoch:14 step:11330 [D loss: 0.327724, acc.: 82.81%] [G loss: 3.435545]\n",
      "epoch:14 step:11331 [D loss: 0.403058, acc.: 79.69%] [G loss: 3.824311]\n",
      "epoch:14 step:11332 [D loss: 0.526703, acc.: 80.47%] [G loss: 3.826648]\n",
      "epoch:14 step:11333 [D loss: 0.447926, acc.: 76.56%] [G loss: 3.762142]\n",
      "epoch:14 step:11334 [D loss: 0.319955, acc.: 85.16%] [G loss: 3.204697]\n",
      "epoch:14 step:11335 [D loss: 0.503080, acc.: 78.91%] [G loss: 4.259914]\n",
      "epoch:14 step:11336 [D loss: 0.417479, acc.: 85.16%] [G loss: 2.827938]\n",
      "epoch:14 step:11337 [D loss: 0.377079, acc.: 82.81%] [G loss: 3.350739]\n",
      "epoch:14 step:11338 [D loss: 0.342633, acc.: 84.38%] [G loss: 2.931228]\n",
      "epoch:14 step:11339 [D loss: 0.332252, acc.: 88.28%] [G loss: 5.116091]\n",
      "epoch:14 step:11340 [D loss: 0.415420, acc.: 81.25%] [G loss: 2.756854]\n",
      "epoch:14 step:11341 [D loss: 0.372968, acc.: 82.03%] [G loss: 3.814423]\n",
      "epoch:14 step:11342 [D loss: 0.333097, acc.: 80.47%] [G loss: 6.012621]\n",
      "epoch:14 step:11343 [D loss: 0.249737, acc.: 87.50%] [G loss: 5.051729]\n",
      "epoch:14 step:11344 [D loss: 0.449209, acc.: 78.91%] [G loss: 2.520025]\n",
      "epoch:14 step:11345 [D loss: 0.308436, acc.: 92.19%] [G loss: 3.396353]\n",
      "epoch:14 step:11346 [D loss: 0.458618, acc.: 78.12%] [G loss: 5.908928]\n",
      "epoch:14 step:11347 [D loss: 0.360438, acc.: 85.94%] [G loss: 2.701927]\n",
      "epoch:14 step:11348 [D loss: 0.293260, acc.: 88.28%] [G loss: 3.558552]\n",
      "epoch:14 step:11349 [D loss: 0.385921, acc.: 84.38%] [G loss: 7.124773]\n",
      "epoch:14 step:11350 [D loss: 0.346417, acc.: 82.03%] [G loss: 3.783897]\n",
      "epoch:14 step:11351 [D loss: 0.304863, acc.: 83.59%] [G loss: 2.955044]\n",
      "epoch:14 step:11352 [D loss: 0.275968, acc.: 88.28%] [G loss: 3.077320]\n",
      "epoch:14 step:11353 [D loss: 0.347922, acc.: 82.03%] [G loss: 5.352277]\n",
      "epoch:14 step:11354 [D loss: 0.378538, acc.: 85.16%] [G loss: 3.101534]\n",
      "epoch:14 step:11355 [D loss: 0.222527, acc.: 92.19%] [G loss: 6.657943]\n",
      "epoch:14 step:11356 [D loss: 0.342613, acc.: 89.84%] [G loss: 2.957084]\n",
      "epoch:14 step:11357 [D loss: 0.432097, acc.: 76.56%] [G loss: 4.044143]\n",
      "epoch:14 step:11358 [D loss: 0.261044, acc.: 88.28%] [G loss: 7.708482]\n",
      "epoch:14 step:11359 [D loss: 0.271683, acc.: 89.84%] [G loss: 5.178683]\n",
      "epoch:14 step:11360 [D loss: 0.369130, acc.: 82.81%] [G loss: 3.533007]\n",
      "epoch:14 step:11361 [D loss: 0.468228, acc.: 77.34%] [G loss: 2.446325]\n",
      "epoch:14 step:11362 [D loss: 0.448781, acc.: 79.69%] [G loss: 3.888792]\n",
      "epoch:14 step:11363 [D loss: 0.479047, acc.: 75.78%] [G loss: 2.654357]\n",
      "epoch:14 step:11364 [D loss: 0.345510, acc.: 87.50%] [G loss: 4.228496]\n",
      "epoch:14 step:11365 [D loss: 0.255919, acc.: 87.50%] [G loss: 4.877725]\n",
      "epoch:14 step:11366 [D loss: 0.388311, acc.: 82.81%] [G loss: 3.329277]\n",
      "epoch:14 step:11367 [D loss: 0.298384, acc.: 86.72%] [G loss: 3.912706]\n",
      "epoch:14 step:11368 [D loss: 0.479047, acc.: 82.81%] [G loss: 3.166670]\n",
      "epoch:14 step:11369 [D loss: 0.232316, acc.: 89.84%] [G loss: 4.672140]\n",
      "epoch:14 step:11370 [D loss: 0.508947, acc.: 71.88%] [G loss: 4.035739]\n",
      "epoch:14 step:11371 [D loss: 0.583646, acc.: 78.12%] [G loss: 5.266445]\n",
      "epoch:14 step:11372 [D loss: 0.757711, acc.: 75.00%] [G loss: 8.496664]\n",
      "epoch:14 step:11373 [D loss: 1.466283, acc.: 58.59%] [G loss: 2.848406]\n",
      "epoch:14 step:11374 [D loss: 1.049677, acc.: 75.78%] [G loss: 4.545719]\n",
      "epoch:14 step:11375 [D loss: 0.266306, acc.: 86.72%] [G loss: 4.690281]\n",
      "epoch:14 step:11376 [D loss: 0.644470, acc.: 83.59%] [G loss: 4.237727]\n",
      "epoch:14 step:11377 [D loss: 0.308931, acc.: 85.94%] [G loss: 5.623130]\n",
      "epoch:14 step:11378 [D loss: 0.329592, acc.: 85.94%] [G loss: 3.845617]\n",
      "epoch:14 step:11379 [D loss: 0.351357, acc.: 85.94%] [G loss: 2.868711]\n",
      "epoch:14 step:11380 [D loss: 0.215507, acc.: 93.75%] [G loss: 3.238687]\n",
      "epoch:14 step:11381 [D loss: 0.253467, acc.: 89.06%] [G loss: 3.058173]\n",
      "epoch:14 step:11382 [D loss: 0.320550, acc.: 85.94%] [G loss: 3.094625]\n",
      "epoch:14 step:11383 [D loss: 0.267720, acc.: 87.50%] [G loss: 2.663192]\n",
      "epoch:14 step:11384 [D loss: 0.248645, acc.: 91.41%] [G loss: 2.087551]\n",
      "epoch:14 step:11385 [D loss: 0.360261, acc.: 84.38%] [G loss: 2.618252]\n",
      "epoch:14 step:11386 [D loss: 0.252206, acc.: 92.97%] [G loss: 4.058688]\n",
      "epoch:14 step:11387 [D loss: 0.211028, acc.: 91.41%] [G loss: 5.220288]\n",
      "epoch:14 step:11388 [D loss: 0.257709, acc.: 91.41%] [G loss: 4.216900]\n",
      "epoch:14 step:11389 [D loss: 0.328953, acc.: 85.16%] [G loss: 2.546812]\n",
      "epoch:14 step:11390 [D loss: 0.372727, acc.: 81.25%] [G loss: 2.370016]\n",
      "epoch:14 step:11391 [D loss: 0.254483, acc.: 92.97%] [G loss: 2.499852]\n",
      "epoch:14 step:11392 [D loss: 0.334984, acc.: 85.94%] [G loss: 2.755572]\n",
      "epoch:14 step:11393 [D loss: 0.314201, acc.: 86.72%] [G loss: 3.210881]\n",
      "epoch:14 step:11394 [D loss: 0.431236, acc.: 76.56%] [G loss: 2.554819]\n",
      "epoch:14 step:11395 [D loss: 0.324535, acc.: 86.72%] [G loss: 3.519704]\n",
      "epoch:14 step:11396 [D loss: 0.259571, acc.: 91.41%] [G loss: 2.594536]\n",
      "epoch:14 step:11397 [D loss: 0.246967, acc.: 89.84%] [G loss: 3.843591]\n",
      "epoch:14 step:11398 [D loss: 0.372744, acc.: 83.59%] [G loss: 2.585799]\n",
      "epoch:14 step:11399 [D loss: 0.349117, acc.: 82.81%] [G loss: 3.852418]\n",
      "epoch:14 step:11400 [D loss: 0.251483, acc.: 89.84%] [G loss: 3.301426]\n",
      "##############\n",
      "[0.84375147 0.85806052 0.79125329 0.8244941  0.77705974 0.83141654\n",
      " 0.88188676 0.82937356 0.80357704 0.80397603]\n",
      "##########\n",
      "epoch:14 step:11401 [D loss: 0.358503, acc.: 81.25%] [G loss: 3.667718]\n",
      "epoch:14 step:11402 [D loss: 0.355244, acc.: 87.50%] [G loss: 3.640498]\n",
      "epoch:14 step:11403 [D loss: 0.294913, acc.: 88.28%] [G loss: 3.659878]\n",
      "epoch:14 step:11404 [D loss: 0.311135, acc.: 86.72%] [G loss: 4.870209]\n",
      "epoch:14 step:11405 [D loss: 0.244265, acc.: 92.19%] [G loss: 3.329455]\n",
      "epoch:14 step:11406 [D loss: 0.271861, acc.: 85.16%] [G loss: 7.093732]\n",
      "epoch:14 step:11407 [D loss: 0.244613, acc.: 89.06%] [G loss: 6.363068]\n",
      "epoch:14 step:11408 [D loss: 0.344101, acc.: 85.16%] [G loss: 2.810493]\n",
      "epoch:14 step:11409 [D loss: 0.219600, acc.: 89.84%] [G loss: 4.427539]\n",
      "epoch:14 step:11410 [D loss: 0.164630, acc.: 94.53%] [G loss: 3.597988]\n",
      "epoch:14 step:11411 [D loss: 0.199765, acc.: 92.97%] [G loss: 3.859693]\n",
      "epoch:14 step:11412 [D loss: 0.239939, acc.: 89.06%] [G loss: 5.369728]\n",
      "epoch:14 step:11413 [D loss: 0.240974, acc.: 89.84%] [G loss: 4.596417]\n",
      "epoch:14 step:11414 [D loss: 0.222481, acc.: 86.72%] [G loss: 6.434860]\n",
      "epoch:14 step:11415 [D loss: 0.307118, acc.: 85.94%] [G loss: 5.201881]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:14 step:11416 [D loss: 0.332105, acc.: 85.94%] [G loss: 3.002040]\n",
      "epoch:14 step:11417 [D loss: 0.299811, acc.: 89.06%] [G loss: 3.015517]\n",
      "epoch:14 step:11418 [D loss: 0.244431, acc.: 93.75%] [G loss: 3.194905]\n",
      "epoch:14 step:11419 [D loss: 0.258682, acc.: 89.84%] [G loss: 2.289413]\n",
      "epoch:14 step:11420 [D loss: 0.394908, acc.: 88.28%] [G loss: 2.874736]\n",
      "epoch:14 step:11421 [D loss: 0.253386, acc.: 91.41%] [G loss: 2.812625]\n",
      "epoch:14 step:11422 [D loss: 0.300863, acc.: 87.50%] [G loss: 2.550326]\n",
      "epoch:14 step:11423 [D loss: 0.378638, acc.: 82.81%] [G loss: 2.368788]\n",
      "epoch:14 step:11424 [D loss: 0.320525, acc.: 88.28%] [G loss: 2.797198]\n",
      "epoch:14 step:11425 [D loss: 0.240085, acc.: 89.06%] [G loss: 2.650301]\n",
      "epoch:14 step:11426 [D loss: 0.382131, acc.: 82.03%] [G loss: 2.457625]\n",
      "epoch:14 step:11427 [D loss: 0.340811, acc.: 85.94%] [G loss: 2.395536]\n",
      "epoch:14 step:11428 [D loss: 0.301648, acc.: 88.28%] [G loss: 2.556713]\n",
      "epoch:14 step:11429 [D loss: 0.330877, acc.: 86.72%] [G loss: 3.125833]\n",
      "epoch:14 step:11430 [D loss: 0.330064, acc.: 85.16%] [G loss: 2.939368]\n",
      "epoch:14 step:11431 [D loss: 0.275797, acc.: 89.84%] [G loss: 3.717335]\n",
      "epoch:14 step:11432 [D loss: 0.262130, acc.: 90.62%] [G loss: 3.171021]\n",
      "epoch:14 step:11433 [D loss: 0.320944, acc.: 87.50%] [G loss: 4.291348]\n",
      "epoch:14 step:11434 [D loss: 0.152769, acc.: 94.53%] [G loss: 7.338145]\n",
      "epoch:14 step:11435 [D loss: 0.331490, acc.: 82.81%] [G loss: 3.579777]\n",
      "epoch:14 step:11436 [D loss: 0.371185, acc.: 88.28%] [G loss: 3.250751]\n",
      "epoch:14 step:11437 [D loss: 0.390032, acc.: 83.59%] [G loss: 2.506047]\n",
      "epoch:14 step:11438 [D loss: 0.323821, acc.: 82.81%] [G loss: 3.218024]\n",
      "epoch:14 step:11439 [D loss: 0.368588, acc.: 83.59%] [G loss: 3.006489]\n",
      "epoch:14 step:11440 [D loss: 0.343699, acc.: 88.28%] [G loss: 2.746303]\n",
      "epoch:14 step:11441 [D loss: 0.333057, acc.: 83.59%] [G loss: 2.079545]\n",
      "epoch:14 step:11442 [D loss: 0.298717, acc.: 91.41%] [G loss: 2.417710]\n",
      "epoch:14 step:11443 [D loss: 0.246165, acc.: 92.19%] [G loss: 2.731804]\n",
      "epoch:14 step:11444 [D loss: 0.412235, acc.: 78.12%] [G loss: 2.675710]\n",
      "epoch:14 step:11445 [D loss: 0.222985, acc.: 92.97%] [G loss: 2.844584]\n",
      "epoch:14 step:11446 [D loss: 0.286196, acc.: 91.41%] [G loss: 2.927813]\n",
      "epoch:14 step:11447 [D loss: 0.381654, acc.: 78.91%] [G loss: 4.134237]\n",
      "epoch:14 step:11448 [D loss: 0.440395, acc.: 82.03%] [G loss: 4.207974]\n",
      "epoch:14 step:11449 [D loss: 0.593882, acc.: 75.78%] [G loss: 5.427250]\n",
      "epoch:14 step:11450 [D loss: 0.658664, acc.: 72.66%] [G loss: 7.968819]\n",
      "epoch:14 step:11451 [D loss: 1.013384, acc.: 68.75%] [G loss: 6.916116]\n",
      "epoch:14 step:11452 [D loss: 0.855646, acc.: 64.06%] [G loss: 2.869339]\n",
      "epoch:14 step:11453 [D loss: 0.573773, acc.: 79.69%] [G loss: 2.946068]\n",
      "epoch:14 step:11454 [D loss: 0.497457, acc.: 83.59%] [G loss: 3.074037]\n",
      "epoch:14 step:11455 [D loss: 0.346274, acc.: 81.25%] [G loss: 3.765334]\n",
      "epoch:14 step:11456 [D loss: 0.270212, acc.: 92.19%] [G loss: 2.985618]\n",
      "epoch:14 step:11457 [D loss: 0.375295, acc.: 83.59%] [G loss: 3.394266]\n",
      "epoch:14 step:11458 [D loss: 0.345256, acc.: 82.81%] [G loss: 3.780494]\n",
      "epoch:14 step:11459 [D loss: 0.271602, acc.: 89.06%] [G loss: 3.585411]\n",
      "epoch:14 step:11460 [D loss: 0.457459, acc.: 82.03%] [G loss: 2.810833]\n",
      "epoch:14 step:11461 [D loss: 0.301059, acc.: 91.41%] [G loss: 3.357898]\n",
      "epoch:14 step:11462 [D loss: 0.226679, acc.: 89.84%] [G loss: 3.143803]\n",
      "epoch:14 step:11463 [D loss: 0.328235, acc.: 82.81%] [G loss: 2.316234]\n",
      "epoch:14 step:11464 [D loss: 0.328441, acc.: 85.94%] [G loss: 3.027176]\n",
      "epoch:14 step:11465 [D loss: 0.211920, acc.: 89.84%] [G loss: 3.130966]\n",
      "epoch:14 step:11466 [D loss: 0.384089, acc.: 81.25%] [G loss: 3.057369]\n",
      "epoch:14 step:11467 [D loss: 0.246793, acc.: 89.06%] [G loss: 3.930093]\n",
      "epoch:14 step:11468 [D loss: 0.341594, acc.: 89.84%] [G loss: 2.777733]\n",
      "epoch:14 step:11469 [D loss: 0.351778, acc.: 85.16%] [G loss: 2.674761]\n",
      "epoch:14 step:11470 [D loss: 0.316161, acc.: 86.72%] [G loss: 2.679554]\n",
      "epoch:14 step:11471 [D loss: 0.319656, acc.: 86.72%] [G loss: 2.508087]\n",
      "epoch:14 step:11472 [D loss: 0.271579, acc.: 90.62%] [G loss: 3.326386]\n",
      "epoch:14 step:11473 [D loss: 0.357757, acc.: 85.94%] [G loss: 2.897187]\n",
      "epoch:14 step:11474 [D loss: 0.285757, acc.: 88.28%] [G loss: 2.766618]\n",
      "epoch:14 step:11475 [D loss: 0.300130, acc.: 87.50%] [G loss: 2.682704]\n",
      "epoch:14 step:11476 [D loss: 0.315873, acc.: 82.81%] [G loss: 3.188497]\n",
      "epoch:14 step:11477 [D loss: 0.385687, acc.: 83.59%] [G loss: 2.942860]\n",
      "epoch:14 step:11478 [D loss: 0.406630, acc.: 82.81%] [G loss: 3.008179]\n",
      "epoch:14 step:11479 [D loss: 0.231451, acc.: 89.06%] [G loss: 3.745790]\n",
      "epoch:14 step:11480 [D loss: 0.347468, acc.: 89.06%] [G loss: 3.526167]\n",
      "epoch:14 step:11481 [D loss: 0.431359, acc.: 82.03%] [G loss: 2.567343]\n",
      "epoch:14 step:11482 [D loss: 0.338786, acc.: 80.47%] [G loss: 2.750720]\n",
      "epoch:14 step:11483 [D loss: 0.274643, acc.: 89.06%] [G loss: 4.196681]\n",
      "epoch:14 step:11484 [D loss: 0.249791, acc.: 90.62%] [G loss: 4.171287]\n",
      "epoch:14 step:11485 [D loss: 0.276248, acc.: 89.06%] [G loss: 3.287313]\n",
      "epoch:14 step:11486 [D loss: 0.393251, acc.: 82.03%] [G loss: 2.835716]\n",
      "epoch:14 step:11487 [D loss: 0.279235, acc.: 89.84%] [G loss: 4.244238]\n",
      "epoch:14 step:11488 [D loss: 0.330997, acc.: 82.81%] [G loss: 2.555774]\n",
      "epoch:14 step:11489 [D loss: 0.278186, acc.: 89.84%] [G loss: 3.795653]\n",
      "epoch:14 step:11490 [D loss: 0.310100, acc.: 83.59%] [G loss: 3.504989]\n",
      "epoch:14 step:11491 [D loss: 0.250384, acc.: 89.84%] [G loss: 2.878691]\n",
      "epoch:14 step:11492 [D loss: 0.364177, acc.: 85.94%] [G loss: 2.331117]\n",
      "epoch:14 step:11493 [D loss: 0.392469, acc.: 84.38%] [G loss: 2.556559]\n",
      "epoch:14 step:11494 [D loss: 0.297053, acc.: 88.28%] [G loss: 2.973388]\n",
      "epoch:14 step:11495 [D loss: 0.374308, acc.: 84.38%] [G loss: 3.637779]\n",
      "epoch:14 step:11496 [D loss: 0.245777, acc.: 91.41%] [G loss: 3.891051]\n",
      "epoch:14 step:11497 [D loss: 0.270126, acc.: 88.28%] [G loss: 2.612116]\n",
      "epoch:14 step:11498 [D loss: 0.227900, acc.: 91.41%] [G loss: 3.856685]\n",
      "epoch:14 step:11499 [D loss: 0.226614, acc.: 89.06%] [G loss: 5.140259]\n",
      "epoch:14 step:11500 [D loss: 0.284656, acc.: 85.94%] [G loss: 4.211632]\n",
      "epoch:14 step:11501 [D loss: 0.380587, acc.: 82.03%] [G loss: 6.240849]\n",
      "epoch:14 step:11502 [D loss: 0.278118, acc.: 88.28%] [G loss: 3.321212]\n",
      "epoch:14 step:11503 [D loss: 0.278456, acc.: 91.41%] [G loss: 2.423972]\n",
      "epoch:14 step:11504 [D loss: 0.266373, acc.: 86.72%] [G loss: 3.674339]\n",
      "epoch:14 step:11505 [D loss: 0.304587, acc.: 86.72%] [G loss: 2.903867]\n",
      "epoch:14 step:11506 [D loss: 0.344046, acc.: 83.59%] [G loss: 2.410002]\n",
      "epoch:14 step:11507 [D loss: 0.274233, acc.: 89.84%] [G loss: 2.524784]\n",
      "epoch:14 step:11508 [D loss: 0.397451, acc.: 85.94%] [G loss: 2.290916]\n",
      "epoch:14 step:11509 [D loss: 0.328188, acc.: 89.84%] [G loss: 2.236973]\n",
      "epoch:14 step:11510 [D loss: 0.289131, acc.: 85.16%] [G loss: 3.322267]\n",
      "epoch:14 step:11511 [D loss: 0.320657, acc.: 83.59%] [G loss: 3.857871]\n",
      "epoch:14 step:11512 [D loss: 0.345422, acc.: 85.94%] [G loss: 3.160851]\n",
      "epoch:14 step:11513 [D loss: 0.309180, acc.: 89.84%] [G loss: 2.469512]\n",
      "epoch:14 step:11514 [D loss: 0.269942, acc.: 86.72%] [G loss: 3.618583]\n",
      "epoch:14 step:11515 [D loss: 0.342092, acc.: 84.38%] [G loss: 3.708483]\n",
      "epoch:14 step:11516 [D loss: 0.249293, acc.: 88.28%] [G loss: 3.335190]\n",
      "epoch:14 step:11517 [D loss: 0.338164, acc.: 83.59%] [G loss: 2.059674]\n",
      "epoch:14 step:11518 [D loss: 0.444488, acc.: 78.12%] [G loss: 2.739918]\n",
      "epoch:14 step:11519 [D loss: 0.355859, acc.: 85.16%] [G loss: 2.833442]\n",
      "epoch:14 step:11520 [D loss: 0.416811, acc.: 81.25%] [G loss: 2.378894]\n",
      "epoch:14 step:11521 [D loss: 0.252046, acc.: 89.06%] [G loss: 3.319678]\n",
      "epoch:14 step:11522 [D loss: 0.219089, acc.: 89.84%] [G loss: 3.827104]\n",
      "epoch:14 step:11523 [D loss: 0.450350, acc.: 79.69%] [G loss: 2.573369]\n",
      "epoch:14 step:11524 [D loss: 0.413462, acc.: 83.59%] [G loss: 2.342923]\n",
      "epoch:14 step:11525 [D loss: 0.367870, acc.: 85.16%] [G loss: 3.111653]\n",
      "epoch:14 step:11526 [D loss: 0.338248, acc.: 84.38%] [G loss: 3.598843]\n",
      "epoch:14 step:11527 [D loss: 0.247696, acc.: 89.84%] [G loss: 6.864277]\n",
      "epoch:14 step:11528 [D loss: 0.304340, acc.: 89.84%] [G loss: 2.818687]\n",
      "epoch:14 step:11529 [D loss: 0.457200, acc.: 78.91%] [G loss: 4.477436]\n",
      "epoch:14 step:11530 [D loss: 0.325845, acc.: 82.81%] [G loss: 5.538134]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:14 step:11531 [D loss: 0.405943, acc.: 82.81%] [G loss: 2.509351]\n",
      "epoch:14 step:11532 [D loss: 0.317184, acc.: 88.28%] [G loss: 2.872565]\n",
      "epoch:14 step:11533 [D loss: 0.363666, acc.: 82.81%] [G loss: 4.548814]\n",
      "epoch:14 step:11534 [D loss: 0.322591, acc.: 85.16%] [G loss: 4.747458]\n",
      "epoch:14 step:11535 [D loss: 0.260404, acc.: 88.28%] [G loss: 3.864629]\n",
      "epoch:14 step:11536 [D loss: 0.318134, acc.: 89.06%] [G loss: 2.559589]\n",
      "epoch:14 step:11537 [D loss: 0.207620, acc.: 93.75%] [G loss: 3.477599]\n",
      "epoch:14 step:11538 [D loss: 0.381691, acc.: 83.59%] [G loss: 4.027918]\n",
      "epoch:14 step:11539 [D loss: 0.407109, acc.: 83.59%] [G loss: 2.903301]\n",
      "epoch:14 step:11540 [D loss: 0.470796, acc.: 77.34%] [G loss: 3.642347]\n",
      "epoch:14 step:11541 [D loss: 0.234681, acc.: 90.62%] [G loss: 4.781494]\n",
      "epoch:14 step:11542 [D loss: 0.298661, acc.: 84.38%] [G loss: 3.856848]\n",
      "epoch:14 step:11543 [D loss: 0.373045, acc.: 83.59%] [G loss: 3.522302]\n",
      "epoch:14 step:11544 [D loss: 0.467462, acc.: 80.47%] [G loss: 5.073349]\n",
      "epoch:14 step:11545 [D loss: 0.487118, acc.: 80.47%] [G loss: 3.810911]\n",
      "epoch:14 step:11546 [D loss: 0.297542, acc.: 89.84%] [G loss: 4.386842]\n",
      "epoch:14 step:11547 [D loss: 0.348576, acc.: 83.59%] [G loss: 3.958889]\n",
      "epoch:14 step:11548 [D loss: 0.505642, acc.: 82.81%] [G loss: 3.799582]\n",
      "epoch:14 step:11549 [D loss: 0.518411, acc.: 81.25%] [G loss: 4.467113]\n",
      "epoch:14 step:11550 [D loss: 0.509782, acc.: 79.69%] [G loss: 3.659406]\n",
      "epoch:14 step:11551 [D loss: 0.514701, acc.: 76.56%] [G loss: 3.012080]\n",
      "epoch:14 step:11552 [D loss: 0.369448, acc.: 82.03%] [G loss: 3.768976]\n",
      "epoch:14 step:11553 [D loss: 0.408488, acc.: 85.94%] [G loss: 4.330707]\n",
      "epoch:14 step:11554 [D loss: 0.386671, acc.: 85.94%] [G loss: 3.564814]\n",
      "epoch:14 step:11555 [D loss: 0.245039, acc.: 90.62%] [G loss: 5.481391]\n",
      "epoch:14 step:11556 [D loss: 0.288922, acc.: 85.94%] [G loss: 3.415935]\n",
      "epoch:14 step:11557 [D loss: 0.381779, acc.: 78.91%] [G loss: 2.143291]\n",
      "epoch:14 step:11558 [D loss: 0.343555, acc.: 85.94%] [G loss: 3.108038]\n",
      "epoch:14 step:11559 [D loss: 0.335330, acc.: 88.28%] [G loss: 3.491213]\n",
      "epoch:14 step:11560 [D loss: 0.305046, acc.: 84.38%] [G loss: 6.338960]\n",
      "epoch:14 step:11561 [D loss: 0.260611, acc.: 88.28%] [G loss: 4.550715]\n",
      "epoch:14 step:11562 [D loss: 0.481442, acc.: 78.91%] [G loss: 4.863675]\n",
      "epoch:14 step:11563 [D loss: 0.486573, acc.: 81.25%] [G loss: 6.497878]\n",
      "epoch:14 step:11564 [D loss: 0.845072, acc.: 67.97%] [G loss: 3.141910]\n",
      "epoch:14 step:11565 [D loss: 0.530797, acc.: 78.12%] [G loss: 2.867479]\n",
      "epoch:14 step:11566 [D loss: 0.401748, acc.: 82.81%] [G loss: 2.750120]\n",
      "epoch:14 step:11567 [D loss: 0.383694, acc.: 79.69%] [G loss: 2.768346]\n",
      "epoch:14 step:11568 [D loss: 0.335282, acc.: 84.38%] [G loss: 3.350023]\n",
      "epoch:14 step:11569 [D loss: 0.417704, acc.: 82.03%] [G loss: 2.606928]\n",
      "epoch:14 step:11570 [D loss: 0.224559, acc.: 94.53%] [G loss: 4.112394]\n",
      "epoch:14 step:11571 [D loss: 0.311930, acc.: 85.94%] [G loss: 3.430288]\n",
      "epoch:14 step:11572 [D loss: 0.327755, acc.: 85.94%] [G loss: 2.646273]\n",
      "epoch:14 step:11573 [D loss: 0.352681, acc.: 81.25%] [G loss: 2.745627]\n",
      "epoch:14 step:11574 [D loss: 0.269446, acc.: 90.62%] [G loss: 2.824505]\n",
      "epoch:14 step:11575 [D loss: 0.279230, acc.: 89.06%] [G loss: 3.419924]\n",
      "epoch:14 step:11576 [D loss: 0.333640, acc.: 84.38%] [G loss: 2.466429]\n",
      "epoch:14 step:11577 [D loss: 0.232762, acc.: 92.97%] [G loss: 2.765493]\n",
      "epoch:14 step:11578 [D loss: 0.275266, acc.: 85.94%] [G loss: 3.668329]\n",
      "epoch:14 step:11579 [D loss: 0.412186, acc.: 84.38%] [G loss: 2.713984]\n",
      "epoch:14 step:11580 [D loss: 0.381657, acc.: 82.03%] [G loss: 2.655988]\n",
      "epoch:14 step:11581 [D loss: 0.247778, acc.: 91.41%] [G loss: 2.956660]\n",
      "epoch:14 step:11582 [D loss: 0.436831, acc.: 76.56%] [G loss: 3.292961]\n",
      "epoch:14 step:11583 [D loss: 0.450429, acc.: 81.25%] [G loss: 2.652942]\n",
      "epoch:14 step:11584 [D loss: 0.368471, acc.: 80.47%] [G loss: 2.692321]\n",
      "epoch:14 step:11585 [D loss: 0.295691, acc.: 88.28%] [G loss: 3.238966]\n",
      "epoch:14 step:11586 [D loss: 0.177088, acc.: 95.31%] [G loss: 3.473184]\n",
      "epoch:14 step:11587 [D loss: 0.294632, acc.: 90.62%] [G loss: 2.305960]\n",
      "epoch:14 step:11588 [D loss: 0.307576, acc.: 89.06%] [G loss: 3.019877]\n",
      "epoch:14 step:11589 [D loss: 0.228152, acc.: 92.97%] [G loss: 3.600214]\n",
      "epoch:14 step:11590 [D loss: 0.275810, acc.: 88.28%] [G loss: 2.984251]\n",
      "epoch:14 step:11591 [D loss: 0.333518, acc.: 85.94%] [G loss: 3.400553]\n",
      "epoch:14 step:11592 [D loss: 0.349224, acc.: 84.38%] [G loss: 3.240610]\n",
      "epoch:14 step:11593 [D loss: 0.273194, acc.: 87.50%] [G loss: 2.941357]\n",
      "epoch:14 step:11594 [D loss: 0.373221, acc.: 81.25%] [G loss: 4.925848]\n",
      "epoch:14 step:11595 [D loss: 0.272583, acc.: 88.28%] [G loss: 4.545115]\n",
      "epoch:14 step:11596 [D loss: 0.412546, acc.: 82.03%] [G loss: 2.735382]\n",
      "epoch:14 step:11597 [D loss: 0.219614, acc.: 92.19%] [G loss: 4.376873]\n",
      "epoch:14 step:11598 [D loss: 0.354100, acc.: 85.16%] [G loss: 3.117635]\n",
      "epoch:14 step:11599 [D loss: 0.284852, acc.: 87.50%] [G loss: 3.203648]\n",
      "epoch:14 step:11600 [D loss: 0.335698, acc.: 84.38%] [G loss: 3.268468]\n",
      "##############\n",
      "[0.82516567 0.85864397 0.79201622 0.80782477 0.7879211  0.83816271\n",
      " 0.86899811 0.82725171 0.82687417 0.81736421]\n",
      "##########\n",
      "epoch:14 step:11601 [D loss: 0.250187, acc.: 90.62%] [G loss: 3.138680]\n",
      "epoch:14 step:11602 [D loss: 0.281388, acc.: 90.62%] [G loss: 3.559488]\n",
      "epoch:14 step:11603 [D loss: 0.267371, acc.: 87.50%] [G loss: 3.072144]\n",
      "epoch:14 step:11604 [D loss: 0.421944, acc.: 84.38%] [G loss: 4.016711]\n",
      "epoch:14 step:11605 [D loss: 0.234183, acc.: 89.84%] [G loss: 4.574074]\n",
      "epoch:14 step:11606 [D loss: 0.341725, acc.: 84.38%] [G loss: 3.516727]\n",
      "epoch:14 step:11607 [D loss: 0.331507, acc.: 85.94%] [G loss: 2.641611]\n",
      "epoch:14 step:11608 [D loss: 0.302990, acc.: 84.38%] [G loss: 4.659861]\n",
      "epoch:14 step:11609 [D loss: 0.323373, acc.: 88.28%] [G loss: 4.690048]\n",
      "epoch:14 step:11610 [D loss: 0.294350, acc.: 86.72%] [G loss: 3.381663]\n",
      "epoch:14 step:11611 [D loss: 0.238014, acc.: 90.62%] [G loss: 3.878942]\n",
      "epoch:14 step:11612 [D loss: 0.241109, acc.: 88.28%] [G loss: 4.002959]\n",
      "epoch:14 step:11613 [D loss: 0.273491, acc.: 86.72%] [G loss: 2.494560]\n",
      "epoch:14 step:11614 [D loss: 0.347757, acc.: 82.03%] [G loss: 3.518674]\n",
      "epoch:14 step:11615 [D loss: 0.324300, acc.: 83.59%] [G loss: 3.153712]\n",
      "epoch:14 step:11616 [D loss: 0.364921, acc.: 79.69%] [G loss: 5.710711]\n",
      "epoch:14 step:11617 [D loss: 0.595564, acc.: 80.47%] [G loss: 6.552855]\n",
      "epoch:14 step:11618 [D loss: 1.352843, acc.: 54.69%] [G loss: 6.009297]\n",
      "epoch:14 step:11619 [D loss: 1.758694, acc.: 50.78%] [G loss: 3.888717]\n",
      "epoch:14 step:11620 [D loss: 0.265177, acc.: 84.38%] [G loss: 5.367455]\n",
      "epoch:14 step:11621 [D loss: 0.946299, acc.: 74.22%] [G loss: 7.742319]\n",
      "epoch:14 step:11622 [D loss: 0.760826, acc.: 75.78%] [G loss: 3.547697]\n",
      "epoch:14 step:11623 [D loss: 0.525931, acc.: 82.81%] [G loss: 2.418057]\n",
      "epoch:14 step:11624 [D loss: 0.378187, acc.: 84.38%] [G loss: 3.603231]\n",
      "epoch:14 step:11625 [D loss: 0.385587, acc.: 82.81%] [G loss: 3.039201]\n",
      "epoch:14 step:11626 [D loss: 0.400969, acc.: 84.38%] [G loss: 3.368709]\n",
      "epoch:14 step:11627 [D loss: 0.298972, acc.: 89.06%] [G loss: 2.632121]\n",
      "epoch:14 step:11628 [D loss: 0.247170, acc.: 89.84%] [G loss: 2.917157]\n",
      "epoch:14 step:11629 [D loss: 0.276774, acc.: 88.28%] [G loss: 3.156129]\n",
      "epoch:14 step:11630 [D loss: 0.347086, acc.: 82.03%] [G loss: 2.866295]\n",
      "epoch:14 step:11631 [D loss: 0.241928, acc.: 93.75%] [G loss: 3.382063]\n",
      "epoch:14 step:11632 [D loss: 0.342171, acc.: 82.81%] [G loss: 3.444700]\n",
      "epoch:14 step:11633 [D loss: 0.260797, acc.: 89.06%] [G loss: 3.097066]\n",
      "epoch:14 step:11634 [D loss: 0.259746, acc.: 91.41%] [G loss: 2.880189]\n",
      "epoch:14 step:11635 [D loss: 0.278787, acc.: 89.84%] [G loss: 2.722015]\n",
      "epoch:14 step:11636 [D loss: 0.280259, acc.: 88.28%] [G loss: 2.895069]\n",
      "epoch:14 step:11637 [D loss: 0.346478, acc.: 83.59%] [G loss: 3.106932]\n",
      "epoch:14 step:11638 [D loss: 0.277886, acc.: 88.28%] [G loss: 3.754187]\n",
      "epoch:14 step:11639 [D loss: 0.307639, acc.: 85.16%] [G loss: 3.360366]\n",
      "epoch:14 step:11640 [D loss: 0.336284, acc.: 83.59%] [G loss: 2.547222]\n",
      "epoch:14 step:11641 [D loss: 0.302491, acc.: 92.97%] [G loss: 2.476749]\n",
      "epoch:14 step:11642 [D loss: 0.362206, acc.: 82.03%] [G loss: 2.897495]\n",
      "epoch:14 step:11643 [D loss: 0.293993, acc.: 85.16%] [G loss: 3.074926]\n",
      "epoch:14 step:11644 [D loss: 0.324111, acc.: 85.94%] [G loss: 2.408638]\n",
      "epoch:14 step:11645 [D loss: 0.293639, acc.: 87.50%] [G loss: 2.700799]\n",
      "epoch:14 step:11646 [D loss: 0.320681, acc.: 85.16%] [G loss: 2.769874]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:14 step:11647 [D loss: 0.431917, acc.: 80.47%] [G loss: 2.175946]\n",
      "epoch:14 step:11648 [D loss: 0.273247, acc.: 89.84%] [G loss: 2.662473]\n",
      "epoch:14 step:11649 [D loss: 0.438648, acc.: 81.25%] [G loss: 3.063684]\n",
      "epoch:14 step:11650 [D loss: 0.314772, acc.: 86.72%] [G loss: 3.632661]\n",
      "epoch:14 step:11651 [D loss: 0.251424, acc.: 89.06%] [G loss: 3.853466]\n",
      "epoch:14 step:11652 [D loss: 0.273225, acc.: 89.06%] [G loss: 3.376147]\n",
      "epoch:14 step:11653 [D loss: 0.286811, acc.: 87.50%] [G loss: 2.257610]\n",
      "epoch:14 step:11654 [D loss: 0.364599, acc.: 84.38%] [G loss: 2.119429]\n",
      "epoch:14 step:11655 [D loss: 0.373790, acc.: 83.59%] [G loss: 2.110583]\n",
      "epoch:14 step:11656 [D loss: 0.358896, acc.: 85.16%] [G loss: 3.028368]\n",
      "epoch:14 step:11657 [D loss: 0.288209, acc.: 90.62%] [G loss: 3.078263]\n",
      "epoch:14 step:11658 [D loss: 0.185949, acc.: 91.41%] [G loss: 3.960760]\n",
      "epoch:14 step:11659 [D loss: 0.322916, acc.: 86.72%] [G loss: 3.656320]\n",
      "epoch:14 step:11660 [D loss: 0.373556, acc.: 83.59%] [G loss: 2.620642]\n",
      "epoch:14 step:11661 [D loss: 0.355273, acc.: 83.59%] [G loss: 2.739349]\n",
      "epoch:14 step:11662 [D loss: 0.341459, acc.: 83.59%] [G loss: 2.987255]\n",
      "epoch:14 step:11663 [D loss: 0.322416, acc.: 87.50%] [G loss: 2.577184]\n",
      "epoch:14 step:11664 [D loss: 0.299441, acc.: 85.94%] [G loss: 2.594862]\n",
      "epoch:14 step:11665 [D loss: 0.268550, acc.: 88.28%] [G loss: 3.064663]\n",
      "epoch:14 step:11666 [D loss: 0.266527, acc.: 87.50%] [G loss: 2.987195]\n",
      "epoch:14 step:11667 [D loss: 0.351393, acc.: 83.59%] [G loss: 3.248727]\n",
      "epoch:14 step:11668 [D loss: 0.243216, acc.: 89.84%] [G loss: 3.268197]\n",
      "epoch:14 step:11669 [D loss: 0.384929, acc.: 82.03%] [G loss: 2.283749]\n",
      "epoch:14 step:11670 [D loss: 0.320990, acc.: 84.38%] [G loss: 2.658641]\n",
      "epoch:14 step:11671 [D loss: 0.263421, acc.: 89.06%] [G loss: 3.788878]\n",
      "epoch:14 step:11672 [D loss: 0.380876, acc.: 84.38%] [G loss: 2.447181]\n",
      "epoch:14 step:11673 [D loss: 0.392156, acc.: 83.59%] [G loss: 2.129238]\n",
      "epoch:14 step:11674 [D loss: 0.336546, acc.: 86.72%] [G loss: 2.452945]\n",
      "epoch:14 step:11675 [D loss: 0.241805, acc.: 89.06%] [G loss: 2.542121]\n",
      "epoch:14 step:11676 [D loss: 0.310479, acc.: 89.84%] [G loss: 2.335518]\n",
      "epoch:14 step:11677 [D loss: 0.333661, acc.: 83.59%] [G loss: 2.449893]\n",
      "epoch:14 step:11678 [D loss: 0.304810, acc.: 90.62%] [G loss: 3.433980]\n",
      "epoch:14 step:11679 [D loss: 0.408939, acc.: 85.94%] [G loss: 2.744389]\n",
      "epoch:14 step:11680 [D loss: 0.324985, acc.: 85.94%] [G loss: 2.766701]\n",
      "epoch:14 step:11681 [D loss: 0.349957, acc.: 83.59%] [G loss: 3.759328]\n",
      "epoch:14 step:11682 [D loss: 0.334757, acc.: 85.16%] [G loss: 3.913765]\n",
      "epoch:14 step:11683 [D loss: 0.318224, acc.: 85.16%] [G loss: 3.095099]\n",
      "epoch:14 step:11684 [D loss: 0.338760, acc.: 87.50%] [G loss: 3.149708]\n",
      "epoch:14 step:11685 [D loss: 0.387248, acc.: 81.25%] [G loss: 2.220443]\n",
      "epoch:14 step:11686 [D loss: 0.439894, acc.: 80.47%] [G loss: 2.261603]\n",
      "epoch:14 step:11687 [D loss: 0.415192, acc.: 78.12%] [G loss: 2.124263]\n",
      "epoch:14 step:11688 [D loss: 0.505722, acc.: 75.00%] [G loss: 2.924988]\n",
      "epoch:14 step:11689 [D loss: 0.298452, acc.: 88.28%] [G loss: 2.974284]\n",
      "epoch:14 step:11690 [D loss: 0.368110, acc.: 85.94%] [G loss: 4.383520]\n",
      "epoch:14 step:11691 [D loss: 0.339816, acc.: 86.72%] [G loss: 3.514030]\n",
      "epoch:14 step:11692 [D loss: 0.324492, acc.: 86.72%] [G loss: 2.827634]\n",
      "epoch:14 step:11693 [D loss: 0.271005, acc.: 88.28%] [G loss: 3.833974]\n",
      "epoch:14 step:11694 [D loss: 0.314361, acc.: 85.16%] [G loss: 2.814220]\n",
      "epoch:14 step:11695 [D loss: 0.415433, acc.: 83.59%] [G loss: 2.761213]\n",
      "epoch:14 step:11696 [D loss: 0.451000, acc.: 78.91%] [G loss: 3.790564]\n",
      "epoch:14 step:11697 [D loss: 0.359382, acc.: 89.06%] [G loss: 2.635613]\n",
      "epoch:14 step:11698 [D loss: 0.280426, acc.: 90.62%] [G loss: 2.713347]\n",
      "epoch:14 step:11699 [D loss: 0.333997, acc.: 88.28%] [G loss: 2.295659]\n",
      "epoch:14 step:11700 [D loss: 0.278651, acc.: 87.50%] [G loss: 2.805845]\n",
      "epoch:14 step:11701 [D loss: 0.288740, acc.: 87.50%] [G loss: 4.176813]\n",
      "epoch:14 step:11702 [D loss: 0.285561, acc.: 85.94%] [G loss: 5.246779]\n",
      "epoch:14 step:11703 [D loss: 0.274184, acc.: 89.06%] [G loss: 2.981937]\n",
      "epoch:14 step:11704 [D loss: 0.415011, acc.: 80.47%] [G loss: 2.788437]\n",
      "epoch:14 step:11705 [D loss: 0.292684, acc.: 88.28%] [G loss: 2.543194]\n",
      "epoch:14 step:11706 [D loss: 0.303878, acc.: 88.28%] [G loss: 2.648356]\n",
      "epoch:14 step:11707 [D loss: 0.285618, acc.: 86.72%] [G loss: 2.550153]\n",
      "epoch:14 step:11708 [D loss: 0.359316, acc.: 83.59%] [G loss: 2.421287]\n",
      "epoch:14 step:11709 [D loss: 0.318371, acc.: 85.94%] [G loss: 2.244739]\n",
      "epoch:14 step:11710 [D loss: 0.303267, acc.: 87.50%] [G loss: 2.384428]\n",
      "epoch:14 step:11711 [D loss: 0.352588, acc.: 85.16%] [G loss: 2.216132]\n",
      "epoch:14 step:11712 [D loss: 0.358645, acc.: 89.06%] [G loss: 2.141282]\n",
      "epoch:14 step:11713 [D loss: 0.331366, acc.: 86.72%] [G loss: 2.460449]\n",
      "epoch:14 step:11714 [D loss: 0.338346, acc.: 89.06%] [G loss: 2.699077]\n",
      "epoch:14 step:11715 [D loss: 0.447372, acc.: 76.56%] [G loss: 2.524510]\n",
      "epoch:15 step:11716 [D loss: 0.352098, acc.: 85.94%] [G loss: 3.006889]\n",
      "epoch:15 step:11717 [D loss: 0.205299, acc.: 93.75%] [G loss: 4.306395]\n",
      "epoch:15 step:11718 [D loss: 0.391114, acc.: 83.59%] [G loss: 3.939686]\n",
      "epoch:15 step:11719 [D loss: 0.340146, acc.: 82.03%] [G loss: 3.215564]\n",
      "epoch:15 step:11720 [D loss: 0.310030, acc.: 87.50%] [G loss: 4.198152]\n",
      "epoch:15 step:11721 [D loss: 0.284929, acc.: 87.50%] [G loss: 4.002202]\n",
      "epoch:15 step:11722 [D loss: 0.286352, acc.: 85.16%] [G loss: 3.354803]\n",
      "epoch:15 step:11723 [D loss: 0.304843, acc.: 88.28%] [G loss: 2.843828]\n",
      "epoch:15 step:11724 [D loss: 0.216072, acc.: 91.41%] [G loss: 3.915227]\n",
      "epoch:15 step:11725 [D loss: 0.225677, acc.: 93.75%] [G loss: 4.484762]\n",
      "epoch:15 step:11726 [D loss: 0.260630, acc.: 89.84%] [G loss: 3.539690]\n",
      "epoch:15 step:11727 [D loss: 0.204321, acc.: 95.31%] [G loss: 5.254050]\n",
      "epoch:15 step:11728 [D loss: 0.335999, acc.: 87.50%] [G loss: 4.040808]\n",
      "epoch:15 step:11729 [D loss: 0.431762, acc.: 82.81%] [G loss: 2.678140]\n",
      "epoch:15 step:11730 [D loss: 0.324509, acc.: 86.72%] [G loss: 2.967939]\n",
      "epoch:15 step:11731 [D loss: 0.221261, acc.: 91.41%] [G loss: 3.767699]\n",
      "epoch:15 step:11732 [D loss: 0.260713, acc.: 86.72%] [G loss: 2.883175]\n",
      "epoch:15 step:11733 [D loss: 0.288403, acc.: 90.62%] [G loss: 3.429819]\n",
      "epoch:15 step:11734 [D loss: 0.317807, acc.: 86.72%] [G loss: 3.730510]\n",
      "epoch:15 step:11735 [D loss: 0.264711, acc.: 88.28%] [G loss: 4.168326]\n",
      "epoch:15 step:11736 [D loss: 0.265739, acc.: 88.28%] [G loss: 4.434635]\n",
      "epoch:15 step:11737 [D loss: 0.247305, acc.: 89.84%] [G loss: 2.757685]\n",
      "epoch:15 step:11738 [D loss: 0.358241, acc.: 80.47%] [G loss: 2.879676]\n",
      "epoch:15 step:11739 [D loss: 0.344458, acc.: 86.72%] [G loss: 2.374373]\n",
      "epoch:15 step:11740 [D loss: 0.361295, acc.: 81.25%] [G loss: 2.965899]\n",
      "epoch:15 step:11741 [D loss: 0.402266, acc.: 85.16%] [G loss: 2.605723]\n",
      "epoch:15 step:11742 [D loss: 0.352651, acc.: 84.38%] [G loss: 3.405631]\n",
      "epoch:15 step:11743 [D loss: 0.333463, acc.: 85.94%] [G loss: 2.877530]\n",
      "epoch:15 step:11744 [D loss: 0.362785, acc.: 85.16%] [G loss: 4.546726]\n",
      "epoch:15 step:11745 [D loss: 0.258563, acc.: 89.06%] [G loss: 3.316762]\n",
      "epoch:15 step:11746 [D loss: 0.286633, acc.: 87.50%] [G loss: 3.874011]\n",
      "epoch:15 step:11747 [D loss: 0.490738, acc.: 73.44%] [G loss: 4.470196]\n",
      "epoch:15 step:11748 [D loss: 0.486411, acc.: 75.00%] [G loss: 2.324842]\n",
      "epoch:15 step:11749 [D loss: 0.496452, acc.: 82.03%] [G loss: 3.275892]\n",
      "epoch:15 step:11750 [D loss: 0.379550, acc.: 79.69%] [G loss: 4.759812]\n",
      "epoch:15 step:11751 [D loss: 0.398275, acc.: 75.00%] [G loss: 3.401054]\n",
      "epoch:15 step:11752 [D loss: 0.223722, acc.: 89.84%] [G loss: 3.770327]\n",
      "epoch:15 step:11753 [D loss: 0.358515, acc.: 79.69%] [G loss: 2.644035]\n",
      "epoch:15 step:11754 [D loss: 0.401145, acc.: 83.59%] [G loss: 3.025646]\n",
      "epoch:15 step:11755 [D loss: 0.257133, acc.: 92.19%] [G loss: 3.191354]\n",
      "epoch:15 step:11756 [D loss: 0.346397, acc.: 85.16%] [G loss: 3.287868]\n",
      "epoch:15 step:11757 [D loss: 0.344622, acc.: 82.03%] [G loss: 2.987623]\n",
      "epoch:15 step:11758 [D loss: 0.268155, acc.: 86.72%] [G loss: 3.185821]\n",
      "epoch:15 step:11759 [D loss: 0.288825, acc.: 88.28%] [G loss: 3.250678]\n",
      "epoch:15 step:11760 [D loss: 0.194489, acc.: 91.41%] [G loss: 5.371829]\n",
      "epoch:15 step:11761 [D loss: 0.367854, acc.: 87.50%] [G loss: 3.918857]\n",
      "epoch:15 step:11762 [D loss: 0.335872, acc.: 85.94%] [G loss: 2.956727]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:15 step:11763 [D loss: 0.292893, acc.: 88.28%] [G loss: 4.814857]\n",
      "epoch:15 step:11764 [D loss: 0.273423, acc.: 85.94%] [G loss: 3.882957]\n",
      "epoch:15 step:11765 [D loss: 0.275682, acc.: 89.06%] [G loss: 3.626382]\n",
      "epoch:15 step:11766 [D loss: 0.254266, acc.: 88.28%] [G loss: 3.230894]\n",
      "epoch:15 step:11767 [D loss: 0.349752, acc.: 85.94%] [G loss: 2.316177]\n",
      "epoch:15 step:11768 [D loss: 0.252458, acc.: 86.72%] [G loss: 4.043782]\n",
      "epoch:15 step:11769 [D loss: 0.334582, acc.: 87.50%] [G loss: 2.583487]\n",
      "epoch:15 step:11770 [D loss: 0.256095, acc.: 87.50%] [G loss: 3.381934]\n",
      "epoch:15 step:11771 [D loss: 0.282586, acc.: 89.84%] [G loss: 3.274936]\n",
      "epoch:15 step:11772 [D loss: 0.304156, acc.: 85.94%] [G loss: 6.183780]\n",
      "epoch:15 step:11773 [D loss: 0.252015, acc.: 89.06%] [G loss: 3.818581]\n",
      "epoch:15 step:11774 [D loss: 0.301236, acc.: 88.28%] [G loss: 3.247250]\n",
      "epoch:15 step:11775 [D loss: 0.314570, acc.: 88.28%] [G loss: 3.064053]\n",
      "epoch:15 step:11776 [D loss: 0.289111, acc.: 86.72%] [G loss: 3.650266]\n",
      "epoch:15 step:11777 [D loss: 0.325754, acc.: 82.81%] [G loss: 4.121681]\n",
      "epoch:15 step:11778 [D loss: 0.209492, acc.: 92.97%] [G loss: 3.688316]\n",
      "epoch:15 step:11779 [D loss: 0.323503, acc.: 83.59%] [G loss: 3.036753]\n",
      "epoch:15 step:11780 [D loss: 0.411618, acc.: 77.34%] [G loss: 3.257354]\n",
      "epoch:15 step:11781 [D loss: 0.401290, acc.: 78.91%] [G loss: 5.359705]\n",
      "epoch:15 step:11782 [D loss: 0.300408, acc.: 86.72%] [G loss: 6.536281]\n",
      "epoch:15 step:11783 [D loss: 0.322921, acc.: 84.38%] [G loss: 4.290933]\n",
      "epoch:15 step:11784 [D loss: 0.317544, acc.: 85.94%] [G loss: 3.326012]\n",
      "epoch:15 step:11785 [D loss: 0.319562, acc.: 89.84%] [G loss: 3.088809]\n",
      "epoch:15 step:11786 [D loss: 0.295716, acc.: 86.72%] [G loss: 4.053458]\n",
      "epoch:15 step:11787 [D loss: 0.233735, acc.: 86.72%] [G loss: 3.758117]\n",
      "epoch:15 step:11788 [D loss: 0.216511, acc.: 91.41%] [G loss: 3.498368]\n",
      "epoch:15 step:11789 [D loss: 0.316653, acc.: 83.59%] [G loss: 3.459193]\n",
      "epoch:15 step:11790 [D loss: 0.320667, acc.: 85.94%] [G loss: 3.098915]\n",
      "epoch:15 step:11791 [D loss: 0.332875, acc.: 84.38%] [G loss: 4.095745]\n",
      "epoch:15 step:11792 [D loss: 0.307602, acc.: 86.72%] [G loss: 5.086756]\n",
      "epoch:15 step:11793 [D loss: 0.512827, acc.: 78.91%] [G loss: 2.891585]\n",
      "epoch:15 step:11794 [D loss: 0.401329, acc.: 79.69%] [G loss: 2.303915]\n",
      "epoch:15 step:11795 [D loss: 0.295507, acc.: 89.84%] [G loss: 2.679808]\n",
      "epoch:15 step:11796 [D loss: 0.325352, acc.: 89.84%] [G loss: 4.240012]\n",
      "epoch:15 step:11797 [D loss: 0.284229, acc.: 89.06%] [G loss: 4.282474]\n",
      "epoch:15 step:11798 [D loss: 0.309710, acc.: 85.94%] [G loss: 5.004197]\n",
      "epoch:15 step:11799 [D loss: 0.416909, acc.: 81.25%] [G loss: 3.565843]\n",
      "epoch:15 step:11800 [D loss: 0.398558, acc.: 79.69%] [G loss: 3.495886]\n",
      "##############\n",
      "[0.85409326 0.85958705 0.81646146 0.79661161 0.77514808 0.84681569\n",
      " 0.85860824 0.79117676 0.8240838  0.80784653]\n",
      "##########\n",
      "epoch:15 step:11801 [D loss: 0.431666, acc.: 76.56%] [G loss: 3.158203]\n",
      "epoch:15 step:11802 [D loss: 0.322186, acc.: 89.06%] [G loss: 3.741553]\n",
      "epoch:15 step:11803 [D loss: 0.359444, acc.: 82.81%] [G loss: 5.791021]\n",
      "epoch:15 step:11804 [D loss: 0.501425, acc.: 73.44%] [G loss: 3.374344]\n",
      "epoch:15 step:11805 [D loss: 0.438834, acc.: 78.91%] [G loss: 4.487531]\n",
      "epoch:15 step:11806 [D loss: 0.378802, acc.: 82.81%] [G loss: 4.264438]\n",
      "epoch:15 step:11807 [D loss: 0.324887, acc.: 84.38%] [G loss: 3.482134]\n",
      "epoch:15 step:11808 [D loss: 0.493741, acc.: 78.12%] [G loss: 5.783239]\n",
      "epoch:15 step:11809 [D loss: 0.390429, acc.: 82.81%] [G loss: 3.539460]\n",
      "epoch:15 step:11810 [D loss: 0.191833, acc.: 92.19%] [G loss: 4.532645]\n",
      "epoch:15 step:11811 [D loss: 0.308986, acc.: 85.94%] [G loss: 3.407608]\n",
      "epoch:15 step:11812 [D loss: 0.407696, acc.: 80.47%] [G loss: 3.867413]\n",
      "epoch:15 step:11813 [D loss: 0.310904, acc.: 85.94%] [G loss: 6.834899]\n",
      "epoch:15 step:11814 [D loss: 0.284706, acc.: 85.94%] [G loss: 6.138218]\n",
      "epoch:15 step:11815 [D loss: 0.305758, acc.: 86.72%] [G loss: 4.562645]\n",
      "epoch:15 step:11816 [D loss: 0.336713, acc.: 85.16%] [G loss: 4.478869]\n",
      "epoch:15 step:11817 [D loss: 0.325405, acc.: 87.50%] [G loss: 2.982538]\n",
      "epoch:15 step:11818 [D loss: 0.318904, acc.: 83.59%] [G loss: 3.796809]\n",
      "epoch:15 step:11819 [D loss: 0.259869, acc.: 89.84%] [G loss: 4.683847]\n",
      "epoch:15 step:11820 [D loss: 0.329112, acc.: 86.72%] [G loss: 2.998488]\n",
      "epoch:15 step:11821 [D loss: 0.293698, acc.: 88.28%] [G loss: 2.836949]\n",
      "epoch:15 step:11822 [D loss: 0.267303, acc.: 92.19%] [G loss: 3.117831]\n",
      "epoch:15 step:11823 [D loss: 0.261567, acc.: 89.06%] [G loss: 4.042599]\n",
      "epoch:15 step:11824 [D loss: 0.449334, acc.: 81.25%] [G loss: 3.200696]\n",
      "epoch:15 step:11825 [D loss: 0.232497, acc.: 89.84%] [G loss: 3.530482]\n",
      "epoch:15 step:11826 [D loss: 0.372178, acc.: 82.81%] [G loss: 3.160602]\n",
      "epoch:15 step:11827 [D loss: 0.277404, acc.: 91.41%] [G loss: 3.038442]\n",
      "epoch:15 step:11828 [D loss: 0.417272, acc.: 82.03%] [G loss: 2.549212]\n",
      "epoch:15 step:11829 [D loss: 0.374107, acc.: 86.72%] [G loss: 2.996172]\n",
      "epoch:15 step:11830 [D loss: 0.447082, acc.: 80.47%] [G loss: 3.685120]\n",
      "epoch:15 step:11831 [D loss: 0.433294, acc.: 82.03%] [G loss: 3.425373]\n",
      "epoch:15 step:11832 [D loss: 0.364211, acc.: 85.94%] [G loss: 3.605139]\n",
      "epoch:15 step:11833 [D loss: 0.400497, acc.: 82.03%] [G loss: 6.429210]\n",
      "epoch:15 step:11834 [D loss: 0.625087, acc.: 76.56%] [G loss: 4.561351]\n",
      "epoch:15 step:11835 [D loss: 0.393369, acc.: 82.81%] [G loss: 4.876300]\n",
      "epoch:15 step:11836 [D loss: 0.501360, acc.: 79.69%] [G loss: 2.629625]\n",
      "epoch:15 step:11837 [D loss: 0.322750, acc.: 87.50%] [G loss: 3.956730]\n",
      "epoch:15 step:11838 [D loss: 0.372286, acc.: 83.59%] [G loss: 4.137630]\n",
      "epoch:15 step:11839 [D loss: 0.410798, acc.: 82.81%] [G loss: 3.821045]\n",
      "epoch:15 step:11840 [D loss: 0.402082, acc.: 83.59%] [G loss: 3.818169]\n",
      "epoch:15 step:11841 [D loss: 0.302439, acc.: 85.94%] [G loss: 4.097489]\n",
      "epoch:15 step:11842 [D loss: 0.298679, acc.: 85.16%] [G loss: 5.999172]\n",
      "epoch:15 step:11843 [D loss: 0.236659, acc.: 91.41%] [G loss: 6.548630]\n",
      "epoch:15 step:11844 [D loss: 0.372758, acc.: 82.03%] [G loss: 3.700632]\n",
      "epoch:15 step:11845 [D loss: 0.297829, acc.: 85.16%] [G loss: 3.257982]\n",
      "epoch:15 step:11846 [D loss: 0.303498, acc.: 85.94%] [G loss: 2.982656]\n",
      "epoch:15 step:11847 [D loss: 0.329115, acc.: 85.94%] [G loss: 2.990732]\n",
      "epoch:15 step:11848 [D loss: 0.338338, acc.: 85.16%] [G loss: 3.292614]\n",
      "epoch:15 step:11849 [D loss: 0.183469, acc.: 92.19%] [G loss: 5.506389]\n",
      "epoch:15 step:11850 [D loss: 0.228035, acc.: 90.62%] [G loss: 5.086529]\n",
      "epoch:15 step:11851 [D loss: 0.263506, acc.: 91.41%] [G loss: 3.355835]\n",
      "epoch:15 step:11852 [D loss: 0.222368, acc.: 91.41%] [G loss: 3.970018]\n",
      "epoch:15 step:11853 [D loss: 0.240257, acc.: 89.06%] [G loss: 3.109336]\n",
      "epoch:15 step:11854 [D loss: 0.283983, acc.: 87.50%] [G loss: 2.522248]\n",
      "epoch:15 step:11855 [D loss: 0.315868, acc.: 87.50%] [G loss: 2.634465]\n",
      "epoch:15 step:11856 [D loss: 0.348364, acc.: 84.38%] [G loss: 3.150031]\n",
      "epoch:15 step:11857 [D loss: 0.374875, acc.: 85.94%] [G loss: 3.134376]\n",
      "epoch:15 step:11858 [D loss: 0.295882, acc.: 85.94%] [G loss: 3.082443]\n",
      "epoch:15 step:11859 [D loss: 0.387181, acc.: 82.03%] [G loss: 2.469973]\n",
      "epoch:15 step:11860 [D loss: 0.303649, acc.: 83.59%] [G loss: 3.147068]\n",
      "epoch:15 step:11861 [D loss: 0.338758, acc.: 83.59%] [G loss: 2.222613]\n",
      "epoch:15 step:11862 [D loss: 0.444380, acc.: 80.47%] [G loss: 3.153995]\n",
      "epoch:15 step:11863 [D loss: 0.331597, acc.: 85.16%] [G loss: 3.446163]\n",
      "epoch:15 step:11864 [D loss: 0.225750, acc.: 89.84%] [G loss: 5.079901]\n",
      "epoch:15 step:11865 [D loss: 0.342188, acc.: 85.94%] [G loss: 4.933809]\n",
      "epoch:15 step:11866 [D loss: 0.238061, acc.: 91.41%] [G loss: 4.452391]\n",
      "epoch:15 step:11867 [D loss: 0.243566, acc.: 88.28%] [G loss: 4.637465]\n",
      "epoch:15 step:11868 [D loss: 0.295585, acc.: 89.06%] [G loss: 2.877071]\n",
      "epoch:15 step:11869 [D loss: 0.289607, acc.: 87.50%] [G loss: 3.647861]\n",
      "epoch:15 step:11870 [D loss: 0.176655, acc.: 92.97%] [G loss: 5.506050]\n",
      "epoch:15 step:11871 [D loss: 0.349429, acc.: 85.16%] [G loss: 2.288146]\n",
      "epoch:15 step:11872 [D loss: 0.379021, acc.: 80.47%] [G loss: 3.083277]\n",
      "epoch:15 step:11873 [D loss: 0.319221, acc.: 85.94%] [G loss: 3.224967]\n",
      "epoch:15 step:11874 [D loss: 0.304354, acc.: 84.38%] [G loss: 5.097093]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:15 step:11875 [D loss: 0.267264, acc.: 92.97%] [G loss: 3.414314]\n",
      "epoch:15 step:11876 [D loss: 0.372180, acc.: 85.16%] [G loss: 2.229396]\n",
      "epoch:15 step:11877 [D loss: 0.241804, acc.: 92.19%] [G loss: 2.891451]\n",
      "epoch:15 step:11878 [D loss: 0.233891, acc.: 89.06%] [G loss: 3.916899]\n",
      "epoch:15 step:11879 [D loss: 0.379028, acc.: 85.94%] [G loss: 2.962373]\n",
      "epoch:15 step:11880 [D loss: 0.449312, acc.: 75.78%] [G loss: 2.204357]\n",
      "epoch:15 step:11881 [D loss: 0.382480, acc.: 86.72%] [G loss: 3.514663]\n",
      "epoch:15 step:11882 [D loss: 0.363633, acc.: 82.81%] [G loss: 2.851563]\n",
      "epoch:15 step:11883 [D loss: 0.343343, acc.: 87.50%] [G loss: 3.932762]\n",
      "epoch:15 step:11884 [D loss: 0.382939, acc.: 83.59%] [G loss: 4.255755]\n",
      "epoch:15 step:11885 [D loss: 0.291496, acc.: 89.06%] [G loss: 3.685163]\n",
      "epoch:15 step:11886 [D loss: 0.566658, acc.: 78.12%] [G loss: 2.135524]\n",
      "epoch:15 step:11887 [D loss: 0.301598, acc.: 88.28%] [G loss: 3.076235]\n",
      "epoch:15 step:11888 [D loss: 0.380331, acc.: 84.38%] [G loss: 3.587459]\n",
      "epoch:15 step:11889 [D loss: 0.351054, acc.: 79.69%] [G loss: 3.393648]\n",
      "epoch:15 step:11890 [D loss: 0.335051, acc.: 82.03%] [G loss: 3.335577]\n",
      "epoch:15 step:11891 [D loss: 0.464855, acc.: 75.78%] [G loss: 4.326363]\n",
      "epoch:15 step:11892 [D loss: 0.395156, acc.: 84.38%] [G loss: 2.900611]\n",
      "epoch:15 step:11893 [D loss: 0.355134, acc.: 82.81%] [G loss: 4.096737]\n",
      "epoch:15 step:11894 [D loss: 0.384723, acc.: 81.25%] [G loss: 3.115920]\n",
      "epoch:15 step:11895 [D loss: 0.392485, acc.: 85.16%] [G loss: 3.326363]\n",
      "epoch:15 step:11896 [D loss: 0.285928, acc.: 89.84%] [G loss: 2.908579]\n",
      "epoch:15 step:11897 [D loss: 0.347319, acc.: 83.59%] [G loss: 4.220548]\n",
      "epoch:15 step:11898 [D loss: 0.302650, acc.: 84.38%] [G loss: 3.785034]\n",
      "epoch:15 step:11899 [D loss: 0.286083, acc.: 89.84%] [G loss: 5.737829]\n",
      "epoch:15 step:11900 [D loss: 0.463146, acc.: 75.78%] [G loss: 4.049810]\n",
      "epoch:15 step:11901 [D loss: 0.201324, acc.: 92.19%] [G loss: 6.245506]\n",
      "epoch:15 step:11902 [D loss: 0.305788, acc.: 88.28%] [G loss: 4.053549]\n",
      "epoch:15 step:11903 [D loss: 0.325012, acc.: 86.72%] [G loss: 4.181049]\n",
      "epoch:15 step:11904 [D loss: 0.319267, acc.: 86.72%] [G loss: 3.756135]\n",
      "epoch:15 step:11905 [D loss: 0.266408, acc.: 92.19%] [G loss: 4.173657]\n",
      "epoch:15 step:11906 [D loss: 0.287835, acc.: 87.50%] [G loss: 3.469675]\n",
      "epoch:15 step:11907 [D loss: 0.221551, acc.: 91.41%] [G loss: 3.689667]\n",
      "epoch:15 step:11908 [D loss: 0.369199, acc.: 82.81%] [G loss: 3.351521]\n",
      "epoch:15 step:11909 [D loss: 0.363009, acc.: 79.69%] [G loss: 3.000560]\n",
      "epoch:15 step:11910 [D loss: 0.481993, acc.: 81.25%] [G loss: 2.642861]\n",
      "epoch:15 step:11911 [D loss: 0.321949, acc.: 86.72%] [G loss: 2.735135]\n",
      "epoch:15 step:11912 [D loss: 0.393064, acc.: 78.91%] [G loss: 3.781675]\n",
      "epoch:15 step:11913 [D loss: 0.287304, acc.: 88.28%] [G loss: 4.569529]\n",
      "epoch:15 step:11914 [D loss: 0.306393, acc.: 89.84%] [G loss: 5.099661]\n",
      "epoch:15 step:11915 [D loss: 0.238124, acc.: 90.62%] [G loss: 3.929419]\n",
      "epoch:15 step:11916 [D loss: 0.310237, acc.: 89.84%] [G loss: 5.394722]\n",
      "epoch:15 step:11917 [D loss: 0.272973, acc.: 89.84%] [G loss: 5.545794]\n",
      "epoch:15 step:11918 [D loss: 0.317492, acc.: 89.06%] [G loss: 4.971971]\n",
      "epoch:15 step:11919 [D loss: 0.272038, acc.: 89.06%] [G loss: 3.531765]\n",
      "epoch:15 step:11920 [D loss: 0.395576, acc.: 82.81%] [G loss: 3.093163]\n",
      "epoch:15 step:11921 [D loss: 0.374814, acc.: 83.59%] [G loss: 3.906396]\n",
      "epoch:15 step:11922 [D loss: 0.470630, acc.: 76.56%] [G loss: 3.868987]\n",
      "epoch:15 step:11923 [D loss: 0.360776, acc.: 80.47%] [G loss: 4.740827]\n",
      "epoch:15 step:11924 [D loss: 0.264658, acc.: 89.84%] [G loss: 4.472135]\n",
      "epoch:15 step:11925 [D loss: 0.305403, acc.: 87.50%] [G loss: 3.088949]\n",
      "epoch:15 step:11926 [D loss: 0.283646, acc.: 86.72%] [G loss: 3.357462]\n",
      "epoch:15 step:11927 [D loss: 0.194898, acc.: 91.41%] [G loss: 4.281993]\n",
      "epoch:15 step:11928 [D loss: 0.208372, acc.: 89.06%] [G loss: 2.863755]\n",
      "epoch:15 step:11929 [D loss: 0.331361, acc.: 83.59%] [G loss: 2.575001]\n",
      "epoch:15 step:11930 [D loss: 0.207722, acc.: 94.53%] [G loss: 3.346284]\n",
      "epoch:15 step:11931 [D loss: 0.298236, acc.: 88.28%] [G loss: 2.730079]\n",
      "epoch:15 step:11932 [D loss: 0.405274, acc.: 83.59%] [G loss: 3.189691]\n",
      "epoch:15 step:11933 [D loss: 0.247837, acc.: 89.06%] [G loss: 3.262496]\n",
      "epoch:15 step:11934 [D loss: 0.244051, acc.: 89.84%] [G loss: 3.473432]\n",
      "epoch:15 step:11935 [D loss: 0.324904, acc.: 87.50%] [G loss: 4.950839]\n",
      "epoch:15 step:11936 [D loss: 0.310334, acc.: 82.81%] [G loss: 2.663915]\n",
      "epoch:15 step:11937 [D loss: 0.267261, acc.: 88.28%] [G loss: 2.711617]\n",
      "epoch:15 step:11938 [D loss: 0.272165, acc.: 87.50%] [G loss: 3.086118]\n",
      "epoch:15 step:11939 [D loss: 0.347253, acc.: 85.16%] [G loss: 2.354294]\n",
      "epoch:15 step:11940 [D loss: 0.389896, acc.: 83.59%] [G loss: 2.939440]\n",
      "epoch:15 step:11941 [D loss: 0.356294, acc.: 85.94%] [G loss: 3.585281]\n",
      "epoch:15 step:11942 [D loss: 0.311548, acc.: 90.62%] [G loss: 3.139110]\n",
      "epoch:15 step:11943 [D loss: 0.276741, acc.: 90.62%] [G loss: 2.912180]\n",
      "epoch:15 step:11944 [D loss: 0.441243, acc.: 79.69%] [G loss: 3.140160]\n",
      "epoch:15 step:11945 [D loss: 0.451939, acc.: 80.47%] [G loss: 6.802589]\n",
      "epoch:15 step:11946 [D loss: 0.540915, acc.: 78.91%] [G loss: 4.129059]\n",
      "epoch:15 step:11947 [D loss: 0.471222, acc.: 77.34%] [G loss: 3.686530]\n",
      "epoch:15 step:11948 [D loss: 0.316193, acc.: 82.81%] [G loss: 2.777767]\n",
      "epoch:15 step:11949 [D loss: 0.274415, acc.: 88.28%] [G loss: 3.239622]\n",
      "epoch:15 step:11950 [D loss: 0.225459, acc.: 93.75%] [G loss: 4.847423]\n",
      "epoch:15 step:11951 [D loss: 0.254734, acc.: 89.06%] [G loss: 5.745015]\n",
      "epoch:15 step:11952 [D loss: 0.337596, acc.: 85.94%] [G loss: 3.965851]\n",
      "epoch:15 step:11953 [D loss: 0.449228, acc.: 75.00%] [G loss: 2.497649]\n",
      "epoch:15 step:11954 [D loss: 0.325074, acc.: 88.28%] [G loss: 3.058830]\n",
      "epoch:15 step:11955 [D loss: 0.352315, acc.: 82.03%] [G loss: 2.858183]\n",
      "epoch:15 step:11956 [D loss: 0.367310, acc.: 84.38%] [G loss: 2.708693]\n",
      "epoch:15 step:11957 [D loss: 0.332356, acc.: 85.16%] [G loss: 3.151940]\n",
      "epoch:15 step:11958 [D loss: 0.287343, acc.: 86.72%] [G loss: 2.968587]\n",
      "epoch:15 step:11959 [D loss: 0.322424, acc.: 85.94%] [G loss: 4.053057]\n",
      "epoch:15 step:11960 [D loss: 0.253682, acc.: 88.28%] [G loss: 3.425498]\n",
      "epoch:15 step:11961 [D loss: 0.257788, acc.: 86.72%] [G loss: 4.024644]\n",
      "epoch:15 step:11962 [D loss: 0.257982, acc.: 90.62%] [G loss: 3.166167]\n",
      "epoch:15 step:11963 [D loss: 0.306264, acc.: 84.38%] [G loss: 4.009435]\n",
      "epoch:15 step:11964 [D loss: 0.303096, acc.: 88.28%] [G loss: 6.210703]\n",
      "epoch:15 step:11965 [D loss: 0.298124, acc.: 83.59%] [G loss: 4.242449]\n",
      "epoch:15 step:11966 [D loss: 0.265352, acc.: 89.06%] [G loss: 3.954271]\n",
      "epoch:15 step:11967 [D loss: 0.371712, acc.: 84.38%] [G loss: 3.650747]\n",
      "epoch:15 step:11968 [D loss: 0.337142, acc.: 85.94%] [G loss: 3.827507]\n",
      "epoch:15 step:11969 [D loss: 0.292672, acc.: 87.50%] [G loss: 3.047557]\n",
      "epoch:15 step:11970 [D loss: 0.269527, acc.: 89.06%] [G loss: 4.287327]\n",
      "epoch:15 step:11971 [D loss: 0.300246, acc.: 85.16%] [G loss: 2.622165]\n",
      "epoch:15 step:11972 [D loss: 0.315802, acc.: 88.28%] [G loss: 3.235218]\n",
      "epoch:15 step:11973 [D loss: 0.365555, acc.: 80.47%] [G loss: 2.992623]\n",
      "epoch:15 step:11974 [D loss: 0.287786, acc.: 88.28%] [G loss: 4.018624]\n",
      "epoch:15 step:11975 [D loss: 0.269010, acc.: 85.94%] [G loss: 5.222404]\n",
      "epoch:15 step:11976 [D loss: 0.278002, acc.: 87.50%] [G loss: 4.195809]\n",
      "epoch:15 step:11977 [D loss: 0.291721, acc.: 88.28%] [G loss: 5.443530]\n",
      "epoch:15 step:11978 [D loss: 0.295839, acc.: 87.50%] [G loss: 4.458844]\n",
      "epoch:15 step:11979 [D loss: 0.258040, acc.: 89.84%] [G loss: 4.038746]\n",
      "epoch:15 step:11980 [D loss: 0.268197, acc.: 86.72%] [G loss: 4.556453]\n",
      "epoch:15 step:11981 [D loss: 0.214455, acc.: 91.41%] [G loss: 7.472166]\n",
      "epoch:15 step:11982 [D loss: 0.443993, acc.: 75.78%] [G loss: 4.632152]\n",
      "epoch:15 step:11983 [D loss: 0.517726, acc.: 79.69%] [G loss: 2.858837]\n",
      "epoch:15 step:11984 [D loss: 0.268722, acc.: 89.84%] [G loss: 7.279180]\n",
      "epoch:15 step:11985 [D loss: 0.620935, acc.: 76.56%] [G loss: 6.109903]\n",
      "epoch:15 step:11986 [D loss: 1.317348, acc.: 62.50%] [G loss: 6.082249]\n",
      "epoch:15 step:11987 [D loss: 0.814020, acc.: 67.97%] [G loss: 6.678964]\n",
      "epoch:15 step:11988 [D loss: 0.747228, acc.: 78.12%] [G loss: 4.067270]\n",
      "epoch:15 step:11989 [D loss: 0.380760, acc.: 82.81%] [G loss: 4.702872]\n",
      "epoch:15 step:11990 [D loss: 0.484251, acc.: 76.56%] [G loss: 3.176293]\n",
      "epoch:15 step:11991 [D loss: 0.350407, acc.: 83.59%] [G loss: 3.740922]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:15 step:11992 [D loss: 0.312117, acc.: 86.72%] [G loss: 5.366632]\n",
      "epoch:15 step:11993 [D loss: 0.429792, acc.: 78.12%] [G loss: 2.732726]\n",
      "epoch:15 step:11994 [D loss: 0.333987, acc.: 86.72%] [G loss: 3.657285]\n",
      "epoch:15 step:11995 [D loss: 0.298992, acc.: 88.28%] [G loss: 3.789573]\n",
      "epoch:15 step:11996 [D loss: 0.285229, acc.: 89.84%] [G loss: 2.606588]\n",
      "epoch:15 step:11997 [D loss: 0.301832, acc.: 89.84%] [G loss: 3.040375]\n",
      "epoch:15 step:11998 [D loss: 0.395984, acc.: 83.59%] [G loss: 2.978491]\n",
      "epoch:15 step:11999 [D loss: 0.341491, acc.: 84.38%] [G loss: 3.205824]\n",
      "epoch:15 step:12000 [D loss: 0.391984, acc.: 85.94%] [G loss: 3.519869]\n",
      "##############\n",
      "[0.83753508 0.84625721 0.80743924 0.77369066 0.78808055 0.80505125\n",
      " 0.85900395 0.87140553 0.82375774 0.83285488]\n",
      "##########\n",
      "epoch:15 step:12001 [D loss: 0.261798, acc.: 89.84%] [G loss: 3.377093]\n",
      "epoch:15 step:12002 [D loss: 0.272682, acc.: 91.41%] [G loss: 3.385182]\n",
      "epoch:15 step:12003 [D loss: 0.375110, acc.: 80.47%] [G loss: 2.971856]\n",
      "epoch:15 step:12004 [D loss: 0.282356, acc.: 85.94%] [G loss: 3.270322]\n",
      "epoch:15 step:12005 [D loss: 0.236554, acc.: 89.84%] [G loss: 2.895319]\n",
      "epoch:15 step:12006 [D loss: 0.337517, acc.: 83.59%] [G loss: 2.669813]\n",
      "epoch:15 step:12007 [D loss: 0.349675, acc.: 85.16%] [G loss: 2.798645]\n",
      "epoch:15 step:12008 [D loss: 0.413871, acc.: 81.25%] [G loss: 2.759115]\n",
      "epoch:15 step:12009 [D loss: 0.385930, acc.: 83.59%] [G loss: 4.325294]\n",
      "epoch:15 step:12010 [D loss: 0.412065, acc.: 82.03%] [G loss: 4.373139]\n",
      "epoch:15 step:12011 [D loss: 0.366693, acc.: 85.16%] [G loss: 2.950594]\n",
      "epoch:15 step:12012 [D loss: 0.485543, acc.: 79.69%] [G loss: 3.176600]\n",
      "epoch:15 step:12013 [D loss: 0.398800, acc.: 84.38%] [G loss: 2.751654]\n",
      "epoch:15 step:12014 [D loss: 0.453970, acc.: 80.47%] [G loss: 2.785434]\n",
      "epoch:15 step:12015 [D loss: 0.261894, acc.: 92.97%] [G loss: 3.712767]\n",
      "epoch:15 step:12016 [D loss: 0.248490, acc.: 89.06%] [G loss: 4.273411]\n",
      "epoch:15 step:12017 [D loss: 0.268683, acc.: 89.84%] [G loss: 3.098495]\n",
      "epoch:15 step:12018 [D loss: 0.336342, acc.: 84.38%] [G loss: 2.676790]\n",
      "epoch:15 step:12019 [D loss: 0.284300, acc.: 89.06%] [G loss: 3.117677]\n",
      "epoch:15 step:12020 [D loss: 0.436982, acc.: 79.69%] [G loss: 2.867051]\n",
      "epoch:15 step:12021 [D loss: 0.376143, acc.: 84.38%] [G loss: 3.016487]\n",
      "epoch:15 step:12022 [D loss: 0.433174, acc.: 78.91%] [G loss: 3.311814]\n",
      "epoch:15 step:12023 [D loss: 0.289640, acc.: 88.28%] [G loss: 2.607308]\n",
      "epoch:15 step:12024 [D loss: 0.252273, acc.: 89.84%] [G loss: 3.284341]\n",
      "epoch:15 step:12025 [D loss: 0.245597, acc.: 90.62%] [G loss: 2.334199]\n",
      "epoch:15 step:12026 [D loss: 0.302918, acc.: 87.50%] [G loss: 4.186747]\n",
      "epoch:15 step:12027 [D loss: 0.412075, acc.: 83.59%] [G loss: 3.713974]\n",
      "epoch:15 step:12028 [D loss: 0.393855, acc.: 81.25%] [G loss: 4.966291]\n",
      "epoch:15 step:12029 [D loss: 0.406885, acc.: 78.91%] [G loss: 2.986125]\n",
      "epoch:15 step:12030 [D loss: 0.463214, acc.: 78.12%] [G loss: 3.441298]\n",
      "epoch:15 step:12031 [D loss: 0.477550, acc.: 82.81%] [G loss: 3.178578]\n",
      "epoch:15 step:12032 [D loss: 0.407778, acc.: 84.38%] [G loss: 2.870990]\n",
      "epoch:15 step:12033 [D loss: 0.352897, acc.: 83.59%] [G loss: 3.195649]\n",
      "epoch:15 step:12034 [D loss: 0.330673, acc.: 85.16%] [G loss: 2.873637]\n",
      "epoch:15 step:12035 [D loss: 0.361688, acc.: 86.72%] [G loss: 2.979912]\n",
      "epoch:15 step:12036 [D loss: 0.309314, acc.: 90.62%] [G loss: 2.630747]\n",
      "epoch:15 step:12037 [D loss: 0.279167, acc.: 88.28%] [G loss: 3.472590]\n",
      "epoch:15 step:12038 [D loss: 0.248690, acc.: 91.41%] [G loss: 4.087580]\n",
      "epoch:15 step:12039 [D loss: 0.292203, acc.: 85.94%] [G loss: 3.759191]\n",
      "epoch:15 step:12040 [D loss: 0.323192, acc.: 83.59%] [G loss: 3.141939]\n",
      "epoch:15 step:12041 [D loss: 0.397320, acc.: 82.81%] [G loss: 3.609862]\n",
      "epoch:15 step:12042 [D loss: 0.429912, acc.: 82.03%] [G loss: 2.631069]\n",
      "epoch:15 step:12043 [D loss: 0.332660, acc.: 86.72%] [G loss: 3.177728]\n",
      "epoch:15 step:12044 [D loss: 0.334904, acc.: 82.81%] [G loss: 3.308867]\n",
      "epoch:15 step:12045 [D loss: 0.331346, acc.: 83.59%] [G loss: 3.746365]\n",
      "epoch:15 step:12046 [D loss: 0.323345, acc.: 83.59%] [G loss: 2.147943]\n",
      "epoch:15 step:12047 [D loss: 0.314974, acc.: 87.50%] [G loss: 4.149778]\n",
      "epoch:15 step:12048 [D loss: 0.309856, acc.: 88.28%] [G loss: 3.646382]\n",
      "epoch:15 step:12049 [D loss: 0.393457, acc.: 82.81%] [G loss: 4.797143]\n",
      "epoch:15 step:12050 [D loss: 0.278196, acc.: 89.84%] [G loss: 2.940843]\n",
      "epoch:15 step:12051 [D loss: 0.340504, acc.: 85.94%] [G loss: 2.736423]\n",
      "epoch:15 step:12052 [D loss: 0.376503, acc.: 82.81%] [G loss: 2.386201]\n",
      "epoch:15 step:12053 [D loss: 0.310441, acc.: 85.94%] [G loss: 4.235043]\n",
      "epoch:15 step:12054 [D loss: 0.350718, acc.: 89.06%] [G loss: 3.335201]\n",
      "epoch:15 step:12055 [D loss: 0.309499, acc.: 88.28%] [G loss: 3.089960]\n",
      "epoch:15 step:12056 [D loss: 0.306740, acc.: 88.28%] [G loss: 3.977605]\n",
      "epoch:15 step:12057 [D loss: 0.367211, acc.: 83.59%] [G loss: 3.255115]\n",
      "epoch:15 step:12058 [D loss: 0.346231, acc.: 86.72%] [G loss: 2.625103]\n",
      "epoch:15 step:12059 [D loss: 0.361696, acc.: 83.59%] [G loss: 2.995708]\n",
      "epoch:15 step:12060 [D loss: 0.374430, acc.: 84.38%] [G loss: 3.186845]\n",
      "epoch:15 step:12061 [D loss: 0.386877, acc.: 82.03%] [G loss: 3.079145]\n",
      "epoch:15 step:12062 [D loss: 0.335111, acc.: 85.94%] [G loss: 2.520540]\n",
      "epoch:15 step:12063 [D loss: 0.335695, acc.: 86.72%] [G loss: 2.563592]\n",
      "epoch:15 step:12064 [D loss: 0.432868, acc.: 81.25%] [G loss: 3.265268]\n",
      "epoch:15 step:12065 [D loss: 0.383549, acc.: 83.59%] [G loss: 2.424343]\n",
      "epoch:15 step:12066 [D loss: 0.359252, acc.: 85.16%] [G loss: 3.192734]\n",
      "epoch:15 step:12067 [D loss: 0.348477, acc.: 85.16%] [G loss: 4.182167]\n",
      "epoch:15 step:12068 [D loss: 0.283349, acc.: 86.72%] [G loss: 5.857864]\n",
      "epoch:15 step:12069 [D loss: 0.281395, acc.: 84.38%] [G loss: 4.592741]\n",
      "epoch:15 step:12070 [D loss: 0.326873, acc.: 84.38%] [G loss: 2.897875]\n",
      "epoch:15 step:12071 [D loss: 0.382891, acc.: 79.69%] [G loss: 3.588969]\n",
      "epoch:15 step:12072 [D loss: 0.369221, acc.: 78.91%] [G loss: 3.315563]\n",
      "epoch:15 step:12073 [D loss: 0.228061, acc.: 89.84%] [G loss: 3.824706]\n",
      "epoch:15 step:12074 [D loss: 0.329976, acc.: 83.59%] [G loss: 4.338441]\n",
      "epoch:15 step:12075 [D loss: 0.453328, acc.: 79.69%] [G loss: 3.216210]\n",
      "epoch:15 step:12076 [D loss: 0.324735, acc.: 87.50%] [G loss: 2.596122]\n",
      "epoch:15 step:12077 [D loss: 0.249593, acc.: 89.84%] [G loss: 4.062432]\n",
      "epoch:15 step:12078 [D loss: 0.336283, acc.: 82.81%] [G loss: 3.525991]\n",
      "epoch:15 step:12079 [D loss: 0.355853, acc.: 86.72%] [G loss: 3.960732]\n",
      "epoch:15 step:12080 [D loss: 0.368677, acc.: 82.03%] [G loss: 3.320076]\n",
      "epoch:15 step:12081 [D loss: 0.302619, acc.: 87.50%] [G loss: 2.565129]\n",
      "epoch:15 step:12082 [D loss: 0.302115, acc.: 86.72%] [G loss: 2.101872]\n",
      "epoch:15 step:12083 [D loss: 0.329197, acc.: 88.28%] [G loss: 2.454518]\n",
      "epoch:15 step:12084 [D loss: 0.314457, acc.: 85.94%] [G loss: 3.099576]\n",
      "epoch:15 step:12085 [D loss: 0.456800, acc.: 78.12%] [G loss: 4.013008]\n",
      "epoch:15 step:12086 [D loss: 0.270551, acc.: 90.62%] [G loss: 2.725955]\n",
      "epoch:15 step:12087 [D loss: 0.389820, acc.: 82.81%] [G loss: 2.539704]\n",
      "epoch:15 step:12088 [D loss: 0.235575, acc.: 90.62%] [G loss: 3.302536]\n",
      "epoch:15 step:12089 [D loss: 0.490953, acc.: 78.91%] [G loss: 3.599933]\n",
      "epoch:15 step:12090 [D loss: 0.345695, acc.: 83.59%] [G loss: 3.759431]\n",
      "epoch:15 step:12091 [D loss: 0.331202, acc.: 85.16%] [G loss: 2.580222]\n",
      "epoch:15 step:12092 [D loss: 0.321466, acc.: 84.38%] [G loss: 4.815078]\n",
      "epoch:15 step:12093 [D loss: 0.350549, acc.: 82.81%] [G loss: 4.565049]\n",
      "epoch:15 step:12094 [D loss: 0.636476, acc.: 77.34%] [G loss: 4.469527]\n",
      "epoch:15 step:12095 [D loss: 0.628121, acc.: 72.66%] [G loss: 5.367878]\n",
      "epoch:15 step:12096 [D loss: 0.715979, acc.: 70.31%] [G loss: 2.313797]\n",
      "epoch:15 step:12097 [D loss: 0.290433, acc.: 87.50%] [G loss: 7.205065]\n",
      "epoch:15 step:12098 [D loss: 0.313391, acc.: 85.94%] [G loss: 4.862723]\n",
      "epoch:15 step:12099 [D loss: 0.177307, acc.: 93.75%] [G loss: 6.853643]\n",
      "epoch:15 step:12100 [D loss: 0.283453, acc.: 87.50%] [G loss: 2.944562]\n",
      "epoch:15 step:12101 [D loss: 0.272848, acc.: 89.06%] [G loss: 6.784717]\n",
      "epoch:15 step:12102 [D loss: 0.380589, acc.: 78.91%] [G loss: 2.857404]\n",
      "epoch:15 step:12103 [D loss: 0.254746, acc.: 86.72%] [G loss: 4.245857]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:15 step:12104 [D loss: 0.266886, acc.: 89.06%] [G loss: 4.009657]\n",
      "epoch:15 step:12105 [D loss: 0.265590, acc.: 89.84%] [G loss: 2.588341]\n",
      "epoch:15 step:12106 [D loss: 0.313796, acc.: 84.38%] [G loss: 4.480885]\n",
      "epoch:15 step:12107 [D loss: 0.321887, acc.: 89.06%] [G loss: 3.450553]\n",
      "epoch:15 step:12108 [D loss: 0.356175, acc.: 82.81%] [G loss: 4.484595]\n",
      "epoch:15 step:12109 [D loss: 0.379892, acc.: 85.16%] [G loss: 3.289823]\n",
      "epoch:15 step:12110 [D loss: 0.362160, acc.: 83.59%] [G loss: 4.984744]\n",
      "epoch:15 step:12111 [D loss: 0.377259, acc.: 79.69%] [G loss: 3.312604]\n",
      "epoch:15 step:12112 [D loss: 0.320669, acc.: 84.38%] [G loss: 3.378271]\n",
      "epoch:15 step:12113 [D loss: 0.279298, acc.: 89.06%] [G loss: 4.025722]\n",
      "epoch:15 step:12114 [D loss: 0.287331, acc.: 89.06%] [G loss: 3.192722]\n",
      "epoch:15 step:12115 [D loss: 0.333824, acc.: 84.38%] [G loss: 4.210073]\n",
      "epoch:15 step:12116 [D loss: 0.247089, acc.: 89.06%] [G loss: 2.932369]\n",
      "epoch:15 step:12117 [D loss: 0.275218, acc.: 89.06%] [G loss: 3.377183]\n",
      "epoch:15 step:12118 [D loss: 0.368138, acc.: 86.72%] [G loss: 3.697474]\n",
      "epoch:15 step:12119 [D loss: 0.328621, acc.: 83.59%] [G loss: 2.928125]\n",
      "epoch:15 step:12120 [D loss: 0.279925, acc.: 87.50%] [G loss: 2.370914]\n",
      "epoch:15 step:12121 [D loss: 0.337042, acc.: 87.50%] [G loss: 2.242130]\n",
      "epoch:15 step:12122 [D loss: 0.307969, acc.: 86.72%] [G loss: 3.540247]\n",
      "epoch:15 step:12123 [D loss: 0.396925, acc.: 79.69%] [G loss: 5.764682]\n",
      "epoch:15 step:12124 [D loss: 0.577520, acc.: 71.09%] [G loss: 5.042671]\n",
      "epoch:15 step:12125 [D loss: 0.613989, acc.: 79.69%] [G loss: 5.146663]\n",
      "epoch:15 step:12126 [D loss: 0.914850, acc.: 71.09%] [G loss: 5.770619]\n",
      "epoch:15 step:12127 [D loss: 0.656806, acc.: 74.22%] [G loss: 4.534889]\n",
      "epoch:15 step:12128 [D loss: 0.335282, acc.: 82.03%] [G loss: 2.693470]\n",
      "epoch:15 step:12129 [D loss: 0.507664, acc.: 74.22%] [G loss: 3.029870]\n",
      "epoch:15 step:12130 [D loss: 0.275146, acc.: 90.62%] [G loss: 3.189091]\n",
      "epoch:15 step:12131 [D loss: 0.333511, acc.: 86.72%] [G loss: 3.311714]\n",
      "epoch:15 step:12132 [D loss: 0.304932, acc.: 89.06%] [G loss: 2.647416]\n",
      "epoch:15 step:12133 [D loss: 0.378363, acc.: 81.25%] [G loss: 3.683539]\n",
      "epoch:15 step:12134 [D loss: 0.279231, acc.: 89.06%] [G loss: 3.600212]\n",
      "epoch:15 step:12135 [D loss: 0.205452, acc.: 94.53%] [G loss: 3.865544]\n",
      "epoch:15 step:12136 [D loss: 0.419113, acc.: 81.25%] [G loss: 3.805244]\n",
      "epoch:15 step:12137 [D loss: 0.415202, acc.: 84.38%] [G loss: 3.651529]\n",
      "epoch:15 step:12138 [D loss: 0.359725, acc.: 80.47%] [G loss: 4.050745]\n",
      "epoch:15 step:12139 [D loss: 0.324865, acc.: 89.84%] [G loss: 2.810871]\n",
      "epoch:15 step:12140 [D loss: 0.305394, acc.: 88.28%] [G loss: 3.445303]\n",
      "epoch:15 step:12141 [D loss: 0.279231, acc.: 90.62%] [G loss: 2.504459]\n",
      "epoch:15 step:12142 [D loss: 0.321791, acc.: 84.38%] [G loss: 2.566145]\n",
      "epoch:15 step:12143 [D loss: 0.311321, acc.: 83.59%] [G loss: 2.946460]\n",
      "epoch:15 step:12144 [D loss: 0.304142, acc.: 85.16%] [G loss: 2.767773]\n",
      "epoch:15 step:12145 [D loss: 0.350413, acc.: 85.16%] [G loss: 3.264129]\n",
      "epoch:15 step:12146 [D loss: 0.336235, acc.: 85.16%] [G loss: 3.281718]\n",
      "epoch:15 step:12147 [D loss: 0.280332, acc.: 89.06%] [G loss: 2.694819]\n",
      "epoch:15 step:12148 [D loss: 0.278995, acc.: 89.06%] [G loss: 3.153736]\n",
      "epoch:15 step:12149 [D loss: 0.350689, acc.: 84.38%] [G loss: 5.266114]\n",
      "epoch:15 step:12150 [D loss: 0.345650, acc.: 87.50%] [G loss: 2.570218]\n",
      "epoch:15 step:12151 [D loss: 0.364442, acc.: 82.81%] [G loss: 3.224901]\n",
      "epoch:15 step:12152 [D loss: 0.316186, acc.: 88.28%] [G loss: 3.194912]\n",
      "epoch:15 step:12153 [D loss: 0.416013, acc.: 83.59%] [G loss: 4.990176]\n",
      "epoch:15 step:12154 [D loss: 0.389936, acc.: 84.38%] [G loss: 3.922647]\n",
      "epoch:15 step:12155 [D loss: 0.226664, acc.: 89.06%] [G loss: 3.242596]\n",
      "epoch:15 step:12156 [D loss: 0.294682, acc.: 86.72%] [G loss: 5.135693]\n",
      "epoch:15 step:12157 [D loss: 0.248070, acc.: 89.06%] [G loss: 3.998739]\n",
      "epoch:15 step:12158 [D loss: 0.286779, acc.: 85.94%] [G loss: 3.563113]\n",
      "epoch:15 step:12159 [D loss: 0.246131, acc.: 91.41%] [G loss: 3.744558]\n",
      "epoch:15 step:12160 [D loss: 0.344542, acc.: 85.94%] [G loss: 2.534026]\n",
      "epoch:15 step:12161 [D loss: 0.266001, acc.: 85.16%] [G loss: 4.164244]\n",
      "epoch:15 step:12162 [D loss: 0.215229, acc.: 92.19%] [G loss: 5.333955]\n",
      "epoch:15 step:12163 [D loss: 0.300958, acc.: 85.94%] [G loss: 4.463776]\n",
      "epoch:15 step:12164 [D loss: 0.299824, acc.: 89.06%] [G loss: 4.133930]\n",
      "epoch:15 step:12165 [D loss: 0.395749, acc.: 82.03%] [G loss: 3.140742]\n",
      "epoch:15 step:12166 [D loss: 0.463041, acc.: 82.81%] [G loss: 2.816339]\n",
      "epoch:15 step:12167 [D loss: 0.391058, acc.: 82.03%] [G loss: 2.565273]\n",
      "epoch:15 step:12168 [D loss: 0.266455, acc.: 90.62%] [G loss: 2.718847]\n",
      "epoch:15 step:12169 [D loss: 0.302164, acc.: 88.28%] [G loss: 2.823627]\n",
      "epoch:15 step:12170 [D loss: 0.301196, acc.: 87.50%] [G loss: 3.018963]\n",
      "epoch:15 step:12171 [D loss: 0.303715, acc.: 86.72%] [G loss: 4.739486]\n",
      "epoch:15 step:12172 [D loss: 0.304800, acc.: 86.72%] [G loss: 2.414565]\n",
      "epoch:15 step:12173 [D loss: 0.256501, acc.: 91.41%] [G loss: 3.094444]\n",
      "epoch:15 step:12174 [D loss: 0.185270, acc.: 96.09%] [G loss: 5.925271]\n",
      "epoch:15 step:12175 [D loss: 0.259887, acc.: 86.72%] [G loss: 4.973904]\n",
      "epoch:15 step:12176 [D loss: 0.233383, acc.: 89.84%] [G loss: 3.888758]\n",
      "epoch:15 step:12177 [D loss: 0.223463, acc.: 93.75%] [G loss: 2.942654]\n",
      "epoch:15 step:12178 [D loss: 0.348245, acc.: 86.72%] [G loss: 2.870600]\n",
      "epoch:15 step:12179 [D loss: 0.221953, acc.: 92.97%] [G loss: 2.636710]\n",
      "epoch:15 step:12180 [D loss: 0.346613, acc.: 84.38%] [G loss: 3.608085]\n",
      "epoch:15 step:12181 [D loss: 0.322133, acc.: 85.16%] [G loss: 2.878399]\n",
      "epoch:15 step:12182 [D loss: 0.260392, acc.: 88.28%] [G loss: 3.092566]\n",
      "epoch:15 step:12183 [D loss: 0.233100, acc.: 93.75%] [G loss: 3.880215]\n",
      "epoch:15 step:12184 [D loss: 0.260078, acc.: 88.28%] [G loss: 3.752313]\n",
      "epoch:15 step:12185 [D loss: 0.363341, acc.: 83.59%] [G loss: 2.829028]\n",
      "epoch:15 step:12186 [D loss: 0.332394, acc.: 87.50%] [G loss: 3.380667]\n",
      "epoch:15 step:12187 [D loss: 0.338962, acc.: 85.16%] [G loss: 2.558976]\n",
      "epoch:15 step:12188 [D loss: 0.355896, acc.: 81.25%] [G loss: 3.343180]\n",
      "epoch:15 step:12189 [D loss: 0.223081, acc.: 89.84%] [G loss: 3.018584]\n",
      "epoch:15 step:12190 [D loss: 0.388771, acc.: 82.81%] [G loss: 2.870219]\n",
      "epoch:15 step:12191 [D loss: 0.420484, acc.: 83.59%] [G loss: 2.633579]\n",
      "epoch:15 step:12192 [D loss: 0.411504, acc.: 85.16%] [G loss: 2.988628]\n",
      "epoch:15 step:12193 [D loss: 0.368001, acc.: 84.38%] [G loss: 2.879058]\n",
      "epoch:15 step:12194 [D loss: 0.230410, acc.: 89.84%] [G loss: 3.226331]\n",
      "epoch:15 step:12195 [D loss: 0.361319, acc.: 86.72%] [G loss: 2.609138]\n",
      "epoch:15 step:12196 [D loss: 0.354129, acc.: 84.38%] [G loss: 2.948546]\n",
      "epoch:15 step:12197 [D loss: 0.331056, acc.: 82.03%] [G loss: 2.945883]\n",
      "epoch:15 step:12198 [D loss: 0.264654, acc.: 90.62%] [G loss: 3.720061]\n",
      "epoch:15 step:12199 [D loss: 0.314573, acc.: 84.38%] [G loss: 4.478108]\n",
      "epoch:15 step:12200 [D loss: 0.182327, acc.: 93.75%] [G loss: 3.993664]\n",
      "##############\n",
      "[0.83796603 0.85593084 0.81459392 0.80543062 0.75269267 0.82382673\n",
      " 0.87152651 0.8521023  0.82527028 0.82445091]\n",
      "##########\n",
      "epoch:15 step:12201 [D loss: 0.273258, acc.: 86.72%] [G loss: 3.370133]\n",
      "epoch:15 step:12202 [D loss: 0.301158, acc.: 87.50%] [G loss: 3.239314]\n",
      "epoch:15 step:12203 [D loss: 0.351058, acc.: 86.72%] [G loss: 2.537016]\n",
      "epoch:15 step:12204 [D loss: 0.285516, acc.: 86.72%] [G loss: 2.661785]\n",
      "epoch:15 step:12205 [D loss: 0.236407, acc.: 92.97%] [G loss: 3.380674]\n",
      "epoch:15 step:12206 [D loss: 0.224443, acc.: 92.97%] [G loss: 4.638967]\n",
      "epoch:15 step:12207 [D loss: 0.229335, acc.: 91.41%] [G loss: 3.220498]\n",
      "epoch:15 step:12208 [D loss: 0.296072, acc.: 87.50%] [G loss: 2.811784]\n",
      "epoch:15 step:12209 [D loss: 0.208018, acc.: 92.19%] [G loss: 3.200580]\n",
      "epoch:15 step:12210 [D loss: 0.203290, acc.: 92.97%] [G loss: 3.905035]\n",
      "epoch:15 step:12211 [D loss: 0.261432, acc.: 91.41%] [G loss: 3.884143]\n",
      "epoch:15 step:12212 [D loss: 0.208592, acc.: 93.75%] [G loss: 3.056328]\n",
      "epoch:15 step:12213 [D loss: 0.395989, acc.: 82.03%] [G loss: 3.197578]\n",
      "epoch:15 step:12214 [D loss: 0.298050, acc.: 86.72%] [G loss: 2.437897]\n",
      "epoch:15 step:12215 [D loss: 0.316783, acc.: 88.28%] [G loss: 2.358280]\n",
      "epoch:15 step:12216 [D loss: 0.402761, acc.: 85.16%] [G loss: 2.374206]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:15 step:12217 [D loss: 0.405850, acc.: 78.91%] [G loss: 2.557270]\n",
      "epoch:15 step:12218 [D loss: 0.362266, acc.: 85.94%] [G loss: 3.493097]\n",
      "epoch:15 step:12219 [D loss: 0.324047, acc.: 85.94%] [G loss: 2.801568]\n",
      "epoch:15 step:12220 [D loss: 0.297604, acc.: 88.28%] [G loss: 2.745701]\n",
      "epoch:15 step:12221 [D loss: 0.276266, acc.: 86.72%] [G loss: 4.967918]\n",
      "epoch:15 step:12222 [D loss: 0.240607, acc.: 91.41%] [G loss: 6.684971]\n",
      "epoch:15 step:12223 [D loss: 0.288475, acc.: 92.19%] [G loss: 2.846350]\n",
      "epoch:15 step:12224 [D loss: 0.218809, acc.: 88.28%] [G loss: 4.548752]\n",
      "epoch:15 step:12225 [D loss: 0.241936, acc.: 88.28%] [G loss: 3.354143]\n",
      "epoch:15 step:12226 [D loss: 0.256536, acc.: 90.62%] [G loss: 3.090129]\n",
      "epoch:15 step:12227 [D loss: 0.247760, acc.: 92.19%] [G loss: 4.910795]\n",
      "epoch:15 step:12228 [D loss: 0.232710, acc.: 90.62%] [G loss: 4.978493]\n",
      "epoch:15 step:12229 [D loss: 0.387673, acc.: 82.81%] [G loss: 3.408322]\n",
      "epoch:15 step:12230 [D loss: 0.414247, acc.: 79.69%] [G loss: 2.652905]\n",
      "epoch:15 step:12231 [D loss: 0.295434, acc.: 90.62%] [G loss: 4.000853]\n",
      "epoch:15 step:12232 [D loss: 0.230003, acc.: 88.28%] [G loss: 5.881311]\n",
      "epoch:15 step:12233 [D loss: 0.279451, acc.: 90.62%] [G loss: 3.668044]\n",
      "epoch:15 step:12234 [D loss: 0.406237, acc.: 85.16%] [G loss: 3.922662]\n",
      "epoch:15 step:12235 [D loss: 0.389802, acc.: 85.94%] [G loss: 5.039178]\n",
      "epoch:15 step:12236 [D loss: 0.777971, acc.: 66.41%] [G loss: 5.089339]\n",
      "epoch:15 step:12237 [D loss: 0.616385, acc.: 75.78%] [G loss: 5.605074]\n",
      "epoch:15 step:12238 [D loss: 0.454903, acc.: 79.69%] [G loss: 3.216883]\n",
      "epoch:15 step:12239 [D loss: 0.401460, acc.: 80.47%] [G loss: 5.326361]\n",
      "epoch:15 step:12240 [D loss: 0.383233, acc.: 84.38%] [G loss: 3.692964]\n",
      "epoch:15 step:12241 [D loss: 0.387140, acc.: 82.03%] [G loss: 3.348236]\n",
      "epoch:15 step:12242 [D loss: 0.371417, acc.: 83.59%] [G loss: 3.075537]\n",
      "epoch:15 step:12243 [D loss: 0.272846, acc.: 92.97%] [G loss: 4.286054]\n",
      "epoch:15 step:12244 [D loss: 0.353004, acc.: 85.16%] [G loss: 3.471133]\n",
      "epoch:15 step:12245 [D loss: 0.360200, acc.: 86.72%] [G loss: 3.292264]\n",
      "epoch:15 step:12246 [D loss: 0.365275, acc.: 85.94%] [G loss: 2.235466]\n",
      "epoch:15 step:12247 [D loss: 0.287889, acc.: 87.50%] [G loss: 3.300135]\n",
      "epoch:15 step:12248 [D loss: 0.269069, acc.: 88.28%] [G loss: 3.271455]\n",
      "epoch:15 step:12249 [D loss: 0.298482, acc.: 87.50%] [G loss: 3.698285]\n",
      "epoch:15 step:12250 [D loss: 0.446481, acc.: 77.34%] [G loss: 4.567588]\n",
      "epoch:15 step:12251 [D loss: 0.455227, acc.: 82.03%] [G loss: 3.042098]\n",
      "epoch:15 step:12252 [D loss: 0.408609, acc.: 81.25%] [G loss: 3.524214]\n",
      "epoch:15 step:12253 [D loss: 0.392931, acc.: 82.03%] [G loss: 4.058219]\n",
      "epoch:15 step:12254 [D loss: 0.335240, acc.: 84.38%] [G loss: 3.110781]\n",
      "epoch:15 step:12255 [D loss: 0.358804, acc.: 81.25%] [G loss: 2.954365]\n",
      "epoch:15 step:12256 [D loss: 0.295741, acc.: 85.94%] [G loss: 3.386187]\n",
      "epoch:15 step:12257 [D loss: 0.388885, acc.: 85.16%] [G loss: 2.749233]\n",
      "epoch:15 step:12258 [D loss: 0.325353, acc.: 81.25%] [G loss: 3.595032]\n",
      "epoch:15 step:12259 [D loss: 0.263634, acc.: 89.84%] [G loss: 5.452792]\n",
      "epoch:15 step:12260 [D loss: 0.384450, acc.: 83.59%] [G loss: 2.783965]\n",
      "epoch:15 step:12261 [D loss: 0.262160, acc.: 90.62%] [G loss: 3.126955]\n",
      "epoch:15 step:12262 [D loss: 0.432583, acc.: 77.34%] [G loss: 4.405687]\n",
      "epoch:15 step:12263 [D loss: 0.302490, acc.: 89.06%] [G loss: 2.990049]\n",
      "epoch:15 step:12264 [D loss: 0.399416, acc.: 85.94%] [G loss: 3.088737]\n",
      "epoch:15 step:12265 [D loss: 0.412499, acc.: 78.91%] [G loss: 2.756452]\n",
      "epoch:15 step:12266 [D loss: 0.273458, acc.: 89.06%] [G loss: 3.929473]\n",
      "epoch:15 step:12267 [D loss: 0.267842, acc.: 87.50%] [G loss: 4.798802]\n",
      "epoch:15 step:12268 [D loss: 0.436163, acc.: 80.47%] [G loss: 2.737632]\n",
      "epoch:15 step:12269 [D loss: 0.240472, acc.: 89.84%] [G loss: 4.182044]\n",
      "epoch:15 step:12270 [D loss: 0.422267, acc.: 78.12%] [G loss: 4.277032]\n",
      "epoch:15 step:12271 [D loss: 0.407688, acc.: 82.03%] [G loss: 5.956913]\n",
      "epoch:15 step:12272 [D loss: 0.320588, acc.: 82.03%] [G loss: 4.561299]\n",
      "epoch:15 step:12273 [D loss: 0.392179, acc.: 77.34%] [G loss: 4.008451]\n",
      "epoch:15 step:12274 [D loss: 0.340218, acc.: 84.38%] [G loss: 3.156627]\n",
      "epoch:15 step:12275 [D loss: 0.383378, acc.: 81.25%] [G loss: 2.971209]\n",
      "epoch:15 step:12276 [D loss: 0.320995, acc.: 82.03%] [G loss: 3.914088]\n",
      "epoch:15 step:12277 [D loss: 0.368330, acc.: 84.38%] [G loss: 4.271275]\n",
      "epoch:15 step:12278 [D loss: 0.409199, acc.: 79.69%] [G loss: 3.534683]\n",
      "epoch:15 step:12279 [D loss: 0.303035, acc.: 86.72%] [G loss: 2.525619]\n",
      "epoch:15 step:12280 [D loss: 0.342528, acc.: 91.41%] [G loss: 3.153591]\n",
      "epoch:15 step:12281 [D loss: 0.348258, acc.: 83.59%] [G loss: 2.995154]\n",
      "epoch:15 step:12282 [D loss: 0.331076, acc.: 85.16%] [G loss: 3.558076]\n",
      "epoch:15 step:12283 [D loss: 0.316749, acc.: 88.28%] [G loss: 2.977469]\n",
      "epoch:15 step:12284 [D loss: 0.245611, acc.: 91.41%] [G loss: 4.296947]\n",
      "epoch:15 step:12285 [D loss: 0.321963, acc.: 85.94%] [G loss: 4.335677]\n",
      "epoch:15 step:12286 [D loss: 0.221904, acc.: 90.62%] [G loss: 5.705749]\n",
      "epoch:15 step:12287 [D loss: 0.341238, acc.: 85.16%] [G loss: 4.107314]\n",
      "epoch:15 step:12288 [D loss: 0.254131, acc.: 89.06%] [G loss: 4.426225]\n",
      "epoch:15 step:12289 [D loss: 0.245410, acc.: 88.28%] [G loss: 4.871753]\n",
      "epoch:15 step:12290 [D loss: 0.373250, acc.: 82.81%] [G loss: 2.817711]\n",
      "epoch:15 step:12291 [D loss: 0.399738, acc.: 85.16%] [G loss: 3.206952]\n",
      "epoch:15 step:12292 [D loss: 0.371553, acc.: 82.03%] [G loss: 2.879441]\n",
      "epoch:15 step:12293 [D loss: 0.356372, acc.: 84.38%] [G loss: 3.469321]\n",
      "epoch:15 step:12294 [D loss: 0.412682, acc.: 81.25%] [G loss: 3.735857]\n",
      "epoch:15 step:12295 [D loss: 0.595008, acc.: 75.78%] [G loss: 6.778847]\n",
      "epoch:15 step:12296 [D loss: 0.365565, acc.: 82.81%] [G loss: 4.714618]\n",
      "epoch:15 step:12297 [D loss: 0.278811, acc.: 87.50%] [G loss: 5.642962]\n",
      "epoch:15 step:12298 [D loss: 0.316170, acc.: 82.81%] [G loss: 3.327812]\n",
      "epoch:15 step:12299 [D loss: 0.327021, acc.: 82.03%] [G loss: 3.738601]\n",
      "epoch:15 step:12300 [D loss: 0.237501, acc.: 92.97%] [G loss: 5.030863]\n",
      "epoch:15 step:12301 [D loss: 0.239608, acc.: 92.19%] [G loss: 3.410912]\n",
      "epoch:15 step:12302 [D loss: 0.307814, acc.: 86.72%] [G loss: 3.376545]\n",
      "epoch:15 step:12303 [D loss: 0.256207, acc.: 89.06%] [G loss: 3.008381]\n",
      "epoch:15 step:12304 [D loss: 0.251916, acc.: 89.84%] [G loss: 3.954168]\n",
      "epoch:15 step:12305 [D loss: 0.298035, acc.: 87.50%] [G loss: 3.373643]\n",
      "epoch:15 step:12306 [D loss: 0.237515, acc.: 89.06%] [G loss: 4.704362]\n",
      "epoch:15 step:12307 [D loss: 0.267365, acc.: 89.84%] [G loss: 5.235291]\n",
      "epoch:15 step:12308 [D loss: 0.410752, acc.: 85.16%] [G loss: 3.281615]\n",
      "epoch:15 step:12309 [D loss: 0.309338, acc.: 89.06%] [G loss: 2.845724]\n",
      "epoch:15 step:12310 [D loss: 0.272654, acc.: 87.50%] [G loss: 4.376859]\n",
      "epoch:15 step:12311 [D loss: 0.203103, acc.: 93.75%] [G loss: 3.771209]\n",
      "epoch:15 step:12312 [D loss: 0.293717, acc.: 85.16%] [G loss: 3.896440]\n",
      "epoch:15 step:12313 [D loss: 0.450249, acc.: 80.47%] [G loss: 2.654676]\n",
      "epoch:15 step:12314 [D loss: 0.364724, acc.: 82.03%] [G loss: 2.571663]\n",
      "epoch:15 step:12315 [D loss: 0.313192, acc.: 87.50%] [G loss: 3.062421]\n",
      "epoch:15 step:12316 [D loss: 0.313249, acc.: 86.72%] [G loss: 3.436763]\n",
      "epoch:15 step:12317 [D loss: 0.381288, acc.: 86.72%] [G loss: 3.757729]\n",
      "epoch:15 step:12318 [D loss: 0.271105, acc.: 87.50%] [G loss: 2.608102]\n",
      "epoch:15 step:12319 [D loss: 0.369549, acc.: 83.59%] [G loss: 3.103817]\n",
      "epoch:15 step:12320 [D loss: 0.366398, acc.: 86.72%] [G loss: 2.814184]\n",
      "epoch:15 step:12321 [D loss: 0.314672, acc.: 86.72%] [G loss: 5.847783]\n",
      "epoch:15 step:12322 [D loss: 0.239010, acc.: 88.28%] [G loss: 7.340360]\n",
      "epoch:15 step:12323 [D loss: 0.308189, acc.: 85.94%] [G loss: 4.732973]\n",
      "epoch:15 step:12324 [D loss: 0.216436, acc.: 89.84%] [G loss: 7.905103]\n",
      "epoch:15 step:12325 [D loss: 0.211948, acc.: 92.97%] [G loss: 7.742036]\n",
      "epoch:15 step:12326 [D loss: 0.319744, acc.: 84.38%] [G loss: 3.219699]\n",
      "epoch:15 step:12327 [D loss: 0.269534, acc.: 86.72%] [G loss: 5.125856]\n",
      "epoch:15 step:12328 [D loss: 0.399243, acc.: 80.47%] [G loss: 3.820089]\n",
      "epoch:15 step:12329 [D loss: 0.187303, acc.: 92.19%] [G loss: 3.691803]\n",
      "epoch:15 step:12330 [D loss: 0.264803, acc.: 88.28%] [G loss: 3.053041]\n",
      "epoch:15 step:12331 [D loss: 0.228803, acc.: 94.53%] [G loss: 3.950670]\n",
      "epoch:15 step:12332 [D loss: 0.182260, acc.: 93.75%] [G loss: 3.186475]\n",
      "epoch:15 step:12333 [D loss: 0.267513, acc.: 85.94%] [G loss: 3.264660]\n",
      "epoch:15 step:12334 [D loss: 0.330642, acc.: 85.94%] [G loss: 4.117789]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:15 step:12335 [D loss: 0.221392, acc.: 91.41%] [G loss: 3.726542]\n",
      "epoch:15 step:12336 [D loss: 0.309111, acc.: 89.06%] [G loss: 2.995026]\n",
      "epoch:15 step:12337 [D loss: 0.347052, acc.: 83.59%] [G loss: 4.834996]\n",
      "epoch:15 step:12338 [D loss: 0.334188, acc.: 85.16%] [G loss: 2.494791]\n",
      "epoch:15 step:12339 [D loss: 0.288204, acc.: 85.94%] [G loss: 2.398286]\n",
      "epoch:15 step:12340 [D loss: 0.410401, acc.: 82.03%] [G loss: 3.735789]\n",
      "epoch:15 step:12341 [D loss: 0.285220, acc.: 88.28%] [G loss: 3.075186]\n",
      "epoch:15 step:12342 [D loss: 0.291187, acc.: 85.94%] [G loss: 3.227588]\n",
      "epoch:15 step:12343 [D loss: 0.316708, acc.: 85.94%] [G loss: 3.160834]\n",
      "epoch:15 step:12344 [D loss: 0.253571, acc.: 89.06%] [G loss: 1.893862]\n",
      "epoch:15 step:12345 [D loss: 0.341882, acc.: 84.38%] [G loss: 2.489472]\n",
      "epoch:15 step:12346 [D loss: 0.395122, acc.: 87.50%] [G loss: 3.044384]\n",
      "epoch:15 step:12347 [D loss: 0.334048, acc.: 85.16%] [G loss: 3.242890]\n",
      "epoch:15 step:12348 [D loss: 0.504564, acc.: 78.12%] [G loss: 3.400738]\n",
      "epoch:15 step:12349 [D loss: 0.584801, acc.: 78.12%] [G loss: 7.612429]\n",
      "epoch:15 step:12350 [D loss: 1.318397, acc.: 67.19%] [G loss: 6.086050]\n",
      "epoch:15 step:12351 [D loss: 1.214990, acc.: 69.53%] [G loss: 3.430418]\n",
      "epoch:15 step:12352 [D loss: 0.367498, acc.: 89.06%] [G loss: 4.206147]\n",
      "epoch:15 step:12353 [D loss: 0.541917, acc.: 75.00%] [G loss: 4.290510]\n",
      "epoch:15 step:12354 [D loss: 0.402393, acc.: 85.16%] [G loss: 3.024796]\n",
      "epoch:15 step:12355 [D loss: 0.313320, acc.: 86.72%] [G loss: 3.813841]\n",
      "epoch:15 step:12356 [D loss: 0.367582, acc.: 82.03%] [G loss: 3.753251]\n",
      "epoch:15 step:12357 [D loss: 0.353481, acc.: 82.03%] [G loss: 2.981389]\n",
      "epoch:15 step:12358 [D loss: 0.326204, acc.: 85.94%] [G loss: 3.082793]\n",
      "epoch:15 step:12359 [D loss: 0.269839, acc.: 89.06%] [G loss: 4.522410]\n",
      "epoch:15 step:12360 [D loss: 0.242333, acc.: 90.62%] [G loss: 2.771788]\n",
      "epoch:15 step:12361 [D loss: 0.364880, acc.: 83.59%] [G loss: 2.908668]\n",
      "epoch:15 step:12362 [D loss: 0.262594, acc.: 88.28%] [G loss: 3.042393]\n",
      "epoch:15 step:12363 [D loss: 0.353943, acc.: 85.94%] [G loss: 2.280592]\n",
      "epoch:15 step:12364 [D loss: 0.272358, acc.: 91.41%] [G loss: 3.268815]\n",
      "epoch:15 step:12365 [D loss: 0.325071, acc.: 87.50%] [G loss: 2.980731]\n",
      "epoch:15 step:12366 [D loss: 0.256950, acc.: 90.62%] [G loss: 2.488646]\n",
      "epoch:15 step:12367 [D loss: 0.304111, acc.: 87.50%] [G loss: 2.783249]\n",
      "epoch:15 step:12368 [D loss: 0.273781, acc.: 90.62%] [G loss: 3.378326]\n",
      "epoch:15 step:12369 [D loss: 0.356985, acc.: 85.94%] [G loss: 3.392167]\n",
      "epoch:15 step:12370 [D loss: 0.264398, acc.: 88.28%] [G loss: 2.993726]\n",
      "epoch:15 step:12371 [D loss: 0.248550, acc.: 92.19%] [G loss: 2.283447]\n",
      "epoch:15 step:12372 [D loss: 0.295238, acc.: 84.38%] [G loss: 2.798226]\n",
      "epoch:15 step:12373 [D loss: 0.326313, acc.: 89.84%] [G loss: 2.436427]\n",
      "epoch:15 step:12374 [D loss: 0.286344, acc.: 90.62%] [G loss: 3.110222]\n",
      "epoch:15 step:12375 [D loss: 0.461039, acc.: 75.78%] [G loss: 3.584755]\n",
      "epoch:15 step:12376 [D loss: 0.479673, acc.: 78.91%] [G loss: 3.276495]\n",
      "epoch:15 step:12377 [D loss: 0.358929, acc.: 80.47%] [G loss: 3.428080]\n",
      "epoch:15 step:12378 [D loss: 0.246168, acc.: 88.28%] [G loss: 3.991635]\n",
      "epoch:15 step:12379 [D loss: 0.302373, acc.: 87.50%] [G loss: 2.238864]\n",
      "epoch:15 step:12380 [D loss: 0.245397, acc.: 92.97%] [G loss: 3.825933]\n",
      "epoch:15 step:12381 [D loss: 0.312848, acc.: 87.50%] [G loss: 4.271082]\n",
      "epoch:15 step:12382 [D loss: 0.378275, acc.: 86.72%] [G loss: 2.816899]\n",
      "epoch:15 step:12383 [D loss: 0.267652, acc.: 87.50%] [G loss: 3.497802]\n",
      "epoch:15 step:12384 [D loss: 0.387639, acc.: 83.59%] [G loss: 4.181411]\n",
      "epoch:15 step:12385 [D loss: 0.530438, acc.: 78.12%] [G loss: 4.270750]\n",
      "epoch:15 step:12386 [D loss: 0.293615, acc.: 86.72%] [G loss: 2.847935]\n",
      "epoch:15 step:12387 [D loss: 0.276567, acc.: 86.72%] [G loss: 2.631219]\n",
      "epoch:15 step:12388 [D loss: 0.271488, acc.: 89.06%] [G loss: 3.757223]\n",
      "epoch:15 step:12389 [D loss: 0.278208, acc.: 91.41%] [G loss: 3.193879]\n",
      "epoch:15 step:12390 [D loss: 0.280827, acc.: 89.84%] [G loss: 2.838449]\n",
      "epoch:15 step:12391 [D loss: 0.325961, acc.: 86.72%] [G loss: 2.924416]\n",
      "epoch:15 step:12392 [D loss: 0.422797, acc.: 85.16%] [G loss: 2.620905]\n",
      "epoch:15 step:12393 [D loss: 0.371744, acc.: 82.81%] [G loss: 3.298534]\n",
      "epoch:15 step:12394 [D loss: 0.353640, acc.: 85.16%] [G loss: 2.512145]\n",
      "epoch:15 step:12395 [D loss: 0.349384, acc.: 83.59%] [G loss: 3.578947]\n",
      "epoch:15 step:12396 [D loss: 0.276386, acc.: 88.28%] [G loss: 3.226076]\n",
      "epoch:15 step:12397 [D loss: 0.361252, acc.: 83.59%] [G loss: 3.591715]\n",
      "epoch:15 step:12398 [D loss: 0.320927, acc.: 83.59%] [G loss: 2.499518]\n",
      "epoch:15 step:12399 [D loss: 0.359360, acc.: 82.03%] [G loss: 3.143924]\n",
      "epoch:15 step:12400 [D loss: 0.234561, acc.: 91.41%] [G loss: 3.881952]\n",
      "##############\n",
      "[0.85215025 0.84970519 0.79273915 0.80599539 0.78706786 0.81116959\n",
      " 0.87322126 0.84203931 0.81434156 0.80424416]\n",
      "##########\n",
      "epoch:15 step:12401 [D loss: 0.291076, acc.: 87.50%] [G loss: 2.844588]\n",
      "epoch:15 step:12402 [D loss: 0.282588, acc.: 91.41%] [G loss: 2.508201]\n",
      "epoch:15 step:12403 [D loss: 0.340835, acc.: 82.03%] [G loss: 3.549419]\n",
      "epoch:15 step:12404 [D loss: 0.244486, acc.: 89.84%] [G loss: 2.270314]\n",
      "epoch:15 step:12405 [D loss: 0.269297, acc.: 87.50%] [G loss: 3.155800]\n",
      "epoch:15 step:12406 [D loss: 0.355823, acc.: 82.81%] [G loss: 2.145318]\n",
      "epoch:15 step:12407 [D loss: 0.358163, acc.: 80.47%] [G loss: 3.684570]\n",
      "epoch:15 step:12408 [D loss: 0.313729, acc.: 85.16%] [G loss: 3.056297]\n",
      "epoch:15 step:12409 [D loss: 0.303628, acc.: 88.28%] [G loss: 2.730763]\n",
      "epoch:15 step:12410 [D loss: 0.322878, acc.: 86.72%] [G loss: 3.265801]\n",
      "epoch:15 step:12411 [D loss: 0.368397, acc.: 79.69%] [G loss: 2.766520]\n",
      "epoch:15 step:12412 [D loss: 0.272189, acc.: 89.06%] [G loss: 3.241663]\n",
      "epoch:15 step:12413 [D loss: 0.370952, acc.: 85.16%] [G loss: 2.444425]\n",
      "epoch:15 step:12414 [D loss: 0.248491, acc.: 89.84%] [G loss: 2.334524]\n",
      "epoch:15 step:12415 [D loss: 0.296224, acc.: 88.28%] [G loss: 2.749967]\n",
      "epoch:15 step:12416 [D loss: 0.402076, acc.: 81.25%] [G loss: 2.829452]\n",
      "epoch:15 step:12417 [D loss: 0.328312, acc.: 87.50%] [G loss: 3.703398]\n",
      "epoch:15 step:12418 [D loss: 0.274646, acc.: 86.72%] [G loss: 5.512207]\n",
      "epoch:15 step:12419 [D loss: 0.359611, acc.: 83.59%] [G loss: 2.984521]\n",
      "epoch:15 step:12420 [D loss: 0.307940, acc.: 84.38%] [G loss: 3.431142]\n",
      "epoch:15 step:12421 [D loss: 0.314538, acc.: 81.25%] [G loss: 4.096274]\n",
      "epoch:15 step:12422 [D loss: 0.364350, acc.: 87.50%] [G loss: 3.132879]\n",
      "epoch:15 step:12423 [D loss: 0.294456, acc.: 85.94%] [G loss: 2.803799]\n",
      "epoch:15 step:12424 [D loss: 0.337208, acc.: 86.72%] [G loss: 3.616830]\n",
      "epoch:15 step:12425 [D loss: 0.368406, acc.: 85.16%] [G loss: 2.704232]\n",
      "epoch:15 step:12426 [D loss: 0.235965, acc.: 89.84%] [G loss: 4.199915]\n",
      "epoch:15 step:12427 [D loss: 0.339931, acc.: 85.94%] [G loss: 4.117435]\n",
      "epoch:15 step:12428 [D loss: 0.331879, acc.: 85.94%] [G loss: 2.637747]\n",
      "epoch:15 step:12429 [D loss: 0.406137, acc.: 82.81%] [G loss: 3.005291]\n",
      "epoch:15 step:12430 [D loss: 0.363912, acc.: 89.06%] [G loss: 2.352386]\n",
      "epoch:15 step:12431 [D loss: 0.380767, acc.: 84.38%] [G loss: 2.709365]\n",
      "epoch:15 step:12432 [D loss: 0.238665, acc.: 91.41%] [G loss: 3.916447]\n",
      "epoch:15 step:12433 [D loss: 0.218203, acc.: 90.62%] [G loss: 4.208621]\n",
      "epoch:15 step:12434 [D loss: 0.219342, acc.: 94.53%] [G loss: 3.080259]\n",
      "epoch:15 step:12435 [D loss: 0.259099, acc.: 90.62%] [G loss: 2.305522]\n",
      "epoch:15 step:12436 [D loss: 0.404515, acc.: 80.47%] [G loss: 2.567381]\n",
      "epoch:15 step:12437 [D loss: 0.264594, acc.: 90.62%] [G loss: 2.562531]\n",
      "epoch:15 step:12438 [D loss: 0.366619, acc.: 85.94%] [G loss: 2.627587]\n",
      "epoch:15 step:12439 [D loss: 0.257246, acc.: 88.28%] [G loss: 2.708512]\n",
      "epoch:15 step:12440 [D loss: 0.253819, acc.: 87.50%] [G loss: 3.388677]\n",
      "epoch:15 step:12441 [D loss: 0.217253, acc.: 90.62%] [G loss: 3.976327]\n",
      "epoch:15 step:12442 [D loss: 0.251450, acc.: 90.62%] [G loss: 5.866855]\n",
      "epoch:15 step:12443 [D loss: 0.332478, acc.: 87.50%] [G loss: 3.188122]\n",
      "epoch:15 step:12444 [D loss: 0.295006, acc.: 85.16%] [G loss: 2.854664]\n",
      "epoch:15 step:12445 [D loss: 0.357647, acc.: 86.72%] [G loss: 3.464784]\n",
      "epoch:15 step:12446 [D loss: 0.326038, acc.: 85.94%] [G loss: 3.051519]\n",
      "epoch:15 step:12447 [D loss: 0.235752, acc.: 92.19%] [G loss: 3.749088]\n",
      "epoch:15 step:12448 [D loss: 0.336082, acc.: 88.28%] [G loss: 3.410997]\n",
      "epoch:15 step:12449 [D loss: 0.212336, acc.: 92.19%] [G loss: 4.103338]\n",
      "epoch:15 step:12450 [D loss: 0.214210, acc.: 93.75%] [G loss: 2.830995]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:15 step:12451 [D loss: 0.283488, acc.: 87.50%] [G loss: 3.280205]\n",
      "epoch:15 step:12452 [D loss: 0.296718, acc.: 87.50%] [G loss: 2.932481]\n",
      "epoch:15 step:12453 [D loss: 0.378204, acc.: 81.25%] [G loss: 3.603662]\n",
      "epoch:15 step:12454 [D loss: 0.315208, acc.: 87.50%] [G loss: 2.956904]\n",
      "epoch:15 step:12455 [D loss: 0.386701, acc.: 82.03%] [G loss: 3.418640]\n",
      "epoch:15 step:12456 [D loss: 0.339679, acc.: 87.50%] [G loss: 2.715336]\n",
      "epoch:15 step:12457 [D loss: 0.248908, acc.: 91.41%] [G loss: 3.025603]\n",
      "epoch:15 step:12458 [D loss: 0.336016, acc.: 85.94%] [G loss: 2.900765]\n",
      "epoch:15 step:12459 [D loss: 0.263999, acc.: 91.41%] [G loss: 2.791193]\n",
      "epoch:15 step:12460 [D loss: 0.285906, acc.: 85.16%] [G loss: 3.855206]\n",
      "epoch:15 step:12461 [D loss: 0.222799, acc.: 90.62%] [G loss: 3.890452]\n",
      "epoch:15 step:12462 [D loss: 0.307270, acc.: 86.72%] [G loss: 3.040891]\n",
      "epoch:15 step:12463 [D loss: 0.255502, acc.: 86.72%] [G loss: 3.828498]\n",
      "epoch:15 step:12464 [D loss: 0.322732, acc.: 86.72%] [G loss: 4.263874]\n",
      "epoch:15 step:12465 [D loss: 0.373340, acc.: 84.38%] [G loss: 6.478461]\n",
      "epoch:15 step:12466 [D loss: 0.271797, acc.: 86.72%] [G loss: 3.290325]\n",
      "epoch:15 step:12467 [D loss: 0.300471, acc.: 85.16%] [G loss: 2.921499]\n",
      "epoch:15 step:12468 [D loss: 0.253852, acc.: 89.06%] [G loss: 2.836813]\n",
      "epoch:15 step:12469 [D loss: 0.395625, acc.: 84.38%] [G loss: 2.928198]\n",
      "epoch:15 step:12470 [D loss: 0.237498, acc.: 89.06%] [G loss: 4.277290]\n",
      "epoch:15 step:12471 [D loss: 0.244634, acc.: 87.50%] [G loss: 2.985631]\n",
      "epoch:15 step:12472 [D loss: 0.324360, acc.: 85.16%] [G loss: 2.958211]\n",
      "epoch:15 step:12473 [D loss: 0.373941, acc.: 80.47%] [G loss: 2.816675]\n",
      "epoch:15 step:12474 [D loss: 0.384760, acc.: 83.59%] [G loss: 3.937360]\n",
      "epoch:15 step:12475 [D loss: 0.560910, acc.: 76.56%] [G loss: 4.207249]\n",
      "epoch:15 step:12476 [D loss: 0.744651, acc.: 68.75%] [G loss: 4.559670]\n",
      "epoch:15 step:12477 [D loss: 1.197208, acc.: 71.88%] [G loss: 9.919319]\n",
      "epoch:15 step:12478 [D loss: 2.627953, acc.: 51.56%] [G loss: 2.083263]\n",
      "epoch:15 step:12479 [D loss: 0.508663, acc.: 78.12%] [G loss: 5.143598]\n",
      "epoch:15 step:12480 [D loss: 0.816130, acc.: 67.19%] [G loss: 4.516610]\n",
      "epoch:15 step:12481 [D loss: 0.386083, acc.: 82.81%] [G loss: 3.587119]\n",
      "epoch:15 step:12482 [D loss: 0.497210, acc.: 75.78%] [G loss: 4.267666]\n",
      "epoch:15 step:12483 [D loss: 0.312493, acc.: 85.16%] [G loss: 4.963789]\n",
      "epoch:15 step:12484 [D loss: 0.451869, acc.: 83.59%] [G loss: 3.659492]\n",
      "epoch:15 step:12485 [D loss: 0.251200, acc.: 89.06%] [G loss: 7.003712]\n",
      "epoch:15 step:12486 [D loss: 0.460040, acc.: 78.91%] [G loss: 2.648222]\n",
      "epoch:15 step:12487 [D loss: 0.260381, acc.: 87.50%] [G loss: 4.350243]\n",
      "epoch:15 step:12488 [D loss: 0.308673, acc.: 86.72%] [G loss: 3.307197]\n",
      "epoch:15 step:12489 [D loss: 0.271828, acc.: 88.28%] [G loss: 3.149241]\n",
      "epoch:15 step:12490 [D loss: 0.408509, acc.: 84.38%] [G loss: 3.820583]\n",
      "epoch:15 step:12491 [D loss: 0.385434, acc.: 82.03%] [G loss: 3.464797]\n",
      "epoch:15 step:12492 [D loss: 0.281752, acc.: 84.38%] [G loss: 4.281781]\n",
      "epoch:15 step:12493 [D loss: 0.243611, acc.: 91.41%] [G loss: 2.686859]\n",
      "epoch:15 step:12494 [D loss: 0.330720, acc.: 85.16%] [G loss: 3.187946]\n",
      "epoch:15 step:12495 [D loss: 0.368417, acc.: 85.94%] [G loss: 4.195932]\n",
      "epoch:15 step:12496 [D loss: 0.373415, acc.: 85.94%] [G loss: 3.787429]\n",
      "epoch:16 step:12497 [D loss: 0.168858, acc.: 94.53%] [G loss: 4.470627]\n",
      "epoch:16 step:12498 [D loss: 0.277013, acc.: 89.06%] [G loss: 2.838140]\n",
      "epoch:16 step:12499 [D loss: 0.318026, acc.: 87.50%] [G loss: 2.819068]\n",
      "epoch:16 step:12500 [D loss: 0.312440, acc.: 85.94%] [G loss: 4.488086]\n",
      "epoch:16 step:12501 [D loss: 0.314458, acc.: 82.81%] [G loss: 2.414877]\n",
      "epoch:16 step:12502 [D loss: 0.295337, acc.: 88.28%] [G loss: 3.017196]\n",
      "epoch:16 step:12503 [D loss: 0.268023, acc.: 90.62%] [G loss: 2.984279]\n",
      "epoch:16 step:12504 [D loss: 0.237213, acc.: 89.06%] [G loss: 2.604350]\n",
      "epoch:16 step:12505 [D loss: 0.265530, acc.: 89.84%] [G loss: 2.874657]\n",
      "epoch:16 step:12506 [D loss: 0.330138, acc.: 82.03%] [G loss: 2.481975]\n",
      "epoch:16 step:12507 [D loss: 0.320288, acc.: 89.06%] [G loss: 2.412892]\n",
      "epoch:16 step:12508 [D loss: 0.247097, acc.: 89.84%] [G loss: 2.402246]\n",
      "epoch:16 step:12509 [D loss: 0.388700, acc.: 82.81%] [G loss: 1.925235]\n",
      "epoch:16 step:12510 [D loss: 0.471064, acc.: 78.12%] [G loss: 2.463876]\n",
      "epoch:16 step:12511 [D loss: 0.508749, acc.: 78.12%] [G loss: 2.703840]\n",
      "epoch:16 step:12512 [D loss: 0.304468, acc.: 89.84%] [G loss: 3.094345]\n",
      "epoch:16 step:12513 [D loss: 0.307448, acc.: 86.72%] [G loss: 2.521985]\n",
      "epoch:16 step:12514 [D loss: 0.350426, acc.: 82.81%] [G loss: 3.154668]\n",
      "epoch:16 step:12515 [D loss: 0.382414, acc.: 79.69%] [G loss: 2.735216]\n",
      "epoch:16 step:12516 [D loss: 0.325595, acc.: 89.84%] [G loss: 2.747105]\n",
      "epoch:16 step:12517 [D loss: 0.260296, acc.: 91.41%] [G loss: 2.986206]\n",
      "epoch:16 step:12518 [D loss: 0.241445, acc.: 89.06%] [G loss: 3.788493]\n",
      "epoch:16 step:12519 [D loss: 0.262789, acc.: 92.19%] [G loss: 3.135457]\n",
      "epoch:16 step:12520 [D loss: 0.346550, acc.: 85.16%] [G loss: 3.887082]\n",
      "epoch:16 step:12521 [D loss: 0.348283, acc.: 82.03%] [G loss: 3.210319]\n",
      "epoch:16 step:12522 [D loss: 0.263724, acc.: 87.50%] [G loss: 3.796456]\n",
      "epoch:16 step:12523 [D loss: 0.287292, acc.: 86.72%] [G loss: 4.257096]\n",
      "epoch:16 step:12524 [D loss: 0.265891, acc.: 87.50%] [G loss: 3.692278]\n",
      "epoch:16 step:12525 [D loss: 0.269435, acc.: 91.41%] [G loss: 2.782929]\n",
      "epoch:16 step:12526 [D loss: 0.265305, acc.: 89.06%] [G loss: 3.374564]\n",
      "epoch:16 step:12527 [D loss: 0.262987, acc.: 86.72%] [G loss: 4.143815]\n",
      "epoch:16 step:12528 [D loss: 0.324054, acc.: 85.16%] [G loss: 3.304442]\n",
      "epoch:16 step:12529 [D loss: 0.330944, acc.: 85.94%] [G loss: 2.101459]\n",
      "epoch:16 step:12530 [D loss: 0.334780, acc.: 85.94%] [G loss: 3.070261]\n",
      "epoch:16 step:12531 [D loss: 0.336180, acc.: 87.50%] [G loss: 2.333358]\n",
      "epoch:16 step:12532 [D loss: 0.281803, acc.: 87.50%] [G loss: 3.906042]\n",
      "epoch:16 step:12533 [D loss: 0.310657, acc.: 87.50%] [G loss: 4.052600]\n",
      "epoch:16 step:12534 [D loss: 0.486414, acc.: 81.25%] [G loss: 2.564679]\n",
      "epoch:16 step:12535 [D loss: 0.426798, acc.: 80.47%] [G loss: 3.593845]\n",
      "epoch:16 step:12536 [D loss: 0.383771, acc.: 82.81%] [G loss: 2.257528]\n",
      "epoch:16 step:12537 [D loss: 0.313314, acc.: 85.94%] [G loss: 2.855218]\n",
      "epoch:16 step:12538 [D loss: 0.383985, acc.: 81.25%] [G loss: 3.480546]\n",
      "epoch:16 step:12539 [D loss: 0.340424, acc.: 80.47%] [G loss: 3.622860]\n",
      "epoch:16 step:12540 [D loss: 0.493595, acc.: 76.56%] [G loss: 2.855753]\n",
      "epoch:16 step:12541 [D loss: 0.480621, acc.: 78.12%] [G loss: 3.008670]\n",
      "epoch:16 step:12542 [D loss: 0.355356, acc.: 84.38%] [G loss: 2.750736]\n",
      "epoch:16 step:12543 [D loss: 0.379403, acc.: 83.59%] [G loss: 5.038169]\n",
      "epoch:16 step:12544 [D loss: 0.551786, acc.: 77.34%] [G loss: 4.302394]\n",
      "epoch:16 step:12545 [D loss: 0.210433, acc.: 92.19%] [G loss: 5.504273]\n",
      "epoch:16 step:12546 [D loss: 0.294560, acc.: 86.72%] [G loss: 4.608776]\n",
      "epoch:16 step:12547 [D loss: 0.266870, acc.: 89.06%] [G loss: 3.927699]\n",
      "epoch:16 step:12548 [D loss: 0.240969, acc.: 91.41%] [G loss: 4.674960]\n",
      "epoch:16 step:12549 [D loss: 0.226265, acc.: 92.19%] [G loss: 5.116682]\n",
      "epoch:16 step:12550 [D loss: 0.330515, acc.: 86.72%] [G loss: 3.488009]\n",
      "epoch:16 step:12551 [D loss: 0.221552, acc.: 91.41%] [G loss: 3.981894]\n",
      "epoch:16 step:12552 [D loss: 0.218683, acc.: 91.41%] [G loss: 3.644737]\n",
      "epoch:16 step:12553 [D loss: 0.440597, acc.: 78.91%] [G loss: 3.302474]\n",
      "epoch:16 step:12554 [D loss: 0.310246, acc.: 85.16%] [G loss: 2.668968]\n",
      "epoch:16 step:12555 [D loss: 0.355033, acc.: 84.38%] [G loss: 3.151971]\n",
      "epoch:16 step:12556 [D loss: 0.346965, acc.: 83.59%] [G loss: 2.805645]\n",
      "epoch:16 step:12557 [D loss: 0.352982, acc.: 84.38%] [G loss: 2.193118]\n",
      "epoch:16 step:12558 [D loss: 0.285782, acc.: 89.84%] [G loss: 3.091690]\n",
      "epoch:16 step:12559 [D loss: 0.242251, acc.: 89.06%] [G loss: 3.013855]\n",
      "epoch:16 step:12560 [D loss: 0.323874, acc.: 85.16%] [G loss: 2.834799]\n",
      "epoch:16 step:12561 [D loss: 0.352104, acc.: 84.38%] [G loss: 2.864044]\n",
      "epoch:16 step:12562 [D loss: 0.441741, acc.: 76.56%] [G loss: 4.497332]\n",
      "epoch:16 step:12563 [D loss: 0.423092, acc.: 79.69%] [G loss: 4.520040]\n",
      "epoch:16 step:12564 [D loss: 0.438076, acc.: 81.25%] [G loss: 3.431484]\n",
      "epoch:16 step:12565 [D loss: 0.380232, acc.: 78.12%] [G loss: 2.743443]\n",
      "epoch:16 step:12566 [D loss: 0.400555, acc.: 85.94%] [G loss: 2.858215]\n",
      "epoch:16 step:12567 [D loss: 0.268869, acc.: 89.84%] [G loss: 2.866824]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:16 step:12568 [D loss: 0.419960, acc.: 81.25%] [G loss: 3.001322]\n",
      "epoch:16 step:12569 [D loss: 0.419013, acc.: 82.03%] [G loss: 2.648873]\n",
      "epoch:16 step:12570 [D loss: 0.281647, acc.: 86.72%] [G loss: 3.770109]\n",
      "epoch:16 step:12571 [D loss: 0.334682, acc.: 85.16%] [G loss: 3.686085]\n",
      "epoch:16 step:12572 [D loss: 0.523273, acc.: 74.22%] [G loss: 2.974011]\n",
      "epoch:16 step:12573 [D loss: 0.393779, acc.: 78.91%] [G loss: 3.267684]\n",
      "epoch:16 step:12574 [D loss: 0.463286, acc.: 73.44%] [G loss: 2.241890]\n",
      "epoch:16 step:12575 [D loss: 0.233035, acc.: 90.62%] [G loss: 3.164098]\n",
      "epoch:16 step:12576 [D loss: 0.289833, acc.: 85.94%] [G loss: 2.903522]\n",
      "epoch:16 step:12577 [D loss: 0.452678, acc.: 78.12%] [G loss: 3.215523]\n",
      "epoch:16 step:12578 [D loss: 0.302537, acc.: 84.38%] [G loss: 2.419351]\n",
      "epoch:16 step:12579 [D loss: 0.306681, acc.: 89.84%] [G loss: 4.596234]\n",
      "epoch:16 step:12580 [D loss: 0.289962, acc.: 88.28%] [G loss: 3.443194]\n",
      "epoch:16 step:12581 [D loss: 0.346943, acc.: 84.38%] [G loss: 4.038325]\n",
      "epoch:16 step:12582 [D loss: 0.203073, acc.: 94.53%] [G loss: 3.624163]\n",
      "epoch:16 step:12583 [D loss: 0.273549, acc.: 86.72%] [G loss: 3.181940]\n",
      "epoch:16 step:12584 [D loss: 0.270239, acc.: 84.38%] [G loss: 4.179597]\n",
      "epoch:16 step:12585 [D loss: 0.248993, acc.: 90.62%] [G loss: 3.975612]\n",
      "epoch:16 step:12586 [D loss: 0.383775, acc.: 82.81%] [G loss: 3.359949]\n",
      "epoch:16 step:12587 [D loss: 0.234857, acc.: 89.84%] [G loss: 5.160212]\n",
      "epoch:16 step:12588 [D loss: 0.247942, acc.: 90.62%] [G loss: 3.270929]\n",
      "epoch:16 step:12589 [D loss: 0.333044, acc.: 85.94%] [G loss: 3.448703]\n",
      "epoch:16 step:12590 [D loss: 0.255001, acc.: 92.19%] [G loss: 3.855644]\n",
      "epoch:16 step:12591 [D loss: 0.322176, acc.: 82.81%] [G loss: 4.482403]\n",
      "epoch:16 step:12592 [D loss: 0.356534, acc.: 84.38%] [G loss: 3.672351]\n",
      "epoch:16 step:12593 [D loss: 0.283396, acc.: 89.84%] [G loss: 3.282388]\n",
      "epoch:16 step:12594 [D loss: 0.411979, acc.: 82.03%] [G loss: 3.141140]\n",
      "epoch:16 step:12595 [D loss: 0.223778, acc.: 93.75%] [G loss: 4.209101]\n",
      "epoch:16 step:12596 [D loss: 0.269080, acc.: 88.28%] [G loss: 2.327486]\n",
      "epoch:16 step:12597 [D loss: 0.261569, acc.: 90.62%] [G loss: 3.332241]\n",
      "epoch:16 step:12598 [D loss: 0.373831, acc.: 83.59%] [G loss: 3.899046]\n",
      "epoch:16 step:12599 [D loss: 0.343710, acc.: 85.16%] [G loss: 3.438091]\n",
      "epoch:16 step:12600 [D loss: 0.378479, acc.: 84.38%] [G loss: 2.369346]\n",
      "##############\n",
      "[0.85179068 0.84992543 0.81500489 0.79786201 0.77007746 0.83904085\n",
      " 0.86092258 0.84448256 0.82327991 0.79906979]\n",
      "##########\n",
      "epoch:16 step:12601 [D loss: 0.324448, acc.: 85.16%] [G loss: 3.045687]\n",
      "epoch:16 step:12602 [D loss: 0.453908, acc.: 80.47%] [G loss: 4.610081]\n",
      "epoch:16 step:12603 [D loss: 0.337526, acc.: 83.59%] [G loss: 3.772816]\n",
      "epoch:16 step:12604 [D loss: 0.348660, acc.: 87.50%] [G loss: 3.482571]\n",
      "epoch:16 step:12605 [D loss: 0.269553, acc.: 87.50%] [G loss: 4.627634]\n",
      "epoch:16 step:12606 [D loss: 0.330935, acc.: 88.28%] [G loss: 3.684726]\n",
      "epoch:16 step:12607 [D loss: 0.431774, acc.: 83.59%] [G loss: 3.021960]\n",
      "epoch:16 step:12608 [D loss: 0.306743, acc.: 85.94%] [G loss: 3.773449]\n",
      "epoch:16 step:12609 [D loss: 0.422776, acc.: 80.47%] [G loss: 2.753369]\n",
      "epoch:16 step:12610 [D loss: 0.374745, acc.: 85.16%] [G loss: 2.584225]\n",
      "epoch:16 step:12611 [D loss: 0.426987, acc.: 83.59%] [G loss: 6.159856]\n",
      "epoch:16 step:12612 [D loss: 0.647841, acc.: 73.44%] [G loss: 4.252454]\n",
      "epoch:16 step:12613 [D loss: 0.551398, acc.: 77.34%] [G loss: 3.491811]\n",
      "epoch:16 step:12614 [D loss: 0.410243, acc.: 82.03%] [G loss: 4.242873]\n",
      "epoch:16 step:12615 [D loss: 0.218724, acc.: 89.84%] [G loss: 3.397304]\n",
      "epoch:16 step:12616 [D loss: 0.380833, acc.: 83.59%] [G loss: 4.531424]\n",
      "epoch:16 step:12617 [D loss: 0.322338, acc.: 85.16%] [G loss: 3.051260]\n",
      "epoch:16 step:12618 [D loss: 0.304263, acc.: 85.16%] [G loss: 3.884610]\n",
      "epoch:16 step:12619 [D loss: 0.284599, acc.: 89.84%] [G loss: 3.873107]\n",
      "epoch:16 step:12620 [D loss: 0.283207, acc.: 89.84%] [G loss: 7.158958]\n",
      "epoch:16 step:12621 [D loss: 0.296910, acc.: 89.06%] [G loss: 3.518421]\n",
      "epoch:16 step:12622 [D loss: 0.254025, acc.: 89.84%] [G loss: 3.725320]\n",
      "epoch:16 step:12623 [D loss: 0.278141, acc.: 89.06%] [G loss: 3.305532]\n",
      "epoch:16 step:12624 [D loss: 0.264007, acc.: 87.50%] [G loss: 5.220732]\n",
      "epoch:16 step:12625 [D loss: 0.195595, acc.: 91.41%] [G loss: 5.134930]\n",
      "epoch:16 step:12626 [D loss: 0.323775, acc.: 85.16%] [G loss: 2.357803]\n",
      "epoch:16 step:12627 [D loss: 0.371446, acc.: 80.47%] [G loss: 2.917775]\n",
      "epoch:16 step:12628 [D loss: 0.283954, acc.: 88.28%] [G loss: 3.558321]\n",
      "epoch:16 step:12629 [D loss: 0.229632, acc.: 91.41%] [G loss: 3.350339]\n",
      "epoch:16 step:12630 [D loss: 0.323353, acc.: 85.16%] [G loss: 3.594357]\n",
      "epoch:16 step:12631 [D loss: 0.369274, acc.: 82.81%] [G loss: 5.162860]\n",
      "epoch:16 step:12632 [D loss: 0.256124, acc.: 92.19%] [G loss: 3.421808]\n",
      "epoch:16 step:12633 [D loss: 0.422801, acc.: 78.12%] [G loss: 5.270102]\n",
      "epoch:16 step:12634 [D loss: 0.387541, acc.: 84.38%] [G loss: 3.932228]\n",
      "epoch:16 step:12635 [D loss: 0.243753, acc.: 89.84%] [G loss: 4.214018]\n",
      "epoch:16 step:12636 [D loss: 0.401312, acc.: 77.34%] [G loss: 2.931614]\n",
      "epoch:16 step:12637 [D loss: 0.349364, acc.: 82.81%] [G loss: 4.587827]\n",
      "epoch:16 step:12638 [D loss: 0.254543, acc.: 89.84%] [G loss: 2.887631]\n",
      "epoch:16 step:12639 [D loss: 0.199728, acc.: 92.97%] [G loss: 4.188208]\n",
      "epoch:16 step:12640 [D loss: 0.552421, acc.: 75.00%] [G loss: 3.312966]\n",
      "epoch:16 step:12641 [D loss: 0.381105, acc.: 81.25%] [G loss: 3.515403]\n",
      "epoch:16 step:12642 [D loss: 0.405709, acc.: 83.59%] [G loss: 4.424282]\n",
      "epoch:16 step:12643 [D loss: 0.421455, acc.: 81.25%] [G loss: 3.984286]\n",
      "epoch:16 step:12644 [D loss: 0.335254, acc.: 87.50%] [G loss: 4.364110]\n",
      "epoch:16 step:12645 [D loss: 0.395678, acc.: 80.47%] [G loss: 2.665717]\n",
      "epoch:16 step:12646 [D loss: 0.353816, acc.: 82.03%] [G loss: 2.802800]\n",
      "epoch:16 step:12647 [D loss: 0.309199, acc.: 88.28%] [G loss: 2.846147]\n",
      "epoch:16 step:12648 [D loss: 0.333759, acc.: 89.06%] [G loss: 2.667320]\n",
      "epoch:16 step:12649 [D loss: 0.392057, acc.: 81.25%] [G loss: 2.493644]\n",
      "epoch:16 step:12650 [D loss: 0.416871, acc.: 85.16%] [G loss: 2.804941]\n",
      "epoch:16 step:12651 [D loss: 0.301971, acc.: 89.06%] [G loss: 3.299287]\n",
      "epoch:16 step:12652 [D loss: 0.334257, acc.: 84.38%] [G loss: 3.234864]\n",
      "epoch:16 step:12653 [D loss: 0.373908, acc.: 87.50%] [G loss: 3.450446]\n",
      "epoch:16 step:12654 [D loss: 0.331074, acc.: 82.03%] [G loss: 4.695174]\n",
      "epoch:16 step:12655 [D loss: 0.361028, acc.: 85.16%] [G loss: 3.692801]\n",
      "epoch:16 step:12656 [D loss: 0.386146, acc.: 86.72%] [G loss: 2.162389]\n",
      "epoch:16 step:12657 [D loss: 0.379361, acc.: 82.03%] [G loss: 3.719041]\n",
      "epoch:16 step:12658 [D loss: 0.461512, acc.: 80.47%] [G loss: 2.589446]\n",
      "epoch:16 step:12659 [D loss: 0.401926, acc.: 83.59%] [G loss: 4.664670]\n",
      "epoch:16 step:12660 [D loss: 0.481372, acc.: 80.47%] [G loss: 3.261878]\n",
      "epoch:16 step:12661 [D loss: 0.317170, acc.: 85.94%] [G loss: 3.580120]\n",
      "epoch:16 step:12662 [D loss: 0.352806, acc.: 82.03%] [G loss: 3.211642]\n",
      "epoch:16 step:12663 [D loss: 0.279041, acc.: 89.06%] [G loss: 5.293469]\n",
      "epoch:16 step:12664 [D loss: 0.436159, acc.: 82.81%] [G loss: 4.062822]\n",
      "epoch:16 step:12665 [D loss: 0.364668, acc.: 85.16%] [G loss: 4.459435]\n",
      "epoch:16 step:12666 [D loss: 0.204303, acc.: 90.62%] [G loss: 5.186996]\n",
      "epoch:16 step:12667 [D loss: 0.356675, acc.: 83.59%] [G loss: 3.432962]\n",
      "epoch:16 step:12668 [D loss: 0.318212, acc.: 83.59%] [G loss: 2.842391]\n",
      "epoch:16 step:12669 [D loss: 0.324565, acc.: 85.94%] [G loss: 4.872728]\n",
      "epoch:16 step:12670 [D loss: 0.204219, acc.: 89.84%] [G loss: 7.015066]\n",
      "epoch:16 step:12671 [D loss: 0.303447, acc.: 86.72%] [G loss: 4.746521]\n",
      "epoch:16 step:12672 [D loss: 0.311332, acc.: 87.50%] [G loss: 5.568188]\n",
      "epoch:16 step:12673 [D loss: 0.347916, acc.: 86.72%] [G loss: 4.603052]\n",
      "epoch:16 step:12674 [D loss: 0.255493, acc.: 89.84%] [G loss: 3.564424]\n",
      "epoch:16 step:12675 [D loss: 0.282702, acc.: 87.50%] [G loss: 5.405546]\n",
      "epoch:16 step:12676 [D loss: 0.403799, acc.: 82.03%] [G loss: 2.612597]\n",
      "epoch:16 step:12677 [D loss: 0.264609, acc.: 92.19%] [G loss: 2.878889]\n",
      "epoch:16 step:12678 [D loss: 0.387401, acc.: 82.81%] [G loss: 2.601161]\n",
      "epoch:16 step:12679 [D loss: 0.457430, acc.: 82.03%] [G loss: 3.492990]\n",
      "epoch:16 step:12680 [D loss: 0.380115, acc.: 79.69%] [G loss: 3.368441]\n",
      "epoch:16 step:12681 [D loss: 0.435263, acc.: 84.38%] [G loss: 3.493445]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:16 step:12682 [D loss: 0.400081, acc.: 82.03%] [G loss: 3.128799]\n",
      "epoch:16 step:12683 [D loss: 0.315104, acc.: 86.72%] [G loss: 3.035334]\n",
      "epoch:16 step:12684 [D loss: 0.409610, acc.: 81.25%] [G loss: 2.323779]\n",
      "epoch:16 step:12685 [D loss: 0.285125, acc.: 90.62%] [G loss: 2.649653]\n",
      "epoch:16 step:12686 [D loss: 0.349883, acc.: 82.03%] [G loss: 3.517927]\n",
      "epoch:16 step:12687 [D loss: 0.227923, acc.: 92.19%] [G loss: 4.913549]\n",
      "epoch:16 step:12688 [D loss: 0.309850, acc.: 85.94%] [G loss: 3.700998]\n",
      "epoch:16 step:12689 [D loss: 0.371766, acc.: 78.91%] [G loss: 3.116212]\n",
      "epoch:16 step:12690 [D loss: 0.227930, acc.: 92.97%] [G loss: 3.467118]\n",
      "epoch:16 step:12691 [D loss: 0.227710, acc.: 90.62%] [G loss: 3.164212]\n",
      "epoch:16 step:12692 [D loss: 0.284065, acc.: 90.62%] [G loss: 3.004075]\n",
      "epoch:16 step:12693 [D loss: 0.233579, acc.: 90.62%] [G loss: 5.884351]\n",
      "epoch:16 step:12694 [D loss: 0.293830, acc.: 88.28%] [G loss: 3.652144]\n",
      "epoch:16 step:12695 [D loss: 0.276018, acc.: 89.06%] [G loss: 3.078818]\n",
      "epoch:16 step:12696 [D loss: 0.256745, acc.: 89.06%] [G loss: 3.538400]\n",
      "epoch:16 step:12697 [D loss: 0.321154, acc.: 86.72%] [G loss: 3.860994]\n",
      "epoch:16 step:12698 [D loss: 0.369909, acc.: 88.28%] [G loss: 3.446289]\n",
      "epoch:16 step:12699 [D loss: 0.370493, acc.: 82.81%] [G loss: 2.899974]\n",
      "epoch:16 step:12700 [D loss: 0.363065, acc.: 83.59%] [G loss: 3.611727]\n",
      "epoch:16 step:12701 [D loss: 0.445933, acc.: 82.03%] [G loss: 3.483470]\n",
      "epoch:16 step:12702 [D loss: 0.471674, acc.: 79.69%] [G loss: 2.278338]\n",
      "epoch:16 step:12703 [D loss: 0.426720, acc.: 79.69%] [G loss: 3.041324]\n",
      "epoch:16 step:12704 [D loss: 0.449277, acc.: 81.25%] [G loss: 2.996926]\n",
      "epoch:16 step:12705 [D loss: 0.414661, acc.: 78.91%] [G loss: 3.271696]\n",
      "epoch:16 step:12706 [D loss: 0.294620, acc.: 85.94%] [G loss: 5.069228]\n",
      "epoch:16 step:12707 [D loss: 0.361837, acc.: 82.81%] [G loss: 4.058497]\n",
      "epoch:16 step:12708 [D loss: 0.361566, acc.: 83.59%] [G loss: 3.680659]\n",
      "epoch:16 step:12709 [D loss: 0.272341, acc.: 86.72%] [G loss: 4.151127]\n",
      "epoch:16 step:12710 [D loss: 0.299255, acc.: 85.94%] [G loss: 4.988772]\n",
      "epoch:16 step:12711 [D loss: 0.419634, acc.: 84.38%] [G loss: 3.611426]\n",
      "epoch:16 step:12712 [D loss: 0.321178, acc.: 87.50%] [G loss: 2.891088]\n",
      "epoch:16 step:12713 [D loss: 0.321837, acc.: 83.59%] [G loss: 3.428347]\n",
      "epoch:16 step:12714 [D loss: 0.340854, acc.: 84.38%] [G loss: 1.894593]\n",
      "epoch:16 step:12715 [D loss: 0.416116, acc.: 82.81%] [G loss: 4.101671]\n",
      "epoch:16 step:12716 [D loss: 0.415286, acc.: 83.59%] [G loss: 3.162267]\n",
      "epoch:16 step:12717 [D loss: 0.352982, acc.: 81.25%] [G loss: 3.710135]\n",
      "epoch:16 step:12718 [D loss: 0.320455, acc.: 85.16%] [G loss: 2.622003]\n",
      "epoch:16 step:12719 [D loss: 0.388881, acc.: 84.38%] [G loss: 4.003129]\n",
      "epoch:16 step:12720 [D loss: 0.345077, acc.: 82.81%] [G loss: 4.022315]\n",
      "epoch:16 step:12721 [D loss: 0.346743, acc.: 82.81%] [G loss: 3.410086]\n",
      "epoch:16 step:12722 [D loss: 0.242178, acc.: 89.84%] [G loss: 3.520161]\n",
      "epoch:16 step:12723 [D loss: 0.365462, acc.: 85.16%] [G loss: 3.056739]\n",
      "epoch:16 step:12724 [D loss: 0.267442, acc.: 89.06%] [G loss: 3.370785]\n",
      "epoch:16 step:12725 [D loss: 0.334939, acc.: 88.28%] [G loss: 4.480560]\n",
      "epoch:16 step:12726 [D loss: 0.328618, acc.: 85.16%] [G loss: 4.038457]\n",
      "epoch:16 step:12727 [D loss: 0.330131, acc.: 85.94%] [G loss: 4.407667]\n",
      "epoch:16 step:12728 [D loss: 0.216293, acc.: 90.62%] [G loss: 3.070779]\n",
      "epoch:16 step:12729 [D loss: 0.339341, acc.: 83.59%] [G loss: 3.618089]\n",
      "epoch:16 step:12730 [D loss: 0.270945, acc.: 92.19%] [G loss: 4.620046]\n",
      "epoch:16 step:12731 [D loss: 0.239752, acc.: 87.50%] [G loss: 4.739600]\n",
      "epoch:16 step:12732 [D loss: 0.316852, acc.: 83.59%] [G loss: 3.733404]\n",
      "epoch:16 step:12733 [D loss: 0.257507, acc.: 89.84%] [G loss: 4.193305]\n",
      "epoch:16 step:12734 [D loss: 0.293874, acc.: 88.28%] [G loss: 3.196320]\n",
      "epoch:16 step:12735 [D loss: 0.303825, acc.: 87.50%] [G loss: 3.012456]\n",
      "epoch:16 step:12736 [D loss: 0.449939, acc.: 79.69%] [G loss: 2.523283]\n",
      "epoch:16 step:12737 [D loss: 0.398790, acc.: 83.59%] [G loss: 3.362705]\n",
      "epoch:16 step:12738 [D loss: 0.339631, acc.: 85.94%] [G loss: 2.470233]\n",
      "epoch:16 step:12739 [D loss: 0.367848, acc.: 80.47%] [G loss: 2.504415]\n",
      "epoch:16 step:12740 [D loss: 0.337043, acc.: 86.72%] [G loss: 2.554780]\n",
      "epoch:16 step:12741 [D loss: 0.351202, acc.: 82.81%] [G loss: 2.337063]\n",
      "epoch:16 step:12742 [D loss: 0.402402, acc.: 84.38%] [G loss: 3.538675]\n",
      "epoch:16 step:12743 [D loss: 0.445033, acc.: 80.47%] [G loss: 2.715731]\n",
      "epoch:16 step:12744 [D loss: 0.341853, acc.: 84.38%] [G loss: 3.224479]\n",
      "epoch:16 step:12745 [D loss: 0.445472, acc.: 75.78%] [G loss: 3.665312]\n",
      "epoch:16 step:12746 [D loss: 0.511074, acc.: 82.03%] [G loss: 5.100542]\n",
      "epoch:16 step:12747 [D loss: 0.840907, acc.: 71.88%] [G loss: 7.177323]\n",
      "epoch:16 step:12748 [D loss: 0.530665, acc.: 79.69%] [G loss: 5.660195]\n",
      "epoch:16 step:12749 [D loss: 0.341594, acc.: 85.94%] [G loss: 4.710628]\n",
      "epoch:16 step:12750 [D loss: 0.299588, acc.: 89.06%] [G loss: 4.557172]\n",
      "epoch:16 step:12751 [D loss: 0.430370, acc.: 78.91%] [G loss: 4.045307]\n",
      "epoch:16 step:12752 [D loss: 0.278849, acc.: 92.19%] [G loss: 4.064002]\n",
      "epoch:16 step:12753 [D loss: 0.340000, acc.: 82.03%] [G loss: 4.868071]\n",
      "epoch:16 step:12754 [D loss: 0.353837, acc.: 82.03%] [G loss: 4.205560]\n",
      "epoch:16 step:12755 [D loss: 0.494744, acc.: 81.25%] [G loss: 5.680140]\n",
      "epoch:16 step:12756 [D loss: 0.377450, acc.: 82.03%] [G loss: 4.731564]\n",
      "epoch:16 step:12757 [D loss: 0.261688, acc.: 85.94%] [G loss: 3.524651]\n",
      "epoch:16 step:12758 [D loss: 0.454912, acc.: 78.12%] [G loss: 2.979127]\n",
      "epoch:16 step:12759 [D loss: 0.349189, acc.: 83.59%] [G loss: 3.561346]\n",
      "epoch:16 step:12760 [D loss: 0.385438, acc.: 82.81%] [G loss: 3.425354]\n",
      "epoch:16 step:12761 [D loss: 0.371276, acc.: 89.06%] [G loss: 3.317729]\n",
      "epoch:16 step:12762 [D loss: 0.273860, acc.: 88.28%] [G loss: 2.891518]\n",
      "epoch:16 step:12763 [D loss: 0.360638, acc.: 83.59%] [G loss: 3.752376]\n",
      "epoch:16 step:12764 [D loss: 0.261731, acc.: 91.41%] [G loss: 3.215753]\n",
      "epoch:16 step:12765 [D loss: 0.309020, acc.: 86.72%] [G loss: 2.923336]\n",
      "epoch:16 step:12766 [D loss: 0.237646, acc.: 89.84%] [G loss: 3.265403]\n",
      "epoch:16 step:12767 [D loss: 0.261772, acc.: 89.84%] [G loss: 2.798151]\n",
      "epoch:16 step:12768 [D loss: 0.343197, acc.: 89.06%] [G loss: 2.697250]\n",
      "epoch:16 step:12769 [D loss: 0.315354, acc.: 88.28%] [G loss: 3.109552]\n",
      "epoch:16 step:12770 [D loss: 0.400951, acc.: 82.03%] [G loss: 6.003715]\n",
      "epoch:16 step:12771 [D loss: 0.360241, acc.: 82.03%] [G loss: 5.737576]\n",
      "epoch:16 step:12772 [D loss: 0.254044, acc.: 89.84%] [G loss: 3.727320]\n",
      "epoch:16 step:12773 [D loss: 0.374982, acc.: 81.25%] [G loss: 4.962907]\n",
      "epoch:16 step:12774 [D loss: 0.322356, acc.: 82.03%] [G loss: 4.838786]\n",
      "epoch:16 step:12775 [D loss: 0.320356, acc.: 87.50%] [G loss: 2.943909]\n",
      "epoch:16 step:12776 [D loss: 0.226377, acc.: 90.62%] [G loss: 2.685544]\n",
      "epoch:16 step:12777 [D loss: 0.318829, acc.: 84.38%] [G loss: 3.709141]\n",
      "epoch:16 step:12778 [D loss: 0.321360, acc.: 87.50%] [G loss: 3.535731]\n",
      "epoch:16 step:12779 [D loss: 0.314418, acc.: 85.94%] [G loss: 5.062762]\n",
      "epoch:16 step:12780 [D loss: 0.309233, acc.: 85.16%] [G loss: 2.985199]\n",
      "epoch:16 step:12781 [D loss: 0.359741, acc.: 82.03%] [G loss: 2.585929]\n",
      "epoch:16 step:12782 [D loss: 0.369614, acc.: 83.59%] [G loss: 3.644450]\n",
      "epoch:16 step:12783 [D loss: 0.241894, acc.: 88.28%] [G loss: 5.095641]\n",
      "epoch:16 step:12784 [D loss: 0.302492, acc.: 85.16%] [G loss: 3.192123]\n",
      "epoch:16 step:12785 [D loss: 0.290961, acc.: 88.28%] [G loss: 2.966390]\n",
      "epoch:16 step:12786 [D loss: 0.316825, acc.: 84.38%] [G loss: 2.791657]\n",
      "epoch:16 step:12787 [D loss: 0.303688, acc.: 82.81%] [G loss: 4.227524]\n",
      "epoch:16 step:12788 [D loss: 0.216547, acc.: 93.75%] [G loss: 5.417955]\n",
      "epoch:16 step:12789 [D loss: 0.321416, acc.: 85.94%] [G loss: 3.241827]\n",
      "epoch:16 step:12790 [D loss: 0.301857, acc.: 84.38%] [G loss: 2.891336]\n",
      "epoch:16 step:12791 [D loss: 0.382305, acc.: 85.94%] [G loss: 3.862634]\n",
      "epoch:16 step:12792 [D loss: 0.312273, acc.: 86.72%] [G loss: 6.008445]\n",
      "epoch:16 step:12793 [D loss: 0.203272, acc.: 92.19%] [G loss: 6.534925]\n",
      "epoch:16 step:12794 [D loss: 0.210237, acc.: 89.84%] [G loss: 6.849679]\n",
      "epoch:16 step:12795 [D loss: 0.141742, acc.: 94.53%] [G loss: 5.707192]\n",
      "epoch:16 step:12796 [D loss: 0.397957, acc.: 80.47%] [G loss: 4.201754]\n",
      "epoch:16 step:12797 [D loss: 0.232816, acc.: 91.41%] [G loss: 6.174795]\n",
      "epoch:16 step:12798 [D loss: 0.272598, acc.: 87.50%] [G loss: 3.621702]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:16 step:12799 [D loss: 0.289458, acc.: 88.28%] [G loss: 3.701850]\n",
      "epoch:16 step:12800 [D loss: 0.307381, acc.: 85.16%] [G loss: 3.247148]\n",
      "##############\n",
      "[0.84629063 0.87490651 0.80408381 0.79131701 0.78795439 0.8118351\n",
      " 0.88810912 0.83800008 0.8288335  0.81456733]\n",
      "##########\n",
      "epoch:16 step:12801 [D loss: 0.417928, acc.: 80.47%] [G loss: 4.003293]\n",
      "epoch:16 step:12802 [D loss: 0.220244, acc.: 89.84%] [G loss: 5.652028]\n",
      "epoch:16 step:12803 [D loss: 0.257588, acc.: 91.41%] [G loss: 3.072287]\n",
      "epoch:16 step:12804 [D loss: 0.275253, acc.: 89.06%] [G loss: 4.897972]\n",
      "epoch:16 step:12805 [D loss: 0.279047, acc.: 88.28%] [G loss: 3.561515]\n",
      "epoch:16 step:12806 [D loss: 0.306369, acc.: 82.81%] [G loss: 3.731141]\n",
      "epoch:16 step:12807 [D loss: 0.421674, acc.: 80.47%] [G loss: 5.453537]\n",
      "epoch:16 step:12808 [D loss: 0.528192, acc.: 75.00%] [G loss: 2.833669]\n",
      "epoch:16 step:12809 [D loss: 0.328660, acc.: 88.28%] [G loss: 3.658724]\n",
      "epoch:16 step:12810 [D loss: 0.424520, acc.: 82.03%] [G loss: 2.413325]\n",
      "epoch:16 step:12811 [D loss: 0.336025, acc.: 85.94%] [G loss: 3.483319]\n",
      "epoch:16 step:12812 [D loss: 0.325935, acc.: 85.94%] [G loss: 2.955471]\n",
      "epoch:16 step:12813 [D loss: 0.325450, acc.: 87.50%] [G loss: 2.944462]\n",
      "epoch:16 step:12814 [D loss: 0.301982, acc.: 89.06%] [G loss: 2.897934]\n",
      "epoch:16 step:12815 [D loss: 0.293313, acc.: 85.16%] [G loss: 2.934313]\n",
      "epoch:16 step:12816 [D loss: 0.309292, acc.: 85.16%] [G loss: 3.360581]\n",
      "epoch:16 step:12817 [D loss: 0.321313, acc.: 86.72%] [G loss: 2.889575]\n",
      "epoch:16 step:12818 [D loss: 0.343308, acc.: 82.03%] [G loss: 3.387570]\n",
      "epoch:16 step:12819 [D loss: 0.388747, acc.: 83.59%] [G loss: 3.061619]\n",
      "epoch:16 step:12820 [D loss: 0.398126, acc.: 82.81%] [G loss: 3.378185]\n",
      "epoch:16 step:12821 [D loss: 0.299031, acc.: 85.16%] [G loss: 3.477398]\n",
      "epoch:16 step:12822 [D loss: 0.312267, acc.: 89.06%] [G loss: 3.327135]\n",
      "epoch:16 step:12823 [D loss: 0.304924, acc.: 86.72%] [G loss: 2.633841]\n",
      "epoch:16 step:12824 [D loss: 0.284079, acc.: 90.62%] [G loss: 2.723102]\n",
      "epoch:16 step:12825 [D loss: 0.233120, acc.: 90.62%] [G loss: 4.337292]\n",
      "epoch:16 step:12826 [D loss: 0.269795, acc.: 89.06%] [G loss: 3.299713]\n",
      "epoch:16 step:12827 [D loss: 0.311126, acc.: 85.94%] [G loss: 4.001817]\n",
      "epoch:16 step:12828 [D loss: 0.300675, acc.: 88.28%] [G loss: 2.593089]\n",
      "epoch:16 step:12829 [D loss: 0.266839, acc.: 90.62%] [G loss: 3.968706]\n",
      "epoch:16 step:12830 [D loss: 0.291429, acc.: 86.72%] [G loss: 5.000228]\n",
      "epoch:16 step:12831 [D loss: 0.346034, acc.: 86.72%] [G loss: 3.236711]\n",
      "epoch:16 step:12832 [D loss: 0.284561, acc.: 85.94%] [G loss: 3.189366]\n",
      "epoch:16 step:12833 [D loss: 0.282907, acc.: 89.06%] [G loss: 3.289924]\n",
      "epoch:16 step:12834 [D loss: 0.327713, acc.: 85.16%] [G loss: 3.434213]\n",
      "epoch:16 step:12835 [D loss: 0.214776, acc.: 92.19%] [G loss: 3.551551]\n",
      "epoch:16 step:12836 [D loss: 0.357274, acc.: 82.03%] [G loss: 4.395885]\n",
      "epoch:16 step:12837 [D loss: 0.284489, acc.: 89.06%] [G loss: 3.959596]\n",
      "epoch:16 step:12838 [D loss: 0.241692, acc.: 93.75%] [G loss: 2.964679]\n",
      "epoch:16 step:12839 [D loss: 0.369181, acc.: 87.50%] [G loss: 3.004069]\n",
      "epoch:16 step:12840 [D loss: 0.266541, acc.: 89.06%] [G loss: 3.111961]\n",
      "epoch:16 step:12841 [D loss: 0.237468, acc.: 88.28%] [G loss: 4.044868]\n",
      "epoch:16 step:12842 [D loss: 0.384089, acc.: 82.03%] [G loss: 4.211024]\n",
      "epoch:16 step:12843 [D loss: 0.346921, acc.: 84.38%] [G loss: 2.923324]\n",
      "epoch:16 step:12844 [D loss: 0.286990, acc.: 85.94%] [G loss: 5.741472]\n",
      "epoch:16 step:12845 [D loss: 0.352783, acc.: 85.16%] [G loss: 3.038272]\n",
      "epoch:16 step:12846 [D loss: 0.331047, acc.: 88.28%] [G loss: 3.495500]\n",
      "epoch:16 step:12847 [D loss: 0.341067, acc.: 84.38%] [G loss: 2.431922]\n",
      "epoch:16 step:12848 [D loss: 0.448345, acc.: 77.34%] [G loss: 2.758097]\n",
      "epoch:16 step:12849 [D loss: 0.394500, acc.: 85.16%] [G loss: 3.576765]\n",
      "epoch:16 step:12850 [D loss: 0.244326, acc.: 87.50%] [G loss: 5.683384]\n",
      "epoch:16 step:12851 [D loss: 0.171654, acc.: 95.31%] [G loss: 3.789499]\n",
      "epoch:16 step:12852 [D loss: 0.312181, acc.: 86.72%] [G loss: 2.761046]\n",
      "epoch:16 step:12853 [D loss: 0.367043, acc.: 85.16%] [G loss: 3.322950]\n",
      "epoch:16 step:12854 [D loss: 0.390922, acc.: 83.59%] [G loss: 2.234360]\n",
      "epoch:16 step:12855 [D loss: 0.442877, acc.: 82.03%] [G loss: 3.321288]\n",
      "epoch:16 step:12856 [D loss: 0.452600, acc.: 79.69%] [G loss: 2.204638]\n",
      "epoch:16 step:12857 [D loss: 0.212064, acc.: 92.97%] [G loss: 2.566950]\n",
      "epoch:16 step:12858 [D loss: 0.279372, acc.: 89.84%] [G loss: 3.654679]\n",
      "epoch:16 step:12859 [D loss: 0.266256, acc.: 87.50%] [G loss: 3.542882]\n",
      "epoch:16 step:12860 [D loss: 0.325381, acc.: 85.16%] [G loss: 2.725009]\n",
      "epoch:16 step:12861 [D loss: 0.313836, acc.: 85.16%] [G loss: 2.798444]\n",
      "epoch:16 step:12862 [D loss: 0.291140, acc.: 88.28%] [G loss: 3.036817]\n",
      "epoch:16 step:12863 [D loss: 0.277841, acc.: 89.06%] [G loss: 3.934722]\n",
      "epoch:16 step:12864 [D loss: 0.336518, acc.: 82.81%] [G loss: 4.795810]\n",
      "epoch:16 step:12865 [D loss: 0.535802, acc.: 75.78%] [G loss: 6.332379]\n",
      "epoch:16 step:12866 [D loss: 0.558548, acc.: 79.69%] [G loss: 4.437070]\n",
      "epoch:16 step:12867 [D loss: 0.255525, acc.: 92.19%] [G loss: 4.020443]\n",
      "epoch:16 step:12868 [D loss: 0.352280, acc.: 82.03%] [G loss: 5.149009]\n",
      "epoch:16 step:12869 [D loss: 0.236927, acc.: 90.62%] [G loss: 3.852211]\n",
      "epoch:16 step:12870 [D loss: 0.463379, acc.: 77.34%] [G loss: 5.467643]\n",
      "epoch:16 step:12871 [D loss: 0.245573, acc.: 91.41%] [G loss: 3.408760]\n",
      "epoch:16 step:12872 [D loss: 0.308365, acc.: 87.50%] [G loss: 3.531836]\n",
      "epoch:16 step:12873 [D loss: 0.204409, acc.: 92.19%] [G loss: 3.662430]\n",
      "epoch:16 step:12874 [D loss: 0.336189, acc.: 84.38%] [G loss: 3.122607]\n",
      "epoch:16 step:12875 [D loss: 0.236957, acc.: 89.84%] [G loss: 3.302867]\n",
      "epoch:16 step:12876 [D loss: 0.357329, acc.: 86.72%] [G loss: 3.189465]\n",
      "epoch:16 step:12877 [D loss: 0.231803, acc.: 92.97%] [G loss: 3.404716]\n",
      "epoch:16 step:12878 [D loss: 0.393865, acc.: 82.03%] [G loss: 3.845533]\n",
      "epoch:16 step:12879 [D loss: 0.249127, acc.: 89.84%] [G loss: 3.908651]\n",
      "epoch:16 step:12880 [D loss: 0.374759, acc.: 84.38%] [G loss: 2.733192]\n",
      "epoch:16 step:12881 [D loss: 0.261571, acc.: 91.41%] [G loss: 2.438514]\n",
      "epoch:16 step:12882 [D loss: 0.347960, acc.: 85.16%] [G loss: 2.483203]\n",
      "epoch:16 step:12883 [D loss: 0.430011, acc.: 82.03%] [G loss: 2.801770]\n",
      "epoch:16 step:12884 [D loss: 0.429933, acc.: 78.91%] [G loss: 2.306878]\n",
      "epoch:16 step:12885 [D loss: 0.376190, acc.: 83.59%] [G loss: 2.157874]\n",
      "epoch:16 step:12886 [D loss: 0.382068, acc.: 85.94%] [G loss: 2.676373]\n",
      "epoch:16 step:12887 [D loss: 0.418277, acc.: 76.56%] [G loss: 3.036081]\n",
      "epoch:16 step:12888 [D loss: 0.314313, acc.: 87.50%] [G loss: 4.761337]\n",
      "epoch:16 step:12889 [D loss: 0.311601, acc.: 86.72%] [G loss: 4.202373]\n",
      "epoch:16 step:12890 [D loss: 0.226267, acc.: 87.50%] [G loss: 4.619447]\n",
      "epoch:16 step:12891 [D loss: 0.352050, acc.: 82.81%] [G loss: 2.403620]\n",
      "epoch:16 step:12892 [D loss: 0.324882, acc.: 83.59%] [G loss: 2.659428]\n",
      "epoch:16 step:12893 [D loss: 0.317479, acc.: 88.28%] [G loss: 3.516317]\n",
      "epoch:16 step:12894 [D loss: 0.396569, acc.: 82.03%] [G loss: 3.494042]\n",
      "epoch:16 step:12895 [D loss: 0.340829, acc.: 82.81%] [G loss: 3.031865]\n",
      "epoch:16 step:12896 [D loss: 0.244427, acc.: 92.19%] [G loss: 5.278075]\n",
      "epoch:16 step:12897 [D loss: 0.304075, acc.: 86.72%] [G loss: 3.737823]\n",
      "epoch:16 step:12898 [D loss: 0.356715, acc.: 85.94%] [G loss: 3.288968]\n",
      "epoch:16 step:12899 [D loss: 0.332252, acc.: 87.50%] [G loss: 2.704406]\n",
      "epoch:16 step:12900 [D loss: 0.291236, acc.: 88.28%] [G loss: 3.487187]\n",
      "epoch:16 step:12901 [D loss: 0.337492, acc.: 85.94%] [G loss: 2.964435]\n",
      "epoch:16 step:12902 [D loss: 0.275574, acc.: 89.84%] [G loss: 3.895239]\n",
      "epoch:16 step:12903 [D loss: 0.305944, acc.: 88.28%] [G loss: 3.990640]\n",
      "epoch:16 step:12904 [D loss: 0.378179, acc.: 85.94%] [G loss: 3.561021]\n",
      "epoch:16 step:12905 [D loss: 0.353513, acc.: 84.38%] [G loss: 3.411088]\n",
      "epoch:16 step:12906 [D loss: 0.420376, acc.: 78.91%] [G loss: 3.539344]\n",
      "epoch:16 step:12907 [D loss: 0.714353, acc.: 71.09%] [G loss: 5.260765]\n",
      "epoch:16 step:12908 [D loss: 0.891332, acc.: 68.75%] [G loss: 6.056985]\n",
      "epoch:16 step:12909 [D loss: 1.114444, acc.: 61.72%] [G loss: 2.276170]\n",
      "epoch:16 step:12910 [D loss: 0.443155, acc.: 81.25%] [G loss: 3.662779]\n",
      "epoch:16 step:12911 [D loss: 0.449120, acc.: 82.81%] [G loss: 3.470730]\n",
      "epoch:16 step:12912 [D loss: 0.299449, acc.: 89.84%] [G loss: 3.885317]\n",
      "epoch:16 step:12913 [D loss: 0.387401, acc.: 85.16%] [G loss: 3.359771]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:16 step:12914 [D loss: 0.416731, acc.: 79.69%] [G loss: 3.631128]\n",
      "epoch:16 step:12915 [D loss: 0.432124, acc.: 84.38%] [G loss: 2.938112]\n",
      "epoch:16 step:12916 [D loss: 0.299461, acc.: 88.28%] [G loss: 3.145546]\n",
      "epoch:16 step:12917 [D loss: 0.276425, acc.: 87.50%] [G loss: 3.989362]\n",
      "epoch:16 step:12918 [D loss: 0.291720, acc.: 87.50%] [G loss: 3.176723]\n",
      "epoch:16 step:12919 [D loss: 0.281845, acc.: 87.50%] [G loss: 2.783895]\n",
      "epoch:16 step:12920 [D loss: 0.303915, acc.: 86.72%] [G loss: 2.717846]\n",
      "epoch:16 step:12921 [D loss: 0.384170, acc.: 82.03%] [G loss: 3.624033]\n",
      "epoch:16 step:12922 [D loss: 0.408293, acc.: 81.25%] [G loss: 3.212339]\n",
      "epoch:16 step:12923 [D loss: 0.275859, acc.: 88.28%] [G loss: 2.821527]\n",
      "epoch:16 step:12924 [D loss: 0.280819, acc.: 89.06%] [G loss: 4.143696]\n",
      "epoch:16 step:12925 [D loss: 0.240233, acc.: 89.06%] [G loss: 5.554505]\n",
      "epoch:16 step:12926 [D loss: 0.326064, acc.: 87.50%] [G loss: 2.569700]\n",
      "epoch:16 step:12927 [D loss: 0.320009, acc.: 83.59%] [G loss: 4.027589]\n",
      "epoch:16 step:12928 [D loss: 0.351097, acc.: 78.91%] [G loss: 5.689552]\n",
      "epoch:16 step:12929 [D loss: 0.344877, acc.: 81.25%] [G loss: 2.499281]\n",
      "epoch:16 step:12930 [D loss: 0.382266, acc.: 82.03%] [G loss: 4.125733]\n",
      "epoch:16 step:12931 [D loss: 0.362807, acc.: 82.81%] [G loss: 3.624738]\n",
      "epoch:16 step:12932 [D loss: 0.329769, acc.: 85.16%] [G loss: 2.845558]\n",
      "epoch:16 step:12933 [D loss: 0.268614, acc.: 90.62%] [G loss: 2.004833]\n",
      "epoch:16 step:12934 [D loss: 0.441268, acc.: 82.03%] [G loss: 2.209236]\n",
      "epoch:16 step:12935 [D loss: 0.325558, acc.: 86.72%] [G loss: 2.864712]\n",
      "epoch:16 step:12936 [D loss: 0.331426, acc.: 85.94%] [G loss: 2.613318]\n",
      "epoch:16 step:12937 [D loss: 0.357135, acc.: 84.38%] [G loss: 2.241680]\n",
      "epoch:16 step:12938 [D loss: 0.282829, acc.: 89.84%] [G loss: 2.664941]\n",
      "epoch:16 step:12939 [D loss: 0.301107, acc.: 85.16%] [G loss: 4.077662]\n",
      "epoch:16 step:12940 [D loss: 0.261450, acc.: 90.62%] [G loss: 4.919325]\n",
      "epoch:16 step:12941 [D loss: 0.252282, acc.: 88.28%] [G loss: 3.892240]\n",
      "epoch:16 step:12942 [D loss: 0.230661, acc.: 90.62%] [G loss: 3.115963]\n",
      "epoch:16 step:12943 [D loss: 0.292893, acc.: 86.72%] [G loss: 3.597998]\n",
      "epoch:16 step:12944 [D loss: 0.265296, acc.: 89.84%] [G loss: 4.385755]\n",
      "epoch:16 step:12945 [D loss: 0.266741, acc.: 88.28%] [G loss: 3.019614]\n",
      "epoch:16 step:12946 [D loss: 0.439340, acc.: 78.91%] [G loss: 2.969894]\n",
      "epoch:16 step:12947 [D loss: 0.437270, acc.: 76.56%] [G loss: 3.080275]\n",
      "epoch:16 step:12948 [D loss: 0.334830, acc.: 82.81%] [G loss: 3.258530]\n",
      "epoch:16 step:12949 [D loss: 0.390194, acc.: 84.38%] [G loss: 2.345195]\n",
      "epoch:16 step:12950 [D loss: 0.228938, acc.: 91.41%] [G loss: 4.453641]\n",
      "epoch:16 step:12951 [D loss: 0.324420, acc.: 86.72%] [G loss: 4.021292]\n",
      "epoch:16 step:12952 [D loss: 0.357598, acc.: 86.72%] [G loss: 3.092087]\n",
      "epoch:16 step:12953 [D loss: 0.357611, acc.: 84.38%] [G loss: 3.311543]\n",
      "epoch:16 step:12954 [D loss: 0.178164, acc.: 93.75%] [G loss: 5.285117]\n",
      "epoch:16 step:12955 [D loss: 0.248455, acc.: 86.72%] [G loss: 3.903793]\n",
      "epoch:16 step:12956 [D loss: 0.264565, acc.: 85.94%] [G loss: 3.407676]\n",
      "epoch:16 step:12957 [D loss: 0.351676, acc.: 79.69%] [G loss: 3.407209]\n",
      "epoch:16 step:12958 [D loss: 0.224641, acc.: 92.19%] [G loss: 2.908390]\n",
      "epoch:16 step:12959 [D loss: 0.421745, acc.: 83.59%] [G loss: 3.716912]\n",
      "epoch:16 step:12960 [D loss: 0.411219, acc.: 83.59%] [G loss: 2.846479]\n",
      "epoch:16 step:12961 [D loss: 0.240027, acc.: 90.62%] [G loss: 4.670807]\n",
      "epoch:16 step:12962 [D loss: 0.321592, acc.: 85.16%] [G loss: 4.114211]\n",
      "epoch:16 step:12963 [D loss: 0.348014, acc.: 82.81%] [G loss: 3.411923]\n",
      "epoch:16 step:12964 [D loss: 0.342249, acc.: 82.81%] [G loss: 3.902228]\n",
      "epoch:16 step:12965 [D loss: 0.309419, acc.: 86.72%] [G loss: 2.838932]\n",
      "epoch:16 step:12966 [D loss: 0.262764, acc.: 90.62%] [G loss: 2.588725]\n",
      "epoch:16 step:12967 [D loss: 0.286281, acc.: 89.84%] [G loss: 4.243201]\n",
      "epoch:16 step:12968 [D loss: 0.298750, acc.: 87.50%] [G loss: 3.378973]\n",
      "epoch:16 step:12969 [D loss: 0.323580, acc.: 89.84%] [G loss: 3.795788]\n",
      "epoch:16 step:12970 [D loss: 0.397526, acc.: 79.69%] [G loss: 5.247931]\n",
      "epoch:16 step:12971 [D loss: 0.562271, acc.: 71.88%] [G loss: 4.192506]\n",
      "epoch:16 step:12972 [D loss: 0.266161, acc.: 86.72%] [G loss: 5.233974]\n",
      "epoch:16 step:12973 [D loss: 0.234695, acc.: 89.84%] [G loss: 3.693830]\n",
      "epoch:16 step:12974 [D loss: 0.195467, acc.: 92.19%] [G loss: 6.010827]\n",
      "epoch:16 step:12975 [D loss: 0.277052, acc.: 89.84%] [G loss: 4.671646]\n",
      "epoch:16 step:12976 [D loss: 0.315152, acc.: 83.59%] [G loss: 4.065966]\n",
      "epoch:16 step:12977 [D loss: 0.222109, acc.: 93.75%] [G loss: 4.152954]\n",
      "epoch:16 step:12978 [D loss: 0.319781, acc.: 86.72%] [G loss: 3.095682]\n",
      "epoch:16 step:12979 [D loss: 0.271053, acc.: 89.84%] [G loss: 2.925436]\n",
      "epoch:16 step:12980 [D loss: 0.295450, acc.: 84.38%] [G loss: 4.485355]\n",
      "epoch:16 step:12981 [D loss: 0.398657, acc.: 79.69%] [G loss: 2.630732]\n",
      "epoch:16 step:12982 [D loss: 0.252446, acc.: 91.41%] [G loss: 3.908282]\n",
      "epoch:16 step:12983 [D loss: 0.257680, acc.: 85.94%] [G loss: 2.873729]\n",
      "epoch:16 step:12984 [D loss: 0.400993, acc.: 80.47%] [G loss: 3.425162]\n",
      "epoch:16 step:12985 [D loss: 0.471848, acc.: 82.81%] [G loss: 2.970795]\n",
      "epoch:16 step:12986 [D loss: 0.290537, acc.: 88.28%] [G loss: 3.351046]\n",
      "epoch:16 step:12987 [D loss: 0.342184, acc.: 85.94%] [G loss: 3.605106]\n",
      "epoch:16 step:12988 [D loss: 0.356618, acc.: 84.38%] [G loss: 3.965099]\n",
      "epoch:16 step:12989 [D loss: 0.278786, acc.: 87.50%] [G loss: 3.519468]\n",
      "epoch:16 step:12990 [D loss: 0.378695, acc.: 84.38%] [G loss: 3.272096]\n",
      "epoch:16 step:12991 [D loss: 0.330696, acc.: 86.72%] [G loss: 3.510818]\n",
      "epoch:16 step:12992 [D loss: 0.224834, acc.: 87.50%] [G loss: 3.824992]\n",
      "epoch:16 step:12993 [D loss: 0.405665, acc.: 79.69%] [G loss: 3.487586]\n",
      "epoch:16 step:12994 [D loss: 0.281226, acc.: 88.28%] [G loss: 3.030104]\n",
      "epoch:16 step:12995 [D loss: 0.454711, acc.: 77.34%] [G loss: 3.636082]\n",
      "epoch:16 step:12996 [D loss: 0.368525, acc.: 85.16%] [G loss: 4.014895]\n",
      "epoch:16 step:12997 [D loss: 0.408143, acc.: 79.69%] [G loss: 4.354856]\n",
      "epoch:16 step:12998 [D loss: 0.468212, acc.: 75.78%] [G loss: 4.656387]\n",
      "epoch:16 step:12999 [D loss: 0.390863, acc.: 79.69%] [G loss: 3.997845]\n",
      "epoch:16 step:13000 [D loss: 0.282664, acc.: 85.16%] [G loss: 4.596675]\n",
      "##############\n",
      "[0.84228355 0.86243479 0.81413626 0.80249431 0.7526576  0.8363545\n",
      " 0.86911509 0.85008689 0.81975541 0.80754971]\n",
      "##########\n",
      "epoch:16 step:13001 [D loss: 0.293690, acc.: 88.28%] [G loss: 3.032690]\n",
      "epoch:16 step:13002 [D loss: 0.394702, acc.: 82.03%] [G loss: 2.072806]\n",
      "epoch:16 step:13003 [D loss: 0.364643, acc.: 83.59%] [G loss: 2.892385]\n",
      "epoch:16 step:13004 [D loss: 0.266198, acc.: 90.62%] [G loss: 3.081047]\n",
      "epoch:16 step:13005 [D loss: 0.362848, acc.: 85.16%] [G loss: 3.801228]\n",
      "epoch:16 step:13006 [D loss: 0.285774, acc.: 84.38%] [G loss: 7.075658]\n",
      "epoch:16 step:13007 [D loss: 0.336855, acc.: 85.16%] [G loss: 4.189755]\n",
      "epoch:16 step:13008 [D loss: 0.168527, acc.: 90.62%] [G loss: 7.221394]\n",
      "epoch:16 step:13009 [D loss: 0.283642, acc.: 89.06%] [G loss: 3.060683]\n",
      "epoch:16 step:13010 [D loss: 0.312730, acc.: 89.06%] [G loss: 5.159391]\n",
      "epoch:16 step:13011 [D loss: 0.376708, acc.: 85.16%] [G loss: 2.588867]\n",
      "epoch:16 step:13012 [D loss: 0.281200, acc.: 89.84%] [G loss: 3.040366]\n",
      "epoch:16 step:13013 [D loss: 0.344974, acc.: 86.72%] [G loss: 3.383051]\n",
      "epoch:16 step:13014 [D loss: 0.296021, acc.: 88.28%] [G loss: 4.436607]\n",
      "epoch:16 step:13015 [D loss: 0.361457, acc.: 85.16%] [G loss: 4.229249]\n",
      "epoch:16 step:13016 [D loss: 0.511911, acc.: 73.44%] [G loss: 3.242704]\n",
      "epoch:16 step:13017 [D loss: 0.350681, acc.: 85.94%] [G loss: 4.100293]\n",
      "epoch:16 step:13018 [D loss: 0.362558, acc.: 83.59%] [G loss: 4.093587]\n",
      "epoch:16 step:13019 [D loss: 0.293038, acc.: 85.94%] [G loss: 3.020676]\n",
      "epoch:16 step:13020 [D loss: 0.297068, acc.: 84.38%] [G loss: 5.037325]\n",
      "epoch:16 step:13021 [D loss: 0.335110, acc.: 82.81%] [G loss: 3.396237]\n",
      "epoch:16 step:13022 [D loss: 0.357546, acc.: 85.16%] [G loss: 5.110134]\n",
      "epoch:16 step:13023 [D loss: 0.385500, acc.: 81.25%] [G loss: 3.012156]\n",
      "epoch:16 step:13024 [D loss: 0.261839, acc.: 89.84%] [G loss: 3.064596]\n",
      "epoch:16 step:13025 [D loss: 0.355309, acc.: 83.59%] [G loss: 3.598369]\n",
      "epoch:16 step:13026 [D loss: 0.395218, acc.: 80.47%] [G loss: 3.343402]\n",
      "epoch:16 step:13027 [D loss: 0.305908, acc.: 87.50%] [G loss: 3.022147]\n",
      "epoch:16 step:13028 [D loss: 0.434306, acc.: 80.47%] [G loss: 3.762708]\n",
      "epoch:16 step:13029 [D loss: 0.344007, acc.: 83.59%] [G loss: 3.517982]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:16 step:13030 [D loss: 0.269787, acc.: 89.06%] [G loss: 3.101716]\n",
      "epoch:16 step:13031 [D loss: 0.347978, acc.: 83.59%] [G loss: 3.040930]\n",
      "epoch:16 step:13032 [D loss: 0.276996, acc.: 88.28%] [G loss: 3.037142]\n",
      "epoch:16 step:13033 [D loss: 0.392798, acc.: 81.25%] [G loss: 3.931070]\n",
      "epoch:16 step:13034 [D loss: 0.312456, acc.: 85.16%] [G loss: 3.705853]\n",
      "epoch:16 step:13035 [D loss: 0.325636, acc.: 86.72%] [G loss: 4.078388]\n",
      "epoch:16 step:13036 [D loss: 0.245886, acc.: 89.84%] [G loss: 3.990057]\n",
      "epoch:16 step:13037 [D loss: 0.299536, acc.: 88.28%] [G loss: 4.183785]\n",
      "epoch:16 step:13038 [D loss: 0.267525, acc.: 87.50%] [G loss: 4.550239]\n",
      "epoch:16 step:13039 [D loss: 0.254340, acc.: 92.19%] [G loss: 3.498898]\n",
      "epoch:16 step:13040 [D loss: 0.360602, acc.: 86.72%] [G loss: 3.629197]\n",
      "epoch:16 step:13041 [D loss: 0.262360, acc.: 91.41%] [G loss: 3.535506]\n",
      "epoch:16 step:13042 [D loss: 0.251544, acc.: 89.06%] [G loss: 4.137038]\n",
      "epoch:16 step:13043 [D loss: 0.263298, acc.: 89.06%] [G loss: 4.073060]\n",
      "epoch:16 step:13044 [D loss: 0.236796, acc.: 91.41%] [G loss: 2.997613]\n",
      "epoch:16 step:13045 [D loss: 0.378181, acc.: 81.25%] [G loss: 3.898342]\n",
      "epoch:16 step:13046 [D loss: 0.345846, acc.: 86.72%] [G loss: 5.032442]\n",
      "epoch:16 step:13047 [D loss: 0.315781, acc.: 85.94%] [G loss: 3.112804]\n",
      "epoch:16 step:13048 [D loss: 0.342902, acc.: 85.94%] [G loss: 4.082905]\n",
      "epoch:16 step:13049 [D loss: 0.412354, acc.: 83.59%] [G loss: 3.157281]\n",
      "epoch:16 step:13050 [D loss: 0.369923, acc.: 84.38%] [G loss: 2.961216]\n",
      "epoch:16 step:13051 [D loss: 0.346841, acc.: 83.59%] [G loss: 4.196106]\n",
      "epoch:16 step:13052 [D loss: 0.241769, acc.: 89.06%] [G loss: 4.429548]\n",
      "epoch:16 step:13053 [D loss: 0.380675, acc.: 85.16%] [G loss: 3.005557]\n",
      "epoch:16 step:13054 [D loss: 0.301641, acc.: 85.94%] [G loss: 2.726732]\n",
      "epoch:16 step:13055 [D loss: 0.261118, acc.: 91.41%] [G loss: 4.085481]\n",
      "epoch:16 step:13056 [D loss: 0.277579, acc.: 86.72%] [G loss: 2.345382]\n",
      "epoch:16 step:13057 [D loss: 0.260580, acc.: 87.50%] [G loss: 4.821559]\n",
      "epoch:16 step:13058 [D loss: 0.376372, acc.: 83.59%] [G loss: 3.224179]\n",
      "epoch:16 step:13059 [D loss: 0.283933, acc.: 90.62%] [G loss: 3.302398]\n",
      "epoch:16 step:13060 [D loss: 0.248979, acc.: 89.06%] [G loss: 3.388342]\n",
      "epoch:16 step:13061 [D loss: 0.355905, acc.: 89.06%] [G loss: 3.094986]\n",
      "epoch:16 step:13062 [D loss: 0.264814, acc.: 88.28%] [G loss: 2.791520]\n",
      "epoch:16 step:13063 [D loss: 0.348003, acc.: 87.50%] [G loss: 4.258353]\n",
      "epoch:16 step:13064 [D loss: 0.390293, acc.: 82.03%] [G loss: 5.328802]\n",
      "epoch:16 step:13065 [D loss: 0.397110, acc.: 83.59%] [G loss: 3.166979]\n",
      "epoch:16 step:13066 [D loss: 0.330920, acc.: 85.94%] [G loss: 3.982728]\n",
      "epoch:16 step:13067 [D loss: 0.275820, acc.: 88.28%] [G loss: 2.770761]\n",
      "epoch:16 step:13068 [D loss: 0.372721, acc.: 87.50%] [G loss: 3.301879]\n",
      "epoch:16 step:13069 [D loss: 0.316321, acc.: 88.28%] [G loss: 3.444719]\n",
      "epoch:16 step:13070 [D loss: 0.302128, acc.: 84.38%] [G loss: 3.220991]\n",
      "epoch:16 step:13071 [D loss: 0.356919, acc.: 83.59%] [G loss: 3.599395]\n",
      "epoch:16 step:13072 [D loss: 0.372658, acc.: 82.81%] [G loss: 3.440827]\n",
      "epoch:16 step:13073 [D loss: 0.209234, acc.: 92.97%] [G loss: 4.760272]\n",
      "epoch:16 step:13074 [D loss: 0.264209, acc.: 87.50%] [G loss: 4.258377]\n",
      "epoch:16 step:13075 [D loss: 0.318294, acc.: 85.94%] [G loss: 2.391547]\n",
      "epoch:16 step:13076 [D loss: 0.274695, acc.: 88.28%] [G loss: 3.706757]\n",
      "epoch:16 step:13077 [D loss: 0.335161, acc.: 85.94%] [G loss: 2.764076]\n",
      "epoch:16 step:13078 [D loss: 0.243504, acc.: 92.19%] [G loss: 3.549688]\n",
      "epoch:16 step:13079 [D loss: 0.412458, acc.: 81.25%] [G loss: 3.170608]\n",
      "epoch:16 step:13080 [D loss: 0.312744, acc.: 85.94%] [G loss: 2.408868]\n",
      "epoch:16 step:13081 [D loss: 0.345696, acc.: 85.94%] [G loss: 3.374619]\n",
      "epoch:16 step:13082 [D loss: 0.258423, acc.: 93.75%] [G loss: 3.672733]\n",
      "epoch:16 step:13083 [D loss: 0.301402, acc.: 88.28%] [G loss: 3.390651]\n",
      "epoch:16 step:13084 [D loss: 0.219753, acc.: 89.84%] [G loss: 5.656291]\n",
      "epoch:16 step:13085 [D loss: 0.294558, acc.: 87.50%] [G loss: 3.351367]\n",
      "epoch:16 step:13086 [D loss: 0.346939, acc.: 85.94%] [G loss: 2.431739]\n",
      "epoch:16 step:13087 [D loss: 0.331308, acc.: 87.50%] [G loss: 2.929765]\n",
      "epoch:16 step:13088 [D loss: 0.362210, acc.: 80.47%] [G loss: 2.812907]\n",
      "epoch:16 step:13089 [D loss: 0.211054, acc.: 91.41%] [G loss: 3.018740]\n",
      "epoch:16 step:13090 [D loss: 0.293322, acc.: 89.06%] [G loss: 4.132817]\n",
      "epoch:16 step:13091 [D loss: 0.241868, acc.: 90.62%] [G loss: 4.473495]\n",
      "epoch:16 step:13092 [D loss: 0.341882, acc.: 86.72%] [G loss: 2.222513]\n",
      "epoch:16 step:13093 [D loss: 0.204011, acc.: 93.75%] [G loss: 2.573855]\n",
      "epoch:16 step:13094 [D loss: 0.312417, acc.: 85.16%] [G loss: 4.974493]\n",
      "epoch:16 step:13095 [D loss: 0.231262, acc.: 88.28%] [G loss: 4.611243]\n",
      "epoch:16 step:13096 [D loss: 0.349522, acc.: 85.16%] [G loss: 4.676551]\n",
      "epoch:16 step:13097 [D loss: 0.278113, acc.: 87.50%] [G loss: 2.921202]\n",
      "epoch:16 step:13098 [D loss: 0.316508, acc.: 85.16%] [G loss: 3.747444]\n",
      "epoch:16 step:13099 [D loss: 0.265605, acc.: 90.62%] [G loss: 3.152466]\n",
      "epoch:16 step:13100 [D loss: 0.373569, acc.: 85.16%] [G loss: 3.253732]\n",
      "epoch:16 step:13101 [D loss: 0.354297, acc.: 85.94%] [G loss: 4.054507]\n",
      "epoch:16 step:13102 [D loss: 0.284485, acc.: 85.94%] [G loss: 3.134471]\n",
      "epoch:16 step:13103 [D loss: 0.260814, acc.: 90.62%] [G loss: 3.589204]\n",
      "epoch:16 step:13104 [D loss: 0.264158, acc.: 85.94%] [G loss: 5.458211]\n",
      "epoch:16 step:13105 [D loss: 0.457701, acc.: 79.69%] [G loss: 2.995755]\n",
      "epoch:16 step:13106 [D loss: 0.390987, acc.: 77.34%] [G loss: 6.638899]\n",
      "epoch:16 step:13107 [D loss: 0.401032, acc.: 78.91%] [G loss: 6.280515]\n",
      "epoch:16 step:13108 [D loss: 0.457660, acc.: 78.91%] [G loss: 2.939721]\n",
      "epoch:16 step:13109 [D loss: 0.372221, acc.: 82.03%] [G loss: 3.805327]\n",
      "epoch:16 step:13110 [D loss: 0.241931, acc.: 90.62%] [G loss: 3.515654]\n",
      "epoch:16 step:13111 [D loss: 0.374898, acc.: 81.25%] [G loss: 3.266123]\n",
      "epoch:16 step:13112 [D loss: 0.290734, acc.: 89.06%] [G loss: 3.021791]\n",
      "epoch:16 step:13113 [D loss: 0.351118, acc.: 84.38%] [G loss: 3.270090]\n",
      "epoch:16 step:13114 [D loss: 0.295911, acc.: 89.06%] [G loss: 2.933174]\n",
      "epoch:16 step:13115 [D loss: 0.256913, acc.: 87.50%] [G loss: 4.422261]\n",
      "epoch:16 step:13116 [D loss: 0.265287, acc.: 86.72%] [G loss: 4.583926]\n",
      "epoch:16 step:13117 [D loss: 0.244027, acc.: 92.19%] [G loss: 4.404410]\n",
      "epoch:16 step:13118 [D loss: 0.299703, acc.: 86.72%] [G loss: 3.312954]\n",
      "epoch:16 step:13119 [D loss: 0.268747, acc.: 89.84%] [G loss: 2.858183]\n",
      "epoch:16 step:13120 [D loss: 0.329070, acc.: 82.03%] [G loss: 2.658350]\n",
      "epoch:16 step:13121 [D loss: 0.263052, acc.: 89.06%] [G loss: 3.377574]\n",
      "epoch:16 step:13122 [D loss: 0.202676, acc.: 92.19%] [G loss: 3.653939]\n",
      "epoch:16 step:13123 [D loss: 0.290000, acc.: 91.41%] [G loss: 2.443135]\n",
      "epoch:16 step:13124 [D loss: 0.266355, acc.: 87.50%] [G loss: 4.896686]\n",
      "epoch:16 step:13125 [D loss: 0.348186, acc.: 82.81%] [G loss: 2.814481]\n",
      "epoch:16 step:13126 [D loss: 0.208854, acc.: 92.97%] [G loss: 2.828767]\n",
      "epoch:16 step:13127 [D loss: 0.344795, acc.: 85.94%] [G loss: 2.498613]\n",
      "epoch:16 step:13128 [D loss: 0.235738, acc.: 90.62%] [G loss: 3.573907]\n",
      "epoch:16 step:13129 [D loss: 0.262824, acc.: 88.28%] [G loss: 2.963106]\n",
      "epoch:16 step:13130 [D loss: 0.265045, acc.: 90.62%] [G loss: 3.943627]\n",
      "epoch:16 step:13131 [D loss: 0.369524, acc.: 85.16%] [G loss: 3.464555]\n",
      "epoch:16 step:13132 [D loss: 0.332910, acc.: 88.28%] [G loss: 2.966532]\n",
      "epoch:16 step:13133 [D loss: 0.394769, acc.: 85.16%] [G loss: 2.618227]\n",
      "epoch:16 step:13134 [D loss: 0.316300, acc.: 87.50%] [G loss: 3.854218]\n",
      "epoch:16 step:13135 [D loss: 0.329407, acc.: 89.06%] [G loss: 4.232624]\n",
      "epoch:16 step:13136 [D loss: 0.356242, acc.: 85.16%] [G loss: 2.307978]\n",
      "epoch:16 step:13137 [D loss: 0.378117, acc.: 79.69%] [G loss: 3.183033]\n",
      "epoch:16 step:13138 [D loss: 0.282074, acc.: 87.50%] [G loss: 5.204193]\n",
      "epoch:16 step:13139 [D loss: 0.318113, acc.: 86.72%] [G loss: 4.647186]\n",
      "epoch:16 step:13140 [D loss: 0.473211, acc.: 77.34%] [G loss: 3.016966]\n",
      "epoch:16 step:13141 [D loss: 0.432205, acc.: 82.81%] [G loss: 3.279491]\n",
      "epoch:16 step:13142 [D loss: 0.328067, acc.: 83.59%] [G loss: 3.440242]\n",
      "epoch:16 step:13143 [D loss: 0.344088, acc.: 86.72%] [G loss: 3.139363]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:16 step:13144 [D loss: 0.373692, acc.: 86.72%] [G loss: 3.055243]\n",
      "epoch:16 step:13145 [D loss: 0.354883, acc.: 82.81%] [G loss: 3.685144]\n",
      "epoch:16 step:13146 [D loss: 0.396047, acc.: 79.69%] [G loss: 2.974710]\n",
      "epoch:16 step:13147 [D loss: 0.298903, acc.: 86.72%] [G loss: 3.184305]\n",
      "epoch:16 step:13148 [D loss: 0.224822, acc.: 92.97%] [G loss: 3.944096]\n",
      "epoch:16 step:13149 [D loss: 0.355718, acc.: 85.16%] [G loss: 2.347965]\n",
      "epoch:16 step:13150 [D loss: 0.317267, acc.: 85.94%] [G loss: 2.922301]\n",
      "epoch:16 step:13151 [D loss: 0.370439, acc.: 82.03%] [G loss: 3.013486]\n",
      "epoch:16 step:13152 [D loss: 0.441097, acc.: 83.59%] [G loss: 2.416557]\n",
      "epoch:16 step:13153 [D loss: 0.319263, acc.: 85.16%] [G loss: 3.249608]\n",
      "epoch:16 step:13154 [D loss: 0.283799, acc.: 89.06%] [G loss: 2.951161]\n",
      "epoch:16 step:13155 [D loss: 0.318651, acc.: 88.28%] [G loss: 3.569793]\n",
      "epoch:16 step:13156 [D loss: 0.347100, acc.: 85.16%] [G loss: 3.538114]\n",
      "epoch:16 step:13157 [D loss: 0.454497, acc.: 82.03%] [G loss: 2.442249]\n",
      "epoch:16 step:13158 [D loss: 0.422983, acc.: 82.81%] [G loss: 3.760297]\n",
      "epoch:16 step:13159 [D loss: 0.336118, acc.: 85.16%] [G loss: 3.158111]\n",
      "epoch:16 step:13160 [D loss: 0.301982, acc.: 87.50%] [G loss: 2.576590]\n",
      "epoch:16 step:13161 [D loss: 0.319502, acc.: 86.72%] [G loss: 3.381299]\n",
      "epoch:16 step:13162 [D loss: 0.306470, acc.: 83.59%] [G loss: 5.345874]\n",
      "epoch:16 step:13163 [D loss: 0.155278, acc.: 94.53%] [G loss: 4.925695]\n",
      "epoch:16 step:13164 [D loss: 0.402680, acc.: 82.81%] [G loss: 5.990432]\n",
      "epoch:16 step:13165 [D loss: 0.362870, acc.: 82.03%] [G loss: 4.934123]\n",
      "epoch:16 step:13166 [D loss: 0.554276, acc.: 75.78%] [G loss: 5.156351]\n",
      "epoch:16 step:13167 [D loss: 0.267956, acc.: 87.50%] [G loss: 4.023604]\n",
      "epoch:16 step:13168 [D loss: 0.370148, acc.: 80.47%] [G loss: 6.406220]\n",
      "epoch:16 step:13169 [D loss: 0.385283, acc.: 83.59%] [G loss: 3.991565]\n",
      "epoch:16 step:13170 [D loss: 0.199577, acc.: 92.97%] [G loss: 4.632337]\n",
      "epoch:16 step:13171 [D loss: 0.343885, acc.: 87.50%] [G loss: 3.681888]\n",
      "epoch:16 step:13172 [D loss: 0.301384, acc.: 88.28%] [G loss: 6.196851]\n",
      "epoch:16 step:13173 [D loss: 0.303661, acc.: 88.28%] [G loss: 3.683752]\n",
      "epoch:16 step:13174 [D loss: 0.379827, acc.: 83.59%] [G loss: 3.598143]\n",
      "epoch:16 step:13175 [D loss: 0.335199, acc.: 89.06%] [G loss: 2.991000]\n",
      "epoch:16 step:13176 [D loss: 0.422706, acc.: 78.91%] [G loss: 4.655544]\n",
      "epoch:16 step:13177 [D loss: 0.284167, acc.: 86.72%] [G loss: 3.699407]\n",
      "epoch:16 step:13178 [D loss: 0.278728, acc.: 86.72%] [G loss: 3.706188]\n",
      "epoch:16 step:13179 [D loss: 0.365952, acc.: 81.25%] [G loss: 2.685663]\n",
      "epoch:16 step:13180 [D loss: 0.333253, acc.: 82.81%] [G loss: 3.433644]\n",
      "epoch:16 step:13181 [D loss: 0.312779, acc.: 88.28%] [G loss: 3.948578]\n",
      "epoch:16 step:13182 [D loss: 0.360574, acc.: 86.72%] [G loss: 2.622083]\n",
      "epoch:16 step:13183 [D loss: 0.292075, acc.: 86.72%] [G loss: 3.350309]\n",
      "epoch:16 step:13184 [D loss: 0.259415, acc.: 89.06%] [G loss: 4.370275]\n",
      "epoch:16 step:13185 [D loss: 0.247189, acc.: 89.06%] [G loss: 4.200324]\n",
      "epoch:16 step:13186 [D loss: 0.249313, acc.: 91.41%] [G loss: 3.354667]\n",
      "epoch:16 step:13187 [D loss: 0.219394, acc.: 92.19%] [G loss: 4.504388]\n",
      "epoch:16 step:13188 [D loss: 0.371123, acc.: 81.25%] [G loss: 5.596372]\n",
      "epoch:16 step:13189 [D loss: 0.275354, acc.: 86.72%] [G loss: 4.218059]\n",
      "epoch:16 step:13190 [D loss: 0.260068, acc.: 89.84%] [G loss: 4.525374]\n",
      "epoch:16 step:13191 [D loss: 0.294466, acc.: 87.50%] [G loss: 3.797857]\n",
      "epoch:16 step:13192 [D loss: 0.233862, acc.: 89.84%] [G loss: 3.961868]\n",
      "epoch:16 step:13193 [D loss: 0.255979, acc.: 89.06%] [G loss: 4.781838]\n",
      "epoch:16 step:13194 [D loss: 0.360168, acc.: 87.50%] [G loss: 4.662789]\n",
      "epoch:16 step:13195 [D loss: 0.336795, acc.: 85.16%] [G loss: 3.211108]\n",
      "epoch:16 step:13196 [D loss: 0.260630, acc.: 87.50%] [G loss: 4.342627]\n",
      "epoch:16 step:13197 [D loss: 0.236169, acc.: 89.06%] [G loss: 3.367273]\n",
      "epoch:16 step:13198 [D loss: 0.264302, acc.: 85.94%] [G loss: 4.208464]\n",
      "epoch:16 step:13199 [D loss: 0.277089, acc.: 89.06%] [G loss: 3.727017]\n",
      "epoch:16 step:13200 [D loss: 0.369187, acc.: 80.47%] [G loss: 3.751537]\n",
      "##############\n",
      "[0.85731789 0.87286836 0.77382296 0.80540013 0.77416729 0.81163126\n",
      " 0.87461044 0.81165436 0.81733433 0.81255346]\n",
      "##########\n",
      "epoch:16 step:13201 [D loss: 0.383494, acc.: 85.94%] [G loss: 4.437987]\n",
      "epoch:16 step:13202 [D loss: 0.344666, acc.: 85.16%] [G loss: 4.389681]\n",
      "epoch:16 step:13203 [D loss: 0.347289, acc.: 83.59%] [G loss: 3.683914]\n",
      "epoch:16 step:13204 [D loss: 0.344653, acc.: 85.94%] [G loss: 3.238203]\n",
      "epoch:16 step:13205 [D loss: 0.280167, acc.: 87.50%] [G loss: 3.599856]\n",
      "epoch:16 step:13206 [D loss: 0.299903, acc.: 88.28%] [G loss: 2.579645]\n",
      "epoch:16 step:13207 [D loss: 0.278717, acc.: 85.94%] [G loss: 5.139100]\n",
      "epoch:16 step:13208 [D loss: 0.400843, acc.: 82.81%] [G loss: 7.677847]\n",
      "epoch:16 step:13209 [D loss: 0.930655, acc.: 61.72%] [G loss: 4.893047]\n",
      "epoch:16 step:13210 [D loss: 0.722317, acc.: 79.69%] [G loss: 3.661799]\n",
      "epoch:16 step:13211 [D loss: 0.289539, acc.: 85.16%] [G loss: 4.733129]\n",
      "epoch:16 step:13212 [D loss: 0.403540, acc.: 82.81%] [G loss: 4.173347]\n",
      "epoch:16 step:13213 [D loss: 0.309216, acc.: 87.50%] [G loss: 3.436458]\n",
      "epoch:16 step:13214 [D loss: 0.250333, acc.: 89.84%] [G loss: 2.992393]\n",
      "epoch:16 step:13215 [D loss: 0.319929, acc.: 85.94%] [G loss: 3.689785]\n",
      "epoch:16 step:13216 [D loss: 0.272353, acc.: 87.50%] [G loss: 4.011824]\n",
      "epoch:16 step:13217 [D loss: 0.329515, acc.: 88.28%] [G loss: 4.672133]\n",
      "epoch:16 step:13218 [D loss: 0.301201, acc.: 86.72%] [G loss: 3.047948]\n",
      "epoch:16 step:13219 [D loss: 0.276925, acc.: 88.28%] [G loss: 4.074303]\n",
      "epoch:16 step:13220 [D loss: 0.394331, acc.: 84.38%] [G loss: 3.010512]\n",
      "epoch:16 step:13221 [D loss: 0.220134, acc.: 91.41%] [G loss: 4.359086]\n",
      "epoch:16 step:13222 [D loss: 0.324735, acc.: 86.72%] [G loss: 4.566100]\n",
      "epoch:16 step:13223 [D loss: 0.336720, acc.: 85.94%] [G loss: 3.210339]\n",
      "epoch:16 step:13224 [D loss: 0.292484, acc.: 92.19%] [G loss: 2.091046]\n",
      "epoch:16 step:13225 [D loss: 0.331877, acc.: 84.38%] [G loss: 2.874835]\n",
      "epoch:16 step:13226 [D loss: 0.374240, acc.: 82.81%] [G loss: 3.115150]\n",
      "epoch:16 step:13227 [D loss: 0.296314, acc.: 88.28%] [G loss: 3.251344]\n",
      "epoch:16 step:13228 [D loss: 0.322073, acc.: 85.94%] [G loss: 3.212535]\n",
      "epoch:16 step:13229 [D loss: 0.393207, acc.: 81.25%] [G loss: 3.970605]\n",
      "epoch:16 step:13230 [D loss: 0.311688, acc.: 81.25%] [G loss: 3.049279]\n",
      "epoch:16 step:13231 [D loss: 0.336483, acc.: 85.94%] [G loss: 2.678196]\n",
      "epoch:16 step:13232 [D loss: 0.299118, acc.: 86.72%] [G loss: 2.926707]\n",
      "epoch:16 step:13233 [D loss: 0.339863, acc.: 87.50%] [G loss: 3.126015]\n",
      "epoch:16 step:13234 [D loss: 0.343648, acc.: 83.59%] [G loss: 2.572472]\n",
      "epoch:16 step:13235 [D loss: 0.299532, acc.: 88.28%] [G loss: 3.258027]\n",
      "epoch:16 step:13236 [D loss: 0.322028, acc.: 86.72%] [G loss: 2.874118]\n",
      "epoch:16 step:13237 [D loss: 0.333348, acc.: 85.94%] [G loss: 2.662587]\n",
      "epoch:16 step:13238 [D loss: 0.357307, acc.: 87.50%] [G loss: 3.041288]\n",
      "epoch:16 step:13239 [D loss: 0.167165, acc.: 93.75%] [G loss: 4.077600]\n",
      "epoch:16 step:13240 [D loss: 0.359464, acc.: 82.03%] [G loss: 2.733151]\n",
      "epoch:16 step:13241 [D loss: 0.303642, acc.: 89.06%] [G loss: 3.490205]\n",
      "epoch:16 step:13242 [D loss: 0.297488, acc.: 89.84%] [G loss: 5.136892]\n",
      "epoch:16 step:13243 [D loss: 0.270012, acc.: 89.06%] [G loss: 3.096750]\n",
      "epoch:16 step:13244 [D loss: 0.281220, acc.: 87.50%] [G loss: 4.085102]\n",
      "epoch:16 step:13245 [D loss: 0.298045, acc.: 85.94%] [G loss: 3.814308]\n",
      "epoch:16 step:13246 [D loss: 0.340557, acc.: 84.38%] [G loss: 3.701616]\n",
      "epoch:16 step:13247 [D loss: 0.390249, acc.: 85.16%] [G loss: 2.471459]\n",
      "epoch:16 step:13248 [D loss: 0.246422, acc.: 90.62%] [G loss: 4.050098]\n",
      "epoch:16 step:13249 [D loss: 0.231030, acc.: 89.84%] [G loss: 3.723180]\n",
      "epoch:16 step:13250 [D loss: 0.410026, acc.: 81.25%] [G loss: 2.336608]\n",
      "epoch:16 step:13251 [D loss: 0.353858, acc.: 84.38%] [G loss: 2.946396]\n",
      "epoch:16 step:13252 [D loss: 0.417687, acc.: 78.12%] [G loss: 3.491053]\n",
      "epoch:16 step:13253 [D loss: 0.225024, acc.: 92.97%] [G loss: 3.638001]\n",
      "epoch:16 step:13254 [D loss: 0.301057, acc.: 88.28%] [G loss: 2.992559]\n",
      "epoch:16 step:13255 [D loss: 0.302843, acc.: 85.94%] [G loss: 3.920810]\n",
      "epoch:16 step:13256 [D loss: 0.217097, acc.: 91.41%] [G loss: 5.246364]\n",
      "epoch:16 step:13257 [D loss: 0.365618, acc.: 82.81%] [G loss: 3.951655]\n",
      "epoch:16 step:13258 [D loss: 0.449431, acc.: 76.56%] [G loss: 2.814936]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:16 step:13259 [D loss: 0.402275, acc.: 80.47%] [G loss: 4.517759]\n",
      "epoch:16 step:13260 [D loss: 0.302173, acc.: 89.84%] [G loss: 3.429827]\n",
      "epoch:16 step:13261 [D loss: 0.294108, acc.: 86.72%] [G loss: 3.253210]\n",
      "epoch:16 step:13262 [D loss: 0.289756, acc.: 89.06%] [G loss: 2.862506]\n",
      "epoch:16 step:13263 [D loss: 0.219835, acc.: 89.06%] [G loss: 4.153922]\n",
      "epoch:16 step:13264 [D loss: 0.281000, acc.: 87.50%] [G loss: 4.931006]\n",
      "epoch:16 step:13265 [D loss: 0.279767, acc.: 85.16%] [G loss: 6.197003]\n",
      "epoch:16 step:13266 [D loss: 0.293662, acc.: 85.16%] [G loss: 3.993472]\n",
      "epoch:16 step:13267 [D loss: 0.360760, acc.: 84.38%] [G loss: 2.934613]\n",
      "epoch:16 step:13268 [D loss: 0.318713, acc.: 85.16%] [G loss: 3.249948]\n",
      "epoch:16 step:13269 [D loss: 0.392717, acc.: 81.25%] [G loss: 2.914160]\n",
      "epoch:16 step:13270 [D loss: 0.282667, acc.: 90.62%] [G loss: 3.381864]\n",
      "epoch:16 step:13271 [D loss: 0.273723, acc.: 89.06%] [G loss: 4.404955]\n",
      "epoch:16 step:13272 [D loss: 0.219992, acc.: 92.19%] [G loss: 3.191631]\n",
      "epoch:16 step:13273 [D loss: 0.333778, acc.: 85.94%] [G loss: 4.558903]\n",
      "epoch:16 step:13274 [D loss: 0.470274, acc.: 76.56%] [G loss: 3.814007]\n",
      "epoch:16 step:13275 [D loss: 0.285722, acc.: 85.16%] [G loss: 2.979260]\n",
      "epoch:16 step:13276 [D loss: 0.360877, acc.: 84.38%] [G loss: 3.476430]\n",
      "epoch:16 step:13277 [D loss: 0.258356, acc.: 90.62%] [G loss: 3.995435]\n",
      "epoch:17 step:13278 [D loss: 0.263235, acc.: 90.62%] [G loss: 5.143960]\n",
      "epoch:17 step:13279 [D loss: 0.249657, acc.: 89.84%] [G loss: 3.231078]\n",
      "epoch:17 step:13280 [D loss: 0.374866, acc.: 85.16%] [G loss: 8.077084]\n",
      "epoch:17 step:13281 [D loss: 0.263108, acc.: 89.84%] [G loss: 4.905208]\n",
      "epoch:17 step:13282 [D loss: 0.246882, acc.: 91.41%] [G loss: 6.702545]\n",
      "epoch:17 step:13283 [D loss: 0.184560, acc.: 92.97%] [G loss: 5.895923]\n",
      "epoch:17 step:13284 [D loss: 0.309345, acc.: 83.59%] [G loss: 3.209828]\n",
      "epoch:17 step:13285 [D loss: 0.452062, acc.: 82.81%] [G loss: 3.521601]\n",
      "epoch:17 step:13286 [D loss: 0.371500, acc.: 82.81%] [G loss: 4.192137]\n",
      "epoch:17 step:13287 [D loss: 0.381741, acc.: 84.38%] [G loss: 5.088132]\n",
      "epoch:17 step:13288 [D loss: 0.378528, acc.: 83.59%] [G loss: 4.264903]\n",
      "epoch:17 step:13289 [D loss: 0.267409, acc.: 89.84%] [G loss: 2.361361]\n",
      "epoch:17 step:13290 [D loss: 0.333437, acc.: 89.06%] [G loss: 3.230518]\n",
      "epoch:17 step:13291 [D loss: 0.332154, acc.: 85.16%] [G loss: 5.012937]\n",
      "epoch:17 step:13292 [D loss: 0.371656, acc.: 82.81%] [G loss: 4.113339]\n",
      "epoch:17 step:13293 [D loss: 0.326044, acc.: 86.72%] [G loss: 2.473397]\n",
      "epoch:17 step:13294 [D loss: 0.301464, acc.: 89.06%] [G loss: 2.720475]\n",
      "epoch:17 step:13295 [D loss: 0.332745, acc.: 86.72%] [G loss: 3.662093]\n",
      "epoch:17 step:13296 [D loss: 0.221786, acc.: 92.19%] [G loss: 4.349276]\n",
      "epoch:17 step:13297 [D loss: 0.251943, acc.: 90.62%] [G loss: 4.726835]\n",
      "epoch:17 step:13298 [D loss: 0.301097, acc.: 85.94%] [G loss: 5.850880]\n",
      "epoch:17 step:13299 [D loss: 0.169838, acc.: 94.53%] [G loss: 5.640285]\n",
      "epoch:17 step:13300 [D loss: 0.274675, acc.: 87.50%] [G loss: 3.076340]\n",
      "epoch:17 step:13301 [D loss: 0.294917, acc.: 85.94%] [G loss: 2.940514]\n",
      "epoch:17 step:13302 [D loss: 0.279867, acc.: 85.94%] [G loss: 3.904307]\n",
      "epoch:17 step:13303 [D loss: 0.236761, acc.: 89.84%] [G loss: 4.924497]\n",
      "epoch:17 step:13304 [D loss: 0.325880, acc.: 86.72%] [G loss: 4.116899]\n",
      "epoch:17 step:13305 [D loss: 0.257815, acc.: 92.19%] [G loss: 5.673310]\n",
      "epoch:17 step:13306 [D loss: 0.272207, acc.: 88.28%] [G loss: 3.891568]\n",
      "epoch:17 step:13307 [D loss: 0.206331, acc.: 93.75%] [G loss: 4.793141]\n",
      "epoch:17 step:13308 [D loss: 0.303169, acc.: 86.72%] [G loss: 3.544489]\n",
      "epoch:17 step:13309 [D loss: 0.336037, acc.: 85.16%] [G loss: 3.432461]\n",
      "epoch:17 step:13310 [D loss: 0.322717, acc.: 82.81%] [G loss: 4.693861]\n",
      "epoch:17 step:13311 [D loss: 0.323773, acc.: 80.47%] [G loss: 6.099280]\n",
      "epoch:17 step:13312 [D loss: 0.293821, acc.: 87.50%] [G loss: 3.960484]\n",
      "epoch:17 step:13313 [D loss: 0.254300, acc.: 87.50%] [G loss: 5.517397]\n",
      "epoch:17 step:13314 [D loss: 0.306215, acc.: 88.28%] [G loss: 2.856110]\n",
      "epoch:17 step:13315 [D loss: 0.236437, acc.: 90.62%] [G loss: 4.542479]\n",
      "epoch:17 step:13316 [D loss: 0.260846, acc.: 88.28%] [G loss: 5.310245]\n",
      "epoch:17 step:13317 [D loss: 0.327307, acc.: 89.84%] [G loss: 4.190065]\n",
      "epoch:17 step:13318 [D loss: 0.413769, acc.: 78.91%] [G loss: 2.995131]\n",
      "epoch:17 step:13319 [D loss: 0.323590, acc.: 84.38%] [G loss: 3.435764]\n",
      "epoch:17 step:13320 [D loss: 0.314038, acc.: 81.25%] [G loss: 3.095378]\n",
      "epoch:17 step:13321 [D loss: 0.323745, acc.: 82.81%] [G loss: 2.753882]\n",
      "epoch:17 step:13322 [D loss: 0.291432, acc.: 89.84%] [G loss: 2.888322]\n",
      "epoch:17 step:13323 [D loss: 0.347727, acc.: 82.81%] [G loss: 3.939870]\n",
      "epoch:17 step:13324 [D loss: 0.325413, acc.: 84.38%] [G loss: 3.154986]\n",
      "epoch:17 step:13325 [D loss: 0.317908, acc.: 86.72%] [G loss: 4.392909]\n",
      "epoch:17 step:13326 [D loss: 0.326243, acc.: 89.06%] [G loss: 5.358951]\n",
      "epoch:17 step:13327 [D loss: 0.317706, acc.: 85.94%] [G loss: 4.697282]\n",
      "epoch:17 step:13328 [D loss: 0.263984, acc.: 89.06%] [G loss: 8.640306]\n",
      "epoch:17 step:13329 [D loss: 0.324755, acc.: 84.38%] [G loss: 5.258162]\n",
      "epoch:17 step:13330 [D loss: 0.392139, acc.: 85.16%] [G loss: 3.369597]\n",
      "epoch:17 step:13331 [D loss: 0.254128, acc.: 90.62%] [G loss: 5.492203]\n",
      "epoch:17 step:13332 [D loss: 0.402596, acc.: 83.59%] [G loss: 3.807133]\n",
      "epoch:17 step:13333 [D loss: 0.289092, acc.: 88.28%] [G loss: 2.590112]\n",
      "epoch:17 step:13334 [D loss: 0.230954, acc.: 92.19%] [G loss: 2.574691]\n",
      "epoch:17 step:13335 [D loss: 0.349037, acc.: 88.28%] [G loss: 2.976087]\n",
      "epoch:17 step:13336 [D loss: 0.242270, acc.: 91.41%] [G loss: 3.445317]\n",
      "epoch:17 step:13337 [D loss: 0.213061, acc.: 90.62%] [G loss: 2.935996]\n",
      "epoch:17 step:13338 [D loss: 0.263619, acc.: 89.06%] [G loss: 3.429928]\n",
      "epoch:17 step:13339 [D loss: 0.223938, acc.: 90.62%] [G loss: 3.300802]\n",
      "epoch:17 step:13340 [D loss: 0.280395, acc.: 87.50%] [G loss: 4.021899]\n",
      "epoch:17 step:13341 [D loss: 0.336422, acc.: 85.94%] [G loss: 3.171061]\n",
      "epoch:17 step:13342 [D loss: 0.359783, acc.: 81.25%] [G loss: 2.885219]\n",
      "epoch:17 step:13343 [D loss: 0.325085, acc.: 88.28%] [G loss: 3.408229]\n",
      "epoch:17 step:13344 [D loss: 0.293790, acc.: 90.62%] [G loss: 3.536689]\n",
      "epoch:17 step:13345 [D loss: 0.353258, acc.: 83.59%] [G loss: 5.256803]\n",
      "epoch:17 step:13346 [D loss: 0.525192, acc.: 81.25%] [G loss: 7.064826]\n",
      "epoch:17 step:13347 [D loss: 0.447019, acc.: 80.47%] [G loss: 7.060623]\n",
      "epoch:17 step:13348 [D loss: 0.612519, acc.: 72.66%] [G loss: 2.924233]\n",
      "epoch:17 step:13349 [D loss: 0.340054, acc.: 85.94%] [G loss: 3.067468]\n",
      "epoch:17 step:13350 [D loss: 0.357574, acc.: 85.16%] [G loss: 4.135550]\n",
      "epoch:17 step:13351 [D loss: 0.436679, acc.: 80.47%] [G loss: 2.723551]\n",
      "epoch:17 step:13352 [D loss: 0.452336, acc.: 85.94%] [G loss: 4.435820]\n",
      "epoch:17 step:13353 [D loss: 0.342404, acc.: 85.16%] [G loss: 3.464879]\n",
      "epoch:17 step:13354 [D loss: 0.307421, acc.: 86.72%] [G loss: 3.097069]\n",
      "epoch:17 step:13355 [D loss: 0.295253, acc.: 89.06%] [G loss: 3.286027]\n",
      "epoch:17 step:13356 [D loss: 0.431414, acc.: 81.25%] [G loss: 2.471259]\n",
      "epoch:17 step:13357 [D loss: 0.287052, acc.: 88.28%] [G loss: 3.311707]\n",
      "epoch:17 step:13358 [D loss: 0.303776, acc.: 86.72%] [G loss: 3.330024]\n",
      "epoch:17 step:13359 [D loss: 0.307872, acc.: 90.62%] [G loss: 2.603276]\n",
      "epoch:17 step:13360 [D loss: 0.311754, acc.: 90.62%] [G loss: 2.888451]\n",
      "epoch:17 step:13361 [D loss: 0.203600, acc.: 92.19%] [G loss: 3.826885]\n",
      "epoch:17 step:13362 [D loss: 0.346100, acc.: 84.38%] [G loss: 3.975559]\n",
      "epoch:17 step:13363 [D loss: 0.278862, acc.: 90.62%] [G loss: 4.392338]\n",
      "epoch:17 step:13364 [D loss: 0.356011, acc.: 82.03%] [G loss: 3.254815]\n",
      "epoch:17 step:13365 [D loss: 0.301354, acc.: 86.72%] [G loss: 3.171289]\n",
      "epoch:17 step:13366 [D loss: 0.378761, acc.: 82.03%] [G loss: 3.345196]\n",
      "epoch:17 step:13367 [D loss: 0.422396, acc.: 82.03%] [G loss: 3.663742]\n",
      "epoch:17 step:13368 [D loss: 0.345074, acc.: 83.59%] [G loss: 3.553739]\n",
      "epoch:17 step:13369 [D loss: 0.336331, acc.: 87.50%] [G loss: 3.525115]\n",
      "epoch:17 step:13370 [D loss: 0.338051, acc.: 85.94%] [G loss: 4.441187]\n",
      "epoch:17 step:13371 [D loss: 0.218931, acc.: 91.41%] [G loss: 5.800390]\n",
      "epoch:17 step:13372 [D loss: 0.342903, acc.: 83.59%] [G loss: 4.664763]\n",
      "epoch:17 step:13373 [D loss: 0.295933, acc.: 88.28%] [G loss: 2.998713]\n",
      "epoch:17 step:13374 [D loss: 0.314876, acc.: 86.72%] [G loss: 3.292283]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:17 step:13375 [D loss: 0.306082, acc.: 86.72%] [G loss: 3.423026]\n",
      "epoch:17 step:13376 [D loss: 0.267568, acc.: 89.06%] [G loss: 3.433809]\n",
      "epoch:17 step:13377 [D loss: 0.363966, acc.: 85.16%] [G loss: 5.033743]\n",
      "epoch:17 step:13378 [D loss: 0.283807, acc.: 90.62%] [G loss: 4.450248]\n",
      "epoch:17 step:13379 [D loss: 0.199523, acc.: 91.41%] [G loss: 4.360148]\n",
      "epoch:17 step:13380 [D loss: 0.308616, acc.: 86.72%] [G loss: 2.795775]\n",
      "epoch:17 step:13381 [D loss: 0.323612, acc.: 84.38%] [G loss: 3.136288]\n",
      "epoch:17 step:13382 [D loss: 0.311078, acc.: 88.28%] [G loss: 3.364344]\n",
      "epoch:17 step:13383 [D loss: 0.316319, acc.: 83.59%] [G loss: 3.973511]\n",
      "epoch:17 step:13384 [D loss: 0.215968, acc.: 91.41%] [G loss: 5.026655]\n",
      "epoch:17 step:13385 [D loss: 0.234505, acc.: 88.28%] [G loss: 3.767255]\n",
      "epoch:17 step:13386 [D loss: 0.282818, acc.: 89.84%] [G loss: 3.019587]\n",
      "epoch:17 step:13387 [D loss: 0.283972, acc.: 86.72%] [G loss: 3.250780]\n",
      "epoch:17 step:13388 [D loss: 0.376987, acc.: 80.47%] [G loss: 3.000322]\n",
      "epoch:17 step:13389 [D loss: 0.398657, acc.: 85.16%] [G loss: 3.737523]\n",
      "epoch:17 step:13390 [D loss: 0.245225, acc.: 90.62%] [G loss: 4.694421]\n",
      "epoch:17 step:13391 [D loss: 0.202653, acc.: 91.41%] [G loss: 4.102379]\n",
      "epoch:17 step:13392 [D loss: 0.329788, acc.: 85.94%] [G loss: 2.868684]\n",
      "epoch:17 step:13393 [D loss: 0.285597, acc.: 88.28%] [G loss: 3.541049]\n",
      "epoch:17 step:13394 [D loss: 0.325744, acc.: 82.81%] [G loss: 3.863570]\n",
      "epoch:17 step:13395 [D loss: 0.273527, acc.: 89.06%] [G loss: 4.742449]\n",
      "epoch:17 step:13396 [D loss: 0.231136, acc.: 90.62%] [G loss: 5.231134]\n",
      "epoch:17 step:13397 [D loss: 0.257882, acc.: 90.62%] [G loss: 4.175276]\n",
      "epoch:17 step:13398 [D loss: 0.265598, acc.: 92.19%] [G loss: 4.804245]\n",
      "epoch:17 step:13399 [D loss: 0.230105, acc.: 90.62%] [G loss: 4.845347]\n",
      "epoch:17 step:13400 [D loss: 0.280566, acc.: 86.72%] [G loss: 4.353791]\n",
      "##############\n",
      "[0.85956428 0.86195188 0.81488618 0.7928139  0.75946947 0.80419478\n",
      " 0.8925606  0.82051363 0.82438506 0.84133414]\n",
      "##########\n",
      "epoch:17 step:13401 [D loss: 0.286602, acc.: 89.84%] [G loss: 5.996741]\n",
      "epoch:17 step:13402 [D loss: 0.355904, acc.: 80.47%] [G loss: 3.233375]\n",
      "epoch:17 step:13403 [D loss: 0.133659, acc.: 95.31%] [G loss: 5.921866]\n",
      "epoch:17 step:13404 [D loss: 0.189620, acc.: 94.53%] [G loss: 4.185433]\n",
      "epoch:17 step:13405 [D loss: 0.179304, acc.: 92.19%] [G loss: 6.992296]\n",
      "epoch:17 step:13406 [D loss: 0.240081, acc.: 89.84%] [G loss: 5.740059]\n",
      "epoch:17 step:13407 [D loss: 0.339406, acc.: 86.72%] [G loss: 3.487797]\n",
      "epoch:17 step:13408 [D loss: 0.206068, acc.: 93.75%] [G loss: 5.809268]\n",
      "epoch:17 step:13409 [D loss: 0.246824, acc.: 91.41%] [G loss: 4.936001]\n",
      "epoch:17 step:13410 [D loss: 0.336789, acc.: 82.81%] [G loss: 4.237806]\n",
      "epoch:17 step:13411 [D loss: 0.202161, acc.: 92.97%] [G loss: 4.152627]\n",
      "epoch:17 step:13412 [D loss: 0.325217, acc.: 85.16%] [G loss: 4.165391]\n",
      "epoch:17 step:13413 [D loss: 0.208670, acc.: 92.19%] [G loss: 3.595792]\n",
      "epoch:17 step:13414 [D loss: 0.341746, acc.: 88.28%] [G loss: 3.413081]\n",
      "epoch:17 step:13415 [D loss: 0.319217, acc.: 85.16%] [G loss: 3.039473]\n",
      "epoch:17 step:13416 [D loss: 0.241641, acc.: 89.06%] [G loss: 3.076578]\n",
      "epoch:17 step:13417 [D loss: 0.290829, acc.: 90.62%] [G loss: 3.605715]\n",
      "epoch:17 step:13418 [D loss: 0.333712, acc.: 83.59%] [G loss: 4.762618]\n",
      "epoch:17 step:13419 [D loss: 0.645021, acc.: 67.19%] [G loss: 6.102983]\n",
      "epoch:17 step:13420 [D loss: 0.597915, acc.: 75.00%] [G loss: 4.019548]\n",
      "epoch:17 step:13421 [D loss: 0.327741, acc.: 84.38%] [G loss: 6.398732]\n",
      "epoch:17 step:13422 [D loss: 0.447790, acc.: 81.25%] [G loss: 3.744632]\n",
      "epoch:17 step:13423 [D loss: 0.509571, acc.: 77.34%] [G loss: 2.802743]\n",
      "epoch:17 step:13424 [D loss: 0.381345, acc.: 82.81%] [G loss: 2.719992]\n",
      "epoch:17 step:13425 [D loss: 0.348483, acc.: 83.59%] [G loss: 3.292572]\n",
      "epoch:17 step:13426 [D loss: 0.301031, acc.: 86.72%] [G loss: 3.436547]\n",
      "epoch:17 step:13427 [D loss: 0.275876, acc.: 86.72%] [G loss: 3.332636]\n",
      "epoch:17 step:13428 [D loss: 0.285303, acc.: 89.06%] [G loss: 2.870317]\n",
      "epoch:17 step:13429 [D loss: 0.361863, acc.: 86.72%] [G loss: 2.646776]\n",
      "epoch:17 step:13430 [D loss: 0.267282, acc.: 87.50%] [G loss: 3.113199]\n",
      "epoch:17 step:13431 [D loss: 0.225476, acc.: 89.84%] [G loss: 3.039428]\n",
      "epoch:17 step:13432 [D loss: 0.287499, acc.: 89.84%] [G loss: 3.682406]\n",
      "epoch:17 step:13433 [D loss: 0.222245, acc.: 92.97%] [G loss: 3.512301]\n",
      "epoch:17 step:13434 [D loss: 0.264126, acc.: 92.19%] [G loss: 4.226808]\n",
      "epoch:17 step:13435 [D loss: 0.329857, acc.: 84.38%] [G loss: 2.872682]\n",
      "epoch:17 step:13436 [D loss: 0.378299, acc.: 80.47%] [G loss: 2.717177]\n",
      "epoch:17 step:13437 [D loss: 0.247134, acc.: 92.97%] [G loss: 6.290511]\n",
      "epoch:17 step:13438 [D loss: 0.380743, acc.: 85.16%] [G loss: 3.142326]\n",
      "epoch:17 step:13439 [D loss: 0.217801, acc.: 92.19%] [G loss: 3.224211]\n",
      "epoch:17 step:13440 [D loss: 0.243683, acc.: 88.28%] [G loss: 3.666827]\n",
      "epoch:17 step:13441 [D loss: 0.284579, acc.: 88.28%] [G loss: 3.579040]\n",
      "epoch:17 step:13442 [D loss: 0.329796, acc.: 85.94%] [G loss: 4.295751]\n",
      "epoch:17 step:13443 [D loss: 0.326564, acc.: 85.16%] [G loss: 3.267032]\n",
      "epoch:17 step:13444 [D loss: 0.339865, acc.: 89.06%] [G loss: 4.080119]\n",
      "epoch:17 step:13445 [D loss: 0.419519, acc.: 82.81%] [G loss: 3.092952]\n",
      "epoch:17 step:13446 [D loss: 0.280513, acc.: 89.06%] [G loss: 6.573075]\n",
      "epoch:17 step:13447 [D loss: 0.314287, acc.: 90.62%] [G loss: 2.835586]\n",
      "epoch:17 step:13448 [D loss: 0.301793, acc.: 85.16%] [G loss: 3.976784]\n",
      "epoch:17 step:13449 [D loss: 0.231259, acc.: 92.19%] [G loss: 4.427122]\n",
      "epoch:17 step:13450 [D loss: 0.314674, acc.: 89.06%] [G loss: 2.861130]\n",
      "epoch:17 step:13451 [D loss: 0.422106, acc.: 83.59%] [G loss: 2.664165]\n",
      "epoch:17 step:13452 [D loss: 0.286604, acc.: 89.06%] [G loss: 2.283850]\n",
      "epoch:17 step:13453 [D loss: 0.423307, acc.: 78.91%] [G loss: 4.775187]\n",
      "epoch:17 step:13454 [D loss: 0.393643, acc.: 81.25%] [G loss: 6.354584]\n",
      "epoch:17 step:13455 [D loss: 0.419096, acc.: 80.47%] [G loss: 3.271197]\n",
      "epoch:17 step:13456 [D loss: 0.223788, acc.: 91.41%] [G loss: 6.026639]\n",
      "epoch:17 step:13457 [D loss: 0.316474, acc.: 85.16%] [G loss: 2.883128]\n",
      "epoch:17 step:13458 [D loss: 0.335487, acc.: 88.28%] [G loss: 3.534242]\n",
      "epoch:17 step:13459 [D loss: 0.415275, acc.: 82.03%] [G loss: 3.069522]\n",
      "epoch:17 step:13460 [D loss: 0.276221, acc.: 89.06%] [G loss: 3.354132]\n",
      "epoch:17 step:13461 [D loss: 0.314934, acc.: 83.59%] [G loss: 2.508850]\n",
      "epoch:17 step:13462 [D loss: 0.294855, acc.: 83.59%] [G loss: 2.837832]\n",
      "epoch:17 step:13463 [D loss: 0.316264, acc.: 85.16%] [G loss: 2.754364]\n",
      "epoch:17 step:13464 [D loss: 0.311934, acc.: 88.28%] [G loss: 3.126118]\n",
      "epoch:17 step:13465 [D loss: 0.331630, acc.: 86.72%] [G loss: 2.791335]\n",
      "epoch:17 step:13466 [D loss: 0.293368, acc.: 87.50%] [G loss: 3.469903]\n",
      "epoch:17 step:13467 [D loss: 0.256461, acc.: 89.84%] [G loss: 4.777638]\n",
      "epoch:17 step:13468 [D loss: 0.280192, acc.: 88.28%] [G loss: 3.093440]\n",
      "epoch:17 step:13469 [D loss: 0.227445, acc.: 91.41%] [G loss: 4.946749]\n",
      "epoch:17 step:13470 [D loss: 0.302259, acc.: 88.28%] [G loss: 3.830872]\n",
      "epoch:17 step:13471 [D loss: 0.304464, acc.: 89.84%] [G loss: 3.321230]\n",
      "epoch:17 step:13472 [D loss: 0.301960, acc.: 89.06%] [G loss: 2.822927]\n",
      "epoch:17 step:13473 [D loss: 0.256855, acc.: 92.19%] [G loss: 3.283587]\n",
      "epoch:17 step:13474 [D loss: 0.305235, acc.: 85.94%] [G loss: 3.948142]\n",
      "epoch:17 step:13475 [D loss: 0.305298, acc.: 86.72%] [G loss: 3.920435]\n",
      "epoch:17 step:13476 [D loss: 0.285727, acc.: 89.06%] [G loss: 3.328921]\n",
      "epoch:17 step:13477 [D loss: 0.281768, acc.: 86.72%] [G loss: 3.628276]\n",
      "epoch:17 step:13478 [D loss: 0.292770, acc.: 89.06%] [G loss: 3.109948]\n",
      "epoch:17 step:13479 [D loss: 0.414070, acc.: 83.59%] [G loss: 3.346849]\n",
      "epoch:17 step:13480 [D loss: 0.421362, acc.: 82.03%] [G loss: 2.811653]\n",
      "epoch:17 step:13481 [D loss: 0.240068, acc.: 89.84%] [G loss: 3.487294]\n",
      "epoch:17 step:13482 [D loss: 0.281682, acc.: 89.84%] [G loss: 3.042197]\n",
      "epoch:17 step:13483 [D loss: 0.193636, acc.: 92.19%] [G loss: 4.809775]\n",
      "epoch:17 step:13484 [D loss: 0.209647, acc.: 92.19%] [G loss: 4.381127]\n",
      "epoch:17 step:13485 [D loss: 0.234898, acc.: 89.06%] [G loss: 3.722442]\n",
      "epoch:17 step:13486 [D loss: 0.236114, acc.: 92.19%] [G loss: 5.495045]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:17 step:13487 [D loss: 0.261014, acc.: 89.06%] [G loss: 5.132659]\n",
      "epoch:17 step:13488 [D loss: 0.261908, acc.: 91.41%] [G loss: 5.329627]\n",
      "epoch:17 step:13489 [D loss: 0.217588, acc.: 89.84%] [G loss: 4.426078]\n",
      "epoch:17 step:13490 [D loss: 0.246050, acc.: 89.06%] [G loss: 2.856350]\n",
      "epoch:17 step:13491 [D loss: 0.270875, acc.: 88.28%] [G loss: 3.456137]\n",
      "epoch:17 step:13492 [D loss: 0.251354, acc.: 91.41%] [G loss: 3.510333]\n",
      "epoch:17 step:13493 [D loss: 0.308670, acc.: 89.84%] [G loss: 2.868317]\n",
      "epoch:17 step:13494 [D loss: 0.315519, acc.: 86.72%] [G loss: 4.573027]\n",
      "epoch:17 step:13495 [D loss: 0.460188, acc.: 76.56%] [G loss: 2.168149]\n",
      "epoch:17 step:13496 [D loss: 0.345818, acc.: 84.38%] [G loss: 2.547652]\n",
      "epoch:17 step:13497 [D loss: 0.304822, acc.: 86.72%] [G loss: 2.827090]\n",
      "epoch:17 step:13498 [D loss: 0.339905, acc.: 82.03%] [G loss: 3.598085]\n",
      "epoch:17 step:13499 [D loss: 0.451117, acc.: 76.56%] [G loss: 2.245880]\n",
      "epoch:17 step:13500 [D loss: 0.170605, acc.: 96.09%] [G loss: 4.182894]\n",
      "epoch:17 step:13501 [D loss: 0.314769, acc.: 85.16%] [G loss: 2.809274]\n",
      "epoch:17 step:13502 [D loss: 0.307128, acc.: 87.50%] [G loss: 2.306549]\n",
      "epoch:17 step:13503 [D loss: 0.286664, acc.: 86.72%] [G loss: 3.029278]\n",
      "epoch:17 step:13504 [D loss: 0.305743, acc.: 89.06%] [G loss: 3.416132]\n",
      "epoch:17 step:13505 [D loss: 0.368875, acc.: 82.81%] [G loss: 2.637956]\n",
      "epoch:17 step:13506 [D loss: 0.332690, acc.: 85.94%] [G loss: 2.568803]\n",
      "epoch:17 step:13507 [D loss: 0.419043, acc.: 82.03%] [G loss: 2.499079]\n",
      "epoch:17 step:13508 [D loss: 0.249748, acc.: 92.19%] [G loss: 3.289217]\n",
      "epoch:17 step:13509 [D loss: 0.258911, acc.: 92.19%] [G loss: 3.444510]\n",
      "epoch:17 step:13510 [D loss: 0.381966, acc.: 78.12%] [G loss: 2.470060]\n",
      "epoch:17 step:13511 [D loss: 0.251035, acc.: 86.72%] [G loss: 3.313025]\n",
      "epoch:17 step:13512 [D loss: 0.264869, acc.: 89.06%] [G loss: 3.810414]\n",
      "epoch:17 step:13513 [D loss: 0.308896, acc.: 85.94%] [G loss: 3.005025]\n",
      "epoch:17 step:13514 [D loss: 0.276483, acc.: 85.94%] [G loss: 3.677262]\n",
      "epoch:17 step:13515 [D loss: 0.279849, acc.: 86.72%] [G loss: 2.952248]\n",
      "epoch:17 step:13516 [D loss: 0.194157, acc.: 92.97%] [G loss: 4.914679]\n",
      "epoch:17 step:13517 [D loss: 0.321476, acc.: 88.28%] [G loss: 3.282449]\n",
      "epoch:17 step:13518 [D loss: 0.314864, acc.: 86.72%] [G loss: 2.449950]\n",
      "epoch:17 step:13519 [D loss: 0.225969, acc.: 89.84%] [G loss: 2.436472]\n",
      "epoch:17 step:13520 [D loss: 0.322001, acc.: 86.72%] [G loss: 3.190394]\n",
      "epoch:17 step:13521 [D loss: 0.268402, acc.: 87.50%] [G loss: 2.944000]\n",
      "epoch:17 step:13522 [D loss: 0.348448, acc.: 82.81%] [G loss: 3.467683]\n",
      "epoch:17 step:13523 [D loss: 0.401478, acc.: 81.25%] [G loss: 3.288609]\n",
      "epoch:17 step:13524 [D loss: 0.366737, acc.: 85.94%] [G loss: 3.768646]\n",
      "epoch:17 step:13525 [D loss: 0.183860, acc.: 92.19%] [G loss: 3.314878]\n",
      "epoch:17 step:13526 [D loss: 0.407585, acc.: 78.91%] [G loss: 2.961356]\n",
      "epoch:17 step:13527 [D loss: 0.302718, acc.: 87.50%] [G loss: 3.516397]\n",
      "epoch:17 step:13528 [D loss: 0.328010, acc.: 89.06%] [G loss: 3.273359]\n",
      "epoch:17 step:13529 [D loss: 0.342685, acc.: 86.72%] [G loss: 3.849156]\n",
      "epoch:17 step:13530 [D loss: 0.262862, acc.: 90.62%] [G loss: 3.040601]\n",
      "epoch:17 step:13531 [D loss: 0.268033, acc.: 90.62%] [G loss: 3.734429]\n",
      "epoch:17 step:13532 [D loss: 0.316461, acc.: 86.72%] [G loss: 3.170248]\n",
      "epoch:17 step:13533 [D loss: 0.330510, acc.: 84.38%] [G loss: 2.697223]\n",
      "epoch:17 step:13534 [D loss: 0.281837, acc.: 85.94%] [G loss: 2.911791]\n",
      "epoch:17 step:13535 [D loss: 0.244468, acc.: 92.97%] [G loss: 3.695522]\n",
      "epoch:17 step:13536 [D loss: 0.281476, acc.: 88.28%] [G loss: 2.974703]\n",
      "epoch:17 step:13537 [D loss: 0.240055, acc.: 89.06%] [G loss: 2.800416]\n",
      "epoch:17 step:13538 [D loss: 0.352040, acc.: 85.94%] [G loss: 2.396875]\n",
      "epoch:17 step:13539 [D loss: 0.303325, acc.: 88.28%] [G loss: 2.568753]\n",
      "epoch:17 step:13540 [D loss: 0.288930, acc.: 89.06%] [G loss: 3.330787]\n",
      "epoch:17 step:13541 [D loss: 0.297287, acc.: 89.06%] [G loss: 3.403000]\n",
      "epoch:17 step:13542 [D loss: 0.344073, acc.: 82.81%] [G loss: 4.012830]\n",
      "epoch:17 step:13543 [D loss: 0.337305, acc.: 85.16%] [G loss: 3.710094]\n",
      "epoch:17 step:13544 [D loss: 0.438766, acc.: 82.03%] [G loss: 3.069806]\n",
      "epoch:17 step:13545 [D loss: 0.362399, acc.: 78.12%] [G loss: 2.922068]\n",
      "epoch:17 step:13546 [D loss: 0.300273, acc.: 90.62%] [G loss: 3.297560]\n",
      "epoch:17 step:13547 [D loss: 0.324259, acc.: 87.50%] [G loss: 2.837033]\n",
      "epoch:17 step:13548 [D loss: 0.344496, acc.: 86.72%] [G loss: 3.200410]\n",
      "epoch:17 step:13549 [D loss: 0.214600, acc.: 91.41%] [G loss: 3.284484]\n",
      "epoch:17 step:13550 [D loss: 0.327316, acc.: 84.38%] [G loss: 3.001446]\n",
      "epoch:17 step:13551 [D loss: 0.237312, acc.: 89.06%] [G loss: 2.837460]\n",
      "epoch:17 step:13552 [D loss: 0.433391, acc.: 77.34%] [G loss: 5.115194]\n",
      "epoch:17 step:13553 [D loss: 0.493282, acc.: 75.00%] [G loss: 3.280761]\n",
      "epoch:17 step:13554 [D loss: 0.332044, acc.: 85.94%] [G loss: 3.567857]\n",
      "epoch:17 step:13555 [D loss: 0.229121, acc.: 92.97%] [G loss: 3.392277]\n",
      "epoch:17 step:13556 [D loss: 0.328245, acc.: 85.94%] [G loss: 5.080803]\n",
      "epoch:17 step:13557 [D loss: 0.277432, acc.: 89.84%] [G loss: 4.134295]\n",
      "epoch:17 step:13558 [D loss: 0.217175, acc.: 91.41%] [G loss: 6.337240]\n",
      "epoch:17 step:13559 [D loss: 0.203940, acc.: 90.62%] [G loss: 4.470002]\n",
      "epoch:17 step:13560 [D loss: 0.239681, acc.: 92.19%] [G loss: 5.788918]\n",
      "epoch:17 step:13561 [D loss: 0.195605, acc.: 92.97%] [G loss: 4.545809]\n",
      "epoch:17 step:13562 [D loss: 0.272589, acc.: 87.50%] [G loss: 4.542811]\n",
      "epoch:17 step:13563 [D loss: 0.370705, acc.: 84.38%] [G loss: 2.853654]\n",
      "epoch:17 step:13564 [D loss: 0.299527, acc.: 86.72%] [G loss: 3.858223]\n",
      "epoch:17 step:13565 [D loss: 0.470147, acc.: 79.69%] [G loss: 4.283579]\n",
      "epoch:17 step:13566 [D loss: 0.329633, acc.: 83.59%] [G loss: 3.605064]\n",
      "epoch:17 step:13567 [D loss: 0.279972, acc.: 89.06%] [G loss: 4.502568]\n",
      "epoch:17 step:13568 [D loss: 0.261233, acc.: 88.28%] [G loss: 3.347273]\n",
      "epoch:17 step:13569 [D loss: 0.297806, acc.: 85.16%] [G loss: 3.995704]\n",
      "epoch:17 step:13570 [D loss: 0.299642, acc.: 86.72%] [G loss: 4.168454]\n",
      "epoch:17 step:13571 [D loss: 0.396539, acc.: 81.25%] [G loss: 2.806087]\n",
      "epoch:17 step:13572 [D loss: 0.323661, acc.: 82.81%] [G loss: 3.797863]\n",
      "epoch:17 step:13573 [D loss: 0.340104, acc.: 89.06%] [G loss: 6.140225]\n",
      "epoch:17 step:13574 [D loss: 0.174794, acc.: 95.31%] [G loss: 4.173716]\n",
      "epoch:17 step:13575 [D loss: 0.223111, acc.: 92.19%] [G loss: 3.319279]\n",
      "epoch:17 step:13576 [D loss: 0.308890, acc.: 87.50%] [G loss: 3.460449]\n",
      "epoch:17 step:13577 [D loss: 0.306710, acc.: 86.72%] [G loss: 4.294814]\n",
      "epoch:17 step:13578 [D loss: 0.403525, acc.: 81.25%] [G loss: 3.264331]\n",
      "epoch:17 step:13579 [D loss: 0.296152, acc.: 85.94%] [G loss: 4.067447]\n",
      "epoch:17 step:13580 [D loss: 0.280943, acc.: 85.94%] [G loss: 3.301822]\n",
      "epoch:17 step:13581 [D loss: 0.281447, acc.: 89.06%] [G loss: 3.224980]\n",
      "epoch:17 step:13582 [D loss: 0.262824, acc.: 90.62%] [G loss: 2.937903]\n",
      "epoch:17 step:13583 [D loss: 0.275492, acc.: 89.84%] [G loss: 3.347108]\n",
      "epoch:17 step:13584 [D loss: 0.252710, acc.: 90.62%] [G loss: 2.624645]\n",
      "epoch:17 step:13585 [D loss: 0.300538, acc.: 86.72%] [G loss: 4.368696]\n",
      "epoch:17 step:13586 [D loss: 0.355561, acc.: 86.72%] [G loss: 2.785566]\n",
      "epoch:17 step:13587 [D loss: 0.360517, acc.: 85.16%] [G loss: 4.440161]\n",
      "epoch:17 step:13588 [D loss: 0.402399, acc.: 83.59%] [G loss: 3.001395]\n",
      "epoch:17 step:13589 [D loss: 0.374382, acc.: 81.25%] [G loss: 6.233277]\n",
      "epoch:17 step:13590 [D loss: 0.466907, acc.: 80.47%] [G loss: 5.214430]\n",
      "epoch:17 step:13591 [D loss: 0.648421, acc.: 69.53%] [G loss: 3.538147]\n",
      "epoch:17 step:13592 [D loss: 0.429552, acc.: 75.78%] [G loss: 3.313597]\n",
      "epoch:17 step:13593 [D loss: 0.198795, acc.: 96.88%] [G loss: 4.367998]\n",
      "epoch:17 step:13594 [D loss: 0.378168, acc.: 84.38%] [G loss: 3.362614]\n",
      "epoch:17 step:13595 [D loss: 0.362453, acc.: 82.81%] [G loss: 2.598267]\n",
      "epoch:17 step:13596 [D loss: 0.290180, acc.: 89.06%] [G loss: 3.209872]\n",
      "epoch:17 step:13597 [D loss: 0.344924, acc.: 84.38%] [G loss: 2.862719]\n",
      "epoch:17 step:13598 [D loss: 0.286083, acc.: 89.06%] [G loss: 3.984059]\n",
      "epoch:17 step:13599 [D loss: 0.248528, acc.: 90.62%] [G loss: 4.118607]\n",
      "epoch:17 step:13600 [D loss: 0.237889, acc.: 90.62%] [G loss: 4.465534]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##############\n",
      "[0.84872705 0.85141006 0.80631453 0.78272918 0.77369718 0.83374497\n",
      " 0.89362067 0.82678254 0.82153776 0.80424003]\n",
      "##########\n",
      "epoch:17 step:13601 [D loss: 0.264363, acc.: 89.84%] [G loss: 5.401329]\n",
      "epoch:17 step:13602 [D loss: 0.419901, acc.: 77.34%] [G loss: 3.222693]\n",
      "epoch:17 step:13603 [D loss: 0.476659, acc.: 83.59%] [G loss: 6.355286]\n",
      "epoch:17 step:13604 [D loss: 0.272146, acc.: 89.84%] [G loss: 5.429139]\n",
      "epoch:17 step:13605 [D loss: 0.316560, acc.: 88.28%] [G loss: 3.506387]\n",
      "epoch:17 step:13606 [D loss: 0.309396, acc.: 88.28%] [G loss: 3.229199]\n",
      "epoch:17 step:13607 [D loss: 0.275943, acc.: 90.62%] [G loss: 3.116688]\n",
      "epoch:17 step:13608 [D loss: 0.362282, acc.: 83.59%] [G loss: 3.315308]\n",
      "epoch:17 step:13609 [D loss: 0.390367, acc.: 84.38%] [G loss: 3.998379]\n",
      "epoch:17 step:13610 [D loss: 0.353540, acc.: 83.59%] [G loss: 3.971173]\n",
      "epoch:17 step:13611 [D loss: 0.373899, acc.: 82.81%] [G loss: 3.384262]\n",
      "epoch:17 step:13612 [D loss: 0.310574, acc.: 85.94%] [G loss: 2.946701]\n",
      "epoch:17 step:13613 [D loss: 0.218718, acc.: 92.19%] [G loss: 3.207619]\n",
      "epoch:17 step:13614 [D loss: 0.497743, acc.: 78.12%] [G loss: 3.982283]\n",
      "epoch:17 step:13615 [D loss: 0.352054, acc.: 82.03%] [G loss: 5.130899]\n",
      "epoch:17 step:13616 [D loss: 0.179198, acc.: 92.19%] [G loss: 6.729532]\n",
      "epoch:17 step:13617 [D loss: 0.247911, acc.: 89.84%] [G loss: 5.959463]\n",
      "epoch:17 step:13618 [D loss: 0.232367, acc.: 90.62%] [G loss: 6.058794]\n",
      "epoch:17 step:13619 [D loss: 0.215002, acc.: 88.28%] [G loss: 5.566578]\n",
      "epoch:17 step:13620 [D loss: 0.147442, acc.: 94.53%] [G loss: 6.059637]\n",
      "epoch:17 step:13621 [D loss: 0.231244, acc.: 89.84%] [G loss: 6.516206]\n",
      "epoch:17 step:13622 [D loss: 0.199603, acc.: 89.06%] [G loss: 6.866389]\n",
      "epoch:17 step:13623 [D loss: 0.296708, acc.: 89.06%] [G loss: 3.257139]\n",
      "epoch:17 step:13624 [D loss: 0.209927, acc.: 93.75%] [G loss: 4.211076]\n",
      "epoch:17 step:13625 [D loss: 0.337514, acc.: 88.28%] [G loss: 5.136491]\n",
      "epoch:17 step:13626 [D loss: 0.296388, acc.: 90.62%] [G loss: 3.470649]\n",
      "epoch:17 step:13627 [D loss: 0.387878, acc.: 82.81%] [G loss: 3.949117]\n",
      "epoch:17 step:13628 [D loss: 0.267726, acc.: 89.06%] [G loss: 4.200479]\n",
      "epoch:17 step:13629 [D loss: 0.336136, acc.: 82.81%] [G loss: 4.551380]\n",
      "epoch:17 step:13630 [D loss: 0.230973, acc.: 92.19%] [G loss: 3.424516]\n",
      "epoch:17 step:13631 [D loss: 0.268828, acc.: 88.28%] [G loss: 3.832724]\n",
      "epoch:17 step:13632 [D loss: 0.256461, acc.: 89.84%] [G loss: 2.781412]\n",
      "epoch:17 step:13633 [D loss: 0.284864, acc.: 87.50%] [G loss: 3.630275]\n",
      "epoch:17 step:13634 [D loss: 0.341699, acc.: 82.81%] [G loss: 3.643960]\n",
      "epoch:17 step:13635 [D loss: 0.386076, acc.: 83.59%] [G loss: 3.332497]\n",
      "epoch:17 step:13636 [D loss: 0.280118, acc.: 89.84%] [G loss: 2.693586]\n",
      "epoch:17 step:13637 [D loss: 0.296424, acc.: 89.84%] [G loss: 2.679446]\n",
      "epoch:17 step:13638 [D loss: 0.328479, acc.: 85.94%] [G loss: 2.807072]\n",
      "epoch:17 step:13639 [D loss: 0.313321, acc.: 87.50%] [G loss: 3.646282]\n",
      "epoch:17 step:13640 [D loss: 0.369788, acc.: 82.81%] [G loss: 2.701275]\n",
      "epoch:17 step:13641 [D loss: 0.256752, acc.: 89.84%] [G loss: 3.441355]\n",
      "epoch:17 step:13642 [D loss: 0.260401, acc.: 87.50%] [G loss: 3.873778]\n",
      "epoch:17 step:13643 [D loss: 0.290564, acc.: 86.72%] [G loss: 2.526957]\n",
      "epoch:17 step:13644 [D loss: 0.354671, acc.: 82.81%] [G loss: 6.079663]\n",
      "epoch:17 step:13645 [D loss: 0.410990, acc.: 81.25%] [G loss: 7.162546]\n",
      "epoch:17 step:13646 [D loss: 0.418320, acc.: 80.47%] [G loss: 6.699569]\n",
      "epoch:17 step:13647 [D loss: 0.531442, acc.: 82.03%] [G loss: 7.735415]\n",
      "epoch:17 step:13648 [D loss: 0.454267, acc.: 82.81%] [G loss: 2.828517]\n",
      "epoch:17 step:13649 [D loss: 0.246811, acc.: 89.84%] [G loss: 4.771281]\n",
      "epoch:17 step:13650 [D loss: 0.196102, acc.: 95.31%] [G loss: 4.917669]\n",
      "epoch:17 step:13651 [D loss: 0.322612, acc.: 87.50%] [G loss: 4.542908]\n",
      "epoch:17 step:13652 [D loss: 0.333862, acc.: 82.03%] [G loss: 2.666755]\n",
      "epoch:17 step:13653 [D loss: 0.226893, acc.: 90.62%] [G loss: 3.712653]\n",
      "epoch:17 step:13654 [D loss: 0.389822, acc.: 83.59%] [G loss: 3.257658]\n",
      "epoch:17 step:13655 [D loss: 0.402187, acc.: 81.25%] [G loss: 2.324719]\n",
      "epoch:17 step:13656 [D loss: 0.302479, acc.: 89.06%] [G loss: 3.829613]\n",
      "epoch:17 step:13657 [D loss: 0.273544, acc.: 88.28%] [G loss: 3.515795]\n",
      "epoch:17 step:13658 [D loss: 0.276709, acc.: 89.84%] [G loss: 3.900578]\n",
      "epoch:17 step:13659 [D loss: 0.291216, acc.: 86.72%] [G loss: 2.957078]\n",
      "epoch:17 step:13660 [D loss: 0.210097, acc.: 91.41%] [G loss: 4.196134]\n",
      "epoch:17 step:13661 [D loss: 0.279064, acc.: 89.06%] [G loss: 2.991658]\n",
      "epoch:17 step:13662 [D loss: 0.228670, acc.: 90.62%] [G loss: 3.981725]\n",
      "epoch:17 step:13663 [D loss: 0.346458, acc.: 82.81%] [G loss: 3.841564]\n",
      "epoch:17 step:13664 [D loss: 0.303516, acc.: 85.16%] [G loss: 2.881450]\n",
      "epoch:17 step:13665 [D loss: 0.353205, acc.: 87.50%] [G loss: 2.965664]\n",
      "epoch:17 step:13666 [D loss: 0.350123, acc.: 85.16%] [G loss: 2.919903]\n",
      "epoch:17 step:13667 [D loss: 0.282187, acc.: 87.50%] [G loss: 3.629796]\n",
      "epoch:17 step:13668 [D loss: 0.381208, acc.: 82.03%] [G loss: 4.502454]\n",
      "epoch:17 step:13669 [D loss: 0.189142, acc.: 93.75%] [G loss: 3.835198]\n",
      "epoch:17 step:13670 [D loss: 0.241297, acc.: 90.62%] [G loss: 3.921990]\n",
      "epoch:17 step:13671 [D loss: 0.342842, acc.: 85.94%] [G loss: 3.932397]\n",
      "epoch:17 step:13672 [D loss: 0.268807, acc.: 91.41%] [G loss: 3.674099]\n",
      "epoch:17 step:13673 [D loss: 0.273660, acc.: 90.62%] [G loss: 3.345041]\n",
      "epoch:17 step:13674 [D loss: 0.327222, acc.: 85.16%] [G loss: 4.245552]\n",
      "epoch:17 step:13675 [D loss: 0.242833, acc.: 91.41%] [G loss: 3.890294]\n",
      "epoch:17 step:13676 [D loss: 0.278943, acc.: 87.50%] [G loss: 2.577033]\n",
      "epoch:17 step:13677 [D loss: 0.283026, acc.: 88.28%] [G loss: 4.053005]\n",
      "epoch:17 step:13678 [D loss: 0.250890, acc.: 89.06%] [G loss: 4.453622]\n",
      "epoch:17 step:13679 [D loss: 0.238784, acc.: 90.62%] [G loss: 3.988922]\n",
      "epoch:17 step:13680 [D loss: 0.265961, acc.: 89.84%] [G loss: 2.841894]\n",
      "epoch:17 step:13681 [D loss: 0.300247, acc.: 92.19%] [G loss: 3.774576]\n",
      "epoch:17 step:13682 [D loss: 0.280867, acc.: 86.72%] [G loss: 3.387577]\n",
      "epoch:17 step:13683 [D loss: 0.277017, acc.: 86.72%] [G loss: 4.738898]\n",
      "epoch:17 step:13684 [D loss: 0.275301, acc.: 89.06%] [G loss: 3.019767]\n",
      "epoch:17 step:13685 [D loss: 0.235540, acc.: 89.84%] [G loss: 4.013232]\n",
      "epoch:17 step:13686 [D loss: 0.245160, acc.: 89.06%] [G loss: 4.087896]\n",
      "epoch:17 step:13687 [D loss: 0.265398, acc.: 89.84%] [G loss: 3.975104]\n",
      "epoch:17 step:13688 [D loss: 0.469653, acc.: 77.34%] [G loss: 2.421043]\n",
      "epoch:17 step:13689 [D loss: 0.406261, acc.: 82.81%] [G loss: 5.201449]\n",
      "epoch:17 step:13690 [D loss: 0.404778, acc.: 78.91%] [G loss: 6.351342]\n",
      "epoch:17 step:13691 [D loss: 0.505561, acc.: 79.69%] [G loss: 2.935139]\n",
      "epoch:17 step:13692 [D loss: 0.293870, acc.: 86.72%] [G loss: 2.961520]\n",
      "epoch:17 step:13693 [D loss: 0.273491, acc.: 89.84%] [G loss: 4.033864]\n",
      "epoch:17 step:13694 [D loss: 0.425596, acc.: 81.25%] [G loss: 4.132900]\n",
      "epoch:17 step:13695 [D loss: 0.555520, acc.: 80.47%] [G loss: 3.949500]\n",
      "epoch:17 step:13696 [D loss: 0.330806, acc.: 87.50%] [G loss: 2.861651]\n",
      "epoch:17 step:13697 [D loss: 0.182711, acc.: 92.19%] [G loss: 4.950385]\n",
      "epoch:17 step:13698 [D loss: 0.278250, acc.: 90.62%] [G loss: 3.762162]\n",
      "epoch:17 step:13699 [D loss: 0.252928, acc.: 89.84%] [G loss: 3.929381]\n",
      "epoch:17 step:13700 [D loss: 0.272646, acc.: 87.50%] [G loss: 3.885199]\n",
      "epoch:17 step:13701 [D loss: 0.304255, acc.: 90.62%] [G loss: 2.906782]\n",
      "epoch:17 step:13702 [D loss: 0.328496, acc.: 84.38%] [G loss: 2.939902]\n",
      "epoch:17 step:13703 [D loss: 0.271781, acc.: 89.06%] [G loss: 3.722001]\n",
      "epoch:17 step:13704 [D loss: 0.280912, acc.: 85.16%] [G loss: 3.918608]\n",
      "epoch:17 step:13705 [D loss: 0.320629, acc.: 85.16%] [G loss: 6.961494]\n",
      "epoch:17 step:13706 [D loss: 0.368567, acc.: 85.94%] [G loss: 6.901072]\n",
      "epoch:17 step:13707 [D loss: 0.311602, acc.: 85.16%] [G loss: 5.882795]\n",
      "epoch:17 step:13708 [D loss: 0.282178, acc.: 88.28%] [G loss: 4.123394]\n",
      "epoch:17 step:13709 [D loss: 0.331274, acc.: 85.16%] [G loss: 3.610707]\n",
      "epoch:17 step:13710 [D loss: 0.206760, acc.: 93.75%] [G loss: 3.810033]\n",
      "epoch:17 step:13711 [D loss: 0.319220, acc.: 88.28%] [G loss: 2.912180]\n",
      "epoch:17 step:13712 [D loss: 0.346209, acc.: 85.16%] [G loss: 3.316080]\n",
      "epoch:17 step:13713 [D loss: 0.445888, acc.: 82.81%] [G loss: 5.056826]\n",
      "epoch:17 step:13714 [D loss: 0.534233, acc.: 80.47%] [G loss: 4.485844]\n",
      "epoch:17 step:13715 [D loss: 0.656399, acc.: 72.66%] [G loss: 4.767542]\n",
      "epoch:17 step:13716 [D loss: 0.797097, acc.: 71.09%] [G loss: 3.086103]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:17 step:13717 [D loss: 0.298207, acc.: 88.28%] [G loss: 6.673739]\n",
      "epoch:17 step:13718 [D loss: 0.405733, acc.: 83.59%] [G loss: 3.375813]\n",
      "epoch:17 step:13719 [D loss: 0.190663, acc.: 93.75%] [G loss: 4.247788]\n",
      "epoch:17 step:13720 [D loss: 0.278997, acc.: 89.84%] [G loss: 3.885360]\n",
      "epoch:17 step:13721 [D loss: 0.393736, acc.: 81.25%] [G loss: 2.626203]\n",
      "epoch:17 step:13722 [D loss: 0.270465, acc.: 86.72%] [G loss: 4.160391]\n",
      "epoch:17 step:13723 [D loss: 0.287447, acc.: 88.28%] [G loss: 4.097049]\n",
      "epoch:17 step:13724 [D loss: 0.232577, acc.: 92.97%] [G loss: 3.501448]\n",
      "epoch:17 step:13725 [D loss: 0.340681, acc.: 82.03%] [G loss: 2.968667]\n",
      "epoch:17 step:13726 [D loss: 0.359492, acc.: 83.59%] [G loss: 3.024719]\n",
      "epoch:17 step:13727 [D loss: 0.297819, acc.: 86.72%] [G loss: 3.306002]\n",
      "epoch:17 step:13728 [D loss: 0.179345, acc.: 92.97%] [G loss: 3.880801]\n",
      "epoch:17 step:13729 [D loss: 0.285009, acc.: 85.16%] [G loss: 2.591504]\n",
      "epoch:17 step:13730 [D loss: 0.268367, acc.: 88.28%] [G loss: 2.724847]\n",
      "epoch:17 step:13731 [D loss: 0.253602, acc.: 89.06%] [G loss: 2.783017]\n",
      "epoch:17 step:13732 [D loss: 0.302047, acc.: 85.94%] [G loss: 3.300415]\n",
      "epoch:17 step:13733 [D loss: 0.269572, acc.: 89.84%] [G loss: 3.249030]\n",
      "epoch:17 step:13734 [D loss: 0.287657, acc.: 88.28%] [G loss: 2.759597]\n",
      "epoch:17 step:13735 [D loss: 0.292729, acc.: 88.28%] [G loss: 3.065662]\n",
      "epoch:17 step:13736 [D loss: 0.274693, acc.: 87.50%] [G loss: 2.984117]\n",
      "epoch:17 step:13737 [D loss: 0.290473, acc.: 89.84%] [G loss: 3.440022]\n",
      "epoch:17 step:13738 [D loss: 0.239529, acc.: 89.06%] [G loss: 3.074089]\n",
      "epoch:17 step:13739 [D loss: 0.288361, acc.: 88.28%] [G loss: 3.836333]\n",
      "epoch:17 step:13740 [D loss: 0.221702, acc.: 92.19%] [G loss: 3.384452]\n",
      "epoch:17 step:13741 [D loss: 0.352908, acc.: 83.59%] [G loss: 3.949636]\n",
      "epoch:17 step:13742 [D loss: 0.225804, acc.: 92.97%] [G loss: 3.326183]\n",
      "epoch:17 step:13743 [D loss: 0.345823, acc.: 85.16%] [G loss: 2.183516]\n",
      "epoch:17 step:13744 [D loss: 0.295780, acc.: 90.62%] [G loss: 2.407693]\n",
      "epoch:17 step:13745 [D loss: 0.326570, acc.: 89.84%] [G loss: 3.375914]\n",
      "epoch:17 step:13746 [D loss: 0.244966, acc.: 92.19%] [G loss: 3.864405]\n",
      "epoch:17 step:13747 [D loss: 0.225610, acc.: 92.97%] [G loss: 3.375791]\n",
      "epoch:17 step:13748 [D loss: 0.267427, acc.: 89.84%] [G loss: 3.293531]\n",
      "epoch:17 step:13749 [D loss: 0.247050, acc.: 89.84%] [G loss: 3.541793]\n",
      "epoch:17 step:13750 [D loss: 0.204314, acc.: 92.19%] [G loss: 3.156240]\n",
      "epoch:17 step:13751 [D loss: 0.250459, acc.: 88.28%] [G loss: 2.625998]\n",
      "epoch:17 step:13752 [D loss: 0.301424, acc.: 92.19%] [G loss: 4.079528]\n",
      "epoch:17 step:13753 [D loss: 0.244140, acc.: 91.41%] [G loss: 3.607837]\n",
      "epoch:17 step:13754 [D loss: 0.316989, acc.: 89.84%] [G loss: 3.482370]\n",
      "epoch:17 step:13755 [D loss: 0.258910, acc.: 88.28%] [G loss: 4.644875]\n",
      "epoch:17 step:13756 [D loss: 0.265240, acc.: 86.72%] [G loss: 4.395797]\n",
      "epoch:17 step:13757 [D loss: 0.279162, acc.: 86.72%] [G loss: 3.592576]\n",
      "epoch:17 step:13758 [D loss: 0.316530, acc.: 89.06%] [G loss: 2.786318]\n",
      "epoch:17 step:13759 [D loss: 0.316114, acc.: 85.16%] [G loss: 2.828038]\n",
      "epoch:17 step:13760 [D loss: 0.186771, acc.: 92.19%] [G loss: 3.366643]\n",
      "epoch:17 step:13761 [D loss: 0.353635, acc.: 85.94%] [G loss: 3.168004]\n",
      "epoch:17 step:13762 [D loss: 0.285328, acc.: 89.84%] [G loss: 2.486392]\n",
      "epoch:17 step:13763 [D loss: 0.262627, acc.: 92.97%] [G loss: 3.040563]\n",
      "epoch:17 step:13764 [D loss: 0.349464, acc.: 86.72%] [G loss: 4.334005]\n",
      "epoch:17 step:13765 [D loss: 0.277222, acc.: 89.06%] [G loss: 4.554768]\n",
      "epoch:17 step:13766 [D loss: 0.282195, acc.: 86.72%] [G loss: 2.678484]\n",
      "epoch:17 step:13767 [D loss: 0.307735, acc.: 86.72%] [G loss: 2.423833]\n",
      "epoch:17 step:13768 [D loss: 0.306142, acc.: 87.50%] [G loss: 3.409321]\n",
      "epoch:17 step:13769 [D loss: 0.380787, acc.: 78.91%] [G loss: 2.719710]\n",
      "epoch:17 step:13770 [D loss: 0.255758, acc.: 89.84%] [G loss: 2.974620]\n",
      "epoch:17 step:13771 [D loss: 0.252628, acc.: 89.84%] [G loss: 3.083163]\n",
      "epoch:17 step:13772 [D loss: 0.302458, acc.: 91.41%] [G loss: 2.872537]\n",
      "epoch:17 step:13773 [D loss: 0.271146, acc.: 88.28%] [G loss: 2.635911]\n",
      "epoch:17 step:13774 [D loss: 0.336271, acc.: 84.38%] [G loss: 2.817820]\n",
      "epoch:17 step:13775 [D loss: 0.315338, acc.: 86.72%] [G loss: 3.876829]\n",
      "epoch:17 step:13776 [D loss: 0.317403, acc.: 87.50%] [G loss: 4.834647]\n",
      "epoch:17 step:13777 [D loss: 0.332188, acc.: 88.28%] [G loss: 3.383792]\n",
      "epoch:17 step:13778 [D loss: 0.257049, acc.: 89.84%] [G loss: 4.292899]\n",
      "epoch:17 step:13779 [D loss: 0.308529, acc.: 86.72%] [G loss: 3.623827]\n",
      "epoch:17 step:13780 [D loss: 0.296172, acc.: 84.38%] [G loss: 3.729497]\n",
      "epoch:17 step:13781 [D loss: 0.187734, acc.: 93.75%] [G loss: 5.698497]\n",
      "epoch:17 step:13782 [D loss: 0.206195, acc.: 92.97%] [G loss: 6.975618]\n",
      "epoch:17 step:13783 [D loss: 0.292810, acc.: 85.16%] [G loss: 5.613970]\n",
      "epoch:17 step:13784 [D loss: 0.247893, acc.: 90.62%] [G loss: 5.279655]\n",
      "epoch:17 step:13785 [D loss: 0.352723, acc.: 81.25%] [G loss: 3.808103]\n",
      "epoch:17 step:13786 [D loss: 0.372353, acc.: 85.16%] [G loss: 2.407562]\n",
      "epoch:17 step:13787 [D loss: 0.221379, acc.: 92.19%] [G loss: 3.949254]\n",
      "epoch:17 step:13788 [D loss: 0.264557, acc.: 89.84%] [G loss: 4.192156]\n",
      "epoch:17 step:13789 [D loss: 0.222575, acc.: 91.41%] [G loss: 3.789140]\n",
      "epoch:17 step:13790 [D loss: 0.291234, acc.: 84.38%] [G loss: 7.760491]\n",
      "epoch:17 step:13791 [D loss: 0.472102, acc.: 77.34%] [G loss: 4.514539]\n",
      "epoch:17 step:13792 [D loss: 0.502871, acc.: 82.81%] [G loss: 4.063708]\n",
      "epoch:17 step:13793 [D loss: 0.255592, acc.: 90.62%] [G loss: 4.348863]\n",
      "epoch:17 step:13794 [D loss: 0.383990, acc.: 84.38%] [G loss: 3.944448]\n",
      "epoch:17 step:13795 [D loss: 0.242485, acc.: 89.06%] [G loss: 3.355615]\n",
      "epoch:17 step:13796 [D loss: 0.280794, acc.: 86.72%] [G loss: 4.511541]\n",
      "epoch:17 step:13797 [D loss: 0.356331, acc.: 85.94%] [G loss: 3.399600]\n",
      "epoch:17 step:13798 [D loss: 0.232952, acc.: 89.06%] [G loss: 3.037102]\n",
      "epoch:17 step:13799 [D loss: 0.335434, acc.: 88.28%] [G loss: 3.205056]\n",
      "epoch:17 step:13800 [D loss: 0.275715, acc.: 85.94%] [G loss: 3.822036]\n",
      "##############\n",
      "[0.8651199  0.83900704 0.79898181 0.80724794 0.791378   0.81605564\n",
      " 0.87614244 0.8489134  0.78963493 0.82072479]\n",
      "##########\n",
      "epoch:17 step:13801 [D loss: 0.308398, acc.: 88.28%] [G loss: 3.552055]\n",
      "epoch:17 step:13802 [D loss: 0.301728, acc.: 82.81%] [G loss: 5.879609]\n",
      "epoch:17 step:13803 [D loss: 0.429371, acc.: 82.03%] [G loss: 3.554716]\n",
      "epoch:17 step:13804 [D loss: 0.335635, acc.: 85.16%] [G loss: 3.925936]\n",
      "epoch:17 step:13805 [D loss: 0.202568, acc.: 90.62%] [G loss: 4.820028]\n",
      "epoch:17 step:13806 [D loss: 0.307668, acc.: 85.16%] [G loss: 4.243010]\n",
      "epoch:17 step:13807 [D loss: 0.260214, acc.: 85.94%] [G loss: 3.818942]\n",
      "epoch:17 step:13808 [D loss: 0.275915, acc.: 86.72%] [G loss: 4.357200]\n",
      "epoch:17 step:13809 [D loss: 0.276213, acc.: 88.28%] [G loss: 5.259973]\n",
      "epoch:17 step:13810 [D loss: 0.231852, acc.: 91.41%] [G loss: 4.070901]\n",
      "epoch:17 step:13811 [D loss: 0.202877, acc.: 94.53%] [G loss: 6.637844]\n",
      "epoch:17 step:13812 [D loss: 0.225305, acc.: 90.62%] [G loss: 4.546544]\n",
      "epoch:17 step:13813 [D loss: 0.290132, acc.: 85.16%] [G loss: 3.828025]\n",
      "epoch:17 step:13814 [D loss: 0.273636, acc.: 86.72%] [G loss: 3.091230]\n",
      "epoch:17 step:13815 [D loss: 0.329437, acc.: 83.59%] [G loss: 2.844540]\n",
      "epoch:17 step:13816 [D loss: 0.251359, acc.: 89.06%] [G loss: 2.891105]\n",
      "epoch:17 step:13817 [D loss: 0.310958, acc.: 88.28%] [G loss: 3.038399]\n",
      "epoch:17 step:13818 [D loss: 0.383681, acc.: 85.16%] [G loss: 2.842587]\n",
      "epoch:17 step:13819 [D loss: 0.306954, acc.: 87.50%] [G loss: 3.368486]\n",
      "epoch:17 step:13820 [D loss: 0.234703, acc.: 92.97%] [G loss: 5.121662]\n",
      "epoch:17 step:13821 [D loss: 0.216182, acc.: 91.41%] [G loss: 5.127538]\n",
      "epoch:17 step:13822 [D loss: 0.360955, acc.: 86.72%] [G loss: 2.606032]\n",
      "epoch:17 step:13823 [D loss: 0.265116, acc.: 86.72%] [G loss: 3.248195]\n",
      "epoch:17 step:13824 [D loss: 0.311496, acc.: 87.50%] [G loss: 3.839117]\n",
      "epoch:17 step:13825 [D loss: 0.190460, acc.: 92.19%] [G loss: 3.685051]\n",
      "epoch:17 step:13826 [D loss: 0.208713, acc.: 93.75%] [G loss: 3.059975]\n",
      "epoch:17 step:13827 [D loss: 0.302744, acc.: 86.72%] [G loss: 3.902961]\n",
      "epoch:17 step:13828 [D loss: 0.181679, acc.: 93.75%] [G loss: 4.172107]\n",
      "epoch:17 step:13829 [D loss: 0.315968, acc.: 86.72%] [G loss: 4.003839]\n",
      "epoch:17 step:13830 [D loss: 0.242789, acc.: 89.84%] [G loss: 3.486239]\n",
      "epoch:17 step:13831 [D loss: 0.279787, acc.: 89.84%] [G loss: 5.315852]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:17 step:13832 [D loss: 0.400028, acc.: 82.81%] [G loss: 6.428291]\n",
      "epoch:17 step:13833 [D loss: 0.565383, acc.: 73.44%] [G loss: 7.302365]\n",
      "epoch:17 step:13834 [D loss: 1.276645, acc.: 67.97%] [G loss: 8.285897]\n",
      "epoch:17 step:13835 [D loss: 1.155006, acc.: 66.41%] [G loss: 5.354601]\n",
      "epoch:17 step:13836 [D loss: 0.564497, acc.: 82.81%] [G loss: 4.607409]\n",
      "epoch:17 step:13837 [D loss: 0.513048, acc.: 81.25%] [G loss: 3.982090]\n",
      "epoch:17 step:13838 [D loss: 0.475025, acc.: 77.34%] [G loss: 3.842964]\n",
      "epoch:17 step:13839 [D loss: 0.393224, acc.: 83.59%] [G loss: 3.204821]\n",
      "epoch:17 step:13840 [D loss: 0.286402, acc.: 90.62%] [G loss: 2.605387]\n",
      "epoch:17 step:13841 [D loss: 0.234667, acc.: 88.28%] [G loss: 4.176472]\n",
      "epoch:17 step:13842 [D loss: 0.306787, acc.: 86.72%] [G loss: 3.834400]\n",
      "epoch:17 step:13843 [D loss: 0.413146, acc.: 80.47%] [G loss: 4.122447]\n",
      "epoch:17 step:13844 [D loss: 0.276486, acc.: 92.19%] [G loss: 4.119348]\n",
      "epoch:17 step:13845 [D loss: 0.272270, acc.: 89.84%] [G loss: 3.520782]\n",
      "epoch:17 step:13846 [D loss: 0.174910, acc.: 92.19%] [G loss: 6.496894]\n",
      "epoch:17 step:13847 [D loss: 0.257167, acc.: 89.06%] [G loss: 5.369310]\n",
      "epoch:17 step:13848 [D loss: 0.229799, acc.: 88.28%] [G loss: 3.303493]\n",
      "epoch:17 step:13849 [D loss: 0.291077, acc.: 92.19%] [G loss: 3.063763]\n",
      "epoch:17 step:13850 [D loss: 0.321054, acc.: 87.50%] [G loss: 4.133678]\n",
      "epoch:17 step:13851 [D loss: 0.380238, acc.: 85.94%] [G loss: 5.225292]\n",
      "epoch:17 step:13852 [D loss: 0.400759, acc.: 82.81%] [G loss: 3.467197]\n",
      "epoch:17 step:13853 [D loss: 0.295818, acc.: 88.28%] [G loss: 3.904291]\n",
      "epoch:17 step:13854 [D loss: 0.339982, acc.: 82.81%] [G loss: 3.577460]\n",
      "epoch:17 step:13855 [D loss: 0.477897, acc.: 84.38%] [G loss: 3.548537]\n",
      "epoch:17 step:13856 [D loss: 0.332380, acc.: 82.81%] [G loss: 2.916461]\n",
      "epoch:17 step:13857 [D loss: 0.270076, acc.: 90.62%] [G loss: 3.792097]\n",
      "epoch:17 step:13858 [D loss: 0.347897, acc.: 85.94%] [G loss: 3.289585]\n",
      "epoch:17 step:13859 [D loss: 0.299275, acc.: 86.72%] [G loss: 3.073229]\n",
      "epoch:17 step:13860 [D loss: 0.355302, acc.: 85.94%] [G loss: 2.908852]\n",
      "epoch:17 step:13861 [D loss: 0.328931, acc.: 86.72%] [G loss: 2.781672]\n",
      "epoch:17 step:13862 [D loss: 0.315644, acc.: 87.50%] [G loss: 3.643872]\n",
      "epoch:17 step:13863 [D loss: 0.360033, acc.: 82.03%] [G loss: 5.280599]\n",
      "epoch:17 step:13864 [D loss: 0.226841, acc.: 92.19%] [G loss: 3.186024]\n",
      "epoch:17 step:13865 [D loss: 0.307209, acc.: 84.38%] [G loss: 2.771405]\n",
      "epoch:17 step:13866 [D loss: 0.430820, acc.: 81.25%] [G loss: 3.226731]\n",
      "epoch:17 step:13867 [D loss: 0.419611, acc.: 79.69%] [G loss: 3.264125]\n",
      "epoch:17 step:13868 [D loss: 0.444557, acc.: 75.78%] [G loss: 3.146779]\n",
      "epoch:17 step:13869 [D loss: 0.357151, acc.: 84.38%] [G loss: 2.100414]\n",
      "epoch:17 step:13870 [D loss: 0.333919, acc.: 86.72%] [G loss: 2.768794]\n",
      "epoch:17 step:13871 [D loss: 0.281278, acc.: 89.06%] [G loss: 3.245175]\n",
      "epoch:17 step:13872 [D loss: 0.393732, acc.: 82.81%] [G loss: 2.780985]\n",
      "epoch:17 step:13873 [D loss: 0.279501, acc.: 89.06%] [G loss: 2.820643]\n",
      "epoch:17 step:13874 [D loss: 0.324644, acc.: 83.59%] [G loss: 4.217739]\n",
      "epoch:17 step:13875 [D loss: 0.343015, acc.: 83.59%] [G loss: 3.864786]\n",
      "epoch:17 step:13876 [D loss: 0.295085, acc.: 84.38%] [G loss: 3.278278]\n",
      "epoch:17 step:13877 [D loss: 0.297151, acc.: 88.28%] [G loss: 2.879329]\n",
      "epoch:17 step:13878 [D loss: 0.186835, acc.: 92.97%] [G loss: 2.897573]\n",
      "epoch:17 step:13879 [D loss: 0.328776, acc.: 87.50%] [G loss: 3.216049]\n",
      "epoch:17 step:13880 [D loss: 0.199724, acc.: 91.41%] [G loss: 2.983468]\n",
      "epoch:17 step:13881 [D loss: 0.339835, acc.: 83.59%] [G loss: 3.533627]\n",
      "epoch:17 step:13882 [D loss: 0.308414, acc.: 83.59%] [G loss: 3.204350]\n",
      "epoch:17 step:13883 [D loss: 0.393441, acc.: 78.12%] [G loss: 4.967912]\n",
      "epoch:17 step:13884 [D loss: 0.237791, acc.: 91.41%] [G loss: 4.105585]\n",
      "epoch:17 step:13885 [D loss: 0.333654, acc.: 88.28%] [G loss: 3.631371]\n",
      "epoch:17 step:13886 [D loss: 0.301914, acc.: 89.06%] [G loss: 2.604046]\n",
      "epoch:17 step:13887 [D loss: 0.287701, acc.: 89.84%] [G loss: 3.121271]\n",
      "epoch:17 step:13888 [D loss: 0.249243, acc.: 88.28%] [G loss: 4.125165]\n",
      "epoch:17 step:13889 [D loss: 0.240282, acc.: 90.62%] [G loss: 3.443091]\n",
      "epoch:17 step:13890 [D loss: 0.324476, acc.: 86.72%] [G loss: 2.560106]\n",
      "epoch:17 step:13891 [D loss: 0.268534, acc.: 89.84%] [G loss: 3.910746]\n",
      "epoch:17 step:13892 [D loss: 0.298899, acc.: 85.16%] [G loss: 3.177001]\n",
      "epoch:17 step:13893 [D loss: 0.258372, acc.: 89.84%] [G loss: 2.964594]\n",
      "epoch:17 step:13894 [D loss: 0.311076, acc.: 87.50%] [G loss: 2.952672]\n",
      "epoch:17 step:13895 [D loss: 0.371978, acc.: 83.59%] [G loss: 3.619784]\n",
      "epoch:17 step:13896 [D loss: 0.243325, acc.: 88.28%] [G loss: 4.085513]\n",
      "epoch:17 step:13897 [D loss: 0.399821, acc.: 82.81%] [G loss: 3.126969]\n",
      "epoch:17 step:13898 [D loss: 0.447480, acc.: 78.91%] [G loss: 3.896856]\n",
      "epoch:17 step:13899 [D loss: 0.146383, acc.: 93.75%] [G loss: 7.153962]\n",
      "epoch:17 step:13900 [D loss: 0.266995, acc.: 88.28%] [G loss: 3.083110]\n",
      "epoch:17 step:13901 [D loss: 0.332159, acc.: 82.81%] [G loss: 3.077276]\n",
      "epoch:17 step:13902 [D loss: 0.370271, acc.: 82.03%] [G loss: 3.794198]\n",
      "epoch:17 step:13903 [D loss: 0.385133, acc.: 86.72%] [G loss: 3.591082]\n",
      "epoch:17 step:13904 [D loss: 0.347843, acc.: 82.03%] [G loss: 3.690255]\n",
      "epoch:17 step:13905 [D loss: 0.374242, acc.: 85.94%] [G loss: 5.607399]\n",
      "epoch:17 step:13906 [D loss: 0.451538, acc.: 83.59%] [G loss: 6.290481]\n",
      "epoch:17 step:13907 [D loss: 0.593952, acc.: 82.81%] [G loss: 6.881576]\n",
      "epoch:17 step:13908 [D loss: 0.776221, acc.: 71.88%] [G loss: 7.305264]\n",
      "epoch:17 step:13909 [D loss: 2.045142, acc.: 62.50%] [G loss: 5.751920]\n",
      "epoch:17 step:13910 [D loss: 0.876161, acc.: 71.88%] [G loss: 8.557858]\n",
      "epoch:17 step:13911 [D loss: 0.488730, acc.: 81.25%] [G loss: 5.393116]\n",
      "epoch:17 step:13912 [D loss: 0.608447, acc.: 75.78%] [G loss: 6.242380]\n",
      "epoch:17 step:13913 [D loss: 0.328272, acc.: 85.16%] [G loss: 5.201426]\n",
      "epoch:17 step:13914 [D loss: 0.337261, acc.: 84.38%] [G loss: 3.426994]\n",
      "epoch:17 step:13915 [D loss: 0.309662, acc.: 85.94%] [G loss: 4.724144]\n",
      "epoch:17 step:13916 [D loss: 0.364883, acc.: 85.94%] [G loss: 2.805758]\n",
      "epoch:17 step:13917 [D loss: 0.224444, acc.: 91.41%] [G loss: 2.803707]\n",
      "epoch:17 step:13918 [D loss: 0.237348, acc.: 92.19%] [G loss: 2.945513]\n",
      "epoch:17 step:13919 [D loss: 0.314757, acc.: 85.16%] [G loss: 3.151923]\n",
      "epoch:17 step:13920 [D loss: 0.279862, acc.: 89.06%] [G loss: 3.214748]\n",
      "epoch:17 step:13921 [D loss: 0.340138, acc.: 84.38%] [G loss: 4.915620]\n",
      "epoch:17 step:13922 [D loss: 0.264795, acc.: 88.28%] [G loss: 3.369852]\n",
      "epoch:17 step:13923 [D loss: 0.242268, acc.: 88.28%] [G loss: 3.285048]\n",
      "epoch:17 step:13924 [D loss: 0.278593, acc.: 89.06%] [G loss: 3.137582]\n",
      "epoch:17 step:13925 [D loss: 0.277438, acc.: 88.28%] [G loss: 3.725601]\n",
      "epoch:17 step:13926 [D loss: 0.294926, acc.: 84.38%] [G loss: 4.058324]\n",
      "epoch:17 step:13927 [D loss: 0.303397, acc.: 87.50%] [G loss: 4.067912]\n",
      "epoch:17 step:13928 [D loss: 0.261912, acc.: 89.84%] [G loss: 3.560917]\n",
      "epoch:17 step:13929 [D loss: 0.308566, acc.: 87.50%] [G loss: 3.833701]\n",
      "epoch:17 step:13930 [D loss: 0.273942, acc.: 86.72%] [G loss: 3.285981]\n",
      "epoch:17 step:13931 [D loss: 0.406791, acc.: 82.03%] [G loss: 2.510362]\n",
      "epoch:17 step:13932 [D loss: 0.350162, acc.: 84.38%] [G loss: 2.546652]\n",
      "epoch:17 step:13933 [D loss: 0.270645, acc.: 89.06%] [G loss: 3.254928]\n",
      "epoch:17 step:13934 [D loss: 0.450050, acc.: 80.47%] [G loss: 2.836042]\n",
      "epoch:17 step:13935 [D loss: 0.308697, acc.: 86.72%] [G loss: 2.395937]\n",
      "epoch:17 step:13936 [D loss: 0.325406, acc.: 86.72%] [G loss: 2.865510]\n",
      "epoch:17 step:13937 [D loss: 0.408271, acc.: 80.47%] [G loss: 2.477082]\n",
      "epoch:17 step:13938 [D loss: 0.353597, acc.: 82.81%] [G loss: 2.494138]\n",
      "epoch:17 step:13939 [D loss: 0.267732, acc.: 87.50%] [G loss: 3.444558]\n",
      "epoch:17 step:13940 [D loss: 0.308996, acc.: 89.84%] [G loss: 4.543258]\n",
      "epoch:17 step:13941 [D loss: 0.387702, acc.: 83.59%] [G loss: 3.508892]\n",
      "epoch:17 step:13942 [D loss: 0.237367, acc.: 90.62%] [G loss: 3.565986]\n",
      "epoch:17 step:13943 [D loss: 0.424738, acc.: 79.69%] [G loss: 3.178919]\n",
      "epoch:17 step:13944 [D loss: 0.336985, acc.: 85.16%] [G loss: 3.635610]\n",
      "epoch:17 step:13945 [D loss: 0.264396, acc.: 91.41%] [G loss: 2.900361]\n",
      "epoch:17 step:13946 [D loss: 0.362453, acc.: 83.59%] [G loss: 3.611618]\n",
      "epoch:17 step:13947 [D loss: 0.357079, acc.: 81.25%] [G loss: 2.801593]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:17 step:13948 [D loss: 0.284907, acc.: 88.28%] [G loss: 2.729123]\n",
      "epoch:17 step:13949 [D loss: 0.294852, acc.: 87.50%] [G loss: 3.222676]\n",
      "epoch:17 step:13950 [D loss: 0.440975, acc.: 78.91%] [G loss: 3.351636]\n",
      "epoch:17 step:13951 [D loss: 0.247357, acc.: 92.97%] [G loss: 3.801005]\n",
      "epoch:17 step:13952 [D loss: 0.342556, acc.: 82.03%] [G loss: 3.265779]\n",
      "epoch:17 step:13953 [D loss: 0.280479, acc.: 88.28%] [G loss: 3.253162]\n",
      "epoch:17 step:13954 [D loss: 0.329583, acc.: 87.50%] [G loss: 2.250188]\n",
      "epoch:17 step:13955 [D loss: 0.292121, acc.: 91.41%] [G loss: 3.408145]\n",
      "epoch:17 step:13956 [D loss: 0.263088, acc.: 89.84%] [G loss: 3.520940]\n",
      "epoch:17 step:13957 [D loss: 0.311125, acc.: 85.16%] [G loss: 3.517451]\n",
      "epoch:17 step:13958 [D loss: 0.363625, acc.: 83.59%] [G loss: 3.471056]\n",
      "epoch:17 step:13959 [D loss: 0.490120, acc.: 80.47%] [G loss: 4.648680]\n",
      "epoch:17 step:13960 [D loss: 0.456115, acc.: 78.91%] [G loss: 3.219811]\n",
      "epoch:17 step:13961 [D loss: 0.229775, acc.: 89.06%] [G loss: 6.252157]\n",
      "epoch:17 step:13962 [D loss: 0.210491, acc.: 92.97%] [G loss: 4.257948]\n",
      "epoch:17 step:13963 [D loss: 0.320933, acc.: 87.50%] [G loss: 5.945790]\n",
      "epoch:17 step:13964 [D loss: 0.189839, acc.: 92.19%] [G loss: 4.507252]\n",
      "epoch:17 step:13965 [D loss: 0.254950, acc.: 91.41%] [G loss: 4.284092]\n",
      "epoch:17 step:13966 [D loss: 0.245311, acc.: 91.41%] [G loss: 3.149544]\n",
      "epoch:17 step:13967 [D loss: 0.203001, acc.: 91.41%] [G loss: 3.164236]\n",
      "epoch:17 step:13968 [D loss: 0.235613, acc.: 89.06%] [G loss: 4.530376]\n",
      "epoch:17 step:13969 [D loss: 0.288723, acc.: 87.50%] [G loss: 3.704354]\n",
      "epoch:17 step:13970 [D loss: 0.352482, acc.: 85.16%] [G loss: 3.387142]\n",
      "epoch:17 step:13971 [D loss: 0.275844, acc.: 87.50%] [G loss: 5.311467]\n",
      "epoch:17 step:13972 [D loss: 0.389619, acc.: 85.16%] [G loss: 3.702881]\n",
      "epoch:17 step:13973 [D loss: 0.379797, acc.: 82.03%] [G loss: 2.533767]\n",
      "epoch:17 step:13974 [D loss: 0.305822, acc.: 86.72%] [G loss: 3.404355]\n",
      "epoch:17 step:13975 [D loss: 0.297854, acc.: 87.50%] [G loss: 3.691766]\n",
      "epoch:17 step:13976 [D loss: 0.374494, acc.: 88.28%] [G loss: 5.621318]\n",
      "epoch:17 step:13977 [D loss: 0.241020, acc.: 92.19%] [G loss: 4.421741]\n",
      "epoch:17 step:13978 [D loss: 0.377508, acc.: 79.69%] [G loss: 3.620371]\n",
      "epoch:17 step:13979 [D loss: 0.331181, acc.: 84.38%] [G loss: 3.283217]\n",
      "epoch:17 step:13980 [D loss: 0.429932, acc.: 82.03%] [G loss: 2.981242]\n",
      "epoch:17 step:13981 [D loss: 0.274529, acc.: 89.84%] [G loss: 3.871012]\n",
      "epoch:17 step:13982 [D loss: 0.377562, acc.: 84.38%] [G loss: 3.557983]\n",
      "epoch:17 step:13983 [D loss: 0.270221, acc.: 89.06%] [G loss: 3.477659]\n",
      "epoch:17 step:13984 [D loss: 0.314097, acc.: 88.28%] [G loss: 4.120416]\n",
      "epoch:17 step:13985 [D loss: 0.280007, acc.: 86.72%] [G loss: 5.178864]\n",
      "epoch:17 step:13986 [D loss: 0.318194, acc.: 81.25%] [G loss: 4.230626]\n",
      "epoch:17 step:13987 [D loss: 0.379060, acc.: 79.69%] [G loss: 3.580618]\n",
      "epoch:17 step:13988 [D loss: 0.355749, acc.: 88.28%] [G loss: 2.810460]\n",
      "epoch:17 step:13989 [D loss: 0.418288, acc.: 82.03%] [G loss: 4.875691]\n",
      "epoch:17 step:13990 [D loss: 0.404360, acc.: 81.25%] [G loss: 4.653563]\n",
      "epoch:17 step:13991 [D loss: 0.303120, acc.: 88.28%] [G loss: 4.917129]\n",
      "epoch:17 step:13992 [D loss: 0.396968, acc.: 80.47%] [G loss: 3.931611]\n",
      "epoch:17 step:13993 [D loss: 0.380700, acc.: 85.16%] [G loss: 3.972452]\n",
      "epoch:17 step:13994 [D loss: 0.369552, acc.: 86.72%] [G loss: 2.999491]\n",
      "epoch:17 step:13995 [D loss: 0.394749, acc.: 85.16%] [G loss: 3.315209]\n",
      "epoch:17 step:13996 [D loss: 0.491656, acc.: 75.78%] [G loss: 3.068328]\n",
      "epoch:17 step:13997 [D loss: 0.418589, acc.: 87.50%] [G loss: 4.324067]\n",
      "epoch:17 step:13998 [D loss: 0.470293, acc.: 75.78%] [G loss: 3.927767]\n",
      "epoch:17 step:13999 [D loss: 0.449830, acc.: 79.69%] [G loss: 3.127546]\n",
      "epoch:17 step:14000 [D loss: 0.351304, acc.: 84.38%] [G loss: 4.949789]\n",
      "##############\n",
      "[0.86447161 0.8322763  0.7957749  0.79495562 0.77855598 0.81891272\n",
      " 0.87318082 0.80410357 0.82741734 0.78684933]\n",
      "##########\n",
      "epoch:17 step:14001 [D loss: 0.459054, acc.: 71.09%] [G loss: 3.237156]\n",
      "epoch:17 step:14002 [D loss: 0.306554, acc.: 88.28%] [G loss: 4.025104]\n",
      "epoch:17 step:14003 [D loss: 0.361942, acc.: 83.59%] [G loss: 3.392268]\n",
      "epoch:17 step:14004 [D loss: 0.314412, acc.: 89.06%] [G loss: 3.130046]\n",
      "epoch:17 step:14005 [D loss: 0.302688, acc.: 83.59%] [G loss: 3.284108]\n",
      "epoch:17 step:14006 [D loss: 0.240840, acc.: 89.06%] [G loss: 4.198184]\n",
      "epoch:17 step:14007 [D loss: 0.213364, acc.: 92.97%] [G loss: 3.599622]\n",
      "epoch:17 step:14008 [D loss: 0.263524, acc.: 90.62%] [G loss: 2.646779]\n",
      "epoch:17 step:14009 [D loss: 0.342781, acc.: 84.38%] [G loss: 3.075134]\n",
      "epoch:17 step:14010 [D loss: 0.269730, acc.: 90.62%] [G loss: 3.185699]\n",
      "epoch:17 step:14011 [D loss: 0.296268, acc.: 88.28%] [G loss: 4.142613]\n",
      "epoch:17 step:14012 [D loss: 0.241096, acc.: 87.50%] [G loss: 4.689522]\n",
      "epoch:17 step:14013 [D loss: 0.255659, acc.: 89.84%] [G loss: 3.325388]\n",
      "epoch:17 step:14014 [D loss: 0.277243, acc.: 88.28%] [G loss: 3.657715]\n",
      "epoch:17 step:14015 [D loss: 0.316841, acc.: 88.28%] [G loss: 4.044195]\n",
      "epoch:17 step:14016 [D loss: 0.381878, acc.: 82.03%] [G loss: 3.365505]\n",
      "epoch:17 step:14017 [D loss: 0.404588, acc.: 79.69%] [G loss: 4.165481]\n",
      "epoch:17 step:14018 [D loss: 0.317295, acc.: 87.50%] [G loss: 3.459756]\n",
      "epoch:17 step:14019 [D loss: 0.345040, acc.: 84.38%] [G loss: 3.319343]\n",
      "epoch:17 step:14020 [D loss: 0.335004, acc.: 83.59%] [G loss: 3.531784]\n",
      "epoch:17 step:14021 [D loss: 0.356349, acc.: 85.94%] [G loss: 3.688490]\n",
      "epoch:17 step:14022 [D loss: 0.333575, acc.: 85.94%] [G loss: 2.328128]\n",
      "epoch:17 step:14023 [D loss: 0.299735, acc.: 89.84%] [G loss: 2.903932]\n",
      "epoch:17 step:14024 [D loss: 0.367409, acc.: 85.16%] [G loss: 3.032507]\n",
      "epoch:17 step:14025 [D loss: 0.307895, acc.: 85.94%] [G loss: 3.692820]\n",
      "epoch:17 step:14026 [D loss: 0.298219, acc.: 91.41%] [G loss: 3.140175]\n",
      "epoch:17 step:14027 [D loss: 0.437541, acc.: 83.59%] [G loss: 3.689566]\n",
      "epoch:17 step:14028 [D loss: 0.296994, acc.: 88.28%] [G loss: 4.514904]\n",
      "epoch:17 step:14029 [D loss: 0.267230, acc.: 86.72%] [G loss: 5.280993]\n",
      "epoch:17 step:14030 [D loss: 0.237787, acc.: 89.84%] [G loss: 4.545141]\n",
      "epoch:17 step:14031 [D loss: 0.425117, acc.: 77.34%] [G loss: 3.660945]\n",
      "epoch:17 step:14032 [D loss: 0.356907, acc.: 83.59%] [G loss: 3.463093]\n",
      "epoch:17 step:14033 [D loss: 0.300886, acc.: 85.94%] [G loss: 4.410428]\n",
      "epoch:17 step:14034 [D loss: 0.309110, acc.: 87.50%] [G loss: 4.658938]\n",
      "epoch:17 step:14035 [D loss: 0.284046, acc.: 93.75%] [G loss: 3.198562]\n",
      "epoch:17 step:14036 [D loss: 0.333144, acc.: 82.03%] [G loss: 4.778382]\n",
      "epoch:17 step:14037 [D loss: 0.440671, acc.: 82.81%] [G loss: 4.736403]\n",
      "epoch:17 step:14038 [D loss: 0.508706, acc.: 84.38%] [G loss: 3.417497]\n",
      "epoch:17 step:14039 [D loss: 0.570522, acc.: 73.44%] [G loss: 3.163185]\n",
      "epoch:17 step:14040 [D loss: 0.278916, acc.: 93.75%] [G loss: 3.328584]\n",
      "epoch:17 step:14041 [D loss: 0.311174, acc.: 86.72%] [G loss: 3.427620]\n",
      "epoch:17 step:14042 [D loss: 0.346195, acc.: 87.50%] [G loss: 3.186684]\n",
      "epoch:17 step:14043 [D loss: 0.336881, acc.: 88.28%] [G loss: 3.352326]\n",
      "epoch:17 step:14044 [D loss: 0.314103, acc.: 85.94%] [G loss: 4.354035]\n",
      "epoch:17 step:14045 [D loss: 0.300128, acc.: 87.50%] [G loss: 2.505482]\n",
      "epoch:17 step:14046 [D loss: 0.365811, acc.: 82.03%] [G loss: 3.235464]\n",
      "epoch:17 step:14047 [D loss: 0.383058, acc.: 80.47%] [G loss: 4.673297]\n",
      "epoch:17 step:14048 [D loss: 0.381692, acc.: 82.03%] [G loss: 4.670024]\n",
      "epoch:17 step:14049 [D loss: 0.440090, acc.: 78.91%] [G loss: 7.001081]\n",
      "epoch:17 step:14050 [D loss: 0.611815, acc.: 76.56%] [G loss: 5.898684]\n",
      "epoch:17 step:14051 [D loss: 0.898784, acc.: 65.62%] [G loss: 9.466852]\n",
      "epoch:17 step:14052 [D loss: 1.874161, acc.: 59.38%] [G loss: 3.267603]\n",
      "epoch:17 step:14053 [D loss: 0.501163, acc.: 79.69%] [G loss: 3.071062]\n",
      "epoch:17 step:14054 [D loss: 0.321228, acc.: 86.72%] [G loss: 3.944827]\n",
      "epoch:17 step:14055 [D loss: 0.365704, acc.: 89.06%] [G loss: 3.837445]\n",
      "epoch:17 step:14056 [D loss: 0.326384, acc.: 84.38%] [G loss: 3.082757]\n",
      "epoch:17 step:14057 [D loss: 0.256249, acc.: 88.28%] [G loss: 3.485218]\n",
      "epoch:17 step:14058 [D loss: 0.245190, acc.: 90.62%] [G loss: 3.958204]\n",
      "epoch:18 step:14059 [D loss: 0.290679, acc.: 87.50%] [G loss: 3.478307]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:18 step:14060 [D loss: 0.266823, acc.: 92.19%] [G loss: 3.394644]\n",
      "epoch:18 step:14061 [D loss: 0.319872, acc.: 87.50%] [G loss: 3.719859]\n",
      "epoch:18 step:14062 [D loss: 0.302954, acc.: 87.50%] [G loss: 3.097894]\n",
      "epoch:18 step:14063 [D loss: 0.346090, acc.: 86.72%] [G loss: 2.914594]\n",
      "epoch:18 step:14064 [D loss: 0.311536, acc.: 85.94%] [G loss: 3.228054]\n",
      "epoch:18 step:14065 [D loss: 0.327969, acc.: 82.81%] [G loss: 3.414070]\n",
      "epoch:18 step:14066 [D loss: 0.265217, acc.: 86.72%] [G loss: 3.394777]\n",
      "epoch:18 step:14067 [D loss: 0.312188, acc.: 88.28%] [G loss: 2.582120]\n",
      "epoch:18 step:14068 [D loss: 0.294550, acc.: 85.94%] [G loss: 2.322757]\n",
      "epoch:18 step:14069 [D loss: 0.328978, acc.: 86.72%] [G loss: 2.481126]\n",
      "epoch:18 step:14070 [D loss: 0.230726, acc.: 92.19%] [G loss: 2.502685]\n",
      "epoch:18 step:14071 [D loss: 0.269317, acc.: 89.06%] [G loss: 2.888166]\n",
      "epoch:18 step:14072 [D loss: 0.221531, acc.: 93.75%] [G loss: 2.695584]\n",
      "epoch:18 step:14073 [D loss: 0.276233, acc.: 89.84%] [G loss: 3.042050]\n",
      "epoch:18 step:14074 [D loss: 0.318628, acc.: 87.50%] [G loss: 3.026115]\n",
      "epoch:18 step:14075 [D loss: 0.228997, acc.: 89.84%] [G loss: 5.408885]\n",
      "epoch:18 step:14076 [D loss: 0.287383, acc.: 87.50%] [G loss: 4.446312]\n",
      "epoch:18 step:14077 [D loss: 0.230326, acc.: 92.97%] [G loss: 2.170843]\n",
      "epoch:18 step:14078 [D loss: 0.211854, acc.: 92.97%] [G loss: 4.733084]\n",
      "epoch:18 step:14079 [D loss: 0.235047, acc.: 87.50%] [G loss: 4.378429]\n",
      "epoch:18 step:14080 [D loss: 0.307383, acc.: 86.72%] [G loss: 4.478308]\n",
      "epoch:18 step:14081 [D loss: 0.303109, acc.: 86.72%] [G loss: 3.578326]\n",
      "epoch:18 step:14082 [D loss: 0.264488, acc.: 89.06%] [G loss: 3.849290]\n",
      "epoch:18 step:14083 [D loss: 0.367181, acc.: 82.81%] [G loss: 3.624846]\n",
      "epoch:18 step:14084 [D loss: 0.231216, acc.: 87.50%] [G loss: 4.325682]\n",
      "epoch:18 step:14085 [D loss: 0.292375, acc.: 88.28%] [G loss: 4.003263]\n",
      "epoch:18 step:14086 [D loss: 0.358488, acc.: 83.59%] [G loss: 2.994378]\n",
      "epoch:18 step:14087 [D loss: 0.325534, acc.: 86.72%] [G loss: 3.380500]\n",
      "epoch:18 step:14088 [D loss: 0.323553, acc.: 86.72%] [G loss: 2.886461]\n",
      "epoch:18 step:14089 [D loss: 0.318815, acc.: 87.50%] [G loss: 3.258994]\n",
      "epoch:18 step:14090 [D loss: 0.314783, acc.: 84.38%] [G loss: 3.449057]\n",
      "epoch:18 step:14091 [D loss: 0.219101, acc.: 93.75%] [G loss: 3.872628]\n",
      "epoch:18 step:14092 [D loss: 0.438901, acc.: 79.69%] [G loss: 2.860822]\n",
      "epoch:18 step:14093 [D loss: 0.264161, acc.: 91.41%] [G loss: 3.393497]\n",
      "epoch:18 step:14094 [D loss: 0.326153, acc.: 86.72%] [G loss: 3.479540]\n",
      "epoch:18 step:14095 [D loss: 0.302033, acc.: 88.28%] [G loss: 2.963134]\n",
      "epoch:18 step:14096 [D loss: 0.342553, acc.: 85.16%] [G loss: 2.345225]\n",
      "epoch:18 step:14097 [D loss: 0.270780, acc.: 87.50%] [G loss: 3.719309]\n",
      "epoch:18 step:14098 [D loss: 0.247174, acc.: 90.62%] [G loss: 3.925072]\n",
      "epoch:18 step:14099 [D loss: 0.303774, acc.: 86.72%] [G loss: 3.926178]\n",
      "epoch:18 step:14100 [D loss: 0.250912, acc.: 88.28%] [G loss: 4.991378]\n",
      "epoch:18 step:14101 [D loss: 0.232210, acc.: 92.97%] [G loss: 4.131539]\n",
      "epoch:18 step:14102 [D loss: 0.239924, acc.: 91.41%] [G loss: 4.050068]\n",
      "epoch:18 step:14103 [D loss: 0.335717, acc.: 85.16%] [G loss: 3.193823]\n",
      "epoch:18 step:14104 [D loss: 0.355381, acc.: 84.38%] [G loss: 3.628129]\n",
      "epoch:18 step:14105 [D loss: 0.440395, acc.: 83.59%] [G loss: 4.694508]\n",
      "epoch:18 step:14106 [D loss: 0.440341, acc.: 80.47%] [G loss: 4.154537]\n",
      "epoch:18 step:14107 [D loss: 0.393991, acc.: 80.47%] [G loss: 3.655720]\n",
      "epoch:18 step:14108 [D loss: 0.350173, acc.: 82.03%] [G loss: 3.318857]\n",
      "epoch:18 step:14109 [D loss: 0.281489, acc.: 88.28%] [G loss: 3.616270]\n",
      "epoch:18 step:14110 [D loss: 0.286642, acc.: 89.06%] [G loss: 2.571913]\n",
      "epoch:18 step:14111 [D loss: 0.293540, acc.: 86.72%] [G loss: 3.643049]\n",
      "epoch:18 step:14112 [D loss: 0.304627, acc.: 89.06%] [G loss: 3.334299]\n",
      "epoch:18 step:14113 [D loss: 0.296077, acc.: 89.06%] [G loss: 3.257833]\n",
      "epoch:18 step:14114 [D loss: 0.293164, acc.: 86.72%] [G loss: 2.857760]\n",
      "epoch:18 step:14115 [D loss: 0.321736, acc.: 85.16%] [G loss: 3.258777]\n",
      "epoch:18 step:14116 [D loss: 0.343413, acc.: 82.03%] [G loss: 3.392114]\n",
      "epoch:18 step:14117 [D loss: 0.450831, acc.: 82.81%] [G loss: 3.117412]\n",
      "epoch:18 step:14118 [D loss: 0.429347, acc.: 82.03%] [G loss: 2.369724]\n",
      "epoch:18 step:14119 [D loss: 0.321737, acc.: 84.38%] [G loss: 2.899295]\n",
      "epoch:18 step:14120 [D loss: 0.377626, acc.: 84.38%] [G loss: 3.220611]\n",
      "epoch:18 step:14121 [D loss: 0.315490, acc.: 87.50%] [G loss: 3.142373]\n",
      "epoch:18 step:14122 [D loss: 0.374987, acc.: 85.16%] [G loss: 3.170546]\n",
      "epoch:18 step:14123 [D loss: 0.325176, acc.: 85.16%] [G loss: 2.287637]\n",
      "epoch:18 step:14124 [D loss: 0.366595, acc.: 87.50%] [G loss: 3.626260]\n",
      "epoch:18 step:14125 [D loss: 0.223906, acc.: 89.06%] [G loss: 4.851967]\n",
      "epoch:18 step:14126 [D loss: 0.314632, acc.: 86.72%] [G loss: 3.125955]\n",
      "epoch:18 step:14127 [D loss: 0.287445, acc.: 85.94%] [G loss: 5.401591]\n",
      "epoch:18 step:14128 [D loss: 0.314486, acc.: 88.28%] [G loss: 4.708108]\n",
      "epoch:18 step:14129 [D loss: 0.368663, acc.: 81.25%] [G loss: 3.005658]\n",
      "epoch:18 step:14130 [D loss: 0.294014, acc.: 88.28%] [G loss: 3.004721]\n",
      "epoch:18 step:14131 [D loss: 0.317738, acc.: 86.72%] [G loss: 4.432880]\n",
      "epoch:18 step:14132 [D loss: 0.361155, acc.: 83.59%] [G loss: 3.982509]\n",
      "epoch:18 step:14133 [D loss: 0.403273, acc.: 83.59%] [G loss: 3.807351]\n",
      "epoch:18 step:14134 [D loss: 0.317825, acc.: 87.50%] [G loss: 4.155771]\n",
      "epoch:18 step:14135 [D loss: 0.261301, acc.: 90.62%] [G loss: 4.436258]\n",
      "epoch:18 step:14136 [D loss: 0.263449, acc.: 89.06%] [G loss: 3.827688]\n",
      "epoch:18 step:14137 [D loss: 0.249944, acc.: 90.62%] [G loss: 3.700541]\n",
      "epoch:18 step:14138 [D loss: 0.294378, acc.: 87.50%] [G loss: 4.912571]\n",
      "epoch:18 step:14139 [D loss: 0.295117, acc.: 88.28%] [G loss: 3.145054]\n",
      "epoch:18 step:14140 [D loss: 0.283980, acc.: 89.06%] [G loss: 4.076699]\n",
      "epoch:18 step:14141 [D loss: 0.370242, acc.: 80.47%] [G loss: 3.366774]\n",
      "epoch:18 step:14142 [D loss: 0.314127, acc.: 85.16%] [G loss: 4.097006]\n",
      "epoch:18 step:14143 [D loss: 0.252971, acc.: 89.06%] [G loss: 2.829669]\n",
      "epoch:18 step:14144 [D loss: 0.322298, acc.: 85.16%] [G loss: 4.777909]\n",
      "epoch:18 step:14145 [D loss: 0.456515, acc.: 79.69%] [G loss: 4.910050]\n",
      "epoch:18 step:14146 [D loss: 0.449531, acc.: 80.47%] [G loss: 3.839575]\n",
      "epoch:18 step:14147 [D loss: 0.305371, acc.: 89.06%] [G loss: 4.788428]\n",
      "epoch:18 step:14148 [D loss: 0.358677, acc.: 81.25%] [G loss: 3.175142]\n",
      "epoch:18 step:14149 [D loss: 0.268631, acc.: 86.72%] [G loss: 4.553917]\n",
      "epoch:18 step:14150 [D loss: 0.336758, acc.: 85.94%] [G loss: 3.342794]\n",
      "epoch:18 step:14151 [D loss: 0.289757, acc.: 86.72%] [G loss: 4.530397]\n",
      "epoch:18 step:14152 [D loss: 0.299554, acc.: 87.50%] [G loss: 3.550164]\n",
      "epoch:18 step:14153 [D loss: 0.478800, acc.: 82.81%] [G loss: 3.261533]\n",
      "epoch:18 step:14154 [D loss: 0.221549, acc.: 88.28%] [G loss: 3.711505]\n",
      "epoch:18 step:14155 [D loss: 0.326781, acc.: 83.59%] [G loss: 2.923798]\n",
      "epoch:18 step:14156 [D loss: 0.396873, acc.: 83.59%] [G loss: 2.917809]\n",
      "epoch:18 step:14157 [D loss: 0.458465, acc.: 72.66%] [G loss: 2.808269]\n",
      "epoch:18 step:14158 [D loss: 0.403244, acc.: 85.16%] [G loss: 2.929042]\n",
      "epoch:18 step:14159 [D loss: 0.677857, acc.: 68.75%] [G loss: 3.391300]\n",
      "epoch:18 step:14160 [D loss: 0.514673, acc.: 77.34%] [G loss: 5.689732]\n",
      "epoch:18 step:14161 [D loss: 0.473763, acc.: 75.00%] [G loss: 3.063597]\n",
      "epoch:18 step:14162 [D loss: 0.253241, acc.: 90.62%] [G loss: 4.595319]\n",
      "epoch:18 step:14163 [D loss: 0.335821, acc.: 85.16%] [G loss: 2.535563]\n",
      "epoch:18 step:14164 [D loss: 0.271093, acc.: 90.62%] [G loss: 3.907210]\n",
      "epoch:18 step:14165 [D loss: 0.361603, acc.: 86.72%] [G loss: 3.834285]\n",
      "epoch:18 step:14166 [D loss: 0.277526, acc.: 88.28%] [G loss: 4.909185]\n",
      "epoch:18 step:14167 [D loss: 0.304384, acc.: 87.50%] [G loss: 2.862770]\n",
      "epoch:18 step:14168 [D loss: 0.294308, acc.: 85.94%] [G loss: 3.659935]\n",
      "epoch:18 step:14169 [D loss: 0.276246, acc.: 90.62%] [G loss: 2.714564]\n",
      "epoch:18 step:14170 [D loss: 0.299366, acc.: 84.38%] [G loss: 3.025800]\n",
      "epoch:18 step:14171 [D loss: 0.323710, acc.: 86.72%] [G loss: 2.302779]\n",
      "epoch:18 step:14172 [D loss: 0.346137, acc.: 85.16%] [G loss: 2.532736]\n",
      "epoch:18 step:14173 [D loss: 0.329180, acc.: 87.50%] [G loss: 2.578937]\n",
      "epoch:18 step:14174 [D loss: 0.390267, acc.: 82.03%] [G loss: 2.662679]\n",
      "epoch:18 step:14175 [D loss: 0.322527, acc.: 85.94%] [G loss: 3.115561]\n",
      "epoch:18 step:14176 [D loss: 0.468460, acc.: 79.69%] [G loss: 4.301159]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:18 step:14177 [D loss: 0.309258, acc.: 84.38%] [G loss: 3.348873]\n",
      "epoch:18 step:14178 [D loss: 0.302283, acc.: 82.81%] [G loss: 3.720088]\n",
      "epoch:18 step:14179 [D loss: 0.329168, acc.: 83.59%] [G loss: 3.516176]\n",
      "epoch:18 step:14180 [D loss: 0.230202, acc.: 89.84%] [G loss: 5.025922]\n",
      "epoch:18 step:14181 [D loss: 0.285293, acc.: 85.16%] [G loss: 3.490989]\n",
      "epoch:18 step:14182 [D loss: 0.401265, acc.: 80.47%] [G loss: 4.438124]\n",
      "epoch:18 step:14183 [D loss: 0.364452, acc.: 82.81%] [G loss: 2.761261]\n",
      "epoch:18 step:14184 [D loss: 0.265171, acc.: 88.28%] [G loss: 3.421678]\n",
      "epoch:18 step:14185 [D loss: 0.241594, acc.: 89.84%] [G loss: 3.793398]\n",
      "epoch:18 step:14186 [D loss: 0.286642, acc.: 88.28%] [G loss: 4.104978]\n",
      "epoch:18 step:14187 [D loss: 0.326573, acc.: 85.16%] [G loss: 3.465263]\n",
      "epoch:18 step:14188 [D loss: 0.266972, acc.: 88.28%] [G loss: 3.599373]\n",
      "epoch:18 step:14189 [D loss: 0.268964, acc.: 90.62%] [G loss: 3.542523]\n",
      "epoch:18 step:14190 [D loss: 0.386321, acc.: 86.72%] [G loss: 2.693704]\n",
      "epoch:18 step:14191 [D loss: 0.300915, acc.: 85.94%] [G loss: 3.366850]\n",
      "epoch:18 step:14192 [D loss: 0.261531, acc.: 86.72%] [G loss: 5.073475]\n",
      "epoch:18 step:14193 [D loss: 0.291286, acc.: 86.72%] [G loss: 5.481365]\n",
      "epoch:18 step:14194 [D loss: 0.303710, acc.: 85.94%] [G loss: 3.243149]\n",
      "epoch:18 step:14195 [D loss: 0.311145, acc.: 85.16%] [G loss: 4.050346]\n",
      "epoch:18 step:14196 [D loss: 0.304044, acc.: 87.50%] [G loss: 2.361310]\n",
      "epoch:18 step:14197 [D loss: 0.261721, acc.: 88.28%] [G loss: 4.073589]\n",
      "epoch:18 step:14198 [D loss: 0.239709, acc.: 91.41%] [G loss: 2.999753]\n",
      "epoch:18 step:14199 [D loss: 0.273353, acc.: 88.28%] [G loss: 3.281191]\n",
      "epoch:18 step:14200 [D loss: 0.381316, acc.: 84.38%] [G loss: 3.525666]\n",
      "##############\n",
      "[0.84793628 0.87609652 0.80743213 0.80010164 0.77034529 0.8282079\n",
      " 0.84978593 0.8504007  0.81168529 0.81809592]\n",
      "##########\n",
      "epoch:18 step:14201 [D loss: 0.369630, acc.: 87.50%] [G loss: 3.518919]\n",
      "epoch:18 step:14202 [D loss: 0.428487, acc.: 78.91%] [G loss: 3.540760]\n",
      "epoch:18 step:14203 [D loss: 0.321599, acc.: 89.84%] [G loss: 3.417781]\n",
      "epoch:18 step:14204 [D loss: 0.311770, acc.: 86.72%] [G loss: 4.056437]\n",
      "epoch:18 step:14205 [D loss: 0.294317, acc.: 85.94%] [G loss: 3.614301]\n",
      "epoch:18 step:14206 [D loss: 0.346732, acc.: 86.72%] [G loss: 4.074818]\n",
      "epoch:18 step:14207 [D loss: 0.502205, acc.: 80.47%] [G loss: 3.600660]\n",
      "epoch:18 step:14208 [D loss: 0.355331, acc.: 85.16%] [G loss: 4.953683]\n",
      "epoch:18 step:14209 [D loss: 0.290322, acc.: 86.72%] [G loss: 3.699192]\n",
      "epoch:18 step:14210 [D loss: 0.274003, acc.: 86.72%] [G loss: 3.479335]\n",
      "epoch:18 step:14211 [D loss: 0.262570, acc.: 88.28%] [G loss: 3.432706]\n",
      "epoch:18 step:14212 [D loss: 0.374806, acc.: 81.25%] [G loss: 2.943993]\n",
      "epoch:18 step:14213 [D loss: 0.436713, acc.: 81.25%] [G loss: 3.268744]\n",
      "epoch:18 step:14214 [D loss: 0.409824, acc.: 86.72%] [G loss: 4.277037]\n",
      "epoch:18 step:14215 [D loss: 0.301843, acc.: 86.72%] [G loss: 4.139442]\n",
      "epoch:18 step:14216 [D loss: 0.411504, acc.: 85.16%] [G loss: 2.543187]\n",
      "epoch:18 step:14217 [D loss: 0.194449, acc.: 94.53%] [G loss: 4.533492]\n",
      "epoch:18 step:14218 [D loss: 0.388287, acc.: 82.03%] [G loss: 3.548634]\n",
      "epoch:18 step:14219 [D loss: 0.297652, acc.: 88.28%] [G loss: 3.304721]\n",
      "epoch:18 step:14220 [D loss: 0.306139, acc.: 83.59%] [G loss: 3.451453]\n",
      "epoch:18 step:14221 [D loss: 0.279840, acc.: 85.94%] [G loss: 3.231565]\n",
      "epoch:18 step:14222 [D loss: 0.278077, acc.: 85.94%] [G loss: 3.029718]\n",
      "epoch:18 step:14223 [D loss: 0.295710, acc.: 86.72%] [G loss: 3.015830]\n",
      "epoch:18 step:14224 [D loss: 0.402639, acc.: 82.03%] [G loss: 3.735583]\n",
      "epoch:18 step:14225 [D loss: 0.401113, acc.: 80.47%] [G loss: 5.112379]\n",
      "epoch:18 step:14226 [D loss: 0.372642, acc.: 85.16%] [G loss: 4.019058]\n",
      "epoch:18 step:14227 [D loss: 0.244655, acc.: 89.06%] [G loss: 4.765270]\n",
      "epoch:18 step:14228 [D loss: 0.317512, acc.: 84.38%] [G loss: 2.762248]\n",
      "epoch:18 step:14229 [D loss: 0.310760, acc.: 86.72%] [G loss: 4.135148]\n",
      "epoch:18 step:14230 [D loss: 0.327796, acc.: 86.72%] [G loss: 3.076959]\n",
      "epoch:18 step:14231 [D loss: 0.330322, acc.: 86.72%] [G loss: 4.631691]\n",
      "epoch:18 step:14232 [D loss: 0.367694, acc.: 84.38%] [G loss: 3.486639]\n",
      "epoch:18 step:14233 [D loss: 0.367059, acc.: 85.16%] [G loss: 2.916782]\n",
      "epoch:18 step:14234 [D loss: 0.292722, acc.: 84.38%] [G loss: 3.087914]\n",
      "epoch:18 step:14235 [D loss: 0.329617, acc.: 83.59%] [G loss: 3.371023]\n",
      "epoch:18 step:14236 [D loss: 0.248057, acc.: 89.84%] [G loss: 3.120486]\n",
      "epoch:18 step:14237 [D loss: 0.334063, acc.: 82.03%] [G loss: 4.376717]\n",
      "epoch:18 step:14238 [D loss: 0.295365, acc.: 87.50%] [G loss: 3.609382]\n",
      "epoch:18 step:14239 [D loss: 0.270738, acc.: 88.28%] [G loss: 3.327160]\n",
      "epoch:18 step:14240 [D loss: 0.282684, acc.: 87.50%] [G loss: 2.707702]\n",
      "epoch:18 step:14241 [D loss: 0.345377, acc.: 84.38%] [G loss: 2.922744]\n",
      "epoch:18 step:14242 [D loss: 0.329707, acc.: 88.28%] [G loss: 3.161774]\n",
      "epoch:18 step:14243 [D loss: 0.414982, acc.: 78.91%] [G loss: 3.167723]\n",
      "epoch:18 step:14244 [D loss: 0.361306, acc.: 81.25%] [G loss: 2.713219]\n",
      "epoch:18 step:14245 [D loss: 0.277800, acc.: 84.38%] [G loss: 4.586668]\n",
      "epoch:18 step:14246 [D loss: 0.250481, acc.: 90.62%] [G loss: 3.425815]\n",
      "epoch:18 step:14247 [D loss: 0.356368, acc.: 84.38%] [G loss: 3.044989]\n",
      "epoch:18 step:14248 [D loss: 0.274296, acc.: 91.41%] [G loss: 3.226356]\n",
      "epoch:18 step:14249 [D loss: 0.234388, acc.: 90.62%] [G loss: 3.644311]\n",
      "epoch:18 step:14250 [D loss: 0.333093, acc.: 88.28%] [G loss: 4.068826]\n",
      "epoch:18 step:14251 [D loss: 0.237727, acc.: 91.41%] [G loss: 3.759380]\n",
      "epoch:18 step:14252 [D loss: 0.523429, acc.: 75.00%] [G loss: 3.124348]\n",
      "epoch:18 step:14253 [D loss: 0.394129, acc.: 82.81%] [G loss: 4.602940]\n",
      "epoch:18 step:14254 [D loss: 0.550953, acc.: 80.47%] [G loss: 5.955872]\n",
      "epoch:18 step:14255 [D loss: 0.792991, acc.: 75.00%] [G loss: 6.385561]\n",
      "epoch:18 step:14256 [D loss: 0.996466, acc.: 70.31%] [G loss: 5.540222]\n",
      "epoch:18 step:14257 [D loss: 0.373903, acc.: 82.81%] [G loss: 5.170297]\n",
      "epoch:18 step:14258 [D loss: 0.555012, acc.: 75.00%] [G loss: 4.050531]\n",
      "epoch:18 step:14259 [D loss: 0.337379, acc.: 84.38%] [G loss: 4.890537]\n",
      "epoch:18 step:14260 [D loss: 0.503714, acc.: 84.38%] [G loss: 5.028502]\n",
      "epoch:18 step:14261 [D loss: 0.547396, acc.: 84.38%] [G loss: 4.929612]\n",
      "epoch:18 step:14262 [D loss: 0.662691, acc.: 80.47%] [G loss: 5.600427]\n",
      "epoch:18 step:14263 [D loss: 0.601443, acc.: 79.69%] [G loss: 4.005888]\n",
      "epoch:18 step:14264 [D loss: 0.287939, acc.: 89.06%] [G loss: 3.156273]\n",
      "epoch:18 step:14265 [D loss: 0.528316, acc.: 80.47%] [G loss: 3.519122]\n",
      "epoch:18 step:14266 [D loss: 0.375431, acc.: 80.47%] [G loss: 2.750828]\n",
      "epoch:18 step:14267 [D loss: 0.345461, acc.: 81.25%] [G loss: 4.221858]\n",
      "epoch:18 step:14268 [D loss: 0.359417, acc.: 82.03%] [G loss: 3.842101]\n",
      "epoch:18 step:14269 [D loss: 0.476768, acc.: 82.81%] [G loss: 3.197788]\n",
      "epoch:18 step:14270 [D loss: 0.249863, acc.: 90.62%] [G loss: 3.844857]\n",
      "epoch:18 step:14271 [D loss: 0.247507, acc.: 91.41%] [G loss: 2.855191]\n",
      "epoch:18 step:14272 [D loss: 0.321396, acc.: 86.72%] [G loss: 4.151116]\n",
      "epoch:18 step:14273 [D loss: 0.267466, acc.: 88.28%] [G loss: 3.051435]\n",
      "epoch:18 step:14274 [D loss: 0.436528, acc.: 80.47%] [G loss: 4.229416]\n",
      "epoch:18 step:14275 [D loss: 0.353493, acc.: 82.81%] [G loss: 4.770086]\n",
      "epoch:18 step:14276 [D loss: 0.228517, acc.: 89.06%] [G loss: 2.791012]\n",
      "epoch:18 step:14277 [D loss: 0.377025, acc.: 79.69%] [G loss: 3.242674]\n",
      "epoch:18 step:14278 [D loss: 0.366076, acc.: 87.50%] [G loss: 2.839536]\n",
      "epoch:18 step:14279 [D loss: 0.315835, acc.: 84.38%] [G loss: 3.118727]\n",
      "epoch:18 step:14280 [D loss: 0.222164, acc.: 89.84%] [G loss: 3.957103]\n",
      "epoch:18 step:14281 [D loss: 0.305510, acc.: 85.94%] [G loss: 3.697887]\n",
      "epoch:18 step:14282 [D loss: 0.415560, acc.: 82.81%] [G loss: 2.550139]\n",
      "epoch:18 step:14283 [D loss: 0.328749, acc.: 82.81%] [G loss: 4.096870]\n",
      "epoch:18 step:14284 [D loss: 0.268667, acc.: 89.84%] [G loss: 2.714298]\n",
      "epoch:18 step:14285 [D loss: 0.313903, acc.: 86.72%] [G loss: 2.507451]\n",
      "epoch:18 step:14286 [D loss: 0.285594, acc.: 85.94%] [G loss: 2.994410]\n",
      "epoch:18 step:14287 [D loss: 0.330340, acc.: 82.03%] [G loss: 2.970606]\n",
      "epoch:18 step:14288 [D loss: 0.357892, acc.: 84.38%] [G loss: 2.765828]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:18 step:14289 [D loss: 0.312022, acc.: 89.06%] [G loss: 3.215775]\n",
      "epoch:18 step:14290 [D loss: 0.239737, acc.: 89.06%] [G loss: 4.072844]\n",
      "epoch:18 step:14291 [D loss: 0.361857, acc.: 84.38%] [G loss: 4.228619]\n",
      "epoch:18 step:14292 [D loss: 0.249320, acc.: 91.41%] [G loss: 4.055729]\n",
      "epoch:18 step:14293 [D loss: 0.320333, acc.: 88.28%] [G loss: 3.268546]\n",
      "epoch:18 step:14294 [D loss: 0.292419, acc.: 87.50%] [G loss: 3.540793]\n",
      "epoch:18 step:14295 [D loss: 0.416654, acc.: 82.81%] [G loss: 2.912823]\n",
      "epoch:18 step:14296 [D loss: 0.390982, acc.: 79.69%] [G loss: 2.801937]\n",
      "epoch:18 step:14297 [D loss: 0.326765, acc.: 87.50%] [G loss: 3.291001]\n",
      "epoch:18 step:14298 [D loss: 0.388190, acc.: 85.16%] [G loss: 2.830349]\n",
      "epoch:18 step:14299 [D loss: 0.260778, acc.: 87.50%] [G loss: 3.301760]\n",
      "epoch:18 step:14300 [D loss: 0.284607, acc.: 86.72%] [G loss: 2.595801]\n",
      "epoch:18 step:14301 [D loss: 0.313953, acc.: 89.84%] [G loss: 2.756915]\n",
      "epoch:18 step:14302 [D loss: 0.333674, acc.: 86.72%] [G loss: 3.726739]\n",
      "epoch:18 step:14303 [D loss: 0.258705, acc.: 89.84%] [G loss: 4.700765]\n",
      "epoch:18 step:14304 [D loss: 0.338859, acc.: 85.94%] [G loss: 4.251283]\n",
      "epoch:18 step:14305 [D loss: 0.323459, acc.: 86.72%] [G loss: 3.073657]\n",
      "epoch:18 step:14306 [D loss: 0.399040, acc.: 84.38%] [G loss: 2.933488]\n",
      "epoch:18 step:14307 [D loss: 0.483394, acc.: 77.34%] [G loss: 2.574702]\n",
      "epoch:18 step:14308 [D loss: 0.242480, acc.: 92.19%] [G loss: 2.752938]\n",
      "epoch:18 step:14309 [D loss: 0.301536, acc.: 85.94%] [G loss: 2.202641]\n",
      "epoch:18 step:14310 [D loss: 0.332469, acc.: 85.16%] [G loss: 2.821763]\n",
      "epoch:18 step:14311 [D loss: 0.301561, acc.: 89.06%] [G loss: 2.920298]\n",
      "epoch:18 step:14312 [D loss: 0.283467, acc.: 90.62%] [G loss: 2.978389]\n",
      "epoch:18 step:14313 [D loss: 0.339550, acc.: 85.94%] [G loss: 2.614775]\n",
      "epoch:18 step:14314 [D loss: 0.285850, acc.: 87.50%] [G loss: 4.457051]\n",
      "epoch:18 step:14315 [D loss: 0.204683, acc.: 94.53%] [G loss: 5.327062]\n",
      "epoch:18 step:14316 [D loss: 0.376027, acc.: 84.38%] [G loss: 3.595679]\n",
      "epoch:18 step:14317 [D loss: 0.370891, acc.: 85.94%] [G loss: 2.890133]\n",
      "epoch:18 step:14318 [D loss: 0.387600, acc.: 86.72%] [G loss: 3.696327]\n",
      "epoch:18 step:14319 [D loss: 0.350920, acc.: 86.72%] [G loss: 5.203270]\n",
      "epoch:18 step:14320 [D loss: 0.383283, acc.: 85.16%] [G loss: 6.201642]\n",
      "epoch:18 step:14321 [D loss: 0.331337, acc.: 85.16%] [G loss: 3.092338]\n",
      "epoch:18 step:14322 [D loss: 0.451827, acc.: 83.59%] [G loss: 3.654641]\n",
      "epoch:18 step:14323 [D loss: 0.329159, acc.: 87.50%] [G loss: 2.842502]\n",
      "epoch:18 step:14324 [D loss: 0.339097, acc.: 84.38%] [G loss: 3.169475]\n",
      "epoch:18 step:14325 [D loss: 0.306809, acc.: 88.28%] [G loss: 4.244199]\n",
      "epoch:18 step:14326 [D loss: 0.212626, acc.: 91.41%] [G loss: 4.000400]\n",
      "epoch:18 step:14327 [D loss: 0.247506, acc.: 89.06%] [G loss: 3.526003]\n",
      "epoch:18 step:14328 [D loss: 0.273196, acc.: 88.28%] [G loss: 2.917082]\n",
      "epoch:18 step:14329 [D loss: 0.230992, acc.: 92.19%] [G loss: 3.667493]\n",
      "epoch:18 step:14330 [D loss: 0.297272, acc.: 87.50%] [G loss: 4.296008]\n",
      "epoch:18 step:14331 [D loss: 0.325990, acc.: 85.94%] [G loss: 3.429326]\n",
      "epoch:18 step:14332 [D loss: 0.211581, acc.: 91.41%] [G loss: 3.692478]\n",
      "epoch:18 step:14333 [D loss: 0.409258, acc.: 79.69%] [G loss: 3.816063]\n",
      "epoch:18 step:14334 [D loss: 0.370283, acc.: 82.81%] [G loss: 3.374350]\n",
      "epoch:18 step:14335 [D loss: 0.278261, acc.: 88.28%] [G loss: 3.883589]\n",
      "epoch:18 step:14336 [D loss: 0.458705, acc.: 82.81%] [G loss: 2.551375]\n",
      "epoch:18 step:14337 [D loss: 0.317763, acc.: 89.06%] [G loss: 3.143060]\n",
      "epoch:18 step:14338 [D loss: 0.219805, acc.: 93.75%] [G loss: 3.845870]\n",
      "epoch:18 step:14339 [D loss: 0.418978, acc.: 82.03%] [G loss: 2.966615]\n",
      "epoch:18 step:14340 [D loss: 0.227169, acc.: 92.19%] [G loss: 3.687290]\n",
      "epoch:18 step:14341 [D loss: 0.516766, acc.: 76.56%] [G loss: 2.974018]\n",
      "epoch:18 step:14342 [D loss: 0.270766, acc.: 87.50%] [G loss: 3.553885]\n",
      "epoch:18 step:14343 [D loss: 0.266471, acc.: 86.72%] [G loss: 4.280817]\n",
      "epoch:18 step:14344 [D loss: 0.324155, acc.: 85.16%] [G loss: 4.004472]\n",
      "epoch:18 step:14345 [D loss: 0.329073, acc.: 88.28%] [G loss: 4.046305]\n",
      "epoch:18 step:14346 [D loss: 0.182427, acc.: 94.53%] [G loss: 4.751782]\n",
      "epoch:18 step:14347 [D loss: 0.281379, acc.: 89.06%] [G loss: 6.051027]\n",
      "epoch:18 step:14348 [D loss: 0.336879, acc.: 82.81%] [G loss: 3.447831]\n",
      "epoch:18 step:14349 [D loss: 0.344497, acc.: 86.72%] [G loss: 3.199092]\n",
      "epoch:18 step:14350 [D loss: 0.260574, acc.: 89.84%] [G loss: 4.329107]\n",
      "epoch:18 step:14351 [D loss: 0.344156, acc.: 85.94%] [G loss: 3.613659]\n",
      "epoch:18 step:14352 [D loss: 0.290536, acc.: 89.84%] [G loss: 3.017415]\n",
      "epoch:18 step:14353 [D loss: 0.307297, acc.: 85.94%] [G loss: 2.367404]\n",
      "epoch:18 step:14354 [D loss: 0.323559, acc.: 87.50%] [G loss: 3.834488]\n",
      "epoch:18 step:14355 [D loss: 0.187558, acc.: 93.75%] [G loss: 2.983765]\n",
      "epoch:18 step:14356 [D loss: 0.257176, acc.: 90.62%] [G loss: 3.316528]\n",
      "epoch:18 step:14357 [D loss: 0.240126, acc.: 89.06%] [G loss: 4.739182]\n",
      "epoch:18 step:14358 [D loss: 0.339691, acc.: 82.81%] [G loss: 5.037542]\n",
      "epoch:18 step:14359 [D loss: 0.425594, acc.: 77.34%] [G loss: 4.798719]\n",
      "epoch:18 step:14360 [D loss: 0.563754, acc.: 72.66%] [G loss: 3.997404]\n",
      "epoch:18 step:14361 [D loss: 0.397070, acc.: 85.16%] [G loss: 3.971344]\n",
      "epoch:18 step:14362 [D loss: 0.303680, acc.: 89.06%] [G loss: 5.739919]\n",
      "epoch:18 step:14363 [D loss: 0.305703, acc.: 83.59%] [G loss: 4.151222]\n",
      "epoch:18 step:14364 [D loss: 0.240517, acc.: 89.06%] [G loss: 8.332342]\n",
      "epoch:18 step:14365 [D loss: 0.277205, acc.: 88.28%] [G loss: 4.699501]\n",
      "epoch:18 step:14366 [D loss: 0.295651, acc.: 87.50%] [G loss: 3.575219]\n",
      "epoch:18 step:14367 [D loss: 0.302588, acc.: 85.16%] [G loss: 3.556827]\n",
      "epoch:18 step:14368 [D loss: 0.195538, acc.: 91.41%] [G loss: 3.959974]\n",
      "epoch:18 step:14369 [D loss: 0.428667, acc.: 80.47%] [G loss: 3.150674]\n",
      "epoch:18 step:14370 [D loss: 0.242487, acc.: 90.62%] [G loss: 3.080628]\n",
      "epoch:18 step:14371 [D loss: 0.286785, acc.: 87.50%] [G loss: 5.329356]\n",
      "epoch:18 step:14372 [D loss: 0.394870, acc.: 86.72%] [G loss: 2.788066]\n",
      "epoch:18 step:14373 [D loss: 0.406824, acc.: 80.47%] [G loss: 2.835740]\n",
      "epoch:18 step:14374 [D loss: 0.392290, acc.: 87.50%] [G loss: 3.874247]\n",
      "epoch:18 step:14375 [D loss: 0.422212, acc.: 82.81%] [G loss: 2.305680]\n",
      "epoch:18 step:14376 [D loss: 0.411167, acc.: 78.91%] [G loss: 2.874650]\n",
      "epoch:18 step:14377 [D loss: 0.273356, acc.: 89.06%] [G loss: 3.641269]\n",
      "epoch:18 step:14378 [D loss: 0.423758, acc.: 80.47%] [G loss: 3.377729]\n",
      "epoch:18 step:14379 [D loss: 0.332368, acc.: 85.94%] [G loss: 3.004969]\n",
      "epoch:18 step:14380 [D loss: 0.338703, acc.: 85.16%] [G loss: 4.353744]\n",
      "epoch:18 step:14381 [D loss: 0.320081, acc.: 84.38%] [G loss: 3.238124]\n",
      "epoch:18 step:14382 [D loss: 0.307241, acc.: 85.94%] [G loss: 3.265705]\n",
      "epoch:18 step:14383 [D loss: 0.304982, acc.: 90.62%] [G loss: 2.482300]\n",
      "epoch:18 step:14384 [D loss: 0.240746, acc.: 92.97%] [G loss: 2.837197]\n",
      "epoch:18 step:14385 [D loss: 0.307192, acc.: 87.50%] [G loss: 2.887434]\n",
      "epoch:18 step:14386 [D loss: 0.254982, acc.: 92.19%] [G loss: 2.757957]\n",
      "epoch:18 step:14387 [D loss: 0.295921, acc.: 87.50%] [G loss: 4.669343]\n",
      "epoch:18 step:14388 [D loss: 0.262473, acc.: 89.84%] [G loss: 2.576177]\n",
      "epoch:18 step:14389 [D loss: 0.293835, acc.: 89.06%] [G loss: 3.113744]\n",
      "epoch:18 step:14390 [D loss: 0.377550, acc.: 83.59%] [G loss: 2.362113]\n",
      "epoch:18 step:14391 [D loss: 0.293544, acc.: 85.94%] [G loss: 3.506812]\n",
      "epoch:18 step:14392 [D loss: 0.399241, acc.: 84.38%] [G loss: 3.507898]\n",
      "epoch:18 step:14393 [D loss: 0.271456, acc.: 89.06%] [G loss: 4.517444]\n",
      "epoch:18 step:14394 [D loss: 0.314474, acc.: 85.94%] [G loss: 4.892447]\n",
      "epoch:18 step:14395 [D loss: 0.292143, acc.: 89.84%] [G loss: 3.183261]\n",
      "epoch:18 step:14396 [D loss: 0.301778, acc.: 86.72%] [G loss: 3.586255]\n",
      "epoch:18 step:14397 [D loss: 0.210101, acc.: 93.75%] [G loss: 4.334565]\n",
      "epoch:18 step:14398 [D loss: 0.325884, acc.: 84.38%] [G loss: 4.345483]\n",
      "epoch:18 step:14399 [D loss: 0.344502, acc.: 83.59%] [G loss: 2.770521]\n",
      "epoch:18 step:14400 [D loss: 0.288581, acc.: 88.28%] [G loss: 2.534627]\n",
      "##############\n",
      "[0.85222248 0.84112944 0.79892171 0.79641703 0.79139262 0.83892601\n",
      " 0.88102068 0.81032777 0.82934726 0.82128784]\n",
      "##########\n",
      "epoch:18 step:14401 [D loss: 0.365586, acc.: 84.38%] [G loss: 2.562066]\n",
      "epoch:18 step:14402 [D loss: 0.320061, acc.: 87.50%] [G loss: 3.374082]\n",
      "epoch:18 step:14403 [D loss: 0.255863, acc.: 91.41%] [G loss: 4.190492]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:18 step:14404 [D loss: 0.249874, acc.: 85.16%] [G loss: 2.816407]\n",
      "epoch:18 step:14405 [D loss: 0.232711, acc.: 89.06%] [G loss: 3.593142]\n",
      "epoch:18 step:14406 [D loss: 0.183344, acc.: 96.09%] [G loss: 4.803441]\n",
      "epoch:18 step:14407 [D loss: 0.296869, acc.: 88.28%] [G loss: 3.329710]\n",
      "epoch:18 step:14408 [D loss: 0.363748, acc.: 83.59%] [G loss: 2.973507]\n",
      "epoch:18 step:14409 [D loss: 0.406399, acc.: 82.03%] [G loss: 2.510726]\n",
      "epoch:18 step:14410 [D loss: 0.303948, acc.: 84.38%] [G loss: 2.555167]\n",
      "epoch:18 step:14411 [D loss: 0.281805, acc.: 89.06%] [G loss: 3.114300]\n",
      "epoch:18 step:14412 [D loss: 0.343285, acc.: 89.06%] [G loss: 3.488804]\n",
      "epoch:18 step:14413 [D loss: 0.251780, acc.: 90.62%] [G loss: 2.938866]\n",
      "epoch:18 step:14414 [D loss: 0.333191, acc.: 83.59%] [G loss: 2.862393]\n",
      "epoch:18 step:14415 [D loss: 0.390157, acc.: 82.81%] [G loss: 3.229002]\n",
      "epoch:18 step:14416 [D loss: 0.265941, acc.: 87.50%] [G loss: 2.955462]\n",
      "epoch:18 step:14417 [D loss: 0.226086, acc.: 91.41%] [G loss: 3.429352]\n",
      "epoch:18 step:14418 [D loss: 0.354940, acc.: 85.94%] [G loss: 3.386218]\n",
      "epoch:18 step:14419 [D loss: 0.474633, acc.: 78.12%] [G loss: 5.124907]\n",
      "epoch:18 step:14420 [D loss: 0.273754, acc.: 89.06%] [G loss: 6.315636]\n",
      "epoch:18 step:14421 [D loss: 0.238982, acc.: 86.72%] [G loss: 4.634122]\n",
      "epoch:18 step:14422 [D loss: 0.315636, acc.: 86.72%] [G loss: 6.060692]\n",
      "epoch:18 step:14423 [D loss: 0.292208, acc.: 86.72%] [G loss: 4.408513]\n",
      "epoch:18 step:14424 [D loss: 0.299192, acc.: 84.38%] [G loss: 4.042924]\n",
      "epoch:18 step:14425 [D loss: 0.409486, acc.: 81.25%] [G loss: 2.629036]\n",
      "epoch:18 step:14426 [D loss: 0.534194, acc.: 74.22%] [G loss: 2.342449]\n",
      "epoch:18 step:14427 [D loss: 0.300256, acc.: 88.28%] [G loss: 3.423656]\n",
      "epoch:18 step:14428 [D loss: 0.388532, acc.: 82.81%] [G loss: 3.837876]\n",
      "epoch:18 step:14429 [D loss: 0.292712, acc.: 85.16%] [G loss: 3.075358]\n",
      "epoch:18 step:14430 [D loss: 0.411150, acc.: 80.47%] [G loss: 2.717887]\n",
      "epoch:18 step:14431 [D loss: 0.222557, acc.: 90.62%] [G loss: 3.792076]\n",
      "epoch:18 step:14432 [D loss: 0.271521, acc.: 88.28%] [G loss: 3.341997]\n",
      "epoch:18 step:14433 [D loss: 0.276400, acc.: 89.84%] [G loss: 3.749983]\n",
      "epoch:18 step:14434 [D loss: 0.248759, acc.: 86.72%] [G loss: 3.000613]\n",
      "epoch:18 step:14435 [D loss: 0.319108, acc.: 85.16%] [G loss: 4.084038]\n",
      "epoch:18 step:14436 [D loss: 0.377195, acc.: 80.47%] [G loss: 3.830625]\n",
      "epoch:18 step:14437 [D loss: 0.315071, acc.: 84.38%] [G loss: 3.338840]\n",
      "epoch:18 step:14438 [D loss: 0.312880, acc.: 85.94%] [G loss: 3.226820]\n",
      "epoch:18 step:14439 [D loss: 0.264177, acc.: 88.28%] [G loss: 3.746959]\n",
      "epoch:18 step:14440 [D loss: 0.241677, acc.: 91.41%] [G loss: 2.751547]\n",
      "epoch:18 step:14441 [D loss: 0.338215, acc.: 82.81%] [G loss: 3.700705]\n",
      "epoch:18 step:14442 [D loss: 0.329809, acc.: 84.38%] [G loss: 5.044703]\n",
      "epoch:18 step:14443 [D loss: 0.319842, acc.: 85.16%] [G loss: 2.877519]\n",
      "epoch:18 step:14444 [D loss: 0.320051, acc.: 87.50%] [G loss: 2.940877]\n",
      "epoch:18 step:14445 [D loss: 0.280901, acc.: 86.72%] [G loss: 4.861895]\n",
      "epoch:18 step:14446 [D loss: 0.315759, acc.: 86.72%] [G loss: 3.737387]\n",
      "epoch:18 step:14447 [D loss: 0.284247, acc.: 89.84%] [G loss: 3.629060]\n",
      "epoch:18 step:14448 [D loss: 0.262420, acc.: 89.84%] [G loss: 3.478521]\n",
      "epoch:18 step:14449 [D loss: 0.285868, acc.: 87.50%] [G loss: 3.000993]\n",
      "epoch:18 step:14450 [D loss: 0.304459, acc.: 87.50%] [G loss: 2.841371]\n",
      "epoch:18 step:14451 [D loss: 0.290748, acc.: 89.06%] [G loss: 3.409238]\n",
      "epoch:18 step:14452 [D loss: 0.403305, acc.: 85.16%] [G loss: 3.003903]\n",
      "epoch:18 step:14453 [D loss: 0.332140, acc.: 82.81%] [G loss: 3.155596]\n",
      "epoch:18 step:14454 [D loss: 0.463912, acc.: 82.81%] [G loss: 3.717483]\n",
      "epoch:18 step:14455 [D loss: 0.414834, acc.: 81.25%] [G loss: 5.477375]\n",
      "epoch:18 step:14456 [D loss: 0.527189, acc.: 77.34%] [G loss: 6.205093]\n",
      "epoch:18 step:14457 [D loss: 1.636515, acc.: 60.94%] [G loss: 9.898241]\n",
      "epoch:18 step:14458 [D loss: 2.768881, acc.: 47.66%] [G loss: 2.788652]\n",
      "epoch:18 step:14459 [D loss: 1.169088, acc.: 67.97%] [G loss: 5.529125]\n",
      "epoch:18 step:14460 [D loss: 0.741873, acc.: 64.84%] [G loss: 3.978815]\n",
      "epoch:18 step:14461 [D loss: 0.542008, acc.: 79.69%] [G loss: 3.774356]\n",
      "epoch:18 step:14462 [D loss: 0.348625, acc.: 85.16%] [G loss: 3.504109]\n",
      "epoch:18 step:14463 [D loss: 0.328793, acc.: 86.72%] [G loss: 3.482247]\n",
      "epoch:18 step:14464 [D loss: 0.449839, acc.: 75.00%] [G loss: 3.583475]\n",
      "epoch:18 step:14465 [D loss: 0.235757, acc.: 89.06%] [G loss: 4.325153]\n",
      "epoch:18 step:14466 [D loss: 0.357545, acc.: 82.81%] [G loss: 3.432969]\n",
      "epoch:18 step:14467 [D loss: 0.320739, acc.: 84.38%] [G loss: 2.692192]\n",
      "epoch:18 step:14468 [D loss: 0.360292, acc.: 83.59%] [G loss: 3.476943]\n",
      "epoch:18 step:14469 [D loss: 0.433233, acc.: 77.34%] [G loss: 5.065369]\n",
      "epoch:18 step:14470 [D loss: 0.267235, acc.: 89.06%] [G loss: 3.783616]\n",
      "epoch:18 step:14471 [D loss: 0.332194, acc.: 86.72%] [G loss: 3.672522]\n",
      "epoch:18 step:14472 [D loss: 0.255598, acc.: 90.62%] [G loss: 3.055171]\n",
      "epoch:18 step:14473 [D loss: 0.372095, acc.: 81.25%] [G loss: 2.609303]\n",
      "epoch:18 step:14474 [D loss: 0.274239, acc.: 86.72%] [G loss: 3.619548]\n",
      "epoch:18 step:14475 [D loss: 0.384360, acc.: 84.38%] [G loss: 3.023351]\n",
      "epoch:18 step:14476 [D loss: 0.313958, acc.: 84.38%] [G loss: 3.197291]\n",
      "epoch:18 step:14477 [D loss: 0.311556, acc.: 85.94%] [G loss: 3.285678]\n",
      "epoch:18 step:14478 [D loss: 0.266132, acc.: 88.28%] [G loss: 4.075288]\n",
      "epoch:18 step:14479 [D loss: 0.311957, acc.: 85.16%] [G loss: 3.375054]\n",
      "epoch:18 step:14480 [D loss: 0.381568, acc.: 85.16%] [G loss: 2.041517]\n",
      "epoch:18 step:14481 [D loss: 0.275265, acc.: 87.50%] [G loss: 2.985125]\n",
      "epoch:18 step:14482 [D loss: 0.359091, acc.: 80.47%] [G loss: 3.206084]\n",
      "epoch:18 step:14483 [D loss: 0.282751, acc.: 86.72%] [G loss: 4.188111]\n",
      "epoch:18 step:14484 [D loss: 0.343179, acc.: 83.59%] [G loss: 3.199663]\n",
      "epoch:18 step:14485 [D loss: 0.291177, acc.: 90.62%] [G loss: 4.945987]\n",
      "epoch:18 step:14486 [D loss: 0.427733, acc.: 83.59%] [G loss: 2.875167]\n",
      "epoch:18 step:14487 [D loss: 0.367900, acc.: 82.03%] [G loss: 2.883989]\n",
      "epoch:18 step:14488 [D loss: 0.408586, acc.: 83.59%] [G loss: 2.727386]\n",
      "epoch:18 step:14489 [D loss: 0.362770, acc.: 81.25%] [G loss: 3.021247]\n",
      "epoch:18 step:14490 [D loss: 0.309139, acc.: 88.28%] [G loss: 4.219556]\n",
      "epoch:18 step:14491 [D loss: 0.330502, acc.: 85.16%] [G loss: 2.729832]\n",
      "epoch:18 step:14492 [D loss: 0.421940, acc.: 86.72%] [G loss: 5.112752]\n",
      "epoch:18 step:14493 [D loss: 0.636918, acc.: 71.09%] [G loss: 2.484936]\n",
      "epoch:18 step:14494 [D loss: 0.269582, acc.: 90.62%] [G loss: 3.389346]\n",
      "epoch:18 step:14495 [D loss: 0.304581, acc.: 85.16%] [G loss: 3.282294]\n",
      "epoch:18 step:14496 [D loss: 0.366336, acc.: 82.81%] [G loss: 3.399421]\n",
      "epoch:18 step:14497 [D loss: 0.431417, acc.: 84.38%] [G loss: 2.492601]\n",
      "epoch:18 step:14498 [D loss: 0.308043, acc.: 86.72%] [G loss: 2.706460]\n",
      "epoch:18 step:14499 [D loss: 0.407509, acc.: 77.34%] [G loss: 2.499849]\n",
      "epoch:18 step:14500 [D loss: 0.291116, acc.: 88.28%] [G loss: 2.418200]\n",
      "epoch:18 step:14501 [D loss: 0.299440, acc.: 89.84%] [G loss: 2.788556]\n",
      "epoch:18 step:14502 [D loss: 0.327176, acc.: 85.16%] [G loss: 2.049307]\n",
      "epoch:18 step:14503 [D loss: 0.280486, acc.: 89.84%] [G loss: 3.123774]\n",
      "epoch:18 step:14504 [D loss: 0.379278, acc.: 82.81%] [G loss: 3.717498]\n",
      "epoch:18 step:14505 [D loss: 0.200134, acc.: 89.84%] [G loss: 5.001476]\n",
      "epoch:18 step:14506 [D loss: 0.302873, acc.: 87.50%] [G loss: 3.039960]\n",
      "epoch:18 step:14507 [D loss: 0.272118, acc.: 86.72%] [G loss: 3.248928]\n",
      "epoch:18 step:14508 [D loss: 0.288158, acc.: 85.94%] [G loss: 2.649760]\n",
      "epoch:18 step:14509 [D loss: 0.285410, acc.: 88.28%] [G loss: 2.665027]\n",
      "epoch:18 step:14510 [D loss: 0.435713, acc.: 80.47%] [G loss: 2.714200]\n",
      "epoch:18 step:14511 [D loss: 0.376440, acc.: 82.03%] [G loss: 2.987826]\n",
      "epoch:18 step:14512 [D loss: 0.251960, acc.: 89.84%] [G loss: 2.859159]\n",
      "epoch:18 step:14513 [D loss: 0.386987, acc.: 80.47%] [G loss: 3.261634]\n",
      "epoch:18 step:14514 [D loss: 0.280765, acc.: 91.41%] [G loss: 2.376118]\n",
      "epoch:18 step:14515 [D loss: 0.460473, acc.: 83.59%] [G loss: 2.538364]\n",
      "epoch:18 step:14516 [D loss: 0.294297, acc.: 85.16%] [G loss: 2.796301]\n",
      "epoch:18 step:14517 [D loss: 0.345416, acc.: 86.72%] [G loss: 2.904262]\n",
      "epoch:18 step:14518 [D loss: 0.272501, acc.: 87.50%] [G loss: 2.556172]\n",
      "epoch:18 step:14519 [D loss: 0.283972, acc.: 89.06%] [G loss: 2.783833]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:18 step:14520 [D loss: 0.241649, acc.: 89.84%] [G loss: 3.919858]\n",
      "epoch:18 step:14521 [D loss: 0.398091, acc.: 80.47%] [G loss: 2.813245]\n",
      "epoch:18 step:14522 [D loss: 0.268611, acc.: 85.94%] [G loss: 3.546852]\n",
      "epoch:18 step:14523 [D loss: 0.322043, acc.: 85.94%] [G loss: 3.237710]\n",
      "epoch:18 step:14524 [D loss: 0.283133, acc.: 87.50%] [G loss: 2.256898]\n",
      "epoch:18 step:14525 [D loss: 0.379683, acc.: 82.81%] [G loss: 2.380594]\n",
      "epoch:18 step:14526 [D loss: 0.328091, acc.: 84.38%] [G loss: 2.666969]\n",
      "epoch:18 step:14527 [D loss: 0.258929, acc.: 88.28%] [G loss: 3.272302]\n",
      "epoch:18 step:14528 [D loss: 0.259529, acc.: 89.06%] [G loss: 3.710204]\n",
      "epoch:18 step:14529 [D loss: 0.330197, acc.: 86.72%] [G loss: 4.023482]\n",
      "epoch:18 step:14530 [D loss: 0.322307, acc.: 85.16%] [G loss: 2.319082]\n",
      "epoch:18 step:14531 [D loss: 0.359722, acc.: 85.16%] [G loss: 2.568769]\n",
      "epoch:18 step:14532 [D loss: 0.305061, acc.: 85.94%] [G loss: 3.082894]\n",
      "epoch:18 step:14533 [D loss: 0.307249, acc.: 88.28%] [G loss: 2.619677]\n",
      "epoch:18 step:14534 [D loss: 0.373956, acc.: 82.81%] [G loss: 2.702147]\n",
      "epoch:18 step:14535 [D loss: 0.288769, acc.: 86.72%] [G loss: 3.964501]\n",
      "epoch:18 step:14536 [D loss: 0.388000, acc.: 82.81%] [G loss: 3.421801]\n",
      "epoch:18 step:14537 [D loss: 0.424633, acc.: 78.91%] [G loss: 2.534047]\n",
      "epoch:18 step:14538 [D loss: 0.268600, acc.: 86.72%] [G loss: 4.041553]\n",
      "epoch:18 step:14539 [D loss: 0.356884, acc.: 85.16%] [G loss: 3.635343]\n",
      "epoch:18 step:14540 [D loss: 0.306763, acc.: 85.94%] [G loss: 3.048793]\n",
      "epoch:18 step:14541 [D loss: 0.304929, acc.: 87.50%] [G loss: 3.199347]\n",
      "epoch:18 step:14542 [D loss: 0.403288, acc.: 82.81%] [G loss: 3.214307]\n",
      "epoch:18 step:14543 [D loss: 0.339856, acc.: 83.59%] [G loss: 2.697111]\n",
      "epoch:18 step:14544 [D loss: 0.290760, acc.: 88.28%] [G loss: 3.083587]\n",
      "epoch:18 step:14545 [D loss: 0.338580, acc.: 85.94%] [G loss: 3.234607]\n",
      "epoch:18 step:14546 [D loss: 0.303728, acc.: 89.06%] [G loss: 2.165056]\n",
      "epoch:18 step:14547 [D loss: 0.366831, acc.: 85.16%] [G loss: 2.965599]\n",
      "epoch:18 step:14548 [D loss: 0.295326, acc.: 88.28%] [G loss: 3.032968]\n",
      "epoch:18 step:14549 [D loss: 0.261781, acc.: 90.62%] [G loss: 2.731864]\n",
      "epoch:18 step:14550 [D loss: 0.377812, acc.: 83.59%] [G loss: 2.732665]\n",
      "epoch:18 step:14551 [D loss: 0.333241, acc.: 85.94%] [G loss: 2.965564]\n",
      "epoch:18 step:14552 [D loss: 0.391834, acc.: 84.38%] [G loss: 4.328353]\n",
      "epoch:18 step:14553 [D loss: 0.363752, acc.: 82.03%] [G loss: 3.920768]\n",
      "epoch:18 step:14554 [D loss: 0.248028, acc.: 89.84%] [G loss: 4.858182]\n",
      "epoch:18 step:14555 [D loss: 0.308207, acc.: 90.62%] [G loss: 2.880638]\n",
      "epoch:18 step:14556 [D loss: 0.230565, acc.: 92.19%] [G loss: 4.443815]\n",
      "epoch:18 step:14557 [D loss: 0.316409, acc.: 85.16%] [G loss: 3.346894]\n",
      "epoch:18 step:14558 [D loss: 0.341302, acc.: 88.28%] [G loss: 4.179880]\n",
      "epoch:18 step:14559 [D loss: 0.364377, acc.: 84.38%] [G loss: 3.173838]\n",
      "epoch:18 step:14560 [D loss: 0.286379, acc.: 86.72%] [G loss: 5.248642]\n",
      "epoch:18 step:14561 [D loss: 0.455763, acc.: 83.59%] [G loss: 2.978449]\n",
      "epoch:18 step:14562 [D loss: 0.294936, acc.: 88.28%] [G loss: 2.934404]\n",
      "epoch:18 step:14563 [D loss: 0.256983, acc.: 88.28%] [G loss: 3.719918]\n",
      "epoch:18 step:14564 [D loss: 0.368549, acc.: 82.81%] [G loss: 4.457951]\n",
      "epoch:18 step:14565 [D loss: 0.294971, acc.: 87.50%] [G loss: 2.524649]\n",
      "epoch:18 step:14566 [D loss: 0.235417, acc.: 89.06%] [G loss: 4.507555]\n",
      "epoch:18 step:14567 [D loss: 0.252159, acc.: 89.06%] [G loss: 4.156879]\n",
      "epoch:18 step:14568 [D loss: 0.325990, acc.: 84.38%] [G loss: 4.466020]\n",
      "epoch:18 step:14569 [D loss: 0.298845, acc.: 85.94%] [G loss: 2.913350]\n",
      "epoch:18 step:14570 [D loss: 0.190830, acc.: 96.88%] [G loss: 3.386272]\n",
      "epoch:18 step:14571 [D loss: 0.261604, acc.: 85.94%] [G loss: 3.955607]\n",
      "epoch:18 step:14572 [D loss: 0.317610, acc.: 87.50%] [G loss: 3.430586]\n",
      "epoch:18 step:14573 [D loss: 0.367645, acc.: 78.91%] [G loss: 3.408086]\n",
      "epoch:18 step:14574 [D loss: 0.254359, acc.: 91.41%] [G loss: 3.240925]\n",
      "epoch:18 step:14575 [D loss: 0.295382, acc.: 88.28%] [G loss: 3.224771]\n",
      "epoch:18 step:14576 [D loss: 0.325224, acc.: 87.50%] [G loss: 2.560258]\n",
      "epoch:18 step:14577 [D loss: 0.348949, acc.: 85.94%] [G loss: 4.232214]\n",
      "epoch:18 step:14578 [D loss: 0.311204, acc.: 88.28%] [G loss: 6.347993]\n",
      "epoch:18 step:14579 [D loss: 0.433625, acc.: 81.25%] [G loss: 3.081878]\n",
      "epoch:18 step:14580 [D loss: 0.266822, acc.: 85.94%] [G loss: 7.720642]\n",
      "epoch:18 step:14581 [D loss: 0.240930, acc.: 86.72%] [G loss: 7.202757]\n",
      "epoch:18 step:14582 [D loss: 0.273459, acc.: 85.16%] [G loss: 3.533043]\n",
      "epoch:18 step:14583 [D loss: 0.216596, acc.: 92.19%] [G loss: 4.254691]\n",
      "epoch:18 step:14584 [D loss: 0.275518, acc.: 86.72%] [G loss: 4.441440]\n",
      "epoch:18 step:14585 [D loss: 0.291570, acc.: 86.72%] [G loss: 3.138011]\n",
      "epoch:18 step:14586 [D loss: 0.345754, acc.: 82.81%] [G loss: 5.009691]\n",
      "epoch:18 step:14587 [D loss: 0.415179, acc.: 80.47%] [G loss: 2.641367]\n",
      "epoch:18 step:14588 [D loss: 0.376297, acc.: 81.25%] [G loss: 2.815640]\n",
      "epoch:18 step:14589 [D loss: 0.321245, acc.: 88.28%] [G loss: 2.442194]\n",
      "epoch:18 step:14590 [D loss: 0.336478, acc.: 85.16%] [G loss: 4.218446]\n",
      "epoch:18 step:14591 [D loss: 0.365199, acc.: 85.16%] [G loss: 4.047208]\n",
      "epoch:18 step:14592 [D loss: 0.222861, acc.: 90.62%] [G loss: 4.686365]\n",
      "epoch:18 step:14593 [D loss: 0.268632, acc.: 88.28%] [G loss: 4.576276]\n",
      "epoch:18 step:14594 [D loss: 0.344113, acc.: 84.38%] [G loss: 3.639834]\n",
      "epoch:18 step:14595 [D loss: 0.309622, acc.: 85.94%] [G loss: 3.033468]\n",
      "epoch:18 step:14596 [D loss: 0.334855, acc.: 85.16%] [G loss: 2.714861]\n",
      "epoch:18 step:14597 [D loss: 0.360908, acc.: 86.72%] [G loss: 3.269304]\n",
      "epoch:18 step:14598 [D loss: 0.320101, acc.: 86.72%] [G loss: 3.589218]\n",
      "epoch:18 step:14599 [D loss: 0.336457, acc.: 85.16%] [G loss: 3.641607]\n",
      "epoch:18 step:14600 [D loss: 0.328959, acc.: 87.50%] [G loss: 3.032222]\n",
      "##############\n",
      "[0.84519681 0.83674065 0.81365303 0.82658044 0.79990098 0.83476574\n",
      " 0.85705759 0.82918636 0.8142325  0.81398986]\n",
      "##########\n",
      "epoch:18 step:14601 [D loss: 0.225816, acc.: 91.41%] [G loss: 2.530771]\n",
      "epoch:18 step:14602 [D loss: 0.348880, acc.: 82.03%] [G loss: 3.545165]\n",
      "epoch:18 step:14603 [D loss: 0.212933, acc.: 92.19%] [G loss: 3.111341]\n",
      "epoch:18 step:14604 [D loss: 0.255840, acc.: 89.06%] [G loss: 5.017800]\n",
      "epoch:18 step:14605 [D loss: 0.355938, acc.: 83.59%] [G loss: 3.752064]\n",
      "epoch:18 step:14606 [D loss: 0.343500, acc.: 79.69%] [G loss: 3.514776]\n",
      "epoch:18 step:14607 [D loss: 0.326726, acc.: 89.06%] [G loss: 5.221210]\n",
      "epoch:18 step:14608 [D loss: 0.282530, acc.: 88.28%] [G loss: 2.936310]\n",
      "epoch:18 step:14609 [D loss: 0.304684, acc.: 89.06%] [G loss: 4.877975]\n",
      "epoch:18 step:14610 [D loss: 0.331496, acc.: 85.94%] [G loss: 3.425528]\n",
      "epoch:18 step:14611 [D loss: 0.220088, acc.: 91.41%] [G loss: 3.960355]\n",
      "epoch:18 step:14612 [D loss: 0.331987, acc.: 84.38%] [G loss: 5.386805]\n",
      "epoch:18 step:14613 [D loss: 0.275704, acc.: 87.50%] [G loss: 3.707207]\n",
      "epoch:18 step:14614 [D loss: 0.318451, acc.: 85.94%] [G loss: 3.735971]\n",
      "epoch:18 step:14615 [D loss: 0.186275, acc.: 91.41%] [G loss: 5.366412]\n",
      "epoch:18 step:14616 [D loss: 0.270571, acc.: 87.50%] [G loss: 4.441073]\n",
      "epoch:18 step:14617 [D loss: 0.313843, acc.: 86.72%] [G loss: 3.573209]\n",
      "epoch:18 step:14618 [D loss: 0.256541, acc.: 86.72%] [G loss: 4.506515]\n",
      "epoch:18 step:14619 [D loss: 0.272323, acc.: 86.72%] [G loss: 3.513781]\n",
      "epoch:18 step:14620 [D loss: 0.405760, acc.: 83.59%] [G loss: 3.214398]\n",
      "epoch:18 step:14621 [D loss: 0.311771, acc.: 85.16%] [G loss: 3.178875]\n",
      "epoch:18 step:14622 [D loss: 0.372187, acc.: 81.25%] [G loss: 2.692250]\n",
      "epoch:18 step:14623 [D loss: 0.294776, acc.: 85.94%] [G loss: 5.008570]\n",
      "epoch:18 step:14624 [D loss: 0.223533, acc.: 92.97%] [G loss: 6.915726]\n",
      "epoch:18 step:14625 [D loss: 0.380383, acc.: 80.47%] [G loss: 4.217342]\n",
      "epoch:18 step:14626 [D loss: 0.292822, acc.: 85.94%] [G loss: 6.891090]\n",
      "epoch:18 step:14627 [D loss: 0.322408, acc.: 84.38%] [G loss: 4.053269]\n",
      "epoch:18 step:14628 [D loss: 0.212869, acc.: 90.62%] [G loss: 3.541702]\n",
      "epoch:18 step:14629 [D loss: 0.268035, acc.: 89.84%] [G loss: 3.031105]\n",
      "epoch:18 step:14630 [D loss: 0.289702, acc.: 87.50%] [G loss: 2.976196]\n",
      "epoch:18 step:14631 [D loss: 0.359335, acc.: 82.03%] [G loss: 2.706408]\n",
      "epoch:18 step:14632 [D loss: 0.399207, acc.: 83.59%] [G loss: 3.356487]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:18 step:14633 [D loss: 0.463837, acc.: 83.59%] [G loss: 4.010783]\n",
      "epoch:18 step:14634 [D loss: 0.396242, acc.: 77.34%] [G loss: 2.886075]\n",
      "epoch:18 step:14635 [D loss: 0.202947, acc.: 92.19%] [G loss: 3.959558]\n",
      "epoch:18 step:14636 [D loss: 0.300037, acc.: 85.16%] [G loss: 5.149379]\n",
      "epoch:18 step:14637 [D loss: 0.307773, acc.: 85.16%] [G loss: 2.776849]\n",
      "epoch:18 step:14638 [D loss: 0.336892, acc.: 85.16%] [G loss: 3.795231]\n",
      "epoch:18 step:14639 [D loss: 0.259298, acc.: 89.84%] [G loss: 3.442272]\n",
      "epoch:18 step:14640 [D loss: 0.303411, acc.: 86.72%] [G loss: 2.695905]\n",
      "epoch:18 step:14641 [D loss: 0.348188, acc.: 85.16%] [G loss: 3.159140]\n",
      "epoch:18 step:14642 [D loss: 0.421426, acc.: 81.25%] [G loss: 2.909407]\n",
      "epoch:18 step:14643 [D loss: 0.348988, acc.: 85.94%] [G loss: 3.303965]\n",
      "epoch:18 step:14644 [D loss: 0.381399, acc.: 83.59%] [G loss: 2.649202]\n",
      "epoch:18 step:14645 [D loss: 0.304020, acc.: 87.50%] [G loss: 3.022480]\n",
      "epoch:18 step:14646 [D loss: 0.262307, acc.: 90.62%] [G loss: 2.600571]\n",
      "epoch:18 step:14647 [D loss: 0.331441, acc.: 85.94%] [G loss: 2.323288]\n",
      "epoch:18 step:14648 [D loss: 0.306421, acc.: 87.50%] [G loss: 2.483211]\n",
      "epoch:18 step:14649 [D loss: 0.343043, acc.: 86.72%] [G loss: 2.483189]\n",
      "epoch:18 step:14650 [D loss: 0.311790, acc.: 89.06%] [G loss: 2.353247]\n",
      "epoch:18 step:14651 [D loss: 0.233005, acc.: 92.97%] [G loss: 3.076669]\n",
      "epoch:18 step:14652 [D loss: 0.287779, acc.: 86.72%] [G loss: 3.415323]\n",
      "epoch:18 step:14653 [D loss: 0.335805, acc.: 88.28%] [G loss: 2.794943]\n",
      "epoch:18 step:14654 [D loss: 0.323613, acc.: 89.06%] [G loss: 2.709951]\n",
      "epoch:18 step:14655 [D loss: 0.289015, acc.: 89.06%] [G loss: 3.066524]\n",
      "epoch:18 step:14656 [D loss: 0.388883, acc.: 81.25%] [G loss: 2.694448]\n",
      "epoch:18 step:14657 [D loss: 0.324978, acc.: 85.94%] [G loss: 3.300104]\n",
      "epoch:18 step:14658 [D loss: 0.428252, acc.: 80.47%] [G loss: 3.238039]\n",
      "epoch:18 step:14659 [D loss: 0.458808, acc.: 78.12%] [G loss: 3.773529]\n",
      "epoch:18 step:14660 [D loss: 0.320878, acc.: 85.16%] [G loss: 3.032358]\n",
      "epoch:18 step:14661 [D loss: 0.318736, acc.: 85.16%] [G loss: 2.937709]\n",
      "epoch:18 step:14662 [D loss: 0.350376, acc.: 83.59%] [G loss: 2.950635]\n",
      "epoch:18 step:14663 [D loss: 0.374045, acc.: 83.59%] [G loss: 2.623235]\n",
      "epoch:18 step:14664 [D loss: 0.321031, acc.: 89.06%] [G loss: 3.626017]\n",
      "epoch:18 step:14665 [D loss: 0.241959, acc.: 91.41%] [G loss: 3.719468]\n",
      "epoch:18 step:14666 [D loss: 0.263942, acc.: 88.28%] [G loss: 2.899306]\n",
      "epoch:18 step:14667 [D loss: 0.317564, acc.: 83.59%] [G loss: 2.997499]\n",
      "epoch:18 step:14668 [D loss: 0.332376, acc.: 86.72%] [G loss: 3.736312]\n",
      "epoch:18 step:14669 [D loss: 0.287124, acc.: 89.84%] [G loss: 4.512559]\n",
      "epoch:18 step:14670 [D loss: 0.279198, acc.: 86.72%] [G loss: 3.760394]\n",
      "epoch:18 step:14671 [D loss: 0.305684, acc.: 86.72%] [G loss: 2.379580]\n",
      "epoch:18 step:14672 [D loss: 0.296558, acc.: 87.50%] [G loss: 3.521228]\n",
      "epoch:18 step:14673 [D loss: 0.229485, acc.: 90.62%] [G loss: 4.070610]\n",
      "epoch:18 step:14674 [D loss: 0.333279, acc.: 85.16%] [G loss: 2.839246]\n",
      "epoch:18 step:14675 [D loss: 0.429719, acc.: 82.03%] [G loss: 3.204352]\n",
      "epoch:18 step:14676 [D loss: 0.319699, acc.: 82.03%] [G loss: 4.093850]\n",
      "epoch:18 step:14677 [D loss: 0.458911, acc.: 79.69%] [G loss: 2.903033]\n",
      "epoch:18 step:14678 [D loss: 0.287307, acc.: 90.62%] [G loss: 3.336299]\n",
      "epoch:18 step:14679 [D loss: 0.262067, acc.: 90.62%] [G loss: 3.229916]\n",
      "epoch:18 step:14680 [D loss: 0.316777, acc.: 87.50%] [G loss: 4.611417]\n",
      "epoch:18 step:14681 [D loss: 0.507839, acc.: 78.12%] [G loss: 3.154418]\n",
      "epoch:18 step:14682 [D loss: 0.420401, acc.: 83.59%] [G loss: 3.205305]\n",
      "epoch:18 step:14683 [D loss: 0.386822, acc.: 84.38%] [G loss: 2.634144]\n",
      "epoch:18 step:14684 [D loss: 0.292187, acc.: 86.72%] [G loss: 2.905672]\n",
      "epoch:18 step:14685 [D loss: 0.365067, acc.: 85.16%] [G loss: 3.448012]\n",
      "epoch:18 step:14686 [D loss: 0.394658, acc.: 84.38%] [G loss: 2.529060]\n",
      "epoch:18 step:14687 [D loss: 0.384834, acc.: 88.28%] [G loss: 3.160921]\n",
      "epoch:18 step:14688 [D loss: 0.301580, acc.: 86.72%] [G loss: 3.523254]\n",
      "epoch:18 step:14689 [D loss: 0.286917, acc.: 86.72%] [G loss: 4.078892]\n",
      "epoch:18 step:14690 [D loss: 0.239084, acc.: 91.41%] [G loss: 2.969418]\n",
      "epoch:18 step:14691 [D loss: 0.398860, acc.: 84.38%] [G loss: 2.523151]\n",
      "epoch:18 step:14692 [D loss: 0.329868, acc.: 85.94%] [G loss: 2.776550]\n",
      "epoch:18 step:14693 [D loss: 0.433414, acc.: 80.47%] [G loss: 2.706116]\n",
      "epoch:18 step:14694 [D loss: 0.344063, acc.: 85.16%] [G loss: 3.056088]\n",
      "epoch:18 step:14695 [D loss: 0.259348, acc.: 90.62%] [G loss: 3.327122]\n",
      "epoch:18 step:14696 [D loss: 0.303017, acc.: 85.16%] [G loss: 4.705532]\n",
      "epoch:18 step:14697 [D loss: 0.389776, acc.: 76.56%] [G loss: 4.422926]\n",
      "epoch:18 step:14698 [D loss: 0.210690, acc.: 91.41%] [G loss: 5.454208]\n",
      "epoch:18 step:14699 [D loss: 0.227975, acc.: 92.19%] [G loss: 3.270797]\n",
      "epoch:18 step:14700 [D loss: 0.315268, acc.: 85.94%] [G loss: 3.637472]\n",
      "epoch:18 step:14701 [D loss: 0.276061, acc.: 89.84%] [G loss: 3.197027]\n",
      "epoch:18 step:14702 [D loss: 0.259574, acc.: 88.28%] [G loss: 3.694126]\n",
      "epoch:18 step:14703 [D loss: 0.408508, acc.: 80.47%] [G loss: 2.734731]\n",
      "epoch:18 step:14704 [D loss: 0.337794, acc.: 85.94%] [G loss: 4.070624]\n",
      "epoch:18 step:14705 [D loss: 0.299835, acc.: 87.50%] [G loss: 5.524897]\n",
      "epoch:18 step:14706 [D loss: 0.345216, acc.: 85.16%] [G loss: 4.217022]\n",
      "epoch:18 step:14707 [D loss: 0.485694, acc.: 78.12%] [G loss: 6.060603]\n",
      "epoch:18 step:14708 [D loss: 0.418893, acc.: 82.03%] [G loss: 3.924105]\n",
      "epoch:18 step:14709 [D loss: 0.361526, acc.: 80.47%] [G loss: 3.310945]\n",
      "epoch:18 step:14710 [D loss: 0.240707, acc.: 88.28%] [G loss: 4.217694]\n",
      "epoch:18 step:14711 [D loss: 0.278034, acc.: 91.41%] [G loss: 3.684443]\n",
      "epoch:18 step:14712 [D loss: 0.392178, acc.: 82.03%] [G loss: 3.180693]\n",
      "epoch:18 step:14713 [D loss: 0.530627, acc.: 77.34%] [G loss: 2.774887]\n",
      "epoch:18 step:14714 [D loss: 0.477539, acc.: 81.25%] [G loss: 2.735531]\n",
      "epoch:18 step:14715 [D loss: 0.347910, acc.: 83.59%] [G loss: 3.803264]\n",
      "epoch:18 step:14716 [D loss: 0.256784, acc.: 85.16%] [G loss: 4.550798]\n",
      "epoch:18 step:14717 [D loss: 0.275013, acc.: 84.38%] [G loss: 3.763516]\n",
      "epoch:18 step:14718 [D loss: 0.351263, acc.: 87.50%] [G loss: 4.123914]\n",
      "epoch:18 step:14719 [D loss: 0.311796, acc.: 87.50%] [G loss: 2.639788]\n",
      "epoch:18 step:14720 [D loss: 0.314606, acc.: 86.72%] [G loss: 3.241860]\n",
      "epoch:18 step:14721 [D loss: 0.311305, acc.: 88.28%] [G loss: 2.931839]\n",
      "epoch:18 step:14722 [D loss: 0.450680, acc.: 82.03%] [G loss: 2.679134]\n",
      "epoch:18 step:14723 [D loss: 0.281845, acc.: 87.50%] [G loss: 2.830490]\n",
      "epoch:18 step:14724 [D loss: 0.380211, acc.: 84.38%] [G loss: 2.845054]\n",
      "epoch:18 step:14725 [D loss: 0.273028, acc.: 87.50%] [G loss: 3.026871]\n",
      "epoch:18 step:14726 [D loss: 0.255729, acc.: 92.97%] [G loss: 3.087382]\n",
      "epoch:18 step:14727 [D loss: 0.229862, acc.: 92.19%] [G loss: 4.045111]\n",
      "epoch:18 step:14728 [D loss: 0.254584, acc.: 90.62%] [G loss: 4.192694]\n",
      "epoch:18 step:14729 [D loss: 0.351894, acc.: 81.25%] [G loss: 3.003785]\n",
      "epoch:18 step:14730 [D loss: 0.286677, acc.: 91.41%] [G loss: 2.956866]\n",
      "epoch:18 step:14731 [D loss: 0.304946, acc.: 84.38%] [G loss: 3.622760]\n",
      "epoch:18 step:14732 [D loss: 0.308569, acc.: 86.72%] [G loss: 3.596233]\n",
      "epoch:18 step:14733 [D loss: 0.266285, acc.: 86.72%] [G loss: 6.080136]\n",
      "epoch:18 step:14734 [D loss: 0.496189, acc.: 75.78%] [G loss: 6.183376]\n",
      "epoch:18 step:14735 [D loss: 0.524948, acc.: 75.00%] [G loss: 4.666889]\n",
      "epoch:18 step:14736 [D loss: 0.665412, acc.: 71.09%] [G loss: 4.963396]\n",
      "epoch:18 step:14737 [D loss: 0.315809, acc.: 85.16%] [G loss: 5.199752]\n",
      "epoch:18 step:14738 [D loss: 0.427683, acc.: 83.59%] [G loss: 4.232922]\n",
      "epoch:18 step:14739 [D loss: 0.367256, acc.: 82.81%] [G loss: 3.538393]\n",
      "epoch:18 step:14740 [D loss: 0.250991, acc.: 91.41%] [G loss: 3.868986]\n",
      "epoch:18 step:14741 [D loss: 0.314956, acc.: 86.72%] [G loss: 4.261460]\n",
      "epoch:18 step:14742 [D loss: 0.342817, acc.: 82.03%] [G loss: 4.778866]\n",
      "epoch:18 step:14743 [D loss: 0.233959, acc.: 90.62%] [G loss: 5.016091]\n",
      "epoch:18 step:14744 [D loss: 0.291098, acc.: 86.72%] [G loss: 3.332438]\n",
      "epoch:18 step:14745 [D loss: 0.270438, acc.: 91.41%] [G loss: 3.707967]\n",
      "epoch:18 step:14746 [D loss: 0.311408, acc.: 87.50%] [G loss: 4.281582]\n",
      "epoch:18 step:14747 [D loss: 0.217510, acc.: 92.97%] [G loss: 3.185918]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:18 step:14748 [D loss: 0.323031, acc.: 84.38%] [G loss: 3.470086]\n",
      "epoch:18 step:14749 [D loss: 0.293181, acc.: 88.28%] [G loss: 3.202298]\n",
      "epoch:18 step:14750 [D loss: 0.327649, acc.: 83.59%] [G loss: 4.027612]\n",
      "epoch:18 step:14751 [D loss: 0.477097, acc.: 75.00%] [G loss: 2.741908]\n",
      "epoch:18 step:14752 [D loss: 0.208613, acc.: 92.97%] [G loss: 3.259245]\n",
      "epoch:18 step:14753 [D loss: 0.273239, acc.: 89.84%] [G loss: 2.808828]\n",
      "epoch:18 step:14754 [D loss: 0.277503, acc.: 87.50%] [G loss: 3.446277]\n",
      "epoch:18 step:14755 [D loss: 0.197842, acc.: 91.41%] [G loss: 3.512606]\n",
      "epoch:18 step:14756 [D loss: 0.280037, acc.: 88.28%] [G loss: 4.708034]\n",
      "epoch:18 step:14757 [D loss: 0.198364, acc.: 92.19%] [G loss: 6.498207]\n",
      "epoch:18 step:14758 [D loss: 0.356559, acc.: 83.59%] [G loss: 2.787021]\n",
      "epoch:18 step:14759 [D loss: 0.398482, acc.: 81.25%] [G loss: 2.671173]\n",
      "epoch:18 step:14760 [D loss: 0.296678, acc.: 85.94%] [G loss: 3.383448]\n",
      "epoch:18 step:14761 [D loss: 0.294401, acc.: 85.94%] [G loss: 3.989432]\n",
      "epoch:18 step:14762 [D loss: 0.230690, acc.: 92.97%] [G loss: 4.553248]\n",
      "epoch:18 step:14763 [D loss: 0.310839, acc.: 89.84%] [G loss: 3.874819]\n",
      "epoch:18 step:14764 [D loss: 0.370924, acc.: 85.94%] [G loss: 4.083525]\n",
      "epoch:18 step:14765 [D loss: 0.311157, acc.: 84.38%] [G loss: 4.639495]\n",
      "epoch:18 step:14766 [D loss: 0.325483, acc.: 82.81%] [G loss: 4.395226]\n",
      "epoch:18 step:14767 [D loss: 0.294894, acc.: 89.06%] [G loss: 3.064757]\n",
      "epoch:18 step:14768 [D loss: 0.288992, acc.: 88.28%] [G loss: 3.833792]\n",
      "epoch:18 step:14769 [D loss: 0.214709, acc.: 92.97%] [G loss: 3.497630]\n",
      "epoch:18 step:14770 [D loss: 0.353661, acc.: 85.16%] [G loss: 3.155664]\n",
      "epoch:18 step:14771 [D loss: 0.323705, acc.: 86.72%] [G loss: 3.131012]\n",
      "epoch:18 step:14772 [D loss: 0.277055, acc.: 87.50%] [G loss: 3.689075]\n",
      "epoch:18 step:14773 [D loss: 0.382658, acc.: 82.81%] [G loss: 3.711095]\n",
      "epoch:18 step:14774 [D loss: 0.220317, acc.: 92.19%] [G loss: 4.832670]\n",
      "epoch:18 step:14775 [D loss: 0.281490, acc.: 85.16%] [G loss: 5.759837]\n",
      "epoch:18 step:14776 [D loss: 0.328110, acc.: 85.16%] [G loss: 3.726701]\n",
      "epoch:18 step:14777 [D loss: 0.290605, acc.: 85.16%] [G loss: 3.417629]\n",
      "epoch:18 step:14778 [D loss: 0.370726, acc.: 81.25%] [G loss: 2.772208]\n",
      "epoch:18 step:14779 [D loss: 0.301736, acc.: 86.72%] [G loss: 4.335360]\n",
      "epoch:18 step:14780 [D loss: 0.514179, acc.: 77.34%] [G loss: 5.741656]\n",
      "epoch:18 step:14781 [D loss: 0.366356, acc.: 81.25%] [G loss: 4.515203]\n",
      "epoch:18 step:14782 [D loss: 0.354414, acc.: 85.94%] [G loss: 2.352790]\n",
      "epoch:18 step:14783 [D loss: 0.340025, acc.: 89.84%] [G loss: 2.927540]\n",
      "epoch:18 step:14784 [D loss: 0.330568, acc.: 87.50%] [G loss: 2.989855]\n",
      "epoch:18 step:14785 [D loss: 0.379531, acc.: 80.47%] [G loss: 2.722758]\n",
      "epoch:18 step:14786 [D loss: 0.354940, acc.: 85.16%] [G loss: 2.200090]\n",
      "epoch:18 step:14787 [D loss: 0.314459, acc.: 82.81%] [G loss: 4.177295]\n",
      "epoch:18 step:14788 [D loss: 0.300948, acc.: 86.72%] [G loss: 2.668518]\n",
      "epoch:18 step:14789 [D loss: 0.354025, acc.: 85.16%] [G loss: 3.666775]\n",
      "epoch:18 step:14790 [D loss: 0.281177, acc.: 89.06%] [G loss: 4.910511]\n",
      "epoch:18 step:14791 [D loss: 0.244127, acc.: 89.06%] [G loss: 3.350339]\n",
      "epoch:18 step:14792 [D loss: 0.314454, acc.: 87.50%] [G loss: 5.078457]\n",
      "epoch:18 step:14793 [D loss: 0.529417, acc.: 74.22%] [G loss: 3.450053]\n",
      "epoch:18 step:14794 [D loss: 0.406261, acc.: 82.03%] [G loss: 3.674372]\n",
      "epoch:18 step:14795 [D loss: 0.232210, acc.: 89.84%] [G loss: 4.325040]\n",
      "epoch:18 step:14796 [D loss: 0.322284, acc.: 84.38%] [G loss: 4.105145]\n",
      "epoch:18 step:14797 [D loss: 0.298431, acc.: 89.06%] [G loss: 4.453443]\n",
      "epoch:18 step:14798 [D loss: 0.269687, acc.: 87.50%] [G loss: 5.204367]\n",
      "epoch:18 step:14799 [D loss: 0.233659, acc.: 92.97%] [G loss: 2.461067]\n",
      "epoch:18 step:14800 [D loss: 0.250776, acc.: 90.62%] [G loss: 3.178632]\n",
      "##############\n",
      "[0.86130643 0.84743528 0.79409744 0.8033568  0.7976873  0.82608786\n",
      " 0.87486957 0.82494844 0.82882277 0.80848075]\n",
      "##########\n",
      "epoch:18 step:14801 [D loss: 0.344312, acc.: 86.72%] [G loss: 4.224432]\n",
      "epoch:18 step:14802 [D loss: 0.220637, acc.: 89.06%] [G loss: 5.601102]\n",
      "epoch:18 step:14803 [D loss: 0.236821, acc.: 90.62%] [G loss: 3.828004]\n",
      "epoch:18 step:14804 [D loss: 0.371723, acc.: 80.47%] [G loss: 4.098501]\n",
      "epoch:18 step:14805 [D loss: 0.240441, acc.: 90.62%] [G loss: 3.319249]\n",
      "epoch:18 step:14806 [D loss: 0.299125, acc.: 86.72%] [G loss: 3.652613]\n",
      "epoch:18 step:14807 [D loss: 0.341929, acc.: 89.06%] [G loss: 3.478705]\n",
      "epoch:18 step:14808 [D loss: 0.339551, acc.: 86.72%] [G loss: 5.351463]\n",
      "epoch:18 step:14809 [D loss: 0.212932, acc.: 91.41%] [G loss: 7.256020]\n",
      "epoch:18 step:14810 [D loss: 0.192604, acc.: 90.62%] [G loss: 3.909588]\n",
      "epoch:18 step:14811 [D loss: 0.267676, acc.: 88.28%] [G loss: 3.809150]\n",
      "epoch:18 step:14812 [D loss: 0.246879, acc.: 91.41%] [G loss: 4.449600]\n",
      "epoch:18 step:14813 [D loss: 0.220983, acc.: 91.41%] [G loss: 3.552560]\n",
      "epoch:18 step:14814 [D loss: 0.239105, acc.: 90.62%] [G loss: 3.323243]\n",
      "epoch:18 step:14815 [D loss: 0.297638, acc.: 85.94%] [G loss: 4.142729]\n",
      "epoch:18 step:14816 [D loss: 0.313387, acc.: 85.16%] [G loss: 5.003067]\n",
      "epoch:18 step:14817 [D loss: 0.206553, acc.: 92.19%] [G loss: 4.065964]\n",
      "epoch:18 step:14818 [D loss: 0.291108, acc.: 87.50%] [G loss: 3.704343]\n",
      "epoch:18 step:14819 [D loss: 0.209390, acc.: 91.41%] [G loss: 3.058138]\n",
      "epoch:18 step:14820 [D loss: 0.345237, acc.: 84.38%] [G loss: 3.543468]\n",
      "epoch:18 step:14821 [D loss: 0.234994, acc.: 91.41%] [G loss: 6.635764]\n",
      "epoch:18 step:14822 [D loss: 0.292624, acc.: 89.84%] [G loss: 4.907475]\n",
      "epoch:18 step:14823 [D loss: 0.289560, acc.: 85.16%] [G loss: 5.066754]\n",
      "epoch:18 step:14824 [D loss: 0.306865, acc.: 89.84%] [G loss: 4.238701]\n",
      "epoch:18 step:14825 [D loss: 0.280969, acc.: 89.84%] [G loss: 2.605822]\n",
      "epoch:18 step:14826 [D loss: 0.316925, acc.: 81.25%] [G loss: 5.170253]\n",
      "epoch:18 step:14827 [D loss: 0.212848, acc.: 87.50%] [G loss: 4.682603]\n",
      "epoch:18 step:14828 [D loss: 0.268706, acc.: 89.06%] [G loss: 3.830425]\n",
      "epoch:18 step:14829 [D loss: 0.273413, acc.: 88.28%] [G loss: 2.463593]\n",
      "epoch:18 step:14830 [D loss: 0.231111, acc.: 92.97%] [G loss: 4.147285]\n",
      "epoch:18 step:14831 [D loss: 0.322335, acc.: 85.16%] [G loss: 4.584536]\n",
      "epoch:18 step:14832 [D loss: 0.466892, acc.: 78.12%] [G loss: 8.150528]\n",
      "epoch:18 step:14833 [D loss: 0.675092, acc.: 70.31%] [G loss: 6.076385]\n",
      "epoch:18 step:14834 [D loss: 1.098397, acc.: 69.53%] [G loss: 5.949587]\n",
      "epoch:18 step:14835 [D loss: 1.121953, acc.: 65.62%] [G loss: 8.443656]\n",
      "epoch:18 step:14836 [D loss: 1.278306, acc.: 63.28%] [G loss: 4.798090]\n",
      "epoch:18 step:14837 [D loss: 0.439233, acc.: 82.03%] [G loss: 4.115701]\n",
      "epoch:18 step:14838 [D loss: 0.491536, acc.: 82.03%] [G loss: 4.081973]\n",
      "epoch:18 step:14839 [D loss: 0.381692, acc.: 85.94%] [G loss: 4.186562]\n",
      "epoch:19 step:14840 [D loss: 0.349489, acc.: 83.59%] [G loss: 2.665741]\n",
      "epoch:19 step:14841 [D loss: 0.252275, acc.: 90.62%] [G loss: 3.644361]\n",
      "epoch:19 step:14842 [D loss: 0.446631, acc.: 82.81%] [G loss: 3.580662]\n",
      "epoch:19 step:14843 [D loss: 0.254297, acc.: 89.06%] [G loss: 3.360462]\n",
      "epoch:19 step:14844 [D loss: 0.283779, acc.: 90.62%] [G loss: 3.599328]\n",
      "epoch:19 step:14845 [D loss: 0.289779, acc.: 89.06%] [G loss: 3.359773]\n",
      "epoch:19 step:14846 [D loss: 0.314848, acc.: 86.72%] [G loss: 2.574618]\n",
      "epoch:19 step:14847 [D loss: 0.302128, acc.: 83.59%] [G loss: 2.898459]\n",
      "epoch:19 step:14848 [D loss: 0.315392, acc.: 91.41%] [G loss: 2.743878]\n",
      "epoch:19 step:14849 [D loss: 0.366779, acc.: 83.59%] [G loss: 2.447440]\n",
      "epoch:19 step:14850 [D loss: 0.257288, acc.: 89.06%] [G loss: 2.094918]\n",
      "epoch:19 step:14851 [D loss: 0.328353, acc.: 85.94%] [G loss: 3.349089]\n",
      "epoch:19 step:14852 [D loss: 0.280375, acc.: 88.28%] [G loss: 2.825455]\n",
      "epoch:19 step:14853 [D loss: 0.314795, acc.: 86.72%] [G loss: 2.940549]\n",
      "epoch:19 step:14854 [D loss: 0.286292, acc.: 90.62%] [G loss: 2.861509]\n",
      "epoch:19 step:14855 [D loss: 0.345338, acc.: 85.94%] [G loss: 3.052854]\n",
      "epoch:19 step:14856 [D loss: 0.308912, acc.: 89.06%] [G loss: 3.506752]\n",
      "epoch:19 step:14857 [D loss: 0.382461, acc.: 78.12%] [G loss: 2.393826]\n",
      "epoch:19 step:14858 [D loss: 0.245624, acc.: 92.19%] [G loss: 3.411927]\n",
      "epoch:19 step:14859 [D loss: 0.313021, acc.: 90.62%] [G loss: 3.602654]\n",
      "epoch:19 step:14860 [D loss: 0.398128, acc.: 82.81%] [G loss: 2.872405]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:19 step:14861 [D loss: 0.297432, acc.: 85.16%] [G loss: 2.519607]\n",
      "epoch:19 step:14862 [D loss: 0.407045, acc.: 82.03%] [G loss: 2.574321]\n",
      "epoch:19 step:14863 [D loss: 0.329987, acc.: 85.94%] [G loss: 2.801054]\n",
      "epoch:19 step:14864 [D loss: 0.298999, acc.: 87.50%] [G loss: 2.132057]\n",
      "epoch:19 step:14865 [D loss: 0.338796, acc.: 87.50%] [G loss: 2.964239]\n",
      "epoch:19 step:14866 [D loss: 0.225040, acc.: 92.97%] [G loss: 3.247879]\n",
      "epoch:19 step:14867 [D loss: 0.387088, acc.: 85.16%] [G loss: 3.156390]\n",
      "epoch:19 step:14868 [D loss: 0.325975, acc.: 83.59%] [G loss: 2.947567]\n",
      "epoch:19 step:14869 [D loss: 0.262437, acc.: 87.50%] [G loss: 2.971829]\n",
      "epoch:19 step:14870 [D loss: 0.354946, acc.: 85.16%] [G loss: 4.216802]\n",
      "epoch:19 step:14871 [D loss: 0.330229, acc.: 89.06%] [G loss: 2.867370]\n",
      "epoch:19 step:14872 [D loss: 0.333305, acc.: 86.72%] [G loss: 2.851103]\n",
      "epoch:19 step:14873 [D loss: 0.359797, acc.: 85.16%] [G loss: 2.649794]\n",
      "epoch:19 step:14874 [D loss: 0.277812, acc.: 88.28%] [G loss: 3.046213]\n",
      "epoch:19 step:14875 [D loss: 0.285195, acc.: 87.50%] [G loss: 3.236570]\n",
      "epoch:19 step:14876 [D loss: 0.348214, acc.: 85.94%] [G loss: 3.308734]\n",
      "epoch:19 step:14877 [D loss: 0.304232, acc.: 88.28%] [G loss: 2.758257]\n",
      "epoch:19 step:14878 [D loss: 0.346854, acc.: 88.28%] [G loss: 2.752702]\n",
      "epoch:19 step:14879 [D loss: 0.351633, acc.: 83.59%] [G loss: 2.835678]\n",
      "epoch:19 step:14880 [D loss: 0.422665, acc.: 78.12%] [G loss: 2.952893]\n",
      "epoch:19 step:14881 [D loss: 0.311730, acc.: 85.94%] [G loss: 2.633594]\n",
      "epoch:19 step:14882 [D loss: 0.410345, acc.: 81.25%] [G loss: 2.494805]\n",
      "epoch:19 step:14883 [D loss: 0.421492, acc.: 79.69%] [G loss: 2.712174]\n",
      "epoch:19 step:14884 [D loss: 0.432741, acc.: 75.78%] [G loss: 3.345780]\n",
      "epoch:19 step:14885 [D loss: 0.380743, acc.: 81.25%] [G loss: 3.445222]\n",
      "epoch:19 step:14886 [D loss: 0.349773, acc.: 85.16%] [G loss: 2.916062]\n",
      "epoch:19 step:14887 [D loss: 0.370418, acc.: 85.16%] [G loss: 2.448083]\n",
      "epoch:19 step:14888 [D loss: 0.245265, acc.: 91.41%] [G loss: 3.138308]\n",
      "epoch:19 step:14889 [D loss: 0.322874, acc.: 81.25%] [G loss: 3.277587]\n",
      "epoch:19 step:14890 [D loss: 0.381153, acc.: 82.03%] [G loss: 3.217603]\n",
      "epoch:19 step:14891 [D loss: 0.287334, acc.: 89.84%] [G loss: 3.975929]\n",
      "epoch:19 step:14892 [D loss: 0.366294, acc.: 81.25%] [G loss: 4.546870]\n",
      "epoch:19 step:14893 [D loss: 0.237371, acc.: 88.28%] [G loss: 4.484125]\n",
      "epoch:19 step:14894 [D loss: 0.283405, acc.: 89.06%] [G loss: 3.634763]\n",
      "epoch:19 step:14895 [D loss: 0.316503, acc.: 88.28%] [G loss: 2.790098]\n",
      "epoch:19 step:14896 [D loss: 0.316917, acc.: 86.72%] [G loss: 3.093384]\n",
      "epoch:19 step:14897 [D loss: 0.228538, acc.: 90.62%] [G loss: 5.245131]\n",
      "epoch:19 step:14898 [D loss: 0.303849, acc.: 80.47%] [G loss: 5.560927]\n",
      "epoch:19 step:14899 [D loss: 0.173381, acc.: 94.53%] [G loss: 5.755116]\n",
      "epoch:19 step:14900 [D loss: 0.344123, acc.: 85.94%] [G loss: 3.984182]\n",
      "epoch:19 step:14901 [D loss: 0.202514, acc.: 92.19%] [G loss: 4.641353]\n",
      "epoch:19 step:14902 [D loss: 0.322134, acc.: 88.28%] [G loss: 3.157703]\n",
      "epoch:19 step:14903 [D loss: 0.318820, acc.: 85.16%] [G loss: 3.514392]\n",
      "epoch:19 step:14904 [D loss: 0.355052, acc.: 79.69%] [G loss: 3.302043]\n",
      "epoch:19 step:14905 [D loss: 0.302167, acc.: 86.72%] [G loss: 4.009578]\n",
      "epoch:19 step:14906 [D loss: 0.289921, acc.: 88.28%] [G loss: 3.322140]\n",
      "epoch:19 step:14907 [D loss: 0.311192, acc.: 89.06%] [G loss: 3.458627]\n",
      "epoch:19 step:14908 [D loss: 0.225265, acc.: 92.97%] [G loss: 4.704165]\n",
      "epoch:19 step:14909 [D loss: 0.245828, acc.: 86.72%] [G loss: 4.446596]\n",
      "epoch:19 step:14910 [D loss: 0.431121, acc.: 76.56%] [G loss: 3.155191]\n",
      "epoch:19 step:14911 [D loss: 0.398473, acc.: 83.59%] [G loss: 4.250749]\n",
      "epoch:19 step:14912 [D loss: 0.347485, acc.: 87.50%] [G loss: 5.056919]\n",
      "epoch:19 step:14913 [D loss: 0.539578, acc.: 77.34%] [G loss: 3.681929]\n",
      "epoch:19 step:14914 [D loss: 0.580726, acc.: 78.91%] [G loss: 4.528449]\n",
      "epoch:19 step:14915 [D loss: 0.446877, acc.: 79.69%] [G loss: 4.200455]\n",
      "epoch:19 step:14916 [D loss: 0.417300, acc.: 81.25%] [G loss: 3.763595]\n",
      "epoch:19 step:14917 [D loss: 0.365873, acc.: 82.81%] [G loss: 3.150066]\n",
      "epoch:19 step:14918 [D loss: 0.485311, acc.: 77.34%] [G loss: 4.503524]\n",
      "epoch:19 step:14919 [D loss: 0.573989, acc.: 73.44%] [G loss: 4.575914]\n",
      "epoch:19 step:14920 [D loss: 0.410958, acc.: 84.38%] [G loss: 3.842700]\n",
      "epoch:19 step:14921 [D loss: 0.454938, acc.: 77.34%] [G loss: 3.933153]\n",
      "epoch:19 step:14922 [D loss: 0.341183, acc.: 82.03%] [G loss: 2.850698]\n",
      "epoch:19 step:14923 [D loss: 0.324775, acc.: 87.50%] [G loss: 3.281110]\n",
      "epoch:19 step:14924 [D loss: 0.316472, acc.: 82.81%] [G loss: 3.134852]\n",
      "epoch:19 step:14925 [D loss: 0.446078, acc.: 75.78%] [G loss: 4.002304]\n",
      "epoch:19 step:14926 [D loss: 0.437334, acc.: 78.91%] [G loss: 3.059171]\n",
      "epoch:19 step:14927 [D loss: 0.256500, acc.: 90.62%] [G loss: 2.729685]\n",
      "epoch:19 step:14928 [D loss: 0.342458, acc.: 86.72%] [G loss: 2.552690]\n",
      "epoch:19 step:14929 [D loss: 0.432049, acc.: 79.69%] [G loss: 3.347004]\n",
      "epoch:19 step:14930 [D loss: 0.210237, acc.: 91.41%] [G loss: 4.202140]\n",
      "epoch:19 step:14931 [D loss: 0.363385, acc.: 87.50%] [G loss: 3.200086]\n",
      "epoch:19 step:14932 [D loss: 0.230065, acc.: 91.41%] [G loss: 3.555327]\n",
      "epoch:19 step:14933 [D loss: 0.182187, acc.: 92.97%] [G loss: 5.215348]\n",
      "epoch:19 step:14934 [D loss: 0.440685, acc.: 75.78%] [G loss: 3.395210]\n",
      "epoch:19 step:14935 [D loss: 0.373043, acc.: 83.59%] [G loss: 3.675049]\n",
      "epoch:19 step:14936 [D loss: 0.207692, acc.: 93.75%] [G loss: 5.305291]\n",
      "epoch:19 step:14937 [D loss: 0.238852, acc.: 89.84%] [G loss: 3.757990]\n",
      "epoch:19 step:14938 [D loss: 0.229053, acc.: 90.62%] [G loss: 4.229618]\n",
      "epoch:19 step:14939 [D loss: 0.299369, acc.: 84.38%] [G loss: 3.739065]\n",
      "epoch:19 step:14940 [D loss: 0.230384, acc.: 92.97%] [G loss: 3.222821]\n",
      "epoch:19 step:14941 [D loss: 0.323151, acc.: 85.94%] [G loss: 3.994406]\n",
      "epoch:19 step:14942 [D loss: 0.331315, acc.: 83.59%] [G loss: 2.617289]\n",
      "epoch:19 step:14943 [D loss: 0.502365, acc.: 76.56%] [G loss: 2.407449]\n",
      "epoch:19 step:14944 [D loss: 0.355451, acc.: 82.03%] [G loss: 3.679431]\n",
      "epoch:19 step:14945 [D loss: 0.359591, acc.: 85.94%] [G loss: 3.319055]\n",
      "epoch:19 step:14946 [D loss: 0.326377, acc.: 85.16%] [G loss: 2.957376]\n",
      "epoch:19 step:14947 [D loss: 0.272720, acc.: 89.84%] [G loss: 3.408899]\n",
      "epoch:19 step:14948 [D loss: 0.355389, acc.: 85.16%] [G loss: 3.060830]\n",
      "epoch:19 step:14949 [D loss: 0.360413, acc.: 85.16%] [G loss: 2.748829]\n",
      "epoch:19 step:14950 [D loss: 0.393299, acc.: 84.38%] [G loss: 3.311801]\n",
      "epoch:19 step:14951 [D loss: 0.333866, acc.: 82.81%] [G loss: 2.752134]\n",
      "epoch:19 step:14952 [D loss: 0.439281, acc.: 84.38%] [G loss: 3.356139]\n",
      "epoch:19 step:14953 [D loss: 0.338752, acc.: 86.72%] [G loss: 3.777354]\n",
      "epoch:19 step:14954 [D loss: 0.317637, acc.: 85.94%] [G loss: 4.650835]\n",
      "epoch:19 step:14955 [D loss: 0.361510, acc.: 82.81%] [G loss: 3.809072]\n",
      "epoch:19 step:14956 [D loss: 0.298900, acc.: 82.03%] [G loss: 2.938766]\n",
      "epoch:19 step:14957 [D loss: 0.254065, acc.: 90.62%] [G loss: 6.656358]\n",
      "epoch:19 step:14958 [D loss: 0.379297, acc.: 83.59%] [G loss: 3.287467]\n",
      "epoch:19 step:14959 [D loss: 0.253919, acc.: 90.62%] [G loss: 3.316441]\n",
      "epoch:19 step:14960 [D loss: 0.343288, acc.: 85.16%] [G loss: 2.752715]\n",
      "epoch:19 step:14961 [D loss: 0.229632, acc.: 92.19%] [G loss: 3.075188]\n",
      "epoch:19 step:14962 [D loss: 0.249218, acc.: 91.41%] [G loss: 3.028432]\n",
      "epoch:19 step:14963 [D loss: 0.314411, acc.: 87.50%] [G loss: 2.858274]\n",
      "epoch:19 step:14964 [D loss: 0.454111, acc.: 78.12%] [G loss: 3.823452]\n",
      "epoch:19 step:14965 [D loss: 0.201369, acc.: 91.41%] [G loss: 4.096385]\n",
      "epoch:19 step:14966 [D loss: 0.196490, acc.: 90.62%] [G loss: 3.475208]\n",
      "epoch:19 step:14967 [D loss: 0.234845, acc.: 93.75%] [G loss: 6.951989]\n",
      "epoch:19 step:14968 [D loss: 0.316359, acc.: 85.16%] [G loss: 4.793204]\n",
      "epoch:19 step:14969 [D loss: 0.274698, acc.: 89.84%] [G loss: 3.997149]\n",
      "epoch:19 step:14970 [D loss: 0.300421, acc.: 87.50%] [G loss: 3.946742]\n",
      "epoch:19 step:14971 [D loss: 0.213219, acc.: 92.19%] [G loss: 3.485839]\n",
      "epoch:19 step:14972 [D loss: 0.356003, acc.: 83.59%] [G loss: 3.515991]\n",
      "epoch:19 step:14973 [D loss: 0.262949, acc.: 91.41%] [G loss: 3.296199]\n",
      "epoch:19 step:14974 [D loss: 0.282196, acc.: 89.06%] [G loss: 3.647659]\n",
      "epoch:19 step:14975 [D loss: 0.322408, acc.: 87.50%] [G loss: 4.302062]\n",
      "epoch:19 step:14976 [D loss: 0.345605, acc.: 83.59%] [G loss: 5.704914]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:19 step:14977 [D loss: 0.354447, acc.: 84.38%] [G loss: 4.068992]\n",
      "epoch:19 step:14978 [D loss: 0.200535, acc.: 92.19%] [G loss: 6.023931]\n",
      "epoch:19 step:14979 [D loss: 0.355657, acc.: 82.03%] [G loss: 6.094577]\n",
      "epoch:19 step:14980 [D loss: 0.311933, acc.: 85.16%] [G loss: 5.385763]\n",
      "epoch:19 step:14981 [D loss: 0.309927, acc.: 85.16%] [G loss: 2.982089]\n",
      "epoch:19 step:14982 [D loss: 0.292369, acc.: 87.50%] [G loss: 3.023445]\n",
      "epoch:19 step:14983 [D loss: 0.458887, acc.: 78.12%] [G loss: 3.020811]\n",
      "epoch:19 step:14984 [D loss: 0.359485, acc.: 85.94%] [G loss: 3.290218]\n",
      "epoch:19 step:14985 [D loss: 0.424282, acc.: 80.47%] [G loss: 3.501597]\n",
      "epoch:19 step:14986 [D loss: 0.288146, acc.: 88.28%] [G loss: 4.120739]\n",
      "epoch:19 step:14987 [D loss: 0.313701, acc.: 88.28%] [G loss: 4.481352]\n",
      "epoch:19 step:14988 [D loss: 0.342710, acc.: 85.94%] [G loss: 3.175103]\n",
      "epoch:19 step:14989 [D loss: 0.469370, acc.: 82.81%] [G loss: 3.025687]\n",
      "epoch:19 step:14990 [D loss: 0.324203, acc.: 85.94%] [G loss: 3.079106]\n",
      "epoch:19 step:14991 [D loss: 0.347420, acc.: 86.72%] [G loss: 4.793447]\n",
      "epoch:19 step:14992 [D loss: 0.291814, acc.: 89.84%] [G loss: 4.298098]\n",
      "epoch:19 step:14993 [D loss: 0.308649, acc.: 88.28%] [G loss: 3.617378]\n",
      "epoch:19 step:14994 [D loss: 0.250563, acc.: 89.84%] [G loss: 4.546174]\n",
      "epoch:19 step:14995 [D loss: 0.473343, acc.: 80.47%] [G loss: 4.370989]\n",
      "epoch:19 step:14996 [D loss: 0.394017, acc.: 80.47%] [G loss: 3.881337]\n",
      "epoch:19 step:14997 [D loss: 0.280856, acc.: 83.59%] [G loss: 2.551676]\n",
      "epoch:19 step:14998 [D loss: 0.300190, acc.: 88.28%] [G loss: 3.634438]\n",
      "epoch:19 step:14999 [D loss: 0.379919, acc.: 80.47%] [G loss: 3.857025]\n",
      "epoch:19 step:15000 [D loss: 0.275631, acc.: 88.28%] [G loss: 3.357942]\n",
      "##############\n",
      "[0.81660168 0.87647462 0.80566432 0.79930624 0.75977796 0.82223439\n",
      " 0.86531687 0.82700242 0.81012915 0.84271359]\n",
      "##########\n",
      "epoch:19 step:15001 [D loss: 0.286426, acc.: 86.72%] [G loss: 3.033015]\n",
      "epoch:19 step:15002 [D loss: 0.320194, acc.: 87.50%] [G loss: 3.404500]\n",
      "epoch:19 step:15003 [D loss: 0.277001, acc.: 87.50%] [G loss: 4.330075]\n",
      "epoch:19 step:15004 [D loss: 0.239341, acc.: 88.28%] [G loss: 3.439823]\n",
      "epoch:19 step:15005 [D loss: 0.280806, acc.: 88.28%] [G loss: 4.308429]\n",
      "epoch:19 step:15006 [D loss: 0.272961, acc.: 89.84%] [G loss: 4.964698]\n",
      "epoch:19 step:15007 [D loss: 0.248139, acc.: 89.84%] [G loss: 5.236111]\n",
      "epoch:19 step:15008 [D loss: 0.298158, acc.: 85.16%] [G loss: 4.814437]\n",
      "epoch:19 step:15009 [D loss: 0.206232, acc.: 90.62%] [G loss: 5.052693]\n",
      "epoch:19 step:15010 [D loss: 0.366371, acc.: 82.03%] [G loss: 3.324572]\n",
      "epoch:19 step:15011 [D loss: 0.223505, acc.: 91.41%] [G loss: 2.891378]\n",
      "epoch:19 step:15012 [D loss: 0.425940, acc.: 82.03%] [G loss: 3.385013]\n",
      "epoch:19 step:15013 [D loss: 0.370100, acc.: 87.50%] [G loss: 4.424330]\n",
      "epoch:19 step:15014 [D loss: 0.341795, acc.: 87.50%] [G loss: 3.469823]\n",
      "epoch:19 step:15015 [D loss: 0.374003, acc.: 82.03%] [G loss: 2.664123]\n",
      "epoch:19 step:15016 [D loss: 0.296242, acc.: 81.25%] [G loss: 2.967136]\n",
      "epoch:19 step:15017 [D loss: 0.309913, acc.: 85.94%] [G loss: 2.533869]\n",
      "epoch:19 step:15018 [D loss: 0.303386, acc.: 89.84%] [G loss: 3.416394]\n",
      "epoch:19 step:15019 [D loss: 0.388392, acc.: 79.69%] [G loss: 4.605731]\n",
      "epoch:19 step:15020 [D loss: 0.249735, acc.: 89.84%] [G loss: 3.557221]\n",
      "epoch:19 step:15021 [D loss: 0.354813, acc.: 82.03%] [G loss: 2.537667]\n",
      "epoch:19 step:15022 [D loss: 0.266540, acc.: 89.84%] [G loss: 2.783875]\n",
      "epoch:19 step:15023 [D loss: 0.360315, acc.: 87.50%] [G loss: 2.879145]\n",
      "epoch:19 step:15024 [D loss: 0.236836, acc.: 87.50%] [G loss: 4.366442]\n",
      "epoch:19 step:15025 [D loss: 0.345656, acc.: 86.72%] [G loss: 5.344646]\n",
      "epoch:19 step:15026 [D loss: 0.254535, acc.: 89.06%] [G loss: 3.101669]\n",
      "epoch:19 step:15027 [D loss: 0.273594, acc.: 88.28%] [G loss: 3.508241]\n",
      "epoch:19 step:15028 [D loss: 0.316925, acc.: 85.16%] [G loss: 3.981154]\n",
      "epoch:19 step:15029 [D loss: 0.252424, acc.: 91.41%] [G loss: 6.368495]\n",
      "epoch:19 step:15030 [D loss: 0.384793, acc.: 82.03%] [G loss: 3.562092]\n",
      "epoch:19 step:15031 [D loss: 0.319261, acc.: 82.03%] [G loss: 3.607277]\n",
      "epoch:19 step:15032 [D loss: 0.356608, acc.: 79.69%] [G loss: 4.773129]\n",
      "epoch:19 step:15033 [D loss: 0.284825, acc.: 85.94%] [G loss: 4.873482]\n",
      "epoch:19 step:15034 [D loss: 0.315594, acc.: 83.59%] [G loss: 3.631634]\n",
      "epoch:19 step:15035 [D loss: 0.322535, acc.: 88.28%] [G loss: 4.651196]\n",
      "epoch:19 step:15036 [D loss: 0.489438, acc.: 81.25%] [G loss: 4.090288]\n",
      "epoch:19 step:15037 [D loss: 0.246539, acc.: 91.41%] [G loss: 3.436411]\n",
      "epoch:19 step:15038 [D loss: 0.341654, acc.: 82.03%] [G loss: 4.396114]\n",
      "epoch:19 step:15039 [D loss: 0.212675, acc.: 90.62%] [G loss: 5.171934]\n",
      "epoch:19 step:15040 [D loss: 0.291062, acc.: 85.94%] [G loss: 3.513286]\n",
      "epoch:19 step:15041 [D loss: 0.426672, acc.: 80.47%] [G loss: 3.846619]\n",
      "epoch:19 step:15042 [D loss: 0.447393, acc.: 75.78%] [G loss: 4.948404]\n",
      "epoch:19 step:15043 [D loss: 0.316459, acc.: 85.94%] [G loss: 3.721952]\n",
      "epoch:19 step:15044 [D loss: 0.233821, acc.: 92.97%] [G loss: 3.530384]\n",
      "epoch:19 step:15045 [D loss: 0.349160, acc.: 82.81%] [G loss: 2.678120]\n",
      "epoch:19 step:15046 [D loss: 0.377600, acc.: 82.81%] [G loss: 2.642786]\n",
      "epoch:19 step:15047 [D loss: 0.505427, acc.: 77.34%] [G loss: 2.831478]\n",
      "epoch:19 step:15048 [D loss: 0.325634, acc.: 85.16%] [G loss: 3.624462]\n",
      "epoch:19 step:15049 [D loss: 0.302278, acc.: 84.38%] [G loss: 4.456782]\n",
      "epoch:19 step:15050 [D loss: 0.326138, acc.: 85.16%] [G loss: 5.940620]\n",
      "epoch:19 step:15051 [D loss: 0.252646, acc.: 91.41%] [G loss: 3.792604]\n",
      "epoch:19 step:15052 [D loss: 0.278653, acc.: 88.28%] [G loss: 6.900426]\n",
      "epoch:19 step:15053 [D loss: 0.343803, acc.: 85.16%] [G loss: 2.653991]\n",
      "epoch:19 step:15054 [D loss: 0.274649, acc.: 89.06%] [G loss: 4.482430]\n",
      "epoch:19 step:15055 [D loss: 0.267962, acc.: 85.16%] [G loss: 4.901795]\n",
      "epoch:19 step:15056 [D loss: 0.388942, acc.: 82.81%] [G loss: 3.326728]\n",
      "epoch:19 step:15057 [D loss: 0.399932, acc.: 84.38%] [G loss: 4.056111]\n",
      "epoch:19 step:15058 [D loss: 0.263502, acc.: 89.84%] [G loss: 5.093670]\n",
      "epoch:19 step:15059 [D loss: 0.351107, acc.: 83.59%] [G loss: 4.143034]\n",
      "epoch:19 step:15060 [D loss: 0.261717, acc.: 90.62%] [G loss: 2.885278]\n",
      "epoch:19 step:15061 [D loss: 0.298174, acc.: 85.16%] [G loss: 3.864977]\n",
      "epoch:19 step:15062 [D loss: 0.343800, acc.: 85.94%] [G loss: 3.586264]\n",
      "epoch:19 step:15063 [D loss: 0.559419, acc.: 73.44%] [G loss: 8.008124]\n",
      "epoch:19 step:15064 [D loss: 0.983372, acc.: 67.19%] [G loss: 8.806667]\n",
      "epoch:19 step:15065 [D loss: 1.209302, acc.: 60.94%] [G loss: 5.686315]\n",
      "epoch:19 step:15066 [D loss: 0.571809, acc.: 75.00%] [G loss: 6.838974]\n",
      "epoch:19 step:15067 [D loss: 0.698442, acc.: 67.97%] [G loss: 4.204269]\n",
      "epoch:19 step:15068 [D loss: 0.393947, acc.: 81.25%] [G loss: 3.716908]\n",
      "epoch:19 step:15069 [D loss: 0.354853, acc.: 85.16%] [G loss: 3.269532]\n",
      "epoch:19 step:15070 [D loss: 0.408742, acc.: 85.16%] [G loss: 3.052972]\n",
      "epoch:19 step:15071 [D loss: 0.399071, acc.: 83.59%] [G loss: 3.399198]\n",
      "epoch:19 step:15072 [D loss: 0.374101, acc.: 87.50%] [G loss: 3.844358]\n",
      "epoch:19 step:15073 [D loss: 0.418787, acc.: 84.38%] [G loss: 4.369579]\n",
      "epoch:19 step:15074 [D loss: 0.357149, acc.: 82.03%] [G loss: 3.217235]\n",
      "epoch:19 step:15075 [D loss: 0.292989, acc.: 85.94%] [G loss: 6.138345]\n",
      "epoch:19 step:15076 [D loss: 0.323598, acc.: 85.94%] [G loss: 5.202507]\n",
      "epoch:19 step:15077 [D loss: 0.322531, acc.: 85.16%] [G loss: 2.932690]\n",
      "epoch:19 step:15078 [D loss: 0.289125, acc.: 88.28%] [G loss: 2.648584]\n",
      "epoch:19 step:15079 [D loss: 0.328530, acc.: 83.59%] [G loss: 2.263474]\n",
      "epoch:19 step:15080 [D loss: 0.382374, acc.: 85.16%] [G loss: 3.110053]\n",
      "epoch:19 step:15081 [D loss: 0.481863, acc.: 74.22%] [G loss: 3.217874]\n",
      "epoch:19 step:15082 [D loss: 0.358348, acc.: 88.28%] [G loss: 2.822071]\n",
      "epoch:19 step:15083 [D loss: 0.277808, acc.: 91.41%] [G loss: 2.208027]\n",
      "epoch:19 step:15084 [D loss: 0.287735, acc.: 87.50%] [G loss: 2.716071]\n",
      "epoch:19 step:15085 [D loss: 0.362483, acc.: 83.59%] [G loss: 2.445618]\n",
      "epoch:19 step:15086 [D loss: 0.353291, acc.: 84.38%] [G loss: 2.773836]\n",
      "epoch:19 step:15087 [D loss: 0.198607, acc.: 95.31%] [G loss: 2.783408]\n",
      "epoch:19 step:15088 [D loss: 0.360391, acc.: 81.25%] [G loss: 2.693148]\n",
      "epoch:19 step:15089 [D loss: 0.366346, acc.: 85.94%] [G loss: 3.084451]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:19 step:15090 [D loss: 0.314555, acc.: 88.28%] [G loss: 2.945795]\n",
      "epoch:19 step:15091 [D loss: 0.374982, acc.: 81.25%] [G loss: 3.149410]\n",
      "epoch:19 step:15092 [D loss: 0.364786, acc.: 85.16%] [G loss: 3.722654]\n",
      "epoch:19 step:15093 [D loss: 0.224124, acc.: 89.06%] [G loss: 2.900102]\n",
      "epoch:19 step:15094 [D loss: 0.304390, acc.: 85.16%] [G loss: 3.875758]\n",
      "epoch:19 step:15095 [D loss: 0.304347, acc.: 87.50%] [G loss: 5.270796]\n",
      "epoch:19 step:15096 [D loss: 0.339573, acc.: 83.59%] [G loss: 3.589898]\n",
      "epoch:19 step:15097 [D loss: 0.487294, acc.: 81.25%] [G loss: 2.764301]\n",
      "epoch:19 step:15098 [D loss: 0.355942, acc.: 85.16%] [G loss: 2.905493]\n",
      "epoch:19 step:15099 [D loss: 0.303674, acc.: 84.38%] [G loss: 3.356121]\n",
      "epoch:19 step:15100 [D loss: 0.351282, acc.: 84.38%] [G loss: 2.967751]\n",
      "epoch:19 step:15101 [D loss: 0.227924, acc.: 92.19%] [G loss: 4.358894]\n",
      "epoch:19 step:15102 [D loss: 0.272974, acc.: 88.28%] [G loss: 3.804470]\n",
      "epoch:19 step:15103 [D loss: 0.247648, acc.: 87.50%] [G loss: 4.626481]\n",
      "epoch:19 step:15104 [D loss: 0.360118, acc.: 84.38%] [G loss: 3.890874]\n",
      "epoch:19 step:15105 [D loss: 0.450482, acc.: 81.25%] [G loss: 3.229178]\n",
      "epoch:19 step:15106 [D loss: 0.279335, acc.: 89.06%] [G loss: 3.896580]\n",
      "epoch:19 step:15107 [D loss: 0.309372, acc.: 84.38%] [G loss: 3.373837]\n",
      "epoch:19 step:15108 [D loss: 0.360996, acc.: 85.94%] [G loss: 3.360231]\n",
      "epoch:19 step:15109 [D loss: 0.248961, acc.: 89.06%] [G loss: 3.389516]\n",
      "epoch:19 step:15110 [D loss: 0.255906, acc.: 88.28%] [G loss: 2.940044]\n",
      "epoch:19 step:15111 [D loss: 0.223903, acc.: 92.19%] [G loss: 3.809017]\n",
      "epoch:19 step:15112 [D loss: 0.374823, acc.: 83.59%] [G loss: 2.815781]\n",
      "epoch:19 step:15113 [D loss: 0.322044, acc.: 89.06%] [G loss: 2.826915]\n",
      "epoch:19 step:15114 [D loss: 0.375109, acc.: 85.94%] [G loss: 2.924070]\n",
      "epoch:19 step:15115 [D loss: 0.292572, acc.: 87.50%] [G loss: 3.651381]\n",
      "epoch:19 step:15116 [D loss: 0.395781, acc.: 81.25%] [G loss: 3.506228]\n",
      "epoch:19 step:15117 [D loss: 0.295516, acc.: 87.50%] [G loss: 3.165737]\n",
      "epoch:19 step:15118 [D loss: 0.363230, acc.: 83.59%] [G loss: 3.265474]\n",
      "epoch:19 step:15119 [D loss: 0.276565, acc.: 87.50%] [G loss: 2.973494]\n",
      "epoch:19 step:15120 [D loss: 0.399653, acc.: 84.38%] [G loss: 3.155343]\n",
      "epoch:19 step:15121 [D loss: 0.169704, acc.: 95.31%] [G loss: 8.419575]\n",
      "epoch:19 step:15122 [D loss: 0.238289, acc.: 89.84%] [G loss: 5.004597]\n",
      "epoch:19 step:15123 [D loss: 0.316581, acc.: 83.59%] [G loss: 6.015654]\n",
      "epoch:19 step:15124 [D loss: 0.239987, acc.: 89.06%] [G loss: 4.348780]\n",
      "epoch:19 step:15125 [D loss: 0.271168, acc.: 87.50%] [G loss: 4.676125]\n",
      "epoch:19 step:15126 [D loss: 0.172940, acc.: 92.97%] [G loss: 5.114263]\n",
      "epoch:19 step:15127 [D loss: 0.343661, acc.: 86.72%] [G loss: 3.640602]\n",
      "epoch:19 step:15128 [D loss: 0.269597, acc.: 85.94%] [G loss: 6.205729]\n",
      "epoch:19 step:15129 [D loss: 0.236786, acc.: 87.50%] [G loss: 3.615106]\n",
      "epoch:19 step:15130 [D loss: 0.325692, acc.: 89.06%] [G loss: 3.839406]\n",
      "epoch:19 step:15131 [D loss: 0.308065, acc.: 91.41%] [G loss: 3.546171]\n",
      "epoch:19 step:15132 [D loss: 0.281788, acc.: 89.84%] [G loss: 3.750417]\n",
      "epoch:19 step:15133 [D loss: 0.327478, acc.: 83.59%] [G loss: 4.825191]\n",
      "epoch:19 step:15134 [D loss: 0.293135, acc.: 85.16%] [G loss: 3.060287]\n",
      "epoch:19 step:15135 [D loss: 0.233334, acc.: 91.41%] [G loss: 3.938057]\n",
      "epoch:19 step:15136 [D loss: 0.303974, acc.: 84.38%] [G loss: 6.053419]\n",
      "epoch:19 step:15137 [D loss: 0.361774, acc.: 85.16%] [G loss: 4.281877]\n",
      "epoch:19 step:15138 [D loss: 0.389393, acc.: 85.16%] [G loss: 4.771541]\n",
      "epoch:19 step:15139 [D loss: 0.659322, acc.: 67.97%] [G loss: 5.996017]\n",
      "epoch:19 step:15140 [D loss: 1.149137, acc.: 73.44%] [G loss: 7.701734]\n",
      "epoch:19 step:15141 [D loss: 1.993707, acc.: 53.12%] [G loss: 2.761662]\n",
      "epoch:19 step:15142 [D loss: 0.326637, acc.: 85.94%] [G loss: 3.430145]\n",
      "epoch:19 step:15143 [D loss: 0.544403, acc.: 77.34%] [G loss: 4.177151]\n",
      "epoch:19 step:15144 [D loss: 0.376739, acc.: 80.47%] [G loss: 3.646709]\n",
      "epoch:19 step:15145 [D loss: 0.533827, acc.: 78.91%] [G loss: 6.118124]\n",
      "epoch:19 step:15146 [D loss: 0.407724, acc.: 82.81%] [G loss: 3.692560]\n",
      "epoch:19 step:15147 [D loss: 0.281098, acc.: 82.81%] [G loss: 3.158684]\n",
      "epoch:19 step:15148 [D loss: 0.296323, acc.: 89.06%] [G loss: 3.641038]\n",
      "epoch:19 step:15149 [D loss: 0.256135, acc.: 88.28%] [G loss: 3.416251]\n",
      "epoch:19 step:15150 [D loss: 0.368981, acc.: 86.72%] [G loss: 3.029175]\n",
      "epoch:19 step:15151 [D loss: 0.323133, acc.: 84.38%] [G loss: 2.825737]\n",
      "epoch:19 step:15152 [D loss: 0.363745, acc.: 86.72%] [G loss: 3.658087]\n",
      "epoch:19 step:15153 [D loss: 0.345067, acc.: 83.59%] [G loss: 3.055114]\n",
      "epoch:19 step:15154 [D loss: 0.549457, acc.: 77.34%] [G loss: 4.049897]\n",
      "epoch:19 step:15155 [D loss: 0.467743, acc.: 82.81%] [G loss: 3.394670]\n",
      "epoch:19 step:15156 [D loss: 0.381103, acc.: 82.03%] [G loss: 3.040846]\n",
      "epoch:19 step:15157 [D loss: 0.224667, acc.: 92.97%] [G loss: 3.263062]\n",
      "epoch:19 step:15158 [D loss: 0.274633, acc.: 87.50%] [G loss: 3.218363]\n",
      "epoch:19 step:15159 [D loss: 0.438880, acc.: 82.81%] [G loss: 3.028391]\n",
      "epoch:19 step:15160 [D loss: 0.294636, acc.: 87.50%] [G loss: 3.320299]\n",
      "epoch:19 step:15161 [D loss: 0.242184, acc.: 89.06%] [G loss: 3.835268]\n",
      "epoch:19 step:15162 [D loss: 0.293688, acc.: 89.06%] [G loss: 3.101117]\n",
      "epoch:19 step:15163 [D loss: 0.377537, acc.: 83.59%] [G loss: 2.833987]\n",
      "epoch:19 step:15164 [D loss: 0.293601, acc.: 85.16%] [G loss: 3.354475]\n",
      "epoch:19 step:15165 [D loss: 0.345495, acc.: 85.16%] [G loss: 2.548478]\n",
      "epoch:19 step:15166 [D loss: 0.334436, acc.: 83.59%] [G loss: 2.692263]\n",
      "epoch:19 step:15167 [D loss: 0.409497, acc.: 82.81%] [G loss: 2.850125]\n",
      "epoch:19 step:15168 [D loss: 0.243351, acc.: 86.72%] [G loss: 3.322573]\n",
      "epoch:19 step:15169 [D loss: 0.322618, acc.: 87.50%] [G loss: 2.862173]\n",
      "epoch:19 step:15170 [D loss: 0.367880, acc.: 83.59%] [G loss: 3.280381]\n",
      "epoch:19 step:15171 [D loss: 0.282616, acc.: 88.28%] [G loss: 2.911730]\n",
      "epoch:19 step:15172 [D loss: 0.232581, acc.: 91.41%] [G loss: 4.205671]\n",
      "epoch:19 step:15173 [D loss: 0.291289, acc.: 88.28%] [G loss: 3.236040]\n",
      "epoch:19 step:15174 [D loss: 0.275082, acc.: 91.41%] [G loss: 3.879334]\n",
      "epoch:19 step:15175 [D loss: 0.237636, acc.: 89.84%] [G loss: 2.946384]\n",
      "epoch:19 step:15176 [D loss: 0.313995, acc.: 86.72%] [G loss: 3.484217]\n",
      "epoch:19 step:15177 [D loss: 0.300154, acc.: 85.16%] [G loss: 3.543911]\n",
      "epoch:19 step:15178 [D loss: 0.277109, acc.: 91.41%] [G loss: 2.930285]\n",
      "epoch:19 step:15179 [D loss: 0.231356, acc.: 91.41%] [G loss: 3.014992]\n",
      "epoch:19 step:15180 [D loss: 0.212875, acc.: 92.19%] [G loss: 3.380523]\n",
      "epoch:19 step:15181 [D loss: 0.303586, acc.: 84.38%] [G loss: 3.599283]\n",
      "epoch:19 step:15182 [D loss: 0.322901, acc.: 85.94%] [G loss: 2.483592]\n",
      "epoch:19 step:15183 [D loss: 0.222202, acc.: 92.19%] [G loss: 5.005116]\n",
      "epoch:19 step:15184 [D loss: 0.205790, acc.: 91.41%] [G loss: 3.120954]\n",
      "epoch:19 step:15185 [D loss: 0.369419, acc.: 83.59%] [G loss: 3.177857]\n",
      "epoch:19 step:15186 [D loss: 0.217211, acc.: 89.84%] [G loss: 4.515299]\n",
      "epoch:19 step:15187 [D loss: 0.341840, acc.: 82.81%] [G loss: 4.186172]\n",
      "epoch:19 step:15188 [D loss: 0.238922, acc.: 88.28%] [G loss: 5.867744]\n",
      "epoch:19 step:15189 [D loss: 0.267078, acc.: 88.28%] [G loss: 2.724308]\n",
      "epoch:19 step:15190 [D loss: 0.361051, acc.: 82.03%] [G loss: 2.492600]\n",
      "epoch:19 step:15191 [D loss: 0.282094, acc.: 85.94%] [G loss: 2.851673]\n",
      "epoch:19 step:15192 [D loss: 0.310668, acc.: 84.38%] [G loss: 3.212666]\n",
      "epoch:19 step:15193 [D loss: 0.290039, acc.: 85.16%] [G loss: 2.738984]\n",
      "epoch:19 step:15194 [D loss: 0.363195, acc.: 82.81%] [G loss: 2.741735]\n",
      "epoch:19 step:15195 [D loss: 0.360538, acc.: 85.16%] [G loss: 3.239377]\n",
      "epoch:19 step:15196 [D loss: 0.498513, acc.: 78.12%] [G loss: 2.594506]\n",
      "epoch:19 step:15197 [D loss: 0.576936, acc.: 70.31%] [G loss: 2.824809]\n",
      "epoch:19 step:15198 [D loss: 0.321922, acc.: 86.72%] [G loss: 3.156109]\n",
      "epoch:19 step:15199 [D loss: 0.269090, acc.: 87.50%] [G loss: 5.429823]\n",
      "epoch:19 step:15200 [D loss: 0.395453, acc.: 78.12%] [G loss: 3.978049]\n",
      "##############\n",
      "[0.87874676 0.86084502 0.81488398 0.80906744 0.77954141 0.83670285\n",
      " 0.86736563 0.82773112 0.80286891 0.83097336]\n",
      "##########\n",
      "epoch:19 step:15201 [D loss: 0.311228, acc.: 88.28%] [G loss: 4.304750]\n",
      "epoch:19 step:15202 [D loss: 0.431861, acc.: 82.03%] [G loss: 3.573572]\n",
      "epoch:19 step:15203 [D loss: 0.293397, acc.: 89.06%] [G loss: 2.577100]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:19 step:15204 [D loss: 0.387605, acc.: 80.47%] [G loss: 3.572418]\n",
      "epoch:19 step:15205 [D loss: 0.326264, acc.: 86.72%] [G loss: 4.102866]\n",
      "epoch:19 step:15206 [D loss: 0.408014, acc.: 82.03%] [G loss: 2.093630]\n",
      "epoch:19 step:15207 [D loss: 0.405947, acc.: 86.72%] [G loss: 4.094566]\n",
      "epoch:19 step:15208 [D loss: 0.565551, acc.: 73.44%] [G loss: 4.521909]\n",
      "epoch:19 step:15209 [D loss: 0.249240, acc.: 87.50%] [G loss: 4.525420]\n",
      "epoch:19 step:15210 [D loss: 0.323352, acc.: 84.38%] [G loss: 3.946402]\n",
      "epoch:19 step:15211 [D loss: 0.342602, acc.: 86.72%] [G loss: 3.145625]\n",
      "epoch:19 step:15212 [D loss: 0.335050, acc.: 88.28%] [G loss: 5.588031]\n",
      "epoch:19 step:15213 [D loss: 0.537115, acc.: 76.56%] [G loss: 3.564864]\n",
      "epoch:19 step:15214 [D loss: 0.340087, acc.: 84.38%] [G loss: 5.192584]\n",
      "epoch:19 step:15215 [D loss: 0.250254, acc.: 89.06%] [G loss: 2.692781]\n",
      "epoch:19 step:15216 [D loss: 0.456820, acc.: 81.25%] [G loss: 2.965208]\n",
      "epoch:19 step:15217 [D loss: 0.429376, acc.: 78.12%] [G loss: 2.667192]\n",
      "epoch:19 step:15218 [D loss: 0.227332, acc.: 90.62%] [G loss: 3.462785]\n",
      "epoch:19 step:15219 [D loss: 0.251171, acc.: 89.84%] [G loss: 4.333540]\n",
      "epoch:19 step:15220 [D loss: 0.312201, acc.: 85.94%] [G loss: 2.319334]\n",
      "epoch:19 step:15221 [D loss: 0.281816, acc.: 85.94%] [G loss: 3.018323]\n",
      "epoch:19 step:15222 [D loss: 0.301686, acc.: 86.72%] [G loss: 3.222822]\n",
      "epoch:19 step:15223 [D loss: 0.260716, acc.: 89.84%] [G loss: 2.810717]\n",
      "epoch:19 step:15224 [D loss: 0.300618, acc.: 86.72%] [G loss: 3.187686]\n",
      "epoch:19 step:15225 [D loss: 0.335506, acc.: 87.50%] [G loss: 3.964750]\n",
      "epoch:19 step:15226 [D loss: 0.246987, acc.: 90.62%] [G loss: 2.121608]\n",
      "epoch:19 step:15227 [D loss: 0.252778, acc.: 88.28%] [G loss: 3.699860]\n",
      "epoch:19 step:15228 [D loss: 0.368771, acc.: 82.81%] [G loss: 3.560716]\n",
      "epoch:19 step:15229 [D loss: 0.428278, acc.: 81.25%] [G loss: 3.205924]\n",
      "epoch:19 step:15230 [D loss: 0.384833, acc.: 78.12%] [G loss: 2.555338]\n",
      "epoch:19 step:15231 [D loss: 0.264708, acc.: 90.62%] [G loss: 2.671614]\n",
      "epoch:19 step:15232 [D loss: 0.285736, acc.: 86.72%] [G loss: 3.368631]\n",
      "epoch:19 step:15233 [D loss: 0.283138, acc.: 87.50%] [G loss: 2.543450]\n",
      "epoch:19 step:15234 [D loss: 0.372513, acc.: 82.03%] [G loss: 4.937026]\n",
      "epoch:19 step:15235 [D loss: 0.297981, acc.: 89.84%] [G loss: 5.184022]\n",
      "epoch:19 step:15236 [D loss: 0.328578, acc.: 85.16%] [G loss: 2.586371]\n",
      "epoch:19 step:15237 [D loss: 0.239467, acc.: 87.50%] [G loss: 3.381875]\n",
      "epoch:19 step:15238 [D loss: 0.325463, acc.: 83.59%] [G loss: 3.288680]\n",
      "epoch:19 step:15239 [D loss: 0.272944, acc.: 89.06%] [G loss: 2.947052]\n",
      "epoch:19 step:15240 [D loss: 0.302354, acc.: 87.50%] [G loss: 3.574276]\n",
      "epoch:19 step:15241 [D loss: 0.302556, acc.: 87.50%] [G loss: 3.124240]\n",
      "epoch:19 step:15242 [D loss: 0.271285, acc.: 88.28%] [G loss: 4.070269]\n",
      "epoch:19 step:15243 [D loss: 0.294532, acc.: 89.06%] [G loss: 3.128886]\n",
      "epoch:19 step:15244 [D loss: 0.263894, acc.: 89.06%] [G loss: 3.115270]\n",
      "epoch:19 step:15245 [D loss: 0.369529, acc.: 82.03%] [G loss: 3.134603]\n",
      "epoch:19 step:15246 [D loss: 0.262670, acc.: 89.06%] [G loss: 3.554514]\n",
      "epoch:19 step:15247 [D loss: 0.213260, acc.: 90.62%] [G loss: 6.503906]\n",
      "epoch:19 step:15248 [D loss: 0.306546, acc.: 85.94%] [G loss: 3.143882]\n",
      "epoch:19 step:15249 [D loss: 0.312924, acc.: 84.38%] [G loss: 3.472235]\n",
      "epoch:19 step:15250 [D loss: 0.286128, acc.: 85.94%] [G loss: 3.364535]\n",
      "epoch:19 step:15251 [D loss: 0.313160, acc.: 85.94%] [G loss: 3.370973]\n",
      "epoch:19 step:15252 [D loss: 0.389392, acc.: 82.81%] [G loss: 2.681234]\n",
      "epoch:19 step:15253 [D loss: 0.266274, acc.: 88.28%] [G loss: 3.893697]\n",
      "epoch:19 step:15254 [D loss: 0.252357, acc.: 89.84%] [G loss: 3.596703]\n",
      "epoch:19 step:15255 [D loss: 0.377224, acc.: 83.59%] [G loss: 2.926997]\n",
      "epoch:19 step:15256 [D loss: 0.396657, acc.: 82.81%] [G loss: 3.284899]\n",
      "epoch:19 step:15257 [D loss: 0.337680, acc.: 85.94%] [G loss: 2.939578]\n",
      "epoch:19 step:15258 [D loss: 0.348511, acc.: 84.38%] [G loss: 2.785467]\n",
      "epoch:19 step:15259 [D loss: 0.359414, acc.: 84.38%] [G loss: 4.384712]\n",
      "epoch:19 step:15260 [D loss: 0.264085, acc.: 90.62%] [G loss: 3.408529]\n",
      "epoch:19 step:15261 [D loss: 0.300751, acc.: 85.94%] [G loss: 3.268717]\n",
      "epoch:19 step:15262 [D loss: 0.275486, acc.: 89.84%] [G loss: 3.459740]\n",
      "epoch:19 step:15263 [D loss: 0.398428, acc.: 82.03%] [G loss: 3.416229]\n",
      "epoch:19 step:15264 [D loss: 0.258312, acc.: 90.62%] [G loss: 5.243459]\n",
      "epoch:19 step:15265 [D loss: 0.226137, acc.: 92.19%] [G loss: 3.279863]\n",
      "epoch:19 step:15266 [D loss: 0.296346, acc.: 84.38%] [G loss: 3.641777]\n",
      "epoch:19 step:15267 [D loss: 0.443002, acc.: 81.25%] [G loss: 3.240201]\n",
      "epoch:19 step:15268 [D loss: 0.354965, acc.: 84.38%] [G loss: 3.227980]\n",
      "epoch:19 step:15269 [D loss: 0.335195, acc.: 82.03%] [G loss: 3.551750]\n",
      "epoch:19 step:15270 [D loss: 0.376208, acc.: 85.16%] [G loss: 2.532852]\n",
      "epoch:19 step:15271 [D loss: 0.465481, acc.: 75.00%] [G loss: 3.759982]\n",
      "epoch:19 step:15272 [D loss: 0.230748, acc.: 91.41%] [G loss: 3.976920]\n",
      "epoch:19 step:15273 [D loss: 0.284353, acc.: 87.50%] [G loss: 5.180423]\n",
      "epoch:19 step:15274 [D loss: 0.439099, acc.: 82.03%] [G loss: 4.199419]\n",
      "epoch:19 step:15275 [D loss: 0.623704, acc.: 73.44%] [G loss: 9.313211]\n",
      "epoch:19 step:15276 [D loss: 1.460644, acc.: 64.84%] [G loss: 2.262192]\n",
      "epoch:19 step:15277 [D loss: 0.452756, acc.: 81.25%] [G loss: 4.278535]\n",
      "epoch:19 step:15278 [D loss: 0.824180, acc.: 70.31%] [G loss: 6.494760]\n",
      "epoch:19 step:15279 [D loss: 0.926724, acc.: 64.06%] [G loss: 2.623373]\n",
      "epoch:19 step:15280 [D loss: 0.480256, acc.: 78.12%] [G loss: 4.176915]\n",
      "epoch:19 step:15281 [D loss: 0.543362, acc.: 77.34%] [G loss: 4.742386]\n",
      "epoch:19 step:15282 [D loss: 0.383227, acc.: 85.94%] [G loss: 4.093614]\n",
      "epoch:19 step:15283 [D loss: 0.419220, acc.: 77.34%] [G loss: 3.586033]\n",
      "epoch:19 step:15284 [D loss: 0.289140, acc.: 87.50%] [G loss: 3.159174]\n",
      "epoch:19 step:15285 [D loss: 0.397862, acc.: 83.59%] [G loss: 2.991172]\n",
      "epoch:19 step:15286 [D loss: 0.364433, acc.: 85.16%] [G loss: 2.866415]\n",
      "epoch:19 step:15287 [D loss: 0.415869, acc.: 82.81%] [G loss: 3.055759]\n",
      "epoch:19 step:15288 [D loss: 0.307417, acc.: 87.50%] [G loss: 3.504737]\n",
      "epoch:19 step:15289 [D loss: 0.261557, acc.: 86.72%] [G loss: 2.821984]\n",
      "epoch:19 step:15290 [D loss: 0.322989, acc.: 83.59%] [G loss: 3.223930]\n",
      "epoch:19 step:15291 [D loss: 0.348752, acc.: 83.59%] [G loss: 3.749982]\n",
      "epoch:19 step:15292 [D loss: 0.262001, acc.: 91.41%] [G loss: 4.700816]\n",
      "epoch:19 step:15293 [D loss: 0.297832, acc.: 87.50%] [G loss: 2.894182]\n",
      "epoch:19 step:15294 [D loss: 0.369007, acc.: 82.03%] [G loss: 2.907580]\n",
      "epoch:19 step:15295 [D loss: 0.293093, acc.: 87.50%] [G loss: 3.646583]\n",
      "epoch:19 step:15296 [D loss: 0.281778, acc.: 87.50%] [G loss: 2.846680]\n",
      "epoch:19 step:15297 [D loss: 0.221384, acc.: 92.19%] [G loss: 3.048358]\n",
      "epoch:19 step:15298 [D loss: 0.352452, acc.: 83.59%] [G loss: 2.965012]\n",
      "epoch:19 step:15299 [D loss: 0.315908, acc.: 88.28%] [G loss: 2.850946]\n",
      "epoch:19 step:15300 [D loss: 0.313803, acc.: 85.16%] [G loss: 3.194028]\n",
      "epoch:19 step:15301 [D loss: 0.250084, acc.: 88.28%] [G loss: 3.094440]\n",
      "epoch:19 step:15302 [D loss: 0.396828, acc.: 79.69%] [G loss: 3.562327]\n",
      "epoch:19 step:15303 [D loss: 0.367930, acc.: 85.94%] [G loss: 3.136759]\n",
      "epoch:19 step:15304 [D loss: 0.350121, acc.: 85.94%] [G loss: 3.289725]\n",
      "epoch:19 step:15305 [D loss: 0.339807, acc.: 82.03%] [G loss: 2.376846]\n",
      "epoch:19 step:15306 [D loss: 0.409452, acc.: 82.81%] [G loss: 3.930032]\n",
      "epoch:19 step:15307 [D loss: 0.272988, acc.: 89.84%] [G loss: 3.477343]\n",
      "epoch:19 step:15308 [D loss: 0.273097, acc.: 92.19%] [G loss: 4.230947]\n",
      "epoch:19 step:15309 [D loss: 0.297931, acc.: 89.06%] [G loss: 4.871223]\n",
      "epoch:19 step:15310 [D loss: 0.297460, acc.: 86.72%] [G loss: 5.113892]\n",
      "epoch:19 step:15311 [D loss: 0.342258, acc.: 84.38%] [G loss: 3.097014]\n",
      "epoch:19 step:15312 [D loss: 0.287484, acc.: 85.94%] [G loss: 4.108618]\n",
      "epoch:19 step:15313 [D loss: 0.354454, acc.: 84.38%] [G loss: 2.158448]\n",
      "epoch:19 step:15314 [D loss: 0.365587, acc.: 80.47%] [G loss: 3.490168]\n",
      "epoch:19 step:15315 [D loss: 0.382888, acc.: 82.03%] [G loss: 3.255793]\n",
      "epoch:19 step:15316 [D loss: 0.320957, acc.: 82.81%] [G loss: 3.669829]\n",
      "epoch:19 step:15317 [D loss: 0.210975, acc.: 90.62%] [G loss: 3.722766]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:19 step:15318 [D loss: 0.296245, acc.: 84.38%] [G loss: 4.332453]\n",
      "epoch:19 step:15319 [D loss: 0.274942, acc.: 88.28%] [G loss: 4.635019]\n",
      "epoch:19 step:15320 [D loss: 0.350384, acc.: 87.50%] [G loss: 2.543166]\n",
      "epoch:19 step:15321 [D loss: 0.334314, acc.: 86.72%] [G loss: 3.265584]\n",
      "epoch:19 step:15322 [D loss: 0.303199, acc.: 92.19%] [G loss: 3.825653]\n",
      "epoch:19 step:15323 [D loss: 0.253741, acc.: 88.28%] [G loss: 5.384665]\n",
      "epoch:19 step:15324 [D loss: 0.205741, acc.: 93.75%] [G loss: 2.985755]\n",
      "epoch:19 step:15325 [D loss: 0.338380, acc.: 82.81%] [G loss: 3.121037]\n",
      "epoch:19 step:15326 [D loss: 0.368873, acc.: 81.25%] [G loss: 3.314378]\n",
      "epoch:19 step:15327 [D loss: 0.395707, acc.: 81.25%] [G loss: 2.938466]\n",
      "epoch:19 step:15328 [D loss: 0.408009, acc.: 80.47%] [G loss: 2.832691]\n",
      "epoch:19 step:15329 [D loss: 0.331689, acc.: 86.72%] [G loss: 2.969540]\n",
      "epoch:19 step:15330 [D loss: 0.408934, acc.: 85.16%] [G loss: 2.676818]\n",
      "epoch:19 step:15331 [D loss: 0.302990, acc.: 89.06%] [G loss: 3.524903]\n",
      "epoch:19 step:15332 [D loss: 0.264859, acc.: 89.06%] [G loss: 5.394507]\n",
      "epoch:19 step:15333 [D loss: 0.326593, acc.: 86.72%] [G loss: 4.490917]\n",
      "epoch:19 step:15334 [D loss: 0.421004, acc.: 82.81%] [G loss: 4.041500]\n",
      "epoch:19 step:15335 [D loss: 0.231212, acc.: 90.62%] [G loss: 5.863902]\n",
      "epoch:19 step:15336 [D loss: 0.345996, acc.: 84.38%] [G loss: 3.623489]\n",
      "epoch:19 step:15337 [D loss: 0.362691, acc.: 85.16%] [G loss: 3.606003]\n",
      "epoch:19 step:15338 [D loss: 0.297114, acc.: 87.50%] [G loss: 4.089566]\n",
      "epoch:19 step:15339 [D loss: 0.274105, acc.: 89.06%] [G loss: 5.146715]\n",
      "epoch:19 step:15340 [D loss: 0.336913, acc.: 85.16%] [G loss: 4.828122]\n",
      "epoch:19 step:15341 [D loss: 0.347172, acc.: 79.69%] [G loss: 3.880339]\n",
      "epoch:19 step:15342 [D loss: 0.381226, acc.: 85.94%] [G loss: 3.745046]\n",
      "epoch:19 step:15343 [D loss: 0.454312, acc.: 79.69%] [G loss: 3.427465]\n",
      "epoch:19 step:15344 [D loss: 0.395102, acc.: 84.38%] [G loss: 3.909726]\n",
      "epoch:19 step:15345 [D loss: 0.332857, acc.: 85.94%] [G loss: 2.857332]\n",
      "epoch:19 step:15346 [D loss: 0.452519, acc.: 80.47%] [G loss: 2.804719]\n",
      "epoch:19 step:15347 [D loss: 0.299701, acc.: 86.72%] [G loss: 3.968900]\n",
      "epoch:19 step:15348 [D loss: 0.300016, acc.: 86.72%] [G loss: 3.396906]\n",
      "epoch:19 step:15349 [D loss: 0.523618, acc.: 72.66%] [G loss: 3.126618]\n",
      "epoch:19 step:15350 [D loss: 0.322325, acc.: 88.28%] [G loss: 2.987427]\n",
      "epoch:19 step:15351 [D loss: 0.264725, acc.: 90.62%] [G loss: 2.783461]\n",
      "epoch:19 step:15352 [D loss: 0.377362, acc.: 84.38%] [G loss: 2.543516]\n",
      "epoch:19 step:15353 [D loss: 0.305928, acc.: 87.50%] [G loss: 3.447941]\n",
      "epoch:19 step:15354 [D loss: 0.353743, acc.: 85.16%] [G loss: 2.729505]\n",
      "epoch:19 step:15355 [D loss: 0.302801, acc.: 87.50%] [G loss: 2.832163]\n",
      "epoch:19 step:15356 [D loss: 0.297552, acc.: 88.28%] [G loss: 2.510350]\n",
      "epoch:19 step:15357 [D loss: 0.394214, acc.: 85.94%] [G loss: 3.460474]\n",
      "epoch:19 step:15358 [D loss: 0.266164, acc.: 86.72%] [G loss: 3.319474]\n",
      "epoch:19 step:15359 [D loss: 0.451970, acc.: 79.69%] [G loss: 4.098428]\n",
      "epoch:19 step:15360 [D loss: 0.462900, acc.: 80.47%] [G loss: 2.323625]\n",
      "epoch:19 step:15361 [D loss: 0.350452, acc.: 85.94%] [G loss: 3.137988]\n",
      "epoch:19 step:15362 [D loss: 0.324140, acc.: 85.16%] [G loss: 2.752948]\n",
      "epoch:19 step:15363 [D loss: 0.359456, acc.: 86.72%] [G loss: 2.707313]\n",
      "epoch:19 step:15364 [D loss: 0.360757, acc.: 84.38%] [G loss: 2.516294]\n",
      "epoch:19 step:15365 [D loss: 0.382603, acc.: 84.38%] [G loss: 3.534184]\n",
      "epoch:19 step:15366 [D loss: 0.444768, acc.: 76.56%] [G loss: 2.363779]\n",
      "epoch:19 step:15367 [D loss: 0.383524, acc.: 81.25%] [G loss: 2.428470]\n",
      "epoch:19 step:15368 [D loss: 0.360349, acc.: 82.81%] [G loss: 3.189930]\n",
      "epoch:19 step:15369 [D loss: 0.306310, acc.: 88.28%] [G loss: 2.970619]\n",
      "epoch:19 step:15370 [D loss: 0.317511, acc.: 85.94%] [G loss: 2.092483]\n",
      "epoch:19 step:15371 [D loss: 0.339422, acc.: 85.16%] [G loss: 2.398026]\n",
      "epoch:19 step:15372 [D loss: 0.290926, acc.: 88.28%] [G loss: 2.719660]\n",
      "epoch:19 step:15373 [D loss: 0.348685, acc.: 85.16%] [G loss: 2.890954]\n",
      "epoch:19 step:15374 [D loss: 0.247657, acc.: 92.19%] [G loss: 3.747935]\n",
      "epoch:19 step:15375 [D loss: 0.314579, acc.: 83.59%] [G loss: 3.132633]\n",
      "epoch:19 step:15376 [D loss: 0.358371, acc.: 82.03%] [G loss: 2.715065]\n",
      "epoch:19 step:15377 [D loss: 0.333667, acc.: 85.94%] [G loss: 2.545855]\n",
      "epoch:19 step:15378 [D loss: 0.415313, acc.: 80.47%] [G loss: 3.565290]\n",
      "epoch:19 step:15379 [D loss: 0.299507, acc.: 85.94%] [G loss: 4.059464]\n",
      "epoch:19 step:15380 [D loss: 0.301215, acc.: 87.50%] [G loss: 4.213219]\n",
      "epoch:19 step:15381 [D loss: 0.309547, acc.: 87.50%] [G loss: 4.386593]\n",
      "epoch:19 step:15382 [D loss: 0.312699, acc.: 85.94%] [G loss: 3.949661]\n",
      "epoch:19 step:15383 [D loss: 0.371513, acc.: 83.59%] [G loss: 3.348169]\n",
      "epoch:19 step:15384 [D loss: 0.315061, acc.: 83.59%] [G loss: 2.674545]\n",
      "epoch:19 step:15385 [D loss: 0.290421, acc.: 85.16%] [G loss: 5.062943]\n",
      "epoch:19 step:15386 [D loss: 0.442182, acc.: 78.12%] [G loss: 4.289300]\n",
      "epoch:19 step:15387 [D loss: 0.245603, acc.: 89.06%] [G loss: 4.766580]\n",
      "epoch:19 step:15388 [D loss: 0.427132, acc.: 80.47%] [G loss: 4.848223]\n",
      "epoch:19 step:15389 [D loss: 0.332987, acc.: 86.72%] [G loss: 3.549391]\n",
      "epoch:19 step:15390 [D loss: 0.330860, acc.: 85.16%] [G loss: 3.632438]\n",
      "epoch:19 step:15391 [D loss: 0.444270, acc.: 80.47%] [G loss: 3.610002]\n",
      "epoch:19 step:15392 [D loss: 0.260536, acc.: 87.50%] [G loss: 3.607259]\n",
      "epoch:19 step:15393 [D loss: 0.235433, acc.: 90.62%] [G loss: 3.625470]\n",
      "epoch:19 step:15394 [D loss: 0.342476, acc.: 85.94%] [G loss: 2.702484]\n",
      "epoch:19 step:15395 [D loss: 0.374717, acc.: 86.72%] [G loss: 3.316931]\n",
      "epoch:19 step:15396 [D loss: 0.354745, acc.: 83.59%] [G loss: 2.341365]\n",
      "epoch:19 step:15397 [D loss: 0.354845, acc.: 85.94%] [G loss: 3.034204]\n",
      "epoch:19 step:15398 [D loss: 0.341083, acc.: 85.16%] [G loss: 3.267165]\n",
      "epoch:19 step:15399 [D loss: 0.271703, acc.: 89.06%] [G loss: 3.886830]\n",
      "epoch:19 step:15400 [D loss: 0.310216, acc.: 85.94%] [G loss: 3.172749]\n",
      "##############\n",
      "[0.86076179 0.85962293 0.79202394 0.80223486 0.77241643 0.83197444\n",
      " 0.88996015 0.84566731 0.82197542 0.80034859]\n",
      "##########\n",
      "epoch:19 step:15401 [D loss: 0.285396, acc.: 89.06%] [G loss: 3.256661]\n",
      "epoch:19 step:15402 [D loss: 0.313432, acc.: 90.62%] [G loss: 3.748125]\n",
      "epoch:19 step:15403 [D loss: 0.253135, acc.: 91.41%] [G loss: 3.305187]\n",
      "epoch:19 step:15404 [D loss: 0.331341, acc.: 85.16%] [G loss: 2.823659]\n",
      "epoch:19 step:15405 [D loss: 0.341229, acc.: 81.25%] [G loss: 3.239897]\n",
      "epoch:19 step:15406 [D loss: 0.270511, acc.: 87.50%] [G loss: 4.003988]\n",
      "epoch:19 step:15407 [D loss: 0.229002, acc.: 89.84%] [G loss: 2.823843]\n",
      "epoch:19 step:15408 [D loss: 0.307106, acc.: 88.28%] [G loss: 3.317097]\n",
      "epoch:19 step:15409 [D loss: 0.301414, acc.: 87.50%] [G loss: 2.716204]\n",
      "epoch:19 step:15410 [D loss: 0.371684, acc.: 83.59%] [G loss: 3.115737]\n",
      "epoch:19 step:15411 [D loss: 0.225689, acc.: 90.62%] [G loss: 3.149906]\n",
      "epoch:19 step:15412 [D loss: 0.259672, acc.: 89.06%] [G loss: 3.971480]\n",
      "epoch:19 step:15413 [D loss: 0.307190, acc.: 85.94%] [G loss: 5.006785]\n",
      "epoch:19 step:15414 [D loss: 0.361619, acc.: 83.59%] [G loss: 6.121874]\n",
      "epoch:19 step:15415 [D loss: 0.223859, acc.: 92.19%] [G loss: 4.090330]\n",
      "epoch:19 step:15416 [D loss: 0.296347, acc.: 85.94%] [G loss: 2.944121]\n",
      "epoch:19 step:15417 [D loss: 0.257146, acc.: 89.84%] [G loss: 4.023606]\n",
      "epoch:19 step:15418 [D loss: 0.306916, acc.: 82.81%] [G loss: 4.087311]\n",
      "epoch:19 step:15419 [D loss: 0.246692, acc.: 89.06%] [G loss: 3.549417]\n",
      "epoch:19 step:15420 [D loss: 0.250573, acc.: 87.50%] [G loss: 4.377948]\n",
      "epoch:19 step:15421 [D loss: 0.276721, acc.: 89.06%] [G loss: 3.852156]\n",
      "epoch:19 step:15422 [D loss: 0.296534, acc.: 85.16%] [G loss: 2.449239]\n",
      "epoch:19 step:15423 [D loss: 0.324162, acc.: 83.59%] [G loss: 2.859051]\n",
      "epoch:19 step:15424 [D loss: 0.259352, acc.: 89.84%] [G loss: 3.617830]\n",
      "epoch:19 step:15425 [D loss: 0.374564, acc.: 80.47%] [G loss: 3.273149]\n",
      "epoch:19 step:15426 [D loss: 0.280568, acc.: 89.84%] [G loss: 3.303728]\n",
      "epoch:19 step:15427 [D loss: 0.290348, acc.: 90.62%] [G loss: 4.386289]\n",
      "epoch:19 step:15428 [D loss: 0.400039, acc.: 81.25%] [G loss: 3.765551]\n",
      "epoch:19 step:15429 [D loss: 0.567023, acc.: 74.22%] [G loss: 4.498946]\n",
      "epoch:19 step:15430 [D loss: 0.531130, acc.: 71.09%] [G loss: 6.022659]\n",
      "epoch:19 step:15431 [D loss: 0.855739, acc.: 67.97%] [G loss: 3.933706]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:19 step:15432 [D loss: 0.594311, acc.: 75.00%] [G loss: 2.349631]\n",
      "epoch:19 step:15433 [D loss: 0.283581, acc.: 86.72%] [G loss: 3.311291]\n",
      "epoch:19 step:15434 [D loss: 0.346230, acc.: 85.94%] [G loss: 3.178983]\n",
      "epoch:19 step:15435 [D loss: 0.372467, acc.: 82.03%] [G loss: 2.824612]\n",
      "epoch:19 step:15436 [D loss: 0.399876, acc.: 79.69%] [G loss: 2.800760]\n",
      "epoch:19 step:15437 [D loss: 0.323251, acc.: 85.16%] [G loss: 3.263919]\n",
      "epoch:19 step:15438 [D loss: 0.327839, acc.: 86.72%] [G loss: 2.743607]\n",
      "epoch:19 step:15439 [D loss: 0.264423, acc.: 90.62%] [G loss: 2.946459]\n",
      "epoch:19 step:15440 [D loss: 0.361397, acc.: 85.16%] [G loss: 2.721848]\n",
      "epoch:19 step:15441 [D loss: 0.246448, acc.: 89.06%] [G loss: 2.702926]\n",
      "epoch:19 step:15442 [D loss: 0.306431, acc.: 85.94%] [G loss: 2.623425]\n",
      "epoch:19 step:15443 [D loss: 0.469618, acc.: 77.34%] [G loss: 2.609571]\n",
      "epoch:19 step:15444 [D loss: 0.372607, acc.: 82.03%] [G loss: 2.927001]\n",
      "epoch:19 step:15445 [D loss: 0.341067, acc.: 86.72%] [G loss: 3.013926]\n",
      "epoch:19 step:15446 [D loss: 0.302632, acc.: 83.59%] [G loss: 2.504626]\n",
      "epoch:19 step:15447 [D loss: 0.349321, acc.: 85.94%] [G loss: 2.609187]\n",
      "epoch:19 step:15448 [D loss: 0.370725, acc.: 83.59%] [G loss: 2.792274]\n",
      "epoch:19 step:15449 [D loss: 0.316950, acc.: 85.94%] [G loss: 5.408787]\n",
      "epoch:19 step:15450 [D loss: 0.361228, acc.: 81.25%] [G loss: 3.510303]\n",
      "epoch:19 step:15451 [D loss: 0.314569, acc.: 84.38%] [G loss: 3.729368]\n",
      "epoch:19 step:15452 [D loss: 0.255669, acc.: 91.41%] [G loss: 3.911111]\n",
      "epoch:19 step:15453 [D loss: 0.187979, acc.: 91.41%] [G loss: 4.706979]\n",
      "epoch:19 step:15454 [D loss: 0.325629, acc.: 89.06%] [G loss: 6.095511]\n",
      "epoch:19 step:15455 [D loss: 0.281667, acc.: 90.62%] [G loss: 5.092898]\n",
      "epoch:19 step:15456 [D loss: 0.215337, acc.: 89.84%] [G loss: 4.077963]\n",
      "epoch:19 step:15457 [D loss: 0.322786, acc.: 84.38%] [G loss: 4.425961]\n",
      "epoch:19 step:15458 [D loss: 0.190389, acc.: 90.62%] [G loss: 5.570095]\n",
      "epoch:19 step:15459 [D loss: 0.290970, acc.: 85.94%] [G loss: 3.304626]\n",
      "epoch:19 step:15460 [D loss: 0.242234, acc.: 90.62%] [G loss: 3.270424]\n",
      "epoch:19 step:15461 [D loss: 0.287441, acc.: 90.62%] [G loss: 3.073496]\n",
      "epoch:19 step:15462 [D loss: 0.314971, acc.: 86.72%] [G loss: 3.833011]\n",
      "epoch:19 step:15463 [D loss: 0.330743, acc.: 81.25%] [G loss: 4.057736]\n",
      "epoch:19 step:15464 [D loss: 0.330276, acc.: 82.81%] [G loss: 3.714592]\n",
      "epoch:19 step:15465 [D loss: 0.303409, acc.: 85.94%] [G loss: 3.825679]\n",
      "epoch:19 step:15466 [D loss: 0.209522, acc.: 93.75%] [G loss: 3.257071]\n",
      "epoch:19 step:15467 [D loss: 0.428280, acc.: 78.12%] [G loss: 3.539118]\n",
      "epoch:19 step:15468 [D loss: 0.448674, acc.: 81.25%] [G loss: 3.378609]\n",
      "epoch:19 step:15469 [D loss: 0.353205, acc.: 82.81%] [G loss: 3.605071]\n",
      "epoch:19 step:15470 [D loss: 0.328010, acc.: 82.81%] [G loss: 6.444701]\n",
      "epoch:19 step:15471 [D loss: 0.297495, acc.: 87.50%] [G loss: 2.296663]\n",
      "epoch:19 step:15472 [D loss: 0.413616, acc.: 77.34%] [G loss: 2.763342]\n",
      "epoch:19 step:15473 [D loss: 0.355041, acc.: 86.72%] [G loss: 2.664566]\n",
      "epoch:19 step:15474 [D loss: 0.410745, acc.: 85.16%] [G loss: 3.273828]\n",
      "epoch:19 step:15475 [D loss: 0.447969, acc.: 84.38%] [G loss: 2.693754]\n",
      "epoch:19 step:15476 [D loss: 0.313198, acc.: 85.16%] [G loss: 2.828008]\n",
      "epoch:19 step:15477 [D loss: 0.442560, acc.: 82.81%] [G loss: 3.285244]\n",
      "epoch:19 step:15478 [D loss: 0.440391, acc.: 80.47%] [G loss: 2.696996]\n",
      "epoch:19 step:15479 [D loss: 0.282856, acc.: 93.75%] [G loss: 2.722837]\n",
      "epoch:19 step:15480 [D loss: 0.286272, acc.: 85.94%] [G loss: 2.409843]\n",
      "epoch:19 step:15481 [D loss: 0.422802, acc.: 81.25%] [G loss: 4.168050]\n",
      "epoch:19 step:15482 [D loss: 0.338267, acc.: 83.59%] [G loss: 3.599298]\n",
      "epoch:19 step:15483 [D loss: 0.312242, acc.: 85.16%] [G loss: 3.153663]\n",
      "epoch:19 step:15484 [D loss: 0.356814, acc.: 85.16%] [G loss: 2.320273]\n",
      "epoch:19 step:15485 [D loss: 0.341139, acc.: 84.38%] [G loss: 2.776594]\n",
      "epoch:19 step:15486 [D loss: 0.229679, acc.: 90.62%] [G loss: 4.754117]\n",
      "epoch:19 step:15487 [D loss: 0.264902, acc.: 88.28%] [G loss: 3.218851]\n",
      "epoch:19 step:15488 [D loss: 0.293502, acc.: 87.50%] [G loss: 3.375439]\n",
      "epoch:19 step:15489 [D loss: 0.297070, acc.: 89.06%] [G loss: 4.125756]\n",
      "epoch:19 step:15490 [D loss: 0.364125, acc.: 85.16%] [G loss: 4.673305]\n",
      "epoch:19 step:15491 [D loss: 0.253426, acc.: 89.84%] [G loss: 4.302486]\n",
      "epoch:19 step:15492 [D loss: 0.348634, acc.: 84.38%] [G loss: 4.668241]\n",
      "epoch:19 step:15493 [D loss: 0.414975, acc.: 83.59%] [G loss: 5.018081]\n",
      "epoch:19 step:15494 [D loss: 0.219454, acc.: 89.84%] [G loss: 3.530744]\n",
      "epoch:19 step:15495 [D loss: 0.353362, acc.: 82.03%] [G loss: 6.119060]\n",
      "epoch:19 step:15496 [D loss: 0.328909, acc.: 83.59%] [G loss: 3.748278]\n",
      "epoch:19 step:15497 [D loss: 0.355020, acc.: 83.59%] [G loss: 4.441021]\n",
      "epoch:19 step:15498 [D loss: 0.327938, acc.: 83.59%] [G loss: 3.551596]\n",
      "epoch:19 step:15499 [D loss: 0.396572, acc.: 82.81%] [G loss: 3.095295]\n",
      "epoch:19 step:15500 [D loss: 0.331605, acc.: 82.81%] [G loss: 4.637137]\n",
      "epoch:19 step:15501 [D loss: 0.262150, acc.: 84.38%] [G loss: 4.235090]\n",
      "epoch:19 step:15502 [D loss: 0.251798, acc.: 89.06%] [G loss: 3.492878]\n",
      "epoch:19 step:15503 [D loss: 0.293838, acc.: 88.28%] [G loss: 2.567150]\n",
      "epoch:19 step:15504 [D loss: 0.251467, acc.: 89.06%] [G loss: 3.114773]\n",
      "epoch:19 step:15505 [D loss: 0.326613, acc.: 84.38%] [G loss: 2.715485]\n",
      "epoch:19 step:15506 [D loss: 0.236498, acc.: 92.97%] [G loss: 3.201560]\n",
      "epoch:19 step:15507 [D loss: 0.256166, acc.: 91.41%] [G loss: 2.349213]\n",
      "epoch:19 step:15508 [D loss: 0.412391, acc.: 80.47%] [G loss: 3.367243]\n",
      "epoch:19 step:15509 [D loss: 0.315473, acc.: 84.38%] [G loss: 2.773610]\n",
      "epoch:19 step:15510 [D loss: 0.266397, acc.: 89.84%] [G loss: 3.590338]\n",
      "epoch:19 step:15511 [D loss: 0.361004, acc.: 79.69%] [G loss: 3.472747]\n",
      "epoch:19 step:15512 [D loss: 0.357968, acc.: 86.72%] [G loss: 2.225238]\n",
      "epoch:19 step:15513 [D loss: 0.343095, acc.: 86.72%] [G loss: 2.334756]\n",
      "epoch:19 step:15514 [D loss: 0.310267, acc.: 85.16%] [G loss: 2.792336]\n",
      "epoch:19 step:15515 [D loss: 0.278488, acc.: 86.72%] [G loss: 2.623689]\n",
      "epoch:19 step:15516 [D loss: 0.320693, acc.: 91.41%] [G loss: 2.647324]\n",
      "epoch:19 step:15517 [D loss: 0.278000, acc.: 89.84%] [G loss: 2.630021]\n",
      "epoch:19 step:15518 [D loss: 0.239799, acc.: 89.84%] [G loss: 3.686121]\n",
      "epoch:19 step:15519 [D loss: 0.276237, acc.: 89.06%] [G loss: 2.694965]\n",
      "epoch:19 step:15520 [D loss: 0.325917, acc.: 82.81%] [G loss: 3.694687]\n",
      "epoch:19 step:15521 [D loss: 0.308679, acc.: 89.06%] [G loss: 2.661515]\n",
      "epoch:19 step:15522 [D loss: 0.414265, acc.: 77.34%] [G loss: 3.864735]\n",
      "epoch:19 step:15523 [D loss: 0.466038, acc.: 78.12%] [G loss: 3.878814]\n",
      "epoch:19 step:15524 [D loss: 0.541756, acc.: 72.66%] [G loss: 3.460463]\n",
      "epoch:19 step:15525 [D loss: 0.345068, acc.: 85.94%] [G loss: 4.343835]\n",
      "epoch:19 step:15526 [D loss: 0.527629, acc.: 78.91%] [G loss: 3.569643]\n",
      "epoch:19 step:15527 [D loss: 0.270831, acc.: 85.94%] [G loss: 3.648152]\n",
      "epoch:19 step:15528 [D loss: 0.298050, acc.: 89.06%] [G loss: 3.182021]\n",
      "epoch:19 step:15529 [D loss: 0.284380, acc.: 87.50%] [G loss: 3.793784]\n",
      "epoch:19 step:15530 [D loss: 0.355066, acc.: 88.28%] [G loss: 3.111617]\n",
      "epoch:19 step:15531 [D loss: 0.482092, acc.: 80.47%] [G loss: 2.713993]\n",
      "epoch:19 step:15532 [D loss: 0.340304, acc.: 88.28%] [G loss: 2.345037]\n",
      "epoch:19 step:15533 [D loss: 0.334829, acc.: 85.94%] [G loss: 4.317181]\n",
      "epoch:19 step:15534 [D loss: 0.204239, acc.: 90.62%] [G loss: 4.467423]\n",
      "epoch:19 step:15535 [D loss: 0.239152, acc.: 89.84%] [G loss: 3.273006]\n",
      "epoch:19 step:15536 [D loss: 0.232599, acc.: 92.19%] [G loss: 2.995859]\n",
      "epoch:19 step:15537 [D loss: 0.209015, acc.: 92.97%] [G loss: 3.305387]\n",
      "epoch:19 step:15538 [D loss: 0.260869, acc.: 90.62%] [G loss: 3.291113]\n",
      "epoch:19 step:15539 [D loss: 0.305571, acc.: 88.28%] [G loss: 3.286299]\n",
      "epoch:19 step:15540 [D loss: 0.274348, acc.: 89.06%] [G loss: 4.250003]\n",
      "epoch:19 step:15541 [D loss: 0.229801, acc.: 92.19%] [G loss: 4.444024]\n",
      "epoch:19 step:15542 [D loss: 0.454771, acc.: 78.91%] [G loss: 2.659160]\n",
      "epoch:19 step:15543 [D loss: 0.293375, acc.: 89.84%] [G loss: 2.814325]\n",
      "epoch:19 step:15544 [D loss: 0.271812, acc.: 89.06%] [G loss: 3.823375]\n",
      "epoch:19 step:15545 [D loss: 0.301827, acc.: 86.72%] [G loss: 3.957004]\n",
      "epoch:19 step:15546 [D loss: 0.361531, acc.: 84.38%] [G loss: 3.562490]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:19 step:15547 [D loss: 0.364541, acc.: 84.38%] [G loss: 3.001826]\n",
      "epoch:19 step:15548 [D loss: 0.417432, acc.: 85.16%] [G loss: 2.960489]\n",
      "epoch:19 step:15549 [D loss: 0.277152, acc.: 90.62%] [G loss: 3.878437]\n",
      "epoch:19 step:15550 [D loss: 0.265778, acc.: 88.28%] [G loss: 2.854331]\n",
      "epoch:19 step:15551 [D loss: 0.331544, acc.: 84.38%] [G loss: 3.218653]\n",
      "epoch:19 step:15552 [D loss: 0.263901, acc.: 88.28%] [G loss: 4.459055]\n",
      "epoch:19 step:15553 [D loss: 0.204991, acc.: 92.97%] [G loss: 4.001959]\n",
      "epoch:19 step:15554 [D loss: 0.381703, acc.: 84.38%] [G loss: 3.331079]\n",
      "epoch:19 step:15555 [D loss: 0.260267, acc.: 89.06%] [G loss: 4.134149]\n",
      "epoch:19 step:15556 [D loss: 0.235237, acc.: 89.84%] [G loss: 4.229061]\n",
      "epoch:19 step:15557 [D loss: 0.345424, acc.: 85.16%] [G loss: 2.371352]\n",
      "epoch:19 step:15558 [D loss: 0.317748, acc.: 82.81%] [G loss: 3.063660]\n",
      "epoch:19 step:15559 [D loss: 0.283450, acc.: 87.50%] [G loss: 3.265334]\n",
      "epoch:19 step:15560 [D loss: 0.314534, acc.: 86.72%] [G loss: 3.200344]\n",
      "epoch:19 step:15561 [D loss: 0.295238, acc.: 90.62%] [G loss: 2.585414]\n",
      "epoch:19 step:15562 [D loss: 0.376280, acc.: 87.50%] [G loss: 2.618252]\n",
      "epoch:19 step:15563 [D loss: 0.318065, acc.: 87.50%] [G loss: 4.269220]\n",
      "epoch:19 step:15564 [D loss: 0.321246, acc.: 85.94%] [G loss: 2.703053]\n",
      "epoch:19 step:15565 [D loss: 0.251780, acc.: 89.84%] [G loss: 3.034502]\n",
      "epoch:19 step:15566 [D loss: 0.314810, acc.: 84.38%] [G loss: 2.449358]\n",
      "epoch:19 step:15567 [D loss: 0.436325, acc.: 78.91%] [G loss: 2.492933]\n",
      "epoch:19 step:15568 [D loss: 0.311244, acc.: 85.94%] [G loss: 3.228302]\n",
      "epoch:19 step:15569 [D loss: 0.283336, acc.: 89.06%] [G loss: 2.531059]\n",
      "epoch:19 step:15570 [D loss: 0.454094, acc.: 84.38%] [G loss: 3.797395]\n",
      "epoch:19 step:15571 [D loss: 0.225579, acc.: 89.84%] [G loss: 5.231953]\n",
      "epoch:19 step:15572 [D loss: 0.262789, acc.: 91.41%] [G loss: 4.335165]\n",
      "epoch:19 step:15573 [D loss: 0.420600, acc.: 80.47%] [G loss: 5.337423]\n",
      "epoch:19 step:15574 [D loss: 0.364849, acc.: 85.16%] [G loss: 3.715596]\n",
      "epoch:19 step:15575 [D loss: 0.452535, acc.: 85.94%] [G loss: 3.348659]\n",
      "epoch:19 step:15576 [D loss: 0.411554, acc.: 78.12%] [G loss: 3.628994]\n",
      "epoch:19 step:15577 [D loss: 0.357526, acc.: 80.47%] [G loss: 3.643643]\n",
      "epoch:19 step:15578 [D loss: 0.268423, acc.: 85.16%] [G loss: 3.721930]\n",
      "epoch:19 step:15579 [D loss: 0.264161, acc.: 86.72%] [G loss: 4.558423]\n",
      "epoch:19 step:15580 [D loss: 0.261640, acc.: 85.16%] [G loss: 5.252314]\n",
      "epoch:19 step:15581 [D loss: 0.307814, acc.: 84.38%] [G loss: 4.472342]\n",
      "epoch:19 step:15582 [D loss: 0.212943, acc.: 91.41%] [G loss: 3.244717]\n",
      "epoch:19 step:15583 [D loss: 0.208683, acc.: 92.19%] [G loss: 3.100872]\n",
      "epoch:19 step:15584 [D loss: 0.288538, acc.: 88.28%] [G loss: 3.902003]\n",
      "epoch:19 step:15585 [D loss: 0.279575, acc.: 87.50%] [G loss: 4.030354]\n",
      "epoch:19 step:15586 [D loss: 0.303170, acc.: 84.38%] [G loss: 3.000595]\n",
      "epoch:19 step:15587 [D loss: 0.243892, acc.: 88.28%] [G loss: 3.913644]\n",
      "epoch:19 step:15588 [D loss: 0.287086, acc.: 86.72%] [G loss: 2.719980]\n",
      "epoch:19 step:15589 [D loss: 0.359985, acc.: 87.50%] [G loss: 3.447958]\n",
      "epoch:19 step:15590 [D loss: 0.316021, acc.: 89.84%] [G loss: 3.892809]\n",
      "epoch:19 step:15591 [D loss: 0.329112, acc.: 86.72%] [G loss: 3.251951]\n",
      "epoch:19 step:15592 [D loss: 0.427711, acc.: 78.91%] [G loss: 4.094811]\n",
      "epoch:19 step:15593 [D loss: 0.368157, acc.: 82.81%] [G loss: 3.416993]\n",
      "epoch:19 step:15594 [D loss: 0.532064, acc.: 77.34%] [G loss: 2.456892]\n",
      "epoch:19 step:15595 [D loss: 0.300316, acc.: 89.84%] [G loss: 3.804695]\n",
      "epoch:19 step:15596 [D loss: 0.249423, acc.: 89.06%] [G loss: 3.763567]\n",
      "epoch:19 step:15597 [D loss: 0.286250, acc.: 89.84%] [G loss: 5.286424]\n",
      "epoch:19 step:15598 [D loss: 0.358160, acc.: 83.59%] [G loss: 7.055563]\n",
      "epoch:19 step:15599 [D loss: 0.239294, acc.: 90.62%] [G loss: 4.883446]\n",
      "epoch:19 step:15600 [D loss: 0.242341, acc.: 90.62%] [G loss: 3.918964]\n",
      "##############\n",
      "[0.84592133 0.85890486 0.80505177 0.79987242 0.77166914 0.81699122\n",
      " 0.88349469 0.84580869 0.80070203 0.83220704]\n",
      "##########\n",
      "epoch:19 step:15601 [D loss: 0.301405, acc.: 86.72%] [G loss: 4.616464]\n",
      "epoch:19 step:15602 [D loss: 0.206143, acc.: 93.75%] [G loss: 2.974187]\n",
      "epoch:19 step:15603 [D loss: 0.352460, acc.: 82.03%] [G loss: 4.059538]\n",
      "epoch:19 step:15604 [D loss: 0.322427, acc.: 82.81%] [G loss: 5.972792]\n",
      "epoch:19 step:15605 [D loss: 0.255413, acc.: 88.28%] [G loss: 4.688700]\n",
      "epoch:19 step:15606 [D loss: 0.266611, acc.: 87.50%] [G loss: 4.198836]\n",
      "epoch:19 step:15607 [D loss: 0.277914, acc.: 86.72%] [G loss: 4.497190]\n",
      "epoch:19 step:15608 [D loss: 0.239800, acc.: 92.19%] [G loss: 2.964942]\n",
      "epoch:19 step:15609 [D loss: 0.279328, acc.: 88.28%] [G loss: 3.064383]\n",
      "epoch:19 step:15610 [D loss: 0.327532, acc.: 84.38%] [G loss: 3.062006]\n",
      "epoch:19 step:15611 [D loss: 0.287067, acc.: 87.50%] [G loss: 2.499279]\n",
      "epoch:19 step:15612 [D loss: 0.363343, acc.: 85.16%] [G loss: 2.970828]\n",
      "epoch:19 step:15613 [D loss: 0.291704, acc.: 82.81%] [G loss: 5.892273]\n",
      "epoch:19 step:15614 [D loss: 0.509558, acc.: 81.25%] [G loss: 3.028132]\n",
      "epoch:19 step:15615 [D loss: 0.404511, acc.: 79.69%] [G loss: 3.660366]\n",
      "epoch:19 step:15616 [D loss: 0.447653, acc.: 78.12%] [G loss: 3.400246]\n",
      "epoch:19 step:15617 [D loss: 0.225934, acc.: 91.41%] [G loss: 3.674973]\n",
      "epoch:19 step:15618 [D loss: 0.315411, acc.: 89.84%] [G loss: 2.822598]\n",
      "epoch:19 step:15619 [D loss: 0.451239, acc.: 82.03%] [G loss: 2.971128]\n",
      "epoch:19 step:15620 [D loss: 0.337193, acc.: 85.16%] [G loss: 3.619416]\n",
      "epoch:20 step:15621 [D loss: 0.286599, acc.: 86.72%] [G loss: 3.525899]\n",
      "epoch:20 step:15622 [D loss: 0.343743, acc.: 84.38%] [G loss: 2.807707]\n",
      "epoch:20 step:15623 [D loss: 0.435614, acc.: 77.34%] [G loss: 2.515925]\n",
      "epoch:20 step:15624 [D loss: 0.312848, acc.: 82.03%] [G loss: 3.019315]\n",
      "epoch:20 step:15625 [D loss: 0.237992, acc.: 91.41%] [G loss: 3.494374]\n",
      "epoch:20 step:15626 [D loss: 0.199765, acc.: 90.62%] [G loss: 4.694022]\n",
      "epoch:20 step:15627 [D loss: 0.212725, acc.: 91.41%] [G loss: 4.097978]\n",
      "epoch:20 step:15628 [D loss: 0.281390, acc.: 87.50%] [G loss: 3.469565]\n",
      "epoch:20 step:15629 [D loss: 0.281176, acc.: 89.84%] [G loss: 3.417954]\n",
      "epoch:20 step:15630 [D loss: 0.343563, acc.: 83.59%] [G loss: 3.970427]\n",
      "epoch:20 step:15631 [D loss: 0.339039, acc.: 83.59%] [G loss: 5.713275]\n",
      "epoch:20 step:15632 [D loss: 0.297931, acc.: 85.94%] [G loss: 3.528310]\n",
      "epoch:20 step:15633 [D loss: 0.334283, acc.: 89.84%] [G loss: 4.994496]\n",
      "epoch:20 step:15634 [D loss: 0.306049, acc.: 84.38%] [G loss: 5.293751]\n",
      "epoch:20 step:15635 [D loss: 0.266884, acc.: 85.94%] [G loss: 3.013765]\n",
      "epoch:20 step:15636 [D loss: 0.318721, acc.: 87.50%] [G loss: 2.954156]\n",
      "epoch:20 step:15637 [D loss: 0.235813, acc.: 89.84%] [G loss: 4.300886]\n",
      "epoch:20 step:15638 [D loss: 0.368315, acc.: 82.03%] [G loss: 3.386467]\n",
      "epoch:20 step:15639 [D loss: 0.320221, acc.: 88.28%] [G loss: 2.961504]\n",
      "epoch:20 step:15640 [D loss: 0.317962, acc.: 89.84%] [G loss: 3.008874]\n",
      "epoch:20 step:15641 [D loss: 0.276128, acc.: 90.62%] [G loss: 3.101919]\n",
      "epoch:20 step:15642 [D loss: 0.348095, acc.: 85.94%] [G loss: 2.723810]\n",
      "epoch:20 step:15643 [D loss: 0.242102, acc.: 88.28%] [G loss: 4.317008]\n",
      "epoch:20 step:15644 [D loss: 0.279849, acc.: 89.06%] [G loss: 3.932396]\n",
      "epoch:20 step:15645 [D loss: 0.235690, acc.: 89.84%] [G loss: 3.730943]\n",
      "epoch:20 step:15646 [D loss: 0.264526, acc.: 89.84%] [G loss: 3.864836]\n",
      "epoch:20 step:15647 [D loss: 0.381562, acc.: 81.25%] [G loss: 3.530041]\n",
      "epoch:20 step:15648 [D loss: 0.349806, acc.: 82.03%] [G loss: 4.009520]\n",
      "epoch:20 step:15649 [D loss: 0.337117, acc.: 85.16%] [G loss: 4.991531]\n",
      "epoch:20 step:15650 [D loss: 0.286140, acc.: 88.28%] [G loss: 3.585249]\n",
      "epoch:20 step:15651 [D loss: 0.408324, acc.: 78.91%] [G loss: 6.851630]\n",
      "epoch:20 step:15652 [D loss: 0.663330, acc.: 67.97%] [G loss: 3.092462]\n",
      "epoch:20 step:15653 [D loss: 0.300131, acc.: 86.72%] [G loss: 4.301985]\n",
      "epoch:20 step:15654 [D loss: 0.422064, acc.: 83.59%] [G loss: 5.963773]\n",
      "epoch:20 step:15655 [D loss: 0.339016, acc.: 85.16%] [G loss: 4.789212]\n",
      "epoch:20 step:15656 [D loss: 0.255679, acc.: 90.62%] [G loss: 4.359857]\n",
      "epoch:20 step:15657 [D loss: 0.339871, acc.: 86.72%] [G loss: 4.583707]\n",
      "epoch:20 step:15658 [D loss: 0.178230, acc.: 92.19%] [G loss: 4.711459]\n",
      "epoch:20 step:15659 [D loss: 0.325246, acc.: 87.50%] [G loss: 4.332296]\n",
      "epoch:20 step:15660 [D loss: 0.218887, acc.: 89.06%] [G loss: 5.092868]\n",
      "epoch:20 step:15661 [D loss: 0.347307, acc.: 82.03%] [G loss: 4.209425]\n",
      "epoch:20 step:15662 [D loss: 0.296009, acc.: 85.94%] [G loss: 3.008954]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:20 step:15663 [D loss: 0.309928, acc.: 88.28%] [G loss: 5.560950]\n",
      "epoch:20 step:15664 [D loss: 0.406237, acc.: 82.81%] [G loss: 2.305644]\n",
      "epoch:20 step:15665 [D loss: 0.348029, acc.: 85.16%] [G loss: 2.804455]\n",
      "epoch:20 step:15666 [D loss: 0.341920, acc.: 84.38%] [G loss: 2.612989]\n",
      "epoch:20 step:15667 [D loss: 0.313064, acc.: 85.94%] [G loss: 3.260441]\n",
      "epoch:20 step:15668 [D loss: 0.321712, acc.: 85.94%] [G loss: 3.674439]\n",
      "epoch:20 step:15669 [D loss: 0.255028, acc.: 89.84%] [G loss: 3.226571]\n",
      "epoch:20 step:15670 [D loss: 0.337508, acc.: 85.16%] [G loss: 3.842382]\n",
      "epoch:20 step:15671 [D loss: 0.345483, acc.: 85.16%] [G loss: 3.348217]\n",
      "epoch:20 step:15672 [D loss: 0.331377, acc.: 88.28%] [G loss: 3.811192]\n",
      "epoch:20 step:15673 [D loss: 0.335127, acc.: 86.72%] [G loss: 5.748992]\n",
      "epoch:20 step:15674 [D loss: 0.258664, acc.: 90.62%] [G loss: 4.960207]\n",
      "epoch:20 step:15675 [D loss: 0.286057, acc.: 89.06%] [G loss: 2.714298]\n",
      "epoch:20 step:15676 [D loss: 0.299559, acc.: 87.50%] [G loss: 3.768876]\n",
      "epoch:20 step:15677 [D loss: 0.392344, acc.: 82.81%] [G loss: 3.063953]\n",
      "epoch:20 step:15678 [D loss: 0.290812, acc.: 85.94%] [G loss: 2.793519]\n",
      "epoch:20 step:15679 [D loss: 0.331761, acc.: 87.50%] [G loss: 3.944510]\n",
      "epoch:20 step:15680 [D loss: 0.336664, acc.: 82.81%] [G loss: 3.340739]\n",
      "epoch:20 step:15681 [D loss: 0.367609, acc.: 82.03%] [G loss: 3.199170]\n",
      "epoch:20 step:15682 [D loss: 0.253940, acc.: 91.41%] [G loss: 3.138997]\n",
      "epoch:20 step:15683 [D loss: 0.363560, acc.: 81.25%] [G loss: 3.897769]\n",
      "epoch:20 step:15684 [D loss: 0.217449, acc.: 89.06%] [G loss: 5.115673]\n",
      "epoch:20 step:15685 [D loss: 0.412700, acc.: 77.34%] [G loss: 4.621169]\n",
      "epoch:20 step:15686 [D loss: 0.329081, acc.: 86.72%] [G loss: 3.190737]\n",
      "epoch:20 step:15687 [D loss: 0.202069, acc.: 91.41%] [G loss: 3.848009]\n",
      "epoch:20 step:15688 [D loss: 0.309543, acc.: 85.16%] [G loss: 4.754450]\n",
      "epoch:20 step:15689 [D loss: 0.391349, acc.: 82.03%] [G loss: 4.887937]\n",
      "epoch:20 step:15690 [D loss: 0.515903, acc.: 77.34%] [G loss: 3.929286]\n",
      "epoch:20 step:15691 [D loss: 0.447174, acc.: 81.25%] [G loss: 3.575594]\n",
      "epoch:20 step:15692 [D loss: 0.398174, acc.: 82.03%] [G loss: 2.476830]\n",
      "epoch:20 step:15693 [D loss: 0.426188, acc.: 82.03%] [G loss: 3.180422]\n",
      "epoch:20 step:15694 [D loss: 0.308024, acc.: 82.81%] [G loss: 3.382705]\n",
      "epoch:20 step:15695 [D loss: 0.379811, acc.: 80.47%] [G loss: 3.144330]\n",
      "epoch:20 step:15696 [D loss: 0.342005, acc.: 85.94%] [G loss: 4.840668]\n",
      "epoch:20 step:15697 [D loss: 0.479330, acc.: 75.78%] [G loss: 4.872149]\n",
      "epoch:20 step:15698 [D loss: 0.286189, acc.: 90.62%] [G loss: 3.095774]\n",
      "epoch:20 step:15699 [D loss: 0.300563, acc.: 84.38%] [G loss: 3.712053]\n",
      "epoch:20 step:15700 [D loss: 0.365700, acc.: 86.72%] [G loss: 2.594653]\n",
      "epoch:20 step:15701 [D loss: 0.350528, acc.: 84.38%] [G loss: 3.969852]\n",
      "epoch:20 step:15702 [D loss: 0.380896, acc.: 83.59%] [G loss: 4.447842]\n",
      "epoch:20 step:15703 [D loss: 0.384617, acc.: 90.62%] [G loss: 3.952652]\n",
      "epoch:20 step:15704 [D loss: 0.329140, acc.: 84.38%] [G loss: 3.289915]\n",
      "epoch:20 step:15705 [D loss: 0.438700, acc.: 79.69%] [G loss: 2.800358]\n",
      "epoch:20 step:15706 [D loss: 0.392629, acc.: 82.81%] [G loss: 2.279856]\n",
      "epoch:20 step:15707 [D loss: 0.282716, acc.: 88.28%] [G loss: 2.723086]\n",
      "epoch:20 step:15708 [D loss: 0.348431, acc.: 82.03%] [G loss: 3.121859]\n",
      "epoch:20 step:15709 [D loss: 0.367685, acc.: 82.81%] [G loss: 2.664958]\n",
      "epoch:20 step:15710 [D loss: 0.353662, acc.: 82.81%] [G loss: 3.267267]\n",
      "epoch:20 step:15711 [D loss: 0.341861, acc.: 85.94%] [G loss: 3.314248]\n",
      "epoch:20 step:15712 [D loss: 0.262853, acc.: 92.97%] [G loss: 3.536136]\n",
      "epoch:20 step:15713 [D loss: 0.380566, acc.: 84.38%] [G loss: 2.439496]\n",
      "epoch:20 step:15714 [D loss: 0.416909, acc.: 82.03%] [G loss: 3.154904]\n",
      "epoch:20 step:15715 [D loss: 0.415408, acc.: 85.94%] [G loss: 4.800919]\n",
      "epoch:20 step:15716 [D loss: 0.332532, acc.: 82.03%] [G loss: 2.503747]\n",
      "epoch:20 step:15717 [D loss: 0.375409, acc.: 85.16%] [G loss: 3.096215]\n",
      "epoch:20 step:15718 [D loss: 0.211774, acc.: 91.41%] [G loss: 5.061687]\n",
      "epoch:20 step:15719 [D loss: 0.261771, acc.: 92.19%] [G loss: 2.758112]\n",
      "epoch:20 step:15720 [D loss: 0.354192, acc.: 85.94%] [G loss: 2.858712]\n",
      "epoch:20 step:15721 [D loss: 0.324033, acc.: 82.81%] [G loss: 6.085122]\n",
      "epoch:20 step:15722 [D loss: 0.412553, acc.: 78.91%] [G loss: 3.306048]\n",
      "epoch:20 step:15723 [D loss: 0.373613, acc.: 82.03%] [G loss: 3.257316]\n",
      "epoch:20 step:15724 [D loss: 0.289394, acc.: 87.50%] [G loss: 5.158403]\n",
      "epoch:20 step:15725 [D loss: 0.326903, acc.: 85.16%] [G loss: 3.332772]\n",
      "epoch:20 step:15726 [D loss: 0.493009, acc.: 77.34%] [G loss: 4.521590]\n",
      "epoch:20 step:15727 [D loss: 0.399468, acc.: 79.69%] [G loss: 3.111992]\n",
      "epoch:20 step:15728 [D loss: 0.232214, acc.: 89.84%] [G loss: 3.357058]\n",
      "epoch:20 step:15729 [D loss: 0.516750, acc.: 77.34%] [G loss: 2.911908]\n",
      "epoch:20 step:15730 [D loss: 0.255650, acc.: 89.84%] [G loss: 2.555402]\n",
      "epoch:20 step:15731 [D loss: 0.354966, acc.: 84.38%] [G loss: 4.486846]\n",
      "epoch:20 step:15732 [D loss: 0.323495, acc.: 85.94%] [G loss: 2.429490]\n",
      "epoch:20 step:15733 [D loss: 0.390516, acc.: 85.94%] [G loss: 3.662639]\n",
      "epoch:20 step:15734 [D loss: 0.206411, acc.: 90.62%] [G loss: 4.338095]\n",
      "epoch:20 step:15735 [D loss: 0.239284, acc.: 89.06%] [G loss: 5.292140]\n",
      "epoch:20 step:15736 [D loss: 0.357504, acc.: 84.38%] [G loss: 2.881230]\n",
      "epoch:20 step:15737 [D loss: 0.282925, acc.: 87.50%] [G loss: 4.573555]\n",
      "epoch:20 step:15738 [D loss: 0.282059, acc.: 87.50%] [G loss: 5.886164]\n",
      "epoch:20 step:15739 [D loss: 0.339187, acc.: 87.50%] [G loss: 4.749473]\n",
      "epoch:20 step:15740 [D loss: 0.346389, acc.: 83.59%] [G loss: 3.210361]\n",
      "epoch:20 step:15741 [D loss: 0.370871, acc.: 85.94%] [G loss: 4.209638]\n",
      "epoch:20 step:15742 [D loss: 0.357524, acc.: 86.72%] [G loss: 3.864169]\n",
      "epoch:20 step:15743 [D loss: 0.374819, acc.: 84.38%] [G loss: 3.686319]\n",
      "epoch:20 step:15744 [D loss: 0.382674, acc.: 82.81%] [G loss: 3.616946]\n",
      "epoch:20 step:15745 [D loss: 0.329116, acc.: 85.94%] [G loss: 3.835160]\n",
      "epoch:20 step:15746 [D loss: 0.353675, acc.: 88.28%] [G loss: 3.368273]\n",
      "epoch:20 step:15747 [D loss: 0.297642, acc.: 88.28%] [G loss: 2.332511]\n",
      "epoch:20 step:15748 [D loss: 0.305319, acc.: 87.50%] [G loss: 3.449840]\n",
      "epoch:20 step:15749 [D loss: 0.286613, acc.: 89.06%] [G loss: 3.286570]\n",
      "epoch:20 step:15750 [D loss: 0.332582, acc.: 82.81%] [G loss: 4.866825]\n",
      "epoch:20 step:15751 [D loss: 0.302470, acc.: 89.06%] [G loss: 4.108169]\n",
      "epoch:20 step:15752 [D loss: 0.389724, acc.: 82.03%] [G loss: 3.710564]\n",
      "epoch:20 step:15753 [D loss: 0.353755, acc.: 84.38%] [G loss: 4.820744]\n",
      "epoch:20 step:15754 [D loss: 0.316542, acc.: 85.16%] [G loss: 3.976391]\n",
      "epoch:20 step:15755 [D loss: 0.314522, acc.: 82.81%] [G loss: 4.781619]\n",
      "epoch:20 step:15756 [D loss: 0.313859, acc.: 86.72%] [G loss: 4.120831]\n",
      "epoch:20 step:15757 [D loss: 0.312832, acc.: 87.50%] [G loss: 7.268890]\n",
      "epoch:20 step:15758 [D loss: 0.309551, acc.: 85.94%] [G loss: 3.902985]\n",
      "epoch:20 step:15759 [D loss: 0.204170, acc.: 92.19%] [G loss: 6.314951]\n",
      "epoch:20 step:15760 [D loss: 0.344319, acc.: 85.94%] [G loss: 2.840724]\n",
      "epoch:20 step:15761 [D loss: 0.233157, acc.: 89.84%] [G loss: 5.853611]\n",
      "epoch:20 step:15762 [D loss: 0.262874, acc.: 89.06%] [G loss: 4.708323]\n",
      "epoch:20 step:15763 [D loss: 0.213534, acc.: 92.19%] [G loss: 4.803247]\n",
      "epoch:20 step:15764 [D loss: 0.341803, acc.: 82.03%] [G loss: 3.910141]\n",
      "epoch:20 step:15765 [D loss: 0.297670, acc.: 85.94%] [G loss: 5.428806]\n",
      "epoch:20 step:15766 [D loss: 0.271986, acc.: 85.94%] [G loss: 3.680768]\n",
      "epoch:20 step:15767 [D loss: 0.254812, acc.: 88.28%] [G loss: 5.317311]\n",
      "epoch:20 step:15768 [D loss: 0.257886, acc.: 89.06%] [G loss: 5.678599]\n",
      "epoch:20 step:15769 [D loss: 0.194369, acc.: 92.97%] [G loss: 4.789683]\n",
      "epoch:20 step:15770 [D loss: 0.259366, acc.: 89.06%] [G loss: 3.073904]\n",
      "epoch:20 step:15771 [D loss: 0.325820, acc.: 89.84%] [G loss: 3.207929]\n",
      "epoch:20 step:15772 [D loss: 0.283736, acc.: 89.84%] [G loss: 3.138558]\n",
      "epoch:20 step:15773 [D loss: 0.343848, acc.: 89.06%] [G loss: 2.949509]\n",
      "epoch:20 step:15774 [D loss: 0.423621, acc.: 80.47%] [G loss: 3.591016]\n",
      "epoch:20 step:15775 [D loss: 0.266487, acc.: 90.62%] [G loss: 4.522889]\n",
      "epoch:20 step:15776 [D loss: 0.292596, acc.: 88.28%] [G loss: 3.660580]\n",
      "epoch:20 step:15777 [D loss: 0.261287, acc.: 90.62%] [G loss: 3.848098]\n",
      "epoch:20 step:15778 [D loss: 0.287786, acc.: 85.16%] [G loss: 3.487741]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:20 step:15779 [D loss: 0.306221, acc.: 87.50%] [G loss: 2.918028]\n",
      "epoch:20 step:15780 [D loss: 0.280082, acc.: 88.28%] [G loss: 3.116156]\n",
      "epoch:20 step:15781 [D loss: 0.378238, acc.: 82.03%] [G loss: 3.305962]\n",
      "epoch:20 step:15782 [D loss: 0.315917, acc.: 85.94%] [G loss: 2.472256]\n",
      "epoch:20 step:15783 [D loss: 0.272229, acc.: 91.41%] [G loss: 2.968975]\n",
      "epoch:20 step:15784 [D loss: 0.217803, acc.: 90.62%] [G loss: 2.610273]\n",
      "epoch:20 step:15785 [D loss: 0.349492, acc.: 82.81%] [G loss: 6.678378]\n",
      "epoch:20 step:15786 [D loss: 0.784969, acc.: 66.41%] [G loss: 5.855264]\n",
      "epoch:20 step:15787 [D loss: 1.992924, acc.: 63.28%] [G loss: 13.244230]\n",
      "epoch:20 step:15788 [D loss: 2.717350, acc.: 64.84%] [G loss: 4.870793]\n",
      "epoch:20 step:15789 [D loss: 1.035793, acc.: 66.41%] [G loss: 3.607573]\n",
      "epoch:20 step:15790 [D loss: 0.754949, acc.: 78.91%] [G loss: 2.885738]\n",
      "epoch:20 step:15791 [D loss: 0.513973, acc.: 78.12%] [G loss: 3.495677]\n",
      "epoch:20 step:15792 [D loss: 0.350836, acc.: 84.38%] [G loss: 4.333569]\n",
      "epoch:20 step:15793 [D loss: 0.304710, acc.: 87.50%] [G loss: 3.118903]\n",
      "epoch:20 step:15794 [D loss: 0.341476, acc.: 87.50%] [G loss: 3.683461]\n",
      "epoch:20 step:15795 [D loss: 0.445497, acc.: 78.91%] [G loss: 3.823247]\n",
      "epoch:20 step:15796 [D loss: 0.394091, acc.: 82.03%] [G loss: 4.130899]\n",
      "epoch:20 step:15797 [D loss: 0.343344, acc.: 82.03%] [G loss: 4.884953]\n",
      "epoch:20 step:15798 [D loss: 0.341671, acc.: 86.72%] [G loss: 6.958005]\n",
      "epoch:20 step:15799 [D loss: 0.261896, acc.: 89.06%] [G loss: 4.050204]\n",
      "epoch:20 step:15800 [D loss: 0.321347, acc.: 85.16%] [G loss: 4.811171]\n",
      "##############\n",
      "[0.85076169 0.85812957 0.79088308 0.79968546 0.7788031  0.82710597\n",
      " 0.87962076 0.84728845 0.83091944 0.8008613 ]\n",
      "##########\n",
      "epoch:20 step:15801 [D loss: 0.275098, acc.: 88.28%] [G loss: 2.304925]\n",
      "epoch:20 step:15802 [D loss: 0.270496, acc.: 89.06%] [G loss: 3.548645]\n",
      "epoch:20 step:15803 [D loss: 0.261886, acc.: 90.62%] [G loss: 3.467230]\n",
      "epoch:20 step:15804 [D loss: 0.408733, acc.: 78.12%] [G loss: 2.581992]\n",
      "epoch:20 step:15805 [D loss: 0.262014, acc.: 88.28%] [G loss: 3.433178]\n",
      "epoch:20 step:15806 [D loss: 0.262467, acc.: 89.84%] [G loss: 2.911689]\n",
      "epoch:20 step:15807 [D loss: 0.319794, acc.: 86.72%] [G loss: 3.242851]\n",
      "epoch:20 step:15808 [D loss: 0.323506, acc.: 82.03%] [G loss: 3.364434]\n",
      "epoch:20 step:15809 [D loss: 0.334956, acc.: 84.38%] [G loss: 2.878812]\n",
      "epoch:20 step:15810 [D loss: 0.510252, acc.: 75.00%] [G loss: 3.937824]\n",
      "epoch:20 step:15811 [D loss: 0.420988, acc.: 81.25%] [G loss: 4.581187]\n",
      "epoch:20 step:15812 [D loss: 0.335170, acc.: 87.50%] [G loss: 2.600358]\n",
      "epoch:20 step:15813 [D loss: 0.245707, acc.: 91.41%] [G loss: 3.179906]\n",
      "epoch:20 step:15814 [D loss: 0.330330, acc.: 85.94%] [G loss: 2.812476]\n",
      "epoch:20 step:15815 [D loss: 0.252845, acc.: 90.62%] [G loss: 3.711473]\n",
      "epoch:20 step:15816 [D loss: 0.307380, acc.: 87.50%] [G loss: 3.451018]\n",
      "epoch:20 step:15817 [D loss: 0.347724, acc.: 85.94%] [G loss: 2.657606]\n",
      "epoch:20 step:15818 [D loss: 0.254952, acc.: 92.19%] [G loss: 2.822051]\n",
      "epoch:20 step:15819 [D loss: 0.283789, acc.: 88.28%] [G loss: 3.017336]\n",
      "epoch:20 step:15820 [D loss: 0.374673, acc.: 79.69%] [G loss: 3.130802]\n",
      "epoch:20 step:15821 [D loss: 0.260265, acc.: 89.84%] [G loss: 3.282818]\n",
      "epoch:20 step:15822 [D loss: 0.320640, acc.: 87.50%] [G loss: 3.142295]\n",
      "epoch:20 step:15823 [D loss: 0.300123, acc.: 88.28%] [G loss: 2.968369]\n",
      "epoch:20 step:15824 [D loss: 0.267083, acc.: 89.84%] [G loss: 3.426325]\n",
      "epoch:20 step:15825 [D loss: 0.333360, acc.: 85.16%] [G loss: 3.483533]\n",
      "epoch:20 step:15826 [D loss: 0.294135, acc.: 85.94%] [G loss: 3.778912]\n",
      "epoch:20 step:15827 [D loss: 0.325820, acc.: 83.59%] [G loss: 3.607881]\n",
      "epoch:20 step:15828 [D loss: 0.404395, acc.: 85.94%] [G loss: 3.183629]\n",
      "epoch:20 step:15829 [D loss: 0.199302, acc.: 92.19%] [G loss: 3.184583]\n",
      "epoch:20 step:15830 [D loss: 0.318742, acc.: 83.59%] [G loss: 4.373639]\n",
      "epoch:20 step:15831 [D loss: 0.203638, acc.: 92.19%] [G loss: 3.586648]\n",
      "epoch:20 step:15832 [D loss: 0.341465, acc.: 85.94%] [G loss: 3.045877]\n",
      "epoch:20 step:15833 [D loss: 0.338196, acc.: 87.50%] [G loss: 3.693243]\n",
      "epoch:20 step:15834 [D loss: 0.270465, acc.: 88.28%] [G loss: 3.307450]\n",
      "epoch:20 step:15835 [D loss: 0.251279, acc.: 89.06%] [G loss: 3.345761]\n",
      "epoch:20 step:15836 [D loss: 0.342587, acc.: 84.38%] [G loss: 2.709107]\n",
      "epoch:20 step:15837 [D loss: 0.347354, acc.: 87.50%] [G loss: 2.938997]\n",
      "epoch:20 step:15838 [D loss: 0.365174, acc.: 82.03%] [G loss: 3.225042]\n",
      "epoch:20 step:15839 [D loss: 0.262218, acc.: 89.84%] [G loss: 2.942113]\n",
      "epoch:20 step:15840 [D loss: 0.370026, acc.: 82.81%] [G loss: 2.619692]\n",
      "epoch:20 step:15841 [D loss: 0.310704, acc.: 86.72%] [G loss: 2.658577]\n",
      "epoch:20 step:15842 [D loss: 0.285417, acc.: 89.84%] [G loss: 2.676156]\n",
      "epoch:20 step:15843 [D loss: 0.328912, acc.: 87.50%] [G loss: 2.760232]\n",
      "epoch:20 step:15844 [D loss: 0.393267, acc.: 80.47%] [G loss: 2.562038]\n",
      "epoch:20 step:15845 [D loss: 0.345266, acc.: 83.59%] [G loss: 2.248754]\n",
      "epoch:20 step:15846 [D loss: 0.417272, acc.: 81.25%] [G loss: 3.152711]\n",
      "epoch:20 step:15847 [D loss: 0.352991, acc.: 85.94%] [G loss: 3.086890]\n",
      "epoch:20 step:15848 [D loss: 0.378085, acc.: 85.16%] [G loss: 3.170133]\n",
      "epoch:20 step:15849 [D loss: 0.491654, acc.: 76.56%] [G loss: 2.750494]\n",
      "epoch:20 step:15850 [D loss: 0.427962, acc.: 82.81%] [G loss: 3.102215]\n",
      "epoch:20 step:15851 [D loss: 0.263384, acc.: 89.84%] [G loss: 2.861866]\n",
      "epoch:20 step:15852 [D loss: 0.259259, acc.: 89.06%] [G loss: 3.269006]\n",
      "epoch:20 step:15853 [D loss: 0.329466, acc.: 85.94%] [G loss: 2.839070]\n",
      "epoch:20 step:15854 [D loss: 0.250753, acc.: 90.62%] [G loss: 3.253638]\n",
      "epoch:20 step:15855 [D loss: 0.217057, acc.: 89.84%] [G loss: 5.046474]\n",
      "epoch:20 step:15856 [D loss: 0.270461, acc.: 87.50%] [G loss: 4.321978]\n",
      "epoch:20 step:15857 [D loss: 0.179694, acc.: 92.19%] [G loss: 2.598225]\n",
      "epoch:20 step:15858 [D loss: 0.312864, acc.: 85.94%] [G loss: 3.402745]\n",
      "epoch:20 step:15859 [D loss: 0.309033, acc.: 89.06%] [G loss: 2.951871]\n",
      "epoch:20 step:15860 [D loss: 0.252883, acc.: 86.72%] [G loss: 4.335107]\n",
      "epoch:20 step:15861 [D loss: 0.403401, acc.: 78.91%] [G loss: 4.355584]\n",
      "epoch:20 step:15862 [D loss: 0.404910, acc.: 79.69%] [G loss: 5.795508]\n",
      "epoch:20 step:15863 [D loss: 0.383311, acc.: 81.25%] [G loss: 2.634639]\n",
      "epoch:20 step:15864 [D loss: 0.279783, acc.: 87.50%] [G loss: 3.005484]\n",
      "epoch:20 step:15865 [D loss: 0.251495, acc.: 87.50%] [G loss: 3.122165]\n",
      "epoch:20 step:15866 [D loss: 0.346201, acc.: 83.59%] [G loss: 3.217525]\n",
      "epoch:20 step:15867 [D loss: 0.256649, acc.: 91.41%] [G loss: 2.749741]\n",
      "epoch:20 step:15868 [D loss: 0.364674, acc.: 86.72%] [G loss: 3.640400]\n",
      "epoch:20 step:15869 [D loss: 0.355131, acc.: 85.16%] [G loss: 2.891878]\n",
      "epoch:20 step:15870 [D loss: 0.403428, acc.: 80.47%] [G loss: 2.276058]\n",
      "epoch:20 step:15871 [D loss: 0.258976, acc.: 88.28%] [G loss: 3.835880]\n",
      "epoch:20 step:15872 [D loss: 0.283857, acc.: 83.59%] [G loss: 6.101410]\n",
      "epoch:20 step:15873 [D loss: 0.222014, acc.: 89.84%] [G loss: 4.179148]\n",
      "epoch:20 step:15874 [D loss: 0.196630, acc.: 92.97%] [G loss: 3.227680]\n",
      "epoch:20 step:15875 [D loss: 0.334103, acc.: 86.72%] [G loss: 4.251776]\n",
      "epoch:20 step:15876 [D loss: 0.305668, acc.: 85.94%] [G loss: 2.838568]\n",
      "epoch:20 step:15877 [D loss: 0.236319, acc.: 90.62%] [G loss: 5.389019]\n",
      "epoch:20 step:15878 [D loss: 0.390811, acc.: 81.25%] [G loss: 2.603825]\n",
      "epoch:20 step:15879 [D loss: 0.302670, acc.: 83.59%] [G loss: 3.936952]\n",
      "epoch:20 step:15880 [D loss: 0.258487, acc.: 88.28%] [G loss: 2.926580]\n",
      "epoch:20 step:15881 [D loss: 0.397470, acc.: 82.03%] [G loss: 3.632666]\n",
      "epoch:20 step:15882 [D loss: 0.282575, acc.: 88.28%] [G loss: 3.116066]\n",
      "epoch:20 step:15883 [D loss: 0.197603, acc.: 90.62%] [G loss: 6.120774]\n",
      "epoch:20 step:15884 [D loss: 0.375877, acc.: 79.69%] [G loss: 3.282283]\n",
      "epoch:20 step:15885 [D loss: 0.346388, acc.: 81.25%] [G loss: 4.410508]\n",
      "epoch:20 step:15886 [D loss: 0.429259, acc.: 83.59%] [G loss: 3.488375]\n",
      "epoch:20 step:15887 [D loss: 0.646874, acc.: 75.00%] [G loss: 8.876945]\n",
      "epoch:20 step:15888 [D loss: 1.067221, acc.: 58.59%] [G loss: 5.024924]\n",
      "epoch:20 step:15889 [D loss: 0.553305, acc.: 76.56%] [G loss: 3.419475]\n",
      "epoch:20 step:15890 [D loss: 0.520811, acc.: 72.66%] [G loss: 4.084998]\n",
      "epoch:20 step:15891 [D loss: 0.317747, acc.: 85.94%] [G loss: 3.229487]\n",
      "epoch:20 step:15892 [D loss: 0.421933, acc.: 82.03%] [G loss: 3.194163]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:20 step:15893 [D loss: 0.315892, acc.: 87.50%] [G loss: 2.762831]\n",
      "epoch:20 step:15894 [D loss: 0.401696, acc.: 85.16%] [G loss: 4.168303]\n",
      "epoch:20 step:15895 [D loss: 0.604025, acc.: 73.44%] [G loss: 3.150020]\n",
      "epoch:20 step:15896 [D loss: 0.426642, acc.: 79.69%] [G loss: 2.968635]\n",
      "epoch:20 step:15897 [D loss: 0.295957, acc.: 87.50%] [G loss: 4.050056]\n",
      "epoch:20 step:15898 [D loss: 0.349840, acc.: 82.81%] [G loss: 3.190511]\n",
      "epoch:20 step:15899 [D loss: 0.257449, acc.: 89.84%] [G loss: 4.076381]\n",
      "epoch:20 step:15900 [D loss: 0.145337, acc.: 93.75%] [G loss: 6.248561]\n",
      "epoch:20 step:15901 [D loss: 0.308707, acc.: 83.59%] [G loss: 3.187977]\n",
      "epoch:20 step:15902 [D loss: 0.178039, acc.: 92.97%] [G loss: 2.912084]\n",
      "epoch:20 step:15903 [D loss: 0.277740, acc.: 87.50%] [G loss: 2.730361]\n",
      "epoch:20 step:15904 [D loss: 0.309443, acc.: 89.06%] [G loss: 3.678432]\n",
      "epoch:20 step:15905 [D loss: 0.200486, acc.: 92.19%] [G loss: 5.599009]\n",
      "epoch:20 step:15906 [D loss: 0.332316, acc.: 89.84%] [G loss: 2.871511]\n",
      "epoch:20 step:15907 [D loss: 0.306938, acc.: 87.50%] [G loss: 3.067649]\n",
      "epoch:20 step:15908 [D loss: 0.424975, acc.: 79.69%] [G loss: 2.954307]\n",
      "epoch:20 step:15909 [D loss: 0.282927, acc.: 84.38%] [G loss: 2.851396]\n",
      "epoch:20 step:15910 [D loss: 0.278770, acc.: 88.28%] [G loss: 2.451649]\n",
      "epoch:20 step:15911 [D loss: 0.241834, acc.: 88.28%] [G loss: 3.413172]\n",
      "epoch:20 step:15912 [D loss: 0.280672, acc.: 89.84%] [G loss: 2.629968]\n",
      "epoch:20 step:15913 [D loss: 0.340802, acc.: 85.94%] [G loss: 3.367334]\n",
      "epoch:20 step:15914 [D loss: 0.348959, acc.: 86.72%] [G loss: 2.792644]\n",
      "epoch:20 step:15915 [D loss: 0.309060, acc.: 90.62%] [G loss: 3.211370]\n",
      "epoch:20 step:15916 [D loss: 0.330370, acc.: 85.94%] [G loss: 3.289579]\n",
      "epoch:20 step:15917 [D loss: 0.436040, acc.: 78.91%] [G loss: 2.217842]\n",
      "epoch:20 step:15918 [D loss: 0.318340, acc.: 86.72%] [G loss: 2.664035]\n",
      "epoch:20 step:15919 [D loss: 0.310235, acc.: 85.94%] [G loss: 3.042722]\n",
      "epoch:20 step:15920 [D loss: 0.440788, acc.: 77.34%] [G loss: 2.657510]\n",
      "epoch:20 step:15921 [D loss: 0.241582, acc.: 92.97%] [G loss: 4.080453]\n",
      "epoch:20 step:15922 [D loss: 0.314186, acc.: 85.94%] [G loss: 4.132568]\n",
      "epoch:20 step:15923 [D loss: 0.282784, acc.: 86.72%] [G loss: 3.865203]\n",
      "epoch:20 step:15924 [D loss: 0.269691, acc.: 88.28%] [G loss: 3.996645]\n",
      "epoch:20 step:15925 [D loss: 0.282138, acc.: 89.84%] [G loss: 3.642869]\n",
      "epoch:20 step:15926 [D loss: 0.332299, acc.: 87.50%] [G loss: 3.972050]\n",
      "epoch:20 step:15927 [D loss: 0.285082, acc.: 88.28%] [G loss: 3.155534]\n",
      "epoch:20 step:15928 [D loss: 0.303798, acc.: 85.94%] [G loss: 4.176098]\n",
      "epoch:20 step:15929 [D loss: 0.355055, acc.: 84.38%] [G loss: 3.367814]\n",
      "epoch:20 step:15930 [D loss: 0.276641, acc.: 88.28%] [G loss: 3.797855]\n",
      "epoch:20 step:15931 [D loss: 0.355552, acc.: 83.59%] [G loss: 4.025510]\n",
      "epoch:20 step:15932 [D loss: 0.381116, acc.: 83.59%] [G loss: 3.701058]\n",
      "epoch:20 step:15933 [D loss: 0.482363, acc.: 79.69%] [G loss: 4.441279]\n",
      "epoch:20 step:15934 [D loss: 0.571915, acc.: 77.34%] [G loss: 3.478960]\n",
      "epoch:20 step:15935 [D loss: 0.458598, acc.: 78.12%] [G loss: 4.203048]\n",
      "epoch:20 step:15936 [D loss: 0.248665, acc.: 89.84%] [G loss: 3.115706]\n",
      "epoch:20 step:15937 [D loss: 0.298287, acc.: 85.16%] [G loss: 5.831605]\n",
      "epoch:20 step:15938 [D loss: 0.308770, acc.: 83.59%] [G loss: 4.376515]\n",
      "epoch:20 step:15939 [D loss: 0.234541, acc.: 92.97%] [G loss: 2.889174]\n",
      "epoch:20 step:15940 [D loss: 0.292793, acc.: 85.94%] [G loss: 3.558582]\n",
      "epoch:20 step:15941 [D loss: 0.263299, acc.: 89.84%] [G loss: 3.467751]\n",
      "epoch:20 step:15942 [D loss: 0.244579, acc.: 89.06%] [G loss: 3.141413]\n",
      "epoch:20 step:15943 [D loss: 0.326487, acc.: 84.38%] [G loss: 2.599563]\n",
      "epoch:20 step:15944 [D loss: 0.271719, acc.: 90.62%] [G loss: 2.583566]\n",
      "epoch:20 step:15945 [D loss: 0.381762, acc.: 83.59%] [G loss: 3.535865]\n",
      "epoch:20 step:15946 [D loss: 0.387373, acc.: 82.81%] [G loss: 2.710844]\n",
      "epoch:20 step:15947 [D loss: 0.299361, acc.: 85.94%] [G loss: 2.549990]\n",
      "epoch:20 step:15948 [D loss: 0.322799, acc.: 86.72%] [G loss: 2.441082]\n",
      "epoch:20 step:15949 [D loss: 0.267009, acc.: 86.72%] [G loss: 3.278500]\n",
      "epoch:20 step:15950 [D loss: 0.315489, acc.: 84.38%] [G loss: 2.912894]\n",
      "epoch:20 step:15951 [D loss: 0.299620, acc.: 86.72%] [G loss: 2.318100]\n",
      "epoch:20 step:15952 [D loss: 0.286259, acc.: 89.84%] [G loss: 3.303450]\n",
      "epoch:20 step:15953 [D loss: 0.303327, acc.: 85.94%] [G loss: 2.668563]\n",
      "epoch:20 step:15954 [D loss: 0.334980, acc.: 84.38%] [G loss: 2.685314]\n",
      "epoch:20 step:15955 [D loss: 0.317517, acc.: 86.72%] [G loss: 2.772477]\n",
      "epoch:20 step:15956 [D loss: 0.355249, acc.: 83.59%] [G loss: 2.835270]\n",
      "epoch:20 step:15957 [D loss: 0.263228, acc.: 89.84%] [G loss: 2.478093]\n",
      "epoch:20 step:15958 [D loss: 0.323734, acc.: 85.94%] [G loss: 2.862885]\n",
      "epoch:20 step:15959 [D loss: 0.272469, acc.: 88.28%] [G loss: 2.859366]\n",
      "epoch:20 step:15960 [D loss: 0.414037, acc.: 83.59%] [G loss: 2.521897]\n",
      "epoch:20 step:15961 [D loss: 0.376805, acc.: 84.38%] [G loss: 3.058063]\n",
      "epoch:20 step:15962 [D loss: 0.413356, acc.: 81.25%] [G loss: 3.387819]\n",
      "epoch:20 step:15963 [D loss: 0.353131, acc.: 84.38%] [G loss: 3.238506]\n",
      "epoch:20 step:15964 [D loss: 0.289475, acc.: 87.50%] [G loss: 3.896725]\n",
      "epoch:20 step:15965 [D loss: 0.308978, acc.: 86.72%] [G loss: 2.858322]\n",
      "epoch:20 step:15966 [D loss: 0.344653, acc.: 82.03%] [G loss: 3.801425]\n",
      "epoch:20 step:15967 [D loss: 0.302664, acc.: 88.28%] [G loss: 3.309060]\n",
      "epoch:20 step:15968 [D loss: 0.416787, acc.: 79.69%] [G loss: 2.288626]\n",
      "epoch:20 step:15969 [D loss: 0.298916, acc.: 88.28%] [G loss: 2.635180]\n",
      "epoch:20 step:15970 [D loss: 0.374517, acc.: 84.38%] [G loss: 2.816730]\n",
      "epoch:20 step:15971 [D loss: 0.398373, acc.: 79.69%] [G loss: 3.537332]\n",
      "epoch:20 step:15972 [D loss: 0.371892, acc.: 80.47%] [G loss: 3.454945]\n",
      "epoch:20 step:15973 [D loss: 0.232309, acc.: 91.41%] [G loss: 3.646446]\n",
      "epoch:20 step:15974 [D loss: 0.290950, acc.: 85.16%] [G loss: 3.148146]\n",
      "epoch:20 step:15975 [D loss: 0.262301, acc.: 85.16%] [G loss: 3.744417]\n",
      "epoch:20 step:15976 [D loss: 0.423408, acc.: 77.34%] [G loss: 3.709234]\n",
      "epoch:20 step:15977 [D loss: 0.305149, acc.: 84.38%] [G loss: 3.695279]\n",
      "epoch:20 step:15978 [D loss: 0.403803, acc.: 82.03%] [G loss: 2.747916]\n",
      "epoch:20 step:15979 [D loss: 0.253780, acc.: 88.28%] [G loss: 3.751976]\n",
      "epoch:20 step:15980 [D loss: 0.349057, acc.: 82.81%] [G loss: 3.115583]\n",
      "epoch:20 step:15981 [D loss: 0.303731, acc.: 85.94%] [G loss: 4.033973]\n",
      "epoch:20 step:15982 [D loss: 0.256852, acc.: 89.06%] [G loss: 3.879109]\n",
      "epoch:20 step:15983 [D loss: 0.352075, acc.: 87.50%] [G loss: 3.823979]\n",
      "epoch:20 step:15984 [D loss: 0.316182, acc.: 81.25%] [G loss: 2.598781]\n",
      "epoch:20 step:15985 [D loss: 0.387692, acc.: 79.69%] [G loss: 3.015172]\n",
      "epoch:20 step:15986 [D loss: 0.398973, acc.: 82.03%] [G loss: 3.334568]\n",
      "epoch:20 step:15987 [D loss: 0.411643, acc.: 83.59%] [G loss: 3.471839]\n",
      "epoch:20 step:15988 [D loss: 0.382783, acc.: 79.69%] [G loss: 3.879713]\n",
      "epoch:20 step:15989 [D loss: 0.386543, acc.: 83.59%] [G loss: 3.046357]\n",
      "epoch:20 step:15990 [D loss: 0.383513, acc.: 85.16%] [G loss: 3.303498]\n",
      "epoch:20 step:15991 [D loss: 0.328114, acc.: 87.50%] [G loss: 3.605102]\n",
      "epoch:20 step:15992 [D loss: 0.361296, acc.: 86.72%] [G loss: 3.900140]\n",
      "epoch:20 step:15993 [D loss: 0.253488, acc.: 89.06%] [G loss: 4.587676]\n",
      "epoch:20 step:15994 [D loss: 0.379793, acc.: 82.81%] [G loss: 3.398026]\n",
      "epoch:20 step:15995 [D loss: 0.333901, acc.: 85.94%] [G loss: 3.206969]\n",
      "epoch:20 step:15996 [D loss: 0.282902, acc.: 89.06%] [G loss: 4.191259]\n",
      "epoch:20 step:15997 [D loss: 0.371229, acc.: 85.16%] [G loss: 2.907700]\n",
      "epoch:20 step:15998 [D loss: 0.278431, acc.: 87.50%] [G loss: 3.211691]\n",
      "epoch:20 step:15999 [D loss: 0.258671, acc.: 89.84%] [G loss: 2.786467]\n",
      "epoch:20 step:16000 [D loss: 0.262818, acc.: 89.84%] [G loss: 3.322755]\n",
      "##############\n",
      "[0.86956617 0.88018588 0.79897271 0.82530977 0.7779586  0.82414314\n",
      " 0.87612234 0.83390038 0.80938928 0.80590805]\n",
      "##########\n",
      "epoch:20 step:16001 [D loss: 0.207625, acc.: 92.19%] [G loss: 2.780600]\n",
      "epoch:20 step:16002 [D loss: 0.271921, acc.: 89.84%] [G loss: 3.017828]\n",
      "epoch:20 step:16003 [D loss: 0.297546, acc.: 85.16%] [G loss: 3.002652]\n",
      "epoch:20 step:16004 [D loss: 0.254292, acc.: 89.84%] [G loss: 2.566818]\n",
      "epoch:20 step:16005 [D loss: 0.341513, acc.: 82.81%] [G loss: 3.748718]\n",
      "epoch:20 step:16006 [D loss: 0.370679, acc.: 82.03%] [G loss: 2.801965]\n",
      "epoch:20 step:16007 [D loss: 0.283158, acc.: 86.72%] [G loss: 2.731122]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:20 step:16008 [D loss: 0.350912, acc.: 88.28%] [G loss: 2.500209]\n",
      "epoch:20 step:16009 [D loss: 0.347742, acc.: 82.81%] [G loss: 3.800153]\n",
      "epoch:20 step:16010 [D loss: 0.328203, acc.: 86.72%] [G loss: 4.803897]\n",
      "epoch:20 step:16011 [D loss: 0.428556, acc.: 81.25%] [G loss: 2.783841]\n",
      "epoch:20 step:16012 [D loss: 0.376553, acc.: 79.69%] [G loss: 2.922394]\n",
      "epoch:20 step:16013 [D loss: 0.289487, acc.: 89.84%] [G loss: 2.761430]\n",
      "epoch:20 step:16014 [D loss: 0.294653, acc.: 86.72%] [G loss: 3.086883]\n",
      "epoch:20 step:16015 [D loss: 0.376721, acc.: 83.59%] [G loss: 3.569818]\n",
      "epoch:20 step:16016 [D loss: 0.456993, acc.: 79.69%] [G loss: 3.374348]\n",
      "epoch:20 step:16017 [D loss: 0.298171, acc.: 87.50%] [G loss: 3.420563]\n",
      "epoch:20 step:16018 [D loss: 0.324660, acc.: 82.81%] [G loss: 4.895226]\n",
      "epoch:20 step:16019 [D loss: 0.286401, acc.: 86.72%] [G loss: 3.300904]\n",
      "epoch:20 step:16020 [D loss: 0.278693, acc.: 90.62%] [G loss: 2.653487]\n",
      "epoch:20 step:16021 [D loss: 0.288353, acc.: 86.72%] [G loss: 3.753528]\n",
      "epoch:20 step:16022 [D loss: 0.404747, acc.: 80.47%] [G loss: 2.481624]\n",
      "epoch:20 step:16023 [D loss: 0.321837, acc.: 84.38%] [G loss: 3.689648]\n",
      "epoch:20 step:16024 [D loss: 0.379198, acc.: 82.81%] [G loss: 3.757604]\n",
      "epoch:20 step:16025 [D loss: 0.352605, acc.: 82.81%] [G loss: 2.485320]\n",
      "epoch:20 step:16026 [D loss: 0.404853, acc.: 82.81%] [G loss: 3.050471]\n",
      "epoch:20 step:16027 [D loss: 0.335610, acc.: 84.38%] [G loss: 3.640171]\n",
      "epoch:20 step:16028 [D loss: 0.379618, acc.: 82.81%] [G loss: 4.330061]\n",
      "epoch:20 step:16029 [D loss: 0.344988, acc.: 82.81%] [G loss: 2.918906]\n",
      "epoch:20 step:16030 [D loss: 0.358397, acc.: 81.25%] [G loss: 3.441620]\n",
      "epoch:20 step:16031 [D loss: 0.393865, acc.: 78.12%] [G loss: 2.285452]\n",
      "epoch:20 step:16032 [D loss: 0.350754, acc.: 85.16%] [G loss: 3.244468]\n",
      "epoch:20 step:16033 [D loss: 0.421104, acc.: 79.69%] [G loss: 2.535625]\n",
      "epoch:20 step:16034 [D loss: 0.335783, acc.: 89.84%] [G loss: 3.453902]\n",
      "epoch:20 step:16035 [D loss: 0.281321, acc.: 87.50%] [G loss: 2.794111]\n",
      "epoch:20 step:16036 [D loss: 0.254598, acc.: 89.06%] [G loss: 3.778275]\n",
      "epoch:20 step:16037 [D loss: 0.341725, acc.: 81.25%] [G loss: 4.238130]\n",
      "epoch:20 step:16038 [D loss: 0.398978, acc.: 79.69%] [G loss: 2.790992]\n",
      "epoch:20 step:16039 [D loss: 0.304572, acc.: 87.50%] [G loss: 3.287775]\n",
      "epoch:20 step:16040 [D loss: 0.337737, acc.: 87.50%] [G loss: 2.558880]\n",
      "epoch:20 step:16041 [D loss: 0.370299, acc.: 83.59%] [G loss: 3.092075]\n",
      "epoch:20 step:16042 [D loss: 0.420172, acc.: 80.47%] [G loss: 2.909966]\n",
      "epoch:20 step:16043 [D loss: 0.417254, acc.: 82.81%] [G loss: 3.491689]\n",
      "epoch:20 step:16044 [D loss: 0.291044, acc.: 83.59%] [G loss: 2.975834]\n",
      "epoch:20 step:16045 [D loss: 0.380986, acc.: 82.81%] [G loss: 4.167547]\n",
      "epoch:20 step:16046 [D loss: 0.355160, acc.: 85.94%] [G loss: 4.142778]\n",
      "epoch:20 step:16047 [D loss: 0.332979, acc.: 81.25%] [G loss: 3.520313]\n",
      "epoch:20 step:16048 [D loss: 0.406851, acc.: 81.25%] [G loss: 4.264519]\n",
      "epoch:20 step:16049 [D loss: 0.374325, acc.: 85.94%] [G loss: 3.443249]\n",
      "epoch:20 step:16050 [D loss: 0.325944, acc.: 83.59%] [G loss: 3.612846]\n",
      "epoch:20 step:16051 [D loss: 0.383332, acc.: 82.81%] [G loss: 3.191701]\n",
      "epoch:20 step:16052 [D loss: 0.371917, acc.: 82.03%] [G loss: 5.159957]\n",
      "epoch:20 step:16053 [D loss: 0.265260, acc.: 89.06%] [G loss: 5.406755]\n",
      "epoch:20 step:16054 [D loss: 0.288148, acc.: 85.94%] [G loss: 5.334035]\n",
      "epoch:20 step:16055 [D loss: 0.471056, acc.: 76.56%] [G loss: 3.002774]\n",
      "epoch:20 step:16056 [D loss: 0.272334, acc.: 87.50%] [G loss: 6.997873]\n",
      "epoch:20 step:16057 [D loss: 0.331390, acc.: 89.06%] [G loss: 3.700685]\n",
      "epoch:20 step:16058 [D loss: 0.222682, acc.: 89.84%] [G loss: 8.682766]\n",
      "epoch:20 step:16059 [D loss: 0.319849, acc.: 88.28%] [G loss: 4.644679]\n",
      "epoch:20 step:16060 [D loss: 0.265207, acc.: 86.72%] [G loss: 7.808453]\n",
      "epoch:20 step:16061 [D loss: 0.202232, acc.: 89.84%] [G loss: 5.106669]\n",
      "epoch:20 step:16062 [D loss: 0.247947, acc.: 91.41%] [G loss: 4.858103]\n",
      "epoch:20 step:16063 [D loss: 0.392399, acc.: 81.25%] [G loss: 4.162817]\n",
      "epoch:20 step:16064 [D loss: 0.300387, acc.: 86.72%] [G loss: 4.014184]\n",
      "epoch:20 step:16065 [D loss: 0.245929, acc.: 88.28%] [G loss: 3.839542]\n",
      "epoch:20 step:16066 [D loss: 0.218110, acc.: 90.62%] [G loss: 3.175504]\n",
      "epoch:20 step:16067 [D loss: 0.281258, acc.: 89.84%] [G loss: 5.163203]\n",
      "epoch:20 step:16068 [D loss: 0.330339, acc.: 85.16%] [G loss: 4.288954]\n",
      "epoch:20 step:16069 [D loss: 0.250562, acc.: 88.28%] [G loss: 3.248333]\n",
      "epoch:20 step:16070 [D loss: 0.382865, acc.: 82.03%] [G loss: 4.610010]\n",
      "epoch:20 step:16071 [D loss: 0.196267, acc.: 92.19%] [G loss: 3.224886]\n",
      "epoch:20 step:16072 [D loss: 0.308715, acc.: 84.38%] [G loss: 3.631817]\n",
      "epoch:20 step:16073 [D loss: 0.254347, acc.: 89.84%] [G loss: 3.653439]\n",
      "epoch:20 step:16074 [D loss: 0.301842, acc.: 88.28%] [G loss: 3.613066]\n",
      "epoch:20 step:16075 [D loss: 0.308766, acc.: 88.28%] [G loss: 3.583748]\n",
      "epoch:20 step:16076 [D loss: 0.304420, acc.: 85.16%] [G loss: 3.380497]\n",
      "epoch:20 step:16077 [D loss: 0.334720, acc.: 85.16%] [G loss: 3.289077]\n",
      "epoch:20 step:16078 [D loss: 0.265769, acc.: 90.62%] [G loss: 3.959235]\n",
      "epoch:20 step:16079 [D loss: 0.369455, acc.: 82.03%] [G loss: 3.391163]\n",
      "epoch:20 step:16080 [D loss: 0.367922, acc.: 85.94%] [G loss: 4.621660]\n",
      "epoch:20 step:16081 [D loss: 0.309251, acc.: 88.28%] [G loss: 3.143329]\n",
      "epoch:20 step:16082 [D loss: 0.372451, acc.: 83.59%] [G loss: 5.191672]\n",
      "epoch:20 step:16083 [D loss: 0.338864, acc.: 85.16%] [G loss: 3.350294]\n",
      "epoch:20 step:16084 [D loss: 0.302619, acc.: 89.06%] [G loss: 3.982624]\n",
      "epoch:20 step:16085 [D loss: 0.286365, acc.: 85.94%] [G loss: 3.804079]\n",
      "epoch:20 step:16086 [D loss: 0.418105, acc.: 83.59%] [G loss: 4.136380]\n",
      "epoch:20 step:16087 [D loss: 0.456021, acc.: 81.25%] [G loss: 3.940767]\n",
      "epoch:20 step:16088 [D loss: 0.339921, acc.: 82.03%] [G loss: 3.941825]\n",
      "epoch:20 step:16089 [D loss: 0.335334, acc.: 84.38%] [G loss: 2.277246]\n",
      "epoch:20 step:16090 [D loss: 0.317938, acc.: 87.50%] [G loss: 4.506371]\n",
      "epoch:20 step:16091 [D loss: 0.313585, acc.: 86.72%] [G loss: 3.216850]\n",
      "epoch:20 step:16092 [D loss: 0.399246, acc.: 82.81%] [G loss: 3.240820]\n",
      "epoch:20 step:16093 [D loss: 0.352889, acc.: 86.72%] [G loss: 2.659614]\n",
      "epoch:20 step:16094 [D loss: 0.264145, acc.: 89.06%] [G loss: 4.522651]\n",
      "epoch:20 step:16095 [D loss: 0.212079, acc.: 89.84%] [G loss: 4.534196]\n",
      "epoch:20 step:16096 [D loss: 0.263299, acc.: 88.28%] [G loss: 2.266498]\n",
      "epoch:20 step:16097 [D loss: 0.284624, acc.: 85.94%] [G loss: 3.586145]\n",
      "epoch:20 step:16098 [D loss: 0.247071, acc.: 86.72%] [G loss: 4.347643]\n",
      "epoch:20 step:16099 [D loss: 0.454286, acc.: 80.47%] [G loss: 3.063098]\n",
      "epoch:20 step:16100 [D loss: 0.234758, acc.: 90.62%] [G loss: 3.808058]\n",
      "epoch:20 step:16101 [D loss: 0.319436, acc.: 83.59%] [G loss: 3.259368]\n",
      "epoch:20 step:16102 [D loss: 0.351849, acc.: 85.94%] [G loss: 3.079163]\n",
      "epoch:20 step:16103 [D loss: 0.350307, acc.: 84.38%] [G loss: 2.826388]\n",
      "epoch:20 step:16104 [D loss: 0.380165, acc.: 79.69%] [G loss: 3.251088]\n",
      "epoch:20 step:16105 [D loss: 0.378268, acc.: 83.59%] [G loss: 3.844648]\n",
      "epoch:20 step:16106 [D loss: 0.576900, acc.: 74.22%] [G loss: 5.534452]\n",
      "epoch:20 step:16107 [D loss: 0.674317, acc.: 78.12%] [G loss: 8.350599]\n",
      "epoch:20 step:16108 [D loss: 1.354745, acc.: 65.62%] [G loss: 4.527899]\n",
      "epoch:20 step:16109 [D loss: 0.696451, acc.: 77.34%] [G loss: 7.548829]\n",
      "epoch:20 step:16110 [D loss: 0.808912, acc.: 75.78%] [G loss: 3.920264]\n",
      "epoch:20 step:16111 [D loss: 0.353157, acc.: 83.59%] [G loss: 5.453446]\n",
      "epoch:20 step:16112 [D loss: 0.291799, acc.: 86.72%] [G loss: 6.615668]\n",
      "epoch:20 step:16113 [D loss: 0.325121, acc.: 87.50%] [G loss: 4.806493]\n",
      "epoch:20 step:16114 [D loss: 0.173042, acc.: 96.88%] [G loss: 4.576428]\n",
      "epoch:20 step:16115 [D loss: 0.385827, acc.: 82.81%] [G loss: 3.362519]\n",
      "epoch:20 step:16116 [D loss: 0.231876, acc.: 89.84%] [G loss: 3.238114]\n",
      "epoch:20 step:16117 [D loss: 0.367215, acc.: 85.94%] [G loss: 3.241892]\n",
      "epoch:20 step:16118 [D loss: 0.273922, acc.: 88.28%] [G loss: 4.002383]\n",
      "epoch:20 step:16119 [D loss: 0.272708, acc.: 89.06%] [G loss: 3.657160]\n",
      "epoch:20 step:16120 [D loss: 0.295247, acc.: 86.72%] [G loss: 3.720958]\n",
      "epoch:20 step:16121 [D loss: 0.523480, acc.: 74.22%] [G loss: 3.817530]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:20 step:16122 [D loss: 0.328075, acc.: 84.38%] [G loss: 3.827071]\n",
      "epoch:20 step:16123 [D loss: 0.329687, acc.: 85.94%] [G loss: 4.689036]\n",
      "epoch:20 step:16124 [D loss: 0.266646, acc.: 86.72%] [G loss: 2.881500]\n",
      "epoch:20 step:16125 [D loss: 0.263347, acc.: 91.41%] [G loss: 3.062923]\n",
      "epoch:20 step:16126 [D loss: 0.374259, acc.: 82.81%] [G loss: 2.967521]\n",
      "epoch:20 step:16127 [D loss: 0.346592, acc.: 83.59%] [G loss: 2.491319]\n",
      "epoch:20 step:16128 [D loss: 0.284276, acc.: 89.06%] [G loss: 2.825850]\n",
      "epoch:20 step:16129 [D loss: 0.293481, acc.: 89.84%] [G loss: 3.151213]\n",
      "epoch:20 step:16130 [D loss: 0.275844, acc.: 84.38%] [G loss: 3.725614]\n",
      "epoch:20 step:16131 [D loss: 0.425002, acc.: 78.12%] [G loss: 3.171608]\n",
      "epoch:20 step:16132 [D loss: 0.374225, acc.: 78.12%] [G loss: 2.766216]\n",
      "epoch:20 step:16133 [D loss: 0.434432, acc.: 79.69%] [G loss: 3.255325]\n",
      "epoch:20 step:16134 [D loss: 0.362506, acc.: 84.38%] [G loss: 3.353590]\n",
      "epoch:20 step:16135 [D loss: 0.461194, acc.: 76.56%] [G loss: 3.163815]\n",
      "epoch:20 step:16136 [D loss: 0.219649, acc.: 91.41%] [G loss: 4.719839]\n",
      "epoch:20 step:16137 [D loss: 0.335241, acc.: 85.94%] [G loss: 4.771812]\n",
      "epoch:20 step:16138 [D loss: 0.275102, acc.: 88.28%] [G loss: 6.286968]\n",
      "epoch:20 step:16139 [D loss: 0.308031, acc.: 86.72%] [G loss: 4.345993]\n",
      "epoch:20 step:16140 [D loss: 0.370876, acc.: 81.25%] [G loss: 3.435929]\n",
      "epoch:20 step:16141 [D loss: 0.339139, acc.: 83.59%] [G loss: 2.559451]\n",
      "epoch:20 step:16142 [D loss: 0.330990, acc.: 83.59%] [G loss: 5.134420]\n",
      "epoch:20 step:16143 [D loss: 0.304201, acc.: 84.38%] [G loss: 4.783471]\n",
      "epoch:20 step:16144 [D loss: 0.407406, acc.: 78.12%] [G loss: 2.596876]\n",
      "epoch:20 step:16145 [D loss: 0.481617, acc.: 83.59%] [G loss: 9.181434]\n",
      "epoch:20 step:16146 [D loss: 0.714468, acc.: 68.75%] [G loss: 5.689082]\n",
      "epoch:20 step:16147 [D loss: 0.831736, acc.: 67.97%] [G loss: 2.756206]\n",
      "epoch:20 step:16148 [D loss: 0.560292, acc.: 81.25%] [G loss: 4.585723]\n",
      "epoch:20 step:16149 [D loss: 0.534728, acc.: 80.47%] [G loss: 3.996611]\n",
      "epoch:20 step:16150 [D loss: 0.435811, acc.: 86.72%] [G loss: 3.411012]\n",
      "epoch:20 step:16151 [D loss: 0.427478, acc.: 80.47%] [G loss: 3.115767]\n",
      "epoch:20 step:16152 [D loss: 0.376361, acc.: 83.59%] [G loss: 2.955365]\n",
      "epoch:20 step:16153 [D loss: 0.359109, acc.: 84.38%] [G loss: 3.864991]\n",
      "epoch:20 step:16154 [D loss: 0.283287, acc.: 89.84%] [G loss: 3.297809]\n",
      "epoch:20 step:16155 [D loss: 0.320358, acc.: 86.72%] [G loss: 3.371847]\n",
      "epoch:20 step:16156 [D loss: 0.354346, acc.: 84.38%] [G loss: 4.326256]\n",
      "epoch:20 step:16157 [D loss: 0.327189, acc.: 82.03%] [G loss: 4.241565]\n",
      "epoch:20 step:16158 [D loss: 0.293467, acc.: 85.94%] [G loss: 3.978845]\n",
      "epoch:20 step:16159 [D loss: 0.309745, acc.: 90.62%] [G loss: 5.665836]\n",
      "epoch:20 step:16160 [D loss: 0.283044, acc.: 86.72%] [G loss: 2.516270]\n",
      "epoch:20 step:16161 [D loss: 0.385043, acc.: 84.38%] [G loss: 5.290677]\n",
      "epoch:20 step:16162 [D loss: 0.458896, acc.: 80.47%] [G loss: 4.885193]\n",
      "epoch:20 step:16163 [D loss: 0.301450, acc.: 85.94%] [G loss: 5.449895]\n",
      "epoch:20 step:16164 [D loss: 0.398068, acc.: 87.50%] [G loss: 3.704148]\n",
      "epoch:20 step:16165 [D loss: 0.341543, acc.: 83.59%] [G loss: 3.277488]\n",
      "epoch:20 step:16166 [D loss: 0.419404, acc.: 84.38%] [G loss: 3.887808]\n",
      "epoch:20 step:16167 [D loss: 0.238154, acc.: 92.19%] [G loss: 2.970785]\n",
      "epoch:20 step:16168 [D loss: 0.304684, acc.: 84.38%] [G loss: 3.260612]\n",
      "epoch:20 step:16169 [D loss: 0.334774, acc.: 85.94%] [G loss: 2.957392]\n",
      "epoch:20 step:16170 [D loss: 0.282164, acc.: 85.94%] [G loss: 4.410280]\n",
      "epoch:20 step:16171 [D loss: 0.259890, acc.: 88.28%] [G loss: 3.054142]\n",
      "epoch:20 step:16172 [D loss: 0.492637, acc.: 82.81%] [G loss: 2.870883]\n",
      "epoch:20 step:16173 [D loss: 0.312362, acc.: 87.50%] [G loss: 2.748932]\n",
      "epoch:20 step:16174 [D loss: 0.395967, acc.: 80.47%] [G loss: 2.798565]\n",
      "epoch:20 step:16175 [D loss: 0.359141, acc.: 84.38%] [G loss: 3.216114]\n",
      "epoch:20 step:16176 [D loss: 0.386217, acc.: 80.47%] [G loss: 3.947009]\n",
      "epoch:20 step:16177 [D loss: 0.278984, acc.: 85.16%] [G loss: 4.520957]\n",
      "epoch:20 step:16178 [D loss: 0.399255, acc.: 80.47%] [G loss: 3.119523]\n",
      "epoch:20 step:16179 [D loss: 0.332012, acc.: 85.16%] [G loss: 3.194823]\n",
      "epoch:20 step:16180 [D loss: 0.336190, acc.: 84.38%] [G loss: 2.984231]\n",
      "epoch:20 step:16181 [D loss: 0.268441, acc.: 89.06%] [G loss: 4.151938]\n",
      "epoch:20 step:16182 [D loss: 0.381003, acc.: 81.25%] [G loss: 3.496299]\n",
      "epoch:20 step:16183 [D loss: 0.302267, acc.: 85.16%] [G loss: 2.616873]\n",
      "epoch:20 step:16184 [D loss: 0.349596, acc.: 83.59%] [G loss: 3.547373]\n",
      "epoch:20 step:16185 [D loss: 0.272560, acc.: 88.28%] [G loss: 2.637911]\n",
      "epoch:20 step:16186 [D loss: 0.290759, acc.: 89.84%] [G loss: 3.108528]\n",
      "epoch:20 step:16187 [D loss: 0.361923, acc.: 86.72%] [G loss: 3.126558]\n",
      "epoch:20 step:16188 [D loss: 0.383159, acc.: 78.91%] [G loss: 2.822923]\n",
      "epoch:20 step:16189 [D loss: 0.238258, acc.: 89.84%] [G loss: 3.349349]\n",
      "epoch:20 step:16190 [D loss: 0.305931, acc.: 85.94%] [G loss: 3.159072]\n",
      "epoch:20 step:16191 [D loss: 0.397330, acc.: 83.59%] [G loss: 5.411829]\n",
      "epoch:20 step:16192 [D loss: 0.418340, acc.: 83.59%] [G loss: 3.409102]\n",
      "epoch:20 step:16193 [D loss: 0.269775, acc.: 88.28%] [G loss: 4.632032]\n",
      "epoch:20 step:16194 [D loss: 0.343808, acc.: 86.72%] [G loss: 2.794320]\n",
      "epoch:20 step:16195 [D loss: 0.489324, acc.: 81.25%] [G loss: 2.926586]\n",
      "epoch:20 step:16196 [D loss: 0.262205, acc.: 89.84%] [G loss: 3.719835]\n",
      "epoch:20 step:16197 [D loss: 0.364726, acc.: 84.38%] [G loss: 4.359590]\n",
      "epoch:20 step:16198 [D loss: 0.478412, acc.: 82.81%] [G loss: 3.576492]\n",
      "epoch:20 step:16199 [D loss: 0.317808, acc.: 88.28%] [G loss: 4.096771]\n",
      "epoch:20 step:16200 [D loss: 0.247594, acc.: 89.84%] [G loss: 4.717959]\n",
      "##############\n",
      "[0.83674302 0.85223628 0.80344373 0.80820566 0.77904238 0.81472414\n",
      " 0.89039539 0.83938218 0.81443629 0.81365395]\n",
      "##########\n",
      "epoch:20 step:16201 [D loss: 0.237305, acc.: 91.41%] [G loss: 2.760184]\n",
      "epoch:20 step:16202 [D loss: 0.334308, acc.: 85.94%] [G loss: 3.580313]\n",
      "epoch:20 step:16203 [D loss: 0.360244, acc.: 84.38%] [G loss: 3.363778]\n",
      "epoch:20 step:16204 [D loss: 0.376097, acc.: 83.59%] [G loss: 3.399321]\n",
      "epoch:20 step:16205 [D loss: 0.363701, acc.: 86.72%] [G loss: 4.218000]\n",
      "epoch:20 step:16206 [D loss: 0.401987, acc.: 79.69%] [G loss: 3.216698]\n",
      "epoch:20 step:16207 [D loss: 0.347104, acc.: 83.59%] [G loss: 3.757771]\n",
      "epoch:20 step:16208 [D loss: 0.270296, acc.: 92.97%] [G loss: 2.859071]\n",
      "epoch:20 step:16209 [D loss: 0.379055, acc.: 84.38%] [G loss: 3.559224]\n",
      "epoch:20 step:16210 [D loss: 0.371415, acc.: 82.81%] [G loss: 4.284413]\n",
      "epoch:20 step:16211 [D loss: 0.400187, acc.: 79.69%] [G loss: 3.035336]\n",
      "epoch:20 step:16212 [D loss: 0.335939, acc.: 85.94%] [G loss: 3.476615]\n",
      "epoch:20 step:16213 [D loss: 0.290797, acc.: 83.59%] [G loss: 3.857418]\n",
      "epoch:20 step:16214 [D loss: 0.277175, acc.: 89.06%] [G loss: 4.293994]\n",
      "epoch:20 step:16215 [D loss: 0.382621, acc.: 82.81%] [G loss: 3.803154]\n",
      "epoch:20 step:16216 [D loss: 0.450522, acc.: 79.69%] [G loss: 2.988014]\n",
      "epoch:20 step:16217 [D loss: 0.343405, acc.: 83.59%] [G loss: 3.365685]\n",
      "epoch:20 step:16218 [D loss: 0.305829, acc.: 83.59%] [G loss: 3.854506]\n",
      "epoch:20 step:16219 [D loss: 0.269351, acc.: 87.50%] [G loss: 3.620956]\n",
      "epoch:20 step:16220 [D loss: 0.324335, acc.: 85.16%] [G loss: 3.455760]\n",
      "epoch:20 step:16221 [D loss: 0.363869, acc.: 84.38%] [G loss: 3.686635]\n",
      "epoch:20 step:16222 [D loss: 0.397522, acc.: 79.69%] [G loss: 5.057806]\n",
      "epoch:20 step:16223 [D loss: 0.202690, acc.: 91.41%] [G loss: 5.413121]\n",
      "epoch:20 step:16224 [D loss: 0.306262, acc.: 85.94%] [G loss: 3.018481]\n",
      "epoch:20 step:16225 [D loss: 0.312602, acc.: 87.50%] [G loss: 3.167488]\n",
      "epoch:20 step:16226 [D loss: 0.469652, acc.: 75.00%] [G loss: 2.935305]\n",
      "epoch:20 step:16227 [D loss: 0.372475, acc.: 83.59%] [G loss: 3.417440]\n",
      "epoch:20 step:16228 [D loss: 0.339098, acc.: 85.94%] [G loss: 3.331911]\n",
      "epoch:20 step:16229 [D loss: 0.354799, acc.: 88.28%] [G loss: 3.772362]\n",
      "epoch:20 step:16230 [D loss: 0.389948, acc.: 82.81%] [G loss: 2.864090]\n",
      "epoch:20 step:16231 [D loss: 0.345175, acc.: 84.38%] [G loss: 2.863708]\n",
      "epoch:20 step:16232 [D loss: 0.308909, acc.: 85.94%] [G loss: 3.012187]\n",
      "epoch:20 step:16233 [D loss: 0.389860, acc.: 84.38%] [G loss: 3.362007]\n",
      "epoch:20 step:16234 [D loss: 0.249096, acc.: 87.50%] [G loss: 4.248031]\n",
      "epoch:20 step:16235 [D loss: 0.185277, acc.: 92.97%] [G loss: 3.730600]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:20 step:16236 [D loss: 0.298299, acc.: 88.28%] [G loss: 3.394748]\n",
      "epoch:20 step:16237 [D loss: 0.335023, acc.: 82.81%] [G loss: 3.819179]\n",
      "epoch:20 step:16238 [D loss: 0.270760, acc.: 84.38%] [G loss: 4.133998]\n",
      "epoch:20 step:16239 [D loss: 0.357535, acc.: 81.25%] [G loss: 2.666387]\n",
      "epoch:20 step:16240 [D loss: 0.256407, acc.: 90.62%] [G loss: 2.909258]\n",
      "epoch:20 step:16241 [D loss: 0.250589, acc.: 89.06%] [G loss: 3.030164]\n",
      "epoch:20 step:16242 [D loss: 0.253314, acc.: 92.19%] [G loss: 3.002845]\n",
      "epoch:20 step:16243 [D loss: 0.307774, acc.: 85.16%] [G loss: 2.783590]\n",
      "epoch:20 step:16244 [D loss: 0.253134, acc.: 86.72%] [G loss: 3.455685]\n",
      "epoch:20 step:16245 [D loss: 0.383798, acc.: 82.81%] [G loss: 3.527277]\n",
      "epoch:20 step:16246 [D loss: 0.343568, acc.: 84.38%] [G loss: 2.800132]\n",
      "epoch:20 step:16247 [D loss: 0.372426, acc.: 81.25%] [G loss: 3.424706]\n",
      "epoch:20 step:16248 [D loss: 0.451823, acc.: 82.03%] [G loss: 3.314034]\n",
      "epoch:20 step:16249 [D loss: 0.271119, acc.: 86.72%] [G loss: 4.152435]\n",
      "epoch:20 step:16250 [D loss: 0.359082, acc.: 82.81%] [G loss: 4.534627]\n",
      "epoch:20 step:16251 [D loss: 0.236619, acc.: 87.50%] [G loss: 8.474282]\n",
      "epoch:20 step:16252 [D loss: 0.176729, acc.: 91.41%] [G loss: 6.962241]\n",
      "epoch:20 step:16253 [D loss: 0.277932, acc.: 84.38%] [G loss: 4.875939]\n",
      "epoch:20 step:16254 [D loss: 0.193522, acc.: 92.19%] [G loss: 6.149560]\n",
      "epoch:20 step:16255 [D loss: 0.387372, acc.: 84.38%] [G loss: 3.398110]\n",
      "epoch:20 step:16256 [D loss: 0.229757, acc.: 91.41%] [G loss: 2.906680]\n",
      "epoch:20 step:16257 [D loss: 0.261714, acc.: 87.50%] [G loss: 2.678520]\n",
      "epoch:20 step:16258 [D loss: 0.416293, acc.: 84.38%] [G loss: 2.352028]\n",
      "epoch:20 step:16259 [D loss: 0.307903, acc.: 89.84%] [G loss: 2.799716]\n",
      "epoch:20 step:16260 [D loss: 0.260738, acc.: 87.50%] [G loss: 2.737706]\n",
      "epoch:20 step:16261 [D loss: 0.381312, acc.: 85.16%] [G loss: 3.266349]\n",
      "epoch:20 step:16262 [D loss: 0.494161, acc.: 77.34%] [G loss: 3.504305]\n",
      "epoch:20 step:16263 [D loss: 0.268846, acc.: 87.50%] [G loss: 3.124443]\n",
      "epoch:20 step:16264 [D loss: 0.401474, acc.: 80.47%] [G loss: 3.101745]\n",
      "epoch:20 step:16265 [D loss: 0.405623, acc.: 81.25%] [G loss: 3.086421]\n",
      "epoch:20 step:16266 [D loss: 0.331659, acc.: 84.38%] [G loss: 3.345171]\n",
      "epoch:20 step:16267 [D loss: 0.318479, acc.: 85.16%] [G loss: 2.403183]\n",
      "epoch:20 step:16268 [D loss: 0.338569, acc.: 85.16%] [G loss: 2.023501]\n",
      "epoch:20 step:16269 [D loss: 0.370742, acc.: 84.38%] [G loss: 2.056928]\n",
      "epoch:20 step:16270 [D loss: 0.416311, acc.: 78.91%] [G loss: 2.980693]\n",
      "epoch:20 step:16271 [D loss: 0.277286, acc.: 87.50%] [G loss: 3.788076]\n",
      "epoch:20 step:16272 [D loss: 0.339149, acc.: 85.94%] [G loss: 4.750625]\n",
      "epoch:20 step:16273 [D loss: 0.460532, acc.: 78.91%] [G loss: 2.670687]\n",
      "epoch:20 step:16274 [D loss: 0.339678, acc.: 82.03%] [G loss: 3.109766]\n",
      "epoch:20 step:16275 [D loss: 0.274056, acc.: 89.84%] [G loss: 3.230768]\n",
      "epoch:20 step:16276 [D loss: 0.324553, acc.: 84.38%] [G loss: 3.731995]\n",
      "epoch:20 step:16277 [D loss: 0.380383, acc.: 84.38%] [G loss: 2.926978]\n",
      "epoch:20 step:16278 [D loss: 0.250463, acc.: 92.19%] [G loss: 2.663684]\n",
      "epoch:20 step:16279 [D loss: 0.315328, acc.: 89.84%] [G loss: 2.878070]\n",
      "epoch:20 step:16280 [D loss: 0.404612, acc.: 80.47%] [G loss: 3.380415]\n",
      "epoch:20 step:16281 [D loss: 0.309940, acc.: 84.38%] [G loss: 4.229647]\n",
      "epoch:20 step:16282 [D loss: 0.413816, acc.: 80.47%] [G loss: 2.621973]\n",
      "epoch:20 step:16283 [D loss: 0.323305, acc.: 89.84%] [G loss: 4.080961]\n",
      "epoch:20 step:16284 [D loss: 0.329844, acc.: 81.25%] [G loss: 4.053137]\n",
      "epoch:20 step:16285 [D loss: 0.309179, acc.: 88.28%] [G loss: 3.915609]\n",
      "epoch:20 step:16286 [D loss: 0.194210, acc.: 92.19%] [G loss: 4.536693]\n",
      "epoch:20 step:16287 [D loss: 0.265135, acc.: 88.28%] [G loss: 2.947271]\n",
      "epoch:20 step:16288 [D loss: 0.337143, acc.: 80.47%] [G loss: 3.697754]\n",
      "epoch:20 step:16289 [D loss: 0.319170, acc.: 85.16%] [G loss: 3.999259]\n",
      "epoch:20 step:16290 [D loss: 0.391104, acc.: 85.94%] [G loss: 2.542372]\n",
      "epoch:20 step:16291 [D loss: 0.438160, acc.: 82.81%] [G loss: 3.012589]\n",
      "epoch:20 step:16292 [D loss: 0.261480, acc.: 90.62%] [G loss: 3.601453]\n",
      "epoch:20 step:16293 [D loss: 0.394099, acc.: 79.69%] [G loss: 3.367612]\n",
      "epoch:20 step:16294 [D loss: 0.436865, acc.: 79.69%] [G loss: 3.064366]\n",
      "epoch:20 step:16295 [D loss: 0.401361, acc.: 82.03%] [G loss: 3.325266]\n",
      "epoch:20 step:16296 [D loss: 0.259718, acc.: 89.06%] [G loss: 2.217574]\n",
      "epoch:20 step:16297 [D loss: 0.245717, acc.: 88.28%] [G loss: 4.008504]\n",
      "epoch:20 step:16298 [D loss: 0.318761, acc.: 87.50%] [G loss: 3.797500]\n",
      "epoch:20 step:16299 [D loss: 0.327737, acc.: 89.06%] [G loss: 4.283185]\n",
      "epoch:20 step:16300 [D loss: 0.298219, acc.: 88.28%] [G loss: 5.329925]\n",
      "epoch:20 step:16301 [D loss: 0.345117, acc.: 83.59%] [G loss: 4.790960]\n",
      "epoch:20 step:16302 [D loss: 0.416404, acc.: 82.03%] [G loss: 3.023620]\n",
      "epoch:20 step:16303 [D loss: 0.275048, acc.: 88.28%] [G loss: 3.803448]\n",
      "epoch:20 step:16304 [D loss: 0.386230, acc.: 78.12%] [G loss: 3.089936]\n",
      "epoch:20 step:16305 [D loss: 0.315439, acc.: 84.38%] [G loss: 3.212114]\n",
      "epoch:20 step:16306 [D loss: 0.323330, acc.: 89.84%] [G loss: 3.208256]\n",
      "epoch:20 step:16307 [D loss: 0.263622, acc.: 89.06%] [G loss: 3.060786]\n",
      "epoch:20 step:16308 [D loss: 0.299625, acc.: 86.72%] [G loss: 3.418762]\n",
      "epoch:20 step:16309 [D loss: 0.164034, acc.: 93.75%] [G loss: 4.838530]\n",
      "epoch:20 step:16310 [D loss: 0.346543, acc.: 84.38%] [G loss: 3.213455]\n",
      "epoch:20 step:16311 [D loss: 0.264969, acc.: 88.28%] [G loss: 4.246011]\n",
      "epoch:20 step:16312 [D loss: 0.369904, acc.: 85.94%] [G loss: 3.120175]\n",
      "epoch:20 step:16313 [D loss: 0.240239, acc.: 88.28%] [G loss: 2.812076]\n",
      "epoch:20 step:16314 [D loss: 0.268589, acc.: 90.62%] [G loss: 3.864810]\n",
      "epoch:20 step:16315 [D loss: 0.258874, acc.: 86.72%] [G loss: 3.861839]\n",
      "epoch:20 step:16316 [D loss: 0.271449, acc.: 86.72%] [G loss: 3.380626]\n",
      "epoch:20 step:16317 [D loss: 0.299374, acc.: 86.72%] [G loss: 2.757739]\n",
      "epoch:20 step:16318 [D loss: 0.249274, acc.: 91.41%] [G loss: 2.849317]\n",
      "epoch:20 step:16319 [D loss: 0.265941, acc.: 90.62%] [G loss: 2.631299]\n",
      "epoch:20 step:16320 [D loss: 0.371888, acc.: 81.25%] [G loss: 2.226041]\n",
      "epoch:20 step:16321 [D loss: 0.329994, acc.: 82.03%] [G loss: 3.253963]\n",
      "epoch:20 step:16322 [D loss: 0.361777, acc.: 82.81%] [G loss: 2.946815]\n",
      "epoch:20 step:16323 [D loss: 0.401049, acc.: 79.69%] [G loss: 3.236022]\n",
      "epoch:20 step:16324 [D loss: 0.356167, acc.: 82.81%] [G loss: 3.013736]\n",
      "epoch:20 step:16325 [D loss: 0.466980, acc.: 78.91%] [G loss: 4.242719]\n",
      "epoch:20 step:16326 [D loss: 0.603919, acc.: 75.00%] [G loss: 5.956688]\n",
      "epoch:20 step:16327 [D loss: 0.698071, acc.: 69.53%] [G loss: 8.272462]\n",
      "epoch:20 step:16328 [D loss: 1.678726, acc.: 51.56%] [G loss: 9.333612]\n",
      "epoch:20 step:16329 [D loss: 1.978295, acc.: 58.59%] [G loss: 8.051650]\n",
      "epoch:20 step:16330 [D loss: 1.634513, acc.: 64.84%] [G loss: 3.339715]\n",
      "epoch:20 step:16331 [D loss: 0.610954, acc.: 75.78%] [G loss: 3.698214]\n",
      "epoch:20 step:16332 [D loss: 0.302851, acc.: 85.94%] [G loss: 4.399089]\n",
      "epoch:20 step:16333 [D loss: 0.598035, acc.: 73.44%] [G loss: 3.111700]\n",
      "epoch:20 step:16334 [D loss: 0.347187, acc.: 81.25%] [G loss: 5.408227]\n",
      "epoch:20 step:16335 [D loss: 0.459410, acc.: 82.03%] [G loss: 4.333105]\n",
      "epoch:20 step:16336 [D loss: 0.339756, acc.: 84.38%] [G loss: 2.727433]\n",
      "epoch:20 step:16337 [D loss: 0.333219, acc.: 85.94%] [G loss: 3.077560]\n",
      "epoch:20 step:16338 [D loss: 0.397943, acc.: 81.25%] [G loss: 3.142656]\n",
      "epoch:20 step:16339 [D loss: 0.324453, acc.: 90.62%] [G loss: 2.643196]\n",
      "epoch:20 step:16340 [D loss: 0.292631, acc.: 88.28%] [G loss: 2.707588]\n",
      "epoch:20 step:16341 [D loss: 0.421882, acc.: 82.03%] [G loss: 2.763897]\n",
      "epoch:20 step:16342 [D loss: 0.317661, acc.: 84.38%] [G loss: 4.454919]\n",
      "epoch:20 step:16343 [D loss: 0.272769, acc.: 85.94%] [G loss: 4.033149]\n",
      "epoch:20 step:16344 [D loss: 0.381230, acc.: 85.16%] [G loss: 2.613012]\n",
      "epoch:20 step:16345 [D loss: 0.179859, acc.: 92.19%] [G loss: 4.826611]\n",
      "epoch:20 step:16346 [D loss: 0.345514, acc.: 84.38%] [G loss: 3.913212]\n",
      "epoch:20 step:16347 [D loss: 0.340906, acc.: 84.38%] [G loss: 2.981205]\n",
      "epoch:20 step:16348 [D loss: 0.327073, acc.: 83.59%] [G loss: 2.677156]\n",
      "epoch:20 step:16349 [D loss: 0.314096, acc.: 89.06%] [G loss: 2.470452]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:20 step:16350 [D loss: 0.333493, acc.: 84.38%] [G loss: 2.419393]\n",
      "epoch:20 step:16351 [D loss: 0.406537, acc.: 81.25%] [G loss: 2.734411]\n",
      "epoch:20 step:16352 [D loss: 0.398904, acc.: 85.16%] [G loss: 2.258262]\n",
      "epoch:20 step:16353 [D loss: 0.268024, acc.: 89.06%] [G loss: 3.599671]\n",
      "epoch:20 step:16354 [D loss: 0.362780, acc.: 83.59%] [G loss: 3.821405]\n",
      "epoch:20 step:16355 [D loss: 0.261313, acc.: 90.62%] [G loss: 3.776798]\n",
      "epoch:20 step:16356 [D loss: 0.343371, acc.: 83.59%] [G loss: 2.813288]\n",
      "epoch:20 step:16357 [D loss: 0.303905, acc.: 89.06%] [G loss: 2.657215]\n",
      "epoch:20 step:16358 [D loss: 0.307648, acc.: 85.94%] [G loss: 2.526886]\n",
      "epoch:20 step:16359 [D loss: 0.426780, acc.: 79.69%] [G loss: 2.445685]\n",
      "epoch:20 step:16360 [D loss: 0.260165, acc.: 90.62%] [G loss: 2.846458]\n",
      "epoch:20 step:16361 [D loss: 0.359974, acc.: 85.16%] [G loss: 3.072456]\n",
      "epoch:20 step:16362 [D loss: 0.392071, acc.: 81.25%] [G loss: 2.533415]\n",
      "epoch:20 step:16363 [D loss: 0.256348, acc.: 92.19%] [G loss: 2.838894]\n",
      "epoch:20 step:16364 [D loss: 0.328576, acc.: 89.06%] [G loss: 2.620714]\n",
      "epoch:20 step:16365 [D loss: 0.368644, acc.: 85.16%] [G loss: 2.570164]\n",
      "epoch:20 step:16366 [D loss: 0.295925, acc.: 83.59%] [G loss: 3.171825]\n",
      "epoch:20 step:16367 [D loss: 0.412667, acc.: 78.12%] [G loss: 2.237669]\n",
      "epoch:20 step:16368 [D loss: 0.357760, acc.: 85.16%] [G loss: 2.852377]\n",
      "epoch:20 step:16369 [D loss: 0.327966, acc.: 85.94%] [G loss: 2.647536]\n",
      "epoch:20 step:16370 [D loss: 0.376082, acc.: 85.94%] [G loss: 2.961459]\n",
      "epoch:20 step:16371 [D loss: 0.206287, acc.: 92.97%] [G loss: 2.554564]\n",
      "epoch:20 step:16372 [D loss: 0.272712, acc.: 90.62%] [G loss: 2.412945]\n",
      "epoch:20 step:16373 [D loss: 0.303416, acc.: 87.50%] [G loss: 3.268874]\n",
      "epoch:20 step:16374 [D loss: 0.385720, acc.: 80.47%] [G loss: 2.659125]\n",
      "epoch:20 step:16375 [D loss: 0.253870, acc.: 92.19%] [G loss: 2.901763]\n",
      "epoch:20 step:16376 [D loss: 0.292149, acc.: 85.94%] [G loss: 2.178586]\n",
      "epoch:20 step:16377 [D loss: 0.392322, acc.: 77.34%] [G loss: 2.229585]\n",
      "epoch:20 step:16378 [D loss: 0.455240, acc.: 82.03%] [G loss: 2.426310]\n",
      "epoch:20 step:16379 [D loss: 0.310409, acc.: 89.84%] [G loss: 3.300070]\n",
      "epoch:20 step:16380 [D loss: 0.309146, acc.: 89.84%] [G loss: 3.746738]\n",
      "epoch:20 step:16381 [D loss: 0.292623, acc.: 89.84%] [G loss: 2.703612]\n",
      "epoch:20 step:16382 [D loss: 0.392000, acc.: 79.69%] [G loss: 3.754528]\n",
      "epoch:20 step:16383 [D loss: 0.306473, acc.: 85.94%] [G loss: 2.794082]\n",
      "epoch:20 step:16384 [D loss: 0.359328, acc.: 84.38%] [G loss: 2.921124]\n",
      "epoch:20 step:16385 [D loss: 0.406284, acc.: 81.25%] [G loss: 2.254477]\n",
      "epoch:20 step:16386 [D loss: 0.351469, acc.: 82.03%] [G loss: 2.369369]\n",
      "epoch:20 step:16387 [D loss: 0.316433, acc.: 86.72%] [G loss: 2.314313]\n",
      "epoch:20 step:16388 [D loss: 0.322088, acc.: 86.72%] [G loss: 3.077257]\n",
      "epoch:20 step:16389 [D loss: 0.281794, acc.: 88.28%] [G loss: 2.461570]\n",
      "epoch:20 step:16390 [D loss: 0.353979, acc.: 83.59%] [G loss: 2.747957]\n",
      "epoch:20 step:16391 [D loss: 0.392572, acc.: 83.59%] [G loss: 2.310701]\n",
      "epoch:20 step:16392 [D loss: 0.366317, acc.: 82.81%] [G loss: 2.371137]\n",
      "epoch:20 step:16393 [D loss: 0.321552, acc.: 85.16%] [G loss: 2.792220]\n",
      "epoch:20 step:16394 [D loss: 0.322780, acc.: 87.50%] [G loss: 2.891634]\n",
      "epoch:20 step:16395 [D loss: 0.387372, acc.: 81.25%] [G loss: 2.183531]\n",
      "epoch:20 step:16396 [D loss: 0.293567, acc.: 86.72%] [G loss: 2.575202]\n",
      "epoch:20 step:16397 [D loss: 0.427208, acc.: 77.34%] [G loss: 2.666130]\n",
      "epoch:20 step:16398 [D loss: 0.356383, acc.: 81.25%] [G loss: 2.762895]\n",
      "epoch:20 step:16399 [D loss: 0.264602, acc.: 90.62%] [G loss: 3.379611]\n",
      "epoch:20 step:16400 [D loss: 0.333199, acc.: 87.50%] [G loss: 2.837177]\n",
      "##############\n",
      "[0.84724132 0.86273896 0.82077447 0.79562146 0.76832608 0.81685261\n",
      " 0.86510441 0.84849635 0.83579665 0.84031355]\n",
      "##########\n",
      "epoch:20 step:16401 [D loss: 0.345120, acc.: 83.59%] [G loss: 3.512100]\n",
      "epoch:21 step:16402 [D loss: 0.327762, acc.: 85.94%] [G loss: 3.213698]\n",
      "epoch:21 step:16403 [D loss: 0.311986, acc.: 86.72%] [G loss: 4.184036]\n",
      "epoch:21 step:16404 [D loss: 0.301535, acc.: 84.38%] [G loss: 2.257903]\n",
      "epoch:21 step:16405 [D loss: 0.312311, acc.: 86.72%] [G loss: 3.234436]\n",
      "epoch:21 step:16406 [D loss: 0.310765, acc.: 87.50%] [G loss: 3.754988]\n",
      "epoch:21 step:16407 [D loss: 0.321314, acc.: 84.38%] [G loss: 3.132424]\n",
      "epoch:21 step:16408 [D loss: 0.263016, acc.: 89.06%] [G loss: 2.920448]\n",
      "epoch:21 step:16409 [D loss: 0.303577, acc.: 85.94%] [G loss: 3.970674]\n",
      "epoch:21 step:16410 [D loss: 0.286206, acc.: 85.94%] [G loss: 5.813514]\n",
      "epoch:21 step:16411 [D loss: 0.253903, acc.: 90.62%] [G loss: 5.365107]\n",
      "epoch:21 step:16412 [D loss: 0.300745, acc.: 84.38%] [G loss: 3.094243]\n",
      "epoch:21 step:16413 [D loss: 0.299782, acc.: 85.16%] [G loss: 3.363552]\n",
      "epoch:21 step:16414 [D loss: 0.312561, acc.: 84.38%] [G loss: 2.454895]\n",
      "epoch:21 step:16415 [D loss: 0.263932, acc.: 89.84%] [G loss: 4.011454]\n",
      "epoch:21 step:16416 [D loss: 0.454476, acc.: 81.25%] [G loss: 3.436055]\n",
      "epoch:21 step:16417 [D loss: 0.279004, acc.: 89.06%] [G loss: 3.527572]\n",
      "epoch:21 step:16418 [D loss: 0.313401, acc.: 89.06%] [G loss: 3.252050]\n",
      "epoch:21 step:16419 [D loss: 0.350487, acc.: 81.25%] [G loss: 2.879794]\n",
      "epoch:21 step:16420 [D loss: 0.264699, acc.: 89.84%] [G loss: 2.979900]\n",
      "epoch:21 step:16421 [D loss: 0.330217, acc.: 85.94%] [G loss: 3.151169]\n",
      "epoch:21 step:16422 [D loss: 0.341706, acc.: 82.81%] [G loss: 2.487395]\n",
      "epoch:21 step:16423 [D loss: 0.369205, acc.: 81.25%] [G loss: 2.714992]\n",
      "epoch:21 step:16424 [D loss: 0.419771, acc.: 78.12%] [G loss: 2.239677]\n",
      "epoch:21 step:16425 [D loss: 0.485417, acc.: 82.03%] [G loss: 3.662447]\n",
      "epoch:21 step:16426 [D loss: 0.314281, acc.: 87.50%] [G loss: 6.329195]\n",
      "epoch:21 step:16427 [D loss: 0.267993, acc.: 92.97%] [G loss: 3.907549]\n",
      "epoch:21 step:16428 [D loss: 0.410905, acc.: 83.59%] [G loss: 6.406918]\n",
      "epoch:21 step:16429 [D loss: 0.559650, acc.: 71.88%] [G loss: 4.203304]\n",
      "epoch:21 step:16430 [D loss: 0.385160, acc.: 84.38%] [G loss: 3.507153]\n",
      "epoch:21 step:16431 [D loss: 0.317393, acc.: 84.38%] [G loss: 3.809512]\n",
      "epoch:21 step:16432 [D loss: 0.225303, acc.: 89.06%] [G loss: 4.505356]\n",
      "epoch:21 step:16433 [D loss: 0.268277, acc.: 89.84%] [G loss: 2.852341]\n",
      "epoch:21 step:16434 [D loss: 0.240863, acc.: 87.50%] [G loss: 2.765640]\n",
      "epoch:21 step:16435 [D loss: 0.470678, acc.: 78.12%] [G loss: 4.403093]\n",
      "epoch:21 step:16436 [D loss: 0.378462, acc.: 82.03%] [G loss: 4.047664]\n",
      "epoch:21 step:16437 [D loss: 0.238898, acc.: 91.41%] [G loss: 3.918486]\n",
      "epoch:21 step:16438 [D loss: 0.387437, acc.: 83.59%] [G loss: 3.439018]\n",
      "epoch:21 step:16439 [D loss: 0.294863, acc.: 85.94%] [G loss: 4.094916]\n",
      "epoch:21 step:16440 [D loss: 0.246781, acc.: 89.06%] [G loss: 3.854387]\n",
      "epoch:21 step:16441 [D loss: 0.403240, acc.: 81.25%] [G loss: 2.945397]\n",
      "epoch:21 step:16442 [D loss: 0.494654, acc.: 77.34%] [G loss: 2.812158]\n",
      "epoch:21 step:16443 [D loss: 0.327279, acc.: 84.38%] [G loss: 3.443902]\n",
      "epoch:21 step:16444 [D loss: 0.399841, acc.: 82.03%] [G loss: 3.203633]\n",
      "epoch:21 step:16445 [D loss: 0.358206, acc.: 82.03%] [G loss: 5.730913]\n",
      "epoch:21 step:16446 [D loss: 0.508054, acc.: 78.12%] [G loss: 2.860823]\n",
      "epoch:21 step:16447 [D loss: 0.350997, acc.: 85.16%] [G loss: 6.456048]\n",
      "epoch:21 step:16448 [D loss: 0.446608, acc.: 75.00%] [G loss: 5.527436]\n",
      "epoch:21 step:16449 [D loss: 0.226515, acc.: 89.84%] [G loss: 4.526203]\n",
      "epoch:21 step:16450 [D loss: 0.289594, acc.: 89.06%] [G loss: 3.615844]\n",
      "epoch:21 step:16451 [D loss: 0.480010, acc.: 75.00%] [G loss: 3.733949]\n",
      "epoch:21 step:16452 [D loss: 0.264051, acc.: 92.19%] [G loss: 2.925062]\n",
      "epoch:21 step:16453 [D loss: 0.247478, acc.: 88.28%] [G loss: 2.967424]\n",
      "epoch:21 step:16454 [D loss: 0.254802, acc.: 89.06%] [G loss: 3.292867]\n",
      "epoch:21 step:16455 [D loss: 0.333296, acc.: 86.72%] [G loss: 2.737577]\n",
      "epoch:21 step:16456 [D loss: 0.370890, acc.: 81.25%] [G loss: 3.125276]\n",
      "epoch:21 step:16457 [D loss: 0.287535, acc.: 87.50%] [G loss: 2.905144]\n",
      "epoch:21 step:16458 [D loss: 0.333290, acc.: 82.81%] [G loss: 2.681386]\n",
      "epoch:21 step:16459 [D loss: 0.298791, acc.: 87.50%] [G loss: 3.385661]\n",
      "epoch:21 step:16460 [D loss: 0.416340, acc.: 77.34%] [G loss: 3.020622]\n",
      "epoch:21 step:16461 [D loss: 0.448982, acc.: 79.69%] [G loss: 2.627646]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:21 step:16462 [D loss: 0.463756, acc.: 78.12%] [G loss: 3.010445]\n",
      "epoch:21 step:16463 [D loss: 0.304968, acc.: 92.19%] [G loss: 3.193437]\n",
      "epoch:21 step:16464 [D loss: 0.295191, acc.: 88.28%] [G loss: 3.651884]\n",
      "epoch:21 step:16465 [D loss: 0.372567, acc.: 80.47%] [G loss: 3.225505]\n",
      "epoch:21 step:16466 [D loss: 0.434579, acc.: 78.91%] [G loss: 2.390923]\n",
      "epoch:21 step:16467 [D loss: 0.289638, acc.: 87.50%] [G loss: 3.004278]\n",
      "epoch:21 step:16468 [D loss: 0.324707, acc.: 83.59%] [G loss: 3.183463]\n",
      "epoch:21 step:16469 [D loss: 0.304370, acc.: 86.72%] [G loss: 2.942396]\n",
      "epoch:21 step:16470 [D loss: 0.409173, acc.: 78.91%] [G loss: 3.681604]\n",
      "epoch:21 step:16471 [D loss: 0.304263, acc.: 89.06%] [G loss: 3.614449]\n",
      "epoch:21 step:16472 [D loss: 0.332448, acc.: 83.59%] [G loss: 2.915877]\n",
      "epoch:21 step:16473 [D loss: 0.300932, acc.: 86.72%] [G loss: 5.257779]\n",
      "epoch:21 step:16474 [D loss: 0.311342, acc.: 85.94%] [G loss: 3.541821]\n",
      "epoch:21 step:16475 [D loss: 0.358752, acc.: 83.59%] [G loss: 4.132653]\n",
      "epoch:21 step:16476 [D loss: 0.233221, acc.: 89.84%] [G loss: 5.865473]\n",
      "epoch:21 step:16477 [D loss: 0.341574, acc.: 82.81%] [G loss: 4.146020]\n",
      "epoch:21 step:16478 [D loss: 0.315765, acc.: 84.38%] [G loss: 5.567319]\n",
      "epoch:21 step:16479 [D loss: 0.222941, acc.: 91.41%] [G loss: 2.965127]\n",
      "epoch:21 step:16480 [D loss: 0.230456, acc.: 89.06%] [G loss: 3.721429]\n",
      "epoch:21 step:16481 [D loss: 0.397853, acc.: 81.25%] [G loss: 4.184277]\n",
      "epoch:21 step:16482 [D loss: 0.379253, acc.: 81.25%] [G loss: 5.619557]\n",
      "epoch:21 step:16483 [D loss: 0.450195, acc.: 84.38%] [G loss: 2.605415]\n",
      "epoch:21 step:16484 [D loss: 0.455524, acc.: 85.94%] [G loss: 2.411953]\n",
      "epoch:21 step:16485 [D loss: 0.381483, acc.: 80.47%] [G loss: 2.565270]\n",
      "epoch:21 step:16486 [D loss: 0.223956, acc.: 95.31%] [G loss: 2.528295]\n",
      "epoch:21 step:16487 [D loss: 0.460821, acc.: 80.47%] [G loss: 3.914432]\n",
      "epoch:21 step:16488 [D loss: 0.246567, acc.: 85.94%] [G loss: 4.814465]\n",
      "epoch:21 step:16489 [D loss: 0.275356, acc.: 88.28%] [G loss: 3.264245]\n",
      "epoch:21 step:16490 [D loss: 0.334037, acc.: 84.38%] [G loss: 4.581804]\n",
      "epoch:21 step:16491 [D loss: 0.386610, acc.: 82.03%] [G loss: 3.456412]\n",
      "epoch:21 step:16492 [D loss: 0.303645, acc.: 85.94%] [G loss: 2.203877]\n",
      "epoch:21 step:16493 [D loss: 0.348818, acc.: 83.59%] [G loss: 5.119628]\n",
      "epoch:21 step:16494 [D loss: 0.283173, acc.: 86.72%] [G loss: 5.842476]\n",
      "epoch:21 step:16495 [D loss: 0.253439, acc.: 89.84%] [G loss: 3.665624]\n",
      "epoch:21 step:16496 [D loss: 0.260238, acc.: 89.06%] [G loss: 3.282724]\n",
      "epoch:21 step:16497 [D loss: 0.288152, acc.: 89.84%] [G loss: 2.889421]\n",
      "epoch:21 step:16498 [D loss: 0.261302, acc.: 91.41%] [G loss: 3.264688]\n",
      "epoch:21 step:16499 [D loss: 0.293948, acc.: 91.41%] [G loss: 3.041057]\n",
      "epoch:21 step:16500 [D loss: 0.345566, acc.: 83.59%] [G loss: 3.226515]\n",
      "epoch:21 step:16501 [D loss: 0.314875, acc.: 85.16%] [G loss: 3.182206]\n",
      "epoch:21 step:16502 [D loss: 0.431028, acc.: 77.34%] [G loss: 2.899352]\n",
      "epoch:21 step:16503 [D loss: 0.390717, acc.: 85.94%] [G loss: 2.702646]\n",
      "epoch:21 step:16504 [D loss: 0.371477, acc.: 84.38%] [G loss: 2.631770]\n",
      "epoch:21 step:16505 [D loss: 0.404331, acc.: 80.47%] [G loss: 2.193002]\n",
      "epoch:21 step:16506 [D loss: 0.341923, acc.: 85.16%] [G loss: 2.850947]\n",
      "epoch:21 step:16507 [D loss: 0.333527, acc.: 84.38%] [G loss: 2.461316]\n",
      "epoch:21 step:16508 [D loss: 0.371075, acc.: 82.03%] [G loss: 2.422529]\n",
      "epoch:21 step:16509 [D loss: 0.396088, acc.: 82.81%] [G loss: 2.408504]\n",
      "epoch:21 step:16510 [D loss: 0.349003, acc.: 83.59%] [G loss: 2.653534]\n",
      "epoch:21 step:16511 [D loss: 0.412150, acc.: 85.16%] [G loss: 2.829988]\n",
      "epoch:21 step:16512 [D loss: 0.427136, acc.: 79.69%] [G loss: 3.276202]\n",
      "epoch:21 step:16513 [D loss: 0.219689, acc.: 91.41%] [G loss: 3.286294]\n",
      "epoch:21 step:16514 [D loss: 0.346335, acc.: 86.72%] [G loss: 3.985666]\n",
      "epoch:21 step:16515 [D loss: 0.412393, acc.: 82.81%] [G loss: 2.829821]\n",
      "epoch:21 step:16516 [D loss: 0.314364, acc.: 89.06%] [G loss: 2.424423]\n",
      "epoch:21 step:16517 [D loss: 0.256358, acc.: 89.84%] [G loss: 3.047399]\n",
      "epoch:21 step:16518 [D loss: 0.317446, acc.: 85.16%] [G loss: 2.703445]\n",
      "epoch:21 step:16519 [D loss: 0.403218, acc.: 82.81%] [G loss: 4.061328]\n",
      "epoch:21 step:16520 [D loss: 0.353974, acc.: 82.81%] [G loss: 2.689202]\n",
      "epoch:21 step:16521 [D loss: 0.349989, acc.: 85.16%] [G loss: 3.021397]\n",
      "epoch:21 step:16522 [D loss: 0.321554, acc.: 89.06%] [G loss: 3.406690]\n",
      "epoch:21 step:16523 [D loss: 0.328228, acc.: 87.50%] [G loss: 3.540559]\n",
      "epoch:21 step:16524 [D loss: 0.314751, acc.: 86.72%] [G loss: 3.138146]\n",
      "epoch:21 step:16525 [D loss: 0.355195, acc.: 85.16%] [G loss: 2.110205]\n",
      "epoch:21 step:16526 [D loss: 0.352333, acc.: 85.16%] [G loss: 3.301012]\n",
      "epoch:21 step:16527 [D loss: 0.369444, acc.: 84.38%] [G loss: 3.006162]\n",
      "epoch:21 step:16528 [D loss: 0.321797, acc.: 85.16%] [G loss: 3.626318]\n",
      "epoch:21 step:16529 [D loss: 0.322166, acc.: 89.06%] [G loss: 2.391519]\n",
      "epoch:21 step:16530 [D loss: 0.250300, acc.: 87.50%] [G loss: 3.566154]\n",
      "epoch:21 step:16531 [D loss: 0.340621, acc.: 82.03%] [G loss: 3.972402]\n",
      "epoch:21 step:16532 [D loss: 0.339247, acc.: 83.59%] [G loss: 3.310840]\n",
      "epoch:21 step:16533 [D loss: 0.317628, acc.: 85.16%] [G loss: 3.690622]\n",
      "epoch:21 step:16534 [D loss: 0.361058, acc.: 83.59%] [G loss: 2.704080]\n",
      "epoch:21 step:16535 [D loss: 0.300366, acc.: 86.72%] [G loss: 2.832290]\n",
      "epoch:21 step:16536 [D loss: 0.289719, acc.: 86.72%] [G loss: 3.792049]\n",
      "epoch:21 step:16537 [D loss: 0.196810, acc.: 92.97%] [G loss: 4.619585]\n",
      "epoch:21 step:16538 [D loss: 0.408440, acc.: 82.81%] [G loss: 5.068888]\n",
      "epoch:21 step:16539 [D loss: 0.302066, acc.: 86.72%] [G loss: 4.066932]\n",
      "epoch:21 step:16540 [D loss: 0.248039, acc.: 90.62%] [G loss: 2.859907]\n",
      "epoch:21 step:16541 [D loss: 0.389492, acc.: 84.38%] [G loss: 3.141482]\n",
      "epoch:21 step:16542 [D loss: 0.224436, acc.: 91.41%] [G loss: 3.769035]\n",
      "epoch:21 step:16543 [D loss: 0.341272, acc.: 83.59%] [G loss: 3.304517]\n",
      "epoch:21 step:16544 [D loss: 0.303705, acc.: 84.38%] [G loss: 3.549014]\n",
      "epoch:21 step:16545 [D loss: 0.302853, acc.: 85.16%] [G loss: 2.929668]\n",
      "epoch:21 step:16546 [D loss: 0.262167, acc.: 90.62%] [G loss: 2.688825]\n",
      "epoch:21 step:16547 [D loss: 0.273443, acc.: 86.72%] [G loss: 3.422688]\n",
      "epoch:21 step:16548 [D loss: 0.286567, acc.: 86.72%] [G loss: 3.783236]\n",
      "epoch:21 step:16549 [D loss: 0.394108, acc.: 81.25%] [G loss: 3.912055]\n",
      "epoch:21 step:16550 [D loss: 0.359298, acc.: 82.81%] [G loss: 5.524622]\n",
      "epoch:21 step:16551 [D loss: 0.287018, acc.: 85.94%] [G loss: 2.960228]\n",
      "epoch:21 step:16552 [D loss: 0.452020, acc.: 85.16%] [G loss: 4.015409]\n",
      "epoch:21 step:16553 [D loss: 0.387091, acc.: 82.81%] [G loss: 5.799901]\n",
      "epoch:21 step:16554 [D loss: 0.388803, acc.: 85.16%] [G loss: 3.906770]\n",
      "epoch:21 step:16555 [D loss: 0.239895, acc.: 89.06%] [G loss: 3.988411]\n",
      "epoch:21 step:16556 [D loss: 0.313246, acc.: 89.84%] [G loss: 3.440428]\n",
      "epoch:21 step:16557 [D loss: 0.278721, acc.: 85.16%] [G loss: 3.628690]\n",
      "epoch:21 step:16558 [D loss: 0.449895, acc.: 82.81%] [G loss: 3.089553]\n",
      "epoch:21 step:16559 [D loss: 0.430501, acc.: 78.91%] [G loss: 2.842793]\n",
      "epoch:21 step:16560 [D loss: 0.302976, acc.: 85.94%] [G loss: 2.621702]\n",
      "epoch:21 step:16561 [D loss: 0.480793, acc.: 76.56%] [G loss: 3.386673]\n",
      "epoch:21 step:16562 [D loss: 0.534861, acc.: 75.00%] [G loss: 4.975448]\n",
      "epoch:21 step:16563 [D loss: 0.570532, acc.: 78.91%] [G loss: 3.993873]\n",
      "epoch:21 step:16564 [D loss: 0.335191, acc.: 86.72%] [G loss: 2.473825]\n",
      "epoch:21 step:16565 [D loss: 0.297629, acc.: 88.28%] [G loss: 3.841211]\n",
      "epoch:21 step:16566 [D loss: 0.363924, acc.: 85.16%] [G loss: 2.538913]\n",
      "epoch:21 step:16567 [D loss: 0.288675, acc.: 87.50%] [G loss: 3.668020]\n",
      "epoch:21 step:16568 [D loss: 0.323291, acc.: 84.38%] [G loss: 3.721709]\n",
      "epoch:21 step:16569 [D loss: 0.348383, acc.: 85.94%] [G loss: 2.241708]\n",
      "epoch:21 step:16570 [D loss: 0.259904, acc.: 89.84%] [G loss: 2.959529]\n",
      "epoch:21 step:16571 [D loss: 0.372089, acc.: 82.03%] [G loss: 3.218703]\n",
      "epoch:21 step:16572 [D loss: 0.327659, acc.: 88.28%] [G loss: 2.781725]\n",
      "epoch:21 step:16573 [D loss: 0.399150, acc.: 82.81%] [G loss: 2.719637]\n",
      "epoch:21 step:16574 [D loss: 0.337659, acc.: 85.16%] [G loss: 3.077034]\n",
      "epoch:21 step:16575 [D loss: 0.355634, acc.: 83.59%] [G loss: 2.700994]\n",
      "epoch:21 step:16576 [D loss: 0.353837, acc.: 88.28%] [G loss: 3.769679]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:21 step:16577 [D loss: 0.221395, acc.: 90.62%] [G loss: 4.980329]\n",
      "epoch:21 step:16578 [D loss: 0.354055, acc.: 85.16%] [G loss: 2.849732]\n",
      "epoch:21 step:16579 [D loss: 0.323058, acc.: 85.16%] [G loss: 5.012539]\n",
      "epoch:21 step:16580 [D loss: 0.264735, acc.: 90.62%] [G loss: 3.262842]\n",
      "epoch:21 step:16581 [D loss: 0.311215, acc.: 85.16%] [G loss: 4.389673]\n",
      "epoch:21 step:16582 [D loss: 0.317721, acc.: 87.50%] [G loss: 3.635247]\n",
      "epoch:21 step:16583 [D loss: 0.345230, acc.: 88.28%] [G loss: 3.877860]\n",
      "epoch:21 step:16584 [D loss: 0.323615, acc.: 84.38%] [G loss: 3.313365]\n",
      "epoch:21 step:16585 [D loss: 0.309795, acc.: 85.94%] [G loss: 3.806758]\n",
      "epoch:21 step:16586 [D loss: 0.403768, acc.: 83.59%] [G loss: 3.269262]\n",
      "epoch:21 step:16587 [D loss: 0.273295, acc.: 91.41%] [G loss: 3.688671]\n",
      "epoch:21 step:16588 [D loss: 0.340156, acc.: 83.59%] [G loss: 2.687524]\n",
      "epoch:21 step:16589 [D loss: 0.369382, acc.: 85.94%] [G loss: 4.958395]\n",
      "epoch:21 step:16590 [D loss: 0.296501, acc.: 87.50%] [G loss: 5.212482]\n",
      "epoch:21 step:16591 [D loss: 0.441319, acc.: 80.47%] [G loss: 3.278693]\n",
      "epoch:21 step:16592 [D loss: 0.318196, acc.: 86.72%] [G loss: 3.268296]\n",
      "epoch:21 step:16593 [D loss: 0.324772, acc.: 85.16%] [G loss: 5.102787]\n",
      "epoch:21 step:16594 [D loss: 0.330281, acc.: 85.16%] [G loss: 4.926563]\n",
      "epoch:21 step:16595 [D loss: 0.219241, acc.: 90.62%] [G loss: 3.819483]\n",
      "epoch:21 step:16596 [D loss: 0.376846, acc.: 83.59%] [G loss: 3.922596]\n",
      "epoch:21 step:16597 [D loss: 0.340282, acc.: 85.16%] [G loss: 3.435749]\n",
      "epoch:21 step:16598 [D loss: 0.314297, acc.: 85.16%] [G loss: 3.522496]\n",
      "epoch:21 step:16599 [D loss: 0.249293, acc.: 90.62%] [G loss: 2.953894]\n",
      "epoch:21 step:16600 [D loss: 0.391274, acc.: 81.25%] [G loss: 2.642155]\n",
      "##############\n",
      "[0.84744757 0.83694954 0.82218266 0.79106385 0.77629658 0.81433914\n",
      " 0.85827755 0.83380461 0.79660158 0.82940572]\n",
      "##########\n",
      "epoch:21 step:16601 [D loss: 0.379875, acc.: 82.03%] [G loss: 2.352448]\n",
      "epoch:21 step:16602 [D loss: 0.367160, acc.: 82.03%] [G loss: 2.962449]\n",
      "epoch:21 step:16603 [D loss: 0.458235, acc.: 80.47%] [G loss: 4.471460]\n",
      "epoch:21 step:16604 [D loss: 0.315116, acc.: 85.16%] [G loss: 3.562746]\n",
      "epoch:21 step:16605 [D loss: 0.347009, acc.: 83.59%] [G loss: 5.320033]\n",
      "epoch:21 step:16606 [D loss: 0.337812, acc.: 84.38%] [G loss: 2.586662]\n",
      "epoch:21 step:16607 [D loss: 0.284701, acc.: 87.50%] [G loss: 4.135273]\n",
      "epoch:21 step:16608 [D loss: 0.289315, acc.: 88.28%] [G loss: 4.169781]\n",
      "epoch:21 step:16609 [D loss: 0.536437, acc.: 75.00%] [G loss: 4.925447]\n",
      "epoch:21 step:16610 [D loss: 0.651173, acc.: 75.00%] [G loss: 6.198745]\n",
      "epoch:21 step:16611 [D loss: 0.564410, acc.: 78.12%] [G loss: 4.734056]\n",
      "epoch:21 step:16612 [D loss: 0.324325, acc.: 84.38%] [G loss: 6.368233]\n",
      "epoch:21 step:16613 [D loss: 0.319164, acc.: 87.50%] [G loss: 6.643536]\n",
      "epoch:21 step:16614 [D loss: 0.282892, acc.: 85.94%] [G loss: 5.559906]\n",
      "epoch:21 step:16615 [D loss: 0.292639, acc.: 89.06%] [G loss: 4.377528]\n",
      "epoch:21 step:16616 [D loss: 0.371969, acc.: 84.38%] [G loss: 5.161687]\n",
      "epoch:21 step:16617 [D loss: 0.320360, acc.: 83.59%] [G loss: 3.584699]\n",
      "epoch:21 step:16618 [D loss: 0.278397, acc.: 88.28%] [G loss: 3.213440]\n",
      "epoch:21 step:16619 [D loss: 0.460446, acc.: 80.47%] [G loss: 3.073384]\n",
      "epoch:21 step:16620 [D loss: 0.322551, acc.: 83.59%] [G loss: 2.798975]\n",
      "epoch:21 step:16621 [D loss: 0.353697, acc.: 85.16%] [G loss: 3.526207]\n",
      "epoch:21 step:16622 [D loss: 0.300283, acc.: 86.72%] [G loss: 4.906733]\n",
      "epoch:21 step:16623 [D loss: 0.341234, acc.: 84.38%] [G loss: 2.393050]\n",
      "epoch:21 step:16624 [D loss: 0.348309, acc.: 82.03%] [G loss: 4.238679]\n",
      "epoch:21 step:16625 [D loss: 0.279262, acc.: 87.50%] [G loss: 4.412275]\n",
      "epoch:21 step:16626 [D loss: 0.290967, acc.: 89.84%] [G loss: 2.490661]\n",
      "epoch:21 step:16627 [D loss: 0.368972, acc.: 84.38%] [G loss: 3.939681]\n",
      "epoch:21 step:16628 [D loss: 0.346790, acc.: 85.16%] [G loss: 3.176313]\n",
      "epoch:21 step:16629 [D loss: 0.445161, acc.: 80.47%] [G loss: 2.633076]\n",
      "epoch:21 step:16630 [D loss: 0.345953, acc.: 82.03%] [G loss: 3.990877]\n",
      "epoch:21 step:16631 [D loss: 0.322392, acc.: 85.16%] [G loss: 3.698207]\n",
      "epoch:21 step:16632 [D loss: 0.286172, acc.: 87.50%] [G loss: 2.984108]\n",
      "epoch:21 step:16633 [D loss: 0.327357, acc.: 87.50%] [G loss: 3.093340]\n",
      "epoch:21 step:16634 [D loss: 0.312133, acc.: 89.06%] [G loss: 3.774454]\n",
      "epoch:21 step:16635 [D loss: 0.400933, acc.: 84.38%] [G loss: 2.468783]\n",
      "epoch:21 step:16636 [D loss: 0.265742, acc.: 91.41%] [G loss: 2.562990]\n",
      "epoch:21 step:16637 [D loss: 0.362815, acc.: 83.59%] [G loss: 2.484549]\n",
      "epoch:21 step:16638 [D loss: 0.255728, acc.: 89.84%] [G loss: 3.221689]\n",
      "epoch:21 step:16639 [D loss: 0.298528, acc.: 83.59%] [G loss: 2.612255]\n",
      "epoch:21 step:16640 [D loss: 0.242721, acc.: 86.72%] [G loss: 2.936234]\n",
      "epoch:21 step:16641 [D loss: 0.407049, acc.: 82.03%] [G loss: 3.526533]\n",
      "epoch:21 step:16642 [D loss: 0.313003, acc.: 85.94%] [G loss: 3.464121]\n",
      "epoch:21 step:16643 [D loss: 0.317113, acc.: 85.94%] [G loss: 3.003526]\n",
      "epoch:21 step:16644 [D loss: 0.199837, acc.: 90.62%] [G loss: 6.017969]\n",
      "epoch:21 step:16645 [D loss: 0.336734, acc.: 84.38%] [G loss: 3.597723]\n",
      "epoch:21 step:16646 [D loss: 0.249531, acc.: 89.84%] [G loss: 2.775476]\n",
      "epoch:21 step:16647 [D loss: 0.281602, acc.: 84.38%] [G loss: 3.378799]\n",
      "epoch:21 step:16648 [D loss: 0.372631, acc.: 82.03%] [G loss: 3.675275]\n",
      "epoch:21 step:16649 [D loss: 0.275354, acc.: 89.06%] [G loss: 2.682923]\n",
      "epoch:21 step:16650 [D loss: 0.278431, acc.: 89.84%] [G loss: 3.243167]\n",
      "epoch:21 step:16651 [D loss: 0.320357, acc.: 86.72%] [G loss: 2.619871]\n",
      "epoch:21 step:16652 [D loss: 0.364997, acc.: 86.72%] [G loss: 3.363202]\n",
      "epoch:21 step:16653 [D loss: 0.299486, acc.: 85.94%] [G loss: 5.534724]\n",
      "epoch:21 step:16654 [D loss: 0.352840, acc.: 82.03%] [G loss: 3.049625]\n",
      "epoch:21 step:16655 [D loss: 0.327942, acc.: 85.94%] [G loss: 4.399774]\n",
      "epoch:21 step:16656 [D loss: 0.286257, acc.: 85.94%] [G loss: 3.769928]\n",
      "epoch:21 step:16657 [D loss: 0.253000, acc.: 89.06%] [G loss: 3.620767]\n",
      "epoch:21 step:16658 [D loss: 0.358488, acc.: 81.25%] [G loss: 4.340453]\n",
      "epoch:21 step:16659 [D loss: 0.323214, acc.: 85.94%] [G loss: 3.703519]\n",
      "epoch:21 step:16660 [D loss: 0.206647, acc.: 91.41%] [G loss: 4.313668]\n",
      "epoch:21 step:16661 [D loss: 0.250531, acc.: 91.41%] [G loss: 3.057558]\n",
      "epoch:21 step:16662 [D loss: 0.355025, acc.: 82.81%] [G loss: 3.511313]\n",
      "epoch:21 step:16663 [D loss: 0.375550, acc.: 81.25%] [G loss: 3.834352]\n",
      "epoch:21 step:16664 [D loss: 0.310142, acc.: 85.16%] [G loss: 3.639134]\n",
      "epoch:21 step:16665 [D loss: 0.313681, acc.: 85.16%] [G loss: 4.506672]\n",
      "epoch:21 step:16666 [D loss: 0.256705, acc.: 88.28%] [G loss: 4.014683]\n",
      "epoch:21 step:16667 [D loss: 0.351481, acc.: 79.69%] [G loss: 2.825007]\n",
      "epoch:21 step:16668 [D loss: 0.292340, acc.: 86.72%] [G loss: 2.881885]\n",
      "epoch:21 step:16669 [D loss: 0.381585, acc.: 80.47%] [G loss: 3.297740]\n",
      "epoch:21 step:16670 [D loss: 0.292966, acc.: 85.94%] [G loss: 2.988367]\n",
      "epoch:21 step:16671 [D loss: 0.315831, acc.: 86.72%] [G loss: 2.903249]\n",
      "epoch:21 step:16672 [D loss: 0.332427, acc.: 85.16%] [G loss: 2.728975]\n",
      "epoch:21 step:16673 [D loss: 0.281228, acc.: 86.72%] [G loss: 2.814651]\n",
      "epoch:21 step:16674 [D loss: 0.251441, acc.: 89.06%] [G loss: 2.586461]\n",
      "epoch:21 step:16675 [D loss: 0.285298, acc.: 89.84%] [G loss: 3.318145]\n",
      "epoch:21 step:16676 [D loss: 0.299689, acc.: 87.50%] [G loss: 3.291900]\n",
      "epoch:21 step:16677 [D loss: 0.351065, acc.: 89.06%] [G loss: 3.528460]\n",
      "epoch:21 step:16678 [D loss: 0.421974, acc.: 78.12%] [G loss: 3.088161]\n",
      "epoch:21 step:16679 [D loss: 0.397796, acc.: 85.16%] [G loss: 4.526630]\n",
      "epoch:21 step:16680 [D loss: 0.384739, acc.: 77.34%] [G loss: 3.110576]\n",
      "epoch:21 step:16681 [D loss: 0.306092, acc.: 85.94%] [G loss: 4.395677]\n",
      "epoch:21 step:16682 [D loss: 0.481473, acc.: 78.12%] [G loss: 3.002887]\n",
      "epoch:21 step:16683 [D loss: 0.328010, acc.: 88.28%] [G loss: 3.724013]\n",
      "epoch:21 step:16684 [D loss: 0.254493, acc.: 87.50%] [G loss: 3.865242]\n",
      "epoch:21 step:16685 [D loss: 0.242183, acc.: 87.50%] [G loss: 4.061179]\n",
      "epoch:21 step:16686 [D loss: 0.314687, acc.: 86.72%] [G loss: 3.174886]\n",
      "epoch:21 step:16687 [D loss: 0.365474, acc.: 85.16%] [G loss: 3.207805]\n",
      "epoch:21 step:16688 [D loss: 0.292067, acc.: 88.28%] [G loss: 2.503159]\n",
      "epoch:21 step:16689 [D loss: 0.348490, acc.: 85.16%] [G loss: 2.314642]\n",
      "epoch:21 step:16690 [D loss: 0.301205, acc.: 89.06%] [G loss: 3.107697]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:21 step:16691 [D loss: 0.356149, acc.: 83.59%] [G loss: 2.482630]\n",
      "epoch:21 step:16692 [D loss: 0.321054, acc.: 86.72%] [G loss: 3.806312]\n",
      "epoch:21 step:16693 [D loss: 0.460089, acc.: 75.00%] [G loss: 3.973956]\n",
      "epoch:21 step:16694 [D loss: 0.463358, acc.: 76.56%] [G loss: 4.691405]\n",
      "epoch:21 step:16695 [D loss: 0.622583, acc.: 74.22%] [G loss: 5.314575]\n",
      "epoch:21 step:16696 [D loss: 0.780301, acc.: 73.44%] [G loss: 8.833905]\n",
      "epoch:21 step:16697 [D loss: 1.597152, acc.: 57.81%] [G loss: 8.567289]\n",
      "epoch:21 step:16698 [D loss: 2.660930, acc.: 58.59%] [G loss: 3.014609]\n",
      "epoch:21 step:16699 [D loss: 0.671680, acc.: 70.31%] [G loss: 6.254294]\n",
      "epoch:21 step:16700 [D loss: 0.938941, acc.: 67.19%] [G loss: 3.555064]\n",
      "epoch:21 step:16701 [D loss: 0.717033, acc.: 73.44%] [G loss: 3.540769]\n",
      "epoch:21 step:16702 [D loss: 0.240545, acc.: 87.50%] [G loss: 6.143826]\n",
      "epoch:21 step:16703 [D loss: 0.419219, acc.: 78.91%] [G loss: 4.053653]\n",
      "epoch:21 step:16704 [D loss: 0.387243, acc.: 80.47%] [G loss: 3.816890]\n",
      "epoch:21 step:16705 [D loss: 0.337067, acc.: 85.16%] [G loss: 3.054520]\n",
      "epoch:21 step:16706 [D loss: 0.334344, acc.: 81.25%] [G loss: 3.449819]\n",
      "epoch:21 step:16707 [D loss: 0.381153, acc.: 86.72%] [G loss: 3.118118]\n",
      "epoch:21 step:16708 [D loss: 0.242164, acc.: 91.41%] [G loss: 3.097451]\n",
      "epoch:21 step:16709 [D loss: 0.425241, acc.: 83.59%] [G loss: 2.422201]\n",
      "epoch:21 step:16710 [D loss: 0.309455, acc.: 85.94%] [G loss: 2.555561]\n",
      "epoch:21 step:16711 [D loss: 0.312690, acc.: 85.94%] [G loss: 2.862884]\n",
      "epoch:21 step:16712 [D loss: 0.380137, acc.: 82.81%] [G loss: 2.606125]\n",
      "epoch:21 step:16713 [D loss: 0.306920, acc.: 87.50%] [G loss: 2.961994]\n",
      "epoch:21 step:16714 [D loss: 0.405203, acc.: 79.69%] [G loss: 3.570361]\n",
      "epoch:21 step:16715 [D loss: 0.351743, acc.: 83.59%] [G loss: 2.432913]\n",
      "epoch:21 step:16716 [D loss: 0.435891, acc.: 75.00%] [G loss: 3.456461]\n",
      "epoch:21 step:16717 [D loss: 0.300719, acc.: 85.94%] [G loss: 3.350181]\n",
      "epoch:21 step:16718 [D loss: 0.433165, acc.: 79.69%] [G loss: 2.356198]\n",
      "epoch:21 step:16719 [D loss: 0.326353, acc.: 85.16%] [G loss: 2.936346]\n",
      "epoch:21 step:16720 [D loss: 0.372279, acc.: 87.50%] [G loss: 2.382667]\n",
      "epoch:21 step:16721 [D loss: 0.358990, acc.: 81.25%] [G loss: 2.704486]\n",
      "epoch:21 step:16722 [D loss: 0.407645, acc.: 83.59%] [G loss: 2.452297]\n",
      "epoch:21 step:16723 [D loss: 0.348944, acc.: 82.81%] [G loss: 3.673700]\n",
      "epoch:21 step:16724 [D loss: 0.430194, acc.: 75.78%] [G loss: 2.617513]\n",
      "epoch:21 step:16725 [D loss: 0.328560, acc.: 85.16%] [G loss: 3.463008]\n",
      "epoch:21 step:16726 [D loss: 0.428408, acc.: 81.25%] [G loss: 2.554940]\n",
      "epoch:21 step:16727 [D loss: 0.349146, acc.: 87.50%] [G loss: 2.948158]\n",
      "epoch:21 step:16728 [D loss: 0.346519, acc.: 82.81%] [G loss: 2.506605]\n",
      "epoch:21 step:16729 [D loss: 0.346739, acc.: 83.59%] [G loss: 2.500669]\n",
      "epoch:21 step:16730 [D loss: 0.341026, acc.: 83.59%] [G loss: 2.769788]\n",
      "epoch:21 step:16731 [D loss: 0.322533, acc.: 87.50%] [G loss: 2.227082]\n",
      "epoch:21 step:16732 [D loss: 0.366593, acc.: 82.03%] [G loss: 2.800979]\n",
      "epoch:21 step:16733 [D loss: 0.359391, acc.: 82.03%] [G loss: 3.368048]\n",
      "epoch:21 step:16734 [D loss: 0.334959, acc.: 88.28%] [G loss: 3.303071]\n",
      "epoch:21 step:16735 [D loss: 0.356027, acc.: 82.81%] [G loss: 3.094609]\n",
      "epoch:21 step:16736 [D loss: 0.377612, acc.: 85.16%] [G loss: 2.730000]\n",
      "epoch:21 step:16737 [D loss: 0.287439, acc.: 84.38%] [G loss: 2.743812]\n",
      "epoch:21 step:16738 [D loss: 0.242101, acc.: 90.62%] [G loss: 2.739946]\n",
      "epoch:21 step:16739 [D loss: 0.417648, acc.: 79.69%] [G loss: 2.568616]\n",
      "epoch:21 step:16740 [D loss: 0.325861, acc.: 84.38%] [G loss: 3.087204]\n",
      "epoch:21 step:16741 [D loss: 0.360226, acc.: 85.16%] [G loss: 2.327880]\n",
      "epoch:21 step:16742 [D loss: 0.391916, acc.: 82.03%] [G loss: 2.988831]\n",
      "epoch:21 step:16743 [D loss: 0.392191, acc.: 84.38%] [G loss: 2.473470]\n",
      "epoch:21 step:16744 [D loss: 0.353522, acc.: 83.59%] [G loss: 2.527848]\n",
      "epoch:21 step:16745 [D loss: 0.381971, acc.: 83.59%] [G loss: 2.574042]\n",
      "epoch:21 step:16746 [D loss: 0.265436, acc.: 88.28%] [G loss: 2.789614]\n",
      "epoch:21 step:16747 [D loss: 0.358495, acc.: 80.47%] [G loss: 3.516964]\n",
      "epoch:21 step:16748 [D loss: 0.349640, acc.: 78.91%] [G loss: 2.799205]\n",
      "epoch:21 step:16749 [D loss: 0.349815, acc.: 84.38%] [G loss: 2.429183]\n",
      "epoch:21 step:16750 [D loss: 0.356456, acc.: 85.94%] [G loss: 2.624968]\n",
      "epoch:21 step:16751 [D loss: 0.311216, acc.: 87.50%] [G loss: 2.568689]\n",
      "epoch:21 step:16752 [D loss: 0.373794, acc.: 83.59%] [G loss: 2.873003]\n",
      "epoch:21 step:16753 [D loss: 0.394738, acc.: 83.59%] [G loss: 2.550799]\n",
      "epoch:21 step:16754 [D loss: 0.262220, acc.: 88.28%] [G loss: 2.674050]\n",
      "epoch:21 step:16755 [D loss: 0.304408, acc.: 87.50%] [G loss: 3.522655]\n",
      "epoch:21 step:16756 [D loss: 0.372543, acc.: 82.81%] [G loss: 3.483807]\n",
      "epoch:21 step:16757 [D loss: 0.487196, acc.: 76.56%] [G loss: 4.677213]\n",
      "epoch:21 step:16758 [D loss: 0.701433, acc.: 71.88%] [G loss: 6.651866]\n",
      "epoch:21 step:16759 [D loss: 0.821318, acc.: 68.75%] [G loss: 5.183441]\n",
      "epoch:21 step:16760 [D loss: 0.456383, acc.: 77.34%] [G loss: 3.422030]\n",
      "epoch:21 step:16761 [D loss: 0.311639, acc.: 86.72%] [G loss: 3.470999]\n",
      "epoch:21 step:16762 [D loss: 0.328247, acc.: 85.94%] [G loss: 4.927462]\n",
      "epoch:21 step:16763 [D loss: 0.322016, acc.: 85.94%] [G loss: 3.328731]\n",
      "epoch:21 step:16764 [D loss: 0.360349, acc.: 85.16%] [G loss: 2.120531]\n",
      "epoch:21 step:16765 [D loss: 0.305662, acc.: 86.72%] [G loss: 2.486885]\n",
      "epoch:21 step:16766 [D loss: 0.394290, acc.: 80.47%] [G loss: 2.515630]\n",
      "epoch:21 step:16767 [D loss: 0.389128, acc.: 79.69%] [G loss: 2.710001]\n",
      "epoch:21 step:16768 [D loss: 0.375954, acc.: 84.38%] [G loss: 2.960807]\n",
      "epoch:21 step:16769 [D loss: 0.382952, acc.: 83.59%] [G loss: 3.545567]\n",
      "epoch:21 step:16770 [D loss: 0.371184, acc.: 86.72%] [G loss: 2.637777]\n",
      "epoch:21 step:16771 [D loss: 0.338475, acc.: 86.72%] [G loss: 2.846019]\n",
      "epoch:21 step:16772 [D loss: 0.265157, acc.: 87.50%] [G loss: 3.293605]\n",
      "epoch:21 step:16773 [D loss: 0.344813, acc.: 85.16%] [G loss: 2.434966]\n",
      "epoch:21 step:16774 [D loss: 0.389801, acc.: 83.59%] [G loss: 2.402153]\n",
      "epoch:21 step:16775 [D loss: 0.377342, acc.: 82.81%] [G loss: 2.477654]\n",
      "epoch:21 step:16776 [D loss: 0.387086, acc.: 82.81%] [G loss: 2.722009]\n",
      "epoch:21 step:16777 [D loss: 0.310887, acc.: 86.72%] [G loss: 2.248710]\n",
      "epoch:21 step:16778 [D loss: 0.295564, acc.: 87.50%] [G loss: 3.099950]\n",
      "epoch:21 step:16779 [D loss: 0.366237, acc.: 85.94%] [G loss: 2.724608]\n",
      "epoch:21 step:16780 [D loss: 0.309407, acc.: 87.50%] [G loss: 2.432828]\n",
      "epoch:21 step:16781 [D loss: 0.356834, acc.: 84.38%] [G loss: 2.598241]\n",
      "epoch:21 step:16782 [D loss: 0.328992, acc.: 88.28%] [G loss: 2.273599]\n",
      "epoch:21 step:16783 [D loss: 0.406243, acc.: 78.91%] [G loss: 2.549945]\n",
      "epoch:21 step:16784 [D loss: 0.414478, acc.: 79.69%] [G loss: 2.953987]\n",
      "epoch:21 step:16785 [D loss: 0.356049, acc.: 81.25%] [G loss: 3.605488]\n",
      "epoch:21 step:16786 [D loss: 0.425893, acc.: 81.25%] [G loss: 3.158744]\n",
      "epoch:21 step:16787 [D loss: 0.269860, acc.: 89.84%] [G loss: 3.751087]\n",
      "epoch:21 step:16788 [D loss: 0.398722, acc.: 82.03%] [G loss: 2.407088]\n",
      "epoch:21 step:16789 [D loss: 0.373238, acc.: 83.59%] [G loss: 2.908930]\n",
      "epoch:21 step:16790 [D loss: 0.358379, acc.: 83.59%] [G loss: 3.057917]\n",
      "epoch:21 step:16791 [D loss: 0.289628, acc.: 86.72%] [G loss: 2.528445]\n",
      "epoch:21 step:16792 [D loss: 0.438792, acc.: 79.69%] [G loss: 2.809264]\n",
      "epoch:21 step:16793 [D loss: 0.342414, acc.: 83.59%] [G loss: 3.350636]\n",
      "epoch:21 step:16794 [D loss: 0.278316, acc.: 87.50%] [G loss: 5.289733]\n",
      "epoch:21 step:16795 [D loss: 0.328428, acc.: 82.81%] [G loss: 2.724493]\n",
      "epoch:21 step:16796 [D loss: 0.352536, acc.: 84.38%] [G loss: 3.548624]\n",
      "epoch:21 step:16797 [D loss: 0.243625, acc.: 89.84%] [G loss: 3.113525]\n",
      "epoch:21 step:16798 [D loss: 0.407065, acc.: 85.16%] [G loss: 2.913019]\n",
      "epoch:21 step:16799 [D loss: 0.318815, acc.: 85.16%] [G loss: 3.220834]\n",
      "epoch:21 step:16800 [D loss: 0.299600, acc.: 85.16%] [G loss: 3.180494]\n",
      "##############\n",
      "[0.85598196 0.8545632  0.79513004 0.803079   0.78267215 0.83047418\n",
      " 0.87999835 0.83358838 0.82700957 0.82722292]\n",
      "##########\n",
      "epoch:21 step:16801 [D loss: 0.285273, acc.: 85.94%] [G loss: 3.507198]\n",
      "epoch:21 step:16802 [D loss: 0.267078, acc.: 88.28%] [G loss: 2.823034]\n",
      "epoch:21 step:16803 [D loss: 0.391921, acc.: 80.47%] [G loss: 2.473384]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:21 step:16804 [D loss: 0.420105, acc.: 80.47%] [G loss: 2.253228]\n",
      "epoch:21 step:16805 [D loss: 0.286212, acc.: 89.06%] [G loss: 2.909745]\n",
      "epoch:21 step:16806 [D loss: 0.361902, acc.: 85.16%] [G loss: 2.659604]\n",
      "epoch:21 step:16807 [D loss: 0.378271, acc.: 82.03%] [G loss: 2.778104]\n",
      "epoch:21 step:16808 [D loss: 0.359107, acc.: 85.94%] [G loss: 2.172842]\n",
      "epoch:21 step:16809 [D loss: 0.354779, acc.: 85.16%] [G loss: 2.293780]\n",
      "epoch:21 step:16810 [D loss: 0.285611, acc.: 89.84%] [G loss: 2.311641]\n",
      "epoch:21 step:16811 [D loss: 0.358083, acc.: 83.59%] [G loss: 2.631495]\n",
      "epoch:21 step:16812 [D loss: 0.432052, acc.: 78.91%] [G loss: 2.217069]\n",
      "epoch:21 step:16813 [D loss: 0.382110, acc.: 82.81%] [G loss: 2.781499]\n",
      "epoch:21 step:16814 [D loss: 0.412204, acc.: 81.25%] [G loss: 3.004853]\n",
      "epoch:21 step:16815 [D loss: 0.379817, acc.: 84.38%] [G loss: 2.780474]\n",
      "epoch:21 step:16816 [D loss: 0.280338, acc.: 88.28%] [G loss: 3.192276]\n",
      "epoch:21 step:16817 [D loss: 0.227720, acc.: 94.53%] [G loss: 2.853185]\n",
      "epoch:21 step:16818 [D loss: 0.332457, acc.: 85.16%] [G loss: 2.723160]\n",
      "epoch:21 step:16819 [D loss: 0.236451, acc.: 91.41%] [G loss: 3.347359]\n",
      "epoch:21 step:16820 [D loss: 0.376398, acc.: 80.47%] [G loss: 2.361290]\n",
      "epoch:21 step:16821 [D loss: 0.469885, acc.: 78.12%] [G loss: 2.778410]\n",
      "epoch:21 step:16822 [D loss: 0.414695, acc.: 83.59%] [G loss: 2.988901]\n",
      "epoch:21 step:16823 [D loss: 0.280205, acc.: 86.72%] [G loss: 6.774571]\n",
      "epoch:21 step:16824 [D loss: 0.381416, acc.: 79.69%] [G loss: 4.419937]\n",
      "epoch:21 step:16825 [D loss: 0.349868, acc.: 81.25%] [G loss: 4.084511]\n",
      "epoch:21 step:16826 [D loss: 0.355131, acc.: 80.47%] [G loss: 3.014949]\n",
      "epoch:21 step:16827 [D loss: 0.334455, acc.: 84.38%] [G loss: 2.845867]\n",
      "epoch:21 step:16828 [D loss: 0.240453, acc.: 89.84%] [G loss: 4.356963]\n",
      "epoch:21 step:16829 [D loss: 0.415238, acc.: 81.25%] [G loss: 3.335453]\n",
      "epoch:21 step:16830 [D loss: 0.371330, acc.: 81.25%] [G loss: 4.303147]\n",
      "epoch:21 step:16831 [D loss: 0.242801, acc.: 88.28%] [G loss: 6.248301]\n",
      "epoch:21 step:16832 [D loss: 0.319877, acc.: 85.16%] [G loss: 3.493846]\n",
      "epoch:21 step:16833 [D loss: 0.402660, acc.: 80.47%] [G loss: 3.364077]\n",
      "epoch:21 step:16834 [D loss: 0.423122, acc.: 82.81%] [G loss: 3.127278]\n",
      "epoch:21 step:16835 [D loss: 0.346465, acc.: 85.94%] [G loss: 3.705320]\n",
      "epoch:21 step:16836 [D loss: 0.517583, acc.: 75.78%] [G loss: 3.128103]\n",
      "epoch:21 step:16837 [D loss: 0.372576, acc.: 81.25%] [G loss: 2.832856]\n",
      "epoch:21 step:16838 [D loss: 0.254951, acc.: 90.62%] [G loss: 4.138904]\n",
      "epoch:21 step:16839 [D loss: 0.547591, acc.: 75.78%] [G loss: 2.871044]\n",
      "epoch:21 step:16840 [D loss: 0.257619, acc.: 92.19%] [G loss: 2.982429]\n",
      "epoch:21 step:16841 [D loss: 0.266139, acc.: 89.06%] [G loss: 3.548607]\n",
      "epoch:21 step:16842 [D loss: 0.335526, acc.: 85.16%] [G loss: 4.141969]\n",
      "epoch:21 step:16843 [D loss: 0.285806, acc.: 87.50%] [G loss: 2.795335]\n",
      "epoch:21 step:16844 [D loss: 0.307809, acc.: 83.59%] [G loss: 3.546140]\n",
      "epoch:21 step:16845 [D loss: 0.301811, acc.: 85.94%] [G loss: 3.323177]\n",
      "epoch:21 step:16846 [D loss: 0.381043, acc.: 80.47%] [G loss: 2.515953]\n",
      "epoch:21 step:16847 [D loss: 0.320164, acc.: 83.59%] [G loss: 3.588860]\n",
      "epoch:21 step:16848 [D loss: 0.294340, acc.: 85.94%] [G loss: 4.355887]\n",
      "epoch:21 step:16849 [D loss: 0.362558, acc.: 86.72%] [G loss: 2.629280]\n",
      "epoch:21 step:16850 [D loss: 0.233010, acc.: 89.84%] [G loss: 2.888464]\n",
      "epoch:21 step:16851 [D loss: 0.329763, acc.: 82.81%] [G loss: 4.306053]\n",
      "epoch:21 step:16852 [D loss: 0.409992, acc.: 82.81%] [G loss: 4.463206]\n",
      "epoch:21 step:16853 [D loss: 0.310485, acc.: 85.94%] [G loss: 3.619274]\n",
      "epoch:21 step:16854 [D loss: 0.275780, acc.: 87.50%] [G loss: 3.560262]\n",
      "epoch:21 step:16855 [D loss: 0.347163, acc.: 83.59%] [G loss: 4.044979]\n",
      "epoch:21 step:16856 [D loss: 0.359688, acc.: 84.38%] [G loss: 3.300651]\n",
      "epoch:21 step:16857 [D loss: 0.383194, acc.: 83.59%] [G loss: 3.804874]\n",
      "epoch:21 step:16858 [D loss: 0.366541, acc.: 87.50%] [G loss: 2.645579]\n",
      "epoch:21 step:16859 [D loss: 0.314583, acc.: 86.72%] [G loss: 2.778276]\n",
      "epoch:21 step:16860 [D loss: 0.273931, acc.: 89.06%] [G loss: 4.050161]\n",
      "epoch:21 step:16861 [D loss: 0.278644, acc.: 89.84%] [G loss: 2.573864]\n",
      "epoch:21 step:16862 [D loss: 0.287515, acc.: 89.84%] [G loss: 3.530938]\n",
      "epoch:21 step:16863 [D loss: 0.259811, acc.: 90.62%] [G loss: 3.464221]\n",
      "epoch:21 step:16864 [D loss: 0.284124, acc.: 88.28%] [G loss: 2.914764]\n",
      "epoch:21 step:16865 [D loss: 0.409706, acc.: 78.91%] [G loss: 2.718316]\n",
      "epoch:21 step:16866 [D loss: 0.331232, acc.: 86.72%] [G loss: 2.856314]\n",
      "epoch:21 step:16867 [D loss: 0.387032, acc.: 81.25%] [G loss: 2.639087]\n",
      "epoch:21 step:16868 [D loss: 0.469683, acc.: 77.34%] [G loss: 2.553518]\n",
      "epoch:21 step:16869 [D loss: 0.344077, acc.: 85.16%] [G loss: 2.257950]\n",
      "epoch:21 step:16870 [D loss: 0.289476, acc.: 89.84%] [G loss: 2.932411]\n",
      "epoch:21 step:16871 [D loss: 0.279601, acc.: 90.62%] [G loss: 2.653348]\n",
      "epoch:21 step:16872 [D loss: 0.377647, acc.: 82.81%] [G loss: 2.289960]\n",
      "epoch:21 step:16873 [D loss: 0.351299, acc.: 84.38%] [G loss: 2.726921]\n",
      "epoch:21 step:16874 [D loss: 0.437108, acc.: 78.12%] [G loss: 3.832742]\n",
      "epoch:21 step:16875 [D loss: 0.391302, acc.: 80.47%] [G loss: 5.011333]\n",
      "epoch:21 step:16876 [D loss: 0.461361, acc.: 78.12%] [G loss: 3.051964]\n",
      "epoch:21 step:16877 [D loss: 0.318751, acc.: 88.28%] [G loss: 2.994711]\n",
      "epoch:21 step:16878 [D loss: 0.312947, acc.: 88.28%] [G loss: 3.039086]\n",
      "epoch:21 step:16879 [D loss: 0.314110, acc.: 89.06%] [G loss: 3.451514]\n",
      "epoch:21 step:16880 [D loss: 0.385855, acc.: 85.94%] [G loss: 2.931289]\n",
      "epoch:21 step:16881 [D loss: 0.301254, acc.: 87.50%] [G loss: 3.378360]\n",
      "epoch:21 step:16882 [D loss: 0.304895, acc.: 86.72%] [G loss: 3.721470]\n",
      "epoch:21 step:16883 [D loss: 0.332331, acc.: 86.72%] [G loss: 3.238470]\n",
      "epoch:21 step:16884 [D loss: 0.299726, acc.: 85.16%] [G loss: 2.618155]\n",
      "epoch:21 step:16885 [D loss: 0.315367, acc.: 86.72%] [G loss: 3.713276]\n",
      "epoch:21 step:16886 [D loss: 0.260351, acc.: 89.84%] [G loss: 3.932192]\n",
      "epoch:21 step:16887 [D loss: 0.248180, acc.: 89.06%] [G loss: 2.568264]\n",
      "epoch:21 step:16888 [D loss: 0.254185, acc.: 89.84%] [G loss: 4.222341]\n",
      "epoch:21 step:16889 [D loss: 0.326109, acc.: 84.38%] [G loss: 3.878261]\n",
      "epoch:21 step:16890 [D loss: 0.349048, acc.: 85.16%] [G loss: 2.884547]\n",
      "epoch:21 step:16891 [D loss: 0.226577, acc.: 86.72%] [G loss: 5.030998]\n",
      "epoch:21 step:16892 [D loss: 0.223491, acc.: 92.19%] [G loss: 4.155947]\n",
      "epoch:21 step:16893 [D loss: 0.228206, acc.: 90.62%] [G loss: 3.593678]\n",
      "epoch:21 step:16894 [D loss: 0.365318, acc.: 85.16%] [G loss: 2.993647]\n",
      "epoch:21 step:16895 [D loss: 0.355858, acc.: 81.25%] [G loss: 4.366699]\n",
      "epoch:21 step:16896 [D loss: 0.321957, acc.: 84.38%] [G loss: 2.699215]\n",
      "epoch:21 step:16897 [D loss: 0.271467, acc.: 90.62%] [G loss: 2.602550]\n",
      "epoch:21 step:16898 [D loss: 0.269651, acc.: 89.06%] [G loss: 4.434185]\n",
      "epoch:21 step:16899 [D loss: 0.284240, acc.: 88.28%] [G loss: 3.679655]\n",
      "epoch:21 step:16900 [D loss: 0.314648, acc.: 89.06%] [G loss: 2.689594]\n",
      "epoch:21 step:16901 [D loss: 0.297832, acc.: 85.16%] [G loss: 2.797440]\n",
      "epoch:21 step:16902 [D loss: 0.389689, acc.: 81.25%] [G loss: 3.514777]\n",
      "epoch:21 step:16903 [D loss: 0.282758, acc.: 89.84%] [G loss: 2.209065]\n",
      "epoch:21 step:16904 [D loss: 0.283808, acc.: 88.28%] [G loss: 2.999336]\n",
      "epoch:21 step:16905 [D loss: 0.372324, acc.: 85.16%] [G loss: 3.308330]\n",
      "epoch:21 step:16906 [D loss: 0.310170, acc.: 84.38%] [G loss: 3.154730]\n",
      "epoch:21 step:16907 [D loss: 0.477327, acc.: 77.34%] [G loss: 3.010447]\n",
      "epoch:21 step:16908 [D loss: 0.349300, acc.: 83.59%] [G loss: 3.632570]\n",
      "epoch:21 step:16909 [D loss: 0.326122, acc.: 87.50%] [G loss: 3.278592]\n",
      "epoch:21 step:16910 [D loss: 0.329722, acc.: 87.50%] [G loss: 2.876105]\n",
      "epoch:21 step:16911 [D loss: 0.383841, acc.: 82.81%] [G loss: 2.511040]\n",
      "epoch:21 step:16912 [D loss: 0.293399, acc.: 87.50%] [G loss: 3.059276]\n",
      "epoch:21 step:16913 [D loss: 0.255687, acc.: 89.84%] [G loss: 3.219402]\n",
      "epoch:21 step:16914 [D loss: 0.358965, acc.: 85.16%] [G loss: 2.809652]\n",
      "epoch:21 step:16915 [D loss: 0.324817, acc.: 85.16%] [G loss: 3.375293]\n",
      "epoch:21 step:16916 [D loss: 0.334928, acc.: 87.50%] [G loss: 4.102511]\n",
      "epoch:21 step:16917 [D loss: 0.268551, acc.: 91.41%] [G loss: 5.032451]\n",
      "epoch:21 step:16918 [D loss: 0.296537, acc.: 89.84%] [G loss: 3.750484]\n",
      "epoch:21 step:16919 [D loss: 0.235405, acc.: 88.28%] [G loss: 4.102447]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:21 step:16920 [D loss: 0.243910, acc.: 89.84%] [G loss: 4.231148]\n",
      "epoch:21 step:16921 [D loss: 0.327672, acc.: 85.16%] [G loss: 4.260225]\n",
      "epoch:21 step:16922 [D loss: 0.283115, acc.: 88.28%] [G loss: 4.038731]\n",
      "epoch:21 step:16923 [D loss: 0.280994, acc.: 90.62%] [G loss: 4.883843]\n",
      "epoch:21 step:16924 [D loss: 0.522143, acc.: 79.69%] [G loss: 3.045338]\n",
      "epoch:21 step:16925 [D loss: 0.421279, acc.: 81.25%] [G loss: 2.286486]\n",
      "epoch:21 step:16926 [D loss: 0.340520, acc.: 84.38%] [G loss: 2.683055]\n",
      "epoch:21 step:16927 [D loss: 0.442725, acc.: 75.78%] [G loss: 3.027926]\n",
      "epoch:21 step:16928 [D loss: 0.350806, acc.: 86.72%] [G loss: 3.614249]\n",
      "epoch:21 step:16929 [D loss: 0.263886, acc.: 89.84%] [G loss: 3.448053]\n",
      "epoch:21 step:16930 [D loss: 0.392463, acc.: 79.69%] [G loss: 5.232099]\n",
      "epoch:21 step:16931 [D loss: 0.782412, acc.: 67.97%] [G loss: 4.537388]\n",
      "epoch:21 step:16932 [D loss: 0.987468, acc.: 71.09%] [G loss: 8.072412]\n",
      "epoch:21 step:16933 [D loss: 1.839520, acc.: 51.56%] [G loss: 4.579823]\n",
      "epoch:21 step:16934 [D loss: 0.913921, acc.: 70.31%] [G loss: 3.167971]\n",
      "epoch:21 step:16935 [D loss: 0.437346, acc.: 79.69%] [G loss: 5.721929]\n",
      "epoch:21 step:16936 [D loss: 0.388188, acc.: 82.81%] [G loss: 2.979804]\n",
      "epoch:21 step:16937 [D loss: 0.285338, acc.: 85.16%] [G loss: 3.976479]\n",
      "epoch:21 step:16938 [D loss: 0.265893, acc.: 86.72%] [G loss: 4.039972]\n",
      "epoch:21 step:16939 [D loss: 0.497104, acc.: 72.66%] [G loss: 3.274690]\n",
      "epoch:21 step:16940 [D loss: 0.382001, acc.: 83.59%] [G loss: 3.673162]\n",
      "epoch:21 step:16941 [D loss: 0.384976, acc.: 83.59%] [G loss: 3.046442]\n",
      "epoch:21 step:16942 [D loss: 0.466485, acc.: 80.47%] [G loss: 2.264609]\n",
      "epoch:21 step:16943 [D loss: 0.301025, acc.: 87.50%] [G loss: 3.196650]\n",
      "epoch:21 step:16944 [D loss: 0.402508, acc.: 79.69%] [G loss: 2.818167]\n",
      "epoch:21 step:16945 [D loss: 0.453802, acc.: 83.59%] [G loss: 2.147661]\n",
      "epoch:21 step:16946 [D loss: 0.393147, acc.: 79.69%] [G loss: 2.428592]\n",
      "epoch:21 step:16947 [D loss: 0.285939, acc.: 87.50%] [G loss: 2.399551]\n",
      "epoch:21 step:16948 [D loss: 0.362948, acc.: 86.72%] [G loss: 2.316711]\n",
      "epoch:21 step:16949 [D loss: 0.283416, acc.: 89.06%] [G loss: 2.178311]\n",
      "epoch:21 step:16950 [D loss: 0.325110, acc.: 86.72%] [G loss: 1.964453]\n",
      "epoch:21 step:16951 [D loss: 0.343787, acc.: 83.59%] [G loss: 2.528409]\n",
      "epoch:21 step:16952 [D loss: 0.322837, acc.: 84.38%] [G loss: 2.764339]\n",
      "epoch:21 step:16953 [D loss: 0.503251, acc.: 80.47%] [G loss: 2.281442]\n",
      "epoch:21 step:16954 [D loss: 0.348646, acc.: 85.16%] [G loss: 2.329934]\n",
      "epoch:21 step:16955 [D loss: 0.337801, acc.: 87.50%] [G loss: 2.488393]\n",
      "epoch:21 step:16956 [D loss: 0.318817, acc.: 84.38%] [G loss: 2.313547]\n",
      "epoch:21 step:16957 [D loss: 0.368651, acc.: 82.81%] [G loss: 2.456125]\n",
      "epoch:21 step:16958 [D loss: 0.362480, acc.: 82.81%] [G loss: 2.766453]\n",
      "epoch:21 step:16959 [D loss: 0.279241, acc.: 88.28%] [G loss: 3.228500]\n",
      "epoch:21 step:16960 [D loss: 0.358545, acc.: 79.69%] [G loss: 3.918219]\n",
      "epoch:21 step:16961 [D loss: 0.312926, acc.: 85.94%] [G loss: 4.135743]\n",
      "epoch:21 step:16962 [D loss: 0.354215, acc.: 84.38%] [G loss: 4.100592]\n",
      "epoch:21 step:16963 [D loss: 0.384167, acc.: 84.38%] [G loss: 2.563678]\n",
      "epoch:21 step:16964 [D loss: 0.251359, acc.: 88.28%] [G loss: 3.269199]\n",
      "epoch:21 step:16965 [D loss: 0.373441, acc.: 80.47%] [G loss: 2.761369]\n",
      "epoch:21 step:16966 [D loss: 0.456883, acc.: 78.12%] [G loss: 2.423527]\n",
      "epoch:21 step:16967 [D loss: 0.407851, acc.: 82.03%] [G loss: 2.539231]\n",
      "epoch:21 step:16968 [D loss: 0.345778, acc.: 84.38%] [G loss: 3.099273]\n",
      "epoch:21 step:16969 [D loss: 0.357196, acc.: 83.59%] [G loss: 2.737977]\n",
      "epoch:21 step:16970 [D loss: 0.346332, acc.: 84.38%] [G loss: 2.731323]\n",
      "epoch:21 step:16971 [D loss: 0.381170, acc.: 83.59%] [G loss: 2.977675]\n",
      "epoch:21 step:16972 [D loss: 0.339300, acc.: 83.59%] [G loss: 2.817807]\n",
      "epoch:21 step:16973 [D loss: 0.238950, acc.: 91.41%] [G loss: 3.124089]\n",
      "epoch:21 step:16974 [D loss: 0.281204, acc.: 88.28%] [G loss: 4.782041]\n",
      "epoch:21 step:16975 [D loss: 0.291829, acc.: 87.50%] [G loss: 2.542431]\n",
      "epoch:21 step:16976 [D loss: 0.279416, acc.: 83.59%] [G loss: 3.991808]\n",
      "epoch:21 step:16977 [D loss: 0.337538, acc.: 88.28%] [G loss: 2.416272]\n",
      "epoch:21 step:16978 [D loss: 0.262904, acc.: 87.50%] [G loss: 5.874364]\n",
      "epoch:21 step:16979 [D loss: 0.390587, acc.: 82.81%] [G loss: 2.893179]\n",
      "epoch:21 step:16980 [D loss: 0.317508, acc.: 90.62%] [G loss: 2.755958]\n",
      "epoch:21 step:16981 [D loss: 0.412767, acc.: 79.69%] [G loss: 2.564874]\n",
      "epoch:21 step:16982 [D loss: 0.289768, acc.: 87.50%] [G loss: 2.665979]\n",
      "epoch:21 step:16983 [D loss: 0.310176, acc.: 87.50%] [G loss: 3.051622]\n",
      "epoch:21 step:16984 [D loss: 0.309834, acc.: 85.16%] [G loss: 2.720070]\n",
      "epoch:21 step:16985 [D loss: 0.281638, acc.: 88.28%] [G loss: 2.749782]\n",
      "epoch:21 step:16986 [D loss: 0.308828, acc.: 85.94%] [G loss: 2.572539]\n",
      "epoch:21 step:16987 [D loss: 0.364938, acc.: 87.50%] [G loss: 2.797707]\n",
      "epoch:21 step:16988 [D loss: 0.307606, acc.: 87.50%] [G loss: 2.464638]\n",
      "epoch:21 step:16989 [D loss: 0.239392, acc.: 91.41%] [G loss: 2.646917]\n",
      "epoch:21 step:16990 [D loss: 0.288050, acc.: 89.84%] [G loss: 2.763371]\n",
      "epoch:21 step:16991 [D loss: 0.448212, acc.: 79.69%] [G loss: 1.933615]\n",
      "epoch:21 step:16992 [D loss: 0.420262, acc.: 82.03%] [G loss: 2.006017]\n",
      "epoch:21 step:16993 [D loss: 0.406889, acc.: 84.38%] [G loss: 2.331165]\n",
      "epoch:21 step:16994 [D loss: 0.242529, acc.: 92.19%] [G loss: 2.881102]\n",
      "epoch:21 step:16995 [D loss: 0.339728, acc.: 83.59%] [G loss: 3.233253]\n",
      "epoch:21 step:16996 [D loss: 0.391741, acc.: 78.12%] [G loss: 2.630850]\n",
      "epoch:21 step:16997 [D loss: 0.452534, acc.: 78.91%] [G loss: 3.484536]\n",
      "epoch:21 step:16998 [D loss: 0.381201, acc.: 82.03%] [G loss: 2.781627]\n",
      "epoch:21 step:16999 [D loss: 0.336120, acc.: 85.16%] [G loss: 2.928990]\n",
      "epoch:21 step:17000 [D loss: 0.340488, acc.: 84.38%] [G loss: 3.327935]\n",
      "##############\n",
      "[0.85521066 0.85080059 0.79258954 0.80742427 0.77009645 0.82114595\n",
      " 0.86814098 0.85222282 0.82722536 0.82192997]\n",
      "##########\n",
      "epoch:21 step:17001 [D loss: 0.513913, acc.: 76.56%] [G loss: 2.877475]\n",
      "epoch:21 step:17002 [D loss: 0.296068, acc.: 85.94%] [G loss: 3.067648]\n",
      "epoch:21 step:17003 [D loss: 0.351615, acc.: 82.81%] [G loss: 1.994427]\n",
      "epoch:21 step:17004 [D loss: 0.362541, acc.: 84.38%] [G loss: 2.866161]\n",
      "epoch:21 step:17005 [D loss: 0.376317, acc.: 82.03%] [G loss: 3.576334]\n",
      "epoch:21 step:17006 [D loss: 0.362474, acc.: 80.47%] [G loss: 4.123176]\n",
      "epoch:21 step:17007 [D loss: 0.473294, acc.: 76.56%] [G loss: 2.370693]\n",
      "epoch:21 step:17008 [D loss: 0.244744, acc.: 90.62%] [G loss: 3.002191]\n",
      "epoch:21 step:17009 [D loss: 0.293370, acc.: 84.38%] [G loss: 5.071126]\n",
      "epoch:21 step:17010 [D loss: 0.341636, acc.: 85.16%] [G loss: 4.120348]\n",
      "epoch:21 step:17011 [D loss: 0.383503, acc.: 86.72%] [G loss: 2.669468]\n",
      "epoch:21 step:17012 [D loss: 0.279388, acc.: 85.94%] [G loss: 3.605433]\n",
      "epoch:21 step:17013 [D loss: 0.326974, acc.: 90.62%] [G loss: 3.226181]\n",
      "epoch:21 step:17014 [D loss: 0.584599, acc.: 69.53%] [G loss: 3.711449]\n",
      "epoch:21 step:17015 [D loss: 0.321169, acc.: 86.72%] [G loss: 3.263429]\n",
      "epoch:21 step:17016 [D loss: 0.296346, acc.: 88.28%] [G loss: 4.305713]\n",
      "epoch:21 step:17017 [D loss: 0.446134, acc.: 82.03%] [G loss: 2.827912]\n",
      "epoch:21 step:17018 [D loss: 0.426252, acc.: 79.69%] [G loss: 4.294327]\n",
      "epoch:21 step:17019 [D loss: 0.321612, acc.: 87.50%] [G loss: 3.697847]\n",
      "epoch:21 step:17020 [D loss: 0.285450, acc.: 88.28%] [G loss: 4.029833]\n",
      "epoch:21 step:17021 [D loss: 0.494204, acc.: 80.47%] [G loss: 4.291657]\n",
      "epoch:21 step:17022 [D loss: 0.251084, acc.: 90.62%] [G loss: 3.509659]\n",
      "epoch:21 step:17023 [D loss: 0.282813, acc.: 92.97%] [G loss: 4.842408]\n",
      "epoch:21 step:17024 [D loss: 0.312901, acc.: 85.94%] [G loss: 5.547076]\n",
      "epoch:21 step:17025 [D loss: 0.278618, acc.: 85.94%] [G loss: 3.759052]\n",
      "epoch:21 step:17026 [D loss: 0.291948, acc.: 89.06%] [G loss: 3.440008]\n",
      "epoch:21 step:17027 [D loss: 0.286660, acc.: 90.62%] [G loss: 2.627013]\n",
      "epoch:21 step:17028 [D loss: 0.404357, acc.: 83.59%] [G loss: 2.702260]\n",
      "epoch:21 step:17029 [D loss: 0.394359, acc.: 83.59%] [G loss: 2.257295]\n",
      "epoch:21 step:17030 [D loss: 0.353013, acc.: 87.50%] [G loss: 3.011001]\n",
      "epoch:21 step:17031 [D loss: 0.466702, acc.: 78.12%] [G loss: 2.686751]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:21 step:17032 [D loss: 0.353719, acc.: 82.03%] [G loss: 2.650664]\n",
      "epoch:21 step:17033 [D loss: 0.322448, acc.: 85.16%] [G loss: 3.827731]\n",
      "epoch:21 step:17034 [D loss: 0.293253, acc.: 83.59%] [G loss: 3.934192]\n",
      "epoch:21 step:17035 [D loss: 0.334175, acc.: 86.72%] [G loss: 2.942981]\n",
      "epoch:21 step:17036 [D loss: 0.344843, acc.: 85.16%] [G loss: 2.479828]\n",
      "epoch:21 step:17037 [D loss: 0.355930, acc.: 80.47%] [G loss: 2.560053]\n",
      "epoch:21 step:17038 [D loss: 0.380226, acc.: 80.47%] [G loss: 2.815109]\n",
      "epoch:21 step:17039 [D loss: 0.386789, acc.: 82.81%] [G loss: 2.590088]\n",
      "epoch:21 step:17040 [D loss: 0.320438, acc.: 87.50%] [G loss: 2.339223]\n",
      "epoch:21 step:17041 [D loss: 0.341272, acc.: 84.38%] [G loss: 2.448990]\n",
      "epoch:21 step:17042 [D loss: 0.354093, acc.: 83.59%] [G loss: 2.325405]\n",
      "epoch:21 step:17043 [D loss: 0.361819, acc.: 86.72%] [G loss: 2.577617]\n",
      "epoch:21 step:17044 [D loss: 0.361390, acc.: 85.94%] [G loss: 3.033832]\n",
      "epoch:21 step:17045 [D loss: 0.241451, acc.: 91.41%] [G loss: 3.294420]\n",
      "epoch:21 step:17046 [D loss: 0.240182, acc.: 92.19%] [G loss: 3.948655]\n",
      "epoch:21 step:17047 [D loss: 0.261386, acc.: 89.84%] [G loss: 4.618023]\n",
      "epoch:21 step:17048 [D loss: 0.233604, acc.: 87.50%] [G loss: 3.575027]\n",
      "epoch:21 step:17049 [D loss: 0.289735, acc.: 86.72%] [G loss: 2.782066]\n",
      "epoch:21 step:17050 [D loss: 0.297734, acc.: 85.94%] [G loss: 3.237894]\n",
      "epoch:21 step:17051 [D loss: 0.340560, acc.: 81.25%] [G loss: 2.976804]\n",
      "epoch:21 step:17052 [D loss: 0.262816, acc.: 88.28%] [G loss: 3.259885]\n",
      "epoch:21 step:17053 [D loss: 0.333245, acc.: 84.38%] [G loss: 3.055688]\n",
      "epoch:21 step:17054 [D loss: 0.329470, acc.: 85.94%] [G loss: 5.321651]\n",
      "epoch:21 step:17055 [D loss: 0.307019, acc.: 89.84%] [G loss: 2.893284]\n",
      "epoch:21 step:17056 [D loss: 0.372838, acc.: 82.81%] [G loss: 3.787178]\n",
      "epoch:21 step:17057 [D loss: 0.402156, acc.: 79.69%] [G loss: 3.617563]\n",
      "epoch:21 step:17058 [D loss: 0.498120, acc.: 75.00%] [G loss: 3.519992]\n",
      "epoch:21 step:17059 [D loss: 0.363502, acc.: 80.47%] [G loss: 3.571357]\n",
      "epoch:21 step:17060 [D loss: 0.331548, acc.: 84.38%] [G loss: 3.663527]\n",
      "epoch:21 step:17061 [D loss: 0.368554, acc.: 82.81%] [G loss: 3.010210]\n",
      "epoch:21 step:17062 [D loss: 0.373557, acc.: 80.47%] [G loss: 2.892752]\n",
      "epoch:21 step:17063 [D loss: 0.289814, acc.: 84.38%] [G loss: 3.878627]\n",
      "epoch:21 step:17064 [D loss: 0.253995, acc.: 89.84%] [G loss: 4.509383]\n",
      "epoch:21 step:17065 [D loss: 0.315234, acc.: 85.94%] [G loss: 2.867222]\n",
      "epoch:21 step:17066 [D loss: 0.327996, acc.: 86.72%] [G loss: 2.911935]\n",
      "epoch:21 step:17067 [D loss: 0.292140, acc.: 91.41%] [G loss: 4.189411]\n",
      "epoch:21 step:17068 [D loss: 0.235063, acc.: 90.62%] [G loss: 4.617825]\n",
      "epoch:21 step:17069 [D loss: 0.259233, acc.: 85.94%] [G loss: 4.374363]\n",
      "epoch:21 step:17070 [D loss: 0.303447, acc.: 85.16%] [G loss: 3.193270]\n",
      "epoch:21 step:17071 [D loss: 0.343756, acc.: 85.94%] [G loss: 3.254821]\n",
      "epoch:21 step:17072 [D loss: 0.280003, acc.: 88.28%] [G loss: 6.041626]\n",
      "epoch:21 step:17073 [D loss: 0.274600, acc.: 85.16%] [G loss: 4.555752]\n",
      "epoch:21 step:17074 [D loss: 0.297846, acc.: 87.50%] [G loss: 3.651774]\n",
      "epoch:21 step:17075 [D loss: 0.199698, acc.: 92.97%] [G loss: 4.888782]\n",
      "epoch:21 step:17076 [D loss: 0.302871, acc.: 85.16%] [G loss: 3.578499]\n",
      "epoch:21 step:17077 [D loss: 0.364689, acc.: 84.38%] [G loss: 2.723531]\n",
      "epoch:21 step:17078 [D loss: 0.265528, acc.: 90.62%] [G loss: 5.646933]\n",
      "epoch:21 step:17079 [D loss: 0.207103, acc.: 91.41%] [G loss: 5.962337]\n",
      "epoch:21 step:17080 [D loss: 0.167528, acc.: 96.09%] [G loss: 4.618340]\n",
      "epoch:21 step:17081 [D loss: 0.301955, acc.: 86.72%] [G loss: 3.694799]\n",
      "epoch:21 step:17082 [D loss: 0.280661, acc.: 90.62%] [G loss: 2.830446]\n",
      "epoch:21 step:17083 [D loss: 0.253219, acc.: 86.72%] [G loss: 3.834406]\n",
      "epoch:21 step:17084 [D loss: 0.249257, acc.: 92.19%] [G loss: 2.857912]\n",
      "epoch:21 step:17085 [D loss: 0.281834, acc.: 88.28%] [G loss: 4.104554]\n",
      "epoch:21 step:17086 [D loss: 0.392677, acc.: 83.59%] [G loss: 5.504263]\n",
      "epoch:21 step:17087 [D loss: 0.376373, acc.: 82.03%] [G loss: 4.028986]\n",
      "epoch:21 step:17088 [D loss: 0.264647, acc.: 83.59%] [G loss: 4.313489]\n",
      "epoch:21 step:17089 [D loss: 0.348383, acc.: 85.94%] [G loss: 5.655900]\n",
      "epoch:21 step:17090 [D loss: 0.268867, acc.: 89.06%] [G loss: 3.340380]\n",
      "epoch:21 step:17091 [D loss: 0.360359, acc.: 85.16%] [G loss: 3.511510]\n",
      "epoch:21 step:17092 [D loss: 0.318422, acc.: 85.94%] [G loss: 3.443939]\n",
      "epoch:21 step:17093 [D loss: 0.270062, acc.: 86.72%] [G loss: 4.770625]\n",
      "epoch:21 step:17094 [D loss: 0.430773, acc.: 81.25%] [G loss: 4.000294]\n",
      "epoch:21 step:17095 [D loss: 0.253880, acc.: 89.06%] [G loss: 5.385308]\n",
      "epoch:21 step:17096 [D loss: 0.303434, acc.: 83.59%] [G loss: 3.736177]\n",
      "epoch:21 step:17097 [D loss: 0.288115, acc.: 86.72%] [G loss: 4.260786]\n",
      "epoch:21 step:17098 [D loss: 0.300655, acc.: 88.28%] [G loss: 3.261107]\n",
      "epoch:21 step:17099 [D loss: 0.358989, acc.: 83.59%] [G loss: 3.576307]\n",
      "epoch:21 step:17100 [D loss: 0.237497, acc.: 94.53%] [G loss: 4.328187]\n",
      "epoch:21 step:17101 [D loss: 0.367464, acc.: 82.81%] [G loss: 3.150832]\n",
      "epoch:21 step:17102 [D loss: 0.302542, acc.: 85.94%] [G loss: 2.818571]\n",
      "epoch:21 step:17103 [D loss: 0.317773, acc.: 84.38%] [G loss: 3.610501]\n",
      "epoch:21 step:17104 [D loss: 0.399909, acc.: 79.69%] [G loss: 4.182842]\n",
      "epoch:21 step:17105 [D loss: 0.297610, acc.: 85.16%] [G loss: 2.781920]\n",
      "epoch:21 step:17106 [D loss: 0.327133, acc.: 84.38%] [G loss: 2.256124]\n",
      "epoch:21 step:17107 [D loss: 0.374763, acc.: 82.81%] [G loss: 3.296353]\n",
      "epoch:21 step:17108 [D loss: 0.410670, acc.: 81.25%] [G loss: 2.500165]\n",
      "epoch:21 step:17109 [D loss: 0.308232, acc.: 83.59%] [G loss: 3.386556]\n",
      "epoch:21 step:17110 [D loss: 0.345520, acc.: 84.38%] [G loss: 3.547346]\n",
      "epoch:21 step:17111 [D loss: 0.210719, acc.: 92.97%] [G loss: 3.464913]\n",
      "epoch:21 step:17112 [D loss: 0.334018, acc.: 84.38%] [G loss: 3.901132]\n",
      "epoch:21 step:17113 [D loss: 0.376924, acc.: 82.03%] [G loss: 3.247740]\n",
      "epoch:21 step:17114 [D loss: 0.349407, acc.: 83.59%] [G loss: 4.355159]\n",
      "epoch:21 step:17115 [D loss: 0.312551, acc.: 84.38%] [G loss: 3.655171]\n",
      "epoch:21 step:17116 [D loss: 0.426708, acc.: 82.03%] [G loss: 2.913909]\n",
      "epoch:21 step:17117 [D loss: 0.347633, acc.: 83.59%] [G loss: 2.893165]\n",
      "epoch:21 step:17118 [D loss: 0.236788, acc.: 91.41%] [G loss: 3.117157]\n",
      "epoch:21 step:17119 [D loss: 0.355597, acc.: 82.81%] [G loss: 3.283206]\n",
      "epoch:21 step:17120 [D loss: 0.361827, acc.: 84.38%] [G loss: 3.080590]\n",
      "epoch:21 step:17121 [D loss: 0.283990, acc.: 89.84%] [G loss: 3.265808]\n",
      "epoch:21 step:17122 [D loss: 0.331422, acc.: 87.50%] [G loss: 3.846751]\n",
      "epoch:21 step:17123 [D loss: 0.265171, acc.: 85.94%] [G loss: 3.085124]\n",
      "epoch:21 step:17124 [D loss: 0.311726, acc.: 85.94%] [G loss: 6.693821]\n",
      "epoch:21 step:17125 [D loss: 0.292849, acc.: 88.28%] [G loss: 4.867125]\n",
      "epoch:21 step:17126 [D loss: 0.246800, acc.: 90.62%] [G loss: 6.350690]\n",
      "epoch:21 step:17127 [D loss: 0.236385, acc.: 91.41%] [G loss: 5.629809]\n",
      "epoch:21 step:17128 [D loss: 0.334536, acc.: 82.03%] [G loss: 3.517859]\n",
      "epoch:21 step:17129 [D loss: 0.339732, acc.: 82.81%] [G loss: 4.569016]\n",
      "epoch:21 step:17130 [D loss: 0.312613, acc.: 85.94%] [G loss: 3.762318]\n",
      "epoch:21 step:17131 [D loss: 0.273219, acc.: 89.06%] [G loss: 3.489356]\n",
      "epoch:21 step:17132 [D loss: 0.227383, acc.: 91.41%] [G loss: 4.185222]\n",
      "epoch:21 step:17133 [D loss: 0.294933, acc.: 89.06%] [G loss: 3.632630]\n",
      "epoch:21 step:17134 [D loss: 0.369202, acc.: 87.50%] [G loss: 2.978546]\n",
      "epoch:21 step:17135 [D loss: 0.288834, acc.: 88.28%] [G loss: 3.134073]\n",
      "epoch:21 step:17136 [D loss: 0.253685, acc.: 91.41%] [G loss: 2.231427]\n",
      "epoch:21 step:17137 [D loss: 0.268851, acc.: 91.41%] [G loss: 2.871793]\n",
      "epoch:21 step:17138 [D loss: 0.233589, acc.: 90.62%] [G loss: 4.279494]\n",
      "epoch:21 step:17139 [D loss: 0.289146, acc.: 85.16%] [G loss: 4.470410]\n",
      "epoch:21 step:17140 [D loss: 0.325258, acc.: 86.72%] [G loss: 3.356785]\n",
      "epoch:21 step:17141 [D loss: 0.416925, acc.: 80.47%] [G loss: 3.220564]\n",
      "epoch:21 step:17142 [D loss: 0.378408, acc.: 85.16%] [G loss: 3.056549]\n",
      "epoch:21 step:17143 [D loss: 0.308531, acc.: 83.59%] [G loss: 3.038629]\n",
      "epoch:21 step:17144 [D loss: 0.224843, acc.: 92.97%] [G loss: 3.297675]\n",
      "epoch:21 step:17145 [D loss: 0.226793, acc.: 89.84%] [G loss: 3.671007]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:21 step:17146 [D loss: 0.359747, acc.: 81.25%] [G loss: 3.555960]\n",
      "epoch:21 step:17147 [D loss: 0.343802, acc.: 85.16%] [G loss: 2.849541]\n",
      "epoch:21 step:17148 [D loss: 0.287025, acc.: 85.16%] [G loss: 2.717843]\n",
      "epoch:21 step:17149 [D loss: 0.399284, acc.: 83.59%] [G loss: 2.679905]\n",
      "epoch:21 step:17150 [D loss: 0.332531, acc.: 85.94%] [G loss: 3.097486]\n",
      "epoch:21 step:17151 [D loss: 0.347076, acc.: 85.16%] [G loss: 3.566889]\n",
      "epoch:21 step:17152 [D loss: 0.450538, acc.: 82.03%] [G loss: 4.417604]\n",
      "epoch:21 step:17153 [D loss: 0.638279, acc.: 80.47%] [G loss: 9.245963]\n",
      "epoch:21 step:17154 [D loss: 1.899890, acc.: 50.78%] [G loss: 3.754452]\n",
      "epoch:21 step:17155 [D loss: 0.653245, acc.: 74.22%] [G loss: 2.239709]\n",
      "epoch:21 step:17156 [D loss: 0.493302, acc.: 78.12%] [G loss: 3.934093]\n",
      "epoch:21 step:17157 [D loss: 0.413447, acc.: 82.03%] [G loss: 3.509885]\n",
      "epoch:21 step:17158 [D loss: 0.249375, acc.: 89.06%] [G loss: 3.633811]\n",
      "epoch:21 step:17159 [D loss: 0.542873, acc.: 77.34%] [G loss: 2.803220]\n",
      "epoch:21 step:17160 [D loss: 0.273501, acc.: 90.62%] [G loss: 3.148004]\n",
      "epoch:21 step:17161 [D loss: 0.395318, acc.: 82.03%] [G loss: 3.366186]\n",
      "epoch:21 step:17162 [D loss: 0.445559, acc.: 81.25%] [G loss: 3.257403]\n",
      "epoch:21 step:17163 [D loss: 0.499510, acc.: 74.22%] [G loss: 3.418649]\n",
      "epoch:21 step:17164 [D loss: 0.298727, acc.: 84.38%] [G loss: 2.701668]\n",
      "epoch:21 step:17165 [D loss: 0.357504, acc.: 85.94%] [G loss: 2.646994]\n",
      "epoch:21 step:17166 [D loss: 0.341920, acc.: 83.59%] [G loss: 2.802384]\n",
      "epoch:21 step:17167 [D loss: 0.320433, acc.: 82.81%] [G loss: 2.804668]\n",
      "epoch:21 step:17168 [D loss: 0.346822, acc.: 89.06%] [G loss: 2.502768]\n",
      "epoch:21 step:17169 [D loss: 0.392439, acc.: 78.12%] [G loss: 2.522849]\n",
      "epoch:21 step:17170 [D loss: 0.258106, acc.: 90.62%] [G loss: 3.244242]\n",
      "epoch:21 step:17171 [D loss: 0.319293, acc.: 84.38%] [G loss: 4.032001]\n",
      "epoch:21 step:17172 [D loss: 0.398807, acc.: 84.38%] [G loss: 3.416161]\n",
      "epoch:21 step:17173 [D loss: 0.461152, acc.: 75.78%] [G loss: 2.988487]\n",
      "epoch:21 step:17174 [D loss: 0.325539, acc.: 89.06%] [G loss: 3.948356]\n",
      "epoch:21 step:17175 [D loss: 0.216951, acc.: 89.06%] [G loss: 3.351393]\n",
      "epoch:21 step:17176 [D loss: 0.369530, acc.: 82.03%] [G loss: 3.237483]\n",
      "epoch:21 step:17177 [D loss: 0.259110, acc.: 89.06%] [G loss: 3.309752]\n",
      "epoch:21 step:17178 [D loss: 0.382464, acc.: 81.25%] [G loss: 2.645533]\n",
      "epoch:21 step:17179 [D loss: 0.241979, acc.: 92.19%] [G loss: 3.312480]\n",
      "epoch:21 step:17180 [D loss: 0.315605, acc.: 88.28%] [G loss: 2.408456]\n",
      "epoch:21 step:17181 [D loss: 0.244700, acc.: 89.84%] [G loss: 2.641206]\n",
      "epoch:21 step:17182 [D loss: 0.325098, acc.: 85.16%] [G loss: 2.352473]\n",
      "epoch:22 step:17183 [D loss: 0.245948, acc.: 90.62%] [G loss: 2.655528]\n",
      "epoch:22 step:17184 [D loss: 0.334842, acc.: 86.72%] [G loss: 2.359647]\n",
      "epoch:22 step:17185 [D loss: 0.394359, acc.: 82.81%] [G loss: 2.661063]\n",
      "epoch:22 step:17186 [D loss: 0.286284, acc.: 84.38%] [G loss: 3.278828]\n",
      "epoch:22 step:17187 [D loss: 0.290220, acc.: 83.59%] [G loss: 2.653328]\n",
      "epoch:22 step:17188 [D loss: 0.326531, acc.: 85.16%] [G loss: 2.899640]\n",
      "epoch:22 step:17189 [D loss: 0.332468, acc.: 84.38%] [G loss: 3.377696]\n",
      "epoch:22 step:17190 [D loss: 0.246800, acc.: 88.28%] [G loss: 2.793970]\n",
      "epoch:22 step:17191 [D loss: 0.423348, acc.: 79.69%] [G loss: 2.605010]\n",
      "epoch:22 step:17192 [D loss: 0.360606, acc.: 82.81%] [G loss: 2.999291]\n",
      "epoch:22 step:17193 [D loss: 0.344652, acc.: 84.38%] [G loss: 2.724711]\n",
      "epoch:22 step:17194 [D loss: 0.247452, acc.: 88.28%] [G loss: 2.846972]\n",
      "epoch:22 step:17195 [D loss: 0.309613, acc.: 87.50%] [G loss: 2.653832]\n",
      "epoch:22 step:17196 [D loss: 0.250033, acc.: 90.62%] [G loss: 3.075312]\n",
      "epoch:22 step:17197 [D loss: 0.303317, acc.: 85.16%] [G loss: 3.272519]\n",
      "epoch:22 step:17198 [D loss: 0.306080, acc.: 87.50%] [G loss: 2.876000]\n",
      "epoch:22 step:17199 [D loss: 0.254756, acc.: 88.28%] [G loss: 3.604517]\n",
      "epoch:22 step:17200 [D loss: 0.314545, acc.: 88.28%] [G loss: 2.694994]\n",
      "##############\n",
      "[0.86274155 0.8516519  0.79838497 0.79451824 0.76382588 0.82355202\n",
      " 0.86934574 0.81983286 0.80906212 0.81474944]\n",
      "##########\n",
      "epoch:22 step:17201 [D loss: 0.251004, acc.: 89.06%] [G loss: 2.782661]\n",
      "epoch:22 step:17202 [D loss: 0.392182, acc.: 85.16%] [G loss: 2.731606]\n",
      "epoch:22 step:17203 [D loss: 0.339466, acc.: 82.81%] [G loss: 3.945726]\n",
      "epoch:22 step:17204 [D loss: 0.241794, acc.: 92.19%] [G loss: 5.035038]\n",
      "epoch:22 step:17205 [D loss: 0.254804, acc.: 85.94%] [G loss: 3.504767]\n",
      "epoch:22 step:17206 [D loss: 0.269190, acc.: 88.28%] [G loss: 2.818439]\n",
      "epoch:22 step:17207 [D loss: 0.244657, acc.: 89.84%] [G loss: 3.560677]\n",
      "epoch:22 step:17208 [D loss: 0.330950, acc.: 85.94%] [G loss: 3.895004]\n",
      "epoch:22 step:17209 [D loss: 0.358754, acc.: 85.94%] [G loss: 2.796452]\n",
      "epoch:22 step:17210 [D loss: 0.300609, acc.: 89.06%] [G loss: 2.693274]\n",
      "epoch:22 step:17211 [D loss: 0.224280, acc.: 92.97%] [G loss: 3.074110]\n",
      "epoch:22 step:17212 [D loss: 0.146727, acc.: 94.53%] [G loss: 3.816715]\n",
      "epoch:22 step:17213 [D loss: 0.251307, acc.: 91.41%] [G loss: 2.892430]\n",
      "epoch:22 step:17214 [D loss: 0.201191, acc.: 93.75%] [G loss: 3.197921]\n",
      "epoch:22 step:17215 [D loss: 0.187433, acc.: 96.09%] [G loss: 3.778640]\n",
      "epoch:22 step:17216 [D loss: 0.299524, acc.: 85.94%] [G loss: 3.662028]\n",
      "epoch:22 step:17217 [D loss: 0.249185, acc.: 86.72%] [G loss: 2.677160]\n",
      "epoch:22 step:17218 [D loss: 0.317941, acc.: 87.50%] [G loss: 3.186253]\n",
      "epoch:22 step:17219 [D loss: 0.324798, acc.: 82.03%] [G loss: 4.858023]\n",
      "epoch:22 step:17220 [D loss: 0.257523, acc.: 90.62%] [G loss: 4.586107]\n",
      "epoch:22 step:17221 [D loss: 0.350716, acc.: 83.59%] [G loss: 3.709692]\n",
      "epoch:22 step:17222 [D loss: 0.254819, acc.: 90.62%] [G loss: 3.506628]\n",
      "epoch:22 step:17223 [D loss: 0.517633, acc.: 77.34%] [G loss: 5.179502]\n",
      "epoch:22 step:17224 [D loss: 0.748621, acc.: 75.00%] [G loss: 5.303747]\n",
      "epoch:22 step:17225 [D loss: 0.748353, acc.: 67.97%] [G loss: 9.290174]\n",
      "epoch:22 step:17226 [D loss: 2.376440, acc.: 46.88%] [G loss: 5.394754]\n",
      "epoch:22 step:17227 [D loss: 0.730513, acc.: 75.00%] [G loss: 4.028738]\n",
      "epoch:22 step:17228 [D loss: 0.462392, acc.: 80.47%] [G loss: 4.708125]\n",
      "epoch:22 step:17229 [D loss: 0.377664, acc.: 85.16%] [G loss: 5.872427]\n",
      "epoch:22 step:17230 [D loss: 0.370542, acc.: 82.81%] [G loss: 2.836176]\n",
      "epoch:22 step:17231 [D loss: 0.236053, acc.: 88.28%] [G loss: 4.072758]\n",
      "epoch:22 step:17232 [D loss: 0.495200, acc.: 74.22%] [G loss: 2.361374]\n",
      "epoch:22 step:17233 [D loss: 0.314376, acc.: 88.28%] [G loss: 3.509032]\n",
      "epoch:22 step:17234 [D loss: 0.267774, acc.: 90.62%] [G loss: 2.917056]\n",
      "epoch:22 step:17235 [D loss: 0.370535, acc.: 84.38%] [G loss: 2.583930]\n",
      "epoch:22 step:17236 [D loss: 0.341652, acc.: 84.38%] [G loss: 3.127488]\n",
      "epoch:22 step:17237 [D loss: 0.412205, acc.: 81.25%] [G loss: 1.840022]\n",
      "epoch:22 step:17238 [D loss: 0.338114, acc.: 86.72%] [G loss: 1.943236]\n",
      "epoch:22 step:17239 [D loss: 0.356286, acc.: 85.16%] [G loss: 2.257233]\n",
      "epoch:22 step:17240 [D loss: 0.287986, acc.: 89.06%] [G loss: 2.362367]\n",
      "epoch:22 step:17241 [D loss: 0.288060, acc.: 87.50%] [G loss: 2.246044]\n",
      "epoch:22 step:17242 [D loss: 0.280736, acc.: 87.50%] [G loss: 2.866432]\n",
      "epoch:22 step:17243 [D loss: 0.296147, acc.: 86.72%] [G loss: 3.155829]\n",
      "epoch:22 step:17244 [D loss: 0.292151, acc.: 88.28%] [G loss: 2.868111]\n",
      "epoch:22 step:17245 [D loss: 0.275264, acc.: 87.50%] [G loss: 4.124006]\n",
      "epoch:22 step:17246 [D loss: 0.263404, acc.: 89.06%] [G loss: 2.711700]\n",
      "epoch:22 step:17247 [D loss: 0.234622, acc.: 92.19%] [G loss: 3.993315]\n",
      "epoch:22 step:17248 [D loss: 0.271664, acc.: 88.28%] [G loss: 2.908044]\n",
      "epoch:22 step:17249 [D loss: 0.296735, acc.: 85.94%] [G loss: 3.711552]\n",
      "epoch:22 step:17250 [D loss: 0.482201, acc.: 77.34%] [G loss: 2.262328]\n",
      "epoch:22 step:17251 [D loss: 0.310200, acc.: 83.59%] [G loss: 2.491550]\n",
      "epoch:22 step:17252 [D loss: 0.301775, acc.: 84.38%] [G loss: 2.888239]\n",
      "epoch:22 step:17253 [D loss: 0.427740, acc.: 80.47%] [G loss: 3.135648]\n",
      "epoch:22 step:17254 [D loss: 0.283051, acc.: 90.62%] [G loss: 3.247304]\n",
      "epoch:22 step:17255 [D loss: 0.310774, acc.: 87.50%] [G loss: 4.394301]\n",
      "epoch:22 step:17256 [D loss: 0.367644, acc.: 85.94%] [G loss: 2.640347]\n",
      "epoch:22 step:17257 [D loss: 0.318437, acc.: 85.94%] [G loss: 3.349893]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:22 step:17258 [D loss: 0.386223, acc.: 81.25%] [G loss: 3.441903]\n",
      "epoch:22 step:17259 [D loss: 0.323784, acc.: 84.38%] [G loss: 3.237383]\n",
      "epoch:22 step:17260 [D loss: 0.221229, acc.: 92.19%] [G loss: 4.888178]\n",
      "epoch:22 step:17261 [D loss: 0.267445, acc.: 87.50%] [G loss: 2.663456]\n",
      "epoch:22 step:17262 [D loss: 0.283329, acc.: 89.84%] [G loss: 3.161132]\n",
      "epoch:22 step:17263 [D loss: 0.429669, acc.: 81.25%] [G loss: 2.609947]\n",
      "epoch:22 step:17264 [D loss: 0.275633, acc.: 89.84%] [G loss: 2.155878]\n",
      "epoch:22 step:17265 [D loss: 0.269278, acc.: 89.84%] [G loss: 2.184229]\n",
      "epoch:22 step:17266 [D loss: 0.182776, acc.: 94.53%] [G loss: 2.579151]\n",
      "epoch:22 step:17267 [D loss: 0.310304, acc.: 89.06%] [G loss: 2.749980]\n",
      "epoch:22 step:17268 [D loss: 0.242251, acc.: 91.41%] [G loss: 3.028607]\n",
      "epoch:22 step:17269 [D loss: 0.353125, acc.: 87.50%] [G loss: 2.544073]\n",
      "epoch:22 step:17270 [D loss: 0.330968, acc.: 85.16%] [G loss: 3.226542]\n",
      "epoch:22 step:17271 [D loss: 0.317085, acc.: 85.94%] [G loss: 2.605296]\n",
      "epoch:22 step:17272 [D loss: 0.384460, acc.: 79.69%] [G loss: 2.007559]\n",
      "epoch:22 step:17273 [D loss: 0.252673, acc.: 88.28%] [G loss: 2.626404]\n",
      "epoch:22 step:17274 [D loss: 0.381538, acc.: 81.25%] [G loss: 2.519303]\n",
      "epoch:22 step:17275 [D loss: 0.368156, acc.: 80.47%] [G loss: 2.310713]\n",
      "epoch:22 step:17276 [D loss: 0.298707, acc.: 89.84%] [G loss: 2.826371]\n",
      "epoch:22 step:17277 [D loss: 0.395895, acc.: 82.03%] [G loss: 2.910890]\n",
      "epoch:22 step:17278 [D loss: 0.313119, acc.: 84.38%] [G loss: 3.192866]\n",
      "epoch:22 step:17279 [D loss: 0.379194, acc.: 82.81%] [G loss: 2.285065]\n",
      "epoch:22 step:17280 [D loss: 0.306262, acc.: 85.16%] [G loss: 3.117579]\n",
      "epoch:22 step:17281 [D loss: 0.305122, acc.: 88.28%] [G loss: 2.604623]\n",
      "epoch:22 step:17282 [D loss: 0.344183, acc.: 82.81%] [G loss: 2.260062]\n",
      "epoch:22 step:17283 [D loss: 0.335541, acc.: 80.47%] [G loss: 3.531651]\n",
      "epoch:22 step:17284 [D loss: 0.353673, acc.: 82.81%] [G loss: 2.355923]\n",
      "epoch:22 step:17285 [D loss: 0.347260, acc.: 83.59%] [G loss: 3.130660]\n",
      "epoch:22 step:17286 [D loss: 0.280948, acc.: 88.28%] [G loss: 3.001309]\n",
      "epoch:22 step:17287 [D loss: 0.206412, acc.: 92.97%] [G loss: 3.379404]\n",
      "epoch:22 step:17288 [D loss: 0.331742, acc.: 82.81%] [G loss: 2.899837]\n",
      "epoch:22 step:17289 [D loss: 0.368691, acc.: 82.81%] [G loss: 2.592584]\n",
      "epoch:22 step:17290 [D loss: 0.216061, acc.: 92.19%] [G loss: 3.045345]\n",
      "epoch:22 step:17291 [D loss: 0.324283, acc.: 85.94%] [G loss: 3.220773]\n",
      "epoch:22 step:17292 [D loss: 0.304719, acc.: 85.16%] [G loss: 2.932197]\n",
      "epoch:22 step:17293 [D loss: 0.269254, acc.: 89.06%] [G loss: 2.160231]\n",
      "epoch:22 step:17294 [D loss: 0.312444, acc.: 89.06%] [G loss: 2.817813]\n",
      "epoch:22 step:17295 [D loss: 0.452987, acc.: 82.81%] [G loss: 2.998271]\n",
      "epoch:22 step:17296 [D loss: 0.441076, acc.: 77.34%] [G loss: 1.962584]\n",
      "epoch:22 step:17297 [D loss: 0.310357, acc.: 85.94%] [G loss: 3.365283]\n",
      "epoch:22 step:17298 [D loss: 0.272419, acc.: 91.41%] [G loss: 3.710822]\n",
      "epoch:22 step:17299 [D loss: 0.233434, acc.: 92.19%] [G loss: 4.641707]\n",
      "epoch:22 step:17300 [D loss: 0.206756, acc.: 92.19%] [G loss: 4.429358]\n",
      "epoch:22 step:17301 [D loss: 0.291106, acc.: 85.94%] [G loss: 2.983806]\n",
      "epoch:22 step:17302 [D loss: 0.315581, acc.: 89.84%] [G loss: 3.945365]\n",
      "epoch:22 step:17303 [D loss: 0.256285, acc.: 88.28%] [G loss: 4.264662]\n",
      "epoch:22 step:17304 [D loss: 0.293792, acc.: 84.38%] [G loss: 3.187841]\n",
      "epoch:22 step:17305 [D loss: 0.300099, acc.: 86.72%] [G loss: 2.736717]\n",
      "epoch:22 step:17306 [D loss: 0.316420, acc.: 85.94%] [G loss: 3.199081]\n",
      "epoch:22 step:17307 [D loss: 0.350846, acc.: 85.16%] [G loss: 2.855704]\n",
      "epoch:22 step:17308 [D loss: 0.262336, acc.: 89.84%] [G loss: 3.177990]\n",
      "epoch:22 step:17309 [D loss: 0.246251, acc.: 88.28%] [G loss: 2.808740]\n",
      "epoch:22 step:17310 [D loss: 0.292959, acc.: 85.16%] [G loss: 2.352680]\n",
      "epoch:22 step:17311 [D loss: 0.323855, acc.: 88.28%] [G loss: 3.294901]\n",
      "epoch:22 step:17312 [D loss: 0.375436, acc.: 85.94%] [G loss: 3.674061]\n",
      "epoch:22 step:17313 [D loss: 0.537472, acc.: 79.69%] [G loss: 6.384112]\n",
      "epoch:22 step:17314 [D loss: 0.954061, acc.: 70.31%] [G loss: 6.431786]\n",
      "epoch:22 step:17315 [D loss: 0.572249, acc.: 72.66%] [G loss: 4.423691]\n",
      "epoch:22 step:17316 [D loss: 0.317220, acc.: 88.28%] [G loss: 4.949934]\n",
      "epoch:22 step:17317 [D loss: 0.372282, acc.: 84.38%] [G loss: 3.484714]\n",
      "epoch:22 step:17318 [D loss: 0.227040, acc.: 92.97%] [G loss: 3.176140]\n",
      "epoch:22 step:17319 [D loss: 0.243428, acc.: 89.84%] [G loss: 3.956762]\n",
      "epoch:22 step:17320 [D loss: 0.304763, acc.: 87.50%] [G loss: 4.137220]\n",
      "epoch:22 step:17321 [D loss: 0.326620, acc.: 82.81%] [G loss: 3.215350]\n",
      "epoch:22 step:17322 [D loss: 0.210299, acc.: 92.19%] [G loss: 4.109992]\n",
      "epoch:22 step:17323 [D loss: 0.440047, acc.: 85.16%] [G loss: 3.366215]\n",
      "epoch:22 step:17324 [D loss: 0.378947, acc.: 85.16%] [G loss: 3.151084]\n",
      "epoch:22 step:17325 [D loss: 0.380692, acc.: 80.47%] [G loss: 2.905457]\n",
      "epoch:22 step:17326 [D loss: 0.261330, acc.: 89.06%] [G loss: 3.226727]\n",
      "epoch:22 step:17327 [D loss: 0.419856, acc.: 82.03%] [G loss: 2.911540]\n",
      "epoch:22 step:17328 [D loss: 0.280715, acc.: 88.28%] [G loss: 3.003233]\n",
      "epoch:22 step:17329 [D loss: 0.303834, acc.: 86.72%] [G loss: 2.739033]\n",
      "epoch:22 step:17330 [D loss: 0.406740, acc.: 79.69%] [G loss: 2.897657]\n",
      "epoch:22 step:17331 [D loss: 0.293235, acc.: 85.94%] [G loss: 2.801848]\n",
      "epoch:22 step:17332 [D loss: 0.325662, acc.: 84.38%] [G loss: 2.962552]\n",
      "epoch:22 step:17333 [D loss: 0.282001, acc.: 88.28%] [G loss: 3.146783]\n",
      "epoch:22 step:17334 [D loss: 0.436523, acc.: 81.25%] [G loss: 3.547197]\n",
      "epoch:22 step:17335 [D loss: 0.270366, acc.: 86.72%] [G loss: 4.131501]\n",
      "epoch:22 step:17336 [D loss: 0.348520, acc.: 87.50%] [G loss: 3.815743]\n",
      "epoch:22 step:17337 [D loss: 0.259186, acc.: 89.06%] [G loss: 3.249416]\n",
      "epoch:22 step:17338 [D loss: 0.276217, acc.: 84.38%] [G loss: 3.683939]\n",
      "epoch:22 step:17339 [D loss: 0.349782, acc.: 85.94%] [G loss: 3.126957]\n",
      "epoch:22 step:17340 [D loss: 0.234771, acc.: 89.84%] [G loss: 4.219484]\n",
      "epoch:22 step:17341 [D loss: 0.289217, acc.: 85.94%] [G loss: 3.334428]\n",
      "epoch:22 step:17342 [D loss: 0.380853, acc.: 82.81%] [G loss: 3.234827]\n",
      "epoch:22 step:17343 [D loss: 0.264998, acc.: 87.50%] [G loss: 6.575090]\n",
      "epoch:22 step:17344 [D loss: 0.174016, acc.: 90.62%] [G loss: 4.500309]\n",
      "epoch:22 step:17345 [D loss: 0.270403, acc.: 87.50%] [G loss: 4.011178]\n",
      "epoch:22 step:17346 [D loss: 0.200401, acc.: 91.41%] [G loss: 4.389524]\n",
      "epoch:22 step:17347 [D loss: 0.258467, acc.: 89.06%] [G loss: 2.814850]\n",
      "epoch:22 step:17348 [D loss: 0.242851, acc.: 90.62%] [G loss: 3.090130]\n",
      "epoch:22 step:17349 [D loss: 0.393640, acc.: 81.25%] [G loss: 3.504480]\n",
      "epoch:22 step:17350 [D loss: 0.328540, acc.: 82.81%] [G loss: 3.280834]\n",
      "epoch:22 step:17351 [D loss: 0.282287, acc.: 86.72%] [G loss: 3.621641]\n",
      "epoch:22 step:17352 [D loss: 0.421886, acc.: 86.72%] [G loss: 2.532196]\n",
      "epoch:22 step:17353 [D loss: 0.300563, acc.: 85.16%] [G loss: 3.352115]\n",
      "epoch:22 step:17354 [D loss: 0.359351, acc.: 84.38%] [G loss: 3.219734]\n",
      "epoch:22 step:17355 [D loss: 0.406683, acc.: 80.47%] [G loss: 2.672018]\n",
      "epoch:22 step:17356 [D loss: 0.340208, acc.: 83.59%] [G loss: 2.721926]\n",
      "epoch:22 step:17357 [D loss: 0.441252, acc.: 83.59%] [G loss: 3.256608]\n",
      "epoch:22 step:17358 [D loss: 0.238872, acc.: 88.28%] [G loss: 3.089316]\n",
      "epoch:22 step:17359 [D loss: 0.387363, acc.: 82.81%] [G loss: 2.937244]\n",
      "epoch:22 step:17360 [D loss: 0.252129, acc.: 89.84%] [G loss: 3.089318]\n",
      "epoch:22 step:17361 [D loss: 0.376248, acc.: 79.69%] [G loss: 3.162606]\n",
      "epoch:22 step:17362 [D loss: 0.251110, acc.: 89.06%] [G loss: 2.860672]\n",
      "epoch:22 step:17363 [D loss: 0.326495, acc.: 89.06%] [G loss: 2.963670]\n",
      "epoch:22 step:17364 [D loss: 0.323751, acc.: 85.16%] [G loss: 3.373070]\n",
      "epoch:22 step:17365 [D loss: 0.282603, acc.: 85.94%] [G loss: 4.089699]\n",
      "epoch:22 step:17366 [D loss: 0.420015, acc.: 78.12%] [G loss: 2.439416]\n",
      "epoch:22 step:17367 [D loss: 0.230878, acc.: 89.84%] [G loss: 3.194192]\n",
      "epoch:22 step:17368 [D loss: 0.363499, acc.: 85.16%] [G loss: 2.265455]\n",
      "epoch:22 step:17369 [D loss: 0.372464, acc.: 83.59%] [G loss: 2.478214]\n",
      "epoch:22 step:17370 [D loss: 0.303539, acc.: 84.38%] [G loss: 2.908857]\n",
      "epoch:22 step:17371 [D loss: 0.265401, acc.: 89.84%] [G loss: 4.138829]\n",
      "epoch:22 step:17372 [D loss: 0.340190, acc.: 84.38%] [G loss: 3.837662]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:22 step:17373 [D loss: 0.374060, acc.: 79.69%] [G loss: 2.202668]\n",
      "epoch:22 step:17374 [D loss: 0.242032, acc.: 90.62%] [G loss: 2.644907]\n",
      "epoch:22 step:17375 [D loss: 0.361337, acc.: 82.81%] [G loss: 3.059881]\n",
      "epoch:22 step:17376 [D loss: 0.338751, acc.: 81.25%] [G loss: 4.554592]\n",
      "epoch:22 step:17377 [D loss: 0.317340, acc.: 89.06%] [G loss: 4.425066]\n",
      "epoch:22 step:17378 [D loss: 0.312402, acc.: 86.72%] [G loss: 3.640669]\n",
      "epoch:22 step:17379 [D loss: 0.275190, acc.: 87.50%] [G loss: 3.747132]\n",
      "epoch:22 step:17380 [D loss: 0.372315, acc.: 83.59%] [G loss: 2.412043]\n",
      "epoch:22 step:17381 [D loss: 0.313950, acc.: 87.50%] [G loss: 2.720633]\n",
      "epoch:22 step:17382 [D loss: 0.217583, acc.: 89.84%] [G loss: 5.231667]\n",
      "epoch:22 step:17383 [D loss: 0.275897, acc.: 84.38%] [G loss: 3.361652]\n",
      "epoch:22 step:17384 [D loss: 0.396824, acc.: 80.47%] [G loss: 4.871378]\n",
      "epoch:22 step:17385 [D loss: 0.337520, acc.: 86.72%] [G loss: 3.139996]\n",
      "epoch:22 step:17386 [D loss: 0.261025, acc.: 89.84%] [G loss: 4.151624]\n",
      "epoch:22 step:17387 [D loss: 0.282845, acc.: 89.84%] [G loss: 4.472700]\n",
      "epoch:22 step:17388 [D loss: 0.330532, acc.: 87.50%] [G loss: 3.099258]\n",
      "epoch:22 step:17389 [D loss: 0.281277, acc.: 89.06%] [G loss: 4.081278]\n",
      "epoch:22 step:17390 [D loss: 0.276814, acc.: 87.50%] [G loss: 4.103748]\n",
      "epoch:22 step:17391 [D loss: 0.302874, acc.: 87.50%] [G loss: 3.624075]\n",
      "epoch:22 step:17392 [D loss: 0.235619, acc.: 91.41%] [G loss: 3.097634]\n",
      "epoch:22 step:17393 [D loss: 0.268435, acc.: 85.94%] [G loss: 3.696300]\n",
      "epoch:22 step:17394 [D loss: 0.295452, acc.: 88.28%] [G loss: 3.036033]\n",
      "epoch:22 step:17395 [D loss: 0.226525, acc.: 90.62%] [G loss: 3.580019]\n",
      "epoch:22 step:17396 [D loss: 0.261468, acc.: 90.62%] [G loss: 4.532374]\n",
      "epoch:22 step:17397 [D loss: 0.270713, acc.: 88.28%] [G loss: 4.389352]\n",
      "epoch:22 step:17398 [D loss: 0.309717, acc.: 86.72%] [G loss: 4.916121]\n",
      "epoch:22 step:17399 [D loss: 0.244702, acc.: 89.06%] [G loss: 7.434001]\n",
      "epoch:22 step:17400 [D loss: 0.232508, acc.: 89.84%] [G loss: 4.746377]\n",
      "##############\n",
      "[0.84759353 0.85796896 0.8123432  0.79620158 0.76612531 0.84506693\n",
      " 0.87785468 0.8286153  0.81011308 0.81646772]\n",
      "##########\n",
      "epoch:22 step:17401 [D loss: 0.248484, acc.: 89.84%] [G loss: 4.490744]\n",
      "epoch:22 step:17402 [D loss: 0.292334, acc.: 88.28%] [G loss: 2.999182]\n",
      "epoch:22 step:17403 [D loss: 0.251437, acc.: 89.84%] [G loss: 3.001812]\n",
      "epoch:22 step:17404 [D loss: 0.426974, acc.: 82.03%] [G loss: 3.023219]\n",
      "epoch:22 step:17405 [D loss: 0.283022, acc.: 89.06%] [G loss: 3.349064]\n",
      "epoch:22 step:17406 [D loss: 0.343891, acc.: 85.94%] [G loss: 2.898143]\n",
      "epoch:22 step:17407 [D loss: 0.437828, acc.: 81.25%] [G loss: 2.514093]\n",
      "epoch:22 step:17408 [D loss: 0.388634, acc.: 82.81%] [G loss: 2.734122]\n",
      "epoch:22 step:17409 [D loss: 0.433947, acc.: 78.91%] [G loss: 3.360870]\n",
      "epoch:22 step:17410 [D loss: 0.459228, acc.: 77.34%] [G loss: 3.746194]\n",
      "epoch:22 step:17411 [D loss: 0.358927, acc.: 80.47%] [G loss: 4.163333]\n",
      "epoch:22 step:17412 [D loss: 0.315589, acc.: 85.16%] [G loss: 2.566338]\n",
      "epoch:22 step:17413 [D loss: 0.400614, acc.: 83.59%] [G loss: 2.597150]\n",
      "epoch:22 step:17414 [D loss: 0.287014, acc.: 86.72%] [G loss: 3.207376]\n",
      "epoch:22 step:17415 [D loss: 0.297845, acc.: 87.50%] [G loss: 2.374683]\n",
      "epoch:22 step:17416 [D loss: 0.325389, acc.: 81.25%] [G loss: 2.713160]\n",
      "epoch:22 step:17417 [D loss: 0.298414, acc.: 84.38%] [G loss: 2.946091]\n",
      "epoch:22 step:17418 [D loss: 0.437664, acc.: 81.25%] [G loss: 3.609758]\n",
      "epoch:22 step:17419 [D loss: 0.323632, acc.: 82.81%] [G loss: 6.328691]\n",
      "epoch:22 step:17420 [D loss: 0.368598, acc.: 81.25%] [G loss: 2.769790]\n",
      "epoch:22 step:17421 [D loss: 0.321712, acc.: 83.59%] [G loss: 3.573512]\n",
      "epoch:22 step:17422 [D loss: 0.317170, acc.: 85.94%] [G loss: 3.892031]\n",
      "epoch:22 step:17423 [D loss: 0.347153, acc.: 82.03%] [G loss: 3.418849]\n",
      "epoch:22 step:17424 [D loss: 0.270514, acc.: 89.06%] [G loss: 4.028509]\n",
      "epoch:22 step:17425 [D loss: 0.240180, acc.: 90.62%] [G loss: 3.083219]\n",
      "epoch:22 step:17426 [D loss: 0.335500, acc.: 85.16%] [G loss: 4.153689]\n",
      "epoch:22 step:17427 [D loss: 0.242764, acc.: 90.62%] [G loss: 3.157848]\n",
      "epoch:22 step:17428 [D loss: 0.309527, acc.: 83.59%] [G loss: 3.172930]\n",
      "epoch:22 step:17429 [D loss: 0.354347, acc.: 87.50%] [G loss: 3.381725]\n",
      "epoch:22 step:17430 [D loss: 0.337217, acc.: 86.72%] [G loss: 4.174763]\n",
      "epoch:22 step:17431 [D loss: 0.476347, acc.: 75.00%] [G loss: 5.555913]\n",
      "epoch:22 step:17432 [D loss: 0.354091, acc.: 81.25%] [G loss: 4.663298]\n",
      "epoch:22 step:17433 [D loss: 0.356451, acc.: 87.50%] [G loss: 3.957877]\n",
      "epoch:22 step:17434 [D loss: 0.369865, acc.: 84.38%] [G loss: 5.898746]\n",
      "epoch:22 step:17435 [D loss: 0.627124, acc.: 74.22%] [G loss: 2.965393]\n",
      "epoch:22 step:17436 [D loss: 0.450182, acc.: 80.47%] [G loss: 3.509910]\n",
      "epoch:22 step:17437 [D loss: 0.398222, acc.: 79.69%] [G loss: 5.614199]\n",
      "epoch:22 step:17438 [D loss: 0.428420, acc.: 80.47%] [G loss: 3.692092]\n",
      "epoch:22 step:17439 [D loss: 0.283465, acc.: 87.50%] [G loss: 3.917050]\n",
      "epoch:22 step:17440 [D loss: 0.270522, acc.: 90.62%] [G loss: 3.369716]\n",
      "epoch:22 step:17441 [D loss: 0.298576, acc.: 87.50%] [G loss: 4.832574]\n",
      "epoch:22 step:17442 [D loss: 0.406233, acc.: 79.69%] [G loss: 4.139748]\n",
      "epoch:22 step:17443 [D loss: 0.268406, acc.: 88.28%] [G loss: 3.217017]\n",
      "epoch:22 step:17444 [D loss: 0.283811, acc.: 87.50%] [G loss: 2.807761]\n",
      "epoch:22 step:17445 [D loss: 0.242765, acc.: 90.62%] [G loss: 2.957498]\n",
      "epoch:22 step:17446 [D loss: 0.283556, acc.: 87.50%] [G loss: 3.531354]\n",
      "epoch:22 step:17447 [D loss: 0.346591, acc.: 82.03%] [G loss: 2.753200]\n",
      "epoch:22 step:17448 [D loss: 0.345326, acc.: 85.94%] [G loss: 3.130503]\n",
      "epoch:22 step:17449 [D loss: 0.410830, acc.: 84.38%] [G loss: 6.401531]\n",
      "epoch:22 step:17450 [D loss: 0.477420, acc.: 78.12%] [G loss: 3.791968]\n",
      "epoch:22 step:17451 [D loss: 0.516829, acc.: 78.12%] [G loss: 8.255579]\n",
      "epoch:22 step:17452 [D loss: 0.726104, acc.: 72.66%] [G loss: 6.373062]\n",
      "epoch:22 step:17453 [D loss: 0.681664, acc.: 78.91%] [G loss: 3.130633]\n",
      "epoch:22 step:17454 [D loss: 0.338981, acc.: 85.94%] [G loss: 6.009080]\n",
      "epoch:22 step:17455 [D loss: 0.437150, acc.: 80.47%] [G loss: 3.786681]\n",
      "epoch:22 step:17456 [D loss: 0.245583, acc.: 89.06%] [G loss: 5.542708]\n",
      "epoch:22 step:17457 [D loss: 0.380537, acc.: 78.91%] [G loss: 5.895525]\n",
      "epoch:22 step:17458 [D loss: 0.441506, acc.: 81.25%] [G loss: 4.372706]\n",
      "epoch:22 step:17459 [D loss: 0.397921, acc.: 85.16%] [G loss: 4.169111]\n",
      "epoch:22 step:17460 [D loss: 0.352382, acc.: 85.16%] [G loss: 3.144824]\n",
      "epoch:22 step:17461 [D loss: 0.323711, acc.: 82.81%] [G loss: 2.891060]\n",
      "epoch:22 step:17462 [D loss: 0.282015, acc.: 89.06%] [G loss: 3.255307]\n",
      "epoch:22 step:17463 [D loss: 0.392054, acc.: 78.12%] [G loss: 2.492141]\n",
      "epoch:22 step:17464 [D loss: 0.241424, acc.: 91.41%] [G loss: 3.238702]\n",
      "epoch:22 step:17465 [D loss: 0.438367, acc.: 78.12%] [G loss: 3.267638]\n",
      "epoch:22 step:17466 [D loss: 0.505347, acc.: 81.25%] [G loss: 3.235651]\n",
      "epoch:22 step:17467 [D loss: 0.362399, acc.: 83.59%] [G loss: 3.370754]\n",
      "epoch:22 step:17468 [D loss: 0.369843, acc.: 83.59%] [G loss: 3.824511]\n",
      "epoch:22 step:17469 [D loss: 0.359427, acc.: 84.38%] [G loss: 4.331050]\n",
      "epoch:22 step:17470 [D loss: 0.332771, acc.: 81.25%] [G loss: 2.865170]\n",
      "epoch:22 step:17471 [D loss: 0.326335, acc.: 86.72%] [G loss: 3.998579]\n",
      "epoch:22 step:17472 [D loss: 0.323688, acc.: 85.16%] [G loss: 3.040167]\n",
      "epoch:22 step:17473 [D loss: 0.313578, acc.: 87.50%] [G loss: 3.582500]\n",
      "epoch:22 step:17474 [D loss: 0.316919, acc.: 85.94%] [G loss: 3.472093]\n",
      "epoch:22 step:17475 [D loss: 0.361573, acc.: 83.59%] [G loss: 2.938736]\n",
      "epoch:22 step:17476 [D loss: 0.325513, acc.: 86.72%] [G loss: 3.357945]\n",
      "epoch:22 step:17477 [D loss: 0.248331, acc.: 89.06%] [G loss: 4.269470]\n",
      "epoch:22 step:17478 [D loss: 0.331833, acc.: 82.03%] [G loss: 3.101850]\n",
      "epoch:22 step:17479 [D loss: 0.244407, acc.: 85.16%] [G loss: 5.163945]\n",
      "epoch:22 step:17480 [D loss: 0.290970, acc.: 85.16%] [G loss: 4.241619]\n",
      "epoch:22 step:17481 [D loss: 0.390850, acc.: 78.12%] [G loss: 3.028182]\n",
      "epoch:22 step:17482 [D loss: 0.331892, acc.: 85.94%] [G loss: 3.372294]\n",
      "epoch:22 step:17483 [D loss: 0.340497, acc.: 83.59%] [G loss: 2.409325]\n",
      "epoch:22 step:17484 [D loss: 0.408843, acc.: 78.91%] [G loss: 2.421323]\n",
      "epoch:22 step:17485 [D loss: 0.303947, acc.: 85.16%] [G loss: 4.411015]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:22 step:17486 [D loss: 0.230628, acc.: 89.06%] [G loss: 5.197524]\n",
      "epoch:22 step:17487 [D loss: 0.283328, acc.: 89.84%] [G loss: 3.237604]\n",
      "epoch:22 step:17488 [D loss: 0.219939, acc.: 89.06%] [G loss: 4.848748]\n",
      "epoch:22 step:17489 [D loss: 0.391303, acc.: 84.38%] [G loss: 3.657819]\n",
      "epoch:22 step:17490 [D loss: 0.248970, acc.: 90.62%] [G loss: 2.922083]\n",
      "epoch:22 step:17491 [D loss: 0.341073, acc.: 83.59%] [G loss: 2.659623]\n",
      "epoch:22 step:17492 [D loss: 0.241161, acc.: 89.84%] [G loss: 2.636542]\n",
      "epoch:22 step:17493 [D loss: 0.291382, acc.: 87.50%] [G loss: 3.332161]\n",
      "epoch:22 step:17494 [D loss: 0.262974, acc.: 87.50%] [G loss: 3.096359]\n",
      "epoch:22 step:17495 [D loss: 0.322701, acc.: 83.59%] [G loss: 4.452521]\n",
      "epoch:22 step:17496 [D loss: 0.346487, acc.: 83.59%] [G loss: 3.240319]\n",
      "epoch:22 step:17497 [D loss: 0.452270, acc.: 74.22%] [G loss: 2.897838]\n",
      "epoch:22 step:17498 [D loss: 0.338199, acc.: 85.16%] [G loss: 3.852998]\n",
      "epoch:22 step:17499 [D loss: 0.431495, acc.: 83.59%] [G loss: 4.768949]\n",
      "epoch:22 step:17500 [D loss: 0.287083, acc.: 85.94%] [G loss: 4.296200]\n",
      "epoch:22 step:17501 [D loss: 0.248840, acc.: 90.62%] [G loss: 4.977528]\n",
      "epoch:22 step:17502 [D loss: 0.387110, acc.: 82.03%] [G loss: 2.852120]\n",
      "epoch:22 step:17503 [D loss: 0.270772, acc.: 89.84%] [G loss: 3.143878]\n",
      "epoch:22 step:17504 [D loss: 0.319718, acc.: 85.16%] [G loss: 2.995540]\n",
      "epoch:22 step:17505 [D loss: 0.342303, acc.: 85.16%] [G loss: 3.512221]\n",
      "epoch:22 step:17506 [D loss: 0.431678, acc.: 79.69%] [G loss: 3.301524]\n",
      "epoch:22 step:17507 [D loss: 0.412721, acc.: 81.25%] [G loss: 2.601537]\n",
      "epoch:22 step:17508 [D loss: 0.356769, acc.: 83.59%] [G loss: 3.251384]\n",
      "epoch:22 step:17509 [D loss: 0.255704, acc.: 89.06%] [G loss: 3.103096]\n",
      "epoch:22 step:17510 [D loss: 0.287247, acc.: 89.06%] [G loss: 3.741282]\n",
      "epoch:22 step:17511 [D loss: 0.315987, acc.: 86.72%] [G loss: 3.368045]\n",
      "epoch:22 step:17512 [D loss: 0.229159, acc.: 89.06%] [G loss: 3.800482]\n",
      "epoch:22 step:17513 [D loss: 0.258176, acc.: 90.62%] [G loss: 3.841556]\n",
      "epoch:22 step:17514 [D loss: 0.303195, acc.: 89.06%] [G loss: 3.191115]\n",
      "epoch:22 step:17515 [D loss: 0.314470, acc.: 83.59%] [G loss: 4.143288]\n",
      "epoch:22 step:17516 [D loss: 0.436780, acc.: 78.12%] [G loss: 3.468370]\n",
      "epoch:22 step:17517 [D loss: 0.301119, acc.: 85.16%] [G loss: 3.942172]\n",
      "epoch:22 step:17518 [D loss: 0.272359, acc.: 85.16%] [G loss: 4.654872]\n",
      "epoch:22 step:17519 [D loss: 0.300819, acc.: 84.38%] [G loss: 3.629426]\n",
      "epoch:22 step:17520 [D loss: 0.214962, acc.: 92.97%] [G loss: 2.772650]\n",
      "epoch:22 step:17521 [D loss: 0.298531, acc.: 82.81%] [G loss: 3.374772]\n",
      "epoch:22 step:17522 [D loss: 0.339026, acc.: 83.59%] [G loss: 4.191654]\n",
      "epoch:22 step:17523 [D loss: 0.282553, acc.: 90.62%] [G loss: 3.775179]\n",
      "epoch:22 step:17524 [D loss: 0.264764, acc.: 87.50%] [G loss: 6.088157]\n",
      "epoch:22 step:17525 [D loss: 0.276200, acc.: 89.06%] [G loss: 5.344663]\n",
      "epoch:22 step:17526 [D loss: 0.250319, acc.: 89.84%] [G loss: 4.037398]\n",
      "epoch:22 step:17527 [D loss: 0.203493, acc.: 89.06%] [G loss: 6.536999]\n",
      "epoch:22 step:17528 [D loss: 0.323896, acc.: 82.81%] [G loss: 4.057045]\n",
      "epoch:22 step:17529 [D loss: 0.249113, acc.: 90.62%] [G loss: 3.663893]\n",
      "epoch:22 step:17530 [D loss: 0.341709, acc.: 82.81%] [G loss: 7.353451]\n",
      "epoch:22 step:17531 [D loss: 0.617346, acc.: 71.88%] [G loss: 3.461072]\n",
      "epoch:22 step:17532 [D loss: 0.521042, acc.: 75.78%] [G loss: 4.200529]\n",
      "epoch:22 step:17533 [D loss: 0.304837, acc.: 87.50%] [G loss: 5.417980]\n",
      "epoch:22 step:17534 [D loss: 0.460674, acc.: 78.91%] [G loss: 3.202745]\n",
      "epoch:22 step:17535 [D loss: 0.269342, acc.: 89.84%] [G loss: 3.153709]\n",
      "epoch:22 step:17536 [D loss: 0.344675, acc.: 84.38%] [G loss: 2.995325]\n",
      "epoch:22 step:17537 [D loss: 0.234822, acc.: 92.19%] [G loss: 2.813614]\n",
      "epoch:22 step:17538 [D loss: 0.341494, acc.: 82.03%] [G loss: 3.034306]\n",
      "epoch:22 step:17539 [D loss: 0.432558, acc.: 78.12%] [G loss: 2.203506]\n",
      "epoch:22 step:17540 [D loss: 0.306270, acc.: 83.59%] [G loss: 2.557307]\n",
      "epoch:22 step:17541 [D loss: 0.294222, acc.: 87.50%] [G loss: 2.788607]\n",
      "epoch:22 step:17542 [D loss: 0.416569, acc.: 78.12%] [G loss: 2.785915]\n",
      "epoch:22 step:17543 [D loss: 0.390436, acc.: 82.81%] [G loss: 3.473243]\n",
      "epoch:22 step:17544 [D loss: 0.343458, acc.: 85.16%] [G loss: 3.071622]\n",
      "epoch:22 step:17545 [D loss: 0.414314, acc.: 80.47%] [G loss: 2.987537]\n",
      "epoch:22 step:17546 [D loss: 0.233077, acc.: 89.06%] [G loss: 4.885112]\n",
      "epoch:22 step:17547 [D loss: 0.482910, acc.: 78.91%] [G loss: 3.105725]\n",
      "epoch:22 step:17548 [D loss: 0.264324, acc.: 89.06%] [G loss: 3.802912]\n",
      "epoch:22 step:17549 [D loss: 0.488657, acc.: 78.91%] [G loss: 3.186904]\n",
      "epoch:22 step:17550 [D loss: 0.364185, acc.: 82.81%] [G loss: 2.972606]\n",
      "epoch:22 step:17551 [D loss: 0.389555, acc.: 78.91%] [G loss: 3.342726]\n",
      "epoch:22 step:17552 [D loss: 0.280819, acc.: 86.72%] [G loss: 3.592894]\n",
      "epoch:22 step:17553 [D loss: 0.263442, acc.: 90.62%] [G loss: 4.535048]\n",
      "epoch:22 step:17554 [D loss: 0.400344, acc.: 78.91%] [G loss: 3.868786]\n",
      "epoch:22 step:17555 [D loss: 0.411696, acc.: 81.25%] [G loss: 2.597427]\n",
      "epoch:22 step:17556 [D loss: 0.376627, acc.: 89.06%] [G loss: 4.004975]\n",
      "epoch:22 step:17557 [D loss: 0.382010, acc.: 82.81%] [G loss: 3.843379]\n",
      "epoch:22 step:17558 [D loss: 0.360691, acc.: 83.59%] [G loss: 3.600368]\n",
      "epoch:22 step:17559 [D loss: 0.256466, acc.: 88.28%] [G loss: 3.664684]\n",
      "epoch:22 step:17560 [D loss: 0.323590, acc.: 86.72%] [G loss: 4.263714]\n",
      "epoch:22 step:17561 [D loss: 0.305958, acc.: 88.28%] [G loss: 4.386924]\n",
      "epoch:22 step:17562 [D loss: 0.243050, acc.: 89.06%] [G loss: 5.919455]\n",
      "epoch:22 step:17563 [D loss: 0.352595, acc.: 81.25%] [G loss: 3.577524]\n",
      "epoch:22 step:17564 [D loss: 0.428044, acc.: 78.12%] [G loss: 3.168957]\n",
      "epoch:22 step:17565 [D loss: 0.285325, acc.: 85.94%] [G loss: 3.579180]\n",
      "epoch:22 step:17566 [D loss: 0.304589, acc.: 84.38%] [G loss: 4.252083]\n",
      "epoch:22 step:17567 [D loss: 0.440045, acc.: 80.47%] [G loss: 3.215374]\n",
      "epoch:22 step:17568 [D loss: 0.413779, acc.: 81.25%] [G loss: 2.536709]\n",
      "epoch:22 step:17569 [D loss: 0.338307, acc.: 84.38%] [G loss: 2.694489]\n",
      "epoch:22 step:17570 [D loss: 0.337885, acc.: 89.06%] [G loss: 3.198306]\n",
      "epoch:22 step:17571 [D loss: 0.389108, acc.: 85.94%] [G loss: 3.899657]\n",
      "epoch:22 step:17572 [D loss: 0.301866, acc.: 89.06%] [G loss: 2.890236]\n",
      "epoch:22 step:17573 [D loss: 0.386475, acc.: 79.69%] [G loss: 3.759084]\n",
      "epoch:22 step:17574 [D loss: 0.265044, acc.: 85.94%] [G loss: 3.993971]\n",
      "epoch:22 step:17575 [D loss: 0.199328, acc.: 89.06%] [G loss: 5.429906]\n",
      "epoch:22 step:17576 [D loss: 0.350560, acc.: 83.59%] [G loss: 4.135793]\n",
      "epoch:22 step:17577 [D loss: 0.252552, acc.: 87.50%] [G loss: 6.192610]\n",
      "epoch:22 step:17578 [D loss: 0.380332, acc.: 86.72%] [G loss: 2.703532]\n",
      "epoch:22 step:17579 [D loss: 0.313363, acc.: 83.59%] [G loss: 6.104901]\n",
      "epoch:22 step:17580 [D loss: 0.414884, acc.: 81.25%] [G loss: 3.543587]\n",
      "epoch:22 step:17581 [D loss: 0.337941, acc.: 84.38%] [G loss: 4.169098]\n",
      "epoch:22 step:17582 [D loss: 0.262192, acc.: 86.72%] [G loss: 3.397455]\n",
      "epoch:22 step:17583 [D loss: 0.274892, acc.: 88.28%] [G loss: 4.236375]\n",
      "epoch:22 step:17584 [D loss: 0.323537, acc.: 84.38%] [G loss: 3.652857]\n",
      "epoch:22 step:17585 [D loss: 0.402991, acc.: 79.69%] [G loss: 3.987800]\n",
      "epoch:22 step:17586 [D loss: 0.274780, acc.: 89.06%] [G loss: 3.319244]\n",
      "epoch:22 step:17587 [D loss: 0.271710, acc.: 87.50%] [G loss: 4.508360]\n",
      "epoch:22 step:17588 [D loss: 0.374826, acc.: 80.47%] [G loss: 6.243741]\n",
      "epoch:22 step:17589 [D loss: 0.324044, acc.: 85.16%] [G loss: 3.986097]\n",
      "epoch:22 step:17590 [D loss: 0.312207, acc.: 85.16%] [G loss: 4.891282]\n",
      "epoch:22 step:17591 [D loss: 0.306434, acc.: 84.38%] [G loss: 4.776088]\n",
      "epoch:22 step:17592 [D loss: 0.296454, acc.: 86.72%] [G loss: 3.302482]\n",
      "epoch:22 step:17593 [D loss: 0.419017, acc.: 84.38%] [G loss: 3.188845]\n",
      "epoch:22 step:17594 [D loss: 0.358560, acc.: 83.59%] [G loss: 2.967520]\n",
      "epoch:22 step:17595 [D loss: 0.311405, acc.: 83.59%] [G loss: 4.754595]\n",
      "epoch:22 step:17596 [D loss: 0.277652, acc.: 87.50%] [G loss: 4.088885]\n",
      "epoch:22 step:17597 [D loss: 0.352933, acc.: 86.72%] [G loss: 3.001476]\n",
      "epoch:22 step:17598 [D loss: 0.307265, acc.: 82.03%] [G loss: 2.607285]\n",
      "epoch:22 step:17599 [D loss: 0.253569, acc.: 89.06%] [G loss: 3.518160]\n",
      "epoch:22 step:17600 [D loss: 0.260353, acc.: 86.72%] [G loss: 5.728467]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##############\n",
      "[0.84272142 0.85963878 0.81318375 0.78224147 0.77287358 0.83077003\n",
      " 0.88476324 0.83864921 0.82797706 0.81387608]\n",
      "##########\n",
      "epoch:22 step:17601 [D loss: 0.261636, acc.: 88.28%] [G loss: 4.634042]\n",
      "epoch:22 step:17602 [D loss: 0.274832, acc.: 86.72%] [G loss: 4.480996]\n",
      "epoch:22 step:17603 [D loss: 0.257214, acc.: 87.50%] [G loss: 4.893610]\n",
      "epoch:22 step:17604 [D loss: 0.186136, acc.: 92.19%] [G loss: 6.181420]\n",
      "epoch:22 step:17605 [D loss: 0.297812, acc.: 86.72%] [G loss: 6.799825]\n",
      "epoch:22 step:17606 [D loss: 0.226688, acc.: 92.97%] [G loss: 4.525379]\n",
      "epoch:22 step:17607 [D loss: 0.249671, acc.: 89.06%] [G loss: 5.282729]\n",
      "epoch:22 step:17608 [D loss: 0.455817, acc.: 79.69%] [G loss: 2.645413]\n",
      "epoch:22 step:17609 [D loss: 0.261665, acc.: 87.50%] [G loss: 3.698795]\n",
      "epoch:22 step:17610 [D loss: 0.486769, acc.: 81.25%] [G loss: 3.054460]\n",
      "epoch:22 step:17611 [D loss: 0.445021, acc.: 80.47%] [G loss: 3.014668]\n",
      "epoch:22 step:17612 [D loss: 0.336107, acc.: 83.59%] [G loss: 3.618269]\n",
      "epoch:22 step:17613 [D loss: 0.388519, acc.: 83.59%] [G loss: 2.698165]\n",
      "epoch:22 step:17614 [D loss: 0.494066, acc.: 81.25%] [G loss: 3.125947]\n",
      "epoch:22 step:17615 [D loss: 0.318623, acc.: 82.03%] [G loss: 3.722268]\n",
      "epoch:22 step:17616 [D loss: 0.373291, acc.: 82.03%] [G loss: 3.774700]\n",
      "epoch:22 step:17617 [D loss: 0.336656, acc.: 83.59%] [G loss: 4.652488]\n",
      "epoch:22 step:17618 [D loss: 0.394546, acc.: 85.16%] [G loss: 5.861296]\n",
      "epoch:22 step:17619 [D loss: 0.657261, acc.: 80.47%] [G loss: 7.217613]\n",
      "epoch:22 step:17620 [D loss: 1.393826, acc.: 63.28%] [G loss: 6.610120]\n",
      "epoch:22 step:17621 [D loss: 1.398055, acc.: 69.53%] [G loss: 6.340177]\n",
      "epoch:22 step:17622 [D loss: 0.877393, acc.: 77.34%] [G loss: 4.471857]\n",
      "epoch:22 step:17623 [D loss: 0.643259, acc.: 67.97%] [G loss: 3.484857]\n",
      "epoch:22 step:17624 [D loss: 0.327717, acc.: 90.62%] [G loss: 4.721439]\n",
      "epoch:22 step:17625 [D loss: 0.421828, acc.: 83.59%] [G loss: 4.038859]\n",
      "epoch:22 step:17626 [D loss: 0.386291, acc.: 81.25%] [G loss: 3.654320]\n",
      "epoch:22 step:17627 [D loss: 0.367766, acc.: 81.25%] [G loss: 3.970716]\n",
      "epoch:22 step:17628 [D loss: 0.261128, acc.: 89.06%] [G loss: 2.931680]\n",
      "epoch:22 step:17629 [D loss: 0.347753, acc.: 88.28%] [G loss: 4.365737]\n",
      "epoch:22 step:17630 [D loss: 0.543872, acc.: 76.56%] [G loss: 2.822565]\n",
      "epoch:22 step:17631 [D loss: 0.387482, acc.: 82.81%] [G loss: 4.247011]\n",
      "epoch:22 step:17632 [D loss: 0.314865, acc.: 86.72%] [G loss: 3.505693]\n",
      "epoch:22 step:17633 [D loss: 0.222919, acc.: 90.62%] [G loss: 4.390497]\n",
      "epoch:22 step:17634 [D loss: 0.334843, acc.: 84.38%] [G loss: 3.441549]\n",
      "epoch:22 step:17635 [D loss: 0.239280, acc.: 90.62%] [G loss: 3.112206]\n",
      "epoch:22 step:17636 [D loss: 0.251716, acc.: 90.62%] [G loss: 3.328572]\n",
      "epoch:22 step:17637 [D loss: 0.412187, acc.: 82.81%] [G loss: 4.171484]\n",
      "epoch:22 step:17638 [D loss: 0.344650, acc.: 85.94%] [G loss: 3.988346]\n",
      "epoch:22 step:17639 [D loss: 0.312794, acc.: 89.84%] [G loss: 3.209817]\n",
      "epoch:22 step:17640 [D loss: 0.235986, acc.: 89.84%] [G loss: 3.484090]\n",
      "epoch:22 step:17641 [D loss: 0.365655, acc.: 84.38%] [G loss: 2.949715]\n",
      "epoch:22 step:17642 [D loss: 0.296773, acc.: 84.38%] [G loss: 3.428582]\n",
      "epoch:22 step:17643 [D loss: 0.293245, acc.: 86.72%] [G loss: 3.209377]\n",
      "epoch:22 step:17644 [D loss: 0.238446, acc.: 90.62%] [G loss: 2.618269]\n",
      "epoch:22 step:17645 [D loss: 0.373316, acc.: 82.03%] [G loss: 4.114605]\n",
      "epoch:22 step:17646 [D loss: 0.319811, acc.: 85.16%] [G loss: 2.988024]\n",
      "epoch:22 step:17647 [D loss: 0.395126, acc.: 80.47%] [G loss: 4.426373]\n",
      "epoch:22 step:17648 [D loss: 0.405034, acc.: 81.25%] [G loss: 1.999819]\n",
      "epoch:22 step:17649 [D loss: 0.304939, acc.: 86.72%] [G loss: 5.159141]\n",
      "epoch:22 step:17650 [D loss: 0.332300, acc.: 86.72%] [G loss: 2.371879]\n",
      "epoch:22 step:17651 [D loss: 0.246962, acc.: 92.19%] [G loss: 4.211667]\n",
      "epoch:22 step:17652 [D loss: 0.265305, acc.: 90.62%] [G loss: 3.293625]\n",
      "epoch:22 step:17653 [D loss: 0.234746, acc.: 89.06%] [G loss: 3.314139]\n",
      "epoch:22 step:17654 [D loss: 0.301606, acc.: 85.16%] [G loss: 4.147341]\n",
      "epoch:22 step:17655 [D loss: 0.280771, acc.: 87.50%] [G loss: 3.401812]\n",
      "epoch:22 step:17656 [D loss: 0.307089, acc.: 86.72%] [G loss: 3.481051]\n",
      "epoch:22 step:17657 [D loss: 0.459518, acc.: 78.12%] [G loss: 2.916591]\n",
      "epoch:22 step:17658 [D loss: 0.310062, acc.: 89.06%] [G loss: 4.138653]\n",
      "epoch:22 step:17659 [D loss: 0.321656, acc.: 85.16%] [G loss: 3.759822]\n",
      "epoch:22 step:17660 [D loss: 0.452570, acc.: 78.91%] [G loss: 4.136198]\n",
      "epoch:22 step:17661 [D loss: 0.514198, acc.: 77.34%] [G loss: 4.032796]\n",
      "epoch:22 step:17662 [D loss: 0.360983, acc.: 85.16%] [G loss: 4.374210]\n",
      "epoch:22 step:17663 [D loss: 0.280557, acc.: 85.94%] [G loss: 4.265796]\n",
      "epoch:22 step:17664 [D loss: 0.400930, acc.: 81.25%] [G loss: 3.049388]\n",
      "epoch:22 step:17665 [D loss: 0.300608, acc.: 87.50%] [G loss: 3.260364]\n",
      "epoch:22 step:17666 [D loss: 0.321742, acc.: 85.16%] [G loss: 2.854447]\n",
      "epoch:22 step:17667 [D loss: 0.237885, acc.: 90.62%] [G loss: 2.706744]\n",
      "epoch:22 step:17668 [D loss: 0.359650, acc.: 82.81%] [G loss: 2.782387]\n",
      "epoch:22 step:17669 [D loss: 0.315090, acc.: 85.16%] [G loss: 3.097314]\n",
      "epoch:22 step:17670 [D loss: 0.380765, acc.: 82.03%] [G loss: 3.112438]\n",
      "epoch:22 step:17671 [D loss: 0.342559, acc.: 84.38%] [G loss: 2.314885]\n",
      "epoch:22 step:17672 [D loss: 0.325243, acc.: 84.38%] [G loss: 3.440379]\n",
      "epoch:22 step:17673 [D loss: 0.370331, acc.: 84.38%] [G loss: 3.094167]\n",
      "epoch:22 step:17674 [D loss: 0.312133, acc.: 87.50%] [G loss: 4.516290]\n",
      "epoch:22 step:17675 [D loss: 0.204419, acc.: 91.41%] [G loss: 5.628680]\n",
      "epoch:22 step:17676 [D loss: 0.262694, acc.: 89.06%] [G loss: 3.024446]\n",
      "epoch:22 step:17677 [D loss: 0.299947, acc.: 85.16%] [G loss: 3.130701]\n",
      "epoch:22 step:17678 [D loss: 0.254726, acc.: 89.84%] [G loss: 3.443261]\n",
      "epoch:22 step:17679 [D loss: 0.220776, acc.: 90.62%] [G loss: 3.562912]\n",
      "epoch:22 step:17680 [D loss: 0.230039, acc.: 92.19%] [G loss: 3.155383]\n",
      "epoch:22 step:17681 [D loss: 0.298648, acc.: 85.94%] [G loss: 2.798834]\n",
      "epoch:22 step:17682 [D loss: 0.343184, acc.: 83.59%] [G loss: 2.657656]\n",
      "epoch:22 step:17683 [D loss: 0.359920, acc.: 84.38%] [G loss: 2.513576]\n",
      "epoch:22 step:17684 [D loss: 0.271916, acc.: 88.28%] [G loss: 2.846917]\n",
      "epoch:22 step:17685 [D loss: 0.354561, acc.: 84.38%] [G loss: 2.625698]\n",
      "epoch:22 step:17686 [D loss: 0.288622, acc.: 88.28%] [G loss: 3.415048]\n",
      "epoch:22 step:17687 [D loss: 0.285520, acc.: 88.28%] [G loss: 3.699532]\n",
      "epoch:22 step:17688 [D loss: 0.265857, acc.: 89.84%] [G loss: 3.503653]\n",
      "epoch:22 step:17689 [D loss: 0.344119, acc.: 89.06%] [G loss: 2.854427]\n",
      "epoch:22 step:17690 [D loss: 0.364915, acc.: 82.81%] [G loss: 2.731061]\n",
      "epoch:22 step:17691 [D loss: 0.307710, acc.: 88.28%] [G loss: 3.117371]\n",
      "epoch:22 step:17692 [D loss: 0.448498, acc.: 73.44%] [G loss: 3.876270]\n",
      "epoch:22 step:17693 [D loss: 0.322482, acc.: 83.59%] [G loss: 3.389712]\n",
      "epoch:22 step:17694 [D loss: 0.198425, acc.: 93.75%] [G loss: 3.722133]\n",
      "epoch:22 step:17695 [D loss: 0.259490, acc.: 86.72%] [G loss: 2.912435]\n",
      "epoch:22 step:17696 [D loss: 0.252417, acc.: 90.62%] [G loss: 3.134167]\n",
      "epoch:22 step:17697 [D loss: 0.296806, acc.: 88.28%] [G loss: 2.739746]\n",
      "epoch:22 step:17698 [D loss: 0.274489, acc.: 88.28%] [G loss: 3.685522]\n",
      "epoch:22 step:17699 [D loss: 0.380641, acc.: 85.94%] [G loss: 3.979669]\n",
      "epoch:22 step:17700 [D loss: 0.320357, acc.: 85.94%] [G loss: 4.041111]\n",
      "epoch:22 step:17701 [D loss: 0.482695, acc.: 78.12%] [G loss: 2.866320]\n",
      "epoch:22 step:17702 [D loss: 0.478952, acc.: 77.34%] [G loss: 3.816952]\n",
      "epoch:22 step:17703 [D loss: 0.384500, acc.: 83.59%] [G loss: 3.874611]\n",
      "epoch:22 step:17704 [D loss: 0.284449, acc.: 87.50%] [G loss: 3.162271]\n",
      "epoch:22 step:17705 [D loss: 0.307968, acc.: 84.38%] [G loss: 4.072690]\n",
      "epoch:22 step:17706 [D loss: 0.399550, acc.: 82.81%] [G loss: 4.713311]\n",
      "epoch:22 step:17707 [D loss: 0.272583, acc.: 89.84%] [G loss: 4.924024]\n",
      "epoch:22 step:17708 [D loss: 0.217273, acc.: 90.62%] [G loss: 3.170631]\n",
      "epoch:22 step:17709 [D loss: 0.497447, acc.: 75.78%] [G loss: 3.454624]\n",
      "epoch:22 step:17710 [D loss: 0.400340, acc.: 78.91%] [G loss: 3.019581]\n",
      "epoch:22 step:17711 [D loss: 0.381146, acc.: 79.69%] [G loss: 4.482761]\n",
      "epoch:22 step:17712 [D loss: 0.365450, acc.: 85.16%] [G loss: 4.266762]\n",
      "epoch:22 step:17713 [D loss: 0.421865, acc.: 80.47%] [G loss: 3.334588]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:22 step:17714 [D loss: 0.314066, acc.: 86.72%] [G loss: 2.485145]\n",
      "epoch:22 step:17715 [D loss: 0.284369, acc.: 90.62%] [G loss: 2.756722]\n",
      "epoch:22 step:17716 [D loss: 0.388171, acc.: 85.16%] [G loss: 3.241782]\n",
      "epoch:22 step:17717 [D loss: 0.296614, acc.: 89.06%] [G loss: 2.646697]\n",
      "epoch:22 step:17718 [D loss: 0.334546, acc.: 84.38%] [G loss: 2.225765]\n",
      "epoch:22 step:17719 [D loss: 0.287481, acc.: 88.28%] [G loss: 4.236069]\n",
      "epoch:22 step:17720 [D loss: 0.344048, acc.: 83.59%] [G loss: 3.445440]\n",
      "epoch:22 step:17721 [D loss: 0.284275, acc.: 88.28%] [G loss: 3.249505]\n",
      "epoch:22 step:17722 [D loss: 0.324253, acc.: 85.94%] [G loss: 2.896196]\n",
      "epoch:22 step:17723 [D loss: 0.382152, acc.: 81.25%] [G loss: 2.955287]\n",
      "epoch:22 step:17724 [D loss: 0.381496, acc.: 84.38%] [G loss: 3.133177]\n",
      "epoch:22 step:17725 [D loss: 0.260600, acc.: 89.84%] [G loss: 2.476808]\n",
      "epoch:22 step:17726 [D loss: 0.342615, acc.: 86.72%] [G loss: 5.014366]\n",
      "epoch:22 step:17727 [D loss: 0.570045, acc.: 81.25%] [G loss: 7.692764]\n",
      "epoch:22 step:17728 [D loss: 0.423882, acc.: 82.03%] [G loss: 3.251245]\n",
      "epoch:22 step:17729 [D loss: 0.318421, acc.: 84.38%] [G loss: 3.810897]\n",
      "epoch:22 step:17730 [D loss: 0.347269, acc.: 85.94%] [G loss: 3.315024]\n",
      "epoch:22 step:17731 [D loss: 0.281525, acc.: 86.72%] [G loss: 3.740040]\n",
      "epoch:22 step:17732 [D loss: 0.347823, acc.: 82.81%] [G loss: 2.904232]\n",
      "epoch:22 step:17733 [D loss: 0.323939, acc.: 85.16%] [G loss: 2.668376]\n",
      "epoch:22 step:17734 [D loss: 0.412352, acc.: 82.03%] [G loss: 3.312112]\n",
      "epoch:22 step:17735 [D loss: 0.318721, acc.: 86.72%] [G loss: 3.397852]\n",
      "epoch:22 step:17736 [D loss: 0.284897, acc.: 87.50%] [G loss: 4.475470]\n",
      "epoch:22 step:17737 [D loss: 0.341290, acc.: 86.72%] [G loss: 2.868665]\n",
      "epoch:22 step:17738 [D loss: 0.370949, acc.: 82.81%] [G loss: 3.360179]\n",
      "epoch:22 step:17739 [D loss: 0.312023, acc.: 87.50%] [G loss: 3.009672]\n",
      "epoch:22 step:17740 [D loss: 0.261831, acc.: 89.84%] [G loss: 3.235842]\n",
      "epoch:22 step:17741 [D loss: 0.331151, acc.: 83.59%] [G loss: 2.985384]\n",
      "epoch:22 step:17742 [D loss: 0.238558, acc.: 91.41%] [G loss: 2.869730]\n",
      "epoch:22 step:17743 [D loss: 0.237744, acc.: 90.62%] [G loss: 3.272413]\n",
      "epoch:22 step:17744 [D loss: 0.408109, acc.: 79.69%] [G loss: 1.988786]\n",
      "epoch:22 step:17745 [D loss: 0.293831, acc.: 85.16%] [G loss: 3.492395]\n",
      "epoch:22 step:17746 [D loss: 0.295009, acc.: 85.16%] [G loss: 3.020307]\n",
      "epoch:22 step:17747 [D loss: 0.281059, acc.: 86.72%] [G loss: 2.875875]\n",
      "epoch:22 step:17748 [D loss: 0.352609, acc.: 83.59%] [G loss: 2.743816]\n",
      "epoch:22 step:17749 [D loss: 0.343481, acc.: 84.38%] [G loss: 2.901961]\n",
      "epoch:22 step:17750 [D loss: 0.394465, acc.: 83.59%] [G loss: 2.788211]\n",
      "epoch:22 step:17751 [D loss: 0.301492, acc.: 85.94%] [G loss: 4.127358]\n",
      "epoch:22 step:17752 [D loss: 0.438318, acc.: 75.78%] [G loss: 3.989861]\n",
      "epoch:22 step:17753 [D loss: 0.561489, acc.: 75.78%] [G loss: 2.614834]\n",
      "epoch:22 step:17754 [D loss: 0.344804, acc.: 84.38%] [G loss: 2.724017]\n",
      "epoch:22 step:17755 [D loss: 0.453730, acc.: 76.56%] [G loss: 3.109463]\n",
      "epoch:22 step:17756 [D loss: 0.289920, acc.: 84.38%] [G loss: 2.657036]\n",
      "epoch:22 step:17757 [D loss: 0.354516, acc.: 83.59%] [G loss: 2.551425]\n",
      "epoch:22 step:17758 [D loss: 0.232277, acc.: 91.41%] [G loss: 3.205734]\n",
      "epoch:22 step:17759 [D loss: 0.216244, acc.: 89.84%] [G loss: 3.827226]\n",
      "epoch:22 step:17760 [D loss: 0.314336, acc.: 85.94%] [G loss: 3.569827]\n",
      "epoch:22 step:17761 [D loss: 0.264623, acc.: 89.84%] [G loss: 3.162001]\n",
      "epoch:22 step:17762 [D loss: 0.288206, acc.: 82.81%] [G loss: 2.857866]\n",
      "epoch:22 step:17763 [D loss: 0.328256, acc.: 83.59%] [G loss: 2.724798]\n",
      "epoch:22 step:17764 [D loss: 0.329794, acc.: 87.50%] [G loss: 3.728087]\n",
      "epoch:22 step:17765 [D loss: 0.327328, acc.: 84.38%] [G loss: 3.068263]\n",
      "epoch:22 step:17766 [D loss: 0.308420, acc.: 87.50%] [G loss: 3.490443]\n",
      "epoch:22 step:17767 [D loss: 0.296811, acc.: 89.06%] [G loss: 4.031017]\n",
      "epoch:22 step:17768 [D loss: 0.367293, acc.: 85.16%] [G loss: 3.472364]\n",
      "epoch:22 step:17769 [D loss: 0.364339, acc.: 83.59%] [G loss: 3.275093]\n",
      "epoch:22 step:17770 [D loss: 0.210995, acc.: 92.19%] [G loss: 3.370045]\n",
      "epoch:22 step:17771 [D loss: 0.269994, acc.: 89.06%] [G loss: 3.829432]\n",
      "epoch:22 step:17772 [D loss: 0.332106, acc.: 84.38%] [G loss: 4.084664]\n",
      "epoch:22 step:17773 [D loss: 0.243262, acc.: 89.06%] [G loss: 2.331693]\n",
      "epoch:22 step:17774 [D loss: 0.368073, acc.: 82.81%] [G loss: 2.584344]\n",
      "epoch:22 step:17775 [D loss: 0.382144, acc.: 83.59%] [G loss: 2.785808]\n",
      "epoch:22 step:17776 [D loss: 0.294647, acc.: 88.28%] [G loss: 2.844393]\n",
      "epoch:22 step:17777 [D loss: 0.379161, acc.: 84.38%] [G loss: 3.097473]\n",
      "epoch:22 step:17778 [D loss: 0.338595, acc.: 85.16%] [G loss: 3.038242]\n",
      "epoch:22 step:17779 [D loss: 0.237605, acc.: 90.62%] [G loss: 3.324775]\n",
      "epoch:22 step:17780 [D loss: 0.334079, acc.: 84.38%] [G loss: 3.404033]\n",
      "epoch:22 step:17781 [D loss: 0.404928, acc.: 82.81%] [G loss: 2.938451]\n",
      "epoch:22 step:17782 [D loss: 0.357984, acc.: 82.81%] [G loss: 3.058665]\n",
      "epoch:22 step:17783 [D loss: 0.337319, acc.: 87.50%] [G loss: 3.413147]\n",
      "epoch:22 step:17784 [D loss: 0.319335, acc.: 85.94%] [G loss: 2.984492]\n",
      "epoch:22 step:17785 [D loss: 0.344242, acc.: 84.38%] [G loss: 2.547848]\n",
      "epoch:22 step:17786 [D loss: 0.339919, acc.: 84.38%] [G loss: 3.440162]\n",
      "epoch:22 step:17787 [D loss: 0.395795, acc.: 80.47%] [G loss: 2.674543]\n",
      "epoch:22 step:17788 [D loss: 0.439564, acc.: 78.91%] [G loss: 2.958465]\n",
      "epoch:22 step:17789 [D loss: 0.281824, acc.: 85.16%] [G loss: 3.227013]\n",
      "epoch:22 step:17790 [D loss: 0.344923, acc.: 86.72%] [G loss: 5.315407]\n",
      "epoch:22 step:17791 [D loss: 0.353980, acc.: 83.59%] [G loss: 4.917142]\n",
      "epoch:22 step:17792 [D loss: 0.409757, acc.: 78.91%] [G loss: 2.635426]\n",
      "epoch:22 step:17793 [D loss: 0.363092, acc.: 84.38%] [G loss: 3.278553]\n",
      "epoch:22 step:17794 [D loss: 0.287498, acc.: 89.84%] [G loss: 2.725648]\n",
      "epoch:22 step:17795 [D loss: 0.304563, acc.: 84.38%] [G loss: 3.896486]\n",
      "epoch:22 step:17796 [D loss: 0.369387, acc.: 82.81%] [G loss: 2.509505]\n",
      "epoch:22 step:17797 [D loss: 0.268364, acc.: 89.06%] [G loss: 5.070019]\n",
      "epoch:22 step:17798 [D loss: 0.348423, acc.: 83.59%] [G loss: 3.159431]\n",
      "epoch:22 step:17799 [D loss: 0.333513, acc.: 83.59%] [G loss: 3.415852]\n",
      "epoch:22 step:17800 [D loss: 0.272836, acc.: 87.50%] [G loss: 4.392268]\n",
      "##############\n",
      "[0.86595123 0.84091131 0.79780924 0.80164721 0.75949765 0.82693059\n",
      " 0.87582875 0.84274891 0.79151593 0.80308686]\n",
      "##########\n",
      "epoch:22 step:17801 [D loss: 0.399360, acc.: 80.47%] [G loss: 3.728172]\n",
      "epoch:22 step:17802 [D loss: 0.380154, acc.: 84.38%] [G loss: 3.096643]\n",
      "epoch:22 step:17803 [D loss: 0.294431, acc.: 89.06%] [G loss: 5.651425]\n",
      "epoch:22 step:17804 [D loss: 0.336236, acc.: 86.72%] [G loss: 3.130232]\n",
      "epoch:22 step:17805 [D loss: 0.391861, acc.: 81.25%] [G loss: 2.807192]\n",
      "epoch:22 step:17806 [D loss: 0.304793, acc.: 89.06%] [G loss: 3.610505]\n",
      "epoch:22 step:17807 [D loss: 0.232717, acc.: 90.62%] [G loss: 4.518804]\n",
      "epoch:22 step:17808 [D loss: 0.254084, acc.: 90.62%] [G loss: 3.496603]\n",
      "epoch:22 step:17809 [D loss: 0.361296, acc.: 80.47%] [G loss: 2.866093]\n",
      "epoch:22 step:17810 [D loss: 0.431389, acc.: 81.25%] [G loss: 2.256995]\n",
      "epoch:22 step:17811 [D loss: 0.415022, acc.: 77.34%] [G loss: 3.508325]\n",
      "epoch:22 step:17812 [D loss: 0.226814, acc.: 90.62%] [G loss: 4.686563]\n",
      "epoch:22 step:17813 [D loss: 0.302445, acc.: 83.59%] [G loss: 3.807047]\n",
      "epoch:22 step:17814 [D loss: 0.180037, acc.: 96.88%] [G loss: 4.730246]\n",
      "epoch:22 step:17815 [D loss: 0.294327, acc.: 86.72%] [G loss: 3.809326]\n",
      "epoch:22 step:17816 [D loss: 0.295538, acc.: 84.38%] [G loss: 3.566120]\n",
      "epoch:22 step:17817 [D loss: 0.353816, acc.: 86.72%] [G loss: 3.477020]\n",
      "epoch:22 step:17818 [D loss: 0.271110, acc.: 86.72%] [G loss: 3.555367]\n",
      "epoch:22 step:17819 [D loss: 0.290988, acc.: 85.16%] [G loss: 4.140906]\n",
      "epoch:22 step:17820 [D loss: 0.321427, acc.: 84.38%] [G loss: 3.697202]\n",
      "epoch:22 step:17821 [D loss: 0.372627, acc.: 83.59%] [G loss: 4.912437]\n",
      "epoch:22 step:17822 [D loss: 0.300584, acc.: 85.94%] [G loss: 3.378241]\n",
      "epoch:22 step:17823 [D loss: 0.365422, acc.: 78.91%] [G loss: 3.927611]\n",
      "epoch:22 step:17824 [D loss: 0.362289, acc.: 82.81%] [G loss: 3.630826]\n",
      "epoch:22 step:17825 [D loss: 0.437753, acc.: 78.12%] [G loss: 3.342654]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:22 step:17826 [D loss: 0.302651, acc.: 83.59%] [G loss: 2.892160]\n",
      "epoch:22 step:17827 [D loss: 0.333795, acc.: 82.03%] [G loss: 3.423645]\n",
      "epoch:22 step:17828 [D loss: 0.390386, acc.: 81.25%] [G loss: 3.148318]\n",
      "epoch:22 step:17829 [D loss: 0.270539, acc.: 85.94%] [G loss: 3.774848]\n",
      "epoch:22 step:17830 [D loss: 0.352286, acc.: 85.16%] [G loss: 4.822568]\n",
      "epoch:22 step:17831 [D loss: 0.423804, acc.: 82.03%] [G loss: 2.843822]\n",
      "epoch:22 step:17832 [D loss: 0.482427, acc.: 74.22%] [G loss: 2.786070]\n",
      "epoch:22 step:17833 [D loss: 0.344502, acc.: 81.25%] [G loss: 2.132577]\n",
      "epoch:22 step:17834 [D loss: 0.274884, acc.: 87.50%] [G loss: 3.360386]\n",
      "epoch:22 step:17835 [D loss: 0.292425, acc.: 89.84%] [G loss: 3.039979]\n",
      "epoch:22 step:17836 [D loss: 0.331690, acc.: 85.94%] [G loss: 2.849869]\n",
      "epoch:22 step:17837 [D loss: 0.349009, acc.: 86.72%] [G loss: 2.975973]\n",
      "epoch:22 step:17838 [D loss: 0.366138, acc.: 85.94%] [G loss: 3.885129]\n",
      "epoch:22 step:17839 [D loss: 0.372016, acc.: 79.69%] [G loss: 3.494236]\n",
      "epoch:22 step:17840 [D loss: 0.229515, acc.: 93.75%] [G loss: 2.332100]\n",
      "epoch:22 step:17841 [D loss: 0.444775, acc.: 73.44%] [G loss: 2.428020]\n",
      "epoch:22 step:17842 [D loss: 0.354605, acc.: 85.94%] [G loss: 2.460168]\n",
      "epoch:22 step:17843 [D loss: 0.373832, acc.: 81.25%] [G loss: 4.862886]\n",
      "epoch:22 step:17844 [D loss: 0.407805, acc.: 76.56%] [G loss: 4.021836]\n",
      "epoch:22 step:17845 [D loss: 0.292017, acc.: 85.16%] [G loss: 4.375934]\n",
      "epoch:22 step:17846 [D loss: 0.451139, acc.: 75.78%] [G loss: 3.580510]\n",
      "epoch:22 step:17847 [D loss: 0.281435, acc.: 88.28%] [G loss: 3.107872]\n",
      "epoch:22 step:17848 [D loss: 0.472084, acc.: 78.12%] [G loss: 4.096749]\n",
      "epoch:22 step:17849 [D loss: 0.300965, acc.: 85.94%] [G loss: 3.710039]\n",
      "epoch:22 step:17850 [D loss: 0.283887, acc.: 89.06%] [G loss: 5.565917]\n",
      "epoch:22 step:17851 [D loss: 0.523896, acc.: 75.00%] [G loss: 3.136285]\n",
      "epoch:22 step:17852 [D loss: 0.343759, acc.: 82.81%] [G loss: 3.549721]\n",
      "epoch:22 step:17853 [D loss: 0.264274, acc.: 85.94%] [G loss: 3.147169]\n",
      "epoch:22 step:17854 [D loss: 0.266678, acc.: 89.06%] [G loss: 3.910619]\n",
      "epoch:22 step:17855 [D loss: 0.233843, acc.: 89.84%] [G loss: 3.767757]\n",
      "epoch:22 step:17856 [D loss: 0.335585, acc.: 82.03%] [G loss: 3.624301]\n",
      "epoch:22 step:17857 [D loss: 0.370131, acc.: 80.47%] [G loss: 3.456905]\n",
      "epoch:22 step:17858 [D loss: 0.349982, acc.: 83.59%] [G loss: 4.354198]\n",
      "epoch:22 step:17859 [D loss: 0.267413, acc.: 86.72%] [G loss: 3.085339]\n",
      "epoch:22 step:17860 [D loss: 0.248562, acc.: 89.06%] [G loss: 3.800950]\n",
      "epoch:22 step:17861 [D loss: 0.358340, acc.: 85.16%] [G loss: 2.980418]\n",
      "epoch:22 step:17862 [D loss: 0.455217, acc.: 81.25%] [G loss: 2.385521]\n",
      "epoch:22 step:17863 [D loss: 0.435845, acc.: 82.03%] [G loss: 3.090512]\n",
      "epoch:22 step:17864 [D loss: 0.356639, acc.: 80.47%] [G loss: 3.692011]\n",
      "epoch:22 step:17865 [D loss: 0.493311, acc.: 76.56%] [G loss: 3.101337]\n",
      "epoch:22 step:17866 [D loss: 0.320615, acc.: 89.06%] [G loss: 2.895846]\n",
      "epoch:22 step:17867 [D loss: 0.474106, acc.: 78.12%] [G loss: 2.945260]\n",
      "epoch:22 step:17868 [D loss: 0.427115, acc.: 82.81%] [G loss: 2.885505]\n",
      "epoch:22 step:17869 [D loss: 0.304214, acc.: 84.38%] [G loss: 4.599964]\n",
      "epoch:22 step:17870 [D loss: 0.292617, acc.: 86.72%] [G loss: 4.507379]\n",
      "epoch:22 step:17871 [D loss: 0.272129, acc.: 89.84%] [G loss: 4.603928]\n",
      "epoch:22 step:17872 [D loss: 0.283202, acc.: 87.50%] [G loss: 3.626561]\n",
      "epoch:22 step:17873 [D loss: 0.210597, acc.: 90.62%] [G loss: 3.665597]\n",
      "epoch:22 step:17874 [D loss: 0.351559, acc.: 83.59%] [G loss: 3.348549]\n",
      "epoch:22 step:17875 [D loss: 0.349445, acc.: 82.81%] [G loss: 2.660460]\n",
      "epoch:22 step:17876 [D loss: 0.236922, acc.: 86.72%] [G loss: 2.971772]\n",
      "epoch:22 step:17877 [D loss: 0.268864, acc.: 87.50%] [G loss: 4.792566]\n",
      "epoch:22 step:17878 [D loss: 0.277312, acc.: 85.16%] [G loss: 4.925604]\n",
      "epoch:22 step:17879 [D loss: 0.360957, acc.: 83.59%] [G loss: 2.748907]\n",
      "epoch:22 step:17880 [D loss: 0.397015, acc.: 79.69%] [G loss: 3.239247]\n",
      "epoch:22 step:17881 [D loss: 0.304821, acc.: 86.72%] [G loss: 4.523163]\n",
      "epoch:22 step:17882 [D loss: 0.355866, acc.: 81.25%] [G loss: 4.517487]\n",
      "epoch:22 step:17883 [D loss: 0.239346, acc.: 92.97%] [G loss: 4.328958]\n",
      "epoch:22 step:17884 [D loss: 0.281136, acc.: 86.72%] [G loss: 4.237181]\n",
      "epoch:22 step:17885 [D loss: 0.329650, acc.: 82.03%] [G loss: 2.720878]\n",
      "epoch:22 step:17886 [D loss: 0.384993, acc.: 82.03%] [G loss: 3.607451]\n",
      "epoch:22 step:17887 [D loss: 0.444494, acc.: 79.69%] [G loss: 3.004967]\n",
      "epoch:22 step:17888 [D loss: 0.412373, acc.: 82.81%] [G loss: 2.827672]\n",
      "epoch:22 step:17889 [D loss: 0.367730, acc.: 79.69%] [G loss: 3.838514]\n",
      "epoch:22 step:17890 [D loss: 0.300257, acc.: 85.16%] [G loss: 3.780984]\n",
      "epoch:22 step:17891 [D loss: 0.386155, acc.: 82.81%] [G loss: 3.281256]\n",
      "epoch:22 step:17892 [D loss: 0.324586, acc.: 84.38%] [G loss: 6.368806]\n",
      "epoch:22 step:17893 [D loss: 0.282581, acc.: 89.06%] [G loss: 3.992773]\n",
      "epoch:22 step:17894 [D loss: 0.362353, acc.: 80.47%] [G loss: 4.576211]\n",
      "epoch:22 step:17895 [D loss: 0.375512, acc.: 83.59%] [G loss: 4.300167]\n",
      "epoch:22 step:17896 [D loss: 0.246235, acc.: 89.84%] [G loss: 4.287814]\n",
      "epoch:22 step:17897 [D loss: 0.341845, acc.: 81.25%] [G loss: 5.423192]\n",
      "epoch:22 step:17898 [D loss: 0.233962, acc.: 89.84%] [G loss: 6.087949]\n",
      "epoch:22 step:17899 [D loss: 0.258810, acc.: 90.62%] [G loss: 3.590705]\n",
      "epoch:22 step:17900 [D loss: 0.328002, acc.: 84.38%] [G loss: 2.630014]\n",
      "epoch:22 step:17901 [D loss: 0.297539, acc.: 83.59%] [G loss: 4.680648]\n",
      "epoch:22 step:17902 [D loss: 0.310891, acc.: 85.94%] [G loss: 3.413032]\n",
      "epoch:22 step:17903 [D loss: 0.284399, acc.: 86.72%] [G loss: 4.108585]\n",
      "epoch:22 step:17904 [D loss: 0.389035, acc.: 81.25%] [G loss: 3.365839]\n",
      "epoch:22 step:17905 [D loss: 0.260064, acc.: 91.41%] [G loss: 3.820519]\n",
      "epoch:22 step:17906 [D loss: 0.289538, acc.: 85.94%] [G loss: 3.388781]\n",
      "epoch:22 step:17907 [D loss: 0.306819, acc.: 85.94%] [G loss: 3.933513]\n",
      "epoch:22 step:17908 [D loss: 0.331990, acc.: 83.59%] [G loss: 3.521878]\n",
      "epoch:22 step:17909 [D loss: 0.352091, acc.: 82.03%] [G loss: 3.356521]\n",
      "epoch:22 step:17910 [D loss: 0.422590, acc.: 77.34%] [G loss: 2.933251]\n",
      "epoch:22 step:17911 [D loss: 0.324853, acc.: 83.59%] [G loss: 3.830224]\n",
      "epoch:22 step:17912 [D loss: 0.389591, acc.: 79.69%] [G loss: 4.288252]\n",
      "epoch:22 step:17913 [D loss: 0.362204, acc.: 86.72%] [G loss: 4.581153]\n",
      "epoch:22 step:17914 [D loss: 0.324803, acc.: 85.16%] [G loss: 6.430222]\n",
      "epoch:22 step:17915 [D loss: 0.370705, acc.: 80.47%] [G loss: 4.951483]\n",
      "epoch:22 step:17916 [D loss: 0.485078, acc.: 78.12%] [G loss: 3.980592]\n",
      "epoch:22 step:17917 [D loss: 0.471901, acc.: 77.34%] [G loss: 3.099690]\n",
      "epoch:22 step:17918 [D loss: 0.235654, acc.: 91.41%] [G loss: 3.332606]\n",
      "epoch:22 step:17919 [D loss: 0.322199, acc.: 84.38%] [G loss: 3.082429]\n",
      "epoch:22 step:17920 [D loss: 0.427218, acc.: 78.12%] [G loss: 3.529794]\n",
      "epoch:22 step:17921 [D loss: 0.389738, acc.: 79.69%] [G loss: 2.573185]\n",
      "epoch:22 step:17922 [D loss: 0.295935, acc.: 89.06%] [G loss: 2.786624]\n",
      "epoch:22 step:17923 [D loss: 0.351184, acc.: 82.03%] [G loss: 3.197770]\n",
      "epoch:22 step:17924 [D loss: 0.326298, acc.: 84.38%] [G loss: 3.139369]\n",
      "epoch:22 step:17925 [D loss: 0.323659, acc.: 85.16%] [G loss: 4.130751]\n",
      "epoch:22 step:17926 [D loss: 0.378275, acc.: 82.81%] [G loss: 4.211226]\n",
      "epoch:22 step:17927 [D loss: 0.456191, acc.: 81.25%] [G loss: 3.090411]\n",
      "epoch:22 step:17928 [D loss: 0.303725, acc.: 83.59%] [G loss: 4.588314]\n",
      "epoch:22 step:17929 [D loss: 0.304781, acc.: 87.50%] [G loss: 3.320557]\n",
      "epoch:22 step:17930 [D loss: 0.288591, acc.: 88.28%] [G loss: 3.053203]\n",
      "epoch:22 step:17931 [D loss: 0.311144, acc.: 85.94%] [G loss: 4.614823]\n",
      "epoch:22 step:17932 [D loss: 0.412360, acc.: 85.16%] [G loss: 4.217062]\n",
      "epoch:22 step:17933 [D loss: 0.380920, acc.: 84.38%] [G loss: 3.706673]\n",
      "epoch:22 step:17934 [D loss: 0.336967, acc.: 85.94%] [G loss: 4.323394]\n",
      "epoch:22 step:17935 [D loss: 0.387832, acc.: 82.81%] [G loss: 3.248151]\n",
      "epoch:22 step:17936 [D loss: 0.485619, acc.: 76.56%] [G loss: 3.869892]\n",
      "epoch:22 step:17937 [D loss: 0.341329, acc.: 82.81%] [G loss: 2.710092]\n",
      "epoch:22 step:17938 [D loss: 0.261152, acc.: 86.72%] [G loss: 4.286272]\n",
      "epoch:22 step:17939 [D loss: 0.286133, acc.: 87.50%] [G loss: 3.094722]\n",
      "epoch:22 step:17940 [D loss: 0.421312, acc.: 80.47%] [G loss: 3.383989]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:22 step:17941 [D loss: 0.171782, acc.: 92.97%] [G loss: 6.433818]\n",
      "epoch:22 step:17942 [D loss: 0.238618, acc.: 87.50%] [G loss: 4.496260]\n",
      "epoch:22 step:17943 [D loss: 0.166919, acc.: 92.97%] [G loss: 6.192907]\n",
      "epoch:22 step:17944 [D loss: 0.303470, acc.: 84.38%] [G loss: 3.495727]\n",
      "epoch:22 step:17945 [D loss: 0.266235, acc.: 87.50%] [G loss: 3.743854]\n",
      "epoch:22 step:17946 [D loss: 0.277428, acc.: 88.28%] [G loss: 3.442774]\n",
      "epoch:22 step:17947 [D loss: 0.361443, acc.: 82.81%] [G loss: 2.827950]\n",
      "epoch:22 step:17948 [D loss: 0.378646, acc.: 84.38%] [G loss: 2.565679]\n",
      "epoch:22 step:17949 [D loss: 0.350004, acc.: 82.81%] [G loss: 2.883453]\n",
      "epoch:22 step:17950 [D loss: 0.391890, acc.: 80.47%] [G loss: 4.224353]\n",
      "epoch:22 step:17951 [D loss: 0.313338, acc.: 83.59%] [G loss: 4.095795]\n",
      "epoch:22 step:17952 [D loss: 0.286221, acc.: 88.28%] [G loss: 3.945477]\n",
      "epoch:22 step:17953 [D loss: 0.351585, acc.: 78.91%] [G loss: 4.262565]\n",
      "epoch:22 step:17954 [D loss: 0.374749, acc.: 84.38%] [G loss: 3.647383]\n",
      "epoch:22 step:17955 [D loss: 0.438630, acc.: 82.81%] [G loss: 6.008548]\n",
      "epoch:22 step:17956 [D loss: 0.730475, acc.: 64.84%] [G loss: 6.364443]\n",
      "epoch:22 step:17957 [D loss: 1.310026, acc.: 68.75%] [G loss: 8.665957]\n",
      "epoch:22 step:17958 [D loss: 1.356622, acc.: 64.06%] [G loss: 2.118143]\n",
      "epoch:22 step:17959 [D loss: 0.459672, acc.: 76.56%] [G loss: 3.578884]\n",
      "epoch:22 step:17960 [D loss: 0.580994, acc.: 79.69%] [G loss: 2.992572]\n",
      "epoch:22 step:17961 [D loss: 0.439974, acc.: 80.47%] [G loss: 4.007887]\n",
      "epoch:22 step:17962 [D loss: 0.359413, acc.: 82.03%] [G loss: 2.950596]\n",
      "epoch:22 step:17963 [D loss: 0.324676, acc.: 83.59%] [G loss: 3.087904]\n",
      "epoch:23 step:17964 [D loss: 0.290128, acc.: 89.06%] [G loss: 3.144541]\n",
      "epoch:23 step:17965 [D loss: 0.274751, acc.: 89.84%] [G loss: 3.321295]\n",
      "epoch:23 step:17966 [D loss: 0.367431, acc.: 83.59%] [G loss: 2.957192]\n",
      "epoch:23 step:17967 [D loss: 0.360765, acc.: 83.59%] [G loss: 2.889690]\n",
      "epoch:23 step:17968 [D loss: 0.356267, acc.: 82.81%] [G loss: 3.410134]\n",
      "epoch:23 step:17969 [D loss: 0.253107, acc.: 89.84%] [G loss: 3.480806]\n",
      "epoch:23 step:17970 [D loss: 0.300116, acc.: 90.62%] [G loss: 2.281940]\n",
      "epoch:23 step:17971 [D loss: 0.239360, acc.: 92.97%] [G loss: 3.379049]\n",
      "epoch:23 step:17972 [D loss: 0.402271, acc.: 85.94%] [G loss: 4.221473]\n",
      "epoch:23 step:17973 [D loss: 0.243939, acc.: 89.06%] [G loss: 6.718925]\n",
      "epoch:23 step:17974 [D loss: 0.352553, acc.: 85.16%] [G loss: 2.561189]\n",
      "epoch:23 step:17975 [D loss: 0.290119, acc.: 88.28%] [G loss: 3.335624]\n",
      "epoch:23 step:17976 [D loss: 0.258853, acc.: 90.62%] [G loss: 3.571053]\n",
      "epoch:23 step:17977 [D loss: 0.379579, acc.: 80.47%] [G loss: 3.019079]\n",
      "epoch:23 step:17978 [D loss: 0.374497, acc.: 79.69%] [G loss: 3.458694]\n",
      "epoch:23 step:17979 [D loss: 0.195071, acc.: 94.53%] [G loss: 4.429904]\n",
      "epoch:23 step:17980 [D loss: 0.209310, acc.: 88.28%] [G loss: 3.839700]\n",
      "epoch:23 step:17981 [D loss: 0.350157, acc.: 81.25%] [G loss: 3.162227]\n",
      "epoch:23 step:17982 [D loss: 0.367000, acc.: 82.81%] [G loss: 3.186145]\n",
      "epoch:23 step:17983 [D loss: 0.266288, acc.: 90.62%] [G loss: 3.530728]\n",
      "epoch:23 step:17984 [D loss: 0.306260, acc.: 85.94%] [G loss: 4.636948]\n",
      "epoch:23 step:17985 [D loss: 0.332817, acc.: 84.38%] [G loss: 3.172737]\n",
      "epoch:23 step:17986 [D loss: 0.335837, acc.: 84.38%] [G loss: 4.691229]\n",
      "epoch:23 step:17987 [D loss: 0.350002, acc.: 85.94%] [G loss: 2.926655]\n",
      "epoch:23 step:17988 [D loss: 0.233853, acc.: 91.41%] [G loss: 3.318417]\n",
      "epoch:23 step:17989 [D loss: 0.264700, acc.: 89.06%] [G loss: 3.261610]\n",
      "epoch:23 step:17990 [D loss: 0.353393, acc.: 84.38%] [G loss: 2.623121]\n",
      "epoch:23 step:17991 [D loss: 0.307454, acc.: 86.72%] [G loss: 2.941298]\n",
      "epoch:23 step:17992 [D loss: 0.358748, acc.: 83.59%] [G loss: 3.223953]\n",
      "epoch:23 step:17993 [D loss: 0.293501, acc.: 91.41%] [G loss: 2.327731]\n",
      "epoch:23 step:17994 [D loss: 0.336014, acc.: 84.38%] [G loss: 3.356805]\n",
      "epoch:23 step:17995 [D loss: 0.369249, acc.: 85.94%] [G loss: 3.239598]\n",
      "epoch:23 step:17996 [D loss: 0.286972, acc.: 88.28%] [G loss: 3.899688]\n",
      "epoch:23 step:17997 [D loss: 0.320129, acc.: 87.50%] [G loss: 2.803247]\n",
      "epoch:23 step:17998 [D loss: 0.353841, acc.: 81.25%] [G loss: 3.203429]\n",
      "epoch:23 step:17999 [D loss: 0.299573, acc.: 85.94%] [G loss: 2.615418]\n",
      "epoch:23 step:18000 [D loss: 0.404029, acc.: 82.03%] [G loss: 3.205663]\n",
      "##############\n",
      "[0.87610962 0.84688744 0.79763196 0.82205938 0.78323915 0.82668396\n",
      " 0.88102353 0.84323062 0.82619239 0.81831621]\n",
      "##########\n",
      "epoch:23 step:18001 [D loss: 0.361800, acc.: 82.03%] [G loss: 5.101028]\n",
      "epoch:23 step:18002 [D loss: 0.504124, acc.: 78.91%] [G loss: 2.446380]\n",
      "epoch:23 step:18003 [D loss: 0.225398, acc.: 92.97%] [G loss: 2.952431]\n",
      "epoch:23 step:18004 [D loss: 0.397541, acc.: 84.38%] [G loss: 2.570474]\n",
      "epoch:23 step:18005 [D loss: 0.278519, acc.: 89.06%] [G loss: 2.836535]\n",
      "epoch:23 step:18006 [D loss: 0.475933, acc.: 76.56%] [G loss: 2.934610]\n",
      "epoch:23 step:18007 [D loss: 0.560595, acc.: 72.66%] [G loss: 7.625771]\n",
      "epoch:23 step:18008 [D loss: 1.548121, acc.: 55.47%] [G loss: 4.416853]\n",
      "epoch:23 step:18009 [D loss: 0.773157, acc.: 71.88%] [G loss: 2.442505]\n",
      "epoch:23 step:18010 [D loss: 0.500965, acc.: 75.78%] [G loss: 4.284997]\n",
      "epoch:23 step:18011 [D loss: 0.396315, acc.: 81.25%] [G loss: 3.730292]\n",
      "epoch:23 step:18012 [D loss: 0.323969, acc.: 86.72%] [G loss: 3.368524]\n",
      "epoch:23 step:18013 [D loss: 0.430625, acc.: 78.12%] [G loss: 2.329903]\n",
      "epoch:23 step:18014 [D loss: 0.342730, acc.: 84.38%] [G loss: 2.801440]\n",
      "epoch:23 step:18015 [D loss: 0.239805, acc.: 92.19%] [G loss: 3.079702]\n",
      "epoch:23 step:18016 [D loss: 0.287680, acc.: 86.72%] [G loss: 2.861521]\n",
      "epoch:23 step:18017 [D loss: 0.295501, acc.: 91.41%] [G loss: 2.930574]\n",
      "epoch:23 step:18018 [D loss: 0.276938, acc.: 86.72%] [G loss: 3.232299]\n",
      "epoch:23 step:18019 [D loss: 0.298408, acc.: 88.28%] [G loss: 3.277889]\n",
      "epoch:23 step:18020 [D loss: 0.301183, acc.: 88.28%] [G loss: 3.803829]\n",
      "epoch:23 step:18021 [D loss: 0.398838, acc.: 79.69%] [G loss: 3.376178]\n",
      "epoch:23 step:18022 [D loss: 0.346336, acc.: 85.94%] [G loss: 2.288118]\n",
      "epoch:23 step:18023 [D loss: 0.287085, acc.: 89.06%] [G loss: 2.939863]\n",
      "epoch:23 step:18024 [D loss: 0.424352, acc.: 86.72%] [G loss: 3.253719]\n",
      "epoch:23 step:18025 [D loss: 0.345472, acc.: 82.81%] [G loss: 3.103938]\n",
      "epoch:23 step:18026 [D loss: 0.310854, acc.: 87.50%] [G loss: 2.793710]\n",
      "epoch:23 step:18027 [D loss: 0.375549, acc.: 84.38%] [G loss: 2.571366]\n",
      "epoch:23 step:18028 [D loss: 0.390663, acc.: 79.69%] [G loss: 2.756614]\n",
      "epoch:23 step:18029 [D loss: 0.363950, acc.: 82.81%] [G loss: 2.150028]\n",
      "epoch:23 step:18030 [D loss: 0.298869, acc.: 89.06%] [G loss: 2.644761]\n",
      "epoch:23 step:18031 [D loss: 0.308278, acc.: 87.50%] [G loss: 2.951468]\n",
      "epoch:23 step:18032 [D loss: 0.341597, acc.: 85.16%] [G loss: 3.212442]\n",
      "epoch:23 step:18033 [D loss: 0.239811, acc.: 89.06%] [G loss: 2.712551]\n",
      "epoch:23 step:18034 [D loss: 0.333322, acc.: 87.50%] [G loss: 3.515395]\n",
      "epoch:23 step:18035 [D loss: 0.333706, acc.: 85.16%] [G loss: 5.069086]\n",
      "epoch:23 step:18036 [D loss: 0.442717, acc.: 82.81%] [G loss: 4.683621]\n",
      "epoch:23 step:18037 [D loss: 0.339458, acc.: 86.72%] [G loss: 4.543187]\n",
      "epoch:23 step:18038 [D loss: 0.256867, acc.: 89.06%] [G loss: 3.650419]\n",
      "epoch:23 step:18039 [D loss: 0.324497, acc.: 85.16%] [G loss: 4.546277]\n",
      "epoch:23 step:18040 [D loss: 0.341650, acc.: 81.25%] [G loss: 2.856703]\n",
      "epoch:23 step:18041 [D loss: 0.348355, acc.: 84.38%] [G loss: 3.966034]\n",
      "epoch:23 step:18042 [D loss: 0.262478, acc.: 88.28%] [G loss: 2.972525]\n",
      "epoch:23 step:18043 [D loss: 0.311112, acc.: 86.72%] [G loss: 2.756001]\n",
      "epoch:23 step:18044 [D loss: 0.235185, acc.: 89.06%] [G loss: 3.359264]\n",
      "epoch:23 step:18045 [D loss: 0.382327, acc.: 80.47%] [G loss: 2.657948]\n",
      "epoch:23 step:18046 [D loss: 0.348740, acc.: 80.47%] [G loss: 3.961402]\n",
      "epoch:23 step:18047 [D loss: 0.265374, acc.: 88.28%] [G loss: 3.891714]\n",
      "epoch:23 step:18048 [D loss: 0.380563, acc.: 85.16%] [G loss: 4.003560]\n",
      "epoch:23 step:18049 [D loss: 0.353108, acc.: 81.25%] [G loss: 1.957831]\n",
      "epoch:23 step:18050 [D loss: 0.363466, acc.: 84.38%] [G loss: 3.263155]\n",
      "epoch:23 step:18051 [D loss: 0.401705, acc.: 83.59%] [G loss: 3.220035]\n",
      "epoch:23 step:18052 [D loss: 0.430807, acc.: 76.56%] [G loss: 2.741838]\n",
      "epoch:23 step:18053 [D loss: 0.268784, acc.: 86.72%] [G loss: 3.002738]\n",
      "epoch:23 step:18054 [D loss: 0.318233, acc.: 84.38%] [G loss: 1.960414]\n",
      "epoch:23 step:18055 [D loss: 0.391315, acc.: 82.03%] [G loss: 2.767203]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:23 step:18056 [D loss: 0.326707, acc.: 84.38%] [G loss: 3.377979]\n",
      "epoch:23 step:18057 [D loss: 0.436823, acc.: 76.56%] [G loss: 2.675845]\n",
      "epoch:23 step:18058 [D loss: 0.283015, acc.: 86.72%] [G loss: 2.695363]\n",
      "epoch:23 step:18059 [D loss: 0.275087, acc.: 87.50%] [G loss: 2.720829]\n",
      "epoch:23 step:18060 [D loss: 0.434829, acc.: 78.91%] [G loss: 2.994147]\n",
      "epoch:23 step:18061 [D loss: 0.343239, acc.: 82.81%] [G loss: 2.768331]\n",
      "epoch:23 step:18062 [D loss: 0.316222, acc.: 86.72%] [G loss: 2.353104]\n",
      "epoch:23 step:18063 [D loss: 0.291712, acc.: 85.94%] [G loss: 2.537622]\n",
      "epoch:23 step:18064 [D loss: 0.303612, acc.: 84.38%] [G loss: 2.852376]\n",
      "epoch:23 step:18065 [D loss: 0.304009, acc.: 86.72%] [G loss: 3.130867]\n",
      "epoch:23 step:18066 [D loss: 0.385101, acc.: 78.91%] [G loss: 2.987605]\n",
      "epoch:23 step:18067 [D loss: 0.339155, acc.: 83.59%] [G loss: 4.493608]\n",
      "epoch:23 step:18068 [D loss: 0.395093, acc.: 81.25%] [G loss: 2.391247]\n",
      "epoch:23 step:18069 [D loss: 0.294178, acc.: 86.72%] [G loss: 2.345603]\n",
      "epoch:23 step:18070 [D loss: 0.297168, acc.: 87.50%] [G loss: 2.604568]\n",
      "epoch:23 step:18071 [D loss: 0.366066, acc.: 82.81%] [G loss: 2.344583]\n",
      "epoch:23 step:18072 [D loss: 0.385577, acc.: 80.47%] [G loss: 2.704205]\n",
      "epoch:23 step:18073 [D loss: 0.362857, acc.: 84.38%] [G loss: 3.076098]\n",
      "epoch:23 step:18074 [D loss: 0.415257, acc.: 78.12%] [G loss: 4.145884]\n",
      "epoch:23 step:18075 [D loss: 0.423543, acc.: 80.47%] [G loss: 3.401143]\n",
      "epoch:23 step:18076 [D loss: 0.331577, acc.: 82.03%] [G loss: 2.984381]\n",
      "epoch:23 step:18077 [D loss: 0.401215, acc.: 86.72%] [G loss: 3.546663]\n",
      "epoch:23 step:18078 [D loss: 0.358440, acc.: 82.81%] [G loss: 2.769752]\n",
      "epoch:23 step:18079 [D loss: 0.304109, acc.: 86.72%] [G loss: 4.162135]\n",
      "epoch:23 step:18080 [D loss: 0.283639, acc.: 89.84%] [G loss: 3.462635]\n",
      "epoch:23 step:18081 [D loss: 0.256320, acc.: 90.62%] [G loss: 3.746368]\n",
      "epoch:23 step:18082 [D loss: 0.220377, acc.: 89.06%] [G loss: 3.058669]\n",
      "epoch:23 step:18083 [D loss: 0.369049, acc.: 83.59%] [G loss: 3.135491]\n",
      "epoch:23 step:18084 [D loss: 0.272487, acc.: 89.84%] [G loss: 3.408708]\n",
      "epoch:23 step:18085 [D loss: 0.279568, acc.: 89.06%] [G loss: 3.752495]\n",
      "epoch:23 step:18086 [D loss: 0.270036, acc.: 85.94%] [G loss: 2.906413]\n",
      "epoch:23 step:18087 [D loss: 0.275973, acc.: 87.50%] [G loss: 2.604010]\n",
      "epoch:23 step:18088 [D loss: 0.367725, acc.: 83.59%] [G loss: 2.970293]\n",
      "epoch:23 step:18089 [D loss: 0.344648, acc.: 85.94%] [G loss: 2.308044]\n",
      "epoch:23 step:18090 [D loss: 0.357480, acc.: 83.59%] [G loss: 2.639216]\n",
      "epoch:23 step:18091 [D loss: 0.357873, acc.: 83.59%] [G loss: 2.370250]\n",
      "epoch:23 step:18092 [D loss: 0.336106, acc.: 86.72%] [G loss: 2.625537]\n",
      "epoch:23 step:18093 [D loss: 0.351707, acc.: 83.59%] [G loss: 2.908354]\n",
      "epoch:23 step:18094 [D loss: 0.405106, acc.: 82.03%] [G loss: 3.098104]\n",
      "epoch:23 step:18095 [D loss: 0.321513, acc.: 85.94%] [G loss: 4.839603]\n",
      "epoch:23 step:18096 [D loss: 0.341340, acc.: 85.94%] [G loss: 4.899981]\n",
      "epoch:23 step:18097 [D loss: 0.275121, acc.: 90.62%] [G loss: 7.268745]\n",
      "epoch:23 step:18098 [D loss: 0.238907, acc.: 89.06%] [G loss: 4.442702]\n",
      "epoch:23 step:18099 [D loss: 0.306898, acc.: 87.50%] [G loss: 4.175309]\n",
      "epoch:23 step:18100 [D loss: 0.413864, acc.: 81.25%] [G loss: 3.501066]\n",
      "epoch:23 step:18101 [D loss: 0.248641, acc.: 88.28%] [G loss: 3.982380]\n",
      "epoch:23 step:18102 [D loss: 0.256939, acc.: 89.06%] [G loss: 3.858347]\n",
      "epoch:23 step:18103 [D loss: 0.251189, acc.: 89.06%] [G loss: 3.263899]\n",
      "epoch:23 step:18104 [D loss: 0.282150, acc.: 86.72%] [G loss: 3.735136]\n",
      "epoch:23 step:18105 [D loss: 0.355461, acc.: 84.38%] [G loss: 4.600121]\n",
      "epoch:23 step:18106 [D loss: 0.286729, acc.: 89.84%] [G loss: 3.887321]\n",
      "epoch:23 step:18107 [D loss: 0.265925, acc.: 90.62%] [G loss: 3.807197]\n",
      "epoch:23 step:18108 [D loss: 0.301271, acc.: 89.06%] [G loss: 4.013970]\n",
      "epoch:23 step:18109 [D loss: 0.310679, acc.: 85.94%] [G loss: 4.141153]\n",
      "epoch:23 step:18110 [D loss: 0.305220, acc.: 85.94%] [G loss: 4.841646]\n",
      "epoch:23 step:18111 [D loss: 0.278078, acc.: 85.94%] [G loss: 3.503281]\n",
      "epoch:23 step:18112 [D loss: 0.313893, acc.: 85.94%] [G loss: 3.725356]\n",
      "epoch:23 step:18113 [D loss: 0.268151, acc.: 89.06%] [G loss: 3.813493]\n",
      "epoch:23 step:18114 [D loss: 0.338628, acc.: 83.59%] [G loss: 4.094284]\n",
      "epoch:23 step:18115 [D loss: 0.275536, acc.: 89.06%] [G loss: 2.957937]\n",
      "epoch:23 step:18116 [D loss: 0.332221, acc.: 87.50%] [G loss: 2.700739]\n",
      "epoch:23 step:18117 [D loss: 0.249788, acc.: 90.62%] [G loss: 2.087483]\n",
      "epoch:23 step:18118 [D loss: 0.354660, acc.: 85.94%] [G loss: 2.870457]\n",
      "epoch:23 step:18119 [D loss: 0.393884, acc.: 78.91%] [G loss: 2.160266]\n",
      "epoch:23 step:18120 [D loss: 0.360744, acc.: 85.94%] [G loss: 3.995254]\n",
      "epoch:23 step:18121 [D loss: 0.577142, acc.: 77.34%] [G loss: 4.564462]\n",
      "epoch:23 step:18122 [D loss: 0.432325, acc.: 80.47%] [G loss: 3.159883]\n",
      "epoch:23 step:18123 [D loss: 0.487179, acc.: 77.34%] [G loss: 3.002488]\n",
      "epoch:23 step:18124 [D loss: 0.375325, acc.: 82.03%] [G loss: 2.602138]\n",
      "epoch:23 step:18125 [D loss: 0.356563, acc.: 85.16%] [G loss: 2.828385]\n",
      "epoch:23 step:18126 [D loss: 0.342530, acc.: 86.72%] [G loss: 3.103145]\n",
      "epoch:23 step:18127 [D loss: 0.327555, acc.: 84.38%] [G loss: 2.929454]\n",
      "epoch:23 step:18128 [D loss: 0.293306, acc.: 89.06%] [G loss: 2.469872]\n",
      "epoch:23 step:18129 [D loss: 0.320608, acc.: 85.94%] [G loss: 2.424932]\n",
      "epoch:23 step:18130 [D loss: 0.368793, acc.: 80.47%] [G loss: 3.240467]\n",
      "epoch:23 step:18131 [D loss: 0.351427, acc.: 85.16%] [G loss: 3.714539]\n",
      "epoch:23 step:18132 [D loss: 0.282371, acc.: 92.19%] [G loss: 3.290801]\n",
      "epoch:23 step:18133 [D loss: 0.291069, acc.: 87.50%] [G loss: 2.844927]\n",
      "epoch:23 step:18134 [D loss: 0.290811, acc.: 87.50%] [G loss: 3.135413]\n",
      "epoch:23 step:18135 [D loss: 0.285125, acc.: 85.16%] [G loss: 2.885921]\n",
      "epoch:23 step:18136 [D loss: 0.318188, acc.: 86.72%] [G loss: 3.398433]\n",
      "epoch:23 step:18137 [D loss: 0.560617, acc.: 78.12%] [G loss: 4.726813]\n",
      "epoch:23 step:18138 [D loss: 0.607150, acc.: 78.91%] [G loss: 6.596474]\n",
      "epoch:23 step:18139 [D loss: 0.540038, acc.: 78.91%] [G loss: 2.901068]\n",
      "epoch:23 step:18140 [D loss: 0.220054, acc.: 89.06%] [G loss: 5.049664]\n",
      "epoch:23 step:18141 [D loss: 0.285693, acc.: 89.06%] [G loss: 3.665038]\n",
      "epoch:23 step:18142 [D loss: 0.282371, acc.: 88.28%] [G loss: 3.328295]\n",
      "epoch:23 step:18143 [D loss: 0.356507, acc.: 82.81%] [G loss: 2.706766]\n",
      "epoch:23 step:18144 [D loss: 0.251696, acc.: 89.84%] [G loss: 3.464095]\n",
      "epoch:23 step:18145 [D loss: 0.407545, acc.: 81.25%] [G loss: 2.757320]\n",
      "epoch:23 step:18146 [D loss: 0.399189, acc.: 86.72%] [G loss: 2.146821]\n",
      "epoch:23 step:18147 [D loss: 0.355665, acc.: 81.25%] [G loss: 2.397729]\n",
      "epoch:23 step:18148 [D loss: 0.292954, acc.: 88.28%] [G loss: 2.333925]\n",
      "epoch:23 step:18149 [D loss: 0.309323, acc.: 88.28%] [G loss: 2.553420]\n",
      "epoch:23 step:18150 [D loss: 0.365489, acc.: 81.25%] [G loss: 2.831812]\n",
      "epoch:23 step:18151 [D loss: 0.355328, acc.: 86.72%] [G loss: 2.908700]\n",
      "epoch:23 step:18152 [D loss: 0.360601, acc.: 79.69%] [G loss: 2.449966]\n",
      "epoch:23 step:18153 [D loss: 0.275040, acc.: 88.28%] [G loss: 2.916030]\n",
      "epoch:23 step:18154 [D loss: 0.416254, acc.: 79.69%] [G loss: 2.471654]\n",
      "epoch:23 step:18155 [D loss: 0.248981, acc.: 90.62%] [G loss: 3.421108]\n",
      "epoch:23 step:18156 [D loss: 0.260778, acc.: 91.41%] [G loss: 3.144367]\n",
      "epoch:23 step:18157 [D loss: 0.305350, acc.: 83.59%] [G loss: 3.176801]\n",
      "epoch:23 step:18158 [D loss: 0.303448, acc.: 86.72%] [G loss: 3.614240]\n",
      "epoch:23 step:18159 [D loss: 0.311920, acc.: 87.50%] [G loss: 3.173702]\n",
      "epoch:23 step:18160 [D loss: 0.333431, acc.: 88.28%] [G loss: 3.075572]\n",
      "epoch:23 step:18161 [D loss: 0.302709, acc.: 88.28%] [G loss: 4.338681]\n",
      "epoch:23 step:18162 [D loss: 0.228604, acc.: 92.97%] [G loss: 4.972538]\n",
      "epoch:23 step:18163 [D loss: 0.218124, acc.: 95.31%] [G loss: 5.162133]\n",
      "epoch:23 step:18164 [D loss: 0.350785, acc.: 84.38%] [G loss: 2.801761]\n",
      "epoch:23 step:18165 [D loss: 0.454078, acc.: 80.47%] [G loss: 3.934578]\n",
      "epoch:23 step:18166 [D loss: 0.424282, acc.: 80.47%] [G loss: 2.737146]\n",
      "epoch:23 step:18167 [D loss: 0.250337, acc.: 91.41%] [G loss: 4.458725]\n",
      "epoch:23 step:18168 [D loss: 0.375261, acc.: 82.81%] [G loss: 6.295099]\n",
      "epoch:23 step:18169 [D loss: 0.366086, acc.: 85.94%] [G loss: 3.748144]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:23 step:18170 [D loss: 0.298827, acc.: 86.72%] [G loss: 3.005649]\n",
      "epoch:23 step:18171 [D loss: 0.309716, acc.: 86.72%] [G loss: 4.909398]\n",
      "epoch:23 step:18172 [D loss: 0.445711, acc.: 78.91%] [G loss: 2.476273]\n",
      "epoch:23 step:18173 [D loss: 0.288443, acc.: 91.41%] [G loss: 3.842273]\n",
      "epoch:23 step:18174 [D loss: 0.356525, acc.: 85.94%] [G loss: 3.427281]\n",
      "epoch:23 step:18175 [D loss: 0.248335, acc.: 86.72%] [G loss: 3.096786]\n",
      "epoch:23 step:18176 [D loss: 0.317155, acc.: 87.50%] [G loss: 2.828971]\n",
      "epoch:23 step:18177 [D loss: 0.311934, acc.: 86.72%] [G loss: 2.868215]\n",
      "epoch:23 step:18178 [D loss: 0.293661, acc.: 91.41%] [G loss: 2.727244]\n",
      "epoch:23 step:18179 [D loss: 0.283979, acc.: 87.50%] [G loss: 2.458388]\n",
      "epoch:23 step:18180 [D loss: 0.393051, acc.: 77.34%] [G loss: 2.844738]\n",
      "epoch:23 step:18181 [D loss: 0.356181, acc.: 82.81%] [G loss: 3.184812]\n",
      "epoch:23 step:18182 [D loss: 0.300106, acc.: 87.50%] [G loss: 3.200328]\n",
      "epoch:23 step:18183 [D loss: 0.348508, acc.: 85.16%] [G loss: 3.513686]\n",
      "epoch:23 step:18184 [D loss: 0.262579, acc.: 85.94%] [G loss: 4.577276]\n",
      "epoch:23 step:18185 [D loss: 0.224401, acc.: 92.19%] [G loss: 4.295226]\n",
      "epoch:23 step:18186 [D loss: 0.332246, acc.: 87.50%] [G loss: 2.784494]\n",
      "epoch:23 step:18187 [D loss: 0.332943, acc.: 82.03%] [G loss: 3.840119]\n",
      "epoch:23 step:18188 [D loss: 0.371942, acc.: 86.72%] [G loss: 2.751417]\n",
      "epoch:23 step:18189 [D loss: 0.288770, acc.: 84.38%] [G loss: 3.835823]\n",
      "epoch:23 step:18190 [D loss: 0.401905, acc.: 85.94%] [G loss: 2.884228]\n",
      "epoch:23 step:18191 [D loss: 0.346281, acc.: 83.59%] [G loss: 2.731424]\n",
      "epoch:23 step:18192 [D loss: 0.484499, acc.: 77.34%] [G loss: 2.475008]\n",
      "epoch:23 step:18193 [D loss: 0.372961, acc.: 83.59%] [G loss: 3.260251]\n",
      "epoch:23 step:18194 [D loss: 0.357962, acc.: 89.84%] [G loss: 2.510875]\n",
      "epoch:23 step:18195 [D loss: 0.367365, acc.: 84.38%] [G loss: 4.306608]\n",
      "epoch:23 step:18196 [D loss: 0.434598, acc.: 80.47%] [G loss: 3.001524]\n",
      "epoch:23 step:18197 [D loss: 0.263809, acc.: 89.84%] [G loss: 3.632856]\n",
      "epoch:23 step:18198 [D loss: 0.330237, acc.: 85.94%] [G loss: 3.877669]\n",
      "epoch:23 step:18199 [D loss: 0.256929, acc.: 88.28%] [G loss: 2.836705]\n",
      "epoch:23 step:18200 [D loss: 0.354459, acc.: 80.47%] [G loss: 4.537627]\n",
      "##############\n",
      "[0.87265258 0.86086817 0.79253678 0.82180277 0.77492355 0.8091996\n",
      " 0.86709884 0.84110611 0.84189615 0.83790533]\n",
      "##########\n",
      "epoch:23 step:18201 [D loss: 0.340634, acc.: 82.03%] [G loss: 3.842661]\n",
      "epoch:23 step:18202 [D loss: 0.212684, acc.: 92.97%] [G loss: 6.500684]\n",
      "epoch:23 step:18203 [D loss: 0.317690, acc.: 82.03%] [G loss: 3.789615]\n",
      "epoch:23 step:18204 [D loss: 0.294940, acc.: 86.72%] [G loss: 3.239027]\n",
      "epoch:23 step:18205 [D loss: 0.276296, acc.: 90.62%] [G loss: 4.473030]\n",
      "epoch:23 step:18206 [D loss: 0.279465, acc.: 86.72%] [G loss: 3.385528]\n",
      "epoch:23 step:18207 [D loss: 0.316675, acc.: 85.16%] [G loss: 2.272726]\n",
      "epoch:23 step:18208 [D loss: 0.341252, acc.: 83.59%] [G loss: 3.447071]\n",
      "epoch:23 step:18209 [D loss: 0.251818, acc.: 89.84%] [G loss: 2.893373]\n",
      "epoch:23 step:18210 [D loss: 0.386398, acc.: 85.94%] [G loss: 4.807601]\n",
      "epoch:23 step:18211 [D loss: 0.338830, acc.: 84.38%] [G loss: 4.087477]\n",
      "epoch:23 step:18212 [D loss: 0.294173, acc.: 85.16%] [G loss: 4.633728]\n",
      "epoch:23 step:18213 [D loss: 0.316233, acc.: 86.72%] [G loss: 4.501058]\n",
      "epoch:23 step:18214 [D loss: 0.152535, acc.: 93.75%] [G loss: 4.926958]\n",
      "epoch:23 step:18215 [D loss: 0.376545, acc.: 77.34%] [G loss: 3.953656]\n",
      "epoch:23 step:18216 [D loss: 0.455223, acc.: 82.03%] [G loss: 3.701167]\n",
      "epoch:23 step:18217 [D loss: 0.326397, acc.: 86.72%] [G loss: 4.367837]\n",
      "epoch:23 step:18218 [D loss: 0.380438, acc.: 85.16%] [G loss: 3.659471]\n",
      "epoch:23 step:18219 [D loss: 0.352958, acc.: 86.72%] [G loss: 4.132423]\n",
      "epoch:23 step:18220 [D loss: 0.317328, acc.: 83.59%] [G loss: 4.387007]\n",
      "epoch:23 step:18221 [D loss: 0.333148, acc.: 85.94%] [G loss: 2.870226]\n",
      "epoch:23 step:18222 [D loss: 0.369660, acc.: 83.59%] [G loss: 4.281301]\n",
      "epoch:23 step:18223 [D loss: 0.304419, acc.: 89.06%] [G loss: 4.215582]\n",
      "epoch:23 step:18224 [D loss: 0.259735, acc.: 89.06%] [G loss: 4.738100]\n",
      "epoch:23 step:18225 [D loss: 0.293366, acc.: 87.50%] [G loss: 5.325200]\n",
      "epoch:23 step:18226 [D loss: 0.207061, acc.: 90.62%] [G loss: 5.649148]\n",
      "epoch:23 step:18227 [D loss: 0.345763, acc.: 82.03%] [G loss: 3.183618]\n",
      "epoch:23 step:18228 [D loss: 0.373617, acc.: 80.47%] [G loss: 3.185905]\n",
      "epoch:23 step:18229 [D loss: 0.234831, acc.: 89.06%] [G loss: 4.043891]\n",
      "epoch:23 step:18230 [D loss: 0.370598, acc.: 80.47%] [G loss: 4.195171]\n",
      "epoch:23 step:18231 [D loss: 0.328724, acc.: 84.38%] [G loss: 3.349170]\n",
      "epoch:23 step:18232 [D loss: 0.311140, acc.: 83.59%] [G loss: 3.383351]\n",
      "epoch:23 step:18233 [D loss: 0.297034, acc.: 89.84%] [G loss: 2.548209]\n",
      "epoch:23 step:18234 [D loss: 0.221741, acc.: 89.06%] [G loss: 3.993023]\n",
      "epoch:23 step:18235 [D loss: 0.266309, acc.: 88.28%] [G loss: 3.711144]\n",
      "epoch:23 step:18236 [D loss: 0.329194, acc.: 86.72%] [G loss: 3.255847]\n",
      "epoch:23 step:18237 [D loss: 0.228267, acc.: 92.19%] [G loss: 2.931351]\n",
      "epoch:23 step:18238 [D loss: 0.451756, acc.: 79.69%] [G loss: 5.346179]\n",
      "epoch:23 step:18239 [D loss: 0.608128, acc.: 68.75%] [G loss: 6.047248]\n",
      "epoch:23 step:18240 [D loss: 0.651594, acc.: 69.53%] [G loss: 7.967132]\n",
      "epoch:23 step:18241 [D loss: 0.687772, acc.: 70.31%] [G loss: 4.004047]\n",
      "epoch:23 step:18242 [D loss: 0.809525, acc.: 74.22%] [G loss: 6.451377]\n",
      "epoch:23 step:18243 [D loss: 0.732691, acc.: 75.78%] [G loss: 3.848207]\n",
      "epoch:23 step:18244 [D loss: 0.360216, acc.: 79.69%] [G loss: 4.605026]\n",
      "epoch:23 step:18245 [D loss: 0.486002, acc.: 79.69%] [G loss: 5.529509]\n",
      "epoch:23 step:18246 [D loss: 0.428654, acc.: 85.16%] [G loss: 4.852570]\n",
      "epoch:23 step:18247 [D loss: 0.381990, acc.: 85.16%] [G loss: 5.019577]\n",
      "epoch:23 step:18248 [D loss: 0.519411, acc.: 78.12%] [G loss: 2.683197]\n",
      "epoch:23 step:18249 [D loss: 0.443680, acc.: 83.59%] [G loss: 2.494720]\n",
      "epoch:23 step:18250 [D loss: 0.403583, acc.: 82.03%] [G loss: 3.501919]\n",
      "epoch:23 step:18251 [D loss: 0.436251, acc.: 80.47%] [G loss: 3.036271]\n",
      "epoch:23 step:18252 [D loss: 0.360476, acc.: 82.81%] [G loss: 3.160559]\n",
      "epoch:23 step:18253 [D loss: 0.357155, acc.: 85.16%] [G loss: 2.728457]\n",
      "epoch:23 step:18254 [D loss: 0.280672, acc.: 86.72%] [G loss: 3.233260]\n",
      "epoch:23 step:18255 [D loss: 0.322470, acc.: 85.16%] [G loss: 2.369889]\n",
      "epoch:23 step:18256 [D loss: 0.349840, acc.: 85.94%] [G loss: 2.689487]\n",
      "epoch:23 step:18257 [D loss: 0.329543, acc.: 86.72%] [G loss: 2.873768]\n",
      "epoch:23 step:18258 [D loss: 0.374786, acc.: 77.34%] [G loss: 2.843379]\n",
      "epoch:23 step:18259 [D loss: 0.318614, acc.: 85.16%] [G loss: 2.767930]\n",
      "epoch:23 step:18260 [D loss: 0.323721, acc.: 85.94%] [G loss: 2.619514]\n",
      "epoch:23 step:18261 [D loss: 0.326824, acc.: 85.16%] [G loss: 2.078538]\n",
      "epoch:23 step:18262 [D loss: 0.329345, acc.: 86.72%] [G loss: 2.448404]\n",
      "epoch:23 step:18263 [D loss: 0.279724, acc.: 89.84%] [G loss: 2.475975]\n",
      "epoch:23 step:18264 [D loss: 0.383768, acc.: 83.59%] [G loss: 2.365881]\n",
      "epoch:23 step:18265 [D loss: 0.374883, acc.: 79.69%] [G loss: 2.901216]\n",
      "epoch:23 step:18266 [D loss: 0.381317, acc.: 84.38%] [G loss: 2.648098]\n",
      "epoch:23 step:18267 [D loss: 0.267350, acc.: 89.06%] [G loss: 2.725320]\n",
      "epoch:23 step:18268 [D loss: 0.336512, acc.: 86.72%] [G loss: 3.059817]\n",
      "epoch:23 step:18269 [D loss: 0.312807, acc.: 83.59%] [G loss: 2.744531]\n",
      "epoch:23 step:18270 [D loss: 0.269617, acc.: 89.84%] [G loss: 2.585661]\n",
      "epoch:23 step:18271 [D loss: 0.389448, acc.: 82.03%] [G loss: 2.729789]\n",
      "epoch:23 step:18272 [D loss: 0.308679, acc.: 85.16%] [G loss: 2.324184]\n",
      "epoch:23 step:18273 [D loss: 0.247279, acc.: 89.84%] [G loss: 2.792364]\n",
      "epoch:23 step:18274 [D loss: 0.363851, acc.: 82.03%] [G loss: 3.159492]\n",
      "epoch:23 step:18275 [D loss: 0.372605, acc.: 81.25%] [G loss: 3.542677]\n",
      "epoch:23 step:18276 [D loss: 0.353157, acc.: 83.59%] [G loss: 2.163584]\n",
      "epoch:23 step:18277 [D loss: 0.407214, acc.: 78.91%] [G loss: 2.945587]\n",
      "epoch:23 step:18278 [D loss: 0.370700, acc.: 82.81%] [G loss: 2.875412]\n",
      "epoch:23 step:18279 [D loss: 0.276297, acc.: 87.50%] [G loss: 3.941919]\n",
      "epoch:23 step:18280 [D loss: 0.341695, acc.: 84.38%] [G loss: 2.754477]\n",
      "epoch:23 step:18281 [D loss: 0.298824, acc.: 87.50%] [G loss: 3.511984]\n",
      "epoch:23 step:18282 [D loss: 0.258779, acc.: 87.50%] [G loss: 4.104164]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:23 step:18283 [D loss: 0.286810, acc.: 86.72%] [G loss: 3.844141]\n",
      "epoch:23 step:18284 [D loss: 0.283402, acc.: 89.06%] [G loss: 3.743037]\n",
      "epoch:23 step:18285 [D loss: 0.287979, acc.: 89.06%] [G loss: 2.720881]\n",
      "epoch:23 step:18286 [D loss: 0.389959, acc.: 84.38%] [G loss: 3.151617]\n",
      "epoch:23 step:18287 [D loss: 0.364447, acc.: 85.16%] [G loss: 2.690835]\n",
      "epoch:23 step:18288 [D loss: 0.333087, acc.: 83.59%] [G loss: 3.317811]\n",
      "epoch:23 step:18289 [D loss: 0.342946, acc.: 85.94%] [G loss: 3.701334]\n",
      "epoch:23 step:18290 [D loss: 0.214155, acc.: 92.97%] [G loss: 3.953053]\n",
      "epoch:23 step:18291 [D loss: 0.308681, acc.: 89.06%] [G loss: 3.976005]\n",
      "epoch:23 step:18292 [D loss: 0.337963, acc.: 82.03%] [G loss: 2.522093]\n",
      "epoch:23 step:18293 [D loss: 0.284385, acc.: 87.50%] [G loss: 3.447261]\n",
      "epoch:23 step:18294 [D loss: 0.338871, acc.: 85.94%] [G loss: 2.860843]\n",
      "epoch:23 step:18295 [D loss: 0.282051, acc.: 85.94%] [G loss: 3.662305]\n",
      "epoch:23 step:18296 [D loss: 0.282739, acc.: 87.50%] [G loss: 3.847919]\n",
      "epoch:23 step:18297 [D loss: 0.323416, acc.: 88.28%] [G loss: 3.743791]\n",
      "epoch:23 step:18298 [D loss: 0.252280, acc.: 92.19%] [G loss: 3.118797]\n",
      "epoch:23 step:18299 [D loss: 0.245185, acc.: 89.06%] [G loss: 2.851017]\n",
      "epoch:23 step:18300 [D loss: 0.287060, acc.: 88.28%] [G loss: 3.224070]\n",
      "epoch:23 step:18301 [D loss: 0.253732, acc.: 90.62%] [G loss: 3.753206]\n",
      "epoch:23 step:18302 [D loss: 0.292529, acc.: 83.59%] [G loss: 2.537163]\n",
      "epoch:23 step:18303 [D loss: 0.361496, acc.: 82.03%] [G loss: 2.762883]\n",
      "epoch:23 step:18304 [D loss: 0.292411, acc.: 87.50%] [G loss: 2.875701]\n",
      "epoch:23 step:18305 [D loss: 0.310572, acc.: 89.06%] [G loss: 2.926310]\n",
      "epoch:23 step:18306 [D loss: 0.333969, acc.: 84.38%] [G loss: 3.124175]\n",
      "epoch:23 step:18307 [D loss: 0.355612, acc.: 84.38%] [G loss: 2.590235]\n",
      "epoch:23 step:18308 [D loss: 0.316262, acc.: 87.50%] [G loss: 2.659815]\n",
      "epoch:23 step:18309 [D loss: 0.431257, acc.: 81.25%] [G loss: 2.851681]\n",
      "epoch:23 step:18310 [D loss: 0.264417, acc.: 88.28%] [G loss: 2.399890]\n",
      "epoch:23 step:18311 [D loss: 0.269741, acc.: 86.72%] [G loss: 3.041961]\n",
      "epoch:23 step:18312 [D loss: 0.361557, acc.: 85.16%] [G loss: 2.381052]\n",
      "epoch:23 step:18313 [D loss: 0.338712, acc.: 82.81%] [G loss: 3.722795]\n",
      "epoch:23 step:18314 [D loss: 0.335505, acc.: 84.38%] [G loss: 2.964650]\n",
      "epoch:23 step:18315 [D loss: 0.398432, acc.: 84.38%] [G loss: 2.380051]\n",
      "epoch:23 step:18316 [D loss: 0.338355, acc.: 85.16%] [G loss: 2.882962]\n",
      "epoch:23 step:18317 [D loss: 0.323433, acc.: 89.06%] [G loss: 3.747417]\n",
      "epoch:23 step:18318 [D loss: 0.247023, acc.: 90.62%] [G loss: 3.644541]\n",
      "epoch:23 step:18319 [D loss: 0.257547, acc.: 89.84%] [G loss: 3.432598]\n",
      "epoch:23 step:18320 [D loss: 0.333785, acc.: 85.16%] [G loss: 2.801629]\n",
      "epoch:23 step:18321 [D loss: 0.351384, acc.: 85.94%] [G loss: 3.241874]\n",
      "epoch:23 step:18322 [D loss: 0.284646, acc.: 84.38%] [G loss: 3.787259]\n",
      "epoch:23 step:18323 [D loss: 0.398602, acc.: 82.81%] [G loss: 3.314992]\n",
      "epoch:23 step:18324 [D loss: 0.239953, acc.: 89.84%] [G loss: 3.572000]\n",
      "epoch:23 step:18325 [D loss: 0.284100, acc.: 89.84%] [G loss: 4.048489]\n",
      "epoch:23 step:18326 [D loss: 0.377464, acc.: 84.38%] [G loss: 3.478369]\n",
      "epoch:23 step:18327 [D loss: 0.371942, acc.: 84.38%] [G loss: 3.547143]\n",
      "epoch:23 step:18328 [D loss: 0.361874, acc.: 81.25%] [G loss: 3.651830]\n",
      "epoch:23 step:18329 [D loss: 0.322804, acc.: 86.72%] [G loss: 3.317538]\n",
      "epoch:23 step:18330 [D loss: 0.366042, acc.: 86.72%] [G loss: 3.429993]\n",
      "epoch:23 step:18331 [D loss: 0.275322, acc.: 89.84%] [G loss: 2.979087]\n",
      "epoch:23 step:18332 [D loss: 0.463747, acc.: 78.91%] [G loss: 4.744875]\n",
      "epoch:23 step:18333 [D loss: 0.278545, acc.: 88.28%] [G loss: 4.090233]\n",
      "epoch:23 step:18334 [D loss: 0.315463, acc.: 83.59%] [G loss: 4.616169]\n",
      "epoch:23 step:18335 [D loss: 0.284710, acc.: 85.16%] [G loss: 4.872232]\n",
      "epoch:23 step:18336 [D loss: 0.343624, acc.: 86.72%] [G loss: 5.159766]\n",
      "epoch:23 step:18337 [D loss: 0.301726, acc.: 88.28%] [G loss: 4.416824]\n",
      "epoch:23 step:18338 [D loss: 0.303606, acc.: 90.62%] [G loss: 4.889694]\n",
      "epoch:23 step:18339 [D loss: 0.280529, acc.: 89.06%] [G loss: 4.191291]\n",
      "epoch:23 step:18340 [D loss: 0.272603, acc.: 89.06%] [G loss: 3.599926]\n",
      "epoch:23 step:18341 [D loss: 0.367064, acc.: 82.81%] [G loss: 3.041911]\n",
      "epoch:23 step:18342 [D loss: 0.341917, acc.: 83.59%] [G loss: 3.690866]\n",
      "epoch:23 step:18343 [D loss: 0.263527, acc.: 88.28%] [G loss: 2.601188]\n",
      "epoch:23 step:18344 [D loss: 0.258773, acc.: 87.50%] [G loss: 3.415193]\n",
      "epoch:23 step:18345 [D loss: 0.336803, acc.: 87.50%] [G loss: 3.485819]\n",
      "epoch:23 step:18346 [D loss: 0.289841, acc.: 84.38%] [G loss: 4.149935]\n",
      "epoch:23 step:18347 [D loss: 0.220490, acc.: 93.75%] [G loss: 3.118469]\n",
      "epoch:23 step:18348 [D loss: 0.317100, acc.: 85.94%] [G loss: 3.452591]\n",
      "epoch:23 step:18349 [D loss: 0.289241, acc.: 85.94%] [G loss: 3.067547]\n",
      "epoch:23 step:18350 [D loss: 0.297280, acc.: 86.72%] [G loss: 2.890404]\n",
      "epoch:23 step:18351 [D loss: 0.371432, acc.: 79.69%] [G loss: 2.743636]\n",
      "epoch:23 step:18352 [D loss: 0.318082, acc.: 85.94%] [G loss: 3.090891]\n",
      "epoch:23 step:18353 [D loss: 0.277414, acc.: 86.72%] [G loss: 3.435363]\n",
      "epoch:23 step:18354 [D loss: 0.307306, acc.: 86.72%] [G loss: 3.692094]\n",
      "epoch:23 step:18355 [D loss: 0.242931, acc.: 88.28%] [G loss: 4.657428]\n",
      "epoch:23 step:18356 [D loss: 0.256566, acc.: 92.97%] [G loss: 4.165082]\n",
      "epoch:23 step:18357 [D loss: 0.352964, acc.: 79.69%] [G loss: 4.046810]\n",
      "epoch:23 step:18358 [D loss: 0.306377, acc.: 86.72%] [G loss: 4.032329]\n",
      "epoch:23 step:18359 [D loss: 0.412582, acc.: 82.81%] [G loss: 4.002061]\n",
      "epoch:23 step:18360 [D loss: 0.497140, acc.: 71.88%] [G loss: 4.015311]\n",
      "epoch:23 step:18361 [D loss: 0.298612, acc.: 85.94%] [G loss: 4.326396]\n",
      "epoch:23 step:18362 [D loss: 0.341862, acc.: 88.28%] [G loss: 4.598504]\n",
      "epoch:23 step:18363 [D loss: 0.311795, acc.: 88.28%] [G loss: 3.683296]\n",
      "epoch:23 step:18364 [D loss: 0.348939, acc.: 88.28%] [G loss: 3.422416]\n",
      "epoch:23 step:18365 [D loss: 0.257945, acc.: 91.41%] [G loss: 4.109855]\n",
      "epoch:23 step:18366 [D loss: 0.343955, acc.: 81.25%] [G loss: 4.988047]\n",
      "epoch:23 step:18367 [D loss: 0.275906, acc.: 89.06%] [G loss: 5.484253]\n",
      "epoch:23 step:18368 [D loss: 0.344896, acc.: 83.59%] [G loss: 3.581687]\n",
      "epoch:23 step:18369 [D loss: 0.364416, acc.: 85.16%] [G loss: 3.934741]\n",
      "epoch:23 step:18370 [D loss: 0.337096, acc.: 84.38%] [G loss: 4.248378]\n",
      "epoch:23 step:18371 [D loss: 0.367325, acc.: 81.25%] [G loss: 3.372072]\n",
      "epoch:23 step:18372 [D loss: 0.342942, acc.: 85.16%] [G loss: 3.434707]\n",
      "epoch:23 step:18373 [D loss: 0.276031, acc.: 89.84%] [G loss: 3.532169]\n",
      "epoch:23 step:18374 [D loss: 0.331093, acc.: 85.94%] [G loss: 5.272738]\n",
      "epoch:23 step:18375 [D loss: 0.629485, acc.: 69.53%] [G loss: 3.396480]\n",
      "epoch:23 step:18376 [D loss: 0.362255, acc.: 81.25%] [G loss: 3.267589]\n",
      "epoch:23 step:18377 [D loss: 0.381952, acc.: 82.81%] [G loss: 4.038337]\n",
      "epoch:23 step:18378 [D loss: 0.327040, acc.: 87.50%] [G loss: 3.646253]\n",
      "epoch:23 step:18379 [D loss: 0.329177, acc.: 84.38%] [G loss: 3.233635]\n",
      "epoch:23 step:18380 [D loss: 0.385947, acc.: 82.03%] [G loss: 2.887708]\n",
      "epoch:23 step:18381 [D loss: 0.291922, acc.: 86.72%] [G loss: 3.312632]\n",
      "epoch:23 step:18382 [D loss: 0.248622, acc.: 89.84%] [G loss: 3.742901]\n",
      "epoch:23 step:18383 [D loss: 0.215615, acc.: 90.62%] [G loss: 2.828245]\n",
      "epoch:23 step:18384 [D loss: 0.327008, acc.: 88.28%] [G loss: 3.179988]\n",
      "epoch:23 step:18385 [D loss: 0.267766, acc.: 87.50%] [G loss: 2.958797]\n",
      "epoch:23 step:18386 [D loss: 0.372665, acc.: 78.91%] [G loss: 3.023640]\n",
      "epoch:23 step:18387 [D loss: 0.251753, acc.: 90.62%] [G loss: 4.018461]\n",
      "epoch:23 step:18388 [D loss: 0.331399, acc.: 87.50%] [G loss: 3.007993]\n",
      "epoch:23 step:18389 [D loss: 0.323756, acc.: 85.16%] [G loss: 3.214733]\n",
      "epoch:23 step:18390 [D loss: 0.276768, acc.: 87.50%] [G loss: 3.791522]\n",
      "epoch:23 step:18391 [D loss: 0.293018, acc.: 89.06%] [G loss: 3.553198]\n",
      "epoch:23 step:18392 [D loss: 0.208674, acc.: 93.75%] [G loss: 4.999261]\n",
      "epoch:23 step:18393 [D loss: 0.278121, acc.: 86.72%] [G loss: 7.047955]\n",
      "epoch:23 step:18394 [D loss: 0.297924, acc.: 82.03%] [G loss: 5.122180]\n",
      "epoch:23 step:18395 [D loss: 0.363048, acc.: 85.94%] [G loss: 5.927229]\n",
      "epoch:23 step:18396 [D loss: 0.280739, acc.: 85.94%] [G loss: 5.750126]\n",
      "epoch:23 step:18397 [D loss: 0.343538, acc.: 84.38%] [G loss: 3.815568]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:23 step:18398 [D loss: 0.255869, acc.: 90.62%] [G loss: 3.260106]\n",
      "epoch:23 step:18399 [D loss: 0.342560, acc.: 86.72%] [G loss: 4.972154]\n",
      "epoch:23 step:18400 [D loss: 0.550406, acc.: 79.69%] [G loss: 5.236705]\n",
      "##############\n",
      "[0.85897203 0.83433469 0.79429198 0.80260668 0.78023338 0.82221338\n",
      " 0.87489624 0.84708584 0.82771558 0.79872687]\n",
      "##########\n",
      "epoch:23 step:18401 [D loss: 0.775079, acc.: 72.66%] [G loss: 6.944473]\n",
      "epoch:23 step:18402 [D loss: 1.367843, acc.: 63.28%] [G loss: 5.290744]\n",
      "epoch:23 step:18403 [D loss: 0.908680, acc.: 75.78%] [G loss: 3.512291]\n",
      "epoch:23 step:18404 [D loss: 0.500006, acc.: 80.47%] [G loss: 4.199267]\n",
      "epoch:23 step:18405 [D loss: 0.418407, acc.: 80.47%] [G loss: 3.687273]\n",
      "epoch:23 step:18406 [D loss: 0.451405, acc.: 78.12%] [G loss: 3.410894]\n",
      "epoch:23 step:18407 [D loss: 0.386355, acc.: 81.25%] [G loss: 4.195013]\n",
      "epoch:23 step:18408 [D loss: 0.349453, acc.: 89.84%] [G loss: 3.350518]\n",
      "epoch:23 step:18409 [D loss: 0.306213, acc.: 82.81%] [G loss: 3.050149]\n",
      "epoch:23 step:18410 [D loss: 0.280948, acc.: 86.72%] [G loss: 3.980278]\n",
      "epoch:23 step:18411 [D loss: 0.429835, acc.: 78.91%] [G loss: 2.644528]\n",
      "epoch:23 step:18412 [D loss: 0.338491, acc.: 88.28%] [G loss: 2.576548]\n",
      "epoch:23 step:18413 [D loss: 0.330637, acc.: 87.50%] [G loss: 3.242787]\n",
      "epoch:23 step:18414 [D loss: 0.313765, acc.: 85.94%] [G loss: 2.534680]\n",
      "epoch:23 step:18415 [D loss: 0.371844, acc.: 86.72%] [G loss: 2.812993]\n",
      "epoch:23 step:18416 [D loss: 0.336143, acc.: 83.59%] [G loss: 2.916517]\n",
      "epoch:23 step:18417 [D loss: 0.282262, acc.: 87.50%] [G loss: 2.647850]\n",
      "epoch:23 step:18418 [D loss: 0.333437, acc.: 86.72%] [G loss: 3.392130]\n",
      "epoch:23 step:18419 [D loss: 0.424002, acc.: 82.81%] [G loss: 3.129594]\n",
      "epoch:23 step:18420 [D loss: 0.283808, acc.: 89.84%] [G loss: 3.683807]\n",
      "epoch:23 step:18421 [D loss: 0.299842, acc.: 85.94%] [G loss: 3.726815]\n",
      "epoch:23 step:18422 [D loss: 0.260467, acc.: 87.50%] [G loss: 3.284906]\n",
      "epoch:23 step:18423 [D loss: 0.363307, acc.: 83.59%] [G loss: 2.633438]\n",
      "epoch:23 step:18424 [D loss: 0.329532, acc.: 83.59%] [G loss: 3.172047]\n",
      "epoch:23 step:18425 [D loss: 0.296074, acc.: 86.72%] [G loss: 3.393831]\n",
      "epoch:23 step:18426 [D loss: 0.286739, acc.: 88.28%] [G loss: 2.907726]\n",
      "epoch:23 step:18427 [D loss: 0.365417, acc.: 85.94%] [G loss: 2.490737]\n",
      "epoch:23 step:18428 [D loss: 0.319820, acc.: 85.94%] [G loss: 2.441345]\n",
      "epoch:23 step:18429 [D loss: 0.381553, acc.: 84.38%] [G loss: 3.168066]\n",
      "epoch:23 step:18430 [D loss: 0.447334, acc.: 75.78%] [G loss: 3.115375]\n",
      "epoch:23 step:18431 [D loss: 0.467914, acc.: 75.78%] [G loss: 2.328271]\n",
      "epoch:23 step:18432 [D loss: 0.304633, acc.: 87.50%] [G loss: 2.852223]\n",
      "epoch:23 step:18433 [D loss: 0.323285, acc.: 86.72%] [G loss: 3.774114]\n",
      "epoch:23 step:18434 [D loss: 0.270669, acc.: 89.84%] [G loss: 4.059827]\n",
      "epoch:23 step:18435 [D loss: 0.242945, acc.: 87.50%] [G loss: 4.140738]\n",
      "epoch:23 step:18436 [D loss: 0.324671, acc.: 84.38%] [G loss: 4.341645]\n",
      "epoch:23 step:18437 [D loss: 0.241099, acc.: 91.41%] [G loss: 3.880537]\n",
      "epoch:23 step:18438 [D loss: 0.250444, acc.: 88.28%] [G loss: 3.142582]\n",
      "epoch:23 step:18439 [D loss: 0.363945, acc.: 84.38%] [G loss: 3.192442]\n",
      "epoch:23 step:18440 [D loss: 0.327197, acc.: 85.16%] [G loss: 2.949727]\n",
      "epoch:23 step:18441 [D loss: 0.253344, acc.: 89.06%] [G loss: 3.296660]\n",
      "epoch:23 step:18442 [D loss: 0.333117, acc.: 80.47%] [G loss: 3.978471]\n",
      "epoch:23 step:18443 [D loss: 0.243349, acc.: 89.06%] [G loss: 4.075819]\n",
      "epoch:23 step:18444 [D loss: 0.278573, acc.: 86.72%] [G loss: 4.667870]\n",
      "epoch:23 step:18445 [D loss: 0.380490, acc.: 78.12%] [G loss: 3.903314]\n",
      "epoch:23 step:18446 [D loss: 0.283902, acc.: 87.50%] [G loss: 4.661466]\n",
      "epoch:23 step:18447 [D loss: 0.280535, acc.: 88.28%] [G loss: 3.296430]\n",
      "epoch:23 step:18448 [D loss: 0.233581, acc.: 91.41%] [G loss: 4.156060]\n",
      "epoch:23 step:18449 [D loss: 0.310825, acc.: 87.50%] [G loss: 3.590624]\n",
      "epoch:23 step:18450 [D loss: 0.264456, acc.: 87.50%] [G loss: 3.585766]\n",
      "epoch:23 step:18451 [D loss: 0.291724, acc.: 88.28%] [G loss: 4.190535]\n",
      "epoch:23 step:18452 [D loss: 0.493805, acc.: 78.12%] [G loss: 2.910553]\n",
      "epoch:23 step:18453 [D loss: 0.342996, acc.: 82.03%] [G loss: 3.292519]\n",
      "epoch:23 step:18454 [D loss: 0.259003, acc.: 87.50%] [G loss: 3.949372]\n",
      "epoch:23 step:18455 [D loss: 0.376967, acc.: 79.69%] [G loss: 3.440174]\n",
      "epoch:23 step:18456 [D loss: 0.285408, acc.: 89.06%] [G loss: 2.530575]\n",
      "epoch:23 step:18457 [D loss: 0.273573, acc.: 85.94%] [G loss: 4.871554]\n",
      "epoch:23 step:18458 [D loss: 0.221516, acc.: 92.19%] [G loss: 3.756178]\n",
      "epoch:23 step:18459 [D loss: 0.267582, acc.: 89.84%] [G loss: 3.180822]\n",
      "epoch:23 step:18460 [D loss: 0.267464, acc.: 88.28%] [G loss: 5.721066]\n",
      "epoch:23 step:18461 [D loss: 0.346440, acc.: 84.38%] [G loss: 3.117241]\n",
      "epoch:23 step:18462 [D loss: 0.345942, acc.: 83.59%] [G loss: 3.055796]\n",
      "epoch:23 step:18463 [D loss: 0.313591, acc.: 85.94%] [G loss: 3.265907]\n",
      "epoch:23 step:18464 [D loss: 0.397150, acc.: 83.59%] [G loss: 3.187896]\n",
      "epoch:23 step:18465 [D loss: 0.402832, acc.: 79.69%] [G loss: 2.501201]\n",
      "epoch:23 step:18466 [D loss: 0.305244, acc.: 87.50%] [G loss: 2.952746]\n",
      "epoch:23 step:18467 [D loss: 0.298588, acc.: 90.62%] [G loss: 2.860600]\n",
      "epoch:23 step:18468 [D loss: 0.314978, acc.: 85.94%] [G loss: 2.872567]\n",
      "epoch:23 step:18469 [D loss: 0.382939, acc.: 82.03%] [G loss: 3.893162]\n",
      "epoch:23 step:18470 [D loss: 0.278797, acc.: 88.28%] [G loss: 5.667147]\n",
      "epoch:23 step:18471 [D loss: 0.225047, acc.: 92.97%] [G loss: 4.219709]\n",
      "epoch:23 step:18472 [D loss: 0.281803, acc.: 86.72%] [G loss: 2.897426]\n",
      "epoch:23 step:18473 [D loss: 0.317086, acc.: 85.16%] [G loss: 4.154836]\n",
      "epoch:23 step:18474 [D loss: 0.300646, acc.: 88.28%] [G loss: 3.918292]\n",
      "epoch:23 step:18475 [D loss: 0.158741, acc.: 94.53%] [G loss: 4.968883]\n",
      "epoch:23 step:18476 [D loss: 0.390540, acc.: 84.38%] [G loss: 4.894125]\n",
      "epoch:23 step:18477 [D loss: 0.300459, acc.: 85.16%] [G loss: 3.524099]\n",
      "epoch:23 step:18478 [D loss: 0.494396, acc.: 73.44%] [G loss: 3.207402]\n",
      "epoch:23 step:18479 [D loss: 0.272515, acc.: 88.28%] [G loss: 3.105913]\n",
      "epoch:23 step:18480 [D loss: 0.335458, acc.: 89.06%] [G loss: 3.320390]\n",
      "epoch:23 step:18481 [D loss: 0.286114, acc.: 89.84%] [G loss: 3.562351]\n",
      "epoch:23 step:18482 [D loss: 0.269822, acc.: 87.50%] [G loss: 3.397418]\n",
      "epoch:23 step:18483 [D loss: 0.404780, acc.: 84.38%] [G loss: 4.718777]\n",
      "epoch:23 step:18484 [D loss: 0.221934, acc.: 89.06%] [G loss: 3.668137]\n",
      "epoch:23 step:18485 [D loss: 0.330970, acc.: 81.25%] [G loss: 5.309145]\n",
      "epoch:23 step:18486 [D loss: 0.250014, acc.: 89.84%] [G loss: 7.051495]\n",
      "epoch:23 step:18487 [D loss: 0.315188, acc.: 82.81%] [G loss: 4.090089]\n",
      "epoch:23 step:18488 [D loss: 0.333805, acc.: 83.59%] [G loss: 3.743640]\n",
      "epoch:23 step:18489 [D loss: 0.353383, acc.: 84.38%] [G loss: 3.838613]\n",
      "epoch:23 step:18490 [D loss: 0.229684, acc.: 89.84%] [G loss: 4.701968]\n",
      "epoch:23 step:18491 [D loss: 0.280411, acc.: 89.06%] [G loss: 4.916184]\n",
      "epoch:23 step:18492 [D loss: 0.299626, acc.: 85.94%] [G loss: 3.246614]\n",
      "epoch:23 step:18493 [D loss: 0.390181, acc.: 83.59%] [G loss: 5.438105]\n",
      "epoch:23 step:18494 [D loss: 0.264992, acc.: 86.72%] [G loss: 3.231606]\n",
      "epoch:23 step:18495 [D loss: 0.247603, acc.: 88.28%] [G loss: 3.395720]\n",
      "epoch:23 step:18496 [D loss: 0.289528, acc.: 89.06%] [G loss: 3.159467]\n",
      "epoch:23 step:18497 [D loss: 0.265723, acc.: 91.41%] [G loss: 2.919295]\n",
      "epoch:23 step:18498 [D loss: 0.311047, acc.: 85.94%] [G loss: 2.454013]\n",
      "epoch:23 step:18499 [D loss: 0.302221, acc.: 86.72%] [G loss: 3.087394]\n",
      "epoch:23 step:18500 [D loss: 0.457998, acc.: 75.00%] [G loss: 2.948627]\n",
      "epoch:23 step:18501 [D loss: 0.304910, acc.: 86.72%] [G loss: 4.338848]\n",
      "epoch:23 step:18502 [D loss: 0.191148, acc.: 90.62%] [G loss: 4.356013]\n",
      "epoch:23 step:18503 [D loss: 0.259580, acc.: 89.06%] [G loss: 3.662418]\n",
      "epoch:23 step:18504 [D loss: 0.222666, acc.: 92.97%] [G loss: 3.108126]\n",
      "epoch:23 step:18505 [D loss: 0.306036, acc.: 83.59%] [G loss: 4.782914]\n",
      "epoch:23 step:18506 [D loss: 0.290039, acc.: 87.50%] [G loss: 2.765181]\n",
      "epoch:23 step:18507 [D loss: 0.383518, acc.: 85.16%] [G loss: 3.552869]\n",
      "epoch:23 step:18508 [D loss: 0.349398, acc.: 85.94%] [G loss: 3.173969]\n",
      "epoch:23 step:18509 [D loss: 0.243522, acc.: 89.84%] [G loss: 4.215998]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:23 step:18510 [D loss: 0.392140, acc.: 78.91%] [G loss: 4.582352]\n",
      "epoch:23 step:18511 [D loss: 0.377908, acc.: 80.47%] [G loss: 4.188249]\n",
      "epoch:23 step:18512 [D loss: 0.346625, acc.: 84.38%] [G loss: 6.100491]\n",
      "epoch:23 step:18513 [D loss: 0.419484, acc.: 82.03%] [G loss: 5.588950]\n",
      "epoch:23 step:18514 [D loss: 0.252594, acc.: 91.41%] [G loss: 4.794643]\n",
      "epoch:23 step:18515 [D loss: 0.438845, acc.: 81.25%] [G loss: 5.171031]\n",
      "epoch:23 step:18516 [D loss: 0.483348, acc.: 78.12%] [G loss: 8.398886]\n",
      "epoch:23 step:18517 [D loss: 0.558496, acc.: 73.44%] [G loss: 3.312572]\n",
      "epoch:23 step:18518 [D loss: 0.264589, acc.: 86.72%] [G loss: 3.479969]\n",
      "epoch:23 step:18519 [D loss: 0.359503, acc.: 83.59%] [G loss: 3.053482]\n",
      "epoch:23 step:18520 [D loss: 0.305465, acc.: 89.06%] [G loss: 3.257502]\n",
      "epoch:23 step:18521 [D loss: 0.335774, acc.: 82.03%] [G loss: 5.576303]\n",
      "epoch:23 step:18522 [D loss: 0.277962, acc.: 89.06%] [G loss: 2.938351]\n",
      "epoch:23 step:18523 [D loss: 0.268944, acc.: 89.06%] [G loss: 4.921163]\n",
      "epoch:23 step:18524 [D loss: 0.335285, acc.: 80.47%] [G loss: 3.138874]\n",
      "epoch:23 step:18525 [D loss: 0.388448, acc.: 79.69%] [G loss: 3.181323]\n",
      "epoch:23 step:18526 [D loss: 0.222116, acc.: 93.75%] [G loss: 3.694878]\n",
      "epoch:23 step:18527 [D loss: 0.308221, acc.: 89.84%] [G loss: 4.181121]\n",
      "epoch:23 step:18528 [D loss: 0.313129, acc.: 86.72%] [G loss: 3.871146]\n",
      "epoch:23 step:18529 [D loss: 0.321037, acc.: 87.50%] [G loss: 3.047053]\n",
      "epoch:23 step:18530 [D loss: 0.229649, acc.: 91.41%] [G loss: 3.906343]\n",
      "epoch:23 step:18531 [D loss: 0.261437, acc.: 92.19%] [G loss: 3.226593]\n",
      "epoch:23 step:18532 [D loss: 0.226934, acc.: 92.19%] [G loss: 3.772165]\n",
      "epoch:23 step:18533 [D loss: 0.259736, acc.: 88.28%] [G loss: 4.144171]\n",
      "epoch:23 step:18534 [D loss: 0.319750, acc.: 80.47%] [G loss: 3.323675]\n",
      "epoch:23 step:18535 [D loss: 0.270238, acc.: 87.50%] [G loss: 3.056220]\n",
      "epoch:23 step:18536 [D loss: 0.299058, acc.: 87.50%] [G loss: 3.710909]\n",
      "epoch:23 step:18537 [D loss: 0.321019, acc.: 87.50%] [G loss: 4.366327]\n",
      "epoch:23 step:18538 [D loss: 0.298623, acc.: 89.06%] [G loss: 3.189228]\n",
      "epoch:23 step:18539 [D loss: 0.314400, acc.: 84.38%] [G loss: 3.486430]\n",
      "epoch:23 step:18540 [D loss: 0.311634, acc.: 86.72%] [G loss: 2.769723]\n",
      "epoch:23 step:18541 [D loss: 0.245602, acc.: 90.62%] [G loss: 3.394834]\n",
      "epoch:23 step:18542 [D loss: 0.289020, acc.: 90.62%] [G loss: 2.508965]\n",
      "epoch:23 step:18543 [D loss: 0.331860, acc.: 82.03%] [G loss: 3.555197]\n",
      "epoch:23 step:18544 [D loss: 0.354713, acc.: 83.59%] [G loss: 4.791930]\n",
      "epoch:23 step:18545 [D loss: 0.307639, acc.: 86.72%] [G loss: 4.164791]\n",
      "epoch:23 step:18546 [D loss: 0.325130, acc.: 82.03%] [G loss: 3.234850]\n",
      "epoch:23 step:18547 [D loss: 0.280293, acc.: 90.62%] [G loss: 3.893799]\n",
      "epoch:23 step:18548 [D loss: 0.254387, acc.: 89.06%] [G loss: 4.286789]\n",
      "epoch:23 step:18549 [D loss: 0.209211, acc.: 91.41%] [G loss: 3.636806]\n",
      "epoch:23 step:18550 [D loss: 0.278482, acc.: 85.94%] [G loss: 3.020529]\n",
      "epoch:23 step:18551 [D loss: 0.274969, acc.: 92.19%] [G loss: 3.268394]\n",
      "epoch:23 step:18552 [D loss: 0.368640, acc.: 82.81%] [G loss: 2.581337]\n",
      "epoch:23 step:18553 [D loss: 0.317787, acc.: 85.94%] [G loss: 3.248756]\n",
      "epoch:23 step:18554 [D loss: 0.293185, acc.: 89.84%] [G loss: 2.803148]\n",
      "epoch:23 step:18555 [D loss: 0.258259, acc.: 89.06%] [G loss: 3.566979]\n",
      "epoch:23 step:18556 [D loss: 0.314452, acc.: 85.94%] [G loss: 2.854163]\n",
      "epoch:23 step:18557 [D loss: 0.306868, acc.: 86.72%] [G loss: 3.419767]\n",
      "epoch:23 step:18558 [D loss: 0.329065, acc.: 84.38%] [G loss: 2.758262]\n",
      "epoch:23 step:18559 [D loss: 0.326245, acc.: 87.50%] [G loss: 2.318485]\n",
      "epoch:23 step:18560 [D loss: 0.383774, acc.: 82.81%] [G loss: 3.172278]\n",
      "epoch:23 step:18561 [D loss: 0.325827, acc.: 85.16%] [G loss: 2.814603]\n",
      "epoch:23 step:18562 [D loss: 0.394268, acc.: 85.94%] [G loss: 3.634122]\n",
      "epoch:23 step:18563 [D loss: 0.369893, acc.: 76.56%] [G loss: 4.007468]\n",
      "epoch:23 step:18564 [D loss: 0.314851, acc.: 84.38%] [G loss: 3.491937]\n",
      "epoch:23 step:18565 [D loss: 0.257140, acc.: 89.84%] [G loss: 3.536747]\n",
      "epoch:23 step:18566 [D loss: 0.298009, acc.: 87.50%] [G loss: 2.940195]\n",
      "epoch:23 step:18567 [D loss: 0.339589, acc.: 85.94%] [G loss: 3.168027]\n",
      "epoch:23 step:18568 [D loss: 0.298932, acc.: 86.72%] [G loss: 2.600158]\n",
      "epoch:23 step:18569 [D loss: 0.428988, acc.: 77.34%] [G loss: 2.594611]\n",
      "epoch:23 step:18570 [D loss: 0.343502, acc.: 85.94%] [G loss: 2.945154]\n",
      "epoch:23 step:18571 [D loss: 0.301713, acc.: 86.72%] [G loss: 2.797482]\n",
      "epoch:23 step:18572 [D loss: 0.347056, acc.: 82.03%] [G loss: 4.801635]\n",
      "epoch:23 step:18573 [D loss: 0.404179, acc.: 82.03%] [G loss: 4.606791]\n",
      "epoch:23 step:18574 [D loss: 0.403723, acc.: 86.72%] [G loss: 3.935215]\n",
      "epoch:23 step:18575 [D loss: 0.451638, acc.: 75.00%] [G loss: 3.599782]\n",
      "epoch:23 step:18576 [D loss: 0.285876, acc.: 89.84%] [G loss: 3.251598]\n",
      "epoch:23 step:18577 [D loss: 0.270994, acc.: 89.84%] [G loss: 3.109260]\n",
      "epoch:23 step:18578 [D loss: 0.361912, acc.: 84.38%] [G loss: 3.029845]\n",
      "epoch:23 step:18579 [D loss: 0.366429, acc.: 86.72%] [G loss: 3.205889]\n",
      "epoch:23 step:18580 [D loss: 0.280965, acc.: 87.50%] [G loss: 3.455653]\n",
      "epoch:23 step:18581 [D loss: 0.249493, acc.: 89.84%] [G loss: 4.604870]\n",
      "epoch:23 step:18582 [D loss: 0.341729, acc.: 82.03%] [G loss: 3.621480]\n",
      "epoch:23 step:18583 [D loss: 0.343649, acc.: 82.81%] [G loss: 3.367914]\n",
      "epoch:23 step:18584 [D loss: 0.235253, acc.: 92.19%] [G loss: 3.734597]\n",
      "epoch:23 step:18585 [D loss: 0.281523, acc.: 88.28%] [G loss: 3.436509]\n",
      "epoch:23 step:18586 [D loss: 0.422765, acc.: 79.69%] [G loss: 3.699659]\n",
      "epoch:23 step:18587 [D loss: 0.273521, acc.: 89.06%] [G loss: 3.414908]\n",
      "epoch:23 step:18588 [D loss: 0.309570, acc.: 84.38%] [G loss: 3.244682]\n",
      "epoch:23 step:18589 [D loss: 0.277602, acc.: 87.50%] [G loss: 3.030537]\n",
      "epoch:23 step:18590 [D loss: 0.301546, acc.: 85.94%] [G loss: 2.812767]\n",
      "epoch:23 step:18591 [D loss: 0.271553, acc.: 89.06%] [G loss: 3.888528]\n",
      "epoch:23 step:18592 [D loss: 0.171298, acc.: 93.75%] [G loss: 6.423041]\n",
      "epoch:23 step:18593 [D loss: 0.282093, acc.: 83.59%] [G loss: 3.236307]\n",
      "epoch:23 step:18594 [D loss: 0.266507, acc.: 89.06%] [G loss: 4.335631]\n",
      "epoch:23 step:18595 [D loss: 0.168099, acc.: 94.53%] [G loss: 6.805720]\n",
      "epoch:23 step:18596 [D loss: 0.301952, acc.: 83.59%] [G loss: 5.261995]\n",
      "epoch:23 step:18597 [D loss: 0.157245, acc.: 94.53%] [G loss: 7.671406]\n",
      "epoch:23 step:18598 [D loss: 0.218177, acc.: 93.75%] [G loss: 4.419103]\n",
      "epoch:23 step:18599 [D loss: 0.211332, acc.: 93.75%] [G loss: 4.727440]\n",
      "epoch:23 step:18600 [D loss: 0.377496, acc.: 83.59%] [G loss: 2.742129]\n",
      "##############\n",
      "[0.87712231 0.84152462 0.77938543 0.83300204 0.77346498 0.84305997\n",
      " 0.86914357 0.84294388 0.82177236 0.81952161]\n",
      "##########\n",
      "epoch:23 step:18601 [D loss: 0.234089, acc.: 90.62%] [G loss: 4.077456]\n",
      "epoch:23 step:18602 [D loss: 0.352558, acc.: 85.94%] [G loss: 2.735535]\n",
      "epoch:23 step:18603 [D loss: 0.206639, acc.: 95.31%] [G loss: 3.612641]\n",
      "epoch:23 step:18604 [D loss: 0.268801, acc.: 89.84%] [G loss: 2.570688]\n",
      "epoch:23 step:18605 [D loss: 0.344359, acc.: 85.16%] [G loss: 2.948525]\n",
      "epoch:23 step:18606 [D loss: 0.322961, acc.: 84.38%] [G loss: 2.716907]\n",
      "epoch:23 step:18607 [D loss: 0.266278, acc.: 88.28%] [G loss: 3.308270]\n",
      "epoch:23 step:18608 [D loss: 0.333916, acc.: 85.94%] [G loss: 3.585090]\n",
      "epoch:23 step:18609 [D loss: 0.335173, acc.: 81.25%] [G loss: 4.499130]\n",
      "epoch:23 step:18610 [D loss: 0.242076, acc.: 89.06%] [G loss: 5.897701]\n",
      "epoch:23 step:18611 [D loss: 0.295103, acc.: 89.06%] [G loss: 4.077350]\n",
      "epoch:23 step:18612 [D loss: 0.267071, acc.: 85.94%] [G loss: 2.785569]\n",
      "epoch:23 step:18613 [D loss: 0.244350, acc.: 89.06%] [G loss: 3.785398]\n",
      "epoch:23 step:18614 [D loss: 0.311404, acc.: 86.72%] [G loss: 4.838770]\n",
      "epoch:23 step:18615 [D loss: 0.318412, acc.: 85.94%] [G loss: 4.268536]\n",
      "epoch:23 step:18616 [D loss: 0.195051, acc.: 92.97%] [G loss: 4.762769]\n",
      "epoch:23 step:18617 [D loss: 0.356382, acc.: 81.25%] [G loss: 4.822837]\n",
      "epoch:23 step:18618 [D loss: 0.280657, acc.: 85.16%] [G loss: 3.169191]\n",
      "epoch:23 step:18619 [D loss: 0.213546, acc.: 90.62%] [G loss: 4.440834]\n",
      "epoch:23 step:18620 [D loss: 0.315908, acc.: 88.28%] [G loss: 5.006395]\n",
      "epoch:23 step:18621 [D loss: 0.347101, acc.: 84.38%] [G loss: 2.934801]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:23 step:18622 [D loss: 0.284719, acc.: 89.84%] [G loss: 4.324328]\n",
      "epoch:23 step:18623 [D loss: 0.348823, acc.: 84.38%] [G loss: 5.060106]\n",
      "epoch:23 step:18624 [D loss: 0.304470, acc.: 85.94%] [G loss: 3.126164]\n",
      "epoch:23 step:18625 [D loss: 0.313268, acc.: 86.72%] [G loss: 4.834348]\n",
      "epoch:23 step:18626 [D loss: 0.372889, acc.: 82.81%] [G loss: 3.800066]\n",
      "epoch:23 step:18627 [D loss: 0.278117, acc.: 85.94%] [G loss: 6.228058]\n",
      "epoch:23 step:18628 [D loss: 0.306000, acc.: 85.94%] [G loss: 3.767993]\n",
      "epoch:23 step:18629 [D loss: 0.318214, acc.: 85.16%] [G loss: 3.626784]\n",
      "epoch:23 step:18630 [D loss: 0.263630, acc.: 89.06%] [G loss: 3.231209]\n",
      "epoch:23 step:18631 [D loss: 0.313267, acc.: 86.72%] [G loss: 3.654295]\n",
      "epoch:23 step:18632 [D loss: 0.364439, acc.: 82.81%] [G loss: 4.794386]\n",
      "epoch:23 step:18633 [D loss: 0.420076, acc.: 80.47%] [G loss: 6.478687]\n",
      "epoch:23 step:18634 [D loss: 0.575145, acc.: 77.34%] [G loss: 3.782002]\n",
      "epoch:23 step:18635 [D loss: 0.508747, acc.: 77.34%] [G loss: 3.265908]\n",
      "epoch:23 step:18636 [D loss: 0.384794, acc.: 85.16%] [G loss: 3.923895]\n",
      "epoch:23 step:18637 [D loss: 0.268554, acc.: 87.50%] [G loss: 4.131542]\n",
      "epoch:23 step:18638 [D loss: 0.242432, acc.: 85.16%] [G loss: 6.230433]\n",
      "epoch:23 step:18639 [D loss: 0.228612, acc.: 90.62%] [G loss: 5.333245]\n",
      "epoch:23 step:18640 [D loss: 0.290153, acc.: 85.16%] [G loss: 3.332355]\n",
      "epoch:23 step:18641 [D loss: 0.243235, acc.: 88.28%] [G loss: 4.155896]\n",
      "epoch:23 step:18642 [D loss: 0.269996, acc.: 86.72%] [G loss: 3.220843]\n",
      "epoch:23 step:18643 [D loss: 0.358142, acc.: 85.94%] [G loss: 3.957873]\n",
      "epoch:23 step:18644 [D loss: 0.223936, acc.: 91.41%] [G loss: 3.660336]\n",
      "epoch:23 step:18645 [D loss: 0.300404, acc.: 86.72%] [G loss: 4.385698]\n",
      "epoch:23 step:18646 [D loss: 0.334187, acc.: 87.50%] [G loss: 3.551428]\n",
      "epoch:23 step:18647 [D loss: 0.297197, acc.: 85.16%] [G loss: 4.462747]\n",
      "epoch:23 step:18648 [D loss: 0.326974, acc.: 84.38%] [G loss: 3.086896]\n",
      "epoch:23 step:18649 [D loss: 0.298809, acc.: 88.28%] [G loss: 3.135693]\n",
      "epoch:23 step:18650 [D loss: 0.262742, acc.: 86.72%] [G loss: 4.487587]\n",
      "epoch:23 step:18651 [D loss: 0.268484, acc.: 86.72%] [G loss: 4.566880]\n",
      "epoch:23 step:18652 [D loss: 0.264753, acc.: 86.72%] [G loss: 3.404246]\n",
      "epoch:23 step:18653 [D loss: 0.338216, acc.: 84.38%] [G loss: 3.268263]\n",
      "epoch:23 step:18654 [D loss: 0.152496, acc.: 96.09%] [G loss: 3.526369]\n",
      "epoch:23 step:18655 [D loss: 0.310270, acc.: 83.59%] [G loss: 2.756024]\n",
      "epoch:23 step:18656 [D loss: 0.272669, acc.: 90.62%] [G loss: 2.872308]\n",
      "epoch:23 step:18657 [D loss: 0.396401, acc.: 82.03%] [G loss: 2.854418]\n",
      "epoch:23 step:18658 [D loss: 0.389302, acc.: 86.72%] [G loss: 4.687265]\n",
      "epoch:23 step:18659 [D loss: 0.419112, acc.: 78.12%] [G loss: 7.215879]\n",
      "epoch:23 step:18660 [D loss: 0.671125, acc.: 71.88%] [G loss: 8.284588]\n",
      "epoch:23 step:18661 [D loss: 1.063815, acc.: 71.88%] [G loss: 3.340651]\n",
      "epoch:23 step:18662 [D loss: 0.382462, acc.: 80.47%] [G loss: 3.040990]\n",
      "epoch:23 step:18663 [D loss: 0.252037, acc.: 88.28%] [G loss: 4.805689]\n",
      "epoch:23 step:18664 [D loss: 0.260010, acc.: 85.94%] [G loss: 3.962223]\n",
      "epoch:23 step:18665 [D loss: 0.373468, acc.: 83.59%] [G loss: 3.925023]\n",
      "epoch:23 step:18666 [D loss: 0.432360, acc.: 82.81%] [G loss: 2.981148]\n",
      "epoch:23 step:18667 [D loss: 0.387084, acc.: 85.16%] [G loss: 3.679779]\n",
      "epoch:23 step:18668 [D loss: 0.333685, acc.: 83.59%] [G loss: 3.635988]\n",
      "epoch:23 step:18669 [D loss: 0.461671, acc.: 83.59%] [G loss: 2.598361]\n",
      "epoch:23 step:18670 [D loss: 0.291977, acc.: 88.28%] [G loss: 3.666331]\n",
      "epoch:23 step:18671 [D loss: 0.326486, acc.: 85.94%] [G loss: 3.293994]\n",
      "epoch:23 step:18672 [D loss: 0.402171, acc.: 82.81%] [G loss: 3.518715]\n",
      "epoch:23 step:18673 [D loss: 0.466194, acc.: 78.12%] [G loss: 2.268721]\n",
      "epoch:23 step:18674 [D loss: 0.427846, acc.: 79.69%] [G loss: 2.881223]\n",
      "epoch:23 step:18675 [D loss: 0.352547, acc.: 84.38%] [G loss: 2.986551]\n",
      "epoch:23 step:18676 [D loss: 0.460218, acc.: 77.34%] [G loss: 2.528960]\n",
      "epoch:23 step:18677 [D loss: 0.334154, acc.: 88.28%] [G loss: 2.724647]\n",
      "epoch:23 step:18678 [D loss: 0.292147, acc.: 88.28%] [G loss: 2.889161]\n",
      "epoch:23 step:18679 [D loss: 0.374969, acc.: 84.38%] [G loss: 2.802575]\n",
      "epoch:23 step:18680 [D loss: 0.283059, acc.: 85.16%] [G loss: 2.741549]\n",
      "epoch:23 step:18681 [D loss: 0.335718, acc.: 87.50%] [G loss: 3.025380]\n",
      "epoch:23 step:18682 [D loss: 0.306878, acc.: 85.94%] [G loss: 2.978576]\n",
      "epoch:23 step:18683 [D loss: 0.269151, acc.: 89.84%] [G loss: 4.674067]\n",
      "epoch:23 step:18684 [D loss: 0.376081, acc.: 82.81%] [G loss: 3.742193]\n",
      "epoch:23 step:18685 [D loss: 0.291929, acc.: 88.28%] [G loss: 2.969655]\n",
      "epoch:23 step:18686 [D loss: 0.302371, acc.: 89.06%] [G loss: 3.913605]\n",
      "epoch:23 step:18687 [D loss: 0.216751, acc.: 89.84%] [G loss: 3.505779]\n",
      "epoch:23 step:18688 [D loss: 0.287897, acc.: 91.41%] [G loss: 2.750430]\n",
      "epoch:23 step:18689 [D loss: 0.363440, acc.: 85.16%] [G loss: 3.642478]\n",
      "epoch:23 step:18690 [D loss: 0.370803, acc.: 84.38%] [G loss: 2.985719]\n",
      "epoch:23 step:18691 [D loss: 0.345444, acc.: 85.16%] [G loss: 2.855108]\n",
      "epoch:23 step:18692 [D loss: 0.282775, acc.: 85.16%] [G loss: 4.153954]\n",
      "epoch:23 step:18693 [D loss: 0.210459, acc.: 90.62%] [G loss: 6.042715]\n",
      "epoch:23 step:18694 [D loss: 0.341063, acc.: 82.81%] [G loss: 2.847482]\n",
      "epoch:23 step:18695 [D loss: 0.263862, acc.: 86.72%] [G loss: 3.202252]\n",
      "epoch:23 step:18696 [D loss: 0.320275, acc.: 84.38%] [G loss: 3.464649]\n",
      "epoch:23 step:18697 [D loss: 0.388679, acc.: 82.03%] [G loss: 4.578680]\n",
      "epoch:23 step:18698 [D loss: 0.422544, acc.: 79.69%] [G loss: 4.391599]\n",
      "epoch:23 step:18699 [D loss: 0.459726, acc.: 81.25%] [G loss: 2.380604]\n",
      "epoch:23 step:18700 [D loss: 0.348998, acc.: 83.59%] [G loss: 2.538022]\n",
      "epoch:23 step:18701 [D loss: 0.442123, acc.: 79.69%] [G loss: 2.373819]\n",
      "epoch:23 step:18702 [D loss: 0.380938, acc.: 82.81%] [G loss: 2.997577]\n",
      "epoch:23 step:18703 [D loss: 0.382318, acc.: 86.72%] [G loss: 2.619163]\n",
      "epoch:23 step:18704 [D loss: 0.370804, acc.: 84.38%] [G loss: 4.187008]\n",
      "epoch:23 step:18705 [D loss: 0.267865, acc.: 89.06%] [G loss: 3.757214]\n",
      "epoch:23 step:18706 [D loss: 0.218942, acc.: 90.62%] [G loss: 4.009542]\n",
      "epoch:23 step:18707 [D loss: 0.317436, acc.: 85.16%] [G loss: 4.193723]\n",
      "epoch:23 step:18708 [D loss: 0.230127, acc.: 92.19%] [G loss: 2.956832]\n",
      "epoch:23 step:18709 [D loss: 0.242962, acc.: 89.06%] [G loss: 3.568162]\n",
      "epoch:23 step:18710 [D loss: 0.216100, acc.: 92.19%] [G loss: 3.232409]\n",
      "epoch:23 step:18711 [D loss: 0.258741, acc.: 90.62%] [G loss: 2.895275]\n",
      "epoch:23 step:18712 [D loss: 0.292747, acc.: 85.16%] [G loss: 2.627250]\n",
      "epoch:23 step:18713 [D loss: 0.317920, acc.: 85.94%] [G loss: 2.869580]\n",
      "epoch:23 step:18714 [D loss: 0.370476, acc.: 84.38%] [G loss: 2.906897]\n",
      "epoch:23 step:18715 [D loss: 0.302577, acc.: 89.06%] [G loss: 2.559371]\n",
      "epoch:23 step:18716 [D loss: 0.324984, acc.: 83.59%] [G loss: 3.134620]\n",
      "epoch:23 step:18717 [D loss: 0.388848, acc.: 82.81%] [G loss: 2.281066]\n",
      "epoch:23 step:18718 [D loss: 0.284365, acc.: 86.72%] [G loss: 2.400123]\n",
      "epoch:23 step:18719 [D loss: 0.265531, acc.: 90.62%] [G loss: 2.172854]\n",
      "epoch:23 step:18720 [D loss: 0.282815, acc.: 87.50%] [G loss: 2.623052]\n",
      "epoch:23 step:18721 [D loss: 0.325382, acc.: 82.81%] [G loss: 3.871282]\n",
      "epoch:23 step:18722 [D loss: 0.302098, acc.: 85.94%] [G loss: 3.601874]\n",
      "epoch:23 step:18723 [D loss: 0.287964, acc.: 89.84%] [G loss: 3.329966]\n",
      "epoch:23 step:18724 [D loss: 0.301252, acc.: 87.50%] [G loss: 2.898893]\n",
      "epoch:23 step:18725 [D loss: 0.341603, acc.: 83.59%] [G loss: 3.402890]\n",
      "epoch:23 step:18726 [D loss: 0.347915, acc.: 85.16%] [G loss: 3.080030]\n",
      "epoch:23 step:18727 [D loss: 0.356670, acc.: 82.81%] [G loss: 3.186061]\n",
      "epoch:23 step:18728 [D loss: 0.357304, acc.: 82.81%] [G loss: 3.854543]\n",
      "epoch:23 step:18729 [D loss: 0.356523, acc.: 82.03%] [G loss: 2.548759]\n",
      "epoch:23 step:18730 [D loss: 0.279269, acc.: 84.38%] [G loss: 2.619610]\n",
      "epoch:23 step:18731 [D loss: 0.336834, acc.: 83.59%] [G loss: 3.035092]\n",
      "epoch:23 step:18732 [D loss: 0.257133, acc.: 89.06%] [G loss: 3.149296]\n",
      "epoch:23 step:18733 [D loss: 0.476568, acc.: 78.12%] [G loss: 2.969278]\n",
      "epoch:23 step:18734 [D loss: 0.237532, acc.: 89.06%] [G loss: 3.137307]\n",
      "epoch:23 step:18735 [D loss: 0.284725, acc.: 88.28%] [G loss: 3.797915]\n",
      "epoch:23 step:18736 [D loss: 0.446562, acc.: 79.69%] [G loss: 3.825158]\n",
      "epoch:23 step:18737 [D loss: 0.405500, acc.: 85.16%] [G loss: 2.485937]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:23 step:18738 [D loss: 0.499277, acc.: 76.56%] [G loss: 4.665524]\n",
      "epoch:23 step:18739 [D loss: 0.256402, acc.: 89.06%] [G loss: 3.449173]\n",
      "epoch:23 step:18740 [D loss: 0.321832, acc.: 86.72%] [G loss: 3.912659]\n",
      "epoch:23 step:18741 [D loss: 0.243622, acc.: 90.62%] [G loss: 2.930489]\n",
      "epoch:23 step:18742 [D loss: 0.239035, acc.: 87.50%] [G loss: 3.705956]\n",
      "epoch:23 step:18743 [D loss: 0.213568, acc.: 90.62%] [G loss: 3.098099]\n",
      "epoch:23 step:18744 [D loss: 0.387877, acc.: 84.38%] [G loss: 3.162669]\n",
      "epoch:24 step:18745 [D loss: 0.371093, acc.: 82.81%] [G loss: 3.856674]\n",
      "epoch:24 step:18746 [D loss: 0.262912, acc.: 92.19%] [G loss: 4.036408]\n",
      "epoch:24 step:18747 [D loss: 0.419294, acc.: 84.38%] [G loss: 5.227965]\n",
      "epoch:24 step:18748 [D loss: 0.348314, acc.: 82.81%] [G loss: 4.956415]\n",
      "epoch:24 step:18749 [D loss: 0.222590, acc.: 89.84%] [G loss: 2.771219]\n",
      "epoch:24 step:18750 [D loss: 0.409193, acc.: 84.38%] [G loss: 6.634274]\n",
      "epoch:24 step:18751 [D loss: 0.269155, acc.: 88.28%] [G loss: 4.042709]\n",
      "epoch:24 step:18752 [D loss: 0.213130, acc.: 93.75%] [G loss: 6.350212]\n",
      "epoch:24 step:18753 [D loss: 0.359544, acc.: 85.94%] [G loss: 4.241035]\n",
      "epoch:24 step:18754 [D loss: 0.231878, acc.: 90.62%] [G loss: 7.557095]\n",
      "epoch:24 step:18755 [D loss: 0.242521, acc.: 90.62%] [G loss: 7.058236]\n",
      "epoch:24 step:18756 [D loss: 0.282042, acc.: 87.50%] [G loss: 4.717693]\n",
      "epoch:24 step:18757 [D loss: 0.295682, acc.: 87.50%] [G loss: 5.168136]\n",
      "epoch:24 step:18758 [D loss: 0.249073, acc.: 89.06%] [G loss: 3.076240]\n",
      "epoch:24 step:18759 [D loss: 0.326690, acc.: 86.72%] [G loss: 3.029803]\n",
      "epoch:24 step:18760 [D loss: 0.470859, acc.: 77.34%] [G loss: 2.807636]\n",
      "epoch:24 step:18761 [D loss: 0.281247, acc.: 82.03%] [G loss: 3.227503]\n",
      "epoch:24 step:18762 [D loss: 0.406454, acc.: 82.81%] [G loss: 3.282513]\n",
      "epoch:24 step:18763 [D loss: 0.387006, acc.: 86.72%] [G loss: 3.075449]\n",
      "epoch:24 step:18764 [D loss: 0.275520, acc.: 88.28%] [G loss: 3.662446]\n",
      "epoch:24 step:18765 [D loss: 0.338828, acc.: 81.25%] [G loss: 2.576513]\n",
      "epoch:24 step:18766 [D loss: 0.331880, acc.: 85.94%] [G loss: 3.408979]\n",
      "epoch:24 step:18767 [D loss: 0.378764, acc.: 81.25%] [G loss: 3.495005]\n",
      "epoch:24 step:18768 [D loss: 0.295633, acc.: 87.50%] [G loss: 2.745453]\n",
      "epoch:24 step:18769 [D loss: 0.308284, acc.: 88.28%] [G loss: 3.227141]\n",
      "epoch:24 step:18770 [D loss: 0.265476, acc.: 92.19%] [G loss: 3.811499]\n",
      "epoch:24 step:18771 [D loss: 0.352797, acc.: 83.59%] [G loss: 3.706215]\n",
      "epoch:24 step:18772 [D loss: 0.247171, acc.: 89.84%] [G loss: 4.089648]\n",
      "epoch:24 step:18773 [D loss: 0.278206, acc.: 90.62%] [G loss: 3.543115]\n",
      "epoch:24 step:18774 [D loss: 0.275447, acc.: 86.72%] [G loss: 3.541286]\n",
      "epoch:24 step:18775 [D loss: 0.286612, acc.: 87.50%] [G loss: 4.319781]\n",
      "epoch:24 step:18776 [D loss: 0.359504, acc.: 82.03%] [G loss: 4.521898]\n",
      "epoch:24 step:18777 [D loss: 0.269236, acc.: 91.41%] [G loss: 3.052923]\n",
      "epoch:24 step:18778 [D loss: 0.237704, acc.: 87.50%] [G loss: 5.368720]\n",
      "epoch:24 step:18779 [D loss: 0.314366, acc.: 89.84%] [G loss: 5.358068]\n",
      "epoch:24 step:18780 [D loss: 0.306164, acc.: 85.94%] [G loss: 6.317498]\n",
      "epoch:24 step:18781 [D loss: 0.261623, acc.: 86.72%] [G loss: 5.466283]\n",
      "epoch:24 step:18782 [D loss: 0.272814, acc.: 88.28%] [G loss: 3.498085]\n",
      "epoch:24 step:18783 [D loss: 0.227857, acc.: 89.84%] [G loss: 4.648109]\n",
      "epoch:24 step:18784 [D loss: 0.280223, acc.: 87.50%] [G loss: 5.653643]\n",
      "epoch:24 step:18785 [D loss: 0.282668, acc.: 89.84%] [G loss: 4.867481]\n",
      "epoch:24 step:18786 [D loss: 0.290040, acc.: 86.72%] [G loss: 4.206406]\n",
      "epoch:24 step:18787 [D loss: 0.332457, acc.: 86.72%] [G loss: 3.845025]\n",
      "epoch:24 step:18788 [D loss: 0.318671, acc.: 86.72%] [G loss: 3.729369]\n",
      "epoch:24 step:18789 [D loss: 0.262108, acc.: 89.06%] [G loss: 3.987720]\n",
      "epoch:24 step:18790 [D loss: 0.353090, acc.: 82.81%] [G loss: 3.521811]\n",
      "epoch:24 step:18791 [D loss: 0.556083, acc.: 76.56%] [G loss: 4.827984]\n",
      "epoch:24 step:18792 [D loss: 0.364436, acc.: 84.38%] [G loss: 5.040305]\n",
      "epoch:24 step:18793 [D loss: 0.193558, acc.: 90.62%] [G loss: 4.027502]\n",
      "epoch:24 step:18794 [D loss: 0.378511, acc.: 84.38%] [G loss: 5.805104]\n",
      "epoch:24 step:18795 [D loss: 0.371829, acc.: 82.81%] [G loss: 4.472261]\n",
      "epoch:24 step:18796 [D loss: 0.207863, acc.: 91.41%] [G loss: 6.869628]\n",
      "epoch:24 step:18797 [D loss: 0.321240, acc.: 83.59%] [G loss: 4.221632]\n",
      "epoch:24 step:18798 [D loss: 0.254817, acc.: 89.84%] [G loss: 5.138795]\n",
      "epoch:24 step:18799 [D loss: 0.302754, acc.: 86.72%] [G loss: 3.361847]\n",
      "epoch:24 step:18800 [D loss: 0.358078, acc.: 85.16%] [G loss: 4.887586]\n",
      "##############\n",
      "[0.87075377 0.85418557 0.79872623 0.80964212 0.78390002 0.80376701\n",
      " 0.88982913 0.82460176 0.81027847 0.82180511]\n",
      "##########\n",
      "epoch:24 step:18801 [D loss: 0.293217, acc.: 86.72%] [G loss: 4.257057]\n",
      "epoch:24 step:18802 [D loss: 0.308984, acc.: 83.59%] [G loss: 3.809050]\n",
      "epoch:24 step:18803 [D loss: 0.227219, acc.: 92.97%] [G loss: 3.204546]\n",
      "epoch:24 step:18804 [D loss: 0.348761, acc.: 82.81%] [G loss: 3.592734]\n",
      "epoch:24 step:18805 [D loss: 0.310219, acc.: 85.94%] [G loss: 2.348516]\n",
      "epoch:24 step:18806 [D loss: 0.259868, acc.: 89.84%] [G loss: 2.875469]\n",
      "epoch:24 step:18807 [D loss: 0.266914, acc.: 89.84%] [G loss: 2.978604]\n",
      "epoch:24 step:18808 [D loss: 0.353914, acc.: 83.59%] [G loss: 3.116484]\n",
      "epoch:24 step:18809 [D loss: 0.383086, acc.: 82.03%] [G loss: 3.086147]\n",
      "epoch:24 step:18810 [D loss: 0.302211, acc.: 89.06%] [G loss: 3.534665]\n",
      "epoch:24 step:18811 [D loss: 0.360969, acc.: 84.38%] [G loss: 2.870812]\n",
      "epoch:24 step:18812 [D loss: 0.306967, acc.: 85.16%] [G loss: 3.611424]\n",
      "epoch:24 step:18813 [D loss: 0.354982, acc.: 86.72%] [G loss: 2.252458]\n",
      "epoch:24 step:18814 [D loss: 0.457277, acc.: 81.25%] [G loss: 3.136304]\n",
      "epoch:24 step:18815 [D loss: 0.294970, acc.: 85.16%] [G loss: 3.082865]\n",
      "epoch:24 step:18816 [D loss: 0.419844, acc.: 85.94%] [G loss: 3.552600]\n",
      "epoch:24 step:18817 [D loss: 0.406416, acc.: 80.47%] [G loss: 3.636126]\n",
      "epoch:24 step:18818 [D loss: 0.353956, acc.: 83.59%] [G loss: 3.294399]\n",
      "epoch:24 step:18819 [D loss: 0.269252, acc.: 86.72%] [G loss: 3.990627]\n",
      "epoch:24 step:18820 [D loss: 0.322043, acc.: 84.38%] [G loss: 3.046588]\n",
      "epoch:24 step:18821 [D loss: 0.423119, acc.: 82.03%] [G loss: 3.167192]\n",
      "epoch:24 step:18822 [D loss: 0.315702, acc.: 85.94%] [G loss: 3.201845]\n",
      "epoch:24 step:18823 [D loss: 0.286912, acc.: 87.50%] [G loss: 2.552591]\n",
      "epoch:24 step:18824 [D loss: 0.355040, acc.: 86.72%] [G loss: 2.992442]\n",
      "epoch:24 step:18825 [D loss: 0.381624, acc.: 81.25%] [G loss: 4.389870]\n",
      "epoch:24 step:18826 [D loss: 0.330067, acc.: 83.59%] [G loss: 3.936876]\n",
      "epoch:24 step:18827 [D loss: 0.261357, acc.: 90.62%] [G loss: 4.141446]\n",
      "epoch:24 step:18828 [D loss: 0.257058, acc.: 90.62%] [G loss: 5.249477]\n",
      "epoch:24 step:18829 [D loss: 0.332867, acc.: 82.03%] [G loss: 3.604524]\n",
      "epoch:24 step:18830 [D loss: 0.312151, acc.: 87.50%] [G loss: 3.470034]\n",
      "epoch:24 step:18831 [D loss: 0.272688, acc.: 88.28%] [G loss: 4.286110]\n",
      "epoch:24 step:18832 [D loss: 0.285981, acc.: 85.94%] [G loss: 3.631522]\n",
      "epoch:24 step:18833 [D loss: 0.217277, acc.: 91.41%] [G loss: 3.502937]\n",
      "epoch:24 step:18834 [D loss: 0.177070, acc.: 90.62%] [G loss: 4.463076]\n",
      "epoch:24 step:18835 [D loss: 0.385314, acc.: 82.81%] [G loss: 5.075027]\n",
      "epoch:24 step:18836 [D loss: 0.217389, acc.: 89.84%] [G loss: 4.800061]\n",
      "epoch:24 step:18837 [D loss: 0.265044, acc.: 87.50%] [G loss: 3.980643]\n",
      "epoch:24 step:18838 [D loss: 0.285119, acc.: 86.72%] [G loss: 2.729457]\n",
      "epoch:24 step:18839 [D loss: 0.300501, acc.: 84.38%] [G loss: 3.880129]\n",
      "epoch:24 step:18840 [D loss: 0.329143, acc.: 85.94%] [G loss: 6.490317]\n",
      "epoch:24 step:18841 [D loss: 0.441529, acc.: 78.12%] [G loss: 2.198432]\n",
      "epoch:24 step:18842 [D loss: 0.215466, acc.: 88.28%] [G loss: 5.293131]\n",
      "epoch:24 step:18843 [D loss: 0.299555, acc.: 86.72%] [G loss: 4.201373]\n",
      "epoch:24 step:18844 [D loss: 0.324511, acc.: 82.03%] [G loss: 3.364116]\n",
      "epoch:24 step:18845 [D loss: 0.261434, acc.: 87.50%] [G loss: 4.359304]\n",
      "epoch:24 step:18846 [D loss: 0.271134, acc.: 91.41%] [G loss: 3.206344]\n",
      "epoch:24 step:18847 [D loss: 0.293814, acc.: 88.28%] [G loss: 3.326317]\n",
      "epoch:24 step:18848 [D loss: 0.248965, acc.: 85.94%] [G loss: 4.244348]\n",
      "epoch:24 step:18849 [D loss: 0.317663, acc.: 88.28%] [G loss: 3.438443]\n",
      "epoch:24 step:18850 [D loss: 0.335220, acc.: 85.16%] [G loss: 3.537491]\n",
      "epoch:24 step:18851 [D loss: 0.403977, acc.: 78.91%] [G loss: 3.395737]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:24 step:18852 [D loss: 0.253672, acc.: 92.19%] [G loss: 3.297373]\n",
      "epoch:24 step:18853 [D loss: 0.342367, acc.: 84.38%] [G loss: 3.911255]\n",
      "epoch:24 step:18854 [D loss: 0.249525, acc.: 88.28%] [G loss: 3.478430]\n",
      "epoch:24 step:18855 [D loss: 0.479693, acc.: 78.91%] [G loss: 3.704791]\n",
      "epoch:24 step:18856 [D loss: 0.352840, acc.: 85.16%] [G loss: 2.730389]\n",
      "epoch:24 step:18857 [D loss: 0.340554, acc.: 83.59%] [G loss: 3.972042]\n",
      "epoch:24 step:18858 [D loss: 0.362570, acc.: 82.03%] [G loss: 4.013001]\n",
      "epoch:24 step:18859 [D loss: 0.367410, acc.: 82.03%] [G loss: 2.967317]\n",
      "epoch:24 step:18860 [D loss: 0.310464, acc.: 89.06%] [G loss: 4.299258]\n",
      "epoch:24 step:18861 [D loss: 0.296585, acc.: 86.72%] [G loss: 3.954436]\n",
      "epoch:24 step:18862 [D loss: 0.449731, acc.: 83.59%] [G loss: 3.936835]\n",
      "epoch:24 step:18863 [D loss: 0.396224, acc.: 81.25%] [G loss: 4.778839]\n",
      "epoch:24 step:18864 [D loss: 0.500203, acc.: 76.56%] [G loss: 3.777774]\n",
      "epoch:24 step:18865 [D loss: 0.301463, acc.: 82.03%] [G loss: 3.627399]\n",
      "epoch:24 step:18866 [D loss: 0.313718, acc.: 87.50%] [G loss: 5.460404]\n",
      "epoch:24 step:18867 [D loss: 0.294490, acc.: 85.94%] [G loss: 3.038181]\n",
      "epoch:24 step:18868 [D loss: 0.229690, acc.: 90.62%] [G loss: 2.728363]\n",
      "epoch:24 step:18869 [D loss: 0.362855, acc.: 85.16%] [G loss: 2.403029]\n",
      "epoch:24 step:18870 [D loss: 0.230271, acc.: 92.97%] [G loss: 3.670381]\n",
      "epoch:24 step:18871 [D loss: 0.302898, acc.: 88.28%] [G loss: 3.422658]\n",
      "epoch:24 step:18872 [D loss: 0.275935, acc.: 87.50%] [G loss: 3.497470]\n",
      "epoch:24 step:18873 [D loss: 0.254420, acc.: 89.06%] [G loss: 4.147170]\n",
      "epoch:24 step:18874 [D loss: 0.231661, acc.: 89.06%] [G loss: 3.445056]\n",
      "epoch:24 step:18875 [D loss: 0.213304, acc.: 91.41%] [G loss: 6.377341]\n",
      "epoch:24 step:18876 [D loss: 0.274074, acc.: 89.84%] [G loss: 5.888278]\n",
      "epoch:24 step:18877 [D loss: 0.293301, acc.: 86.72%] [G loss: 4.214463]\n",
      "epoch:24 step:18878 [D loss: 0.185546, acc.: 92.97%] [G loss: 5.720975]\n",
      "epoch:24 step:18879 [D loss: 0.219176, acc.: 91.41%] [G loss: 4.218529]\n",
      "epoch:24 step:18880 [D loss: 0.283960, acc.: 86.72%] [G loss: 4.207876]\n",
      "epoch:24 step:18881 [D loss: 0.227928, acc.: 89.84%] [G loss: 4.543108]\n",
      "epoch:24 step:18882 [D loss: 0.304408, acc.: 88.28%] [G loss: 3.548945]\n",
      "epoch:24 step:18883 [D loss: 0.211471, acc.: 91.41%] [G loss: 3.098270]\n",
      "epoch:24 step:18884 [D loss: 0.274559, acc.: 87.50%] [G loss: 3.798995]\n",
      "epoch:24 step:18885 [D loss: 0.363336, acc.: 83.59%] [G loss: 2.592940]\n",
      "epoch:24 step:18886 [D loss: 0.302585, acc.: 86.72%] [G loss: 2.491751]\n",
      "epoch:24 step:18887 [D loss: 0.281191, acc.: 86.72%] [G loss: 3.275542]\n",
      "epoch:24 step:18888 [D loss: 0.327707, acc.: 87.50%] [G loss: 3.243211]\n",
      "epoch:24 step:18889 [D loss: 0.415053, acc.: 82.03%] [G loss: 2.842298]\n",
      "epoch:24 step:18890 [D loss: 0.379227, acc.: 79.69%] [G loss: 3.438573]\n",
      "epoch:24 step:18891 [D loss: 0.254992, acc.: 88.28%] [G loss: 2.556073]\n",
      "epoch:24 step:18892 [D loss: 0.341001, acc.: 85.16%] [G loss: 4.543916]\n",
      "epoch:24 step:18893 [D loss: 0.407761, acc.: 82.81%] [G loss: 2.818434]\n",
      "epoch:24 step:18894 [D loss: 0.269842, acc.: 87.50%] [G loss: 3.333224]\n",
      "epoch:24 step:18895 [D loss: 0.389736, acc.: 82.81%] [G loss: 2.893472]\n",
      "epoch:24 step:18896 [D loss: 0.380224, acc.: 80.47%] [G loss: 3.404211]\n",
      "epoch:24 step:18897 [D loss: 0.323700, acc.: 85.94%] [G loss: 2.411092]\n",
      "epoch:24 step:18898 [D loss: 0.335803, acc.: 84.38%] [G loss: 3.573358]\n",
      "epoch:24 step:18899 [D loss: 0.341234, acc.: 89.06%] [G loss: 3.760094]\n",
      "epoch:24 step:18900 [D loss: 0.257859, acc.: 88.28%] [G loss: 2.962280]\n",
      "epoch:24 step:18901 [D loss: 0.353284, acc.: 85.94%] [G loss: 4.301531]\n",
      "epoch:24 step:18902 [D loss: 0.355507, acc.: 87.50%] [G loss: 3.141270]\n",
      "epoch:24 step:18903 [D loss: 0.220506, acc.: 90.62%] [G loss: 3.725905]\n",
      "epoch:24 step:18904 [D loss: 0.310975, acc.: 88.28%] [G loss: 3.112453]\n",
      "epoch:24 step:18905 [D loss: 0.269516, acc.: 84.38%] [G loss: 3.545900]\n",
      "epoch:24 step:18906 [D loss: 0.225737, acc.: 89.84%] [G loss: 3.547898]\n",
      "epoch:24 step:18907 [D loss: 0.350612, acc.: 88.28%] [G loss: 4.316454]\n",
      "epoch:24 step:18908 [D loss: 0.375548, acc.: 84.38%] [G loss: 3.465115]\n",
      "epoch:24 step:18909 [D loss: 0.496823, acc.: 75.78%] [G loss: 3.344964]\n",
      "epoch:24 step:18910 [D loss: 0.547078, acc.: 75.78%] [G loss: 3.443517]\n",
      "epoch:24 step:18911 [D loss: 0.434936, acc.: 78.12%] [G loss: 2.664979]\n",
      "epoch:24 step:18912 [D loss: 0.377423, acc.: 81.25%] [G loss: 2.694614]\n",
      "epoch:24 step:18913 [D loss: 0.386830, acc.: 81.25%] [G loss: 2.689291]\n",
      "epoch:24 step:18914 [D loss: 0.260600, acc.: 86.72%] [G loss: 3.408838]\n",
      "epoch:24 step:18915 [D loss: 0.333040, acc.: 84.38%] [G loss: 5.498843]\n",
      "epoch:24 step:18916 [D loss: 0.328773, acc.: 89.06%] [G loss: 3.633465]\n",
      "epoch:24 step:18917 [D loss: 0.330295, acc.: 87.50%] [G loss: 3.113379]\n",
      "epoch:24 step:18918 [D loss: 0.285238, acc.: 86.72%] [G loss: 2.329222]\n",
      "epoch:24 step:18919 [D loss: 0.354856, acc.: 83.59%] [G loss: 3.280725]\n",
      "epoch:24 step:18920 [D loss: 0.324169, acc.: 83.59%] [G loss: 3.048822]\n",
      "epoch:24 step:18921 [D loss: 0.242061, acc.: 88.28%] [G loss: 5.154656]\n",
      "epoch:24 step:18922 [D loss: 0.246241, acc.: 85.16%] [G loss: 3.366860]\n",
      "epoch:24 step:18923 [D loss: 0.362981, acc.: 86.72%] [G loss: 5.330570]\n",
      "epoch:24 step:18924 [D loss: 0.294379, acc.: 87.50%] [G loss: 6.891178]\n",
      "epoch:24 step:18925 [D loss: 0.259566, acc.: 89.06%] [G loss: 3.056977]\n",
      "epoch:24 step:18926 [D loss: 0.355580, acc.: 82.03%] [G loss: 2.675688]\n",
      "epoch:24 step:18927 [D loss: 0.250383, acc.: 89.06%] [G loss: 3.557588]\n",
      "epoch:24 step:18928 [D loss: 0.339645, acc.: 82.81%] [G loss: 3.179787]\n",
      "epoch:24 step:18929 [D loss: 0.197797, acc.: 89.84%] [G loss: 4.128710]\n",
      "epoch:24 step:18930 [D loss: 0.259416, acc.: 92.19%] [G loss: 2.655353]\n",
      "epoch:24 step:18931 [D loss: 0.310614, acc.: 85.94%] [G loss: 2.559053]\n",
      "epoch:24 step:18932 [D loss: 0.355697, acc.: 82.81%] [G loss: 3.161349]\n",
      "epoch:24 step:18933 [D loss: 0.306629, acc.: 86.72%] [G loss: 4.456991]\n",
      "epoch:24 step:18934 [D loss: 0.431154, acc.: 78.91%] [G loss: 3.066345]\n",
      "epoch:24 step:18935 [D loss: 0.352144, acc.: 83.59%] [G loss: 2.810657]\n",
      "epoch:24 step:18936 [D loss: 0.267212, acc.: 87.50%] [G loss: 4.245733]\n",
      "epoch:24 step:18937 [D loss: 0.252241, acc.: 89.84%] [G loss: 3.111629]\n",
      "epoch:24 step:18938 [D loss: 0.370577, acc.: 84.38%] [G loss: 3.560981]\n",
      "epoch:24 step:18939 [D loss: 0.375978, acc.: 84.38%] [G loss: 2.693783]\n",
      "epoch:24 step:18940 [D loss: 0.241307, acc.: 92.19%] [G loss: 3.373416]\n",
      "epoch:24 step:18941 [D loss: 0.328010, acc.: 85.94%] [G loss: 2.955841]\n",
      "epoch:24 step:18942 [D loss: 0.260001, acc.: 88.28%] [G loss: 2.622418]\n",
      "epoch:24 step:18943 [D loss: 0.314909, acc.: 85.94%] [G loss: 3.074896]\n",
      "epoch:24 step:18944 [D loss: 0.272509, acc.: 89.84%] [G loss: 4.778303]\n",
      "epoch:24 step:18945 [D loss: 0.332812, acc.: 84.38%] [G loss: 3.608104]\n",
      "epoch:24 step:18946 [D loss: 0.301014, acc.: 86.72%] [G loss: 2.982823]\n",
      "epoch:24 step:18947 [D loss: 0.191022, acc.: 93.75%] [G loss: 3.126450]\n",
      "epoch:24 step:18948 [D loss: 0.349822, acc.: 81.25%] [G loss: 3.331465]\n",
      "epoch:24 step:18949 [D loss: 0.419203, acc.: 85.94%] [G loss: 4.313365]\n",
      "epoch:24 step:18950 [D loss: 0.338285, acc.: 86.72%] [G loss: 6.073477]\n",
      "epoch:24 step:18951 [D loss: 0.319885, acc.: 86.72%] [G loss: 5.761189]\n",
      "epoch:24 step:18952 [D loss: 0.345525, acc.: 85.94%] [G loss: 3.721779]\n",
      "epoch:24 step:18953 [D loss: 0.146817, acc.: 95.31%] [G loss: 3.622722]\n",
      "epoch:24 step:18954 [D loss: 0.337939, acc.: 84.38%] [G loss: 3.275918]\n",
      "epoch:24 step:18955 [D loss: 0.265163, acc.: 89.84%] [G loss: 6.227338]\n",
      "epoch:24 step:18956 [D loss: 0.184259, acc.: 93.75%] [G loss: 5.114387]\n",
      "epoch:24 step:18957 [D loss: 0.267087, acc.: 86.72%] [G loss: 3.915107]\n",
      "epoch:24 step:18958 [D loss: 0.282388, acc.: 86.72%] [G loss: 4.182384]\n",
      "epoch:24 step:18959 [D loss: 0.204789, acc.: 89.84%] [G loss: 3.535219]\n",
      "epoch:24 step:18960 [D loss: 0.258673, acc.: 89.06%] [G loss: 4.111813]\n",
      "epoch:24 step:18961 [D loss: 0.369952, acc.: 82.03%] [G loss: 2.688639]\n",
      "epoch:24 step:18962 [D loss: 0.325196, acc.: 85.94%] [G loss: 2.703147]\n",
      "epoch:24 step:18963 [D loss: 0.296878, acc.: 84.38%] [G loss: 3.294474]\n",
      "epoch:24 step:18964 [D loss: 0.341741, acc.: 89.06%] [G loss: 2.885547]\n",
      "epoch:24 step:18965 [D loss: 0.311525, acc.: 89.84%] [G loss: 3.182581]\n",
      "epoch:24 step:18966 [D loss: 0.260916, acc.: 86.72%] [G loss: 4.228428]\n",
      "epoch:24 step:18967 [D loss: 0.318180, acc.: 89.84%] [G loss: 3.333706]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:24 step:18968 [D loss: 0.307271, acc.: 83.59%] [G loss: 4.402131]\n",
      "epoch:24 step:18969 [D loss: 0.376378, acc.: 83.59%] [G loss: 3.714679]\n",
      "epoch:24 step:18970 [D loss: 0.368127, acc.: 82.03%] [G loss: 4.411272]\n",
      "epoch:24 step:18971 [D loss: 0.354678, acc.: 81.25%] [G loss: 4.952518]\n",
      "epoch:24 step:18972 [D loss: 0.292075, acc.: 89.84%] [G loss: 2.776876]\n",
      "epoch:24 step:18973 [D loss: 0.262232, acc.: 88.28%] [G loss: 3.922057]\n",
      "epoch:24 step:18974 [D loss: 0.334397, acc.: 86.72%] [G loss: 2.950057]\n",
      "epoch:24 step:18975 [D loss: 0.265947, acc.: 90.62%] [G loss: 3.113786]\n",
      "epoch:24 step:18976 [D loss: 0.243491, acc.: 93.75%] [G loss: 3.018658]\n",
      "epoch:24 step:18977 [D loss: 0.251642, acc.: 86.72%] [G loss: 3.626260]\n",
      "epoch:24 step:18978 [D loss: 0.388770, acc.: 78.12%] [G loss: 3.038459]\n",
      "epoch:24 step:18979 [D loss: 0.301268, acc.: 86.72%] [G loss: 5.748470]\n",
      "epoch:24 step:18980 [D loss: 0.331795, acc.: 85.94%] [G loss: 4.822540]\n",
      "epoch:24 step:18981 [D loss: 0.367898, acc.: 86.72%] [G loss: 3.467237]\n",
      "epoch:24 step:18982 [D loss: 0.424844, acc.: 80.47%] [G loss: 4.019430]\n",
      "epoch:24 step:18983 [D loss: 0.287723, acc.: 88.28%] [G loss: 5.136862]\n",
      "epoch:24 step:18984 [D loss: 0.397067, acc.: 83.59%] [G loss: 3.684166]\n",
      "epoch:24 step:18985 [D loss: 0.376476, acc.: 81.25%] [G loss: 5.178133]\n",
      "epoch:24 step:18986 [D loss: 0.666541, acc.: 76.56%] [G loss: 7.470958]\n",
      "epoch:24 step:18987 [D loss: 1.804682, acc.: 63.28%] [G loss: 9.404949]\n",
      "epoch:24 step:18988 [D loss: 2.562051, acc.: 59.38%] [G loss: 5.570825]\n",
      "epoch:24 step:18989 [D loss: 1.342427, acc.: 69.53%] [G loss: 6.480048]\n",
      "epoch:24 step:18990 [D loss: 0.243216, acc.: 91.41%] [G loss: 8.066401]\n",
      "epoch:24 step:18991 [D loss: 0.698342, acc.: 75.00%] [G loss: 3.037523]\n",
      "epoch:24 step:18992 [D loss: 0.399175, acc.: 78.91%] [G loss: 4.184622]\n",
      "epoch:24 step:18993 [D loss: 0.379994, acc.: 82.81%] [G loss: 3.863488]\n",
      "epoch:24 step:18994 [D loss: 0.407750, acc.: 82.81%] [G loss: 3.243777]\n",
      "epoch:24 step:18995 [D loss: 0.283423, acc.: 89.06%] [G loss: 3.184195]\n",
      "epoch:24 step:18996 [D loss: 0.282711, acc.: 89.06%] [G loss: 3.508640]\n",
      "epoch:24 step:18997 [D loss: 0.297765, acc.: 88.28%] [G loss: 2.701371]\n",
      "epoch:24 step:18998 [D loss: 0.369038, acc.: 85.16%] [G loss: 4.114384]\n",
      "epoch:24 step:18999 [D loss: 0.276035, acc.: 86.72%] [G loss: 3.775618]\n",
      "epoch:24 step:19000 [D loss: 0.253264, acc.: 87.50%] [G loss: 3.655623]\n",
      "##############\n",
      "[0.86680516 0.85085131 0.81233744 0.79883094 0.76901873 0.81753721\n",
      " 0.84560696 0.81693798 0.81540041 0.82179826]\n",
      "##########\n",
      "epoch:24 step:19001 [D loss: 0.240398, acc.: 90.62%] [G loss: 3.357811]\n",
      "epoch:24 step:19002 [D loss: 0.354241, acc.: 83.59%] [G loss: 3.076197]\n",
      "epoch:24 step:19003 [D loss: 0.306529, acc.: 85.16%] [G loss: 3.746563]\n",
      "epoch:24 step:19004 [D loss: 0.404677, acc.: 82.03%] [G loss: 2.945185]\n",
      "epoch:24 step:19005 [D loss: 0.311598, acc.: 83.59%] [G loss: 2.327261]\n",
      "epoch:24 step:19006 [D loss: 0.283503, acc.: 89.84%] [G loss: 3.001816]\n",
      "epoch:24 step:19007 [D loss: 0.315151, acc.: 86.72%] [G loss: 3.617744]\n",
      "epoch:24 step:19008 [D loss: 0.314240, acc.: 85.94%] [G loss: 3.409276]\n",
      "epoch:24 step:19009 [D loss: 0.286532, acc.: 87.50%] [G loss: 4.334445]\n",
      "epoch:24 step:19010 [D loss: 0.457121, acc.: 84.38%] [G loss: 4.460405]\n",
      "epoch:24 step:19011 [D loss: 0.323490, acc.: 85.16%] [G loss: 3.726605]\n",
      "epoch:24 step:19012 [D loss: 0.333219, acc.: 85.94%] [G loss: 2.030995]\n",
      "epoch:24 step:19013 [D loss: 0.264223, acc.: 87.50%] [G loss: 4.510622]\n",
      "epoch:24 step:19014 [D loss: 0.286913, acc.: 86.72%] [G loss: 2.916775]\n",
      "epoch:24 step:19015 [D loss: 0.241755, acc.: 87.50%] [G loss: 3.139189]\n",
      "epoch:24 step:19016 [D loss: 0.263079, acc.: 85.94%] [G loss: 5.138484]\n",
      "epoch:24 step:19017 [D loss: 0.293465, acc.: 87.50%] [G loss: 3.197918]\n",
      "epoch:24 step:19018 [D loss: 0.228143, acc.: 90.62%] [G loss: 3.588262]\n",
      "epoch:24 step:19019 [D loss: 0.332980, acc.: 85.16%] [G loss: 2.654309]\n",
      "epoch:24 step:19020 [D loss: 0.437327, acc.: 75.78%] [G loss: 3.516176]\n",
      "epoch:24 step:19021 [D loss: 0.387322, acc.: 84.38%] [G loss: 3.001884]\n",
      "epoch:24 step:19022 [D loss: 0.385401, acc.: 82.81%] [G loss: 3.083132]\n",
      "epoch:24 step:19023 [D loss: 0.302330, acc.: 85.94%] [G loss: 2.810609]\n",
      "epoch:24 step:19024 [D loss: 0.299844, acc.: 88.28%] [G loss: 3.056018]\n",
      "epoch:24 step:19025 [D loss: 0.376739, acc.: 86.72%] [G loss: 2.764790]\n",
      "epoch:24 step:19026 [D loss: 0.168985, acc.: 94.53%] [G loss: 3.584395]\n",
      "epoch:24 step:19027 [D loss: 0.314239, acc.: 86.72%] [G loss: 2.754054]\n",
      "epoch:24 step:19028 [D loss: 0.320260, acc.: 85.94%] [G loss: 2.892052]\n",
      "epoch:24 step:19029 [D loss: 0.382565, acc.: 82.03%] [G loss: 3.152179]\n",
      "epoch:24 step:19030 [D loss: 0.323595, acc.: 85.16%] [G loss: 2.921314]\n",
      "epoch:24 step:19031 [D loss: 0.227241, acc.: 91.41%] [G loss: 2.768003]\n",
      "epoch:24 step:19032 [D loss: 0.274388, acc.: 89.06%] [G loss: 2.645425]\n",
      "epoch:24 step:19033 [D loss: 0.268158, acc.: 90.62%] [G loss: 3.149405]\n",
      "epoch:24 step:19034 [D loss: 0.226287, acc.: 92.19%] [G loss: 3.116090]\n",
      "epoch:24 step:19035 [D loss: 0.327169, acc.: 86.72%] [G loss: 2.595921]\n",
      "epoch:24 step:19036 [D loss: 0.283189, acc.: 88.28%] [G loss: 3.294404]\n",
      "epoch:24 step:19037 [D loss: 0.331552, acc.: 86.72%] [G loss: 3.019006]\n",
      "epoch:24 step:19038 [D loss: 0.331985, acc.: 83.59%] [G loss: 2.922001]\n",
      "epoch:24 step:19039 [D loss: 0.364586, acc.: 87.50%] [G loss: 3.003102]\n",
      "epoch:24 step:19040 [D loss: 0.293875, acc.: 89.84%] [G loss: 2.314898]\n",
      "epoch:24 step:19041 [D loss: 0.289511, acc.: 88.28%] [G loss: 3.342779]\n",
      "epoch:24 step:19042 [D loss: 0.317084, acc.: 87.50%] [G loss: 4.459203]\n",
      "epoch:24 step:19043 [D loss: 0.258919, acc.: 85.16%] [G loss: 3.385961]\n",
      "epoch:24 step:19044 [D loss: 0.327510, acc.: 88.28%] [G loss: 4.517499]\n",
      "epoch:24 step:19045 [D loss: 0.387088, acc.: 82.03%] [G loss: 2.746994]\n",
      "epoch:24 step:19046 [D loss: 0.482782, acc.: 82.03%] [G loss: 2.834970]\n",
      "epoch:24 step:19047 [D loss: 0.159783, acc.: 94.53%] [G loss: 3.353360]\n",
      "epoch:24 step:19048 [D loss: 0.274005, acc.: 85.94%] [G loss: 3.702838]\n",
      "epoch:24 step:19049 [D loss: 0.318770, acc.: 91.41%] [G loss: 3.614034]\n",
      "epoch:24 step:19050 [D loss: 0.314672, acc.: 85.94%] [G loss: 3.043745]\n",
      "epoch:24 step:19051 [D loss: 0.387376, acc.: 81.25%] [G loss: 2.794999]\n",
      "epoch:24 step:19052 [D loss: 0.252381, acc.: 89.84%] [G loss: 3.227505]\n",
      "epoch:24 step:19053 [D loss: 0.424224, acc.: 81.25%] [G loss: 3.275608]\n",
      "epoch:24 step:19054 [D loss: 0.228700, acc.: 89.84%] [G loss: 4.935051]\n",
      "epoch:24 step:19055 [D loss: 0.278331, acc.: 88.28%] [G loss: 4.132068]\n",
      "epoch:24 step:19056 [D loss: 0.237066, acc.: 89.84%] [G loss: 3.956947]\n",
      "epoch:24 step:19057 [D loss: 0.351206, acc.: 84.38%] [G loss: 4.380713]\n",
      "epoch:24 step:19058 [D loss: 0.282634, acc.: 88.28%] [G loss: 3.143883]\n",
      "epoch:24 step:19059 [D loss: 0.368009, acc.: 80.47%] [G loss: 5.440204]\n",
      "epoch:24 step:19060 [D loss: 0.305188, acc.: 89.84%] [G loss: 4.066569]\n",
      "epoch:24 step:19061 [D loss: 0.288033, acc.: 87.50%] [G loss: 4.740331]\n",
      "epoch:24 step:19062 [D loss: 0.245120, acc.: 91.41%] [G loss: 3.875878]\n",
      "epoch:24 step:19063 [D loss: 0.335108, acc.: 85.16%] [G loss: 2.729373]\n",
      "epoch:24 step:19064 [D loss: 0.308455, acc.: 89.06%] [G loss: 2.659976]\n",
      "epoch:24 step:19065 [D loss: 0.232374, acc.: 92.19%] [G loss: 3.909149]\n",
      "epoch:24 step:19066 [D loss: 0.305331, acc.: 85.16%] [G loss: 3.845737]\n",
      "epoch:24 step:19067 [D loss: 0.303789, acc.: 85.16%] [G loss: 2.529404]\n",
      "epoch:24 step:19068 [D loss: 0.278778, acc.: 85.94%] [G loss: 3.396645]\n",
      "epoch:24 step:19069 [D loss: 0.238087, acc.: 89.84%] [G loss: 3.445369]\n",
      "epoch:24 step:19070 [D loss: 0.427194, acc.: 82.81%] [G loss: 2.805927]\n",
      "epoch:24 step:19071 [D loss: 0.265977, acc.: 89.06%] [G loss: 3.773048]\n",
      "epoch:24 step:19072 [D loss: 0.316822, acc.: 85.94%] [G loss: 2.765921]\n",
      "epoch:24 step:19073 [D loss: 0.213599, acc.: 93.75%] [G loss: 3.423160]\n",
      "epoch:24 step:19074 [D loss: 0.223614, acc.: 90.62%] [G loss: 4.927062]\n",
      "epoch:24 step:19075 [D loss: 0.227406, acc.: 92.19%] [G loss: 3.813921]\n",
      "epoch:24 step:19076 [D loss: 0.316829, acc.: 85.16%] [G loss: 3.314564]\n",
      "epoch:24 step:19077 [D loss: 0.189005, acc.: 94.53%] [G loss: 3.340380]\n",
      "epoch:24 step:19078 [D loss: 0.281602, acc.: 86.72%] [G loss: 3.502401]\n",
      "epoch:24 step:19079 [D loss: 0.295913, acc.: 88.28%] [G loss: 2.612540]\n",
      "epoch:24 step:19080 [D loss: 0.320971, acc.: 87.50%] [G loss: 3.285299]\n",
      "epoch:24 step:19081 [D loss: 0.300585, acc.: 90.62%] [G loss: 6.450746]\n",
      "epoch:24 step:19082 [D loss: 0.637565, acc.: 69.53%] [G loss: 3.353116]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:24 step:19083 [D loss: 0.340372, acc.: 89.06%] [G loss: 3.166878]\n",
      "epoch:24 step:19084 [D loss: 0.374487, acc.: 79.69%] [G loss: 2.901373]\n",
      "epoch:24 step:19085 [D loss: 0.317018, acc.: 87.50%] [G loss: 2.974624]\n",
      "epoch:24 step:19086 [D loss: 0.508588, acc.: 76.56%] [G loss: 3.420046]\n",
      "epoch:24 step:19087 [D loss: 0.288423, acc.: 88.28%] [G loss: 3.019255]\n",
      "epoch:24 step:19088 [D loss: 0.265939, acc.: 87.50%] [G loss: 3.323617]\n",
      "epoch:24 step:19089 [D loss: 0.236311, acc.: 90.62%] [G loss: 2.733125]\n",
      "epoch:24 step:19090 [D loss: 0.346414, acc.: 82.03%] [G loss: 2.681906]\n",
      "epoch:24 step:19091 [D loss: 0.302271, acc.: 88.28%] [G loss: 3.796088]\n",
      "epoch:24 step:19092 [D loss: 0.284420, acc.: 85.94%] [G loss: 3.009393]\n",
      "epoch:24 step:19093 [D loss: 0.317182, acc.: 85.16%] [G loss: 2.811034]\n",
      "epoch:24 step:19094 [D loss: 0.306443, acc.: 85.94%] [G loss: 3.496238]\n",
      "epoch:24 step:19095 [D loss: 0.366234, acc.: 82.81%] [G loss: 3.662892]\n",
      "epoch:24 step:19096 [D loss: 0.304930, acc.: 82.81%] [G loss: 3.313918]\n",
      "epoch:24 step:19097 [D loss: 0.400718, acc.: 85.16%] [G loss: 2.503466]\n",
      "epoch:24 step:19098 [D loss: 0.184708, acc.: 93.75%] [G loss: 3.133187]\n",
      "epoch:24 step:19099 [D loss: 0.335732, acc.: 85.94%] [G loss: 2.615487]\n",
      "epoch:24 step:19100 [D loss: 0.319845, acc.: 82.81%] [G loss: 3.255927]\n",
      "epoch:24 step:19101 [D loss: 0.364487, acc.: 84.38%] [G loss: 3.064137]\n",
      "epoch:24 step:19102 [D loss: 0.264717, acc.: 90.62%] [G loss: 3.104195]\n",
      "epoch:24 step:19103 [D loss: 0.258717, acc.: 89.84%] [G loss: 3.725708]\n",
      "epoch:24 step:19104 [D loss: 0.172685, acc.: 94.53%] [G loss: 6.859728]\n",
      "epoch:24 step:19105 [D loss: 0.201777, acc.: 92.19%] [G loss: 5.872491]\n",
      "epoch:24 step:19106 [D loss: 0.263868, acc.: 91.41%] [G loss: 5.900925]\n",
      "epoch:24 step:19107 [D loss: 0.264314, acc.: 90.62%] [G loss: 4.154122]\n",
      "epoch:24 step:19108 [D loss: 0.150022, acc.: 94.53%] [G loss: 4.893544]\n",
      "epoch:24 step:19109 [D loss: 0.279925, acc.: 85.16%] [G loss: 6.392433]\n",
      "epoch:24 step:19110 [D loss: 0.229396, acc.: 88.28%] [G loss: 4.831062]\n",
      "epoch:24 step:19111 [D loss: 0.294980, acc.: 86.72%] [G loss: 4.192220]\n",
      "epoch:24 step:19112 [D loss: 0.323821, acc.: 88.28%] [G loss: 5.637043]\n",
      "epoch:24 step:19113 [D loss: 0.335484, acc.: 81.25%] [G loss: 3.044351]\n",
      "epoch:24 step:19114 [D loss: 0.206256, acc.: 94.53%] [G loss: 3.552137]\n",
      "epoch:24 step:19115 [D loss: 0.327425, acc.: 88.28%] [G loss: 3.613345]\n",
      "epoch:24 step:19116 [D loss: 0.280712, acc.: 88.28%] [G loss: 2.651264]\n",
      "epoch:24 step:19117 [D loss: 0.326443, acc.: 82.81%] [G loss: 3.220686]\n",
      "epoch:24 step:19118 [D loss: 0.331660, acc.: 84.38%] [G loss: 2.572670]\n",
      "epoch:24 step:19119 [D loss: 0.262050, acc.: 88.28%] [G loss: 3.226312]\n",
      "epoch:24 step:19120 [D loss: 0.269854, acc.: 85.94%] [G loss: 2.851617]\n",
      "epoch:24 step:19121 [D loss: 0.279977, acc.: 85.16%] [G loss: 3.519146]\n",
      "epoch:24 step:19122 [D loss: 0.224823, acc.: 86.72%] [G loss: 3.838017]\n",
      "epoch:24 step:19123 [D loss: 0.265567, acc.: 89.84%] [G loss: 4.497397]\n",
      "epoch:24 step:19124 [D loss: 0.257795, acc.: 89.06%] [G loss: 2.967537]\n",
      "epoch:24 step:19125 [D loss: 0.193182, acc.: 92.19%] [G loss: 3.134859]\n",
      "epoch:24 step:19126 [D loss: 0.280493, acc.: 87.50%] [G loss: 2.532794]\n",
      "epoch:24 step:19127 [D loss: 0.282213, acc.: 85.94%] [G loss: 3.781021]\n",
      "epoch:24 step:19128 [D loss: 0.286831, acc.: 89.06%] [G loss: 4.377315]\n",
      "epoch:24 step:19129 [D loss: 0.254880, acc.: 90.62%] [G loss: 3.012612]\n",
      "epoch:24 step:19130 [D loss: 0.326484, acc.: 85.16%] [G loss: 3.049569]\n",
      "epoch:24 step:19131 [D loss: 0.272894, acc.: 89.06%] [G loss: 3.193231]\n",
      "epoch:24 step:19132 [D loss: 0.238259, acc.: 89.84%] [G loss: 4.772268]\n",
      "epoch:24 step:19133 [D loss: 0.317371, acc.: 84.38%] [G loss: 3.776259]\n",
      "epoch:24 step:19134 [D loss: 0.318048, acc.: 87.50%] [G loss: 2.674901]\n",
      "epoch:24 step:19135 [D loss: 0.307491, acc.: 85.16%] [G loss: 5.211053]\n",
      "epoch:24 step:19136 [D loss: 0.202067, acc.: 92.97%] [G loss: 6.662766]\n",
      "epoch:24 step:19137 [D loss: 0.264358, acc.: 87.50%] [G loss: 4.592341]\n",
      "epoch:24 step:19138 [D loss: 0.198702, acc.: 92.97%] [G loss: 6.253034]\n",
      "epoch:24 step:19139 [D loss: 0.371030, acc.: 83.59%] [G loss: 3.552306]\n",
      "epoch:24 step:19140 [D loss: 0.344463, acc.: 78.91%] [G loss: 3.420751]\n",
      "epoch:24 step:19141 [D loss: 0.318671, acc.: 84.38%] [G loss: 3.168142]\n",
      "epoch:24 step:19142 [D loss: 0.231563, acc.: 89.84%] [G loss: 3.617599]\n",
      "epoch:24 step:19143 [D loss: 0.250852, acc.: 89.06%] [G loss: 3.888933]\n",
      "epoch:24 step:19144 [D loss: 0.290781, acc.: 88.28%] [G loss: 4.611428]\n",
      "epoch:24 step:19145 [D loss: 0.330343, acc.: 85.16%] [G loss: 3.507985]\n",
      "epoch:24 step:19146 [D loss: 0.311300, acc.: 85.16%] [G loss: 4.181920]\n",
      "epoch:24 step:19147 [D loss: 0.194452, acc.: 90.62%] [G loss: 4.631704]\n",
      "epoch:24 step:19148 [D loss: 0.188019, acc.: 95.31%] [G loss: 4.307275]\n",
      "epoch:24 step:19149 [D loss: 0.254202, acc.: 92.19%] [G loss: 4.870140]\n",
      "epoch:24 step:19150 [D loss: 0.229006, acc.: 90.62%] [G loss: 4.694454]\n",
      "epoch:24 step:19151 [D loss: 0.301562, acc.: 86.72%] [G loss: 3.841627]\n",
      "epoch:24 step:19152 [D loss: 0.236430, acc.: 86.72%] [G loss: 4.348102]\n",
      "epoch:24 step:19153 [D loss: 0.272334, acc.: 85.16%] [G loss: 2.985738]\n",
      "epoch:24 step:19154 [D loss: 0.281238, acc.: 83.59%] [G loss: 4.965262]\n",
      "epoch:24 step:19155 [D loss: 0.260084, acc.: 89.06%] [G loss: 3.930654]\n",
      "epoch:24 step:19156 [D loss: 0.356423, acc.: 86.72%] [G loss: 4.021127]\n",
      "epoch:24 step:19157 [D loss: 0.309478, acc.: 84.38%] [G loss: 4.559544]\n",
      "epoch:24 step:19158 [D loss: 0.286963, acc.: 83.59%] [G loss: 4.661769]\n",
      "epoch:24 step:19159 [D loss: 0.279242, acc.: 89.84%] [G loss: 2.648511]\n",
      "epoch:24 step:19160 [D loss: 0.289971, acc.: 87.50%] [G loss: 2.843015]\n",
      "epoch:24 step:19161 [D loss: 0.370419, acc.: 81.25%] [G loss: 3.298369]\n",
      "epoch:24 step:19162 [D loss: 0.399205, acc.: 87.50%] [G loss: 5.975283]\n",
      "epoch:24 step:19163 [D loss: 0.512890, acc.: 75.78%] [G loss: 5.266401]\n",
      "epoch:24 step:19164 [D loss: 0.552880, acc.: 77.34%] [G loss: 4.371868]\n",
      "epoch:24 step:19165 [D loss: 0.476961, acc.: 77.34%] [G loss: 4.783202]\n",
      "epoch:24 step:19166 [D loss: 0.259657, acc.: 87.50%] [G loss: 6.701975]\n",
      "epoch:24 step:19167 [D loss: 0.284228, acc.: 85.94%] [G loss: 5.724567]\n",
      "epoch:24 step:19168 [D loss: 0.249826, acc.: 87.50%] [G loss: 5.532497]\n",
      "epoch:24 step:19169 [D loss: 0.254706, acc.: 90.62%] [G loss: 3.881731]\n",
      "epoch:24 step:19170 [D loss: 0.314061, acc.: 87.50%] [G loss: 3.785121]\n",
      "epoch:24 step:19171 [D loss: 0.329279, acc.: 87.50%] [G loss: 3.682144]\n",
      "epoch:24 step:19172 [D loss: 0.348966, acc.: 84.38%] [G loss: 3.549569]\n",
      "epoch:24 step:19173 [D loss: 0.242660, acc.: 89.84%] [G loss: 3.251574]\n",
      "epoch:24 step:19174 [D loss: 0.352618, acc.: 83.59%] [G loss: 3.195064]\n",
      "epoch:24 step:19175 [D loss: 0.395593, acc.: 82.03%] [G loss: 3.255286]\n",
      "epoch:24 step:19176 [D loss: 0.587589, acc.: 71.88%] [G loss: 3.622999]\n",
      "epoch:24 step:19177 [D loss: 0.319238, acc.: 85.94%] [G loss: 3.623382]\n",
      "epoch:24 step:19178 [D loss: 0.340611, acc.: 88.28%] [G loss: 4.161245]\n",
      "epoch:24 step:19179 [D loss: 0.335291, acc.: 88.28%] [G loss: 3.893490]\n",
      "epoch:24 step:19180 [D loss: 0.478078, acc.: 74.22%] [G loss: 3.885804]\n",
      "epoch:24 step:19181 [D loss: 0.510315, acc.: 82.81%] [G loss: 3.871267]\n",
      "epoch:24 step:19182 [D loss: 0.348129, acc.: 85.16%] [G loss: 3.326828]\n",
      "epoch:24 step:19183 [D loss: 0.362336, acc.: 85.94%] [G loss: 3.904887]\n",
      "epoch:24 step:19184 [D loss: 0.278414, acc.: 88.28%] [G loss: 3.916596]\n",
      "epoch:24 step:19185 [D loss: 0.408885, acc.: 78.91%] [G loss: 3.442841]\n",
      "epoch:24 step:19186 [D loss: 0.266202, acc.: 87.50%] [G loss: 4.306315]\n",
      "epoch:24 step:19187 [D loss: 0.278170, acc.: 88.28%] [G loss: 3.222812]\n",
      "epoch:24 step:19188 [D loss: 0.232966, acc.: 91.41%] [G loss: 3.732676]\n",
      "epoch:24 step:19189 [D loss: 0.266853, acc.: 89.06%] [G loss: 3.031260]\n",
      "epoch:24 step:19190 [D loss: 0.248349, acc.: 89.84%] [G loss: 3.116599]\n",
      "epoch:24 step:19191 [D loss: 0.239998, acc.: 92.19%] [G loss: 3.750965]\n",
      "epoch:24 step:19192 [D loss: 0.270814, acc.: 87.50%] [G loss: 4.210849]\n",
      "epoch:24 step:19193 [D loss: 0.350479, acc.: 83.59%] [G loss: 2.666753]\n",
      "epoch:24 step:19194 [D loss: 0.238671, acc.: 91.41%] [G loss: 4.863787]\n",
      "epoch:24 step:19195 [D loss: 0.248344, acc.: 92.19%] [G loss: 3.910238]\n",
      "epoch:24 step:19196 [D loss: 0.252759, acc.: 90.62%] [G loss: 3.950575]\n",
      "epoch:24 step:19197 [D loss: 0.250132, acc.: 89.06%] [G loss: 4.496565]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:24 step:19198 [D loss: 0.205459, acc.: 92.97%] [G loss: 4.020521]\n",
      "epoch:24 step:19199 [D loss: 0.269144, acc.: 92.19%] [G loss: 3.524915]\n",
      "epoch:24 step:19200 [D loss: 0.273872, acc.: 87.50%] [G loss: 3.785328]\n",
      "##############\n",
      "[0.86224112 0.88199311 0.81215483 0.79909145 0.76703709 0.81535573\n",
      " 0.86540366 0.81838164 0.8010676  0.80046922]\n",
      "##########\n",
      "epoch:24 step:19201 [D loss: 0.255016, acc.: 90.62%] [G loss: 3.262182]\n",
      "epoch:24 step:19202 [D loss: 0.261249, acc.: 88.28%] [G loss: 4.379903]\n",
      "epoch:24 step:19203 [D loss: 0.308661, acc.: 85.94%] [G loss: 4.121171]\n",
      "epoch:24 step:19204 [D loss: 0.274916, acc.: 85.94%] [G loss: 5.127369]\n",
      "epoch:24 step:19205 [D loss: 0.275561, acc.: 89.06%] [G loss: 3.002050]\n",
      "epoch:24 step:19206 [D loss: 0.232348, acc.: 89.06%] [G loss: 4.181997]\n",
      "epoch:24 step:19207 [D loss: 0.340505, acc.: 85.94%] [G loss: 3.213370]\n",
      "epoch:24 step:19208 [D loss: 0.244616, acc.: 90.62%] [G loss: 3.430790]\n",
      "epoch:24 step:19209 [D loss: 0.387802, acc.: 79.69%] [G loss: 4.173295]\n",
      "epoch:24 step:19210 [D loss: 0.339845, acc.: 81.25%] [G loss: 3.565453]\n",
      "epoch:24 step:19211 [D loss: 0.424357, acc.: 82.03%] [G loss: 4.170517]\n",
      "epoch:24 step:19212 [D loss: 0.304165, acc.: 86.72%] [G loss: 3.330218]\n",
      "epoch:24 step:19213 [D loss: 0.345556, acc.: 82.81%] [G loss: 2.544208]\n",
      "epoch:24 step:19214 [D loss: 0.311343, acc.: 87.50%] [G loss: 3.238596]\n",
      "epoch:24 step:19215 [D loss: 0.450174, acc.: 77.34%] [G loss: 3.355457]\n",
      "epoch:24 step:19216 [D loss: 0.280914, acc.: 87.50%] [G loss: 2.920674]\n",
      "epoch:24 step:19217 [D loss: 0.255913, acc.: 87.50%] [G loss: 4.076006]\n",
      "epoch:24 step:19218 [D loss: 0.382800, acc.: 83.59%] [G loss: 5.505663]\n",
      "epoch:24 step:19219 [D loss: 0.361710, acc.: 84.38%] [G loss: 3.513420]\n",
      "epoch:24 step:19220 [D loss: 0.285164, acc.: 85.94%] [G loss: 3.964452]\n",
      "epoch:24 step:19221 [D loss: 0.187147, acc.: 96.09%] [G loss: 4.324776]\n",
      "epoch:24 step:19222 [D loss: 0.344588, acc.: 83.59%] [G loss: 3.824044]\n",
      "epoch:24 step:19223 [D loss: 0.285055, acc.: 87.50%] [G loss: 3.205456]\n",
      "epoch:24 step:19224 [D loss: 0.205413, acc.: 92.19%] [G loss: 4.207421]\n",
      "epoch:24 step:19225 [D loss: 0.318164, acc.: 85.16%] [G loss: 5.147204]\n",
      "epoch:24 step:19226 [D loss: 0.259158, acc.: 90.62%] [G loss: 2.718725]\n",
      "epoch:24 step:19227 [D loss: 0.285878, acc.: 87.50%] [G loss: 3.242409]\n",
      "epoch:24 step:19228 [D loss: 0.396758, acc.: 83.59%] [G loss: 3.738568]\n",
      "epoch:24 step:19229 [D loss: 0.296730, acc.: 89.06%] [G loss: 2.611266]\n",
      "epoch:24 step:19230 [D loss: 0.426698, acc.: 74.22%] [G loss: 2.844633]\n",
      "epoch:24 step:19231 [D loss: 0.286357, acc.: 87.50%] [G loss: 3.889297]\n",
      "epoch:24 step:19232 [D loss: 0.344896, acc.: 87.50%] [G loss: 4.410234]\n",
      "epoch:24 step:19233 [D loss: 0.340838, acc.: 83.59%] [G loss: 5.569153]\n",
      "epoch:24 step:19234 [D loss: 0.471592, acc.: 72.66%] [G loss: 3.354346]\n",
      "epoch:24 step:19235 [D loss: 0.286421, acc.: 85.94%] [G loss: 4.146578]\n",
      "epoch:24 step:19236 [D loss: 0.447450, acc.: 81.25%] [G loss: 3.498399]\n",
      "epoch:24 step:19237 [D loss: 0.241197, acc.: 91.41%] [G loss: 3.574960]\n",
      "epoch:24 step:19238 [D loss: 0.318374, acc.: 89.06%] [G loss: 3.817345]\n",
      "epoch:24 step:19239 [D loss: 0.321099, acc.: 87.50%] [G loss: 3.763225]\n",
      "epoch:24 step:19240 [D loss: 0.317604, acc.: 83.59%] [G loss: 3.213958]\n",
      "epoch:24 step:19241 [D loss: 0.253855, acc.: 92.97%] [G loss: 2.990240]\n",
      "epoch:24 step:19242 [D loss: 0.305628, acc.: 85.94%] [G loss: 2.608597]\n",
      "epoch:24 step:19243 [D loss: 0.332755, acc.: 85.16%] [G loss: 2.465771]\n",
      "epoch:24 step:19244 [D loss: 0.304316, acc.: 88.28%] [G loss: 3.392980]\n",
      "epoch:24 step:19245 [D loss: 0.359321, acc.: 82.03%] [G loss: 4.519149]\n",
      "epoch:24 step:19246 [D loss: 0.250794, acc.: 89.06%] [G loss: 7.097386]\n",
      "epoch:24 step:19247 [D loss: 0.299082, acc.: 87.50%] [G loss: 3.487475]\n",
      "epoch:24 step:19248 [D loss: 0.318431, acc.: 83.59%] [G loss: 3.496696]\n",
      "epoch:24 step:19249 [D loss: 0.334087, acc.: 83.59%] [G loss: 4.343158]\n",
      "epoch:24 step:19250 [D loss: 0.522176, acc.: 77.34%] [G loss: 6.080529]\n",
      "epoch:24 step:19251 [D loss: 0.719294, acc.: 74.22%] [G loss: 7.894689]\n",
      "epoch:24 step:19252 [D loss: 1.375402, acc.: 64.84%] [G loss: 7.237667]\n",
      "epoch:24 step:19253 [D loss: 1.133907, acc.: 64.06%] [G loss: 4.713027]\n",
      "epoch:24 step:19254 [D loss: 0.525827, acc.: 78.12%] [G loss: 6.505109]\n",
      "epoch:24 step:19255 [D loss: 0.596863, acc.: 78.12%] [G loss: 3.658508]\n",
      "epoch:24 step:19256 [D loss: 0.298294, acc.: 90.62%] [G loss: 3.371964]\n",
      "epoch:24 step:19257 [D loss: 0.357753, acc.: 85.94%] [G loss: 4.417498]\n",
      "epoch:24 step:19258 [D loss: 0.353162, acc.: 84.38%] [G loss: 3.027717]\n",
      "epoch:24 step:19259 [D loss: 0.325063, acc.: 86.72%] [G loss: 3.246990]\n",
      "epoch:24 step:19260 [D loss: 0.367147, acc.: 86.72%] [G loss: 3.311928]\n",
      "epoch:24 step:19261 [D loss: 0.317553, acc.: 84.38%] [G loss: 3.235010]\n",
      "epoch:24 step:19262 [D loss: 0.326500, acc.: 83.59%] [G loss: 6.270347]\n",
      "epoch:24 step:19263 [D loss: 0.296633, acc.: 83.59%] [G loss: 5.171401]\n",
      "epoch:24 step:19264 [D loss: 0.323569, acc.: 83.59%] [G loss: 4.113895]\n",
      "epoch:24 step:19265 [D loss: 0.281216, acc.: 88.28%] [G loss: 4.813925]\n",
      "epoch:24 step:19266 [D loss: 0.438581, acc.: 76.56%] [G loss: 3.752646]\n",
      "epoch:24 step:19267 [D loss: 0.237634, acc.: 89.06%] [G loss: 3.815407]\n",
      "epoch:24 step:19268 [D loss: 0.442371, acc.: 78.12%] [G loss: 3.093336]\n",
      "epoch:24 step:19269 [D loss: 0.264987, acc.: 88.28%] [G loss: 2.644823]\n",
      "epoch:24 step:19270 [D loss: 0.316263, acc.: 86.72%] [G loss: 2.955801]\n",
      "epoch:24 step:19271 [D loss: 0.388477, acc.: 85.94%] [G loss: 3.051645]\n",
      "epoch:24 step:19272 [D loss: 0.299188, acc.: 88.28%] [G loss: 3.101761]\n",
      "epoch:24 step:19273 [D loss: 0.247577, acc.: 91.41%] [G loss: 3.000482]\n",
      "epoch:24 step:19274 [D loss: 0.288282, acc.: 85.16%] [G loss: 3.885934]\n",
      "epoch:24 step:19275 [D loss: 0.179825, acc.: 92.19%] [G loss: 4.749927]\n",
      "epoch:24 step:19276 [D loss: 0.400886, acc.: 82.03%] [G loss: 3.053528]\n",
      "epoch:24 step:19277 [D loss: 0.278618, acc.: 86.72%] [G loss: 3.182270]\n",
      "epoch:24 step:19278 [D loss: 0.246067, acc.: 89.84%] [G loss: 2.796942]\n",
      "epoch:24 step:19279 [D loss: 0.374485, acc.: 82.03%] [G loss: 3.777127]\n",
      "epoch:24 step:19280 [D loss: 0.244408, acc.: 90.62%] [G loss: 3.378562]\n",
      "epoch:24 step:19281 [D loss: 0.315379, acc.: 84.38%] [G loss: 3.203922]\n",
      "epoch:24 step:19282 [D loss: 0.313890, acc.: 85.94%] [G loss: 3.065091]\n",
      "epoch:24 step:19283 [D loss: 0.307972, acc.: 88.28%] [G loss: 3.114710]\n",
      "epoch:24 step:19284 [D loss: 0.382653, acc.: 82.81%] [G loss: 2.915684]\n",
      "epoch:24 step:19285 [D loss: 0.389564, acc.: 85.16%] [G loss: 3.474083]\n",
      "epoch:24 step:19286 [D loss: 0.309757, acc.: 87.50%] [G loss: 3.221826]\n",
      "epoch:24 step:19287 [D loss: 0.244660, acc.: 92.19%] [G loss: 3.561403]\n",
      "epoch:24 step:19288 [D loss: 0.265949, acc.: 89.06%] [G loss: 2.970905]\n",
      "epoch:24 step:19289 [D loss: 0.268894, acc.: 85.16%] [G loss: 2.284336]\n",
      "epoch:24 step:19290 [D loss: 0.313388, acc.: 87.50%] [G loss: 2.606077]\n",
      "epoch:24 step:19291 [D loss: 0.274678, acc.: 87.50%] [G loss: 3.537213]\n",
      "epoch:24 step:19292 [D loss: 0.305256, acc.: 87.50%] [G loss: 2.965311]\n",
      "epoch:24 step:19293 [D loss: 0.338566, acc.: 85.94%] [G loss: 3.459431]\n",
      "epoch:24 step:19294 [D loss: 0.362436, acc.: 82.81%] [G loss: 3.171277]\n",
      "epoch:24 step:19295 [D loss: 0.167699, acc.: 93.75%] [G loss: 4.090017]\n",
      "epoch:24 step:19296 [D loss: 0.385374, acc.: 82.81%] [G loss: 3.756350]\n",
      "epoch:24 step:19297 [D loss: 0.311247, acc.: 82.81%] [G loss: 3.689506]\n",
      "epoch:24 step:19298 [D loss: 0.279148, acc.: 90.62%] [G loss: 3.052417]\n",
      "epoch:24 step:19299 [D loss: 0.328912, acc.: 86.72%] [G loss: 4.142197]\n",
      "epoch:24 step:19300 [D loss: 0.218094, acc.: 91.41%] [G loss: 4.333575]\n",
      "epoch:24 step:19301 [D loss: 0.239536, acc.: 92.19%] [G loss: 2.991412]\n",
      "epoch:24 step:19302 [D loss: 0.349130, acc.: 85.94%] [G loss: 3.142874]\n",
      "epoch:24 step:19303 [D loss: 0.340891, acc.: 84.38%] [G loss: 3.188320]\n",
      "epoch:24 step:19304 [D loss: 0.274003, acc.: 86.72%] [G loss: 3.350295]\n",
      "epoch:24 step:19305 [D loss: 0.179450, acc.: 95.31%] [G loss: 4.730422]\n",
      "epoch:24 step:19306 [D loss: 0.383049, acc.: 86.72%] [G loss: 3.573356]\n",
      "epoch:24 step:19307 [D loss: 0.280859, acc.: 83.59%] [G loss: 3.081728]\n",
      "epoch:24 step:19308 [D loss: 0.343761, acc.: 83.59%] [G loss: 3.549902]\n",
      "epoch:24 step:19309 [D loss: 0.312422, acc.: 83.59%] [G loss: 3.264604]\n",
      "epoch:24 step:19310 [D loss: 0.348159, acc.: 82.03%] [G loss: 3.999981]\n",
      "epoch:24 step:19311 [D loss: 0.340071, acc.: 85.16%] [G loss: 3.528815]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:24 step:19312 [D loss: 0.281335, acc.: 86.72%] [G loss: 4.657926]\n",
      "epoch:24 step:19313 [D loss: 0.224055, acc.: 91.41%] [G loss: 3.053367]\n",
      "epoch:24 step:19314 [D loss: 0.359814, acc.: 82.03%] [G loss: 3.706964]\n",
      "epoch:24 step:19315 [D loss: 0.365164, acc.: 86.72%] [G loss: 3.144489]\n",
      "epoch:24 step:19316 [D loss: 0.359109, acc.: 83.59%] [G loss: 3.580737]\n",
      "epoch:24 step:19317 [D loss: 0.337764, acc.: 82.81%] [G loss: 3.636136]\n",
      "epoch:24 step:19318 [D loss: 0.442412, acc.: 80.47%] [G loss: 3.019813]\n",
      "epoch:24 step:19319 [D loss: 0.300120, acc.: 88.28%] [G loss: 3.705612]\n",
      "epoch:24 step:19320 [D loss: 0.346206, acc.: 84.38%] [G loss: 3.063460]\n",
      "epoch:24 step:19321 [D loss: 0.258973, acc.: 88.28%] [G loss: 2.953404]\n",
      "epoch:24 step:19322 [D loss: 0.298532, acc.: 85.16%] [G loss: 2.385169]\n",
      "epoch:24 step:19323 [D loss: 0.338128, acc.: 85.16%] [G loss: 2.468701]\n",
      "epoch:24 step:19324 [D loss: 0.291890, acc.: 86.72%] [G loss: 3.616246]\n",
      "epoch:24 step:19325 [D loss: 0.304622, acc.: 88.28%] [G loss: 2.599565]\n",
      "epoch:24 step:19326 [D loss: 0.252627, acc.: 90.62%] [G loss: 3.224202]\n",
      "epoch:24 step:19327 [D loss: 0.478262, acc.: 79.69%] [G loss: 2.718364]\n",
      "epoch:24 step:19328 [D loss: 0.386239, acc.: 82.03%] [G loss: 3.388802]\n",
      "epoch:24 step:19329 [D loss: 0.270129, acc.: 89.06%] [G loss: 3.777546]\n",
      "epoch:24 step:19330 [D loss: 0.392379, acc.: 84.38%] [G loss: 6.179869]\n",
      "epoch:24 step:19331 [D loss: 0.270509, acc.: 89.06%] [G loss: 5.502926]\n",
      "epoch:24 step:19332 [D loss: 0.234545, acc.: 92.19%] [G loss: 5.050162]\n",
      "epoch:24 step:19333 [D loss: 0.298461, acc.: 87.50%] [G loss: 4.548268]\n",
      "epoch:24 step:19334 [D loss: 0.330994, acc.: 82.81%] [G loss: 3.297484]\n",
      "epoch:24 step:19335 [D loss: 0.317691, acc.: 84.38%] [G loss: 3.349887]\n",
      "epoch:24 step:19336 [D loss: 0.305846, acc.: 85.94%] [G loss: 3.221038]\n",
      "epoch:24 step:19337 [D loss: 0.387278, acc.: 82.81%] [G loss: 3.794370]\n",
      "epoch:24 step:19338 [D loss: 0.275131, acc.: 86.72%] [G loss: 3.339991]\n",
      "epoch:24 step:19339 [D loss: 0.338743, acc.: 85.94%] [G loss: 2.747297]\n",
      "epoch:24 step:19340 [D loss: 0.286276, acc.: 86.72%] [G loss: 2.442191]\n",
      "epoch:24 step:19341 [D loss: 0.323041, acc.: 85.94%] [G loss: 2.970345]\n",
      "epoch:24 step:19342 [D loss: 0.249292, acc.: 90.62%] [G loss: 3.464931]\n",
      "epoch:24 step:19343 [D loss: 0.339344, acc.: 85.16%] [G loss: 2.992958]\n",
      "epoch:24 step:19344 [D loss: 0.351558, acc.: 85.94%] [G loss: 3.340202]\n",
      "epoch:24 step:19345 [D loss: 0.268729, acc.: 89.84%] [G loss: 3.308072]\n",
      "epoch:24 step:19346 [D loss: 0.276601, acc.: 86.72%] [G loss: 2.731808]\n",
      "epoch:24 step:19347 [D loss: 0.289065, acc.: 88.28%] [G loss: 3.430756]\n",
      "epoch:24 step:19348 [D loss: 0.358264, acc.: 80.47%] [G loss: 3.310209]\n",
      "epoch:24 step:19349 [D loss: 0.330794, acc.: 84.38%] [G loss: 3.113698]\n",
      "epoch:24 step:19350 [D loss: 0.337377, acc.: 81.25%] [G loss: 5.562761]\n",
      "epoch:24 step:19351 [D loss: 0.287678, acc.: 89.06%] [G loss: 3.328820]\n",
      "epoch:24 step:19352 [D loss: 0.303046, acc.: 88.28%] [G loss: 3.345279]\n",
      "epoch:24 step:19353 [D loss: 0.312421, acc.: 85.94%] [G loss: 3.454994]\n",
      "epoch:24 step:19354 [D loss: 0.368287, acc.: 84.38%] [G loss: 2.777401]\n",
      "epoch:24 step:19355 [D loss: 0.346229, acc.: 82.81%] [G loss: 2.499393]\n",
      "epoch:24 step:19356 [D loss: 0.297874, acc.: 89.84%] [G loss: 3.124793]\n",
      "epoch:24 step:19357 [D loss: 0.312587, acc.: 87.50%] [G loss: 3.624827]\n",
      "epoch:24 step:19358 [D loss: 0.310108, acc.: 85.16%] [G loss: 3.855798]\n",
      "epoch:24 step:19359 [D loss: 0.268723, acc.: 89.06%] [G loss: 4.222294]\n",
      "epoch:24 step:19360 [D loss: 0.283698, acc.: 89.06%] [G loss: 3.268723]\n",
      "epoch:24 step:19361 [D loss: 0.240065, acc.: 89.06%] [G loss: 3.697733]\n",
      "epoch:24 step:19362 [D loss: 0.264869, acc.: 87.50%] [G loss: 4.461926]\n",
      "epoch:24 step:19363 [D loss: 0.214084, acc.: 91.41%] [G loss: 5.095066]\n",
      "epoch:24 step:19364 [D loss: 0.252533, acc.: 89.84%] [G loss: 4.601908]\n",
      "epoch:24 step:19365 [D loss: 0.261946, acc.: 87.50%] [G loss: 3.808199]\n",
      "epoch:24 step:19366 [D loss: 0.151903, acc.: 94.53%] [G loss: 4.410595]\n",
      "epoch:24 step:19367 [D loss: 0.280382, acc.: 87.50%] [G loss: 3.343410]\n",
      "epoch:24 step:19368 [D loss: 0.248528, acc.: 88.28%] [G loss: 4.292514]\n",
      "epoch:24 step:19369 [D loss: 0.321123, acc.: 88.28%] [G loss: 3.405783]\n",
      "epoch:24 step:19370 [D loss: 0.223616, acc.: 94.53%] [G loss: 4.408977]\n",
      "epoch:24 step:19371 [D loss: 0.305594, acc.: 82.81%] [G loss: 2.952421]\n",
      "epoch:24 step:19372 [D loss: 0.321340, acc.: 82.81%] [G loss: 2.856571]\n",
      "epoch:24 step:19373 [D loss: 0.355695, acc.: 85.16%] [G loss: 2.936153]\n",
      "epoch:24 step:19374 [D loss: 0.318762, acc.: 82.81%] [G loss: 4.259708]\n",
      "epoch:24 step:19375 [D loss: 0.333056, acc.: 86.72%] [G loss: 5.534971]\n",
      "epoch:24 step:19376 [D loss: 0.524299, acc.: 74.22%] [G loss: 5.064090]\n",
      "epoch:24 step:19377 [D loss: 1.277245, acc.: 67.19%] [G loss: 12.218516]\n",
      "epoch:24 step:19378 [D loss: 2.480882, acc.: 50.78%] [G loss: 4.966716]\n",
      "epoch:24 step:19379 [D loss: 0.603313, acc.: 81.25%] [G loss: 4.953009]\n",
      "epoch:24 step:19380 [D loss: 0.943505, acc.: 70.31%] [G loss: 5.341308]\n",
      "epoch:24 step:19381 [D loss: 0.422385, acc.: 85.16%] [G loss: 6.088896]\n",
      "epoch:24 step:19382 [D loss: 0.498689, acc.: 80.47%] [G loss: 4.193222]\n",
      "epoch:24 step:19383 [D loss: 0.375027, acc.: 83.59%] [G loss: 4.655732]\n",
      "epoch:24 step:19384 [D loss: 0.250414, acc.: 89.84%] [G loss: 4.710525]\n",
      "epoch:24 step:19385 [D loss: 0.196812, acc.: 92.97%] [G loss: 4.603863]\n",
      "epoch:24 step:19386 [D loss: 0.363857, acc.: 78.91%] [G loss: 4.139238]\n",
      "epoch:24 step:19387 [D loss: 0.308745, acc.: 88.28%] [G loss: 2.678762]\n",
      "epoch:24 step:19388 [D loss: 0.327433, acc.: 79.69%] [G loss: 2.963600]\n",
      "epoch:24 step:19389 [D loss: 0.295207, acc.: 88.28%] [G loss: 2.957899]\n",
      "epoch:24 step:19390 [D loss: 0.362473, acc.: 85.16%] [G loss: 2.649403]\n",
      "epoch:24 step:19391 [D loss: 0.382372, acc.: 83.59%] [G loss: 3.414459]\n",
      "epoch:24 step:19392 [D loss: 0.271390, acc.: 84.38%] [G loss: 2.187828]\n",
      "epoch:24 step:19393 [D loss: 0.348980, acc.: 85.16%] [G loss: 3.005865]\n",
      "epoch:24 step:19394 [D loss: 0.399208, acc.: 83.59%] [G loss: 2.989352]\n",
      "epoch:24 step:19395 [D loss: 0.172421, acc.: 93.75%] [G loss: 2.699916]\n",
      "epoch:24 step:19396 [D loss: 0.293131, acc.: 89.06%] [G loss: 3.588822]\n",
      "epoch:24 step:19397 [D loss: 0.327409, acc.: 85.16%] [G loss: 2.534038]\n",
      "epoch:24 step:19398 [D loss: 0.394044, acc.: 82.81%] [G loss: 2.359371]\n",
      "epoch:24 step:19399 [D loss: 0.280646, acc.: 88.28%] [G loss: 2.597723]\n",
      "epoch:24 step:19400 [D loss: 0.319815, acc.: 89.06%] [G loss: 2.880395]\n",
      "##############\n",
      "[0.85059926 0.84473841 0.81657003 0.80782699 0.77281324 0.821168\n",
      " 0.88781278 0.80853463 0.8102351  0.83362852]\n",
      "##########\n",
      "epoch:24 step:19401 [D loss: 0.317409, acc.: 89.84%] [G loss: 2.371092]\n",
      "epoch:24 step:19402 [D loss: 0.295756, acc.: 89.06%] [G loss: 3.413793]\n",
      "epoch:24 step:19403 [D loss: 0.332030, acc.: 82.03%] [G loss: 2.845962]\n",
      "epoch:24 step:19404 [D loss: 0.356735, acc.: 86.72%] [G loss: 2.851831]\n",
      "epoch:24 step:19405 [D loss: 0.379491, acc.: 85.16%] [G loss: 2.718863]\n",
      "epoch:24 step:19406 [D loss: 0.370541, acc.: 79.69%] [G loss: 2.962742]\n",
      "epoch:24 step:19407 [D loss: 0.256993, acc.: 86.72%] [G loss: 2.882788]\n",
      "epoch:24 step:19408 [D loss: 0.299196, acc.: 87.50%] [G loss: 3.350142]\n",
      "epoch:24 step:19409 [D loss: 0.267251, acc.: 88.28%] [G loss: 2.667531]\n",
      "epoch:24 step:19410 [D loss: 0.348107, acc.: 87.50%] [G loss: 2.619414]\n",
      "epoch:24 step:19411 [D loss: 0.264645, acc.: 85.16%] [G loss: 2.468534]\n",
      "epoch:24 step:19412 [D loss: 0.278025, acc.: 89.06%] [G loss: 2.535072]\n",
      "epoch:24 step:19413 [D loss: 0.351126, acc.: 88.28%] [G loss: 2.741234]\n",
      "epoch:24 step:19414 [D loss: 0.300716, acc.: 85.94%] [G loss: 2.613583]\n",
      "epoch:24 step:19415 [D loss: 0.316262, acc.: 87.50%] [G loss: 3.831309]\n",
      "epoch:24 step:19416 [D loss: 0.261872, acc.: 89.84%] [G loss: 6.806077]\n",
      "epoch:24 step:19417 [D loss: 0.315300, acc.: 89.06%] [G loss: 2.583608]\n",
      "epoch:24 step:19418 [D loss: 0.244768, acc.: 89.84%] [G loss: 3.297570]\n",
      "epoch:24 step:19419 [D loss: 0.249925, acc.: 89.06%] [G loss: 2.915815]\n",
      "epoch:24 step:19420 [D loss: 0.313430, acc.: 83.59%] [G loss: 3.462610]\n",
      "epoch:24 step:19421 [D loss: 0.212032, acc.: 89.84%] [G loss: 2.915941]\n",
      "epoch:24 step:19422 [D loss: 0.277389, acc.: 86.72%] [G loss: 2.900968]\n",
      "epoch:24 step:19423 [D loss: 0.292083, acc.: 89.06%] [G loss: 2.846511]\n",
      "epoch:24 step:19424 [D loss: 0.424216, acc.: 82.03%] [G loss: 2.991665]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:24 step:19425 [D loss: 0.321416, acc.: 87.50%] [G loss: 2.931503]\n",
      "epoch:24 step:19426 [D loss: 0.351515, acc.: 82.03%] [G loss: 2.567431]\n",
      "epoch:24 step:19427 [D loss: 0.384951, acc.: 84.38%] [G loss: 2.542345]\n",
      "epoch:24 step:19428 [D loss: 0.388888, acc.: 83.59%] [G loss: 2.377617]\n",
      "epoch:24 step:19429 [D loss: 0.315495, acc.: 88.28%] [G loss: 3.613839]\n",
      "epoch:24 step:19430 [D loss: 0.285212, acc.: 86.72%] [G loss: 2.644087]\n",
      "epoch:24 step:19431 [D loss: 0.367364, acc.: 83.59%] [G loss: 4.557824]\n",
      "epoch:24 step:19432 [D loss: 0.510131, acc.: 79.69%] [G loss: 4.139555]\n",
      "epoch:24 step:19433 [D loss: 0.437780, acc.: 82.03%] [G loss: 3.591742]\n",
      "epoch:24 step:19434 [D loss: 0.357194, acc.: 86.72%] [G loss: 7.216524]\n",
      "epoch:24 step:19435 [D loss: 0.222107, acc.: 90.62%] [G loss: 4.902684]\n",
      "epoch:24 step:19436 [D loss: 0.433470, acc.: 78.91%] [G loss: 5.900949]\n",
      "epoch:24 step:19437 [D loss: 0.614466, acc.: 73.44%] [G loss: 2.932478]\n",
      "epoch:24 step:19438 [D loss: 0.231073, acc.: 87.50%] [G loss: 4.527810]\n",
      "epoch:24 step:19439 [D loss: 0.211895, acc.: 90.62%] [G loss: 4.434259]\n",
      "epoch:24 step:19440 [D loss: 0.308805, acc.: 86.72%] [G loss: 3.535344]\n",
      "epoch:24 step:19441 [D loss: 0.282973, acc.: 89.06%] [G loss: 2.871821]\n",
      "epoch:24 step:19442 [D loss: 0.270756, acc.: 89.06%] [G loss: 3.088556]\n",
      "epoch:24 step:19443 [D loss: 0.232968, acc.: 89.06%] [G loss: 4.297450]\n",
      "epoch:24 step:19444 [D loss: 0.216630, acc.: 91.41%] [G loss: 3.020738]\n",
      "epoch:24 step:19445 [D loss: 0.217350, acc.: 89.84%] [G loss: 3.429307]\n",
      "epoch:24 step:19446 [D loss: 0.269388, acc.: 89.84%] [G loss: 3.276128]\n",
      "epoch:24 step:19447 [D loss: 0.335890, acc.: 86.72%] [G loss: 4.888855]\n",
      "epoch:24 step:19448 [D loss: 0.211816, acc.: 90.62%] [G loss: 3.714880]\n",
      "epoch:24 step:19449 [D loss: 0.226211, acc.: 92.19%] [G loss: 3.083579]\n",
      "epoch:24 step:19450 [D loss: 0.252365, acc.: 92.19%] [G loss: 3.627589]\n",
      "epoch:24 step:19451 [D loss: 0.263650, acc.: 86.72%] [G loss: 5.356625]\n",
      "epoch:24 step:19452 [D loss: 0.363901, acc.: 85.94%] [G loss: 3.795937]\n",
      "epoch:24 step:19453 [D loss: 0.406579, acc.: 83.59%] [G loss: 3.487680]\n",
      "epoch:24 step:19454 [D loss: 0.353675, acc.: 84.38%] [G loss: 3.826554]\n",
      "epoch:24 step:19455 [D loss: 0.378987, acc.: 80.47%] [G loss: 3.085301]\n",
      "epoch:24 step:19456 [D loss: 0.288517, acc.: 86.72%] [G loss: 3.943884]\n",
      "epoch:24 step:19457 [D loss: 0.359855, acc.: 82.03%] [G loss: 3.199572]\n",
      "epoch:24 step:19458 [D loss: 0.319476, acc.: 85.94%] [G loss: 3.611881]\n",
      "epoch:24 step:19459 [D loss: 0.358288, acc.: 82.81%] [G loss: 3.513308]\n",
      "epoch:24 step:19460 [D loss: 0.318918, acc.: 84.38%] [G loss: 3.201222]\n",
      "epoch:24 step:19461 [D loss: 0.221086, acc.: 92.19%] [G loss: 3.565088]\n",
      "epoch:24 step:19462 [D loss: 0.294471, acc.: 87.50%] [G loss: 2.522213]\n",
      "epoch:24 step:19463 [D loss: 0.325092, acc.: 82.81%] [G loss: 2.406350]\n",
      "epoch:24 step:19464 [D loss: 0.271324, acc.: 87.50%] [G loss: 4.529625]\n",
      "epoch:24 step:19465 [D loss: 0.229552, acc.: 90.62%] [G loss: 4.455567]\n",
      "epoch:24 step:19466 [D loss: 0.318174, acc.: 86.72%] [G loss: 4.000852]\n",
      "epoch:24 step:19467 [D loss: 0.278238, acc.: 91.41%] [G loss: 3.804009]\n",
      "epoch:24 step:19468 [D loss: 0.231571, acc.: 91.41%] [G loss: 3.645744]\n",
      "epoch:24 step:19469 [D loss: 0.194771, acc.: 92.19%] [G loss: 5.881314]\n",
      "epoch:24 step:19470 [D loss: 0.298743, acc.: 86.72%] [G loss: 3.601034]\n",
      "epoch:24 step:19471 [D loss: 0.257303, acc.: 87.50%] [G loss: 4.492432]\n",
      "epoch:24 step:19472 [D loss: 0.357619, acc.: 82.81%] [G loss: 3.345762]\n",
      "epoch:24 step:19473 [D loss: 0.486400, acc.: 78.12%] [G loss: 3.679950]\n",
      "epoch:24 step:19474 [D loss: 0.468006, acc.: 77.34%] [G loss: 4.151162]\n",
      "epoch:24 step:19475 [D loss: 0.415470, acc.: 79.69%] [G loss: 3.801107]\n",
      "epoch:24 step:19476 [D loss: 0.401919, acc.: 85.16%] [G loss: 7.908309]\n",
      "epoch:24 step:19477 [D loss: 0.389689, acc.: 85.16%] [G loss: 5.364998]\n",
      "epoch:24 step:19478 [D loss: 0.385351, acc.: 78.91%] [G loss: 5.996628]\n",
      "epoch:24 step:19479 [D loss: 0.410890, acc.: 84.38%] [G loss: 4.311021]\n",
      "epoch:24 step:19480 [D loss: 0.335072, acc.: 85.16%] [G loss: 4.079312]\n",
      "epoch:24 step:19481 [D loss: 0.234266, acc.: 89.06%] [G loss: 4.498664]\n",
      "epoch:24 step:19482 [D loss: 0.264865, acc.: 89.84%] [G loss: 4.301561]\n",
      "epoch:24 step:19483 [D loss: 0.273357, acc.: 87.50%] [G loss: 5.987255]\n",
      "epoch:24 step:19484 [D loss: 0.281963, acc.: 84.38%] [G loss: 3.650026]\n",
      "epoch:24 step:19485 [D loss: 0.323524, acc.: 85.16%] [G loss: 4.533481]\n",
      "epoch:24 step:19486 [D loss: 0.384265, acc.: 82.03%] [G loss: 4.403507]\n",
      "epoch:24 step:19487 [D loss: 0.182649, acc.: 95.31%] [G loss: 5.254085]\n",
      "epoch:24 step:19488 [D loss: 0.293699, acc.: 87.50%] [G loss: 3.844670]\n",
      "epoch:24 step:19489 [D loss: 0.384125, acc.: 80.47%] [G loss: 2.810115]\n",
      "epoch:24 step:19490 [D loss: 0.236091, acc.: 89.84%] [G loss: 2.987308]\n",
      "epoch:24 step:19491 [D loss: 0.370019, acc.: 86.72%] [G loss: 3.270273]\n",
      "epoch:24 step:19492 [D loss: 0.263463, acc.: 91.41%] [G loss: 3.859640]\n",
      "epoch:24 step:19493 [D loss: 0.221810, acc.: 89.84%] [G loss: 3.942095]\n",
      "epoch:24 step:19494 [D loss: 0.324899, acc.: 84.38%] [G loss: 3.090831]\n",
      "epoch:24 step:19495 [D loss: 0.242045, acc.: 90.62%] [G loss: 3.814016]\n",
      "epoch:24 step:19496 [D loss: 0.222096, acc.: 91.41%] [G loss: 3.096410]\n",
      "epoch:24 step:19497 [D loss: 0.416451, acc.: 78.12%] [G loss: 3.395190]\n",
      "epoch:24 step:19498 [D loss: 0.341510, acc.: 80.47%] [G loss: 4.946817]\n",
      "epoch:24 step:19499 [D loss: 0.329634, acc.: 85.16%] [G loss: 2.533130]\n",
      "epoch:24 step:19500 [D loss: 0.237422, acc.: 90.62%] [G loss: 5.396850]\n",
      "epoch:24 step:19501 [D loss: 0.246908, acc.: 89.84%] [G loss: 2.519258]\n",
      "epoch:24 step:19502 [D loss: 0.292265, acc.: 85.94%] [G loss: 4.840817]\n",
      "epoch:24 step:19503 [D loss: 0.301017, acc.: 82.81%] [G loss: 2.919870]\n",
      "epoch:24 step:19504 [D loss: 0.245875, acc.: 88.28%] [G loss: 3.142994]\n",
      "epoch:24 step:19505 [D loss: 0.186764, acc.: 92.97%] [G loss: 4.196061]\n",
      "epoch:24 step:19506 [D loss: 0.311567, acc.: 86.72%] [G loss: 3.028272]\n",
      "epoch:24 step:19507 [D loss: 0.356487, acc.: 84.38%] [G loss: 2.244835]\n",
      "epoch:24 step:19508 [D loss: 0.304434, acc.: 85.94%] [G loss: 2.474669]\n",
      "epoch:24 step:19509 [D loss: 0.327371, acc.: 83.59%] [G loss: 2.948700]\n",
      "epoch:24 step:19510 [D loss: 0.345547, acc.: 86.72%] [G loss: 3.013539]\n",
      "epoch:24 step:19511 [D loss: 0.249649, acc.: 89.06%] [G loss: 3.167684]\n",
      "epoch:24 step:19512 [D loss: 0.247965, acc.: 88.28%] [G loss: 3.662110]\n",
      "epoch:24 step:19513 [D loss: 0.262317, acc.: 91.41%] [G loss: 3.699955]\n",
      "epoch:24 step:19514 [D loss: 0.395048, acc.: 83.59%] [G loss: 3.969885]\n",
      "epoch:24 step:19515 [D loss: 0.305530, acc.: 87.50%] [G loss: 3.040600]\n",
      "epoch:24 step:19516 [D loss: 0.319438, acc.: 84.38%] [G loss: 2.807840]\n",
      "epoch:24 step:19517 [D loss: 0.303289, acc.: 88.28%] [G loss: 2.386265]\n",
      "epoch:24 step:19518 [D loss: 0.355340, acc.: 86.72%] [G loss: 2.651073]\n",
      "epoch:24 step:19519 [D loss: 0.235490, acc.: 91.41%] [G loss: 3.555748]\n",
      "epoch:24 step:19520 [D loss: 0.253897, acc.: 88.28%] [G loss: 3.639400]\n",
      "epoch:24 step:19521 [D loss: 0.444670, acc.: 78.12%] [G loss: 2.815413]\n",
      "epoch:24 step:19522 [D loss: 0.361496, acc.: 86.72%] [G loss: 3.099213]\n",
      "epoch:24 step:19523 [D loss: 0.255268, acc.: 89.06%] [G loss: 4.312742]\n",
      "epoch:24 step:19524 [D loss: 0.315365, acc.: 88.28%] [G loss: 4.416690]\n",
      "epoch:24 step:19525 [D loss: 0.280102, acc.: 87.50%] [G loss: 4.077966]\n",
      "epoch:25 step:19526 [D loss: 0.257487, acc.: 90.62%] [G loss: 3.121792]\n",
      "epoch:25 step:19527 [D loss: 0.221549, acc.: 90.62%] [G loss: 2.954513]\n",
      "epoch:25 step:19528 [D loss: 0.207099, acc.: 93.75%] [G loss: 3.934246]\n",
      "epoch:25 step:19529 [D loss: 0.360184, acc.: 84.38%] [G loss: 3.534002]\n",
      "epoch:25 step:19530 [D loss: 0.267708, acc.: 89.84%] [G loss: 3.137023]\n",
      "epoch:25 step:19531 [D loss: 0.404971, acc.: 79.69%] [G loss: 2.870302]\n",
      "epoch:25 step:19532 [D loss: 0.219121, acc.: 89.06%] [G loss: 3.608561]\n",
      "epoch:25 step:19533 [D loss: 0.264835, acc.: 90.62%] [G loss: 3.272562]\n",
      "epoch:25 step:19534 [D loss: 0.308273, acc.: 88.28%] [G loss: 3.312158]\n",
      "epoch:25 step:19535 [D loss: 0.278635, acc.: 89.84%] [G loss: 3.701463]\n",
      "epoch:25 step:19536 [D loss: 0.333324, acc.: 85.94%] [G loss: 3.405392]\n",
      "epoch:25 step:19537 [D loss: 0.347765, acc.: 89.06%] [G loss: 3.291972]\n",
      "epoch:25 step:19538 [D loss: 0.293043, acc.: 90.62%] [G loss: 2.962100]\n",
      "epoch:25 step:19539 [D loss: 0.350402, acc.: 82.03%] [G loss: 2.465214]\n",
      "epoch:25 step:19540 [D loss: 0.361209, acc.: 82.81%] [G loss: 3.720469]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:25 step:19541 [D loss: 0.374990, acc.: 83.59%] [G loss: 3.143362]\n",
      "epoch:25 step:19542 [D loss: 0.264894, acc.: 89.84%] [G loss: 3.636854]\n",
      "epoch:25 step:19543 [D loss: 0.399301, acc.: 82.03%] [G loss: 2.979388]\n",
      "epoch:25 step:19544 [D loss: 0.398468, acc.: 84.38%] [G loss: 3.563350]\n",
      "epoch:25 step:19545 [D loss: 0.320062, acc.: 86.72%] [G loss: 2.804562]\n",
      "epoch:25 step:19546 [D loss: 0.493512, acc.: 71.88%] [G loss: 2.812529]\n",
      "epoch:25 step:19547 [D loss: 0.427689, acc.: 82.03%] [G loss: 4.553603]\n",
      "epoch:25 step:19548 [D loss: 0.386537, acc.: 84.38%] [G loss: 6.611969]\n",
      "epoch:25 step:19549 [D loss: 0.378415, acc.: 82.81%] [G loss: 4.322886]\n",
      "epoch:25 step:19550 [D loss: 0.326655, acc.: 87.50%] [G loss: 3.383260]\n",
      "epoch:25 step:19551 [D loss: 0.260836, acc.: 91.41%] [G loss: 3.864102]\n",
      "epoch:25 step:19552 [D loss: 0.334371, acc.: 85.94%] [G loss: 4.984076]\n",
      "epoch:25 step:19553 [D loss: 0.409449, acc.: 82.03%] [G loss: 3.463155]\n",
      "epoch:25 step:19554 [D loss: 0.247424, acc.: 90.62%] [G loss: 5.064633]\n",
      "epoch:25 step:19555 [D loss: 0.294351, acc.: 85.94%] [G loss: 3.413505]\n",
      "epoch:25 step:19556 [D loss: 0.278674, acc.: 88.28%] [G loss: 3.164735]\n",
      "epoch:25 step:19557 [D loss: 0.297344, acc.: 86.72%] [G loss: 4.009972]\n",
      "epoch:25 step:19558 [D loss: 0.277100, acc.: 89.84%] [G loss: 3.773363]\n",
      "epoch:25 step:19559 [D loss: 0.430665, acc.: 82.81%] [G loss: 4.012745]\n",
      "epoch:25 step:19560 [D loss: 0.245880, acc.: 85.94%] [G loss: 4.929085]\n",
      "epoch:25 step:19561 [D loss: 0.256002, acc.: 89.06%] [G loss: 3.590145]\n",
      "epoch:25 step:19562 [D loss: 0.336348, acc.: 82.81%] [G loss: 3.682097]\n",
      "epoch:25 step:19563 [D loss: 0.255185, acc.: 92.97%] [G loss: 3.036561]\n",
      "epoch:25 step:19564 [D loss: 0.272637, acc.: 86.72%] [G loss: 3.073823]\n",
      "epoch:25 step:19565 [D loss: 0.299421, acc.: 89.84%] [G loss: 3.443449]\n",
      "epoch:25 step:19566 [D loss: 0.277325, acc.: 84.38%] [G loss: 3.436796]\n",
      "epoch:25 step:19567 [D loss: 0.249447, acc.: 89.84%] [G loss: 3.896785]\n",
      "epoch:25 step:19568 [D loss: 0.245873, acc.: 88.28%] [G loss: 3.933592]\n",
      "epoch:25 step:19569 [D loss: 0.277998, acc.: 87.50%] [G loss: 5.277458]\n",
      "epoch:25 step:19570 [D loss: 0.370210, acc.: 82.03%] [G loss: 3.835155]\n",
      "epoch:25 step:19571 [D loss: 0.414254, acc.: 82.03%] [G loss: 4.764590]\n",
      "epoch:25 step:19572 [D loss: 0.439781, acc.: 83.59%] [G loss: 4.626050]\n",
      "epoch:25 step:19573 [D loss: 0.325404, acc.: 85.94%] [G loss: 3.139736]\n",
      "epoch:25 step:19574 [D loss: 0.249503, acc.: 89.84%] [G loss: 3.602816]\n",
      "epoch:25 step:19575 [D loss: 0.340823, acc.: 84.38%] [G loss: 3.114997]\n",
      "epoch:25 step:19576 [D loss: 0.303547, acc.: 84.38%] [G loss: 4.752785]\n",
      "epoch:25 step:19577 [D loss: 0.298315, acc.: 89.06%] [G loss: 4.502156]\n",
      "epoch:25 step:19578 [D loss: 0.160261, acc.: 92.97%] [G loss: 5.249921]\n",
      "epoch:25 step:19579 [D loss: 0.275303, acc.: 87.50%] [G loss: 4.196201]\n",
      "epoch:25 step:19580 [D loss: 0.329672, acc.: 86.72%] [G loss: 3.298484]\n",
      "epoch:25 step:19581 [D loss: 0.330411, acc.: 85.94%] [G loss: 3.740887]\n",
      "epoch:25 step:19582 [D loss: 0.325361, acc.: 80.47%] [G loss: 4.285474]\n",
      "epoch:25 step:19583 [D loss: 0.300417, acc.: 86.72%] [G loss: 4.689463]\n",
      "epoch:25 step:19584 [D loss: 0.323576, acc.: 82.81%] [G loss: 4.081446]\n",
      "epoch:25 step:19585 [D loss: 0.304978, acc.: 88.28%] [G loss: 7.805551]\n",
      "epoch:25 step:19586 [D loss: 0.260690, acc.: 87.50%] [G loss: 5.046584]\n",
      "epoch:25 step:19587 [D loss: 0.233682, acc.: 92.19%] [G loss: 3.957998]\n",
      "epoch:25 step:19588 [D loss: 0.250993, acc.: 89.84%] [G loss: 3.381445]\n",
      "epoch:25 step:19589 [D loss: 0.301971, acc.: 86.72%] [G loss: 4.103012]\n",
      "epoch:25 step:19590 [D loss: 0.434460, acc.: 78.91%] [G loss: 2.802407]\n",
      "epoch:25 step:19591 [D loss: 0.318585, acc.: 85.16%] [G loss: 2.588253]\n",
      "epoch:25 step:19592 [D loss: 0.319138, acc.: 84.38%] [G loss: 3.546897]\n",
      "epoch:25 step:19593 [D loss: 0.310226, acc.: 83.59%] [G loss: 2.681976]\n",
      "epoch:25 step:19594 [D loss: 0.306691, acc.: 82.81%] [G loss: 3.505869]\n",
      "epoch:25 step:19595 [D loss: 0.394858, acc.: 85.16%] [G loss: 3.327291]\n",
      "epoch:25 step:19596 [D loss: 0.357783, acc.: 83.59%] [G loss: 3.361330]\n",
      "epoch:25 step:19597 [D loss: 0.299630, acc.: 86.72%] [G loss: 6.625871]\n",
      "epoch:25 step:19598 [D loss: 0.567148, acc.: 76.56%] [G loss: 8.271086]\n",
      "epoch:25 step:19599 [D loss: 1.329053, acc.: 62.50%] [G loss: 6.924582]\n",
      "epoch:25 step:19600 [D loss: 2.618852, acc.: 54.69%] [G loss: 4.267094]\n",
      "##############\n",
      "[0.85942579 0.86367692 0.79679782 0.81020207 0.77375335 0.81881675\n",
      " 0.86037847 0.8157699  0.82927607 0.82590687]\n",
      "##########\n",
      "epoch:25 step:19601 [D loss: 1.731569, acc.: 67.19%] [G loss: 10.700555]\n",
      "epoch:25 step:19602 [D loss: 2.064642, acc.: 59.38%] [G loss: 5.369026]\n",
      "epoch:25 step:19603 [D loss: 0.898416, acc.: 77.34%] [G loss: 5.597521]\n",
      "epoch:25 step:19604 [D loss: 0.397863, acc.: 82.03%] [G loss: 7.106174]\n",
      "epoch:25 step:19605 [D loss: 0.554491, acc.: 69.53%] [G loss: 4.252838]\n",
      "epoch:25 step:19606 [D loss: 0.260209, acc.: 86.72%] [G loss: 5.989694]\n",
      "epoch:25 step:19607 [D loss: 0.329766, acc.: 83.59%] [G loss: 5.257347]\n",
      "epoch:25 step:19608 [D loss: 0.270429, acc.: 87.50%] [G loss: 4.454444]\n",
      "epoch:25 step:19609 [D loss: 0.242849, acc.: 90.62%] [G loss: 3.130136]\n",
      "epoch:25 step:19610 [D loss: 0.388707, acc.: 81.25%] [G loss: 2.831437]\n",
      "epoch:25 step:19611 [D loss: 0.320988, acc.: 86.72%] [G loss: 3.200684]\n",
      "epoch:25 step:19612 [D loss: 0.304371, acc.: 88.28%] [G loss: 3.076758]\n",
      "epoch:25 step:19613 [D loss: 0.221688, acc.: 91.41%] [G loss: 3.868250]\n",
      "epoch:25 step:19614 [D loss: 0.435206, acc.: 77.34%] [G loss: 2.834464]\n",
      "epoch:25 step:19615 [D loss: 0.342601, acc.: 82.81%] [G loss: 3.414730]\n",
      "epoch:25 step:19616 [D loss: 0.227712, acc.: 89.06%] [G loss: 3.153172]\n",
      "epoch:25 step:19617 [D loss: 0.366685, acc.: 87.50%] [G loss: 3.079597]\n",
      "epoch:25 step:19618 [D loss: 0.316151, acc.: 84.38%] [G loss: 2.820009]\n",
      "epoch:25 step:19619 [D loss: 0.327230, acc.: 85.94%] [G loss: 2.922070]\n",
      "epoch:25 step:19620 [D loss: 0.271251, acc.: 89.84%] [G loss: 2.927091]\n",
      "epoch:25 step:19621 [D loss: 0.354666, acc.: 82.03%] [G loss: 3.221087]\n",
      "epoch:25 step:19622 [D loss: 0.303333, acc.: 88.28%] [G loss: 3.909794]\n",
      "epoch:25 step:19623 [D loss: 0.399441, acc.: 83.59%] [G loss: 2.645001]\n",
      "epoch:25 step:19624 [D loss: 0.342261, acc.: 82.03%] [G loss: 4.179778]\n",
      "epoch:25 step:19625 [D loss: 0.337933, acc.: 82.03%] [G loss: 2.860852]\n",
      "epoch:25 step:19626 [D loss: 0.431424, acc.: 78.12%] [G loss: 4.642972]\n",
      "epoch:25 step:19627 [D loss: 0.317498, acc.: 88.28%] [G loss: 2.543490]\n",
      "epoch:25 step:19628 [D loss: 0.289267, acc.: 89.06%] [G loss: 2.660963]\n",
      "epoch:25 step:19629 [D loss: 0.302301, acc.: 87.50%] [G loss: 3.471630]\n",
      "epoch:25 step:19630 [D loss: 0.329886, acc.: 86.72%] [G loss: 2.607240]\n",
      "epoch:25 step:19631 [D loss: 0.330867, acc.: 82.03%] [G loss: 3.793408]\n",
      "epoch:25 step:19632 [D loss: 0.254953, acc.: 87.50%] [G loss: 3.523627]\n",
      "epoch:25 step:19633 [D loss: 0.310688, acc.: 85.94%] [G loss: 2.994599]\n",
      "epoch:25 step:19634 [D loss: 0.252111, acc.: 88.28%] [G loss: 3.495161]\n",
      "epoch:25 step:19635 [D loss: 0.372427, acc.: 84.38%] [G loss: 2.533175]\n",
      "epoch:25 step:19636 [D loss: 0.367117, acc.: 84.38%] [G loss: 2.456684]\n",
      "epoch:25 step:19637 [D loss: 0.300580, acc.: 89.06%] [G loss: 3.287859]\n",
      "epoch:25 step:19638 [D loss: 0.395250, acc.: 82.81%] [G loss: 2.850172]\n",
      "epoch:25 step:19639 [D loss: 0.354033, acc.: 82.81%] [G loss: 3.110131]\n",
      "epoch:25 step:19640 [D loss: 0.303691, acc.: 87.50%] [G loss: 3.570356]\n",
      "epoch:25 step:19641 [D loss: 0.403418, acc.: 81.25%] [G loss: 3.978665]\n",
      "epoch:25 step:19642 [D loss: 0.264446, acc.: 89.84%] [G loss: 4.513829]\n",
      "epoch:25 step:19643 [D loss: 0.351523, acc.: 83.59%] [G loss: 3.387267]\n",
      "epoch:25 step:19644 [D loss: 0.362379, acc.: 85.16%] [G loss: 3.394591]\n",
      "epoch:25 step:19645 [D loss: 0.257525, acc.: 89.84%] [G loss: 2.474453]\n",
      "epoch:25 step:19646 [D loss: 0.338606, acc.: 85.94%] [G loss: 2.518156]\n",
      "epoch:25 step:19647 [D loss: 0.314537, acc.: 85.16%] [G loss: 2.849265]\n",
      "epoch:25 step:19648 [D loss: 0.264266, acc.: 89.06%] [G loss: 3.418228]\n",
      "epoch:25 step:19649 [D loss: 0.427747, acc.: 84.38%] [G loss: 2.941406]\n",
      "epoch:25 step:19650 [D loss: 0.567888, acc.: 71.88%] [G loss: 3.774479]\n",
      "epoch:25 step:19651 [D loss: 0.400066, acc.: 83.59%] [G loss: 3.344507]\n",
      "epoch:25 step:19652 [D loss: 0.231716, acc.: 91.41%] [G loss: 4.502693]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:25 step:19653 [D loss: 0.302144, acc.: 85.94%] [G loss: 4.103170]\n",
      "epoch:25 step:19654 [D loss: 0.316546, acc.: 85.94%] [G loss: 3.228323]\n",
      "epoch:25 step:19655 [D loss: 0.262936, acc.: 89.06%] [G loss: 3.573715]\n",
      "epoch:25 step:19656 [D loss: 0.337785, acc.: 83.59%] [G loss: 3.300592]\n",
      "epoch:25 step:19657 [D loss: 0.412931, acc.: 77.34%] [G loss: 3.941630]\n",
      "epoch:25 step:19658 [D loss: 0.422695, acc.: 79.69%] [G loss: 2.961056]\n",
      "epoch:25 step:19659 [D loss: 0.378141, acc.: 84.38%] [G loss: 3.384876]\n",
      "epoch:25 step:19660 [D loss: 0.282229, acc.: 91.41%] [G loss: 3.193878]\n",
      "epoch:25 step:19661 [D loss: 0.191327, acc.: 91.41%] [G loss: 3.647354]\n",
      "epoch:25 step:19662 [D loss: 0.355862, acc.: 84.38%] [G loss: 3.555656]\n",
      "epoch:25 step:19663 [D loss: 0.420186, acc.: 81.25%] [G loss: 2.642242]\n",
      "epoch:25 step:19664 [D loss: 0.316281, acc.: 85.16%] [G loss: 3.934687]\n",
      "epoch:25 step:19665 [D loss: 0.326190, acc.: 85.94%] [G loss: 2.410028]\n",
      "epoch:25 step:19666 [D loss: 0.370504, acc.: 84.38%] [G loss: 2.669460]\n",
      "epoch:25 step:19667 [D loss: 0.320691, acc.: 88.28%] [G loss: 3.658841]\n",
      "epoch:25 step:19668 [D loss: 0.223203, acc.: 92.97%] [G loss: 3.886632]\n",
      "epoch:25 step:19669 [D loss: 0.355340, acc.: 83.59%] [G loss: 2.888402]\n",
      "epoch:25 step:19670 [D loss: 0.288772, acc.: 86.72%] [G loss: 3.624716]\n",
      "epoch:25 step:19671 [D loss: 0.301850, acc.: 86.72%] [G loss: 2.626992]\n",
      "epoch:25 step:19672 [D loss: 0.345111, acc.: 85.16%] [G loss: 2.661424]\n",
      "epoch:25 step:19673 [D loss: 0.321907, acc.: 85.94%] [G loss: 3.064802]\n",
      "epoch:25 step:19674 [D loss: 0.273987, acc.: 87.50%] [G loss: 2.965277]\n",
      "epoch:25 step:19675 [D loss: 0.261191, acc.: 90.62%] [G loss: 2.642256]\n",
      "epoch:25 step:19676 [D loss: 0.261067, acc.: 89.06%] [G loss: 2.890026]\n",
      "epoch:25 step:19677 [D loss: 0.392676, acc.: 84.38%] [G loss: 3.055891]\n",
      "epoch:25 step:19678 [D loss: 0.331883, acc.: 85.16%] [G loss: 3.362523]\n",
      "epoch:25 step:19679 [D loss: 0.385558, acc.: 82.03%] [G loss: 2.528519]\n",
      "epoch:25 step:19680 [D loss: 0.370885, acc.: 83.59%] [G loss: 2.420330]\n",
      "epoch:25 step:19681 [D loss: 0.351442, acc.: 84.38%] [G loss: 2.637603]\n",
      "epoch:25 step:19682 [D loss: 0.354747, acc.: 80.47%] [G loss: 2.637417]\n",
      "epoch:25 step:19683 [D loss: 0.325769, acc.: 84.38%] [G loss: 3.147250]\n",
      "epoch:25 step:19684 [D loss: 0.398388, acc.: 83.59%] [G loss: 2.328013]\n",
      "epoch:25 step:19685 [D loss: 0.419481, acc.: 82.81%] [G loss: 2.814775]\n",
      "epoch:25 step:19686 [D loss: 0.337298, acc.: 87.50%] [G loss: 2.487468]\n",
      "epoch:25 step:19687 [D loss: 0.366682, acc.: 80.47%] [G loss: 2.213384]\n",
      "epoch:25 step:19688 [D loss: 0.292244, acc.: 87.50%] [G loss: 2.243257]\n",
      "epoch:25 step:19689 [D loss: 0.287745, acc.: 89.06%] [G loss: 3.126256]\n",
      "epoch:25 step:19690 [D loss: 0.402053, acc.: 82.81%] [G loss: 3.792361]\n",
      "epoch:25 step:19691 [D loss: 0.259105, acc.: 88.28%] [G loss: 3.913128]\n",
      "epoch:25 step:19692 [D loss: 0.249274, acc.: 90.62%] [G loss: 3.137890]\n",
      "epoch:25 step:19693 [D loss: 0.375354, acc.: 84.38%] [G loss: 2.949270]\n",
      "epoch:25 step:19694 [D loss: 0.355247, acc.: 84.38%] [G loss: 2.818116]\n",
      "epoch:25 step:19695 [D loss: 0.294956, acc.: 85.94%] [G loss: 2.622088]\n",
      "epoch:25 step:19696 [D loss: 0.313201, acc.: 86.72%] [G loss: 3.078258]\n",
      "epoch:25 step:19697 [D loss: 0.361410, acc.: 82.81%] [G loss: 2.904875]\n",
      "epoch:25 step:19698 [D loss: 0.287588, acc.: 88.28%] [G loss: 3.285132]\n",
      "epoch:25 step:19699 [D loss: 0.365796, acc.: 86.72%] [G loss: 2.068374]\n",
      "epoch:25 step:19700 [D loss: 0.385879, acc.: 78.91%] [G loss: 2.240351]\n",
      "epoch:25 step:19701 [D loss: 0.236726, acc.: 91.41%] [G loss: 2.418514]\n",
      "epoch:25 step:19702 [D loss: 0.415230, acc.: 80.47%] [G loss: 2.226631]\n",
      "epoch:25 step:19703 [D loss: 0.366086, acc.: 86.72%] [G loss: 2.385420]\n",
      "epoch:25 step:19704 [D loss: 0.311426, acc.: 89.06%] [G loss: 2.682160]\n",
      "epoch:25 step:19705 [D loss: 0.356279, acc.: 84.38%] [G loss: 2.566622]\n",
      "epoch:25 step:19706 [D loss: 0.313295, acc.: 84.38%] [G loss: 2.625114]\n",
      "epoch:25 step:19707 [D loss: 0.456535, acc.: 78.91%] [G loss: 2.527057]\n",
      "epoch:25 step:19708 [D loss: 0.301158, acc.: 83.59%] [G loss: 2.866168]\n",
      "epoch:25 step:19709 [D loss: 0.355374, acc.: 82.03%] [G loss: 3.106383]\n",
      "epoch:25 step:19710 [D loss: 0.170652, acc.: 94.53%] [G loss: 3.398534]\n",
      "epoch:25 step:19711 [D loss: 0.297615, acc.: 88.28%] [G loss: 2.770292]\n",
      "epoch:25 step:19712 [D loss: 0.364768, acc.: 82.03%] [G loss: 4.808594]\n",
      "epoch:25 step:19713 [D loss: 0.408601, acc.: 82.81%] [G loss: 5.639927]\n",
      "epoch:25 step:19714 [D loss: 0.358498, acc.: 86.72%] [G loss: 3.908174]\n",
      "epoch:25 step:19715 [D loss: 0.273926, acc.: 88.28%] [G loss: 4.068029]\n",
      "epoch:25 step:19716 [D loss: 0.430849, acc.: 81.25%] [G loss: 2.496916]\n",
      "epoch:25 step:19717 [D loss: 0.311886, acc.: 85.16%] [G loss: 3.018178]\n",
      "epoch:25 step:19718 [D loss: 0.240124, acc.: 89.84%] [G loss: 2.675719]\n",
      "epoch:25 step:19719 [D loss: 0.347747, acc.: 85.94%] [G loss: 3.255055]\n",
      "epoch:25 step:19720 [D loss: 0.235076, acc.: 91.41%] [G loss: 2.445367]\n",
      "epoch:25 step:19721 [D loss: 0.258323, acc.: 90.62%] [G loss: 2.439086]\n",
      "epoch:25 step:19722 [D loss: 0.330284, acc.: 86.72%] [G loss: 2.841918]\n",
      "epoch:25 step:19723 [D loss: 0.365799, acc.: 87.50%] [G loss: 2.143287]\n",
      "epoch:25 step:19724 [D loss: 0.325548, acc.: 89.06%] [G loss: 3.305390]\n",
      "epoch:25 step:19725 [D loss: 0.326583, acc.: 84.38%] [G loss: 3.293240]\n",
      "epoch:25 step:19726 [D loss: 0.312186, acc.: 86.72%] [G loss: 2.474337]\n",
      "epoch:25 step:19727 [D loss: 0.346699, acc.: 83.59%] [G loss: 4.391796]\n",
      "epoch:25 step:19728 [D loss: 0.381650, acc.: 81.25%] [G loss: 3.120775]\n",
      "epoch:25 step:19729 [D loss: 0.407668, acc.: 85.16%] [G loss: 5.244767]\n",
      "epoch:25 step:19730 [D loss: 0.242932, acc.: 93.75%] [G loss: 3.432598]\n",
      "epoch:25 step:19731 [D loss: 0.387204, acc.: 84.38%] [G loss: 3.104963]\n",
      "epoch:25 step:19732 [D loss: 0.267275, acc.: 85.16%] [G loss: 3.327532]\n",
      "epoch:25 step:19733 [D loss: 0.385743, acc.: 81.25%] [G loss: 4.464749]\n",
      "epoch:25 step:19734 [D loss: 0.464396, acc.: 77.34%] [G loss: 3.422389]\n",
      "epoch:25 step:19735 [D loss: 0.320723, acc.: 85.94%] [G loss: 3.103149]\n",
      "epoch:25 step:19736 [D loss: 0.355318, acc.: 82.81%] [G loss: 2.500481]\n",
      "epoch:25 step:19737 [D loss: 0.291951, acc.: 88.28%] [G loss: 2.738836]\n",
      "epoch:25 step:19738 [D loss: 0.314379, acc.: 85.16%] [G loss: 3.111676]\n",
      "epoch:25 step:19739 [D loss: 0.399000, acc.: 78.91%] [G loss: 2.470243]\n",
      "epoch:25 step:19740 [D loss: 0.310557, acc.: 86.72%] [G loss: 2.688614]\n",
      "epoch:25 step:19741 [D loss: 0.316655, acc.: 85.16%] [G loss: 3.079206]\n",
      "epoch:25 step:19742 [D loss: 0.329937, acc.: 89.84%] [G loss: 2.878440]\n",
      "epoch:25 step:19743 [D loss: 0.249938, acc.: 89.06%] [G loss: 3.773586]\n",
      "epoch:25 step:19744 [D loss: 0.354530, acc.: 82.81%] [G loss: 4.377315]\n",
      "epoch:25 step:19745 [D loss: 0.313190, acc.: 85.16%] [G loss: 3.635229]\n",
      "epoch:25 step:19746 [D loss: 0.257519, acc.: 90.62%] [G loss: 2.792196]\n",
      "epoch:25 step:19747 [D loss: 0.202165, acc.: 92.97%] [G loss: 3.215534]\n",
      "epoch:25 step:19748 [D loss: 0.284826, acc.: 88.28%] [G loss: 2.874720]\n",
      "epoch:25 step:19749 [D loss: 0.288462, acc.: 89.06%] [G loss: 3.634894]\n",
      "epoch:25 step:19750 [D loss: 0.299832, acc.: 84.38%] [G loss: 3.474817]\n",
      "epoch:25 step:19751 [D loss: 0.401521, acc.: 80.47%] [G loss: 3.029392]\n",
      "epoch:25 step:19752 [D loss: 0.253918, acc.: 88.28%] [G loss: 3.261829]\n",
      "epoch:25 step:19753 [D loss: 0.235594, acc.: 89.84%] [G loss: 6.180658]\n",
      "epoch:25 step:19754 [D loss: 0.327610, acc.: 84.38%] [G loss: 3.910984]\n",
      "epoch:25 step:19755 [D loss: 0.240630, acc.: 87.50%] [G loss: 2.947517]\n",
      "epoch:25 step:19756 [D loss: 0.237168, acc.: 90.62%] [G loss: 5.547535]\n",
      "epoch:25 step:19757 [D loss: 0.258267, acc.: 90.62%] [G loss: 3.885132]\n",
      "epoch:25 step:19758 [D loss: 0.293468, acc.: 85.94%] [G loss: 4.986747]\n",
      "epoch:25 step:19759 [D loss: 0.257526, acc.: 91.41%] [G loss: 3.057624]\n",
      "epoch:25 step:19760 [D loss: 0.351222, acc.: 84.38%] [G loss: 3.779697]\n",
      "epoch:25 step:19761 [D loss: 0.413467, acc.: 74.22%] [G loss: 2.702563]\n",
      "epoch:25 step:19762 [D loss: 0.336742, acc.: 84.38%] [G loss: 3.351672]\n",
      "epoch:25 step:19763 [D loss: 0.333430, acc.: 84.38%] [G loss: 3.263835]\n",
      "epoch:25 step:19764 [D loss: 0.356111, acc.: 81.25%] [G loss: 4.761380]\n",
      "epoch:25 step:19765 [D loss: 0.318704, acc.: 86.72%] [G loss: 3.860799]\n",
      "epoch:25 step:19766 [D loss: 0.354126, acc.: 80.47%] [G loss: 3.488464]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:25 step:19767 [D loss: 0.283214, acc.: 86.72%] [G loss: 3.534533]\n",
      "epoch:25 step:19768 [D loss: 0.276149, acc.: 85.16%] [G loss: 4.345465]\n",
      "epoch:25 step:19769 [D loss: 0.517887, acc.: 74.22%] [G loss: 3.295558]\n",
      "epoch:25 step:19770 [D loss: 0.222130, acc.: 92.19%] [G loss: 4.215178]\n",
      "epoch:25 step:19771 [D loss: 0.296192, acc.: 87.50%] [G loss: 2.562160]\n",
      "epoch:25 step:19772 [D loss: 0.320134, acc.: 85.94%] [G loss: 4.094825]\n",
      "epoch:25 step:19773 [D loss: 0.312753, acc.: 87.50%] [G loss: 3.409279]\n",
      "epoch:25 step:19774 [D loss: 0.371107, acc.: 83.59%] [G loss: 3.030147]\n",
      "epoch:25 step:19775 [D loss: 0.415356, acc.: 80.47%] [G loss: 2.659106]\n",
      "epoch:25 step:19776 [D loss: 0.303871, acc.: 84.38%] [G loss: 3.960151]\n",
      "epoch:25 step:19777 [D loss: 0.408859, acc.: 81.25%] [G loss: 4.710829]\n",
      "epoch:25 step:19778 [D loss: 0.264916, acc.: 90.62%] [G loss: 3.638714]\n",
      "epoch:25 step:19779 [D loss: 0.415207, acc.: 79.69%] [G loss: 4.364359]\n",
      "epoch:25 step:19780 [D loss: 0.388501, acc.: 79.69%] [G loss: 3.436081]\n",
      "epoch:25 step:19781 [D loss: 0.289093, acc.: 85.94%] [G loss: 3.634076]\n",
      "epoch:25 step:19782 [D loss: 0.392020, acc.: 80.47%] [G loss: 3.319781]\n",
      "epoch:25 step:19783 [D loss: 0.440635, acc.: 78.12%] [G loss: 3.121301]\n",
      "epoch:25 step:19784 [D loss: 0.380044, acc.: 83.59%] [G loss: 2.617414]\n",
      "epoch:25 step:19785 [D loss: 0.255642, acc.: 89.06%] [G loss: 3.159701]\n",
      "epoch:25 step:19786 [D loss: 0.393942, acc.: 83.59%] [G loss: 3.805303]\n",
      "epoch:25 step:19787 [D loss: 0.281962, acc.: 85.16%] [G loss: 4.018727]\n",
      "epoch:25 step:19788 [D loss: 0.273797, acc.: 87.50%] [G loss: 4.183652]\n",
      "epoch:25 step:19789 [D loss: 0.417475, acc.: 81.25%] [G loss: 2.372008]\n",
      "epoch:25 step:19790 [D loss: 0.394269, acc.: 77.34%] [G loss: 3.087283]\n",
      "epoch:25 step:19791 [D loss: 0.279380, acc.: 89.06%] [G loss: 3.546199]\n",
      "epoch:25 step:19792 [D loss: 0.297618, acc.: 85.16%] [G loss: 3.191768]\n",
      "epoch:25 step:19793 [D loss: 0.255288, acc.: 87.50%] [G loss: 3.334868]\n",
      "epoch:25 step:19794 [D loss: 0.323298, acc.: 85.94%] [G loss: 3.635146]\n",
      "epoch:25 step:19795 [D loss: 0.385816, acc.: 78.91%] [G loss: 3.744996]\n",
      "epoch:25 step:19796 [D loss: 0.372279, acc.: 83.59%] [G loss: 3.658511]\n",
      "epoch:25 step:19797 [D loss: 0.250264, acc.: 91.41%] [G loss: 3.264920]\n",
      "epoch:25 step:19798 [D loss: 0.304250, acc.: 84.38%] [G loss: 3.561515]\n",
      "epoch:25 step:19799 [D loss: 0.317733, acc.: 86.72%] [G loss: 3.459201]\n",
      "epoch:25 step:19800 [D loss: 0.415184, acc.: 82.03%] [G loss: 3.910546]\n",
      "##############\n",
      "[0.85896242 0.87093792 0.81196566 0.8143886  0.76164822 0.82686344\n",
      " 0.87048026 0.78853311 0.81648813 0.81111396]\n",
      "##########\n",
      "epoch:25 step:19801 [D loss: 0.368498, acc.: 82.81%] [G loss: 3.730647]\n",
      "epoch:25 step:19802 [D loss: 0.308317, acc.: 85.16%] [G loss: 5.780023]\n",
      "epoch:25 step:19803 [D loss: 0.320732, acc.: 82.81%] [G loss: 4.281248]\n",
      "epoch:25 step:19804 [D loss: 0.219673, acc.: 89.84%] [G loss: 4.548450]\n",
      "epoch:25 step:19805 [D loss: 0.270790, acc.: 90.62%] [G loss: 4.041657]\n",
      "epoch:25 step:19806 [D loss: 0.333915, acc.: 83.59%] [G loss: 3.441244]\n",
      "epoch:25 step:19807 [D loss: 0.225820, acc.: 92.97%] [G loss: 4.313033]\n",
      "epoch:25 step:19808 [D loss: 0.318167, acc.: 85.16%] [G loss: 4.635346]\n",
      "epoch:25 step:19809 [D loss: 0.214429, acc.: 92.97%] [G loss: 4.826323]\n",
      "epoch:25 step:19810 [D loss: 0.314131, acc.: 85.16%] [G loss: 4.048918]\n",
      "epoch:25 step:19811 [D loss: 0.317647, acc.: 85.16%] [G loss: 2.677225]\n",
      "epoch:25 step:19812 [D loss: 0.262514, acc.: 88.28%] [G loss: 3.334806]\n",
      "epoch:25 step:19813 [D loss: 0.318018, acc.: 86.72%] [G loss: 3.827971]\n",
      "epoch:25 step:19814 [D loss: 0.274675, acc.: 87.50%] [G loss: 4.140513]\n",
      "epoch:25 step:19815 [D loss: 0.350109, acc.: 85.94%] [G loss: 2.941443]\n",
      "epoch:25 step:19816 [D loss: 0.258579, acc.: 89.06%] [G loss: 5.299249]\n",
      "epoch:25 step:19817 [D loss: 0.255418, acc.: 88.28%] [G loss: 4.464746]\n",
      "epoch:25 step:19818 [D loss: 0.274413, acc.: 91.41%] [G loss: 5.042917]\n",
      "epoch:25 step:19819 [D loss: 0.399956, acc.: 83.59%] [G loss: 4.127831]\n",
      "epoch:25 step:19820 [D loss: 0.352136, acc.: 80.47%] [G loss: 3.700469]\n",
      "epoch:25 step:19821 [D loss: 0.305729, acc.: 83.59%] [G loss: 5.185616]\n",
      "epoch:25 step:19822 [D loss: 0.309797, acc.: 88.28%] [G loss: 4.192637]\n",
      "epoch:25 step:19823 [D loss: 0.325515, acc.: 89.84%] [G loss: 3.874941]\n",
      "epoch:25 step:19824 [D loss: 0.372974, acc.: 81.25%] [G loss: 2.552849]\n",
      "epoch:25 step:19825 [D loss: 0.383579, acc.: 84.38%] [G loss: 2.319661]\n",
      "epoch:25 step:19826 [D loss: 0.349822, acc.: 83.59%] [G loss: 2.687335]\n",
      "epoch:25 step:19827 [D loss: 0.379913, acc.: 78.91%] [G loss: 2.683441]\n",
      "epoch:25 step:19828 [D loss: 0.342720, acc.: 85.16%] [G loss: 2.569333]\n",
      "epoch:25 step:19829 [D loss: 0.283206, acc.: 86.72%] [G loss: 3.147057]\n",
      "epoch:25 step:19830 [D loss: 0.423532, acc.: 81.25%] [G loss: 3.374596]\n",
      "epoch:25 step:19831 [D loss: 0.289172, acc.: 84.38%] [G loss: 5.108558]\n",
      "epoch:25 step:19832 [D loss: 0.253662, acc.: 88.28%] [G loss: 4.030887]\n",
      "epoch:25 step:19833 [D loss: 0.264537, acc.: 88.28%] [G loss: 4.002045]\n",
      "epoch:25 step:19834 [D loss: 0.313379, acc.: 85.94%] [G loss: 2.767535]\n",
      "epoch:25 step:19835 [D loss: 0.292004, acc.: 87.50%] [G loss: 4.686625]\n",
      "epoch:25 step:19836 [D loss: 0.316458, acc.: 85.16%] [G loss: 7.085770]\n",
      "epoch:25 step:19837 [D loss: 0.279609, acc.: 88.28%] [G loss: 4.674159]\n",
      "epoch:25 step:19838 [D loss: 0.384816, acc.: 84.38%] [G loss: 4.934610]\n",
      "epoch:25 step:19839 [D loss: 0.433968, acc.: 82.03%] [G loss: 5.386605]\n",
      "epoch:25 step:19840 [D loss: 0.546207, acc.: 73.44%] [G loss: 4.530917]\n",
      "epoch:25 step:19841 [D loss: 0.252497, acc.: 89.06%] [G loss: 4.410747]\n",
      "epoch:25 step:19842 [D loss: 0.397677, acc.: 81.25%] [G loss: 4.282025]\n",
      "epoch:25 step:19843 [D loss: 0.318745, acc.: 86.72%] [G loss: 5.519661]\n",
      "epoch:25 step:19844 [D loss: 0.320197, acc.: 83.59%] [G loss: 7.223518]\n",
      "epoch:25 step:19845 [D loss: 0.333751, acc.: 85.16%] [G loss: 5.239863]\n",
      "epoch:25 step:19846 [D loss: 0.373014, acc.: 84.38%] [G loss: 4.587542]\n",
      "epoch:25 step:19847 [D loss: 0.321224, acc.: 86.72%] [G loss: 4.586012]\n",
      "epoch:25 step:19848 [D loss: 0.285798, acc.: 85.16%] [G loss: 4.494467]\n",
      "epoch:25 step:19849 [D loss: 0.290716, acc.: 86.72%] [G loss: 5.264016]\n",
      "epoch:25 step:19850 [D loss: 0.547162, acc.: 74.22%] [G loss: 3.720723]\n",
      "epoch:25 step:19851 [D loss: 0.200086, acc.: 94.53%] [G loss: 3.038568]\n",
      "epoch:25 step:19852 [D loss: 0.349283, acc.: 82.03%] [G loss: 3.713401]\n",
      "epoch:25 step:19853 [D loss: 0.370356, acc.: 81.25%] [G loss: 3.237952]\n",
      "epoch:25 step:19854 [D loss: 0.300879, acc.: 87.50%] [G loss: 3.465601]\n",
      "epoch:25 step:19855 [D loss: 0.380630, acc.: 83.59%] [G loss: 3.157978]\n",
      "epoch:25 step:19856 [D loss: 0.302796, acc.: 89.06%] [G loss: 3.407160]\n",
      "epoch:25 step:19857 [D loss: 0.398299, acc.: 81.25%] [G loss: 3.268518]\n",
      "epoch:25 step:19858 [D loss: 0.230000, acc.: 92.97%] [G loss: 2.594372]\n",
      "epoch:25 step:19859 [D loss: 0.355644, acc.: 85.94%] [G loss: 2.933963]\n",
      "epoch:25 step:19860 [D loss: 0.341213, acc.: 83.59%] [G loss: 3.056309]\n",
      "epoch:25 step:19861 [D loss: 0.444610, acc.: 78.91%] [G loss: 4.186042]\n",
      "epoch:25 step:19862 [D loss: 0.468293, acc.: 82.03%] [G loss: 4.949939]\n",
      "epoch:25 step:19863 [D loss: 0.475127, acc.: 75.00%] [G loss: 3.159659]\n",
      "epoch:25 step:19864 [D loss: 0.284263, acc.: 87.50%] [G loss: 3.994458]\n",
      "epoch:25 step:19865 [D loss: 0.337693, acc.: 81.25%] [G loss: 3.462400]\n",
      "epoch:25 step:19866 [D loss: 0.200265, acc.: 92.97%] [G loss: 4.512567]\n",
      "epoch:25 step:19867 [D loss: 0.306578, acc.: 89.84%] [G loss: 2.289035]\n",
      "epoch:25 step:19868 [D loss: 0.347134, acc.: 78.91%] [G loss: 2.627940]\n",
      "epoch:25 step:19869 [D loss: 0.283159, acc.: 89.06%] [G loss: 2.582184]\n",
      "epoch:25 step:19870 [D loss: 0.335214, acc.: 85.16%] [G loss: 2.674030]\n",
      "epoch:25 step:19871 [D loss: 0.325846, acc.: 86.72%] [G loss: 2.893556]\n",
      "epoch:25 step:19872 [D loss: 0.351360, acc.: 83.59%] [G loss: 3.927073]\n",
      "epoch:25 step:19873 [D loss: 0.279262, acc.: 88.28%] [G loss: 3.690542]\n",
      "epoch:25 step:19874 [D loss: 0.377033, acc.: 78.12%] [G loss: 3.148165]\n",
      "epoch:25 step:19875 [D loss: 0.310370, acc.: 85.94%] [G loss: 2.860231]\n",
      "epoch:25 step:19876 [D loss: 0.417982, acc.: 84.38%] [G loss: 2.377904]\n",
      "epoch:25 step:19877 [D loss: 0.420431, acc.: 78.91%] [G loss: 2.454252]\n",
      "epoch:25 step:19878 [D loss: 0.248974, acc.: 90.62%] [G loss: 2.634685]\n",
      "epoch:25 step:19879 [D loss: 0.284103, acc.: 86.72%] [G loss: 2.380700]\n",
      "epoch:25 step:19880 [D loss: 0.269688, acc.: 89.06%] [G loss: 2.394480]\n",
      "epoch:25 step:19881 [D loss: 0.374807, acc.: 82.81%] [G loss: 3.578837]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:25 step:19882 [D loss: 0.350425, acc.: 83.59%] [G loss: 4.162009]\n",
      "epoch:25 step:19883 [D loss: 0.431932, acc.: 82.03%] [G loss: 2.861975]\n",
      "epoch:25 step:19884 [D loss: 0.275257, acc.: 89.06%] [G loss: 2.931664]\n",
      "epoch:25 step:19885 [D loss: 0.323696, acc.: 84.38%] [G loss: 2.741580]\n",
      "epoch:25 step:19886 [D loss: 0.264980, acc.: 88.28%] [G loss: 2.817133]\n",
      "epoch:25 step:19887 [D loss: 0.339970, acc.: 85.16%] [G loss: 3.037711]\n",
      "epoch:25 step:19888 [D loss: 0.266238, acc.: 89.84%] [G loss: 2.977423]\n",
      "epoch:25 step:19889 [D loss: 0.303978, acc.: 85.94%] [G loss: 2.769139]\n",
      "epoch:25 step:19890 [D loss: 0.444592, acc.: 80.47%] [G loss: 2.825947]\n",
      "epoch:25 step:19891 [D loss: 0.361818, acc.: 85.16%] [G loss: 3.140481]\n",
      "epoch:25 step:19892 [D loss: 0.341495, acc.: 82.03%] [G loss: 3.292315]\n",
      "epoch:25 step:19893 [D loss: 0.353523, acc.: 83.59%] [G loss: 3.260673]\n",
      "epoch:25 step:19894 [D loss: 0.211919, acc.: 90.62%] [G loss: 3.993335]\n",
      "epoch:25 step:19895 [D loss: 0.227503, acc.: 89.84%] [G loss: 4.357950]\n",
      "epoch:25 step:19896 [D loss: 0.257637, acc.: 89.06%] [G loss: 4.157370]\n",
      "epoch:25 step:19897 [D loss: 0.171717, acc.: 96.09%] [G loss: 3.550883]\n",
      "epoch:25 step:19898 [D loss: 0.207153, acc.: 90.62%] [G loss: 4.082222]\n",
      "epoch:25 step:19899 [D loss: 0.314564, acc.: 83.59%] [G loss: 3.794929]\n",
      "epoch:25 step:19900 [D loss: 0.368067, acc.: 84.38%] [G loss: 2.866413]\n",
      "epoch:25 step:19901 [D loss: 0.304219, acc.: 89.06%] [G loss: 3.034775]\n",
      "epoch:25 step:19902 [D loss: 0.319849, acc.: 85.16%] [G loss: 3.298970]\n",
      "epoch:25 step:19903 [D loss: 0.380482, acc.: 86.72%] [G loss: 4.816286]\n",
      "epoch:25 step:19904 [D loss: 0.255616, acc.: 88.28%] [G loss: 4.118536]\n",
      "epoch:25 step:19905 [D loss: 0.251885, acc.: 87.50%] [G loss: 3.403984]\n",
      "epoch:25 step:19906 [D loss: 0.206543, acc.: 90.62%] [G loss: 3.747612]\n",
      "epoch:25 step:19907 [D loss: 0.354100, acc.: 85.16%] [G loss: 3.618133]\n",
      "epoch:25 step:19908 [D loss: 0.409057, acc.: 75.78%] [G loss: 3.015565]\n",
      "epoch:25 step:19909 [D loss: 0.302515, acc.: 85.94%] [G loss: 3.060329]\n",
      "epoch:25 step:19910 [D loss: 0.361922, acc.: 85.16%] [G loss: 3.307159]\n",
      "epoch:25 step:19911 [D loss: 0.369904, acc.: 83.59%] [G loss: 3.934855]\n",
      "epoch:25 step:19912 [D loss: 0.316058, acc.: 85.94%] [G loss: 4.878302]\n",
      "epoch:25 step:19913 [D loss: 0.270573, acc.: 89.06%] [G loss: 3.444529]\n",
      "epoch:25 step:19914 [D loss: 0.297982, acc.: 85.16%] [G loss: 2.378506]\n",
      "epoch:25 step:19915 [D loss: 0.278913, acc.: 87.50%] [G loss: 3.798123]\n",
      "epoch:25 step:19916 [D loss: 0.323901, acc.: 86.72%] [G loss: 2.897427]\n",
      "epoch:25 step:19917 [D loss: 0.346168, acc.: 81.25%] [G loss: 3.190809]\n",
      "epoch:25 step:19918 [D loss: 0.266433, acc.: 92.19%] [G loss: 2.708085]\n",
      "epoch:25 step:19919 [D loss: 0.269267, acc.: 89.84%] [G loss: 2.925451]\n",
      "epoch:25 step:19920 [D loss: 0.369414, acc.: 82.81%] [G loss: 3.979368]\n",
      "epoch:25 step:19921 [D loss: 0.442028, acc.: 78.91%] [G loss: 4.507325]\n",
      "epoch:25 step:19922 [D loss: 0.526158, acc.: 77.34%] [G loss: 3.146019]\n",
      "epoch:25 step:19923 [D loss: 0.273004, acc.: 87.50%] [G loss: 2.488629]\n",
      "epoch:25 step:19924 [D loss: 0.414631, acc.: 79.69%] [G loss: 2.242726]\n",
      "epoch:25 step:19925 [D loss: 0.346492, acc.: 85.94%] [G loss: 3.030083]\n",
      "epoch:25 step:19926 [D loss: 0.359599, acc.: 82.03%] [G loss: 3.507846]\n",
      "epoch:25 step:19927 [D loss: 0.278743, acc.: 88.28%] [G loss: 3.248300]\n",
      "epoch:25 step:19928 [D loss: 0.308365, acc.: 83.59%] [G loss: 3.126853]\n",
      "epoch:25 step:19929 [D loss: 0.267287, acc.: 89.06%] [G loss: 3.842590]\n",
      "epoch:25 step:19930 [D loss: 0.289492, acc.: 86.72%] [G loss: 3.502201]\n",
      "epoch:25 step:19931 [D loss: 0.256329, acc.: 89.84%] [G loss: 2.981733]\n",
      "epoch:25 step:19932 [D loss: 0.286334, acc.: 85.16%] [G loss: 2.800855]\n",
      "epoch:25 step:19933 [D loss: 0.243322, acc.: 91.41%] [G loss: 3.710318]\n",
      "epoch:25 step:19934 [D loss: 0.296811, acc.: 83.59%] [G loss: 2.580177]\n",
      "epoch:25 step:19935 [D loss: 0.240447, acc.: 91.41%] [G loss: 2.577422]\n",
      "epoch:25 step:19936 [D loss: 0.336330, acc.: 82.03%] [G loss: 6.473084]\n",
      "epoch:25 step:19937 [D loss: 0.206646, acc.: 91.41%] [G loss: 5.483354]\n",
      "epoch:25 step:19938 [D loss: 0.228883, acc.: 90.62%] [G loss: 7.032154]\n",
      "epoch:25 step:19939 [D loss: 0.220009, acc.: 92.19%] [G loss: 3.912440]\n",
      "epoch:25 step:19940 [D loss: 0.320178, acc.: 85.16%] [G loss: 5.117975]\n",
      "epoch:25 step:19941 [D loss: 0.275495, acc.: 85.94%] [G loss: 4.350252]\n",
      "epoch:25 step:19942 [D loss: 0.275845, acc.: 89.06%] [G loss: 4.951827]\n",
      "epoch:25 step:19943 [D loss: 0.397817, acc.: 82.81%] [G loss: 2.901121]\n",
      "epoch:25 step:19944 [D loss: 0.261177, acc.: 87.50%] [G loss: 5.274323]\n",
      "epoch:25 step:19945 [D loss: 0.303382, acc.: 86.72%] [G loss: 3.520259]\n",
      "epoch:25 step:19946 [D loss: 0.329928, acc.: 83.59%] [G loss: 3.600079]\n",
      "epoch:25 step:19947 [D loss: 0.336911, acc.: 85.16%] [G loss: 2.965831]\n",
      "epoch:25 step:19948 [D loss: 0.212370, acc.: 92.97%] [G loss: 3.479852]\n",
      "epoch:25 step:19949 [D loss: 0.385753, acc.: 82.03%] [G loss: 3.873717]\n",
      "epoch:25 step:19950 [D loss: 0.309524, acc.: 85.16%] [G loss: 3.796347]\n",
      "epoch:25 step:19951 [D loss: 0.325262, acc.: 83.59%] [G loss: 2.485560]\n",
      "epoch:25 step:19952 [D loss: 0.328287, acc.: 86.72%] [G loss: 3.490922]\n",
      "epoch:25 step:19953 [D loss: 0.378227, acc.: 80.47%] [G loss: 2.190538]\n",
      "epoch:25 step:19954 [D loss: 0.361880, acc.: 82.81%] [G loss: 2.804230]\n",
      "epoch:25 step:19955 [D loss: 0.381348, acc.: 86.72%] [G loss: 3.074955]\n",
      "epoch:25 step:19956 [D loss: 0.386299, acc.: 81.25%] [G loss: 3.009092]\n",
      "epoch:25 step:19957 [D loss: 0.369965, acc.: 78.91%] [G loss: 2.709974]\n",
      "epoch:25 step:19958 [D loss: 0.356199, acc.: 86.72%] [G loss: 2.416855]\n",
      "epoch:25 step:19959 [D loss: 0.328454, acc.: 87.50%] [G loss: 4.506054]\n",
      "epoch:25 step:19960 [D loss: 0.469651, acc.: 78.12%] [G loss: 3.855263]\n",
      "epoch:25 step:19961 [D loss: 0.561557, acc.: 73.44%] [G loss: 6.191935]\n",
      "epoch:25 step:19962 [D loss: 1.496051, acc.: 60.94%] [G loss: 3.539119]\n",
      "epoch:25 step:19963 [D loss: 0.259357, acc.: 89.84%] [G loss: 5.798425]\n",
      "epoch:25 step:19964 [D loss: 0.373806, acc.: 86.72%] [G loss: 3.458400]\n",
      "epoch:25 step:19965 [D loss: 0.319348, acc.: 85.94%] [G loss: 3.903018]\n",
      "epoch:25 step:19966 [D loss: 0.341513, acc.: 85.94%] [G loss: 3.935099]\n",
      "epoch:25 step:19967 [D loss: 0.318345, acc.: 85.94%] [G loss: 3.090480]\n",
      "epoch:25 step:19968 [D loss: 0.244291, acc.: 86.72%] [G loss: 3.043009]\n",
      "epoch:25 step:19969 [D loss: 0.305929, acc.: 85.16%] [G loss: 3.312866]\n",
      "epoch:25 step:19970 [D loss: 0.313117, acc.: 87.50%] [G loss: 3.768514]\n",
      "epoch:25 step:19971 [D loss: 0.309514, acc.: 85.94%] [G loss: 4.088852]\n",
      "epoch:25 step:19972 [D loss: 0.279523, acc.: 87.50%] [G loss: 3.168021]\n",
      "epoch:25 step:19973 [D loss: 0.436858, acc.: 75.78%] [G loss: 2.825793]\n",
      "epoch:25 step:19974 [D loss: 0.410621, acc.: 84.38%] [G loss: 3.131015]\n",
      "epoch:25 step:19975 [D loss: 0.389396, acc.: 81.25%] [G loss: 2.419605]\n",
      "epoch:25 step:19976 [D loss: 0.295962, acc.: 89.06%] [G loss: 2.953611]\n",
      "epoch:25 step:19977 [D loss: 0.371245, acc.: 84.38%] [G loss: 3.101435]\n",
      "epoch:25 step:19978 [D loss: 0.242973, acc.: 89.06%] [G loss: 2.956444]\n",
      "epoch:25 step:19979 [D loss: 0.331369, acc.: 85.94%] [G loss: 4.283213]\n",
      "epoch:25 step:19980 [D loss: 0.373958, acc.: 84.38%] [G loss: 2.597563]\n",
      "epoch:25 step:19981 [D loss: 0.354083, acc.: 85.94%] [G loss: 3.062533]\n",
      "epoch:25 step:19982 [D loss: 0.266712, acc.: 85.94%] [G loss: 5.421401]\n",
      "epoch:25 step:19983 [D loss: 0.242304, acc.: 90.62%] [G loss: 3.918072]\n",
      "epoch:25 step:19984 [D loss: 0.354704, acc.: 85.16%] [G loss: 3.730945]\n",
      "epoch:25 step:19985 [D loss: 0.389989, acc.: 80.47%] [G loss: 3.267552]\n",
      "epoch:25 step:19986 [D loss: 0.225411, acc.: 90.62%] [G loss: 3.456195]\n",
      "epoch:25 step:19987 [D loss: 0.288703, acc.: 89.06%] [G loss: 2.707703]\n",
      "epoch:25 step:19988 [D loss: 0.323224, acc.: 86.72%] [G loss: 3.294728]\n",
      "epoch:25 step:19989 [D loss: 0.282164, acc.: 87.50%] [G loss: 3.119012]\n",
      "epoch:25 step:19990 [D loss: 0.346664, acc.: 85.94%] [G loss: 3.117066]\n",
      "epoch:25 step:19991 [D loss: 0.312639, acc.: 86.72%] [G loss: 2.441002]\n",
      "epoch:25 step:19992 [D loss: 0.399679, acc.: 83.59%] [G loss: 3.160265]\n",
      "epoch:25 step:19993 [D loss: 0.343818, acc.: 84.38%] [G loss: 2.508328]\n",
      "epoch:25 step:19994 [D loss: 0.302027, acc.: 85.94%] [G loss: 3.541580]\n",
      "epoch:25 step:19995 [D loss: 0.370858, acc.: 82.03%] [G loss: 2.769204]\n",
      "epoch:25 step:19996 [D loss: 0.380577, acc.: 84.38%] [G loss: 4.223368]\n",
      "epoch:25 step:19997 [D loss: 0.260162, acc.: 86.72%] [G loss: 3.979397]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:25 step:19998 [D loss: 0.396631, acc.: 83.59%] [G loss: 4.803317]\n",
      "epoch:25 step:19999 [D loss: 0.301227, acc.: 88.28%] [G loss: 3.482155]\n",
      "epoch:25 step:20000 [D loss: 0.260517, acc.: 88.28%] [G loss: 2.908204]\n",
      "##############\n",
      "[0.85568877 0.85582855 0.80996997 0.79261114 0.76554939 0.82320367\n",
      " 0.90027503 0.82035141 0.80574708 0.80270068]\n",
      "##########\n",
      "epoch:25 step:20001 [D loss: 0.326585, acc.: 82.81%] [G loss: 3.275260]\n",
      "epoch:25 step:20002 [D loss: 0.232607, acc.: 89.84%] [G loss: 2.993268]\n",
      "epoch:25 step:20003 [D loss: 0.297557, acc.: 89.06%] [G loss: 2.905735]\n",
      "epoch:25 step:20004 [D loss: 0.323029, acc.: 87.50%] [G loss: 2.794313]\n",
      "epoch:25 step:20005 [D loss: 0.247823, acc.: 89.84%] [G loss: 2.835015]\n",
      "epoch:25 step:20006 [D loss: 0.353072, acc.: 83.59%] [G loss: 2.936246]\n",
      "epoch:25 step:20007 [D loss: 0.385136, acc.: 84.38%] [G loss: 2.774588]\n",
      "epoch:25 step:20008 [D loss: 0.307587, acc.: 85.94%] [G loss: 4.351341]\n",
      "epoch:25 step:20009 [D loss: 0.408840, acc.: 82.03%] [G loss: 3.467811]\n",
      "epoch:25 step:20010 [D loss: 0.262927, acc.: 89.06%] [G loss: 4.039188]\n",
      "epoch:25 step:20011 [D loss: 0.279733, acc.: 89.06%] [G loss: 3.444716]\n",
      "epoch:25 step:20012 [D loss: 0.297266, acc.: 85.16%] [G loss: 3.710777]\n",
      "epoch:25 step:20013 [D loss: 0.274123, acc.: 85.94%] [G loss: 3.967882]\n",
      "epoch:25 step:20014 [D loss: 0.254855, acc.: 88.28%] [G loss: 2.985707]\n",
      "epoch:25 step:20015 [D loss: 0.319774, acc.: 87.50%] [G loss: 3.223163]\n",
      "epoch:25 step:20016 [D loss: 0.356737, acc.: 81.25%] [G loss: 3.212271]\n",
      "epoch:25 step:20017 [D loss: 0.472954, acc.: 79.69%] [G loss: 3.452792]\n",
      "epoch:25 step:20018 [D loss: 0.401122, acc.: 83.59%] [G loss: 3.991200]\n",
      "epoch:25 step:20019 [D loss: 0.371257, acc.: 80.47%] [G loss: 2.432909]\n",
      "epoch:25 step:20020 [D loss: 0.287055, acc.: 88.28%] [G loss: 4.643216]\n",
      "epoch:25 step:20021 [D loss: 0.338199, acc.: 82.03%] [G loss: 5.147322]\n",
      "epoch:25 step:20022 [D loss: 0.297773, acc.: 86.72%] [G loss: 3.214256]\n",
      "epoch:25 step:20023 [D loss: 0.390205, acc.: 82.81%] [G loss: 4.641210]\n",
      "epoch:25 step:20024 [D loss: 0.359675, acc.: 84.38%] [G loss: 2.672388]\n",
      "epoch:25 step:20025 [D loss: 0.316700, acc.: 85.16%] [G loss: 2.374410]\n",
      "epoch:25 step:20026 [D loss: 0.386322, acc.: 78.91%] [G loss: 3.111109]\n",
      "epoch:25 step:20027 [D loss: 0.308952, acc.: 85.16%] [G loss: 2.555865]\n",
      "epoch:25 step:20028 [D loss: 0.287096, acc.: 88.28%] [G loss: 3.924861]\n",
      "epoch:25 step:20029 [D loss: 0.359379, acc.: 85.16%] [G loss: 3.693727]\n",
      "epoch:25 step:20030 [D loss: 0.389378, acc.: 85.16%] [G loss: 3.302208]\n",
      "epoch:25 step:20031 [D loss: 0.405282, acc.: 79.69%] [G loss: 6.244656]\n",
      "epoch:25 step:20032 [D loss: 0.361187, acc.: 82.03%] [G loss: 4.061509]\n",
      "epoch:25 step:20033 [D loss: 0.273539, acc.: 85.94%] [G loss: 2.919250]\n",
      "epoch:25 step:20034 [D loss: 0.279344, acc.: 86.72%] [G loss: 2.915538]\n",
      "epoch:25 step:20035 [D loss: 0.322655, acc.: 85.16%] [G loss: 3.414647]\n",
      "epoch:25 step:20036 [D loss: 0.275924, acc.: 88.28%] [G loss: 4.299011]\n",
      "epoch:25 step:20037 [D loss: 0.300778, acc.: 88.28%] [G loss: 3.566195]\n",
      "epoch:25 step:20038 [D loss: 0.254497, acc.: 88.28%] [G loss: 2.946714]\n",
      "epoch:25 step:20039 [D loss: 0.328041, acc.: 84.38%] [G loss: 3.359928]\n",
      "epoch:25 step:20040 [D loss: 0.240815, acc.: 89.06%] [G loss: 3.635245]\n",
      "epoch:25 step:20041 [D loss: 0.295092, acc.: 86.72%] [G loss: 2.721920]\n",
      "epoch:25 step:20042 [D loss: 0.371784, acc.: 81.25%] [G loss: 2.524724]\n",
      "epoch:25 step:20043 [D loss: 0.269769, acc.: 89.06%] [G loss: 2.324355]\n",
      "epoch:25 step:20044 [D loss: 0.222845, acc.: 92.97%] [G loss: 2.682867]\n",
      "epoch:25 step:20045 [D loss: 0.320047, acc.: 85.94%] [G loss: 3.603772]\n",
      "epoch:25 step:20046 [D loss: 0.385742, acc.: 83.59%] [G loss: 2.666696]\n",
      "epoch:25 step:20047 [D loss: 0.311419, acc.: 81.25%] [G loss: 3.386733]\n",
      "epoch:25 step:20048 [D loss: 0.222974, acc.: 92.19%] [G loss: 4.111367]\n",
      "epoch:25 step:20049 [D loss: 0.287100, acc.: 89.84%] [G loss: 4.619688]\n",
      "epoch:25 step:20050 [D loss: 0.243036, acc.: 89.06%] [G loss: 5.206650]\n",
      "epoch:25 step:20051 [D loss: 0.414672, acc.: 79.69%] [G loss: 3.093623]\n",
      "epoch:25 step:20052 [D loss: 0.262407, acc.: 88.28%] [G loss: 2.756304]\n",
      "epoch:25 step:20053 [D loss: 0.297751, acc.: 87.50%] [G loss: 5.402740]\n",
      "epoch:25 step:20054 [D loss: 0.252726, acc.: 84.38%] [G loss: 4.987483]\n",
      "epoch:25 step:20055 [D loss: 0.441406, acc.: 75.00%] [G loss: 4.380985]\n",
      "epoch:25 step:20056 [D loss: 0.203419, acc.: 93.75%] [G loss: 4.170341]\n",
      "epoch:25 step:20057 [D loss: 0.231508, acc.: 94.53%] [G loss: 3.601576]\n",
      "epoch:25 step:20058 [D loss: 0.351744, acc.: 86.72%] [G loss: 2.969907]\n",
      "epoch:25 step:20059 [D loss: 0.283824, acc.: 85.16%] [G loss: 3.666603]\n",
      "epoch:25 step:20060 [D loss: 0.264612, acc.: 89.06%] [G loss: 3.531566]\n",
      "epoch:25 step:20061 [D loss: 0.328108, acc.: 87.50%] [G loss: 3.342579]\n",
      "epoch:25 step:20062 [D loss: 0.399135, acc.: 75.78%] [G loss: 2.638194]\n",
      "epoch:25 step:20063 [D loss: 0.329682, acc.: 85.94%] [G loss: 3.651395]\n",
      "epoch:25 step:20064 [D loss: 0.246813, acc.: 89.06%] [G loss: 2.335602]\n",
      "epoch:25 step:20065 [D loss: 0.314050, acc.: 86.72%] [G loss: 3.200562]\n",
      "epoch:25 step:20066 [D loss: 0.324188, acc.: 85.94%] [G loss: 4.198808]\n",
      "epoch:25 step:20067 [D loss: 0.279197, acc.: 85.16%] [G loss: 2.982505]\n",
      "epoch:25 step:20068 [D loss: 0.283282, acc.: 84.38%] [G loss: 3.460573]\n",
      "epoch:25 step:20069 [D loss: 0.283162, acc.: 89.06%] [G loss: 3.411466]\n",
      "epoch:25 step:20070 [D loss: 0.358881, acc.: 85.94%] [G loss: 2.831787]\n",
      "epoch:25 step:20071 [D loss: 0.318131, acc.: 87.50%] [G loss: 3.136958]\n",
      "epoch:25 step:20072 [D loss: 0.389068, acc.: 75.00%] [G loss: 4.002948]\n",
      "epoch:25 step:20073 [D loss: 0.164830, acc.: 92.97%] [G loss: 4.124320]\n",
      "epoch:25 step:20074 [D loss: 0.383958, acc.: 79.69%] [G loss: 3.276637]\n",
      "epoch:25 step:20075 [D loss: 0.264392, acc.: 89.84%] [G loss: 3.299677]\n",
      "epoch:25 step:20076 [D loss: 0.206420, acc.: 92.19%] [G loss: 2.997515]\n",
      "epoch:25 step:20077 [D loss: 0.394316, acc.: 85.16%] [G loss: 3.909061]\n",
      "epoch:25 step:20078 [D loss: 0.367743, acc.: 82.03%] [G loss: 3.382144]\n",
      "epoch:25 step:20079 [D loss: 0.270863, acc.: 86.72%] [G loss: 4.111691]\n",
      "epoch:25 step:20080 [D loss: 0.470036, acc.: 77.34%] [G loss: 3.459432]\n",
      "epoch:25 step:20081 [D loss: 0.307048, acc.: 87.50%] [G loss: 4.604937]\n",
      "epoch:25 step:20082 [D loss: 0.252987, acc.: 91.41%] [G loss: 3.920753]\n",
      "epoch:25 step:20083 [D loss: 0.316399, acc.: 85.16%] [G loss: 3.252212]\n",
      "epoch:25 step:20084 [D loss: 0.362166, acc.: 85.94%] [G loss: 3.451235]\n",
      "epoch:25 step:20085 [D loss: 0.217737, acc.: 92.19%] [G loss: 5.907857]\n",
      "epoch:25 step:20086 [D loss: 0.283279, acc.: 88.28%] [G loss: 4.061352]\n",
      "epoch:25 step:20087 [D loss: 0.292342, acc.: 88.28%] [G loss: 2.776779]\n",
      "epoch:25 step:20088 [D loss: 0.210800, acc.: 92.19%] [G loss: 4.424697]\n",
      "epoch:25 step:20089 [D loss: 0.218433, acc.: 89.84%] [G loss: 5.922093]\n",
      "epoch:25 step:20090 [D loss: 0.292133, acc.: 85.94%] [G loss: 4.275169]\n",
      "epoch:25 step:20091 [D loss: 0.185502, acc.: 95.31%] [G loss: 3.609468]\n",
      "epoch:25 step:20092 [D loss: 0.256167, acc.: 87.50%] [G loss: 3.042486]\n",
      "epoch:25 step:20093 [D loss: 0.317451, acc.: 88.28%] [G loss: 3.718916]\n",
      "epoch:25 step:20094 [D loss: 0.227930, acc.: 89.84%] [G loss: 3.918787]\n",
      "epoch:25 step:20095 [D loss: 0.293433, acc.: 86.72%] [G loss: 3.126969]\n",
      "epoch:25 step:20096 [D loss: 0.321089, acc.: 83.59%] [G loss: 5.513900]\n",
      "epoch:25 step:20097 [D loss: 0.254414, acc.: 88.28%] [G loss: 3.016321]\n",
      "epoch:25 step:20098 [D loss: 0.273985, acc.: 88.28%] [G loss: 3.855602]\n",
      "epoch:25 step:20099 [D loss: 0.499256, acc.: 78.91%] [G loss: 3.788455]\n",
      "epoch:25 step:20100 [D loss: 0.481454, acc.: 79.69%] [G loss: 6.403146]\n",
      "epoch:25 step:20101 [D loss: 0.595931, acc.: 75.00%] [G loss: 6.072190]\n",
      "epoch:25 step:20102 [D loss: 0.408885, acc.: 85.16%] [G loss: 4.178663]\n",
      "epoch:25 step:20103 [D loss: 0.390501, acc.: 83.59%] [G loss: 3.082701]\n",
      "epoch:25 step:20104 [D loss: 0.356978, acc.: 84.38%] [G loss: 3.173859]\n",
      "epoch:25 step:20105 [D loss: 0.387390, acc.: 80.47%] [G loss: 3.309458]\n",
      "epoch:25 step:20106 [D loss: 0.308802, acc.: 87.50%] [G loss: 2.525505]\n",
      "epoch:25 step:20107 [D loss: 0.334053, acc.: 88.28%] [G loss: 4.322117]\n",
      "epoch:25 step:20108 [D loss: 0.330413, acc.: 80.47%] [G loss: 3.642275]\n",
      "epoch:25 step:20109 [D loss: 0.318855, acc.: 87.50%] [G loss: 2.631733]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:25 step:20110 [D loss: 0.411446, acc.: 82.03%] [G loss: 2.735356]\n",
      "epoch:25 step:20111 [D loss: 0.366242, acc.: 85.16%] [G loss: 2.862055]\n",
      "epoch:25 step:20112 [D loss: 0.328290, acc.: 83.59%] [G loss: 3.385612]\n",
      "epoch:25 step:20113 [D loss: 0.343626, acc.: 83.59%] [G loss: 4.053478]\n",
      "epoch:25 step:20114 [D loss: 0.395253, acc.: 82.81%] [G loss: 4.524431]\n",
      "epoch:25 step:20115 [D loss: 0.430472, acc.: 84.38%] [G loss: 4.011902]\n",
      "epoch:25 step:20116 [D loss: 0.594092, acc.: 69.53%] [G loss: 3.026718]\n",
      "epoch:25 step:20117 [D loss: 0.243160, acc.: 93.75%] [G loss: 2.858224]\n",
      "epoch:25 step:20118 [D loss: 0.398356, acc.: 82.81%] [G loss: 2.813642]\n",
      "epoch:25 step:20119 [D loss: 0.338555, acc.: 84.38%] [G loss: 3.208963]\n",
      "epoch:25 step:20120 [D loss: 0.410517, acc.: 81.25%] [G loss: 3.668485]\n",
      "epoch:25 step:20121 [D loss: 0.439314, acc.: 77.34%] [G loss: 3.021116]\n",
      "epoch:25 step:20122 [D loss: 0.340101, acc.: 86.72%] [G loss: 2.963790]\n",
      "epoch:25 step:20123 [D loss: 0.392346, acc.: 84.38%] [G loss: 3.395587]\n",
      "epoch:25 step:20124 [D loss: 0.290649, acc.: 89.84%] [G loss: 4.039959]\n",
      "epoch:25 step:20125 [D loss: 0.315002, acc.: 88.28%] [G loss: 2.467119]\n",
      "epoch:25 step:20126 [D loss: 0.305791, acc.: 87.50%] [G loss: 3.397036]\n",
      "epoch:25 step:20127 [D loss: 0.220082, acc.: 90.62%] [G loss: 3.414542]\n",
      "epoch:25 step:20128 [D loss: 0.256852, acc.: 91.41%] [G loss: 4.059275]\n",
      "epoch:25 step:20129 [D loss: 0.396616, acc.: 82.03%] [G loss: 2.948507]\n",
      "epoch:25 step:20130 [D loss: 0.205539, acc.: 94.53%] [G loss: 4.298113]\n",
      "epoch:25 step:20131 [D loss: 0.424066, acc.: 81.25%] [G loss: 2.927970]\n",
      "epoch:25 step:20132 [D loss: 0.246204, acc.: 88.28%] [G loss: 3.409969]\n",
      "epoch:25 step:20133 [D loss: 0.270257, acc.: 88.28%] [G loss: 3.564403]\n",
      "epoch:25 step:20134 [D loss: 0.384502, acc.: 82.81%] [G loss: 4.104613]\n",
      "epoch:25 step:20135 [D loss: 0.245947, acc.: 90.62%] [G loss: 5.134688]\n",
      "epoch:25 step:20136 [D loss: 0.326556, acc.: 82.03%] [G loss: 3.564693]\n",
      "epoch:25 step:20137 [D loss: 0.345114, acc.: 87.50%] [G loss: 2.940447]\n",
      "epoch:25 step:20138 [D loss: 0.273401, acc.: 86.72%] [G loss: 3.233336]\n",
      "epoch:25 step:20139 [D loss: 0.416029, acc.: 81.25%] [G loss: 3.405887]\n",
      "epoch:25 step:20140 [D loss: 0.319209, acc.: 85.94%] [G loss: 2.860702]\n",
      "epoch:25 step:20141 [D loss: 0.260104, acc.: 86.72%] [G loss: 3.003110]\n",
      "epoch:25 step:20142 [D loss: 0.254412, acc.: 89.84%] [G loss: 3.187079]\n",
      "epoch:25 step:20143 [D loss: 0.313223, acc.: 85.94%] [G loss: 3.004341]\n",
      "epoch:25 step:20144 [D loss: 0.319789, acc.: 85.16%] [G loss: 2.935206]\n",
      "epoch:25 step:20145 [D loss: 0.341706, acc.: 88.28%] [G loss: 3.911637]\n",
      "epoch:25 step:20146 [D loss: 0.295894, acc.: 86.72%] [G loss: 3.044182]\n",
      "epoch:25 step:20147 [D loss: 0.278531, acc.: 87.50%] [G loss: 3.159773]\n",
      "epoch:25 step:20148 [D loss: 0.432738, acc.: 82.81%] [G loss: 3.407175]\n",
      "epoch:25 step:20149 [D loss: 0.273716, acc.: 89.06%] [G loss: 3.573324]\n",
      "epoch:25 step:20150 [D loss: 0.338066, acc.: 85.16%] [G loss: 4.098067]\n",
      "epoch:25 step:20151 [D loss: 0.318446, acc.: 85.94%] [G loss: 3.424909]\n",
      "epoch:25 step:20152 [D loss: 0.282875, acc.: 84.38%] [G loss: 4.356824]\n",
      "epoch:25 step:20153 [D loss: 0.410977, acc.: 78.91%] [G loss: 3.936537]\n",
      "epoch:25 step:20154 [D loss: 0.314302, acc.: 88.28%] [G loss: 3.354439]\n",
      "epoch:25 step:20155 [D loss: 0.357457, acc.: 82.81%] [G loss: 3.085390]\n",
      "epoch:25 step:20156 [D loss: 0.186349, acc.: 90.62%] [G loss: 3.631216]\n",
      "epoch:25 step:20157 [D loss: 0.234378, acc.: 90.62%] [G loss: 4.930624]\n",
      "epoch:25 step:20158 [D loss: 0.320415, acc.: 87.50%] [G loss: 3.144579]\n",
      "epoch:25 step:20159 [D loss: 0.205078, acc.: 91.41%] [G loss: 5.144725]\n",
      "epoch:25 step:20160 [D loss: 0.367189, acc.: 82.81%] [G loss: 5.543941]\n",
      "epoch:25 step:20161 [D loss: 0.298855, acc.: 82.81%] [G loss: 3.554193]\n",
      "epoch:25 step:20162 [D loss: 0.300347, acc.: 89.06%] [G loss: 4.772959]\n",
      "epoch:25 step:20163 [D loss: 0.295383, acc.: 88.28%] [G loss: 4.282858]\n",
      "epoch:25 step:20164 [D loss: 0.475001, acc.: 78.91%] [G loss: 4.681501]\n",
      "epoch:25 step:20165 [D loss: 0.408586, acc.: 82.03%] [G loss: 4.271783]\n",
      "epoch:25 step:20166 [D loss: 0.269133, acc.: 89.84%] [G loss: 3.274913]\n",
      "epoch:25 step:20167 [D loss: 0.355813, acc.: 83.59%] [G loss: 4.133780]\n",
      "epoch:25 step:20168 [D loss: 0.244898, acc.: 87.50%] [G loss: 5.743949]\n",
      "epoch:25 step:20169 [D loss: 0.316707, acc.: 83.59%] [G loss: 3.744730]\n",
      "epoch:25 step:20170 [D loss: 0.378555, acc.: 82.81%] [G loss: 4.553567]\n",
      "epoch:25 step:20171 [D loss: 0.429602, acc.: 82.03%] [G loss: 5.631626]\n",
      "epoch:25 step:20172 [D loss: 0.269830, acc.: 89.06%] [G loss: 3.996315]\n",
      "epoch:25 step:20173 [D loss: 0.435435, acc.: 79.69%] [G loss: 3.280497]\n",
      "epoch:25 step:20174 [D loss: 0.397285, acc.: 85.16%] [G loss: 3.317123]\n",
      "epoch:25 step:20175 [D loss: 0.349290, acc.: 83.59%] [G loss: 3.321011]\n",
      "epoch:25 step:20176 [D loss: 0.410521, acc.: 82.03%] [G loss: 4.191010]\n",
      "epoch:25 step:20177 [D loss: 0.271480, acc.: 89.06%] [G loss: 3.654244]\n",
      "epoch:25 step:20178 [D loss: 0.304639, acc.: 88.28%] [G loss: 3.094792]\n",
      "epoch:25 step:20179 [D loss: 0.394419, acc.: 82.03%] [G loss: 2.974889]\n",
      "epoch:25 step:20180 [D loss: 0.333017, acc.: 80.47%] [G loss: 4.366510]\n",
      "epoch:25 step:20181 [D loss: 0.279231, acc.: 89.06%] [G loss: 4.095540]\n",
      "epoch:25 step:20182 [D loss: 0.425909, acc.: 82.81%] [G loss: 5.110148]\n",
      "epoch:25 step:20183 [D loss: 0.676318, acc.: 73.44%] [G loss: 6.021672]\n",
      "epoch:25 step:20184 [D loss: 0.326656, acc.: 85.94%] [G loss: 4.269139]\n",
      "epoch:25 step:20185 [D loss: 0.292629, acc.: 86.72%] [G loss: 3.984111]\n",
      "epoch:25 step:20186 [D loss: 0.406185, acc.: 81.25%] [G loss: 3.543266]\n",
      "epoch:25 step:20187 [D loss: 0.329170, acc.: 85.94%] [G loss: 2.634412]\n",
      "epoch:25 step:20188 [D loss: 0.350051, acc.: 85.94%] [G loss: 3.472757]\n",
      "epoch:25 step:20189 [D loss: 0.320600, acc.: 82.81%] [G loss: 3.346105]\n",
      "epoch:25 step:20190 [D loss: 0.313635, acc.: 87.50%] [G loss: 4.087330]\n",
      "epoch:25 step:20191 [D loss: 0.356534, acc.: 83.59%] [G loss: 4.188507]\n",
      "epoch:25 step:20192 [D loss: 0.265350, acc.: 89.06%] [G loss: 4.592641]\n",
      "epoch:25 step:20193 [D loss: 0.221509, acc.: 91.41%] [G loss: 8.909576]\n",
      "epoch:25 step:20194 [D loss: 0.309778, acc.: 83.59%] [G loss: 4.335672]\n",
      "epoch:25 step:20195 [D loss: 0.275040, acc.: 84.38%] [G loss: 2.811852]\n",
      "epoch:25 step:20196 [D loss: 0.421378, acc.: 80.47%] [G loss: 2.530075]\n",
      "epoch:25 step:20197 [D loss: 0.336066, acc.: 85.16%] [G loss: 2.963330]\n",
      "epoch:25 step:20198 [D loss: 0.377846, acc.: 81.25%] [G loss: 3.899910]\n",
      "epoch:25 step:20199 [D loss: 0.234824, acc.: 93.75%] [G loss: 2.777309]\n",
      "epoch:25 step:20200 [D loss: 0.368398, acc.: 83.59%] [G loss: 4.307968]\n",
      "##############\n",
      "[0.87045691 0.85354376 0.79958164 0.78910701 0.77016859 0.81035316\n",
      " 0.89748899 0.83104413 0.81077323 0.80958283]\n",
      "##########\n",
      "epoch:25 step:20201 [D loss: 0.328749, acc.: 84.38%] [G loss: 2.965626]\n",
      "epoch:25 step:20202 [D loss: 0.413280, acc.: 80.47%] [G loss: 3.672653]\n",
      "epoch:25 step:20203 [D loss: 0.393051, acc.: 79.69%] [G loss: 3.742942]\n",
      "epoch:25 step:20204 [D loss: 0.312957, acc.: 89.06%] [G loss: 3.216691]\n",
      "epoch:25 step:20205 [D loss: 0.382326, acc.: 84.38%] [G loss: 3.907271]\n",
      "epoch:25 step:20206 [D loss: 0.372368, acc.: 85.94%] [G loss: 3.136723]\n",
      "epoch:25 step:20207 [D loss: 0.439703, acc.: 79.69%] [G loss: 3.350435]\n",
      "epoch:25 step:20208 [D loss: 0.308022, acc.: 82.03%] [G loss: 3.718400]\n",
      "epoch:25 step:20209 [D loss: 0.442743, acc.: 78.12%] [G loss: 3.371081]\n",
      "epoch:25 step:20210 [D loss: 0.310782, acc.: 84.38%] [G loss: 3.234752]\n",
      "epoch:25 step:20211 [D loss: 0.318606, acc.: 87.50%] [G loss: 3.470131]\n",
      "epoch:25 step:20212 [D loss: 0.217821, acc.: 91.41%] [G loss: 7.367872]\n",
      "epoch:25 step:20213 [D loss: 0.379080, acc.: 85.16%] [G loss: 4.129181]\n",
      "epoch:25 step:20214 [D loss: 0.265546, acc.: 89.84%] [G loss: 3.032288]\n",
      "epoch:25 step:20215 [D loss: 0.309529, acc.: 88.28%] [G loss: 3.295251]\n",
      "epoch:25 step:20216 [D loss: 0.293873, acc.: 86.72%] [G loss: 4.730103]\n",
      "epoch:25 step:20217 [D loss: 0.374662, acc.: 83.59%] [G loss: 3.991611]\n",
      "epoch:25 step:20218 [D loss: 0.322642, acc.: 82.81%] [G loss: 4.618927]\n",
      "epoch:25 step:20219 [D loss: 0.309189, acc.: 83.59%] [G loss: 4.119450]\n",
      "epoch:25 step:20220 [D loss: 0.188276, acc.: 92.97%] [G loss: 3.663637]\n",
      "epoch:25 step:20221 [D loss: 0.331639, acc.: 86.72%] [G loss: 3.884222]\n",
      "epoch:25 step:20222 [D loss: 0.335089, acc.: 88.28%] [G loss: 3.786056]\n",
      "epoch:25 step:20223 [D loss: 0.318985, acc.: 86.72%] [G loss: 4.273059]\n",
      "epoch:25 step:20224 [D loss: 0.289386, acc.: 87.50%] [G loss: 4.044466]\n",
      "epoch:25 step:20225 [D loss: 0.362769, acc.: 85.94%] [G loss: 3.316511]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:25 step:20226 [D loss: 0.417776, acc.: 80.47%] [G loss: 3.471584]\n",
      "epoch:25 step:20227 [D loss: 0.289205, acc.: 87.50%] [G loss: 2.993262]\n",
      "epoch:25 step:20228 [D loss: 0.361608, acc.: 82.81%] [G loss: 2.722638]\n",
      "epoch:25 step:20229 [D loss: 0.430681, acc.: 78.91%] [G loss: 3.743998]\n",
      "epoch:25 step:20230 [D loss: 0.373163, acc.: 85.16%] [G loss: 6.413021]\n",
      "epoch:25 step:20231 [D loss: 0.699283, acc.: 74.22%] [G loss: 6.887973]\n",
      "epoch:25 step:20232 [D loss: 1.501163, acc.: 63.28%] [G loss: 7.762762]\n",
      "epoch:25 step:20233 [D loss: 2.257447, acc.: 57.03%] [G loss: 2.586713]\n",
      "epoch:25 step:20234 [D loss: 0.504940, acc.: 77.34%] [G loss: 2.592309]\n",
      "epoch:25 step:20235 [D loss: 0.519412, acc.: 81.25%] [G loss: 2.570517]\n",
      "epoch:25 step:20236 [D loss: 0.507994, acc.: 77.34%] [G loss: 5.301281]\n",
      "epoch:25 step:20237 [D loss: 0.393349, acc.: 89.06%] [G loss: 3.659723]\n",
      "epoch:25 step:20238 [D loss: 0.457585, acc.: 79.69%] [G loss: 3.602272]\n",
      "epoch:25 step:20239 [D loss: 0.642074, acc.: 72.66%] [G loss: 3.293855]\n",
      "epoch:25 step:20240 [D loss: 0.334312, acc.: 81.25%] [G loss: 2.982729]\n",
      "epoch:25 step:20241 [D loss: 0.408930, acc.: 79.69%] [G loss: 3.270076]\n",
      "epoch:25 step:20242 [D loss: 0.346615, acc.: 84.38%] [G loss: 2.954157]\n",
      "epoch:25 step:20243 [D loss: 0.327140, acc.: 85.16%] [G loss: 3.975639]\n",
      "epoch:25 step:20244 [D loss: 0.410655, acc.: 81.25%] [G loss: 2.790671]\n",
      "epoch:25 step:20245 [D loss: 0.342790, acc.: 88.28%] [G loss: 3.353434]\n",
      "epoch:25 step:20246 [D loss: 0.387942, acc.: 82.03%] [G loss: 2.541330]\n",
      "epoch:25 step:20247 [D loss: 0.435948, acc.: 80.47%] [G loss: 2.931605]\n",
      "epoch:25 step:20248 [D loss: 0.398501, acc.: 83.59%] [G loss: 3.680717]\n",
      "epoch:25 step:20249 [D loss: 0.266082, acc.: 89.84%] [G loss: 3.112339]\n",
      "epoch:25 step:20250 [D loss: 0.368673, acc.: 85.94%] [G loss: 2.555132]\n",
      "epoch:25 step:20251 [D loss: 0.314696, acc.: 88.28%] [G loss: 4.987434]\n",
      "epoch:25 step:20252 [D loss: 0.365223, acc.: 84.38%] [G loss: 2.366285]\n",
      "epoch:25 step:20253 [D loss: 0.424056, acc.: 78.91%] [G loss: 2.537736]\n",
      "epoch:25 step:20254 [D loss: 0.264072, acc.: 92.19%] [G loss: 2.505414]\n",
      "epoch:25 step:20255 [D loss: 0.336854, acc.: 85.94%] [G loss: 2.902658]\n",
      "epoch:25 step:20256 [D loss: 0.170471, acc.: 94.53%] [G loss: 3.681280]\n",
      "epoch:25 step:20257 [D loss: 0.417964, acc.: 81.25%] [G loss: 3.402166]\n",
      "epoch:25 step:20258 [D loss: 0.180648, acc.: 93.75%] [G loss: 3.499604]\n",
      "epoch:25 step:20259 [D loss: 0.306904, acc.: 86.72%] [G loss: 2.802158]\n",
      "epoch:25 step:20260 [D loss: 0.399157, acc.: 80.47%] [G loss: 3.025992]\n",
      "epoch:25 step:20261 [D loss: 0.243846, acc.: 94.53%] [G loss: 3.394592]\n",
      "epoch:25 step:20262 [D loss: 0.350057, acc.: 82.81%] [G loss: 3.168674]\n",
      "epoch:25 step:20263 [D loss: 0.392983, acc.: 81.25%] [G loss: 2.903309]\n",
      "epoch:25 step:20264 [D loss: 0.408951, acc.: 83.59%] [G loss: 3.830844]\n",
      "epoch:25 step:20265 [D loss: 0.449265, acc.: 79.69%] [G loss: 3.516428]\n",
      "epoch:25 step:20266 [D loss: 0.297289, acc.: 87.50%] [G loss: 3.637129]\n",
      "epoch:25 step:20267 [D loss: 0.370181, acc.: 85.16%] [G loss: 3.341779]\n",
      "epoch:25 step:20268 [D loss: 0.347553, acc.: 85.94%] [G loss: 2.503477]\n",
      "epoch:25 step:20269 [D loss: 0.404797, acc.: 83.59%] [G loss: 2.710314]\n",
      "epoch:25 step:20270 [D loss: 0.395946, acc.: 78.91%] [G loss: 2.785012]\n",
      "epoch:25 step:20271 [D loss: 0.294347, acc.: 86.72%] [G loss: 3.984038]\n",
      "epoch:25 step:20272 [D loss: 0.240861, acc.: 89.84%] [G loss: 5.362233]\n",
      "epoch:25 step:20273 [D loss: 0.325111, acc.: 83.59%] [G loss: 3.566161]\n",
      "epoch:25 step:20274 [D loss: 0.239317, acc.: 89.84%] [G loss: 4.301295]\n",
      "epoch:25 step:20275 [D loss: 0.325298, acc.: 82.81%] [G loss: 3.374406]\n",
      "epoch:25 step:20276 [D loss: 0.210072, acc.: 92.19%] [G loss: 4.226621]\n",
      "epoch:25 step:20277 [D loss: 0.230530, acc.: 92.19%] [G loss: 2.785912]\n",
      "epoch:25 step:20278 [D loss: 0.316099, acc.: 86.72%] [G loss: 2.699872]\n",
      "epoch:25 step:20279 [D loss: 0.321417, acc.: 80.47%] [G loss: 2.896217]\n",
      "epoch:25 step:20280 [D loss: 0.256062, acc.: 89.84%] [G loss: 3.271623]\n",
      "epoch:25 step:20281 [D loss: 0.386618, acc.: 81.25%] [G loss: 2.645545]\n",
      "epoch:25 step:20282 [D loss: 0.241960, acc.: 91.41%] [G loss: 2.124744]\n",
      "epoch:25 step:20283 [D loss: 0.432439, acc.: 80.47%] [G loss: 3.582146]\n",
      "epoch:25 step:20284 [D loss: 0.186059, acc.: 90.62%] [G loss: 3.127341]\n",
      "epoch:25 step:20285 [D loss: 0.299331, acc.: 88.28%] [G loss: 3.266118]\n",
      "epoch:25 step:20286 [D loss: 0.255228, acc.: 89.06%] [G loss: 2.276778]\n",
      "epoch:25 step:20287 [D loss: 0.469543, acc.: 79.69%] [G loss: 2.735383]\n",
      "epoch:25 step:20288 [D loss: 0.299121, acc.: 85.16%] [G loss: 2.675041]\n",
      "epoch:25 step:20289 [D loss: 0.310159, acc.: 88.28%] [G loss: 3.519989]\n",
      "epoch:25 step:20290 [D loss: 0.355759, acc.: 80.47%] [G loss: 3.226026]\n",
      "epoch:25 step:20291 [D loss: 0.322500, acc.: 86.72%] [G loss: 3.226488]\n",
      "epoch:25 step:20292 [D loss: 0.319415, acc.: 89.84%] [G loss: 2.854194]\n",
      "epoch:25 step:20293 [D loss: 0.309992, acc.: 85.94%] [G loss: 3.651841]\n",
      "epoch:25 step:20294 [D loss: 0.297713, acc.: 89.84%] [G loss: 3.816945]\n",
      "epoch:25 step:20295 [D loss: 0.305116, acc.: 85.16%] [G loss: 3.630989]\n",
      "epoch:25 step:20296 [D loss: 0.310928, acc.: 84.38%] [G loss: 2.972984]\n",
      "epoch:25 step:20297 [D loss: 0.343116, acc.: 84.38%] [G loss: 3.232009]\n",
      "epoch:25 step:20298 [D loss: 0.275169, acc.: 88.28%] [G loss: 3.256963]\n",
      "epoch:25 step:20299 [D loss: 0.252114, acc.: 86.72%] [G loss: 3.311471]\n",
      "epoch:25 step:20300 [D loss: 0.426730, acc.: 78.12%] [G loss: 2.928592]\n",
      "epoch:25 step:20301 [D loss: 0.217695, acc.: 95.31%] [G loss: 2.481889]\n",
      "epoch:25 step:20302 [D loss: 0.371869, acc.: 83.59%] [G loss: 3.458555]\n",
      "epoch:25 step:20303 [D loss: 0.344735, acc.: 86.72%] [G loss: 3.402202]\n",
      "epoch:25 step:20304 [D loss: 0.247120, acc.: 90.62%] [G loss: 3.762857]\n",
      "epoch:25 step:20305 [D loss: 0.235982, acc.: 89.06%] [G loss: 3.048540]\n",
      "epoch:25 step:20306 [D loss: 0.335282, acc.: 83.59%] [G loss: 3.300386]\n",
      "epoch:26 step:20307 [D loss: 0.377683, acc.: 83.59%] [G loss: 4.172030]\n",
      "epoch:26 step:20308 [D loss: 0.165500, acc.: 95.31%] [G loss: 4.422289]\n",
      "epoch:26 step:20309 [D loss: 0.288518, acc.: 85.16%] [G loss: 4.360163]\n",
      "epoch:26 step:20310 [D loss: 0.340821, acc.: 82.81%] [G loss: 3.491697]\n",
      "epoch:26 step:20311 [D loss: 0.230728, acc.: 90.62%] [G loss: 3.265349]\n",
      "epoch:26 step:20312 [D loss: 0.376981, acc.: 87.50%] [G loss: 3.448705]\n",
      "epoch:26 step:20313 [D loss: 0.352661, acc.: 80.47%] [G loss: 3.624705]\n",
      "epoch:26 step:20314 [D loss: 0.292915, acc.: 88.28%] [G loss: 3.427722]\n",
      "epoch:26 step:20315 [D loss: 0.311039, acc.: 83.59%] [G loss: 2.888996]\n",
      "epoch:26 step:20316 [D loss: 0.273897, acc.: 87.50%] [G loss: 2.817450]\n",
      "epoch:26 step:20317 [D loss: 0.310436, acc.: 89.84%] [G loss: 2.377225]\n",
      "epoch:26 step:20318 [D loss: 0.253077, acc.: 91.41%] [G loss: 2.936409]\n",
      "epoch:26 step:20319 [D loss: 0.291933, acc.: 87.50%] [G loss: 2.327578]\n",
      "epoch:26 step:20320 [D loss: 0.300192, acc.: 87.50%] [G loss: 2.527020]\n",
      "epoch:26 step:20321 [D loss: 0.301589, acc.: 87.50%] [G loss: 2.429834]\n",
      "epoch:26 step:20322 [D loss: 0.333256, acc.: 83.59%] [G loss: 3.459653]\n",
      "epoch:26 step:20323 [D loss: 0.238671, acc.: 89.84%] [G loss: 3.320919]\n",
      "epoch:26 step:20324 [D loss: 0.367414, acc.: 83.59%] [G loss: 2.761508]\n",
      "epoch:26 step:20325 [D loss: 0.337212, acc.: 85.94%] [G loss: 3.171330]\n",
      "epoch:26 step:20326 [D loss: 0.305669, acc.: 85.16%] [G loss: 3.453930]\n",
      "epoch:26 step:20327 [D loss: 0.450019, acc.: 74.22%] [G loss: 3.096828]\n",
      "epoch:26 step:20328 [D loss: 0.320131, acc.: 84.38%] [G loss: 2.674848]\n",
      "epoch:26 step:20329 [D loss: 0.244011, acc.: 88.28%] [G loss: 4.854486]\n",
      "epoch:26 step:20330 [D loss: 0.435744, acc.: 80.47%] [G loss: 2.935623]\n",
      "epoch:26 step:20331 [D loss: 0.320690, acc.: 85.94%] [G loss: 3.464773]\n",
      "epoch:26 step:20332 [D loss: 0.347042, acc.: 85.94%] [G loss: 3.595537]\n",
      "epoch:26 step:20333 [D loss: 0.426329, acc.: 80.47%] [G loss: 3.126388]\n",
      "epoch:26 step:20334 [D loss: 0.392836, acc.: 83.59%] [G loss: 3.250611]\n",
      "epoch:26 step:20335 [D loss: 0.288987, acc.: 87.50%] [G loss: 3.008305]\n",
      "epoch:26 step:20336 [D loss: 0.401252, acc.: 82.03%] [G loss: 3.181554]\n",
      "epoch:26 step:20337 [D loss: 0.299199, acc.: 89.06%] [G loss: 3.308300]\n",
      "epoch:26 step:20338 [D loss: 0.321982, acc.: 87.50%] [G loss: 3.651078]\n",
      "epoch:26 step:20339 [D loss: 0.265849, acc.: 86.72%] [G loss: 4.506046]\n",
      "epoch:26 step:20340 [D loss: 0.417952, acc.: 78.91%] [G loss: 2.731550]\n",
      "epoch:26 step:20341 [D loss: 0.304146, acc.: 85.16%] [G loss: 3.371149]\n",
      "epoch:26 step:20342 [D loss: 0.333810, acc.: 85.94%] [G loss: 4.886434]\n",
      "epoch:26 step:20343 [D loss: 0.204594, acc.: 90.62%] [G loss: 4.992619]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:26 step:20344 [D loss: 0.297701, acc.: 86.72%] [G loss: 3.544574]\n",
      "epoch:26 step:20345 [D loss: 0.271545, acc.: 86.72%] [G loss: 3.084399]\n",
      "epoch:26 step:20346 [D loss: 0.283736, acc.: 89.06%] [G loss: 2.876846]\n",
      "epoch:26 step:20347 [D loss: 0.327612, acc.: 79.69%] [G loss: 3.489216]\n",
      "epoch:26 step:20348 [D loss: 0.255204, acc.: 90.62%] [G loss: 3.369994]\n",
      "epoch:26 step:20349 [D loss: 0.317204, acc.: 87.50%] [G loss: 4.854862]\n",
      "epoch:26 step:20350 [D loss: 0.423445, acc.: 81.25%] [G loss: 4.086395]\n",
      "epoch:26 step:20351 [D loss: 0.410034, acc.: 82.03%] [G loss: 3.629678]\n",
      "epoch:26 step:20352 [D loss: 0.251865, acc.: 88.28%] [G loss: 3.092200]\n",
      "epoch:26 step:20353 [D loss: 0.352274, acc.: 85.16%] [G loss: 3.638149]\n",
      "epoch:26 step:20354 [D loss: 0.228374, acc.: 90.62%] [G loss: 4.020380]\n",
      "epoch:26 step:20355 [D loss: 0.223901, acc.: 89.06%] [G loss: 2.986600]\n",
      "epoch:26 step:20356 [D loss: 0.322262, acc.: 85.94%] [G loss: 3.616050]\n",
      "epoch:26 step:20357 [D loss: 0.226980, acc.: 92.97%] [G loss: 3.121201]\n",
      "epoch:26 step:20358 [D loss: 0.212119, acc.: 92.97%] [G loss: 4.450386]\n",
      "epoch:26 step:20359 [D loss: 0.418630, acc.: 81.25%] [G loss: 2.763387]\n",
      "epoch:26 step:20360 [D loss: 0.316349, acc.: 85.94%] [G loss: 4.503235]\n",
      "epoch:26 step:20361 [D loss: 0.317441, acc.: 82.81%] [G loss: 3.139266]\n",
      "epoch:26 step:20362 [D loss: 0.301699, acc.: 86.72%] [G loss: 5.160166]\n",
      "epoch:26 step:20363 [D loss: 0.342664, acc.: 85.16%] [G loss: 2.982086]\n",
      "epoch:26 step:20364 [D loss: 0.262323, acc.: 85.94%] [G loss: 3.283439]\n",
      "epoch:26 step:20365 [D loss: 0.278583, acc.: 85.16%] [G loss: 3.468791]\n",
      "epoch:26 step:20366 [D loss: 0.356111, acc.: 83.59%] [G loss: 2.822844]\n",
      "epoch:26 step:20367 [D loss: 0.328015, acc.: 86.72%] [G loss: 4.886744]\n",
      "epoch:26 step:20368 [D loss: 0.366408, acc.: 82.03%] [G loss: 2.963542]\n",
      "epoch:26 step:20369 [D loss: 0.355024, acc.: 83.59%] [G loss: 3.705130]\n",
      "epoch:26 step:20370 [D loss: 0.287207, acc.: 88.28%] [G loss: 4.157587]\n",
      "epoch:26 step:20371 [D loss: 0.395827, acc.: 81.25%] [G loss: 4.432584]\n",
      "epoch:26 step:20372 [D loss: 0.345016, acc.: 86.72%] [G loss: 5.181410]\n",
      "epoch:26 step:20373 [D loss: 0.338505, acc.: 85.94%] [G loss: 6.180878]\n",
      "epoch:26 step:20374 [D loss: 0.274370, acc.: 90.62%] [G loss: 3.089725]\n",
      "epoch:26 step:20375 [D loss: 0.248033, acc.: 89.84%] [G loss: 3.391379]\n",
      "epoch:26 step:20376 [D loss: 0.249917, acc.: 87.50%] [G loss: 3.456570]\n",
      "epoch:26 step:20377 [D loss: 0.290324, acc.: 89.84%] [G loss: 3.033597]\n",
      "epoch:26 step:20378 [D loss: 0.256164, acc.: 91.41%] [G loss: 3.218340]\n",
      "epoch:26 step:20379 [D loss: 0.317226, acc.: 85.94%] [G loss: 4.058405]\n",
      "epoch:26 step:20380 [D loss: 0.530073, acc.: 76.56%] [G loss: 4.234646]\n",
      "epoch:26 step:20381 [D loss: 0.653411, acc.: 75.00%] [G loss: 3.876250]\n",
      "epoch:26 step:20382 [D loss: 0.433777, acc.: 78.12%] [G loss: 3.922391]\n",
      "epoch:26 step:20383 [D loss: 0.256539, acc.: 86.72%] [G loss: 4.354346]\n",
      "epoch:26 step:20384 [D loss: 0.464022, acc.: 79.69%] [G loss: 3.834057]\n",
      "epoch:26 step:20385 [D loss: 0.329760, acc.: 85.16%] [G loss: 3.778432]\n",
      "epoch:26 step:20386 [D loss: 0.310050, acc.: 86.72%] [G loss: 3.514831]\n",
      "epoch:26 step:20387 [D loss: 0.452106, acc.: 83.59%] [G loss: 6.297673]\n",
      "epoch:26 step:20388 [D loss: 0.691779, acc.: 75.00%] [G loss: 4.211484]\n",
      "epoch:26 step:20389 [D loss: 0.514513, acc.: 76.56%] [G loss: 3.627388]\n",
      "epoch:26 step:20390 [D loss: 0.352305, acc.: 87.50%] [G loss: 4.341763]\n",
      "epoch:26 step:20391 [D loss: 0.344003, acc.: 85.94%] [G loss: 2.928581]\n",
      "epoch:26 step:20392 [D loss: 0.448511, acc.: 80.47%] [G loss: 2.231709]\n",
      "epoch:26 step:20393 [D loss: 0.357055, acc.: 82.81%] [G loss: 2.919238]\n",
      "epoch:26 step:20394 [D loss: 0.361883, acc.: 83.59%] [G loss: 3.126101]\n",
      "epoch:26 step:20395 [D loss: 0.369428, acc.: 83.59%] [G loss: 3.556331]\n",
      "epoch:26 step:20396 [D loss: 0.453200, acc.: 78.12%] [G loss: 2.085375]\n",
      "epoch:26 step:20397 [D loss: 0.277800, acc.: 86.72%] [G loss: 2.799080]\n",
      "epoch:26 step:20398 [D loss: 0.354582, acc.: 84.38%] [G loss: 3.366872]\n",
      "epoch:26 step:20399 [D loss: 0.519103, acc.: 68.75%] [G loss: 3.515886]\n",
      "epoch:26 step:20400 [D loss: 0.300770, acc.: 85.94%] [G loss: 3.096312]\n",
      "##############\n",
      "[0.86311092 0.87446559 0.81496445 0.78332935 0.77642187 0.82510769\n",
      " 0.85724574 0.81051204 0.80309443 0.81447642]\n",
      "##########\n",
      "epoch:26 step:20401 [D loss: 0.505935, acc.: 78.91%] [G loss: 3.001602]\n",
      "epoch:26 step:20402 [D loss: 0.407633, acc.: 79.69%] [G loss: 2.734108]\n",
      "epoch:26 step:20403 [D loss: 0.251944, acc.: 89.06%] [G loss: 2.465460]\n",
      "epoch:26 step:20404 [D loss: 0.291790, acc.: 90.62%] [G loss: 3.789084]\n",
      "epoch:26 step:20405 [D loss: 0.281565, acc.: 85.16%] [G loss: 4.414019]\n",
      "epoch:26 step:20406 [D loss: 0.280470, acc.: 87.50%] [G loss: 2.988142]\n",
      "epoch:26 step:20407 [D loss: 0.468505, acc.: 79.69%] [G loss: 3.012479]\n",
      "epoch:26 step:20408 [D loss: 0.269819, acc.: 89.06%] [G loss: 2.796654]\n",
      "epoch:26 step:20409 [D loss: 0.352052, acc.: 86.72%] [G loss: 3.510064]\n",
      "epoch:26 step:20410 [D loss: 0.191015, acc.: 94.53%] [G loss: 3.703345]\n",
      "epoch:26 step:20411 [D loss: 0.214226, acc.: 92.19%] [G loss: 4.175910]\n",
      "epoch:26 step:20412 [D loss: 0.253012, acc.: 89.06%] [G loss: 2.875505]\n",
      "epoch:26 step:20413 [D loss: 0.221148, acc.: 90.62%] [G loss: 4.340037]\n",
      "epoch:26 step:20414 [D loss: 0.325395, acc.: 85.16%] [G loss: 3.363773]\n",
      "epoch:26 step:20415 [D loss: 0.248483, acc.: 93.75%] [G loss: 3.039780]\n",
      "epoch:26 step:20416 [D loss: 0.305328, acc.: 83.59%] [G loss: 4.319925]\n",
      "epoch:26 step:20417 [D loss: 0.385652, acc.: 79.69%] [G loss: 5.526431]\n",
      "epoch:26 step:20418 [D loss: 0.272808, acc.: 87.50%] [G loss: 2.513967]\n",
      "epoch:26 step:20419 [D loss: 0.365645, acc.: 84.38%] [G loss: 4.872837]\n",
      "epoch:26 step:20420 [D loss: 0.340259, acc.: 85.94%] [G loss: 2.621953]\n",
      "epoch:26 step:20421 [D loss: 0.368858, acc.: 82.03%] [G loss: 3.111324]\n",
      "epoch:26 step:20422 [D loss: 0.290033, acc.: 89.84%] [G loss: 3.295564]\n",
      "epoch:26 step:20423 [D loss: 0.387503, acc.: 85.16%] [G loss: 3.458882]\n",
      "epoch:26 step:20424 [D loss: 0.285081, acc.: 85.94%] [G loss: 4.880824]\n",
      "epoch:26 step:20425 [D loss: 0.318003, acc.: 83.59%] [G loss: 3.521837]\n",
      "epoch:26 step:20426 [D loss: 0.357511, acc.: 82.81%] [G loss: 3.605421]\n",
      "epoch:26 step:20427 [D loss: 0.194144, acc.: 92.19%] [G loss: 3.898708]\n",
      "epoch:26 step:20428 [D loss: 0.368224, acc.: 81.25%] [G loss: 3.411621]\n",
      "epoch:26 step:20429 [D loss: 0.246341, acc.: 87.50%] [G loss: 2.596478]\n",
      "epoch:26 step:20430 [D loss: 0.371629, acc.: 85.16%] [G loss: 2.389782]\n",
      "epoch:26 step:20431 [D loss: 0.383687, acc.: 81.25%] [G loss: 3.225891]\n",
      "epoch:26 step:20432 [D loss: 0.323542, acc.: 86.72%] [G loss: 2.807559]\n",
      "epoch:26 step:20433 [D loss: 0.325717, acc.: 81.25%] [G loss: 2.998369]\n",
      "epoch:26 step:20434 [D loss: 0.256692, acc.: 91.41%] [G loss: 2.683907]\n",
      "epoch:26 step:20435 [D loss: 0.306553, acc.: 89.06%] [G loss: 4.136707]\n",
      "epoch:26 step:20436 [D loss: 0.224253, acc.: 90.62%] [G loss: 2.977051]\n",
      "epoch:26 step:20437 [D loss: 0.325002, acc.: 85.16%] [G loss: 2.854119]\n",
      "epoch:26 step:20438 [D loss: 0.366336, acc.: 90.62%] [G loss: 2.709921]\n",
      "epoch:26 step:20439 [D loss: 0.375714, acc.: 82.81%] [G loss: 3.008297]\n",
      "epoch:26 step:20440 [D loss: 0.357131, acc.: 86.72%] [G loss: 2.540632]\n",
      "epoch:26 step:20441 [D loss: 0.278560, acc.: 86.72%] [G loss: 2.872281]\n",
      "epoch:26 step:20442 [D loss: 0.269475, acc.: 89.06%] [G loss: 3.087123]\n",
      "epoch:26 step:20443 [D loss: 0.395453, acc.: 78.91%] [G loss: 2.416667]\n",
      "epoch:26 step:20444 [D loss: 0.306829, acc.: 85.94%] [G loss: 2.702294]\n",
      "epoch:26 step:20445 [D loss: 0.258709, acc.: 89.06%] [G loss: 3.162059]\n",
      "epoch:26 step:20446 [D loss: 0.243452, acc.: 89.84%] [G loss: 3.728744]\n",
      "epoch:26 step:20447 [D loss: 0.348558, acc.: 85.16%] [G loss: 3.336504]\n",
      "epoch:26 step:20448 [D loss: 0.387205, acc.: 84.38%] [G loss: 2.926513]\n",
      "epoch:26 step:20449 [D loss: 0.362974, acc.: 84.38%] [G loss: 2.739032]\n",
      "epoch:26 step:20450 [D loss: 0.377259, acc.: 86.72%] [G loss: 2.805013]\n",
      "epoch:26 step:20451 [D loss: 0.316563, acc.: 89.06%] [G loss: 2.294681]\n",
      "epoch:26 step:20452 [D loss: 0.328419, acc.: 82.81%] [G loss: 2.767529]\n",
      "epoch:26 step:20453 [D loss: 0.345193, acc.: 84.38%] [G loss: 3.459014]\n",
      "epoch:26 step:20454 [D loss: 0.376533, acc.: 82.81%] [G loss: 2.497516]\n",
      "epoch:26 step:20455 [D loss: 0.350260, acc.: 82.81%] [G loss: 3.636457]\n",
      "epoch:26 step:20456 [D loss: 0.328233, acc.: 82.81%] [G loss: 4.378286]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:26 step:20457 [D loss: 0.339150, acc.: 81.25%] [G loss: 3.173054]\n",
      "epoch:26 step:20458 [D loss: 0.528396, acc.: 76.56%] [G loss: 4.410898]\n",
      "epoch:26 step:20459 [D loss: 0.306621, acc.: 85.16%] [G loss: 4.248884]\n",
      "epoch:26 step:20460 [D loss: 0.211800, acc.: 93.75%] [G loss: 5.514572]\n",
      "epoch:26 step:20461 [D loss: 0.288594, acc.: 86.72%] [G loss: 4.300983]\n",
      "epoch:26 step:20462 [D loss: 0.460206, acc.: 80.47%] [G loss: 3.291190]\n",
      "epoch:26 step:20463 [D loss: 0.392618, acc.: 82.03%] [G loss: 3.876491]\n",
      "epoch:26 step:20464 [D loss: 0.276656, acc.: 90.62%] [G loss: 5.016912]\n",
      "epoch:26 step:20465 [D loss: 0.156547, acc.: 94.53%] [G loss: 4.207563]\n",
      "epoch:26 step:20466 [D loss: 0.365342, acc.: 82.81%] [G loss: 3.709979]\n",
      "epoch:26 step:20467 [D loss: 0.288669, acc.: 89.84%] [G loss: 5.340619]\n",
      "epoch:26 step:20468 [D loss: 0.304073, acc.: 89.06%] [G loss: 3.275901]\n",
      "epoch:26 step:20469 [D loss: 0.228330, acc.: 89.84%] [G loss: 4.617421]\n",
      "epoch:26 step:20470 [D loss: 0.338497, acc.: 85.16%] [G loss: 3.124276]\n",
      "epoch:26 step:20471 [D loss: 0.303782, acc.: 88.28%] [G loss: 3.227138]\n",
      "epoch:26 step:20472 [D loss: 0.279269, acc.: 88.28%] [G loss: 3.494293]\n",
      "epoch:26 step:20473 [D loss: 0.391066, acc.: 80.47%] [G loss: 2.250962]\n",
      "epoch:26 step:20474 [D loss: 0.409337, acc.: 82.81%] [G loss: 2.281945]\n",
      "epoch:26 step:20475 [D loss: 0.229130, acc.: 90.62%] [G loss: 3.548638]\n",
      "epoch:26 step:20476 [D loss: 0.330424, acc.: 83.59%] [G loss: 3.701009]\n",
      "epoch:26 step:20477 [D loss: 0.309759, acc.: 85.94%] [G loss: 5.538846]\n",
      "epoch:26 step:20478 [D loss: 0.286737, acc.: 88.28%] [G loss: 3.902749]\n",
      "epoch:26 step:20479 [D loss: 0.277963, acc.: 85.16%] [G loss: 3.317686]\n",
      "epoch:26 step:20480 [D loss: 0.416057, acc.: 78.91%] [G loss: 3.366813]\n",
      "epoch:26 step:20481 [D loss: 0.354111, acc.: 82.03%] [G loss: 3.284834]\n",
      "epoch:26 step:20482 [D loss: 0.373869, acc.: 79.69%] [G loss: 3.513453]\n",
      "epoch:26 step:20483 [D loss: 0.304104, acc.: 85.16%] [G loss: 4.035384]\n",
      "epoch:26 step:20484 [D loss: 0.354142, acc.: 85.16%] [G loss: 3.326906]\n",
      "epoch:26 step:20485 [D loss: 0.341139, acc.: 85.16%] [G loss: 3.598411]\n",
      "epoch:26 step:20486 [D loss: 0.404961, acc.: 79.69%] [G loss: 3.577221]\n",
      "epoch:26 step:20487 [D loss: 0.278443, acc.: 88.28%] [G loss: 3.157205]\n",
      "epoch:26 step:20488 [D loss: 0.361836, acc.: 82.03%] [G loss: 3.902133]\n",
      "epoch:26 step:20489 [D loss: 0.357972, acc.: 84.38%] [G loss: 4.249287]\n",
      "epoch:26 step:20490 [D loss: 0.398322, acc.: 82.81%] [G loss: 3.603044]\n",
      "epoch:26 step:20491 [D loss: 0.339006, acc.: 83.59%] [G loss: 2.977552]\n",
      "epoch:26 step:20492 [D loss: 0.306759, acc.: 87.50%] [G loss: 3.264759]\n",
      "epoch:26 step:20493 [D loss: 0.335882, acc.: 84.38%] [G loss: 3.578826]\n",
      "epoch:26 step:20494 [D loss: 0.375618, acc.: 82.03%] [G loss: 2.919227]\n",
      "epoch:26 step:20495 [D loss: 0.388309, acc.: 83.59%] [G loss: 2.425434]\n",
      "epoch:26 step:20496 [D loss: 0.354079, acc.: 85.94%] [G loss: 4.260917]\n",
      "epoch:26 step:20497 [D loss: 0.414555, acc.: 82.03%] [G loss: 5.587234]\n",
      "epoch:26 step:20498 [D loss: 0.395787, acc.: 82.81%] [G loss: 3.729411]\n",
      "epoch:26 step:20499 [D loss: 0.348024, acc.: 82.81%] [G loss: 4.122246]\n",
      "epoch:26 step:20500 [D loss: 0.273850, acc.: 87.50%] [G loss: 6.970826]\n",
      "epoch:26 step:20501 [D loss: 0.222680, acc.: 92.97%] [G loss: 4.923116]\n",
      "epoch:26 step:20502 [D loss: 0.253689, acc.: 89.06%] [G loss: 5.129249]\n",
      "epoch:26 step:20503 [D loss: 0.353602, acc.: 85.16%] [G loss: 4.702070]\n",
      "epoch:26 step:20504 [D loss: 0.293816, acc.: 89.06%] [G loss: 2.584136]\n",
      "epoch:26 step:20505 [D loss: 0.265255, acc.: 89.06%] [G loss: 4.047645]\n",
      "epoch:26 step:20506 [D loss: 0.309223, acc.: 85.16%] [G loss: 3.090429]\n",
      "epoch:26 step:20507 [D loss: 0.388934, acc.: 82.81%] [G loss: 3.181620]\n",
      "epoch:26 step:20508 [D loss: 0.399501, acc.: 83.59%] [G loss: 2.772153]\n",
      "epoch:26 step:20509 [D loss: 0.317750, acc.: 88.28%] [G loss: 2.894834]\n",
      "epoch:26 step:20510 [D loss: 0.267132, acc.: 90.62%] [G loss: 3.496827]\n",
      "epoch:26 step:20511 [D loss: 0.358109, acc.: 87.50%] [G loss: 3.628500]\n",
      "epoch:26 step:20512 [D loss: 0.385654, acc.: 81.25%] [G loss: 2.131581]\n",
      "epoch:26 step:20513 [D loss: 0.429561, acc.: 80.47%] [G loss: 2.956384]\n",
      "epoch:26 step:20514 [D loss: 0.460819, acc.: 79.69%] [G loss: 9.296178]\n",
      "epoch:26 step:20515 [D loss: 1.160786, acc.: 57.81%] [G loss: 5.232718]\n",
      "epoch:26 step:20516 [D loss: 0.487375, acc.: 78.91%] [G loss: 3.121700]\n",
      "epoch:26 step:20517 [D loss: 0.566779, acc.: 75.78%] [G loss: 4.583750]\n",
      "epoch:26 step:20518 [D loss: 0.431749, acc.: 78.91%] [G loss: 3.276155]\n",
      "epoch:26 step:20519 [D loss: 0.316972, acc.: 88.28%] [G loss: 3.643821]\n",
      "epoch:26 step:20520 [D loss: 0.398628, acc.: 80.47%] [G loss: 2.970167]\n",
      "epoch:26 step:20521 [D loss: 0.431717, acc.: 76.56%] [G loss: 3.295540]\n",
      "epoch:26 step:20522 [D loss: 0.510803, acc.: 71.88%] [G loss: 2.968216]\n",
      "epoch:26 step:20523 [D loss: 0.322009, acc.: 86.72%] [G loss: 2.805344]\n",
      "epoch:26 step:20524 [D loss: 0.293668, acc.: 86.72%] [G loss: 3.706714]\n",
      "epoch:26 step:20525 [D loss: 0.370856, acc.: 82.81%] [G loss: 3.146813]\n",
      "epoch:26 step:20526 [D loss: 0.388481, acc.: 83.59%] [G loss: 3.367309]\n",
      "epoch:26 step:20527 [D loss: 0.198593, acc.: 92.97%] [G loss: 2.983477]\n",
      "epoch:26 step:20528 [D loss: 0.339863, acc.: 82.81%] [G loss: 3.425575]\n",
      "epoch:26 step:20529 [D loss: 0.243066, acc.: 89.84%] [G loss: 5.452039]\n",
      "epoch:26 step:20530 [D loss: 0.302228, acc.: 84.38%] [G loss: 4.281776]\n",
      "epoch:26 step:20531 [D loss: 0.326355, acc.: 85.16%] [G loss: 3.950969]\n",
      "epoch:26 step:20532 [D loss: 0.214208, acc.: 92.19%] [G loss: 3.839045]\n",
      "epoch:26 step:20533 [D loss: 0.331988, acc.: 85.16%] [G loss: 3.196667]\n",
      "epoch:26 step:20534 [D loss: 0.371089, acc.: 82.03%] [G loss: 3.024803]\n",
      "epoch:26 step:20535 [D loss: 0.233435, acc.: 89.06%] [G loss: 3.450430]\n",
      "epoch:26 step:20536 [D loss: 0.308128, acc.: 85.16%] [G loss: 2.926482]\n",
      "epoch:26 step:20537 [D loss: 0.297170, acc.: 86.72%] [G loss: 3.015548]\n",
      "epoch:26 step:20538 [D loss: 0.297405, acc.: 89.06%] [G loss: 3.216938]\n",
      "epoch:26 step:20539 [D loss: 0.327102, acc.: 89.06%] [G loss: 2.260574]\n",
      "epoch:26 step:20540 [D loss: 0.252938, acc.: 91.41%] [G loss: 3.010115]\n",
      "epoch:26 step:20541 [D loss: 0.285253, acc.: 88.28%] [G loss: 3.504891]\n",
      "epoch:26 step:20542 [D loss: 0.452418, acc.: 74.22%] [G loss: 2.833399]\n",
      "epoch:26 step:20543 [D loss: 0.302906, acc.: 89.06%] [G loss: 2.707652]\n",
      "epoch:26 step:20544 [D loss: 0.364581, acc.: 83.59%] [G loss: 2.804939]\n",
      "epoch:26 step:20545 [D loss: 0.215488, acc.: 92.97%] [G loss: 2.678148]\n",
      "epoch:26 step:20546 [D loss: 0.323133, acc.: 84.38%] [G loss: 2.677074]\n",
      "epoch:26 step:20547 [D loss: 0.331086, acc.: 82.81%] [G loss: 3.142016]\n",
      "epoch:26 step:20548 [D loss: 0.334585, acc.: 87.50%] [G loss: 4.120529]\n",
      "epoch:26 step:20549 [D loss: 0.311094, acc.: 83.59%] [G loss: 4.384562]\n",
      "epoch:26 step:20550 [D loss: 0.271712, acc.: 89.06%] [G loss: 3.180288]\n",
      "epoch:26 step:20551 [D loss: 0.234363, acc.: 92.19%] [G loss: 3.197352]\n",
      "epoch:26 step:20552 [D loss: 0.312843, acc.: 87.50%] [G loss: 3.553157]\n",
      "epoch:26 step:20553 [D loss: 0.287138, acc.: 89.06%] [G loss: 5.040958]\n",
      "epoch:26 step:20554 [D loss: 0.224945, acc.: 90.62%] [G loss: 4.930068]\n",
      "epoch:26 step:20555 [D loss: 0.356922, acc.: 80.47%] [G loss: 4.991923]\n",
      "epoch:26 step:20556 [D loss: 0.295802, acc.: 85.16%] [G loss: 4.237693]\n",
      "epoch:26 step:20557 [D loss: 0.311007, acc.: 86.72%] [G loss: 4.185860]\n",
      "epoch:26 step:20558 [D loss: 0.260638, acc.: 87.50%] [G loss: 3.630559]\n",
      "epoch:26 step:20559 [D loss: 0.291580, acc.: 86.72%] [G loss: 3.655796]\n",
      "epoch:26 step:20560 [D loss: 0.178840, acc.: 95.31%] [G loss: 4.694655]\n",
      "epoch:26 step:20561 [D loss: 0.296618, acc.: 89.84%] [G loss: 5.148711]\n",
      "epoch:26 step:20562 [D loss: 0.517140, acc.: 76.56%] [G loss: 3.115364]\n",
      "epoch:26 step:20563 [D loss: 0.335969, acc.: 85.94%] [G loss: 3.383373]\n",
      "epoch:26 step:20564 [D loss: 0.260292, acc.: 88.28%] [G loss: 3.902436]\n",
      "epoch:26 step:20565 [D loss: 0.330823, acc.: 84.38%] [G loss: 2.560587]\n",
      "epoch:26 step:20566 [D loss: 0.286555, acc.: 86.72%] [G loss: 2.862248]\n",
      "epoch:26 step:20567 [D loss: 0.355623, acc.: 83.59%] [G loss: 2.893864]\n",
      "epoch:26 step:20568 [D loss: 0.350223, acc.: 83.59%] [G loss: 2.229753]\n",
      "epoch:26 step:20569 [D loss: 0.267700, acc.: 84.38%] [G loss: 3.145098]\n",
      "epoch:26 step:20570 [D loss: 0.354060, acc.: 82.03%] [G loss: 2.796852]\n",
      "epoch:26 step:20571 [D loss: 0.407988, acc.: 78.12%] [G loss: 3.382003]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:26 step:20572 [D loss: 0.382998, acc.: 79.69%] [G loss: 2.785587]\n",
      "epoch:26 step:20573 [D loss: 0.426753, acc.: 78.12%] [G loss: 5.298561]\n",
      "epoch:26 step:20574 [D loss: 0.494479, acc.: 77.34%] [G loss: 4.621879]\n",
      "epoch:26 step:20575 [D loss: 0.280711, acc.: 90.62%] [G loss: 3.967916]\n",
      "epoch:26 step:20576 [D loss: 0.335668, acc.: 84.38%] [G loss: 4.285321]\n",
      "epoch:26 step:20577 [D loss: 0.152245, acc.: 93.75%] [G loss: 5.473479]\n",
      "epoch:26 step:20578 [D loss: 0.218034, acc.: 89.06%] [G loss: 3.751499]\n",
      "epoch:26 step:20579 [D loss: 0.234126, acc.: 87.50%] [G loss: 3.689205]\n",
      "epoch:26 step:20580 [D loss: 0.171750, acc.: 95.31%] [G loss: 3.922924]\n",
      "epoch:26 step:20581 [D loss: 0.427032, acc.: 76.56%] [G loss: 3.207879]\n",
      "epoch:26 step:20582 [D loss: 0.381043, acc.: 84.38%] [G loss: 3.447333]\n",
      "epoch:26 step:20583 [D loss: 0.318989, acc.: 85.94%] [G loss: 2.958486]\n",
      "epoch:26 step:20584 [D loss: 0.387028, acc.: 79.69%] [G loss: 3.736327]\n",
      "epoch:26 step:20585 [D loss: 0.260281, acc.: 89.06%] [G loss: 3.872298]\n",
      "epoch:26 step:20586 [D loss: 0.225028, acc.: 91.41%] [G loss: 3.630772]\n",
      "epoch:26 step:20587 [D loss: 0.395196, acc.: 82.03%] [G loss: 3.946290]\n",
      "epoch:26 step:20588 [D loss: 0.357227, acc.: 84.38%] [G loss: 3.551624]\n",
      "epoch:26 step:20589 [D loss: 0.303626, acc.: 85.16%] [G loss: 4.868370]\n",
      "epoch:26 step:20590 [D loss: 0.278803, acc.: 91.41%] [G loss: 3.093646]\n",
      "epoch:26 step:20591 [D loss: 0.362283, acc.: 87.50%] [G loss: 2.872703]\n",
      "epoch:26 step:20592 [D loss: 0.337068, acc.: 84.38%] [G loss: 3.129850]\n",
      "epoch:26 step:20593 [D loss: 0.314881, acc.: 84.38%] [G loss: 3.043870]\n",
      "epoch:26 step:20594 [D loss: 0.273306, acc.: 90.62%] [G loss: 5.828137]\n",
      "epoch:26 step:20595 [D loss: 0.274720, acc.: 89.06%] [G loss: 3.997346]\n",
      "epoch:26 step:20596 [D loss: 0.274000, acc.: 88.28%] [G loss: 2.790067]\n",
      "epoch:26 step:20597 [D loss: 0.266614, acc.: 88.28%] [G loss: 3.832257]\n",
      "epoch:26 step:20598 [D loss: 0.267287, acc.: 92.19%] [G loss: 2.848868]\n",
      "epoch:26 step:20599 [D loss: 0.353689, acc.: 85.16%] [G loss: 2.741358]\n",
      "epoch:26 step:20600 [D loss: 0.298592, acc.: 89.06%] [G loss: 3.425390]\n",
      "##############\n",
      "[0.84123562 0.8748934  0.80644973 0.80986971 0.75293823 0.82810439\n",
      " 0.87253568 0.84108914 0.82034161 0.84260536]\n",
      "##########\n",
      "epoch:26 step:20601 [D loss: 0.323207, acc.: 85.94%] [G loss: 3.474497]\n",
      "epoch:26 step:20602 [D loss: 0.333785, acc.: 86.72%] [G loss: 4.428819]\n",
      "epoch:26 step:20603 [D loss: 0.479274, acc.: 78.91%] [G loss: 3.188149]\n",
      "epoch:26 step:20604 [D loss: 0.316974, acc.: 84.38%] [G loss: 3.534281]\n",
      "epoch:26 step:20605 [D loss: 0.404665, acc.: 87.50%] [G loss: 4.736372]\n",
      "epoch:26 step:20606 [D loss: 0.352612, acc.: 85.94%] [G loss: 3.288773]\n",
      "epoch:26 step:20607 [D loss: 0.431623, acc.: 78.91%] [G loss: 5.511214]\n",
      "epoch:26 step:20608 [D loss: 0.285119, acc.: 85.16%] [G loss: 3.630321]\n",
      "epoch:26 step:20609 [D loss: 0.375616, acc.: 85.94%] [G loss: 3.493441]\n",
      "epoch:26 step:20610 [D loss: 0.365931, acc.: 84.38%] [G loss: 2.550173]\n",
      "epoch:26 step:20611 [D loss: 0.279805, acc.: 89.06%] [G loss: 3.371550]\n",
      "epoch:26 step:20612 [D loss: 0.302401, acc.: 87.50%] [G loss: 2.661266]\n",
      "epoch:26 step:20613 [D loss: 0.242893, acc.: 89.84%] [G loss: 3.382282]\n",
      "epoch:26 step:20614 [D loss: 0.313821, acc.: 83.59%] [G loss: 4.612899]\n",
      "epoch:26 step:20615 [D loss: 0.366267, acc.: 84.38%] [G loss: 4.193010]\n",
      "epoch:26 step:20616 [D loss: 0.181022, acc.: 95.31%] [G loss: 2.991884]\n",
      "epoch:26 step:20617 [D loss: 0.345571, acc.: 84.38%] [G loss: 4.004466]\n",
      "epoch:26 step:20618 [D loss: 0.439010, acc.: 81.25%] [G loss: 2.387573]\n",
      "epoch:26 step:20619 [D loss: 0.313831, acc.: 85.16%] [G loss: 2.925340]\n",
      "epoch:26 step:20620 [D loss: 0.318117, acc.: 82.81%] [G loss: 2.980109]\n",
      "epoch:26 step:20621 [D loss: 0.382543, acc.: 81.25%] [G loss: 3.578436]\n",
      "epoch:26 step:20622 [D loss: 0.290364, acc.: 86.72%] [G loss: 3.040142]\n",
      "epoch:26 step:20623 [D loss: 0.310175, acc.: 85.16%] [G loss: 2.722700]\n",
      "epoch:26 step:20624 [D loss: 0.300228, acc.: 88.28%] [G loss: 2.955842]\n",
      "epoch:26 step:20625 [D loss: 0.261480, acc.: 88.28%] [G loss: 2.489164]\n",
      "epoch:26 step:20626 [D loss: 0.350590, acc.: 84.38%] [G loss: 3.407595]\n",
      "epoch:26 step:20627 [D loss: 0.324596, acc.: 82.03%] [G loss: 2.961400]\n",
      "epoch:26 step:20628 [D loss: 0.319761, acc.: 84.38%] [G loss: 3.906858]\n",
      "epoch:26 step:20629 [D loss: 0.337045, acc.: 86.72%] [G loss: 2.400134]\n",
      "epoch:26 step:20630 [D loss: 0.322811, acc.: 86.72%] [G loss: 2.654999]\n",
      "epoch:26 step:20631 [D loss: 0.367438, acc.: 80.47%] [G loss: 2.736267]\n",
      "epoch:26 step:20632 [D loss: 0.331965, acc.: 87.50%] [G loss: 3.035762]\n",
      "epoch:26 step:20633 [D loss: 0.262808, acc.: 88.28%] [G loss: 2.864182]\n",
      "epoch:26 step:20634 [D loss: 0.259735, acc.: 88.28%] [G loss: 2.566583]\n",
      "epoch:26 step:20635 [D loss: 0.319746, acc.: 85.16%] [G loss: 2.787088]\n",
      "epoch:26 step:20636 [D loss: 0.279484, acc.: 88.28%] [G loss: 3.100773]\n",
      "epoch:26 step:20637 [D loss: 0.266672, acc.: 89.06%] [G loss: 3.429892]\n",
      "epoch:26 step:20638 [D loss: 0.277607, acc.: 86.72%] [G loss: 3.537875]\n",
      "epoch:26 step:20639 [D loss: 0.326207, acc.: 88.28%] [G loss: 3.088624]\n",
      "epoch:26 step:20640 [D loss: 0.411006, acc.: 78.91%] [G loss: 3.547919]\n",
      "epoch:26 step:20641 [D loss: 0.254525, acc.: 92.19%] [G loss: 3.425270]\n",
      "epoch:26 step:20642 [D loss: 0.371180, acc.: 81.25%] [G loss: 3.235029]\n",
      "epoch:26 step:20643 [D loss: 0.386735, acc.: 85.94%] [G loss: 2.988373]\n",
      "epoch:26 step:20644 [D loss: 0.231277, acc.: 91.41%] [G loss: 2.857026]\n",
      "epoch:26 step:20645 [D loss: 0.230590, acc.: 91.41%] [G loss: 3.856192]\n",
      "epoch:26 step:20646 [D loss: 0.331926, acc.: 82.81%] [G loss: 3.920820]\n",
      "epoch:26 step:20647 [D loss: 0.239476, acc.: 88.28%] [G loss: 4.977891]\n",
      "epoch:26 step:20648 [D loss: 0.282667, acc.: 85.16%] [G loss: 2.464232]\n",
      "epoch:26 step:20649 [D loss: 0.314418, acc.: 80.47%] [G loss: 3.451111]\n",
      "epoch:26 step:20650 [D loss: 0.327269, acc.: 85.94%] [G loss: 4.915826]\n",
      "epoch:26 step:20651 [D loss: 0.262009, acc.: 89.84%] [G loss: 3.113222]\n",
      "epoch:26 step:20652 [D loss: 0.344631, acc.: 83.59%] [G loss: 3.235671]\n",
      "epoch:26 step:20653 [D loss: 0.274476, acc.: 85.16%] [G loss: 3.649623]\n",
      "epoch:26 step:20654 [D loss: 0.339023, acc.: 82.03%] [G loss: 3.092633]\n",
      "epoch:26 step:20655 [D loss: 0.344975, acc.: 85.16%] [G loss: 3.077667]\n",
      "epoch:26 step:20656 [D loss: 0.339357, acc.: 82.81%] [G loss: 3.571778]\n",
      "epoch:26 step:20657 [D loss: 0.194917, acc.: 92.19%] [G loss: 3.395682]\n",
      "epoch:26 step:20658 [D loss: 0.318078, acc.: 83.59%] [G loss: 4.100122]\n",
      "epoch:26 step:20659 [D loss: 0.328694, acc.: 83.59%] [G loss: 3.613858]\n",
      "epoch:26 step:20660 [D loss: 0.250280, acc.: 89.06%] [G loss: 2.807258]\n",
      "epoch:26 step:20661 [D loss: 0.286908, acc.: 85.16%] [G loss: 3.910529]\n",
      "epoch:26 step:20662 [D loss: 0.335115, acc.: 84.38%] [G loss: 3.436620]\n",
      "epoch:26 step:20663 [D loss: 0.303046, acc.: 89.06%] [G loss: 3.358988]\n",
      "epoch:26 step:20664 [D loss: 0.270175, acc.: 85.16%] [G loss: 3.221622]\n",
      "epoch:26 step:20665 [D loss: 0.284950, acc.: 86.72%] [G loss: 2.883523]\n",
      "epoch:26 step:20666 [D loss: 0.418922, acc.: 81.25%] [G loss: 3.059672]\n",
      "epoch:26 step:20667 [D loss: 0.325295, acc.: 86.72%] [G loss: 3.032520]\n",
      "epoch:26 step:20668 [D loss: 0.296380, acc.: 89.84%] [G loss: 4.819289]\n",
      "epoch:26 step:20669 [D loss: 0.252833, acc.: 87.50%] [G loss: 4.337203]\n",
      "epoch:26 step:20670 [D loss: 0.284793, acc.: 89.06%] [G loss: 3.388230]\n",
      "epoch:26 step:20671 [D loss: 0.194481, acc.: 88.28%] [G loss: 3.953853]\n",
      "epoch:26 step:20672 [D loss: 0.272336, acc.: 89.84%] [G loss: 4.060324]\n",
      "epoch:26 step:20673 [D loss: 0.304609, acc.: 85.16%] [G loss: 3.492761]\n",
      "epoch:26 step:20674 [D loss: 0.332988, acc.: 81.25%] [G loss: 3.664526]\n",
      "epoch:26 step:20675 [D loss: 0.334124, acc.: 86.72%] [G loss: 2.421296]\n",
      "epoch:26 step:20676 [D loss: 0.338515, acc.: 87.50%] [G loss: 3.337649]\n",
      "epoch:26 step:20677 [D loss: 0.287667, acc.: 89.06%] [G loss: 2.671119]\n",
      "epoch:26 step:20678 [D loss: 0.291659, acc.: 87.50%] [G loss: 3.726406]\n",
      "epoch:26 step:20679 [D loss: 0.290889, acc.: 86.72%] [G loss: 7.487244]\n",
      "epoch:26 step:20680 [D loss: 0.386664, acc.: 82.03%] [G loss: 4.609950]\n",
      "epoch:26 step:20681 [D loss: 0.266503, acc.: 88.28%] [G loss: 3.874032]\n",
      "epoch:26 step:20682 [D loss: 0.211569, acc.: 91.41%] [G loss: 3.976008]\n",
      "epoch:26 step:20683 [D loss: 0.201624, acc.: 91.41%] [G loss: 4.489730]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:26 step:20684 [D loss: 0.343497, acc.: 86.72%] [G loss: 3.905304]\n",
      "epoch:26 step:20685 [D loss: 0.363429, acc.: 82.03%] [G loss: 4.214455]\n",
      "epoch:26 step:20686 [D loss: 0.310606, acc.: 87.50%] [G loss: 3.590487]\n",
      "epoch:26 step:20687 [D loss: 0.309100, acc.: 86.72%] [G loss: 3.927952]\n",
      "epoch:26 step:20688 [D loss: 0.257759, acc.: 88.28%] [G loss: 3.008961]\n",
      "epoch:26 step:20689 [D loss: 0.211798, acc.: 92.19%] [G loss: 3.403302]\n",
      "epoch:26 step:20690 [D loss: 0.329973, acc.: 85.94%] [G loss: 3.186356]\n",
      "epoch:26 step:20691 [D loss: 0.304187, acc.: 84.38%] [G loss: 3.386518]\n",
      "epoch:26 step:20692 [D loss: 0.398302, acc.: 82.03%] [G loss: 3.092215]\n",
      "epoch:26 step:20693 [D loss: 0.267608, acc.: 89.84%] [G loss: 3.100334]\n",
      "epoch:26 step:20694 [D loss: 0.355616, acc.: 83.59%] [G loss: 4.882796]\n",
      "epoch:26 step:20695 [D loss: 0.353043, acc.: 82.81%] [G loss: 4.687219]\n",
      "epoch:26 step:20696 [D loss: 0.371835, acc.: 82.81%] [G loss: 3.105027]\n",
      "epoch:26 step:20697 [D loss: 0.338822, acc.: 84.38%] [G loss: 2.795448]\n",
      "epoch:26 step:20698 [D loss: 0.314076, acc.: 87.50%] [G loss: 4.412065]\n",
      "epoch:26 step:20699 [D loss: 0.235577, acc.: 89.84%] [G loss: 3.722359]\n",
      "epoch:26 step:20700 [D loss: 0.298040, acc.: 87.50%] [G loss: 4.298505]\n",
      "epoch:26 step:20701 [D loss: 0.365444, acc.: 84.38%] [G loss: 3.230812]\n",
      "epoch:26 step:20702 [D loss: 0.338800, acc.: 82.81%] [G loss: 4.041207]\n",
      "epoch:26 step:20703 [D loss: 0.440516, acc.: 78.91%] [G loss: 5.607653]\n",
      "epoch:26 step:20704 [D loss: 0.544624, acc.: 75.78%] [G loss: 4.566317]\n",
      "epoch:26 step:20705 [D loss: 0.406238, acc.: 81.25%] [G loss: 3.497049]\n",
      "epoch:26 step:20706 [D loss: 0.441977, acc.: 78.12%] [G loss: 5.376925]\n",
      "epoch:26 step:20707 [D loss: 0.360219, acc.: 82.03%] [G loss: 4.415313]\n",
      "epoch:26 step:20708 [D loss: 0.156059, acc.: 96.09%] [G loss: 3.090688]\n",
      "epoch:26 step:20709 [D loss: 0.282327, acc.: 85.16%] [G loss: 4.784647]\n",
      "epoch:26 step:20710 [D loss: 0.263323, acc.: 87.50%] [G loss: 3.203639]\n",
      "epoch:26 step:20711 [D loss: 0.413348, acc.: 83.59%] [G loss: 2.659239]\n",
      "epoch:26 step:20712 [D loss: 0.379401, acc.: 83.59%] [G loss: 2.954905]\n",
      "epoch:26 step:20713 [D loss: 0.389733, acc.: 83.59%] [G loss: 5.929168]\n",
      "epoch:26 step:20714 [D loss: 0.434448, acc.: 77.34%] [G loss: 3.323748]\n",
      "epoch:26 step:20715 [D loss: 0.277542, acc.: 87.50%] [G loss: 3.319695]\n",
      "epoch:26 step:20716 [D loss: 0.218545, acc.: 90.62%] [G loss: 4.889145]\n",
      "epoch:26 step:20717 [D loss: 0.345129, acc.: 84.38%] [G loss: 3.095816]\n",
      "epoch:26 step:20718 [D loss: 0.306757, acc.: 86.72%] [G loss: 4.507522]\n",
      "epoch:26 step:20719 [D loss: 0.347671, acc.: 84.38%] [G loss: 3.739364]\n",
      "epoch:26 step:20720 [D loss: 0.367638, acc.: 82.81%] [G loss: 6.048620]\n",
      "epoch:26 step:20721 [D loss: 0.426555, acc.: 79.69%] [G loss: 4.730840]\n",
      "epoch:26 step:20722 [D loss: 0.377907, acc.: 85.16%] [G loss: 3.689712]\n",
      "epoch:26 step:20723 [D loss: 0.249200, acc.: 92.97%] [G loss: 3.449819]\n",
      "epoch:26 step:20724 [D loss: 0.410355, acc.: 85.94%] [G loss: 3.077419]\n",
      "epoch:26 step:20725 [D loss: 0.402938, acc.: 83.59%] [G loss: 3.345366]\n",
      "epoch:26 step:20726 [D loss: 0.328827, acc.: 82.81%] [G loss: 3.764955]\n",
      "epoch:26 step:20727 [D loss: 0.274786, acc.: 89.06%] [G loss: 3.797880]\n",
      "epoch:26 step:20728 [D loss: 0.285581, acc.: 86.72%] [G loss: 3.815540]\n",
      "epoch:26 step:20729 [D loss: 0.329855, acc.: 84.38%] [G loss: 2.631565]\n",
      "epoch:26 step:20730 [D loss: 0.304862, acc.: 87.50%] [G loss: 3.106348]\n",
      "epoch:26 step:20731 [D loss: 0.367169, acc.: 80.47%] [G loss: 2.893586]\n",
      "epoch:26 step:20732 [D loss: 0.261416, acc.: 90.62%] [G loss: 3.719657]\n",
      "epoch:26 step:20733 [D loss: 0.332142, acc.: 84.38%] [G loss: 3.724301]\n",
      "epoch:26 step:20734 [D loss: 0.301509, acc.: 86.72%] [G loss: 3.268463]\n",
      "epoch:26 step:20735 [D loss: 0.243602, acc.: 91.41%] [G loss: 3.280657]\n",
      "epoch:26 step:20736 [D loss: 0.305145, acc.: 86.72%] [G loss: 3.036927]\n",
      "epoch:26 step:20737 [D loss: 0.445157, acc.: 76.56%] [G loss: 2.287570]\n",
      "epoch:26 step:20738 [D loss: 0.438399, acc.: 78.91%] [G loss: 3.639672]\n",
      "epoch:26 step:20739 [D loss: 0.327647, acc.: 89.06%] [G loss: 3.337291]\n",
      "epoch:26 step:20740 [D loss: 0.415543, acc.: 82.03%] [G loss: 5.567379]\n",
      "epoch:26 step:20741 [D loss: 0.557827, acc.: 75.00%] [G loss: 7.617624]\n",
      "epoch:26 step:20742 [D loss: 1.414730, acc.: 69.53%] [G loss: 10.727718]\n",
      "epoch:26 step:20743 [D loss: 1.677829, acc.: 65.62%] [G loss: 5.277599]\n",
      "epoch:26 step:20744 [D loss: 0.790250, acc.: 66.41%] [G loss: 3.924729]\n",
      "epoch:26 step:20745 [D loss: 0.298342, acc.: 90.62%] [G loss: 4.137673]\n",
      "epoch:26 step:20746 [D loss: 0.473304, acc.: 82.03%] [G loss: 3.883587]\n",
      "epoch:26 step:20747 [D loss: 0.428455, acc.: 82.81%] [G loss: 3.983691]\n",
      "epoch:26 step:20748 [D loss: 0.376096, acc.: 84.38%] [G loss: 2.121171]\n",
      "epoch:26 step:20749 [D loss: 0.280647, acc.: 89.84%] [G loss: 3.367247]\n",
      "epoch:26 step:20750 [D loss: 0.325891, acc.: 85.16%] [G loss: 2.450480]\n",
      "epoch:26 step:20751 [D loss: 0.368601, acc.: 82.03%] [G loss: 2.886323]\n",
      "epoch:26 step:20752 [D loss: 0.425987, acc.: 78.91%] [G loss: 3.198596]\n",
      "epoch:26 step:20753 [D loss: 0.266084, acc.: 89.06%] [G loss: 3.464706]\n",
      "epoch:26 step:20754 [D loss: 0.295749, acc.: 88.28%] [G loss: 4.297686]\n",
      "epoch:26 step:20755 [D loss: 0.371652, acc.: 85.94%] [G loss: 3.345616]\n",
      "epoch:26 step:20756 [D loss: 0.290611, acc.: 88.28%] [G loss: 2.857472]\n",
      "epoch:26 step:20757 [D loss: 0.263227, acc.: 87.50%] [G loss: 3.021745]\n",
      "epoch:26 step:20758 [D loss: 0.261962, acc.: 91.41%] [G loss: 3.641008]\n",
      "epoch:26 step:20759 [D loss: 0.242047, acc.: 91.41%] [G loss: 2.804335]\n",
      "epoch:26 step:20760 [D loss: 0.278128, acc.: 86.72%] [G loss: 2.474244]\n",
      "epoch:26 step:20761 [D loss: 0.304022, acc.: 86.72%] [G loss: 2.792662]\n",
      "epoch:26 step:20762 [D loss: 0.308688, acc.: 86.72%] [G loss: 2.491840]\n",
      "epoch:26 step:20763 [D loss: 0.309559, acc.: 85.16%] [G loss: 2.910305]\n",
      "epoch:26 step:20764 [D loss: 0.219177, acc.: 90.62%] [G loss: 2.662414]\n",
      "epoch:26 step:20765 [D loss: 0.263258, acc.: 86.72%] [G loss: 3.314743]\n",
      "epoch:26 step:20766 [D loss: 0.377840, acc.: 82.81%] [G loss: 2.868253]\n",
      "epoch:26 step:20767 [D loss: 0.386563, acc.: 82.81%] [G loss: 2.542515]\n",
      "epoch:26 step:20768 [D loss: 0.257314, acc.: 92.19%] [G loss: 2.634723]\n",
      "epoch:26 step:20769 [D loss: 0.400040, acc.: 83.59%] [G loss: 3.075303]\n",
      "epoch:26 step:20770 [D loss: 0.345118, acc.: 84.38%] [G loss: 2.757999]\n",
      "epoch:26 step:20771 [D loss: 0.357995, acc.: 83.59%] [G loss: 2.481533]\n",
      "epoch:26 step:20772 [D loss: 0.377857, acc.: 82.81%] [G loss: 2.693594]\n",
      "epoch:26 step:20773 [D loss: 0.363579, acc.: 81.25%] [G loss: 2.384492]\n",
      "epoch:26 step:20774 [D loss: 0.374533, acc.: 80.47%] [G loss: 2.636284]\n",
      "epoch:26 step:20775 [D loss: 0.198840, acc.: 90.62%] [G loss: 3.733400]\n",
      "epoch:26 step:20776 [D loss: 0.316255, acc.: 85.16%] [G loss: 2.574709]\n",
      "epoch:26 step:20777 [D loss: 0.255753, acc.: 90.62%] [G loss: 2.562585]\n",
      "epoch:26 step:20778 [D loss: 0.302598, acc.: 91.41%] [G loss: 2.675589]\n",
      "epoch:26 step:20779 [D loss: 0.411831, acc.: 81.25%] [G loss: 2.191727]\n",
      "epoch:26 step:20780 [D loss: 0.323741, acc.: 89.06%] [G loss: 2.171905]\n",
      "epoch:26 step:20781 [D loss: 0.375859, acc.: 85.94%] [G loss: 2.084240]\n",
      "epoch:26 step:20782 [D loss: 0.360696, acc.: 82.03%] [G loss: 2.420755]\n",
      "epoch:26 step:20783 [D loss: 0.305733, acc.: 87.50%] [G loss: 2.517260]\n",
      "epoch:26 step:20784 [D loss: 0.357242, acc.: 86.72%] [G loss: 2.771286]\n",
      "epoch:26 step:20785 [D loss: 0.327880, acc.: 86.72%] [G loss: 3.212654]\n",
      "epoch:26 step:20786 [D loss: 0.301385, acc.: 86.72%] [G loss: 3.068758]\n",
      "epoch:26 step:20787 [D loss: 0.316371, acc.: 89.84%] [G loss: 3.316436]\n",
      "epoch:26 step:20788 [D loss: 0.221383, acc.: 89.06%] [G loss: 4.040319]\n",
      "epoch:26 step:20789 [D loss: 0.369826, acc.: 84.38%] [G loss: 3.379676]\n",
      "epoch:26 step:20790 [D loss: 0.281327, acc.: 87.50%] [G loss: 2.966975]\n",
      "epoch:26 step:20791 [D loss: 0.267784, acc.: 89.06%] [G loss: 3.116436]\n",
      "epoch:26 step:20792 [D loss: 0.298855, acc.: 85.16%] [G loss: 2.390095]\n",
      "epoch:26 step:20793 [D loss: 0.228113, acc.: 90.62%] [G loss: 3.225729]\n",
      "epoch:26 step:20794 [D loss: 0.253622, acc.: 91.41%] [G loss: 2.292934]\n",
      "epoch:26 step:20795 [D loss: 0.349801, acc.: 82.03%] [G loss: 3.084083]\n",
      "epoch:26 step:20796 [D loss: 0.239205, acc.: 89.06%] [G loss: 2.822490]\n",
      "epoch:26 step:20797 [D loss: 0.177779, acc.: 94.53%] [G loss: 2.667495]\n",
      "epoch:26 step:20798 [D loss: 0.351688, acc.: 84.38%] [G loss: 2.721370]\n",
      "epoch:26 step:20799 [D loss: 0.285742, acc.: 88.28%] [G loss: 2.841233]\n",
      "epoch:26 step:20800 [D loss: 0.338272, acc.: 85.16%] [G loss: 2.981410]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##############\n",
      "[0.86941831 0.8662966  0.80729942 0.81619128 0.75003851 0.8303383\n",
      " 0.87376623 0.81734342 0.8249361  0.83454652]\n",
      "##########\n",
      "epoch:26 step:20801 [D loss: 0.455720, acc.: 82.03%] [G loss: 2.310189]\n",
      "epoch:26 step:20802 [D loss: 0.274097, acc.: 85.94%] [G loss: 2.493243]\n",
      "epoch:26 step:20803 [D loss: 0.352446, acc.: 82.81%] [G loss: 2.895874]\n",
      "epoch:26 step:20804 [D loss: 0.311424, acc.: 85.16%] [G loss: 3.413031]\n",
      "epoch:26 step:20805 [D loss: 0.299445, acc.: 86.72%] [G loss: 3.465119]\n",
      "epoch:26 step:20806 [D loss: 0.351196, acc.: 83.59%] [G loss: 3.498621]\n",
      "epoch:26 step:20807 [D loss: 0.353234, acc.: 83.59%] [G loss: 3.591063]\n",
      "epoch:26 step:20808 [D loss: 0.322499, acc.: 87.50%] [G loss: 3.841081]\n",
      "epoch:26 step:20809 [D loss: 0.201023, acc.: 91.41%] [G loss: 5.975371]\n",
      "epoch:26 step:20810 [D loss: 0.311506, acc.: 85.94%] [G loss: 2.932411]\n",
      "epoch:26 step:20811 [D loss: 0.312418, acc.: 85.94%] [G loss: 2.824344]\n",
      "epoch:26 step:20812 [D loss: 0.324360, acc.: 85.94%] [G loss: 2.886730]\n",
      "epoch:26 step:20813 [D loss: 0.281348, acc.: 88.28%] [G loss: 3.189730]\n",
      "epoch:26 step:20814 [D loss: 0.377640, acc.: 79.69%] [G loss: 4.606368]\n",
      "epoch:26 step:20815 [D loss: 0.300484, acc.: 85.94%] [G loss: 4.586842]\n",
      "epoch:26 step:20816 [D loss: 0.372910, acc.: 83.59%] [G loss: 3.367045]\n",
      "epoch:26 step:20817 [D loss: 0.263918, acc.: 86.72%] [G loss: 3.756065]\n",
      "epoch:26 step:20818 [D loss: 0.290258, acc.: 90.62%] [G loss: 2.883090]\n",
      "epoch:26 step:20819 [D loss: 0.273399, acc.: 85.94%] [G loss: 3.884612]\n",
      "epoch:26 step:20820 [D loss: 0.294020, acc.: 89.06%] [G loss: 3.361883]\n",
      "epoch:26 step:20821 [D loss: 0.344773, acc.: 83.59%] [G loss: 3.171745]\n",
      "epoch:26 step:20822 [D loss: 0.279496, acc.: 88.28%] [G loss: 2.571595]\n",
      "epoch:26 step:20823 [D loss: 0.194622, acc.: 92.97%] [G loss: 4.072321]\n",
      "epoch:26 step:20824 [D loss: 0.270438, acc.: 85.16%] [G loss: 2.794205]\n",
      "epoch:26 step:20825 [D loss: 0.335893, acc.: 85.16%] [G loss: 2.925488]\n",
      "epoch:26 step:20826 [D loss: 0.317882, acc.: 85.94%] [G loss: 2.649306]\n",
      "epoch:26 step:20827 [D loss: 0.261899, acc.: 89.84%] [G loss: 2.847351]\n",
      "epoch:26 step:20828 [D loss: 0.329331, acc.: 83.59%] [G loss: 2.678404]\n",
      "epoch:26 step:20829 [D loss: 0.359155, acc.: 84.38%] [G loss: 3.142628]\n",
      "epoch:26 step:20830 [D loss: 0.325480, acc.: 82.03%] [G loss: 2.279704]\n",
      "epoch:26 step:20831 [D loss: 0.449635, acc.: 82.81%] [G loss: 3.378737]\n",
      "epoch:26 step:20832 [D loss: 0.300594, acc.: 88.28%] [G loss: 4.111594]\n",
      "epoch:26 step:20833 [D loss: 0.261545, acc.: 89.06%] [G loss: 3.437345]\n",
      "epoch:26 step:20834 [D loss: 0.233979, acc.: 90.62%] [G loss: 3.576874]\n",
      "epoch:26 step:20835 [D loss: 0.273011, acc.: 86.72%] [G loss: 3.521924]\n",
      "epoch:26 step:20836 [D loss: 0.308917, acc.: 88.28%] [G loss: 2.988943]\n",
      "epoch:26 step:20837 [D loss: 0.299811, acc.: 87.50%] [G loss: 3.634369]\n",
      "epoch:26 step:20838 [D loss: 0.272422, acc.: 86.72%] [G loss: 3.197836]\n",
      "epoch:26 step:20839 [D loss: 0.210928, acc.: 91.41%] [G loss: 4.314310]\n",
      "epoch:26 step:20840 [D loss: 0.416746, acc.: 80.47%] [G loss: 4.207676]\n",
      "epoch:26 step:20841 [D loss: 0.254411, acc.: 87.50%] [G loss: 3.694370]\n",
      "epoch:26 step:20842 [D loss: 0.282020, acc.: 87.50%] [G loss: 4.706913]\n",
      "epoch:26 step:20843 [D loss: 0.410380, acc.: 79.69%] [G loss: 5.835800]\n",
      "epoch:26 step:20844 [D loss: 0.321701, acc.: 89.06%] [G loss: 4.400379]\n",
      "epoch:26 step:20845 [D loss: 0.372254, acc.: 78.12%] [G loss: 3.796115]\n",
      "epoch:26 step:20846 [D loss: 0.303366, acc.: 86.72%] [G loss: 4.038098]\n",
      "epoch:26 step:20847 [D loss: 0.279822, acc.: 89.06%] [G loss: 4.095319]\n",
      "epoch:26 step:20848 [D loss: 0.399013, acc.: 79.69%] [G loss: 3.744952]\n",
      "epoch:26 step:20849 [D loss: 0.331246, acc.: 85.94%] [G loss: 3.799102]\n",
      "epoch:26 step:20850 [D loss: 0.354754, acc.: 84.38%] [G loss: 3.718365]\n",
      "epoch:26 step:20851 [D loss: 0.285185, acc.: 87.50%] [G loss: 4.651056]\n",
      "epoch:26 step:20852 [D loss: 0.273540, acc.: 90.62%] [G loss: 3.498856]\n",
      "epoch:26 step:20853 [D loss: 0.377310, acc.: 76.56%] [G loss: 4.085644]\n",
      "epoch:26 step:20854 [D loss: 0.238258, acc.: 89.84%] [G loss: 4.131269]\n",
      "epoch:26 step:20855 [D loss: 0.297600, acc.: 84.38%] [G loss: 4.084188]\n",
      "epoch:26 step:20856 [D loss: 0.354056, acc.: 83.59%] [G loss: 3.472448]\n",
      "epoch:26 step:20857 [D loss: 0.311997, acc.: 86.72%] [G loss: 3.676990]\n",
      "epoch:26 step:20858 [D loss: 0.318980, acc.: 83.59%] [G loss: 3.425630]\n",
      "epoch:26 step:20859 [D loss: 0.258779, acc.: 87.50%] [G loss: 3.939189]\n",
      "epoch:26 step:20860 [D loss: 0.339286, acc.: 85.94%] [G loss: 3.060888]\n",
      "epoch:26 step:20861 [D loss: 0.354403, acc.: 83.59%] [G loss: 3.504157]\n",
      "epoch:26 step:20862 [D loss: 0.432820, acc.: 77.34%] [G loss: 4.385306]\n",
      "epoch:26 step:20863 [D loss: 0.343332, acc.: 85.16%] [G loss: 3.194135]\n",
      "epoch:26 step:20864 [D loss: 0.295760, acc.: 86.72%] [G loss: 3.185119]\n",
      "epoch:26 step:20865 [D loss: 0.319540, acc.: 87.50%] [G loss: 3.086285]\n",
      "epoch:26 step:20866 [D loss: 0.289734, acc.: 89.84%] [G loss: 2.979291]\n",
      "epoch:26 step:20867 [D loss: 0.289529, acc.: 87.50%] [G loss: 2.401480]\n",
      "epoch:26 step:20868 [D loss: 0.342316, acc.: 80.47%] [G loss: 2.662390]\n",
      "epoch:26 step:20869 [D loss: 0.378621, acc.: 85.16%] [G loss: 3.488997]\n",
      "epoch:26 step:20870 [D loss: 0.248081, acc.: 92.19%] [G loss: 3.437710]\n",
      "epoch:26 step:20871 [D loss: 0.329258, acc.: 85.94%] [G loss: 3.359681]\n",
      "epoch:26 step:20872 [D loss: 0.397322, acc.: 84.38%] [G loss: 4.046682]\n",
      "epoch:26 step:20873 [D loss: 0.349624, acc.: 87.50%] [G loss: 2.656159]\n",
      "epoch:26 step:20874 [D loss: 0.348010, acc.: 82.81%] [G loss: 4.122292]\n",
      "epoch:26 step:20875 [D loss: 0.348387, acc.: 85.94%] [G loss: 5.155747]\n",
      "epoch:26 step:20876 [D loss: 0.343805, acc.: 81.25%] [G loss: 2.950864]\n",
      "epoch:26 step:20877 [D loss: 0.317404, acc.: 89.84%] [G loss: 2.369064]\n",
      "epoch:26 step:20878 [D loss: 0.359162, acc.: 81.25%] [G loss: 2.759444]\n",
      "epoch:26 step:20879 [D loss: 0.290673, acc.: 89.06%] [G loss: 3.067925]\n",
      "epoch:26 step:20880 [D loss: 0.290012, acc.: 85.16%] [G loss: 2.833579]\n",
      "epoch:26 step:20881 [D loss: 0.352485, acc.: 82.03%] [G loss: 2.986721]\n",
      "epoch:26 step:20882 [D loss: 0.291123, acc.: 90.62%] [G loss: 3.130206]\n",
      "epoch:26 step:20883 [D loss: 0.330375, acc.: 85.16%] [G loss: 2.608459]\n",
      "epoch:26 step:20884 [D loss: 0.227926, acc.: 92.97%] [G loss: 2.703789]\n",
      "epoch:26 step:20885 [D loss: 0.372703, acc.: 82.81%] [G loss: 2.948937]\n",
      "epoch:26 step:20886 [D loss: 0.409611, acc.: 82.03%] [G loss: 2.833135]\n",
      "epoch:26 step:20887 [D loss: 0.314180, acc.: 89.06%] [G loss: 2.970024]\n",
      "epoch:26 step:20888 [D loss: 0.276242, acc.: 88.28%] [G loss: 3.061399]\n",
      "epoch:26 step:20889 [D loss: 0.447855, acc.: 77.34%] [G loss: 3.063466]\n",
      "epoch:26 step:20890 [D loss: 0.287193, acc.: 88.28%] [G loss: 4.028221]\n",
      "epoch:26 step:20891 [D loss: 0.307937, acc.: 86.72%] [G loss: 2.862902]\n",
      "epoch:26 step:20892 [D loss: 0.360616, acc.: 82.81%] [G loss: 3.939946]\n",
      "epoch:26 step:20893 [D loss: 0.227147, acc.: 92.97%] [G loss: 3.484019]\n",
      "epoch:26 step:20894 [D loss: 0.355211, acc.: 82.81%] [G loss: 3.429245]\n",
      "epoch:26 step:20895 [D loss: 0.434653, acc.: 82.03%] [G loss: 3.623871]\n",
      "epoch:26 step:20896 [D loss: 0.419805, acc.: 83.59%] [G loss: 4.249339]\n",
      "epoch:26 step:20897 [D loss: 0.362181, acc.: 83.59%] [G loss: 3.407254]\n",
      "epoch:26 step:20898 [D loss: 0.229100, acc.: 92.97%] [G loss: 3.689592]\n",
      "epoch:26 step:20899 [D loss: 0.350776, acc.: 85.94%] [G loss: 3.791818]\n",
      "epoch:26 step:20900 [D loss: 0.336558, acc.: 85.16%] [G loss: 3.003744]\n",
      "epoch:26 step:20901 [D loss: 0.309781, acc.: 88.28%] [G loss: 5.268306]\n",
      "epoch:26 step:20902 [D loss: 0.319226, acc.: 89.06%] [G loss: 3.672789]\n",
      "epoch:26 step:20903 [D loss: 0.373341, acc.: 86.72%] [G loss: 2.564021]\n",
      "epoch:26 step:20904 [D loss: 0.333098, acc.: 84.38%] [G loss: 3.955941]\n",
      "epoch:26 step:20905 [D loss: 0.342584, acc.: 85.94%] [G loss: 5.749756]\n",
      "epoch:26 step:20906 [D loss: 0.260689, acc.: 86.72%] [G loss: 4.708034]\n",
      "epoch:26 step:20907 [D loss: 0.262658, acc.: 87.50%] [G loss: 4.448843]\n",
      "epoch:26 step:20908 [D loss: 0.302630, acc.: 85.16%] [G loss: 2.408235]\n",
      "epoch:26 step:20909 [D loss: 0.272310, acc.: 87.50%] [G loss: 4.067739]\n",
      "epoch:26 step:20910 [D loss: 0.395918, acc.: 77.34%] [G loss: 3.259377]\n",
      "epoch:26 step:20911 [D loss: 0.353912, acc.: 83.59%] [G loss: 2.762604]\n",
      "epoch:26 step:20912 [D loss: 0.320707, acc.: 86.72%] [G loss: 3.413114]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:26 step:20913 [D loss: 0.389925, acc.: 82.03%] [G loss: 2.426808]\n",
      "epoch:26 step:20914 [D loss: 0.331104, acc.: 84.38%] [G loss: 4.608517]\n",
      "epoch:26 step:20915 [D loss: 0.437550, acc.: 79.69%] [G loss: 3.742401]\n",
      "epoch:26 step:20916 [D loss: 0.408580, acc.: 80.47%] [G loss: 3.600528]\n",
      "epoch:26 step:20917 [D loss: 0.364038, acc.: 82.81%] [G loss: 4.426014]\n",
      "epoch:26 step:20918 [D loss: 0.406565, acc.: 76.56%] [G loss: 3.139056]\n",
      "epoch:26 step:20919 [D loss: 0.366437, acc.: 81.25%] [G loss: 3.670978]\n",
      "epoch:26 step:20920 [D loss: 0.296809, acc.: 89.84%] [G loss: 2.916207]\n",
      "epoch:26 step:20921 [D loss: 0.331938, acc.: 82.81%] [G loss: 4.340784]\n",
      "epoch:26 step:20922 [D loss: 0.298778, acc.: 90.62%] [G loss: 3.547631]\n",
      "epoch:26 step:20923 [D loss: 0.347494, acc.: 85.16%] [G loss: 3.326620]\n",
      "epoch:26 step:20924 [D loss: 0.434043, acc.: 76.56%] [G loss: 3.820048]\n",
      "epoch:26 step:20925 [D loss: 0.346160, acc.: 86.72%] [G loss: 2.461232]\n",
      "epoch:26 step:20926 [D loss: 0.243018, acc.: 86.72%] [G loss: 3.660406]\n",
      "epoch:26 step:20927 [D loss: 0.296183, acc.: 86.72%] [G loss: 3.242260]\n",
      "epoch:26 step:20928 [D loss: 0.264325, acc.: 89.84%] [G loss: 2.839922]\n",
      "epoch:26 step:20929 [D loss: 0.349456, acc.: 88.28%] [G loss: 2.461558]\n",
      "epoch:26 step:20930 [D loss: 0.381399, acc.: 82.81%] [G loss: 2.928471]\n",
      "epoch:26 step:20931 [D loss: 0.297627, acc.: 88.28%] [G loss: 3.503158]\n",
      "epoch:26 step:20932 [D loss: 0.179094, acc.: 93.75%] [G loss: 3.988211]\n",
      "epoch:26 step:20933 [D loss: 0.378015, acc.: 85.16%] [G loss: 4.170858]\n",
      "epoch:26 step:20934 [D loss: 0.322220, acc.: 83.59%] [G loss: 5.577696]\n",
      "epoch:26 step:20935 [D loss: 0.233207, acc.: 93.75%] [G loss: 4.297346]\n",
      "epoch:26 step:20936 [D loss: 0.288605, acc.: 83.59%] [G loss: 3.530539]\n",
      "epoch:26 step:20937 [D loss: 0.325997, acc.: 86.72%] [G loss: 4.945875]\n",
      "epoch:26 step:20938 [D loss: 0.368221, acc.: 83.59%] [G loss: 5.014718]\n",
      "epoch:26 step:20939 [D loss: 0.271613, acc.: 87.50%] [G loss: 5.105282]\n",
      "epoch:26 step:20940 [D loss: 0.419060, acc.: 82.81%] [G loss: 5.267828]\n",
      "epoch:26 step:20941 [D loss: 0.335803, acc.: 86.72%] [G loss: 4.595659]\n",
      "epoch:26 step:20942 [D loss: 0.234049, acc.: 87.50%] [G loss: 7.519527]\n",
      "epoch:26 step:20943 [D loss: 0.230945, acc.: 87.50%] [G loss: 5.543454]\n",
      "epoch:26 step:20944 [D loss: 0.263642, acc.: 89.84%] [G loss: 3.891491]\n",
      "epoch:26 step:20945 [D loss: 0.257409, acc.: 89.06%] [G loss: 3.121723]\n",
      "epoch:26 step:20946 [D loss: 0.306816, acc.: 85.16%] [G loss: 3.119229]\n",
      "epoch:26 step:20947 [D loss: 0.228801, acc.: 89.84%] [G loss: 2.847336]\n",
      "epoch:26 step:20948 [D loss: 0.292979, acc.: 89.06%] [G loss: 3.284004]\n",
      "epoch:26 step:20949 [D loss: 0.356937, acc.: 80.47%] [G loss: 3.093862]\n",
      "epoch:26 step:20950 [D loss: 0.310812, acc.: 85.94%] [G loss: 4.324669]\n",
      "epoch:26 step:20951 [D loss: 0.286501, acc.: 85.16%] [G loss: 3.777868]\n",
      "epoch:26 step:20952 [D loss: 0.328177, acc.: 82.81%] [G loss: 2.727261]\n",
      "epoch:26 step:20953 [D loss: 0.216768, acc.: 90.62%] [G loss: 4.512907]\n",
      "epoch:26 step:20954 [D loss: 0.273484, acc.: 89.84%] [G loss: 3.801598]\n",
      "epoch:26 step:20955 [D loss: 0.317909, acc.: 85.16%] [G loss: 2.851911]\n",
      "epoch:26 step:20956 [D loss: 0.365823, acc.: 85.16%] [G loss: 2.876411]\n",
      "epoch:26 step:20957 [D loss: 0.338748, acc.: 88.28%] [G loss: 3.399015]\n",
      "epoch:26 step:20958 [D loss: 0.339956, acc.: 81.25%] [G loss: 3.280098]\n",
      "epoch:26 step:20959 [D loss: 0.303612, acc.: 87.50%] [G loss: 3.930350]\n",
      "epoch:26 step:20960 [D loss: 0.367310, acc.: 82.81%] [G loss: 2.710955]\n",
      "epoch:26 step:20961 [D loss: 0.203457, acc.: 89.06%] [G loss: 5.960759]\n",
      "epoch:26 step:20962 [D loss: 0.282641, acc.: 86.72%] [G loss: 3.069695]\n",
      "epoch:26 step:20963 [D loss: 0.333529, acc.: 85.16%] [G loss: 3.356689]\n",
      "epoch:26 step:20964 [D loss: 0.410898, acc.: 80.47%] [G loss: 2.715954]\n",
      "epoch:26 step:20965 [D loss: 0.273255, acc.: 86.72%] [G loss: 6.243942]\n",
      "epoch:26 step:20966 [D loss: 0.253888, acc.: 90.62%] [G loss: 4.709573]\n",
      "epoch:26 step:20967 [D loss: 0.341749, acc.: 87.50%] [G loss: 3.333630]\n",
      "epoch:26 step:20968 [D loss: 0.290590, acc.: 87.50%] [G loss: 5.432230]\n",
      "epoch:26 step:20969 [D loss: 0.269081, acc.: 88.28%] [G loss: 3.207178]\n",
      "epoch:26 step:20970 [D loss: 0.376063, acc.: 77.34%] [G loss: 4.020311]\n",
      "epoch:26 step:20971 [D loss: 0.276652, acc.: 89.84%] [G loss: 4.657712]\n",
      "epoch:26 step:20972 [D loss: 0.316412, acc.: 86.72%] [G loss: 3.855352]\n",
      "epoch:26 step:20973 [D loss: 0.237984, acc.: 90.62%] [G loss: 3.173497]\n",
      "epoch:26 step:20974 [D loss: 0.328980, acc.: 82.03%] [G loss: 3.495663]\n",
      "epoch:26 step:20975 [D loss: 0.252691, acc.: 88.28%] [G loss: 3.594219]\n",
      "epoch:26 step:20976 [D loss: 0.218830, acc.: 93.75%] [G loss: 5.117831]\n",
      "epoch:26 step:20977 [D loss: 0.275990, acc.: 88.28%] [G loss: 3.500437]\n",
      "epoch:26 step:20978 [D loss: 0.307963, acc.: 88.28%] [G loss: 4.457812]\n",
      "epoch:26 step:20979 [D loss: 0.345883, acc.: 84.38%] [G loss: 3.026176]\n",
      "epoch:26 step:20980 [D loss: 0.338523, acc.: 82.03%] [G loss: 3.840646]\n",
      "epoch:26 step:20981 [D loss: 0.425687, acc.: 83.59%] [G loss: 4.675450]\n",
      "epoch:26 step:20982 [D loss: 0.369929, acc.: 81.25%] [G loss: 3.560686]\n",
      "epoch:26 step:20983 [D loss: 0.346414, acc.: 84.38%] [G loss: 3.504060]\n",
      "epoch:26 step:20984 [D loss: 0.238469, acc.: 87.50%] [G loss: 2.766474]\n",
      "epoch:26 step:20985 [D loss: 0.299521, acc.: 88.28%] [G loss: 3.797877]\n",
      "epoch:26 step:20986 [D loss: 0.239397, acc.: 90.62%] [G loss: 3.747951]\n",
      "epoch:26 step:20987 [D loss: 0.241281, acc.: 88.28%] [G loss: 3.608678]\n",
      "epoch:26 step:20988 [D loss: 0.303816, acc.: 85.16%] [G loss: 4.128736]\n",
      "epoch:26 step:20989 [D loss: 0.318914, acc.: 85.94%] [G loss: 5.454997]\n",
      "epoch:26 step:20990 [D loss: 0.299051, acc.: 85.94%] [G loss: 5.049997]\n",
      "epoch:26 step:20991 [D loss: 0.262811, acc.: 92.19%] [G loss: 4.450818]\n",
      "epoch:26 step:20992 [D loss: 0.239223, acc.: 89.84%] [G loss: 4.581647]\n",
      "epoch:26 step:20993 [D loss: 0.265757, acc.: 89.06%] [G loss: 4.208794]\n",
      "epoch:26 step:20994 [D loss: 0.325480, acc.: 84.38%] [G loss: 3.587602]\n",
      "epoch:26 step:20995 [D loss: 0.282885, acc.: 87.50%] [G loss: 2.780865]\n",
      "epoch:26 step:20996 [D loss: 0.352617, acc.: 80.47%] [G loss: 3.018720]\n",
      "epoch:26 step:20997 [D loss: 0.259297, acc.: 89.84%] [G loss: 4.215806]\n",
      "epoch:26 step:20998 [D loss: 0.405301, acc.: 83.59%] [G loss: 3.730050]\n",
      "epoch:26 step:20999 [D loss: 0.495955, acc.: 79.69%] [G loss: 6.418434]\n",
      "epoch:26 step:21000 [D loss: 0.527797, acc.: 82.81%] [G loss: 6.343060]\n",
      "##############\n",
      "[0.86576491 0.86109382 0.78165551 0.80610127 0.77188049 0.82220095\n",
      " 0.88311385 0.82790482 0.80322152 0.81991442]\n",
      "##########\n",
      "epoch:26 step:21001 [D loss: 0.528392, acc.: 76.56%] [G loss: 3.836464]\n",
      "epoch:26 step:21002 [D loss: 0.334020, acc.: 85.16%] [G loss: 2.723225]\n",
      "epoch:26 step:21003 [D loss: 0.293453, acc.: 87.50%] [G loss: 3.814209]\n",
      "epoch:26 step:21004 [D loss: 0.217036, acc.: 92.97%] [G loss: 3.889127]\n",
      "epoch:26 step:21005 [D loss: 0.328557, acc.: 83.59%] [G loss: 3.148880]\n",
      "epoch:26 step:21006 [D loss: 0.262812, acc.: 90.62%] [G loss: 3.757914]\n",
      "epoch:26 step:21007 [D loss: 0.319665, acc.: 85.94%] [G loss: 3.081395]\n",
      "epoch:26 step:21008 [D loss: 0.184212, acc.: 92.97%] [G loss: 3.070327]\n",
      "epoch:26 step:21009 [D loss: 0.379544, acc.: 86.72%] [G loss: 3.298589]\n",
      "epoch:26 step:21010 [D loss: 0.299852, acc.: 85.94%] [G loss: 3.227608]\n",
      "epoch:26 step:21011 [D loss: 0.315791, acc.: 86.72%] [G loss: 3.690178]\n",
      "epoch:26 step:21012 [D loss: 0.277667, acc.: 88.28%] [G loss: 3.101938]\n",
      "epoch:26 step:21013 [D loss: 0.389542, acc.: 81.25%] [G loss: 4.951464]\n",
      "epoch:26 step:21014 [D loss: 0.315762, acc.: 85.16%] [G loss: 3.705303]\n",
      "epoch:26 step:21015 [D loss: 0.279796, acc.: 89.06%] [G loss: 4.387376]\n",
      "epoch:26 step:21016 [D loss: 0.282872, acc.: 83.59%] [G loss: 5.447351]\n",
      "epoch:26 step:21017 [D loss: 0.259356, acc.: 85.16%] [G loss: 3.921196]\n",
      "epoch:26 step:21018 [D loss: 0.388723, acc.: 81.25%] [G loss: 4.052422]\n",
      "epoch:26 step:21019 [D loss: 0.318311, acc.: 82.81%] [G loss: 3.238513]\n",
      "epoch:26 step:21020 [D loss: 0.403122, acc.: 83.59%] [G loss: 3.016258]\n",
      "epoch:26 step:21021 [D loss: 0.383767, acc.: 83.59%] [G loss: 3.159604]\n",
      "epoch:26 step:21022 [D loss: 0.269642, acc.: 88.28%] [G loss: 3.425962]\n",
      "epoch:26 step:21023 [D loss: 0.348489, acc.: 85.94%] [G loss: 2.665414]\n",
      "epoch:26 step:21024 [D loss: 0.394538, acc.: 83.59%] [G loss: 2.747139]\n",
      "epoch:26 step:21025 [D loss: 0.231197, acc.: 90.62%] [G loss: 3.413418]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:26 step:21026 [D loss: 0.307762, acc.: 85.94%] [G loss: 3.619972]\n",
      "epoch:26 step:21027 [D loss: 0.259957, acc.: 89.06%] [G loss: 3.312127]\n",
      "epoch:26 step:21028 [D loss: 0.269070, acc.: 85.16%] [G loss: 3.956436]\n",
      "epoch:26 step:21029 [D loss: 0.296819, acc.: 85.94%] [G loss: 4.427767]\n",
      "epoch:26 step:21030 [D loss: 0.186038, acc.: 92.97%] [G loss: 3.417535]\n",
      "epoch:26 step:21031 [D loss: 0.288131, acc.: 88.28%] [G loss: 2.792240]\n",
      "epoch:26 step:21032 [D loss: 0.242058, acc.: 91.41%] [G loss: 3.120265]\n",
      "epoch:26 step:21033 [D loss: 0.292463, acc.: 89.84%] [G loss: 2.697823]\n",
      "epoch:26 step:21034 [D loss: 0.329878, acc.: 85.94%] [G loss: 3.114797]\n",
      "epoch:26 step:21035 [D loss: 0.325020, acc.: 87.50%] [G loss: 3.177669]\n",
      "epoch:26 step:21036 [D loss: 0.258279, acc.: 85.94%] [G loss: 3.626515]\n",
      "epoch:26 step:21037 [D loss: 0.457681, acc.: 77.34%] [G loss: 3.203144]\n",
      "epoch:26 step:21038 [D loss: 0.284445, acc.: 85.16%] [G loss: 3.139171]\n",
      "epoch:26 step:21039 [D loss: 0.254050, acc.: 89.84%] [G loss: 3.680723]\n",
      "epoch:26 step:21040 [D loss: 0.343421, acc.: 84.38%] [G loss: 3.034815]\n",
      "epoch:26 step:21041 [D loss: 0.426691, acc.: 81.25%] [G loss: 3.319633]\n",
      "epoch:26 step:21042 [D loss: 0.403968, acc.: 82.81%] [G loss: 4.139956]\n",
      "epoch:26 step:21043 [D loss: 0.283448, acc.: 84.38%] [G loss: 3.583390]\n",
      "epoch:26 step:21044 [D loss: 0.333093, acc.: 85.16%] [G loss: 4.618910]\n",
      "epoch:26 step:21045 [D loss: 0.436426, acc.: 79.69%] [G loss: 5.598988]\n",
      "epoch:26 step:21046 [D loss: 0.384085, acc.: 81.25%] [G loss: 2.709563]\n",
      "epoch:26 step:21047 [D loss: 0.400010, acc.: 78.91%] [G loss: 3.421006]\n",
      "epoch:26 step:21048 [D loss: 0.254792, acc.: 89.06%] [G loss: 3.563585]\n",
      "epoch:26 step:21049 [D loss: 0.270571, acc.: 89.06%] [G loss: 3.721474]\n",
      "epoch:26 step:21050 [D loss: 0.226011, acc.: 90.62%] [G loss: 4.384293]\n",
      "epoch:26 step:21051 [D loss: 0.343945, acc.: 82.81%] [G loss: 4.970907]\n",
      "epoch:26 step:21052 [D loss: 0.209277, acc.: 89.84%] [G loss: 3.725326]\n",
      "epoch:26 step:21053 [D loss: 0.316085, acc.: 82.03%] [G loss: 3.992548]\n",
      "epoch:26 step:21054 [D loss: 0.373010, acc.: 84.38%] [G loss: 2.732937]\n",
      "epoch:26 step:21055 [D loss: 0.250850, acc.: 89.06%] [G loss: 3.202302]\n",
      "epoch:26 step:21056 [D loss: 0.344251, acc.: 85.94%] [G loss: 2.988514]\n",
      "epoch:26 step:21057 [D loss: 0.399338, acc.: 82.81%] [G loss: 2.905448]\n",
      "epoch:26 step:21058 [D loss: 0.404876, acc.: 84.38%] [G loss: 5.489251]\n",
      "epoch:26 step:21059 [D loss: 0.460157, acc.: 80.47%] [G loss: 6.464755]\n",
      "epoch:26 step:21060 [D loss: 0.465388, acc.: 74.22%] [G loss: 3.174109]\n",
      "epoch:26 step:21061 [D loss: 0.259642, acc.: 85.94%] [G loss: 4.818626]\n",
      "epoch:26 step:21062 [D loss: 0.361036, acc.: 82.03%] [G loss: 3.069239]\n",
      "epoch:26 step:21063 [D loss: 0.249467, acc.: 89.84%] [G loss: 3.703053]\n",
      "epoch:26 step:21064 [D loss: 0.406634, acc.: 86.72%] [G loss: 2.348372]\n",
      "epoch:26 step:21065 [D loss: 0.277437, acc.: 89.84%] [G loss: 3.096528]\n",
      "epoch:26 step:21066 [D loss: 0.357321, acc.: 84.38%] [G loss: 3.603730]\n",
      "epoch:26 step:21067 [D loss: 0.317806, acc.: 87.50%] [G loss: 3.975669]\n",
      "epoch:26 step:21068 [D loss: 0.361684, acc.: 82.03%] [G loss: 3.553541]\n",
      "epoch:26 step:21069 [D loss: 0.336832, acc.: 84.38%] [G loss: 2.941003]\n",
      "epoch:26 step:21070 [D loss: 0.373598, acc.: 88.28%] [G loss: 3.096931]\n",
      "epoch:26 step:21071 [D loss: 0.520545, acc.: 75.00%] [G loss: 4.890792]\n",
      "epoch:26 step:21072 [D loss: 0.352161, acc.: 82.81%] [G loss: 4.071780]\n",
      "epoch:26 step:21073 [D loss: 0.248475, acc.: 85.94%] [G loss: 5.342605]\n",
      "epoch:26 step:21074 [D loss: 0.218640, acc.: 89.84%] [G loss: 4.974326]\n",
      "epoch:26 step:21075 [D loss: 0.357533, acc.: 78.12%] [G loss: 3.506948]\n",
      "epoch:26 step:21076 [D loss: 0.281515, acc.: 85.94%] [G loss: 4.448510]\n",
      "epoch:26 step:21077 [D loss: 0.386037, acc.: 82.03%] [G loss: 3.418155]\n",
      "epoch:26 step:21078 [D loss: 0.182795, acc.: 96.09%] [G loss: 4.848162]\n",
      "epoch:26 step:21079 [D loss: 0.402263, acc.: 83.59%] [G loss: 2.904493]\n",
      "epoch:26 step:21080 [D loss: 0.278058, acc.: 89.06%] [G loss: 4.389961]\n",
      "epoch:26 step:21081 [D loss: 0.269932, acc.: 89.06%] [G loss: 4.115938]\n",
      "epoch:26 step:21082 [D loss: 0.292594, acc.: 85.94%] [G loss: 3.246335]\n",
      "epoch:26 step:21083 [D loss: 0.262163, acc.: 85.94%] [G loss: 3.305263]\n",
      "epoch:26 step:21084 [D loss: 0.286363, acc.: 88.28%] [G loss: 3.348272]\n",
      "epoch:26 step:21085 [D loss: 0.361873, acc.: 82.81%] [G loss: 3.027221]\n",
      "epoch:26 step:21086 [D loss: 0.364236, acc.: 85.94%] [G loss: 2.891305]\n",
      "epoch:26 step:21087 [D loss: 0.444134, acc.: 81.25%] [G loss: 3.294742]\n",
      "epoch:27 step:21088 [D loss: 0.284315, acc.: 90.62%] [G loss: 3.107859]\n",
      "epoch:27 step:21089 [D loss: 0.349184, acc.: 84.38%] [G loss: 2.804041]\n",
      "epoch:27 step:21090 [D loss: 0.366434, acc.: 84.38%] [G loss: 3.068760]\n",
      "epoch:27 step:21091 [D loss: 0.278933, acc.: 85.16%] [G loss: 3.405737]\n",
      "epoch:27 step:21092 [D loss: 0.399242, acc.: 81.25%] [G loss: 4.139999]\n",
      "epoch:27 step:21093 [D loss: 0.316265, acc.: 85.94%] [G loss: 2.851363]\n",
      "epoch:27 step:21094 [D loss: 0.311918, acc.: 88.28%] [G loss: 4.445989]\n",
      "epoch:27 step:21095 [D loss: 0.355436, acc.: 85.94%] [G loss: 2.872519]\n",
      "epoch:27 step:21096 [D loss: 0.373304, acc.: 78.12%] [G loss: 3.170626]\n",
      "epoch:27 step:21097 [D loss: 0.425944, acc.: 82.81%] [G loss: 2.388847]\n",
      "epoch:27 step:21098 [D loss: 0.285959, acc.: 89.06%] [G loss: 3.400926]\n",
      "epoch:27 step:21099 [D loss: 0.442590, acc.: 80.47%] [G loss: 2.561101]\n",
      "epoch:27 step:21100 [D loss: 0.270238, acc.: 89.84%] [G loss: 5.348879]\n",
      "epoch:27 step:21101 [D loss: 0.290527, acc.: 87.50%] [G loss: 4.131604]\n",
      "epoch:27 step:21102 [D loss: 0.280922, acc.: 85.94%] [G loss: 4.550845]\n",
      "epoch:27 step:21103 [D loss: 0.227921, acc.: 90.62%] [G loss: 5.558887]\n",
      "epoch:27 step:21104 [D loss: 0.220789, acc.: 92.19%] [G loss: 5.271336]\n",
      "epoch:27 step:21105 [D loss: 0.361476, acc.: 84.38%] [G loss: 3.173525]\n",
      "epoch:27 step:21106 [D loss: 0.320437, acc.: 83.59%] [G loss: 3.822325]\n",
      "epoch:27 step:21107 [D loss: 0.310265, acc.: 89.84%] [G loss: 3.200998]\n",
      "epoch:27 step:21108 [D loss: 0.279160, acc.: 89.84%] [G loss: 3.062176]\n",
      "epoch:27 step:21109 [D loss: 0.281803, acc.: 86.72%] [G loss: 2.965298]\n",
      "epoch:27 step:21110 [D loss: 0.293303, acc.: 92.19%] [G loss: 3.377080]\n",
      "epoch:27 step:21111 [D loss: 0.390815, acc.: 78.91%] [G loss: 3.976960]\n",
      "epoch:27 step:21112 [D loss: 0.259671, acc.: 89.06%] [G loss: 3.779994]\n",
      "epoch:27 step:21113 [D loss: 0.338348, acc.: 82.03%] [G loss: 3.194069]\n",
      "epoch:27 step:21114 [D loss: 0.279687, acc.: 86.72%] [G loss: 3.279234]\n",
      "epoch:27 step:21115 [D loss: 0.312068, acc.: 85.94%] [G loss: 3.505772]\n",
      "epoch:27 step:21116 [D loss: 0.278266, acc.: 89.84%] [G loss: 3.610208]\n",
      "epoch:27 step:21117 [D loss: 0.270751, acc.: 92.19%] [G loss: 3.198225]\n",
      "epoch:27 step:21118 [D loss: 0.376022, acc.: 77.34%] [G loss: 3.701573]\n",
      "epoch:27 step:21119 [D loss: 0.313951, acc.: 87.50%] [G loss: 3.460511]\n",
      "epoch:27 step:21120 [D loss: 0.250078, acc.: 87.50%] [G loss: 3.376462]\n",
      "epoch:27 step:21121 [D loss: 0.346017, acc.: 83.59%] [G loss: 3.363591]\n",
      "epoch:27 step:21122 [D loss: 0.310442, acc.: 86.72%] [G loss: 2.400226]\n",
      "epoch:27 step:21123 [D loss: 0.229662, acc.: 89.84%] [G loss: 4.153743]\n",
      "epoch:27 step:21124 [D loss: 0.316601, acc.: 85.94%] [G loss: 3.284884]\n",
      "epoch:27 step:21125 [D loss: 0.309326, acc.: 84.38%] [G loss: 3.435656]\n",
      "epoch:27 step:21126 [D loss: 0.297244, acc.: 88.28%] [G loss: 3.513810]\n",
      "epoch:27 step:21127 [D loss: 0.373519, acc.: 82.81%] [G loss: 2.510637]\n",
      "epoch:27 step:21128 [D loss: 0.405973, acc.: 78.12%] [G loss: 2.838861]\n",
      "epoch:27 step:21129 [D loss: 0.319127, acc.: 88.28%] [G loss: 2.999827]\n",
      "epoch:27 step:21130 [D loss: 0.309628, acc.: 86.72%] [G loss: 2.893997]\n",
      "epoch:27 step:21131 [D loss: 0.299886, acc.: 86.72%] [G loss: 2.830466]\n",
      "epoch:27 step:21132 [D loss: 0.391429, acc.: 83.59%] [G loss: 4.535929]\n",
      "epoch:27 step:21133 [D loss: 0.398239, acc.: 81.25%] [G loss: 4.144689]\n",
      "epoch:27 step:21134 [D loss: 0.358390, acc.: 83.59%] [G loss: 3.697494]\n",
      "epoch:27 step:21135 [D loss: 0.399077, acc.: 85.94%] [G loss: 4.355551]\n",
      "epoch:27 step:21136 [D loss: 0.244452, acc.: 89.06%] [G loss: 4.059052]\n",
      "epoch:27 step:21137 [D loss: 0.310125, acc.: 84.38%] [G loss: 3.025417]\n",
      "epoch:27 step:21138 [D loss: 0.357575, acc.: 81.25%] [G loss: 3.966998]\n",
      "epoch:27 step:21139 [D loss: 0.266010, acc.: 89.84%] [G loss: 3.707708]\n",
      "epoch:27 step:21140 [D loss: 0.250816, acc.: 91.41%] [G loss: 2.898942]\n",
      "epoch:27 step:21141 [D loss: 0.378233, acc.: 81.25%] [G loss: 3.312197]\n",
      "epoch:27 step:21142 [D loss: 0.278827, acc.: 89.06%] [G loss: 3.129385]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:27 step:21143 [D loss: 0.357124, acc.: 82.81%] [G loss: 2.912861]\n",
      "epoch:27 step:21144 [D loss: 0.321911, acc.: 86.72%] [G loss: 4.327543]\n",
      "epoch:27 step:21145 [D loss: 0.261435, acc.: 87.50%] [G loss: 4.771658]\n",
      "epoch:27 step:21146 [D loss: 0.312337, acc.: 85.94%] [G loss: 3.654342]\n",
      "epoch:27 step:21147 [D loss: 0.317873, acc.: 82.81%] [G loss: 2.831326]\n",
      "epoch:27 step:21148 [D loss: 0.194471, acc.: 90.62%] [G loss: 3.467399]\n",
      "epoch:27 step:21149 [D loss: 0.277703, acc.: 88.28%] [G loss: 3.163693]\n",
      "epoch:27 step:21150 [D loss: 0.293714, acc.: 84.38%] [G loss: 3.299161]\n",
      "epoch:27 step:21151 [D loss: 0.345400, acc.: 81.25%] [G loss: 3.526329]\n",
      "epoch:27 step:21152 [D loss: 0.491742, acc.: 76.56%] [G loss: 3.160549]\n",
      "epoch:27 step:21153 [D loss: 0.334332, acc.: 86.72%] [G loss: 3.026293]\n",
      "epoch:27 step:21154 [D loss: 0.334991, acc.: 85.16%] [G loss: 3.538033]\n",
      "epoch:27 step:21155 [D loss: 0.405881, acc.: 81.25%] [G loss: 3.142234]\n",
      "epoch:27 step:21156 [D loss: 0.351772, acc.: 82.03%] [G loss: 2.766056]\n",
      "epoch:27 step:21157 [D loss: 0.313947, acc.: 86.72%] [G loss: 3.056167]\n",
      "epoch:27 step:21158 [D loss: 0.402937, acc.: 80.47%] [G loss: 2.607409]\n",
      "epoch:27 step:21159 [D loss: 0.285298, acc.: 88.28%] [G loss: 3.064997]\n",
      "epoch:27 step:21160 [D loss: 0.352642, acc.: 83.59%] [G loss: 2.198620]\n",
      "epoch:27 step:21161 [D loss: 0.356844, acc.: 82.81%] [G loss: 3.184069]\n",
      "epoch:27 step:21162 [D loss: 0.333027, acc.: 88.28%] [G loss: 3.627884]\n",
      "epoch:27 step:21163 [D loss: 0.415805, acc.: 78.91%] [G loss: 2.921839]\n",
      "epoch:27 step:21164 [D loss: 0.304398, acc.: 85.16%] [G loss: 4.275490]\n",
      "epoch:27 step:21165 [D loss: 0.397326, acc.: 82.81%] [G loss: 4.274146]\n",
      "epoch:27 step:21166 [D loss: 0.319011, acc.: 89.06%] [G loss: 3.094403]\n",
      "epoch:27 step:21167 [D loss: 0.221490, acc.: 90.62%] [G loss: 2.908594]\n",
      "epoch:27 step:21168 [D loss: 0.356666, acc.: 85.16%] [G loss: 3.071468]\n",
      "epoch:27 step:21169 [D loss: 0.299529, acc.: 85.16%] [G loss: 3.232286]\n",
      "epoch:27 step:21170 [D loss: 0.369302, acc.: 87.50%] [G loss: 2.799789]\n",
      "epoch:27 step:21171 [D loss: 0.241162, acc.: 89.06%] [G loss: 3.099656]\n",
      "epoch:27 step:21172 [D loss: 0.199737, acc.: 93.75%] [G loss: 3.380749]\n",
      "epoch:27 step:21173 [D loss: 0.299626, acc.: 87.50%] [G loss: 6.572777]\n",
      "epoch:27 step:21174 [D loss: 0.397432, acc.: 82.81%] [G loss: 3.149667]\n",
      "epoch:27 step:21175 [D loss: 0.291688, acc.: 87.50%] [G loss: 2.892515]\n",
      "epoch:27 step:21176 [D loss: 0.350566, acc.: 85.16%] [G loss: 3.410224]\n",
      "epoch:27 step:21177 [D loss: 0.518536, acc.: 78.91%] [G loss: 7.757417]\n",
      "epoch:27 step:21178 [D loss: 1.149292, acc.: 67.97%] [G loss: 8.264042]\n",
      "epoch:27 step:21179 [D loss: 2.392557, acc.: 54.69%] [G loss: 10.021278]\n",
      "epoch:27 step:21180 [D loss: 2.089215, acc.: 50.78%] [G loss: 3.057632]\n",
      "epoch:27 step:21181 [D loss: 0.771741, acc.: 74.22%] [G loss: 5.319657]\n",
      "epoch:27 step:21182 [D loss: 1.181318, acc.: 67.19%] [G loss: 4.613329]\n",
      "epoch:27 step:21183 [D loss: 0.406401, acc.: 81.25%] [G loss: 3.768669]\n",
      "epoch:27 step:21184 [D loss: 0.394524, acc.: 83.59%] [G loss: 3.338641]\n",
      "epoch:27 step:21185 [D loss: 0.293492, acc.: 89.06%] [G loss: 3.609166]\n",
      "epoch:27 step:21186 [D loss: 0.494141, acc.: 73.44%] [G loss: 3.414163]\n",
      "epoch:27 step:21187 [D loss: 0.502335, acc.: 81.25%] [G loss: 3.050111]\n",
      "epoch:27 step:21188 [D loss: 0.346012, acc.: 82.03%] [G loss: 3.008951]\n",
      "epoch:27 step:21189 [D loss: 0.447714, acc.: 80.47%] [G loss: 3.468887]\n",
      "epoch:27 step:21190 [D loss: 0.312314, acc.: 85.16%] [G loss: 2.978616]\n",
      "epoch:27 step:21191 [D loss: 0.309458, acc.: 84.38%] [G loss: 3.128382]\n",
      "epoch:27 step:21192 [D loss: 0.371947, acc.: 84.38%] [G loss: 2.871852]\n",
      "epoch:27 step:21193 [D loss: 0.261909, acc.: 90.62%] [G loss: 2.976418]\n",
      "epoch:27 step:21194 [D loss: 0.301709, acc.: 83.59%] [G loss: 3.326459]\n",
      "epoch:27 step:21195 [D loss: 0.286947, acc.: 89.84%] [G loss: 2.875180]\n",
      "epoch:27 step:21196 [D loss: 0.347224, acc.: 84.38%] [G loss: 3.101125]\n",
      "epoch:27 step:21197 [D loss: 0.273959, acc.: 91.41%] [G loss: 3.319881]\n",
      "epoch:27 step:21198 [D loss: 0.351718, acc.: 85.94%] [G loss: 2.460617]\n",
      "epoch:27 step:21199 [D loss: 0.246393, acc.: 91.41%] [G loss: 2.636334]\n",
      "epoch:27 step:21200 [D loss: 0.536084, acc.: 74.22%] [G loss: 3.886648]\n",
      "##############\n",
      "[0.82672232 0.84374904 0.80926573 0.8072843  0.7617286  0.83155458\n",
      " 0.87598387 0.8382939  0.81681037 0.83746603]\n",
      "##########\n",
      "epoch:27 step:21201 [D loss: 0.614905, acc.: 72.66%] [G loss: 2.832647]\n",
      "epoch:27 step:21202 [D loss: 0.246121, acc.: 90.62%] [G loss: 2.956954]\n",
      "epoch:27 step:21203 [D loss: 0.483180, acc.: 81.25%] [G loss: 3.404747]\n",
      "epoch:27 step:21204 [D loss: 0.270093, acc.: 89.06%] [G loss: 2.877489]\n",
      "epoch:27 step:21205 [D loss: 0.424873, acc.: 82.81%] [G loss: 4.614295]\n",
      "epoch:27 step:21206 [D loss: 0.403023, acc.: 83.59%] [G loss: 2.602097]\n",
      "epoch:27 step:21207 [D loss: 0.343990, acc.: 83.59%] [G loss: 3.691332]\n",
      "epoch:27 step:21208 [D loss: 0.229049, acc.: 89.84%] [G loss: 3.409703]\n",
      "epoch:27 step:21209 [D loss: 0.380354, acc.: 82.81%] [G loss: 3.120078]\n",
      "epoch:27 step:21210 [D loss: 0.296146, acc.: 87.50%] [G loss: 3.377156]\n",
      "epoch:27 step:21211 [D loss: 0.302713, acc.: 89.06%] [G loss: 2.637778]\n",
      "epoch:27 step:21212 [D loss: 0.306720, acc.: 85.16%] [G loss: 2.428028]\n",
      "epoch:27 step:21213 [D loss: 0.391093, acc.: 79.69%] [G loss: 2.522308]\n",
      "epoch:27 step:21214 [D loss: 0.317227, acc.: 85.16%] [G loss: 2.822315]\n",
      "epoch:27 step:21215 [D loss: 0.308134, acc.: 87.50%] [G loss: 3.124827]\n",
      "epoch:27 step:21216 [D loss: 0.305982, acc.: 86.72%] [G loss: 3.088058]\n",
      "epoch:27 step:21217 [D loss: 0.290894, acc.: 88.28%] [G loss: 2.869084]\n",
      "epoch:27 step:21218 [D loss: 0.234147, acc.: 91.41%] [G loss: 3.132721]\n",
      "epoch:27 step:21219 [D loss: 0.288705, acc.: 86.72%] [G loss: 2.714593]\n",
      "epoch:27 step:21220 [D loss: 0.361884, acc.: 81.25%] [G loss: 3.182859]\n",
      "epoch:27 step:21221 [D loss: 0.302414, acc.: 87.50%] [G loss: 3.237834]\n",
      "epoch:27 step:21222 [D loss: 0.359654, acc.: 81.25%] [G loss: 3.833373]\n",
      "epoch:27 step:21223 [D loss: 0.420206, acc.: 84.38%] [G loss: 3.091949]\n",
      "epoch:27 step:21224 [D loss: 0.381543, acc.: 84.38%] [G loss: 2.676779]\n",
      "epoch:27 step:21225 [D loss: 0.246322, acc.: 89.06%] [G loss: 2.897937]\n",
      "epoch:27 step:21226 [D loss: 0.290873, acc.: 87.50%] [G loss: 2.859518]\n",
      "epoch:27 step:21227 [D loss: 0.308977, acc.: 85.94%] [G loss: 2.696883]\n",
      "epoch:27 step:21228 [D loss: 0.265170, acc.: 85.94%] [G loss: 4.122557]\n",
      "epoch:27 step:21229 [D loss: 0.227661, acc.: 90.62%] [G loss: 5.498164]\n",
      "epoch:27 step:21230 [D loss: 0.289069, acc.: 86.72%] [G loss: 4.382002]\n",
      "epoch:27 step:21231 [D loss: 0.289535, acc.: 85.94%] [G loss: 3.051176]\n",
      "epoch:27 step:21232 [D loss: 0.291032, acc.: 89.06%] [G loss: 3.112357]\n",
      "epoch:27 step:21233 [D loss: 0.414199, acc.: 82.03%] [G loss: 3.324766]\n",
      "epoch:27 step:21234 [D loss: 0.290350, acc.: 89.06%] [G loss: 3.670581]\n",
      "epoch:27 step:21235 [D loss: 0.351472, acc.: 82.81%] [G loss: 3.038133]\n",
      "epoch:27 step:21236 [D loss: 0.391318, acc.: 79.69%] [G loss: 2.377658]\n",
      "epoch:27 step:21237 [D loss: 0.303566, acc.: 89.84%] [G loss: 2.843687]\n",
      "epoch:27 step:21238 [D loss: 0.325315, acc.: 85.94%] [G loss: 2.838438]\n",
      "epoch:27 step:21239 [D loss: 0.420790, acc.: 83.59%] [G loss: 2.777282]\n",
      "epoch:27 step:21240 [D loss: 0.261855, acc.: 88.28%] [G loss: 2.670596]\n",
      "epoch:27 step:21241 [D loss: 0.302356, acc.: 86.72%] [G loss: 2.606654]\n",
      "epoch:27 step:21242 [D loss: 0.295042, acc.: 89.84%] [G loss: 2.943119]\n",
      "epoch:27 step:21243 [D loss: 0.442936, acc.: 78.91%] [G loss: 2.605764]\n",
      "epoch:27 step:21244 [D loss: 0.378869, acc.: 82.03%] [G loss: 3.558067]\n",
      "epoch:27 step:21245 [D loss: 0.453183, acc.: 78.91%] [G loss: 2.651357]\n",
      "epoch:27 step:21246 [D loss: 0.380654, acc.: 82.03%] [G loss: 3.645513]\n",
      "epoch:27 step:21247 [D loss: 0.369898, acc.: 82.81%] [G loss: 4.340188]\n",
      "epoch:27 step:21248 [D loss: 0.454581, acc.: 80.47%] [G loss: 2.701192]\n",
      "epoch:27 step:21249 [D loss: 0.336303, acc.: 84.38%] [G loss: 2.533370]\n",
      "epoch:27 step:21250 [D loss: 0.322748, acc.: 85.94%] [G loss: 2.890873]\n",
      "epoch:27 step:21251 [D loss: 0.352458, acc.: 85.94%] [G loss: 2.551172]\n",
      "epoch:27 step:21252 [D loss: 0.319198, acc.: 85.16%] [G loss: 3.237553]\n",
      "epoch:27 step:21253 [D loss: 0.429318, acc.: 81.25%] [G loss: 3.808187]\n",
      "epoch:27 step:21254 [D loss: 0.423378, acc.: 79.69%] [G loss: 3.838231]\n",
      "epoch:27 step:21255 [D loss: 0.418787, acc.: 81.25%] [G loss: 3.391387]\n",
      "epoch:27 step:21256 [D loss: 0.430353, acc.: 79.69%] [G loss: 3.163716]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:27 step:21257 [D loss: 0.245587, acc.: 89.84%] [G loss: 4.477910]\n",
      "epoch:27 step:21258 [D loss: 0.351727, acc.: 84.38%] [G loss: 5.086040]\n",
      "epoch:27 step:21259 [D loss: 0.385318, acc.: 82.03%] [G loss: 3.985883]\n",
      "epoch:27 step:21260 [D loss: 0.229376, acc.: 85.16%] [G loss: 5.059216]\n",
      "epoch:27 step:21261 [D loss: 0.355418, acc.: 82.81%] [G loss: 3.482542]\n",
      "epoch:27 step:21262 [D loss: 0.420296, acc.: 79.69%] [G loss: 2.341297]\n",
      "epoch:27 step:21263 [D loss: 0.304428, acc.: 85.16%] [G loss: 3.856731]\n",
      "epoch:27 step:21264 [D loss: 0.285676, acc.: 86.72%] [G loss: 3.505001]\n",
      "epoch:27 step:21265 [D loss: 0.364138, acc.: 82.81%] [G loss: 3.945169]\n",
      "epoch:27 step:21266 [D loss: 0.245104, acc.: 89.06%] [G loss: 5.805145]\n",
      "epoch:27 step:21267 [D loss: 0.237005, acc.: 87.50%] [G loss: 3.611376]\n",
      "epoch:27 step:21268 [D loss: 0.340216, acc.: 87.50%] [G loss: 3.210552]\n",
      "epoch:27 step:21269 [D loss: 0.266279, acc.: 88.28%] [G loss: 3.377839]\n",
      "epoch:27 step:21270 [D loss: 0.305385, acc.: 86.72%] [G loss: 2.749721]\n",
      "epoch:27 step:21271 [D loss: 0.322940, acc.: 84.38%] [G loss: 3.446131]\n",
      "epoch:27 step:21272 [D loss: 0.283960, acc.: 88.28%] [G loss: 3.341207]\n",
      "epoch:27 step:21273 [D loss: 0.222328, acc.: 92.19%] [G loss: 3.920066]\n",
      "epoch:27 step:21274 [D loss: 0.359402, acc.: 85.94%] [G loss: 2.870586]\n",
      "epoch:27 step:21275 [D loss: 0.317123, acc.: 87.50%] [G loss: 4.214928]\n",
      "epoch:27 step:21276 [D loss: 0.306457, acc.: 89.06%] [G loss: 3.522310]\n",
      "epoch:27 step:21277 [D loss: 0.311932, acc.: 85.16%] [G loss: 3.440334]\n",
      "epoch:27 step:21278 [D loss: 0.484243, acc.: 78.91%] [G loss: 2.478359]\n",
      "epoch:27 step:21279 [D loss: 0.311508, acc.: 84.38%] [G loss: 4.186490]\n",
      "epoch:27 step:21280 [D loss: 0.271237, acc.: 86.72%] [G loss: 4.325294]\n",
      "epoch:27 step:21281 [D loss: 0.359757, acc.: 82.81%] [G loss: 2.164713]\n",
      "epoch:27 step:21282 [D loss: 0.410745, acc.: 80.47%] [G loss: 2.629166]\n",
      "epoch:27 step:21283 [D loss: 0.210647, acc.: 91.41%] [G loss: 2.979171]\n",
      "epoch:27 step:21284 [D loss: 0.436259, acc.: 79.69%] [G loss: 2.564253]\n",
      "epoch:27 step:21285 [D loss: 0.272689, acc.: 91.41%] [G loss: 3.486977]\n",
      "epoch:27 step:21286 [D loss: 0.304935, acc.: 87.50%] [G loss: 2.778991]\n",
      "epoch:27 step:21287 [D loss: 0.343132, acc.: 83.59%] [G loss: 4.069464]\n",
      "epoch:27 step:21288 [D loss: 0.291600, acc.: 85.16%] [G loss: 3.912836]\n",
      "epoch:27 step:21289 [D loss: 0.340893, acc.: 85.16%] [G loss: 3.711883]\n",
      "epoch:27 step:21290 [D loss: 0.353643, acc.: 85.16%] [G loss: 3.725736]\n",
      "epoch:27 step:21291 [D loss: 0.237735, acc.: 91.41%] [G loss: 3.662150]\n",
      "epoch:27 step:21292 [D loss: 0.303537, acc.: 88.28%] [G loss: 3.815335]\n",
      "epoch:27 step:21293 [D loss: 0.335830, acc.: 86.72%] [G loss: 3.641411]\n",
      "epoch:27 step:21294 [D loss: 0.254025, acc.: 91.41%] [G loss: 3.556608]\n",
      "epoch:27 step:21295 [D loss: 0.454558, acc.: 80.47%] [G loss: 3.645663]\n",
      "epoch:27 step:21296 [D loss: 0.350635, acc.: 86.72%] [G loss: 3.250344]\n",
      "epoch:27 step:21297 [D loss: 0.253249, acc.: 87.50%] [G loss: 3.948511]\n",
      "epoch:27 step:21298 [D loss: 0.303058, acc.: 85.94%] [G loss: 3.410136]\n",
      "epoch:27 step:21299 [D loss: 0.309242, acc.: 84.38%] [G loss: 3.539737]\n",
      "epoch:27 step:21300 [D loss: 0.338912, acc.: 85.94%] [G loss: 3.611127]\n",
      "epoch:27 step:21301 [D loss: 0.368851, acc.: 82.81%] [G loss: 5.607096]\n",
      "epoch:27 step:21302 [D loss: 0.345270, acc.: 84.38%] [G loss: 4.149095]\n",
      "epoch:27 step:21303 [D loss: 0.325939, acc.: 85.94%] [G loss: 3.421280]\n",
      "epoch:27 step:21304 [D loss: 0.262249, acc.: 87.50%] [G loss: 4.257576]\n",
      "epoch:27 step:21305 [D loss: 0.349092, acc.: 83.59%] [G loss: 3.510286]\n",
      "epoch:27 step:21306 [D loss: 0.246988, acc.: 90.62%] [G loss: 3.524856]\n",
      "epoch:27 step:21307 [D loss: 0.266815, acc.: 89.06%] [G loss: 3.973648]\n",
      "epoch:27 step:21308 [D loss: 0.433784, acc.: 78.12%] [G loss: 3.967362]\n",
      "epoch:27 step:21309 [D loss: 0.534671, acc.: 74.22%] [G loss: 2.578469]\n",
      "epoch:27 step:21310 [D loss: 0.351398, acc.: 84.38%] [G loss: 3.185993]\n",
      "epoch:27 step:21311 [D loss: 0.446191, acc.: 79.69%] [G loss: 3.111971]\n",
      "epoch:27 step:21312 [D loss: 0.384906, acc.: 82.03%] [G loss: 2.863877]\n",
      "epoch:27 step:21313 [D loss: 0.398962, acc.: 84.38%] [G loss: 3.997339]\n",
      "epoch:27 step:21314 [D loss: 0.270104, acc.: 91.41%] [G loss: 3.263319]\n",
      "epoch:27 step:21315 [D loss: 0.407865, acc.: 82.81%] [G loss: 3.166907]\n",
      "epoch:27 step:21316 [D loss: 0.340889, acc.: 85.16%] [G loss: 3.994302]\n",
      "epoch:27 step:21317 [D loss: 0.323302, acc.: 82.81%] [G loss: 4.112456]\n",
      "epoch:27 step:21318 [D loss: 0.433884, acc.: 84.38%] [G loss: 5.206831]\n",
      "epoch:27 step:21319 [D loss: 0.400339, acc.: 81.25%] [G loss: 3.068696]\n",
      "epoch:27 step:21320 [D loss: 0.314880, acc.: 84.38%] [G loss: 4.715347]\n",
      "epoch:27 step:21321 [D loss: 0.323943, acc.: 85.94%] [G loss: 5.125696]\n",
      "epoch:27 step:21322 [D loss: 0.311725, acc.: 84.38%] [G loss: 3.451934]\n",
      "epoch:27 step:21323 [D loss: 0.297493, acc.: 87.50%] [G loss: 2.936854]\n",
      "epoch:27 step:21324 [D loss: 0.278966, acc.: 85.94%] [G loss: 2.893363]\n",
      "epoch:27 step:21325 [D loss: 0.459372, acc.: 78.12%] [G loss: 3.299265]\n",
      "epoch:27 step:21326 [D loss: 0.289557, acc.: 91.41%] [G loss: 2.893362]\n",
      "epoch:27 step:21327 [D loss: 0.317086, acc.: 84.38%] [G loss: 3.670785]\n",
      "epoch:27 step:21328 [D loss: 0.267141, acc.: 89.06%] [G loss: 4.116752]\n",
      "epoch:27 step:21329 [D loss: 0.373103, acc.: 82.03%] [G loss: 4.599805]\n",
      "epoch:27 step:21330 [D loss: 0.365304, acc.: 77.34%] [G loss: 2.607056]\n",
      "epoch:27 step:21331 [D loss: 0.302308, acc.: 87.50%] [G loss: 6.148683]\n",
      "epoch:27 step:21332 [D loss: 0.346096, acc.: 84.38%] [G loss: 2.883470]\n",
      "epoch:27 step:21333 [D loss: 0.350399, acc.: 81.25%] [G loss: 2.977858]\n",
      "epoch:27 step:21334 [D loss: 0.379949, acc.: 83.59%] [G loss: 2.841659]\n",
      "epoch:27 step:21335 [D loss: 0.267279, acc.: 88.28%] [G loss: 3.182473]\n",
      "epoch:27 step:21336 [D loss: 0.408971, acc.: 81.25%] [G loss: 3.641806]\n",
      "epoch:27 step:21337 [D loss: 0.389489, acc.: 81.25%] [G loss: 4.092059]\n",
      "epoch:27 step:21338 [D loss: 0.260726, acc.: 89.84%] [G loss: 3.103020]\n",
      "epoch:27 step:21339 [D loss: 0.351837, acc.: 81.25%] [G loss: 3.723781]\n",
      "epoch:27 step:21340 [D loss: 0.357178, acc.: 84.38%] [G loss: 2.204293]\n",
      "epoch:27 step:21341 [D loss: 0.305879, acc.: 86.72%] [G loss: 3.199578]\n",
      "epoch:27 step:21342 [D loss: 0.326273, acc.: 82.03%] [G loss: 4.574002]\n",
      "epoch:27 step:21343 [D loss: 0.212805, acc.: 91.41%] [G loss: 3.400280]\n",
      "epoch:27 step:21344 [D loss: 0.316123, acc.: 83.59%] [G loss: 4.370635]\n",
      "epoch:27 step:21345 [D loss: 0.357345, acc.: 84.38%] [G loss: 4.644470]\n",
      "epoch:27 step:21346 [D loss: 0.494779, acc.: 80.47%] [G loss: 3.634040]\n",
      "epoch:27 step:21347 [D loss: 0.261498, acc.: 92.19%] [G loss: 3.580798]\n",
      "epoch:27 step:21348 [D loss: 0.286441, acc.: 89.84%] [G loss: 3.531759]\n",
      "epoch:27 step:21349 [D loss: 0.316695, acc.: 85.16%] [G loss: 3.651565]\n",
      "epoch:27 step:21350 [D loss: 0.321675, acc.: 82.81%] [G loss: 3.430286]\n",
      "epoch:27 step:21351 [D loss: 0.341777, acc.: 83.59%] [G loss: 2.921774]\n",
      "epoch:27 step:21352 [D loss: 0.375283, acc.: 85.16%] [G loss: 4.062601]\n",
      "epoch:27 step:21353 [D loss: 0.321347, acc.: 83.59%] [G loss: 6.259200]\n",
      "epoch:27 step:21354 [D loss: 0.400431, acc.: 84.38%] [G loss: 3.548330]\n",
      "epoch:27 step:21355 [D loss: 0.262300, acc.: 89.06%] [G loss: 3.491941]\n",
      "epoch:27 step:21356 [D loss: 0.312294, acc.: 88.28%] [G loss: 3.823496]\n",
      "epoch:27 step:21357 [D loss: 0.272453, acc.: 85.94%] [G loss: 2.742040]\n",
      "epoch:27 step:21358 [D loss: 0.301170, acc.: 86.72%] [G loss: 4.108951]\n",
      "epoch:27 step:21359 [D loss: 0.333037, acc.: 84.38%] [G loss: 3.271983]\n",
      "epoch:27 step:21360 [D loss: 0.285465, acc.: 89.06%] [G loss: 4.519532]\n",
      "epoch:27 step:21361 [D loss: 0.313438, acc.: 86.72%] [G loss: 2.973997]\n",
      "epoch:27 step:21362 [D loss: 0.299406, acc.: 87.50%] [G loss: 5.249309]\n",
      "epoch:27 step:21363 [D loss: 0.402969, acc.: 82.03%] [G loss: 3.239958]\n",
      "epoch:27 step:21364 [D loss: 0.334745, acc.: 85.94%] [G loss: 2.916217]\n",
      "epoch:27 step:21365 [D loss: 0.394232, acc.: 83.59%] [G loss: 3.065376]\n",
      "epoch:27 step:21366 [D loss: 0.389019, acc.: 78.12%] [G loss: 4.564330]\n",
      "epoch:27 step:21367 [D loss: 0.252128, acc.: 89.84%] [G loss: 3.862469]\n",
      "epoch:27 step:21368 [D loss: 0.285900, acc.: 89.84%] [G loss: 7.675294]\n",
      "epoch:27 step:21369 [D loss: 0.280508, acc.: 90.62%] [G loss: 4.937499]\n",
      "epoch:27 step:21370 [D loss: 0.436615, acc.: 82.81%] [G loss: 4.418336]\n",
      "epoch:27 step:21371 [D loss: 0.257043, acc.: 85.94%] [G loss: 4.353368]\n",
      "epoch:27 step:21372 [D loss: 0.284120, acc.: 87.50%] [G loss: 2.968064]\n",
      "epoch:27 step:21373 [D loss: 0.401563, acc.: 82.81%] [G loss: 3.003395]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:27 step:21374 [D loss: 0.283522, acc.: 85.94%] [G loss: 3.185029]\n",
      "epoch:27 step:21375 [D loss: 0.372699, acc.: 82.03%] [G loss: 2.950424]\n",
      "epoch:27 step:21376 [D loss: 0.280115, acc.: 85.94%] [G loss: 2.939585]\n",
      "epoch:27 step:21377 [D loss: 0.302764, acc.: 85.94%] [G loss: 2.864798]\n",
      "epoch:27 step:21378 [D loss: 0.331273, acc.: 82.81%] [G loss: 2.608541]\n",
      "epoch:27 step:21379 [D loss: 0.247740, acc.: 88.28%] [G loss: 2.456612]\n",
      "epoch:27 step:21380 [D loss: 0.352548, acc.: 86.72%] [G loss: 2.361420]\n",
      "epoch:27 step:21381 [D loss: 0.328708, acc.: 83.59%] [G loss: 2.991366]\n",
      "epoch:27 step:21382 [D loss: 0.382406, acc.: 82.03%] [G loss: 3.956967]\n",
      "epoch:27 step:21383 [D loss: 0.279569, acc.: 87.50%] [G loss: 3.653426]\n",
      "epoch:27 step:21384 [D loss: 0.359368, acc.: 85.16%] [G loss: 3.129776]\n",
      "epoch:27 step:21385 [D loss: 0.391339, acc.: 81.25%] [G loss: 2.735060]\n",
      "epoch:27 step:21386 [D loss: 0.320345, acc.: 85.16%] [G loss: 2.846362]\n",
      "epoch:27 step:21387 [D loss: 0.446322, acc.: 79.69%] [G loss: 4.686592]\n",
      "epoch:27 step:21388 [D loss: 0.716157, acc.: 71.88%] [G loss: 7.826587]\n",
      "epoch:27 step:21389 [D loss: 1.817096, acc.: 60.16%] [G loss: 7.411114]\n",
      "epoch:27 step:21390 [D loss: 2.882358, acc.: 46.88%] [G loss: 4.273427]\n",
      "epoch:27 step:21391 [D loss: 0.457591, acc.: 82.03%] [G loss: 5.832835]\n",
      "epoch:27 step:21392 [D loss: 0.339734, acc.: 85.94%] [G loss: 3.378908]\n",
      "epoch:27 step:21393 [D loss: 0.347774, acc.: 86.72%] [G loss: 6.372275]\n",
      "epoch:27 step:21394 [D loss: 0.357946, acc.: 85.16%] [G loss: 2.957208]\n",
      "epoch:27 step:21395 [D loss: 0.350172, acc.: 85.16%] [G loss: 3.826934]\n",
      "epoch:27 step:21396 [D loss: 0.467600, acc.: 75.78%] [G loss: 4.430708]\n",
      "epoch:27 step:21397 [D loss: 0.217530, acc.: 92.19%] [G loss: 4.664938]\n",
      "epoch:27 step:21398 [D loss: 0.456777, acc.: 80.47%] [G loss: 5.913894]\n",
      "epoch:27 step:21399 [D loss: 0.329600, acc.: 82.03%] [G loss: 4.061498]\n",
      "epoch:27 step:21400 [D loss: 0.397746, acc.: 83.59%] [G loss: 4.562417]\n",
      "##############\n",
      "[0.85638143 0.85538671 0.8295837  0.79005817 0.78750407 0.80882618\n",
      " 0.89335799 0.83982728 0.81935075 0.83133405]\n",
      "##########\n",
      "epoch:27 step:21401 [D loss: 0.351148, acc.: 82.81%] [G loss: 2.955292]\n",
      "epoch:27 step:21402 [D loss: 0.370721, acc.: 81.25%] [G loss: 4.249424]\n",
      "epoch:27 step:21403 [D loss: 0.276026, acc.: 89.06%] [G loss: 4.778358]\n",
      "epoch:27 step:21404 [D loss: 0.351994, acc.: 82.81%] [G loss: 4.032236]\n",
      "epoch:27 step:21405 [D loss: 0.259727, acc.: 88.28%] [G loss: 5.429829]\n",
      "epoch:27 step:21406 [D loss: 0.384542, acc.: 81.25%] [G loss: 3.610278]\n",
      "epoch:27 step:21407 [D loss: 0.326000, acc.: 85.16%] [G loss: 6.051641]\n",
      "epoch:27 step:21408 [D loss: 0.252133, acc.: 88.28%] [G loss: 3.705760]\n",
      "epoch:27 step:21409 [D loss: 0.267910, acc.: 91.41%] [G loss: 4.735006]\n",
      "epoch:27 step:21410 [D loss: 0.327491, acc.: 84.38%] [G loss: 4.818308]\n",
      "epoch:27 step:21411 [D loss: 0.252497, acc.: 88.28%] [G loss: 3.359236]\n",
      "epoch:27 step:21412 [D loss: 0.328268, acc.: 85.16%] [G loss: 2.606374]\n",
      "epoch:27 step:21413 [D loss: 0.365541, acc.: 83.59%] [G loss: 2.198435]\n",
      "epoch:27 step:21414 [D loss: 0.298078, acc.: 85.16%] [G loss: 2.730947]\n",
      "epoch:27 step:21415 [D loss: 0.325432, acc.: 87.50%] [G loss: 2.238534]\n",
      "epoch:27 step:21416 [D loss: 0.329824, acc.: 88.28%] [G loss: 2.777827]\n",
      "epoch:27 step:21417 [D loss: 0.323119, acc.: 85.94%] [G loss: 2.334179]\n",
      "epoch:27 step:21418 [D loss: 0.347643, acc.: 84.38%] [G loss: 2.375490]\n",
      "epoch:27 step:21419 [D loss: 0.308102, acc.: 88.28%] [G loss: 3.006001]\n",
      "epoch:27 step:21420 [D loss: 0.285994, acc.: 88.28%] [G loss: 2.672967]\n",
      "epoch:27 step:21421 [D loss: 0.356355, acc.: 83.59%] [G loss: 2.131707]\n",
      "epoch:27 step:21422 [D loss: 0.357297, acc.: 85.16%] [G loss: 3.306807]\n",
      "epoch:27 step:21423 [D loss: 0.290829, acc.: 86.72%] [G loss: 2.690911]\n",
      "epoch:27 step:21424 [D loss: 0.309482, acc.: 85.94%] [G loss: 2.881620]\n",
      "epoch:27 step:21425 [D loss: 0.421451, acc.: 81.25%] [G loss: 2.283985]\n",
      "epoch:27 step:21426 [D loss: 0.304837, acc.: 84.38%] [G loss: 3.332160]\n",
      "epoch:27 step:21427 [D loss: 0.288659, acc.: 89.06%] [G loss: 3.090728]\n",
      "epoch:27 step:21428 [D loss: 0.273747, acc.: 88.28%] [G loss: 2.870537]\n",
      "epoch:27 step:21429 [D loss: 0.282444, acc.: 86.72%] [G loss: 2.617634]\n",
      "epoch:27 step:21430 [D loss: 0.342592, acc.: 85.94%] [G loss: 2.456235]\n",
      "epoch:27 step:21431 [D loss: 0.347367, acc.: 83.59%] [G loss: 2.733230]\n",
      "epoch:27 step:21432 [D loss: 0.371464, acc.: 81.25%] [G loss: 2.204079]\n",
      "epoch:27 step:21433 [D loss: 0.300355, acc.: 90.62%] [G loss: 3.092302]\n",
      "epoch:27 step:21434 [D loss: 0.422659, acc.: 82.81%] [G loss: 2.402435]\n",
      "epoch:27 step:21435 [D loss: 0.330443, acc.: 86.72%] [G loss: 2.938470]\n",
      "epoch:27 step:21436 [D loss: 0.356133, acc.: 84.38%] [G loss: 2.815341]\n",
      "epoch:27 step:21437 [D loss: 0.441943, acc.: 75.78%] [G loss: 2.808110]\n",
      "epoch:27 step:21438 [D loss: 0.326951, acc.: 85.16%] [G loss: 4.172609]\n",
      "epoch:27 step:21439 [D loss: 0.421350, acc.: 82.03%] [G loss: 4.290476]\n",
      "epoch:27 step:21440 [D loss: 0.254378, acc.: 89.06%] [G loss: 5.918917]\n",
      "epoch:27 step:21441 [D loss: 0.306990, acc.: 85.16%] [G loss: 2.787900]\n",
      "epoch:27 step:21442 [D loss: 0.158740, acc.: 92.97%] [G loss: 5.138013]\n",
      "epoch:27 step:21443 [D loss: 0.200614, acc.: 92.19%] [G loss: 4.379665]\n",
      "epoch:27 step:21444 [D loss: 0.403754, acc.: 82.81%] [G loss: 2.599491]\n",
      "epoch:27 step:21445 [D loss: 0.247753, acc.: 87.50%] [G loss: 4.129044]\n",
      "epoch:27 step:21446 [D loss: 0.218110, acc.: 89.06%] [G loss: 3.801604]\n",
      "epoch:27 step:21447 [D loss: 0.263153, acc.: 89.84%] [G loss: 3.804696]\n",
      "epoch:27 step:21448 [D loss: 0.272733, acc.: 86.72%] [G loss: 2.362340]\n",
      "epoch:27 step:21449 [D loss: 0.307389, acc.: 89.84%] [G loss: 3.331275]\n",
      "epoch:27 step:21450 [D loss: 0.319526, acc.: 86.72%] [G loss: 2.905387]\n",
      "epoch:27 step:21451 [D loss: 0.312686, acc.: 86.72%] [G loss: 2.530734]\n",
      "epoch:27 step:21452 [D loss: 0.302831, acc.: 88.28%] [G loss: 2.799708]\n",
      "epoch:27 step:21453 [D loss: 0.379615, acc.: 84.38%] [G loss: 2.502678]\n",
      "epoch:27 step:21454 [D loss: 0.395694, acc.: 78.12%] [G loss: 2.153803]\n",
      "epoch:27 step:21455 [D loss: 0.415905, acc.: 81.25%] [G loss: 2.569085]\n",
      "epoch:27 step:21456 [D loss: 0.332428, acc.: 86.72%] [G loss: 2.426529]\n",
      "epoch:27 step:21457 [D loss: 0.429464, acc.: 81.25%] [G loss: 3.206579]\n",
      "epoch:27 step:21458 [D loss: 0.304755, acc.: 89.06%] [G loss: 2.811930]\n",
      "epoch:27 step:21459 [D loss: 0.364022, acc.: 83.59%] [G loss: 2.840731]\n",
      "epoch:27 step:21460 [D loss: 0.428185, acc.: 82.03%] [G loss: 3.109982]\n",
      "epoch:27 step:21461 [D loss: 0.412628, acc.: 80.47%] [G loss: 2.738314]\n",
      "epoch:27 step:21462 [D loss: 0.406176, acc.: 78.12%] [G loss: 3.217708]\n",
      "epoch:27 step:21463 [D loss: 0.232291, acc.: 92.19%] [G loss: 2.639582]\n",
      "epoch:27 step:21464 [D loss: 0.362069, acc.: 80.47%] [G loss: 3.326647]\n",
      "epoch:27 step:21465 [D loss: 0.364676, acc.: 81.25%] [G loss: 2.629074]\n",
      "epoch:27 step:21466 [D loss: 0.270561, acc.: 87.50%] [G loss: 2.766670]\n",
      "epoch:27 step:21467 [D loss: 0.324904, acc.: 86.72%] [G loss: 2.863423]\n",
      "epoch:27 step:21468 [D loss: 0.219319, acc.: 89.84%] [G loss: 3.078471]\n",
      "epoch:27 step:21469 [D loss: 0.333364, acc.: 82.03%] [G loss: 2.344549]\n",
      "epoch:27 step:21470 [D loss: 0.328277, acc.: 82.81%] [G loss: 2.895496]\n",
      "epoch:27 step:21471 [D loss: 0.270353, acc.: 88.28%] [G loss: 2.742036]\n",
      "epoch:27 step:21472 [D loss: 0.363171, acc.: 78.12%] [G loss: 3.300622]\n",
      "epoch:27 step:21473 [D loss: 0.348761, acc.: 78.12%] [G loss: 3.051715]\n",
      "epoch:27 step:21474 [D loss: 0.324440, acc.: 85.94%] [G loss: 2.596491]\n",
      "epoch:27 step:21475 [D loss: 0.460043, acc.: 82.03%] [G loss: 3.153864]\n",
      "epoch:27 step:21476 [D loss: 0.462467, acc.: 79.69%] [G loss: 2.462047]\n",
      "epoch:27 step:21477 [D loss: 0.315140, acc.: 86.72%] [G loss: 3.301484]\n",
      "epoch:27 step:21478 [D loss: 0.361079, acc.: 80.47%] [G loss: 3.632337]\n",
      "epoch:27 step:21479 [D loss: 0.253884, acc.: 89.06%] [G loss: 4.162534]\n",
      "epoch:27 step:21480 [D loss: 0.319348, acc.: 86.72%] [G loss: 3.097791]\n",
      "epoch:27 step:21481 [D loss: 0.277160, acc.: 90.62%] [G loss: 3.114564]\n",
      "epoch:27 step:21482 [D loss: 0.298226, acc.: 88.28%] [G loss: 4.909772]\n",
      "epoch:27 step:21483 [D loss: 0.311484, acc.: 86.72%] [G loss: 3.578510]\n",
      "epoch:27 step:21484 [D loss: 0.318107, acc.: 84.38%] [G loss: 5.101032]\n",
      "epoch:27 step:21485 [D loss: 0.275526, acc.: 89.84%] [G loss: 3.989269]\n",
      "epoch:27 step:21486 [D loss: 0.343348, acc.: 85.94%] [G loss: 2.381928]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:27 step:21487 [D loss: 0.243034, acc.: 89.06%] [G loss: 2.741979]\n",
      "epoch:27 step:21488 [D loss: 0.391388, acc.: 82.81%] [G loss: 2.696359]\n",
      "epoch:27 step:21489 [D loss: 0.295813, acc.: 84.38%] [G loss: 2.988708]\n",
      "epoch:27 step:21490 [D loss: 0.260278, acc.: 89.06%] [G loss: 3.490352]\n",
      "epoch:27 step:21491 [D loss: 0.288275, acc.: 87.50%] [G loss: 2.867750]\n",
      "epoch:27 step:21492 [D loss: 0.294889, acc.: 85.16%] [G loss: 3.268227]\n",
      "epoch:27 step:21493 [D loss: 0.424168, acc.: 82.81%] [G loss: 3.697354]\n",
      "epoch:27 step:21494 [D loss: 0.321070, acc.: 85.94%] [G loss: 3.848437]\n",
      "epoch:27 step:21495 [D loss: 0.309244, acc.: 87.50%] [G loss: 3.445093]\n",
      "epoch:27 step:21496 [D loss: 0.277130, acc.: 89.84%] [G loss: 2.385773]\n",
      "epoch:27 step:21497 [D loss: 0.285705, acc.: 87.50%] [G loss: 3.290933]\n",
      "epoch:27 step:21498 [D loss: 0.454574, acc.: 77.34%] [G loss: 3.037101]\n",
      "epoch:27 step:21499 [D loss: 0.252516, acc.: 89.06%] [G loss: 3.289936]\n",
      "epoch:27 step:21500 [D loss: 0.332882, acc.: 84.38%] [G loss: 2.814195]\n",
      "epoch:27 step:21501 [D loss: 0.357994, acc.: 84.38%] [G loss: 3.175595]\n",
      "epoch:27 step:21502 [D loss: 0.351015, acc.: 85.16%] [G loss: 2.748831]\n",
      "epoch:27 step:21503 [D loss: 0.381591, acc.: 86.72%] [G loss: 2.886864]\n",
      "epoch:27 step:21504 [D loss: 0.282085, acc.: 88.28%] [G loss: 3.246993]\n",
      "epoch:27 step:21505 [D loss: 0.292586, acc.: 86.72%] [G loss: 4.800507]\n",
      "epoch:27 step:21506 [D loss: 0.429004, acc.: 77.34%] [G loss: 2.749674]\n",
      "epoch:27 step:21507 [D loss: 0.197091, acc.: 90.62%] [G loss: 2.997352]\n",
      "epoch:27 step:21508 [D loss: 0.353625, acc.: 80.47%] [G loss: 3.125171]\n",
      "epoch:27 step:21509 [D loss: 0.288837, acc.: 89.84%] [G loss: 5.188647]\n",
      "epoch:27 step:21510 [D loss: 0.326424, acc.: 81.25%] [G loss: 3.604280]\n",
      "epoch:27 step:21511 [D loss: 0.276689, acc.: 85.94%] [G loss: 3.330806]\n",
      "epoch:27 step:21512 [D loss: 0.369300, acc.: 83.59%] [G loss: 2.473977]\n",
      "epoch:27 step:21513 [D loss: 0.253669, acc.: 91.41%] [G loss: 2.932124]\n",
      "epoch:27 step:21514 [D loss: 0.396586, acc.: 87.50%] [G loss: 2.605187]\n",
      "epoch:27 step:21515 [D loss: 0.438522, acc.: 79.69%] [G loss: 2.768339]\n",
      "epoch:27 step:21516 [D loss: 0.332123, acc.: 84.38%] [G loss: 2.827016]\n",
      "epoch:27 step:21517 [D loss: 0.261405, acc.: 89.84%] [G loss: 2.816435]\n",
      "epoch:27 step:21518 [D loss: 0.318343, acc.: 87.50%] [G loss: 2.363068]\n",
      "epoch:27 step:21519 [D loss: 0.365629, acc.: 81.25%] [G loss: 2.218960]\n",
      "epoch:27 step:21520 [D loss: 0.299093, acc.: 87.50%] [G loss: 3.336602]\n",
      "epoch:27 step:21521 [D loss: 0.487765, acc.: 81.25%] [G loss: 4.963154]\n",
      "epoch:27 step:21522 [D loss: 0.683511, acc.: 80.47%] [G loss: 7.744802]\n",
      "epoch:27 step:21523 [D loss: 1.747940, acc.: 57.03%] [G loss: 6.219597]\n",
      "epoch:27 step:21524 [D loss: 1.571516, acc.: 64.06%] [G loss: 4.321058]\n",
      "epoch:27 step:21525 [D loss: 0.378648, acc.: 82.03%] [G loss: 6.034496]\n",
      "epoch:27 step:21526 [D loss: 0.420636, acc.: 82.81%] [G loss: 4.653697]\n",
      "epoch:27 step:21527 [D loss: 0.475722, acc.: 80.47%] [G loss: 4.636769]\n",
      "epoch:27 step:21528 [D loss: 0.326216, acc.: 85.94%] [G loss: 6.702868]\n",
      "epoch:27 step:21529 [D loss: 0.642913, acc.: 76.56%] [G loss: 2.939772]\n",
      "epoch:27 step:21530 [D loss: 0.252696, acc.: 87.50%] [G loss: 4.044530]\n",
      "epoch:27 step:21531 [D loss: 0.245691, acc.: 90.62%] [G loss: 3.636364]\n",
      "epoch:27 step:21532 [D loss: 0.339811, acc.: 85.16%] [G loss: 3.276303]\n",
      "epoch:27 step:21533 [D loss: 0.313465, acc.: 87.50%] [G loss: 3.629372]\n",
      "epoch:27 step:21534 [D loss: 0.341020, acc.: 85.16%] [G loss: 2.749104]\n",
      "epoch:27 step:21535 [D loss: 0.403536, acc.: 81.25%] [G loss: 2.594186]\n",
      "epoch:27 step:21536 [D loss: 0.342570, acc.: 83.59%] [G loss: 2.406350]\n",
      "epoch:27 step:21537 [D loss: 0.267344, acc.: 89.84%] [G loss: 2.768280]\n",
      "epoch:27 step:21538 [D loss: 0.304356, acc.: 89.84%] [G loss: 3.505665]\n",
      "epoch:27 step:21539 [D loss: 0.323211, acc.: 85.94%] [G loss: 2.685468]\n",
      "epoch:27 step:21540 [D loss: 0.251912, acc.: 87.50%] [G loss: 2.804835]\n",
      "epoch:27 step:21541 [D loss: 0.237507, acc.: 92.19%] [G loss: 3.077831]\n",
      "epoch:27 step:21542 [D loss: 0.291608, acc.: 86.72%] [G loss: 2.403641]\n",
      "epoch:27 step:21543 [D loss: 0.419496, acc.: 78.12%] [G loss: 2.413610]\n",
      "epoch:27 step:21544 [D loss: 0.236915, acc.: 86.72%] [G loss: 2.761776]\n",
      "epoch:27 step:21545 [D loss: 0.262632, acc.: 93.75%] [G loss: 2.610851]\n",
      "epoch:27 step:21546 [D loss: 0.407341, acc.: 79.69%] [G loss: 2.932874]\n",
      "epoch:27 step:21547 [D loss: 0.348233, acc.: 83.59%] [G loss: 2.565449]\n",
      "epoch:27 step:21548 [D loss: 0.345900, acc.: 82.03%] [G loss: 2.740179]\n",
      "epoch:27 step:21549 [D loss: 0.321694, acc.: 85.16%] [G loss: 2.958096]\n",
      "epoch:27 step:21550 [D loss: 0.301269, acc.: 88.28%] [G loss: 2.862602]\n",
      "epoch:27 step:21551 [D loss: 0.304845, acc.: 86.72%] [G loss: 2.822630]\n",
      "epoch:27 step:21552 [D loss: 0.339030, acc.: 84.38%] [G loss: 2.158371]\n",
      "epoch:27 step:21553 [D loss: 0.271265, acc.: 90.62%] [G loss: 2.602115]\n",
      "epoch:27 step:21554 [D loss: 0.260560, acc.: 86.72%] [G loss: 3.356228]\n",
      "epoch:27 step:21555 [D loss: 0.314572, acc.: 85.94%] [G loss: 3.330000]\n",
      "epoch:27 step:21556 [D loss: 0.260646, acc.: 88.28%] [G loss: 2.638123]\n",
      "epoch:27 step:21557 [D loss: 0.382996, acc.: 81.25%] [G loss: 3.333210]\n",
      "epoch:27 step:21558 [D loss: 0.273504, acc.: 88.28%] [G loss: 2.580000]\n",
      "epoch:27 step:21559 [D loss: 0.295082, acc.: 87.50%] [G loss: 3.005354]\n",
      "epoch:27 step:21560 [D loss: 0.267309, acc.: 88.28%] [G loss: 3.382363]\n",
      "epoch:27 step:21561 [D loss: 0.312243, acc.: 85.94%] [G loss: 3.545902]\n",
      "epoch:27 step:21562 [D loss: 0.349976, acc.: 85.94%] [G loss: 2.892108]\n",
      "epoch:27 step:21563 [D loss: 0.342966, acc.: 87.50%] [G loss: 3.370631]\n",
      "epoch:27 step:21564 [D loss: 0.220162, acc.: 93.75%] [G loss: 2.758299]\n",
      "epoch:27 step:21565 [D loss: 0.264360, acc.: 90.62%] [G loss: 2.681728]\n",
      "epoch:27 step:21566 [D loss: 0.270816, acc.: 88.28%] [G loss: 2.923716]\n",
      "epoch:27 step:21567 [D loss: 0.227140, acc.: 91.41%] [G loss: 2.837427]\n",
      "epoch:27 step:21568 [D loss: 0.316277, acc.: 84.38%] [G loss: 2.404814]\n",
      "epoch:27 step:21569 [D loss: 0.344374, acc.: 82.03%] [G loss: 2.905166]\n",
      "epoch:27 step:21570 [D loss: 0.341021, acc.: 83.59%] [G loss: 3.636768]\n",
      "epoch:27 step:21571 [D loss: 0.325841, acc.: 85.16%] [G loss: 3.575794]\n",
      "epoch:27 step:21572 [D loss: 0.285656, acc.: 85.94%] [G loss: 3.457256]\n",
      "epoch:27 step:21573 [D loss: 0.349145, acc.: 82.81%] [G loss: 2.794493]\n",
      "epoch:27 step:21574 [D loss: 0.277531, acc.: 88.28%] [G loss: 3.123041]\n",
      "epoch:27 step:21575 [D loss: 0.340461, acc.: 82.81%] [G loss: 2.549748]\n",
      "epoch:27 step:21576 [D loss: 0.412216, acc.: 78.12%] [G loss: 2.503355]\n",
      "epoch:27 step:21577 [D loss: 0.349135, acc.: 83.59%] [G loss: 2.865979]\n",
      "epoch:27 step:21578 [D loss: 0.345400, acc.: 82.03%] [G loss: 2.941305]\n",
      "epoch:27 step:21579 [D loss: 0.294725, acc.: 89.84%] [G loss: 2.012788]\n",
      "epoch:27 step:21580 [D loss: 0.332086, acc.: 83.59%] [G loss: 2.836086]\n",
      "epoch:27 step:21581 [D loss: 0.270095, acc.: 89.84%] [G loss: 3.340569]\n",
      "epoch:27 step:21582 [D loss: 0.356263, acc.: 84.38%] [G loss: 2.826117]\n",
      "epoch:27 step:21583 [D loss: 0.256894, acc.: 88.28%] [G loss: 2.985127]\n",
      "epoch:27 step:21584 [D loss: 0.311725, acc.: 83.59%] [G loss: 3.221922]\n",
      "epoch:27 step:21585 [D loss: 0.201787, acc.: 92.19%] [G loss: 5.061124]\n",
      "epoch:27 step:21586 [D loss: 0.240387, acc.: 87.50%] [G loss: 3.669399]\n",
      "epoch:27 step:21587 [D loss: 0.378115, acc.: 82.81%] [G loss: 3.830040]\n",
      "epoch:27 step:21588 [D loss: 0.282597, acc.: 87.50%] [G loss: 3.837267]\n",
      "epoch:27 step:21589 [D loss: 0.185528, acc.: 92.19%] [G loss: 3.751572]\n",
      "epoch:27 step:21590 [D loss: 0.317557, acc.: 85.94%] [G loss: 2.693335]\n",
      "epoch:27 step:21591 [D loss: 0.283213, acc.: 87.50%] [G loss: 2.159616]\n",
      "epoch:27 step:21592 [D loss: 0.305732, acc.: 89.06%] [G loss: 3.263806]\n",
      "epoch:27 step:21593 [D loss: 0.313120, acc.: 83.59%] [G loss: 3.766544]\n",
      "epoch:27 step:21594 [D loss: 0.269665, acc.: 85.94%] [G loss: 2.824513]\n",
      "epoch:27 step:21595 [D loss: 0.328115, acc.: 82.81%] [G loss: 3.151798]\n",
      "epoch:27 step:21596 [D loss: 0.307831, acc.: 84.38%] [G loss: 3.780634]\n",
      "epoch:27 step:21597 [D loss: 0.439400, acc.: 74.22%] [G loss: 3.325368]\n",
      "epoch:27 step:21598 [D loss: 0.227454, acc.: 94.53%] [G loss: 3.227587]\n",
      "epoch:27 step:21599 [D loss: 0.282314, acc.: 85.16%] [G loss: 2.970962]\n",
      "epoch:27 step:21600 [D loss: 0.389689, acc.: 82.03%] [G loss: 3.360676]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##############\n",
      "[0.85062576 0.87259545 0.82154118 0.81974284 0.79021942 0.82320591\n",
      " 0.88893289 0.83746449 0.78824846 0.80358515]\n",
      "##########\n",
      "epoch:27 step:21601 [D loss: 0.323791, acc.: 85.94%] [G loss: 3.171060]\n",
      "epoch:27 step:21602 [D loss: 0.373471, acc.: 84.38%] [G loss: 2.912806]\n",
      "epoch:27 step:21603 [D loss: 0.276400, acc.: 86.72%] [G loss: 3.537704]\n",
      "epoch:27 step:21604 [D loss: 0.495473, acc.: 78.12%] [G loss: 4.420231]\n",
      "epoch:27 step:21605 [D loss: 0.472177, acc.: 78.12%] [G loss: 2.737719]\n",
      "epoch:27 step:21606 [D loss: 0.301891, acc.: 85.94%] [G loss: 3.461391]\n",
      "epoch:27 step:21607 [D loss: 0.405749, acc.: 81.25%] [G loss: 4.660527]\n",
      "epoch:27 step:21608 [D loss: 0.418623, acc.: 82.03%] [G loss: 2.253531]\n",
      "epoch:27 step:21609 [D loss: 0.418575, acc.: 77.34%] [G loss: 3.751142]\n",
      "epoch:27 step:21610 [D loss: 0.267504, acc.: 89.84%] [G loss: 3.472573]\n",
      "epoch:27 step:21611 [D loss: 0.537928, acc.: 70.31%] [G loss: 5.097736]\n",
      "epoch:27 step:21612 [D loss: 0.409431, acc.: 82.81%] [G loss: 5.821784]\n",
      "epoch:27 step:21613 [D loss: 0.404972, acc.: 82.81%] [G loss: 3.429110]\n",
      "epoch:27 step:21614 [D loss: 0.275155, acc.: 89.84%] [G loss: 2.503135]\n",
      "epoch:27 step:21615 [D loss: 0.297291, acc.: 86.72%] [G loss: 2.771117]\n",
      "epoch:27 step:21616 [D loss: 0.348192, acc.: 81.25%] [G loss: 2.798602]\n",
      "epoch:27 step:21617 [D loss: 0.249533, acc.: 90.62%] [G loss: 2.806308]\n",
      "epoch:27 step:21618 [D loss: 0.225747, acc.: 89.84%] [G loss: 2.942469]\n",
      "epoch:27 step:21619 [D loss: 0.361494, acc.: 85.16%] [G loss: 3.545638]\n",
      "epoch:27 step:21620 [D loss: 0.277863, acc.: 85.94%] [G loss: 4.121532]\n",
      "epoch:27 step:21621 [D loss: 0.269163, acc.: 88.28%] [G loss: 2.399255]\n",
      "epoch:27 step:21622 [D loss: 0.344590, acc.: 82.81%] [G loss: 3.357264]\n",
      "epoch:27 step:21623 [D loss: 0.367405, acc.: 81.25%] [G loss: 2.331157]\n",
      "epoch:27 step:21624 [D loss: 0.372163, acc.: 82.03%] [G loss: 2.921648]\n",
      "epoch:27 step:21625 [D loss: 0.358501, acc.: 85.94%] [G loss: 2.331648]\n",
      "epoch:27 step:21626 [D loss: 0.348429, acc.: 88.28%] [G loss: 2.202826]\n",
      "epoch:27 step:21627 [D loss: 0.384227, acc.: 84.38%] [G loss: 2.431285]\n",
      "epoch:27 step:21628 [D loss: 0.347070, acc.: 82.81%] [G loss: 3.613914]\n",
      "epoch:27 step:21629 [D loss: 0.385211, acc.: 78.91%] [G loss: 2.764501]\n",
      "epoch:27 step:21630 [D loss: 0.287284, acc.: 83.59%] [G loss: 3.213233]\n",
      "epoch:27 step:21631 [D loss: 0.297750, acc.: 89.06%] [G loss: 3.478794]\n",
      "epoch:27 step:21632 [D loss: 0.318603, acc.: 86.72%] [G loss: 4.121058]\n",
      "epoch:27 step:21633 [D loss: 0.297936, acc.: 85.94%] [G loss: 2.832168]\n",
      "epoch:27 step:21634 [D loss: 0.357422, acc.: 82.03%] [G loss: 2.556318]\n",
      "epoch:27 step:21635 [D loss: 0.304912, acc.: 82.81%] [G loss: 3.165335]\n",
      "epoch:27 step:21636 [D loss: 0.294639, acc.: 87.50%] [G loss: 2.679407]\n",
      "epoch:27 step:21637 [D loss: 0.324578, acc.: 89.06%] [G loss: 2.434873]\n",
      "epoch:27 step:21638 [D loss: 0.323024, acc.: 85.16%] [G loss: 2.806016]\n",
      "epoch:27 step:21639 [D loss: 0.465457, acc.: 82.03%] [G loss: 3.015972]\n",
      "epoch:27 step:21640 [D loss: 0.323902, acc.: 84.38%] [G loss: 2.835475]\n",
      "epoch:27 step:21641 [D loss: 0.393785, acc.: 84.38%] [G loss: 2.544487]\n",
      "epoch:27 step:21642 [D loss: 0.354393, acc.: 85.16%] [G loss: 3.656586]\n",
      "epoch:27 step:21643 [D loss: 0.582157, acc.: 75.00%] [G loss: 4.143184]\n",
      "epoch:27 step:21644 [D loss: 0.426625, acc.: 83.59%] [G loss: 4.535456]\n",
      "epoch:27 step:21645 [D loss: 0.290545, acc.: 86.72%] [G loss: 3.083208]\n",
      "epoch:27 step:21646 [D loss: 0.270624, acc.: 91.41%] [G loss: 2.927921]\n",
      "epoch:27 step:21647 [D loss: 0.277535, acc.: 87.50%] [G loss: 2.754340]\n",
      "epoch:27 step:21648 [D loss: 0.351793, acc.: 86.72%] [G loss: 3.507279]\n",
      "epoch:27 step:21649 [D loss: 0.298973, acc.: 89.06%] [G loss: 4.408883]\n",
      "epoch:27 step:21650 [D loss: 0.264395, acc.: 88.28%] [G loss: 4.273114]\n",
      "epoch:27 step:21651 [D loss: 0.354579, acc.: 82.81%] [G loss: 4.068209]\n",
      "epoch:27 step:21652 [D loss: 0.226036, acc.: 90.62%] [G loss: 4.510250]\n",
      "epoch:27 step:21653 [D loss: 0.262421, acc.: 89.06%] [G loss: 2.850120]\n",
      "epoch:27 step:21654 [D loss: 0.319459, acc.: 85.16%] [G loss: 3.349834]\n",
      "epoch:27 step:21655 [D loss: 0.294984, acc.: 86.72%] [G loss: 3.354762]\n",
      "epoch:27 step:21656 [D loss: 0.252918, acc.: 89.84%] [G loss: 2.968634]\n",
      "epoch:27 step:21657 [D loss: 0.303195, acc.: 85.94%] [G loss: 5.361238]\n",
      "epoch:27 step:21658 [D loss: 0.219057, acc.: 92.19%] [G loss: 4.867817]\n",
      "epoch:27 step:21659 [D loss: 0.224183, acc.: 92.19%] [G loss: 4.483643]\n",
      "epoch:27 step:21660 [D loss: 0.257998, acc.: 89.06%] [G loss: 4.909659]\n",
      "epoch:27 step:21661 [D loss: 0.244642, acc.: 94.53%] [G loss: 3.284213]\n",
      "epoch:27 step:21662 [D loss: 0.308695, acc.: 84.38%] [G loss: 3.548413]\n",
      "epoch:27 step:21663 [D loss: 0.294434, acc.: 86.72%] [G loss: 3.498607]\n",
      "epoch:27 step:21664 [D loss: 0.316912, acc.: 88.28%] [G loss: 3.359739]\n",
      "epoch:27 step:21665 [D loss: 0.220298, acc.: 87.50%] [G loss: 3.738201]\n",
      "epoch:27 step:21666 [D loss: 0.204208, acc.: 91.41%] [G loss: 3.683574]\n",
      "epoch:27 step:21667 [D loss: 0.261578, acc.: 89.84%] [G loss: 3.767509]\n",
      "epoch:27 step:21668 [D loss: 0.212055, acc.: 90.62%] [G loss: 4.381164]\n",
      "epoch:27 step:21669 [D loss: 0.353431, acc.: 83.59%] [G loss: 3.336014]\n",
      "epoch:27 step:21670 [D loss: 0.303502, acc.: 86.72%] [G loss: 3.564632]\n",
      "epoch:27 step:21671 [D loss: 0.426234, acc.: 80.47%] [G loss: 2.831678]\n",
      "epoch:27 step:21672 [D loss: 0.323703, acc.: 85.16%] [G loss: 3.252751]\n",
      "epoch:27 step:21673 [D loss: 0.377315, acc.: 81.25%] [G loss: 4.364546]\n",
      "epoch:27 step:21674 [D loss: 0.408354, acc.: 79.69%] [G loss: 3.679008]\n",
      "epoch:27 step:21675 [D loss: 0.208489, acc.: 93.75%] [G loss: 3.324610]\n",
      "epoch:27 step:21676 [D loss: 0.351193, acc.: 85.94%] [G loss: 4.677289]\n",
      "epoch:27 step:21677 [D loss: 0.441821, acc.: 79.69%] [G loss: 4.306131]\n",
      "epoch:27 step:21678 [D loss: 0.451415, acc.: 78.12%] [G loss: 3.001761]\n",
      "epoch:27 step:21679 [D loss: 0.263712, acc.: 89.06%] [G loss: 4.064774]\n",
      "epoch:27 step:21680 [D loss: 0.314592, acc.: 86.72%] [G loss: 3.819752]\n",
      "epoch:27 step:21681 [D loss: 0.382196, acc.: 83.59%] [G loss: 3.276704]\n",
      "epoch:27 step:21682 [D loss: 0.291604, acc.: 82.81%] [G loss: 3.833863]\n",
      "epoch:27 step:21683 [D loss: 0.447956, acc.: 78.91%] [G loss: 4.012888]\n",
      "epoch:27 step:21684 [D loss: 0.348857, acc.: 83.59%] [G loss: 3.257519]\n",
      "epoch:27 step:21685 [D loss: 0.278702, acc.: 86.72%] [G loss: 3.153192]\n",
      "epoch:27 step:21686 [D loss: 0.351656, acc.: 81.25%] [G loss: 2.888119]\n",
      "epoch:27 step:21687 [D loss: 0.248382, acc.: 90.62%] [G loss: 5.580836]\n",
      "epoch:27 step:21688 [D loss: 0.231063, acc.: 91.41%] [G loss: 4.011793]\n",
      "epoch:27 step:21689 [D loss: 0.253850, acc.: 89.06%] [G loss: 5.725885]\n",
      "epoch:27 step:21690 [D loss: 0.206992, acc.: 92.19%] [G loss: 3.507483]\n",
      "epoch:27 step:21691 [D loss: 0.253633, acc.: 88.28%] [G loss: 3.853033]\n",
      "epoch:27 step:21692 [D loss: 0.374376, acc.: 81.25%] [G loss: 3.031220]\n",
      "epoch:27 step:21693 [D loss: 0.335257, acc.: 82.03%] [G loss: 2.784301]\n",
      "epoch:27 step:21694 [D loss: 0.225808, acc.: 92.19%] [G loss: 2.612359]\n",
      "epoch:27 step:21695 [D loss: 0.270213, acc.: 86.72%] [G loss: 3.918833]\n",
      "epoch:27 step:21696 [D loss: 0.234163, acc.: 89.84%] [G loss: 3.049539]\n",
      "epoch:27 step:21697 [D loss: 0.361976, acc.: 85.94%] [G loss: 4.117779]\n",
      "epoch:27 step:21698 [D loss: 0.385830, acc.: 78.91%] [G loss: 2.874520]\n",
      "epoch:27 step:21699 [D loss: 0.277286, acc.: 86.72%] [G loss: 2.839285]\n",
      "epoch:27 step:21700 [D loss: 0.353086, acc.: 83.59%] [G loss: 3.200892]\n",
      "epoch:27 step:21701 [D loss: 0.247874, acc.: 89.06%] [G loss: 4.020504]\n",
      "epoch:27 step:21702 [D loss: 0.367127, acc.: 83.59%] [G loss: 2.600793]\n",
      "epoch:27 step:21703 [D loss: 0.349339, acc.: 85.16%] [G loss: 3.444542]\n",
      "epoch:27 step:21704 [D loss: 0.324686, acc.: 86.72%] [G loss: 3.034813]\n",
      "epoch:27 step:21705 [D loss: 0.340074, acc.: 85.16%] [G loss: 3.393695]\n",
      "epoch:27 step:21706 [D loss: 0.304951, acc.: 88.28%] [G loss: 2.539243]\n",
      "epoch:27 step:21707 [D loss: 0.320717, acc.: 85.16%] [G loss: 4.200397]\n",
      "epoch:27 step:21708 [D loss: 0.301539, acc.: 87.50%] [G loss: 4.913208]\n",
      "epoch:27 step:21709 [D loss: 0.266313, acc.: 89.06%] [G loss: 5.273829]\n",
      "epoch:27 step:21710 [D loss: 0.257598, acc.: 89.84%] [G loss: 4.014148]\n",
      "epoch:27 step:21711 [D loss: 0.311504, acc.: 86.72%] [G loss: 2.936918]\n",
      "epoch:27 step:21712 [D loss: 0.288320, acc.: 85.94%] [G loss: 3.622353]\n",
      "epoch:27 step:21713 [D loss: 0.269665, acc.: 86.72%] [G loss: 2.983922]\n",
      "epoch:27 step:21714 [D loss: 0.356212, acc.: 82.81%] [G loss: 3.061541]\n",
      "epoch:27 step:21715 [D loss: 0.340960, acc.: 80.47%] [G loss: 4.287431]\n",
      "epoch:27 step:21716 [D loss: 0.294147, acc.: 87.50%] [G loss: 4.698468]\n",
      "epoch:27 step:21717 [D loss: 0.364912, acc.: 84.38%] [G loss: 4.229977]\n",
      "epoch:27 step:21718 [D loss: 0.457354, acc.: 74.22%] [G loss: 4.027410]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:27 step:21719 [D loss: 0.284801, acc.: 85.94%] [G loss: 3.520041]\n",
      "epoch:27 step:21720 [D loss: 0.234818, acc.: 90.62%] [G loss: 4.247921]\n",
      "epoch:27 step:21721 [D loss: 0.378013, acc.: 81.25%] [G loss: 4.373945]\n",
      "epoch:27 step:21722 [D loss: 0.274404, acc.: 86.72%] [G loss: 4.443593]\n",
      "epoch:27 step:21723 [D loss: 0.196178, acc.: 93.75%] [G loss: 3.838205]\n",
      "epoch:27 step:21724 [D loss: 0.272214, acc.: 89.06%] [G loss: 3.958356]\n",
      "epoch:27 step:21725 [D loss: 0.280587, acc.: 88.28%] [G loss: 6.881105]\n",
      "epoch:27 step:21726 [D loss: 0.361955, acc.: 82.81%] [G loss: 4.192802]\n",
      "epoch:27 step:21727 [D loss: 0.210528, acc.: 90.62%] [G loss: 3.168520]\n",
      "epoch:27 step:21728 [D loss: 0.228973, acc.: 91.41%] [G loss: 3.050981]\n",
      "epoch:27 step:21729 [D loss: 0.337720, acc.: 82.81%] [G loss: 2.636776]\n",
      "epoch:27 step:21730 [D loss: 0.216212, acc.: 93.75%] [G loss: 3.732775]\n",
      "epoch:27 step:21731 [D loss: 0.307087, acc.: 85.16%] [G loss: 2.932375]\n",
      "epoch:27 step:21732 [D loss: 0.336767, acc.: 85.16%] [G loss: 3.107780]\n",
      "epoch:27 step:21733 [D loss: 0.295018, acc.: 87.50%] [G loss: 2.884057]\n",
      "epoch:27 step:21734 [D loss: 0.204702, acc.: 92.19%] [G loss: 3.784183]\n",
      "epoch:27 step:21735 [D loss: 0.411763, acc.: 78.91%] [G loss: 3.374213]\n",
      "epoch:27 step:21736 [D loss: 0.258859, acc.: 89.06%] [G loss: 4.339887]\n",
      "epoch:27 step:21737 [D loss: 0.330109, acc.: 85.16%] [G loss: 2.833260]\n",
      "epoch:27 step:21738 [D loss: 0.322333, acc.: 84.38%] [G loss: 2.873605]\n",
      "epoch:27 step:21739 [D loss: 0.255420, acc.: 86.72%] [G loss: 3.763467]\n",
      "epoch:27 step:21740 [D loss: 0.266991, acc.: 84.38%] [G loss: 3.463667]\n",
      "epoch:27 step:21741 [D loss: 0.345378, acc.: 84.38%] [G loss: 3.710854]\n",
      "epoch:27 step:21742 [D loss: 0.389239, acc.: 84.38%] [G loss: 4.969852]\n",
      "epoch:27 step:21743 [D loss: 0.455495, acc.: 78.12%] [G loss: 3.206684]\n",
      "epoch:27 step:21744 [D loss: 0.329159, acc.: 83.59%] [G loss: 4.586335]\n",
      "epoch:27 step:21745 [D loss: 0.301406, acc.: 86.72%] [G loss: 3.606112]\n",
      "epoch:27 step:21746 [D loss: 0.363877, acc.: 79.69%] [G loss: 4.092860]\n",
      "epoch:27 step:21747 [D loss: 0.343196, acc.: 87.50%] [G loss: 3.228787]\n",
      "epoch:27 step:21748 [D loss: 0.381812, acc.: 82.81%] [G loss: 3.306637]\n",
      "epoch:27 step:21749 [D loss: 0.368478, acc.: 81.25%] [G loss: 3.466846]\n",
      "epoch:27 step:21750 [D loss: 0.273891, acc.: 88.28%] [G loss: 4.988477]\n",
      "epoch:27 step:21751 [D loss: 0.400497, acc.: 79.69%] [G loss: 3.452199]\n",
      "epoch:27 step:21752 [D loss: 0.205229, acc.: 90.62%] [G loss: 4.191813]\n",
      "epoch:27 step:21753 [D loss: 0.369070, acc.: 84.38%] [G loss: 4.983078]\n",
      "epoch:27 step:21754 [D loss: 0.283258, acc.: 88.28%] [G loss: 3.883367]\n",
      "epoch:27 step:21755 [D loss: 0.292414, acc.: 89.06%] [G loss: 3.499272]\n",
      "epoch:27 step:21756 [D loss: 0.235373, acc.: 89.06%] [G loss: 4.592703]\n",
      "epoch:27 step:21757 [D loss: 0.351783, acc.: 84.38%] [G loss: 3.726961]\n",
      "epoch:27 step:21758 [D loss: 0.328039, acc.: 84.38%] [G loss: 3.177522]\n",
      "epoch:27 step:21759 [D loss: 0.233001, acc.: 91.41%] [G loss: 3.350987]\n",
      "epoch:27 step:21760 [D loss: 0.391744, acc.: 85.16%] [G loss: 2.898099]\n",
      "epoch:27 step:21761 [D loss: 0.317824, acc.: 84.38%] [G loss: 4.266331]\n",
      "epoch:27 step:21762 [D loss: 0.316917, acc.: 88.28%] [G loss: 5.240919]\n",
      "epoch:27 step:21763 [D loss: 0.313416, acc.: 85.94%] [G loss: 4.675674]\n",
      "epoch:27 step:21764 [D loss: 0.305063, acc.: 85.94%] [G loss: 3.766260]\n",
      "epoch:27 step:21765 [D loss: 0.336897, acc.: 83.59%] [G loss: 4.829384]\n",
      "epoch:27 step:21766 [D loss: 0.352887, acc.: 80.47%] [G loss: 3.746354]\n",
      "epoch:27 step:21767 [D loss: 0.346774, acc.: 86.72%] [G loss: 4.287996]\n",
      "epoch:27 step:21768 [D loss: 0.256525, acc.: 88.28%] [G loss: 4.076221]\n",
      "epoch:27 step:21769 [D loss: 0.360627, acc.: 81.25%] [G loss: 2.949501]\n",
      "epoch:27 step:21770 [D loss: 0.351036, acc.: 85.16%] [G loss: 3.423932]\n",
      "epoch:27 step:21771 [D loss: 0.421827, acc.: 78.12%] [G loss: 3.879522]\n",
      "epoch:27 step:21772 [D loss: 0.353067, acc.: 81.25%] [G loss: 3.082105]\n",
      "epoch:27 step:21773 [D loss: 0.318242, acc.: 81.25%] [G loss: 3.676629]\n",
      "epoch:27 step:21774 [D loss: 0.195998, acc.: 95.31%] [G loss: 5.089724]\n",
      "epoch:27 step:21775 [D loss: 0.341742, acc.: 83.59%] [G loss: 3.865762]\n",
      "epoch:27 step:21776 [D loss: 0.437203, acc.: 83.59%] [G loss: 4.994679]\n",
      "epoch:27 step:21777 [D loss: 0.332449, acc.: 84.38%] [G loss: 4.098928]\n",
      "epoch:27 step:21778 [D loss: 0.376451, acc.: 84.38%] [G loss: 4.170796]\n",
      "epoch:27 step:21779 [D loss: 0.429677, acc.: 78.91%] [G loss: 5.307503]\n",
      "epoch:27 step:21780 [D loss: 0.369360, acc.: 86.72%] [G loss: 3.771303]\n",
      "epoch:27 step:21781 [D loss: 0.285014, acc.: 83.59%] [G loss: 3.833445]\n",
      "epoch:27 step:21782 [D loss: 0.445668, acc.: 78.91%] [G loss: 7.331175]\n",
      "epoch:27 step:21783 [D loss: 0.568462, acc.: 75.78%] [G loss: 6.205471]\n",
      "epoch:27 step:21784 [D loss: 0.835273, acc.: 69.53%] [G loss: 8.416048]\n",
      "epoch:27 step:21785 [D loss: 0.852093, acc.: 74.22%] [G loss: 4.341743]\n",
      "epoch:27 step:21786 [D loss: 0.324830, acc.: 86.72%] [G loss: 4.174712]\n",
      "epoch:27 step:21787 [D loss: 0.338843, acc.: 83.59%] [G loss: 4.802293]\n",
      "epoch:27 step:21788 [D loss: 0.440610, acc.: 82.81%] [G loss: 6.119377]\n",
      "epoch:27 step:21789 [D loss: 0.330016, acc.: 85.94%] [G loss: 5.804995]\n",
      "epoch:27 step:21790 [D loss: 0.319496, acc.: 84.38%] [G loss: 5.088166]\n",
      "epoch:27 step:21791 [D loss: 0.345979, acc.: 82.81%] [G loss: 3.798401]\n",
      "epoch:27 step:21792 [D loss: 0.207146, acc.: 92.19%] [G loss: 4.579593]\n",
      "epoch:27 step:21793 [D loss: 0.299667, acc.: 85.16%] [G loss: 4.907880]\n",
      "epoch:27 step:21794 [D loss: 0.339277, acc.: 84.38%] [G loss: 4.160120]\n",
      "epoch:27 step:21795 [D loss: 0.360824, acc.: 82.81%] [G loss: 3.012408]\n",
      "epoch:27 step:21796 [D loss: 0.276499, acc.: 86.72%] [G loss: 3.822233]\n",
      "epoch:27 step:21797 [D loss: 0.363987, acc.: 81.25%] [G loss: 3.880159]\n",
      "epoch:27 step:21798 [D loss: 0.353500, acc.: 86.72%] [G loss: 2.961878]\n",
      "epoch:27 step:21799 [D loss: 0.311822, acc.: 84.38%] [G loss: 4.056376]\n",
      "epoch:27 step:21800 [D loss: 0.402005, acc.: 80.47%] [G loss: 4.200929]\n",
      "##############\n",
      "[0.86092912 0.862411   0.79192386 0.82081874 0.76840843 0.81311275\n",
      " 0.86893915 0.83620613 0.81284006 0.83306619]\n",
      "##########\n",
      "epoch:27 step:21801 [D loss: 0.400567, acc.: 80.47%] [G loss: 3.257513]\n",
      "epoch:27 step:21802 [D loss: 0.391396, acc.: 78.91%] [G loss: 3.970808]\n",
      "epoch:27 step:21803 [D loss: 0.450642, acc.: 79.69%] [G loss: 3.652749]\n",
      "epoch:27 step:21804 [D loss: 0.295487, acc.: 85.94%] [G loss: 3.049494]\n",
      "epoch:27 step:21805 [D loss: 0.285282, acc.: 86.72%] [G loss: 3.320620]\n",
      "epoch:27 step:21806 [D loss: 0.346153, acc.: 81.25%] [G loss: 3.749721]\n",
      "epoch:27 step:21807 [D loss: 0.306969, acc.: 86.72%] [G loss: 3.165656]\n",
      "epoch:27 step:21808 [D loss: 0.541363, acc.: 76.56%] [G loss: 4.157128]\n",
      "epoch:27 step:21809 [D loss: 0.326479, acc.: 88.28%] [G loss: 2.718928]\n",
      "epoch:27 step:21810 [D loss: 0.314451, acc.: 87.50%] [G loss: 4.784935]\n",
      "epoch:27 step:21811 [D loss: 0.496000, acc.: 77.34%] [G loss: 3.165115]\n",
      "epoch:27 step:21812 [D loss: 0.295692, acc.: 85.94%] [G loss: 3.848531]\n",
      "epoch:27 step:21813 [D loss: 0.311273, acc.: 85.16%] [G loss: 4.990617]\n",
      "epoch:27 step:21814 [D loss: 0.369663, acc.: 83.59%] [G loss: 4.188623]\n",
      "epoch:27 step:21815 [D loss: 0.424460, acc.: 79.69%] [G loss: 3.049835]\n",
      "epoch:27 step:21816 [D loss: 0.321678, acc.: 87.50%] [G loss: 3.744456]\n",
      "epoch:27 step:21817 [D loss: 0.261383, acc.: 88.28%] [G loss: 3.547683]\n",
      "epoch:27 step:21818 [D loss: 0.411519, acc.: 82.81%] [G loss: 3.419407]\n",
      "epoch:27 step:21819 [D loss: 0.311090, acc.: 85.94%] [G loss: 3.780383]\n",
      "epoch:27 step:21820 [D loss: 0.268910, acc.: 85.94%] [G loss: 3.881514]\n",
      "epoch:27 step:21821 [D loss: 0.267020, acc.: 89.06%] [G loss: 3.258480]\n",
      "epoch:27 step:21822 [D loss: 0.279168, acc.: 87.50%] [G loss: 2.462130]\n",
      "epoch:27 step:21823 [D loss: 0.284069, acc.: 84.38%] [G loss: 3.588636]\n",
      "epoch:27 step:21824 [D loss: 0.358134, acc.: 80.47%] [G loss: 3.915059]\n",
      "epoch:27 step:21825 [D loss: 0.277378, acc.: 87.50%] [G loss: 2.529013]\n",
      "epoch:27 step:21826 [D loss: 0.273074, acc.: 88.28%] [G loss: 3.734334]\n",
      "epoch:27 step:21827 [D loss: 0.359957, acc.: 79.69%] [G loss: 3.078273]\n",
      "epoch:27 step:21828 [D loss: 0.295582, acc.: 83.59%] [G loss: 3.120323]\n",
      "epoch:27 step:21829 [D loss: 0.262550, acc.: 89.84%] [G loss: 3.453205]\n",
      "epoch:27 step:21830 [D loss: 0.304202, acc.: 89.84%] [G loss: 2.869745]\n",
      "epoch:27 step:21831 [D loss: 0.372110, acc.: 78.91%] [G loss: 3.288150]\n",
      "epoch:27 step:21832 [D loss: 0.348440, acc.: 85.94%] [G loss: 2.712250]\n",
      "epoch:27 step:21833 [D loss: 0.264025, acc.: 90.62%] [G loss: 4.127021]\n",
      "epoch:27 step:21834 [D loss: 0.284084, acc.: 88.28%] [G loss: 3.620812]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:27 step:21835 [D loss: 0.321126, acc.: 85.16%] [G loss: 3.659495]\n",
      "epoch:27 step:21836 [D loss: 0.331286, acc.: 83.59%] [G loss: 3.658430]\n",
      "epoch:27 step:21837 [D loss: 0.290455, acc.: 89.06%] [G loss: 4.195717]\n",
      "epoch:27 step:21838 [D loss: 0.245077, acc.: 90.62%] [G loss: 3.127236]\n",
      "epoch:27 step:21839 [D loss: 0.269978, acc.: 84.38%] [G loss: 5.127408]\n",
      "epoch:27 step:21840 [D loss: 0.303908, acc.: 89.06%] [G loss: 3.226334]\n",
      "epoch:27 step:21841 [D loss: 0.333403, acc.: 84.38%] [G loss: 4.135912]\n",
      "epoch:27 step:21842 [D loss: 0.241696, acc.: 89.84%] [G loss: 4.262575]\n",
      "epoch:27 step:21843 [D loss: 0.366570, acc.: 85.16%] [G loss: 2.967993]\n",
      "epoch:27 step:21844 [D loss: 0.322177, acc.: 83.59%] [G loss: 2.857811]\n",
      "epoch:27 step:21845 [D loss: 0.256977, acc.: 91.41%] [G loss: 3.127194]\n",
      "epoch:27 step:21846 [D loss: 0.260602, acc.: 91.41%] [G loss: 3.417057]\n",
      "epoch:27 step:21847 [D loss: 0.351799, acc.: 85.16%] [G loss: 3.632956]\n",
      "epoch:27 step:21848 [D loss: 0.337835, acc.: 87.50%] [G loss: 3.136621]\n",
      "epoch:27 step:21849 [D loss: 0.295044, acc.: 87.50%] [G loss: 3.334110]\n",
      "epoch:27 step:21850 [D loss: 0.324167, acc.: 85.16%] [G loss: 2.524632]\n",
      "epoch:27 step:21851 [D loss: 0.347570, acc.: 85.16%] [G loss: 3.401719]\n",
      "epoch:27 step:21852 [D loss: 0.310182, acc.: 85.94%] [G loss: 3.347570]\n",
      "epoch:27 step:21853 [D loss: 0.401403, acc.: 80.47%] [G loss: 2.637174]\n",
      "epoch:27 step:21854 [D loss: 0.301294, acc.: 87.50%] [G loss: 3.275946]\n",
      "epoch:27 step:21855 [D loss: 0.269703, acc.: 87.50%] [G loss: 3.363949]\n",
      "epoch:27 step:21856 [D loss: 0.204572, acc.: 90.62%] [G loss: 7.045246]\n",
      "epoch:27 step:21857 [D loss: 0.217878, acc.: 89.06%] [G loss: 5.556153]\n",
      "epoch:27 step:21858 [D loss: 0.273615, acc.: 89.06%] [G loss: 4.740096]\n",
      "epoch:27 step:21859 [D loss: 0.286871, acc.: 86.72%] [G loss: 4.671894]\n",
      "epoch:27 step:21860 [D loss: 0.379570, acc.: 82.81%] [G loss: 6.893765]\n",
      "epoch:27 step:21861 [D loss: 0.717543, acc.: 67.19%] [G loss: 7.369299]\n",
      "epoch:27 step:21862 [D loss: 1.459554, acc.: 71.09%] [G loss: 10.277378]\n",
      "epoch:27 step:21863 [D loss: 1.941045, acc.: 65.62%] [G loss: 4.150403]\n",
      "epoch:27 step:21864 [D loss: 0.573983, acc.: 75.00%] [G loss: 4.750210]\n",
      "epoch:27 step:21865 [D loss: 0.451702, acc.: 80.47%] [G loss: 3.522663]\n",
      "epoch:27 step:21866 [D loss: 0.506196, acc.: 76.56%] [G loss: 3.376929]\n",
      "epoch:27 step:21867 [D loss: 0.310574, acc.: 84.38%] [G loss: 3.509302]\n",
      "epoch:27 step:21868 [D loss: 0.395979, acc.: 83.59%] [G loss: 3.883781]\n",
      "epoch:28 step:21869 [D loss: 0.339522, acc.: 85.16%] [G loss: 2.913596]\n",
      "epoch:28 step:21870 [D loss: 0.238650, acc.: 91.41%] [G loss: 3.276374]\n",
      "epoch:28 step:21871 [D loss: 0.426609, acc.: 80.47%] [G loss: 2.763030]\n",
      "epoch:28 step:21872 [D loss: 0.244861, acc.: 89.06%] [G loss: 2.969208]\n",
      "epoch:28 step:21873 [D loss: 0.262067, acc.: 89.84%] [G loss: 3.027884]\n",
      "epoch:28 step:21874 [D loss: 0.343410, acc.: 87.50%] [G loss: 2.698372]\n",
      "epoch:28 step:21875 [D loss: 0.306812, acc.: 85.94%] [G loss: 2.645670]\n",
      "epoch:28 step:21876 [D loss: 0.351388, acc.: 84.38%] [G loss: 2.298846]\n",
      "epoch:28 step:21877 [D loss: 0.350789, acc.: 83.59%] [G loss: 2.342648]\n",
      "epoch:28 step:21878 [D loss: 0.356572, acc.: 85.94%] [G loss: 2.591698]\n",
      "epoch:28 step:21879 [D loss: 0.420449, acc.: 78.91%] [G loss: 3.271852]\n",
      "epoch:28 step:21880 [D loss: 0.313830, acc.: 89.06%] [G loss: 3.356049]\n",
      "epoch:28 step:21881 [D loss: 0.294948, acc.: 85.94%] [G loss: 3.507903]\n",
      "epoch:28 step:21882 [D loss: 0.305931, acc.: 85.94%] [G loss: 3.221744]\n",
      "epoch:28 step:21883 [D loss: 0.269535, acc.: 87.50%] [G loss: 3.479963]\n",
      "epoch:28 step:21884 [D loss: 0.280915, acc.: 89.06%] [G loss: 2.526886]\n",
      "epoch:28 step:21885 [D loss: 0.335943, acc.: 82.81%] [G loss: 3.285428]\n",
      "epoch:28 step:21886 [D loss: 0.389244, acc.: 81.25%] [G loss: 2.975102]\n",
      "epoch:28 step:21887 [D loss: 0.241475, acc.: 88.28%] [G loss: 4.572966]\n",
      "epoch:28 step:21888 [D loss: 0.303751, acc.: 85.94%] [G loss: 3.525764]\n",
      "epoch:28 step:21889 [D loss: 0.289064, acc.: 86.72%] [G loss: 3.649150]\n",
      "epoch:28 step:21890 [D loss: 0.290557, acc.: 87.50%] [G loss: 2.884392]\n",
      "epoch:28 step:21891 [D loss: 0.208405, acc.: 90.62%] [G loss: 4.644578]\n",
      "epoch:28 step:21892 [D loss: 0.334571, acc.: 84.38%] [G loss: 4.042752]\n",
      "epoch:28 step:21893 [D loss: 0.213416, acc.: 90.62%] [G loss: 3.365111]\n",
      "epoch:28 step:21894 [D loss: 0.279874, acc.: 89.06%] [G loss: 4.095641]\n",
      "epoch:28 step:21895 [D loss: 0.328535, acc.: 85.16%] [G loss: 5.847553]\n",
      "epoch:28 step:21896 [D loss: 0.408923, acc.: 80.47%] [G loss: 3.839609]\n",
      "epoch:28 step:21897 [D loss: 0.367502, acc.: 85.94%] [G loss: 3.937288]\n",
      "epoch:28 step:21898 [D loss: 0.272865, acc.: 89.84%] [G loss: 3.290754]\n",
      "epoch:28 step:21899 [D loss: 0.304060, acc.: 85.16%] [G loss: 3.641943]\n",
      "epoch:28 step:21900 [D loss: 0.386112, acc.: 84.38%] [G loss: 2.499019]\n",
      "epoch:28 step:21901 [D loss: 0.306898, acc.: 89.06%] [G loss: 2.627775]\n",
      "epoch:28 step:21902 [D loss: 0.372796, acc.: 82.81%] [G loss: 2.543513]\n",
      "epoch:28 step:21903 [D loss: 0.320224, acc.: 87.50%] [G loss: 3.010051]\n",
      "epoch:28 step:21904 [D loss: 0.242646, acc.: 89.84%] [G loss: 3.488276]\n",
      "epoch:28 step:21905 [D loss: 0.349907, acc.: 82.81%] [G loss: 2.367340]\n",
      "epoch:28 step:21906 [D loss: 0.376382, acc.: 85.16%] [G loss: 3.263293]\n",
      "epoch:28 step:21907 [D loss: 0.227214, acc.: 89.84%] [G loss: 2.649399]\n",
      "epoch:28 step:21908 [D loss: 0.254849, acc.: 89.06%] [G loss: 4.032581]\n",
      "epoch:28 step:21909 [D loss: 0.352320, acc.: 82.81%] [G loss: 3.409686]\n",
      "epoch:28 step:21910 [D loss: 0.283290, acc.: 85.94%] [G loss: 2.768899]\n",
      "epoch:28 step:21911 [D loss: 0.303628, acc.: 88.28%] [G loss: 2.750817]\n",
      "epoch:28 step:21912 [D loss: 0.334165, acc.: 85.16%] [G loss: 3.402762]\n",
      "epoch:28 step:21913 [D loss: 0.288008, acc.: 92.19%] [G loss: 3.641726]\n",
      "epoch:28 step:21914 [D loss: 0.345678, acc.: 82.03%] [G loss: 2.687212]\n",
      "epoch:28 step:21915 [D loss: 0.320720, acc.: 84.38%] [G loss: 3.264360]\n",
      "epoch:28 step:21916 [D loss: 0.235706, acc.: 89.84%] [G loss: 3.142948]\n",
      "epoch:28 step:21917 [D loss: 0.280369, acc.: 88.28%] [G loss: 3.442527]\n",
      "epoch:28 step:21918 [D loss: 0.375585, acc.: 80.47%] [G loss: 3.596605]\n",
      "epoch:28 step:21919 [D loss: 0.238482, acc.: 88.28%] [G loss: 3.357608]\n",
      "epoch:28 step:21920 [D loss: 0.250600, acc.: 91.41%] [G loss: 3.316554]\n",
      "epoch:28 step:21921 [D loss: 0.249287, acc.: 89.84%] [G loss: 2.814991]\n",
      "epoch:28 step:21922 [D loss: 0.286218, acc.: 85.16%] [G loss: 3.839059]\n",
      "epoch:28 step:21923 [D loss: 0.289787, acc.: 86.72%] [G loss: 4.767451]\n",
      "epoch:28 step:21924 [D loss: 0.296397, acc.: 89.06%] [G loss: 3.748964]\n",
      "epoch:28 step:21925 [D loss: 0.359980, acc.: 78.91%] [G loss: 3.098190]\n",
      "epoch:28 step:21926 [D loss: 0.292845, acc.: 87.50%] [G loss: 4.020450]\n",
      "epoch:28 step:21927 [D loss: 0.408914, acc.: 79.69%] [G loss: 2.924026]\n",
      "epoch:28 step:21928 [D loss: 0.274279, acc.: 90.62%] [G loss: 3.028440]\n",
      "epoch:28 step:21929 [D loss: 0.359148, acc.: 85.94%] [G loss: 3.398499]\n",
      "epoch:28 step:21930 [D loss: 0.345145, acc.: 85.94%] [G loss: 2.717382]\n",
      "epoch:28 step:21931 [D loss: 0.341807, acc.: 82.03%] [G loss: 4.086663]\n",
      "epoch:28 step:21932 [D loss: 0.399393, acc.: 83.59%] [G loss: 3.969916]\n",
      "epoch:28 step:21933 [D loss: 0.424562, acc.: 80.47%] [G loss: 3.148801]\n",
      "epoch:28 step:21934 [D loss: 0.369591, acc.: 84.38%] [G loss: 2.462849]\n",
      "epoch:28 step:21935 [D loss: 0.369887, acc.: 82.81%] [G loss: 3.482241]\n",
      "epoch:28 step:21936 [D loss: 0.271533, acc.: 86.72%] [G loss: 4.658964]\n",
      "epoch:28 step:21937 [D loss: 0.239344, acc.: 90.62%] [G loss: 7.681845]\n",
      "epoch:28 step:21938 [D loss: 0.249273, acc.: 87.50%] [G loss: 5.852702]\n",
      "epoch:28 step:21939 [D loss: 0.217465, acc.: 90.62%] [G loss: 5.121431]\n",
      "epoch:28 step:21940 [D loss: 0.158807, acc.: 91.41%] [G loss: 5.424481]\n",
      "epoch:28 step:21941 [D loss: 0.287766, acc.: 87.50%] [G loss: 5.124008]\n",
      "epoch:28 step:21942 [D loss: 0.328808, acc.: 92.97%] [G loss: 4.809004]\n",
      "epoch:28 step:21943 [D loss: 0.312843, acc.: 86.72%] [G loss: 3.885230]\n",
      "epoch:28 step:21944 [D loss: 0.281484, acc.: 85.16%] [G loss: 2.981747]\n",
      "epoch:28 step:21945 [D loss: 0.313676, acc.: 86.72%] [G loss: 3.128288]\n",
      "epoch:28 step:21946 [D loss: 0.283464, acc.: 84.38%] [G loss: 3.485562]\n",
      "epoch:28 step:21947 [D loss: 0.361188, acc.: 83.59%] [G loss: 2.928572]\n",
      "epoch:28 step:21948 [D loss: 0.243696, acc.: 87.50%] [G loss: 3.179351]\n",
      "epoch:28 step:21949 [D loss: 0.371034, acc.: 83.59%] [G loss: 3.035321]\n",
      "epoch:28 step:21950 [D loss: 0.307312, acc.: 84.38%] [G loss: 4.178946]\n",
      "epoch:28 step:21951 [D loss: 0.219899, acc.: 89.84%] [G loss: 4.888510]\n",
      "epoch:28 step:21952 [D loss: 0.295749, acc.: 83.59%] [G loss: 2.778216]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:28 step:21953 [D loss: 0.227358, acc.: 89.84%] [G loss: 3.824450]\n",
      "epoch:28 step:21954 [D loss: 0.267871, acc.: 88.28%] [G loss: 2.931678]\n",
      "epoch:28 step:21955 [D loss: 0.294827, acc.: 84.38%] [G loss: 3.947131]\n",
      "epoch:28 step:21956 [D loss: 0.325781, acc.: 85.16%] [G loss: 2.443971]\n",
      "epoch:28 step:21957 [D loss: 0.379718, acc.: 81.25%] [G loss: 2.648487]\n",
      "epoch:28 step:21958 [D loss: 0.328536, acc.: 84.38%] [G loss: 3.053303]\n",
      "epoch:28 step:21959 [D loss: 0.283333, acc.: 86.72%] [G loss: 3.096814]\n",
      "epoch:28 step:21960 [D loss: 0.443148, acc.: 76.56%] [G loss: 2.980622]\n",
      "epoch:28 step:21961 [D loss: 0.287298, acc.: 85.94%] [G loss: 3.010046]\n",
      "epoch:28 step:21962 [D loss: 0.256124, acc.: 89.84%] [G loss: 3.021555]\n",
      "epoch:28 step:21963 [D loss: 0.379255, acc.: 83.59%] [G loss: 4.114914]\n",
      "epoch:28 step:21964 [D loss: 0.441799, acc.: 79.69%] [G loss: 5.731638]\n",
      "epoch:28 step:21965 [D loss: 0.394143, acc.: 82.81%] [G loss: 3.686607]\n",
      "epoch:28 step:21966 [D loss: 0.253617, acc.: 90.62%] [G loss: 3.510984]\n",
      "epoch:28 step:21967 [D loss: 0.368962, acc.: 82.81%] [G loss: 3.298600]\n",
      "epoch:28 step:21968 [D loss: 0.275057, acc.: 88.28%] [G loss: 4.081089]\n",
      "epoch:28 step:21969 [D loss: 0.293091, acc.: 82.81%] [G loss: 4.628553]\n",
      "epoch:28 step:21970 [D loss: 0.280504, acc.: 85.94%] [G loss: 4.013690]\n",
      "epoch:28 step:21971 [D loss: 0.294208, acc.: 87.50%] [G loss: 3.926041]\n",
      "epoch:28 step:21972 [D loss: 0.272302, acc.: 87.50%] [G loss: 3.163869]\n",
      "epoch:28 step:21973 [D loss: 0.275097, acc.: 89.06%] [G loss: 3.090255]\n",
      "epoch:28 step:21974 [D loss: 0.260540, acc.: 88.28%] [G loss: 2.711874]\n",
      "epoch:28 step:21975 [D loss: 0.257271, acc.: 89.06%] [G loss: 4.704020]\n",
      "epoch:28 step:21976 [D loss: 0.207072, acc.: 93.75%] [G loss: 4.208825]\n",
      "epoch:28 step:21977 [D loss: 0.288123, acc.: 85.94%] [G loss: 4.979286]\n",
      "epoch:28 step:21978 [D loss: 0.468124, acc.: 78.12%] [G loss: 2.849390]\n",
      "epoch:28 step:21979 [D loss: 0.352087, acc.: 83.59%] [G loss: 3.796482]\n",
      "epoch:28 step:21980 [D loss: 0.429686, acc.: 85.94%] [G loss: 3.853581]\n",
      "epoch:28 step:21981 [D loss: 0.578022, acc.: 74.22%] [G loss: 3.790221]\n",
      "epoch:28 step:21982 [D loss: 0.394634, acc.: 83.59%] [G loss: 3.722960]\n",
      "epoch:28 step:21983 [D loss: 0.387545, acc.: 82.81%] [G loss: 4.306449]\n",
      "epoch:28 step:21984 [D loss: 0.317491, acc.: 85.94%] [G loss: 4.341539]\n",
      "epoch:28 step:21985 [D loss: 0.331459, acc.: 85.16%] [G loss: 5.994655]\n",
      "epoch:28 step:21986 [D loss: 0.206942, acc.: 92.19%] [G loss: 3.964014]\n",
      "epoch:28 step:21987 [D loss: 0.270752, acc.: 88.28%] [G loss: 3.032142]\n",
      "epoch:28 step:21988 [D loss: 0.288978, acc.: 84.38%] [G loss: 3.277849]\n",
      "epoch:28 step:21989 [D loss: 0.284899, acc.: 89.84%] [G loss: 2.804988]\n",
      "epoch:28 step:21990 [D loss: 0.321370, acc.: 87.50%] [G loss: 2.530451]\n",
      "epoch:28 step:21991 [D loss: 0.243422, acc.: 83.59%] [G loss: 3.497528]\n",
      "epoch:28 step:21992 [D loss: 0.258836, acc.: 89.06%] [G loss: 3.397684]\n",
      "epoch:28 step:21993 [D loss: 0.353823, acc.: 82.81%] [G loss: 3.884819]\n",
      "epoch:28 step:21994 [D loss: 0.368614, acc.: 84.38%] [G loss: 3.697618]\n",
      "epoch:28 step:21995 [D loss: 0.302742, acc.: 85.16%] [G loss: 3.418617]\n",
      "epoch:28 step:21996 [D loss: 0.347106, acc.: 86.72%] [G loss: 3.996927]\n",
      "epoch:28 step:21997 [D loss: 0.320815, acc.: 86.72%] [G loss: 3.840929]\n",
      "epoch:28 step:21998 [D loss: 0.447334, acc.: 82.81%] [G loss: 2.793342]\n",
      "epoch:28 step:21999 [D loss: 0.339871, acc.: 85.16%] [G loss: 3.796152]\n",
      "epoch:28 step:22000 [D loss: 0.286998, acc.: 87.50%] [G loss: 5.450816]\n",
      "##############\n",
      "[0.87044054 0.83886503 0.79912361 0.81085437 0.77445666 0.8140891\n",
      " 0.87265213 0.82297782 0.79482285 0.82411686]\n",
      "##########\n",
      "epoch:28 step:22001 [D loss: 0.397954, acc.: 82.03%] [G loss: 8.231306]\n",
      "epoch:28 step:22002 [D loss: 0.568292, acc.: 81.25%] [G loss: 5.009411]\n",
      "epoch:28 step:22003 [D loss: 0.293904, acc.: 88.28%] [G loss: 6.077437]\n",
      "epoch:28 step:22004 [D loss: 0.209373, acc.: 91.41%] [G loss: 5.282207]\n",
      "epoch:28 step:22005 [D loss: 0.275258, acc.: 86.72%] [G loss: 5.116554]\n",
      "epoch:28 step:22006 [D loss: 0.243771, acc.: 89.06%] [G loss: 3.644036]\n",
      "epoch:28 step:22007 [D loss: 0.243642, acc.: 90.62%] [G loss: 3.760515]\n",
      "epoch:28 step:22008 [D loss: 0.254011, acc.: 87.50%] [G loss: 4.080297]\n",
      "epoch:28 step:22009 [D loss: 0.366632, acc.: 82.81%] [G loss: 3.164026]\n",
      "epoch:28 step:22010 [D loss: 0.590613, acc.: 68.75%] [G loss: 3.686170]\n",
      "epoch:28 step:22011 [D loss: 0.390652, acc.: 85.94%] [G loss: 4.462545]\n",
      "epoch:28 step:22012 [D loss: 0.299335, acc.: 86.72%] [G loss: 2.470361]\n",
      "epoch:28 step:22013 [D loss: 0.232545, acc.: 85.94%] [G loss: 5.658376]\n",
      "epoch:28 step:22014 [D loss: 0.365759, acc.: 78.91%] [G loss: 4.182673]\n",
      "epoch:28 step:22015 [D loss: 0.331773, acc.: 85.16%] [G loss: 2.798469]\n",
      "epoch:28 step:22016 [D loss: 0.275948, acc.: 86.72%] [G loss: 3.494982]\n",
      "epoch:28 step:22017 [D loss: 0.295855, acc.: 88.28%] [G loss: 3.678936]\n",
      "epoch:28 step:22018 [D loss: 0.305529, acc.: 86.72%] [G loss: 3.145996]\n",
      "epoch:28 step:22019 [D loss: 0.339599, acc.: 87.50%] [G loss: 2.648566]\n",
      "epoch:28 step:22020 [D loss: 0.358899, acc.: 84.38%] [G loss: 3.275820]\n",
      "epoch:28 step:22021 [D loss: 0.291852, acc.: 86.72%] [G loss: 2.897232]\n",
      "epoch:28 step:22022 [D loss: 0.268651, acc.: 89.84%] [G loss: 3.879846]\n",
      "epoch:28 step:22023 [D loss: 0.241193, acc.: 91.41%] [G loss: 3.585061]\n",
      "epoch:28 step:22024 [D loss: 0.371301, acc.: 82.81%] [G loss: 3.678330]\n",
      "epoch:28 step:22025 [D loss: 0.300113, acc.: 85.94%] [G loss: 2.912017]\n",
      "epoch:28 step:22026 [D loss: 0.310647, acc.: 83.59%] [G loss: 3.238289]\n",
      "epoch:28 step:22027 [D loss: 0.286910, acc.: 89.06%] [G loss: 3.245536]\n",
      "epoch:28 step:22028 [D loss: 0.309701, acc.: 86.72%] [G loss: 2.911951]\n",
      "epoch:28 step:22029 [D loss: 0.394855, acc.: 81.25%] [G loss: 3.025510]\n",
      "epoch:28 step:22030 [D loss: 0.316911, acc.: 85.94%] [G loss: 3.352523]\n",
      "epoch:28 step:22031 [D loss: 0.259541, acc.: 89.84%] [G loss: 2.856830]\n",
      "epoch:28 step:22032 [D loss: 0.295774, acc.: 85.94%] [G loss: 2.487873]\n",
      "epoch:28 step:22033 [D loss: 0.297084, acc.: 85.94%] [G loss: 3.439640]\n",
      "epoch:28 step:22034 [D loss: 0.254925, acc.: 89.84%] [G loss: 4.121234]\n",
      "epoch:28 step:22035 [D loss: 0.304716, acc.: 87.50%] [G loss: 2.501805]\n",
      "epoch:28 step:22036 [D loss: 0.411930, acc.: 82.03%] [G loss: 2.791113]\n",
      "epoch:28 step:22037 [D loss: 0.352275, acc.: 85.16%] [G loss: 2.955029]\n",
      "epoch:28 step:22038 [D loss: 0.156947, acc.: 92.97%] [G loss: 3.751188]\n",
      "epoch:28 step:22039 [D loss: 0.427554, acc.: 82.03%] [G loss: 3.066565]\n",
      "epoch:28 step:22040 [D loss: 0.361990, acc.: 77.34%] [G loss: 3.242724]\n",
      "epoch:28 step:22041 [D loss: 0.348653, acc.: 83.59%] [G loss: 3.594396]\n",
      "epoch:28 step:22042 [D loss: 0.292275, acc.: 88.28%] [G loss: 3.063779]\n",
      "epoch:28 step:22043 [D loss: 0.296382, acc.: 88.28%] [G loss: 3.076158]\n",
      "epoch:28 step:22044 [D loss: 0.258747, acc.: 89.06%] [G loss: 3.902479]\n",
      "epoch:28 step:22045 [D loss: 0.296513, acc.: 86.72%] [G loss: 4.394798]\n",
      "epoch:28 step:22046 [D loss: 0.331743, acc.: 83.59%] [G loss: 3.130553]\n",
      "epoch:28 step:22047 [D loss: 0.265604, acc.: 88.28%] [G loss: 3.312693]\n",
      "epoch:28 step:22048 [D loss: 0.335228, acc.: 85.94%] [G loss: 3.205178]\n",
      "epoch:28 step:22049 [D loss: 0.361275, acc.: 82.81%] [G loss: 2.629635]\n",
      "epoch:28 step:22050 [D loss: 0.343708, acc.: 89.84%] [G loss: 2.798778]\n",
      "epoch:28 step:22051 [D loss: 0.219795, acc.: 93.75%] [G loss: 4.061646]\n",
      "epoch:28 step:22052 [D loss: 0.389938, acc.: 81.25%] [G loss: 2.680323]\n",
      "epoch:28 step:22053 [D loss: 0.253196, acc.: 89.84%] [G loss: 2.621077]\n",
      "epoch:28 step:22054 [D loss: 0.287241, acc.: 87.50%] [G loss: 4.969147]\n",
      "epoch:28 step:22055 [D loss: 0.289170, acc.: 87.50%] [G loss: 3.318511]\n",
      "epoch:28 step:22056 [D loss: 0.265526, acc.: 91.41%] [G loss: 4.352841]\n",
      "epoch:28 step:22057 [D loss: 0.278527, acc.: 86.72%] [G loss: 4.380263]\n",
      "epoch:28 step:22058 [D loss: 0.312863, acc.: 82.81%] [G loss: 2.821794]\n",
      "epoch:28 step:22059 [D loss: 0.370250, acc.: 81.25%] [G loss: 5.610356]\n",
      "epoch:28 step:22060 [D loss: 0.303212, acc.: 88.28%] [G loss: 4.805461]\n",
      "epoch:28 step:22061 [D loss: 0.285184, acc.: 86.72%] [G loss: 5.239550]\n",
      "epoch:28 step:22062 [D loss: 0.257389, acc.: 89.84%] [G loss: 3.392506]\n",
      "epoch:28 step:22063 [D loss: 0.330783, acc.: 83.59%] [G loss: 3.024417]\n",
      "epoch:28 step:22064 [D loss: 0.227344, acc.: 91.41%] [G loss: 4.241978]\n",
      "epoch:28 step:22065 [D loss: 0.318024, acc.: 85.16%] [G loss: 2.807575]\n",
      "epoch:28 step:22066 [D loss: 0.279931, acc.: 85.94%] [G loss: 3.611041]\n",
      "epoch:28 step:22067 [D loss: 0.249268, acc.: 91.41%] [G loss: 3.968660]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:28 step:22068 [D loss: 0.235137, acc.: 89.06%] [G loss: 3.137449]\n",
      "epoch:28 step:22069 [D loss: 0.196511, acc.: 93.75%] [G loss: 3.541881]\n",
      "epoch:28 step:22070 [D loss: 0.343295, acc.: 82.03%] [G loss: 2.694793]\n",
      "epoch:28 step:22071 [D loss: 0.225579, acc.: 91.41%] [G loss: 3.231082]\n",
      "epoch:28 step:22072 [D loss: 0.277947, acc.: 87.50%] [G loss: 4.018251]\n",
      "epoch:28 step:22073 [D loss: 0.394410, acc.: 84.38%] [G loss: 3.327496]\n",
      "epoch:28 step:22074 [D loss: 0.463262, acc.: 76.56%] [G loss: 2.557555]\n",
      "epoch:28 step:22075 [D loss: 0.303675, acc.: 85.16%] [G loss: 2.939619]\n",
      "epoch:28 step:22076 [D loss: 0.367798, acc.: 85.94%] [G loss: 3.197166]\n",
      "epoch:28 step:22077 [D loss: 0.304575, acc.: 88.28%] [G loss: 2.883402]\n",
      "epoch:28 step:22078 [D loss: 0.272274, acc.: 88.28%] [G loss: 3.251427]\n",
      "epoch:28 step:22079 [D loss: 0.396153, acc.: 83.59%] [G loss: 3.191291]\n",
      "epoch:28 step:22080 [D loss: 0.426924, acc.: 78.12%] [G loss: 5.073510]\n",
      "epoch:28 step:22081 [D loss: 0.505763, acc.: 79.69%] [G loss: 6.912849]\n",
      "epoch:28 step:22082 [D loss: 0.740336, acc.: 75.78%] [G loss: 4.639655]\n",
      "epoch:28 step:22083 [D loss: 0.272247, acc.: 86.72%] [G loss: 4.275327]\n",
      "epoch:28 step:22084 [D loss: 0.518950, acc.: 74.22%] [G loss: 4.209347]\n",
      "epoch:28 step:22085 [D loss: 0.316733, acc.: 89.06%] [G loss: 5.031103]\n",
      "epoch:28 step:22086 [D loss: 0.532848, acc.: 75.78%] [G loss: 3.944077]\n",
      "epoch:28 step:22087 [D loss: 0.436221, acc.: 83.59%] [G loss: 2.703190]\n",
      "epoch:28 step:22088 [D loss: 0.298180, acc.: 86.72%] [G loss: 3.594097]\n",
      "epoch:28 step:22089 [D loss: 0.255741, acc.: 91.41%] [G loss: 3.834927]\n",
      "epoch:28 step:22090 [D loss: 0.279595, acc.: 88.28%] [G loss: 4.706684]\n",
      "epoch:28 step:22091 [D loss: 0.264423, acc.: 89.06%] [G loss: 3.092889]\n",
      "epoch:28 step:22092 [D loss: 0.348783, acc.: 82.81%] [G loss: 2.839095]\n",
      "epoch:28 step:22093 [D loss: 0.343359, acc.: 84.38%] [G loss: 2.261165]\n",
      "epoch:28 step:22094 [D loss: 0.358372, acc.: 87.50%] [G loss: 4.243885]\n",
      "epoch:28 step:22095 [D loss: 0.233513, acc.: 90.62%] [G loss: 2.509194]\n",
      "epoch:28 step:22096 [D loss: 0.345714, acc.: 82.03%] [G loss: 4.773058]\n",
      "epoch:28 step:22097 [D loss: 0.296360, acc.: 82.03%] [G loss: 5.269582]\n",
      "epoch:28 step:22098 [D loss: 0.299083, acc.: 85.94%] [G loss: 6.488569]\n",
      "epoch:28 step:22099 [D loss: 0.317846, acc.: 84.38%] [G loss: 4.564411]\n",
      "epoch:28 step:22100 [D loss: 0.252730, acc.: 89.06%] [G loss: 5.299350]\n",
      "epoch:28 step:22101 [D loss: 0.246884, acc.: 89.84%] [G loss: 4.177836]\n",
      "epoch:28 step:22102 [D loss: 0.281193, acc.: 87.50%] [G loss: 3.949010]\n",
      "epoch:28 step:22103 [D loss: 0.321722, acc.: 89.06%] [G loss: 4.639935]\n",
      "epoch:28 step:22104 [D loss: 0.350960, acc.: 86.72%] [G loss: 4.348487]\n",
      "epoch:28 step:22105 [D loss: 0.281183, acc.: 85.94%] [G loss: 4.063947]\n",
      "epoch:28 step:22106 [D loss: 0.266396, acc.: 89.84%] [G loss: 4.435791]\n",
      "epoch:28 step:22107 [D loss: 0.226979, acc.: 89.84%] [G loss: 4.161777]\n",
      "epoch:28 step:22108 [D loss: 0.327627, acc.: 83.59%] [G loss: 3.316516]\n",
      "epoch:28 step:22109 [D loss: 0.279493, acc.: 89.06%] [G loss: 3.264749]\n",
      "epoch:28 step:22110 [D loss: 0.261114, acc.: 87.50%] [G loss: 4.284628]\n",
      "epoch:28 step:22111 [D loss: 0.325507, acc.: 85.16%] [G loss: 2.344573]\n",
      "epoch:28 step:22112 [D loss: 0.367579, acc.: 80.47%] [G loss: 2.687416]\n",
      "epoch:28 step:22113 [D loss: 0.227485, acc.: 92.19%] [G loss: 3.473369]\n",
      "epoch:28 step:22114 [D loss: 0.282250, acc.: 89.06%] [G loss: 3.934633]\n",
      "epoch:28 step:22115 [D loss: 0.418159, acc.: 81.25%] [G loss: 3.269957]\n",
      "epoch:28 step:22116 [D loss: 0.360739, acc.: 81.25%] [G loss: 5.152555]\n",
      "epoch:28 step:22117 [D loss: 0.452342, acc.: 78.12%] [G loss: 4.306521]\n",
      "epoch:28 step:22118 [D loss: 0.491594, acc.: 76.56%] [G loss: 4.233572]\n",
      "epoch:28 step:22119 [D loss: 0.277354, acc.: 85.94%] [G loss: 4.690290]\n",
      "epoch:28 step:22120 [D loss: 0.304860, acc.: 88.28%] [G loss: 5.406950]\n",
      "epoch:28 step:22121 [D loss: 0.315778, acc.: 84.38%] [G loss: 3.331290]\n",
      "epoch:28 step:22122 [D loss: 0.259555, acc.: 90.62%] [G loss: 3.739883]\n",
      "epoch:28 step:22123 [D loss: 0.317830, acc.: 84.38%] [G loss: 3.730085]\n",
      "epoch:28 step:22124 [D loss: 0.308150, acc.: 84.38%] [G loss: 4.549847]\n",
      "epoch:28 step:22125 [D loss: 0.256564, acc.: 91.41%] [G loss: 3.968824]\n",
      "epoch:28 step:22126 [D loss: 0.266808, acc.: 85.16%] [G loss: 4.338048]\n",
      "epoch:28 step:22127 [D loss: 0.244364, acc.: 89.84%] [G loss: 4.117910]\n",
      "epoch:28 step:22128 [D loss: 0.285506, acc.: 87.50%] [G loss: 4.235802]\n",
      "epoch:28 step:22129 [D loss: 0.477457, acc.: 79.69%] [G loss: 3.420352]\n",
      "epoch:28 step:22130 [D loss: 0.245489, acc.: 89.06%] [G loss: 2.540903]\n",
      "epoch:28 step:22131 [D loss: 0.262356, acc.: 89.06%] [G loss: 4.140688]\n",
      "epoch:28 step:22132 [D loss: 0.276607, acc.: 88.28%] [G loss: 4.292623]\n",
      "epoch:28 step:22133 [D loss: 0.342381, acc.: 82.81%] [G loss: 4.993377]\n",
      "epoch:28 step:22134 [D loss: 0.466485, acc.: 75.78%] [G loss: 3.306561]\n",
      "epoch:28 step:22135 [D loss: 0.290240, acc.: 86.72%] [G loss: 3.585173]\n",
      "epoch:28 step:22136 [D loss: 0.354301, acc.: 84.38%] [G loss: 3.986439]\n",
      "epoch:28 step:22137 [D loss: 0.281457, acc.: 87.50%] [G loss: 2.775453]\n",
      "epoch:28 step:22138 [D loss: 0.247147, acc.: 87.50%] [G loss: 3.899676]\n",
      "epoch:28 step:22139 [D loss: 0.203783, acc.: 92.19%] [G loss: 3.482209]\n",
      "epoch:28 step:22140 [D loss: 0.309851, acc.: 85.16%] [G loss: 3.559676]\n",
      "epoch:28 step:22141 [D loss: 0.272120, acc.: 89.06%] [G loss: 4.181573]\n",
      "epoch:28 step:22142 [D loss: 0.240038, acc.: 88.28%] [G loss: 2.977075]\n",
      "epoch:28 step:22143 [D loss: 0.413798, acc.: 80.47%] [G loss: 3.705751]\n",
      "epoch:28 step:22144 [D loss: 0.393512, acc.: 79.69%] [G loss: 3.580844]\n",
      "epoch:28 step:22145 [D loss: 0.335420, acc.: 91.41%] [G loss: 3.280619]\n",
      "epoch:28 step:22146 [D loss: 0.296019, acc.: 85.94%] [G loss: 3.063351]\n",
      "epoch:28 step:22147 [D loss: 0.416409, acc.: 80.47%] [G loss: 3.274662]\n",
      "epoch:28 step:22148 [D loss: 0.318810, acc.: 86.72%] [G loss: 3.247773]\n",
      "epoch:28 step:22149 [D loss: 0.512417, acc.: 78.91%] [G loss: 7.390544]\n",
      "epoch:28 step:22150 [D loss: 0.303765, acc.: 89.06%] [G loss: 4.401260]\n",
      "epoch:28 step:22151 [D loss: 0.371564, acc.: 82.03%] [G loss: 7.000203]\n",
      "epoch:28 step:22152 [D loss: 0.350439, acc.: 82.81%] [G loss: 3.430790]\n",
      "epoch:28 step:22153 [D loss: 0.278113, acc.: 84.38%] [G loss: 5.343915]\n",
      "epoch:28 step:22154 [D loss: 0.347290, acc.: 88.28%] [G loss: 3.861012]\n",
      "epoch:28 step:22155 [D loss: 0.299299, acc.: 86.72%] [G loss: 3.498916]\n",
      "epoch:28 step:22156 [D loss: 0.280565, acc.: 85.94%] [G loss: 3.494638]\n",
      "epoch:28 step:22157 [D loss: 0.216536, acc.: 89.84%] [G loss: 4.495989]\n",
      "epoch:28 step:22158 [D loss: 0.363931, acc.: 84.38%] [G loss: 4.700716]\n",
      "epoch:28 step:22159 [D loss: 0.443095, acc.: 82.81%] [G loss: 4.187211]\n",
      "epoch:28 step:22160 [D loss: 0.232855, acc.: 89.84%] [G loss: 4.262983]\n",
      "epoch:28 step:22161 [D loss: 0.350901, acc.: 82.03%] [G loss: 3.096317]\n",
      "epoch:28 step:22162 [D loss: 0.257742, acc.: 87.50%] [G loss: 4.008868]\n",
      "epoch:28 step:22163 [D loss: 0.372748, acc.: 78.91%] [G loss: 3.327624]\n",
      "epoch:28 step:22164 [D loss: 0.302290, acc.: 85.16%] [G loss: 2.981990]\n",
      "epoch:28 step:22165 [D loss: 0.280271, acc.: 86.72%] [G loss: 3.846480]\n",
      "epoch:28 step:22166 [D loss: 0.250219, acc.: 91.41%] [G loss: 2.657353]\n",
      "epoch:28 step:22167 [D loss: 0.262790, acc.: 91.41%] [G loss: 3.306385]\n",
      "epoch:28 step:22168 [D loss: 0.340338, acc.: 81.25%] [G loss: 2.638000]\n",
      "epoch:28 step:22169 [D loss: 0.271068, acc.: 88.28%] [G loss: 3.121568]\n",
      "epoch:28 step:22170 [D loss: 0.357031, acc.: 87.50%] [G loss: 3.809860]\n",
      "epoch:28 step:22171 [D loss: 0.365955, acc.: 84.38%] [G loss: 5.346836]\n",
      "epoch:28 step:22172 [D loss: 0.255171, acc.: 89.06%] [G loss: 4.390905]\n",
      "epoch:28 step:22173 [D loss: 0.402509, acc.: 84.38%] [G loss: 6.087409]\n",
      "epoch:28 step:22174 [D loss: 0.575262, acc.: 75.78%] [G loss: 6.225178]\n",
      "epoch:28 step:22175 [D loss: 0.300189, acc.: 85.94%] [G loss: 4.469451]\n",
      "epoch:28 step:22176 [D loss: 0.234420, acc.: 89.06%] [G loss: 4.443666]\n",
      "epoch:28 step:22177 [D loss: 0.298649, acc.: 85.16%] [G loss: 3.807405]\n",
      "epoch:28 step:22178 [D loss: 0.245489, acc.: 89.06%] [G loss: 3.535767]\n",
      "epoch:28 step:22179 [D loss: 0.369106, acc.: 84.38%] [G loss: 3.900676]\n",
      "epoch:28 step:22180 [D loss: 0.292074, acc.: 86.72%] [G loss: 4.785865]\n",
      "epoch:28 step:22181 [D loss: 0.333595, acc.: 85.16%] [G loss: 3.595842]\n",
      "epoch:28 step:22182 [D loss: 0.412603, acc.: 80.47%] [G loss: 3.571270]\n",
      "epoch:28 step:22183 [D loss: 0.342084, acc.: 85.16%] [G loss: 3.327521]\n",
      "epoch:28 step:22184 [D loss: 0.365287, acc.: 83.59%] [G loss: 2.984906]\n",
      "epoch:28 step:22185 [D loss: 0.343201, acc.: 84.38%] [G loss: 5.448361]\n",
      "epoch:28 step:22186 [D loss: 0.360069, acc.: 81.25%] [G loss: 4.929358]\n",
      "epoch:28 step:22187 [D loss: 0.150130, acc.: 92.97%] [G loss: 6.711402]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:28 step:22188 [D loss: 0.325040, acc.: 84.38%] [G loss: 5.715019]\n",
      "epoch:28 step:22189 [D loss: 0.248866, acc.: 92.19%] [G loss: 3.691116]\n",
      "epoch:28 step:22190 [D loss: 0.322379, acc.: 86.72%] [G loss: 4.331201]\n",
      "epoch:28 step:22191 [D loss: 0.254855, acc.: 89.06%] [G loss: 3.768749]\n",
      "epoch:28 step:22192 [D loss: 0.243648, acc.: 87.50%] [G loss: 3.313157]\n",
      "epoch:28 step:22193 [D loss: 0.367246, acc.: 81.25%] [G loss: 5.380027]\n",
      "epoch:28 step:22194 [D loss: 0.273846, acc.: 88.28%] [G loss: 3.914742]\n",
      "epoch:28 step:22195 [D loss: 0.304453, acc.: 84.38%] [G loss: 3.207464]\n",
      "epoch:28 step:22196 [D loss: 0.199027, acc.: 93.75%] [G loss: 3.011661]\n",
      "epoch:28 step:22197 [D loss: 0.371953, acc.: 82.03%] [G loss: 2.697923]\n",
      "epoch:28 step:22198 [D loss: 0.317179, acc.: 84.38%] [G loss: 2.825616]\n",
      "epoch:28 step:22199 [D loss: 0.320433, acc.: 85.16%] [G loss: 3.008130]\n",
      "epoch:28 step:22200 [D loss: 0.278204, acc.: 90.62%] [G loss: 3.893402]\n",
      "##############\n",
      "[0.88433228 0.85785434 0.81205857 0.78905914 0.78351135 0.81821489\n",
      " 0.85858306 0.84164584 0.8374348  0.79176125]\n",
      "##########\n",
      "epoch:28 step:22201 [D loss: 0.302098, acc.: 85.94%] [G loss: 2.807546]\n",
      "epoch:28 step:22202 [D loss: 0.353383, acc.: 83.59%] [G loss: 2.663412]\n",
      "epoch:28 step:22203 [D loss: 0.247793, acc.: 90.62%] [G loss: 3.400958]\n",
      "epoch:28 step:22204 [D loss: 0.367345, acc.: 85.16%] [G loss: 2.884952]\n",
      "epoch:28 step:22205 [D loss: 0.368989, acc.: 85.94%] [G loss: 3.302980]\n",
      "epoch:28 step:22206 [D loss: 0.394135, acc.: 79.69%] [G loss: 4.119938]\n",
      "epoch:28 step:22207 [D loss: 0.327611, acc.: 85.94%] [G loss: 4.806198]\n",
      "epoch:28 step:22208 [D loss: 0.384178, acc.: 79.69%] [G loss: 6.511477]\n",
      "epoch:28 step:22209 [D loss: 0.846053, acc.: 75.78%] [G loss: 4.809515]\n",
      "epoch:28 step:22210 [D loss: 0.350424, acc.: 83.59%] [G loss: 3.997398]\n",
      "epoch:28 step:22211 [D loss: 0.500231, acc.: 82.81%] [G loss: 5.172819]\n",
      "epoch:28 step:22212 [D loss: 0.506561, acc.: 75.00%] [G loss: 5.254525]\n",
      "epoch:28 step:22213 [D loss: 0.288238, acc.: 84.38%] [G loss: 5.557926]\n",
      "epoch:28 step:22214 [D loss: 0.275389, acc.: 85.94%] [G loss: 3.679005]\n",
      "epoch:28 step:22215 [D loss: 0.354901, acc.: 84.38%] [G loss: 4.622082]\n",
      "epoch:28 step:22216 [D loss: 0.325584, acc.: 84.38%] [G loss: 4.161557]\n",
      "epoch:28 step:22217 [D loss: 0.298683, acc.: 85.94%] [G loss: 3.452919]\n",
      "epoch:28 step:22218 [D loss: 0.321394, acc.: 82.81%] [G loss: 4.614802]\n",
      "epoch:28 step:22219 [D loss: 0.528210, acc.: 78.12%] [G loss: 3.984591]\n",
      "epoch:28 step:22220 [D loss: 0.367115, acc.: 79.69%] [G loss: 4.406911]\n",
      "epoch:28 step:22221 [D loss: 0.238641, acc.: 89.06%] [G loss: 3.670160]\n",
      "epoch:28 step:22222 [D loss: 0.343340, acc.: 85.94%] [G loss: 3.366486]\n",
      "epoch:28 step:22223 [D loss: 0.185465, acc.: 92.19%] [G loss: 5.137717]\n",
      "epoch:28 step:22224 [D loss: 0.346987, acc.: 79.69%] [G loss: 4.285630]\n",
      "epoch:28 step:22225 [D loss: 0.301209, acc.: 85.94%] [G loss: 3.290697]\n",
      "epoch:28 step:22226 [D loss: 0.316503, acc.: 86.72%] [G loss: 3.444273]\n",
      "epoch:28 step:22227 [D loss: 0.295273, acc.: 84.38%] [G loss: 2.901331]\n",
      "epoch:28 step:22228 [D loss: 0.322967, acc.: 85.16%] [G loss: 2.675910]\n",
      "epoch:28 step:22229 [D loss: 0.246490, acc.: 85.94%] [G loss: 2.410765]\n",
      "epoch:28 step:22230 [D loss: 0.338182, acc.: 86.72%] [G loss: 2.833165]\n",
      "epoch:28 step:22231 [D loss: 0.377916, acc.: 81.25%] [G loss: 3.831003]\n",
      "epoch:28 step:22232 [D loss: 0.306041, acc.: 84.38%] [G loss: 3.470994]\n",
      "epoch:28 step:22233 [D loss: 0.401711, acc.: 82.03%] [G loss: 2.862609]\n",
      "epoch:28 step:22234 [D loss: 0.389626, acc.: 82.03%] [G loss: 2.724920]\n",
      "epoch:28 step:22235 [D loss: 0.375997, acc.: 83.59%] [G loss: 3.075248]\n",
      "epoch:28 step:22236 [D loss: 0.318940, acc.: 84.38%] [G loss: 3.127663]\n",
      "epoch:28 step:22237 [D loss: 0.292011, acc.: 89.06%] [G loss: 3.497051]\n",
      "epoch:28 step:22238 [D loss: 0.318960, acc.: 85.16%] [G loss: 3.053646]\n",
      "epoch:28 step:22239 [D loss: 0.244210, acc.: 91.41%] [G loss: 2.448831]\n",
      "epoch:28 step:22240 [D loss: 0.282719, acc.: 89.84%] [G loss: 2.959445]\n",
      "epoch:28 step:22241 [D loss: 0.278033, acc.: 89.84%] [G loss: 2.746577]\n",
      "epoch:28 step:22242 [D loss: 0.321545, acc.: 86.72%] [G loss: 2.521328]\n",
      "epoch:28 step:22243 [D loss: 0.406921, acc.: 81.25%] [G loss: 2.755602]\n",
      "epoch:28 step:22244 [D loss: 0.288341, acc.: 89.06%] [G loss: 3.378273]\n",
      "epoch:28 step:22245 [D loss: 0.295436, acc.: 89.84%] [G loss: 2.297986]\n",
      "epoch:28 step:22246 [D loss: 0.298416, acc.: 86.72%] [G loss: 2.502360]\n",
      "epoch:28 step:22247 [D loss: 0.337310, acc.: 84.38%] [G loss: 2.677947]\n",
      "epoch:28 step:22248 [D loss: 0.279648, acc.: 88.28%] [G loss: 2.541553]\n",
      "epoch:28 step:22249 [D loss: 0.309513, acc.: 85.16%] [G loss: 2.327553]\n",
      "epoch:28 step:22250 [D loss: 0.316071, acc.: 81.25%] [G loss: 4.068040]\n",
      "epoch:28 step:22251 [D loss: 0.264002, acc.: 89.06%] [G loss: 3.947866]\n",
      "epoch:28 step:22252 [D loss: 0.177375, acc.: 92.19%] [G loss: 4.500669]\n",
      "epoch:28 step:22253 [D loss: 0.223268, acc.: 89.84%] [G loss: 4.434933]\n",
      "epoch:28 step:22254 [D loss: 0.317018, acc.: 86.72%] [G loss: 4.601562]\n",
      "epoch:28 step:22255 [D loss: 0.339186, acc.: 82.81%] [G loss: 3.866735]\n",
      "epoch:28 step:22256 [D loss: 0.279113, acc.: 91.41%] [G loss: 3.962371]\n",
      "epoch:28 step:22257 [D loss: 0.340575, acc.: 85.16%] [G loss: 3.390231]\n",
      "epoch:28 step:22258 [D loss: 0.278176, acc.: 89.84%] [G loss: 4.488402]\n",
      "epoch:28 step:22259 [D loss: 0.424708, acc.: 82.81%] [G loss: 5.341481]\n",
      "epoch:28 step:22260 [D loss: 0.368990, acc.: 85.16%] [G loss: 4.203026]\n",
      "epoch:28 step:22261 [D loss: 0.311010, acc.: 85.94%] [G loss: 3.769679]\n",
      "epoch:28 step:22262 [D loss: 0.275585, acc.: 89.06%] [G loss: 4.375405]\n",
      "epoch:28 step:22263 [D loss: 0.387419, acc.: 81.25%] [G loss: 3.616724]\n",
      "epoch:28 step:22264 [D loss: 0.352217, acc.: 84.38%] [G loss: 2.770873]\n",
      "epoch:28 step:22265 [D loss: 0.322370, acc.: 84.38%] [G loss: 3.422223]\n",
      "epoch:28 step:22266 [D loss: 0.356297, acc.: 84.38%] [G loss: 3.244774]\n",
      "epoch:28 step:22267 [D loss: 0.410400, acc.: 82.03%] [G loss: 2.976282]\n",
      "epoch:28 step:22268 [D loss: 0.256715, acc.: 89.84%] [G loss: 2.762858]\n",
      "epoch:28 step:22269 [D loss: 0.312170, acc.: 85.16%] [G loss: 2.746195]\n",
      "epoch:28 step:22270 [D loss: 0.220716, acc.: 89.84%] [G loss: 3.455278]\n",
      "epoch:28 step:22271 [D loss: 0.287881, acc.: 89.06%] [G loss: 3.186577]\n",
      "epoch:28 step:22272 [D loss: 0.273222, acc.: 88.28%] [G loss: 2.438516]\n",
      "epoch:28 step:22273 [D loss: 0.296646, acc.: 89.06%] [G loss: 3.059681]\n",
      "epoch:28 step:22274 [D loss: 0.354123, acc.: 86.72%] [G loss: 3.033992]\n",
      "epoch:28 step:22275 [D loss: 0.368717, acc.: 82.03%] [G loss: 2.768948]\n",
      "epoch:28 step:22276 [D loss: 0.279805, acc.: 87.50%] [G loss: 2.880863]\n",
      "epoch:28 step:22277 [D loss: 0.288072, acc.: 89.06%] [G loss: 2.118207]\n",
      "epoch:28 step:22278 [D loss: 0.267021, acc.: 88.28%] [G loss: 2.846028]\n",
      "epoch:28 step:22279 [D loss: 0.394127, acc.: 82.81%] [G loss: 3.544100]\n",
      "epoch:28 step:22280 [D loss: 0.325803, acc.: 82.81%] [G loss: 2.866975]\n",
      "epoch:28 step:22281 [D loss: 0.266515, acc.: 89.84%] [G loss: 4.764028]\n",
      "epoch:28 step:22282 [D loss: 0.237980, acc.: 88.28%] [G loss: 4.278326]\n",
      "epoch:28 step:22283 [D loss: 0.212593, acc.: 93.75%] [G loss: 4.209387]\n",
      "epoch:28 step:22284 [D loss: 0.256478, acc.: 87.50%] [G loss: 3.639301]\n",
      "epoch:28 step:22285 [D loss: 0.230745, acc.: 90.62%] [G loss: 4.028223]\n",
      "epoch:28 step:22286 [D loss: 0.308287, acc.: 82.81%] [G loss: 5.031790]\n",
      "epoch:28 step:22287 [D loss: 0.476582, acc.: 81.25%] [G loss: 4.102314]\n",
      "epoch:28 step:22288 [D loss: 0.237497, acc.: 92.19%] [G loss: 4.351134]\n",
      "epoch:28 step:22289 [D loss: 0.362123, acc.: 84.38%] [G loss: 3.165976]\n",
      "epoch:28 step:22290 [D loss: 0.212235, acc.: 92.97%] [G loss: 4.557809]\n",
      "epoch:28 step:22291 [D loss: 0.334853, acc.: 83.59%] [G loss: 3.338829]\n",
      "epoch:28 step:22292 [D loss: 0.335092, acc.: 89.84%] [G loss: 3.434274]\n",
      "epoch:28 step:22293 [D loss: 0.274055, acc.: 85.94%] [G loss: 3.322052]\n",
      "epoch:28 step:22294 [D loss: 0.221814, acc.: 92.97%] [G loss: 3.860340]\n",
      "epoch:28 step:22295 [D loss: 0.290994, acc.: 89.84%] [G loss: 3.218246]\n",
      "epoch:28 step:22296 [D loss: 0.303832, acc.: 85.16%] [G loss: 3.914786]\n",
      "epoch:28 step:22297 [D loss: 0.424508, acc.: 84.38%] [G loss: 4.123619]\n",
      "epoch:28 step:22298 [D loss: 0.334070, acc.: 83.59%] [G loss: 3.726763]\n",
      "epoch:28 step:22299 [D loss: 0.359639, acc.: 83.59%] [G loss: 3.522341]\n",
      "epoch:28 step:22300 [D loss: 0.375987, acc.: 84.38%] [G loss: 3.776751]\n",
      "epoch:28 step:22301 [D loss: 0.211847, acc.: 92.19%] [G loss: 4.224384]\n",
      "epoch:28 step:22302 [D loss: 0.220663, acc.: 90.62%] [G loss: 5.286556]\n",
      "epoch:28 step:22303 [D loss: 0.400292, acc.: 79.69%] [G loss: 3.931195]\n",
      "epoch:28 step:22304 [D loss: 0.302803, acc.: 84.38%] [G loss: 4.198535]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:28 step:22305 [D loss: 0.386367, acc.: 85.94%] [G loss: 4.378877]\n",
      "epoch:28 step:22306 [D loss: 0.296745, acc.: 89.06%] [G loss: 4.233016]\n",
      "epoch:28 step:22307 [D loss: 0.314322, acc.: 83.59%] [G loss: 2.697334]\n",
      "epoch:28 step:22308 [D loss: 0.246418, acc.: 89.84%] [G loss: 4.317745]\n",
      "epoch:28 step:22309 [D loss: 0.332450, acc.: 83.59%] [G loss: 4.028508]\n",
      "epoch:28 step:22310 [D loss: 0.283013, acc.: 88.28%] [G loss: 4.009290]\n",
      "epoch:28 step:22311 [D loss: 0.412386, acc.: 82.03%] [G loss: 4.406306]\n",
      "epoch:28 step:22312 [D loss: 0.357547, acc.: 83.59%] [G loss: 3.259191]\n",
      "epoch:28 step:22313 [D loss: 0.268649, acc.: 86.72%] [G loss: 3.675062]\n",
      "epoch:28 step:22314 [D loss: 0.231215, acc.: 89.06%] [G loss: 3.642116]\n",
      "epoch:28 step:22315 [D loss: 0.257515, acc.: 89.06%] [G loss: 3.910230]\n",
      "epoch:28 step:22316 [D loss: 0.278673, acc.: 85.16%] [G loss: 3.445651]\n",
      "epoch:28 step:22317 [D loss: 0.313045, acc.: 83.59%] [G loss: 2.747911]\n",
      "epoch:28 step:22318 [D loss: 0.276432, acc.: 89.84%] [G loss: 2.438016]\n",
      "epoch:28 step:22319 [D loss: 0.280600, acc.: 90.62%] [G loss: 3.178639]\n",
      "epoch:28 step:22320 [D loss: 0.354966, acc.: 82.81%] [G loss: 3.015578]\n",
      "epoch:28 step:22321 [D loss: 0.332068, acc.: 83.59%] [G loss: 2.647233]\n",
      "epoch:28 step:22322 [D loss: 0.271357, acc.: 86.72%] [G loss: 3.066937]\n",
      "epoch:28 step:22323 [D loss: 0.251987, acc.: 90.62%] [G loss: 4.388275]\n",
      "epoch:28 step:22324 [D loss: 0.247992, acc.: 92.97%] [G loss: 3.780203]\n",
      "epoch:28 step:22325 [D loss: 0.272369, acc.: 86.72%] [G loss: 3.053589]\n",
      "epoch:28 step:22326 [D loss: 0.261631, acc.: 86.72%] [G loss: 3.042717]\n",
      "epoch:28 step:22327 [D loss: 0.288737, acc.: 87.50%] [G loss: 2.969330]\n",
      "epoch:28 step:22328 [D loss: 0.332157, acc.: 85.16%] [G loss: 3.032906]\n",
      "epoch:28 step:22329 [D loss: 0.325740, acc.: 87.50%] [G loss: 2.895413]\n",
      "epoch:28 step:22330 [D loss: 0.227091, acc.: 89.06%] [G loss: 3.777621]\n",
      "epoch:28 step:22331 [D loss: 0.361518, acc.: 81.25%] [G loss: 3.789859]\n",
      "epoch:28 step:22332 [D loss: 0.262123, acc.: 89.06%] [G loss: 3.810785]\n",
      "epoch:28 step:22333 [D loss: 0.448705, acc.: 77.34%] [G loss: 2.816939]\n",
      "epoch:28 step:22334 [D loss: 0.328761, acc.: 82.81%] [G loss: 3.378512]\n",
      "epoch:28 step:22335 [D loss: 0.547536, acc.: 73.44%] [G loss: 3.334198]\n",
      "epoch:28 step:22336 [D loss: 0.493402, acc.: 78.12%] [G loss: 2.814781]\n",
      "epoch:28 step:22337 [D loss: 0.282739, acc.: 89.84%] [G loss: 4.406124]\n",
      "epoch:28 step:22338 [D loss: 0.382228, acc.: 83.59%] [G loss: 4.300239]\n",
      "epoch:28 step:22339 [D loss: 0.437011, acc.: 79.69%] [G loss: 5.611643]\n",
      "epoch:28 step:22340 [D loss: 0.337022, acc.: 83.59%] [G loss: 4.938426]\n",
      "epoch:28 step:22341 [D loss: 0.393432, acc.: 83.59%] [G loss: 4.165697]\n",
      "epoch:28 step:22342 [D loss: 0.349127, acc.: 82.03%] [G loss: 3.725711]\n",
      "epoch:28 step:22343 [D loss: 0.352139, acc.: 84.38%] [G loss: 3.769129]\n",
      "epoch:28 step:22344 [D loss: 0.313234, acc.: 84.38%] [G loss: 4.035005]\n",
      "epoch:28 step:22345 [D loss: 0.328088, acc.: 84.38%] [G loss: 3.999146]\n",
      "epoch:28 step:22346 [D loss: 0.305186, acc.: 87.50%] [G loss: 2.679302]\n",
      "epoch:28 step:22347 [D loss: 0.222225, acc.: 91.41%] [G loss: 4.498088]\n",
      "epoch:28 step:22348 [D loss: 0.234358, acc.: 89.84%] [G loss: 3.752117]\n",
      "epoch:28 step:22349 [D loss: 0.347365, acc.: 85.94%] [G loss: 4.341132]\n",
      "epoch:28 step:22350 [D loss: 0.302963, acc.: 88.28%] [G loss: 5.324007]\n",
      "epoch:28 step:22351 [D loss: 0.383144, acc.: 83.59%] [G loss: 4.177989]\n",
      "epoch:28 step:22352 [D loss: 0.355166, acc.: 83.59%] [G loss: 2.999222]\n",
      "epoch:28 step:22353 [D loss: 0.303038, acc.: 86.72%] [G loss: 2.865208]\n",
      "epoch:28 step:22354 [D loss: 0.295868, acc.: 87.50%] [G loss: 3.844545]\n",
      "epoch:28 step:22355 [D loss: 0.208696, acc.: 90.62%] [G loss: 4.524961]\n",
      "epoch:28 step:22356 [D loss: 0.328915, acc.: 84.38%] [G loss: 2.907694]\n",
      "epoch:28 step:22357 [D loss: 0.350311, acc.: 86.72%] [G loss: 4.086452]\n",
      "epoch:28 step:22358 [D loss: 0.226130, acc.: 92.19%] [G loss: 3.494267]\n",
      "epoch:28 step:22359 [D loss: 0.368241, acc.: 85.16%] [G loss: 3.773028]\n",
      "epoch:28 step:22360 [D loss: 0.273999, acc.: 87.50%] [G loss: 2.838781]\n",
      "epoch:28 step:22361 [D loss: 0.364650, acc.: 82.03%] [G loss: 2.977093]\n",
      "epoch:28 step:22362 [D loss: 0.288698, acc.: 89.84%] [G loss: 3.479435]\n",
      "epoch:28 step:22363 [D loss: 0.331529, acc.: 82.03%] [G loss: 3.894907]\n",
      "epoch:28 step:22364 [D loss: 0.353901, acc.: 85.94%] [G loss: 2.820789]\n",
      "epoch:28 step:22365 [D loss: 0.177667, acc.: 94.53%] [G loss: 4.638391]\n",
      "epoch:28 step:22366 [D loss: 0.278443, acc.: 86.72%] [G loss: 4.089133]\n",
      "epoch:28 step:22367 [D loss: 0.299646, acc.: 85.16%] [G loss: 3.315279]\n",
      "epoch:28 step:22368 [D loss: 0.313793, acc.: 85.94%] [G loss: 3.481380]\n",
      "epoch:28 step:22369 [D loss: 0.273044, acc.: 88.28%] [G loss: 3.430203]\n",
      "epoch:28 step:22370 [D loss: 0.292695, acc.: 86.72%] [G loss: 4.527823]\n",
      "epoch:28 step:22371 [D loss: 0.314694, acc.: 86.72%] [G loss: 5.357702]\n",
      "epoch:28 step:22372 [D loss: 0.313734, acc.: 85.16%] [G loss: 3.357451]\n",
      "epoch:28 step:22373 [D loss: 0.359148, acc.: 82.03%] [G loss: 3.019820]\n",
      "epoch:28 step:22374 [D loss: 0.380579, acc.: 82.03%] [G loss: 3.527946]\n",
      "epoch:28 step:22375 [D loss: 0.320791, acc.: 84.38%] [G loss: 3.104655]\n",
      "epoch:28 step:22376 [D loss: 0.327177, acc.: 85.94%] [G loss: 2.644969]\n",
      "epoch:28 step:22377 [D loss: 0.313109, acc.: 85.16%] [G loss: 3.424615]\n",
      "epoch:28 step:22378 [D loss: 0.307911, acc.: 85.16%] [G loss: 3.580126]\n",
      "epoch:28 step:22379 [D loss: 0.323719, acc.: 84.38%] [G loss: 2.897384]\n",
      "epoch:28 step:22380 [D loss: 0.290709, acc.: 87.50%] [G loss: 3.286822]\n",
      "epoch:28 step:22381 [D loss: 0.382816, acc.: 85.16%] [G loss: 2.883491]\n",
      "epoch:28 step:22382 [D loss: 0.380303, acc.: 82.81%] [G loss: 3.222670]\n",
      "epoch:28 step:22383 [D loss: 0.252042, acc.: 88.28%] [G loss: 3.191556]\n",
      "epoch:28 step:22384 [D loss: 0.328381, acc.: 83.59%] [G loss: 3.307267]\n",
      "epoch:28 step:22385 [D loss: 0.373715, acc.: 82.03%] [G loss: 4.580928]\n",
      "epoch:28 step:22386 [D loss: 0.220614, acc.: 92.19%] [G loss: 3.872778]\n",
      "epoch:28 step:22387 [D loss: 0.243620, acc.: 85.94%] [G loss: 4.677417]\n",
      "epoch:28 step:22388 [D loss: 0.321607, acc.: 82.03%] [G loss: 2.708660]\n",
      "epoch:28 step:22389 [D loss: 0.249254, acc.: 89.06%] [G loss: 4.897727]\n",
      "epoch:28 step:22390 [D loss: 0.381785, acc.: 80.47%] [G loss: 4.060934]\n",
      "epoch:28 step:22391 [D loss: 0.329428, acc.: 85.16%] [G loss: 3.174940]\n",
      "epoch:28 step:22392 [D loss: 0.231365, acc.: 88.28%] [G loss: 4.222377]\n",
      "epoch:28 step:22393 [D loss: 0.330817, acc.: 87.50%] [G loss: 4.169991]\n",
      "epoch:28 step:22394 [D loss: 0.402327, acc.: 85.16%] [G loss: 4.060034]\n",
      "epoch:28 step:22395 [D loss: 0.421131, acc.: 79.69%] [G loss: 3.664612]\n",
      "epoch:28 step:22396 [D loss: 0.277848, acc.: 89.06%] [G loss: 2.967734]\n",
      "epoch:28 step:22397 [D loss: 0.328934, acc.: 84.38%] [G loss: 3.629412]\n",
      "epoch:28 step:22398 [D loss: 0.388676, acc.: 79.69%] [G loss: 2.702712]\n",
      "epoch:28 step:22399 [D loss: 0.303265, acc.: 87.50%] [G loss: 4.134365]\n",
      "epoch:28 step:22400 [D loss: 0.379749, acc.: 84.38%] [G loss: 3.647395]\n",
      "##############\n",
      "[0.86140422 0.88809866 0.8107058  0.8097239  0.78208655 0.8059869\n",
      " 0.87438901 0.84440131 0.83550969 0.81355654]\n",
      "##########\n",
      "epoch:28 step:22401 [D loss: 0.303057, acc.: 86.72%] [G loss: 3.888623]\n",
      "epoch:28 step:22402 [D loss: 0.221290, acc.: 89.84%] [G loss: 3.643268]\n",
      "epoch:28 step:22403 [D loss: 0.350835, acc.: 86.72%] [G loss: 4.541058]\n",
      "epoch:28 step:22404 [D loss: 0.324651, acc.: 85.16%] [G loss: 4.735639]\n",
      "epoch:28 step:22405 [D loss: 0.463129, acc.: 78.12%] [G loss: 3.360106]\n",
      "epoch:28 step:22406 [D loss: 0.289472, acc.: 84.38%] [G loss: 3.303626]\n",
      "epoch:28 step:22407 [D loss: 0.288109, acc.: 89.06%] [G loss: 4.161815]\n",
      "epoch:28 step:22408 [D loss: 0.287292, acc.: 85.94%] [G loss: 5.883101]\n",
      "epoch:28 step:22409 [D loss: 0.276904, acc.: 89.06%] [G loss: 5.413246]\n",
      "epoch:28 step:22410 [D loss: 0.401900, acc.: 81.25%] [G loss: 3.003834]\n",
      "epoch:28 step:22411 [D loss: 0.206866, acc.: 91.41%] [G loss: 3.703103]\n",
      "epoch:28 step:22412 [D loss: 0.203825, acc.: 92.97%] [G loss: 4.375265]\n",
      "epoch:28 step:22413 [D loss: 0.277015, acc.: 89.84%] [G loss: 3.081493]\n",
      "epoch:28 step:22414 [D loss: 0.287233, acc.: 86.72%] [G loss: 6.423151]\n",
      "epoch:28 step:22415 [D loss: 0.370692, acc.: 78.91%] [G loss: 4.094025]\n",
      "epoch:28 step:22416 [D loss: 0.293722, acc.: 87.50%] [G loss: 3.146692]\n",
      "epoch:28 step:22417 [D loss: 0.407503, acc.: 83.59%] [G loss: 4.612037]\n",
      "epoch:28 step:22418 [D loss: 0.300637, acc.: 85.16%] [G loss: 3.199268]\n",
      "epoch:28 step:22419 [D loss: 0.283092, acc.: 86.72%] [G loss: 2.991480]\n",
      "epoch:28 step:22420 [D loss: 0.539569, acc.: 78.91%] [G loss: 4.648608]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:28 step:22421 [D loss: 0.277679, acc.: 88.28%] [G loss: 3.129460]\n",
      "epoch:28 step:22422 [D loss: 0.188786, acc.: 90.62%] [G loss: 5.254501]\n",
      "epoch:28 step:22423 [D loss: 0.400078, acc.: 80.47%] [G loss: 8.485746]\n",
      "epoch:28 step:22424 [D loss: 0.482016, acc.: 79.69%] [G loss: 5.067014]\n",
      "epoch:28 step:22425 [D loss: 0.482721, acc.: 81.25%] [G loss: 6.642462]\n",
      "epoch:28 step:22426 [D loss: 0.733250, acc.: 77.34%] [G loss: 8.265182]\n",
      "epoch:28 step:22427 [D loss: 0.415056, acc.: 80.47%] [G loss: 5.510509]\n",
      "epoch:28 step:22428 [D loss: 0.299694, acc.: 85.16%] [G loss: 4.848153]\n",
      "epoch:28 step:22429 [D loss: 0.338688, acc.: 87.50%] [G loss: 3.082488]\n",
      "epoch:28 step:22430 [D loss: 0.394735, acc.: 84.38%] [G loss: 3.341542]\n",
      "epoch:28 step:22431 [D loss: 0.207395, acc.: 92.97%] [G loss: 3.673938]\n",
      "epoch:28 step:22432 [D loss: 0.319159, acc.: 88.28%] [G loss: 3.695511]\n",
      "epoch:28 step:22433 [D loss: 0.318140, acc.: 83.59%] [G loss: 4.038798]\n",
      "epoch:28 step:22434 [D loss: 0.247025, acc.: 91.41%] [G loss: 4.159225]\n",
      "epoch:28 step:22435 [D loss: 0.274952, acc.: 89.06%] [G loss: 4.024166]\n",
      "epoch:28 step:22436 [D loss: 0.285477, acc.: 89.84%] [G loss: 3.814823]\n",
      "epoch:28 step:22437 [D loss: 0.278549, acc.: 87.50%] [G loss: 3.962712]\n",
      "epoch:28 step:22438 [D loss: 0.282650, acc.: 89.84%] [G loss: 2.455872]\n",
      "epoch:28 step:22439 [D loss: 0.284982, acc.: 89.06%] [G loss: 3.467505]\n",
      "epoch:28 step:22440 [D loss: 0.361602, acc.: 80.47%] [G loss: 2.863197]\n",
      "epoch:28 step:22441 [D loss: 0.342631, acc.: 85.94%] [G loss: 2.957244]\n",
      "epoch:28 step:22442 [D loss: 0.330762, acc.: 83.59%] [G loss: 3.267941]\n",
      "epoch:28 step:22443 [D loss: 0.293431, acc.: 85.94%] [G loss: 3.000572]\n",
      "epoch:28 step:22444 [D loss: 0.344485, acc.: 82.81%] [G loss: 3.540778]\n",
      "epoch:28 step:22445 [D loss: 0.407437, acc.: 78.12%] [G loss: 2.865200]\n",
      "epoch:28 step:22446 [D loss: 0.352222, acc.: 87.50%] [G loss: 3.061606]\n",
      "epoch:28 step:22447 [D loss: 0.270493, acc.: 89.06%] [G loss: 3.419209]\n",
      "epoch:28 step:22448 [D loss: 0.341132, acc.: 85.16%] [G loss: 2.511915]\n",
      "epoch:28 step:22449 [D loss: 0.244519, acc.: 89.84%] [G loss: 3.427007]\n",
      "epoch:28 step:22450 [D loss: 0.287114, acc.: 84.38%] [G loss: 2.764487]\n",
      "epoch:28 step:22451 [D loss: 0.302435, acc.: 86.72%] [G loss: 2.414343]\n",
      "epoch:28 step:22452 [D loss: 0.298561, acc.: 85.16%] [G loss: 3.147060]\n",
      "epoch:28 step:22453 [D loss: 0.351441, acc.: 84.38%] [G loss: 3.598993]\n",
      "epoch:28 step:22454 [D loss: 0.302715, acc.: 82.03%] [G loss: 2.497849]\n",
      "epoch:28 step:22455 [D loss: 0.222211, acc.: 90.62%] [G loss: 3.348948]\n",
      "epoch:28 step:22456 [D loss: 0.295570, acc.: 86.72%] [G loss: 5.641957]\n",
      "epoch:28 step:22457 [D loss: 0.307931, acc.: 85.94%] [G loss: 4.455693]\n",
      "epoch:28 step:22458 [D loss: 0.333115, acc.: 83.59%] [G loss: 4.649220]\n",
      "epoch:28 step:22459 [D loss: 0.230033, acc.: 90.62%] [G loss: 4.443181]\n",
      "epoch:28 step:22460 [D loss: 0.303912, acc.: 84.38%] [G loss: 3.518837]\n",
      "epoch:28 step:22461 [D loss: 0.257126, acc.: 88.28%] [G loss: 4.439037]\n",
      "epoch:28 step:22462 [D loss: 0.374911, acc.: 82.81%] [G loss: 2.996313]\n",
      "epoch:28 step:22463 [D loss: 0.300545, acc.: 87.50%] [G loss: 4.292912]\n",
      "epoch:28 step:22464 [D loss: 0.324552, acc.: 84.38%] [G loss: 3.434444]\n",
      "epoch:28 step:22465 [D loss: 0.386316, acc.: 83.59%] [G loss: 3.508750]\n",
      "epoch:28 step:22466 [D loss: 0.369154, acc.: 85.94%] [G loss: 4.622366]\n",
      "epoch:28 step:22467 [D loss: 0.394739, acc.: 78.12%] [G loss: 3.163470]\n",
      "epoch:28 step:22468 [D loss: 0.215416, acc.: 91.41%] [G loss: 3.080519]\n",
      "epoch:28 step:22469 [D loss: 0.357293, acc.: 83.59%] [G loss: 2.984726]\n",
      "epoch:28 step:22470 [D loss: 0.266394, acc.: 85.94%] [G loss: 2.699152]\n",
      "epoch:28 step:22471 [D loss: 0.263498, acc.: 87.50%] [G loss: 2.664833]\n",
      "epoch:28 step:22472 [D loss: 0.295095, acc.: 85.94%] [G loss: 2.572492]\n",
      "epoch:28 step:22473 [D loss: 0.367035, acc.: 81.25%] [G loss: 2.697982]\n",
      "epoch:28 step:22474 [D loss: 0.300182, acc.: 83.59%] [G loss: 2.714096]\n",
      "epoch:28 step:22475 [D loss: 0.250777, acc.: 91.41%] [G loss: 3.204656]\n",
      "epoch:28 step:22476 [D loss: 0.299211, acc.: 86.72%] [G loss: 3.512789]\n",
      "epoch:28 step:22477 [D loss: 0.266418, acc.: 88.28%] [G loss: 2.765348]\n",
      "epoch:28 step:22478 [D loss: 0.296483, acc.: 89.06%] [G loss: 3.158145]\n",
      "epoch:28 step:22479 [D loss: 0.257483, acc.: 90.62%] [G loss: 3.002580]\n",
      "epoch:28 step:22480 [D loss: 0.255769, acc.: 89.84%] [G loss: 2.929014]\n",
      "epoch:28 step:22481 [D loss: 0.391177, acc.: 85.94%] [G loss: 2.618259]\n",
      "epoch:28 step:22482 [D loss: 0.304585, acc.: 84.38%] [G loss: 3.383236]\n",
      "epoch:28 step:22483 [D loss: 0.269752, acc.: 85.16%] [G loss: 3.444871]\n",
      "epoch:28 step:22484 [D loss: 0.286160, acc.: 88.28%] [G loss: 5.510458]\n",
      "epoch:28 step:22485 [D loss: 0.377047, acc.: 84.38%] [G loss: 3.812414]\n",
      "epoch:28 step:22486 [D loss: 0.374983, acc.: 83.59%] [G loss: 6.107937]\n",
      "epoch:28 step:22487 [D loss: 0.523905, acc.: 78.91%] [G loss: 6.671459]\n",
      "epoch:28 step:22488 [D loss: 0.716876, acc.: 74.22%] [G loss: 4.871176]\n",
      "epoch:28 step:22489 [D loss: 0.214703, acc.: 89.84%] [G loss: 5.345639]\n",
      "epoch:28 step:22490 [D loss: 0.313158, acc.: 81.25%] [G loss: 3.828534]\n",
      "epoch:28 step:22491 [D loss: 0.299160, acc.: 82.81%] [G loss: 3.738755]\n",
      "epoch:28 step:22492 [D loss: 0.301547, acc.: 86.72%] [G loss: 2.972284]\n",
      "epoch:28 step:22493 [D loss: 0.384819, acc.: 83.59%] [G loss: 3.404952]\n",
      "epoch:28 step:22494 [D loss: 0.419053, acc.: 82.03%] [G loss: 3.327937]\n",
      "epoch:28 step:22495 [D loss: 0.271311, acc.: 89.06%] [G loss: 3.458556]\n",
      "epoch:28 step:22496 [D loss: 0.459858, acc.: 81.25%] [G loss: 3.651799]\n",
      "epoch:28 step:22497 [D loss: 0.338050, acc.: 84.38%] [G loss: 3.006579]\n",
      "epoch:28 step:22498 [D loss: 0.276297, acc.: 87.50%] [G loss: 2.817802]\n",
      "epoch:28 step:22499 [D loss: 0.318029, acc.: 86.72%] [G loss: 4.000740]\n",
      "epoch:28 step:22500 [D loss: 0.372865, acc.: 85.94%] [G loss: 2.958548]\n",
      "epoch:28 step:22501 [D loss: 0.297319, acc.: 87.50%] [G loss: 2.976047]\n",
      "epoch:28 step:22502 [D loss: 0.335481, acc.: 85.16%] [G loss: 3.180384]\n",
      "epoch:28 step:22503 [D loss: 0.487634, acc.: 76.56%] [G loss: 2.531751]\n",
      "epoch:28 step:22504 [D loss: 0.261727, acc.: 89.06%] [G loss: 2.602585]\n",
      "epoch:28 step:22505 [D loss: 0.307020, acc.: 86.72%] [G loss: 3.411785]\n",
      "epoch:28 step:22506 [D loss: 0.377374, acc.: 83.59%] [G loss: 3.756126]\n",
      "epoch:28 step:22507 [D loss: 0.337552, acc.: 88.28%] [G loss: 3.519835]\n",
      "epoch:28 step:22508 [D loss: 0.253999, acc.: 89.06%] [G loss: 4.232250]\n",
      "epoch:28 step:22509 [D loss: 0.230870, acc.: 91.41%] [G loss: 2.656001]\n",
      "epoch:28 step:22510 [D loss: 0.355531, acc.: 80.47%] [G loss: 3.404413]\n",
      "epoch:28 step:22511 [D loss: 0.315872, acc.: 83.59%] [G loss: 2.688213]\n",
      "epoch:28 step:22512 [D loss: 0.239730, acc.: 90.62%] [G loss: 3.041151]\n",
      "epoch:28 step:22513 [D loss: 0.394689, acc.: 80.47%] [G loss: 3.417363]\n",
      "epoch:28 step:22514 [D loss: 0.385220, acc.: 81.25%] [G loss: 3.099095]\n",
      "epoch:28 step:22515 [D loss: 0.347719, acc.: 83.59%] [G loss: 3.376642]\n",
      "epoch:28 step:22516 [D loss: 0.403614, acc.: 83.59%] [G loss: 2.539697]\n",
      "epoch:28 step:22517 [D loss: 0.341139, acc.: 85.94%] [G loss: 2.997757]\n",
      "epoch:28 step:22518 [D loss: 0.342987, acc.: 86.72%] [G loss: 3.052818]\n",
      "epoch:28 step:22519 [D loss: 0.307136, acc.: 85.94%] [G loss: 2.750938]\n",
      "epoch:28 step:22520 [D loss: 0.251098, acc.: 89.06%] [G loss: 4.755804]\n",
      "epoch:28 step:22521 [D loss: 0.296611, acc.: 86.72%] [G loss: 3.515233]\n",
      "epoch:28 step:22522 [D loss: 0.289117, acc.: 87.50%] [G loss: 3.737526]\n",
      "epoch:28 step:22523 [D loss: 0.306462, acc.: 89.06%] [G loss: 2.458792]\n",
      "epoch:28 step:22524 [D loss: 0.363714, acc.: 84.38%] [G loss: 2.946548]\n",
      "epoch:28 step:22525 [D loss: 0.463135, acc.: 80.47%] [G loss: 5.514637]\n",
      "epoch:28 step:22526 [D loss: 0.552846, acc.: 78.12%] [G loss: 6.608244]\n",
      "epoch:28 step:22527 [D loss: 0.487754, acc.: 82.03%] [G loss: 6.658034]\n",
      "epoch:28 step:22528 [D loss: 0.288991, acc.: 90.62%] [G loss: 3.076446]\n",
      "epoch:28 step:22529 [D loss: 0.283903, acc.: 85.16%] [G loss: 3.271462]\n",
      "epoch:28 step:22530 [D loss: 0.341722, acc.: 84.38%] [G loss: 3.461087]\n",
      "epoch:28 step:22531 [D loss: 0.304335, acc.: 85.16%] [G loss: 4.331307]\n",
      "epoch:28 step:22532 [D loss: 0.309875, acc.: 89.84%] [G loss: 4.423153]\n",
      "epoch:28 step:22533 [D loss: 0.225606, acc.: 92.97%] [G loss: 4.367600]\n",
      "epoch:28 step:22534 [D loss: 0.410203, acc.: 78.12%] [G loss: 4.040391]\n",
      "epoch:28 step:22535 [D loss: 0.279241, acc.: 88.28%] [G loss: 3.888729]\n",
      "epoch:28 step:22536 [D loss: 0.329933, acc.: 82.03%] [G loss: 3.070401]\n",
      "epoch:28 step:22537 [D loss: 0.407057, acc.: 80.47%] [G loss: 3.026698]\n",
      "epoch:28 step:22538 [D loss: 0.304602, acc.: 82.81%] [G loss: 3.609047]\n",
      "epoch:28 step:22539 [D loss: 0.309847, acc.: 84.38%] [G loss: 3.119458]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:28 step:22540 [D loss: 0.280842, acc.: 87.50%] [G loss: 2.871056]\n",
      "epoch:28 step:22541 [D loss: 0.278983, acc.: 92.19%] [G loss: 2.956409]\n",
      "epoch:28 step:22542 [D loss: 0.222409, acc.: 91.41%] [G loss: 3.412143]\n",
      "epoch:28 step:22543 [D loss: 0.268147, acc.: 91.41%] [G loss: 2.688169]\n",
      "epoch:28 step:22544 [D loss: 0.324410, acc.: 84.38%] [G loss: 3.540847]\n",
      "epoch:28 step:22545 [D loss: 0.269581, acc.: 88.28%] [G loss: 2.938249]\n",
      "epoch:28 step:22546 [D loss: 0.251295, acc.: 87.50%] [G loss: 3.865374]\n",
      "epoch:28 step:22547 [D loss: 0.250642, acc.: 89.06%] [G loss: 4.246794]\n",
      "epoch:28 step:22548 [D loss: 0.310206, acc.: 86.72%] [G loss: 4.479871]\n",
      "epoch:28 step:22549 [D loss: 0.258557, acc.: 89.06%] [G loss: 6.876181]\n",
      "epoch:28 step:22550 [D loss: 0.223287, acc.: 90.62%] [G loss: 4.790571]\n",
      "epoch:28 step:22551 [D loss: 0.302360, acc.: 86.72%] [G loss: 4.168575]\n",
      "epoch:28 step:22552 [D loss: 0.347994, acc.: 82.03%] [G loss: 3.537288]\n",
      "epoch:28 step:22553 [D loss: 0.330398, acc.: 86.72%] [G loss: 3.866956]\n",
      "epoch:28 step:22554 [D loss: 0.316815, acc.: 86.72%] [G loss: 4.054001]\n",
      "epoch:28 step:22555 [D loss: 0.270072, acc.: 86.72%] [G loss: 4.639972]\n",
      "epoch:28 step:22556 [D loss: 0.380013, acc.: 81.25%] [G loss: 5.032605]\n",
      "epoch:28 step:22557 [D loss: 0.381652, acc.: 85.16%] [G loss: 4.535459]\n",
      "epoch:28 step:22558 [D loss: 0.333084, acc.: 85.16%] [G loss: 5.876672]\n",
      "epoch:28 step:22559 [D loss: 0.316265, acc.: 85.16%] [G loss: 3.503016]\n",
      "epoch:28 step:22560 [D loss: 0.364254, acc.: 82.03%] [G loss: 3.649310]\n",
      "epoch:28 step:22561 [D loss: 0.382684, acc.: 81.25%] [G loss: 2.747203]\n",
      "epoch:28 step:22562 [D loss: 0.246373, acc.: 89.84%] [G loss: 4.150163]\n",
      "epoch:28 step:22563 [D loss: 0.254672, acc.: 87.50%] [G loss: 4.107179]\n",
      "epoch:28 step:22564 [D loss: 0.220476, acc.: 90.62%] [G loss: 3.400464]\n",
      "epoch:28 step:22565 [D loss: 0.268200, acc.: 89.06%] [G loss: 4.522506]\n",
      "epoch:28 step:22566 [D loss: 0.236840, acc.: 92.19%] [G loss: 4.077105]\n",
      "epoch:28 step:22567 [D loss: 0.251598, acc.: 89.84%] [G loss: 4.550487]\n",
      "epoch:28 step:22568 [D loss: 0.267238, acc.: 87.50%] [G loss: 3.723215]\n",
      "epoch:28 step:22569 [D loss: 0.385554, acc.: 83.59%] [G loss: 3.175971]\n",
      "epoch:28 step:22570 [D loss: 0.261139, acc.: 88.28%] [G loss: 5.322372]\n",
      "epoch:28 step:22571 [D loss: 0.314689, acc.: 81.25%] [G loss: 4.032294]\n",
      "epoch:28 step:22572 [D loss: 0.279395, acc.: 86.72%] [G loss: 4.579026]\n",
      "epoch:28 step:22573 [D loss: 0.314746, acc.: 85.16%] [G loss: 3.523085]\n",
      "epoch:28 step:22574 [D loss: 0.376629, acc.: 85.16%] [G loss: 2.708972]\n",
      "epoch:28 step:22575 [D loss: 0.444977, acc.: 77.34%] [G loss: 2.647665]\n",
      "epoch:28 step:22576 [D loss: 0.361326, acc.: 82.03%] [G loss: 3.132519]\n",
      "epoch:28 step:22577 [D loss: 0.408461, acc.: 79.69%] [G loss: 2.919817]\n",
      "epoch:28 step:22578 [D loss: 0.378897, acc.: 79.69%] [G loss: 4.831609]\n",
      "epoch:28 step:22579 [D loss: 0.189878, acc.: 92.19%] [G loss: 3.549412]\n",
      "epoch:28 step:22580 [D loss: 0.312163, acc.: 86.72%] [G loss: 3.664237]\n",
      "epoch:28 step:22581 [D loss: 0.360146, acc.: 85.16%] [G loss: 4.244574]\n",
      "epoch:28 step:22582 [D loss: 0.323532, acc.: 85.16%] [G loss: 3.201643]\n",
      "epoch:28 step:22583 [D loss: 0.304557, acc.: 84.38%] [G loss: 4.151918]\n",
      "epoch:28 step:22584 [D loss: 0.415980, acc.: 83.59%] [G loss: 4.831399]\n",
      "epoch:28 step:22585 [D loss: 0.433107, acc.: 83.59%] [G loss: 6.164510]\n",
      "epoch:28 step:22586 [D loss: 0.372038, acc.: 85.94%] [G loss: 3.058025]\n",
      "epoch:28 step:22587 [D loss: 0.281156, acc.: 86.72%] [G loss: 3.778612]\n",
      "epoch:28 step:22588 [D loss: 0.320221, acc.: 87.50%] [G loss: 4.071354]\n",
      "epoch:28 step:22589 [D loss: 0.363637, acc.: 84.38%] [G loss: 3.959622]\n",
      "epoch:28 step:22590 [D loss: 0.274395, acc.: 89.06%] [G loss: 2.681061]\n",
      "epoch:28 step:22591 [D loss: 0.307605, acc.: 87.50%] [G loss: 3.648149]\n",
      "epoch:28 step:22592 [D loss: 0.312916, acc.: 86.72%] [G loss: 3.427471]\n",
      "epoch:28 step:22593 [D loss: 0.301306, acc.: 90.62%] [G loss: 4.031208]\n",
      "epoch:28 step:22594 [D loss: 0.262039, acc.: 90.62%] [G loss: 2.868495]\n",
      "epoch:28 step:22595 [D loss: 0.398220, acc.: 82.81%] [G loss: 3.487317]\n",
      "epoch:28 step:22596 [D loss: 0.340952, acc.: 84.38%] [G loss: 3.509256]\n",
      "epoch:28 step:22597 [D loss: 0.370177, acc.: 80.47%] [G loss: 3.464907]\n",
      "epoch:28 step:22598 [D loss: 0.251055, acc.: 89.84%] [G loss: 3.355897]\n",
      "epoch:28 step:22599 [D loss: 0.331723, acc.: 85.94%] [G loss: 3.477304]\n",
      "epoch:28 step:22600 [D loss: 0.331821, acc.: 86.72%] [G loss: 3.034154]\n",
      "##############\n",
      "[0.85680782 0.86146675 0.77719018 0.802509   0.77434354 0.8130325\n",
      " 0.89495465 0.86134685 0.81553923 0.81381395]\n",
      "##########\n",
      "epoch:28 step:22601 [D loss: 0.209894, acc.: 89.06%] [G loss: 4.122770]\n",
      "epoch:28 step:22602 [D loss: 0.367775, acc.: 81.25%] [G loss: 2.673281]\n",
      "epoch:28 step:22603 [D loss: 0.306360, acc.: 86.72%] [G loss: 3.397174]\n",
      "epoch:28 step:22604 [D loss: 0.350794, acc.: 83.59%] [G loss: 3.323719]\n",
      "epoch:28 step:22605 [D loss: 0.312194, acc.: 86.72%] [G loss: 3.390492]\n",
      "epoch:28 step:22606 [D loss: 0.333119, acc.: 87.50%] [G loss: 4.500154]\n",
      "epoch:28 step:22607 [D loss: 0.330886, acc.: 86.72%] [G loss: 3.257187]\n",
      "epoch:28 step:22608 [D loss: 0.280991, acc.: 85.94%] [G loss: 3.633289]\n",
      "epoch:28 step:22609 [D loss: 0.281248, acc.: 86.72%] [G loss: 2.982925]\n",
      "epoch:28 step:22610 [D loss: 0.209534, acc.: 91.41%] [G loss: 3.286609]\n",
      "epoch:28 step:22611 [D loss: 0.236763, acc.: 90.62%] [G loss: 3.551519]\n",
      "epoch:28 step:22612 [D loss: 0.265557, acc.: 87.50%] [G loss: 3.609005]\n",
      "epoch:28 step:22613 [D loss: 0.385536, acc.: 84.38%] [G loss: 2.975023]\n",
      "epoch:28 step:22614 [D loss: 0.242647, acc.: 89.06%] [G loss: 3.764807]\n",
      "epoch:28 step:22615 [D loss: 0.232412, acc.: 92.19%] [G loss: 3.965411]\n",
      "epoch:28 step:22616 [D loss: 0.279659, acc.: 88.28%] [G loss: 3.537516]\n",
      "epoch:28 step:22617 [D loss: 0.269040, acc.: 89.06%] [G loss: 3.291644]\n",
      "epoch:28 step:22618 [D loss: 0.311444, acc.: 88.28%] [G loss: 3.441739]\n",
      "epoch:28 step:22619 [D loss: 0.439551, acc.: 81.25%] [G loss: 2.940396]\n",
      "epoch:28 step:22620 [D loss: 0.239748, acc.: 89.84%] [G loss: 3.824101]\n",
      "epoch:28 step:22621 [D loss: 0.333091, acc.: 84.38%] [G loss: 2.835392]\n",
      "epoch:28 step:22622 [D loss: 0.288553, acc.: 86.72%] [G loss: 3.776249]\n",
      "epoch:28 step:22623 [D loss: 0.287158, acc.: 84.38%] [G loss: 4.656199]\n",
      "epoch:28 step:22624 [D loss: 0.228772, acc.: 90.62%] [G loss: 4.279095]\n",
      "epoch:28 step:22625 [D loss: 0.275444, acc.: 88.28%] [G loss: 3.223850]\n",
      "epoch:28 step:22626 [D loss: 0.399409, acc.: 82.03%] [G loss: 3.510949]\n",
      "epoch:28 step:22627 [D loss: 0.255644, acc.: 87.50%] [G loss: 3.597977]\n",
      "epoch:28 step:22628 [D loss: 0.312862, acc.: 84.38%] [G loss: 3.173135]\n",
      "epoch:28 step:22629 [D loss: 0.384992, acc.: 85.16%] [G loss: 3.885784]\n",
      "epoch:28 step:22630 [D loss: 0.368510, acc.: 82.81%] [G loss: 3.094555]\n",
      "epoch:28 step:22631 [D loss: 0.357683, acc.: 85.16%] [G loss: 3.660787]\n",
      "epoch:28 step:22632 [D loss: 0.285552, acc.: 89.06%] [G loss: 2.453428]\n",
      "epoch:28 step:22633 [D loss: 0.289193, acc.: 87.50%] [G loss: 3.007510]\n",
      "epoch:28 step:22634 [D loss: 0.369429, acc.: 82.81%] [G loss: 3.030351]\n",
      "epoch:28 step:22635 [D loss: 0.317275, acc.: 87.50%] [G loss: 2.649801]\n",
      "epoch:28 step:22636 [D loss: 0.321329, acc.: 83.59%] [G loss: 3.233886]\n",
      "epoch:28 step:22637 [D loss: 0.271454, acc.: 90.62%] [G loss: 2.789428]\n",
      "epoch:28 step:22638 [D loss: 0.286964, acc.: 85.16%] [G loss: 3.768137]\n",
      "epoch:28 step:22639 [D loss: 0.371922, acc.: 79.69%] [G loss: 2.690214]\n",
      "epoch:28 step:22640 [D loss: 0.219096, acc.: 91.41%] [G loss: 3.600814]\n",
      "epoch:28 step:22641 [D loss: 0.292410, acc.: 86.72%] [G loss: 3.713234]\n",
      "epoch:28 step:22642 [D loss: 0.377603, acc.: 80.47%] [G loss: 4.969632]\n",
      "epoch:28 step:22643 [D loss: 0.307236, acc.: 83.59%] [G loss: 3.627502]\n",
      "epoch:28 step:22644 [D loss: 0.304415, acc.: 83.59%] [G loss: 3.633308]\n",
      "epoch:28 step:22645 [D loss: 0.302153, acc.: 84.38%] [G loss: 3.418844]\n",
      "epoch:28 step:22646 [D loss: 0.334158, acc.: 84.38%] [G loss: 3.914462]\n",
      "epoch:28 step:22647 [D loss: 0.327059, acc.: 85.16%] [G loss: 5.661969]\n",
      "epoch:28 step:22648 [D loss: 0.653526, acc.: 74.22%] [G loss: 8.608577]\n",
      "epoch:28 step:22649 [D loss: 1.393410, acc.: 65.62%] [G loss: 4.642311]\n",
      "epoch:29 step:22650 [D loss: 1.164685, acc.: 56.25%] [G loss: 4.557642]\n",
      "epoch:29 step:22651 [D loss: 0.348194, acc.: 85.16%] [G loss: 7.131048]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:29 step:22652 [D loss: 1.011386, acc.: 71.88%] [G loss: 5.002998]\n",
      "epoch:29 step:22653 [D loss: 0.368162, acc.: 87.50%] [G loss: 5.093752]\n",
      "epoch:29 step:22654 [D loss: 0.515043, acc.: 75.00%] [G loss: 4.287135]\n",
      "epoch:29 step:22655 [D loss: 0.328624, acc.: 85.94%] [G loss: 3.584073]\n",
      "epoch:29 step:22656 [D loss: 0.255419, acc.: 87.50%] [G loss: 3.784296]\n",
      "epoch:29 step:22657 [D loss: 0.342917, acc.: 85.16%] [G loss: 3.082411]\n",
      "epoch:29 step:22658 [D loss: 0.261197, acc.: 89.84%] [G loss: 4.424594]\n",
      "epoch:29 step:22659 [D loss: 0.309851, acc.: 82.81%] [G loss: 3.444893]\n",
      "epoch:29 step:22660 [D loss: 0.262599, acc.: 91.41%] [G loss: 3.587753]\n",
      "epoch:29 step:22661 [D loss: 0.292694, acc.: 85.16%] [G loss: 2.497658]\n",
      "epoch:29 step:22662 [D loss: 0.236804, acc.: 89.06%] [G loss: 3.238256]\n",
      "epoch:29 step:22663 [D loss: 0.302819, acc.: 87.50%] [G loss: 2.615500]\n",
      "epoch:29 step:22664 [D loss: 0.266120, acc.: 91.41%] [G loss: 2.811071]\n",
      "epoch:29 step:22665 [D loss: 0.304440, acc.: 86.72%] [G loss: 2.662707]\n",
      "epoch:29 step:22666 [D loss: 0.319464, acc.: 85.16%] [G loss: 2.512420]\n",
      "epoch:29 step:22667 [D loss: 0.337517, acc.: 85.94%] [G loss: 2.205754]\n",
      "epoch:29 step:22668 [D loss: 0.233687, acc.: 93.75%] [G loss: 2.258979]\n",
      "epoch:29 step:22669 [D loss: 0.248745, acc.: 86.72%] [G loss: 2.977711]\n",
      "epoch:29 step:22670 [D loss: 0.334012, acc.: 87.50%] [G loss: 2.852179]\n",
      "epoch:29 step:22671 [D loss: 0.294771, acc.: 86.72%] [G loss: 3.446454]\n",
      "epoch:29 step:22672 [D loss: 0.285215, acc.: 89.84%] [G loss: 2.796700]\n",
      "epoch:29 step:22673 [D loss: 0.352719, acc.: 83.59%] [G loss: 2.293032]\n",
      "epoch:29 step:22674 [D loss: 0.250008, acc.: 90.62%] [G loss: 3.420026]\n",
      "epoch:29 step:22675 [D loss: 0.360814, acc.: 82.03%] [G loss: 3.909696]\n",
      "epoch:29 step:22676 [D loss: 0.340651, acc.: 85.16%] [G loss: 2.385382]\n",
      "epoch:29 step:22677 [D loss: 0.294595, acc.: 85.94%] [G loss: 3.020393]\n",
      "epoch:29 step:22678 [D loss: 0.272386, acc.: 85.94%] [G loss: 3.906898]\n",
      "epoch:29 step:22679 [D loss: 0.216481, acc.: 90.62%] [G loss: 3.267776]\n",
      "epoch:29 step:22680 [D loss: 0.358330, acc.: 81.25%] [G loss: 2.840258]\n",
      "epoch:29 step:22681 [D loss: 0.327356, acc.: 85.16%] [G loss: 2.442819]\n",
      "epoch:29 step:22682 [D loss: 0.312292, acc.: 89.06%] [G loss: 3.463514]\n",
      "epoch:29 step:22683 [D loss: 0.395081, acc.: 81.25%] [G loss: 3.434150]\n",
      "epoch:29 step:22684 [D loss: 0.285008, acc.: 90.62%] [G loss: 2.370893]\n",
      "epoch:29 step:22685 [D loss: 0.274970, acc.: 87.50%] [G loss: 3.581070]\n",
      "epoch:29 step:22686 [D loss: 0.234558, acc.: 91.41%] [G loss: 2.967622]\n",
      "epoch:29 step:22687 [D loss: 0.277448, acc.: 87.50%] [G loss: 3.355930]\n",
      "epoch:29 step:22688 [D loss: 0.268979, acc.: 86.72%] [G loss: 3.240241]\n",
      "epoch:29 step:22689 [D loss: 0.311550, acc.: 87.50%] [G loss: 2.743425]\n",
      "epoch:29 step:22690 [D loss: 0.297845, acc.: 88.28%] [G loss: 2.643414]\n",
      "epoch:29 step:22691 [D loss: 0.261946, acc.: 89.06%] [G loss: 2.802315]\n",
      "epoch:29 step:22692 [D loss: 0.353351, acc.: 84.38%] [G loss: 3.370683]\n",
      "epoch:29 step:22693 [D loss: 0.337962, acc.: 85.16%] [G loss: 3.022719]\n",
      "epoch:29 step:22694 [D loss: 0.254126, acc.: 90.62%] [G loss: 3.054259]\n",
      "epoch:29 step:22695 [D loss: 0.304339, acc.: 84.38%] [G loss: 2.524379]\n",
      "epoch:29 step:22696 [D loss: 0.315991, acc.: 86.72%] [G loss: 3.162720]\n",
      "epoch:29 step:22697 [D loss: 0.399409, acc.: 82.03%] [G loss: 2.766206]\n",
      "epoch:29 step:22698 [D loss: 0.334036, acc.: 85.16%] [G loss: 2.746555]\n",
      "epoch:29 step:22699 [D loss: 0.356349, acc.: 82.81%] [G loss: 4.484451]\n",
      "epoch:29 step:22700 [D loss: 0.404150, acc.: 80.47%] [G loss: 4.001867]\n",
      "epoch:29 step:22701 [D loss: 0.474489, acc.: 81.25%] [G loss: 3.457018]\n",
      "epoch:29 step:22702 [D loss: 0.316124, acc.: 84.38%] [G loss: 3.270800]\n",
      "epoch:29 step:22703 [D loss: 0.287828, acc.: 89.84%] [G loss: 3.672653]\n",
      "epoch:29 step:22704 [D loss: 0.298453, acc.: 84.38%] [G loss: 3.020595]\n",
      "epoch:29 step:22705 [D loss: 0.280414, acc.: 88.28%] [G loss: 3.039026]\n",
      "epoch:29 step:22706 [D loss: 0.339982, acc.: 87.50%] [G loss: 3.400627]\n",
      "epoch:29 step:22707 [D loss: 0.181951, acc.: 96.09%] [G loss: 3.416668]\n",
      "epoch:29 step:22708 [D loss: 0.277685, acc.: 86.72%] [G loss: 3.981108]\n",
      "epoch:29 step:22709 [D loss: 0.232235, acc.: 89.84%] [G loss: 4.383245]\n",
      "epoch:29 step:22710 [D loss: 0.249179, acc.: 89.06%] [G loss: 4.578222]\n",
      "epoch:29 step:22711 [D loss: 0.309567, acc.: 87.50%] [G loss: 3.360911]\n",
      "epoch:29 step:22712 [D loss: 0.252136, acc.: 89.06%] [G loss: 3.597503]\n",
      "epoch:29 step:22713 [D loss: 0.211453, acc.: 92.97%] [G loss: 4.660395]\n",
      "epoch:29 step:22714 [D loss: 0.346924, acc.: 82.81%] [G loss: 4.834532]\n",
      "epoch:29 step:22715 [D loss: 0.285112, acc.: 89.06%] [G loss: 3.209446]\n",
      "epoch:29 step:22716 [D loss: 0.298060, acc.: 86.72%] [G loss: 3.995737]\n",
      "epoch:29 step:22717 [D loss: 0.338128, acc.: 84.38%] [G loss: 3.913416]\n",
      "epoch:29 step:22718 [D loss: 0.268894, acc.: 91.41%] [G loss: 2.294757]\n",
      "epoch:29 step:22719 [D loss: 0.347945, acc.: 86.72%] [G loss: 3.174455]\n",
      "epoch:29 step:22720 [D loss: 0.368073, acc.: 84.38%] [G loss: 2.963294]\n",
      "epoch:29 step:22721 [D loss: 0.357147, acc.: 85.16%] [G loss: 2.767488]\n",
      "epoch:29 step:22722 [D loss: 0.383335, acc.: 83.59%] [G loss: 2.573587]\n",
      "epoch:29 step:22723 [D loss: 0.364386, acc.: 82.81%] [G loss: 3.423758]\n",
      "epoch:29 step:22724 [D loss: 0.399825, acc.: 85.16%] [G loss: 2.746189]\n",
      "epoch:29 step:22725 [D loss: 0.315796, acc.: 86.72%] [G loss: 3.648970]\n",
      "epoch:29 step:22726 [D loss: 0.238341, acc.: 89.84%] [G loss: 3.798089]\n",
      "epoch:29 step:22727 [D loss: 0.317603, acc.: 88.28%] [G loss: 2.724531]\n",
      "epoch:29 step:22728 [D loss: 0.344046, acc.: 85.94%] [G loss: 2.900728]\n",
      "epoch:29 step:22729 [D loss: 0.300842, acc.: 84.38%] [G loss: 3.185000]\n",
      "epoch:29 step:22730 [D loss: 0.331855, acc.: 83.59%] [G loss: 2.849015]\n",
      "epoch:29 step:22731 [D loss: 0.360625, acc.: 88.28%] [G loss: 3.284422]\n",
      "epoch:29 step:22732 [D loss: 0.230606, acc.: 90.62%] [G loss: 2.986067]\n",
      "epoch:29 step:22733 [D loss: 0.310570, acc.: 86.72%] [G loss: 4.129546]\n",
      "epoch:29 step:22734 [D loss: 0.290683, acc.: 90.62%] [G loss: 4.284276]\n",
      "epoch:29 step:22735 [D loss: 0.272007, acc.: 89.06%] [G loss: 3.337904]\n",
      "epoch:29 step:22736 [D loss: 0.382183, acc.: 82.81%] [G loss: 3.246420]\n",
      "epoch:29 step:22737 [D loss: 0.275283, acc.: 88.28%] [G loss: 3.012758]\n",
      "epoch:29 step:22738 [D loss: 0.304400, acc.: 85.94%] [G loss: 2.961328]\n",
      "epoch:29 step:22739 [D loss: 0.327450, acc.: 85.94%] [G loss: 3.018921]\n",
      "epoch:29 step:22740 [D loss: 0.310357, acc.: 85.94%] [G loss: 2.438519]\n",
      "epoch:29 step:22741 [D loss: 0.297878, acc.: 89.06%] [G loss: 4.568749]\n",
      "epoch:29 step:22742 [D loss: 0.442626, acc.: 82.03%] [G loss: 4.243742]\n",
      "epoch:29 step:22743 [D loss: 0.355633, acc.: 84.38%] [G loss: 2.185712]\n",
      "epoch:29 step:22744 [D loss: 0.282045, acc.: 86.72%] [G loss: 3.454173]\n",
      "epoch:29 step:22745 [D loss: 0.225441, acc.: 89.84%] [G loss: 4.525957]\n",
      "epoch:29 step:22746 [D loss: 0.339644, acc.: 85.16%] [G loss: 2.960302]\n",
      "epoch:29 step:22747 [D loss: 0.290900, acc.: 86.72%] [G loss: 3.528434]\n",
      "epoch:29 step:22748 [D loss: 0.270280, acc.: 89.06%] [G loss: 3.997161]\n",
      "epoch:29 step:22749 [D loss: 0.222706, acc.: 88.28%] [G loss: 3.059752]\n",
      "epoch:29 step:22750 [D loss: 0.302506, acc.: 85.16%] [G loss: 4.334486]\n",
      "epoch:29 step:22751 [D loss: 0.432609, acc.: 82.81%] [G loss: 5.022195]\n",
      "epoch:29 step:22752 [D loss: 0.446561, acc.: 78.91%] [G loss: 4.725494]\n",
      "epoch:29 step:22753 [D loss: 0.604327, acc.: 71.88%] [G loss: 5.360951]\n",
      "epoch:29 step:22754 [D loss: 1.079394, acc.: 71.09%] [G loss: 8.261772]\n",
      "epoch:29 step:22755 [D loss: 1.857517, acc.: 54.69%] [G loss: 6.914642]\n",
      "epoch:29 step:22756 [D loss: 0.763013, acc.: 71.09%] [G loss: 3.668117]\n",
      "epoch:29 step:22757 [D loss: 0.760630, acc.: 69.53%] [G loss: 4.390526]\n",
      "epoch:29 step:22758 [D loss: 0.487365, acc.: 82.81%] [G loss: 4.958466]\n",
      "epoch:29 step:22759 [D loss: 0.318826, acc.: 84.38%] [G loss: 5.000903]\n",
      "epoch:29 step:22760 [D loss: 0.516879, acc.: 75.78%] [G loss: 3.509295]\n",
      "epoch:29 step:22761 [D loss: 0.298130, acc.: 86.72%] [G loss: 3.664273]\n",
      "epoch:29 step:22762 [D loss: 0.379638, acc.: 84.38%] [G loss: 2.582005]\n",
      "epoch:29 step:22763 [D loss: 0.268512, acc.: 85.94%] [G loss: 3.259867]\n",
      "epoch:29 step:22764 [D loss: 0.358408, acc.: 81.25%] [G loss: 2.886587]\n",
      "epoch:29 step:22765 [D loss: 0.303664, acc.: 84.38%] [G loss: 2.601864]\n",
      "epoch:29 step:22766 [D loss: 0.315079, acc.: 87.50%] [G loss: 3.019485]\n",
      "epoch:29 step:22767 [D loss: 0.311637, acc.: 88.28%] [G loss: 3.916285]\n",
      "epoch:29 step:22768 [D loss: 0.168011, acc.: 92.97%] [G loss: 4.576441]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:29 step:22769 [D loss: 0.368624, acc.: 81.25%] [G loss: 3.411423]\n",
      "epoch:29 step:22770 [D loss: 0.261433, acc.: 86.72%] [G loss: 4.007801]\n",
      "epoch:29 step:22771 [D loss: 0.346473, acc.: 82.81%] [G loss: 2.942169]\n",
      "epoch:29 step:22772 [D loss: 0.304925, acc.: 85.94%] [G loss: 3.856503]\n",
      "epoch:29 step:22773 [D loss: 0.318409, acc.: 89.84%] [G loss: 2.962978]\n",
      "epoch:29 step:22774 [D loss: 0.296985, acc.: 85.16%] [G loss: 2.173989]\n",
      "epoch:29 step:22775 [D loss: 0.283310, acc.: 87.50%] [G loss: 2.197015]\n",
      "epoch:29 step:22776 [D loss: 0.299552, acc.: 86.72%] [G loss: 2.557227]\n",
      "epoch:29 step:22777 [D loss: 0.253598, acc.: 89.84%] [G loss: 2.667236]\n",
      "epoch:29 step:22778 [D loss: 0.278548, acc.: 89.84%] [G loss: 2.678945]\n",
      "epoch:29 step:22779 [D loss: 0.348693, acc.: 80.47%] [G loss: 2.426869]\n",
      "epoch:29 step:22780 [D loss: 0.299804, acc.: 82.03%] [G loss: 2.857476]\n",
      "epoch:29 step:22781 [D loss: 0.260315, acc.: 89.06%] [G loss: 3.355284]\n",
      "epoch:29 step:22782 [D loss: 0.313232, acc.: 87.50%] [G loss: 2.691016]\n",
      "epoch:29 step:22783 [D loss: 0.355683, acc.: 85.94%] [G loss: 2.718066]\n",
      "epoch:29 step:22784 [D loss: 0.264464, acc.: 87.50%] [G loss: 2.954284]\n",
      "epoch:29 step:22785 [D loss: 0.252938, acc.: 91.41%] [G loss: 3.273912]\n",
      "epoch:29 step:22786 [D loss: 0.277756, acc.: 88.28%] [G loss: 3.583272]\n",
      "epoch:29 step:22787 [D loss: 0.264631, acc.: 89.06%] [G loss: 3.490402]\n",
      "epoch:29 step:22788 [D loss: 0.264136, acc.: 85.16%] [G loss: 2.453413]\n",
      "epoch:29 step:22789 [D loss: 0.273961, acc.: 90.62%] [G loss: 2.839314]\n",
      "epoch:29 step:22790 [D loss: 0.243519, acc.: 89.84%] [G loss: 2.683962]\n",
      "epoch:29 step:22791 [D loss: 0.315631, acc.: 86.72%] [G loss: 2.977899]\n",
      "epoch:29 step:22792 [D loss: 0.364807, acc.: 85.16%] [G loss: 3.158632]\n",
      "epoch:29 step:22793 [D loss: 0.297370, acc.: 88.28%] [G loss: 3.246503]\n",
      "epoch:29 step:22794 [D loss: 0.211448, acc.: 89.84%] [G loss: 3.161912]\n",
      "epoch:29 step:22795 [D loss: 0.358873, acc.: 86.72%] [G loss: 3.030273]\n",
      "epoch:29 step:22796 [D loss: 0.368625, acc.: 83.59%] [G loss: 3.328883]\n",
      "epoch:29 step:22797 [D loss: 0.260477, acc.: 89.06%] [G loss: 2.739103]\n",
      "epoch:29 step:22798 [D loss: 0.268749, acc.: 90.62%] [G loss: 3.340567]\n",
      "epoch:29 step:22799 [D loss: 0.208797, acc.: 92.19%] [G loss: 3.455076]\n",
      "epoch:29 step:22800 [D loss: 0.213184, acc.: 89.84%] [G loss: 4.496757]\n",
      "##############\n",
      "[0.85834426 0.84943022 0.8178212  0.81538624 0.78319312 0.82833524\n",
      " 0.87477787 0.82872667 0.80725644 0.80756901]\n",
      "##########\n",
      "epoch:29 step:22801 [D loss: 0.284961, acc.: 89.06%] [G loss: 4.185374]\n",
      "epoch:29 step:22802 [D loss: 0.214892, acc.: 91.41%] [G loss: 3.460442]\n",
      "epoch:29 step:22803 [D loss: 0.251384, acc.: 90.62%] [G loss: 5.033216]\n",
      "epoch:29 step:22804 [D loss: 0.222245, acc.: 92.97%] [G loss: 4.900666]\n",
      "epoch:29 step:22805 [D loss: 0.255157, acc.: 89.06%] [G loss: 3.716765]\n",
      "epoch:29 step:22806 [D loss: 0.354509, acc.: 85.16%] [G loss: 4.723903]\n",
      "epoch:29 step:22807 [D loss: 0.387753, acc.: 80.47%] [G loss: 3.367259]\n",
      "epoch:29 step:22808 [D loss: 0.300646, acc.: 85.16%] [G loss: 3.269850]\n",
      "epoch:29 step:22809 [D loss: 0.307157, acc.: 85.94%] [G loss: 2.954614]\n",
      "epoch:29 step:22810 [D loss: 0.370945, acc.: 82.03%] [G loss: 2.598072]\n",
      "epoch:29 step:22811 [D loss: 0.308429, acc.: 88.28%] [G loss: 3.270736]\n",
      "epoch:29 step:22812 [D loss: 0.248210, acc.: 89.84%] [G loss: 3.059135]\n",
      "epoch:29 step:22813 [D loss: 0.268766, acc.: 89.06%] [G loss: 2.807242]\n",
      "epoch:29 step:22814 [D loss: 0.346602, acc.: 85.94%] [G loss: 2.503296]\n",
      "epoch:29 step:22815 [D loss: 0.275493, acc.: 88.28%] [G loss: 2.702105]\n",
      "epoch:29 step:22816 [D loss: 0.321338, acc.: 85.94%] [G loss: 2.226322]\n",
      "epoch:29 step:22817 [D loss: 0.319016, acc.: 85.94%] [G loss: 2.743238]\n",
      "epoch:29 step:22818 [D loss: 0.308523, acc.: 86.72%] [G loss: 2.517103]\n",
      "epoch:29 step:22819 [D loss: 0.325010, acc.: 82.81%] [G loss: 2.423025]\n",
      "epoch:29 step:22820 [D loss: 0.314376, acc.: 83.59%] [G loss: 2.736444]\n",
      "epoch:29 step:22821 [D loss: 0.320743, acc.: 86.72%] [G loss: 2.693159]\n",
      "epoch:29 step:22822 [D loss: 0.405743, acc.: 82.81%] [G loss: 3.832155]\n",
      "epoch:29 step:22823 [D loss: 0.327713, acc.: 85.94%] [G loss: 4.598796]\n",
      "epoch:29 step:22824 [D loss: 0.225295, acc.: 90.62%] [G loss: 4.341697]\n",
      "epoch:29 step:22825 [D loss: 0.236370, acc.: 91.41%] [G loss: 5.180906]\n",
      "epoch:29 step:22826 [D loss: 0.206658, acc.: 92.97%] [G loss: 5.386807]\n",
      "epoch:29 step:22827 [D loss: 0.273537, acc.: 86.72%] [G loss: 3.794765]\n",
      "epoch:29 step:22828 [D loss: 0.229280, acc.: 87.50%] [G loss: 3.260670]\n",
      "epoch:29 step:22829 [D loss: 0.319880, acc.: 85.94%] [G loss: 2.740898]\n",
      "epoch:29 step:22830 [D loss: 0.326920, acc.: 83.59%] [G loss: 3.355151]\n",
      "epoch:29 step:22831 [D loss: 0.349717, acc.: 85.16%] [G loss: 2.773248]\n",
      "epoch:29 step:22832 [D loss: 0.310135, acc.: 85.16%] [G loss: 3.151824]\n",
      "epoch:29 step:22833 [D loss: 0.334743, acc.: 85.94%] [G loss: 2.994661]\n",
      "epoch:29 step:22834 [D loss: 0.228791, acc.: 91.41%] [G loss: 3.351546]\n",
      "epoch:29 step:22835 [D loss: 0.308414, acc.: 83.59%] [G loss: 2.793637]\n",
      "epoch:29 step:22836 [D loss: 0.307491, acc.: 88.28%] [G loss: 4.706376]\n",
      "epoch:29 step:22837 [D loss: 0.271214, acc.: 89.06%] [G loss: 3.038862]\n",
      "epoch:29 step:22838 [D loss: 0.375572, acc.: 83.59%] [G loss: 3.080896]\n",
      "epoch:29 step:22839 [D loss: 0.323061, acc.: 85.94%] [G loss: 3.624503]\n",
      "epoch:29 step:22840 [D loss: 0.352742, acc.: 83.59%] [G loss: 3.599550]\n",
      "epoch:29 step:22841 [D loss: 0.338164, acc.: 87.50%] [G loss: 3.402906]\n",
      "epoch:29 step:22842 [D loss: 0.450210, acc.: 80.47%] [G loss: 2.741161]\n",
      "epoch:29 step:22843 [D loss: 0.272438, acc.: 89.06%] [G loss: 2.649258]\n",
      "epoch:29 step:22844 [D loss: 0.348763, acc.: 83.59%] [G loss: 2.851904]\n",
      "epoch:29 step:22845 [D loss: 0.291385, acc.: 89.84%] [G loss: 4.230718]\n",
      "epoch:29 step:22846 [D loss: 0.515617, acc.: 80.47%] [G loss: 3.256312]\n",
      "epoch:29 step:22847 [D loss: 0.352320, acc.: 83.59%] [G loss: 3.013092]\n",
      "epoch:29 step:22848 [D loss: 0.303482, acc.: 89.06%] [G loss: 2.675732]\n",
      "epoch:29 step:22849 [D loss: 0.388278, acc.: 81.25%] [G loss: 3.104434]\n",
      "epoch:29 step:22850 [D loss: 0.369831, acc.: 84.38%] [G loss: 3.797237]\n",
      "epoch:29 step:22851 [D loss: 0.406364, acc.: 82.81%] [G loss: 5.522279]\n",
      "epoch:29 step:22852 [D loss: 0.364359, acc.: 83.59%] [G loss: 3.629187]\n",
      "epoch:29 step:22853 [D loss: 0.273569, acc.: 89.84%] [G loss: 3.295778]\n",
      "epoch:29 step:22854 [D loss: 0.337555, acc.: 84.38%] [G loss: 4.009109]\n",
      "epoch:29 step:22855 [D loss: 0.351732, acc.: 86.72%] [G loss: 4.677795]\n",
      "epoch:29 step:22856 [D loss: 0.350334, acc.: 85.16%] [G loss: 7.516204]\n",
      "epoch:29 step:22857 [D loss: 0.410440, acc.: 79.69%] [G loss: 4.460439]\n",
      "epoch:29 step:22858 [D loss: 0.348159, acc.: 82.03%] [G loss: 4.163211]\n",
      "epoch:29 step:22859 [D loss: 0.319846, acc.: 84.38%] [G loss: 4.152758]\n",
      "epoch:29 step:22860 [D loss: 0.316412, acc.: 89.06%] [G loss: 4.261919]\n",
      "epoch:29 step:22861 [D loss: 0.357113, acc.: 82.81%] [G loss: 3.404260]\n",
      "epoch:29 step:22862 [D loss: 0.325904, acc.: 83.59%] [G loss: 4.359584]\n",
      "epoch:29 step:22863 [D loss: 0.302698, acc.: 87.50%] [G loss: 3.034773]\n",
      "epoch:29 step:22864 [D loss: 0.258095, acc.: 87.50%] [G loss: 3.384768]\n",
      "epoch:29 step:22865 [D loss: 0.238087, acc.: 86.72%] [G loss: 3.658243]\n",
      "epoch:29 step:22866 [D loss: 0.332221, acc.: 82.81%] [G loss: 3.186565]\n",
      "epoch:29 step:22867 [D loss: 0.267740, acc.: 89.06%] [G loss: 3.234075]\n",
      "epoch:29 step:22868 [D loss: 0.287245, acc.: 86.72%] [G loss: 2.855733]\n",
      "epoch:29 step:22869 [D loss: 0.468396, acc.: 78.12%] [G loss: 3.998501]\n",
      "epoch:29 step:22870 [D loss: 0.231923, acc.: 92.19%] [G loss: 5.898273]\n",
      "epoch:29 step:22871 [D loss: 0.242996, acc.: 85.94%] [G loss: 3.912959]\n",
      "epoch:29 step:22872 [D loss: 0.281182, acc.: 88.28%] [G loss: 3.770585]\n",
      "epoch:29 step:22873 [D loss: 0.424683, acc.: 79.69%] [G loss: 3.230168]\n",
      "epoch:29 step:22874 [D loss: 0.379150, acc.: 82.81%] [G loss: 4.320102]\n",
      "epoch:29 step:22875 [D loss: 0.279278, acc.: 86.72%] [G loss: 2.990979]\n",
      "epoch:29 step:22876 [D loss: 0.244007, acc.: 87.50%] [G loss: 3.377537]\n",
      "epoch:29 step:22877 [D loss: 0.356934, acc.: 80.47%] [G loss: 4.313869]\n",
      "epoch:29 step:22878 [D loss: 0.394580, acc.: 80.47%] [G loss: 5.419528]\n",
      "epoch:29 step:22879 [D loss: 0.639263, acc.: 67.97%] [G loss: 3.936389]\n",
      "epoch:29 step:22880 [D loss: 0.261001, acc.: 89.84%] [G loss: 4.160995]\n",
      "epoch:29 step:22881 [D loss: 0.311199, acc.: 89.06%] [G loss: 3.624274]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:29 step:22882 [D loss: 0.376741, acc.: 80.47%] [G loss: 3.727892]\n",
      "epoch:29 step:22883 [D loss: 0.257291, acc.: 87.50%] [G loss: 4.991857]\n",
      "epoch:29 step:22884 [D loss: 0.349552, acc.: 82.03%] [G loss: 4.478302]\n",
      "epoch:29 step:22885 [D loss: 0.357094, acc.: 83.59%] [G loss: 3.593385]\n",
      "epoch:29 step:22886 [D loss: 0.299329, acc.: 86.72%] [G loss: 3.674041]\n",
      "epoch:29 step:22887 [D loss: 0.365592, acc.: 86.72%] [G loss: 3.287217]\n",
      "epoch:29 step:22888 [D loss: 0.353018, acc.: 82.03%] [G loss: 3.469151]\n",
      "epoch:29 step:22889 [D loss: 0.381370, acc.: 78.91%] [G loss: 3.802038]\n",
      "epoch:29 step:22890 [D loss: 0.298860, acc.: 87.50%] [G loss: 3.865724]\n",
      "epoch:29 step:22891 [D loss: 0.406794, acc.: 82.03%] [G loss: 2.891997]\n",
      "epoch:29 step:22892 [D loss: 0.295916, acc.: 86.72%] [G loss: 3.558540]\n",
      "epoch:29 step:22893 [D loss: 0.415252, acc.: 79.69%] [G loss: 2.978242]\n",
      "epoch:29 step:22894 [D loss: 0.359006, acc.: 82.81%] [G loss: 5.595319]\n",
      "epoch:29 step:22895 [D loss: 0.377958, acc.: 81.25%] [G loss: 4.519704]\n",
      "epoch:29 step:22896 [D loss: 0.363828, acc.: 85.94%] [G loss: 4.641061]\n",
      "epoch:29 step:22897 [D loss: 0.296638, acc.: 85.94%] [G loss: 2.657288]\n",
      "epoch:29 step:22898 [D loss: 0.241774, acc.: 89.84%] [G loss: 3.906979]\n",
      "epoch:29 step:22899 [D loss: 0.450936, acc.: 82.81%] [G loss: 3.714987]\n",
      "epoch:29 step:22900 [D loss: 0.230631, acc.: 90.62%] [G loss: 4.573192]\n",
      "epoch:29 step:22901 [D loss: 0.341021, acc.: 83.59%] [G loss: 5.423167]\n",
      "epoch:29 step:22902 [D loss: 0.226476, acc.: 88.28%] [G loss: 3.535625]\n",
      "epoch:29 step:22903 [D loss: 0.229837, acc.: 90.62%] [G loss: 3.488544]\n",
      "epoch:29 step:22904 [D loss: 0.285219, acc.: 85.94%] [G loss: 3.353189]\n",
      "epoch:29 step:22905 [D loss: 0.277946, acc.: 84.38%] [G loss: 2.909796]\n",
      "epoch:29 step:22906 [D loss: 0.398925, acc.: 84.38%] [G loss: 3.493501]\n",
      "epoch:29 step:22907 [D loss: 0.282221, acc.: 88.28%] [G loss: 2.463499]\n",
      "epoch:29 step:22908 [D loss: 0.393336, acc.: 82.03%] [G loss: 2.750509]\n",
      "epoch:29 step:22909 [D loss: 0.316166, acc.: 85.16%] [G loss: 2.701359]\n",
      "epoch:29 step:22910 [D loss: 0.319206, acc.: 85.94%] [G loss: 2.678807]\n",
      "epoch:29 step:22911 [D loss: 0.340761, acc.: 85.16%] [G loss: 2.834671]\n",
      "epoch:29 step:22912 [D loss: 0.255645, acc.: 89.06%] [G loss: 2.834314]\n",
      "epoch:29 step:22913 [D loss: 0.392782, acc.: 78.91%] [G loss: 3.311090]\n",
      "epoch:29 step:22914 [D loss: 0.394040, acc.: 86.72%] [G loss: 3.188089]\n",
      "epoch:29 step:22915 [D loss: 0.396287, acc.: 81.25%] [G loss: 2.608494]\n",
      "epoch:29 step:22916 [D loss: 0.268450, acc.: 89.06%] [G loss: 4.152586]\n",
      "epoch:29 step:22917 [D loss: 0.323050, acc.: 87.50%] [G loss: 3.008242]\n",
      "epoch:29 step:22918 [D loss: 0.371008, acc.: 84.38%] [G loss: 2.890619]\n",
      "epoch:29 step:22919 [D loss: 0.229171, acc.: 92.19%] [G loss: 3.486743]\n",
      "epoch:29 step:22920 [D loss: 0.370652, acc.: 82.81%] [G loss: 2.851372]\n",
      "epoch:29 step:22921 [D loss: 0.283406, acc.: 85.94%] [G loss: 2.708551]\n",
      "epoch:29 step:22922 [D loss: 0.258259, acc.: 88.28%] [G loss: 3.329262]\n",
      "epoch:29 step:22923 [D loss: 0.408583, acc.: 80.47%] [G loss: 3.167099]\n",
      "epoch:29 step:22924 [D loss: 0.329643, acc.: 85.16%] [G loss: 2.742822]\n",
      "epoch:29 step:22925 [D loss: 0.292836, acc.: 84.38%] [G loss: 2.668203]\n",
      "epoch:29 step:22926 [D loss: 0.343411, acc.: 86.72%] [G loss: 3.144799]\n",
      "epoch:29 step:22927 [D loss: 0.311345, acc.: 85.16%] [G loss: 3.298508]\n",
      "epoch:29 step:22928 [D loss: 0.370835, acc.: 83.59%] [G loss: 5.693156]\n",
      "epoch:29 step:22929 [D loss: 0.326988, acc.: 87.50%] [G loss: 3.972467]\n",
      "epoch:29 step:22930 [D loss: 0.253878, acc.: 88.28%] [G loss: 4.689006]\n",
      "epoch:29 step:22931 [D loss: 0.322127, acc.: 84.38%] [G loss: 2.675752]\n",
      "epoch:29 step:22932 [D loss: 0.328009, acc.: 88.28%] [G loss: 5.559711]\n",
      "epoch:29 step:22933 [D loss: 0.387666, acc.: 82.81%] [G loss: 3.324865]\n",
      "epoch:29 step:22934 [D loss: 0.268667, acc.: 89.84%] [G loss: 3.248634]\n",
      "epoch:29 step:22935 [D loss: 0.361687, acc.: 81.25%] [G loss: 3.778094]\n",
      "epoch:29 step:22936 [D loss: 0.231575, acc.: 93.75%] [G loss: 4.361258]\n",
      "epoch:29 step:22937 [D loss: 0.335638, acc.: 84.38%] [G loss: 3.517984]\n",
      "epoch:29 step:22938 [D loss: 0.290908, acc.: 89.06%] [G loss: 4.253346]\n",
      "epoch:29 step:22939 [D loss: 0.382743, acc.: 82.03%] [G loss: 3.074486]\n",
      "epoch:29 step:22940 [D loss: 0.317549, acc.: 85.16%] [G loss: 3.396448]\n",
      "epoch:29 step:22941 [D loss: 0.325985, acc.: 88.28%] [G loss: 4.813637]\n",
      "epoch:29 step:22942 [D loss: 0.355689, acc.: 85.16%] [G loss: 3.719810]\n",
      "epoch:29 step:22943 [D loss: 0.338620, acc.: 85.94%] [G loss: 3.431624]\n",
      "epoch:29 step:22944 [D loss: 0.320444, acc.: 87.50%] [G loss: 3.290600]\n",
      "epoch:29 step:22945 [D loss: 0.354773, acc.: 80.47%] [G loss: 3.813500]\n",
      "epoch:29 step:22946 [D loss: 0.294656, acc.: 82.03%] [G loss: 3.950955]\n",
      "epoch:29 step:22947 [D loss: 0.355908, acc.: 82.81%] [G loss: 3.456121]\n",
      "epoch:29 step:22948 [D loss: 0.302368, acc.: 85.94%] [G loss: 4.926218]\n",
      "epoch:29 step:22949 [D loss: 0.307954, acc.: 83.59%] [G loss: 4.175306]\n",
      "epoch:29 step:22950 [D loss: 0.321274, acc.: 86.72%] [G loss: 4.279204]\n",
      "epoch:29 step:22951 [D loss: 0.250952, acc.: 87.50%] [G loss: 5.709149]\n",
      "epoch:29 step:22952 [D loss: 0.268661, acc.: 87.50%] [G loss: 4.057556]\n",
      "epoch:29 step:22953 [D loss: 0.300524, acc.: 82.81%] [G loss: 3.264789]\n",
      "epoch:29 step:22954 [D loss: 0.354944, acc.: 83.59%] [G loss: 3.870392]\n",
      "epoch:29 step:22955 [D loss: 0.153301, acc.: 96.09%] [G loss: 3.554329]\n",
      "epoch:29 step:22956 [D loss: 0.436957, acc.: 79.69%] [G loss: 3.437306]\n",
      "epoch:29 step:22957 [D loss: 0.250364, acc.: 86.72%] [G loss: 3.558647]\n",
      "epoch:29 step:22958 [D loss: 0.328928, acc.: 85.16%] [G loss: 2.797132]\n",
      "epoch:29 step:22959 [D loss: 0.251910, acc.: 90.62%] [G loss: 3.303718]\n",
      "epoch:29 step:22960 [D loss: 0.284638, acc.: 88.28%] [G loss: 3.190323]\n",
      "epoch:29 step:22961 [D loss: 0.333554, acc.: 84.38%] [G loss: 2.920188]\n",
      "epoch:29 step:22962 [D loss: 0.322243, acc.: 84.38%] [G loss: 2.987233]\n",
      "epoch:29 step:22963 [D loss: 0.323040, acc.: 83.59%] [G loss: 4.620355]\n",
      "epoch:29 step:22964 [D loss: 0.605834, acc.: 69.53%] [G loss: 5.219397]\n",
      "epoch:29 step:22965 [D loss: 0.458895, acc.: 77.34%] [G loss: 3.685547]\n",
      "epoch:29 step:22966 [D loss: 0.386021, acc.: 82.03%] [G loss: 4.049243]\n",
      "epoch:29 step:22967 [D loss: 0.294765, acc.: 86.72%] [G loss: 3.703339]\n",
      "epoch:29 step:22968 [D loss: 0.305946, acc.: 85.94%] [G loss: 4.882657]\n",
      "epoch:29 step:22969 [D loss: 0.253455, acc.: 89.06%] [G loss: 4.597265]\n",
      "epoch:29 step:22970 [D loss: 0.282397, acc.: 87.50%] [G loss: 3.313555]\n",
      "epoch:29 step:22971 [D loss: 0.286653, acc.: 90.62%] [G loss: 3.342791]\n",
      "epoch:29 step:22972 [D loss: 0.270516, acc.: 88.28%] [G loss: 5.355492]\n",
      "epoch:29 step:22973 [D loss: 0.228373, acc.: 91.41%] [G loss: 3.456335]\n",
      "epoch:29 step:22974 [D loss: 0.314297, acc.: 85.16%] [G loss: 2.895870]\n",
      "epoch:29 step:22975 [D loss: 0.352089, acc.: 84.38%] [G loss: 5.040947]\n",
      "epoch:29 step:22976 [D loss: 0.517119, acc.: 82.03%] [G loss: 4.955180]\n",
      "epoch:29 step:22977 [D loss: 0.468634, acc.: 80.47%] [G loss: 6.735905]\n",
      "epoch:29 step:22978 [D loss: 0.642862, acc.: 81.25%] [G loss: 7.093033]\n",
      "epoch:29 step:22979 [D loss: 0.929846, acc.: 69.53%] [G loss: 4.217134]\n",
      "epoch:29 step:22980 [D loss: 1.109605, acc.: 57.81%] [G loss: 4.581281]\n",
      "epoch:29 step:22981 [D loss: 0.405297, acc.: 81.25%] [G loss: 3.951435]\n",
      "epoch:29 step:22982 [D loss: 0.448123, acc.: 81.25%] [G loss: 5.923149]\n",
      "epoch:29 step:22983 [D loss: 0.504845, acc.: 76.56%] [G loss: 4.777808]\n",
      "epoch:29 step:22984 [D loss: 0.326002, acc.: 85.94%] [G loss: 3.103382]\n",
      "epoch:29 step:22985 [D loss: 0.260891, acc.: 88.28%] [G loss: 4.401177]\n",
      "epoch:29 step:22986 [D loss: 0.305525, acc.: 85.94%] [G loss: 4.046808]\n",
      "epoch:29 step:22987 [D loss: 0.380903, acc.: 85.94%] [G loss: 3.074883]\n",
      "epoch:29 step:22988 [D loss: 0.284894, acc.: 89.06%] [G loss: 3.153199]\n",
      "epoch:29 step:22989 [D loss: 0.390344, acc.: 84.38%] [G loss: 4.037380]\n",
      "epoch:29 step:22990 [D loss: 0.255135, acc.: 89.84%] [G loss: 2.605007]\n",
      "epoch:29 step:22991 [D loss: 0.279789, acc.: 87.50%] [G loss: 2.648387]\n",
      "epoch:29 step:22992 [D loss: 0.355165, acc.: 83.59%] [G loss: 2.733029]\n",
      "epoch:29 step:22993 [D loss: 0.344204, acc.: 85.94%] [G loss: 2.486268]\n",
      "epoch:29 step:22994 [D loss: 0.333007, acc.: 85.16%] [G loss: 2.094390]\n",
      "epoch:29 step:22995 [D loss: 0.329062, acc.: 83.59%] [G loss: 2.878392]\n",
      "epoch:29 step:22996 [D loss: 0.215441, acc.: 92.19%] [G loss: 2.950805]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:29 step:22997 [D loss: 0.286939, acc.: 87.50%] [G loss: 2.628009]\n",
      "epoch:29 step:22998 [D loss: 0.257117, acc.: 92.97%] [G loss: 2.715973]\n",
      "epoch:29 step:22999 [D loss: 0.338180, acc.: 84.38%] [G loss: 2.198109]\n",
      "epoch:29 step:23000 [D loss: 0.236782, acc.: 91.41%] [G loss: 2.870125]\n",
      "##############\n",
      "[0.87384053 0.85693397 0.77311007 0.80901723 0.75022479 0.824966\n",
      " 0.8374465  0.85407173 0.81940337 0.81374043]\n",
      "##########\n",
      "epoch:29 step:23001 [D loss: 0.309550, acc.: 88.28%] [G loss: 3.334414]\n",
      "epoch:29 step:23002 [D loss: 0.300635, acc.: 87.50%] [G loss: 2.546920]\n",
      "epoch:29 step:23003 [D loss: 0.234869, acc.: 91.41%] [G loss: 3.037101]\n",
      "epoch:29 step:23004 [D loss: 0.243763, acc.: 89.06%] [G loss: 3.375812]\n",
      "epoch:29 step:23005 [D loss: 0.341512, acc.: 84.38%] [G loss: 2.350630]\n",
      "epoch:29 step:23006 [D loss: 0.311159, acc.: 84.38%] [G loss: 3.313485]\n",
      "epoch:29 step:23007 [D loss: 0.364793, acc.: 83.59%] [G loss: 2.488255]\n",
      "epoch:29 step:23008 [D loss: 0.236213, acc.: 89.06%] [G loss: 2.906972]\n",
      "epoch:29 step:23009 [D loss: 0.330118, acc.: 84.38%] [G loss: 3.021052]\n",
      "epoch:29 step:23010 [D loss: 0.263735, acc.: 90.62%] [G loss: 3.487195]\n",
      "epoch:29 step:23011 [D loss: 0.398149, acc.: 82.81%] [G loss: 2.531633]\n",
      "epoch:29 step:23012 [D loss: 0.255172, acc.: 92.97%] [G loss: 3.645009]\n",
      "epoch:29 step:23013 [D loss: 0.303895, acc.: 87.50%] [G loss: 2.648188]\n",
      "epoch:29 step:23014 [D loss: 0.332509, acc.: 86.72%] [G loss: 2.847398]\n",
      "epoch:29 step:23015 [D loss: 0.342969, acc.: 85.16%] [G loss: 4.315753]\n",
      "epoch:29 step:23016 [D loss: 0.299781, acc.: 87.50%] [G loss: 3.865903]\n",
      "epoch:29 step:23017 [D loss: 0.379237, acc.: 81.25%] [G loss: 2.398484]\n",
      "epoch:29 step:23018 [D loss: 0.357556, acc.: 82.81%] [G loss: 2.794736]\n",
      "epoch:29 step:23019 [D loss: 0.274593, acc.: 85.16%] [G loss: 2.432024]\n",
      "epoch:29 step:23020 [D loss: 0.414743, acc.: 82.81%] [G loss: 2.891671]\n",
      "epoch:29 step:23021 [D loss: 0.248188, acc.: 90.62%] [G loss: 2.855783]\n",
      "epoch:29 step:23022 [D loss: 0.324452, acc.: 89.06%] [G loss: 3.392709]\n",
      "epoch:29 step:23023 [D loss: 0.308028, acc.: 85.16%] [G loss: 3.425988]\n",
      "epoch:29 step:23024 [D loss: 0.245659, acc.: 90.62%] [G loss: 4.064393]\n",
      "epoch:29 step:23025 [D loss: 0.258937, acc.: 91.41%] [G loss: 3.060383]\n",
      "epoch:29 step:23026 [D loss: 0.315558, acc.: 89.06%] [G loss: 3.396126]\n",
      "epoch:29 step:23027 [D loss: 0.278372, acc.: 87.50%] [G loss: 2.994558]\n",
      "epoch:29 step:23028 [D loss: 0.340155, acc.: 85.94%] [G loss: 3.472605]\n",
      "epoch:29 step:23029 [D loss: 0.276581, acc.: 91.41%] [G loss: 2.797893]\n",
      "epoch:29 step:23030 [D loss: 0.256064, acc.: 89.06%] [G loss: 2.445992]\n",
      "epoch:29 step:23031 [D loss: 0.377724, acc.: 83.59%] [G loss: 3.460188]\n",
      "epoch:29 step:23032 [D loss: 0.254002, acc.: 91.41%] [G loss: 3.156569]\n",
      "epoch:29 step:23033 [D loss: 0.220090, acc.: 86.72%] [G loss: 3.631023]\n",
      "epoch:29 step:23034 [D loss: 0.298640, acc.: 87.50%] [G loss: 3.167111]\n",
      "epoch:29 step:23035 [D loss: 0.246737, acc.: 90.62%] [G loss: 3.734838]\n",
      "epoch:29 step:23036 [D loss: 0.323026, acc.: 82.81%] [G loss: 3.604589]\n",
      "epoch:29 step:23037 [D loss: 0.389141, acc.: 85.16%] [G loss: 4.855136]\n",
      "epoch:29 step:23038 [D loss: 0.302319, acc.: 85.94%] [G loss: 5.821377]\n",
      "epoch:29 step:23039 [D loss: 0.243826, acc.: 88.28%] [G loss: 6.523819]\n",
      "epoch:29 step:23040 [D loss: 0.321195, acc.: 85.16%] [G loss: 3.533836]\n",
      "epoch:29 step:23041 [D loss: 0.221536, acc.: 90.62%] [G loss: 4.956238]\n",
      "epoch:29 step:23042 [D loss: 0.291620, acc.: 86.72%] [G loss: 3.008657]\n",
      "epoch:29 step:23043 [D loss: 0.263378, acc.: 88.28%] [G loss: 3.161595]\n",
      "epoch:29 step:23044 [D loss: 0.262470, acc.: 91.41%] [G loss: 3.871906]\n",
      "epoch:29 step:23045 [D loss: 0.251419, acc.: 90.62%] [G loss: 3.226655]\n",
      "epoch:29 step:23046 [D loss: 0.334904, acc.: 84.38%] [G loss: 2.864439]\n",
      "epoch:29 step:23047 [D loss: 0.425687, acc.: 77.34%] [G loss: 3.040182]\n",
      "epoch:29 step:23048 [D loss: 0.361941, acc.: 82.81%] [G loss: 3.232635]\n",
      "epoch:29 step:23049 [D loss: 0.303167, acc.: 86.72%] [G loss: 2.943289]\n",
      "epoch:29 step:23050 [D loss: 0.336418, acc.: 85.16%] [G loss: 3.700990]\n",
      "epoch:29 step:23051 [D loss: 0.275981, acc.: 88.28%] [G loss: 2.672491]\n",
      "epoch:29 step:23052 [D loss: 0.299243, acc.: 87.50%] [G loss: 4.958125]\n",
      "epoch:29 step:23053 [D loss: 0.320256, acc.: 87.50%] [G loss: 5.269035]\n",
      "epoch:29 step:23054 [D loss: 0.291503, acc.: 86.72%] [G loss: 4.209515]\n",
      "epoch:29 step:23055 [D loss: 0.331080, acc.: 85.16%] [G loss: 4.305745]\n",
      "epoch:29 step:23056 [D loss: 0.302386, acc.: 88.28%] [G loss: 3.519654]\n",
      "epoch:29 step:23057 [D loss: 0.260671, acc.: 88.28%] [G loss: 3.850833]\n",
      "epoch:29 step:23058 [D loss: 0.282028, acc.: 86.72%] [G loss: 2.782539]\n",
      "epoch:29 step:23059 [D loss: 0.370297, acc.: 84.38%] [G loss: 2.891976]\n",
      "epoch:29 step:23060 [D loss: 0.246592, acc.: 89.84%] [G loss: 2.624893]\n",
      "epoch:29 step:23061 [D loss: 0.356103, acc.: 83.59%] [G loss: 2.359809]\n",
      "epoch:29 step:23062 [D loss: 0.238474, acc.: 89.06%] [G loss: 3.182828]\n",
      "epoch:29 step:23063 [D loss: 0.302427, acc.: 87.50%] [G loss: 2.854902]\n",
      "epoch:29 step:23064 [D loss: 0.374567, acc.: 83.59%] [G loss: 2.737713]\n",
      "epoch:29 step:23065 [D loss: 0.247750, acc.: 88.28%] [G loss: 3.288608]\n",
      "epoch:29 step:23066 [D loss: 0.375494, acc.: 82.81%] [G loss: 3.756973]\n",
      "epoch:29 step:23067 [D loss: 0.268821, acc.: 88.28%] [G loss: 3.576142]\n",
      "epoch:29 step:23068 [D loss: 0.346165, acc.: 85.16%] [G loss: 2.422420]\n",
      "epoch:29 step:23069 [D loss: 0.332725, acc.: 84.38%] [G loss: 2.502618]\n",
      "epoch:29 step:23070 [D loss: 0.319203, acc.: 82.81%] [G loss: 3.691887]\n",
      "epoch:29 step:23071 [D loss: 0.251874, acc.: 87.50%] [G loss: 3.496475]\n",
      "epoch:29 step:23072 [D loss: 0.307446, acc.: 84.38%] [G loss: 3.733229]\n",
      "epoch:29 step:23073 [D loss: 0.187958, acc.: 93.75%] [G loss: 4.600819]\n",
      "epoch:29 step:23074 [D loss: 0.274946, acc.: 88.28%] [G loss: 3.201379]\n",
      "epoch:29 step:23075 [D loss: 0.222790, acc.: 91.41%] [G loss: 2.920253]\n",
      "epoch:29 step:23076 [D loss: 0.239592, acc.: 89.84%] [G loss: 3.404602]\n",
      "epoch:29 step:23077 [D loss: 0.330340, acc.: 85.94%] [G loss: 3.056303]\n",
      "epoch:29 step:23078 [D loss: 0.262274, acc.: 88.28%] [G loss: 3.676559]\n",
      "epoch:29 step:23079 [D loss: 0.284796, acc.: 88.28%] [G loss: 3.057500]\n",
      "epoch:29 step:23080 [D loss: 0.254681, acc.: 89.84%] [G loss: 3.052937]\n",
      "epoch:29 step:23081 [D loss: 0.423695, acc.: 77.34%] [G loss: 3.635039]\n",
      "epoch:29 step:23082 [D loss: 0.330524, acc.: 91.41%] [G loss: 3.077484]\n",
      "epoch:29 step:23083 [D loss: 0.381197, acc.: 81.25%] [G loss: 6.034310]\n",
      "epoch:29 step:23084 [D loss: 0.518295, acc.: 82.81%] [G loss: 4.078899]\n",
      "epoch:29 step:23085 [D loss: 0.898797, acc.: 62.50%] [G loss: 7.382262]\n",
      "epoch:29 step:23086 [D loss: 1.575372, acc.: 67.19%] [G loss: 4.958716]\n",
      "epoch:29 step:23087 [D loss: 0.346213, acc.: 82.81%] [G loss: 8.041445]\n",
      "epoch:29 step:23088 [D loss: 0.496665, acc.: 82.81%] [G loss: 3.725794]\n",
      "epoch:29 step:23089 [D loss: 0.216111, acc.: 93.75%] [G loss: 4.280330]\n",
      "epoch:29 step:23090 [D loss: 0.343653, acc.: 84.38%] [G loss: 4.409725]\n",
      "epoch:29 step:23091 [D loss: 0.249228, acc.: 85.94%] [G loss: 3.771815]\n",
      "epoch:29 step:23092 [D loss: 0.258197, acc.: 89.84%] [G loss: 3.902568]\n",
      "epoch:29 step:23093 [D loss: 0.263502, acc.: 86.72%] [G loss: 4.698106]\n",
      "epoch:29 step:23094 [D loss: 0.315387, acc.: 85.94%] [G loss: 3.463788]\n",
      "epoch:29 step:23095 [D loss: 0.199417, acc.: 94.53%] [G loss: 3.255497]\n",
      "epoch:29 step:23096 [D loss: 0.288256, acc.: 85.16%] [G loss: 4.333528]\n",
      "epoch:29 step:23097 [D loss: 0.334568, acc.: 87.50%] [G loss: 3.643575]\n",
      "epoch:29 step:23098 [D loss: 0.354197, acc.: 87.50%] [G loss: 3.834513]\n",
      "epoch:29 step:23099 [D loss: 0.185328, acc.: 92.97%] [G loss: 3.499905]\n",
      "epoch:29 step:23100 [D loss: 0.237532, acc.: 92.19%] [G loss: 3.190727]\n",
      "epoch:29 step:23101 [D loss: 0.328866, acc.: 85.94%] [G loss: 3.392123]\n",
      "epoch:29 step:23102 [D loss: 0.263461, acc.: 89.84%] [G loss: 4.451680]\n",
      "epoch:29 step:23103 [D loss: 0.251404, acc.: 87.50%] [G loss: 3.650967]\n",
      "epoch:29 step:23104 [D loss: 0.253215, acc.: 88.28%] [G loss: 3.567220]\n",
      "epoch:29 step:23105 [D loss: 0.255376, acc.: 88.28%] [G loss: 2.592440]\n",
      "epoch:29 step:23106 [D loss: 0.227879, acc.: 92.97%] [G loss: 4.390564]\n",
      "epoch:29 step:23107 [D loss: 0.226809, acc.: 90.62%] [G loss: 3.439096]\n",
      "epoch:29 step:23108 [D loss: 0.158845, acc.: 92.97%] [G loss: 6.408017]\n",
      "epoch:29 step:23109 [D loss: 0.289845, acc.: 86.72%] [G loss: 2.367724]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:29 step:23110 [D loss: 0.237180, acc.: 87.50%] [G loss: 3.694584]\n",
      "epoch:29 step:23111 [D loss: 0.339343, acc.: 85.16%] [G loss: 3.690160]\n",
      "epoch:29 step:23112 [D loss: 0.245805, acc.: 87.50%] [G loss: 3.725407]\n",
      "epoch:29 step:23113 [D loss: 0.263750, acc.: 85.94%] [G loss: 3.445079]\n",
      "epoch:29 step:23114 [D loss: 0.320914, acc.: 83.59%] [G loss: 3.319118]\n",
      "epoch:29 step:23115 [D loss: 0.307988, acc.: 85.16%] [G loss: 3.465189]\n",
      "epoch:29 step:23116 [D loss: 0.304558, acc.: 85.94%] [G loss: 3.136898]\n",
      "epoch:29 step:23117 [D loss: 0.286665, acc.: 87.50%] [G loss: 2.661977]\n",
      "epoch:29 step:23118 [D loss: 0.328263, acc.: 85.16%] [G loss: 2.351154]\n",
      "epoch:29 step:23119 [D loss: 0.312669, acc.: 89.06%] [G loss: 2.872935]\n",
      "epoch:29 step:23120 [D loss: 0.343658, acc.: 83.59%] [G loss: 2.728688]\n",
      "epoch:29 step:23121 [D loss: 0.388017, acc.: 79.69%] [G loss: 3.056569]\n",
      "epoch:29 step:23122 [D loss: 0.385816, acc.: 81.25%] [G loss: 2.757860]\n",
      "epoch:29 step:23123 [D loss: 0.263434, acc.: 87.50%] [G loss: 2.883447]\n",
      "epoch:29 step:23124 [D loss: 0.349010, acc.: 83.59%] [G loss: 3.157655]\n",
      "epoch:29 step:23125 [D loss: 0.356771, acc.: 80.47%] [G loss: 2.603118]\n",
      "epoch:29 step:23126 [D loss: 0.287401, acc.: 86.72%] [G loss: 3.609772]\n",
      "epoch:29 step:23127 [D loss: 0.270757, acc.: 86.72%] [G loss: 3.631327]\n",
      "epoch:29 step:23128 [D loss: 0.378023, acc.: 82.03%] [G loss: 2.695362]\n",
      "epoch:29 step:23129 [D loss: 0.210353, acc.: 89.84%] [G loss: 3.205884]\n",
      "epoch:29 step:23130 [D loss: 0.293106, acc.: 85.94%] [G loss: 2.577010]\n",
      "epoch:29 step:23131 [D loss: 0.340528, acc.: 86.72%] [G loss: 3.115699]\n",
      "epoch:29 step:23132 [D loss: 0.208116, acc.: 91.41%] [G loss: 3.430423]\n",
      "epoch:29 step:23133 [D loss: 0.383749, acc.: 82.03%] [G loss: 3.113595]\n",
      "epoch:29 step:23134 [D loss: 0.229471, acc.: 92.97%] [G loss: 2.614234]\n",
      "epoch:29 step:23135 [D loss: 0.236586, acc.: 90.62%] [G loss: 3.015881]\n",
      "epoch:29 step:23136 [D loss: 0.311140, acc.: 84.38%] [G loss: 3.762015]\n",
      "epoch:29 step:23137 [D loss: 0.209908, acc.: 88.28%] [G loss: 3.569506]\n",
      "epoch:29 step:23138 [D loss: 0.375712, acc.: 82.03%] [G loss: 4.877203]\n",
      "epoch:29 step:23139 [D loss: 0.249942, acc.: 87.50%] [G loss: 3.859908]\n",
      "epoch:29 step:23140 [D loss: 0.268558, acc.: 89.84%] [G loss: 7.217283]\n",
      "epoch:29 step:23141 [D loss: 0.279819, acc.: 89.06%] [G loss: 6.940315]\n",
      "epoch:29 step:23142 [D loss: 0.281076, acc.: 89.84%] [G loss: 4.036528]\n",
      "epoch:29 step:23143 [D loss: 0.317911, acc.: 86.72%] [G loss: 4.338801]\n",
      "epoch:29 step:23144 [D loss: 0.325305, acc.: 88.28%] [G loss: 3.253035]\n",
      "epoch:29 step:23145 [D loss: 0.223205, acc.: 91.41%] [G loss: 4.033456]\n",
      "epoch:29 step:23146 [D loss: 0.256070, acc.: 88.28%] [G loss: 3.191700]\n",
      "epoch:29 step:23147 [D loss: 0.376873, acc.: 83.59%] [G loss: 3.426247]\n",
      "epoch:29 step:23148 [D loss: 0.219989, acc.: 89.84%] [G loss: 4.576146]\n",
      "epoch:29 step:23149 [D loss: 0.281859, acc.: 88.28%] [G loss: 3.579411]\n",
      "epoch:29 step:23150 [D loss: 0.343236, acc.: 86.72%] [G loss: 2.923059]\n",
      "epoch:29 step:23151 [D loss: 0.331473, acc.: 89.06%] [G loss: 3.048357]\n",
      "epoch:29 step:23152 [D loss: 0.261115, acc.: 88.28%] [G loss: 3.243114]\n",
      "epoch:29 step:23153 [D loss: 0.291639, acc.: 87.50%] [G loss: 3.437522]\n",
      "epoch:29 step:23154 [D loss: 0.299537, acc.: 89.06%] [G loss: 2.678531]\n",
      "epoch:29 step:23155 [D loss: 0.373158, acc.: 83.59%] [G loss: 5.875889]\n",
      "epoch:29 step:23156 [D loss: 0.369739, acc.: 85.94%] [G loss: 3.742485]\n",
      "epoch:29 step:23157 [D loss: 0.258981, acc.: 90.62%] [G loss: 2.626801]\n",
      "epoch:29 step:23158 [D loss: 0.348576, acc.: 85.16%] [G loss: 3.832439]\n",
      "epoch:29 step:23159 [D loss: 0.226400, acc.: 90.62%] [G loss: 5.773580]\n",
      "epoch:29 step:23160 [D loss: 0.244318, acc.: 90.62%] [G loss: 3.210568]\n",
      "epoch:29 step:23161 [D loss: 0.229566, acc.: 92.19%] [G loss: 4.625371]\n",
      "epoch:29 step:23162 [D loss: 0.312782, acc.: 85.94%] [G loss: 3.856133]\n",
      "epoch:29 step:23163 [D loss: 0.319447, acc.: 84.38%] [G loss: 3.411352]\n",
      "epoch:29 step:23164 [D loss: 0.208815, acc.: 92.97%] [G loss: 3.252485]\n",
      "epoch:29 step:23165 [D loss: 0.289420, acc.: 85.94%] [G loss: 2.910269]\n",
      "epoch:29 step:23166 [D loss: 0.299845, acc.: 88.28%] [G loss: 2.953357]\n",
      "epoch:29 step:23167 [D loss: 0.269880, acc.: 90.62%] [G loss: 2.979852]\n",
      "epoch:29 step:23168 [D loss: 0.337762, acc.: 85.94%] [G loss: 3.407114]\n",
      "epoch:29 step:23169 [D loss: 0.512341, acc.: 75.00%] [G loss: 2.295561]\n",
      "epoch:29 step:23170 [D loss: 0.270667, acc.: 90.62%] [G loss: 3.716414]\n",
      "epoch:29 step:23171 [D loss: 0.281555, acc.: 89.06%] [G loss: 2.789558]\n",
      "epoch:29 step:23172 [D loss: 0.384743, acc.: 79.69%] [G loss: 2.818563]\n",
      "epoch:29 step:23173 [D loss: 0.333116, acc.: 82.03%] [G loss: 2.721933]\n",
      "epoch:29 step:23174 [D loss: 0.287198, acc.: 85.94%] [G loss: 2.661256]\n",
      "epoch:29 step:23175 [D loss: 0.405368, acc.: 83.59%] [G loss: 2.864802]\n",
      "epoch:29 step:23176 [D loss: 0.335093, acc.: 85.94%] [G loss: 3.343075]\n",
      "epoch:29 step:23177 [D loss: 0.311788, acc.: 86.72%] [G loss: 3.717226]\n",
      "epoch:29 step:23178 [D loss: 0.386915, acc.: 83.59%] [G loss: 4.528172]\n",
      "epoch:29 step:23179 [D loss: 0.493587, acc.: 81.25%] [G loss: 5.256003]\n",
      "epoch:29 step:23180 [D loss: 0.448927, acc.: 80.47%] [G loss: 3.017289]\n",
      "epoch:29 step:23181 [D loss: 0.301336, acc.: 84.38%] [G loss: 3.886101]\n",
      "epoch:29 step:23182 [D loss: 0.312567, acc.: 86.72%] [G loss: 3.333551]\n",
      "epoch:29 step:23183 [D loss: 0.311931, acc.: 86.72%] [G loss: 3.289992]\n",
      "epoch:29 step:23184 [D loss: 0.281932, acc.: 86.72%] [G loss: 3.253067]\n",
      "epoch:29 step:23185 [D loss: 0.346967, acc.: 82.81%] [G loss: 4.416887]\n",
      "epoch:29 step:23186 [D loss: 0.394907, acc.: 82.03%] [G loss: 5.714768]\n",
      "epoch:29 step:23187 [D loss: 0.486837, acc.: 75.00%] [G loss: 4.274405]\n",
      "epoch:29 step:23188 [D loss: 0.328063, acc.: 86.72%] [G loss: 4.342712]\n",
      "epoch:29 step:23189 [D loss: 0.251907, acc.: 86.72%] [G loss: 5.311763]\n",
      "epoch:29 step:23190 [D loss: 0.266945, acc.: 89.06%] [G loss: 3.172514]\n",
      "epoch:29 step:23191 [D loss: 0.398297, acc.: 81.25%] [G loss: 3.855861]\n",
      "epoch:29 step:23192 [D loss: 0.383689, acc.: 80.47%] [G loss: 2.685548]\n",
      "epoch:29 step:23193 [D loss: 0.357342, acc.: 81.25%] [G loss: 3.511838]\n",
      "epoch:29 step:23194 [D loss: 0.329350, acc.: 85.94%] [G loss: 4.046360]\n",
      "epoch:29 step:23195 [D loss: 0.254299, acc.: 89.84%] [G loss: 4.410547]\n",
      "epoch:29 step:23196 [D loss: 0.324454, acc.: 79.69%] [G loss: 3.012943]\n",
      "epoch:29 step:23197 [D loss: 0.196301, acc.: 91.41%] [G loss: 3.386216]\n",
      "epoch:29 step:23198 [D loss: 0.527908, acc.: 75.00%] [G loss: 3.097786]\n",
      "epoch:29 step:23199 [D loss: 0.292114, acc.: 84.38%] [G loss: 3.491482]\n",
      "epoch:29 step:23200 [D loss: 0.241704, acc.: 89.84%] [G loss: 3.900103]\n",
      "##############\n",
      "[0.85926109 0.84405159 0.7983415  0.77468025 0.75118857 0.83913549\n",
      " 0.86899425 0.82135681 0.82222856 0.84540356]\n",
      "##########\n",
      "epoch:29 step:23201 [D loss: 0.412867, acc.: 83.59%] [G loss: 3.357624]\n",
      "epoch:29 step:23202 [D loss: 0.354814, acc.: 85.94%] [G loss: 3.403269]\n",
      "epoch:29 step:23203 [D loss: 0.439059, acc.: 81.25%] [G loss: 3.384197]\n",
      "epoch:29 step:23204 [D loss: 0.297720, acc.: 87.50%] [G loss: 3.219134]\n",
      "epoch:29 step:23205 [D loss: 0.353347, acc.: 85.94%] [G loss: 3.613565]\n",
      "epoch:29 step:23206 [D loss: 0.277048, acc.: 90.62%] [G loss: 3.438872]\n",
      "epoch:29 step:23207 [D loss: 0.353676, acc.: 83.59%] [G loss: 2.772858]\n",
      "epoch:29 step:23208 [D loss: 0.312825, acc.: 87.50%] [G loss: 2.678438]\n",
      "epoch:29 step:23209 [D loss: 0.430480, acc.: 81.25%] [G loss: 2.988411]\n",
      "epoch:29 step:23210 [D loss: 0.265467, acc.: 92.97%] [G loss: 3.379175]\n",
      "epoch:29 step:23211 [D loss: 0.346664, acc.: 84.38%] [G loss: 2.683679]\n",
      "epoch:29 step:23212 [D loss: 0.298005, acc.: 88.28%] [G loss: 3.517569]\n",
      "epoch:29 step:23213 [D loss: 0.346528, acc.: 85.16%] [G loss: 3.151700]\n",
      "epoch:29 step:23214 [D loss: 0.424188, acc.: 78.12%] [G loss: 3.373794]\n",
      "epoch:29 step:23215 [D loss: 0.360267, acc.: 84.38%] [G loss: 3.202110]\n",
      "epoch:29 step:23216 [D loss: 0.232169, acc.: 89.06%] [G loss: 3.139994]\n",
      "epoch:29 step:23217 [D loss: 0.269970, acc.: 89.84%] [G loss: 3.289582]\n",
      "epoch:29 step:23218 [D loss: 0.294090, acc.: 89.84%] [G loss: 3.268515]\n",
      "epoch:29 step:23219 [D loss: 0.274716, acc.: 88.28%] [G loss: 3.874388]\n",
      "epoch:29 step:23220 [D loss: 0.332937, acc.: 86.72%] [G loss: 3.486783]\n",
      "epoch:29 step:23221 [D loss: 0.394273, acc.: 86.72%] [G loss: 3.128296]\n",
      "epoch:29 step:23222 [D loss: 0.549653, acc.: 75.00%] [G loss: 3.784344]\n",
      "epoch:29 step:23223 [D loss: 0.305838, acc.: 85.16%] [G loss: 4.990619]\n",
      "epoch:29 step:23224 [D loss: 0.336995, acc.: 84.38%] [G loss: 3.402119]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:29 step:23225 [D loss: 0.308958, acc.: 83.59%] [G loss: 3.843303]\n",
      "epoch:29 step:23226 [D loss: 0.273661, acc.: 90.62%] [G loss: 3.681486]\n",
      "epoch:29 step:23227 [D loss: 0.278688, acc.: 84.38%] [G loss: 4.071743]\n",
      "epoch:29 step:23228 [D loss: 0.316974, acc.: 79.69%] [G loss: 3.440615]\n",
      "epoch:29 step:23229 [D loss: 0.286926, acc.: 88.28%] [G loss: 3.466650]\n",
      "epoch:29 step:23230 [D loss: 0.271052, acc.: 85.16%] [G loss: 4.046305]\n",
      "epoch:29 step:23231 [D loss: 0.330689, acc.: 85.94%] [G loss: 2.644673]\n",
      "epoch:29 step:23232 [D loss: 0.285837, acc.: 88.28%] [G loss: 3.337305]\n",
      "epoch:29 step:23233 [D loss: 0.312856, acc.: 85.94%] [G loss: 3.797354]\n",
      "epoch:29 step:23234 [D loss: 0.355305, acc.: 87.50%] [G loss: 3.150038]\n",
      "epoch:29 step:23235 [D loss: 0.339047, acc.: 84.38%] [G loss: 4.204172]\n",
      "epoch:29 step:23236 [D loss: 0.360743, acc.: 86.72%] [G loss: 3.310004]\n",
      "epoch:29 step:23237 [D loss: 0.320438, acc.: 84.38%] [G loss: 2.906750]\n",
      "epoch:29 step:23238 [D loss: 0.400117, acc.: 78.91%] [G loss: 3.281040]\n",
      "epoch:29 step:23239 [D loss: 0.351499, acc.: 80.47%] [G loss: 3.048367]\n",
      "epoch:29 step:23240 [D loss: 0.158077, acc.: 96.09%] [G loss: 4.259390]\n",
      "epoch:29 step:23241 [D loss: 0.171003, acc.: 92.97%] [G loss: 3.709129]\n",
      "epoch:29 step:23242 [D loss: 0.322993, acc.: 86.72%] [G loss: 4.101020]\n",
      "epoch:29 step:23243 [D loss: 0.330575, acc.: 86.72%] [G loss: 3.900385]\n",
      "epoch:29 step:23244 [D loss: 0.284390, acc.: 89.06%] [G loss: 5.409192]\n",
      "epoch:29 step:23245 [D loss: 0.271548, acc.: 88.28%] [G loss: 4.690671]\n",
      "epoch:29 step:23246 [D loss: 0.315229, acc.: 84.38%] [G loss: 4.301928]\n",
      "epoch:29 step:23247 [D loss: 0.354567, acc.: 83.59%] [G loss: 2.758318]\n",
      "epoch:29 step:23248 [D loss: 0.288418, acc.: 89.06%] [G loss: 4.842002]\n",
      "epoch:29 step:23249 [D loss: 0.258640, acc.: 89.06%] [G loss: 3.777406]\n",
      "epoch:29 step:23250 [D loss: 0.418577, acc.: 78.91%] [G loss: 4.231654]\n",
      "epoch:29 step:23251 [D loss: 0.317409, acc.: 85.16%] [G loss: 4.163669]\n",
      "epoch:29 step:23252 [D loss: 0.243135, acc.: 90.62%] [G loss: 9.102799]\n",
      "epoch:29 step:23253 [D loss: 0.363090, acc.: 79.69%] [G loss: 8.057438]\n",
      "epoch:29 step:23254 [D loss: 0.327132, acc.: 84.38%] [G loss: 4.793449]\n",
      "epoch:29 step:23255 [D loss: 0.307674, acc.: 85.16%] [G loss: 3.617630]\n",
      "epoch:29 step:23256 [D loss: 0.293123, acc.: 83.59%] [G loss: 3.025390]\n",
      "epoch:29 step:23257 [D loss: 0.248445, acc.: 87.50%] [G loss: 2.632976]\n",
      "epoch:29 step:23258 [D loss: 0.494233, acc.: 82.03%] [G loss: 2.895710]\n",
      "epoch:29 step:23259 [D loss: 0.265890, acc.: 89.06%] [G loss: 4.773081]\n",
      "epoch:29 step:23260 [D loss: 0.444415, acc.: 77.34%] [G loss: 4.149273]\n",
      "epoch:29 step:23261 [D loss: 0.246235, acc.: 89.06%] [G loss: 5.620539]\n",
      "epoch:29 step:23262 [D loss: 0.454706, acc.: 81.25%] [G loss: 7.694317]\n",
      "epoch:29 step:23263 [D loss: 0.407296, acc.: 80.47%] [G loss: 5.747963]\n",
      "epoch:29 step:23264 [D loss: 0.428074, acc.: 83.59%] [G loss: 5.743605]\n",
      "epoch:29 step:23265 [D loss: 0.396108, acc.: 82.81%] [G loss: 8.191477]\n",
      "epoch:29 step:23266 [D loss: 0.492698, acc.: 74.22%] [G loss: 4.061212]\n",
      "epoch:29 step:23267 [D loss: 0.436379, acc.: 77.34%] [G loss: 3.342584]\n",
      "epoch:29 step:23268 [D loss: 0.366919, acc.: 85.16%] [G loss: 3.576512]\n",
      "epoch:29 step:23269 [D loss: 0.269117, acc.: 87.50%] [G loss: 3.912328]\n",
      "epoch:29 step:23270 [D loss: 0.235438, acc.: 90.62%] [G loss: 4.158347]\n",
      "epoch:29 step:23271 [D loss: 0.411495, acc.: 82.81%] [G loss: 3.495869]\n",
      "epoch:29 step:23272 [D loss: 0.408638, acc.: 83.59%] [G loss: 3.134685]\n",
      "epoch:29 step:23273 [D loss: 0.314532, acc.: 85.94%] [G loss: 2.783864]\n",
      "epoch:29 step:23274 [D loss: 0.298361, acc.: 89.06%] [G loss: 2.953777]\n",
      "epoch:29 step:23275 [D loss: 0.321873, acc.: 86.72%] [G loss: 3.070342]\n",
      "epoch:29 step:23276 [D loss: 0.349130, acc.: 89.06%] [G loss: 3.240645]\n",
      "epoch:29 step:23277 [D loss: 0.380112, acc.: 82.81%] [G loss: 3.307188]\n",
      "epoch:29 step:23278 [D loss: 0.320865, acc.: 82.81%] [G loss: 3.469297]\n",
      "epoch:29 step:23279 [D loss: 0.322032, acc.: 84.38%] [G loss: 3.266258]\n",
      "epoch:29 step:23280 [D loss: 0.316037, acc.: 84.38%] [G loss: 3.902014]\n",
      "epoch:29 step:23281 [D loss: 0.312389, acc.: 85.16%] [G loss: 4.501260]\n",
      "epoch:29 step:23282 [D loss: 0.266912, acc.: 89.06%] [G loss: 3.289987]\n",
      "epoch:29 step:23283 [D loss: 0.356975, acc.: 81.25%] [G loss: 3.104538]\n",
      "epoch:29 step:23284 [D loss: 0.382934, acc.: 84.38%] [G loss: 2.967472]\n",
      "epoch:29 step:23285 [D loss: 0.319350, acc.: 88.28%] [G loss: 2.948459]\n",
      "epoch:29 step:23286 [D loss: 0.343335, acc.: 84.38%] [G loss: 2.840362]\n",
      "epoch:29 step:23287 [D loss: 0.404377, acc.: 81.25%] [G loss: 5.304203]\n",
      "epoch:29 step:23288 [D loss: 0.549321, acc.: 75.00%] [G loss: 4.197678]\n",
      "epoch:29 step:23289 [D loss: 0.432128, acc.: 78.12%] [G loss: 2.863975]\n",
      "epoch:29 step:23290 [D loss: 0.409528, acc.: 79.69%] [G loss: 3.644209]\n",
      "epoch:29 step:23291 [D loss: 0.410637, acc.: 81.25%] [G loss: 3.397463]\n",
      "epoch:29 step:23292 [D loss: 0.410995, acc.: 82.03%] [G loss: 3.126295]\n",
      "epoch:29 step:23293 [D loss: 0.347144, acc.: 85.94%] [G loss: 2.955998]\n",
      "epoch:29 step:23294 [D loss: 0.240891, acc.: 93.75%] [G loss: 3.334979]\n",
      "epoch:29 step:23295 [D loss: 0.409132, acc.: 80.47%] [G loss: 2.972872]\n",
      "epoch:29 step:23296 [D loss: 0.341246, acc.: 84.38%] [G loss: 4.073842]\n",
      "epoch:29 step:23297 [D loss: 0.438121, acc.: 75.78%] [G loss: 7.007080]\n",
      "epoch:29 step:23298 [D loss: 0.510133, acc.: 75.00%] [G loss: 4.933975]\n",
      "epoch:29 step:23299 [D loss: 0.283737, acc.: 85.16%] [G loss: 4.578229]\n",
      "epoch:29 step:23300 [D loss: 0.308348, acc.: 83.59%] [G loss: 3.641208]\n",
      "epoch:29 step:23301 [D loss: 0.296181, acc.: 89.06%] [G loss: 3.134800]\n",
      "epoch:29 step:23302 [D loss: 0.351841, acc.: 85.94%] [G loss: 2.827001]\n",
      "epoch:29 step:23303 [D loss: 0.296863, acc.: 87.50%] [G loss: 3.214530]\n",
      "epoch:29 step:23304 [D loss: 0.256275, acc.: 90.62%] [G loss: 3.479520]\n",
      "epoch:29 step:23305 [D loss: 0.309612, acc.: 85.94%] [G loss: 3.845643]\n",
      "epoch:29 step:23306 [D loss: 0.355211, acc.: 85.16%] [G loss: 5.294829]\n",
      "epoch:29 step:23307 [D loss: 0.359173, acc.: 82.81%] [G loss: 6.058319]\n",
      "epoch:29 step:23308 [D loss: 0.357595, acc.: 82.81%] [G loss: 4.221059]\n",
      "epoch:29 step:23309 [D loss: 0.257220, acc.: 88.28%] [G loss: 3.704190]\n",
      "epoch:29 step:23310 [D loss: 0.338953, acc.: 85.94%] [G loss: 3.729408]\n",
      "epoch:29 step:23311 [D loss: 0.322945, acc.: 79.69%] [G loss: 3.967522]\n",
      "epoch:29 step:23312 [D loss: 0.257034, acc.: 86.72%] [G loss: 3.298888]\n",
      "epoch:29 step:23313 [D loss: 0.406621, acc.: 80.47%] [G loss: 4.526020]\n",
      "epoch:29 step:23314 [D loss: 0.293576, acc.: 89.84%] [G loss: 2.687538]\n",
      "epoch:29 step:23315 [D loss: 0.316820, acc.: 84.38%] [G loss: 4.963616]\n",
      "epoch:29 step:23316 [D loss: 0.277547, acc.: 86.72%] [G loss: 4.175038]\n",
      "epoch:29 step:23317 [D loss: 0.246408, acc.: 92.19%] [G loss: 3.410537]\n",
      "epoch:29 step:23318 [D loss: 0.453907, acc.: 79.69%] [G loss: 2.941486]\n",
      "epoch:29 step:23319 [D loss: 0.271629, acc.: 87.50%] [G loss: 3.240934]\n",
      "epoch:29 step:23320 [D loss: 0.357160, acc.: 85.16%] [G loss: 3.305545]\n",
      "epoch:29 step:23321 [D loss: 0.292341, acc.: 88.28%] [G loss: 4.524589]\n",
      "epoch:29 step:23322 [D loss: 0.283891, acc.: 89.84%] [G loss: 2.902245]\n",
      "epoch:29 step:23323 [D loss: 0.255827, acc.: 90.62%] [G loss: 3.330482]\n",
      "epoch:29 step:23324 [D loss: 0.249473, acc.: 90.62%] [G loss: 3.050959]\n",
      "epoch:29 step:23325 [D loss: 0.310452, acc.: 84.38%] [G loss: 2.534200]\n",
      "epoch:29 step:23326 [D loss: 0.282626, acc.: 85.94%] [G loss: 3.127656]\n",
      "epoch:29 step:23327 [D loss: 0.285987, acc.: 86.72%] [G loss: 3.291643]\n",
      "epoch:29 step:23328 [D loss: 0.279676, acc.: 90.62%] [G loss: 3.460067]\n",
      "epoch:29 step:23329 [D loss: 0.351356, acc.: 83.59%] [G loss: 3.916356]\n",
      "epoch:29 step:23330 [D loss: 0.248605, acc.: 89.84%] [G loss: 5.044082]\n",
      "epoch:29 step:23331 [D loss: 0.337046, acc.: 84.38%] [G loss: 3.814511]\n",
      "epoch:29 step:23332 [D loss: 0.207713, acc.: 92.19%] [G loss: 6.164078]\n",
      "epoch:29 step:23333 [D loss: 0.288822, acc.: 85.16%] [G loss: 4.077519]\n",
      "epoch:29 step:23334 [D loss: 0.247341, acc.: 89.06%] [G loss: 4.730753]\n",
      "epoch:29 step:23335 [D loss: 0.324145, acc.: 85.94%] [G loss: 3.219336]\n",
      "epoch:29 step:23336 [D loss: 0.307195, acc.: 87.50%] [G loss: 3.309319]\n",
      "epoch:29 step:23337 [D loss: 0.273295, acc.: 88.28%] [G loss: 2.988518]\n",
      "epoch:29 step:23338 [D loss: 0.324530, acc.: 85.94%] [G loss: 2.825426]\n",
      "epoch:29 step:23339 [D loss: 0.237934, acc.: 92.19%] [G loss: 3.577686]\n",
      "epoch:29 step:23340 [D loss: 0.243317, acc.: 90.62%] [G loss: 4.961919]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:29 step:23341 [D loss: 0.395271, acc.: 80.47%] [G loss: 4.157324]\n",
      "epoch:29 step:23342 [D loss: 0.320415, acc.: 88.28%] [G loss: 2.858795]\n",
      "epoch:29 step:23343 [D loss: 0.327408, acc.: 87.50%] [G loss: 3.677006]\n",
      "epoch:29 step:23344 [D loss: 0.253335, acc.: 85.94%] [G loss: 3.490218]\n",
      "epoch:29 step:23345 [D loss: 0.229964, acc.: 90.62%] [G loss: 3.566692]\n",
      "epoch:29 step:23346 [D loss: 0.329574, acc.: 85.94%] [G loss: 3.955528]\n",
      "epoch:29 step:23347 [D loss: 0.192268, acc.: 92.97%] [G loss: 3.516655]\n",
      "epoch:29 step:23348 [D loss: 0.253177, acc.: 87.50%] [G loss: 3.181926]\n",
      "epoch:29 step:23349 [D loss: 0.247526, acc.: 88.28%] [G loss: 3.282035]\n",
      "epoch:29 step:23350 [D loss: 0.340415, acc.: 86.72%] [G loss: 3.652673]\n",
      "epoch:29 step:23351 [D loss: 0.290625, acc.: 86.72%] [G loss: 3.015392]\n",
      "epoch:29 step:23352 [D loss: 0.267542, acc.: 87.50%] [G loss: 4.011883]\n",
      "epoch:29 step:23353 [D loss: 0.394847, acc.: 81.25%] [G loss: 3.483425]\n",
      "epoch:29 step:23354 [D loss: 0.280856, acc.: 89.06%] [G loss: 2.983117]\n",
      "epoch:29 step:23355 [D loss: 0.270892, acc.: 88.28%] [G loss: 4.213307]\n",
      "epoch:29 step:23356 [D loss: 0.271498, acc.: 89.06%] [G loss: 2.908660]\n",
      "epoch:29 step:23357 [D loss: 0.280699, acc.: 85.94%] [G loss: 4.311393]\n",
      "epoch:29 step:23358 [D loss: 0.357100, acc.: 82.81%] [G loss: 4.256822]\n",
      "epoch:29 step:23359 [D loss: 0.273133, acc.: 87.50%] [G loss: 3.666124]\n",
      "epoch:29 step:23360 [D loss: 0.332540, acc.: 86.72%] [G loss: 4.748705]\n",
      "epoch:29 step:23361 [D loss: 0.395256, acc.: 80.47%] [G loss: 2.870124]\n",
      "epoch:29 step:23362 [D loss: 0.461785, acc.: 81.25%] [G loss: 3.059030]\n",
      "epoch:29 step:23363 [D loss: 0.301611, acc.: 88.28%] [G loss: 3.768952]\n",
      "epoch:29 step:23364 [D loss: 0.388002, acc.: 82.03%] [G loss: 3.752718]\n",
      "epoch:29 step:23365 [D loss: 0.232926, acc.: 89.84%] [G loss: 4.614542]\n",
      "epoch:29 step:23366 [D loss: 0.330674, acc.: 87.50%] [G loss: 2.879116]\n",
      "epoch:29 step:23367 [D loss: 0.481881, acc.: 78.91%] [G loss: 6.931146]\n",
      "epoch:29 step:23368 [D loss: 0.493295, acc.: 78.91%] [G loss: 3.962435]\n",
      "epoch:29 step:23369 [D loss: 0.227805, acc.: 90.62%] [G loss: 4.406754]\n",
      "epoch:29 step:23370 [D loss: 0.327522, acc.: 81.25%] [G loss: 4.622612]\n",
      "epoch:29 step:23371 [D loss: 0.382605, acc.: 82.03%] [G loss: 3.772321]\n",
      "epoch:29 step:23372 [D loss: 0.226461, acc.: 92.19%] [G loss: 3.140702]\n",
      "epoch:29 step:23373 [D loss: 0.400027, acc.: 82.03%] [G loss: 3.131347]\n",
      "epoch:29 step:23374 [D loss: 0.300430, acc.: 86.72%] [G loss: 3.407309]\n",
      "epoch:29 step:23375 [D loss: 0.273688, acc.: 87.50%] [G loss: 3.737990]\n",
      "epoch:29 step:23376 [D loss: 0.339731, acc.: 82.81%] [G loss: 4.435681]\n",
      "epoch:29 step:23377 [D loss: 0.375594, acc.: 82.03%] [G loss: 4.026440]\n",
      "epoch:29 step:23378 [D loss: 0.382352, acc.: 80.47%] [G loss: 3.232277]\n",
      "epoch:29 step:23379 [D loss: 0.249778, acc.: 90.62%] [G loss: 3.675357]\n",
      "epoch:29 step:23380 [D loss: 0.303374, acc.: 84.38%] [G loss: 3.792443]\n",
      "epoch:29 step:23381 [D loss: 0.335840, acc.: 85.16%] [G loss: 3.097372]\n",
      "epoch:29 step:23382 [D loss: 0.290137, acc.: 86.72%] [G loss: 3.306199]\n",
      "epoch:29 step:23383 [D loss: 0.382408, acc.: 86.72%] [G loss: 2.961003]\n",
      "epoch:29 step:23384 [D loss: 0.273560, acc.: 85.16%] [G loss: 2.425867]\n",
      "epoch:29 step:23385 [D loss: 0.241210, acc.: 90.62%] [G loss: 4.520062]\n",
      "epoch:29 step:23386 [D loss: 0.360971, acc.: 82.03%] [G loss: 3.306441]\n",
      "epoch:29 step:23387 [D loss: 0.289077, acc.: 90.62%] [G loss: 3.933395]\n",
      "epoch:29 step:23388 [D loss: 0.337072, acc.: 85.16%] [G loss: 3.676147]\n",
      "epoch:29 step:23389 [D loss: 0.360400, acc.: 84.38%] [G loss: 3.104806]\n",
      "epoch:29 step:23390 [D loss: 0.332224, acc.: 85.16%] [G loss: 2.733739]\n",
      "epoch:29 step:23391 [D loss: 0.367698, acc.: 82.81%] [G loss: 3.815146]\n",
      "epoch:29 step:23392 [D loss: 0.346461, acc.: 84.38%] [G loss: 4.165962]\n",
      "epoch:29 step:23393 [D loss: 0.295765, acc.: 83.59%] [G loss: 3.449088]\n",
      "epoch:29 step:23394 [D loss: 0.289712, acc.: 84.38%] [G loss: 3.378324]\n",
      "epoch:29 step:23395 [D loss: 0.235963, acc.: 86.72%] [G loss: 3.915547]\n",
      "epoch:29 step:23396 [D loss: 0.203894, acc.: 91.41%] [G loss: 5.882514]\n",
      "epoch:29 step:23397 [D loss: 0.280775, acc.: 86.72%] [G loss: 6.378947]\n",
      "epoch:29 step:23398 [D loss: 0.349217, acc.: 84.38%] [G loss: 2.722764]\n",
      "epoch:29 step:23399 [D loss: 0.497326, acc.: 82.03%] [G loss: 3.608567]\n",
      "epoch:29 step:23400 [D loss: 0.494293, acc.: 82.03%] [G loss: 3.317081]\n",
      "##############\n",
      "[0.8556316  0.8656328  0.8150244  0.78442788 0.76589436 0.81890318\n",
      " 0.85336229 0.83455957 0.81977134 0.81332097]\n",
      "##########\n",
      "epoch:29 step:23401 [D loss: 0.345460, acc.: 85.94%] [G loss: 3.783532]\n",
      "epoch:29 step:23402 [D loss: 0.405749, acc.: 80.47%] [G loss: 3.508265]\n",
      "epoch:29 step:23403 [D loss: 0.377189, acc.: 81.25%] [G loss: 7.387531]\n",
      "epoch:29 step:23404 [D loss: 0.532664, acc.: 77.34%] [G loss: 4.502449]\n",
      "epoch:29 step:23405 [D loss: 0.309403, acc.: 87.50%] [G loss: 5.061421]\n",
      "epoch:29 step:23406 [D loss: 0.290352, acc.: 89.84%] [G loss: 2.928736]\n",
      "epoch:29 step:23407 [D loss: 0.477598, acc.: 76.56%] [G loss: 4.434039]\n",
      "epoch:29 step:23408 [D loss: 0.279993, acc.: 86.72%] [G loss: 3.454734]\n",
      "epoch:29 step:23409 [D loss: 0.332728, acc.: 85.94%] [G loss: 5.084013]\n",
      "epoch:29 step:23410 [D loss: 0.272406, acc.: 86.72%] [G loss: 3.495430]\n",
      "epoch:29 step:23411 [D loss: 0.400284, acc.: 85.16%] [G loss: 3.794189]\n",
      "epoch:29 step:23412 [D loss: 0.308424, acc.: 82.81%] [G loss: 4.082110]\n",
      "epoch:29 step:23413 [D loss: 0.391565, acc.: 78.91%] [G loss: 3.155933]\n",
      "epoch:29 step:23414 [D loss: 0.401319, acc.: 82.03%] [G loss: 5.908028]\n",
      "epoch:29 step:23415 [D loss: 0.390701, acc.: 82.81%] [G loss: 2.848738]\n",
      "epoch:29 step:23416 [D loss: 0.228648, acc.: 90.62%] [G loss: 5.058887]\n",
      "epoch:29 step:23417 [D loss: 0.224917, acc.: 90.62%] [G loss: 2.632153]\n",
      "epoch:29 step:23418 [D loss: 0.277160, acc.: 88.28%] [G loss: 5.047712]\n",
      "epoch:29 step:23419 [D loss: 0.290779, acc.: 87.50%] [G loss: 4.078618]\n",
      "epoch:29 step:23420 [D loss: 0.442697, acc.: 79.69%] [G loss: 4.566073]\n",
      "epoch:29 step:23421 [D loss: 0.351627, acc.: 83.59%] [G loss: 4.647540]\n",
      "epoch:29 step:23422 [D loss: 0.303503, acc.: 87.50%] [G loss: 3.740528]\n",
      "epoch:29 step:23423 [D loss: 0.237125, acc.: 89.84%] [G loss: 2.529306]\n",
      "epoch:29 step:23424 [D loss: 0.269803, acc.: 89.06%] [G loss: 3.737729]\n",
      "epoch:29 step:23425 [D loss: 0.202752, acc.: 91.41%] [G loss: 3.299075]\n",
      "epoch:29 step:23426 [D loss: 0.475567, acc.: 79.69%] [G loss: 2.368924]\n",
      "epoch:29 step:23427 [D loss: 0.346857, acc.: 85.16%] [G loss: 2.700300]\n",
      "epoch:29 step:23428 [D loss: 0.368628, acc.: 83.59%] [G loss: 4.182924]\n",
      "epoch:29 step:23429 [D loss: 0.281370, acc.: 85.16%] [G loss: 4.224988]\n",
      "epoch:29 step:23430 [D loss: 0.308780, acc.: 87.50%] [G loss: 2.954035]\n",
      "epoch:30 step:23431 [D loss: 0.335949, acc.: 82.03%] [G loss: 3.366155]\n",
      "epoch:30 step:23432 [D loss: 0.215395, acc.: 92.19%] [G loss: 3.955117]\n",
      "epoch:30 step:23433 [D loss: 0.466729, acc.: 80.47%] [G loss: 5.405787]\n",
      "epoch:30 step:23434 [D loss: 0.410111, acc.: 82.03%] [G loss: 5.067337]\n",
      "epoch:30 step:23435 [D loss: 0.358917, acc.: 81.25%] [G loss: 5.976020]\n",
      "epoch:30 step:23436 [D loss: 0.239541, acc.: 90.62%] [G loss: 6.488661]\n",
      "epoch:30 step:23437 [D loss: 0.325498, acc.: 86.72%] [G loss: 3.590241]\n",
      "epoch:30 step:23438 [D loss: 0.225802, acc.: 89.84%] [G loss: 4.335065]\n",
      "epoch:30 step:23439 [D loss: 0.202111, acc.: 92.19%] [G loss: 6.613450]\n",
      "epoch:30 step:23440 [D loss: 0.438397, acc.: 77.34%] [G loss: 4.527393]\n",
      "epoch:30 step:23441 [D loss: 0.204629, acc.: 92.19%] [G loss: 4.595486]\n",
      "epoch:30 step:23442 [D loss: 0.316984, acc.: 86.72%] [G loss: 4.210922]\n",
      "epoch:30 step:23443 [D loss: 0.333885, acc.: 85.94%] [G loss: 3.579588]\n",
      "epoch:30 step:23444 [D loss: 0.253785, acc.: 88.28%] [G loss: 4.580283]\n",
      "epoch:30 step:23445 [D loss: 0.276126, acc.: 88.28%] [G loss: 2.902423]\n",
      "epoch:30 step:23446 [D loss: 0.299541, acc.: 84.38%] [G loss: 3.506252]\n",
      "epoch:30 step:23447 [D loss: 0.306020, acc.: 88.28%] [G loss: 3.816762]\n",
      "epoch:30 step:23448 [D loss: 0.359506, acc.: 84.38%] [G loss: 4.239779]\n",
      "epoch:30 step:23449 [D loss: 0.506843, acc.: 74.22%] [G loss: 3.021017]\n",
      "epoch:30 step:23450 [D loss: 0.335418, acc.: 84.38%] [G loss: 2.817289]\n",
      "epoch:30 step:23451 [D loss: 0.452547, acc.: 75.78%] [G loss: 4.658942]\n",
      "epoch:30 step:23452 [D loss: 0.589498, acc.: 67.97%] [G loss: 5.975341]\n",
      "epoch:30 step:23453 [D loss: 0.617246, acc.: 82.03%] [G loss: 4.347095]\n",
      "epoch:30 step:23454 [D loss: 0.417369, acc.: 84.38%] [G loss: 2.866528]\n",
      "epoch:30 step:23455 [D loss: 0.326646, acc.: 82.81%] [G loss: 2.511147]\n",
      "epoch:30 step:23456 [D loss: 0.306973, acc.: 85.16%] [G loss: 2.651125]\n",
      "epoch:30 step:23457 [D loss: 0.288583, acc.: 88.28%] [G loss: 4.737534]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:30 step:23458 [D loss: 0.461886, acc.: 82.03%] [G loss: 3.624362]\n",
      "epoch:30 step:23459 [D loss: 0.317943, acc.: 85.94%] [G loss: 3.845616]\n",
      "epoch:30 step:23460 [D loss: 0.272364, acc.: 86.72%] [G loss: 3.991477]\n",
      "epoch:30 step:23461 [D loss: 0.424966, acc.: 78.12%] [G loss: 4.614162]\n",
      "epoch:30 step:23462 [D loss: 0.321751, acc.: 83.59%] [G loss: 2.483858]\n",
      "epoch:30 step:23463 [D loss: 0.272998, acc.: 89.06%] [G loss: 3.005517]\n",
      "epoch:30 step:23464 [D loss: 0.395994, acc.: 83.59%] [G loss: 3.248345]\n",
      "epoch:30 step:23465 [D loss: 0.303552, acc.: 87.50%] [G loss: 2.677549]\n",
      "epoch:30 step:23466 [D loss: 0.293767, acc.: 88.28%] [G loss: 3.446828]\n",
      "epoch:30 step:23467 [D loss: 0.258191, acc.: 89.06%] [G loss: 2.949863]\n",
      "epoch:30 step:23468 [D loss: 0.374220, acc.: 81.25%] [G loss: 4.611132]\n",
      "epoch:30 step:23469 [D loss: 0.393895, acc.: 78.91%] [G loss: 3.088652]\n",
      "epoch:30 step:23470 [D loss: 0.269759, acc.: 89.06%] [G loss: 4.288264]\n",
      "epoch:30 step:23471 [D loss: 0.325604, acc.: 86.72%] [G loss: 3.059358]\n",
      "epoch:30 step:23472 [D loss: 0.296959, acc.: 87.50%] [G loss: 3.813672]\n",
      "epoch:30 step:23473 [D loss: 0.403905, acc.: 79.69%] [G loss: 2.744290]\n",
      "epoch:30 step:23474 [D loss: 0.443006, acc.: 79.69%] [G loss: 3.034875]\n",
      "epoch:30 step:23475 [D loss: 0.317515, acc.: 82.03%] [G loss: 2.644768]\n",
      "epoch:30 step:23476 [D loss: 0.323480, acc.: 83.59%] [G loss: 4.075834]\n",
      "epoch:30 step:23477 [D loss: 0.305996, acc.: 84.38%] [G loss: 5.012327]\n",
      "epoch:30 step:23478 [D loss: 0.334044, acc.: 82.81%] [G loss: 5.099889]\n",
      "epoch:30 step:23479 [D loss: 0.320534, acc.: 84.38%] [G loss: 3.478168]\n",
      "epoch:30 step:23480 [D loss: 0.257611, acc.: 88.28%] [G loss: 4.493158]\n",
      "epoch:30 step:23481 [D loss: 0.273252, acc.: 88.28%] [G loss: 3.990160]\n",
      "epoch:30 step:23482 [D loss: 0.332425, acc.: 86.72%] [G loss: 4.628692]\n",
      "epoch:30 step:23483 [D loss: 0.218562, acc.: 92.19%] [G loss: 4.927194]\n",
      "epoch:30 step:23484 [D loss: 0.245311, acc.: 89.84%] [G loss: 6.497442]\n",
      "epoch:30 step:23485 [D loss: 0.237596, acc.: 92.19%] [G loss: 6.892151]\n",
      "epoch:30 step:23486 [D loss: 0.281757, acc.: 87.50%] [G loss: 4.481405]\n",
      "epoch:30 step:23487 [D loss: 0.219610, acc.: 90.62%] [G loss: 4.534601]\n",
      "epoch:30 step:23488 [D loss: 0.357549, acc.: 82.81%] [G loss: 3.163008]\n",
      "epoch:30 step:23489 [D loss: 0.258120, acc.: 90.62%] [G loss: 3.491556]\n",
      "epoch:30 step:23490 [D loss: 0.269436, acc.: 89.06%] [G loss: 4.064743]\n",
      "epoch:30 step:23491 [D loss: 0.390817, acc.: 82.81%] [G loss: 3.196529]\n",
      "epoch:30 step:23492 [D loss: 0.334767, acc.: 81.25%] [G loss: 3.215235]\n",
      "epoch:30 step:23493 [D loss: 0.282465, acc.: 85.94%] [G loss: 4.673786]\n",
      "epoch:30 step:23494 [D loss: 0.299523, acc.: 87.50%] [G loss: 3.581357]\n",
      "epoch:30 step:23495 [D loss: 0.303496, acc.: 82.81%] [G loss: 3.700984]\n",
      "epoch:30 step:23496 [D loss: 0.408161, acc.: 85.16%] [G loss: 3.063880]\n",
      "epoch:30 step:23497 [D loss: 0.348956, acc.: 87.50%] [G loss: 3.583050]\n",
      "epoch:30 step:23498 [D loss: 0.395712, acc.: 78.91%] [G loss: 3.681589]\n",
      "epoch:30 step:23499 [D loss: 0.403369, acc.: 84.38%] [G loss: 3.751774]\n",
      "epoch:30 step:23500 [D loss: 0.340469, acc.: 85.94%] [G loss: 4.627103]\n",
      "epoch:30 step:23501 [D loss: 0.271536, acc.: 86.72%] [G loss: 3.071780]\n",
      "epoch:30 step:23502 [D loss: 0.283990, acc.: 91.41%] [G loss: 3.529727]\n",
      "epoch:30 step:23503 [D loss: 0.337556, acc.: 78.12%] [G loss: 3.638877]\n",
      "epoch:30 step:23504 [D loss: 0.320259, acc.: 82.03%] [G loss: 3.097993]\n",
      "epoch:30 step:23505 [D loss: 0.330409, acc.: 83.59%] [G loss: 3.472972]\n",
      "epoch:30 step:23506 [D loss: 0.349981, acc.: 82.81%] [G loss: 3.434990]\n",
      "epoch:30 step:23507 [D loss: 0.328915, acc.: 85.16%] [G loss: 4.563284]\n",
      "epoch:30 step:23508 [D loss: 0.387668, acc.: 85.16%] [G loss: 2.670045]\n",
      "epoch:30 step:23509 [D loss: 0.220349, acc.: 90.62%] [G loss: 3.053358]\n",
      "epoch:30 step:23510 [D loss: 0.292185, acc.: 87.50%] [G loss: 3.421505]\n",
      "epoch:30 step:23511 [D loss: 0.301193, acc.: 87.50%] [G loss: 2.953623]\n",
      "epoch:30 step:23512 [D loss: 0.483485, acc.: 78.91%] [G loss: 3.138691]\n",
      "epoch:30 step:23513 [D loss: 0.273255, acc.: 87.50%] [G loss: 2.807314]\n",
      "epoch:30 step:23514 [D loss: 0.304953, acc.: 89.06%] [G loss: 2.728684]\n",
      "epoch:30 step:23515 [D loss: 0.378731, acc.: 82.81%] [G loss: 2.768525]\n",
      "epoch:30 step:23516 [D loss: 0.282059, acc.: 89.06%] [G loss: 3.485665]\n",
      "epoch:30 step:23517 [D loss: 0.272781, acc.: 86.72%] [G loss: 3.793930]\n",
      "epoch:30 step:23518 [D loss: 0.325807, acc.: 87.50%] [G loss: 2.441443]\n",
      "epoch:30 step:23519 [D loss: 0.270279, acc.: 89.06%] [G loss: 3.536263]\n",
      "epoch:30 step:23520 [D loss: 0.368179, acc.: 82.81%] [G loss: 4.104620]\n",
      "epoch:30 step:23521 [D loss: 0.323861, acc.: 85.16%] [G loss: 4.713484]\n",
      "epoch:30 step:23522 [D loss: 0.258575, acc.: 90.62%] [G loss: 3.646365]\n",
      "epoch:30 step:23523 [D loss: 0.239017, acc.: 93.75%] [G loss: 4.187949]\n",
      "epoch:30 step:23524 [D loss: 0.294602, acc.: 86.72%] [G loss: 3.303316]\n",
      "epoch:30 step:23525 [D loss: 0.438982, acc.: 78.12%] [G loss: 3.084328]\n",
      "epoch:30 step:23526 [D loss: 0.251547, acc.: 89.84%] [G loss: 3.762560]\n",
      "epoch:30 step:23527 [D loss: 0.364730, acc.: 82.03%] [G loss: 4.042864]\n",
      "epoch:30 step:23528 [D loss: 0.344437, acc.: 82.81%] [G loss: 3.149904]\n",
      "epoch:30 step:23529 [D loss: 0.289369, acc.: 86.72%] [G loss: 4.712169]\n",
      "epoch:30 step:23530 [D loss: 0.351572, acc.: 86.72%] [G loss: 3.167097]\n",
      "epoch:30 step:23531 [D loss: 0.416944, acc.: 81.25%] [G loss: 2.825692]\n",
      "epoch:30 step:23532 [D loss: 0.313493, acc.: 89.06%] [G loss: 2.980763]\n",
      "epoch:30 step:23533 [D loss: 0.385636, acc.: 80.47%] [G loss: 3.674557]\n",
      "epoch:30 step:23534 [D loss: 0.408656, acc.: 83.59%] [G loss: 2.780650]\n",
      "epoch:30 step:23535 [D loss: 0.270704, acc.: 87.50%] [G loss: 2.693780]\n",
      "epoch:30 step:23536 [D loss: 0.395622, acc.: 79.69%] [G loss: 3.164139]\n",
      "epoch:30 step:23537 [D loss: 0.380264, acc.: 83.59%] [G loss: 3.077953]\n",
      "epoch:30 step:23538 [D loss: 0.328055, acc.: 85.94%] [G loss: 4.798405]\n",
      "epoch:30 step:23539 [D loss: 0.465427, acc.: 75.78%] [G loss: 8.837242]\n",
      "epoch:30 step:23540 [D loss: 0.496566, acc.: 78.12%] [G loss: 3.004390]\n",
      "epoch:30 step:23541 [D loss: 0.358961, acc.: 84.38%] [G loss: 4.577032]\n",
      "epoch:30 step:23542 [D loss: 0.238236, acc.: 88.28%] [G loss: 3.114717]\n",
      "epoch:30 step:23543 [D loss: 0.236550, acc.: 91.41%] [G loss: 3.757350]\n",
      "epoch:30 step:23544 [D loss: 0.416231, acc.: 81.25%] [G loss: 3.645084]\n",
      "epoch:30 step:23545 [D loss: 0.281540, acc.: 89.84%] [G loss: 2.266412]\n",
      "epoch:30 step:23546 [D loss: 0.385545, acc.: 86.72%] [G loss: 3.076229]\n",
      "epoch:30 step:23547 [D loss: 0.284191, acc.: 89.06%] [G loss: 3.685938]\n",
      "epoch:30 step:23548 [D loss: 0.354600, acc.: 84.38%] [G loss: 4.115499]\n",
      "epoch:30 step:23549 [D loss: 0.313983, acc.: 86.72%] [G loss: 3.156517]\n",
      "epoch:30 step:23550 [D loss: 0.371911, acc.: 82.03%] [G loss: 2.698123]\n",
      "epoch:30 step:23551 [D loss: 0.284636, acc.: 88.28%] [G loss: 2.841896]\n",
      "epoch:30 step:23552 [D loss: 0.301353, acc.: 84.38%] [G loss: 3.342786]\n",
      "epoch:30 step:23553 [D loss: 0.282972, acc.: 86.72%] [G loss: 2.450041]\n",
      "epoch:30 step:23554 [D loss: 0.344427, acc.: 85.94%] [G loss: 2.764929]\n",
      "epoch:30 step:23555 [D loss: 0.366958, acc.: 81.25%] [G loss: 2.609939]\n",
      "epoch:30 step:23556 [D loss: 0.313130, acc.: 86.72%] [G loss: 3.826459]\n",
      "epoch:30 step:23557 [D loss: 0.289274, acc.: 88.28%] [G loss: 3.309503]\n",
      "epoch:30 step:23558 [D loss: 0.329954, acc.: 82.03%] [G loss: 4.214516]\n",
      "epoch:30 step:23559 [D loss: 0.248348, acc.: 92.19%] [G loss: 5.462409]\n",
      "epoch:30 step:23560 [D loss: 0.428988, acc.: 79.69%] [G loss: 5.783144]\n",
      "epoch:30 step:23561 [D loss: 0.324453, acc.: 82.81%] [G loss: 3.626036]\n",
      "epoch:30 step:23562 [D loss: 0.276110, acc.: 87.50%] [G loss: 2.469058]\n",
      "epoch:30 step:23563 [D loss: 0.319546, acc.: 86.72%] [G loss: 3.163315]\n",
      "epoch:30 step:23564 [D loss: 0.334232, acc.: 83.59%] [G loss: 3.305749]\n",
      "epoch:30 step:23565 [D loss: 0.396981, acc.: 81.25%] [G loss: 2.735217]\n",
      "epoch:30 step:23566 [D loss: 0.211116, acc.: 92.19%] [G loss: 3.471906]\n",
      "epoch:30 step:23567 [D loss: 0.419099, acc.: 77.34%] [G loss: 4.923124]\n",
      "epoch:30 step:23568 [D loss: 0.332765, acc.: 82.81%] [G loss: 3.108967]\n",
      "epoch:30 step:23569 [D loss: 0.194036, acc.: 92.19%] [G loss: 6.331854]\n",
      "epoch:30 step:23570 [D loss: 0.230627, acc.: 92.19%] [G loss: 3.794281]\n",
      "epoch:30 step:23571 [D loss: 0.404842, acc.: 80.47%] [G loss: 4.977591]\n",
      "epoch:30 step:23572 [D loss: 0.551046, acc.: 77.34%] [G loss: 3.806773]\n",
      "epoch:30 step:23573 [D loss: 0.260683, acc.: 89.06%] [G loss: 3.444849]\n",
      "epoch:30 step:23574 [D loss: 0.368158, acc.: 82.03%] [G loss: 3.917267]\n",
      "epoch:30 step:23575 [D loss: 0.285459, acc.: 85.16%] [G loss: 3.692507]\n",
      "epoch:30 step:23576 [D loss: 0.326976, acc.: 82.81%] [G loss: 2.928127]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:30 step:23577 [D loss: 0.332607, acc.: 83.59%] [G loss: 3.569204]\n",
      "epoch:30 step:23578 [D loss: 0.256002, acc.: 89.06%] [G loss: 4.121694]\n",
      "epoch:30 step:23579 [D loss: 0.278911, acc.: 88.28%] [G loss: 3.430973]\n",
      "epoch:30 step:23580 [D loss: 0.345452, acc.: 83.59%] [G loss: 3.873081]\n",
      "epoch:30 step:23581 [D loss: 0.262608, acc.: 91.41%] [G loss: 5.163288]\n",
      "epoch:30 step:23582 [D loss: 0.322975, acc.: 86.72%] [G loss: 3.286866]\n",
      "epoch:30 step:23583 [D loss: 0.340967, acc.: 80.47%] [G loss: 3.709944]\n",
      "epoch:30 step:23584 [D loss: 0.300607, acc.: 87.50%] [G loss: 3.176222]\n",
      "epoch:30 step:23585 [D loss: 0.405400, acc.: 83.59%] [G loss: 3.723244]\n",
      "epoch:30 step:23586 [D loss: 0.345868, acc.: 85.16%] [G loss: 3.816090]\n",
      "epoch:30 step:23587 [D loss: 0.443346, acc.: 79.69%] [G loss: 2.944558]\n",
      "epoch:30 step:23588 [D loss: 0.239459, acc.: 89.84%] [G loss: 3.437108]\n",
      "epoch:30 step:23589 [D loss: 0.340920, acc.: 85.16%] [G loss: 3.249836]\n",
      "epoch:30 step:23590 [D loss: 0.414784, acc.: 82.03%] [G loss: 3.599926]\n",
      "epoch:30 step:23591 [D loss: 0.354393, acc.: 82.81%] [G loss: 3.536150]\n",
      "epoch:30 step:23592 [D loss: 0.284130, acc.: 86.72%] [G loss: 3.923833]\n",
      "epoch:30 step:23593 [D loss: 0.342571, acc.: 84.38%] [G loss: 2.708639]\n",
      "epoch:30 step:23594 [D loss: 0.368041, acc.: 85.16%] [G loss: 3.525356]\n",
      "epoch:30 step:23595 [D loss: 0.353640, acc.: 89.06%] [G loss: 3.250620]\n",
      "epoch:30 step:23596 [D loss: 0.269532, acc.: 86.72%] [G loss: 2.512998]\n",
      "epoch:30 step:23597 [D loss: 0.288567, acc.: 85.94%] [G loss: 2.983820]\n",
      "epoch:30 step:23598 [D loss: 0.487724, acc.: 76.56%] [G loss: 2.667865]\n",
      "epoch:30 step:23599 [D loss: 0.285916, acc.: 85.94%] [G loss: 3.207598]\n",
      "epoch:30 step:23600 [D loss: 0.235478, acc.: 89.06%] [G loss: 3.069678]\n",
      "##############\n",
      "[0.85502882 0.84937496 0.81291171 0.7892234  0.78372825 0.83059625\n",
      " 0.85787867 0.80469099 0.80286367 0.82240419]\n",
      "##########\n",
      "epoch:30 step:23601 [D loss: 0.385066, acc.: 82.03%] [G loss: 2.693687]\n",
      "epoch:30 step:23602 [D loss: 0.297730, acc.: 85.94%] [G loss: 2.755718]\n",
      "epoch:30 step:23603 [D loss: 0.302665, acc.: 85.16%] [G loss: 4.043391]\n",
      "epoch:30 step:23604 [D loss: 0.419273, acc.: 81.25%] [G loss: 2.919923]\n",
      "epoch:30 step:23605 [D loss: 0.321542, acc.: 84.38%] [G loss: 3.266140]\n",
      "epoch:30 step:23606 [D loss: 0.259044, acc.: 89.84%] [G loss: 3.447417]\n",
      "epoch:30 step:23607 [D loss: 0.345235, acc.: 82.03%] [G loss: 3.128087]\n",
      "epoch:30 step:23608 [D loss: 0.268406, acc.: 89.84%] [G loss: 4.664964]\n",
      "epoch:30 step:23609 [D loss: 0.300496, acc.: 84.38%] [G loss: 2.840281]\n",
      "epoch:30 step:23610 [D loss: 0.262069, acc.: 88.28%] [G loss: 3.292336]\n",
      "epoch:30 step:23611 [D loss: 0.328745, acc.: 85.94%] [G loss: 2.679229]\n",
      "epoch:30 step:23612 [D loss: 0.299613, acc.: 86.72%] [G loss: 3.577914]\n",
      "epoch:30 step:23613 [D loss: 0.402978, acc.: 82.03%] [G loss: 2.873930]\n",
      "epoch:30 step:23614 [D loss: 0.262212, acc.: 89.84%] [G loss: 3.388353]\n",
      "epoch:30 step:23615 [D loss: 0.255595, acc.: 87.50%] [G loss: 3.652035]\n",
      "epoch:30 step:23616 [D loss: 0.253890, acc.: 87.50%] [G loss: 3.881565]\n",
      "epoch:30 step:23617 [D loss: 0.341487, acc.: 82.03%] [G loss: 3.769609]\n",
      "epoch:30 step:23618 [D loss: 0.342464, acc.: 83.59%] [G loss: 4.058738]\n",
      "epoch:30 step:23619 [D loss: 0.285781, acc.: 88.28%] [G loss: 4.884497]\n",
      "epoch:30 step:23620 [D loss: 0.391290, acc.: 82.03%] [G loss: 5.942462]\n",
      "epoch:30 step:23621 [D loss: 0.465756, acc.: 76.56%] [G loss: 3.966116]\n",
      "epoch:30 step:23622 [D loss: 0.446463, acc.: 77.34%] [G loss: 3.253064]\n",
      "epoch:30 step:23623 [D loss: 0.399735, acc.: 81.25%] [G loss: 3.029951]\n",
      "epoch:30 step:23624 [D loss: 0.243843, acc.: 89.84%] [G loss: 2.928986]\n",
      "epoch:30 step:23625 [D loss: 0.311958, acc.: 85.16%] [G loss: 2.815991]\n",
      "epoch:30 step:23626 [D loss: 0.227741, acc.: 89.84%] [G loss: 3.571098]\n",
      "epoch:30 step:23627 [D loss: 0.447976, acc.: 75.78%] [G loss: 2.628883]\n",
      "epoch:30 step:23628 [D loss: 0.390346, acc.: 78.91%] [G loss: 3.487409]\n",
      "epoch:30 step:23629 [D loss: 0.298738, acc.: 86.72%] [G loss: 3.547807]\n",
      "epoch:30 step:23630 [D loss: 0.358162, acc.: 81.25%] [G loss: 3.951199]\n",
      "epoch:30 step:23631 [D loss: 0.352962, acc.: 84.38%] [G loss: 3.028833]\n",
      "epoch:30 step:23632 [D loss: 0.339903, acc.: 86.72%] [G loss: 3.998679]\n",
      "epoch:30 step:23633 [D loss: 0.287919, acc.: 86.72%] [G loss: 3.136769]\n",
      "epoch:30 step:23634 [D loss: 0.239945, acc.: 91.41%] [G loss: 4.182257]\n",
      "epoch:30 step:23635 [D loss: 0.390566, acc.: 85.16%] [G loss: 4.704983]\n",
      "epoch:30 step:23636 [D loss: 0.968472, acc.: 75.78%] [G loss: 6.329807]\n",
      "epoch:30 step:23637 [D loss: 0.633896, acc.: 74.22%] [G loss: 4.481325]\n",
      "epoch:30 step:23638 [D loss: 0.486581, acc.: 80.47%] [G loss: 4.430025]\n",
      "epoch:30 step:23639 [D loss: 0.359646, acc.: 78.12%] [G loss: 3.365453]\n",
      "epoch:30 step:23640 [D loss: 0.400039, acc.: 82.03%] [G loss: 3.181961]\n",
      "epoch:30 step:23641 [D loss: 0.352156, acc.: 85.94%] [G loss: 4.718567]\n",
      "epoch:30 step:23642 [D loss: 0.543618, acc.: 77.34%] [G loss: 3.210810]\n",
      "epoch:30 step:23643 [D loss: 0.193319, acc.: 91.41%] [G loss: 6.222251]\n",
      "epoch:30 step:23644 [D loss: 0.238625, acc.: 92.19%] [G loss: 3.453289]\n",
      "epoch:30 step:23645 [D loss: 0.334038, acc.: 87.50%] [G loss: 3.790256]\n",
      "epoch:30 step:23646 [D loss: 0.230005, acc.: 90.62%] [G loss: 3.480583]\n",
      "epoch:30 step:23647 [D loss: 0.405300, acc.: 78.12%] [G loss: 3.135286]\n",
      "epoch:30 step:23648 [D loss: 0.172986, acc.: 94.53%] [G loss: 4.622598]\n",
      "epoch:30 step:23649 [D loss: 0.298268, acc.: 85.94%] [G loss: 3.657586]\n",
      "epoch:30 step:23650 [D loss: 0.303234, acc.: 87.50%] [G loss: 2.761789]\n",
      "epoch:30 step:23651 [D loss: 0.268866, acc.: 88.28%] [G loss: 3.419484]\n",
      "epoch:30 step:23652 [D loss: 0.271000, acc.: 88.28%] [G loss: 3.395915]\n",
      "epoch:30 step:23653 [D loss: 0.222214, acc.: 94.53%] [G loss: 3.976331]\n",
      "epoch:30 step:23654 [D loss: 0.335203, acc.: 88.28%] [G loss: 3.279925]\n",
      "epoch:30 step:23655 [D loss: 0.271261, acc.: 89.84%] [G loss: 3.044189]\n",
      "epoch:30 step:23656 [D loss: 0.292524, acc.: 86.72%] [G loss: 3.051324]\n",
      "epoch:30 step:23657 [D loss: 0.262069, acc.: 87.50%] [G loss: 3.076514]\n",
      "epoch:30 step:23658 [D loss: 0.395073, acc.: 80.47%] [G loss: 2.702246]\n",
      "epoch:30 step:23659 [D loss: 0.301593, acc.: 85.16%] [G loss: 3.168514]\n",
      "epoch:30 step:23660 [D loss: 0.404356, acc.: 79.69%] [G loss: 3.853629]\n",
      "epoch:30 step:23661 [D loss: 0.257085, acc.: 89.06%] [G loss: 3.667371]\n",
      "epoch:30 step:23662 [D loss: 0.337836, acc.: 82.81%] [G loss: 3.475364]\n",
      "epoch:30 step:23663 [D loss: 0.417050, acc.: 79.69%] [G loss: 3.712949]\n",
      "epoch:30 step:23664 [D loss: 0.314947, acc.: 84.38%] [G loss: 3.119348]\n",
      "epoch:30 step:23665 [D loss: 0.327633, acc.: 84.38%] [G loss: 2.938259]\n",
      "epoch:30 step:23666 [D loss: 0.330612, acc.: 85.94%] [G loss: 3.449797]\n",
      "epoch:30 step:23667 [D loss: 0.290004, acc.: 87.50%] [G loss: 3.742378]\n",
      "epoch:30 step:23668 [D loss: 0.266169, acc.: 86.72%] [G loss: 3.599473]\n",
      "epoch:30 step:23669 [D loss: 0.350464, acc.: 81.25%] [G loss: 3.669879]\n",
      "epoch:30 step:23670 [D loss: 0.280445, acc.: 88.28%] [G loss: 3.644392]\n",
      "epoch:30 step:23671 [D loss: 0.390754, acc.: 83.59%] [G loss: 3.281613]\n",
      "epoch:30 step:23672 [D loss: 0.298531, acc.: 87.50%] [G loss: 4.692288]\n",
      "epoch:30 step:23673 [D loss: 0.368895, acc.: 82.03%] [G loss: 4.775651]\n",
      "epoch:30 step:23674 [D loss: 0.440729, acc.: 79.69%] [G loss: 4.020064]\n",
      "epoch:30 step:23675 [D loss: 0.277152, acc.: 85.94%] [G loss: 5.985727]\n",
      "epoch:30 step:23676 [D loss: 0.335704, acc.: 82.81%] [G loss: 4.880666]\n",
      "epoch:30 step:23677 [D loss: 0.492350, acc.: 77.34%] [G loss: 6.157290]\n",
      "epoch:30 step:23678 [D loss: 0.289207, acc.: 85.94%] [G loss: 5.108094]\n",
      "epoch:30 step:23679 [D loss: 0.264673, acc.: 89.06%] [G loss: 4.947576]\n",
      "epoch:30 step:23680 [D loss: 0.247172, acc.: 88.28%] [G loss: 4.880575]\n",
      "epoch:30 step:23681 [D loss: 0.292512, acc.: 90.62%] [G loss: 4.599017]\n",
      "epoch:30 step:23682 [D loss: 0.257304, acc.: 85.94%] [G loss: 6.625453]\n",
      "epoch:30 step:23683 [D loss: 0.252230, acc.: 89.06%] [G loss: 6.557699]\n",
      "epoch:30 step:23684 [D loss: 0.204154, acc.: 91.41%] [G loss: 3.383197]\n",
      "epoch:30 step:23685 [D loss: 0.284042, acc.: 88.28%] [G loss: 5.721727]\n",
      "epoch:30 step:23686 [D loss: 0.289751, acc.: 89.84%] [G loss: 4.033113]\n",
      "epoch:30 step:23687 [D loss: 0.367562, acc.: 82.03%] [G loss: 5.718631]\n",
      "epoch:30 step:23688 [D loss: 0.352964, acc.: 85.16%] [G loss: 4.992985]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:30 step:23689 [D loss: 0.239612, acc.: 88.28%] [G loss: 4.433005]\n",
      "epoch:30 step:23690 [D loss: 0.250608, acc.: 92.19%] [G loss: 5.747551]\n",
      "epoch:30 step:23691 [D loss: 0.328382, acc.: 83.59%] [G loss: 4.763700]\n",
      "epoch:30 step:23692 [D loss: 0.294417, acc.: 87.50%] [G loss: 3.863362]\n",
      "epoch:30 step:23693 [D loss: 0.248966, acc.: 89.06%] [G loss: 3.577794]\n",
      "epoch:30 step:23694 [D loss: 0.232418, acc.: 91.41%] [G loss: 3.732164]\n",
      "epoch:30 step:23695 [D loss: 0.309041, acc.: 89.06%] [G loss: 4.055265]\n",
      "epoch:30 step:23696 [D loss: 0.377698, acc.: 85.94%] [G loss: 3.596276]\n",
      "epoch:30 step:23697 [D loss: 0.350670, acc.: 85.16%] [G loss: 3.991491]\n",
      "epoch:30 step:23698 [D loss: 0.276872, acc.: 88.28%] [G loss: 3.746508]\n",
      "epoch:30 step:23699 [D loss: 0.364652, acc.: 86.72%] [G loss: 3.470058]\n",
      "epoch:30 step:23700 [D loss: 0.239139, acc.: 88.28%] [G loss: 4.197597]\n",
      "epoch:30 step:23701 [D loss: 0.298157, acc.: 87.50%] [G loss: 4.685589]\n",
      "epoch:30 step:23702 [D loss: 0.332060, acc.: 86.72%] [G loss: 3.393006]\n",
      "epoch:30 step:23703 [D loss: 0.292398, acc.: 85.94%] [G loss: 3.339574]\n",
      "epoch:30 step:23704 [D loss: 0.316197, acc.: 83.59%] [G loss: 3.409285]\n",
      "epoch:30 step:23705 [D loss: 0.471272, acc.: 76.56%] [G loss: 7.566232]\n",
      "epoch:30 step:23706 [D loss: 0.621945, acc.: 71.09%] [G loss: 3.913847]\n",
      "epoch:30 step:23707 [D loss: 0.554418, acc.: 73.44%] [G loss: 5.406255]\n",
      "epoch:30 step:23708 [D loss: 0.734432, acc.: 67.19%] [G loss: 5.051462]\n",
      "epoch:30 step:23709 [D loss: 0.666172, acc.: 72.66%] [G loss: 6.215344]\n",
      "epoch:30 step:23710 [D loss: 0.399740, acc.: 88.28%] [G loss: 5.214524]\n",
      "epoch:30 step:23711 [D loss: 0.391924, acc.: 78.91%] [G loss: 7.617509]\n",
      "epoch:30 step:23712 [D loss: 0.274261, acc.: 87.50%] [G loss: 5.454928]\n",
      "epoch:30 step:23713 [D loss: 0.272494, acc.: 89.06%] [G loss: 4.479887]\n",
      "epoch:30 step:23714 [D loss: 0.220201, acc.: 90.62%] [G loss: 3.349459]\n",
      "epoch:30 step:23715 [D loss: 0.274341, acc.: 87.50%] [G loss: 3.235344]\n",
      "epoch:30 step:23716 [D loss: 0.441199, acc.: 72.66%] [G loss: 3.345454]\n",
      "epoch:30 step:23717 [D loss: 0.260967, acc.: 89.06%] [G loss: 3.601553]\n",
      "epoch:30 step:23718 [D loss: 0.398721, acc.: 80.47%] [G loss: 3.342211]\n",
      "epoch:30 step:23719 [D loss: 0.413010, acc.: 79.69%] [G loss: 4.101728]\n",
      "epoch:30 step:23720 [D loss: 0.273490, acc.: 85.16%] [G loss: 3.820574]\n",
      "epoch:30 step:23721 [D loss: 0.278461, acc.: 85.94%] [G loss: 4.116853]\n",
      "epoch:30 step:23722 [D loss: 0.364993, acc.: 85.16%] [G loss: 2.876497]\n",
      "epoch:30 step:23723 [D loss: 0.367481, acc.: 84.38%] [G loss: 3.688765]\n",
      "epoch:30 step:23724 [D loss: 0.277986, acc.: 87.50%] [G loss: 3.448350]\n",
      "epoch:30 step:23725 [D loss: 0.357932, acc.: 82.81%] [G loss: 3.174681]\n",
      "epoch:30 step:23726 [D loss: 0.297378, acc.: 88.28%] [G loss: 5.121894]\n",
      "epoch:30 step:23727 [D loss: 0.269687, acc.: 89.84%] [G loss: 2.902353]\n",
      "epoch:30 step:23728 [D loss: 0.326922, acc.: 86.72%] [G loss: 3.201503]\n",
      "epoch:30 step:23729 [D loss: 0.344141, acc.: 84.38%] [G loss: 2.738158]\n",
      "epoch:30 step:23730 [D loss: 0.334519, acc.: 85.94%] [G loss: 2.624863]\n",
      "epoch:30 step:23731 [D loss: 0.226707, acc.: 89.06%] [G loss: 3.140919]\n",
      "epoch:30 step:23732 [D loss: 0.319864, acc.: 88.28%] [G loss: 2.894248]\n",
      "epoch:30 step:23733 [D loss: 0.432966, acc.: 82.03%] [G loss: 3.260324]\n",
      "epoch:30 step:23734 [D loss: 0.387571, acc.: 79.69%] [G loss: 3.240486]\n",
      "epoch:30 step:23735 [D loss: 0.396977, acc.: 85.16%] [G loss: 2.799230]\n",
      "epoch:30 step:23736 [D loss: 0.318683, acc.: 84.38%] [G loss: 2.481485]\n",
      "epoch:30 step:23737 [D loss: 0.318217, acc.: 87.50%] [G loss: 3.872155]\n",
      "epoch:30 step:23738 [D loss: 0.270333, acc.: 85.94%] [G loss: 3.735955]\n",
      "epoch:30 step:23739 [D loss: 0.375931, acc.: 80.47%] [G loss: 2.877635]\n",
      "epoch:30 step:23740 [D loss: 0.325514, acc.: 85.94%] [G loss: 3.888554]\n",
      "epoch:30 step:23741 [D loss: 0.372875, acc.: 82.03%] [G loss: 5.108261]\n",
      "epoch:30 step:23742 [D loss: 0.414482, acc.: 81.25%] [G loss: 3.242287]\n",
      "epoch:30 step:23743 [D loss: 0.389524, acc.: 82.03%] [G loss: 3.917517]\n",
      "epoch:30 step:23744 [D loss: 0.388946, acc.: 80.47%] [G loss: 3.538652]\n",
      "epoch:30 step:23745 [D loss: 0.362994, acc.: 84.38%] [G loss: 4.429105]\n",
      "epoch:30 step:23746 [D loss: 0.255680, acc.: 91.41%] [G loss: 3.548528]\n",
      "epoch:30 step:23747 [D loss: 0.393782, acc.: 85.94%] [G loss: 3.261962]\n",
      "epoch:30 step:23748 [D loss: 0.290570, acc.: 89.06%] [G loss: 5.157421]\n",
      "epoch:30 step:23749 [D loss: 0.296287, acc.: 87.50%] [G loss: 5.187554]\n",
      "epoch:30 step:23750 [D loss: 0.367415, acc.: 82.03%] [G loss: 5.854609]\n",
      "epoch:30 step:23751 [D loss: 0.333313, acc.: 80.47%] [G loss: 4.301898]\n",
      "epoch:30 step:23752 [D loss: 0.304836, acc.: 87.50%] [G loss: 3.598222]\n",
      "epoch:30 step:23753 [D loss: 0.359357, acc.: 83.59%] [G loss: 3.869279]\n",
      "epoch:30 step:23754 [D loss: 0.273879, acc.: 89.84%] [G loss: 4.108478]\n",
      "epoch:30 step:23755 [D loss: 0.362682, acc.: 80.47%] [G loss: 2.974037]\n",
      "epoch:30 step:23756 [D loss: 0.373091, acc.: 86.72%] [G loss: 2.912283]\n",
      "epoch:30 step:23757 [D loss: 0.260525, acc.: 89.06%] [G loss: 3.095512]\n",
      "epoch:30 step:23758 [D loss: 0.413055, acc.: 82.03%] [G loss: 3.255975]\n",
      "epoch:30 step:23759 [D loss: 0.384381, acc.: 82.81%] [G loss: 2.542908]\n",
      "epoch:30 step:23760 [D loss: 0.332734, acc.: 82.81%] [G loss: 2.503936]\n",
      "epoch:30 step:23761 [D loss: 0.312830, acc.: 86.72%] [G loss: 3.415506]\n",
      "epoch:30 step:23762 [D loss: 0.343867, acc.: 85.16%] [G loss: 4.367196]\n",
      "epoch:30 step:23763 [D loss: 0.337465, acc.: 83.59%] [G loss: 2.968379]\n",
      "epoch:30 step:23764 [D loss: 0.335404, acc.: 83.59%] [G loss: 3.184099]\n",
      "epoch:30 step:23765 [D loss: 0.232522, acc.: 89.84%] [G loss: 2.948692]\n",
      "epoch:30 step:23766 [D loss: 0.327706, acc.: 80.47%] [G loss: 3.588521]\n",
      "epoch:30 step:23767 [D loss: 0.394481, acc.: 81.25%] [G loss: 2.912475]\n",
      "epoch:30 step:23768 [D loss: 0.331038, acc.: 82.81%] [G loss: 2.773312]\n",
      "epoch:30 step:23769 [D loss: 0.260422, acc.: 89.06%] [G loss: 3.079776]\n",
      "epoch:30 step:23770 [D loss: 0.418205, acc.: 82.03%] [G loss: 4.488805]\n",
      "epoch:30 step:23771 [D loss: 0.291876, acc.: 85.94%] [G loss: 3.370175]\n",
      "epoch:30 step:23772 [D loss: 0.307897, acc.: 83.59%] [G loss: 5.256067]\n",
      "epoch:30 step:23773 [D loss: 0.291019, acc.: 85.16%] [G loss: 3.266489]\n",
      "epoch:30 step:23774 [D loss: 0.274189, acc.: 89.06%] [G loss: 4.698433]\n",
      "epoch:30 step:23775 [D loss: 0.221789, acc.: 92.19%] [G loss: 4.500762]\n",
      "epoch:30 step:23776 [D loss: 0.277161, acc.: 88.28%] [G loss: 4.047575]\n",
      "epoch:30 step:23777 [D loss: 0.299173, acc.: 85.16%] [G loss: 4.299129]\n",
      "epoch:30 step:23778 [D loss: 0.350300, acc.: 87.50%] [G loss: 3.633258]\n",
      "epoch:30 step:23779 [D loss: 0.383675, acc.: 82.03%] [G loss: 3.573874]\n",
      "epoch:30 step:23780 [D loss: 0.342566, acc.: 84.38%] [G loss: 3.107195]\n",
      "epoch:30 step:23781 [D loss: 0.257645, acc.: 89.84%] [G loss: 4.328790]\n",
      "epoch:30 step:23782 [D loss: 0.392055, acc.: 82.81%] [G loss: 4.536679]\n",
      "epoch:30 step:23783 [D loss: 0.349982, acc.: 81.25%] [G loss: 4.506836]\n",
      "epoch:30 step:23784 [D loss: 0.231663, acc.: 89.84%] [G loss: 3.180059]\n",
      "epoch:30 step:23785 [D loss: 0.335923, acc.: 82.03%] [G loss: 2.912274]\n",
      "epoch:30 step:23786 [D loss: 0.354787, acc.: 84.38%] [G loss: 2.791081]\n",
      "epoch:30 step:23787 [D loss: 0.315293, acc.: 87.50%] [G loss: 4.060143]\n",
      "epoch:30 step:23788 [D loss: 0.288135, acc.: 88.28%] [G loss: 2.992130]\n",
      "epoch:30 step:23789 [D loss: 0.301299, acc.: 87.50%] [G loss: 4.026342]\n",
      "epoch:30 step:23790 [D loss: 0.399473, acc.: 79.69%] [G loss: 2.508006]\n",
      "epoch:30 step:23791 [D loss: 0.411237, acc.: 79.69%] [G loss: 2.906686]\n",
      "epoch:30 step:23792 [D loss: 0.415381, acc.: 82.03%] [G loss: 3.933752]\n",
      "epoch:30 step:23793 [D loss: 0.409662, acc.: 80.47%] [G loss: 3.667950]\n",
      "epoch:30 step:23794 [D loss: 0.287894, acc.: 86.72%] [G loss: 2.876871]\n",
      "epoch:30 step:23795 [D loss: 0.270614, acc.: 87.50%] [G loss: 3.476800]\n",
      "epoch:30 step:23796 [D loss: 0.383138, acc.: 82.81%] [G loss: 2.452503]\n",
      "epoch:30 step:23797 [D loss: 0.325720, acc.: 85.94%] [G loss: 2.303111]\n",
      "epoch:30 step:23798 [D loss: 0.261774, acc.: 89.06%] [G loss: 2.494689]\n",
      "epoch:30 step:23799 [D loss: 0.342160, acc.: 84.38%] [G loss: 4.041887]\n",
      "epoch:30 step:23800 [D loss: 0.303319, acc.: 86.72%] [G loss: 3.245705]\n",
      "##############\n",
      "[0.86779879 0.89863317 0.81790627 0.78057475 0.75670317 0.82969856\n",
      " 0.88321342 0.80638334 0.8200053  0.81055193]\n",
      "##########\n",
      "epoch:30 step:23801 [D loss: 0.290883, acc.: 89.06%] [G loss: 2.826994]\n",
      "epoch:30 step:23802 [D loss: 0.287292, acc.: 89.84%] [G loss: 2.917251]\n",
      "epoch:30 step:23803 [D loss: 0.304824, acc.: 87.50%] [G loss: 3.316970]\n",
      "epoch:30 step:23804 [D loss: 0.365793, acc.: 83.59%] [G loss: 2.581431]\n",
      "epoch:30 step:23805 [D loss: 0.254943, acc.: 89.06%] [G loss: 3.662258]\n",
      "epoch:30 step:23806 [D loss: 0.284012, acc.: 85.16%] [G loss: 2.818478]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:30 step:23807 [D loss: 0.289979, acc.: 85.16%] [G loss: 3.430712]\n",
      "epoch:30 step:23808 [D loss: 0.267051, acc.: 88.28%] [G loss: 2.658390]\n",
      "epoch:30 step:23809 [D loss: 0.400147, acc.: 80.47%] [G loss: 2.854107]\n",
      "epoch:30 step:23810 [D loss: 0.371232, acc.: 83.59%] [G loss: 3.521492]\n",
      "epoch:30 step:23811 [D loss: 0.276356, acc.: 86.72%] [G loss: 3.401560]\n",
      "epoch:30 step:23812 [D loss: 0.273540, acc.: 90.62%] [G loss: 3.761087]\n",
      "epoch:30 step:23813 [D loss: 0.370571, acc.: 84.38%] [G loss: 3.053268]\n",
      "epoch:30 step:23814 [D loss: 0.337628, acc.: 82.03%] [G loss: 3.023738]\n",
      "epoch:30 step:23815 [D loss: 0.315041, acc.: 85.16%] [G loss: 3.607993]\n",
      "epoch:30 step:23816 [D loss: 0.364874, acc.: 82.81%] [G loss: 3.506285]\n",
      "epoch:30 step:23817 [D loss: 0.321286, acc.: 84.38%] [G loss: 2.435887]\n",
      "epoch:30 step:23818 [D loss: 0.328666, acc.: 85.16%] [G loss: 2.766770]\n",
      "epoch:30 step:23819 [D loss: 0.337832, acc.: 85.94%] [G loss: 4.240187]\n",
      "epoch:30 step:23820 [D loss: 0.235140, acc.: 91.41%] [G loss: 4.345631]\n",
      "epoch:30 step:23821 [D loss: 0.406659, acc.: 80.47%] [G loss: 2.933146]\n",
      "epoch:30 step:23822 [D loss: 0.281179, acc.: 87.50%] [G loss: 3.562890]\n",
      "epoch:30 step:23823 [D loss: 0.322092, acc.: 86.72%] [G loss: 3.039835]\n",
      "epoch:30 step:23824 [D loss: 0.258537, acc.: 89.84%] [G loss: 3.078164]\n",
      "epoch:30 step:23825 [D loss: 0.442532, acc.: 81.25%] [G loss: 3.549120]\n",
      "epoch:30 step:23826 [D loss: 0.459016, acc.: 78.91%] [G loss: 3.898314]\n",
      "epoch:30 step:23827 [D loss: 0.429915, acc.: 82.03%] [G loss: 3.170483]\n",
      "epoch:30 step:23828 [D loss: 0.192658, acc.: 89.06%] [G loss: 4.027584]\n",
      "epoch:30 step:23829 [D loss: 0.452272, acc.: 80.47%] [G loss: 3.430312]\n",
      "epoch:30 step:23830 [D loss: 0.374757, acc.: 85.16%] [G loss: 3.008240]\n",
      "epoch:30 step:23831 [D loss: 0.269618, acc.: 85.94%] [G loss: 3.671200]\n",
      "epoch:30 step:23832 [D loss: 0.227520, acc.: 89.84%] [G loss: 3.387767]\n",
      "epoch:30 step:23833 [D loss: 0.260943, acc.: 89.84%] [G loss: 4.053345]\n",
      "epoch:30 step:23834 [D loss: 0.348390, acc.: 84.38%] [G loss: 3.660316]\n",
      "epoch:30 step:23835 [D loss: 0.299724, acc.: 88.28%] [G loss: 3.123025]\n",
      "epoch:30 step:23836 [D loss: 0.520841, acc.: 78.12%] [G loss: 2.981065]\n",
      "epoch:30 step:23837 [D loss: 0.292637, acc.: 87.50%] [G loss: 3.541564]\n",
      "epoch:30 step:23838 [D loss: 0.400649, acc.: 83.59%] [G loss: 2.351397]\n",
      "epoch:30 step:23839 [D loss: 0.327157, acc.: 83.59%] [G loss: 4.331972]\n",
      "epoch:30 step:23840 [D loss: 0.381027, acc.: 79.69%] [G loss: 2.977855]\n",
      "epoch:30 step:23841 [D loss: 0.458136, acc.: 79.69%] [G loss: 5.424977]\n",
      "epoch:30 step:23842 [D loss: 0.466858, acc.: 77.34%] [G loss: 5.821900]\n",
      "epoch:30 step:23843 [D loss: 0.595759, acc.: 71.09%] [G loss: 2.886145]\n",
      "epoch:30 step:23844 [D loss: 0.354549, acc.: 87.50%] [G loss: 2.593766]\n",
      "epoch:30 step:23845 [D loss: 0.384885, acc.: 80.47%] [G loss: 3.432825]\n",
      "epoch:30 step:23846 [D loss: 0.316762, acc.: 90.62%] [G loss: 2.882441]\n",
      "epoch:30 step:23847 [D loss: 0.345027, acc.: 85.94%] [G loss: 3.256031]\n",
      "epoch:30 step:23848 [D loss: 0.321899, acc.: 87.50%] [G loss: 2.397514]\n",
      "epoch:30 step:23849 [D loss: 0.252959, acc.: 91.41%] [G loss: 3.244571]\n",
      "epoch:30 step:23850 [D loss: 0.340367, acc.: 85.94%] [G loss: 2.928424]\n",
      "epoch:30 step:23851 [D loss: 0.220891, acc.: 92.19%] [G loss: 3.594961]\n",
      "epoch:30 step:23852 [D loss: 0.322722, acc.: 85.16%] [G loss: 4.110296]\n",
      "epoch:30 step:23853 [D loss: 0.344672, acc.: 83.59%] [G loss: 4.407211]\n",
      "epoch:30 step:23854 [D loss: 0.327466, acc.: 84.38%] [G loss: 3.263311]\n",
      "epoch:30 step:23855 [D loss: 0.367614, acc.: 81.25%] [G loss: 3.644060]\n",
      "epoch:30 step:23856 [D loss: 0.321867, acc.: 85.94%] [G loss: 3.223361]\n",
      "epoch:30 step:23857 [D loss: 0.325524, acc.: 82.03%] [G loss: 2.813414]\n",
      "epoch:30 step:23858 [D loss: 0.441536, acc.: 78.91%] [G loss: 3.192042]\n",
      "epoch:30 step:23859 [D loss: 0.278275, acc.: 89.06%] [G loss: 2.974503]\n",
      "epoch:30 step:23860 [D loss: 0.334329, acc.: 82.81%] [G loss: 3.491235]\n",
      "epoch:30 step:23861 [D loss: 0.359425, acc.: 82.81%] [G loss: 3.403561]\n",
      "epoch:30 step:23862 [D loss: 0.312779, acc.: 85.16%] [G loss: 3.424223]\n",
      "epoch:30 step:23863 [D loss: 0.245582, acc.: 89.84%] [G loss: 3.331837]\n",
      "epoch:30 step:23864 [D loss: 0.319595, acc.: 85.94%] [G loss: 3.698183]\n",
      "epoch:30 step:23865 [D loss: 0.344607, acc.: 84.38%] [G loss: 4.290904]\n",
      "epoch:30 step:23866 [D loss: 0.348363, acc.: 82.03%] [G loss: 2.910491]\n",
      "epoch:30 step:23867 [D loss: 0.325500, acc.: 85.94%] [G loss: 3.224983]\n",
      "epoch:30 step:23868 [D loss: 0.399032, acc.: 79.69%] [G loss: 2.511137]\n",
      "epoch:30 step:23869 [D loss: 0.269492, acc.: 88.28%] [G loss: 3.003953]\n",
      "epoch:30 step:23870 [D loss: 0.327473, acc.: 86.72%] [G loss: 2.896832]\n",
      "epoch:30 step:23871 [D loss: 0.426168, acc.: 79.69%] [G loss: 2.793690]\n",
      "epoch:30 step:23872 [D loss: 0.231317, acc.: 89.84%] [G loss: 5.064855]\n",
      "epoch:30 step:23873 [D loss: 0.300964, acc.: 88.28%] [G loss: 4.002857]\n",
      "epoch:30 step:23874 [D loss: 0.275263, acc.: 88.28%] [G loss: 2.844925]\n",
      "epoch:30 step:23875 [D loss: 0.255412, acc.: 88.28%] [G loss: 4.417497]\n",
      "epoch:30 step:23876 [D loss: 0.276679, acc.: 87.50%] [G loss: 3.075486]\n",
      "epoch:30 step:23877 [D loss: 0.288559, acc.: 85.16%] [G loss: 3.098371]\n",
      "epoch:30 step:23878 [D loss: 0.283543, acc.: 84.38%] [G loss: 3.964231]\n",
      "epoch:30 step:23879 [D loss: 0.372859, acc.: 82.81%] [G loss: 3.265970]\n",
      "epoch:30 step:23880 [D loss: 0.366534, acc.: 82.03%] [G loss: 2.787955]\n",
      "epoch:30 step:23881 [D loss: 0.391130, acc.: 79.69%] [G loss: 4.251112]\n",
      "epoch:30 step:23882 [D loss: 0.364733, acc.: 85.16%] [G loss: 2.798729]\n",
      "epoch:30 step:23883 [D loss: 0.266382, acc.: 90.62%] [G loss: 3.009996]\n",
      "epoch:30 step:23884 [D loss: 0.284802, acc.: 88.28%] [G loss: 3.832530]\n",
      "epoch:30 step:23885 [D loss: 0.360522, acc.: 85.16%] [G loss: 6.224423]\n",
      "epoch:30 step:23886 [D loss: 0.796674, acc.: 69.53%] [G loss: 7.042167]\n",
      "epoch:30 step:23887 [D loss: 2.183266, acc.: 55.47%] [G loss: 9.386701]\n",
      "epoch:30 step:23888 [D loss: 1.678717, acc.: 56.25%] [G loss: 2.097977]\n",
      "epoch:30 step:23889 [D loss: 0.628156, acc.: 77.34%] [G loss: 2.963139]\n",
      "epoch:30 step:23890 [D loss: 0.427406, acc.: 80.47%] [G loss: 4.128451]\n",
      "epoch:30 step:23891 [D loss: 0.376773, acc.: 86.72%] [G loss: 4.754378]\n",
      "epoch:30 step:23892 [D loss: 0.453992, acc.: 81.25%] [G loss: 3.360090]\n",
      "epoch:30 step:23893 [D loss: 0.490091, acc.: 78.12%] [G loss: 3.295613]\n",
      "epoch:30 step:23894 [D loss: 0.284172, acc.: 88.28%] [G loss: 3.133359]\n",
      "epoch:30 step:23895 [D loss: 0.415712, acc.: 78.91%] [G loss: 2.479753]\n",
      "epoch:30 step:23896 [D loss: 0.341786, acc.: 84.38%] [G loss: 2.650110]\n",
      "epoch:30 step:23897 [D loss: 0.356960, acc.: 79.69%] [G loss: 3.133949]\n",
      "epoch:30 step:23898 [D loss: 0.312313, acc.: 88.28%] [G loss: 2.777196]\n",
      "epoch:30 step:23899 [D loss: 0.283949, acc.: 88.28%] [G loss: 2.537151]\n",
      "epoch:30 step:23900 [D loss: 0.340647, acc.: 85.94%] [G loss: 3.286650]\n",
      "epoch:30 step:23901 [D loss: 0.309874, acc.: 84.38%] [G loss: 3.678055]\n",
      "epoch:30 step:23902 [D loss: 0.276704, acc.: 86.72%] [G loss: 3.488717]\n",
      "epoch:30 step:23903 [D loss: 0.318580, acc.: 85.94%] [G loss: 2.558524]\n",
      "epoch:30 step:23904 [D loss: 0.247233, acc.: 90.62%] [G loss: 3.049207]\n",
      "epoch:30 step:23905 [D loss: 0.233790, acc.: 89.84%] [G loss: 2.575963]\n",
      "epoch:30 step:23906 [D loss: 0.452551, acc.: 81.25%] [G loss: 2.730861]\n",
      "epoch:30 step:23907 [D loss: 0.501318, acc.: 82.03%] [G loss: 2.836998]\n",
      "epoch:30 step:23908 [D loss: 0.389673, acc.: 84.38%] [G loss: 3.086343]\n",
      "epoch:30 step:23909 [D loss: 0.306717, acc.: 85.16%] [G loss: 3.014250]\n",
      "epoch:30 step:23910 [D loss: 0.386152, acc.: 78.91%] [G loss: 2.566408]\n",
      "epoch:30 step:23911 [D loss: 0.260006, acc.: 89.84%] [G loss: 3.039783]\n",
      "epoch:30 step:23912 [D loss: 0.302572, acc.: 86.72%] [G loss: 2.666926]\n",
      "epoch:30 step:23913 [D loss: 0.333803, acc.: 86.72%] [G loss: 2.712973]\n",
      "epoch:30 step:23914 [D loss: 0.358111, acc.: 82.03%] [G loss: 3.707974]\n",
      "epoch:30 step:23915 [D loss: 0.350356, acc.: 81.25%] [G loss: 3.251138]\n",
      "epoch:30 step:23916 [D loss: 0.325769, acc.: 85.16%] [G loss: 3.022898]\n",
      "epoch:30 step:23917 [D loss: 0.246693, acc.: 88.28%] [G loss: 3.714418]\n",
      "epoch:30 step:23918 [D loss: 0.326139, acc.: 84.38%] [G loss: 2.656193]\n",
      "epoch:30 step:23919 [D loss: 0.334176, acc.: 85.94%] [G loss: 3.288426]\n",
      "epoch:30 step:23920 [D loss: 0.289623, acc.: 89.84%] [G loss: 4.109476]\n",
      "epoch:30 step:23921 [D loss: 0.307112, acc.: 86.72%] [G loss: 3.105047]\n",
      "epoch:30 step:23922 [D loss: 0.337725, acc.: 83.59%] [G loss: 3.690878]\n",
      "epoch:30 step:23923 [D loss: 0.245226, acc.: 88.28%] [G loss: 2.883725]\n",
      "epoch:30 step:23924 [D loss: 0.284005, acc.: 86.72%] [G loss: 3.573125]\n",
      "epoch:30 step:23925 [D loss: 0.285998, acc.: 85.94%] [G loss: 3.746750]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:30 step:23926 [D loss: 0.296660, acc.: 87.50%] [G loss: 3.047647]\n",
      "epoch:30 step:23927 [D loss: 0.338629, acc.: 82.03%] [G loss: 3.102859]\n",
      "epoch:30 step:23928 [D loss: 0.285303, acc.: 89.06%] [G loss: 3.445562]\n",
      "epoch:30 step:23929 [D loss: 0.274267, acc.: 87.50%] [G loss: 3.081554]\n",
      "epoch:30 step:23930 [D loss: 0.466239, acc.: 76.56%] [G loss: 3.857969]\n",
      "epoch:30 step:23931 [D loss: 0.390254, acc.: 82.81%] [G loss: 3.130972]\n",
      "epoch:30 step:23932 [D loss: 0.344898, acc.: 83.59%] [G loss: 2.525570]\n",
      "epoch:30 step:23933 [D loss: 0.286585, acc.: 86.72%] [G loss: 2.080412]\n",
      "epoch:30 step:23934 [D loss: 0.367371, acc.: 86.72%] [G loss: 2.380401]\n",
      "epoch:30 step:23935 [D loss: 0.377838, acc.: 82.81%] [G loss: 2.811204]\n",
      "epoch:30 step:23936 [D loss: 0.307196, acc.: 85.94%] [G loss: 3.114697]\n",
      "epoch:30 step:23937 [D loss: 0.333766, acc.: 86.72%] [G loss: 2.648894]\n",
      "epoch:30 step:23938 [D loss: 0.380819, acc.: 82.03%] [G loss: 2.895925]\n",
      "epoch:30 step:23939 [D loss: 0.396652, acc.: 80.47%] [G loss: 2.907101]\n",
      "epoch:30 step:23940 [D loss: 0.320967, acc.: 86.72%] [G loss: 3.151473]\n",
      "epoch:30 step:23941 [D loss: 0.318332, acc.: 85.94%] [G loss: 2.727683]\n",
      "epoch:30 step:23942 [D loss: 0.310591, acc.: 84.38%] [G loss: 2.553113]\n",
      "epoch:30 step:23943 [D loss: 0.443949, acc.: 76.56%] [G loss: 3.124009]\n",
      "epoch:30 step:23944 [D loss: 0.390217, acc.: 82.81%] [G loss: 3.070338]\n",
      "epoch:30 step:23945 [D loss: 0.311112, acc.: 85.16%] [G loss: 3.111500]\n",
      "epoch:30 step:23946 [D loss: 0.275291, acc.: 90.62%] [G loss: 3.317219]\n",
      "epoch:30 step:23947 [D loss: 0.312676, acc.: 85.16%] [G loss: 3.022550]\n",
      "epoch:30 step:23948 [D loss: 0.357856, acc.: 80.47%] [G loss: 3.249505]\n",
      "epoch:30 step:23949 [D loss: 0.376532, acc.: 82.03%] [G loss: 3.950886]\n",
      "epoch:30 step:23950 [D loss: 0.452546, acc.: 76.56%] [G loss: 5.100832]\n",
      "epoch:30 step:23951 [D loss: 0.354104, acc.: 82.81%] [G loss: 3.347216]\n",
      "epoch:30 step:23952 [D loss: 0.313024, acc.: 85.16%] [G loss: 2.765135]\n",
      "epoch:30 step:23953 [D loss: 0.254430, acc.: 86.72%] [G loss: 3.822372]\n",
      "epoch:30 step:23954 [D loss: 0.345465, acc.: 84.38%] [G loss: 3.265050]\n",
      "epoch:30 step:23955 [D loss: 0.294875, acc.: 87.50%] [G loss: 2.446869]\n",
      "epoch:30 step:23956 [D loss: 0.462409, acc.: 78.12%] [G loss: 4.102334]\n",
      "epoch:30 step:23957 [D loss: 0.362329, acc.: 84.38%] [G loss: 3.060370]\n",
      "epoch:30 step:23958 [D loss: 0.315985, acc.: 87.50%] [G loss: 3.244038]\n",
      "epoch:30 step:23959 [D loss: 0.488416, acc.: 77.34%] [G loss: 3.823035]\n",
      "epoch:30 step:23960 [D loss: 0.565657, acc.: 76.56%] [G loss: 6.084013]\n",
      "epoch:30 step:23961 [D loss: 0.864062, acc.: 75.00%] [G loss: 6.418973]\n",
      "epoch:30 step:23962 [D loss: 0.918609, acc.: 67.19%] [G loss: 4.745432]\n",
      "epoch:30 step:23963 [D loss: 0.822968, acc.: 61.72%] [G loss: 2.866574]\n",
      "epoch:30 step:23964 [D loss: 0.377189, acc.: 78.91%] [G loss: 3.450553]\n",
      "epoch:30 step:23965 [D loss: 0.333692, acc.: 86.72%] [G loss: 4.988016]\n",
      "epoch:30 step:23966 [D loss: 0.326505, acc.: 84.38%] [G loss: 3.609162]\n",
      "epoch:30 step:23967 [D loss: 0.387567, acc.: 81.25%] [G loss: 2.954273]\n",
      "epoch:30 step:23968 [D loss: 0.279363, acc.: 85.94%] [G loss: 3.444281]\n",
      "epoch:30 step:23969 [D loss: 0.355275, acc.: 82.03%] [G loss: 3.298359]\n",
      "epoch:30 step:23970 [D loss: 0.433201, acc.: 77.34%] [G loss: 2.994757]\n",
      "epoch:30 step:23971 [D loss: 0.344139, acc.: 85.16%] [G loss: 4.244534]\n",
      "epoch:30 step:23972 [D loss: 0.271698, acc.: 87.50%] [G loss: 3.599632]\n",
      "epoch:30 step:23973 [D loss: 0.342951, acc.: 85.94%] [G loss: 2.971130]\n",
      "epoch:30 step:23974 [D loss: 0.338463, acc.: 88.28%] [G loss: 2.850945]\n",
      "epoch:30 step:23975 [D loss: 0.316156, acc.: 85.94%] [G loss: 2.708358]\n",
      "epoch:30 step:23976 [D loss: 0.207084, acc.: 90.62%] [G loss: 3.628178]\n",
      "epoch:30 step:23977 [D loss: 0.317098, acc.: 83.59%] [G loss: 2.648395]\n",
      "epoch:30 step:23978 [D loss: 0.287571, acc.: 86.72%] [G loss: 3.325839]\n",
      "epoch:30 step:23979 [D loss: 0.316975, acc.: 82.03%] [G loss: 3.232131]\n",
      "epoch:30 step:23980 [D loss: 0.317661, acc.: 85.16%] [G loss: 2.616189]\n",
      "epoch:30 step:23981 [D loss: 0.326138, acc.: 82.81%] [G loss: 2.591410]\n",
      "epoch:30 step:23982 [D loss: 0.426828, acc.: 82.03%] [G loss: 2.382447]\n",
      "epoch:30 step:23983 [D loss: 0.338189, acc.: 82.81%] [G loss: 2.796493]\n",
      "epoch:30 step:23984 [D loss: 0.396117, acc.: 80.47%] [G loss: 3.896250]\n",
      "epoch:30 step:23985 [D loss: 0.294052, acc.: 87.50%] [G loss: 3.305298]\n",
      "epoch:30 step:23986 [D loss: 0.290277, acc.: 85.16%] [G loss: 3.326638]\n",
      "epoch:30 step:23987 [D loss: 0.352214, acc.: 81.25%] [G loss: 3.201842]\n",
      "epoch:30 step:23988 [D loss: 0.393073, acc.: 81.25%] [G loss: 3.885548]\n",
      "epoch:30 step:23989 [D loss: 0.341207, acc.: 83.59%] [G loss: 2.949822]\n",
      "epoch:30 step:23990 [D loss: 0.409200, acc.: 84.38%] [G loss: 2.886098]\n",
      "epoch:30 step:23991 [D loss: 0.387317, acc.: 83.59%] [G loss: 2.631299]\n",
      "epoch:30 step:23992 [D loss: 0.348021, acc.: 82.81%] [G loss: 2.555820]\n",
      "epoch:30 step:23993 [D loss: 0.278037, acc.: 89.06%] [G loss: 2.827153]\n",
      "epoch:30 step:23994 [D loss: 0.319017, acc.: 84.38%] [G loss: 2.302029]\n",
      "epoch:30 step:23995 [D loss: 0.313805, acc.: 85.94%] [G loss: 2.793160]\n",
      "epoch:30 step:23996 [D loss: 0.267827, acc.: 88.28%] [G loss: 2.883421]\n",
      "epoch:30 step:23997 [D loss: 0.305858, acc.: 87.50%] [G loss: 2.419737]\n",
      "epoch:30 step:23998 [D loss: 0.297933, acc.: 88.28%] [G loss: 2.633397]\n",
      "epoch:30 step:23999 [D loss: 0.351715, acc.: 85.16%] [G loss: 3.189958]\n",
      "epoch:30 step:24000 [D loss: 0.247638, acc.: 91.41%] [G loss: 2.615842]\n",
      "##############\n",
      "[0.86079738 0.86262098 0.81059672 0.8070963  0.7538624  0.83969436\n",
      " 0.8673124  0.82682999 0.80460591 0.81911594]\n",
      "##########\n",
      "epoch:30 step:24001 [D loss: 0.335643, acc.: 85.16%] [G loss: 3.831714]\n",
      "epoch:30 step:24002 [D loss: 0.248803, acc.: 89.84%] [G loss: 3.379596]\n",
      "epoch:30 step:24003 [D loss: 0.356217, acc.: 81.25%] [G loss: 2.499310]\n",
      "epoch:30 step:24004 [D loss: 0.271475, acc.: 87.50%] [G loss: 2.529164]\n",
      "epoch:30 step:24005 [D loss: 0.345747, acc.: 85.94%] [G loss: 2.671244]\n",
      "epoch:30 step:24006 [D loss: 0.231888, acc.: 89.84%] [G loss: 4.011032]\n",
      "epoch:30 step:24007 [D loss: 0.290063, acc.: 85.16%] [G loss: 3.002328]\n",
      "epoch:30 step:24008 [D loss: 0.321048, acc.: 84.38%] [G loss: 4.497976]\n",
      "epoch:30 step:24009 [D loss: 0.249900, acc.: 89.06%] [G loss: 3.785844]\n",
      "epoch:30 step:24010 [D loss: 0.310196, acc.: 85.94%] [G loss: 3.216452]\n",
      "epoch:30 step:24011 [D loss: 0.312913, acc.: 85.94%] [G loss: 3.011983]\n",
      "epoch:30 step:24012 [D loss: 0.379644, acc.: 81.25%] [G loss: 3.071082]\n",
      "epoch:30 step:24013 [D loss: 0.278149, acc.: 86.72%] [G loss: 3.578603]\n",
      "epoch:30 step:24014 [D loss: 0.282547, acc.: 86.72%] [G loss: 2.747577]\n",
      "epoch:30 step:24015 [D loss: 0.241049, acc.: 91.41%] [G loss: 3.386393]\n",
      "epoch:30 step:24016 [D loss: 0.270612, acc.: 87.50%] [G loss: 3.405240]\n",
      "epoch:30 step:24017 [D loss: 0.293020, acc.: 86.72%] [G loss: 3.716723]\n",
      "epoch:30 step:24018 [D loss: 0.385318, acc.: 81.25%] [G loss: 2.548225]\n",
      "epoch:30 step:24019 [D loss: 0.343328, acc.: 85.94%] [G loss: 2.635093]\n",
      "epoch:30 step:24020 [D loss: 0.339038, acc.: 85.94%] [G loss: 4.250304]\n",
      "epoch:30 step:24021 [D loss: 0.336486, acc.: 82.03%] [G loss: 3.447814]\n",
      "epoch:30 step:24022 [D loss: 0.203566, acc.: 94.53%] [G loss: 3.156716]\n",
      "epoch:30 step:24023 [D loss: 0.303548, acc.: 85.94%] [G loss: 3.535293]\n",
      "epoch:30 step:24024 [D loss: 0.228706, acc.: 90.62%] [G loss: 3.092086]\n",
      "epoch:30 step:24025 [D loss: 0.401841, acc.: 79.69%] [G loss: 2.418123]\n",
      "epoch:30 step:24026 [D loss: 0.318538, acc.: 84.38%] [G loss: 2.360122]\n",
      "epoch:30 step:24027 [D loss: 0.388646, acc.: 85.94%] [G loss: 2.287624]\n",
      "epoch:30 step:24028 [D loss: 0.423771, acc.: 82.03%] [G loss: 2.948596]\n",
      "epoch:30 step:24029 [D loss: 0.310627, acc.: 83.59%] [G loss: 3.906916]\n",
      "epoch:30 step:24030 [D loss: 0.249740, acc.: 88.28%] [G loss: 3.652771]\n",
      "epoch:30 step:24031 [D loss: 0.315676, acc.: 87.50%] [G loss: 4.021531]\n",
      "epoch:30 step:24032 [D loss: 0.267521, acc.: 87.50%] [G loss: 3.610052]\n",
      "epoch:30 step:24033 [D loss: 0.355184, acc.: 79.69%] [G loss: 2.919564]\n",
      "epoch:30 step:24034 [D loss: 0.288610, acc.: 89.06%] [G loss: 2.724697]\n",
      "epoch:30 step:24035 [D loss: 0.366002, acc.: 79.69%] [G loss: 3.304760]\n",
      "epoch:30 step:24036 [D loss: 0.378473, acc.: 82.03%] [G loss: 2.987107]\n",
      "epoch:30 step:24037 [D loss: 0.254455, acc.: 89.06%] [G loss: 2.937990]\n",
      "epoch:30 step:24038 [D loss: 0.215762, acc.: 89.06%] [G loss: 4.358360]\n",
      "epoch:30 step:24039 [D loss: 0.395418, acc.: 80.47%] [G loss: 2.468404]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:30 step:24040 [D loss: 0.431613, acc.: 81.25%] [G loss: 2.931794]\n",
      "epoch:30 step:24041 [D loss: 0.280055, acc.: 90.62%] [G loss: 2.887015]\n",
      "epoch:30 step:24042 [D loss: 0.359778, acc.: 83.59%] [G loss: 3.352189]\n",
      "epoch:30 step:24043 [D loss: 0.347296, acc.: 85.94%] [G loss: 3.415548]\n",
      "epoch:30 step:24044 [D loss: 0.242626, acc.: 87.50%] [G loss: 3.467557]\n",
      "epoch:30 step:24045 [D loss: 0.317031, acc.: 87.50%] [G loss: 3.252493]\n",
      "epoch:30 step:24046 [D loss: 0.370886, acc.: 87.50%] [G loss: 3.173726]\n",
      "epoch:30 step:24047 [D loss: 0.394342, acc.: 85.94%] [G loss: 3.502278]\n",
      "epoch:30 step:24048 [D loss: 0.278176, acc.: 88.28%] [G loss: 3.414907]\n",
      "epoch:30 step:24049 [D loss: 0.276053, acc.: 85.94%] [G loss: 3.222489]\n",
      "epoch:30 step:24050 [D loss: 0.314195, acc.: 85.94%] [G loss: 3.311318]\n",
      "epoch:30 step:24051 [D loss: 0.280187, acc.: 84.38%] [G loss: 3.154889]\n",
      "epoch:30 step:24052 [D loss: 0.308494, acc.: 81.25%] [G loss: 3.793758]\n",
      "epoch:30 step:24053 [D loss: 0.414808, acc.: 80.47%] [G loss: 3.488874]\n",
      "epoch:30 step:24054 [D loss: 0.326191, acc.: 85.94%] [G loss: 3.252396]\n",
      "epoch:30 step:24055 [D loss: 0.387451, acc.: 83.59%] [G loss: 3.223291]\n",
      "epoch:30 step:24056 [D loss: 0.252002, acc.: 89.06%] [G loss: 3.951131]\n",
      "epoch:30 step:24057 [D loss: 0.270532, acc.: 86.72%] [G loss: 4.100800]\n",
      "epoch:30 step:24058 [D loss: 0.345096, acc.: 83.59%] [G loss: 2.932341]\n",
      "epoch:30 step:24059 [D loss: 0.258394, acc.: 90.62%] [G loss: 3.792194]\n",
      "epoch:30 step:24060 [D loss: 0.326991, acc.: 85.94%] [G loss: 4.941926]\n",
      "epoch:30 step:24061 [D loss: 0.277463, acc.: 89.84%] [G loss: 2.532526]\n",
      "epoch:30 step:24062 [D loss: 0.360029, acc.: 80.47%] [G loss: 2.996965]\n",
      "epoch:30 step:24063 [D loss: 0.354438, acc.: 85.16%] [G loss: 2.745437]\n",
      "epoch:30 step:24064 [D loss: 0.290728, acc.: 88.28%] [G loss: 3.298830]\n",
      "epoch:30 step:24065 [D loss: 0.357654, acc.: 85.16%] [G loss: 2.354162]\n",
      "epoch:30 step:24066 [D loss: 0.270963, acc.: 89.84%] [G loss: 2.851686]\n",
      "epoch:30 step:24067 [D loss: 0.314242, acc.: 85.94%] [G loss: 2.838652]\n",
      "epoch:30 step:24068 [D loss: 0.307145, acc.: 82.81%] [G loss: 3.026860]\n",
      "epoch:30 step:24069 [D loss: 0.454821, acc.: 78.12%] [G loss: 2.648376]\n",
      "epoch:30 step:24070 [D loss: 0.204900, acc.: 92.19%] [G loss: 3.376686]\n",
      "epoch:30 step:24071 [D loss: 0.331931, acc.: 86.72%] [G loss: 3.557338]\n",
      "epoch:30 step:24072 [D loss: 0.258077, acc.: 89.84%] [G loss: 2.458453]\n",
      "epoch:30 step:24073 [D loss: 0.341645, acc.: 84.38%] [G loss: 3.610725]\n",
      "epoch:30 step:24074 [D loss: 0.469903, acc.: 78.12%] [G loss: 3.374646]\n",
      "epoch:30 step:24075 [D loss: 0.358357, acc.: 84.38%] [G loss: 3.066792]\n",
      "epoch:30 step:24076 [D loss: 0.384864, acc.: 83.59%] [G loss: 3.052500]\n",
      "epoch:30 step:24077 [D loss: 0.313969, acc.: 85.94%] [G loss: 3.403504]\n",
      "epoch:30 step:24078 [D loss: 0.365039, acc.: 83.59%] [G loss: 4.942976]\n",
      "epoch:30 step:24079 [D loss: 0.345831, acc.: 85.94%] [G loss: 3.579042]\n",
      "epoch:30 step:24080 [D loss: 0.396536, acc.: 80.47%] [G loss: 5.754467]\n",
      "epoch:30 step:24081 [D loss: 0.386871, acc.: 82.03%] [G loss: 3.599205]\n",
      "epoch:30 step:24082 [D loss: 0.241602, acc.: 92.19%] [G loss: 3.787681]\n",
      "epoch:30 step:24083 [D loss: 0.256329, acc.: 89.84%] [G loss: 4.263690]\n",
      "epoch:30 step:24084 [D loss: 0.405811, acc.: 81.25%] [G loss: 4.469012]\n",
      "epoch:30 step:24085 [D loss: 0.348634, acc.: 83.59%] [G loss: 4.490462]\n",
      "epoch:30 step:24086 [D loss: 0.255996, acc.: 87.50%] [G loss: 3.037894]\n",
      "epoch:30 step:24087 [D loss: 0.378721, acc.: 85.16%] [G loss: 3.904244]\n",
      "epoch:30 step:24088 [D loss: 0.404672, acc.: 78.12%] [G loss: 3.611842]\n",
      "epoch:30 step:24089 [D loss: 0.372469, acc.: 83.59%] [G loss: 3.680258]\n",
      "epoch:30 step:24090 [D loss: 0.329177, acc.: 83.59%] [G loss: 3.283690]\n",
      "epoch:30 step:24091 [D loss: 0.341355, acc.: 85.16%] [G loss: 3.023497]\n",
      "epoch:30 step:24092 [D loss: 0.381040, acc.: 84.38%] [G loss: 2.899462]\n",
      "epoch:30 step:24093 [D loss: 0.338104, acc.: 83.59%] [G loss: 2.501837]\n",
      "epoch:30 step:24094 [D loss: 0.317239, acc.: 86.72%] [G loss: 2.757200]\n",
      "epoch:30 step:24095 [D loss: 0.428725, acc.: 81.25%] [G loss: 2.868329]\n",
      "epoch:30 step:24096 [D loss: 0.386234, acc.: 85.94%] [G loss: 2.775800]\n",
      "epoch:30 step:24097 [D loss: 0.351448, acc.: 81.25%] [G loss: 3.433655]\n",
      "epoch:30 step:24098 [D loss: 0.319179, acc.: 84.38%] [G loss: 4.061522]\n",
      "epoch:30 step:24099 [D loss: 0.537289, acc.: 76.56%] [G loss: 4.278012]\n",
      "epoch:30 step:24100 [D loss: 0.467985, acc.: 76.56%] [G loss: 3.462564]\n",
      "epoch:30 step:24101 [D loss: 0.324150, acc.: 83.59%] [G loss: 3.870739]\n",
      "epoch:30 step:24102 [D loss: 0.358920, acc.: 79.69%] [G loss: 3.594861]\n",
      "epoch:30 step:24103 [D loss: 0.311135, acc.: 88.28%] [G loss: 3.795700]\n",
      "epoch:30 step:24104 [D loss: 0.306155, acc.: 83.59%] [G loss: 3.647403]\n",
      "epoch:30 step:24105 [D loss: 0.319784, acc.: 89.06%] [G loss: 4.707535]\n",
      "epoch:30 step:24106 [D loss: 0.328400, acc.: 83.59%] [G loss: 4.288938]\n",
      "epoch:30 step:24107 [D loss: 0.361667, acc.: 83.59%] [G loss: 2.723618]\n",
      "epoch:30 step:24108 [D loss: 0.358063, acc.: 84.38%] [G loss: 3.828328]\n",
      "epoch:30 step:24109 [D loss: 0.260747, acc.: 89.06%] [G loss: 3.585770]\n",
      "epoch:30 step:24110 [D loss: 0.237776, acc.: 92.19%] [G loss: 2.573683]\n",
      "epoch:30 step:24111 [D loss: 0.268185, acc.: 86.72%] [G loss: 2.976004]\n",
      "epoch:30 step:24112 [D loss: 0.237604, acc.: 92.19%] [G loss: 2.464576]\n",
      "epoch:30 step:24113 [D loss: 0.327255, acc.: 85.16%] [G loss: 2.881430]\n",
      "epoch:30 step:24114 [D loss: 0.344592, acc.: 83.59%] [G loss: 2.674407]\n",
      "epoch:30 step:24115 [D loss: 0.361637, acc.: 84.38%] [G loss: 2.305618]\n",
      "epoch:30 step:24116 [D loss: 0.348881, acc.: 85.94%] [G loss: 3.558373]\n",
      "epoch:30 step:24117 [D loss: 0.307114, acc.: 83.59%] [G loss: 3.797840]\n",
      "epoch:30 step:24118 [D loss: 0.308225, acc.: 86.72%] [G loss: 2.877667]\n",
      "epoch:30 step:24119 [D loss: 0.355298, acc.: 87.50%] [G loss: 3.783493]\n",
      "epoch:30 step:24120 [D loss: 0.439446, acc.: 75.78%] [G loss: 4.489559]\n",
      "epoch:30 step:24121 [D loss: 0.299959, acc.: 89.84%] [G loss: 3.413270]\n",
      "epoch:30 step:24122 [D loss: 0.409855, acc.: 85.16%] [G loss: 5.015831]\n",
      "epoch:30 step:24123 [D loss: 0.751979, acc.: 68.75%] [G loss: 4.028678]\n",
      "epoch:30 step:24124 [D loss: 0.686988, acc.: 71.88%] [G loss: 3.354827]\n",
      "epoch:30 step:24125 [D loss: 0.377860, acc.: 78.12%] [G loss: 3.200594]\n",
      "epoch:30 step:24126 [D loss: 0.546629, acc.: 73.44%] [G loss: 5.778834]\n",
      "epoch:30 step:24127 [D loss: 0.376647, acc.: 84.38%] [G loss: 3.910566]\n",
      "epoch:30 step:24128 [D loss: 0.246116, acc.: 89.84%] [G loss: 5.193477]\n",
      "epoch:30 step:24129 [D loss: 0.215222, acc.: 90.62%] [G loss: 4.846870]\n",
      "epoch:30 step:24130 [D loss: 0.296017, acc.: 85.94%] [G loss: 3.946009]\n",
      "epoch:30 step:24131 [D loss: 0.343285, acc.: 82.81%] [G loss: 4.005700]\n",
      "epoch:30 step:24132 [D loss: 0.365099, acc.: 82.81%] [G loss: 3.573985]\n",
      "epoch:30 step:24133 [D loss: 0.353584, acc.: 85.16%] [G loss: 3.289945]\n",
      "epoch:30 step:24134 [D loss: 0.425800, acc.: 80.47%] [G loss: 2.704515]\n",
      "epoch:30 step:24135 [D loss: 0.254698, acc.: 90.62%] [G loss: 2.904824]\n",
      "epoch:30 step:24136 [D loss: 0.282678, acc.: 85.94%] [G loss: 3.273141]\n",
      "epoch:30 step:24137 [D loss: 0.427759, acc.: 78.91%] [G loss: 2.763659]\n",
      "epoch:30 step:24138 [D loss: 0.341565, acc.: 84.38%] [G loss: 2.310459]\n",
      "epoch:30 step:24139 [D loss: 0.391332, acc.: 83.59%] [G loss: 2.764681]\n",
      "epoch:30 step:24140 [D loss: 0.314428, acc.: 85.94%] [G loss: 2.907101]\n",
      "epoch:30 step:24141 [D loss: 0.315067, acc.: 87.50%] [G loss: 3.384672]\n",
      "epoch:30 step:24142 [D loss: 0.341636, acc.: 85.16%] [G loss: 3.038592]\n",
      "epoch:30 step:24143 [D loss: 0.295310, acc.: 83.59%] [G loss: 4.223796]\n",
      "epoch:30 step:24144 [D loss: 0.239551, acc.: 91.41%] [G loss: 2.781716]\n",
      "epoch:30 step:24145 [D loss: 0.320268, acc.: 83.59%] [G loss: 3.055334]\n",
      "epoch:30 step:24146 [D loss: 0.265826, acc.: 85.94%] [G loss: 3.404933]\n",
      "epoch:30 step:24147 [D loss: 0.351705, acc.: 82.81%] [G loss: 3.354827]\n",
      "epoch:30 step:24148 [D loss: 0.408216, acc.: 81.25%] [G loss: 3.770036]\n",
      "epoch:30 step:24149 [D loss: 0.382768, acc.: 86.72%] [G loss: 3.349098]\n",
      "epoch:30 step:24150 [D loss: 0.361529, acc.: 82.03%] [G loss: 2.695546]\n",
      "epoch:30 step:24151 [D loss: 0.428517, acc.: 82.81%] [G loss: 2.381063]\n",
      "epoch:30 step:24152 [D loss: 0.295449, acc.: 87.50%] [G loss: 2.770156]\n",
      "epoch:30 step:24153 [D loss: 0.304354, acc.: 88.28%] [G loss: 3.797472]\n",
      "epoch:30 step:24154 [D loss: 0.381144, acc.: 83.59%] [G loss: 2.489280]\n",
      "epoch:30 step:24155 [D loss: 0.324928, acc.: 84.38%] [G loss: 2.957316]\n",
      "epoch:30 step:24156 [D loss: 0.288658, acc.: 85.16%] [G loss: 2.815616]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:30 step:24157 [D loss: 0.418350, acc.: 83.59%] [G loss: 2.784681]\n",
      "epoch:30 step:24158 [D loss: 0.417746, acc.: 81.25%] [G loss: 2.948321]\n",
      "epoch:30 step:24159 [D loss: 0.314295, acc.: 89.06%] [G loss: 3.139026]\n",
      "epoch:30 step:24160 [D loss: 0.417031, acc.: 81.25%] [G loss: 2.881966]\n",
      "epoch:30 step:24161 [D loss: 0.401878, acc.: 83.59%] [G loss: 4.080231]\n",
      "epoch:30 step:24162 [D loss: 0.209391, acc.: 92.19%] [G loss: 3.889699]\n",
      "epoch:30 step:24163 [D loss: 0.374926, acc.: 86.72%] [G loss: 3.212169]\n",
      "epoch:30 step:24164 [D loss: 0.256175, acc.: 86.72%] [G loss: 4.266010]\n",
      "epoch:30 step:24165 [D loss: 0.266111, acc.: 91.41%] [G loss: 3.627002]\n",
      "epoch:30 step:24166 [D loss: 0.329192, acc.: 84.38%] [G loss: 2.996464]\n",
      "epoch:30 step:24167 [D loss: 0.282140, acc.: 89.84%] [G loss: 3.798780]\n",
      "epoch:30 step:24168 [D loss: 0.255588, acc.: 89.06%] [G loss: 3.395245]\n",
      "epoch:30 step:24169 [D loss: 0.265420, acc.: 86.72%] [G loss: 6.884235]\n",
      "epoch:30 step:24170 [D loss: 0.323317, acc.: 88.28%] [G loss: 5.851370]\n",
      "epoch:30 step:24171 [D loss: 0.231451, acc.: 90.62%] [G loss: 7.567252]\n",
      "epoch:30 step:24172 [D loss: 0.281066, acc.: 89.84%] [G loss: 6.084570]\n",
      "epoch:30 step:24173 [D loss: 0.234627, acc.: 90.62%] [G loss: 4.772729]\n",
      "epoch:30 step:24174 [D loss: 0.238842, acc.: 89.06%] [G loss: 5.060698]\n",
      "epoch:30 step:24175 [D loss: 0.287228, acc.: 87.50%] [G loss: 3.307257]\n",
      "epoch:30 step:24176 [D loss: 0.171760, acc.: 93.75%] [G loss: 3.433845]\n",
      "epoch:30 step:24177 [D loss: 0.266607, acc.: 84.38%] [G loss: 3.546551]\n",
      "epoch:30 step:24178 [D loss: 0.438115, acc.: 76.56%] [G loss: 2.516906]\n",
      "epoch:30 step:24179 [D loss: 0.230236, acc.: 91.41%] [G loss: 3.510448]\n",
      "epoch:30 step:24180 [D loss: 0.349981, acc.: 86.72%] [G loss: 3.188830]\n",
      "epoch:30 step:24181 [D loss: 0.251310, acc.: 86.72%] [G loss: 4.478034]\n",
      "epoch:30 step:24182 [D loss: 0.311020, acc.: 85.16%] [G loss: 2.585686]\n",
      "epoch:30 step:24183 [D loss: 0.301065, acc.: 83.59%] [G loss: 3.391348]\n",
      "epoch:30 step:24184 [D loss: 0.316159, acc.: 85.16%] [G loss: 3.957667]\n",
      "epoch:30 step:24185 [D loss: 0.342905, acc.: 81.25%] [G loss: 3.516055]\n",
      "epoch:30 step:24186 [D loss: 0.302086, acc.: 89.84%] [G loss: 2.700740]\n",
      "epoch:30 step:24187 [D loss: 0.298115, acc.: 89.84%] [G loss: 3.373457]\n",
      "epoch:30 step:24188 [D loss: 0.299153, acc.: 86.72%] [G loss: 2.444142]\n",
      "epoch:30 step:24189 [D loss: 0.369271, acc.: 78.91%] [G loss: 3.066232]\n",
      "epoch:30 step:24190 [D loss: 0.412691, acc.: 79.69%] [G loss: 3.909738]\n",
      "epoch:30 step:24191 [D loss: 0.214311, acc.: 89.06%] [G loss: 3.467847]\n",
      "epoch:30 step:24192 [D loss: 0.390981, acc.: 84.38%] [G loss: 3.372503]\n",
      "epoch:30 step:24193 [D loss: 0.366857, acc.: 81.25%] [G loss: 3.782443]\n",
      "epoch:30 step:24194 [D loss: 0.275076, acc.: 86.72%] [G loss: 3.589984]\n",
      "epoch:30 step:24195 [D loss: 0.429217, acc.: 78.12%] [G loss: 4.684250]\n",
      "epoch:30 step:24196 [D loss: 0.273115, acc.: 85.94%] [G loss: 3.583280]\n",
      "epoch:30 step:24197 [D loss: 0.268659, acc.: 88.28%] [G loss: 3.203243]\n",
      "epoch:30 step:24198 [D loss: 0.299953, acc.: 86.72%] [G loss: 2.960192]\n",
      "epoch:30 step:24199 [D loss: 0.378387, acc.: 80.47%] [G loss: 3.792607]\n",
      "epoch:30 step:24200 [D loss: 0.317887, acc.: 85.94%] [G loss: 3.022165]\n",
      "##############\n",
      "[0.87371079 0.87430612 0.77756264 0.80978211 0.76707227 0.83605216\n",
      " 0.85907873 0.83435657 0.83254961 0.82336664]\n",
      "##########\n",
      "epoch:30 step:24201 [D loss: 0.382322, acc.: 82.03%] [G loss: 3.089964]\n",
      "epoch:30 step:24202 [D loss: 0.470716, acc.: 71.88%] [G loss: 2.747511]\n",
      "epoch:30 step:24203 [D loss: 0.294243, acc.: 86.72%] [G loss: 2.875525]\n",
      "epoch:30 step:24204 [D loss: 0.329641, acc.: 83.59%] [G loss: 2.672682]\n",
      "epoch:30 step:24205 [D loss: 0.270271, acc.: 89.84%] [G loss: 3.819962]\n",
      "epoch:30 step:24206 [D loss: 0.302450, acc.: 84.38%] [G loss: 3.196913]\n",
      "epoch:30 step:24207 [D loss: 0.366711, acc.: 82.03%] [G loss: 6.830780]\n",
      "epoch:30 step:24208 [D loss: 0.432791, acc.: 80.47%] [G loss: 4.122623]\n",
      "epoch:30 step:24209 [D loss: 0.561553, acc.: 78.91%] [G loss: 3.497460]\n",
      "epoch:30 step:24210 [D loss: 0.742282, acc.: 77.34%] [G loss: 4.578989]\n",
      "epoch:30 step:24211 [D loss: 0.744325, acc.: 72.66%] [G loss: 3.807042]\n",
      "epoch:31 step:24212 [D loss: 0.421890, acc.: 80.47%] [G loss: 3.033786]\n",
      "epoch:31 step:24213 [D loss: 0.284062, acc.: 86.72%] [G loss: 3.375327]\n",
      "epoch:31 step:24214 [D loss: 0.549646, acc.: 77.34%] [G loss: 2.883358]\n",
      "epoch:31 step:24215 [D loss: 0.294461, acc.: 85.94%] [G loss: 3.279261]\n",
      "epoch:31 step:24216 [D loss: 0.334613, acc.: 81.25%] [G loss: 3.839218]\n",
      "epoch:31 step:24217 [D loss: 0.324377, acc.: 83.59%] [G loss: 3.221446]\n",
      "epoch:31 step:24218 [D loss: 0.318153, acc.: 85.16%] [G loss: 2.837378]\n",
      "epoch:31 step:24219 [D loss: 0.279255, acc.: 86.72%] [G loss: 2.914934]\n",
      "epoch:31 step:24220 [D loss: 0.351747, acc.: 82.81%] [G loss: 2.667225]\n",
      "epoch:31 step:24221 [D loss: 0.313684, acc.: 85.16%] [G loss: 3.043215]\n",
      "epoch:31 step:24222 [D loss: 0.299659, acc.: 85.94%] [G loss: 3.201546]\n",
      "epoch:31 step:24223 [D loss: 0.324245, acc.: 89.06%] [G loss: 2.838278]\n",
      "epoch:31 step:24224 [D loss: 0.263996, acc.: 90.62%] [G loss: 2.553011]\n",
      "epoch:31 step:24225 [D loss: 0.345870, acc.: 82.81%] [G loss: 2.771581]\n",
      "epoch:31 step:24226 [D loss: 0.361831, acc.: 82.81%] [G loss: 2.566858]\n",
      "epoch:31 step:24227 [D loss: 0.264836, acc.: 89.84%] [G loss: 2.904478]\n",
      "epoch:31 step:24228 [D loss: 0.313449, acc.: 85.16%] [G loss: 2.655139]\n",
      "epoch:31 step:24229 [D loss: 0.442790, acc.: 78.91%] [G loss: 2.627095]\n",
      "epoch:31 step:24230 [D loss: 0.368351, acc.: 84.38%] [G loss: 2.730285]\n",
      "epoch:31 step:24231 [D loss: 0.277159, acc.: 91.41%] [G loss: 2.600052]\n",
      "epoch:31 step:24232 [D loss: 0.313070, acc.: 83.59%] [G loss: 2.819211]\n",
      "epoch:31 step:24233 [D loss: 0.381862, acc.: 79.69%] [G loss: 2.393486]\n",
      "epoch:31 step:24234 [D loss: 0.402398, acc.: 81.25%] [G loss: 2.559436]\n",
      "epoch:31 step:24235 [D loss: 0.354209, acc.: 82.81%] [G loss: 2.929769]\n",
      "epoch:31 step:24236 [D loss: 0.243890, acc.: 89.84%] [G loss: 2.966970]\n",
      "epoch:31 step:24237 [D loss: 0.417257, acc.: 82.03%] [G loss: 2.653177]\n",
      "epoch:31 step:24238 [D loss: 0.327101, acc.: 83.59%] [G loss: 3.496574]\n",
      "epoch:31 step:24239 [D loss: 0.423873, acc.: 78.12%] [G loss: 3.378891]\n",
      "epoch:31 step:24240 [D loss: 0.498930, acc.: 76.56%] [G loss: 3.503636]\n",
      "epoch:31 step:24241 [D loss: 0.213292, acc.: 92.19%] [G loss: 4.941921]\n",
      "epoch:31 step:24242 [D loss: 0.413790, acc.: 79.69%] [G loss: 4.493656]\n",
      "epoch:31 step:24243 [D loss: 0.453191, acc.: 79.69%] [G loss: 3.499473]\n",
      "epoch:31 step:24244 [D loss: 0.337815, acc.: 83.59%] [G loss: 4.421443]\n",
      "epoch:31 step:24245 [D loss: 0.389327, acc.: 84.38%] [G loss: 2.918368]\n",
      "epoch:31 step:24246 [D loss: 0.282302, acc.: 89.06%] [G loss: 4.061417]\n",
      "epoch:31 step:24247 [D loss: 0.281431, acc.: 89.06%] [G loss: 3.197250]\n",
      "epoch:31 step:24248 [D loss: 0.337937, acc.: 84.38%] [G loss: 2.766755]\n",
      "epoch:31 step:24249 [D loss: 0.271220, acc.: 86.72%] [G loss: 2.382197]\n",
      "epoch:31 step:24250 [D loss: 0.317932, acc.: 88.28%] [G loss: 3.418322]\n",
      "epoch:31 step:24251 [D loss: 0.267099, acc.: 89.84%] [G loss: 2.703998]\n",
      "epoch:31 step:24252 [D loss: 0.385006, acc.: 81.25%] [G loss: 2.567041]\n",
      "epoch:31 step:24253 [D loss: 0.255473, acc.: 87.50%] [G loss: 2.944474]\n",
      "epoch:31 step:24254 [D loss: 0.442719, acc.: 79.69%] [G loss: 2.108509]\n",
      "epoch:31 step:24255 [D loss: 0.314048, acc.: 85.94%] [G loss: 2.563292]\n",
      "epoch:31 step:24256 [D loss: 0.306296, acc.: 87.50%] [G loss: 2.975986]\n",
      "epoch:31 step:24257 [D loss: 0.446499, acc.: 85.94%] [G loss: 2.620260]\n",
      "epoch:31 step:24258 [D loss: 0.355699, acc.: 82.81%] [G loss: 2.943312]\n",
      "epoch:31 step:24259 [D loss: 0.341474, acc.: 86.72%] [G loss: 3.104353]\n",
      "epoch:31 step:24260 [D loss: 0.203664, acc.: 90.62%] [G loss: 3.655076]\n",
      "epoch:31 step:24261 [D loss: 0.423897, acc.: 78.12%] [G loss: 5.358835]\n",
      "epoch:31 step:24262 [D loss: 0.429978, acc.: 82.03%] [G loss: 4.847732]\n",
      "epoch:31 step:24263 [D loss: 0.275433, acc.: 86.72%] [G loss: 4.907698]\n",
      "epoch:31 step:24264 [D loss: 0.304543, acc.: 84.38%] [G loss: 3.359029]\n",
      "epoch:31 step:24265 [D loss: 0.278622, acc.: 86.72%] [G loss: 4.449934]\n",
      "epoch:31 step:24266 [D loss: 0.318424, acc.: 88.28%] [G loss: 3.483126]\n",
      "epoch:31 step:24267 [D loss: 0.380757, acc.: 78.91%] [G loss: 4.542319]\n",
      "epoch:31 step:24268 [D loss: 0.326660, acc.: 82.81%] [G loss: 3.721155]\n",
      "epoch:31 step:24269 [D loss: 0.323858, acc.: 87.50%] [G loss: 3.656936]\n",
      "epoch:31 step:24270 [D loss: 0.320316, acc.: 85.16%] [G loss: 4.102720]\n",
      "epoch:31 step:24271 [D loss: 0.324586, acc.: 86.72%] [G loss: 3.065012]\n",
      "epoch:31 step:24272 [D loss: 0.318283, acc.: 86.72%] [G loss: 5.594025]\n",
      "epoch:31 step:24273 [D loss: 0.334519, acc.: 88.28%] [G loss: 3.687144]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:31 step:24274 [D loss: 0.269538, acc.: 89.06%] [G loss: 3.540295]\n",
      "epoch:31 step:24275 [D loss: 0.255509, acc.: 89.06%] [G loss: 3.054046]\n",
      "epoch:31 step:24276 [D loss: 0.370775, acc.: 78.91%] [G loss: 3.568395]\n",
      "epoch:31 step:24277 [D loss: 0.283877, acc.: 84.38%] [G loss: 3.022063]\n",
      "epoch:31 step:24278 [D loss: 0.312764, acc.: 85.16%] [G loss: 2.899844]\n",
      "epoch:31 step:24279 [D loss: 0.360138, acc.: 82.03%] [G loss: 2.568542]\n",
      "epoch:31 step:24280 [D loss: 0.345923, acc.: 83.59%] [G loss: 2.878268]\n",
      "epoch:31 step:24281 [D loss: 0.244783, acc.: 90.62%] [G loss: 2.847056]\n",
      "epoch:31 step:24282 [D loss: 0.362498, acc.: 86.72%] [G loss: 2.959092]\n",
      "epoch:31 step:24283 [D loss: 0.499610, acc.: 81.25%] [G loss: 3.085141]\n",
      "epoch:31 step:24284 [D loss: 0.373407, acc.: 83.59%] [G loss: 4.072374]\n",
      "epoch:31 step:24285 [D loss: 0.206744, acc.: 92.97%] [G loss: 4.759864]\n",
      "epoch:31 step:24286 [D loss: 0.305861, acc.: 86.72%] [G loss: 4.095446]\n",
      "epoch:31 step:24287 [D loss: 0.294229, acc.: 86.72%] [G loss: 4.380786]\n",
      "epoch:31 step:24288 [D loss: 0.339670, acc.: 81.25%] [G loss: 2.691486]\n",
      "epoch:31 step:24289 [D loss: 0.314693, acc.: 87.50%] [G loss: 3.804756]\n",
      "epoch:31 step:24290 [D loss: 0.369298, acc.: 82.81%] [G loss: 4.095841]\n",
      "epoch:31 step:24291 [D loss: 0.270436, acc.: 87.50%] [G loss: 5.392590]\n",
      "epoch:31 step:24292 [D loss: 0.327655, acc.: 86.72%] [G loss: 4.064903]\n",
      "epoch:31 step:24293 [D loss: 0.154834, acc.: 92.97%] [G loss: 7.029293]\n",
      "epoch:31 step:24294 [D loss: 0.310046, acc.: 87.50%] [G loss: 3.671607]\n",
      "epoch:31 step:24295 [D loss: 0.402625, acc.: 83.59%] [G loss: 3.648253]\n",
      "epoch:31 step:24296 [D loss: 0.250065, acc.: 89.06%] [G loss: 4.098164]\n",
      "epoch:31 step:24297 [D loss: 0.364938, acc.: 83.59%] [G loss: 3.078568]\n",
      "epoch:31 step:24298 [D loss: 0.383151, acc.: 82.81%] [G loss: 3.266494]\n",
      "epoch:31 step:24299 [D loss: 0.321869, acc.: 83.59%] [G loss: 3.123166]\n",
      "epoch:31 step:24300 [D loss: 0.328867, acc.: 85.16%] [G loss: 2.991282]\n",
      "epoch:31 step:24301 [D loss: 0.390792, acc.: 79.69%] [G loss: 3.717573]\n",
      "epoch:31 step:24302 [D loss: 0.351921, acc.: 85.94%] [G loss: 4.558977]\n",
      "epoch:31 step:24303 [D loss: 0.427678, acc.: 78.12%] [G loss: 2.953194]\n",
      "epoch:31 step:24304 [D loss: 0.277755, acc.: 88.28%] [G loss: 4.354817]\n",
      "epoch:31 step:24305 [D loss: 0.568700, acc.: 77.34%] [G loss: 3.938485]\n",
      "epoch:31 step:24306 [D loss: 0.430534, acc.: 79.69%] [G loss: 2.710476]\n",
      "epoch:31 step:24307 [D loss: 0.347598, acc.: 85.16%] [G loss: 2.281205]\n",
      "epoch:31 step:24308 [D loss: 0.351120, acc.: 79.69%] [G loss: 2.876013]\n",
      "epoch:31 step:24309 [D loss: 0.445399, acc.: 82.03%] [G loss: 3.018022]\n",
      "epoch:31 step:24310 [D loss: 0.380619, acc.: 82.81%] [G loss: 3.101865]\n",
      "epoch:31 step:24311 [D loss: 0.301903, acc.: 89.84%] [G loss: 2.860067]\n",
      "epoch:31 step:24312 [D loss: 0.368485, acc.: 79.69%] [G loss: 3.352151]\n",
      "epoch:31 step:24313 [D loss: 0.327540, acc.: 84.38%] [G loss: 2.896113]\n",
      "epoch:31 step:24314 [D loss: 0.349185, acc.: 84.38%] [G loss: 3.502337]\n",
      "epoch:31 step:24315 [D loss: 0.292309, acc.: 89.06%] [G loss: 2.669769]\n",
      "epoch:31 step:24316 [D loss: 0.318685, acc.: 84.38%] [G loss: 3.459668]\n",
      "epoch:31 step:24317 [D loss: 0.344160, acc.: 82.03%] [G loss: 2.477141]\n",
      "epoch:31 step:24318 [D loss: 0.408810, acc.: 78.91%] [G loss: 3.213623]\n",
      "epoch:31 step:24319 [D loss: 0.232707, acc.: 92.97%] [G loss: 2.709183]\n",
      "epoch:31 step:24320 [D loss: 0.438571, acc.: 82.03%] [G loss: 2.909582]\n",
      "epoch:31 step:24321 [D loss: 0.309582, acc.: 85.94%] [G loss: 2.914041]\n",
      "epoch:31 step:24322 [D loss: 0.314174, acc.: 85.16%] [G loss: 4.380307]\n",
      "epoch:31 step:24323 [D loss: 0.339770, acc.: 85.94%] [G loss: 3.827754]\n",
      "epoch:31 step:24324 [D loss: 0.273066, acc.: 89.06%] [G loss: 3.886231]\n",
      "epoch:31 step:24325 [D loss: 0.267579, acc.: 90.62%] [G loss: 4.245325]\n",
      "epoch:31 step:24326 [D loss: 0.339109, acc.: 85.94%] [G loss: 3.688620]\n",
      "epoch:31 step:24327 [D loss: 0.482282, acc.: 74.22%] [G loss: 2.860324]\n",
      "epoch:31 step:24328 [D loss: 0.388927, acc.: 82.03%] [G loss: 3.143214]\n",
      "epoch:31 step:24329 [D loss: 0.265221, acc.: 86.72%] [G loss: 3.672237]\n",
      "epoch:31 step:24330 [D loss: 0.263881, acc.: 89.06%] [G loss: 3.603750]\n",
      "epoch:31 step:24331 [D loss: 0.295033, acc.: 88.28%] [G loss: 3.410701]\n",
      "epoch:31 step:24332 [D loss: 0.273148, acc.: 87.50%] [G loss: 3.834617]\n",
      "epoch:31 step:24333 [D loss: 0.308139, acc.: 86.72%] [G loss: 3.786510]\n",
      "epoch:31 step:24334 [D loss: 0.268429, acc.: 89.06%] [G loss: 3.293998]\n",
      "epoch:31 step:24335 [D loss: 0.357608, acc.: 82.03%] [G loss: 2.927752]\n",
      "epoch:31 step:24336 [D loss: 0.303769, acc.: 86.72%] [G loss: 3.181263]\n",
      "epoch:31 step:24337 [D loss: 0.300636, acc.: 87.50%] [G loss: 2.635876]\n",
      "epoch:31 step:24338 [D loss: 0.493360, acc.: 82.81%] [G loss: 4.537504]\n",
      "epoch:31 step:24339 [D loss: 0.284251, acc.: 87.50%] [G loss: 3.895184]\n",
      "epoch:31 step:24340 [D loss: 0.323049, acc.: 83.59%] [G loss: 4.231661]\n",
      "epoch:31 step:24341 [D loss: 0.389866, acc.: 82.81%] [G loss: 2.898594]\n",
      "epoch:31 step:24342 [D loss: 0.337963, acc.: 82.03%] [G loss: 3.568108]\n",
      "epoch:31 step:24343 [D loss: 0.365987, acc.: 81.25%] [G loss: 2.963647]\n",
      "epoch:31 step:24344 [D loss: 0.269385, acc.: 89.06%] [G loss: 3.875743]\n",
      "epoch:31 step:24345 [D loss: 0.268536, acc.: 88.28%] [G loss: 3.135020]\n",
      "epoch:31 step:24346 [D loss: 0.326448, acc.: 81.25%] [G loss: 5.884386]\n",
      "epoch:31 step:24347 [D loss: 0.278039, acc.: 85.94%] [G loss: 3.878886]\n",
      "epoch:31 step:24348 [D loss: 0.186346, acc.: 93.75%] [G loss: 6.351673]\n",
      "epoch:31 step:24349 [D loss: 0.370935, acc.: 82.03%] [G loss: 3.361369]\n",
      "epoch:31 step:24350 [D loss: 0.249909, acc.: 89.06%] [G loss: 4.560377]\n",
      "epoch:31 step:24351 [D loss: 0.423946, acc.: 78.91%] [G loss: 4.718895]\n",
      "epoch:31 step:24352 [D loss: 0.392411, acc.: 81.25%] [G loss: 4.162376]\n",
      "epoch:31 step:24353 [D loss: 0.429911, acc.: 77.34%] [G loss: 4.344189]\n",
      "epoch:31 step:24354 [D loss: 0.284617, acc.: 89.06%] [G loss: 4.063350]\n",
      "epoch:31 step:24355 [D loss: 0.337947, acc.: 80.47%] [G loss: 4.460128]\n",
      "epoch:31 step:24356 [D loss: 0.237617, acc.: 89.06%] [G loss: 2.766817]\n",
      "epoch:31 step:24357 [D loss: 0.220043, acc.: 90.62%] [G loss: 5.033653]\n",
      "epoch:31 step:24358 [D loss: 0.264823, acc.: 85.94%] [G loss: 4.135859]\n",
      "epoch:31 step:24359 [D loss: 0.202077, acc.: 90.62%] [G loss: 3.945072]\n",
      "epoch:31 step:24360 [D loss: 0.233936, acc.: 89.06%] [G loss: 4.224183]\n",
      "epoch:31 step:24361 [D loss: 0.210166, acc.: 88.28%] [G loss: 4.128476]\n",
      "epoch:31 step:24362 [D loss: 0.353671, acc.: 80.47%] [G loss: 3.176598]\n",
      "epoch:31 step:24363 [D loss: 0.277706, acc.: 89.06%] [G loss: 3.668083]\n",
      "epoch:31 step:24364 [D loss: 0.328652, acc.: 85.94%] [G loss: 2.985745]\n",
      "epoch:31 step:24365 [D loss: 0.406047, acc.: 78.12%] [G loss: 2.794346]\n",
      "epoch:31 step:24366 [D loss: 0.449733, acc.: 81.25%] [G loss: 3.571995]\n",
      "epoch:31 step:24367 [D loss: 0.309520, acc.: 83.59%] [G loss: 4.773201]\n",
      "epoch:31 step:24368 [D loss: 0.267737, acc.: 86.72%] [G loss: 3.641199]\n",
      "epoch:31 step:24369 [D loss: 0.310430, acc.: 84.38%] [G loss: 4.056737]\n",
      "epoch:31 step:24370 [D loss: 0.309734, acc.: 85.94%] [G loss: 3.746123]\n",
      "epoch:31 step:24371 [D loss: 0.386810, acc.: 82.03%] [G loss: 3.622450]\n",
      "epoch:31 step:24372 [D loss: 0.306236, acc.: 84.38%] [G loss: 3.662387]\n",
      "epoch:31 step:24373 [D loss: 0.242572, acc.: 89.06%] [G loss: 3.567511]\n",
      "epoch:31 step:24374 [D loss: 0.344226, acc.: 85.16%] [G loss: 2.725888]\n",
      "epoch:31 step:24375 [D loss: 0.281657, acc.: 88.28%] [G loss: 2.421846]\n",
      "epoch:31 step:24376 [D loss: 0.333545, acc.: 81.25%] [G loss: 2.734421]\n",
      "epoch:31 step:24377 [D loss: 0.307885, acc.: 86.72%] [G loss: 2.603441]\n",
      "epoch:31 step:24378 [D loss: 0.388778, acc.: 85.94%] [G loss: 4.998903]\n",
      "epoch:31 step:24379 [D loss: 0.487181, acc.: 77.34%] [G loss: 4.320154]\n",
      "epoch:31 step:24380 [D loss: 0.344376, acc.: 86.72%] [G loss: 3.149717]\n",
      "epoch:31 step:24381 [D loss: 0.347688, acc.: 85.16%] [G loss: 2.870988]\n",
      "epoch:31 step:24382 [D loss: 0.359856, acc.: 82.03%] [G loss: 4.439442]\n",
      "epoch:31 step:24383 [D loss: 0.350322, acc.: 85.94%] [G loss: 3.847794]\n",
      "epoch:31 step:24384 [D loss: 0.220079, acc.: 92.97%] [G loss: 4.423688]\n",
      "epoch:31 step:24385 [D loss: 0.422199, acc.: 79.69%] [G loss: 2.710665]\n",
      "epoch:31 step:24386 [D loss: 0.335275, acc.: 81.25%] [G loss: 3.103847]\n",
      "epoch:31 step:24387 [D loss: 0.336674, acc.: 87.50%] [G loss: 3.476817]\n",
      "epoch:31 step:24388 [D loss: 0.393333, acc.: 84.38%] [G loss: 2.858815]\n",
      "epoch:31 step:24389 [D loss: 0.391695, acc.: 81.25%] [G loss: 3.166426]\n",
      "epoch:31 step:24390 [D loss: 0.458137, acc.: 78.12%] [G loss: 2.823494]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:31 step:24391 [D loss: 0.346776, acc.: 84.38%] [G loss: 2.748168]\n",
      "epoch:31 step:24392 [D loss: 0.286065, acc.: 85.16%] [G loss: 3.147403]\n",
      "epoch:31 step:24393 [D loss: 0.406875, acc.: 82.03%] [G loss: 3.179147]\n",
      "epoch:31 step:24394 [D loss: 0.275290, acc.: 87.50%] [G loss: 2.780291]\n",
      "epoch:31 step:24395 [D loss: 0.320311, acc.: 86.72%] [G loss: 2.283351]\n",
      "epoch:31 step:24396 [D loss: 0.265790, acc.: 86.72%] [G loss: 3.173930]\n",
      "epoch:31 step:24397 [D loss: 0.295013, acc.: 88.28%] [G loss: 2.486671]\n",
      "epoch:31 step:24398 [D loss: 0.420291, acc.: 76.56%] [G loss: 3.048242]\n",
      "epoch:31 step:24399 [D loss: 0.415254, acc.: 80.47%] [G loss: 3.773658]\n",
      "epoch:31 step:24400 [D loss: 0.307885, acc.: 84.38%] [G loss: 2.664394]\n",
      "##############\n",
      "[0.8651876  0.84308552 0.77564586 0.78837026 0.78728993 0.84131044\n",
      " 0.88714242 0.84964976 0.82398267 0.82243931]\n",
      "##########\n",
      "epoch:31 step:24401 [D loss: 0.308346, acc.: 86.72%] [G loss: 2.893701]\n",
      "epoch:31 step:24402 [D loss: 0.323702, acc.: 82.03%] [G loss: 2.925377]\n",
      "epoch:31 step:24403 [D loss: 0.385130, acc.: 84.38%] [G loss: 2.584453]\n",
      "epoch:31 step:24404 [D loss: 0.224382, acc.: 90.62%] [G loss: 3.167152]\n",
      "epoch:31 step:24405 [D loss: 0.358354, acc.: 80.47%] [G loss: 2.415444]\n",
      "epoch:31 step:24406 [D loss: 0.390096, acc.: 83.59%] [G loss: 2.639926]\n",
      "epoch:31 step:24407 [D loss: 0.263330, acc.: 88.28%] [G loss: 3.251707]\n",
      "epoch:31 step:24408 [D loss: 0.426382, acc.: 80.47%] [G loss: 3.115284]\n",
      "epoch:31 step:24409 [D loss: 0.421008, acc.: 79.69%] [G loss: 3.121843]\n",
      "epoch:31 step:24410 [D loss: 0.363823, acc.: 87.50%] [G loss: 3.268867]\n",
      "epoch:31 step:24411 [D loss: 0.283413, acc.: 87.50%] [G loss: 4.127981]\n",
      "epoch:31 step:24412 [D loss: 0.400920, acc.: 85.16%] [G loss: 4.334707]\n",
      "epoch:31 step:24413 [D loss: 0.380591, acc.: 82.03%] [G loss: 3.618017]\n",
      "epoch:31 step:24414 [D loss: 0.468284, acc.: 81.25%] [G loss: 2.234158]\n",
      "epoch:31 step:24415 [D loss: 0.356965, acc.: 80.47%] [G loss: 3.989928]\n",
      "epoch:31 step:24416 [D loss: 0.298375, acc.: 87.50%] [G loss: 3.678586]\n",
      "epoch:31 step:24417 [D loss: 0.385847, acc.: 79.69%] [G loss: 3.497190]\n",
      "epoch:31 step:24418 [D loss: 0.330756, acc.: 85.16%] [G loss: 2.498588]\n",
      "epoch:31 step:24419 [D loss: 0.337940, acc.: 82.03%] [G loss: 2.731657]\n",
      "epoch:31 step:24420 [D loss: 0.311441, acc.: 84.38%] [G loss: 2.751626]\n",
      "epoch:31 step:24421 [D loss: 0.309217, acc.: 84.38%] [G loss: 3.020605]\n",
      "epoch:31 step:24422 [D loss: 0.377963, acc.: 81.25%] [G loss: 2.205387]\n",
      "epoch:31 step:24423 [D loss: 0.276331, acc.: 83.59%] [G loss: 2.773059]\n",
      "epoch:31 step:24424 [D loss: 0.261223, acc.: 87.50%] [G loss: 2.548849]\n",
      "epoch:31 step:24425 [D loss: 0.367528, acc.: 82.03%] [G loss: 2.747951]\n",
      "epoch:31 step:24426 [D loss: 0.263731, acc.: 90.62%] [G loss: 2.875224]\n",
      "epoch:31 step:24427 [D loss: 0.414188, acc.: 79.69%] [G loss: 4.363258]\n",
      "epoch:31 step:24428 [D loss: 0.460195, acc.: 79.69%] [G loss: 3.223023]\n",
      "epoch:31 step:24429 [D loss: 0.452311, acc.: 78.91%] [G loss: 3.504451]\n",
      "epoch:31 step:24430 [D loss: 0.456723, acc.: 79.69%] [G loss: 2.487092]\n",
      "epoch:31 step:24431 [D loss: 0.309686, acc.: 86.72%] [G loss: 3.124574]\n",
      "epoch:31 step:24432 [D loss: 0.326660, acc.: 89.84%] [G loss: 2.748264]\n",
      "epoch:31 step:24433 [D loss: 0.471570, acc.: 78.91%] [G loss: 2.779199]\n",
      "epoch:31 step:24434 [D loss: 0.355971, acc.: 83.59%] [G loss: 2.503039]\n",
      "epoch:31 step:24435 [D loss: 0.406261, acc.: 81.25%] [G loss: 3.113934]\n",
      "epoch:31 step:24436 [D loss: 0.321135, acc.: 83.59%] [G loss: 3.829618]\n",
      "epoch:31 step:24437 [D loss: 0.321806, acc.: 84.38%] [G loss: 5.474866]\n",
      "epoch:31 step:24438 [D loss: 0.352130, acc.: 84.38%] [G loss: 2.632047]\n",
      "epoch:31 step:24439 [D loss: 0.255941, acc.: 90.62%] [G loss: 3.389318]\n",
      "epoch:31 step:24440 [D loss: 0.287649, acc.: 87.50%] [G loss: 4.338326]\n",
      "epoch:31 step:24441 [D loss: 0.327643, acc.: 87.50%] [G loss: 2.811957]\n",
      "epoch:31 step:24442 [D loss: 0.282050, acc.: 86.72%] [G loss: 3.634760]\n",
      "epoch:31 step:24443 [D loss: 0.308466, acc.: 85.94%] [G loss: 3.825144]\n",
      "epoch:31 step:24444 [D loss: 0.366286, acc.: 82.81%] [G loss: 2.722243]\n",
      "epoch:31 step:24445 [D loss: 0.258185, acc.: 89.06%] [G loss: 3.275044]\n",
      "epoch:31 step:24446 [D loss: 0.253610, acc.: 89.84%] [G loss: 3.602663]\n",
      "epoch:31 step:24447 [D loss: 0.307162, acc.: 86.72%] [G loss: 3.569366]\n",
      "epoch:31 step:24448 [D loss: 0.267866, acc.: 87.50%] [G loss: 3.335522]\n",
      "epoch:31 step:24449 [D loss: 0.338165, acc.: 83.59%] [G loss: 3.234163]\n",
      "epoch:31 step:24450 [D loss: 0.328858, acc.: 86.72%] [G loss: 3.424520]\n",
      "epoch:31 step:24451 [D loss: 0.269443, acc.: 89.06%] [G loss: 4.194797]\n",
      "epoch:31 step:24452 [D loss: 0.332714, acc.: 82.03%] [G loss: 3.274173]\n",
      "epoch:31 step:24453 [D loss: 0.245268, acc.: 88.28%] [G loss: 3.867944]\n",
      "epoch:31 step:24454 [D loss: 0.199540, acc.: 92.19%] [G loss: 3.859405]\n",
      "epoch:31 step:24455 [D loss: 0.283065, acc.: 85.16%] [G loss: 5.022745]\n",
      "epoch:31 step:24456 [D loss: 0.313447, acc.: 86.72%] [G loss: 4.870846]\n",
      "epoch:31 step:24457 [D loss: 0.289350, acc.: 84.38%] [G loss: 4.398374]\n",
      "epoch:31 step:24458 [D loss: 0.272615, acc.: 90.62%] [G loss: 2.837613]\n",
      "epoch:31 step:24459 [D loss: 0.373998, acc.: 82.03%] [G loss: 3.209865]\n",
      "epoch:31 step:24460 [D loss: 0.465576, acc.: 80.47%] [G loss: 4.139629]\n",
      "epoch:31 step:24461 [D loss: 0.409578, acc.: 83.59%] [G loss: 3.825102]\n",
      "epoch:31 step:24462 [D loss: 0.469490, acc.: 75.00%] [G loss: 6.752707]\n",
      "epoch:31 step:24463 [D loss: 0.602400, acc.: 68.75%] [G loss: 6.806808]\n",
      "epoch:31 step:24464 [D loss: 0.760625, acc.: 71.09%] [G loss: 7.127551]\n",
      "epoch:31 step:24465 [D loss: 0.883930, acc.: 68.75%] [G loss: 4.505782]\n",
      "epoch:31 step:24466 [D loss: 0.984659, acc.: 68.75%] [G loss: 5.833154]\n",
      "epoch:31 step:24467 [D loss: 0.512612, acc.: 76.56%] [G loss: 3.631944]\n",
      "epoch:31 step:24468 [D loss: 0.530351, acc.: 79.69%] [G loss: 4.451906]\n",
      "epoch:31 step:24469 [D loss: 0.413155, acc.: 79.69%] [G loss: 3.359090]\n",
      "epoch:31 step:24470 [D loss: 0.450533, acc.: 80.47%] [G loss: 5.228339]\n",
      "epoch:31 step:24471 [D loss: 0.319290, acc.: 85.94%] [G loss: 5.919547]\n",
      "epoch:31 step:24472 [D loss: 0.428743, acc.: 81.25%] [G loss: 4.013271]\n",
      "epoch:31 step:24473 [D loss: 0.289558, acc.: 85.16%] [G loss: 5.393096]\n",
      "epoch:31 step:24474 [D loss: 0.252153, acc.: 89.06%] [G loss: 3.181497]\n",
      "epoch:31 step:24475 [D loss: 0.264288, acc.: 87.50%] [G loss: 4.303071]\n",
      "epoch:31 step:24476 [D loss: 0.384475, acc.: 85.16%] [G loss: 2.202455]\n",
      "epoch:31 step:24477 [D loss: 0.362358, acc.: 85.16%] [G loss: 2.898168]\n",
      "epoch:31 step:24478 [D loss: 0.328842, acc.: 85.16%] [G loss: 3.176738]\n",
      "epoch:31 step:24479 [D loss: 0.406987, acc.: 78.12%] [G loss: 2.521710]\n",
      "epoch:31 step:24480 [D loss: 0.297176, acc.: 84.38%] [G loss: 3.117569]\n",
      "epoch:31 step:24481 [D loss: 0.256142, acc.: 89.84%] [G loss: 3.049082]\n",
      "epoch:31 step:24482 [D loss: 0.369852, acc.: 82.81%] [G loss: 2.428777]\n",
      "epoch:31 step:24483 [D loss: 0.306999, acc.: 82.81%] [G loss: 2.531811]\n",
      "epoch:31 step:24484 [D loss: 0.278515, acc.: 85.94%] [G loss: 2.691882]\n",
      "epoch:31 step:24485 [D loss: 0.331018, acc.: 85.16%] [G loss: 2.493513]\n",
      "epoch:31 step:24486 [D loss: 0.441270, acc.: 79.69%] [G loss: 2.544038]\n",
      "epoch:31 step:24487 [D loss: 0.301213, acc.: 86.72%] [G loss: 2.413011]\n",
      "epoch:31 step:24488 [D loss: 0.402594, acc.: 82.03%] [G loss: 3.156294]\n",
      "epoch:31 step:24489 [D loss: 0.409547, acc.: 83.59%] [G loss: 3.108876]\n",
      "epoch:31 step:24490 [D loss: 0.376407, acc.: 81.25%] [G loss: 3.590854]\n",
      "epoch:31 step:24491 [D loss: 0.298090, acc.: 85.16%] [G loss: 2.966938]\n",
      "epoch:31 step:24492 [D loss: 0.325955, acc.: 84.38%] [G loss: 3.009532]\n",
      "epoch:31 step:24493 [D loss: 0.410314, acc.: 82.81%] [G loss: 2.961410]\n",
      "epoch:31 step:24494 [D loss: 0.365653, acc.: 86.72%] [G loss: 2.979559]\n",
      "epoch:31 step:24495 [D loss: 0.330555, acc.: 85.16%] [G loss: 2.701557]\n",
      "epoch:31 step:24496 [D loss: 0.279335, acc.: 89.84%] [G loss: 2.981411]\n",
      "epoch:31 step:24497 [D loss: 0.365846, acc.: 85.16%] [G loss: 3.254134]\n",
      "epoch:31 step:24498 [D loss: 0.297642, acc.: 85.94%] [G loss: 2.788946]\n",
      "epoch:31 step:24499 [D loss: 0.348889, acc.: 84.38%] [G loss: 2.225398]\n",
      "epoch:31 step:24500 [D loss: 0.343212, acc.: 83.59%] [G loss: 2.918670]\n",
      "epoch:31 step:24501 [D loss: 0.359083, acc.: 84.38%] [G loss: 2.474301]\n",
      "epoch:31 step:24502 [D loss: 0.343591, acc.: 79.69%] [G loss: 2.302542]\n",
      "epoch:31 step:24503 [D loss: 0.289265, acc.: 85.94%] [G loss: 2.326390]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:31 step:24504 [D loss: 0.469000, acc.: 82.81%] [G loss: 2.254305]\n",
      "epoch:31 step:24505 [D loss: 0.370656, acc.: 83.59%] [G loss: 2.261691]\n",
      "epoch:31 step:24506 [D loss: 0.336795, acc.: 87.50%] [G loss: 2.423610]\n",
      "epoch:31 step:24507 [D loss: 0.365901, acc.: 85.16%] [G loss: 2.556281]\n",
      "epoch:31 step:24508 [D loss: 0.346218, acc.: 83.59%] [G loss: 3.233869]\n",
      "epoch:31 step:24509 [D loss: 0.372176, acc.: 84.38%] [G loss: 2.784353]\n",
      "epoch:31 step:24510 [D loss: 0.262553, acc.: 90.62%] [G loss: 2.822774]\n",
      "epoch:31 step:24511 [D loss: 0.334068, acc.: 84.38%] [G loss: 2.897565]\n",
      "epoch:31 step:24512 [D loss: 0.270657, acc.: 87.50%] [G loss: 3.193661]\n",
      "epoch:31 step:24513 [D loss: 0.242222, acc.: 90.62%] [G loss: 4.009660]\n",
      "epoch:31 step:24514 [D loss: 0.227671, acc.: 88.28%] [G loss: 4.297222]\n",
      "epoch:31 step:24515 [D loss: 0.302748, acc.: 85.94%] [G loss: 3.032980]\n",
      "epoch:31 step:24516 [D loss: 0.345502, acc.: 82.03%] [G loss: 2.827958]\n",
      "epoch:31 step:24517 [D loss: 0.345113, acc.: 82.81%] [G loss: 2.731852]\n",
      "epoch:31 step:24518 [D loss: 0.298329, acc.: 86.72%] [G loss: 3.817822]\n",
      "epoch:31 step:24519 [D loss: 0.303297, acc.: 84.38%] [G loss: 3.023879]\n",
      "epoch:31 step:24520 [D loss: 0.248997, acc.: 89.06%] [G loss: 4.213068]\n",
      "epoch:31 step:24521 [D loss: 0.301061, acc.: 85.16%] [G loss: 3.574578]\n",
      "epoch:31 step:24522 [D loss: 0.384631, acc.: 78.91%] [G loss: 4.821787]\n",
      "epoch:31 step:24523 [D loss: 0.270672, acc.: 88.28%] [G loss: 4.895683]\n",
      "epoch:31 step:24524 [D loss: 0.243868, acc.: 88.28%] [G loss: 3.573481]\n",
      "epoch:31 step:24525 [D loss: 0.267645, acc.: 87.50%] [G loss: 4.470582]\n",
      "epoch:31 step:24526 [D loss: 0.432469, acc.: 82.81%] [G loss: 6.572358]\n",
      "epoch:31 step:24527 [D loss: 0.532825, acc.: 81.25%] [G loss: 4.539829]\n",
      "epoch:31 step:24528 [D loss: 0.739435, acc.: 76.56%] [G loss: 7.236927]\n",
      "epoch:31 step:24529 [D loss: 2.033403, acc.: 59.38%] [G loss: 5.431959]\n",
      "epoch:31 step:24530 [D loss: 0.346795, acc.: 82.81%] [G loss: 6.195518]\n",
      "epoch:31 step:24531 [D loss: 0.483270, acc.: 77.34%] [G loss: 4.027339]\n",
      "epoch:31 step:24532 [D loss: 0.338365, acc.: 87.50%] [G loss: 5.908320]\n",
      "epoch:31 step:24533 [D loss: 0.421346, acc.: 78.12%] [G loss: 2.975149]\n",
      "epoch:31 step:24534 [D loss: 0.229554, acc.: 91.41%] [G loss: 3.224583]\n",
      "epoch:31 step:24535 [D loss: 0.344680, acc.: 85.94%] [G loss: 4.043139]\n",
      "epoch:31 step:24536 [D loss: 0.342862, acc.: 83.59%] [G loss: 3.229216]\n",
      "epoch:31 step:24537 [D loss: 0.240567, acc.: 89.84%] [G loss: 3.090824]\n",
      "epoch:31 step:24538 [D loss: 0.312239, acc.: 86.72%] [G loss: 3.989168]\n",
      "epoch:31 step:24539 [D loss: 0.276984, acc.: 86.72%] [G loss: 4.259997]\n",
      "epoch:31 step:24540 [D loss: 0.275692, acc.: 92.19%] [G loss: 3.414828]\n",
      "epoch:31 step:24541 [D loss: 0.343965, acc.: 86.72%] [G loss: 2.836807]\n",
      "epoch:31 step:24542 [D loss: 0.384326, acc.: 85.16%] [G loss: 3.536691]\n",
      "epoch:31 step:24543 [D loss: 0.367950, acc.: 82.81%] [G loss: 2.295126]\n",
      "epoch:31 step:24544 [D loss: 0.309013, acc.: 84.38%] [G loss: 3.641534]\n",
      "epoch:31 step:24545 [D loss: 0.352453, acc.: 85.16%] [G loss: 3.044326]\n",
      "epoch:31 step:24546 [D loss: 0.338465, acc.: 88.28%] [G loss: 2.387465]\n",
      "epoch:31 step:24547 [D loss: 0.269493, acc.: 88.28%] [G loss: 3.331333]\n",
      "epoch:31 step:24548 [D loss: 0.274010, acc.: 88.28%] [G loss: 3.327011]\n",
      "epoch:31 step:24549 [D loss: 0.354537, acc.: 82.03%] [G loss: 3.307925]\n",
      "epoch:31 step:24550 [D loss: 0.351236, acc.: 85.94%] [G loss: 2.690996]\n",
      "epoch:31 step:24551 [D loss: 0.321325, acc.: 89.06%] [G loss: 2.476142]\n",
      "epoch:31 step:24552 [D loss: 0.314212, acc.: 85.94%] [G loss: 2.634565]\n",
      "epoch:31 step:24553 [D loss: 0.277817, acc.: 90.62%] [G loss: 2.920502]\n",
      "epoch:31 step:24554 [D loss: 0.292170, acc.: 89.84%] [G loss: 3.083261]\n",
      "epoch:31 step:24555 [D loss: 0.345096, acc.: 82.81%] [G loss: 2.696331]\n",
      "epoch:31 step:24556 [D loss: 0.321228, acc.: 82.81%] [G loss: 2.503340]\n",
      "epoch:31 step:24557 [D loss: 0.424679, acc.: 79.69%] [G loss: 3.049586]\n",
      "epoch:31 step:24558 [D loss: 0.311036, acc.: 88.28%] [G loss: 3.226788]\n",
      "epoch:31 step:24559 [D loss: 0.341795, acc.: 84.38%] [G loss: 2.399716]\n",
      "epoch:31 step:24560 [D loss: 0.308908, acc.: 85.94%] [G loss: 2.900461]\n",
      "epoch:31 step:24561 [D loss: 0.358521, acc.: 83.59%] [G loss: 2.385735]\n",
      "epoch:31 step:24562 [D loss: 0.321563, acc.: 84.38%] [G loss: 3.203542]\n",
      "epoch:31 step:24563 [D loss: 0.353691, acc.: 82.03%] [G loss: 3.809651]\n",
      "epoch:31 step:24564 [D loss: 0.276571, acc.: 87.50%] [G loss: 2.943935]\n",
      "epoch:31 step:24565 [D loss: 0.279256, acc.: 86.72%] [G loss: 3.150890]\n",
      "epoch:31 step:24566 [D loss: 0.364966, acc.: 81.25%] [G loss: 4.315260]\n",
      "epoch:31 step:24567 [D loss: 0.428755, acc.: 76.56%] [G loss: 2.473612]\n",
      "epoch:31 step:24568 [D loss: 0.296175, acc.: 89.06%] [G loss: 3.659229]\n",
      "epoch:31 step:24569 [D loss: 0.410638, acc.: 80.47%] [G loss: 2.579210]\n",
      "epoch:31 step:24570 [D loss: 0.259284, acc.: 87.50%] [G loss: 3.118332]\n",
      "epoch:31 step:24571 [D loss: 0.378974, acc.: 80.47%] [G loss: 2.709638]\n",
      "epoch:31 step:24572 [D loss: 0.307833, acc.: 89.84%] [G loss: 3.394125]\n",
      "epoch:31 step:24573 [D loss: 0.365277, acc.: 82.03%] [G loss: 2.271394]\n",
      "epoch:31 step:24574 [D loss: 0.347895, acc.: 83.59%] [G loss: 2.495031]\n",
      "epoch:31 step:24575 [D loss: 0.331574, acc.: 82.81%] [G loss: 2.482825]\n",
      "epoch:31 step:24576 [D loss: 0.375833, acc.: 85.94%] [G loss: 2.335086]\n",
      "epoch:31 step:24577 [D loss: 0.426246, acc.: 78.91%] [G loss: 2.170306]\n",
      "epoch:31 step:24578 [D loss: 0.361727, acc.: 87.50%] [G loss: 2.399426]\n",
      "epoch:31 step:24579 [D loss: 0.276172, acc.: 87.50%] [G loss: 3.126757]\n",
      "epoch:31 step:24580 [D loss: 0.406864, acc.: 80.47%] [G loss: 2.596780]\n",
      "epoch:31 step:24581 [D loss: 0.399413, acc.: 81.25%] [G loss: 2.603414]\n",
      "epoch:31 step:24582 [D loss: 0.268194, acc.: 90.62%] [G loss: 3.814243]\n",
      "epoch:31 step:24583 [D loss: 0.312682, acc.: 85.94%] [G loss: 2.638021]\n",
      "epoch:31 step:24584 [D loss: 0.247550, acc.: 90.62%] [G loss: 3.085312]\n",
      "epoch:31 step:24585 [D loss: 0.332832, acc.: 81.25%] [G loss: 3.028376]\n",
      "epoch:31 step:24586 [D loss: 0.230268, acc.: 89.84%] [G loss: 3.969271]\n",
      "epoch:31 step:24587 [D loss: 0.357983, acc.: 85.16%] [G loss: 2.710155]\n",
      "epoch:31 step:24588 [D loss: 0.281814, acc.: 87.50%] [G loss: 2.893861]\n",
      "epoch:31 step:24589 [D loss: 0.294083, acc.: 85.16%] [G loss: 3.128329]\n",
      "epoch:31 step:24590 [D loss: 0.360534, acc.: 80.47%] [G loss: 3.192972]\n",
      "epoch:31 step:24591 [D loss: 0.379897, acc.: 83.59%] [G loss: 2.868455]\n",
      "epoch:31 step:24592 [D loss: 0.277728, acc.: 89.84%] [G loss: 2.847120]\n",
      "epoch:31 step:24593 [D loss: 0.393403, acc.: 81.25%] [G loss: 5.619084]\n",
      "epoch:31 step:24594 [D loss: 0.644080, acc.: 72.66%] [G loss: 3.915773]\n",
      "epoch:31 step:24595 [D loss: 0.569508, acc.: 76.56%] [G loss: 4.936249]\n",
      "epoch:31 step:24596 [D loss: 0.686884, acc.: 75.00%] [G loss: 3.867217]\n",
      "epoch:31 step:24597 [D loss: 0.473331, acc.: 76.56%] [G loss: 3.242329]\n",
      "epoch:31 step:24598 [D loss: 0.493599, acc.: 71.88%] [G loss: 3.235980]\n",
      "epoch:31 step:24599 [D loss: 0.357012, acc.: 86.72%] [G loss: 3.316595]\n",
      "epoch:31 step:24600 [D loss: 0.510905, acc.: 68.75%] [G loss: 2.745562]\n",
      "##############\n",
      "[0.87542973 0.85768111 0.79870805 0.79053932 0.79543269 0.85078634\n",
      " 0.85229674 0.84121661 0.80327008 0.81375814]\n",
      "##########\n",
      "epoch:31 step:24601 [D loss: 0.340813, acc.: 84.38%] [G loss: 2.528146]\n",
      "epoch:31 step:24602 [D loss: 0.369703, acc.: 83.59%] [G loss: 3.521102]\n",
      "epoch:31 step:24603 [D loss: 0.250996, acc.: 91.41%] [G loss: 2.831781]\n",
      "epoch:31 step:24604 [D loss: 0.232440, acc.: 90.62%] [G loss: 2.511060]\n",
      "epoch:31 step:24605 [D loss: 0.301382, acc.: 89.84%] [G loss: 2.600961]\n",
      "epoch:31 step:24606 [D loss: 0.321612, acc.: 83.59%] [G loss: 2.794431]\n",
      "epoch:31 step:24607 [D loss: 0.321418, acc.: 85.16%] [G loss: 2.528305]\n",
      "epoch:31 step:24608 [D loss: 0.349845, acc.: 84.38%] [G loss: 2.517801]\n",
      "epoch:31 step:24609 [D loss: 0.443679, acc.: 75.78%] [G loss: 2.618545]\n",
      "epoch:31 step:24610 [D loss: 0.357476, acc.: 85.94%] [G loss: 2.859940]\n",
      "epoch:31 step:24611 [D loss: 0.288030, acc.: 85.94%] [G loss: 2.805668]\n",
      "epoch:31 step:24612 [D loss: 0.339490, acc.: 85.16%] [G loss: 2.342880]\n",
      "epoch:31 step:24613 [D loss: 0.364703, acc.: 81.25%] [G loss: 3.273951]\n",
      "epoch:31 step:24614 [D loss: 0.256754, acc.: 88.28%] [G loss: 3.796239]\n",
      "epoch:31 step:24615 [D loss: 0.322758, acc.: 87.50%] [G loss: 2.754333]\n",
      "epoch:31 step:24616 [D loss: 0.376216, acc.: 84.38%] [G loss: 2.774524]\n",
      "epoch:31 step:24617 [D loss: 0.271207, acc.: 89.06%] [G loss: 2.878509]\n",
      "epoch:31 step:24618 [D loss: 0.275964, acc.: 86.72%] [G loss: 3.323933]\n",
      "epoch:31 step:24619 [D loss: 0.259720, acc.: 90.62%] [G loss: 2.636522]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:31 step:24620 [D loss: 0.276318, acc.: 88.28%] [G loss: 2.709585]\n",
      "epoch:31 step:24621 [D loss: 0.350262, acc.: 85.16%] [G loss: 2.525825]\n",
      "epoch:31 step:24622 [D loss: 0.394446, acc.: 83.59%] [G loss: 2.312710]\n",
      "epoch:31 step:24623 [D loss: 0.315946, acc.: 88.28%] [G loss: 3.684202]\n",
      "epoch:31 step:24624 [D loss: 0.368717, acc.: 84.38%] [G loss: 2.821679]\n",
      "epoch:31 step:24625 [D loss: 0.302028, acc.: 85.16%] [G loss: 2.995913]\n",
      "epoch:31 step:24626 [D loss: 0.305790, acc.: 87.50%] [G loss: 2.288975]\n",
      "epoch:31 step:24627 [D loss: 0.277770, acc.: 86.72%] [G loss: 4.645660]\n",
      "epoch:31 step:24628 [D loss: 0.297256, acc.: 88.28%] [G loss: 4.198157]\n",
      "epoch:31 step:24629 [D loss: 0.296880, acc.: 88.28%] [G loss: 5.024480]\n",
      "epoch:31 step:24630 [D loss: 0.273099, acc.: 89.06%] [G loss: 3.898327]\n",
      "epoch:31 step:24631 [D loss: 0.297719, acc.: 84.38%] [G loss: 4.518682]\n",
      "epoch:31 step:24632 [D loss: 0.227465, acc.: 90.62%] [G loss: 5.226889]\n",
      "epoch:31 step:24633 [D loss: 0.339429, acc.: 84.38%] [G loss: 2.377161]\n",
      "epoch:31 step:24634 [D loss: 0.232915, acc.: 90.62%] [G loss: 3.831262]\n",
      "epoch:31 step:24635 [D loss: 0.280703, acc.: 87.50%] [G loss: 4.406709]\n",
      "epoch:31 step:24636 [D loss: 0.361526, acc.: 83.59%] [G loss: 6.630410]\n",
      "epoch:31 step:24637 [D loss: 0.359580, acc.: 80.47%] [G loss: 3.695393]\n",
      "epoch:31 step:24638 [D loss: 0.278980, acc.: 85.16%] [G loss: 4.257994]\n",
      "epoch:31 step:24639 [D loss: 0.307115, acc.: 83.59%] [G loss: 4.107344]\n",
      "epoch:31 step:24640 [D loss: 0.322182, acc.: 81.25%] [G loss: 4.152460]\n",
      "epoch:31 step:24641 [D loss: 0.286473, acc.: 87.50%] [G loss: 3.164217]\n",
      "epoch:31 step:24642 [D loss: 0.350311, acc.: 82.03%] [G loss: 2.576827]\n",
      "epoch:31 step:24643 [D loss: 0.352025, acc.: 82.81%] [G loss: 3.890701]\n",
      "epoch:31 step:24644 [D loss: 0.312849, acc.: 85.94%] [G loss: 2.462095]\n",
      "epoch:31 step:24645 [D loss: 0.420639, acc.: 78.91%] [G loss: 2.928198]\n",
      "epoch:31 step:24646 [D loss: 0.382788, acc.: 82.81%] [G loss: 2.272424]\n",
      "epoch:31 step:24647 [D loss: 0.422280, acc.: 81.25%] [G loss: 3.431629]\n",
      "epoch:31 step:24648 [D loss: 0.354241, acc.: 86.72%] [G loss: 4.233185]\n",
      "epoch:31 step:24649 [D loss: 0.346143, acc.: 84.38%] [G loss: 2.436383]\n",
      "epoch:31 step:24650 [D loss: 0.296802, acc.: 86.72%] [G loss: 2.789136]\n",
      "epoch:31 step:24651 [D loss: 0.247977, acc.: 87.50%] [G loss: 2.928194]\n",
      "epoch:31 step:24652 [D loss: 0.357100, acc.: 86.72%] [G loss: 2.884913]\n",
      "epoch:31 step:24653 [D loss: 0.306694, acc.: 88.28%] [G loss: 2.764566]\n",
      "epoch:31 step:24654 [D loss: 0.362523, acc.: 86.72%] [G loss: 2.732409]\n",
      "epoch:31 step:24655 [D loss: 0.425971, acc.: 78.12%] [G loss: 2.996927]\n",
      "epoch:31 step:24656 [D loss: 0.429600, acc.: 80.47%] [G loss: 2.982120]\n",
      "epoch:31 step:24657 [D loss: 0.290183, acc.: 89.06%] [G loss: 3.613170]\n",
      "epoch:31 step:24658 [D loss: 0.414632, acc.: 80.47%] [G loss: 4.011431]\n",
      "epoch:31 step:24659 [D loss: 0.408168, acc.: 75.78%] [G loss: 2.351278]\n",
      "epoch:31 step:24660 [D loss: 0.277695, acc.: 86.72%] [G loss: 2.783197]\n",
      "epoch:31 step:24661 [D loss: 0.340588, acc.: 84.38%] [G loss: 2.827169]\n",
      "epoch:31 step:24662 [D loss: 0.358420, acc.: 82.81%] [G loss: 3.073320]\n",
      "epoch:31 step:24663 [D loss: 0.332741, acc.: 84.38%] [G loss: 2.985703]\n",
      "epoch:31 step:24664 [D loss: 0.331235, acc.: 82.81%] [G loss: 3.657777]\n",
      "epoch:31 step:24665 [D loss: 0.301482, acc.: 88.28%] [G loss: 2.932703]\n",
      "epoch:31 step:24666 [D loss: 0.333318, acc.: 82.81%] [G loss: 3.981503]\n",
      "epoch:31 step:24667 [D loss: 0.315534, acc.: 88.28%] [G loss: 3.733707]\n",
      "epoch:31 step:24668 [D loss: 0.265184, acc.: 87.50%] [G loss: 3.846870]\n",
      "epoch:31 step:24669 [D loss: 0.231501, acc.: 89.84%] [G loss: 3.500294]\n",
      "epoch:31 step:24670 [D loss: 0.355419, acc.: 79.69%] [G loss: 4.112024]\n",
      "epoch:31 step:24671 [D loss: 0.377527, acc.: 79.69%] [G loss: 4.902577]\n",
      "epoch:31 step:24672 [D loss: 0.304643, acc.: 85.94%] [G loss: 3.081148]\n",
      "epoch:31 step:24673 [D loss: 0.232227, acc.: 92.19%] [G loss: 5.260563]\n",
      "epoch:31 step:24674 [D loss: 0.290213, acc.: 87.50%] [G loss: 5.383137]\n",
      "epoch:31 step:24675 [D loss: 0.298006, acc.: 85.94%] [G loss: 3.383422]\n",
      "epoch:31 step:24676 [D loss: 0.416279, acc.: 79.69%] [G loss: 3.691836]\n",
      "epoch:31 step:24677 [D loss: 0.378013, acc.: 82.03%] [G loss: 3.354597]\n",
      "epoch:31 step:24678 [D loss: 0.367988, acc.: 83.59%] [G loss: 4.172379]\n",
      "epoch:31 step:24679 [D loss: 0.264508, acc.: 86.72%] [G loss: 4.765130]\n",
      "epoch:31 step:24680 [D loss: 0.282424, acc.: 85.94%] [G loss: 3.963060]\n",
      "epoch:31 step:24681 [D loss: 0.232825, acc.: 89.06%] [G loss: 4.010921]\n",
      "epoch:31 step:24682 [D loss: 0.421502, acc.: 79.69%] [G loss: 3.143270]\n",
      "epoch:31 step:24683 [D loss: 0.313324, acc.: 82.03%] [G loss: 3.944317]\n",
      "epoch:31 step:24684 [D loss: 0.368160, acc.: 88.28%] [G loss: 4.026621]\n",
      "epoch:31 step:24685 [D loss: 0.353905, acc.: 85.16%] [G loss: 4.431952]\n",
      "epoch:31 step:24686 [D loss: 0.221758, acc.: 91.41%] [G loss: 3.616042]\n",
      "epoch:31 step:24687 [D loss: 0.302115, acc.: 85.94%] [G loss: 3.761542]\n",
      "epoch:31 step:24688 [D loss: 0.373673, acc.: 81.25%] [G loss: 2.777335]\n",
      "epoch:31 step:24689 [D loss: 0.284163, acc.: 87.50%] [G loss: 3.399968]\n",
      "epoch:31 step:24690 [D loss: 0.303670, acc.: 86.72%] [G loss: 2.371608]\n",
      "epoch:31 step:24691 [D loss: 0.381898, acc.: 83.59%] [G loss: 3.957515]\n",
      "epoch:31 step:24692 [D loss: 0.347670, acc.: 86.72%] [G loss: 2.875920]\n",
      "epoch:31 step:24693 [D loss: 0.361352, acc.: 83.59%] [G loss: 3.722904]\n",
      "epoch:31 step:24694 [D loss: 0.390771, acc.: 84.38%] [G loss: 3.219144]\n",
      "epoch:31 step:24695 [D loss: 0.366764, acc.: 84.38%] [G loss: 3.469209]\n",
      "epoch:31 step:24696 [D loss: 0.293531, acc.: 89.06%] [G loss: 2.992214]\n",
      "epoch:31 step:24697 [D loss: 0.325724, acc.: 83.59%] [G loss: 4.117598]\n",
      "epoch:31 step:24698 [D loss: 0.186521, acc.: 92.19%] [G loss: 3.235394]\n",
      "epoch:31 step:24699 [D loss: 0.375078, acc.: 83.59%] [G loss: 2.646237]\n",
      "epoch:31 step:24700 [D loss: 0.351981, acc.: 78.91%] [G loss: 3.183033]\n",
      "epoch:31 step:24701 [D loss: 0.255656, acc.: 89.84%] [G loss: 3.927370]\n",
      "epoch:31 step:24702 [D loss: 0.280762, acc.: 84.38%] [G loss: 5.252471]\n",
      "epoch:31 step:24703 [D loss: 0.350221, acc.: 86.72%] [G loss: 2.995242]\n",
      "epoch:31 step:24704 [D loss: 0.401658, acc.: 79.69%] [G loss: 2.769332]\n",
      "epoch:31 step:24705 [D loss: 0.298298, acc.: 88.28%] [G loss: 2.653936]\n",
      "epoch:31 step:24706 [D loss: 0.315072, acc.: 87.50%] [G loss: 3.986356]\n",
      "epoch:31 step:24707 [D loss: 0.348779, acc.: 84.38%] [G loss: 3.045094]\n",
      "epoch:31 step:24708 [D loss: 0.274649, acc.: 85.16%] [G loss: 3.232696]\n",
      "epoch:31 step:24709 [D loss: 0.359434, acc.: 82.81%] [G loss: 4.191003]\n",
      "epoch:31 step:24710 [D loss: 0.323516, acc.: 89.84%] [G loss: 3.790612]\n",
      "epoch:31 step:24711 [D loss: 0.293168, acc.: 85.16%] [G loss: 2.926888]\n",
      "epoch:31 step:24712 [D loss: 0.429067, acc.: 78.91%] [G loss: 2.666519]\n",
      "epoch:31 step:24713 [D loss: 0.413071, acc.: 79.69%] [G loss: 3.015266]\n",
      "epoch:31 step:24714 [D loss: 0.528028, acc.: 78.12%] [G loss: 3.583594]\n",
      "epoch:31 step:24715 [D loss: 0.385923, acc.: 85.94%] [G loss: 3.403984]\n",
      "epoch:31 step:24716 [D loss: 0.299955, acc.: 86.72%] [G loss: 2.807481]\n",
      "epoch:31 step:24717 [D loss: 0.287629, acc.: 89.84%] [G loss: 3.069597]\n",
      "epoch:31 step:24718 [D loss: 0.313895, acc.: 83.59%] [G loss: 3.547925]\n",
      "epoch:31 step:24719 [D loss: 0.328144, acc.: 86.72%] [G loss: 4.060109]\n",
      "epoch:31 step:24720 [D loss: 0.299581, acc.: 84.38%] [G loss: 3.102417]\n",
      "epoch:31 step:24721 [D loss: 0.296706, acc.: 86.72%] [G loss: 3.063910]\n",
      "epoch:31 step:24722 [D loss: 0.350285, acc.: 86.72%] [G loss: 3.469764]\n",
      "epoch:31 step:24723 [D loss: 0.216632, acc.: 90.62%] [G loss: 6.305327]\n",
      "epoch:31 step:24724 [D loss: 0.369308, acc.: 82.81%] [G loss: 3.443501]\n",
      "epoch:31 step:24725 [D loss: 0.247859, acc.: 92.97%] [G loss: 3.314218]\n",
      "epoch:31 step:24726 [D loss: 0.311463, acc.: 83.59%] [G loss: 4.094257]\n",
      "epoch:31 step:24727 [D loss: 0.283573, acc.: 88.28%] [G loss: 3.756433]\n",
      "epoch:31 step:24728 [D loss: 0.237039, acc.: 88.28%] [G loss: 6.207638]\n",
      "epoch:31 step:24729 [D loss: 0.234417, acc.: 89.06%] [G loss: 4.335694]\n",
      "epoch:31 step:24730 [D loss: 0.250098, acc.: 90.62%] [G loss: 5.230689]\n",
      "epoch:31 step:24731 [D loss: 0.263652, acc.: 85.16%] [G loss: 6.765083]\n",
      "epoch:31 step:24732 [D loss: 0.264200, acc.: 87.50%] [G loss: 5.056508]\n",
      "epoch:31 step:24733 [D loss: 0.218984, acc.: 91.41%] [G loss: 6.179657]\n",
      "epoch:31 step:24734 [D loss: 0.185363, acc.: 92.19%] [G loss: 4.939180]\n",
      "epoch:31 step:24735 [D loss: 0.284905, acc.: 86.72%] [G loss: 4.746103]\n",
      "epoch:31 step:24736 [D loss: 0.271980, acc.: 89.84%] [G loss: 3.378270]\n",
      "epoch:31 step:24737 [D loss: 0.378085, acc.: 82.03%] [G loss: 5.059617]\n",
      "epoch:31 step:24738 [D loss: 0.356208, acc.: 83.59%] [G loss: 3.153001]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:31 step:24739 [D loss: 0.252473, acc.: 89.84%] [G loss: 3.742148]\n",
      "epoch:31 step:24740 [D loss: 0.298113, acc.: 85.16%] [G loss: 3.666544]\n",
      "epoch:31 step:24741 [D loss: 0.538061, acc.: 71.88%] [G loss: 4.555541]\n",
      "epoch:31 step:24742 [D loss: 0.378659, acc.: 81.25%] [G loss: 4.361804]\n",
      "epoch:31 step:24743 [D loss: 0.299806, acc.: 85.16%] [G loss: 4.262266]\n",
      "epoch:31 step:24744 [D loss: 0.344008, acc.: 83.59%] [G loss: 3.964280]\n",
      "epoch:31 step:24745 [D loss: 0.266115, acc.: 89.06%] [G loss: 3.293314]\n",
      "epoch:31 step:24746 [D loss: 0.365521, acc.: 82.03%] [G loss: 2.950532]\n",
      "epoch:31 step:24747 [D loss: 0.276706, acc.: 88.28%] [G loss: 2.774371]\n",
      "epoch:31 step:24748 [D loss: 0.322947, acc.: 85.94%] [G loss: 3.019482]\n",
      "epoch:31 step:24749 [D loss: 0.367622, acc.: 81.25%] [G loss: 2.818026]\n",
      "epoch:31 step:24750 [D loss: 0.206704, acc.: 90.62%] [G loss: 2.661892]\n",
      "epoch:31 step:24751 [D loss: 0.352594, acc.: 84.38%] [G loss: 3.987004]\n",
      "epoch:31 step:24752 [D loss: 0.415253, acc.: 78.91%] [G loss: 4.751976]\n",
      "epoch:31 step:24753 [D loss: 0.483473, acc.: 78.12%] [G loss: 4.320455]\n",
      "epoch:31 step:24754 [D loss: 0.444837, acc.: 78.91%] [G loss: 2.822657]\n",
      "epoch:31 step:24755 [D loss: 0.311213, acc.: 85.16%] [G loss: 4.236971]\n",
      "epoch:31 step:24756 [D loss: 0.279904, acc.: 86.72%] [G loss: 3.094905]\n",
      "epoch:31 step:24757 [D loss: 0.172931, acc.: 92.97%] [G loss: 4.310959]\n",
      "epoch:31 step:24758 [D loss: 0.314373, acc.: 87.50%] [G loss: 3.313695]\n",
      "epoch:31 step:24759 [D loss: 0.222565, acc.: 91.41%] [G loss: 2.609693]\n",
      "epoch:31 step:24760 [D loss: 0.296805, acc.: 86.72%] [G loss: 3.577951]\n",
      "epoch:31 step:24761 [D loss: 0.349218, acc.: 82.81%] [G loss: 3.798645]\n",
      "epoch:31 step:24762 [D loss: 0.288506, acc.: 88.28%] [G loss: 3.309771]\n",
      "epoch:31 step:24763 [D loss: 0.409917, acc.: 81.25%] [G loss: 4.862875]\n",
      "epoch:31 step:24764 [D loss: 0.434693, acc.: 84.38%] [G loss: 4.105692]\n",
      "epoch:31 step:24765 [D loss: 0.517479, acc.: 72.66%] [G loss: 5.098899]\n",
      "epoch:31 step:24766 [D loss: 0.590626, acc.: 71.88%] [G loss: 4.108597]\n",
      "epoch:31 step:24767 [D loss: 0.493875, acc.: 80.47%] [G loss: 4.295463]\n",
      "epoch:31 step:24768 [D loss: 0.279207, acc.: 85.16%] [G loss: 4.011712]\n",
      "epoch:31 step:24769 [D loss: 0.291624, acc.: 87.50%] [G loss: 3.633908]\n",
      "epoch:31 step:24770 [D loss: 0.312252, acc.: 86.72%] [G loss: 3.427767]\n",
      "epoch:31 step:24771 [D loss: 0.279469, acc.: 88.28%] [G loss: 4.316763]\n",
      "epoch:31 step:24772 [D loss: 0.257167, acc.: 90.62%] [G loss: 3.299050]\n",
      "epoch:31 step:24773 [D loss: 0.293516, acc.: 88.28%] [G loss: 2.589746]\n",
      "epoch:31 step:24774 [D loss: 0.298537, acc.: 84.38%] [G loss: 2.262952]\n",
      "epoch:31 step:24775 [D loss: 0.361577, acc.: 82.03%] [G loss: 2.898275]\n",
      "epoch:31 step:24776 [D loss: 0.319180, acc.: 85.94%] [G loss: 2.821073]\n",
      "epoch:31 step:24777 [D loss: 0.288895, acc.: 89.06%] [G loss: 2.489077]\n",
      "epoch:31 step:24778 [D loss: 0.331206, acc.: 85.16%] [G loss: 3.027673]\n",
      "epoch:31 step:24779 [D loss: 0.326340, acc.: 85.16%] [G loss: 2.552903]\n",
      "epoch:31 step:24780 [D loss: 0.265146, acc.: 90.62%] [G loss: 3.416327]\n",
      "epoch:31 step:24781 [D loss: 0.261659, acc.: 91.41%] [G loss: 3.169343]\n",
      "epoch:31 step:24782 [D loss: 0.306755, acc.: 86.72%] [G loss: 2.646439]\n",
      "epoch:31 step:24783 [D loss: 0.276127, acc.: 90.62%] [G loss: 3.226368]\n",
      "epoch:31 step:24784 [D loss: 0.219304, acc.: 92.19%] [G loss: 3.587023]\n",
      "epoch:31 step:24785 [D loss: 0.276401, acc.: 84.38%] [G loss: 3.996197]\n",
      "epoch:31 step:24786 [D loss: 0.331254, acc.: 87.50%] [G loss: 2.962146]\n",
      "epoch:31 step:24787 [D loss: 0.354070, acc.: 82.81%] [G loss: 3.261488]\n",
      "epoch:31 step:24788 [D loss: 0.339866, acc.: 85.94%] [G loss: 3.603369]\n",
      "epoch:31 step:24789 [D loss: 0.328100, acc.: 85.16%] [G loss: 3.328363]\n",
      "epoch:31 step:24790 [D loss: 0.340286, acc.: 84.38%] [G loss: 5.569327]\n",
      "epoch:31 step:24791 [D loss: 0.313508, acc.: 85.94%] [G loss: 4.014824]\n",
      "epoch:31 step:24792 [D loss: 0.291776, acc.: 89.84%] [G loss: 3.392379]\n",
      "epoch:31 step:24793 [D loss: 0.259665, acc.: 88.28%] [G loss: 2.976600]\n",
      "epoch:31 step:24794 [D loss: 0.333918, acc.: 83.59%] [G loss: 2.991230]\n",
      "epoch:31 step:24795 [D loss: 0.312686, acc.: 85.16%] [G loss: 2.844186]\n",
      "epoch:31 step:24796 [D loss: 0.289929, acc.: 88.28%] [G loss: 3.090087]\n",
      "epoch:31 step:24797 [D loss: 0.262171, acc.: 89.06%] [G loss: 3.284633]\n",
      "epoch:31 step:24798 [D loss: 0.298954, acc.: 86.72%] [G loss: 2.951684]\n",
      "epoch:31 step:24799 [D loss: 0.353914, acc.: 82.81%] [G loss: 2.982277]\n",
      "epoch:31 step:24800 [D loss: 0.291919, acc.: 86.72%] [G loss: 3.014329]\n",
      "##############\n",
      "[0.86057068 0.85484468 0.80474692 0.80671473 0.76094134 0.82551706\n",
      " 0.89257777 0.83625241 0.82022138 0.8266086 ]\n",
      "##########\n",
      "epoch:31 step:24801 [D loss: 0.402827, acc.: 79.69%] [G loss: 3.335146]\n",
      "epoch:31 step:24802 [D loss: 0.294092, acc.: 88.28%] [G loss: 3.211251]\n",
      "epoch:31 step:24803 [D loss: 0.319460, acc.: 85.16%] [G loss: 2.678980]\n",
      "epoch:31 step:24804 [D loss: 0.306238, acc.: 84.38%] [G loss: 4.150243]\n",
      "epoch:31 step:24805 [D loss: 0.356807, acc.: 84.38%] [G loss: 4.209409]\n",
      "epoch:31 step:24806 [D loss: 0.348069, acc.: 85.94%] [G loss: 3.984659]\n",
      "epoch:31 step:24807 [D loss: 0.284267, acc.: 85.94%] [G loss: 4.264260]\n",
      "epoch:31 step:24808 [D loss: 0.271205, acc.: 89.84%] [G loss: 3.259009]\n",
      "epoch:31 step:24809 [D loss: 0.265933, acc.: 89.06%] [G loss: 3.420506]\n",
      "epoch:31 step:24810 [D loss: 0.331409, acc.: 85.94%] [G loss: 2.915028]\n",
      "epoch:31 step:24811 [D loss: 0.298408, acc.: 87.50%] [G loss: 2.739416]\n",
      "epoch:31 step:24812 [D loss: 0.265000, acc.: 88.28%] [G loss: 3.368820]\n",
      "epoch:31 step:24813 [D loss: 0.162121, acc.: 96.09%] [G loss: 3.075252]\n",
      "epoch:31 step:24814 [D loss: 0.183803, acc.: 93.75%] [G loss: 3.864962]\n",
      "epoch:31 step:24815 [D loss: 0.287584, acc.: 87.50%] [G loss: 2.848875]\n",
      "epoch:31 step:24816 [D loss: 0.373175, acc.: 85.94%] [G loss: 3.352609]\n",
      "epoch:31 step:24817 [D loss: 0.381423, acc.: 78.91%] [G loss: 2.912976]\n",
      "epoch:31 step:24818 [D loss: 0.243183, acc.: 88.28%] [G loss: 3.443253]\n",
      "epoch:31 step:24819 [D loss: 0.381079, acc.: 84.38%] [G loss: 3.284189]\n",
      "epoch:31 step:24820 [D loss: 0.359126, acc.: 79.69%] [G loss: 3.154454]\n",
      "epoch:31 step:24821 [D loss: 0.308400, acc.: 88.28%] [G loss: 2.855250]\n",
      "epoch:31 step:24822 [D loss: 0.331975, acc.: 85.16%] [G loss: 2.994737]\n",
      "epoch:31 step:24823 [D loss: 0.373141, acc.: 78.91%] [G loss: 2.824092]\n",
      "epoch:31 step:24824 [D loss: 0.335775, acc.: 83.59%] [G loss: 3.054222]\n",
      "epoch:31 step:24825 [D loss: 0.353170, acc.: 84.38%] [G loss: 3.035975]\n",
      "epoch:31 step:24826 [D loss: 0.228620, acc.: 90.62%] [G loss: 3.242217]\n",
      "epoch:31 step:24827 [D loss: 0.321128, acc.: 89.06%] [G loss: 3.846223]\n",
      "epoch:31 step:24828 [D loss: 0.330142, acc.: 82.81%] [G loss: 2.510829]\n",
      "epoch:31 step:24829 [D loss: 0.290171, acc.: 85.94%] [G loss: 2.519505]\n",
      "epoch:31 step:24830 [D loss: 0.281697, acc.: 84.38%] [G loss: 3.382074]\n",
      "epoch:31 step:24831 [D loss: 0.307947, acc.: 88.28%] [G loss: 2.756466]\n",
      "epoch:31 step:24832 [D loss: 0.234140, acc.: 89.84%] [G loss: 4.099732]\n",
      "epoch:31 step:24833 [D loss: 0.363749, acc.: 85.94%] [G loss: 2.973204]\n",
      "epoch:31 step:24834 [D loss: 0.398061, acc.: 80.47%] [G loss: 3.141549]\n",
      "epoch:31 step:24835 [D loss: 0.389700, acc.: 82.81%] [G loss: 4.551577]\n",
      "epoch:31 step:24836 [D loss: 0.473820, acc.: 79.69%] [G loss: 4.392820]\n",
      "epoch:31 step:24837 [D loss: 0.365636, acc.: 81.25%] [G loss: 5.808270]\n",
      "epoch:31 step:24838 [D loss: 0.282288, acc.: 90.62%] [G loss: 5.844286]\n",
      "epoch:31 step:24839 [D loss: 0.180682, acc.: 93.75%] [G loss: 5.897327]\n",
      "epoch:31 step:24840 [D loss: 0.333472, acc.: 85.94%] [G loss: 5.788488]\n",
      "epoch:31 step:24841 [D loss: 0.259256, acc.: 89.06%] [G loss: 4.550143]\n",
      "epoch:31 step:24842 [D loss: 0.313180, acc.: 85.94%] [G loss: 3.480888]\n",
      "epoch:31 step:24843 [D loss: 0.230430, acc.: 91.41%] [G loss: 3.496473]\n",
      "epoch:31 step:24844 [D loss: 0.395639, acc.: 79.69%] [G loss: 4.876550]\n",
      "epoch:31 step:24845 [D loss: 0.384313, acc.: 82.81%] [G loss: 5.574036]\n",
      "epoch:31 step:24846 [D loss: 0.372341, acc.: 81.25%] [G loss: 4.796794]\n",
      "epoch:31 step:24847 [D loss: 0.386581, acc.: 82.03%] [G loss: 3.993957]\n",
      "epoch:31 step:24848 [D loss: 0.313704, acc.: 84.38%] [G loss: 2.797398]\n",
      "epoch:31 step:24849 [D loss: 0.324600, acc.: 85.94%] [G loss: 3.594033]\n",
      "epoch:31 step:24850 [D loss: 0.282958, acc.: 86.72%] [G loss: 2.586870]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:31 step:24851 [D loss: 0.253406, acc.: 90.62%] [G loss: 3.138527]\n",
      "epoch:31 step:24852 [D loss: 0.280382, acc.: 85.94%] [G loss: 3.944076]\n",
      "epoch:31 step:24853 [D loss: 0.275684, acc.: 89.06%] [G loss: 3.077324]\n",
      "epoch:31 step:24854 [D loss: 0.315022, acc.: 84.38%] [G loss: 3.374830]\n",
      "epoch:31 step:24855 [D loss: 0.130661, acc.: 98.44%] [G loss: 3.476733]\n",
      "epoch:31 step:24856 [D loss: 0.299493, acc.: 87.50%] [G loss: 3.616426]\n",
      "epoch:31 step:24857 [D loss: 0.402098, acc.: 81.25%] [G loss: 3.149005]\n",
      "epoch:31 step:24858 [D loss: 0.340325, acc.: 84.38%] [G loss: 3.109729]\n",
      "epoch:31 step:24859 [D loss: 0.328594, acc.: 84.38%] [G loss: 3.617823]\n",
      "epoch:31 step:24860 [D loss: 0.391871, acc.: 82.81%] [G loss: 3.027918]\n",
      "epoch:31 step:24861 [D loss: 0.315905, acc.: 85.16%] [G loss: 3.051370]\n",
      "epoch:31 step:24862 [D loss: 0.261763, acc.: 89.06%] [G loss: 3.865498]\n",
      "epoch:31 step:24863 [D loss: 0.352866, acc.: 84.38%] [G loss: 3.069877]\n",
      "epoch:31 step:24864 [D loss: 0.239085, acc.: 88.28%] [G loss: 3.990044]\n",
      "epoch:31 step:24865 [D loss: 0.354284, acc.: 85.16%] [G loss: 3.684091]\n",
      "epoch:31 step:24866 [D loss: 0.300123, acc.: 85.94%] [G loss: 3.268458]\n",
      "epoch:31 step:24867 [D loss: 0.278706, acc.: 89.06%] [G loss: 3.514844]\n",
      "epoch:31 step:24868 [D loss: 0.553888, acc.: 75.00%] [G loss: 4.255146]\n",
      "epoch:31 step:24869 [D loss: 0.541578, acc.: 78.12%] [G loss: 4.048009]\n",
      "epoch:31 step:24870 [D loss: 0.285414, acc.: 89.84%] [G loss: 3.370474]\n",
      "epoch:31 step:24871 [D loss: 0.402860, acc.: 83.59%] [G loss: 5.560247]\n",
      "epoch:31 step:24872 [D loss: 0.534877, acc.: 75.78%] [G loss: 5.938163]\n",
      "epoch:31 step:24873 [D loss: 0.483058, acc.: 76.56%] [G loss: 3.399328]\n",
      "epoch:31 step:24874 [D loss: 0.301078, acc.: 86.72%] [G loss: 4.780726]\n",
      "epoch:31 step:24875 [D loss: 0.344520, acc.: 83.59%] [G loss: 3.675321]\n",
      "epoch:31 step:24876 [D loss: 0.363896, acc.: 85.16%] [G loss: 4.036781]\n",
      "epoch:31 step:24877 [D loss: 0.202377, acc.: 95.31%] [G loss: 2.723730]\n",
      "epoch:31 step:24878 [D loss: 0.312307, acc.: 85.16%] [G loss: 3.529487]\n",
      "epoch:31 step:24879 [D loss: 0.321549, acc.: 86.72%] [G loss: 3.217437]\n",
      "epoch:31 step:24880 [D loss: 0.368004, acc.: 79.69%] [G loss: 4.015203]\n",
      "epoch:31 step:24881 [D loss: 0.256781, acc.: 89.06%] [G loss: 4.057209]\n",
      "epoch:31 step:24882 [D loss: 0.438234, acc.: 80.47%] [G loss: 3.520061]\n",
      "epoch:31 step:24883 [D loss: 0.284305, acc.: 85.94%] [G loss: 4.128484]\n",
      "epoch:31 step:24884 [D loss: 0.313487, acc.: 83.59%] [G loss: 3.039871]\n",
      "epoch:31 step:24885 [D loss: 0.361062, acc.: 82.03%] [G loss: 3.209901]\n",
      "epoch:31 step:24886 [D loss: 0.305688, acc.: 85.94%] [G loss: 3.938980]\n",
      "epoch:31 step:24887 [D loss: 0.303417, acc.: 85.16%] [G loss: 3.753024]\n",
      "epoch:31 step:24888 [D loss: 0.333897, acc.: 86.72%] [G loss: 2.924999]\n",
      "epoch:31 step:24889 [D loss: 0.302008, acc.: 85.94%] [G loss: 2.729370]\n",
      "epoch:31 step:24890 [D loss: 0.339744, acc.: 85.16%] [G loss: 3.358314]\n",
      "epoch:31 step:24891 [D loss: 0.186114, acc.: 95.31%] [G loss: 4.558892]\n",
      "epoch:31 step:24892 [D loss: 0.236013, acc.: 91.41%] [G loss: 4.082912]\n",
      "epoch:31 step:24893 [D loss: 0.252439, acc.: 89.06%] [G loss: 3.039288]\n",
      "epoch:31 step:24894 [D loss: 0.244169, acc.: 87.50%] [G loss: 3.465667]\n",
      "epoch:31 step:24895 [D loss: 0.277509, acc.: 89.06%] [G loss: 2.814117]\n",
      "epoch:31 step:24896 [D loss: 0.322548, acc.: 86.72%] [G loss: 3.158251]\n",
      "epoch:31 step:24897 [D loss: 0.275865, acc.: 85.16%] [G loss: 3.759293]\n",
      "epoch:31 step:24898 [D loss: 0.335806, acc.: 85.94%] [G loss: 3.673609]\n",
      "epoch:31 step:24899 [D loss: 0.423283, acc.: 81.25%] [G loss: 3.844337]\n",
      "epoch:31 step:24900 [D loss: 0.364660, acc.: 83.59%] [G loss: 4.093108]\n",
      "epoch:31 step:24901 [D loss: 0.264838, acc.: 89.06%] [G loss: 3.722378]\n",
      "epoch:31 step:24902 [D loss: 0.261635, acc.: 85.94%] [G loss: 4.255244]\n",
      "epoch:31 step:24903 [D loss: 0.327780, acc.: 85.16%] [G loss: 2.817604]\n",
      "epoch:31 step:24904 [D loss: 0.245439, acc.: 89.84%] [G loss: 3.198836]\n",
      "epoch:31 step:24905 [D loss: 0.304281, acc.: 84.38%] [G loss: 2.950550]\n",
      "epoch:31 step:24906 [D loss: 0.196586, acc.: 94.53%] [G loss: 2.684400]\n",
      "epoch:31 step:24907 [D loss: 0.245155, acc.: 89.06%] [G loss: 4.077889]\n",
      "epoch:31 step:24908 [D loss: 0.383343, acc.: 82.81%] [G loss: 3.047316]\n",
      "epoch:31 step:24909 [D loss: 0.263507, acc.: 86.72%] [G loss: 3.160105]\n",
      "epoch:31 step:24910 [D loss: 0.311799, acc.: 85.94%] [G loss: 3.155603]\n",
      "epoch:31 step:24911 [D loss: 0.276378, acc.: 85.16%] [G loss: 3.464529]\n",
      "epoch:31 step:24912 [D loss: 0.315273, acc.: 84.38%] [G loss: 2.839946]\n",
      "epoch:31 step:24913 [D loss: 0.236781, acc.: 88.28%] [G loss: 2.803728]\n",
      "epoch:31 step:24914 [D loss: 0.457408, acc.: 77.34%] [G loss: 3.667976]\n",
      "epoch:31 step:24915 [D loss: 0.304747, acc.: 83.59%] [G loss: 4.816401]\n",
      "epoch:31 step:24916 [D loss: 0.222152, acc.: 92.97%] [G loss: 3.109560]\n",
      "epoch:31 step:24917 [D loss: 0.317648, acc.: 84.38%] [G loss: 3.565023]\n",
      "epoch:31 step:24918 [D loss: 0.287165, acc.: 87.50%] [G loss: 3.948732]\n",
      "epoch:31 step:24919 [D loss: 0.351136, acc.: 82.03%] [G loss: 3.002648]\n",
      "epoch:31 step:24920 [D loss: 0.325605, acc.: 82.81%] [G loss: 2.611260]\n",
      "epoch:31 step:24921 [D loss: 0.303435, acc.: 85.16%] [G loss: 3.916291]\n",
      "epoch:31 step:24922 [D loss: 0.189612, acc.: 92.97%] [G loss: 3.382681]\n",
      "epoch:31 step:24923 [D loss: 0.275410, acc.: 89.06%] [G loss: 2.593590]\n",
      "epoch:31 step:24924 [D loss: 0.399630, acc.: 82.03%] [G loss: 4.953750]\n",
      "epoch:31 step:24925 [D loss: 0.344309, acc.: 85.94%] [G loss: 6.114439]\n",
      "epoch:31 step:24926 [D loss: 0.340263, acc.: 81.25%] [G loss: 3.888735]\n",
      "epoch:31 step:24927 [D loss: 0.230187, acc.: 92.97%] [G loss: 3.035104]\n",
      "epoch:31 step:24928 [D loss: 0.322965, acc.: 88.28%] [G loss: 4.889208]\n",
      "epoch:31 step:24929 [D loss: 0.260295, acc.: 89.06%] [G loss: 3.672609]\n",
      "epoch:31 step:24930 [D loss: 0.239198, acc.: 89.84%] [G loss: 4.528393]\n",
      "epoch:31 step:24931 [D loss: 0.280742, acc.: 85.94%] [G loss: 3.899531]\n",
      "epoch:31 step:24932 [D loss: 0.301910, acc.: 86.72%] [G loss: 4.378376]\n",
      "epoch:31 step:24933 [D loss: 0.292534, acc.: 84.38%] [G loss: 3.697778]\n",
      "epoch:31 step:24934 [D loss: 0.314901, acc.: 84.38%] [G loss: 2.801672]\n",
      "epoch:31 step:24935 [D loss: 0.236720, acc.: 89.84%] [G loss: 3.670627]\n",
      "epoch:31 step:24936 [D loss: 0.280891, acc.: 89.84%] [G loss: 3.172939]\n",
      "epoch:31 step:24937 [D loss: 0.227969, acc.: 92.97%] [G loss: 3.546156]\n",
      "epoch:31 step:24938 [D loss: 0.294104, acc.: 87.50%] [G loss: 3.738877]\n",
      "epoch:31 step:24939 [D loss: 0.340056, acc.: 84.38%] [G loss: 3.936940]\n",
      "epoch:31 step:24940 [D loss: 0.278846, acc.: 87.50%] [G loss: 3.074107]\n",
      "epoch:31 step:24941 [D loss: 0.282672, acc.: 90.62%] [G loss: 3.321272]\n",
      "epoch:31 step:24942 [D loss: 0.256892, acc.: 89.06%] [G loss: 3.514309]\n",
      "epoch:31 step:24943 [D loss: 0.396621, acc.: 80.47%] [G loss: 2.574811]\n",
      "epoch:31 step:24944 [D loss: 0.229437, acc.: 90.62%] [G loss: 3.413523]\n",
      "epoch:31 step:24945 [D loss: 0.265889, acc.: 88.28%] [G loss: 2.864555]\n",
      "epoch:31 step:24946 [D loss: 0.408848, acc.: 78.91%] [G loss: 4.073782]\n",
      "epoch:31 step:24947 [D loss: 0.423770, acc.: 81.25%] [G loss: 5.939843]\n",
      "epoch:31 step:24948 [D loss: 0.442388, acc.: 82.03%] [G loss: 4.735170]\n",
      "epoch:31 step:24949 [D loss: 0.521693, acc.: 71.09%] [G loss: 3.611851]\n",
      "epoch:31 step:24950 [D loss: 0.290579, acc.: 89.84%] [G loss: 3.306668]\n",
      "epoch:31 step:24951 [D loss: 0.386818, acc.: 82.81%] [G loss: 3.294810]\n",
      "epoch:31 step:24952 [D loss: 0.310845, acc.: 84.38%] [G loss: 3.026111]\n",
      "epoch:31 step:24953 [D loss: 0.290302, acc.: 84.38%] [G loss: 2.966719]\n",
      "epoch:31 step:24954 [D loss: 0.271685, acc.: 88.28%] [G loss: 2.947468]\n",
      "epoch:31 step:24955 [D loss: 0.240794, acc.: 92.19%] [G loss: 2.539800]\n",
      "epoch:31 step:24956 [D loss: 0.270078, acc.: 85.94%] [G loss: 3.932102]\n",
      "epoch:31 step:24957 [D loss: 0.307689, acc.: 84.38%] [G loss: 2.704286]\n",
      "epoch:31 step:24958 [D loss: 0.237046, acc.: 90.62%] [G loss: 2.998976]\n",
      "epoch:31 step:24959 [D loss: 0.351872, acc.: 82.03%] [G loss: 3.514851]\n",
      "epoch:31 step:24960 [D loss: 0.316884, acc.: 83.59%] [G loss: 4.220252]\n",
      "epoch:31 step:24961 [D loss: 0.302346, acc.: 86.72%] [G loss: 3.881182]\n",
      "epoch:31 step:24962 [D loss: 0.200868, acc.: 93.75%] [G loss: 3.272301]\n",
      "epoch:31 step:24963 [D loss: 0.319177, acc.: 87.50%] [G loss: 3.485834]\n",
      "epoch:31 step:24964 [D loss: 0.265642, acc.: 85.94%] [G loss: 3.797712]\n",
      "epoch:31 step:24965 [D loss: 0.187182, acc.: 90.62%] [G loss: 3.460571]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:31 step:24966 [D loss: 0.291785, acc.: 86.72%] [G loss: 3.125733]\n",
      "epoch:31 step:24967 [D loss: 0.253294, acc.: 89.84%] [G loss: 2.469447]\n",
      "epoch:31 step:24968 [D loss: 0.276291, acc.: 89.84%] [G loss: 2.880497]\n",
      "epoch:31 step:24969 [D loss: 0.356830, acc.: 84.38%] [G loss: 2.957297]\n",
      "epoch:31 step:24970 [D loss: 0.363960, acc.: 81.25%] [G loss: 2.768978]\n",
      "epoch:31 step:24971 [D loss: 0.269678, acc.: 88.28%] [G loss: 2.901916]\n",
      "epoch:31 step:24972 [D loss: 0.451442, acc.: 80.47%] [G loss: 4.337418]\n",
      "epoch:31 step:24973 [D loss: 0.362214, acc.: 79.69%] [G loss: 5.146781]\n",
      "epoch:31 step:24974 [D loss: 0.641804, acc.: 71.88%] [G loss: 4.247615]\n",
      "epoch:31 step:24975 [D loss: 0.432547, acc.: 83.59%] [G loss: 4.547840]\n",
      "epoch:31 step:24976 [D loss: 0.556705, acc.: 76.56%] [G loss: 3.117061]\n",
      "epoch:31 step:24977 [D loss: 0.387190, acc.: 82.03%] [G loss: 3.128948]\n",
      "epoch:31 step:24978 [D loss: 0.370348, acc.: 84.38%] [G loss: 3.877676]\n",
      "epoch:31 step:24979 [D loss: 0.284720, acc.: 89.84%] [G loss: 4.662416]\n",
      "epoch:31 step:24980 [D loss: 0.309172, acc.: 85.16%] [G loss: 3.329623]\n",
      "epoch:31 step:24981 [D loss: 0.296940, acc.: 85.94%] [G loss: 3.185401]\n",
      "epoch:31 step:24982 [D loss: 0.323783, acc.: 85.94%] [G loss: 3.978400]\n",
      "epoch:31 step:24983 [D loss: 0.290902, acc.: 85.94%] [G loss: 2.951502]\n",
      "epoch:31 step:24984 [D loss: 0.281334, acc.: 86.72%] [G loss: 3.539041]\n",
      "epoch:31 step:24985 [D loss: 0.327474, acc.: 84.38%] [G loss: 3.268247]\n",
      "epoch:31 step:24986 [D loss: 0.334075, acc.: 84.38%] [G loss: 3.899588]\n",
      "epoch:31 step:24987 [D loss: 0.204812, acc.: 93.75%] [G loss: 3.233877]\n",
      "epoch:31 step:24988 [D loss: 0.265919, acc.: 87.50%] [G loss: 3.473581]\n",
      "epoch:31 step:24989 [D loss: 0.381164, acc.: 81.25%] [G loss: 2.775496]\n",
      "epoch:31 step:24990 [D loss: 0.314336, acc.: 87.50%] [G loss: 3.166937]\n",
      "epoch:31 step:24991 [D loss: 0.244922, acc.: 89.84%] [G loss: 2.859942]\n",
      "epoch:31 step:24992 [D loss: 0.311922, acc.: 89.06%] [G loss: 3.117103]\n",
      "epoch:32 step:24993 [D loss: 0.206164, acc.: 91.41%] [G loss: 3.007387]\n",
      "epoch:32 step:24994 [D loss: 0.321623, acc.: 82.81%] [G loss: 2.750067]\n",
      "epoch:32 step:24995 [D loss: 0.501959, acc.: 72.66%] [G loss: 2.374599]\n",
      "epoch:32 step:24996 [D loss: 0.342637, acc.: 79.69%] [G loss: 3.151170]\n",
      "epoch:32 step:24997 [D loss: 0.290618, acc.: 85.16%] [G loss: 2.957535]\n",
      "epoch:32 step:24998 [D loss: 0.330551, acc.: 85.94%] [G loss: 4.493636]\n",
      "epoch:32 step:24999 [D loss: 0.300498, acc.: 88.28%] [G loss: 3.073001]\n",
      "epoch:32 step:25000 [D loss: 0.212955, acc.: 88.28%] [G loss: 5.498434]\n",
      "##############\n",
      "[0.86184419 0.8396233  0.81505964 0.79310264 0.76660256 0.82235128\n",
      " 0.86931422 0.82793421 0.8066472  0.82183283]\n",
      "##########\n",
      "epoch:32 step:25001 [D loss: 0.391818, acc.: 80.47%] [G loss: 3.117121]\n",
      "epoch:32 step:25002 [D loss: 0.190129, acc.: 92.19%] [G loss: 3.039490]\n",
      "epoch:32 step:25003 [D loss: 0.338316, acc.: 83.59%] [G loss: 3.427502]\n",
      "epoch:32 step:25004 [D loss: 0.243235, acc.: 89.06%] [G loss: 5.017042]\n",
      "epoch:32 step:25005 [D loss: 0.245051, acc.: 89.84%] [G loss: 4.450732]\n",
      "epoch:32 step:25006 [D loss: 0.284350, acc.: 85.94%] [G loss: 4.225666]\n",
      "epoch:32 step:25007 [D loss: 0.391824, acc.: 83.59%] [G loss: 4.043331]\n",
      "epoch:32 step:25008 [D loss: 0.236883, acc.: 89.84%] [G loss: 3.181533]\n",
      "epoch:32 step:25009 [D loss: 0.177869, acc.: 93.75%] [G loss: 4.044896]\n",
      "epoch:32 step:25010 [D loss: 0.363295, acc.: 83.59%] [G loss: 3.125794]\n",
      "epoch:32 step:25011 [D loss: 0.228739, acc.: 89.06%] [G loss: 2.772841]\n",
      "epoch:32 step:25012 [D loss: 0.283654, acc.: 85.94%] [G loss: 3.691366]\n",
      "epoch:32 step:25013 [D loss: 0.386978, acc.: 82.03%] [G loss: 3.453519]\n",
      "epoch:32 step:25014 [D loss: 0.273831, acc.: 86.72%] [G loss: 5.703186]\n",
      "epoch:32 step:25015 [D loss: 0.190411, acc.: 92.19%] [G loss: 3.266438]\n",
      "epoch:32 step:25016 [D loss: 0.301972, acc.: 89.06%] [G loss: 3.583807]\n",
      "epoch:32 step:25017 [D loss: 0.276876, acc.: 89.06%] [G loss: 2.709468]\n",
      "epoch:32 step:25018 [D loss: 0.239750, acc.: 91.41%] [G loss: 3.709816]\n",
      "epoch:32 step:25019 [D loss: 0.225380, acc.: 89.06%] [G loss: 3.148325]\n",
      "epoch:32 step:25020 [D loss: 0.237350, acc.: 90.62%] [G loss: 3.764434]\n",
      "epoch:32 step:25021 [D loss: 0.315651, acc.: 88.28%] [G loss: 3.137589]\n",
      "epoch:32 step:25022 [D loss: 0.225829, acc.: 88.28%] [G loss: 3.308315]\n",
      "epoch:32 step:25023 [D loss: 0.280364, acc.: 86.72%] [G loss: 4.284438]\n",
      "epoch:32 step:25024 [D loss: 0.376745, acc.: 82.81%] [G loss: 3.194175]\n",
      "epoch:32 step:25025 [D loss: 0.145253, acc.: 93.75%] [G loss: 3.862087]\n",
      "epoch:32 step:25026 [D loss: 0.354551, acc.: 85.16%] [G loss: 4.341177]\n",
      "epoch:32 step:25027 [D loss: 0.297514, acc.: 89.84%] [G loss: 3.091685]\n",
      "epoch:32 step:25028 [D loss: 0.298348, acc.: 83.59%] [G loss: 4.529771]\n",
      "epoch:32 step:25029 [D loss: 0.303999, acc.: 85.94%] [G loss: 4.340322]\n",
      "epoch:32 step:25030 [D loss: 0.410848, acc.: 80.47%] [G loss: 3.354538]\n",
      "epoch:32 step:25031 [D loss: 0.255958, acc.: 89.06%] [G loss: 3.655162]\n",
      "epoch:32 step:25032 [D loss: 0.336015, acc.: 86.72%] [G loss: 3.901845]\n",
      "epoch:32 step:25033 [D loss: 0.334764, acc.: 85.94%] [G loss: 3.231663]\n",
      "epoch:32 step:25034 [D loss: 0.229804, acc.: 89.84%] [G loss: 3.898505]\n",
      "epoch:32 step:25035 [D loss: 0.339347, acc.: 82.03%] [G loss: 3.112413]\n",
      "epoch:32 step:25036 [D loss: 0.221653, acc.: 95.31%] [G loss: 3.991594]\n",
      "epoch:32 step:25037 [D loss: 0.266589, acc.: 90.62%] [G loss: 3.467818]\n",
      "epoch:32 step:25038 [D loss: 0.356591, acc.: 83.59%] [G loss: 4.480712]\n",
      "epoch:32 step:25039 [D loss: 0.418481, acc.: 80.47%] [G loss: 3.519987]\n",
      "epoch:32 step:25040 [D loss: 0.408165, acc.: 82.81%] [G loss: 2.360008]\n",
      "epoch:32 step:25041 [D loss: 0.268306, acc.: 86.72%] [G loss: 3.471715]\n",
      "epoch:32 step:25042 [D loss: 0.232983, acc.: 89.84%] [G loss: 4.137754]\n",
      "epoch:32 step:25043 [D loss: 0.408123, acc.: 78.91%] [G loss: 4.337150]\n",
      "epoch:32 step:25044 [D loss: 0.382342, acc.: 80.47%] [G loss: 3.928865]\n",
      "epoch:32 step:25045 [D loss: 0.287280, acc.: 85.94%] [G loss: 4.390192]\n",
      "epoch:32 step:25046 [D loss: 0.252454, acc.: 90.62%] [G loss: 4.209093]\n",
      "epoch:32 step:25047 [D loss: 0.292337, acc.: 88.28%] [G loss: 4.225463]\n",
      "epoch:32 step:25048 [D loss: 0.340126, acc.: 84.38%] [G loss: 2.936979]\n",
      "epoch:32 step:25049 [D loss: 0.294729, acc.: 85.94%] [G loss: 4.602352]\n",
      "epoch:32 step:25050 [D loss: 0.366611, acc.: 85.94%] [G loss: 3.863060]\n",
      "epoch:32 step:25051 [D loss: 0.339395, acc.: 85.94%] [G loss: 4.057126]\n",
      "epoch:32 step:25052 [D loss: 0.228056, acc.: 91.41%] [G loss: 2.764042]\n",
      "epoch:32 step:25053 [D loss: 0.368636, acc.: 81.25%] [G loss: 4.398212]\n",
      "epoch:32 step:25054 [D loss: 0.332970, acc.: 84.38%] [G loss: 3.380541]\n",
      "epoch:32 step:25055 [D loss: 0.367399, acc.: 82.81%] [G loss: 3.687345]\n",
      "epoch:32 step:25056 [D loss: 0.445444, acc.: 72.66%] [G loss: 4.160879]\n",
      "epoch:32 step:25057 [D loss: 0.394724, acc.: 82.81%] [G loss: 4.594608]\n",
      "epoch:32 step:25058 [D loss: 0.478576, acc.: 76.56%] [G loss: 3.298840]\n",
      "epoch:32 step:25059 [D loss: 0.252103, acc.: 90.62%] [G loss: 3.937294]\n",
      "epoch:32 step:25060 [D loss: 0.431137, acc.: 78.12%] [G loss: 3.882746]\n",
      "epoch:32 step:25061 [D loss: 0.553910, acc.: 71.09%] [G loss: 3.105614]\n",
      "epoch:32 step:25062 [D loss: 0.446135, acc.: 77.34%] [G loss: 4.633778]\n",
      "epoch:32 step:25063 [D loss: 0.605939, acc.: 73.44%] [G loss: 4.755947]\n",
      "epoch:32 step:25064 [D loss: 0.448314, acc.: 85.16%] [G loss: 2.638739]\n",
      "epoch:32 step:25065 [D loss: 0.243207, acc.: 90.62%] [G loss: 2.719814]\n",
      "epoch:32 step:25066 [D loss: 0.275184, acc.: 87.50%] [G loss: 3.259310]\n",
      "epoch:32 step:25067 [D loss: 0.415408, acc.: 84.38%] [G loss: 3.241963]\n",
      "epoch:32 step:25068 [D loss: 0.381364, acc.: 79.69%] [G loss: 4.952158]\n",
      "epoch:32 step:25069 [D loss: 0.370580, acc.: 78.12%] [G loss: 3.303045]\n",
      "epoch:32 step:25070 [D loss: 0.328770, acc.: 85.94%] [G loss: 3.115283]\n",
      "epoch:32 step:25071 [D loss: 0.352328, acc.: 82.81%] [G loss: 3.609821]\n",
      "epoch:32 step:25072 [D loss: 0.260944, acc.: 89.06%] [G loss: 3.500406]\n",
      "epoch:32 step:25073 [D loss: 0.358064, acc.: 82.03%] [G loss: 3.248519]\n",
      "epoch:32 step:25074 [D loss: 0.216100, acc.: 92.19%] [G loss: 2.836266]\n",
      "epoch:32 step:25075 [D loss: 0.221285, acc.: 91.41%] [G loss: 3.452344]\n",
      "epoch:32 step:25076 [D loss: 0.188800, acc.: 93.75%] [G loss: 3.590065]\n",
      "epoch:32 step:25077 [D loss: 0.238221, acc.: 90.62%] [G loss: 2.804848]\n",
      "epoch:32 step:25078 [D loss: 0.233055, acc.: 92.19%] [G loss: 4.193361]\n",
      "epoch:32 step:25079 [D loss: 0.311997, acc.: 85.16%] [G loss: 3.799811]\n",
      "epoch:32 step:25080 [D loss: 0.304974, acc.: 87.50%] [G loss: 2.931921]\n",
      "epoch:32 step:25081 [D loss: 0.360395, acc.: 79.69%] [G loss: 2.598531]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:32 step:25082 [D loss: 0.305984, acc.: 84.38%] [G loss: 3.552485]\n",
      "epoch:32 step:25083 [D loss: 0.183300, acc.: 92.97%] [G loss: 3.689828]\n",
      "epoch:32 step:25084 [D loss: 0.314700, acc.: 84.38%] [G loss: 3.526842]\n",
      "epoch:32 step:25085 [D loss: 0.244979, acc.: 89.06%] [G loss: 4.342081]\n",
      "epoch:32 step:25086 [D loss: 0.274792, acc.: 85.16%] [G loss: 4.016154]\n",
      "epoch:32 step:25087 [D loss: 0.274309, acc.: 89.06%] [G loss: 3.070392]\n",
      "epoch:32 step:25088 [D loss: 0.291374, acc.: 87.50%] [G loss: 2.981507]\n",
      "epoch:32 step:25089 [D loss: 0.233924, acc.: 89.84%] [G loss: 4.303113]\n",
      "epoch:32 step:25090 [D loss: 0.199217, acc.: 94.53%] [G loss: 4.547534]\n",
      "epoch:32 step:25091 [D loss: 0.302188, acc.: 82.81%] [G loss: 4.529382]\n",
      "epoch:32 step:25092 [D loss: 0.312784, acc.: 84.38%] [G loss: 3.491193]\n",
      "epoch:32 step:25093 [D loss: 0.260550, acc.: 89.06%] [G loss: 4.171883]\n",
      "epoch:32 step:25094 [D loss: 0.343899, acc.: 85.16%] [G loss: 3.569892]\n",
      "epoch:32 step:25095 [D loss: 0.259919, acc.: 91.41%] [G loss: 4.059367]\n",
      "epoch:32 step:25096 [D loss: 0.285333, acc.: 85.94%] [G loss: 3.753159]\n",
      "epoch:32 step:25097 [D loss: 0.386816, acc.: 83.59%] [G loss: 3.170358]\n",
      "epoch:32 step:25098 [D loss: 0.319642, acc.: 84.38%] [G loss: 2.879173]\n",
      "epoch:32 step:25099 [D loss: 0.420861, acc.: 79.69%] [G loss: 4.512443]\n",
      "epoch:32 step:25100 [D loss: 0.188261, acc.: 93.75%] [G loss: 3.362560]\n",
      "epoch:32 step:25101 [D loss: 0.338911, acc.: 86.72%] [G loss: 3.437624]\n",
      "epoch:32 step:25102 [D loss: 0.392105, acc.: 82.03%] [G loss: 2.698322]\n",
      "epoch:32 step:25103 [D loss: 0.250568, acc.: 91.41%] [G loss: 3.037795]\n",
      "epoch:32 step:25104 [D loss: 0.338854, acc.: 85.94%] [G loss: 3.178634]\n",
      "epoch:32 step:25105 [D loss: 0.341146, acc.: 83.59%] [G loss: 3.202302]\n",
      "epoch:32 step:25106 [D loss: 0.296613, acc.: 90.62%] [G loss: 3.384558]\n",
      "epoch:32 step:25107 [D loss: 0.384016, acc.: 78.12%] [G loss: 3.016752]\n",
      "epoch:32 step:25108 [D loss: 0.354923, acc.: 84.38%] [G loss: 3.713667]\n",
      "epoch:32 step:25109 [D loss: 0.332846, acc.: 85.16%] [G loss: 4.187737]\n",
      "epoch:32 step:25110 [D loss: 0.266015, acc.: 87.50%] [G loss: 3.711613]\n",
      "epoch:32 step:25111 [D loss: 0.410223, acc.: 82.81%] [G loss: 6.552746]\n",
      "epoch:32 step:25112 [D loss: 0.453368, acc.: 78.12%] [G loss: 4.013862]\n",
      "epoch:32 step:25113 [D loss: 0.270141, acc.: 88.28%] [G loss: 3.508168]\n",
      "epoch:32 step:25114 [D loss: 0.251545, acc.: 87.50%] [G loss: 3.749844]\n",
      "epoch:32 step:25115 [D loss: 0.239914, acc.: 90.62%] [G loss: 4.202938]\n",
      "epoch:32 step:25116 [D loss: 0.333768, acc.: 82.81%] [G loss: 4.064840]\n",
      "epoch:32 step:25117 [D loss: 0.251950, acc.: 88.28%] [G loss: 4.032092]\n",
      "epoch:32 step:25118 [D loss: 0.216585, acc.: 91.41%] [G loss: 3.812130]\n",
      "epoch:32 step:25119 [D loss: 0.243671, acc.: 88.28%] [G loss: 3.673426]\n",
      "epoch:32 step:25120 [D loss: 0.136729, acc.: 93.75%] [G loss: 4.052494]\n",
      "epoch:32 step:25121 [D loss: 0.405331, acc.: 81.25%] [G loss: 4.653955]\n",
      "epoch:32 step:25122 [D loss: 0.230662, acc.: 92.19%] [G loss: 2.852363]\n",
      "epoch:32 step:25123 [D loss: 0.331441, acc.: 85.94%] [G loss: 4.142238]\n",
      "epoch:32 step:25124 [D loss: 0.331152, acc.: 83.59%] [G loss: 4.529838]\n",
      "epoch:32 step:25125 [D loss: 0.282159, acc.: 89.06%] [G loss: 3.793255]\n",
      "epoch:32 step:25126 [D loss: 0.248867, acc.: 92.19%] [G loss: 4.679287]\n",
      "epoch:32 step:25127 [D loss: 0.456790, acc.: 84.38%] [G loss: 5.257276]\n",
      "epoch:32 step:25128 [D loss: 0.259858, acc.: 88.28%] [G loss: 3.708674]\n",
      "epoch:32 step:25129 [D loss: 0.307388, acc.: 85.94%] [G loss: 6.649632]\n",
      "epoch:32 step:25130 [D loss: 0.263173, acc.: 87.50%] [G loss: 4.126898]\n",
      "epoch:32 step:25131 [D loss: 0.330482, acc.: 89.06%] [G loss: 5.279724]\n",
      "epoch:32 step:25132 [D loss: 0.243001, acc.: 89.84%] [G loss: 4.560492]\n",
      "epoch:32 step:25133 [D loss: 0.281947, acc.: 87.50%] [G loss: 3.247498]\n",
      "epoch:32 step:25134 [D loss: 0.361202, acc.: 85.16%] [G loss: 5.386168]\n",
      "epoch:32 step:25135 [D loss: 0.262484, acc.: 87.50%] [G loss: 3.280938]\n",
      "epoch:32 step:25136 [D loss: 0.381123, acc.: 82.03%] [G loss: 4.837164]\n",
      "epoch:32 step:25137 [D loss: 0.505325, acc.: 76.56%] [G loss: 5.197768]\n",
      "epoch:32 step:25138 [D loss: 0.521474, acc.: 78.91%] [G loss: 4.712476]\n",
      "epoch:32 step:25139 [D loss: 0.385126, acc.: 81.25%] [G loss: 3.759763]\n",
      "epoch:32 step:25140 [D loss: 0.294720, acc.: 87.50%] [G loss: 4.384388]\n",
      "epoch:32 step:25141 [D loss: 0.358175, acc.: 83.59%] [G loss: 2.742539]\n",
      "epoch:32 step:25142 [D loss: 0.277526, acc.: 87.50%] [G loss: 2.827991]\n",
      "epoch:32 step:25143 [D loss: 0.496349, acc.: 82.03%] [G loss: 3.567859]\n",
      "epoch:32 step:25144 [D loss: 0.384694, acc.: 82.81%] [G loss: 3.181234]\n",
      "epoch:32 step:25145 [D loss: 0.372679, acc.: 84.38%] [G loss: 2.603727]\n",
      "epoch:32 step:25146 [D loss: 0.253259, acc.: 87.50%] [G loss: 2.993643]\n",
      "epoch:32 step:25147 [D loss: 0.289796, acc.: 89.06%] [G loss: 3.161353]\n",
      "epoch:32 step:25148 [D loss: 0.286638, acc.: 89.84%] [G loss: 2.870851]\n",
      "epoch:32 step:25149 [D loss: 0.361842, acc.: 82.81%] [G loss: 2.655452]\n",
      "epoch:32 step:25150 [D loss: 0.252580, acc.: 92.19%] [G loss: 2.882433]\n",
      "epoch:32 step:25151 [D loss: 0.332417, acc.: 88.28%] [G loss: 2.685414]\n",
      "epoch:32 step:25152 [D loss: 0.517472, acc.: 74.22%] [G loss: 4.239630]\n",
      "epoch:32 step:25153 [D loss: 0.368848, acc.: 85.94%] [G loss: 4.913687]\n",
      "epoch:32 step:25154 [D loss: 0.331749, acc.: 85.16%] [G loss: 8.130451]\n",
      "epoch:32 step:25155 [D loss: 0.227378, acc.: 90.62%] [G loss: 4.838754]\n",
      "epoch:32 step:25156 [D loss: 0.253568, acc.: 89.06%] [G loss: 3.515038]\n",
      "epoch:32 step:25157 [D loss: 0.336295, acc.: 85.16%] [G loss: 3.790378]\n",
      "epoch:32 step:25158 [D loss: 0.257699, acc.: 89.84%] [G loss: 4.390108]\n",
      "epoch:32 step:25159 [D loss: 0.244006, acc.: 89.84%] [G loss: 4.225748]\n",
      "epoch:32 step:25160 [D loss: 0.253007, acc.: 87.50%] [G loss: 3.895795]\n",
      "epoch:32 step:25161 [D loss: 0.232585, acc.: 89.06%] [G loss: 5.506557]\n",
      "epoch:32 step:25162 [D loss: 0.193235, acc.: 92.19%] [G loss: 3.922240]\n",
      "epoch:32 step:25163 [D loss: 0.363293, acc.: 84.38%] [G loss: 4.340881]\n",
      "epoch:32 step:25164 [D loss: 0.250088, acc.: 89.06%] [G loss: 3.314919]\n",
      "epoch:32 step:25165 [D loss: 0.281211, acc.: 85.16%] [G loss: 3.743719]\n",
      "epoch:32 step:25166 [D loss: 0.312914, acc.: 88.28%] [G loss: 3.720548]\n",
      "epoch:32 step:25167 [D loss: 0.323296, acc.: 85.16%] [G loss: 2.926784]\n",
      "epoch:32 step:25168 [D loss: 0.273904, acc.: 90.62%] [G loss: 2.247848]\n",
      "epoch:32 step:25169 [D loss: 0.296425, acc.: 88.28%] [G loss: 4.002872]\n",
      "epoch:32 step:25170 [D loss: 0.367256, acc.: 82.03%] [G loss: 4.652833]\n",
      "epoch:32 step:25171 [D loss: 0.243281, acc.: 89.06%] [G loss: 4.111006]\n",
      "epoch:32 step:25172 [D loss: 0.326422, acc.: 81.25%] [G loss: 4.611080]\n",
      "epoch:32 step:25173 [D loss: 0.246910, acc.: 89.06%] [G loss: 5.077265]\n",
      "epoch:32 step:25174 [D loss: 0.287007, acc.: 85.94%] [G loss: 3.830472]\n",
      "epoch:32 step:25175 [D loss: 0.279360, acc.: 90.62%] [G loss: 4.739337]\n",
      "epoch:32 step:25176 [D loss: 0.342246, acc.: 82.03%] [G loss: 3.392303]\n",
      "epoch:32 step:25177 [D loss: 0.271053, acc.: 86.72%] [G loss: 3.724895]\n",
      "epoch:32 step:25178 [D loss: 0.315881, acc.: 82.81%] [G loss: 2.594890]\n",
      "epoch:32 step:25179 [D loss: 0.324150, acc.: 85.16%] [G loss: 3.240494]\n",
      "epoch:32 step:25180 [D loss: 0.459449, acc.: 81.25%] [G loss: 2.692220]\n",
      "epoch:32 step:25181 [D loss: 0.427197, acc.: 75.78%] [G loss: 3.672986]\n",
      "epoch:32 step:25182 [D loss: 0.418299, acc.: 78.91%] [G loss: 3.389202]\n",
      "epoch:32 step:25183 [D loss: 0.398828, acc.: 79.69%] [G loss: 2.982687]\n",
      "epoch:32 step:25184 [D loss: 0.218773, acc.: 91.41%] [G loss: 3.905335]\n",
      "epoch:32 step:25185 [D loss: 0.292612, acc.: 85.94%] [G loss: 5.387258]\n",
      "epoch:32 step:25186 [D loss: 0.326894, acc.: 84.38%] [G loss: 4.939316]\n",
      "epoch:32 step:25187 [D loss: 0.307624, acc.: 85.94%] [G loss: 4.503543]\n",
      "epoch:32 step:25188 [D loss: 0.197355, acc.: 89.84%] [G loss: 4.146510]\n",
      "epoch:32 step:25189 [D loss: 0.315649, acc.: 86.72%] [G loss: 3.840510]\n",
      "epoch:32 step:25190 [D loss: 0.296312, acc.: 84.38%] [G loss: 3.406229]\n",
      "epoch:32 step:25191 [D loss: 0.262320, acc.: 89.06%] [G loss: 4.517706]\n",
      "epoch:32 step:25192 [D loss: 0.359609, acc.: 84.38%] [G loss: 3.828811]\n",
      "epoch:32 step:25193 [D loss: 0.229539, acc.: 92.19%] [G loss: 2.999324]\n",
      "epoch:32 step:25194 [D loss: 0.460862, acc.: 73.44%] [G loss: 3.191855]\n",
      "epoch:32 step:25195 [D loss: 0.247840, acc.: 91.41%] [G loss: 3.047244]\n",
      "epoch:32 step:25196 [D loss: 0.295957, acc.: 83.59%] [G loss: 4.296196]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:32 step:25197 [D loss: 0.294568, acc.: 86.72%] [G loss: 3.191580]\n",
      "epoch:32 step:25198 [D loss: 0.211896, acc.: 94.53%] [G loss: 3.979070]\n",
      "epoch:32 step:25199 [D loss: 0.319890, acc.: 86.72%] [G loss: 4.271413]\n",
      "epoch:32 step:25200 [D loss: 0.304195, acc.: 85.94%] [G loss: 5.437127]\n",
      "##############\n",
      "[0.87664087 0.8698329  0.80918172 0.83554969 0.79322924 0.81946773\n",
      " 0.89243793 0.84172576 0.81125444 0.83226568]\n",
      "##########\n",
      "epoch:32 step:25201 [D loss: 0.353137, acc.: 78.91%] [G loss: 4.144979]\n",
      "epoch:32 step:25202 [D loss: 0.266360, acc.: 90.62%] [G loss: 5.279501]\n",
      "epoch:32 step:25203 [D loss: 0.519077, acc.: 74.22%] [G loss: 2.990144]\n",
      "epoch:32 step:25204 [D loss: 0.436184, acc.: 76.56%] [G loss: 3.501135]\n",
      "epoch:32 step:25205 [D loss: 0.292386, acc.: 90.62%] [G loss: 3.015197]\n",
      "epoch:32 step:25206 [D loss: 0.314185, acc.: 82.03%] [G loss: 4.834697]\n",
      "epoch:32 step:25207 [D loss: 0.273001, acc.: 89.84%] [G loss: 4.568070]\n",
      "epoch:32 step:25208 [D loss: 0.272817, acc.: 89.06%] [G loss: 3.165373]\n",
      "epoch:32 step:25209 [D loss: 0.306537, acc.: 85.94%] [G loss: 3.394047]\n",
      "epoch:32 step:25210 [D loss: 0.331507, acc.: 85.16%] [G loss: 3.119337]\n",
      "epoch:32 step:25211 [D loss: 0.324984, acc.: 85.16%] [G loss: 2.752820]\n",
      "epoch:32 step:25212 [D loss: 0.384634, acc.: 82.03%] [G loss: 3.539183]\n",
      "epoch:32 step:25213 [D loss: 0.432568, acc.: 83.59%] [G loss: 4.564953]\n",
      "epoch:32 step:25214 [D loss: 0.370100, acc.: 78.12%] [G loss: 4.341084]\n",
      "epoch:32 step:25215 [D loss: 0.394351, acc.: 84.38%] [G loss: 3.114408]\n",
      "epoch:32 step:25216 [D loss: 0.364313, acc.: 83.59%] [G loss: 3.791719]\n",
      "epoch:32 step:25217 [D loss: 0.352427, acc.: 80.47%] [G loss: 3.688502]\n",
      "epoch:32 step:25218 [D loss: 0.438897, acc.: 78.12%] [G loss: 3.328159]\n",
      "epoch:32 step:25219 [D loss: 0.436833, acc.: 81.25%] [G loss: 3.374393]\n",
      "epoch:32 step:25220 [D loss: 0.350889, acc.: 84.38%] [G loss: 3.160880]\n",
      "epoch:32 step:25221 [D loss: 0.371524, acc.: 82.81%] [G loss: 4.369173]\n",
      "epoch:32 step:25222 [D loss: 0.301284, acc.: 89.06%] [G loss: 3.466579]\n",
      "epoch:32 step:25223 [D loss: 0.240921, acc.: 88.28%] [G loss: 3.900211]\n",
      "epoch:32 step:25224 [D loss: 0.240983, acc.: 89.84%] [G loss: 3.593247]\n",
      "epoch:32 step:25225 [D loss: 0.283400, acc.: 87.50%] [G loss: 3.399993]\n",
      "epoch:32 step:25226 [D loss: 0.431395, acc.: 82.03%] [G loss: 3.992611]\n",
      "epoch:32 step:25227 [D loss: 0.344358, acc.: 84.38%] [G loss: 6.317711]\n",
      "epoch:32 step:25228 [D loss: 0.237946, acc.: 89.84%] [G loss: 3.442967]\n",
      "epoch:32 step:25229 [D loss: 0.354617, acc.: 84.38%] [G loss: 3.232685]\n",
      "epoch:32 step:25230 [D loss: 0.301426, acc.: 88.28%] [G loss: 3.292216]\n",
      "epoch:32 step:25231 [D loss: 0.272613, acc.: 87.50%] [G loss: 2.968916]\n",
      "epoch:32 step:25232 [D loss: 0.314159, acc.: 85.16%] [G loss: 2.850631]\n",
      "epoch:32 step:25233 [D loss: 0.370864, acc.: 85.94%] [G loss: 3.993186]\n",
      "epoch:32 step:25234 [D loss: 0.296818, acc.: 85.94%] [G loss: 2.116441]\n",
      "epoch:32 step:25235 [D loss: 0.286667, acc.: 85.16%] [G loss: 2.693811]\n",
      "epoch:32 step:25236 [D loss: 0.259709, acc.: 89.06%] [G loss: 3.313936]\n",
      "epoch:32 step:25237 [D loss: 0.327341, acc.: 85.16%] [G loss: 2.797797]\n",
      "epoch:32 step:25238 [D loss: 0.388712, acc.: 78.91%] [G loss: 4.113630]\n",
      "epoch:32 step:25239 [D loss: 0.575323, acc.: 75.00%] [G loss: 3.141093]\n",
      "epoch:32 step:25240 [D loss: 0.261474, acc.: 87.50%] [G loss: 3.068529]\n",
      "epoch:32 step:25241 [D loss: 0.361026, acc.: 82.03%] [G loss: 3.156093]\n",
      "epoch:32 step:25242 [D loss: 0.362204, acc.: 79.69%] [G loss: 3.205250]\n",
      "epoch:32 step:25243 [D loss: 0.276727, acc.: 88.28%] [G loss: 3.329316]\n",
      "epoch:32 step:25244 [D loss: 0.392006, acc.: 84.38%] [G loss: 4.101420]\n",
      "epoch:32 step:25245 [D loss: 0.351070, acc.: 78.91%] [G loss: 3.341523]\n",
      "epoch:32 step:25246 [D loss: 0.313885, acc.: 87.50%] [G loss: 3.171323]\n",
      "epoch:32 step:25247 [D loss: 0.270267, acc.: 88.28%] [G loss: 2.824480]\n",
      "epoch:32 step:25248 [D loss: 0.307089, acc.: 85.94%] [G loss: 2.789971]\n",
      "epoch:32 step:25249 [D loss: 0.357395, acc.: 85.16%] [G loss: 3.380866]\n",
      "epoch:32 step:25250 [D loss: 0.461848, acc.: 80.47%] [G loss: 2.755650]\n",
      "epoch:32 step:25251 [D loss: 0.336237, acc.: 84.38%] [G loss: 3.399392]\n",
      "epoch:32 step:25252 [D loss: 0.265467, acc.: 87.50%] [G loss: 3.021469]\n",
      "epoch:32 step:25253 [D loss: 0.305452, acc.: 85.94%] [G loss: 2.773035]\n",
      "epoch:32 step:25254 [D loss: 0.291035, acc.: 86.72%] [G loss: 3.376938]\n",
      "epoch:32 step:25255 [D loss: 0.227254, acc.: 89.84%] [G loss: 3.535905]\n",
      "epoch:32 step:25256 [D loss: 0.329724, acc.: 84.38%] [G loss: 3.884340]\n",
      "epoch:32 step:25257 [D loss: 0.256775, acc.: 90.62%] [G loss: 3.095273]\n",
      "epoch:32 step:25258 [D loss: 0.256087, acc.: 89.06%] [G loss: 3.315449]\n",
      "epoch:32 step:25259 [D loss: 0.221042, acc.: 89.84%] [G loss: 3.475994]\n",
      "epoch:32 step:25260 [D loss: 0.356586, acc.: 82.81%] [G loss: 3.168951]\n",
      "epoch:32 step:25261 [D loss: 0.329221, acc.: 83.59%] [G loss: 3.751458]\n",
      "epoch:32 step:25262 [D loss: 0.281787, acc.: 85.16%] [G loss: 3.168064]\n",
      "epoch:32 step:25263 [D loss: 0.252129, acc.: 87.50%] [G loss: 3.278805]\n",
      "epoch:32 step:25264 [D loss: 0.253076, acc.: 90.62%] [G loss: 3.842457]\n",
      "epoch:32 step:25265 [D loss: 0.361541, acc.: 82.03%] [G loss: 3.289605]\n",
      "epoch:32 step:25266 [D loss: 0.443970, acc.: 80.47%] [G loss: 4.437225]\n",
      "epoch:32 step:25267 [D loss: 0.388450, acc.: 81.25%] [G loss: 4.595467]\n",
      "epoch:32 step:25268 [D loss: 0.420585, acc.: 77.34%] [G loss: 6.239674]\n",
      "epoch:32 step:25269 [D loss: 0.307874, acc.: 89.06%] [G loss: 4.752208]\n",
      "epoch:32 step:25270 [D loss: 0.294501, acc.: 86.72%] [G loss: 5.262716]\n",
      "epoch:32 step:25271 [D loss: 0.206820, acc.: 90.62%] [G loss: 5.563797]\n",
      "epoch:32 step:25272 [D loss: 0.154555, acc.: 95.31%] [G loss: 3.928747]\n",
      "epoch:32 step:25273 [D loss: 0.269182, acc.: 90.62%] [G loss: 2.682795]\n",
      "epoch:32 step:25274 [D loss: 0.236432, acc.: 86.72%] [G loss: 3.726757]\n",
      "epoch:32 step:25275 [D loss: 0.260855, acc.: 91.41%] [G loss: 2.885348]\n",
      "epoch:32 step:25276 [D loss: 0.368773, acc.: 82.03%] [G loss: 2.389784]\n",
      "epoch:32 step:25277 [D loss: 0.361851, acc.: 79.69%] [G loss: 2.898253]\n",
      "epoch:32 step:25278 [D loss: 0.447772, acc.: 83.59%] [G loss: 4.508972]\n",
      "epoch:32 step:25279 [D loss: 0.326640, acc.: 85.16%] [G loss: 5.589122]\n",
      "epoch:32 step:25280 [D loss: 0.380991, acc.: 82.03%] [G loss: 4.604806]\n",
      "epoch:32 step:25281 [D loss: 0.525288, acc.: 75.78%] [G loss: 2.436954]\n",
      "epoch:32 step:25282 [D loss: 0.186922, acc.: 93.75%] [G loss: 4.900448]\n",
      "epoch:32 step:25283 [D loss: 0.258333, acc.: 89.84%] [G loss: 3.276634]\n",
      "epoch:32 step:25284 [D loss: 0.256397, acc.: 89.84%] [G loss: 3.527987]\n",
      "epoch:32 step:25285 [D loss: 0.235371, acc.: 90.62%] [G loss: 3.785966]\n",
      "epoch:32 step:25286 [D loss: 0.293034, acc.: 89.84%] [G loss: 3.758847]\n",
      "epoch:32 step:25287 [D loss: 0.323099, acc.: 83.59%] [G loss: 2.677472]\n",
      "epoch:32 step:25288 [D loss: 0.311740, acc.: 90.62%] [G loss: 3.307696]\n",
      "epoch:32 step:25289 [D loss: 0.284166, acc.: 87.50%] [G loss: 3.174516]\n",
      "epoch:32 step:25290 [D loss: 0.262705, acc.: 90.62%] [G loss: 3.475477]\n",
      "epoch:32 step:25291 [D loss: 0.343109, acc.: 85.94%] [G loss: 2.657655]\n",
      "epoch:32 step:25292 [D loss: 0.293848, acc.: 87.50%] [G loss: 4.118644]\n",
      "epoch:32 step:25293 [D loss: 0.286829, acc.: 86.72%] [G loss: 4.919355]\n",
      "epoch:32 step:25294 [D loss: 0.296164, acc.: 84.38%] [G loss: 3.367125]\n",
      "epoch:32 step:25295 [D loss: 0.270882, acc.: 89.06%] [G loss: 5.934392]\n",
      "epoch:32 step:25296 [D loss: 0.279306, acc.: 85.16%] [G loss: 4.062725]\n",
      "epoch:32 step:25297 [D loss: 0.283076, acc.: 89.06%] [G loss: 3.482313]\n",
      "epoch:32 step:25298 [D loss: 0.275806, acc.: 87.50%] [G loss: 4.594059]\n",
      "epoch:32 step:25299 [D loss: 0.319567, acc.: 85.16%] [G loss: 2.888700]\n",
      "epoch:32 step:25300 [D loss: 0.332641, acc.: 82.81%] [G loss: 4.109510]\n",
      "epoch:32 step:25301 [D loss: 0.258733, acc.: 89.84%] [G loss: 4.761822]\n",
      "epoch:32 step:25302 [D loss: 0.349403, acc.: 83.59%] [G loss: 3.978823]\n",
      "epoch:32 step:25303 [D loss: 0.314199, acc.: 84.38%] [G loss: 5.182028]\n",
      "epoch:32 step:25304 [D loss: 0.260959, acc.: 91.41%] [G loss: 3.114416]\n",
      "epoch:32 step:25305 [D loss: 0.302807, acc.: 85.94%] [G loss: 5.849503]\n",
      "epoch:32 step:25306 [D loss: 0.537285, acc.: 78.12%] [G loss: 6.935572]\n",
      "epoch:32 step:25307 [D loss: 1.307302, acc.: 56.25%] [G loss: 7.971723]\n",
      "epoch:32 step:25308 [D loss: 2.297850, acc.: 62.50%] [G loss: 6.630213]\n",
      "epoch:32 step:25309 [D loss: 1.270626, acc.: 68.75%] [G loss: 7.403623]\n",
      "epoch:32 step:25310 [D loss: 0.324175, acc.: 79.69%] [G loss: 7.998289]\n",
      "epoch:32 step:25311 [D loss: 0.587183, acc.: 77.34%] [G loss: 5.325577]\n",
      "epoch:32 step:25312 [D loss: 0.565730, acc.: 77.34%] [G loss: 5.949903]\n",
      "epoch:32 step:25313 [D loss: 0.247069, acc.: 88.28%] [G loss: 3.721370]\n",
      "epoch:32 step:25314 [D loss: 0.363388, acc.: 85.16%] [G loss: 7.582353]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:32 step:25315 [D loss: 0.319564, acc.: 88.28%] [G loss: 4.196797]\n",
      "epoch:32 step:25316 [D loss: 0.262246, acc.: 89.06%] [G loss: 5.259396]\n",
      "epoch:32 step:25317 [D loss: 0.405832, acc.: 82.81%] [G loss: 5.187476]\n",
      "epoch:32 step:25318 [D loss: 0.349464, acc.: 88.28%] [G loss: 4.397029]\n",
      "epoch:32 step:25319 [D loss: 0.357762, acc.: 82.81%] [G loss: 3.584234]\n",
      "epoch:32 step:25320 [D loss: 0.189336, acc.: 94.53%] [G loss: 3.773102]\n",
      "epoch:32 step:25321 [D loss: 0.333191, acc.: 83.59%] [G loss: 3.280637]\n",
      "epoch:32 step:25322 [D loss: 0.288518, acc.: 87.50%] [G loss: 3.099508]\n",
      "epoch:32 step:25323 [D loss: 0.340389, acc.: 85.94%] [G loss: 3.165358]\n",
      "epoch:32 step:25324 [D loss: 0.252392, acc.: 88.28%] [G loss: 2.605546]\n",
      "epoch:32 step:25325 [D loss: 0.335573, acc.: 85.94%] [G loss: 3.313860]\n",
      "epoch:32 step:25326 [D loss: 0.295206, acc.: 85.16%] [G loss: 2.708249]\n",
      "epoch:32 step:25327 [D loss: 0.286985, acc.: 91.41%] [G loss: 3.102602]\n",
      "epoch:32 step:25328 [D loss: 0.261127, acc.: 89.84%] [G loss: 3.184456]\n",
      "epoch:32 step:25329 [D loss: 0.304660, acc.: 85.16%] [G loss: 2.378470]\n",
      "epoch:32 step:25330 [D loss: 0.425755, acc.: 82.81%] [G loss: 2.333656]\n",
      "epoch:32 step:25331 [D loss: 0.354170, acc.: 82.81%] [G loss: 2.841824]\n",
      "epoch:32 step:25332 [D loss: 0.409913, acc.: 82.03%] [G loss: 2.698089]\n",
      "epoch:32 step:25333 [D loss: 0.315788, acc.: 86.72%] [G loss: 2.984124]\n",
      "epoch:32 step:25334 [D loss: 0.360642, acc.: 85.94%] [G loss: 2.864566]\n",
      "epoch:32 step:25335 [D loss: 0.321824, acc.: 86.72%] [G loss: 1.997167]\n",
      "epoch:32 step:25336 [D loss: 0.324944, acc.: 87.50%] [G loss: 2.392040]\n",
      "epoch:32 step:25337 [D loss: 0.295488, acc.: 87.50%] [G loss: 2.878044]\n",
      "epoch:32 step:25338 [D loss: 0.297783, acc.: 88.28%] [G loss: 2.478130]\n",
      "epoch:32 step:25339 [D loss: 0.283581, acc.: 85.94%] [G loss: 2.942521]\n",
      "epoch:32 step:25340 [D loss: 0.221540, acc.: 89.06%] [G loss: 3.491068]\n",
      "epoch:32 step:25341 [D loss: 0.306416, acc.: 84.38%] [G loss: 3.084383]\n",
      "epoch:32 step:25342 [D loss: 0.294798, acc.: 85.16%] [G loss: 3.045926]\n",
      "epoch:32 step:25343 [D loss: 0.483893, acc.: 78.91%] [G loss: 3.121779]\n",
      "epoch:32 step:25344 [D loss: 0.367945, acc.: 84.38%] [G loss: 2.205911]\n",
      "epoch:32 step:25345 [D loss: 0.314462, acc.: 84.38%] [G loss: 2.890604]\n",
      "epoch:32 step:25346 [D loss: 0.271689, acc.: 85.94%] [G loss: 3.651690]\n",
      "epoch:32 step:25347 [D loss: 0.285467, acc.: 86.72%] [G loss: 3.865402]\n",
      "epoch:32 step:25348 [D loss: 0.324820, acc.: 81.25%] [G loss: 3.708100]\n",
      "epoch:32 step:25349 [D loss: 0.325796, acc.: 83.59%] [G loss: 3.440656]\n",
      "epoch:32 step:25350 [D loss: 0.292737, acc.: 88.28%] [G loss: 3.370994]\n",
      "epoch:32 step:25351 [D loss: 0.361877, acc.: 85.16%] [G loss: 3.686239]\n",
      "epoch:32 step:25352 [D loss: 0.333574, acc.: 83.59%] [G loss: 3.103667]\n",
      "epoch:32 step:25353 [D loss: 0.242367, acc.: 89.06%] [G loss: 3.706536]\n",
      "epoch:32 step:25354 [D loss: 0.263472, acc.: 89.06%] [G loss: 2.663546]\n",
      "epoch:32 step:25355 [D loss: 0.264601, acc.: 89.06%] [G loss: 3.326103]\n",
      "epoch:32 step:25356 [D loss: 0.290643, acc.: 86.72%] [G loss: 3.204266]\n",
      "epoch:32 step:25357 [D loss: 0.293652, acc.: 83.59%] [G loss: 3.667789]\n",
      "epoch:32 step:25358 [D loss: 0.380915, acc.: 83.59%] [G loss: 2.864512]\n",
      "epoch:32 step:25359 [D loss: 0.392189, acc.: 82.81%] [G loss: 3.381164]\n",
      "epoch:32 step:25360 [D loss: 0.314074, acc.: 82.81%] [G loss: 3.509456]\n",
      "epoch:32 step:25361 [D loss: 0.290097, acc.: 89.84%] [G loss: 4.353580]\n",
      "epoch:32 step:25362 [D loss: 0.249727, acc.: 89.84%] [G loss: 3.631405]\n",
      "epoch:32 step:25363 [D loss: 0.260781, acc.: 86.72%] [G loss: 4.502970]\n",
      "epoch:32 step:25364 [D loss: 0.263161, acc.: 91.41%] [G loss: 3.563890]\n",
      "epoch:32 step:25365 [D loss: 0.295682, acc.: 89.06%] [G loss: 3.197690]\n",
      "epoch:32 step:25366 [D loss: 0.257705, acc.: 90.62%] [G loss: 3.258740]\n",
      "epoch:32 step:25367 [D loss: 0.306326, acc.: 86.72%] [G loss: 4.318735]\n",
      "epoch:32 step:25368 [D loss: 0.262281, acc.: 86.72%] [G loss: 4.084625]\n",
      "epoch:32 step:25369 [D loss: 0.356493, acc.: 88.28%] [G loss: 3.572204]\n",
      "epoch:32 step:25370 [D loss: 0.342284, acc.: 85.16%] [G loss: 3.105044]\n",
      "epoch:32 step:25371 [D loss: 0.278355, acc.: 89.06%] [G loss: 3.163992]\n",
      "epoch:32 step:25372 [D loss: 0.277104, acc.: 87.50%] [G loss: 2.786633]\n",
      "epoch:32 step:25373 [D loss: 0.323484, acc.: 83.59%] [G loss: 2.923693]\n",
      "epoch:32 step:25374 [D loss: 0.259359, acc.: 88.28%] [G loss: 4.386393]\n",
      "epoch:32 step:25375 [D loss: 0.254952, acc.: 92.97%] [G loss: 3.841882]\n",
      "epoch:32 step:25376 [D loss: 0.229164, acc.: 91.41%] [G loss: 2.863540]\n",
      "epoch:32 step:25377 [D loss: 0.291609, acc.: 86.72%] [G loss: 4.234763]\n",
      "epoch:32 step:25378 [D loss: 0.425759, acc.: 76.56%] [G loss: 3.664653]\n",
      "epoch:32 step:25379 [D loss: 0.361315, acc.: 81.25%] [G loss: 2.852709]\n",
      "epoch:32 step:25380 [D loss: 0.407981, acc.: 80.47%] [G loss: 2.992378]\n",
      "epoch:32 step:25381 [D loss: 0.307008, acc.: 86.72%] [G loss: 4.130419]\n",
      "epoch:32 step:25382 [D loss: 0.430148, acc.: 82.81%] [G loss: 3.419686]\n",
      "epoch:32 step:25383 [D loss: 0.460170, acc.: 73.44%] [G loss: 2.159389]\n",
      "epoch:32 step:25384 [D loss: 0.358817, acc.: 80.47%] [G loss: 2.716837]\n",
      "epoch:32 step:25385 [D loss: 0.369181, acc.: 85.16%] [G loss: 2.631331]\n",
      "epoch:32 step:25386 [D loss: 0.270514, acc.: 89.84%] [G loss: 2.775799]\n",
      "epoch:32 step:25387 [D loss: 0.325114, acc.: 87.50%] [G loss: 3.163687]\n",
      "epoch:32 step:25388 [D loss: 0.317988, acc.: 83.59%] [G loss: 2.998629]\n",
      "epoch:32 step:25389 [D loss: 0.350873, acc.: 85.16%] [G loss: 3.199821]\n",
      "epoch:32 step:25390 [D loss: 0.227525, acc.: 89.84%] [G loss: 4.399876]\n",
      "epoch:32 step:25391 [D loss: 0.375843, acc.: 84.38%] [G loss: 3.989551]\n",
      "epoch:32 step:25392 [D loss: 0.370888, acc.: 82.81%] [G loss: 3.161151]\n",
      "epoch:32 step:25393 [D loss: 0.270289, acc.: 87.50%] [G loss: 3.060612]\n",
      "epoch:32 step:25394 [D loss: 0.340391, acc.: 82.81%] [G loss: 3.094486]\n",
      "epoch:32 step:25395 [D loss: 0.209408, acc.: 94.53%] [G loss: 3.464520]\n",
      "epoch:32 step:25396 [D loss: 0.439539, acc.: 81.25%] [G loss: 2.536795]\n",
      "epoch:32 step:25397 [D loss: 0.318604, acc.: 85.16%] [G loss: 2.872390]\n",
      "epoch:32 step:25398 [D loss: 0.363213, acc.: 79.69%] [G loss: 2.769356]\n",
      "epoch:32 step:25399 [D loss: 0.438761, acc.: 77.34%] [G loss: 2.382621]\n",
      "epoch:32 step:25400 [D loss: 0.429805, acc.: 82.81%] [G loss: 3.753374]\n",
      "##############\n",
      "[0.86470433 0.85408588 0.84428945 0.79960783 0.77891454 0.83570093\n",
      " 0.86311541 0.83160058 0.81697061 0.79601929]\n",
      "##########\n",
      "epoch:32 step:25401 [D loss: 0.468043, acc.: 78.91%] [G loss: 4.587655]\n",
      "epoch:32 step:25402 [D loss: 0.591995, acc.: 72.66%] [G loss: 4.823167]\n",
      "epoch:32 step:25403 [D loss: 0.782461, acc.: 79.69%] [G loss: 5.990191]\n",
      "epoch:32 step:25404 [D loss: 1.896476, acc.: 52.34%] [G loss: 7.281872]\n",
      "epoch:32 step:25405 [D loss: 1.621737, acc.: 56.25%] [G loss: 5.123511]\n",
      "epoch:32 step:25406 [D loss: 0.503033, acc.: 80.47%] [G loss: 3.897386]\n",
      "epoch:32 step:25407 [D loss: 0.740950, acc.: 75.00%] [G loss: 3.699571]\n",
      "epoch:32 step:25408 [D loss: 0.290128, acc.: 89.06%] [G loss: 4.400094]\n",
      "epoch:32 step:25409 [D loss: 0.471476, acc.: 79.69%] [G loss: 3.260348]\n",
      "epoch:32 step:25410 [D loss: 0.353433, acc.: 86.72%] [G loss: 3.164752]\n",
      "epoch:32 step:25411 [D loss: 0.349639, acc.: 81.25%] [G loss: 4.773534]\n",
      "epoch:32 step:25412 [D loss: 0.327081, acc.: 85.94%] [G loss: 3.418579]\n",
      "epoch:32 step:25413 [D loss: 0.263647, acc.: 91.41%] [G loss: 3.334861]\n",
      "epoch:32 step:25414 [D loss: 0.365224, acc.: 83.59%] [G loss: 2.875325]\n",
      "epoch:32 step:25415 [D loss: 0.387853, acc.: 81.25%] [G loss: 3.554937]\n",
      "epoch:32 step:25416 [D loss: 0.344075, acc.: 88.28%] [G loss: 3.378621]\n",
      "epoch:32 step:25417 [D loss: 0.374076, acc.: 80.47%] [G loss: 3.598356]\n",
      "epoch:32 step:25418 [D loss: 0.302069, acc.: 89.06%] [G loss: 3.244604]\n",
      "epoch:32 step:25419 [D loss: 0.259740, acc.: 89.84%] [G loss: 2.793010]\n",
      "epoch:32 step:25420 [D loss: 0.334397, acc.: 85.16%] [G loss: 2.473551]\n",
      "epoch:32 step:25421 [D loss: 0.494794, acc.: 75.78%] [G loss: 2.087602]\n",
      "epoch:32 step:25422 [D loss: 0.389666, acc.: 81.25%] [G loss: 2.476485]\n",
      "epoch:32 step:25423 [D loss: 0.366306, acc.: 82.03%] [G loss: 2.420450]\n",
      "epoch:32 step:25424 [D loss: 0.438917, acc.: 84.38%] [G loss: 2.358843]\n",
      "epoch:32 step:25425 [D loss: 0.307048, acc.: 87.50%] [G loss: 2.932533]\n",
      "epoch:32 step:25426 [D loss: 0.331886, acc.: 82.81%] [G loss: 2.712783]\n",
      "epoch:32 step:25427 [D loss: 0.370828, acc.: 84.38%] [G loss: 3.044107]\n",
      "epoch:32 step:25428 [D loss: 0.389367, acc.: 83.59%] [G loss: 4.140140]\n",
      "epoch:32 step:25429 [D loss: 0.386808, acc.: 84.38%] [G loss: 5.203564]\n",
      "epoch:32 step:25430 [D loss: 0.347783, acc.: 84.38%] [G loss: 3.444903]\n",
      "epoch:32 step:25431 [D loss: 0.224403, acc.: 92.19%] [G loss: 4.471704]\n",
      "epoch:32 step:25432 [D loss: 0.324830, acc.: 80.47%] [G loss: 2.565733]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:32 step:25433 [D loss: 0.263212, acc.: 91.41%] [G loss: 3.369845]\n",
      "epoch:32 step:25434 [D loss: 0.252364, acc.: 90.62%] [G loss: 2.664089]\n",
      "epoch:32 step:25435 [D loss: 0.299139, acc.: 85.94%] [G loss: 3.018323]\n",
      "epoch:32 step:25436 [D loss: 0.298953, acc.: 85.94%] [G loss: 2.556039]\n",
      "epoch:32 step:25437 [D loss: 0.332960, acc.: 87.50%] [G loss: 2.161057]\n",
      "epoch:32 step:25438 [D loss: 0.358496, acc.: 79.69%] [G loss: 2.311670]\n",
      "epoch:32 step:25439 [D loss: 0.413965, acc.: 78.91%] [G loss: 2.595203]\n",
      "epoch:32 step:25440 [D loss: 0.394801, acc.: 79.69%] [G loss: 2.811167]\n",
      "epoch:32 step:25441 [D loss: 0.339789, acc.: 85.16%] [G loss: 3.141300]\n",
      "epoch:32 step:25442 [D loss: 0.282392, acc.: 88.28%] [G loss: 2.962680]\n",
      "epoch:32 step:25443 [D loss: 0.254896, acc.: 89.06%] [G loss: 3.381506]\n",
      "epoch:32 step:25444 [D loss: 0.340102, acc.: 85.94%] [G loss: 2.969860]\n",
      "epoch:32 step:25445 [D loss: 0.265899, acc.: 89.06%] [G loss: 3.556284]\n",
      "epoch:32 step:25446 [D loss: 0.354853, acc.: 84.38%] [G loss: 3.290818]\n",
      "epoch:32 step:25447 [D loss: 0.363219, acc.: 85.94%] [G loss: 2.888619]\n",
      "epoch:32 step:25448 [D loss: 0.339403, acc.: 84.38%] [G loss: 2.149264]\n",
      "epoch:32 step:25449 [D loss: 0.269428, acc.: 89.06%] [G loss: 2.570459]\n",
      "epoch:32 step:25450 [D loss: 0.284207, acc.: 88.28%] [G loss: 3.082425]\n",
      "epoch:32 step:25451 [D loss: 0.288989, acc.: 85.16%] [G loss: 3.318117]\n",
      "epoch:32 step:25452 [D loss: 0.266745, acc.: 87.50%] [G loss: 3.903793]\n",
      "epoch:32 step:25453 [D loss: 0.304771, acc.: 88.28%] [G loss: 2.230412]\n",
      "epoch:32 step:25454 [D loss: 0.322596, acc.: 82.81%] [G loss: 2.629166]\n",
      "epoch:32 step:25455 [D loss: 0.422318, acc.: 75.78%] [G loss: 2.701550]\n",
      "epoch:32 step:25456 [D loss: 0.357759, acc.: 87.50%] [G loss: 2.760174]\n",
      "epoch:32 step:25457 [D loss: 0.389265, acc.: 81.25%] [G loss: 2.639108]\n",
      "epoch:32 step:25458 [D loss: 0.359231, acc.: 84.38%] [G loss: 3.707117]\n",
      "epoch:32 step:25459 [D loss: 0.401138, acc.: 83.59%] [G loss: 2.979435]\n",
      "epoch:32 step:25460 [D loss: 0.357425, acc.: 82.81%] [G loss: 3.255048]\n",
      "epoch:32 step:25461 [D loss: 0.269233, acc.: 91.41%] [G loss: 3.568588]\n",
      "epoch:32 step:25462 [D loss: 0.302975, acc.: 88.28%] [G loss: 2.635973]\n",
      "epoch:32 step:25463 [D loss: 0.363315, acc.: 80.47%] [G loss: 2.699798]\n",
      "epoch:32 step:25464 [D loss: 0.349290, acc.: 84.38%] [G loss: 3.859235]\n",
      "epoch:32 step:25465 [D loss: 0.391330, acc.: 82.81%] [G loss: 2.679792]\n",
      "epoch:32 step:25466 [D loss: 0.328803, acc.: 85.16%] [G loss: 3.403082]\n",
      "epoch:32 step:25467 [D loss: 0.303946, acc.: 85.16%] [G loss: 2.861670]\n",
      "epoch:32 step:25468 [D loss: 0.368778, acc.: 80.47%] [G loss: 2.552483]\n",
      "epoch:32 step:25469 [D loss: 0.380141, acc.: 81.25%] [G loss: 2.833229]\n",
      "epoch:32 step:25470 [D loss: 0.277577, acc.: 88.28%] [G loss: 3.349228]\n",
      "epoch:32 step:25471 [D loss: 0.305209, acc.: 86.72%] [G loss: 3.201223]\n",
      "epoch:32 step:25472 [D loss: 0.266317, acc.: 89.06%] [G loss: 2.845054]\n",
      "epoch:32 step:25473 [D loss: 0.352367, acc.: 83.59%] [G loss: 2.783838]\n",
      "epoch:32 step:25474 [D loss: 0.326725, acc.: 85.94%] [G loss: 2.674492]\n",
      "epoch:32 step:25475 [D loss: 0.258890, acc.: 89.06%] [G loss: 3.293733]\n",
      "epoch:32 step:25476 [D loss: 0.322782, acc.: 86.72%] [G loss: 2.992889]\n",
      "epoch:32 step:25477 [D loss: 0.343124, acc.: 83.59%] [G loss: 2.595240]\n",
      "epoch:32 step:25478 [D loss: 0.263726, acc.: 89.84%] [G loss: 3.582870]\n",
      "epoch:32 step:25479 [D loss: 0.315121, acc.: 84.38%] [G loss: 3.499747]\n",
      "epoch:32 step:25480 [D loss: 0.330167, acc.: 85.94%] [G loss: 2.243695]\n",
      "epoch:32 step:25481 [D loss: 0.283450, acc.: 88.28%] [G loss: 3.811703]\n",
      "epoch:32 step:25482 [D loss: 0.294625, acc.: 86.72%] [G loss: 3.288079]\n",
      "epoch:32 step:25483 [D loss: 0.332332, acc.: 85.16%] [G loss: 5.706015]\n",
      "epoch:32 step:25484 [D loss: 0.283389, acc.: 87.50%] [G loss: 3.734928]\n",
      "epoch:32 step:25485 [D loss: 0.274551, acc.: 91.41%] [G loss: 4.014788]\n",
      "epoch:32 step:25486 [D loss: 0.269608, acc.: 89.06%] [G loss: 2.804642]\n",
      "epoch:32 step:25487 [D loss: 0.323815, acc.: 83.59%] [G loss: 3.317608]\n",
      "epoch:32 step:25488 [D loss: 0.223561, acc.: 92.19%] [G loss: 3.730899]\n",
      "epoch:32 step:25489 [D loss: 0.282782, acc.: 85.16%] [G loss: 4.586328]\n",
      "epoch:32 step:25490 [D loss: 0.305774, acc.: 86.72%] [G loss: 4.381294]\n",
      "epoch:32 step:25491 [D loss: 0.237878, acc.: 91.41%] [G loss: 4.517077]\n",
      "epoch:32 step:25492 [D loss: 0.334879, acc.: 82.03%] [G loss: 4.702479]\n",
      "epoch:32 step:25493 [D loss: 0.312726, acc.: 84.38%] [G loss: 4.055933]\n",
      "epoch:32 step:25494 [D loss: 0.292540, acc.: 85.94%] [G loss: 4.005791]\n",
      "epoch:32 step:25495 [D loss: 0.284350, acc.: 85.94%] [G loss: 2.643832]\n",
      "epoch:32 step:25496 [D loss: 0.259529, acc.: 89.84%] [G loss: 4.660925]\n",
      "epoch:32 step:25497 [D loss: 0.204521, acc.: 90.62%] [G loss: 4.931672]\n",
      "epoch:32 step:25498 [D loss: 0.321224, acc.: 84.38%] [G loss: 4.587001]\n",
      "epoch:32 step:25499 [D loss: 0.295683, acc.: 85.94%] [G loss: 4.154284]\n",
      "epoch:32 step:25500 [D loss: 0.364736, acc.: 85.16%] [G loss: 2.847821]\n",
      "epoch:32 step:25501 [D loss: 0.246092, acc.: 90.62%] [G loss: 4.040161]\n",
      "epoch:32 step:25502 [D loss: 0.365533, acc.: 81.25%] [G loss: 3.126870]\n",
      "epoch:32 step:25503 [D loss: 0.269975, acc.: 88.28%] [G loss: 3.994596]\n",
      "epoch:32 step:25504 [D loss: 0.259129, acc.: 86.72%] [G loss: 2.630018]\n",
      "epoch:32 step:25505 [D loss: 0.465312, acc.: 78.12%] [G loss: 3.851022]\n",
      "epoch:32 step:25506 [D loss: 0.423075, acc.: 82.03%] [G loss: 3.344418]\n",
      "epoch:32 step:25507 [D loss: 0.415953, acc.: 80.47%] [G loss: 2.911105]\n",
      "epoch:32 step:25508 [D loss: 0.327553, acc.: 84.38%] [G loss: 2.771338]\n",
      "epoch:32 step:25509 [D loss: 0.373075, acc.: 79.69%] [G loss: 3.070615]\n",
      "epoch:32 step:25510 [D loss: 0.339467, acc.: 86.72%] [G loss: 2.400876]\n",
      "epoch:32 step:25511 [D loss: 0.329502, acc.: 84.38%] [G loss: 2.631094]\n",
      "epoch:32 step:25512 [D loss: 0.419087, acc.: 81.25%] [G loss: 2.368003]\n",
      "epoch:32 step:25513 [D loss: 0.397660, acc.: 78.12%] [G loss: 2.634202]\n",
      "epoch:32 step:25514 [D loss: 0.367235, acc.: 82.03%] [G loss: 2.425648]\n",
      "epoch:32 step:25515 [D loss: 0.435824, acc.: 82.03%] [G loss: 3.389662]\n",
      "epoch:32 step:25516 [D loss: 0.432405, acc.: 80.47%] [G loss: 3.289961]\n",
      "epoch:32 step:25517 [D loss: 0.463176, acc.: 76.56%] [G loss: 4.682268]\n",
      "epoch:32 step:25518 [D loss: 0.473411, acc.: 79.69%] [G loss: 2.974797]\n",
      "epoch:32 step:25519 [D loss: 0.305267, acc.: 83.59%] [G loss: 2.795067]\n",
      "epoch:32 step:25520 [D loss: 0.217333, acc.: 92.97%] [G loss: 3.473544]\n",
      "epoch:32 step:25521 [D loss: 0.292990, acc.: 88.28%] [G loss: 2.773036]\n",
      "epoch:32 step:25522 [D loss: 0.448063, acc.: 75.78%] [G loss: 3.205842]\n",
      "epoch:32 step:25523 [D loss: 0.363225, acc.: 83.59%] [G loss: 4.148880]\n",
      "epoch:32 step:25524 [D loss: 0.366297, acc.: 82.81%] [G loss: 3.900068]\n",
      "epoch:32 step:25525 [D loss: 0.386934, acc.: 85.94%] [G loss: 3.821327]\n",
      "epoch:32 step:25526 [D loss: 0.371493, acc.: 84.38%] [G loss: 2.962664]\n",
      "epoch:32 step:25527 [D loss: 0.338367, acc.: 82.81%] [G loss: 5.982124]\n",
      "epoch:32 step:25528 [D loss: 0.417312, acc.: 83.59%] [G loss: 3.423783]\n",
      "epoch:32 step:25529 [D loss: 0.367867, acc.: 82.03%] [G loss: 3.682680]\n",
      "epoch:32 step:25530 [D loss: 0.387064, acc.: 82.81%] [G loss: 2.577508]\n",
      "epoch:32 step:25531 [D loss: 0.261946, acc.: 90.62%] [G loss: 3.461516]\n",
      "epoch:32 step:25532 [D loss: 0.333903, acc.: 84.38%] [G loss: 3.006507]\n",
      "epoch:32 step:25533 [D loss: 0.363547, acc.: 87.50%] [G loss: 3.193586]\n",
      "epoch:32 step:25534 [D loss: 0.436052, acc.: 82.81%] [G loss: 3.304379]\n",
      "epoch:32 step:25535 [D loss: 0.394113, acc.: 82.03%] [G loss: 2.417860]\n",
      "epoch:32 step:25536 [D loss: 0.383511, acc.: 82.81%] [G loss: 6.055980]\n",
      "epoch:32 step:25537 [D loss: 0.194430, acc.: 90.62%] [G loss: 4.904092]\n",
      "epoch:32 step:25538 [D loss: 0.239709, acc.: 89.84%] [G loss: 5.078040]\n",
      "epoch:32 step:25539 [D loss: 0.301096, acc.: 85.16%] [G loss: 3.955399]\n",
      "epoch:32 step:25540 [D loss: 0.216242, acc.: 91.41%] [G loss: 4.153303]\n",
      "epoch:32 step:25541 [D loss: 0.335536, acc.: 83.59%] [G loss: 2.874827]\n",
      "epoch:32 step:25542 [D loss: 0.374519, acc.: 82.03%] [G loss: 3.412633]\n",
      "epoch:32 step:25543 [D loss: 0.263668, acc.: 87.50%] [G loss: 3.864643]\n",
      "epoch:32 step:25544 [D loss: 0.327596, acc.: 87.50%] [G loss: 4.799952]\n",
      "epoch:32 step:25545 [D loss: 0.364741, acc.: 84.38%] [G loss: 3.760394]\n",
      "epoch:32 step:25546 [D loss: 0.321693, acc.: 82.81%] [G loss: 2.953477]\n",
      "epoch:32 step:25547 [D loss: 0.314673, acc.: 86.72%] [G loss: 3.541681]\n",
      "epoch:32 step:25548 [D loss: 0.288006, acc.: 86.72%] [G loss: 3.349803]\n",
      "epoch:32 step:25549 [D loss: 0.285126, acc.: 87.50%] [G loss: 3.143279]\n",
      "epoch:32 step:25550 [D loss: 0.403674, acc.: 80.47%] [G loss: 3.653673]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:32 step:25551 [D loss: 0.368407, acc.: 81.25%] [G loss: 3.621546]\n",
      "epoch:32 step:25552 [D loss: 0.388165, acc.: 85.16%] [G loss: 2.580544]\n",
      "epoch:32 step:25553 [D loss: 0.272670, acc.: 89.06%] [G loss: 4.013132]\n",
      "epoch:32 step:25554 [D loss: 0.373558, acc.: 82.03%] [G loss: 2.598663]\n",
      "epoch:32 step:25555 [D loss: 0.286564, acc.: 87.50%] [G loss: 3.282197]\n",
      "epoch:32 step:25556 [D loss: 0.350203, acc.: 82.03%] [G loss: 2.800225]\n",
      "epoch:32 step:25557 [D loss: 0.303397, acc.: 85.94%] [G loss: 3.274303]\n",
      "epoch:32 step:25558 [D loss: 0.339320, acc.: 86.72%] [G loss: 3.497936]\n",
      "epoch:32 step:25559 [D loss: 0.370191, acc.: 81.25%] [G loss: 3.624201]\n",
      "epoch:32 step:25560 [D loss: 0.312157, acc.: 87.50%] [G loss: 2.932534]\n",
      "epoch:32 step:25561 [D loss: 0.289244, acc.: 89.84%] [G loss: 3.088667]\n",
      "epoch:32 step:25562 [D loss: 0.333303, acc.: 85.16%] [G loss: 2.813926]\n",
      "epoch:32 step:25563 [D loss: 0.383709, acc.: 81.25%] [G loss: 2.771143]\n",
      "epoch:32 step:25564 [D loss: 0.391022, acc.: 81.25%] [G loss: 2.384901]\n",
      "epoch:32 step:25565 [D loss: 0.296015, acc.: 87.50%] [G loss: 2.768818]\n",
      "epoch:32 step:25566 [D loss: 0.284923, acc.: 87.50%] [G loss: 2.618843]\n",
      "epoch:32 step:25567 [D loss: 0.383506, acc.: 83.59%] [G loss: 2.167570]\n",
      "epoch:32 step:25568 [D loss: 0.320857, acc.: 85.16%] [G loss: 3.193246]\n",
      "epoch:32 step:25569 [D loss: 0.293952, acc.: 86.72%] [G loss: 2.579162]\n",
      "epoch:32 step:25570 [D loss: 0.352485, acc.: 82.03%] [G loss: 4.675858]\n",
      "epoch:32 step:25571 [D loss: 0.364366, acc.: 83.59%] [G loss: 4.206090]\n",
      "epoch:32 step:25572 [D loss: 0.323596, acc.: 82.03%] [G loss: 3.990601]\n",
      "epoch:32 step:25573 [D loss: 0.308380, acc.: 82.03%] [G loss: 3.699490]\n",
      "epoch:32 step:25574 [D loss: 0.331149, acc.: 85.16%] [G loss: 4.653057]\n",
      "epoch:32 step:25575 [D loss: 0.443623, acc.: 78.12%] [G loss: 3.004746]\n",
      "epoch:32 step:25576 [D loss: 0.340096, acc.: 85.16%] [G loss: 3.410368]\n",
      "epoch:32 step:25577 [D loss: 0.278609, acc.: 86.72%] [G loss: 4.144400]\n",
      "epoch:32 step:25578 [D loss: 0.321830, acc.: 89.06%] [G loss: 3.052550]\n",
      "epoch:32 step:25579 [D loss: 0.256183, acc.: 88.28%] [G loss: 2.480902]\n",
      "epoch:32 step:25580 [D loss: 0.289322, acc.: 88.28%] [G loss: 3.680740]\n",
      "epoch:32 step:25581 [D loss: 0.314044, acc.: 85.16%] [G loss: 2.863941]\n",
      "epoch:32 step:25582 [D loss: 0.334876, acc.: 84.38%] [G loss: 4.401779]\n",
      "epoch:32 step:25583 [D loss: 0.485298, acc.: 77.34%] [G loss: 3.432945]\n",
      "epoch:32 step:25584 [D loss: 0.348101, acc.: 86.72%] [G loss: 3.715895]\n",
      "epoch:32 step:25585 [D loss: 0.331456, acc.: 84.38%] [G loss: 3.159628]\n",
      "epoch:32 step:25586 [D loss: 0.282931, acc.: 85.94%] [G loss: 5.334288]\n",
      "epoch:32 step:25587 [D loss: 0.427753, acc.: 76.56%] [G loss: 2.173002]\n",
      "epoch:32 step:25588 [D loss: 0.325039, acc.: 88.28%] [G loss: 3.328138]\n",
      "epoch:32 step:25589 [D loss: 0.331331, acc.: 82.81%] [G loss: 3.234237]\n",
      "epoch:32 step:25590 [D loss: 0.318180, acc.: 85.94%] [G loss: 2.944131]\n",
      "epoch:32 step:25591 [D loss: 0.258664, acc.: 88.28%] [G loss: 2.830651]\n",
      "epoch:32 step:25592 [D loss: 0.262304, acc.: 88.28%] [G loss: 3.382695]\n",
      "epoch:32 step:25593 [D loss: 0.311776, acc.: 87.50%] [G loss: 2.773197]\n",
      "epoch:32 step:25594 [D loss: 0.277044, acc.: 87.50%] [G loss: 3.574466]\n",
      "epoch:32 step:25595 [D loss: 0.214126, acc.: 91.41%] [G loss: 2.786440]\n",
      "epoch:32 step:25596 [D loss: 0.270358, acc.: 88.28%] [G loss: 2.724266]\n",
      "epoch:32 step:25597 [D loss: 0.328493, acc.: 85.16%] [G loss: 3.510906]\n",
      "epoch:32 step:25598 [D loss: 0.399371, acc.: 80.47%] [G loss: 3.403870]\n",
      "epoch:32 step:25599 [D loss: 0.331784, acc.: 82.03%] [G loss: 3.718147]\n",
      "epoch:32 step:25600 [D loss: 0.314045, acc.: 86.72%] [G loss: 3.482333]\n",
      "##############\n",
      "[0.85301646 0.86093325 0.81064509 0.81215375 0.76712232 0.83121576\n",
      " 0.87838728 0.83087272 0.8536248  0.81323755]\n",
      "##########\n",
      "epoch:32 step:25601 [D loss: 0.379898, acc.: 83.59%] [G loss: 3.035160]\n",
      "epoch:32 step:25602 [D loss: 0.412395, acc.: 79.69%] [G loss: 5.531154]\n",
      "epoch:32 step:25603 [D loss: 0.389504, acc.: 83.59%] [G loss: 3.491333]\n",
      "epoch:32 step:25604 [D loss: 0.269844, acc.: 90.62%] [G loss: 4.254183]\n",
      "epoch:32 step:25605 [D loss: 0.353990, acc.: 81.25%] [G loss: 2.871531]\n",
      "epoch:32 step:25606 [D loss: 0.317228, acc.: 88.28%] [G loss: 3.258893]\n",
      "epoch:32 step:25607 [D loss: 0.252005, acc.: 88.28%] [G loss: 3.046574]\n",
      "epoch:32 step:25608 [D loss: 0.378933, acc.: 82.81%] [G loss: 3.805130]\n",
      "epoch:32 step:25609 [D loss: 0.307910, acc.: 86.72%] [G loss: 3.445788]\n",
      "epoch:32 step:25610 [D loss: 0.406045, acc.: 80.47%] [G loss: 3.262136]\n",
      "epoch:32 step:25611 [D loss: 0.268071, acc.: 86.72%] [G loss: 3.588450]\n",
      "epoch:32 step:25612 [D loss: 0.243187, acc.: 88.28%] [G loss: 2.930500]\n",
      "epoch:32 step:25613 [D loss: 0.236965, acc.: 89.84%] [G loss: 4.543235]\n",
      "epoch:32 step:25614 [D loss: 0.291592, acc.: 85.94%] [G loss: 2.787117]\n",
      "epoch:32 step:25615 [D loss: 0.296789, acc.: 82.81%] [G loss: 3.595918]\n",
      "epoch:32 step:25616 [D loss: 0.356429, acc.: 81.25%] [G loss: 3.260489]\n",
      "epoch:32 step:25617 [D loss: 0.355268, acc.: 82.81%] [G loss: 3.396765]\n",
      "epoch:32 step:25618 [D loss: 0.308002, acc.: 82.03%] [G loss: 4.037008]\n",
      "epoch:32 step:25619 [D loss: 0.323594, acc.: 85.16%] [G loss: 8.939396]\n",
      "epoch:32 step:25620 [D loss: 0.224936, acc.: 89.84%] [G loss: 7.661510]\n",
      "epoch:32 step:25621 [D loss: 0.187414, acc.: 92.97%] [G loss: 8.629610]\n",
      "epoch:32 step:25622 [D loss: 0.240453, acc.: 90.62%] [G loss: 7.621485]\n",
      "epoch:32 step:25623 [D loss: 0.213728, acc.: 93.75%] [G loss: 6.002090]\n",
      "epoch:32 step:25624 [D loss: 0.229747, acc.: 89.84%] [G loss: 5.457105]\n",
      "epoch:32 step:25625 [D loss: 0.290410, acc.: 88.28%] [G loss: 6.610863]\n",
      "epoch:32 step:25626 [D loss: 0.224921, acc.: 92.97%] [G loss: 6.323030]\n",
      "epoch:32 step:25627 [D loss: 0.269632, acc.: 88.28%] [G loss: 6.646345]\n",
      "epoch:32 step:25628 [D loss: 0.234194, acc.: 87.50%] [G loss: 4.725387]\n",
      "epoch:32 step:25629 [D loss: 0.179109, acc.: 92.19%] [G loss: 3.606151]\n",
      "epoch:32 step:25630 [D loss: 0.237044, acc.: 92.19%] [G loss: 3.551070]\n",
      "epoch:32 step:25631 [D loss: 0.291821, acc.: 86.72%] [G loss: 4.899547]\n",
      "epoch:32 step:25632 [D loss: 0.164306, acc.: 92.97%] [G loss: 6.567404]\n",
      "epoch:32 step:25633 [D loss: 0.150026, acc.: 94.53%] [G loss: 6.604754]\n",
      "epoch:32 step:25634 [D loss: 0.275048, acc.: 85.94%] [G loss: 5.631298]\n",
      "epoch:32 step:25635 [D loss: 0.355792, acc.: 82.81%] [G loss: 5.134249]\n",
      "epoch:32 step:25636 [D loss: 0.278863, acc.: 84.38%] [G loss: 3.884703]\n",
      "epoch:32 step:25637 [D loss: 0.314630, acc.: 85.94%] [G loss: 3.704230]\n",
      "epoch:32 step:25638 [D loss: 0.304369, acc.: 85.16%] [G loss: 3.361551]\n",
      "epoch:32 step:25639 [D loss: 0.261452, acc.: 89.06%] [G loss: 2.829767]\n",
      "epoch:32 step:25640 [D loss: 0.270950, acc.: 85.16%] [G loss: 3.396510]\n",
      "epoch:32 step:25641 [D loss: 0.346423, acc.: 85.16%] [G loss: 3.411365]\n",
      "epoch:32 step:25642 [D loss: 0.314095, acc.: 85.16%] [G loss: 3.402566]\n",
      "epoch:32 step:25643 [D loss: 0.324905, acc.: 83.59%] [G loss: 5.685040]\n",
      "epoch:32 step:25644 [D loss: 0.528215, acc.: 75.00%] [G loss: 5.593589]\n",
      "epoch:32 step:25645 [D loss: 0.408893, acc.: 85.16%] [G loss: 5.522509]\n",
      "epoch:32 step:25646 [D loss: 0.393630, acc.: 79.69%] [G loss: 3.334019]\n",
      "epoch:32 step:25647 [D loss: 0.258663, acc.: 89.84%] [G loss: 3.394395]\n",
      "epoch:32 step:25648 [D loss: 0.325000, acc.: 84.38%] [G loss: 3.511059]\n",
      "epoch:32 step:25649 [D loss: 0.298107, acc.: 87.50%] [G loss: 3.191742]\n",
      "epoch:32 step:25650 [D loss: 0.312046, acc.: 84.38%] [G loss: 3.852652]\n",
      "epoch:32 step:25651 [D loss: 0.412765, acc.: 80.47%] [G loss: 3.186437]\n",
      "epoch:32 step:25652 [D loss: 0.354576, acc.: 83.59%] [G loss: 2.872799]\n",
      "epoch:32 step:25653 [D loss: 0.230964, acc.: 89.84%] [G loss: 3.196414]\n",
      "epoch:32 step:25654 [D loss: 0.340649, acc.: 86.72%] [G loss: 3.522334]\n",
      "epoch:32 step:25655 [D loss: 0.282645, acc.: 90.62%] [G loss: 3.390679]\n",
      "epoch:32 step:25656 [D loss: 0.420341, acc.: 77.34%] [G loss: 4.195531]\n",
      "epoch:32 step:25657 [D loss: 0.331898, acc.: 85.16%] [G loss: 3.493078]\n",
      "epoch:32 step:25658 [D loss: 0.270606, acc.: 87.50%] [G loss: 3.177382]\n",
      "epoch:32 step:25659 [D loss: 0.249204, acc.: 88.28%] [G loss: 3.056036]\n",
      "epoch:32 step:25660 [D loss: 0.270297, acc.: 89.84%] [G loss: 2.801278]\n",
      "epoch:32 step:25661 [D loss: 0.357116, acc.: 85.94%] [G loss: 3.770577]\n",
      "epoch:32 step:25662 [D loss: 0.430898, acc.: 82.03%] [G loss: 4.054006]\n",
      "epoch:32 step:25663 [D loss: 0.328116, acc.: 83.59%] [G loss: 2.462944]\n",
      "epoch:32 step:25664 [D loss: 0.251472, acc.: 88.28%] [G loss: 3.658178]\n",
      "epoch:32 step:25665 [D loss: 0.455405, acc.: 76.56%] [G loss: 4.363643]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:32 step:25666 [D loss: 0.488438, acc.: 78.91%] [G loss: 5.010214]\n",
      "epoch:32 step:25667 [D loss: 0.338702, acc.: 82.81%] [G loss: 4.008435]\n",
      "epoch:32 step:25668 [D loss: 0.292487, acc.: 84.38%] [G loss: 3.990844]\n",
      "epoch:32 step:25669 [D loss: 0.251707, acc.: 87.50%] [G loss: 4.041134]\n",
      "epoch:32 step:25670 [D loss: 0.254373, acc.: 89.06%] [G loss: 3.975716]\n",
      "epoch:32 step:25671 [D loss: 0.188370, acc.: 90.62%] [G loss: 3.326007]\n",
      "epoch:32 step:25672 [D loss: 0.351342, acc.: 85.16%] [G loss: 3.764989]\n",
      "epoch:32 step:25673 [D loss: 0.237845, acc.: 87.50%] [G loss: 4.055483]\n",
      "epoch:32 step:25674 [D loss: 0.216840, acc.: 90.62%] [G loss: 3.890267]\n",
      "epoch:32 step:25675 [D loss: 0.235983, acc.: 91.41%] [G loss: 5.866120]\n",
      "epoch:32 step:25676 [D loss: 0.336568, acc.: 86.72%] [G loss: 4.674015]\n",
      "epoch:32 step:25677 [D loss: 0.297850, acc.: 90.62%] [G loss: 3.669936]\n",
      "epoch:32 step:25678 [D loss: 0.235582, acc.: 86.72%] [G loss: 3.797054]\n",
      "epoch:32 step:25679 [D loss: 0.357659, acc.: 85.16%] [G loss: 4.259831]\n",
      "epoch:32 step:25680 [D loss: 0.403095, acc.: 79.69%] [G loss: 3.853135]\n",
      "epoch:32 step:25681 [D loss: 0.415246, acc.: 78.91%] [G loss: 3.941528]\n",
      "epoch:32 step:25682 [D loss: 0.277539, acc.: 89.06%] [G loss: 2.973964]\n",
      "epoch:32 step:25683 [D loss: 0.309377, acc.: 82.81%] [G loss: 2.894309]\n",
      "epoch:32 step:25684 [D loss: 0.393574, acc.: 81.25%] [G loss: 2.779880]\n",
      "epoch:32 step:25685 [D loss: 0.414848, acc.: 82.03%] [G loss: 3.327773]\n",
      "epoch:32 step:25686 [D loss: 0.207124, acc.: 91.41%] [G loss: 2.965326]\n",
      "epoch:32 step:25687 [D loss: 0.275676, acc.: 86.72%] [G loss: 2.906019]\n",
      "epoch:32 step:25688 [D loss: 0.346268, acc.: 85.16%] [G loss: 3.891271]\n",
      "epoch:32 step:25689 [D loss: 0.345553, acc.: 85.16%] [G loss: 3.086032]\n",
      "epoch:32 step:25690 [D loss: 0.265911, acc.: 89.84%] [G loss: 3.013158]\n",
      "epoch:32 step:25691 [D loss: 0.302527, acc.: 85.16%] [G loss: 3.046805]\n",
      "epoch:32 step:25692 [D loss: 0.260803, acc.: 89.84%] [G loss: 3.245516]\n",
      "epoch:32 step:25693 [D loss: 0.330657, acc.: 85.16%] [G loss: 3.155435]\n",
      "epoch:32 step:25694 [D loss: 0.219705, acc.: 92.97%] [G loss: 3.349574]\n",
      "epoch:32 step:25695 [D loss: 0.344461, acc.: 83.59%] [G loss: 2.861418]\n",
      "epoch:32 step:25696 [D loss: 0.275945, acc.: 88.28%] [G loss: 2.633802]\n",
      "epoch:32 step:25697 [D loss: 0.304475, acc.: 85.94%] [G loss: 2.972071]\n",
      "epoch:32 step:25698 [D loss: 0.400818, acc.: 77.34%] [G loss: 5.062163]\n",
      "epoch:32 step:25699 [D loss: 0.438242, acc.: 78.12%] [G loss: 5.654161]\n",
      "epoch:32 step:25700 [D loss: 0.310629, acc.: 86.72%] [G loss: 3.494981]\n",
      "epoch:32 step:25701 [D loss: 0.177665, acc.: 94.53%] [G loss: 4.011023]\n",
      "epoch:32 step:25702 [D loss: 0.278586, acc.: 90.62%] [G loss: 4.538704]\n",
      "epoch:32 step:25703 [D loss: 0.374438, acc.: 80.47%] [G loss: 3.229618]\n",
      "epoch:32 step:25704 [D loss: 0.309486, acc.: 87.50%] [G loss: 3.298191]\n",
      "epoch:32 step:25705 [D loss: 0.290978, acc.: 85.94%] [G loss: 3.266668]\n",
      "epoch:32 step:25706 [D loss: 0.297766, acc.: 86.72%] [G loss: 3.360861]\n",
      "epoch:32 step:25707 [D loss: 0.322562, acc.: 87.50%] [G loss: 3.615942]\n",
      "epoch:32 step:25708 [D loss: 0.320515, acc.: 87.50%] [G loss: 3.565514]\n",
      "epoch:32 step:25709 [D loss: 0.320808, acc.: 85.94%] [G loss: 3.439971]\n",
      "epoch:32 step:25710 [D loss: 0.266950, acc.: 86.72%] [G loss: 3.366098]\n",
      "epoch:32 step:25711 [D loss: 0.238900, acc.: 89.84%] [G loss: 3.733165]\n",
      "epoch:32 step:25712 [D loss: 0.285880, acc.: 89.84%] [G loss: 3.070454]\n",
      "epoch:32 step:25713 [D loss: 0.409024, acc.: 78.91%] [G loss: 3.014728]\n",
      "epoch:32 step:25714 [D loss: 0.313576, acc.: 87.50%] [G loss: 3.037651]\n",
      "epoch:32 step:25715 [D loss: 0.273264, acc.: 89.84%] [G loss: 3.402695]\n",
      "epoch:32 step:25716 [D loss: 0.259473, acc.: 85.94%] [G loss: 2.427002]\n",
      "epoch:32 step:25717 [D loss: 0.274636, acc.: 86.72%] [G loss: 3.657227]\n",
      "epoch:32 step:25718 [D loss: 0.358682, acc.: 82.81%] [G loss: 3.086236]\n",
      "epoch:32 step:25719 [D loss: 0.319873, acc.: 89.06%] [G loss: 2.418470]\n",
      "epoch:32 step:25720 [D loss: 0.410979, acc.: 82.03%] [G loss: 3.610301]\n",
      "epoch:32 step:25721 [D loss: 0.301166, acc.: 85.16%] [G loss: 4.230718]\n",
      "epoch:32 step:25722 [D loss: 0.226084, acc.: 90.62%] [G loss: 4.164987]\n",
      "epoch:32 step:25723 [D loss: 0.297798, acc.: 88.28%] [G loss: 3.869197]\n",
      "epoch:32 step:25724 [D loss: 0.326958, acc.: 85.16%] [G loss: 3.043096]\n",
      "epoch:32 step:25725 [D loss: 0.405310, acc.: 81.25%] [G loss: 4.096355]\n",
      "epoch:32 step:25726 [D loss: 0.327829, acc.: 85.16%] [G loss: 3.784083]\n",
      "epoch:32 step:25727 [D loss: 0.278988, acc.: 87.50%] [G loss: 2.808826]\n",
      "epoch:32 step:25728 [D loss: 0.273083, acc.: 87.50%] [G loss: 4.791040]\n",
      "epoch:32 step:25729 [D loss: 0.364391, acc.: 82.03%] [G loss: 4.546495]\n",
      "epoch:32 step:25730 [D loss: 0.284969, acc.: 85.16%] [G loss: 4.409685]\n",
      "epoch:32 step:25731 [D loss: 0.336056, acc.: 83.59%] [G loss: 3.075112]\n",
      "epoch:32 step:25732 [D loss: 0.318451, acc.: 85.94%] [G loss: 3.054679]\n",
      "epoch:32 step:25733 [D loss: 0.311611, acc.: 85.94%] [G loss: 3.016489]\n",
      "epoch:32 step:25734 [D loss: 0.296876, acc.: 87.50%] [G loss: 2.919366]\n",
      "epoch:32 step:25735 [D loss: 0.218374, acc.: 92.19%] [G loss: 3.176658]\n",
      "epoch:32 step:25736 [D loss: 0.272523, acc.: 87.50%] [G loss: 3.263284]\n",
      "epoch:32 step:25737 [D loss: 0.223646, acc.: 90.62%] [G loss: 3.426896]\n",
      "epoch:32 step:25738 [D loss: 0.280968, acc.: 87.50%] [G loss: 3.882598]\n",
      "epoch:32 step:25739 [D loss: 0.244188, acc.: 89.06%] [G loss: 3.781341]\n",
      "epoch:32 step:25740 [D loss: 0.335247, acc.: 85.16%] [G loss: 3.212868]\n",
      "epoch:32 step:25741 [D loss: 0.336665, acc.: 85.16%] [G loss: 2.954153]\n",
      "epoch:32 step:25742 [D loss: 0.555728, acc.: 72.66%] [G loss: 3.280554]\n",
      "epoch:32 step:25743 [D loss: 0.291197, acc.: 89.06%] [G loss: 3.063202]\n",
      "epoch:32 step:25744 [D loss: 0.360756, acc.: 83.59%] [G loss: 3.017098]\n",
      "epoch:32 step:25745 [D loss: 0.371605, acc.: 83.59%] [G loss: 2.516179]\n",
      "epoch:32 step:25746 [D loss: 0.396450, acc.: 82.03%] [G loss: 3.040781]\n",
      "epoch:32 step:25747 [D loss: 0.255217, acc.: 89.06%] [G loss: 2.486555]\n",
      "epoch:32 step:25748 [D loss: 0.350395, acc.: 82.81%] [G loss: 3.214253]\n",
      "epoch:32 step:25749 [D loss: 0.358733, acc.: 78.12%] [G loss: 3.756591]\n",
      "epoch:32 step:25750 [D loss: 0.333750, acc.: 84.38%] [G loss: 2.929566]\n",
      "epoch:32 step:25751 [D loss: 0.264741, acc.: 89.84%] [G loss: 3.112923]\n",
      "epoch:32 step:25752 [D loss: 0.319529, acc.: 87.50%] [G loss: 4.081938]\n",
      "epoch:32 step:25753 [D loss: 0.335353, acc.: 85.16%] [G loss: 5.535451]\n",
      "epoch:32 step:25754 [D loss: 0.441801, acc.: 80.47%] [G loss: 2.898995]\n",
      "epoch:32 step:25755 [D loss: 0.456708, acc.: 77.34%] [G loss: 2.900918]\n",
      "epoch:32 step:25756 [D loss: 0.259176, acc.: 88.28%] [G loss: 3.203807]\n",
      "epoch:32 step:25757 [D loss: 0.334849, acc.: 86.72%] [G loss: 3.767498]\n",
      "epoch:32 step:25758 [D loss: 0.341044, acc.: 86.72%] [G loss: 4.307930]\n",
      "epoch:32 step:25759 [D loss: 0.292617, acc.: 85.94%] [G loss: 5.062174]\n",
      "epoch:32 step:25760 [D loss: 0.232865, acc.: 89.84%] [G loss: 3.698495]\n",
      "epoch:32 step:25761 [D loss: 0.211405, acc.: 90.62%] [G loss: 4.142341]\n",
      "epoch:32 step:25762 [D loss: 0.233327, acc.: 89.84%] [G loss: 2.583449]\n",
      "epoch:32 step:25763 [D loss: 0.326434, acc.: 82.03%] [G loss: 4.184564]\n",
      "epoch:32 step:25764 [D loss: 0.317605, acc.: 85.16%] [G loss: 3.297566]\n",
      "epoch:32 step:25765 [D loss: 0.298915, acc.: 86.72%] [G loss: 3.207737]\n",
      "epoch:32 step:25766 [D loss: 0.253583, acc.: 89.84%] [G loss: 3.756595]\n",
      "epoch:32 step:25767 [D loss: 0.268997, acc.: 89.06%] [G loss: 3.308590]\n",
      "epoch:32 step:25768 [D loss: 0.259697, acc.: 89.84%] [G loss: 3.934822]\n",
      "epoch:32 step:25769 [D loss: 0.326421, acc.: 82.03%] [G loss: 4.670476]\n",
      "epoch:32 step:25770 [D loss: 0.377386, acc.: 84.38%] [G loss: 2.707748]\n",
      "epoch:32 step:25771 [D loss: 0.231029, acc.: 88.28%] [G loss: 3.250040]\n",
      "epoch:32 step:25772 [D loss: 0.261086, acc.: 88.28%] [G loss: 2.549466]\n",
      "epoch:32 step:25773 [D loss: 0.322701, acc.: 85.16%] [G loss: 2.973814]\n",
      "epoch:33 step:25774 [D loss: 0.311763, acc.: 82.03%] [G loss: 4.030896]\n",
      "epoch:33 step:25775 [D loss: 0.312228, acc.: 83.59%] [G loss: 5.425498]\n",
      "epoch:33 step:25776 [D loss: 0.357828, acc.: 86.72%] [G loss: 4.477264]\n",
      "epoch:33 step:25777 [D loss: 0.249322, acc.: 88.28%] [G loss: 4.787807]\n",
      "epoch:33 step:25778 [D loss: 0.271210, acc.: 90.62%] [G loss: 4.421767]\n",
      "epoch:33 step:25779 [D loss: 0.156955, acc.: 92.19%] [G loss: 7.902212]\n",
      "epoch:33 step:25780 [D loss: 0.201052, acc.: 92.97%] [G loss: 6.150338]\n",
      "epoch:33 step:25781 [D loss: 0.191326, acc.: 91.41%] [G loss: 4.829680]\n",
      "epoch:33 step:25782 [D loss: 0.242765, acc.: 90.62%] [G loss: 6.419478]\n",
      "epoch:33 step:25783 [D loss: 0.291187, acc.: 83.59%] [G loss: 3.920994]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:33 step:25784 [D loss: 0.265752, acc.: 87.50%] [G loss: 3.955249]\n",
      "epoch:33 step:25785 [D loss: 0.259037, acc.: 89.06%] [G loss: 4.378810]\n",
      "epoch:33 step:25786 [D loss: 0.330603, acc.: 85.94%] [G loss: 3.787326]\n",
      "epoch:33 step:25787 [D loss: 0.276354, acc.: 88.28%] [G loss: 3.357988]\n",
      "epoch:33 step:25788 [D loss: 0.275518, acc.: 88.28%] [G loss: 3.967375]\n",
      "epoch:33 step:25789 [D loss: 0.305848, acc.: 84.38%] [G loss: 6.548366]\n",
      "epoch:33 step:25790 [D loss: 0.222434, acc.: 89.06%] [G loss: 5.145654]\n",
      "epoch:33 step:25791 [D loss: 0.206529, acc.: 89.84%] [G loss: 4.354764]\n",
      "epoch:33 step:25792 [D loss: 0.240235, acc.: 89.84%] [G loss: 3.577600]\n",
      "epoch:33 step:25793 [D loss: 0.255020, acc.: 89.84%] [G loss: 3.629049]\n",
      "epoch:33 step:25794 [D loss: 0.377975, acc.: 81.25%] [G loss: 3.069514]\n",
      "epoch:33 step:25795 [D loss: 0.339565, acc.: 84.38%] [G loss: 2.311242]\n",
      "epoch:33 step:25796 [D loss: 0.231356, acc.: 89.84%] [G loss: 2.968027]\n",
      "epoch:33 step:25797 [D loss: 0.357155, acc.: 82.03%] [G loss: 3.348730]\n",
      "epoch:33 step:25798 [D loss: 0.338344, acc.: 82.03%] [G loss: 3.143816]\n",
      "epoch:33 step:25799 [D loss: 0.316436, acc.: 89.84%] [G loss: 3.429105]\n",
      "epoch:33 step:25800 [D loss: 0.455154, acc.: 77.34%] [G loss: 3.249949]\n",
      "##############\n",
      "[0.86777721 0.86157205 0.79782328 0.84025187 0.80152268 0.82206349\n",
      " 0.87280583 0.82948394 0.84639052 0.80058576]\n",
      "##########\n",
      "epoch:33 step:25801 [D loss: 0.280251, acc.: 85.94%] [G loss: 3.211221]\n",
      "epoch:33 step:25802 [D loss: 0.338069, acc.: 88.28%] [G loss: 2.445816]\n",
      "epoch:33 step:25803 [D loss: 0.244887, acc.: 88.28%] [G loss: 3.047184]\n",
      "epoch:33 step:25804 [D loss: 0.297117, acc.: 83.59%] [G loss: 3.428490]\n",
      "epoch:33 step:25805 [D loss: 0.201870, acc.: 93.75%] [G loss: 4.222628]\n",
      "epoch:33 step:25806 [D loss: 0.322831, acc.: 84.38%] [G loss: 4.155704]\n",
      "epoch:33 step:25807 [D loss: 0.319864, acc.: 82.81%] [G loss: 4.893206]\n",
      "epoch:33 step:25808 [D loss: 0.296086, acc.: 89.06%] [G loss: 3.740888]\n",
      "epoch:33 step:25809 [D loss: 0.285100, acc.: 87.50%] [G loss: 4.083995]\n",
      "epoch:33 step:25810 [D loss: 0.317679, acc.: 85.16%] [G loss: 3.323371]\n",
      "epoch:33 step:25811 [D loss: 0.342292, acc.: 83.59%] [G loss: 4.402018]\n",
      "epoch:33 step:25812 [D loss: 0.363499, acc.: 82.03%] [G loss: 4.106555]\n",
      "epoch:33 step:25813 [D loss: 0.655339, acc.: 71.09%] [G loss: 4.267929]\n",
      "epoch:33 step:25814 [D loss: 0.418397, acc.: 80.47%] [G loss: 4.943586]\n",
      "epoch:33 step:25815 [D loss: 0.332728, acc.: 87.50%] [G loss: 6.072073]\n",
      "epoch:33 step:25816 [D loss: 0.419907, acc.: 81.25%] [G loss: 6.751560]\n",
      "epoch:33 step:25817 [D loss: 0.382373, acc.: 81.25%] [G loss: 3.537307]\n",
      "epoch:33 step:25818 [D loss: 0.405820, acc.: 75.78%] [G loss: 3.569438]\n",
      "epoch:33 step:25819 [D loss: 0.336228, acc.: 85.94%] [G loss: 2.296782]\n",
      "epoch:33 step:25820 [D loss: 0.404405, acc.: 78.91%] [G loss: 3.241935]\n",
      "epoch:33 step:25821 [D loss: 0.274579, acc.: 92.19%] [G loss: 2.043320]\n",
      "epoch:33 step:25822 [D loss: 0.294887, acc.: 84.38%] [G loss: 2.983085]\n",
      "epoch:33 step:25823 [D loss: 0.297050, acc.: 87.50%] [G loss: 3.698260]\n",
      "epoch:33 step:25824 [D loss: 0.387055, acc.: 83.59%] [G loss: 3.385397]\n",
      "epoch:33 step:25825 [D loss: 0.238150, acc.: 91.41%] [G loss: 2.973230]\n",
      "epoch:33 step:25826 [D loss: 0.267198, acc.: 91.41%] [G loss: 3.120689]\n",
      "epoch:33 step:25827 [D loss: 0.243878, acc.: 89.06%] [G loss: 3.796512]\n",
      "epoch:33 step:25828 [D loss: 0.338166, acc.: 82.03%] [G loss: 3.005727]\n",
      "epoch:33 step:25829 [D loss: 0.455291, acc.: 79.69%] [G loss: 4.224849]\n",
      "epoch:33 step:25830 [D loss: 0.365217, acc.: 82.81%] [G loss: 3.614857]\n",
      "epoch:33 step:25831 [D loss: 0.331405, acc.: 80.47%] [G loss: 4.203100]\n",
      "epoch:33 step:25832 [D loss: 0.373588, acc.: 81.25%] [G loss: 2.896570]\n",
      "epoch:33 step:25833 [D loss: 0.308612, acc.: 84.38%] [G loss: 3.779327]\n",
      "epoch:33 step:25834 [D loss: 0.317316, acc.: 85.94%] [G loss: 3.443111]\n",
      "epoch:33 step:25835 [D loss: 0.394752, acc.: 82.81%] [G loss: 3.330610]\n",
      "epoch:33 step:25836 [D loss: 0.300869, acc.: 87.50%] [G loss: 3.887200]\n",
      "epoch:33 step:25837 [D loss: 0.258587, acc.: 92.19%] [G loss: 4.071001]\n",
      "epoch:33 step:25838 [D loss: 0.321122, acc.: 87.50%] [G loss: 4.426361]\n",
      "epoch:33 step:25839 [D loss: 0.367449, acc.: 81.25%] [G loss: 3.350614]\n",
      "epoch:33 step:25840 [D loss: 0.221128, acc.: 92.19%] [G loss: 3.405830]\n",
      "epoch:33 step:25841 [D loss: 0.356540, acc.: 85.16%] [G loss: 3.029517]\n",
      "epoch:33 step:25842 [D loss: 0.271944, acc.: 85.94%] [G loss: 3.195991]\n",
      "epoch:33 step:25843 [D loss: 0.322006, acc.: 87.50%] [G loss: 2.873868]\n",
      "epoch:33 step:25844 [D loss: 0.355430, acc.: 81.25%] [G loss: 3.233212]\n",
      "epoch:33 step:25845 [D loss: 0.336431, acc.: 86.72%] [G loss: 6.169492]\n",
      "epoch:33 step:25846 [D loss: 0.612287, acc.: 78.12%] [G loss: 5.812012]\n",
      "epoch:33 step:25847 [D loss: 1.161545, acc.: 68.75%] [G loss: 9.917497]\n",
      "epoch:33 step:25848 [D loss: 2.212985, acc.: 71.88%] [G loss: 5.772437]\n",
      "epoch:33 step:25849 [D loss: 1.661479, acc.: 60.94%] [G loss: 12.277325]\n",
      "epoch:33 step:25850 [D loss: 2.918556, acc.: 62.50%] [G loss: 7.677141]\n",
      "epoch:33 step:25851 [D loss: 1.289138, acc.: 60.94%] [G loss: 3.680933]\n",
      "epoch:33 step:25852 [D loss: 0.489505, acc.: 78.12%] [G loss: 4.604029]\n",
      "epoch:33 step:25853 [D loss: 0.559910, acc.: 75.78%] [G loss: 3.395211]\n",
      "epoch:33 step:25854 [D loss: 0.500170, acc.: 81.25%] [G loss: 4.730797]\n",
      "epoch:33 step:25855 [D loss: 0.443101, acc.: 81.25%] [G loss: 3.768805]\n",
      "epoch:33 step:25856 [D loss: 0.357254, acc.: 82.81%] [G loss: 4.811064]\n",
      "epoch:33 step:25857 [D loss: 0.338497, acc.: 85.94%] [G loss: 3.449789]\n",
      "epoch:33 step:25858 [D loss: 0.373203, acc.: 82.81%] [G loss: 3.489507]\n",
      "epoch:33 step:25859 [D loss: 0.297516, acc.: 87.50%] [G loss: 4.072061]\n",
      "epoch:33 step:25860 [D loss: 0.315465, acc.: 85.94%] [G loss: 2.692301]\n",
      "epoch:33 step:25861 [D loss: 0.332363, acc.: 82.81%] [G loss: 2.936170]\n",
      "epoch:33 step:25862 [D loss: 0.318417, acc.: 84.38%] [G loss: 2.586336]\n",
      "epoch:33 step:25863 [D loss: 0.309799, acc.: 85.16%] [G loss: 3.013549]\n",
      "epoch:33 step:25864 [D loss: 0.296886, acc.: 89.06%] [G loss: 3.022503]\n",
      "epoch:33 step:25865 [D loss: 0.342368, acc.: 85.16%] [G loss: 2.937822]\n",
      "epoch:33 step:25866 [D loss: 0.306710, acc.: 87.50%] [G loss: 2.691689]\n",
      "epoch:33 step:25867 [D loss: 0.289260, acc.: 86.72%] [G loss: 3.289309]\n",
      "epoch:33 step:25868 [D loss: 0.415983, acc.: 82.03%] [G loss: 2.821593]\n",
      "epoch:33 step:25869 [D loss: 0.335721, acc.: 85.94%] [G loss: 2.211995]\n",
      "epoch:33 step:25870 [D loss: 0.349022, acc.: 86.72%] [G loss: 2.678403]\n",
      "epoch:33 step:25871 [D loss: 0.356179, acc.: 83.59%] [G loss: 2.210260]\n",
      "epoch:33 step:25872 [D loss: 0.399096, acc.: 80.47%] [G loss: 2.121093]\n",
      "epoch:33 step:25873 [D loss: 0.378386, acc.: 85.16%] [G loss: 2.462142]\n",
      "epoch:33 step:25874 [D loss: 0.305330, acc.: 86.72%] [G loss: 2.565280]\n",
      "epoch:33 step:25875 [D loss: 0.403843, acc.: 85.16%] [G loss: 2.735093]\n",
      "epoch:33 step:25876 [D loss: 0.338895, acc.: 84.38%] [G loss: 2.849634]\n",
      "epoch:33 step:25877 [D loss: 0.240375, acc.: 89.84%] [G loss: 2.403781]\n",
      "epoch:33 step:25878 [D loss: 0.300040, acc.: 85.94%] [G loss: 2.724822]\n",
      "epoch:33 step:25879 [D loss: 0.326251, acc.: 88.28%] [G loss: 2.025621]\n",
      "epoch:33 step:25880 [D loss: 0.277688, acc.: 89.84%] [G loss: 2.558923]\n",
      "epoch:33 step:25881 [D loss: 0.318493, acc.: 87.50%] [G loss: 2.957622]\n",
      "epoch:33 step:25882 [D loss: 0.431320, acc.: 79.69%] [G loss: 2.857453]\n",
      "epoch:33 step:25883 [D loss: 0.335218, acc.: 83.59%] [G loss: 3.423524]\n",
      "epoch:33 step:25884 [D loss: 0.337354, acc.: 84.38%] [G loss: 2.273329]\n",
      "epoch:33 step:25885 [D loss: 0.291946, acc.: 87.50%] [G loss: 2.524631]\n",
      "epoch:33 step:25886 [D loss: 0.427350, acc.: 75.78%] [G loss: 2.543970]\n",
      "epoch:33 step:25887 [D loss: 0.306483, acc.: 88.28%] [G loss: 3.028568]\n",
      "epoch:33 step:25888 [D loss: 0.337626, acc.: 87.50%] [G loss: 2.549709]\n",
      "epoch:33 step:25889 [D loss: 0.410718, acc.: 81.25%] [G loss: 2.950032]\n",
      "epoch:33 step:25890 [D loss: 0.287442, acc.: 85.16%] [G loss: 2.258683]\n",
      "epoch:33 step:25891 [D loss: 0.244010, acc.: 89.06%] [G loss: 3.786800]\n",
      "epoch:33 step:25892 [D loss: 0.253025, acc.: 88.28%] [G loss: 2.729699]\n",
      "epoch:33 step:25893 [D loss: 0.299144, acc.: 85.16%] [G loss: 2.602504]\n",
      "epoch:33 step:25894 [D loss: 0.240350, acc.: 92.19%] [G loss: 2.823667]\n",
      "epoch:33 step:25895 [D loss: 0.318482, acc.: 85.16%] [G loss: 3.029362]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:33 step:25896 [D loss: 0.324258, acc.: 83.59%] [G loss: 2.705287]\n",
      "epoch:33 step:25897 [D loss: 0.347156, acc.: 84.38%] [G loss: 2.902497]\n",
      "epoch:33 step:25898 [D loss: 0.445193, acc.: 82.03%] [G loss: 2.550259]\n",
      "epoch:33 step:25899 [D loss: 0.265846, acc.: 91.41%] [G loss: 3.060229]\n",
      "epoch:33 step:25900 [D loss: 0.383716, acc.: 85.94%] [G loss: 2.672187]\n",
      "epoch:33 step:25901 [D loss: 0.327286, acc.: 83.59%] [G loss: 2.577892]\n",
      "epoch:33 step:25902 [D loss: 0.361886, acc.: 85.94%] [G loss: 2.283519]\n",
      "epoch:33 step:25903 [D loss: 0.328344, acc.: 82.03%] [G loss: 2.999563]\n",
      "epoch:33 step:25904 [D loss: 0.413311, acc.: 80.47%] [G loss: 2.743955]\n",
      "epoch:33 step:25905 [D loss: 0.301699, acc.: 89.84%] [G loss: 2.705727]\n",
      "epoch:33 step:25906 [D loss: 0.315749, acc.: 87.50%] [G loss: 2.935549]\n",
      "epoch:33 step:25907 [D loss: 0.318635, acc.: 89.06%] [G loss: 3.638420]\n",
      "epoch:33 step:25908 [D loss: 0.382694, acc.: 80.47%] [G loss: 2.799272]\n",
      "epoch:33 step:25909 [D loss: 0.274164, acc.: 90.62%] [G loss: 2.777124]\n",
      "epoch:33 step:25910 [D loss: 0.296935, acc.: 88.28%] [G loss: 2.951249]\n",
      "epoch:33 step:25911 [D loss: 0.322917, acc.: 89.84%] [G loss: 2.518014]\n",
      "epoch:33 step:25912 [D loss: 0.308590, acc.: 85.94%] [G loss: 3.326365]\n",
      "epoch:33 step:25913 [D loss: 0.366650, acc.: 84.38%] [G loss: 3.072957]\n",
      "epoch:33 step:25914 [D loss: 0.267834, acc.: 89.06%] [G loss: 2.373186]\n",
      "epoch:33 step:25915 [D loss: 0.406234, acc.: 78.91%] [G loss: 2.703058]\n",
      "epoch:33 step:25916 [D loss: 0.251545, acc.: 90.62%] [G loss: 2.744631]\n",
      "epoch:33 step:25917 [D loss: 0.418187, acc.: 83.59%] [G loss: 2.259421]\n",
      "epoch:33 step:25918 [D loss: 0.336861, acc.: 87.50%] [G loss: 2.644652]\n",
      "epoch:33 step:25919 [D loss: 0.391137, acc.: 80.47%] [G loss: 2.168388]\n",
      "epoch:33 step:25920 [D loss: 0.315183, acc.: 86.72%] [G loss: 2.809361]\n",
      "epoch:33 step:25921 [D loss: 0.294135, acc.: 86.72%] [G loss: 3.121806]\n",
      "epoch:33 step:25922 [D loss: 0.336240, acc.: 84.38%] [G loss: 2.505249]\n",
      "epoch:33 step:25923 [D loss: 0.281110, acc.: 85.16%] [G loss: 3.063026]\n",
      "epoch:33 step:25924 [D loss: 0.336384, acc.: 84.38%] [G loss: 2.900424]\n",
      "epoch:33 step:25925 [D loss: 0.367640, acc.: 81.25%] [G loss: 3.349384]\n",
      "epoch:33 step:25926 [D loss: 0.253548, acc.: 88.28%] [G loss: 4.158766]\n",
      "epoch:33 step:25927 [D loss: 0.310508, acc.: 84.38%] [G loss: 5.282798]\n",
      "epoch:33 step:25928 [D loss: 0.326566, acc.: 89.06%] [G loss: 3.507399]\n",
      "epoch:33 step:25929 [D loss: 0.276800, acc.: 86.72%] [G loss: 3.009808]\n",
      "epoch:33 step:25930 [D loss: 0.227030, acc.: 92.19%] [G loss: 4.079429]\n",
      "epoch:33 step:25931 [D loss: 0.290938, acc.: 88.28%] [G loss: 3.023358]\n",
      "epoch:33 step:25932 [D loss: 0.233931, acc.: 90.62%] [G loss: 4.126095]\n",
      "epoch:33 step:25933 [D loss: 0.375181, acc.: 85.94%] [G loss: 4.382794]\n",
      "epoch:33 step:25934 [D loss: 0.294818, acc.: 86.72%] [G loss: 3.475118]\n",
      "epoch:33 step:25935 [D loss: 0.218045, acc.: 92.19%] [G loss: 3.280758]\n",
      "epoch:33 step:25936 [D loss: 0.334460, acc.: 85.94%] [G loss: 3.485696]\n",
      "epoch:33 step:25937 [D loss: 0.210656, acc.: 92.97%] [G loss: 2.643818]\n",
      "epoch:33 step:25938 [D loss: 0.376459, acc.: 81.25%] [G loss: 3.266650]\n",
      "epoch:33 step:25939 [D loss: 0.337442, acc.: 82.03%] [G loss: 2.633428]\n",
      "epoch:33 step:25940 [D loss: 0.268527, acc.: 90.62%] [G loss: 2.925960]\n",
      "epoch:33 step:25941 [D loss: 0.357971, acc.: 82.03%] [G loss: 2.814356]\n",
      "epoch:33 step:25942 [D loss: 0.350213, acc.: 85.94%] [G loss: 2.953887]\n",
      "epoch:33 step:25943 [D loss: 0.318445, acc.: 86.72%] [G loss: 2.499135]\n",
      "epoch:33 step:25944 [D loss: 0.433218, acc.: 82.03%] [G loss: 2.837946]\n",
      "epoch:33 step:25945 [D loss: 0.398156, acc.: 82.03%] [G loss: 3.015209]\n",
      "epoch:33 step:25946 [D loss: 0.362108, acc.: 81.25%] [G loss: 3.065003]\n",
      "epoch:33 step:25947 [D loss: 0.343274, acc.: 86.72%] [G loss: 2.841368]\n",
      "epoch:33 step:25948 [D loss: 0.328530, acc.: 84.38%] [G loss: 2.256893]\n",
      "epoch:33 step:25949 [D loss: 0.313863, acc.: 87.50%] [G loss: 2.872298]\n",
      "epoch:33 step:25950 [D loss: 0.415039, acc.: 80.47%] [G loss: 2.119473]\n",
      "epoch:33 step:25951 [D loss: 0.441257, acc.: 74.22%] [G loss: 2.873826]\n",
      "epoch:33 step:25952 [D loss: 0.329301, acc.: 85.16%] [G loss: 3.200843]\n",
      "epoch:33 step:25953 [D loss: 0.359444, acc.: 86.72%] [G loss: 3.084911]\n",
      "epoch:33 step:25954 [D loss: 0.354917, acc.: 85.94%] [G loss: 3.263644]\n",
      "epoch:33 step:25955 [D loss: 0.421258, acc.: 79.69%] [G loss: 3.749626]\n",
      "epoch:33 step:25956 [D loss: 0.280136, acc.: 90.62%] [G loss: 3.624542]\n",
      "epoch:33 step:25957 [D loss: 0.281838, acc.: 85.94%] [G loss: 2.965170]\n",
      "epoch:33 step:25958 [D loss: 0.320368, acc.: 88.28%] [G loss: 2.752671]\n",
      "epoch:33 step:25959 [D loss: 0.207095, acc.: 92.19%] [G loss: 2.806664]\n",
      "epoch:33 step:25960 [D loss: 0.301584, acc.: 85.16%] [G loss: 2.648189]\n",
      "epoch:33 step:25961 [D loss: 0.416499, acc.: 82.03%] [G loss: 3.214444]\n",
      "epoch:33 step:25962 [D loss: 0.322191, acc.: 83.59%] [G loss: 2.535164]\n",
      "epoch:33 step:25963 [D loss: 0.389437, acc.: 83.59%] [G loss: 4.045236]\n",
      "epoch:33 step:25964 [D loss: 0.487661, acc.: 80.47%] [G loss: 4.264562]\n",
      "epoch:33 step:25965 [D loss: 0.329646, acc.: 87.50%] [G loss: 4.167530]\n",
      "epoch:33 step:25966 [D loss: 0.394740, acc.: 82.81%] [G loss: 3.889962]\n",
      "epoch:33 step:25967 [D loss: 0.375553, acc.: 82.81%] [G loss: 2.426680]\n",
      "epoch:33 step:25968 [D loss: 0.360557, acc.: 87.50%] [G loss: 4.088180]\n",
      "epoch:33 step:25969 [D loss: 0.275579, acc.: 88.28%] [G loss: 4.299192]\n",
      "epoch:33 step:25970 [D loss: 0.457726, acc.: 74.22%] [G loss: 4.229998]\n",
      "epoch:33 step:25971 [D loss: 0.300929, acc.: 88.28%] [G loss: 3.112920]\n",
      "epoch:33 step:25972 [D loss: 0.321980, acc.: 85.94%] [G loss: 3.934324]\n",
      "epoch:33 step:25973 [D loss: 0.211413, acc.: 92.19%] [G loss: 4.434051]\n",
      "epoch:33 step:25974 [D loss: 0.390482, acc.: 81.25%] [G loss: 2.867560]\n",
      "epoch:33 step:25975 [D loss: 0.232378, acc.: 88.28%] [G loss: 5.392282]\n",
      "epoch:33 step:25976 [D loss: 0.323752, acc.: 85.94%] [G loss: 2.523797]\n",
      "epoch:33 step:25977 [D loss: 0.262934, acc.: 89.84%] [G loss: 4.115610]\n",
      "epoch:33 step:25978 [D loss: 0.347541, acc.: 87.50%] [G loss: 2.850205]\n",
      "epoch:33 step:25979 [D loss: 0.447847, acc.: 82.03%] [G loss: 3.103763]\n",
      "epoch:33 step:25980 [D loss: 0.303712, acc.: 85.94%] [G loss: 3.728400]\n",
      "epoch:33 step:25981 [D loss: 0.334825, acc.: 86.72%] [G loss: 6.289677]\n",
      "epoch:33 step:25982 [D loss: 0.219158, acc.: 89.84%] [G loss: 3.746944]\n",
      "epoch:33 step:25983 [D loss: 0.269064, acc.: 87.50%] [G loss: 3.426850]\n",
      "epoch:33 step:25984 [D loss: 0.304020, acc.: 85.16%] [G loss: 2.601999]\n",
      "epoch:33 step:25985 [D loss: 0.324596, acc.: 85.16%] [G loss: 2.839429]\n",
      "epoch:33 step:25986 [D loss: 0.380077, acc.: 85.94%] [G loss: 2.983838]\n",
      "epoch:33 step:25987 [D loss: 0.441708, acc.: 80.47%] [G loss: 2.775982]\n",
      "epoch:33 step:25988 [D loss: 0.360211, acc.: 82.03%] [G loss: 3.100856]\n",
      "epoch:33 step:25989 [D loss: 0.330216, acc.: 85.94%] [G loss: 2.909187]\n",
      "epoch:33 step:25990 [D loss: 0.353306, acc.: 84.38%] [G loss: 2.983108]\n",
      "epoch:33 step:25991 [D loss: 0.311532, acc.: 85.16%] [G loss: 2.730055]\n",
      "epoch:33 step:25992 [D loss: 0.260825, acc.: 90.62%] [G loss: 2.593241]\n",
      "epoch:33 step:25993 [D loss: 0.408914, acc.: 79.69%] [G loss: 2.258061]\n",
      "epoch:33 step:25994 [D loss: 0.316593, acc.: 85.16%] [G loss: 3.453530]\n",
      "epoch:33 step:25995 [D loss: 0.282540, acc.: 87.50%] [G loss: 4.097510]\n",
      "epoch:33 step:25996 [D loss: 0.370512, acc.: 85.94%] [G loss: 2.537764]\n",
      "epoch:33 step:25997 [D loss: 0.291291, acc.: 86.72%] [G loss: 3.467476]\n",
      "epoch:33 step:25998 [D loss: 0.390622, acc.: 79.69%] [G loss: 4.029368]\n",
      "epoch:33 step:25999 [D loss: 0.353704, acc.: 84.38%] [G loss: 3.438533]\n",
      "epoch:33 step:26000 [D loss: 0.206584, acc.: 92.97%] [G loss: 3.781683]\n",
      "##############\n",
      "[0.86224154 0.84678446 0.8135236  0.81637598 0.78482802 0.84110863\n",
      " 0.87306093 0.83989564 0.80391513 0.80660009]\n",
      "##########\n",
      "epoch:33 step:26001 [D loss: 0.318801, acc.: 85.94%] [G loss: 3.243848]\n",
      "epoch:33 step:26002 [D loss: 0.405266, acc.: 83.59%] [G loss: 4.003342]\n",
      "epoch:33 step:26003 [D loss: 0.262305, acc.: 87.50%] [G loss: 4.991801]\n",
      "epoch:33 step:26004 [D loss: 0.279131, acc.: 85.94%] [G loss: 3.521632]\n",
      "epoch:33 step:26005 [D loss: 0.328541, acc.: 85.94%] [G loss: 3.366745]\n",
      "epoch:33 step:26006 [D loss: 0.306352, acc.: 82.81%] [G loss: 2.895726]\n",
      "epoch:33 step:26007 [D loss: 0.249836, acc.: 90.62%] [G loss: 4.107908]\n",
      "epoch:33 step:26008 [D loss: 0.280423, acc.: 85.16%] [G loss: 5.888340]\n",
      "epoch:33 step:26009 [D loss: 0.233538, acc.: 89.06%] [G loss: 4.943946]\n",
      "epoch:33 step:26010 [D loss: 0.242558, acc.: 87.50%] [G loss: 3.196117]\n",
      "epoch:33 step:26011 [D loss: 0.334659, acc.: 82.81%] [G loss: 3.169766]\n",
      "epoch:33 step:26012 [D loss: 0.308545, acc.: 89.06%] [G loss: 2.674662]\n",
      "epoch:33 step:26013 [D loss: 0.345128, acc.: 85.16%] [G loss: 4.439008]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:33 step:26014 [D loss: 0.403988, acc.: 83.59%] [G loss: 3.003675]\n",
      "epoch:33 step:26015 [D loss: 0.395946, acc.: 82.81%] [G loss: 2.412743]\n",
      "epoch:33 step:26016 [D loss: 0.339763, acc.: 82.03%] [G loss: 3.410249]\n",
      "epoch:33 step:26017 [D loss: 0.281308, acc.: 89.84%] [G loss: 3.490195]\n",
      "epoch:33 step:26018 [D loss: 0.297620, acc.: 89.06%] [G loss: 2.442337]\n",
      "epoch:33 step:26019 [D loss: 0.274144, acc.: 90.62%] [G loss: 5.071971]\n",
      "epoch:33 step:26020 [D loss: 0.361541, acc.: 84.38%] [G loss: 2.426969]\n",
      "epoch:33 step:26021 [D loss: 0.307918, acc.: 86.72%] [G loss: 3.381579]\n",
      "epoch:33 step:26022 [D loss: 0.317634, acc.: 89.84%] [G loss: 3.073471]\n",
      "epoch:33 step:26023 [D loss: 0.499547, acc.: 76.56%] [G loss: 5.752135]\n",
      "epoch:33 step:26024 [D loss: 0.254086, acc.: 89.84%] [G loss: 2.846128]\n",
      "epoch:33 step:26025 [D loss: 0.314930, acc.: 85.94%] [G loss: 2.895219]\n",
      "epoch:33 step:26026 [D loss: 0.320183, acc.: 86.72%] [G loss: 3.395733]\n",
      "epoch:33 step:26027 [D loss: 0.225573, acc.: 89.06%] [G loss: 3.337982]\n",
      "epoch:33 step:26028 [D loss: 0.324998, acc.: 85.94%] [G loss: 2.742794]\n",
      "epoch:33 step:26029 [D loss: 0.204107, acc.: 92.19%] [G loss: 3.427902]\n",
      "epoch:33 step:26030 [D loss: 0.312536, acc.: 82.81%] [G loss: 3.326436]\n",
      "epoch:33 step:26031 [D loss: 0.293013, acc.: 87.50%] [G loss: 3.362675]\n",
      "epoch:33 step:26032 [D loss: 0.262971, acc.: 87.50%] [G loss: 4.189250]\n",
      "epoch:33 step:26033 [D loss: 0.286305, acc.: 85.16%] [G loss: 3.363334]\n",
      "epoch:33 step:26034 [D loss: 0.297627, acc.: 84.38%] [G loss: 5.025127]\n",
      "epoch:33 step:26035 [D loss: 0.295480, acc.: 87.50%] [G loss: 5.507220]\n",
      "epoch:33 step:26036 [D loss: 0.309141, acc.: 88.28%] [G loss: 4.199300]\n",
      "epoch:33 step:26037 [D loss: 0.299072, acc.: 88.28%] [G loss: 4.759347]\n",
      "epoch:33 step:26038 [D loss: 0.294482, acc.: 89.06%] [G loss: 4.099828]\n",
      "epoch:33 step:26039 [D loss: 0.261759, acc.: 86.72%] [G loss: 4.781947]\n",
      "epoch:33 step:26040 [D loss: 0.353460, acc.: 81.25%] [G loss: 4.102576]\n",
      "epoch:33 step:26041 [D loss: 0.282534, acc.: 88.28%] [G loss: 3.928498]\n",
      "epoch:33 step:26042 [D loss: 0.267649, acc.: 87.50%] [G loss: 3.129271]\n",
      "epoch:33 step:26043 [D loss: 0.209847, acc.: 90.62%] [G loss: 3.273684]\n",
      "epoch:33 step:26044 [D loss: 0.305108, acc.: 88.28%] [G loss: 2.562105]\n",
      "epoch:33 step:26045 [D loss: 0.351119, acc.: 86.72%] [G loss: 2.648641]\n",
      "epoch:33 step:26046 [D loss: 0.250619, acc.: 88.28%] [G loss: 3.125953]\n",
      "epoch:33 step:26047 [D loss: 0.333478, acc.: 87.50%] [G loss: 2.973974]\n",
      "epoch:33 step:26048 [D loss: 0.485700, acc.: 76.56%] [G loss: 2.576344]\n",
      "epoch:33 step:26049 [D loss: 0.273595, acc.: 90.62%] [G loss: 2.724876]\n",
      "epoch:33 step:26050 [D loss: 0.495628, acc.: 75.00%] [G loss: 2.221966]\n",
      "epoch:33 step:26051 [D loss: 0.371125, acc.: 83.59%] [G loss: 4.497303]\n",
      "epoch:33 step:26052 [D loss: 0.316607, acc.: 83.59%] [G loss: 4.436673]\n",
      "epoch:33 step:26053 [D loss: 0.275970, acc.: 87.50%] [G loss: 3.473468]\n",
      "epoch:33 step:26054 [D loss: 0.261796, acc.: 87.50%] [G loss: 4.928470]\n",
      "epoch:33 step:26055 [D loss: 0.314225, acc.: 85.94%] [G loss: 3.143784]\n",
      "epoch:33 step:26056 [D loss: 0.328229, acc.: 82.81%] [G loss: 4.709804]\n",
      "epoch:33 step:26057 [D loss: 0.358528, acc.: 82.81%] [G loss: 2.789429]\n",
      "epoch:33 step:26058 [D loss: 0.261402, acc.: 88.28%] [G loss: 3.211776]\n",
      "epoch:33 step:26059 [D loss: 0.378066, acc.: 85.16%] [G loss: 2.777735]\n",
      "epoch:33 step:26060 [D loss: 0.205839, acc.: 92.19%] [G loss: 3.033210]\n",
      "epoch:33 step:26061 [D loss: 0.361569, acc.: 83.59%] [G loss: 2.467388]\n",
      "epoch:33 step:26062 [D loss: 0.379556, acc.: 82.81%] [G loss: 2.974833]\n",
      "epoch:33 step:26063 [D loss: 0.287248, acc.: 85.94%] [G loss: 3.784615]\n",
      "epoch:33 step:26064 [D loss: 0.311607, acc.: 85.16%] [G loss: 3.071146]\n",
      "epoch:33 step:26065 [D loss: 0.397924, acc.: 75.78%] [G loss: 2.987803]\n",
      "epoch:33 step:26066 [D loss: 0.382665, acc.: 85.94%] [G loss: 2.926041]\n",
      "epoch:33 step:26067 [D loss: 0.453311, acc.: 81.25%] [G loss: 2.863088]\n",
      "epoch:33 step:26068 [D loss: 0.308702, acc.: 85.16%] [G loss: 3.620995]\n",
      "epoch:33 step:26069 [D loss: 0.374565, acc.: 82.81%] [G loss: 3.074843]\n",
      "epoch:33 step:26070 [D loss: 0.347906, acc.: 86.72%] [G loss: 2.259502]\n",
      "epoch:33 step:26071 [D loss: 0.302560, acc.: 85.94%] [G loss: 3.217129]\n",
      "epoch:33 step:26072 [D loss: 0.242562, acc.: 89.84%] [G loss: 2.913967]\n",
      "epoch:33 step:26073 [D loss: 0.406029, acc.: 80.47%] [G loss: 3.561827]\n",
      "epoch:33 step:26074 [D loss: 0.381643, acc.: 82.03%] [G loss: 2.293422]\n",
      "epoch:33 step:26075 [D loss: 0.310220, acc.: 84.38%] [G loss: 2.339615]\n",
      "epoch:33 step:26076 [D loss: 0.298922, acc.: 85.94%] [G loss: 2.789701]\n",
      "epoch:33 step:26077 [D loss: 0.335884, acc.: 82.81%] [G loss: 2.859141]\n",
      "epoch:33 step:26078 [D loss: 0.390344, acc.: 84.38%] [G loss: 4.615078]\n",
      "epoch:33 step:26079 [D loss: 0.440472, acc.: 82.03%] [G loss: 5.243196]\n",
      "epoch:33 step:26080 [D loss: 0.280869, acc.: 86.72%] [G loss: 4.168475]\n",
      "epoch:33 step:26081 [D loss: 0.362693, acc.: 85.16%] [G loss: 4.829638]\n",
      "epoch:33 step:26082 [D loss: 0.212271, acc.: 94.53%] [G loss: 5.146849]\n",
      "epoch:33 step:26083 [D loss: 0.295061, acc.: 85.16%] [G loss: 4.000118]\n",
      "epoch:33 step:26084 [D loss: 0.381444, acc.: 80.47%] [G loss: 4.396559]\n",
      "epoch:33 step:26085 [D loss: 0.276890, acc.: 89.06%] [G loss: 3.446223]\n",
      "epoch:33 step:26086 [D loss: 0.297761, acc.: 87.50%] [G loss: 3.910873]\n",
      "epoch:33 step:26087 [D loss: 0.354187, acc.: 83.59%] [G loss: 3.862228]\n",
      "epoch:33 step:26088 [D loss: 0.421748, acc.: 81.25%] [G loss: 4.010270]\n",
      "epoch:33 step:26089 [D loss: 0.527304, acc.: 81.25%] [G loss: 6.768128]\n",
      "epoch:33 step:26090 [D loss: 1.380451, acc.: 65.62%] [G loss: 7.893208]\n",
      "epoch:33 step:26091 [D loss: 2.994426, acc.: 55.47%] [G loss: 3.677935]\n",
      "epoch:33 step:26092 [D loss: 1.077513, acc.: 69.53%] [G loss: 8.937231]\n",
      "epoch:33 step:26093 [D loss: 0.774092, acc.: 69.53%] [G loss: 5.735248]\n",
      "epoch:33 step:26094 [D loss: 0.356907, acc.: 80.47%] [G loss: 3.892612]\n",
      "epoch:33 step:26095 [D loss: 0.568273, acc.: 82.03%] [G loss: 5.417735]\n",
      "epoch:33 step:26096 [D loss: 0.225206, acc.: 90.62%] [G loss: 4.262949]\n",
      "epoch:33 step:26097 [D loss: 0.444012, acc.: 79.69%] [G loss: 4.379926]\n",
      "epoch:33 step:26098 [D loss: 0.374993, acc.: 80.47%] [G loss: 3.244462]\n",
      "epoch:33 step:26099 [D loss: 0.429554, acc.: 77.34%] [G loss: 2.136758]\n",
      "epoch:33 step:26100 [D loss: 0.220570, acc.: 92.19%] [G loss: 2.428217]\n",
      "epoch:33 step:26101 [D loss: 0.315396, acc.: 86.72%] [G loss: 2.217043]\n",
      "epoch:33 step:26102 [D loss: 0.384753, acc.: 85.16%] [G loss: 2.379839]\n",
      "epoch:33 step:26103 [D loss: 0.344481, acc.: 80.47%] [G loss: 2.351973]\n",
      "epoch:33 step:26104 [D loss: 0.450419, acc.: 80.47%] [G loss: 3.251450]\n",
      "epoch:33 step:26105 [D loss: 0.326548, acc.: 88.28%] [G loss: 2.581283]\n",
      "epoch:33 step:26106 [D loss: 0.264075, acc.: 91.41%] [G loss: 2.546906]\n",
      "epoch:33 step:26107 [D loss: 0.431842, acc.: 79.69%] [G loss: 1.998477]\n",
      "epoch:33 step:26108 [D loss: 0.278726, acc.: 88.28%] [G loss: 2.472250]\n",
      "epoch:33 step:26109 [D loss: 0.326252, acc.: 89.06%] [G loss: 2.261118]\n",
      "epoch:33 step:26110 [D loss: 0.273429, acc.: 89.06%] [G loss: 2.792189]\n",
      "epoch:33 step:26111 [D loss: 0.368708, acc.: 83.59%] [G loss: 2.867949]\n",
      "epoch:33 step:26112 [D loss: 0.369031, acc.: 85.16%] [G loss: 2.377142]\n",
      "epoch:33 step:26113 [D loss: 0.369099, acc.: 79.69%] [G loss: 2.588045]\n",
      "epoch:33 step:26114 [D loss: 0.282434, acc.: 87.50%] [G loss: 2.259182]\n",
      "epoch:33 step:26115 [D loss: 0.458248, acc.: 73.44%] [G loss: 2.049988]\n",
      "epoch:33 step:26116 [D loss: 0.387842, acc.: 82.81%] [G loss: 2.769204]\n",
      "epoch:33 step:26117 [D loss: 0.325614, acc.: 83.59%] [G loss: 2.588000]\n",
      "epoch:33 step:26118 [D loss: 0.355156, acc.: 82.81%] [G loss: 3.086593]\n",
      "epoch:33 step:26119 [D loss: 0.277314, acc.: 89.06%] [G loss: 3.368269]\n",
      "epoch:33 step:26120 [D loss: 0.273635, acc.: 88.28%] [G loss: 2.863129]\n",
      "epoch:33 step:26121 [D loss: 0.257407, acc.: 89.06%] [G loss: 2.617462]\n",
      "epoch:33 step:26122 [D loss: 0.404512, acc.: 82.03%] [G loss: 2.319743]\n",
      "epoch:33 step:26123 [D loss: 0.360152, acc.: 82.81%] [G loss: 2.639184]\n",
      "epoch:33 step:26124 [D loss: 0.360592, acc.: 82.81%] [G loss: 2.881637]\n",
      "epoch:33 step:26125 [D loss: 0.375586, acc.: 78.91%] [G loss: 2.733692]\n",
      "epoch:33 step:26126 [D loss: 0.261262, acc.: 91.41%] [G loss: 2.731272]\n",
      "epoch:33 step:26127 [D loss: 0.297905, acc.: 85.16%] [G loss: 3.136672]\n",
      "epoch:33 step:26128 [D loss: 0.255206, acc.: 92.19%] [G loss: 2.805871]\n",
      "epoch:33 step:26129 [D loss: 0.277371, acc.: 87.50%] [G loss: 3.563931]\n",
      "epoch:33 step:26130 [D loss: 0.334253, acc.: 84.38%] [G loss: 2.969615]\n",
      "epoch:33 step:26131 [D loss: 0.243715, acc.: 90.62%] [G loss: 3.748871]\n",
      "epoch:33 step:26132 [D loss: 0.290189, acc.: 88.28%] [G loss: 2.821571]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:33 step:26133 [D loss: 0.391893, acc.: 86.72%] [G loss: 3.854807]\n",
      "epoch:33 step:26134 [D loss: 0.296271, acc.: 85.94%] [G loss: 3.076030]\n",
      "epoch:33 step:26135 [D loss: 0.304695, acc.: 85.16%] [G loss: 3.224377]\n",
      "epoch:33 step:26136 [D loss: 0.406412, acc.: 80.47%] [G loss: 3.592395]\n",
      "epoch:33 step:26137 [D loss: 0.398439, acc.: 78.12%] [G loss: 2.833333]\n",
      "epoch:33 step:26138 [D loss: 0.321617, acc.: 86.72%] [G loss: 2.647171]\n",
      "epoch:33 step:26139 [D loss: 0.389916, acc.: 81.25%] [G loss: 3.058817]\n",
      "epoch:33 step:26140 [D loss: 0.293995, acc.: 87.50%] [G loss: 3.252856]\n",
      "epoch:33 step:26141 [D loss: 0.343650, acc.: 85.94%] [G loss: 3.255637]\n",
      "epoch:33 step:26142 [D loss: 0.360184, acc.: 83.59%] [G loss: 3.291682]\n",
      "epoch:33 step:26143 [D loss: 0.195174, acc.: 92.97%] [G loss: 4.048838]\n",
      "epoch:33 step:26144 [D loss: 0.330184, acc.: 86.72%] [G loss: 3.254522]\n",
      "epoch:33 step:26145 [D loss: 0.279090, acc.: 88.28%] [G loss: 3.132021]\n",
      "epoch:33 step:26146 [D loss: 0.323839, acc.: 82.81%] [G loss: 2.953398]\n",
      "epoch:33 step:26147 [D loss: 0.405494, acc.: 81.25%] [G loss: 2.529977]\n",
      "epoch:33 step:26148 [D loss: 0.258612, acc.: 90.62%] [G loss: 5.250607]\n",
      "epoch:33 step:26149 [D loss: 0.355583, acc.: 80.47%] [G loss: 2.812479]\n",
      "epoch:33 step:26150 [D loss: 0.263843, acc.: 87.50%] [G loss: 4.146413]\n",
      "epoch:33 step:26151 [D loss: 0.306451, acc.: 83.59%] [G loss: 3.435323]\n",
      "epoch:33 step:26152 [D loss: 0.314846, acc.: 87.50%] [G loss: 2.938639]\n",
      "epoch:33 step:26153 [D loss: 0.278120, acc.: 88.28%] [G loss: 3.032679]\n",
      "epoch:33 step:26154 [D loss: 0.226959, acc.: 89.84%] [G loss: 2.617817]\n",
      "epoch:33 step:26155 [D loss: 0.279872, acc.: 88.28%] [G loss: 2.699641]\n",
      "epoch:33 step:26156 [D loss: 0.291484, acc.: 85.94%] [G loss: 3.369902]\n",
      "epoch:33 step:26157 [D loss: 0.285221, acc.: 85.94%] [G loss: 3.208165]\n",
      "epoch:33 step:26158 [D loss: 0.361614, acc.: 79.69%] [G loss: 2.466012]\n",
      "epoch:33 step:26159 [D loss: 0.430612, acc.: 78.12%] [G loss: 2.596669]\n",
      "epoch:33 step:26160 [D loss: 0.307239, acc.: 86.72%] [G loss: 2.967356]\n",
      "epoch:33 step:26161 [D loss: 0.428709, acc.: 83.59%] [G loss: 2.832934]\n",
      "epoch:33 step:26162 [D loss: 0.302087, acc.: 87.50%] [G loss: 3.136080]\n",
      "epoch:33 step:26163 [D loss: 0.348827, acc.: 81.25%] [G loss: 3.733544]\n",
      "epoch:33 step:26164 [D loss: 0.420343, acc.: 78.12%] [G loss: 2.676135]\n",
      "epoch:33 step:26165 [D loss: 0.252747, acc.: 90.62%] [G loss: 3.294086]\n",
      "epoch:33 step:26166 [D loss: 0.306791, acc.: 88.28%] [G loss: 2.753633]\n",
      "epoch:33 step:26167 [D loss: 0.386849, acc.: 85.94%] [G loss: 5.338036]\n",
      "epoch:33 step:26168 [D loss: 0.406181, acc.: 84.38%] [G loss: 4.902863]\n",
      "epoch:33 step:26169 [D loss: 0.323025, acc.: 89.06%] [G loss: 3.264543]\n",
      "epoch:33 step:26170 [D loss: 0.419740, acc.: 78.91%] [G loss: 3.994818]\n",
      "epoch:33 step:26171 [D loss: 0.420819, acc.: 77.34%] [G loss: 3.081691]\n",
      "epoch:33 step:26172 [D loss: 0.320840, acc.: 85.94%] [G loss: 3.652250]\n",
      "epoch:33 step:26173 [D loss: 0.354977, acc.: 82.03%] [G loss: 2.248322]\n",
      "epoch:33 step:26174 [D loss: 0.288429, acc.: 89.06%] [G loss: 3.384005]\n",
      "epoch:33 step:26175 [D loss: 0.306211, acc.: 85.94%] [G loss: 2.993665]\n",
      "epoch:33 step:26176 [D loss: 0.330652, acc.: 82.03%] [G loss: 3.976359]\n",
      "epoch:33 step:26177 [D loss: 0.287111, acc.: 87.50%] [G loss: 3.862681]\n",
      "epoch:33 step:26178 [D loss: 0.351571, acc.: 82.03%] [G loss: 3.664682]\n",
      "epoch:33 step:26179 [D loss: 0.444095, acc.: 74.22%] [G loss: 2.370203]\n",
      "epoch:33 step:26180 [D loss: 0.365920, acc.: 84.38%] [G loss: 3.092573]\n",
      "epoch:33 step:26181 [D loss: 0.351873, acc.: 85.16%] [G loss: 2.301196]\n",
      "epoch:33 step:26182 [D loss: 0.249605, acc.: 92.97%] [G loss: 2.722165]\n",
      "epoch:33 step:26183 [D loss: 0.318234, acc.: 86.72%] [G loss: 2.706645]\n",
      "epoch:33 step:26184 [D loss: 0.324351, acc.: 87.50%] [G loss: 3.923501]\n",
      "epoch:33 step:26185 [D loss: 0.217580, acc.: 89.84%] [G loss: 6.812194]\n",
      "epoch:33 step:26186 [D loss: 0.283335, acc.: 87.50%] [G loss: 6.636517]\n",
      "epoch:33 step:26187 [D loss: 0.275411, acc.: 86.72%] [G loss: 4.510138]\n",
      "epoch:33 step:26188 [D loss: 0.207197, acc.: 92.19%] [G loss: 4.630099]\n",
      "epoch:33 step:26189 [D loss: 0.262828, acc.: 87.50%] [G loss: 3.273745]\n",
      "epoch:33 step:26190 [D loss: 0.286673, acc.: 86.72%] [G loss: 5.159484]\n",
      "epoch:33 step:26191 [D loss: 0.296284, acc.: 88.28%] [G loss: 3.870535]\n",
      "epoch:33 step:26192 [D loss: 0.283822, acc.: 89.84%] [G loss: 4.771972]\n",
      "epoch:33 step:26193 [D loss: 0.278662, acc.: 87.50%] [G loss: 3.859113]\n",
      "epoch:33 step:26194 [D loss: 0.198596, acc.: 89.84%] [G loss: 4.104561]\n",
      "epoch:33 step:26195 [D loss: 0.298860, acc.: 85.16%] [G loss: 2.839719]\n",
      "epoch:33 step:26196 [D loss: 0.310230, acc.: 84.38%] [G loss: 4.091916]\n",
      "epoch:33 step:26197 [D loss: 0.387474, acc.: 83.59%] [G loss: 2.981751]\n",
      "epoch:33 step:26198 [D loss: 0.276280, acc.: 89.06%] [G loss: 2.878355]\n",
      "epoch:33 step:26199 [D loss: 0.328092, acc.: 87.50%] [G loss: 2.679852]\n",
      "epoch:33 step:26200 [D loss: 0.322328, acc.: 82.81%] [G loss: 2.904141]\n",
      "##############\n",
      "[0.86807953 0.86848249 0.8326608  0.82115478 0.76435883 0.81521213\n",
      " 0.84974207 0.84952737 0.80835717 0.82150827]\n",
      "##########\n",
      "epoch:33 step:26201 [D loss: 0.408131, acc.: 77.34%] [G loss: 2.369255]\n",
      "epoch:33 step:26202 [D loss: 0.350917, acc.: 82.81%] [G loss: 2.553427]\n",
      "epoch:33 step:26203 [D loss: 0.408398, acc.: 81.25%] [G loss: 2.966524]\n",
      "epoch:33 step:26204 [D loss: 0.326165, acc.: 84.38%] [G loss: 2.858222]\n",
      "epoch:33 step:26205 [D loss: 0.374831, acc.: 82.81%] [G loss: 2.616436]\n",
      "epoch:33 step:26206 [D loss: 0.227782, acc.: 89.84%] [G loss: 3.396907]\n",
      "epoch:33 step:26207 [D loss: 0.344886, acc.: 82.81%] [G loss: 2.906696]\n",
      "epoch:33 step:26208 [D loss: 0.271265, acc.: 87.50%] [G loss: 4.669926]\n",
      "epoch:33 step:26209 [D loss: 0.404266, acc.: 83.59%] [G loss: 2.687232]\n",
      "epoch:33 step:26210 [D loss: 0.489490, acc.: 79.69%] [G loss: 2.733809]\n",
      "epoch:33 step:26211 [D loss: 0.389231, acc.: 83.59%] [G loss: 3.859257]\n",
      "epoch:33 step:26212 [D loss: 0.424556, acc.: 79.69%] [G loss: 3.131408]\n",
      "epoch:33 step:26213 [D loss: 0.237204, acc.: 85.94%] [G loss: 7.169964]\n",
      "epoch:33 step:26214 [D loss: 0.297964, acc.: 85.16%] [G loss: 4.273201]\n",
      "epoch:33 step:26215 [D loss: 0.271030, acc.: 85.16%] [G loss: 5.663915]\n",
      "epoch:33 step:26216 [D loss: 0.158270, acc.: 92.97%] [G loss: 4.563801]\n",
      "epoch:33 step:26217 [D loss: 0.297166, acc.: 85.16%] [G loss: 3.242233]\n",
      "epoch:33 step:26218 [D loss: 0.326748, acc.: 84.38%] [G loss: 3.710186]\n",
      "epoch:33 step:26219 [D loss: 0.259822, acc.: 88.28%] [G loss: 3.963149]\n",
      "epoch:33 step:26220 [D loss: 0.331554, acc.: 82.81%] [G loss: 2.608411]\n",
      "epoch:33 step:26221 [D loss: 0.283409, acc.: 89.84%] [G loss: 3.495337]\n",
      "epoch:33 step:26222 [D loss: 0.337295, acc.: 86.72%] [G loss: 4.618039]\n",
      "epoch:33 step:26223 [D loss: 0.388872, acc.: 82.81%] [G loss: 2.884577]\n",
      "epoch:33 step:26224 [D loss: 0.242717, acc.: 87.50%] [G loss: 4.579975]\n",
      "epoch:33 step:26225 [D loss: 0.294052, acc.: 89.06%] [G loss: 3.618554]\n",
      "epoch:33 step:26226 [D loss: 0.295811, acc.: 86.72%] [G loss: 3.210338]\n",
      "epoch:33 step:26227 [D loss: 0.242178, acc.: 89.06%] [G loss: 3.736883]\n",
      "epoch:33 step:26228 [D loss: 0.264877, acc.: 88.28%] [G loss: 3.094422]\n",
      "epoch:33 step:26229 [D loss: 0.363293, acc.: 86.72%] [G loss: 3.056428]\n",
      "epoch:33 step:26230 [D loss: 0.325553, acc.: 87.50%] [G loss: 2.894067]\n",
      "epoch:33 step:26231 [D loss: 0.259123, acc.: 89.84%] [G loss: 3.666929]\n",
      "epoch:33 step:26232 [D loss: 0.414004, acc.: 77.34%] [G loss: 2.905469]\n",
      "epoch:33 step:26233 [D loss: 0.421003, acc.: 78.91%] [G loss: 2.831667]\n",
      "epoch:33 step:26234 [D loss: 0.245519, acc.: 90.62%] [G loss: 3.464200]\n",
      "epoch:33 step:26235 [D loss: 0.322367, acc.: 83.59%] [G loss: 3.009700]\n",
      "epoch:33 step:26236 [D loss: 0.375657, acc.: 82.81%] [G loss: 3.113345]\n",
      "epoch:33 step:26237 [D loss: 0.227714, acc.: 90.62%] [G loss: 4.509301]\n",
      "epoch:33 step:26238 [D loss: 0.322557, acc.: 85.94%] [G loss: 3.641460]\n",
      "epoch:33 step:26239 [D loss: 0.277057, acc.: 89.84%] [G loss: 3.011356]\n",
      "epoch:33 step:26240 [D loss: 0.280223, acc.: 86.72%] [G loss: 3.033937]\n",
      "epoch:33 step:26241 [D loss: 0.345465, acc.: 85.16%] [G loss: 3.245811]\n",
      "epoch:33 step:26242 [D loss: 0.256638, acc.: 88.28%] [G loss: 4.249551]\n",
      "epoch:33 step:26243 [D loss: 0.188869, acc.: 91.41%] [G loss: 3.085979]\n",
      "epoch:33 step:26244 [D loss: 0.247183, acc.: 88.28%] [G loss: 4.377965]\n",
      "epoch:33 step:26245 [D loss: 0.339548, acc.: 85.16%] [G loss: 3.876496]\n",
      "epoch:33 step:26246 [D loss: 0.265812, acc.: 85.16%] [G loss: 3.938056]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:33 step:26247 [D loss: 0.252906, acc.: 88.28%] [G loss: 2.891007]\n",
      "epoch:33 step:26248 [D loss: 0.355264, acc.: 83.59%] [G loss: 3.999797]\n",
      "epoch:33 step:26249 [D loss: 0.319677, acc.: 84.38%] [G loss: 2.425845]\n",
      "epoch:33 step:26250 [D loss: 0.309603, acc.: 87.50%] [G loss: 3.454913]\n",
      "epoch:33 step:26251 [D loss: 0.316250, acc.: 87.50%] [G loss: 3.755423]\n",
      "epoch:33 step:26252 [D loss: 0.300924, acc.: 85.16%] [G loss: 3.241942]\n",
      "epoch:33 step:26253 [D loss: 0.300085, acc.: 85.94%] [G loss: 2.710619]\n",
      "epoch:33 step:26254 [D loss: 0.265350, acc.: 89.84%] [G loss: 2.782189]\n",
      "epoch:33 step:26255 [D loss: 0.423110, acc.: 78.12%] [G loss: 2.667785]\n",
      "epoch:33 step:26256 [D loss: 0.333976, acc.: 86.72%] [G loss: 1.918594]\n",
      "epoch:33 step:26257 [D loss: 0.322304, acc.: 85.16%] [G loss: 3.068454]\n",
      "epoch:33 step:26258 [D loss: 0.310121, acc.: 86.72%] [G loss: 3.850961]\n",
      "epoch:33 step:26259 [D loss: 0.386769, acc.: 82.03%] [G loss: 3.258209]\n",
      "epoch:33 step:26260 [D loss: 0.339748, acc.: 84.38%] [G loss: 3.769276]\n",
      "epoch:33 step:26261 [D loss: 0.377450, acc.: 82.81%] [G loss: 2.854749]\n",
      "epoch:33 step:26262 [D loss: 0.396975, acc.: 81.25%] [G loss: 3.396633]\n",
      "epoch:33 step:26263 [D loss: 0.367695, acc.: 85.16%] [G loss: 2.562934]\n",
      "epoch:33 step:26264 [D loss: 0.316877, acc.: 86.72%] [G loss: 2.691811]\n",
      "epoch:33 step:26265 [D loss: 0.308615, acc.: 86.72%] [G loss: 3.641258]\n",
      "epoch:33 step:26266 [D loss: 0.363934, acc.: 82.81%] [G loss: 3.355798]\n",
      "epoch:33 step:26267 [D loss: 0.386321, acc.: 82.81%] [G loss: 4.052474]\n",
      "epoch:33 step:26268 [D loss: 0.227769, acc.: 90.62%] [G loss: 3.940211]\n",
      "epoch:33 step:26269 [D loss: 0.264453, acc.: 90.62%] [G loss: 3.421510]\n",
      "epoch:33 step:26270 [D loss: 0.307164, acc.: 84.38%] [G loss: 5.725214]\n",
      "epoch:33 step:26271 [D loss: 0.235757, acc.: 90.62%] [G loss: 5.188446]\n",
      "epoch:33 step:26272 [D loss: 0.227642, acc.: 90.62%] [G loss: 8.363430]\n",
      "epoch:33 step:26273 [D loss: 0.210288, acc.: 89.84%] [G loss: 5.584980]\n",
      "epoch:33 step:26274 [D loss: 0.269281, acc.: 88.28%] [G loss: 4.135839]\n",
      "epoch:33 step:26275 [D loss: 0.313131, acc.: 84.38%] [G loss: 3.932048]\n",
      "epoch:33 step:26276 [D loss: 0.185205, acc.: 92.19%] [G loss: 5.332910]\n",
      "epoch:33 step:26277 [D loss: 0.331718, acc.: 87.50%] [G loss: 3.219515]\n",
      "epoch:33 step:26278 [D loss: 0.383802, acc.: 79.69%] [G loss: 4.104193]\n",
      "epoch:33 step:26279 [D loss: 0.410321, acc.: 83.59%] [G loss: 3.106005]\n",
      "epoch:33 step:26280 [D loss: 0.344630, acc.: 84.38%] [G loss: 4.284584]\n",
      "epoch:33 step:26281 [D loss: 0.420948, acc.: 83.59%] [G loss: 5.190001]\n",
      "epoch:33 step:26282 [D loss: 0.271613, acc.: 89.06%] [G loss: 3.972977]\n",
      "epoch:33 step:26283 [D loss: 0.311169, acc.: 85.16%] [G loss: 3.927738]\n",
      "epoch:33 step:26284 [D loss: 0.307584, acc.: 87.50%] [G loss: 4.689156]\n",
      "epoch:33 step:26285 [D loss: 0.244987, acc.: 91.41%] [G loss: 3.752094]\n",
      "epoch:33 step:26286 [D loss: 0.472023, acc.: 78.12%] [G loss: 2.841596]\n",
      "epoch:33 step:26287 [D loss: 0.368431, acc.: 82.81%] [G loss: 4.371523]\n",
      "epoch:33 step:26288 [D loss: 0.366722, acc.: 82.81%] [G loss: 3.301728]\n",
      "epoch:33 step:26289 [D loss: 0.349286, acc.: 81.25%] [G loss: 2.562459]\n",
      "epoch:33 step:26290 [D loss: 0.328913, acc.: 84.38%] [G loss: 2.551010]\n",
      "epoch:33 step:26291 [D loss: 0.257940, acc.: 89.84%] [G loss: 3.955383]\n",
      "epoch:33 step:26292 [D loss: 0.266625, acc.: 86.72%] [G loss: 3.069890]\n",
      "epoch:33 step:26293 [D loss: 0.448538, acc.: 76.56%] [G loss: 4.559492]\n",
      "epoch:33 step:26294 [D loss: 0.529910, acc.: 77.34%] [G loss: 3.793277]\n",
      "epoch:33 step:26295 [D loss: 0.259912, acc.: 90.62%] [G loss: 3.982416]\n",
      "epoch:33 step:26296 [D loss: 0.431985, acc.: 75.00%] [G loss: 2.589895]\n",
      "epoch:33 step:26297 [D loss: 0.357880, acc.: 81.25%] [G loss: 2.871312]\n",
      "epoch:33 step:26298 [D loss: 0.406840, acc.: 82.81%] [G loss: 2.868205]\n",
      "epoch:33 step:26299 [D loss: 0.394134, acc.: 84.38%] [G loss: 2.452826]\n",
      "epoch:33 step:26300 [D loss: 0.279033, acc.: 88.28%] [G loss: 2.352256]\n",
      "epoch:33 step:26301 [D loss: 0.359212, acc.: 85.94%] [G loss: 2.474636]\n",
      "epoch:33 step:26302 [D loss: 0.326948, acc.: 86.72%] [G loss: 2.801618]\n",
      "epoch:33 step:26303 [D loss: 0.330009, acc.: 85.16%] [G loss: 4.187692]\n",
      "epoch:33 step:26304 [D loss: 0.294350, acc.: 87.50%] [G loss: 4.821100]\n",
      "epoch:33 step:26305 [D loss: 0.317579, acc.: 87.50%] [G loss: 3.244674]\n",
      "epoch:33 step:26306 [D loss: 0.411850, acc.: 81.25%] [G loss: 4.070117]\n",
      "epoch:33 step:26307 [D loss: 0.262619, acc.: 90.62%] [G loss: 2.990248]\n",
      "epoch:33 step:26308 [D loss: 0.338343, acc.: 86.72%] [G loss: 4.635882]\n",
      "epoch:33 step:26309 [D loss: 0.361209, acc.: 81.25%] [G loss: 3.287933]\n",
      "epoch:33 step:26310 [D loss: 0.422377, acc.: 78.12%] [G loss: 3.041922]\n",
      "epoch:33 step:26311 [D loss: 0.384606, acc.: 84.38%] [G loss: 4.630857]\n",
      "epoch:33 step:26312 [D loss: 0.400012, acc.: 82.81%] [G loss: 4.136991]\n",
      "epoch:33 step:26313 [D loss: 0.193254, acc.: 92.19%] [G loss: 3.368486]\n",
      "epoch:33 step:26314 [D loss: 0.304667, acc.: 86.72%] [G loss: 3.427579]\n",
      "epoch:33 step:26315 [D loss: 0.333641, acc.: 83.59%] [G loss: 2.822104]\n",
      "epoch:33 step:26316 [D loss: 0.256886, acc.: 89.84%] [G loss: 3.669151]\n",
      "epoch:33 step:26317 [D loss: 0.388700, acc.: 81.25%] [G loss: 3.152323]\n",
      "epoch:33 step:26318 [D loss: 0.309507, acc.: 85.94%] [G loss: 3.178607]\n",
      "epoch:33 step:26319 [D loss: 0.272067, acc.: 89.06%] [G loss: 2.515162]\n",
      "epoch:33 step:26320 [D loss: 0.334650, acc.: 82.03%] [G loss: 2.744695]\n",
      "epoch:33 step:26321 [D loss: 0.271978, acc.: 87.50%] [G loss: 3.030149]\n",
      "epoch:33 step:26322 [D loss: 0.386059, acc.: 80.47%] [G loss: 2.711175]\n",
      "epoch:33 step:26323 [D loss: 0.384556, acc.: 83.59%] [G loss: 3.477148]\n",
      "epoch:33 step:26324 [D loss: 0.283872, acc.: 85.94%] [G loss: 3.752390]\n",
      "epoch:33 step:26325 [D loss: 0.452856, acc.: 78.91%] [G loss: 3.394281]\n",
      "epoch:33 step:26326 [D loss: 0.278746, acc.: 89.06%] [G loss: 3.992370]\n",
      "epoch:33 step:26327 [D loss: 0.313875, acc.: 87.50%] [G loss: 4.007438]\n",
      "epoch:33 step:26328 [D loss: 0.321611, acc.: 85.94%] [G loss: 2.272563]\n",
      "epoch:33 step:26329 [D loss: 0.333897, acc.: 86.72%] [G loss: 4.300818]\n",
      "epoch:33 step:26330 [D loss: 0.324292, acc.: 82.81%] [G loss: 3.453375]\n",
      "epoch:33 step:26331 [D loss: 0.234722, acc.: 92.97%] [G loss: 2.181746]\n",
      "epoch:33 step:26332 [D loss: 0.290315, acc.: 88.28%] [G loss: 3.536716]\n",
      "epoch:33 step:26333 [D loss: 0.303767, acc.: 85.16%] [G loss: 2.977614]\n",
      "epoch:33 step:26334 [D loss: 0.307536, acc.: 82.81%] [G loss: 2.855413]\n",
      "epoch:33 step:26335 [D loss: 0.289629, acc.: 84.38%] [G loss: 3.073838]\n",
      "epoch:33 step:26336 [D loss: 0.271071, acc.: 89.84%] [G loss: 3.508366]\n",
      "epoch:33 step:26337 [D loss: 0.273647, acc.: 87.50%] [G loss: 3.059435]\n",
      "epoch:33 step:26338 [D loss: 0.284068, acc.: 88.28%] [G loss: 3.594134]\n",
      "epoch:33 step:26339 [D loss: 0.321877, acc.: 84.38%] [G loss: 3.290840]\n",
      "epoch:33 step:26340 [D loss: 0.261671, acc.: 89.84%] [G loss: 2.848046]\n",
      "epoch:33 step:26341 [D loss: 0.430345, acc.: 82.03%] [G loss: 4.116798]\n",
      "epoch:33 step:26342 [D loss: 0.391533, acc.: 78.91%] [G loss: 3.908872]\n",
      "epoch:33 step:26343 [D loss: 0.280055, acc.: 87.50%] [G loss: 2.973780]\n",
      "epoch:33 step:26344 [D loss: 0.263248, acc.: 84.38%] [G loss: 2.511812]\n",
      "epoch:33 step:26345 [D loss: 0.307108, acc.: 84.38%] [G loss: 3.127450]\n",
      "epoch:33 step:26346 [D loss: 0.316732, acc.: 85.16%] [G loss: 2.866680]\n",
      "epoch:33 step:26347 [D loss: 0.334154, acc.: 83.59%] [G loss: 2.580226]\n",
      "epoch:33 step:26348 [D loss: 0.318400, acc.: 86.72%] [G loss: 2.544259]\n",
      "epoch:33 step:26349 [D loss: 0.329118, acc.: 84.38%] [G loss: 3.100983]\n",
      "epoch:33 step:26350 [D loss: 0.327814, acc.: 86.72%] [G loss: 2.946589]\n",
      "epoch:33 step:26351 [D loss: 0.267312, acc.: 87.50%] [G loss: 3.573128]\n",
      "epoch:33 step:26352 [D loss: 0.380320, acc.: 83.59%] [G loss: 2.647116]\n",
      "epoch:33 step:26353 [D loss: 0.265149, acc.: 89.84%] [G loss: 2.879433]\n",
      "epoch:33 step:26354 [D loss: 0.226631, acc.: 90.62%] [G loss: 3.495179]\n",
      "epoch:33 step:26355 [D loss: 0.253510, acc.: 87.50%] [G loss: 3.230509]\n",
      "epoch:33 step:26356 [D loss: 0.367250, acc.: 83.59%] [G loss: 3.656269]\n",
      "epoch:33 step:26357 [D loss: 0.330524, acc.: 82.81%] [G loss: 2.603931]\n",
      "epoch:33 step:26358 [D loss: 0.231432, acc.: 90.62%] [G loss: 4.313872]\n",
      "epoch:33 step:26359 [D loss: 0.321408, acc.: 84.38%] [G loss: 4.536565]\n",
      "epoch:33 step:26360 [D loss: 0.337278, acc.: 86.72%] [G loss: 3.958226]\n",
      "epoch:33 step:26361 [D loss: 0.307582, acc.: 89.06%] [G loss: 4.151922]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:33 step:26362 [D loss: 0.321031, acc.: 84.38%] [G loss: 3.851921]\n",
      "epoch:33 step:26363 [D loss: 0.359530, acc.: 82.81%] [G loss: 5.862807]\n",
      "epoch:33 step:26364 [D loss: 0.485599, acc.: 73.44%] [G loss: 5.093324]\n",
      "epoch:33 step:26365 [D loss: 0.493971, acc.: 73.44%] [G loss: 2.113763]\n",
      "epoch:33 step:26366 [D loss: 0.249780, acc.: 89.84%] [G loss: 4.336398]\n",
      "epoch:33 step:26367 [D loss: 0.382056, acc.: 83.59%] [G loss: 4.073359]\n",
      "epoch:33 step:26368 [D loss: 0.385274, acc.: 84.38%] [G loss: 4.354434]\n",
      "epoch:33 step:26369 [D loss: 0.258798, acc.: 90.62%] [G loss: 2.891988]\n",
      "epoch:33 step:26370 [D loss: 0.346897, acc.: 83.59%] [G loss: 3.521553]\n",
      "epoch:33 step:26371 [D loss: 0.255636, acc.: 85.16%] [G loss: 3.391798]\n",
      "epoch:33 step:26372 [D loss: 0.356328, acc.: 81.25%] [G loss: 3.666533]\n",
      "epoch:33 step:26373 [D loss: 0.272815, acc.: 87.50%] [G loss: 2.473304]\n",
      "epoch:33 step:26374 [D loss: 0.254754, acc.: 88.28%] [G loss: 2.384279]\n",
      "epoch:33 step:26375 [D loss: 0.298628, acc.: 87.50%] [G loss: 3.446403]\n",
      "epoch:33 step:26376 [D loss: 0.317697, acc.: 84.38%] [G loss: 3.007541]\n",
      "epoch:33 step:26377 [D loss: 0.314370, acc.: 85.16%] [G loss: 3.502506]\n",
      "epoch:33 step:26378 [D loss: 0.321201, acc.: 84.38%] [G loss: 4.023727]\n",
      "epoch:33 step:26379 [D loss: 0.326195, acc.: 84.38%] [G loss: 3.074722]\n",
      "epoch:33 step:26380 [D loss: 0.286845, acc.: 85.94%] [G loss: 5.211203]\n",
      "epoch:33 step:26381 [D loss: 0.225825, acc.: 89.84%] [G loss: 3.801528]\n",
      "epoch:33 step:26382 [D loss: 0.295502, acc.: 85.16%] [G loss: 5.267342]\n",
      "epoch:33 step:26383 [D loss: 0.273644, acc.: 89.06%] [G loss: 3.316430]\n",
      "epoch:33 step:26384 [D loss: 0.262931, acc.: 88.28%] [G loss: 3.508713]\n",
      "epoch:33 step:26385 [D loss: 0.292144, acc.: 91.41%] [G loss: 3.851289]\n",
      "epoch:33 step:26386 [D loss: 0.323525, acc.: 85.16%] [G loss: 3.118401]\n",
      "epoch:33 step:26387 [D loss: 0.278971, acc.: 89.84%] [G loss: 3.852433]\n",
      "epoch:33 step:26388 [D loss: 0.313846, acc.: 83.59%] [G loss: 3.251212]\n",
      "epoch:33 step:26389 [D loss: 0.356995, acc.: 80.47%] [G loss: 3.165206]\n",
      "epoch:33 step:26390 [D loss: 0.325348, acc.: 85.16%] [G loss: 3.949913]\n",
      "epoch:33 step:26391 [D loss: 0.340581, acc.: 84.38%] [G loss: 3.589315]\n",
      "epoch:33 step:26392 [D loss: 0.272528, acc.: 87.50%] [G loss: 3.477936]\n",
      "epoch:33 step:26393 [D loss: 0.345722, acc.: 86.72%] [G loss: 2.605887]\n",
      "epoch:33 step:26394 [D loss: 0.286548, acc.: 87.50%] [G loss: 2.467669]\n",
      "epoch:33 step:26395 [D loss: 0.321203, acc.: 86.72%] [G loss: 2.769950]\n",
      "epoch:33 step:26396 [D loss: 0.387097, acc.: 80.47%] [G loss: 2.743599]\n",
      "epoch:33 step:26397 [D loss: 0.337340, acc.: 84.38%] [G loss: 2.852900]\n",
      "epoch:33 step:26398 [D loss: 0.327381, acc.: 86.72%] [G loss: 3.737902]\n",
      "epoch:33 step:26399 [D loss: 0.225789, acc.: 89.06%] [G loss: 3.364113]\n",
      "epoch:33 step:26400 [D loss: 0.375912, acc.: 83.59%] [G loss: 5.463114]\n",
      "##############\n",
      "[0.86428583 0.84203034 0.80594913 0.79902502 0.75404834 0.83128531\n",
      " 0.87374843 0.82342713 0.80484188 0.82297235]\n",
      "##########\n",
      "epoch:33 step:26401 [D loss: 0.381602, acc.: 82.81%] [G loss: 2.968172]\n",
      "epoch:33 step:26402 [D loss: 0.226614, acc.: 92.19%] [G loss: 4.291716]\n",
      "epoch:33 step:26403 [D loss: 0.245647, acc.: 89.06%] [G loss: 3.444409]\n",
      "epoch:33 step:26404 [D loss: 0.284860, acc.: 88.28%] [G loss: 4.249822]\n",
      "epoch:33 step:26405 [D loss: 0.349213, acc.: 85.94%] [G loss: 4.163707]\n",
      "epoch:33 step:26406 [D loss: 0.488636, acc.: 78.12%] [G loss: 7.617056]\n",
      "epoch:33 step:26407 [D loss: 1.497554, acc.: 58.59%] [G loss: 8.392414]\n",
      "epoch:33 step:26408 [D loss: 2.199225, acc.: 67.19%] [G loss: 4.732785]\n",
      "epoch:33 step:26409 [D loss: 0.323844, acc.: 85.94%] [G loss: 5.457441]\n",
      "epoch:33 step:26410 [D loss: 0.410918, acc.: 83.59%] [G loss: 3.197874]\n",
      "epoch:33 step:26411 [D loss: 0.324917, acc.: 85.94%] [G loss: 3.072391]\n",
      "epoch:33 step:26412 [D loss: 0.452712, acc.: 78.12%] [G loss: 3.139471]\n",
      "epoch:33 step:26413 [D loss: 0.281491, acc.: 87.50%] [G loss: 3.065780]\n",
      "epoch:33 step:26414 [D loss: 0.256025, acc.: 86.72%] [G loss: 3.915430]\n",
      "epoch:33 step:26415 [D loss: 0.298167, acc.: 89.06%] [G loss: 2.847471]\n",
      "epoch:33 step:26416 [D loss: 0.337724, acc.: 83.59%] [G loss: 3.518477]\n",
      "epoch:33 step:26417 [D loss: 0.238174, acc.: 89.06%] [G loss: 3.861860]\n",
      "epoch:33 step:26418 [D loss: 0.261651, acc.: 89.84%] [G loss: 2.681700]\n",
      "epoch:33 step:26419 [D loss: 0.413683, acc.: 82.03%] [G loss: 2.534002]\n",
      "epoch:33 step:26420 [D loss: 0.257795, acc.: 92.19%] [G loss: 2.844704]\n",
      "epoch:33 step:26421 [D loss: 0.346488, acc.: 85.16%] [G loss: 3.768293]\n",
      "epoch:33 step:26422 [D loss: 0.315959, acc.: 88.28%] [G loss: 2.387053]\n",
      "epoch:33 step:26423 [D loss: 0.359009, acc.: 85.94%] [G loss: 3.627501]\n",
      "epoch:33 step:26424 [D loss: 0.402161, acc.: 78.12%] [G loss: 2.524867]\n",
      "epoch:33 step:26425 [D loss: 0.330300, acc.: 83.59%] [G loss: 2.382339]\n",
      "epoch:33 step:26426 [D loss: 0.237285, acc.: 91.41%] [G loss: 3.476106]\n",
      "epoch:33 step:26427 [D loss: 0.258212, acc.: 89.84%] [G loss: 2.547356]\n",
      "epoch:33 step:26428 [D loss: 0.310167, acc.: 85.94%] [G loss: 3.528383]\n",
      "epoch:33 step:26429 [D loss: 0.276600, acc.: 89.84%] [G loss: 2.367700]\n",
      "epoch:33 step:26430 [D loss: 0.311746, acc.: 82.03%] [G loss: 3.205258]\n",
      "epoch:33 step:26431 [D loss: 0.275707, acc.: 88.28%] [G loss: 2.818496]\n",
      "epoch:33 step:26432 [D loss: 0.317324, acc.: 86.72%] [G loss: 3.145956]\n",
      "epoch:33 step:26433 [D loss: 0.297670, acc.: 88.28%] [G loss: 3.277920]\n",
      "epoch:33 step:26434 [D loss: 0.367996, acc.: 84.38%] [G loss: 2.165967]\n",
      "epoch:33 step:26435 [D loss: 0.293982, acc.: 88.28%] [G loss: 2.716363]\n",
      "epoch:33 step:26436 [D loss: 0.312583, acc.: 88.28%] [G loss: 2.507643]\n",
      "epoch:33 step:26437 [D loss: 0.237710, acc.: 92.97%] [G loss: 2.661467]\n",
      "epoch:33 step:26438 [D loss: 0.287972, acc.: 86.72%] [G loss: 2.718241]\n",
      "epoch:33 step:26439 [D loss: 0.295521, acc.: 89.84%] [G loss: 2.393032]\n",
      "epoch:33 step:26440 [D loss: 0.244738, acc.: 92.19%] [G loss: 2.566890]\n",
      "epoch:33 step:26441 [D loss: 0.306439, acc.: 84.38%] [G loss: 3.442370]\n",
      "epoch:33 step:26442 [D loss: 0.353063, acc.: 85.16%] [G loss: 2.838742]\n",
      "epoch:33 step:26443 [D loss: 0.316755, acc.: 85.94%] [G loss: 2.785733]\n",
      "epoch:33 step:26444 [D loss: 0.281670, acc.: 85.16%] [G loss: 3.240405]\n",
      "epoch:33 step:26445 [D loss: 0.281280, acc.: 89.06%] [G loss: 3.259676]\n",
      "epoch:33 step:26446 [D loss: 0.252980, acc.: 89.84%] [G loss: 2.755601]\n",
      "epoch:33 step:26447 [D loss: 0.394165, acc.: 83.59%] [G loss: 3.180236]\n",
      "epoch:33 step:26448 [D loss: 0.328290, acc.: 85.94%] [G loss: 3.368935]\n",
      "epoch:33 step:26449 [D loss: 0.262635, acc.: 86.72%] [G loss: 3.278613]\n",
      "epoch:33 step:26450 [D loss: 0.284069, acc.: 85.94%] [G loss: 2.563550]\n",
      "epoch:33 step:26451 [D loss: 0.306037, acc.: 87.50%] [G loss: 2.556147]\n",
      "epoch:33 step:26452 [D loss: 0.286698, acc.: 85.94%] [G loss: 2.612360]\n",
      "epoch:33 step:26453 [D loss: 0.398763, acc.: 80.47%] [G loss: 3.086737]\n",
      "epoch:33 step:26454 [D loss: 0.263591, acc.: 86.72%] [G loss: 3.382306]\n",
      "epoch:33 step:26455 [D loss: 0.399473, acc.: 80.47%] [G loss: 5.528434]\n",
      "epoch:33 step:26456 [D loss: 0.369142, acc.: 84.38%] [G loss: 5.159091]\n",
      "epoch:33 step:26457 [D loss: 0.326627, acc.: 83.59%] [G loss: 4.003481]\n",
      "epoch:33 step:26458 [D loss: 0.320267, acc.: 87.50%] [G loss: 3.187770]\n",
      "epoch:33 step:26459 [D loss: 0.208962, acc.: 90.62%] [G loss: 3.853536]\n",
      "epoch:33 step:26460 [D loss: 0.303471, acc.: 85.16%] [G loss: 3.868834]\n",
      "epoch:33 step:26461 [D loss: 0.281988, acc.: 89.06%] [G loss: 2.916846]\n",
      "epoch:33 step:26462 [D loss: 0.220083, acc.: 92.19%] [G loss: 3.599896]\n",
      "epoch:33 step:26463 [D loss: 0.252987, acc.: 91.41%] [G loss: 3.834562]\n",
      "epoch:33 step:26464 [D loss: 0.322494, acc.: 86.72%] [G loss: 2.946923]\n",
      "epoch:33 step:26465 [D loss: 0.476954, acc.: 81.25%] [G loss: 2.753317]\n",
      "epoch:33 step:26466 [D loss: 0.236495, acc.: 92.19%] [G loss: 2.377682]\n",
      "epoch:33 step:26467 [D loss: 0.296718, acc.: 85.94%] [G loss: 3.334038]\n",
      "epoch:33 step:26468 [D loss: 0.255594, acc.: 89.06%] [G loss: 2.732512]\n",
      "epoch:33 step:26469 [D loss: 0.325259, acc.: 82.03%] [G loss: 3.182251]\n",
      "epoch:33 step:26470 [D loss: 0.348440, acc.: 81.25%] [G loss: 2.440345]\n",
      "epoch:33 step:26471 [D loss: 0.292992, acc.: 90.62%] [G loss: 2.324925]\n",
      "epoch:33 step:26472 [D loss: 0.306740, acc.: 89.84%] [G loss: 2.500476]\n",
      "epoch:33 step:26473 [D loss: 0.259484, acc.: 89.84%] [G loss: 3.135610]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:33 step:26474 [D loss: 0.388668, acc.: 84.38%] [G loss: 2.389558]\n",
      "epoch:33 step:26475 [D loss: 0.305399, acc.: 85.94%] [G loss: 2.480740]\n",
      "epoch:33 step:26476 [D loss: 0.325098, acc.: 86.72%] [G loss: 2.360467]\n",
      "epoch:33 step:26477 [D loss: 0.313066, acc.: 85.94%] [G loss: 3.028186]\n",
      "epoch:33 step:26478 [D loss: 0.335421, acc.: 82.03%] [G loss: 2.430775]\n",
      "epoch:33 step:26479 [D loss: 0.312228, acc.: 88.28%] [G loss: 3.204486]\n",
      "epoch:33 step:26480 [D loss: 0.374526, acc.: 80.47%] [G loss: 3.572311]\n",
      "epoch:33 step:26481 [D loss: 0.379036, acc.: 80.47%] [G loss: 3.116372]\n",
      "epoch:33 step:26482 [D loss: 0.239723, acc.: 89.84%] [G loss: 2.646768]\n",
      "epoch:33 step:26483 [D loss: 0.395880, acc.: 85.94%] [G loss: 2.628852]\n",
      "epoch:33 step:26484 [D loss: 0.348664, acc.: 84.38%] [G loss: 2.701901]\n",
      "epoch:33 step:26485 [D loss: 0.391656, acc.: 80.47%] [G loss: 2.757461]\n",
      "epoch:33 step:26486 [D loss: 0.373931, acc.: 79.69%] [G loss: 3.736023]\n",
      "epoch:33 step:26487 [D loss: 0.261633, acc.: 89.84%] [G loss: 3.465417]\n",
      "epoch:33 step:26488 [D loss: 0.307799, acc.: 84.38%] [G loss: 3.321022]\n",
      "epoch:33 step:26489 [D loss: 0.308997, acc.: 89.06%] [G loss: 2.712605]\n",
      "epoch:33 step:26490 [D loss: 0.458611, acc.: 85.16%] [G loss: 2.729209]\n",
      "epoch:33 step:26491 [D loss: 0.337447, acc.: 85.16%] [G loss: 2.678630]\n",
      "epoch:33 step:26492 [D loss: 0.283672, acc.: 89.84%] [G loss: 2.836443]\n",
      "epoch:33 step:26493 [D loss: 0.241573, acc.: 90.62%] [G loss: 4.539687]\n",
      "epoch:33 step:26494 [D loss: 0.226923, acc.: 90.62%] [G loss: 4.790305]\n",
      "epoch:33 step:26495 [D loss: 0.216145, acc.: 89.84%] [G loss: 2.695275]\n",
      "epoch:33 step:26496 [D loss: 0.263812, acc.: 86.72%] [G loss: 3.215282]\n",
      "epoch:33 step:26497 [D loss: 0.353032, acc.: 84.38%] [G loss: 2.485594]\n",
      "epoch:33 step:26498 [D loss: 0.337849, acc.: 83.59%] [G loss: 2.743065]\n",
      "epoch:33 step:26499 [D loss: 0.281048, acc.: 89.06%] [G loss: 3.028407]\n",
      "epoch:33 step:26500 [D loss: 0.367421, acc.: 83.59%] [G loss: 3.099528]\n",
      "epoch:33 step:26501 [D loss: 0.424371, acc.: 82.81%] [G loss: 4.713468]\n",
      "epoch:33 step:26502 [D loss: 0.315058, acc.: 88.28%] [G loss: 6.956153]\n",
      "epoch:33 step:26503 [D loss: 0.461597, acc.: 81.25%] [G loss: 3.612829]\n",
      "epoch:33 step:26504 [D loss: 0.283109, acc.: 91.41%] [G loss: 4.083632]\n",
      "epoch:33 step:26505 [D loss: 0.357749, acc.: 82.03%] [G loss: 3.241871]\n",
      "epoch:33 step:26506 [D loss: 0.285567, acc.: 89.06%] [G loss: 4.350520]\n",
      "epoch:33 step:26507 [D loss: 0.307814, acc.: 85.16%] [G loss: 4.175736]\n",
      "epoch:33 step:26508 [D loss: 0.290883, acc.: 87.50%] [G loss: 3.699708]\n",
      "epoch:33 step:26509 [D loss: 0.357385, acc.: 82.81%] [G loss: 2.875738]\n",
      "epoch:33 step:26510 [D loss: 0.238979, acc.: 92.19%] [G loss: 3.648313]\n",
      "epoch:33 step:26511 [D loss: 0.304179, acc.: 87.50%] [G loss: 3.260659]\n",
      "epoch:33 step:26512 [D loss: 0.383171, acc.: 83.59%] [G loss: 3.581970]\n",
      "epoch:33 step:26513 [D loss: 0.403891, acc.: 78.12%] [G loss: 3.952990]\n",
      "epoch:33 step:26514 [D loss: 0.266161, acc.: 88.28%] [G loss: 3.429228]\n",
      "epoch:33 step:26515 [D loss: 0.402699, acc.: 79.69%] [G loss: 5.373626]\n",
      "epoch:33 step:26516 [D loss: 0.272753, acc.: 89.84%] [G loss: 3.618419]\n",
      "epoch:33 step:26517 [D loss: 0.257095, acc.: 89.84%] [G loss: 4.407756]\n",
      "epoch:33 step:26518 [D loss: 0.367110, acc.: 83.59%] [G loss: 2.784721]\n",
      "epoch:33 step:26519 [D loss: 0.224872, acc.: 92.19%] [G loss: 2.954597]\n",
      "epoch:33 step:26520 [D loss: 0.267336, acc.: 89.84%] [G loss: 3.730713]\n",
      "epoch:33 step:26521 [D loss: 0.243157, acc.: 93.75%] [G loss: 3.110074]\n",
      "epoch:33 step:26522 [D loss: 0.334492, acc.: 83.59%] [G loss: 3.794378]\n",
      "epoch:33 step:26523 [D loss: 0.278372, acc.: 89.06%] [G loss: 2.727485]\n",
      "epoch:33 step:26524 [D loss: 0.311394, acc.: 89.06%] [G loss: 2.906578]\n",
      "epoch:33 step:26525 [D loss: 0.267096, acc.: 88.28%] [G loss: 2.936555]\n",
      "epoch:33 step:26526 [D loss: 0.316238, acc.: 87.50%] [G loss: 3.461655]\n",
      "epoch:33 step:26527 [D loss: 0.416128, acc.: 82.81%] [G loss: 2.611447]\n",
      "epoch:33 step:26528 [D loss: 0.380362, acc.: 81.25%] [G loss: 2.585755]\n",
      "epoch:33 step:26529 [D loss: 0.302782, acc.: 87.50%] [G loss: 2.657511]\n",
      "epoch:33 step:26530 [D loss: 0.284908, acc.: 89.06%] [G loss: 2.163712]\n",
      "epoch:33 step:26531 [D loss: 0.431492, acc.: 80.47%] [G loss: 3.089989]\n",
      "epoch:33 step:26532 [D loss: 0.393595, acc.: 78.91%] [G loss: 3.289546]\n",
      "epoch:33 step:26533 [D loss: 0.540300, acc.: 80.47%] [G loss: 8.316462]\n",
      "epoch:33 step:26534 [D loss: 0.762610, acc.: 75.00%] [G loss: 7.465369]\n",
      "epoch:33 step:26535 [D loss: 1.245118, acc.: 64.06%] [G loss: 4.878817]\n",
      "epoch:33 step:26536 [D loss: 1.321549, acc.: 67.19%] [G loss: 8.116797]\n",
      "epoch:33 step:26537 [D loss: 0.622628, acc.: 75.00%] [G loss: 7.814867]\n",
      "epoch:33 step:26538 [D loss: 0.494514, acc.: 73.44%] [G loss: 2.712845]\n",
      "epoch:33 step:26539 [D loss: 0.341560, acc.: 84.38%] [G loss: 3.892665]\n",
      "epoch:33 step:26540 [D loss: 0.225149, acc.: 89.06%] [G loss: 3.331210]\n",
      "epoch:33 step:26541 [D loss: 0.333758, acc.: 85.16%] [G loss: 2.715373]\n",
      "epoch:33 step:26542 [D loss: 0.328969, acc.: 84.38%] [G loss: 3.077856]\n",
      "epoch:33 step:26543 [D loss: 0.324237, acc.: 82.81%] [G loss: 4.135611]\n",
      "epoch:33 step:26544 [D loss: 0.444697, acc.: 75.00%] [G loss: 3.865882]\n",
      "epoch:33 step:26545 [D loss: 0.264676, acc.: 85.94%] [G loss: 4.078105]\n",
      "epoch:33 step:26546 [D loss: 0.248218, acc.: 91.41%] [G loss: 4.021745]\n",
      "epoch:33 step:26547 [D loss: 0.327943, acc.: 85.94%] [G loss: 3.109606]\n",
      "epoch:33 step:26548 [D loss: 0.287017, acc.: 86.72%] [G loss: 3.617262]\n",
      "epoch:33 step:26549 [D loss: 0.231166, acc.: 92.19%] [G loss: 3.643971]\n",
      "epoch:33 step:26550 [D loss: 0.289094, acc.: 84.38%] [G loss: 2.802837]\n",
      "epoch:33 step:26551 [D loss: 0.234444, acc.: 92.97%] [G loss: 3.171465]\n",
      "epoch:33 step:26552 [D loss: 0.368646, acc.: 85.16%] [G loss: 3.718418]\n",
      "epoch:33 step:26553 [D loss: 0.298602, acc.: 87.50%] [G loss: 3.013916]\n",
      "epoch:33 step:26554 [D loss: 0.320707, acc.: 85.16%] [G loss: 3.358294]\n",
      "epoch:34 step:26555 [D loss: 0.313047, acc.: 87.50%] [G loss: 2.870395]\n",
      "epoch:34 step:26556 [D loss: 0.267190, acc.: 86.72%] [G loss: 2.530839]\n",
      "epoch:34 step:26557 [D loss: 0.328279, acc.: 82.81%] [G loss: 3.169845]\n",
      "epoch:34 step:26558 [D loss: 0.311175, acc.: 85.16%] [G loss: 2.916792]\n",
      "epoch:34 step:26559 [D loss: 0.236942, acc.: 89.06%] [G loss: 3.909580]\n",
      "epoch:34 step:26560 [D loss: 0.271426, acc.: 89.06%] [G loss: 3.356759]\n",
      "epoch:34 step:26561 [D loss: 0.269220, acc.: 91.41%] [G loss: 2.888058]\n",
      "epoch:34 step:26562 [D loss: 0.253907, acc.: 90.62%] [G loss: 2.377021]\n",
      "epoch:34 step:26563 [D loss: 0.364989, acc.: 82.03%] [G loss: 2.672210]\n",
      "epoch:34 step:26564 [D loss: 0.442365, acc.: 77.34%] [G loss: 3.103114]\n",
      "epoch:34 step:26565 [D loss: 0.334060, acc.: 85.94%] [G loss: 2.762558]\n",
      "epoch:34 step:26566 [D loss: 0.394390, acc.: 85.16%] [G loss: 2.633249]\n",
      "epoch:34 step:26567 [D loss: 0.364396, acc.: 84.38%] [G loss: 3.156828]\n",
      "epoch:34 step:26568 [D loss: 0.307937, acc.: 81.25%] [G loss: 2.988727]\n",
      "epoch:34 step:26569 [D loss: 0.391816, acc.: 83.59%] [G loss: 2.419681]\n",
      "epoch:34 step:26570 [D loss: 0.343003, acc.: 85.94%] [G loss: 3.094597]\n",
      "epoch:34 step:26571 [D loss: 0.442150, acc.: 80.47%] [G loss: 3.117881]\n",
      "epoch:34 step:26572 [D loss: 0.369849, acc.: 84.38%] [G loss: 3.822447]\n",
      "epoch:34 step:26573 [D loss: 0.243678, acc.: 90.62%] [G loss: 3.885643]\n",
      "epoch:34 step:26574 [D loss: 0.436942, acc.: 75.00%] [G loss: 3.727775]\n",
      "epoch:34 step:26575 [D loss: 0.356911, acc.: 82.03%] [G loss: 2.494781]\n",
      "epoch:34 step:26576 [D loss: 0.329344, acc.: 84.38%] [G loss: 3.929314]\n",
      "epoch:34 step:26577 [D loss: 0.373730, acc.: 87.50%] [G loss: 3.494483]\n",
      "epoch:34 step:26578 [D loss: 0.346646, acc.: 85.94%] [G loss: 3.136514]\n",
      "epoch:34 step:26579 [D loss: 0.292185, acc.: 88.28%] [G loss: 3.047534]\n",
      "epoch:34 step:26580 [D loss: 0.436611, acc.: 82.03%] [G loss: 2.845585]\n",
      "epoch:34 step:26581 [D loss: 0.389138, acc.: 84.38%] [G loss: 2.656602]\n",
      "epoch:34 step:26582 [D loss: 0.366890, acc.: 82.81%] [G loss: 2.903787]\n",
      "epoch:34 step:26583 [D loss: 0.346891, acc.: 85.16%] [G loss: 2.473910]\n",
      "epoch:34 step:26584 [D loss: 0.373997, acc.: 83.59%] [G loss: 3.085134]\n",
      "epoch:34 step:26585 [D loss: 0.360734, acc.: 85.94%] [G loss: 2.455329]\n",
      "epoch:34 step:26586 [D loss: 0.357391, acc.: 85.94%] [G loss: 3.478782]\n",
      "epoch:34 step:26587 [D loss: 0.356871, acc.: 84.38%] [G loss: 4.632369]\n",
      "epoch:34 step:26588 [D loss: 0.351585, acc.: 82.81%] [G loss: 3.252852]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:34 step:26589 [D loss: 0.380930, acc.: 82.03%] [G loss: 3.097410]\n",
      "epoch:34 step:26590 [D loss: 0.231083, acc.: 89.06%] [G loss: 3.456783]\n",
      "epoch:34 step:26591 [D loss: 0.256648, acc.: 89.06%] [G loss: 3.710033]\n",
      "epoch:34 step:26592 [D loss: 0.303752, acc.: 86.72%] [G loss: 3.678399]\n",
      "epoch:34 step:26593 [D loss: 0.246515, acc.: 90.62%] [G loss: 2.559329]\n",
      "epoch:34 step:26594 [D loss: 0.247451, acc.: 89.84%] [G loss: 2.081525]\n",
      "epoch:34 step:26595 [D loss: 0.329046, acc.: 85.94%] [G loss: 2.601813]\n",
      "epoch:34 step:26596 [D loss: 0.254507, acc.: 90.62%] [G loss: 3.378004]\n",
      "epoch:34 step:26597 [D loss: 0.340639, acc.: 85.94%] [G loss: 2.289966]\n",
      "epoch:34 step:26598 [D loss: 0.292291, acc.: 86.72%] [G loss: 3.827613]\n",
      "epoch:34 step:26599 [D loss: 0.376638, acc.: 81.25%] [G loss: 2.734642]\n",
      "epoch:34 step:26600 [D loss: 0.282165, acc.: 85.94%] [G loss: 4.719420]\n",
      "##############\n",
      "[0.88509292 0.85485978 0.81642273 0.81050913 0.77512841 0.84461985\n",
      " 0.86431149 0.83304527 0.8140816  0.79752155]\n",
      "##########\n",
      "epoch:34 step:26601 [D loss: 0.299696, acc.: 89.06%] [G loss: 3.899592]\n",
      "epoch:34 step:26602 [D loss: 0.260114, acc.: 89.06%] [G loss: 3.598182]\n",
      "epoch:34 step:26603 [D loss: 0.298412, acc.: 89.06%] [G loss: 3.067479]\n",
      "epoch:34 step:26604 [D loss: 0.342064, acc.: 84.38%] [G loss: 2.299318]\n",
      "epoch:34 step:26605 [D loss: 0.226375, acc.: 89.84%] [G loss: 5.244706]\n",
      "epoch:34 step:26606 [D loss: 0.279699, acc.: 88.28%] [G loss: 4.165003]\n",
      "epoch:34 step:26607 [D loss: 0.260708, acc.: 89.06%] [G loss: 4.583411]\n",
      "epoch:34 step:26608 [D loss: 0.317777, acc.: 84.38%] [G loss: 2.754078]\n",
      "epoch:34 step:26609 [D loss: 0.239404, acc.: 86.72%] [G loss: 2.904982]\n",
      "epoch:34 step:26610 [D loss: 0.200018, acc.: 91.41%] [G loss: 4.331962]\n",
      "epoch:34 step:26611 [D loss: 0.190039, acc.: 89.84%] [G loss: 3.862703]\n",
      "epoch:34 step:26612 [D loss: 0.312149, acc.: 83.59%] [G loss: 4.016243]\n",
      "epoch:34 step:26613 [D loss: 0.376329, acc.: 86.72%] [G loss: 3.967653]\n",
      "epoch:34 step:26614 [D loss: 0.293583, acc.: 86.72%] [G loss: 4.106843]\n",
      "epoch:34 step:26615 [D loss: 0.277661, acc.: 89.84%] [G loss: 3.068810]\n",
      "epoch:34 step:26616 [D loss: 0.202565, acc.: 92.19%] [G loss: 2.900072]\n",
      "epoch:34 step:26617 [D loss: 0.281896, acc.: 85.16%] [G loss: 3.136488]\n",
      "epoch:34 step:26618 [D loss: 0.260246, acc.: 89.06%] [G loss: 2.363584]\n",
      "epoch:34 step:26619 [D loss: 0.435850, acc.: 76.56%] [G loss: 2.616737]\n",
      "epoch:34 step:26620 [D loss: 0.283309, acc.: 89.06%] [G loss: 2.828154]\n",
      "epoch:34 step:26621 [D loss: 0.370987, acc.: 81.25%] [G loss: 2.884109]\n",
      "epoch:34 step:26622 [D loss: 0.339013, acc.: 86.72%] [G loss: 2.959614]\n",
      "epoch:34 step:26623 [D loss: 0.321979, acc.: 85.16%] [G loss: 2.584778]\n",
      "epoch:34 step:26624 [D loss: 0.297200, acc.: 85.16%] [G loss: 1.921247]\n",
      "epoch:34 step:26625 [D loss: 0.287929, acc.: 93.75%] [G loss: 2.415003]\n",
      "epoch:34 step:26626 [D loss: 0.488466, acc.: 82.03%] [G loss: 2.742903]\n",
      "epoch:34 step:26627 [D loss: 0.348651, acc.: 86.72%] [G loss: 3.160043]\n",
      "epoch:34 step:26628 [D loss: 0.324416, acc.: 86.72%] [G loss: 3.168458]\n",
      "epoch:34 step:26629 [D loss: 0.442401, acc.: 76.56%] [G loss: 3.003328]\n",
      "epoch:34 step:26630 [D loss: 0.302207, acc.: 83.59%] [G loss: 3.145945]\n",
      "epoch:34 step:26631 [D loss: 0.331753, acc.: 85.94%] [G loss: 3.976325]\n",
      "epoch:34 step:26632 [D loss: 0.381647, acc.: 82.03%] [G loss: 4.142440]\n",
      "epoch:34 step:26633 [D loss: 0.284061, acc.: 85.94%] [G loss: 3.956275]\n",
      "epoch:34 step:26634 [D loss: 0.317652, acc.: 84.38%] [G loss: 3.529139]\n",
      "epoch:34 step:26635 [D loss: 0.277666, acc.: 88.28%] [G loss: 3.791373]\n",
      "epoch:34 step:26636 [D loss: 0.242377, acc.: 89.06%] [G loss: 3.158433]\n",
      "epoch:34 step:26637 [D loss: 0.296020, acc.: 85.94%] [G loss: 3.751747]\n",
      "epoch:34 step:26638 [D loss: 0.163375, acc.: 96.88%] [G loss: 3.956810]\n",
      "epoch:34 step:26639 [D loss: 0.328857, acc.: 85.94%] [G loss: 2.923980]\n",
      "epoch:34 step:26640 [D loss: 0.291617, acc.: 86.72%] [G loss: 4.252997]\n",
      "epoch:34 step:26641 [D loss: 0.297850, acc.: 88.28%] [G loss: 3.233986]\n",
      "epoch:34 step:26642 [D loss: 0.363005, acc.: 83.59%] [G loss: 2.907488]\n",
      "epoch:34 step:26643 [D loss: 0.371053, acc.: 84.38%] [G loss: 3.585392]\n",
      "epoch:34 step:26644 [D loss: 0.381131, acc.: 82.03%] [G loss: 3.665535]\n",
      "epoch:34 step:26645 [D loss: 0.297917, acc.: 88.28%] [G loss: 3.054431]\n",
      "epoch:34 step:26646 [D loss: 0.385877, acc.: 83.59%] [G loss: 4.245897]\n",
      "epoch:34 step:26647 [D loss: 0.347215, acc.: 82.81%] [G loss: 3.083516]\n",
      "epoch:34 step:26648 [D loss: 0.335513, acc.: 85.16%] [G loss: 3.355924]\n",
      "epoch:34 step:26649 [D loss: 0.384631, acc.: 78.91%] [G loss: 4.796939]\n",
      "epoch:34 step:26650 [D loss: 0.399112, acc.: 81.25%] [G loss: 3.174056]\n",
      "epoch:34 step:26651 [D loss: 0.411697, acc.: 82.81%] [G loss: 2.987337]\n",
      "epoch:34 step:26652 [D loss: 0.401265, acc.: 82.81%] [G loss: 2.938534]\n",
      "epoch:34 step:26653 [D loss: 0.291748, acc.: 85.94%] [G loss: 3.488832]\n",
      "epoch:34 step:26654 [D loss: 0.254698, acc.: 89.84%] [G loss: 5.594564]\n",
      "epoch:34 step:26655 [D loss: 0.234507, acc.: 90.62%] [G loss: 6.636615]\n",
      "epoch:34 step:26656 [D loss: 0.356896, acc.: 82.03%] [G loss: 4.141073]\n",
      "epoch:34 step:26657 [D loss: 0.280724, acc.: 89.84%] [G loss: 5.264839]\n",
      "epoch:34 step:26658 [D loss: 0.235996, acc.: 91.41%] [G loss: 3.479162]\n",
      "epoch:34 step:26659 [D loss: 0.317876, acc.: 85.16%] [G loss: 3.911472]\n",
      "epoch:34 step:26660 [D loss: 0.334554, acc.: 82.81%] [G loss: 3.535975]\n",
      "epoch:34 step:26661 [D loss: 0.233457, acc.: 92.19%] [G loss: 4.445310]\n",
      "epoch:34 step:26662 [D loss: 0.257419, acc.: 87.50%] [G loss: 5.294238]\n",
      "epoch:34 step:26663 [D loss: 0.299690, acc.: 85.94%] [G loss: 3.885046]\n",
      "epoch:34 step:26664 [D loss: 0.431355, acc.: 78.12%] [G loss: 2.710083]\n",
      "epoch:34 step:26665 [D loss: 0.188048, acc.: 89.06%] [G loss: 4.967104]\n",
      "epoch:34 step:26666 [D loss: 0.333365, acc.: 85.94%] [G loss: 3.592356]\n",
      "epoch:34 step:26667 [D loss: 0.290608, acc.: 83.59%] [G loss: 5.206646]\n",
      "epoch:34 step:26668 [D loss: 0.387274, acc.: 78.91%] [G loss: 3.564141]\n",
      "epoch:34 step:26669 [D loss: 0.395191, acc.: 85.16%] [G loss: 3.721369]\n",
      "epoch:34 step:26670 [D loss: 0.273615, acc.: 89.06%] [G loss: 4.859864]\n",
      "epoch:34 step:26671 [D loss: 0.340852, acc.: 82.81%] [G loss: 4.072817]\n",
      "epoch:34 step:26672 [D loss: 0.245575, acc.: 88.28%] [G loss: 4.078122]\n",
      "epoch:34 step:26673 [D loss: 0.332652, acc.: 86.72%] [G loss: 3.082124]\n",
      "epoch:34 step:26674 [D loss: 0.387594, acc.: 82.81%] [G loss: 3.273137]\n",
      "epoch:34 step:26675 [D loss: 0.390877, acc.: 82.03%] [G loss: 3.572796]\n",
      "epoch:34 step:26676 [D loss: 0.297836, acc.: 89.06%] [G loss: 3.551957]\n",
      "epoch:34 step:26677 [D loss: 0.304545, acc.: 86.72%] [G loss: 4.322935]\n",
      "epoch:34 step:26678 [D loss: 0.285198, acc.: 86.72%] [G loss: 4.628512]\n",
      "epoch:34 step:26679 [D loss: 0.485985, acc.: 78.12%] [G loss: 4.499201]\n",
      "epoch:34 step:26680 [D loss: 0.655849, acc.: 70.31%] [G loss: 6.965121]\n",
      "epoch:34 step:26681 [D loss: 0.872675, acc.: 73.44%] [G loss: 5.471979]\n",
      "epoch:34 step:26682 [D loss: 0.439296, acc.: 79.69%] [G loss: 3.272437]\n",
      "epoch:34 step:26683 [D loss: 0.547687, acc.: 74.22%] [G loss: 5.707366]\n",
      "epoch:34 step:26684 [D loss: 0.952630, acc.: 73.44%] [G loss: 3.887338]\n",
      "epoch:34 step:26685 [D loss: 0.321192, acc.: 86.72%] [G loss: 5.067367]\n",
      "epoch:34 step:26686 [D loss: 0.342266, acc.: 82.03%] [G loss: 3.592597]\n",
      "epoch:34 step:26687 [D loss: 0.291559, acc.: 86.72%] [G loss: 3.393747]\n",
      "epoch:34 step:26688 [D loss: 0.313629, acc.: 86.72%] [G loss: 3.647413]\n",
      "epoch:34 step:26689 [D loss: 0.457342, acc.: 76.56%] [G loss: 2.563878]\n",
      "epoch:34 step:26690 [D loss: 0.297873, acc.: 88.28%] [G loss: 3.482300]\n",
      "epoch:34 step:26691 [D loss: 0.389127, acc.: 82.81%] [G loss: 2.735682]\n",
      "epoch:34 step:26692 [D loss: 0.295501, acc.: 90.62%] [G loss: 2.451249]\n",
      "epoch:34 step:26693 [D loss: 0.406079, acc.: 82.81%] [G loss: 3.016278]\n",
      "epoch:34 step:26694 [D loss: 0.358115, acc.: 81.25%] [G loss: 3.535528]\n",
      "epoch:34 step:26695 [D loss: 0.387888, acc.: 83.59%] [G loss: 2.850417]\n",
      "epoch:34 step:26696 [D loss: 0.352624, acc.: 84.38%] [G loss: 2.615439]\n",
      "epoch:34 step:26697 [D loss: 0.261389, acc.: 86.72%] [G loss: 2.750168]\n",
      "epoch:34 step:26698 [D loss: 0.382953, acc.: 82.03%] [G loss: 2.596295]\n",
      "epoch:34 step:26699 [D loss: 0.256111, acc.: 90.62%] [G loss: 2.950504]\n",
      "epoch:34 step:26700 [D loss: 0.358129, acc.: 79.69%] [G loss: 3.012789]\n",
      "epoch:34 step:26701 [D loss: 0.377213, acc.: 78.91%] [G loss: 2.414954]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:34 step:26702 [D loss: 0.341505, acc.: 85.16%] [G loss: 2.616803]\n",
      "epoch:34 step:26703 [D loss: 0.332625, acc.: 85.16%] [G loss: 3.003195]\n",
      "epoch:34 step:26704 [D loss: 0.208216, acc.: 94.53%] [G loss: 3.031297]\n",
      "epoch:34 step:26705 [D loss: 0.365706, acc.: 85.16%] [G loss: 2.724993]\n",
      "epoch:34 step:26706 [D loss: 0.282810, acc.: 89.06%] [G loss: 3.177244]\n",
      "epoch:34 step:26707 [D loss: 0.266939, acc.: 85.94%] [G loss: 2.799504]\n",
      "epoch:34 step:26708 [D loss: 0.284626, acc.: 87.50%] [G loss: 3.892044]\n",
      "epoch:34 step:26709 [D loss: 0.279892, acc.: 89.84%] [G loss: 3.232270]\n",
      "epoch:34 step:26710 [D loss: 0.339181, acc.: 86.72%] [G loss: 3.177835]\n",
      "epoch:34 step:26711 [D loss: 0.399577, acc.: 81.25%] [G loss: 2.559701]\n",
      "epoch:34 step:26712 [D loss: 0.386712, acc.: 79.69%] [G loss: 2.848614]\n",
      "epoch:34 step:26713 [D loss: 0.399880, acc.: 82.81%] [G loss: 3.501963]\n",
      "epoch:34 step:26714 [D loss: 0.431668, acc.: 79.69%] [G loss: 3.628313]\n",
      "epoch:34 step:26715 [D loss: 0.323819, acc.: 83.59%] [G loss: 3.319030]\n",
      "epoch:34 step:26716 [D loss: 0.317189, acc.: 85.16%] [G loss: 2.916057]\n",
      "epoch:34 step:26717 [D loss: 0.561101, acc.: 76.56%] [G loss: 3.067780]\n",
      "epoch:34 step:26718 [D loss: 0.300978, acc.: 88.28%] [G loss: 3.062138]\n",
      "epoch:34 step:26719 [D loss: 0.449012, acc.: 79.69%] [G loss: 4.955989]\n",
      "epoch:34 step:26720 [D loss: 0.312438, acc.: 84.38%] [G loss: 4.738466]\n",
      "epoch:34 step:26721 [D loss: 0.363856, acc.: 81.25%] [G loss: 5.110966]\n",
      "epoch:34 step:26722 [D loss: 0.229765, acc.: 90.62%] [G loss: 3.525298]\n",
      "epoch:34 step:26723 [D loss: 0.353954, acc.: 80.47%] [G loss: 3.168826]\n",
      "epoch:34 step:26724 [D loss: 0.176255, acc.: 94.53%] [G loss: 2.667930]\n",
      "epoch:34 step:26725 [D loss: 0.307085, acc.: 87.50%] [G loss: 4.180989]\n",
      "epoch:34 step:26726 [D loss: 0.295279, acc.: 87.50%] [G loss: 3.379826]\n",
      "epoch:34 step:26727 [D loss: 0.271137, acc.: 87.50%] [G loss: 3.527182]\n",
      "epoch:34 step:26728 [D loss: 0.310603, acc.: 85.16%] [G loss: 2.912017]\n",
      "epoch:34 step:26729 [D loss: 0.244201, acc.: 91.41%] [G loss: 4.405519]\n",
      "epoch:34 step:26730 [D loss: 0.306570, acc.: 88.28%] [G loss: 2.912774]\n",
      "epoch:34 step:26731 [D loss: 0.426827, acc.: 79.69%] [G loss: 3.451904]\n",
      "epoch:34 step:26732 [D loss: 0.243328, acc.: 90.62%] [G loss: 3.339413]\n",
      "epoch:34 step:26733 [D loss: 0.226244, acc.: 89.84%] [G loss: 3.642377]\n",
      "epoch:34 step:26734 [D loss: 0.341200, acc.: 82.03%] [G loss: 3.127292]\n",
      "epoch:34 step:26735 [D loss: 0.277639, acc.: 89.06%] [G loss: 2.484538]\n",
      "epoch:34 step:26736 [D loss: 0.359168, acc.: 84.38%] [G loss: 2.850007]\n",
      "epoch:34 step:26737 [D loss: 0.271173, acc.: 85.94%] [G loss: 2.472275]\n",
      "epoch:34 step:26738 [D loss: 0.354893, acc.: 83.59%] [G loss: 2.161445]\n",
      "epoch:34 step:26739 [D loss: 0.294953, acc.: 87.50%] [G loss: 2.845353]\n",
      "epoch:34 step:26740 [D loss: 0.319492, acc.: 85.94%] [G loss: 2.755593]\n",
      "epoch:34 step:26741 [D loss: 0.514867, acc.: 70.31%] [G loss: 2.488480]\n",
      "epoch:34 step:26742 [D loss: 0.347662, acc.: 80.47%] [G loss: 3.100751]\n",
      "epoch:34 step:26743 [D loss: 0.401805, acc.: 78.12%] [G loss: 2.994927]\n",
      "epoch:34 step:26744 [D loss: 0.273103, acc.: 89.84%] [G loss: 3.156490]\n",
      "epoch:34 step:26745 [D loss: 0.353896, acc.: 84.38%] [G loss: 3.532451]\n",
      "epoch:34 step:26746 [D loss: 0.230531, acc.: 90.62%] [G loss: 4.158257]\n",
      "epoch:34 step:26747 [D loss: 0.294616, acc.: 86.72%] [G loss: 5.533734]\n",
      "epoch:34 step:26748 [D loss: 0.369188, acc.: 84.38%] [G loss: 6.198864]\n",
      "epoch:34 step:26749 [D loss: 0.230012, acc.: 89.84%] [G loss: 4.629372]\n",
      "epoch:34 step:26750 [D loss: 0.246586, acc.: 90.62%] [G loss: 3.416633]\n",
      "epoch:34 step:26751 [D loss: 0.218894, acc.: 89.84%] [G loss: 3.199127]\n",
      "epoch:34 step:26752 [D loss: 0.250673, acc.: 89.84%] [G loss: 4.107350]\n",
      "epoch:34 step:26753 [D loss: 0.213847, acc.: 92.19%] [G loss: 4.384961]\n",
      "epoch:34 step:26754 [D loss: 0.312560, acc.: 84.38%] [G loss: 3.236515]\n",
      "epoch:34 step:26755 [D loss: 0.308775, acc.: 89.06%] [G loss: 3.468238]\n",
      "epoch:34 step:26756 [D loss: 0.446731, acc.: 79.69%] [G loss: 3.431346]\n",
      "epoch:34 step:26757 [D loss: 0.323741, acc.: 85.16%] [G loss: 2.485890]\n",
      "epoch:34 step:26758 [D loss: 0.291843, acc.: 85.94%] [G loss: 3.121860]\n",
      "epoch:34 step:26759 [D loss: 0.386767, acc.: 82.03%] [G loss: 3.717134]\n",
      "epoch:34 step:26760 [D loss: 0.235151, acc.: 90.62%] [G loss: 3.350767]\n",
      "epoch:34 step:26761 [D loss: 0.372230, acc.: 81.25%] [G loss: 3.203850]\n",
      "epoch:34 step:26762 [D loss: 0.402405, acc.: 81.25%] [G loss: 2.871268]\n",
      "epoch:34 step:26763 [D loss: 0.331954, acc.: 86.72%] [G loss: 3.449870]\n",
      "epoch:34 step:26764 [D loss: 0.337179, acc.: 84.38%] [G loss: 2.548914]\n",
      "epoch:34 step:26765 [D loss: 0.356559, acc.: 81.25%] [G loss: 3.905645]\n",
      "epoch:34 step:26766 [D loss: 0.263708, acc.: 88.28%] [G loss: 5.169916]\n",
      "epoch:34 step:26767 [D loss: 0.274671, acc.: 85.16%] [G loss: 4.762673]\n",
      "epoch:34 step:26768 [D loss: 0.345838, acc.: 83.59%] [G loss: 3.394334]\n",
      "epoch:34 step:26769 [D loss: 0.223686, acc.: 89.06%] [G loss: 5.056222]\n",
      "epoch:34 step:26770 [D loss: 0.294541, acc.: 88.28%] [G loss: 3.571354]\n",
      "epoch:34 step:26771 [D loss: 0.276437, acc.: 85.16%] [G loss: 5.228435]\n",
      "epoch:34 step:26772 [D loss: 0.286323, acc.: 88.28%] [G loss: 4.292195]\n",
      "epoch:34 step:26773 [D loss: 0.128055, acc.: 96.88%] [G loss: 5.232209]\n",
      "epoch:34 step:26774 [D loss: 0.219893, acc.: 92.19%] [G loss: 3.886225]\n",
      "epoch:34 step:26775 [D loss: 0.231736, acc.: 91.41%] [G loss: 3.540656]\n",
      "epoch:34 step:26776 [D loss: 0.236505, acc.: 89.84%] [G loss: 2.987796]\n",
      "epoch:34 step:26777 [D loss: 0.298349, acc.: 87.50%] [G loss: 3.432422]\n",
      "epoch:34 step:26778 [D loss: 0.376531, acc.: 82.03%] [G loss: 2.831348]\n",
      "epoch:34 step:26779 [D loss: 0.337948, acc.: 82.03%] [G loss: 2.399016]\n",
      "epoch:34 step:26780 [D loss: 0.354731, acc.: 82.03%] [G loss: 2.412468]\n",
      "epoch:34 step:26781 [D loss: 0.258270, acc.: 89.06%] [G loss: 3.181527]\n",
      "epoch:34 step:26782 [D loss: 0.421912, acc.: 79.69%] [G loss: 3.825239]\n",
      "epoch:34 step:26783 [D loss: 0.451675, acc.: 78.91%] [G loss: 4.655240]\n",
      "epoch:34 step:26784 [D loss: 0.439774, acc.: 77.34%] [G loss: 4.475338]\n",
      "epoch:34 step:26785 [D loss: 0.405845, acc.: 81.25%] [G loss: 2.433431]\n",
      "epoch:34 step:26786 [D loss: 0.243124, acc.: 91.41%] [G loss: 3.729619]\n",
      "epoch:34 step:26787 [D loss: 0.290060, acc.: 88.28%] [G loss: 3.004245]\n",
      "epoch:34 step:26788 [D loss: 0.243995, acc.: 89.06%] [G loss: 3.231130]\n",
      "epoch:34 step:26789 [D loss: 0.341636, acc.: 86.72%] [G loss: 2.705494]\n",
      "epoch:34 step:26790 [D loss: 0.326211, acc.: 85.16%] [G loss: 3.232008]\n",
      "epoch:34 step:26791 [D loss: 0.251874, acc.: 89.06%] [G loss: 3.537065]\n",
      "epoch:34 step:26792 [D loss: 0.331146, acc.: 85.94%] [G loss: 3.988311]\n",
      "epoch:34 step:26793 [D loss: 0.240392, acc.: 89.06%] [G loss: 4.077011]\n",
      "epoch:34 step:26794 [D loss: 0.309848, acc.: 84.38%] [G loss: 4.087461]\n",
      "epoch:34 step:26795 [D loss: 0.262922, acc.: 88.28%] [G loss: 3.576380]\n",
      "epoch:34 step:26796 [D loss: 0.244912, acc.: 89.06%] [G loss: 2.824051]\n",
      "epoch:34 step:26797 [D loss: 0.349074, acc.: 85.16%] [G loss: 3.493589]\n",
      "epoch:34 step:26798 [D loss: 0.415273, acc.: 82.03%] [G loss: 5.304327]\n",
      "epoch:34 step:26799 [D loss: 0.214706, acc.: 92.19%] [G loss: 3.963979]\n",
      "epoch:34 step:26800 [D loss: 0.250632, acc.: 91.41%] [G loss: 2.575346]\n",
      "##############\n",
      "[0.87849501 0.85380828 0.81782735 0.82844988 0.76876471 0.83851596\n",
      " 0.85564506 0.83956887 0.79886994 0.81694035]\n",
      "##########\n",
      "epoch:34 step:26801 [D loss: 0.315330, acc.: 83.59%] [G loss: 2.772481]\n",
      "epoch:34 step:26802 [D loss: 0.289520, acc.: 87.50%] [G loss: 2.936590]\n",
      "epoch:34 step:26803 [D loss: 0.367585, acc.: 81.25%] [G loss: 2.294160]\n",
      "epoch:34 step:26804 [D loss: 0.378811, acc.: 82.03%] [G loss: 3.285709]\n",
      "epoch:34 step:26805 [D loss: 0.272243, acc.: 88.28%] [G loss: 3.601052]\n",
      "epoch:34 step:26806 [D loss: 0.285042, acc.: 86.72%] [G loss: 3.534448]\n",
      "epoch:34 step:26807 [D loss: 0.255976, acc.: 89.84%] [G loss: 2.476194]\n",
      "epoch:34 step:26808 [D loss: 0.311881, acc.: 82.03%] [G loss: 2.933673]\n",
      "epoch:34 step:26809 [D loss: 0.366539, acc.: 85.94%] [G loss: 2.799622]\n",
      "epoch:34 step:26810 [D loss: 0.275367, acc.: 90.62%] [G loss: 2.882461]\n",
      "epoch:34 step:26811 [D loss: 0.396938, acc.: 79.69%] [G loss: 3.568459]\n",
      "epoch:34 step:26812 [D loss: 0.377253, acc.: 82.81%] [G loss: 2.735258]\n",
      "epoch:34 step:26813 [D loss: 0.365091, acc.: 85.16%] [G loss: 2.942388]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:34 step:26814 [D loss: 0.238500, acc.: 89.06%] [G loss: 6.081232]\n",
      "epoch:34 step:26815 [D loss: 0.416514, acc.: 80.47%] [G loss: 3.513345]\n",
      "epoch:34 step:26816 [D loss: 0.242771, acc.: 89.06%] [G loss: 4.198131]\n",
      "epoch:34 step:26817 [D loss: 0.219543, acc.: 89.06%] [G loss: 3.343294]\n",
      "epoch:34 step:26818 [D loss: 0.301483, acc.: 84.38%] [G loss: 2.776247]\n",
      "epoch:34 step:26819 [D loss: 0.325626, acc.: 80.47%] [G loss: 3.501968]\n",
      "epoch:34 step:26820 [D loss: 0.323554, acc.: 82.81%] [G loss: 2.917351]\n",
      "epoch:34 step:26821 [D loss: 0.263160, acc.: 89.06%] [G loss: 3.757639]\n",
      "epoch:34 step:26822 [D loss: 0.371229, acc.: 83.59%] [G loss: 3.030569]\n",
      "epoch:34 step:26823 [D loss: 0.352952, acc.: 82.03%] [G loss: 3.469909]\n",
      "epoch:34 step:26824 [D loss: 0.300714, acc.: 85.94%] [G loss: 3.158707]\n",
      "epoch:34 step:26825 [D loss: 0.286920, acc.: 85.94%] [G loss: 4.947361]\n",
      "epoch:34 step:26826 [D loss: 0.279301, acc.: 89.06%] [G loss: 5.604619]\n",
      "epoch:34 step:26827 [D loss: 0.237397, acc.: 89.84%] [G loss: 5.021667]\n",
      "epoch:34 step:26828 [D loss: 0.308422, acc.: 88.28%] [G loss: 5.074295]\n",
      "epoch:34 step:26829 [D loss: 0.213596, acc.: 89.84%] [G loss: 7.601279]\n",
      "epoch:34 step:26830 [D loss: 0.288559, acc.: 87.50%] [G loss: 5.971065]\n",
      "epoch:34 step:26831 [D loss: 0.281911, acc.: 85.94%] [G loss: 6.468666]\n",
      "epoch:34 step:26832 [D loss: 0.291729, acc.: 88.28%] [G loss: 6.047582]\n",
      "epoch:34 step:26833 [D loss: 0.204013, acc.: 90.62%] [G loss: 5.161989]\n",
      "epoch:34 step:26834 [D loss: 0.120520, acc.: 95.31%] [G loss: 5.549227]\n",
      "epoch:34 step:26835 [D loss: 0.247660, acc.: 87.50%] [G loss: 3.943769]\n",
      "epoch:34 step:26836 [D loss: 0.327349, acc.: 85.16%] [G loss: 3.514443]\n",
      "epoch:34 step:26837 [D loss: 0.213444, acc.: 92.97%] [G loss: 2.758524]\n",
      "epoch:34 step:26838 [D loss: 0.305309, acc.: 89.06%] [G loss: 3.341428]\n",
      "epoch:34 step:26839 [D loss: 0.315646, acc.: 84.38%] [G loss: 3.430555]\n",
      "epoch:34 step:26840 [D loss: 0.335693, acc.: 82.03%] [G loss: 4.438606]\n",
      "epoch:34 step:26841 [D loss: 0.319454, acc.: 85.16%] [G loss: 5.138554]\n",
      "epoch:34 step:26842 [D loss: 0.279134, acc.: 87.50%] [G loss: 3.568599]\n",
      "epoch:34 step:26843 [D loss: 0.336408, acc.: 86.72%] [G loss: 2.948470]\n",
      "epoch:34 step:26844 [D loss: 0.256726, acc.: 89.84%] [G loss: 3.243854]\n",
      "epoch:34 step:26845 [D loss: 0.316690, acc.: 84.38%] [G loss: 3.107023]\n",
      "epoch:34 step:26846 [D loss: 0.263031, acc.: 91.41%] [G loss: 3.726095]\n",
      "epoch:34 step:26847 [D loss: 0.281402, acc.: 86.72%] [G loss: 4.139408]\n",
      "epoch:34 step:26848 [D loss: 0.390466, acc.: 85.16%] [G loss: 3.604375]\n",
      "epoch:34 step:26849 [D loss: 0.348334, acc.: 82.81%] [G loss: 3.007740]\n",
      "epoch:34 step:26850 [D loss: 0.365272, acc.: 82.81%] [G loss: 2.837182]\n",
      "epoch:34 step:26851 [D loss: 0.381115, acc.: 79.69%] [G loss: 3.559940]\n",
      "epoch:34 step:26852 [D loss: 0.334935, acc.: 86.72%] [G loss: 3.879252]\n",
      "epoch:34 step:26853 [D loss: 0.392557, acc.: 84.38%] [G loss: 5.716831]\n",
      "epoch:34 step:26854 [D loss: 0.594321, acc.: 71.88%] [G loss: 5.574299]\n",
      "epoch:34 step:26855 [D loss: 0.939560, acc.: 68.75%] [G loss: 8.384220]\n",
      "epoch:34 step:26856 [D loss: 2.727502, acc.: 57.81%] [G loss: 2.744723]\n",
      "epoch:34 step:26857 [D loss: 0.397370, acc.: 85.16%] [G loss: 3.055678]\n",
      "epoch:34 step:26858 [D loss: 0.325324, acc.: 84.38%] [G loss: 2.903432]\n",
      "epoch:34 step:26859 [D loss: 0.481900, acc.: 77.34%] [G loss: 4.343123]\n",
      "epoch:34 step:26860 [D loss: 0.273716, acc.: 89.84%] [G loss: 3.577736]\n",
      "epoch:34 step:26861 [D loss: 0.253152, acc.: 88.28%] [G loss: 3.919100]\n",
      "epoch:34 step:26862 [D loss: 0.346233, acc.: 85.16%] [G loss: 2.899227]\n",
      "epoch:34 step:26863 [D loss: 0.365215, acc.: 79.69%] [G loss: 2.472903]\n",
      "epoch:34 step:26864 [D loss: 0.207996, acc.: 91.41%] [G loss: 2.991151]\n",
      "epoch:34 step:26865 [D loss: 0.262650, acc.: 87.50%] [G loss: 4.287408]\n",
      "epoch:34 step:26866 [D loss: 0.366314, acc.: 80.47%] [G loss: 2.737441]\n",
      "epoch:34 step:26867 [D loss: 0.335761, acc.: 88.28%] [G loss: 3.682801]\n",
      "epoch:34 step:26868 [D loss: 0.296896, acc.: 89.06%] [G loss: 4.070081]\n",
      "epoch:34 step:26869 [D loss: 0.331372, acc.: 81.25%] [G loss: 4.018014]\n",
      "epoch:34 step:26870 [D loss: 0.303151, acc.: 84.38%] [G loss: 2.671241]\n",
      "epoch:34 step:26871 [D loss: 0.328335, acc.: 86.72%] [G loss: 3.701399]\n",
      "epoch:34 step:26872 [D loss: 0.407182, acc.: 79.69%] [G loss: 4.646575]\n",
      "epoch:34 step:26873 [D loss: 0.330502, acc.: 84.38%] [G loss: 2.867354]\n",
      "epoch:34 step:26874 [D loss: 0.423350, acc.: 79.69%] [G loss: 3.712649]\n",
      "epoch:34 step:26875 [D loss: 0.337528, acc.: 82.81%] [G loss: 3.622760]\n",
      "epoch:34 step:26876 [D loss: 0.496837, acc.: 80.47%] [G loss: 3.151741]\n",
      "epoch:34 step:26877 [D loss: 0.339960, acc.: 85.94%] [G loss: 3.129535]\n",
      "epoch:34 step:26878 [D loss: 0.418041, acc.: 82.03%] [G loss: 3.450358]\n",
      "epoch:34 step:26879 [D loss: 0.426065, acc.: 78.91%] [G loss: 3.342989]\n",
      "epoch:34 step:26880 [D loss: 0.390389, acc.: 82.03%] [G loss: 2.957812]\n",
      "epoch:34 step:26881 [D loss: 0.376468, acc.: 82.81%] [G loss: 4.388729]\n",
      "epoch:34 step:26882 [D loss: 0.381947, acc.: 85.16%] [G loss: 3.154006]\n",
      "epoch:34 step:26883 [D loss: 0.435387, acc.: 79.69%] [G loss: 4.426049]\n",
      "epoch:34 step:26884 [D loss: 0.322828, acc.: 88.28%] [G loss: 4.175389]\n",
      "epoch:34 step:26885 [D loss: 0.383195, acc.: 81.25%] [G loss: 2.909489]\n",
      "epoch:34 step:26886 [D loss: 0.355711, acc.: 86.72%] [G loss: 4.211351]\n",
      "epoch:34 step:26887 [D loss: 0.415088, acc.: 80.47%] [G loss: 2.788060]\n",
      "epoch:34 step:26888 [D loss: 0.280007, acc.: 88.28%] [G loss: 3.492300]\n",
      "epoch:34 step:26889 [D loss: 0.379891, acc.: 85.16%] [G loss: 3.237913]\n",
      "epoch:34 step:26890 [D loss: 0.284627, acc.: 89.84%] [G loss: 2.665138]\n",
      "epoch:34 step:26891 [D loss: 0.357885, acc.: 84.38%] [G loss: 3.271677]\n",
      "epoch:34 step:26892 [D loss: 0.340637, acc.: 84.38%] [G loss: 3.696244]\n",
      "epoch:34 step:26893 [D loss: 0.274466, acc.: 89.06%] [G loss: 3.260481]\n",
      "epoch:34 step:26894 [D loss: 0.452989, acc.: 75.78%] [G loss: 3.847490]\n",
      "epoch:34 step:26895 [D loss: 0.366611, acc.: 83.59%] [G loss: 2.555921]\n",
      "epoch:34 step:26896 [D loss: 0.326882, acc.: 85.16%] [G loss: 3.515764]\n",
      "epoch:34 step:26897 [D loss: 0.309471, acc.: 88.28%] [G loss: 2.628304]\n",
      "epoch:34 step:26898 [D loss: 0.309467, acc.: 84.38%] [G loss: 3.096982]\n",
      "epoch:34 step:26899 [D loss: 0.345597, acc.: 81.25%] [G loss: 2.765341]\n",
      "epoch:34 step:26900 [D loss: 0.261979, acc.: 89.06%] [G loss: 2.521905]\n",
      "epoch:34 step:26901 [D loss: 0.398684, acc.: 83.59%] [G loss: 2.761562]\n",
      "epoch:34 step:26902 [D loss: 0.373191, acc.: 83.59%] [G loss: 2.861576]\n",
      "epoch:34 step:26903 [D loss: 0.345176, acc.: 81.25%] [G loss: 3.437789]\n",
      "epoch:34 step:26904 [D loss: 0.374789, acc.: 80.47%] [G loss: 2.781380]\n",
      "epoch:34 step:26905 [D loss: 0.406836, acc.: 82.03%] [G loss: 3.190824]\n",
      "epoch:34 step:26906 [D loss: 0.270856, acc.: 89.06%] [G loss: 3.029037]\n",
      "epoch:34 step:26907 [D loss: 0.311993, acc.: 82.81%] [G loss: 3.004392]\n",
      "epoch:34 step:26908 [D loss: 0.266127, acc.: 88.28%] [G loss: 2.495275]\n",
      "epoch:34 step:26909 [D loss: 0.282108, acc.: 85.94%] [G loss: 3.031356]\n",
      "epoch:34 step:26910 [D loss: 0.301094, acc.: 86.72%] [G loss: 2.540473]\n",
      "epoch:34 step:26911 [D loss: 0.367430, acc.: 82.81%] [G loss: 3.397736]\n",
      "epoch:34 step:26912 [D loss: 0.228326, acc.: 91.41%] [G loss: 3.889652]\n",
      "epoch:34 step:26913 [D loss: 0.260375, acc.: 89.06%] [G loss: 3.123278]\n",
      "epoch:34 step:26914 [D loss: 0.297887, acc.: 85.94%] [G loss: 3.272769]\n",
      "epoch:34 step:26915 [D loss: 0.338348, acc.: 82.81%] [G loss: 2.513661]\n",
      "epoch:34 step:26916 [D loss: 0.374009, acc.: 80.47%] [G loss: 2.937479]\n",
      "epoch:34 step:26917 [D loss: 0.265244, acc.: 86.72%] [G loss: 2.772961]\n",
      "epoch:34 step:26918 [D loss: 0.260775, acc.: 90.62%] [G loss: 3.038317]\n",
      "epoch:34 step:26919 [D loss: 0.240476, acc.: 89.06%] [G loss: 4.884588]\n",
      "epoch:34 step:26920 [D loss: 0.302542, acc.: 87.50%] [G loss: 4.844764]\n",
      "epoch:34 step:26921 [D loss: 0.346629, acc.: 82.81%] [G loss: 3.796678]\n",
      "epoch:34 step:26922 [D loss: 0.337248, acc.: 84.38%] [G loss: 4.535857]\n",
      "epoch:34 step:26923 [D loss: 0.257880, acc.: 89.06%] [G loss: 2.981207]\n",
      "epoch:34 step:26924 [D loss: 0.281216, acc.: 89.06%] [G loss: 4.195249]\n",
      "epoch:34 step:26925 [D loss: 0.242332, acc.: 89.06%] [G loss: 3.547860]\n",
      "epoch:34 step:26926 [D loss: 0.268362, acc.: 89.06%] [G loss: 4.067587]\n",
      "epoch:34 step:26927 [D loss: 0.296021, acc.: 85.94%] [G loss: 2.908074]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:34 step:26928 [D loss: 0.283089, acc.: 85.16%] [G loss: 3.887933]\n",
      "epoch:34 step:26929 [D loss: 0.385594, acc.: 81.25%] [G loss: 2.798025]\n",
      "epoch:34 step:26930 [D loss: 0.305852, acc.: 85.94%] [G loss: 3.330543]\n",
      "epoch:34 step:26931 [D loss: 0.289701, acc.: 87.50%] [G loss: 2.905519]\n",
      "epoch:34 step:26932 [D loss: 0.309809, acc.: 85.94%] [G loss: 4.329096]\n",
      "epoch:34 step:26933 [D loss: 0.297963, acc.: 85.94%] [G loss: 5.462696]\n",
      "epoch:34 step:26934 [D loss: 0.265982, acc.: 88.28%] [G loss: 6.851660]\n",
      "epoch:34 step:26935 [D loss: 0.237928, acc.: 91.41%] [G loss: 3.879536]\n",
      "epoch:34 step:26936 [D loss: 0.442944, acc.: 77.34%] [G loss: 2.638932]\n",
      "epoch:34 step:26937 [D loss: 0.370252, acc.: 85.94%] [G loss: 4.174571]\n",
      "epoch:34 step:26938 [D loss: 0.292912, acc.: 83.59%] [G loss: 3.541311]\n",
      "epoch:34 step:26939 [D loss: 0.300778, acc.: 84.38%] [G loss: 2.803991]\n",
      "epoch:34 step:26940 [D loss: 0.352082, acc.: 84.38%] [G loss: 3.779156]\n",
      "epoch:34 step:26941 [D loss: 0.340183, acc.: 84.38%] [G loss: 3.104636]\n",
      "epoch:34 step:26942 [D loss: 0.335055, acc.: 83.59%] [G loss: 2.638866]\n",
      "epoch:34 step:26943 [D loss: 0.466694, acc.: 79.69%] [G loss: 3.159295]\n",
      "epoch:34 step:26944 [D loss: 0.286233, acc.: 89.84%] [G loss: 3.763106]\n",
      "epoch:34 step:26945 [D loss: 0.486063, acc.: 81.25%] [G loss: 3.328551]\n",
      "epoch:34 step:26946 [D loss: 0.350800, acc.: 82.03%] [G loss: 4.431237]\n",
      "epoch:34 step:26947 [D loss: 0.289699, acc.: 86.72%] [G loss: 4.692867]\n",
      "epoch:34 step:26948 [D loss: 0.340406, acc.: 85.16%] [G loss: 3.097776]\n",
      "epoch:34 step:26949 [D loss: 0.253774, acc.: 88.28%] [G loss: 4.817786]\n",
      "epoch:34 step:26950 [D loss: 0.326889, acc.: 87.50%] [G loss: 4.415207]\n",
      "epoch:34 step:26951 [D loss: 0.259022, acc.: 88.28%] [G loss: 5.610223]\n",
      "epoch:34 step:26952 [D loss: 0.308177, acc.: 85.16%] [G loss: 4.786997]\n",
      "epoch:34 step:26953 [D loss: 0.337259, acc.: 83.59%] [G loss: 4.063197]\n",
      "epoch:34 step:26954 [D loss: 0.362379, acc.: 85.94%] [G loss: 2.941101]\n",
      "epoch:34 step:26955 [D loss: 0.314210, acc.: 84.38%] [G loss: 2.970036]\n",
      "epoch:34 step:26956 [D loss: 0.332045, acc.: 85.94%] [G loss: 2.668092]\n",
      "epoch:34 step:26957 [D loss: 0.272779, acc.: 88.28%] [G loss: 3.173889]\n",
      "epoch:34 step:26958 [D loss: 0.330378, acc.: 83.59%] [G loss: 3.614957]\n",
      "epoch:34 step:26959 [D loss: 0.279481, acc.: 86.72%] [G loss: 2.624860]\n",
      "epoch:34 step:26960 [D loss: 0.317299, acc.: 85.94%] [G loss: 2.295203]\n",
      "epoch:34 step:26961 [D loss: 0.310067, acc.: 87.50%] [G loss: 3.564545]\n",
      "epoch:34 step:26962 [D loss: 0.330133, acc.: 82.81%] [G loss: 3.993428]\n",
      "epoch:34 step:26963 [D loss: 0.234433, acc.: 88.28%] [G loss: 3.669978]\n",
      "epoch:34 step:26964 [D loss: 0.412509, acc.: 81.25%] [G loss: 2.962961]\n",
      "epoch:34 step:26965 [D loss: 0.315355, acc.: 85.94%] [G loss: 3.303676]\n",
      "epoch:34 step:26966 [D loss: 0.379025, acc.: 78.12%] [G loss: 2.216630]\n",
      "epoch:34 step:26967 [D loss: 0.275735, acc.: 89.06%] [G loss: 2.390512]\n",
      "epoch:34 step:26968 [D loss: 0.293549, acc.: 87.50%] [G loss: 2.280996]\n",
      "epoch:34 step:26969 [D loss: 0.265747, acc.: 86.72%] [G loss: 3.550921]\n",
      "epoch:34 step:26970 [D loss: 0.211067, acc.: 92.19%] [G loss: 3.750907]\n",
      "epoch:34 step:26971 [D loss: 0.276933, acc.: 90.62%] [G loss: 3.024792]\n",
      "epoch:34 step:26972 [D loss: 0.265014, acc.: 89.84%] [G loss: 3.853654]\n",
      "epoch:34 step:26973 [D loss: 0.393219, acc.: 82.81%] [G loss: 3.419122]\n",
      "epoch:34 step:26974 [D loss: 0.283692, acc.: 84.38%] [G loss: 4.175611]\n",
      "epoch:34 step:26975 [D loss: 0.273044, acc.: 89.84%] [G loss: 3.171400]\n",
      "epoch:34 step:26976 [D loss: 0.304000, acc.: 85.16%] [G loss: 3.023855]\n",
      "epoch:34 step:26977 [D loss: 0.318822, acc.: 82.03%] [G loss: 3.588359]\n",
      "epoch:34 step:26978 [D loss: 0.243454, acc.: 89.06%] [G loss: 3.727650]\n",
      "epoch:34 step:26979 [D loss: 0.410775, acc.: 78.12%] [G loss: 3.178543]\n",
      "epoch:34 step:26980 [D loss: 0.315619, acc.: 82.81%] [G loss: 3.077872]\n",
      "epoch:34 step:26981 [D loss: 0.312633, acc.: 87.50%] [G loss: 3.708841]\n",
      "epoch:34 step:26982 [D loss: 0.321133, acc.: 82.03%] [G loss: 4.820086]\n",
      "epoch:34 step:26983 [D loss: 0.375554, acc.: 80.47%] [G loss: 3.331547]\n",
      "epoch:34 step:26984 [D loss: 0.450370, acc.: 75.78%] [G loss: 5.243549]\n",
      "epoch:34 step:26985 [D loss: 0.450414, acc.: 79.69%] [G loss: 6.384289]\n",
      "epoch:34 step:26986 [D loss: 0.339346, acc.: 85.16%] [G loss: 5.984036]\n",
      "epoch:34 step:26987 [D loss: 0.282782, acc.: 82.81%] [G loss: 3.978292]\n",
      "epoch:34 step:26988 [D loss: 0.203359, acc.: 92.19%] [G loss: 3.450995]\n",
      "epoch:34 step:26989 [D loss: 0.352109, acc.: 89.06%] [G loss: 3.151420]\n",
      "epoch:34 step:26990 [D loss: 0.285848, acc.: 83.59%] [G loss: 4.489065]\n",
      "epoch:34 step:26991 [D loss: 0.364210, acc.: 85.16%] [G loss: 2.500763]\n",
      "epoch:34 step:26992 [D loss: 0.300670, acc.: 85.94%] [G loss: 3.056550]\n",
      "epoch:34 step:26993 [D loss: 0.278210, acc.: 89.84%] [G loss: 3.547116]\n",
      "epoch:34 step:26994 [D loss: 0.226817, acc.: 90.62%] [G loss: 3.546512]\n",
      "epoch:34 step:26995 [D loss: 0.243105, acc.: 90.62%] [G loss: 2.944767]\n",
      "epoch:34 step:26996 [D loss: 0.269395, acc.: 89.84%] [G loss: 2.662567]\n",
      "epoch:34 step:26997 [D loss: 0.372739, acc.: 77.34%] [G loss: 2.585556]\n",
      "epoch:34 step:26998 [D loss: 0.311131, acc.: 85.16%] [G loss: 2.316952]\n",
      "epoch:34 step:26999 [D loss: 0.379995, acc.: 84.38%] [G loss: 3.352611]\n",
      "epoch:34 step:27000 [D loss: 0.341368, acc.: 85.16%] [G loss: 3.005120]\n",
      "##############\n",
      "[0.86856398 0.84425509 0.79094876 0.82604564 0.76835738 0.82655993\n",
      " 0.89532968 0.84966049 0.84323434 0.81241222]\n",
      "##########\n",
      "epoch:34 step:27001 [D loss: 0.272737, acc.: 89.06%] [G loss: 3.649252]\n",
      "epoch:34 step:27002 [D loss: 0.315897, acc.: 85.94%] [G loss: 2.893600]\n",
      "epoch:34 step:27003 [D loss: 0.240943, acc.: 89.06%] [G loss: 3.615223]\n",
      "epoch:34 step:27004 [D loss: 0.331400, acc.: 84.38%] [G loss: 2.695372]\n",
      "epoch:34 step:27005 [D loss: 0.249361, acc.: 92.19%] [G loss: 4.111253]\n",
      "epoch:34 step:27006 [D loss: 0.317388, acc.: 85.94%] [G loss: 3.584741]\n",
      "epoch:34 step:27007 [D loss: 0.361238, acc.: 82.81%] [G loss: 3.094182]\n",
      "epoch:34 step:27008 [D loss: 0.355934, acc.: 82.81%] [G loss: 3.271143]\n",
      "epoch:34 step:27009 [D loss: 0.312792, acc.: 86.72%] [G loss: 3.495495]\n",
      "epoch:34 step:27010 [D loss: 0.416309, acc.: 85.16%] [G loss: 5.456055]\n",
      "epoch:34 step:27011 [D loss: 0.470365, acc.: 76.56%] [G loss: 3.905148]\n",
      "epoch:34 step:27012 [D loss: 0.241470, acc.: 92.19%] [G loss: 4.182150]\n",
      "epoch:34 step:27013 [D loss: 0.299805, acc.: 85.16%] [G loss: 3.723985]\n",
      "epoch:34 step:27014 [D loss: 0.304673, acc.: 87.50%] [G loss: 3.428909]\n",
      "epoch:34 step:27015 [D loss: 0.264139, acc.: 85.16%] [G loss: 3.192100]\n",
      "epoch:34 step:27016 [D loss: 0.377233, acc.: 82.81%] [G loss: 2.923274]\n",
      "epoch:34 step:27017 [D loss: 0.392050, acc.: 80.47%] [G loss: 3.511163]\n",
      "epoch:34 step:27018 [D loss: 0.312842, acc.: 82.81%] [G loss: 3.794767]\n",
      "epoch:34 step:27019 [D loss: 0.344473, acc.: 85.94%] [G loss: 2.690570]\n",
      "epoch:34 step:27020 [D loss: 0.312263, acc.: 84.38%] [G loss: 3.648445]\n",
      "epoch:34 step:27021 [D loss: 0.298528, acc.: 86.72%] [G loss: 3.777011]\n",
      "epoch:34 step:27022 [D loss: 0.278934, acc.: 87.50%] [G loss: 2.945407]\n",
      "epoch:34 step:27023 [D loss: 0.264672, acc.: 89.06%] [G loss: 3.486582]\n",
      "epoch:34 step:27024 [D loss: 0.304623, acc.: 88.28%] [G loss: 3.490399]\n",
      "epoch:34 step:27025 [D loss: 0.206469, acc.: 93.75%] [G loss: 3.359440]\n",
      "epoch:34 step:27026 [D loss: 0.218058, acc.: 89.84%] [G loss: 3.649759]\n",
      "epoch:34 step:27027 [D loss: 0.321787, acc.: 88.28%] [G loss: 2.944032]\n",
      "epoch:34 step:27028 [D loss: 0.298038, acc.: 89.06%] [G loss: 5.331112]\n",
      "epoch:34 step:27029 [D loss: 0.429966, acc.: 85.16%] [G loss: 6.561911]\n",
      "epoch:34 step:27030 [D loss: 0.375323, acc.: 82.81%] [G loss: 3.467558]\n",
      "epoch:34 step:27031 [D loss: 0.284321, acc.: 88.28%] [G loss: 4.577748]\n",
      "epoch:34 step:27032 [D loss: 0.260780, acc.: 88.28%] [G loss: 3.958032]\n",
      "epoch:34 step:27033 [D loss: 0.322830, acc.: 85.16%] [G loss: 3.456060]\n",
      "epoch:34 step:27034 [D loss: 0.227226, acc.: 92.19%] [G loss: 3.096067]\n",
      "epoch:34 step:27035 [D loss: 0.321759, acc.: 81.25%] [G loss: 3.062426]\n",
      "epoch:34 step:27036 [D loss: 0.474596, acc.: 76.56%] [G loss: 3.225382]\n",
      "epoch:34 step:27037 [D loss: 0.421700, acc.: 78.12%] [G loss: 5.037735]\n",
      "epoch:34 step:27038 [D loss: 0.401473, acc.: 81.25%] [G loss: 4.765503]\n",
      "epoch:34 step:27039 [D loss: 0.471074, acc.: 81.25%] [G loss: 3.394276]\n",
      "epoch:34 step:27040 [D loss: 0.360269, acc.: 82.81%] [G loss: 3.967153]\n",
      "epoch:34 step:27041 [D loss: 0.318501, acc.: 85.94%] [G loss: 3.480081]\n",
      "epoch:34 step:27042 [D loss: 0.215082, acc.: 92.19%] [G loss: 3.949559]\n",
      "epoch:34 step:27043 [D loss: 0.268183, acc.: 90.62%] [G loss: 3.575181]\n",
      "epoch:34 step:27044 [D loss: 0.270959, acc.: 88.28%] [G loss: 2.852133]\n",
      "epoch:34 step:27045 [D loss: 0.372407, acc.: 85.16%] [G loss: 4.007968]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:34 step:27046 [D loss: 0.378758, acc.: 82.03%] [G loss: 3.860998]\n",
      "epoch:34 step:27047 [D loss: 0.423283, acc.: 80.47%] [G loss: 3.193459]\n",
      "epoch:34 step:27048 [D loss: 0.286013, acc.: 85.94%] [G loss: 3.149127]\n",
      "epoch:34 step:27049 [D loss: 0.310471, acc.: 85.16%] [G loss: 3.016790]\n",
      "epoch:34 step:27050 [D loss: 0.215099, acc.: 91.41%] [G loss: 2.945304]\n",
      "epoch:34 step:27051 [D loss: 0.301044, acc.: 88.28%] [G loss: 3.456645]\n",
      "epoch:34 step:27052 [D loss: 0.373948, acc.: 83.59%] [G loss: 2.966126]\n",
      "epoch:34 step:27053 [D loss: 0.302686, acc.: 89.06%] [G loss: 3.064171]\n",
      "epoch:34 step:27054 [D loss: 0.316611, acc.: 83.59%] [G loss: 3.232791]\n",
      "epoch:34 step:27055 [D loss: 0.311448, acc.: 87.50%] [G loss: 3.253377]\n",
      "epoch:34 step:27056 [D loss: 0.274746, acc.: 85.94%] [G loss: 3.490231]\n",
      "epoch:34 step:27057 [D loss: 0.263561, acc.: 86.72%] [G loss: 2.863597]\n",
      "epoch:34 step:27058 [D loss: 0.288424, acc.: 85.94%] [G loss: 3.230737]\n",
      "epoch:34 step:27059 [D loss: 0.289249, acc.: 86.72%] [G loss: 4.629425]\n",
      "epoch:34 step:27060 [D loss: 0.314788, acc.: 89.84%] [G loss: 3.643464]\n",
      "epoch:34 step:27061 [D loss: 0.304557, acc.: 87.50%] [G loss: 3.917757]\n",
      "epoch:34 step:27062 [D loss: 0.225802, acc.: 92.19%] [G loss: 3.713464]\n",
      "epoch:34 step:27063 [D loss: 0.259027, acc.: 89.06%] [G loss: 3.981453]\n",
      "epoch:34 step:27064 [D loss: 0.282350, acc.: 89.06%] [G loss: 3.643681]\n",
      "epoch:34 step:27065 [D loss: 0.315027, acc.: 85.94%] [G loss: 4.030650]\n",
      "epoch:34 step:27066 [D loss: 0.255697, acc.: 89.06%] [G loss: 3.683668]\n",
      "epoch:34 step:27067 [D loss: 0.363616, acc.: 85.16%] [G loss: 4.263454]\n",
      "epoch:34 step:27068 [D loss: 0.368623, acc.: 80.47%] [G loss: 4.381064]\n",
      "epoch:34 step:27069 [D loss: 0.253419, acc.: 89.84%] [G loss: 4.243865]\n",
      "epoch:34 step:27070 [D loss: 0.260042, acc.: 88.28%] [G loss: 4.738412]\n",
      "epoch:34 step:27071 [D loss: 0.369874, acc.: 83.59%] [G loss: 6.026680]\n",
      "epoch:34 step:27072 [D loss: 0.310536, acc.: 86.72%] [G loss: 3.689433]\n",
      "epoch:34 step:27073 [D loss: 0.265312, acc.: 88.28%] [G loss: 5.452418]\n",
      "epoch:34 step:27074 [D loss: 0.454524, acc.: 79.69%] [G loss: 5.216001]\n",
      "epoch:34 step:27075 [D loss: 0.421801, acc.: 79.69%] [G loss: 3.866846]\n",
      "epoch:34 step:27076 [D loss: 0.245989, acc.: 91.41%] [G loss: 3.633834]\n",
      "epoch:34 step:27077 [D loss: 0.233545, acc.: 89.84%] [G loss: 4.277640]\n",
      "epoch:34 step:27078 [D loss: 0.340574, acc.: 85.94%] [G loss: 2.589481]\n",
      "epoch:34 step:27079 [D loss: 0.337723, acc.: 83.59%] [G loss: 4.037910]\n",
      "epoch:34 step:27080 [D loss: 0.402141, acc.: 81.25%] [G loss: 4.734291]\n",
      "epoch:34 step:27081 [D loss: 0.514632, acc.: 77.34%] [G loss: 4.620525]\n",
      "epoch:34 step:27082 [D loss: 0.313385, acc.: 80.47%] [G loss: 4.390773]\n",
      "epoch:34 step:27083 [D loss: 0.351847, acc.: 79.69%] [G loss: 5.074169]\n",
      "epoch:34 step:27084 [D loss: 0.422126, acc.: 82.03%] [G loss: 4.416137]\n",
      "epoch:34 step:27085 [D loss: 0.405495, acc.: 82.81%] [G loss: 4.257689]\n",
      "epoch:34 step:27086 [D loss: 0.409847, acc.: 78.91%] [G loss: 3.221179]\n",
      "epoch:34 step:27087 [D loss: 0.270998, acc.: 89.84%] [G loss: 2.927032]\n",
      "epoch:34 step:27088 [D loss: 0.283847, acc.: 85.94%] [G loss: 2.666198]\n",
      "epoch:34 step:27089 [D loss: 0.287535, acc.: 87.50%] [G loss: 2.906507]\n",
      "epoch:34 step:27090 [D loss: 0.296199, acc.: 86.72%] [G loss: 2.809389]\n",
      "epoch:34 step:27091 [D loss: 0.383864, acc.: 83.59%] [G loss: 2.509406]\n",
      "epoch:34 step:27092 [D loss: 0.326532, acc.: 85.16%] [G loss: 3.033216]\n",
      "epoch:34 step:27093 [D loss: 0.219529, acc.: 90.62%] [G loss: 3.780493]\n",
      "epoch:34 step:27094 [D loss: 0.292479, acc.: 85.94%] [G loss: 3.858203]\n",
      "epoch:34 step:27095 [D loss: 0.270297, acc.: 89.06%] [G loss: 3.221580]\n",
      "epoch:34 step:27096 [D loss: 0.304619, acc.: 84.38%] [G loss: 3.409380]\n",
      "epoch:34 step:27097 [D loss: 0.297457, acc.: 85.16%] [G loss: 4.098071]\n",
      "epoch:34 step:27098 [D loss: 0.358657, acc.: 82.03%] [G loss: 3.443674]\n",
      "epoch:34 step:27099 [D loss: 0.305404, acc.: 88.28%] [G loss: 4.411063]\n",
      "epoch:34 step:27100 [D loss: 0.251106, acc.: 90.62%] [G loss: 3.863982]\n",
      "epoch:34 step:27101 [D loss: 0.325360, acc.: 85.94%] [G loss: 3.789647]\n",
      "epoch:34 step:27102 [D loss: 0.285646, acc.: 89.06%] [G loss: 3.560561]\n",
      "epoch:34 step:27103 [D loss: 0.248429, acc.: 88.28%] [G loss: 3.439454]\n",
      "epoch:34 step:27104 [D loss: 0.317538, acc.: 86.72%] [G loss: 2.413473]\n",
      "epoch:34 step:27105 [D loss: 0.392377, acc.: 82.03%] [G loss: 2.852684]\n",
      "epoch:34 step:27106 [D loss: 0.480893, acc.: 79.69%] [G loss: 2.703821]\n",
      "epoch:34 step:27107 [D loss: 0.275872, acc.: 86.72%] [G loss: 2.941739]\n",
      "epoch:34 step:27108 [D loss: 0.314774, acc.: 84.38%] [G loss: 2.049571]\n",
      "epoch:34 step:27109 [D loss: 0.329904, acc.: 86.72%] [G loss: 2.460927]\n",
      "epoch:34 step:27110 [D loss: 0.301482, acc.: 83.59%] [G loss: 2.536269]\n",
      "epoch:34 step:27111 [D loss: 0.324067, acc.: 84.38%] [G loss: 2.406162]\n",
      "epoch:34 step:27112 [D loss: 0.349751, acc.: 84.38%] [G loss: 3.000796]\n",
      "epoch:34 step:27113 [D loss: 0.295025, acc.: 88.28%] [G loss: 3.111048]\n",
      "epoch:34 step:27114 [D loss: 0.211711, acc.: 90.62%] [G loss: 4.149659]\n",
      "epoch:34 step:27115 [D loss: 0.368305, acc.: 82.03%] [G loss: 2.753353]\n",
      "epoch:34 step:27116 [D loss: 0.346040, acc.: 85.16%] [G loss: 2.719189]\n",
      "epoch:34 step:27117 [D loss: 0.280424, acc.: 85.94%] [G loss: 3.413097]\n",
      "epoch:34 step:27118 [D loss: 0.243051, acc.: 88.28%] [G loss: 3.392064]\n",
      "epoch:34 step:27119 [D loss: 0.263223, acc.: 86.72%] [G loss: 4.104846]\n",
      "epoch:34 step:27120 [D loss: 0.254884, acc.: 90.62%] [G loss: 2.996059]\n",
      "epoch:34 step:27121 [D loss: 0.233176, acc.: 89.84%] [G loss: 4.620344]\n",
      "epoch:34 step:27122 [D loss: 0.344716, acc.: 83.59%] [G loss: 3.445112]\n",
      "epoch:34 step:27123 [D loss: 0.228081, acc.: 89.84%] [G loss: 3.629013]\n",
      "epoch:34 step:27124 [D loss: 0.321422, acc.: 87.50%] [G loss: 3.388250]\n",
      "epoch:34 step:27125 [D loss: 0.344140, acc.: 85.94%] [G loss: 4.327259]\n",
      "epoch:34 step:27126 [D loss: 0.347411, acc.: 83.59%] [G loss: 2.816323]\n",
      "epoch:34 step:27127 [D loss: 0.288248, acc.: 86.72%] [G loss: 4.547761]\n",
      "epoch:34 step:27128 [D loss: 0.334703, acc.: 85.94%] [G loss: 4.296235]\n",
      "epoch:34 step:27129 [D loss: 0.278847, acc.: 88.28%] [G loss: 4.055605]\n",
      "epoch:34 step:27130 [D loss: 0.220387, acc.: 89.06%] [G loss: 4.029766]\n",
      "epoch:34 step:27131 [D loss: 0.289667, acc.: 88.28%] [G loss: 2.801430]\n",
      "epoch:34 step:27132 [D loss: 0.260893, acc.: 91.41%] [G loss: 3.096993]\n",
      "epoch:34 step:27133 [D loss: 0.266193, acc.: 88.28%] [G loss: 3.696979]\n",
      "epoch:34 step:27134 [D loss: 0.260340, acc.: 86.72%] [G loss: 5.024991]\n",
      "epoch:34 step:27135 [D loss: 0.344178, acc.: 82.81%] [G loss: 2.407595]\n",
      "epoch:34 step:27136 [D loss: 0.283437, acc.: 87.50%] [G loss: 3.630802]\n",
      "epoch:34 step:27137 [D loss: 0.361128, acc.: 82.03%] [G loss: 4.123738]\n",
      "epoch:34 step:27138 [D loss: 0.359388, acc.: 82.03%] [G loss: 3.021919]\n",
      "epoch:34 step:27139 [D loss: 0.276021, acc.: 88.28%] [G loss: 3.562204]\n",
      "epoch:34 step:27140 [D loss: 0.373159, acc.: 85.16%] [G loss: 4.707133]\n",
      "epoch:34 step:27141 [D loss: 0.297662, acc.: 88.28%] [G loss: 3.173338]\n",
      "epoch:34 step:27142 [D loss: 0.287144, acc.: 90.62%] [G loss: 6.357669]\n",
      "epoch:34 step:27143 [D loss: 0.437565, acc.: 80.47%] [G loss: 3.922132]\n",
      "epoch:34 step:27144 [D loss: 0.559217, acc.: 78.91%] [G loss: 4.848696]\n",
      "epoch:34 step:27145 [D loss: 0.557975, acc.: 75.78%] [G loss: 6.316602]\n",
      "epoch:34 step:27146 [D loss: 0.529012, acc.: 80.47%] [G loss: 7.025151]\n",
      "epoch:34 step:27147 [D loss: 0.170341, acc.: 93.75%] [G loss: 5.300052]\n",
      "epoch:34 step:27148 [D loss: 0.254706, acc.: 89.84%] [G loss: 4.109816]\n",
      "epoch:34 step:27149 [D loss: 0.280879, acc.: 86.72%] [G loss: 4.619610]\n",
      "epoch:34 step:27150 [D loss: 0.212876, acc.: 91.41%] [G loss: 3.817388]\n",
      "epoch:34 step:27151 [D loss: 0.227934, acc.: 89.06%] [G loss: 4.387089]\n",
      "epoch:34 step:27152 [D loss: 0.241758, acc.: 90.62%] [G loss: 3.126113]\n",
      "epoch:34 step:27153 [D loss: 0.246578, acc.: 90.62%] [G loss: 4.251494]\n",
      "epoch:34 step:27154 [D loss: 0.413745, acc.: 83.59%] [G loss: 3.008495]\n",
      "epoch:34 step:27155 [D loss: 0.358694, acc.: 82.03%] [G loss: 4.446039]\n",
      "epoch:34 step:27156 [D loss: 0.386152, acc.: 81.25%] [G loss: 3.975704]\n",
      "epoch:34 step:27157 [D loss: 0.465138, acc.: 78.91%] [G loss: 4.822108]\n",
      "epoch:34 step:27158 [D loss: 0.337821, acc.: 84.38%] [G loss: 3.416168]\n",
      "epoch:34 step:27159 [D loss: 0.362077, acc.: 83.59%] [G loss: 5.241781]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:34 step:27160 [D loss: 0.327344, acc.: 85.16%] [G loss: 5.034010]\n",
      "epoch:34 step:27161 [D loss: 0.265690, acc.: 88.28%] [G loss: 4.444377]\n",
      "epoch:34 step:27162 [D loss: 0.234083, acc.: 89.06%] [G loss: 4.658154]\n",
      "epoch:34 step:27163 [D loss: 0.400583, acc.: 83.59%] [G loss: 3.676894]\n",
      "epoch:34 step:27164 [D loss: 0.417197, acc.: 81.25%] [G loss: 3.223989]\n",
      "epoch:34 step:27165 [D loss: 0.357320, acc.: 80.47%] [G loss: 3.526390]\n",
      "epoch:34 step:27166 [D loss: 0.171774, acc.: 96.09%] [G loss: 2.637353]\n",
      "epoch:34 step:27167 [D loss: 0.455012, acc.: 78.91%] [G loss: 3.064772]\n",
      "epoch:34 step:27168 [D loss: 0.308055, acc.: 85.94%] [G loss: 2.819834]\n",
      "epoch:34 step:27169 [D loss: 0.313078, acc.: 85.94%] [G loss: 4.326118]\n",
      "epoch:34 step:27170 [D loss: 0.294519, acc.: 84.38%] [G loss: 4.849465]\n",
      "epoch:34 step:27171 [D loss: 0.339854, acc.: 84.38%] [G loss: 3.187063]\n",
      "epoch:34 step:27172 [D loss: 0.379497, acc.: 81.25%] [G loss: 3.309228]\n",
      "epoch:34 step:27173 [D loss: 0.246262, acc.: 89.84%] [G loss: 4.137551]\n",
      "epoch:34 step:27174 [D loss: 0.364574, acc.: 84.38%] [G loss: 3.776701]\n",
      "epoch:34 step:27175 [D loss: 0.288194, acc.: 90.62%] [G loss: 4.114192]\n",
      "epoch:34 step:27176 [D loss: 0.291366, acc.: 85.16%] [G loss: 4.061595]\n",
      "epoch:34 step:27177 [D loss: 0.382467, acc.: 81.25%] [G loss: 3.119262]\n",
      "epoch:34 step:27178 [D loss: 0.262854, acc.: 88.28%] [G loss: 3.481032]\n",
      "epoch:34 step:27179 [D loss: 0.310712, acc.: 88.28%] [G loss: 3.560535]\n",
      "epoch:34 step:27180 [D loss: 0.286113, acc.: 88.28%] [G loss: 2.603055]\n",
      "epoch:34 step:27181 [D loss: 0.256622, acc.: 89.84%] [G loss: 3.884809]\n",
      "epoch:34 step:27182 [D loss: 0.367196, acc.: 80.47%] [G loss: 4.250048]\n",
      "epoch:34 step:27183 [D loss: 0.361941, acc.: 80.47%] [G loss: 3.627246]\n",
      "epoch:34 step:27184 [D loss: 0.234188, acc.: 89.06%] [G loss: 3.130981]\n",
      "epoch:34 step:27185 [D loss: 0.383063, acc.: 78.12%] [G loss: 2.963039]\n",
      "epoch:34 step:27186 [D loss: 0.314573, acc.: 85.94%] [G loss: 2.657334]\n",
      "epoch:34 step:27187 [D loss: 0.322014, acc.: 80.47%] [G loss: 3.583235]\n",
      "epoch:34 step:27188 [D loss: 0.361804, acc.: 83.59%] [G loss: 2.977632]\n",
      "epoch:34 step:27189 [D loss: 0.305595, acc.: 82.81%] [G loss: 3.363893]\n",
      "epoch:34 step:27190 [D loss: 0.208750, acc.: 93.75%] [G loss: 3.768894]\n",
      "epoch:34 step:27191 [D loss: 0.330143, acc.: 83.59%] [G loss: 4.348269]\n",
      "epoch:34 step:27192 [D loss: 0.231597, acc.: 87.50%] [G loss: 3.330863]\n",
      "epoch:34 step:27193 [D loss: 0.390729, acc.: 81.25%] [G loss: 3.850822]\n",
      "epoch:34 step:27194 [D loss: 0.226166, acc.: 89.84%] [G loss: 3.999718]\n",
      "epoch:34 step:27195 [D loss: 0.264708, acc.: 88.28%] [G loss: 3.845366]\n",
      "epoch:34 step:27196 [D loss: 0.294504, acc.: 89.84%] [G loss: 3.044123]\n",
      "epoch:34 step:27197 [D loss: 0.311745, acc.: 85.16%] [G loss: 2.874080]\n",
      "epoch:34 step:27198 [D loss: 0.330351, acc.: 82.03%] [G loss: 2.852642]\n",
      "epoch:34 step:27199 [D loss: 0.295103, acc.: 85.16%] [G loss: 3.544981]\n",
      "epoch:34 step:27200 [D loss: 0.476809, acc.: 78.91%] [G loss: 3.533744]\n",
      "##############\n",
      "[0.86247245 0.86687137 0.8144454  0.81558306 0.79092079 0.83626419\n",
      " 0.87038328 0.83087125 0.80233612 0.83982236]\n",
      "##########\n",
      "epoch:34 step:27201 [D loss: 0.356211, acc.: 80.47%] [G loss: 2.597013]\n",
      "epoch:34 step:27202 [D loss: 0.304660, acc.: 85.94%] [G loss: 2.785612]\n",
      "epoch:34 step:27203 [D loss: 0.288769, acc.: 85.16%] [G loss: 3.583745]\n",
      "epoch:34 step:27204 [D loss: 0.475409, acc.: 77.34%] [G loss: 6.207860]\n",
      "epoch:34 step:27205 [D loss: 0.623614, acc.: 73.44%] [G loss: 5.806832]\n",
      "epoch:34 step:27206 [D loss: 0.595473, acc.: 78.91%] [G loss: 9.142042]\n",
      "epoch:34 step:27207 [D loss: 1.274219, acc.: 64.84%] [G loss: 6.332798]\n",
      "epoch:34 step:27208 [D loss: 0.574429, acc.: 75.78%] [G loss: 3.116511]\n",
      "epoch:34 step:27209 [D loss: 0.369263, acc.: 85.16%] [G loss: 4.223457]\n",
      "epoch:34 step:27210 [D loss: 0.349656, acc.: 84.38%] [G loss: 3.179552]\n",
      "epoch:34 step:27211 [D loss: 0.409317, acc.: 86.72%] [G loss: 3.732916]\n",
      "epoch:34 step:27212 [D loss: 0.318799, acc.: 85.16%] [G loss: 3.330197]\n",
      "epoch:34 step:27213 [D loss: 0.388782, acc.: 83.59%] [G loss: 2.885184]\n",
      "epoch:34 step:27214 [D loss: 0.315215, acc.: 86.72%] [G loss: 2.734560]\n",
      "epoch:34 step:27215 [D loss: 0.436863, acc.: 79.69%] [G loss: 3.433932]\n",
      "epoch:34 step:27216 [D loss: 0.329627, acc.: 82.03%] [G loss: 3.350051]\n",
      "epoch:34 step:27217 [D loss: 0.257413, acc.: 89.06%] [G loss: 3.565347]\n",
      "epoch:34 step:27218 [D loss: 0.417026, acc.: 81.25%] [G loss: 2.490871]\n",
      "epoch:34 step:27219 [D loss: 0.274700, acc.: 89.06%] [G loss: 3.163109]\n",
      "epoch:34 step:27220 [D loss: 0.352151, acc.: 86.72%] [G loss: 2.804922]\n",
      "epoch:34 step:27221 [D loss: 0.319118, acc.: 84.38%] [G loss: 2.774363]\n",
      "epoch:34 step:27222 [D loss: 0.353670, acc.: 81.25%] [G loss: 3.177311]\n",
      "epoch:34 step:27223 [D loss: 0.471922, acc.: 79.69%] [G loss: 3.083686]\n",
      "epoch:34 step:27224 [D loss: 0.290513, acc.: 85.94%] [G loss: 3.323860]\n",
      "epoch:34 step:27225 [D loss: 0.324197, acc.: 86.72%] [G loss: 2.951105]\n",
      "epoch:34 step:27226 [D loss: 0.216758, acc.: 89.84%] [G loss: 3.481928]\n",
      "epoch:34 step:27227 [D loss: 0.301998, acc.: 87.50%] [G loss: 2.389091]\n",
      "epoch:34 step:27228 [D loss: 0.324289, acc.: 83.59%] [G loss: 3.190509]\n",
      "epoch:34 step:27229 [D loss: 0.331805, acc.: 83.59%] [G loss: 2.640383]\n",
      "epoch:34 step:27230 [D loss: 0.341866, acc.: 83.59%] [G loss: 3.626523]\n",
      "epoch:34 step:27231 [D loss: 0.340751, acc.: 86.72%] [G loss: 3.154628]\n",
      "epoch:34 step:27232 [D loss: 0.373194, acc.: 84.38%] [G loss: 2.900071]\n",
      "epoch:34 step:27233 [D loss: 0.332684, acc.: 86.72%] [G loss: 2.700055]\n",
      "epoch:34 step:27234 [D loss: 0.341380, acc.: 84.38%] [G loss: 2.549006]\n",
      "epoch:34 step:27235 [D loss: 0.334218, acc.: 89.06%] [G loss: 2.904170]\n",
      "epoch:34 step:27236 [D loss: 0.405990, acc.: 82.81%] [G loss: 2.277590]\n",
      "epoch:34 step:27237 [D loss: 0.382657, acc.: 82.03%] [G loss: 2.430631]\n",
      "epoch:34 step:27238 [D loss: 0.392056, acc.: 88.28%] [G loss: 2.540766]\n",
      "epoch:34 step:27239 [D loss: 0.367743, acc.: 83.59%] [G loss: 3.806027]\n",
      "epoch:34 step:27240 [D loss: 0.271486, acc.: 90.62%] [G loss: 3.706301]\n",
      "epoch:34 step:27241 [D loss: 0.299342, acc.: 86.72%] [G loss: 3.434122]\n",
      "epoch:34 step:27242 [D loss: 0.411740, acc.: 77.34%] [G loss: 2.584623]\n",
      "epoch:34 step:27243 [D loss: 0.309221, acc.: 85.16%] [G loss: 3.772151]\n",
      "epoch:34 step:27244 [D loss: 0.253531, acc.: 91.41%] [G loss: 2.702855]\n",
      "epoch:34 step:27245 [D loss: 0.299682, acc.: 87.50%] [G loss: 2.665557]\n",
      "epoch:34 step:27246 [D loss: 0.395952, acc.: 82.81%] [G loss: 2.669257]\n",
      "epoch:34 step:27247 [D loss: 0.267342, acc.: 86.72%] [G loss: 2.542593]\n",
      "epoch:34 step:27248 [D loss: 0.315548, acc.: 86.72%] [G loss: 2.294027]\n",
      "epoch:34 step:27249 [D loss: 0.293880, acc.: 88.28%] [G loss: 2.901514]\n",
      "epoch:34 step:27250 [D loss: 0.277934, acc.: 89.84%] [G loss: 2.350245]\n",
      "epoch:34 step:27251 [D loss: 0.361111, acc.: 81.25%] [G loss: 2.675951]\n",
      "epoch:34 step:27252 [D loss: 0.329049, acc.: 86.72%] [G loss: 2.543933]\n",
      "epoch:34 step:27253 [D loss: 0.361206, acc.: 86.72%] [G loss: 2.098547]\n",
      "epoch:34 step:27254 [D loss: 0.267771, acc.: 89.84%] [G loss: 2.991460]\n",
      "epoch:34 step:27255 [D loss: 0.309026, acc.: 85.94%] [G loss: 3.218068]\n",
      "epoch:34 step:27256 [D loss: 0.293988, acc.: 85.94%] [G loss: 3.130053]\n",
      "epoch:34 step:27257 [D loss: 0.416788, acc.: 78.91%] [G loss: 3.179704]\n",
      "epoch:34 step:27258 [D loss: 0.376445, acc.: 82.03%] [G loss: 2.928610]\n",
      "epoch:34 step:27259 [D loss: 0.349580, acc.: 85.16%] [G loss: 4.946365]\n",
      "epoch:34 step:27260 [D loss: 0.368430, acc.: 82.81%] [G loss: 6.171788]\n",
      "epoch:34 step:27261 [D loss: 0.507509, acc.: 75.00%] [G loss: 3.030226]\n",
      "epoch:34 step:27262 [D loss: 0.253314, acc.: 89.06%] [G loss: 4.011933]\n",
      "epoch:34 step:27263 [D loss: 0.406940, acc.: 83.59%] [G loss: 4.600643]\n",
      "epoch:34 step:27264 [D loss: 0.401791, acc.: 78.12%] [G loss: 3.290569]\n",
      "epoch:34 step:27265 [D loss: 0.244355, acc.: 89.06%] [G loss: 4.259618]\n",
      "epoch:34 step:27266 [D loss: 0.376868, acc.: 82.03%] [G loss: 4.179996]\n",
      "epoch:34 step:27267 [D loss: 0.345239, acc.: 85.16%] [G loss: 3.493824]\n",
      "epoch:34 step:27268 [D loss: 0.303640, acc.: 89.06%] [G loss: 2.937605]\n",
      "epoch:34 step:27269 [D loss: 0.291128, acc.: 85.94%] [G loss: 3.309806]\n",
      "epoch:34 step:27270 [D loss: 0.385310, acc.: 81.25%] [G loss: 2.789015]\n",
      "epoch:34 step:27271 [D loss: 0.259757, acc.: 91.41%] [G loss: 2.962322]\n",
      "epoch:34 step:27272 [D loss: 0.332973, acc.: 84.38%] [G loss: 4.032649]\n",
      "epoch:34 step:27273 [D loss: 0.418336, acc.: 81.25%] [G loss: 2.498143]\n",
      "epoch:34 step:27274 [D loss: 0.235994, acc.: 90.62%] [G loss: 3.481034]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:34 step:27275 [D loss: 0.342722, acc.: 85.94%] [G loss: 4.873760]\n",
      "epoch:34 step:27276 [D loss: 0.300018, acc.: 85.16%] [G loss: 3.436457]\n",
      "epoch:34 step:27277 [D loss: 0.299220, acc.: 87.50%] [G loss: 3.094606]\n",
      "epoch:34 step:27278 [D loss: 0.223790, acc.: 92.97%] [G loss: 2.673303]\n",
      "epoch:34 step:27279 [D loss: 0.284438, acc.: 89.06%] [G loss: 3.067286]\n",
      "epoch:34 step:27280 [D loss: 0.238393, acc.: 91.41%] [G loss: 3.146300]\n",
      "epoch:34 step:27281 [D loss: 0.314809, acc.: 88.28%] [G loss: 2.658454]\n",
      "epoch:34 step:27282 [D loss: 0.339807, acc.: 83.59%] [G loss: 2.935489]\n",
      "epoch:34 step:27283 [D loss: 0.304470, acc.: 83.59%] [G loss: 3.142658]\n",
      "epoch:34 step:27284 [D loss: 0.244294, acc.: 90.62%] [G loss: 3.366354]\n",
      "epoch:34 step:27285 [D loss: 0.339949, acc.: 83.59%] [G loss: 2.711161]\n",
      "epoch:34 step:27286 [D loss: 0.342525, acc.: 84.38%] [G loss: 2.848251]\n",
      "epoch:34 step:27287 [D loss: 0.217151, acc.: 89.84%] [G loss: 2.936265]\n",
      "epoch:34 step:27288 [D loss: 0.296432, acc.: 88.28%] [G loss: 2.775736]\n",
      "epoch:34 step:27289 [D loss: 0.333889, acc.: 86.72%] [G loss: 2.512425]\n",
      "epoch:34 step:27290 [D loss: 0.269160, acc.: 90.62%] [G loss: 2.320581]\n",
      "epoch:34 step:27291 [D loss: 0.250162, acc.: 89.84%] [G loss: 3.146880]\n",
      "epoch:34 step:27292 [D loss: 0.328748, acc.: 83.59%] [G loss: 3.496047]\n",
      "epoch:34 step:27293 [D loss: 0.379888, acc.: 81.25%] [G loss: 2.692694]\n",
      "epoch:34 step:27294 [D loss: 0.296564, acc.: 87.50%] [G loss: 2.738976]\n",
      "epoch:34 step:27295 [D loss: 0.271293, acc.: 85.94%] [G loss: 2.982156]\n",
      "epoch:34 step:27296 [D loss: 0.357055, acc.: 82.81%] [G loss: 4.054000]\n",
      "epoch:34 step:27297 [D loss: 0.408807, acc.: 79.69%] [G loss: 5.721005]\n",
      "epoch:34 step:27298 [D loss: 0.273940, acc.: 89.06%] [G loss: 2.891412]\n",
      "epoch:34 step:27299 [D loss: 0.241670, acc.: 92.19%] [G loss: 3.376336]\n",
      "epoch:34 step:27300 [D loss: 0.346493, acc.: 85.94%] [G loss: 2.535511]\n",
      "epoch:34 step:27301 [D loss: 0.351093, acc.: 83.59%] [G loss: 3.512807]\n",
      "epoch:34 step:27302 [D loss: 0.236401, acc.: 91.41%] [G loss: 3.130474]\n",
      "epoch:34 step:27303 [D loss: 0.304873, acc.: 86.72%] [G loss: 4.820458]\n",
      "epoch:34 step:27304 [D loss: 0.558018, acc.: 74.22%] [G loss: 4.532922]\n",
      "epoch:34 step:27305 [D loss: 0.420170, acc.: 83.59%] [G loss: 4.846216]\n",
      "epoch:34 step:27306 [D loss: 0.492951, acc.: 81.25%] [G loss: 5.697123]\n",
      "epoch:34 step:27307 [D loss: 0.812207, acc.: 63.28%] [G loss: 4.536329]\n",
      "epoch:34 step:27308 [D loss: 0.624163, acc.: 74.22%] [G loss: 3.167520]\n",
      "epoch:34 step:27309 [D loss: 0.434362, acc.: 82.81%] [G loss: 2.679721]\n",
      "epoch:34 step:27310 [D loss: 0.292844, acc.: 89.06%] [G loss: 3.582671]\n",
      "epoch:34 step:27311 [D loss: 0.332104, acc.: 85.94%] [G loss: 2.686142]\n",
      "epoch:34 step:27312 [D loss: 0.348227, acc.: 84.38%] [G loss: 2.368337]\n",
      "epoch:34 step:27313 [D loss: 0.233774, acc.: 88.28%] [G loss: 3.146849]\n",
      "epoch:34 step:27314 [D loss: 0.326046, acc.: 84.38%] [G loss: 2.776495]\n",
      "epoch:34 step:27315 [D loss: 0.243387, acc.: 89.84%] [G loss: 3.643042]\n",
      "epoch:34 step:27316 [D loss: 0.395927, acc.: 81.25%] [G loss: 2.286594]\n",
      "epoch:34 step:27317 [D loss: 0.351537, acc.: 82.03%] [G loss: 2.434274]\n",
      "epoch:34 step:27318 [D loss: 0.309852, acc.: 84.38%] [G loss: 3.126886]\n",
      "epoch:34 step:27319 [D loss: 0.391722, acc.: 79.69%] [G loss: 2.546085]\n",
      "epoch:34 step:27320 [D loss: 0.381693, acc.: 84.38%] [G loss: 2.560074]\n",
      "epoch:34 step:27321 [D loss: 0.446315, acc.: 79.69%] [G loss: 5.696344]\n",
      "epoch:34 step:27322 [D loss: 0.408153, acc.: 84.38%] [G loss: 2.859837]\n",
      "epoch:34 step:27323 [D loss: 0.293541, acc.: 85.94%] [G loss: 4.944185]\n",
      "epoch:34 step:27324 [D loss: 0.277549, acc.: 88.28%] [G loss: 3.837168]\n",
      "epoch:34 step:27325 [D loss: 0.283493, acc.: 88.28%] [G loss: 4.494359]\n",
      "epoch:34 step:27326 [D loss: 0.270906, acc.: 89.84%] [G loss: 3.323945]\n",
      "epoch:34 step:27327 [D loss: 0.255330, acc.: 92.19%] [G loss: 4.596447]\n",
      "epoch:34 step:27328 [D loss: 0.336990, acc.: 82.81%] [G loss: 3.084665]\n",
      "epoch:34 step:27329 [D loss: 0.266098, acc.: 87.50%] [G loss: 3.175665]\n",
      "epoch:34 step:27330 [D loss: 0.226640, acc.: 89.06%] [G loss: 4.783933]\n",
      "epoch:34 step:27331 [D loss: 0.235113, acc.: 89.84%] [G loss: 3.718668]\n",
      "epoch:34 step:27332 [D loss: 0.250224, acc.: 91.41%] [G loss: 4.537943]\n",
      "epoch:34 step:27333 [D loss: 0.196793, acc.: 92.97%] [G loss: 3.346542]\n",
      "epoch:34 step:27334 [D loss: 0.249788, acc.: 89.06%] [G loss: 3.602594]\n",
      "epoch:34 step:27335 [D loss: 0.340344, acc.: 83.59%] [G loss: 3.268472]\n",
      "epoch:35 step:27336 [D loss: 0.287128, acc.: 88.28%] [G loss: 3.819190]\n",
      "epoch:35 step:27337 [D loss: 0.283824, acc.: 88.28%] [G loss: 4.473570]\n",
      "epoch:35 step:27338 [D loss: 0.307819, acc.: 86.72%] [G loss: 4.110376]\n",
      "epoch:35 step:27339 [D loss: 0.357484, acc.: 82.81%] [G loss: 4.786932]\n",
      "epoch:35 step:27340 [D loss: 0.345084, acc.: 82.81%] [G loss: 4.949177]\n",
      "epoch:35 step:27341 [D loss: 0.281308, acc.: 85.16%] [G loss: 4.504275]\n",
      "epoch:35 step:27342 [D loss: 0.223726, acc.: 90.62%] [G loss: 3.261872]\n",
      "epoch:35 step:27343 [D loss: 0.232248, acc.: 92.19%] [G loss: 3.899758]\n",
      "epoch:35 step:27344 [D loss: 0.295713, acc.: 86.72%] [G loss: 3.650682]\n",
      "epoch:35 step:27345 [D loss: 0.318119, acc.: 85.94%] [G loss: 3.377870]\n",
      "epoch:35 step:27346 [D loss: 0.310148, acc.: 86.72%] [G loss: 3.380568]\n",
      "epoch:35 step:27347 [D loss: 0.337078, acc.: 82.81%] [G loss: 3.655811]\n",
      "epoch:35 step:27348 [D loss: 0.359578, acc.: 84.38%] [G loss: 2.456331]\n",
      "epoch:35 step:27349 [D loss: 0.313361, acc.: 84.38%] [G loss: 3.387761]\n",
      "epoch:35 step:27350 [D loss: 0.335095, acc.: 83.59%] [G loss: 3.967852]\n",
      "epoch:35 step:27351 [D loss: 0.255771, acc.: 88.28%] [G loss: 3.556621]\n",
      "epoch:35 step:27352 [D loss: 0.223015, acc.: 89.06%] [G loss: 4.028242]\n",
      "epoch:35 step:27353 [D loss: 0.256541, acc.: 89.84%] [G loss: 5.133343]\n",
      "epoch:35 step:27354 [D loss: 0.390718, acc.: 80.47%] [G loss: 2.939068]\n",
      "epoch:35 step:27355 [D loss: 0.276270, acc.: 86.72%] [G loss: 3.593931]\n",
      "epoch:35 step:27356 [D loss: 0.340229, acc.: 85.94%] [G loss: 3.394924]\n",
      "epoch:35 step:27357 [D loss: 0.346391, acc.: 85.16%] [G loss: 2.710625]\n",
      "epoch:35 step:27358 [D loss: 0.335402, acc.: 82.81%] [G loss: 3.906314]\n",
      "epoch:35 step:27359 [D loss: 0.401771, acc.: 82.03%] [G loss: 3.383991]\n",
      "epoch:35 step:27360 [D loss: 0.307981, acc.: 87.50%] [G loss: 3.359103]\n",
      "epoch:35 step:27361 [D loss: 0.256847, acc.: 89.06%] [G loss: 4.330602]\n",
      "epoch:35 step:27362 [D loss: 0.324902, acc.: 87.50%] [G loss: 3.328861]\n",
      "epoch:35 step:27363 [D loss: 0.378678, acc.: 82.03%] [G loss: 4.946667]\n",
      "epoch:35 step:27364 [D loss: 0.429137, acc.: 80.47%] [G loss: 3.030831]\n",
      "epoch:35 step:27365 [D loss: 0.270669, acc.: 89.06%] [G loss: 4.705565]\n",
      "epoch:35 step:27366 [D loss: 0.248996, acc.: 88.28%] [G loss: 4.808827]\n",
      "epoch:35 step:27367 [D loss: 0.312942, acc.: 85.94%] [G loss: 3.522500]\n",
      "epoch:35 step:27368 [D loss: 0.292030, acc.: 86.72%] [G loss: 3.464075]\n",
      "epoch:35 step:27369 [D loss: 0.396747, acc.: 80.47%] [G loss: 3.373591]\n",
      "epoch:35 step:27370 [D loss: 0.416429, acc.: 81.25%] [G loss: 2.694972]\n",
      "epoch:35 step:27371 [D loss: 0.296782, acc.: 87.50%] [G loss: 3.030053]\n",
      "epoch:35 step:27372 [D loss: 0.329441, acc.: 87.50%] [G loss: 3.531857]\n",
      "epoch:35 step:27373 [D loss: 0.411699, acc.: 82.03%] [G loss: 5.451766]\n",
      "epoch:35 step:27374 [D loss: 0.401987, acc.: 84.38%] [G loss: 4.218859]\n",
      "epoch:35 step:27375 [D loss: 0.431973, acc.: 80.47%] [G loss: 4.202405]\n",
      "epoch:35 step:27376 [D loss: 0.295010, acc.: 85.94%] [G loss: 4.038772]\n",
      "epoch:35 step:27377 [D loss: 0.271774, acc.: 88.28%] [G loss: 3.595437]\n",
      "epoch:35 step:27378 [D loss: 0.379696, acc.: 83.59%] [G loss: 3.346123]\n",
      "epoch:35 step:27379 [D loss: 0.283408, acc.: 90.62%] [G loss: 3.534425]\n",
      "epoch:35 step:27380 [D loss: 0.352938, acc.: 81.25%] [G loss: 2.895016]\n",
      "epoch:35 step:27381 [D loss: 0.383162, acc.: 82.81%] [G loss: 3.281988]\n",
      "epoch:35 step:27382 [D loss: 0.364124, acc.: 86.72%] [G loss: 3.726578]\n",
      "epoch:35 step:27383 [D loss: 0.351122, acc.: 81.25%] [G loss: 2.259362]\n",
      "epoch:35 step:27384 [D loss: 0.212842, acc.: 92.97%] [G loss: 3.188772]\n",
      "epoch:35 step:27385 [D loss: 0.366041, acc.: 82.81%] [G loss: 3.274649]\n",
      "epoch:35 step:27386 [D loss: 0.280993, acc.: 86.72%] [G loss: 3.273689]\n",
      "epoch:35 step:27387 [D loss: 0.274835, acc.: 87.50%] [G loss: 3.784460]\n",
      "epoch:35 step:27388 [D loss: 0.315425, acc.: 88.28%] [G loss: 3.666061]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:35 step:27389 [D loss: 0.257940, acc.: 87.50%] [G loss: 3.647918]\n",
      "epoch:35 step:27390 [D loss: 0.363001, acc.: 84.38%] [G loss: 4.305884]\n",
      "epoch:35 step:27391 [D loss: 0.373690, acc.: 81.25%] [G loss: 2.776426]\n",
      "epoch:35 step:27392 [D loss: 0.306630, acc.: 86.72%] [G loss: 2.989735]\n",
      "epoch:35 step:27393 [D loss: 0.308467, acc.: 85.16%] [G loss: 2.862855]\n",
      "epoch:35 step:27394 [D loss: 0.323471, acc.: 85.16%] [G loss: 4.796401]\n",
      "epoch:35 step:27395 [D loss: 0.345621, acc.: 83.59%] [G loss: 3.264773]\n",
      "epoch:35 step:27396 [D loss: 0.380421, acc.: 83.59%] [G loss: 4.074330]\n",
      "epoch:35 step:27397 [D loss: 0.438969, acc.: 79.69%] [G loss: 5.248278]\n",
      "epoch:35 step:27398 [D loss: 0.399535, acc.: 80.47%] [G loss: 3.867237]\n",
      "epoch:35 step:27399 [D loss: 0.284817, acc.: 86.72%] [G loss: 4.200160]\n",
      "epoch:35 step:27400 [D loss: 0.352642, acc.: 82.81%] [G loss: 2.568255]\n",
      "##############\n",
      "[0.8662557  0.84669975 0.81618976 0.80163787 0.78172133 0.83566917\n",
      " 0.87632962 0.85205254 0.82208321 0.82942061]\n",
      "##########\n",
      "epoch:35 step:27401 [D loss: 0.249624, acc.: 89.84%] [G loss: 3.519443]\n",
      "epoch:35 step:27402 [D loss: 0.275727, acc.: 88.28%] [G loss: 4.176305]\n",
      "epoch:35 step:27403 [D loss: 0.312919, acc.: 82.81%] [G loss: 2.717509]\n",
      "epoch:35 step:27404 [D loss: 0.275944, acc.: 86.72%] [G loss: 3.586956]\n",
      "epoch:35 step:27405 [D loss: 0.265423, acc.: 89.06%] [G loss: 3.326241]\n",
      "epoch:35 step:27406 [D loss: 0.281672, acc.: 89.84%] [G loss: 3.930867]\n",
      "epoch:35 step:27407 [D loss: 0.457871, acc.: 84.38%] [G loss: 3.231226]\n",
      "epoch:35 step:27408 [D loss: 0.261383, acc.: 86.72%] [G loss: 2.767859]\n",
      "epoch:35 step:27409 [D loss: 0.398644, acc.: 82.03%] [G loss: 2.697775]\n",
      "epoch:35 step:27410 [D loss: 0.308372, acc.: 85.94%] [G loss: 2.313871]\n",
      "epoch:35 step:27411 [D loss: 0.297310, acc.: 89.06%] [G loss: 3.207484]\n",
      "epoch:35 step:27412 [D loss: 0.301466, acc.: 88.28%] [G loss: 2.901865]\n",
      "epoch:35 step:27413 [D loss: 0.331929, acc.: 82.03%] [G loss: 2.837333]\n",
      "epoch:35 step:27414 [D loss: 0.398804, acc.: 80.47%] [G loss: 3.279384]\n",
      "epoch:35 step:27415 [D loss: 0.376551, acc.: 77.34%] [G loss: 3.766104]\n",
      "epoch:35 step:27416 [D loss: 0.254278, acc.: 88.28%] [G loss: 6.634677]\n",
      "epoch:35 step:27417 [D loss: 0.247566, acc.: 86.72%] [G loss: 4.848112]\n",
      "epoch:35 step:27418 [D loss: 0.244183, acc.: 92.19%] [G loss: 4.088357]\n",
      "epoch:35 step:27419 [D loss: 0.196328, acc.: 92.19%] [G loss: 5.080506]\n",
      "epoch:35 step:27420 [D loss: 0.254500, acc.: 87.50%] [G loss: 4.700308]\n",
      "epoch:35 step:27421 [D loss: 0.245944, acc.: 90.62%] [G loss: 3.375744]\n",
      "epoch:35 step:27422 [D loss: 0.334788, acc.: 84.38%] [G loss: 3.846439]\n",
      "epoch:35 step:27423 [D loss: 0.364350, acc.: 82.03%] [G loss: 3.697823]\n",
      "epoch:35 step:27424 [D loss: 0.338691, acc.: 84.38%] [G loss: 4.109720]\n",
      "epoch:35 step:27425 [D loss: 0.471913, acc.: 77.34%] [G loss: 5.278752]\n",
      "epoch:35 step:27426 [D loss: 0.553629, acc.: 82.81%] [G loss: 6.384553]\n",
      "epoch:35 step:27427 [D loss: 0.469692, acc.: 76.56%] [G loss: 4.045612]\n",
      "epoch:35 step:27428 [D loss: 0.301987, acc.: 86.72%] [G loss: 3.518237]\n",
      "epoch:35 step:27429 [D loss: 0.394960, acc.: 87.50%] [G loss: 4.315035]\n",
      "epoch:35 step:27430 [D loss: 0.285668, acc.: 85.94%] [G loss: 3.909477]\n",
      "epoch:35 step:27431 [D loss: 0.346760, acc.: 82.81%] [G loss: 2.947192]\n",
      "epoch:35 step:27432 [D loss: 0.222763, acc.: 92.97%] [G loss: 3.865931]\n",
      "epoch:35 step:27433 [D loss: 0.298535, acc.: 85.94%] [G loss: 3.514708]\n",
      "epoch:35 step:27434 [D loss: 0.366011, acc.: 80.47%] [G loss: 3.024162]\n",
      "epoch:35 step:27435 [D loss: 0.341739, acc.: 85.16%] [G loss: 5.793912]\n",
      "epoch:35 step:27436 [D loss: 0.391308, acc.: 79.69%] [G loss: 4.080913]\n",
      "epoch:35 step:27437 [D loss: 0.440786, acc.: 81.25%] [G loss: 3.388891]\n",
      "epoch:35 step:27438 [D loss: 0.279562, acc.: 88.28%] [G loss: 3.226588]\n",
      "epoch:35 step:27439 [D loss: 0.361293, acc.: 86.72%] [G loss: 4.439650]\n",
      "epoch:35 step:27440 [D loss: 0.355264, acc.: 83.59%] [G loss: 3.236878]\n",
      "epoch:35 step:27441 [D loss: 0.327825, acc.: 85.94%] [G loss: 2.982378]\n",
      "epoch:35 step:27442 [D loss: 0.341833, acc.: 82.81%] [G loss: 3.208032]\n",
      "epoch:35 step:27443 [D loss: 0.350395, acc.: 87.50%] [G loss: 4.282697]\n",
      "epoch:35 step:27444 [D loss: 0.270890, acc.: 89.06%] [G loss: 5.507444]\n",
      "epoch:35 step:27445 [D loss: 0.289383, acc.: 85.94%] [G loss: 3.538037]\n",
      "epoch:35 step:27446 [D loss: 0.301156, acc.: 86.72%] [G loss: 3.690847]\n",
      "epoch:35 step:27447 [D loss: 0.243114, acc.: 88.28%] [G loss: 4.393019]\n",
      "epoch:35 step:27448 [D loss: 0.314207, acc.: 87.50%] [G loss: 2.982625]\n",
      "epoch:35 step:27449 [D loss: 0.330091, acc.: 85.16%] [G loss: 3.986606]\n",
      "epoch:35 step:27450 [D loss: 0.339464, acc.: 79.69%] [G loss: 3.725487]\n",
      "epoch:35 step:27451 [D loss: 0.347761, acc.: 85.16%] [G loss: 3.764368]\n",
      "epoch:35 step:27452 [D loss: 0.285075, acc.: 89.06%] [G loss: 3.295269]\n",
      "epoch:35 step:27453 [D loss: 0.209242, acc.: 93.75%] [G loss: 3.713188]\n",
      "epoch:35 step:27454 [D loss: 0.311157, acc.: 84.38%] [G loss: 3.338102]\n",
      "epoch:35 step:27455 [D loss: 0.396169, acc.: 80.47%] [G loss: 3.117294]\n",
      "epoch:35 step:27456 [D loss: 0.265210, acc.: 87.50%] [G loss: 3.791255]\n",
      "epoch:35 step:27457 [D loss: 0.283929, acc.: 86.72%] [G loss: 4.293021]\n",
      "epoch:35 step:27458 [D loss: 0.346291, acc.: 82.81%] [G loss: 3.897907]\n",
      "epoch:35 step:27459 [D loss: 0.251187, acc.: 89.84%] [G loss: 3.483809]\n",
      "epoch:35 step:27460 [D loss: 0.437656, acc.: 80.47%] [G loss: 3.345125]\n",
      "epoch:35 step:27461 [D loss: 0.292984, acc.: 88.28%] [G loss: 3.208642]\n",
      "epoch:35 step:27462 [D loss: 0.215189, acc.: 89.84%] [G loss: 3.305480]\n",
      "epoch:35 step:27463 [D loss: 0.293267, acc.: 86.72%] [G loss: 3.544938]\n",
      "epoch:35 step:27464 [D loss: 0.229008, acc.: 90.62%] [G loss: 4.006825]\n",
      "epoch:35 step:27465 [D loss: 0.283978, acc.: 85.94%] [G loss: 4.445383]\n",
      "epoch:35 step:27466 [D loss: 0.302843, acc.: 85.94%] [G loss: 3.721019]\n",
      "epoch:35 step:27467 [D loss: 0.241843, acc.: 89.84%] [G loss: 3.132385]\n",
      "epoch:35 step:27468 [D loss: 0.387954, acc.: 81.25%] [G loss: 3.107162]\n",
      "epoch:35 step:27469 [D loss: 0.298012, acc.: 89.84%] [G loss: 3.185293]\n",
      "epoch:35 step:27470 [D loss: 0.367369, acc.: 82.81%] [G loss: 4.754800]\n",
      "epoch:35 step:27471 [D loss: 0.248952, acc.: 87.50%] [G loss: 2.831193]\n",
      "epoch:35 step:27472 [D loss: 0.324257, acc.: 82.81%] [G loss: 5.386687]\n",
      "epoch:35 step:27473 [D loss: 0.279085, acc.: 88.28%] [G loss: 4.452826]\n",
      "epoch:35 step:27474 [D loss: 0.289524, acc.: 88.28%] [G loss: 3.669406]\n",
      "epoch:35 step:27475 [D loss: 0.222575, acc.: 89.06%] [G loss: 2.998345]\n",
      "epoch:35 step:27476 [D loss: 0.257072, acc.: 85.94%] [G loss: 3.509849]\n",
      "epoch:35 step:27477 [D loss: 0.377821, acc.: 84.38%] [G loss: 3.570904]\n",
      "epoch:35 step:27478 [D loss: 0.257977, acc.: 87.50%] [G loss: 2.791085]\n",
      "epoch:35 step:27479 [D loss: 0.296515, acc.: 83.59%] [G loss: 3.656999]\n",
      "epoch:35 step:27480 [D loss: 0.332670, acc.: 84.38%] [G loss: 3.396852]\n",
      "epoch:35 step:27481 [D loss: 0.282364, acc.: 87.50%] [G loss: 2.981909]\n",
      "epoch:35 step:27482 [D loss: 0.308174, acc.: 86.72%] [G loss: 2.936915]\n",
      "epoch:35 step:27483 [D loss: 0.288521, acc.: 89.06%] [G loss: 3.661232]\n",
      "epoch:35 step:27484 [D loss: 0.228454, acc.: 89.06%] [G loss: 4.074091]\n",
      "epoch:35 step:27485 [D loss: 0.145128, acc.: 92.97%] [G loss: 4.948394]\n",
      "epoch:35 step:27486 [D loss: 0.263071, acc.: 92.97%] [G loss: 4.883301]\n",
      "epoch:35 step:27487 [D loss: 0.269620, acc.: 85.94%] [G loss: 4.194036]\n",
      "epoch:35 step:27488 [D loss: 0.231689, acc.: 89.84%] [G loss: 3.619725]\n",
      "epoch:35 step:27489 [D loss: 0.187120, acc.: 92.97%] [G loss: 4.030881]\n",
      "epoch:35 step:27490 [D loss: 0.292495, acc.: 88.28%] [G loss: 3.434116]\n",
      "epoch:35 step:27491 [D loss: 0.251711, acc.: 86.72%] [G loss: 5.579162]\n",
      "epoch:35 step:27492 [D loss: 0.307266, acc.: 86.72%] [G loss: 3.593702]\n",
      "epoch:35 step:27493 [D loss: 0.504512, acc.: 79.69%] [G loss: 2.544798]\n",
      "epoch:35 step:27494 [D loss: 0.397086, acc.: 80.47%] [G loss: 4.333652]\n",
      "epoch:35 step:27495 [D loss: 0.353238, acc.: 87.50%] [G loss: 3.224456]\n",
      "epoch:35 step:27496 [D loss: 0.389863, acc.: 78.91%] [G loss: 3.729456]\n",
      "epoch:35 step:27497 [D loss: 0.244888, acc.: 89.84%] [G loss: 3.805630]\n",
      "epoch:35 step:27498 [D loss: 0.232961, acc.: 89.84%] [G loss: 3.755582]\n",
      "epoch:35 step:27499 [D loss: 0.222515, acc.: 89.84%] [G loss: 2.618657]\n",
      "epoch:35 step:27500 [D loss: 0.416553, acc.: 82.03%] [G loss: 3.555115]\n",
      "epoch:35 step:27501 [D loss: 0.351118, acc.: 82.81%] [G loss: 3.823199]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:35 step:27502 [D loss: 0.305229, acc.: 88.28%] [G loss: 4.102491]\n",
      "epoch:35 step:27503 [D loss: 0.318562, acc.: 89.06%] [G loss: 2.884380]\n",
      "epoch:35 step:27504 [D loss: 0.339180, acc.: 85.16%] [G loss: 4.670794]\n",
      "epoch:35 step:27505 [D loss: 0.295994, acc.: 87.50%] [G loss: 6.015481]\n",
      "epoch:35 step:27506 [D loss: 0.339524, acc.: 82.03%] [G loss: 3.037308]\n",
      "epoch:35 step:27507 [D loss: 0.189119, acc.: 94.53%] [G loss: 4.362323]\n",
      "epoch:35 step:27508 [D loss: 0.296565, acc.: 85.94%] [G loss: 2.632757]\n",
      "epoch:35 step:27509 [D loss: 0.359555, acc.: 85.16%] [G loss: 2.704101]\n",
      "epoch:35 step:27510 [D loss: 0.350716, acc.: 89.06%] [G loss: 3.652788]\n",
      "epoch:35 step:27511 [D loss: 0.269757, acc.: 87.50%] [G loss: 5.479601]\n",
      "epoch:35 step:27512 [D loss: 0.307515, acc.: 86.72%] [G loss: 6.231367]\n",
      "epoch:35 step:27513 [D loss: 0.533378, acc.: 73.44%] [G loss: 4.116830]\n",
      "epoch:35 step:27514 [D loss: 0.316689, acc.: 88.28%] [G loss: 3.357095]\n",
      "epoch:35 step:27515 [D loss: 0.352647, acc.: 84.38%] [G loss: 4.556967]\n",
      "epoch:35 step:27516 [D loss: 0.561644, acc.: 79.69%] [G loss: 2.744914]\n",
      "epoch:35 step:27517 [D loss: 0.342481, acc.: 83.59%] [G loss: 3.880801]\n",
      "epoch:35 step:27518 [D loss: 0.466148, acc.: 81.25%] [G loss: 5.908288]\n",
      "epoch:35 step:27519 [D loss: 0.496917, acc.: 79.69%] [G loss: 4.980878]\n",
      "epoch:35 step:27520 [D loss: 0.315159, acc.: 85.94%] [G loss: 3.166513]\n",
      "epoch:35 step:27521 [D loss: 0.456208, acc.: 79.69%] [G loss: 4.423959]\n",
      "epoch:35 step:27522 [D loss: 0.391087, acc.: 80.47%] [G loss: 4.305322]\n",
      "epoch:35 step:27523 [D loss: 0.376017, acc.: 82.81%] [G loss: 5.930760]\n",
      "epoch:35 step:27524 [D loss: 0.543929, acc.: 81.25%] [G loss: 5.009413]\n",
      "epoch:35 step:27525 [D loss: 0.397276, acc.: 83.59%] [G loss: 3.369524]\n",
      "epoch:35 step:27526 [D loss: 0.290520, acc.: 88.28%] [G loss: 3.663899]\n",
      "epoch:35 step:27527 [D loss: 0.231637, acc.: 92.97%] [G loss: 3.665350]\n",
      "epoch:35 step:27528 [D loss: 0.320710, acc.: 83.59%] [G loss: 3.593263]\n",
      "epoch:35 step:27529 [D loss: 0.396295, acc.: 82.03%] [G loss: 4.063634]\n",
      "epoch:35 step:27530 [D loss: 0.293157, acc.: 88.28%] [G loss: 3.056913]\n",
      "epoch:35 step:27531 [D loss: 0.223960, acc.: 90.62%] [G loss: 3.577735]\n",
      "epoch:35 step:27532 [D loss: 0.408325, acc.: 79.69%] [G loss: 2.846664]\n",
      "epoch:35 step:27533 [D loss: 0.287386, acc.: 86.72%] [G loss: 3.051086]\n",
      "epoch:35 step:27534 [D loss: 0.321581, acc.: 85.94%] [G loss: 2.786623]\n",
      "epoch:35 step:27535 [D loss: 0.291988, acc.: 90.62%] [G loss: 2.509570]\n",
      "epoch:35 step:27536 [D loss: 0.317109, acc.: 87.50%] [G loss: 2.883634]\n",
      "epoch:35 step:27537 [D loss: 0.374013, acc.: 83.59%] [G loss: 3.110093]\n",
      "epoch:35 step:27538 [D loss: 0.283417, acc.: 88.28%] [G loss: 3.349633]\n",
      "epoch:35 step:27539 [D loss: 0.325207, acc.: 84.38%] [G loss: 3.522776]\n",
      "epoch:35 step:27540 [D loss: 0.204630, acc.: 92.97%] [G loss: 4.345802]\n",
      "epoch:35 step:27541 [D loss: 0.273061, acc.: 86.72%] [G loss: 4.566670]\n",
      "epoch:35 step:27542 [D loss: 0.206146, acc.: 92.19%] [G loss: 2.946400]\n",
      "epoch:35 step:27543 [D loss: 0.325921, acc.: 86.72%] [G loss: 2.991471]\n",
      "epoch:35 step:27544 [D loss: 0.304107, acc.: 91.41%] [G loss: 3.150699]\n",
      "epoch:35 step:27545 [D loss: 0.328093, acc.: 85.16%] [G loss: 3.237393]\n",
      "epoch:35 step:27546 [D loss: 0.319561, acc.: 83.59%] [G loss: 3.242541]\n",
      "epoch:35 step:27547 [D loss: 0.339257, acc.: 85.94%] [G loss: 3.085862]\n",
      "epoch:35 step:27548 [D loss: 0.393484, acc.: 81.25%] [G loss: 3.305933]\n",
      "epoch:35 step:27549 [D loss: 0.365456, acc.: 84.38%] [G loss: 3.359914]\n",
      "epoch:35 step:27550 [D loss: 0.397068, acc.: 81.25%] [G loss: 3.028293]\n",
      "epoch:35 step:27551 [D loss: 0.319931, acc.: 82.03%] [G loss: 3.344188]\n",
      "epoch:35 step:27552 [D loss: 0.345474, acc.: 85.16%] [G loss: 2.850045]\n",
      "epoch:35 step:27553 [D loss: 0.356437, acc.: 84.38%] [G loss: 3.810756]\n",
      "epoch:35 step:27554 [D loss: 0.264309, acc.: 88.28%] [G loss: 3.313973]\n",
      "epoch:35 step:27555 [D loss: 0.357327, acc.: 85.94%] [G loss: 2.449704]\n",
      "epoch:35 step:27556 [D loss: 0.326106, acc.: 85.94%] [G loss: 2.938965]\n",
      "epoch:35 step:27557 [D loss: 0.341289, acc.: 80.47%] [G loss: 3.266903]\n",
      "epoch:35 step:27558 [D loss: 0.344425, acc.: 87.50%] [G loss: 3.842328]\n",
      "epoch:35 step:27559 [D loss: 0.334945, acc.: 80.47%] [G loss: 3.545764]\n",
      "epoch:35 step:27560 [D loss: 0.257339, acc.: 87.50%] [G loss: 2.906811]\n",
      "epoch:35 step:27561 [D loss: 0.311346, acc.: 86.72%] [G loss: 2.899624]\n",
      "epoch:35 step:27562 [D loss: 0.439985, acc.: 82.81%] [G loss: 4.758064]\n",
      "epoch:35 step:27563 [D loss: 0.416461, acc.: 80.47%] [G loss: 3.494701]\n",
      "epoch:35 step:27564 [D loss: 0.419985, acc.: 72.66%] [G loss: 3.240176]\n",
      "epoch:35 step:27565 [D loss: 0.263968, acc.: 88.28%] [G loss: 4.655666]\n",
      "epoch:35 step:27566 [D loss: 0.181537, acc.: 93.75%] [G loss: 4.910702]\n",
      "epoch:35 step:27567 [D loss: 0.308352, acc.: 85.16%] [G loss: 3.536307]\n",
      "epoch:35 step:27568 [D loss: 0.428034, acc.: 82.81%] [G loss: 5.540926]\n",
      "epoch:35 step:27569 [D loss: 0.344358, acc.: 82.81%] [G loss: 2.575632]\n",
      "epoch:35 step:27570 [D loss: 0.237535, acc.: 89.84%] [G loss: 4.874107]\n",
      "epoch:35 step:27571 [D loss: 0.364601, acc.: 86.72%] [G loss: 3.169042]\n",
      "epoch:35 step:27572 [D loss: 0.277839, acc.: 89.84%] [G loss: 3.394503]\n",
      "epoch:35 step:27573 [D loss: 0.331806, acc.: 82.03%] [G loss: 2.704261]\n",
      "epoch:35 step:27574 [D loss: 0.256107, acc.: 89.84%] [G loss: 3.768111]\n",
      "epoch:35 step:27575 [D loss: 0.371775, acc.: 83.59%] [G loss: 3.003508]\n",
      "epoch:35 step:27576 [D loss: 0.327831, acc.: 85.16%] [G loss: 3.935832]\n",
      "epoch:35 step:27577 [D loss: 0.415224, acc.: 80.47%] [G loss: 7.657151]\n",
      "epoch:35 step:27578 [D loss: 0.529175, acc.: 75.00%] [G loss: 3.737159]\n",
      "epoch:35 step:27579 [D loss: 0.294219, acc.: 86.72%] [G loss: 2.926766]\n",
      "epoch:35 step:27580 [D loss: 0.403273, acc.: 82.03%] [G loss: 2.943578]\n",
      "epoch:35 step:27581 [D loss: 0.550468, acc.: 78.12%] [G loss: 4.434372]\n",
      "epoch:35 step:27582 [D loss: 0.417653, acc.: 82.03%] [G loss: 3.492756]\n",
      "epoch:35 step:27583 [D loss: 0.300161, acc.: 87.50%] [G loss: 3.412641]\n",
      "epoch:35 step:27584 [D loss: 0.330954, acc.: 82.81%] [G loss: 3.256736]\n",
      "epoch:35 step:27585 [D loss: 0.387156, acc.: 86.72%] [G loss: 3.241664]\n",
      "epoch:35 step:27586 [D loss: 0.260551, acc.: 89.84%] [G loss: 3.214511]\n",
      "epoch:35 step:27587 [D loss: 0.266328, acc.: 85.16%] [G loss: 3.913774]\n",
      "epoch:35 step:27588 [D loss: 0.313040, acc.: 84.38%] [G loss: 3.070148]\n",
      "epoch:35 step:27589 [D loss: 0.288169, acc.: 85.94%] [G loss: 3.884302]\n",
      "epoch:35 step:27590 [D loss: 0.368841, acc.: 82.03%] [G loss: 3.611873]\n",
      "epoch:35 step:27591 [D loss: 0.282253, acc.: 85.94%] [G loss: 3.608765]\n",
      "epoch:35 step:27592 [D loss: 0.279929, acc.: 85.94%] [G loss: 4.549556]\n",
      "epoch:35 step:27593 [D loss: 0.290259, acc.: 89.84%] [G loss: 3.446511]\n",
      "epoch:35 step:27594 [D loss: 0.256289, acc.: 87.50%] [G loss: 3.163684]\n",
      "epoch:35 step:27595 [D loss: 0.227011, acc.: 86.72%] [G loss: 3.841076]\n",
      "epoch:35 step:27596 [D loss: 0.373411, acc.: 85.16%] [G loss: 3.914051]\n",
      "epoch:35 step:27597 [D loss: 0.233754, acc.: 88.28%] [G loss: 3.744416]\n",
      "epoch:35 step:27598 [D loss: 0.243817, acc.: 89.06%] [G loss: 4.205773]\n",
      "epoch:35 step:27599 [D loss: 0.252673, acc.: 87.50%] [G loss: 4.073119]\n",
      "epoch:35 step:27600 [D loss: 0.347930, acc.: 84.38%] [G loss: 3.271042]\n",
      "##############\n",
      "[0.85064739 0.8516865  0.81626525 0.8127376  0.7703159  0.82581643\n",
      " 0.85415936 0.85024982 0.815887   0.82234481]\n",
      "##########\n",
      "epoch:35 step:27601 [D loss: 0.252709, acc.: 88.28%] [G loss: 4.042252]\n",
      "epoch:35 step:27602 [D loss: 0.327388, acc.: 88.28%] [G loss: 3.160722]\n",
      "epoch:35 step:27603 [D loss: 0.227031, acc.: 89.84%] [G loss: 3.005451]\n",
      "epoch:35 step:27604 [D loss: 0.261849, acc.: 92.19%] [G loss: 1.826353]\n",
      "epoch:35 step:27605 [D loss: 0.207949, acc.: 92.97%] [G loss: 2.943341]\n",
      "epoch:35 step:27606 [D loss: 0.300093, acc.: 89.06%] [G loss: 2.623848]\n",
      "epoch:35 step:27607 [D loss: 0.236687, acc.: 90.62%] [G loss: 4.578121]\n",
      "epoch:35 step:27608 [D loss: 0.330692, acc.: 87.50%] [G loss: 3.303735]\n",
      "epoch:35 step:27609 [D loss: 0.328345, acc.: 88.28%] [G loss: 2.850096]\n",
      "epoch:35 step:27610 [D loss: 0.432472, acc.: 77.34%] [G loss: 4.741211]\n",
      "epoch:35 step:27611 [D loss: 0.300406, acc.: 85.16%] [G loss: 4.331541]\n",
      "epoch:35 step:27612 [D loss: 0.328075, acc.: 84.38%] [G loss: 4.469054]\n",
      "epoch:35 step:27613 [D loss: 0.237847, acc.: 89.84%] [G loss: 3.086337]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:35 step:27614 [D loss: 0.480585, acc.: 78.12%] [G loss: 2.958916]\n",
      "epoch:35 step:27615 [D loss: 0.212721, acc.: 90.62%] [G loss: 4.081420]\n",
      "epoch:35 step:27616 [D loss: 0.320919, acc.: 86.72%] [G loss: 3.321624]\n",
      "epoch:35 step:27617 [D loss: 0.394723, acc.: 79.69%] [G loss: 3.087896]\n",
      "epoch:35 step:27618 [D loss: 0.364556, acc.: 82.03%] [G loss: 4.650180]\n",
      "epoch:35 step:27619 [D loss: 0.326405, acc.: 81.25%] [G loss: 4.378829]\n",
      "epoch:35 step:27620 [D loss: 0.386443, acc.: 82.81%] [G loss: 4.056938]\n",
      "epoch:35 step:27621 [D loss: 0.335878, acc.: 83.59%] [G loss: 3.430061]\n",
      "epoch:35 step:27622 [D loss: 0.334920, acc.: 88.28%] [G loss: 3.095186]\n",
      "epoch:35 step:27623 [D loss: 0.357359, acc.: 85.94%] [G loss: 3.664368]\n",
      "epoch:35 step:27624 [D loss: 0.313183, acc.: 83.59%] [G loss: 4.108602]\n",
      "epoch:35 step:27625 [D loss: 0.335598, acc.: 85.16%] [G loss: 2.912281]\n",
      "epoch:35 step:27626 [D loss: 0.252884, acc.: 86.72%] [G loss: 3.991296]\n",
      "epoch:35 step:27627 [D loss: 0.298207, acc.: 87.50%] [G loss: 3.206552]\n",
      "epoch:35 step:27628 [D loss: 0.329467, acc.: 85.94%] [G loss: 3.090440]\n",
      "epoch:35 step:27629 [D loss: 0.249960, acc.: 90.62%] [G loss: 4.202292]\n",
      "epoch:35 step:27630 [D loss: 0.364962, acc.: 79.69%] [G loss: 3.316083]\n",
      "epoch:35 step:27631 [D loss: 0.332016, acc.: 84.38%] [G loss: 4.829245]\n",
      "epoch:35 step:27632 [D loss: 0.362714, acc.: 79.69%] [G loss: 3.540089]\n",
      "epoch:35 step:27633 [D loss: 0.292391, acc.: 82.03%] [G loss: 4.390684]\n",
      "epoch:35 step:27634 [D loss: 0.304827, acc.: 87.50%] [G loss: 5.006642]\n",
      "epoch:35 step:27635 [D loss: 0.356814, acc.: 84.38%] [G loss: 3.820642]\n",
      "epoch:35 step:27636 [D loss: 0.241898, acc.: 89.84%] [G loss: 3.877231]\n",
      "epoch:35 step:27637 [D loss: 0.241088, acc.: 89.84%] [G loss: 4.700755]\n",
      "epoch:35 step:27638 [D loss: 0.278224, acc.: 89.06%] [G loss: 3.996762]\n",
      "epoch:35 step:27639 [D loss: 0.282812, acc.: 84.38%] [G loss: 3.663816]\n",
      "epoch:35 step:27640 [D loss: 0.288477, acc.: 85.94%] [G loss: 4.690330]\n",
      "epoch:35 step:27641 [D loss: 0.282746, acc.: 85.94%] [G loss: 4.739515]\n",
      "epoch:35 step:27642 [D loss: 0.267503, acc.: 88.28%] [G loss: 6.657646]\n",
      "epoch:35 step:27643 [D loss: 0.250054, acc.: 89.84%] [G loss: 5.656211]\n",
      "epoch:35 step:27644 [D loss: 0.291727, acc.: 82.03%] [G loss: 5.900189]\n",
      "epoch:35 step:27645 [D loss: 0.193622, acc.: 89.06%] [G loss: 5.330498]\n",
      "epoch:35 step:27646 [D loss: 0.292919, acc.: 86.72%] [G loss: 5.057186]\n",
      "epoch:35 step:27647 [D loss: 0.337021, acc.: 83.59%] [G loss: 3.939114]\n",
      "epoch:35 step:27648 [D loss: 0.435928, acc.: 80.47%] [G loss: 3.921000]\n",
      "epoch:35 step:27649 [D loss: 0.390344, acc.: 78.91%] [G loss: 3.811437]\n",
      "epoch:35 step:27650 [D loss: 0.400996, acc.: 80.47%] [G loss: 4.695316]\n",
      "epoch:35 step:27651 [D loss: 0.333612, acc.: 82.03%] [G loss: 4.411549]\n",
      "epoch:35 step:27652 [D loss: 0.436848, acc.: 84.38%] [G loss: 3.858294]\n",
      "epoch:35 step:27653 [D loss: 0.434134, acc.: 81.25%] [G loss: 2.845984]\n",
      "epoch:35 step:27654 [D loss: 0.214438, acc.: 89.84%] [G loss: 4.742429]\n",
      "epoch:35 step:27655 [D loss: 0.329287, acc.: 86.72%] [G loss: 4.402534]\n",
      "epoch:35 step:27656 [D loss: 0.244374, acc.: 91.41%] [G loss: 4.662377]\n",
      "epoch:35 step:27657 [D loss: 0.309436, acc.: 86.72%] [G loss: 3.802888]\n",
      "epoch:35 step:27658 [D loss: 0.261295, acc.: 85.16%] [G loss: 4.977343]\n",
      "epoch:35 step:27659 [D loss: 0.339325, acc.: 84.38%] [G loss: 2.362988]\n",
      "epoch:35 step:27660 [D loss: 0.309927, acc.: 82.03%] [G loss: 5.038201]\n",
      "epoch:35 step:27661 [D loss: 0.259935, acc.: 89.06%] [G loss: 4.410251]\n",
      "epoch:35 step:27662 [D loss: 0.361256, acc.: 84.38%] [G loss: 4.495089]\n",
      "epoch:35 step:27663 [D loss: 0.324714, acc.: 82.81%] [G loss: 2.897870]\n",
      "epoch:35 step:27664 [D loss: 0.238685, acc.: 90.62%] [G loss: 3.894952]\n",
      "epoch:35 step:27665 [D loss: 0.325915, acc.: 83.59%] [G loss: 3.727613]\n",
      "epoch:35 step:27666 [D loss: 0.306182, acc.: 88.28%] [G loss: 3.496186]\n",
      "epoch:35 step:27667 [D loss: 0.331329, acc.: 85.16%] [G loss: 3.623176]\n",
      "epoch:35 step:27668 [D loss: 0.313964, acc.: 82.03%] [G loss: 4.434298]\n",
      "epoch:35 step:27669 [D loss: 0.291215, acc.: 85.16%] [G loss: 3.999095]\n",
      "epoch:35 step:27670 [D loss: 0.287170, acc.: 89.84%] [G loss: 3.931411]\n",
      "epoch:35 step:27671 [D loss: 0.365106, acc.: 87.50%] [G loss: 4.104006]\n",
      "epoch:35 step:27672 [D loss: 0.259843, acc.: 86.72%] [G loss: 4.589574]\n",
      "epoch:35 step:27673 [D loss: 0.236781, acc.: 92.97%] [G loss: 2.806239]\n",
      "epoch:35 step:27674 [D loss: 0.263765, acc.: 88.28%] [G loss: 2.859568]\n",
      "epoch:35 step:27675 [D loss: 0.281711, acc.: 87.50%] [G loss: 3.652558]\n",
      "epoch:35 step:27676 [D loss: 0.246506, acc.: 90.62%] [G loss: 3.534932]\n",
      "epoch:35 step:27677 [D loss: 0.340658, acc.: 84.38%] [G loss: 2.669073]\n",
      "epoch:35 step:27678 [D loss: 0.353299, acc.: 85.94%] [G loss: 3.101668]\n",
      "epoch:35 step:27679 [D loss: 0.317793, acc.: 85.94%] [G loss: 2.965080]\n",
      "epoch:35 step:27680 [D loss: 0.322326, acc.: 87.50%] [G loss: 3.787413]\n",
      "epoch:35 step:27681 [D loss: 0.355899, acc.: 83.59%] [G loss: 2.384805]\n",
      "epoch:35 step:27682 [D loss: 0.263956, acc.: 87.50%] [G loss: 3.406561]\n",
      "epoch:35 step:27683 [D loss: 0.393033, acc.: 83.59%] [G loss: 3.034253]\n",
      "epoch:35 step:27684 [D loss: 0.347895, acc.: 85.94%] [G loss: 2.516164]\n",
      "epoch:35 step:27685 [D loss: 0.292755, acc.: 88.28%] [G loss: 4.372236]\n",
      "epoch:35 step:27686 [D loss: 0.300507, acc.: 87.50%] [G loss: 3.466353]\n",
      "epoch:35 step:27687 [D loss: 0.311199, acc.: 86.72%] [G loss: 4.796597]\n",
      "epoch:35 step:27688 [D loss: 0.277182, acc.: 92.19%] [G loss: 3.286190]\n",
      "epoch:35 step:27689 [D loss: 0.335039, acc.: 85.16%] [G loss: 3.409598]\n",
      "epoch:35 step:27690 [D loss: 0.334997, acc.: 86.72%] [G loss: 5.431939]\n",
      "epoch:35 step:27691 [D loss: 0.412077, acc.: 82.81%] [G loss: 5.215037]\n",
      "epoch:35 step:27692 [D loss: 0.355587, acc.: 85.16%] [G loss: 4.764942]\n",
      "epoch:35 step:27693 [D loss: 0.294019, acc.: 87.50%] [G loss: 4.103756]\n",
      "epoch:35 step:27694 [D loss: 0.333679, acc.: 85.94%] [G loss: 3.316035]\n",
      "epoch:35 step:27695 [D loss: 0.369950, acc.: 83.59%] [G loss: 3.189391]\n",
      "epoch:35 step:27696 [D loss: 0.293255, acc.: 89.84%] [G loss: 2.925051]\n",
      "epoch:35 step:27697 [D loss: 0.296341, acc.: 89.84%] [G loss: 3.874440]\n",
      "epoch:35 step:27698 [D loss: 0.276120, acc.: 85.94%] [G loss: 4.581624]\n",
      "epoch:35 step:27699 [D loss: 0.276860, acc.: 85.94%] [G loss: 3.086690]\n",
      "epoch:35 step:27700 [D loss: 0.218664, acc.: 89.06%] [G loss: 3.284190]\n",
      "epoch:35 step:27701 [D loss: 0.260304, acc.: 91.41%] [G loss: 3.622656]\n",
      "epoch:35 step:27702 [D loss: 0.355953, acc.: 85.94%] [G loss: 3.611885]\n",
      "epoch:35 step:27703 [D loss: 0.446455, acc.: 75.00%] [G loss: 3.655510]\n",
      "epoch:35 step:27704 [D loss: 0.347388, acc.: 80.47%] [G loss: 3.032761]\n",
      "epoch:35 step:27705 [D loss: 0.370143, acc.: 81.25%] [G loss: 3.295043]\n",
      "epoch:35 step:27706 [D loss: 0.265307, acc.: 88.28%] [G loss: 3.269288]\n",
      "epoch:35 step:27707 [D loss: 0.430231, acc.: 82.03%] [G loss: 3.867166]\n",
      "epoch:35 step:27708 [D loss: 0.246992, acc.: 89.84%] [G loss: 3.432782]\n",
      "epoch:35 step:27709 [D loss: 0.370582, acc.: 86.72%] [G loss: 2.996923]\n",
      "epoch:35 step:27710 [D loss: 0.277110, acc.: 89.06%] [G loss: 3.066574]\n",
      "epoch:35 step:27711 [D loss: 0.360594, acc.: 81.25%] [G loss: 3.084468]\n",
      "epoch:35 step:27712 [D loss: 0.308559, acc.: 85.94%] [G loss: 3.460918]\n",
      "epoch:35 step:27713 [D loss: 0.274376, acc.: 86.72%] [G loss: 3.000758]\n",
      "epoch:35 step:27714 [D loss: 0.310563, acc.: 85.16%] [G loss: 3.593082]\n",
      "epoch:35 step:27715 [D loss: 0.352511, acc.: 82.03%] [G loss: 3.150928]\n",
      "epoch:35 step:27716 [D loss: 0.300458, acc.: 85.94%] [G loss: 3.199283]\n",
      "epoch:35 step:27717 [D loss: 0.320551, acc.: 87.50%] [G loss: 3.904612]\n",
      "epoch:35 step:27718 [D loss: 0.392665, acc.: 82.03%] [G loss: 3.829522]\n",
      "epoch:35 step:27719 [D loss: 0.511157, acc.: 78.12%] [G loss: 5.977596]\n",
      "epoch:35 step:27720 [D loss: 0.453491, acc.: 81.25%] [G loss: 3.869554]\n",
      "epoch:35 step:27721 [D loss: 0.263405, acc.: 87.50%] [G loss: 5.265678]\n",
      "epoch:35 step:27722 [D loss: 0.361254, acc.: 86.72%] [G loss: 3.738616]\n",
      "epoch:35 step:27723 [D loss: 0.333451, acc.: 85.16%] [G loss: 3.587793]\n",
      "epoch:35 step:27724 [D loss: 0.293038, acc.: 88.28%] [G loss: 3.395240]\n",
      "epoch:35 step:27725 [D loss: 0.234160, acc.: 90.62%] [G loss: 3.686812]\n",
      "epoch:35 step:27726 [D loss: 0.292180, acc.: 92.19%] [G loss: 3.032336]\n",
      "epoch:35 step:27727 [D loss: 0.251599, acc.: 89.06%] [G loss: 4.633037]\n",
      "epoch:35 step:27728 [D loss: 0.268818, acc.: 88.28%] [G loss: 4.109586]\n",
      "epoch:35 step:27729 [D loss: 0.262717, acc.: 91.41%] [G loss: 4.283498]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:35 step:27730 [D loss: 0.346504, acc.: 82.81%] [G loss: 4.745118]\n",
      "epoch:35 step:27731 [D loss: 0.162657, acc.: 93.75%] [G loss: 5.710347]\n",
      "epoch:35 step:27732 [D loss: 0.282666, acc.: 88.28%] [G loss: 3.489475]\n",
      "epoch:35 step:27733 [D loss: 0.283090, acc.: 86.72%] [G loss: 2.826270]\n",
      "epoch:35 step:27734 [D loss: 0.252132, acc.: 89.06%] [G loss: 2.435748]\n",
      "epoch:35 step:27735 [D loss: 0.251516, acc.: 90.62%] [G loss: 2.863052]\n",
      "epoch:35 step:27736 [D loss: 0.284729, acc.: 88.28%] [G loss: 2.923772]\n",
      "epoch:35 step:27737 [D loss: 0.331451, acc.: 84.38%] [G loss: 3.029067]\n",
      "epoch:35 step:27738 [D loss: 0.289550, acc.: 85.94%] [G loss: 2.977917]\n",
      "epoch:35 step:27739 [D loss: 0.326548, acc.: 85.94%] [G loss: 2.594833]\n",
      "epoch:35 step:27740 [D loss: 0.317779, acc.: 85.94%] [G loss: 4.365028]\n",
      "epoch:35 step:27741 [D loss: 0.386994, acc.: 81.25%] [G loss: 3.633214]\n",
      "epoch:35 step:27742 [D loss: 0.372450, acc.: 84.38%] [G loss: 3.960647]\n",
      "epoch:35 step:27743 [D loss: 0.345389, acc.: 84.38%] [G loss: 2.918023]\n",
      "epoch:35 step:27744 [D loss: 0.287748, acc.: 86.72%] [G loss: 2.669255]\n",
      "epoch:35 step:27745 [D loss: 0.372282, acc.: 80.47%] [G loss: 4.026022]\n",
      "epoch:35 step:27746 [D loss: 0.347942, acc.: 88.28%] [G loss: 3.044495]\n",
      "epoch:35 step:27747 [D loss: 0.250038, acc.: 90.62%] [G loss: 3.055859]\n",
      "epoch:35 step:27748 [D loss: 0.284957, acc.: 88.28%] [G loss: 3.373125]\n",
      "epoch:35 step:27749 [D loss: 0.306809, acc.: 89.06%] [G loss: 2.419067]\n",
      "epoch:35 step:27750 [D loss: 0.287402, acc.: 87.50%] [G loss: 3.126741]\n",
      "epoch:35 step:27751 [D loss: 0.310671, acc.: 85.16%] [G loss: 3.425869]\n",
      "epoch:35 step:27752 [D loss: 0.327083, acc.: 82.81%] [G loss: 3.916557]\n",
      "epoch:35 step:27753 [D loss: 0.330023, acc.: 87.50%] [G loss: 3.137705]\n",
      "epoch:35 step:27754 [D loss: 0.380834, acc.: 82.03%] [G loss: 3.255177]\n",
      "epoch:35 step:27755 [D loss: 0.424549, acc.: 79.69%] [G loss: 2.598732]\n",
      "epoch:35 step:27756 [D loss: 0.319234, acc.: 88.28%] [G loss: 3.036610]\n",
      "epoch:35 step:27757 [D loss: 0.373843, acc.: 85.16%] [G loss: 3.585649]\n",
      "epoch:35 step:27758 [D loss: 0.317568, acc.: 85.94%] [G loss: 3.225995]\n",
      "epoch:35 step:27759 [D loss: 0.413672, acc.: 78.91%] [G loss: 3.177157]\n",
      "epoch:35 step:27760 [D loss: 0.295822, acc.: 87.50%] [G loss: 3.469286]\n",
      "epoch:35 step:27761 [D loss: 0.342602, acc.: 85.16%] [G loss: 3.582758]\n",
      "epoch:35 step:27762 [D loss: 0.435873, acc.: 79.69%] [G loss: 3.451417]\n",
      "epoch:35 step:27763 [D loss: 0.421020, acc.: 78.12%] [G loss: 3.450366]\n",
      "epoch:35 step:27764 [D loss: 0.475901, acc.: 74.22%] [G loss: 4.425080]\n",
      "epoch:35 step:27765 [D loss: 0.421297, acc.: 80.47%] [G loss: 3.961575]\n",
      "epoch:35 step:27766 [D loss: 0.365682, acc.: 80.47%] [G loss: 3.508602]\n",
      "epoch:35 step:27767 [D loss: 0.416209, acc.: 79.69%] [G loss: 6.344357]\n",
      "epoch:35 step:27768 [D loss: 0.180906, acc.: 92.97%] [G loss: 6.185374]\n",
      "epoch:35 step:27769 [D loss: 0.427331, acc.: 81.25%] [G loss: 5.497795]\n",
      "epoch:35 step:27770 [D loss: 0.860472, acc.: 71.88%] [G loss: 9.138953]\n",
      "epoch:35 step:27771 [D loss: 2.715740, acc.: 51.56%] [G loss: 8.630007]\n",
      "epoch:35 step:27772 [D loss: 1.286509, acc.: 70.31%] [G loss: 4.668820]\n",
      "epoch:35 step:27773 [D loss: 0.737677, acc.: 71.09%] [G loss: 2.715081]\n",
      "epoch:35 step:27774 [D loss: 0.501068, acc.: 78.91%] [G loss: 4.218043]\n",
      "epoch:35 step:27775 [D loss: 0.311510, acc.: 88.28%] [G loss: 4.413156]\n",
      "epoch:35 step:27776 [D loss: 0.364970, acc.: 85.16%] [G loss: 4.272476]\n",
      "epoch:35 step:27777 [D loss: 0.310711, acc.: 89.06%] [G loss: 3.737306]\n",
      "epoch:35 step:27778 [D loss: 0.292739, acc.: 85.16%] [G loss: 3.186568]\n",
      "epoch:35 step:27779 [D loss: 0.379839, acc.: 82.03%] [G loss: 2.882509]\n",
      "epoch:35 step:27780 [D loss: 0.350536, acc.: 82.03%] [G loss: 3.235184]\n",
      "epoch:35 step:27781 [D loss: 0.276784, acc.: 88.28%] [G loss: 2.829662]\n",
      "epoch:35 step:27782 [D loss: 0.292888, acc.: 85.16%] [G loss: 3.187614]\n",
      "epoch:35 step:27783 [D loss: 0.370002, acc.: 82.81%] [G loss: 2.200381]\n",
      "epoch:35 step:27784 [D loss: 0.258583, acc.: 90.62%] [G loss: 3.228661]\n",
      "epoch:35 step:27785 [D loss: 0.192207, acc.: 92.97%] [G loss: 3.126306]\n",
      "epoch:35 step:27786 [D loss: 0.283869, acc.: 89.06%] [G loss: 3.292689]\n",
      "epoch:35 step:27787 [D loss: 0.287536, acc.: 88.28%] [G loss: 2.853297]\n",
      "epoch:35 step:27788 [D loss: 0.327569, acc.: 83.59%] [G loss: 3.102771]\n",
      "epoch:35 step:27789 [D loss: 0.238005, acc.: 92.19%] [G loss: 3.203055]\n",
      "epoch:35 step:27790 [D loss: 0.511249, acc.: 78.91%] [G loss: 3.179936]\n",
      "epoch:35 step:27791 [D loss: 0.380884, acc.: 82.81%] [G loss: 2.256425]\n",
      "epoch:35 step:27792 [D loss: 0.377283, acc.: 81.25%] [G loss: 2.429258]\n",
      "epoch:35 step:27793 [D loss: 0.273233, acc.: 89.06%] [G loss: 2.724917]\n",
      "epoch:35 step:27794 [D loss: 0.321999, acc.: 85.16%] [G loss: 2.134852]\n",
      "epoch:35 step:27795 [D loss: 0.296454, acc.: 88.28%] [G loss: 2.701434]\n",
      "epoch:35 step:27796 [D loss: 0.277818, acc.: 85.94%] [G loss: 2.933433]\n",
      "epoch:35 step:27797 [D loss: 0.295412, acc.: 89.06%] [G loss: 2.905873]\n",
      "epoch:35 step:27798 [D loss: 0.273196, acc.: 88.28%] [G loss: 2.747722]\n",
      "epoch:35 step:27799 [D loss: 0.296923, acc.: 82.81%] [G loss: 3.308208]\n",
      "epoch:35 step:27800 [D loss: 0.395724, acc.: 82.81%] [G loss: 3.259223]\n",
      "##############\n",
      "[0.88187929 0.85920919 0.81571705 0.80911087 0.76596437 0.83078543\n",
      " 0.89924755 0.81612216 0.78674341 0.82885926]\n",
      "##########\n",
      "epoch:35 step:27801 [D loss: 0.292906, acc.: 84.38%] [G loss: 2.533341]\n",
      "epoch:35 step:27802 [D loss: 0.348528, acc.: 84.38%] [G loss: 2.641480]\n",
      "epoch:35 step:27803 [D loss: 0.300303, acc.: 86.72%] [G loss: 2.847929]\n",
      "epoch:35 step:27804 [D loss: 0.299886, acc.: 82.81%] [G loss: 3.002995]\n",
      "epoch:35 step:27805 [D loss: 0.218142, acc.: 90.62%] [G loss: 3.216484]\n",
      "epoch:35 step:27806 [D loss: 0.308051, acc.: 85.16%] [G loss: 3.318324]\n",
      "epoch:35 step:27807 [D loss: 0.267152, acc.: 86.72%] [G loss: 3.612089]\n",
      "epoch:35 step:27808 [D loss: 0.334627, acc.: 85.16%] [G loss: 3.384620]\n",
      "epoch:35 step:27809 [D loss: 0.251868, acc.: 91.41%] [G loss: 4.174651]\n",
      "epoch:35 step:27810 [D loss: 0.366964, acc.: 82.03%] [G loss: 2.148790]\n",
      "epoch:35 step:27811 [D loss: 0.326773, acc.: 85.16%] [G loss: 3.060778]\n",
      "epoch:35 step:27812 [D loss: 0.231841, acc.: 89.84%] [G loss: 5.105128]\n",
      "epoch:35 step:27813 [D loss: 0.297216, acc.: 84.38%] [G loss: 3.591579]\n",
      "epoch:35 step:27814 [D loss: 0.257048, acc.: 88.28%] [G loss: 3.538226]\n",
      "epoch:35 step:27815 [D loss: 0.278049, acc.: 89.84%] [G loss: 5.276474]\n",
      "epoch:35 step:27816 [D loss: 0.272138, acc.: 88.28%] [G loss: 3.461195]\n",
      "epoch:35 step:27817 [D loss: 0.412531, acc.: 82.03%] [G loss: 2.911220]\n",
      "epoch:35 step:27818 [D loss: 0.357273, acc.: 85.16%] [G loss: 3.580054]\n",
      "epoch:35 step:27819 [D loss: 0.282704, acc.: 85.16%] [G loss: 2.700044]\n",
      "epoch:35 step:27820 [D loss: 0.339392, acc.: 85.94%] [G loss: 3.857765]\n",
      "epoch:35 step:27821 [D loss: 0.285165, acc.: 88.28%] [G loss: 3.392465]\n",
      "epoch:35 step:27822 [D loss: 0.284949, acc.: 86.72%] [G loss: 4.448908]\n",
      "epoch:35 step:27823 [D loss: 0.343805, acc.: 82.81%] [G loss: 2.894589]\n",
      "epoch:35 step:27824 [D loss: 0.325862, acc.: 85.16%] [G loss: 3.287052]\n",
      "epoch:35 step:27825 [D loss: 0.350666, acc.: 84.38%] [G loss: 2.969901]\n",
      "epoch:35 step:27826 [D loss: 0.302159, acc.: 84.38%] [G loss: 2.554707]\n",
      "epoch:35 step:27827 [D loss: 0.453469, acc.: 75.00%] [G loss: 2.282209]\n",
      "epoch:35 step:27828 [D loss: 0.356318, acc.: 83.59%] [G loss: 3.061620]\n",
      "epoch:35 step:27829 [D loss: 0.204082, acc.: 93.75%] [G loss: 2.855099]\n",
      "epoch:35 step:27830 [D loss: 0.399586, acc.: 77.34%] [G loss: 2.774832]\n",
      "epoch:35 step:27831 [D loss: 0.286143, acc.: 90.62%] [G loss: 3.267871]\n",
      "epoch:35 step:27832 [D loss: 0.388522, acc.: 80.47%] [G loss: 3.064950]\n",
      "epoch:35 step:27833 [D loss: 0.302290, acc.: 86.72%] [G loss: 2.749820]\n",
      "epoch:35 step:27834 [D loss: 0.307774, acc.: 85.94%] [G loss: 2.877511]\n",
      "epoch:35 step:27835 [D loss: 0.319884, acc.: 86.72%] [G loss: 3.127654]\n",
      "epoch:35 step:27836 [D loss: 0.372937, acc.: 84.38%] [G loss: 2.799658]\n",
      "epoch:35 step:27837 [D loss: 0.305038, acc.: 88.28%] [G loss: 2.983342]\n",
      "epoch:35 step:27838 [D loss: 0.271511, acc.: 86.72%] [G loss: 2.713820]\n",
      "epoch:35 step:27839 [D loss: 0.272612, acc.: 88.28%] [G loss: 4.196341]\n",
      "epoch:35 step:27840 [D loss: 0.233859, acc.: 92.19%] [G loss: 3.953763]\n",
      "epoch:35 step:27841 [D loss: 0.316550, acc.: 83.59%] [G loss: 4.901263]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:35 step:27842 [D loss: 0.239259, acc.: 89.84%] [G loss: 6.559976]\n",
      "epoch:35 step:27843 [D loss: 0.257951, acc.: 86.72%] [G loss: 8.771486]\n",
      "epoch:35 step:27844 [D loss: 0.286284, acc.: 87.50%] [G loss: 4.174874]\n",
      "epoch:35 step:27845 [D loss: 0.216304, acc.: 89.06%] [G loss: 6.373240]\n",
      "epoch:35 step:27846 [D loss: 0.290886, acc.: 83.59%] [G loss: 3.721462]\n",
      "epoch:35 step:27847 [D loss: 0.158706, acc.: 93.75%] [G loss: 5.218340]\n",
      "epoch:35 step:27848 [D loss: 0.328722, acc.: 84.38%] [G loss: 3.096127]\n",
      "epoch:35 step:27849 [D loss: 0.256079, acc.: 90.62%] [G loss: 4.145349]\n",
      "epoch:35 step:27850 [D loss: 0.362993, acc.: 84.38%] [G loss: 3.240574]\n",
      "epoch:35 step:27851 [D loss: 0.253950, acc.: 89.06%] [G loss: 4.938430]\n",
      "epoch:35 step:27852 [D loss: 0.268922, acc.: 87.50%] [G loss: 4.864628]\n",
      "epoch:35 step:27853 [D loss: 0.322073, acc.: 85.16%] [G loss: 4.515761]\n",
      "epoch:35 step:27854 [D loss: 0.246139, acc.: 89.84%] [G loss: 4.118415]\n",
      "epoch:35 step:27855 [D loss: 0.371519, acc.: 83.59%] [G loss: 2.544837]\n",
      "epoch:35 step:27856 [D loss: 0.283075, acc.: 89.06%] [G loss: 3.130208]\n",
      "epoch:35 step:27857 [D loss: 0.365800, acc.: 84.38%] [G loss: 2.421826]\n",
      "epoch:35 step:27858 [D loss: 0.327923, acc.: 85.94%] [G loss: 2.608130]\n",
      "epoch:35 step:27859 [D loss: 0.417212, acc.: 76.56%] [G loss: 2.531948]\n",
      "epoch:35 step:27860 [D loss: 0.321838, acc.: 85.16%] [G loss: 2.791565]\n",
      "epoch:35 step:27861 [D loss: 0.352028, acc.: 81.25%] [G loss: 2.997892]\n",
      "epoch:35 step:27862 [D loss: 0.391990, acc.: 83.59%] [G loss: 2.447216]\n",
      "epoch:35 step:27863 [D loss: 0.301621, acc.: 84.38%] [G loss: 2.824931]\n",
      "epoch:35 step:27864 [D loss: 0.353683, acc.: 85.16%] [G loss: 3.058636]\n",
      "epoch:35 step:27865 [D loss: 0.533216, acc.: 75.78%] [G loss: 2.806841]\n",
      "epoch:35 step:27866 [D loss: 0.399569, acc.: 78.91%] [G loss: 3.444766]\n",
      "epoch:35 step:27867 [D loss: 0.451511, acc.: 77.34%] [G loss: 4.629603]\n",
      "epoch:35 step:27868 [D loss: 0.423458, acc.: 79.69%] [G loss: 4.328899]\n",
      "epoch:35 step:27869 [D loss: 0.213703, acc.: 90.62%] [G loss: 6.526735]\n",
      "epoch:35 step:27870 [D loss: 0.332696, acc.: 85.94%] [G loss: 5.997658]\n",
      "epoch:35 step:27871 [D loss: 0.397795, acc.: 81.25%] [G loss: 3.046384]\n",
      "epoch:35 step:27872 [D loss: 0.338394, acc.: 82.81%] [G loss: 4.202077]\n",
      "epoch:35 step:27873 [D loss: 0.256105, acc.: 89.84%] [G loss: 3.798411]\n",
      "epoch:35 step:27874 [D loss: 0.234781, acc.: 88.28%] [G loss: 3.389850]\n",
      "epoch:35 step:27875 [D loss: 0.223393, acc.: 91.41%] [G loss: 3.112544]\n",
      "epoch:35 step:27876 [D loss: 0.397940, acc.: 84.38%] [G loss: 4.093583]\n",
      "epoch:35 step:27877 [D loss: 0.546429, acc.: 71.88%] [G loss: 3.119639]\n",
      "epoch:35 step:27878 [D loss: 0.421055, acc.: 79.69%] [G loss: 2.804501]\n",
      "epoch:35 step:27879 [D loss: 0.449051, acc.: 75.78%] [G loss: 3.467933]\n",
      "epoch:35 step:27880 [D loss: 0.344089, acc.: 85.16%] [G loss: 4.653877]\n",
      "epoch:35 step:27881 [D loss: 0.222978, acc.: 91.41%] [G loss: 3.625365]\n",
      "epoch:35 step:27882 [D loss: 0.368048, acc.: 79.69%] [G loss: 3.036079]\n",
      "epoch:35 step:27883 [D loss: 0.217957, acc.: 89.84%] [G loss: 3.769955]\n",
      "epoch:35 step:27884 [D loss: 0.289151, acc.: 87.50%] [G loss: 2.991766]\n",
      "epoch:35 step:27885 [D loss: 0.338808, acc.: 84.38%] [G loss: 3.630332]\n",
      "epoch:35 step:27886 [D loss: 0.272124, acc.: 89.06%] [G loss: 2.979852]\n",
      "epoch:35 step:27887 [D loss: 0.418147, acc.: 82.81%] [G loss: 3.334067]\n",
      "epoch:35 step:27888 [D loss: 0.326938, acc.: 87.50%] [G loss: 3.047700]\n",
      "epoch:35 step:27889 [D loss: 0.269706, acc.: 87.50%] [G loss: 2.410704]\n",
      "epoch:35 step:27890 [D loss: 0.362105, acc.: 82.03%] [G loss: 2.564099]\n",
      "epoch:35 step:27891 [D loss: 0.258238, acc.: 89.06%] [G loss: 3.098733]\n",
      "epoch:35 step:27892 [D loss: 0.303843, acc.: 87.50%] [G loss: 2.844047]\n",
      "epoch:35 step:27893 [D loss: 0.327204, acc.: 85.16%] [G loss: 2.516392]\n",
      "epoch:35 step:27894 [D loss: 0.316418, acc.: 88.28%] [G loss: 2.746280]\n",
      "epoch:35 step:27895 [D loss: 0.342697, acc.: 82.03%] [G loss: 2.591249]\n",
      "epoch:35 step:27896 [D loss: 0.346672, acc.: 85.16%] [G loss: 3.116225]\n",
      "epoch:35 step:27897 [D loss: 0.324573, acc.: 86.72%] [G loss: 2.815442]\n",
      "epoch:35 step:27898 [D loss: 0.349666, acc.: 85.94%] [G loss: 3.234704]\n",
      "epoch:35 step:27899 [D loss: 0.379910, acc.: 77.34%] [G loss: 2.206923]\n",
      "epoch:35 step:27900 [D loss: 0.302265, acc.: 88.28%] [G loss: 4.483457]\n",
      "epoch:35 step:27901 [D loss: 0.322150, acc.: 83.59%] [G loss: 2.685400]\n",
      "epoch:35 step:27902 [D loss: 0.260496, acc.: 87.50%] [G loss: 3.312528]\n",
      "epoch:35 step:27903 [D loss: 0.333185, acc.: 82.81%] [G loss: 4.460264]\n",
      "epoch:35 step:27904 [D loss: 0.208310, acc.: 90.62%] [G loss: 5.066111]\n",
      "epoch:35 step:27905 [D loss: 0.289546, acc.: 85.94%] [G loss: 3.405850]\n",
      "epoch:35 step:27906 [D loss: 0.273112, acc.: 87.50%] [G loss: 2.884572]\n",
      "epoch:35 step:27907 [D loss: 0.312994, acc.: 88.28%] [G loss: 3.324418]\n",
      "epoch:35 step:27908 [D loss: 0.273681, acc.: 88.28%] [G loss: 4.036712]\n",
      "epoch:35 step:27909 [D loss: 0.282432, acc.: 85.94%] [G loss: 4.642321]\n",
      "epoch:35 step:27910 [D loss: 0.438143, acc.: 79.69%] [G loss: 4.367805]\n",
      "epoch:35 step:27911 [D loss: 0.292597, acc.: 85.16%] [G loss: 5.478555]\n",
      "epoch:35 step:27912 [D loss: 0.251564, acc.: 90.62%] [G loss: 3.504809]\n",
      "epoch:35 step:27913 [D loss: 0.272394, acc.: 84.38%] [G loss: 2.788828]\n",
      "epoch:35 step:27914 [D loss: 0.270991, acc.: 88.28%] [G loss: 4.202409]\n",
      "epoch:35 step:27915 [D loss: 0.314813, acc.: 84.38%] [G loss: 2.844256]\n",
      "epoch:35 step:27916 [D loss: 0.277167, acc.: 88.28%] [G loss: 2.558518]\n",
      "epoch:35 step:27917 [D loss: 0.364934, acc.: 82.81%] [G loss: 2.172898]\n",
      "epoch:35 step:27918 [D loss: 0.343776, acc.: 82.81%] [G loss: 3.012930]\n",
      "epoch:35 step:27919 [D loss: 0.360982, acc.: 85.16%] [G loss: 2.579293]\n",
      "epoch:35 step:27920 [D loss: 0.337677, acc.: 85.16%] [G loss: 2.991311]\n",
      "epoch:35 step:27921 [D loss: 0.325442, acc.: 86.72%] [G loss: 2.739287]\n",
      "epoch:35 step:27922 [D loss: 0.379098, acc.: 85.16%] [G loss: 2.668954]\n",
      "epoch:35 step:27923 [D loss: 0.287903, acc.: 88.28%] [G loss: 2.751238]\n",
      "epoch:35 step:27924 [D loss: 0.257566, acc.: 89.84%] [G loss: 3.105893]\n",
      "epoch:35 step:27925 [D loss: 0.315323, acc.: 83.59%] [G loss: 3.991289]\n",
      "epoch:35 step:27926 [D loss: 0.411405, acc.: 79.69%] [G loss: 3.781012]\n",
      "epoch:35 step:27927 [D loss: 0.350029, acc.: 84.38%] [G loss: 3.368464]\n",
      "epoch:35 step:27928 [D loss: 0.297276, acc.: 85.16%] [G loss: 2.734132]\n",
      "epoch:35 step:27929 [D loss: 0.181750, acc.: 92.97%] [G loss: 3.180821]\n",
      "epoch:35 step:27930 [D loss: 0.478865, acc.: 82.03%] [G loss: 2.682787]\n",
      "epoch:35 step:27931 [D loss: 0.417776, acc.: 82.81%] [G loss: 3.131457]\n",
      "epoch:35 step:27932 [D loss: 0.431945, acc.: 77.34%] [G loss: 2.265051]\n",
      "epoch:35 step:27933 [D loss: 0.357579, acc.: 80.47%] [G loss: 3.319961]\n",
      "epoch:35 step:27934 [D loss: 0.340078, acc.: 82.81%] [G loss: 3.010148]\n",
      "epoch:35 step:27935 [D loss: 0.417814, acc.: 83.59%] [G loss: 3.560581]\n",
      "epoch:35 step:27936 [D loss: 0.334253, acc.: 84.38%] [G loss: 3.452552]\n",
      "epoch:35 step:27937 [D loss: 0.319126, acc.: 84.38%] [G loss: 3.726770]\n",
      "epoch:35 step:27938 [D loss: 0.249391, acc.: 87.50%] [G loss: 3.784217]\n",
      "epoch:35 step:27939 [D loss: 0.420140, acc.: 82.03%] [G loss: 3.084352]\n",
      "epoch:35 step:27940 [D loss: 0.365460, acc.: 83.59%] [G loss: 3.254388]\n",
      "epoch:35 step:27941 [D loss: 0.403885, acc.: 78.12%] [G loss: 2.820659]\n",
      "epoch:35 step:27942 [D loss: 0.200805, acc.: 92.19%] [G loss: 3.363371]\n",
      "epoch:35 step:27943 [D loss: 0.316835, acc.: 86.72%] [G loss: 3.821242]\n",
      "epoch:35 step:27944 [D loss: 0.294439, acc.: 87.50%] [G loss: 3.418461]\n",
      "epoch:35 step:27945 [D loss: 0.321425, acc.: 89.84%] [G loss: 3.470381]\n",
      "epoch:35 step:27946 [D loss: 0.400146, acc.: 79.69%] [G loss: 2.627257]\n",
      "epoch:35 step:27947 [D loss: 0.274382, acc.: 85.94%] [G loss: 3.866397]\n",
      "epoch:35 step:27948 [D loss: 0.538446, acc.: 78.12%] [G loss: 3.525877]\n",
      "epoch:35 step:27949 [D loss: 0.331424, acc.: 84.38%] [G loss: 5.218802]\n",
      "epoch:35 step:27950 [D loss: 0.330524, acc.: 87.50%] [G loss: 3.668256]\n",
      "epoch:35 step:27951 [D loss: 0.357722, acc.: 81.25%] [G loss: 5.413708]\n",
      "epoch:35 step:27952 [D loss: 0.364334, acc.: 85.16%] [G loss: 3.558330]\n",
      "epoch:35 step:27953 [D loss: 0.312233, acc.: 87.50%] [G loss: 3.272003]\n",
      "epoch:35 step:27954 [D loss: 0.357133, acc.: 84.38%] [G loss: 3.560285]\n",
      "epoch:35 step:27955 [D loss: 0.388878, acc.: 80.47%] [G loss: 2.964412]\n",
      "epoch:35 step:27956 [D loss: 0.260805, acc.: 90.62%] [G loss: 3.516445]\n",
      "epoch:35 step:27957 [D loss: 0.238371, acc.: 92.19%] [G loss: 3.929212]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:35 step:27958 [D loss: 0.436774, acc.: 80.47%] [G loss: 2.873880]\n",
      "epoch:35 step:27959 [D loss: 0.308728, acc.: 85.16%] [G loss: 2.536417]\n",
      "epoch:35 step:27960 [D loss: 0.321766, acc.: 85.16%] [G loss: 3.632741]\n",
      "epoch:35 step:27961 [D loss: 0.296352, acc.: 89.06%] [G loss: 3.064775]\n",
      "epoch:35 step:27962 [D loss: 0.310724, acc.: 85.94%] [G loss: 3.473485]\n",
      "epoch:35 step:27963 [D loss: 0.481401, acc.: 78.12%] [G loss: 2.767009]\n",
      "epoch:35 step:27964 [D loss: 0.306865, acc.: 86.72%] [G loss: 4.942564]\n",
      "epoch:35 step:27965 [D loss: 0.301283, acc.: 85.16%] [G loss: 6.736987]\n",
      "epoch:35 step:27966 [D loss: 0.261787, acc.: 89.06%] [G loss: 5.572956]\n",
      "epoch:35 step:27967 [D loss: 0.374292, acc.: 88.28%] [G loss: 3.764791]\n",
      "epoch:35 step:27968 [D loss: 0.226726, acc.: 91.41%] [G loss: 3.494050]\n",
      "epoch:35 step:27969 [D loss: 0.248355, acc.: 88.28%] [G loss: 3.529400]\n",
      "epoch:35 step:27970 [D loss: 0.325451, acc.: 85.16%] [G loss: 3.507936]\n",
      "epoch:35 step:27971 [D loss: 0.266630, acc.: 89.84%] [G loss: 3.717543]\n",
      "epoch:35 step:27972 [D loss: 0.236112, acc.: 92.19%] [G loss: 5.084177]\n",
      "epoch:35 step:27973 [D loss: 0.261997, acc.: 87.50%] [G loss: 4.621114]\n",
      "epoch:35 step:27974 [D loss: 0.328695, acc.: 85.16%] [G loss: 2.903017]\n",
      "epoch:35 step:27975 [D loss: 0.329927, acc.: 85.16%] [G loss: 4.366089]\n",
      "epoch:35 step:27976 [D loss: 0.261698, acc.: 88.28%] [G loss: 3.665937]\n",
      "epoch:35 step:27977 [D loss: 0.302231, acc.: 86.72%] [G loss: 3.297611]\n",
      "epoch:35 step:27978 [D loss: 0.433675, acc.: 79.69%] [G loss: 3.122710]\n",
      "epoch:35 step:27979 [D loss: 0.339647, acc.: 83.59%] [G loss: 3.316130]\n",
      "epoch:35 step:27980 [D loss: 0.393537, acc.: 84.38%] [G loss: 4.636871]\n",
      "epoch:35 step:27981 [D loss: 0.443033, acc.: 79.69%] [G loss: 4.346983]\n",
      "epoch:35 step:27982 [D loss: 0.390382, acc.: 81.25%] [G loss: 6.290833]\n",
      "epoch:35 step:27983 [D loss: 0.319501, acc.: 86.72%] [G loss: 4.668012]\n",
      "epoch:35 step:27984 [D loss: 0.282027, acc.: 85.94%] [G loss: 6.290843]\n",
      "epoch:35 step:27985 [D loss: 0.388767, acc.: 79.69%] [G loss: 4.396711]\n",
      "epoch:35 step:27986 [D loss: 0.194517, acc.: 92.19%] [G loss: 4.074778]\n",
      "epoch:35 step:27987 [D loss: 0.292936, acc.: 87.50%] [G loss: 4.107356]\n",
      "epoch:35 step:27988 [D loss: 0.338973, acc.: 86.72%] [G loss: 2.493968]\n",
      "epoch:35 step:27989 [D loss: 0.297546, acc.: 86.72%] [G loss: 3.667841]\n",
      "epoch:35 step:27990 [D loss: 0.320137, acc.: 85.16%] [G loss: 3.826547]\n",
      "epoch:35 step:27991 [D loss: 0.302309, acc.: 84.38%] [G loss: 2.691450]\n",
      "epoch:35 step:27992 [D loss: 0.463418, acc.: 75.00%] [G loss: 2.939554]\n",
      "epoch:35 step:27993 [D loss: 0.328575, acc.: 82.81%] [G loss: 3.144850]\n",
      "epoch:35 step:27994 [D loss: 0.403221, acc.: 79.69%] [G loss: 3.774565]\n",
      "epoch:35 step:27995 [D loss: 0.378390, acc.: 85.16%] [G loss: 3.628133]\n",
      "epoch:35 step:27996 [D loss: 0.397500, acc.: 80.47%] [G loss: 2.552338]\n",
      "epoch:35 step:27997 [D loss: 0.284520, acc.: 85.94%] [G loss: 4.906293]\n",
      "epoch:35 step:27998 [D loss: 0.371517, acc.: 78.12%] [G loss: 4.375879]\n",
      "epoch:35 step:27999 [D loss: 0.360148, acc.: 82.03%] [G loss: 4.979310]\n",
      "epoch:35 step:28000 [D loss: 0.315084, acc.: 82.81%] [G loss: 4.345393]\n",
      "##############\n",
      "[0.85812109 0.85218936 0.81756501 0.81458489 0.77009411 0.84157049\n",
      " 0.88297667 0.83844214 0.82821579 0.80938133]\n",
      "##########\n",
      "epoch:35 step:28001 [D loss: 0.304432, acc.: 84.38%] [G loss: 3.589646]\n",
      "epoch:35 step:28002 [D loss: 0.354371, acc.: 85.16%] [G loss: 3.621116]\n",
      "epoch:35 step:28003 [D loss: 0.243980, acc.: 90.62%] [G loss: 3.739419]\n",
      "epoch:35 step:28004 [D loss: 0.323935, acc.: 86.72%] [G loss: 3.874080]\n",
      "epoch:35 step:28005 [D loss: 0.357979, acc.: 81.25%] [G loss: 5.226217]\n",
      "epoch:35 step:28006 [D loss: 0.424768, acc.: 82.81%] [G loss: 4.258389]\n",
      "epoch:35 step:28007 [D loss: 0.475269, acc.: 80.47%] [G loss: 4.638740]\n",
      "epoch:35 step:28008 [D loss: 0.417743, acc.: 76.56%] [G loss: 7.695045]\n",
      "epoch:35 step:28009 [D loss: 0.742025, acc.: 71.09%] [G loss: 6.167865]\n",
      "epoch:35 step:28010 [D loss: 0.724157, acc.: 74.22%] [G loss: 5.770341]\n",
      "epoch:35 step:28011 [D loss: 0.949818, acc.: 66.41%] [G loss: 4.699424]\n",
      "epoch:35 step:28012 [D loss: 0.277408, acc.: 82.81%] [G loss: 5.792637]\n",
      "epoch:35 step:28013 [D loss: 0.468035, acc.: 82.03%] [G loss: 3.049199]\n",
      "epoch:35 step:28014 [D loss: 0.359342, acc.: 85.94%] [G loss: 4.229146]\n",
      "epoch:35 step:28015 [D loss: 0.448291, acc.: 78.12%] [G loss: 3.678740]\n",
      "epoch:35 step:28016 [D loss: 0.332622, acc.: 82.81%] [G loss: 2.958848]\n",
      "epoch:35 step:28017 [D loss: 0.461665, acc.: 78.12%] [G loss: 3.792587]\n",
      "epoch:35 step:28018 [D loss: 0.359750, acc.: 83.59%] [G loss: 2.809615]\n",
      "epoch:35 step:28019 [D loss: 0.455930, acc.: 76.56%] [G loss: 2.897981]\n",
      "epoch:35 step:28020 [D loss: 0.464749, acc.: 75.00%] [G loss: 2.867121]\n",
      "epoch:35 step:28021 [D loss: 0.271547, acc.: 89.84%] [G loss: 4.059847]\n",
      "epoch:35 step:28022 [D loss: 0.334244, acc.: 84.38%] [G loss: 3.770641]\n",
      "epoch:35 step:28023 [D loss: 0.424028, acc.: 82.03%] [G loss: 2.807690]\n",
      "epoch:35 step:28024 [D loss: 0.311373, acc.: 83.59%] [G loss: 3.466413]\n",
      "epoch:35 step:28025 [D loss: 0.324467, acc.: 83.59%] [G loss: 4.291198]\n",
      "epoch:35 step:28026 [D loss: 0.248070, acc.: 89.06%] [G loss: 3.209498]\n",
      "epoch:35 step:28027 [D loss: 0.264448, acc.: 91.41%] [G loss: 3.362953]\n",
      "epoch:35 step:28028 [D loss: 0.273585, acc.: 88.28%] [G loss: 3.182311]\n",
      "epoch:35 step:28029 [D loss: 0.297392, acc.: 85.94%] [G loss: 3.384291]\n",
      "epoch:35 step:28030 [D loss: 0.285212, acc.: 86.72%] [G loss: 3.016581]\n",
      "epoch:35 step:28031 [D loss: 0.354373, acc.: 83.59%] [G loss: 3.624459]\n",
      "epoch:35 step:28032 [D loss: 0.321809, acc.: 86.72%] [G loss: 2.866587]\n",
      "epoch:35 step:28033 [D loss: 0.285417, acc.: 89.84%] [G loss: 4.001140]\n",
      "epoch:35 step:28034 [D loss: 0.182244, acc.: 92.97%] [G loss: 4.624258]\n",
      "epoch:35 step:28035 [D loss: 0.382132, acc.: 82.81%] [G loss: 3.236521]\n",
      "epoch:35 step:28036 [D loss: 0.276785, acc.: 87.50%] [G loss: 4.121025]\n",
      "epoch:35 step:28037 [D loss: 0.370306, acc.: 82.03%] [G loss: 3.402261]\n",
      "epoch:35 step:28038 [D loss: 0.342249, acc.: 82.81%] [G loss: 3.165616]\n",
      "epoch:35 step:28039 [D loss: 0.271738, acc.: 87.50%] [G loss: 2.573538]\n",
      "epoch:35 step:28040 [D loss: 0.283485, acc.: 87.50%] [G loss: 3.522822]\n",
      "epoch:35 step:28041 [D loss: 0.318954, acc.: 89.84%] [G loss: 2.649796]\n",
      "epoch:35 step:28042 [D loss: 0.299577, acc.: 85.94%] [G loss: 3.245851]\n",
      "epoch:35 step:28043 [D loss: 0.329636, acc.: 87.50%] [G loss: 2.480731]\n",
      "epoch:35 step:28044 [D loss: 0.317884, acc.: 85.94%] [G loss: 3.255292]\n",
      "epoch:35 step:28045 [D loss: 0.401343, acc.: 82.03%] [G loss: 2.502975]\n",
      "epoch:35 step:28046 [D loss: 0.317877, acc.: 84.38%] [G loss: 3.304013]\n",
      "epoch:35 step:28047 [D loss: 0.347102, acc.: 84.38%] [G loss: 2.433876]\n",
      "epoch:35 step:28048 [D loss: 0.476935, acc.: 79.69%] [G loss: 2.745124]\n",
      "epoch:35 step:28049 [D loss: 0.256927, acc.: 88.28%] [G loss: 3.777354]\n",
      "epoch:35 step:28050 [D loss: 0.360699, acc.: 83.59%] [G loss: 2.676960]\n",
      "epoch:35 step:28051 [D loss: 0.248633, acc.: 89.84%] [G loss: 2.531657]\n",
      "epoch:35 step:28052 [D loss: 0.310779, acc.: 86.72%] [G loss: 5.703911]\n",
      "epoch:35 step:28053 [D loss: 0.254630, acc.: 89.84%] [G loss: 4.179519]\n",
      "epoch:35 step:28054 [D loss: 0.363493, acc.: 80.47%] [G loss: 3.478671]\n",
      "epoch:35 step:28055 [D loss: 0.380712, acc.: 83.59%] [G loss: 3.443994]\n",
      "epoch:35 step:28056 [D loss: 0.323058, acc.: 84.38%] [G loss: 2.891508]\n",
      "epoch:35 step:28057 [D loss: 0.290458, acc.: 87.50%] [G loss: 2.897666]\n",
      "epoch:35 step:28058 [D loss: 0.303834, acc.: 85.94%] [G loss: 2.614456]\n",
      "epoch:35 step:28059 [D loss: 0.260926, acc.: 89.06%] [G loss: 2.544343]\n",
      "epoch:35 step:28060 [D loss: 0.244797, acc.: 88.28%] [G loss: 3.439375]\n",
      "epoch:35 step:28061 [D loss: 0.231525, acc.: 90.62%] [G loss: 2.930242]\n",
      "epoch:35 step:28062 [D loss: 0.296786, acc.: 84.38%] [G loss: 3.351512]\n",
      "epoch:35 step:28063 [D loss: 0.314112, acc.: 88.28%] [G loss: 2.669978]\n",
      "epoch:35 step:28064 [D loss: 0.262234, acc.: 89.06%] [G loss: 2.467862]\n",
      "epoch:35 step:28065 [D loss: 0.281127, acc.: 88.28%] [G loss: 2.648837]\n",
      "epoch:35 step:28066 [D loss: 0.371144, acc.: 83.59%] [G loss: 2.600056]\n",
      "epoch:35 step:28067 [D loss: 0.330326, acc.: 85.16%] [G loss: 3.827994]\n",
      "epoch:35 step:28068 [D loss: 0.346544, acc.: 83.59%] [G loss: 3.331026]\n",
      "epoch:35 step:28069 [D loss: 0.333921, acc.: 82.81%] [G loss: 2.857130]\n",
      "epoch:35 step:28070 [D loss: 0.323602, acc.: 88.28%] [G loss: 2.575654]\n",
      "epoch:35 step:28071 [D loss: 0.418850, acc.: 82.81%] [G loss: 3.203740]\n",
      "epoch:35 step:28072 [D loss: 0.397916, acc.: 81.25%] [G loss: 2.627306]\n",
      "epoch:35 step:28073 [D loss: 0.278058, acc.: 87.50%] [G loss: 3.056978]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:35 step:28074 [D loss: 0.410490, acc.: 81.25%] [G loss: 2.644389]\n",
      "epoch:35 step:28075 [D loss: 0.309429, acc.: 82.03%] [G loss: 3.824107]\n",
      "epoch:35 step:28076 [D loss: 0.346220, acc.: 86.72%] [G loss: 3.148890]\n",
      "epoch:35 step:28077 [D loss: 0.285594, acc.: 85.94%] [G loss: 3.516493]\n",
      "epoch:35 step:28078 [D loss: 0.270507, acc.: 87.50%] [G loss: 3.231245]\n",
      "epoch:35 step:28079 [D loss: 0.354231, acc.: 85.16%] [G loss: 3.244443]\n",
      "epoch:35 step:28080 [D loss: 0.314348, acc.: 86.72%] [G loss: 3.571172]\n",
      "epoch:35 step:28081 [D loss: 0.289239, acc.: 85.16%] [G loss: 4.656695]\n",
      "epoch:35 step:28082 [D loss: 0.292717, acc.: 82.81%] [G loss: 3.299298]\n",
      "epoch:35 step:28083 [D loss: 0.266880, acc.: 88.28%] [G loss: 3.566520]\n",
      "epoch:35 step:28084 [D loss: 0.254541, acc.: 89.06%] [G loss: 2.772797]\n",
      "epoch:35 step:28085 [D loss: 0.478553, acc.: 80.47%] [G loss: 4.047492]\n",
      "epoch:35 step:28086 [D loss: 0.385260, acc.: 85.16%] [G loss: 4.018519]\n",
      "epoch:35 step:28087 [D loss: 0.381448, acc.: 81.25%] [G loss: 3.157069]\n",
      "epoch:35 step:28088 [D loss: 0.463755, acc.: 82.81%] [G loss: 3.651447]\n",
      "epoch:35 step:28089 [D loss: 0.356906, acc.: 80.47%] [G loss: 3.573789]\n",
      "epoch:35 step:28090 [D loss: 0.333079, acc.: 85.94%] [G loss: 3.966831]\n",
      "epoch:35 step:28091 [D loss: 0.354251, acc.: 86.72%] [G loss: 4.346272]\n",
      "epoch:35 step:28092 [D loss: 0.302598, acc.: 87.50%] [G loss: 2.840797]\n",
      "epoch:35 step:28093 [D loss: 0.290778, acc.: 88.28%] [G loss: 3.335927]\n",
      "epoch:35 step:28094 [D loss: 0.268553, acc.: 89.84%] [G loss: 3.378731]\n",
      "epoch:35 step:28095 [D loss: 0.385972, acc.: 85.94%] [G loss: 3.355263]\n",
      "epoch:35 step:28096 [D loss: 0.397401, acc.: 81.25%] [G loss: 4.338577]\n",
      "epoch:35 step:28097 [D loss: 0.320712, acc.: 89.06%] [G loss: 2.637074]\n",
      "epoch:35 step:28098 [D loss: 0.323919, acc.: 82.03%] [G loss: 4.636751]\n",
      "epoch:35 step:28099 [D loss: 0.258984, acc.: 86.72%] [G loss: 5.413040]\n",
      "epoch:35 step:28100 [D loss: 0.349246, acc.: 84.38%] [G loss: 6.446164]\n",
      "epoch:35 step:28101 [D loss: 0.317046, acc.: 85.94%] [G loss: 6.215082]\n",
      "epoch:35 step:28102 [D loss: 0.238464, acc.: 91.41%] [G loss: 5.728591]\n",
      "epoch:35 step:28103 [D loss: 0.201421, acc.: 89.84%] [G loss: 5.318860]\n",
      "epoch:35 step:28104 [D loss: 0.416543, acc.: 76.56%] [G loss: 3.982917]\n",
      "epoch:35 step:28105 [D loss: 0.289747, acc.: 87.50%] [G loss: 5.007842]\n",
      "epoch:35 step:28106 [D loss: 0.385370, acc.: 79.69%] [G loss: 4.007585]\n",
      "epoch:35 step:28107 [D loss: 0.315444, acc.: 84.38%] [G loss: 3.407123]\n",
      "epoch:35 step:28108 [D loss: 0.346375, acc.: 85.94%] [G loss: 3.229780]\n",
      "epoch:35 step:28109 [D loss: 0.408914, acc.: 82.03%] [G loss: 2.886611]\n",
      "epoch:35 step:28110 [D loss: 0.344215, acc.: 81.25%] [G loss: 3.377855]\n",
      "epoch:35 step:28111 [D loss: 0.219718, acc.: 92.19%] [G loss: 2.651349]\n",
      "epoch:35 step:28112 [D loss: 0.326248, acc.: 84.38%] [G loss: 3.147511]\n",
      "epoch:35 step:28113 [D loss: 0.222229, acc.: 90.62%] [G loss: 3.097095]\n",
      "epoch:35 step:28114 [D loss: 0.271719, acc.: 86.72%] [G loss: 2.836283]\n",
      "epoch:35 step:28115 [D loss: 0.239136, acc.: 92.19%] [G loss: 3.097332]\n",
      "epoch:35 step:28116 [D loss: 0.425366, acc.: 82.03%] [G loss: 2.724548]\n",
      "epoch:36 step:28117 [D loss: 0.174443, acc.: 95.31%] [G loss: 2.615688]\n",
      "epoch:36 step:28118 [D loss: 0.305922, acc.: 85.94%] [G loss: 3.445075]\n",
      "epoch:36 step:28119 [D loss: 0.383806, acc.: 82.81%] [G loss: 3.010906]\n",
      "epoch:36 step:28120 [D loss: 0.291944, acc.: 89.06%] [G loss: 2.782101]\n",
      "epoch:36 step:28121 [D loss: 0.400876, acc.: 80.47%] [G loss: 3.339826]\n",
      "epoch:36 step:28122 [D loss: 0.327126, acc.: 82.81%] [G loss: 3.482976]\n",
      "epoch:36 step:28123 [D loss: 0.255852, acc.: 87.50%] [G loss: 5.821562]\n",
      "epoch:36 step:28124 [D loss: 0.496254, acc.: 82.81%] [G loss: 2.981395]\n",
      "epoch:36 step:28125 [D loss: 0.473495, acc.: 78.12%] [G loss: 3.176203]\n",
      "epoch:36 step:28126 [D loss: 0.336583, acc.: 85.94%] [G loss: 3.499522]\n",
      "epoch:36 step:28127 [D loss: 0.366281, acc.: 80.47%] [G loss: 3.232164]\n",
      "epoch:36 step:28128 [D loss: 0.330882, acc.: 85.16%] [G loss: 3.643908]\n",
      "epoch:36 step:28129 [D loss: 0.284633, acc.: 89.06%] [G loss: 2.880581]\n",
      "epoch:36 step:28130 [D loss: 0.331564, acc.: 83.59%] [G loss: 3.706081]\n",
      "epoch:36 step:28131 [D loss: 0.285010, acc.: 83.59%] [G loss: 2.748540]\n",
      "epoch:36 step:28132 [D loss: 0.288427, acc.: 87.50%] [G loss: 3.120810]\n",
      "epoch:36 step:28133 [D loss: 0.324526, acc.: 84.38%] [G loss: 4.220977]\n",
      "epoch:36 step:28134 [D loss: 0.215130, acc.: 93.75%] [G loss: 3.162038]\n",
      "epoch:36 step:28135 [D loss: 0.263301, acc.: 88.28%] [G loss: 3.102981]\n",
      "epoch:36 step:28136 [D loss: 0.436551, acc.: 80.47%] [G loss: 2.485046]\n",
      "epoch:36 step:28137 [D loss: 0.322360, acc.: 84.38%] [G loss: 2.677630]\n",
      "epoch:36 step:28138 [D loss: 0.361762, acc.: 80.47%] [G loss: 2.977427]\n",
      "epoch:36 step:28139 [D loss: 0.261114, acc.: 89.84%] [G loss: 3.283731]\n",
      "epoch:36 step:28140 [D loss: 0.466600, acc.: 81.25%] [G loss: 5.746890]\n",
      "epoch:36 step:28141 [D loss: 0.262345, acc.: 89.06%] [G loss: 3.861675]\n",
      "epoch:36 step:28142 [D loss: 0.263786, acc.: 88.28%] [G loss: 4.571334]\n",
      "epoch:36 step:28143 [D loss: 0.276536, acc.: 85.16%] [G loss: 3.476596]\n",
      "epoch:36 step:28144 [D loss: 0.384318, acc.: 85.16%] [G loss: 2.935955]\n",
      "epoch:36 step:28145 [D loss: 0.330038, acc.: 82.81%] [G loss: 3.537767]\n",
      "epoch:36 step:28146 [D loss: 0.363529, acc.: 82.03%] [G loss: 3.291972]\n",
      "epoch:36 step:28147 [D loss: 0.304827, acc.: 89.06%] [G loss: 3.076694]\n",
      "epoch:36 step:28148 [D loss: 0.463663, acc.: 79.69%] [G loss: 3.740769]\n",
      "epoch:36 step:28149 [D loss: 0.365888, acc.: 85.16%] [G loss: 3.491604]\n",
      "epoch:36 step:28150 [D loss: 0.419811, acc.: 80.47%] [G loss: 3.173317]\n",
      "epoch:36 step:28151 [D loss: 0.367136, acc.: 82.81%] [G loss: 3.439649]\n",
      "epoch:36 step:28152 [D loss: 0.313958, acc.: 85.16%] [G loss: 3.132844]\n",
      "epoch:36 step:28153 [D loss: 0.284961, acc.: 88.28%] [G loss: 3.703549]\n",
      "epoch:36 step:28154 [D loss: 0.447062, acc.: 78.91%] [G loss: 3.557602]\n",
      "epoch:36 step:28155 [D loss: 0.377736, acc.: 85.94%] [G loss: 3.073762]\n",
      "epoch:36 step:28156 [D loss: 0.273832, acc.: 88.28%] [G loss: 2.803603]\n",
      "epoch:36 step:28157 [D loss: 0.313942, acc.: 84.38%] [G loss: 3.837474]\n",
      "epoch:36 step:28158 [D loss: 0.361010, acc.: 85.16%] [G loss: 3.751223]\n",
      "epoch:36 step:28159 [D loss: 0.426630, acc.: 75.00%] [G loss: 4.363941]\n",
      "epoch:36 step:28160 [D loss: 0.694704, acc.: 73.44%] [G loss: 5.787355]\n",
      "epoch:36 step:28161 [D loss: 1.205352, acc.: 57.03%] [G loss: 6.926369]\n",
      "epoch:36 step:28162 [D loss: 0.634620, acc.: 79.69%] [G loss: 4.428247]\n",
      "epoch:36 step:28163 [D loss: 0.444452, acc.: 80.47%] [G loss: 3.535475]\n",
      "epoch:36 step:28164 [D loss: 0.284979, acc.: 85.94%] [G loss: 4.890126]\n",
      "epoch:36 step:28165 [D loss: 0.200618, acc.: 91.41%] [G loss: 4.525114]\n",
      "epoch:36 step:28166 [D loss: 0.366792, acc.: 82.03%] [G loss: 2.956839]\n",
      "epoch:36 step:28167 [D loss: 0.233831, acc.: 90.62%] [G loss: 3.205863]\n",
      "epoch:36 step:28168 [D loss: 0.277961, acc.: 88.28%] [G loss: 3.470241]\n",
      "epoch:36 step:28169 [D loss: 0.385705, acc.: 82.81%] [G loss: 3.389507]\n",
      "epoch:36 step:28170 [D loss: 0.331974, acc.: 83.59%] [G loss: 3.377720]\n",
      "epoch:36 step:28171 [D loss: 0.259778, acc.: 88.28%] [G loss: 3.886448]\n",
      "epoch:36 step:28172 [D loss: 0.346905, acc.: 85.16%] [G loss: 3.050476]\n",
      "epoch:36 step:28173 [D loss: 0.302318, acc.: 87.50%] [G loss: 3.496894]\n",
      "epoch:36 step:28174 [D loss: 0.381078, acc.: 79.69%] [G loss: 2.909745]\n",
      "epoch:36 step:28175 [D loss: 0.303410, acc.: 85.16%] [G loss: 3.707140]\n",
      "epoch:36 step:28176 [D loss: 0.228564, acc.: 91.41%] [G loss: 3.670881]\n",
      "epoch:36 step:28177 [D loss: 0.324961, acc.: 86.72%] [G loss: 2.526638]\n",
      "epoch:36 step:28178 [D loss: 0.212225, acc.: 92.19%] [G loss: 3.262411]\n",
      "epoch:36 step:28179 [D loss: 0.299344, acc.: 86.72%] [G loss: 2.803792]\n",
      "epoch:36 step:28180 [D loss: 0.333476, acc.: 84.38%] [G loss: 3.052462]\n",
      "epoch:36 step:28181 [D loss: 0.344306, acc.: 84.38%] [G loss: 3.428643]\n",
      "epoch:36 step:28182 [D loss: 0.353133, acc.: 85.16%] [G loss: 3.124282]\n",
      "epoch:36 step:28183 [D loss: 0.276932, acc.: 90.62%] [G loss: 3.200978]\n",
      "epoch:36 step:28184 [D loss: 0.412754, acc.: 78.91%] [G loss: 2.610272]\n",
      "epoch:36 step:28185 [D loss: 0.321000, acc.: 87.50%] [G loss: 3.696206]\n",
      "epoch:36 step:28186 [D loss: 0.371421, acc.: 85.16%] [G loss: 2.389543]\n",
      "epoch:36 step:28187 [D loss: 0.355608, acc.: 81.25%] [G loss: 3.569259]\n",
      "epoch:36 step:28188 [D loss: 0.324660, acc.: 86.72%] [G loss: 3.742002]\n",
      "epoch:36 step:28189 [D loss: 0.309601, acc.: 88.28%] [G loss: 3.366496]\n",
      "epoch:36 step:28190 [D loss: 0.246052, acc.: 92.19%] [G loss: 3.087988]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:36 step:28191 [D loss: 0.316118, acc.: 82.81%] [G loss: 2.822082]\n",
      "epoch:36 step:28192 [D loss: 0.276017, acc.: 89.06%] [G loss: 3.660314]\n",
      "epoch:36 step:28193 [D loss: 0.422641, acc.: 81.25%] [G loss: 3.682321]\n",
      "epoch:36 step:28194 [D loss: 0.341667, acc.: 81.25%] [G loss: 2.538341]\n",
      "epoch:36 step:28195 [D loss: 0.287710, acc.: 87.50%] [G loss: 4.652349]\n",
      "epoch:36 step:28196 [D loss: 0.305334, acc.: 84.38%] [G loss: 3.580981]\n",
      "epoch:36 step:28197 [D loss: 0.447928, acc.: 82.03%] [G loss: 3.982501]\n",
      "epoch:36 step:28198 [D loss: 0.375429, acc.: 83.59%] [G loss: 3.200127]\n",
      "epoch:36 step:28199 [D loss: 0.313539, acc.: 89.06%] [G loss: 4.681869]\n",
      "epoch:36 step:28200 [D loss: 0.297889, acc.: 88.28%] [G loss: 4.808005]\n",
      "##############\n",
      "[0.8595954  0.85829275 0.80829011 0.817671   0.7874432  0.83639272\n",
      " 0.90023342 0.8320136  0.83188742 0.82063864]\n",
      "##########\n",
      "epoch:36 step:28201 [D loss: 0.397829, acc.: 85.94%] [G loss: 6.950912]\n",
      "epoch:36 step:28202 [D loss: 0.316679, acc.: 85.16%] [G loss: 3.525667]\n",
      "epoch:36 step:28203 [D loss: 0.327085, acc.: 85.16%] [G loss: 3.527701]\n",
      "epoch:36 step:28204 [D loss: 0.182341, acc.: 92.19%] [G loss: 5.415846]\n",
      "epoch:36 step:28205 [D loss: 0.308148, acc.: 88.28%] [G loss: 3.412934]\n",
      "epoch:36 step:28206 [D loss: 0.344447, acc.: 82.81%] [G loss: 2.235495]\n",
      "epoch:36 step:28207 [D loss: 0.232061, acc.: 91.41%] [G loss: 3.774649]\n",
      "epoch:36 step:28208 [D loss: 0.294148, acc.: 83.59%] [G loss: 5.146243]\n",
      "epoch:36 step:28209 [D loss: 0.332506, acc.: 84.38%] [G loss: 3.332564]\n",
      "epoch:36 step:28210 [D loss: 0.241200, acc.: 87.50%] [G loss: 4.840847]\n",
      "epoch:36 step:28211 [D loss: 0.370166, acc.: 83.59%] [G loss: 5.028189]\n",
      "epoch:36 step:28212 [D loss: 0.418486, acc.: 80.47%] [G loss: 4.113437]\n",
      "epoch:36 step:28213 [D loss: 0.235361, acc.: 89.06%] [G loss: 3.245904]\n",
      "epoch:36 step:28214 [D loss: 0.382379, acc.: 78.91%] [G loss: 3.167546]\n",
      "epoch:36 step:28215 [D loss: 0.297713, acc.: 88.28%] [G loss: 3.216452]\n",
      "epoch:36 step:28216 [D loss: 0.304637, acc.: 88.28%] [G loss: 3.412364]\n",
      "epoch:36 step:28217 [D loss: 0.277652, acc.: 85.94%] [G loss: 2.744371]\n",
      "epoch:36 step:28218 [D loss: 0.376862, acc.: 82.81%] [G loss: 2.295396]\n",
      "epoch:36 step:28219 [D loss: 0.294001, acc.: 87.50%] [G loss: 3.009211]\n",
      "epoch:36 step:28220 [D loss: 0.268852, acc.: 89.06%] [G loss: 3.078680]\n",
      "epoch:36 step:28221 [D loss: 0.366843, acc.: 83.59%] [G loss: 3.476601]\n",
      "epoch:36 step:28222 [D loss: 0.371166, acc.: 78.91%] [G loss: 3.510687]\n",
      "epoch:36 step:28223 [D loss: 0.280042, acc.: 89.84%] [G loss: 3.538756]\n",
      "epoch:36 step:28224 [D loss: 0.183546, acc.: 93.75%] [G loss: 3.800854]\n",
      "epoch:36 step:28225 [D loss: 0.315996, acc.: 86.72%] [G loss: 3.112261]\n",
      "epoch:36 step:28226 [D loss: 0.315545, acc.: 85.94%] [G loss: 3.064059]\n",
      "epoch:36 step:28227 [D loss: 0.419721, acc.: 81.25%] [G loss: 2.322290]\n",
      "epoch:36 step:28228 [D loss: 0.258765, acc.: 91.41%] [G loss: 2.388120]\n",
      "epoch:36 step:28229 [D loss: 0.401918, acc.: 82.81%] [G loss: 3.040918]\n",
      "epoch:36 step:28230 [D loss: 0.352328, acc.: 85.16%] [G loss: 2.463053]\n",
      "epoch:36 step:28231 [D loss: 0.336747, acc.: 86.72%] [G loss: 4.129993]\n",
      "epoch:36 step:28232 [D loss: 0.307586, acc.: 85.16%] [G loss: 3.681522]\n",
      "epoch:36 step:28233 [D loss: 0.248415, acc.: 92.19%] [G loss: 2.722433]\n",
      "epoch:36 step:28234 [D loss: 0.347904, acc.: 87.50%] [G loss: 4.660733]\n",
      "epoch:36 step:28235 [D loss: 0.322639, acc.: 84.38%] [G loss: 3.803179]\n",
      "epoch:36 step:28236 [D loss: 0.330758, acc.: 85.16%] [G loss: 3.122797]\n",
      "epoch:36 step:28237 [D loss: 0.356452, acc.: 78.91%] [G loss: 3.501410]\n",
      "epoch:36 step:28238 [D loss: 0.286684, acc.: 86.72%] [G loss: 2.430557]\n",
      "epoch:36 step:28239 [D loss: 0.336520, acc.: 83.59%] [G loss: 4.288005]\n",
      "epoch:36 step:28240 [D loss: 0.404671, acc.: 83.59%] [G loss: 4.733253]\n",
      "epoch:36 step:28241 [D loss: 0.686468, acc.: 71.88%] [G loss: 8.874882]\n",
      "epoch:36 step:28242 [D loss: 1.428547, acc.: 54.69%] [G loss: 10.412479]\n",
      "epoch:36 step:28243 [D loss: 2.218650, acc.: 62.50%] [G loss: 6.214168]\n",
      "epoch:36 step:28244 [D loss: 0.745364, acc.: 74.22%] [G loss: 6.980010]\n",
      "epoch:36 step:28245 [D loss: 0.687803, acc.: 82.03%] [G loss: 5.448509]\n",
      "epoch:36 step:28246 [D loss: 0.398973, acc.: 85.16%] [G loss: 4.445409]\n",
      "epoch:36 step:28247 [D loss: 0.348136, acc.: 85.16%] [G loss: 5.215618]\n",
      "epoch:36 step:28248 [D loss: 0.247356, acc.: 90.62%] [G loss: 4.345093]\n",
      "epoch:36 step:28249 [D loss: 0.397849, acc.: 82.03%] [G loss: 3.777964]\n",
      "epoch:36 step:28250 [D loss: 0.405539, acc.: 83.59%] [G loss: 3.339895]\n",
      "epoch:36 step:28251 [D loss: 0.337568, acc.: 82.03%] [G loss: 3.740007]\n",
      "epoch:36 step:28252 [D loss: 0.307384, acc.: 84.38%] [G loss: 3.060227]\n",
      "epoch:36 step:28253 [D loss: 0.245048, acc.: 89.06%] [G loss: 3.518116]\n",
      "epoch:36 step:28254 [D loss: 0.389902, acc.: 80.47%] [G loss: 2.868787]\n",
      "epoch:36 step:28255 [D loss: 0.338648, acc.: 85.16%] [G loss: 3.602506]\n",
      "epoch:36 step:28256 [D loss: 0.248109, acc.: 89.06%] [G loss: 2.722996]\n",
      "epoch:36 step:28257 [D loss: 0.337632, acc.: 83.59%] [G loss: 3.569034]\n",
      "epoch:36 step:28258 [D loss: 0.326880, acc.: 84.38%] [G loss: 3.050199]\n",
      "epoch:36 step:28259 [D loss: 0.332861, acc.: 85.16%] [G loss: 2.938627]\n",
      "epoch:36 step:28260 [D loss: 0.401067, acc.: 82.03%] [G loss: 3.078349]\n",
      "epoch:36 step:28261 [D loss: 0.360657, acc.: 86.72%] [G loss: 3.088078]\n",
      "epoch:36 step:28262 [D loss: 0.324323, acc.: 84.38%] [G loss: 3.287557]\n",
      "epoch:36 step:28263 [D loss: 0.413522, acc.: 76.56%] [G loss: 2.773864]\n",
      "epoch:36 step:28264 [D loss: 0.320246, acc.: 81.25%] [G loss: 3.191707]\n",
      "epoch:36 step:28265 [D loss: 0.300108, acc.: 84.38%] [G loss: 2.930301]\n",
      "epoch:36 step:28266 [D loss: 0.369874, acc.: 82.03%] [G loss: 2.994267]\n",
      "epoch:36 step:28267 [D loss: 0.237993, acc.: 90.62%] [G loss: 2.958293]\n",
      "epoch:36 step:28268 [D loss: 0.321922, acc.: 88.28%] [G loss: 3.358331]\n",
      "epoch:36 step:28269 [D loss: 0.278489, acc.: 89.06%] [G loss: 2.471179]\n",
      "epoch:36 step:28270 [D loss: 0.278291, acc.: 89.06%] [G loss: 2.633196]\n",
      "epoch:36 step:28271 [D loss: 0.355064, acc.: 85.94%] [G loss: 2.048091]\n",
      "epoch:36 step:28272 [D loss: 0.304882, acc.: 84.38%] [G loss: 2.606340]\n",
      "epoch:36 step:28273 [D loss: 0.310324, acc.: 84.38%] [G loss: 2.688576]\n",
      "epoch:36 step:28274 [D loss: 0.350015, acc.: 86.72%] [G loss: 2.822904]\n",
      "epoch:36 step:28275 [D loss: 0.286260, acc.: 87.50%] [G loss: 2.830756]\n",
      "epoch:36 step:28276 [D loss: 0.353591, acc.: 84.38%] [G loss: 2.750476]\n",
      "epoch:36 step:28277 [D loss: 0.272804, acc.: 85.16%] [G loss: 2.433797]\n",
      "epoch:36 step:28278 [D loss: 0.230792, acc.: 89.06%] [G loss: 3.168694]\n",
      "epoch:36 step:28279 [D loss: 0.377961, acc.: 83.59%] [G loss: 3.408268]\n",
      "epoch:36 step:28280 [D loss: 0.311820, acc.: 86.72%] [G loss: 3.110789]\n",
      "epoch:36 step:28281 [D loss: 0.349140, acc.: 82.81%] [G loss: 2.856600]\n",
      "epoch:36 step:28282 [D loss: 0.314325, acc.: 85.16%] [G loss: 3.234069]\n",
      "epoch:36 step:28283 [D loss: 0.320025, acc.: 85.16%] [G loss: 3.089792]\n",
      "epoch:36 step:28284 [D loss: 0.304616, acc.: 88.28%] [G loss: 2.352299]\n",
      "epoch:36 step:28285 [D loss: 0.446191, acc.: 79.69%] [G loss: 3.080544]\n",
      "epoch:36 step:28286 [D loss: 0.412146, acc.: 83.59%] [G loss: 2.687092]\n",
      "epoch:36 step:28287 [D loss: 0.464745, acc.: 78.12%] [G loss: 2.569485]\n",
      "epoch:36 step:28288 [D loss: 0.216968, acc.: 89.06%] [G loss: 2.437006]\n",
      "epoch:36 step:28289 [D loss: 0.406509, acc.: 84.38%] [G loss: 3.138137]\n",
      "epoch:36 step:28290 [D loss: 0.363949, acc.: 83.59%] [G loss: 2.741702]\n",
      "epoch:36 step:28291 [D loss: 0.331068, acc.: 85.94%] [G loss: 2.647651]\n",
      "epoch:36 step:28292 [D loss: 0.319229, acc.: 85.94%] [G loss: 3.030915]\n",
      "epoch:36 step:28293 [D loss: 0.376857, acc.: 81.25%] [G loss: 3.846340]\n",
      "epoch:36 step:28294 [D loss: 0.252120, acc.: 89.06%] [G loss: 2.746375]\n",
      "epoch:36 step:28295 [D loss: 0.367270, acc.: 82.81%] [G loss: 3.348868]\n",
      "epoch:36 step:28296 [D loss: 0.444739, acc.: 78.91%] [G loss: 3.296844]\n",
      "epoch:36 step:28297 [D loss: 0.309548, acc.: 86.72%] [G loss: 2.687897]\n",
      "epoch:36 step:28298 [D loss: 0.311259, acc.: 85.16%] [G loss: 3.482440]\n",
      "epoch:36 step:28299 [D loss: 0.342444, acc.: 85.94%] [G loss: 3.787613]\n",
      "epoch:36 step:28300 [D loss: 0.319324, acc.: 85.94%] [G loss: 3.309690]\n",
      "epoch:36 step:28301 [D loss: 0.237723, acc.: 89.06%] [G loss: 3.075603]\n",
      "epoch:36 step:28302 [D loss: 0.246738, acc.: 91.41%] [G loss: 3.560219]\n",
      "epoch:36 step:28303 [D loss: 0.247329, acc.: 89.84%] [G loss: 3.989329]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:36 step:28304 [D loss: 0.320116, acc.: 82.03%] [G loss: 3.535416]\n",
      "epoch:36 step:28305 [D loss: 0.257871, acc.: 86.72%] [G loss: 3.466085]\n",
      "epoch:36 step:28306 [D loss: 0.321308, acc.: 85.16%] [G loss: 3.382546]\n",
      "epoch:36 step:28307 [D loss: 0.329832, acc.: 85.94%] [G loss: 3.455940]\n",
      "epoch:36 step:28308 [D loss: 0.345845, acc.: 81.25%] [G loss: 4.580896]\n",
      "epoch:36 step:28309 [D loss: 0.273228, acc.: 89.06%] [G loss: 4.383858]\n",
      "epoch:36 step:28310 [D loss: 0.290003, acc.: 88.28%] [G loss: 3.698952]\n",
      "epoch:36 step:28311 [D loss: 0.257177, acc.: 89.84%] [G loss: 4.327283]\n",
      "epoch:36 step:28312 [D loss: 0.285837, acc.: 89.06%] [G loss: 2.684462]\n",
      "epoch:36 step:28313 [D loss: 0.280385, acc.: 89.06%] [G loss: 3.782834]\n",
      "epoch:36 step:28314 [D loss: 0.298320, acc.: 85.94%] [G loss: 2.691176]\n",
      "epoch:36 step:28315 [D loss: 0.248561, acc.: 85.94%] [G loss: 4.223485]\n",
      "epoch:36 step:28316 [D loss: 0.245611, acc.: 89.06%] [G loss: 3.519834]\n",
      "epoch:36 step:28317 [D loss: 0.265670, acc.: 88.28%] [G loss: 4.776025]\n",
      "epoch:36 step:28318 [D loss: 0.238205, acc.: 89.06%] [G loss: 4.637821]\n",
      "epoch:36 step:28319 [D loss: 0.193983, acc.: 92.97%] [G loss: 4.487493]\n",
      "epoch:36 step:28320 [D loss: 0.191764, acc.: 93.75%] [G loss: 4.809369]\n",
      "epoch:36 step:28321 [D loss: 0.282151, acc.: 86.72%] [G loss: 4.167573]\n",
      "epoch:36 step:28322 [D loss: 0.349942, acc.: 82.03%] [G loss: 2.945237]\n",
      "epoch:36 step:28323 [D loss: 0.257645, acc.: 89.84%] [G loss: 3.380298]\n",
      "epoch:36 step:28324 [D loss: 0.323113, acc.: 84.38%] [G loss: 3.209585]\n",
      "epoch:36 step:28325 [D loss: 0.348370, acc.: 81.25%] [G loss: 3.604222]\n",
      "epoch:36 step:28326 [D loss: 0.325927, acc.: 83.59%] [G loss: 3.437089]\n",
      "epoch:36 step:28327 [D loss: 0.387516, acc.: 83.59%] [G loss: 4.426099]\n",
      "epoch:36 step:28328 [D loss: 0.327428, acc.: 86.72%] [G loss: 3.438844]\n",
      "epoch:36 step:28329 [D loss: 0.183021, acc.: 92.97%] [G loss: 3.591927]\n",
      "epoch:36 step:28330 [D loss: 0.330381, acc.: 85.94%] [G loss: 4.027679]\n",
      "epoch:36 step:28331 [D loss: 0.376704, acc.: 82.03%] [G loss: 4.399869]\n",
      "epoch:36 step:28332 [D loss: 0.411689, acc.: 83.59%] [G loss: 3.363158]\n",
      "epoch:36 step:28333 [D loss: 0.385245, acc.: 81.25%] [G loss: 6.345677]\n",
      "epoch:36 step:28334 [D loss: 0.391099, acc.: 83.59%] [G loss: 3.182965]\n",
      "epoch:36 step:28335 [D loss: 0.331894, acc.: 90.62%] [G loss: 3.097880]\n",
      "epoch:36 step:28336 [D loss: 0.293809, acc.: 90.62%] [G loss: 5.030454]\n",
      "epoch:36 step:28337 [D loss: 0.253885, acc.: 86.72%] [G loss: 3.739999]\n",
      "epoch:36 step:28338 [D loss: 0.275226, acc.: 86.72%] [G loss: 4.106051]\n",
      "epoch:36 step:28339 [D loss: 0.373140, acc.: 84.38%] [G loss: 3.003907]\n",
      "epoch:36 step:28340 [D loss: 0.302419, acc.: 85.94%] [G loss: 3.274636]\n",
      "epoch:36 step:28341 [D loss: 0.414332, acc.: 82.03%] [G loss: 3.060308]\n",
      "epoch:36 step:28342 [D loss: 0.350124, acc.: 84.38%] [G loss: 3.370987]\n",
      "epoch:36 step:28343 [D loss: 0.286272, acc.: 86.72%] [G loss: 2.595345]\n",
      "epoch:36 step:28344 [D loss: 0.400776, acc.: 81.25%] [G loss: 2.595357]\n",
      "epoch:36 step:28345 [D loss: 0.323245, acc.: 88.28%] [G loss: 2.626070]\n",
      "epoch:36 step:28346 [D loss: 0.398891, acc.: 79.69%] [G loss: 3.937634]\n",
      "epoch:36 step:28347 [D loss: 0.249836, acc.: 88.28%] [G loss: 3.371143]\n",
      "epoch:36 step:28348 [D loss: 0.279948, acc.: 87.50%] [G loss: 3.669497]\n",
      "epoch:36 step:28349 [D loss: 0.328574, acc.: 82.81%] [G loss: 3.054624]\n",
      "epoch:36 step:28350 [D loss: 0.361376, acc.: 82.81%] [G loss: 2.783746]\n",
      "epoch:36 step:28351 [D loss: 0.365270, acc.: 87.50%] [G loss: 2.891610]\n",
      "epoch:36 step:28352 [D loss: 0.333399, acc.: 89.06%] [G loss: 3.028384]\n",
      "epoch:36 step:28353 [D loss: 0.328026, acc.: 86.72%] [G loss: 3.210409]\n",
      "epoch:36 step:28354 [D loss: 0.379148, acc.: 83.59%] [G loss: 4.087314]\n",
      "epoch:36 step:28355 [D loss: 0.257416, acc.: 85.94%] [G loss: 3.213936]\n",
      "epoch:36 step:28356 [D loss: 0.346328, acc.: 86.72%] [G loss: 2.919635]\n",
      "epoch:36 step:28357 [D loss: 0.246990, acc.: 90.62%] [G loss: 2.552256]\n",
      "epoch:36 step:28358 [D loss: 0.314610, acc.: 85.16%] [G loss: 2.785650]\n",
      "epoch:36 step:28359 [D loss: 0.268625, acc.: 87.50%] [G loss: 3.203238]\n",
      "epoch:36 step:28360 [D loss: 0.374592, acc.: 84.38%] [G loss: 3.171105]\n",
      "epoch:36 step:28361 [D loss: 0.296938, acc.: 87.50%] [G loss: 3.014386]\n",
      "epoch:36 step:28362 [D loss: 0.356375, acc.: 83.59%] [G loss: 2.709693]\n",
      "epoch:36 step:28363 [D loss: 0.293541, acc.: 89.84%] [G loss: 3.086720]\n",
      "epoch:36 step:28364 [D loss: 0.392608, acc.: 80.47%] [G loss: 2.388145]\n",
      "epoch:36 step:28365 [D loss: 0.385308, acc.: 83.59%] [G loss: 2.547243]\n",
      "epoch:36 step:28366 [D loss: 0.329388, acc.: 87.50%] [G loss: 2.954963]\n",
      "epoch:36 step:28367 [D loss: 0.233606, acc.: 90.62%] [G loss: 3.396879]\n",
      "epoch:36 step:28368 [D loss: 0.373283, acc.: 82.81%] [G loss: 3.061004]\n",
      "epoch:36 step:28369 [D loss: 0.231086, acc.: 90.62%] [G loss: 3.637597]\n",
      "epoch:36 step:28370 [D loss: 0.278041, acc.: 88.28%] [G loss: 3.075631]\n",
      "epoch:36 step:28371 [D loss: 0.276859, acc.: 85.94%] [G loss: 3.858028]\n",
      "epoch:36 step:28372 [D loss: 0.322436, acc.: 84.38%] [G loss: 3.030508]\n",
      "epoch:36 step:28373 [D loss: 0.246818, acc.: 89.84%] [G loss: 4.422228]\n",
      "epoch:36 step:28374 [D loss: 0.294977, acc.: 85.16%] [G loss: 4.174999]\n",
      "epoch:36 step:28375 [D loss: 0.225055, acc.: 92.19%] [G loss: 3.462505]\n",
      "epoch:36 step:28376 [D loss: 0.245236, acc.: 88.28%] [G loss: 4.221190]\n",
      "epoch:36 step:28377 [D loss: 0.342344, acc.: 87.50%] [G loss: 4.586514]\n",
      "epoch:36 step:28378 [D loss: 0.260762, acc.: 88.28%] [G loss: 4.318744]\n",
      "epoch:36 step:28379 [D loss: 0.273781, acc.: 86.72%] [G loss: 2.814259]\n",
      "epoch:36 step:28380 [D loss: 0.301197, acc.: 85.16%] [G loss: 4.895008]\n",
      "epoch:36 step:28381 [D loss: 0.302933, acc.: 84.38%] [G loss: 3.480974]\n",
      "epoch:36 step:28382 [D loss: 0.260955, acc.: 88.28%] [G loss: 3.821297]\n",
      "epoch:36 step:28383 [D loss: 0.389263, acc.: 82.81%] [G loss: 3.967539]\n",
      "epoch:36 step:28384 [D loss: 0.294355, acc.: 88.28%] [G loss: 3.383722]\n",
      "epoch:36 step:28385 [D loss: 0.314769, acc.: 88.28%] [G loss: 2.999323]\n",
      "epoch:36 step:28386 [D loss: 0.327248, acc.: 89.06%] [G loss: 4.571498]\n",
      "epoch:36 step:28387 [D loss: 0.187323, acc.: 92.97%] [G loss: 4.594797]\n",
      "epoch:36 step:28388 [D loss: 0.277629, acc.: 88.28%] [G loss: 4.270383]\n",
      "epoch:36 step:28389 [D loss: 0.250350, acc.: 87.50%] [G loss: 3.438870]\n",
      "epoch:36 step:28390 [D loss: 0.292156, acc.: 89.84%] [G loss: 4.799617]\n",
      "epoch:36 step:28391 [D loss: 0.350960, acc.: 83.59%] [G loss: 8.796337]\n",
      "epoch:36 step:28392 [D loss: 0.779992, acc.: 69.53%] [G loss: 4.319787]\n",
      "epoch:36 step:28393 [D loss: 0.362668, acc.: 87.50%] [G loss: 3.465082]\n",
      "epoch:36 step:28394 [D loss: 0.395586, acc.: 78.91%] [G loss: 4.607512]\n",
      "epoch:36 step:28395 [D loss: 0.626794, acc.: 71.09%] [G loss: 4.377537]\n",
      "epoch:36 step:28396 [D loss: 0.212944, acc.: 90.62%] [G loss: 4.102086]\n",
      "epoch:36 step:28397 [D loss: 0.318802, acc.: 86.72%] [G loss: 3.679702]\n",
      "epoch:36 step:28398 [D loss: 0.392903, acc.: 81.25%] [G loss: 2.980612]\n",
      "epoch:36 step:28399 [D loss: 0.278397, acc.: 89.84%] [G loss: 3.661950]\n",
      "epoch:36 step:28400 [D loss: 0.363906, acc.: 79.69%] [G loss: 3.011453]\n",
      "##############\n",
      "[0.84830179 0.86964747 0.82537548 0.79586487 0.76123816 0.81341246\n",
      " 0.88292513 0.85212812 0.82834455 0.80604638]\n",
      "##########\n",
      "epoch:36 step:28401 [D loss: 0.269063, acc.: 88.28%] [G loss: 2.836965]\n",
      "epoch:36 step:28402 [D loss: 0.303359, acc.: 89.06%] [G loss: 3.323086]\n",
      "epoch:36 step:28403 [D loss: 0.298483, acc.: 85.94%] [G loss: 2.951970]\n",
      "epoch:36 step:28404 [D loss: 0.348489, acc.: 82.03%] [G loss: 2.532804]\n",
      "epoch:36 step:28405 [D loss: 0.374635, acc.: 82.81%] [G loss: 2.540998]\n",
      "epoch:36 step:28406 [D loss: 0.348300, acc.: 82.81%] [G loss: 3.621595]\n",
      "epoch:36 step:28407 [D loss: 0.301184, acc.: 87.50%] [G loss: 2.971169]\n",
      "epoch:36 step:28408 [D loss: 0.315214, acc.: 85.16%] [G loss: 3.598413]\n",
      "epoch:36 step:28409 [D loss: 0.318375, acc.: 85.16%] [G loss: 3.096224]\n",
      "epoch:36 step:28410 [D loss: 0.381334, acc.: 84.38%] [G loss: 3.285686]\n",
      "epoch:36 step:28411 [D loss: 0.394161, acc.: 83.59%] [G loss: 4.187393]\n",
      "epoch:36 step:28412 [D loss: 0.298777, acc.: 84.38%] [G loss: 3.367782]\n",
      "epoch:36 step:28413 [D loss: 0.259245, acc.: 90.62%] [G loss: 3.604851]\n",
      "epoch:36 step:28414 [D loss: 0.230021, acc.: 92.97%] [G loss: 4.424371]\n",
      "epoch:36 step:28415 [D loss: 0.258785, acc.: 90.62%] [G loss: 2.916822]\n",
      "epoch:36 step:28416 [D loss: 0.376559, acc.: 80.47%] [G loss: 2.879317]\n",
      "epoch:36 step:28417 [D loss: 0.330239, acc.: 85.16%] [G loss: 3.436802]\n",
      "epoch:36 step:28418 [D loss: 0.324756, acc.: 85.16%] [G loss: 3.741895]\n",
      "epoch:36 step:28419 [D loss: 0.294385, acc.: 89.06%] [G loss: 2.744233]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:36 step:28420 [D loss: 0.227999, acc.: 92.19%] [G loss: 2.845006]\n",
      "epoch:36 step:28421 [D loss: 0.350252, acc.: 85.94%] [G loss: 2.843371]\n",
      "epoch:36 step:28422 [D loss: 0.336484, acc.: 82.03%] [G loss: 2.567916]\n",
      "epoch:36 step:28423 [D loss: 0.310421, acc.: 88.28%] [G loss: 3.187197]\n",
      "epoch:36 step:28424 [D loss: 0.246703, acc.: 88.28%] [G loss: 3.636026]\n",
      "epoch:36 step:28425 [D loss: 0.313089, acc.: 85.94%] [G loss: 3.308220]\n",
      "epoch:36 step:28426 [D loss: 0.234957, acc.: 92.97%] [G loss: 2.913215]\n",
      "epoch:36 step:28427 [D loss: 0.313470, acc.: 83.59%] [G loss: 5.867436]\n",
      "epoch:36 step:28428 [D loss: 0.357185, acc.: 79.69%] [G loss: 4.178672]\n",
      "epoch:36 step:28429 [D loss: 0.344093, acc.: 85.94%] [G loss: 4.709929]\n",
      "epoch:36 step:28430 [D loss: 0.244712, acc.: 88.28%] [G loss: 4.418075]\n",
      "epoch:36 step:28431 [D loss: 0.589226, acc.: 70.31%] [G loss: 3.817744]\n",
      "epoch:36 step:28432 [D loss: 0.327372, acc.: 85.16%] [G loss: 3.330102]\n",
      "epoch:36 step:28433 [D loss: 0.470196, acc.: 75.00%] [G loss: 3.219075]\n",
      "epoch:36 step:28434 [D loss: 0.381083, acc.: 82.03%] [G loss: 3.103294]\n",
      "epoch:36 step:28435 [D loss: 0.290439, acc.: 88.28%] [G loss: 3.331225]\n",
      "epoch:36 step:28436 [D loss: 0.353092, acc.: 83.59%] [G loss: 2.469568]\n",
      "epoch:36 step:28437 [D loss: 0.344006, acc.: 85.94%] [G loss: 2.623887]\n",
      "epoch:36 step:28438 [D loss: 0.264019, acc.: 89.06%] [G loss: 4.115288]\n",
      "epoch:36 step:28439 [D loss: 0.337721, acc.: 83.59%] [G loss: 3.379406]\n",
      "epoch:36 step:28440 [D loss: 0.293691, acc.: 86.72%] [G loss: 3.557986]\n",
      "epoch:36 step:28441 [D loss: 0.483705, acc.: 76.56%] [G loss: 2.325655]\n",
      "epoch:36 step:28442 [D loss: 0.304101, acc.: 86.72%] [G loss: 3.329942]\n",
      "epoch:36 step:28443 [D loss: 0.384260, acc.: 82.81%] [G loss: 2.924682]\n",
      "epoch:36 step:28444 [D loss: 0.378672, acc.: 79.69%] [G loss: 2.987183]\n",
      "epoch:36 step:28445 [D loss: 0.272214, acc.: 89.84%] [G loss: 3.501522]\n",
      "epoch:36 step:28446 [D loss: 0.311590, acc.: 85.16%] [G loss: 3.263740]\n",
      "epoch:36 step:28447 [D loss: 0.333199, acc.: 85.16%] [G loss: 2.601253]\n",
      "epoch:36 step:28448 [D loss: 0.357259, acc.: 82.03%] [G loss: 2.978511]\n",
      "epoch:36 step:28449 [D loss: 0.229967, acc.: 88.28%] [G loss: 2.855564]\n",
      "epoch:36 step:28450 [D loss: 0.356512, acc.: 85.16%] [G loss: 2.989694]\n",
      "epoch:36 step:28451 [D loss: 0.380653, acc.: 82.81%] [G loss: 3.572078]\n",
      "epoch:36 step:28452 [D loss: 0.311617, acc.: 85.16%] [G loss: 3.144640]\n",
      "epoch:36 step:28453 [D loss: 0.349229, acc.: 86.72%] [G loss: 4.738338]\n",
      "epoch:36 step:28454 [D loss: 0.277826, acc.: 87.50%] [G loss: 3.106575]\n",
      "epoch:36 step:28455 [D loss: 0.272825, acc.: 89.84%] [G loss: 4.534883]\n",
      "epoch:36 step:28456 [D loss: 0.335184, acc.: 82.03%] [G loss: 3.862576]\n",
      "epoch:36 step:28457 [D loss: 0.371005, acc.: 82.81%] [G loss: 3.466455]\n",
      "epoch:36 step:28458 [D loss: 0.334288, acc.: 88.28%] [G loss: 3.756586]\n",
      "epoch:36 step:28459 [D loss: 0.306801, acc.: 87.50%] [G loss: 3.249125]\n",
      "epoch:36 step:28460 [D loss: 0.307291, acc.: 87.50%] [G loss: 3.349751]\n",
      "epoch:36 step:28461 [D loss: 0.311642, acc.: 84.38%] [G loss: 3.045535]\n",
      "epoch:36 step:28462 [D loss: 0.279553, acc.: 89.06%] [G loss: 3.368325]\n",
      "epoch:36 step:28463 [D loss: 0.252001, acc.: 91.41%] [G loss: 4.103438]\n",
      "epoch:36 step:28464 [D loss: 0.288864, acc.: 89.84%] [G loss: 3.623261]\n",
      "epoch:36 step:28465 [D loss: 0.279006, acc.: 84.38%] [G loss: 3.446580]\n",
      "epoch:36 step:28466 [D loss: 0.328989, acc.: 85.16%] [G loss: 5.284374]\n",
      "epoch:36 step:28467 [D loss: 0.297928, acc.: 85.94%] [G loss: 3.878776]\n",
      "epoch:36 step:28468 [D loss: 0.335734, acc.: 82.81%] [G loss: 3.762283]\n",
      "epoch:36 step:28469 [D loss: 0.273425, acc.: 90.62%] [G loss: 2.662217]\n",
      "epoch:36 step:28470 [D loss: 0.261873, acc.: 88.28%] [G loss: 4.781501]\n",
      "epoch:36 step:28471 [D loss: 0.256076, acc.: 90.62%] [G loss: 3.133735]\n",
      "epoch:36 step:28472 [D loss: 0.366259, acc.: 82.81%] [G loss: 3.032108]\n",
      "epoch:36 step:28473 [D loss: 0.367164, acc.: 82.81%] [G loss: 2.712629]\n",
      "epoch:36 step:28474 [D loss: 0.267787, acc.: 88.28%] [G loss: 3.641320]\n",
      "epoch:36 step:28475 [D loss: 0.290496, acc.: 83.59%] [G loss: 3.071163]\n",
      "epoch:36 step:28476 [D loss: 0.356880, acc.: 79.69%] [G loss: 3.241523]\n",
      "epoch:36 step:28477 [D loss: 0.308683, acc.: 87.50%] [G loss: 3.256273]\n",
      "epoch:36 step:28478 [D loss: 0.432626, acc.: 81.25%] [G loss: 3.747988]\n",
      "epoch:36 step:28479 [D loss: 0.465148, acc.: 77.34%] [G loss: 4.831187]\n",
      "epoch:36 step:28480 [D loss: 0.470998, acc.: 78.91%] [G loss: 4.500751]\n",
      "epoch:36 step:28481 [D loss: 0.392233, acc.: 81.25%] [G loss: 4.172048]\n",
      "epoch:36 step:28482 [D loss: 0.317330, acc.: 84.38%] [G loss: 3.320720]\n",
      "epoch:36 step:28483 [D loss: 0.398508, acc.: 83.59%] [G loss: 4.110117]\n",
      "epoch:36 step:28484 [D loss: 0.320002, acc.: 85.94%] [G loss: 3.487579]\n",
      "epoch:36 step:28485 [D loss: 0.310906, acc.: 88.28%] [G loss: 3.043839]\n",
      "epoch:36 step:28486 [D loss: 0.289128, acc.: 85.16%] [G loss: 3.351666]\n",
      "epoch:36 step:28487 [D loss: 0.290536, acc.: 86.72%] [G loss: 4.031740]\n",
      "epoch:36 step:28488 [D loss: 0.466466, acc.: 79.69%] [G loss: 3.024341]\n",
      "epoch:36 step:28489 [D loss: 0.290421, acc.: 88.28%] [G loss: 2.733814]\n",
      "epoch:36 step:28490 [D loss: 0.347462, acc.: 82.81%] [G loss: 2.952143]\n",
      "epoch:36 step:28491 [D loss: 0.409885, acc.: 82.81%] [G loss: 2.948495]\n",
      "epoch:36 step:28492 [D loss: 0.262595, acc.: 88.28%] [G loss: 2.766584]\n",
      "epoch:36 step:28493 [D loss: 0.345053, acc.: 87.50%] [G loss: 3.442029]\n",
      "epoch:36 step:28494 [D loss: 0.343951, acc.: 80.47%] [G loss: 3.512816]\n",
      "epoch:36 step:28495 [D loss: 0.352035, acc.: 87.50%] [G loss: 4.294979]\n",
      "epoch:36 step:28496 [D loss: 0.255593, acc.: 89.84%] [G loss: 3.692762]\n",
      "epoch:36 step:28497 [D loss: 0.271521, acc.: 86.72%] [G loss: 3.335248]\n",
      "epoch:36 step:28498 [D loss: 0.334941, acc.: 85.16%] [G loss: 3.915037]\n",
      "epoch:36 step:28499 [D loss: 0.349072, acc.: 82.03%] [G loss: 2.796065]\n",
      "epoch:36 step:28500 [D loss: 0.248276, acc.: 89.06%] [G loss: 3.677814]\n",
      "epoch:36 step:28501 [D loss: 0.369155, acc.: 85.16%] [G loss: 3.147613]\n",
      "epoch:36 step:28502 [D loss: 0.275960, acc.: 89.84%] [G loss: 2.890585]\n",
      "epoch:36 step:28503 [D loss: 0.326977, acc.: 85.94%] [G loss: 3.116714]\n",
      "epoch:36 step:28504 [D loss: 0.331114, acc.: 84.38%] [G loss: 2.593651]\n",
      "epoch:36 step:28505 [D loss: 0.334552, acc.: 83.59%] [G loss: 2.264102]\n",
      "epoch:36 step:28506 [D loss: 0.235492, acc.: 89.84%] [G loss: 3.016420]\n",
      "epoch:36 step:28507 [D loss: 0.389807, acc.: 81.25%] [G loss: 3.135017]\n",
      "epoch:36 step:28508 [D loss: 0.305663, acc.: 82.03%] [G loss: 2.983413]\n",
      "epoch:36 step:28509 [D loss: 0.322512, acc.: 85.94%] [G loss: 3.539072]\n",
      "epoch:36 step:28510 [D loss: 0.381963, acc.: 85.16%] [G loss: 3.394354]\n",
      "epoch:36 step:28511 [D loss: 0.342061, acc.: 84.38%] [G loss: 2.902298]\n",
      "epoch:36 step:28512 [D loss: 0.337425, acc.: 86.72%] [G loss: 4.415100]\n",
      "epoch:36 step:28513 [D loss: 0.257606, acc.: 89.84%] [G loss: 3.749687]\n",
      "epoch:36 step:28514 [D loss: 0.227407, acc.: 89.06%] [G loss: 3.670005]\n",
      "epoch:36 step:28515 [D loss: 0.316279, acc.: 84.38%] [G loss: 4.114975]\n",
      "epoch:36 step:28516 [D loss: 0.237747, acc.: 89.84%] [G loss: 3.922325]\n",
      "epoch:36 step:28517 [D loss: 0.289993, acc.: 88.28%] [G loss: 3.773112]\n",
      "epoch:36 step:28518 [D loss: 0.275820, acc.: 86.72%] [G loss: 5.553730]\n",
      "epoch:36 step:28519 [D loss: 0.245740, acc.: 89.84%] [G loss: 3.059860]\n",
      "epoch:36 step:28520 [D loss: 0.265984, acc.: 85.94%] [G loss: 4.181611]\n",
      "epoch:36 step:28521 [D loss: 0.336362, acc.: 83.59%] [G loss: 2.854800]\n",
      "epoch:36 step:28522 [D loss: 0.319008, acc.: 87.50%] [G loss: 3.312361]\n",
      "epoch:36 step:28523 [D loss: 0.438183, acc.: 81.25%] [G loss: 4.267828]\n",
      "epoch:36 step:28524 [D loss: 0.316889, acc.: 84.38%] [G loss: 3.364262]\n",
      "epoch:36 step:28525 [D loss: 0.312531, acc.: 84.38%] [G loss: 4.505594]\n",
      "epoch:36 step:28526 [D loss: 0.335000, acc.: 88.28%] [G loss: 4.090018]\n",
      "epoch:36 step:28527 [D loss: 0.245001, acc.: 88.28%] [G loss: 3.874839]\n",
      "epoch:36 step:28528 [D loss: 0.407326, acc.: 82.03%] [G loss: 3.860695]\n",
      "epoch:36 step:28529 [D loss: 0.297203, acc.: 89.06%] [G loss: 3.643119]\n",
      "epoch:36 step:28530 [D loss: 0.272950, acc.: 86.72%] [G loss: 4.089854]\n",
      "epoch:36 step:28531 [D loss: 0.252125, acc.: 90.62%] [G loss: 2.876273]\n",
      "epoch:36 step:28532 [D loss: 0.247434, acc.: 91.41%] [G loss: 3.698786]\n",
      "epoch:36 step:28533 [D loss: 0.221150, acc.: 89.84%] [G loss: 3.853233]\n",
      "epoch:36 step:28534 [D loss: 0.376022, acc.: 83.59%] [G loss: 2.854415]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:36 step:28535 [D loss: 0.304856, acc.: 89.06%] [G loss: 2.838081]\n",
      "epoch:36 step:28536 [D loss: 0.368566, acc.: 81.25%] [G loss: 4.422308]\n",
      "epoch:36 step:28537 [D loss: 0.296327, acc.: 88.28%] [G loss: 4.997860]\n",
      "epoch:36 step:28538 [D loss: 0.292509, acc.: 84.38%] [G loss: 3.518417]\n",
      "epoch:36 step:28539 [D loss: 0.276920, acc.: 88.28%] [G loss: 3.528260]\n",
      "epoch:36 step:28540 [D loss: 0.274704, acc.: 88.28%] [G loss: 3.938461]\n",
      "epoch:36 step:28541 [D loss: 0.316011, acc.: 85.16%] [G loss: 3.066212]\n",
      "epoch:36 step:28542 [D loss: 0.273729, acc.: 89.06%] [G loss: 3.041948]\n",
      "epoch:36 step:28543 [D loss: 0.340006, acc.: 81.25%] [G loss: 2.815523]\n",
      "epoch:36 step:28544 [D loss: 0.280561, acc.: 87.50%] [G loss: 2.923082]\n",
      "epoch:36 step:28545 [D loss: 0.321903, acc.: 85.94%] [G loss: 3.651493]\n",
      "epoch:36 step:28546 [D loss: 0.246646, acc.: 87.50%] [G loss: 3.520794]\n",
      "epoch:36 step:28547 [D loss: 0.252573, acc.: 89.06%] [G loss: 4.681306]\n",
      "epoch:36 step:28548 [D loss: 0.310100, acc.: 87.50%] [G loss: 3.976000]\n",
      "epoch:36 step:28549 [D loss: 0.258371, acc.: 87.50%] [G loss: 2.987147]\n",
      "epoch:36 step:28550 [D loss: 0.369992, acc.: 80.47%] [G loss: 5.646192]\n",
      "epoch:36 step:28551 [D loss: 0.339186, acc.: 82.81%] [G loss: 3.296636]\n",
      "epoch:36 step:28552 [D loss: 0.352910, acc.: 82.81%] [G loss: 6.669649]\n",
      "epoch:36 step:28553 [D loss: 0.289824, acc.: 85.94%] [G loss: 4.143836]\n",
      "epoch:36 step:28554 [D loss: 0.299103, acc.: 85.94%] [G loss: 8.669429]\n",
      "epoch:36 step:28555 [D loss: 0.720453, acc.: 70.31%] [G loss: 5.803735]\n",
      "epoch:36 step:28556 [D loss: 0.449180, acc.: 86.72%] [G loss: 6.776554]\n",
      "epoch:36 step:28557 [D loss: 0.619692, acc.: 75.00%] [G loss: 4.141230]\n",
      "epoch:36 step:28558 [D loss: 0.201926, acc.: 92.19%] [G loss: 7.698096]\n",
      "epoch:36 step:28559 [D loss: 0.236064, acc.: 86.72%] [G loss: 4.445747]\n",
      "epoch:36 step:28560 [D loss: 0.318857, acc.: 86.72%] [G loss: 4.878080]\n",
      "epoch:36 step:28561 [D loss: 0.144797, acc.: 94.53%] [G loss: 5.711032]\n",
      "epoch:36 step:28562 [D loss: 0.254879, acc.: 89.84%] [G loss: 6.112811]\n",
      "epoch:36 step:28563 [D loss: 0.332238, acc.: 87.50%] [G loss: 5.757607]\n",
      "epoch:36 step:28564 [D loss: 0.346085, acc.: 83.59%] [G loss: 4.073525]\n",
      "epoch:36 step:28565 [D loss: 0.248572, acc.: 88.28%] [G loss: 3.585365]\n",
      "epoch:36 step:28566 [D loss: 0.247970, acc.: 89.84%] [G loss: 3.584871]\n",
      "epoch:36 step:28567 [D loss: 0.280645, acc.: 88.28%] [G loss: 4.356914]\n",
      "epoch:36 step:28568 [D loss: 0.314285, acc.: 86.72%] [G loss: 3.147964]\n",
      "epoch:36 step:28569 [D loss: 0.244781, acc.: 90.62%] [G loss: 3.133240]\n",
      "epoch:36 step:28570 [D loss: 0.332313, acc.: 83.59%] [G loss: 3.736369]\n",
      "epoch:36 step:28571 [D loss: 0.312362, acc.: 86.72%] [G loss: 3.297057]\n",
      "epoch:36 step:28572 [D loss: 0.466871, acc.: 75.78%] [G loss: 3.069584]\n",
      "epoch:36 step:28573 [D loss: 0.372718, acc.: 83.59%] [G loss: 2.834881]\n",
      "epoch:36 step:28574 [D loss: 0.373753, acc.: 83.59%] [G loss: 3.276372]\n",
      "epoch:36 step:28575 [D loss: 0.324905, acc.: 85.94%] [G loss: 2.469561]\n",
      "epoch:36 step:28576 [D loss: 0.349876, acc.: 83.59%] [G loss: 2.747308]\n",
      "epoch:36 step:28577 [D loss: 0.259028, acc.: 89.06%] [G loss: 4.195664]\n",
      "epoch:36 step:28578 [D loss: 0.237102, acc.: 86.72%] [G loss: 3.637692]\n",
      "epoch:36 step:28579 [D loss: 0.247330, acc.: 90.62%] [G loss: 3.593421]\n",
      "epoch:36 step:28580 [D loss: 0.360811, acc.: 85.16%] [G loss: 2.856216]\n",
      "epoch:36 step:28581 [D loss: 0.472540, acc.: 76.56%] [G loss: 2.041814]\n",
      "epoch:36 step:28582 [D loss: 0.346817, acc.: 84.38%] [G loss: 2.645320]\n",
      "epoch:36 step:28583 [D loss: 0.364008, acc.: 80.47%] [G loss: 2.723699]\n",
      "epoch:36 step:28584 [D loss: 0.348958, acc.: 88.28%] [G loss: 2.854687]\n",
      "epoch:36 step:28585 [D loss: 0.348511, acc.: 84.38%] [G loss: 3.664788]\n",
      "epoch:36 step:28586 [D loss: 0.278946, acc.: 88.28%] [G loss: 3.503874]\n",
      "epoch:36 step:28587 [D loss: 0.341542, acc.: 85.94%] [G loss: 3.664870]\n",
      "epoch:36 step:28588 [D loss: 0.382590, acc.: 76.56%] [G loss: 2.838882]\n",
      "epoch:36 step:28589 [D loss: 0.297527, acc.: 85.94%] [G loss: 2.920992]\n",
      "epoch:36 step:28590 [D loss: 0.290676, acc.: 87.50%] [G loss: 3.288568]\n",
      "epoch:36 step:28591 [D loss: 0.312652, acc.: 89.06%] [G loss: 2.823543]\n",
      "epoch:36 step:28592 [D loss: 0.394786, acc.: 82.03%] [G loss: 4.437171]\n",
      "epoch:36 step:28593 [D loss: 0.308075, acc.: 87.50%] [G loss: 3.534579]\n",
      "epoch:36 step:28594 [D loss: 0.226192, acc.: 92.19%] [G loss: 3.031028]\n",
      "epoch:36 step:28595 [D loss: 0.312902, acc.: 86.72%] [G loss: 3.825220]\n",
      "epoch:36 step:28596 [D loss: 0.225291, acc.: 92.19%] [G loss: 4.888073]\n",
      "epoch:36 step:28597 [D loss: 0.217708, acc.: 91.41%] [G loss: 3.950537]\n",
      "epoch:36 step:28598 [D loss: 0.230070, acc.: 90.62%] [G loss: 5.405116]\n",
      "epoch:36 step:28599 [D loss: 0.245448, acc.: 88.28%] [G loss: 4.858171]\n",
      "epoch:36 step:28600 [D loss: 0.229568, acc.: 88.28%] [G loss: 6.664125]\n",
      "##############\n",
      "[0.8643918  0.85238619 0.80525062 0.78316991 0.78853231 0.83856349\n",
      " 0.8757081  0.84495666 0.85592875 0.82799225]\n",
      "##########\n",
      "epoch:36 step:28601 [D loss: 0.208421, acc.: 92.97%] [G loss: 4.416989]\n",
      "epoch:36 step:28602 [D loss: 0.306092, acc.: 83.59%] [G loss: 3.634848]\n",
      "epoch:36 step:28603 [D loss: 0.211935, acc.: 91.41%] [G loss: 3.022809]\n",
      "epoch:36 step:28604 [D loss: 0.261773, acc.: 88.28%] [G loss: 3.676142]\n",
      "epoch:36 step:28605 [D loss: 0.314796, acc.: 82.81%] [G loss: 3.145327]\n",
      "epoch:36 step:28606 [D loss: 0.269726, acc.: 91.41%] [G loss: 4.162505]\n",
      "epoch:36 step:28607 [D loss: 0.308792, acc.: 86.72%] [G loss: 4.233233]\n",
      "epoch:36 step:28608 [D loss: 0.339243, acc.: 82.81%] [G loss: 5.029428]\n",
      "epoch:36 step:28609 [D loss: 0.283660, acc.: 87.50%] [G loss: 3.916494]\n",
      "epoch:36 step:28610 [D loss: 0.275196, acc.: 84.38%] [G loss: 4.419291]\n",
      "epoch:36 step:28611 [D loss: 0.431986, acc.: 79.69%] [G loss: 3.368242]\n",
      "epoch:36 step:28612 [D loss: 0.214763, acc.: 91.41%] [G loss: 4.605266]\n",
      "epoch:36 step:28613 [D loss: 0.345154, acc.: 85.94%] [G loss: 3.417969]\n",
      "epoch:36 step:28614 [D loss: 0.270507, acc.: 88.28%] [G loss: 3.698092]\n",
      "epoch:36 step:28615 [D loss: 0.256495, acc.: 91.41%] [G loss: 3.836990]\n",
      "epoch:36 step:28616 [D loss: 0.288724, acc.: 85.94%] [G loss: 3.126925]\n",
      "epoch:36 step:28617 [D loss: 0.412147, acc.: 80.47%] [G loss: 3.165257]\n",
      "epoch:36 step:28618 [D loss: 0.255257, acc.: 86.72%] [G loss: 3.764809]\n",
      "epoch:36 step:28619 [D loss: 0.276293, acc.: 85.16%] [G loss: 2.973143]\n",
      "epoch:36 step:28620 [D loss: 0.246286, acc.: 91.41%] [G loss: 3.321691]\n",
      "epoch:36 step:28621 [D loss: 0.361948, acc.: 82.81%] [G loss: 2.927011]\n",
      "epoch:36 step:28622 [D loss: 0.328268, acc.: 86.72%] [G loss: 3.221816]\n",
      "epoch:36 step:28623 [D loss: 0.281008, acc.: 85.94%] [G loss: 3.545935]\n",
      "epoch:36 step:28624 [D loss: 0.315405, acc.: 84.38%] [G loss: 4.231723]\n",
      "epoch:36 step:28625 [D loss: 0.381159, acc.: 81.25%] [G loss: 3.001747]\n",
      "epoch:36 step:28626 [D loss: 0.252132, acc.: 87.50%] [G loss: 3.679278]\n",
      "epoch:36 step:28627 [D loss: 0.350960, acc.: 86.72%] [G loss: 4.690730]\n",
      "epoch:36 step:28628 [D loss: 0.244434, acc.: 90.62%] [G loss: 3.248222]\n",
      "epoch:36 step:28629 [D loss: 0.323583, acc.: 87.50%] [G loss: 2.766815]\n",
      "epoch:36 step:28630 [D loss: 0.325477, acc.: 86.72%] [G loss: 2.896024]\n",
      "epoch:36 step:28631 [D loss: 0.306698, acc.: 88.28%] [G loss: 2.825065]\n",
      "epoch:36 step:28632 [D loss: 0.297343, acc.: 88.28%] [G loss: 4.110832]\n",
      "epoch:36 step:28633 [D loss: 0.448038, acc.: 78.12%] [G loss: 3.641116]\n",
      "epoch:36 step:28634 [D loss: 0.289723, acc.: 86.72%] [G loss: 3.627359]\n",
      "epoch:36 step:28635 [D loss: 0.368257, acc.: 82.81%] [G loss: 3.801986]\n",
      "epoch:36 step:28636 [D loss: 0.337707, acc.: 81.25%] [G loss: 3.471103]\n",
      "epoch:36 step:28637 [D loss: 0.268153, acc.: 88.28%] [G loss: 6.126218]\n",
      "epoch:36 step:28638 [D loss: 0.354156, acc.: 82.81%] [G loss: 4.780208]\n",
      "epoch:36 step:28639 [D loss: 0.185790, acc.: 92.19%] [G loss: 4.861768]\n",
      "epoch:36 step:28640 [D loss: 0.264068, acc.: 88.28%] [G loss: 4.978663]\n",
      "epoch:36 step:28641 [D loss: 0.250155, acc.: 88.28%] [G loss: 3.968992]\n",
      "epoch:36 step:28642 [D loss: 0.360462, acc.: 84.38%] [G loss: 4.474789]\n",
      "epoch:36 step:28643 [D loss: 0.290831, acc.: 87.50%] [G loss: 5.073423]\n",
      "epoch:36 step:28644 [D loss: 0.399377, acc.: 80.47%] [G loss: 3.083237]\n",
      "epoch:36 step:28645 [D loss: 0.296562, acc.: 87.50%] [G loss: 4.170786]\n",
      "epoch:36 step:28646 [D loss: 0.324935, acc.: 83.59%] [G loss: 3.284135]\n",
      "epoch:36 step:28647 [D loss: 0.382429, acc.: 84.38%] [G loss: 4.208518]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:36 step:28648 [D loss: 0.303031, acc.: 84.38%] [G loss: 2.786123]\n",
      "epoch:36 step:28649 [D loss: 0.283189, acc.: 88.28%] [G loss: 2.836576]\n",
      "epoch:36 step:28650 [D loss: 0.273903, acc.: 89.06%] [G loss: 3.234815]\n",
      "epoch:36 step:28651 [D loss: 0.408944, acc.: 83.59%] [G loss: 3.554935]\n",
      "epoch:36 step:28652 [D loss: 0.252699, acc.: 89.06%] [G loss: 3.917212]\n",
      "epoch:36 step:28653 [D loss: 0.417550, acc.: 79.69%] [G loss: 2.642885]\n",
      "epoch:36 step:28654 [D loss: 0.283002, acc.: 88.28%] [G loss: 3.796089]\n",
      "epoch:36 step:28655 [D loss: 0.213057, acc.: 92.19%] [G loss: 3.404683]\n",
      "epoch:36 step:28656 [D loss: 0.340173, acc.: 85.16%] [G loss: 3.319463]\n",
      "epoch:36 step:28657 [D loss: 0.287710, acc.: 85.94%] [G loss: 3.501746]\n",
      "epoch:36 step:28658 [D loss: 0.266290, acc.: 89.06%] [G loss: 3.963137]\n",
      "epoch:36 step:28659 [D loss: 0.272782, acc.: 88.28%] [G loss: 3.399291]\n",
      "epoch:36 step:28660 [D loss: 0.201781, acc.: 93.75%] [G loss: 5.123795]\n",
      "epoch:36 step:28661 [D loss: 0.244978, acc.: 89.06%] [G loss: 3.640988]\n",
      "epoch:36 step:28662 [D loss: 0.331501, acc.: 84.38%] [G loss: 3.139004]\n",
      "epoch:36 step:28663 [D loss: 0.417817, acc.: 78.12%] [G loss: 4.520564]\n",
      "epoch:36 step:28664 [D loss: 0.193629, acc.: 92.97%] [G loss: 4.080660]\n",
      "epoch:36 step:28665 [D loss: 0.302375, acc.: 88.28%] [G loss: 4.143117]\n",
      "epoch:36 step:28666 [D loss: 0.295491, acc.: 86.72%] [G loss: 3.316032]\n",
      "epoch:36 step:28667 [D loss: 0.347030, acc.: 84.38%] [G loss: 3.051896]\n",
      "epoch:36 step:28668 [D loss: 0.413783, acc.: 84.38%] [G loss: 2.330497]\n",
      "epoch:36 step:28669 [D loss: 0.345544, acc.: 85.16%] [G loss: 3.449712]\n",
      "epoch:36 step:28670 [D loss: 0.395572, acc.: 83.59%] [G loss: 4.743410]\n",
      "epoch:36 step:28671 [D loss: 0.604360, acc.: 77.34%] [G loss: 8.238005]\n",
      "epoch:36 step:28672 [D loss: 1.287016, acc.: 67.19%] [G loss: 9.524445]\n",
      "epoch:36 step:28673 [D loss: 2.779098, acc.: 57.03%] [G loss: 3.541342]\n",
      "epoch:36 step:28674 [D loss: 1.249502, acc.: 67.19%] [G loss: 4.281007]\n",
      "epoch:36 step:28675 [D loss: 0.617655, acc.: 77.34%] [G loss: 3.939046]\n",
      "epoch:36 step:28676 [D loss: 0.431679, acc.: 82.03%] [G loss: 4.333663]\n",
      "epoch:36 step:28677 [D loss: 0.333524, acc.: 82.81%] [G loss: 3.493663]\n",
      "epoch:36 step:28678 [D loss: 0.500213, acc.: 80.47%] [G loss: 2.656171]\n",
      "epoch:36 step:28679 [D loss: 0.167830, acc.: 94.53%] [G loss: 3.445543]\n",
      "epoch:36 step:28680 [D loss: 0.360084, acc.: 86.72%] [G loss: 2.559008]\n",
      "epoch:36 step:28681 [D loss: 0.345683, acc.: 83.59%] [G loss: 2.785821]\n",
      "epoch:36 step:28682 [D loss: 0.430028, acc.: 80.47%] [G loss: 3.352845]\n",
      "epoch:36 step:28683 [D loss: 0.312164, acc.: 87.50%] [G loss: 3.686651]\n",
      "epoch:36 step:28684 [D loss: 0.329176, acc.: 84.38%] [G loss: 2.800534]\n",
      "epoch:36 step:28685 [D loss: 0.234491, acc.: 93.75%] [G loss: 3.390787]\n",
      "epoch:36 step:28686 [D loss: 0.242240, acc.: 86.72%] [G loss: 3.520415]\n",
      "epoch:36 step:28687 [D loss: 0.384678, acc.: 78.91%] [G loss: 3.332213]\n",
      "epoch:36 step:28688 [D loss: 0.312138, acc.: 84.38%] [G loss: 2.648361]\n",
      "epoch:36 step:28689 [D loss: 0.221034, acc.: 92.19%] [G loss: 3.877718]\n",
      "epoch:36 step:28690 [D loss: 0.259535, acc.: 90.62%] [G loss: 3.545407]\n",
      "epoch:36 step:28691 [D loss: 0.292232, acc.: 86.72%] [G loss: 2.855078]\n",
      "epoch:36 step:28692 [D loss: 0.259461, acc.: 85.16%] [G loss: 3.196457]\n",
      "epoch:36 step:28693 [D loss: 0.241981, acc.: 90.62%] [G loss: 2.908186]\n",
      "epoch:36 step:28694 [D loss: 0.334587, acc.: 84.38%] [G loss: 4.106452]\n",
      "epoch:36 step:28695 [D loss: 0.339801, acc.: 85.16%] [G loss: 3.204410]\n",
      "epoch:36 step:28696 [D loss: 0.249918, acc.: 88.28%] [G loss: 3.165474]\n",
      "epoch:36 step:28697 [D loss: 0.323408, acc.: 88.28%] [G loss: 3.191544]\n",
      "epoch:36 step:28698 [D loss: 0.314864, acc.: 86.72%] [G loss: 2.911936]\n",
      "epoch:36 step:28699 [D loss: 0.296145, acc.: 84.38%] [G loss: 4.066023]\n",
      "epoch:36 step:28700 [D loss: 0.312460, acc.: 87.50%] [G loss: 3.702701]\n",
      "epoch:36 step:28701 [D loss: 0.261597, acc.: 85.16%] [G loss: 3.620290]\n",
      "epoch:36 step:28702 [D loss: 0.244317, acc.: 89.84%] [G loss: 2.570477]\n",
      "epoch:36 step:28703 [D loss: 0.332237, acc.: 82.81%] [G loss: 2.668126]\n",
      "epoch:36 step:28704 [D loss: 0.244113, acc.: 91.41%] [G loss: 3.471989]\n",
      "epoch:36 step:28705 [D loss: 0.304732, acc.: 86.72%] [G loss: 3.328757]\n",
      "epoch:36 step:28706 [D loss: 0.403270, acc.: 78.12%] [G loss: 3.446718]\n",
      "epoch:36 step:28707 [D loss: 0.403454, acc.: 84.38%] [G loss: 3.027901]\n",
      "epoch:36 step:28708 [D loss: 0.357420, acc.: 85.16%] [G loss: 3.275248]\n",
      "epoch:36 step:28709 [D loss: 0.318971, acc.: 85.94%] [G loss: 3.446640]\n",
      "epoch:36 step:28710 [D loss: 0.271829, acc.: 85.94%] [G loss: 2.435453]\n",
      "epoch:36 step:28711 [D loss: 0.335061, acc.: 87.50%] [G loss: 2.820028]\n",
      "epoch:36 step:28712 [D loss: 0.331310, acc.: 85.94%] [G loss: 2.353463]\n",
      "epoch:36 step:28713 [D loss: 0.300716, acc.: 88.28%] [G loss: 2.564541]\n",
      "epoch:36 step:28714 [D loss: 0.252956, acc.: 87.50%] [G loss: 2.615486]\n",
      "epoch:36 step:28715 [D loss: 0.254530, acc.: 86.72%] [G loss: 4.049959]\n",
      "epoch:36 step:28716 [D loss: 0.292781, acc.: 87.50%] [G loss: 5.394875]\n",
      "epoch:36 step:28717 [D loss: 0.291910, acc.: 85.94%] [G loss: 2.839231]\n",
      "epoch:36 step:28718 [D loss: 0.251492, acc.: 89.06%] [G loss: 3.955938]\n",
      "epoch:36 step:28719 [D loss: 0.264253, acc.: 88.28%] [G loss: 3.747834]\n",
      "epoch:36 step:28720 [D loss: 0.310324, acc.: 85.16%] [G loss: 2.773313]\n",
      "epoch:36 step:28721 [D loss: 0.334048, acc.: 82.03%] [G loss: 4.583136]\n",
      "epoch:36 step:28722 [D loss: 0.286191, acc.: 83.59%] [G loss: 3.679806]\n",
      "epoch:36 step:28723 [D loss: 0.337842, acc.: 84.38%] [G loss: 3.368920]\n",
      "epoch:36 step:28724 [D loss: 0.187445, acc.: 92.97%] [G loss: 3.778924]\n",
      "epoch:36 step:28725 [D loss: 0.236997, acc.: 90.62%] [G loss: 4.252450]\n",
      "epoch:36 step:28726 [D loss: 0.331242, acc.: 85.16%] [G loss: 3.573774]\n",
      "epoch:36 step:28727 [D loss: 0.266956, acc.: 85.16%] [G loss: 4.472878]\n",
      "epoch:36 step:28728 [D loss: 0.321148, acc.: 88.28%] [G loss: 3.736332]\n",
      "epoch:36 step:28729 [D loss: 0.330147, acc.: 83.59%] [G loss: 3.499543]\n",
      "epoch:36 step:28730 [D loss: 0.227877, acc.: 91.41%] [G loss: 3.245328]\n",
      "epoch:36 step:28731 [D loss: 0.261940, acc.: 89.84%] [G loss: 3.275771]\n",
      "epoch:36 step:28732 [D loss: 0.214074, acc.: 92.97%] [G loss: 3.227890]\n",
      "epoch:36 step:28733 [D loss: 0.206144, acc.: 89.84%] [G loss: 3.659656]\n",
      "epoch:36 step:28734 [D loss: 0.330890, acc.: 82.81%] [G loss: 3.611661]\n",
      "epoch:36 step:28735 [D loss: 0.290533, acc.: 89.06%] [G loss: 3.207893]\n",
      "epoch:36 step:28736 [D loss: 0.283557, acc.: 88.28%] [G loss: 2.775506]\n",
      "epoch:36 step:28737 [D loss: 0.208465, acc.: 92.97%] [G loss: 2.761699]\n",
      "epoch:36 step:28738 [D loss: 0.241879, acc.: 88.28%] [G loss: 2.820600]\n",
      "epoch:36 step:28739 [D loss: 0.258788, acc.: 90.62%] [G loss: 4.328958]\n",
      "epoch:36 step:28740 [D loss: 0.264040, acc.: 87.50%] [G loss: 2.938791]\n",
      "epoch:36 step:28741 [D loss: 0.317540, acc.: 88.28%] [G loss: 2.844914]\n",
      "epoch:36 step:28742 [D loss: 0.256972, acc.: 89.06%] [G loss: 2.946371]\n",
      "epoch:36 step:28743 [D loss: 0.378559, acc.: 83.59%] [G loss: 2.722104]\n",
      "epoch:36 step:28744 [D loss: 0.421849, acc.: 82.81%] [G loss: 2.246005]\n",
      "epoch:36 step:28745 [D loss: 0.250562, acc.: 92.97%] [G loss: 2.681054]\n",
      "epoch:36 step:28746 [D loss: 0.412764, acc.: 80.47%] [G loss: 2.656297]\n",
      "epoch:36 step:28747 [D loss: 0.297765, acc.: 88.28%] [G loss: 2.616615]\n",
      "epoch:36 step:28748 [D loss: 0.249917, acc.: 91.41%] [G loss: 3.007509]\n",
      "epoch:36 step:28749 [D loss: 0.346179, acc.: 82.81%] [G loss: 3.830481]\n",
      "epoch:36 step:28750 [D loss: 0.382747, acc.: 82.81%] [G loss: 5.659979]\n",
      "epoch:36 step:28751 [D loss: 0.476046, acc.: 82.03%] [G loss: 3.810879]\n",
      "epoch:36 step:28752 [D loss: 0.332546, acc.: 83.59%] [G loss: 4.094984]\n",
      "epoch:36 step:28753 [D loss: 0.300251, acc.: 87.50%] [G loss: 5.307944]\n",
      "epoch:36 step:28754 [D loss: 0.236771, acc.: 89.84%] [G loss: 6.202233]\n",
      "epoch:36 step:28755 [D loss: 0.321758, acc.: 85.94%] [G loss: 6.909025]\n",
      "epoch:36 step:28756 [D loss: 0.335607, acc.: 85.16%] [G loss: 3.140348]\n",
      "epoch:36 step:28757 [D loss: 0.368051, acc.: 82.81%] [G loss: 2.919520]\n",
      "epoch:36 step:28758 [D loss: 0.308266, acc.: 82.81%] [G loss: 3.998423]\n",
      "epoch:36 step:28759 [D loss: 0.363593, acc.: 82.81%] [G loss: 3.244899]\n",
      "epoch:36 step:28760 [D loss: 0.205426, acc.: 93.75%] [G loss: 2.874365]\n",
      "epoch:36 step:28761 [D loss: 0.263829, acc.: 90.62%] [G loss: 3.006765]\n",
      "epoch:36 step:28762 [D loss: 0.285506, acc.: 87.50%] [G loss: 4.061702]\n",
      "epoch:36 step:28763 [D loss: 0.322613, acc.: 87.50%] [G loss: 3.374265]\n",
      "epoch:36 step:28764 [D loss: 0.273468, acc.: 89.84%] [G loss: 4.832865]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:36 step:28765 [D loss: 0.291759, acc.: 85.16%] [G loss: 3.830399]\n",
      "epoch:36 step:28766 [D loss: 0.341483, acc.: 81.25%] [G loss: 4.425016]\n",
      "epoch:36 step:28767 [D loss: 0.246125, acc.: 89.84%] [G loss: 3.518174]\n",
      "epoch:36 step:28768 [D loss: 0.261870, acc.: 89.84%] [G loss: 3.973508]\n",
      "epoch:36 step:28769 [D loss: 0.252586, acc.: 90.62%] [G loss: 3.496543]\n",
      "epoch:36 step:28770 [D loss: 0.328797, acc.: 85.16%] [G loss: 4.256765]\n",
      "epoch:36 step:28771 [D loss: 0.295905, acc.: 85.94%] [G loss: 3.168699]\n",
      "epoch:36 step:28772 [D loss: 0.213557, acc.: 91.41%] [G loss: 4.692122]\n",
      "epoch:36 step:28773 [D loss: 0.300602, acc.: 85.16%] [G loss: 4.215318]\n",
      "epoch:36 step:28774 [D loss: 0.425270, acc.: 77.34%] [G loss: 3.542439]\n",
      "epoch:36 step:28775 [D loss: 0.380670, acc.: 79.69%] [G loss: 3.964822]\n",
      "epoch:36 step:28776 [D loss: 0.324493, acc.: 85.94%] [G loss: 4.061060]\n",
      "epoch:36 step:28777 [D loss: 0.505140, acc.: 75.00%] [G loss: 3.898667]\n",
      "epoch:36 step:28778 [D loss: 0.524616, acc.: 75.00%] [G loss: 3.346046]\n",
      "epoch:36 step:28779 [D loss: 0.368011, acc.: 86.72%] [G loss: 3.684441]\n",
      "epoch:36 step:28780 [D loss: 0.347442, acc.: 86.72%] [G loss: 4.584313]\n",
      "epoch:36 step:28781 [D loss: 0.276034, acc.: 89.84%] [G loss: 4.144707]\n",
      "epoch:36 step:28782 [D loss: 0.377610, acc.: 84.38%] [G loss: 4.069466]\n",
      "epoch:36 step:28783 [D loss: 0.288318, acc.: 89.84%] [G loss: 3.377918]\n",
      "epoch:36 step:28784 [D loss: 0.303001, acc.: 86.72%] [G loss: 2.851696]\n",
      "epoch:36 step:28785 [D loss: 0.417539, acc.: 79.69%] [G loss: 3.809880]\n",
      "epoch:36 step:28786 [D loss: 0.253488, acc.: 89.06%] [G loss: 3.030112]\n",
      "epoch:36 step:28787 [D loss: 0.238947, acc.: 89.84%] [G loss: 3.504764]\n",
      "epoch:36 step:28788 [D loss: 0.306912, acc.: 90.62%] [G loss: 3.039997]\n",
      "epoch:36 step:28789 [D loss: 0.346561, acc.: 89.06%] [G loss: 3.195032]\n",
      "epoch:36 step:28790 [D loss: 0.258768, acc.: 89.06%] [G loss: 3.327284]\n",
      "epoch:36 step:28791 [D loss: 0.245897, acc.: 89.84%] [G loss: 4.060349]\n",
      "epoch:36 step:28792 [D loss: 0.371120, acc.: 85.94%] [G loss: 2.579504]\n",
      "epoch:36 step:28793 [D loss: 0.260902, acc.: 86.72%] [G loss: 3.204703]\n",
      "epoch:36 step:28794 [D loss: 0.318667, acc.: 84.38%] [G loss: 2.951363]\n",
      "epoch:36 step:28795 [D loss: 0.302588, acc.: 85.94%] [G loss: 3.401131]\n",
      "epoch:36 step:28796 [D loss: 0.244723, acc.: 90.62%] [G loss: 2.589669]\n",
      "epoch:36 step:28797 [D loss: 0.275009, acc.: 89.06%] [G loss: 3.172995]\n",
      "epoch:36 step:28798 [D loss: 0.365129, acc.: 82.03%] [G loss: 3.795726]\n",
      "epoch:36 step:28799 [D loss: 0.306526, acc.: 85.94%] [G loss: 3.071804]\n",
      "epoch:36 step:28800 [D loss: 0.320662, acc.: 85.94%] [G loss: 2.667484]\n",
      "##############\n",
      "[0.86895661 0.84005583 0.80568043 0.80661324 0.77821836 0.80995215\n",
      " 0.91003702 0.85765112 0.79731152 0.80813014]\n",
      "##########\n",
      "epoch:36 step:28801 [D loss: 0.309931, acc.: 88.28%] [G loss: 3.760191]\n",
      "epoch:36 step:28802 [D loss: 0.366579, acc.: 85.16%] [G loss: 3.543958]\n",
      "epoch:36 step:28803 [D loss: 0.191380, acc.: 89.84%] [G loss: 4.680605]\n",
      "epoch:36 step:28804 [D loss: 0.259384, acc.: 89.84%] [G loss: 4.255273]\n",
      "epoch:36 step:28805 [D loss: 0.296450, acc.: 86.72%] [G loss: 5.284066]\n",
      "epoch:36 step:28806 [D loss: 0.255229, acc.: 89.06%] [G loss: 4.770909]\n",
      "epoch:36 step:28807 [D loss: 0.277705, acc.: 89.84%] [G loss: 5.094247]\n",
      "epoch:36 step:28808 [D loss: 0.340124, acc.: 83.59%] [G loss: 4.612082]\n",
      "epoch:36 step:28809 [D loss: 0.225419, acc.: 92.19%] [G loss: 6.064198]\n",
      "epoch:36 step:28810 [D loss: 0.276186, acc.: 86.72%] [G loss: 5.138153]\n",
      "epoch:36 step:28811 [D loss: 0.258335, acc.: 88.28%] [G loss: 5.157786]\n",
      "epoch:36 step:28812 [D loss: 0.236858, acc.: 91.41%] [G loss: 3.877862]\n",
      "epoch:36 step:28813 [D loss: 0.315011, acc.: 82.03%] [G loss: 4.385512]\n",
      "epoch:36 step:28814 [D loss: 0.285907, acc.: 89.06%] [G loss: 5.649849]\n",
      "epoch:36 step:28815 [D loss: 0.348124, acc.: 85.94%] [G loss: 3.506444]\n",
      "epoch:36 step:28816 [D loss: 0.365495, acc.: 82.81%] [G loss: 4.390739]\n",
      "epoch:36 step:28817 [D loss: 0.307434, acc.: 83.59%] [G loss: 2.649800]\n",
      "epoch:36 step:28818 [D loss: 0.189701, acc.: 95.31%] [G loss: 3.173149]\n",
      "epoch:36 step:28819 [D loss: 0.342519, acc.: 83.59%] [G loss: 2.709767]\n",
      "epoch:36 step:28820 [D loss: 0.294289, acc.: 83.59%] [G loss: 3.197268]\n",
      "epoch:36 step:28821 [D loss: 0.250500, acc.: 91.41%] [G loss: 2.645985]\n",
      "epoch:36 step:28822 [D loss: 0.358712, acc.: 85.94%] [G loss: 3.462912]\n",
      "epoch:36 step:28823 [D loss: 0.401705, acc.: 83.59%] [G loss: 2.721135]\n",
      "epoch:36 step:28824 [D loss: 0.267327, acc.: 87.50%] [G loss: 3.514110]\n",
      "epoch:36 step:28825 [D loss: 0.265983, acc.: 89.84%] [G loss: 3.091082]\n",
      "epoch:36 step:28826 [D loss: 0.357714, acc.: 84.38%] [G loss: 4.305513]\n",
      "epoch:36 step:28827 [D loss: 0.234203, acc.: 92.19%] [G loss: 3.450699]\n",
      "epoch:36 step:28828 [D loss: 0.363672, acc.: 83.59%] [G loss: 3.553205]\n",
      "epoch:36 step:28829 [D loss: 0.303371, acc.: 84.38%] [G loss: 3.347481]\n",
      "epoch:36 step:28830 [D loss: 0.339131, acc.: 85.16%] [G loss: 2.912150]\n",
      "epoch:36 step:28831 [D loss: 0.354995, acc.: 82.03%] [G loss: 3.212982]\n",
      "epoch:36 step:28832 [D loss: 0.312866, acc.: 85.16%] [G loss: 3.086687]\n",
      "epoch:36 step:28833 [D loss: 0.319919, acc.: 87.50%] [G loss: 2.618271]\n",
      "epoch:36 step:28834 [D loss: 0.321179, acc.: 85.16%] [G loss: 2.949600]\n",
      "epoch:36 step:28835 [D loss: 0.254274, acc.: 89.06%] [G loss: 2.691835]\n",
      "epoch:36 step:28836 [D loss: 0.268991, acc.: 88.28%] [G loss: 3.448800]\n",
      "epoch:36 step:28837 [D loss: 0.332974, acc.: 83.59%] [G loss: 3.079396]\n",
      "epoch:36 step:28838 [D loss: 0.214451, acc.: 91.41%] [G loss: 4.476412]\n",
      "epoch:36 step:28839 [D loss: 0.347437, acc.: 85.94%] [G loss: 3.526555]\n",
      "epoch:36 step:28840 [D loss: 0.229855, acc.: 89.84%] [G loss: 4.864943]\n",
      "epoch:36 step:28841 [D loss: 0.248403, acc.: 91.41%] [G loss: 4.829095]\n",
      "epoch:36 step:28842 [D loss: 0.329950, acc.: 82.81%] [G loss: 3.943141]\n",
      "epoch:36 step:28843 [D loss: 0.267199, acc.: 88.28%] [G loss: 3.475297]\n",
      "epoch:36 step:28844 [D loss: 0.379864, acc.: 81.25%] [G loss: 3.266117]\n",
      "epoch:36 step:28845 [D loss: 0.260694, acc.: 87.50%] [G loss: 5.144306]\n",
      "epoch:36 step:28846 [D loss: 0.263101, acc.: 89.84%] [G loss: 3.786632]\n",
      "epoch:36 step:28847 [D loss: 0.238210, acc.: 89.06%] [G loss: 5.114706]\n",
      "epoch:36 step:28848 [D loss: 0.370755, acc.: 78.12%] [G loss: 4.232447]\n",
      "epoch:36 step:28849 [D loss: 0.243310, acc.: 89.06%] [G loss: 4.524014]\n",
      "epoch:36 step:28850 [D loss: 0.360940, acc.: 80.47%] [G loss: 2.902756]\n",
      "epoch:36 step:28851 [D loss: 0.329256, acc.: 84.38%] [G loss: 3.315241]\n",
      "epoch:36 step:28852 [D loss: 0.246036, acc.: 90.62%] [G loss: 3.962419]\n",
      "epoch:36 step:28853 [D loss: 0.334556, acc.: 83.59%] [G loss: 4.133518]\n",
      "epoch:36 step:28854 [D loss: 0.257461, acc.: 89.06%] [G loss: 3.372850]\n",
      "epoch:36 step:28855 [D loss: 0.391852, acc.: 84.38%] [G loss: 3.380478]\n",
      "epoch:36 step:28856 [D loss: 0.300759, acc.: 87.50%] [G loss: 3.696205]\n",
      "epoch:36 step:28857 [D loss: 0.382459, acc.: 78.12%] [G loss: 4.070016]\n",
      "epoch:36 step:28858 [D loss: 0.253028, acc.: 88.28%] [G loss: 4.165689]\n",
      "epoch:36 step:28859 [D loss: 0.252239, acc.: 89.84%] [G loss: 3.438898]\n",
      "epoch:36 step:28860 [D loss: 0.253105, acc.: 89.84%] [G loss: 4.958272]\n",
      "epoch:36 step:28861 [D loss: 0.238822, acc.: 89.84%] [G loss: 3.175432]\n",
      "epoch:36 step:28862 [D loss: 0.258353, acc.: 89.84%] [G loss: 4.576632]\n",
      "epoch:36 step:28863 [D loss: 0.225975, acc.: 90.62%] [G loss: 3.622204]\n",
      "epoch:36 step:28864 [D loss: 0.379513, acc.: 83.59%] [G loss: 4.612992]\n",
      "epoch:36 step:28865 [D loss: 0.226323, acc.: 89.84%] [G loss: 5.255182]\n",
      "epoch:36 step:28866 [D loss: 0.310159, acc.: 85.94%] [G loss: 2.932378]\n",
      "epoch:36 step:28867 [D loss: 0.264156, acc.: 88.28%] [G loss: 4.019220]\n",
      "epoch:36 step:28868 [D loss: 0.314234, acc.: 84.38%] [G loss: 4.240659]\n",
      "epoch:36 step:28869 [D loss: 0.272236, acc.: 88.28%] [G loss: 4.955407]\n",
      "epoch:36 step:28870 [D loss: 0.287515, acc.: 87.50%] [G loss: 3.393044]\n",
      "epoch:36 step:28871 [D loss: 0.303932, acc.: 86.72%] [G loss: 5.108542]\n",
      "epoch:36 step:28872 [D loss: 0.294619, acc.: 89.06%] [G loss: 4.342581]\n",
      "epoch:36 step:28873 [D loss: 0.271610, acc.: 84.38%] [G loss: 4.096125]\n",
      "epoch:36 step:28874 [D loss: 0.358862, acc.: 84.38%] [G loss: 4.910430]\n",
      "epoch:36 step:28875 [D loss: 0.292092, acc.: 86.72%] [G loss: 3.350929]\n",
      "epoch:36 step:28876 [D loss: 0.265965, acc.: 91.41%] [G loss: 4.428769]\n",
      "epoch:36 step:28877 [D loss: 0.256761, acc.: 86.72%] [G loss: 6.899323]\n",
      "epoch:36 step:28878 [D loss: 0.283577, acc.: 88.28%] [G loss: 5.085523]\n",
      "epoch:36 step:28879 [D loss: 0.302217, acc.: 87.50%] [G loss: 4.934837]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:36 step:28880 [D loss: 0.241787, acc.: 88.28%] [G loss: 3.943069]\n",
      "epoch:36 step:28881 [D loss: 0.414417, acc.: 78.91%] [G loss: 3.319297]\n",
      "epoch:36 step:28882 [D loss: 0.267829, acc.: 91.41%] [G loss: 5.576961]\n",
      "epoch:36 step:28883 [D loss: 0.235147, acc.: 91.41%] [G loss: 5.373456]\n",
      "epoch:36 step:28884 [D loss: 0.190334, acc.: 92.97%] [G loss: 6.235857]\n",
      "epoch:36 step:28885 [D loss: 0.244779, acc.: 90.62%] [G loss: 4.487470]\n",
      "epoch:36 step:28886 [D loss: 0.248243, acc.: 89.84%] [G loss: 5.089626]\n",
      "epoch:36 step:28887 [D loss: 0.263612, acc.: 90.62%] [G loss: 4.761353]\n",
      "epoch:36 step:28888 [D loss: 0.257841, acc.: 89.06%] [G loss: 3.145077]\n",
      "epoch:36 step:28889 [D loss: 0.308056, acc.: 89.84%] [G loss: 2.923438]\n",
      "epoch:36 step:28890 [D loss: 0.285745, acc.: 86.72%] [G loss: 3.848634]\n",
      "epoch:36 step:28891 [D loss: 0.303481, acc.: 88.28%] [G loss: 4.532268]\n",
      "epoch:36 step:28892 [D loss: 0.238383, acc.: 88.28%] [G loss: 3.541023]\n",
      "epoch:36 step:28893 [D loss: 0.315103, acc.: 85.16%] [G loss: 7.216735]\n",
      "epoch:36 step:28894 [D loss: 0.549995, acc.: 75.00%] [G loss: 8.277203]\n",
      "epoch:36 step:28895 [D loss: 1.632321, acc.: 71.88%] [G loss: 9.548706]\n",
      "epoch:36 step:28896 [D loss: 3.368533, acc.: 53.91%] [G loss: 3.567152]\n",
      "epoch:36 step:28897 [D loss: 1.339164, acc.: 64.06%] [G loss: 5.874920]\n",
      "epoch:37 step:28898 [D loss: 0.624428, acc.: 74.22%] [G loss: 6.496238]\n",
      "epoch:37 step:28899 [D loss: 0.354540, acc.: 84.38%] [G loss: 3.563837]\n",
      "epoch:37 step:28900 [D loss: 0.510418, acc.: 84.38%] [G loss: 4.898278]\n",
      "epoch:37 step:28901 [D loss: 0.383586, acc.: 82.03%] [G loss: 4.991604]\n",
      "epoch:37 step:28902 [D loss: 0.466180, acc.: 78.91%] [G loss: 3.105241]\n",
      "epoch:37 step:28903 [D loss: 0.313328, acc.: 84.38%] [G loss: 3.375213]\n",
      "epoch:37 step:28904 [D loss: 0.407505, acc.: 81.25%] [G loss: 3.175402]\n",
      "epoch:37 step:28905 [D loss: 0.258378, acc.: 88.28%] [G loss: 3.804049]\n",
      "epoch:37 step:28906 [D loss: 0.315101, acc.: 83.59%] [G loss: 2.714285]\n",
      "epoch:37 step:28907 [D loss: 0.344461, acc.: 84.38%] [G loss: 3.256923]\n",
      "epoch:37 step:28908 [D loss: 0.317955, acc.: 85.94%] [G loss: 3.632943]\n",
      "epoch:37 step:28909 [D loss: 0.396747, acc.: 81.25%] [G loss: 3.295721]\n",
      "epoch:37 step:28910 [D loss: 0.263590, acc.: 89.84%] [G loss: 3.411011]\n",
      "epoch:37 step:28911 [D loss: 0.273175, acc.: 86.72%] [G loss: 4.590314]\n",
      "epoch:37 step:28912 [D loss: 0.391307, acc.: 82.81%] [G loss: 3.130934]\n",
      "epoch:37 step:28913 [D loss: 0.320220, acc.: 88.28%] [G loss: 4.388137]\n",
      "epoch:37 step:28914 [D loss: 0.235652, acc.: 88.28%] [G loss: 3.898711]\n",
      "epoch:37 step:28915 [D loss: 0.335419, acc.: 82.81%] [G loss: 2.975233]\n",
      "epoch:37 step:28916 [D loss: 0.253224, acc.: 87.50%] [G loss: 4.206369]\n",
      "epoch:37 step:28917 [D loss: 0.318488, acc.: 86.72%] [G loss: 3.908665]\n",
      "epoch:37 step:28918 [D loss: 0.336696, acc.: 85.16%] [G loss: 2.802950]\n",
      "epoch:37 step:28919 [D loss: 0.206448, acc.: 91.41%] [G loss: 4.467313]\n",
      "epoch:37 step:28920 [D loss: 0.272535, acc.: 89.06%] [G loss: 3.847292]\n",
      "epoch:37 step:28921 [D loss: 0.250117, acc.: 89.06%] [G loss: 3.510555]\n",
      "epoch:37 step:28922 [D loss: 0.331482, acc.: 84.38%] [G loss: 3.184620]\n",
      "epoch:37 step:28923 [D loss: 0.219737, acc.: 89.06%] [G loss: 3.741223]\n",
      "epoch:37 step:28924 [D loss: 0.411325, acc.: 79.69%] [G loss: 2.816488]\n",
      "epoch:37 step:28925 [D loss: 0.296570, acc.: 86.72%] [G loss: 2.593834]\n",
      "epoch:37 step:28926 [D loss: 0.277533, acc.: 86.72%] [G loss: 3.187044]\n",
      "epoch:37 step:28927 [D loss: 0.303649, acc.: 89.06%] [G loss: 3.484480]\n",
      "epoch:37 step:28928 [D loss: 0.232725, acc.: 91.41%] [G loss: 2.829221]\n",
      "epoch:37 step:28929 [D loss: 0.438901, acc.: 75.00%] [G loss: 3.643878]\n",
      "epoch:37 step:28930 [D loss: 0.307162, acc.: 84.38%] [G loss: 3.607227]\n",
      "epoch:37 step:28931 [D loss: 0.323533, acc.: 82.03%] [G loss: 3.477380]\n",
      "epoch:37 step:28932 [D loss: 0.283637, acc.: 90.62%] [G loss: 2.879115]\n",
      "epoch:37 step:28933 [D loss: 0.438874, acc.: 77.34%] [G loss: 3.605525]\n",
      "epoch:37 step:28934 [D loss: 0.315850, acc.: 83.59%] [G loss: 2.575959]\n",
      "epoch:37 step:28935 [D loss: 0.283131, acc.: 89.84%] [G loss: 2.827675]\n",
      "epoch:37 step:28936 [D loss: 0.301426, acc.: 86.72%] [G loss: 2.461366]\n",
      "epoch:37 step:28937 [D loss: 0.392216, acc.: 83.59%] [G loss: 2.745538]\n",
      "epoch:37 step:28938 [D loss: 0.424130, acc.: 78.91%] [G loss: 2.841414]\n",
      "epoch:37 step:28939 [D loss: 0.307457, acc.: 85.94%] [G loss: 2.619236]\n",
      "epoch:37 step:28940 [D loss: 0.382727, acc.: 79.69%] [G loss: 2.940254]\n",
      "epoch:37 step:28941 [D loss: 0.357784, acc.: 83.59%] [G loss: 2.758712]\n",
      "epoch:37 step:28942 [D loss: 0.269758, acc.: 90.62%] [G loss: 2.964540]\n",
      "epoch:37 step:28943 [D loss: 0.312221, acc.: 85.94%] [G loss: 3.272215]\n",
      "epoch:37 step:28944 [D loss: 0.418214, acc.: 84.38%] [G loss: 2.813605]\n",
      "epoch:37 step:28945 [D loss: 0.194661, acc.: 92.97%] [G loss: 2.911222]\n",
      "epoch:37 step:28946 [D loss: 0.325980, acc.: 85.16%] [G loss: 2.448612]\n",
      "epoch:37 step:28947 [D loss: 0.405415, acc.: 76.56%] [G loss: 2.744727]\n",
      "epoch:37 step:28948 [D loss: 0.269472, acc.: 89.06%] [G loss: 3.320105]\n",
      "epoch:37 step:28949 [D loss: 0.219561, acc.: 88.28%] [G loss: 3.578855]\n",
      "epoch:37 step:28950 [D loss: 0.266190, acc.: 88.28%] [G loss: 2.636713]\n",
      "epoch:37 step:28951 [D loss: 0.327969, acc.: 88.28%] [G loss: 3.246752]\n",
      "epoch:37 step:28952 [D loss: 0.279033, acc.: 89.84%] [G loss: 2.989122]\n",
      "epoch:37 step:28953 [D loss: 0.303589, acc.: 85.16%] [G loss: 2.797849]\n",
      "epoch:37 step:28954 [D loss: 0.250714, acc.: 89.84%] [G loss: 2.545504]\n",
      "epoch:37 step:28955 [D loss: 0.285506, acc.: 89.84%] [G loss: 2.817986]\n",
      "epoch:37 step:28956 [D loss: 0.265682, acc.: 86.72%] [G loss: 3.539477]\n",
      "epoch:37 step:28957 [D loss: 0.261551, acc.: 89.84%] [G loss: 3.339502]\n",
      "epoch:37 step:28958 [D loss: 0.358942, acc.: 84.38%] [G loss: 3.262674]\n",
      "epoch:37 step:28959 [D loss: 0.247186, acc.: 89.84%] [G loss: 3.257385]\n",
      "epoch:37 step:28960 [D loss: 0.365914, acc.: 82.03%] [G loss: 3.628635]\n",
      "epoch:37 step:28961 [D loss: 0.327017, acc.: 84.38%] [G loss: 4.430174]\n",
      "epoch:37 step:28962 [D loss: 0.395125, acc.: 82.03%] [G loss: 7.539716]\n",
      "epoch:37 step:28963 [D loss: 0.356179, acc.: 84.38%] [G loss: 3.967836]\n",
      "epoch:37 step:28964 [D loss: 0.302980, acc.: 84.38%] [G loss: 4.029140]\n",
      "epoch:37 step:28965 [D loss: 0.355534, acc.: 82.81%] [G loss: 4.079008]\n",
      "epoch:37 step:28966 [D loss: 0.274803, acc.: 86.72%] [G loss: 3.860837]\n",
      "epoch:37 step:28967 [D loss: 0.320049, acc.: 85.16%] [G loss: 3.278234]\n",
      "epoch:37 step:28968 [D loss: 0.317777, acc.: 87.50%] [G loss: 2.377968]\n",
      "epoch:37 step:28969 [D loss: 0.292827, acc.: 89.84%] [G loss: 3.809786]\n",
      "epoch:37 step:28970 [D loss: 0.374921, acc.: 84.38%] [G loss: 2.469490]\n",
      "epoch:37 step:28971 [D loss: 0.286111, acc.: 88.28%] [G loss: 2.823514]\n",
      "epoch:37 step:28972 [D loss: 0.437072, acc.: 85.16%] [G loss: 4.099167]\n",
      "epoch:37 step:28973 [D loss: 0.325082, acc.: 85.94%] [G loss: 3.107427]\n",
      "epoch:37 step:28974 [D loss: 0.258821, acc.: 88.28%] [G loss: 2.608587]\n",
      "epoch:37 step:28975 [D loss: 0.434140, acc.: 81.25%] [G loss: 3.658394]\n",
      "epoch:37 step:28976 [D loss: 0.333119, acc.: 85.94%] [G loss: 3.032659]\n",
      "epoch:37 step:28977 [D loss: 0.330987, acc.: 85.94%] [G loss: 3.301992]\n",
      "epoch:37 step:28978 [D loss: 0.442863, acc.: 79.69%] [G loss: 5.691304]\n",
      "epoch:37 step:28979 [D loss: 0.390481, acc.: 83.59%] [G loss: 6.004580]\n",
      "epoch:37 step:28980 [D loss: 0.320850, acc.: 86.72%] [G loss: 3.875550]\n",
      "epoch:37 step:28981 [D loss: 0.275637, acc.: 87.50%] [G loss: 3.073541]\n",
      "epoch:37 step:28982 [D loss: 0.279975, acc.: 86.72%] [G loss: 3.524725]\n",
      "epoch:37 step:28983 [D loss: 0.334314, acc.: 85.94%] [G loss: 3.562138]\n",
      "epoch:37 step:28984 [D loss: 0.367367, acc.: 83.59%] [G loss: 2.549592]\n",
      "epoch:37 step:28985 [D loss: 0.312270, acc.: 89.06%] [G loss: 2.851034]\n",
      "epoch:37 step:28986 [D loss: 0.337500, acc.: 85.94%] [G loss: 3.357511]\n",
      "epoch:37 step:28987 [D loss: 0.347846, acc.: 82.81%] [G loss: 2.273979]\n",
      "epoch:37 step:28988 [D loss: 0.317683, acc.: 85.94%] [G loss: 3.335002]\n",
      "epoch:37 step:28989 [D loss: 0.337180, acc.: 84.38%] [G loss: 2.945385]\n",
      "epoch:37 step:28990 [D loss: 0.274750, acc.: 87.50%] [G loss: 2.490060]\n",
      "epoch:37 step:28991 [D loss: 0.283824, acc.: 87.50%] [G loss: 3.228040]\n",
      "epoch:37 step:28992 [D loss: 0.446796, acc.: 78.12%] [G loss: 2.238424]\n",
      "epoch:37 step:28993 [D loss: 0.273725, acc.: 89.06%] [G loss: 3.518184]\n",
      "epoch:37 step:28994 [D loss: 0.284475, acc.: 88.28%] [G loss: 3.169298]\n",
      "epoch:37 step:28995 [D loss: 0.318364, acc.: 87.50%] [G loss: 2.990936]\n",
      "epoch:37 step:28996 [D loss: 0.353630, acc.: 78.12%] [G loss: 3.354693]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:37 step:28997 [D loss: 0.271419, acc.: 87.50%] [G loss: 2.929092]\n",
      "epoch:37 step:28998 [D loss: 0.342043, acc.: 88.28%] [G loss: 3.513337]\n",
      "epoch:37 step:28999 [D loss: 0.455324, acc.: 78.91%] [G loss: 3.893243]\n",
      "epoch:37 step:29000 [D loss: 0.337142, acc.: 82.81%] [G loss: 4.158706]\n",
      "##############\n",
      "[0.86265412 0.86459741 0.81926374 0.79775233 0.764122   0.8394721\n",
      " 0.8830239  0.82747009 0.81356455 0.83048041]\n",
      "##########\n",
      "epoch:37 step:29001 [D loss: 0.302354, acc.: 89.84%] [G loss: 4.039292]\n",
      "epoch:37 step:29002 [D loss: 0.349090, acc.: 83.59%] [G loss: 4.944360]\n",
      "epoch:37 step:29003 [D loss: 0.413482, acc.: 80.47%] [G loss: 4.655587]\n",
      "epoch:37 step:29004 [D loss: 0.376745, acc.: 83.59%] [G loss: 3.076679]\n",
      "epoch:37 step:29005 [D loss: 0.254872, acc.: 89.84%] [G loss: 3.658111]\n",
      "epoch:37 step:29006 [D loss: 0.364850, acc.: 85.94%] [G loss: 3.480848]\n",
      "epoch:37 step:29007 [D loss: 0.221183, acc.: 92.19%] [G loss: 3.563780]\n",
      "epoch:37 step:29008 [D loss: 0.332942, acc.: 88.28%] [G loss: 2.901121]\n",
      "epoch:37 step:29009 [D loss: 0.263096, acc.: 89.06%] [G loss: 2.669827]\n",
      "epoch:37 step:29010 [D loss: 0.362247, acc.: 82.03%] [G loss: 3.855840]\n",
      "epoch:37 step:29011 [D loss: 0.363567, acc.: 83.59%] [G loss: 3.001430]\n",
      "epoch:37 step:29012 [D loss: 0.317474, acc.: 84.38%] [G loss: 4.043885]\n",
      "epoch:37 step:29013 [D loss: 0.326811, acc.: 83.59%] [G loss: 3.045462]\n",
      "epoch:37 step:29014 [D loss: 0.264081, acc.: 89.06%] [G loss: 5.058766]\n",
      "epoch:37 step:29015 [D loss: 0.266587, acc.: 85.94%] [G loss: 5.367032]\n",
      "epoch:37 step:29016 [D loss: 0.207727, acc.: 92.19%] [G loss: 5.118358]\n",
      "epoch:37 step:29017 [D loss: 0.237566, acc.: 92.19%] [G loss: 6.011453]\n",
      "epoch:37 step:29018 [D loss: 0.228295, acc.: 89.84%] [G loss: 5.890454]\n",
      "epoch:37 step:29019 [D loss: 0.248225, acc.: 87.50%] [G loss: 3.457507]\n",
      "epoch:37 step:29020 [D loss: 0.187616, acc.: 90.62%] [G loss: 4.515460]\n",
      "epoch:37 step:29021 [D loss: 0.339414, acc.: 81.25%] [G loss: 4.335979]\n",
      "epoch:37 step:29022 [D loss: 0.348084, acc.: 85.16%] [G loss: 2.949228]\n",
      "epoch:37 step:29023 [D loss: 0.239203, acc.: 89.84%] [G loss: 3.462135]\n",
      "epoch:37 step:29024 [D loss: 0.272825, acc.: 89.06%] [G loss: 3.227750]\n",
      "epoch:37 step:29025 [D loss: 0.279087, acc.: 87.50%] [G loss: 3.675076]\n",
      "epoch:37 step:29026 [D loss: 0.342679, acc.: 86.72%] [G loss: 4.641292]\n",
      "epoch:37 step:29027 [D loss: 0.314428, acc.: 85.94%] [G loss: 4.451700]\n",
      "epoch:37 step:29028 [D loss: 0.204781, acc.: 90.62%] [G loss: 3.640487]\n",
      "epoch:37 step:29029 [D loss: 0.314368, acc.: 85.94%] [G loss: 3.472760]\n",
      "epoch:37 step:29030 [D loss: 0.370074, acc.: 84.38%] [G loss: 3.664702]\n",
      "epoch:37 step:29031 [D loss: 0.299351, acc.: 85.94%] [G loss: 4.581017]\n",
      "epoch:37 step:29032 [D loss: 0.339865, acc.: 85.16%] [G loss: 3.393178]\n",
      "epoch:37 step:29033 [D loss: 0.305538, acc.: 85.16%] [G loss: 3.809485]\n",
      "epoch:37 step:29034 [D loss: 0.262615, acc.: 90.62%] [G loss: 4.070995]\n",
      "epoch:37 step:29035 [D loss: 0.335217, acc.: 82.81%] [G loss: 4.054463]\n",
      "epoch:37 step:29036 [D loss: 0.317936, acc.: 83.59%] [G loss: 3.765295]\n",
      "epoch:37 step:29037 [D loss: 0.282072, acc.: 88.28%] [G loss: 3.488660]\n",
      "epoch:37 step:29038 [D loss: 0.340937, acc.: 84.38%] [G loss: 4.572421]\n",
      "epoch:37 step:29039 [D loss: 0.282304, acc.: 87.50%] [G loss: 6.209409]\n",
      "epoch:37 step:29040 [D loss: 0.253000, acc.: 89.06%] [G loss: 6.400643]\n",
      "epoch:37 step:29041 [D loss: 0.284941, acc.: 87.50%] [G loss: 4.048876]\n",
      "epoch:37 step:29042 [D loss: 0.267979, acc.: 86.72%] [G loss: 4.149503]\n",
      "epoch:37 step:29043 [D loss: 0.302677, acc.: 88.28%] [G loss: 2.706778]\n",
      "epoch:37 step:29044 [D loss: 0.249434, acc.: 89.06%] [G loss: 3.330239]\n",
      "epoch:37 step:29045 [D loss: 0.266978, acc.: 90.62%] [G loss: 2.660938]\n",
      "epoch:37 step:29046 [D loss: 0.257377, acc.: 89.84%] [G loss: 3.460605]\n",
      "epoch:37 step:29047 [D loss: 0.308192, acc.: 84.38%] [G loss: 3.144443]\n",
      "epoch:37 step:29048 [D loss: 0.280156, acc.: 91.41%] [G loss: 2.926083]\n",
      "epoch:37 step:29049 [D loss: 0.345192, acc.: 84.38%] [G loss: 3.369796]\n",
      "epoch:37 step:29050 [D loss: 0.400316, acc.: 82.81%] [G loss: 3.666935]\n",
      "epoch:37 step:29051 [D loss: 0.371413, acc.: 82.03%] [G loss: 2.837746]\n",
      "epoch:37 step:29052 [D loss: 0.261777, acc.: 85.94%] [G loss: 2.683506]\n",
      "epoch:37 step:29053 [D loss: 0.379281, acc.: 85.94%] [G loss: 2.761337]\n",
      "epoch:37 step:29054 [D loss: 0.299031, acc.: 87.50%] [G loss: 3.030499]\n",
      "epoch:37 step:29055 [D loss: 0.275280, acc.: 89.84%] [G loss: 2.895784]\n",
      "epoch:37 step:29056 [D loss: 0.244136, acc.: 88.28%] [G loss: 5.130580]\n",
      "epoch:37 step:29057 [D loss: 0.370882, acc.: 83.59%] [G loss: 3.383036]\n",
      "epoch:37 step:29058 [D loss: 0.340762, acc.: 83.59%] [G loss: 4.208623]\n",
      "epoch:37 step:29059 [D loss: 0.321690, acc.: 84.38%] [G loss: 3.733167]\n",
      "epoch:37 step:29060 [D loss: 0.315910, acc.: 86.72%] [G loss: 3.537117]\n",
      "epoch:37 step:29061 [D loss: 0.226540, acc.: 87.50%] [G loss: 2.923400]\n",
      "epoch:37 step:29062 [D loss: 0.303190, acc.: 85.94%] [G loss: 2.686162]\n",
      "epoch:37 step:29063 [D loss: 0.250482, acc.: 89.84%] [G loss: 3.437764]\n",
      "epoch:37 step:29064 [D loss: 0.332833, acc.: 85.16%] [G loss: 3.385127]\n",
      "epoch:37 step:29065 [D loss: 0.404779, acc.: 83.59%] [G loss: 2.880228]\n",
      "epoch:37 step:29066 [D loss: 0.407231, acc.: 84.38%] [G loss: 3.822354]\n",
      "epoch:37 step:29067 [D loss: 0.248046, acc.: 89.06%] [G loss: 2.639473]\n",
      "epoch:37 step:29068 [D loss: 0.350140, acc.: 85.16%] [G loss: 4.431692]\n",
      "epoch:37 step:29069 [D loss: 0.381350, acc.: 80.47%] [G loss: 4.717478]\n",
      "epoch:37 step:29070 [D loss: 0.308310, acc.: 84.38%] [G loss: 4.864010]\n",
      "epoch:37 step:29071 [D loss: 0.428031, acc.: 80.47%] [G loss: 5.154348]\n",
      "epoch:37 step:29072 [D loss: 0.341657, acc.: 88.28%] [G loss: 3.754456]\n",
      "epoch:37 step:29073 [D loss: 0.266219, acc.: 88.28%] [G loss: 3.967814]\n",
      "epoch:37 step:29074 [D loss: 0.297752, acc.: 84.38%] [G loss: 4.584198]\n",
      "epoch:37 step:29075 [D loss: 0.253535, acc.: 91.41%] [G loss: 3.137458]\n",
      "epoch:37 step:29076 [D loss: 0.278894, acc.: 89.06%] [G loss: 3.486686]\n",
      "epoch:37 step:29077 [D loss: 0.468775, acc.: 78.91%] [G loss: 3.494303]\n",
      "epoch:37 step:29078 [D loss: 0.240295, acc.: 90.62%] [G loss: 3.165292]\n",
      "epoch:37 step:29079 [D loss: 0.282763, acc.: 89.06%] [G loss: 3.635904]\n",
      "epoch:37 step:29080 [D loss: 0.211754, acc.: 91.41%] [G loss: 2.480509]\n",
      "epoch:37 step:29081 [D loss: 0.266022, acc.: 87.50%] [G loss: 2.792945]\n",
      "epoch:37 step:29082 [D loss: 0.239029, acc.: 90.62%] [G loss: 3.672045]\n",
      "epoch:37 step:29083 [D loss: 0.289319, acc.: 88.28%] [G loss: 4.110236]\n",
      "epoch:37 step:29084 [D loss: 0.243757, acc.: 88.28%] [G loss: 3.581212]\n",
      "epoch:37 step:29085 [D loss: 0.297850, acc.: 86.72%] [G loss: 3.105076]\n",
      "epoch:37 step:29086 [D loss: 0.325147, acc.: 83.59%] [G loss: 3.628893]\n",
      "epoch:37 step:29087 [D loss: 0.347679, acc.: 83.59%] [G loss: 3.380988]\n",
      "epoch:37 step:29088 [D loss: 0.375577, acc.: 78.91%] [G loss: 2.794548]\n",
      "epoch:37 step:29089 [D loss: 0.301737, acc.: 86.72%] [G loss: 3.033448]\n",
      "epoch:37 step:29090 [D loss: 0.362249, acc.: 81.25%] [G loss: 3.510382]\n",
      "epoch:37 step:29091 [D loss: 0.348952, acc.: 86.72%] [G loss: 3.284748]\n",
      "epoch:37 step:29092 [D loss: 0.334239, acc.: 84.38%] [G loss: 3.209002]\n",
      "epoch:37 step:29093 [D loss: 0.310322, acc.: 87.50%] [G loss: 3.273053]\n",
      "epoch:37 step:29094 [D loss: 0.457616, acc.: 73.44%] [G loss: 2.642600]\n",
      "epoch:37 step:29095 [D loss: 0.255625, acc.: 89.84%] [G loss: 3.040261]\n",
      "epoch:37 step:29096 [D loss: 0.317024, acc.: 84.38%] [G loss: 3.298825]\n",
      "epoch:37 step:29097 [D loss: 0.293597, acc.: 85.16%] [G loss: 3.227574]\n",
      "epoch:37 step:29098 [D loss: 0.326297, acc.: 85.16%] [G loss: 4.917085]\n",
      "epoch:37 step:29099 [D loss: 0.322915, acc.: 83.59%] [G loss: 2.921401]\n",
      "epoch:37 step:29100 [D loss: 0.299257, acc.: 87.50%] [G loss: 3.490872]\n",
      "epoch:37 step:29101 [D loss: 0.314793, acc.: 85.94%] [G loss: 4.504894]\n",
      "epoch:37 step:29102 [D loss: 0.368338, acc.: 85.16%] [G loss: 3.818200]\n",
      "epoch:37 step:29103 [D loss: 0.183262, acc.: 92.19%] [G loss: 2.664107]\n",
      "epoch:37 step:29104 [D loss: 0.277223, acc.: 87.50%] [G loss: 2.856473]\n",
      "epoch:37 step:29105 [D loss: 0.317395, acc.: 84.38%] [G loss: 3.192519]\n",
      "epoch:37 step:29106 [D loss: 0.414651, acc.: 82.81%] [G loss: 3.392718]\n",
      "epoch:37 step:29107 [D loss: 0.367775, acc.: 80.47%] [G loss: 3.478196]\n",
      "epoch:37 step:29108 [D loss: 0.417808, acc.: 78.91%] [G loss: 3.226667]\n",
      "epoch:37 step:29109 [D loss: 0.359117, acc.: 84.38%] [G loss: 2.909350]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:37 step:29110 [D loss: 0.355741, acc.: 85.16%] [G loss: 3.692880]\n",
      "epoch:37 step:29111 [D loss: 0.525394, acc.: 72.66%] [G loss: 4.341784]\n",
      "epoch:37 step:29112 [D loss: 0.545318, acc.: 75.78%] [G loss: 6.533645]\n",
      "epoch:37 step:29113 [D loss: 1.056804, acc.: 71.88%] [G loss: 5.338136]\n",
      "epoch:37 step:29114 [D loss: 0.567926, acc.: 75.00%] [G loss: 3.224522]\n",
      "epoch:37 step:29115 [D loss: 0.297268, acc.: 89.06%] [G loss: 4.469242]\n",
      "epoch:37 step:29116 [D loss: 0.288102, acc.: 88.28%] [G loss: 4.980831]\n",
      "epoch:37 step:29117 [D loss: 0.341315, acc.: 83.59%] [G loss: 4.291763]\n",
      "epoch:37 step:29118 [D loss: 0.354613, acc.: 86.72%] [G loss: 3.866643]\n",
      "epoch:37 step:29119 [D loss: 0.392767, acc.: 79.69%] [G loss: 4.427366]\n",
      "epoch:37 step:29120 [D loss: 0.355485, acc.: 85.94%] [G loss: 4.106975]\n",
      "epoch:37 step:29121 [D loss: 0.377960, acc.: 82.03%] [G loss: 3.895559]\n",
      "epoch:37 step:29122 [D loss: 0.336320, acc.: 84.38%] [G loss: 3.969037]\n",
      "epoch:37 step:29123 [D loss: 0.317562, acc.: 82.81%] [G loss: 3.995906]\n",
      "epoch:37 step:29124 [D loss: 0.260387, acc.: 88.28%] [G loss: 3.309195]\n",
      "epoch:37 step:29125 [D loss: 0.383388, acc.: 85.94%] [G loss: 3.028719]\n",
      "epoch:37 step:29126 [D loss: 0.360664, acc.: 78.12%] [G loss: 3.372256]\n",
      "epoch:37 step:29127 [D loss: 0.302506, acc.: 87.50%] [G loss: 4.741161]\n",
      "epoch:37 step:29128 [D loss: 0.186703, acc.: 91.41%] [G loss: 4.299421]\n",
      "epoch:37 step:29129 [D loss: 0.352696, acc.: 81.25%] [G loss: 4.172377]\n",
      "epoch:37 step:29130 [D loss: 0.322746, acc.: 85.94%] [G loss: 4.091920]\n",
      "epoch:37 step:29131 [D loss: 0.279042, acc.: 85.94%] [G loss: 3.897759]\n",
      "epoch:37 step:29132 [D loss: 0.266213, acc.: 86.72%] [G loss: 3.363626]\n",
      "epoch:37 step:29133 [D loss: 0.201486, acc.: 92.97%] [G loss: 4.015455]\n",
      "epoch:37 step:29134 [D loss: 0.352095, acc.: 83.59%] [G loss: 3.238490]\n",
      "epoch:37 step:29135 [D loss: 0.253118, acc.: 89.06%] [G loss: 2.354748]\n",
      "epoch:37 step:29136 [D loss: 0.213734, acc.: 93.75%] [G loss: 2.732514]\n",
      "epoch:37 step:29137 [D loss: 0.328286, acc.: 85.94%] [G loss: 3.162721]\n",
      "epoch:37 step:29138 [D loss: 0.286656, acc.: 87.50%] [G loss: 2.699584]\n",
      "epoch:37 step:29139 [D loss: 0.385018, acc.: 84.38%] [G loss: 2.945461]\n",
      "epoch:37 step:29140 [D loss: 0.328393, acc.: 82.81%] [G loss: 2.746843]\n",
      "epoch:37 step:29141 [D loss: 0.384624, acc.: 80.47%] [G loss: 2.576979]\n",
      "epoch:37 step:29142 [D loss: 0.397007, acc.: 81.25%] [G loss: 2.326498]\n",
      "epoch:37 step:29143 [D loss: 0.363414, acc.: 82.03%] [G loss: 2.554530]\n",
      "epoch:37 step:29144 [D loss: 0.259891, acc.: 87.50%] [G loss: 3.098870]\n",
      "epoch:37 step:29145 [D loss: 0.335687, acc.: 86.72%] [G loss: 3.271934]\n",
      "epoch:37 step:29146 [D loss: 0.455640, acc.: 80.47%] [G loss: 4.267484]\n",
      "epoch:37 step:29147 [D loss: 0.388500, acc.: 82.03%] [G loss: 2.992099]\n",
      "epoch:37 step:29148 [D loss: 0.257371, acc.: 88.28%] [G loss: 4.270967]\n",
      "epoch:37 step:29149 [D loss: 0.319659, acc.: 84.38%] [G loss: 2.694814]\n",
      "epoch:37 step:29150 [D loss: 0.277271, acc.: 89.06%] [G loss: 4.003111]\n",
      "epoch:37 step:29151 [D loss: 0.259122, acc.: 91.41%] [G loss: 3.116257]\n",
      "epoch:37 step:29152 [D loss: 0.180575, acc.: 94.53%] [G loss: 3.675156]\n",
      "epoch:37 step:29153 [D loss: 0.270875, acc.: 87.50%] [G loss: 3.655146]\n",
      "epoch:37 step:29154 [D loss: 0.309121, acc.: 87.50%] [G loss: 3.401425]\n",
      "epoch:37 step:29155 [D loss: 0.240232, acc.: 89.06%] [G loss: 3.154267]\n",
      "epoch:37 step:29156 [D loss: 0.221626, acc.: 91.41%] [G loss: 3.438141]\n",
      "epoch:37 step:29157 [D loss: 0.294760, acc.: 85.94%] [G loss: 2.657830]\n",
      "epoch:37 step:29158 [D loss: 0.412942, acc.: 81.25%] [G loss: 2.633391]\n",
      "epoch:37 step:29159 [D loss: 0.265034, acc.: 87.50%] [G loss: 3.461180]\n",
      "epoch:37 step:29160 [D loss: 0.290160, acc.: 86.72%] [G loss: 3.263873]\n",
      "epoch:37 step:29161 [D loss: 0.299472, acc.: 86.72%] [G loss: 3.571324]\n",
      "epoch:37 step:29162 [D loss: 0.356301, acc.: 86.72%] [G loss: 2.742142]\n",
      "epoch:37 step:29163 [D loss: 0.328253, acc.: 82.81%] [G loss: 2.970111]\n",
      "epoch:37 step:29164 [D loss: 0.303133, acc.: 85.16%] [G loss: 3.177197]\n",
      "epoch:37 step:29165 [D loss: 0.272639, acc.: 87.50%] [G loss: 3.410950]\n",
      "epoch:37 step:29166 [D loss: 0.311003, acc.: 86.72%] [G loss: 3.073968]\n",
      "epoch:37 step:29167 [D loss: 0.222288, acc.: 91.41%] [G loss: 2.533546]\n",
      "epoch:37 step:29168 [D loss: 0.247545, acc.: 90.62%] [G loss: 3.587530]\n",
      "epoch:37 step:29169 [D loss: 0.318745, acc.: 87.50%] [G loss: 4.029767]\n",
      "epoch:37 step:29170 [D loss: 0.265180, acc.: 86.72%] [G loss: 4.575914]\n",
      "epoch:37 step:29171 [D loss: 0.269572, acc.: 87.50%] [G loss: 3.582678]\n",
      "epoch:37 step:29172 [D loss: 0.379073, acc.: 80.47%] [G loss: 2.419197]\n",
      "epoch:37 step:29173 [D loss: 0.307743, acc.: 87.50%] [G loss: 3.038854]\n",
      "epoch:37 step:29174 [D loss: 0.309431, acc.: 85.94%] [G loss: 3.022331]\n",
      "epoch:37 step:29175 [D loss: 0.356784, acc.: 83.59%] [G loss: 3.353251]\n",
      "epoch:37 step:29176 [D loss: 0.321519, acc.: 85.16%] [G loss: 2.584299]\n",
      "epoch:37 step:29177 [D loss: 0.285837, acc.: 87.50%] [G loss: 3.282679]\n",
      "epoch:37 step:29178 [D loss: 0.325457, acc.: 87.50%] [G loss: 2.822890]\n",
      "epoch:37 step:29179 [D loss: 0.305082, acc.: 85.16%] [G loss: 2.852329]\n",
      "epoch:37 step:29180 [D loss: 0.250635, acc.: 91.41%] [G loss: 3.335695]\n",
      "epoch:37 step:29181 [D loss: 0.336394, acc.: 86.72%] [G loss: 3.479109]\n",
      "epoch:37 step:29182 [D loss: 0.287525, acc.: 89.84%] [G loss: 2.907186]\n",
      "epoch:37 step:29183 [D loss: 0.320388, acc.: 85.94%] [G loss: 3.697934]\n",
      "epoch:37 step:29184 [D loss: 0.238490, acc.: 89.06%] [G loss: 4.216833]\n",
      "epoch:37 step:29185 [D loss: 0.235391, acc.: 89.84%] [G loss: 4.333526]\n",
      "epoch:37 step:29186 [D loss: 0.263787, acc.: 88.28%] [G loss: 3.496684]\n",
      "epoch:37 step:29187 [D loss: 0.259388, acc.: 84.38%] [G loss: 5.493937]\n",
      "epoch:37 step:29188 [D loss: 0.226959, acc.: 92.19%] [G loss: 4.345275]\n",
      "epoch:37 step:29189 [D loss: 0.227047, acc.: 91.41%] [G loss: 3.621602]\n",
      "epoch:37 step:29190 [D loss: 0.295281, acc.: 88.28%] [G loss: 3.672989]\n",
      "epoch:37 step:29191 [D loss: 0.311064, acc.: 87.50%] [G loss: 3.216832]\n",
      "epoch:37 step:29192 [D loss: 0.290312, acc.: 85.16%] [G loss: 3.712976]\n",
      "epoch:37 step:29193 [D loss: 0.255367, acc.: 91.41%] [G loss: 3.312557]\n",
      "epoch:37 step:29194 [D loss: 0.301896, acc.: 90.62%] [G loss: 2.823751]\n",
      "epoch:37 step:29195 [D loss: 0.231971, acc.: 91.41%] [G loss: 3.251997]\n",
      "epoch:37 step:29196 [D loss: 0.242654, acc.: 92.19%] [G loss: 2.791476]\n",
      "epoch:37 step:29197 [D loss: 0.280545, acc.: 89.06%] [G loss: 3.120731]\n",
      "epoch:37 step:29198 [D loss: 0.337968, acc.: 82.03%] [G loss: 3.670548]\n",
      "epoch:37 step:29199 [D loss: 0.382427, acc.: 81.25%] [G loss: 4.621733]\n",
      "epoch:37 step:29200 [D loss: 0.437114, acc.: 78.12%] [G loss: 6.267132]\n",
      "##############\n",
      "[0.85775445 0.84450624 0.81032853 0.78249855 0.80337797 0.81732083\n",
      " 0.87065105 0.82368559 0.83883383 0.83340724]\n",
      "##########\n",
      "epoch:37 step:29201 [D loss: 0.582011, acc.: 83.59%] [G loss: 3.070495]\n",
      "epoch:37 step:29202 [D loss: 0.242192, acc.: 90.62%] [G loss: 3.891061]\n",
      "epoch:37 step:29203 [D loss: 0.482200, acc.: 81.25%] [G loss: 4.162699]\n",
      "epoch:37 step:29204 [D loss: 0.282880, acc.: 86.72%] [G loss: 4.888372]\n",
      "epoch:37 step:29205 [D loss: 0.435040, acc.: 77.34%] [G loss: 3.250654]\n",
      "epoch:37 step:29206 [D loss: 0.316048, acc.: 86.72%] [G loss: 3.137645]\n",
      "epoch:37 step:29207 [D loss: 0.246429, acc.: 88.28%] [G loss: 3.640222]\n",
      "epoch:37 step:29208 [D loss: 0.456384, acc.: 77.34%] [G loss: 3.398466]\n",
      "epoch:37 step:29209 [D loss: 0.358071, acc.: 83.59%] [G loss: 3.232702]\n",
      "epoch:37 step:29210 [D loss: 0.330093, acc.: 82.03%] [G loss: 3.329062]\n",
      "epoch:37 step:29211 [D loss: 0.241437, acc.: 88.28%] [G loss: 3.517648]\n",
      "epoch:37 step:29212 [D loss: 0.475148, acc.: 75.00%] [G loss: 4.068135]\n",
      "epoch:37 step:29213 [D loss: 0.271119, acc.: 90.62%] [G loss: 5.539076]\n",
      "epoch:37 step:29214 [D loss: 0.513941, acc.: 75.00%] [G loss: 4.154488]\n",
      "epoch:37 step:29215 [D loss: 0.567089, acc.: 78.12%] [G loss: 3.676355]\n",
      "epoch:37 step:29216 [D loss: 0.237115, acc.: 89.06%] [G loss: 3.974687]\n",
      "epoch:37 step:29217 [D loss: 0.380905, acc.: 84.38%] [G loss: 4.757741]\n",
      "epoch:37 step:29218 [D loss: 0.448770, acc.: 77.34%] [G loss: 6.052847]\n",
      "epoch:37 step:29219 [D loss: 0.323804, acc.: 85.94%] [G loss: 3.336669]\n",
      "epoch:37 step:29220 [D loss: 0.225110, acc.: 90.62%] [G loss: 3.237731]\n",
      "epoch:37 step:29221 [D loss: 0.256905, acc.: 92.19%] [G loss: 3.807690]\n",
      "epoch:37 step:29222 [D loss: 0.277519, acc.: 89.06%] [G loss: 2.629815]\n",
      "epoch:37 step:29223 [D loss: 0.287864, acc.: 88.28%] [G loss: 2.908209]\n",
      "epoch:37 step:29224 [D loss: 0.289624, acc.: 85.94%] [G loss: 3.232013]\n",
      "epoch:37 step:29225 [D loss: 0.295659, acc.: 87.50%] [G loss: 3.025763]\n",
      "epoch:37 step:29226 [D loss: 0.232220, acc.: 89.84%] [G loss: 2.737277]\n",
      "epoch:37 step:29227 [D loss: 0.346144, acc.: 85.94%] [G loss: 3.212887]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:37 step:29228 [D loss: 0.382791, acc.: 83.59%] [G loss: 2.644638]\n",
      "epoch:37 step:29229 [D loss: 0.340136, acc.: 87.50%] [G loss: 2.937967]\n",
      "epoch:37 step:29230 [D loss: 0.373331, acc.: 84.38%] [G loss: 3.540150]\n",
      "epoch:37 step:29231 [D loss: 0.324092, acc.: 85.94%] [G loss: 3.500658]\n",
      "epoch:37 step:29232 [D loss: 0.243200, acc.: 89.84%] [G loss: 3.359409]\n",
      "epoch:37 step:29233 [D loss: 0.282996, acc.: 85.94%] [G loss: 3.073533]\n",
      "epoch:37 step:29234 [D loss: 0.234273, acc.: 89.06%] [G loss: 4.870339]\n",
      "epoch:37 step:29235 [D loss: 0.304663, acc.: 82.81%] [G loss: 5.325478]\n",
      "epoch:37 step:29236 [D loss: 0.233998, acc.: 90.62%] [G loss: 4.662442]\n",
      "epoch:37 step:29237 [D loss: 0.368654, acc.: 81.25%] [G loss: 8.189936]\n",
      "epoch:37 step:29238 [D loss: 0.495625, acc.: 82.03%] [G loss: 2.761920]\n",
      "epoch:37 step:29239 [D loss: 0.233412, acc.: 90.62%] [G loss: 3.299160]\n",
      "epoch:37 step:29240 [D loss: 0.272219, acc.: 89.06%] [G loss: 2.980120]\n",
      "epoch:37 step:29241 [D loss: 0.260954, acc.: 89.06%] [G loss: 4.722638]\n",
      "epoch:37 step:29242 [D loss: 0.303563, acc.: 88.28%] [G loss: 3.856406]\n",
      "epoch:37 step:29243 [D loss: 0.260304, acc.: 89.84%] [G loss: 4.671789]\n",
      "epoch:37 step:29244 [D loss: 0.269036, acc.: 85.94%] [G loss: 3.370720]\n",
      "epoch:37 step:29245 [D loss: 0.212543, acc.: 92.97%] [G loss: 2.803200]\n",
      "epoch:37 step:29246 [D loss: 0.283768, acc.: 85.16%] [G loss: 3.255864]\n",
      "epoch:37 step:29247 [D loss: 0.281681, acc.: 85.94%] [G loss: 3.972705]\n",
      "epoch:37 step:29248 [D loss: 0.248987, acc.: 91.41%] [G loss: 4.104959]\n",
      "epoch:37 step:29249 [D loss: 0.315832, acc.: 83.59%] [G loss: 3.143605]\n",
      "epoch:37 step:29250 [D loss: 0.259232, acc.: 89.06%] [G loss: 2.721717]\n",
      "epoch:37 step:29251 [D loss: 0.329434, acc.: 86.72%] [G loss: 2.727583]\n",
      "epoch:37 step:29252 [D loss: 0.291425, acc.: 85.94%] [G loss: 3.510509]\n",
      "epoch:37 step:29253 [D loss: 0.303682, acc.: 87.50%] [G loss: 2.797405]\n",
      "epoch:37 step:29254 [D loss: 0.347913, acc.: 85.16%] [G loss: 4.001157]\n",
      "epoch:37 step:29255 [D loss: 0.202832, acc.: 89.84%] [G loss: 4.351398]\n",
      "epoch:37 step:29256 [D loss: 0.370753, acc.: 82.81%] [G loss: 3.332968]\n",
      "epoch:37 step:29257 [D loss: 0.301588, acc.: 85.94%] [G loss: 3.458447]\n",
      "epoch:37 step:29258 [D loss: 0.246872, acc.: 89.84%] [G loss: 3.316514]\n",
      "epoch:37 step:29259 [D loss: 0.323924, acc.: 83.59%] [G loss: 3.149451]\n",
      "epoch:37 step:29260 [D loss: 0.384422, acc.: 84.38%] [G loss: 3.310811]\n",
      "epoch:37 step:29261 [D loss: 0.316335, acc.: 85.16%] [G loss: 2.211396]\n",
      "epoch:37 step:29262 [D loss: 0.330719, acc.: 84.38%] [G loss: 3.219043]\n",
      "epoch:37 step:29263 [D loss: 0.433257, acc.: 81.25%] [G loss: 5.806453]\n",
      "epoch:37 step:29264 [D loss: 0.537429, acc.: 75.78%] [G loss: 3.997098]\n",
      "epoch:37 step:29265 [D loss: 0.660063, acc.: 75.78%] [G loss: 7.221186]\n",
      "epoch:37 step:29266 [D loss: 1.303005, acc.: 69.53%] [G loss: 9.288865]\n",
      "epoch:37 step:29267 [D loss: 1.602636, acc.: 67.97%] [G loss: 4.865547]\n",
      "epoch:37 step:29268 [D loss: 0.843183, acc.: 71.09%] [G loss: 3.843267]\n",
      "epoch:37 step:29269 [D loss: 0.420616, acc.: 87.50%] [G loss: 4.993384]\n",
      "epoch:37 step:29270 [D loss: 0.383914, acc.: 87.50%] [G loss: 3.653239]\n",
      "epoch:37 step:29271 [D loss: 0.438884, acc.: 80.47%] [G loss: 3.249703]\n",
      "epoch:37 step:29272 [D loss: 0.220300, acc.: 92.97%] [G loss: 3.809049]\n",
      "epoch:37 step:29273 [D loss: 0.225447, acc.: 90.62%] [G loss: 3.245728]\n",
      "epoch:37 step:29274 [D loss: 0.265188, acc.: 89.06%] [G loss: 3.063699]\n",
      "epoch:37 step:29275 [D loss: 0.344317, acc.: 83.59%] [G loss: 3.634219]\n",
      "epoch:37 step:29276 [D loss: 0.421569, acc.: 84.38%] [G loss: 3.322918]\n",
      "epoch:37 step:29277 [D loss: 0.301953, acc.: 89.06%] [G loss: 3.011851]\n",
      "epoch:37 step:29278 [D loss: 0.204606, acc.: 90.62%] [G loss: 3.969225]\n",
      "epoch:37 step:29279 [D loss: 0.409594, acc.: 76.56%] [G loss: 2.905823]\n",
      "epoch:37 step:29280 [D loss: 0.260831, acc.: 88.28%] [G loss: 2.764180]\n",
      "epoch:37 step:29281 [D loss: 0.243022, acc.: 91.41%] [G loss: 3.787054]\n",
      "epoch:37 step:29282 [D loss: 0.338152, acc.: 85.16%] [G loss: 3.591615]\n",
      "epoch:37 step:29283 [D loss: 0.241725, acc.: 89.06%] [G loss: 3.331115]\n",
      "epoch:37 step:29284 [D loss: 0.296377, acc.: 85.94%] [G loss: 3.486824]\n",
      "epoch:37 step:29285 [D loss: 0.304159, acc.: 87.50%] [G loss: 3.588382]\n",
      "epoch:37 step:29286 [D loss: 0.379893, acc.: 80.47%] [G loss: 3.070897]\n",
      "epoch:37 step:29287 [D loss: 0.248628, acc.: 89.06%] [G loss: 3.570745]\n",
      "epoch:37 step:29288 [D loss: 0.338615, acc.: 85.16%] [G loss: 2.278459]\n",
      "epoch:37 step:29289 [D loss: 0.238770, acc.: 91.41%] [G loss: 3.179982]\n",
      "epoch:37 step:29290 [D loss: 0.267219, acc.: 87.50%] [G loss: 2.834708]\n",
      "epoch:37 step:29291 [D loss: 0.323536, acc.: 89.06%] [G loss: 2.748148]\n",
      "epoch:37 step:29292 [D loss: 0.325739, acc.: 87.50%] [G loss: 2.710196]\n",
      "epoch:37 step:29293 [D loss: 0.280382, acc.: 85.16%] [G loss: 3.022537]\n",
      "epoch:37 step:29294 [D loss: 0.416049, acc.: 80.47%] [G loss: 2.225311]\n",
      "epoch:37 step:29295 [D loss: 0.295518, acc.: 87.50%] [G loss: 2.658580]\n",
      "epoch:37 step:29296 [D loss: 0.330971, acc.: 89.06%] [G loss: 2.595212]\n",
      "epoch:37 step:29297 [D loss: 0.332570, acc.: 85.16%] [G loss: 3.055251]\n",
      "epoch:37 step:29298 [D loss: 0.290027, acc.: 86.72%] [G loss: 3.219494]\n",
      "epoch:37 step:29299 [D loss: 0.315730, acc.: 84.38%] [G loss: 3.724517]\n",
      "epoch:37 step:29300 [D loss: 0.302893, acc.: 86.72%] [G loss: 3.872422]\n",
      "epoch:37 step:29301 [D loss: 0.247035, acc.: 90.62%] [G loss: 3.220993]\n",
      "epoch:37 step:29302 [D loss: 0.282605, acc.: 86.72%] [G loss: 3.136829]\n",
      "epoch:37 step:29303 [D loss: 0.407942, acc.: 81.25%] [G loss: 2.590985]\n",
      "epoch:37 step:29304 [D loss: 0.323954, acc.: 85.16%] [G loss: 3.358744]\n",
      "epoch:37 step:29305 [D loss: 0.348857, acc.: 84.38%] [G loss: 3.207771]\n",
      "epoch:37 step:29306 [D loss: 0.285439, acc.: 87.50%] [G loss: 3.211841]\n",
      "epoch:37 step:29307 [D loss: 0.256230, acc.: 87.50%] [G loss: 3.587536]\n",
      "epoch:37 step:29308 [D loss: 0.327759, acc.: 82.03%] [G loss: 3.534126]\n",
      "epoch:37 step:29309 [D loss: 0.249942, acc.: 90.62%] [G loss: 3.731791]\n",
      "epoch:37 step:29310 [D loss: 0.266194, acc.: 90.62%] [G loss: 2.820219]\n",
      "epoch:37 step:29311 [D loss: 0.257484, acc.: 89.06%] [G loss: 4.102274]\n",
      "epoch:37 step:29312 [D loss: 0.321605, acc.: 80.47%] [G loss: 4.736788]\n",
      "epoch:37 step:29313 [D loss: 0.278179, acc.: 87.50%] [G loss: 3.808193]\n",
      "epoch:37 step:29314 [D loss: 0.284828, acc.: 88.28%] [G loss: 5.529923]\n",
      "epoch:37 step:29315 [D loss: 0.177542, acc.: 94.53%] [G loss: 4.034991]\n",
      "epoch:37 step:29316 [D loss: 0.305607, acc.: 89.06%] [G loss: 4.185812]\n",
      "epoch:37 step:29317 [D loss: 0.320523, acc.: 86.72%] [G loss: 3.320625]\n",
      "epoch:37 step:29318 [D loss: 0.267372, acc.: 91.41%] [G loss: 3.050070]\n",
      "epoch:37 step:29319 [D loss: 0.316254, acc.: 93.75%] [G loss: 2.906845]\n",
      "epoch:37 step:29320 [D loss: 0.342236, acc.: 86.72%] [G loss: 3.350263]\n",
      "epoch:37 step:29321 [D loss: 0.285832, acc.: 88.28%] [G loss: 3.497604]\n",
      "epoch:37 step:29322 [D loss: 0.253684, acc.: 90.62%] [G loss: 3.054717]\n",
      "epoch:37 step:29323 [D loss: 0.288911, acc.: 86.72%] [G loss: 3.022831]\n",
      "epoch:37 step:29324 [D loss: 0.224663, acc.: 92.19%] [G loss: 3.050091]\n",
      "epoch:37 step:29325 [D loss: 0.393424, acc.: 84.38%] [G loss: 2.137042]\n",
      "epoch:37 step:29326 [D loss: 0.390370, acc.: 78.91%] [G loss: 2.918867]\n",
      "epoch:37 step:29327 [D loss: 0.326750, acc.: 88.28%] [G loss: 2.541028]\n",
      "epoch:37 step:29328 [D loss: 0.319254, acc.: 90.62%] [G loss: 3.778360]\n",
      "epoch:37 step:29329 [D loss: 0.345284, acc.: 85.16%] [G loss: 3.786396]\n",
      "epoch:37 step:29330 [D loss: 0.384442, acc.: 82.81%] [G loss: 3.912940]\n",
      "epoch:37 step:29331 [D loss: 0.250122, acc.: 89.06%] [G loss: 3.912449]\n",
      "epoch:37 step:29332 [D loss: 0.343188, acc.: 89.06%] [G loss: 3.445794]\n",
      "epoch:37 step:29333 [D loss: 0.390811, acc.: 82.03%] [G loss: 2.428934]\n",
      "epoch:37 step:29334 [D loss: 0.306813, acc.: 89.84%] [G loss: 2.888604]\n",
      "epoch:37 step:29335 [D loss: 0.475743, acc.: 70.31%] [G loss: 4.243489]\n",
      "epoch:37 step:29336 [D loss: 0.330570, acc.: 86.72%] [G loss: 2.730341]\n",
      "epoch:37 step:29337 [D loss: 0.289910, acc.: 91.41%] [G loss: 4.650632]\n",
      "epoch:37 step:29338 [D loss: 0.342952, acc.: 85.94%] [G loss: 3.395584]\n",
      "epoch:37 step:29339 [D loss: 0.300625, acc.: 83.59%] [G loss: 3.519014]\n",
      "epoch:37 step:29340 [D loss: 0.376841, acc.: 82.81%] [G loss: 2.632910]\n",
      "epoch:37 step:29341 [D loss: 0.257007, acc.: 90.62%] [G loss: 3.234359]\n",
      "epoch:37 step:29342 [D loss: 0.307956, acc.: 84.38%] [G loss: 2.831351]\n",
      "epoch:37 step:29343 [D loss: 0.257797, acc.: 90.62%] [G loss: 3.937519]\n",
      "epoch:37 step:29344 [D loss: 0.271218, acc.: 89.84%] [G loss: 3.089673]\n",
      "epoch:37 step:29345 [D loss: 0.385466, acc.: 81.25%] [G loss: 2.691330]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:37 step:29346 [D loss: 0.343507, acc.: 85.94%] [G loss: 2.985337]\n",
      "epoch:37 step:29347 [D loss: 0.404975, acc.: 78.12%] [G loss: 2.367487]\n",
      "epoch:37 step:29348 [D loss: 0.301577, acc.: 85.16%] [G loss: 3.271659]\n",
      "epoch:37 step:29349 [D loss: 0.387103, acc.: 82.03%] [G loss: 2.899679]\n",
      "epoch:37 step:29350 [D loss: 0.295779, acc.: 85.94%] [G loss: 3.467758]\n",
      "epoch:37 step:29351 [D loss: 0.289183, acc.: 87.50%] [G loss: 3.270792]\n",
      "epoch:37 step:29352 [D loss: 0.350325, acc.: 85.16%] [G loss: 3.165894]\n",
      "epoch:37 step:29353 [D loss: 0.309430, acc.: 85.94%] [G loss: 2.791913]\n",
      "epoch:37 step:29354 [D loss: 0.251061, acc.: 89.84%] [G loss: 2.694535]\n",
      "epoch:37 step:29355 [D loss: 0.237069, acc.: 89.06%] [G loss: 2.609529]\n",
      "epoch:37 step:29356 [D loss: 0.402385, acc.: 80.47%] [G loss: 2.438860]\n",
      "epoch:37 step:29357 [D loss: 0.251831, acc.: 91.41%] [G loss: 2.660597]\n",
      "epoch:37 step:29358 [D loss: 0.253951, acc.: 91.41%] [G loss: 3.127127]\n",
      "epoch:37 step:29359 [D loss: 0.370034, acc.: 82.03%] [G loss: 2.921718]\n",
      "epoch:37 step:29360 [D loss: 0.315537, acc.: 85.16%] [G loss: 2.938640]\n",
      "epoch:37 step:29361 [D loss: 0.295832, acc.: 85.16%] [G loss: 3.209123]\n",
      "epoch:37 step:29362 [D loss: 0.306078, acc.: 86.72%] [G loss: 3.664730]\n",
      "epoch:37 step:29363 [D loss: 0.366009, acc.: 82.03%] [G loss: 4.067717]\n",
      "epoch:37 step:29364 [D loss: 0.272246, acc.: 83.59%] [G loss: 3.486249]\n",
      "epoch:37 step:29365 [D loss: 0.299210, acc.: 85.16%] [G loss: 3.970334]\n",
      "epoch:37 step:29366 [D loss: 0.315233, acc.: 84.38%] [G loss: 3.443428]\n",
      "epoch:37 step:29367 [D loss: 0.299900, acc.: 85.94%] [G loss: 3.006980]\n",
      "epoch:37 step:29368 [D loss: 0.338009, acc.: 87.50%] [G loss: 3.053874]\n",
      "epoch:37 step:29369 [D loss: 0.336079, acc.: 83.59%] [G loss: 3.277403]\n",
      "epoch:37 step:29370 [D loss: 0.340106, acc.: 82.81%] [G loss: 3.958712]\n",
      "epoch:37 step:29371 [D loss: 0.343705, acc.: 83.59%] [G loss: 3.730309]\n",
      "epoch:37 step:29372 [D loss: 0.285092, acc.: 86.72%] [G loss: 2.327722]\n",
      "epoch:37 step:29373 [D loss: 0.333226, acc.: 85.16%] [G loss: 2.945491]\n",
      "epoch:37 step:29374 [D loss: 0.347647, acc.: 88.28%] [G loss: 2.418395]\n",
      "epoch:37 step:29375 [D loss: 0.403910, acc.: 81.25%] [G loss: 3.590418]\n",
      "epoch:37 step:29376 [D loss: 0.374905, acc.: 82.03%] [G loss: 3.076254]\n",
      "epoch:37 step:29377 [D loss: 0.236424, acc.: 88.28%] [G loss: 3.770970]\n",
      "epoch:37 step:29378 [D loss: 0.422610, acc.: 82.81%] [G loss: 3.429682]\n",
      "epoch:37 step:29379 [D loss: 0.285004, acc.: 85.16%] [G loss: 3.425300]\n",
      "epoch:37 step:29380 [D loss: 0.358046, acc.: 83.59%] [G loss: 3.211172]\n",
      "epoch:37 step:29381 [D loss: 0.228828, acc.: 90.62%] [G loss: 4.867976]\n",
      "epoch:37 step:29382 [D loss: 0.267628, acc.: 89.06%] [G loss: 4.021271]\n",
      "epoch:37 step:29383 [D loss: 0.248353, acc.: 89.84%] [G loss: 4.796265]\n",
      "epoch:37 step:29384 [D loss: 0.292698, acc.: 81.25%] [G loss: 3.751617]\n",
      "epoch:37 step:29385 [D loss: 0.217109, acc.: 91.41%] [G loss: 3.056850]\n",
      "epoch:37 step:29386 [D loss: 0.258077, acc.: 89.06%] [G loss: 2.957497]\n",
      "epoch:37 step:29387 [D loss: 0.259469, acc.: 92.19%] [G loss: 2.502956]\n",
      "epoch:37 step:29388 [D loss: 0.392885, acc.: 82.81%] [G loss: 2.453650]\n",
      "epoch:37 step:29389 [D loss: 0.314174, acc.: 85.94%] [G loss: 3.190105]\n",
      "epoch:37 step:29390 [D loss: 0.290111, acc.: 86.72%] [G loss: 3.419114]\n",
      "epoch:37 step:29391 [D loss: 0.260284, acc.: 89.06%] [G loss: 3.350209]\n",
      "epoch:37 step:29392 [D loss: 0.273733, acc.: 85.94%] [G loss: 3.423162]\n",
      "epoch:37 step:29393 [D loss: 0.339974, acc.: 82.81%] [G loss: 3.019067]\n",
      "epoch:37 step:29394 [D loss: 0.314039, acc.: 86.72%] [G loss: 3.121964]\n",
      "epoch:37 step:29395 [D loss: 0.304388, acc.: 83.59%] [G loss: 3.438471]\n",
      "epoch:37 step:29396 [D loss: 0.298367, acc.: 88.28%] [G loss: 3.168784]\n",
      "epoch:37 step:29397 [D loss: 0.345598, acc.: 88.28%] [G loss: 2.791174]\n",
      "epoch:37 step:29398 [D loss: 0.331668, acc.: 85.94%] [G loss: 3.515594]\n",
      "epoch:37 step:29399 [D loss: 0.384804, acc.: 82.81%] [G loss: 3.739811]\n",
      "epoch:37 step:29400 [D loss: 0.412761, acc.: 85.16%] [G loss: 4.796712]\n",
      "##############\n",
      "[0.87535945 0.8502515  0.79103475 0.81689228 0.75059719 0.83528857\n",
      " 0.87741381 0.82735914 0.83570116 0.807123  ]\n",
      "##########\n",
      "epoch:37 step:29401 [D loss: 0.501194, acc.: 78.12%] [G loss: 6.580696]\n",
      "epoch:37 step:29402 [D loss: 0.649146, acc.: 71.09%] [G loss: 4.787334]\n",
      "epoch:37 step:29403 [D loss: 0.385870, acc.: 82.03%] [G loss: 4.224627]\n",
      "epoch:37 step:29404 [D loss: 0.276761, acc.: 85.94%] [G loss: 5.098490]\n",
      "epoch:37 step:29405 [D loss: 0.353592, acc.: 84.38%] [G loss: 4.281868]\n",
      "epoch:37 step:29406 [D loss: 0.344561, acc.: 82.03%] [G loss: 3.468871]\n",
      "epoch:37 step:29407 [D loss: 0.269680, acc.: 89.84%] [G loss: 3.402917]\n",
      "epoch:37 step:29408 [D loss: 0.300607, acc.: 83.59%] [G loss: 4.358914]\n",
      "epoch:37 step:29409 [D loss: 0.311416, acc.: 87.50%] [G loss: 4.203356]\n",
      "epoch:37 step:29410 [D loss: 0.320874, acc.: 84.38%] [G loss: 3.411476]\n",
      "epoch:37 step:29411 [D loss: 0.326633, acc.: 82.81%] [G loss: 3.166075]\n",
      "epoch:37 step:29412 [D loss: 0.301417, acc.: 86.72%] [G loss: 3.011101]\n",
      "epoch:37 step:29413 [D loss: 0.357614, acc.: 82.81%] [G loss: 2.484105]\n",
      "epoch:37 step:29414 [D loss: 0.364844, acc.: 85.16%] [G loss: 3.112060]\n",
      "epoch:37 step:29415 [D loss: 0.304865, acc.: 86.72%] [G loss: 2.853184]\n",
      "epoch:37 step:29416 [D loss: 0.295189, acc.: 87.50%] [G loss: 4.216278]\n",
      "epoch:37 step:29417 [D loss: 0.270524, acc.: 88.28%] [G loss: 3.683138]\n",
      "epoch:37 step:29418 [D loss: 0.280761, acc.: 88.28%] [G loss: 3.510163]\n",
      "epoch:37 step:29419 [D loss: 0.377438, acc.: 82.81%] [G loss: 3.331480]\n",
      "epoch:37 step:29420 [D loss: 0.419518, acc.: 83.59%] [G loss: 2.426300]\n",
      "epoch:37 step:29421 [D loss: 0.272548, acc.: 89.84%] [G loss: 3.496165]\n",
      "epoch:37 step:29422 [D loss: 0.354304, acc.: 88.28%] [G loss: 2.217802]\n",
      "epoch:37 step:29423 [D loss: 0.343826, acc.: 81.25%] [G loss: 3.136825]\n",
      "epoch:37 step:29424 [D loss: 0.299319, acc.: 85.94%] [G loss: 3.649063]\n",
      "epoch:37 step:29425 [D loss: 0.355862, acc.: 85.94%] [G loss: 3.888994]\n",
      "epoch:37 step:29426 [D loss: 0.385552, acc.: 81.25%] [G loss: 3.039458]\n",
      "epoch:37 step:29427 [D loss: 0.282857, acc.: 85.94%] [G loss: 3.879663]\n",
      "epoch:37 step:29428 [D loss: 0.400884, acc.: 79.69%] [G loss: 2.482674]\n",
      "epoch:37 step:29429 [D loss: 0.273294, acc.: 88.28%] [G loss: 2.925564]\n",
      "epoch:37 step:29430 [D loss: 0.309674, acc.: 89.84%] [G loss: 2.594383]\n",
      "epoch:37 step:29431 [D loss: 0.295983, acc.: 84.38%] [G loss: 3.360173]\n",
      "epoch:37 step:29432 [D loss: 0.259572, acc.: 91.41%] [G loss: 3.553063]\n",
      "epoch:37 step:29433 [D loss: 0.333598, acc.: 85.16%] [G loss: 3.063123]\n",
      "epoch:37 step:29434 [D loss: 0.415080, acc.: 78.91%] [G loss: 2.982609]\n",
      "epoch:37 step:29435 [D loss: 0.395635, acc.: 82.03%] [G loss: 3.872949]\n",
      "epoch:37 step:29436 [D loss: 0.308573, acc.: 87.50%] [G loss: 3.345547]\n",
      "epoch:37 step:29437 [D loss: 0.301486, acc.: 86.72%] [G loss: 4.690665]\n",
      "epoch:37 step:29438 [D loss: 0.487131, acc.: 78.91%] [G loss: 3.891866]\n",
      "epoch:37 step:29439 [D loss: 0.398552, acc.: 78.12%] [G loss: 3.359367]\n",
      "epoch:37 step:29440 [D loss: 0.333340, acc.: 84.38%] [G loss: 3.116066]\n",
      "epoch:37 step:29441 [D loss: 0.353997, acc.: 82.81%] [G loss: 3.651773]\n",
      "epoch:37 step:29442 [D loss: 0.236642, acc.: 92.19%] [G loss: 2.792869]\n",
      "epoch:37 step:29443 [D loss: 0.315118, acc.: 85.94%] [G loss: 2.783436]\n",
      "epoch:37 step:29444 [D loss: 0.373550, acc.: 78.91%] [G loss: 3.004790]\n",
      "epoch:37 step:29445 [D loss: 0.260809, acc.: 89.84%] [G loss: 3.279984]\n",
      "epoch:37 step:29446 [D loss: 0.294010, acc.: 89.06%] [G loss: 3.278547]\n",
      "epoch:37 step:29447 [D loss: 0.400017, acc.: 76.56%] [G loss: 2.656129]\n",
      "epoch:37 step:29448 [D loss: 0.290803, acc.: 84.38%] [G loss: 4.751659]\n",
      "epoch:37 step:29449 [D loss: 0.513798, acc.: 79.69%] [G loss: 5.102596]\n",
      "epoch:37 step:29450 [D loss: 0.391253, acc.: 82.03%] [G loss: 2.404419]\n",
      "epoch:37 step:29451 [D loss: 0.374688, acc.: 83.59%] [G loss: 3.688436]\n",
      "epoch:37 step:29452 [D loss: 0.347551, acc.: 85.94%] [G loss: 3.158352]\n",
      "epoch:37 step:29453 [D loss: 0.358399, acc.: 85.16%] [G loss: 2.710004]\n",
      "epoch:37 step:29454 [D loss: 0.301990, acc.: 88.28%] [G loss: 2.761845]\n",
      "epoch:37 step:29455 [D loss: 0.415626, acc.: 75.78%] [G loss: 2.645157]\n",
      "epoch:37 step:29456 [D loss: 0.352183, acc.: 85.94%] [G loss: 2.842690]\n",
      "epoch:37 step:29457 [D loss: 0.380978, acc.: 83.59%] [G loss: 2.931908]\n",
      "epoch:37 step:29458 [D loss: 0.356380, acc.: 82.03%] [G loss: 2.774197]\n",
      "epoch:37 step:29459 [D loss: 0.358045, acc.: 85.16%] [G loss: 3.220325]\n",
      "epoch:37 step:29460 [D loss: 0.316569, acc.: 85.94%] [G loss: 2.778664]\n",
      "epoch:37 step:29461 [D loss: 0.314720, acc.: 88.28%] [G loss: 3.551246]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:37 step:29462 [D loss: 0.331880, acc.: 84.38%] [G loss: 3.405118]\n",
      "epoch:37 step:29463 [D loss: 0.285077, acc.: 85.16%] [G loss: 3.709186]\n",
      "epoch:37 step:29464 [D loss: 0.224437, acc.: 92.19%] [G loss: 3.121938]\n",
      "epoch:37 step:29465 [D loss: 0.295344, acc.: 89.84%] [G loss: 2.588008]\n",
      "epoch:37 step:29466 [D loss: 0.277472, acc.: 86.72%] [G loss: 4.337083]\n",
      "epoch:37 step:29467 [D loss: 0.178924, acc.: 92.97%] [G loss: 5.921420]\n",
      "epoch:37 step:29468 [D loss: 0.218501, acc.: 90.62%] [G loss: 6.516688]\n",
      "epoch:37 step:29469 [D loss: 0.258677, acc.: 87.50%] [G loss: 5.990992]\n",
      "epoch:37 step:29470 [D loss: 0.329165, acc.: 82.03%] [G loss: 2.970494]\n",
      "epoch:37 step:29471 [D loss: 0.256630, acc.: 89.06%] [G loss: 4.905946]\n",
      "epoch:37 step:29472 [D loss: 0.292243, acc.: 89.06%] [G loss: 3.708138]\n",
      "epoch:37 step:29473 [D loss: 0.309518, acc.: 85.94%] [G loss: 2.989111]\n",
      "epoch:37 step:29474 [D loss: 0.402113, acc.: 83.59%] [G loss: 3.663433]\n",
      "epoch:37 step:29475 [D loss: 0.270833, acc.: 87.50%] [G loss: 2.990985]\n",
      "epoch:37 step:29476 [D loss: 0.341318, acc.: 85.94%] [G loss: 2.860616]\n",
      "epoch:37 step:29477 [D loss: 0.394102, acc.: 85.16%] [G loss: 4.059429]\n",
      "epoch:37 step:29478 [D loss: 0.277034, acc.: 85.16%] [G loss: 3.615716]\n",
      "epoch:37 step:29479 [D loss: 0.324042, acc.: 85.94%] [G loss: 5.584939]\n",
      "epoch:37 step:29480 [D loss: 0.539456, acc.: 74.22%] [G loss: 4.101007]\n",
      "epoch:37 step:29481 [D loss: 0.467138, acc.: 79.69%] [G loss: 3.644787]\n",
      "epoch:37 step:29482 [D loss: 0.322486, acc.: 86.72%] [G loss: 3.207978]\n",
      "epoch:37 step:29483 [D loss: 0.494347, acc.: 75.00%] [G loss: 5.780500]\n",
      "epoch:37 step:29484 [D loss: 0.342898, acc.: 82.03%] [G loss: 3.721994]\n",
      "epoch:37 step:29485 [D loss: 0.275444, acc.: 89.06%] [G loss: 3.430392]\n",
      "epoch:37 step:29486 [D loss: 0.292903, acc.: 86.72%] [G loss: 3.562462]\n",
      "epoch:37 step:29487 [D loss: 0.327116, acc.: 84.38%] [G loss: 3.215233]\n",
      "epoch:37 step:29488 [D loss: 0.405089, acc.: 78.91%] [G loss: 3.853820]\n",
      "epoch:37 step:29489 [D loss: 0.253675, acc.: 91.41%] [G loss: 3.702910]\n",
      "epoch:37 step:29490 [D loss: 0.332667, acc.: 86.72%] [G loss: 3.931937]\n",
      "epoch:37 step:29491 [D loss: 0.283555, acc.: 86.72%] [G loss: 3.683031]\n",
      "epoch:37 step:29492 [D loss: 0.367990, acc.: 84.38%] [G loss: 3.546932]\n",
      "epoch:37 step:29493 [D loss: 0.319918, acc.: 82.81%] [G loss: 2.755563]\n",
      "epoch:37 step:29494 [D loss: 0.266602, acc.: 89.06%] [G loss: 4.872068]\n",
      "epoch:37 step:29495 [D loss: 0.304693, acc.: 85.16%] [G loss: 3.468668]\n",
      "epoch:37 step:29496 [D loss: 0.351976, acc.: 88.28%] [G loss: 3.681082]\n",
      "epoch:37 step:29497 [D loss: 0.336215, acc.: 85.16%] [G loss: 3.259305]\n",
      "epoch:37 step:29498 [D loss: 0.227218, acc.: 92.97%] [G loss: 2.371252]\n",
      "epoch:37 step:29499 [D loss: 0.239012, acc.: 90.62%] [G loss: 4.104891]\n",
      "epoch:37 step:29500 [D loss: 0.198214, acc.: 90.62%] [G loss: 4.010057]\n",
      "epoch:37 step:29501 [D loss: 0.278686, acc.: 85.94%] [G loss: 2.700468]\n",
      "epoch:37 step:29502 [D loss: 0.371216, acc.: 84.38%] [G loss: 4.627158]\n",
      "epoch:37 step:29503 [D loss: 0.381166, acc.: 84.38%] [G loss: 4.199083]\n",
      "epoch:37 step:29504 [D loss: 0.318788, acc.: 86.72%] [G loss: 4.348063]\n",
      "epoch:37 step:29505 [D loss: 0.314672, acc.: 86.72%] [G loss: 3.913587]\n",
      "epoch:37 step:29506 [D loss: 0.374217, acc.: 83.59%] [G loss: 3.707586]\n",
      "epoch:37 step:29507 [D loss: 0.305413, acc.: 86.72%] [G loss: 3.202523]\n",
      "epoch:37 step:29508 [D loss: 0.426844, acc.: 75.78%] [G loss: 3.516652]\n",
      "epoch:37 step:29509 [D loss: 0.206527, acc.: 92.97%] [G loss: 3.979198]\n",
      "epoch:37 step:29510 [D loss: 0.309692, acc.: 84.38%] [G loss: 4.763348]\n",
      "epoch:37 step:29511 [D loss: 0.283958, acc.: 87.50%] [G loss: 3.023686]\n",
      "epoch:37 step:29512 [D loss: 0.342085, acc.: 84.38%] [G loss: 4.359736]\n",
      "epoch:37 step:29513 [D loss: 0.441529, acc.: 79.69%] [G loss: 7.827024]\n",
      "epoch:37 step:29514 [D loss: 0.817081, acc.: 67.19%] [G loss: 4.347359]\n",
      "epoch:37 step:29515 [D loss: 0.488307, acc.: 77.34%] [G loss: 3.984620]\n",
      "epoch:37 step:29516 [D loss: 0.332602, acc.: 82.03%] [G loss: 2.999603]\n",
      "epoch:37 step:29517 [D loss: 0.321869, acc.: 82.03%] [G loss: 3.225182]\n",
      "epoch:37 step:29518 [D loss: 0.310321, acc.: 84.38%] [G loss: 4.031708]\n",
      "epoch:37 step:29519 [D loss: 0.343611, acc.: 85.16%] [G loss: 3.578387]\n",
      "epoch:37 step:29520 [D loss: 0.351817, acc.: 82.03%] [G loss: 4.779642]\n",
      "epoch:37 step:29521 [D loss: 0.386625, acc.: 79.69%] [G loss: 2.565724]\n",
      "epoch:37 step:29522 [D loss: 0.339059, acc.: 85.94%] [G loss: 3.396735]\n",
      "epoch:37 step:29523 [D loss: 0.284866, acc.: 85.16%] [G loss: 3.361564]\n",
      "epoch:37 step:29524 [D loss: 0.361064, acc.: 81.25%] [G loss: 4.607974]\n",
      "epoch:37 step:29525 [D loss: 0.304712, acc.: 85.94%] [G loss: 3.991722]\n",
      "epoch:37 step:29526 [D loss: 0.253641, acc.: 86.72%] [G loss: 3.253890]\n",
      "epoch:37 step:29527 [D loss: 0.486056, acc.: 75.78%] [G loss: 3.937610]\n",
      "epoch:37 step:29528 [D loss: 0.463813, acc.: 75.78%] [G loss: 3.604329]\n",
      "epoch:37 step:29529 [D loss: 0.437329, acc.: 81.25%] [G loss: 3.196144]\n",
      "epoch:37 step:29530 [D loss: 0.385479, acc.: 83.59%] [G loss: 4.466876]\n",
      "epoch:37 step:29531 [D loss: 0.306747, acc.: 90.62%] [G loss: 2.763883]\n",
      "epoch:37 step:29532 [D loss: 0.404281, acc.: 82.81%] [G loss: 2.398933]\n",
      "epoch:37 step:29533 [D loss: 0.286712, acc.: 89.84%] [G loss: 2.344768]\n",
      "epoch:37 step:29534 [D loss: 0.251157, acc.: 90.62%] [G loss: 2.812030]\n",
      "epoch:37 step:29535 [D loss: 0.325522, acc.: 85.16%] [G loss: 3.138963]\n",
      "epoch:37 step:29536 [D loss: 0.349070, acc.: 84.38%] [G loss: 2.590184]\n",
      "epoch:37 step:29537 [D loss: 0.411647, acc.: 80.47%] [G loss: 2.740604]\n",
      "epoch:37 step:29538 [D loss: 0.319873, acc.: 88.28%] [G loss: 3.415884]\n",
      "epoch:37 step:29539 [D loss: 0.362904, acc.: 84.38%] [G loss: 3.997960]\n",
      "epoch:37 step:29540 [D loss: 0.387618, acc.: 81.25%] [G loss: 2.682171]\n",
      "epoch:37 step:29541 [D loss: 0.327212, acc.: 87.50%] [G loss: 2.599518]\n",
      "epoch:37 step:29542 [D loss: 0.279624, acc.: 86.72%] [G loss: 3.113762]\n",
      "epoch:37 step:29543 [D loss: 0.394727, acc.: 82.03%] [G loss: 2.957547]\n",
      "epoch:37 step:29544 [D loss: 0.248547, acc.: 89.84%] [G loss: 3.296547]\n",
      "epoch:37 step:29545 [D loss: 0.340972, acc.: 85.94%] [G loss: 2.529229]\n",
      "epoch:37 step:29546 [D loss: 0.269147, acc.: 87.50%] [G loss: 3.175968]\n",
      "epoch:37 step:29547 [D loss: 0.356399, acc.: 83.59%] [G loss: 2.791894]\n",
      "epoch:37 step:29548 [D loss: 0.376112, acc.: 82.81%] [G loss: 2.862453]\n",
      "epoch:37 step:29549 [D loss: 0.289960, acc.: 88.28%] [G loss: 2.859150]\n",
      "epoch:37 step:29550 [D loss: 0.317879, acc.: 87.50%] [G loss: 2.858966]\n",
      "epoch:37 step:29551 [D loss: 0.326116, acc.: 83.59%] [G loss: 2.979383]\n",
      "epoch:37 step:29552 [D loss: 0.237412, acc.: 91.41%] [G loss: 3.133572]\n",
      "epoch:37 step:29553 [D loss: 0.260560, acc.: 87.50%] [G loss: 3.444406]\n",
      "epoch:37 step:29554 [D loss: 0.322494, acc.: 89.06%] [G loss: 3.260376]\n",
      "epoch:37 step:29555 [D loss: 0.279977, acc.: 87.50%] [G loss: 2.652287]\n",
      "epoch:37 step:29556 [D loss: 0.287166, acc.: 89.06%] [G loss: 2.695941]\n",
      "epoch:37 step:29557 [D loss: 0.300555, acc.: 88.28%] [G loss: 3.043415]\n",
      "epoch:37 step:29558 [D loss: 0.257703, acc.: 89.06%] [G loss: 2.913781]\n",
      "epoch:37 step:29559 [D loss: 0.312390, acc.: 85.16%] [G loss: 3.143494]\n",
      "epoch:37 step:29560 [D loss: 0.274940, acc.: 86.72%] [G loss: 2.960938]\n",
      "epoch:37 step:29561 [D loss: 0.263179, acc.: 88.28%] [G loss: 3.244956]\n",
      "epoch:37 step:29562 [D loss: 0.353242, acc.: 82.81%] [G loss: 2.699554]\n",
      "epoch:37 step:29563 [D loss: 0.319539, acc.: 85.16%] [G loss: 3.182134]\n",
      "epoch:37 step:29564 [D loss: 0.338234, acc.: 83.59%] [G loss: 2.635440]\n",
      "epoch:37 step:29565 [D loss: 0.210560, acc.: 93.75%] [G loss: 3.640405]\n",
      "epoch:37 step:29566 [D loss: 0.386182, acc.: 82.81%] [G loss: 2.338701]\n",
      "epoch:37 step:29567 [D loss: 0.289669, acc.: 86.72%] [G loss: 2.902241]\n",
      "epoch:37 step:29568 [D loss: 0.283310, acc.: 89.06%] [G loss: 2.977981]\n",
      "epoch:37 step:29569 [D loss: 0.272802, acc.: 85.16%] [G loss: 4.432735]\n",
      "epoch:37 step:29570 [D loss: 0.465543, acc.: 83.59%] [G loss: 3.908805]\n",
      "epoch:37 step:29571 [D loss: 0.552859, acc.: 74.22%] [G loss: 5.743348]\n",
      "epoch:37 step:29572 [D loss: 0.397176, acc.: 83.59%] [G loss: 4.170905]\n",
      "epoch:37 step:29573 [D loss: 0.474523, acc.: 78.91%] [G loss: 3.509794]\n",
      "epoch:37 step:29574 [D loss: 0.301635, acc.: 88.28%] [G loss: 3.727951]\n",
      "epoch:37 step:29575 [D loss: 0.303645, acc.: 88.28%] [G loss: 3.491372]\n",
      "epoch:37 step:29576 [D loss: 0.232216, acc.: 93.75%] [G loss: 2.993637]\n",
      "epoch:37 step:29577 [D loss: 0.189002, acc.: 93.75%] [G loss: 4.338732]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:37 step:29578 [D loss: 0.190707, acc.: 90.62%] [G loss: 3.206101]\n",
      "epoch:37 step:29579 [D loss: 0.235121, acc.: 91.41%] [G loss: 3.189038]\n",
      "epoch:37 step:29580 [D loss: 0.236396, acc.: 86.72%] [G loss: 3.060122]\n",
      "epoch:37 step:29581 [D loss: 0.253622, acc.: 85.94%] [G loss: 3.490640]\n",
      "epoch:37 step:29582 [D loss: 0.318868, acc.: 83.59%] [G loss: 2.878986]\n",
      "epoch:37 step:29583 [D loss: 0.237919, acc.: 93.75%] [G loss: 3.246818]\n",
      "epoch:37 step:29584 [D loss: 0.262364, acc.: 91.41%] [G loss: 3.211676]\n",
      "epoch:37 step:29585 [D loss: 0.346515, acc.: 81.25%] [G loss: 3.158960]\n",
      "epoch:37 step:29586 [D loss: 0.303846, acc.: 85.16%] [G loss: 3.484536]\n",
      "epoch:37 step:29587 [D loss: 0.251886, acc.: 87.50%] [G loss: 3.842582]\n",
      "epoch:37 step:29588 [D loss: 0.300688, acc.: 86.72%] [G loss: 3.831938]\n",
      "epoch:37 step:29589 [D loss: 0.255583, acc.: 91.41%] [G loss: 3.941304]\n",
      "epoch:37 step:29590 [D loss: 0.277452, acc.: 86.72%] [G loss: 4.840600]\n",
      "epoch:37 step:29591 [D loss: 0.262038, acc.: 90.62%] [G loss: 3.438411]\n",
      "epoch:37 step:29592 [D loss: 0.223567, acc.: 90.62%] [G loss: 3.774525]\n",
      "epoch:37 step:29593 [D loss: 0.225066, acc.: 90.62%] [G loss: 2.882665]\n",
      "epoch:37 step:29594 [D loss: 0.349560, acc.: 85.16%] [G loss: 2.961754]\n",
      "epoch:37 step:29595 [D loss: 0.290181, acc.: 89.84%] [G loss: 2.523469]\n",
      "epoch:37 step:29596 [D loss: 0.220575, acc.: 93.75%] [G loss: 3.554928]\n",
      "epoch:37 step:29597 [D loss: 0.282750, acc.: 89.84%] [G loss: 4.072663]\n",
      "epoch:37 step:29598 [D loss: 0.358064, acc.: 84.38%] [G loss: 2.636963]\n",
      "epoch:37 step:29599 [D loss: 0.262041, acc.: 89.84%] [G loss: 2.907182]\n",
      "epoch:37 step:29600 [D loss: 0.418769, acc.: 79.69%] [G loss: 2.667745]\n",
      "##############\n",
      "[0.87902318 0.86478261 0.8157463  0.80010976 0.77937411 0.83644542\n",
      " 0.88703216 0.8362814  0.79863058 0.82642632]\n",
      "##########\n",
      "epoch:37 step:29601 [D loss: 0.353728, acc.: 85.94%] [G loss: 2.665636]\n",
      "epoch:37 step:29602 [D loss: 0.309605, acc.: 85.94%] [G loss: 2.823835]\n",
      "epoch:37 step:29603 [D loss: 0.361105, acc.: 83.59%] [G loss: 2.655589]\n",
      "epoch:37 step:29604 [D loss: 0.299435, acc.: 88.28%] [G loss: 1.961186]\n",
      "epoch:37 step:29605 [D loss: 0.410509, acc.: 81.25%] [G loss: 2.966079]\n",
      "epoch:37 step:29606 [D loss: 0.328257, acc.: 89.84%] [G loss: 3.371525]\n",
      "epoch:37 step:29607 [D loss: 0.364721, acc.: 82.81%] [G loss: 3.425543]\n",
      "epoch:37 step:29608 [D loss: 0.378058, acc.: 84.38%] [G loss: 3.673646]\n",
      "epoch:37 step:29609 [D loss: 0.404850, acc.: 78.91%] [G loss: 3.241155]\n",
      "epoch:37 step:29610 [D loss: 0.381160, acc.: 82.81%] [G loss: 2.936985]\n",
      "epoch:37 step:29611 [D loss: 0.336654, acc.: 85.16%] [G loss: 3.185765]\n",
      "epoch:37 step:29612 [D loss: 0.284249, acc.: 84.38%] [G loss: 3.397893]\n",
      "epoch:37 step:29613 [D loss: 0.274463, acc.: 85.94%] [G loss: 3.925351]\n",
      "epoch:37 step:29614 [D loss: 0.279910, acc.: 83.59%] [G loss: 3.559848]\n",
      "epoch:37 step:29615 [D loss: 0.354165, acc.: 85.16%] [G loss: 3.011494]\n",
      "epoch:37 step:29616 [D loss: 0.234510, acc.: 90.62%] [G loss: 4.225861]\n",
      "epoch:37 step:29617 [D loss: 0.397049, acc.: 79.69%] [G loss: 3.184411]\n",
      "epoch:37 step:29618 [D loss: 0.469554, acc.: 76.56%] [G loss: 3.329506]\n",
      "epoch:37 step:29619 [D loss: 0.315314, acc.: 89.06%] [G loss: 2.757589]\n",
      "epoch:37 step:29620 [D loss: 0.316085, acc.: 84.38%] [G loss: 3.199374]\n",
      "epoch:37 step:29621 [D loss: 0.228303, acc.: 89.84%] [G loss: 3.328003]\n",
      "epoch:37 step:29622 [D loss: 0.400594, acc.: 82.81%] [G loss: 4.456624]\n",
      "epoch:37 step:29623 [D loss: 0.333767, acc.: 86.72%] [G loss: 4.165019]\n",
      "epoch:37 step:29624 [D loss: 0.304011, acc.: 91.41%] [G loss: 2.913578]\n",
      "epoch:37 step:29625 [D loss: 0.418886, acc.: 81.25%] [G loss: 3.778004]\n",
      "epoch:37 step:29626 [D loss: 0.277625, acc.: 86.72%] [G loss: 3.658155]\n",
      "epoch:37 step:29627 [D loss: 0.287371, acc.: 86.72%] [G loss: 3.290504]\n",
      "epoch:37 step:29628 [D loss: 0.336798, acc.: 85.16%] [G loss: 2.999031]\n",
      "epoch:37 step:29629 [D loss: 0.291411, acc.: 86.72%] [G loss: 3.742730]\n",
      "epoch:37 step:29630 [D loss: 0.370107, acc.: 81.25%] [G loss: 3.445740]\n",
      "epoch:37 step:29631 [D loss: 0.327905, acc.: 83.59%] [G loss: 2.998019]\n",
      "epoch:37 step:29632 [D loss: 0.332324, acc.: 82.81%] [G loss: 3.128040]\n",
      "epoch:37 step:29633 [D loss: 0.284713, acc.: 89.06%] [G loss: 4.125723]\n",
      "epoch:37 step:29634 [D loss: 0.273159, acc.: 86.72%] [G loss: 3.996680]\n",
      "epoch:37 step:29635 [D loss: 0.350193, acc.: 85.16%] [G loss: 4.808973]\n",
      "epoch:37 step:29636 [D loss: 0.235300, acc.: 88.28%] [G loss: 5.328612]\n",
      "epoch:37 step:29637 [D loss: 0.257662, acc.: 89.06%] [G loss: 3.638389]\n",
      "epoch:37 step:29638 [D loss: 0.199752, acc.: 94.53%] [G loss: 5.503386]\n",
      "epoch:37 step:29639 [D loss: 0.317883, acc.: 88.28%] [G loss: 4.170078]\n",
      "epoch:37 step:29640 [D loss: 0.226287, acc.: 89.84%] [G loss: 5.308112]\n",
      "epoch:37 step:29641 [D loss: 0.267344, acc.: 85.16%] [G loss: 3.772508]\n",
      "epoch:37 step:29642 [D loss: 0.255886, acc.: 85.94%] [G loss: 4.013861]\n",
      "epoch:37 step:29643 [D loss: 0.190253, acc.: 92.19%] [G loss: 3.761266]\n",
      "epoch:37 step:29644 [D loss: 0.215390, acc.: 94.53%] [G loss: 3.708131]\n",
      "epoch:37 step:29645 [D loss: 0.308287, acc.: 85.16%] [G loss: 4.105270]\n",
      "epoch:37 step:29646 [D loss: 0.311303, acc.: 88.28%] [G loss: 4.861378]\n",
      "epoch:37 step:29647 [D loss: 0.370508, acc.: 84.38%] [G loss: 5.186141]\n",
      "epoch:37 step:29648 [D loss: 0.314708, acc.: 85.16%] [G loss: 4.399355]\n",
      "epoch:37 step:29649 [D loss: 0.248718, acc.: 90.62%] [G loss: 4.055050]\n",
      "epoch:37 step:29650 [D loss: 0.305874, acc.: 85.16%] [G loss: 4.691296]\n",
      "epoch:37 step:29651 [D loss: 0.416916, acc.: 76.56%] [G loss: 4.206143]\n",
      "epoch:37 step:29652 [D loss: 0.293916, acc.: 85.16%] [G loss: 3.786809]\n",
      "epoch:37 step:29653 [D loss: 0.278586, acc.: 89.06%] [G loss: 4.074480]\n",
      "epoch:37 step:29654 [D loss: 0.287850, acc.: 86.72%] [G loss: 3.492008]\n",
      "epoch:37 step:29655 [D loss: 0.330012, acc.: 85.94%] [G loss: 3.239033]\n",
      "epoch:37 step:29656 [D loss: 0.301573, acc.: 84.38%] [G loss: 4.089511]\n",
      "epoch:37 step:29657 [D loss: 0.378857, acc.: 82.03%] [G loss: 3.643209]\n",
      "epoch:37 step:29658 [D loss: 0.275880, acc.: 90.62%] [G loss: 4.205575]\n",
      "epoch:37 step:29659 [D loss: 0.336174, acc.: 83.59%] [G loss: 2.661307]\n",
      "epoch:37 step:29660 [D loss: 0.281267, acc.: 86.72%] [G loss: 3.372841]\n",
      "epoch:37 step:29661 [D loss: 0.316404, acc.: 83.59%] [G loss: 3.887057]\n",
      "epoch:37 step:29662 [D loss: 0.409155, acc.: 79.69%] [G loss: 5.851409]\n",
      "epoch:37 step:29663 [D loss: 0.434467, acc.: 79.69%] [G loss: 4.204826]\n",
      "epoch:37 step:29664 [D loss: 0.266901, acc.: 85.94%] [G loss: 5.092875]\n",
      "epoch:37 step:29665 [D loss: 0.433916, acc.: 78.91%] [G loss: 4.720228]\n",
      "epoch:37 step:29666 [D loss: 0.231720, acc.: 88.28%] [G loss: 4.803258]\n",
      "epoch:37 step:29667 [D loss: 0.311134, acc.: 85.94%] [G loss: 3.607572]\n",
      "epoch:37 step:29668 [D loss: 0.331640, acc.: 85.94%] [G loss: 3.413325]\n",
      "epoch:37 step:29669 [D loss: 0.381195, acc.: 78.91%] [G loss: 4.367014]\n",
      "epoch:37 step:29670 [D loss: 0.275876, acc.: 87.50%] [G loss: 3.162354]\n",
      "epoch:37 step:29671 [D loss: 0.300553, acc.: 90.62%] [G loss: 3.772109]\n",
      "epoch:37 step:29672 [D loss: 0.355002, acc.: 83.59%] [G loss: 3.494352]\n",
      "epoch:37 step:29673 [D loss: 0.199551, acc.: 92.97%] [G loss: 3.246967]\n",
      "epoch:37 step:29674 [D loss: 0.287148, acc.: 83.59%] [G loss: 3.785910]\n",
      "epoch:37 step:29675 [D loss: 0.214003, acc.: 88.28%] [G loss: 4.006671]\n",
      "epoch:37 step:29676 [D loss: 0.316879, acc.: 82.81%] [G loss: 3.772963]\n",
      "epoch:37 step:29677 [D loss: 0.231379, acc.: 87.50%] [G loss: 4.294528]\n",
      "epoch:37 step:29678 [D loss: 0.255724, acc.: 89.84%] [G loss: 4.378595]\n",
      "epoch:38 step:29679 [D loss: 0.289159, acc.: 85.94%] [G loss: 5.533689]\n",
      "epoch:38 step:29680 [D loss: 0.282079, acc.: 89.84%] [G loss: 4.867534]\n",
      "epoch:38 step:29681 [D loss: 0.367275, acc.: 82.03%] [G loss: 3.157138]\n",
      "epoch:38 step:29682 [D loss: 0.362659, acc.: 82.81%] [G loss: 3.453765]\n",
      "epoch:38 step:29683 [D loss: 0.382324, acc.: 84.38%] [G loss: 3.517098]\n",
      "epoch:38 step:29684 [D loss: 0.275962, acc.: 89.06%] [G loss: 3.117858]\n",
      "epoch:38 step:29685 [D loss: 0.403547, acc.: 79.69%] [G loss: 3.165218]\n",
      "epoch:38 step:29686 [D loss: 0.350286, acc.: 83.59%] [G loss: 3.538383]\n",
      "epoch:38 step:29687 [D loss: 0.348048, acc.: 83.59%] [G loss: 3.572639]\n",
      "epoch:38 step:29688 [D loss: 0.328280, acc.: 83.59%] [G loss: 4.036279]\n",
      "epoch:38 step:29689 [D loss: 0.384622, acc.: 78.91%] [G loss: 2.949231]\n",
      "epoch:38 step:29690 [D loss: 0.173549, acc.: 92.97%] [G loss: 3.226739]\n",
      "epoch:38 step:29691 [D loss: 0.435872, acc.: 84.38%] [G loss: 8.500558]\n",
      "epoch:38 step:29692 [D loss: 0.962875, acc.: 65.62%] [G loss: 7.309952]\n",
      "epoch:38 step:29693 [D loss: 1.598694, acc.: 63.28%] [G loss: 5.097280]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:38 step:29694 [D loss: 0.670217, acc.: 71.09%] [G loss: 3.808507]\n",
      "epoch:38 step:29695 [D loss: 0.313971, acc.: 87.50%] [G loss: 3.987408]\n",
      "epoch:38 step:29696 [D loss: 0.469736, acc.: 78.91%] [G loss: 3.682339]\n",
      "epoch:38 step:29697 [D loss: 0.271728, acc.: 87.50%] [G loss: 4.037555]\n",
      "epoch:38 step:29698 [D loss: 0.397611, acc.: 78.12%] [G loss: 3.034527]\n",
      "epoch:38 step:29699 [D loss: 0.217779, acc.: 89.84%] [G loss: 3.999513]\n",
      "epoch:38 step:29700 [D loss: 0.387611, acc.: 87.50%] [G loss: 2.186515]\n",
      "epoch:38 step:29701 [D loss: 0.321220, acc.: 85.94%] [G loss: 3.151645]\n",
      "epoch:38 step:29702 [D loss: 0.421485, acc.: 81.25%] [G loss: 2.876789]\n",
      "epoch:38 step:29703 [D loss: 0.426332, acc.: 78.91%] [G loss: 2.504343]\n",
      "epoch:38 step:29704 [D loss: 0.359386, acc.: 87.50%] [G loss: 3.225929]\n",
      "epoch:38 step:29705 [D loss: 0.346535, acc.: 85.94%] [G loss: 4.240488]\n",
      "epoch:38 step:29706 [D loss: 0.362736, acc.: 85.16%] [G loss: 5.164465]\n",
      "epoch:38 step:29707 [D loss: 0.260470, acc.: 88.28%] [G loss: 5.369919]\n",
      "epoch:38 step:29708 [D loss: 0.252191, acc.: 89.84%] [G loss: 4.000206]\n",
      "epoch:38 step:29709 [D loss: 0.453336, acc.: 83.59%] [G loss: 3.041213]\n",
      "epoch:38 step:29710 [D loss: 0.312386, acc.: 89.84%] [G loss: 3.886697]\n",
      "epoch:38 step:29711 [D loss: 0.305246, acc.: 86.72%] [G loss: 3.214661]\n",
      "epoch:38 step:29712 [D loss: 0.428181, acc.: 78.91%] [G loss: 3.196640]\n",
      "epoch:38 step:29713 [D loss: 0.326702, acc.: 86.72%] [G loss: 3.126362]\n",
      "epoch:38 step:29714 [D loss: 0.330500, acc.: 81.25%] [G loss: 2.870663]\n",
      "epoch:38 step:29715 [D loss: 0.262610, acc.: 88.28%] [G loss: 2.988210]\n",
      "epoch:38 step:29716 [D loss: 0.349903, acc.: 85.16%] [G loss: 2.383705]\n",
      "epoch:38 step:29717 [D loss: 0.264697, acc.: 88.28%] [G loss: 2.753504]\n",
      "epoch:38 step:29718 [D loss: 0.272338, acc.: 88.28%] [G loss: 2.770552]\n",
      "epoch:38 step:29719 [D loss: 0.388679, acc.: 81.25%] [G loss: 4.817629]\n",
      "epoch:38 step:29720 [D loss: 0.310241, acc.: 85.94%] [G loss: 5.073327]\n",
      "epoch:38 step:29721 [D loss: 0.422508, acc.: 79.69%] [G loss: 4.015637]\n",
      "epoch:38 step:29722 [D loss: 0.266253, acc.: 88.28%] [G loss: 3.601689]\n",
      "epoch:38 step:29723 [D loss: 0.402936, acc.: 77.34%] [G loss: 2.646252]\n",
      "epoch:38 step:29724 [D loss: 0.414640, acc.: 76.56%] [G loss: 3.775781]\n",
      "epoch:38 step:29725 [D loss: 0.389338, acc.: 82.81%] [G loss: 3.111488]\n",
      "epoch:38 step:29726 [D loss: 0.272616, acc.: 90.62%] [G loss: 2.899853]\n",
      "epoch:38 step:29727 [D loss: 0.258186, acc.: 89.06%] [G loss: 2.793583]\n",
      "epoch:38 step:29728 [D loss: 0.352731, acc.: 83.59%] [G loss: 2.387096]\n",
      "epoch:38 step:29729 [D loss: 0.272098, acc.: 89.06%] [G loss: 2.375994]\n",
      "epoch:38 step:29730 [D loss: 0.365104, acc.: 85.16%] [G loss: 2.587198]\n",
      "epoch:38 step:29731 [D loss: 0.331767, acc.: 87.50%] [G loss: 2.642238]\n",
      "epoch:38 step:29732 [D loss: 0.374062, acc.: 85.94%] [G loss: 2.978215]\n",
      "epoch:38 step:29733 [D loss: 0.331960, acc.: 82.03%] [G loss: 2.391180]\n",
      "epoch:38 step:29734 [D loss: 0.352540, acc.: 84.38%] [G loss: 2.767712]\n",
      "epoch:38 step:29735 [D loss: 0.284304, acc.: 86.72%] [G loss: 3.477922]\n",
      "epoch:38 step:29736 [D loss: 0.327381, acc.: 84.38%] [G loss: 3.042791]\n",
      "epoch:38 step:29737 [D loss: 0.279322, acc.: 88.28%] [G loss: 2.839777]\n",
      "epoch:38 step:29738 [D loss: 0.257851, acc.: 89.84%] [G loss: 3.144526]\n",
      "epoch:38 step:29739 [D loss: 0.237046, acc.: 90.62%] [G loss: 3.643718]\n",
      "epoch:38 step:29740 [D loss: 0.249265, acc.: 90.62%] [G loss: 3.171329]\n",
      "epoch:38 step:29741 [D loss: 0.230882, acc.: 90.62%] [G loss: 3.656381]\n",
      "epoch:38 step:29742 [D loss: 0.453232, acc.: 76.56%] [G loss: 2.305799]\n",
      "epoch:38 step:29743 [D loss: 0.359991, acc.: 83.59%] [G loss: 3.339948]\n",
      "epoch:38 step:29744 [D loss: 0.404993, acc.: 77.34%] [G loss: 2.705487]\n",
      "epoch:38 step:29745 [D loss: 0.404002, acc.: 82.81%] [G loss: 3.107385]\n",
      "epoch:38 step:29746 [D loss: 0.399806, acc.: 82.03%] [G loss: 2.824028]\n",
      "epoch:38 step:29747 [D loss: 0.252243, acc.: 89.06%] [G loss: 2.723972]\n",
      "epoch:38 step:29748 [D loss: 0.391204, acc.: 79.69%] [G loss: 2.473477]\n",
      "epoch:38 step:29749 [D loss: 0.308071, acc.: 93.75%] [G loss: 2.294783]\n",
      "epoch:38 step:29750 [D loss: 0.253450, acc.: 89.84%] [G loss: 2.488076]\n",
      "epoch:38 step:29751 [D loss: 0.304289, acc.: 84.38%] [G loss: 3.582154]\n",
      "epoch:38 step:29752 [D loss: 0.304277, acc.: 86.72%] [G loss: 3.084866]\n",
      "epoch:38 step:29753 [D loss: 0.313339, acc.: 88.28%] [G loss: 3.859577]\n",
      "epoch:38 step:29754 [D loss: 0.292705, acc.: 87.50%] [G loss: 2.488925]\n",
      "epoch:38 step:29755 [D loss: 0.429122, acc.: 77.34%] [G loss: 2.427622]\n",
      "epoch:38 step:29756 [D loss: 0.378020, acc.: 78.91%] [G loss: 3.919602]\n",
      "epoch:38 step:29757 [D loss: 0.347050, acc.: 85.94%] [G loss: 4.191360]\n",
      "epoch:38 step:29758 [D loss: 0.337979, acc.: 84.38%] [G loss: 3.366573]\n",
      "epoch:38 step:29759 [D loss: 0.398294, acc.: 82.81%] [G loss: 3.772086]\n",
      "epoch:38 step:29760 [D loss: 0.273365, acc.: 87.50%] [G loss: 5.340607]\n",
      "epoch:38 step:29761 [D loss: 0.311695, acc.: 85.94%] [G loss: 2.885225]\n",
      "epoch:38 step:29762 [D loss: 0.253870, acc.: 90.62%] [G loss: 3.991339]\n",
      "epoch:38 step:29763 [D loss: 0.287348, acc.: 89.06%] [G loss: 3.317760]\n",
      "epoch:38 step:29764 [D loss: 0.218010, acc.: 92.97%] [G loss: 3.052892]\n",
      "epoch:38 step:29765 [D loss: 0.398114, acc.: 81.25%] [G loss: 3.478224]\n",
      "epoch:38 step:29766 [D loss: 0.288942, acc.: 86.72%] [G loss: 3.293507]\n",
      "epoch:38 step:29767 [D loss: 0.337546, acc.: 85.16%] [G loss: 2.512503]\n",
      "epoch:38 step:29768 [D loss: 0.440572, acc.: 82.03%] [G loss: 2.222652]\n",
      "epoch:38 step:29769 [D loss: 0.261145, acc.: 89.84%] [G loss: 2.945364]\n",
      "epoch:38 step:29770 [D loss: 0.360418, acc.: 83.59%] [G loss: 3.416447]\n",
      "epoch:38 step:29771 [D loss: 0.352710, acc.: 83.59%] [G loss: 2.942140]\n",
      "epoch:38 step:29772 [D loss: 0.267889, acc.: 86.72%] [G loss: 4.548880]\n",
      "epoch:38 step:29773 [D loss: 0.449161, acc.: 82.03%] [G loss: 4.352395]\n",
      "epoch:38 step:29774 [D loss: 0.480777, acc.: 80.47%] [G loss: 5.987119]\n",
      "epoch:38 step:29775 [D loss: 0.428777, acc.: 82.03%] [G loss: 3.403469]\n",
      "epoch:38 step:29776 [D loss: 0.252337, acc.: 88.28%] [G loss: 4.314006]\n",
      "epoch:38 step:29777 [D loss: 0.387913, acc.: 80.47%] [G loss: 2.955534]\n",
      "epoch:38 step:29778 [D loss: 0.209646, acc.: 91.41%] [G loss: 5.344095]\n",
      "epoch:38 step:29779 [D loss: 0.282941, acc.: 86.72%] [G loss: 4.446666]\n",
      "epoch:38 step:29780 [D loss: 0.295843, acc.: 84.38%] [G loss: 3.801888]\n",
      "epoch:38 step:29781 [D loss: 0.350105, acc.: 80.47%] [G loss: 2.937084]\n",
      "epoch:38 step:29782 [D loss: 0.270969, acc.: 86.72%] [G loss: 2.969918]\n",
      "epoch:38 step:29783 [D loss: 0.528638, acc.: 77.34%] [G loss: 3.392279]\n",
      "epoch:38 step:29784 [D loss: 0.432544, acc.: 80.47%] [G loss: 3.224561]\n",
      "epoch:38 step:29785 [D loss: 0.303965, acc.: 87.50%] [G loss: 3.826433]\n",
      "epoch:38 step:29786 [D loss: 0.249160, acc.: 91.41%] [G loss: 3.670887]\n",
      "epoch:38 step:29787 [D loss: 0.398892, acc.: 81.25%] [G loss: 3.042881]\n",
      "epoch:38 step:29788 [D loss: 0.319543, acc.: 88.28%] [G loss: 3.098876]\n",
      "epoch:38 step:29789 [D loss: 0.368948, acc.: 82.03%] [G loss: 3.695354]\n",
      "epoch:38 step:29790 [D loss: 0.254656, acc.: 88.28%] [G loss: 3.947787]\n",
      "epoch:38 step:29791 [D loss: 0.374179, acc.: 82.03%] [G loss: 3.202084]\n",
      "epoch:38 step:29792 [D loss: 0.382477, acc.: 82.03%] [G loss: 2.567059]\n",
      "epoch:38 step:29793 [D loss: 0.257355, acc.: 88.28%] [G loss: 3.675728]\n",
      "epoch:38 step:29794 [D loss: 0.286779, acc.: 89.84%] [G loss: 2.492149]\n",
      "epoch:38 step:29795 [D loss: 0.267715, acc.: 91.41%] [G loss: 2.769879]\n",
      "epoch:38 step:29796 [D loss: 0.303352, acc.: 85.94%] [G loss: 3.147369]\n",
      "epoch:38 step:29797 [D loss: 0.398273, acc.: 80.47%] [G loss: 2.931715]\n",
      "epoch:38 step:29798 [D loss: 0.324323, acc.: 86.72%] [G loss: 4.260074]\n",
      "epoch:38 step:29799 [D loss: 0.314570, acc.: 82.81%] [G loss: 4.002170]\n",
      "epoch:38 step:29800 [D loss: 0.303475, acc.: 88.28%] [G loss: 3.968059]\n",
      "##############\n",
      "[0.8671885  0.84700662 0.8271725  0.81647452 0.78837311 0.81138803\n",
      " 0.88323461 0.83554772 0.83274107 0.83285193]\n",
      "##########\n",
      "epoch:38 step:29801 [D loss: 0.277184, acc.: 89.84%] [G loss: 3.564310]\n",
      "epoch:38 step:29802 [D loss: 0.368343, acc.: 86.72%] [G loss: 3.646899]\n",
      "epoch:38 step:29803 [D loss: 0.401376, acc.: 83.59%] [G loss: 2.756193]\n",
      "epoch:38 step:29804 [D loss: 0.370146, acc.: 86.72%] [G loss: 3.225449]\n",
      "epoch:38 step:29805 [D loss: 0.370682, acc.: 83.59%] [G loss: 2.199661]\n",
      "epoch:38 step:29806 [D loss: 0.330760, acc.: 83.59%] [G loss: 3.050635]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:38 step:29807 [D loss: 0.337465, acc.: 84.38%] [G loss: 3.202313]\n",
      "epoch:38 step:29808 [D loss: 0.453101, acc.: 79.69%] [G loss: 3.191992]\n",
      "epoch:38 step:29809 [D loss: 0.317703, acc.: 85.94%] [G loss: 2.928251]\n",
      "epoch:38 step:29810 [D loss: 0.339719, acc.: 82.03%] [G loss: 3.220989]\n",
      "epoch:38 step:29811 [D loss: 0.329114, acc.: 85.94%] [G loss: 3.190741]\n",
      "epoch:38 step:29812 [D loss: 0.253958, acc.: 90.62%] [G loss: 4.002693]\n",
      "epoch:38 step:29813 [D loss: 0.171421, acc.: 92.19%] [G loss: 4.249538]\n",
      "epoch:38 step:29814 [D loss: 0.253943, acc.: 90.62%] [G loss: 2.663240]\n",
      "epoch:38 step:29815 [D loss: 0.200149, acc.: 92.97%] [G loss: 3.620342]\n",
      "epoch:38 step:29816 [D loss: 0.247279, acc.: 89.06%] [G loss: 3.501720]\n",
      "epoch:38 step:29817 [D loss: 0.323391, acc.: 90.62%] [G loss: 3.352669]\n",
      "epoch:38 step:29818 [D loss: 0.356238, acc.: 87.50%] [G loss: 4.446177]\n",
      "epoch:38 step:29819 [D loss: 0.368586, acc.: 81.25%] [G loss: 4.206425]\n",
      "epoch:38 step:29820 [D loss: 0.332392, acc.: 84.38%] [G loss: 3.859223]\n",
      "epoch:38 step:29821 [D loss: 0.294529, acc.: 85.16%] [G loss: 4.969513]\n",
      "epoch:38 step:29822 [D loss: 0.404602, acc.: 80.47%] [G loss: 3.665312]\n",
      "epoch:38 step:29823 [D loss: 0.341501, acc.: 84.38%] [G loss: 3.011599]\n",
      "epoch:38 step:29824 [D loss: 0.261186, acc.: 89.06%] [G loss: 3.592669]\n",
      "epoch:38 step:29825 [D loss: 0.278112, acc.: 92.19%] [G loss: 3.149640]\n",
      "epoch:38 step:29826 [D loss: 0.331250, acc.: 84.38%] [G loss: 3.766483]\n",
      "epoch:38 step:29827 [D loss: 0.335323, acc.: 85.16%] [G loss: 3.235389]\n",
      "epoch:38 step:29828 [D loss: 0.258644, acc.: 90.62%] [G loss: 3.485940]\n",
      "epoch:38 step:29829 [D loss: 0.278933, acc.: 85.94%] [G loss: 3.261581]\n",
      "epoch:38 step:29830 [D loss: 0.263357, acc.: 86.72%] [G loss: 3.440472]\n",
      "epoch:38 step:29831 [D loss: 0.280534, acc.: 85.94%] [G loss: 2.547046]\n",
      "epoch:38 step:29832 [D loss: 0.327486, acc.: 83.59%] [G loss: 2.995672]\n",
      "epoch:38 step:29833 [D loss: 0.364990, acc.: 85.16%] [G loss: 2.647364]\n",
      "epoch:38 step:29834 [D loss: 0.283207, acc.: 85.94%] [G loss: 3.009110]\n",
      "epoch:38 step:29835 [D loss: 0.402501, acc.: 78.12%] [G loss: 2.928476]\n",
      "epoch:38 step:29836 [D loss: 0.305340, acc.: 85.16%] [G loss: 3.272951]\n",
      "epoch:38 step:29837 [D loss: 0.423310, acc.: 84.38%] [G loss: 6.201863]\n",
      "epoch:38 step:29838 [D loss: 0.847533, acc.: 72.66%] [G loss: 9.044434]\n",
      "epoch:38 step:29839 [D loss: 1.863764, acc.: 57.81%] [G loss: 4.815556]\n",
      "epoch:38 step:29840 [D loss: 0.313486, acc.: 85.94%] [G loss: 4.550461]\n",
      "epoch:38 step:29841 [D loss: 0.699078, acc.: 78.12%] [G loss: 3.765906]\n",
      "epoch:38 step:29842 [D loss: 0.265450, acc.: 89.06%] [G loss: 4.427437]\n",
      "epoch:38 step:29843 [D loss: 0.680275, acc.: 75.78%] [G loss: 2.464527]\n",
      "epoch:38 step:29844 [D loss: 0.329011, acc.: 84.38%] [G loss: 3.360712]\n",
      "epoch:38 step:29845 [D loss: 0.568667, acc.: 77.34%] [G loss: 3.737927]\n",
      "epoch:38 step:29846 [D loss: 0.402797, acc.: 79.69%] [G loss: 3.047136]\n",
      "epoch:38 step:29847 [D loss: 0.352592, acc.: 85.16%] [G loss: 2.980384]\n",
      "epoch:38 step:29848 [D loss: 0.356584, acc.: 87.50%] [G loss: 2.744889]\n",
      "epoch:38 step:29849 [D loss: 0.478570, acc.: 81.25%] [G loss: 2.695957]\n",
      "epoch:38 step:29850 [D loss: 0.352719, acc.: 83.59%] [G loss: 3.532597]\n",
      "epoch:38 step:29851 [D loss: 0.410299, acc.: 79.69%] [G loss: 3.439494]\n",
      "epoch:38 step:29852 [D loss: 0.271419, acc.: 88.28%] [G loss: 3.581209]\n",
      "epoch:38 step:29853 [D loss: 0.244767, acc.: 89.84%] [G loss: 3.223859]\n",
      "epoch:38 step:29854 [D loss: 0.314319, acc.: 85.94%] [G loss: 3.216451]\n",
      "epoch:38 step:29855 [D loss: 0.339916, acc.: 83.59%] [G loss: 2.831314]\n",
      "epoch:38 step:29856 [D loss: 0.358414, acc.: 82.81%] [G loss: 3.044777]\n",
      "epoch:38 step:29857 [D loss: 0.291198, acc.: 85.94%] [G loss: 3.165103]\n",
      "epoch:38 step:29858 [D loss: 0.354390, acc.: 85.16%] [G loss: 3.385475]\n",
      "epoch:38 step:29859 [D loss: 0.166932, acc.: 95.31%] [G loss: 3.419561]\n",
      "epoch:38 step:29860 [D loss: 0.313473, acc.: 85.94%] [G loss: 3.054639]\n",
      "epoch:38 step:29861 [D loss: 0.307175, acc.: 83.59%] [G loss: 3.005872]\n",
      "epoch:38 step:29862 [D loss: 0.401203, acc.: 80.47%] [G loss: 3.034111]\n",
      "epoch:38 step:29863 [D loss: 0.330778, acc.: 83.59%] [G loss: 3.344098]\n",
      "epoch:38 step:29864 [D loss: 0.236289, acc.: 91.41%] [G loss: 3.390915]\n",
      "epoch:38 step:29865 [D loss: 0.296849, acc.: 87.50%] [G loss: 4.170498]\n",
      "epoch:38 step:29866 [D loss: 0.270938, acc.: 86.72%] [G loss: 2.901659]\n",
      "epoch:38 step:29867 [D loss: 0.341222, acc.: 82.81%] [G loss: 5.232292]\n",
      "epoch:38 step:29868 [D loss: 0.337541, acc.: 85.16%] [G loss: 2.514129]\n",
      "epoch:38 step:29869 [D loss: 0.359620, acc.: 83.59%] [G loss: 4.273600]\n",
      "epoch:38 step:29870 [D loss: 0.294314, acc.: 86.72%] [G loss: 2.655652]\n",
      "epoch:38 step:29871 [D loss: 0.291419, acc.: 89.84%] [G loss: 3.209218]\n",
      "epoch:38 step:29872 [D loss: 0.260239, acc.: 89.84%] [G loss: 2.861929]\n",
      "epoch:38 step:29873 [D loss: 0.409015, acc.: 82.03%] [G loss: 3.188457]\n",
      "epoch:38 step:29874 [D loss: 0.240211, acc.: 92.19%] [G loss: 2.921937]\n",
      "epoch:38 step:29875 [D loss: 0.376282, acc.: 85.94%] [G loss: 4.408560]\n",
      "epoch:38 step:29876 [D loss: 0.298255, acc.: 87.50%] [G loss: 3.221636]\n",
      "epoch:38 step:29877 [D loss: 0.291175, acc.: 85.94%] [G loss: 4.137659]\n",
      "epoch:38 step:29878 [D loss: 0.247113, acc.: 89.06%] [G loss: 2.858864]\n",
      "epoch:38 step:29879 [D loss: 0.346393, acc.: 85.16%] [G loss: 3.327852]\n",
      "epoch:38 step:29880 [D loss: 0.200759, acc.: 90.62%] [G loss: 3.408993]\n",
      "epoch:38 step:29881 [D loss: 0.272891, acc.: 89.84%] [G loss: 3.142600]\n",
      "epoch:38 step:29882 [D loss: 0.199078, acc.: 92.97%] [G loss: 3.249876]\n",
      "epoch:38 step:29883 [D loss: 0.375001, acc.: 82.81%] [G loss: 3.211103]\n",
      "epoch:38 step:29884 [D loss: 0.352618, acc.: 82.81%] [G loss: 3.955486]\n",
      "epoch:38 step:29885 [D loss: 0.413924, acc.: 80.47%] [G loss: 3.947384]\n",
      "epoch:38 step:29886 [D loss: 0.297696, acc.: 85.94%] [G loss: 2.780674]\n",
      "epoch:38 step:29887 [D loss: 0.380397, acc.: 83.59%] [G loss: 2.935118]\n",
      "epoch:38 step:29888 [D loss: 0.296638, acc.: 86.72%] [G loss: 3.121201]\n",
      "epoch:38 step:29889 [D loss: 0.389404, acc.: 83.59%] [G loss: 3.271002]\n",
      "epoch:38 step:29890 [D loss: 0.397022, acc.: 81.25%] [G loss: 4.736947]\n",
      "epoch:38 step:29891 [D loss: 0.381288, acc.: 82.81%] [G loss: 3.359605]\n",
      "epoch:38 step:29892 [D loss: 0.530620, acc.: 72.66%] [G loss: 3.365649]\n",
      "epoch:38 step:29893 [D loss: 0.309135, acc.: 85.94%] [G loss: 3.033528]\n",
      "epoch:38 step:29894 [D loss: 0.384841, acc.: 82.03%] [G loss: 4.786257]\n",
      "epoch:38 step:29895 [D loss: 0.361263, acc.: 85.94%] [G loss: 4.613124]\n",
      "epoch:38 step:29896 [D loss: 0.262563, acc.: 90.62%] [G loss: 3.381847]\n",
      "epoch:38 step:29897 [D loss: 0.188464, acc.: 93.75%] [G loss: 4.758583]\n",
      "epoch:38 step:29898 [D loss: 0.220685, acc.: 89.84%] [G loss: 3.914239]\n",
      "epoch:38 step:29899 [D loss: 0.280747, acc.: 85.94%] [G loss: 3.715412]\n",
      "epoch:38 step:29900 [D loss: 0.286995, acc.: 85.16%] [G loss: 4.335778]\n",
      "epoch:38 step:29901 [D loss: 0.260351, acc.: 85.94%] [G loss: 6.291216]\n",
      "epoch:38 step:29902 [D loss: 0.290681, acc.: 89.84%] [G loss: 5.382034]\n",
      "epoch:38 step:29903 [D loss: 0.254743, acc.: 89.06%] [G loss: 4.984788]\n",
      "epoch:38 step:29904 [D loss: 0.229547, acc.: 91.41%] [G loss: 5.550344]\n",
      "epoch:38 step:29905 [D loss: 0.280711, acc.: 88.28%] [G loss: 6.609749]\n",
      "epoch:38 step:29906 [D loss: 0.287364, acc.: 85.94%] [G loss: 3.960291]\n",
      "epoch:38 step:29907 [D loss: 0.309471, acc.: 83.59%] [G loss: 4.891237]\n",
      "epoch:38 step:29908 [D loss: 0.266643, acc.: 86.72%] [G loss: 3.586845]\n",
      "epoch:38 step:29909 [D loss: 0.232822, acc.: 92.97%] [G loss: 3.298105]\n",
      "epoch:38 step:29910 [D loss: 0.246984, acc.: 91.41%] [G loss: 3.519083]\n",
      "epoch:38 step:29911 [D loss: 0.349524, acc.: 82.81%] [G loss: 4.021681]\n",
      "epoch:38 step:29912 [D loss: 0.293248, acc.: 89.06%] [G loss: 3.417257]\n",
      "epoch:38 step:29913 [D loss: 0.367725, acc.: 84.38%] [G loss: 3.250399]\n",
      "epoch:38 step:29914 [D loss: 0.459219, acc.: 82.03%] [G loss: 4.112676]\n",
      "epoch:38 step:29915 [D loss: 0.364122, acc.: 85.16%] [G loss: 2.747852]\n",
      "epoch:38 step:29916 [D loss: 0.469008, acc.: 78.12%] [G loss: 3.525959]\n",
      "epoch:38 step:29917 [D loss: 0.267715, acc.: 87.50%] [G loss: 2.744345]\n",
      "epoch:38 step:29918 [D loss: 0.382930, acc.: 85.16%] [G loss: 2.680892]\n",
      "epoch:38 step:29919 [D loss: 0.456382, acc.: 81.25%] [G loss: 2.820511]\n",
      "epoch:38 step:29920 [D loss: 0.270504, acc.: 88.28%] [G loss: 2.866464]\n",
      "epoch:38 step:29921 [D loss: 0.393818, acc.: 80.47%] [G loss: 2.701564]\n",
      "epoch:38 step:29922 [D loss: 0.382140, acc.: 81.25%] [G loss: 2.965317]\n",
      "epoch:38 step:29923 [D loss: 0.384270, acc.: 84.38%] [G loss: 2.725552]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:38 step:29924 [D loss: 0.400769, acc.: 78.91%] [G loss: 3.180217]\n",
      "epoch:38 step:29925 [D loss: 0.440061, acc.: 80.47%] [G loss: 3.027759]\n",
      "epoch:38 step:29926 [D loss: 0.297537, acc.: 86.72%] [G loss: 3.494568]\n",
      "epoch:38 step:29927 [D loss: 0.373575, acc.: 82.03%] [G loss: 4.978841]\n",
      "epoch:38 step:29928 [D loss: 0.466976, acc.: 81.25%] [G loss: 3.960505]\n",
      "epoch:38 step:29929 [D loss: 0.499724, acc.: 76.56%] [G loss: 3.264555]\n",
      "epoch:38 step:29930 [D loss: 0.302387, acc.: 85.94%] [G loss: 3.409661]\n",
      "epoch:38 step:29931 [D loss: 0.272642, acc.: 88.28%] [G loss: 2.845737]\n",
      "epoch:38 step:29932 [D loss: 0.272327, acc.: 85.94%] [G loss: 3.511494]\n",
      "epoch:38 step:29933 [D loss: 0.360975, acc.: 82.81%] [G loss: 3.002289]\n",
      "epoch:38 step:29934 [D loss: 0.285788, acc.: 87.50%] [G loss: 3.116496]\n",
      "epoch:38 step:29935 [D loss: 0.348731, acc.: 87.50%] [G loss: 3.111762]\n",
      "epoch:38 step:29936 [D loss: 0.356343, acc.: 83.59%] [G loss: 2.956291]\n",
      "epoch:38 step:29937 [D loss: 0.394291, acc.: 81.25%] [G loss: 2.403459]\n",
      "epoch:38 step:29938 [D loss: 0.246402, acc.: 86.72%] [G loss: 3.133913]\n",
      "epoch:38 step:29939 [D loss: 0.390249, acc.: 83.59%] [G loss: 4.903200]\n",
      "epoch:38 step:29940 [D loss: 0.330768, acc.: 83.59%] [G loss: 3.168819]\n",
      "epoch:38 step:29941 [D loss: 0.374596, acc.: 84.38%] [G loss: 3.074906]\n",
      "epoch:38 step:29942 [D loss: 0.363801, acc.: 85.16%] [G loss: 3.780535]\n",
      "epoch:38 step:29943 [D loss: 0.405202, acc.: 85.94%] [G loss: 4.806638]\n",
      "epoch:38 step:29944 [D loss: 0.472379, acc.: 79.69%] [G loss: 3.723929]\n",
      "epoch:38 step:29945 [D loss: 0.565821, acc.: 72.66%] [G loss: 4.169528]\n",
      "epoch:38 step:29946 [D loss: 0.530641, acc.: 73.44%] [G loss: 3.238439]\n",
      "epoch:38 step:29947 [D loss: 0.362309, acc.: 87.50%] [G loss: 3.469322]\n",
      "epoch:38 step:29948 [D loss: 0.398747, acc.: 78.91%] [G loss: 3.892488]\n",
      "epoch:38 step:29949 [D loss: 0.180399, acc.: 92.97%] [G loss: 3.779399]\n",
      "epoch:38 step:29950 [D loss: 0.316775, acc.: 87.50%] [G loss: 4.217865]\n",
      "epoch:38 step:29951 [D loss: 0.206256, acc.: 90.62%] [G loss: 2.576648]\n",
      "epoch:38 step:29952 [D loss: 0.284608, acc.: 88.28%] [G loss: 3.688262]\n",
      "epoch:38 step:29953 [D loss: 0.482589, acc.: 76.56%] [G loss: 3.307018]\n",
      "epoch:38 step:29954 [D loss: 0.352968, acc.: 85.94%] [G loss: 3.554523]\n",
      "epoch:38 step:29955 [D loss: 0.355595, acc.: 84.38%] [G loss: 3.289680]\n",
      "epoch:38 step:29956 [D loss: 0.257351, acc.: 90.62%] [G loss: 3.208741]\n",
      "epoch:38 step:29957 [D loss: 0.327542, acc.: 85.94%] [G loss: 2.654975]\n",
      "epoch:38 step:29958 [D loss: 0.250019, acc.: 90.62%] [G loss: 2.587489]\n",
      "epoch:38 step:29959 [D loss: 0.255081, acc.: 89.84%] [G loss: 3.196407]\n",
      "epoch:38 step:29960 [D loss: 0.302657, acc.: 88.28%] [G loss: 2.991298]\n",
      "epoch:38 step:29961 [D loss: 0.344394, acc.: 84.38%] [G loss: 3.670131]\n",
      "epoch:38 step:29962 [D loss: 0.332443, acc.: 85.94%] [G loss: 3.165644]\n",
      "epoch:38 step:29963 [D loss: 0.268223, acc.: 87.50%] [G loss: 4.570035]\n",
      "epoch:38 step:29964 [D loss: 0.381007, acc.: 83.59%] [G loss: 2.553655]\n",
      "epoch:38 step:29965 [D loss: 0.194633, acc.: 91.41%] [G loss: 3.278570]\n",
      "epoch:38 step:29966 [D loss: 0.285212, acc.: 85.94%] [G loss: 3.545958]\n",
      "epoch:38 step:29967 [D loss: 0.316776, acc.: 89.06%] [G loss: 3.016682]\n",
      "epoch:38 step:29968 [D loss: 0.298011, acc.: 89.06%] [G loss: 4.581054]\n",
      "epoch:38 step:29969 [D loss: 0.277491, acc.: 85.16%] [G loss: 3.679468]\n",
      "epoch:38 step:29970 [D loss: 0.313867, acc.: 86.72%] [G loss: 3.833578]\n",
      "epoch:38 step:29971 [D loss: 0.431915, acc.: 80.47%] [G loss: 2.840590]\n",
      "epoch:38 step:29972 [D loss: 0.394505, acc.: 82.03%] [G loss: 3.603704]\n",
      "epoch:38 step:29973 [D loss: 0.355244, acc.: 84.38%] [G loss: 3.777034]\n",
      "epoch:38 step:29974 [D loss: 0.361230, acc.: 84.38%] [G loss: 3.456918]\n",
      "epoch:38 step:29975 [D loss: 0.253673, acc.: 89.06%] [G loss: 3.608370]\n",
      "epoch:38 step:29976 [D loss: 0.421940, acc.: 81.25%] [G loss: 3.174692]\n",
      "epoch:38 step:29977 [D loss: 0.331067, acc.: 85.16%] [G loss: 3.143285]\n",
      "epoch:38 step:29978 [D loss: 0.431326, acc.: 76.56%] [G loss: 2.704957]\n",
      "epoch:38 step:29979 [D loss: 0.386660, acc.: 80.47%] [G loss: 5.773535]\n",
      "epoch:38 step:29980 [D loss: 0.404200, acc.: 83.59%] [G loss: 5.774081]\n",
      "epoch:38 step:29981 [D loss: 0.342210, acc.: 86.72%] [G loss: 3.812121]\n",
      "epoch:38 step:29982 [D loss: 0.212438, acc.: 92.19%] [G loss: 4.121173]\n",
      "epoch:38 step:29983 [D loss: 0.306531, acc.: 85.16%] [G loss: 3.549407]\n",
      "epoch:38 step:29984 [D loss: 0.266751, acc.: 89.06%] [G loss: 3.667367]\n",
      "epoch:38 step:29985 [D loss: 0.212402, acc.: 91.41%] [G loss: 3.177970]\n",
      "epoch:38 step:29986 [D loss: 0.270791, acc.: 85.94%] [G loss: 3.039343]\n",
      "epoch:38 step:29987 [D loss: 0.288242, acc.: 89.06%] [G loss: 2.899788]\n",
      "epoch:38 step:29988 [D loss: 0.275515, acc.: 88.28%] [G loss: 3.808206]\n",
      "epoch:38 step:29989 [D loss: 0.308001, acc.: 87.50%] [G loss: 2.857038]\n",
      "epoch:38 step:29990 [D loss: 0.318034, acc.: 85.16%] [G loss: 4.432196]\n",
      "epoch:38 step:29991 [D loss: 0.392584, acc.: 81.25%] [G loss: 4.231674]\n",
      "epoch:38 step:29992 [D loss: 0.348552, acc.: 84.38%] [G loss: 3.332194]\n",
      "epoch:38 step:29993 [D loss: 0.500145, acc.: 77.34%] [G loss: 4.300778]\n",
      "epoch:38 step:29994 [D loss: 0.351707, acc.: 89.06%] [G loss: 4.123445]\n",
      "epoch:38 step:29995 [D loss: 0.560611, acc.: 72.66%] [G loss: 2.833709]\n",
      "epoch:38 step:29996 [D loss: 0.384512, acc.: 80.47%] [G loss: 2.628201]\n",
      "epoch:38 step:29997 [D loss: 0.285142, acc.: 87.50%] [G loss: 2.573407]\n",
      "epoch:38 step:29998 [D loss: 0.472789, acc.: 78.91%] [G loss: 3.690221]\n",
      "epoch:38 step:29999 [D loss: 0.421210, acc.: 78.12%] [G loss: 3.584772]\n",
      "epoch:38 step:30000 [D loss: 0.354457, acc.: 84.38%] [G loss: 4.259820]\n",
      "##############\n",
      "[0.86437062 0.86705747 0.82009224 0.81845004 0.79109216 0.82135915\n",
      " 0.86694543 0.81087333 0.80714419 0.8390282 ]\n",
      "##########\n",
      "epoch:38 step:30001 [D loss: 0.334783, acc.: 84.38%] [G loss: 4.768754]\n",
      "epoch:38 step:30002 [D loss: 0.447843, acc.: 76.56%] [G loss: 3.094513]\n",
      "epoch:38 step:30003 [D loss: 0.330486, acc.: 83.59%] [G loss: 2.739956]\n",
      "epoch:38 step:30004 [D loss: 0.298562, acc.: 85.16%] [G loss: 3.294582]\n",
      "epoch:38 step:30005 [D loss: 0.343349, acc.: 85.16%] [G loss: 3.056345]\n",
      "epoch:38 step:30006 [D loss: 0.251260, acc.: 89.84%] [G loss: 3.101803]\n",
      "epoch:38 step:30007 [D loss: 0.322387, acc.: 85.16%] [G loss: 3.306581]\n",
      "epoch:38 step:30008 [D loss: 0.423782, acc.: 82.03%] [G loss: 2.777890]\n",
      "epoch:38 step:30009 [D loss: 0.309695, acc.: 86.72%] [G loss: 3.968739]\n",
      "epoch:38 step:30010 [D loss: 0.372113, acc.: 85.94%] [G loss: 3.556897]\n",
      "epoch:38 step:30011 [D loss: 0.393034, acc.: 81.25%] [G loss: 3.807954]\n",
      "epoch:38 step:30012 [D loss: 0.350538, acc.: 85.94%] [G loss: 3.478108]\n",
      "epoch:38 step:30013 [D loss: 0.197859, acc.: 95.31%] [G loss: 5.134685]\n",
      "epoch:38 step:30014 [D loss: 0.315396, acc.: 87.50%] [G loss: 4.924050]\n",
      "epoch:38 step:30015 [D loss: 0.254576, acc.: 91.41%] [G loss: 2.405861]\n",
      "epoch:38 step:30016 [D loss: 0.353235, acc.: 81.25%] [G loss: 4.186810]\n",
      "epoch:38 step:30017 [D loss: 0.333910, acc.: 85.16%] [G loss: 2.898530]\n",
      "epoch:38 step:30018 [D loss: 0.242669, acc.: 89.06%] [G loss: 4.314415]\n",
      "epoch:38 step:30019 [D loss: 0.351088, acc.: 85.16%] [G loss: 3.483062]\n",
      "epoch:38 step:30020 [D loss: 0.334033, acc.: 85.94%] [G loss: 3.104455]\n",
      "epoch:38 step:30021 [D loss: 0.297179, acc.: 85.16%] [G loss: 3.042734]\n",
      "epoch:38 step:30022 [D loss: 0.249745, acc.: 89.84%] [G loss: 2.825506]\n",
      "epoch:38 step:30023 [D loss: 0.318616, acc.: 85.94%] [G loss: 3.252896]\n",
      "epoch:38 step:30024 [D loss: 0.438153, acc.: 78.91%] [G loss: 3.435081]\n",
      "epoch:38 step:30025 [D loss: 0.378302, acc.: 82.81%] [G loss: 3.239250]\n",
      "epoch:38 step:30026 [D loss: 0.321441, acc.: 84.38%] [G loss: 2.868924]\n",
      "epoch:38 step:30027 [D loss: 0.278413, acc.: 85.16%] [G loss: 3.778877]\n",
      "epoch:38 step:30028 [D loss: 0.346577, acc.: 82.81%] [G loss: 2.406138]\n",
      "epoch:38 step:30029 [D loss: 0.405244, acc.: 80.47%] [G loss: 4.290630]\n",
      "epoch:38 step:30030 [D loss: 0.396415, acc.: 80.47%] [G loss: 4.759815]\n",
      "epoch:38 step:30031 [D loss: 0.532263, acc.: 75.78%] [G loss: 4.322650]\n",
      "epoch:38 step:30032 [D loss: 0.404256, acc.: 85.16%] [G loss: 4.365271]\n",
      "epoch:38 step:30033 [D loss: 0.239212, acc.: 92.19%] [G loss: 4.580977]\n",
      "epoch:38 step:30034 [D loss: 0.362632, acc.: 83.59%] [G loss: 3.477009]\n",
      "epoch:38 step:30035 [D loss: 0.365340, acc.: 80.47%] [G loss: 2.900258]\n",
      "epoch:38 step:30036 [D loss: 0.293866, acc.: 89.84%] [G loss: 3.236188]\n",
      "epoch:38 step:30037 [D loss: 0.271322, acc.: 88.28%] [G loss: 3.236418]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:38 step:30038 [D loss: 0.422438, acc.: 80.47%] [G loss: 3.468389]\n",
      "epoch:38 step:30039 [D loss: 0.299742, acc.: 88.28%] [G loss: 2.959310]\n",
      "epoch:38 step:30040 [D loss: 0.326569, acc.: 86.72%] [G loss: 2.759501]\n",
      "epoch:38 step:30041 [D loss: 0.340808, acc.: 82.03%] [G loss: 4.367188]\n",
      "epoch:38 step:30042 [D loss: 0.338449, acc.: 85.16%] [G loss: 2.215878]\n",
      "epoch:38 step:30043 [D loss: 0.286643, acc.: 85.94%] [G loss: 2.991884]\n",
      "epoch:38 step:30044 [D loss: 0.333171, acc.: 87.50%] [G loss: 3.116121]\n",
      "epoch:38 step:30045 [D loss: 0.253296, acc.: 91.41%] [G loss: 3.282622]\n",
      "epoch:38 step:30046 [D loss: 0.456664, acc.: 79.69%] [G loss: 3.059694]\n",
      "epoch:38 step:30047 [D loss: 0.274698, acc.: 88.28%] [G loss: 3.710811]\n",
      "epoch:38 step:30048 [D loss: 0.306613, acc.: 89.84%] [G loss: 3.052302]\n",
      "epoch:38 step:30049 [D loss: 0.438282, acc.: 81.25%] [G loss: 4.071097]\n",
      "epoch:38 step:30050 [D loss: 0.505667, acc.: 77.34%] [G loss: 3.093007]\n",
      "epoch:38 step:30051 [D loss: 0.294290, acc.: 86.72%] [G loss: 3.186475]\n",
      "epoch:38 step:30052 [D loss: 0.323750, acc.: 82.81%] [G loss: 2.884182]\n",
      "epoch:38 step:30053 [D loss: 0.315334, acc.: 89.06%] [G loss: 2.726964]\n",
      "epoch:38 step:30054 [D loss: 0.227051, acc.: 94.53%] [G loss: 2.950367]\n",
      "epoch:38 step:30055 [D loss: 0.262830, acc.: 85.94%] [G loss: 2.969329]\n",
      "epoch:38 step:30056 [D loss: 0.348559, acc.: 83.59%] [G loss: 2.592877]\n",
      "epoch:38 step:30057 [D loss: 0.270379, acc.: 89.06%] [G loss: 3.802139]\n",
      "epoch:38 step:30058 [D loss: 0.409662, acc.: 82.81%] [G loss: 2.753012]\n",
      "epoch:38 step:30059 [D loss: 0.250450, acc.: 89.06%] [G loss: 3.222613]\n",
      "epoch:38 step:30060 [D loss: 0.364474, acc.: 78.91%] [G loss: 2.894537]\n",
      "epoch:38 step:30061 [D loss: 0.306468, acc.: 86.72%] [G loss: 3.075093]\n",
      "epoch:38 step:30062 [D loss: 0.263098, acc.: 89.06%] [G loss: 2.798235]\n",
      "epoch:38 step:30063 [D loss: 0.269422, acc.: 89.84%] [G loss: 3.142848]\n",
      "epoch:38 step:30064 [D loss: 0.290930, acc.: 86.72%] [G loss: 2.917405]\n",
      "epoch:38 step:30065 [D loss: 0.358077, acc.: 85.16%] [G loss: 2.190088]\n",
      "epoch:38 step:30066 [D loss: 0.368768, acc.: 81.25%] [G loss: 3.853575]\n",
      "epoch:38 step:30067 [D loss: 0.378300, acc.: 82.81%] [G loss: 3.338979]\n",
      "epoch:38 step:30068 [D loss: 0.349941, acc.: 85.16%] [G loss: 2.525457]\n",
      "epoch:38 step:30069 [D loss: 0.366649, acc.: 82.81%] [G loss: 3.313692]\n",
      "epoch:38 step:30070 [D loss: 0.504020, acc.: 78.12%] [G loss: 3.306166]\n",
      "epoch:38 step:30071 [D loss: 0.324801, acc.: 82.81%] [G loss: 5.362988]\n",
      "epoch:38 step:30072 [D loss: 0.249245, acc.: 88.28%] [G loss: 3.766434]\n",
      "epoch:38 step:30073 [D loss: 0.279192, acc.: 86.72%] [G loss: 5.548389]\n",
      "epoch:38 step:30074 [D loss: 0.289960, acc.: 86.72%] [G loss: 2.595519]\n",
      "epoch:38 step:30075 [D loss: 0.230017, acc.: 90.62%] [G loss: 3.565053]\n",
      "epoch:38 step:30076 [D loss: 0.216791, acc.: 89.06%] [G loss: 2.932044]\n",
      "epoch:38 step:30077 [D loss: 0.353081, acc.: 84.38%] [G loss: 3.868166]\n",
      "epoch:38 step:30078 [D loss: 0.371290, acc.: 85.94%] [G loss: 2.966266]\n",
      "epoch:38 step:30079 [D loss: 0.359452, acc.: 82.81%] [G loss: 2.583555]\n",
      "epoch:38 step:30080 [D loss: 0.303697, acc.: 83.59%] [G loss: 2.984635]\n",
      "epoch:38 step:30081 [D loss: 0.327435, acc.: 85.16%] [G loss: 3.432158]\n",
      "epoch:38 step:30082 [D loss: 0.300762, acc.: 84.38%] [G loss: 3.208865]\n",
      "epoch:38 step:30083 [D loss: 0.309599, acc.: 85.94%] [G loss: 3.492494]\n",
      "epoch:38 step:30084 [D loss: 0.344278, acc.: 85.16%] [G loss: 3.264854]\n",
      "epoch:38 step:30085 [D loss: 0.387105, acc.: 85.16%] [G loss: 3.053128]\n",
      "epoch:38 step:30086 [D loss: 0.380588, acc.: 82.03%] [G loss: 3.721832]\n",
      "epoch:38 step:30087 [D loss: 0.359969, acc.: 85.94%] [G loss: 2.149266]\n",
      "epoch:38 step:30088 [D loss: 0.361501, acc.: 82.81%] [G loss: 3.690010]\n",
      "epoch:38 step:30089 [D loss: 0.234377, acc.: 91.41%] [G loss: 5.005970]\n",
      "epoch:38 step:30090 [D loss: 0.315132, acc.: 82.03%] [G loss: 2.963294]\n",
      "epoch:38 step:30091 [D loss: 0.302452, acc.: 85.94%] [G loss: 3.100022]\n",
      "epoch:38 step:30092 [D loss: 0.344888, acc.: 85.94%] [G loss: 2.720482]\n",
      "epoch:38 step:30093 [D loss: 0.291734, acc.: 86.72%] [G loss: 2.804402]\n",
      "epoch:38 step:30094 [D loss: 0.282381, acc.: 84.38%] [G loss: 3.509151]\n",
      "epoch:38 step:30095 [D loss: 0.326771, acc.: 82.03%] [G loss: 3.390006]\n",
      "epoch:38 step:30096 [D loss: 0.273334, acc.: 88.28%] [G loss: 4.639106]\n",
      "epoch:38 step:30097 [D loss: 0.435978, acc.: 79.69%] [G loss: 2.965176]\n",
      "epoch:38 step:30098 [D loss: 0.263857, acc.: 92.19%] [G loss: 2.948984]\n",
      "epoch:38 step:30099 [D loss: 0.327503, acc.: 86.72%] [G loss: 2.486080]\n",
      "epoch:38 step:30100 [D loss: 0.308119, acc.: 86.72%] [G loss: 3.093886]\n",
      "epoch:38 step:30101 [D loss: 0.346594, acc.: 83.59%] [G loss: 2.417809]\n",
      "epoch:38 step:30102 [D loss: 0.286785, acc.: 87.50%] [G loss: 3.121184]\n",
      "epoch:38 step:30103 [D loss: 0.283431, acc.: 89.06%] [G loss: 2.693428]\n",
      "epoch:38 step:30104 [D loss: 0.356014, acc.: 82.81%] [G loss: 3.471454]\n",
      "epoch:38 step:30105 [D loss: 0.323650, acc.: 88.28%] [G loss: 3.112603]\n",
      "epoch:38 step:30106 [D loss: 0.422076, acc.: 78.91%] [G loss: 3.618620]\n",
      "epoch:38 step:30107 [D loss: 0.282174, acc.: 86.72%] [G loss: 2.808801]\n",
      "epoch:38 step:30108 [D loss: 0.435559, acc.: 76.56%] [G loss: 2.449221]\n",
      "epoch:38 step:30109 [D loss: 0.398203, acc.: 84.38%] [G loss: 2.942694]\n",
      "epoch:38 step:30110 [D loss: 0.407132, acc.: 85.16%] [G loss: 3.757464]\n",
      "epoch:38 step:30111 [D loss: 0.314478, acc.: 87.50%] [G loss: 3.187731]\n",
      "epoch:38 step:30112 [D loss: 0.276511, acc.: 85.16%] [G loss: 4.167196]\n",
      "epoch:38 step:30113 [D loss: 0.323782, acc.: 85.94%] [G loss: 7.032685]\n",
      "epoch:38 step:30114 [D loss: 0.328164, acc.: 86.72%] [G loss: 4.572586]\n",
      "epoch:38 step:30115 [D loss: 0.344672, acc.: 81.25%] [G loss: 4.318803]\n",
      "epoch:38 step:30116 [D loss: 0.214120, acc.: 95.31%] [G loss: 4.462739]\n",
      "epoch:38 step:30117 [D loss: 0.312462, acc.: 86.72%] [G loss: 3.645250]\n",
      "epoch:38 step:30118 [D loss: 0.311133, acc.: 85.94%] [G loss: 6.750921]\n",
      "epoch:38 step:30119 [D loss: 0.292461, acc.: 85.16%] [G loss: 5.324496]\n",
      "epoch:38 step:30120 [D loss: 0.270940, acc.: 86.72%] [G loss: 3.658599]\n",
      "epoch:38 step:30121 [D loss: 0.281670, acc.: 86.72%] [G loss: 4.172093]\n",
      "epoch:38 step:30122 [D loss: 0.330629, acc.: 81.25%] [G loss: 3.417004]\n",
      "epoch:38 step:30123 [D loss: 0.520320, acc.: 79.69%] [G loss: 3.983666]\n",
      "epoch:38 step:30124 [D loss: 0.272249, acc.: 90.62%] [G loss: 3.500615]\n",
      "epoch:38 step:30125 [D loss: 0.195465, acc.: 88.28%] [G loss: 3.387325]\n",
      "epoch:38 step:30126 [D loss: 0.290190, acc.: 87.50%] [G loss: 3.820308]\n",
      "epoch:38 step:30127 [D loss: 0.209780, acc.: 92.97%] [G loss: 3.881128]\n",
      "epoch:38 step:30128 [D loss: 0.244560, acc.: 89.84%] [G loss: 4.661961]\n",
      "epoch:38 step:30129 [D loss: 0.238334, acc.: 89.06%] [G loss: 4.878584]\n",
      "epoch:38 step:30130 [D loss: 0.341618, acc.: 85.16%] [G loss: 3.655341]\n",
      "epoch:38 step:30131 [D loss: 0.274194, acc.: 87.50%] [G loss: 4.117421]\n",
      "epoch:38 step:30132 [D loss: 0.179427, acc.: 91.41%] [G loss: 5.351840]\n",
      "epoch:38 step:30133 [D loss: 0.364446, acc.: 82.81%] [G loss: 5.739431]\n",
      "epoch:38 step:30134 [D loss: 0.482674, acc.: 79.69%] [G loss: 5.140109]\n",
      "epoch:38 step:30135 [D loss: 0.867086, acc.: 68.75%] [G loss: 10.544044]\n",
      "epoch:38 step:30136 [D loss: 2.467323, acc.: 56.25%] [G loss: 4.921110]\n",
      "epoch:38 step:30137 [D loss: 0.655865, acc.: 71.88%] [G loss: 3.587899]\n",
      "epoch:38 step:30138 [D loss: 0.523739, acc.: 78.12%] [G loss: 5.150820]\n",
      "epoch:38 step:30139 [D loss: 0.347194, acc.: 85.16%] [G loss: 5.871229]\n",
      "epoch:38 step:30140 [D loss: 0.374067, acc.: 80.47%] [G loss: 4.224243]\n",
      "epoch:38 step:30141 [D loss: 0.325615, acc.: 83.59%] [G loss: 4.268042]\n",
      "epoch:38 step:30142 [D loss: 0.373253, acc.: 84.38%] [G loss: 3.289348]\n",
      "epoch:38 step:30143 [D loss: 0.342480, acc.: 83.59%] [G loss: 4.209323]\n",
      "epoch:38 step:30144 [D loss: 0.284661, acc.: 87.50%] [G loss: 5.137763]\n",
      "epoch:38 step:30145 [D loss: 0.361393, acc.: 82.81%] [G loss: 3.402122]\n",
      "epoch:38 step:30146 [D loss: 0.285808, acc.: 84.38%] [G loss: 3.973519]\n",
      "epoch:38 step:30147 [D loss: 0.354510, acc.: 85.16%] [G loss: 3.680864]\n",
      "epoch:38 step:30148 [D loss: 0.239403, acc.: 90.62%] [G loss: 3.547001]\n",
      "epoch:38 step:30149 [D loss: 0.294130, acc.: 86.72%] [G loss: 3.166254]\n",
      "epoch:38 step:30150 [D loss: 0.412568, acc.: 81.25%] [G loss: 2.844874]\n",
      "epoch:38 step:30151 [D loss: 0.387693, acc.: 88.28%] [G loss: 3.335703]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:38 step:30152 [D loss: 0.453393, acc.: 81.25%] [G loss: 2.417037]\n",
      "epoch:38 step:30153 [D loss: 0.283104, acc.: 85.16%] [G loss: 3.377931]\n",
      "epoch:38 step:30154 [D loss: 0.324068, acc.: 88.28%] [G loss: 2.949762]\n",
      "epoch:38 step:30155 [D loss: 0.413161, acc.: 78.91%] [G loss: 2.841380]\n",
      "epoch:38 step:30156 [D loss: 0.320838, acc.: 88.28%] [G loss: 2.525198]\n",
      "epoch:38 step:30157 [D loss: 0.380089, acc.: 82.81%] [G loss: 3.807458]\n",
      "epoch:38 step:30158 [D loss: 0.282844, acc.: 85.94%] [G loss: 3.732647]\n",
      "epoch:38 step:30159 [D loss: 0.355875, acc.: 89.06%] [G loss: 2.862151]\n",
      "epoch:38 step:30160 [D loss: 0.366960, acc.: 83.59%] [G loss: 4.408168]\n",
      "epoch:38 step:30161 [D loss: 0.260205, acc.: 90.62%] [G loss: 3.166271]\n",
      "epoch:38 step:30162 [D loss: 0.330298, acc.: 85.16%] [G loss: 3.824522]\n",
      "epoch:38 step:30163 [D loss: 0.231553, acc.: 91.41%] [G loss: 2.921322]\n",
      "epoch:38 step:30164 [D loss: 0.317622, acc.: 83.59%] [G loss: 4.788000]\n",
      "epoch:38 step:30165 [D loss: 0.265661, acc.: 87.50%] [G loss: 3.470977]\n",
      "epoch:38 step:30166 [D loss: 0.335710, acc.: 88.28%] [G loss: 3.373148]\n",
      "epoch:38 step:30167 [D loss: 0.476756, acc.: 78.91%] [G loss: 2.750007]\n",
      "epoch:38 step:30168 [D loss: 0.358121, acc.: 84.38%] [G loss: 3.658186]\n",
      "epoch:38 step:30169 [D loss: 0.350407, acc.: 84.38%] [G loss: 2.830954]\n",
      "epoch:38 step:30170 [D loss: 0.410694, acc.: 80.47%] [G loss: 4.313853]\n",
      "epoch:38 step:30171 [D loss: 0.290296, acc.: 86.72%] [G loss: 3.369610]\n",
      "epoch:38 step:30172 [D loss: 0.352348, acc.: 85.94%] [G loss: 3.663054]\n",
      "epoch:38 step:30173 [D loss: 0.273068, acc.: 89.84%] [G loss: 3.371459]\n",
      "epoch:38 step:30174 [D loss: 0.312030, acc.: 87.50%] [G loss: 5.322330]\n",
      "epoch:38 step:30175 [D loss: 0.443751, acc.: 78.12%] [G loss: 3.259733]\n",
      "epoch:38 step:30176 [D loss: 0.331727, acc.: 85.16%] [G loss: 3.325608]\n",
      "epoch:38 step:30177 [D loss: 0.393142, acc.: 81.25%] [G loss: 3.553063]\n",
      "epoch:38 step:30178 [D loss: 0.291088, acc.: 88.28%] [G loss: 3.665887]\n",
      "epoch:38 step:30179 [D loss: 0.308654, acc.: 84.38%] [G loss: 3.035590]\n",
      "epoch:38 step:30180 [D loss: 0.372391, acc.: 84.38%] [G loss: 2.352495]\n",
      "epoch:38 step:30181 [D loss: 0.381892, acc.: 82.03%] [G loss: 3.943976]\n",
      "epoch:38 step:30182 [D loss: 0.304290, acc.: 85.94%] [G loss: 2.979530]\n",
      "epoch:38 step:30183 [D loss: 0.402934, acc.: 78.91%] [G loss: 3.032192]\n",
      "epoch:38 step:30184 [D loss: 0.456920, acc.: 73.44%] [G loss: 3.543632]\n",
      "epoch:38 step:30185 [D loss: 0.360303, acc.: 78.91%] [G loss: 2.621610]\n",
      "epoch:38 step:30186 [D loss: 0.265372, acc.: 89.06%] [G loss: 3.130017]\n",
      "epoch:38 step:30187 [D loss: 0.363293, acc.: 82.03%] [G loss: 3.186497]\n",
      "epoch:38 step:30188 [D loss: 0.288260, acc.: 87.50%] [G loss: 4.236787]\n",
      "epoch:38 step:30189 [D loss: 0.353642, acc.: 86.72%] [G loss: 6.239376]\n",
      "epoch:38 step:30190 [D loss: 0.335354, acc.: 86.72%] [G loss: 3.587137]\n",
      "epoch:38 step:30191 [D loss: 0.379546, acc.: 82.81%] [G loss: 5.748984]\n",
      "epoch:38 step:30192 [D loss: 0.562360, acc.: 72.66%] [G loss: 5.375347]\n",
      "epoch:38 step:30193 [D loss: 0.631919, acc.: 71.09%] [G loss: 3.319578]\n",
      "epoch:38 step:30194 [D loss: 0.397467, acc.: 81.25%] [G loss: 2.757588]\n",
      "epoch:38 step:30195 [D loss: 0.361625, acc.: 84.38%] [G loss: 3.035288]\n",
      "epoch:38 step:30196 [D loss: 0.344503, acc.: 84.38%] [G loss: 3.781787]\n",
      "epoch:38 step:30197 [D loss: 0.329315, acc.: 82.81%] [G loss: 3.323113]\n",
      "epoch:38 step:30198 [D loss: 0.330557, acc.: 88.28%] [G loss: 3.840265]\n",
      "epoch:38 step:30199 [D loss: 0.327965, acc.: 87.50%] [G loss: 3.551830]\n",
      "epoch:38 step:30200 [D loss: 0.419267, acc.: 81.25%] [G loss: 3.659496]\n",
      "##############\n",
      "[0.86166057 0.85632696 0.82245879 0.81141092 0.77067732 0.83610393\n",
      " 0.87667523 0.84375331 0.82884161 0.80827167]\n",
      "##########\n",
      "epoch:38 step:30201 [D loss: 0.265503, acc.: 89.06%] [G loss: 4.218616]\n",
      "epoch:38 step:30202 [D loss: 0.453995, acc.: 77.34%] [G loss: 3.615929]\n",
      "epoch:38 step:30203 [D loss: 0.385486, acc.: 83.59%] [G loss: 3.503897]\n",
      "epoch:38 step:30204 [D loss: 0.533724, acc.: 75.00%] [G loss: 3.324340]\n",
      "epoch:38 step:30205 [D loss: 0.216658, acc.: 91.41%] [G loss: 3.914925]\n",
      "epoch:38 step:30206 [D loss: 0.382843, acc.: 80.47%] [G loss: 3.582618]\n",
      "epoch:38 step:30207 [D loss: 0.238356, acc.: 90.62%] [G loss: 4.369752]\n",
      "epoch:38 step:30208 [D loss: 0.357845, acc.: 83.59%] [G loss: 4.377153]\n",
      "epoch:38 step:30209 [D loss: 0.280217, acc.: 89.84%] [G loss: 5.803124]\n",
      "epoch:38 step:30210 [D loss: 0.327715, acc.: 84.38%] [G loss: 3.034587]\n",
      "epoch:38 step:30211 [D loss: 0.399624, acc.: 82.81%] [G loss: 3.934328]\n",
      "epoch:38 step:30212 [D loss: 0.239125, acc.: 92.19%] [G loss: 3.429868]\n",
      "epoch:38 step:30213 [D loss: 0.260366, acc.: 89.06%] [G loss: 3.352019]\n",
      "epoch:38 step:30214 [D loss: 0.360634, acc.: 85.16%] [G loss: 4.365345]\n",
      "epoch:38 step:30215 [D loss: 0.477223, acc.: 73.44%] [G loss: 3.369799]\n",
      "epoch:38 step:30216 [D loss: 0.277627, acc.: 86.72%] [G loss: 3.183609]\n",
      "epoch:38 step:30217 [D loss: 0.315345, acc.: 86.72%] [G loss: 2.826285]\n",
      "epoch:38 step:30218 [D loss: 0.410120, acc.: 81.25%] [G loss: 2.895748]\n",
      "epoch:38 step:30219 [D loss: 0.422680, acc.: 80.47%] [G loss: 2.390475]\n",
      "epoch:38 step:30220 [D loss: 0.367568, acc.: 82.03%] [G loss: 3.930815]\n",
      "epoch:38 step:30221 [D loss: 0.320302, acc.: 86.72%] [G loss: 2.529594]\n",
      "epoch:38 step:30222 [D loss: 0.344580, acc.: 84.38%] [G loss: 3.859431]\n",
      "epoch:38 step:30223 [D loss: 0.419875, acc.: 82.81%] [G loss: 2.471729]\n",
      "epoch:38 step:30224 [D loss: 0.327028, acc.: 86.72%] [G loss: 3.333490]\n",
      "epoch:38 step:30225 [D loss: 0.343916, acc.: 87.50%] [G loss: 3.019764]\n",
      "epoch:38 step:30226 [D loss: 0.167111, acc.: 92.19%] [G loss: 3.162120]\n",
      "epoch:38 step:30227 [D loss: 0.319460, acc.: 85.16%] [G loss: 4.388904]\n",
      "epoch:38 step:30228 [D loss: 0.238210, acc.: 89.84%] [G loss: 5.013497]\n",
      "epoch:38 step:30229 [D loss: 0.296333, acc.: 88.28%] [G loss: 3.760848]\n",
      "epoch:38 step:30230 [D loss: 0.329525, acc.: 87.50%] [G loss: 5.182512]\n",
      "epoch:38 step:30231 [D loss: 0.366207, acc.: 83.59%] [G loss: 3.154320]\n",
      "epoch:38 step:30232 [D loss: 0.403861, acc.: 82.81%] [G loss: 3.170801]\n",
      "epoch:38 step:30233 [D loss: 0.322109, acc.: 82.81%] [G loss: 3.741711]\n",
      "epoch:38 step:30234 [D loss: 0.320391, acc.: 87.50%] [G loss: 3.164387]\n",
      "epoch:38 step:30235 [D loss: 0.298831, acc.: 81.25%] [G loss: 3.725591]\n",
      "epoch:38 step:30236 [D loss: 0.334812, acc.: 90.62%] [G loss: 3.595059]\n",
      "epoch:38 step:30237 [D loss: 0.282309, acc.: 85.94%] [G loss: 3.706544]\n",
      "epoch:38 step:30238 [D loss: 0.400232, acc.: 81.25%] [G loss: 3.604339]\n",
      "epoch:38 step:30239 [D loss: 0.285066, acc.: 87.50%] [G loss: 3.657610]\n",
      "epoch:38 step:30240 [D loss: 0.391636, acc.: 78.91%] [G loss: 3.007367]\n",
      "epoch:38 step:30241 [D loss: 0.299957, acc.: 89.84%] [G loss: 3.298966]\n",
      "epoch:38 step:30242 [D loss: 0.428756, acc.: 79.69%] [G loss: 3.404408]\n",
      "epoch:38 step:30243 [D loss: 0.321931, acc.: 86.72%] [G loss: 3.551271]\n",
      "epoch:38 step:30244 [D loss: 0.407520, acc.: 84.38%] [G loss: 3.334296]\n",
      "epoch:38 step:30245 [D loss: 0.493053, acc.: 78.91%] [G loss: 3.455225]\n",
      "epoch:38 step:30246 [D loss: 0.373416, acc.: 82.81%] [G loss: 3.767977]\n",
      "epoch:38 step:30247 [D loss: 0.221121, acc.: 90.62%] [G loss: 2.877121]\n",
      "epoch:38 step:30248 [D loss: 0.360657, acc.: 83.59%] [G loss: 3.616706]\n",
      "epoch:38 step:30249 [D loss: 0.245892, acc.: 89.84%] [G loss: 3.251405]\n",
      "epoch:38 step:30250 [D loss: 0.243180, acc.: 90.62%] [G loss: 4.258783]\n",
      "epoch:38 step:30251 [D loss: 0.402414, acc.: 80.47%] [G loss: 2.687730]\n",
      "epoch:38 step:30252 [D loss: 0.371339, acc.: 82.03%] [G loss: 2.898966]\n",
      "epoch:38 step:30253 [D loss: 0.430669, acc.: 77.34%] [G loss: 2.312668]\n",
      "epoch:38 step:30254 [D loss: 0.393538, acc.: 81.25%] [G loss: 2.922652]\n",
      "epoch:38 step:30255 [D loss: 0.287912, acc.: 85.16%] [G loss: 3.071936]\n",
      "epoch:38 step:30256 [D loss: 0.290947, acc.: 90.62%] [G loss: 3.560098]\n",
      "epoch:38 step:30257 [D loss: 0.345371, acc.: 82.81%] [G loss: 3.508611]\n",
      "epoch:38 step:30258 [D loss: 0.327587, acc.: 84.38%] [G loss: 3.372455]\n",
      "epoch:38 step:30259 [D loss: 0.345043, acc.: 82.81%] [G loss: 2.780734]\n",
      "epoch:38 step:30260 [D loss: 0.304746, acc.: 84.38%] [G loss: 4.151070]\n",
      "epoch:38 step:30261 [D loss: 0.285523, acc.: 86.72%] [G loss: 4.174860]\n",
      "epoch:38 step:30262 [D loss: 0.339109, acc.: 85.16%] [G loss: 4.297211]\n",
      "epoch:38 step:30263 [D loss: 0.323258, acc.: 85.16%] [G loss: 3.331255]\n",
      "epoch:38 step:30264 [D loss: 0.290207, acc.: 87.50%] [G loss: 3.015866]\n",
      "epoch:38 step:30265 [D loss: 0.257956, acc.: 92.19%] [G loss: 2.493821]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:38 step:30266 [D loss: 0.388310, acc.: 85.94%] [G loss: 3.069898]\n",
      "epoch:38 step:30267 [D loss: 0.304394, acc.: 88.28%] [G loss: 3.609731]\n",
      "epoch:38 step:30268 [D loss: 0.380097, acc.: 79.69%] [G loss: 2.929517]\n",
      "epoch:38 step:30269 [D loss: 0.423511, acc.: 79.69%] [G loss: 4.316784]\n",
      "epoch:38 step:30270 [D loss: 0.438856, acc.: 82.81%] [G loss: 5.855332]\n",
      "epoch:38 step:30271 [D loss: 0.528205, acc.: 71.88%] [G loss: 3.492590]\n",
      "epoch:38 step:30272 [D loss: 0.276818, acc.: 85.16%] [G loss: 3.757685]\n",
      "epoch:38 step:30273 [D loss: 0.385711, acc.: 83.59%] [G loss: 3.233464]\n",
      "epoch:38 step:30274 [D loss: 0.293480, acc.: 87.50%] [G loss: 2.922412]\n",
      "epoch:38 step:30275 [D loss: 0.251884, acc.: 90.62%] [G loss: 3.102256]\n",
      "epoch:38 step:30276 [D loss: 0.332970, acc.: 88.28%] [G loss: 2.759717]\n",
      "epoch:38 step:30277 [D loss: 0.294096, acc.: 86.72%] [G loss: 2.656375]\n",
      "epoch:38 step:30278 [D loss: 0.304744, acc.: 85.16%] [G loss: 3.818888]\n",
      "epoch:38 step:30279 [D loss: 0.350848, acc.: 82.81%] [G loss: 2.791358]\n",
      "epoch:38 step:30280 [D loss: 0.214169, acc.: 89.84%] [G loss: 5.063411]\n",
      "epoch:38 step:30281 [D loss: 0.284054, acc.: 89.84%] [G loss: 4.610064]\n",
      "epoch:38 step:30282 [D loss: 0.339446, acc.: 85.16%] [G loss: 2.502465]\n",
      "epoch:38 step:30283 [D loss: 0.254795, acc.: 89.84%] [G loss: 2.492679]\n",
      "epoch:38 step:30284 [D loss: 0.354280, acc.: 79.69%] [G loss: 4.999018]\n",
      "epoch:38 step:30285 [D loss: 0.396141, acc.: 79.69%] [G loss: 3.420678]\n",
      "epoch:38 step:30286 [D loss: 0.210495, acc.: 93.75%] [G loss: 3.861499]\n",
      "epoch:38 step:30287 [D loss: 0.359803, acc.: 84.38%] [G loss: 5.070915]\n",
      "epoch:38 step:30288 [D loss: 0.606567, acc.: 71.88%] [G loss: 3.456636]\n",
      "epoch:38 step:30289 [D loss: 0.443570, acc.: 82.03%] [G loss: 2.991209]\n",
      "epoch:38 step:30290 [D loss: 0.342643, acc.: 85.94%] [G loss: 3.591606]\n",
      "epoch:38 step:30291 [D loss: 0.318438, acc.: 84.38%] [G loss: 2.746813]\n",
      "epoch:38 step:30292 [D loss: 0.219058, acc.: 91.41%] [G loss: 3.392402]\n",
      "epoch:38 step:30293 [D loss: 0.318251, acc.: 86.72%] [G loss: 3.128566]\n",
      "epoch:38 step:30294 [D loss: 0.281822, acc.: 86.72%] [G loss: 2.967359]\n",
      "epoch:38 step:30295 [D loss: 0.409538, acc.: 80.47%] [G loss: 4.300491]\n",
      "epoch:38 step:30296 [D loss: 0.473615, acc.: 81.25%] [G loss: 5.933146]\n",
      "epoch:38 step:30297 [D loss: 0.704370, acc.: 75.00%] [G loss: 5.111362]\n",
      "epoch:38 step:30298 [D loss: 0.332871, acc.: 85.94%] [G loss: 5.224542]\n",
      "epoch:38 step:30299 [D loss: 0.161347, acc.: 92.97%] [G loss: 6.236402]\n",
      "epoch:38 step:30300 [D loss: 0.230403, acc.: 92.19%] [G loss: 7.572998]\n",
      "epoch:38 step:30301 [D loss: 0.247670, acc.: 86.72%] [G loss: 5.460540]\n",
      "epoch:38 step:30302 [D loss: 0.450177, acc.: 82.03%] [G loss: 4.406389]\n",
      "epoch:38 step:30303 [D loss: 0.340867, acc.: 78.91%] [G loss: 4.189466]\n",
      "epoch:38 step:30304 [D loss: 0.310767, acc.: 82.81%] [G loss: 3.572104]\n",
      "epoch:38 step:30305 [D loss: 0.295387, acc.: 83.59%] [G loss: 3.445727]\n",
      "epoch:38 step:30306 [D loss: 0.386684, acc.: 81.25%] [G loss: 3.335214]\n",
      "epoch:38 step:30307 [D loss: 0.382227, acc.: 82.81%] [G loss: 3.775004]\n",
      "epoch:38 step:30308 [D loss: 0.346922, acc.: 83.59%] [G loss: 4.703250]\n",
      "epoch:38 step:30309 [D loss: 0.413550, acc.: 78.12%] [G loss: 3.055840]\n",
      "epoch:38 step:30310 [D loss: 0.408493, acc.: 79.69%] [G loss: 3.454423]\n",
      "epoch:38 step:30311 [D loss: 0.279895, acc.: 87.50%] [G loss: 3.336656]\n",
      "epoch:38 step:30312 [D loss: 0.308222, acc.: 85.94%] [G loss: 2.943534]\n",
      "epoch:38 step:30313 [D loss: 0.366542, acc.: 85.16%] [G loss: 2.930702]\n",
      "epoch:38 step:30314 [D loss: 0.400306, acc.: 85.16%] [G loss: 2.553669]\n",
      "epoch:38 step:30315 [D loss: 0.263487, acc.: 89.06%] [G loss: 2.807315]\n",
      "epoch:38 step:30316 [D loss: 0.409586, acc.: 80.47%] [G loss: 2.968894]\n",
      "epoch:38 step:30317 [D loss: 0.344220, acc.: 85.16%] [G loss: 3.087405]\n",
      "epoch:38 step:30318 [D loss: 0.274539, acc.: 88.28%] [G loss: 3.745039]\n",
      "epoch:38 step:30319 [D loss: 0.270630, acc.: 90.62%] [G loss: 2.934657]\n",
      "epoch:38 step:30320 [D loss: 0.334859, acc.: 85.16%] [G loss: 3.065853]\n",
      "epoch:38 step:30321 [D loss: 0.341761, acc.: 85.16%] [G loss: 2.451910]\n",
      "epoch:38 step:30322 [D loss: 0.302767, acc.: 86.72%] [G loss: 2.588455]\n",
      "epoch:38 step:30323 [D loss: 0.349974, acc.: 82.03%] [G loss: 2.447114]\n",
      "epoch:38 step:30324 [D loss: 0.379555, acc.: 82.81%] [G loss: 2.986729]\n",
      "epoch:38 step:30325 [D loss: 0.244166, acc.: 89.06%] [G loss: 2.843519]\n",
      "epoch:38 step:30326 [D loss: 0.328307, acc.: 82.81%] [G loss: 4.168781]\n",
      "epoch:38 step:30327 [D loss: 0.427094, acc.: 76.56%] [G loss: 3.875802]\n",
      "epoch:38 step:30328 [D loss: 0.396096, acc.: 80.47%] [G loss: 3.636059]\n",
      "epoch:38 step:30329 [D loss: 0.219442, acc.: 92.19%] [G loss: 3.646484]\n",
      "epoch:38 step:30330 [D loss: 0.297824, acc.: 89.06%] [G loss: 3.246824]\n",
      "epoch:38 step:30331 [D loss: 0.279773, acc.: 89.84%] [G loss: 3.626795]\n",
      "epoch:38 step:30332 [D loss: 0.323230, acc.: 88.28%] [G loss: 3.227273]\n",
      "epoch:38 step:30333 [D loss: 0.318744, acc.: 87.50%] [G loss: 3.259053]\n",
      "epoch:38 step:30334 [D loss: 0.322336, acc.: 86.72%] [G loss: 3.319213]\n",
      "epoch:38 step:30335 [D loss: 0.343270, acc.: 86.72%] [G loss: 3.844740]\n",
      "epoch:38 step:30336 [D loss: 0.373118, acc.: 84.38%] [G loss: 3.572159]\n",
      "epoch:38 step:30337 [D loss: 0.322275, acc.: 85.94%] [G loss: 3.574594]\n",
      "epoch:38 step:30338 [D loss: 0.337641, acc.: 86.72%] [G loss: 3.456011]\n",
      "epoch:38 step:30339 [D loss: 0.320124, acc.: 81.25%] [G loss: 3.324519]\n",
      "epoch:38 step:30340 [D loss: 0.365191, acc.: 82.03%] [G loss: 3.172247]\n",
      "epoch:38 step:30341 [D loss: 0.388638, acc.: 80.47%] [G loss: 2.468551]\n",
      "epoch:38 step:30342 [D loss: 0.319254, acc.: 85.94%] [G loss: 3.750300]\n",
      "epoch:38 step:30343 [D loss: 0.321984, acc.: 87.50%] [G loss: 4.023407]\n",
      "epoch:38 step:30344 [D loss: 0.412768, acc.: 83.59%] [G loss: 3.261611]\n",
      "epoch:38 step:30345 [D loss: 0.265283, acc.: 89.84%] [G loss: 2.956676]\n",
      "epoch:38 step:30346 [D loss: 0.341018, acc.: 82.03%] [G loss: 3.500224]\n",
      "epoch:38 step:30347 [D loss: 0.409709, acc.: 82.81%] [G loss: 3.427754]\n",
      "epoch:38 step:30348 [D loss: 0.363664, acc.: 85.94%] [G loss: 4.146382]\n",
      "epoch:38 step:30349 [D loss: 0.334852, acc.: 80.47%] [G loss: 2.775260]\n",
      "epoch:38 step:30350 [D loss: 0.321525, acc.: 86.72%] [G loss: 3.133315]\n",
      "epoch:38 step:30351 [D loss: 0.253621, acc.: 85.94%] [G loss: 3.322583]\n",
      "epoch:38 step:30352 [D loss: 0.263523, acc.: 89.06%] [G loss: 3.446242]\n",
      "epoch:38 step:30353 [D loss: 0.335952, acc.: 86.72%] [G loss: 2.678532]\n",
      "epoch:38 step:30354 [D loss: 0.276985, acc.: 88.28%] [G loss: 3.276879]\n",
      "epoch:38 step:30355 [D loss: 0.365062, acc.: 84.38%] [G loss: 3.627432]\n",
      "epoch:38 step:30356 [D loss: 0.305781, acc.: 87.50%] [G loss: 2.563660]\n",
      "epoch:38 step:30357 [D loss: 0.291187, acc.: 85.16%] [G loss: 2.608494]\n",
      "epoch:38 step:30358 [D loss: 0.252485, acc.: 89.06%] [G loss: 3.522029]\n",
      "epoch:38 step:30359 [D loss: 0.274779, acc.: 90.62%] [G loss: 2.941319]\n",
      "epoch:38 step:30360 [D loss: 0.332250, acc.: 85.16%] [G loss: 3.519317]\n",
      "epoch:38 step:30361 [D loss: 0.296850, acc.: 85.94%] [G loss: 3.469962]\n",
      "epoch:38 step:30362 [D loss: 0.325884, acc.: 85.94%] [G loss: 3.869459]\n",
      "epoch:38 step:30363 [D loss: 0.323218, acc.: 85.94%] [G loss: 3.930278]\n",
      "epoch:38 step:30364 [D loss: 0.306801, acc.: 84.38%] [G loss: 4.630816]\n",
      "epoch:38 step:30365 [D loss: 0.389431, acc.: 85.94%] [G loss: 3.316922]\n",
      "epoch:38 step:30366 [D loss: 0.302940, acc.: 87.50%] [G loss: 2.133518]\n",
      "epoch:38 step:30367 [D loss: 0.276193, acc.: 85.94%] [G loss: 2.880552]\n",
      "epoch:38 step:30368 [D loss: 0.395336, acc.: 78.91%] [G loss: 3.481228]\n",
      "epoch:38 step:30369 [D loss: 0.312516, acc.: 85.16%] [G loss: 3.458833]\n",
      "epoch:38 step:30370 [D loss: 0.321931, acc.: 87.50%] [G loss: 2.915111]\n",
      "epoch:38 step:30371 [D loss: 0.430009, acc.: 81.25%] [G loss: 3.385962]\n",
      "epoch:38 step:30372 [D loss: 0.281543, acc.: 89.06%] [G loss: 3.826786]\n",
      "epoch:38 step:30373 [D loss: 0.298191, acc.: 87.50%] [G loss: 3.614280]\n",
      "epoch:38 step:30374 [D loss: 0.369025, acc.: 82.81%] [G loss: 3.370082]\n",
      "epoch:38 step:30375 [D loss: 0.390696, acc.: 81.25%] [G loss: 2.907006]\n",
      "epoch:38 step:30376 [D loss: 0.342051, acc.: 85.94%] [G loss: 2.701524]\n",
      "epoch:38 step:30377 [D loss: 0.410548, acc.: 81.25%] [G loss: 3.097609]\n",
      "epoch:38 step:30378 [D loss: 0.507380, acc.: 76.56%] [G loss: 3.009102]\n",
      "epoch:38 step:30379 [D loss: 0.319821, acc.: 83.59%] [G loss: 3.146668]\n",
      "epoch:38 step:30380 [D loss: 0.252851, acc.: 89.06%] [G loss: 3.748711]\n",
      "epoch:38 step:30381 [D loss: 0.363403, acc.: 80.47%] [G loss: 4.974704]\n",
      "epoch:38 step:30382 [D loss: 0.411420, acc.: 82.81%] [G loss: 4.361165]\n",
      "epoch:38 step:30383 [D loss: 0.467621, acc.: 78.91%] [G loss: 2.637971]\n",
      "epoch:38 step:30384 [D loss: 0.579505, acc.: 76.56%] [G loss: 2.522640]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:38 step:30385 [D loss: 0.353497, acc.: 85.16%] [G loss: 3.254756]\n",
      "epoch:38 step:30386 [D loss: 0.380801, acc.: 85.94%] [G loss: 3.295117]\n",
      "epoch:38 step:30387 [D loss: 0.296364, acc.: 87.50%] [G loss: 3.450934]\n",
      "epoch:38 step:30388 [D loss: 0.281953, acc.: 85.94%] [G loss: 4.253563]\n",
      "epoch:38 step:30389 [D loss: 0.262891, acc.: 86.72%] [G loss: 3.165819]\n",
      "epoch:38 step:30390 [D loss: 0.353191, acc.: 85.16%] [G loss: 3.685848]\n",
      "epoch:38 step:30391 [D loss: 0.322680, acc.: 84.38%] [G loss: 2.808496]\n",
      "epoch:38 step:30392 [D loss: 0.256603, acc.: 92.19%] [G loss: 3.093583]\n",
      "epoch:38 step:30393 [D loss: 0.277907, acc.: 90.62%] [G loss: 2.524815]\n",
      "epoch:38 step:30394 [D loss: 0.229293, acc.: 91.41%] [G loss: 3.213480]\n",
      "epoch:38 step:30395 [D loss: 0.266328, acc.: 89.06%] [G loss: 3.279681]\n",
      "epoch:38 step:30396 [D loss: 0.298827, acc.: 88.28%] [G loss: 3.311165]\n",
      "epoch:38 step:30397 [D loss: 0.338119, acc.: 84.38%] [G loss: 3.947471]\n",
      "epoch:38 step:30398 [D loss: 0.298675, acc.: 86.72%] [G loss: 3.015246]\n",
      "epoch:38 step:30399 [D loss: 0.345111, acc.: 86.72%] [G loss: 3.660378]\n",
      "epoch:38 step:30400 [D loss: 0.365496, acc.: 84.38%] [G loss: 2.956491]\n",
      "##############\n",
      "[0.85536804 0.83946947 0.79996862 0.79987443 0.80251691 0.82107789\n",
      " 0.87173971 0.82985547 0.82493881 0.8347809 ]\n",
      "##########\n",
      "epoch:38 step:30401 [D loss: 0.296829, acc.: 89.06%] [G loss: 2.904083]\n",
      "epoch:38 step:30402 [D loss: 0.368863, acc.: 80.47%] [G loss: 2.351691]\n",
      "epoch:38 step:30403 [D loss: 0.245379, acc.: 91.41%] [G loss: 2.976635]\n",
      "epoch:38 step:30404 [D loss: 0.394526, acc.: 79.69%] [G loss: 2.674566]\n",
      "epoch:38 step:30405 [D loss: 0.336155, acc.: 81.25%] [G loss: 2.320420]\n",
      "epoch:38 step:30406 [D loss: 0.338750, acc.: 82.81%] [G loss: 3.537640]\n",
      "epoch:38 step:30407 [D loss: 0.309041, acc.: 85.16%] [G loss: 3.292581]\n",
      "epoch:38 step:30408 [D loss: 0.255335, acc.: 86.72%] [G loss: 3.259657]\n",
      "epoch:38 step:30409 [D loss: 0.319510, acc.: 85.94%] [G loss: 3.380489]\n",
      "epoch:38 step:30410 [D loss: 0.258256, acc.: 89.84%] [G loss: 2.947225]\n",
      "epoch:38 step:30411 [D loss: 0.292450, acc.: 87.50%] [G loss: 3.024538]\n",
      "epoch:38 step:30412 [D loss: 0.336953, acc.: 82.81%] [G loss: 3.773251]\n",
      "epoch:38 step:30413 [D loss: 0.381392, acc.: 79.69%] [G loss: 3.032001]\n",
      "epoch:38 step:30414 [D loss: 0.310742, acc.: 85.16%] [G loss: 3.457985]\n",
      "epoch:38 step:30415 [D loss: 0.304364, acc.: 87.50%] [G loss: 2.851290]\n",
      "epoch:38 step:30416 [D loss: 0.240140, acc.: 89.06%] [G loss: 3.412016]\n",
      "epoch:38 step:30417 [D loss: 0.275322, acc.: 88.28%] [G loss: 2.997362]\n",
      "epoch:38 step:30418 [D loss: 0.481411, acc.: 80.47%] [G loss: 2.752105]\n",
      "epoch:38 step:30419 [D loss: 0.289471, acc.: 86.72%] [G loss: 3.991462]\n",
      "epoch:38 step:30420 [D loss: 0.276597, acc.: 91.41%] [G loss: 3.609388]\n",
      "epoch:38 step:30421 [D loss: 0.254632, acc.: 89.06%] [G loss: 3.567593]\n",
      "epoch:38 step:30422 [D loss: 0.232696, acc.: 90.62%] [G loss: 4.124132]\n",
      "epoch:38 step:30423 [D loss: 0.295185, acc.: 87.50%] [G loss: 3.721970]\n",
      "epoch:38 step:30424 [D loss: 0.364881, acc.: 82.81%] [G loss: 3.120728]\n",
      "epoch:38 step:30425 [D loss: 0.351557, acc.: 86.72%] [G loss: 3.526809]\n",
      "epoch:38 step:30426 [D loss: 0.237594, acc.: 88.28%] [G loss: 5.809762]\n",
      "epoch:38 step:30427 [D loss: 0.417891, acc.: 78.12%] [G loss: 3.479461]\n",
      "epoch:38 step:30428 [D loss: 0.328436, acc.: 82.81%] [G loss: 2.948775]\n",
      "epoch:38 step:30429 [D loss: 0.367224, acc.: 84.38%] [G loss: 3.555985]\n",
      "epoch:38 step:30430 [D loss: 0.487680, acc.: 78.12%] [G loss: 5.559713]\n",
      "epoch:38 step:30431 [D loss: 0.603210, acc.: 77.34%] [G loss: 5.856237]\n",
      "epoch:38 step:30432 [D loss: 0.766216, acc.: 66.41%] [G loss: 4.918404]\n",
      "epoch:38 step:30433 [D loss: 0.780754, acc.: 72.66%] [G loss: 4.789148]\n",
      "epoch:38 step:30434 [D loss: 0.427239, acc.: 79.69%] [G loss: 3.194500]\n",
      "epoch:38 step:30435 [D loss: 0.385666, acc.: 82.03%] [G loss: 3.174135]\n",
      "epoch:38 step:30436 [D loss: 0.280366, acc.: 89.06%] [G loss: 2.969336]\n",
      "epoch:38 step:30437 [D loss: 0.367983, acc.: 86.72%] [G loss: 4.417863]\n",
      "epoch:38 step:30438 [D loss: 0.376932, acc.: 85.16%] [G loss: 3.814727]\n",
      "epoch:38 step:30439 [D loss: 0.278716, acc.: 88.28%] [G loss: 2.981888]\n",
      "epoch:38 step:30440 [D loss: 0.450064, acc.: 79.69%] [G loss: 3.046695]\n",
      "epoch:38 step:30441 [D loss: 0.358807, acc.: 81.25%] [G loss: 2.581645]\n",
      "epoch:38 step:30442 [D loss: 0.232226, acc.: 89.06%] [G loss: 3.367811]\n",
      "epoch:38 step:30443 [D loss: 0.299637, acc.: 85.16%] [G loss: 3.051266]\n",
      "epoch:38 step:30444 [D loss: 0.258411, acc.: 89.84%] [G loss: 2.950700]\n",
      "epoch:38 step:30445 [D loss: 0.316264, acc.: 86.72%] [G loss: 3.280995]\n",
      "epoch:38 step:30446 [D loss: 0.297853, acc.: 86.72%] [G loss: 3.025360]\n",
      "epoch:38 step:30447 [D loss: 0.181137, acc.: 92.97%] [G loss: 4.126472]\n",
      "epoch:38 step:30448 [D loss: 0.259392, acc.: 89.06%] [G loss: 3.252291]\n",
      "epoch:38 step:30449 [D loss: 0.365816, acc.: 82.03%] [G loss: 2.459732]\n",
      "epoch:38 step:30450 [D loss: 0.315486, acc.: 82.81%] [G loss: 3.012465]\n",
      "epoch:38 step:30451 [D loss: 0.385459, acc.: 81.25%] [G loss: 3.099933]\n",
      "epoch:38 step:30452 [D loss: 0.348869, acc.: 85.94%] [G loss: 3.035689]\n",
      "epoch:38 step:30453 [D loss: 0.443687, acc.: 80.47%] [G loss: 2.502576]\n",
      "epoch:38 step:30454 [D loss: 0.279742, acc.: 86.72%] [G loss: 2.988316]\n",
      "epoch:38 step:30455 [D loss: 0.276666, acc.: 88.28%] [G loss: 5.618755]\n",
      "epoch:38 step:30456 [D loss: 0.362233, acc.: 81.25%] [G loss: 2.951520]\n",
      "epoch:38 step:30457 [D loss: 0.347530, acc.: 85.16%] [G loss: 5.237283]\n",
      "epoch:38 step:30458 [D loss: 0.427318, acc.: 78.12%] [G loss: 4.214306]\n",
      "epoch:38 step:30459 [D loss: 0.226782, acc.: 92.97%] [G loss: 3.077479]\n",
      "epoch:39 step:30460 [D loss: 0.298802, acc.: 88.28%] [G loss: 3.282346]\n",
      "epoch:39 step:30461 [D loss: 0.301794, acc.: 87.50%] [G loss: 4.193828]\n",
      "epoch:39 step:30462 [D loss: 0.308806, acc.: 87.50%] [G loss: 3.043313]\n",
      "epoch:39 step:30463 [D loss: 0.277253, acc.: 91.41%] [G loss: 3.281808]\n",
      "epoch:39 step:30464 [D loss: 0.360935, acc.: 85.94%] [G loss: 2.860098]\n",
      "epoch:39 step:30465 [D loss: 0.354034, acc.: 84.38%] [G loss: 3.125543]\n",
      "epoch:39 step:30466 [D loss: 0.230147, acc.: 93.75%] [G loss: 2.571056]\n",
      "epoch:39 step:30467 [D loss: 0.367872, acc.: 85.94%] [G loss: 2.301145]\n",
      "epoch:39 step:30468 [D loss: 0.378099, acc.: 82.81%] [G loss: 3.340074]\n",
      "epoch:39 step:30469 [D loss: 0.260450, acc.: 90.62%] [G loss: 3.933800]\n",
      "epoch:39 step:30470 [D loss: 0.269839, acc.: 90.62%] [G loss: 2.586375]\n",
      "epoch:39 step:30471 [D loss: 0.284886, acc.: 88.28%] [G loss: 4.328659]\n",
      "epoch:39 step:30472 [D loss: 0.268365, acc.: 90.62%] [G loss: 2.745800]\n",
      "epoch:39 step:30473 [D loss: 0.238254, acc.: 92.19%] [G loss: 3.838240]\n",
      "epoch:39 step:30474 [D loss: 0.372030, acc.: 82.03%] [G loss: 3.148259]\n",
      "epoch:39 step:30475 [D loss: 0.314555, acc.: 89.06%] [G loss: 2.835968]\n",
      "epoch:39 step:30476 [D loss: 0.299311, acc.: 87.50%] [G loss: 3.184001]\n",
      "epoch:39 step:30477 [D loss: 0.367097, acc.: 81.25%] [G loss: 2.907685]\n",
      "epoch:39 step:30478 [D loss: 0.307587, acc.: 86.72%] [G loss: 2.208141]\n",
      "epoch:39 step:30479 [D loss: 0.430981, acc.: 80.47%] [G loss: 2.234811]\n",
      "epoch:39 step:30480 [D loss: 0.330442, acc.: 85.16%] [G loss: 3.675202]\n",
      "epoch:39 step:30481 [D loss: 0.279244, acc.: 87.50%] [G loss: 3.199727]\n",
      "epoch:39 step:30482 [D loss: 0.316560, acc.: 85.94%] [G loss: 3.188978]\n",
      "epoch:39 step:30483 [D loss: 0.338365, acc.: 85.94%] [G loss: 3.243579]\n",
      "epoch:39 step:30484 [D loss: 0.358713, acc.: 78.12%] [G loss: 2.607134]\n",
      "epoch:39 step:30485 [D loss: 0.269626, acc.: 88.28%] [G loss: 3.073709]\n",
      "epoch:39 step:30486 [D loss: 0.370095, acc.: 84.38%] [G loss: 3.964283]\n",
      "epoch:39 step:30487 [D loss: 0.640187, acc.: 71.09%] [G loss: 3.598691]\n",
      "epoch:39 step:30488 [D loss: 0.355278, acc.: 84.38%] [G loss: 3.285139]\n",
      "epoch:39 step:30489 [D loss: 0.295132, acc.: 86.72%] [G loss: 2.817276]\n",
      "epoch:39 step:30490 [D loss: 0.393096, acc.: 76.56%] [G loss: 4.150777]\n",
      "epoch:39 step:30491 [D loss: 0.352325, acc.: 79.69%] [G loss: 3.795076]\n",
      "epoch:39 step:30492 [D loss: 0.271268, acc.: 88.28%] [G loss: 3.881964]\n",
      "epoch:39 step:30493 [D loss: 0.312996, acc.: 84.38%] [G loss: 2.837123]\n",
      "epoch:39 step:30494 [D loss: 0.374029, acc.: 82.81%] [G loss: 2.776917]\n",
      "epoch:39 step:30495 [D loss: 0.303595, acc.: 89.06%] [G loss: 3.391996]\n",
      "epoch:39 step:30496 [D loss: 0.197601, acc.: 93.75%] [G loss: 4.018785]\n",
      "epoch:39 step:30497 [D loss: 0.366984, acc.: 81.25%] [G loss: 3.620955]\n",
      "epoch:39 step:30498 [D loss: 0.203694, acc.: 92.97%] [G loss: 3.301534]\n",
      "epoch:39 step:30499 [D loss: 0.310551, acc.: 83.59%] [G loss: 3.320675]\n",
      "epoch:39 step:30500 [D loss: 0.305398, acc.: 85.16%] [G loss: 3.269442]\n",
      "epoch:39 step:30501 [D loss: 0.292464, acc.: 85.94%] [G loss: 2.831928]\n",
      "epoch:39 step:30502 [D loss: 0.442292, acc.: 75.78%] [G loss: 3.481895]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:39 step:30503 [D loss: 0.332353, acc.: 85.94%] [G loss: 2.865746]\n",
      "epoch:39 step:30504 [D loss: 0.303627, acc.: 87.50%] [G loss: 3.254561]\n",
      "epoch:39 step:30505 [D loss: 0.349588, acc.: 83.59%] [G loss: 4.361124]\n",
      "epoch:39 step:30506 [D loss: 0.398913, acc.: 79.69%] [G loss: 3.625896]\n",
      "epoch:39 step:30507 [D loss: 0.185818, acc.: 94.53%] [G loss: 3.641395]\n",
      "epoch:39 step:30508 [D loss: 0.228541, acc.: 89.84%] [G loss: 3.425456]\n",
      "epoch:39 step:30509 [D loss: 0.275127, acc.: 85.94%] [G loss: 4.025376]\n",
      "epoch:39 step:30510 [D loss: 0.388874, acc.: 82.81%] [G loss: 3.475287]\n",
      "epoch:39 step:30511 [D loss: 0.192919, acc.: 92.19%] [G loss: 3.690375]\n",
      "epoch:39 step:30512 [D loss: 0.195382, acc.: 92.97%] [G loss: 2.685674]\n",
      "epoch:39 step:30513 [D loss: 0.336299, acc.: 85.16%] [G loss: 2.913711]\n",
      "epoch:39 step:30514 [D loss: 0.297986, acc.: 87.50%] [G loss: 2.526359]\n",
      "epoch:39 step:30515 [D loss: 0.468047, acc.: 78.91%] [G loss: 2.940242]\n",
      "epoch:39 step:30516 [D loss: 0.476114, acc.: 76.56%] [G loss: 4.487242]\n",
      "epoch:39 step:30517 [D loss: 0.457500, acc.: 75.00%] [G loss: 3.277928]\n",
      "epoch:39 step:30518 [D loss: 0.346268, acc.: 84.38%] [G loss: 3.541573]\n",
      "epoch:39 step:30519 [D loss: 0.412610, acc.: 82.03%] [G loss: 3.727852]\n",
      "epoch:39 step:30520 [D loss: 0.351417, acc.: 81.25%] [G loss: 3.685687]\n",
      "epoch:39 step:30521 [D loss: 0.288381, acc.: 85.94%] [G loss: 4.539023]\n",
      "epoch:39 step:30522 [D loss: 0.236355, acc.: 90.62%] [G loss: 2.828257]\n",
      "epoch:39 step:30523 [D loss: 0.347641, acc.: 85.94%] [G loss: 4.231166]\n",
      "epoch:39 step:30524 [D loss: 0.276709, acc.: 85.16%] [G loss: 3.906506]\n",
      "epoch:39 step:30525 [D loss: 0.291712, acc.: 88.28%] [G loss: 4.370030]\n",
      "epoch:39 step:30526 [D loss: 0.340162, acc.: 87.50%] [G loss: 3.457823]\n",
      "epoch:39 step:30527 [D loss: 0.270624, acc.: 89.06%] [G loss: 3.592322]\n",
      "epoch:39 step:30528 [D loss: 0.240314, acc.: 89.06%] [G loss: 3.656404]\n",
      "epoch:39 step:30529 [D loss: 0.361811, acc.: 83.59%] [G loss: 3.615422]\n",
      "epoch:39 step:30530 [D loss: 0.378193, acc.: 82.03%] [G loss: 3.712078]\n",
      "epoch:39 step:30531 [D loss: 0.308980, acc.: 85.94%] [G loss: 3.642214]\n",
      "epoch:39 step:30532 [D loss: 0.596758, acc.: 72.66%] [G loss: 2.525671]\n",
      "epoch:39 step:30533 [D loss: 0.427524, acc.: 82.03%] [G loss: 3.258354]\n",
      "epoch:39 step:30534 [D loss: 0.450817, acc.: 82.81%] [G loss: 3.015609]\n",
      "epoch:39 step:30535 [D loss: 0.327060, acc.: 85.94%] [G loss: 2.933489]\n",
      "epoch:39 step:30536 [D loss: 0.301969, acc.: 82.81%] [G loss: 2.788540]\n",
      "epoch:39 step:30537 [D loss: 0.445374, acc.: 77.34%] [G loss: 2.435562]\n",
      "epoch:39 step:30538 [D loss: 0.395280, acc.: 84.38%] [G loss: 2.685511]\n",
      "epoch:39 step:30539 [D loss: 0.282344, acc.: 89.06%] [G loss: 2.405926]\n",
      "epoch:39 step:30540 [D loss: 0.368008, acc.: 82.81%] [G loss: 4.547469]\n",
      "epoch:39 step:30541 [D loss: 0.368266, acc.: 82.81%] [G loss: 5.713162]\n",
      "epoch:39 step:30542 [D loss: 0.453943, acc.: 75.78%] [G loss: 5.456743]\n",
      "epoch:39 step:30543 [D loss: 0.284443, acc.: 85.94%] [G loss: 3.918355]\n",
      "epoch:39 step:30544 [D loss: 0.302936, acc.: 85.16%] [G loss: 4.636260]\n",
      "epoch:39 step:30545 [D loss: 0.297968, acc.: 86.72%] [G loss: 3.883347]\n",
      "epoch:39 step:30546 [D loss: 0.303386, acc.: 87.50%] [G loss: 3.096777]\n",
      "epoch:39 step:30547 [D loss: 0.325041, acc.: 85.94%] [G loss: 2.710392]\n",
      "epoch:39 step:30548 [D loss: 0.314948, acc.: 85.16%] [G loss: 3.352018]\n",
      "epoch:39 step:30549 [D loss: 0.457754, acc.: 80.47%] [G loss: 2.812376]\n",
      "epoch:39 step:30550 [D loss: 0.250342, acc.: 91.41%] [G loss: 3.170464]\n",
      "epoch:39 step:30551 [D loss: 0.295768, acc.: 88.28%] [G loss: 3.249068]\n",
      "epoch:39 step:30552 [D loss: 0.335962, acc.: 83.59%] [G loss: 3.221141]\n",
      "epoch:39 step:30553 [D loss: 0.258292, acc.: 89.84%] [G loss: 3.200354]\n",
      "epoch:39 step:30554 [D loss: 0.341488, acc.: 85.16%] [G loss: 2.792042]\n",
      "epoch:39 step:30555 [D loss: 0.280105, acc.: 85.94%] [G loss: 2.786325]\n",
      "epoch:39 step:30556 [D loss: 0.382559, acc.: 81.25%] [G loss: 2.815413]\n",
      "epoch:39 step:30557 [D loss: 0.272542, acc.: 89.06%] [G loss: 2.954040]\n",
      "epoch:39 step:30558 [D loss: 0.320871, acc.: 83.59%] [G loss: 2.422572]\n",
      "epoch:39 step:30559 [D loss: 0.344983, acc.: 85.16%] [G loss: 2.405422]\n",
      "epoch:39 step:30560 [D loss: 0.362181, acc.: 79.69%] [G loss: 2.543934]\n",
      "epoch:39 step:30561 [D loss: 0.297623, acc.: 84.38%] [G loss: 3.552586]\n",
      "epoch:39 step:30562 [D loss: 0.337421, acc.: 85.94%] [G loss: 3.234555]\n",
      "epoch:39 step:30563 [D loss: 0.307977, acc.: 87.50%] [G loss: 2.635012]\n",
      "epoch:39 step:30564 [D loss: 0.289630, acc.: 89.84%] [G loss: 3.354906]\n",
      "epoch:39 step:30565 [D loss: 0.363263, acc.: 80.47%] [G loss: 3.254691]\n",
      "epoch:39 step:30566 [D loss: 0.223182, acc.: 89.84%] [G loss: 2.785186]\n",
      "epoch:39 step:30567 [D loss: 0.325472, acc.: 82.81%] [G loss: 3.387817]\n",
      "epoch:39 step:30568 [D loss: 0.323691, acc.: 88.28%] [G loss: 3.159536]\n",
      "epoch:39 step:30569 [D loss: 0.394455, acc.: 84.38%] [G loss: 3.412610]\n",
      "epoch:39 step:30570 [D loss: 0.405812, acc.: 83.59%] [G loss: 2.771532]\n",
      "epoch:39 step:30571 [D loss: 0.414688, acc.: 80.47%] [G loss: 2.836927]\n",
      "epoch:39 step:30572 [D loss: 0.466841, acc.: 77.34%] [G loss: 2.789611]\n",
      "epoch:39 step:30573 [D loss: 0.421366, acc.: 78.91%] [G loss: 4.790970]\n",
      "epoch:39 step:30574 [D loss: 0.387060, acc.: 85.16%] [G loss: 3.999166]\n",
      "epoch:39 step:30575 [D loss: 0.399209, acc.: 86.72%] [G loss: 5.105875]\n",
      "epoch:39 step:30576 [D loss: 0.300343, acc.: 85.94%] [G loss: 4.337408]\n",
      "epoch:39 step:30577 [D loss: 0.451181, acc.: 81.25%] [G loss: 5.311511]\n",
      "epoch:39 step:30578 [D loss: 0.422470, acc.: 80.47%] [G loss: 3.872511]\n",
      "epoch:39 step:30579 [D loss: 0.390483, acc.: 82.03%] [G loss: 4.016156]\n",
      "epoch:39 step:30580 [D loss: 0.263074, acc.: 89.06%] [G loss: 3.555097]\n",
      "epoch:39 step:30581 [D loss: 0.281135, acc.: 88.28%] [G loss: 3.511532]\n",
      "epoch:39 step:30582 [D loss: 0.370038, acc.: 83.59%] [G loss: 3.129667]\n",
      "epoch:39 step:30583 [D loss: 0.359183, acc.: 85.94%] [G loss: 2.815266]\n",
      "epoch:39 step:30584 [D loss: 0.318829, acc.: 89.06%] [G loss: 3.403181]\n",
      "epoch:39 step:30585 [D loss: 0.347620, acc.: 86.72%] [G loss: 2.455268]\n",
      "epoch:39 step:30586 [D loss: 0.326508, acc.: 85.94%] [G loss: 2.491574]\n",
      "epoch:39 step:30587 [D loss: 0.258551, acc.: 89.06%] [G loss: 4.925041]\n",
      "epoch:39 step:30588 [D loss: 0.311959, acc.: 87.50%] [G loss: 3.914213]\n",
      "epoch:39 step:30589 [D loss: 0.243121, acc.: 87.50%] [G loss: 3.059662]\n",
      "epoch:39 step:30590 [D loss: 0.288750, acc.: 89.06%] [G loss: 3.760672]\n",
      "epoch:39 step:30591 [D loss: 0.339979, acc.: 84.38%] [G loss: 3.154511]\n",
      "epoch:39 step:30592 [D loss: 0.312956, acc.: 85.94%] [G loss: 2.482040]\n",
      "epoch:39 step:30593 [D loss: 0.354650, acc.: 85.16%] [G loss: 3.608000]\n",
      "epoch:39 step:30594 [D loss: 0.268741, acc.: 90.62%] [G loss: 2.934631]\n",
      "epoch:39 step:30595 [D loss: 0.290473, acc.: 85.94%] [G loss: 3.377627]\n",
      "epoch:39 step:30596 [D loss: 0.280173, acc.: 89.84%] [G loss: 2.109690]\n",
      "epoch:39 step:30597 [D loss: 0.359710, acc.: 82.03%] [G loss: 2.988610]\n",
      "epoch:39 step:30598 [D loss: 0.318638, acc.: 86.72%] [G loss: 3.054410]\n",
      "epoch:39 step:30599 [D loss: 0.281535, acc.: 85.94%] [G loss: 2.589415]\n",
      "epoch:39 step:30600 [D loss: 0.257032, acc.: 89.84%] [G loss: 2.915304]\n",
      "##############\n",
      "[0.8576705  0.88159193 0.79764972 0.82764222 0.79394506 0.82838692\n",
      " 0.87083721 0.83288629 0.81778807 0.83731111]\n",
      "##########\n",
      "epoch:39 step:30601 [D loss: 0.441344, acc.: 79.69%] [G loss: 2.915195]\n",
      "epoch:39 step:30602 [D loss: 0.353279, acc.: 84.38%] [G loss: 2.885807]\n",
      "epoch:39 step:30603 [D loss: 0.411747, acc.: 79.69%] [G loss: 3.445130]\n",
      "epoch:39 step:30604 [D loss: 0.303692, acc.: 84.38%] [G loss: 2.368812]\n",
      "epoch:39 step:30605 [D loss: 0.329623, acc.: 86.72%] [G loss: 2.892108]\n",
      "epoch:39 step:30606 [D loss: 0.251710, acc.: 91.41%] [G loss: 3.284108]\n",
      "epoch:39 step:30607 [D loss: 0.245260, acc.: 92.19%] [G loss: 3.538188]\n",
      "epoch:39 step:30608 [D loss: 0.331444, acc.: 86.72%] [G loss: 3.375593]\n",
      "epoch:39 step:30609 [D loss: 0.257064, acc.: 88.28%] [G loss: 3.745255]\n",
      "epoch:39 step:30610 [D loss: 0.235079, acc.: 90.62%] [G loss: 3.667239]\n",
      "epoch:39 step:30611 [D loss: 0.277279, acc.: 88.28%] [G loss: 2.823273]\n",
      "epoch:39 step:30612 [D loss: 0.294479, acc.: 85.16%] [G loss: 3.944129]\n",
      "epoch:39 step:30613 [D loss: 0.301376, acc.: 84.38%] [G loss: 3.084871]\n",
      "epoch:39 step:30614 [D loss: 0.366251, acc.: 82.81%] [G loss: 3.144794]\n",
      "epoch:39 step:30615 [D loss: 0.380315, acc.: 85.16%] [G loss: 3.122074]\n",
      "epoch:39 step:30616 [D loss: 0.355599, acc.: 82.81%] [G loss: 3.204088]\n",
      "epoch:39 step:30617 [D loss: 0.326033, acc.: 85.16%] [G loss: 2.913728]\n",
      "epoch:39 step:30618 [D loss: 0.415161, acc.: 78.12%] [G loss: 3.135486]\n",
      "epoch:39 step:30619 [D loss: 0.339624, acc.: 79.69%] [G loss: 3.668018]\n",
      "epoch:39 step:30620 [D loss: 0.246425, acc.: 87.50%] [G loss: 5.207793]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:39 step:30621 [D loss: 0.257638, acc.: 89.06%] [G loss: 5.574518]\n",
      "epoch:39 step:30622 [D loss: 0.204002, acc.: 92.19%] [G loss: 6.320522]\n",
      "epoch:39 step:30623 [D loss: 0.305475, acc.: 82.81%] [G loss: 4.290890]\n",
      "epoch:39 step:30624 [D loss: 0.306189, acc.: 82.81%] [G loss: 6.380295]\n",
      "epoch:39 step:30625 [D loss: 0.295815, acc.: 85.94%] [G loss: 4.570084]\n",
      "epoch:39 step:30626 [D loss: 0.280105, acc.: 89.06%] [G loss: 3.206501]\n",
      "epoch:39 step:30627 [D loss: 0.264425, acc.: 89.06%] [G loss: 3.476248]\n",
      "epoch:39 step:30628 [D loss: 0.383584, acc.: 80.47%] [G loss: 2.793222]\n",
      "epoch:39 step:30629 [D loss: 0.311110, acc.: 86.72%] [G loss: 2.636840]\n",
      "epoch:39 step:30630 [D loss: 0.442426, acc.: 79.69%] [G loss: 3.347653]\n",
      "epoch:39 step:30631 [D loss: 0.213771, acc.: 89.84%] [G loss: 4.030036]\n",
      "epoch:39 step:30632 [D loss: 0.349710, acc.: 84.38%] [G loss: 4.182127]\n",
      "epoch:39 step:30633 [D loss: 0.473670, acc.: 82.81%] [G loss: 5.438226]\n",
      "epoch:39 step:30634 [D loss: 0.626799, acc.: 80.47%] [G loss: 3.424186]\n",
      "epoch:39 step:30635 [D loss: 0.674158, acc.: 69.53%] [G loss: 3.455589]\n",
      "epoch:39 step:30636 [D loss: 0.372771, acc.: 85.16%] [G loss: 2.814855]\n",
      "epoch:39 step:30637 [D loss: 0.375912, acc.: 82.03%] [G loss: 3.074790]\n",
      "epoch:39 step:30638 [D loss: 0.296175, acc.: 84.38%] [G loss: 3.426840]\n",
      "epoch:39 step:30639 [D loss: 0.297991, acc.: 91.41%] [G loss: 6.768027]\n",
      "epoch:39 step:30640 [D loss: 0.186705, acc.: 90.62%] [G loss: 5.232331]\n",
      "epoch:39 step:30641 [D loss: 0.233203, acc.: 89.84%] [G loss: 5.718111]\n",
      "epoch:39 step:30642 [D loss: 0.234127, acc.: 87.50%] [G loss: 4.611604]\n",
      "epoch:39 step:30643 [D loss: 0.300005, acc.: 85.94%] [G loss: 3.247697]\n",
      "epoch:39 step:30644 [D loss: 0.156067, acc.: 92.97%] [G loss: 3.788147]\n",
      "epoch:39 step:30645 [D loss: 0.318437, acc.: 82.81%] [G loss: 3.504303]\n",
      "epoch:39 step:30646 [D loss: 0.335419, acc.: 86.72%] [G loss: 2.750177]\n",
      "epoch:39 step:30647 [D loss: 0.374446, acc.: 82.81%] [G loss: 3.255749]\n",
      "epoch:39 step:30648 [D loss: 0.321230, acc.: 82.81%] [G loss: 2.822917]\n",
      "epoch:39 step:30649 [D loss: 0.304044, acc.: 86.72%] [G loss: 2.893607]\n",
      "epoch:39 step:30650 [D loss: 0.353625, acc.: 84.38%] [G loss: 2.732237]\n",
      "epoch:39 step:30651 [D loss: 0.290348, acc.: 89.84%] [G loss: 3.563808]\n",
      "epoch:39 step:30652 [D loss: 0.307993, acc.: 85.16%] [G loss: 4.100613]\n",
      "epoch:39 step:30653 [D loss: 0.265185, acc.: 89.06%] [G loss: 3.711544]\n",
      "epoch:39 step:30654 [D loss: 0.340270, acc.: 82.81%] [G loss: 2.942234]\n",
      "epoch:39 step:30655 [D loss: 0.280760, acc.: 85.16%] [G loss: 3.144467]\n",
      "epoch:39 step:30656 [D loss: 0.403989, acc.: 81.25%] [G loss: 2.569963]\n",
      "epoch:39 step:30657 [D loss: 0.248850, acc.: 91.41%] [G loss: 3.687586]\n",
      "epoch:39 step:30658 [D loss: 0.307705, acc.: 87.50%] [G loss: 3.173931]\n",
      "epoch:39 step:30659 [D loss: 0.376706, acc.: 80.47%] [G loss: 4.263373]\n",
      "epoch:39 step:30660 [D loss: 0.352111, acc.: 81.25%] [G loss: 4.684725]\n",
      "epoch:39 step:30661 [D loss: 0.359660, acc.: 85.94%] [G loss: 3.242574]\n",
      "epoch:39 step:30662 [D loss: 0.454679, acc.: 78.12%] [G loss: 2.412440]\n",
      "epoch:39 step:30663 [D loss: 0.307805, acc.: 85.94%] [G loss: 3.817671]\n",
      "epoch:39 step:30664 [D loss: 0.298773, acc.: 86.72%] [G loss: 3.477105]\n",
      "epoch:39 step:30665 [D loss: 0.394789, acc.: 85.16%] [G loss: 4.423699]\n",
      "epoch:39 step:30666 [D loss: 0.378786, acc.: 82.03%] [G loss: 3.323001]\n",
      "epoch:39 step:30667 [D loss: 0.276742, acc.: 84.38%] [G loss: 4.583230]\n",
      "epoch:39 step:30668 [D loss: 0.366990, acc.: 84.38%] [G loss: 4.668756]\n",
      "epoch:39 step:30669 [D loss: 0.355368, acc.: 85.94%] [G loss: 3.295213]\n",
      "epoch:39 step:30670 [D loss: 0.273374, acc.: 89.06%] [G loss: 3.974205]\n",
      "epoch:39 step:30671 [D loss: 0.310140, acc.: 87.50%] [G loss: 3.022933]\n",
      "epoch:39 step:30672 [D loss: 0.399562, acc.: 77.34%] [G loss: 2.887053]\n",
      "epoch:39 step:30673 [D loss: 0.337356, acc.: 83.59%] [G loss: 3.443877]\n",
      "epoch:39 step:30674 [D loss: 0.328831, acc.: 82.03%] [G loss: 2.827388]\n",
      "epoch:39 step:30675 [D loss: 0.376489, acc.: 82.81%] [G loss: 2.534407]\n",
      "epoch:39 step:30676 [D loss: 0.279537, acc.: 93.75%] [G loss: 2.717919]\n",
      "epoch:39 step:30677 [D loss: 0.304874, acc.: 85.94%] [G loss: 3.768473]\n",
      "epoch:39 step:30678 [D loss: 0.303205, acc.: 88.28%] [G loss: 3.549899]\n",
      "epoch:39 step:30679 [D loss: 0.330538, acc.: 84.38%] [G loss: 4.399291]\n",
      "epoch:39 step:30680 [D loss: 0.314991, acc.: 84.38%] [G loss: 3.257873]\n",
      "epoch:39 step:30681 [D loss: 0.334113, acc.: 89.06%] [G loss: 3.589104]\n",
      "epoch:39 step:30682 [D loss: 0.312178, acc.: 85.16%] [G loss: 3.753373]\n",
      "epoch:39 step:30683 [D loss: 0.371143, acc.: 81.25%] [G loss: 3.550643]\n",
      "epoch:39 step:30684 [D loss: 0.335519, acc.: 82.81%] [G loss: 2.973830]\n",
      "epoch:39 step:30685 [D loss: 0.304752, acc.: 86.72%] [G loss: 3.936438]\n",
      "epoch:39 step:30686 [D loss: 0.297954, acc.: 86.72%] [G loss: 3.236395]\n",
      "epoch:39 step:30687 [D loss: 0.311167, acc.: 83.59%] [G loss: 3.072192]\n",
      "epoch:39 step:30688 [D loss: 0.315056, acc.: 85.16%] [G loss: 3.516223]\n",
      "epoch:39 step:30689 [D loss: 0.217198, acc.: 92.97%] [G loss: 4.999740]\n",
      "epoch:39 step:30690 [D loss: 0.217922, acc.: 89.84%] [G loss: 4.323359]\n",
      "epoch:39 step:30691 [D loss: 0.235100, acc.: 88.28%] [G loss: 3.536799]\n",
      "epoch:39 step:30692 [D loss: 0.280099, acc.: 83.59%] [G loss: 3.491886]\n",
      "epoch:39 step:30693 [D loss: 0.294027, acc.: 88.28%] [G loss: 2.900295]\n",
      "epoch:39 step:30694 [D loss: 0.320094, acc.: 85.94%] [G loss: 3.073567]\n",
      "epoch:39 step:30695 [D loss: 0.300164, acc.: 81.25%] [G loss: 4.447696]\n",
      "epoch:39 step:30696 [D loss: 0.254526, acc.: 87.50%] [G loss: 3.233144]\n",
      "epoch:39 step:30697 [D loss: 0.239936, acc.: 88.28%] [G loss: 3.142327]\n",
      "epoch:39 step:30698 [D loss: 0.228318, acc.: 92.97%] [G loss: 3.262335]\n",
      "epoch:39 step:30699 [D loss: 0.298444, acc.: 85.94%] [G loss: 2.794089]\n",
      "epoch:39 step:30700 [D loss: 0.325855, acc.: 82.03%] [G loss: 3.232377]\n",
      "epoch:39 step:30701 [D loss: 0.397763, acc.: 78.12%] [G loss: 3.443296]\n",
      "epoch:39 step:30702 [D loss: 0.420538, acc.: 82.81%] [G loss: 3.076344]\n",
      "epoch:39 step:30703 [D loss: 0.212053, acc.: 92.19%] [G loss: 3.102264]\n",
      "epoch:39 step:30704 [D loss: 0.334079, acc.: 84.38%] [G loss: 2.628802]\n",
      "epoch:39 step:30705 [D loss: 0.369800, acc.: 82.81%] [G loss: 3.429767]\n",
      "epoch:39 step:30706 [D loss: 0.457460, acc.: 75.78%] [G loss: 4.359253]\n",
      "epoch:39 step:30707 [D loss: 0.302145, acc.: 86.72%] [G loss: 2.698051]\n",
      "epoch:39 step:30708 [D loss: 0.436790, acc.: 75.78%] [G loss: 3.697856]\n",
      "epoch:39 step:30709 [D loss: 0.744639, acc.: 67.97%] [G loss: 6.910274]\n",
      "epoch:39 step:30710 [D loss: 0.849921, acc.: 75.78%] [G loss: 4.028325]\n",
      "epoch:39 step:30711 [D loss: 0.444124, acc.: 75.78%] [G loss: 3.382854]\n",
      "epoch:39 step:30712 [D loss: 0.475315, acc.: 76.56%] [G loss: 3.777563]\n",
      "epoch:39 step:30713 [D loss: 0.263039, acc.: 89.06%] [G loss: 3.989727]\n",
      "epoch:39 step:30714 [D loss: 0.378671, acc.: 85.16%] [G loss: 2.931684]\n",
      "epoch:39 step:30715 [D loss: 0.345909, acc.: 82.81%] [G loss: 2.533306]\n",
      "epoch:39 step:30716 [D loss: 0.349874, acc.: 85.16%] [G loss: 2.904176]\n",
      "epoch:39 step:30717 [D loss: 0.347419, acc.: 85.94%] [G loss: 2.450856]\n",
      "epoch:39 step:30718 [D loss: 0.354743, acc.: 83.59%] [G loss: 2.347110]\n",
      "epoch:39 step:30719 [D loss: 0.230876, acc.: 92.19%] [G loss: 2.766421]\n",
      "epoch:39 step:30720 [D loss: 0.379031, acc.: 80.47%] [G loss: 2.674689]\n",
      "epoch:39 step:30721 [D loss: 0.423781, acc.: 82.81%] [G loss: 3.164159]\n",
      "epoch:39 step:30722 [D loss: 0.262898, acc.: 92.19%] [G loss: 3.281229]\n",
      "epoch:39 step:30723 [D loss: 0.441548, acc.: 79.69%] [G loss: 2.588491]\n",
      "epoch:39 step:30724 [D loss: 0.342887, acc.: 85.94%] [G loss: 2.838675]\n",
      "epoch:39 step:30725 [D loss: 0.282121, acc.: 88.28%] [G loss: 3.497461]\n",
      "epoch:39 step:30726 [D loss: 0.282437, acc.: 91.41%] [G loss: 3.936120]\n",
      "epoch:39 step:30727 [D loss: 0.231052, acc.: 91.41%] [G loss: 2.471041]\n",
      "epoch:39 step:30728 [D loss: 0.239041, acc.: 89.84%] [G loss: 4.248853]\n",
      "epoch:39 step:30729 [D loss: 0.292451, acc.: 86.72%] [G loss: 2.700723]\n",
      "epoch:39 step:30730 [D loss: 0.278149, acc.: 89.06%] [G loss: 3.120498]\n",
      "epoch:39 step:30731 [D loss: 0.267609, acc.: 89.84%] [G loss: 3.328470]\n",
      "epoch:39 step:30732 [D loss: 0.271118, acc.: 88.28%] [G loss: 2.921358]\n",
      "epoch:39 step:30733 [D loss: 0.278789, acc.: 89.06%] [G loss: 3.283532]\n",
      "epoch:39 step:30734 [D loss: 0.347131, acc.: 82.81%] [G loss: 3.027586]\n",
      "epoch:39 step:30735 [D loss: 0.347068, acc.: 85.16%] [G loss: 2.873741]\n",
      "epoch:39 step:30736 [D loss: 0.448379, acc.: 78.91%] [G loss: 2.996799]\n",
      "epoch:39 step:30737 [D loss: 0.313254, acc.: 85.94%] [G loss: 2.666689]\n",
      "epoch:39 step:30738 [D loss: 0.324703, acc.: 87.50%] [G loss: 3.471121]\n",
      "epoch:39 step:30739 [D loss: 0.282190, acc.: 90.62%] [G loss: 2.937140]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:39 step:30740 [D loss: 0.310635, acc.: 86.72%] [G loss: 3.100810]\n",
      "epoch:39 step:30741 [D loss: 0.318052, acc.: 85.94%] [G loss: 4.123981]\n",
      "epoch:39 step:30742 [D loss: 0.369830, acc.: 85.16%] [G loss: 9.076468]\n",
      "epoch:39 step:30743 [D loss: 0.595344, acc.: 78.12%] [G loss: 4.554488]\n",
      "epoch:39 step:30744 [D loss: 0.705175, acc.: 71.09%] [G loss: 3.779428]\n",
      "epoch:39 step:30745 [D loss: 0.397255, acc.: 78.91%] [G loss: 4.012942]\n",
      "epoch:39 step:30746 [D loss: 0.291900, acc.: 85.16%] [G loss: 3.389722]\n",
      "epoch:39 step:30747 [D loss: 0.384825, acc.: 81.25%] [G loss: 3.126882]\n",
      "epoch:39 step:30748 [D loss: 0.362026, acc.: 82.81%] [G loss: 2.960583]\n",
      "epoch:39 step:30749 [D loss: 0.361164, acc.: 85.94%] [G loss: 2.890443]\n",
      "epoch:39 step:30750 [D loss: 0.302925, acc.: 91.41%] [G loss: 2.658494]\n",
      "epoch:39 step:30751 [D loss: 0.316220, acc.: 83.59%] [G loss: 2.523712]\n",
      "epoch:39 step:30752 [D loss: 0.419147, acc.: 78.91%] [G loss: 2.666321]\n",
      "epoch:39 step:30753 [D loss: 0.348069, acc.: 87.50%] [G loss: 2.476253]\n",
      "epoch:39 step:30754 [D loss: 0.363852, acc.: 83.59%] [G loss: 2.843405]\n",
      "epoch:39 step:30755 [D loss: 0.228611, acc.: 91.41%] [G loss: 2.294525]\n",
      "epoch:39 step:30756 [D loss: 0.248866, acc.: 86.72%] [G loss: 2.892329]\n",
      "epoch:39 step:30757 [D loss: 0.363295, acc.: 82.81%] [G loss: 2.551968]\n",
      "epoch:39 step:30758 [D loss: 0.266239, acc.: 89.06%] [G loss: 3.137429]\n",
      "epoch:39 step:30759 [D loss: 0.329611, acc.: 86.72%] [G loss: 3.003571]\n",
      "epoch:39 step:30760 [D loss: 0.439205, acc.: 84.38%] [G loss: 2.477666]\n",
      "epoch:39 step:30761 [D loss: 0.340260, acc.: 82.03%] [G loss: 3.764806]\n",
      "epoch:39 step:30762 [D loss: 0.307581, acc.: 84.38%] [G loss: 3.034448]\n",
      "epoch:39 step:30763 [D loss: 0.230680, acc.: 89.84%] [G loss: 3.963266]\n",
      "epoch:39 step:30764 [D loss: 0.191584, acc.: 93.75%] [G loss: 4.168713]\n",
      "epoch:39 step:30765 [D loss: 0.244542, acc.: 89.06%] [G loss: 2.870712]\n",
      "epoch:39 step:30766 [D loss: 0.245194, acc.: 90.62%] [G loss: 3.258800]\n",
      "epoch:39 step:30767 [D loss: 0.300819, acc.: 85.94%] [G loss: 2.705006]\n",
      "epoch:39 step:30768 [D loss: 0.296661, acc.: 87.50%] [G loss: 3.597012]\n",
      "epoch:39 step:30769 [D loss: 0.285241, acc.: 89.06%] [G loss: 3.014136]\n",
      "epoch:39 step:30770 [D loss: 0.366533, acc.: 81.25%] [G loss: 2.731065]\n",
      "epoch:39 step:30771 [D loss: 0.272716, acc.: 89.84%] [G loss: 3.146547]\n",
      "epoch:39 step:30772 [D loss: 0.315759, acc.: 84.38%] [G loss: 4.025051]\n",
      "epoch:39 step:30773 [D loss: 0.232979, acc.: 95.31%] [G loss: 3.108726]\n",
      "epoch:39 step:30774 [D loss: 0.343417, acc.: 82.81%] [G loss: 3.504727]\n",
      "epoch:39 step:30775 [D loss: 0.433188, acc.: 84.38%] [G loss: 2.742134]\n",
      "epoch:39 step:30776 [D loss: 0.374044, acc.: 84.38%] [G loss: 3.118599]\n",
      "epoch:39 step:30777 [D loss: 0.414324, acc.: 78.12%] [G loss: 3.506764]\n",
      "epoch:39 step:30778 [D loss: 0.262097, acc.: 88.28%] [G loss: 2.954196]\n",
      "epoch:39 step:30779 [D loss: 0.468223, acc.: 78.12%] [G loss: 7.685448]\n",
      "epoch:39 step:30780 [D loss: 0.539792, acc.: 76.56%] [G loss: 3.425279]\n",
      "epoch:39 step:30781 [D loss: 0.376820, acc.: 84.38%] [G loss: 5.217232]\n",
      "epoch:39 step:30782 [D loss: 0.233434, acc.: 89.84%] [G loss: 4.495308]\n",
      "epoch:39 step:30783 [D loss: 0.266949, acc.: 89.06%] [G loss: 3.303653]\n",
      "epoch:39 step:30784 [D loss: 0.325484, acc.: 86.72%] [G loss: 3.578356]\n",
      "epoch:39 step:30785 [D loss: 0.269819, acc.: 89.06%] [G loss: 3.538068]\n",
      "epoch:39 step:30786 [D loss: 0.243073, acc.: 89.06%] [G loss: 3.531801]\n",
      "epoch:39 step:30787 [D loss: 0.274566, acc.: 89.84%] [G loss: 2.951261]\n",
      "epoch:39 step:30788 [D loss: 0.264527, acc.: 87.50%] [G loss: 3.608350]\n",
      "epoch:39 step:30789 [D loss: 0.299041, acc.: 88.28%] [G loss: 2.940563]\n",
      "epoch:39 step:30790 [D loss: 0.340589, acc.: 81.25%] [G loss: 3.861442]\n",
      "epoch:39 step:30791 [D loss: 0.280110, acc.: 87.50%] [G loss: 3.287380]\n",
      "epoch:39 step:30792 [D loss: 0.269050, acc.: 89.84%] [G loss: 4.248410]\n",
      "epoch:39 step:30793 [D loss: 0.251511, acc.: 89.06%] [G loss: 4.680688]\n",
      "epoch:39 step:30794 [D loss: 0.247168, acc.: 88.28%] [G loss: 3.314596]\n",
      "epoch:39 step:30795 [D loss: 0.202683, acc.: 91.41%] [G loss: 5.447244]\n",
      "epoch:39 step:30796 [D loss: 0.307741, acc.: 89.06%] [G loss: 3.552517]\n",
      "epoch:39 step:30797 [D loss: 0.279502, acc.: 89.84%] [G loss: 3.915586]\n",
      "epoch:39 step:30798 [D loss: 0.340483, acc.: 85.94%] [G loss: 3.615552]\n",
      "epoch:39 step:30799 [D loss: 0.215787, acc.: 91.41%] [G loss: 4.157778]\n",
      "epoch:39 step:30800 [D loss: 0.305757, acc.: 86.72%] [G loss: 3.296455]\n",
      "##############\n",
      "[0.86849509 0.86764978 0.79303361 0.8183509  0.75399237 0.82838911\n",
      " 0.87256557 0.81667115 0.81891983 0.82135348]\n",
      "##########\n",
      "epoch:39 step:30801 [D loss: 0.278915, acc.: 89.84%] [G loss: 2.649266]\n",
      "epoch:39 step:30802 [D loss: 0.407464, acc.: 82.03%] [G loss: 2.546595]\n",
      "epoch:39 step:30803 [D loss: 0.262981, acc.: 86.72%] [G loss: 2.627739]\n",
      "epoch:39 step:30804 [D loss: 0.250315, acc.: 89.84%] [G loss: 2.568038]\n",
      "epoch:39 step:30805 [D loss: 0.323059, acc.: 85.16%] [G loss: 3.095233]\n",
      "epoch:39 step:30806 [D loss: 0.272523, acc.: 89.84%] [G loss: 3.168828]\n",
      "epoch:39 step:30807 [D loss: 0.319125, acc.: 89.84%] [G loss: 2.445768]\n",
      "epoch:39 step:30808 [D loss: 0.304106, acc.: 85.94%] [G loss: 2.737560]\n",
      "epoch:39 step:30809 [D loss: 0.333639, acc.: 87.50%] [G loss: 3.735959]\n",
      "epoch:39 step:30810 [D loss: 0.412332, acc.: 78.91%] [G loss: 3.592657]\n",
      "epoch:39 step:30811 [D loss: 0.392968, acc.: 82.81%] [G loss: 3.073904]\n",
      "epoch:39 step:30812 [D loss: 0.292108, acc.: 86.72%] [G loss: 3.349485]\n",
      "epoch:39 step:30813 [D loss: 0.272889, acc.: 87.50%] [G loss: 3.428249]\n",
      "epoch:39 step:30814 [D loss: 0.234154, acc.: 93.75%] [G loss: 3.465063]\n",
      "epoch:39 step:30815 [D loss: 0.367409, acc.: 82.03%] [G loss: 2.944961]\n",
      "epoch:39 step:30816 [D loss: 0.326158, acc.: 86.72%] [G loss: 4.408949]\n",
      "epoch:39 step:30817 [D loss: 0.323616, acc.: 84.38%] [G loss: 4.322220]\n",
      "epoch:39 step:30818 [D loss: 0.228714, acc.: 90.62%] [G loss: 2.570765]\n",
      "epoch:39 step:30819 [D loss: 0.256926, acc.: 87.50%] [G loss: 4.500117]\n",
      "epoch:39 step:30820 [D loss: 0.299541, acc.: 85.16%] [G loss: 4.592368]\n",
      "epoch:39 step:30821 [D loss: 0.276021, acc.: 84.38%] [G loss: 2.876400]\n",
      "epoch:39 step:30822 [D loss: 0.347442, acc.: 83.59%] [G loss: 3.249761]\n",
      "epoch:39 step:30823 [D loss: 0.333176, acc.: 85.94%] [G loss: 3.373274]\n",
      "epoch:39 step:30824 [D loss: 0.261216, acc.: 87.50%] [G loss: 2.545314]\n",
      "epoch:39 step:30825 [D loss: 0.263358, acc.: 89.06%] [G loss: 3.239932]\n",
      "epoch:39 step:30826 [D loss: 0.333578, acc.: 84.38%] [G loss: 4.154499]\n",
      "epoch:39 step:30827 [D loss: 0.369279, acc.: 82.81%] [G loss: 2.656017]\n",
      "epoch:39 step:30828 [D loss: 0.272792, acc.: 89.06%] [G loss: 3.289603]\n",
      "epoch:39 step:30829 [D loss: 0.320451, acc.: 87.50%] [G loss: 2.354000]\n",
      "epoch:39 step:30830 [D loss: 0.307678, acc.: 89.06%] [G loss: 2.705786]\n",
      "epoch:39 step:30831 [D loss: 0.180517, acc.: 94.53%] [G loss: 3.203753]\n",
      "epoch:39 step:30832 [D loss: 0.343374, acc.: 89.84%] [G loss: 2.728900]\n",
      "epoch:39 step:30833 [D loss: 0.340442, acc.: 83.59%] [G loss: 2.374111]\n",
      "epoch:39 step:30834 [D loss: 0.279072, acc.: 89.06%] [G loss: 3.226293]\n",
      "epoch:39 step:30835 [D loss: 0.173610, acc.: 95.31%] [G loss: 3.828764]\n",
      "epoch:39 step:30836 [D loss: 0.277089, acc.: 87.50%] [G loss: 2.952460]\n",
      "epoch:39 step:30837 [D loss: 0.226565, acc.: 91.41%] [G loss: 4.290679]\n",
      "epoch:39 step:30838 [D loss: 0.327444, acc.: 85.94%] [G loss: 3.592145]\n",
      "epoch:39 step:30839 [D loss: 0.184884, acc.: 92.97%] [G loss: 4.497148]\n",
      "epoch:39 step:30840 [D loss: 0.277108, acc.: 89.06%] [G loss: 5.158744]\n",
      "epoch:39 step:30841 [D loss: 0.232110, acc.: 89.84%] [G loss: 4.503522]\n",
      "epoch:39 step:30842 [D loss: 0.390042, acc.: 84.38%] [G loss: 5.115010]\n",
      "epoch:39 step:30843 [D loss: 0.373673, acc.: 84.38%] [G loss: 5.148876]\n",
      "epoch:39 step:30844 [D loss: 0.355085, acc.: 85.16%] [G loss: 4.488468]\n",
      "epoch:39 step:30845 [D loss: 0.292515, acc.: 90.62%] [G loss: 4.054712]\n",
      "epoch:39 step:30846 [D loss: 0.264876, acc.: 87.50%] [G loss: 3.731167]\n",
      "epoch:39 step:30847 [D loss: 0.256586, acc.: 90.62%] [G loss: 2.917568]\n",
      "epoch:39 step:30848 [D loss: 0.296685, acc.: 85.16%] [G loss: 3.814873]\n",
      "epoch:39 step:30849 [D loss: 0.297253, acc.: 87.50%] [G loss: 4.713444]\n",
      "epoch:39 step:30850 [D loss: 0.309630, acc.: 85.16%] [G loss: 3.532534]\n",
      "epoch:39 step:30851 [D loss: 0.257094, acc.: 86.72%] [G loss: 5.531572]\n",
      "epoch:39 step:30852 [D loss: 0.290406, acc.: 87.50%] [G loss: 4.958221]\n",
      "epoch:39 step:30853 [D loss: 0.209265, acc.: 92.19%] [G loss: 4.403162]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:39 step:30854 [D loss: 0.316109, acc.: 86.72%] [G loss: 5.015381]\n",
      "epoch:39 step:30855 [D loss: 0.206182, acc.: 89.84%] [G loss: 4.590881]\n",
      "epoch:39 step:30856 [D loss: 0.346145, acc.: 81.25%] [G loss: 3.249545]\n",
      "epoch:39 step:30857 [D loss: 0.213898, acc.: 90.62%] [G loss: 3.505650]\n",
      "epoch:39 step:30858 [D loss: 0.418109, acc.: 77.34%] [G loss: 3.585234]\n",
      "epoch:39 step:30859 [D loss: 0.394706, acc.: 79.69%] [G loss: 4.199107]\n",
      "epoch:39 step:30860 [D loss: 0.239619, acc.: 92.19%] [G loss: 3.832001]\n",
      "epoch:39 step:30861 [D loss: 0.299423, acc.: 86.72%] [G loss: 3.425229]\n",
      "epoch:39 step:30862 [D loss: 0.269697, acc.: 88.28%] [G loss: 3.308891]\n",
      "epoch:39 step:30863 [D loss: 0.241017, acc.: 89.06%] [G loss: 3.313519]\n",
      "epoch:39 step:30864 [D loss: 0.369625, acc.: 80.47%] [G loss: 2.857692]\n",
      "epoch:39 step:30865 [D loss: 0.326687, acc.: 84.38%] [G loss: 3.100726]\n",
      "epoch:39 step:30866 [D loss: 0.285372, acc.: 87.50%] [G loss: 2.405200]\n",
      "epoch:39 step:30867 [D loss: 0.299632, acc.: 85.16%] [G loss: 2.949202]\n",
      "epoch:39 step:30868 [D loss: 0.327581, acc.: 83.59%] [G loss: 3.197559]\n",
      "epoch:39 step:30869 [D loss: 0.320007, acc.: 85.16%] [G loss: 2.719431]\n",
      "epoch:39 step:30870 [D loss: 0.351261, acc.: 83.59%] [G loss: 3.561850]\n",
      "epoch:39 step:30871 [D loss: 0.435922, acc.: 78.91%] [G loss: 3.197861]\n",
      "epoch:39 step:30872 [D loss: 0.298509, acc.: 85.94%] [G loss: 2.978977]\n",
      "epoch:39 step:30873 [D loss: 0.291497, acc.: 86.72%] [G loss: 3.233321]\n",
      "epoch:39 step:30874 [D loss: 0.292124, acc.: 87.50%] [G loss: 2.955443]\n",
      "epoch:39 step:30875 [D loss: 0.219408, acc.: 89.06%] [G loss: 2.872748]\n",
      "epoch:39 step:30876 [D loss: 0.381479, acc.: 84.38%] [G loss: 3.289002]\n",
      "epoch:39 step:30877 [D loss: 0.314631, acc.: 87.50%] [G loss: 2.745054]\n",
      "epoch:39 step:30878 [D loss: 0.390773, acc.: 79.69%] [G loss: 2.993789]\n",
      "epoch:39 step:30879 [D loss: 0.280166, acc.: 89.06%] [G loss: 3.250114]\n",
      "epoch:39 step:30880 [D loss: 0.255488, acc.: 89.06%] [G loss: 3.555412]\n",
      "epoch:39 step:30881 [D loss: 0.400432, acc.: 81.25%] [G loss: 3.097104]\n",
      "epoch:39 step:30882 [D loss: 0.433728, acc.: 76.56%] [G loss: 3.805949]\n",
      "epoch:39 step:30883 [D loss: 0.362843, acc.: 80.47%] [G loss: 4.359976]\n",
      "epoch:39 step:30884 [D loss: 0.392693, acc.: 81.25%] [G loss: 3.130108]\n",
      "epoch:39 step:30885 [D loss: 0.301670, acc.: 86.72%] [G loss: 3.833885]\n",
      "epoch:39 step:30886 [D loss: 0.327030, acc.: 84.38%] [G loss: 3.098902]\n",
      "epoch:39 step:30887 [D loss: 0.443246, acc.: 78.91%] [G loss: 3.327497]\n",
      "epoch:39 step:30888 [D loss: 0.342583, acc.: 85.16%] [G loss: 2.781467]\n",
      "epoch:39 step:30889 [D loss: 0.286118, acc.: 86.72%] [G loss: 2.694318]\n",
      "epoch:39 step:30890 [D loss: 0.351877, acc.: 86.72%] [G loss: 3.043030]\n",
      "epoch:39 step:30891 [D loss: 0.393127, acc.: 83.59%] [G loss: 2.752007]\n",
      "epoch:39 step:30892 [D loss: 0.307887, acc.: 84.38%] [G loss: 3.135776]\n",
      "epoch:39 step:30893 [D loss: 0.348869, acc.: 84.38%] [G loss: 4.320797]\n",
      "epoch:39 step:30894 [D loss: 0.379416, acc.: 82.03%] [G loss: 5.540737]\n",
      "epoch:39 step:30895 [D loss: 0.621556, acc.: 71.88%] [G loss: 6.403813]\n",
      "epoch:39 step:30896 [D loss: 1.488985, acc.: 58.59%] [G loss: 7.174622]\n",
      "epoch:39 step:30897 [D loss: 1.012602, acc.: 70.31%] [G loss: 4.082544]\n",
      "epoch:39 step:30898 [D loss: 0.383714, acc.: 78.91%] [G loss: 3.675797]\n",
      "epoch:39 step:30899 [D loss: 0.386361, acc.: 81.25%] [G loss: 4.158144]\n",
      "epoch:39 step:30900 [D loss: 0.441226, acc.: 80.47%] [G loss: 2.520768]\n",
      "epoch:39 step:30901 [D loss: 0.338582, acc.: 84.38%] [G loss: 3.275015]\n",
      "epoch:39 step:30902 [D loss: 0.233736, acc.: 90.62%] [G loss: 3.786433]\n",
      "epoch:39 step:30903 [D loss: 0.339275, acc.: 82.81%] [G loss: 2.440697]\n",
      "epoch:39 step:30904 [D loss: 0.371571, acc.: 83.59%] [G loss: 5.637341]\n",
      "epoch:39 step:30905 [D loss: 0.252481, acc.: 89.06%] [G loss: 5.530726]\n",
      "epoch:39 step:30906 [D loss: 0.224694, acc.: 89.06%] [G loss: 7.410659]\n",
      "epoch:39 step:30907 [D loss: 0.237141, acc.: 90.62%] [G loss: 6.039698]\n",
      "epoch:39 step:30908 [D loss: 0.223726, acc.: 90.62%] [G loss: 5.421200]\n",
      "epoch:39 step:30909 [D loss: 0.210266, acc.: 92.97%] [G loss: 4.482520]\n",
      "epoch:39 step:30910 [D loss: 0.233956, acc.: 87.50%] [G loss: 4.037223]\n",
      "epoch:39 step:30911 [D loss: 0.292829, acc.: 88.28%] [G loss: 3.319899]\n",
      "epoch:39 step:30912 [D loss: 0.198368, acc.: 92.19%] [G loss: 2.879262]\n",
      "epoch:39 step:30913 [D loss: 0.340526, acc.: 86.72%] [G loss: 3.031512]\n",
      "epoch:39 step:30914 [D loss: 0.341499, acc.: 83.59%] [G loss: 3.377408]\n",
      "epoch:39 step:30915 [D loss: 0.421723, acc.: 78.91%] [G loss: 2.363948]\n",
      "epoch:39 step:30916 [D loss: 0.220460, acc.: 91.41%] [G loss: 2.871049]\n",
      "epoch:39 step:30917 [D loss: 0.272115, acc.: 92.97%] [G loss: 3.132452]\n",
      "epoch:39 step:30918 [D loss: 0.344224, acc.: 84.38%] [G loss: 3.521255]\n",
      "epoch:39 step:30919 [D loss: 0.361971, acc.: 82.81%] [G loss: 2.671438]\n",
      "epoch:39 step:30920 [D loss: 0.316359, acc.: 89.06%] [G loss: 3.093489]\n",
      "epoch:39 step:30921 [D loss: 0.321213, acc.: 88.28%] [G loss: 3.086673]\n",
      "epoch:39 step:30922 [D loss: 0.378756, acc.: 82.03%] [G loss: 2.556212]\n",
      "epoch:39 step:30923 [D loss: 0.422232, acc.: 80.47%] [G loss: 2.609814]\n",
      "epoch:39 step:30924 [D loss: 0.348280, acc.: 83.59%] [G loss: 2.617565]\n",
      "epoch:39 step:30925 [D loss: 0.389774, acc.: 84.38%] [G loss: 2.771814]\n",
      "epoch:39 step:30926 [D loss: 0.370539, acc.: 82.81%] [G loss: 2.768740]\n",
      "epoch:39 step:30927 [D loss: 0.261368, acc.: 89.84%] [G loss: 2.729446]\n",
      "epoch:39 step:30928 [D loss: 0.255717, acc.: 89.84%] [G loss: 2.382459]\n",
      "epoch:39 step:30929 [D loss: 0.238210, acc.: 88.28%] [G loss: 3.319829]\n",
      "epoch:39 step:30930 [D loss: 0.299068, acc.: 86.72%] [G loss: 3.668941]\n",
      "epoch:39 step:30931 [D loss: 0.345422, acc.: 85.94%] [G loss: 2.887093]\n",
      "epoch:39 step:30932 [D loss: 0.399473, acc.: 81.25%] [G loss: 2.868599]\n",
      "epoch:39 step:30933 [D loss: 0.299547, acc.: 85.16%] [G loss: 2.876016]\n",
      "epoch:39 step:30934 [D loss: 0.276045, acc.: 86.72%] [G loss: 5.893956]\n",
      "epoch:39 step:30935 [D loss: 0.288828, acc.: 89.84%] [G loss: 3.994368]\n",
      "epoch:39 step:30936 [D loss: 0.249179, acc.: 87.50%] [G loss: 3.632289]\n",
      "epoch:39 step:30937 [D loss: 0.299752, acc.: 85.16%] [G loss: 3.559247]\n",
      "epoch:39 step:30938 [D loss: 0.389476, acc.: 81.25%] [G loss: 3.866040]\n",
      "epoch:39 step:30939 [D loss: 0.213089, acc.: 92.97%] [G loss: 4.209881]\n",
      "epoch:39 step:30940 [D loss: 0.358195, acc.: 83.59%] [G loss: 4.186588]\n",
      "epoch:39 step:30941 [D loss: 0.394401, acc.: 78.91%] [G loss: 3.507183]\n",
      "epoch:39 step:30942 [D loss: 0.235071, acc.: 93.75%] [G loss: 2.627822]\n",
      "epoch:39 step:30943 [D loss: 0.299528, acc.: 86.72%] [G loss: 2.560289]\n",
      "epoch:39 step:30944 [D loss: 0.351111, acc.: 82.81%] [G loss: 3.169117]\n",
      "epoch:39 step:30945 [D loss: 0.287449, acc.: 89.06%] [G loss: 3.816794]\n",
      "epoch:39 step:30946 [D loss: 0.329048, acc.: 83.59%] [G loss: 3.106456]\n",
      "epoch:39 step:30947 [D loss: 0.266460, acc.: 90.62%] [G loss: 3.195815]\n",
      "epoch:39 step:30948 [D loss: 0.220319, acc.: 91.41%] [G loss: 4.951677]\n",
      "epoch:39 step:30949 [D loss: 0.218233, acc.: 92.97%] [G loss: 3.718525]\n",
      "epoch:39 step:30950 [D loss: 0.291578, acc.: 86.72%] [G loss: 3.462063]\n",
      "epoch:39 step:30951 [D loss: 0.331452, acc.: 84.38%] [G loss: 3.475953]\n",
      "epoch:39 step:30952 [D loss: 0.276669, acc.: 88.28%] [G loss: 4.869383]\n",
      "epoch:39 step:30953 [D loss: 0.243822, acc.: 87.50%] [G loss: 4.698065]\n",
      "epoch:39 step:30954 [D loss: 0.267223, acc.: 89.06%] [G loss: 4.955255]\n",
      "epoch:39 step:30955 [D loss: 0.226851, acc.: 92.19%] [G loss: 5.182590]\n",
      "epoch:39 step:30956 [D loss: 0.187577, acc.: 89.84%] [G loss: 4.442514]\n",
      "epoch:39 step:30957 [D loss: 0.282014, acc.: 88.28%] [G loss: 3.208536]\n",
      "epoch:39 step:30958 [D loss: 0.280618, acc.: 85.94%] [G loss: 3.888543]\n",
      "epoch:39 step:30959 [D loss: 0.397612, acc.: 82.81%] [G loss: 3.311706]\n",
      "epoch:39 step:30960 [D loss: 0.287611, acc.: 86.72%] [G loss: 3.256375]\n",
      "epoch:39 step:30961 [D loss: 0.231784, acc.: 91.41%] [G loss: 3.660656]\n",
      "epoch:39 step:30962 [D loss: 0.330184, acc.: 83.59%] [G loss: 3.949115]\n",
      "epoch:39 step:30963 [D loss: 0.221397, acc.: 89.06%] [G loss: 5.040973]\n",
      "epoch:39 step:30964 [D loss: 0.189732, acc.: 92.97%] [G loss: 3.978716]\n",
      "epoch:39 step:30965 [D loss: 0.294954, acc.: 88.28%] [G loss: 3.123343]\n",
      "epoch:39 step:30966 [D loss: 0.241391, acc.: 89.84%] [G loss: 3.627202]\n",
      "epoch:39 step:30967 [D loss: 0.308453, acc.: 85.16%] [G loss: 2.444350]\n",
      "epoch:39 step:30968 [D loss: 0.269322, acc.: 89.06%] [G loss: 3.229351]\n",
      "epoch:39 step:30969 [D loss: 0.364002, acc.: 82.81%] [G loss: 2.881743]\n",
      "epoch:39 step:30970 [D loss: 0.351696, acc.: 85.16%] [G loss: 3.000048]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:39 step:30971 [D loss: 0.232514, acc.: 89.06%] [G loss: 2.873892]\n",
      "epoch:39 step:30972 [D loss: 0.380823, acc.: 85.94%] [G loss: 2.634094]\n",
      "epoch:39 step:30973 [D loss: 0.351518, acc.: 83.59%] [G loss: 3.808024]\n",
      "epoch:39 step:30974 [D loss: 0.283591, acc.: 91.41%] [G loss: 3.957382]\n",
      "epoch:39 step:30975 [D loss: 0.461832, acc.: 79.69%] [G loss: 3.136714]\n",
      "epoch:39 step:30976 [D loss: 0.282534, acc.: 91.41%] [G loss: 3.515879]\n",
      "epoch:39 step:30977 [D loss: 0.266477, acc.: 89.84%] [G loss: 2.726579]\n",
      "epoch:39 step:30978 [D loss: 0.290452, acc.: 87.50%] [G loss: 2.946312]\n",
      "epoch:39 step:30979 [D loss: 0.227050, acc.: 90.62%] [G loss: 3.366367]\n",
      "epoch:39 step:30980 [D loss: 0.252098, acc.: 89.84%] [G loss: 3.923300]\n",
      "epoch:39 step:30981 [D loss: 0.296185, acc.: 86.72%] [G loss: 3.172054]\n",
      "epoch:39 step:30982 [D loss: 0.275468, acc.: 88.28%] [G loss: 3.389096]\n",
      "epoch:39 step:30983 [D loss: 0.361665, acc.: 82.81%] [G loss: 3.884552]\n",
      "epoch:39 step:30984 [D loss: 0.284322, acc.: 85.94%] [G loss: 3.448294]\n",
      "epoch:39 step:30985 [D loss: 0.271573, acc.: 88.28%] [G loss: 2.843187]\n",
      "epoch:39 step:30986 [D loss: 0.344429, acc.: 83.59%] [G loss: 2.825696]\n",
      "epoch:39 step:30987 [D loss: 0.447210, acc.: 78.91%] [G loss: 3.514869]\n",
      "epoch:39 step:30988 [D loss: 0.402699, acc.: 80.47%] [G loss: 4.420066]\n",
      "epoch:39 step:30989 [D loss: 0.428286, acc.: 81.25%] [G loss: 4.725583]\n",
      "epoch:39 step:30990 [D loss: 0.490776, acc.: 79.69%] [G loss: 6.254667]\n",
      "epoch:39 step:30991 [D loss: 1.173952, acc.: 65.62%] [G loss: 9.067164]\n",
      "epoch:39 step:30992 [D loss: 1.441214, acc.: 62.50%] [G loss: 4.315310]\n",
      "epoch:39 step:30993 [D loss: 0.256989, acc.: 86.72%] [G loss: 4.215376]\n",
      "epoch:39 step:30994 [D loss: 0.321610, acc.: 83.59%] [G loss: 3.605800]\n",
      "epoch:39 step:30995 [D loss: 0.374499, acc.: 80.47%] [G loss: 3.136615]\n",
      "epoch:39 step:30996 [D loss: 0.334548, acc.: 82.03%] [G loss: 3.088398]\n",
      "epoch:39 step:30997 [D loss: 0.307929, acc.: 88.28%] [G loss: 3.076547]\n",
      "epoch:39 step:30998 [D loss: 0.333718, acc.: 85.94%] [G loss: 3.346770]\n",
      "epoch:39 step:30999 [D loss: 0.272048, acc.: 87.50%] [G loss: 2.876353]\n",
      "epoch:39 step:31000 [D loss: 0.324556, acc.: 86.72%] [G loss: 2.327642]\n",
      "##############\n",
      "[0.87997396 0.87340532 0.81154216 0.83236349 0.77868363 0.84379897\n",
      " 0.87098305 0.82744893 0.82888132 0.82505001]\n",
      "##########\n",
      "epoch:39 step:31001 [D loss: 0.394629, acc.: 82.81%] [G loss: 2.456023]\n",
      "epoch:39 step:31002 [D loss: 0.344386, acc.: 85.94%] [G loss: 2.770095]\n",
      "epoch:39 step:31003 [D loss: 0.389510, acc.: 82.03%] [G loss: 2.734436]\n",
      "epoch:39 step:31004 [D loss: 0.300436, acc.: 89.06%] [G loss: 2.833545]\n",
      "epoch:39 step:31005 [D loss: 0.308606, acc.: 86.72%] [G loss: 3.036374]\n",
      "epoch:39 step:31006 [D loss: 0.278249, acc.: 88.28%] [G loss: 3.045721]\n",
      "epoch:39 step:31007 [D loss: 0.228651, acc.: 90.62%] [G loss: 3.060739]\n",
      "epoch:39 step:31008 [D loss: 0.335448, acc.: 87.50%] [G loss: 5.899221]\n",
      "epoch:39 step:31009 [D loss: 0.442447, acc.: 78.91%] [G loss: 2.678457]\n",
      "epoch:39 step:31010 [D loss: 0.231383, acc.: 89.84%] [G loss: 3.755215]\n",
      "epoch:39 step:31011 [D loss: 0.414322, acc.: 82.03%] [G loss: 2.554278]\n",
      "epoch:39 step:31012 [D loss: 0.232450, acc.: 90.62%] [G loss: 3.165991]\n",
      "epoch:39 step:31013 [D loss: 0.327600, acc.: 87.50%] [G loss: 3.258697]\n",
      "epoch:39 step:31014 [D loss: 0.348657, acc.: 83.59%] [G loss: 2.982680]\n",
      "epoch:39 step:31015 [D loss: 0.356423, acc.: 82.81%] [G loss: 3.064826]\n",
      "epoch:39 step:31016 [D loss: 0.295489, acc.: 89.06%] [G loss: 2.566532]\n",
      "epoch:39 step:31017 [D loss: 0.280588, acc.: 89.84%] [G loss: 3.000733]\n",
      "epoch:39 step:31018 [D loss: 0.381736, acc.: 80.47%] [G loss: 2.563660]\n",
      "epoch:39 step:31019 [D loss: 0.319415, acc.: 83.59%] [G loss: 2.660942]\n",
      "epoch:39 step:31020 [D loss: 0.318236, acc.: 82.03%] [G loss: 3.342029]\n",
      "epoch:39 step:31021 [D loss: 0.290823, acc.: 84.38%] [G loss: 2.392059]\n",
      "epoch:39 step:31022 [D loss: 0.277165, acc.: 89.06%] [G loss: 3.150512]\n",
      "epoch:39 step:31023 [D loss: 0.431826, acc.: 82.03%] [G loss: 3.270219]\n",
      "epoch:39 step:31024 [D loss: 0.263719, acc.: 89.06%] [G loss: 2.453816]\n",
      "epoch:39 step:31025 [D loss: 0.298007, acc.: 88.28%] [G loss: 3.935830]\n",
      "epoch:39 step:31026 [D loss: 0.334701, acc.: 87.50%] [G loss: 3.265640]\n",
      "epoch:39 step:31027 [D loss: 0.308419, acc.: 86.72%] [G loss: 3.323650]\n",
      "epoch:39 step:31028 [D loss: 0.222905, acc.: 89.06%] [G loss: 3.788052]\n",
      "epoch:39 step:31029 [D loss: 0.319451, acc.: 88.28%] [G loss: 3.854120]\n",
      "epoch:39 step:31030 [D loss: 0.323277, acc.: 86.72%] [G loss: 2.693057]\n",
      "epoch:39 step:31031 [D loss: 0.264869, acc.: 86.72%] [G loss: 3.762433]\n",
      "epoch:39 step:31032 [D loss: 0.342532, acc.: 85.94%] [G loss: 3.719237]\n",
      "epoch:39 step:31033 [D loss: 0.249184, acc.: 89.84%] [G loss: 3.595694]\n",
      "epoch:39 step:31034 [D loss: 0.374064, acc.: 83.59%] [G loss: 2.954548]\n",
      "epoch:39 step:31035 [D loss: 0.363500, acc.: 82.81%] [G loss: 2.864088]\n",
      "epoch:39 step:31036 [D loss: 0.220425, acc.: 91.41%] [G loss: 2.976347]\n",
      "epoch:39 step:31037 [D loss: 0.354732, acc.: 82.03%] [G loss: 3.716957]\n",
      "epoch:39 step:31038 [D loss: 0.307744, acc.: 85.94%] [G loss: 2.722834]\n",
      "epoch:39 step:31039 [D loss: 0.289187, acc.: 91.41%] [G loss: 2.548692]\n",
      "epoch:39 step:31040 [D loss: 0.310435, acc.: 87.50%] [G loss: 2.919668]\n",
      "epoch:39 step:31041 [D loss: 0.244161, acc.: 88.28%] [G loss: 3.928871]\n",
      "epoch:39 step:31042 [D loss: 0.333783, acc.: 87.50%] [G loss: 3.178475]\n",
      "epoch:39 step:31043 [D loss: 0.253612, acc.: 89.06%] [G loss: 3.408949]\n",
      "epoch:39 step:31044 [D loss: 0.260596, acc.: 87.50%] [G loss: 2.259674]\n",
      "epoch:39 step:31045 [D loss: 0.300947, acc.: 86.72%] [G loss: 2.321122]\n",
      "epoch:39 step:31046 [D loss: 0.289413, acc.: 86.72%] [G loss: 2.314010]\n",
      "epoch:39 step:31047 [D loss: 0.276123, acc.: 90.62%] [G loss: 4.043868]\n",
      "epoch:39 step:31048 [D loss: 0.345526, acc.: 84.38%] [G loss: 3.678068]\n",
      "epoch:39 step:31049 [D loss: 0.341827, acc.: 83.59%] [G loss: 2.115856]\n",
      "epoch:39 step:31050 [D loss: 0.328226, acc.: 86.72%] [G loss: 3.570067]\n",
      "epoch:39 step:31051 [D loss: 0.321712, acc.: 85.16%] [G loss: 2.856127]\n",
      "epoch:39 step:31052 [D loss: 0.261917, acc.: 92.19%] [G loss: 2.840884]\n",
      "epoch:39 step:31053 [D loss: 0.323747, acc.: 85.94%] [G loss: 3.237194]\n",
      "epoch:39 step:31054 [D loss: 0.376322, acc.: 83.59%] [G loss: 6.288645]\n",
      "epoch:39 step:31055 [D loss: 0.800909, acc.: 72.66%] [G loss: 5.987086]\n",
      "epoch:39 step:31056 [D loss: 0.892805, acc.: 78.12%] [G loss: 6.190218]\n",
      "epoch:39 step:31057 [D loss: 0.604719, acc.: 71.09%] [G loss: 3.968421]\n",
      "epoch:39 step:31058 [D loss: 0.345830, acc.: 84.38%] [G loss: 3.899428]\n",
      "epoch:39 step:31059 [D loss: 0.471871, acc.: 76.56%] [G loss: 4.458522]\n",
      "epoch:39 step:31060 [D loss: 0.286639, acc.: 87.50%] [G loss: 4.003991]\n",
      "epoch:39 step:31061 [D loss: 0.295482, acc.: 89.06%] [G loss: 2.723332]\n",
      "epoch:39 step:31062 [D loss: 0.314208, acc.: 85.16%] [G loss: 5.628032]\n",
      "epoch:39 step:31063 [D loss: 0.319811, acc.: 85.16%] [G loss: 3.110246]\n",
      "epoch:39 step:31064 [D loss: 0.319778, acc.: 83.59%] [G loss: 4.568387]\n",
      "epoch:39 step:31065 [D loss: 0.346655, acc.: 82.81%] [G loss: 4.320869]\n",
      "epoch:39 step:31066 [D loss: 0.488610, acc.: 75.00%] [G loss: 4.050107]\n",
      "epoch:39 step:31067 [D loss: 0.238137, acc.: 89.06%] [G loss: 4.863016]\n",
      "epoch:39 step:31068 [D loss: 0.365965, acc.: 81.25%] [G loss: 3.371312]\n",
      "epoch:39 step:31069 [D loss: 0.422663, acc.: 82.03%] [G loss: 4.684491]\n",
      "epoch:39 step:31070 [D loss: 0.364717, acc.: 85.94%] [G loss: 2.716224]\n",
      "epoch:39 step:31071 [D loss: 0.188748, acc.: 92.97%] [G loss: 3.278154]\n",
      "epoch:39 step:31072 [D loss: 0.242256, acc.: 90.62%] [G loss: 3.171086]\n",
      "epoch:39 step:31073 [D loss: 0.229370, acc.: 89.06%] [G loss: 2.967185]\n",
      "epoch:39 step:31074 [D loss: 0.242031, acc.: 89.84%] [G loss: 3.453361]\n",
      "epoch:39 step:31075 [D loss: 0.237941, acc.: 88.28%] [G loss: 3.705394]\n",
      "epoch:39 step:31076 [D loss: 0.386274, acc.: 80.47%] [G loss: 2.582587]\n",
      "epoch:39 step:31077 [D loss: 0.314883, acc.: 84.38%] [G loss: 3.568042]\n",
      "epoch:39 step:31078 [D loss: 0.254967, acc.: 88.28%] [G loss: 2.673508]\n",
      "epoch:39 step:31079 [D loss: 0.266771, acc.: 89.06%] [G loss: 3.171152]\n",
      "epoch:39 step:31080 [D loss: 0.232809, acc.: 93.75%] [G loss: 2.346358]\n",
      "epoch:39 step:31081 [D loss: 0.316819, acc.: 85.16%] [G loss: 3.131373]\n",
      "epoch:39 step:31082 [D loss: 0.273992, acc.: 89.84%] [G loss: 2.864783]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:39 step:31083 [D loss: 0.324630, acc.: 88.28%] [G loss: 3.763248]\n",
      "epoch:39 step:31084 [D loss: 0.316952, acc.: 83.59%] [G loss: 2.758508]\n",
      "epoch:39 step:31085 [D loss: 0.260346, acc.: 87.50%] [G loss: 3.133186]\n",
      "epoch:39 step:31086 [D loss: 0.278808, acc.: 89.06%] [G loss: 3.053768]\n",
      "epoch:39 step:31087 [D loss: 0.340718, acc.: 85.94%] [G loss: 3.105778]\n",
      "epoch:39 step:31088 [D loss: 0.303263, acc.: 89.06%] [G loss: 2.750687]\n",
      "epoch:39 step:31089 [D loss: 0.329706, acc.: 86.72%] [G loss: 2.279962]\n",
      "epoch:39 step:31090 [D loss: 0.351015, acc.: 82.03%] [G loss: 2.731173]\n",
      "epoch:39 step:31091 [D loss: 0.365210, acc.: 82.81%] [G loss: 3.264295]\n",
      "epoch:39 step:31092 [D loss: 0.291223, acc.: 89.06%] [G loss: 2.938318]\n",
      "epoch:39 step:31093 [D loss: 0.340096, acc.: 86.72%] [G loss: 2.569534]\n",
      "epoch:39 step:31094 [D loss: 0.344892, acc.: 87.50%] [G loss: 3.069546]\n",
      "epoch:39 step:31095 [D loss: 0.260154, acc.: 88.28%] [G loss: 3.274824]\n",
      "epoch:39 step:31096 [D loss: 0.292407, acc.: 86.72%] [G loss: 3.039062]\n",
      "epoch:39 step:31097 [D loss: 0.290751, acc.: 85.94%] [G loss: 2.961003]\n",
      "epoch:39 step:31098 [D loss: 0.395383, acc.: 81.25%] [G loss: 3.709585]\n",
      "epoch:39 step:31099 [D loss: 0.246665, acc.: 89.06%] [G loss: 2.921451]\n",
      "epoch:39 step:31100 [D loss: 0.324357, acc.: 84.38%] [G loss: 3.856390]\n",
      "epoch:39 step:31101 [D loss: 0.255153, acc.: 91.41%] [G loss: 2.773542]\n",
      "epoch:39 step:31102 [D loss: 0.412324, acc.: 84.38%] [G loss: 3.396561]\n",
      "epoch:39 step:31103 [D loss: 0.257904, acc.: 89.84%] [G loss: 4.843918]\n",
      "epoch:39 step:31104 [D loss: 0.233811, acc.: 89.06%] [G loss: 3.272614]\n",
      "epoch:39 step:31105 [D loss: 0.304803, acc.: 87.50%] [G loss: 3.163299]\n",
      "epoch:39 step:31106 [D loss: 0.227574, acc.: 91.41%] [G loss: 3.996087]\n",
      "epoch:39 step:31107 [D loss: 0.312194, acc.: 84.38%] [G loss: 3.340955]\n",
      "epoch:39 step:31108 [D loss: 0.313941, acc.: 84.38%] [G loss: 3.699687]\n",
      "epoch:39 step:31109 [D loss: 0.430381, acc.: 79.69%] [G loss: 5.387903]\n",
      "epoch:39 step:31110 [D loss: 0.465688, acc.: 78.12%] [G loss: 4.010326]\n",
      "epoch:39 step:31111 [D loss: 0.242835, acc.: 86.72%] [G loss: 3.071393]\n",
      "epoch:39 step:31112 [D loss: 0.241669, acc.: 89.84%] [G loss: 3.104598]\n",
      "epoch:39 step:31113 [D loss: 0.321223, acc.: 85.16%] [G loss: 3.694031]\n",
      "epoch:39 step:31114 [D loss: 0.442690, acc.: 79.69%] [G loss: 3.128020]\n",
      "epoch:39 step:31115 [D loss: 0.241992, acc.: 89.84%] [G loss: 4.381899]\n",
      "epoch:39 step:31116 [D loss: 0.345192, acc.: 84.38%] [G loss: 3.375592]\n",
      "epoch:39 step:31117 [D loss: 0.276737, acc.: 89.84%] [G loss: 3.377868]\n",
      "epoch:39 step:31118 [D loss: 0.315161, acc.: 89.84%] [G loss: 3.105501]\n",
      "epoch:39 step:31119 [D loss: 0.454726, acc.: 76.56%] [G loss: 2.187076]\n",
      "epoch:39 step:31120 [D loss: 0.335961, acc.: 82.03%] [G loss: 2.775337]\n",
      "epoch:39 step:31121 [D loss: 0.266034, acc.: 89.06%] [G loss: 2.959351]\n",
      "epoch:39 step:31122 [D loss: 0.307473, acc.: 87.50%] [G loss: 2.653497]\n",
      "epoch:39 step:31123 [D loss: 0.436523, acc.: 78.91%] [G loss: 2.913996]\n",
      "epoch:39 step:31124 [D loss: 0.268244, acc.: 88.28%] [G loss: 2.569330]\n",
      "epoch:39 step:31125 [D loss: 0.319354, acc.: 83.59%] [G loss: 2.850634]\n",
      "epoch:39 step:31126 [D loss: 0.383815, acc.: 82.81%] [G loss: 3.045792]\n",
      "epoch:39 step:31127 [D loss: 0.306169, acc.: 85.94%] [G loss: 2.904911]\n",
      "epoch:39 step:31128 [D loss: 0.410248, acc.: 82.03%] [G loss: 2.747247]\n",
      "epoch:39 step:31129 [D loss: 0.285355, acc.: 87.50%] [G loss: 3.042272]\n",
      "epoch:39 step:31130 [D loss: 0.382225, acc.: 81.25%] [G loss: 3.243452]\n",
      "epoch:39 step:31131 [D loss: 0.314674, acc.: 86.72%] [G loss: 3.271620]\n",
      "epoch:39 step:31132 [D loss: 0.336772, acc.: 87.50%] [G loss: 3.015635]\n",
      "epoch:39 step:31133 [D loss: 0.243447, acc.: 91.41%] [G loss: 2.414677]\n",
      "epoch:39 step:31134 [D loss: 0.289281, acc.: 89.84%] [G loss: 2.863051]\n",
      "epoch:39 step:31135 [D loss: 0.316320, acc.: 86.72%] [G loss: 2.619674]\n",
      "epoch:39 step:31136 [D loss: 0.337968, acc.: 86.72%] [G loss: 2.961514]\n",
      "epoch:39 step:31137 [D loss: 0.386584, acc.: 84.38%] [G loss: 3.047036]\n",
      "epoch:39 step:31138 [D loss: 0.367834, acc.: 83.59%] [G loss: 3.317539]\n",
      "epoch:39 step:31139 [D loss: 0.445289, acc.: 80.47%] [G loss: 2.661957]\n",
      "epoch:39 step:31140 [D loss: 0.330285, acc.: 83.59%] [G loss: 2.865617]\n",
      "epoch:39 step:31141 [D loss: 0.439380, acc.: 78.12%] [G loss: 2.459989]\n",
      "epoch:39 step:31142 [D loss: 0.261273, acc.: 88.28%] [G loss: 3.489455]\n",
      "epoch:39 step:31143 [D loss: 0.345526, acc.: 85.94%] [G loss: 3.036199]\n",
      "epoch:39 step:31144 [D loss: 0.375212, acc.: 83.59%] [G loss: 4.667292]\n",
      "epoch:39 step:31145 [D loss: 0.449833, acc.: 78.12%] [G loss: 5.648351]\n",
      "epoch:39 step:31146 [D loss: 0.313801, acc.: 85.94%] [G loss: 2.891233]\n",
      "epoch:39 step:31147 [D loss: 0.282015, acc.: 87.50%] [G loss: 2.901050]\n",
      "epoch:39 step:31148 [D loss: 0.264636, acc.: 90.62%] [G loss: 2.877611]\n",
      "epoch:39 step:31149 [D loss: 0.314899, acc.: 86.72%] [G loss: 2.924557]\n",
      "epoch:39 step:31150 [D loss: 0.304537, acc.: 86.72%] [G loss: 3.803061]\n",
      "epoch:39 step:31151 [D loss: 0.341542, acc.: 85.94%] [G loss: 3.228021]\n",
      "epoch:39 step:31152 [D loss: 0.338475, acc.: 86.72%] [G loss: 3.728367]\n",
      "epoch:39 step:31153 [D loss: 0.267835, acc.: 89.06%] [G loss: 3.361261]\n",
      "epoch:39 step:31154 [D loss: 0.292783, acc.: 85.94%] [G loss: 4.145597]\n",
      "epoch:39 step:31155 [D loss: 0.392831, acc.: 81.25%] [G loss: 3.147534]\n",
      "epoch:39 step:31156 [D loss: 0.286030, acc.: 88.28%] [G loss: 3.797173]\n",
      "epoch:39 step:31157 [D loss: 0.315666, acc.: 89.06%] [G loss: 2.766799]\n",
      "epoch:39 step:31158 [D loss: 0.276226, acc.: 89.84%] [G loss: 2.492487]\n",
      "epoch:39 step:31159 [D loss: 0.274406, acc.: 89.06%] [G loss: 3.575671]\n",
      "epoch:39 step:31160 [D loss: 0.346801, acc.: 89.06%] [G loss: 2.615548]\n",
      "epoch:39 step:31161 [D loss: 0.209024, acc.: 93.75%] [G loss: 2.977268]\n",
      "epoch:39 step:31162 [D loss: 0.354276, acc.: 81.25%] [G loss: 4.015220]\n",
      "epoch:39 step:31163 [D loss: 0.268617, acc.: 87.50%] [G loss: 3.008135]\n",
      "epoch:39 step:31164 [D loss: 0.285938, acc.: 86.72%] [G loss: 3.356184]\n",
      "epoch:39 step:31165 [D loss: 0.308569, acc.: 86.72%] [G loss: 2.960932]\n",
      "epoch:39 step:31166 [D loss: 0.327553, acc.: 84.38%] [G loss: 1.982797]\n",
      "epoch:39 step:31167 [D loss: 0.305662, acc.: 86.72%] [G loss: 2.765469]\n",
      "epoch:39 step:31168 [D loss: 0.331180, acc.: 83.59%] [G loss: 3.087471]\n",
      "epoch:39 step:31169 [D loss: 0.304203, acc.: 86.72%] [G loss: 2.497516]\n",
      "epoch:39 step:31170 [D loss: 0.208560, acc.: 92.97%] [G loss: 3.082204]\n",
      "epoch:39 step:31171 [D loss: 0.320689, acc.: 82.03%] [G loss: 4.265585]\n",
      "epoch:39 step:31172 [D loss: 0.387983, acc.: 80.47%] [G loss: 4.706956]\n",
      "epoch:39 step:31173 [D loss: 0.325609, acc.: 87.50%] [G loss: 3.883030]\n",
      "epoch:39 step:31174 [D loss: 0.291993, acc.: 87.50%] [G loss: 4.253076]\n",
      "epoch:39 step:31175 [D loss: 0.322311, acc.: 85.16%] [G loss: 3.132791]\n",
      "epoch:39 step:31176 [D loss: 0.269562, acc.: 89.06%] [G loss: 4.059771]\n",
      "epoch:39 step:31177 [D loss: 0.251283, acc.: 90.62%] [G loss: 4.383926]\n",
      "epoch:39 step:31178 [D loss: 0.284309, acc.: 85.16%] [G loss: 3.969823]\n",
      "epoch:39 step:31179 [D loss: 0.288817, acc.: 89.06%] [G loss: 3.922652]\n",
      "epoch:39 step:31180 [D loss: 0.404319, acc.: 83.59%] [G loss: 4.377770]\n",
      "epoch:39 step:31181 [D loss: 0.344529, acc.: 83.59%] [G loss: 3.297163]\n",
      "epoch:39 step:31182 [D loss: 0.266907, acc.: 87.50%] [G loss: 4.569567]\n",
      "epoch:39 step:31183 [D loss: 0.209480, acc.: 90.62%] [G loss: 4.023932]\n",
      "epoch:39 step:31184 [D loss: 0.323998, acc.: 78.91%] [G loss: 3.838765]\n",
      "epoch:39 step:31185 [D loss: 0.218079, acc.: 89.06%] [G loss: 5.336596]\n",
      "epoch:39 step:31186 [D loss: 0.363119, acc.: 84.38%] [G loss: 3.355711]\n",
      "epoch:39 step:31187 [D loss: 0.311327, acc.: 85.94%] [G loss: 3.855620]\n",
      "epoch:39 step:31188 [D loss: 0.281536, acc.: 84.38%] [G loss: 5.070228]\n",
      "epoch:39 step:31189 [D loss: 0.267553, acc.: 88.28%] [G loss: 3.452559]\n",
      "epoch:39 step:31190 [D loss: 0.384328, acc.: 82.81%] [G loss: 2.974587]\n",
      "epoch:39 step:31191 [D loss: 0.248664, acc.: 87.50%] [G loss: 3.726299]\n",
      "epoch:39 step:31192 [D loss: 0.312556, acc.: 85.16%] [G loss: 2.986818]\n",
      "epoch:39 step:31193 [D loss: 0.332013, acc.: 85.16%] [G loss: 2.860683]\n",
      "epoch:39 step:31194 [D loss: 0.308035, acc.: 88.28%] [G loss: 3.858083]\n",
      "epoch:39 step:31195 [D loss: 0.303571, acc.: 89.06%] [G loss: 3.351714]\n",
      "epoch:39 step:31196 [D loss: 0.280104, acc.: 88.28%] [G loss: 2.702917]\n",
      "epoch:39 step:31197 [D loss: 0.388111, acc.: 83.59%] [G loss: 2.648794]\n",
      "epoch:39 step:31198 [D loss: 0.327094, acc.: 84.38%] [G loss: 6.207157]\n",
      "epoch:39 step:31199 [D loss: 0.366058, acc.: 81.25%] [G loss: 4.568199]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:39 step:31200 [D loss: 0.205019, acc.: 92.97%] [G loss: 6.537778]\n",
      "##############\n",
      "[0.85166553 0.86458012 0.81403547 0.8222109  0.80142825 0.83013042\n",
      " 0.87635464 0.82271609 0.81680457 0.82029065]\n",
      "##########\n",
      "epoch:39 step:31201 [D loss: 0.258258, acc.: 88.28%] [G loss: 5.857513]\n",
      "epoch:39 step:31202 [D loss: 0.236177, acc.: 89.06%] [G loss: 3.722612]\n",
      "epoch:39 step:31203 [D loss: 0.226794, acc.: 89.84%] [G loss: 4.652822]\n",
      "epoch:39 step:31204 [D loss: 0.317380, acc.: 84.38%] [G loss: 3.507651]\n",
      "epoch:39 step:31205 [D loss: 0.291986, acc.: 88.28%] [G loss: 3.102592]\n",
      "epoch:39 step:31206 [D loss: 0.263105, acc.: 90.62%] [G loss: 3.410566]\n",
      "epoch:39 step:31207 [D loss: 0.355282, acc.: 81.25%] [G loss: 3.862071]\n",
      "epoch:39 step:31208 [D loss: 0.295119, acc.: 87.50%] [G loss: 3.842809]\n",
      "epoch:39 step:31209 [D loss: 0.260321, acc.: 87.50%] [G loss: 3.410604]\n",
      "epoch:39 step:31210 [D loss: 0.279127, acc.: 91.41%] [G loss: 3.598204]\n",
      "epoch:39 step:31211 [D loss: 0.248677, acc.: 89.06%] [G loss: 3.250946]\n",
      "epoch:39 step:31212 [D loss: 0.272231, acc.: 87.50%] [G loss: 6.067959]\n",
      "epoch:39 step:31213 [D loss: 0.254676, acc.: 89.06%] [G loss: 3.393773]\n",
      "epoch:39 step:31214 [D loss: 0.279960, acc.: 85.16%] [G loss: 3.649361]\n",
      "epoch:39 step:31215 [D loss: 0.261007, acc.: 86.72%] [G loss: 3.788877]\n",
      "epoch:39 step:31216 [D loss: 0.322962, acc.: 85.94%] [G loss: 3.530899]\n",
      "epoch:39 step:31217 [D loss: 0.303897, acc.: 86.72%] [G loss: 4.249329]\n",
      "epoch:39 step:31218 [D loss: 0.262066, acc.: 89.06%] [G loss: 3.979054]\n",
      "epoch:39 step:31219 [D loss: 0.245078, acc.: 85.94%] [G loss: 2.757382]\n",
      "epoch:39 step:31220 [D loss: 0.262802, acc.: 89.84%] [G loss: 3.130099]\n",
      "epoch:39 step:31221 [D loss: 0.345733, acc.: 82.03%] [G loss: 2.934333]\n",
      "epoch:39 step:31222 [D loss: 0.440867, acc.: 78.91%] [G loss: 3.300839]\n",
      "epoch:39 step:31223 [D loss: 0.271676, acc.: 87.50%] [G loss: 3.642605]\n",
      "epoch:39 step:31224 [D loss: 0.418715, acc.: 82.03%] [G loss: 6.585447]\n",
      "epoch:39 step:31225 [D loss: 0.622195, acc.: 71.09%] [G loss: 3.004344]\n",
      "epoch:39 step:31226 [D loss: 0.309614, acc.: 87.50%] [G loss: 3.945412]\n",
      "epoch:39 step:31227 [D loss: 0.356828, acc.: 85.94%] [G loss: 3.609558]\n",
      "epoch:39 step:31228 [D loss: 0.292039, acc.: 89.06%] [G loss: 3.609799]\n",
      "epoch:39 step:31229 [D loss: 0.305549, acc.: 85.16%] [G loss: 2.793617]\n",
      "epoch:39 step:31230 [D loss: 0.369567, acc.: 81.25%] [G loss: 3.751455]\n",
      "epoch:39 step:31231 [D loss: 0.275308, acc.: 89.06%] [G loss: 3.848644]\n",
      "epoch:39 step:31232 [D loss: 0.306146, acc.: 89.84%] [G loss: 3.488833]\n",
      "epoch:39 step:31233 [D loss: 0.297547, acc.: 85.94%] [G loss: 2.860632]\n",
      "epoch:39 step:31234 [D loss: 0.303568, acc.: 88.28%] [G loss: 2.761325]\n",
      "epoch:39 step:31235 [D loss: 0.244813, acc.: 89.06%] [G loss: 3.677850]\n",
      "epoch:39 step:31236 [D loss: 0.455115, acc.: 78.91%] [G loss: 2.843632]\n",
      "epoch:39 step:31237 [D loss: 0.244488, acc.: 85.94%] [G loss: 3.621590]\n",
      "epoch:39 step:31238 [D loss: 0.326571, acc.: 85.16%] [G loss: 3.425486]\n",
      "epoch:39 step:31239 [D loss: 0.411314, acc.: 85.16%] [G loss: 7.976998]\n",
      "epoch:39 step:31240 [D loss: 0.473771, acc.: 80.47%] [G loss: 4.358044]\n",
      "epoch:40 step:31241 [D loss: 0.361812, acc.: 82.03%] [G loss: 3.952621]\n",
      "epoch:40 step:31242 [D loss: 0.277341, acc.: 90.62%] [G loss: 4.617076]\n",
      "epoch:40 step:31243 [D loss: 0.352591, acc.: 82.03%] [G loss: 3.871754]\n",
      "epoch:40 step:31244 [D loss: 0.371069, acc.: 82.03%] [G loss: 3.769032]\n",
      "epoch:40 step:31245 [D loss: 0.322479, acc.: 84.38%] [G loss: 3.058220]\n",
      "epoch:40 step:31246 [D loss: 0.193610, acc.: 93.75%] [G loss: 2.767269]\n",
      "epoch:40 step:31247 [D loss: 0.234094, acc.: 91.41%] [G loss: 3.153745]\n",
      "epoch:40 step:31248 [D loss: 0.306133, acc.: 84.38%] [G loss: 3.542410]\n",
      "epoch:40 step:31249 [D loss: 0.343164, acc.: 85.94%] [G loss: 2.744809]\n",
      "epoch:40 step:31250 [D loss: 0.247520, acc.: 89.06%] [G loss: 2.988073]\n",
      "epoch:40 step:31251 [D loss: 0.429004, acc.: 79.69%] [G loss: 3.163422]\n",
      "epoch:40 step:31252 [D loss: 0.293210, acc.: 84.38%] [G loss: 2.930068]\n",
      "epoch:40 step:31253 [D loss: 0.321547, acc.: 83.59%] [G loss: 2.724579]\n",
      "epoch:40 step:31254 [D loss: 0.368207, acc.: 85.94%] [G loss: 2.313133]\n",
      "epoch:40 step:31255 [D loss: 0.287158, acc.: 82.81%] [G loss: 2.832918]\n",
      "epoch:40 step:31256 [D loss: 0.288612, acc.: 89.84%] [G loss: 3.071263]\n",
      "epoch:40 step:31257 [D loss: 0.244412, acc.: 92.19%] [G loss: 3.689584]\n",
      "epoch:40 step:31258 [D loss: 0.294426, acc.: 85.16%] [G loss: 3.313722]\n",
      "epoch:40 step:31259 [D loss: 0.307280, acc.: 89.06%] [G loss: 2.792462]\n",
      "epoch:40 step:31260 [D loss: 0.327632, acc.: 83.59%] [G loss: 2.651437]\n",
      "epoch:40 step:31261 [D loss: 0.340255, acc.: 88.28%] [G loss: 3.060903]\n",
      "epoch:40 step:31262 [D loss: 0.382863, acc.: 82.81%] [G loss: 3.098190]\n",
      "epoch:40 step:31263 [D loss: 0.341909, acc.: 83.59%] [G loss: 2.605105]\n",
      "epoch:40 step:31264 [D loss: 0.290653, acc.: 85.16%] [G loss: 3.498052]\n",
      "epoch:40 step:31265 [D loss: 0.259065, acc.: 92.97%] [G loss: 2.605345]\n",
      "epoch:40 step:31266 [D loss: 0.188850, acc.: 92.97%] [G loss: 4.243261]\n",
      "epoch:40 step:31267 [D loss: 0.318918, acc.: 85.16%] [G loss: 3.247622]\n",
      "epoch:40 step:31268 [D loss: 0.320540, acc.: 82.81%] [G loss: 3.917177]\n",
      "epoch:40 step:31269 [D loss: 0.446035, acc.: 77.34%] [G loss: 2.795216]\n",
      "epoch:40 step:31270 [D loss: 0.269279, acc.: 87.50%] [G loss: 2.661024]\n",
      "epoch:40 step:31271 [D loss: 0.301617, acc.: 87.50%] [G loss: 3.039798]\n",
      "epoch:40 step:31272 [D loss: 0.305365, acc.: 85.16%] [G loss: 2.679242]\n",
      "epoch:40 step:31273 [D loss: 0.238107, acc.: 91.41%] [G loss: 3.110597]\n",
      "epoch:40 step:31274 [D loss: 0.290704, acc.: 85.94%] [G loss: 2.980865]\n",
      "epoch:40 step:31275 [D loss: 0.297527, acc.: 88.28%] [G loss: 3.738903]\n",
      "epoch:40 step:31276 [D loss: 0.292400, acc.: 85.16%] [G loss: 4.040664]\n",
      "epoch:40 step:31277 [D loss: 0.270875, acc.: 87.50%] [G loss: 4.180796]\n",
      "epoch:40 step:31278 [D loss: 0.280803, acc.: 89.84%] [G loss: 2.777385]\n",
      "epoch:40 step:31279 [D loss: 0.292998, acc.: 88.28%] [G loss: 4.314654]\n",
      "epoch:40 step:31280 [D loss: 0.352805, acc.: 85.16%] [G loss: 3.038825]\n",
      "epoch:40 step:31281 [D loss: 0.308994, acc.: 84.38%] [G loss: 5.783053]\n",
      "epoch:40 step:31282 [D loss: 0.411457, acc.: 83.59%] [G loss: 4.069951]\n",
      "epoch:40 step:31283 [D loss: 0.375269, acc.: 83.59%] [G loss: 3.470237]\n",
      "epoch:40 step:31284 [D loss: 0.454596, acc.: 80.47%] [G loss: 4.616135]\n",
      "epoch:40 step:31285 [D loss: 0.401923, acc.: 81.25%] [G loss: 3.707499]\n",
      "epoch:40 step:31286 [D loss: 0.324500, acc.: 85.94%] [G loss: 3.289024]\n",
      "epoch:40 step:31287 [D loss: 0.384521, acc.: 82.03%] [G loss: 4.737646]\n",
      "epoch:40 step:31288 [D loss: 0.178045, acc.: 93.75%] [G loss: 3.482672]\n",
      "epoch:40 step:31289 [D loss: 0.305284, acc.: 87.50%] [G loss: 3.191148]\n",
      "epoch:40 step:31290 [D loss: 0.382784, acc.: 83.59%] [G loss: 3.174388]\n",
      "epoch:40 step:31291 [D loss: 0.269591, acc.: 86.72%] [G loss: 3.985887]\n",
      "epoch:40 step:31292 [D loss: 0.360337, acc.: 86.72%] [G loss: 2.972605]\n",
      "epoch:40 step:31293 [D loss: 0.344829, acc.: 84.38%] [G loss: 3.504913]\n",
      "epoch:40 step:31294 [D loss: 0.297835, acc.: 84.38%] [G loss: 3.841424]\n",
      "epoch:40 step:31295 [D loss: 0.233186, acc.: 89.84%] [G loss: 3.535472]\n",
      "epoch:40 step:31296 [D loss: 0.399326, acc.: 81.25%] [G loss: 3.372341]\n",
      "epoch:40 step:31297 [D loss: 0.235431, acc.: 92.19%] [G loss: 3.315505]\n",
      "epoch:40 step:31298 [D loss: 0.273516, acc.: 88.28%] [G loss: 2.641755]\n",
      "epoch:40 step:31299 [D loss: 0.356749, acc.: 85.16%] [G loss: 3.681534]\n",
      "epoch:40 step:31300 [D loss: 0.382134, acc.: 85.16%] [G loss: 3.421903]\n",
      "epoch:40 step:31301 [D loss: 0.318189, acc.: 85.94%] [G loss: 3.202181]\n",
      "epoch:40 step:31302 [D loss: 0.247697, acc.: 89.06%] [G loss: 4.225514]\n",
      "epoch:40 step:31303 [D loss: 0.246424, acc.: 89.84%] [G loss: 3.267437]\n",
      "epoch:40 step:31304 [D loss: 0.518104, acc.: 74.22%] [G loss: 3.958246]\n",
      "epoch:40 step:31305 [D loss: 0.368934, acc.: 84.38%] [G loss: 3.775100]\n",
      "epoch:40 step:31306 [D loss: 0.365411, acc.: 79.69%] [G loss: 4.029653]\n",
      "epoch:40 step:31307 [D loss: 0.397761, acc.: 82.81%] [G loss: 3.721870]\n",
      "epoch:40 step:31308 [D loss: 0.355764, acc.: 85.16%] [G loss: 3.414760]\n",
      "epoch:40 step:31309 [D loss: 0.413573, acc.: 82.03%] [G loss: 3.238144]\n",
      "epoch:40 step:31310 [D loss: 0.533960, acc.: 78.12%] [G loss: 8.057735]\n",
      "epoch:40 step:31311 [D loss: 1.000710, acc.: 64.84%] [G loss: 6.123191]\n",
      "epoch:40 step:31312 [D loss: 1.476563, acc.: 62.50%] [G loss: 7.879579]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:40 step:31313 [D loss: 1.762281, acc.: 67.97%] [G loss: 6.130482]\n",
      "epoch:40 step:31314 [D loss: 0.658516, acc.: 74.22%] [G loss: 3.655252]\n",
      "epoch:40 step:31315 [D loss: 0.485011, acc.: 82.03%] [G loss: 5.605803]\n",
      "epoch:40 step:31316 [D loss: 0.325449, acc.: 85.94%] [G loss: 4.034578]\n",
      "epoch:40 step:31317 [D loss: 0.349770, acc.: 85.94%] [G loss: 4.786690]\n",
      "epoch:40 step:31318 [D loss: 0.310481, acc.: 85.94%] [G loss: 3.020356]\n",
      "epoch:40 step:31319 [D loss: 0.306560, acc.: 86.72%] [G loss: 3.174726]\n",
      "epoch:40 step:31320 [D loss: 0.354950, acc.: 78.91%] [G loss: 1.994905]\n",
      "epoch:40 step:31321 [D loss: 0.310896, acc.: 88.28%] [G loss: 2.734074]\n",
      "epoch:40 step:31322 [D loss: 0.259034, acc.: 89.06%] [G loss: 3.324694]\n",
      "epoch:40 step:31323 [D loss: 0.300571, acc.: 89.06%] [G loss: 2.392276]\n",
      "epoch:40 step:31324 [D loss: 0.279184, acc.: 89.06%] [G loss: 3.515141]\n",
      "epoch:40 step:31325 [D loss: 0.276389, acc.: 88.28%] [G loss: 3.460977]\n",
      "epoch:40 step:31326 [D loss: 0.296932, acc.: 88.28%] [G loss: 3.074265]\n",
      "epoch:40 step:31327 [D loss: 0.373460, acc.: 85.16%] [G loss: 3.600275]\n",
      "epoch:40 step:31328 [D loss: 0.345479, acc.: 82.03%] [G loss: 3.276801]\n",
      "epoch:40 step:31329 [D loss: 0.379345, acc.: 80.47%] [G loss: 3.121226]\n",
      "epoch:40 step:31330 [D loss: 0.251683, acc.: 89.06%] [G loss: 2.564266]\n",
      "epoch:40 step:31331 [D loss: 0.236820, acc.: 89.84%] [G loss: 3.234514]\n",
      "epoch:40 step:31332 [D loss: 0.316905, acc.: 85.16%] [G loss: 2.923091]\n",
      "epoch:40 step:31333 [D loss: 0.371776, acc.: 79.69%] [G loss: 3.069215]\n",
      "epoch:40 step:31334 [D loss: 0.369120, acc.: 85.94%] [G loss: 3.633485]\n",
      "epoch:40 step:31335 [D loss: 0.262469, acc.: 90.62%] [G loss: 3.780349]\n",
      "epoch:40 step:31336 [D loss: 0.284700, acc.: 89.06%] [G loss: 3.834565]\n",
      "epoch:40 step:31337 [D loss: 0.296226, acc.: 89.06%] [G loss: 3.517743]\n",
      "epoch:40 step:31338 [D loss: 0.320293, acc.: 85.16%] [G loss: 3.576569]\n",
      "epoch:40 step:31339 [D loss: 0.307861, acc.: 85.94%] [G loss: 2.837727]\n",
      "epoch:40 step:31340 [D loss: 0.318090, acc.: 85.94%] [G loss: 2.332065]\n",
      "epoch:40 step:31341 [D loss: 0.360520, acc.: 83.59%] [G loss: 2.982664]\n",
      "epoch:40 step:31342 [D loss: 0.289340, acc.: 90.62%] [G loss: 2.733597]\n",
      "epoch:40 step:31343 [D loss: 0.424200, acc.: 78.91%] [G loss: 3.259397]\n",
      "epoch:40 step:31344 [D loss: 0.406637, acc.: 78.12%] [G loss: 2.411387]\n",
      "epoch:40 step:31345 [D loss: 0.387587, acc.: 82.81%] [G loss: 2.339110]\n",
      "epoch:40 step:31346 [D loss: 0.440215, acc.: 78.91%] [G loss: 3.261779]\n",
      "epoch:40 step:31347 [D loss: 0.296981, acc.: 87.50%] [G loss: 3.451509]\n",
      "epoch:40 step:31348 [D loss: 0.224720, acc.: 89.06%] [G loss: 3.182784]\n",
      "epoch:40 step:31349 [D loss: 0.405323, acc.: 78.91%] [G loss: 2.826021]\n",
      "epoch:40 step:31350 [D loss: 0.434843, acc.: 82.03%] [G loss: 2.948930]\n",
      "epoch:40 step:31351 [D loss: 0.421186, acc.: 78.12%] [G loss: 2.300786]\n",
      "epoch:40 step:31352 [D loss: 0.288998, acc.: 86.72%] [G loss: 3.050994]\n",
      "epoch:40 step:31353 [D loss: 0.349755, acc.: 84.38%] [G loss: 3.364179]\n",
      "epoch:40 step:31354 [D loss: 0.375653, acc.: 81.25%] [G loss: 2.762633]\n",
      "epoch:40 step:31355 [D loss: 0.388596, acc.: 77.34%] [G loss: 2.927459]\n",
      "epoch:40 step:31356 [D loss: 0.244140, acc.: 86.72%] [G loss: 3.307431]\n",
      "epoch:40 step:31357 [D loss: 0.292135, acc.: 84.38%] [G loss: 2.847136]\n",
      "epoch:40 step:31358 [D loss: 0.319156, acc.: 84.38%] [G loss: 3.972929]\n",
      "epoch:40 step:31359 [D loss: 0.264444, acc.: 89.84%] [G loss: 3.349696]\n",
      "epoch:40 step:31360 [D loss: 0.302517, acc.: 89.84%] [G loss: 3.568984]\n",
      "epoch:40 step:31361 [D loss: 0.300929, acc.: 87.50%] [G loss: 3.140285]\n",
      "epoch:40 step:31362 [D loss: 0.266265, acc.: 89.84%] [G loss: 2.860730]\n",
      "epoch:40 step:31363 [D loss: 0.351213, acc.: 80.47%] [G loss: 2.278737]\n",
      "epoch:40 step:31364 [D loss: 0.342142, acc.: 82.81%] [G loss: 2.596579]\n",
      "epoch:40 step:31365 [D loss: 0.303351, acc.: 85.16%] [G loss: 2.696248]\n",
      "epoch:40 step:31366 [D loss: 0.291682, acc.: 85.94%] [G loss: 2.769678]\n",
      "epoch:40 step:31367 [D loss: 0.280137, acc.: 90.62%] [G loss: 2.863526]\n",
      "epoch:40 step:31368 [D loss: 0.362545, acc.: 85.94%] [G loss: 3.235124]\n",
      "epoch:40 step:31369 [D loss: 0.286257, acc.: 90.62%] [G loss: 2.758331]\n",
      "epoch:40 step:31370 [D loss: 0.298836, acc.: 87.50%] [G loss: 3.568035]\n",
      "epoch:40 step:31371 [D loss: 0.344043, acc.: 82.03%] [G loss: 2.345973]\n",
      "epoch:40 step:31372 [D loss: 0.263419, acc.: 85.94%] [G loss: 2.588854]\n",
      "epoch:40 step:31373 [D loss: 0.391240, acc.: 82.81%] [G loss: 2.392320]\n",
      "epoch:40 step:31374 [D loss: 0.284206, acc.: 85.94%] [G loss: 3.402413]\n",
      "epoch:40 step:31375 [D loss: 0.337123, acc.: 85.94%] [G loss: 3.908038]\n",
      "epoch:40 step:31376 [D loss: 0.265259, acc.: 89.06%] [G loss: 2.924481]\n",
      "epoch:40 step:31377 [D loss: 0.300499, acc.: 87.50%] [G loss: 3.706075]\n",
      "epoch:40 step:31378 [D loss: 0.337532, acc.: 85.16%] [G loss: 4.444157]\n",
      "epoch:40 step:31379 [D loss: 0.229159, acc.: 92.19%] [G loss: 3.201591]\n",
      "epoch:40 step:31380 [D loss: 0.297006, acc.: 86.72%] [G loss: 3.539364]\n",
      "epoch:40 step:31381 [D loss: 0.347020, acc.: 87.50%] [G loss: 3.285162]\n",
      "epoch:40 step:31382 [D loss: 0.255878, acc.: 90.62%] [G loss: 3.129933]\n",
      "epoch:40 step:31383 [D loss: 0.281785, acc.: 89.06%] [G loss: 3.360802]\n",
      "epoch:40 step:31384 [D loss: 0.312894, acc.: 85.16%] [G loss: 3.691347]\n",
      "epoch:40 step:31385 [D loss: 0.282834, acc.: 85.94%] [G loss: 2.624947]\n",
      "epoch:40 step:31386 [D loss: 0.276602, acc.: 87.50%] [G loss: 3.224716]\n",
      "epoch:40 step:31387 [D loss: 0.307096, acc.: 84.38%] [G loss: 2.701069]\n",
      "epoch:40 step:31388 [D loss: 0.360960, acc.: 82.81%] [G loss: 2.743949]\n",
      "epoch:40 step:31389 [D loss: 0.337299, acc.: 83.59%] [G loss: 2.496168]\n",
      "epoch:40 step:31390 [D loss: 0.286830, acc.: 86.72%] [G loss: 3.271194]\n",
      "epoch:40 step:31391 [D loss: 0.329451, acc.: 89.06%] [G loss: 3.066175]\n",
      "epoch:40 step:31392 [D loss: 0.428464, acc.: 80.47%] [G loss: 2.752016]\n",
      "epoch:40 step:31393 [D loss: 0.306416, acc.: 89.06%] [G loss: 3.037673]\n",
      "epoch:40 step:31394 [D loss: 0.337831, acc.: 84.38%] [G loss: 2.994757]\n",
      "epoch:40 step:31395 [D loss: 0.363489, acc.: 85.94%] [G loss: 2.953708]\n",
      "epoch:40 step:31396 [D loss: 0.314986, acc.: 84.38%] [G loss: 2.423059]\n",
      "epoch:40 step:31397 [D loss: 0.313302, acc.: 85.94%] [G loss: 3.516639]\n",
      "epoch:40 step:31398 [D loss: 0.343609, acc.: 83.59%] [G loss: 3.983687]\n",
      "epoch:40 step:31399 [D loss: 0.445570, acc.: 82.03%] [G loss: 2.667409]\n",
      "epoch:40 step:31400 [D loss: 0.382626, acc.: 82.81%] [G loss: 2.883816]\n",
      "##############\n",
      "[0.87157851 0.84055222 0.79334776 0.79398735 0.80177358 0.81305107\n",
      " 0.8892329  0.83362236 0.83557813 0.81516687]\n",
      "##########\n",
      "epoch:40 step:31401 [D loss: 0.336886, acc.: 84.38%] [G loss: 3.364598]\n",
      "epoch:40 step:31402 [D loss: 0.263238, acc.: 84.38%] [G loss: 4.179971]\n",
      "epoch:40 step:31403 [D loss: 0.320837, acc.: 85.16%] [G loss: 2.922042]\n",
      "epoch:40 step:31404 [D loss: 0.334269, acc.: 85.94%] [G loss: 3.365885]\n",
      "epoch:40 step:31405 [D loss: 0.295312, acc.: 85.16%] [G loss: 3.048622]\n",
      "epoch:40 step:31406 [D loss: 0.369033, acc.: 83.59%] [G loss: 3.606115]\n",
      "epoch:40 step:31407 [D loss: 0.444360, acc.: 79.69%] [G loss: 3.260559]\n",
      "epoch:40 step:31408 [D loss: 0.387900, acc.: 82.03%] [G loss: 3.012369]\n",
      "epoch:40 step:31409 [D loss: 0.319788, acc.: 82.03%] [G loss: 3.048027]\n",
      "epoch:40 step:31410 [D loss: 0.263420, acc.: 90.62%] [G loss: 3.146502]\n",
      "epoch:40 step:31411 [D loss: 0.319833, acc.: 82.81%] [G loss: 2.240850]\n",
      "epoch:40 step:31412 [D loss: 0.232181, acc.: 91.41%] [G loss: 3.510396]\n",
      "epoch:40 step:31413 [D loss: 0.369685, acc.: 84.38%] [G loss: 2.775759]\n",
      "epoch:40 step:31414 [D loss: 0.507888, acc.: 76.56%] [G loss: 3.139109]\n",
      "epoch:40 step:31415 [D loss: 0.316445, acc.: 89.84%] [G loss: 2.973177]\n",
      "epoch:40 step:31416 [D loss: 0.346173, acc.: 80.47%] [G loss: 3.219725]\n",
      "epoch:40 step:31417 [D loss: 0.405035, acc.: 78.91%] [G loss: 3.882000]\n",
      "epoch:40 step:31418 [D loss: 0.336298, acc.: 83.59%] [G loss: 2.990122]\n",
      "epoch:40 step:31419 [D loss: 0.299664, acc.: 87.50%] [G loss: 3.112792]\n",
      "epoch:40 step:31420 [D loss: 0.459135, acc.: 81.25%] [G loss: 2.971645]\n",
      "epoch:40 step:31421 [D loss: 0.252968, acc.: 91.41%] [G loss: 2.946870]\n",
      "epoch:40 step:31422 [D loss: 0.296908, acc.: 85.94%] [G loss: 4.532121]\n",
      "epoch:40 step:31423 [D loss: 0.289170, acc.: 86.72%] [G loss: 4.912827]\n",
      "epoch:40 step:31424 [D loss: 0.287310, acc.: 86.72%] [G loss: 6.539165]\n",
      "epoch:40 step:31425 [D loss: 0.230372, acc.: 91.41%] [G loss: 4.230850]\n",
      "epoch:40 step:31426 [D loss: 0.220578, acc.: 95.31%] [G loss: 5.217734]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:40 step:31427 [D loss: 0.259383, acc.: 91.41%] [G loss: 4.705156]\n",
      "epoch:40 step:31428 [D loss: 0.271646, acc.: 89.06%] [G loss: 4.289100]\n",
      "epoch:40 step:31429 [D loss: 0.273008, acc.: 86.72%] [G loss: 3.675237]\n",
      "epoch:40 step:31430 [D loss: 0.250965, acc.: 86.72%] [G loss: 3.199502]\n",
      "epoch:40 step:31431 [D loss: 0.281259, acc.: 85.16%] [G loss: 5.221007]\n",
      "epoch:40 step:31432 [D loss: 0.233588, acc.: 91.41%] [G loss: 4.251627]\n",
      "epoch:40 step:31433 [D loss: 0.214785, acc.: 92.97%] [G loss: 4.082002]\n",
      "epoch:40 step:31434 [D loss: 0.366407, acc.: 85.16%] [G loss: 2.833814]\n",
      "epoch:40 step:31435 [D loss: 0.365149, acc.: 85.16%] [G loss: 3.332360]\n",
      "epoch:40 step:31436 [D loss: 0.418961, acc.: 81.25%] [G loss: 3.453758]\n",
      "epoch:40 step:31437 [D loss: 0.474433, acc.: 76.56%] [G loss: 2.298441]\n",
      "epoch:40 step:31438 [D loss: 0.304403, acc.: 84.38%] [G loss: 2.550531]\n",
      "epoch:40 step:31439 [D loss: 0.289452, acc.: 86.72%] [G loss: 2.788945]\n",
      "epoch:40 step:31440 [D loss: 0.335580, acc.: 85.16%] [G loss: 3.210814]\n",
      "epoch:40 step:31441 [D loss: 0.290320, acc.: 89.84%] [G loss: 2.976521]\n",
      "epoch:40 step:31442 [D loss: 0.363703, acc.: 81.25%] [G loss: 3.146547]\n",
      "epoch:40 step:31443 [D loss: 0.284314, acc.: 88.28%] [G loss: 2.861485]\n",
      "epoch:40 step:31444 [D loss: 0.250331, acc.: 86.72%] [G loss: 3.329014]\n",
      "epoch:40 step:31445 [D loss: 0.282198, acc.: 87.50%] [G loss: 2.699377]\n",
      "epoch:40 step:31446 [D loss: 0.324812, acc.: 84.38%] [G loss: 3.626804]\n",
      "epoch:40 step:31447 [D loss: 0.227134, acc.: 89.84%] [G loss: 3.482287]\n",
      "epoch:40 step:31448 [D loss: 0.366639, acc.: 80.47%] [G loss: 3.573221]\n",
      "epoch:40 step:31449 [D loss: 0.361914, acc.: 84.38%] [G loss: 3.339918]\n",
      "epoch:40 step:31450 [D loss: 0.277147, acc.: 87.50%] [G loss: 3.902013]\n",
      "epoch:40 step:31451 [D loss: 0.381090, acc.: 81.25%] [G loss: 4.028415]\n",
      "epoch:40 step:31452 [D loss: 0.341082, acc.: 82.81%] [G loss: 4.119154]\n",
      "epoch:40 step:31453 [D loss: 0.355377, acc.: 82.81%] [G loss: 3.299623]\n",
      "epoch:40 step:31454 [D loss: 0.368224, acc.: 82.81%] [G loss: 2.732931]\n",
      "epoch:40 step:31455 [D loss: 0.396774, acc.: 85.94%] [G loss: 5.339900]\n",
      "epoch:40 step:31456 [D loss: 0.431038, acc.: 81.25%] [G loss: 3.534171]\n",
      "epoch:40 step:31457 [D loss: 0.238255, acc.: 89.06%] [G loss: 4.068070]\n",
      "epoch:40 step:31458 [D loss: 0.220331, acc.: 92.19%] [G loss: 2.710478]\n",
      "epoch:40 step:31459 [D loss: 0.231567, acc.: 90.62%] [G loss: 3.898646]\n",
      "epoch:40 step:31460 [D loss: 0.244813, acc.: 89.84%] [G loss: 2.692617]\n",
      "epoch:40 step:31461 [D loss: 0.269409, acc.: 86.72%] [G loss: 4.485967]\n",
      "epoch:40 step:31462 [D loss: 0.398218, acc.: 82.81%] [G loss: 3.004891]\n",
      "epoch:40 step:31463 [D loss: 0.279075, acc.: 86.72%] [G loss: 3.125496]\n",
      "epoch:40 step:31464 [D loss: 0.297721, acc.: 86.72%] [G loss: 4.046696]\n",
      "epoch:40 step:31465 [D loss: 0.299538, acc.: 85.16%] [G loss: 2.965387]\n",
      "epoch:40 step:31466 [D loss: 0.253119, acc.: 89.84%] [G loss: 3.251467]\n",
      "epoch:40 step:31467 [D loss: 0.238075, acc.: 90.62%] [G loss: 2.865960]\n",
      "epoch:40 step:31468 [D loss: 0.389769, acc.: 78.91%] [G loss: 3.497094]\n",
      "epoch:40 step:31469 [D loss: 0.322397, acc.: 87.50%] [G loss: 2.313812]\n",
      "epoch:40 step:31470 [D loss: 0.354291, acc.: 80.47%] [G loss: 2.899611]\n",
      "epoch:40 step:31471 [D loss: 0.287057, acc.: 89.06%] [G loss: 3.698789]\n",
      "epoch:40 step:31472 [D loss: 0.326776, acc.: 88.28%] [G loss: 3.247257]\n",
      "epoch:40 step:31473 [D loss: 0.345632, acc.: 81.25%] [G loss: 3.479709]\n",
      "epoch:40 step:31474 [D loss: 0.314104, acc.: 86.72%] [G loss: 3.384169]\n",
      "epoch:40 step:31475 [D loss: 0.376887, acc.: 79.69%] [G loss: 2.832877]\n",
      "epoch:40 step:31476 [D loss: 0.375808, acc.: 85.16%] [G loss: 2.357926]\n",
      "epoch:40 step:31477 [D loss: 0.325598, acc.: 85.16%] [G loss: 3.020575]\n",
      "epoch:40 step:31478 [D loss: 0.378865, acc.: 82.81%] [G loss: 2.530234]\n",
      "epoch:40 step:31479 [D loss: 0.289808, acc.: 87.50%] [G loss: 3.056041]\n",
      "epoch:40 step:31480 [D loss: 0.397290, acc.: 82.03%] [G loss: 2.920061]\n",
      "epoch:40 step:31481 [D loss: 0.351646, acc.: 83.59%] [G loss: 3.019103]\n",
      "epoch:40 step:31482 [D loss: 0.371195, acc.: 82.03%] [G loss: 4.259527]\n",
      "epoch:40 step:31483 [D loss: 0.370336, acc.: 82.03%] [G loss: 2.790976]\n",
      "epoch:40 step:31484 [D loss: 0.293549, acc.: 87.50%] [G loss: 3.112158]\n",
      "epoch:40 step:31485 [D loss: 0.308907, acc.: 89.06%] [G loss: 2.755238]\n",
      "epoch:40 step:31486 [D loss: 0.268439, acc.: 87.50%] [G loss: 2.727441]\n",
      "epoch:40 step:31487 [D loss: 0.318673, acc.: 83.59%] [G loss: 2.594007]\n",
      "epoch:40 step:31488 [D loss: 0.404129, acc.: 83.59%] [G loss: 3.748744]\n",
      "epoch:40 step:31489 [D loss: 0.280800, acc.: 90.62%] [G loss: 3.103733]\n",
      "epoch:40 step:31490 [D loss: 0.259851, acc.: 86.72%] [G loss: 3.024249]\n",
      "epoch:40 step:31491 [D loss: 0.342351, acc.: 85.16%] [G loss: 2.666133]\n",
      "epoch:40 step:31492 [D loss: 0.372542, acc.: 82.81%] [G loss: 2.621267]\n",
      "epoch:40 step:31493 [D loss: 0.340174, acc.: 82.81%] [G loss: 3.841016]\n",
      "epoch:40 step:31494 [D loss: 0.269318, acc.: 90.62%] [G loss: 2.689145]\n",
      "epoch:40 step:31495 [D loss: 0.291000, acc.: 87.50%] [G loss: 4.472030]\n",
      "epoch:40 step:31496 [D loss: 0.330981, acc.: 85.16%] [G loss: 2.910169]\n",
      "epoch:40 step:31497 [D loss: 0.289133, acc.: 85.94%] [G loss: 5.883362]\n",
      "epoch:40 step:31498 [D loss: 0.369578, acc.: 85.16%] [G loss: 4.610970]\n",
      "epoch:40 step:31499 [D loss: 0.408804, acc.: 79.69%] [G loss: 4.435310]\n",
      "epoch:40 step:31500 [D loss: 0.296210, acc.: 86.72%] [G loss: 3.244297]\n",
      "epoch:40 step:31501 [D loss: 0.376773, acc.: 81.25%] [G loss: 4.655877]\n",
      "epoch:40 step:31502 [D loss: 0.298047, acc.: 85.16%] [G loss: 3.250645]\n",
      "epoch:40 step:31503 [D loss: 0.340467, acc.: 83.59%] [G loss: 2.815954]\n",
      "epoch:40 step:31504 [D loss: 0.302293, acc.: 87.50%] [G loss: 3.165433]\n",
      "epoch:40 step:31505 [D loss: 0.352651, acc.: 82.03%] [G loss: 2.460135]\n",
      "epoch:40 step:31506 [D loss: 0.436489, acc.: 82.81%] [G loss: 2.843197]\n",
      "epoch:40 step:31507 [D loss: 0.327292, acc.: 85.94%] [G loss: 4.724667]\n",
      "epoch:40 step:31508 [D loss: 0.279745, acc.: 87.50%] [G loss: 3.517230]\n",
      "epoch:40 step:31509 [D loss: 0.280242, acc.: 89.84%] [G loss: 3.471018]\n",
      "epoch:40 step:31510 [D loss: 0.317135, acc.: 87.50%] [G loss: 3.482122]\n",
      "epoch:40 step:31511 [D loss: 0.192607, acc.: 91.41%] [G loss: 4.923120]\n",
      "epoch:40 step:31512 [D loss: 0.272960, acc.: 86.72%] [G loss: 2.809173]\n",
      "epoch:40 step:31513 [D loss: 0.273933, acc.: 87.50%] [G loss: 2.809446]\n",
      "epoch:40 step:31514 [D loss: 0.207762, acc.: 91.41%] [G loss: 3.118111]\n",
      "epoch:40 step:31515 [D loss: 0.412135, acc.: 84.38%] [G loss: 4.070716]\n",
      "epoch:40 step:31516 [D loss: 0.362474, acc.: 86.72%] [G loss: 3.247193]\n",
      "epoch:40 step:31517 [D loss: 0.383509, acc.: 83.59%] [G loss: 3.235717]\n",
      "epoch:40 step:31518 [D loss: 0.401067, acc.: 76.56%] [G loss: 4.203979]\n",
      "epoch:40 step:31519 [D loss: 0.423681, acc.: 82.03%] [G loss: 3.254345]\n",
      "epoch:40 step:31520 [D loss: 0.293701, acc.: 88.28%] [G loss: 2.687281]\n",
      "epoch:40 step:31521 [D loss: 0.289957, acc.: 88.28%] [G loss: 3.566962]\n",
      "epoch:40 step:31522 [D loss: 0.343267, acc.: 82.03%] [G loss: 3.556535]\n",
      "epoch:40 step:31523 [D loss: 0.308507, acc.: 85.16%] [G loss: 2.642669]\n",
      "epoch:40 step:31524 [D loss: 0.313521, acc.: 85.16%] [G loss: 2.785917]\n",
      "epoch:40 step:31525 [D loss: 0.292870, acc.: 87.50%] [G loss: 4.332240]\n",
      "epoch:40 step:31526 [D loss: 0.337256, acc.: 83.59%] [G loss: 3.174823]\n",
      "epoch:40 step:31527 [D loss: 0.302243, acc.: 85.94%] [G loss: 3.430947]\n",
      "epoch:40 step:31528 [D loss: 0.304638, acc.: 84.38%] [G loss: 3.394657]\n",
      "epoch:40 step:31529 [D loss: 0.335289, acc.: 82.81%] [G loss: 3.845115]\n",
      "epoch:40 step:31530 [D loss: 0.319190, acc.: 85.94%] [G loss: 3.767273]\n",
      "epoch:40 step:31531 [D loss: 0.244487, acc.: 88.28%] [G loss: 4.162971]\n",
      "epoch:40 step:31532 [D loss: 0.222697, acc.: 88.28%] [G loss: 6.100789]\n",
      "epoch:40 step:31533 [D loss: 0.216327, acc.: 89.84%] [G loss: 3.529366]\n",
      "epoch:40 step:31534 [D loss: 0.278528, acc.: 86.72%] [G loss: 4.310292]\n",
      "epoch:40 step:31535 [D loss: 0.309613, acc.: 85.16%] [G loss: 4.166467]\n",
      "epoch:40 step:31536 [D loss: 0.306026, acc.: 86.72%] [G loss: 4.146890]\n",
      "epoch:40 step:31537 [D loss: 0.384419, acc.: 84.38%] [G loss: 3.882707]\n",
      "epoch:40 step:31538 [D loss: 0.233697, acc.: 89.84%] [G loss: 4.646024]\n",
      "epoch:40 step:31539 [D loss: 0.277256, acc.: 83.59%] [G loss: 3.446398]\n",
      "epoch:40 step:31540 [D loss: 0.342294, acc.: 83.59%] [G loss: 4.215647]\n",
      "epoch:40 step:31541 [D loss: 0.246003, acc.: 89.06%] [G loss: 4.661743]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:40 step:31542 [D loss: 0.226541, acc.: 88.28%] [G loss: 4.862058]\n",
      "epoch:40 step:31543 [D loss: 0.304795, acc.: 85.16%] [G loss: 3.434393]\n",
      "epoch:40 step:31544 [D loss: 0.257776, acc.: 90.62%] [G loss: 3.573969]\n",
      "epoch:40 step:31545 [D loss: 0.330693, acc.: 84.38%] [G loss: 2.475640]\n",
      "epoch:40 step:31546 [D loss: 0.241858, acc.: 91.41%] [G loss: 3.394725]\n",
      "epoch:40 step:31547 [D loss: 0.319818, acc.: 87.50%] [G loss: 3.337409]\n",
      "epoch:40 step:31548 [D loss: 0.379347, acc.: 84.38%] [G loss: 3.228660]\n",
      "epoch:40 step:31549 [D loss: 0.375490, acc.: 83.59%] [G loss: 2.997152]\n",
      "epoch:40 step:31550 [D loss: 0.272099, acc.: 87.50%] [G loss: 2.988741]\n",
      "epoch:40 step:31551 [D loss: 0.284521, acc.: 86.72%] [G loss: 4.035156]\n",
      "epoch:40 step:31552 [D loss: 0.341991, acc.: 82.81%] [G loss: 3.629514]\n",
      "epoch:40 step:31553 [D loss: 0.371421, acc.: 83.59%] [G loss: 4.899189]\n",
      "epoch:40 step:31554 [D loss: 0.342289, acc.: 84.38%] [G loss: 3.837504]\n",
      "epoch:40 step:31555 [D loss: 0.326794, acc.: 85.16%] [G loss: 3.575594]\n",
      "epoch:40 step:31556 [D loss: 0.297548, acc.: 88.28%] [G loss: 3.811631]\n",
      "epoch:40 step:31557 [D loss: 0.393345, acc.: 82.81%] [G loss: 2.798378]\n",
      "epoch:40 step:31558 [D loss: 0.263844, acc.: 87.50%] [G loss: 3.467823]\n",
      "epoch:40 step:31559 [D loss: 0.267431, acc.: 89.84%] [G loss: 4.245732]\n",
      "epoch:40 step:31560 [D loss: 0.345525, acc.: 82.81%] [G loss: 5.868220]\n",
      "epoch:40 step:31561 [D loss: 0.447523, acc.: 82.81%] [G loss: 4.371031]\n",
      "epoch:40 step:31562 [D loss: 0.210945, acc.: 89.84%] [G loss: 3.356241]\n",
      "epoch:40 step:31563 [D loss: 0.283771, acc.: 86.72%] [G loss: 3.060563]\n",
      "epoch:40 step:31564 [D loss: 0.302468, acc.: 86.72%] [G loss: 3.218338]\n",
      "epoch:40 step:31565 [D loss: 0.391725, acc.: 80.47%] [G loss: 2.840908]\n",
      "epoch:40 step:31566 [D loss: 0.449134, acc.: 77.34%] [G loss: 3.638689]\n",
      "epoch:40 step:31567 [D loss: 0.327870, acc.: 85.16%] [G loss: 4.606386]\n",
      "epoch:40 step:31568 [D loss: 0.395633, acc.: 82.81%] [G loss: 3.689942]\n",
      "epoch:40 step:31569 [D loss: 0.370878, acc.: 82.81%] [G loss: 4.169635]\n",
      "epoch:40 step:31570 [D loss: 0.188882, acc.: 94.53%] [G loss: 4.214499]\n",
      "epoch:40 step:31571 [D loss: 0.339708, acc.: 85.16%] [G loss: 3.760467]\n",
      "epoch:40 step:31572 [D loss: 0.246996, acc.: 91.41%] [G loss: 3.384739]\n",
      "epoch:40 step:31573 [D loss: 0.290729, acc.: 84.38%] [G loss: 3.657238]\n",
      "epoch:40 step:31574 [D loss: 0.369653, acc.: 87.50%] [G loss: 2.640855]\n",
      "epoch:40 step:31575 [D loss: 0.207532, acc.: 92.97%] [G loss: 3.820219]\n",
      "epoch:40 step:31576 [D loss: 0.289658, acc.: 88.28%] [G loss: 3.065022]\n",
      "epoch:40 step:31577 [D loss: 0.268501, acc.: 89.84%] [G loss: 3.267498]\n",
      "epoch:40 step:31578 [D loss: 0.313611, acc.: 87.50%] [G loss: 2.730258]\n",
      "epoch:40 step:31579 [D loss: 0.228502, acc.: 89.84%] [G loss: 3.633696]\n",
      "epoch:40 step:31580 [D loss: 0.355260, acc.: 83.59%] [G loss: 3.356264]\n",
      "epoch:40 step:31581 [D loss: 0.254844, acc.: 88.28%] [G loss: 3.682836]\n",
      "epoch:40 step:31582 [D loss: 0.259369, acc.: 88.28%] [G loss: 3.138958]\n",
      "epoch:40 step:31583 [D loss: 0.223744, acc.: 90.62%] [G loss: 3.400811]\n",
      "epoch:40 step:31584 [D loss: 0.291544, acc.: 85.94%] [G loss: 3.388914]\n",
      "epoch:40 step:31585 [D loss: 0.389247, acc.: 84.38%] [G loss: 2.787127]\n",
      "epoch:40 step:31586 [D loss: 0.331555, acc.: 83.59%] [G loss: 3.097336]\n",
      "epoch:40 step:31587 [D loss: 0.388348, acc.: 80.47%] [G loss: 2.720616]\n",
      "epoch:40 step:31588 [D loss: 0.295426, acc.: 89.06%] [G loss: 2.763466]\n",
      "epoch:40 step:31589 [D loss: 0.294600, acc.: 89.84%] [G loss: 2.765924]\n",
      "epoch:40 step:31590 [D loss: 0.303456, acc.: 83.59%] [G loss: 2.965679]\n",
      "epoch:40 step:31591 [D loss: 0.431561, acc.: 84.38%] [G loss: 4.758229]\n",
      "epoch:40 step:31592 [D loss: 0.365652, acc.: 82.81%] [G loss: 3.756073]\n",
      "epoch:40 step:31593 [D loss: 0.227082, acc.: 88.28%] [G loss: 4.378252]\n",
      "epoch:40 step:31594 [D loss: 0.359193, acc.: 85.16%] [G loss: 4.852780]\n",
      "epoch:40 step:31595 [D loss: 0.349670, acc.: 82.81%] [G loss: 3.176268]\n",
      "epoch:40 step:31596 [D loss: 0.376088, acc.: 83.59%] [G loss: 4.518082]\n",
      "epoch:40 step:31597 [D loss: 0.317579, acc.: 85.94%] [G loss: 3.058559]\n",
      "epoch:40 step:31598 [D loss: 0.249792, acc.: 89.06%] [G loss: 4.501775]\n",
      "epoch:40 step:31599 [D loss: 0.307647, acc.: 85.94%] [G loss: 3.808783]\n",
      "epoch:40 step:31600 [D loss: 0.359991, acc.: 82.03%] [G loss: 3.182905]\n",
      "##############\n",
      "[0.87818626 0.85732169 0.80348978 0.81061696 0.79510932 0.83512057\n",
      " 0.88797426 0.82831476 0.81942486 0.81863145]\n",
      "##########\n",
      "epoch:40 step:31601 [D loss: 0.343301, acc.: 81.25%] [G loss: 3.370665]\n",
      "epoch:40 step:31602 [D loss: 0.361605, acc.: 82.81%] [G loss: 4.455350]\n",
      "epoch:40 step:31603 [D loss: 0.353285, acc.: 86.72%] [G loss: 3.263589]\n",
      "epoch:40 step:31604 [D loss: 0.467260, acc.: 78.12%] [G loss: 3.297118]\n",
      "epoch:40 step:31605 [D loss: 0.385932, acc.: 85.16%] [G loss: 3.796190]\n",
      "epoch:40 step:31606 [D loss: 0.345414, acc.: 85.16%] [G loss: 3.000609]\n",
      "epoch:40 step:31607 [D loss: 0.322649, acc.: 88.28%] [G loss: 3.091675]\n",
      "epoch:40 step:31608 [D loss: 0.380508, acc.: 83.59%] [G loss: 2.980396]\n",
      "epoch:40 step:31609 [D loss: 0.332294, acc.: 85.94%] [G loss: 3.450403]\n",
      "epoch:40 step:31610 [D loss: 0.348896, acc.: 84.38%] [G loss: 3.037777]\n",
      "epoch:40 step:31611 [D loss: 0.216073, acc.: 89.84%] [G loss: 3.165210]\n",
      "epoch:40 step:31612 [D loss: 0.278876, acc.: 89.84%] [G loss: 3.010839]\n",
      "epoch:40 step:31613 [D loss: 0.236186, acc.: 91.41%] [G loss: 2.630213]\n",
      "epoch:40 step:31614 [D loss: 0.390922, acc.: 83.59%] [G loss: 3.409470]\n",
      "epoch:40 step:31615 [D loss: 0.225336, acc.: 92.19%] [G loss: 2.650892]\n",
      "epoch:40 step:31616 [D loss: 0.326789, acc.: 84.38%] [G loss: 2.595676]\n",
      "epoch:40 step:31617 [D loss: 0.344836, acc.: 86.72%] [G loss: 2.754472]\n",
      "epoch:40 step:31618 [D loss: 0.410892, acc.: 82.81%] [G loss: 3.934057]\n",
      "epoch:40 step:31619 [D loss: 0.393117, acc.: 84.38%] [G loss: 4.195468]\n",
      "epoch:40 step:31620 [D loss: 0.320323, acc.: 82.03%] [G loss: 3.730560]\n",
      "epoch:40 step:31621 [D loss: 0.225629, acc.: 90.62%] [G loss: 3.708693]\n",
      "epoch:40 step:31622 [D loss: 0.378333, acc.: 82.03%] [G loss: 3.408249]\n",
      "epoch:40 step:31623 [D loss: 0.297346, acc.: 82.81%] [G loss: 3.026209]\n",
      "epoch:40 step:31624 [D loss: 0.202601, acc.: 92.97%] [G loss: 3.789559]\n",
      "epoch:40 step:31625 [D loss: 0.362567, acc.: 85.16%] [G loss: 2.586275]\n",
      "epoch:40 step:31626 [D loss: 0.292801, acc.: 85.94%] [G loss: 2.613348]\n",
      "epoch:40 step:31627 [D loss: 0.345911, acc.: 86.72%] [G loss: 2.757272]\n",
      "epoch:40 step:31628 [D loss: 0.323856, acc.: 85.16%] [G loss: 3.749925]\n",
      "epoch:40 step:31629 [D loss: 0.359047, acc.: 78.91%] [G loss: 3.291144]\n",
      "epoch:40 step:31630 [D loss: 0.203214, acc.: 92.19%] [G loss: 4.173582]\n",
      "epoch:40 step:31631 [D loss: 0.452442, acc.: 80.47%] [G loss: 2.752150]\n",
      "epoch:40 step:31632 [D loss: 0.303778, acc.: 87.50%] [G loss: 4.793340]\n",
      "epoch:40 step:31633 [D loss: 0.271536, acc.: 88.28%] [G loss: 5.427590]\n",
      "epoch:40 step:31634 [D loss: 0.324414, acc.: 86.72%] [G loss: 3.182876]\n",
      "epoch:40 step:31635 [D loss: 0.417651, acc.: 80.47%] [G loss: 3.656127]\n",
      "epoch:40 step:31636 [D loss: 0.274742, acc.: 87.50%] [G loss: 4.010186]\n",
      "epoch:40 step:31637 [D loss: 0.351850, acc.: 82.03%] [G loss: 3.651886]\n",
      "epoch:40 step:31638 [D loss: 0.262514, acc.: 88.28%] [G loss: 3.879257]\n",
      "epoch:40 step:31639 [D loss: 0.382668, acc.: 82.03%] [G loss: 3.392124]\n",
      "epoch:40 step:31640 [D loss: 0.291900, acc.: 84.38%] [G loss: 4.242567]\n",
      "epoch:40 step:31641 [D loss: 0.338359, acc.: 84.38%] [G loss: 3.680770]\n",
      "epoch:40 step:31642 [D loss: 0.242418, acc.: 88.28%] [G loss: 3.796828]\n",
      "epoch:40 step:31643 [D loss: 0.425502, acc.: 79.69%] [G loss: 3.106373]\n",
      "epoch:40 step:31644 [D loss: 0.249820, acc.: 92.97%] [G loss: 3.325248]\n",
      "epoch:40 step:31645 [D loss: 0.353599, acc.: 84.38%] [G loss: 3.259488]\n",
      "epoch:40 step:31646 [D loss: 0.376333, acc.: 85.16%] [G loss: 3.474648]\n",
      "epoch:40 step:31647 [D loss: 0.351970, acc.: 82.03%] [G loss: 3.237773]\n",
      "epoch:40 step:31648 [D loss: 0.319992, acc.: 85.16%] [G loss: 2.485624]\n",
      "epoch:40 step:31649 [D loss: 0.269481, acc.: 88.28%] [G loss: 3.077903]\n",
      "epoch:40 step:31650 [D loss: 0.366029, acc.: 83.59%] [G loss: 2.589370]\n",
      "epoch:40 step:31651 [D loss: 0.300516, acc.: 87.50%] [G loss: 2.386600]\n",
      "epoch:40 step:31652 [D loss: 0.372565, acc.: 82.81%] [G loss: 2.547526]\n",
      "epoch:40 step:31653 [D loss: 0.345101, acc.: 85.16%] [G loss: 2.356577]\n",
      "epoch:40 step:31654 [D loss: 0.381040, acc.: 81.25%] [G loss: 2.435861]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:40 step:31655 [D loss: 0.292108, acc.: 87.50%] [G loss: 2.661062]\n",
      "epoch:40 step:31656 [D loss: 0.316277, acc.: 89.06%] [G loss: 3.001255]\n",
      "epoch:40 step:31657 [D loss: 0.351562, acc.: 87.50%] [G loss: 2.766820]\n",
      "epoch:40 step:31658 [D loss: 0.353464, acc.: 85.16%] [G loss: 3.038901]\n",
      "epoch:40 step:31659 [D loss: 0.306655, acc.: 92.19%] [G loss: 2.411601]\n",
      "epoch:40 step:31660 [D loss: 0.289022, acc.: 85.16%] [G loss: 4.208717]\n",
      "epoch:40 step:31661 [D loss: 0.337284, acc.: 86.72%] [G loss: 3.267536]\n",
      "epoch:40 step:31662 [D loss: 0.205904, acc.: 89.06%] [G loss: 4.043876]\n",
      "epoch:40 step:31663 [D loss: 0.293401, acc.: 87.50%] [G loss: 4.257240]\n",
      "epoch:40 step:31664 [D loss: 0.337824, acc.: 84.38%] [G loss: 3.217162]\n",
      "epoch:40 step:31665 [D loss: 0.339957, acc.: 86.72%] [G loss: 3.432912]\n",
      "epoch:40 step:31666 [D loss: 0.198030, acc.: 96.09%] [G loss: 3.208184]\n",
      "epoch:40 step:31667 [D loss: 0.302461, acc.: 91.41%] [G loss: 3.366695]\n",
      "epoch:40 step:31668 [D loss: 0.404656, acc.: 80.47%] [G loss: 3.207243]\n",
      "epoch:40 step:31669 [D loss: 0.343273, acc.: 86.72%] [G loss: 3.586358]\n",
      "epoch:40 step:31670 [D loss: 0.255409, acc.: 87.50%] [G loss: 3.432034]\n",
      "epoch:40 step:31671 [D loss: 0.331159, acc.: 85.94%] [G loss: 3.616812]\n",
      "epoch:40 step:31672 [D loss: 0.430603, acc.: 78.91%] [G loss: 5.980375]\n",
      "epoch:40 step:31673 [D loss: 0.687464, acc.: 74.22%] [G loss: 5.571853]\n",
      "epoch:40 step:31674 [D loss: 0.875670, acc.: 71.88%] [G loss: 6.891553]\n",
      "epoch:40 step:31675 [D loss: 1.332380, acc.: 69.53%] [G loss: 9.872623]\n",
      "epoch:40 step:31676 [D loss: 3.602987, acc.: 47.66%] [G loss: 4.836701]\n",
      "epoch:40 step:31677 [D loss: 0.859121, acc.: 78.91%] [G loss: 5.387550]\n",
      "epoch:40 step:31678 [D loss: 0.943344, acc.: 71.09%] [G loss: 4.480719]\n",
      "epoch:40 step:31679 [D loss: 0.377983, acc.: 79.69%] [G loss: 4.917603]\n",
      "epoch:40 step:31680 [D loss: 0.537072, acc.: 79.69%] [G loss: 4.756353]\n",
      "epoch:40 step:31681 [D loss: 0.294578, acc.: 86.72%] [G loss: 3.414376]\n",
      "epoch:40 step:31682 [D loss: 0.413293, acc.: 82.81%] [G loss: 3.370754]\n",
      "epoch:40 step:31683 [D loss: 0.502307, acc.: 81.25%] [G loss: 2.420429]\n",
      "epoch:40 step:31684 [D loss: 0.443944, acc.: 76.56%] [G loss: 2.628973]\n",
      "epoch:40 step:31685 [D loss: 0.321928, acc.: 84.38%] [G loss: 2.978260]\n",
      "epoch:40 step:31686 [D loss: 0.307365, acc.: 88.28%] [G loss: 2.881411]\n",
      "epoch:40 step:31687 [D loss: 0.357244, acc.: 85.16%] [G loss: 2.879241]\n",
      "epoch:40 step:31688 [D loss: 0.380734, acc.: 81.25%] [G loss: 2.332330]\n",
      "epoch:40 step:31689 [D loss: 0.440185, acc.: 76.56%] [G loss: 2.686927]\n",
      "epoch:40 step:31690 [D loss: 0.307460, acc.: 84.38%] [G loss: 3.185723]\n",
      "epoch:40 step:31691 [D loss: 0.306808, acc.: 85.94%] [G loss: 2.785069]\n",
      "epoch:40 step:31692 [D loss: 0.376284, acc.: 80.47%] [G loss: 3.010443]\n",
      "epoch:40 step:31693 [D loss: 0.292936, acc.: 87.50%] [G loss: 2.693667]\n",
      "epoch:40 step:31694 [D loss: 0.290460, acc.: 84.38%] [G loss: 3.588256]\n",
      "epoch:40 step:31695 [D loss: 0.408047, acc.: 83.59%] [G loss: 2.661747]\n",
      "epoch:40 step:31696 [D loss: 0.381530, acc.: 83.59%] [G loss: 2.407069]\n",
      "epoch:40 step:31697 [D loss: 0.331521, acc.: 84.38%] [G loss: 2.532406]\n",
      "epoch:40 step:31698 [D loss: 0.263198, acc.: 91.41%] [G loss: 3.092518]\n",
      "epoch:40 step:31699 [D loss: 0.353129, acc.: 85.94%] [G loss: 3.235872]\n",
      "epoch:40 step:31700 [D loss: 0.359154, acc.: 85.94%] [G loss: 3.075470]\n",
      "epoch:40 step:31701 [D loss: 0.402970, acc.: 79.69%] [G loss: 2.446839]\n",
      "epoch:40 step:31702 [D loss: 0.338256, acc.: 85.16%] [G loss: 2.763730]\n",
      "epoch:40 step:31703 [D loss: 0.236718, acc.: 92.19%] [G loss: 2.869239]\n",
      "epoch:40 step:31704 [D loss: 0.488730, acc.: 78.91%] [G loss: 2.673667]\n",
      "epoch:40 step:31705 [D loss: 0.281707, acc.: 85.16%] [G loss: 3.100031]\n",
      "epoch:40 step:31706 [D loss: 0.352346, acc.: 81.25%] [G loss: 3.001878]\n",
      "epoch:40 step:31707 [D loss: 0.311937, acc.: 85.16%] [G loss: 2.520875]\n",
      "epoch:40 step:31708 [D loss: 0.362101, acc.: 86.72%] [G loss: 2.473966]\n",
      "epoch:40 step:31709 [D loss: 0.278197, acc.: 89.84%] [G loss: 3.115293]\n",
      "epoch:40 step:31710 [D loss: 0.257848, acc.: 89.84%] [G loss: 2.894430]\n",
      "epoch:40 step:31711 [D loss: 0.247025, acc.: 89.06%] [G loss: 2.741744]\n",
      "epoch:40 step:31712 [D loss: 0.282638, acc.: 87.50%] [G loss: 2.793337]\n",
      "epoch:40 step:31713 [D loss: 0.378752, acc.: 86.72%] [G loss: 2.451689]\n",
      "epoch:40 step:31714 [D loss: 0.410807, acc.: 82.81%] [G loss: 2.174651]\n",
      "epoch:40 step:31715 [D loss: 0.253873, acc.: 90.62%] [G loss: 2.929034]\n",
      "epoch:40 step:31716 [D loss: 0.289693, acc.: 85.94%] [G loss: 1.983220]\n",
      "epoch:40 step:31717 [D loss: 0.282693, acc.: 89.06%] [G loss: 2.273823]\n",
      "epoch:40 step:31718 [D loss: 0.318672, acc.: 88.28%] [G loss: 2.699520]\n",
      "epoch:40 step:31719 [D loss: 0.349227, acc.: 83.59%] [G loss: 2.615175]\n",
      "epoch:40 step:31720 [D loss: 0.233890, acc.: 89.84%] [G loss: 2.601439]\n",
      "epoch:40 step:31721 [D loss: 0.383319, acc.: 81.25%] [G loss: 2.748429]\n",
      "epoch:40 step:31722 [D loss: 0.366042, acc.: 85.16%] [G loss: 2.551813]\n",
      "epoch:40 step:31723 [D loss: 0.417841, acc.: 80.47%] [G loss: 3.316330]\n",
      "epoch:40 step:31724 [D loss: 0.311446, acc.: 85.16%] [G loss: 7.669448]\n",
      "epoch:40 step:31725 [D loss: 0.302599, acc.: 84.38%] [G loss: 4.051781]\n",
      "epoch:40 step:31726 [D loss: 0.294083, acc.: 85.94%] [G loss: 3.518533]\n",
      "epoch:40 step:31727 [D loss: 0.232323, acc.: 89.06%] [G loss: 4.104572]\n",
      "epoch:40 step:31728 [D loss: 0.305986, acc.: 86.72%] [G loss: 3.314167]\n",
      "epoch:40 step:31729 [D loss: 0.400271, acc.: 78.12%] [G loss: 3.087642]\n",
      "epoch:40 step:31730 [D loss: 0.314514, acc.: 83.59%] [G loss: 2.937271]\n",
      "epoch:40 step:31731 [D loss: 0.298487, acc.: 86.72%] [G loss: 2.747405]\n",
      "epoch:40 step:31732 [D loss: 0.333345, acc.: 84.38%] [G loss: 3.206272]\n",
      "epoch:40 step:31733 [D loss: 0.301251, acc.: 85.16%] [G loss: 3.284317]\n",
      "epoch:40 step:31734 [D loss: 0.246648, acc.: 89.06%] [G loss: 3.402180]\n",
      "epoch:40 step:31735 [D loss: 0.226409, acc.: 92.97%] [G loss: 2.992029]\n",
      "epoch:40 step:31736 [D loss: 0.261850, acc.: 89.06%] [G loss: 4.357376]\n",
      "epoch:40 step:31737 [D loss: 0.250337, acc.: 89.84%] [G loss: 4.738838]\n",
      "epoch:40 step:31738 [D loss: 0.291388, acc.: 85.94%] [G loss: 4.710541]\n",
      "epoch:40 step:31739 [D loss: 0.262254, acc.: 88.28%] [G loss: 4.627166]\n",
      "epoch:40 step:31740 [D loss: 0.297567, acc.: 82.81%] [G loss: 4.133955]\n",
      "epoch:40 step:31741 [D loss: 0.318583, acc.: 83.59%] [G loss: 3.409937]\n",
      "epoch:40 step:31742 [D loss: 0.249112, acc.: 90.62%] [G loss: 3.163202]\n",
      "epoch:40 step:31743 [D loss: 0.308383, acc.: 88.28%] [G loss: 3.762390]\n",
      "epoch:40 step:31744 [D loss: 0.270176, acc.: 88.28%] [G loss: 3.515814]\n",
      "epoch:40 step:31745 [D loss: 0.271688, acc.: 90.62%] [G loss: 3.189401]\n",
      "epoch:40 step:31746 [D loss: 0.254537, acc.: 86.72%] [G loss: 2.405303]\n",
      "epoch:40 step:31747 [D loss: 0.304610, acc.: 85.94%] [G loss: 3.665519]\n",
      "epoch:40 step:31748 [D loss: 0.281657, acc.: 87.50%] [G loss: 3.179429]\n",
      "epoch:40 step:31749 [D loss: 0.276814, acc.: 85.16%] [G loss: 4.512835]\n",
      "epoch:40 step:31750 [D loss: 0.284819, acc.: 90.62%] [G loss: 3.054722]\n",
      "epoch:40 step:31751 [D loss: 0.303380, acc.: 85.94%] [G loss: 2.969057]\n",
      "epoch:40 step:31752 [D loss: 0.223240, acc.: 90.62%] [G loss: 3.634773]\n",
      "epoch:40 step:31753 [D loss: 0.341202, acc.: 85.16%] [G loss: 3.813698]\n",
      "epoch:40 step:31754 [D loss: 0.358369, acc.: 86.72%] [G loss: 2.233659]\n",
      "epoch:40 step:31755 [D loss: 0.292970, acc.: 88.28%] [G loss: 2.996093]\n",
      "epoch:40 step:31756 [D loss: 0.289618, acc.: 87.50%] [G loss: 2.812607]\n",
      "epoch:40 step:31757 [D loss: 0.283958, acc.: 86.72%] [G loss: 3.194593]\n",
      "epoch:40 step:31758 [D loss: 0.252991, acc.: 91.41%] [G loss: 2.527681]\n",
      "epoch:40 step:31759 [D loss: 0.346348, acc.: 82.81%] [G loss: 3.150877]\n",
      "epoch:40 step:31760 [D loss: 0.362039, acc.: 80.47%] [G loss: 2.966583]\n",
      "epoch:40 step:31761 [D loss: 0.352245, acc.: 85.16%] [G loss: 3.002474]\n",
      "epoch:40 step:31762 [D loss: 0.470111, acc.: 82.03%] [G loss: 3.308356]\n",
      "epoch:40 step:31763 [D loss: 0.445800, acc.: 80.47%] [G loss: 3.155887]\n",
      "epoch:40 step:31764 [D loss: 0.291366, acc.: 85.94%] [G loss: 3.097980]\n",
      "epoch:40 step:31765 [D loss: 0.287693, acc.: 85.94%] [G loss: 2.447788]\n",
      "epoch:40 step:31766 [D loss: 0.350224, acc.: 82.81%] [G loss: 3.938622]\n",
      "epoch:40 step:31767 [D loss: 0.358306, acc.: 85.16%] [G loss: 3.024556]\n",
      "epoch:40 step:31768 [D loss: 0.353699, acc.: 82.81%] [G loss: 2.328096]\n",
      "epoch:40 step:31769 [D loss: 0.419514, acc.: 82.03%] [G loss: 2.515363]\n",
      "epoch:40 step:31770 [D loss: 0.318004, acc.: 89.84%] [G loss: 2.886626]\n",
      "epoch:40 step:31771 [D loss: 0.369043, acc.: 84.38%] [G loss: 2.231304]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:40 step:31772 [D loss: 0.327607, acc.: 86.72%] [G loss: 2.139326]\n",
      "epoch:40 step:31773 [D loss: 0.336113, acc.: 86.72%] [G loss: 2.844083]\n",
      "epoch:40 step:31774 [D loss: 0.363605, acc.: 78.91%] [G loss: 3.987628]\n",
      "epoch:40 step:31775 [D loss: 0.261788, acc.: 89.84%] [G loss: 3.909825]\n",
      "epoch:40 step:31776 [D loss: 0.421997, acc.: 78.12%] [G loss: 3.835319]\n",
      "epoch:40 step:31777 [D loss: 0.628188, acc.: 75.78%] [G loss: 4.310000]\n",
      "epoch:40 step:31778 [D loss: 0.704265, acc.: 75.78%] [G loss: 4.991898]\n",
      "epoch:40 step:31779 [D loss: 0.459632, acc.: 85.16%] [G loss: 2.480330]\n",
      "epoch:40 step:31780 [D loss: 0.528515, acc.: 82.81%] [G loss: 4.198627]\n",
      "epoch:40 step:31781 [D loss: 0.377871, acc.: 82.03%] [G loss: 3.011224]\n",
      "epoch:40 step:31782 [D loss: 0.230922, acc.: 89.84%] [G loss: 4.743588]\n",
      "epoch:40 step:31783 [D loss: 0.370799, acc.: 86.72%] [G loss: 3.731625]\n",
      "epoch:40 step:31784 [D loss: 0.291209, acc.: 86.72%] [G loss: 3.333383]\n",
      "epoch:40 step:31785 [D loss: 0.300377, acc.: 85.94%] [G loss: 4.210194]\n",
      "epoch:40 step:31786 [D loss: 0.232873, acc.: 90.62%] [G loss: 3.103637]\n",
      "epoch:40 step:31787 [D loss: 0.432421, acc.: 78.12%] [G loss: 4.132571]\n",
      "epoch:40 step:31788 [D loss: 0.291405, acc.: 84.38%] [G loss: 3.797928]\n",
      "epoch:40 step:31789 [D loss: 0.306697, acc.: 90.62%] [G loss: 4.957390]\n",
      "epoch:40 step:31790 [D loss: 0.280446, acc.: 90.62%] [G loss: 3.906971]\n",
      "epoch:40 step:31791 [D loss: 0.345427, acc.: 82.03%] [G loss: 5.103772]\n",
      "epoch:40 step:31792 [D loss: 0.449329, acc.: 81.25%] [G loss: 4.186177]\n",
      "epoch:40 step:31793 [D loss: 0.292056, acc.: 87.50%] [G loss: 4.594371]\n",
      "epoch:40 step:31794 [D loss: 0.348220, acc.: 85.94%] [G loss: 4.335262]\n",
      "epoch:40 step:31795 [D loss: 0.314780, acc.: 84.38%] [G loss: 3.400920]\n",
      "epoch:40 step:31796 [D loss: 0.336536, acc.: 85.16%] [G loss: 3.564752]\n",
      "epoch:40 step:31797 [D loss: 0.211895, acc.: 89.84%] [G loss: 3.315721]\n",
      "epoch:40 step:31798 [D loss: 0.379720, acc.: 82.03%] [G loss: 3.021914]\n",
      "epoch:40 step:31799 [D loss: 0.243970, acc.: 88.28%] [G loss: 3.587410]\n",
      "epoch:40 step:31800 [D loss: 0.383289, acc.: 83.59%] [G loss: 2.846969]\n",
      "##############\n",
      "[0.87342348 0.83146126 0.81031278 0.77934617 0.76236002 0.83127858\n",
      " 0.87330195 0.827749   0.82106717 0.83998157]\n",
      "##########\n",
      "epoch:40 step:31801 [D loss: 0.294458, acc.: 90.62%] [G loss: 2.698816]\n",
      "epoch:40 step:31802 [D loss: 0.386207, acc.: 84.38%] [G loss: 3.431313]\n",
      "epoch:40 step:31803 [D loss: 0.308182, acc.: 86.72%] [G loss: 2.671471]\n",
      "epoch:40 step:31804 [D loss: 0.339556, acc.: 87.50%] [G loss: 3.511431]\n",
      "epoch:40 step:31805 [D loss: 0.292162, acc.: 86.72%] [G loss: 3.197571]\n",
      "epoch:40 step:31806 [D loss: 0.312228, acc.: 87.50%] [G loss: 3.867310]\n",
      "epoch:40 step:31807 [D loss: 0.301985, acc.: 86.72%] [G loss: 2.949620]\n",
      "epoch:40 step:31808 [D loss: 0.297110, acc.: 87.50%] [G loss: 2.748526]\n",
      "epoch:40 step:31809 [D loss: 0.299597, acc.: 85.94%] [G loss: 3.466762]\n",
      "epoch:40 step:31810 [D loss: 0.303990, acc.: 84.38%] [G loss: 2.998965]\n",
      "epoch:40 step:31811 [D loss: 0.369477, acc.: 81.25%] [G loss: 4.221377]\n",
      "epoch:40 step:31812 [D loss: 0.263192, acc.: 89.84%] [G loss: 3.854820]\n",
      "epoch:40 step:31813 [D loss: 0.269971, acc.: 89.84%] [G loss: 3.316521]\n",
      "epoch:40 step:31814 [D loss: 0.281502, acc.: 86.72%] [G loss: 3.031007]\n",
      "epoch:40 step:31815 [D loss: 0.393015, acc.: 82.81%] [G loss: 3.352650]\n",
      "epoch:40 step:31816 [D loss: 0.311754, acc.: 84.38%] [G loss: 2.917582]\n",
      "epoch:40 step:31817 [D loss: 0.348382, acc.: 84.38%] [G loss: 2.904833]\n",
      "epoch:40 step:31818 [D loss: 0.275960, acc.: 88.28%] [G loss: 3.721744]\n",
      "epoch:40 step:31819 [D loss: 0.335227, acc.: 84.38%] [G loss: 3.749371]\n",
      "epoch:40 step:31820 [D loss: 0.219084, acc.: 91.41%] [G loss: 6.312568]\n",
      "epoch:40 step:31821 [D loss: 0.203559, acc.: 92.19%] [G loss: 3.516214]\n",
      "epoch:40 step:31822 [D loss: 0.283756, acc.: 87.50%] [G loss: 5.117966]\n",
      "epoch:40 step:31823 [D loss: 0.452522, acc.: 81.25%] [G loss: 2.617282]\n",
      "epoch:40 step:31824 [D loss: 0.318397, acc.: 85.16%] [G loss: 4.089679]\n",
      "epoch:40 step:31825 [D loss: 0.288532, acc.: 83.59%] [G loss: 3.453133]\n",
      "epoch:40 step:31826 [D loss: 0.359587, acc.: 84.38%] [G loss: 4.384507]\n",
      "epoch:40 step:31827 [D loss: 0.241316, acc.: 89.06%] [G loss: 3.504832]\n",
      "epoch:40 step:31828 [D loss: 0.365799, acc.: 81.25%] [G loss: 3.389231]\n",
      "epoch:40 step:31829 [D loss: 0.391292, acc.: 81.25%] [G loss: 4.115690]\n",
      "epoch:40 step:31830 [D loss: 0.378980, acc.: 78.91%] [G loss: 3.236356]\n",
      "epoch:40 step:31831 [D loss: 0.466182, acc.: 75.00%] [G loss: 3.505654]\n",
      "epoch:40 step:31832 [D loss: 0.303177, acc.: 87.50%] [G loss: 3.225246]\n",
      "epoch:40 step:31833 [D loss: 0.396985, acc.: 82.03%] [G loss: 3.361563]\n",
      "epoch:40 step:31834 [D loss: 0.356514, acc.: 82.81%] [G loss: 2.994787]\n",
      "epoch:40 step:31835 [D loss: 0.276910, acc.: 89.06%] [G loss: 3.113282]\n",
      "epoch:40 step:31836 [D loss: 0.375921, acc.: 76.56%] [G loss: 3.713264]\n",
      "epoch:40 step:31837 [D loss: 0.283882, acc.: 86.72%] [G loss: 3.451973]\n",
      "epoch:40 step:31838 [D loss: 0.340484, acc.: 86.72%] [G loss: 2.875221]\n",
      "epoch:40 step:31839 [D loss: 0.351834, acc.: 82.03%] [G loss: 3.356257]\n",
      "epoch:40 step:31840 [D loss: 0.357446, acc.: 83.59%] [G loss: 4.906819]\n",
      "epoch:40 step:31841 [D loss: 0.266625, acc.: 84.38%] [G loss: 3.640790]\n",
      "epoch:40 step:31842 [D loss: 0.310777, acc.: 85.16%] [G loss: 4.038983]\n",
      "epoch:40 step:31843 [D loss: 0.232333, acc.: 91.41%] [G loss: 4.583737]\n",
      "epoch:40 step:31844 [D loss: 0.329904, acc.: 85.94%] [G loss: 3.762235]\n",
      "epoch:40 step:31845 [D loss: 0.307288, acc.: 84.38%] [G loss: 3.443700]\n",
      "epoch:40 step:31846 [D loss: 0.394095, acc.: 79.69%] [G loss: 4.765836]\n",
      "epoch:40 step:31847 [D loss: 0.318270, acc.: 85.94%] [G loss: 4.202401]\n",
      "epoch:40 step:31848 [D loss: 0.174129, acc.: 94.53%] [G loss: 4.905040]\n",
      "epoch:40 step:31849 [D loss: 0.373306, acc.: 83.59%] [G loss: 3.950055]\n",
      "epoch:40 step:31850 [D loss: 0.328289, acc.: 86.72%] [G loss: 4.065149]\n",
      "epoch:40 step:31851 [D loss: 0.314091, acc.: 84.38%] [G loss: 2.736265]\n",
      "epoch:40 step:31852 [D loss: 0.388895, acc.: 82.03%] [G loss: 2.843697]\n",
      "epoch:40 step:31853 [D loss: 0.359890, acc.: 82.81%] [G loss: 5.684834]\n",
      "epoch:40 step:31854 [D loss: 0.447146, acc.: 85.16%] [G loss: 7.521277]\n",
      "epoch:40 step:31855 [D loss: 0.321900, acc.: 82.81%] [G loss: 4.815356]\n",
      "epoch:40 step:31856 [D loss: 0.302487, acc.: 86.72%] [G loss: 4.060001]\n",
      "epoch:40 step:31857 [D loss: 0.253334, acc.: 85.94%] [G loss: 3.303141]\n",
      "epoch:40 step:31858 [D loss: 0.305772, acc.: 87.50%] [G loss: 4.051429]\n",
      "epoch:40 step:31859 [D loss: 0.236356, acc.: 91.41%] [G loss: 4.967763]\n",
      "epoch:40 step:31860 [D loss: 0.253716, acc.: 89.06%] [G loss: 3.449306]\n",
      "epoch:40 step:31861 [D loss: 0.187560, acc.: 92.97%] [G loss: 4.113776]\n",
      "epoch:40 step:31862 [D loss: 0.324485, acc.: 86.72%] [G loss: 3.723562]\n",
      "epoch:40 step:31863 [D loss: 0.293981, acc.: 85.94%] [G loss: 3.823079]\n",
      "epoch:40 step:31864 [D loss: 0.341941, acc.: 89.06%] [G loss: 3.934989]\n",
      "epoch:40 step:31865 [D loss: 0.281785, acc.: 89.84%] [G loss: 2.914422]\n",
      "epoch:40 step:31866 [D loss: 0.433884, acc.: 78.91%] [G loss: 2.530634]\n",
      "epoch:40 step:31867 [D loss: 0.261939, acc.: 89.06%] [G loss: 3.218022]\n",
      "epoch:40 step:31868 [D loss: 0.353344, acc.: 85.16%] [G loss: 2.786198]\n",
      "epoch:40 step:31869 [D loss: 0.246922, acc.: 89.06%] [G loss: 3.309262]\n",
      "epoch:40 step:31870 [D loss: 0.380987, acc.: 81.25%] [G loss: 3.872309]\n",
      "epoch:40 step:31871 [D loss: 0.333430, acc.: 84.38%] [G loss: 2.154368]\n",
      "epoch:40 step:31872 [D loss: 0.321391, acc.: 89.06%] [G loss: 3.658598]\n",
      "epoch:40 step:31873 [D loss: 0.228731, acc.: 91.41%] [G loss: 4.207304]\n",
      "epoch:40 step:31874 [D loss: 0.252263, acc.: 89.06%] [G loss: 3.931350]\n",
      "epoch:40 step:31875 [D loss: 0.295027, acc.: 85.16%] [G loss: 3.747083]\n",
      "epoch:40 step:31876 [D loss: 0.247071, acc.: 89.84%] [G loss: 6.110245]\n",
      "epoch:40 step:31877 [D loss: 0.287399, acc.: 89.84%] [G loss: 6.967783]\n",
      "epoch:40 step:31878 [D loss: 0.201283, acc.: 90.62%] [G loss: 5.954309]\n",
      "epoch:40 step:31879 [D loss: 0.290187, acc.: 86.72%] [G loss: 4.848469]\n",
      "epoch:40 step:31880 [D loss: 0.242247, acc.: 91.41%] [G loss: 3.659051]\n",
      "epoch:40 step:31881 [D loss: 0.282699, acc.: 88.28%] [G loss: 5.104678]\n",
      "epoch:40 step:31882 [D loss: 0.310086, acc.: 85.94%] [G loss: 4.833421]\n",
      "epoch:40 step:31883 [D loss: 0.229883, acc.: 89.84%] [G loss: 4.211918]\n",
      "epoch:40 step:31884 [D loss: 0.285597, acc.: 86.72%] [G loss: 3.445750]\n",
      "epoch:40 step:31885 [D loss: 0.273595, acc.: 87.50%] [G loss: 2.254486]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:40 step:31886 [D loss: 0.354540, acc.: 82.03%] [G loss: 3.147145]\n",
      "epoch:40 step:31887 [D loss: 0.295620, acc.: 86.72%] [G loss: 3.745350]\n",
      "epoch:40 step:31888 [D loss: 0.358850, acc.: 82.03%] [G loss: 3.100088]\n",
      "epoch:40 step:31889 [D loss: 0.295119, acc.: 85.94%] [G loss: 3.699175]\n",
      "epoch:40 step:31890 [D loss: 0.325637, acc.: 85.16%] [G loss: 2.936008]\n",
      "epoch:40 step:31891 [D loss: 0.268050, acc.: 89.06%] [G loss: 3.017528]\n",
      "epoch:40 step:31892 [D loss: 0.447730, acc.: 77.34%] [G loss: 2.598365]\n",
      "epoch:40 step:31893 [D loss: 0.382048, acc.: 82.03%] [G loss: 2.672394]\n",
      "epoch:40 step:31894 [D loss: 0.370137, acc.: 81.25%] [G loss: 2.705158]\n",
      "epoch:40 step:31895 [D loss: 0.320922, acc.: 86.72%] [G loss: 2.531683]\n",
      "epoch:40 step:31896 [D loss: 0.292112, acc.: 83.59%] [G loss: 2.750407]\n",
      "epoch:40 step:31897 [D loss: 0.304850, acc.: 83.59%] [G loss: 2.325933]\n",
      "epoch:40 step:31898 [D loss: 0.281097, acc.: 86.72%] [G loss: 2.565065]\n",
      "epoch:40 step:31899 [D loss: 0.292264, acc.: 84.38%] [G loss: 2.337737]\n",
      "epoch:40 step:31900 [D loss: 0.303340, acc.: 89.06%] [G loss: 2.749223]\n",
      "epoch:40 step:31901 [D loss: 0.293932, acc.: 86.72%] [G loss: 2.069768]\n",
      "epoch:40 step:31902 [D loss: 0.302819, acc.: 88.28%] [G loss: 2.483940]\n",
      "epoch:40 step:31903 [D loss: 0.296584, acc.: 89.06%] [G loss: 2.679627]\n",
      "epoch:40 step:31904 [D loss: 0.316479, acc.: 87.50%] [G loss: 3.560573]\n",
      "epoch:40 step:31905 [D loss: 0.346299, acc.: 85.16%] [G loss: 3.319069]\n",
      "epoch:40 step:31906 [D loss: 0.321178, acc.: 85.94%] [G loss: 4.187652]\n",
      "epoch:40 step:31907 [D loss: 0.355216, acc.: 83.59%] [G loss: 2.749572]\n",
      "epoch:40 step:31908 [D loss: 0.242294, acc.: 92.19%] [G loss: 4.365855]\n",
      "epoch:40 step:31909 [D loss: 0.355550, acc.: 78.91%] [G loss: 3.752923]\n",
      "epoch:40 step:31910 [D loss: 0.330032, acc.: 85.94%] [G loss: 2.661043]\n",
      "epoch:40 step:31911 [D loss: 0.275760, acc.: 87.50%] [G loss: 3.985899]\n",
      "epoch:40 step:31912 [D loss: 0.260427, acc.: 87.50%] [G loss: 3.675169]\n",
      "epoch:40 step:31913 [D loss: 0.292453, acc.: 88.28%] [G loss: 2.884555]\n",
      "epoch:40 step:31914 [D loss: 0.262153, acc.: 90.62%] [G loss: 2.446845]\n",
      "epoch:40 step:31915 [D loss: 0.273914, acc.: 87.50%] [G loss: 3.517237]\n",
      "epoch:40 step:31916 [D loss: 0.293364, acc.: 85.94%] [G loss: 4.562480]\n",
      "epoch:40 step:31917 [D loss: 0.302738, acc.: 85.94%] [G loss: 2.428878]\n",
      "epoch:40 step:31918 [D loss: 0.237849, acc.: 91.41%] [G loss: 3.119769]\n",
      "epoch:40 step:31919 [D loss: 0.289676, acc.: 89.06%] [G loss: 2.882751]\n",
      "epoch:40 step:31920 [D loss: 0.270845, acc.: 87.50%] [G loss: 2.600747]\n",
      "epoch:40 step:31921 [D loss: 0.257932, acc.: 88.28%] [G loss: 3.229725]\n",
      "epoch:40 step:31922 [D loss: 0.377491, acc.: 84.38%] [G loss: 4.509177]\n",
      "epoch:40 step:31923 [D loss: 0.373733, acc.: 81.25%] [G loss: 3.937128]\n",
      "epoch:40 step:31924 [D loss: 0.326730, acc.: 85.94%] [G loss: 4.074765]\n",
      "epoch:40 step:31925 [D loss: 0.336152, acc.: 89.06%] [G loss: 3.171957]\n",
      "epoch:40 step:31926 [D loss: 0.246031, acc.: 89.84%] [G loss: 3.004877]\n",
      "epoch:40 step:31927 [D loss: 0.354163, acc.: 83.59%] [G loss: 3.488925]\n",
      "epoch:40 step:31928 [D loss: 0.342449, acc.: 85.16%] [G loss: 2.975899]\n",
      "epoch:40 step:31929 [D loss: 0.301811, acc.: 84.38%] [G loss: 2.443824]\n",
      "epoch:40 step:31930 [D loss: 0.279148, acc.: 88.28%] [G loss: 2.943418]\n",
      "epoch:40 step:31931 [D loss: 0.355665, acc.: 82.81%] [G loss: 3.219588]\n",
      "epoch:40 step:31932 [D loss: 0.332326, acc.: 83.59%] [G loss: 2.927382]\n",
      "epoch:40 step:31933 [D loss: 0.330193, acc.: 80.47%] [G loss: 3.275934]\n",
      "epoch:40 step:31934 [D loss: 0.254536, acc.: 89.84%] [G loss: 3.061841]\n",
      "epoch:40 step:31935 [D loss: 0.284104, acc.: 88.28%] [G loss: 4.196596]\n",
      "epoch:40 step:31936 [D loss: 0.371910, acc.: 82.81%] [G loss: 3.493337]\n",
      "epoch:40 step:31937 [D loss: 0.368474, acc.: 81.25%] [G loss: 3.251428]\n",
      "epoch:40 step:31938 [D loss: 0.371371, acc.: 83.59%] [G loss: 2.871229]\n",
      "epoch:40 step:31939 [D loss: 0.195766, acc.: 92.19%] [G loss: 3.238286]\n",
      "epoch:40 step:31940 [D loss: 0.358791, acc.: 85.16%] [G loss: 3.964177]\n",
      "epoch:40 step:31941 [D loss: 0.254670, acc.: 88.28%] [G loss: 3.612009]\n",
      "epoch:40 step:31942 [D loss: 0.244660, acc.: 89.06%] [G loss: 3.943087]\n",
      "epoch:40 step:31943 [D loss: 0.276026, acc.: 90.62%] [G loss: 4.193075]\n",
      "epoch:40 step:31944 [D loss: 0.318730, acc.: 84.38%] [G loss: 3.463007]\n",
      "epoch:40 step:31945 [D loss: 0.378472, acc.: 78.91%] [G loss: 3.277229]\n",
      "epoch:40 step:31946 [D loss: 0.252132, acc.: 89.84%] [G loss: 4.391712]\n",
      "epoch:40 step:31947 [D loss: 0.325237, acc.: 83.59%] [G loss: 3.330904]\n",
      "epoch:40 step:31948 [D loss: 0.326399, acc.: 86.72%] [G loss: 3.357242]\n",
      "epoch:40 step:31949 [D loss: 0.344715, acc.: 85.16%] [G loss: 3.643819]\n",
      "epoch:40 step:31950 [D loss: 0.294183, acc.: 83.59%] [G loss: 3.345321]\n",
      "epoch:40 step:31951 [D loss: 0.322087, acc.: 86.72%] [G loss: 2.915174]\n",
      "epoch:40 step:31952 [D loss: 0.286167, acc.: 89.84%] [G loss: 2.836905]\n",
      "epoch:40 step:31953 [D loss: 0.350935, acc.: 85.94%] [G loss: 3.250741]\n",
      "epoch:40 step:31954 [D loss: 0.308105, acc.: 85.94%] [G loss: 3.264022]\n",
      "epoch:40 step:31955 [D loss: 0.323143, acc.: 87.50%] [G loss: 3.270217]\n",
      "epoch:40 step:31956 [D loss: 0.254004, acc.: 92.19%] [G loss: 3.056228]\n",
      "epoch:40 step:31957 [D loss: 0.316745, acc.: 87.50%] [G loss: 3.997733]\n",
      "epoch:40 step:31958 [D loss: 0.366425, acc.: 82.81%] [G loss: 2.761920]\n",
      "epoch:40 step:31959 [D loss: 0.223900, acc.: 93.75%] [G loss: 2.894472]\n",
      "epoch:40 step:31960 [D loss: 0.297396, acc.: 82.81%] [G loss: 2.867567]\n",
      "epoch:40 step:31961 [D loss: 0.352363, acc.: 85.16%] [G loss: 5.907304]\n",
      "epoch:40 step:31962 [D loss: 0.413607, acc.: 83.59%] [G loss: 4.618577]\n",
      "epoch:40 step:31963 [D loss: 0.334467, acc.: 83.59%] [G loss: 4.657269]\n",
      "epoch:40 step:31964 [D loss: 0.321220, acc.: 85.16%] [G loss: 4.421724]\n",
      "epoch:40 step:31965 [D loss: 0.179439, acc.: 94.53%] [G loss: 7.636383]\n",
      "epoch:40 step:31966 [D loss: 0.295543, acc.: 82.03%] [G loss: 4.832803]\n",
      "epoch:40 step:31967 [D loss: 0.240108, acc.: 89.84%] [G loss: 3.736356]\n",
      "epoch:40 step:31968 [D loss: 0.268391, acc.: 86.72%] [G loss: 3.544935]\n",
      "epoch:40 step:31969 [D loss: 0.294876, acc.: 85.94%] [G loss: 3.231745]\n",
      "epoch:40 step:31970 [D loss: 0.279330, acc.: 85.16%] [G loss: 3.284493]\n",
      "epoch:40 step:31971 [D loss: 0.240837, acc.: 89.84%] [G loss: 3.304489]\n",
      "epoch:40 step:31972 [D loss: 0.313567, acc.: 89.06%] [G loss: 2.386564]\n",
      "epoch:40 step:31973 [D loss: 0.330263, acc.: 85.16%] [G loss: 2.695777]\n",
      "epoch:40 step:31974 [D loss: 0.248835, acc.: 89.84%] [G loss: 2.521424]\n",
      "epoch:40 step:31975 [D loss: 0.362902, acc.: 79.69%] [G loss: 2.459393]\n",
      "epoch:40 step:31976 [D loss: 0.307274, acc.: 87.50%] [G loss: 2.511821]\n",
      "epoch:40 step:31977 [D loss: 0.360663, acc.: 85.16%] [G loss: 2.778032]\n",
      "epoch:40 step:31978 [D loss: 0.324437, acc.: 82.03%] [G loss: 2.597118]\n",
      "epoch:40 step:31979 [D loss: 0.309745, acc.: 85.16%] [G loss: 2.927666]\n",
      "epoch:40 step:31980 [D loss: 0.381650, acc.: 82.03%] [G loss: 2.620495]\n",
      "epoch:40 step:31981 [D loss: 0.368576, acc.: 82.03%] [G loss: 2.468378]\n",
      "epoch:40 step:31982 [D loss: 0.236234, acc.: 92.19%] [G loss: 2.918448]\n",
      "epoch:40 step:31983 [D loss: 0.302373, acc.: 86.72%] [G loss: 3.413387]\n",
      "epoch:40 step:31984 [D loss: 0.256825, acc.: 85.94%] [G loss: 3.514995]\n",
      "epoch:40 step:31985 [D loss: 0.232834, acc.: 91.41%] [G loss: 3.953954]\n",
      "epoch:40 step:31986 [D loss: 0.266167, acc.: 86.72%] [G loss: 3.107280]\n",
      "epoch:40 step:31987 [D loss: 0.294080, acc.: 88.28%] [G loss: 3.405568]\n",
      "epoch:40 step:31988 [D loss: 0.330399, acc.: 86.72%] [G loss: 2.605135]\n",
      "epoch:40 step:31989 [D loss: 0.228370, acc.: 90.62%] [G loss: 3.333449]\n",
      "epoch:40 step:31990 [D loss: 0.258314, acc.: 90.62%] [G loss: 3.364234]\n",
      "epoch:40 step:31991 [D loss: 0.237902, acc.: 90.62%] [G loss: 2.674623]\n",
      "epoch:40 step:31992 [D loss: 0.317765, acc.: 87.50%] [G loss: 4.363226]\n",
      "epoch:40 step:31993 [D loss: 0.349535, acc.: 83.59%] [G loss: 3.052798]\n",
      "epoch:40 step:31994 [D loss: 0.295364, acc.: 88.28%] [G loss: 3.896797]\n",
      "epoch:40 step:31995 [D loss: 0.300280, acc.: 83.59%] [G loss: 3.800488]\n",
      "epoch:40 step:31996 [D loss: 0.314226, acc.: 85.16%] [G loss: 2.947468]\n",
      "epoch:40 step:31997 [D loss: 0.293668, acc.: 88.28%] [G loss: 3.715474]\n",
      "epoch:40 step:31998 [D loss: 0.297816, acc.: 90.62%] [G loss: 3.129482]\n",
      "epoch:40 step:31999 [D loss: 0.245284, acc.: 89.06%] [G loss: 3.198813]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:40 step:32000 [D loss: 0.242504, acc.: 89.06%] [G loss: 3.139970]\n",
      "##############\n",
      "[0.846343   0.86989599 0.78966907 0.78902612 0.76405963 0.82023114\n",
      " 0.90208933 0.81469342 0.81367349 0.83258671]\n",
      "##########\n",
      "epoch:40 step:32001 [D loss: 0.254208, acc.: 90.62%] [G loss: 4.102588]\n",
      "epoch:40 step:32002 [D loss: 0.286955, acc.: 89.84%] [G loss: 4.643417]\n",
      "epoch:40 step:32003 [D loss: 0.319638, acc.: 84.38%] [G loss: 2.982447]\n",
      "epoch:40 step:32004 [D loss: 0.289368, acc.: 85.94%] [G loss: 4.388631]\n",
      "epoch:40 step:32005 [D loss: 0.289533, acc.: 85.16%] [G loss: 5.419611]\n",
      "epoch:40 step:32006 [D loss: 0.210874, acc.: 92.97%] [G loss: 5.082770]\n",
      "epoch:40 step:32007 [D loss: 0.201195, acc.: 93.75%] [G loss: 3.257776]\n",
      "epoch:40 step:32008 [D loss: 0.377473, acc.: 79.69%] [G loss: 3.657840]\n",
      "epoch:40 step:32009 [D loss: 0.264440, acc.: 89.06%] [G loss: 3.768723]\n",
      "epoch:40 step:32010 [D loss: 0.260951, acc.: 89.84%] [G loss: 3.850771]\n",
      "epoch:40 step:32011 [D loss: 0.345114, acc.: 83.59%] [G loss: 2.639207]\n",
      "epoch:40 step:32012 [D loss: 0.386976, acc.: 84.38%] [G loss: 5.586526]\n",
      "epoch:40 step:32013 [D loss: 0.351869, acc.: 83.59%] [G loss: 7.251479]\n",
      "epoch:40 step:32014 [D loss: 0.843872, acc.: 73.44%] [G loss: 7.326209]\n",
      "epoch:40 step:32015 [D loss: 1.120638, acc.: 74.22%] [G loss: 7.932621]\n",
      "epoch:40 step:32016 [D loss: 1.965993, acc.: 64.06%] [G loss: 3.424174]\n",
      "epoch:40 step:32017 [D loss: 0.520350, acc.: 78.91%] [G loss: 3.303129]\n",
      "epoch:40 step:32018 [D loss: 0.369211, acc.: 82.03%] [G loss: 3.658215]\n",
      "epoch:40 step:32019 [D loss: 0.293673, acc.: 87.50%] [G loss: 4.200987]\n",
      "epoch:40 step:32020 [D loss: 0.343488, acc.: 86.72%] [G loss: 3.025589]\n",
      "epoch:40 step:32021 [D loss: 0.334692, acc.: 86.72%] [G loss: 3.078447]\n",
      "epoch:41 step:32022 [D loss: 0.372001, acc.: 85.94%] [G loss: 3.452722]\n",
      "epoch:41 step:32023 [D loss: 0.295828, acc.: 87.50%] [G loss: 3.719440]\n",
      "epoch:41 step:32024 [D loss: 0.366992, acc.: 82.81%] [G loss: 3.491040]\n",
      "epoch:41 step:32025 [D loss: 0.347982, acc.: 82.81%] [G loss: 3.102745]\n",
      "epoch:41 step:32026 [D loss: 0.367897, acc.: 80.47%] [G loss: 3.098512]\n",
      "epoch:41 step:32027 [D loss: 0.335390, acc.: 86.72%] [G loss: 3.058898]\n",
      "epoch:41 step:32028 [D loss: 0.293636, acc.: 88.28%] [G loss: 3.584835]\n",
      "epoch:41 step:32029 [D loss: 0.195846, acc.: 92.19%] [G loss: 2.801269]\n",
      "epoch:41 step:32030 [D loss: 0.321090, acc.: 85.94%] [G loss: 2.881556]\n",
      "epoch:41 step:32031 [D loss: 0.313229, acc.: 86.72%] [G loss: 2.658965]\n",
      "epoch:41 step:32032 [D loss: 0.378563, acc.: 83.59%] [G loss: 2.659990]\n",
      "epoch:41 step:32033 [D loss: 0.322357, acc.: 85.94%] [G loss: 2.819141]\n",
      "epoch:41 step:32034 [D loss: 0.309154, acc.: 88.28%] [G loss: 2.467181]\n",
      "epoch:41 step:32035 [D loss: 0.307309, acc.: 88.28%] [G loss: 2.988927]\n",
      "epoch:41 step:32036 [D loss: 0.294850, acc.: 87.50%] [G loss: 2.777111]\n",
      "epoch:41 step:32037 [D loss: 0.281464, acc.: 89.06%] [G loss: 2.794403]\n",
      "epoch:41 step:32038 [D loss: 0.253849, acc.: 88.28%] [G loss: 3.538462]\n",
      "epoch:41 step:32039 [D loss: 0.372528, acc.: 82.81%] [G loss: 2.706829]\n",
      "epoch:41 step:32040 [D loss: 0.261280, acc.: 89.06%] [G loss: 2.602296]\n",
      "epoch:41 step:32041 [D loss: 0.347320, acc.: 85.16%] [G loss: 2.092794]\n",
      "epoch:41 step:32042 [D loss: 0.416069, acc.: 78.91%] [G loss: 2.360548]\n",
      "epoch:41 step:32043 [D loss: 0.347163, acc.: 85.16%] [G loss: 2.325664]\n",
      "epoch:41 step:32044 [D loss: 0.259347, acc.: 88.28%] [G loss: 2.577525]\n",
      "epoch:41 step:32045 [D loss: 0.351654, acc.: 86.72%] [G loss: 2.774465]\n",
      "epoch:41 step:32046 [D loss: 0.312562, acc.: 84.38%] [G loss: 2.791049]\n",
      "epoch:41 step:32047 [D loss: 0.323281, acc.: 85.16%] [G loss: 2.432707]\n",
      "epoch:41 step:32048 [D loss: 0.345721, acc.: 85.94%] [G loss: 2.915679]\n",
      "epoch:41 step:32049 [D loss: 0.385022, acc.: 83.59%] [G loss: 2.696618]\n",
      "epoch:41 step:32050 [D loss: 0.347314, acc.: 86.72%] [G loss: 3.313200]\n",
      "epoch:41 step:32051 [D loss: 0.309201, acc.: 85.94%] [G loss: 3.136616]\n",
      "epoch:41 step:32052 [D loss: 0.346157, acc.: 85.16%] [G loss: 3.222914]\n",
      "epoch:41 step:32053 [D loss: 0.271240, acc.: 86.72%] [G loss: 3.022679]\n",
      "epoch:41 step:32054 [D loss: 0.344525, acc.: 88.28%] [G loss: 2.966520]\n",
      "epoch:41 step:32055 [D loss: 0.314962, acc.: 85.16%] [G loss: 3.902411]\n",
      "epoch:41 step:32056 [D loss: 0.437040, acc.: 78.12%] [G loss: 3.328273]\n",
      "epoch:41 step:32057 [D loss: 0.287262, acc.: 87.50%] [G loss: 4.737946]\n",
      "epoch:41 step:32058 [D loss: 0.301338, acc.: 86.72%] [G loss: 3.333484]\n",
      "epoch:41 step:32059 [D loss: 0.243067, acc.: 89.84%] [G loss: 2.955510]\n",
      "epoch:41 step:32060 [D loss: 0.240628, acc.: 91.41%] [G loss: 2.823919]\n",
      "epoch:41 step:32061 [D loss: 0.365268, acc.: 80.47%] [G loss: 2.841290]\n",
      "epoch:41 step:32062 [D loss: 0.345636, acc.: 84.38%] [G loss: 2.564395]\n",
      "epoch:41 step:32063 [D loss: 0.300238, acc.: 85.94%] [G loss: 2.947019]\n",
      "epoch:41 step:32064 [D loss: 0.418179, acc.: 82.03%] [G loss: 2.964709]\n",
      "epoch:41 step:32065 [D loss: 0.354414, acc.: 83.59%] [G loss: 2.657245]\n",
      "epoch:41 step:32066 [D loss: 0.221420, acc.: 92.19%] [G loss: 2.781012]\n",
      "epoch:41 step:32067 [D loss: 0.348642, acc.: 82.81%] [G loss: 2.587676]\n",
      "epoch:41 step:32068 [D loss: 0.354659, acc.: 80.47%] [G loss: 2.676250]\n",
      "epoch:41 step:32069 [D loss: 0.476693, acc.: 79.69%] [G loss: 2.794771]\n",
      "epoch:41 step:32070 [D loss: 0.290698, acc.: 86.72%] [G loss: 3.541387]\n",
      "epoch:41 step:32071 [D loss: 0.366334, acc.: 83.59%] [G loss: 2.450268]\n",
      "epoch:41 step:32072 [D loss: 0.355646, acc.: 85.16%] [G loss: 3.594453]\n",
      "epoch:41 step:32073 [D loss: 0.336113, acc.: 81.25%] [G loss: 2.813155]\n",
      "epoch:41 step:32074 [D loss: 0.341809, acc.: 84.38%] [G loss: 3.113172]\n",
      "epoch:41 step:32075 [D loss: 0.335849, acc.: 84.38%] [G loss: 3.088537]\n",
      "epoch:41 step:32076 [D loss: 0.271527, acc.: 89.06%] [G loss: 3.543778]\n",
      "epoch:41 step:32077 [D loss: 0.388815, acc.: 80.47%] [G loss: 5.597563]\n",
      "epoch:41 step:32078 [D loss: 0.265130, acc.: 85.94%] [G loss: 5.874657]\n",
      "epoch:41 step:32079 [D loss: 0.173747, acc.: 92.97%] [G loss: 5.355935]\n",
      "epoch:41 step:32080 [D loss: 0.288975, acc.: 84.38%] [G loss: 4.297581]\n",
      "epoch:41 step:32081 [D loss: 0.212024, acc.: 91.41%] [G loss: 4.569854]\n",
      "epoch:41 step:32082 [D loss: 0.245933, acc.: 88.28%] [G loss: 2.881234]\n",
      "epoch:41 step:32083 [D loss: 0.297562, acc.: 85.94%] [G loss: 3.465044]\n",
      "epoch:41 step:32084 [D loss: 0.287354, acc.: 83.59%] [G loss: 3.578952]\n",
      "epoch:41 step:32085 [D loss: 0.245884, acc.: 86.72%] [G loss: 4.749474]\n",
      "epoch:41 step:32086 [D loss: 0.295570, acc.: 87.50%] [G loss: 5.029309]\n",
      "epoch:41 step:32087 [D loss: 0.223551, acc.: 90.62%] [G loss: 4.383270]\n",
      "epoch:41 step:32088 [D loss: 0.266239, acc.: 89.06%] [G loss: 4.255738]\n",
      "epoch:41 step:32089 [D loss: 0.266011, acc.: 89.06%] [G loss: 4.662615]\n",
      "epoch:41 step:32090 [D loss: 0.354965, acc.: 84.38%] [G loss: 4.764988]\n",
      "epoch:41 step:32091 [D loss: 0.332393, acc.: 82.81%] [G loss: 3.252723]\n",
      "epoch:41 step:32092 [D loss: 0.312862, acc.: 89.84%] [G loss: 3.732991]\n",
      "epoch:41 step:32093 [D loss: 0.276736, acc.: 90.62%] [G loss: 2.552801]\n",
      "epoch:41 step:32094 [D loss: 0.266060, acc.: 85.16%] [G loss: 3.040152]\n",
      "epoch:41 step:32095 [D loss: 0.318745, acc.: 85.16%] [G loss: 3.233562]\n",
      "epoch:41 step:32096 [D loss: 0.322742, acc.: 85.16%] [G loss: 2.649467]\n",
      "epoch:41 step:32097 [D loss: 0.413491, acc.: 82.03%] [G loss: 3.212881]\n",
      "epoch:41 step:32098 [D loss: 0.270422, acc.: 92.19%] [G loss: 3.269210]\n",
      "epoch:41 step:32099 [D loss: 0.322846, acc.: 84.38%] [G loss: 2.881958]\n",
      "epoch:41 step:32100 [D loss: 0.320327, acc.: 85.16%] [G loss: 3.693896]\n",
      "epoch:41 step:32101 [D loss: 0.322419, acc.: 85.16%] [G loss: 4.042007]\n",
      "epoch:41 step:32102 [D loss: 0.216506, acc.: 90.62%] [G loss: 3.935720]\n",
      "epoch:41 step:32103 [D loss: 0.205893, acc.: 91.41%] [G loss: 2.812645]\n",
      "epoch:41 step:32104 [D loss: 0.302967, acc.: 89.84%] [G loss: 3.965361]\n",
      "epoch:41 step:32105 [D loss: 0.228245, acc.: 89.84%] [G loss: 3.241637]\n",
      "epoch:41 step:32106 [D loss: 0.323536, acc.: 84.38%] [G loss: 2.794712]\n",
      "epoch:41 step:32107 [D loss: 0.343485, acc.: 85.16%] [G loss: 3.223276]\n",
      "epoch:41 step:32108 [D loss: 0.261882, acc.: 88.28%] [G loss: 2.933836]\n",
      "epoch:41 step:32109 [D loss: 0.364315, acc.: 82.81%] [G loss: 2.966901]\n",
      "epoch:41 step:32110 [D loss: 0.350477, acc.: 82.81%] [G loss: 3.605819]\n",
      "epoch:41 step:32111 [D loss: 0.315348, acc.: 82.81%] [G loss: 3.769091]\n",
      "epoch:41 step:32112 [D loss: 0.297976, acc.: 82.81%] [G loss: 3.416848]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:41 step:32113 [D loss: 0.233747, acc.: 89.84%] [G loss: 3.911433]\n",
      "epoch:41 step:32114 [D loss: 0.339022, acc.: 85.16%] [G loss: 3.773614]\n",
      "epoch:41 step:32115 [D loss: 0.408221, acc.: 79.69%] [G loss: 3.534289]\n",
      "epoch:41 step:32116 [D loss: 0.336157, acc.: 85.16%] [G loss: 3.770183]\n",
      "epoch:41 step:32117 [D loss: 0.282070, acc.: 90.62%] [G loss: 3.388308]\n",
      "epoch:41 step:32118 [D loss: 0.321794, acc.: 85.16%] [G loss: 4.022024]\n",
      "epoch:41 step:32119 [D loss: 0.391851, acc.: 78.12%] [G loss: 5.566735]\n",
      "epoch:41 step:32120 [D loss: 0.388343, acc.: 82.81%] [G loss: 3.772167]\n",
      "epoch:41 step:32121 [D loss: 0.334420, acc.: 85.94%] [G loss: 3.735014]\n",
      "epoch:41 step:32122 [D loss: 0.303619, acc.: 85.16%] [G loss: 3.223875]\n",
      "epoch:41 step:32123 [D loss: 0.302729, acc.: 87.50%] [G loss: 3.440085]\n",
      "epoch:41 step:32124 [D loss: 0.339522, acc.: 80.47%] [G loss: 3.097828]\n",
      "epoch:41 step:32125 [D loss: 0.277413, acc.: 85.94%] [G loss: 3.939518]\n",
      "epoch:41 step:32126 [D loss: 0.228309, acc.: 90.62%] [G loss: 3.375822]\n",
      "epoch:41 step:32127 [D loss: 0.403193, acc.: 76.56%] [G loss: 2.818476]\n",
      "epoch:41 step:32128 [D loss: 0.296897, acc.: 88.28%] [G loss: 3.216906]\n",
      "epoch:41 step:32129 [D loss: 0.247823, acc.: 85.94%] [G loss: 3.260222]\n",
      "epoch:41 step:32130 [D loss: 0.462794, acc.: 76.56%] [G loss: 2.763801]\n",
      "epoch:41 step:32131 [D loss: 0.291374, acc.: 88.28%] [G loss: 2.872753]\n",
      "epoch:41 step:32132 [D loss: 0.442513, acc.: 83.59%] [G loss: 4.345843]\n",
      "epoch:41 step:32133 [D loss: 0.401155, acc.: 82.81%] [G loss: 2.911562]\n",
      "epoch:41 step:32134 [D loss: 0.241362, acc.: 91.41%] [G loss: 3.711410]\n",
      "epoch:41 step:32135 [D loss: 0.303452, acc.: 88.28%] [G loss: 3.184212]\n",
      "epoch:41 step:32136 [D loss: 0.342822, acc.: 86.72%] [G loss: 2.837893]\n",
      "epoch:41 step:32137 [D loss: 0.315833, acc.: 85.94%] [G loss: 2.674111]\n",
      "epoch:41 step:32138 [D loss: 0.377515, acc.: 83.59%] [G loss: 2.963839]\n",
      "epoch:41 step:32139 [D loss: 0.284957, acc.: 87.50%] [G loss: 2.764921]\n",
      "epoch:41 step:32140 [D loss: 0.396040, acc.: 79.69%] [G loss: 2.535257]\n",
      "epoch:41 step:32141 [D loss: 0.397543, acc.: 82.03%] [G loss: 3.921362]\n",
      "epoch:41 step:32142 [D loss: 0.318412, acc.: 85.16%] [G loss: 3.752998]\n",
      "epoch:41 step:32143 [D loss: 0.308518, acc.: 85.94%] [G loss: 4.570635]\n",
      "epoch:41 step:32144 [D loss: 0.343781, acc.: 82.81%] [G loss: 3.492196]\n",
      "epoch:41 step:32145 [D loss: 0.247217, acc.: 89.84%] [G loss: 4.092373]\n",
      "epoch:41 step:32146 [D loss: 0.377298, acc.: 83.59%] [G loss: 3.407678]\n",
      "epoch:41 step:32147 [D loss: 0.302392, acc.: 85.94%] [G loss: 2.545966]\n",
      "epoch:41 step:32148 [D loss: 0.272304, acc.: 85.16%] [G loss: 3.049861]\n",
      "epoch:41 step:32149 [D loss: 0.217695, acc.: 92.19%] [G loss: 2.874091]\n",
      "epoch:41 step:32150 [D loss: 0.366092, acc.: 87.50%] [G loss: 3.944980]\n",
      "epoch:41 step:32151 [D loss: 0.299332, acc.: 86.72%] [G loss: 4.985086]\n",
      "epoch:41 step:32152 [D loss: 0.496419, acc.: 79.69%] [G loss: 2.431014]\n",
      "epoch:41 step:32153 [D loss: 0.251466, acc.: 84.38%] [G loss: 3.267354]\n",
      "epoch:41 step:32154 [D loss: 0.266790, acc.: 88.28%] [G loss: 4.610661]\n",
      "epoch:41 step:32155 [D loss: 0.320719, acc.: 89.06%] [G loss: 2.972547]\n",
      "epoch:41 step:32156 [D loss: 0.284946, acc.: 89.84%] [G loss: 4.256386]\n",
      "epoch:41 step:32157 [D loss: 0.205909, acc.: 89.06%] [G loss: 5.942314]\n",
      "epoch:41 step:32158 [D loss: 0.384013, acc.: 85.16%] [G loss: 3.532148]\n",
      "epoch:41 step:32159 [D loss: 0.252097, acc.: 88.28%] [G loss: 3.475937]\n",
      "epoch:41 step:32160 [D loss: 0.280192, acc.: 88.28%] [G loss: 3.737560]\n",
      "epoch:41 step:32161 [D loss: 0.313047, acc.: 85.94%] [G loss: 3.334688]\n",
      "epoch:41 step:32162 [D loss: 0.341816, acc.: 82.81%] [G loss: 3.907408]\n",
      "epoch:41 step:32163 [D loss: 0.310369, acc.: 85.94%] [G loss: 3.613060]\n",
      "epoch:41 step:32164 [D loss: 0.265011, acc.: 86.72%] [G loss: 3.563282]\n",
      "epoch:41 step:32165 [D loss: 0.378149, acc.: 81.25%] [G loss: 3.888432]\n",
      "epoch:41 step:32166 [D loss: 0.316381, acc.: 89.84%] [G loss: 4.565156]\n",
      "epoch:41 step:32167 [D loss: 0.338513, acc.: 84.38%] [G loss: 3.444064]\n",
      "epoch:41 step:32168 [D loss: 0.235010, acc.: 89.06%] [G loss: 3.279049]\n",
      "epoch:41 step:32169 [D loss: 0.360804, acc.: 82.03%] [G loss: 4.666696]\n",
      "epoch:41 step:32170 [D loss: 0.333695, acc.: 82.81%] [G loss: 3.452312]\n",
      "epoch:41 step:32171 [D loss: 0.296945, acc.: 82.81%] [G loss: 3.371304]\n",
      "epoch:41 step:32172 [D loss: 0.220216, acc.: 91.41%] [G loss: 4.166134]\n",
      "epoch:41 step:32173 [D loss: 0.356274, acc.: 82.81%] [G loss: 2.828521]\n",
      "epoch:41 step:32174 [D loss: 0.316518, acc.: 80.47%] [G loss: 3.302327]\n",
      "epoch:41 step:32175 [D loss: 0.271517, acc.: 86.72%] [G loss: 3.206311]\n",
      "epoch:41 step:32176 [D loss: 0.350058, acc.: 85.16%] [G loss: 3.007530]\n",
      "epoch:41 step:32177 [D loss: 0.337914, acc.: 82.81%] [G loss: 2.677730]\n",
      "epoch:41 step:32178 [D loss: 0.308251, acc.: 87.50%] [G loss: 4.617053]\n",
      "epoch:41 step:32179 [D loss: 0.370371, acc.: 85.16%] [G loss: 4.165663]\n",
      "epoch:41 step:32180 [D loss: 0.233143, acc.: 88.28%] [G loss: 3.606576]\n",
      "epoch:41 step:32181 [D loss: 0.296707, acc.: 83.59%] [G loss: 3.370895]\n",
      "epoch:41 step:32182 [D loss: 0.340506, acc.: 85.16%] [G loss: 3.675561]\n",
      "epoch:41 step:32183 [D loss: 0.376101, acc.: 82.03%] [G loss: 3.212523]\n",
      "epoch:41 step:32184 [D loss: 0.317597, acc.: 86.72%] [G loss: 4.969637]\n",
      "epoch:41 step:32185 [D loss: 0.285531, acc.: 87.50%] [G loss: 2.751290]\n",
      "epoch:41 step:32186 [D loss: 0.311118, acc.: 85.16%] [G loss: 3.754241]\n",
      "epoch:41 step:32187 [D loss: 0.314457, acc.: 89.06%] [G loss: 2.517318]\n",
      "epoch:41 step:32188 [D loss: 0.296020, acc.: 88.28%] [G loss: 3.728046]\n",
      "epoch:41 step:32189 [D loss: 0.358224, acc.: 82.03%] [G loss: 4.005660]\n",
      "epoch:41 step:32190 [D loss: 0.266575, acc.: 88.28%] [G loss: 3.630571]\n",
      "epoch:41 step:32191 [D loss: 0.297399, acc.: 85.94%] [G loss: 3.322975]\n",
      "epoch:41 step:32192 [D loss: 0.310991, acc.: 84.38%] [G loss: 3.677431]\n",
      "epoch:41 step:32193 [D loss: 0.355855, acc.: 83.59%] [G loss: 3.768010]\n",
      "epoch:41 step:32194 [D loss: 0.257487, acc.: 89.06%] [G loss: 5.058075]\n",
      "epoch:41 step:32195 [D loss: 0.374594, acc.: 83.59%] [G loss: 5.757545]\n",
      "epoch:41 step:32196 [D loss: 0.346644, acc.: 88.28%] [G loss: 3.198907]\n",
      "epoch:41 step:32197 [D loss: 0.277968, acc.: 87.50%] [G loss: 3.808924]\n",
      "epoch:41 step:32198 [D loss: 0.308385, acc.: 89.06%] [G loss: 2.971684]\n",
      "epoch:41 step:32199 [D loss: 0.211511, acc.: 90.62%] [G loss: 3.706484]\n",
      "epoch:41 step:32200 [D loss: 0.263241, acc.: 85.94%] [G loss: 3.332780]\n",
      "##############\n",
      "[0.85376011 0.88754255 0.79826741 0.80863102 0.77900578 0.8254877\n",
      " 0.85009688 0.82084741 0.83962355 0.80233619]\n",
      "##########\n",
      "epoch:41 step:32201 [D loss: 0.495109, acc.: 74.22%] [G loss: 3.146969]\n",
      "epoch:41 step:32202 [D loss: 0.207422, acc.: 89.84%] [G loss: 3.837987]\n",
      "epoch:41 step:32203 [D loss: 0.424181, acc.: 82.81%] [G loss: 2.881853]\n",
      "epoch:41 step:32204 [D loss: 0.284038, acc.: 82.03%] [G loss: 4.543413]\n",
      "epoch:41 step:32205 [D loss: 0.218576, acc.: 91.41%] [G loss: 2.774457]\n",
      "epoch:41 step:32206 [D loss: 0.204362, acc.: 93.75%] [G loss: 3.626026]\n",
      "epoch:41 step:32207 [D loss: 0.288153, acc.: 82.81%] [G loss: 4.423065]\n",
      "epoch:41 step:32208 [D loss: 0.355438, acc.: 85.16%] [G loss: 3.011625]\n",
      "epoch:41 step:32209 [D loss: 0.370815, acc.: 83.59%] [G loss: 2.871369]\n",
      "epoch:41 step:32210 [D loss: 0.278415, acc.: 86.72%] [G loss: 3.433936]\n",
      "epoch:41 step:32211 [D loss: 0.386249, acc.: 83.59%] [G loss: 3.163334]\n",
      "epoch:41 step:32212 [D loss: 0.349057, acc.: 82.81%] [G loss: 4.753352]\n",
      "epoch:41 step:32213 [D loss: 0.394708, acc.: 82.81%] [G loss: 3.801977]\n",
      "epoch:41 step:32214 [D loss: 0.463016, acc.: 78.91%] [G loss: 3.841216]\n",
      "epoch:41 step:32215 [D loss: 0.249206, acc.: 89.06%] [G loss: 5.457333]\n",
      "epoch:41 step:32216 [D loss: 0.246787, acc.: 87.50%] [G loss: 3.238628]\n",
      "epoch:41 step:32217 [D loss: 0.249555, acc.: 89.06%] [G loss: 3.199688]\n",
      "epoch:41 step:32218 [D loss: 0.298390, acc.: 87.50%] [G loss: 2.652095]\n",
      "epoch:41 step:32219 [D loss: 0.259903, acc.: 89.06%] [G loss: 3.326958]\n",
      "epoch:41 step:32220 [D loss: 0.262449, acc.: 86.72%] [G loss: 3.257736]\n",
      "epoch:41 step:32221 [D loss: 0.354633, acc.: 79.69%] [G loss: 3.642667]\n",
      "epoch:41 step:32222 [D loss: 0.258268, acc.: 89.84%] [G loss: 3.235092]\n",
      "epoch:41 step:32223 [D loss: 0.338677, acc.: 84.38%] [G loss: 3.473783]\n",
      "epoch:41 step:32224 [D loss: 0.305654, acc.: 89.06%] [G loss: 3.230406]\n",
      "epoch:41 step:32225 [D loss: 0.347341, acc.: 83.59%] [G loss: 3.291845]\n",
      "epoch:41 step:32226 [D loss: 0.386943, acc.: 80.47%] [G loss: 3.656883]\n",
      "epoch:41 step:32227 [D loss: 0.358574, acc.: 82.81%] [G loss: 2.964705]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:41 step:32228 [D loss: 0.276230, acc.: 87.50%] [G loss: 2.962945]\n",
      "epoch:41 step:32229 [D loss: 0.304203, acc.: 88.28%] [G loss: 3.202872]\n",
      "epoch:41 step:32230 [D loss: 0.353183, acc.: 82.81%] [G loss: 2.824281]\n",
      "epoch:41 step:32231 [D loss: 0.266757, acc.: 86.72%] [G loss: 3.153101]\n",
      "epoch:41 step:32232 [D loss: 0.358797, acc.: 79.69%] [G loss: 3.015428]\n",
      "epoch:41 step:32233 [D loss: 0.355834, acc.: 83.59%] [G loss: 3.540169]\n",
      "epoch:41 step:32234 [D loss: 0.272740, acc.: 85.94%] [G loss: 4.481303]\n",
      "epoch:41 step:32235 [D loss: 0.336863, acc.: 84.38%] [G loss: 3.790481]\n",
      "epoch:41 step:32236 [D loss: 0.305831, acc.: 85.94%] [G loss: 3.912651]\n",
      "epoch:41 step:32237 [D loss: 0.326881, acc.: 84.38%] [G loss: 3.201234]\n",
      "epoch:41 step:32238 [D loss: 0.282533, acc.: 86.72%] [G loss: 3.751723]\n",
      "epoch:41 step:32239 [D loss: 0.426637, acc.: 81.25%] [G loss: 4.046348]\n",
      "epoch:41 step:32240 [D loss: 0.303519, acc.: 89.06%] [G loss: 4.317431]\n",
      "epoch:41 step:32241 [D loss: 0.201492, acc.: 92.19%] [G loss: 4.299555]\n",
      "epoch:41 step:32242 [D loss: 0.215317, acc.: 92.19%] [G loss: 5.992838]\n",
      "epoch:41 step:32243 [D loss: 0.232352, acc.: 89.84%] [G loss: 4.415433]\n",
      "epoch:41 step:32244 [D loss: 0.431774, acc.: 82.81%] [G loss: 4.271756]\n",
      "epoch:41 step:32245 [D loss: 0.334857, acc.: 87.50%] [G loss: 4.467325]\n",
      "epoch:41 step:32246 [D loss: 0.289746, acc.: 89.06%] [G loss: 4.174967]\n",
      "epoch:41 step:32247 [D loss: 0.482283, acc.: 78.91%] [G loss: 4.062294]\n",
      "epoch:41 step:32248 [D loss: 0.314243, acc.: 83.59%] [G loss: 3.722517]\n",
      "epoch:41 step:32249 [D loss: 0.309685, acc.: 85.94%] [G loss: 3.349219]\n",
      "epoch:41 step:32250 [D loss: 0.449982, acc.: 82.81%] [G loss: 3.497459]\n",
      "epoch:41 step:32251 [D loss: 0.233199, acc.: 91.41%] [G loss: 3.420686]\n",
      "epoch:41 step:32252 [D loss: 0.239557, acc.: 89.84%] [G loss: 3.585941]\n",
      "epoch:41 step:32253 [D loss: 0.342685, acc.: 85.16%] [G loss: 3.449376]\n",
      "epoch:41 step:32254 [D loss: 0.329511, acc.: 85.16%] [G loss: 3.119483]\n",
      "epoch:41 step:32255 [D loss: 0.321035, acc.: 89.84%] [G loss: 2.902681]\n",
      "epoch:41 step:32256 [D loss: 0.334835, acc.: 83.59%] [G loss: 2.811704]\n",
      "epoch:41 step:32257 [D loss: 0.204237, acc.: 92.97%] [G loss: 3.580281]\n",
      "epoch:41 step:32258 [D loss: 0.296284, acc.: 88.28%] [G loss: 2.498768]\n",
      "epoch:41 step:32259 [D loss: 0.379317, acc.: 82.03%] [G loss: 3.906106]\n",
      "epoch:41 step:32260 [D loss: 0.366893, acc.: 85.16%] [G loss: 5.942281]\n",
      "epoch:41 step:32261 [D loss: 0.407998, acc.: 80.47%] [G loss: 6.964656]\n",
      "epoch:41 step:32262 [D loss: 0.289920, acc.: 84.38%] [G loss: 3.690431]\n",
      "epoch:41 step:32263 [D loss: 0.330783, acc.: 85.16%] [G loss: 4.626486]\n",
      "epoch:41 step:32264 [D loss: 0.242535, acc.: 89.06%] [G loss: 4.008698]\n",
      "epoch:41 step:32265 [D loss: 0.426714, acc.: 79.69%] [G loss: 4.246957]\n",
      "epoch:41 step:32266 [D loss: 0.355250, acc.: 86.72%] [G loss: 3.609776]\n",
      "epoch:41 step:32267 [D loss: 0.414638, acc.: 78.91%] [G loss: 4.563145]\n",
      "epoch:41 step:32268 [D loss: 0.291702, acc.: 85.94%] [G loss: 3.961462]\n",
      "epoch:41 step:32269 [D loss: 0.313683, acc.: 88.28%] [G loss: 2.648589]\n",
      "epoch:41 step:32270 [D loss: 0.274616, acc.: 89.06%] [G loss: 2.785670]\n",
      "epoch:41 step:32271 [D loss: 0.393163, acc.: 78.91%] [G loss: 3.116915]\n",
      "epoch:41 step:32272 [D loss: 0.292133, acc.: 88.28%] [G loss: 3.129132]\n",
      "epoch:41 step:32273 [D loss: 0.441014, acc.: 82.03%] [G loss: 4.189041]\n",
      "epoch:41 step:32274 [D loss: 0.284994, acc.: 88.28%] [G loss: 3.367198]\n",
      "epoch:41 step:32275 [D loss: 0.260193, acc.: 89.84%] [G loss: 4.648284]\n",
      "epoch:41 step:32276 [D loss: 0.422287, acc.: 79.69%] [G loss: 3.658315]\n",
      "epoch:41 step:32277 [D loss: 0.379313, acc.: 82.81%] [G loss: 3.793918]\n",
      "epoch:41 step:32278 [D loss: 0.409945, acc.: 80.47%] [G loss: 3.263064]\n",
      "epoch:41 step:32279 [D loss: 0.345814, acc.: 82.03%] [G loss: 3.201002]\n",
      "epoch:41 step:32280 [D loss: 0.390834, acc.: 82.81%] [G loss: 2.965838]\n",
      "epoch:41 step:32281 [D loss: 0.369903, acc.: 82.81%] [G loss: 3.653967]\n",
      "epoch:41 step:32282 [D loss: 0.322954, acc.: 87.50%] [G loss: 6.046612]\n",
      "epoch:41 step:32283 [D loss: 0.488045, acc.: 80.47%] [G loss: 4.752947]\n",
      "epoch:41 step:32284 [D loss: 0.377905, acc.: 85.16%] [G loss: 3.571028]\n",
      "epoch:41 step:32285 [D loss: 0.324345, acc.: 85.94%] [G loss: 3.295770]\n",
      "epoch:41 step:32286 [D loss: 0.352826, acc.: 82.03%] [G loss: 2.685092]\n",
      "epoch:41 step:32287 [D loss: 0.312348, acc.: 88.28%] [G loss: 3.348675]\n",
      "epoch:41 step:32288 [D loss: 0.279389, acc.: 87.50%] [G loss: 4.205911]\n",
      "epoch:41 step:32289 [D loss: 0.399627, acc.: 79.69%] [G loss: 3.543222]\n",
      "epoch:41 step:32290 [D loss: 0.293536, acc.: 85.16%] [G loss: 4.385887]\n",
      "epoch:41 step:32291 [D loss: 0.299410, acc.: 85.16%] [G loss: 3.579350]\n",
      "epoch:41 step:32292 [D loss: 0.260266, acc.: 90.62%] [G loss: 3.603394]\n",
      "epoch:41 step:32293 [D loss: 0.359280, acc.: 82.81%] [G loss: 3.607694]\n",
      "epoch:41 step:32294 [D loss: 0.296060, acc.: 85.16%] [G loss: 3.684752]\n",
      "epoch:41 step:32295 [D loss: 0.268215, acc.: 89.84%] [G loss: 3.142957]\n",
      "epoch:41 step:32296 [D loss: 0.303499, acc.: 86.72%] [G loss: 3.442618]\n",
      "epoch:41 step:32297 [D loss: 0.394644, acc.: 75.78%] [G loss: 3.427999]\n",
      "epoch:41 step:32298 [D loss: 0.405195, acc.: 82.81%] [G loss: 4.065530]\n",
      "epoch:41 step:32299 [D loss: 0.391292, acc.: 84.38%] [G loss: 3.234843]\n",
      "epoch:41 step:32300 [D loss: 0.401467, acc.: 79.69%] [G loss: 2.771496]\n",
      "epoch:41 step:32301 [D loss: 0.279750, acc.: 89.06%] [G loss: 3.214692]\n",
      "epoch:41 step:32302 [D loss: 0.327581, acc.: 85.94%] [G loss: 2.268488]\n",
      "epoch:41 step:32303 [D loss: 0.356819, acc.: 83.59%] [G loss: 2.276407]\n",
      "epoch:41 step:32304 [D loss: 0.392810, acc.: 80.47%] [G loss: 3.311788]\n",
      "epoch:41 step:32305 [D loss: 0.387817, acc.: 81.25%] [G loss: 3.520226]\n",
      "epoch:41 step:32306 [D loss: 0.371373, acc.: 81.25%] [G loss: 3.667700]\n",
      "epoch:41 step:32307 [D loss: 0.371041, acc.: 81.25%] [G loss: 2.715893]\n",
      "epoch:41 step:32308 [D loss: 0.323332, acc.: 89.84%] [G loss: 3.087420]\n",
      "epoch:41 step:32309 [D loss: 0.334490, acc.: 84.38%] [G loss: 2.847693]\n",
      "epoch:41 step:32310 [D loss: 0.386459, acc.: 78.91%] [G loss: 2.702499]\n",
      "epoch:41 step:32311 [D loss: 0.358121, acc.: 84.38%] [G loss: 4.330005]\n",
      "epoch:41 step:32312 [D loss: 0.260051, acc.: 92.19%] [G loss: 3.167090]\n",
      "epoch:41 step:32313 [D loss: 0.349480, acc.: 83.59%] [G loss: 2.855471]\n",
      "epoch:41 step:32314 [D loss: 0.333083, acc.: 85.94%] [G loss: 3.852615]\n",
      "epoch:41 step:32315 [D loss: 0.487410, acc.: 78.12%] [G loss: 2.720028]\n",
      "epoch:41 step:32316 [D loss: 0.364806, acc.: 84.38%] [G loss: 3.047442]\n",
      "epoch:41 step:32317 [D loss: 0.319197, acc.: 86.72%] [G loss: 2.441201]\n",
      "epoch:41 step:32318 [D loss: 0.500321, acc.: 75.78%] [G loss: 3.235812]\n",
      "epoch:41 step:32319 [D loss: 0.346923, acc.: 88.28%] [G loss: 4.160487]\n",
      "epoch:41 step:32320 [D loss: 0.318671, acc.: 85.16%] [G loss: 3.372784]\n",
      "epoch:41 step:32321 [D loss: 0.303723, acc.: 88.28%] [G loss: 2.939990]\n",
      "epoch:41 step:32322 [D loss: 0.420130, acc.: 77.34%] [G loss: 3.856415]\n",
      "epoch:41 step:32323 [D loss: 0.397883, acc.: 82.03%] [G loss: 3.071386]\n",
      "epoch:41 step:32324 [D loss: 0.324668, acc.: 80.47%] [G loss: 3.817274]\n",
      "epoch:41 step:32325 [D loss: 0.326033, acc.: 86.72%] [G loss: 2.749315]\n",
      "epoch:41 step:32326 [D loss: 0.374954, acc.: 83.59%] [G loss: 3.614048]\n",
      "epoch:41 step:32327 [D loss: 0.314633, acc.: 84.38%] [G loss: 2.728776]\n",
      "epoch:41 step:32328 [D loss: 0.341422, acc.: 87.50%] [G loss: 4.073740]\n",
      "epoch:41 step:32329 [D loss: 0.336884, acc.: 86.72%] [G loss: 2.913204]\n",
      "epoch:41 step:32330 [D loss: 0.248731, acc.: 89.84%] [G loss: 3.569308]\n",
      "epoch:41 step:32331 [D loss: 0.232120, acc.: 90.62%] [G loss: 3.731058]\n",
      "epoch:41 step:32332 [D loss: 0.304392, acc.: 82.03%] [G loss: 4.930516]\n",
      "epoch:41 step:32333 [D loss: 0.313018, acc.: 85.16%] [G loss: 3.444948]\n",
      "epoch:41 step:32334 [D loss: 0.215215, acc.: 92.19%] [G loss: 5.955037]\n",
      "epoch:41 step:32335 [D loss: 0.279451, acc.: 84.38%] [G loss: 3.892202]\n",
      "epoch:41 step:32336 [D loss: 0.325677, acc.: 81.25%] [G loss: 4.861294]\n",
      "epoch:41 step:32337 [D loss: 0.323377, acc.: 82.81%] [G loss: 3.409595]\n",
      "epoch:41 step:32338 [D loss: 0.463392, acc.: 78.91%] [G loss: 3.519195]\n",
      "epoch:41 step:32339 [D loss: 0.264149, acc.: 92.19%] [G loss: 2.728567]\n",
      "epoch:41 step:32340 [D loss: 0.316113, acc.: 86.72%] [G loss: 2.815772]\n",
      "epoch:41 step:32341 [D loss: 0.290881, acc.: 87.50%] [G loss: 2.788044]\n",
      "epoch:41 step:32342 [D loss: 0.326005, acc.: 85.94%] [G loss: 3.284276]\n",
      "epoch:41 step:32343 [D loss: 0.267067, acc.: 87.50%] [G loss: 3.013172]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:41 step:32344 [D loss: 0.354355, acc.: 82.03%] [G loss: 4.004926]\n",
      "epoch:41 step:32345 [D loss: 0.329940, acc.: 86.72%] [G loss: 2.724637]\n",
      "epoch:41 step:32346 [D loss: 0.368377, acc.: 82.03%] [G loss: 3.699763]\n",
      "epoch:41 step:32347 [D loss: 0.288893, acc.: 90.62%] [G loss: 3.223852]\n",
      "epoch:41 step:32348 [D loss: 0.311368, acc.: 85.16%] [G loss: 3.834267]\n",
      "epoch:41 step:32349 [D loss: 0.228006, acc.: 91.41%] [G loss: 4.212254]\n",
      "epoch:41 step:32350 [D loss: 0.375970, acc.: 84.38%] [G loss: 3.185826]\n",
      "epoch:41 step:32351 [D loss: 0.274910, acc.: 85.94%] [G loss: 2.689829]\n",
      "epoch:41 step:32352 [D loss: 0.333836, acc.: 84.38%] [G loss: 3.001879]\n",
      "epoch:41 step:32353 [D loss: 0.342482, acc.: 84.38%] [G loss: 2.907847]\n",
      "epoch:41 step:32354 [D loss: 0.369968, acc.: 85.16%] [G loss: 3.129967]\n",
      "epoch:41 step:32355 [D loss: 0.410674, acc.: 83.59%] [G loss: 3.658238]\n",
      "epoch:41 step:32356 [D loss: 0.289655, acc.: 87.50%] [G loss: 3.681297]\n",
      "epoch:41 step:32357 [D loss: 0.370960, acc.: 85.16%] [G loss: 3.582115]\n",
      "epoch:41 step:32358 [D loss: 0.339638, acc.: 86.72%] [G loss: 2.844261]\n",
      "epoch:41 step:32359 [D loss: 0.260616, acc.: 88.28%] [G loss: 4.057367]\n",
      "epoch:41 step:32360 [D loss: 0.310358, acc.: 89.06%] [G loss: 3.290248]\n",
      "epoch:41 step:32361 [D loss: 0.274179, acc.: 89.06%] [G loss: 3.898433]\n",
      "epoch:41 step:32362 [D loss: 0.254927, acc.: 85.94%] [G loss: 2.546288]\n",
      "epoch:41 step:32363 [D loss: 0.302682, acc.: 85.16%] [G loss: 3.725898]\n",
      "epoch:41 step:32364 [D loss: 0.385765, acc.: 85.16%] [G loss: 3.622710]\n",
      "epoch:41 step:32365 [D loss: 0.316191, acc.: 82.03%] [G loss: 2.483899]\n",
      "epoch:41 step:32366 [D loss: 0.406736, acc.: 82.81%] [G loss: 3.324351]\n",
      "epoch:41 step:32367 [D loss: 0.371065, acc.: 83.59%] [G loss: 3.469564]\n",
      "epoch:41 step:32368 [D loss: 0.388094, acc.: 82.81%] [G loss: 4.229463]\n",
      "epoch:41 step:32369 [D loss: 0.434196, acc.: 82.03%] [G loss: 6.098568]\n",
      "epoch:41 step:32370 [D loss: 0.648462, acc.: 71.88%] [G loss: 5.122355]\n",
      "epoch:41 step:32371 [D loss: 0.825930, acc.: 71.09%] [G loss: 5.286808]\n",
      "epoch:41 step:32372 [D loss: 0.848435, acc.: 70.31%] [G loss: 4.435417]\n",
      "epoch:41 step:32373 [D loss: 0.348002, acc.: 86.72%] [G loss: 4.634086]\n",
      "epoch:41 step:32374 [D loss: 0.604335, acc.: 75.78%] [G loss: 2.697115]\n",
      "epoch:41 step:32375 [D loss: 0.311220, acc.: 87.50%] [G loss: 4.709728]\n",
      "epoch:41 step:32376 [D loss: 0.278173, acc.: 89.84%] [G loss: 4.251390]\n",
      "epoch:41 step:32377 [D loss: 0.330431, acc.: 88.28%] [G loss: 6.605927]\n",
      "epoch:41 step:32378 [D loss: 0.379853, acc.: 80.47%] [G loss: 5.012651]\n",
      "epoch:41 step:32379 [D loss: 0.151218, acc.: 93.75%] [G loss: 3.900934]\n",
      "epoch:41 step:32380 [D loss: 0.233723, acc.: 89.84%] [G loss: 3.300899]\n",
      "epoch:41 step:32381 [D loss: 0.417541, acc.: 82.03%] [G loss: 3.497082]\n",
      "epoch:41 step:32382 [D loss: 0.280115, acc.: 89.84%] [G loss: 3.622838]\n",
      "epoch:41 step:32383 [D loss: 0.291716, acc.: 85.94%] [G loss: 3.648891]\n",
      "epoch:41 step:32384 [D loss: 0.288807, acc.: 88.28%] [G loss: 3.097523]\n",
      "epoch:41 step:32385 [D loss: 0.294992, acc.: 86.72%] [G loss: 2.845021]\n",
      "epoch:41 step:32386 [D loss: 0.228860, acc.: 89.06%] [G loss: 3.150959]\n",
      "epoch:41 step:32387 [D loss: 0.302243, acc.: 86.72%] [G loss: 2.872821]\n",
      "epoch:41 step:32388 [D loss: 0.308176, acc.: 89.06%] [G loss: 3.052049]\n",
      "epoch:41 step:32389 [D loss: 0.348861, acc.: 86.72%] [G loss: 2.884695]\n",
      "epoch:41 step:32390 [D loss: 0.390698, acc.: 77.34%] [G loss: 2.939779]\n",
      "epoch:41 step:32391 [D loss: 0.292222, acc.: 87.50%] [G loss: 4.099996]\n",
      "epoch:41 step:32392 [D loss: 0.289722, acc.: 85.94%] [G loss: 2.792818]\n",
      "epoch:41 step:32393 [D loss: 0.323640, acc.: 85.94%] [G loss: 3.238315]\n",
      "epoch:41 step:32394 [D loss: 0.413529, acc.: 81.25%] [G loss: 3.580008]\n",
      "epoch:41 step:32395 [D loss: 0.298791, acc.: 87.50%] [G loss: 4.195616]\n",
      "epoch:41 step:32396 [D loss: 0.257897, acc.: 89.84%] [G loss: 2.646083]\n",
      "epoch:41 step:32397 [D loss: 0.191094, acc.: 93.75%] [G loss: 3.941133]\n",
      "epoch:41 step:32398 [D loss: 0.381397, acc.: 82.81%] [G loss: 3.097738]\n",
      "epoch:41 step:32399 [D loss: 0.372346, acc.: 82.81%] [G loss: 3.306766]\n",
      "epoch:41 step:32400 [D loss: 0.349748, acc.: 85.16%] [G loss: 3.428109]\n",
      "##############\n",
      "[0.87092289 0.85932064 0.81625179 0.80147512 0.74898069 0.82920061\n",
      " 0.88781611 0.82573839 0.8030344  0.8334243 ]\n",
      "##########\n",
      "epoch:41 step:32401 [D loss: 0.287622, acc.: 89.06%] [G loss: 2.663396]\n",
      "epoch:41 step:32402 [D loss: 0.341174, acc.: 88.28%] [G loss: 2.965620]\n",
      "epoch:41 step:32403 [D loss: 0.459637, acc.: 80.47%] [G loss: 3.183450]\n",
      "epoch:41 step:32404 [D loss: 0.261430, acc.: 89.06%] [G loss: 3.452241]\n",
      "epoch:41 step:32405 [D loss: 0.226247, acc.: 88.28%] [G loss: 3.044440]\n",
      "epoch:41 step:32406 [D loss: 0.317148, acc.: 86.72%] [G loss: 2.782823]\n",
      "epoch:41 step:32407 [D loss: 0.341198, acc.: 85.16%] [G loss: 2.546011]\n",
      "epoch:41 step:32408 [D loss: 0.373044, acc.: 82.03%] [G loss: 2.799499]\n",
      "epoch:41 step:32409 [D loss: 0.270022, acc.: 86.72%] [G loss: 3.832101]\n",
      "epoch:41 step:32410 [D loss: 0.369764, acc.: 82.81%] [G loss: 2.975452]\n",
      "epoch:41 step:32411 [D loss: 0.323168, acc.: 84.38%] [G loss: 3.065374]\n",
      "epoch:41 step:32412 [D loss: 0.414953, acc.: 79.69%] [G loss: 3.825070]\n",
      "epoch:41 step:32413 [D loss: 0.369258, acc.: 82.81%] [G loss: 3.323870]\n",
      "epoch:41 step:32414 [D loss: 0.333404, acc.: 88.28%] [G loss: 4.497048]\n",
      "epoch:41 step:32415 [D loss: 0.228404, acc.: 90.62%] [G loss: 3.420077]\n",
      "epoch:41 step:32416 [D loss: 0.260809, acc.: 87.50%] [G loss: 3.471040]\n",
      "epoch:41 step:32417 [D loss: 0.317710, acc.: 85.94%] [G loss: 3.547028]\n",
      "epoch:41 step:32418 [D loss: 0.264744, acc.: 88.28%] [G loss: 3.567943]\n",
      "epoch:41 step:32419 [D loss: 0.313447, acc.: 86.72%] [G loss: 2.840469]\n",
      "epoch:41 step:32420 [D loss: 0.298635, acc.: 87.50%] [G loss: 3.279370]\n",
      "epoch:41 step:32421 [D loss: 0.306141, acc.: 87.50%] [G loss: 3.175804]\n",
      "epoch:41 step:32422 [D loss: 0.305583, acc.: 87.50%] [G loss: 3.295474]\n",
      "epoch:41 step:32423 [D loss: 0.263717, acc.: 89.84%] [G loss: 3.482427]\n",
      "epoch:41 step:32424 [D loss: 0.298369, acc.: 89.06%] [G loss: 3.487247]\n",
      "epoch:41 step:32425 [D loss: 0.303982, acc.: 88.28%] [G loss: 3.346568]\n",
      "epoch:41 step:32426 [D loss: 0.281803, acc.: 89.84%] [G loss: 2.951381]\n",
      "epoch:41 step:32427 [D loss: 0.326212, acc.: 86.72%] [G loss: 3.478813]\n",
      "epoch:41 step:32428 [D loss: 0.349225, acc.: 83.59%] [G loss: 3.721243]\n",
      "epoch:41 step:32429 [D loss: 0.416421, acc.: 82.81%] [G loss: 2.975490]\n",
      "epoch:41 step:32430 [D loss: 0.244748, acc.: 89.06%] [G loss: 3.991363]\n",
      "epoch:41 step:32431 [D loss: 0.280757, acc.: 85.94%] [G loss: 4.416631]\n",
      "epoch:41 step:32432 [D loss: 0.186560, acc.: 91.41%] [G loss: 4.652744]\n",
      "epoch:41 step:32433 [D loss: 0.253351, acc.: 89.84%] [G loss: 4.674974]\n",
      "epoch:41 step:32434 [D loss: 0.353230, acc.: 84.38%] [G loss: 3.398625]\n",
      "epoch:41 step:32435 [D loss: 0.233120, acc.: 89.06%] [G loss: 4.748672]\n",
      "epoch:41 step:32436 [D loss: 0.324138, acc.: 85.94%] [G loss: 3.937936]\n",
      "epoch:41 step:32437 [D loss: 0.305965, acc.: 85.16%] [G loss: 3.794679]\n",
      "epoch:41 step:32438 [D loss: 0.453373, acc.: 76.56%] [G loss: 5.088039]\n",
      "epoch:41 step:32439 [D loss: 0.407383, acc.: 80.47%] [G loss: 4.664684]\n",
      "epoch:41 step:32440 [D loss: 0.290111, acc.: 86.72%] [G loss: 4.976916]\n",
      "epoch:41 step:32441 [D loss: 0.200195, acc.: 92.19%] [G loss: 5.925834]\n",
      "epoch:41 step:32442 [D loss: 0.254393, acc.: 88.28%] [G loss: 4.561775]\n",
      "epoch:41 step:32443 [D loss: 0.230945, acc.: 90.62%] [G loss: 3.687203]\n",
      "epoch:41 step:32444 [D loss: 0.287887, acc.: 87.50%] [G loss: 5.008692]\n",
      "epoch:41 step:32445 [D loss: 0.397714, acc.: 83.59%] [G loss: 4.398261]\n",
      "epoch:41 step:32446 [D loss: 0.204058, acc.: 92.97%] [G loss: 3.113860]\n",
      "epoch:41 step:32447 [D loss: 0.320484, acc.: 83.59%] [G loss: 4.616200]\n",
      "epoch:41 step:32448 [D loss: 0.281628, acc.: 88.28%] [G loss: 3.106501]\n",
      "epoch:41 step:32449 [D loss: 0.461877, acc.: 78.12%] [G loss: 3.284305]\n",
      "epoch:41 step:32450 [D loss: 0.361837, acc.: 82.03%] [G loss: 2.775081]\n",
      "epoch:41 step:32451 [D loss: 0.352436, acc.: 83.59%] [G loss: 2.808748]\n",
      "epoch:41 step:32452 [D loss: 0.309680, acc.: 88.28%] [G loss: 2.566304]\n",
      "epoch:41 step:32453 [D loss: 0.434959, acc.: 77.34%] [G loss: 2.690809]\n",
      "epoch:41 step:32454 [D loss: 0.318469, acc.: 85.94%] [G loss: 3.014891]\n",
      "epoch:41 step:32455 [D loss: 0.320689, acc.: 86.72%] [G loss: 4.589085]\n",
      "epoch:41 step:32456 [D loss: 0.407628, acc.: 80.47%] [G loss: 4.693090]\n",
      "epoch:41 step:32457 [D loss: 0.551670, acc.: 76.56%] [G loss: 8.775770]\n",
      "epoch:41 step:32458 [D loss: 1.144863, acc.: 64.84%] [G loss: 3.606229]\n",
      "epoch:41 step:32459 [D loss: 0.365061, acc.: 84.38%] [G loss: 7.171443]\n",
      "epoch:41 step:32460 [D loss: 0.811957, acc.: 67.97%] [G loss: 4.480472]\n",
      "epoch:41 step:32461 [D loss: 0.615703, acc.: 77.34%] [G loss: 4.254619]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:41 step:32462 [D loss: 0.348876, acc.: 82.03%] [G loss: 3.915457]\n",
      "epoch:41 step:32463 [D loss: 0.368346, acc.: 82.03%] [G loss: 2.876780]\n",
      "epoch:41 step:32464 [D loss: 0.332520, acc.: 85.94%] [G loss: 3.343369]\n",
      "epoch:41 step:32465 [D loss: 0.368552, acc.: 78.91%] [G loss: 2.699245]\n",
      "epoch:41 step:32466 [D loss: 0.392807, acc.: 80.47%] [G loss: 2.893291]\n",
      "epoch:41 step:32467 [D loss: 0.325984, acc.: 85.16%] [G loss: 2.852957]\n",
      "epoch:41 step:32468 [D loss: 0.256201, acc.: 89.06%] [G loss: 2.287429]\n",
      "epoch:41 step:32469 [D loss: 0.514409, acc.: 76.56%] [G loss: 2.580731]\n",
      "epoch:41 step:32470 [D loss: 0.355859, acc.: 83.59%] [G loss: 3.385624]\n",
      "epoch:41 step:32471 [D loss: 0.315932, acc.: 86.72%] [G loss: 4.283867]\n",
      "epoch:41 step:32472 [D loss: 0.301005, acc.: 85.16%] [G loss: 6.473546]\n",
      "epoch:41 step:32473 [D loss: 0.326138, acc.: 89.06%] [G loss: 3.962095]\n",
      "epoch:41 step:32474 [D loss: 0.202862, acc.: 92.97%] [G loss: 4.175164]\n",
      "epoch:41 step:32475 [D loss: 0.366065, acc.: 83.59%] [G loss: 4.499277]\n",
      "epoch:41 step:32476 [D loss: 0.212300, acc.: 91.41%] [G loss: 4.407807]\n",
      "epoch:41 step:32477 [D loss: 0.333901, acc.: 82.03%] [G loss: 3.929587]\n",
      "epoch:41 step:32478 [D loss: 0.300544, acc.: 88.28%] [G loss: 4.053141]\n",
      "epoch:41 step:32479 [D loss: 0.270490, acc.: 86.72%] [G loss: 3.119066]\n",
      "epoch:41 step:32480 [D loss: 0.323944, acc.: 84.38%] [G loss: 3.728010]\n",
      "epoch:41 step:32481 [D loss: 0.288211, acc.: 89.84%] [G loss: 4.225036]\n",
      "epoch:41 step:32482 [D loss: 0.275359, acc.: 90.62%] [G loss: 3.036856]\n",
      "epoch:41 step:32483 [D loss: 0.245455, acc.: 89.84%] [G loss: 3.845399]\n",
      "epoch:41 step:32484 [D loss: 0.295549, acc.: 85.94%] [G loss: 2.914689]\n",
      "epoch:41 step:32485 [D loss: 0.278522, acc.: 88.28%] [G loss: 2.825054]\n",
      "epoch:41 step:32486 [D loss: 0.336055, acc.: 86.72%] [G loss: 2.884374]\n",
      "epoch:41 step:32487 [D loss: 0.312164, acc.: 85.16%] [G loss: 3.988332]\n",
      "epoch:41 step:32488 [D loss: 0.353214, acc.: 82.81%] [G loss: 3.923278]\n",
      "epoch:41 step:32489 [D loss: 0.337335, acc.: 81.25%] [G loss: 3.292611]\n",
      "epoch:41 step:32490 [D loss: 0.363438, acc.: 83.59%] [G loss: 3.572353]\n",
      "epoch:41 step:32491 [D loss: 0.325213, acc.: 84.38%] [G loss: 3.414006]\n",
      "epoch:41 step:32492 [D loss: 0.286718, acc.: 88.28%] [G loss: 2.856406]\n",
      "epoch:41 step:32493 [D loss: 0.438445, acc.: 82.81%] [G loss: 2.775982]\n",
      "epoch:41 step:32494 [D loss: 0.320464, acc.: 85.16%] [G loss: 2.671165]\n",
      "epoch:41 step:32495 [D loss: 0.475955, acc.: 81.25%] [G loss: 2.292786]\n",
      "epoch:41 step:32496 [D loss: 0.394215, acc.: 80.47%] [G loss: 3.065524]\n",
      "epoch:41 step:32497 [D loss: 0.406263, acc.: 80.47%] [G loss: 3.822640]\n",
      "epoch:41 step:32498 [D loss: 0.559296, acc.: 78.12%] [G loss: 2.703568]\n",
      "epoch:41 step:32499 [D loss: 0.282418, acc.: 85.94%] [G loss: 2.846583]\n",
      "epoch:41 step:32500 [D loss: 0.392294, acc.: 84.38%] [G loss: 3.408920]\n",
      "epoch:41 step:32501 [D loss: 0.300645, acc.: 84.38%] [G loss: 2.906179]\n",
      "epoch:41 step:32502 [D loss: 0.515036, acc.: 79.69%] [G loss: 5.174112]\n",
      "epoch:41 step:32503 [D loss: 0.396095, acc.: 78.91%] [G loss: 2.333574]\n",
      "epoch:41 step:32504 [D loss: 0.404671, acc.: 79.69%] [G loss: 4.307444]\n",
      "epoch:41 step:32505 [D loss: 0.333250, acc.: 85.16%] [G loss: 3.450037]\n",
      "epoch:41 step:32506 [D loss: 0.294486, acc.: 85.16%] [G loss: 3.078296]\n",
      "epoch:41 step:32507 [D loss: 0.303804, acc.: 85.94%] [G loss: 3.926451]\n",
      "epoch:41 step:32508 [D loss: 0.331689, acc.: 83.59%] [G loss: 2.379908]\n",
      "epoch:41 step:32509 [D loss: 0.374973, acc.: 84.38%] [G loss: 2.625533]\n",
      "epoch:41 step:32510 [D loss: 0.527436, acc.: 71.88%] [G loss: 3.491256]\n",
      "epoch:41 step:32511 [D loss: 0.286148, acc.: 84.38%] [G loss: 3.580371]\n",
      "epoch:41 step:32512 [D loss: 0.375726, acc.: 79.69%] [G loss: 3.873917]\n",
      "epoch:41 step:32513 [D loss: 0.334920, acc.: 88.28%] [G loss: 3.612670]\n",
      "epoch:41 step:32514 [D loss: 0.259498, acc.: 86.72%] [G loss: 3.973157]\n",
      "epoch:41 step:32515 [D loss: 0.270529, acc.: 90.62%] [G loss: 4.058537]\n",
      "epoch:41 step:32516 [D loss: 0.313914, acc.: 85.94%] [G loss: 3.056242]\n",
      "epoch:41 step:32517 [D loss: 0.311405, acc.: 86.72%] [G loss: 3.670536]\n",
      "epoch:41 step:32518 [D loss: 0.308577, acc.: 87.50%] [G loss: 3.902628]\n",
      "epoch:41 step:32519 [D loss: 0.224325, acc.: 91.41%] [G loss: 4.586682]\n",
      "epoch:41 step:32520 [D loss: 0.277553, acc.: 88.28%] [G loss: 3.526778]\n",
      "epoch:41 step:32521 [D loss: 0.241935, acc.: 89.84%] [G loss: 3.796974]\n",
      "epoch:41 step:32522 [D loss: 0.372989, acc.: 85.16%] [G loss: 4.013581]\n",
      "epoch:41 step:32523 [D loss: 0.299270, acc.: 89.06%] [G loss: 3.260701]\n",
      "epoch:41 step:32524 [D loss: 0.337991, acc.: 85.16%] [G loss: 2.697255]\n",
      "epoch:41 step:32525 [D loss: 0.241056, acc.: 90.62%] [G loss: 3.159870]\n",
      "epoch:41 step:32526 [D loss: 0.350496, acc.: 83.59%] [G loss: 2.728606]\n",
      "epoch:41 step:32527 [D loss: 0.240140, acc.: 92.19%] [G loss: 2.741583]\n",
      "epoch:41 step:32528 [D loss: 0.297053, acc.: 85.16%] [G loss: 2.442771]\n",
      "epoch:41 step:32529 [D loss: 0.357680, acc.: 82.81%] [G loss: 2.809634]\n",
      "epoch:41 step:32530 [D loss: 0.302772, acc.: 86.72%] [G loss: 3.191821]\n",
      "epoch:41 step:32531 [D loss: 0.414538, acc.: 81.25%] [G loss: 3.234396]\n",
      "epoch:41 step:32532 [D loss: 0.397887, acc.: 81.25%] [G loss: 5.591391]\n",
      "epoch:41 step:32533 [D loss: 0.345060, acc.: 85.16%] [G loss: 4.385893]\n",
      "epoch:41 step:32534 [D loss: 0.319685, acc.: 88.28%] [G loss: 3.418612]\n",
      "epoch:41 step:32535 [D loss: 0.375408, acc.: 82.03%] [G loss: 4.236142]\n",
      "epoch:41 step:32536 [D loss: 0.498950, acc.: 76.56%] [G loss: 3.018163]\n",
      "epoch:41 step:32537 [D loss: 0.370341, acc.: 84.38%] [G loss: 3.676167]\n",
      "epoch:41 step:32538 [D loss: 0.316778, acc.: 83.59%] [G loss: 3.375200]\n",
      "epoch:41 step:32539 [D loss: 0.246712, acc.: 92.97%] [G loss: 3.225547]\n",
      "epoch:41 step:32540 [D loss: 0.358859, acc.: 84.38%] [G loss: 3.373808]\n",
      "epoch:41 step:32541 [D loss: 0.335581, acc.: 84.38%] [G loss: 3.885148]\n",
      "epoch:41 step:32542 [D loss: 0.336224, acc.: 82.81%] [G loss: 3.360293]\n",
      "epoch:41 step:32543 [D loss: 0.327424, acc.: 82.81%] [G loss: 3.443109]\n",
      "epoch:41 step:32544 [D loss: 0.403595, acc.: 82.81%] [G loss: 3.340757]\n",
      "epoch:41 step:32545 [D loss: 0.297748, acc.: 88.28%] [G loss: 3.764758]\n",
      "epoch:41 step:32546 [D loss: 0.320851, acc.: 86.72%] [G loss: 2.610585]\n",
      "epoch:41 step:32547 [D loss: 0.383690, acc.: 81.25%] [G loss: 2.942097]\n",
      "epoch:41 step:32548 [D loss: 0.363273, acc.: 83.59%] [G loss: 3.155357]\n",
      "epoch:41 step:32549 [D loss: 0.249054, acc.: 90.62%] [G loss: 3.949582]\n",
      "epoch:41 step:32550 [D loss: 0.312803, acc.: 88.28%] [G loss: 2.886883]\n",
      "epoch:41 step:32551 [D loss: 0.324939, acc.: 85.16%] [G loss: 3.698246]\n",
      "epoch:41 step:32552 [D loss: 0.276013, acc.: 89.84%] [G loss: 3.444358]\n",
      "epoch:41 step:32553 [D loss: 0.310490, acc.: 86.72%] [G loss: 4.406984]\n",
      "epoch:41 step:32554 [D loss: 0.271936, acc.: 84.38%] [G loss: 2.859431]\n",
      "epoch:41 step:32555 [D loss: 0.257026, acc.: 87.50%] [G loss: 7.217258]\n",
      "epoch:41 step:32556 [D loss: 0.312561, acc.: 88.28%] [G loss: 3.637096]\n",
      "epoch:41 step:32557 [D loss: 0.336035, acc.: 83.59%] [G loss: 3.475673]\n",
      "epoch:41 step:32558 [D loss: 0.419841, acc.: 79.69%] [G loss: 3.451519]\n",
      "epoch:41 step:32559 [D loss: 0.281464, acc.: 87.50%] [G loss: 3.648957]\n",
      "epoch:41 step:32560 [D loss: 0.256537, acc.: 89.84%] [G loss: 3.253164]\n",
      "epoch:41 step:32561 [D loss: 0.386518, acc.: 83.59%] [G loss: 3.165999]\n",
      "epoch:41 step:32562 [D loss: 0.391678, acc.: 78.91%] [G loss: 3.100341]\n",
      "epoch:41 step:32563 [D loss: 0.477572, acc.: 79.69%] [G loss: 2.620214]\n",
      "epoch:41 step:32564 [D loss: 0.317723, acc.: 89.06%] [G loss: 3.057956]\n",
      "epoch:41 step:32565 [D loss: 0.244171, acc.: 89.84%] [G loss: 3.074869]\n",
      "epoch:41 step:32566 [D loss: 0.297403, acc.: 84.38%] [G loss: 2.436502]\n",
      "epoch:41 step:32567 [D loss: 0.267887, acc.: 85.94%] [G loss: 3.047302]\n",
      "epoch:41 step:32568 [D loss: 0.312460, acc.: 85.94%] [G loss: 2.718822]\n",
      "epoch:41 step:32569 [D loss: 0.247564, acc.: 90.62%] [G loss: 2.904726]\n",
      "epoch:41 step:32570 [D loss: 0.376764, acc.: 82.81%] [G loss: 3.312100]\n",
      "epoch:41 step:32571 [D loss: 0.356623, acc.: 84.38%] [G loss: 2.369565]\n",
      "epoch:41 step:32572 [D loss: 0.198827, acc.: 92.19%] [G loss: 3.531597]\n",
      "epoch:41 step:32573 [D loss: 0.302924, acc.: 92.19%] [G loss: 2.958125]\n",
      "epoch:41 step:32574 [D loss: 0.273490, acc.: 87.50%] [G loss: 2.870870]\n",
      "epoch:41 step:32575 [D loss: 0.338615, acc.: 86.72%] [G loss: 2.942044]\n",
      "epoch:41 step:32576 [D loss: 0.357176, acc.: 85.94%] [G loss: 3.657148]\n",
      "epoch:41 step:32577 [D loss: 0.314843, acc.: 86.72%] [G loss: 3.049293]\n",
      "epoch:41 step:32578 [D loss: 0.331338, acc.: 89.06%] [G loss: 2.794063]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:41 step:32579 [D loss: 0.368648, acc.: 84.38%] [G loss: 4.903985]\n",
      "epoch:41 step:32580 [D loss: 0.195244, acc.: 90.62%] [G loss: 3.376167]\n",
      "epoch:41 step:32581 [D loss: 0.260715, acc.: 89.84%] [G loss: 3.944678]\n",
      "epoch:41 step:32582 [D loss: 0.236396, acc.: 88.28%] [G loss: 2.662165]\n",
      "epoch:41 step:32583 [D loss: 0.310258, acc.: 86.72%] [G loss: 3.135825]\n",
      "epoch:41 step:32584 [D loss: 0.255797, acc.: 88.28%] [G loss: 4.111559]\n",
      "epoch:41 step:32585 [D loss: 0.382293, acc.: 81.25%] [G loss: 3.367930]\n",
      "epoch:41 step:32586 [D loss: 0.349581, acc.: 85.94%] [G loss: 4.090411]\n",
      "epoch:41 step:32587 [D loss: 0.281075, acc.: 89.06%] [G loss: 3.714389]\n",
      "epoch:41 step:32588 [D loss: 0.249979, acc.: 93.75%] [G loss: 4.777191]\n",
      "epoch:41 step:32589 [D loss: 0.313147, acc.: 86.72%] [G loss: 4.831846]\n",
      "epoch:41 step:32590 [D loss: 0.279810, acc.: 86.72%] [G loss: 5.102174]\n",
      "epoch:41 step:32591 [D loss: 0.248439, acc.: 87.50%] [G loss: 5.147570]\n",
      "epoch:41 step:32592 [D loss: 0.193469, acc.: 93.75%] [G loss: 4.650395]\n",
      "epoch:41 step:32593 [D loss: 0.277789, acc.: 89.84%] [G loss: 7.089284]\n",
      "epoch:41 step:32594 [D loss: 0.238839, acc.: 91.41%] [G loss: 5.275141]\n",
      "epoch:41 step:32595 [D loss: 0.190709, acc.: 92.97%] [G loss: 4.620246]\n",
      "epoch:41 step:32596 [D loss: 0.456931, acc.: 79.69%] [G loss: 3.612635]\n",
      "epoch:41 step:32597 [D loss: 0.329365, acc.: 86.72%] [G loss: 4.917752]\n",
      "epoch:41 step:32598 [D loss: 0.270862, acc.: 89.06%] [G loss: 3.371157]\n",
      "epoch:41 step:32599 [D loss: 0.310474, acc.: 86.72%] [G loss: 3.776962]\n",
      "epoch:41 step:32600 [D loss: 0.296724, acc.: 84.38%] [G loss: 3.581566]\n",
      "##############\n",
      "[0.85001932 0.84983846 0.82680277 0.80080649 0.76006308 0.84525677\n",
      " 0.89159676 0.85581582 0.81746493 0.82230061]\n",
      "##########\n",
      "epoch:41 step:32601 [D loss: 0.249829, acc.: 89.06%] [G loss: 2.893856]\n",
      "epoch:41 step:32602 [D loss: 0.307651, acc.: 89.84%] [G loss: 3.761954]\n",
      "epoch:41 step:32603 [D loss: 0.315795, acc.: 84.38%] [G loss: 3.573527]\n",
      "epoch:41 step:32604 [D loss: 0.222610, acc.: 91.41%] [G loss: 3.307775]\n",
      "epoch:41 step:32605 [D loss: 0.395709, acc.: 78.91%] [G loss: 4.059635]\n",
      "epoch:41 step:32606 [D loss: 0.292591, acc.: 92.97%] [G loss: 3.314503]\n",
      "epoch:41 step:32607 [D loss: 0.241413, acc.: 91.41%] [G loss: 3.750241]\n",
      "epoch:41 step:32608 [D loss: 0.313846, acc.: 82.81%] [G loss: 3.106649]\n",
      "epoch:41 step:32609 [D loss: 0.287967, acc.: 85.94%] [G loss: 3.206536]\n",
      "epoch:41 step:32610 [D loss: 0.283769, acc.: 85.94%] [G loss: 3.096686]\n",
      "epoch:41 step:32611 [D loss: 0.274746, acc.: 87.50%] [G loss: 2.703160]\n",
      "epoch:41 step:32612 [D loss: 0.353485, acc.: 81.25%] [G loss: 3.312289]\n",
      "epoch:41 step:32613 [D loss: 0.309738, acc.: 89.06%] [G loss: 3.167496]\n",
      "epoch:41 step:32614 [D loss: 0.298253, acc.: 88.28%] [G loss: 2.992082]\n",
      "epoch:41 step:32615 [D loss: 0.254139, acc.: 87.50%] [G loss: 2.866742]\n",
      "epoch:41 step:32616 [D loss: 0.310870, acc.: 85.94%] [G loss: 3.241755]\n",
      "epoch:41 step:32617 [D loss: 0.370178, acc.: 87.50%] [G loss: 2.662317]\n",
      "epoch:41 step:32618 [D loss: 0.364323, acc.: 81.25%] [G loss: 2.792554]\n",
      "epoch:41 step:32619 [D loss: 0.321417, acc.: 85.16%] [G loss: 2.779344]\n",
      "epoch:41 step:32620 [D loss: 0.302720, acc.: 88.28%] [G loss: 3.117729]\n",
      "epoch:41 step:32621 [D loss: 0.435944, acc.: 79.69%] [G loss: 2.785264]\n",
      "epoch:41 step:32622 [D loss: 0.327060, acc.: 85.94%] [G loss: 2.788877]\n",
      "epoch:41 step:32623 [D loss: 0.277796, acc.: 88.28%] [G loss: 3.509873]\n",
      "epoch:41 step:32624 [D loss: 0.365535, acc.: 83.59%] [G loss: 3.117644]\n",
      "epoch:41 step:32625 [D loss: 0.300429, acc.: 90.62%] [G loss: 2.887357]\n",
      "epoch:41 step:32626 [D loss: 0.383004, acc.: 82.81%] [G loss: 3.646584]\n",
      "epoch:41 step:32627 [D loss: 0.331025, acc.: 81.25%] [G loss: 5.123620]\n",
      "epoch:41 step:32628 [D loss: 0.332719, acc.: 85.16%] [G loss: 2.918493]\n",
      "epoch:41 step:32629 [D loss: 0.353259, acc.: 83.59%] [G loss: 3.849825]\n",
      "epoch:41 step:32630 [D loss: 0.390168, acc.: 83.59%] [G loss: 4.036164]\n",
      "epoch:41 step:32631 [D loss: 0.558275, acc.: 77.34%] [G loss: 4.300821]\n",
      "epoch:41 step:32632 [D loss: 0.340566, acc.: 85.94%] [G loss: 3.675188]\n",
      "epoch:41 step:32633 [D loss: 0.303906, acc.: 86.72%] [G loss: 3.441896]\n",
      "epoch:41 step:32634 [D loss: 0.265123, acc.: 85.16%] [G loss: 2.861019]\n",
      "epoch:41 step:32635 [D loss: 0.213228, acc.: 90.62%] [G loss: 3.865621]\n",
      "epoch:41 step:32636 [D loss: 0.237192, acc.: 90.62%] [G loss: 3.399486]\n",
      "epoch:41 step:32637 [D loss: 0.284942, acc.: 88.28%] [G loss: 2.781252]\n",
      "epoch:41 step:32638 [D loss: 0.241227, acc.: 91.41%] [G loss: 2.775884]\n",
      "epoch:41 step:32639 [D loss: 0.279397, acc.: 89.06%] [G loss: 3.247859]\n",
      "epoch:41 step:32640 [D loss: 0.331161, acc.: 87.50%] [G loss: 2.963673]\n",
      "epoch:41 step:32641 [D loss: 0.344284, acc.: 84.38%] [G loss: 2.774343]\n",
      "epoch:41 step:32642 [D loss: 0.227146, acc.: 92.19%] [G loss: 3.632281]\n",
      "epoch:41 step:32643 [D loss: 0.292747, acc.: 89.06%] [G loss: 3.613840]\n",
      "epoch:41 step:32644 [D loss: 0.371874, acc.: 81.25%] [G loss: 4.606834]\n",
      "epoch:41 step:32645 [D loss: 0.375751, acc.: 84.38%] [G loss: 4.696922]\n",
      "epoch:41 step:32646 [D loss: 0.543958, acc.: 75.78%] [G loss: 4.460463]\n",
      "epoch:41 step:32647 [D loss: 0.528577, acc.: 77.34%] [G loss: 3.831032]\n",
      "epoch:41 step:32648 [D loss: 0.373974, acc.: 82.81%] [G loss: 2.558889]\n",
      "epoch:41 step:32649 [D loss: 0.290724, acc.: 90.62%] [G loss: 3.196053]\n",
      "epoch:41 step:32650 [D loss: 0.313265, acc.: 86.72%] [G loss: 2.947229]\n",
      "epoch:41 step:32651 [D loss: 0.301659, acc.: 85.94%] [G loss: 3.931766]\n",
      "epoch:41 step:32652 [D loss: 0.416036, acc.: 77.34%] [G loss: 2.926832]\n",
      "epoch:41 step:32653 [D loss: 0.254982, acc.: 90.62%] [G loss: 3.451109]\n",
      "epoch:41 step:32654 [D loss: 0.384413, acc.: 85.16%] [G loss: 2.704183]\n",
      "epoch:41 step:32655 [D loss: 0.369241, acc.: 83.59%] [G loss: 3.858729]\n",
      "epoch:41 step:32656 [D loss: 0.305951, acc.: 85.94%] [G loss: 3.554443]\n",
      "epoch:41 step:32657 [D loss: 0.352063, acc.: 85.94%] [G loss: 3.699877]\n",
      "epoch:41 step:32658 [D loss: 0.198612, acc.: 92.97%] [G loss: 4.124669]\n",
      "epoch:41 step:32659 [D loss: 0.209386, acc.: 89.84%] [G loss: 3.474484]\n",
      "epoch:41 step:32660 [D loss: 0.252275, acc.: 89.06%] [G loss: 3.009361]\n",
      "epoch:41 step:32661 [D loss: 0.221572, acc.: 94.53%] [G loss: 3.497800]\n",
      "epoch:41 step:32662 [D loss: 0.322601, acc.: 86.72%] [G loss: 3.313612]\n",
      "epoch:41 step:32663 [D loss: 0.287058, acc.: 87.50%] [G loss: 2.932691]\n",
      "epoch:41 step:32664 [D loss: 0.369564, acc.: 82.81%] [G loss: 2.728405]\n",
      "epoch:41 step:32665 [D loss: 0.235140, acc.: 89.84%] [G loss: 3.379887]\n",
      "epoch:41 step:32666 [D loss: 0.307531, acc.: 85.94%] [G loss: 2.581579]\n",
      "epoch:41 step:32667 [D loss: 0.359983, acc.: 85.94%] [G loss: 3.394409]\n",
      "epoch:41 step:32668 [D loss: 0.199668, acc.: 93.75%] [G loss: 2.896643]\n",
      "epoch:41 step:32669 [D loss: 0.301575, acc.: 89.06%] [G loss: 3.581688]\n",
      "epoch:41 step:32670 [D loss: 0.347020, acc.: 80.47%] [G loss: 3.743564]\n",
      "epoch:41 step:32671 [D loss: 0.322375, acc.: 82.81%] [G loss: 5.012433]\n",
      "epoch:41 step:32672 [D loss: 0.269023, acc.: 89.84%] [G loss: 3.417300]\n",
      "epoch:41 step:32673 [D loss: 0.340305, acc.: 84.38%] [G loss: 3.346817]\n",
      "epoch:41 step:32674 [D loss: 0.279426, acc.: 88.28%] [G loss: 3.431127]\n",
      "epoch:41 step:32675 [D loss: 0.381977, acc.: 83.59%] [G loss: 3.548928]\n",
      "epoch:41 step:32676 [D loss: 0.264417, acc.: 89.06%] [G loss: 3.000098]\n",
      "epoch:41 step:32677 [D loss: 0.289137, acc.: 85.16%] [G loss: 3.634472]\n",
      "epoch:41 step:32678 [D loss: 0.412325, acc.: 78.91%] [G loss: 3.212259]\n",
      "epoch:41 step:32679 [D loss: 0.239435, acc.: 89.06%] [G loss: 3.602972]\n",
      "epoch:41 step:32680 [D loss: 0.384171, acc.: 83.59%] [G loss: 2.691222]\n",
      "epoch:41 step:32681 [D loss: 0.372007, acc.: 85.16%] [G loss: 2.880626]\n",
      "epoch:41 step:32682 [D loss: 0.373273, acc.: 80.47%] [G loss: 3.086979]\n",
      "epoch:41 step:32683 [D loss: 0.313372, acc.: 87.50%] [G loss: 2.988670]\n",
      "epoch:41 step:32684 [D loss: 0.240679, acc.: 87.50%] [G loss: 3.682735]\n",
      "epoch:41 step:32685 [D loss: 0.396534, acc.: 79.69%] [G loss: 3.288579]\n",
      "epoch:41 step:32686 [D loss: 0.358623, acc.: 85.16%] [G loss: 3.735705]\n",
      "epoch:41 step:32687 [D loss: 0.282775, acc.: 87.50%] [G loss: 3.404359]\n",
      "epoch:41 step:32688 [D loss: 0.242473, acc.: 91.41%] [G loss: 3.280806]\n",
      "epoch:41 step:32689 [D loss: 0.257814, acc.: 92.19%] [G loss: 4.336600]\n",
      "epoch:41 step:32690 [D loss: 0.275710, acc.: 89.84%] [G loss: 4.041728]\n",
      "epoch:41 step:32691 [D loss: 0.297454, acc.: 85.16%] [G loss: 3.208268]\n",
      "epoch:41 step:32692 [D loss: 0.347568, acc.: 81.25%] [G loss: 3.265964]\n",
      "epoch:41 step:32693 [D loss: 0.462953, acc.: 75.00%] [G loss: 4.000881]\n",
      "epoch:41 step:32694 [D loss: 0.262564, acc.: 89.06%] [G loss: 2.738534]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:41 step:32695 [D loss: 0.356421, acc.: 82.03%] [G loss: 3.115965]\n",
      "epoch:41 step:32696 [D loss: 0.347549, acc.: 85.16%] [G loss: 2.995741]\n",
      "epoch:41 step:32697 [D loss: 0.422593, acc.: 78.91%] [G loss: 4.630160]\n",
      "epoch:41 step:32698 [D loss: 0.486699, acc.: 78.12%] [G loss: 3.854744]\n",
      "epoch:41 step:32699 [D loss: 0.358876, acc.: 82.81%] [G loss: 3.263666]\n",
      "epoch:41 step:32700 [D loss: 0.219763, acc.: 92.19%] [G loss: 3.251360]\n",
      "epoch:41 step:32701 [D loss: 0.347525, acc.: 85.16%] [G loss: 4.115641]\n",
      "epoch:41 step:32702 [D loss: 0.301679, acc.: 86.72%] [G loss: 2.751242]\n",
      "epoch:41 step:32703 [D loss: 0.320685, acc.: 88.28%] [G loss: 4.276986]\n",
      "epoch:41 step:32704 [D loss: 0.274910, acc.: 88.28%] [G loss: 3.650342]\n",
      "epoch:41 step:32705 [D loss: 0.277461, acc.: 85.94%] [G loss: 3.479117]\n",
      "epoch:41 step:32706 [D loss: 0.295098, acc.: 88.28%] [G loss: 3.278294]\n",
      "epoch:41 step:32707 [D loss: 0.273258, acc.: 89.06%] [G loss: 3.425289]\n",
      "epoch:41 step:32708 [D loss: 0.296467, acc.: 86.72%] [G loss: 3.392954]\n",
      "epoch:41 step:32709 [D loss: 0.336636, acc.: 82.03%] [G loss: 3.537542]\n",
      "epoch:41 step:32710 [D loss: 0.309238, acc.: 86.72%] [G loss: 2.207227]\n",
      "epoch:41 step:32711 [D loss: 0.300778, acc.: 86.72%] [G loss: 4.635939]\n",
      "epoch:41 step:32712 [D loss: 0.255631, acc.: 86.72%] [G loss: 3.537451]\n",
      "epoch:41 step:32713 [D loss: 0.381094, acc.: 82.81%] [G loss: 4.910954]\n",
      "epoch:41 step:32714 [D loss: 0.325169, acc.: 86.72%] [G loss: 3.216286]\n",
      "epoch:41 step:32715 [D loss: 0.239093, acc.: 89.06%] [G loss: 3.451756]\n",
      "epoch:41 step:32716 [D loss: 0.197089, acc.: 91.41%] [G loss: 4.073955]\n",
      "epoch:41 step:32717 [D loss: 0.273163, acc.: 85.16%] [G loss: 3.020236]\n",
      "epoch:41 step:32718 [D loss: 0.281593, acc.: 89.06%] [G loss: 3.573309]\n",
      "epoch:41 step:32719 [D loss: 0.314388, acc.: 85.94%] [G loss: 2.578453]\n",
      "epoch:41 step:32720 [D loss: 0.370518, acc.: 79.69%] [G loss: 3.280889]\n",
      "epoch:41 step:32721 [D loss: 0.316466, acc.: 84.38%] [G loss: 3.322320]\n",
      "epoch:41 step:32722 [D loss: 0.344308, acc.: 85.16%] [G loss: 3.543724]\n",
      "epoch:41 step:32723 [D loss: 0.253801, acc.: 89.84%] [G loss: 4.876500]\n",
      "epoch:41 step:32724 [D loss: 0.235408, acc.: 89.84%] [G loss: 5.715226]\n",
      "epoch:41 step:32725 [D loss: 0.286425, acc.: 88.28%] [G loss: 3.562273]\n",
      "epoch:41 step:32726 [D loss: 0.321353, acc.: 85.16%] [G loss: 4.645892]\n",
      "epoch:41 step:32727 [D loss: 0.335488, acc.: 85.16%] [G loss: 4.425987]\n",
      "epoch:41 step:32728 [D loss: 0.381449, acc.: 81.25%] [G loss: 4.516917]\n",
      "epoch:41 step:32729 [D loss: 0.224065, acc.: 91.41%] [G loss: 4.345312]\n",
      "epoch:41 step:32730 [D loss: 0.341537, acc.: 83.59%] [G loss: 4.218712]\n",
      "epoch:41 step:32731 [D loss: 0.293960, acc.: 85.94%] [G loss: 3.690549]\n",
      "epoch:41 step:32732 [D loss: 0.343000, acc.: 86.72%] [G loss: 2.883410]\n",
      "epoch:41 step:32733 [D loss: 0.344998, acc.: 84.38%] [G loss: 3.254727]\n",
      "epoch:41 step:32734 [D loss: 0.361062, acc.: 80.47%] [G loss: 3.204680]\n",
      "epoch:41 step:32735 [D loss: 0.320621, acc.: 85.16%] [G loss: 3.789407]\n",
      "epoch:41 step:32736 [D loss: 0.320748, acc.: 82.03%] [G loss: 4.075644]\n",
      "epoch:41 step:32737 [D loss: 0.191674, acc.: 93.75%] [G loss: 3.983652]\n",
      "epoch:41 step:32738 [D loss: 0.304294, acc.: 86.72%] [G loss: 5.801932]\n",
      "epoch:41 step:32739 [D loss: 0.284239, acc.: 89.84%] [G loss: 4.181625]\n",
      "epoch:41 step:32740 [D loss: 0.267365, acc.: 90.62%] [G loss: 3.675439]\n",
      "epoch:41 step:32741 [D loss: 0.258427, acc.: 89.84%] [G loss: 3.925876]\n",
      "epoch:41 step:32742 [D loss: 0.308953, acc.: 86.72%] [G loss: 3.912187]\n",
      "epoch:41 step:32743 [D loss: 0.273533, acc.: 89.06%] [G loss: 3.851207]\n",
      "epoch:41 step:32744 [D loss: 0.225303, acc.: 92.97%] [G loss: 4.097232]\n",
      "epoch:41 step:32745 [D loss: 0.166371, acc.: 96.88%] [G loss: 3.665007]\n",
      "epoch:41 step:32746 [D loss: 0.291306, acc.: 85.16%] [G loss: 3.149877]\n",
      "epoch:41 step:32747 [D loss: 0.371808, acc.: 84.38%] [G loss: 3.785634]\n",
      "epoch:41 step:32748 [D loss: 0.332277, acc.: 85.94%] [G loss: 2.892198]\n",
      "epoch:41 step:32749 [D loss: 0.391713, acc.: 82.81%] [G loss: 4.310276]\n",
      "epoch:41 step:32750 [D loss: 0.321544, acc.: 89.84%] [G loss: 2.895033]\n",
      "epoch:41 step:32751 [D loss: 0.309491, acc.: 83.59%] [G loss: 3.799754]\n",
      "epoch:41 step:32752 [D loss: 0.320327, acc.: 88.28%] [G loss: 3.535852]\n",
      "epoch:41 step:32753 [D loss: 0.370763, acc.: 84.38%] [G loss: 2.701503]\n",
      "epoch:41 step:32754 [D loss: 0.216118, acc.: 91.41%] [G loss: 3.963153]\n",
      "epoch:41 step:32755 [D loss: 0.293028, acc.: 85.94%] [G loss: 3.577061]\n",
      "epoch:41 step:32756 [D loss: 0.319858, acc.: 85.94%] [G loss: 3.993165]\n",
      "epoch:41 step:32757 [D loss: 0.301415, acc.: 87.50%] [G loss: 3.993020]\n",
      "epoch:41 step:32758 [D loss: 0.363911, acc.: 83.59%] [G loss: 4.189338]\n",
      "epoch:41 step:32759 [D loss: 0.374653, acc.: 85.94%] [G loss: 3.539837]\n",
      "epoch:41 step:32760 [D loss: 0.288826, acc.: 88.28%] [G loss: 3.985999]\n",
      "epoch:41 step:32761 [D loss: 0.311699, acc.: 85.16%] [G loss: 4.352233]\n",
      "epoch:41 step:32762 [D loss: 0.265559, acc.: 90.62%] [G loss: 3.984916]\n",
      "epoch:41 step:32763 [D loss: 0.277929, acc.: 86.72%] [G loss: 3.682009]\n",
      "epoch:41 step:32764 [D loss: 0.240323, acc.: 91.41%] [G loss: 4.178614]\n",
      "epoch:41 step:32765 [D loss: 0.283665, acc.: 85.94%] [G loss: 3.289448]\n",
      "epoch:41 step:32766 [D loss: 0.204263, acc.: 89.84%] [G loss: 3.571290]\n",
      "epoch:41 step:32767 [D loss: 0.249277, acc.: 88.28%] [G loss: 4.196357]\n",
      "epoch:41 step:32768 [D loss: 0.252086, acc.: 86.72%] [G loss: 4.537348]\n",
      "epoch:41 step:32769 [D loss: 0.255516, acc.: 90.62%] [G loss: 3.265737]\n",
      "epoch:41 step:32770 [D loss: 0.246921, acc.: 87.50%] [G loss: 5.144923]\n",
      "epoch:41 step:32771 [D loss: 0.345849, acc.: 88.28%] [G loss: 3.881338]\n",
      "epoch:41 step:32772 [D loss: 0.328929, acc.: 85.16%] [G loss: 3.131375]\n",
      "epoch:41 step:32773 [D loss: 0.505209, acc.: 78.12%] [G loss: 3.254355]\n",
      "epoch:41 step:32774 [D loss: 0.375944, acc.: 80.47%] [G loss: 3.372411]\n",
      "epoch:41 step:32775 [D loss: 0.418584, acc.: 81.25%] [G loss: 3.036160]\n",
      "epoch:41 step:32776 [D loss: 0.255546, acc.: 89.06%] [G loss: 2.843899]\n",
      "epoch:41 step:32777 [D loss: 0.315937, acc.: 85.16%] [G loss: 3.043663]\n",
      "epoch:41 step:32778 [D loss: 0.246071, acc.: 89.84%] [G loss: 3.639397]\n",
      "epoch:41 step:32779 [D loss: 0.290356, acc.: 85.94%] [G loss: 4.064996]\n",
      "epoch:41 step:32780 [D loss: 0.216928, acc.: 92.97%] [G loss: 3.700027]\n",
      "epoch:41 step:32781 [D loss: 0.333900, acc.: 83.59%] [G loss: 5.226306]\n",
      "epoch:41 step:32782 [D loss: 0.313990, acc.: 83.59%] [G loss: 4.869203]\n",
      "epoch:41 step:32783 [D loss: 0.421549, acc.: 79.69%] [G loss: 4.246907]\n",
      "epoch:41 step:32784 [D loss: 0.447928, acc.: 76.56%] [G loss: 3.783159]\n",
      "epoch:41 step:32785 [D loss: 0.302384, acc.: 88.28%] [G loss: 3.155845]\n",
      "epoch:41 step:32786 [D loss: 0.318858, acc.: 83.59%] [G loss: 4.990950]\n",
      "epoch:41 step:32787 [D loss: 0.348501, acc.: 85.16%] [G loss: 2.410503]\n",
      "epoch:41 step:32788 [D loss: 0.311852, acc.: 86.72%] [G loss: 2.951066]\n",
      "epoch:41 step:32789 [D loss: 0.460346, acc.: 82.81%] [G loss: 2.963647]\n",
      "epoch:41 step:32790 [D loss: 0.242893, acc.: 88.28%] [G loss: 3.503970]\n",
      "epoch:41 step:32791 [D loss: 0.268289, acc.: 86.72%] [G loss: 3.057023]\n",
      "epoch:41 step:32792 [D loss: 0.369995, acc.: 86.72%] [G loss: 2.846412]\n",
      "epoch:41 step:32793 [D loss: 0.370754, acc.: 86.72%] [G loss: 3.586502]\n",
      "epoch:41 step:32794 [D loss: 0.258942, acc.: 89.06%] [G loss: 3.457700]\n",
      "epoch:41 step:32795 [D loss: 0.232395, acc.: 92.19%] [G loss: 2.606257]\n",
      "epoch:41 step:32796 [D loss: 0.342190, acc.: 84.38%] [G loss: 2.776056]\n",
      "epoch:41 step:32797 [D loss: 0.269255, acc.: 89.06%] [G loss: 3.467378]\n",
      "epoch:41 step:32798 [D loss: 0.369404, acc.: 82.81%] [G loss: 4.042104]\n",
      "epoch:41 step:32799 [D loss: 0.261003, acc.: 85.94%] [G loss: 3.509766]\n",
      "epoch:41 step:32800 [D loss: 0.269384, acc.: 85.94%] [G loss: 5.034613]\n",
      "##############\n",
      "[0.85466037 0.85953669 0.82035683 0.82257021 0.77686459 0.82282574\n",
      " 0.87501274 0.83482353 0.81698838 0.82100826]\n",
      "##########\n",
      "epoch:41 step:32801 [D loss: 0.300460, acc.: 90.62%] [G loss: 4.826782]\n",
      "epoch:41 step:32802 [D loss: 0.230305, acc.: 89.06%] [G loss: 3.779923]\n",
      "epoch:42 step:32803 [D loss: 0.167315, acc.: 96.09%] [G loss: 4.065095]\n",
      "epoch:42 step:32804 [D loss: 0.279385, acc.: 91.41%] [G loss: 2.949663]\n",
      "epoch:42 step:32805 [D loss: 0.343975, acc.: 83.59%] [G loss: 3.010435]\n",
      "epoch:42 step:32806 [D loss: 0.285387, acc.: 86.72%] [G loss: 3.216360]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:42 step:32807 [D loss: 0.299612, acc.: 86.72%] [G loss: 3.049395]\n",
      "epoch:42 step:32808 [D loss: 0.350735, acc.: 85.16%] [G loss: 3.169734]\n",
      "epoch:42 step:32809 [D loss: 0.212350, acc.: 92.97%] [G loss: 3.528214]\n",
      "epoch:42 step:32810 [D loss: 0.267714, acc.: 87.50%] [G loss: 3.244868]\n",
      "epoch:42 step:32811 [D loss: 0.358033, acc.: 85.94%] [G loss: 3.050854]\n",
      "epoch:42 step:32812 [D loss: 0.288306, acc.: 86.72%] [G loss: 3.426394]\n",
      "epoch:42 step:32813 [D loss: 0.276686, acc.: 89.06%] [G loss: 3.055369]\n",
      "epoch:42 step:32814 [D loss: 0.310042, acc.: 88.28%] [G loss: 3.022435]\n",
      "epoch:42 step:32815 [D loss: 0.296084, acc.: 85.94%] [G loss: 3.971220]\n",
      "epoch:42 step:32816 [D loss: 0.342886, acc.: 82.81%] [G loss: 6.096559]\n",
      "epoch:42 step:32817 [D loss: 0.379707, acc.: 78.91%] [G loss: 3.801471]\n",
      "epoch:42 step:32818 [D loss: 0.192129, acc.: 92.19%] [G loss: 3.535020]\n",
      "epoch:42 step:32819 [D loss: 0.315811, acc.: 85.94%] [G loss: 3.296652]\n",
      "epoch:42 step:32820 [D loss: 0.260878, acc.: 84.38%] [G loss: 3.834894]\n",
      "epoch:42 step:32821 [D loss: 0.284525, acc.: 85.16%] [G loss: 2.861104]\n",
      "epoch:42 step:32822 [D loss: 0.233492, acc.: 91.41%] [G loss: 3.791277]\n",
      "epoch:42 step:32823 [D loss: 0.223796, acc.: 91.41%] [G loss: 3.003974]\n",
      "epoch:42 step:32824 [D loss: 0.295652, acc.: 85.94%] [G loss: 3.133861]\n",
      "epoch:42 step:32825 [D loss: 0.277719, acc.: 87.50%] [G loss: 3.383014]\n",
      "epoch:42 step:32826 [D loss: 0.334171, acc.: 81.25%] [G loss: 2.453345]\n",
      "epoch:42 step:32827 [D loss: 0.169488, acc.: 95.31%] [G loss: 2.572266]\n",
      "epoch:42 step:32828 [D loss: 0.279841, acc.: 88.28%] [G loss: 3.374744]\n",
      "epoch:42 step:32829 [D loss: 0.314704, acc.: 86.72%] [G loss: 3.200187]\n",
      "epoch:42 step:32830 [D loss: 0.270579, acc.: 88.28%] [G loss: 4.874448]\n",
      "epoch:42 step:32831 [D loss: 0.376673, acc.: 80.47%] [G loss: 2.712891]\n",
      "epoch:42 step:32832 [D loss: 0.218623, acc.: 89.06%] [G loss: 3.786333]\n",
      "epoch:42 step:32833 [D loss: 0.318948, acc.: 87.50%] [G loss: 2.931342]\n",
      "epoch:42 step:32834 [D loss: 0.410754, acc.: 79.69%] [G loss: 2.869118]\n",
      "epoch:42 step:32835 [D loss: 0.190343, acc.: 91.41%] [G loss: 3.418830]\n",
      "epoch:42 step:32836 [D loss: 0.372234, acc.: 83.59%] [G loss: 3.635931]\n",
      "epoch:42 step:32837 [D loss: 0.479458, acc.: 79.69%] [G loss: 3.902264]\n",
      "epoch:42 step:32838 [D loss: 0.330181, acc.: 83.59%] [G loss: 3.557862]\n",
      "epoch:42 step:32839 [D loss: 0.266546, acc.: 88.28%] [G loss: 6.270362]\n",
      "epoch:42 step:32840 [D loss: 0.287988, acc.: 85.94%] [G loss: 4.276621]\n",
      "epoch:42 step:32841 [D loss: 0.206678, acc.: 91.41%] [G loss: 3.729133]\n",
      "epoch:42 step:32842 [D loss: 0.276525, acc.: 86.72%] [G loss: 3.461037]\n",
      "epoch:42 step:32843 [D loss: 0.282116, acc.: 85.94%] [G loss: 3.648162]\n",
      "epoch:42 step:32844 [D loss: 0.281574, acc.: 90.62%] [G loss: 2.934876]\n",
      "epoch:42 step:32845 [D loss: 0.360884, acc.: 82.03%] [G loss: 2.984379]\n",
      "epoch:42 step:32846 [D loss: 0.294132, acc.: 89.06%] [G loss: 3.530376]\n",
      "epoch:42 step:32847 [D loss: 0.366706, acc.: 84.38%] [G loss: 3.411832]\n",
      "epoch:42 step:32848 [D loss: 0.203841, acc.: 93.75%] [G loss: 3.199417]\n",
      "epoch:42 step:32849 [D loss: 0.372112, acc.: 85.16%] [G loss: 3.385415]\n",
      "epoch:42 step:32850 [D loss: 0.287586, acc.: 83.59%] [G loss: 4.155617]\n",
      "epoch:42 step:32851 [D loss: 0.310964, acc.: 86.72%] [G loss: 3.120454]\n",
      "epoch:42 step:32852 [D loss: 0.331077, acc.: 82.81%] [G loss: 3.442655]\n",
      "epoch:42 step:32853 [D loss: 0.406532, acc.: 83.59%] [G loss: 3.578683]\n",
      "epoch:42 step:32854 [D loss: 0.248243, acc.: 88.28%] [G loss: 3.095564]\n",
      "epoch:42 step:32855 [D loss: 0.307196, acc.: 85.94%] [G loss: 3.685724]\n",
      "epoch:42 step:32856 [D loss: 0.351190, acc.: 84.38%] [G loss: 4.369709]\n",
      "epoch:42 step:32857 [D loss: 0.383985, acc.: 82.81%] [G loss: 4.341049]\n",
      "epoch:42 step:32858 [D loss: 0.431877, acc.: 75.00%] [G loss: 3.390822]\n",
      "epoch:42 step:32859 [D loss: 0.346774, acc.: 85.16%] [G loss: 3.813757]\n",
      "epoch:42 step:32860 [D loss: 0.303529, acc.: 86.72%] [G loss: 3.746064]\n",
      "epoch:42 step:32861 [D loss: 0.287090, acc.: 89.84%] [G loss: 3.522882]\n",
      "epoch:42 step:32862 [D loss: 0.396676, acc.: 78.12%] [G loss: 4.066536]\n",
      "epoch:42 step:32863 [D loss: 0.340995, acc.: 85.16%] [G loss: 3.403488]\n",
      "epoch:42 step:32864 [D loss: 0.279020, acc.: 87.50%] [G loss: 3.942728]\n",
      "epoch:42 step:32865 [D loss: 0.284997, acc.: 90.62%] [G loss: 4.058081]\n",
      "epoch:42 step:32866 [D loss: 0.342607, acc.: 83.59%] [G loss: 4.910829]\n",
      "epoch:42 step:32867 [D loss: 0.329829, acc.: 85.94%] [G loss: 5.755688]\n",
      "epoch:42 step:32868 [D loss: 0.316130, acc.: 82.81%] [G loss: 3.960248]\n",
      "epoch:42 step:32869 [D loss: 0.204348, acc.: 89.84%] [G loss: 7.875346]\n",
      "epoch:42 step:32870 [D loss: 0.264451, acc.: 86.72%] [G loss: 5.788394]\n",
      "epoch:42 step:32871 [D loss: 0.215628, acc.: 91.41%] [G loss: 4.692807]\n",
      "epoch:42 step:32872 [D loss: 0.253474, acc.: 88.28%] [G loss: 4.666739]\n",
      "epoch:42 step:32873 [D loss: 0.265716, acc.: 85.16%] [G loss: 4.322655]\n",
      "epoch:42 step:32874 [D loss: 0.302049, acc.: 85.94%] [G loss: 4.262463]\n",
      "epoch:42 step:32875 [D loss: 0.427736, acc.: 81.25%] [G loss: 3.153288]\n",
      "epoch:42 step:32876 [D loss: 0.347472, acc.: 82.03%] [G loss: 3.237793]\n",
      "epoch:42 step:32877 [D loss: 0.344732, acc.: 83.59%] [G loss: 2.926660]\n",
      "epoch:42 step:32878 [D loss: 0.239001, acc.: 90.62%] [G loss: 3.494423]\n",
      "epoch:42 step:32879 [D loss: 0.263510, acc.: 88.28%] [G loss: 3.529283]\n",
      "epoch:42 step:32880 [D loss: 0.312191, acc.: 82.81%] [G loss: 2.778884]\n",
      "epoch:42 step:32881 [D loss: 0.285840, acc.: 85.16%] [G loss: 4.192153]\n",
      "epoch:42 step:32882 [D loss: 0.402434, acc.: 82.03%] [G loss: 4.276845]\n",
      "epoch:42 step:32883 [D loss: 0.304178, acc.: 86.72%] [G loss: 4.464077]\n",
      "epoch:42 step:32884 [D loss: 0.277718, acc.: 89.06%] [G loss: 3.408344]\n",
      "epoch:42 step:32885 [D loss: 0.281625, acc.: 88.28%] [G loss: 3.991387]\n",
      "epoch:42 step:32886 [D loss: 0.205671, acc.: 89.84%] [G loss: 3.335858]\n",
      "epoch:42 step:32887 [D loss: 0.275254, acc.: 89.06%] [G loss: 3.243245]\n",
      "epoch:42 step:32888 [D loss: 0.310496, acc.: 86.72%] [G loss: 2.955760]\n",
      "epoch:42 step:32889 [D loss: 0.271589, acc.: 86.72%] [G loss: 4.299662]\n",
      "epoch:42 step:32890 [D loss: 0.267331, acc.: 86.72%] [G loss: 3.694379]\n",
      "epoch:42 step:32891 [D loss: 0.228584, acc.: 90.62%] [G loss: 4.496866]\n",
      "epoch:42 step:32892 [D loss: 0.381469, acc.: 82.03%] [G loss: 3.886369]\n",
      "epoch:42 step:32893 [D loss: 0.356946, acc.: 85.16%] [G loss: 3.438692]\n",
      "epoch:42 step:32894 [D loss: 0.266404, acc.: 89.06%] [G loss: 3.530108]\n",
      "epoch:42 step:32895 [D loss: 0.292351, acc.: 85.94%] [G loss: 3.620960]\n",
      "epoch:42 step:32896 [D loss: 0.242635, acc.: 91.41%] [G loss: 3.429101]\n",
      "epoch:42 step:32897 [D loss: 0.398281, acc.: 83.59%] [G loss: 3.437876]\n",
      "epoch:42 step:32898 [D loss: 0.278424, acc.: 89.06%] [G loss: 2.915021]\n",
      "epoch:42 step:32899 [D loss: 0.300465, acc.: 84.38%] [G loss: 3.430380]\n",
      "epoch:42 step:32900 [D loss: 0.555861, acc.: 75.78%] [G loss: 8.739758]\n",
      "epoch:42 step:32901 [D loss: 0.951912, acc.: 66.41%] [G loss: 7.154630]\n",
      "epoch:42 step:32902 [D loss: 1.259362, acc.: 67.97%] [G loss: 8.605104]\n",
      "epoch:42 step:32903 [D loss: 2.439087, acc.: 40.62%] [G loss: 3.090178]\n",
      "epoch:42 step:32904 [D loss: 0.538325, acc.: 79.69%] [G loss: 3.974646]\n",
      "epoch:42 step:32905 [D loss: 0.469357, acc.: 78.12%] [G loss: 3.280057]\n",
      "epoch:42 step:32906 [D loss: 0.428701, acc.: 82.03%] [G loss: 3.990044]\n",
      "epoch:42 step:32907 [D loss: 0.463392, acc.: 75.78%] [G loss: 3.547584]\n",
      "epoch:42 step:32908 [D loss: 0.407986, acc.: 82.03%] [G loss: 3.675595]\n",
      "epoch:42 step:32909 [D loss: 0.334005, acc.: 82.81%] [G loss: 3.573520]\n",
      "epoch:42 step:32910 [D loss: 0.269508, acc.: 87.50%] [G loss: 3.589689]\n",
      "epoch:42 step:32911 [D loss: 0.385588, acc.: 82.81%] [G loss: 2.982072]\n",
      "epoch:42 step:32912 [D loss: 0.370803, acc.: 83.59%] [G loss: 2.610789]\n",
      "epoch:42 step:32913 [D loss: 0.408355, acc.: 79.69%] [G loss: 2.919555]\n",
      "epoch:42 step:32914 [D loss: 0.266281, acc.: 90.62%] [G loss: 2.700081]\n",
      "epoch:42 step:32915 [D loss: 0.336847, acc.: 86.72%] [G loss: 3.268307]\n",
      "epoch:42 step:32916 [D loss: 0.337123, acc.: 83.59%] [G loss: 3.222595]\n",
      "epoch:42 step:32917 [D loss: 0.342947, acc.: 85.16%] [G loss: 3.092383]\n",
      "epoch:42 step:32918 [D loss: 0.332207, acc.: 85.16%] [G loss: 2.932327]\n",
      "epoch:42 step:32919 [D loss: 0.297615, acc.: 88.28%] [G loss: 3.150505]\n",
      "epoch:42 step:32920 [D loss: 0.348548, acc.: 82.81%] [G loss: 3.043053]\n",
      "epoch:42 step:32921 [D loss: 0.297906, acc.: 85.94%] [G loss: 2.897741]\n",
      "epoch:42 step:32922 [D loss: 0.307259, acc.: 86.72%] [G loss: 2.886155]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:42 step:32923 [D loss: 0.202554, acc.: 93.75%] [G loss: 2.705323]\n",
      "epoch:42 step:32924 [D loss: 0.411470, acc.: 78.91%] [G loss: 2.563993]\n",
      "epoch:42 step:32925 [D loss: 0.340909, acc.: 85.94%] [G loss: 2.513081]\n",
      "epoch:42 step:32926 [D loss: 0.332547, acc.: 85.16%] [G loss: 3.101063]\n",
      "epoch:42 step:32927 [D loss: 0.302876, acc.: 88.28%] [G loss: 2.833476]\n",
      "epoch:42 step:32928 [D loss: 0.315933, acc.: 88.28%] [G loss: 3.395677]\n",
      "epoch:42 step:32929 [D loss: 0.279261, acc.: 88.28%] [G loss: 3.011571]\n",
      "epoch:42 step:32930 [D loss: 0.272916, acc.: 86.72%] [G loss: 3.309367]\n",
      "epoch:42 step:32931 [D loss: 0.247633, acc.: 90.62%] [G loss: 2.501140]\n",
      "epoch:42 step:32932 [D loss: 0.272270, acc.: 89.84%] [G loss: 2.421490]\n",
      "epoch:42 step:32933 [D loss: 0.288887, acc.: 87.50%] [G loss: 2.721137]\n",
      "epoch:42 step:32934 [D loss: 0.329344, acc.: 85.94%] [G loss: 2.769016]\n",
      "epoch:42 step:32935 [D loss: 0.317851, acc.: 83.59%] [G loss: 3.738123]\n",
      "epoch:42 step:32936 [D loss: 0.338777, acc.: 83.59%] [G loss: 2.932339]\n",
      "epoch:42 step:32937 [D loss: 0.297184, acc.: 85.94%] [G loss: 3.388320]\n",
      "epoch:42 step:32938 [D loss: 0.386719, acc.: 81.25%] [G loss: 2.857028]\n",
      "epoch:42 step:32939 [D loss: 0.278888, acc.: 85.16%] [G loss: 3.688491]\n",
      "epoch:42 step:32940 [D loss: 0.358391, acc.: 82.81%] [G loss: 3.026031]\n",
      "epoch:42 step:32941 [D loss: 0.358096, acc.: 84.38%] [G loss: 3.023465]\n",
      "epoch:42 step:32942 [D loss: 0.271830, acc.: 85.94%] [G loss: 2.950431]\n",
      "epoch:42 step:32943 [D loss: 0.376839, acc.: 79.69%] [G loss: 3.340720]\n",
      "epoch:42 step:32944 [D loss: 0.345435, acc.: 86.72%] [G loss: 3.171218]\n",
      "epoch:42 step:32945 [D loss: 0.330857, acc.: 85.16%] [G loss: 3.032354]\n",
      "epoch:42 step:32946 [D loss: 0.424365, acc.: 81.25%] [G loss: 3.765416]\n",
      "epoch:42 step:32947 [D loss: 0.342167, acc.: 84.38%] [G loss: 2.551313]\n",
      "epoch:42 step:32948 [D loss: 0.396849, acc.: 82.81%] [G loss: 3.563166]\n",
      "epoch:42 step:32949 [D loss: 0.370757, acc.: 82.03%] [G loss: 3.253864]\n",
      "epoch:42 step:32950 [D loss: 0.429044, acc.: 79.69%] [G loss: 3.923263]\n",
      "epoch:42 step:32951 [D loss: 0.352095, acc.: 84.38%] [G loss: 3.020561]\n",
      "epoch:42 step:32952 [D loss: 0.283781, acc.: 86.72%] [G loss: 3.886920]\n",
      "epoch:42 step:32953 [D loss: 0.336003, acc.: 84.38%] [G loss: 3.443588]\n",
      "epoch:42 step:32954 [D loss: 0.388662, acc.: 81.25%] [G loss: 2.918693]\n",
      "epoch:42 step:32955 [D loss: 0.325038, acc.: 86.72%] [G loss: 3.177984]\n",
      "epoch:42 step:32956 [D loss: 0.311262, acc.: 86.72%] [G loss: 3.452662]\n",
      "epoch:42 step:32957 [D loss: 0.374057, acc.: 83.59%] [G loss: 3.161552]\n",
      "epoch:42 step:32958 [D loss: 0.234190, acc.: 89.84%] [G loss: 3.963905]\n",
      "epoch:42 step:32959 [D loss: 0.380301, acc.: 86.72%] [G loss: 2.692009]\n",
      "epoch:42 step:32960 [D loss: 0.302059, acc.: 87.50%] [G loss: 3.280393]\n",
      "epoch:42 step:32961 [D loss: 0.266919, acc.: 89.06%] [G loss: 3.122029]\n",
      "epoch:42 step:32962 [D loss: 0.364022, acc.: 85.16%] [G loss: 2.967323]\n",
      "epoch:42 step:32963 [D loss: 0.281245, acc.: 87.50%] [G loss: 2.637857]\n",
      "epoch:42 step:32964 [D loss: 0.264315, acc.: 89.06%] [G loss: 3.369861]\n",
      "epoch:42 step:32965 [D loss: 0.287069, acc.: 86.72%] [G loss: 3.694456]\n",
      "epoch:42 step:32966 [D loss: 0.269357, acc.: 88.28%] [G loss: 3.285207]\n",
      "epoch:42 step:32967 [D loss: 0.297679, acc.: 82.81%] [G loss: 2.558077]\n",
      "epoch:42 step:32968 [D loss: 0.339646, acc.: 85.16%] [G loss: 3.176533]\n",
      "epoch:42 step:32969 [D loss: 0.277149, acc.: 88.28%] [G loss: 2.651319]\n",
      "epoch:42 step:32970 [D loss: 0.381742, acc.: 78.91%] [G loss: 3.089158]\n",
      "epoch:42 step:32971 [D loss: 0.386509, acc.: 81.25%] [G loss: 2.497293]\n",
      "epoch:42 step:32972 [D loss: 0.293562, acc.: 86.72%] [G loss: 2.506954]\n",
      "epoch:42 step:32973 [D loss: 0.301499, acc.: 90.62%] [G loss: 2.308298]\n",
      "epoch:42 step:32974 [D loss: 0.360333, acc.: 84.38%] [G loss: 2.597004]\n",
      "epoch:42 step:32975 [D loss: 0.556118, acc.: 74.22%] [G loss: 3.119581]\n",
      "epoch:42 step:32976 [D loss: 0.492501, acc.: 77.34%] [G loss: 3.197761]\n",
      "epoch:42 step:32977 [D loss: 0.343117, acc.: 82.03%] [G loss: 3.677168]\n",
      "epoch:42 step:32978 [D loss: 0.339799, acc.: 85.16%] [G loss: 2.876843]\n",
      "epoch:42 step:32979 [D loss: 0.385167, acc.: 80.47%] [G loss: 3.798899]\n",
      "epoch:42 step:32980 [D loss: 0.338551, acc.: 81.25%] [G loss: 4.201280]\n",
      "epoch:42 step:32981 [D loss: 0.315182, acc.: 89.06%] [G loss: 2.846030]\n",
      "epoch:42 step:32982 [D loss: 0.449262, acc.: 82.03%] [G loss: 3.585276]\n",
      "epoch:42 step:32983 [D loss: 0.247688, acc.: 89.06%] [G loss: 3.715434]\n",
      "epoch:42 step:32984 [D loss: 0.527133, acc.: 77.34%] [G loss: 3.362179]\n",
      "epoch:42 step:32985 [D loss: 0.380248, acc.: 82.03%] [G loss: 3.333386]\n",
      "epoch:42 step:32986 [D loss: 0.303672, acc.: 88.28%] [G loss: 3.972735]\n",
      "epoch:42 step:32987 [D loss: 0.301773, acc.: 87.50%] [G loss: 3.282431]\n",
      "epoch:42 step:32988 [D loss: 0.327981, acc.: 85.94%] [G loss: 2.457803]\n",
      "epoch:42 step:32989 [D loss: 0.400106, acc.: 80.47%] [G loss: 2.682059]\n",
      "epoch:42 step:32990 [D loss: 0.350376, acc.: 85.16%] [G loss: 3.995205]\n",
      "epoch:42 step:32991 [D loss: 0.405711, acc.: 85.16%] [G loss: 3.715730]\n",
      "epoch:42 step:32992 [D loss: 0.343955, acc.: 85.94%] [G loss: 3.262451]\n",
      "epoch:42 step:32993 [D loss: 0.289352, acc.: 87.50%] [G loss: 3.087087]\n",
      "epoch:42 step:32994 [D loss: 0.480005, acc.: 82.03%] [G loss: 3.790459]\n",
      "epoch:42 step:32995 [D loss: 0.387499, acc.: 84.38%] [G loss: 3.206490]\n",
      "epoch:42 step:32996 [D loss: 0.318696, acc.: 86.72%] [G loss: 3.781845]\n",
      "epoch:42 step:32997 [D loss: 0.348286, acc.: 83.59%] [G loss: 3.233197]\n",
      "epoch:42 step:32998 [D loss: 0.192544, acc.: 93.75%] [G loss: 3.307758]\n",
      "epoch:42 step:32999 [D loss: 0.376941, acc.: 84.38%] [G loss: 3.221390]\n",
      "epoch:42 step:33000 [D loss: 0.278209, acc.: 87.50%] [G loss: 3.370110]\n",
      "##############\n",
      "[0.87125615 0.85917018 0.82276588 0.83682845 0.77110636 0.81603324\n",
      " 0.87179739 0.84057641 0.81296432 0.82814738]\n",
      "##########\n",
      "epoch:42 step:33001 [D loss: 0.285919, acc.: 89.84%] [G loss: 3.131863]\n",
      "epoch:42 step:33002 [D loss: 0.294309, acc.: 85.94%] [G loss: 3.477358]\n",
      "epoch:42 step:33003 [D loss: 0.256176, acc.: 89.06%] [G loss: 3.278974]\n",
      "epoch:42 step:33004 [D loss: 0.368263, acc.: 81.25%] [G loss: 3.208429]\n",
      "epoch:42 step:33005 [D loss: 0.252987, acc.: 89.06%] [G loss: 3.163287]\n",
      "epoch:42 step:33006 [D loss: 0.256754, acc.: 90.62%] [G loss: 3.137006]\n",
      "epoch:42 step:33007 [D loss: 0.300849, acc.: 88.28%] [G loss: 3.572576]\n",
      "epoch:42 step:33008 [D loss: 0.325673, acc.: 84.38%] [G loss: 2.880225]\n",
      "epoch:42 step:33009 [D loss: 0.229257, acc.: 88.28%] [G loss: 2.875386]\n",
      "epoch:42 step:33010 [D loss: 0.486038, acc.: 82.03%] [G loss: 2.519206]\n",
      "epoch:42 step:33011 [D loss: 0.311735, acc.: 84.38%] [G loss: 3.635590]\n",
      "epoch:42 step:33012 [D loss: 0.261880, acc.: 88.28%] [G loss: 3.408211]\n",
      "epoch:42 step:33013 [D loss: 0.446978, acc.: 78.91%] [G loss: 3.998827]\n",
      "epoch:42 step:33014 [D loss: 0.253038, acc.: 90.62%] [G loss: 3.185808]\n",
      "epoch:42 step:33015 [D loss: 0.303042, acc.: 88.28%] [G loss: 4.395736]\n",
      "epoch:42 step:33016 [D loss: 0.428508, acc.: 79.69%] [G loss: 2.961594]\n",
      "epoch:42 step:33017 [D loss: 0.233617, acc.: 91.41%] [G loss: 2.719013]\n",
      "epoch:42 step:33018 [D loss: 0.337877, acc.: 82.81%] [G loss: 3.010189]\n",
      "epoch:42 step:33019 [D loss: 0.307236, acc.: 85.94%] [G loss: 3.105254]\n",
      "epoch:42 step:33020 [D loss: 0.340187, acc.: 84.38%] [G loss: 3.265115]\n",
      "epoch:42 step:33021 [D loss: 0.294771, acc.: 85.16%] [G loss: 3.108472]\n",
      "epoch:42 step:33022 [D loss: 0.265403, acc.: 89.06%] [G loss: 3.776100]\n",
      "epoch:42 step:33023 [D loss: 0.288008, acc.: 89.06%] [G loss: 3.264724]\n",
      "epoch:42 step:33024 [D loss: 0.319344, acc.: 86.72%] [G loss: 3.035599]\n",
      "epoch:42 step:33025 [D loss: 0.279586, acc.: 88.28%] [G loss: 3.324698]\n",
      "epoch:42 step:33026 [D loss: 0.351730, acc.: 82.81%] [G loss: 3.600650]\n",
      "epoch:42 step:33027 [D loss: 0.352819, acc.: 85.16%] [G loss: 3.290267]\n",
      "epoch:42 step:33028 [D loss: 0.227847, acc.: 92.19%] [G loss: 3.474012]\n",
      "epoch:42 step:33029 [D loss: 0.398796, acc.: 79.69%] [G loss: 2.932096]\n",
      "epoch:42 step:33030 [D loss: 0.431576, acc.: 82.81%] [G loss: 4.893841]\n",
      "epoch:42 step:33031 [D loss: 0.419111, acc.: 84.38%] [G loss: 3.618136]\n",
      "epoch:42 step:33032 [D loss: 0.380876, acc.: 83.59%] [G loss: 3.860709]\n",
      "epoch:42 step:33033 [D loss: 0.372595, acc.: 82.81%] [G loss: 3.299380]\n",
      "epoch:42 step:33034 [D loss: 0.278044, acc.: 88.28%] [G loss: 4.686875]\n",
      "epoch:42 step:33035 [D loss: 0.402221, acc.: 79.69%] [G loss: 4.440488]\n",
      "epoch:42 step:33036 [D loss: 0.288033, acc.: 85.94%] [G loss: 3.537385]\n",
      "epoch:42 step:33037 [D loss: 0.416501, acc.: 82.03%] [G loss: 3.251761]\n",
      "epoch:42 step:33038 [D loss: 0.415574, acc.: 78.91%] [G loss: 3.427570]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:42 step:33039 [D loss: 0.356288, acc.: 84.38%] [G loss: 3.582782]\n",
      "epoch:42 step:33040 [D loss: 0.363935, acc.: 85.16%] [G loss: 3.398515]\n",
      "epoch:42 step:33041 [D loss: 0.325252, acc.: 88.28%] [G loss: 3.120720]\n",
      "epoch:42 step:33042 [D loss: 0.386629, acc.: 80.47%] [G loss: 2.990937]\n",
      "epoch:42 step:33043 [D loss: 0.268053, acc.: 89.06%] [G loss: 2.604453]\n",
      "epoch:42 step:33044 [D loss: 0.445275, acc.: 78.91%] [G loss: 4.135076]\n",
      "epoch:42 step:33045 [D loss: 0.317068, acc.: 85.94%] [G loss: 3.804234]\n",
      "epoch:42 step:33046 [D loss: 0.391548, acc.: 85.94%] [G loss: 2.996444]\n",
      "epoch:42 step:33047 [D loss: 0.284818, acc.: 88.28%] [G loss: 3.150647]\n",
      "epoch:42 step:33048 [D loss: 0.364924, acc.: 82.03%] [G loss: 3.109191]\n",
      "epoch:42 step:33049 [D loss: 0.333190, acc.: 84.38%] [G loss: 2.534947]\n",
      "epoch:42 step:33050 [D loss: 0.365730, acc.: 82.81%] [G loss: 3.450223]\n",
      "epoch:42 step:33051 [D loss: 0.381519, acc.: 84.38%] [G loss: 2.345926]\n",
      "epoch:42 step:33052 [D loss: 0.380632, acc.: 80.47%] [G loss: 2.727862]\n",
      "epoch:42 step:33053 [D loss: 0.279133, acc.: 85.16%] [G loss: 3.856453]\n",
      "epoch:42 step:33054 [D loss: 0.291610, acc.: 85.16%] [G loss: 3.042875]\n",
      "epoch:42 step:33055 [D loss: 0.317833, acc.: 83.59%] [G loss: 3.417299]\n",
      "epoch:42 step:33056 [D loss: 0.237359, acc.: 90.62%] [G loss: 4.372462]\n",
      "epoch:42 step:33057 [D loss: 0.359393, acc.: 82.81%] [G loss: 3.655216]\n",
      "epoch:42 step:33058 [D loss: 0.325121, acc.: 84.38%] [G loss: 3.064082]\n",
      "epoch:42 step:33059 [D loss: 0.369064, acc.: 81.25%] [G loss: 3.517777]\n",
      "epoch:42 step:33060 [D loss: 0.323527, acc.: 82.81%] [G loss: 2.545200]\n",
      "epoch:42 step:33061 [D loss: 0.334172, acc.: 85.94%] [G loss: 3.482049]\n",
      "epoch:42 step:33062 [D loss: 0.265687, acc.: 89.06%] [G loss: 2.997496]\n",
      "epoch:42 step:33063 [D loss: 0.301275, acc.: 85.94%] [G loss: 3.590187]\n",
      "epoch:42 step:33064 [D loss: 0.237398, acc.: 88.28%] [G loss: 2.784731]\n",
      "epoch:42 step:33065 [D loss: 0.269011, acc.: 89.06%] [G loss: 2.722748]\n",
      "epoch:42 step:33066 [D loss: 0.340490, acc.: 85.94%] [G loss: 3.774840]\n",
      "epoch:42 step:33067 [D loss: 0.256627, acc.: 89.84%] [G loss: 4.341130]\n",
      "epoch:42 step:33068 [D loss: 0.303561, acc.: 82.81%] [G loss: 3.513607]\n",
      "epoch:42 step:33069 [D loss: 0.448882, acc.: 79.69%] [G loss: 4.213319]\n",
      "epoch:42 step:33070 [D loss: 0.326391, acc.: 85.16%] [G loss: 3.352758]\n",
      "epoch:42 step:33071 [D loss: 0.335441, acc.: 82.81%] [G loss: 3.720877]\n",
      "epoch:42 step:33072 [D loss: 0.344719, acc.: 83.59%] [G loss: 3.208153]\n",
      "epoch:42 step:33073 [D loss: 0.355012, acc.: 82.03%] [G loss: 3.323926]\n",
      "epoch:42 step:33074 [D loss: 0.268103, acc.: 88.28%] [G loss: 4.130799]\n",
      "epoch:42 step:33075 [D loss: 0.336399, acc.: 85.16%] [G loss: 3.436808]\n",
      "epoch:42 step:33076 [D loss: 0.318788, acc.: 86.72%] [G loss: 3.471838]\n",
      "epoch:42 step:33077 [D loss: 0.328174, acc.: 85.94%] [G loss: 3.257102]\n",
      "epoch:42 step:33078 [D loss: 0.336459, acc.: 85.16%] [G loss: 2.478085]\n",
      "epoch:42 step:33079 [D loss: 0.323176, acc.: 85.94%] [G loss: 2.976756]\n",
      "epoch:42 step:33080 [D loss: 0.376048, acc.: 83.59%] [G loss: 2.933643]\n",
      "epoch:42 step:33081 [D loss: 0.496385, acc.: 84.38%] [G loss: 3.358005]\n",
      "epoch:42 step:33082 [D loss: 0.327748, acc.: 85.94%] [G loss: 3.490560]\n",
      "epoch:42 step:33083 [D loss: 0.361062, acc.: 80.47%] [G loss: 5.250317]\n",
      "epoch:42 step:33084 [D loss: 0.413659, acc.: 80.47%] [G loss: 3.822397]\n",
      "epoch:42 step:33085 [D loss: 0.337698, acc.: 84.38%] [G loss: 3.622431]\n",
      "epoch:42 step:33086 [D loss: 0.389094, acc.: 83.59%] [G loss: 2.600684]\n",
      "epoch:42 step:33087 [D loss: 0.435006, acc.: 78.91%] [G loss: 3.077213]\n",
      "epoch:42 step:33088 [D loss: 0.418886, acc.: 82.81%] [G loss: 2.558928]\n",
      "epoch:42 step:33089 [D loss: 0.294509, acc.: 89.84%] [G loss: 3.802322]\n",
      "epoch:42 step:33090 [D loss: 0.301490, acc.: 89.84%] [G loss: 3.100135]\n",
      "epoch:42 step:33091 [D loss: 0.269966, acc.: 89.84%] [G loss: 2.854539]\n",
      "epoch:42 step:33092 [D loss: 0.333036, acc.: 84.38%] [G loss: 4.795647]\n",
      "epoch:42 step:33093 [D loss: 0.244700, acc.: 89.84%] [G loss: 5.554342]\n",
      "epoch:42 step:33094 [D loss: 0.250591, acc.: 89.84%] [G loss: 4.590476]\n",
      "epoch:42 step:33095 [D loss: 0.318008, acc.: 83.59%] [G loss: 3.624306]\n",
      "epoch:42 step:33096 [D loss: 0.360190, acc.: 84.38%] [G loss: 3.899202]\n",
      "epoch:42 step:33097 [D loss: 0.302213, acc.: 87.50%] [G loss: 3.324148]\n",
      "epoch:42 step:33098 [D loss: 0.289253, acc.: 88.28%] [G loss: 3.782625]\n",
      "epoch:42 step:33099 [D loss: 0.316908, acc.: 85.94%] [G loss: 2.844327]\n",
      "epoch:42 step:33100 [D loss: 0.323504, acc.: 89.06%] [G loss: 3.180218]\n",
      "epoch:42 step:33101 [D loss: 0.251037, acc.: 87.50%] [G loss: 2.371697]\n",
      "epoch:42 step:33102 [D loss: 0.333330, acc.: 80.47%] [G loss: 3.236063]\n",
      "epoch:42 step:33103 [D loss: 0.328561, acc.: 86.72%] [G loss: 4.113096]\n",
      "epoch:42 step:33104 [D loss: 0.409234, acc.: 79.69%] [G loss: 2.919858]\n",
      "epoch:42 step:33105 [D loss: 0.275950, acc.: 89.06%] [G loss: 3.727137]\n",
      "epoch:42 step:33106 [D loss: 0.322341, acc.: 84.38%] [G loss: 3.351064]\n",
      "epoch:42 step:33107 [D loss: 0.291060, acc.: 87.50%] [G loss: 3.395297]\n",
      "epoch:42 step:33108 [D loss: 0.329559, acc.: 88.28%] [G loss: 3.058049]\n",
      "epoch:42 step:33109 [D loss: 0.343054, acc.: 89.84%] [G loss: 3.272357]\n",
      "epoch:42 step:33110 [D loss: 0.330355, acc.: 83.59%] [G loss: 3.029814]\n",
      "epoch:42 step:33111 [D loss: 0.308178, acc.: 84.38%] [G loss: 4.586208]\n",
      "epoch:42 step:33112 [D loss: 0.290102, acc.: 86.72%] [G loss: 3.406611]\n",
      "epoch:42 step:33113 [D loss: 0.341176, acc.: 84.38%] [G loss: 4.758051]\n",
      "epoch:42 step:33114 [D loss: 0.344402, acc.: 85.16%] [G loss: 3.638853]\n",
      "epoch:42 step:33115 [D loss: 0.297389, acc.: 86.72%] [G loss: 3.711383]\n",
      "epoch:42 step:33116 [D loss: 0.272780, acc.: 88.28%] [G loss: 3.741125]\n",
      "epoch:42 step:33117 [D loss: 0.595920, acc.: 66.41%] [G loss: 2.772679]\n",
      "epoch:42 step:33118 [D loss: 0.290856, acc.: 88.28%] [G loss: 2.842594]\n",
      "epoch:42 step:33119 [D loss: 0.386608, acc.: 80.47%] [G loss: 3.673031]\n",
      "epoch:42 step:33120 [D loss: 0.410915, acc.: 77.34%] [G loss: 4.173654]\n",
      "epoch:42 step:33121 [D loss: 0.354581, acc.: 82.03%] [G loss: 3.868475]\n",
      "epoch:42 step:33122 [D loss: 0.396511, acc.: 79.69%] [G loss: 4.718427]\n",
      "epoch:42 step:33123 [D loss: 0.441855, acc.: 81.25%] [G loss: 3.079123]\n",
      "epoch:42 step:33124 [D loss: 0.215879, acc.: 92.97%] [G loss: 3.526683]\n",
      "epoch:42 step:33125 [D loss: 0.293186, acc.: 85.94%] [G loss: 3.266994]\n",
      "epoch:42 step:33126 [D loss: 0.317723, acc.: 83.59%] [G loss: 3.093229]\n",
      "epoch:42 step:33127 [D loss: 0.502121, acc.: 73.44%] [G loss: 2.986377]\n",
      "epoch:42 step:33128 [D loss: 0.443215, acc.: 80.47%] [G loss: 3.599617]\n",
      "epoch:42 step:33129 [D loss: 0.339959, acc.: 82.03%] [G loss: 4.244276]\n",
      "epoch:42 step:33130 [D loss: 0.358580, acc.: 82.81%] [G loss: 3.109761]\n",
      "epoch:42 step:33131 [D loss: 0.257922, acc.: 90.62%] [G loss: 3.455203]\n",
      "epoch:42 step:33132 [D loss: 0.292926, acc.: 85.94%] [G loss: 3.265251]\n",
      "epoch:42 step:33133 [D loss: 0.329794, acc.: 85.94%] [G loss: 3.349583]\n",
      "epoch:42 step:33134 [D loss: 0.330915, acc.: 86.72%] [G loss: 3.793016]\n",
      "epoch:42 step:33135 [D loss: 0.323896, acc.: 82.81%] [G loss: 2.603307]\n",
      "epoch:42 step:33136 [D loss: 0.368847, acc.: 85.94%] [G loss: 3.279190]\n",
      "epoch:42 step:33137 [D loss: 0.298292, acc.: 86.72%] [G loss: 3.202321]\n",
      "epoch:42 step:33138 [D loss: 0.214663, acc.: 94.53%] [G loss: 2.952803]\n",
      "epoch:42 step:33139 [D loss: 0.351891, acc.: 84.38%] [G loss: 2.127228]\n",
      "epoch:42 step:33140 [D loss: 0.256896, acc.: 88.28%] [G loss: 2.790738]\n",
      "epoch:42 step:33141 [D loss: 0.308808, acc.: 85.16%] [G loss: 4.053457]\n",
      "epoch:42 step:33142 [D loss: 0.292510, acc.: 86.72%] [G loss: 4.856349]\n",
      "epoch:42 step:33143 [D loss: 0.310370, acc.: 88.28%] [G loss: 2.884379]\n",
      "epoch:42 step:33144 [D loss: 0.269630, acc.: 87.50%] [G loss: 3.427011]\n",
      "epoch:42 step:33145 [D loss: 0.248559, acc.: 88.28%] [G loss: 3.427793]\n",
      "epoch:42 step:33146 [D loss: 0.290521, acc.: 85.94%] [G loss: 4.063456]\n",
      "epoch:42 step:33147 [D loss: 0.230854, acc.: 89.84%] [G loss: 4.244095]\n",
      "epoch:42 step:33148 [D loss: 0.274375, acc.: 89.84%] [G loss: 3.809873]\n",
      "epoch:42 step:33149 [D loss: 0.253156, acc.: 89.84%] [G loss: 5.444933]\n",
      "epoch:42 step:33150 [D loss: 0.286406, acc.: 85.94%] [G loss: 3.288759]\n",
      "epoch:42 step:33151 [D loss: 0.294539, acc.: 82.03%] [G loss: 4.526096]\n",
      "epoch:42 step:33152 [D loss: 0.373642, acc.: 81.25%] [G loss: 3.021843]\n",
      "epoch:42 step:33153 [D loss: 0.313316, acc.: 85.94%] [G loss: 3.270399]\n",
      "epoch:42 step:33154 [D loss: 0.374629, acc.: 85.16%] [G loss: 3.794297]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:42 step:33155 [D loss: 0.341696, acc.: 83.59%] [G loss: 3.843706]\n",
      "epoch:42 step:33156 [D loss: 0.252557, acc.: 90.62%] [G loss: 4.842000]\n",
      "epoch:42 step:33157 [D loss: 0.210469, acc.: 94.53%] [G loss: 5.077035]\n",
      "epoch:42 step:33158 [D loss: 0.242685, acc.: 89.06%] [G loss: 4.369882]\n",
      "epoch:42 step:33159 [D loss: 0.280158, acc.: 88.28%] [G loss: 3.205564]\n",
      "epoch:42 step:33160 [D loss: 0.318757, acc.: 89.84%] [G loss: 3.025766]\n",
      "epoch:42 step:33161 [D loss: 0.289903, acc.: 89.84%] [G loss: 2.826387]\n",
      "epoch:42 step:33162 [D loss: 0.353775, acc.: 83.59%] [G loss: 3.249816]\n",
      "epoch:42 step:33163 [D loss: 0.427282, acc.: 82.03%] [G loss: 2.966147]\n",
      "epoch:42 step:33164 [D loss: 0.283731, acc.: 88.28%] [G loss: 3.716275]\n",
      "epoch:42 step:33165 [D loss: 0.267804, acc.: 89.06%] [G loss: 3.929837]\n",
      "epoch:42 step:33166 [D loss: 0.346037, acc.: 81.25%] [G loss: 4.656389]\n",
      "epoch:42 step:33167 [D loss: 0.308295, acc.: 86.72%] [G loss: 3.702499]\n",
      "epoch:42 step:33168 [D loss: 0.311038, acc.: 87.50%] [G loss: 3.429781]\n",
      "epoch:42 step:33169 [D loss: 0.294253, acc.: 88.28%] [G loss: 6.166922]\n",
      "epoch:42 step:33170 [D loss: 0.315994, acc.: 85.94%] [G loss: 4.200415]\n",
      "epoch:42 step:33171 [D loss: 0.299319, acc.: 85.16%] [G loss: 2.823344]\n",
      "epoch:42 step:33172 [D loss: 0.249363, acc.: 89.06%] [G loss: 3.463345]\n",
      "epoch:42 step:33173 [D loss: 0.256659, acc.: 90.62%] [G loss: 3.828870]\n",
      "epoch:42 step:33174 [D loss: 0.241413, acc.: 92.19%] [G loss: 3.058301]\n",
      "epoch:42 step:33175 [D loss: 0.290196, acc.: 85.16%] [G loss: 2.960208]\n",
      "epoch:42 step:33176 [D loss: 0.410131, acc.: 78.91%] [G loss: 2.441599]\n",
      "epoch:42 step:33177 [D loss: 0.339799, acc.: 85.16%] [G loss: 3.367774]\n",
      "epoch:42 step:33178 [D loss: 0.351732, acc.: 82.81%] [G loss: 2.844844]\n",
      "epoch:42 step:33179 [D loss: 0.270017, acc.: 89.06%] [G loss: 6.066915]\n",
      "epoch:42 step:33180 [D loss: 0.455512, acc.: 75.00%] [G loss: 4.201118]\n",
      "epoch:42 step:33181 [D loss: 0.337369, acc.: 84.38%] [G loss: 3.681651]\n",
      "epoch:42 step:33182 [D loss: 0.419143, acc.: 78.91%] [G loss: 3.450621]\n",
      "epoch:42 step:33183 [D loss: 0.273364, acc.: 87.50%] [G loss: 4.048552]\n",
      "epoch:42 step:33184 [D loss: 0.282869, acc.: 86.72%] [G loss: 2.930161]\n",
      "epoch:42 step:33185 [D loss: 0.300671, acc.: 83.59%] [G loss: 3.221427]\n",
      "epoch:42 step:33186 [D loss: 0.224260, acc.: 89.06%] [G loss: 2.980305]\n",
      "epoch:42 step:33187 [D loss: 0.283508, acc.: 88.28%] [G loss: 3.570582]\n",
      "epoch:42 step:33188 [D loss: 0.332844, acc.: 84.38%] [G loss: 2.885859]\n",
      "epoch:42 step:33189 [D loss: 0.300731, acc.: 85.94%] [G loss: 3.395640]\n",
      "epoch:42 step:33190 [D loss: 0.380082, acc.: 82.81%] [G loss: 3.016374]\n",
      "epoch:42 step:33191 [D loss: 0.284769, acc.: 90.62%] [G loss: 3.220740]\n",
      "epoch:42 step:33192 [D loss: 0.331783, acc.: 90.62%] [G loss: 3.750064]\n",
      "epoch:42 step:33193 [D loss: 0.376487, acc.: 84.38%] [G loss: 3.229434]\n",
      "epoch:42 step:33194 [D loss: 0.294483, acc.: 87.50%] [G loss: 2.856412]\n",
      "epoch:42 step:33195 [D loss: 0.307710, acc.: 88.28%] [G loss: 4.242875]\n",
      "epoch:42 step:33196 [D loss: 0.309453, acc.: 88.28%] [G loss: 2.708861]\n",
      "epoch:42 step:33197 [D loss: 0.207800, acc.: 92.19%] [G loss: 4.473629]\n",
      "epoch:42 step:33198 [D loss: 0.279771, acc.: 89.84%] [G loss: 3.981751]\n",
      "epoch:42 step:33199 [D loss: 0.269943, acc.: 90.62%] [G loss: 5.204310]\n",
      "epoch:42 step:33200 [D loss: 0.290766, acc.: 83.59%] [G loss: 5.694593]\n",
      "##############\n",
      "[0.86880871 0.85590635 0.82521707 0.82376117 0.7937786  0.83257321\n",
      " 0.87165911 0.8232485  0.79167241 0.81200582]\n",
      "##########\n",
      "epoch:42 step:33201 [D loss: 0.410114, acc.: 83.59%] [G loss: 4.263562]\n",
      "epoch:42 step:33202 [D loss: 0.277457, acc.: 89.06%] [G loss: 3.069486]\n",
      "epoch:42 step:33203 [D loss: 0.288655, acc.: 89.06%] [G loss: 3.747356]\n",
      "epoch:42 step:33204 [D loss: 0.279750, acc.: 86.72%] [G loss: 4.658161]\n",
      "epoch:42 step:33205 [D loss: 0.350549, acc.: 82.81%] [G loss: 3.785197]\n",
      "epoch:42 step:33206 [D loss: 0.111025, acc.: 96.88%] [G loss: 5.164042]\n",
      "epoch:42 step:33207 [D loss: 0.303643, acc.: 85.94%] [G loss: 3.444884]\n",
      "epoch:42 step:33208 [D loss: 0.373866, acc.: 82.81%] [G loss: 3.885206]\n",
      "epoch:42 step:33209 [D loss: 0.212118, acc.: 89.84%] [G loss: 4.499997]\n",
      "epoch:42 step:33210 [D loss: 0.262854, acc.: 89.84%] [G loss: 5.350277]\n",
      "epoch:42 step:33211 [D loss: 0.251930, acc.: 87.50%] [G loss: 4.509316]\n",
      "epoch:42 step:33212 [D loss: 0.331371, acc.: 89.06%] [G loss: 3.426743]\n",
      "epoch:42 step:33213 [D loss: 0.350892, acc.: 82.81%] [G loss: 2.968398]\n",
      "epoch:42 step:33214 [D loss: 0.219459, acc.: 92.97%] [G loss: 4.465610]\n",
      "epoch:42 step:33215 [D loss: 0.320492, acc.: 83.59%] [G loss: 5.197442]\n",
      "epoch:42 step:33216 [D loss: 0.450353, acc.: 79.69%] [G loss: 5.126915]\n",
      "epoch:42 step:33217 [D loss: 0.356275, acc.: 86.72%] [G loss: 3.648570]\n",
      "epoch:42 step:33218 [D loss: 0.258435, acc.: 85.16%] [G loss: 4.009192]\n",
      "epoch:42 step:33219 [D loss: 0.376022, acc.: 82.81%] [G loss: 3.076357]\n",
      "epoch:42 step:33220 [D loss: 0.284472, acc.: 88.28%] [G loss: 4.004271]\n",
      "epoch:42 step:33221 [D loss: 0.286889, acc.: 88.28%] [G loss: 3.460728]\n",
      "epoch:42 step:33222 [D loss: 0.230282, acc.: 89.06%] [G loss: 3.843786]\n",
      "epoch:42 step:33223 [D loss: 0.345910, acc.: 84.38%] [G loss: 4.061460]\n",
      "epoch:42 step:33224 [D loss: 0.403031, acc.: 79.69%] [G loss: 3.157700]\n",
      "epoch:42 step:33225 [D loss: 0.291267, acc.: 87.50%] [G loss: 3.447686]\n",
      "epoch:42 step:33226 [D loss: 0.335274, acc.: 85.16%] [G loss: 3.806746]\n",
      "epoch:42 step:33227 [D loss: 0.274811, acc.: 87.50%] [G loss: 3.153483]\n",
      "epoch:42 step:33228 [D loss: 0.273710, acc.: 89.06%] [G loss: 3.179989]\n",
      "epoch:42 step:33229 [D loss: 0.317248, acc.: 85.94%] [G loss: 3.321168]\n",
      "epoch:42 step:33230 [D loss: 0.250704, acc.: 85.94%] [G loss: 4.762153]\n",
      "epoch:42 step:33231 [D loss: 0.424004, acc.: 78.12%] [G loss: 2.825019]\n",
      "epoch:42 step:33232 [D loss: 0.374286, acc.: 82.81%] [G loss: 3.023725]\n",
      "epoch:42 step:33233 [D loss: 0.314128, acc.: 86.72%] [G loss: 2.578991]\n",
      "epoch:42 step:33234 [D loss: 0.368663, acc.: 84.38%] [G loss: 2.301340]\n",
      "epoch:42 step:33235 [D loss: 0.263380, acc.: 91.41%] [G loss: 2.144934]\n",
      "epoch:42 step:33236 [D loss: 0.288252, acc.: 87.50%] [G loss: 3.780800]\n",
      "epoch:42 step:33237 [D loss: 0.498222, acc.: 80.47%] [G loss: 7.729443]\n",
      "epoch:42 step:33238 [D loss: 1.318162, acc.: 59.38%] [G loss: 10.514819]\n",
      "epoch:42 step:33239 [D loss: 2.958368, acc.: 65.62%] [G loss: 9.454595]\n",
      "epoch:42 step:33240 [D loss: 4.063214, acc.: 54.69%] [G loss: 4.314120]\n",
      "epoch:42 step:33241 [D loss: 0.474338, acc.: 82.03%] [G loss: 7.895967]\n",
      "epoch:42 step:33242 [D loss: 0.860383, acc.: 72.66%] [G loss: 4.175960]\n",
      "epoch:42 step:33243 [D loss: 0.447295, acc.: 82.81%] [G loss: 4.481951]\n",
      "epoch:42 step:33244 [D loss: 0.420080, acc.: 82.81%] [G loss: 3.720063]\n",
      "epoch:42 step:33245 [D loss: 0.339963, acc.: 87.50%] [G loss: 3.200282]\n",
      "epoch:42 step:33246 [D loss: 0.420317, acc.: 81.25%] [G loss: 3.127566]\n",
      "epoch:42 step:33247 [D loss: 0.225196, acc.: 91.41%] [G loss: 3.105950]\n",
      "epoch:42 step:33248 [D loss: 0.344112, acc.: 82.81%] [G loss: 2.816520]\n",
      "epoch:42 step:33249 [D loss: 0.264408, acc.: 89.84%] [G loss: 2.337263]\n",
      "epoch:42 step:33250 [D loss: 0.351185, acc.: 83.59%] [G loss: 3.104450]\n",
      "epoch:42 step:33251 [D loss: 0.300586, acc.: 85.16%] [G loss: 3.207426]\n",
      "epoch:42 step:33252 [D loss: 0.338370, acc.: 82.03%] [G loss: 2.773255]\n",
      "epoch:42 step:33253 [D loss: 0.247904, acc.: 89.84%] [G loss: 3.790615]\n",
      "epoch:42 step:33254 [D loss: 0.292484, acc.: 89.84%] [G loss: 2.763514]\n",
      "epoch:42 step:33255 [D loss: 0.240083, acc.: 92.19%] [G loss: 2.492207]\n",
      "epoch:42 step:33256 [D loss: 0.294989, acc.: 86.72%] [G loss: 2.834287]\n",
      "epoch:42 step:33257 [D loss: 0.232136, acc.: 92.19%] [G loss: 3.543301]\n",
      "epoch:42 step:33258 [D loss: 0.265752, acc.: 89.84%] [G loss: 2.485644]\n",
      "epoch:42 step:33259 [D loss: 0.360886, acc.: 83.59%] [G loss: 2.781108]\n",
      "epoch:42 step:33260 [D loss: 0.362899, acc.: 79.69%] [G loss: 3.227702]\n",
      "epoch:42 step:33261 [D loss: 0.328153, acc.: 86.72%] [G loss: 3.179110]\n",
      "epoch:42 step:33262 [D loss: 0.354663, acc.: 89.06%] [G loss: 3.173018]\n",
      "epoch:42 step:33263 [D loss: 0.371841, acc.: 85.16%] [G loss: 2.714815]\n",
      "epoch:42 step:33264 [D loss: 0.331249, acc.: 86.72%] [G loss: 3.158936]\n",
      "epoch:42 step:33265 [D loss: 0.276092, acc.: 86.72%] [G loss: 3.140366]\n",
      "epoch:42 step:33266 [D loss: 0.306799, acc.: 89.84%] [G loss: 2.760355]\n",
      "epoch:42 step:33267 [D loss: 0.313237, acc.: 86.72%] [G loss: 2.836602]\n",
      "epoch:42 step:33268 [D loss: 0.290853, acc.: 89.06%] [G loss: 2.890714]\n",
      "epoch:42 step:33269 [D loss: 0.353086, acc.: 82.81%] [G loss: 3.281974]\n",
      "epoch:42 step:33270 [D loss: 0.374813, acc.: 79.69%] [G loss: 2.625508]\n",
      "epoch:42 step:33271 [D loss: 0.277276, acc.: 87.50%] [G loss: 4.922984]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:42 step:33272 [D loss: 0.358449, acc.: 86.72%] [G loss: 3.007797]\n",
      "epoch:42 step:33273 [D loss: 0.307941, acc.: 83.59%] [G loss: 2.451757]\n",
      "epoch:42 step:33274 [D loss: 0.233838, acc.: 90.62%] [G loss: 3.323826]\n",
      "epoch:42 step:33275 [D loss: 0.252935, acc.: 92.19%] [G loss: 2.923883]\n",
      "epoch:42 step:33276 [D loss: 0.290935, acc.: 83.59%] [G loss: 3.514605]\n",
      "epoch:42 step:33277 [D loss: 0.326213, acc.: 84.38%] [G loss: 3.710671]\n",
      "epoch:42 step:33278 [D loss: 0.315476, acc.: 87.50%] [G loss: 3.146410]\n",
      "epoch:42 step:33279 [D loss: 0.296800, acc.: 88.28%] [G loss: 2.867715]\n",
      "epoch:42 step:33280 [D loss: 0.355903, acc.: 84.38%] [G loss: 3.057384]\n",
      "epoch:42 step:33281 [D loss: 0.505730, acc.: 76.56%] [G loss: 3.675078]\n",
      "epoch:42 step:33282 [D loss: 0.302669, acc.: 85.94%] [G loss: 3.183042]\n",
      "epoch:42 step:33283 [D loss: 0.411640, acc.: 79.69%] [G loss: 4.570567]\n",
      "epoch:42 step:33284 [D loss: 0.345160, acc.: 84.38%] [G loss: 4.182018]\n",
      "epoch:42 step:33285 [D loss: 0.363025, acc.: 82.81%] [G loss: 2.852039]\n",
      "epoch:42 step:33286 [D loss: 0.261308, acc.: 87.50%] [G loss: 2.238227]\n",
      "epoch:42 step:33287 [D loss: 0.315481, acc.: 85.94%] [G loss: 3.036032]\n",
      "epoch:42 step:33288 [D loss: 0.221844, acc.: 89.84%] [G loss: 3.003001]\n",
      "epoch:42 step:33289 [D loss: 0.301868, acc.: 88.28%] [G loss: 3.982158]\n",
      "epoch:42 step:33290 [D loss: 0.299543, acc.: 86.72%] [G loss: 2.817919]\n",
      "epoch:42 step:33291 [D loss: 0.299954, acc.: 85.94%] [G loss: 3.133689]\n",
      "epoch:42 step:33292 [D loss: 0.334342, acc.: 85.94%] [G loss: 3.601903]\n",
      "epoch:42 step:33293 [D loss: 0.395467, acc.: 83.59%] [G loss: 2.980831]\n",
      "epoch:42 step:33294 [D loss: 0.268961, acc.: 86.72%] [G loss: 4.164133]\n",
      "epoch:42 step:33295 [D loss: 0.165482, acc.: 94.53%] [G loss: 4.198290]\n",
      "epoch:42 step:33296 [D loss: 0.248425, acc.: 88.28%] [G loss: 3.680358]\n",
      "epoch:42 step:33297 [D loss: 0.364660, acc.: 82.03%] [G loss: 4.521160]\n",
      "epoch:42 step:33298 [D loss: 0.373232, acc.: 82.81%] [G loss: 2.878357]\n",
      "epoch:42 step:33299 [D loss: 0.326677, acc.: 83.59%] [G loss: 3.920040]\n",
      "epoch:42 step:33300 [D loss: 0.214478, acc.: 90.62%] [G loss: 4.879834]\n",
      "epoch:42 step:33301 [D loss: 0.269889, acc.: 89.06%] [G loss: 3.741011]\n",
      "epoch:42 step:33302 [D loss: 0.264793, acc.: 89.84%] [G loss: 3.706001]\n",
      "epoch:42 step:33303 [D loss: 0.284848, acc.: 85.94%] [G loss: 2.880207]\n",
      "epoch:42 step:33304 [D loss: 0.371668, acc.: 82.03%] [G loss: 3.092638]\n",
      "epoch:42 step:33305 [D loss: 0.250981, acc.: 88.28%] [G loss: 2.850049]\n",
      "epoch:42 step:33306 [D loss: 0.290636, acc.: 85.94%] [G loss: 2.732867]\n",
      "epoch:42 step:33307 [D loss: 0.302632, acc.: 85.94%] [G loss: 2.810416]\n",
      "epoch:42 step:33308 [D loss: 0.268365, acc.: 89.06%] [G loss: 3.398191]\n",
      "epoch:42 step:33309 [D loss: 0.351252, acc.: 83.59%] [G loss: 2.707134]\n",
      "epoch:42 step:33310 [D loss: 0.280308, acc.: 86.72%] [G loss: 2.971081]\n",
      "epoch:42 step:33311 [D loss: 0.345123, acc.: 82.81%] [G loss: 3.562205]\n",
      "epoch:42 step:33312 [D loss: 0.281176, acc.: 90.62%] [G loss: 3.123456]\n",
      "epoch:42 step:33313 [D loss: 0.266973, acc.: 87.50%] [G loss: 2.583745]\n",
      "epoch:42 step:33314 [D loss: 0.207069, acc.: 92.97%] [G loss: 3.005228]\n",
      "epoch:42 step:33315 [D loss: 0.246604, acc.: 89.84%] [G loss: 3.102596]\n",
      "epoch:42 step:33316 [D loss: 0.251792, acc.: 92.19%] [G loss: 4.007087]\n",
      "epoch:42 step:33317 [D loss: 0.380646, acc.: 85.16%] [G loss: 3.766170]\n",
      "epoch:42 step:33318 [D loss: 0.341128, acc.: 90.62%] [G loss: 3.501189]\n",
      "epoch:42 step:33319 [D loss: 0.314196, acc.: 85.94%] [G loss: 3.116276]\n",
      "epoch:42 step:33320 [D loss: 0.272863, acc.: 85.94%] [G loss: 3.644481]\n",
      "epoch:42 step:33321 [D loss: 0.335176, acc.: 84.38%] [G loss: 2.949572]\n",
      "epoch:42 step:33322 [D loss: 0.402981, acc.: 82.03%] [G loss: 3.452188]\n",
      "epoch:42 step:33323 [D loss: 0.275347, acc.: 91.41%] [G loss: 3.012938]\n",
      "epoch:42 step:33324 [D loss: 0.340754, acc.: 83.59%] [G loss: 3.946116]\n",
      "epoch:42 step:33325 [D loss: 0.282979, acc.: 88.28%] [G loss: 2.810557]\n",
      "epoch:42 step:33326 [D loss: 0.336215, acc.: 83.59%] [G loss: 2.794183]\n",
      "epoch:42 step:33327 [D loss: 0.244557, acc.: 90.62%] [G loss: 2.862981]\n",
      "epoch:42 step:33328 [D loss: 0.468689, acc.: 78.91%] [G loss: 2.381926]\n",
      "epoch:42 step:33329 [D loss: 0.314200, acc.: 86.72%] [G loss: 2.701691]\n",
      "epoch:42 step:33330 [D loss: 0.260006, acc.: 89.84%] [G loss: 3.975839]\n",
      "epoch:42 step:33331 [D loss: 0.318728, acc.: 87.50%] [G loss: 3.093512]\n",
      "epoch:42 step:33332 [D loss: 0.294789, acc.: 85.94%] [G loss: 2.853451]\n",
      "epoch:42 step:33333 [D loss: 0.322503, acc.: 85.16%] [G loss: 3.104226]\n",
      "epoch:42 step:33334 [D loss: 0.246755, acc.: 92.19%] [G loss: 2.929499]\n",
      "epoch:42 step:33335 [D loss: 0.261911, acc.: 89.84%] [G loss: 3.265657]\n",
      "epoch:42 step:33336 [D loss: 0.228281, acc.: 90.62%] [G loss: 3.321337]\n",
      "epoch:42 step:33337 [D loss: 0.304021, acc.: 85.94%] [G loss: 3.139599]\n",
      "epoch:42 step:33338 [D loss: 0.332022, acc.: 84.38%] [G loss: 3.427214]\n",
      "epoch:42 step:33339 [D loss: 0.453681, acc.: 75.00%] [G loss: 2.555893]\n",
      "epoch:42 step:33340 [D loss: 0.317598, acc.: 83.59%] [G loss: 2.943183]\n",
      "epoch:42 step:33341 [D loss: 0.329509, acc.: 85.16%] [G loss: 2.291718]\n",
      "epoch:42 step:33342 [D loss: 0.256328, acc.: 89.06%] [G loss: 2.475667]\n",
      "epoch:42 step:33343 [D loss: 0.261982, acc.: 92.19%] [G loss: 2.819715]\n",
      "epoch:42 step:33344 [D loss: 0.320642, acc.: 87.50%] [G loss: 2.835015]\n",
      "epoch:42 step:33345 [D loss: 0.269772, acc.: 87.50%] [G loss: 2.838498]\n",
      "epoch:42 step:33346 [D loss: 0.294787, acc.: 88.28%] [G loss: 3.386791]\n",
      "epoch:42 step:33347 [D loss: 0.330058, acc.: 85.16%] [G loss: 2.759526]\n",
      "epoch:42 step:33348 [D loss: 0.283354, acc.: 88.28%] [G loss: 2.526832]\n",
      "epoch:42 step:33349 [D loss: 0.328905, acc.: 82.81%] [G loss: 2.750632]\n",
      "epoch:42 step:33350 [D loss: 0.232109, acc.: 90.62%] [G loss: 3.449153]\n",
      "epoch:42 step:33351 [D loss: 0.310469, acc.: 87.50%] [G loss: 3.506799]\n",
      "epoch:42 step:33352 [D loss: 0.253357, acc.: 89.84%] [G loss: 2.552514]\n",
      "epoch:42 step:33353 [D loss: 0.257334, acc.: 90.62%] [G loss: 3.433119]\n",
      "epoch:42 step:33354 [D loss: 0.371923, acc.: 82.81%] [G loss: 3.216840]\n",
      "epoch:42 step:33355 [D loss: 0.290563, acc.: 89.84%] [G loss: 2.795291]\n",
      "epoch:42 step:33356 [D loss: 0.320728, acc.: 85.16%] [G loss: 3.471033]\n",
      "epoch:42 step:33357 [D loss: 0.250853, acc.: 92.19%] [G loss: 3.749249]\n",
      "epoch:42 step:33358 [D loss: 0.335381, acc.: 83.59%] [G loss: 3.640837]\n",
      "epoch:42 step:33359 [D loss: 0.305475, acc.: 88.28%] [G loss: 3.524686]\n",
      "epoch:42 step:33360 [D loss: 0.237334, acc.: 89.06%] [G loss: 2.520063]\n",
      "epoch:42 step:33361 [D loss: 0.267338, acc.: 85.94%] [G loss: 2.638867]\n",
      "epoch:42 step:33362 [D loss: 0.334944, acc.: 85.16%] [G loss: 3.344925]\n",
      "epoch:42 step:33363 [D loss: 0.335371, acc.: 83.59%] [G loss: 2.606975]\n",
      "epoch:42 step:33364 [D loss: 0.261977, acc.: 91.41%] [G loss: 2.936187]\n",
      "epoch:42 step:33365 [D loss: 0.390310, acc.: 84.38%] [G loss: 3.565523]\n",
      "epoch:42 step:33366 [D loss: 0.340281, acc.: 82.03%] [G loss: 3.321470]\n",
      "epoch:42 step:33367 [D loss: 0.325931, acc.: 83.59%] [G loss: 2.965888]\n",
      "epoch:42 step:33368 [D loss: 0.316814, acc.: 88.28%] [G loss: 2.866217]\n",
      "epoch:42 step:33369 [D loss: 0.291941, acc.: 85.16%] [G loss: 2.846973]\n",
      "epoch:42 step:33370 [D loss: 0.382472, acc.: 82.81%] [G loss: 4.039877]\n",
      "epoch:42 step:33371 [D loss: 0.356354, acc.: 85.16%] [G loss: 3.772234]\n",
      "epoch:42 step:33372 [D loss: 0.309339, acc.: 85.94%] [G loss: 4.588632]\n",
      "epoch:42 step:33373 [D loss: 0.270097, acc.: 86.72%] [G loss: 3.369578]\n",
      "epoch:42 step:33374 [D loss: 0.281718, acc.: 87.50%] [G loss: 3.361445]\n",
      "epoch:42 step:33375 [D loss: 0.288041, acc.: 87.50%] [G loss: 4.610393]\n",
      "epoch:42 step:33376 [D loss: 0.263378, acc.: 88.28%] [G loss: 3.699844]\n",
      "epoch:42 step:33377 [D loss: 0.236114, acc.: 89.06%] [G loss: 3.524169]\n",
      "epoch:42 step:33378 [D loss: 0.341133, acc.: 82.81%] [G loss: 2.913109]\n",
      "epoch:42 step:33379 [D loss: 0.208211, acc.: 91.41%] [G loss: 2.686827]\n",
      "epoch:42 step:33380 [D loss: 0.308759, acc.: 85.94%] [G loss: 3.441702]\n",
      "epoch:42 step:33381 [D loss: 0.233525, acc.: 90.62%] [G loss: 3.300252]\n",
      "epoch:42 step:33382 [D loss: 0.263035, acc.: 88.28%] [G loss: 4.879330]\n",
      "epoch:42 step:33383 [D loss: 0.232072, acc.: 89.06%] [G loss: 3.100068]\n",
      "epoch:42 step:33384 [D loss: 0.235329, acc.: 89.84%] [G loss: 4.888085]\n",
      "epoch:42 step:33385 [D loss: 0.397024, acc.: 83.59%] [G loss: 4.124664]\n",
      "epoch:42 step:33386 [D loss: 0.366631, acc.: 82.81%] [G loss: 4.035805]\n",
      "epoch:42 step:33387 [D loss: 0.310087, acc.: 88.28%] [G loss: 4.892728]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:42 step:33388 [D loss: 0.287742, acc.: 87.50%] [G loss: 6.535524]\n",
      "epoch:42 step:33389 [D loss: 0.266053, acc.: 88.28%] [G loss: 6.540883]\n",
      "epoch:42 step:33390 [D loss: 0.292450, acc.: 88.28%] [G loss: 3.236661]\n",
      "epoch:42 step:33391 [D loss: 0.330406, acc.: 82.03%] [G loss: 3.761346]\n",
      "epoch:42 step:33392 [D loss: 0.269405, acc.: 87.50%] [G loss: 4.616575]\n",
      "epoch:42 step:33393 [D loss: 0.265574, acc.: 86.72%] [G loss: 3.336321]\n",
      "epoch:42 step:33394 [D loss: 0.418966, acc.: 81.25%] [G loss: 4.043895]\n",
      "epoch:42 step:33395 [D loss: 0.244456, acc.: 89.84%] [G loss: 4.037677]\n",
      "epoch:42 step:33396 [D loss: 0.376557, acc.: 84.38%] [G loss: 2.943352]\n",
      "epoch:42 step:33397 [D loss: 0.385839, acc.: 82.03%] [G loss: 2.587259]\n",
      "epoch:42 step:33398 [D loss: 0.270811, acc.: 89.06%] [G loss: 3.309412]\n",
      "epoch:42 step:33399 [D loss: 0.290468, acc.: 86.72%] [G loss: 2.746585]\n",
      "epoch:42 step:33400 [D loss: 0.322069, acc.: 85.94%] [G loss: 3.480461]\n",
      "##############\n",
      "[0.87573781 0.85637224 0.80728167 0.82340097 0.7637625  0.8392966\n",
      " 0.8703587  0.86457275 0.82609698 0.81321263]\n",
      "##########\n",
      "epoch:42 step:33401 [D loss: 0.350400, acc.: 88.28%] [G loss: 2.391072]\n",
      "epoch:42 step:33402 [D loss: 0.211865, acc.: 91.41%] [G loss: 3.528542]\n",
      "epoch:42 step:33403 [D loss: 0.414081, acc.: 82.03%] [G loss: 3.108210]\n",
      "epoch:42 step:33404 [D loss: 0.265893, acc.: 89.84%] [G loss: 2.216711]\n",
      "epoch:42 step:33405 [D loss: 0.297065, acc.: 88.28%] [G loss: 2.748673]\n",
      "epoch:42 step:33406 [D loss: 0.321938, acc.: 88.28%] [G loss: 3.053443]\n",
      "epoch:42 step:33407 [D loss: 0.348260, acc.: 80.47%] [G loss: 2.600547]\n",
      "epoch:42 step:33408 [D loss: 0.346750, acc.: 85.94%] [G loss: 3.245878]\n",
      "epoch:42 step:33409 [D loss: 0.369381, acc.: 84.38%] [G loss: 3.675038]\n",
      "epoch:42 step:33410 [D loss: 0.261533, acc.: 89.84%] [G loss: 3.455742]\n",
      "epoch:42 step:33411 [D loss: 0.451574, acc.: 75.00%] [G loss: 7.533573]\n",
      "epoch:42 step:33412 [D loss: 0.699067, acc.: 75.78%] [G loss: 5.508193]\n",
      "epoch:42 step:33413 [D loss: 0.768942, acc.: 75.78%] [G loss: 5.068410]\n",
      "epoch:42 step:33414 [D loss: 0.701725, acc.: 69.53%] [G loss: 3.522531]\n",
      "epoch:42 step:33415 [D loss: 0.377368, acc.: 85.94%] [G loss: 4.526258]\n",
      "epoch:42 step:33416 [D loss: 0.378275, acc.: 85.94%] [G loss: 2.580063]\n",
      "epoch:42 step:33417 [D loss: 0.304832, acc.: 87.50%] [G loss: 5.386160]\n",
      "epoch:42 step:33418 [D loss: 0.140290, acc.: 96.09%] [G loss: 4.247203]\n",
      "epoch:42 step:33419 [D loss: 0.326955, acc.: 82.03%] [G loss: 3.130587]\n",
      "epoch:42 step:33420 [D loss: 0.402760, acc.: 85.16%] [G loss: 2.853942]\n",
      "epoch:42 step:33421 [D loss: 0.389854, acc.: 80.47%] [G loss: 3.554597]\n",
      "epoch:42 step:33422 [D loss: 0.264940, acc.: 89.06%] [G loss: 3.284205]\n",
      "epoch:42 step:33423 [D loss: 0.233973, acc.: 90.62%] [G loss: 3.502299]\n",
      "epoch:42 step:33424 [D loss: 0.252758, acc.: 91.41%] [G loss: 4.403717]\n",
      "epoch:42 step:33425 [D loss: 0.458002, acc.: 82.81%] [G loss: 2.977853]\n",
      "epoch:42 step:33426 [D loss: 0.323973, acc.: 84.38%] [G loss: 4.153486]\n",
      "epoch:42 step:33427 [D loss: 0.292683, acc.: 89.06%] [G loss: 3.246299]\n",
      "epoch:42 step:33428 [D loss: 0.309371, acc.: 83.59%] [G loss: 3.226300]\n",
      "epoch:42 step:33429 [D loss: 0.374469, acc.: 82.03%] [G loss: 3.885774]\n",
      "epoch:42 step:33430 [D loss: 0.364245, acc.: 86.72%] [G loss: 3.677475]\n",
      "epoch:42 step:33431 [D loss: 0.488658, acc.: 81.25%] [G loss: 3.221447]\n",
      "epoch:42 step:33432 [D loss: 0.290275, acc.: 85.94%] [G loss: 2.945304]\n",
      "epoch:42 step:33433 [D loss: 0.434690, acc.: 83.59%] [G loss: 4.167951]\n",
      "epoch:42 step:33434 [D loss: 0.383101, acc.: 79.69%] [G loss: 2.902468]\n",
      "epoch:42 step:33435 [D loss: 0.352653, acc.: 82.81%] [G loss: 4.753706]\n",
      "epoch:42 step:33436 [D loss: 0.376077, acc.: 78.91%] [G loss: 3.673210]\n",
      "epoch:42 step:33437 [D loss: 0.322994, acc.: 85.94%] [G loss: 3.138627]\n",
      "epoch:42 step:33438 [D loss: 0.222400, acc.: 92.19%] [G loss: 2.668801]\n",
      "epoch:42 step:33439 [D loss: 0.355997, acc.: 84.38%] [G loss: 3.272194]\n",
      "epoch:42 step:33440 [D loss: 0.269027, acc.: 85.94%] [G loss: 2.739760]\n",
      "epoch:42 step:33441 [D loss: 0.371060, acc.: 82.81%] [G loss: 2.991232]\n",
      "epoch:42 step:33442 [D loss: 0.272783, acc.: 89.06%] [G loss: 2.523187]\n",
      "epoch:42 step:33443 [D loss: 0.277674, acc.: 85.94%] [G loss: 2.686043]\n",
      "epoch:42 step:33444 [D loss: 0.273416, acc.: 86.72%] [G loss: 2.470193]\n",
      "epoch:42 step:33445 [D loss: 0.324665, acc.: 85.94%] [G loss: 2.753685]\n",
      "epoch:42 step:33446 [D loss: 0.263535, acc.: 89.06%] [G loss: 3.008215]\n",
      "epoch:42 step:33447 [D loss: 0.230466, acc.: 89.84%] [G loss: 2.817543]\n",
      "epoch:42 step:33448 [D loss: 0.407059, acc.: 80.47%] [G loss: 2.956893]\n",
      "epoch:42 step:33449 [D loss: 0.337376, acc.: 85.94%] [G loss: 2.590312]\n",
      "epoch:42 step:33450 [D loss: 0.431558, acc.: 78.12%] [G loss: 2.923204]\n",
      "epoch:42 step:33451 [D loss: 0.306769, acc.: 86.72%] [G loss: 2.949372]\n",
      "epoch:42 step:33452 [D loss: 0.358188, acc.: 83.59%] [G loss: 2.776410]\n",
      "epoch:42 step:33453 [D loss: 0.340681, acc.: 86.72%] [G loss: 2.660990]\n",
      "epoch:42 step:33454 [D loss: 0.287362, acc.: 89.06%] [G loss: 3.131863]\n",
      "epoch:42 step:33455 [D loss: 0.286245, acc.: 86.72%] [G loss: 2.874518]\n",
      "epoch:42 step:33456 [D loss: 0.314075, acc.: 88.28%] [G loss: 2.965059]\n",
      "epoch:42 step:33457 [D loss: 0.378780, acc.: 84.38%] [G loss: 3.230688]\n",
      "epoch:42 step:33458 [D loss: 0.219199, acc.: 89.06%] [G loss: 3.248935]\n",
      "epoch:42 step:33459 [D loss: 0.360612, acc.: 84.38%] [G loss: 2.730196]\n",
      "epoch:42 step:33460 [D loss: 0.289290, acc.: 89.06%] [G loss: 3.218155]\n",
      "epoch:42 step:33461 [D loss: 0.266556, acc.: 89.06%] [G loss: 3.747515]\n",
      "epoch:42 step:33462 [D loss: 0.248225, acc.: 90.62%] [G loss: 3.019740]\n",
      "epoch:42 step:33463 [D loss: 0.301903, acc.: 85.94%] [G loss: 3.552868]\n",
      "epoch:42 step:33464 [D loss: 0.304721, acc.: 85.16%] [G loss: 2.596788]\n",
      "epoch:42 step:33465 [D loss: 0.293181, acc.: 85.94%] [G loss: 2.883707]\n",
      "epoch:42 step:33466 [D loss: 0.312344, acc.: 82.03%] [G loss: 2.668678]\n",
      "epoch:42 step:33467 [D loss: 0.252185, acc.: 89.06%] [G loss: 2.739573]\n",
      "epoch:42 step:33468 [D loss: 0.290609, acc.: 89.84%] [G loss: 4.300237]\n",
      "epoch:42 step:33469 [D loss: 0.260243, acc.: 88.28%] [G loss: 3.414464]\n",
      "epoch:42 step:33470 [D loss: 0.300585, acc.: 85.16%] [G loss: 2.963192]\n",
      "epoch:42 step:33471 [D loss: 0.364154, acc.: 78.91%] [G loss: 3.302671]\n",
      "epoch:42 step:33472 [D loss: 0.340652, acc.: 85.16%] [G loss: 2.951408]\n",
      "epoch:42 step:33473 [D loss: 0.323944, acc.: 87.50%] [G loss: 3.395048]\n",
      "epoch:42 step:33474 [D loss: 0.263822, acc.: 88.28%] [G loss: 3.160160]\n",
      "epoch:42 step:33475 [D loss: 0.226033, acc.: 89.06%] [G loss: 3.113386]\n",
      "epoch:42 step:33476 [D loss: 0.300088, acc.: 92.19%] [G loss: 3.334468]\n",
      "epoch:42 step:33477 [D loss: 0.308369, acc.: 87.50%] [G loss: 3.816631]\n",
      "epoch:42 step:33478 [D loss: 0.258829, acc.: 87.50%] [G loss: 2.561973]\n",
      "epoch:42 step:33479 [D loss: 0.273553, acc.: 88.28%] [G loss: 4.261307]\n",
      "epoch:42 step:33480 [D loss: 0.223964, acc.: 93.75%] [G loss: 3.729703]\n",
      "epoch:42 step:33481 [D loss: 0.314885, acc.: 87.50%] [G loss: 2.338527]\n",
      "epoch:42 step:33482 [D loss: 0.284929, acc.: 89.84%] [G loss: 3.174738]\n",
      "epoch:42 step:33483 [D loss: 0.251873, acc.: 88.28%] [G loss: 3.283169]\n",
      "epoch:42 step:33484 [D loss: 0.395907, acc.: 80.47%] [G loss: 2.971818]\n",
      "epoch:42 step:33485 [D loss: 0.268545, acc.: 90.62%] [G loss: 2.846038]\n",
      "epoch:42 step:33486 [D loss: 0.292973, acc.: 85.16%] [G loss: 3.336829]\n",
      "epoch:42 step:33487 [D loss: 0.340929, acc.: 84.38%] [G loss: 3.789444]\n",
      "epoch:42 step:33488 [D loss: 0.386305, acc.: 82.03%] [G loss: 2.347927]\n",
      "epoch:42 step:33489 [D loss: 0.216227, acc.: 89.06%] [G loss: 3.860378]\n",
      "epoch:42 step:33490 [D loss: 0.288780, acc.: 88.28%] [G loss: 2.552687]\n",
      "epoch:42 step:33491 [D loss: 0.305128, acc.: 88.28%] [G loss: 3.554007]\n",
      "epoch:42 step:33492 [D loss: 0.274091, acc.: 90.62%] [G loss: 2.732676]\n",
      "epoch:42 step:33493 [D loss: 0.339356, acc.: 86.72%] [G loss: 3.698621]\n",
      "epoch:42 step:33494 [D loss: 0.407672, acc.: 82.03%] [G loss: 3.253181]\n",
      "epoch:42 step:33495 [D loss: 0.302916, acc.: 85.94%] [G loss: 3.017674]\n",
      "epoch:42 step:33496 [D loss: 0.303957, acc.: 85.16%] [G loss: 3.424191]\n",
      "epoch:42 step:33497 [D loss: 0.220192, acc.: 92.19%] [G loss: 3.081671]\n",
      "epoch:42 step:33498 [D loss: 0.260322, acc.: 90.62%] [G loss: 2.774145]\n",
      "epoch:42 step:33499 [D loss: 0.280631, acc.: 89.06%] [G loss: 4.757063]\n",
      "epoch:42 step:33500 [D loss: 0.284743, acc.: 86.72%] [G loss: 3.076652]\n",
      "epoch:42 step:33501 [D loss: 0.232586, acc.: 89.84%] [G loss: 4.136129]\n",
      "epoch:42 step:33502 [D loss: 0.336105, acc.: 83.59%] [G loss: 4.077822]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:42 step:33503 [D loss: 0.247810, acc.: 86.72%] [G loss: 3.293497]\n",
      "epoch:42 step:33504 [D loss: 0.153507, acc.: 94.53%] [G loss: 4.619076]\n",
      "epoch:42 step:33505 [D loss: 0.241227, acc.: 90.62%] [G loss: 4.691590]\n",
      "epoch:42 step:33506 [D loss: 0.241618, acc.: 84.38%] [G loss: 6.149053]\n",
      "epoch:42 step:33507 [D loss: 0.292196, acc.: 88.28%] [G loss: 6.055730]\n",
      "epoch:42 step:33508 [D loss: 0.238471, acc.: 89.06%] [G loss: 3.871239]\n",
      "epoch:42 step:33509 [D loss: 0.271663, acc.: 85.16%] [G loss: 3.154120]\n",
      "epoch:42 step:33510 [D loss: 0.359505, acc.: 82.81%] [G loss: 3.392850]\n",
      "epoch:42 step:33511 [D loss: 0.286286, acc.: 89.06%] [G loss: 3.228465]\n",
      "epoch:42 step:33512 [D loss: 0.288563, acc.: 89.06%] [G loss: 2.940810]\n",
      "epoch:42 step:33513 [D loss: 0.297891, acc.: 85.16%] [G loss: 3.158820]\n",
      "epoch:42 step:33514 [D loss: 0.351462, acc.: 83.59%] [G loss: 3.448309]\n",
      "epoch:42 step:33515 [D loss: 0.374166, acc.: 86.72%] [G loss: 2.607280]\n",
      "epoch:42 step:33516 [D loss: 0.293158, acc.: 89.06%] [G loss: 2.835539]\n",
      "epoch:42 step:33517 [D loss: 0.338596, acc.: 83.59%] [G loss: 3.405538]\n",
      "epoch:42 step:33518 [D loss: 0.321862, acc.: 85.16%] [G loss: 4.963401]\n",
      "epoch:42 step:33519 [D loss: 0.249238, acc.: 89.06%] [G loss: 3.811582]\n",
      "epoch:42 step:33520 [D loss: 0.276504, acc.: 89.84%] [G loss: 3.124724]\n",
      "epoch:42 step:33521 [D loss: 0.215438, acc.: 89.06%] [G loss: 3.910909]\n",
      "epoch:42 step:33522 [D loss: 0.324005, acc.: 85.16%] [G loss: 3.553125]\n",
      "epoch:42 step:33523 [D loss: 0.338747, acc.: 84.38%] [G loss: 4.918689]\n",
      "epoch:42 step:33524 [D loss: 0.314832, acc.: 85.16%] [G loss: 4.317936]\n",
      "epoch:42 step:33525 [D loss: 0.305238, acc.: 85.94%] [G loss: 4.452498]\n",
      "epoch:42 step:33526 [D loss: 0.244255, acc.: 89.06%] [G loss: 4.800267]\n",
      "epoch:42 step:33527 [D loss: 0.245197, acc.: 87.50%] [G loss: 3.795363]\n",
      "epoch:42 step:33528 [D loss: 0.322181, acc.: 82.81%] [G loss: 3.837997]\n",
      "epoch:42 step:33529 [D loss: 0.350197, acc.: 85.94%] [G loss: 3.707367]\n",
      "epoch:42 step:33530 [D loss: 0.406482, acc.: 80.47%] [G loss: 2.205076]\n",
      "epoch:42 step:33531 [D loss: 0.430251, acc.: 79.69%] [G loss: 3.970653]\n",
      "epoch:42 step:33532 [D loss: 0.350951, acc.: 85.16%] [G loss: 3.922385]\n",
      "epoch:42 step:33533 [D loss: 0.281918, acc.: 87.50%] [G loss: 4.509246]\n",
      "epoch:42 step:33534 [D loss: 0.334571, acc.: 82.81%] [G loss: 7.444238]\n",
      "epoch:42 step:33535 [D loss: 0.429115, acc.: 81.25%] [G loss: 4.930776]\n",
      "epoch:42 step:33536 [D loss: 0.420983, acc.: 82.03%] [G loss: 3.201897]\n",
      "epoch:42 step:33537 [D loss: 0.261806, acc.: 89.84%] [G loss: 3.977809]\n",
      "epoch:42 step:33538 [D loss: 0.278711, acc.: 83.59%] [G loss: 3.906458]\n",
      "epoch:42 step:33539 [D loss: 0.321886, acc.: 83.59%] [G loss: 3.298360]\n",
      "epoch:42 step:33540 [D loss: 0.315892, acc.: 86.72%] [G loss: 4.250627]\n",
      "epoch:42 step:33541 [D loss: 0.303781, acc.: 87.50%] [G loss: 5.946942]\n",
      "epoch:42 step:33542 [D loss: 0.205225, acc.: 93.75%] [G loss: 4.535506]\n",
      "epoch:42 step:33543 [D loss: 0.215758, acc.: 90.62%] [G loss: 6.598402]\n",
      "epoch:42 step:33544 [D loss: 0.196975, acc.: 93.75%] [G loss: 4.632137]\n",
      "epoch:42 step:33545 [D loss: 0.243918, acc.: 89.06%] [G loss: 5.152807]\n",
      "epoch:42 step:33546 [D loss: 0.231928, acc.: 89.84%] [G loss: 4.232367]\n",
      "epoch:42 step:33547 [D loss: 0.278080, acc.: 86.72%] [G loss: 3.619511]\n",
      "epoch:42 step:33548 [D loss: 0.294965, acc.: 85.94%] [G loss: 4.189762]\n",
      "epoch:42 step:33549 [D loss: 0.298166, acc.: 87.50%] [G loss: 3.726776]\n",
      "epoch:42 step:33550 [D loss: 0.312204, acc.: 85.94%] [G loss: 3.245262]\n",
      "epoch:42 step:33551 [D loss: 0.333228, acc.: 84.38%] [G loss: 3.869799]\n",
      "epoch:42 step:33552 [D loss: 0.365667, acc.: 86.72%] [G loss: 2.600430]\n",
      "epoch:42 step:33553 [D loss: 0.247300, acc.: 86.72%] [G loss: 2.711135]\n",
      "epoch:42 step:33554 [D loss: 0.285839, acc.: 85.16%] [G loss: 2.997001]\n",
      "epoch:42 step:33555 [D loss: 0.418769, acc.: 80.47%] [G loss: 4.666372]\n",
      "epoch:42 step:33556 [D loss: 0.289077, acc.: 88.28%] [G loss: 4.413657]\n",
      "epoch:42 step:33557 [D loss: 0.215959, acc.: 91.41%] [G loss: 4.646935]\n",
      "epoch:42 step:33558 [D loss: 0.216070, acc.: 91.41%] [G loss: 4.112581]\n",
      "epoch:42 step:33559 [D loss: 0.290205, acc.: 86.72%] [G loss: 2.924492]\n",
      "epoch:42 step:33560 [D loss: 0.353796, acc.: 85.16%] [G loss: 2.719452]\n",
      "epoch:42 step:33561 [D loss: 0.269764, acc.: 89.06%] [G loss: 2.958042]\n",
      "epoch:42 step:33562 [D loss: 0.304605, acc.: 87.50%] [G loss: 3.386806]\n",
      "epoch:42 step:33563 [D loss: 0.280125, acc.: 88.28%] [G loss: 4.783901]\n",
      "epoch:42 step:33564 [D loss: 0.554226, acc.: 70.31%] [G loss: 6.257211]\n",
      "epoch:42 step:33565 [D loss: 0.751912, acc.: 71.88%] [G loss: 4.469294]\n",
      "epoch:42 step:33566 [D loss: 0.459425, acc.: 79.69%] [G loss: 4.375379]\n",
      "epoch:42 step:33567 [D loss: 0.312998, acc.: 84.38%] [G loss: 4.946216]\n",
      "epoch:42 step:33568 [D loss: 0.393327, acc.: 85.94%] [G loss: 4.607363]\n",
      "epoch:42 step:33569 [D loss: 0.284637, acc.: 86.72%] [G loss: 4.015776]\n",
      "epoch:42 step:33570 [D loss: 0.423737, acc.: 82.03%] [G loss: 3.560668]\n",
      "epoch:42 step:33571 [D loss: 0.318953, acc.: 87.50%] [G loss: 3.353014]\n",
      "epoch:42 step:33572 [D loss: 0.235926, acc.: 88.28%] [G loss: 4.254106]\n",
      "epoch:42 step:33573 [D loss: 0.364423, acc.: 82.81%] [G loss: 2.954519]\n",
      "epoch:42 step:33574 [D loss: 0.335432, acc.: 87.50%] [G loss: 3.628678]\n",
      "epoch:42 step:33575 [D loss: 0.408058, acc.: 80.47%] [G loss: 3.170232]\n",
      "epoch:42 step:33576 [D loss: 0.523209, acc.: 73.44%] [G loss: 3.637755]\n",
      "epoch:42 step:33577 [D loss: 0.465075, acc.: 79.69%] [G loss: 4.264625]\n",
      "epoch:42 step:33578 [D loss: 0.644400, acc.: 77.34%] [G loss: 3.761992]\n",
      "epoch:42 step:33579 [D loss: 0.478580, acc.: 78.12%] [G loss: 3.188188]\n",
      "epoch:42 step:33580 [D loss: 0.362355, acc.: 82.03%] [G loss: 3.666782]\n",
      "epoch:42 step:33581 [D loss: 0.371418, acc.: 83.59%] [G loss: 3.762192]\n",
      "epoch:42 step:33582 [D loss: 0.354805, acc.: 82.03%] [G loss: 3.707793]\n",
      "epoch:42 step:33583 [D loss: 0.304590, acc.: 85.94%] [G loss: 3.105496]\n",
      "epoch:43 step:33584 [D loss: 0.310852, acc.: 87.50%] [G loss: 2.976289]\n",
      "epoch:43 step:33585 [D loss: 0.198672, acc.: 92.97%] [G loss: 3.519806]\n",
      "epoch:43 step:33586 [D loss: 0.300357, acc.: 85.16%] [G loss: 3.256172]\n",
      "epoch:43 step:33587 [D loss: 0.334657, acc.: 80.47%] [G loss: 2.566657]\n",
      "epoch:43 step:33588 [D loss: 0.380513, acc.: 82.03%] [G loss: 3.916687]\n",
      "epoch:43 step:33589 [D loss: 0.364791, acc.: 82.03%] [G loss: 2.984482]\n",
      "epoch:43 step:33590 [D loss: 0.295121, acc.: 85.94%] [G loss: 2.843919]\n",
      "epoch:43 step:33591 [D loss: 0.271257, acc.: 89.84%] [G loss: 2.521192]\n",
      "epoch:43 step:33592 [D loss: 0.364655, acc.: 78.91%] [G loss: 3.565392]\n",
      "epoch:43 step:33593 [D loss: 0.310417, acc.: 86.72%] [G loss: 2.967372]\n",
      "epoch:43 step:33594 [D loss: 0.340511, acc.: 87.50%] [G loss: 2.867689]\n",
      "epoch:43 step:33595 [D loss: 0.292232, acc.: 85.16%] [G loss: 3.347878]\n",
      "epoch:43 step:33596 [D loss: 0.257873, acc.: 91.41%] [G loss: 2.937552]\n",
      "epoch:43 step:33597 [D loss: 0.365399, acc.: 79.69%] [G loss: 4.243515]\n",
      "epoch:43 step:33598 [D loss: 0.364090, acc.: 81.25%] [G loss: 2.918251]\n",
      "epoch:43 step:33599 [D loss: 0.270754, acc.: 88.28%] [G loss: 4.118747]\n",
      "epoch:43 step:33600 [D loss: 0.282739, acc.: 87.50%] [G loss: 2.994264]\n",
      "##############\n",
      "[0.85469967 0.83655362 0.79674202 0.82137133 0.77235687 0.83070022\n",
      " 0.88164952 0.83708717 0.80843563 0.84773549]\n",
      "##########\n",
      "epoch:43 step:33601 [D loss: 0.304486, acc.: 85.16%] [G loss: 3.561999]\n",
      "epoch:43 step:33602 [D loss: 0.375032, acc.: 83.59%] [G loss: 2.845077]\n",
      "epoch:43 step:33603 [D loss: 0.347381, acc.: 85.94%] [G loss: 2.512821]\n",
      "epoch:43 step:33604 [D loss: 0.289097, acc.: 85.94%] [G loss: 2.966764]\n",
      "epoch:43 step:33605 [D loss: 0.331946, acc.: 85.16%] [G loss: 3.218122]\n",
      "epoch:43 step:33606 [D loss: 0.308754, acc.: 85.94%] [G loss: 3.509095]\n",
      "epoch:43 step:33607 [D loss: 0.269642, acc.: 89.06%] [G loss: 4.394420]\n",
      "epoch:43 step:33608 [D loss: 0.272236, acc.: 86.72%] [G loss: 3.478829]\n",
      "epoch:43 step:33609 [D loss: 0.182788, acc.: 92.97%] [G loss: 2.938268]\n",
      "epoch:43 step:33610 [D loss: 0.329270, acc.: 86.72%] [G loss: 2.697844]\n",
      "epoch:43 step:33611 [D loss: 0.307243, acc.: 87.50%] [G loss: 3.740632]\n",
      "epoch:43 step:33612 [D loss: 0.329613, acc.: 84.38%] [G loss: 4.205868]\n",
      "epoch:43 step:33613 [D loss: 0.219862, acc.: 88.28%] [G loss: 4.193864]\n",
      "epoch:43 step:33614 [D loss: 0.309021, acc.: 84.38%] [G loss: 4.193368]\n",
      "epoch:43 step:33615 [D loss: 0.265460, acc.: 89.06%] [G loss: 3.983966]\n",
      "epoch:43 step:33616 [D loss: 0.281883, acc.: 89.84%] [G loss: 4.198004]\n",
      "epoch:43 step:33617 [D loss: 0.253099, acc.: 89.06%] [G loss: 3.219112]\n",
      "epoch:43 step:33618 [D loss: 0.382055, acc.: 80.47%] [G loss: 3.763694]\n",
      "epoch:43 step:33619 [D loss: 0.276500, acc.: 89.84%] [G loss: 2.937811]\n",
      "epoch:43 step:33620 [D loss: 0.344095, acc.: 87.50%] [G loss: 4.942666]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:43 step:33621 [D loss: 0.220523, acc.: 92.19%] [G loss: 5.975534]\n",
      "epoch:43 step:33622 [D loss: 0.234003, acc.: 89.84%] [G loss: 4.304441]\n",
      "epoch:43 step:33623 [D loss: 0.218132, acc.: 92.19%] [G loss: 4.321355]\n",
      "epoch:43 step:33624 [D loss: 0.319898, acc.: 85.94%] [G loss: 3.674705]\n",
      "epoch:43 step:33625 [D loss: 0.378710, acc.: 81.25%] [G loss: 3.391501]\n",
      "epoch:43 step:33626 [D loss: 0.293120, acc.: 88.28%] [G loss: 3.696582]\n",
      "epoch:43 step:33627 [D loss: 0.315094, acc.: 84.38%] [G loss: 4.400860]\n",
      "epoch:43 step:33628 [D loss: 0.299433, acc.: 89.84%] [G loss: 3.301927]\n",
      "epoch:43 step:33629 [D loss: 0.258465, acc.: 87.50%] [G loss: 4.311420]\n",
      "epoch:43 step:33630 [D loss: 0.303906, acc.: 85.16%] [G loss: 3.099989]\n",
      "epoch:43 step:33631 [D loss: 0.236875, acc.: 90.62%] [G loss: 4.421554]\n",
      "epoch:43 step:33632 [D loss: 0.263138, acc.: 88.28%] [G loss: 3.808419]\n",
      "epoch:43 step:33633 [D loss: 0.331369, acc.: 84.38%] [G loss: 4.080174]\n",
      "epoch:43 step:33634 [D loss: 0.365419, acc.: 85.94%] [G loss: 3.614958]\n",
      "epoch:43 step:33635 [D loss: 0.342225, acc.: 88.28%] [G loss: 3.988997]\n",
      "epoch:43 step:33636 [D loss: 0.298453, acc.: 84.38%] [G loss: 4.608938]\n",
      "epoch:43 step:33637 [D loss: 0.348636, acc.: 87.50%] [G loss: 3.131498]\n",
      "epoch:43 step:33638 [D loss: 0.310288, acc.: 85.94%] [G loss: 3.825122]\n",
      "epoch:43 step:33639 [D loss: 0.388464, acc.: 81.25%] [G loss: 4.091936]\n",
      "epoch:43 step:33640 [D loss: 0.467759, acc.: 80.47%] [G loss: 6.300033]\n",
      "epoch:43 step:33641 [D loss: 0.361753, acc.: 83.59%] [G loss: 3.579185]\n",
      "epoch:43 step:33642 [D loss: 0.273075, acc.: 88.28%] [G loss: 3.455715]\n",
      "epoch:43 step:33643 [D loss: 0.326462, acc.: 84.38%] [G loss: 2.257125]\n",
      "epoch:43 step:33644 [D loss: 0.365960, acc.: 81.25%] [G loss: 3.514574]\n",
      "epoch:43 step:33645 [D loss: 0.309012, acc.: 85.16%] [G loss: 3.807361]\n",
      "epoch:43 step:33646 [D loss: 0.222865, acc.: 91.41%] [G loss: 3.783186]\n",
      "epoch:43 step:33647 [D loss: 0.340448, acc.: 86.72%] [G loss: 3.589519]\n",
      "epoch:43 step:33648 [D loss: 0.455671, acc.: 77.34%] [G loss: 4.060235]\n",
      "epoch:43 step:33649 [D loss: 0.407178, acc.: 82.03%] [G loss: 3.824452]\n",
      "epoch:43 step:33650 [D loss: 0.542523, acc.: 78.91%] [G loss: 2.459723]\n",
      "epoch:43 step:33651 [D loss: 0.360495, acc.: 85.94%] [G loss: 3.146358]\n",
      "epoch:43 step:33652 [D loss: 0.237631, acc.: 89.84%] [G loss: 3.554813]\n",
      "epoch:43 step:33653 [D loss: 0.311244, acc.: 85.16%] [G loss: 3.303630]\n",
      "epoch:43 step:33654 [D loss: 0.336205, acc.: 84.38%] [G loss: 3.508386]\n",
      "epoch:43 step:33655 [D loss: 0.296607, acc.: 86.72%] [G loss: 2.603257]\n",
      "epoch:43 step:33656 [D loss: 0.361270, acc.: 83.59%] [G loss: 2.567256]\n",
      "epoch:43 step:33657 [D loss: 0.345079, acc.: 85.94%] [G loss: 4.961882]\n",
      "epoch:43 step:33658 [D loss: 0.376726, acc.: 84.38%] [G loss: 7.137432]\n",
      "epoch:43 step:33659 [D loss: 0.431916, acc.: 80.47%] [G loss: 3.909680]\n",
      "epoch:43 step:33660 [D loss: 0.428339, acc.: 80.47%] [G loss: 4.149671]\n",
      "epoch:43 step:33661 [D loss: 0.272384, acc.: 91.41%] [G loss: 3.835610]\n",
      "epoch:43 step:33662 [D loss: 0.363164, acc.: 85.16%] [G loss: 4.694802]\n",
      "epoch:43 step:33663 [D loss: 0.395818, acc.: 81.25%] [G loss: 4.699037]\n",
      "epoch:43 step:33664 [D loss: 0.400032, acc.: 82.03%] [G loss: 4.135386]\n",
      "epoch:43 step:33665 [D loss: 0.236881, acc.: 90.62%] [G loss: 4.118148]\n",
      "epoch:43 step:33666 [D loss: 0.177365, acc.: 93.75%] [G loss: 4.672283]\n",
      "epoch:43 step:33667 [D loss: 0.268125, acc.: 89.84%] [G loss: 3.239545]\n",
      "epoch:43 step:33668 [D loss: 0.328002, acc.: 85.94%] [G loss: 2.948800]\n",
      "epoch:43 step:33669 [D loss: 0.396469, acc.: 79.69%] [G loss: 3.984570]\n",
      "epoch:43 step:33670 [D loss: 0.270541, acc.: 86.72%] [G loss: 5.204474]\n",
      "epoch:43 step:33671 [D loss: 0.252589, acc.: 88.28%] [G loss: 4.446756]\n",
      "epoch:43 step:33672 [D loss: 0.281455, acc.: 85.94%] [G loss: 3.977557]\n",
      "epoch:43 step:33673 [D loss: 0.319108, acc.: 85.16%] [G loss: 3.628608]\n",
      "epoch:43 step:33674 [D loss: 0.257585, acc.: 89.84%] [G loss: 3.564936]\n",
      "epoch:43 step:33675 [D loss: 0.280627, acc.: 88.28%] [G loss: 3.528819]\n",
      "epoch:43 step:33676 [D loss: 0.296584, acc.: 86.72%] [G loss: 3.736795]\n",
      "epoch:43 step:33677 [D loss: 0.339994, acc.: 83.59%] [G loss: 3.034667]\n",
      "epoch:43 step:33678 [D loss: 0.349058, acc.: 85.16%] [G loss: 3.378107]\n",
      "epoch:43 step:33679 [D loss: 0.260201, acc.: 88.28%] [G loss: 3.346809]\n",
      "epoch:43 step:33680 [D loss: 0.251369, acc.: 90.62%] [G loss: 4.776003]\n",
      "epoch:43 step:33681 [D loss: 0.354740, acc.: 84.38%] [G loss: 2.891795]\n",
      "epoch:43 step:33682 [D loss: 0.297860, acc.: 88.28%] [G loss: 3.531610]\n",
      "epoch:43 step:33683 [D loss: 0.379379, acc.: 80.47%] [G loss: 4.109162]\n",
      "epoch:43 step:33684 [D loss: 0.436214, acc.: 77.34%] [G loss: 3.862565]\n",
      "epoch:43 step:33685 [D loss: 0.324602, acc.: 85.16%] [G loss: 3.181461]\n",
      "epoch:43 step:33686 [D loss: 0.307803, acc.: 84.38%] [G loss: 3.134448]\n",
      "epoch:43 step:33687 [D loss: 0.379613, acc.: 83.59%] [G loss: 2.578972]\n",
      "epoch:43 step:33688 [D loss: 0.306346, acc.: 88.28%] [G loss: 3.165612]\n",
      "epoch:43 step:33689 [D loss: 0.397857, acc.: 83.59%] [G loss: 4.734459]\n",
      "epoch:43 step:33690 [D loss: 0.411051, acc.: 78.91%] [G loss: 3.516769]\n",
      "epoch:43 step:33691 [D loss: 0.232870, acc.: 91.41%] [G loss: 4.540714]\n",
      "epoch:43 step:33692 [D loss: 0.302622, acc.: 85.94%] [G loss: 3.594523]\n",
      "epoch:43 step:33693 [D loss: 0.349068, acc.: 85.94%] [G loss: 2.807968]\n",
      "epoch:43 step:33694 [D loss: 0.305477, acc.: 86.72%] [G loss: 5.574120]\n",
      "epoch:43 step:33695 [D loss: 0.242115, acc.: 89.06%] [G loss: 6.768293]\n",
      "epoch:43 step:33696 [D loss: 0.271692, acc.: 85.94%] [G loss: 3.348350]\n",
      "epoch:43 step:33697 [D loss: 0.321226, acc.: 82.81%] [G loss: 3.001776]\n",
      "epoch:43 step:33698 [D loss: 0.333972, acc.: 81.25%] [G loss: 4.484187]\n",
      "epoch:43 step:33699 [D loss: 0.247434, acc.: 91.41%] [G loss: 4.213872]\n",
      "epoch:43 step:33700 [D loss: 0.288364, acc.: 84.38%] [G loss: 3.586096]\n",
      "epoch:43 step:33701 [D loss: 0.296177, acc.: 89.84%] [G loss: 3.099851]\n",
      "epoch:43 step:33702 [D loss: 0.314335, acc.: 84.38%] [G loss: 4.059100]\n",
      "epoch:43 step:33703 [D loss: 0.340537, acc.: 84.38%] [G loss: 2.660049]\n",
      "epoch:43 step:33704 [D loss: 0.283784, acc.: 87.50%] [G loss: 2.628787]\n",
      "epoch:43 step:33705 [D loss: 0.272392, acc.: 89.06%] [G loss: 3.566149]\n",
      "epoch:43 step:33706 [D loss: 0.397861, acc.: 82.03%] [G loss: 2.643928]\n",
      "epoch:43 step:33707 [D loss: 0.419125, acc.: 81.25%] [G loss: 3.498995]\n",
      "epoch:43 step:33708 [D loss: 0.298575, acc.: 89.06%] [G loss: 3.418898]\n",
      "epoch:43 step:33709 [D loss: 0.317239, acc.: 85.16%] [G loss: 3.791302]\n",
      "epoch:43 step:33710 [D loss: 0.274890, acc.: 85.94%] [G loss: 3.486929]\n",
      "epoch:43 step:33711 [D loss: 0.222034, acc.: 94.53%] [G loss: 4.173198]\n",
      "epoch:43 step:33712 [D loss: 0.246474, acc.: 89.06%] [G loss: 3.902534]\n",
      "epoch:43 step:33713 [D loss: 0.353718, acc.: 85.16%] [G loss: 4.437615]\n",
      "epoch:43 step:33714 [D loss: 0.380495, acc.: 82.81%] [G loss: 2.824968]\n",
      "epoch:43 step:33715 [D loss: 0.361071, acc.: 85.16%] [G loss: 2.550066]\n",
      "epoch:43 step:33716 [D loss: 0.328593, acc.: 81.25%] [G loss: 3.178780]\n",
      "epoch:43 step:33717 [D loss: 0.302743, acc.: 89.06%] [G loss: 3.632152]\n",
      "epoch:43 step:33718 [D loss: 0.293466, acc.: 86.72%] [G loss: 3.191732]\n",
      "epoch:43 step:33719 [D loss: 0.328378, acc.: 85.16%] [G loss: 3.186322]\n",
      "epoch:43 step:33720 [D loss: 0.373575, acc.: 82.03%] [G loss: 3.142047]\n",
      "epoch:43 step:33721 [D loss: 0.349535, acc.: 82.81%] [G loss: 2.901474]\n",
      "epoch:43 step:33722 [D loss: 0.309611, acc.: 87.50%] [G loss: 2.814862]\n",
      "epoch:43 step:33723 [D loss: 0.257740, acc.: 89.84%] [G loss: 3.115468]\n",
      "epoch:43 step:33724 [D loss: 0.357502, acc.: 83.59%] [G loss: 3.722948]\n",
      "epoch:43 step:33725 [D loss: 0.410037, acc.: 82.03%] [G loss: 3.792865]\n",
      "epoch:43 step:33726 [D loss: 0.272498, acc.: 85.94%] [G loss: 3.182400]\n",
      "epoch:43 step:33727 [D loss: 0.269796, acc.: 85.94%] [G loss: 5.048995]\n",
      "epoch:43 step:33728 [D loss: 0.399347, acc.: 84.38%] [G loss: 3.487911]\n",
      "epoch:43 step:33729 [D loss: 0.233469, acc.: 87.50%] [G loss: 2.764091]\n",
      "epoch:43 step:33730 [D loss: 0.339760, acc.: 84.38%] [G loss: 2.845237]\n",
      "epoch:43 step:33731 [D loss: 0.306950, acc.: 83.59%] [G loss: 2.833819]\n",
      "epoch:43 step:33732 [D loss: 0.239743, acc.: 85.16%] [G loss: 3.403425]\n",
      "epoch:43 step:33733 [D loss: 0.240286, acc.: 90.62%] [G loss: 2.741425]\n",
      "epoch:43 step:33734 [D loss: 0.312028, acc.: 87.50%] [G loss: 3.911916]\n",
      "epoch:43 step:33735 [D loss: 0.388310, acc.: 78.91%] [G loss: 3.123949]\n",
      "epoch:43 step:33736 [D loss: 0.390889, acc.: 78.91%] [G loss: 3.418475]\n",
      "epoch:43 step:33737 [D loss: 0.282521, acc.: 85.94%] [G loss: 4.253983]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:43 step:33738 [D loss: 0.281530, acc.: 87.50%] [G loss: 4.308607]\n",
      "epoch:43 step:33739 [D loss: 0.211947, acc.: 91.41%] [G loss: 4.802927]\n",
      "epoch:43 step:33740 [D loss: 0.345652, acc.: 86.72%] [G loss: 5.383369]\n",
      "epoch:43 step:33741 [D loss: 0.297870, acc.: 85.94%] [G loss: 4.301319]\n",
      "epoch:43 step:33742 [D loss: 0.303188, acc.: 87.50%] [G loss: 5.106973]\n",
      "epoch:43 step:33743 [D loss: 0.332876, acc.: 82.81%] [G loss: 3.572257]\n",
      "epoch:43 step:33744 [D loss: 0.351765, acc.: 88.28%] [G loss: 3.949711]\n",
      "epoch:43 step:33745 [D loss: 0.355733, acc.: 82.03%] [G loss: 4.547511]\n",
      "epoch:43 step:33746 [D loss: 0.291877, acc.: 86.72%] [G loss: 3.347734]\n",
      "epoch:43 step:33747 [D loss: 0.319509, acc.: 83.59%] [G loss: 2.785778]\n",
      "epoch:43 step:33748 [D loss: 0.359858, acc.: 84.38%] [G loss: 3.122329]\n",
      "epoch:43 step:33749 [D loss: 0.321862, acc.: 85.94%] [G loss: 3.326862]\n",
      "epoch:43 step:33750 [D loss: 0.343536, acc.: 82.81%] [G loss: 3.197765]\n",
      "epoch:43 step:33751 [D loss: 0.344477, acc.: 87.50%] [G loss: 3.321757]\n",
      "epoch:43 step:33752 [D loss: 0.490531, acc.: 79.69%] [G loss: 5.983964]\n",
      "epoch:43 step:33753 [D loss: 0.366177, acc.: 84.38%] [G loss: 4.306605]\n",
      "epoch:43 step:33754 [D loss: 0.392193, acc.: 82.81%] [G loss: 4.533691]\n",
      "epoch:43 step:33755 [D loss: 0.304260, acc.: 85.16%] [G loss: 4.354883]\n",
      "epoch:43 step:33756 [D loss: 0.332290, acc.: 82.81%] [G loss: 4.306076]\n",
      "epoch:43 step:33757 [D loss: 0.383711, acc.: 79.69%] [G loss: 3.624806]\n",
      "epoch:43 step:33758 [D loss: 0.244087, acc.: 87.50%] [G loss: 3.273736]\n",
      "epoch:43 step:33759 [D loss: 0.297835, acc.: 86.72%] [G loss: 3.987413]\n",
      "epoch:43 step:33760 [D loss: 0.325494, acc.: 85.16%] [G loss: 3.742130]\n",
      "epoch:43 step:33761 [D loss: 0.306777, acc.: 86.72%] [G loss: 3.990180]\n",
      "epoch:43 step:33762 [D loss: 0.300784, acc.: 88.28%] [G loss: 4.138879]\n",
      "epoch:43 step:33763 [D loss: 0.373235, acc.: 82.03%] [G loss: 4.501507]\n",
      "epoch:43 step:33764 [D loss: 0.283673, acc.: 89.06%] [G loss: 3.372016]\n",
      "epoch:43 step:33765 [D loss: 0.240804, acc.: 87.50%] [G loss: 4.421905]\n",
      "epoch:43 step:33766 [D loss: 0.329527, acc.: 79.69%] [G loss: 4.448723]\n",
      "epoch:43 step:33767 [D loss: 0.301496, acc.: 87.50%] [G loss: 4.238767]\n",
      "epoch:43 step:33768 [D loss: 0.325046, acc.: 83.59%] [G loss: 2.485193]\n",
      "epoch:43 step:33769 [D loss: 0.364205, acc.: 84.38%] [G loss: 4.087924]\n",
      "epoch:43 step:33770 [D loss: 0.392742, acc.: 82.03%] [G loss: 2.548210]\n",
      "epoch:43 step:33771 [D loss: 0.213789, acc.: 92.19%] [G loss: 3.796881]\n",
      "epoch:43 step:33772 [D loss: 0.411095, acc.: 84.38%] [G loss: 2.893613]\n",
      "epoch:43 step:33773 [D loss: 0.373973, acc.: 85.16%] [G loss: 5.145617]\n",
      "epoch:43 step:33774 [D loss: 0.370299, acc.: 83.59%] [G loss: 3.397359]\n",
      "epoch:43 step:33775 [D loss: 0.214709, acc.: 92.19%] [G loss: 3.503111]\n",
      "epoch:43 step:33776 [D loss: 0.218205, acc.: 90.62%] [G loss: 3.505543]\n",
      "epoch:43 step:33777 [D loss: 0.333119, acc.: 86.72%] [G loss: 2.939082]\n",
      "epoch:43 step:33778 [D loss: 0.390256, acc.: 82.03%] [G loss: 3.123679]\n",
      "epoch:43 step:33779 [D loss: 0.227999, acc.: 92.97%] [G loss: 3.640355]\n",
      "epoch:43 step:33780 [D loss: 0.268493, acc.: 90.62%] [G loss: 2.778174]\n",
      "epoch:43 step:33781 [D loss: 0.277128, acc.: 85.16%] [G loss: 3.945012]\n",
      "epoch:43 step:33782 [D loss: 0.325489, acc.: 83.59%] [G loss: 3.952332]\n",
      "epoch:43 step:33783 [D loss: 0.273789, acc.: 92.19%] [G loss: 3.483558]\n",
      "epoch:43 step:33784 [D loss: 0.314719, acc.: 87.50%] [G loss: 3.322618]\n",
      "epoch:43 step:33785 [D loss: 0.292836, acc.: 88.28%] [G loss: 3.911081]\n",
      "epoch:43 step:33786 [D loss: 0.397835, acc.: 83.59%] [G loss: 3.390635]\n",
      "epoch:43 step:33787 [D loss: 0.280317, acc.: 89.84%] [G loss: 3.935188]\n",
      "epoch:43 step:33788 [D loss: 0.335760, acc.: 82.81%] [G loss: 3.904574]\n",
      "epoch:43 step:33789 [D loss: 0.326021, acc.: 85.94%] [G loss: 2.810687]\n",
      "epoch:43 step:33790 [D loss: 0.252767, acc.: 88.28%] [G loss: 2.929792]\n",
      "epoch:43 step:33791 [D loss: 0.407323, acc.: 84.38%] [G loss: 3.817970]\n",
      "epoch:43 step:33792 [D loss: 0.251192, acc.: 89.06%] [G loss: 3.274577]\n",
      "epoch:43 step:33793 [D loss: 0.225180, acc.: 90.62%] [G loss: 3.531733]\n",
      "epoch:43 step:33794 [D loss: 0.248373, acc.: 90.62%] [G loss: 3.583008]\n",
      "epoch:43 step:33795 [D loss: 0.232664, acc.: 90.62%] [G loss: 3.520023]\n",
      "epoch:43 step:33796 [D loss: 0.210602, acc.: 91.41%] [G loss: 4.535819]\n",
      "epoch:43 step:33797 [D loss: 0.402243, acc.: 82.81%] [G loss: 3.021017]\n",
      "epoch:43 step:33798 [D loss: 0.286705, acc.: 87.50%] [G loss: 3.744140]\n",
      "epoch:43 step:33799 [D loss: 0.256668, acc.: 86.72%] [G loss: 3.336505]\n",
      "epoch:43 step:33800 [D loss: 0.367515, acc.: 83.59%] [G loss: 3.496809]\n",
      "##############\n",
      "[0.86913902 0.85755409 0.7851384  0.81558634 0.78389429 0.84714542\n",
      " 0.85947128 0.84033756 0.80327274 0.82344817]\n",
      "##########\n",
      "epoch:43 step:33801 [D loss: 0.373986, acc.: 84.38%] [G loss: 3.716146]\n",
      "epoch:43 step:33802 [D loss: 0.256473, acc.: 87.50%] [G loss: 3.469347]\n",
      "epoch:43 step:33803 [D loss: 0.296095, acc.: 89.06%] [G loss: 2.912003]\n",
      "epoch:43 step:33804 [D loss: 0.249584, acc.: 88.28%] [G loss: 2.821764]\n",
      "epoch:43 step:33805 [D loss: 0.329100, acc.: 89.06%] [G loss: 2.768605]\n",
      "epoch:43 step:33806 [D loss: 0.242392, acc.: 92.19%] [G loss: 3.610337]\n",
      "epoch:43 step:33807 [D loss: 0.304390, acc.: 85.16%] [G loss: 4.095245]\n",
      "epoch:43 step:33808 [D loss: 0.329949, acc.: 85.94%] [G loss: 5.515224]\n",
      "epoch:43 step:33809 [D loss: 0.363128, acc.: 84.38%] [G loss: 3.137957]\n",
      "epoch:43 step:33810 [D loss: 0.277964, acc.: 87.50%] [G loss: 3.452757]\n",
      "epoch:43 step:33811 [D loss: 0.489234, acc.: 78.12%] [G loss: 3.356746]\n",
      "epoch:43 step:33812 [D loss: 0.424043, acc.: 78.12%] [G loss: 4.262939]\n",
      "epoch:43 step:33813 [D loss: 0.522935, acc.: 78.12%] [G loss: 5.524272]\n",
      "epoch:43 step:33814 [D loss: 0.468310, acc.: 79.69%] [G loss: 2.666672]\n",
      "epoch:43 step:33815 [D loss: 0.206907, acc.: 92.19%] [G loss: 4.185628]\n",
      "epoch:43 step:33816 [D loss: 0.341331, acc.: 85.94%] [G loss: 3.104005]\n",
      "epoch:43 step:33817 [D loss: 0.297669, acc.: 85.94%] [G loss: 3.343524]\n",
      "epoch:43 step:33818 [D loss: 0.349878, acc.: 80.47%] [G loss: 3.484410]\n",
      "epoch:43 step:33819 [D loss: 0.265564, acc.: 85.94%] [G loss: 3.871168]\n",
      "epoch:43 step:33820 [D loss: 0.320797, acc.: 85.94%] [G loss: 3.985894]\n",
      "epoch:43 step:33821 [D loss: 0.307491, acc.: 84.38%] [G loss: 5.179914]\n",
      "epoch:43 step:33822 [D loss: 0.269590, acc.: 88.28%] [G loss: 3.455352]\n",
      "epoch:43 step:33823 [D loss: 0.343818, acc.: 81.25%] [G loss: 4.856199]\n",
      "epoch:43 step:33824 [D loss: 0.225455, acc.: 90.62%] [G loss: 4.319507]\n",
      "epoch:43 step:33825 [D loss: 0.236926, acc.: 88.28%] [G loss: 3.619475]\n",
      "epoch:43 step:33826 [D loss: 0.209508, acc.: 92.19%] [G loss: 4.789509]\n",
      "epoch:43 step:33827 [D loss: 0.305199, acc.: 85.16%] [G loss: 3.590998]\n",
      "epoch:43 step:33828 [D loss: 0.269378, acc.: 89.06%] [G loss: 2.846961]\n",
      "epoch:43 step:33829 [D loss: 0.226489, acc.: 91.41%] [G loss: 3.365061]\n",
      "epoch:43 step:33830 [D loss: 0.358849, acc.: 84.38%] [G loss: 2.763502]\n",
      "epoch:43 step:33831 [D loss: 0.418299, acc.: 76.56%] [G loss: 2.822087]\n",
      "epoch:43 step:33832 [D loss: 0.409751, acc.: 81.25%] [G loss: 3.810148]\n",
      "epoch:43 step:33833 [D loss: 0.483459, acc.: 78.91%] [G loss: 4.020857]\n",
      "epoch:43 step:33834 [D loss: 0.379986, acc.: 82.81%] [G loss: 5.161065]\n",
      "epoch:43 step:33835 [D loss: 0.332787, acc.: 85.16%] [G loss: 3.768159]\n",
      "epoch:43 step:33836 [D loss: 0.276696, acc.: 85.16%] [G loss: 3.739425]\n",
      "epoch:43 step:33837 [D loss: 0.279982, acc.: 86.72%] [G loss: 3.801611]\n",
      "epoch:43 step:33838 [D loss: 0.304726, acc.: 87.50%] [G loss: 3.171471]\n",
      "epoch:43 step:33839 [D loss: 0.368990, acc.: 81.25%] [G loss: 2.718022]\n",
      "epoch:43 step:33840 [D loss: 0.312443, acc.: 82.81%] [G loss: 2.999378]\n",
      "epoch:43 step:33841 [D loss: 0.280190, acc.: 88.28%] [G loss: 2.735388]\n",
      "epoch:43 step:33842 [D loss: 0.317851, acc.: 84.38%] [G loss: 4.220676]\n",
      "epoch:43 step:33843 [D loss: 0.356189, acc.: 84.38%] [G loss: 5.662368]\n",
      "epoch:43 step:33844 [D loss: 0.359128, acc.: 81.25%] [G loss: 4.503739]\n",
      "epoch:43 step:33845 [D loss: 0.350940, acc.: 86.72%] [G loss: 4.627673]\n",
      "epoch:43 step:33846 [D loss: 0.289769, acc.: 87.50%] [G loss: 4.661592]\n",
      "epoch:43 step:33847 [D loss: 0.294869, acc.: 86.72%] [G loss: 4.042601]\n",
      "epoch:43 step:33848 [D loss: 0.255294, acc.: 86.72%] [G loss: 4.164424]\n",
      "epoch:43 step:33849 [D loss: 0.303564, acc.: 87.50%] [G loss: 3.042508]\n",
      "epoch:43 step:33850 [D loss: 0.358143, acc.: 82.03%] [G loss: 3.485579]\n",
      "epoch:43 step:33851 [D loss: 0.355845, acc.: 83.59%] [G loss: 4.359561]\n",
      "epoch:43 step:33852 [D loss: 0.316233, acc.: 85.94%] [G loss: 3.736942]\n",
      "epoch:43 step:33853 [D loss: 0.223894, acc.: 91.41%] [G loss: 4.122059]\n",
      "epoch:43 step:33854 [D loss: 0.255522, acc.: 87.50%] [G loss: 3.414417]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:43 step:33855 [D loss: 0.308704, acc.: 85.16%] [G loss: 3.227035]\n",
      "epoch:43 step:33856 [D loss: 0.285414, acc.: 87.50%] [G loss: 3.660218]\n",
      "epoch:43 step:33857 [D loss: 0.227981, acc.: 91.41%] [G loss: 3.395440]\n",
      "epoch:43 step:33858 [D loss: 0.336797, acc.: 82.81%] [G loss: 3.523298]\n",
      "epoch:43 step:33859 [D loss: 0.395169, acc.: 84.38%] [G loss: 2.655076]\n",
      "epoch:43 step:33860 [D loss: 0.305958, acc.: 85.94%] [G loss: 2.896795]\n",
      "epoch:43 step:33861 [D loss: 0.363743, acc.: 80.47%] [G loss: 2.533808]\n",
      "epoch:43 step:33862 [D loss: 0.290772, acc.: 89.84%] [G loss: 2.484089]\n",
      "epoch:43 step:33863 [D loss: 0.312303, acc.: 86.72%] [G loss: 2.866938]\n",
      "epoch:43 step:33864 [D loss: 0.343804, acc.: 82.03%] [G loss: 3.724745]\n",
      "epoch:43 step:33865 [D loss: 0.254089, acc.: 91.41%] [G loss: 3.592456]\n",
      "epoch:43 step:33866 [D loss: 0.369706, acc.: 84.38%] [G loss: 4.037981]\n",
      "epoch:43 step:33867 [D loss: 0.414439, acc.: 82.81%] [G loss: 2.426943]\n",
      "epoch:43 step:33868 [D loss: 0.369657, acc.: 85.16%] [G loss: 2.964138]\n",
      "epoch:43 step:33869 [D loss: 0.280878, acc.: 85.16%] [G loss: 3.511524]\n",
      "epoch:43 step:33870 [D loss: 0.480247, acc.: 78.12%] [G loss: 7.660587]\n",
      "epoch:43 step:33871 [D loss: 0.878317, acc.: 64.84%] [G loss: 5.615952]\n",
      "epoch:43 step:33872 [D loss: 1.133865, acc.: 63.28%] [G loss: 3.496645]\n",
      "epoch:43 step:33873 [D loss: 0.577758, acc.: 81.25%] [G loss: 4.579029]\n",
      "epoch:43 step:33874 [D loss: 0.684954, acc.: 75.78%] [G loss: 3.918687]\n",
      "epoch:43 step:33875 [D loss: 0.424507, acc.: 77.34%] [G loss: 5.482024]\n",
      "epoch:43 step:33876 [D loss: 0.497792, acc.: 78.12%] [G loss: 4.640293]\n",
      "epoch:43 step:33877 [D loss: 0.390977, acc.: 82.03%] [G loss: 4.560086]\n",
      "epoch:43 step:33878 [D loss: 0.438223, acc.: 83.59%] [G loss: 4.309996]\n",
      "epoch:43 step:33879 [D loss: 0.294803, acc.: 85.94%] [G loss: 4.876647]\n",
      "epoch:43 step:33880 [D loss: 0.230730, acc.: 92.19%] [G loss: 3.421199]\n",
      "epoch:43 step:33881 [D loss: 0.285577, acc.: 90.62%] [G loss: 2.787779]\n",
      "epoch:43 step:33882 [D loss: 0.259041, acc.: 85.94%] [G loss: 3.935365]\n",
      "epoch:43 step:33883 [D loss: 0.348902, acc.: 82.81%] [G loss: 3.315461]\n",
      "epoch:43 step:33884 [D loss: 0.467509, acc.: 75.78%] [G loss: 3.184230]\n",
      "epoch:43 step:33885 [D loss: 0.375418, acc.: 79.69%] [G loss: 3.214669]\n",
      "epoch:43 step:33886 [D loss: 0.237341, acc.: 92.19%] [G loss: 2.992905]\n",
      "epoch:43 step:33887 [D loss: 0.215162, acc.: 92.19%] [G loss: 3.164919]\n",
      "epoch:43 step:33888 [D loss: 0.324267, acc.: 85.16%] [G loss: 3.038310]\n",
      "epoch:43 step:33889 [D loss: 0.462256, acc.: 78.12%] [G loss: 2.730727]\n",
      "epoch:43 step:33890 [D loss: 0.274968, acc.: 89.06%] [G loss: 3.261084]\n",
      "epoch:43 step:33891 [D loss: 0.314009, acc.: 88.28%] [G loss: 2.957694]\n",
      "epoch:43 step:33892 [D loss: 0.362818, acc.: 84.38%] [G loss: 2.338383]\n",
      "epoch:43 step:33893 [D loss: 0.319423, acc.: 84.38%] [G loss: 3.133294]\n",
      "epoch:43 step:33894 [D loss: 0.467126, acc.: 78.91%] [G loss: 3.610424]\n",
      "epoch:43 step:33895 [D loss: 0.304510, acc.: 88.28%] [G loss: 3.344707]\n",
      "epoch:43 step:33896 [D loss: 0.301447, acc.: 85.94%] [G loss: 4.206285]\n",
      "epoch:43 step:33897 [D loss: 0.267076, acc.: 87.50%] [G loss: 4.587066]\n",
      "epoch:43 step:33898 [D loss: 0.422336, acc.: 80.47%] [G loss: 2.741193]\n",
      "epoch:43 step:33899 [D loss: 0.285778, acc.: 85.94%] [G loss: 3.835597]\n",
      "epoch:43 step:33900 [D loss: 0.284838, acc.: 87.50%] [G loss: 2.938437]\n",
      "epoch:43 step:33901 [D loss: 0.269405, acc.: 87.50%] [G loss: 3.538165]\n",
      "epoch:43 step:33902 [D loss: 0.245730, acc.: 90.62%] [G loss: 3.579178]\n",
      "epoch:43 step:33903 [D loss: 0.306880, acc.: 89.06%] [G loss: 3.000161]\n",
      "epoch:43 step:33904 [D loss: 0.226729, acc.: 91.41%] [G loss: 4.839310]\n",
      "epoch:43 step:33905 [D loss: 0.258146, acc.: 88.28%] [G loss: 4.452806]\n",
      "epoch:43 step:33906 [D loss: 0.241333, acc.: 89.06%] [G loss: 3.582245]\n",
      "epoch:43 step:33907 [D loss: 0.354223, acc.: 84.38%] [G loss: 3.435180]\n",
      "epoch:43 step:33908 [D loss: 0.279895, acc.: 86.72%] [G loss: 2.805354]\n",
      "epoch:43 step:33909 [D loss: 0.347719, acc.: 86.72%] [G loss: 3.737679]\n",
      "epoch:43 step:33910 [D loss: 0.339233, acc.: 82.03%] [G loss: 3.387538]\n",
      "epoch:43 step:33911 [D loss: 0.238754, acc.: 88.28%] [G loss: 3.342991]\n",
      "epoch:43 step:33912 [D loss: 0.363338, acc.: 84.38%] [G loss: 3.906242]\n",
      "epoch:43 step:33913 [D loss: 0.307527, acc.: 83.59%] [G loss: 2.982478]\n",
      "epoch:43 step:33914 [D loss: 0.330573, acc.: 82.81%] [G loss: 2.728249]\n",
      "epoch:43 step:33915 [D loss: 0.355165, acc.: 85.16%] [G loss: 2.731285]\n",
      "epoch:43 step:33916 [D loss: 0.288979, acc.: 85.16%] [G loss: 3.131798]\n",
      "epoch:43 step:33917 [D loss: 0.264043, acc.: 91.41%] [G loss: 3.450628]\n",
      "epoch:43 step:33918 [D loss: 0.374410, acc.: 84.38%] [G loss: 3.370291]\n",
      "epoch:43 step:33919 [D loss: 0.247877, acc.: 89.06%] [G loss: 2.729307]\n",
      "epoch:43 step:33920 [D loss: 0.276945, acc.: 90.62%] [G loss: 2.790134]\n",
      "epoch:43 step:33921 [D loss: 0.325924, acc.: 85.94%] [G loss: 3.805226]\n",
      "epoch:43 step:33922 [D loss: 0.290269, acc.: 85.16%] [G loss: 2.830626]\n",
      "epoch:43 step:33923 [D loss: 0.303326, acc.: 87.50%] [G loss: 3.243788]\n",
      "epoch:43 step:33924 [D loss: 0.293768, acc.: 85.16%] [G loss: 3.213417]\n",
      "epoch:43 step:33925 [D loss: 0.297848, acc.: 87.50%] [G loss: 3.118231]\n",
      "epoch:43 step:33926 [D loss: 0.335042, acc.: 82.81%] [G loss: 3.801137]\n",
      "epoch:43 step:33927 [D loss: 0.294966, acc.: 85.16%] [G loss: 2.542874]\n",
      "epoch:43 step:33928 [D loss: 0.359010, acc.: 84.38%] [G loss: 2.284306]\n",
      "epoch:43 step:33929 [D loss: 0.299400, acc.: 85.16%] [G loss: 2.869785]\n",
      "epoch:43 step:33930 [D loss: 0.316773, acc.: 85.94%] [G loss: 2.939613]\n",
      "epoch:43 step:33931 [D loss: 0.369219, acc.: 82.81%] [G loss: 3.352518]\n",
      "epoch:43 step:33932 [D loss: 0.272291, acc.: 86.72%] [G loss: 3.972046]\n",
      "epoch:43 step:33933 [D loss: 0.276784, acc.: 87.50%] [G loss: 3.498480]\n",
      "epoch:43 step:33934 [D loss: 0.283639, acc.: 88.28%] [G loss: 2.888517]\n",
      "epoch:43 step:33935 [D loss: 0.291023, acc.: 89.84%] [G loss: 2.733569]\n",
      "epoch:43 step:33936 [D loss: 0.346127, acc.: 82.81%] [G loss: 2.252303]\n",
      "epoch:43 step:33937 [D loss: 0.322850, acc.: 85.94%] [G loss: 2.838644]\n",
      "epoch:43 step:33938 [D loss: 0.334309, acc.: 83.59%] [G loss: 3.060814]\n",
      "epoch:43 step:33939 [D loss: 0.319202, acc.: 89.06%] [G loss: 2.738948]\n",
      "epoch:43 step:33940 [D loss: 0.332162, acc.: 85.16%] [G loss: 2.340889]\n",
      "epoch:43 step:33941 [D loss: 0.264437, acc.: 89.84%] [G loss: 3.194228]\n",
      "epoch:43 step:33942 [D loss: 0.302579, acc.: 89.06%] [G loss: 2.606819]\n",
      "epoch:43 step:33943 [D loss: 0.404710, acc.: 78.12%] [G loss: 2.743742]\n",
      "epoch:43 step:33944 [D loss: 0.393374, acc.: 80.47%] [G loss: 2.821627]\n",
      "epoch:43 step:33945 [D loss: 0.296975, acc.: 85.16%] [G loss: 2.669676]\n",
      "epoch:43 step:33946 [D loss: 0.440498, acc.: 82.81%] [G loss: 2.603230]\n",
      "epoch:43 step:33947 [D loss: 0.328881, acc.: 85.94%] [G loss: 3.161464]\n",
      "epoch:43 step:33948 [D loss: 0.278092, acc.: 86.72%] [G loss: 2.680707]\n",
      "epoch:43 step:33949 [D loss: 0.325842, acc.: 88.28%] [G loss: 2.934601]\n",
      "epoch:43 step:33950 [D loss: 0.270714, acc.: 89.06%] [G loss: 4.509467]\n",
      "epoch:43 step:33951 [D loss: 0.327974, acc.: 83.59%] [G loss: 4.513968]\n",
      "epoch:43 step:33952 [D loss: 0.208207, acc.: 92.19%] [G loss: 3.069923]\n",
      "epoch:43 step:33953 [D loss: 0.268239, acc.: 89.06%] [G loss: 2.791258]\n",
      "epoch:43 step:33954 [D loss: 0.247027, acc.: 89.84%] [G loss: 2.788853]\n",
      "epoch:43 step:33955 [D loss: 0.286722, acc.: 88.28%] [G loss: 2.136927]\n",
      "epoch:43 step:33956 [D loss: 0.238260, acc.: 88.28%] [G loss: 3.405370]\n",
      "epoch:43 step:33957 [D loss: 0.271179, acc.: 85.16%] [G loss: 2.970377]\n",
      "epoch:43 step:33958 [D loss: 0.210984, acc.: 92.19%] [G loss: 3.144116]\n",
      "epoch:43 step:33959 [D loss: 0.333145, acc.: 82.03%] [G loss: 3.033448]\n",
      "epoch:43 step:33960 [D loss: 0.235875, acc.: 90.62%] [G loss: 2.689747]\n",
      "epoch:43 step:33961 [D loss: 0.347102, acc.: 84.38%] [G loss: 3.394990]\n",
      "epoch:43 step:33962 [D loss: 0.484545, acc.: 80.47%] [G loss: 3.658446]\n",
      "epoch:43 step:33963 [D loss: 0.304165, acc.: 87.50%] [G loss: 2.800483]\n",
      "epoch:43 step:33964 [D loss: 0.281489, acc.: 85.16%] [G loss: 3.278848]\n",
      "epoch:43 step:33965 [D loss: 0.319706, acc.: 84.38%] [G loss: 3.122460]\n",
      "epoch:43 step:33966 [D loss: 0.265779, acc.: 85.94%] [G loss: 3.430418]\n",
      "epoch:43 step:33967 [D loss: 0.214901, acc.: 92.19%] [G loss: 3.328026]\n",
      "epoch:43 step:33968 [D loss: 0.229181, acc.: 90.62%] [G loss: 2.945082]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:43 step:33969 [D loss: 0.395513, acc.: 84.38%] [G loss: 2.631078]\n",
      "epoch:43 step:33970 [D loss: 0.253619, acc.: 89.06%] [G loss: 2.340227]\n",
      "epoch:43 step:33971 [D loss: 0.301811, acc.: 86.72%] [G loss: 2.846062]\n",
      "epoch:43 step:33972 [D loss: 0.323312, acc.: 87.50%] [G loss: 3.260292]\n",
      "epoch:43 step:33973 [D loss: 0.240818, acc.: 92.19%] [G loss: 2.899599]\n",
      "epoch:43 step:33974 [D loss: 0.313919, acc.: 84.38%] [G loss: 4.054889]\n",
      "epoch:43 step:33975 [D loss: 0.386270, acc.: 82.03%] [G loss: 4.005159]\n",
      "epoch:43 step:33976 [D loss: 0.276994, acc.: 88.28%] [G loss: 3.578778]\n",
      "epoch:43 step:33977 [D loss: 0.331707, acc.: 86.72%] [G loss: 3.669743]\n",
      "epoch:43 step:33978 [D loss: 0.343116, acc.: 82.81%] [G loss: 3.154118]\n",
      "epoch:43 step:33979 [D loss: 0.287519, acc.: 85.94%] [G loss: 2.803016]\n",
      "epoch:43 step:33980 [D loss: 0.315151, acc.: 85.94%] [G loss: 3.649537]\n",
      "epoch:43 step:33981 [D loss: 0.267911, acc.: 84.38%] [G loss: 3.211528]\n",
      "epoch:43 step:33982 [D loss: 0.246009, acc.: 89.84%] [G loss: 4.403309]\n",
      "epoch:43 step:33983 [D loss: 0.268926, acc.: 89.06%] [G loss: 2.437407]\n",
      "epoch:43 step:33984 [D loss: 0.303277, acc.: 85.94%] [G loss: 2.798372]\n",
      "epoch:43 step:33985 [D loss: 0.338386, acc.: 85.94%] [G loss: 2.840395]\n",
      "epoch:43 step:33986 [D loss: 0.334409, acc.: 85.16%] [G loss: 3.461857]\n",
      "epoch:43 step:33987 [D loss: 0.371996, acc.: 82.81%] [G loss: 2.802454]\n",
      "epoch:43 step:33988 [D loss: 0.292019, acc.: 87.50%] [G loss: 4.810176]\n",
      "epoch:43 step:33989 [D loss: 0.388761, acc.: 77.34%] [G loss: 3.261877]\n",
      "epoch:43 step:33990 [D loss: 0.335283, acc.: 88.28%] [G loss: 3.311154]\n",
      "epoch:43 step:33991 [D loss: 0.272788, acc.: 90.62%] [G loss: 3.179631]\n",
      "epoch:43 step:33992 [D loss: 0.359009, acc.: 83.59%] [G loss: 2.998415]\n",
      "epoch:43 step:33993 [D loss: 0.284475, acc.: 85.16%] [G loss: 2.810244]\n",
      "epoch:43 step:33994 [D loss: 0.261290, acc.: 86.72%] [G loss: 2.582502]\n",
      "epoch:43 step:33995 [D loss: 0.312414, acc.: 84.38%] [G loss: 5.011589]\n",
      "epoch:43 step:33996 [D loss: 0.230603, acc.: 90.62%] [G loss: 3.394226]\n",
      "epoch:43 step:33997 [D loss: 0.213214, acc.: 92.97%] [G loss: 4.824306]\n",
      "epoch:43 step:33998 [D loss: 0.317152, acc.: 84.38%] [G loss: 4.320486]\n",
      "epoch:43 step:33999 [D loss: 0.302404, acc.: 88.28%] [G loss: 5.346576]\n",
      "epoch:43 step:34000 [D loss: 0.258535, acc.: 90.62%] [G loss: 3.263271]\n",
      "##############\n",
      "[0.85349218 0.86776564 0.7902955  0.80732303 0.77320978 0.818834\n",
      " 0.90421484 0.82329156 0.80361852 0.81701723]\n",
      "##########\n",
      "epoch:43 step:34001 [D loss: 0.281982, acc.: 87.50%] [G loss: 5.238637]\n",
      "epoch:43 step:34002 [D loss: 0.324665, acc.: 82.81%] [G loss: 4.321170]\n",
      "epoch:43 step:34003 [D loss: 0.246838, acc.: 88.28%] [G loss: 4.830840]\n",
      "epoch:43 step:34004 [D loss: 0.208444, acc.: 92.97%] [G loss: 3.459826]\n",
      "epoch:43 step:34005 [D loss: 0.210816, acc.: 91.41%] [G loss: 3.612834]\n",
      "epoch:43 step:34006 [D loss: 0.316975, acc.: 87.50%] [G loss: 3.843788]\n",
      "epoch:43 step:34007 [D loss: 0.315401, acc.: 84.38%] [G loss: 4.488060]\n",
      "epoch:43 step:34008 [D loss: 0.289076, acc.: 84.38%] [G loss: 3.063544]\n",
      "epoch:43 step:34009 [D loss: 0.385749, acc.: 82.81%] [G loss: 4.196475]\n",
      "epoch:43 step:34010 [D loss: 0.307613, acc.: 85.16%] [G loss: 3.133286]\n",
      "epoch:43 step:34011 [D loss: 0.242821, acc.: 92.19%] [G loss: 4.183007]\n",
      "epoch:43 step:34012 [D loss: 0.305181, acc.: 82.81%] [G loss: 3.363068]\n",
      "epoch:43 step:34013 [D loss: 0.270841, acc.: 88.28%] [G loss: 3.452542]\n",
      "epoch:43 step:34014 [D loss: 0.292783, acc.: 87.50%] [G loss: 3.858579]\n",
      "epoch:43 step:34015 [D loss: 0.440438, acc.: 84.38%] [G loss: 3.817724]\n",
      "epoch:43 step:34016 [D loss: 0.285921, acc.: 89.06%] [G loss: 3.321013]\n",
      "epoch:43 step:34017 [D loss: 0.291808, acc.: 83.59%] [G loss: 6.875328]\n",
      "epoch:43 step:34018 [D loss: 0.532325, acc.: 76.56%] [G loss: 4.290896]\n",
      "epoch:43 step:34019 [D loss: 0.689869, acc.: 70.31%] [G loss: 9.284607]\n",
      "epoch:43 step:34020 [D loss: 1.656956, acc.: 64.84%] [G loss: 5.413178]\n",
      "epoch:43 step:34021 [D loss: 0.552652, acc.: 81.25%] [G loss: 7.132110]\n",
      "epoch:43 step:34022 [D loss: 0.581061, acc.: 75.00%] [G loss: 3.746577]\n",
      "epoch:43 step:34023 [D loss: 0.324172, acc.: 85.16%] [G loss: 3.283458]\n",
      "epoch:43 step:34024 [D loss: 0.340768, acc.: 88.28%] [G loss: 4.183004]\n",
      "epoch:43 step:34025 [D loss: 0.187743, acc.: 93.75%] [G loss: 3.130416]\n",
      "epoch:43 step:34026 [D loss: 0.337008, acc.: 83.59%] [G loss: 5.631082]\n",
      "epoch:43 step:34027 [D loss: 0.216615, acc.: 87.50%] [G loss: 4.369593]\n",
      "epoch:43 step:34028 [D loss: 0.314398, acc.: 82.03%] [G loss: 3.820916]\n",
      "epoch:43 step:34029 [D loss: 0.222708, acc.: 89.06%] [G loss: 4.204063]\n",
      "epoch:43 step:34030 [D loss: 0.291029, acc.: 87.50%] [G loss: 3.252518]\n",
      "epoch:43 step:34031 [D loss: 0.362026, acc.: 79.69%] [G loss: 4.706225]\n",
      "epoch:43 step:34032 [D loss: 0.343318, acc.: 84.38%] [G loss: 2.900189]\n",
      "epoch:43 step:34033 [D loss: 0.266065, acc.: 89.06%] [G loss: 3.931015]\n",
      "epoch:43 step:34034 [D loss: 0.293492, acc.: 82.81%] [G loss: 3.638156]\n",
      "epoch:43 step:34035 [D loss: 0.322485, acc.: 85.16%] [G loss: 3.602364]\n",
      "epoch:43 step:34036 [D loss: 0.194257, acc.: 92.97%] [G loss: 3.949185]\n",
      "epoch:43 step:34037 [D loss: 0.394004, acc.: 86.72%] [G loss: 2.816988]\n",
      "epoch:43 step:34038 [D loss: 0.302436, acc.: 84.38%] [G loss: 3.527798]\n",
      "epoch:43 step:34039 [D loss: 0.414005, acc.: 84.38%] [G loss: 3.777709]\n",
      "epoch:43 step:34040 [D loss: 0.265036, acc.: 89.84%] [G loss: 2.542581]\n",
      "epoch:43 step:34041 [D loss: 0.226328, acc.: 92.19%] [G loss: 4.025019]\n",
      "epoch:43 step:34042 [D loss: 0.227908, acc.: 90.62%] [G loss: 2.953690]\n",
      "epoch:43 step:34043 [D loss: 0.387971, acc.: 81.25%] [G loss: 4.907659]\n",
      "epoch:43 step:34044 [D loss: 0.278368, acc.: 86.72%] [G loss: 7.444404]\n",
      "epoch:43 step:34045 [D loss: 0.246607, acc.: 91.41%] [G loss: 4.064368]\n",
      "epoch:43 step:34046 [D loss: 0.329387, acc.: 82.81%] [G loss: 4.520076]\n",
      "epoch:43 step:34047 [D loss: 0.381256, acc.: 81.25%] [G loss: 2.641915]\n",
      "epoch:43 step:34048 [D loss: 0.275674, acc.: 89.84%] [G loss: 3.051601]\n",
      "epoch:43 step:34049 [D loss: 0.326037, acc.: 84.38%] [G loss: 3.132634]\n",
      "epoch:43 step:34050 [D loss: 0.292734, acc.: 87.50%] [G loss: 2.851105]\n",
      "epoch:43 step:34051 [D loss: 0.176248, acc.: 93.75%] [G loss: 3.726839]\n",
      "epoch:43 step:34052 [D loss: 0.321683, acc.: 86.72%] [G loss: 3.154434]\n",
      "epoch:43 step:34053 [D loss: 0.258130, acc.: 90.62%] [G loss: 3.297264]\n",
      "epoch:43 step:34054 [D loss: 0.328471, acc.: 86.72%] [G loss: 3.495290]\n",
      "epoch:43 step:34055 [D loss: 0.292200, acc.: 86.72%] [G loss: 2.937455]\n",
      "epoch:43 step:34056 [D loss: 0.301115, acc.: 91.41%] [G loss: 3.149541]\n",
      "epoch:43 step:34057 [D loss: 0.329812, acc.: 82.81%] [G loss: 2.415245]\n",
      "epoch:43 step:34058 [D loss: 0.242896, acc.: 89.84%] [G loss: 2.192389]\n",
      "epoch:43 step:34059 [D loss: 0.305107, acc.: 87.50%] [G loss: 2.454056]\n",
      "epoch:43 step:34060 [D loss: 0.259670, acc.: 89.06%] [G loss: 3.000499]\n",
      "epoch:43 step:34061 [D loss: 0.274735, acc.: 89.06%] [G loss: 2.684947]\n",
      "epoch:43 step:34062 [D loss: 0.285555, acc.: 86.72%] [G loss: 2.569345]\n",
      "epoch:43 step:34063 [D loss: 0.269568, acc.: 89.06%] [G loss: 3.180169]\n",
      "epoch:43 step:34064 [D loss: 0.329130, acc.: 83.59%] [G loss: 3.195064]\n",
      "epoch:43 step:34065 [D loss: 0.314720, acc.: 85.16%] [G loss: 3.147886]\n",
      "epoch:43 step:34066 [D loss: 0.264099, acc.: 89.06%] [G loss: 3.038668]\n",
      "epoch:43 step:34067 [D loss: 0.324520, acc.: 82.03%] [G loss: 3.424973]\n",
      "epoch:43 step:34068 [D loss: 0.217052, acc.: 90.62%] [G loss: 3.619699]\n",
      "epoch:43 step:34069 [D loss: 0.328070, acc.: 87.50%] [G loss: 4.377208]\n",
      "epoch:43 step:34070 [D loss: 0.251597, acc.: 91.41%] [G loss: 4.640471]\n",
      "epoch:43 step:34071 [D loss: 0.235281, acc.: 89.06%] [G loss: 4.180058]\n",
      "epoch:43 step:34072 [D loss: 0.204922, acc.: 90.62%] [G loss: 3.995200]\n",
      "epoch:43 step:34073 [D loss: 0.294653, acc.: 89.84%] [G loss: 3.764094]\n",
      "epoch:43 step:34074 [D loss: 0.377410, acc.: 84.38%] [G loss: 4.722177]\n",
      "epoch:43 step:34075 [D loss: 0.298596, acc.: 86.72%] [G loss: 2.911922]\n",
      "epoch:43 step:34076 [D loss: 0.225905, acc.: 91.41%] [G loss: 4.520529]\n",
      "epoch:43 step:34077 [D loss: 0.267696, acc.: 84.38%] [G loss: 3.288636]\n",
      "epoch:43 step:34078 [D loss: 0.255978, acc.: 89.84%] [G loss: 4.968178]\n",
      "epoch:43 step:34079 [D loss: 0.312821, acc.: 86.72%] [G loss: 5.229664]\n",
      "epoch:43 step:34080 [D loss: 0.333873, acc.: 82.03%] [G loss: 3.201481]\n",
      "epoch:43 step:34081 [D loss: 0.165433, acc.: 94.53%] [G loss: 2.902703]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:43 step:34082 [D loss: 0.224486, acc.: 89.06%] [G loss: 4.366026]\n",
      "epoch:43 step:34083 [D loss: 0.304579, acc.: 86.72%] [G loss: 3.115562]\n",
      "epoch:43 step:34084 [D loss: 0.329615, acc.: 85.94%] [G loss: 3.300568]\n",
      "epoch:43 step:34085 [D loss: 0.344965, acc.: 83.59%] [G loss: 3.273655]\n",
      "epoch:43 step:34086 [D loss: 0.320287, acc.: 88.28%] [G loss: 2.642951]\n",
      "epoch:43 step:34087 [D loss: 0.278838, acc.: 89.84%] [G loss: 2.623250]\n",
      "epoch:43 step:34088 [D loss: 0.372821, acc.: 81.25%] [G loss: 3.400715]\n",
      "epoch:43 step:34089 [D loss: 0.313588, acc.: 88.28%] [G loss: 3.526178]\n",
      "epoch:43 step:34090 [D loss: 0.342979, acc.: 82.03%] [G loss: 2.825128]\n",
      "epoch:43 step:34091 [D loss: 0.435646, acc.: 81.25%] [G loss: 3.250267]\n",
      "epoch:43 step:34092 [D loss: 0.309101, acc.: 88.28%] [G loss: 2.673755]\n",
      "epoch:43 step:34093 [D loss: 0.386078, acc.: 82.81%] [G loss: 3.911831]\n",
      "epoch:43 step:34094 [D loss: 0.377612, acc.: 83.59%] [G loss: 3.170311]\n",
      "epoch:43 step:34095 [D loss: 0.270238, acc.: 88.28%] [G loss: 3.422593]\n",
      "epoch:43 step:34096 [D loss: 0.282421, acc.: 89.06%] [G loss: 3.824637]\n",
      "epoch:43 step:34097 [D loss: 0.294972, acc.: 88.28%] [G loss: 3.215804]\n",
      "epoch:43 step:34098 [D loss: 0.352256, acc.: 85.94%] [G loss: 2.718668]\n",
      "epoch:43 step:34099 [D loss: 0.327756, acc.: 85.94%] [G loss: 2.710567]\n",
      "epoch:43 step:34100 [D loss: 0.266487, acc.: 89.84%] [G loss: 3.127759]\n",
      "epoch:43 step:34101 [D loss: 0.261496, acc.: 85.94%] [G loss: 3.749537]\n",
      "epoch:43 step:34102 [D loss: 0.378460, acc.: 80.47%] [G loss: 3.718324]\n",
      "epoch:43 step:34103 [D loss: 0.369567, acc.: 79.69%] [G loss: 2.573255]\n",
      "epoch:43 step:34104 [D loss: 0.295420, acc.: 88.28%] [G loss: 3.012395]\n",
      "epoch:43 step:34105 [D loss: 0.330213, acc.: 85.94%] [G loss: 3.624823]\n",
      "epoch:43 step:34106 [D loss: 0.272523, acc.: 88.28%] [G loss: 3.208006]\n",
      "epoch:43 step:34107 [D loss: 0.283223, acc.: 88.28%] [G loss: 3.651189]\n",
      "epoch:43 step:34108 [D loss: 0.475085, acc.: 79.69%] [G loss: 2.833609]\n",
      "epoch:43 step:34109 [D loss: 0.418063, acc.: 82.81%] [G loss: 6.116045]\n",
      "epoch:43 step:34110 [D loss: 0.706597, acc.: 75.78%] [G loss: 5.694009]\n",
      "epoch:43 step:34111 [D loss: 0.582785, acc.: 78.91%] [G loss: 2.705743]\n",
      "epoch:43 step:34112 [D loss: 0.665696, acc.: 80.47%] [G loss: 2.348650]\n",
      "epoch:43 step:34113 [D loss: 0.415626, acc.: 78.91%] [G loss: 3.570459]\n",
      "epoch:43 step:34114 [D loss: 0.267597, acc.: 85.94%] [G loss: 3.045436]\n",
      "epoch:43 step:34115 [D loss: 0.273488, acc.: 89.84%] [G loss: 3.417083]\n",
      "epoch:43 step:34116 [D loss: 0.309256, acc.: 88.28%] [G loss: 3.398719]\n",
      "epoch:43 step:34117 [D loss: 0.232363, acc.: 90.62%] [G loss: 3.591968]\n",
      "epoch:43 step:34118 [D loss: 0.324502, acc.: 84.38%] [G loss: 3.333553]\n",
      "epoch:43 step:34119 [D loss: 0.323545, acc.: 82.03%] [G loss: 3.545706]\n",
      "epoch:43 step:34120 [D loss: 0.415327, acc.: 80.47%] [G loss: 3.531868]\n",
      "epoch:43 step:34121 [D loss: 0.400080, acc.: 78.12%] [G loss: 3.278072]\n",
      "epoch:43 step:34122 [D loss: 0.215485, acc.: 92.97%] [G loss: 4.882487]\n",
      "epoch:43 step:34123 [D loss: 0.303571, acc.: 89.06%] [G loss: 3.546744]\n",
      "epoch:43 step:34124 [D loss: 0.270177, acc.: 89.06%] [G loss: 4.390843]\n",
      "epoch:43 step:34125 [D loss: 0.428006, acc.: 83.59%] [G loss: 3.208525]\n",
      "epoch:43 step:34126 [D loss: 0.285538, acc.: 86.72%] [G loss: 4.027680]\n",
      "epoch:43 step:34127 [D loss: 0.401675, acc.: 78.91%] [G loss: 3.128987]\n",
      "epoch:43 step:34128 [D loss: 0.312292, acc.: 82.03%] [G loss: 3.515781]\n",
      "epoch:43 step:34129 [D loss: 0.236169, acc.: 90.62%] [G loss: 2.833136]\n",
      "epoch:43 step:34130 [D loss: 0.314387, acc.: 84.38%] [G loss: 3.266389]\n",
      "epoch:43 step:34131 [D loss: 0.283877, acc.: 89.84%] [G loss: 2.637677]\n",
      "epoch:43 step:34132 [D loss: 0.232814, acc.: 89.06%] [G loss: 4.483812]\n",
      "epoch:43 step:34133 [D loss: 0.233978, acc.: 88.28%] [G loss: 2.950794]\n",
      "epoch:43 step:34134 [D loss: 0.245790, acc.: 89.06%] [G loss: 2.868183]\n",
      "epoch:43 step:34135 [D loss: 0.392965, acc.: 82.81%] [G loss: 3.491336]\n",
      "epoch:43 step:34136 [D loss: 0.292354, acc.: 86.72%] [G loss: 3.392100]\n",
      "epoch:43 step:34137 [D loss: 0.332412, acc.: 86.72%] [G loss: 2.891468]\n",
      "epoch:43 step:34138 [D loss: 0.290134, acc.: 88.28%] [G loss: 3.330284]\n",
      "epoch:43 step:34139 [D loss: 0.356589, acc.: 85.16%] [G loss: 3.135319]\n",
      "epoch:43 step:34140 [D loss: 0.340028, acc.: 85.16%] [G loss: 2.925746]\n",
      "epoch:43 step:34141 [D loss: 0.347635, acc.: 83.59%] [G loss: 3.445711]\n",
      "epoch:43 step:34142 [D loss: 0.246211, acc.: 86.72%] [G loss: 3.577583]\n",
      "epoch:43 step:34143 [D loss: 0.278019, acc.: 87.50%] [G loss: 3.884820]\n",
      "epoch:43 step:34144 [D loss: 0.265928, acc.: 88.28%] [G loss: 2.762163]\n",
      "epoch:43 step:34145 [D loss: 0.340037, acc.: 83.59%] [G loss: 3.051412]\n",
      "epoch:43 step:34146 [D loss: 0.307984, acc.: 89.84%] [G loss: 3.452687]\n",
      "epoch:43 step:34147 [D loss: 0.282798, acc.: 85.94%] [G loss: 3.692982]\n",
      "epoch:43 step:34148 [D loss: 0.313855, acc.: 89.06%] [G loss: 3.116335]\n",
      "epoch:43 step:34149 [D loss: 0.233209, acc.: 90.62%] [G loss: 2.645187]\n",
      "epoch:43 step:34150 [D loss: 0.313761, acc.: 87.50%] [G loss: 3.181971]\n",
      "epoch:43 step:34151 [D loss: 0.321394, acc.: 85.16%] [G loss: 4.031724]\n",
      "epoch:43 step:34152 [D loss: 0.263586, acc.: 85.94%] [G loss: 3.841534]\n",
      "epoch:43 step:34153 [D loss: 0.350925, acc.: 87.50%] [G loss: 3.324810]\n",
      "epoch:43 step:34154 [D loss: 0.216080, acc.: 91.41%] [G loss: 3.155623]\n",
      "epoch:43 step:34155 [D loss: 0.367352, acc.: 76.56%] [G loss: 3.438539]\n",
      "epoch:43 step:34156 [D loss: 0.382510, acc.: 82.81%] [G loss: 2.882542]\n",
      "epoch:43 step:34157 [D loss: 0.259124, acc.: 89.06%] [G loss: 3.105787]\n",
      "epoch:43 step:34158 [D loss: 0.335747, acc.: 85.94%] [G loss: 2.796538]\n",
      "epoch:43 step:34159 [D loss: 0.379720, acc.: 86.72%] [G loss: 2.879076]\n",
      "epoch:43 step:34160 [D loss: 0.315164, acc.: 85.94%] [G loss: 3.822771]\n",
      "epoch:43 step:34161 [D loss: 0.348657, acc.: 85.16%] [G loss: 3.045258]\n",
      "epoch:43 step:34162 [D loss: 0.361776, acc.: 83.59%] [G loss: 3.547960]\n",
      "epoch:43 step:34163 [D loss: 0.339817, acc.: 85.16%] [G loss: 3.886790]\n",
      "epoch:43 step:34164 [D loss: 0.252872, acc.: 90.62%] [G loss: 2.732464]\n",
      "epoch:43 step:34165 [D loss: 0.343012, acc.: 86.72%] [G loss: 3.068723]\n",
      "epoch:43 step:34166 [D loss: 0.436491, acc.: 82.03%] [G loss: 2.816306]\n",
      "epoch:43 step:34167 [D loss: 0.356020, acc.: 85.16%] [G loss: 2.794496]\n",
      "epoch:43 step:34168 [D loss: 0.243538, acc.: 89.06%] [G loss: 3.350627]\n",
      "epoch:43 step:34169 [D loss: 0.321154, acc.: 85.94%] [G loss: 3.224766]\n",
      "epoch:43 step:34170 [D loss: 0.318847, acc.: 87.50%] [G loss: 2.995006]\n",
      "epoch:43 step:34171 [D loss: 0.263801, acc.: 88.28%] [G loss: 4.444370]\n",
      "epoch:43 step:34172 [D loss: 0.389691, acc.: 85.16%] [G loss: 3.102567]\n",
      "epoch:43 step:34173 [D loss: 0.345956, acc.: 85.16%] [G loss: 3.445774]\n",
      "epoch:43 step:34174 [D loss: 0.281477, acc.: 85.16%] [G loss: 2.843191]\n",
      "epoch:43 step:34175 [D loss: 0.246737, acc.: 89.84%] [G loss: 3.170997]\n",
      "epoch:43 step:34176 [D loss: 0.309530, acc.: 86.72%] [G loss: 2.830736]\n",
      "epoch:43 step:34177 [D loss: 0.305220, acc.: 85.16%] [G loss: 2.772733]\n",
      "epoch:43 step:34178 [D loss: 0.325802, acc.: 87.50%] [G loss: 2.658360]\n",
      "epoch:43 step:34179 [D loss: 0.314914, acc.: 85.94%] [G loss: 3.386922]\n",
      "epoch:43 step:34180 [D loss: 0.306793, acc.: 85.94%] [G loss: 2.943292]\n",
      "epoch:43 step:34181 [D loss: 0.255186, acc.: 90.62%] [G loss: 3.148187]\n",
      "epoch:43 step:34182 [D loss: 0.255453, acc.: 90.62%] [G loss: 3.451142]\n",
      "epoch:43 step:34183 [D loss: 0.335290, acc.: 82.81%] [G loss: 2.801621]\n",
      "epoch:43 step:34184 [D loss: 0.314354, acc.: 85.94%] [G loss: 2.872066]\n",
      "epoch:43 step:34185 [D loss: 0.342108, acc.: 84.38%] [G loss: 2.753836]\n",
      "epoch:43 step:34186 [D loss: 0.272061, acc.: 89.84%] [G loss: 2.559392]\n",
      "epoch:43 step:34187 [D loss: 0.341747, acc.: 85.16%] [G loss: 2.758441]\n",
      "epoch:43 step:34188 [D loss: 0.243359, acc.: 90.62%] [G loss: 2.856638]\n",
      "epoch:43 step:34189 [D loss: 0.486849, acc.: 75.78%] [G loss: 2.413135]\n",
      "epoch:43 step:34190 [D loss: 0.367678, acc.: 82.81%] [G loss: 2.156808]\n",
      "epoch:43 step:34191 [D loss: 0.256336, acc.: 91.41%] [G loss: 2.814210]\n",
      "epoch:43 step:34192 [D loss: 0.361807, acc.: 85.94%] [G loss: 2.955265]\n",
      "epoch:43 step:34193 [D loss: 0.304980, acc.: 88.28%] [G loss: 2.518664]\n",
      "epoch:43 step:34194 [D loss: 0.270373, acc.: 89.84%] [G loss: 3.497167]\n",
      "epoch:43 step:34195 [D loss: 0.285970, acc.: 89.06%] [G loss: 4.483377]\n",
      "epoch:43 step:34196 [D loss: 0.451031, acc.: 75.78%] [G loss: 3.399530]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:43 step:34197 [D loss: 0.250263, acc.: 86.72%] [G loss: 3.242630]\n",
      "epoch:43 step:34198 [D loss: 0.282224, acc.: 85.94%] [G loss: 3.005440]\n",
      "epoch:43 step:34199 [D loss: 0.312760, acc.: 88.28%] [G loss: 3.281099]\n",
      "epoch:43 step:34200 [D loss: 0.263217, acc.: 88.28%] [G loss: 2.739597]\n",
      "##############\n",
      "[0.85659044 0.86386167 0.81278316 0.79703868 0.73069633 0.82464937\n",
      " 0.88326055 0.83971352 0.79324734 0.82806084]\n",
      "##########\n",
      "epoch:43 step:34201 [D loss: 0.410705, acc.: 83.59%] [G loss: 2.333062]\n",
      "epoch:43 step:34202 [D loss: 0.329589, acc.: 87.50%] [G loss: 3.193072]\n",
      "epoch:43 step:34203 [D loss: 0.279678, acc.: 86.72%] [G loss: 3.532355]\n",
      "epoch:43 step:34204 [D loss: 0.280693, acc.: 89.06%] [G loss: 3.154227]\n",
      "epoch:43 step:34205 [D loss: 0.238874, acc.: 88.28%] [G loss: 4.116264]\n",
      "epoch:43 step:34206 [D loss: 0.350163, acc.: 82.81%] [G loss: 4.429101]\n",
      "epoch:43 step:34207 [D loss: 0.507736, acc.: 75.00%] [G loss: 3.758132]\n",
      "epoch:43 step:34208 [D loss: 0.396624, acc.: 82.03%] [G loss: 3.704952]\n",
      "epoch:43 step:34209 [D loss: 0.457062, acc.: 75.78%] [G loss: 3.193426]\n",
      "epoch:43 step:34210 [D loss: 0.263641, acc.: 88.28%] [G loss: 4.432331]\n",
      "epoch:43 step:34211 [D loss: 0.357900, acc.: 83.59%] [G loss: 4.304213]\n",
      "epoch:43 step:34212 [D loss: 0.298161, acc.: 86.72%] [G loss: 4.706625]\n",
      "epoch:43 step:34213 [D loss: 0.287219, acc.: 89.84%] [G loss: 3.288018]\n",
      "epoch:43 step:34214 [D loss: 0.376476, acc.: 84.38%] [G loss: 4.063603]\n",
      "epoch:43 step:34215 [D loss: 0.344759, acc.: 83.59%] [G loss: 4.174485]\n",
      "epoch:43 step:34216 [D loss: 0.289197, acc.: 89.84%] [G loss: 4.162668]\n",
      "epoch:43 step:34217 [D loss: 0.287572, acc.: 88.28%] [G loss: 2.728315]\n",
      "epoch:43 step:34218 [D loss: 0.296761, acc.: 90.62%] [G loss: 3.490045]\n",
      "epoch:43 step:34219 [D loss: 0.369533, acc.: 85.94%] [G loss: 2.263388]\n",
      "epoch:43 step:34220 [D loss: 0.225167, acc.: 89.84%] [G loss: 4.165044]\n",
      "epoch:43 step:34221 [D loss: 0.342112, acc.: 85.16%] [G loss: 3.606666]\n",
      "epoch:43 step:34222 [D loss: 0.298804, acc.: 89.06%] [G loss: 4.314563]\n",
      "epoch:43 step:34223 [D loss: 0.326943, acc.: 84.38%] [G loss: 3.066892]\n",
      "epoch:43 step:34224 [D loss: 0.307981, acc.: 85.16%] [G loss: 3.239817]\n",
      "epoch:43 step:34225 [D loss: 0.314958, acc.: 88.28%] [G loss: 3.120736]\n",
      "epoch:43 step:34226 [D loss: 0.247102, acc.: 89.06%] [G loss: 4.171750]\n",
      "epoch:43 step:34227 [D loss: 0.228768, acc.: 91.41%] [G loss: 3.023917]\n",
      "epoch:43 step:34228 [D loss: 0.291661, acc.: 83.59%] [G loss: 3.645425]\n",
      "epoch:43 step:34229 [D loss: 0.328675, acc.: 85.94%] [G loss: 3.052108]\n",
      "epoch:43 step:34230 [D loss: 0.308026, acc.: 85.94%] [G loss: 3.428841]\n",
      "epoch:43 step:34231 [D loss: 0.296005, acc.: 86.72%] [G loss: 2.775780]\n",
      "epoch:43 step:34232 [D loss: 0.302574, acc.: 90.62%] [G loss: 2.606251]\n",
      "epoch:43 step:34233 [D loss: 0.347037, acc.: 82.81%] [G loss: 3.330317]\n",
      "epoch:43 step:34234 [D loss: 0.395878, acc.: 85.16%] [G loss: 3.297933]\n",
      "epoch:43 step:34235 [D loss: 0.291568, acc.: 87.50%] [G loss: 3.076421]\n",
      "epoch:43 step:34236 [D loss: 0.284557, acc.: 89.84%] [G loss: 3.536821]\n",
      "epoch:43 step:34237 [D loss: 0.358404, acc.: 83.59%] [G loss: 3.067398]\n",
      "epoch:43 step:34238 [D loss: 0.387599, acc.: 79.69%] [G loss: 2.548403]\n",
      "epoch:43 step:34239 [D loss: 0.271630, acc.: 85.16%] [G loss: 2.683508]\n",
      "epoch:43 step:34240 [D loss: 0.297206, acc.: 86.72%] [G loss: 2.746964]\n",
      "epoch:43 step:34241 [D loss: 0.295188, acc.: 87.50%] [G loss: 2.783014]\n",
      "epoch:43 step:34242 [D loss: 0.258043, acc.: 89.06%] [G loss: 3.447964]\n",
      "epoch:43 step:34243 [D loss: 0.373738, acc.: 82.81%] [G loss: 3.566734]\n",
      "epoch:43 step:34244 [D loss: 0.298840, acc.: 89.84%] [G loss: 3.271277]\n",
      "epoch:43 step:34245 [D loss: 0.336981, acc.: 85.16%] [G loss: 2.887429]\n",
      "epoch:43 step:34246 [D loss: 0.313868, acc.: 86.72%] [G loss: 2.704969]\n",
      "epoch:43 step:34247 [D loss: 0.303557, acc.: 83.59%] [G loss: 3.086554]\n",
      "epoch:43 step:34248 [D loss: 0.304691, acc.: 86.72%] [G loss: 3.536049]\n",
      "epoch:43 step:34249 [D loss: 0.285980, acc.: 82.03%] [G loss: 2.998503]\n",
      "epoch:43 step:34250 [D loss: 0.197650, acc.: 89.84%] [G loss: 3.341624]\n",
      "epoch:43 step:34251 [D loss: 0.304188, acc.: 89.06%] [G loss: 3.532562]\n",
      "epoch:43 step:34252 [D loss: 0.261403, acc.: 90.62%] [G loss: 3.328732]\n",
      "epoch:43 step:34253 [D loss: 0.296183, acc.: 85.16%] [G loss: 2.769445]\n",
      "epoch:43 step:34254 [D loss: 0.322208, acc.: 84.38%] [G loss: 3.635401]\n",
      "epoch:43 step:34255 [D loss: 0.333215, acc.: 84.38%] [G loss: 3.255074]\n",
      "epoch:43 step:34256 [D loss: 0.354311, acc.: 85.16%] [G loss: 3.043375]\n",
      "epoch:43 step:34257 [D loss: 0.343982, acc.: 83.59%] [G loss: 2.480498]\n",
      "epoch:43 step:34258 [D loss: 0.234714, acc.: 89.06%] [G loss: 3.271708]\n",
      "epoch:43 step:34259 [D loss: 0.274180, acc.: 85.94%] [G loss: 3.132639]\n",
      "epoch:43 step:34260 [D loss: 0.306997, acc.: 88.28%] [G loss: 3.215574]\n",
      "epoch:43 step:34261 [D loss: 0.358189, acc.: 85.94%] [G loss: 4.420181]\n",
      "epoch:43 step:34262 [D loss: 0.255012, acc.: 89.84%] [G loss: 3.408014]\n",
      "epoch:43 step:34263 [D loss: 0.349839, acc.: 83.59%] [G loss: 3.485589]\n",
      "epoch:43 step:34264 [D loss: 0.340929, acc.: 89.84%] [G loss: 3.300334]\n",
      "epoch:43 step:34265 [D loss: 0.328109, acc.: 85.94%] [G loss: 2.804657]\n",
      "epoch:43 step:34266 [D loss: 0.258424, acc.: 89.84%] [G loss: 2.673804]\n",
      "epoch:43 step:34267 [D loss: 0.231843, acc.: 89.84%] [G loss: 2.833341]\n",
      "epoch:43 step:34268 [D loss: 0.329483, acc.: 82.03%] [G loss: 2.780131]\n",
      "epoch:43 step:34269 [D loss: 0.231637, acc.: 91.41%] [G loss: 2.715397]\n",
      "epoch:43 step:34270 [D loss: 0.423631, acc.: 84.38%] [G loss: 3.647788]\n",
      "epoch:43 step:34271 [D loss: 0.378437, acc.: 84.38%] [G loss: 3.828305]\n",
      "epoch:43 step:34272 [D loss: 0.376586, acc.: 83.59%] [G loss: 3.194475]\n",
      "epoch:43 step:34273 [D loss: 0.284413, acc.: 87.50%] [G loss: 2.844524]\n",
      "epoch:43 step:34274 [D loss: 0.241003, acc.: 89.84%] [G loss: 3.011194]\n",
      "epoch:43 step:34275 [D loss: 0.349818, acc.: 84.38%] [G loss: 4.032032]\n",
      "epoch:43 step:34276 [D loss: 0.295528, acc.: 85.16%] [G loss: 5.580761]\n",
      "epoch:43 step:34277 [D loss: 0.277474, acc.: 85.94%] [G loss: 4.410111]\n",
      "epoch:43 step:34278 [D loss: 0.191259, acc.: 89.06%] [G loss: 3.598465]\n",
      "epoch:43 step:34279 [D loss: 0.296475, acc.: 84.38%] [G loss: 3.913531]\n",
      "epoch:43 step:34280 [D loss: 0.316855, acc.: 84.38%] [G loss: 3.948093]\n",
      "epoch:43 step:34281 [D loss: 0.214520, acc.: 94.53%] [G loss: 4.013389]\n",
      "epoch:43 step:34282 [D loss: 0.230001, acc.: 91.41%] [G loss: 4.470263]\n",
      "epoch:43 step:34283 [D loss: 0.277725, acc.: 87.50%] [G loss: 3.058246]\n",
      "epoch:43 step:34284 [D loss: 0.229886, acc.: 91.41%] [G loss: 3.701650]\n",
      "epoch:43 step:34285 [D loss: 0.212426, acc.: 88.28%] [G loss: 3.628930]\n",
      "epoch:43 step:34286 [D loss: 0.326431, acc.: 84.38%] [G loss: 4.276536]\n",
      "epoch:43 step:34287 [D loss: 0.324341, acc.: 84.38%] [G loss: 2.877800]\n",
      "epoch:43 step:34288 [D loss: 0.228542, acc.: 92.19%] [G loss: 3.693791]\n",
      "epoch:43 step:34289 [D loss: 0.312169, acc.: 88.28%] [G loss: 3.814225]\n",
      "epoch:43 step:34290 [D loss: 0.314691, acc.: 85.94%] [G loss: 4.238533]\n",
      "epoch:43 step:34291 [D loss: 0.338967, acc.: 90.62%] [G loss: 4.726270]\n",
      "epoch:43 step:34292 [D loss: 0.340967, acc.: 86.72%] [G loss: 4.564279]\n",
      "epoch:43 step:34293 [D loss: 0.242375, acc.: 93.75%] [G loss: 5.027807]\n",
      "epoch:43 step:34294 [D loss: 0.236794, acc.: 89.06%] [G loss: 4.864099]\n",
      "epoch:43 step:34295 [D loss: 0.209438, acc.: 90.62%] [G loss: 5.502636]\n",
      "epoch:43 step:34296 [D loss: 0.271537, acc.: 89.06%] [G loss: 3.862736]\n",
      "epoch:43 step:34297 [D loss: 0.334388, acc.: 84.38%] [G loss: 4.448533]\n",
      "epoch:43 step:34298 [D loss: 0.258907, acc.: 86.72%] [G loss: 3.395494]\n",
      "epoch:43 step:34299 [D loss: 0.255229, acc.: 89.84%] [G loss: 4.579858]\n",
      "epoch:43 step:34300 [D loss: 0.306910, acc.: 85.16%] [G loss: 3.013080]\n",
      "epoch:43 step:34301 [D loss: 0.271444, acc.: 89.06%] [G loss: 3.432012]\n",
      "epoch:43 step:34302 [D loss: 0.201891, acc.: 92.19%] [G loss: 3.980398]\n",
      "epoch:43 step:34303 [D loss: 0.336067, acc.: 82.81%] [G loss: 3.227516]\n",
      "epoch:43 step:34304 [D loss: 0.284254, acc.: 85.94%] [G loss: 3.270717]\n",
      "epoch:43 step:34305 [D loss: 0.340825, acc.: 84.38%] [G loss: 3.919582]\n",
      "epoch:43 step:34306 [D loss: 0.297455, acc.: 89.06%] [G loss: 3.062090]\n",
      "epoch:43 step:34307 [D loss: 0.298213, acc.: 87.50%] [G loss: 3.566166]\n",
      "epoch:43 step:34308 [D loss: 0.342416, acc.: 82.81%] [G loss: 3.571060]\n",
      "epoch:43 step:34309 [D loss: 0.230879, acc.: 90.62%] [G loss: 4.143296]\n",
      "epoch:43 step:34310 [D loss: 0.319099, acc.: 85.94%] [G loss: 6.189740]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:43 step:34311 [D loss: 0.322950, acc.: 82.81%] [G loss: 4.011164]\n",
      "epoch:43 step:34312 [D loss: 0.329623, acc.: 87.50%] [G loss: 3.988371]\n",
      "epoch:43 step:34313 [D loss: 0.413580, acc.: 82.81%] [G loss: 3.387670]\n",
      "epoch:43 step:34314 [D loss: 0.345798, acc.: 86.72%] [G loss: 4.154901]\n",
      "epoch:43 step:34315 [D loss: 0.323064, acc.: 85.16%] [G loss: 7.021446]\n",
      "epoch:43 step:34316 [D loss: 0.236196, acc.: 89.06%] [G loss: 3.919477]\n",
      "epoch:43 step:34317 [D loss: 0.295215, acc.: 88.28%] [G loss: 5.171716]\n",
      "epoch:43 step:34318 [D loss: 0.313602, acc.: 87.50%] [G loss: 2.727387]\n",
      "epoch:43 step:34319 [D loss: 0.243745, acc.: 88.28%] [G loss: 3.266507]\n",
      "epoch:43 step:34320 [D loss: 0.323993, acc.: 84.38%] [G loss: 3.931745]\n",
      "epoch:43 step:34321 [D loss: 0.353524, acc.: 82.81%] [G loss: 4.500109]\n",
      "epoch:43 step:34322 [D loss: 0.355575, acc.: 83.59%] [G loss: 4.251863]\n",
      "epoch:43 step:34323 [D loss: 0.221267, acc.: 89.84%] [G loss: 3.652241]\n",
      "epoch:43 step:34324 [D loss: 0.355254, acc.: 82.03%] [G loss: 3.561730]\n",
      "epoch:43 step:34325 [D loss: 0.353778, acc.: 79.69%] [G loss: 3.931640]\n",
      "epoch:43 step:34326 [D loss: 0.246286, acc.: 90.62%] [G loss: 3.726520]\n",
      "epoch:43 step:34327 [D loss: 0.248108, acc.: 90.62%] [G loss: 3.894172]\n",
      "epoch:43 step:34328 [D loss: 0.314872, acc.: 85.16%] [G loss: 3.316834]\n",
      "epoch:43 step:34329 [D loss: 0.311855, acc.: 85.16%] [G loss: 2.767123]\n",
      "epoch:43 step:34330 [D loss: 0.295770, acc.: 86.72%] [G loss: 3.772963]\n",
      "epoch:43 step:34331 [D loss: 0.239970, acc.: 92.19%] [G loss: 3.957451]\n",
      "epoch:43 step:34332 [D loss: 0.271097, acc.: 87.50%] [G loss: 4.561538]\n",
      "epoch:43 step:34333 [D loss: 0.290015, acc.: 85.94%] [G loss: 3.403808]\n",
      "epoch:43 step:34334 [D loss: 0.353881, acc.: 83.59%] [G loss: 4.221224]\n",
      "epoch:43 step:34335 [D loss: 0.295680, acc.: 85.94%] [G loss: 4.111228]\n",
      "epoch:43 step:34336 [D loss: 0.443389, acc.: 78.91%] [G loss: 3.644351]\n",
      "epoch:43 step:34337 [D loss: 0.239190, acc.: 89.06%] [G loss: 4.909096]\n",
      "epoch:43 step:34338 [D loss: 0.244900, acc.: 90.62%] [G loss: 3.289093]\n",
      "epoch:43 step:34339 [D loss: 0.234953, acc.: 89.84%] [G loss: 4.816084]\n",
      "epoch:43 step:34340 [D loss: 0.270765, acc.: 86.72%] [G loss: 3.543721]\n",
      "epoch:43 step:34341 [D loss: 0.300913, acc.: 87.50%] [G loss: 4.457732]\n",
      "epoch:43 step:34342 [D loss: 0.317301, acc.: 85.16%] [G loss: 2.588210]\n",
      "epoch:43 step:34343 [D loss: 0.425627, acc.: 82.03%] [G loss: 5.833916]\n",
      "epoch:43 step:34344 [D loss: 0.330835, acc.: 84.38%] [G loss: 6.559979]\n",
      "epoch:43 step:34345 [D loss: 0.537570, acc.: 80.47%] [G loss: 5.448378]\n",
      "epoch:43 step:34346 [D loss: 0.758906, acc.: 73.44%] [G loss: 5.608402]\n",
      "epoch:43 step:34347 [D loss: 0.600670, acc.: 68.75%] [G loss: 3.780578]\n",
      "epoch:43 step:34348 [D loss: 0.370203, acc.: 85.16%] [G loss: 4.752655]\n",
      "epoch:43 step:34349 [D loss: 0.227208, acc.: 92.19%] [G loss: 3.843493]\n",
      "epoch:43 step:34350 [D loss: 0.397309, acc.: 78.12%] [G loss: 3.836515]\n",
      "epoch:43 step:34351 [D loss: 0.289800, acc.: 87.50%] [G loss: 6.097667]\n",
      "epoch:43 step:34352 [D loss: 0.321571, acc.: 84.38%] [G loss: 3.654808]\n",
      "epoch:43 step:34353 [D loss: 0.282498, acc.: 85.94%] [G loss: 3.215079]\n",
      "epoch:43 step:34354 [D loss: 0.310730, acc.: 86.72%] [G loss: 3.305108]\n",
      "epoch:43 step:34355 [D loss: 0.351668, acc.: 82.81%] [G loss: 3.755025]\n",
      "epoch:43 step:34356 [D loss: 0.327729, acc.: 86.72%] [G loss: 3.040550]\n",
      "epoch:43 step:34357 [D loss: 0.246450, acc.: 89.84%] [G loss: 3.428043]\n",
      "epoch:43 step:34358 [D loss: 0.296657, acc.: 85.16%] [G loss: 3.757298]\n",
      "epoch:43 step:34359 [D loss: 0.207776, acc.: 90.62%] [G loss: 4.319797]\n",
      "epoch:43 step:34360 [D loss: 0.392091, acc.: 85.16%] [G loss: 3.725694]\n",
      "epoch:43 step:34361 [D loss: 0.244533, acc.: 90.62%] [G loss: 3.419263]\n",
      "epoch:43 step:34362 [D loss: 0.299610, acc.: 87.50%] [G loss: 3.451746]\n",
      "epoch:43 step:34363 [D loss: 0.302572, acc.: 87.50%] [G loss: 3.498639]\n",
      "epoch:43 step:34364 [D loss: 0.257175, acc.: 89.06%] [G loss: 3.667184]\n",
      "epoch:44 step:34365 [D loss: 0.220526, acc.: 92.97%] [G loss: 3.468485]\n",
      "epoch:44 step:34366 [D loss: 0.321970, acc.: 86.72%] [G loss: 3.276659]\n",
      "epoch:44 step:34367 [D loss: 0.383524, acc.: 82.03%] [G loss: 2.630134]\n",
      "epoch:44 step:34368 [D loss: 0.256054, acc.: 89.06%] [G loss: 3.013401]\n",
      "epoch:44 step:34369 [D loss: 0.263106, acc.: 88.28%] [G loss: 3.493418]\n",
      "epoch:44 step:34370 [D loss: 0.316289, acc.: 85.16%] [G loss: 3.551314]\n",
      "epoch:44 step:34371 [D loss: 0.311754, acc.: 85.94%] [G loss: 4.312183]\n",
      "epoch:44 step:34372 [D loss: 0.236085, acc.: 92.19%] [G loss: 2.877973]\n",
      "epoch:44 step:34373 [D loss: 0.304761, acc.: 89.06%] [G loss: 3.695146]\n",
      "epoch:44 step:34374 [D loss: 0.272412, acc.: 85.16%] [G loss: 2.841527]\n",
      "epoch:44 step:34375 [D loss: 0.196946, acc.: 92.19%] [G loss: 4.324470]\n",
      "epoch:44 step:34376 [D loss: 0.321883, acc.: 89.06%] [G loss: 3.297416]\n",
      "epoch:44 step:34377 [D loss: 0.260668, acc.: 89.06%] [G loss: 5.888108]\n",
      "epoch:44 step:34378 [D loss: 0.336173, acc.: 82.03%] [G loss: 4.678434]\n",
      "epoch:44 step:34379 [D loss: 0.309263, acc.: 86.72%] [G loss: 3.753732]\n",
      "epoch:44 step:34380 [D loss: 0.326176, acc.: 86.72%] [G loss: 3.546099]\n",
      "epoch:44 step:34381 [D loss: 0.182678, acc.: 93.75%] [G loss: 3.385664]\n",
      "epoch:44 step:34382 [D loss: 0.370474, acc.: 80.47%] [G loss: 2.308748]\n",
      "epoch:44 step:34383 [D loss: 0.236468, acc.: 92.19%] [G loss: 3.968654]\n",
      "epoch:44 step:34384 [D loss: 0.349357, acc.: 82.03%] [G loss: 2.822733]\n",
      "epoch:44 step:34385 [D loss: 0.398534, acc.: 81.25%] [G loss: 3.117200]\n",
      "epoch:44 step:34386 [D loss: 0.332361, acc.: 84.38%] [G loss: 3.769786]\n",
      "epoch:44 step:34387 [D loss: 0.347967, acc.: 85.94%] [G loss: 3.381595]\n",
      "epoch:44 step:34388 [D loss: 0.251976, acc.: 92.97%] [G loss: 4.163294]\n",
      "epoch:44 step:34389 [D loss: 0.148171, acc.: 96.09%] [G loss: 3.651995]\n",
      "epoch:44 step:34390 [D loss: 0.328419, acc.: 83.59%] [G loss: 4.611252]\n",
      "epoch:44 step:34391 [D loss: 0.305980, acc.: 82.03%] [G loss: 4.587101]\n",
      "epoch:44 step:34392 [D loss: 0.332031, acc.: 81.25%] [G loss: 4.280513]\n",
      "epoch:44 step:34393 [D loss: 0.263848, acc.: 92.19%] [G loss: 3.642504]\n",
      "epoch:44 step:34394 [D loss: 0.213469, acc.: 91.41%] [G loss: 5.256187]\n",
      "epoch:44 step:34395 [D loss: 0.271513, acc.: 89.06%] [G loss: 3.725821]\n",
      "epoch:44 step:34396 [D loss: 0.344918, acc.: 82.81%] [G loss: 3.592590]\n",
      "epoch:44 step:34397 [D loss: 0.323946, acc.: 85.16%] [G loss: 4.081337]\n",
      "epoch:44 step:34398 [D loss: 0.298192, acc.: 89.06%] [G loss: 3.195193]\n",
      "epoch:44 step:34399 [D loss: 0.295542, acc.: 88.28%] [G loss: 5.159085]\n",
      "epoch:44 step:34400 [D loss: 0.471627, acc.: 81.25%] [G loss: 3.199368]\n",
      "##############\n",
      "[0.87054366 0.85110832 0.80679497 0.81898987 0.80095573 0.84420843\n",
      " 0.8743802  0.82352204 0.8266267  0.82365425]\n",
      "##########\n",
      "epoch:44 step:34401 [D loss: 0.249934, acc.: 90.62%] [G loss: 2.936033]\n",
      "epoch:44 step:34402 [D loss: 0.268048, acc.: 86.72%] [G loss: 3.152493]\n",
      "epoch:44 step:34403 [D loss: 0.378489, acc.: 84.38%] [G loss: 2.748814]\n",
      "epoch:44 step:34404 [D loss: 0.289651, acc.: 85.94%] [G loss: 3.613867]\n",
      "epoch:44 step:34405 [D loss: 0.318019, acc.: 89.06%] [G loss: 2.535651]\n",
      "epoch:44 step:34406 [D loss: 0.264350, acc.: 89.84%] [G loss: 3.057829]\n",
      "epoch:44 step:34407 [D loss: 0.390297, acc.: 85.94%] [G loss: 2.537872]\n",
      "epoch:44 step:34408 [D loss: 0.399665, acc.: 82.03%] [G loss: 3.094240]\n",
      "epoch:44 step:34409 [D loss: 0.260162, acc.: 90.62%] [G loss: 2.808131]\n",
      "epoch:44 step:34410 [D loss: 0.333293, acc.: 86.72%] [G loss: 2.392416]\n",
      "epoch:44 step:34411 [D loss: 0.484519, acc.: 76.56%] [G loss: 4.383042]\n",
      "epoch:44 step:34412 [D loss: 0.359825, acc.: 83.59%] [G loss: 2.856046]\n",
      "epoch:44 step:34413 [D loss: 0.319837, acc.: 84.38%] [G loss: 3.064658]\n",
      "epoch:44 step:34414 [D loss: 0.340324, acc.: 82.03%] [G loss: 3.347034]\n",
      "epoch:44 step:34415 [D loss: 0.300121, acc.: 85.94%] [G loss: 5.658382]\n",
      "epoch:44 step:34416 [D loss: 0.257809, acc.: 88.28%] [G loss: 3.365070]\n",
      "epoch:44 step:34417 [D loss: 0.280396, acc.: 89.06%] [G loss: 5.997345]\n",
      "epoch:44 step:34418 [D loss: 0.290619, acc.: 85.94%] [G loss: 5.574152]\n",
      "epoch:44 step:34419 [D loss: 0.215889, acc.: 94.53%] [G loss: 4.177672]\n",
      "epoch:44 step:34420 [D loss: 0.224101, acc.: 90.62%] [G loss: 3.958311]\n",
      "epoch:44 step:34421 [D loss: 0.230752, acc.: 92.19%] [G loss: 4.210909]\n",
      "epoch:44 step:34422 [D loss: 0.363714, acc.: 82.03%] [G loss: 2.502103]\n",
      "epoch:44 step:34423 [D loss: 0.308316, acc.: 85.94%] [G loss: 4.191259]\n",
      "epoch:44 step:34424 [D loss: 0.457416, acc.: 78.12%] [G loss: 3.429652]\n",
      "epoch:44 step:34425 [D loss: 0.327476, acc.: 86.72%] [G loss: 3.673838]\n",
      "epoch:44 step:34426 [D loss: 0.345424, acc.: 84.38%] [G loss: 3.012972]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:44 step:34427 [D loss: 0.278834, acc.: 87.50%] [G loss: 3.329504]\n",
      "epoch:44 step:34428 [D loss: 0.263565, acc.: 87.50%] [G loss: 3.579326]\n",
      "epoch:44 step:34429 [D loss: 0.309230, acc.: 85.16%] [G loss: 2.846744]\n",
      "epoch:44 step:34430 [D loss: 0.280303, acc.: 89.06%] [G loss: 3.177566]\n",
      "epoch:44 step:34431 [D loss: 0.274556, acc.: 86.72%] [G loss: 3.170326]\n",
      "epoch:44 step:34432 [D loss: 0.390915, acc.: 78.91%] [G loss: 2.561577]\n",
      "epoch:44 step:34433 [D loss: 0.358979, acc.: 82.03%] [G loss: 3.088760]\n",
      "epoch:44 step:34434 [D loss: 0.274289, acc.: 88.28%] [G loss: 2.891383]\n",
      "epoch:44 step:34435 [D loss: 0.387716, acc.: 78.91%] [G loss: 3.367691]\n",
      "epoch:44 step:34436 [D loss: 0.385708, acc.: 81.25%] [G loss: 3.988744]\n",
      "epoch:44 step:34437 [D loss: 0.569322, acc.: 75.00%] [G loss: 6.683731]\n",
      "epoch:44 step:34438 [D loss: 0.996078, acc.: 70.31%] [G loss: 9.691591]\n",
      "epoch:44 step:34439 [D loss: 1.935406, acc.: 69.53%] [G loss: 6.418150]\n",
      "epoch:44 step:34440 [D loss: 0.912762, acc.: 66.41%] [G loss: 5.169674]\n",
      "epoch:44 step:34441 [D loss: 0.784946, acc.: 71.88%] [G loss: 4.132836]\n",
      "epoch:44 step:34442 [D loss: 0.706865, acc.: 68.75%] [G loss: 2.912831]\n",
      "epoch:44 step:34443 [D loss: 0.373357, acc.: 84.38%] [G loss: 3.786563]\n",
      "epoch:44 step:34444 [D loss: 0.352766, acc.: 86.72%] [G loss: 3.490818]\n",
      "epoch:44 step:34445 [D loss: 0.329297, acc.: 85.94%] [G loss: 4.132417]\n",
      "epoch:44 step:34446 [D loss: 0.335800, acc.: 85.94%] [G loss: 2.853369]\n",
      "epoch:44 step:34447 [D loss: 0.288952, acc.: 89.06%] [G loss: 3.028550]\n",
      "epoch:44 step:34448 [D loss: 0.235232, acc.: 89.84%] [G loss: 3.521539]\n",
      "epoch:44 step:34449 [D loss: 0.282665, acc.: 87.50%] [G loss: 3.454039]\n",
      "epoch:44 step:34450 [D loss: 0.306111, acc.: 86.72%] [G loss: 3.422607]\n",
      "epoch:44 step:34451 [D loss: 0.254551, acc.: 89.06%] [G loss: 3.140581]\n",
      "epoch:44 step:34452 [D loss: 0.288781, acc.: 88.28%] [G loss: 3.490337]\n",
      "epoch:44 step:34453 [D loss: 0.292718, acc.: 88.28%] [G loss: 2.893193]\n",
      "epoch:44 step:34454 [D loss: 0.393965, acc.: 84.38%] [G loss: 2.516414]\n",
      "epoch:44 step:34455 [D loss: 0.251380, acc.: 89.06%] [G loss: 2.754826]\n",
      "epoch:44 step:34456 [D loss: 0.285457, acc.: 89.06%] [G loss: 3.065419]\n",
      "epoch:44 step:34457 [D loss: 0.263772, acc.: 86.72%] [G loss: 2.647122]\n",
      "epoch:44 step:34458 [D loss: 0.370843, acc.: 85.16%] [G loss: 2.808028]\n",
      "epoch:44 step:34459 [D loss: 0.339915, acc.: 86.72%] [G loss: 2.487731]\n",
      "epoch:44 step:34460 [D loss: 0.295076, acc.: 87.50%] [G loss: 2.994323]\n",
      "epoch:44 step:34461 [D loss: 0.355064, acc.: 82.03%] [G loss: 3.275448]\n",
      "epoch:44 step:34462 [D loss: 0.361028, acc.: 85.16%] [G loss: 2.549721]\n",
      "epoch:44 step:34463 [D loss: 0.314098, acc.: 87.50%] [G loss: 2.944561]\n",
      "epoch:44 step:34464 [D loss: 0.287578, acc.: 90.62%] [G loss: 2.597932]\n",
      "epoch:44 step:34465 [D loss: 0.340513, acc.: 87.50%] [G loss: 2.475993]\n",
      "epoch:44 step:34466 [D loss: 0.300885, acc.: 83.59%] [G loss: 2.501736]\n",
      "epoch:44 step:34467 [D loss: 0.355623, acc.: 84.38%] [G loss: 3.029073]\n",
      "epoch:44 step:34468 [D loss: 0.283884, acc.: 86.72%] [G loss: 2.983384]\n",
      "epoch:44 step:34469 [D loss: 0.236819, acc.: 89.84%] [G loss: 2.510968]\n",
      "epoch:44 step:34470 [D loss: 0.247460, acc.: 89.06%] [G loss: 2.982102]\n",
      "epoch:44 step:34471 [D loss: 0.247209, acc.: 89.06%] [G loss: 2.903430]\n",
      "epoch:44 step:34472 [D loss: 0.231977, acc.: 89.06%] [G loss: 3.460714]\n",
      "epoch:44 step:34473 [D loss: 0.198936, acc.: 92.19%] [G loss: 3.708420]\n",
      "epoch:44 step:34474 [D loss: 0.221955, acc.: 92.19%] [G loss: 2.672817]\n",
      "epoch:44 step:34475 [D loss: 0.313498, acc.: 85.16%] [G loss: 2.771789]\n",
      "epoch:44 step:34476 [D loss: 0.302628, acc.: 91.41%] [G loss: 2.911092]\n",
      "epoch:44 step:34477 [D loss: 0.307772, acc.: 84.38%] [G loss: 3.265132]\n",
      "epoch:44 step:34478 [D loss: 0.293298, acc.: 85.94%] [G loss: 2.466847]\n",
      "epoch:44 step:34479 [D loss: 0.293408, acc.: 88.28%] [G loss: 2.874020]\n",
      "epoch:44 step:34480 [D loss: 0.311756, acc.: 87.50%] [G loss: 2.611005]\n",
      "epoch:44 step:34481 [D loss: 0.259745, acc.: 89.06%] [G loss: 3.076125]\n",
      "epoch:44 step:34482 [D loss: 0.338961, acc.: 84.38%] [G loss: 2.898482]\n",
      "epoch:44 step:34483 [D loss: 0.263978, acc.: 88.28%] [G loss: 2.912114]\n",
      "epoch:44 step:34484 [D loss: 0.221125, acc.: 92.19%] [G loss: 3.517462]\n",
      "epoch:44 step:34485 [D loss: 0.289183, acc.: 89.06%] [G loss: 4.011119]\n",
      "epoch:44 step:34486 [D loss: 0.374031, acc.: 86.72%] [G loss: 4.032728]\n",
      "epoch:44 step:34487 [D loss: 0.263072, acc.: 86.72%] [G loss: 4.036283]\n",
      "epoch:44 step:34488 [D loss: 0.294088, acc.: 87.50%] [G loss: 4.154099]\n",
      "epoch:44 step:34489 [D loss: 0.372366, acc.: 83.59%] [G loss: 4.014506]\n",
      "epoch:44 step:34490 [D loss: 0.316209, acc.: 85.16%] [G loss: 3.414869]\n",
      "epoch:44 step:34491 [D loss: 0.322438, acc.: 85.16%] [G loss: 2.705778]\n",
      "epoch:44 step:34492 [D loss: 0.172503, acc.: 93.75%] [G loss: 3.542442]\n",
      "epoch:44 step:34493 [D loss: 0.247922, acc.: 89.84%] [G loss: 2.902384]\n",
      "epoch:44 step:34494 [D loss: 0.305881, acc.: 86.72%] [G loss: 3.057279]\n",
      "epoch:44 step:34495 [D loss: 0.243667, acc.: 91.41%] [G loss: 3.009273]\n",
      "epoch:44 step:34496 [D loss: 0.328546, acc.: 87.50%] [G loss: 2.534605]\n",
      "epoch:44 step:34497 [D loss: 0.256055, acc.: 89.84%] [G loss: 2.638224]\n",
      "epoch:44 step:34498 [D loss: 0.327286, acc.: 85.94%] [G loss: 2.750254]\n",
      "epoch:44 step:34499 [D loss: 0.252131, acc.: 86.72%] [G loss: 3.502465]\n",
      "epoch:44 step:34500 [D loss: 0.266250, acc.: 89.84%] [G loss: 3.457228]\n",
      "epoch:44 step:34501 [D loss: 0.298732, acc.: 86.72%] [G loss: 3.056562]\n",
      "epoch:44 step:34502 [D loss: 0.279739, acc.: 87.50%] [G loss: 3.199848]\n",
      "epoch:44 step:34503 [D loss: 0.268970, acc.: 89.84%] [G loss: 4.598290]\n",
      "epoch:44 step:34504 [D loss: 0.301342, acc.: 88.28%] [G loss: 3.238392]\n",
      "epoch:44 step:34505 [D loss: 0.264779, acc.: 87.50%] [G loss: 3.433587]\n",
      "epoch:44 step:34506 [D loss: 0.393748, acc.: 81.25%] [G loss: 3.250068]\n",
      "epoch:44 step:34507 [D loss: 0.220439, acc.: 91.41%] [G loss: 3.396038]\n",
      "epoch:44 step:34508 [D loss: 0.292583, acc.: 86.72%] [G loss: 3.101506]\n",
      "epoch:44 step:34509 [D loss: 0.364560, acc.: 83.59%] [G loss: 3.114995]\n",
      "epoch:44 step:34510 [D loss: 0.289200, acc.: 85.94%] [G loss: 2.817055]\n",
      "epoch:44 step:34511 [D loss: 0.271383, acc.: 87.50%] [G loss: 2.760870]\n",
      "epoch:44 step:34512 [D loss: 0.276694, acc.: 89.06%] [G loss: 3.311728]\n",
      "epoch:44 step:34513 [D loss: 0.250150, acc.: 90.62%] [G loss: 3.865192]\n",
      "epoch:44 step:34514 [D loss: 0.205935, acc.: 92.97%] [G loss: 3.057007]\n",
      "epoch:44 step:34515 [D loss: 0.239466, acc.: 90.62%] [G loss: 3.410653]\n",
      "epoch:44 step:34516 [D loss: 0.314675, acc.: 85.16%] [G loss: 3.260285]\n",
      "epoch:44 step:34517 [D loss: 0.341233, acc.: 83.59%] [G loss: 2.833930]\n",
      "epoch:44 step:34518 [D loss: 0.265485, acc.: 88.28%] [G loss: 3.110887]\n",
      "epoch:44 step:34519 [D loss: 0.358834, acc.: 85.94%] [G loss: 3.098445]\n",
      "epoch:44 step:34520 [D loss: 0.366555, acc.: 85.16%] [G loss: 3.032673]\n",
      "epoch:44 step:34521 [D loss: 0.411416, acc.: 80.47%] [G loss: 2.684825]\n",
      "epoch:44 step:34522 [D loss: 0.304572, acc.: 85.16%] [G loss: 3.130743]\n",
      "epoch:44 step:34523 [D loss: 0.236793, acc.: 91.41%] [G loss: 2.567993]\n",
      "epoch:44 step:34524 [D loss: 0.317199, acc.: 88.28%] [G loss: 2.839976]\n",
      "epoch:44 step:34525 [D loss: 0.281307, acc.: 89.84%] [G loss: 2.829923]\n",
      "epoch:44 step:34526 [D loss: 0.191678, acc.: 94.53%] [G loss: 3.215758]\n",
      "epoch:44 step:34527 [D loss: 0.229527, acc.: 90.62%] [G loss: 3.562347]\n",
      "epoch:44 step:34528 [D loss: 0.275530, acc.: 89.06%] [G loss: 3.053685]\n",
      "epoch:44 step:34529 [D loss: 0.297185, acc.: 88.28%] [G loss: 2.648773]\n",
      "epoch:44 step:34530 [D loss: 0.230621, acc.: 92.19%] [G loss: 3.005829]\n",
      "epoch:44 step:34531 [D loss: 0.312016, acc.: 87.50%] [G loss: 3.432999]\n",
      "epoch:44 step:34532 [D loss: 0.310206, acc.: 87.50%] [G loss: 2.960556]\n",
      "epoch:44 step:34533 [D loss: 0.281826, acc.: 87.50%] [G loss: 3.375555]\n",
      "epoch:44 step:34534 [D loss: 0.225303, acc.: 90.62%] [G loss: 3.041754]\n",
      "epoch:44 step:34535 [D loss: 0.390601, acc.: 82.03%] [G loss: 3.424826]\n",
      "epoch:44 step:34536 [D loss: 0.336876, acc.: 82.03%] [G loss: 3.472666]\n",
      "epoch:44 step:34537 [D loss: 0.271173, acc.: 89.06%] [G loss: 5.298781]\n",
      "epoch:44 step:34538 [D loss: 0.444627, acc.: 79.69%] [G loss: 4.093641]\n",
      "epoch:44 step:34539 [D loss: 0.207963, acc.: 92.97%] [G loss: 4.973153]\n",
      "epoch:44 step:34540 [D loss: 0.245985, acc.: 88.28%] [G loss: 3.483181]\n",
      "epoch:44 step:34541 [D loss: 0.302564, acc.: 85.16%] [G loss: 3.819337]\n",
      "epoch:44 step:34542 [D loss: 0.378323, acc.: 83.59%] [G loss: 2.776761]\n",
      "epoch:44 step:34543 [D loss: 0.327219, acc.: 86.72%] [G loss: 3.606302]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:44 step:34544 [D loss: 0.303858, acc.: 85.16%] [G loss: 3.076668]\n",
      "epoch:44 step:34545 [D loss: 0.239207, acc.: 89.84%] [G loss: 3.480011]\n",
      "epoch:44 step:34546 [D loss: 0.321225, acc.: 85.16%] [G loss: 3.693169]\n",
      "epoch:44 step:34547 [D loss: 0.191794, acc.: 93.75%] [G loss: 3.229379]\n",
      "epoch:44 step:34548 [D loss: 0.308299, acc.: 87.50%] [G loss: 2.730073]\n",
      "epoch:44 step:34549 [D loss: 0.251705, acc.: 90.62%] [G loss: 3.308557]\n",
      "epoch:44 step:34550 [D loss: 0.353468, acc.: 83.59%] [G loss: 2.688865]\n",
      "epoch:44 step:34551 [D loss: 0.424506, acc.: 85.94%] [G loss: 2.980371]\n",
      "epoch:44 step:34552 [D loss: 0.274704, acc.: 85.16%] [G loss: 2.521199]\n",
      "epoch:44 step:34553 [D loss: 0.249714, acc.: 89.84%] [G loss: 3.868191]\n",
      "epoch:44 step:34554 [D loss: 0.437613, acc.: 77.34%] [G loss: 4.275021]\n",
      "epoch:44 step:34555 [D loss: 0.262742, acc.: 88.28%] [G loss: 2.966769]\n",
      "epoch:44 step:34556 [D loss: 0.240031, acc.: 92.19%] [G loss: 4.490576]\n",
      "epoch:44 step:34557 [D loss: 0.271359, acc.: 89.06%] [G loss: 2.813179]\n",
      "epoch:44 step:34558 [D loss: 0.263019, acc.: 89.06%] [G loss: 3.156474]\n",
      "epoch:44 step:34559 [D loss: 0.370367, acc.: 85.16%] [G loss: 2.844171]\n",
      "epoch:44 step:34560 [D loss: 0.283031, acc.: 88.28%] [G loss: 3.267565]\n",
      "epoch:44 step:34561 [D loss: 0.488093, acc.: 80.47%] [G loss: 5.954379]\n",
      "epoch:44 step:34562 [D loss: 0.540024, acc.: 75.00%] [G loss: 5.250278]\n",
      "epoch:44 step:34563 [D loss: 0.334076, acc.: 84.38%] [G loss: 5.698145]\n",
      "epoch:44 step:34564 [D loss: 0.220472, acc.: 90.62%] [G loss: 6.359329]\n",
      "epoch:44 step:34565 [D loss: 0.234189, acc.: 89.06%] [G loss: 4.529335]\n",
      "epoch:44 step:34566 [D loss: 0.305284, acc.: 92.19%] [G loss: 5.658265]\n",
      "epoch:44 step:34567 [D loss: 0.327342, acc.: 82.03%] [G loss: 4.957574]\n",
      "epoch:44 step:34568 [D loss: 0.317210, acc.: 86.72%] [G loss: 4.781409]\n",
      "epoch:44 step:34569 [D loss: 0.356723, acc.: 82.81%] [G loss: 3.864096]\n",
      "epoch:44 step:34570 [D loss: 0.275515, acc.: 87.50%] [G loss: 4.020275]\n",
      "epoch:44 step:34571 [D loss: 0.264621, acc.: 88.28%] [G loss: 4.884776]\n",
      "epoch:44 step:34572 [D loss: 0.276290, acc.: 86.72%] [G loss: 3.741547]\n",
      "epoch:44 step:34573 [D loss: 0.284578, acc.: 84.38%] [G loss: 3.444927]\n",
      "epoch:44 step:34574 [D loss: 0.212821, acc.: 91.41%] [G loss: 3.961615]\n",
      "epoch:44 step:34575 [D loss: 0.284651, acc.: 85.94%] [G loss: 3.220376]\n",
      "epoch:44 step:34576 [D loss: 0.191643, acc.: 92.97%] [G loss: 2.909549]\n",
      "epoch:44 step:34577 [D loss: 0.380301, acc.: 83.59%] [G loss: 2.907784]\n",
      "epoch:44 step:34578 [D loss: 0.363872, acc.: 85.94%] [G loss: 2.952660]\n",
      "epoch:44 step:34579 [D loss: 0.242988, acc.: 90.62%] [G loss: 4.369367]\n",
      "epoch:44 step:34580 [D loss: 0.256659, acc.: 87.50%] [G loss: 3.473256]\n",
      "epoch:44 step:34581 [D loss: 0.277745, acc.: 89.84%] [G loss: 4.000907]\n",
      "epoch:44 step:34582 [D loss: 0.293716, acc.: 87.50%] [G loss: 3.905646]\n",
      "epoch:44 step:34583 [D loss: 0.325721, acc.: 85.16%] [G loss: 4.072015]\n",
      "epoch:44 step:34584 [D loss: 0.324837, acc.: 87.50%] [G loss: 3.189323]\n",
      "epoch:44 step:34585 [D loss: 0.249037, acc.: 89.06%] [G loss: 3.552765]\n",
      "epoch:44 step:34586 [D loss: 0.186360, acc.: 93.75%] [G loss: 3.692262]\n",
      "epoch:44 step:34587 [D loss: 0.242349, acc.: 90.62%] [G loss: 3.595005]\n",
      "epoch:44 step:34588 [D loss: 0.287482, acc.: 85.94%] [G loss: 2.746926]\n",
      "epoch:44 step:34589 [D loss: 0.338416, acc.: 85.16%] [G loss: 3.745872]\n",
      "epoch:44 step:34590 [D loss: 0.208715, acc.: 92.19%] [G loss: 3.151202]\n",
      "epoch:44 step:34591 [D loss: 0.232893, acc.: 87.50%] [G loss: 3.434100]\n",
      "epoch:44 step:34592 [D loss: 0.299048, acc.: 88.28%] [G loss: 2.547858]\n",
      "epoch:44 step:34593 [D loss: 0.447036, acc.: 80.47%] [G loss: 5.218617]\n",
      "epoch:44 step:34594 [D loss: 0.308736, acc.: 87.50%] [G loss: 6.886643]\n",
      "epoch:44 step:34595 [D loss: 0.348039, acc.: 82.81%] [G loss: 2.844285]\n",
      "epoch:44 step:34596 [D loss: 0.266743, acc.: 89.06%] [G loss: 4.464481]\n",
      "epoch:44 step:34597 [D loss: 0.408167, acc.: 79.69%] [G loss: 3.206785]\n",
      "epoch:44 step:34598 [D loss: 0.381069, acc.: 80.47%] [G loss: 3.134870]\n",
      "epoch:44 step:34599 [D loss: 0.435807, acc.: 80.47%] [G loss: 2.393177]\n",
      "epoch:44 step:34600 [D loss: 0.266211, acc.: 89.06%] [G loss: 3.016164]\n",
      "##############\n",
      "[0.87050765 0.8594555  0.79741645 0.7844351  0.78735653 0.82580159\n",
      " 0.87339321 0.84876556 0.80915928 0.81369362]\n",
      "##########\n",
      "epoch:44 step:34601 [D loss: 0.284803, acc.: 89.06%] [G loss: 3.141855]\n",
      "epoch:44 step:34602 [D loss: 0.274937, acc.: 85.94%] [G loss: 3.909248]\n",
      "epoch:44 step:34603 [D loss: 0.257286, acc.: 91.41%] [G loss: 2.933946]\n",
      "epoch:44 step:34604 [D loss: 0.260917, acc.: 87.50%] [G loss: 3.282886]\n",
      "epoch:44 step:34605 [D loss: 0.336446, acc.: 81.25%] [G loss: 3.385485]\n",
      "epoch:44 step:34606 [D loss: 0.287661, acc.: 87.50%] [G loss: 3.056404]\n",
      "epoch:44 step:34607 [D loss: 0.314985, acc.: 82.81%] [G loss: 4.187854]\n",
      "epoch:44 step:34608 [D loss: 0.275071, acc.: 89.84%] [G loss: 3.248739]\n",
      "epoch:44 step:34609 [D loss: 0.329565, acc.: 80.47%] [G loss: 4.807599]\n",
      "epoch:44 step:34610 [D loss: 0.271918, acc.: 88.28%] [G loss: 3.239013]\n",
      "epoch:44 step:34611 [D loss: 0.375506, acc.: 82.03%] [G loss: 2.807813]\n",
      "epoch:44 step:34612 [D loss: 0.308127, acc.: 87.50%] [G loss: 2.879401]\n",
      "epoch:44 step:34613 [D loss: 0.355985, acc.: 85.94%] [G loss: 5.245173]\n",
      "epoch:44 step:34614 [D loss: 0.641894, acc.: 77.34%] [G loss: 6.580584]\n",
      "epoch:44 step:34615 [D loss: 0.570207, acc.: 82.03%] [G loss: 5.611496]\n",
      "epoch:44 step:34616 [D loss: 0.323913, acc.: 86.72%] [G loss: 4.913160]\n",
      "epoch:44 step:34617 [D loss: 0.177649, acc.: 94.53%] [G loss: 4.649300]\n",
      "epoch:44 step:34618 [D loss: 0.418484, acc.: 80.47%] [G loss: 4.056919]\n",
      "epoch:44 step:34619 [D loss: 0.212711, acc.: 86.72%] [G loss: 6.336971]\n",
      "epoch:44 step:34620 [D loss: 0.321163, acc.: 88.28%] [G loss: 5.866848]\n",
      "epoch:44 step:34621 [D loss: 0.250627, acc.: 92.97%] [G loss: 5.288247]\n",
      "epoch:44 step:34622 [D loss: 0.274703, acc.: 85.94%] [G loss: 4.111825]\n",
      "epoch:44 step:34623 [D loss: 0.308108, acc.: 88.28%] [G loss: 3.365650]\n",
      "epoch:44 step:34624 [D loss: 0.223243, acc.: 89.84%] [G loss: 4.326905]\n",
      "epoch:44 step:34625 [D loss: 0.385199, acc.: 79.69%] [G loss: 4.524811]\n",
      "epoch:44 step:34626 [D loss: 0.270864, acc.: 86.72%] [G loss: 4.468985]\n",
      "epoch:44 step:34627 [D loss: 0.248628, acc.: 90.62%] [G loss: 4.592520]\n",
      "epoch:44 step:34628 [D loss: 0.321858, acc.: 82.81%] [G loss: 3.664366]\n",
      "epoch:44 step:34629 [D loss: 0.260139, acc.: 90.62%] [G loss: 3.696373]\n",
      "epoch:44 step:34630 [D loss: 0.248566, acc.: 89.84%] [G loss: 3.517331]\n",
      "epoch:44 step:34631 [D loss: 0.168801, acc.: 94.53%] [G loss: 3.618714]\n",
      "epoch:44 step:34632 [D loss: 0.238139, acc.: 90.62%] [G loss: 3.324078]\n",
      "epoch:44 step:34633 [D loss: 0.245904, acc.: 89.84%] [G loss: 3.319686]\n",
      "epoch:44 step:34634 [D loss: 0.228077, acc.: 91.41%] [G loss: 3.416333]\n",
      "epoch:44 step:34635 [D loss: 0.246080, acc.: 88.28%] [G loss: 3.601938]\n",
      "epoch:44 step:34636 [D loss: 0.301325, acc.: 85.94%] [G loss: 3.436002]\n",
      "epoch:44 step:34637 [D loss: 0.255707, acc.: 90.62%] [G loss: 3.609047]\n",
      "epoch:44 step:34638 [D loss: 0.353050, acc.: 85.16%] [G loss: 3.574357]\n",
      "epoch:44 step:34639 [D loss: 0.362821, acc.: 82.81%] [G loss: 2.489464]\n",
      "epoch:44 step:34640 [D loss: 0.355092, acc.: 83.59%] [G loss: 2.705353]\n",
      "epoch:44 step:34641 [D loss: 0.263227, acc.: 89.06%] [G loss: 2.631779]\n",
      "epoch:44 step:34642 [D loss: 0.425924, acc.: 83.59%] [G loss: 3.046814]\n",
      "epoch:44 step:34643 [D loss: 0.318182, acc.: 87.50%] [G loss: 2.810236]\n",
      "epoch:44 step:34644 [D loss: 0.253207, acc.: 89.84%] [G loss: 3.313924]\n",
      "epoch:44 step:34645 [D loss: 0.295442, acc.: 85.16%] [G loss: 2.755042]\n",
      "epoch:44 step:34646 [D loss: 0.234638, acc.: 88.28%] [G loss: 3.030062]\n",
      "epoch:44 step:34647 [D loss: 0.372346, acc.: 82.03%] [G loss: 3.196171]\n",
      "epoch:44 step:34648 [D loss: 0.337678, acc.: 82.81%] [G loss: 3.013502]\n",
      "epoch:44 step:34649 [D loss: 0.255458, acc.: 90.62%] [G loss: 2.712176]\n",
      "epoch:44 step:34650 [D loss: 0.343084, acc.: 85.16%] [G loss: 3.188764]\n",
      "epoch:44 step:34651 [D loss: 0.296594, acc.: 87.50%] [G loss: 3.165013]\n",
      "epoch:44 step:34652 [D loss: 0.299020, acc.: 90.62%] [G loss: 2.988909]\n",
      "epoch:44 step:34653 [D loss: 0.291510, acc.: 86.72%] [G loss: 3.157485]\n",
      "epoch:44 step:34654 [D loss: 0.372126, acc.: 82.81%] [G loss: 4.020964]\n",
      "epoch:44 step:34655 [D loss: 0.314068, acc.: 85.94%] [G loss: 3.831690]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:44 step:34656 [D loss: 0.258592, acc.: 86.72%] [G loss: 3.977190]\n",
      "epoch:44 step:34657 [D loss: 0.329910, acc.: 87.50%] [G loss: 2.782431]\n",
      "epoch:44 step:34658 [D loss: 0.332238, acc.: 85.94%] [G loss: 2.983782]\n",
      "epoch:44 step:34659 [D loss: 0.331519, acc.: 84.38%] [G loss: 3.337256]\n",
      "epoch:44 step:34660 [D loss: 0.338212, acc.: 82.03%] [G loss: 2.494968]\n",
      "epoch:44 step:34661 [D loss: 0.352123, acc.: 80.47%] [G loss: 2.835283]\n",
      "epoch:44 step:34662 [D loss: 0.288537, acc.: 86.72%] [G loss: 2.896085]\n",
      "epoch:44 step:34663 [D loss: 0.364852, acc.: 84.38%] [G loss: 3.105314]\n",
      "epoch:44 step:34664 [D loss: 0.248448, acc.: 89.06%] [G loss: 3.588348]\n",
      "epoch:44 step:34665 [D loss: 0.340364, acc.: 83.59%] [G loss: 2.592561]\n",
      "epoch:44 step:34666 [D loss: 0.255838, acc.: 88.28%] [G loss: 3.199779]\n",
      "epoch:44 step:34667 [D loss: 0.349829, acc.: 83.59%] [G loss: 2.950674]\n",
      "epoch:44 step:34668 [D loss: 0.306081, acc.: 86.72%] [G loss: 3.545281]\n",
      "epoch:44 step:34669 [D loss: 0.291716, acc.: 86.72%] [G loss: 3.436764]\n",
      "epoch:44 step:34670 [D loss: 0.232400, acc.: 91.41%] [G loss: 3.326153]\n",
      "epoch:44 step:34671 [D loss: 0.267811, acc.: 89.06%] [G loss: 2.559174]\n",
      "epoch:44 step:34672 [D loss: 0.324316, acc.: 84.38%] [G loss: 3.033963]\n",
      "epoch:44 step:34673 [D loss: 0.283702, acc.: 87.50%] [G loss: 4.144309]\n",
      "epoch:44 step:34674 [D loss: 0.244975, acc.: 87.50%] [G loss: 4.299584]\n",
      "epoch:44 step:34675 [D loss: 0.281862, acc.: 89.06%] [G loss: 4.619271]\n",
      "epoch:44 step:34676 [D loss: 0.359180, acc.: 78.91%] [G loss: 3.604599]\n",
      "epoch:44 step:34677 [D loss: 0.354520, acc.: 83.59%] [G loss: 5.240940]\n",
      "epoch:44 step:34678 [D loss: 0.322837, acc.: 82.03%] [G loss: 4.864026]\n",
      "epoch:44 step:34679 [D loss: 0.395551, acc.: 81.25%] [G loss: 4.010137]\n",
      "epoch:44 step:34680 [D loss: 0.294340, acc.: 89.84%] [G loss: 3.648342]\n",
      "epoch:44 step:34681 [D loss: 0.388271, acc.: 86.72%] [G loss: 3.193327]\n",
      "epoch:44 step:34682 [D loss: 0.388790, acc.: 82.81%] [G loss: 3.547584]\n",
      "epoch:44 step:34683 [D loss: 0.260317, acc.: 85.16%] [G loss: 3.922864]\n",
      "epoch:44 step:34684 [D loss: 0.310226, acc.: 83.59%] [G loss: 3.600868]\n",
      "epoch:44 step:34685 [D loss: 0.297048, acc.: 83.59%] [G loss: 4.718870]\n",
      "epoch:44 step:34686 [D loss: 0.257420, acc.: 89.06%] [G loss: 3.823870]\n",
      "epoch:44 step:34687 [D loss: 0.322434, acc.: 85.16%] [G loss: 3.432935]\n",
      "epoch:44 step:34688 [D loss: 0.348753, acc.: 85.94%] [G loss: 3.492802]\n",
      "epoch:44 step:34689 [D loss: 0.295456, acc.: 84.38%] [G loss: 3.491273]\n",
      "epoch:44 step:34690 [D loss: 0.395411, acc.: 86.72%] [G loss: 2.487965]\n",
      "epoch:44 step:34691 [D loss: 0.274237, acc.: 87.50%] [G loss: 3.506988]\n",
      "epoch:44 step:34692 [D loss: 0.348598, acc.: 82.81%] [G loss: 3.238970]\n",
      "epoch:44 step:34693 [D loss: 0.280982, acc.: 86.72%] [G loss: 2.786911]\n",
      "epoch:44 step:34694 [D loss: 0.399978, acc.: 78.12%] [G loss: 2.924571]\n",
      "epoch:44 step:34695 [D loss: 0.369118, acc.: 82.03%] [G loss: 3.067338]\n",
      "epoch:44 step:34696 [D loss: 0.286983, acc.: 86.72%] [G loss: 3.438542]\n",
      "epoch:44 step:34697 [D loss: 0.407800, acc.: 80.47%] [G loss: 3.091792]\n",
      "epoch:44 step:34698 [D loss: 0.269898, acc.: 89.06%] [G loss: 2.577025]\n",
      "epoch:44 step:34699 [D loss: 0.277035, acc.: 86.72%] [G loss: 3.082973]\n",
      "epoch:44 step:34700 [D loss: 0.283507, acc.: 89.06%] [G loss: 3.428913]\n",
      "epoch:44 step:34701 [D loss: 0.272248, acc.: 89.84%] [G loss: 3.150980]\n",
      "epoch:44 step:34702 [D loss: 0.277235, acc.: 89.84%] [G loss: 3.014122]\n",
      "epoch:44 step:34703 [D loss: 0.290926, acc.: 88.28%] [G loss: 3.931923]\n",
      "epoch:44 step:34704 [D loss: 0.327493, acc.: 85.16%] [G loss: 3.927145]\n",
      "epoch:44 step:34705 [D loss: 0.234876, acc.: 92.19%] [G loss: 3.882983]\n",
      "epoch:44 step:34706 [D loss: 0.235491, acc.: 86.72%] [G loss: 4.016752]\n",
      "epoch:44 step:34707 [D loss: 0.283718, acc.: 85.94%] [G loss: 2.995354]\n",
      "epoch:44 step:34708 [D loss: 0.376188, acc.: 82.81%] [G loss: 2.855597]\n",
      "epoch:44 step:34709 [D loss: 0.296379, acc.: 87.50%] [G loss: 3.139363]\n",
      "epoch:44 step:34710 [D loss: 0.260783, acc.: 89.06%] [G loss: 2.743529]\n",
      "epoch:44 step:34711 [D loss: 0.253034, acc.: 89.06%] [G loss: 4.766476]\n",
      "epoch:44 step:34712 [D loss: 0.215071, acc.: 90.62%] [G loss: 4.528138]\n",
      "epoch:44 step:34713 [D loss: 0.297405, acc.: 87.50%] [G loss: 4.547119]\n",
      "epoch:44 step:34714 [D loss: 0.310032, acc.: 84.38%] [G loss: 3.444643]\n",
      "epoch:44 step:34715 [D loss: 0.266911, acc.: 85.16%] [G loss: 3.880546]\n",
      "epoch:44 step:34716 [D loss: 0.349392, acc.: 85.94%] [G loss: 3.169049]\n",
      "epoch:44 step:34717 [D loss: 0.342413, acc.: 85.16%] [G loss: 2.909193]\n",
      "epoch:44 step:34718 [D loss: 0.209350, acc.: 90.62%] [G loss: 5.321327]\n",
      "epoch:44 step:34719 [D loss: 0.281938, acc.: 87.50%] [G loss: 3.493277]\n",
      "epoch:44 step:34720 [D loss: 0.269508, acc.: 89.84%] [G loss: 3.006758]\n",
      "epoch:44 step:34721 [D loss: 0.239189, acc.: 89.84%] [G loss: 3.132701]\n",
      "epoch:44 step:34722 [D loss: 0.296069, acc.: 87.50%] [G loss: 3.231242]\n",
      "epoch:44 step:34723 [D loss: 0.356649, acc.: 85.94%] [G loss: 3.230019]\n",
      "epoch:44 step:34724 [D loss: 0.339393, acc.: 84.38%] [G loss: 3.086332]\n",
      "epoch:44 step:34725 [D loss: 0.387165, acc.: 83.59%] [G loss: 2.586923]\n",
      "epoch:44 step:34726 [D loss: 0.236591, acc.: 92.19%] [G loss: 2.734746]\n",
      "epoch:44 step:34727 [D loss: 0.297668, acc.: 89.06%] [G loss: 3.425013]\n",
      "epoch:44 step:34728 [D loss: 0.348425, acc.: 82.81%] [G loss: 4.919870]\n",
      "epoch:44 step:34729 [D loss: 0.283885, acc.: 90.62%] [G loss: 3.412374]\n",
      "epoch:44 step:34730 [D loss: 0.295842, acc.: 87.50%] [G loss: 3.329736]\n",
      "epoch:44 step:34731 [D loss: 0.236139, acc.: 91.41%] [G loss: 3.117482]\n",
      "epoch:44 step:34732 [D loss: 0.313822, acc.: 84.38%] [G loss: 3.662899]\n",
      "epoch:44 step:34733 [D loss: 0.324305, acc.: 84.38%] [G loss: 3.745710]\n",
      "epoch:44 step:34734 [D loss: 0.272459, acc.: 85.16%] [G loss: 3.417227]\n",
      "epoch:44 step:34735 [D loss: 0.269637, acc.: 87.50%] [G loss: 3.559066]\n",
      "epoch:44 step:34736 [D loss: 0.296169, acc.: 86.72%] [G loss: 3.556202]\n",
      "epoch:44 step:34737 [D loss: 0.290680, acc.: 87.50%] [G loss: 4.342726]\n",
      "epoch:44 step:34738 [D loss: 0.345210, acc.: 83.59%] [G loss: 4.608679]\n",
      "epoch:44 step:34739 [D loss: 0.224676, acc.: 88.28%] [G loss: 2.809444]\n",
      "epoch:44 step:34740 [D loss: 0.268196, acc.: 88.28%] [G loss: 2.978302]\n",
      "epoch:44 step:34741 [D loss: 0.308324, acc.: 87.50%] [G loss: 3.366434]\n",
      "epoch:44 step:34742 [D loss: 0.278158, acc.: 86.72%] [G loss: 2.829943]\n",
      "epoch:44 step:34743 [D loss: 0.296192, acc.: 85.16%] [G loss: 3.475511]\n",
      "epoch:44 step:34744 [D loss: 0.294262, acc.: 88.28%] [G loss: 3.559321]\n",
      "epoch:44 step:34745 [D loss: 0.355865, acc.: 82.03%] [G loss: 3.611619]\n",
      "epoch:44 step:34746 [D loss: 0.279237, acc.: 89.06%] [G loss: 2.752352]\n",
      "epoch:44 step:34747 [D loss: 0.226854, acc.: 89.84%] [G loss: 3.828077]\n",
      "epoch:44 step:34748 [D loss: 0.198355, acc.: 90.62%] [G loss: 3.537187]\n",
      "epoch:44 step:34749 [D loss: 0.238953, acc.: 89.06%] [G loss: 4.063978]\n",
      "epoch:44 step:34750 [D loss: 0.367523, acc.: 85.16%] [G loss: 3.024853]\n",
      "epoch:44 step:34751 [D loss: 0.266876, acc.: 89.84%] [G loss: 3.779616]\n",
      "epoch:44 step:34752 [D loss: 0.352936, acc.: 87.50%] [G loss: 3.646791]\n",
      "epoch:44 step:34753 [D loss: 0.296655, acc.: 87.50%] [G loss: 4.405313]\n",
      "epoch:44 step:34754 [D loss: 0.323093, acc.: 84.38%] [G loss: 4.410096]\n",
      "epoch:44 step:34755 [D loss: 0.336803, acc.: 85.16%] [G loss: 5.215729]\n",
      "epoch:44 step:34756 [D loss: 0.282187, acc.: 85.94%] [G loss: 2.900666]\n",
      "epoch:44 step:34757 [D loss: 0.294904, acc.: 86.72%] [G loss: 3.550739]\n",
      "epoch:44 step:34758 [D loss: 0.277981, acc.: 88.28%] [G loss: 3.449702]\n",
      "epoch:44 step:34759 [D loss: 0.424416, acc.: 80.47%] [G loss: 4.770269]\n",
      "epoch:44 step:34760 [D loss: 0.334058, acc.: 82.81%] [G loss: 5.344400]\n",
      "epoch:44 step:34761 [D loss: 0.334699, acc.: 83.59%] [G loss: 3.522181]\n",
      "epoch:44 step:34762 [D loss: 0.296347, acc.: 89.06%] [G loss: 5.208256]\n",
      "epoch:44 step:34763 [D loss: 0.358898, acc.: 84.38%] [G loss: 3.366179]\n",
      "epoch:44 step:34764 [D loss: 0.237710, acc.: 87.50%] [G loss: 3.541835]\n",
      "epoch:44 step:34765 [D loss: 0.285043, acc.: 85.16%] [G loss: 3.282029]\n",
      "epoch:44 step:34766 [D loss: 0.258260, acc.: 89.84%] [G loss: 3.254195]\n",
      "epoch:44 step:34767 [D loss: 0.241775, acc.: 90.62%] [G loss: 3.274507]\n",
      "epoch:44 step:34768 [D loss: 0.254504, acc.: 91.41%] [G loss: 3.101426]\n",
      "epoch:44 step:34769 [D loss: 0.290720, acc.: 87.50%] [G loss: 2.918764]\n",
      "epoch:44 step:34770 [D loss: 0.383857, acc.: 81.25%] [G loss: 3.211369]\n",
      "epoch:44 step:34771 [D loss: 0.313206, acc.: 81.25%] [G loss: 4.200018]\n",
      "epoch:44 step:34772 [D loss: 0.348380, acc.: 84.38%] [G loss: 2.910643]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:44 step:34773 [D loss: 0.184548, acc.: 93.75%] [G loss: 3.244361]\n",
      "epoch:44 step:34774 [D loss: 0.268736, acc.: 89.06%] [G loss: 2.346608]\n",
      "epoch:44 step:34775 [D loss: 0.337148, acc.: 82.81%] [G loss: 2.841442]\n",
      "epoch:44 step:34776 [D loss: 0.279101, acc.: 89.84%] [G loss: 3.110584]\n",
      "epoch:44 step:34777 [D loss: 0.315794, acc.: 87.50%] [G loss: 2.932511]\n",
      "epoch:44 step:34778 [D loss: 0.259321, acc.: 90.62%] [G loss: 4.175481]\n",
      "epoch:44 step:34779 [D loss: 0.320491, acc.: 89.06%] [G loss: 7.204428]\n",
      "epoch:44 step:34780 [D loss: 0.540257, acc.: 74.22%] [G loss: 3.678723]\n",
      "epoch:44 step:34781 [D loss: 0.371931, acc.: 84.38%] [G loss: 4.417127]\n",
      "epoch:44 step:34782 [D loss: 0.347785, acc.: 84.38%] [G loss: 3.176500]\n",
      "epoch:44 step:34783 [D loss: 0.378594, acc.: 82.81%] [G loss: 4.131058]\n",
      "epoch:44 step:34784 [D loss: 0.299109, acc.: 89.06%] [G loss: 4.560972]\n",
      "epoch:44 step:34785 [D loss: 0.225647, acc.: 89.84%] [G loss: 3.583240]\n",
      "epoch:44 step:34786 [D loss: 0.291501, acc.: 88.28%] [G loss: 3.570330]\n",
      "epoch:44 step:34787 [D loss: 0.299614, acc.: 86.72%] [G loss: 3.148286]\n",
      "epoch:44 step:34788 [D loss: 0.282695, acc.: 88.28%] [G loss: 2.728968]\n",
      "epoch:44 step:34789 [D loss: 0.350235, acc.: 82.03%] [G loss: 3.645562]\n",
      "epoch:44 step:34790 [D loss: 0.320334, acc.: 84.38%] [G loss: 2.796884]\n",
      "epoch:44 step:34791 [D loss: 0.202134, acc.: 92.97%] [G loss: 4.239741]\n",
      "epoch:44 step:34792 [D loss: 0.340918, acc.: 82.81%] [G loss: 3.249741]\n",
      "epoch:44 step:34793 [D loss: 0.400891, acc.: 84.38%] [G loss: 2.611955]\n",
      "epoch:44 step:34794 [D loss: 0.205512, acc.: 91.41%] [G loss: 3.590080]\n",
      "epoch:44 step:34795 [D loss: 0.322947, acc.: 84.38%] [G loss: 3.232159]\n",
      "epoch:44 step:34796 [D loss: 0.400046, acc.: 79.69%] [G loss: 2.883363]\n",
      "epoch:44 step:34797 [D loss: 0.234374, acc.: 89.84%] [G loss: 3.521192]\n",
      "epoch:44 step:34798 [D loss: 0.241352, acc.: 89.84%] [G loss: 4.206976]\n",
      "epoch:44 step:34799 [D loss: 0.323087, acc.: 87.50%] [G loss: 4.103897]\n",
      "epoch:44 step:34800 [D loss: 0.481951, acc.: 75.78%] [G loss: 4.160249]\n",
      "##############\n",
      "[0.85261772 0.8546671  0.80329983 0.80517965 0.74740928 0.82270045\n",
      " 0.88063033 0.86083773 0.82130689 0.82290556]\n",
      "##########\n",
      "epoch:44 step:34801 [D loss: 0.354561, acc.: 85.94%] [G loss: 5.113875]\n",
      "epoch:44 step:34802 [D loss: 0.424786, acc.: 82.81%] [G loss: 3.635628]\n",
      "epoch:44 step:34803 [D loss: 0.403284, acc.: 78.12%] [G loss: 3.937691]\n",
      "epoch:44 step:34804 [D loss: 0.305218, acc.: 86.72%] [G loss: 4.194179]\n",
      "epoch:44 step:34805 [D loss: 0.369209, acc.: 82.81%] [G loss: 3.028295]\n",
      "epoch:44 step:34806 [D loss: 0.297001, acc.: 85.94%] [G loss: 3.897845]\n",
      "epoch:44 step:34807 [D loss: 0.433837, acc.: 79.69%] [G loss: 3.549243]\n",
      "epoch:44 step:34808 [D loss: 0.272916, acc.: 87.50%] [G loss: 3.165131]\n",
      "epoch:44 step:34809 [D loss: 0.217678, acc.: 90.62%] [G loss: 3.515790]\n",
      "epoch:44 step:34810 [D loss: 0.258324, acc.: 85.16%] [G loss: 3.887185]\n",
      "epoch:44 step:34811 [D loss: 0.297488, acc.: 87.50%] [G loss: 2.783835]\n",
      "epoch:44 step:34812 [D loss: 0.308479, acc.: 83.59%] [G loss: 3.955174]\n",
      "epoch:44 step:34813 [D loss: 0.275394, acc.: 89.06%] [G loss: 3.540729]\n",
      "epoch:44 step:34814 [D loss: 0.236597, acc.: 88.28%] [G loss: 3.803493]\n",
      "epoch:44 step:34815 [D loss: 0.248599, acc.: 89.06%] [G loss: 4.005465]\n",
      "epoch:44 step:34816 [D loss: 0.347200, acc.: 86.72%] [G loss: 3.585817]\n",
      "epoch:44 step:34817 [D loss: 0.241451, acc.: 87.50%] [G loss: 4.032125]\n",
      "epoch:44 step:34818 [D loss: 0.284134, acc.: 84.38%] [G loss: 4.269699]\n",
      "epoch:44 step:34819 [D loss: 0.330804, acc.: 86.72%] [G loss: 3.715201]\n",
      "epoch:44 step:34820 [D loss: 0.278531, acc.: 85.16%] [G loss: 3.223613]\n",
      "epoch:44 step:34821 [D loss: 0.266941, acc.: 88.28%] [G loss: 4.063712]\n",
      "epoch:44 step:34822 [D loss: 0.239234, acc.: 91.41%] [G loss: 2.909265]\n",
      "epoch:44 step:34823 [D loss: 0.264568, acc.: 86.72%] [G loss: 3.266235]\n",
      "epoch:44 step:34824 [D loss: 0.383348, acc.: 81.25%] [G loss: 4.319658]\n",
      "epoch:44 step:34825 [D loss: 0.288814, acc.: 87.50%] [G loss: 3.331966]\n",
      "epoch:44 step:34826 [D loss: 0.395496, acc.: 79.69%] [G loss: 3.289599]\n",
      "epoch:44 step:34827 [D loss: 0.292074, acc.: 86.72%] [G loss: 3.527101]\n",
      "epoch:44 step:34828 [D loss: 0.344331, acc.: 83.59%] [G loss: 3.002240]\n",
      "epoch:44 step:34829 [D loss: 0.397177, acc.: 82.03%] [G loss: 2.613235]\n",
      "epoch:44 step:34830 [D loss: 0.271795, acc.: 89.06%] [G loss: 3.254040]\n",
      "epoch:44 step:34831 [D loss: 0.264891, acc.: 87.50%] [G loss: 3.332702]\n",
      "epoch:44 step:34832 [D loss: 0.316731, acc.: 82.81%] [G loss: 2.957903]\n",
      "epoch:44 step:34833 [D loss: 0.339334, acc.: 85.94%] [G loss: 4.300764]\n",
      "epoch:44 step:34834 [D loss: 0.257713, acc.: 87.50%] [G loss: 4.000578]\n",
      "epoch:44 step:34835 [D loss: 0.253826, acc.: 89.84%] [G loss: 5.017699]\n",
      "epoch:44 step:34836 [D loss: 0.266941, acc.: 88.28%] [G loss: 4.414277]\n",
      "epoch:44 step:34837 [D loss: 0.377117, acc.: 86.72%] [G loss: 3.933795]\n",
      "epoch:44 step:34838 [D loss: 0.293489, acc.: 87.50%] [G loss: 3.808784]\n",
      "epoch:44 step:34839 [D loss: 0.348592, acc.: 84.38%] [G loss: 2.876737]\n",
      "epoch:44 step:34840 [D loss: 0.327228, acc.: 85.94%] [G loss: 2.785948]\n",
      "epoch:44 step:34841 [D loss: 0.186107, acc.: 92.97%] [G loss: 4.132347]\n",
      "epoch:44 step:34842 [D loss: 0.274040, acc.: 86.72%] [G loss: 3.221072]\n",
      "epoch:44 step:34843 [D loss: 0.276411, acc.: 86.72%] [G loss: 3.775909]\n",
      "epoch:44 step:34844 [D loss: 0.237485, acc.: 90.62%] [G loss: 3.802428]\n",
      "epoch:44 step:34845 [D loss: 0.306937, acc.: 85.94%] [G loss: 3.445595]\n",
      "epoch:44 step:34846 [D loss: 0.306514, acc.: 86.72%] [G loss: 3.375600]\n",
      "epoch:44 step:34847 [D loss: 0.327536, acc.: 86.72%] [G loss: 3.031825]\n",
      "epoch:44 step:34848 [D loss: 0.361070, acc.: 79.69%] [G loss: 4.244663]\n",
      "epoch:44 step:34849 [D loss: 0.290639, acc.: 85.94%] [G loss: 4.241982]\n",
      "epoch:44 step:34850 [D loss: 0.371284, acc.: 81.25%] [G loss: 3.063552]\n",
      "epoch:44 step:34851 [D loss: 0.274910, acc.: 87.50%] [G loss: 3.339465]\n",
      "epoch:44 step:34852 [D loss: 0.246135, acc.: 89.84%] [G loss: 3.611999]\n",
      "epoch:44 step:34853 [D loss: 0.342276, acc.: 84.38%] [G loss: 3.911114]\n",
      "epoch:44 step:34854 [D loss: 0.328676, acc.: 85.16%] [G loss: 3.740381]\n",
      "epoch:44 step:34855 [D loss: 0.333450, acc.: 83.59%] [G loss: 5.567534]\n",
      "epoch:44 step:34856 [D loss: 0.384243, acc.: 84.38%] [G loss: 3.643839]\n",
      "epoch:44 step:34857 [D loss: 0.279541, acc.: 86.72%] [G loss: 5.000100]\n",
      "epoch:44 step:34858 [D loss: 0.332713, acc.: 85.16%] [G loss: 3.375118]\n",
      "epoch:44 step:34859 [D loss: 0.305825, acc.: 88.28%] [G loss: 3.649055]\n",
      "epoch:44 step:34860 [D loss: 0.386589, acc.: 82.03%] [G loss: 3.976735]\n",
      "epoch:44 step:34861 [D loss: 0.356362, acc.: 85.94%] [G loss: 5.473777]\n",
      "epoch:44 step:34862 [D loss: 0.359372, acc.: 83.59%] [G loss: 3.441907]\n",
      "epoch:44 step:34863 [D loss: 0.301972, acc.: 85.16%] [G loss: 4.149182]\n",
      "epoch:44 step:34864 [D loss: 0.245818, acc.: 91.41%] [G loss: 4.981506]\n",
      "epoch:44 step:34865 [D loss: 0.329790, acc.: 84.38%] [G loss: 3.310694]\n",
      "epoch:44 step:34866 [D loss: 0.235732, acc.: 89.84%] [G loss: 3.543962]\n",
      "epoch:44 step:34867 [D loss: 0.205829, acc.: 92.97%] [G loss: 3.820717]\n",
      "epoch:44 step:34868 [D loss: 0.227076, acc.: 89.84%] [G loss: 3.237741]\n",
      "epoch:44 step:34869 [D loss: 0.215003, acc.: 91.41%] [G loss: 3.128885]\n",
      "epoch:44 step:34870 [D loss: 0.314748, acc.: 86.72%] [G loss: 2.867229]\n",
      "epoch:44 step:34871 [D loss: 0.300417, acc.: 87.50%] [G loss: 3.026455]\n",
      "epoch:44 step:34872 [D loss: 0.335627, acc.: 80.47%] [G loss: 2.914718]\n",
      "epoch:44 step:34873 [D loss: 0.314315, acc.: 84.38%] [G loss: 2.966221]\n",
      "epoch:44 step:34874 [D loss: 0.285057, acc.: 89.06%] [G loss: 3.303014]\n",
      "epoch:44 step:34875 [D loss: 0.316617, acc.: 85.16%] [G loss: 3.643817]\n",
      "epoch:44 step:34876 [D loss: 0.240320, acc.: 88.28%] [G loss: 4.225016]\n",
      "epoch:44 step:34877 [D loss: 0.245695, acc.: 90.62%] [G loss: 4.801419]\n",
      "epoch:44 step:34878 [D loss: 0.212379, acc.: 91.41%] [G loss: 4.881207]\n",
      "epoch:44 step:34879 [D loss: 0.278867, acc.: 87.50%] [G loss: 3.646506]\n",
      "epoch:44 step:34880 [D loss: 0.243435, acc.: 89.84%] [G loss: 4.199734]\n",
      "epoch:44 step:34881 [D loss: 0.257965, acc.: 91.41%] [G loss: 3.534377]\n",
      "epoch:44 step:34882 [D loss: 0.274580, acc.: 84.38%] [G loss: 3.934265]\n",
      "epoch:44 step:34883 [D loss: 0.202760, acc.: 91.41%] [G loss: 3.283849]\n",
      "epoch:44 step:34884 [D loss: 0.280816, acc.: 88.28%] [G loss: 3.995662]\n",
      "epoch:44 step:34885 [D loss: 0.310315, acc.: 85.94%] [G loss: 3.303927]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:44 step:34886 [D loss: 0.212491, acc.: 90.62%] [G loss: 3.471454]\n",
      "epoch:44 step:34887 [D loss: 0.310099, acc.: 88.28%] [G loss: 4.228319]\n",
      "epoch:44 step:34888 [D loss: 0.272788, acc.: 89.06%] [G loss: 3.373872]\n",
      "epoch:44 step:34889 [D loss: 0.243466, acc.: 89.06%] [G loss: 3.711371]\n",
      "epoch:44 step:34890 [D loss: 0.409240, acc.: 80.47%] [G loss: 3.217031]\n",
      "epoch:44 step:34891 [D loss: 0.292935, acc.: 87.50%] [G loss: 4.032508]\n",
      "epoch:44 step:34892 [D loss: 0.329248, acc.: 89.06%] [G loss: 3.180296]\n",
      "epoch:44 step:34893 [D loss: 0.300917, acc.: 84.38%] [G loss: 3.557051]\n",
      "epoch:44 step:34894 [D loss: 0.398691, acc.: 82.81%] [G loss: 3.094435]\n",
      "epoch:44 step:34895 [D loss: 0.282785, acc.: 86.72%] [G loss: 3.442710]\n",
      "epoch:44 step:34896 [D loss: 0.242520, acc.: 92.19%] [G loss: 3.398135]\n",
      "epoch:44 step:34897 [D loss: 0.448736, acc.: 81.25%] [G loss: 4.229577]\n",
      "epoch:44 step:34898 [D loss: 0.326723, acc.: 83.59%] [G loss: 4.570044]\n",
      "epoch:44 step:34899 [D loss: 0.223384, acc.: 89.84%] [G loss: 7.029633]\n",
      "epoch:44 step:34900 [D loss: 0.202448, acc.: 90.62%] [G loss: 6.291991]\n",
      "epoch:44 step:34901 [D loss: 0.277655, acc.: 84.38%] [G loss: 5.483316]\n",
      "epoch:44 step:34902 [D loss: 0.192089, acc.: 86.72%] [G loss: 5.473063]\n",
      "epoch:44 step:34903 [D loss: 0.205389, acc.: 89.06%] [G loss: 5.814771]\n",
      "epoch:44 step:34904 [D loss: 0.280299, acc.: 87.50%] [G loss: 5.722037]\n",
      "epoch:44 step:34905 [D loss: 0.234404, acc.: 88.28%] [G loss: 5.428720]\n",
      "epoch:44 step:34906 [D loss: 0.397851, acc.: 79.69%] [G loss: 5.921250]\n",
      "epoch:44 step:34907 [D loss: 0.685090, acc.: 69.53%] [G loss: 3.526721]\n",
      "epoch:44 step:34908 [D loss: 0.897373, acc.: 78.12%] [G loss: 10.268253]\n",
      "epoch:44 step:34909 [D loss: 1.308985, acc.: 60.16%] [G loss: 5.262344]\n",
      "epoch:44 step:34910 [D loss: 0.492167, acc.: 81.25%] [G loss: 2.999508]\n",
      "epoch:44 step:34911 [D loss: 0.305774, acc.: 84.38%] [G loss: 3.504057]\n",
      "epoch:44 step:34912 [D loss: 0.239872, acc.: 91.41%] [G loss: 3.514313]\n",
      "epoch:44 step:34913 [D loss: 0.224759, acc.: 90.62%] [G loss: 3.329801]\n",
      "epoch:44 step:34914 [D loss: 0.260870, acc.: 89.06%] [G loss: 2.998296]\n",
      "epoch:44 step:34915 [D loss: 0.310298, acc.: 86.72%] [G loss: 4.150279]\n",
      "epoch:44 step:34916 [D loss: 0.427160, acc.: 82.81%] [G loss: 2.700925]\n",
      "epoch:44 step:34917 [D loss: 0.244163, acc.: 87.50%] [G loss: 3.191247]\n",
      "epoch:44 step:34918 [D loss: 0.299507, acc.: 86.72%] [G loss: 3.403354]\n",
      "epoch:44 step:34919 [D loss: 0.371991, acc.: 79.69%] [G loss: 2.616188]\n",
      "epoch:44 step:34920 [D loss: 0.421338, acc.: 78.12%] [G loss: 2.965451]\n",
      "epoch:44 step:34921 [D loss: 0.253548, acc.: 90.62%] [G loss: 3.026367]\n",
      "epoch:44 step:34922 [D loss: 0.306507, acc.: 88.28%] [G loss: 3.735336]\n",
      "epoch:44 step:34923 [D loss: 0.238061, acc.: 93.75%] [G loss: 3.608662]\n",
      "epoch:44 step:34924 [D loss: 0.324609, acc.: 80.47%] [G loss: 3.343366]\n",
      "epoch:44 step:34925 [D loss: 0.309076, acc.: 88.28%] [G loss: 3.081648]\n",
      "epoch:44 step:34926 [D loss: 0.307230, acc.: 85.94%] [G loss: 3.126083]\n",
      "epoch:44 step:34927 [D loss: 0.268456, acc.: 87.50%] [G loss: 3.264097]\n",
      "epoch:44 step:34928 [D loss: 0.271616, acc.: 87.50%] [G loss: 3.698673]\n",
      "epoch:44 step:34929 [D loss: 0.213367, acc.: 91.41%] [G loss: 3.917654]\n",
      "epoch:44 step:34930 [D loss: 0.259755, acc.: 89.06%] [G loss: 3.264865]\n",
      "epoch:44 step:34931 [D loss: 0.317060, acc.: 86.72%] [G loss: 2.588065]\n",
      "epoch:44 step:34932 [D loss: 0.330573, acc.: 81.25%] [G loss: 3.788599]\n",
      "epoch:44 step:34933 [D loss: 0.260310, acc.: 89.06%] [G loss: 3.495887]\n",
      "epoch:44 step:34934 [D loss: 0.265295, acc.: 92.19%] [G loss: 4.096656]\n",
      "epoch:44 step:34935 [D loss: 0.289977, acc.: 87.50%] [G loss: 3.119722]\n",
      "epoch:44 step:34936 [D loss: 0.282071, acc.: 86.72%] [G loss: 3.086185]\n",
      "epoch:44 step:34937 [D loss: 0.357755, acc.: 82.03%] [G loss: 3.530389]\n",
      "epoch:44 step:34938 [D loss: 0.245398, acc.: 90.62%] [G loss: 3.260843]\n",
      "epoch:44 step:34939 [D loss: 0.382051, acc.: 80.47%] [G loss: 3.224356]\n",
      "epoch:44 step:34940 [D loss: 0.252496, acc.: 92.19%] [G loss: 2.687107]\n",
      "epoch:44 step:34941 [D loss: 0.242315, acc.: 91.41%] [G loss: 3.063005]\n",
      "epoch:44 step:34942 [D loss: 0.295324, acc.: 84.38%] [G loss: 3.871210]\n",
      "epoch:44 step:34943 [D loss: 0.235264, acc.: 89.84%] [G loss: 3.716813]\n",
      "epoch:44 step:34944 [D loss: 0.319696, acc.: 84.38%] [G loss: 3.854373]\n",
      "epoch:44 step:34945 [D loss: 0.218994, acc.: 91.41%] [G loss: 3.664388]\n",
      "epoch:44 step:34946 [D loss: 0.384671, acc.: 79.69%] [G loss: 2.863169]\n",
      "epoch:44 step:34947 [D loss: 0.328446, acc.: 84.38%] [G loss: 2.366407]\n",
      "epoch:44 step:34948 [D loss: 0.289404, acc.: 88.28%] [G loss: 2.577188]\n",
      "epoch:44 step:34949 [D loss: 0.203214, acc.: 90.62%] [G loss: 2.721057]\n",
      "epoch:44 step:34950 [D loss: 0.275478, acc.: 87.50%] [G loss: 2.141874]\n",
      "epoch:44 step:34951 [D loss: 0.253179, acc.: 89.84%] [G loss: 3.086875]\n",
      "epoch:44 step:34952 [D loss: 0.222588, acc.: 92.97%] [G loss: 2.725219]\n",
      "epoch:44 step:34953 [D loss: 0.258634, acc.: 89.84%] [G loss: 2.978275]\n",
      "epoch:44 step:34954 [D loss: 0.272436, acc.: 90.62%] [G loss: 3.357481]\n",
      "epoch:44 step:34955 [D loss: 0.402646, acc.: 82.81%] [G loss: 4.241232]\n",
      "epoch:44 step:34956 [D loss: 0.320816, acc.: 85.16%] [G loss: 3.299864]\n",
      "epoch:44 step:34957 [D loss: 0.251284, acc.: 88.28%] [G loss: 3.559906]\n",
      "epoch:44 step:34958 [D loss: 0.286544, acc.: 88.28%] [G loss: 3.472783]\n",
      "epoch:44 step:34959 [D loss: 0.311634, acc.: 86.72%] [G loss: 3.231192]\n",
      "epoch:44 step:34960 [D loss: 0.323046, acc.: 88.28%] [G loss: 3.334993]\n",
      "epoch:44 step:34961 [D loss: 0.211318, acc.: 91.41%] [G loss: 3.580840]\n",
      "epoch:44 step:34962 [D loss: 0.330199, acc.: 85.94%] [G loss: 3.368722]\n",
      "epoch:44 step:34963 [D loss: 0.363387, acc.: 85.16%] [G loss: 3.656410]\n",
      "epoch:44 step:34964 [D loss: 0.269595, acc.: 90.62%] [G loss: 2.684783]\n",
      "epoch:44 step:34965 [D loss: 0.340037, acc.: 86.72%] [G loss: 2.745246]\n",
      "epoch:44 step:34966 [D loss: 0.260235, acc.: 88.28%] [G loss: 4.024038]\n",
      "epoch:44 step:34967 [D loss: 0.297341, acc.: 87.50%] [G loss: 4.030251]\n",
      "epoch:44 step:34968 [D loss: 0.266938, acc.: 88.28%] [G loss: 3.179795]\n",
      "epoch:44 step:34969 [D loss: 0.303678, acc.: 84.38%] [G loss: 3.868504]\n",
      "epoch:44 step:34970 [D loss: 0.299880, acc.: 86.72%] [G loss: 3.265307]\n",
      "epoch:44 step:34971 [D loss: 0.294267, acc.: 88.28%] [G loss: 2.418792]\n",
      "epoch:44 step:34972 [D loss: 0.360938, acc.: 83.59%] [G loss: 3.105135]\n",
      "epoch:44 step:34973 [D loss: 0.292631, acc.: 85.94%] [G loss: 2.884787]\n",
      "epoch:44 step:34974 [D loss: 0.293375, acc.: 87.50%] [G loss: 4.068816]\n",
      "epoch:44 step:34975 [D loss: 0.355909, acc.: 79.69%] [G loss: 3.881204]\n",
      "epoch:44 step:34976 [D loss: 0.277486, acc.: 88.28%] [G loss: 2.880026]\n",
      "epoch:44 step:34977 [D loss: 0.381548, acc.: 85.94%] [G loss: 3.300494]\n",
      "epoch:44 step:34978 [D loss: 0.271146, acc.: 89.06%] [G loss: 3.705096]\n",
      "epoch:44 step:34979 [D loss: 0.284088, acc.: 85.94%] [G loss: 3.952228]\n",
      "epoch:44 step:34980 [D loss: 0.205384, acc.: 92.97%] [G loss: 4.467264]\n",
      "epoch:44 step:34981 [D loss: 0.277898, acc.: 85.16%] [G loss: 4.828168]\n",
      "epoch:44 step:34982 [D loss: 0.239108, acc.: 88.28%] [G loss: 5.861674]\n",
      "epoch:44 step:34983 [D loss: 0.325981, acc.: 88.28%] [G loss: 6.236167]\n",
      "epoch:44 step:34984 [D loss: 0.248880, acc.: 89.06%] [G loss: 5.626388]\n",
      "epoch:44 step:34985 [D loss: 0.287275, acc.: 89.06%] [G loss: 5.490880]\n",
      "epoch:44 step:34986 [D loss: 0.180615, acc.: 92.19%] [G loss: 4.263395]\n",
      "epoch:44 step:34987 [D loss: 0.373338, acc.: 84.38%] [G loss: 3.426793]\n",
      "epoch:44 step:34988 [D loss: 0.156762, acc.: 96.09%] [G loss: 3.672096]\n",
      "epoch:44 step:34989 [D loss: 0.403963, acc.: 79.69%] [G loss: 3.448756]\n",
      "epoch:44 step:34990 [D loss: 0.288880, acc.: 85.16%] [G loss: 3.086750]\n",
      "epoch:44 step:34991 [D loss: 0.251599, acc.: 88.28%] [G loss: 3.471167]\n",
      "epoch:44 step:34992 [D loss: 0.330521, acc.: 85.94%] [G loss: 2.788909]\n",
      "epoch:44 step:34993 [D loss: 0.382597, acc.: 83.59%] [G loss: 2.823933]\n",
      "epoch:44 step:34994 [D loss: 0.267982, acc.: 86.72%] [G loss: 4.036636]\n",
      "epoch:44 step:34995 [D loss: 0.325873, acc.: 84.38%] [G loss: 2.967477]\n",
      "epoch:44 step:34996 [D loss: 0.355601, acc.: 84.38%] [G loss: 3.842393]\n",
      "epoch:44 step:34997 [D loss: 0.440058, acc.: 78.12%] [G loss: 3.706172]\n",
      "epoch:44 step:34998 [D loss: 0.521782, acc.: 77.34%] [G loss: 8.104597]\n",
      "epoch:44 step:34999 [D loss: 1.331704, acc.: 69.53%] [G loss: 9.137554]\n",
      "epoch:44 step:35000 [D loss: 1.769993, acc.: 64.84%] [G loss: 3.807964]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##############\n",
      "[0.84462967 0.86244259 0.8282565  0.8407747  0.75044982 0.81493168\n",
      " 0.88418565 0.84838026 0.81367676 0.83501454]\n",
      "##########\n",
      "epoch:44 step:35001 [D loss: 0.607279, acc.: 82.81%] [G loss: 4.208808]\n",
      "epoch:44 step:35002 [D loss: 0.654482, acc.: 71.09%] [G loss: 2.849678]\n",
      "epoch:44 step:35003 [D loss: 0.381563, acc.: 82.03%] [G loss: 3.918762]\n",
      "epoch:44 step:35004 [D loss: 0.242928, acc.: 89.84%] [G loss: 4.848537]\n",
      "epoch:44 step:35005 [D loss: 0.273308, acc.: 90.62%] [G loss: 4.374804]\n",
      "epoch:44 step:35006 [D loss: 0.309001, acc.: 88.28%] [G loss: 4.174427]\n",
      "epoch:44 step:35007 [D loss: 0.326628, acc.: 85.16%] [G loss: 3.830027]\n",
      "epoch:44 step:35008 [D loss: 0.367216, acc.: 82.81%] [G loss: 2.925309]\n",
      "epoch:44 step:35009 [D loss: 0.174881, acc.: 93.75%] [G loss: 3.579033]\n",
      "epoch:44 step:35010 [D loss: 0.359657, acc.: 87.50%] [G loss: 2.814970]\n",
      "epoch:44 step:35011 [D loss: 0.338345, acc.: 85.16%] [G loss: 2.822597]\n",
      "epoch:44 step:35012 [D loss: 0.363797, acc.: 83.59%] [G loss: 3.831413]\n",
      "epoch:44 step:35013 [D loss: 0.272899, acc.: 87.50%] [G loss: 2.829956]\n",
      "epoch:44 step:35014 [D loss: 0.340586, acc.: 83.59%] [G loss: 3.848257]\n",
      "epoch:44 step:35015 [D loss: 0.246774, acc.: 91.41%] [G loss: 2.632075]\n",
      "epoch:44 step:35016 [D loss: 0.257682, acc.: 88.28%] [G loss: 2.949624]\n",
      "epoch:44 step:35017 [D loss: 0.316173, acc.: 85.16%] [G loss: 2.611579]\n",
      "epoch:44 step:35018 [D loss: 0.248995, acc.: 89.06%] [G loss: 2.969974]\n",
      "epoch:44 step:35019 [D loss: 0.271883, acc.: 88.28%] [G loss: 2.784500]\n",
      "epoch:44 step:35020 [D loss: 0.274915, acc.: 91.41%] [G loss: 3.040823]\n",
      "epoch:44 step:35021 [D loss: 0.292741, acc.: 87.50%] [G loss: 4.047134]\n",
      "epoch:44 step:35022 [D loss: 0.260396, acc.: 88.28%] [G loss: 3.226934]\n",
      "epoch:44 step:35023 [D loss: 0.258597, acc.: 90.62%] [G loss: 2.903291]\n",
      "epoch:44 step:35024 [D loss: 0.283792, acc.: 87.50%] [G loss: 3.339612]\n",
      "epoch:44 step:35025 [D loss: 0.240177, acc.: 89.84%] [G loss: 3.719077]\n",
      "epoch:44 step:35026 [D loss: 0.424289, acc.: 81.25%] [G loss: 3.081447]\n",
      "epoch:44 step:35027 [D loss: 0.280380, acc.: 85.94%] [G loss: 3.212894]\n",
      "epoch:44 step:35028 [D loss: 0.293325, acc.: 85.94%] [G loss: 4.331843]\n",
      "epoch:44 step:35029 [D loss: 0.320919, acc.: 82.81%] [G loss: 4.006957]\n",
      "epoch:44 step:35030 [D loss: 0.229866, acc.: 89.06%] [G loss: 3.768898]\n",
      "epoch:44 step:35031 [D loss: 0.175962, acc.: 92.97%] [G loss: 2.512210]\n",
      "epoch:44 step:35032 [D loss: 0.235594, acc.: 89.06%] [G loss: 3.228445]\n",
      "epoch:44 step:35033 [D loss: 0.324913, acc.: 84.38%] [G loss: 3.454781]\n",
      "epoch:44 step:35034 [D loss: 0.273247, acc.: 90.62%] [G loss: 2.586798]\n",
      "epoch:44 step:35035 [D loss: 0.300312, acc.: 88.28%] [G loss: 3.791827]\n",
      "epoch:44 step:35036 [D loss: 0.261506, acc.: 88.28%] [G loss: 3.550842]\n",
      "epoch:44 step:35037 [D loss: 0.236406, acc.: 89.06%] [G loss: 4.276559]\n",
      "epoch:44 step:35038 [D loss: 0.330466, acc.: 84.38%] [G loss: 3.284481]\n",
      "epoch:44 step:35039 [D loss: 0.280770, acc.: 88.28%] [G loss: 2.782202]\n",
      "epoch:44 step:35040 [D loss: 0.308477, acc.: 86.72%] [G loss: 3.667231]\n",
      "epoch:44 step:35041 [D loss: 0.320674, acc.: 86.72%] [G loss: 5.056371]\n",
      "epoch:44 step:35042 [D loss: 0.264851, acc.: 90.62%] [G loss: 3.520016]\n",
      "epoch:44 step:35043 [D loss: 0.216197, acc.: 93.75%] [G loss: 3.369142]\n",
      "epoch:44 step:35044 [D loss: 0.276545, acc.: 86.72%] [G loss: 3.582191]\n",
      "epoch:44 step:35045 [D loss: 0.233738, acc.: 90.62%] [G loss: 3.218766]\n",
      "epoch:44 step:35046 [D loss: 0.199423, acc.: 92.19%] [G loss: 3.437051]\n",
      "epoch:44 step:35047 [D loss: 0.259424, acc.: 88.28%] [G loss: 4.321747]\n",
      "epoch:44 step:35048 [D loss: 0.217535, acc.: 90.62%] [G loss: 3.954357]\n",
      "epoch:44 step:35049 [D loss: 0.258504, acc.: 89.06%] [G loss: 3.831256]\n",
      "epoch:44 step:35050 [D loss: 0.338748, acc.: 86.72%] [G loss: 3.106413]\n",
      "epoch:44 step:35051 [D loss: 0.234846, acc.: 92.19%] [G loss: 3.084312]\n",
      "epoch:44 step:35052 [D loss: 0.282936, acc.: 90.62%] [G loss: 2.368535]\n",
      "epoch:44 step:35053 [D loss: 0.254729, acc.: 87.50%] [G loss: 3.315479]\n",
      "epoch:44 step:35054 [D loss: 0.241019, acc.: 92.19%] [G loss: 3.167813]\n",
      "epoch:44 step:35055 [D loss: 0.337393, acc.: 84.38%] [G loss: 3.427822]\n",
      "epoch:44 step:35056 [D loss: 0.386461, acc.: 82.81%] [G loss: 3.947305]\n",
      "epoch:44 step:35057 [D loss: 0.519322, acc.: 78.12%] [G loss: 3.057364]\n",
      "epoch:44 step:35058 [D loss: 0.306991, acc.: 83.59%] [G loss: 3.453857]\n",
      "epoch:44 step:35059 [D loss: 0.442534, acc.: 83.59%] [G loss: 3.531964]\n",
      "epoch:44 step:35060 [D loss: 0.383447, acc.: 80.47%] [G loss: 3.314155]\n",
      "epoch:44 step:35061 [D loss: 0.459261, acc.: 85.16%] [G loss: 3.822683]\n",
      "epoch:44 step:35062 [D loss: 0.371011, acc.: 85.16%] [G loss: 3.767348]\n",
      "epoch:44 step:35063 [D loss: 0.374859, acc.: 84.38%] [G loss: 5.108986]\n",
      "epoch:44 step:35064 [D loss: 0.354043, acc.: 84.38%] [G loss: 4.227036]\n",
      "epoch:44 step:35065 [D loss: 0.326270, acc.: 85.94%] [G loss: 3.237245]\n",
      "epoch:44 step:35066 [D loss: 0.274594, acc.: 88.28%] [G loss: 2.971887]\n",
      "epoch:44 step:35067 [D loss: 0.353379, acc.: 84.38%] [G loss: 3.911363]\n",
      "epoch:44 step:35068 [D loss: 0.352879, acc.: 82.03%] [G loss: 3.620023]\n",
      "epoch:44 step:35069 [D loss: 0.229110, acc.: 87.50%] [G loss: 3.594754]\n",
      "epoch:44 step:35070 [D loss: 0.262935, acc.: 87.50%] [G loss: 2.547386]\n",
      "epoch:44 step:35071 [D loss: 0.305150, acc.: 83.59%] [G loss: 3.313986]\n",
      "epoch:44 step:35072 [D loss: 0.407106, acc.: 85.94%] [G loss: 2.468531]\n",
      "epoch:44 step:35073 [D loss: 0.286543, acc.: 86.72%] [G loss: 3.768604]\n",
      "epoch:44 step:35074 [D loss: 0.245335, acc.: 86.72%] [G loss: 4.135353]\n",
      "epoch:44 step:35075 [D loss: 0.227160, acc.: 92.19%] [G loss: 6.901014]\n",
      "epoch:44 step:35076 [D loss: 0.294940, acc.: 89.84%] [G loss: 4.793038]\n",
      "epoch:44 step:35077 [D loss: 0.249388, acc.: 87.50%] [G loss: 5.083527]\n",
      "epoch:44 step:35078 [D loss: 0.221684, acc.: 92.97%] [G loss: 4.759613]\n",
      "epoch:44 step:35079 [D loss: 0.285577, acc.: 85.94%] [G loss: 5.785995]\n",
      "epoch:44 step:35080 [D loss: 0.264797, acc.: 89.06%] [G loss: 3.932796]\n",
      "epoch:44 step:35081 [D loss: 0.294035, acc.: 87.50%] [G loss: 3.375652]\n",
      "epoch:44 step:35082 [D loss: 0.246316, acc.: 90.62%] [G loss: 3.302374]\n",
      "epoch:44 step:35083 [D loss: 0.274403, acc.: 89.84%] [G loss: 3.538274]\n",
      "epoch:44 step:35084 [D loss: 0.257335, acc.: 90.62%] [G loss: 2.778167]\n",
      "epoch:44 step:35085 [D loss: 0.269916, acc.: 89.06%] [G loss: 2.870251]\n",
      "epoch:44 step:35086 [D loss: 0.343450, acc.: 87.50%] [G loss: 3.942683]\n",
      "epoch:44 step:35087 [D loss: 0.326152, acc.: 85.16%] [G loss: 2.914109]\n",
      "epoch:44 step:35088 [D loss: 0.206912, acc.: 91.41%] [G loss: 3.114432]\n",
      "epoch:44 step:35089 [D loss: 0.208780, acc.: 92.97%] [G loss: 3.168736]\n",
      "epoch:44 step:35090 [D loss: 0.273112, acc.: 87.50%] [G loss: 3.018068]\n",
      "epoch:44 step:35091 [D loss: 0.350442, acc.: 86.72%] [G loss: 3.132176]\n",
      "epoch:44 step:35092 [D loss: 0.388720, acc.: 81.25%] [G loss: 3.411547]\n",
      "epoch:44 step:35093 [D loss: 0.307818, acc.: 88.28%] [G loss: 4.336077]\n",
      "epoch:44 step:35094 [D loss: 0.266682, acc.: 88.28%] [G loss: 4.215702]\n",
      "epoch:44 step:35095 [D loss: 0.243806, acc.: 88.28%] [G loss: 4.366053]\n",
      "epoch:44 step:35096 [D loss: 0.279746, acc.: 85.16%] [G loss: 3.036085]\n",
      "epoch:44 step:35097 [D loss: 0.233285, acc.: 89.06%] [G loss: 3.124171]\n",
      "epoch:44 step:35098 [D loss: 0.285896, acc.: 87.50%] [G loss: 3.393051]\n",
      "epoch:44 step:35099 [D loss: 0.298755, acc.: 86.72%] [G loss: 2.892332]\n",
      "epoch:44 step:35100 [D loss: 0.293904, acc.: 87.50%] [G loss: 2.662269]\n",
      "epoch:44 step:35101 [D loss: 0.418086, acc.: 83.59%] [G loss: 3.358874]\n",
      "epoch:44 step:35102 [D loss: 0.375317, acc.: 82.03%] [G loss: 5.021517]\n",
      "epoch:44 step:35103 [D loss: 0.408640, acc.: 77.34%] [G loss: 4.225209]\n",
      "epoch:44 step:35104 [D loss: 0.405065, acc.: 78.91%] [G loss: 3.701768]\n",
      "epoch:44 step:35105 [D loss: 0.248441, acc.: 89.06%] [G loss: 2.967829]\n",
      "epoch:44 step:35106 [D loss: 0.350308, acc.: 82.03%] [G loss: 3.258879]\n",
      "epoch:44 step:35107 [D loss: 0.261518, acc.: 90.62%] [G loss: 3.944571]\n",
      "epoch:44 step:35108 [D loss: 0.301342, acc.: 84.38%] [G loss: 3.330852]\n",
      "epoch:44 step:35109 [D loss: 0.279929, acc.: 85.94%] [G loss: 3.094094]\n",
      "epoch:44 step:35110 [D loss: 0.279534, acc.: 87.50%] [G loss: 3.155258]\n",
      "epoch:44 step:35111 [D loss: 0.285138, acc.: 86.72%] [G loss: 3.038742]\n",
      "epoch:44 step:35112 [D loss: 0.322915, acc.: 83.59%] [G loss: 2.777011]\n",
      "epoch:44 step:35113 [D loss: 0.313641, acc.: 85.16%] [G loss: 2.524904]\n",
      "epoch:44 step:35114 [D loss: 0.379674, acc.: 82.81%] [G loss: 3.230470]\n",
      "epoch:44 step:35115 [D loss: 0.271604, acc.: 88.28%] [G loss: 2.768777]\n",
      "epoch:44 step:35116 [D loss: 0.313590, acc.: 85.94%] [G loss: 2.658864]\n",
      "epoch:44 step:35117 [D loss: 0.289349, acc.: 88.28%] [G loss: 3.447591]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:44 step:35118 [D loss: 0.379857, acc.: 83.59%] [G loss: 3.749871]\n",
      "epoch:44 step:35119 [D loss: 0.289942, acc.: 86.72%] [G loss: 3.735880]\n",
      "epoch:44 step:35120 [D loss: 0.234855, acc.: 91.41%] [G loss: 5.404441]\n",
      "epoch:44 step:35121 [D loss: 0.249448, acc.: 89.84%] [G loss: 4.079674]\n",
      "epoch:44 step:35122 [D loss: 0.253034, acc.: 87.50%] [G loss: 4.522008]\n",
      "epoch:44 step:35123 [D loss: 0.255540, acc.: 89.06%] [G loss: 3.483330]\n",
      "epoch:44 step:35124 [D loss: 0.171165, acc.: 93.75%] [G loss: 3.961897]\n",
      "epoch:44 step:35125 [D loss: 0.319899, acc.: 87.50%] [G loss: 3.599546]\n",
      "epoch:44 step:35126 [D loss: 0.304646, acc.: 89.06%] [G loss: 3.518460]\n",
      "epoch:44 step:35127 [D loss: 0.320824, acc.: 85.94%] [G loss: 4.361993]\n",
      "epoch:44 step:35128 [D loss: 0.247907, acc.: 92.19%] [G loss: 3.610951]\n",
      "epoch:44 step:35129 [D loss: 0.366717, acc.: 85.16%] [G loss: 4.585752]\n",
      "epoch:44 step:35130 [D loss: 0.298814, acc.: 88.28%] [G loss: 3.805945]\n",
      "epoch:44 step:35131 [D loss: 0.208975, acc.: 88.28%] [G loss: 4.023362]\n",
      "epoch:44 step:35132 [D loss: 0.237294, acc.: 92.19%] [G loss: 3.394536]\n",
      "epoch:44 step:35133 [D loss: 0.308121, acc.: 83.59%] [G loss: 3.115845]\n",
      "epoch:44 step:35134 [D loss: 0.337612, acc.: 83.59%] [G loss: 3.797591]\n",
      "epoch:44 step:35135 [D loss: 0.327446, acc.: 84.38%] [G loss: 3.222277]\n",
      "epoch:44 step:35136 [D loss: 0.315385, acc.: 84.38%] [G loss: 3.915383]\n",
      "epoch:44 step:35137 [D loss: 0.262743, acc.: 89.06%] [G loss: 4.555948]\n",
      "epoch:44 step:35138 [D loss: 0.268244, acc.: 91.41%] [G loss: 5.996152]\n",
      "epoch:44 step:35139 [D loss: 0.301411, acc.: 87.50%] [G loss: 3.128777]\n",
      "epoch:44 step:35140 [D loss: 0.273864, acc.: 86.72%] [G loss: 3.909015]\n",
      "epoch:44 step:35141 [D loss: 0.320553, acc.: 85.16%] [G loss: 3.900745]\n",
      "epoch:44 step:35142 [D loss: 0.242794, acc.: 89.06%] [G loss: 4.391561]\n",
      "epoch:44 step:35143 [D loss: 0.381519, acc.: 86.72%] [G loss: 4.668099]\n",
      "epoch:44 step:35144 [D loss: 0.309850, acc.: 85.94%] [G loss: 3.588582]\n",
      "epoch:44 step:35145 [D loss: 0.311907, acc.: 82.03%] [G loss: 2.938434]\n",
      "epoch:45 step:35146 [D loss: 0.280757, acc.: 89.84%] [G loss: 4.181523]\n",
      "epoch:45 step:35147 [D loss: 0.258381, acc.: 89.06%] [G loss: 3.279204]\n",
      "epoch:45 step:35148 [D loss: 0.363096, acc.: 86.72%] [G loss: 3.265986]\n",
      "epoch:45 step:35149 [D loss: 0.251616, acc.: 89.84%] [G loss: 3.441649]\n",
      "epoch:45 step:35150 [D loss: 0.267643, acc.: 88.28%] [G loss: 3.702297]\n",
      "epoch:45 step:35151 [D loss: 0.261631, acc.: 89.84%] [G loss: 2.779774]\n",
      "epoch:45 step:35152 [D loss: 0.328468, acc.: 85.94%] [G loss: 3.429888]\n",
      "epoch:45 step:35153 [D loss: 0.390847, acc.: 79.69%] [G loss: 2.759153]\n",
      "epoch:45 step:35154 [D loss: 0.343874, acc.: 87.50%] [G loss: 2.736689]\n",
      "epoch:45 step:35155 [D loss: 0.327285, acc.: 88.28%] [G loss: 3.125148]\n",
      "epoch:45 step:35156 [D loss: 0.331482, acc.: 84.38%] [G loss: 2.810414]\n",
      "epoch:45 step:35157 [D loss: 0.268768, acc.: 88.28%] [G loss: 3.024203]\n",
      "epoch:45 step:35158 [D loss: 0.307818, acc.: 87.50%] [G loss: 3.259909]\n",
      "epoch:45 step:35159 [D loss: 0.376781, acc.: 78.91%] [G loss: 3.778522]\n",
      "epoch:45 step:35160 [D loss: 0.323044, acc.: 85.16%] [G loss: 2.619888]\n",
      "epoch:45 step:35161 [D loss: 0.277040, acc.: 89.06%] [G loss: 2.880830]\n",
      "epoch:45 step:35162 [D loss: 0.265826, acc.: 87.50%] [G loss: 3.287799]\n",
      "epoch:45 step:35163 [D loss: 0.353185, acc.: 85.16%] [G loss: 3.776104]\n",
      "epoch:45 step:35164 [D loss: 0.377239, acc.: 86.72%] [G loss: 3.333889]\n",
      "epoch:45 step:35165 [D loss: 0.264868, acc.: 90.62%] [G loss: 3.727670]\n",
      "epoch:45 step:35166 [D loss: 0.287117, acc.: 85.94%] [G loss: 3.104443]\n",
      "epoch:45 step:35167 [D loss: 0.270616, acc.: 85.16%] [G loss: 4.270522]\n",
      "epoch:45 step:35168 [D loss: 0.272317, acc.: 89.84%] [G loss: 3.307700]\n",
      "epoch:45 step:35169 [D loss: 0.317595, acc.: 88.28%] [G loss: 3.540571]\n",
      "epoch:45 step:35170 [D loss: 0.207648, acc.: 91.41%] [G loss: 3.601521]\n",
      "epoch:45 step:35171 [D loss: 0.297956, acc.: 87.50%] [G loss: 4.190136]\n",
      "epoch:45 step:35172 [D loss: 0.327233, acc.: 82.81%] [G loss: 5.603830]\n",
      "epoch:45 step:35173 [D loss: 0.229250, acc.: 89.06%] [G loss: 6.383853]\n",
      "epoch:45 step:35174 [D loss: 0.221048, acc.: 89.84%] [G loss: 12.809093]\n",
      "epoch:45 step:35175 [D loss: 0.131735, acc.: 94.53%] [G loss: 8.596472]\n",
      "epoch:45 step:35176 [D loss: 0.177360, acc.: 94.53%] [G loss: 6.363481]\n",
      "epoch:45 step:35177 [D loss: 0.245593, acc.: 91.41%] [G loss: 6.928438]\n",
      "epoch:45 step:35178 [D loss: 0.209486, acc.: 90.62%] [G loss: 6.152207]\n",
      "epoch:45 step:35179 [D loss: 0.369077, acc.: 85.94%] [G loss: 5.302421]\n",
      "epoch:45 step:35180 [D loss: 0.237237, acc.: 90.62%] [G loss: 4.353437]\n",
      "epoch:45 step:35181 [D loss: 0.290314, acc.: 84.38%] [G loss: 3.470188]\n",
      "epoch:45 step:35182 [D loss: 0.288049, acc.: 85.94%] [G loss: 4.206554]\n",
      "epoch:45 step:35183 [D loss: 0.253570, acc.: 87.50%] [G loss: 2.870835]\n",
      "epoch:45 step:35184 [D loss: 0.328975, acc.: 83.59%] [G loss: 3.775493]\n",
      "epoch:45 step:35185 [D loss: 0.276397, acc.: 87.50%] [G loss: 3.269173]\n",
      "epoch:45 step:35186 [D loss: 0.302661, acc.: 87.50%] [G loss: 3.652773]\n",
      "epoch:45 step:35187 [D loss: 0.283501, acc.: 86.72%] [G loss: 4.552531]\n",
      "epoch:45 step:35188 [D loss: 0.375208, acc.: 79.69%] [G loss: 3.718916]\n",
      "epoch:45 step:35189 [D loss: 0.343202, acc.: 87.50%] [G loss: 3.835625]\n",
      "epoch:45 step:35190 [D loss: 0.434139, acc.: 79.69%] [G loss: 7.293702]\n",
      "epoch:45 step:35191 [D loss: 0.320245, acc.: 87.50%] [G loss: 5.486054]\n",
      "epoch:45 step:35192 [D loss: 0.413784, acc.: 82.03%] [G loss: 5.334128]\n",
      "epoch:45 step:35193 [D loss: 0.261454, acc.: 86.72%] [G loss: 4.733512]\n",
      "epoch:45 step:35194 [D loss: 0.275038, acc.: 85.16%] [G loss: 4.650013]\n",
      "epoch:45 step:35195 [D loss: 0.254250, acc.: 90.62%] [G loss: 4.297964]\n",
      "epoch:45 step:35196 [D loss: 0.204092, acc.: 91.41%] [G loss: 3.469527]\n",
      "epoch:45 step:35197 [D loss: 0.270768, acc.: 86.72%] [G loss: 3.862827]\n",
      "epoch:45 step:35198 [D loss: 0.360810, acc.: 85.16%] [G loss: 3.277068]\n",
      "epoch:45 step:35199 [D loss: 0.342802, acc.: 82.03%] [G loss: 3.430474]\n",
      "epoch:45 step:35200 [D loss: 0.330855, acc.: 82.81%] [G loss: 2.277103]\n",
      "##############\n",
      "[0.86327613 0.84931684 0.81716317 0.77501485 0.75483425 0.82223497\n",
      " 0.87752021 0.81040098 0.83521039 0.84414933]\n",
      "##########\n",
      "epoch:45 step:35201 [D loss: 0.403076, acc.: 83.59%] [G loss: 3.248359]\n",
      "epoch:45 step:35202 [D loss: 0.281987, acc.: 89.06%] [G loss: 3.202407]\n",
      "epoch:45 step:35203 [D loss: 0.336795, acc.: 84.38%] [G loss: 2.830521]\n",
      "epoch:45 step:35204 [D loss: 0.302767, acc.: 85.16%] [G loss: 4.005705]\n",
      "epoch:45 step:35205 [D loss: 0.438770, acc.: 78.91%] [G loss: 4.374892]\n",
      "epoch:45 step:35206 [D loss: 0.343103, acc.: 85.94%] [G loss: 4.241842]\n",
      "epoch:45 step:35207 [D loss: 0.225437, acc.: 89.84%] [G loss: 5.172123]\n",
      "epoch:45 step:35208 [D loss: 0.322624, acc.: 86.72%] [G loss: 3.946393]\n",
      "epoch:45 step:35209 [D loss: 0.304564, acc.: 89.06%] [G loss: 4.412361]\n",
      "epoch:45 step:35210 [D loss: 0.341958, acc.: 82.03%] [G loss: 3.087693]\n",
      "epoch:45 step:35211 [D loss: 0.374382, acc.: 85.94%] [G loss: 2.696164]\n",
      "epoch:45 step:35212 [D loss: 0.370682, acc.: 84.38%] [G loss: 2.834223]\n",
      "epoch:45 step:35213 [D loss: 0.345708, acc.: 84.38%] [G loss: 2.771733]\n",
      "epoch:45 step:35214 [D loss: 0.286014, acc.: 85.94%] [G loss: 3.472859]\n",
      "epoch:45 step:35215 [D loss: 0.309402, acc.: 85.94%] [G loss: 3.387578]\n",
      "epoch:45 step:35216 [D loss: 0.435100, acc.: 78.12%] [G loss: 2.858002]\n",
      "epoch:45 step:35217 [D loss: 0.327844, acc.: 88.28%] [G loss: 4.017432]\n",
      "epoch:45 step:35218 [D loss: 0.361038, acc.: 86.72%] [G loss: 3.374956]\n",
      "epoch:45 step:35219 [D loss: 0.253476, acc.: 88.28%] [G loss: 3.756064]\n",
      "epoch:45 step:35220 [D loss: 0.415237, acc.: 80.47%] [G loss: 4.113640]\n",
      "epoch:45 step:35221 [D loss: 0.359006, acc.: 85.16%] [G loss: 4.381141]\n",
      "epoch:45 step:35222 [D loss: 0.244367, acc.: 86.72%] [G loss: 3.583882]\n",
      "epoch:45 step:35223 [D loss: 0.379906, acc.: 79.69%] [G loss: 2.274386]\n",
      "epoch:45 step:35224 [D loss: 0.292930, acc.: 89.06%] [G loss: 3.339066]\n",
      "epoch:45 step:35225 [D loss: 0.210156, acc.: 92.97%] [G loss: 2.960190]\n",
      "epoch:45 step:35226 [D loss: 0.287045, acc.: 87.50%] [G loss: 3.034426]\n",
      "epoch:45 step:35227 [D loss: 0.277259, acc.: 86.72%] [G loss: 2.364500]\n",
      "epoch:45 step:35228 [D loss: 0.281005, acc.: 87.50%] [G loss: 2.977743]\n",
      "epoch:45 step:35229 [D loss: 0.236495, acc.: 89.84%] [G loss: 3.825864]\n",
      "epoch:45 step:35230 [D loss: 0.244981, acc.: 88.28%] [G loss: 4.745819]\n",
      "epoch:45 step:35231 [D loss: 0.387255, acc.: 79.69%] [G loss: 3.410504]\n",
      "epoch:45 step:35232 [D loss: 0.246657, acc.: 91.41%] [G loss: 2.830979]\n",
      "epoch:45 step:35233 [D loss: 0.252165, acc.: 88.28%] [G loss: 3.320894]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:45 step:35234 [D loss: 0.280886, acc.: 88.28%] [G loss: 3.188428]\n",
      "epoch:45 step:35235 [D loss: 0.377353, acc.: 84.38%] [G loss: 2.960331]\n",
      "epoch:45 step:35236 [D loss: 0.235158, acc.: 90.62%] [G loss: 2.523848]\n",
      "epoch:45 step:35237 [D loss: 0.305545, acc.: 86.72%] [G loss: 3.049359]\n",
      "epoch:45 step:35238 [D loss: 0.496952, acc.: 75.78%] [G loss: 3.888202]\n",
      "epoch:45 step:35239 [D loss: 0.511822, acc.: 85.16%] [G loss: 7.070496]\n",
      "epoch:45 step:35240 [D loss: 1.608036, acc.: 67.97%] [G loss: 11.275170]\n",
      "epoch:45 step:35241 [D loss: 2.435826, acc.: 63.28%] [G loss: 4.666952]\n",
      "epoch:45 step:35242 [D loss: 0.652015, acc.: 79.69%] [G loss: 4.750809]\n",
      "epoch:45 step:35243 [D loss: 0.460737, acc.: 82.81%] [G loss: 5.023540]\n",
      "epoch:45 step:35244 [D loss: 0.409869, acc.: 78.12%] [G loss: 4.320228]\n",
      "epoch:45 step:35245 [D loss: 0.244630, acc.: 90.62%] [G loss: 7.981294]\n",
      "epoch:45 step:35246 [D loss: 0.329760, acc.: 84.38%] [G loss: 4.707871]\n",
      "epoch:45 step:35247 [D loss: 0.264585, acc.: 90.62%] [G loss: 4.160294]\n",
      "epoch:45 step:35248 [D loss: 0.222969, acc.: 92.19%] [G loss: 3.430650]\n",
      "epoch:45 step:35249 [D loss: 0.248971, acc.: 91.41%] [G loss: 4.110754]\n",
      "epoch:45 step:35250 [D loss: 0.302477, acc.: 88.28%] [G loss: 2.777138]\n",
      "epoch:45 step:35251 [D loss: 0.222101, acc.: 91.41%] [G loss: 4.031041]\n",
      "epoch:45 step:35252 [D loss: 0.296000, acc.: 88.28%] [G loss: 2.891799]\n",
      "epoch:45 step:35253 [D loss: 0.273944, acc.: 87.50%] [G loss: 3.747555]\n",
      "epoch:45 step:35254 [D loss: 0.234340, acc.: 89.84%] [G loss: 3.435917]\n",
      "epoch:45 step:35255 [D loss: 0.236770, acc.: 88.28%] [G loss: 3.484945]\n",
      "epoch:45 step:35256 [D loss: 0.301317, acc.: 88.28%] [G loss: 3.423832]\n",
      "epoch:45 step:35257 [D loss: 0.333620, acc.: 85.94%] [G loss: 3.090282]\n",
      "epoch:45 step:35258 [D loss: 0.197971, acc.: 92.97%] [G loss: 2.657764]\n",
      "epoch:45 step:35259 [D loss: 0.361436, acc.: 82.81%] [G loss: 2.967266]\n",
      "epoch:45 step:35260 [D loss: 0.384340, acc.: 80.47%] [G loss: 3.310380]\n",
      "epoch:45 step:35261 [D loss: 0.320940, acc.: 86.72%] [G loss: 2.966345]\n",
      "epoch:45 step:35262 [D loss: 0.420399, acc.: 78.12%] [G loss: 3.084214]\n",
      "epoch:45 step:35263 [D loss: 0.270141, acc.: 86.72%] [G loss: 3.147779]\n",
      "epoch:45 step:35264 [D loss: 0.283790, acc.: 87.50%] [G loss: 3.658010]\n",
      "epoch:45 step:35265 [D loss: 0.308321, acc.: 83.59%] [G loss: 3.415669]\n",
      "epoch:45 step:35266 [D loss: 0.299072, acc.: 84.38%] [G loss: 3.415685]\n",
      "epoch:45 step:35267 [D loss: 0.293381, acc.: 88.28%] [G loss: 3.092854]\n",
      "epoch:45 step:35268 [D loss: 0.231185, acc.: 89.84%] [G loss: 2.765108]\n",
      "epoch:45 step:35269 [D loss: 0.315039, acc.: 84.38%] [G loss: 2.615140]\n",
      "epoch:45 step:35270 [D loss: 0.401570, acc.: 78.91%] [G loss: 2.936785]\n",
      "epoch:45 step:35271 [D loss: 0.307921, acc.: 85.94%] [G loss: 2.990516]\n",
      "epoch:45 step:35272 [D loss: 0.340624, acc.: 85.94%] [G loss: 2.913106]\n",
      "epoch:45 step:35273 [D loss: 0.270466, acc.: 90.62%] [G loss: 3.138944]\n",
      "epoch:45 step:35274 [D loss: 0.278297, acc.: 90.62%] [G loss: 2.850953]\n",
      "epoch:45 step:35275 [D loss: 0.328761, acc.: 82.03%] [G loss: 2.769128]\n",
      "epoch:45 step:35276 [D loss: 0.321261, acc.: 84.38%] [G loss: 2.746235]\n",
      "epoch:45 step:35277 [D loss: 0.342153, acc.: 82.03%] [G loss: 2.780052]\n",
      "epoch:45 step:35278 [D loss: 0.274853, acc.: 88.28%] [G loss: 3.121530]\n",
      "epoch:45 step:35279 [D loss: 0.318089, acc.: 89.06%] [G loss: 3.085717]\n",
      "epoch:45 step:35280 [D loss: 0.197565, acc.: 92.97%] [G loss: 3.475827]\n",
      "epoch:45 step:35281 [D loss: 0.299685, acc.: 84.38%] [G loss: 2.909495]\n",
      "epoch:45 step:35282 [D loss: 0.291008, acc.: 89.06%] [G loss: 3.049547]\n",
      "epoch:45 step:35283 [D loss: 0.424937, acc.: 79.69%] [G loss: 2.780197]\n",
      "epoch:45 step:35284 [D loss: 0.267045, acc.: 88.28%] [G loss: 3.116862]\n",
      "epoch:45 step:35285 [D loss: 0.263031, acc.: 87.50%] [G loss: 3.012908]\n",
      "epoch:45 step:35286 [D loss: 0.317390, acc.: 85.16%] [G loss: 2.888277]\n",
      "epoch:45 step:35287 [D loss: 0.329797, acc.: 88.28%] [G loss: 3.366007]\n",
      "epoch:45 step:35288 [D loss: 0.240555, acc.: 92.19%] [G loss: 2.884786]\n",
      "epoch:45 step:35289 [D loss: 0.341119, acc.: 85.94%] [G loss: 3.194198]\n",
      "epoch:45 step:35290 [D loss: 0.370833, acc.: 82.81%] [G loss: 3.632923]\n",
      "epoch:45 step:35291 [D loss: 0.309412, acc.: 85.94%] [G loss: 4.757552]\n",
      "epoch:45 step:35292 [D loss: 0.335578, acc.: 85.16%] [G loss: 3.092605]\n",
      "epoch:45 step:35293 [D loss: 0.262783, acc.: 88.28%] [G loss: 4.221591]\n",
      "epoch:45 step:35294 [D loss: 0.296211, acc.: 87.50%] [G loss: 5.089499]\n",
      "epoch:45 step:35295 [D loss: 0.231834, acc.: 86.72%] [G loss: 4.599857]\n",
      "epoch:45 step:35296 [D loss: 0.253803, acc.: 87.50%] [G loss: 4.511261]\n",
      "epoch:45 step:35297 [D loss: 0.210928, acc.: 89.06%] [G loss: 3.830406]\n",
      "epoch:45 step:35298 [D loss: 0.254838, acc.: 90.62%] [G loss: 2.944403]\n",
      "epoch:45 step:35299 [D loss: 0.290186, acc.: 85.16%] [G loss: 3.716056]\n",
      "epoch:45 step:35300 [D loss: 0.319691, acc.: 83.59%] [G loss: 2.795609]\n",
      "epoch:45 step:35301 [D loss: 0.317253, acc.: 89.06%] [G loss: 3.002000]\n",
      "epoch:45 step:35302 [D loss: 0.278930, acc.: 87.50%] [G loss: 3.947192]\n",
      "epoch:45 step:35303 [D loss: 0.242049, acc.: 92.19%] [G loss: 3.505530]\n",
      "epoch:45 step:35304 [D loss: 0.202249, acc.: 92.19%] [G loss: 3.653624]\n",
      "epoch:45 step:35305 [D loss: 0.365059, acc.: 82.03%] [G loss: 3.178592]\n",
      "epoch:45 step:35306 [D loss: 0.346914, acc.: 82.03%] [G loss: 2.266017]\n",
      "epoch:45 step:35307 [D loss: 0.272692, acc.: 89.84%] [G loss: 2.995751]\n",
      "epoch:45 step:35308 [D loss: 0.332449, acc.: 85.16%] [G loss: 2.676207]\n",
      "epoch:45 step:35309 [D loss: 0.262869, acc.: 89.84%] [G loss: 3.467068]\n",
      "epoch:45 step:35310 [D loss: 0.303014, acc.: 88.28%] [G loss: 2.591708]\n",
      "epoch:45 step:35311 [D loss: 0.328348, acc.: 85.94%] [G loss: 2.942015]\n",
      "epoch:45 step:35312 [D loss: 0.338017, acc.: 85.16%] [G loss: 3.641026]\n",
      "epoch:45 step:35313 [D loss: 0.299430, acc.: 86.72%] [G loss: 2.858297]\n",
      "epoch:45 step:35314 [D loss: 0.318940, acc.: 89.06%] [G loss: 3.283616]\n",
      "epoch:45 step:35315 [D loss: 0.231263, acc.: 90.62%] [G loss: 3.038821]\n",
      "epoch:45 step:35316 [D loss: 0.357911, acc.: 82.81%] [G loss: 5.227955]\n",
      "epoch:45 step:35317 [D loss: 0.331181, acc.: 82.81%] [G loss: 3.876256]\n",
      "epoch:45 step:35318 [D loss: 0.237911, acc.: 89.84%] [G loss: 4.653988]\n",
      "epoch:45 step:35319 [D loss: 0.301999, acc.: 85.94%] [G loss: 7.581882]\n",
      "epoch:45 step:35320 [D loss: 0.234155, acc.: 90.62%] [G loss: 4.481023]\n",
      "epoch:45 step:35321 [D loss: 0.282046, acc.: 88.28%] [G loss: 4.980958]\n",
      "epoch:45 step:35322 [D loss: 0.214025, acc.: 89.84%] [G loss: 5.897560]\n",
      "epoch:45 step:35323 [D loss: 0.189484, acc.: 93.75%] [G loss: 3.277382]\n",
      "epoch:45 step:35324 [D loss: 0.288195, acc.: 86.72%] [G loss: 2.238097]\n",
      "epoch:45 step:35325 [D loss: 0.333973, acc.: 84.38%] [G loss: 2.896820]\n",
      "epoch:45 step:35326 [D loss: 0.285827, acc.: 87.50%] [G loss: 3.067439]\n",
      "epoch:45 step:35327 [D loss: 0.396071, acc.: 80.47%] [G loss: 2.727633]\n",
      "epoch:45 step:35328 [D loss: 0.375942, acc.: 85.94%] [G loss: 2.506419]\n",
      "epoch:45 step:35329 [D loss: 0.415946, acc.: 81.25%] [G loss: 2.579472]\n",
      "epoch:45 step:35330 [D loss: 0.253107, acc.: 89.06%] [G loss: 3.109230]\n",
      "epoch:45 step:35331 [D loss: 0.212671, acc.: 90.62%] [G loss: 2.872506]\n",
      "epoch:45 step:35332 [D loss: 0.269954, acc.: 90.62%] [G loss: 2.635262]\n",
      "epoch:45 step:35333 [D loss: 0.365898, acc.: 81.25%] [G loss: 2.864716]\n",
      "epoch:45 step:35334 [D loss: 0.267103, acc.: 89.84%] [G loss: 3.211810]\n",
      "epoch:45 step:35335 [D loss: 0.250626, acc.: 90.62%] [G loss: 2.741172]\n",
      "epoch:45 step:35336 [D loss: 0.293079, acc.: 84.38%] [G loss: 2.705103]\n",
      "epoch:45 step:35337 [D loss: 0.276031, acc.: 90.62%] [G loss: 3.788972]\n",
      "epoch:45 step:35338 [D loss: 0.269429, acc.: 92.19%] [G loss: 2.944600]\n",
      "epoch:45 step:35339 [D loss: 0.335000, acc.: 85.16%] [G loss: 2.976367]\n",
      "epoch:45 step:35340 [D loss: 0.274058, acc.: 88.28%] [G loss: 2.958469]\n",
      "epoch:45 step:35341 [D loss: 0.258778, acc.: 89.06%] [G loss: 3.548802]\n",
      "epoch:45 step:35342 [D loss: 0.340332, acc.: 82.81%] [G loss: 2.907675]\n",
      "epoch:45 step:35343 [D loss: 0.339681, acc.: 82.03%] [G loss: 3.705364]\n",
      "epoch:45 step:35344 [D loss: 0.248671, acc.: 90.62%] [G loss: 3.295911]\n",
      "epoch:45 step:35345 [D loss: 0.444018, acc.: 77.34%] [G loss: 3.292191]\n",
      "epoch:45 step:35346 [D loss: 0.353395, acc.: 83.59%] [G loss: 3.768046]\n",
      "epoch:45 step:35347 [D loss: 0.287940, acc.: 86.72%] [G loss: 3.763487]\n",
      "epoch:45 step:35348 [D loss: 0.245541, acc.: 87.50%] [G loss: 3.840610]\n",
      "epoch:45 step:35349 [D loss: 0.343704, acc.: 82.81%] [G loss: 6.653607]\n",
      "epoch:45 step:35350 [D loss: 0.409380, acc.: 84.38%] [G loss: 4.544424]\n",
      "epoch:45 step:35351 [D loss: 0.290135, acc.: 85.16%] [G loss: 3.942860]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:45 step:35352 [D loss: 0.343136, acc.: 85.94%] [G loss: 4.638028]\n",
      "epoch:45 step:35353 [D loss: 0.302871, acc.: 85.94%] [G loss: 5.260076]\n",
      "epoch:45 step:35354 [D loss: 0.379019, acc.: 84.38%] [G loss: 4.084256]\n",
      "epoch:45 step:35355 [D loss: 0.253553, acc.: 95.31%] [G loss: 3.121084]\n",
      "epoch:45 step:35356 [D loss: 0.282174, acc.: 86.72%] [G loss: 2.844970]\n",
      "epoch:45 step:35357 [D loss: 0.316122, acc.: 85.94%] [G loss: 3.303313]\n",
      "epoch:45 step:35358 [D loss: 0.264010, acc.: 88.28%] [G loss: 3.144891]\n",
      "epoch:45 step:35359 [D loss: 0.311828, acc.: 85.94%] [G loss: 4.231271]\n",
      "epoch:45 step:35360 [D loss: 0.330392, acc.: 84.38%] [G loss: 3.284904]\n",
      "epoch:45 step:35361 [D loss: 0.252925, acc.: 88.28%] [G loss: 3.094754]\n",
      "epoch:45 step:35362 [D loss: 0.265744, acc.: 85.16%] [G loss: 3.231797]\n",
      "epoch:45 step:35363 [D loss: 0.321434, acc.: 85.94%] [G loss: 3.054290]\n",
      "epoch:45 step:35364 [D loss: 0.326101, acc.: 82.81%] [G loss: 2.954739]\n",
      "epoch:45 step:35365 [D loss: 0.368388, acc.: 84.38%] [G loss: 3.488315]\n",
      "epoch:45 step:35366 [D loss: 0.211580, acc.: 90.62%] [G loss: 2.996143]\n",
      "epoch:45 step:35367 [D loss: 0.337940, acc.: 87.50%] [G loss: 2.539820]\n",
      "epoch:45 step:35368 [D loss: 0.254529, acc.: 89.84%] [G loss: 3.405386]\n",
      "epoch:45 step:35369 [D loss: 0.235820, acc.: 91.41%] [G loss: 2.990763]\n",
      "epoch:45 step:35370 [D loss: 0.304296, acc.: 84.38%] [G loss: 4.120404]\n",
      "epoch:45 step:35371 [D loss: 0.265706, acc.: 90.62%] [G loss: 3.473356]\n",
      "epoch:45 step:35372 [D loss: 0.145081, acc.: 92.97%] [G loss: 6.250299]\n",
      "epoch:45 step:35373 [D loss: 0.393474, acc.: 81.25%] [G loss: 3.948128]\n",
      "epoch:45 step:35374 [D loss: 0.329931, acc.: 85.16%] [G loss: 4.412174]\n",
      "epoch:45 step:35375 [D loss: 0.294778, acc.: 85.94%] [G loss: 3.896272]\n",
      "epoch:45 step:35376 [D loss: 0.242417, acc.: 91.41%] [G loss: 4.332628]\n",
      "epoch:45 step:35377 [D loss: 0.224263, acc.: 89.06%] [G loss: 3.654725]\n",
      "epoch:45 step:35378 [D loss: 0.228481, acc.: 89.84%] [G loss: 3.624863]\n",
      "epoch:45 step:35379 [D loss: 0.329182, acc.: 87.50%] [G loss: 3.005545]\n",
      "epoch:45 step:35380 [D loss: 0.251787, acc.: 90.62%] [G loss: 3.556021]\n",
      "epoch:45 step:35381 [D loss: 0.262594, acc.: 88.28%] [G loss: 3.608927]\n",
      "epoch:45 step:35382 [D loss: 0.309851, acc.: 85.94%] [G loss: 3.548776]\n",
      "epoch:45 step:35383 [D loss: 0.377827, acc.: 80.47%] [G loss: 3.399850]\n",
      "epoch:45 step:35384 [D loss: 0.377313, acc.: 82.81%] [G loss: 3.062319]\n",
      "epoch:45 step:35385 [D loss: 0.331204, acc.: 84.38%] [G loss: 2.583375]\n",
      "epoch:45 step:35386 [D loss: 0.265365, acc.: 88.28%] [G loss: 3.825008]\n",
      "epoch:45 step:35387 [D loss: 0.269853, acc.: 85.94%] [G loss: 3.877989]\n",
      "epoch:45 step:35388 [D loss: 0.281365, acc.: 87.50%] [G loss: 3.637909]\n",
      "epoch:45 step:35389 [D loss: 0.198056, acc.: 92.97%] [G loss: 4.213025]\n",
      "epoch:45 step:35390 [D loss: 0.333683, acc.: 83.59%] [G loss: 4.117495]\n",
      "epoch:45 step:35391 [D loss: 0.294915, acc.: 88.28%] [G loss: 3.978543]\n",
      "epoch:45 step:35392 [D loss: 0.364928, acc.: 85.94%] [G loss: 5.858977]\n",
      "epoch:45 step:35393 [D loss: 0.470439, acc.: 81.25%] [G loss: 3.080944]\n",
      "epoch:45 step:35394 [D loss: 0.420967, acc.: 82.81%] [G loss: 2.817388]\n",
      "epoch:45 step:35395 [D loss: 0.382687, acc.: 80.47%] [G loss: 6.277254]\n",
      "epoch:45 step:35396 [D loss: 0.382453, acc.: 82.81%] [G loss: 7.391563]\n",
      "epoch:45 step:35397 [D loss: 0.372798, acc.: 82.81%] [G loss: 4.040301]\n",
      "epoch:45 step:35398 [D loss: 0.314531, acc.: 85.16%] [G loss: 4.050102]\n",
      "epoch:45 step:35399 [D loss: 0.211999, acc.: 90.62%] [G loss: 5.001957]\n",
      "epoch:45 step:35400 [D loss: 0.370419, acc.: 81.25%] [G loss: 4.371518]\n",
      "##############\n",
      "[0.86244248 0.86890482 0.81426614 0.81192473 0.7697056  0.80207701\n",
      " 0.88468356 0.83720229 0.80321757 0.82702961]\n",
      "##########\n",
      "epoch:45 step:35401 [D loss: 0.247199, acc.: 89.06%] [G loss: 3.510860]\n",
      "epoch:45 step:35402 [D loss: 0.312432, acc.: 87.50%] [G loss: 3.452326]\n",
      "epoch:45 step:35403 [D loss: 0.251406, acc.: 91.41%] [G loss: 3.729789]\n",
      "epoch:45 step:35404 [D loss: 0.288752, acc.: 87.50%] [G loss: 4.344507]\n",
      "epoch:45 step:35405 [D loss: 0.293276, acc.: 87.50%] [G loss: 5.309890]\n",
      "epoch:45 step:35406 [D loss: 0.384538, acc.: 82.81%] [G loss: 5.251850]\n",
      "epoch:45 step:35407 [D loss: 0.305224, acc.: 84.38%] [G loss: 5.846616]\n",
      "epoch:45 step:35408 [D loss: 0.234295, acc.: 89.06%] [G loss: 3.298283]\n",
      "epoch:45 step:35409 [D loss: 0.327675, acc.: 82.81%] [G loss: 4.833784]\n",
      "epoch:45 step:35410 [D loss: 0.242241, acc.: 90.62%] [G loss: 3.590025]\n",
      "epoch:45 step:35411 [D loss: 0.267516, acc.: 90.62%] [G loss: 3.268069]\n",
      "epoch:45 step:35412 [D loss: 0.252647, acc.: 87.50%] [G loss: 3.634517]\n",
      "epoch:45 step:35413 [D loss: 0.232890, acc.: 88.28%] [G loss: 3.447066]\n",
      "epoch:45 step:35414 [D loss: 0.259405, acc.: 89.84%] [G loss: 2.424288]\n",
      "epoch:45 step:35415 [D loss: 0.238607, acc.: 89.84%] [G loss: 3.904615]\n",
      "epoch:45 step:35416 [D loss: 0.297218, acc.: 89.06%] [G loss: 3.534894]\n",
      "epoch:45 step:35417 [D loss: 0.361357, acc.: 87.50%] [G loss: 3.021775]\n",
      "epoch:45 step:35418 [D loss: 0.269533, acc.: 85.16%] [G loss: 3.820587]\n",
      "epoch:45 step:35419 [D loss: 0.287382, acc.: 85.16%] [G loss: 2.942950]\n",
      "epoch:45 step:35420 [D loss: 0.435175, acc.: 79.69%] [G loss: 3.474061]\n",
      "epoch:45 step:35421 [D loss: 0.484370, acc.: 78.12%] [G loss: 3.652187]\n",
      "epoch:45 step:35422 [D loss: 0.330731, acc.: 84.38%] [G loss: 2.869542]\n",
      "epoch:45 step:35423 [D loss: 0.312215, acc.: 87.50%] [G loss: 3.510866]\n",
      "epoch:45 step:35424 [D loss: 0.323875, acc.: 85.94%] [G loss: 3.341635]\n",
      "epoch:45 step:35425 [D loss: 0.241703, acc.: 90.62%] [G loss: 3.621638]\n",
      "epoch:45 step:35426 [D loss: 0.247982, acc.: 91.41%] [G loss: 3.649338]\n",
      "epoch:45 step:35427 [D loss: 0.312068, acc.: 86.72%] [G loss: 3.378304]\n",
      "epoch:45 step:35428 [D loss: 0.309849, acc.: 85.94%] [G loss: 4.006382]\n",
      "epoch:45 step:35429 [D loss: 0.324770, acc.: 85.94%] [G loss: 2.890814]\n",
      "epoch:45 step:35430 [D loss: 0.364527, acc.: 81.25%] [G loss: 5.335416]\n",
      "epoch:45 step:35431 [D loss: 0.428211, acc.: 78.12%] [G loss: 4.937534]\n",
      "epoch:45 step:35432 [D loss: 0.509225, acc.: 78.12%] [G loss: 3.592170]\n",
      "epoch:45 step:35433 [D loss: 0.221868, acc.: 90.62%] [G loss: 3.793876]\n",
      "epoch:45 step:35434 [D loss: 0.365159, acc.: 85.94%] [G loss: 3.142673]\n",
      "epoch:45 step:35435 [D loss: 0.278963, acc.: 87.50%] [G loss: 3.448822]\n",
      "epoch:45 step:35436 [D loss: 0.324681, acc.: 84.38%] [G loss: 3.065670]\n",
      "epoch:45 step:35437 [D loss: 0.397201, acc.: 82.03%] [G loss: 4.228872]\n",
      "epoch:45 step:35438 [D loss: 0.345479, acc.: 84.38%] [G loss: 3.126325]\n",
      "epoch:45 step:35439 [D loss: 0.294156, acc.: 85.94%] [G loss: 3.820726]\n",
      "epoch:45 step:35440 [D loss: 0.270281, acc.: 86.72%] [G loss: 3.502160]\n",
      "epoch:45 step:35441 [D loss: 0.358939, acc.: 85.16%] [G loss: 3.881937]\n",
      "epoch:45 step:35442 [D loss: 0.340052, acc.: 82.81%] [G loss: 3.692410]\n",
      "epoch:45 step:35443 [D loss: 0.278803, acc.: 90.62%] [G loss: 4.016751]\n",
      "epoch:45 step:35444 [D loss: 0.392310, acc.: 80.47%] [G loss: 3.189517]\n",
      "epoch:45 step:35445 [D loss: 0.375668, acc.: 82.03%] [G loss: 3.483469]\n",
      "epoch:45 step:35446 [D loss: 0.338014, acc.: 82.03%] [G loss: 3.031869]\n",
      "epoch:45 step:35447 [D loss: 0.279238, acc.: 86.72%] [G loss: 2.985237]\n",
      "epoch:45 step:35448 [D loss: 0.338893, acc.: 82.81%] [G loss: 3.188605]\n",
      "epoch:45 step:35449 [D loss: 0.306356, acc.: 84.38%] [G loss: 3.171613]\n",
      "epoch:45 step:35450 [D loss: 0.332537, acc.: 86.72%] [G loss: 2.493137]\n",
      "epoch:45 step:35451 [D loss: 0.233599, acc.: 91.41%] [G loss: 3.357604]\n",
      "epoch:45 step:35452 [D loss: 0.269726, acc.: 85.94%] [G loss: 2.515092]\n",
      "epoch:45 step:35453 [D loss: 0.255082, acc.: 88.28%] [G loss: 3.484512]\n",
      "epoch:45 step:35454 [D loss: 0.200843, acc.: 92.97%] [G loss: 4.296258]\n",
      "epoch:45 step:35455 [D loss: 0.253231, acc.: 86.72%] [G loss: 4.153319]\n",
      "epoch:45 step:35456 [D loss: 0.296194, acc.: 84.38%] [G loss: 4.208307]\n",
      "epoch:45 step:35457 [D loss: 0.270532, acc.: 88.28%] [G loss: 3.787041]\n",
      "epoch:45 step:35458 [D loss: 0.240101, acc.: 85.94%] [G loss: 4.801003]\n",
      "epoch:45 step:35459 [D loss: 0.267365, acc.: 86.72%] [G loss: 3.170168]\n",
      "epoch:45 step:35460 [D loss: 0.324025, acc.: 84.38%] [G loss: 3.219198]\n",
      "epoch:45 step:35461 [D loss: 0.326694, acc.: 88.28%] [G loss: 4.842387]\n",
      "epoch:45 step:35462 [D loss: 0.275199, acc.: 89.84%] [G loss: 3.360071]\n",
      "epoch:45 step:35463 [D loss: 0.369188, acc.: 83.59%] [G loss: 3.316695]\n",
      "epoch:45 step:35464 [D loss: 0.274049, acc.: 87.50%] [G loss: 3.586036]\n",
      "epoch:45 step:35465 [D loss: 0.433712, acc.: 79.69%] [G loss: 4.089559]\n",
      "epoch:45 step:35466 [D loss: 0.322217, acc.: 85.16%] [G loss: 4.572848]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:45 step:35467 [D loss: 0.291589, acc.: 87.50%] [G loss: 3.573729]\n",
      "epoch:45 step:35468 [D loss: 0.314641, acc.: 87.50%] [G loss: 5.035892]\n",
      "epoch:45 step:35469 [D loss: 0.442697, acc.: 80.47%] [G loss: 3.049151]\n",
      "epoch:45 step:35470 [D loss: 0.318136, acc.: 86.72%] [G loss: 3.341322]\n",
      "epoch:45 step:35471 [D loss: 0.278220, acc.: 88.28%] [G loss: 3.597099]\n",
      "epoch:45 step:35472 [D loss: 0.272237, acc.: 89.84%] [G loss: 3.848762]\n",
      "epoch:45 step:35473 [D loss: 0.283602, acc.: 89.06%] [G loss: 3.343924]\n",
      "epoch:45 step:35474 [D loss: 0.397845, acc.: 80.47%] [G loss: 3.497821]\n",
      "epoch:45 step:35475 [D loss: 0.327270, acc.: 85.16%] [G loss: 3.008759]\n",
      "epoch:45 step:35476 [D loss: 0.342943, acc.: 86.72%] [G loss: 3.670191]\n",
      "epoch:45 step:35477 [D loss: 0.310488, acc.: 87.50%] [G loss: 3.586038]\n",
      "epoch:45 step:35478 [D loss: 0.406430, acc.: 79.69%] [G loss: 3.166978]\n",
      "epoch:45 step:35479 [D loss: 0.374168, acc.: 82.81%] [G loss: 3.035475]\n",
      "epoch:45 step:35480 [D loss: 0.376647, acc.: 80.47%] [G loss: 3.612443]\n",
      "epoch:45 step:35481 [D loss: 0.267500, acc.: 89.84%] [G loss: 2.971524]\n",
      "epoch:45 step:35482 [D loss: 0.261311, acc.: 89.06%] [G loss: 3.294579]\n",
      "epoch:45 step:35483 [D loss: 0.280499, acc.: 84.38%] [G loss: 4.208540]\n",
      "epoch:45 step:35484 [D loss: 0.247081, acc.: 90.62%] [G loss: 3.543915]\n",
      "epoch:45 step:35485 [D loss: 0.246439, acc.: 88.28%] [G loss: 6.699330]\n",
      "epoch:45 step:35486 [D loss: 0.376035, acc.: 85.16%] [G loss: 4.486611]\n",
      "epoch:45 step:35487 [D loss: 0.402021, acc.: 80.47%] [G loss: 3.304606]\n",
      "epoch:45 step:35488 [D loss: 0.423269, acc.: 75.78%] [G loss: 3.313855]\n",
      "epoch:45 step:35489 [D loss: 0.336069, acc.: 85.16%] [G loss: 3.623921]\n",
      "epoch:45 step:35490 [D loss: 0.385653, acc.: 84.38%] [G loss: 3.646240]\n",
      "epoch:45 step:35491 [D loss: 0.406172, acc.: 81.25%] [G loss: 2.758630]\n",
      "epoch:45 step:35492 [D loss: 0.281814, acc.: 88.28%] [G loss: 3.674017]\n",
      "epoch:45 step:35493 [D loss: 0.302297, acc.: 87.50%] [G loss: 3.219586]\n",
      "epoch:45 step:35494 [D loss: 0.279469, acc.: 89.84%] [G loss: 3.676274]\n",
      "epoch:45 step:35495 [D loss: 0.295688, acc.: 86.72%] [G loss: 3.080898]\n",
      "epoch:45 step:35496 [D loss: 0.296278, acc.: 88.28%] [G loss: 5.211443]\n",
      "epoch:45 step:35497 [D loss: 0.410918, acc.: 80.47%] [G loss: 6.698558]\n",
      "epoch:45 step:35498 [D loss: 0.304744, acc.: 81.25%] [G loss: 4.830996]\n",
      "epoch:45 step:35499 [D loss: 0.130536, acc.: 96.88%] [G loss: 11.162859]\n",
      "epoch:45 step:35500 [D loss: 0.209677, acc.: 90.62%] [G loss: 6.472992]\n",
      "epoch:45 step:35501 [D loss: 0.244930, acc.: 88.28%] [G loss: 4.210522]\n",
      "epoch:45 step:35502 [D loss: 0.358644, acc.: 81.25%] [G loss: 4.031789]\n",
      "epoch:45 step:35503 [D loss: 0.295003, acc.: 88.28%] [G loss: 3.088823]\n",
      "epoch:45 step:35504 [D loss: 0.254408, acc.: 89.06%] [G loss: 2.853974]\n",
      "epoch:45 step:35505 [D loss: 0.350207, acc.: 82.81%] [G loss: 4.216572]\n",
      "epoch:45 step:35506 [D loss: 0.400269, acc.: 80.47%] [G loss: 4.335073]\n",
      "epoch:45 step:35507 [D loss: 0.298323, acc.: 88.28%] [G loss: 3.734277]\n",
      "epoch:45 step:35508 [D loss: 0.325850, acc.: 85.16%] [G loss: 4.626156]\n",
      "epoch:45 step:35509 [D loss: 0.289929, acc.: 85.94%] [G loss: 4.122840]\n",
      "epoch:45 step:35510 [D loss: 0.248630, acc.: 89.84%] [G loss: 2.805967]\n",
      "epoch:45 step:35511 [D loss: 0.299438, acc.: 88.28%] [G loss: 3.580869]\n",
      "epoch:45 step:35512 [D loss: 0.280680, acc.: 88.28%] [G loss: 3.352534]\n",
      "epoch:45 step:35513 [D loss: 0.293201, acc.: 87.50%] [G loss: 3.015607]\n",
      "epoch:45 step:35514 [D loss: 0.367493, acc.: 84.38%] [G loss: 3.896162]\n",
      "epoch:45 step:35515 [D loss: 0.401373, acc.: 85.94%] [G loss: 3.026838]\n",
      "epoch:45 step:35516 [D loss: 0.273854, acc.: 84.38%] [G loss: 2.926105]\n",
      "epoch:45 step:35517 [D loss: 0.304420, acc.: 88.28%] [G loss: 3.507113]\n",
      "epoch:45 step:35518 [D loss: 0.319343, acc.: 85.16%] [G loss: 3.197458]\n",
      "epoch:45 step:35519 [D loss: 0.346707, acc.: 85.16%] [G loss: 3.491114]\n",
      "epoch:45 step:35520 [D loss: 0.256929, acc.: 88.28%] [G loss: 2.881468]\n",
      "epoch:45 step:35521 [D loss: 0.254716, acc.: 92.97%] [G loss: 3.164999]\n",
      "epoch:45 step:35522 [D loss: 0.223148, acc.: 91.41%] [G loss: 3.491349]\n",
      "epoch:45 step:35523 [D loss: 0.372577, acc.: 82.81%] [G loss: 4.091949]\n",
      "epoch:45 step:35524 [D loss: 0.334945, acc.: 82.03%] [G loss: 2.978264]\n",
      "epoch:45 step:35525 [D loss: 0.308928, acc.: 84.38%] [G loss: 3.601384]\n",
      "epoch:45 step:35526 [D loss: 0.319711, acc.: 84.38%] [G loss: 3.230907]\n",
      "epoch:45 step:35527 [D loss: 0.416773, acc.: 82.03%] [G loss: 3.202820]\n",
      "epoch:45 step:35528 [D loss: 0.356444, acc.: 84.38%] [G loss: 3.807550]\n",
      "epoch:45 step:35529 [D loss: 0.184436, acc.: 94.53%] [G loss: 2.811803]\n",
      "epoch:45 step:35530 [D loss: 0.222956, acc.: 92.19%] [G loss: 6.592972]\n",
      "epoch:45 step:35531 [D loss: 0.338881, acc.: 81.25%] [G loss: 4.521328]\n",
      "epoch:45 step:35532 [D loss: 0.267591, acc.: 89.84%] [G loss: 7.272744]\n",
      "epoch:45 step:35533 [D loss: 0.283837, acc.: 89.06%] [G loss: 3.657108]\n",
      "epoch:45 step:35534 [D loss: 0.291238, acc.: 85.94%] [G loss: 3.900807]\n",
      "epoch:45 step:35535 [D loss: 0.219473, acc.: 90.62%] [G loss: 4.690550]\n",
      "epoch:45 step:35536 [D loss: 0.327078, acc.: 84.38%] [G loss: 4.924791]\n",
      "epoch:45 step:35537 [D loss: 0.190820, acc.: 90.62%] [G loss: 4.245002]\n",
      "epoch:45 step:35538 [D loss: 0.233058, acc.: 89.06%] [G loss: 5.544436]\n",
      "epoch:45 step:35539 [D loss: 0.334746, acc.: 83.59%] [G loss: 5.398337]\n",
      "epoch:45 step:35540 [D loss: 0.238152, acc.: 89.84%] [G loss: 4.956023]\n",
      "epoch:45 step:35541 [D loss: 0.299386, acc.: 85.94%] [G loss: 3.206440]\n",
      "epoch:45 step:35542 [D loss: 0.298872, acc.: 88.28%] [G loss: 3.501717]\n",
      "epoch:45 step:35543 [D loss: 0.266431, acc.: 87.50%] [G loss: 2.597643]\n",
      "epoch:45 step:35544 [D loss: 0.406648, acc.: 82.03%] [G loss: 3.197061]\n",
      "epoch:45 step:35545 [D loss: 0.266206, acc.: 88.28%] [G loss: 3.768493]\n",
      "epoch:45 step:35546 [D loss: 0.236453, acc.: 89.06%] [G loss: 2.682943]\n",
      "epoch:45 step:35547 [D loss: 0.342679, acc.: 89.84%] [G loss: 2.992260]\n",
      "epoch:45 step:35548 [D loss: 0.380890, acc.: 82.03%] [G loss: 2.992864]\n",
      "epoch:45 step:35549 [D loss: 0.249824, acc.: 89.06%] [G loss: 2.939008]\n",
      "epoch:45 step:35550 [D loss: 0.336144, acc.: 83.59%] [G loss: 2.914584]\n",
      "epoch:45 step:35551 [D loss: 0.500908, acc.: 80.47%] [G loss: 3.038066]\n",
      "epoch:45 step:35552 [D loss: 0.337510, acc.: 85.94%] [G loss: 3.656058]\n",
      "epoch:45 step:35553 [D loss: 0.428619, acc.: 78.12%] [G loss: 5.565057]\n",
      "epoch:45 step:35554 [D loss: 0.528209, acc.: 81.25%] [G loss: 7.850516]\n",
      "epoch:45 step:35555 [D loss: 1.056710, acc.: 68.75%] [G loss: 8.454810]\n",
      "epoch:45 step:35556 [D loss: 3.139949, acc.: 53.12%] [G loss: 3.375022]\n",
      "epoch:45 step:35557 [D loss: 0.406466, acc.: 85.16%] [G loss: 3.165098]\n",
      "epoch:45 step:35558 [D loss: 0.299719, acc.: 85.16%] [G loss: 4.134116]\n",
      "epoch:45 step:35559 [D loss: 0.585644, acc.: 75.00%] [G loss: 4.110936]\n",
      "epoch:45 step:35560 [D loss: 0.379400, acc.: 84.38%] [G loss: 5.391842]\n",
      "epoch:45 step:35561 [D loss: 0.404586, acc.: 82.81%] [G loss: 3.716168]\n",
      "epoch:45 step:35562 [D loss: 0.379392, acc.: 81.25%] [G loss: 3.558435]\n",
      "epoch:45 step:35563 [D loss: 0.274283, acc.: 89.06%] [G loss: 2.536898]\n",
      "epoch:45 step:35564 [D loss: 0.304040, acc.: 87.50%] [G loss: 2.469928]\n",
      "epoch:45 step:35565 [D loss: 0.302912, acc.: 85.94%] [G loss: 2.724139]\n",
      "epoch:45 step:35566 [D loss: 0.242358, acc.: 89.06%] [G loss: 2.995772]\n",
      "epoch:45 step:35567 [D loss: 0.358180, acc.: 81.25%] [G loss: 2.991314]\n",
      "epoch:45 step:35568 [D loss: 0.275930, acc.: 91.41%] [G loss: 2.488837]\n",
      "epoch:45 step:35569 [D loss: 0.392564, acc.: 80.47%] [G loss: 2.568357]\n",
      "epoch:45 step:35570 [D loss: 0.299031, acc.: 83.59%] [G loss: 2.938686]\n",
      "epoch:45 step:35571 [D loss: 0.304112, acc.: 87.50%] [G loss: 3.064025]\n",
      "epoch:45 step:35572 [D loss: 0.257162, acc.: 86.72%] [G loss: 3.807515]\n",
      "epoch:45 step:35573 [D loss: 0.382085, acc.: 80.47%] [G loss: 2.494852]\n",
      "epoch:45 step:35574 [D loss: 0.365507, acc.: 82.81%] [G loss: 2.991408]\n",
      "epoch:45 step:35575 [D loss: 0.355356, acc.: 80.47%] [G loss: 2.811379]\n",
      "epoch:45 step:35576 [D loss: 0.375352, acc.: 82.03%] [G loss: 3.201742]\n",
      "epoch:45 step:35577 [D loss: 0.420701, acc.: 79.69%] [G loss: 3.524689]\n",
      "epoch:45 step:35578 [D loss: 0.233283, acc.: 90.62%] [G loss: 3.683005]\n",
      "epoch:45 step:35579 [D loss: 0.268288, acc.: 88.28%] [G loss: 3.196525]\n",
      "epoch:45 step:35580 [D loss: 0.359052, acc.: 86.72%] [G loss: 2.011422]\n",
      "epoch:45 step:35581 [D loss: 0.353180, acc.: 81.25%] [G loss: 2.670192]\n",
      "epoch:45 step:35582 [D loss: 0.247515, acc.: 91.41%] [G loss: 3.287675]\n",
      "epoch:45 step:35583 [D loss: 0.443477, acc.: 80.47%] [G loss: 4.259954]\n",
      "epoch:45 step:35584 [D loss: 0.379645, acc.: 81.25%] [G loss: 2.823291]\n",
      "epoch:45 step:35585 [D loss: 0.261741, acc.: 89.84%] [G loss: 3.587976]\n",
      "epoch:45 step:35586 [D loss: 0.383287, acc.: 79.69%] [G loss: 3.520106]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:45 step:35587 [D loss: 0.197925, acc.: 92.19%] [G loss: 3.246006]\n",
      "epoch:45 step:35588 [D loss: 0.333087, acc.: 83.59%] [G loss: 3.872380]\n",
      "epoch:45 step:35589 [D loss: 0.304992, acc.: 83.59%] [G loss: 3.541190]\n",
      "epoch:45 step:35590 [D loss: 0.340497, acc.: 81.25%] [G loss: 2.753716]\n",
      "epoch:45 step:35591 [D loss: 0.273950, acc.: 85.16%] [G loss: 3.060121]\n",
      "epoch:45 step:35592 [D loss: 0.300691, acc.: 88.28%] [G loss: 2.880212]\n",
      "epoch:45 step:35593 [D loss: 0.354163, acc.: 85.94%] [G loss: 2.610813]\n",
      "epoch:45 step:35594 [D loss: 0.220902, acc.: 90.62%] [G loss: 2.689567]\n",
      "epoch:45 step:35595 [D loss: 0.261735, acc.: 89.84%] [G loss: 2.968088]\n",
      "epoch:45 step:35596 [D loss: 0.265575, acc.: 89.84%] [G loss: 4.548473]\n",
      "epoch:45 step:35597 [D loss: 0.311175, acc.: 85.16%] [G loss: 3.434802]\n",
      "epoch:45 step:35598 [D loss: 0.339114, acc.: 84.38%] [G loss: 2.759701]\n",
      "epoch:45 step:35599 [D loss: 0.323344, acc.: 87.50%] [G loss: 2.700703]\n",
      "epoch:45 step:35600 [D loss: 0.415978, acc.: 83.59%] [G loss: 2.581429]\n",
      "##############\n",
      "[0.8492068  0.85686456 0.82364302 0.79093393 0.7450953  0.84974593\n",
      " 0.86252928 0.82519129 0.80424708 0.81731183]\n",
      "##########\n",
      "epoch:45 step:35601 [D loss: 0.337275, acc.: 85.16%] [G loss: 3.293519]\n",
      "epoch:45 step:35602 [D loss: 0.316565, acc.: 84.38%] [G loss: 3.034286]\n",
      "epoch:45 step:35603 [D loss: 0.293427, acc.: 86.72%] [G loss: 3.037213]\n",
      "epoch:45 step:35604 [D loss: 0.300017, acc.: 85.94%] [G loss: 2.488171]\n",
      "epoch:45 step:35605 [D loss: 0.368375, acc.: 82.81%] [G loss: 2.220900]\n",
      "epoch:45 step:35606 [D loss: 0.359963, acc.: 85.16%] [G loss: 3.256658]\n",
      "epoch:45 step:35607 [D loss: 0.319239, acc.: 86.72%] [G loss: 3.320470]\n",
      "epoch:45 step:35608 [D loss: 0.298454, acc.: 88.28%] [G loss: 2.977701]\n",
      "epoch:45 step:35609 [D loss: 0.291404, acc.: 85.94%] [G loss: 2.595453]\n",
      "epoch:45 step:35610 [D loss: 0.353935, acc.: 85.16%] [G loss: 2.582954]\n",
      "epoch:45 step:35611 [D loss: 0.350299, acc.: 84.38%] [G loss: 2.691452]\n",
      "epoch:45 step:35612 [D loss: 0.513319, acc.: 75.00%] [G loss: 5.152554]\n",
      "epoch:45 step:35613 [D loss: 0.667028, acc.: 75.00%] [G loss: 5.478441]\n",
      "epoch:45 step:35614 [D loss: 0.406442, acc.: 82.81%] [G loss: 3.501758]\n",
      "epoch:45 step:35615 [D loss: 0.286320, acc.: 88.28%] [G loss: 4.615895]\n",
      "epoch:45 step:35616 [D loss: 0.356595, acc.: 84.38%] [G loss: 3.860279]\n",
      "epoch:45 step:35617 [D loss: 0.211419, acc.: 91.41%] [G loss: 3.898930]\n",
      "epoch:45 step:35618 [D loss: 0.257827, acc.: 90.62%] [G loss: 2.762008]\n",
      "epoch:45 step:35619 [D loss: 0.340365, acc.: 85.16%] [G loss: 3.650051]\n",
      "epoch:45 step:35620 [D loss: 0.239506, acc.: 89.84%] [G loss: 3.820016]\n",
      "epoch:45 step:35621 [D loss: 0.224021, acc.: 92.19%] [G loss: 4.856308]\n",
      "epoch:45 step:35622 [D loss: 0.295325, acc.: 87.50%] [G loss: 3.463941]\n",
      "epoch:45 step:35623 [D loss: 0.259905, acc.: 89.84%] [G loss: 2.533703]\n",
      "epoch:45 step:35624 [D loss: 0.260605, acc.: 87.50%] [G loss: 3.152228]\n",
      "epoch:45 step:35625 [D loss: 0.296640, acc.: 85.94%] [G loss: 2.824577]\n",
      "epoch:45 step:35626 [D loss: 0.313347, acc.: 85.16%] [G loss: 2.740440]\n",
      "epoch:45 step:35627 [D loss: 0.290000, acc.: 88.28%] [G loss: 3.253651]\n",
      "epoch:45 step:35628 [D loss: 0.273409, acc.: 92.97%] [G loss: 2.977763]\n",
      "epoch:45 step:35629 [D loss: 0.284963, acc.: 85.16%] [G loss: 3.147828]\n",
      "epoch:45 step:35630 [D loss: 0.238846, acc.: 89.84%] [G loss: 2.615970]\n",
      "epoch:45 step:35631 [D loss: 0.285080, acc.: 82.03%] [G loss: 2.719379]\n",
      "epoch:45 step:35632 [D loss: 0.317291, acc.: 88.28%] [G loss: 3.165640]\n",
      "epoch:45 step:35633 [D loss: 0.308332, acc.: 88.28%] [G loss: 2.903213]\n",
      "epoch:45 step:35634 [D loss: 0.372819, acc.: 85.94%] [G loss: 3.621300]\n",
      "epoch:45 step:35635 [D loss: 0.333855, acc.: 84.38%] [G loss: 2.871681]\n",
      "epoch:45 step:35636 [D loss: 0.303096, acc.: 84.38%] [G loss: 3.283340]\n",
      "epoch:45 step:35637 [D loss: 0.311436, acc.: 86.72%] [G loss: 3.137197]\n",
      "epoch:45 step:35638 [D loss: 0.303527, acc.: 86.72%] [G loss: 3.036931]\n",
      "epoch:45 step:35639 [D loss: 0.318330, acc.: 84.38%] [G loss: 3.084274]\n",
      "epoch:45 step:35640 [D loss: 0.386864, acc.: 82.81%] [G loss: 2.477670]\n",
      "epoch:45 step:35641 [D loss: 0.225099, acc.: 93.75%] [G loss: 3.176866]\n",
      "epoch:45 step:35642 [D loss: 0.243060, acc.: 89.84%] [G loss: 2.799172]\n",
      "epoch:45 step:35643 [D loss: 0.235984, acc.: 89.84%] [G loss: 3.692010]\n",
      "epoch:45 step:35644 [D loss: 0.290901, acc.: 86.72%] [G loss: 2.784802]\n",
      "epoch:45 step:35645 [D loss: 0.308215, acc.: 89.06%] [G loss: 3.499127]\n",
      "epoch:45 step:35646 [D loss: 0.360445, acc.: 87.50%] [G loss: 3.105629]\n",
      "epoch:45 step:35647 [D loss: 0.371323, acc.: 85.94%] [G loss: 4.096147]\n",
      "epoch:45 step:35648 [D loss: 0.237202, acc.: 90.62%] [G loss: 3.614606]\n",
      "epoch:45 step:35649 [D loss: 0.246377, acc.: 86.72%] [G loss: 3.512710]\n",
      "epoch:45 step:35650 [D loss: 0.274722, acc.: 85.94%] [G loss: 3.010199]\n",
      "epoch:45 step:35651 [D loss: 0.368588, acc.: 85.94%] [G loss: 3.422406]\n",
      "epoch:45 step:35652 [D loss: 0.374771, acc.: 82.03%] [G loss: 3.153022]\n",
      "epoch:45 step:35653 [D loss: 0.256270, acc.: 92.19%] [G loss: 2.663478]\n",
      "epoch:45 step:35654 [D loss: 0.342410, acc.: 81.25%] [G loss: 3.049507]\n",
      "epoch:45 step:35655 [D loss: 0.294768, acc.: 86.72%] [G loss: 4.162058]\n",
      "epoch:45 step:35656 [D loss: 0.241183, acc.: 91.41%] [G loss: 3.549383]\n",
      "epoch:45 step:35657 [D loss: 0.195732, acc.: 91.41%] [G loss: 3.841758]\n",
      "epoch:45 step:35658 [D loss: 0.295401, acc.: 88.28%] [G loss: 2.920975]\n",
      "epoch:45 step:35659 [D loss: 0.316670, acc.: 86.72%] [G loss: 2.845074]\n",
      "epoch:45 step:35660 [D loss: 0.275666, acc.: 86.72%] [G loss: 3.749580]\n",
      "epoch:45 step:35661 [D loss: 0.294363, acc.: 85.94%] [G loss: 3.201006]\n",
      "epoch:45 step:35662 [D loss: 0.321438, acc.: 85.16%] [G loss: 4.856251]\n",
      "epoch:45 step:35663 [D loss: 0.187406, acc.: 92.19%] [G loss: 4.109139]\n",
      "epoch:45 step:35664 [D loss: 0.329369, acc.: 87.50%] [G loss: 3.251834]\n",
      "epoch:45 step:35665 [D loss: 0.312583, acc.: 85.16%] [G loss: 2.286889]\n",
      "epoch:45 step:35666 [D loss: 0.235999, acc.: 91.41%] [G loss: 3.028004]\n",
      "epoch:45 step:35667 [D loss: 0.352662, acc.: 85.94%] [G loss: 3.375307]\n",
      "epoch:45 step:35668 [D loss: 0.299802, acc.: 87.50%] [G loss: 3.423639]\n",
      "epoch:45 step:35669 [D loss: 0.343139, acc.: 85.94%] [G loss: 2.943786]\n",
      "epoch:45 step:35670 [D loss: 0.256308, acc.: 85.94%] [G loss: 4.463149]\n",
      "epoch:45 step:35671 [D loss: 0.495313, acc.: 82.81%] [G loss: 2.970085]\n",
      "epoch:45 step:35672 [D loss: 0.260226, acc.: 86.72%] [G loss: 2.887269]\n",
      "epoch:45 step:35673 [D loss: 0.223614, acc.: 92.97%] [G loss: 3.781166]\n",
      "epoch:45 step:35674 [D loss: 0.282613, acc.: 89.06%] [G loss: 5.339109]\n",
      "epoch:45 step:35675 [D loss: 0.447992, acc.: 80.47%] [G loss: 3.702132]\n",
      "epoch:45 step:35676 [D loss: 0.312225, acc.: 83.59%] [G loss: 3.266310]\n",
      "epoch:45 step:35677 [D loss: 0.284551, acc.: 89.06%] [G loss: 3.019136]\n",
      "epoch:45 step:35678 [D loss: 0.255939, acc.: 92.19%] [G loss: 3.139601]\n",
      "epoch:45 step:35679 [D loss: 0.270425, acc.: 88.28%] [G loss: 2.832742]\n",
      "epoch:45 step:35680 [D loss: 0.250751, acc.: 88.28%] [G loss: 3.419328]\n",
      "epoch:45 step:35681 [D loss: 0.299006, acc.: 84.38%] [G loss: 3.730970]\n",
      "epoch:45 step:35682 [D loss: 0.284391, acc.: 89.06%] [G loss: 3.111500]\n",
      "epoch:45 step:35683 [D loss: 0.378365, acc.: 85.94%] [G loss: 4.092616]\n",
      "epoch:45 step:35684 [D loss: 0.338681, acc.: 86.72%] [G loss: 3.825255]\n",
      "epoch:45 step:35685 [D loss: 0.268091, acc.: 85.16%] [G loss: 3.232143]\n",
      "epoch:45 step:35686 [D loss: 0.262005, acc.: 89.06%] [G loss: 3.182146]\n",
      "epoch:45 step:35687 [D loss: 0.280670, acc.: 90.62%] [G loss: 3.298606]\n",
      "epoch:45 step:35688 [D loss: 0.333555, acc.: 84.38%] [G loss: 2.379217]\n",
      "epoch:45 step:35689 [D loss: 0.326385, acc.: 83.59%] [G loss: 4.252397]\n",
      "epoch:45 step:35690 [D loss: 0.332128, acc.: 83.59%] [G loss: 3.874137]\n",
      "epoch:45 step:35691 [D loss: 0.273273, acc.: 92.19%] [G loss: 3.458835]\n",
      "epoch:45 step:35692 [D loss: 0.318727, acc.: 84.38%] [G loss: 2.985767]\n",
      "epoch:45 step:35693 [D loss: 0.357946, acc.: 87.50%] [G loss: 3.733538]\n",
      "epoch:45 step:35694 [D loss: 0.261836, acc.: 89.06%] [G loss: 3.457358]\n",
      "epoch:45 step:35695 [D loss: 0.313102, acc.: 89.84%] [G loss: 3.572066]\n",
      "epoch:45 step:35696 [D loss: 0.251460, acc.: 87.50%] [G loss: 3.703773]\n",
      "epoch:45 step:35697 [D loss: 0.306500, acc.: 85.94%] [G loss: 3.780295]\n",
      "epoch:45 step:35698 [D loss: 0.386431, acc.: 83.59%] [G loss: 4.207999]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:45 step:35699 [D loss: 0.271074, acc.: 89.84%] [G loss: 3.603795]\n",
      "epoch:45 step:35700 [D loss: 0.341384, acc.: 78.91%] [G loss: 3.325552]\n",
      "epoch:45 step:35701 [D loss: 0.279305, acc.: 89.84%] [G loss: 3.923016]\n",
      "epoch:45 step:35702 [D loss: 0.293310, acc.: 86.72%] [G loss: 2.924416]\n",
      "epoch:45 step:35703 [D loss: 0.328195, acc.: 84.38%] [G loss: 2.908664]\n",
      "epoch:45 step:35704 [D loss: 0.279981, acc.: 88.28%] [G loss: 3.499430]\n",
      "epoch:45 step:35705 [D loss: 0.266825, acc.: 91.41%] [G loss: 3.119102]\n",
      "epoch:45 step:35706 [D loss: 0.204158, acc.: 93.75%] [G loss: 4.105309]\n",
      "epoch:45 step:35707 [D loss: 0.350670, acc.: 79.69%] [G loss: 4.411155]\n",
      "epoch:45 step:35708 [D loss: 0.226444, acc.: 91.41%] [G loss: 5.724549]\n",
      "epoch:45 step:35709 [D loss: 0.289982, acc.: 85.94%] [G loss: 2.436312]\n",
      "epoch:45 step:35710 [D loss: 0.274089, acc.: 90.62%] [G loss: 3.987247]\n",
      "epoch:45 step:35711 [D loss: 0.248361, acc.: 89.06%] [G loss: 3.671387]\n",
      "epoch:45 step:35712 [D loss: 0.279925, acc.: 87.50%] [G loss: 3.094404]\n",
      "epoch:45 step:35713 [D loss: 0.271519, acc.: 87.50%] [G loss: 2.648617]\n",
      "epoch:45 step:35714 [D loss: 0.349469, acc.: 82.81%] [G loss: 2.733022]\n",
      "epoch:45 step:35715 [D loss: 0.247150, acc.: 88.28%] [G loss: 2.735912]\n",
      "epoch:45 step:35716 [D loss: 0.469606, acc.: 77.34%] [G loss: 3.767470]\n",
      "epoch:45 step:35717 [D loss: 0.427424, acc.: 85.94%] [G loss: 4.322628]\n",
      "epoch:45 step:35718 [D loss: 0.329253, acc.: 85.16%] [G loss: 3.225076]\n",
      "epoch:45 step:35719 [D loss: 0.337467, acc.: 87.50%] [G loss: 3.309191]\n",
      "epoch:45 step:35720 [D loss: 0.275483, acc.: 87.50%] [G loss: 3.041493]\n",
      "epoch:45 step:35721 [D loss: 0.260234, acc.: 92.19%] [G loss: 2.858938]\n",
      "epoch:45 step:35722 [D loss: 0.357455, acc.: 82.03%] [G loss: 3.599205]\n",
      "epoch:45 step:35723 [D loss: 0.346781, acc.: 84.38%] [G loss: 2.702816]\n",
      "epoch:45 step:35724 [D loss: 0.287753, acc.: 89.06%] [G loss: 3.929053]\n",
      "epoch:45 step:35725 [D loss: 0.291954, acc.: 83.59%] [G loss: 3.013151]\n",
      "epoch:45 step:35726 [D loss: 0.267240, acc.: 85.94%] [G loss: 3.217380]\n",
      "epoch:45 step:35727 [D loss: 0.256513, acc.: 90.62%] [G loss: 5.497556]\n",
      "epoch:45 step:35728 [D loss: 0.405302, acc.: 85.16%] [G loss: 3.806666]\n",
      "epoch:45 step:35729 [D loss: 0.254401, acc.: 90.62%] [G loss: 3.656594]\n",
      "epoch:45 step:35730 [D loss: 0.328221, acc.: 84.38%] [G loss: 3.648190]\n",
      "epoch:45 step:35731 [D loss: 0.285817, acc.: 88.28%] [G loss: 4.036243]\n",
      "epoch:45 step:35732 [D loss: 0.208814, acc.: 90.62%] [G loss: 3.418022]\n",
      "epoch:45 step:35733 [D loss: 0.233339, acc.: 92.19%] [G loss: 4.316820]\n",
      "epoch:45 step:35734 [D loss: 0.467564, acc.: 77.34%] [G loss: 3.327086]\n",
      "epoch:45 step:35735 [D loss: 0.330591, acc.: 83.59%] [G loss: 3.097269]\n",
      "epoch:45 step:35736 [D loss: 0.501584, acc.: 75.00%] [G loss: 4.269326]\n",
      "epoch:45 step:35737 [D loss: 0.193023, acc.: 92.97%] [G loss: 3.751973]\n",
      "epoch:45 step:35738 [D loss: 0.249387, acc.: 88.28%] [G loss: 4.263220]\n",
      "epoch:45 step:35739 [D loss: 0.302565, acc.: 89.06%] [G loss: 4.680714]\n",
      "epoch:45 step:35740 [D loss: 0.302546, acc.: 89.84%] [G loss: 4.959360]\n",
      "epoch:45 step:35741 [D loss: 0.421060, acc.: 83.59%] [G loss: 5.010218]\n",
      "epoch:45 step:35742 [D loss: 0.435006, acc.: 80.47%] [G loss: 3.429185]\n",
      "epoch:45 step:35743 [D loss: 0.414323, acc.: 75.78%] [G loss: 4.315349]\n",
      "epoch:45 step:35744 [D loss: 0.292797, acc.: 85.16%] [G loss: 4.243270]\n",
      "epoch:45 step:35745 [D loss: 0.413277, acc.: 82.03%] [G loss: 3.106368]\n",
      "epoch:45 step:35746 [D loss: 0.360361, acc.: 80.47%] [G loss: 2.991110]\n",
      "epoch:45 step:35747 [D loss: 0.260914, acc.: 90.62%] [G loss: 3.024196]\n",
      "epoch:45 step:35748 [D loss: 0.220492, acc.: 91.41%] [G loss: 4.587837]\n",
      "epoch:45 step:35749 [D loss: 0.343743, acc.: 83.59%] [G loss: 3.432168]\n",
      "epoch:45 step:35750 [D loss: 0.327629, acc.: 85.16%] [G loss: 3.100422]\n",
      "epoch:45 step:35751 [D loss: 0.307339, acc.: 85.16%] [G loss: 3.097335]\n",
      "epoch:45 step:35752 [D loss: 0.315872, acc.: 85.16%] [G loss: 3.980702]\n",
      "epoch:45 step:35753 [D loss: 0.279041, acc.: 91.41%] [G loss: 3.134914]\n",
      "epoch:45 step:35754 [D loss: 0.265672, acc.: 86.72%] [G loss: 3.672979]\n",
      "epoch:45 step:35755 [D loss: 0.353068, acc.: 84.38%] [G loss: 3.042259]\n",
      "epoch:45 step:35756 [D loss: 0.306378, acc.: 85.16%] [G loss: 3.475869]\n",
      "epoch:45 step:35757 [D loss: 0.219648, acc.: 93.75%] [G loss: 3.012261]\n",
      "epoch:45 step:35758 [D loss: 0.341084, acc.: 84.38%] [G loss: 3.391852]\n",
      "epoch:45 step:35759 [D loss: 0.375663, acc.: 82.81%] [G loss: 3.047854]\n",
      "epoch:45 step:35760 [D loss: 0.236872, acc.: 89.06%] [G loss: 4.180854]\n",
      "epoch:45 step:35761 [D loss: 0.387412, acc.: 84.38%] [G loss: 3.348460]\n",
      "epoch:45 step:35762 [D loss: 0.349730, acc.: 85.16%] [G loss: 2.579854]\n",
      "epoch:45 step:35763 [D loss: 0.306999, acc.: 85.94%] [G loss: 3.038851]\n",
      "epoch:45 step:35764 [D loss: 0.325872, acc.: 86.72%] [G loss: 3.172379]\n",
      "epoch:45 step:35765 [D loss: 0.375526, acc.: 85.16%] [G loss: 3.302721]\n",
      "epoch:45 step:35766 [D loss: 0.224523, acc.: 92.19%] [G loss: 3.734089]\n",
      "epoch:45 step:35767 [D loss: 0.217679, acc.: 89.06%] [G loss: 3.610274]\n",
      "epoch:45 step:35768 [D loss: 0.409442, acc.: 80.47%] [G loss: 3.508990]\n",
      "epoch:45 step:35769 [D loss: 0.243350, acc.: 91.41%] [G loss: 4.829751]\n",
      "epoch:45 step:35770 [D loss: 0.311092, acc.: 85.94%] [G loss: 4.326836]\n",
      "epoch:45 step:35771 [D loss: 0.202878, acc.: 92.97%] [G loss: 4.449921]\n",
      "epoch:45 step:35772 [D loss: 0.259613, acc.: 89.84%] [G loss: 4.133304]\n",
      "epoch:45 step:35773 [D loss: 0.315388, acc.: 83.59%] [G loss: 2.535045]\n",
      "epoch:45 step:35774 [D loss: 0.256887, acc.: 89.84%] [G loss: 3.775784]\n",
      "epoch:45 step:35775 [D loss: 0.263510, acc.: 88.28%] [G loss: 3.196068]\n",
      "epoch:45 step:35776 [D loss: 0.386719, acc.: 82.03%] [G loss: 3.239462]\n",
      "epoch:45 step:35777 [D loss: 0.251206, acc.: 89.84%] [G loss: 3.148057]\n",
      "epoch:45 step:35778 [D loss: 0.283140, acc.: 89.84%] [G loss: 2.852695]\n",
      "epoch:45 step:35779 [D loss: 0.310516, acc.: 85.94%] [G loss: 3.005745]\n",
      "epoch:45 step:35780 [D loss: 0.331641, acc.: 86.72%] [G loss: 2.951182]\n",
      "epoch:45 step:35781 [D loss: 0.436323, acc.: 76.56%] [G loss: 2.626464]\n",
      "epoch:45 step:35782 [D loss: 0.231798, acc.: 87.50%] [G loss: 4.369123]\n",
      "epoch:45 step:35783 [D loss: 0.238727, acc.: 90.62%] [G loss: 3.442078]\n",
      "epoch:45 step:35784 [D loss: 0.336974, acc.: 85.16%] [G loss: 4.781956]\n",
      "epoch:45 step:35785 [D loss: 0.294785, acc.: 89.06%] [G loss: 4.400574]\n",
      "epoch:45 step:35786 [D loss: 0.237733, acc.: 91.41%] [G loss: 4.101870]\n",
      "epoch:45 step:35787 [D loss: 0.258174, acc.: 88.28%] [G loss: 3.336807]\n",
      "epoch:45 step:35788 [D loss: 0.283469, acc.: 87.50%] [G loss: 4.441159]\n",
      "epoch:45 step:35789 [D loss: 0.253424, acc.: 87.50%] [G loss: 3.439237]\n",
      "epoch:45 step:35790 [D loss: 0.195445, acc.: 93.75%] [G loss: 3.545400]\n",
      "epoch:45 step:35791 [D loss: 0.386684, acc.: 85.94%] [G loss: 2.880251]\n",
      "epoch:45 step:35792 [D loss: 0.354985, acc.: 83.59%] [G loss: 3.274375]\n",
      "epoch:45 step:35793 [D loss: 0.331454, acc.: 85.94%] [G loss: 2.783472]\n",
      "epoch:45 step:35794 [D loss: 0.217127, acc.: 93.75%] [G loss: 3.587658]\n",
      "epoch:45 step:35795 [D loss: 0.370242, acc.: 82.81%] [G loss: 2.965099]\n",
      "epoch:45 step:35796 [D loss: 0.258393, acc.: 88.28%] [G loss: 3.194168]\n",
      "epoch:45 step:35797 [D loss: 0.406168, acc.: 78.91%] [G loss: 3.033605]\n",
      "epoch:45 step:35798 [D loss: 0.290400, acc.: 87.50%] [G loss: 3.072410]\n",
      "epoch:45 step:35799 [D loss: 0.315950, acc.: 86.72%] [G loss: 4.036803]\n",
      "epoch:45 step:35800 [D loss: 0.232168, acc.: 88.28%] [G loss: 3.844779]\n",
      "##############\n",
      "[0.85778506 0.86860261 0.80038495 0.77512022 0.76364241 0.84077094\n",
      " 0.86586295 0.85305773 0.84131222 0.82555433]\n",
      "##########\n",
      "epoch:45 step:35801 [D loss: 0.315909, acc.: 86.72%] [G loss: 3.776005]\n",
      "epoch:45 step:35802 [D loss: 0.371208, acc.: 85.16%] [G loss: 3.543984]\n",
      "epoch:45 step:35803 [D loss: 0.275015, acc.: 89.06%] [G loss: 3.358254]\n",
      "epoch:45 step:35804 [D loss: 0.291858, acc.: 89.84%] [G loss: 3.025913]\n",
      "epoch:45 step:35805 [D loss: 0.351001, acc.: 82.81%] [G loss: 3.937418]\n",
      "epoch:45 step:35806 [D loss: 0.268450, acc.: 89.06%] [G loss: 4.018733]\n",
      "epoch:45 step:35807 [D loss: 0.334180, acc.: 82.81%] [G loss: 2.978947]\n",
      "epoch:45 step:35808 [D loss: 0.202559, acc.: 91.41%] [G loss: 3.426900]\n",
      "epoch:45 step:35809 [D loss: 0.445770, acc.: 78.91%] [G loss: 3.519720]\n",
      "epoch:45 step:35810 [D loss: 0.209062, acc.: 91.41%] [G loss: 3.509402]\n",
      "epoch:45 step:35811 [D loss: 0.254153, acc.: 89.06%] [G loss: 3.310447]\n",
      "epoch:45 step:35812 [D loss: 0.311708, acc.: 82.03%] [G loss: 3.238497]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:45 step:35813 [D loss: 0.219648, acc.: 91.41%] [G loss: 3.092541]\n",
      "epoch:45 step:35814 [D loss: 0.299344, acc.: 88.28%] [G loss: 2.894534]\n",
      "epoch:45 step:35815 [D loss: 0.295550, acc.: 85.94%] [G loss: 2.903528]\n",
      "epoch:45 step:35816 [D loss: 0.362208, acc.: 83.59%] [G loss: 2.890875]\n",
      "epoch:45 step:35817 [D loss: 0.312986, acc.: 85.16%] [G loss: 3.574983]\n",
      "epoch:45 step:35818 [D loss: 0.249803, acc.: 90.62%] [G loss: 3.506126]\n",
      "epoch:45 step:35819 [D loss: 0.298992, acc.: 89.06%] [G loss: 4.102670]\n",
      "epoch:45 step:35820 [D loss: 0.326971, acc.: 86.72%] [G loss: 7.701855]\n",
      "epoch:45 step:35821 [D loss: 0.691875, acc.: 71.09%] [G loss: 4.152425]\n",
      "epoch:45 step:35822 [D loss: 0.552357, acc.: 79.69%] [G loss: 3.440428]\n",
      "epoch:45 step:35823 [D loss: 0.340496, acc.: 85.16%] [G loss: 4.204087]\n",
      "epoch:45 step:35824 [D loss: 0.269285, acc.: 86.72%] [G loss: 3.237739]\n",
      "epoch:45 step:35825 [D loss: 0.308630, acc.: 87.50%] [G loss: 3.267669]\n",
      "epoch:45 step:35826 [D loss: 0.343060, acc.: 81.25%] [G loss: 2.686009]\n",
      "epoch:45 step:35827 [D loss: 0.691004, acc.: 69.53%] [G loss: 3.604472]\n",
      "epoch:45 step:35828 [D loss: 0.723795, acc.: 71.09%] [G loss: 3.913589]\n",
      "epoch:45 step:35829 [D loss: 0.483101, acc.: 79.69%] [G loss: 3.470477]\n",
      "epoch:45 step:35830 [D loss: 0.537908, acc.: 73.44%] [G loss: 3.012135]\n",
      "epoch:45 step:35831 [D loss: 0.225633, acc.: 90.62%] [G loss: 3.466320]\n",
      "epoch:45 step:35832 [D loss: 0.250229, acc.: 89.06%] [G loss: 3.928722]\n",
      "epoch:45 step:35833 [D loss: 0.240239, acc.: 87.50%] [G loss: 4.676937]\n",
      "epoch:45 step:35834 [D loss: 0.299099, acc.: 85.16%] [G loss: 3.843013]\n",
      "epoch:45 step:35835 [D loss: 0.310135, acc.: 85.94%] [G loss: 3.064149]\n",
      "epoch:45 step:35836 [D loss: 0.327072, acc.: 86.72%] [G loss: 3.375629]\n",
      "epoch:45 step:35837 [D loss: 0.267438, acc.: 90.62%] [G loss: 2.736471]\n",
      "epoch:45 step:35838 [D loss: 0.215596, acc.: 91.41%] [G loss: 2.991003]\n",
      "epoch:45 step:35839 [D loss: 0.285902, acc.: 87.50%] [G loss: 2.832437]\n",
      "epoch:45 step:35840 [D loss: 0.273594, acc.: 91.41%] [G loss: 4.024223]\n",
      "epoch:45 step:35841 [D loss: 0.361509, acc.: 83.59%] [G loss: 4.085993]\n",
      "epoch:45 step:35842 [D loss: 0.370874, acc.: 84.38%] [G loss: 3.435104]\n",
      "epoch:45 step:35843 [D loss: 0.352664, acc.: 82.81%] [G loss: 3.669088]\n",
      "epoch:45 step:35844 [D loss: 0.286696, acc.: 89.84%] [G loss: 3.516913]\n",
      "epoch:45 step:35845 [D loss: 0.312588, acc.: 86.72%] [G loss: 3.033941]\n",
      "epoch:45 step:35846 [D loss: 0.346021, acc.: 89.06%] [G loss: 2.929473]\n",
      "epoch:45 step:35847 [D loss: 0.269599, acc.: 85.94%] [G loss: 3.400433]\n",
      "epoch:45 step:35848 [D loss: 0.382527, acc.: 82.81%] [G loss: 2.721090]\n",
      "epoch:45 step:35849 [D loss: 0.286998, acc.: 88.28%] [G loss: 3.814557]\n",
      "epoch:45 step:35850 [D loss: 0.356076, acc.: 82.81%] [G loss: 3.896158]\n",
      "epoch:45 step:35851 [D loss: 0.370996, acc.: 85.94%] [G loss: 2.566477]\n",
      "epoch:45 step:35852 [D loss: 0.305427, acc.: 86.72%] [G loss: 2.701893]\n",
      "epoch:45 step:35853 [D loss: 0.306488, acc.: 86.72%] [G loss: 2.807782]\n",
      "epoch:45 step:35854 [D loss: 0.276834, acc.: 85.94%] [G loss: 3.530759]\n",
      "epoch:45 step:35855 [D loss: 0.316033, acc.: 87.50%] [G loss: 3.213048]\n",
      "epoch:45 step:35856 [D loss: 0.193400, acc.: 92.19%] [G loss: 2.981457]\n",
      "epoch:45 step:35857 [D loss: 0.284577, acc.: 89.06%] [G loss: 3.480540]\n",
      "epoch:45 step:35858 [D loss: 0.315578, acc.: 85.94%] [G loss: 3.746673]\n",
      "epoch:45 step:35859 [D loss: 0.300613, acc.: 86.72%] [G loss: 3.064634]\n",
      "epoch:45 step:35860 [D loss: 0.312166, acc.: 86.72%] [G loss: 3.226419]\n",
      "epoch:45 step:35861 [D loss: 0.305109, acc.: 84.38%] [G loss: 3.175910]\n",
      "epoch:45 step:35862 [D loss: 0.390033, acc.: 83.59%] [G loss: 3.209607]\n",
      "epoch:45 step:35863 [D loss: 0.344541, acc.: 83.59%] [G loss: 3.113009]\n",
      "epoch:45 step:35864 [D loss: 0.288286, acc.: 88.28%] [G loss: 3.726153]\n",
      "epoch:45 step:35865 [D loss: 0.380588, acc.: 78.91%] [G loss: 3.024344]\n",
      "epoch:45 step:35866 [D loss: 0.307066, acc.: 88.28%] [G loss: 4.026680]\n",
      "epoch:45 step:35867 [D loss: 0.500967, acc.: 78.91%] [G loss: 3.493676]\n",
      "epoch:45 step:35868 [D loss: 0.402027, acc.: 82.81%] [G loss: 2.788366]\n",
      "epoch:45 step:35869 [D loss: 0.365818, acc.: 83.59%] [G loss: 3.002057]\n",
      "epoch:45 step:35870 [D loss: 0.312477, acc.: 86.72%] [G loss: 3.305321]\n",
      "epoch:45 step:35871 [D loss: 0.284696, acc.: 85.16%] [G loss: 4.127141]\n",
      "epoch:45 step:35872 [D loss: 0.266531, acc.: 91.41%] [G loss: 4.388731]\n",
      "epoch:45 step:35873 [D loss: 0.345569, acc.: 85.16%] [G loss: 2.698403]\n",
      "epoch:45 step:35874 [D loss: 0.330339, acc.: 83.59%] [G loss: 3.518351]\n",
      "epoch:45 step:35875 [D loss: 0.464471, acc.: 81.25%] [G loss: 2.986104]\n",
      "epoch:45 step:35876 [D loss: 0.278402, acc.: 89.84%] [G loss: 4.580567]\n",
      "epoch:45 step:35877 [D loss: 0.246576, acc.: 91.41%] [G loss: 3.899307]\n",
      "epoch:45 step:35878 [D loss: 0.233851, acc.: 90.62%] [G loss: 4.792646]\n",
      "epoch:45 step:35879 [D loss: 0.258411, acc.: 86.72%] [G loss: 4.736531]\n",
      "epoch:45 step:35880 [D loss: 0.339249, acc.: 85.94%] [G loss: 4.636430]\n",
      "epoch:45 step:35881 [D loss: 0.241200, acc.: 89.84%] [G loss: 4.326197]\n",
      "epoch:45 step:35882 [D loss: 0.288254, acc.: 87.50%] [G loss: 3.822201]\n",
      "epoch:45 step:35883 [D loss: 0.265977, acc.: 91.41%] [G loss: 2.570606]\n",
      "epoch:45 step:35884 [D loss: 0.304113, acc.: 88.28%] [G loss: 3.532071]\n",
      "epoch:45 step:35885 [D loss: 0.384393, acc.: 84.38%] [G loss: 3.552710]\n",
      "epoch:45 step:35886 [D loss: 0.284471, acc.: 83.59%] [G loss: 3.831861]\n",
      "epoch:45 step:35887 [D loss: 0.407731, acc.: 82.03%] [G loss: 2.494679]\n",
      "epoch:45 step:35888 [D loss: 0.325999, acc.: 84.38%] [G loss: 2.979443]\n",
      "epoch:45 step:35889 [D loss: 0.276360, acc.: 89.06%] [G loss: 3.141001]\n",
      "epoch:45 step:35890 [D loss: 0.293862, acc.: 85.94%] [G loss: 3.713571]\n",
      "epoch:45 step:35891 [D loss: 0.313349, acc.: 83.59%] [G loss: 3.159734]\n",
      "epoch:45 step:35892 [D loss: 0.255300, acc.: 89.06%] [G loss: 3.651071]\n",
      "epoch:45 step:35893 [D loss: 0.241325, acc.: 91.41%] [G loss: 3.366494]\n",
      "epoch:45 step:35894 [D loss: 0.344944, acc.: 85.16%] [G loss: 3.559980]\n",
      "epoch:45 step:35895 [D loss: 0.303257, acc.: 87.50%] [G loss: 2.489318]\n",
      "epoch:45 step:35896 [D loss: 0.336559, acc.: 86.72%] [G loss: 3.822432]\n",
      "epoch:45 step:35897 [D loss: 0.267802, acc.: 92.19%] [G loss: 3.178952]\n",
      "epoch:45 step:35898 [D loss: 0.281528, acc.: 90.62%] [G loss: 3.929621]\n",
      "epoch:45 step:35899 [D loss: 0.401033, acc.: 86.72%] [G loss: 4.225237]\n",
      "epoch:45 step:35900 [D loss: 0.345482, acc.: 85.16%] [G loss: 5.056821]\n",
      "epoch:45 step:35901 [D loss: 0.299739, acc.: 88.28%] [G loss: 3.545982]\n",
      "epoch:45 step:35902 [D loss: 0.235355, acc.: 90.62%] [G loss: 2.813803]\n",
      "epoch:45 step:35903 [D loss: 0.253038, acc.: 89.84%] [G loss: 3.669789]\n",
      "epoch:45 step:35904 [D loss: 0.290660, acc.: 87.50%] [G loss: 3.109337]\n",
      "epoch:45 step:35905 [D loss: 0.278495, acc.: 89.84%] [G loss: 4.177633]\n",
      "epoch:45 step:35906 [D loss: 0.172974, acc.: 93.75%] [G loss: 6.889880]\n",
      "epoch:45 step:35907 [D loss: 0.278079, acc.: 87.50%] [G loss: 4.378482]\n",
      "epoch:45 step:35908 [D loss: 0.372505, acc.: 82.03%] [G loss: 3.727403]\n",
      "epoch:45 step:35909 [D loss: 0.180380, acc.: 94.53%] [G loss: 3.994399]\n",
      "epoch:45 step:35910 [D loss: 0.298274, acc.: 84.38%] [G loss: 4.839517]\n",
      "epoch:45 step:35911 [D loss: 0.345217, acc.: 86.72%] [G loss: 4.467139]\n",
      "epoch:45 step:35912 [D loss: 0.208633, acc.: 90.62%] [G loss: 4.538033]\n",
      "epoch:45 step:35913 [D loss: 0.336599, acc.: 84.38%] [G loss: 4.055375]\n",
      "epoch:45 step:35914 [D loss: 0.283222, acc.: 86.72%] [G loss: 4.230155]\n",
      "epoch:45 step:35915 [D loss: 0.371201, acc.: 84.38%] [G loss: 2.846728]\n",
      "epoch:45 step:35916 [D loss: 0.332204, acc.: 82.81%] [G loss: 3.415828]\n",
      "epoch:45 step:35917 [D loss: 0.270874, acc.: 87.50%] [G loss: 3.132226]\n",
      "epoch:45 step:35918 [D loss: 0.239661, acc.: 89.06%] [G loss: 3.472162]\n",
      "epoch:45 step:35919 [D loss: 0.404090, acc.: 81.25%] [G loss: 6.188404]\n",
      "epoch:45 step:35920 [D loss: 0.643373, acc.: 80.47%] [G loss: 4.502990]\n",
      "epoch:45 step:35921 [D loss: 0.424987, acc.: 82.81%] [G loss: 4.741578]\n",
      "epoch:45 step:35922 [D loss: 0.486489, acc.: 77.34%] [G loss: 5.029881]\n",
      "epoch:45 step:35923 [D loss: 0.527339, acc.: 75.00%] [G loss: 3.563318]\n",
      "epoch:45 step:35924 [D loss: 0.305903, acc.: 85.16%] [G loss: 5.533118]\n",
      "epoch:45 step:35925 [D loss: 0.282092, acc.: 88.28%] [G loss: 3.082890]\n",
      "epoch:45 step:35926 [D loss: 0.364483, acc.: 82.81%] [G loss: 3.499619]\n",
      "epoch:46 step:35927 [D loss: 0.371828, acc.: 82.81%] [G loss: 3.744174]\n",
      "epoch:46 step:35928 [D loss: 0.199967, acc.: 89.84%] [G loss: 3.316696]\n",
      "epoch:46 step:35929 [D loss: 0.356888, acc.: 84.38%] [G loss: 3.396197]\n",
      "epoch:46 step:35930 [D loss: 0.320761, acc.: 85.94%] [G loss: 3.414446]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:46 step:35931 [D loss: 0.286253, acc.: 86.72%] [G loss: 3.422168]\n",
      "epoch:46 step:35932 [D loss: 0.262773, acc.: 90.62%] [G loss: 2.603395]\n",
      "epoch:46 step:35933 [D loss: 0.250853, acc.: 90.62%] [G loss: 2.382974]\n",
      "epoch:46 step:35934 [D loss: 0.329104, acc.: 84.38%] [G loss: 3.278938]\n",
      "epoch:46 step:35935 [D loss: 0.239339, acc.: 91.41%] [G loss: 3.056628]\n",
      "epoch:46 step:35936 [D loss: 0.401613, acc.: 82.03%] [G loss: 3.439334]\n",
      "epoch:46 step:35937 [D loss: 0.403446, acc.: 82.81%] [G loss: 4.496799]\n",
      "epoch:46 step:35938 [D loss: 0.306677, acc.: 86.72%] [G loss: 2.999938]\n",
      "epoch:46 step:35939 [D loss: 0.315825, acc.: 87.50%] [G loss: 4.571029]\n",
      "epoch:46 step:35940 [D loss: 0.233464, acc.: 87.50%] [G loss: 3.709813]\n",
      "epoch:46 step:35941 [D loss: 0.379551, acc.: 82.03%] [G loss: 3.440960]\n",
      "epoch:46 step:35942 [D loss: 0.259118, acc.: 89.84%] [G loss: 3.887917]\n",
      "epoch:46 step:35943 [D loss: 0.235810, acc.: 89.84%] [G loss: 3.258880]\n",
      "epoch:46 step:35944 [D loss: 0.288732, acc.: 87.50%] [G loss: 3.224229]\n",
      "epoch:46 step:35945 [D loss: 0.331604, acc.: 89.06%] [G loss: 3.514473]\n",
      "epoch:46 step:35946 [D loss: 0.285032, acc.: 85.16%] [G loss: 2.677653]\n",
      "epoch:46 step:35947 [D loss: 0.348993, acc.: 82.81%] [G loss: 2.729809]\n",
      "epoch:46 step:35948 [D loss: 0.372067, acc.: 85.16%] [G loss: 2.837751]\n",
      "epoch:46 step:35949 [D loss: 0.341021, acc.: 85.16%] [G loss: 3.095733]\n",
      "epoch:46 step:35950 [D loss: 0.360343, acc.: 85.94%] [G loss: 4.414379]\n",
      "epoch:46 step:35951 [D loss: 0.353155, acc.: 85.16%] [G loss: 3.148496]\n",
      "epoch:46 step:35952 [D loss: 0.322526, acc.: 85.16%] [G loss: 2.771901]\n",
      "epoch:46 step:35953 [D loss: 0.369524, acc.: 86.72%] [G loss: 4.197583]\n",
      "epoch:46 step:35954 [D loss: 0.366134, acc.: 82.81%] [G loss: 4.236621]\n",
      "epoch:46 step:35955 [D loss: 0.479239, acc.: 77.34%] [G loss: 3.362702]\n",
      "epoch:46 step:35956 [D loss: 0.285712, acc.: 87.50%] [G loss: 2.826600]\n",
      "epoch:46 step:35957 [D loss: 0.364082, acc.: 79.69%] [G loss: 2.882754]\n",
      "epoch:46 step:35958 [D loss: 0.327721, acc.: 82.81%] [G loss: 3.929048]\n",
      "epoch:46 step:35959 [D loss: 0.309195, acc.: 88.28%] [G loss: 3.407218]\n",
      "epoch:46 step:35960 [D loss: 0.319792, acc.: 85.94%] [G loss: 3.096565]\n",
      "epoch:46 step:35961 [D loss: 0.401334, acc.: 83.59%] [G loss: 4.136069]\n",
      "epoch:46 step:35962 [D loss: 0.317381, acc.: 88.28%] [G loss: 4.013057]\n",
      "epoch:46 step:35963 [D loss: 0.325157, acc.: 88.28%] [G loss: 3.862257]\n",
      "epoch:46 step:35964 [D loss: 0.384613, acc.: 80.47%] [G loss: 2.759402]\n",
      "epoch:46 step:35965 [D loss: 0.318321, acc.: 87.50%] [G loss: 3.185883]\n",
      "epoch:46 step:35966 [D loss: 0.329812, acc.: 83.59%] [G loss: 3.113988]\n",
      "epoch:46 step:35967 [D loss: 0.337364, acc.: 80.47%] [G loss: 5.318161]\n",
      "epoch:46 step:35968 [D loss: 0.525695, acc.: 75.00%] [G loss: 4.307315]\n",
      "epoch:46 step:35969 [D loss: 0.514965, acc.: 74.22%] [G loss: 3.270849]\n",
      "epoch:46 step:35970 [D loss: 0.587342, acc.: 75.78%] [G loss: 3.150292]\n",
      "epoch:46 step:35971 [D loss: 0.291976, acc.: 85.94%] [G loss: 3.121655]\n",
      "epoch:46 step:35972 [D loss: 0.258642, acc.: 91.41%] [G loss: 3.678189]\n",
      "epoch:46 step:35973 [D loss: 0.416864, acc.: 80.47%] [G loss: 4.348618]\n",
      "epoch:46 step:35974 [D loss: 0.311041, acc.: 86.72%] [G loss: 4.503339]\n",
      "epoch:46 step:35975 [D loss: 0.300372, acc.: 84.38%] [G loss: 3.423231]\n",
      "epoch:46 step:35976 [D loss: 0.457182, acc.: 81.25%] [G loss: 2.880499]\n",
      "epoch:46 step:35977 [D loss: 0.241660, acc.: 89.84%] [G loss: 3.186898]\n",
      "epoch:46 step:35978 [D loss: 0.277378, acc.: 86.72%] [G loss: 2.707367]\n",
      "epoch:46 step:35979 [D loss: 0.482220, acc.: 78.12%] [G loss: 3.076989]\n",
      "epoch:46 step:35980 [D loss: 0.395298, acc.: 82.03%] [G loss: 3.710399]\n",
      "epoch:46 step:35981 [D loss: 0.386902, acc.: 80.47%] [G loss: 3.667813]\n",
      "epoch:46 step:35982 [D loss: 0.395203, acc.: 77.34%] [G loss: 2.733666]\n",
      "epoch:46 step:35983 [D loss: 0.367486, acc.: 85.94%] [G loss: 3.892492]\n",
      "epoch:46 step:35984 [D loss: 0.279373, acc.: 83.59%] [G loss: 3.702673]\n",
      "epoch:46 step:35985 [D loss: 0.433820, acc.: 78.91%] [G loss: 3.995722]\n",
      "epoch:46 step:35986 [D loss: 0.285036, acc.: 85.94%] [G loss: 3.386151]\n",
      "epoch:46 step:35987 [D loss: 0.245610, acc.: 89.84%] [G loss: 3.867295]\n",
      "epoch:46 step:35988 [D loss: 0.257827, acc.: 90.62%] [G loss: 3.178908]\n",
      "epoch:46 step:35989 [D loss: 0.355101, acc.: 81.25%] [G loss: 3.187504]\n",
      "epoch:46 step:35990 [D loss: 0.221521, acc.: 91.41%] [G loss: 3.787748]\n",
      "epoch:46 step:35991 [D loss: 0.368361, acc.: 81.25%] [G loss: 3.781662]\n",
      "epoch:46 step:35992 [D loss: 0.415165, acc.: 80.47%] [G loss: 3.766629]\n",
      "epoch:46 step:35993 [D loss: 0.318537, acc.: 85.94%] [G loss: 3.267323]\n",
      "epoch:46 step:35994 [D loss: 0.301566, acc.: 85.94%] [G loss: 3.423992]\n",
      "epoch:46 step:35995 [D loss: 0.423533, acc.: 81.25%] [G loss: 3.116135]\n",
      "epoch:46 step:35996 [D loss: 0.274713, acc.: 88.28%] [G loss: 3.382143]\n",
      "epoch:46 step:35997 [D loss: 0.282775, acc.: 87.50%] [G loss: 2.633965]\n",
      "epoch:46 step:35998 [D loss: 0.316839, acc.: 85.94%] [G loss: 2.852019]\n",
      "epoch:46 step:35999 [D loss: 0.342214, acc.: 85.94%] [G loss: 3.332746]\n",
      "epoch:46 step:36000 [D loss: 0.385690, acc.: 83.59%] [G loss: 4.044699]\n",
      "##############\n",
      "[0.87842415 0.85559487 0.81906128 0.80724779 0.7638049  0.81800156\n",
      " 0.85713557 0.84734898 0.80661758 0.80229844]\n",
      "##########\n",
      "epoch:46 step:36001 [D loss: 0.474807, acc.: 76.56%] [G loss: 4.866050]\n",
      "epoch:46 step:36002 [D loss: 0.641106, acc.: 78.91%] [G loss: 7.444950]\n",
      "epoch:46 step:36003 [D loss: 1.306636, acc.: 62.50%] [G loss: 7.347626]\n",
      "epoch:46 step:36004 [D loss: 2.021999, acc.: 52.34%] [G loss: 4.446253]\n",
      "epoch:46 step:36005 [D loss: 0.788968, acc.: 73.44%] [G loss: 3.318104]\n",
      "epoch:46 step:36006 [D loss: 0.330427, acc.: 85.16%] [G loss: 4.115039]\n",
      "epoch:46 step:36007 [D loss: 0.364993, acc.: 87.50%] [G loss: 3.496513]\n",
      "epoch:46 step:36008 [D loss: 0.269064, acc.: 85.16%] [G loss: 3.417231]\n",
      "epoch:46 step:36009 [D loss: 0.396622, acc.: 83.59%] [G loss: 3.363425]\n",
      "epoch:46 step:36010 [D loss: 0.458945, acc.: 82.81%] [G loss: 3.278687]\n",
      "epoch:46 step:36011 [D loss: 0.347939, acc.: 83.59%] [G loss: 3.406322]\n",
      "epoch:46 step:36012 [D loss: 0.400736, acc.: 82.03%] [G loss: 2.710254]\n",
      "epoch:46 step:36013 [D loss: 0.296346, acc.: 85.16%] [G loss: 3.075754]\n",
      "epoch:46 step:36014 [D loss: 0.465421, acc.: 78.12%] [G loss: 3.497025]\n",
      "epoch:46 step:36015 [D loss: 0.288701, acc.: 86.72%] [G loss: 3.330984]\n",
      "epoch:46 step:36016 [D loss: 0.425853, acc.: 84.38%] [G loss: 3.325297]\n",
      "epoch:46 step:36017 [D loss: 0.227609, acc.: 89.84%] [G loss: 3.230854]\n",
      "epoch:46 step:36018 [D loss: 0.316895, acc.: 87.50%] [G loss: 3.175796]\n",
      "epoch:46 step:36019 [D loss: 0.368882, acc.: 84.38%] [G loss: 2.642074]\n",
      "epoch:46 step:36020 [D loss: 0.349325, acc.: 85.16%] [G loss: 3.287035]\n",
      "epoch:46 step:36021 [D loss: 0.426077, acc.: 82.03%] [G loss: 2.688588]\n",
      "epoch:46 step:36022 [D loss: 0.314418, acc.: 89.06%] [G loss: 2.893270]\n",
      "epoch:46 step:36023 [D loss: 0.306120, acc.: 87.50%] [G loss: 2.726917]\n",
      "epoch:46 step:36024 [D loss: 0.420948, acc.: 78.91%] [G loss: 2.219402]\n",
      "epoch:46 step:36025 [D loss: 0.304248, acc.: 86.72%] [G loss: 3.228434]\n",
      "epoch:46 step:36026 [D loss: 0.448613, acc.: 81.25%] [G loss: 2.634340]\n",
      "epoch:46 step:36027 [D loss: 0.332396, acc.: 85.16%] [G loss: 2.875003]\n",
      "epoch:46 step:36028 [D loss: 0.362251, acc.: 78.91%] [G loss: 2.424721]\n",
      "epoch:46 step:36029 [D loss: 0.346759, acc.: 82.81%] [G loss: 2.913947]\n",
      "epoch:46 step:36030 [D loss: 0.334599, acc.: 83.59%] [G loss: 2.774415]\n",
      "epoch:46 step:36031 [D loss: 0.294762, acc.: 87.50%] [G loss: 2.582967]\n",
      "epoch:46 step:36032 [D loss: 0.349504, acc.: 82.81%] [G loss: 2.578816]\n",
      "epoch:46 step:36033 [D loss: 0.301415, acc.: 86.72%] [G loss: 2.588442]\n",
      "epoch:46 step:36034 [D loss: 0.335049, acc.: 83.59%] [G loss: 3.060867]\n",
      "epoch:46 step:36035 [D loss: 0.393279, acc.: 78.91%] [G loss: 3.125410]\n",
      "epoch:46 step:36036 [D loss: 0.322213, acc.: 86.72%] [G loss: 2.723569]\n",
      "epoch:46 step:36037 [D loss: 0.408714, acc.: 80.47%] [G loss: 2.418709]\n",
      "epoch:46 step:36038 [D loss: 0.292679, acc.: 88.28%] [G loss: 2.741977]\n",
      "epoch:46 step:36039 [D loss: 0.383584, acc.: 82.81%] [G loss: 3.381346]\n",
      "epoch:46 step:36040 [D loss: 0.297957, acc.: 89.84%] [G loss: 3.989659]\n",
      "epoch:46 step:36041 [D loss: 0.325054, acc.: 86.72%] [G loss: 3.192159]\n",
      "epoch:46 step:36042 [D loss: 0.289947, acc.: 90.62%] [G loss: 2.600776]\n",
      "epoch:46 step:36043 [D loss: 0.302072, acc.: 88.28%] [G loss: 3.232132]\n",
      "epoch:46 step:36044 [D loss: 0.333586, acc.: 86.72%] [G loss: 3.300627]\n",
      "epoch:46 step:36045 [D loss: 0.306956, acc.: 85.94%] [G loss: 3.438910]\n",
      "epoch:46 step:36046 [D loss: 0.328818, acc.: 86.72%] [G loss: 3.343609]\n",
      "epoch:46 step:36047 [D loss: 0.242306, acc.: 87.50%] [G loss: 3.944858]\n",
      "epoch:46 step:36048 [D loss: 0.207501, acc.: 90.62%] [G loss: 3.418690]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:46 step:36049 [D loss: 0.359332, acc.: 89.84%] [G loss: 3.709912]\n",
      "epoch:46 step:36050 [D loss: 0.272151, acc.: 87.50%] [G loss: 4.157318]\n",
      "epoch:46 step:36051 [D loss: 0.403930, acc.: 81.25%] [G loss: 2.039107]\n",
      "epoch:46 step:36052 [D loss: 0.253222, acc.: 87.50%] [G loss: 2.850896]\n",
      "epoch:46 step:36053 [D loss: 0.310559, acc.: 85.16%] [G loss: 2.468261]\n",
      "epoch:46 step:36054 [D loss: 0.269244, acc.: 88.28%] [G loss: 3.646343]\n",
      "epoch:46 step:36055 [D loss: 0.372775, acc.: 83.59%] [G loss: 2.911840]\n",
      "epoch:46 step:36056 [D loss: 0.269663, acc.: 87.50%] [G loss: 2.791787]\n",
      "epoch:46 step:36057 [D loss: 0.326907, acc.: 85.16%] [G loss: 2.483857]\n",
      "epoch:46 step:36058 [D loss: 0.332579, acc.: 82.81%] [G loss: 2.503712]\n",
      "epoch:46 step:36059 [D loss: 0.278589, acc.: 89.06%] [G loss: 3.297393]\n",
      "epoch:46 step:36060 [D loss: 0.305504, acc.: 89.84%] [G loss: 2.765032]\n",
      "epoch:46 step:36061 [D loss: 0.259816, acc.: 89.84%] [G loss: 2.787507]\n",
      "epoch:46 step:36062 [D loss: 0.193154, acc.: 90.62%] [G loss: 2.867865]\n",
      "epoch:46 step:36063 [D loss: 0.326906, acc.: 85.94%] [G loss: 3.067372]\n",
      "epoch:46 step:36064 [D loss: 0.343944, acc.: 82.03%] [G loss: 2.216883]\n",
      "epoch:46 step:36065 [D loss: 0.228630, acc.: 90.62%] [G loss: 2.860741]\n",
      "epoch:46 step:36066 [D loss: 0.357230, acc.: 84.38%] [G loss: 3.119501]\n",
      "epoch:46 step:36067 [D loss: 0.319947, acc.: 86.72%] [G loss: 2.948883]\n",
      "epoch:46 step:36068 [D loss: 0.353702, acc.: 83.59%] [G loss: 2.499688]\n",
      "epoch:46 step:36069 [D loss: 0.216959, acc.: 93.75%] [G loss: 2.874216]\n",
      "epoch:46 step:36070 [D loss: 0.380225, acc.: 81.25%] [G loss: 2.453063]\n",
      "epoch:46 step:36071 [D loss: 0.274226, acc.: 88.28%] [G loss: 2.390198]\n",
      "epoch:46 step:36072 [D loss: 0.301539, acc.: 85.94%] [G loss: 3.019773]\n",
      "epoch:46 step:36073 [D loss: 0.405520, acc.: 82.81%] [G loss: 2.687420]\n",
      "epoch:46 step:36074 [D loss: 0.305955, acc.: 86.72%] [G loss: 2.941211]\n",
      "epoch:46 step:36075 [D loss: 0.312244, acc.: 83.59%] [G loss: 2.716695]\n",
      "epoch:46 step:36076 [D loss: 0.301169, acc.: 84.38%] [G loss: 3.408285]\n",
      "epoch:46 step:36077 [D loss: 0.292438, acc.: 86.72%] [G loss: 2.296438]\n",
      "epoch:46 step:36078 [D loss: 0.400980, acc.: 82.03%] [G loss: 3.052733]\n",
      "epoch:46 step:36079 [D loss: 0.269150, acc.: 87.50%] [G loss: 3.440987]\n",
      "epoch:46 step:36080 [D loss: 0.378994, acc.: 81.25%] [G loss: 2.636183]\n",
      "epoch:46 step:36081 [D loss: 0.301715, acc.: 84.38%] [G loss: 2.959029]\n",
      "epoch:46 step:36082 [D loss: 0.369725, acc.: 87.50%] [G loss: 2.151470]\n",
      "epoch:46 step:36083 [D loss: 0.415262, acc.: 79.69%] [G loss: 3.274246]\n",
      "epoch:46 step:36084 [D loss: 0.357970, acc.: 84.38%] [G loss: 2.706959]\n",
      "epoch:46 step:36085 [D loss: 0.306068, acc.: 89.84%] [G loss: 4.057874]\n",
      "epoch:46 step:36086 [D loss: 0.443157, acc.: 80.47%] [G loss: 5.785732]\n",
      "epoch:46 step:36087 [D loss: 0.309119, acc.: 85.94%] [G loss: 4.070591]\n",
      "epoch:46 step:36088 [D loss: 0.279929, acc.: 89.84%] [G loss: 3.917218]\n",
      "epoch:46 step:36089 [D loss: 0.265988, acc.: 84.38%] [G loss: 3.323534]\n",
      "epoch:46 step:36090 [D loss: 0.182633, acc.: 92.97%] [G loss: 2.872844]\n",
      "epoch:46 step:36091 [D loss: 0.339023, acc.: 86.72%] [G loss: 2.787660]\n",
      "epoch:46 step:36092 [D loss: 0.320244, acc.: 80.47%] [G loss: 3.841893]\n",
      "epoch:46 step:36093 [D loss: 0.221805, acc.: 91.41%] [G loss: 3.440270]\n",
      "epoch:46 step:36094 [D loss: 0.357164, acc.: 85.94%] [G loss: 3.223517]\n",
      "epoch:46 step:36095 [D loss: 0.370228, acc.: 80.47%] [G loss: 3.650073]\n",
      "epoch:46 step:36096 [D loss: 0.266773, acc.: 89.84%] [G loss: 3.067853]\n",
      "epoch:46 step:36097 [D loss: 0.342730, acc.: 84.38%] [G loss: 3.594385]\n",
      "epoch:46 step:36098 [D loss: 0.242715, acc.: 93.75%] [G loss: 3.667781]\n",
      "epoch:46 step:36099 [D loss: 0.277923, acc.: 86.72%] [G loss: 4.426223]\n",
      "epoch:46 step:36100 [D loss: 0.380220, acc.: 82.81%] [G loss: 3.650948]\n",
      "epoch:46 step:36101 [D loss: 0.312080, acc.: 83.59%] [G loss: 3.042874]\n",
      "epoch:46 step:36102 [D loss: 0.388632, acc.: 84.38%] [G loss: 2.464108]\n",
      "epoch:46 step:36103 [D loss: 0.379502, acc.: 82.81%] [G loss: 3.632245]\n",
      "epoch:46 step:36104 [D loss: 0.191337, acc.: 94.53%] [G loss: 2.546553]\n",
      "epoch:46 step:36105 [D loss: 0.360996, acc.: 83.59%] [G loss: 2.856786]\n",
      "epoch:46 step:36106 [D loss: 0.395449, acc.: 78.91%] [G loss: 2.976598]\n",
      "epoch:46 step:36107 [D loss: 0.301682, acc.: 86.72%] [G loss: 3.422231]\n",
      "epoch:46 step:36108 [D loss: 0.255710, acc.: 89.84%] [G loss: 2.726761]\n",
      "epoch:46 step:36109 [D loss: 0.337282, acc.: 87.50%] [G loss: 3.198773]\n",
      "epoch:46 step:36110 [D loss: 0.330886, acc.: 87.50%] [G loss: 3.449115]\n",
      "epoch:46 step:36111 [D loss: 0.261915, acc.: 85.94%] [G loss: 3.183832]\n",
      "epoch:46 step:36112 [D loss: 0.348894, acc.: 82.03%] [G loss: 2.872125]\n",
      "epoch:46 step:36113 [D loss: 0.354455, acc.: 87.50%] [G loss: 3.338980]\n",
      "epoch:46 step:36114 [D loss: 0.367390, acc.: 82.03%] [G loss: 2.197083]\n",
      "epoch:46 step:36115 [D loss: 0.231617, acc.: 89.06%] [G loss: 3.279693]\n",
      "epoch:46 step:36116 [D loss: 0.273407, acc.: 89.06%] [G loss: 3.929964]\n",
      "epoch:46 step:36117 [D loss: 0.386698, acc.: 79.69%] [G loss: 2.506447]\n",
      "epoch:46 step:36118 [D loss: 0.260322, acc.: 90.62%] [G loss: 4.125376]\n",
      "epoch:46 step:36119 [D loss: 0.360248, acc.: 87.50%] [G loss: 3.514458]\n",
      "epoch:46 step:36120 [D loss: 0.347057, acc.: 82.81%] [G loss: 3.832448]\n",
      "epoch:46 step:36121 [D loss: 0.363861, acc.: 82.81%] [G loss: 2.810161]\n",
      "epoch:46 step:36122 [D loss: 0.224775, acc.: 89.84%] [G loss: 3.636445]\n",
      "epoch:46 step:36123 [D loss: 0.356461, acc.: 83.59%] [G loss: 2.285228]\n",
      "epoch:46 step:36124 [D loss: 0.207029, acc.: 90.62%] [G loss: 3.953143]\n",
      "epoch:46 step:36125 [D loss: 0.333542, acc.: 85.16%] [G loss: 3.282058]\n",
      "epoch:46 step:36126 [D loss: 0.236240, acc.: 87.50%] [G loss: 3.424537]\n",
      "epoch:46 step:36127 [D loss: 0.255036, acc.: 88.28%] [G loss: 4.133882]\n",
      "epoch:46 step:36128 [D loss: 0.274223, acc.: 89.06%] [G loss: 4.583817]\n",
      "epoch:46 step:36129 [D loss: 0.242464, acc.: 88.28%] [G loss: 3.610305]\n",
      "epoch:46 step:36130 [D loss: 0.332264, acc.: 84.38%] [G loss: 2.917689]\n",
      "epoch:46 step:36131 [D loss: 0.231106, acc.: 92.19%] [G loss: 3.541857]\n",
      "epoch:46 step:36132 [D loss: 0.329533, acc.: 85.94%] [G loss: 3.356572]\n",
      "epoch:46 step:36133 [D loss: 0.293785, acc.: 87.50%] [G loss: 3.382757]\n",
      "epoch:46 step:36134 [D loss: 0.302865, acc.: 88.28%] [G loss: 3.625729]\n",
      "epoch:46 step:36135 [D loss: 0.378727, acc.: 82.81%] [G loss: 3.061742]\n",
      "epoch:46 step:36136 [D loss: 0.263095, acc.: 91.41%] [G loss: 3.628691]\n",
      "epoch:46 step:36137 [D loss: 0.421041, acc.: 81.25%] [G loss: 2.959297]\n",
      "epoch:46 step:36138 [D loss: 0.308013, acc.: 87.50%] [G loss: 3.189551]\n",
      "epoch:46 step:36139 [D loss: 0.403689, acc.: 84.38%] [G loss: 3.062459]\n",
      "epoch:46 step:36140 [D loss: 0.367790, acc.: 85.16%] [G loss: 3.938516]\n",
      "epoch:46 step:36141 [D loss: 0.347510, acc.: 85.16%] [G loss: 6.484760]\n",
      "epoch:46 step:36142 [D loss: 0.189378, acc.: 92.97%] [G loss: 5.047544]\n",
      "epoch:46 step:36143 [D loss: 0.328441, acc.: 85.94%] [G loss: 3.757050]\n",
      "epoch:46 step:36144 [D loss: 0.365675, acc.: 85.16%] [G loss: 3.616001]\n",
      "epoch:46 step:36145 [D loss: 0.317039, acc.: 83.59%] [G loss: 3.446783]\n",
      "epoch:46 step:36146 [D loss: 0.422112, acc.: 78.91%] [G loss: 3.225830]\n",
      "epoch:46 step:36147 [D loss: 0.203049, acc.: 94.53%] [G loss: 3.214233]\n",
      "epoch:46 step:36148 [D loss: 0.201884, acc.: 91.41%] [G loss: 3.416029]\n",
      "epoch:46 step:36149 [D loss: 0.177870, acc.: 95.31%] [G loss: 4.136493]\n",
      "epoch:46 step:36150 [D loss: 0.280211, acc.: 88.28%] [G loss: 4.219379]\n",
      "epoch:46 step:36151 [D loss: 0.394872, acc.: 83.59%] [G loss: 3.129867]\n",
      "epoch:46 step:36152 [D loss: 0.310565, acc.: 85.94%] [G loss: 3.541306]\n",
      "epoch:46 step:36153 [D loss: 0.211380, acc.: 89.06%] [G loss: 4.137403]\n",
      "epoch:46 step:36154 [D loss: 0.257275, acc.: 88.28%] [G loss: 3.940172]\n",
      "epoch:46 step:36155 [D loss: 0.353275, acc.: 83.59%] [G loss: 3.433167]\n",
      "epoch:46 step:36156 [D loss: 0.235204, acc.: 86.72%] [G loss: 2.877919]\n",
      "epoch:46 step:36157 [D loss: 0.310154, acc.: 87.50%] [G loss: 2.805832]\n",
      "epoch:46 step:36158 [D loss: 0.310463, acc.: 87.50%] [G loss: 3.309213]\n",
      "epoch:46 step:36159 [D loss: 0.402715, acc.: 84.38%] [G loss: 2.656634]\n",
      "epoch:46 step:36160 [D loss: 0.362556, acc.: 85.16%] [G loss: 3.707788]\n",
      "epoch:46 step:36161 [D loss: 0.320497, acc.: 82.81%] [G loss: 3.393754]\n",
      "epoch:46 step:36162 [D loss: 0.351146, acc.: 81.25%] [G loss: 3.124306]\n",
      "epoch:46 step:36163 [D loss: 0.250756, acc.: 89.06%] [G loss: 2.926488]\n",
      "epoch:46 step:36164 [D loss: 0.397879, acc.: 80.47%] [G loss: 3.131089]\n",
      "epoch:46 step:36165 [D loss: 0.239151, acc.: 91.41%] [G loss: 3.360323]\n",
      "epoch:46 step:36166 [D loss: 0.289891, acc.: 84.38%] [G loss: 2.981530]\n",
      "epoch:46 step:36167 [D loss: 0.352905, acc.: 83.59%] [G loss: 3.947150]\n",
      "epoch:46 step:36168 [D loss: 0.204969, acc.: 91.41%] [G loss: 7.753377]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:46 step:36169 [D loss: 0.228975, acc.: 89.84%] [G loss: 3.552946]\n",
      "epoch:46 step:36170 [D loss: 0.261681, acc.: 87.50%] [G loss: 5.031130]\n",
      "epoch:46 step:36171 [D loss: 0.231035, acc.: 90.62%] [G loss: 3.690940]\n",
      "epoch:46 step:36172 [D loss: 0.308632, acc.: 85.94%] [G loss: 3.330617]\n",
      "epoch:46 step:36173 [D loss: 0.276415, acc.: 85.16%] [G loss: 4.281906]\n",
      "epoch:46 step:36174 [D loss: 0.329535, acc.: 87.50%] [G loss: 4.309319]\n",
      "epoch:46 step:36175 [D loss: 0.332123, acc.: 84.38%] [G loss: 4.209685]\n",
      "epoch:46 step:36176 [D loss: 0.417047, acc.: 84.38%] [G loss: 5.210025]\n",
      "epoch:46 step:36177 [D loss: 0.259615, acc.: 86.72%] [G loss: 5.052401]\n",
      "epoch:46 step:36178 [D loss: 0.437194, acc.: 79.69%] [G loss: 4.000866]\n",
      "epoch:46 step:36179 [D loss: 0.263034, acc.: 88.28%] [G loss: 3.835102]\n",
      "epoch:46 step:36180 [D loss: 0.274128, acc.: 86.72%] [G loss: 3.436739]\n",
      "epoch:46 step:36181 [D loss: 0.323569, acc.: 85.94%] [G loss: 3.573234]\n",
      "epoch:46 step:36182 [D loss: 0.352028, acc.: 80.47%] [G loss: 2.813035]\n",
      "epoch:46 step:36183 [D loss: 0.317331, acc.: 86.72%] [G loss: 3.804079]\n",
      "epoch:46 step:36184 [D loss: 0.296604, acc.: 89.06%] [G loss: 3.106607]\n",
      "epoch:46 step:36185 [D loss: 0.332880, acc.: 82.81%] [G loss: 3.222981]\n",
      "epoch:46 step:36186 [D loss: 0.246159, acc.: 92.19%] [G loss: 2.639349]\n",
      "epoch:46 step:36187 [D loss: 0.342189, acc.: 82.03%] [G loss: 3.010691]\n",
      "epoch:46 step:36188 [D loss: 0.276498, acc.: 88.28%] [G loss: 3.202125]\n",
      "epoch:46 step:36189 [D loss: 0.186011, acc.: 92.97%] [G loss: 3.872430]\n",
      "epoch:46 step:36190 [D loss: 0.258620, acc.: 89.06%] [G loss: 3.630688]\n",
      "epoch:46 step:36191 [D loss: 0.314271, acc.: 85.16%] [G loss: 3.295822]\n",
      "epoch:46 step:36192 [D loss: 0.318783, acc.: 87.50%] [G loss: 3.081988]\n",
      "epoch:46 step:36193 [D loss: 0.244476, acc.: 91.41%] [G loss: 4.168124]\n",
      "epoch:46 step:36194 [D loss: 0.338625, acc.: 81.25%] [G loss: 3.266291]\n",
      "epoch:46 step:36195 [D loss: 0.280457, acc.: 86.72%] [G loss: 3.316238]\n",
      "epoch:46 step:36196 [D loss: 0.330115, acc.: 84.38%] [G loss: 2.955788]\n",
      "epoch:46 step:36197 [D loss: 0.285403, acc.: 85.94%] [G loss: 2.802140]\n",
      "epoch:46 step:36198 [D loss: 0.369246, acc.: 82.03%] [G loss: 3.091508]\n",
      "epoch:46 step:36199 [D loss: 0.260036, acc.: 89.06%] [G loss: 4.411670]\n",
      "epoch:46 step:36200 [D loss: 0.257584, acc.: 90.62%] [G loss: 3.940906]\n",
      "##############\n",
      "[0.84254221 0.86999052 0.80134311 0.80408591 0.77330631 0.82417759\n",
      " 0.84640832 0.84746104 0.81525225 0.83249335]\n",
      "##########\n",
      "epoch:46 step:36201 [D loss: 0.405925, acc.: 80.47%] [G loss: 4.319509]\n",
      "epoch:46 step:36202 [D loss: 0.302172, acc.: 85.94%] [G loss: 3.509637]\n",
      "epoch:46 step:36203 [D loss: 0.389705, acc.: 85.16%] [G loss: 4.854660]\n",
      "epoch:46 step:36204 [D loss: 0.393891, acc.: 82.03%] [G loss: 5.771417]\n",
      "epoch:46 step:36205 [D loss: 0.620023, acc.: 76.56%] [G loss: 4.274311]\n",
      "epoch:46 step:36206 [D loss: 0.514560, acc.: 82.03%] [G loss: 4.464447]\n",
      "epoch:46 step:36207 [D loss: 0.314798, acc.: 86.72%] [G loss: 3.440392]\n",
      "epoch:46 step:36208 [D loss: 0.308879, acc.: 84.38%] [G loss: 4.547672]\n",
      "epoch:46 step:36209 [D loss: 0.284556, acc.: 87.50%] [G loss: 4.500654]\n",
      "epoch:46 step:36210 [D loss: 0.313818, acc.: 86.72%] [G loss: 4.941362]\n",
      "epoch:46 step:36211 [D loss: 0.434406, acc.: 83.59%] [G loss: 4.639692]\n",
      "epoch:46 step:36212 [D loss: 0.287090, acc.: 86.72%] [G loss: 3.327312]\n",
      "epoch:46 step:36213 [D loss: 0.259233, acc.: 88.28%] [G loss: 9.387936]\n",
      "epoch:46 step:36214 [D loss: 0.305248, acc.: 84.38%] [G loss: 4.831363]\n",
      "epoch:46 step:36215 [D loss: 0.261095, acc.: 89.06%] [G loss: 4.273878]\n",
      "epoch:46 step:36216 [D loss: 0.261192, acc.: 92.19%] [G loss: 5.382051]\n",
      "epoch:46 step:36217 [D loss: 0.296298, acc.: 86.72%] [G loss: 4.578807]\n",
      "epoch:46 step:36218 [D loss: 0.208550, acc.: 91.41%] [G loss: 5.629589]\n",
      "epoch:46 step:36219 [D loss: 0.235719, acc.: 88.28%] [G loss: 5.837701]\n",
      "epoch:46 step:36220 [D loss: 0.215894, acc.: 92.97%] [G loss: 4.649068]\n",
      "epoch:46 step:36221 [D loss: 0.270895, acc.: 88.28%] [G loss: 4.618708]\n",
      "epoch:46 step:36222 [D loss: 0.311871, acc.: 86.72%] [G loss: 4.781536]\n",
      "epoch:46 step:36223 [D loss: 0.347925, acc.: 88.28%] [G loss: 2.806121]\n",
      "epoch:46 step:36224 [D loss: 0.226035, acc.: 92.19%] [G loss: 3.163041]\n",
      "epoch:46 step:36225 [D loss: 0.279604, acc.: 90.62%] [G loss: 3.013562]\n",
      "epoch:46 step:36226 [D loss: 0.344266, acc.: 85.16%] [G loss: 2.978247]\n",
      "epoch:46 step:36227 [D loss: 0.394050, acc.: 82.81%] [G loss: 2.819381]\n",
      "epoch:46 step:36228 [D loss: 0.369838, acc.: 81.25%] [G loss: 3.075996]\n",
      "epoch:46 step:36229 [D loss: 0.313782, acc.: 85.16%] [G loss: 2.872530]\n",
      "epoch:46 step:36230 [D loss: 0.206589, acc.: 92.97%] [G loss: 3.459843]\n",
      "epoch:46 step:36231 [D loss: 0.374467, acc.: 83.59%] [G loss: 2.800130]\n",
      "epoch:46 step:36232 [D loss: 0.300797, acc.: 85.94%] [G loss: 2.900772]\n",
      "epoch:46 step:36233 [D loss: 0.256141, acc.: 89.84%] [G loss: 2.983853]\n",
      "epoch:46 step:36234 [D loss: 0.274986, acc.: 86.72%] [G loss: 3.658654]\n",
      "epoch:46 step:36235 [D loss: 0.364053, acc.: 82.81%] [G loss: 2.800987]\n",
      "epoch:46 step:36236 [D loss: 0.277981, acc.: 85.94%] [G loss: 3.469795]\n",
      "epoch:46 step:36237 [D loss: 0.298570, acc.: 87.50%] [G loss: 3.125129]\n",
      "epoch:46 step:36238 [D loss: 0.349720, acc.: 82.81%] [G loss: 3.197286]\n",
      "epoch:46 step:36239 [D loss: 0.373450, acc.: 82.03%] [G loss: 3.717679]\n",
      "epoch:46 step:36240 [D loss: 0.327348, acc.: 86.72%] [G loss: 3.798998]\n",
      "epoch:46 step:36241 [D loss: 0.478620, acc.: 78.91%] [G loss: 3.692412]\n",
      "epoch:46 step:36242 [D loss: 0.448637, acc.: 79.69%] [G loss: 3.018500]\n",
      "epoch:46 step:36243 [D loss: 0.457992, acc.: 81.25%] [G loss: 3.179071]\n",
      "epoch:46 step:36244 [D loss: 0.412310, acc.: 84.38%] [G loss: 2.609287]\n",
      "epoch:46 step:36245 [D loss: 0.382847, acc.: 84.38%] [G loss: 3.061638]\n",
      "epoch:46 step:36246 [D loss: 0.386696, acc.: 79.69%] [G loss: 3.374107]\n",
      "epoch:46 step:36247 [D loss: 0.346694, acc.: 84.38%] [G loss: 2.891727]\n",
      "epoch:46 step:36248 [D loss: 0.287233, acc.: 85.16%] [G loss: 5.678933]\n",
      "epoch:46 step:36249 [D loss: 0.317328, acc.: 86.72%] [G loss: 3.193082]\n",
      "epoch:46 step:36250 [D loss: 0.269887, acc.: 88.28%] [G loss: 3.941477]\n",
      "epoch:46 step:36251 [D loss: 0.315187, acc.: 85.16%] [G loss: 2.883902]\n",
      "epoch:46 step:36252 [D loss: 0.343154, acc.: 85.94%] [G loss: 2.631381]\n",
      "epoch:46 step:36253 [D loss: 0.279346, acc.: 86.72%] [G loss: 3.196671]\n",
      "epoch:46 step:36254 [D loss: 0.257877, acc.: 88.28%] [G loss: 2.693874]\n",
      "epoch:46 step:36255 [D loss: 0.324969, acc.: 85.94%] [G loss: 3.248627]\n",
      "epoch:46 step:36256 [D loss: 0.292137, acc.: 89.06%] [G loss: 2.833910]\n",
      "epoch:46 step:36257 [D loss: 0.481189, acc.: 76.56%] [G loss: 3.476556]\n",
      "epoch:46 step:36258 [D loss: 0.396463, acc.: 81.25%] [G loss: 3.278065]\n",
      "epoch:46 step:36259 [D loss: 0.376895, acc.: 80.47%] [G loss: 3.153347]\n",
      "epoch:46 step:36260 [D loss: 0.324229, acc.: 86.72%] [G loss: 3.265446]\n",
      "epoch:46 step:36261 [D loss: 0.428987, acc.: 81.25%] [G loss: 2.848790]\n",
      "epoch:46 step:36262 [D loss: 0.284421, acc.: 86.72%] [G loss: 3.743991]\n",
      "epoch:46 step:36263 [D loss: 0.292895, acc.: 87.50%] [G loss: 2.725781]\n",
      "epoch:46 step:36264 [D loss: 0.239208, acc.: 89.06%] [G loss: 4.621993]\n",
      "epoch:46 step:36265 [D loss: 0.312360, acc.: 84.38%] [G loss: 3.213943]\n",
      "epoch:46 step:36266 [D loss: 0.326240, acc.: 82.03%] [G loss: 4.523202]\n",
      "epoch:46 step:36267 [D loss: 0.395034, acc.: 82.81%] [G loss: 2.811072]\n",
      "epoch:46 step:36268 [D loss: 0.288095, acc.: 88.28%] [G loss: 3.836836]\n",
      "epoch:46 step:36269 [D loss: 0.307217, acc.: 82.81%] [G loss: 3.444417]\n",
      "epoch:46 step:36270 [D loss: 0.284990, acc.: 89.84%] [G loss: 3.495516]\n",
      "epoch:46 step:36271 [D loss: 0.361135, acc.: 86.72%] [G loss: 3.690139]\n",
      "epoch:46 step:36272 [D loss: 0.363884, acc.: 85.16%] [G loss: 4.278625]\n",
      "epoch:46 step:36273 [D loss: 0.298065, acc.: 87.50%] [G loss: 3.407565]\n",
      "epoch:46 step:36274 [D loss: 0.270843, acc.: 88.28%] [G loss: 3.346507]\n",
      "epoch:46 step:36275 [D loss: 0.403892, acc.: 82.81%] [G loss: 4.425473]\n",
      "epoch:46 step:36276 [D loss: 0.406771, acc.: 78.91%] [G loss: 2.727793]\n",
      "epoch:46 step:36277 [D loss: 0.281128, acc.: 86.72%] [G loss: 3.513961]\n",
      "epoch:46 step:36278 [D loss: 0.266364, acc.: 85.94%] [G loss: 2.894947]\n",
      "epoch:46 step:36279 [D loss: 0.245702, acc.: 89.06%] [G loss: 2.893840]\n",
      "epoch:46 step:36280 [D loss: 0.205025, acc.: 91.41%] [G loss: 3.613235]\n",
      "epoch:46 step:36281 [D loss: 0.328956, acc.: 85.94%] [G loss: 3.377377]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:46 step:36282 [D loss: 0.255129, acc.: 91.41%] [G loss: 2.597540]\n",
      "epoch:46 step:36283 [D loss: 0.316148, acc.: 85.16%] [G loss: 2.976031]\n",
      "epoch:46 step:36284 [D loss: 0.318306, acc.: 86.72%] [G loss: 3.175618]\n",
      "epoch:46 step:36285 [D loss: 0.250889, acc.: 90.62%] [G loss: 2.548506]\n",
      "epoch:46 step:36286 [D loss: 0.380908, acc.: 82.03%] [G loss: 3.796892]\n",
      "epoch:46 step:36287 [D loss: 0.352963, acc.: 82.03%] [G loss: 3.338913]\n",
      "epoch:46 step:36288 [D loss: 0.274308, acc.: 87.50%] [G loss: 4.106609]\n",
      "epoch:46 step:36289 [D loss: 0.284798, acc.: 87.50%] [G loss: 3.507081]\n",
      "epoch:46 step:36290 [D loss: 0.267002, acc.: 88.28%] [G loss: 3.495219]\n",
      "epoch:46 step:36291 [D loss: 0.318229, acc.: 89.06%] [G loss: 3.610708]\n",
      "epoch:46 step:36292 [D loss: 0.302177, acc.: 84.38%] [G loss: 4.117848]\n",
      "epoch:46 step:36293 [D loss: 0.328278, acc.: 83.59%] [G loss: 2.926425]\n",
      "epoch:46 step:36294 [D loss: 0.305666, acc.: 85.16%] [G loss: 4.925201]\n",
      "epoch:46 step:36295 [D loss: 0.354987, acc.: 84.38%] [G loss: 3.886303]\n",
      "epoch:46 step:36296 [D loss: 0.224389, acc.: 88.28%] [G loss: 3.798903]\n",
      "epoch:46 step:36297 [D loss: 0.291172, acc.: 88.28%] [G loss: 3.716897]\n",
      "epoch:46 step:36298 [D loss: 0.267343, acc.: 88.28%] [G loss: 4.855675]\n",
      "epoch:46 step:36299 [D loss: 0.314130, acc.: 88.28%] [G loss: 4.035797]\n",
      "epoch:46 step:36300 [D loss: 0.320761, acc.: 83.59%] [G loss: 2.454759]\n",
      "epoch:46 step:36301 [D loss: 0.324083, acc.: 89.84%] [G loss: 2.682920]\n",
      "epoch:46 step:36302 [D loss: 0.305906, acc.: 85.94%] [G loss: 2.889730]\n",
      "epoch:46 step:36303 [D loss: 0.355625, acc.: 84.38%] [G loss: 2.464121]\n",
      "epoch:46 step:36304 [D loss: 0.225417, acc.: 88.28%] [G loss: 2.882223]\n",
      "epoch:46 step:36305 [D loss: 0.334941, acc.: 86.72%] [G loss: 2.976876]\n",
      "epoch:46 step:36306 [D loss: 0.334021, acc.: 82.81%] [G loss: 3.132122]\n",
      "epoch:46 step:36307 [D loss: 0.221464, acc.: 91.41%] [G loss: 3.784956]\n",
      "epoch:46 step:36308 [D loss: 0.438566, acc.: 78.12%] [G loss: 3.395539]\n",
      "epoch:46 step:36309 [D loss: 0.257294, acc.: 86.72%] [G loss: 4.013388]\n",
      "epoch:46 step:36310 [D loss: 0.254803, acc.: 89.84%] [G loss: 3.558555]\n",
      "epoch:46 step:36311 [D loss: 0.230124, acc.: 92.19%] [G loss: 4.275311]\n",
      "epoch:46 step:36312 [D loss: 0.295645, acc.: 88.28%] [G loss: 3.517109]\n",
      "epoch:46 step:36313 [D loss: 0.340748, acc.: 86.72%] [G loss: 4.187175]\n",
      "epoch:46 step:36314 [D loss: 0.326120, acc.: 86.72%] [G loss: 4.724310]\n",
      "epoch:46 step:36315 [D loss: 0.367665, acc.: 80.47%] [G loss: 3.994370]\n",
      "epoch:46 step:36316 [D loss: 0.223182, acc.: 89.84%] [G loss: 4.507958]\n",
      "epoch:46 step:36317 [D loss: 0.307191, acc.: 85.16%] [G loss: 4.845294]\n",
      "epoch:46 step:36318 [D loss: 0.213879, acc.: 91.41%] [G loss: 3.368137]\n",
      "epoch:46 step:36319 [D loss: 0.334241, acc.: 84.38%] [G loss: 3.762022]\n",
      "epoch:46 step:36320 [D loss: 0.246716, acc.: 89.84%] [G loss: 3.596057]\n",
      "epoch:46 step:36321 [D loss: 0.335109, acc.: 84.38%] [G loss: 3.999873]\n",
      "epoch:46 step:36322 [D loss: 0.206451, acc.: 92.97%] [G loss: 4.405494]\n",
      "epoch:46 step:36323 [D loss: 0.193617, acc.: 89.84%] [G loss: 3.359863]\n",
      "epoch:46 step:36324 [D loss: 0.384176, acc.: 84.38%] [G loss: 3.284921]\n",
      "epoch:46 step:36325 [D loss: 0.290152, acc.: 89.06%] [G loss: 3.563044]\n",
      "epoch:46 step:36326 [D loss: 0.215288, acc.: 91.41%] [G loss: 4.423069]\n",
      "epoch:46 step:36327 [D loss: 0.240308, acc.: 89.06%] [G loss: 2.825653]\n",
      "epoch:46 step:36328 [D loss: 0.260780, acc.: 86.72%] [G loss: 3.810115]\n",
      "epoch:46 step:36329 [D loss: 0.205805, acc.: 92.19%] [G loss: 3.926406]\n",
      "epoch:46 step:36330 [D loss: 0.297324, acc.: 88.28%] [G loss: 3.161618]\n",
      "epoch:46 step:36331 [D loss: 0.254466, acc.: 90.62%] [G loss: 3.296419]\n",
      "epoch:46 step:36332 [D loss: 0.341068, acc.: 82.81%] [G loss: 3.080855]\n",
      "epoch:46 step:36333 [D loss: 0.439572, acc.: 78.91%] [G loss: 2.565532]\n",
      "epoch:46 step:36334 [D loss: 0.307066, acc.: 85.16%] [G loss: 3.031869]\n",
      "epoch:46 step:36335 [D loss: 0.315931, acc.: 85.16%] [G loss: 3.218874]\n",
      "epoch:46 step:36336 [D loss: 0.331719, acc.: 86.72%] [G loss: 2.990837]\n",
      "epoch:46 step:36337 [D loss: 0.317320, acc.: 86.72%] [G loss: 3.894990]\n",
      "epoch:46 step:36338 [D loss: 0.279969, acc.: 85.16%] [G loss: 3.242899]\n",
      "epoch:46 step:36339 [D loss: 0.315819, acc.: 85.16%] [G loss: 2.858744]\n",
      "epoch:46 step:36340 [D loss: 0.252699, acc.: 89.06%] [G loss: 3.802840]\n",
      "epoch:46 step:36341 [D loss: 0.261068, acc.: 92.19%] [G loss: 3.469706]\n",
      "epoch:46 step:36342 [D loss: 0.299745, acc.: 86.72%] [G loss: 3.715494]\n",
      "epoch:46 step:36343 [D loss: 0.301873, acc.: 83.59%] [G loss: 3.676891]\n",
      "epoch:46 step:36344 [D loss: 0.360827, acc.: 85.16%] [G loss: 2.494784]\n",
      "epoch:46 step:36345 [D loss: 0.380349, acc.: 81.25%] [G loss: 3.597345]\n",
      "epoch:46 step:36346 [D loss: 0.294974, acc.: 84.38%] [G loss: 3.331678]\n",
      "epoch:46 step:36347 [D loss: 0.341731, acc.: 82.81%] [G loss: 3.099885]\n",
      "epoch:46 step:36348 [D loss: 0.299516, acc.: 86.72%] [G loss: 3.526940]\n",
      "epoch:46 step:36349 [D loss: 0.289263, acc.: 89.06%] [G loss: 3.602910]\n",
      "epoch:46 step:36350 [D loss: 0.269811, acc.: 85.94%] [G loss: 4.403522]\n",
      "epoch:46 step:36351 [D loss: 0.460154, acc.: 76.56%] [G loss: 3.306192]\n",
      "epoch:46 step:36352 [D loss: 0.275399, acc.: 89.06%] [G loss: 2.922586]\n",
      "epoch:46 step:36353 [D loss: 0.273210, acc.: 89.84%] [G loss: 3.101521]\n",
      "epoch:46 step:36354 [D loss: 0.332572, acc.: 81.25%] [G loss: 3.889630]\n",
      "epoch:46 step:36355 [D loss: 0.287443, acc.: 88.28%] [G loss: 3.403255]\n",
      "epoch:46 step:36356 [D loss: 0.307410, acc.: 85.94%] [G loss: 2.978784]\n",
      "epoch:46 step:36357 [D loss: 0.315483, acc.: 85.94%] [G loss: 4.418246]\n",
      "epoch:46 step:36358 [D loss: 0.494123, acc.: 79.69%] [G loss: 2.771743]\n",
      "epoch:46 step:36359 [D loss: 0.357575, acc.: 82.81%] [G loss: 3.231622]\n",
      "epoch:46 step:36360 [D loss: 0.200910, acc.: 93.75%] [G loss: 3.841481]\n",
      "epoch:46 step:36361 [D loss: 0.297602, acc.: 85.16%] [G loss: 3.242272]\n",
      "epoch:46 step:36362 [D loss: 0.453103, acc.: 82.03%] [G loss: 3.857131]\n",
      "epoch:46 step:36363 [D loss: 0.344970, acc.: 82.81%] [G loss: 2.593280]\n",
      "epoch:46 step:36364 [D loss: 0.348084, acc.: 81.25%] [G loss: 3.888959]\n",
      "epoch:46 step:36365 [D loss: 0.312371, acc.: 83.59%] [G loss: 2.963595]\n",
      "epoch:46 step:36366 [D loss: 0.268182, acc.: 86.72%] [G loss: 3.887864]\n",
      "epoch:46 step:36367 [D loss: 0.365669, acc.: 86.72%] [G loss: 3.427096]\n",
      "epoch:46 step:36368 [D loss: 0.228732, acc.: 91.41%] [G loss: 3.411675]\n",
      "epoch:46 step:36369 [D loss: 0.220225, acc.: 89.84%] [G loss: 5.338498]\n",
      "epoch:46 step:36370 [D loss: 0.326590, acc.: 84.38%] [G loss: 2.960146]\n",
      "epoch:46 step:36371 [D loss: 0.245940, acc.: 88.28%] [G loss: 4.087883]\n",
      "epoch:46 step:36372 [D loss: 0.285808, acc.: 88.28%] [G loss: 4.013780]\n",
      "epoch:46 step:36373 [D loss: 0.161164, acc.: 93.75%] [G loss: 3.330406]\n",
      "epoch:46 step:36374 [D loss: 0.301808, acc.: 87.50%] [G loss: 3.493117]\n",
      "epoch:46 step:36375 [D loss: 0.285496, acc.: 86.72%] [G loss: 3.039989]\n",
      "epoch:46 step:36376 [D loss: 0.374274, acc.: 82.81%] [G loss: 2.804949]\n",
      "epoch:46 step:36377 [D loss: 0.320262, acc.: 85.16%] [G loss: 3.156637]\n",
      "epoch:46 step:36378 [D loss: 0.298429, acc.: 83.59%] [G loss: 3.764690]\n",
      "epoch:46 step:36379 [D loss: 0.221248, acc.: 93.75%] [G loss: 4.592696]\n",
      "epoch:46 step:36380 [D loss: 0.355782, acc.: 85.94%] [G loss: 5.783844]\n",
      "epoch:46 step:36381 [D loss: 0.681485, acc.: 75.00%] [G loss: 7.701575]\n",
      "epoch:46 step:36382 [D loss: 2.266359, acc.: 60.94%] [G loss: 6.127838]\n",
      "epoch:46 step:36383 [D loss: 0.906568, acc.: 68.75%] [G loss: 4.391100]\n",
      "epoch:46 step:36384 [D loss: 0.135817, acc.: 94.53%] [G loss: 6.570891]\n",
      "epoch:46 step:36385 [D loss: 0.242179, acc.: 90.62%] [G loss: 6.560446]\n",
      "epoch:46 step:36386 [D loss: 0.334483, acc.: 86.72%] [G loss: 3.782269]\n",
      "epoch:46 step:36387 [D loss: 0.293289, acc.: 88.28%] [G loss: 4.576202]\n",
      "epoch:46 step:36388 [D loss: 0.362388, acc.: 85.94%] [G loss: 3.252644]\n",
      "epoch:46 step:36389 [D loss: 0.315334, acc.: 86.72%] [G loss: 3.200723]\n",
      "epoch:46 step:36390 [D loss: 0.320187, acc.: 87.50%] [G loss: 4.487930]\n",
      "epoch:46 step:36391 [D loss: 0.289720, acc.: 86.72%] [G loss: 4.311817]\n",
      "epoch:46 step:36392 [D loss: 0.409178, acc.: 82.81%] [G loss: 3.433622]\n",
      "epoch:46 step:36393 [D loss: 0.305387, acc.: 86.72%] [G loss: 3.467987]\n",
      "epoch:46 step:36394 [D loss: 0.279569, acc.: 86.72%] [G loss: 4.045582]\n",
      "epoch:46 step:36395 [D loss: 0.317297, acc.: 82.81%] [G loss: 3.084830]\n",
      "epoch:46 step:36396 [D loss: 0.361184, acc.: 88.28%] [G loss: 2.346941]\n",
      "epoch:46 step:36397 [D loss: 0.372164, acc.: 84.38%] [G loss: 2.157365]\n",
      "epoch:46 step:36398 [D loss: 0.354329, acc.: 81.25%] [G loss: 3.131893]\n",
      "epoch:46 step:36399 [D loss: 0.418340, acc.: 78.12%] [G loss: 2.358582]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:46 step:36400 [D loss: 0.279721, acc.: 88.28%] [G loss: 3.390271]\n",
      "##############\n",
      "[0.86770418 0.85413084 0.7959961  0.80552368 0.76986624 0.84499193\n",
      " 0.8702237  0.8361157  0.7978785  0.83804244]\n",
      "##########\n",
      "epoch:46 step:36401 [D loss: 0.286331, acc.: 85.16%] [G loss: 2.432618]\n",
      "epoch:46 step:36402 [D loss: 0.331709, acc.: 85.94%] [G loss: 2.852138]\n",
      "epoch:46 step:36403 [D loss: 0.248381, acc.: 89.06%] [G loss: 3.296316]\n",
      "epoch:46 step:36404 [D loss: 0.326162, acc.: 85.16%] [G loss: 3.370852]\n",
      "epoch:46 step:36405 [D loss: 0.296940, acc.: 86.72%] [G loss: 3.156057]\n",
      "epoch:46 step:36406 [D loss: 0.223352, acc.: 92.97%] [G loss: 2.770172]\n",
      "epoch:46 step:36407 [D loss: 0.293695, acc.: 88.28%] [G loss: 3.259931]\n",
      "epoch:46 step:36408 [D loss: 0.369768, acc.: 84.38%] [G loss: 2.582482]\n",
      "epoch:46 step:36409 [D loss: 0.289669, acc.: 89.84%] [G loss: 2.983897]\n",
      "epoch:46 step:36410 [D loss: 0.276822, acc.: 90.62%] [G loss: 2.788045]\n",
      "epoch:46 step:36411 [D loss: 0.295509, acc.: 85.94%] [G loss: 2.545931]\n",
      "epoch:46 step:36412 [D loss: 0.363415, acc.: 84.38%] [G loss: 3.376987]\n",
      "epoch:46 step:36413 [D loss: 0.404946, acc.: 82.81%] [G loss: 3.159548]\n",
      "epoch:46 step:36414 [D loss: 0.354620, acc.: 82.81%] [G loss: 2.920941]\n",
      "epoch:46 step:36415 [D loss: 0.341573, acc.: 84.38%] [G loss: 3.113052]\n",
      "epoch:46 step:36416 [D loss: 0.342915, acc.: 86.72%] [G loss: 3.146081]\n",
      "epoch:46 step:36417 [D loss: 0.357912, acc.: 81.25%] [G loss: 2.695135]\n",
      "epoch:46 step:36418 [D loss: 0.337354, acc.: 85.16%] [G loss: 3.432486]\n",
      "epoch:46 step:36419 [D loss: 0.304050, acc.: 85.94%] [G loss: 3.657921]\n",
      "epoch:46 step:36420 [D loss: 0.327476, acc.: 87.50%] [G loss: 2.598458]\n",
      "epoch:46 step:36421 [D loss: 0.371069, acc.: 83.59%] [G loss: 4.222459]\n",
      "epoch:46 step:36422 [D loss: 0.235136, acc.: 94.53%] [G loss: 3.829887]\n",
      "epoch:46 step:36423 [D loss: 0.215644, acc.: 93.75%] [G loss: 3.652445]\n",
      "epoch:46 step:36424 [D loss: 0.248432, acc.: 89.84%] [G loss: 3.578706]\n",
      "epoch:46 step:36425 [D loss: 0.391793, acc.: 81.25%] [G loss: 2.957893]\n",
      "epoch:46 step:36426 [D loss: 0.305176, acc.: 87.50%] [G loss: 3.877076]\n",
      "epoch:46 step:36427 [D loss: 0.313192, acc.: 85.94%] [G loss: 2.737115]\n",
      "epoch:46 step:36428 [D loss: 0.294776, acc.: 89.84%] [G loss: 2.988102]\n",
      "epoch:46 step:36429 [D loss: 0.327940, acc.: 84.38%] [G loss: 2.539022]\n",
      "epoch:46 step:36430 [D loss: 0.345990, acc.: 81.25%] [G loss: 2.862207]\n",
      "epoch:46 step:36431 [D loss: 0.299361, acc.: 89.06%] [G loss: 2.569429]\n",
      "epoch:46 step:36432 [D loss: 0.377830, acc.: 78.12%] [G loss: 2.956157]\n",
      "epoch:46 step:36433 [D loss: 0.315455, acc.: 85.94%] [G loss: 1.922552]\n",
      "epoch:46 step:36434 [D loss: 0.349873, acc.: 84.38%] [G loss: 2.763490]\n",
      "epoch:46 step:36435 [D loss: 0.345258, acc.: 82.03%] [G loss: 2.857392]\n",
      "epoch:46 step:36436 [D loss: 0.293741, acc.: 83.59%] [G loss: 2.694151]\n",
      "epoch:46 step:36437 [D loss: 0.348592, acc.: 82.81%] [G loss: 2.852605]\n",
      "epoch:46 step:36438 [D loss: 0.266767, acc.: 85.94%] [G loss: 3.053886]\n",
      "epoch:46 step:36439 [D loss: 0.312064, acc.: 84.38%] [G loss: 2.810366]\n",
      "epoch:46 step:36440 [D loss: 0.396225, acc.: 77.34%] [G loss: 2.848150]\n",
      "epoch:46 step:36441 [D loss: 0.289757, acc.: 86.72%] [G loss: 2.116673]\n",
      "epoch:46 step:36442 [D loss: 0.300173, acc.: 86.72%] [G loss: 2.292996]\n",
      "epoch:46 step:36443 [D loss: 0.310705, acc.: 87.50%] [G loss: 3.282487]\n",
      "epoch:46 step:36444 [D loss: 0.282608, acc.: 87.50%] [G loss: 3.129284]\n",
      "epoch:46 step:36445 [D loss: 0.303603, acc.: 84.38%] [G loss: 3.896370]\n",
      "epoch:46 step:36446 [D loss: 0.344488, acc.: 84.38%] [G loss: 2.929009]\n",
      "epoch:46 step:36447 [D loss: 0.206815, acc.: 92.19%] [G loss: 3.625349]\n",
      "epoch:46 step:36448 [D loss: 0.277092, acc.: 88.28%] [G loss: 3.420992]\n",
      "epoch:46 step:36449 [D loss: 0.316518, acc.: 85.16%] [G loss: 2.450896]\n",
      "epoch:46 step:36450 [D loss: 0.282918, acc.: 88.28%] [G loss: 3.326846]\n",
      "epoch:46 step:36451 [D loss: 0.308974, acc.: 86.72%] [G loss: 2.368267]\n",
      "epoch:46 step:36452 [D loss: 0.383430, acc.: 85.16%] [G loss: 3.130327]\n",
      "epoch:46 step:36453 [D loss: 0.350195, acc.: 79.69%] [G loss: 3.024853]\n",
      "epoch:46 step:36454 [D loss: 0.275576, acc.: 87.50%] [G loss: 2.781477]\n",
      "epoch:46 step:36455 [D loss: 0.252683, acc.: 89.06%] [G loss: 3.147808]\n",
      "epoch:46 step:36456 [D loss: 0.280586, acc.: 90.62%] [G loss: 2.611947]\n",
      "epoch:46 step:36457 [D loss: 0.252480, acc.: 88.28%] [G loss: 3.110820]\n",
      "epoch:46 step:36458 [D loss: 0.265273, acc.: 86.72%] [G loss: 3.604142]\n",
      "epoch:46 step:36459 [D loss: 0.279657, acc.: 89.06%] [G loss: 4.079495]\n",
      "epoch:46 step:36460 [D loss: 0.295337, acc.: 83.59%] [G loss: 4.709688]\n",
      "epoch:46 step:36461 [D loss: 0.248240, acc.: 89.06%] [G loss: 4.795660]\n",
      "epoch:46 step:36462 [D loss: 0.290608, acc.: 85.94%] [G loss: 4.083611]\n",
      "epoch:46 step:36463 [D loss: 0.323438, acc.: 86.72%] [G loss: 3.067174]\n",
      "epoch:46 step:36464 [D loss: 0.328027, acc.: 87.50%] [G loss: 3.097708]\n",
      "epoch:46 step:36465 [D loss: 0.292335, acc.: 85.94%] [G loss: 3.512786]\n",
      "epoch:46 step:36466 [D loss: 0.229493, acc.: 91.41%] [G loss: 3.536116]\n",
      "epoch:46 step:36467 [D loss: 0.390727, acc.: 82.03%] [G loss: 2.253991]\n",
      "epoch:46 step:36468 [D loss: 0.319079, acc.: 85.16%] [G loss: 3.065535]\n",
      "epoch:46 step:36469 [D loss: 0.284150, acc.: 84.38%] [G loss: 2.792395]\n",
      "epoch:46 step:36470 [D loss: 0.310835, acc.: 88.28%] [G loss: 2.817552]\n",
      "epoch:46 step:36471 [D loss: 0.301571, acc.: 86.72%] [G loss: 3.782991]\n",
      "epoch:46 step:36472 [D loss: 0.355509, acc.: 84.38%] [G loss: 3.775320]\n",
      "epoch:46 step:36473 [D loss: 0.277084, acc.: 86.72%] [G loss: 3.510380]\n",
      "epoch:46 step:36474 [D loss: 0.235082, acc.: 89.84%] [G loss: 3.237755]\n",
      "epoch:46 step:36475 [D loss: 0.320574, acc.: 83.59%] [G loss: 3.488707]\n",
      "epoch:46 step:36476 [D loss: 0.260283, acc.: 89.06%] [G loss: 2.970722]\n",
      "epoch:46 step:36477 [D loss: 0.267847, acc.: 91.41%] [G loss: 2.865395]\n",
      "epoch:46 step:36478 [D loss: 0.376251, acc.: 83.59%] [G loss: 3.831307]\n",
      "epoch:46 step:36479 [D loss: 0.281375, acc.: 85.94%] [G loss: 3.381820]\n",
      "epoch:46 step:36480 [D loss: 0.214260, acc.: 90.62%] [G loss: 2.939046]\n",
      "epoch:46 step:36481 [D loss: 0.277186, acc.: 88.28%] [G loss: 3.972989]\n",
      "epoch:46 step:36482 [D loss: 0.343567, acc.: 86.72%] [G loss: 3.217103]\n",
      "epoch:46 step:36483 [D loss: 0.230161, acc.: 92.19%] [G loss: 3.923662]\n",
      "epoch:46 step:36484 [D loss: 0.369408, acc.: 82.81%] [G loss: 3.143466]\n",
      "epoch:46 step:36485 [D loss: 0.355087, acc.: 87.50%] [G loss: 3.353934]\n",
      "epoch:46 step:36486 [D loss: 0.362985, acc.: 82.03%] [G loss: 3.974522]\n",
      "epoch:46 step:36487 [D loss: 0.243602, acc.: 90.62%] [G loss: 3.564254]\n",
      "epoch:46 step:36488 [D loss: 0.300924, acc.: 84.38%] [G loss: 3.474154]\n",
      "epoch:46 step:36489 [D loss: 0.238008, acc.: 88.28%] [G loss: 4.373423]\n",
      "epoch:46 step:36490 [D loss: 0.324859, acc.: 88.28%] [G loss: 3.378636]\n",
      "epoch:46 step:36491 [D loss: 0.278802, acc.: 88.28%] [G loss: 3.050592]\n",
      "epoch:46 step:36492 [D loss: 0.339709, acc.: 85.16%] [G loss: 3.332227]\n",
      "epoch:46 step:36493 [D loss: 0.264623, acc.: 85.16%] [G loss: 3.407422]\n",
      "epoch:46 step:36494 [D loss: 0.335749, acc.: 85.16%] [G loss: 2.720218]\n",
      "epoch:46 step:36495 [D loss: 0.181933, acc.: 94.53%] [G loss: 3.442306]\n",
      "epoch:46 step:36496 [D loss: 0.323722, acc.: 85.94%] [G loss: 2.435114]\n",
      "epoch:46 step:36497 [D loss: 0.309585, acc.: 89.06%] [G loss: 3.490191]\n",
      "epoch:46 step:36498 [D loss: 0.305235, acc.: 86.72%] [G loss: 3.338090]\n",
      "epoch:46 step:36499 [D loss: 0.303503, acc.: 85.16%] [G loss: 2.867319]\n",
      "epoch:46 step:36500 [D loss: 0.215927, acc.: 91.41%] [G loss: 2.719234]\n",
      "epoch:46 step:36501 [D loss: 0.306253, acc.: 85.16%] [G loss: 3.638859]\n",
      "epoch:46 step:36502 [D loss: 0.354493, acc.: 82.81%] [G loss: 3.225693]\n",
      "epoch:46 step:36503 [D loss: 0.335807, acc.: 81.25%] [G loss: 3.436922]\n",
      "epoch:46 step:36504 [D loss: 0.226902, acc.: 89.84%] [G loss: 2.858705]\n",
      "epoch:46 step:36505 [D loss: 0.306974, acc.: 89.06%] [G loss: 3.764915]\n",
      "epoch:46 step:36506 [D loss: 0.359346, acc.: 86.72%] [G loss: 3.489724]\n",
      "epoch:46 step:36507 [D loss: 0.253146, acc.: 86.72%] [G loss: 4.650675]\n",
      "epoch:46 step:36508 [D loss: 0.264585, acc.: 88.28%] [G loss: 3.037674]\n",
      "epoch:46 step:36509 [D loss: 0.313061, acc.: 86.72%] [G loss: 3.753942]\n",
      "epoch:46 step:36510 [D loss: 0.261409, acc.: 87.50%] [G loss: 3.492238]\n",
      "epoch:46 step:36511 [D loss: 0.216840, acc.: 90.62%] [G loss: 3.697841]\n",
      "epoch:46 step:36512 [D loss: 0.219735, acc.: 89.84%] [G loss: 4.727385]\n",
      "epoch:46 step:36513 [D loss: 0.240001, acc.: 89.84%] [G loss: 4.705441]\n",
      "epoch:46 step:36514 [D loss: 0.222542, acc.: 89.06%] [G loss: 5.907857]\n",
      "epoch:46 step:36515 [D loss: 0.230476, acc.: 89.84%] [G loss: 5.769814]\n",
      "epoch:46 step:36516 [D loss: 0.311867, acc.: 88.28%] [G loss: 3.350173]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:46 step:36517 [D loss: 0.280649, acc.: 86.72%] [G loss: 2.827116]\n",
      "epoch:46 step:36518 [D loss: 0.262764, acc.: 85.94%] [G loss: 3.996784]\n",
      "epoch:46 step:36519 [D loss: 0.305071, acc.: 90.62%] [G loss: 2.695780]\n",
      "epoch:46 step:36520 [D loss: 0.333215, acc.: 82.03%] [G loss: 2.588323]\n",
      "epoch:46 step:36521 [D loss: 0.340540, acc.: 84.38%] [G loss: 2.869690]\n",
      "epoch:46 step:36522 [D loss: 0.253297, acc.: 91.41%] [G loss: 3.072467]\n",
      "epoch:46 step:36523 [D loss: 0.360750, acc.: 79.69%] [G loss: 2.931090]\n",
      "epoch:46 step:36524 [D loss: 0.308434, acc.: 85.16%] [G loss: 4.400759]\n",
      "epoch:46 step:36525 [D loss: 0.360557, acc.: 84.38%] [G loss: 4.260991]\n",
      "epoch:46 step:36526 [D loss: 0.315176, acc.: 83.59%] [G loss: 5.352172]\n",
      "epoch:46 step:36527 [D loss: 0.263675, acc.: 88.28%] [G loss: 3.502399]\n",
      "epoch:46 step:36528 [D loss: 0.225901, acc.: 90.62%] [G loss: 4.343023]\n",
      "epoch:46 step:36529 [D loss: 0.246295, acc.: 89.06%] [G loss: 4.101855]\n",
      "epoch:46 step:36530 [D loss: 0.294080, acc.: 86.72%] [G loss: 3.865269]\n",
      "epoch:46 step:36531 [D loss: 0.381892, acc.: 81.25%] [G loss: 3.216392]\n",
      "epoch:46 step:36532 [D loss: 0.309840, acc.: 85.16%] [G loss: 3.585071]\n",
      "epoch:46 step:36533 [D loss: 0.249384, acc.: 89.06%] [G loss: 3.666922]\n",
      "epoch:46 step:36534 [D loss: 0.254338, acc.: 90.62%] [G loss: 3.556990]\n",
      "epoch:46 step:36535 [D loss: 0.283163, acc.: 89.06%] [G loss: 4.442973]\n",
      "epoch:46 step:36536 [D loss: 0.359008, acc.: 85.94%] [G loss: 4.012784]\n",
      "epoch:46 step:36537 [D loss: 0.333492, acc.: 83.59%] [G loss: 4.773139]\n",
      "epoch:46 step:36538 [D loss: 0.353266, acc.: 85.16%] [G loss: 4.366460]\n",
      "epoch:46 step:36539 [D loss: 0.321110, acc.: 86.72%] [G loss: 4.235756]\n",
      "epoch:46 step:36540 [D loss: 0.301797, acc.: 87.50%] [G loss: 3.887322]\n",
      "epoch:46 step:36541 [D loss: 0.274895, acc.: 89.06%] [G loss: 3.254799]\n",
      "epoch:46 step:36542 [D loss: 0.289154, acc.: 85.94%] [G loss: 4.092851]\n",
      "epoch:46 step:36543 [D loss: 0.301730, acc.: 84.38%] [G loss: 3.324446]\n",
      "epoch:46 step:36544 [D loss: 0.336364, acc.: 85.94%] [G loss: 3.543879]\n",
      "epoch:46 step:36545 [D loss: 0.330178, acc.: 85.94%] [G loss: 3.762443]\n",
      "epoch:46 step:36546 [D loss: 0.286401, acc.: 88.28%] [G loss: 2.733468]\n",
      "epoch:46 step:36547 [D loss: 0.256835, acc.: 89.84%] [G loss: 4.009549]\n",
      "epoch:46 step:36548 [D loss: 0.364308, acc.: 87.50%] [G loss: 3.075921]\n",
      "epoch:46 step:36549 [D loss: 0.265102, acc.: 86.72%] [G loss: 3.684026]\n",
      "epoch:46 step:36550 [D loss: 0.379262, acc.: 82.03%] [G loss: 3.435584]\n",
      "epoch:46 step:36551 [D loss: 0.343147, acc.: 85.16%] [G loss: 3.198948]\n",
      "epoch:46 step:36552 [D loss: 0.373770, acc.: 85.16%] [G loss: 4.468649]\n",
      "epoch:46 step:36553 [D loss: 0.265026, acc.: 86.72%] [G loss: 3.834039]\n",
      "epoch:46 step:36554 [D loss: 0.342869, acc.: 83.59%] [G loss: 6.791630]\n",
      "epoch:46 step:36555 [D loss: 0.540652, acc.: 75.78%] [G loss: 4.458825]\n",
      "epoch:46 step:36556 [D loss: 0.377759, acc.: 83.59%] [G loss: 4.197540]\n",
      "epoch:46 step:36557 [D loss: 0.394324, acc.: 82.03%] [G loss: 3.602771]\n",
      "epoch:46 step:36558 [D loss: 0.278845, acc.: 87.50%] [G loss: 3.491361]\n",
      "epoch:46 step:36559 [D loss: 0.321943, acc.: 85.16%] [G loss: 3.670022]\n",
      "epoch:46 step:36560 [D loss: 0.380860, acc.: 80.47%] [G loss: 2.502578]\n",
      "epoch:46 step:36561 [D loss: 0.382255, acc.: 85.16%] [G loss: 2.782090]\n",
      "epoch:46 step:36562 [D loss: 0.359271, acc.: 83.59%] [G loss: 2.972831]\n",
      "epoch:46 step:36563 [D loss: 0.309758, acc.: 88.28%] [G loss: 3.443455]\n",
      "epoch:46 step:36564 [D loss: 0.389528, acc.: 86.72%] [G loss: 3.523429]\n",
      "epoch:46 step:36565 [D loss: 0.297415, acc.: 86.72%] [G loss: 3.167849]\n",
      "epoch:46 step:36566 [D loss: 0.382331, acc.: 79.69%] [G loss: 2.822028]\n",
      "epoch:46 step:36567 [D loss: 0.222905, acc.: 89.84%] [G loss: 3.292415]\n",
      "epoch:46 step:36568 [D loss: 0.325522, acc.: 82.81%] [G loss: 3.531953]\n",
      "epoch:46 step:36569 [D loss: 0.318767, acc.: 89.06%] [G loss: 2.250976]\n",
      "epoch:46 step:36570 [D loss: 0.345850, acc.: 83.59%] [G loss: 2.829858]\n",
      "epoch:46 step:36571 [D loss: 0.258868, acc.: 89.84%] [G loss: 4.897779]\n",
      "epoch:46 step:36572 [D loss: 0.346907, acc.: 85.94%] [G loss: 3.191984]\n",
      "epoch:46 step:36573 [D loss: 0.346626, acc.: 86.72%] [G loss: 3.923045]\n",
      "epoch:46 step:36574 [D loss: 0.294122, acc.: 85.16%] [G loss: 5.150341]\n",
      "epoch:46 step:36575 [D loss: 0.329392, acc.: 85.94%] [G loss: 3.081524]\n",
      "epoch:46 step:36576 [D loss: 0.238705, acc.: 88.28%] [G loss: 5.144029]\n",
      "epoch:46 step:36577 [D loss: 0.342700, acc.: 85.94%] [G loss: 3.914311]\n",
      "epoch:46 step:36578 [D loss: 0.331160, acc.: 85.94%] [G loss: 3.511177]\n",
      "epoch:46 step:36579 [D loss: 0.188045, acc.: 91.41%] [G loss: 3.383590]\n",
      "epoch:46 step:36580 [D loss: 0.337457, acc.: 89.06%] [G loss: 4.941122]\n",
      "epoch:46 step:36581 [D loss: 0.268872, acc.: 85.94%] [G loss: 4.087105]\n",
      "epoch:46 step:36582 [D loss: 0.312759, acc.: 86.72%] [G loss: 4.609712]\n",
      "epoch:46 step:36583 [D loss: 0.414267, acc.: 82.03%] [G loss: 3.248543]\n",
      "epoch:46 step:36584 [D loss: 0.284452, acc.: 85.94%] [G loss: 3.375336]\n",
      "epoch:46 step:36585 [D loss: 0.285246, acc.: 87.50%] [G loss: 2.584601]\n",
      "epoch:46 step:36586 [D loss: 0.276012, acc.: 85.16%] [G loss: 3.379138]\n",
      "epoch:46 step:36587 [D loss: 0.309373, acc.: 87.50%] [G loss: 2.752766]\n",
      "epoch:46 step:36588 [D loss: 0.374454, acc.: 82.81%] [G loss: 3.199847]\n",
      "epoch:46 step:36589 [D loss: 0.316577, acc.: 83.59%] [G loss: 3.590654]\n",
      "epoch:46 step:36590 [D loss: 0.345539, acc.: 86.72%] [G loss: 3.258505]\n",
      "epoch:46 step:36591 [D loss: 0.315095, acc.: 82.03%] [G loss: 3.679865]\n",
      "epoch:46 step:36592 [D loss: 0.309412, acc.: 83.59%] [G loss: 5.301371]\n",
      "epoch:46 step:36593 [D loss: 0.507055, acc.: 75.00%] [G loss: 2.763418]\n",
      "epoch:46 step:36594 [D loss: 0.209857, acc.: 92.19%] [G loss: 4.152789]\n",
      "epoch:46 step:36595 [D loss: 0.295499, acc.: 85.94%] [G loss: 3.198767]\n",
      "epoch:46 step:36596 [D loss: 0.313823, acc.: 84.38%] [G loss: 3.152271]\n",
      "epoch:46 step:36597 [D loss: 0.334345, acc.: 83.59%] [G loss: 3.776249]\n",
      "epoch:46 step:36598 [D loss: 0.339580, acc.: 84.38%] [G loss: 2.802480]\n",
      "epoch:46 step:36599 [D loss: 0.284794, acc.: 87.50%] [G loss: 3.262770]\n",
      "epoch:46 step:36600 [D loss: 0.423158, acc.: 82.81%] [G loss: 3.782224]\n",
      "##############\n",
      "[0.84692701 0.85907029 0.80160772 0.79522843 0.78166608 0.84763697\n",
      " 0.87119526 0.84252021 0.82368783 0.81262749]\n",
      "##########\n",
      "epoch:46 step:36601 [D loss: 0.420298, acc.: 83.59%] [G loss: 5.714234]\n",
      "epoch:46 step:36602 [D loss: 0.483261, acc.: 78.91%] [G loss: 5.868135]\n",
      "epoch:46 step:36603 [D loss: 0.507646, acc.: 75.78%] [G loss: 3.803584]\n",
      "epoch:46 step:36604 [D loss: 0.349945, acc.: 83.59%] [G loss: 3.786980]\n",
      "epoch:46 step:36605 [D loss: 0.299847, acc.: 87.50%] [G loss: 3.723140]\n",
      "epoch:46 step:36606 [D loss: 0.358115, acc.: 84.38%] [G loss: 3.289325]\n",
      "epoch:46 step:36607 [D loss: 0.299811, acc.: 86.72%] [G loss: 2.897620]\n",
      "epoch:46 step:36608 [D loss: 0.335757, acc.: 83.59%] [G loss: 2.789886]\n",
      "epoch:46 step:36609 [D loss: 0.360070, acc.: 82.81%] [G loss: 4.044131]\n",
      "epoch:46 step:36610 [D loss: 0.164086, acc.: 92.19%] [G loss: 3.821294]\n",
      "epoch:46 step:36611 [D loss: 0.290152, acc.: 87.50%] [G loss: 4.832917]\n",
      "epoch:46 step:36612 [D loss: 0.315858, acc.: 83.59%] [G loss: 4.449098]\n",
      "epoch:46 step:36613 [D loss: 0.287679, acc.: 88.28%] [G loss: 4.338955]\n",
      "epoch:46 step:36614 [D loss: 0.267350, acc.: 87.50%] [G loss: 3.910601]\n",
      "epoch:46 step:36615 [D loss: 0.200577, acc.: 92.19%] [G loss: 3.486869]\n",
      "epoch:46 step:36616 [D loss: 0.270184, acc.: 89.84%] [G loss: 3.404757]\n",
      "epoch:46 step:36617 [D loss: 0.230388, acc.: 90.62%] [G loss: 3.646353]\n",
      "epoch:46 step:36618 [D loss: 0.393910, acc.: 79.69%] [G loss: 3.631561]\n",
      "epoch:46 step:36619 [D loss: 0.334277, acc.: 84.38%] [G loss: 2.594967]\n",
      "epoch:46 step:36620 [D loss: 0.396169, acc.: 80.47%] [G loss: 3.469486]\n",
      "epoch:46 step:36621 [D loss: 0.305021, acc.: 85.94%] [G loss: 3.022056]\n",
      "epoch:46 step:36622 [D loss: 0.285496, acc.: 86.72%] [G loss: 2.880172]\n",
      "epoch:46 step:36623 [D loss: 0.292701, acc.: 89.06%] [G loss: 2.757476]\n",
      "epoch:46 step:36624 [D loss: 0.339729, acc.: 86.72%] [G loss: 3.041801]\n",
      "epoch:46 step:36625 [D loss: 0.309723, acc.: 85.16%] [G loss: 3.451848]\n",
      "epoch:46 step:36626 [D loss: 0.316742, acc.: 87.50%] [G loss: 2.725718]\n",
      "epoch:46 step:36627 [D loss: 0.301253, acc.: 84.38%] [G loss: 3.282013]\n",
      "epoch:46 step:36628 [D loss: 0.315930, acc.: 84.38%] [G loss: 2.935753]\n",
      "epoch:46 step:36629 [D loss: 0.295748, acc.: 85.94%] [G loss: 2.860868]\n",
      "epoch:46 step:36630 [D loss: 0.296124, acc.: 86.72%] [G loss: 3.203331]\n",
      "epoch:46 step:36631 [D loss: 0.325099, acc.: 81.25%] [G loss: 3.908191]\n",
      "epoch:46 step:36632 [D loss: 0.363544, acc.: 85.94%] [G loss: 3.916237]\n",
      "epoch:46 step:36633 [D loss: 0.423114, acc.: 79.69%] [G loss: 3.134332]\n",
      "epoch:46 step:36634 [D loss: 0.236637, acc.: 91.41%] [G loss: 4.128060]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:46 step:36635 [D loss: 0.469802, acc.: 80.47%] [G loss: 3.618358]\n",
      "epoch:46 step:36636 [D loss: 0.330209, acc.: 86.72%] [G loss: 3.540120]\n",
      "epoch:46 step:36637 [D loss: 0.339368, acc.: 80.47%] [G loss: 3.253146]\n",
      "epoch:46 step:36638 [D loss: 0.333847, acc.: 82.81%] [G loss: 2.780658]\n",
      "epoch:46 step:36639 [D loss: 0.405094, acc.: 82.03%] [G loss: 2.784305]\n",
      "epoch:46 step:36640 [D loss: 0.373923, acc.: 87.50%] [G loss: 3.241724]\n",
      "epoch:46 step:36641 [D loss: 0.384538, acc.: 85.16%] [G loss: 3.419270]\n",
      "epoch:46 step:36642 [D loss: 0.282935, acc.: 86.72%] [G loss: 2.838958]\n",
      "epoch:46 step:36643 [D loss: 0.359229, acc.: 85.94%] [G loss: 2.524885]\n",
      "epoch:46 step:36644 [D loss: 0.343583, acc.: 81.25%] [G loss: 2.812890]\n",
      "epoch:46 step:36645 [D loss: 0.296067, acc.: 85.16%] [G loss: 3.335382]\n",
      "epoch:46 step:36646 [D loss: 0.287002, acc.: 85.94%] [G loss: 3.452336]\n",
      "epoch:46 step:36647 [D loss: 0.217169, acc.: 90.62%] [G loss: 3.201941]\n",
      "epoch:46 step:36648 [D loss: 0.254959, acc.: 89.06%] [G loss: 3.263043]\n",
      "epoch:46 step:36649 [D loss: 0.264074, acc.: 88.28%] [G loss: 3.283878]\n",
      "epoch:46 step:36650 [D loss: 0.252640, acc.: 89.06%] [G loss: 4.063025]\n",
      "epoch:46 step:36651 [D loss: 0.322137, acc.: 84.38%] [G loss: 3.980424]\n",
      "epoch:46 step:36652 [D loss: 0.340311, acc.: 84.38%] [G loss: 4.568826]\n",
      "epoch:46 step:36653 [D loss: 0.340753, acc.: 85.94%] [G loss: 3.395679]\n",
      "epoch:46 step:36654 [D loss: 0.304246, acc.: 83.59%] [G loss: 3.036721]\n",
      "epoch:46 step:36655 [D loss: 0.185832, acc.: 92.97%] [G loss: 3.217319]\n",
      "epoch:46 step:36656 [D loss: 0.294529, acc.: 86.72%] [G loss: 4.344460]\n",
      "epoch:46 step:36657 [D loss: 0.281820, acc.: 85.94%] [G loss: 4.021173]\n",
      "epoch:46 step:36658 [D loss: 0.232052, acc.: 91.41%] [G loss: 3.950513]\n",
      "epoch:46 step:36659 [D loss: 0.292526, acc.: 85.94%] [G loss: 2.726524]\n",
      "epoch:46 step:36660 [D loss: 0.310430, acc.: 85.16%] [G loss: 3.350024]\n",
      "epoch:46 step:36661 [D loss: 0.278001, acc.: 91.41%] [G loss: 3.114309]\n",
      "epoch:46 step:36662 [D loss: 0.292643, acc.: 85.94%] [G loss: 3.710789]\n",
      "epoch:46 step:36663 [D loss: 0.237871, acc.: 91.41%] [G loss: 3.673167]\n",
      "epoch:46 step:36664 [D loss: 0.296566, acc.: 85.16%] [G loss: 4.216806]\n",
      "epoch:46 step:36665 [D loss: 0.257667, acc.: 85.94%] [G loss: 3.677050]\n",
      "epoch:46 step:36666 [D loss: 0.301973, acc.: 88.28%] [G loss: 3.273855]\n",
      "epoch:46 step:36667 [D loss: 0.371854, acc.: 86.72%] [G loss: 3.450056]\n",
      "epoch:46 step:36668 [D loss: 0.265254, acc.: 89.06%] [G loss: 3.253593]\n",
      "epoch:46 step:36669 [D loss: 0.262698, acc.: 89.06%] [G loss: 2.759959]\n",
      "epoch:46 step:36670 [D loss: 0.260462, acc.: 86.72%] [G loss: 3.139397]\n",
      "epoch:46 step:36671 [D loss: 0.270147, acc.: 83.59%] [G loss: 3.565759]\n",
      "epoch:46 step:36672 [D loss: 0.319299, acc.: 84.38%] [G loss: 3.174246]\n",
      "epoch:46 step:36673 [D loss: 0.209572, acc.: 92.19%] [G loss: 2.947411]\n",
      "epoch:46 step:36674 [D loss: 0.342722, acc.: 85.16%] [G loss: 3.906563]\n",
      "epoch:46 step:36675 [D loss: 0.218251, acc.: 92.19%] [G loss: 3.260078]\n",
      "epoch:46 step:36676 [D loss: 0.291023, acc.: 88.28%] [G loss: 4.090309]\n",
      "epoch:46 step:36677 [D loss: 0.272151, acc.: 87.50%] [G loss: 5.302615]\n",
      "epoch:46 step:36678 [D loss: 0.394661, acc.: 81.25%] [G loss: 3.932965]\n",
      "epoch:46 step:36679 [D loss: 0.291712, acc.: 86.72%] [G loss: 3.127685]\n",
      "epoch:46 step:36680 [D loss: 0.286969, acc.: 86.72%] [G loss: 3.668562]\n",
      "epoch:46 step:36681 [D loss: 0.253728, acc.: 88.28%] [G loss: 2.961015]\n",
      "epoch:46 step:36682 [D loss: 0.214139, acc.: 92.19%] [G loss: 3.792587]\n",
      "epoch:46 step:36683 [D loss: 0.256074, acc.: 89.84%] [G loss: 2.837379]\n",
      "epoch:46 step:36684 [D loss: 0.346898, acc.: 82.81%] [G loss: 3.017633]\n",
      "epoch:46 step:36685 [D loss: 0.340092, acc.: 85.16%] [G loss: 2.897399]\n",
      "epoch:46 step:36686 [D loss: 0.357085, acc.: 85.16%] [G loss: 2.766160]\n",
      "epoch:46 step:36687 [D loss: 0.239495, acc.: 90.62%] [G loss: 2.933294]\n",
      "epoch:46 step:36688 [D loss: 0.358005, acc.: 84.38%] [G loss: 3.279941]\n",
      "epoch:46 step:36689 [D loss: 0.266803, acc.: 89.84%] [G loss: 2.706303]\n",
      "epoch:46 step:36690 [D loss: 0.300917, acc.: 87.50%] [G loss: 3.316814]\n",
      "epoch:46 step:36691 [D loss: 0.369803, acc.: 85.16%] [G loss: 3.388477]\n",
      "epoch:46 step:36692 [D loss: 0.298829, acc.: 84.38%] [G loss: 3.280836]\n",
      "epoch:46 step:36693 [D loss: 0.223305, acc.: 89.84%] [G loss: 4.888151]\n",
      "epoch:46 step:36694 [D loss: 0.258068, acc.: 90.62%] [G loss: 3.386937]\n",
      "epoch:46 step:36695 [D loss: 0.256318, acc.: 88.28%] [G loss: 4.961260]\n",
      "epoch:46 step:36696 [D loss: 0.225182, acc.: 90.62%] [G loss: 5.311535]\n",
      "epoch:46 step:36697 [D loss: 0.352093, acc.: 79.69%] [G loss: 4.022344]\n",
      "epoch:46 step:36698 [D loss: 0.297436, acc.: 86.72%] [G loss: 3.587143]\n",
      "epoch:46 step:36699 [D loss: 0.285264, acc.: 89.06%] [G loss: 4.094356]\n",
      "epoch:46 step:36700 [D loss: 0.321605, acc.: 85.16%] [G loss: 6.627341]\n",
      "epoch:46 step:36701 [D loss: 0.614263, acc.: 75.00%] [G loss: 6.944495]\n",
      "epoch:46 step:36702 [D loss: 1.605001, acc.: 61.72%] [G loss: 7.079664]\n",
      "epoch:46 step:36703 [D loss: 1.686054, acc.: 58.59%] [G loss: 5.113546]\n",
      "epoch:46 step:36704 [D loss: 1.267020, acc.: 67.19%] [G loss: 5.036531]\n",
      "epoch:46 step:36705 [D loss: 0.360364, acc.: 81.25%] [G loss: 3.598035]\n",
      "epoch:46 step:36706 [D loss: 0.377841, acc.: 84.38%] [G loss: 3.870953]\n",
      "epoch:46 step:36707 [D loss: 0.398984, acc.: 81.25%] [G loss: 3.931642]\n",
      "epoch:47 step:36708 [D loss: 0.313252, acc.: 85.16%] [G loss: 4.568042]\n",
      "epoch:47 step:36709 [D loss: 0.324146, acc.: 88.28%] [G loss: 3.536919]\n",
      "epoch:47 step:36710 [D loss: 0.410291, acc.: 86.72%] [G loss: 3.878033]\n",
      "epoch:47 step:36711 [D loss: 0.298054, acc.: 86.72%] [G loss: 3.867589]\n",
      "epoch:47 step:36712 [D loss: 0.309315, acc.: 87.50%] [G loss: 3.842513]\n",
      "epoch:47 step:36713 [D loss: 0.281760, acc.: 86.72%] [G loss: 4.637788]\n",
      "epoch:47 step:36714 [D loss: 0.253763, acc.: 89.06%] [G loss: 2.915677]\n",
      "epoch:47 step:36715 [D loss: 0.360747, acc.: 82.03%] [G loss: 3.044198]\n",
      "epoch:47 step:36716 [D loss: 0.338305, acc.: 83.59%] [G loss: 3.050046]\n",
      "epoch:47 step:36717 [D loss: 0.233680, acc.: 89.06%] [G loss: 2.742556]\n",
      "epoch:47 step:36718 [D loss: 0.269006, acc.: 92.19%] [G loss: 2.579765]\n",
      "epoch:47 step:36719 [D loss: 0.275374, acc.: 87.50%] [G loss: 3.065110]\n",
      "epoch:47 step:36720 [D loss: 0.288431, acc.: 87.50%] [G loss: 2.943426]\n",
      "epoch:47 step:36721 [D loss: 0.363304, acc.: 82.03%] [G loss: 2.633647]\n",
      "epoch:47 step:36722 [D loss: 0.353907, acc.: 83.59%] [G loss: 2.912564]\n",
      "epoch:47 step:36723 [D loss: 0.283268, acc.: 86.72%] [G loss: 2.737360]\n",
      "epoch:47 step:36724 [D loss: 0.391873, acc.: 80.47%] [G loss: 2.619702]\n",
      "epoch:47 step:36725 [D loss: 0.338174, acc.: 86.72%] [G loss: 3.174516]\n",
      "epoch:47 step:36726 [D loss: 0.292683, acc.: 85.16%] [G loss: 3.555650]\n",
      "epoch:47 step:36727 [D loss: 0.384906, acc.: 85.16%] [G loss: 3.593169]\n",
      "epoch:47 step:36728 [D loss: 0.287157, acc.: 88.28%] [G loss: 7.538752]\n",
      "epoch:47 step:36729 [D loss: 0.277931, acc.: 85.94%] [G loss: 3.088965]\n",
      "epoch:47 step:36730 [D loss: 0.209944, acc.: 91.41%] [G loss: 3.924685]\n",
      "epoch:47 step:36731 [D loss: 0.234999, acc.: 91.41%] [G loss: 2.987082]\n",
      "epoch:47 step:36732 [D loss: 0.215408, acc.: 92.19%] [G loss: 3.648069]\n",
      "epoch:47 step:36733 [D loss: 0.284903, acc.: 88.28%] [G loss: 3.128648]\n",
      "epoch:47 step:36734 [D loss: 0.270431, acc.: 89.06%] [G loss: 3.534564]\n",
      "epoch:47 step:36735 [D loss: 0.327517, acc.: 85.94%] [G loss: 3.411363]\n",
      "epoch:47 step:36736 [D loss: 0.278142, acc.: 89.06%] [G loss: 3.094507]\n",
      "epoch:47 step:36737 [D loss: 0.307452, acc.: 85.94%] [G loss: 2.552448]\n",
      "epoch:47 step:36738 [D loss: 0.261094, acc.: 89.06%] [G loss: 4.220270]\n",
      "epoch:47 step:36739 [D loss: 0.409796, acc.: 82.03%] [G loss: 2.693202]\n",
      "epoch:47 step:36740 [D loss: 0.248917, acc.: 89.84%] [G loss: 4.045574]\n",
      "epoch:47 step:36741 [D loss: 0.319390, acc.: 85.94%] [G loss: 3.831247]\n",
      "epoch:47 step:36742 [D loss: 0.367872, acc.: 82.81%] [G loss: 3.399445]\n",
      "epoch:47 step:36743 [D loss: 0.346018, acc.: 82.03%] [G loss: 3.287595]\n",
      "epoch:47 step:36744 [D loss: 0.326888, acc.: 85.94%] [G loss: 4.663120]\n",
      "epoch:47 step:36745 [D loss: 0.258861, acc.: 91.41%] [G loss: 3.898061]\n",
      "epoch:47 step:36746 [D loss: 0.279992, acc.: 88.28%] [G loss: 4.176894]\n",
      "epoch:47 step:36747 [D loss: 0.304258, acc.: 86.72%] [G loss: 2.705274]\n",
      "epoch:47 step:36748 [D loss: 0.317709, acc.: 87.50%] [G loss: 2.933832]\n",
      "epoch:47 step:36749 [D loss: 0.302399, acc.: 85.94%] [G loss: 3.678307]\n",
      "epoch:47 step:36750 [D loss: 0.389705, acc.: 83.59%] [G loss: 3.092165]\n",
      "epoch:47 step:36751 [D loss: 0.388964, acc.: 80.47%] [G loss: 3.605499]\n",
      "epoch:47 step:36752 [D loss: 0.428903, acc.: 84.38%] [G loss: 3.318280]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:47 step:36753 [D loss: 0.352581, acc.: 84.38%] [G loss: 3.196913]\n",
      "epoch:47 step:36754 [D loss: 0.222298, acc.: 90.62%] [G loss: 2.679893]\n",
      "epoch:47 step:36755 [D loss: 0.335998, acc.: 88.28%] [G loss: 3.197102]\n",
      "epoch:47 step:36756 [D loss: 0.284615, acc.: 89.06%] [G loss: 3.735217]\n",
      "epoch:47 step:36757 [D loss: 0.308256, acc.: 85.94%] [G loss: 3.088443]\n",
      "epoch:47 step:36758 [D loss: 0.398763, acc.: 83.59%] [G loss: 3.444105]\n",
      "epoch:47 step:36759 [D loss: 0.276299, acc.: 85.94%] [G loss: 3.637810]\n",
      "epoch:47 step:36760 [D loss: 0.387808, acc.: 80.47%] [G loss: 3.200884]\n",
      "epoch:47 step:36761 [D loss: 0.291656, acc.: 87.50%] [G loss: 4.100883]\n",
      "epoch:47 step:36762 [D loss: 0.228855, acc.: 92.19%] [G loss: 3.726770]\n",
      "epoch:47 step:36763 [D loss: 0.285382, acc.: 85.94%] [G loss: 3.511757]\n",
      "epoch:47 step:36764 [D loss: 0.356117, acc.: 83.59%] [G loss: 2.600720]\n",
      "epoch:47 step:36765 [D loss: 0.266588, acc.: 89.06%] [G loss: 3.872820]\n",
      "epoch:47 step:36766 [D loss: 0.222043, acc.: 92.19%] [G loss: 3.045671]\n",
      "epoch:47 step:36767 [D loss: 0.323556, acc.: 83.59%] [G loss: 4.019154]\n",
      "epoch:47 step:36768 [D loss: 0.358227, acc.: 83.59%] [G loss: 4.328025]\n",
      "epoch:47 step:36769 [D loss: 0.290164, acc.: 89.06%] [G loss: 2.888870]\n",
      "epoch:47 step:36770 [D loss: 0.197091, acc.: 94.53%] [G loss: 3.407379]\n",
      "epoch:47 step:36771 [D loss: 0.303806, acc.: 85.94%] [G loss: 2.802214]\n",
      "epoch:47 step:36772 [D loss: 0.305640, acc.: 83.59%] [G loss: 3.505461]\n",
      "epoch:47 step:36773 [D loss: 0.317497, acc.: 85.94%] [G loss: 3.858736]\n",
      "epoch:47 step:36774 [D loss: 0.293022, acc.: 90.62%] [G loss: 3.047760]\n",
      "epoch:47 step:36775 [D loss: 0.412896, acc.: 80.47%] [G loss: 2.809041]\n",
      "epoch:47 step:36776 [D loss: 0.316379, acc.: 82.03%] [G loss: 3.259479]\n",
      "epoch:47 step:36777 [D loss: 0.325681, acc.: 85.94%] [G loss: 4.096310]\n",
      "epoch:47 step:36778 [D loss: 0.293836, acc.: 88.28%] [G loss: 6.395123]\n",
      "epoch:47 step:36779 [D loss: 0.400203, acc.: 82.81%] [G loss: 4.764763]\n",
      "epoch:47 step:36780 [D loss: 0.361866, acc.: 85.16%] [G loss: 4.731420]\n",
      "epoch:47 step:36781 [D loss: 0.396878, acc.: 82.81%] [G loss: 3.622366]\n",
      "epoch:47 step:36782 [D loss: 0.279257, acc.: 85.16%] [G loss: 4.428524]\n",
      "epoch:47 step:36783 [D loss: 0.290425, acc.: 85.94%] [G loss: 4.666018]\n",
      "epoch:47 step:36784 [D loss: 0.208614, acc.: 89.06%] [G loss: 4.331145]\n",
      "epoch:47 step:36785 [D loss: 0.413572, acc.: 82.03%] [G loss: 4.783070]\n",
      "epoch:47 step:36786 [D loss: 0.313434, acc.: 87.50%] [G loss: 2.656899]\n",
      "epoch:47 step:36787 [D loss: 0.231752, acc.: 88.28%] [G loss: 3.326868]\n",
      "epoch:47 step:36788 [D loss: 0.321427, acc.: 86.72%] [G loss: 2.414584]\n",
      "epoch:47 step:36789 [D loss: 0.216149, acc.: 94.53%] [G loss: 2.679394]\n",
      "epoch:47 step:36790 [D loss: 0.276740, acc.: 91.41%] [G loss: 3.014895]\n",
      "epoch:47 step:36791 [D loss: 0.361767, acc.: 81.25%] [G loss: 2.964731]\n",
      "epoch:47 step:36792 [D loss: 0.300791, acc.: 87.50%] [G loss: 2.444822]\n",
      "epoch:47 step:36793 [D loss: 0.261343, acc.: 90.62%] [G loss: 2.807058]\n",
      "epoch:47 step:36794 [D loss: 0.317880, acc.: 86.72%] [G loss: 2.757080]\n",
      "epoch:47 step:36795 [D loss: 0.332557, acc.: 82.81%] [G loss: 2.987025]\n",
      "epoch:47 step:36796 [D loss: 0.399035, acc.: 82.81%] [G loss: 3.831849]\n",
      "epoch:47 step:36797 [D loss: 0.383819, acc.: 82.81%] [G loss: 2.844704]\n",
      "epoch:47 step:36798 [D loss: 0.315992, acc.: 85.16%] [G loss: 3.006299]\n",
      "epoch:47 step:36799 [D loss: 0.365977, acc.: 82.81%] [G loss: 3.872730]\n",
      "epoch:47 step:36800 [D loss: 0.340648, acc.: 85.94%] [G loss: 3.390765]\n",
      "##############\n",
      "[0.88922893 0.85203595 0.79396732 0.84887677 0.78856137 0.82965774\n",
      " 0.84926731 0.84892572 0.81837926 0.80350031]\n",
      "##########\n",
      "epoch:47 step:36801 [D loss: 0.252391, acc.: 89.84%] [G loss: 3.431863]\n",
      "epoch:47 step:36802 [D loss: 0.344303, acc.: 85.16%] [G loss: 4.205920]\n",
      "epoch:47 step:36803 [D loss: 0.271769, acc.: 87.50%] [G loss: 3.006443]\n",
      "epoch:47 step:36804 [D loss: 0.271223, acc.: 89.84%] [G loss: 4.523199]\n",
      "epoch:47 step:36805 [D loss: 0.462850, acc.: 78.12%] [G loss: 4.208141]\n",
      "epoch:47 step:36806 [D loss: 0.273537, acc.: 88.28%] [G loss: 3.906048]\n",
      "epoch:47 step:36807 [D loss: 0.365307, acc.: 84.38%] [G loss: 3.736472]\n",
      "epoch:47 step:36808 [D loss: 0.314503, acc.: 85.16%] [G loss: 3.385395]\n",
      "epoch:47 step:36809 [D loss: 0.318300, acc.: 85.94%] [G loss: 3.581038]\n",
      "epoch:47 step:36810 [D loss: 0.410594, acc.: 85.94%] [G loss: 2.437466]\n",
      "epoch:47 step:36811 [D loss: 0.248498, acc.: 89.06%] [G loss: 3.197582]\n",
      "epoch:47 step:36812 [D loss: 0.310471, acc.: 86.72%] [G loss: 2.711797]\n",
      "epoch:47 step:36813 [D loss: 0.265148, acc.: 88.28%] [G loss: 2.518743]\n",
      "epoch:47 step:36814 [D loss: 0.269292, acc.: 88.28%] [G loss: 2.423713]\n",
      "epoch:47 step:36815 [D loss: 0.332985, acc.: 86.72%] [G loss: 3.039178]\n",
      "epoch:47 step:36816 [D loss: 0.421675, acc.: 78.91%] [G loss: 2.433927]\n",
      "epoch:47 step:36817 [D loss: 0.401398, acc.: 80.47%] [G loss: 2.982613]\n",
      "epoch:47 step:36818 [D loss: 0.334787, acc.: 82.81%] [G loss: 3.452958]\n",
      "epoch:47 step:36819 [D loss: 0.409486, acc.: 79.69%] [G loss: 2.983786]\n",
      "epoch:47 step:36820 [D loss: 0.423824, acc.: 79.69%] [G loss: 2.974299]\n",
      "epoch:47 step:36821 [D loss: 0.366854, acc.: 84.38%] [G loss: 3.140935]\n",
      "epoch:47 step:36822 [D loss: 0.259928, acc.: 87.50%] [G loss: 3.152376]\n",
      "epoch:47 step:36823 [D loss: 0.332531, acc.: 84.38%] [G loss: 2.353614]\n",
      "epoch:47 step:36824 [D loss: 0.350748, acc.: 82.81%] [G loss: 2.512594]\n",
      "epoch:47 step:36825 [D loss: 0.339233, acc.: 84.38%] [G loss: 3.247777]\n",
      "epoch:47 step:36826 [D loss: 0.324214, acc.: 86.72%] [G loss: 2.899969]\n",
      "epoch:47 step:36827 [D loss: 0.391041, acc.: 85.94%] [G loss: 3.068258]\n",
      "epoch:47 step:36828 [D loss: 0.347826, acc.: 84.38%] [G loss: 3.645617]\n",
      "epoch:47 step:36829 [D loss: 0.404902, acc.: 81.25%] [G loss: 2.712045]\n",
      "epoch:47 step:36830 [D loss: 0.350643, acc.: 85.94%] [G loss: 3.329205]\n",
      "epoch:47 step:36831 [D loss: 0.384820, acc.: 84.38%] [G loss: 2.866786]\n",
      "epoch:47 step:36832 [D loss: 0.389231, acc.: 82.81%] [G loss: 3.045755]\n",
      "epoch:47 step:36833 [D loss: 0.376364, acc.: 80.47%] [G loss: 2.752144]\n",
      "epoch:47 step:36834 [D loss: 0.340467, acc.: 85.16%] [G loss: 2.225289]\n",
      "epoch:47 step:36835 [D loss: 0.271309, acc.: 84.38%] [G loss: 2.603460]\n",
      "epoch:47 step:36836 [D loss: 0.392715, acc.: 81.25%] [G loss: 2.830484]\n",
      "epoch:47 step:36837 [D loss: 0.389256, acc.: 82.03%] [G loss: 2.333644]\n",
      "epoch:47 step:36838 [D loss: 0.351969, acc.: 84.38%] [G loss: 2.859554]\n",
      "epoch:47 step:36839 [D loss: 0.335797, acc.: 85.94%] [G loss: 2.447275]\n",
      "epoch:47 step:36840 [D loss: 0.339028, acc.: 85.16%] [G loss: 3.489752]\n",
      "epoch:47 step:36841 [D loss: 0.315581, acc.: 86.72%] [G loss: 4.013833]\n",
      "epoch:47 step:36842 [D loss: 0.390592, acc.: 84.38%] [G loss: 3.341307]\n",
      "epoch:47 step:36843 [D loss: 0.347652, acc.: 80.47%] [G loss: 2.986754]\n",
      "epoch:47 step:36844 [D loss: 0.326479, acc.: 86.72%] [G loss: 4.538295]\n",
      "epoch:47 step:36845 [D loss: 0.319079, acc.: 87.50%] [G loss: 3.383670]\n",
      "epoch:47 step:36846 [D loss: 0.222863, acc.: 90.62%] [G loss: 4.074306]\n",
      "epoch:47 step:36847 [D loss: 0.256778, acc.: 89.84%] [G loss: 3.748565]\n",
      "epoch:47 step:36848 [D loss: 0.242339, acc.: 89.06%] [G loss: 3.142658]\n",
      "epoch:47 step:36849 [D loss: 0.337355, acc.: 83.59%] [G loss: 3.497130]\n",
      "epoch:47 step:36850 [D loss: 0.204183, acc.: 94.53%] [G loss: 4.709636]\n",
      "epoch:47 step:36851 [D loss: 0.415780, acc.: 81.25%] [G loss: 2.711514]\n",
      "epoch:47 step:36852 [D loss: 0.245419, acc.: 89.84%] [G loss: 3.468132]\n",
      "epoch:47 step:36853 [D loss: 0.366980, acc.: 80.47%] [G loss: 3.544154]\n",
      "epoch:47 step:36854 [D loss: 0.382176, acc.: 78.91%] [G loss: 3.711583]\n",
      "epoch:47 step:36855 [D loss: 0.418809, acc.: 78.91%] [G loss: 3.471525]\n",
      "epoch:47 step:36856 [D loss: 0.332865, acc.: 85.16%] [G loss: 2.773311]\n",
      "epoch:47 step:36857 [D loss: 0.264123, acc.: 89.06%] [G loss: 2.933496]\n",
      "epoch:47 step:36858 [D loss: 0.323579, acc.: 85.94%] [G loss: 2.808549]\n",
      "epoch:47 step:36859 [D loss: 0.274025, acc.: 90.62%] [G loss: 2.673541]\n",
      "epoch:47 step:36860 [D loss: 0.323034, acc.: 86.72%] [G loss: 3.140169]\n",
      "epoch:47 step:36861 [D loss: 0.237454, acc.: 88.28%] [G loss: 3.018576]\n",
      "epoch:47 step:36862 [D loss: 0.260181, acc.: 88.28%] [G loss: 3.411076]\n",
      "epoch:47 step:36863 [D loss: 0.250270, acc.: 87.50%] [G loss: 3.265375]\n",
      "epoch:47 step:36864 [D loss: 0.389616, acc.: 80.47%] [G loss: 3.448969]\n",
      "epoch:47 step:36865 [D loss: 0.325986, acc.: 87.50%] [G loss: 2.757151]\n",
      "epoch:47 step:36866 [D loss: 0.231054, acc.: 88.28%] [G loss: 3.729401]\n",
      "epoch:47 step:36867 [D loss: 0.388976, acc.: 82.03%] [G loss: 3.948468]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:47 step:36868 [D loss: 0.400668, acc.: 83.59%] [G loss: 5.605769]\n",
      "epoch:47 step:36869 [D loss: 0.369400, acc.: 82.81%] [G loss: 4.689853]\n",
      "epoch:47 step:36870 [D loss: 0.368184, acc.: 83.59%] [G loss: 3.829679]\n",
      "epoch:47 step:36871 [D loss: 0.302075, acc.: 86.72%] [G loss: 4.300619]\n",
      "epoch:47 step:36872 [D loss: 0.291653, acc.: 89.84%] [G loss: 4.879694]\n",
      "epoch:47 step:36873 [D loss: 0.293721, acc.: 85.94%] [G loss: 4.460921]\n",
      "epoch:47 step:36874 [D loss: 0.251899, acc.: 88.28%] [G loss: 5.256996]\n",
      "epoch:47 step:36875 [D loss: 0.278282, acc.: 85.94%] [G loss: 3.908471]\n",
      "epoch:47 step:36876 [D loss: 0.258926, acc.: 87.50%] [G loss: 4.997387]\n",
      "epoch:47 step:36877 [D loss: 0.221902, acc.: 91.41%] [G loss: 3.935602]\n",
      "epoch:47 step:36878 [D loss: 0.317930, acc.: 84.38%] [G loss: 2.604529]\n",
      "epoch:47 step:36879 [D loss: 0.245161, acc.: 89.06%] [G loss: 3.400617]\n",
      "epoch:47 step:36880 [D loss: 0.260687, acc.: 89.06%] [G loss: 3.510606]\n",
      "epoch:47 step:36881 [D loss: 0.318094, acc.: 86.72%] [G loss: 2.873718]\n",
      "epoch:47 step:36882 [D loss: 0.339033, acc.: 85.16%] [G loss: 3.447784]\n",
      "epoch:47 step:36883 [D loss: 0.316747, acc.: 86.72%] [G loss: 2.370363]\n",
      "epoch:47 step:36884 [D loss: 0.407531, acc.: 78.91%] [G loss: 2.999156]\n",
      "epoch:47 step:36885 [D loss: 0.311808, acc.: 87.50%] [G loss: 3.860013]\n",
      "epoch:47 step:36886 [D loss: 0.393577, acc.: 83.59%] [G loss: 3.181986]\n",
      "epoch:47 step:36887 [D loss: 0.316288, acc.: 87.50%] [G loss: 5.379821]\n",
      "epoch:47 step:36888 [D loss: 0.209056, acc.: 92.97%] [G loss: 3.655754]\n",
      "epoch:47 step:36889 [D loss: 0.307359, acc.: 85.16%] [G loss: 4.063669]\n",
      "epoch:47 step:36890 [D loss: 0.327515, acc.: 84.38%] [G loss: 2.679335]\n",
      "epoch:47 step:36891 [D loss: 0.317502, acc.: 87.50%] [G loss: 3.881369]\n",
      "epoch:47 step:36892 [D loss: 0.266721, acc.: 86.72%] [G loss: 3.883973]\n",
      "epoch:47 step:36893 [D loss: 0.330659, acc.: 85.16%] [G loss: 2.951393]\n",
      "epoch:47 step:36894 [D loss: 0.315995, acc.: 84.38%] [G loss: 3.780447]\n",
      "epoch:47 step:36895 [D loss: 0.398852, acc.: 76.56%] [G loss: 3.526497]\n",
      "epoch:47 step:36896 [D loss: 0.241014, acc.: 87.50%] [G loss: 3.393951]\n",
      "epoch:47 step:36897 [D loss: 0.290690, acc.: 87.50%] [G loss: 3.540583]\n",
      "epoch:47 step:36898 [D loss: 0.295053, acc.: 89.06%] [G loss: 3.426965]\n",
      "epoch:47 step:36899 [D loss: 0.245833, acc.: 92.97%] [G loss: 3.547201]\n",
      "epoch:47 step:36900 [D loss: 0.305745, acc.: 89.06%] [G loss: 4.564033]\n",
      "epoch:47 step:36901 [D loss: 0.445973, acc.: 82.03%] [G loss: 3.446513]\n",
      "epoch:47 step:36902 [D loss: 0.352424, acc.: 86.72%] [G loss: 4.056019]\n",
      "epoch:47 step:36903 [D loss: 0.227935, acc.: 89.84%] [G loss: 3.725330]\n",
      "epoch:47 step:36904 [D loss: 0.338239, acc.: 85.16%] [G loss: 4.150074]\n",
      "epoch:47 step:36905 [D loss: 0.327236, acc.: 82.81%] [G loss: 4.085079]\n",
      "epoch:47 step:36906 [D loss: 0.269811, acc.: 89.06%] [G loss: 3.551147]\n",
      "epoch:47 step:36907 [D loss: 0.291270, acc.: 89.06%] [G loss: 3.455998]\n",
      "epoch:47 step:36908 [D loss: 0.270174, acc.: 86.72%] [G loss: 3.719355]\n",
      "epoch:47 step:36909 [D loss: 0.295857, acc.: 84.38%] [G loss: 3.275086]\n",
      "epoch:47 step:36910 [D loss: 0.178970, acc.: 93.75%] [G loss: 4.307953]\n",
      "epoch:47 step:36911 [D loss: 0.363779, acc.: 82.81%] [G loss: 2.723271]\n",
      "epoch:47 step:36912 [D loss: 0.202035, acc.: 92.97%] [G loss: 4.818153]\n",
      "epoch:47 step:36913 [D loss: 0.310612, acc.: 85.16%] [G loss: 3.356971]\n",
      "epoch:47 step:36914 [D loss: 0.271635, acc.: 83.59%] [G loss: 4.560759]\n",
      "epoch:47 step:36915 [D loss: 0.267509, acc.: 87.50%] [G loss: 3.847797]\n",
      "epoch:47 step:36916 [D loss: 0.354798, acc.: 82.81%] [G loss: 4.764463]\n",
      "epoch:47 step:36917 [D loss: 0.233752, acc.: 88.28%] [G loss: 6.074486]\n",
      "epoch:47 step:36918 [D loss: 0.353848, acc.: 82.81%] [G loss: 3.905640]\n",
      "epoch:47 step:36919 [D loss: 0.223272, acc.: 89.84%] [G loss: 3.289912]\n",
      "epoch:47 step:36920 [D loss: 0.360955, acc.: 85.16%] [G loss: 3.105363]\n",
      "epoch:47 step:36921 [D loss: 0.321600, acc.: 88.28%] [G loss: 3.167167]\n",
      "epoch:47 step:36922 [D loss: 0.271180, acc.: 87.50%] [G loss: 3.779556]\n",
      "epoch:47 step:36923 [D loss: 0.385254, acc.: 82.81%] [G loss: 3.935155]\n",
      "epoch:47 step:36924 [D loss: 0.352923, acc.: 86.72%] [G loss: 4.050374]\n",
      "epoch:47 step:36925 [D loss: 0.327479, acc.: 83.59%] [G loss: 2.965322]\n",
      "epoch:47 step:36926 [D loss: 0.331181, acc.: 82.03%] [G loss: 3.806177]\n",
      "epoch:47 step:36927 [D loss: 0.345034, acc.: 85.94%] [G loss: 3.293734]\n",
      "epoch:47 step:36928 [D loss: 0.320430, acc.: 84.38%] [G loss: 3.276309]\n",
      "epoch:47 step:36929 [D loss: 0.284224, acc.: 85.16%] [G loss: 3.253638]\n",
      "epoch:47 step:36930 [D loss: 0.365434, acc.: 82.81%] [G loss: 3.375108]\n",
      "epoch:47 step:36931 [D loss: 0.320904, acc.: 87.50%] [G loss: 4.059700]\n",
      "epoch:47 step:36932 [D loss: 0.533099, acc.: 78.91%] [G loss: 3.615331]\n",
      "epoch:47 step:36933 [D loss: 0.339069, acc.: 85.16%] [G loss: 3.598091]\n",
      "epoch:47 step:36934 [D loss: 0.327980, acc.: 86.72%] [G loss: 2.974763]\n",
      "epoch:47 step:36935 [D loss: 0.317201, acc.: 88.28%] [G loss: 2.792920]\n",
      "epoch:47 step:36936 [D loss: 0.314122, acc.: 82.81%] [G loss: 2.863382]\n",
      "epoch:47 step:36937 [D loss: 0.390903, acc.: 82.81%] [G loss: 2.773806]\n",
      "epoch:47 step:36938 [D loss: 0.247635, acc.: 89.06%] [G loss: 2.794288]\n",
      "epoch:47 step:36939 [D loss: 0.268685, acc.: 88.28%] [G loss: 3.268561]\n",
      "epoch:47 step:36940 [D loss: 0.445854, acc.: 82.81%] [G loss: 2.815065]\n",
      "epoch:47 step:36941 [D loss: 0.468677, acc.: 76.56%] [G loss: 2.801429]\n",
      "epoch:47 step:36942 [D loss: 0.356507, acc.: 80.47%] [G loss: 2.766833]\n",
      "epoch:47 step:36943 [D loss: 0.303451, acc.: 85.94%] [G loss: 4.103246]\n",
      "epoch:47 step:36944 [D loss: 0.384917, acc.: 80.47%] [G loss: 2.917256]\n",
      "epoch:47 step:36945 [D loss: 0.336343, acc.: 83.59%] [G loss: 4.572138]\n",
      "epoch:47 step:36946 [D loss: 0.271029, acc.: 87.50%] [G loss: 3.193946]\n",
      "epoch:47 step:36947 [D loss: 0.307562, acc.: 88.28%] [G loss: 3.074431]\n",
      "epoch:47 step:36948 [D loss: 0.298035, acc.: 85.16%] [G loss: 3.450215]\n",
      "epoch:47 step:36949 [D loss: 0.281461, acc.: 88.28%] [G loss: 3.020229]\n",
      "epoch:47 step:36950 [D loss: 0.311546, acc.: 89.06%] [G loss: 3.434370]\n",
      "epoch:47 step:36951 [D loss: 0.286864, acc.: 86.72%] [G loss: 2.859019]\n",
      "epoch:47 step:36952 [D loss: 0.315399, acc.: 88.28%] [G loss: 3.211141]\n",
      "epoch:47 step:36953 [D loss: 0.266681, acc.: 91.41%] [G loss: 2.959657]\n",
      "epoch:47 step:36954 [D loss: 0.553067, acc.: 72.66%] [G loss: 3.428536]\n",
      "epoch:47 step:36955 [D loss: 0.485998, acc.: 81.25%] [G loss: 3.075147]\n",
      "epoch:47 step:36956 [D loss: 0.323876, acc.: 85.16%] [G loss: 3.138903]\n",
      "epoch:47 step:36957 [D loss: 0.429758, acc.: 80.47%] [G loss: 4.553295]\n",
      "epoch:47 step:36958 [D loss: 0.211303, acc.: 92.19%] [G loss: 3.635233]\n",
      "epoch:47 step:36959 [D loss: 0.446235, acc.: 78.91%] [G loss: 7.108403]\n",
      "epoch:47 step:36960 [D loss: 0.340873, acc.: 82.81%] [G loss: 4.500736]\n",
      "epoch:47 step:36961 [D loss: 0.266781, acc.: 89.84%] [G loss: 3.864995]\n",
      "epoch:47 step:36962 [D loss: 0.253534, acc.: 87.50%] [G loss: 3.335592]\n",
      "epoch:47 step:36963 [D loss: 0.229872, acc.: 92.97%] [G loss: 3.454991]\n",
      "epoch:47 step:36964 [D loss: 0.310272, acc.: 88.28%] [G loss: 3.097465]\n",
      "epoch:47 step:36965 [D loss: 0.299497, acc.: 84.38%] [G loss: 3.638760]\n",
      "epoch:47 step:36966 [D loss: 0.265548, acc.: 86.72%] [G loss: 3.370378]\n",
      "epoch:47 step:36967 [D loss: 0.301776, acc.: 87.50%] [G loss: 5.026686]\n",
      "epoch:47 step:36968 [D loss: 0.378013, acc.: 84.38%] [G loss: 3.612338]\n",
      "epoch:47 step:36969 [D loss: 0.240605, acc.: 90.62%] [G loss: 4.152036]\n",
      "epoch:47 step:36970 [D loss: 0.210996, acc.: 92.19%] [G loss: 3.629479]\n",
      "epoch:47 step:36971 [D loss: 0.264221, acc.: 89.84%] [G loss: 3.357434]\n",
      "epoch:47 step:36972 [D loss: 0.284509, acc.: 88.28%] [G loss: 3.395200]\n",
      "epoch:47 step:36973 [D loss: 0.291584, acc.: 86.72%] [G loss: 3.488800]\n",
      "epoch:47 step:36974 [D loss: 0.488622, acc.: 75.00%] [G loss: 3.719445]\n",
      "epoch:47 step:36975 [D loss: 0.267301, acc.: 88.28%] [G loss: 4.892333]\n",
      "epoch:47 step:36976 [D loss: 0.386567, acc.: 79.69%] [G loss: 2.917964]\n",
      "epoch:47 step:36977 [D loss: 0.232199, acc.: 89.84%] [G loss: 3.979439]\n",
      "epoch:47 step:36978 [D loss: 0.240294, acc.: 87.50%] [G loss: 3.546866]\n",
      "epoch:47 step:36979 [D loss: 0.287946, acc.: 85.94%] [G loss: 3.487340]\n",
      "epoch:47 step:36980 [D loss: 0.317432, acc.: 86.72%] [G loss: 3.035229]\n",
      "epoch:47 step:36981 [D loss: 0.304543, acc.: 89.06%] [G loss: 3.463735]\n",
      "epoch:47 step:36982 [D loss: 0.351832, acc.: 85.16%] [G loss: 2.709573]\n",
      "epoch:47 step:36983 [D loss: 0.344794, acc.: 83.59%] [G loss: 2.702321]\n",
      "epoch:47 step:36984 [D loss: 0.265883, acc.: 85.94%] [G loss: 3.418450]\n",
      "epoch:47 step:36985 [D loss: 0.431686, acc.: 80.47%] [G loss: 3.538405]\n",
      "epoch:47 step:36986 [D loss: 0.389859, acc.: 86.72%] [G loss: 3.130974]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:47 step:36987 [D loss: 0.270855, acc.: 89.84%] [G loss: 3.119522]\n",
      "epoch:47 step:36988 [D loss: 0.378466, acc.: 81.25%] [G loss: 3.107235]\n",
      "epoch:47 step:36989 [D loss: 0.504788, acc.: 77.34%] [G loss: 3.685421]\n",
      "epoch:47 step:36990 [D loss: 0.243752, acc.: 91.41%] [G loss: 3.054415]\n",
      "epoch:47 step:36991 [D loss: 0.415555, acc.: 80.47%] [G loss: 4.650830]\n",
      "epoch:47 step:36992 [D loss: 0.435763, acc.: 80.47%] [G loss: 3.330253]\n",
      "epoch:47 step:36993 [D loss: 0.393866, acc.: 82.03%] [G loss: 3.447042]\n",
      "epoch:47 step:36994 [D loss: 0.355132, acc.: 80.47%] [G loss: 3.499225]\n",
      "epoch:47 step:36995 [D loss: 0.284234, acc.: 87.50%] [G loss: 5.163668]\n",
      "epoch:47 step:36996 [D loss: 0.255374, acc.: 91.41%] [G loss: 3.189098]\n",
      "epoch:47 step:36997 [D loss: 0.323856, acc.: 80.47%] [G loss: 3.589476]\n",
      "epoch:47 step:36998 [D loss: 0.283112, acc.: 89.06%] [G loss: 4.335721]\n",
      "epoch:47 step:36999 [D loss: 0.225665, acc.: 90.62%] [G loss: 3.628924]\n",
      "epoch:47 step:37000 [D loss: 0.309649, acc.: 82.81%] [G loss: 4.121477]\n",
      "##############\n",
      "[0.85086566 0.86704902 0.83561683 0.82787822 0.78456783 0.83559164\n",
      " 0.88990332 0.83870725 0.79928807 0.82200477]\n",
      "##########\n",
      "epoch:47 step:37001 [D loss: 0.370139, acc.: 84.38%] [G loss: 2.523201]\n",
      "epoch:47 step:37002 [D loss: 0.333672, acc.: 84.38%] [G loss: 4.235307]\n",
      "epoch:47 step:37003 [D loss: 0.447905, acc.: 79.69%] [G loss: 3.826524]\n",
      "epoch:47 step:37004 [D loss: 0.474061, acc.: 81.25%] [G loss: 3.740754]\n",
      "epoch:47 step:37005 [D loss: 0.310333, acc.: 87.50%] [G loss: 4.547527]\n",
      "epoch:47 step:37006 [D loss: 0.323345, acc.: 87.50%] [G loss: 3.732170]\n",
      "epoch:47 step:37007 [D loss: 0.318666, acc.: 85.16%] [G loss: 3.735855]\n",
      "epoch:47 step:37008 [D loss: 0.284054, acc.: 86.72%] [G loss: 3.235106]\n",
      "epoch:47 step:37009 [D loss: 0.242093, acc.: 90.62%] [G loss: 4.072690]\n",
      "epoch:47 step:37010 [D loss: 0.261978, acc.: 89.06%] [G loss: 3.366612]\n",
      "epoch:47 step:37011 [D loss: 0.276236, acc.: 89.06%] [G loss: 2.660325]\n",
      "epoch:47 step:37012 [D loss: 0.309341, acc.: 85.94%] [G loss: 3.505579]\n",
      "epoch:47 step:37013 [D loss: 0.270346, acc.: 88.28%] [G loss: 3.881361]\n",
      "epoch:47 step:37014 [D loss: 0.315899, acc.: 89.06%] [G loss: 3.254462]\n",
      "epoch:47 step:37015 [D loss: 0.289091, acc.: 88.28%] [G loss: 2.951205]\n",
      "epoch:47 step:37016 [D loss: 0.275096, acc.: 86.72%] [G loss: 4.130520]\n",
      "epoch:47 step:37017 [D loss: 0.358394, acc.: 83.59%] [G loss: 5.029670]\n",
      "epoch:47 step:37018 [D loss: 0.341965, acc.: 83.59%] [G loss: 8.360345]\n",
      "epoch:47 step:37019 [D loss: 0.715236, acc.: 72.66%] [G loss: 8.186285]\n",
      "epoch:47 step:37020 [D loss: 0.176863, acc.: 94.53%] [G loss: 6.499886]\n",
      "epoch:47 step:37021 [D loss: 0.232837, acc.: 89.84%] [G loss: 5.294821]\n",
      "epoch:47 step:37022 [D loss: 0.447601, acc.: 75.78%] [G loss: 5.326001]\n",
      "epoch:47 step:37023 [D loss: 0.293512, acc.: 87.50%] [G loss: 5.625075]\n",
      "epoch:47 step:37024 [D loss: 0.401649, acc.: 85.16%] [G loss: 7.933413]\n",
      "epoch:47 step:37025 [D loss: 0.205590, acc.: 90.62%] [G loss: 5.447654]\n",
      "epoch:47 step:37026 [D loss: 0.307776, acc.: 87.50%] [G loss: 4.782026]\n",
      "epoch:47 step:37027 [D loss: 0.382925, acc.: 82.81%] [G loss: 5.122160]\n",
      "epoch:47 step:37028 [D loss: 0.232823, acc.: 89.84%] [G loss: 4.406462]\n",
      "epoch:47 step:37029 [D loss: 0.376139, acc.: 82.81%] [G loss: 4.131097]\n",
      "epoch:47 step:37030 [D loss: 0.330514, acc.: 81.25%] [G loss: 3.539028]\n",
      "epoch:47 step:37031 [D loss: 0.409695, acc.: 81.25%] [G loss: 3.486444]\n",
      "epoch:47 step:37032 [D loss: 0.333534, acc.: 82.81%] [G loss: 3.344602]\n",
      "epoch:47 step:37033 [D loss: 0.361787, acc.: 87.50%] [G loss: 3.569716]\n",
      "epoch:47 step:37034 [D loss: 0.305110, acc.: 87.50%] [G loss: 2.979400]\n",
      "epoch:47 step:37035 [D loss: 0.271885, acc.: 85.94%] [G loss: 3.591054]\n",
      "epoch:47 step:37036 [D loss: 0.369782, acc.: 79.69%] [G loss: 4.544188]\n",
      "epoch:47 step:37037 [D loss: 0.277740, acc.: 85.16%] [G loss: 5.844477]\n",
      "epoch:47 step:37038 [D loss: 0.230201, acc.: 92.19%] [G loss: 5.068369]\n",
      "epoch:47 step:37039 [D loss: 0.351049, acc.: 84.38%] [G loss: 4.866643]\n",
      "epoch:47 step:37040 [D loss: 0.216519, acc.: 90.62%] [G loss: 3.909118]\n",
      "epoch:47 step:37041 [D loss: 0.328222, acc.: 85.16%] [G loss: 3.321709]\n",
      "epoch:47 step:37042 [D loss: 0.302180, acc.: 84.38%] [G loss: 3.654027]\n",
      "epoch:47 step:37043 [D loss: 0.264265, acc.: 86.72%] [G loss: 3.359576]\n",
      "epoch:47 step:37044 [D loss: 0.315028, acc.: 82.81%] [G loss: 3.837304]\n",
      "epoch:47 step:37045 [D loss: 0.232423, acc.: 89.06%] [G loss: 3.438745]\n",
      "epoch:47 step:37046 [D loss: 0.206698, acc.: 90.62%] [G loss: 3.316560]\n",
      "epoch:47 step:37047 [D loss: 0.262889, acc.: 86.72%] [G loss: 3.035499]\n",
      "epoch:47 step:37048 [D loss: 0.318231, acc.: 85.16%] [G loss: 2.545260]\n",
      "epoch:47 step:37049 [D loss: 0.289234, acc.: 88.28%] [G loss: 2.644781]\n",
      "epoch:47 step:37050 [D loss: 0.337693, acc.: 78.91%] [G loss: 2.950942]\n",
      "epoch:47 step:37051 [D loss: 0.420246, acc.: 78.12%] [G loss: 2.360202]\n",
      "epoch:47 step:37052 [D loss: 0.355815, acc.: 83.59%] [G loss: 3.981830]\n",
      "epoch:47 step:37053 [D loss: 0.296299, acc.: 87.50%] [G loss: 3.225436]\n",
      "epoch:47 step:37054 [D loss: 0.338290, acc.: 83.59%] [G loss: 3.587429]\n",
      "epoch:47 step:37055 [D loss: 0.291522, acc.: 86.72%] [G loss: 3.015957]\n",
      "epoch:47 step:37056 [D loss: 0.344945, acc.: 87.50%] [G loss: 2.967407]\n",
      "epoch:47 step:37057 [D loss: 0.249662, acc.: 89.06%] [G loss: 3.000831]\n",
      "epoch:47 step:37058 [D loss: 0.304036, acc.: 86.72%] [G loss: 2.582255]\n",
      "epoch:47 step:37059 [D loss: 0.371089, acc.: 81.25%] [G loss: 2.457009]\n",
      "epoch:47 step:37060 [D loss: 0.432007, acc.: 79.69%] [G loss: 3.785219]\n",
      "epoch:47 step:37061 [D loss: 0.295351, acc.: 85.94%] [G loss: 3.669379]\n",
      "epoch:47 step:37062 [D loss: 0.294428, acc.: 89.06%] [G loss: 4.007755]\n",
      "epoch:47 step:37063 [D loss: 0.294501, acc.: 82.81%] [G loss: 5.352498]\n",
      "epoch:47 step:37064 [D loss: 0.465730, acc.: 75.00%] [G loss: 3.337202]\n",
      "epoch:47 step:37065 [D loss: 0.447414, acc.: 78.91%] [G loss: 3.219379]\n",
      "epoch:47 step:37066 [D loss: 0.259392, acc.: 86.72%] [G loss: 3.419020]\n",
      "epoch:47 step:37067 [D loss: 0.293294, acc.: 82.03%] [G loss: 4.193083]\n",
      "epoch:47 step:37068 [D loss: 0.321114, acc.: 85.16%] [G loss: 7.045248]\n",
      "epoch:47 step:37069 [D loss: 0.241468, acc.: 89.84%] [G loss: 3.835799]\n",
      "epoch:47 step:37070 [D loss: 0.202829, acc.: 90.62%] [G loss: 3.026665]\n",
      "epoch:47 step:37071 [D loss: 0.347357, acc.: 86.72%] [G loss: 3.317968]\n",
      "epoch:47 step:37072 [D loss: 0.382180, acc.: 85.16%] [G loss: 2.864351]\n",
      "epoch:47 step:37073 [D loss: 0.358697, acc.: 85.16%] [G loss: 2.788454]\n",
      "epoch:47 step:37074 [D loss: 0.323611, acc.: 85.94%] [G loss: 3.220651]\n",
      "epoch:47 step:37075 [D loss: 0.327644, acc.: 89.06%] [G loss: 2.559148]\n",
      "epoch:47 step:37076 [D loss: 0.353261, acc.: 82.81%] [G loss: 2.787134]\n",
      "epoch:47 step:37077 [D loss: 0.388722, acc.: 82.81%] [G loss: 3.397449]\n",
      "epoch:47 step:37078 [D loss: 0.276506, acc.: 85.94%] [G loss: 4.118264]\n",
      "epoch:47 step:37079 [D loss: 0.307519, acc.: 86.72%] [G loss: 4.661673]\n",
      "epoch:47 step:37080 [D loss: 0.253727, acc.: 87.50%] [G loss: 4.133835]\n",
      "epoch:47 step:37081 [D loss: 0.222249, acc.: 90.62%] [G loss: 5.069859]\n",
      "epoch:47 step:37082 [D loss: 0.292861, acc.: 85.16%] [G loss: 4.711084]\n",
      "epoch:47 step:37083 [D loss: 0.319797, acc.: 85.94%] [G loss: 3.035471]\n",
      "epoch:47 step:37084 [D loss: 0.328803, acc.: 87.50%] [G loss: 3.935133]\n",
      "epoch:47 step:37085 [D loss: 0.321243, acc.: 84.38%] [G loss: 3.749876]\n",
      "epoch:47 step:37086 [D loss: 0.382703, acc.: 86.72%] [G loss: 4.257421]\n",
      "epoch:47 step:37087 [D loss: 0.285505, acc.: 90.62%] [G loss: 3.781590]\n",
      "epoch:47 step:37088 [D loss: 0.212558, acc.: 90.62%] [G loss: 3.972320]\n",
      "epoch:47 step:37089 [D loss: 0.370724, acc.: 83.59%] [G loss: 4.287923]\n",
      "epoch:47 step:37090 [D loss: 0.342255, acc.: 83.59%] [G loss: 4.024548]\n",
      "epoch:47 step:37091 [D loss: 0.262315, acc.: 87.50%] [G loss: 3.140038]\n",
      "epoch:47 step:37092 [D loss: 0.545681, acc.: 77.34%] [G loss: 8.616814]\n",
      "epoch:47 step:37093 [D loss: 0.718199, acc.: 69.53%] [G loss: 3.223095]\n",
      "epoch:47 step:37094 [D loss: 0.358084, acc.: 82.81%] [G loss: 2.807062]\n",
      "epoch:47 step:37095 [D loss: 0.375540, acc.: 83.59%] [G loss: 3.556360]\n",
      "epoch:47 step:37096 [D loss: 0.311709, acc.: 86.72%] [G loss: 3.489742]\n",
      "epoch:47 step:37097 [D loss: 0.352160, acc.: 81.25%] [G loss: 2.462206]\n",
      "epoch:47 step:37098 [D loss: 0.376250, acc.: 84.38%] [G loss: 3.299297]\n",
      "epoch:47 step:37099 [D loss: 0.385006, acc.: 86.72%] [G loss: 3.979370]\n",
      "epoch:47 step:37100 [D loss: 0.369501, acc.: 84.38%] [G loss: 3.007554]\n",
      "epoch:47 step:37101 [D loss: 0.210067, acc.: 92.19%] [G loss: 2.507369]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:47 step:37102 [D loss: 0.287969, acc.: 86.72%] [G loss: 3.399549]\n",
      "epoch:47 step:37103 [D loss: 0.337084, acc.: 83.59%] [G loss: 3.346404]\n",
      "epoch:47 step:37104 [D loss: 0.338729, acc.: 83.59%] [G loss: 3.544916]\n",
      "epoch:47 step:37105 [D loss: 0.353796, acc.: 84.38%] [G loss: 3.793603]\n",
      "epoch:47 step:37106 [D loss: 0.234721, acc.: 90.62%] [G loss: 3.278968]\n",
      "epoch:47 step:37107 [D loss: 0.359945, acc.: 84.38%] [G loss: 2.600795]\n",
      "epoch:47 step:37108 [D loss: 0.319449, acc.: 85.94%] [G loss: 2.700324]\n",
      "epoch:47 step:37109 [D loss: 0.319174, acc.: 84.38%] [G loss: 2.981198]\n",
      "epoch:47 step:37110 [D loss: 0.312665, acc.: 85.94%] [G loss: 3.548351]\n",
      "epoch:47 step:37111 [D loss: 0.413723, acc.: 85.94%] [G loss: 2.769892]\n",
      "epoch:47 step:37112 [D loss: 0.288421, acc.: 88.28%] [G loss: 3.020830]\n",
      "epoch:47 step:37113 [D loss: 0.305702, acc.: 83.59%] [G loss: 3.720686]\n",
      "epoch:47 step:37114 [D loss: 0.361363, acc.: 85.94%] [G loss: 2.998841]\n",
      "epoch:47 step:37115 [D loss: 0.266498, acc.: 89.06%] [G loss: 2.969936]\n",
      "epoch:47 step:37116 [D loss: 0.229392, acc.: 93.75%] [G loss: 3.604574]\n",
      "epoch:47 step:37117 [D loss: 0.307562, acc.: 85.94%] [G loss: 3.033777]\n",
      "epoch:47 step:37118 [D loss: 0.241858, acc.: 91.41%] [G loss: 2.808952]\n",
      "epoch:47 step:37119 [D loss: 0.318194, acc.: 83.59%] [G loss: 3.179008]\n",
      "epoch:47 step:37120 [D loss: 0.290435, acc.: 89.84%] [G loss: 2.984338]\n",
      "epoch:47 step:37121 [D loss: 0.315822, acc.: 85.94%] [G loss: 3.813084]\n",
      "epoch:47 step:37122 [D loss: 0.281058, acc.: 86.72%] [G loss: 2.546129]\n",
      "epoch:47 step:37123 [D loss: 0.360410, acc.: 86.72%] [G loss: 3.278237]\n",
      "epoch:47 step:37124 [D loss: 0.385542, acc.: 83.59%] [G loss: 2.942270]\n",
      "epoch:47 step:37125 [D loss: 0.400782, acc.: 78.12%] [G loss: 3.396842]\n",
      "epoch:47 step:37126 [D loss: 0.323942, acc.: 82.81%] [G loss: 4.028737]\n",
      "epoch:47 step:37127 [D loss: 0.284992, acc.: 88.28%] [G loss: 3.564724]\n",
      "epoch:47 step:37128 [D loss: 0.255189, acc.: 89.84%] [G loss: 3.418849]\n",
      "epoch:47 step:37129 [D loss: 0.317791, acc.: 83.59%] [G loss: 3.392012]\n",
      "epoch:47 step:37130 [D loss: 0.293175, acc.: 87.50%] [G loss: 3.150619]\n",
      "epoch:47 step:37131 [D loss: 0.275028, acc.: 89.84%] [G loss: 3.907149]\n",
      "epoch:47 step:37132 [D loss: 0.292417, acc.: 85.16%] [G loss: 2.772329]\n",
      "epoch:47 step:37133 [D loss: 0.279949, acc.: 88.28%] [G loss: 3.420576]\n",
      "epoch:47 step:37134 [D loss: 0.252727, acc.: 90.62%] [G loss: 3.189498]\n",
      "epoch:47 step:37135 [D loss: 0.373454, acc.: 82.03%] [G loss: 2.964539]\n",
      "epoch:47 step:37136 [D loss: 0.328996, acc.: 84.38%] [G loss: 4.189486]\n",
      "epoch:47 step:37137 [D loss: 0.303096, acc.: 87.50%] [G loss: 3.076355]\n",
      "epoch:47 step:37138 [D loss: 0.367009, acc.: 81.25%] [G loss: 2.728920]\n",
      "epoch:47 step:37139 [D loss: 0.330975, acc.: 84.38%] [G loss: 3.481939]\n",
      "epoch:47 step:37140 [D loss: 0.235383, acc.: 91.41%] [G loss: 4.425977]\n",
      "epoch:47 step:37141 [D loss: 0.348194, acc.: 85.16%] [G loss: 5.069567]\n",
      "epoch:47 step:37142 [D loss: 0.458841, acc.: 81.25%] [G loss: 5.756484]\n",
      "epoch:47 step:37143 [D loss: 0.994806, acc.: 63.28%] [G loss: 7.297008]\n",
      "epoch:47 step:37144 [D loss: 1.502551, acc.: 54.69%] [G loss: 5.230372]\n",
      "epoch:47 step:37145 [D loss: 0.530619, acc.: 78.12%] [G loss: 7.325227]\n",
      "epoch:47 step:37146 [D loss: 0.936016, acc.: 67.19%] [G loss: 7.588154]\n",
      "epoch:47 step:37147 [D loss: 0.806683, acc.: 71.09%] [G loss: 4.156741]\n",
      "epoch:47 step:37148 [D loss: 0.348350, acc.: 83.59%] [G loss: 3.777550]\n",
      "epoch:47 step:37149 [D loss: 0.323693, acc.: 85.94%] [G loss: 3.112108]\n",
      "epoch:47 step:37150 [D loss: 0.351184, acc.: 82.03%] [G loss: 3.350673]\n",
      "epoch:47 step:37151 [D loss: 0.363538, acc.: 82.03%] [G loss: 3.266497]\n",
      "epoch:47 step:37152 [D loss: 0.291143, acc.: 82.03%] [G loss: 3.050730]\n",
      "epoch:47 step:37153 [D loss: 0.275062, acc.: 89.84%] [G loss: 2.945426]\n",
      "epoch:47 step:37154 [D loss: 0.354041, acc.: 84.38%] [G loss: 3.179699]\n",
      "epoch:47 step:37155 [D loss: 0.338204, acc.: 85.16%] [G loss: 3.397949]\n",
      "epoch:47 step:37156 [D loss: 0.267805, acc.: 87.50%] [G loss: 3.032834]\n",
      "epoch:47 step:37157 [D loss: 0.357105, acc.: 83.59%] [G loss: 3.623781]\n",
      "epoch:47 step:37158 [D loss: 0.269733, acc.: 87.50%] [G loss: 3.378045]\n",
      "epoch:47 step:37159 [D loss: 0.238937, acc.: 89.06%] [G loss: 3.532844]\n",
      "epoch:47 step:37160 [D loss: 0.272127, acc.: 89.06%] [G loss: 2.906988]\n",
      "epoch:47 step:37161 [D loss: 0.289818, acc.: 86.72%] [G loss: 3.418688]\n",
      "epoch:47 step:37162 [D loss: 0.224556, acc.: 89.84%] [G loss: 2.531492]\n",
      "epoch:47 step:37163 [D loss: 0.268139, acc.: 88.28%] [G loss: 2.710833]\n",
      "epoch:47 step:37164 [D loss: 0.311691, acc.: 81.25%] [G loss: 2.999785]\n",
      "epoch:47 step:37165 [D loss: 0.294715, acc.: 88.28%] [G loss: 3.435771]\n",
      "epoch:47 step:37166 [D loss: 0.369955, acc.: 82.03%] [G loss: 3.105039]\n",
      "epoch:47 step:37167 [D loss: 0.395196, acc.: 84.38%] [G loss: 3.187853]\n",
      "epoch:47 step:37168 [D loss: 0.286343, acc.: 86.72%] [G loss: 2.657356]\n",
      "epoch:47 step:37169 [D loss: 0.359676, acc.: 82.81%] [G loss: 2.971582]\n",
      "epoch:47 step:37170 [D loss: 0.323161, acc.: 84.38%] [G loss: 3.057494]\n",
      "epoch:47 step:37171 [D loss: 0.399785, acc.: 83.59%] [G loss: 3.258077]\n",
      "epoch:47 step:37172 [D loss: 0.384889, acc.: 85.16%] [G loss: 3.352638]\n",
      "epoch:47 step:37173 [D loss: 0.324721, acc.: 84.38%] [G loss: 3.560442]\n",
      "epoch:47 step:37174 [D loss: 0.257694, acc.: 89.84%] [G loss: 4.559436]\n",
      "epoch:47 step:37175 [D loss: 0.345694, acc.: 85.16%] [G loss: 3.483141]\n",
      "epoch:47 step:37176 [D loss: 0.234206, acc.: 91.41%] [G loss: 3.380397]\n",
      "epoch:47 step:37177 [D loss: 0.284026, acc.: 83.59%] [G loss: 2.757607]\n",
      "epoch:47 step:37178 [D loss: 0.267650, acc.: 89.06%] [G loss: 4.333341]\n",
      "epoch:47 step:37179 [D loss: 0.352946, acc.: 87.50%] [G loss: 2.456287]\n",
      "epoch:47 step:37180 [D loss: 0.332071, acc.: 84.38%] [G loss: 3.188098]\n",
      "epoch:47 step:37181 [D loss: 0.319206, acc.: 87.50%] [G loss: 2.813416]\n",
      "epoch:47 step:37182 [D loss: 0.291308, acc.: 89.06%] [G loss: 3.078984]\n",
      "epoch:47 step:37183 [D loss: 0.363586, acc.: 86.72%] [G loss: 3.433887]\n",
      "epoch:47 step:37184 [D loss: 0.233058, acc.: 91.41%] [G loss: 2.721225]\n",
      "epoch:47 step:37185 [D loss: 0.382912, acc.: 80.47%] [G loss: 3.214355]\n",
      "epoch:47 step:37186 [D loss: 0.263487, acc.: 88.28%] [G loss: 3.076119]\n",
      "epoch:47 step:37187 [D loss: 0.272512, acc.: 88.28%] [G loss: 3.062921]\n",
      "epoch:47 step:37188 [D loss: 0.397027, acc.: 82.03%] [G loss: 2.616371]\n",
      "epoch:47 step:37189 [D loss: 0.330962, acc.: 88.28%] [G loss: 3.322821]\n",
      "epoch:47 step:37190 [D loss: 0.330049, acc.: 83.59%] [G loss: 2.615522]\n",
      "epoch:47 step:37191 [D loss: 0.298294, acc.: 87.50%] [G loss: 3.032037]\n",
      "epoch:47 step:37192 [D loss: 0.320312, acc.: 87.50%] [G loss: 3.124562]\n",
      "epoch:47 step:37193 [D loss: 0.325474, acc.: 83.59%] [G loss: 3.248640]\n",
      "epoch:47 step:37194 [D loss: 0.358301, acc.: 85.16%] [G loss: 3.482410]\n",
      "epoch:47 step:37195 [D loss: 0.345414, acc.: 82.81%] [G loss: 3.447795]\n",
      "epoch:47 step:37196 [D loss: 0.350355, acc.: 84.38%] [G loss: 2.497367]\n",
      "epoch:47 step:37197 [D loss: 0.257980, acc.: 91.41%] [G loss: 2.824946]\n",
      "epoch:47 step:37198 [D loss: 0.435668, acc.: 78.91%] [G loss: 3.050524]\n",
      "epoch:47 step:37199 [D loss: 0.387151, acc.: 79.69%] [G loss: 2.654943]\n",
      "epoch:47 step:37200 [D loss: 0.269618, acc.: 88.28%] [G loss: 2.681609]\n",
      "##############\n",
      "[0.8716937  0.86047848 0.81106778 0.81491564 0.77740153 0.83744268\n",
      " 0.86851711 0.83166209 0.82308683 0.82511826]\n",
      "##########\n",
      "epoch:47 step:37201 [D loss: 0.243161, acc.: 92.19%] [G loss: 2.558947]\n",
      "epoch:47 step:37202 [D loss: 0.416573, acc.: 82.81%] [G loss: 2.864248]\n",
      "epoch:47 step:37203 [D loss: 0.318701, acc.: 87.50%] [G loss: 2.430537]\n",
      "epoch:47 step:37204 [D loss: 0.384446, acc.: 84.38%] [G loss: 2.320769]\n",
      "epoch:47 step:37205 [D loss: 0.267747, acc.: 88.28%] [G loss: 3.278432]\n",
      "epoch:47 step:37206 [D loss: 0.364670, acc.: 80.47%] [G loss: 2.992794]\n",
      "epoch:47 step:37207 [D loss: 0.400620, acc.: 85.16%] [G loss: 2.399833]\n",
      "epoch:47 step:37208 [D loss: 0.329739, acc.: 83.59%] [G loss: 2.248630]\n",
      "epoch:47 step:37209 [D loss: 0.279727, acc.: 89.06%] [G loss: 2.857799]\n",
      "epoch:47 step:37210 [D loss: 0.244806, acc.: 91.41%] [G loss: 2.897018]\n",
      "epoch:47 step:37211 [D loss: 0.356805, acc.: 83.59%] [G loss: 2.420765]\n",
      "epoch:47 step:37212 [D loss: 0.352170, acc.: 82.81%] [G loss: 2.807076]\n",
      "epoch:47 step:37213 [D loss: 0.384921, acc.: 82.03%] [G loss: 3.042026]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:47 step:37214 [D loss: 0.293728, acc.: 87.50%] [G loss: 2.537771]\n",
      "epoch:47 step:37215 [D loss: 0.317955, acc.: 83.59%] [G loss: 3.397236]\n",
      "epoch:47 step:37216 [D loss: 0.417473, acc.: 82.03%] [G loss: 2.978158]\n",
      "epoch:47 step:37217 [D loss: 0.255486, acc.: 87.50%] [G loss: 7.688059]\n",
      "epoch:47 step:37218 [D loss: 0.220707, acc.: 90.62%] [G loss: 4.973197]\n",
      "epoch:47 step:37219 [D loss: 0.301203, acc.: 86.72%] [G loss: 4.856407]\n",
      "epoch:47 step:37220 [D loss: 0.223154, acc.: 90.62%] [G loss: 6.439835]\n",
      "epoch:47 step:37221 [D loss: 0.292156, acc.: 82.81%] [G loss: 4.048823]\n",
      "epoch:47 step:37222 [D loss: 0.253551, acc.: 92.19%] [G loss: 4.141009]\n",
      "epoch:47 step:37223 [D loss: 0.339468, acc.: 85.94%] [G loss: 2.577235]\n",
      "epoch:47 step:37224 [D loss: 0.459943, acc.: 82.03%] [G loss: 3.803550]\n",
      "epoch:47 step:37225 [D loss: 0.347275, acc.: 83.59%] [G loss: 2.764708]\n",
      "epoch:47 step:37226 [D loss: 0.351284, acc.: 82.03%] [G loss: 4.415848]\n",
      "epoch:47 step:37227 [D loss: 0.380400, acc.: 82.81%] [G loss: 3.469615]\n",
      "epoch:47 step:37228 [D loss: 0.413234, acc.: 80.47%] [G loss: 2.421424]\n",
      "epoch:47 step:37229 [D loss: 0.298477, acc.: 86.72%] [G loss: 3.214647]\n",
      "epoch:47 step:37230 [D loss: 0.320128, acc.: 85.16%] [G loss: 3.628688]\n",
      "epoch:47 step:37231 [D loss: 0.301266, acc.: 85.94%] [G loss: 3.923095]\n",
      "epoch:47 step:37232 [D loss: 0.238011, acc.: 89.84%] [G loss: 3.349100]\n",
      "epoch:47 step:37233 [D loss: 0.366785, acc.: 83.59%] [G loss: 3.309774]\n",
      "epoch:47 step:37234 [D loss: 0.355371, acc.: 82.81%] [G loss: 3.244594]\n",
      "epoch:47 step:37235 [D loss: 0.234639, acc.: 89.06%] [G loss: 4.588851]\n",
      "epoch:47 step:37236 [D loss: 0.304218, acc.: 83.59%] [G loss: 4.632812]\n",
      "epoch:47 step:37237 [D loss: 0.260377, acc.: 89.84%] [G loss: 4.196515]\n",
      "epoch:47 step:37238 [D loss: 0.334254, acc.: 87.50%] [G loss: 3.443368]\n",
      "epoch:47 step:37239 [D loss: 0.323972, acc.: 83.59%] [G loss: 3.088034]\n",
      "epoch:47 step:37240 [D loss: 0.279211, acc.: 89.84%] [G loss: 3.043858]\n",
      "epoch:47 step:37241 [D loss: 0.361737, acc.: 83.59%] [G loss: 3.756293]\n",
      "epoch:47 step:37242 [D loss: 0.411922, acc.: 86.72%] [G loss: 2.928495]\n",
      "epoch:47 step:37243 [D loss: 0.304676, acc.: 82.81%] [G loss: 2.989779]\n",
      "epoch:47 step:37244 [D loss: 0.374258, acc.: 82.81%] [G loss: 2.535507]\n",
      "epoch:47 step:37245 [D loss: 0.300025, acc.: 84.38%] [G loss: 2.644577]\n",
      "epoch:47 step:37246 [D loss: 0.291454, acc.: 86.72%] [G loss: 2.939172]\n",
      "epoch:47 step:37247 [D loss: 0.254342, acc.: 89.84%] [G loss: 2.977902]\n",
      "epoch:47 step:37248 [D loss: 0.334730, acc.: 85.94%] [G loss: 4.283626]\n",
      "epoch:47 step:37249 [D loss: 0.348023, acc.: 88.28%] [G loss: 3.418191]\n",
      "epoch:47 step:37250 [D loss: 0.280138, acc.: 89.06%] [G loss: 2.952144]\n",
      "epoch:47 step:37251 [D loss: 0.443859, acc.: 80.47%] [G loss: 2.724500]\n",
      "epoch:47 step:37252 [D loss: 0.365099, acc.: 84.38%] [G loss: 2.709976]\n",
      "epoch:47 step:37253 [D loss: 0.258856, acc.: 87.50%] [G loss: 3.046046]\n",
      "epoch:47 step:37254 [D loss: 0.443558, acc.: 78.12%] [G loss: 2.847011]\n",
      "epoch:47 step:37255 [D loss: 0.317820, acc.: 86.72%] [G loss: 3.307260]\n",
      "epoch:47 step:37256 [D loss: 0.313934, acc.: 85.94%] [G loss: 2.760933]\n",
      "epoch:47 step:37257 [D loss: 0.309065, acc.: 88.28%] [G loss: 2.978272]\n",
      "epoch:47 step:37258 [D loss: 0.266824, acc.: 86.72%] [G loss: 3.252249]\n",
      "epoch:47 step:37259 [D loss: 0.404423, acc.: 84.38%] [G loss: 3.487303]\n",
      "epoch:47 step:37260 [D loss: 0.349634, acc.: 83.59%] [G loss: 5.022466]\n",
      "epoch:47 step:37261 [D loss: 0.315507, acc.: 87.50%] [G loss: 3.252209]\n",
      "epoch:47 step:37262 [D loss: 0.377978, acc.: 85.94%] [G loss: 3.500610]\n",
      "epoch:47 step:37263 [D loss: 0.468566, acc.: 81.25%] [G loss: 4.678538]\n",
      "epoch:47 step:37264 [D loss: 0.389476, acc.: 82.03%] [G loss: 3.586279]\n",
      "epoch:47 step:37265 [D loss: 0.494648, acc.: 82.03%] [G loss: 3.097533]\n",
      "epoch:47 step:37266 [D loss: 0.311037, acc.: 86.72%] [G loss: 3.741634]\n",
      "epoch:47 step:37267 [D loss: 0.272740, acc.: 90.62%] [G loss: 3.016765]\n",
      "epoch:47 step:37268 [D loss: 0.301592, acc.: 88.28%] [G loss: 4.555821]\n",
      "epoch:47 step:37269 [D loss: 0.296928, acc.: 85.94%] [G loss: 3.514011]\n",
      "epoch:47 step:37270 [D loss: 0.266113, acc.: 89.06%] [G loss: 3.895577]\n",
      "epoch:47 step:37271 [D loss: 0.411690, acc.: 81.25%] [G loss: 2.887807]\n",
      "epoch:47 step:37272 [D loss: 0.295795, acc.: 84.38%] [G loss: 4.659892]\n",
      "epoch:47 step:37273 [D loss: 0.346601, acc.: 85.16%] [G loss: 4.434875]\n",
      "epoch:47 step:37274 [D loss: 0.391551, acc.: 85.94%] [G loss: 3.531966]\n",
      "epoch:47 step:37275 [D loss: 0.449786, acc.: 78.91%] [G loss: 6.733968]\n",
      "epoch:47 step:37276 [D loss: 0.697091, acc.: 75.00%] [G loss: 5.607193]\n",
      "epoch:47 step:37277 [D loss: 0.376629, acc.: 85.16%] [G loss: 3.058088]\n",
      "epoch:47 step:37278 [D loss: 0.323440, acc.: 83.59%] [G loss: 3.488905]\n",
      "epoch:47 step:37279 [D loss: 0.431092, acc.: 82.81%] [G loss: 4.139251]\n",
      "epoch:47 step:37280 [D loss: 0.343441, acc.: 88.28%] [G loss: 3.824389]\n",
      "epoch:47 step:37281 [D loss: 0.324448, acc.: 82.81%] [G loss: 2.751409]\n",
      "epoch:47 step:37282 [D loss: 0.365890, acc.: 83.59%] [G loss: 2.974490]\n",
      "epoch:47 step:37283 [D loss: 0.370247, acc.: 82.03%] [G loss: 3.246312]\n",
      "epoch:47 step:37284 [D loss: 0.300519, acc.: 83.59%] [G loss: 3.400356]\n",
      "epoch:47 step:37285 [D loss: 0.349945, acc.: 85.94%] [G loss: 3.329526]\n",
      "epoch:47 step:37286 [D loss: 0.406389, acc.: 76.56%] [G loss: 3.007936]\n",
      "epoch:47 step:37287 [D loss: 0.440127, acc.: 82.81%] [G loss: 2.861797]\n",
      "epoch:47 step:37288 [D loss: 0.358637, acc.: 85.16%] [G loss: 3.418696]\n",
      "epoch:47 step:37289 [D loss: 0.338793, acc.: 85.94%] [G loss: 3.417782]\n",
      "epoch:47 step:37290 [D loss: 0.420317, acc.: 80.47%] [G loss: 3.388668]\n",
      "epoch:47 step:37291 [D loss: 0.383895, acc.: 84.38%] [G loss: 2.842185]\n",
      "epoch:47 step:37292 [D loss: 0.300594, acc.: 89.06%] [G loss: 3.171264]\n",
      "epoch:47 step:37293 [D loss: 0.264574, acc.: 89.06%] [G loss: 3.389705]\n",
      "epoch:47 step:37294 [D loss: 0.164689, acc.: 95.31%] [G loss: 3.297560]\n",
      "epoch:47 step:37295 [D loss: 0.367130, acc.: 83.59%] [G loss: 3.341156]\n",
      "epoch:47 step:37296 [D loss: 0.272630, acc.: 88.28%] [G loss: 3.728612]\n",
      "epoch:47 step:37297 [D loss: 0.399317, acc.: 78.91%] [G loss: 4.167498]\n",
      "epoch:47 step:37298 [D loss: 0.367718, acc.: 85.94%] [G loss: 4.397475]\n",
      "epoch:47 step:37299 [D loss: 0.218036, acc.: 90.62%] [G loss: 4.728047]\n",
      "epoch:47 step:37300 [D loss: 0.421702, acc.: 77.34%] [G loss: 3.648731]\n",
      "epoch:47 step:37301 [D loss: 0.285641, acc.: 88.28%] [G loss: 3.649783]\n",
      "epoch:47 step:37302 [D loss: 0.388277, acc.: 81.25%] [G loss: 3.270202]\n",
      "epoch:47 step:37303 [D loss: 0.524108, acc.: 76.56%] [G loss: 2.829052]\n",
      "epoch:47 step:37304 [D loss: 0.284706, acc.: 91.41%] [G loss: 4.080295]\n",
      "epoch:47 step:37305 [D loss: 0.363820, acc.: 82.03%] [G loss: 3.350490]\n",
      "epoch:47 step:37306 [D loss: 0.296105, acc.: 88.28%] [G loss: 3.586744]\n",
      "epoch:47 step:37307 [D loss: 0.367680, acc.: 79.69%] [G loss: 2.899799]\n",
      "epoch:47 step:37308 [D loss: 0.310078, acc.: 84.38%] [G loss: 3.659896]\n",
      "epoch:47 step:37309 [D loss: 0.266758, acc.: 90.62%] [G loss: 3.008349]\n",
      "epoch:47 step:37310 [D loss: 0.288515, acc.: 87.50%] [G loss: 4.432685]\n",
      "epoch:47 step:37311 [D loss: 0.262394, acc.: 89.84%] [G loss: 2.957232]\n",
      "epoch:47 step:37312 [D loss: 0.346765, acc.: 81.25%] [G loss: 3.728433]\n",
      "epoch:47 step:37313 [D loss: 0.376339, acc.: 82.81%] [G loss: 2.671077]\n",
      "epoch:47 step:37314 [D loss: 0.328711, acc.: 86.72%] [G loss: 2.742178]\n",
      "epoch:47 step:37315 [D loss: 0.313114, acc.: 86.72%] [G loss: 3.289874]\n",
      "epoch:47 step:37316 [D loss: 0.324623, acc.: 85.16%] [G loss: 3.176178]\n",
      "epoch:47 step:37317 [D loss: 0.269488, acc.: 87.50%] [G loss: 2.653214]\n",
      "epoch:47 step:37318 [D loss: 0.293557, acc.: 88.28%] [G loss: 3.288652]\n",
      "epoch:47 step:37319 [D loss: 0.228230, acc.: 89.84%] [G loss: 3.450903]\n",
      "epoch:47 step:37320 [D loss: 0.380297, acc.: 80.47%] [G loss: 2.710143]\n",
      "epoch:47 step:37321 [D loss: 0.235331, acc.: 86.72%] [G loss: 3.486246]\n",
      "epoch:47 step:37322 [D loss: 0.251737, acc.: 88.28%] [G loss: 2.962213]\n",
      "epoch:47 step:37323 [D loss: 0.411232, acc.: 80.47%] [G loss: 2.781640]\n",
      "epoch:47 step:37324 [D loss: 0.311583, acc.: 86.72%] [G loss: 2.971693]\n",
      "epoch:47 step:37325 [D loss: 0.443130, acc.: 77.34%] [G loss: 3.495639]\n",
      "epoch:47 step:37326 [D loss: 0.488215, acc.: 77.34%] [G loss: 2.860059]\n",
      "epoch:47 step:37327 [D loss: 0.321432, acc.: 85.94%] [G loss: 2.474578]\n",
      "epoch:47 step:37328 [D loss: 0.320720, acc.: 85.94%] [G loss: 2.720172]\n",
      "epoch:47 step:37329 [D loss: 0.217969, acc.: 88.28%] [G loss: 3.924726]\n",
      "epoch:47 step:37330 [D loss: 0.437865, acc.: 84.38%] [G loss: 2.937363]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:47 step:37331 [D loss: 0.326366, acc.: 85.94%] [G loss: 3.859076]\n",
      "epoch:47 step:37332 [D loss: 0.380736, acc.: 79.69%] [G loss: 3.794364]\n",
      "epoch:47 step:37333 [D loss: 0.177160, acc.: 95.31%] [G loss: 3.484781]\n",
      "epoch:47 step:37334 [D loss: 0.255998, acc.: 90.62%] [G loss: 4.172449]\n",
      "epoch:47 step:37335 [D loss: 0.380529, acc.: 85.16%] [G loss: 2.939559]\n",
      "epoch:47 step:37336 [D loss: 0.341646, acc.: 85.16%] [G loss: 3.511822]\n",
      "epoch:47 step:37337 [D loss: 0.381166, acc.: 81.25%] [G loss: 2.694110]\n",
      "epoch:47 step:37338 [D loss: 0.251828, acc.: 88.28%] [G loss: 3.061769]\n",
      "epoch:47 step:37339 [D loss: 0.329781, acc.: 82.81%] [G loss: 2.414361]\n",
      "epoch:47 step:37340 [D loss: 0.327303, acc.: 87.50%] [G loss: 3.470712]\n",
      "epoch:47 step:37341 [D loss: 0.364174, acc.: 82.03%] [G loss: 2.583863]\n",
      "epoch:47 step:37342 [D loss: 0.298468, acc.: 89.06%] [G loss: 3.458175]\n",
      "epoch:47 step:37343 [D loss: 0.305454, acc.: 85.94%] [G loss: 3.196712]\n",
      "epoch:47 step:37344 [D loss: 0.240323, acc.: 89.06%] [G loss: 2.472281]\n",
      "epoch:47 step:37345 [D loss: 0.358338, acc.: 87.50%] [G loss: 4.066859]\n",
      "epoch:47 step:37346 [D loss: 0.398425, acc.: 84.38%] [G loss: 4.050074]\n",
      "epoch:47 step:37347 [D loss: 0.207923, acc.: 90.62%] [G loss: 3.655030]\n",
      "epoch:47 step:37348 [D loss: 0.332485, acc.: 82.81%] [G loss: 3.397031]\n",
      "epoch:47 step:37349 [D loss: 0.278376, acc.: 90.62%] [G loss: 3.197889]\n",
      "epoch:47 step:37350 [D loss: 0.394252, acc.: 82.03%] [G loss: 2.946659]\n",
      "epoch:47 step:37351 [D loss: 0.243627, acc.: 89.06%] [G loss: 3.682940]\n",
      "epoch:47 step:37352 [D loss: 0.309857, acc.: 87.50%] [G loss: 2.413590]\n",
      "epoch:47 step:37353 [D loss: 0.385701, acc.: 83.59%] [G loss: 2.862730]\n",
      "epoch:47 step:37354 [D loss: 0.305078, acc.: 85.16%] [G loss: 3.128812]\n",
      "epoch:47 step:37355 [D loss: 0.255146, acc.: 89.06%] [G loss: 3.440146]\n",
      "epoch:47 step:37356 [D loss: 0.294961, acc.: 86.72%] [G loss: 4.047736]\n",
      "epoch:47 step:37357 [D loss: 0.284743, acc.: 86.72%] [G loss: 3.627832]\n",
      "epoch:47 step:37358 [D loss: 0.347175, acc.: 83.59%] [G loss: 2.700101]\n",
      "epoch:47 step:37359 [D loss: 0.324641, acc.: 87.50%] [G loss: 3.562358]\n",
      "epoch:47 step:37360 [D loss: 0.274801, acc.: 88.28%] [G loss: 4.979841]\n",
      "epoch:47 step:37361 [D loss: 0.277410, acc.: 90.62%] [G loss: 4.304852]\n",
      "epoch:47 step:37362 [D loss: 0.257514, acc.: 86.72%] [G loss: 4.490773]\n",
      "epoch:47 step:37363 [D loss: 0.259531, acc.: 88.28%] [G loss: 3.509755]\n",
      "epoch:47 step:37364 [D loss: 0.343161, acc.: 85.94%] [G loss: 3.264852]\n",
      "epoch:47 step:37365 [D loss: 0.256939, acc.: 88.28%] [G loss: 4.009000]\n",
      "epoch:47 step:37366 [D loss: 0.233621, acc.: 89.06%] [G loss: 3.823896]\n",
      "epoch:47 step:37367 [D loss: 0.324362, acc.: 83.59%] [G loss: 3.028981]\n",
      "epoch:47 step:37368 [D loss: 0.340997, acc.: 88.28%] [G loss: 3.131626]\n",
      "epoch:47 step:37369 [D loss: 0.345730, acc.: 85.16%] [G loss: 2.788529]\n",
      "epoch:47 step:37370 [D loss: 0.280300, acc.: 85.16%] [G loss: 3.962476]\n",
      "epoch:47 step:37371 [D loss: 0.230444, acc.: 89.06%] [G loss: 3.857862]\n",
      "epoch:47 step:37372 [D loss: 0.347396, acc.: 80.47%] [G loss: 4.341436]\n",
      "epoch:47 step:37373 [D loss: 0.362199, acc.: 84.38%] [G loss: 4.725204]\n",
      "epoch:47 step:37374 [D loss: 0.280136, acc.: 85.94%] [G loss: 4.939777]\n",
      "epoch:47 step:37375 [D loss: 0.282109, acc.: 85.94%] [G loss: 4.112316]\n",
      "epoch:47 step:37376 [D loss: 0.296594, acc.: 89.84%] [G loss: 3.689349]\n",
      "epoch:47 step:37377 [D loss: 0.333377, acc.: 85.16%] [G loss: 2.969932]\n",
      "epoch:47 step:37378 [D loss: 0.250063, acc.: 91.41%] [G loss: 2.845653]\n",
      "epoch:47 step:37379 [D loss: 0.386685, acc.: 77.34%] [G loss: 3.193368]\n",
      "epoch:47 step:37380 [D loss: 0.410133, acc.: 85.16%] [G loss: 3.307907]\n",
      "epoch:47 step:37381 [D loss: 0.259627, acc.: 87.50%] [G loss: 3.406524]\n",
      "epoch:47 step:37382 [D loss: 0.324559, acc.: 85.94%] [G loss: 5.364091]\n",
      "epoch:47 step:37383 [D loss: 0.353017, acc.: 82.03%] [G loss: 3.307804]\n",
      "epoch:47 step:37384 [D loss: 0.373402, acc.: 84.38%] [G loss: 4.948291]\n",
      "epoch:47 step:37385 [D loss: 0.304452, acc.: 86.72%] [G loss: 3.857580]\n",
      "epoch:47 step:37386 [D loss: 0.339224, acc.: 85.94%] [G loss: 3.148711]\n",
      "epoch:47 step:37387 [D loss: 0.344098, acc.: 83.59%] [G loss: 3.807251]\n",
      "epoch:47 step:37388 [D loss: 0.327635, acc.: 89.84%] [G loss: 2.868838]\n",
      "epoch:47 step:37389 [D loss: 0.381474, acc.: 85.94%] [G loss: 3.896819]\n",
      "epoch:47 step:37390 [D loss: 0.289100, acc.: 87.50%] [G loss: 3.392463]\n",
      "epoch:47 step:37391 [D loss: 0.269185, acc.: 87.50%] [G loss: 3.533492]\n",
      "epoch:47 step:37392 [D loss: 0.323499, acc.: 85.94%] [G loss: 3.315420]\n",
      "epoch:47 step:37393 [D loss: 0.293928, acc.: 87.50%] [G loss: 3.398810]\n",
      "epoch:47 step:37394 [D loss: 0.365939, acc.: 82.03%] [G loss: 4.000051]\n",
      "epoch:47 step:37395 [D loss: 0.330537, acc.: 87.50%] [G loss: 5.690083]\n",
      "epoch:47 step:37396 [D loss: 0.284555, acc.: 85.16%] [G loss: 4.911707]\n",
      "epoch:47 step:37397 [D loss: 0.217944, acc.: 89.06%] [G loss: 4.461948]\n",
      "epoch:47 step:37398 [D loss: 0.396583, acc.: 83.59%] [G loss: 4.053251]\n",
      "epoch:47 step:37399 [D loss: 0.381682, acc.: 83.59%] [G loss: 3.610628]\n",
      "epoch:47 step:37400 [D loss: 0.284001, acc.: 85.94%] [G loss: 5.023627]\n",
      "##############\n",
      "[0.8663546  0.86490404 0.81186765 0.81297737 0.78745697 0.81793821\n",
      " 0.87193794 0.82975823 0.81899037 0.81205675]\n",
      "##########\n",
      "epoch:47 step:37401 [D loss: 0.277376, acc.: 89.84%] [G loss: 4.647250]\n",
      "epoch:47 step:37402 [D loss: 0.269805, acc.: 89.06%] [G loss: 3.123533]\n",
      "epoch:47 step:37403 [D loss: 0.277980, acc.: 88.28%] [G loss: 3.914571]\n",
      "epoch:47 step:37404 [D loss: 0.253420, acc.: 87.50%] [G loss: 3.502840]\n",
      "epoch:47 step:37405 [D loss: 0.391452, acc.: 83.59%] [G loss: 4.701491]\n",
      "epoch:47 step:37406 [D loss: 0.182670, acc.: 92.19%] [G loss: 7.100629]\n",
      "epoch:47 step:37407 [D loss: 0.207887, acc.: 92.19%] [G loss: 5.291971]\n",
      "epoch:47 step:37408 [D loss: 0.274221, acc.: 90.62%] [G loss: 5.974174]\n",
      "epoch:47 step:37409 [D loss: 0.204837, acc.: 92.97%] [G loss: 4.748549]\n",
      "epoch:47 step:37410 [D loss: 0.256475, acc.: 90.62%] [G loss: 4.634611]\n",
      "epoch:47 step:37411 [D loss: 0.266568, acc.: 89.84%] [G loss: 3.114388]\n",
      "epoch:47 step:37412 [D loss: 0.249447, acc.: 89.84%] [G loss: 4.637808]\n",
      "epoch:47 step:37413 [D loss: 0.286005, acc.: 89.84%] [G loss: 4.874826]\n",
      "epoch:47 step:37414 [D loss: 0.397062, acc.: 81.25%] [G loss: 3.450284]\n",
      "epoch:47 step:37415 [D loss: 0.293416, acc.: 84.38%] [G loss: 3.978739]\n",
      "epoch:47 step:37416 [D loss: 0.419707, acc.: 82.03%] [G loss: 2.909666]\n",
      "epoch:47 step:37417 [D loss: 0.328066, acc.: 86.72%] [G loss: 3.322483]\n",
      "epoch:47 step:37418 [D loss: 0.260145, acc.: 88.28%] [G loss: 3.740126]\n",
      "epoch:47 step:37419 [D loss: 0.312856, acc.: 89.84%] [G loss: 3.823170]\n",
      "epoch:47 step:37420 [D loss: 0.319860, acc.: 81.25%] [G loss: 2.669357]\n",
      "epoch:47 step:37421 [D loss: 0.426998, acc.: 82.81%] [G loss: 3.308932]\n",
      "epoch:47 step:37422 [D loss: 0.252762, acc.: 85.16%] [G loss: 3.566303]\n",
      "epoch:47 step:37423 [D loss: 0.326443, acc.: 87.50%] [G loss: 4.566300]\n",
      "epoch:47 step:37424 [D loss: 0.315418, acc.: 86.72%] [G loss: 5.562654]\n",
      "epoch:47 step:37425 [D loss: 0.242400, acc.: 88.28%] [G loss: 5.817090]\n",
      "epoch:47 step:37426 [D loss: 0.172722, acc.: 93.75%] [G loss: 7.282623]\n",
      "epoch:47 step:37427 [D loss: 0.226914, acc.: 91.41%] [G loss: 6.837831]\n",
      "epoch:47 step:37428 [D loss: 0.195273, acc.: 93.75%] [G loss: 5.285230]\n",
      "epoch:47 step:37429 [D loss: 0.161489, acc.: 92.97%] [G loss: 4.982744]\n",
      "epoch:47 step:37430 [D loss: 0.178967, acc.: 94.53%] [G loss: 3.660495]\n",
      "epoch:47 step:37431 [D loss: 0.233926, acc.: 90.62%] [G loss: 3.621779]\n",
      "epoch:47 step:37432 [D loss: 0.198940, acc.: 92.19%] [G loss: 3.776086]\n",
      "epoch:47 step:37433 [D loss: 0.312102, acc.: 85.16%] [G loss: 3.013352]\n",
      "epoch:47 step:37434 [D loss: 0.269090, acc.: 85.16%] [G loss: 2.835622]\n",
      "epoch:47 step:37435 [D loss: 0.327014, acc.: 86.72%] [G loss: 3.223163]\n",
      "epoch:47 step:37436 [D loss: 0.337341, acc.: 88.28%] [G loss: 4.466171]\n",
      "epoch:47 step:37437 [D loss: 0.307800, acc.: 83.59%] [G loss: 3.518872]\n",
      "epoch:47 step:37438 [D loss: 0.501398, acc.: 82.03%] [G loss: 2.809659]\n",
      "epoch:47 step:37439 [D loss: 0.353325, acc.: 82.03%] [G loss: 3.406022]\n",
      "epoch:47 step:37440 [D loss: 0.313324, acc.: 87.50%] [G loss: 2.954901]\n",
      "epoch:47 step:37441 [D loss: 0.256797, acc.: 86.72%] [G loss: 4.032550]\n",
      "epoch:47 step:37442 [D loss: 0.294737, acc.: 87.50%] [G loss: 3.381193]\n",
      "epoch:47 step:37443 [D loss: 0.246917, acc.: 90.62%] [G loss: 2.988185]\n",
      "epoch:47 step:37444 [D loss: 0.302969, acc.: 87.50%] [G loss: 3.613716]\n",
      "epoch:47 step:37445 [D loss: 0.417480, acc.: 82.81%] [G loss: 3.239114]\n",
      "epoch:47 step:37446 [D loss: 0.302633, acc.: 83.59%] [G loss: 4.151596]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:47 step:37447 [D loss: 0.346976, acc.: 81.25%] [G loss: 6.049321]\n",
      "epoch:47 step:37448 [D loss: 0.265070, acc.: 85.16%] [G loss: 4.246575]\n",
      "epoch:47 step:37449 [D loss: 0.340371, acc.: 82.03%] [G loss: 4.885318]\n",
      "epoch:47 step:37450 [D loss: 0.418746, acc.: 82.03%] [G loss: 3.328992]\n",
      "epoch:47 step:37451 [D loss: 0.356779, acc.: 84.38%] [G loss: 3.348552]\n",
      "epoch:47 step:37452 [D loss: 0.316424, acc.: 86.72%] [G loss: 3.691963]\n",
      "epoch:47 step:37453 [D loss: 0.285916, acc.: 87.50%] [G loss: 2.932876]\n",
      "epoch:47 step:37454 [D loss: 0.323553, acc.: 86.72%] [G loss: 3.859004]\n",
      "epoch:47 step:37455 [D loss: 0.294600, acc.: 84.38%] [G loss: 2.905583]\n",
      "epoch:47 step:37456 [D loss: 0.272103, acc.: 90.62%] [G loss: 3.652358]\n",
      "epoch:47 step:37457 [D loss: 0.330205, acc.: 82.81%] [G loss: 2.610036]\n",
      "epoch:47 step:37458 [D loss: 0.353240, acc.: 84.38%] [G loss: 3.705585]\n",
      "epoch:47 step:37459 [D loss: 0.343970, acc.: 85.16%] [G loss: 3.108949]\n",
      "epoch:47 step:37460 [D loss: 0.371877, acc.: 83.59%] [G loss: 3.360852]\n",
      "epoch:47 step:37461 [D loss: 0.356527, acc.: 83.59%] [G loss: 2.606498]\n",
      "epoch:47 step:37462 [D loss: 0.265112, acc.: 90.62%] [G loss: 2.890371]\n",
      "epoch:47 step:37463 [D loss: 0.292741, acc.: 86.72%] [G loss: 4.790591]\n",
      "epoch:47 step:37464 [D loss: 0.452685, acc.: 78.91%] [G loss: 4.907279]\n",
      "epoch:47 step:37465 [D loss: 0.406702, acc.: 80.47%] [G loss: 4.724540]\n",
      "epoch:47 step:37466 [D loss: 0.327294, acc.: 82.81%] [G loss: 4.836829]\n",
      "epoch:47 step:37467 [D loss: 0.460568, acc.: 81.25%] [G loss: 3.997260]\n",
      "epoch:47 step:37468 [D loss: 0.223844, acc.: 92.19%] [G loss: 3.442826]\n",
      "epoch:47 step:37469 [D loss: 0.421315, acc.: 79.69%] [G loss: 4.200704]\n",
      "epoch:47 step:37470 [D loss: 0.273519, acc.: 87.50%] [G loss: 3.018373]\n",
      "epoch:47 step:37471 [D loss: 0.288402, acc.: 85.16%] [G loss: 2.541853]\n",
      "epoch:47 step:37472 [D loss: 0.451990, acc.: 76.56%] [G loss: 3.743596]\n",
      "epoch:47 step:37473 [D loss: 0.222903, acc.: 91.41%] [G loss: 2.826642]\n",
      "epoch:47 step:37474 [D loss: 0.228159, acc.: 89.84%] [G loss: 3.578146]\n",
      "epoch:47 step:37475 [D loss: 0.371591, acc.: 80.47%] [G loss: 3.072280]\n",
      "epoch:47 step:37476 [D loss: 0.230984, acc.: 90.62%] [G loss: 4.146384]\n",
      "epoch:47 step:37477 [D loss: 0.280414, acc.: 89.06%] [G loss: 3.327606]\n",
      "epoch:47 step:37478 [D loss: 0.275767, acc.: 91.41%] [G loss: 3.617339]\n",
      "epoch:47 step:37479 [D loss: 0.466477, acc.: 82.81%] [G loss: 6.254641]\n",
      "epoch:47 step:37480 [D loss: 0.582069, acc.: 81.25%] [G loss: 8.015064]\n",
      "epoch:47 step:37481 [D loss: 2.317026, acc.: 53.91%] [G loss: 9.063283]\n",
      "epoch:47 step:37482 [D loss: 2.897273, acc.: 50.00%] [G loss: 4.530669]\n",
      "epoch:47 step:37483 [D loss: 1.189807, acc.: 77.34%] [G loss: 5.748473]\n",
      "epoch:47 step:37484 [D loss: 1.423068, acc.: 60.94%] [G loss: 4.236838]\n",
      "epoch:47 step:37485 [D loss: 0.638079, acc.: 78.91%] [G loss: 4.071472]\n",
      "epoch:47 step:37486 [D loss: 0.431908, acc.: 83.59%] [G loss: 4.304764]\n",
      "epoch:47 step:37487 [D loss: 0.303225, acc.: 87.50%] [G loss: 3.421832]\n",
      "epoch:47 step:37488 [D loss: 0.309964, acc.: 85.94%] [G loss: 3.840893]\n",
      "epoch:48 step:37489 [D loss: 0.347241, acc.: 84.38%] [G loss: 2.973596]\n",
      "epoch:48 step:37490 [D loss: 0.194910, acc.: 89.84%] [G loss: 3.706235]\n",
      "epoch:48 step:37491 [D loss: 0.459078, acc.: 80.47%] [G loss: 2.631737]\n",
      "epoch:48 step:37492 [D loss: 0.261073, acc.: 87.50%] [G loss: 3.465196]\n",
      "epoch:48 step:37493 [D loss: 0.328230, acc.: 85.16%] [G loss: 3.085057]\n",
      "epoch:48 step:37494 [D loss: 0.341885, acc.: 87.50%] [G loss: 2.445334]\n",
      "epoch:48 step:37495 [D loss: 0.316190, acc.: 91.41%] [G loss: 2.695974]\n",
      "epoch:48 step:37496 [D loss: 0.365621, acc.: 81.25%] [G loss: 3.038766]\n",
      "epoch:48 step:37497 [D loss: 0.356176, acc.: 84.38%] [G loss: 2.910758]\n",
      "epoch:48 step:37498 [D loss: 0.406662, acc.: 78.91%] [G loss: 2.472846]\n",
      "epoch:48 step:37499 [D loss: 0.285823, acc.: 88.28%] [G loss: 2.412470]\n",
      "epoch:48 step:37500 [D loss: 0.268652, acc.: 89.84%] [G loss: 2.294383]\n",
      "epoch:48 step:37501 [D loss: 0.310286, acc.: 86.72%] [G loss: 2.796565]\n",
      "epoch:48 step:37502 [D loss: 0.268703, acc.: 84.38%] [G loss: 3.132784]\n",
      "epoch:48 step:37503 [D loss: 0.405297, acc.: 82.03%] [G loss: 3.868867]\n",
      "epoch:48 step:37504 [D loss: 0.214315, acc.: 94.53%] [G loss: 3.302085]\n",
      "epoch:48 step:37505 [D loss: 0.264353, acc.: 86.72%] [G loss: 3.060536]\n",
      "epoch:48 step:37506 [D loss: 0.337505, acc.: 85.16%] [G loss: 3.017589]\n",
      "epoch:48 step:37507 [D loss: 0.231973, acc.: 90.62%] [G loss: 2.632329]\n",
      "epoch:48 step:37508 [D loss: 0.302838, acc.: 87.50%] [G loss: 3.289811]\n",
      "epoch:48 step:37509 [D loss: 0.263749, acc.: 89.84%] [G loss: 3.265953]\n",
      "epoch:48 step:37510 [D loss: 0.320096, acc.: 89.06%] [G loss: 2.842285]\n",
      "epoch:48 step:37511 [D loss: 0.311304, acc.: 87.50%] [G loss: 2.579565]\n",
      "epoch:48 step:37512 [D loss: 0.321982, acc.: 84.38%] [G loss: 3.091594]\n",
      "epoch:48 step:37513 [D loss: 0.329839, acc.: 82.03%] [G loss: 3.009609]\n",
      "epoch:48 step:37514 [D loss: 0.271170, acc.: 86.72%] [G loss: 2.673289]\n",
      "epoch:48 step:37515 [D loss: 0.348025, acc.: 83.59%] [G loss: 2.294832]\n",
      "epoch:48 step:37516 [D loss: 0.205333, acc.: 93.75%] [G loss: 3.004460]\n",
      "epoch:48 step:37517 [D loss: 0.240715, acc.: 90.62%] [G loss: 2.511152]\n",
      "epoch:48 step:37518 [D loss: 0.291845, acc.: 89.06%] [G loss: 2.938728]\n",
      "epoch:48 step:37519 [D loss: 0.381786, acc.: 84.38%] [G loss: 2.973010]\n",
      "epoch:48 step:37520 [D loss: 0.342876, acc.: 83.59%] [G loss: 2.243378]\n",
      "epoch:48 step:37521 [D loss: 0.214510, acc.: 91.41%] [G loss: 2.804561]\n",
      "epoch:48 step:37522 [D loss: 0.296472, acc.: 86.72%] [G loss: 2.288846]\n",
      "epoch:48 step:37523 [D loss: 0.266433, acc.: 89.06%] [G loss: 2.803772]\n",
      "epoch:48 step:37524 [D loss: 0.332079, acc.: 82.81%] [G loss: 2.640202]\n",
      "epoch:48 step:37525 [D loss: 0.327078, acc.: 85.94%] [G loss: 3.409927]\n",
      "epoch:48 step:37526 [D loss: 0.255430, acc.: 89.84%] [G loss: 3.194114]\n",
      "epoch:48 step:37527 [D loss: 0.337121, acc.: 89.06%] [G loss: 3.413254]\n",
      "epoch:48 step:37528 [D loss: 0.327910, acc.: 85.16%] [G loss: 2.889099]\n",
      "epoch:48 step:37529 [D loss: 0.361846, acc.: 81.25%] [G loss: 3.696101]\n",
      "epoch:48 step:37530 [D loss: 0.261845, acc.: 89.06%] [G loss: 3.428812]\n",
      "epoch:48 step:37531 [D loss: 0.289392, acc.: 88.28%] [G loss: 2.070767]\n",
      "epoch:48 step:37532 [D loss: 0.255042, acc.: 93.75%] [G loss: 3.182478]\n",
      "epoch:48 step:37533 [D loss: 0.310893, acc.: 90.62%] [G loss: 4.034007]\n",
      "epoch:48 step:37534 [D loss: 0.278103, acc.: 87.50%] [G loss: 2.961196]\n",
      "epoch:48 step:37535 [D loss: 0.288152, acc.: 85.94%] [G loss: 3.484054]\n",
      "epoch:48 step:37536 [D loss: 0.235466, acc.: 91.41%] [G loss: 3.001934]\n",
      "epoch:48 step:37537 [D loss: 0.297034, acc.: 84.38%] [G loss: 3.335718]\n",
      "epoch:48 step:37538 [D loss: 0.236251, acc.: 89.84%] [G loss: 3.203843]\n",
      "epoch:48 step:37539 [D loss: 0.329388, acc.: 86.72%] [G loss: 2.729117]\n",
      "epoch:48 step:37540 [D loss: 0.193110, acc.: 92.19%] [G loss: 2.921452]\n",
      "epoch:48 step:37541 [D loss: 0.328759, acc.: 83.59%] [G loss: 2.920360]\n",
      "epoch:48 step:37542 [D loss: 0.279548, acc.: 87.50%] [G loss: 3.425085]\n",
      "epoch:48 step:37543 [D loss: 0.249225, acc.: 91.41%] [G loss: 3.613805]\n",
      "epoch:48 step:37544 [D loss: 0.314245, acc.: 83.59%] [G loss: 3.612923]\n",
      "epoch:48 step:37545 [D loss: 0.329438, acc.: 86.72%] [G loss: 2.952483]\n",
      "epoch:48 step:37546 [D loss: 0.254003, acc.: 89.84%] [G loss: 2.785940]\n",
      "epoch:48 step:37547 [D loss: 0.246208, acc.: 91.41%] [G loss: 2.431255]\n",
      "epoch:48 step:37548 [D loss: 0.297573, acc.: 87.50%] [G loss: 2.984820]\n",
      "epoch:48 step:37549 [D loss: 0.314959, acc.: 87.50%] [G loss: 2.736712]\n",
      "epoch:48 step:37550 [D loss: 0.300324, acc.: 86.72%] [G loss: 2.827107]\n",
      "epoch:48 step:37551 [D loss: 0.245402, acc.: 89.06%] [G loss: 2.932117]\n",
      "epoch:48 step:37552 [D loss: 0.399240, acc.: 82.81%] [G loss: 2.256786]\n",
      "epoch:48 step:37553 [D loss: 0.371090, acc.: 85.16%] [G loss: 3.000710]\n",
      "epoch:48 step:37554 [D loss: 0.324964, acc.: 87.50%] [G loss: 3.017009]\n",
      "epoch:48 step:37555 [D loss: 0.277562, acc.: 85.94%] [G loss: 3.783205]\n",
      "epoch:48 step:37556 [D loss: 0.373751, acc.: 81.25%] [G loss: 2.687179]\n",
      "epoch:48 step:37557 [D loss: 0.296101, acc.: 85.16%] [G loss: 2.875220]\n",
      "epoch:48 step:37558 [D loss: 0.346205, acc.: 82.81%] [G loss: 3.034731]\n",
      "epoch:48 step:37559 [D loss: 0.317025, acc.: 85.94%] [G loss: 3.357273]\n",
      "epoch:48 step:37560 [D loss: 0.398567, acc.: 85.94%] [G loss: 5.538025]\n",
      "epoch:48 step:37561 [D loss: 0.747058, acc.: 74.22%] [G loss: 4.178673]\n",
      "epoch:48 step:37562 [D loss: 0.857081, acc.: 64.06%] [G loss: 4.320540]\n",
      "epoch:48 step:37563 [D loss: 0.572838, acc.: 75.00%] [G loss: 3.176039]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:48 step:37564 [D loss: 0.410329, acc.: 82.03%] [G loss: 3.456664]\n",
      "epoch:48 step:37565 [D loss: 0.403286, acc.: 80.47%] [G loss: 2.746434]\n",
      "epoch:48 step:37566 [D loss: 0.373781, acc.: 82.81%] [G loss: 2.599286]\n",
      "epoch:48 step:37567 [D loss: 0.354823, acc.: 83.59%] [G loss: 3.058854]\n",
      "epoch:48 step:37568 [D loss: 0.266377, acc.: 85.94%] [G loss: 3.019410]\n",
      "epoch:48 step:37569 [D loss: 0.361441, acc.: 83.59%] [G loss: 3.528414]\n",
      "epoch:48 step:37570 [D loss: 0.313547, acc.: 83.59%] [G loss: 3.787325]\n",
      "epoch:48 step:37571 [D loss: 0.173125, acc.: 95.31%] [G loss: 3.144033]\n",
      "epoch:48 step:37572 [D loss: 0.246154, acc.: 90.62%] [G loss: 2.685051]\n",
      "epoch:48 step:37573 [D loss: 0.305842, acc.: 82.81%] [G loss: 2.723907]\n",
      "epoch:48 step:37574 [D loss: 0.230300, acc.: 90.62%] [G loss: 2.970134]\n",
      "epoch:48 step:37575 [D loss: 0.270249, acc.: 88.28%] [G loss: 2.410737]\n",
      "epoch:48 step:37576 [D loss: 0.326093, acc.: 85.94%] [G loss: 3.442722]\n",
      "epoch:48 step:37577 [D loss: 0.319296, acc.: 82.81%] [G loss: 3.170328]\n",
      "epoch:48 step:37578 [D loss: 0.393366, acc.: 82.03%] [G loss: 2.973017]\n",
      "epoch:48 step:37579 [D loss: 0.242948, acc.: 88.28%] [G loss: 2.876556]\n",
      "epoch:48 step:37580 [D loss: 0.434673, acc.: 79.69%] [G loss: 2.983645]\n",
      "epoch:48 step:37581 [D loss: 0.310506, acc.: 86.72%] [G loss: 2.891590]\n",
      "epoch:48 step:37582 [D loss: 0.231539, acc.: 91.41%] [G loss: 4.005873]\n",
      "epoch:48 step:37583 [D loss: 0.317676, acc.: 85.16%] [G loss: 3.296906]\n",
      "epoch:48 step:37584 [D loss: 0.460175, acc.: 80.47%] [G loss: 5.382480]\n",
      "epoch:48 step:37585 [D loss: 0.171749, acc.: 96.09%] [G loss: 4.394697]\n",
      "epoch:48 step:37586 [D loss: 0.305521, acc.: 86.72%] [G loss: 4.142255]\n",
      "epoch:48 step:37587 [D loss: 0.295112, acc.: 85.16%] [G loss: 3.878177]\n",
      "epoch:48 step:37588 [D loss: 0.297420, acc.: 85.94%] [G loss: 5.292654]\n",
      "epoch:48 step:37589 [D loss: 0.412617, acc.: 82.03%] [G loss: 3.683574]\n",
      "epoch:48 step:37590 [D loss: 0.232277, acc.: 89.06%] [G loss: 3.954012]\n",
      "epoch:48 step:37591 [D loss: 0.322007, acc.: 85.94%] [G loss: 2.735759]\n",
      "epoch:48 step:37592 [D loss: 0.249577, acc.: 89.06%] [G loss: 3.583099]\n",
      "epoch:48 step:37593 [D loss: 0.244049, acc.: 92.19%] [G loss: 3.435429]\n",
      "epoch:48 step:37594 [D loss: 0.260672, acc.: 89.84%] [G loss: 2.421363]\n",
      "epoch:48 step:37595 [D loss: 0.343442, acc.: 78.91%] [G loss: 4.173265]\n",
      "epoch:48 step:37596 [D loss: 0.281525, acc.: 88.28%] [G loss: 3.103135]\n",
      "epoch:48 step:37597 [D loss: 0.273997, acc.: 89.06%] [G loss: 3.502522]\n",
      "epoch:48 step:37598 [D loss: 0.396358, acc.: 78.91%] [G loss: 2.462834]\n",
      "epoch:48 step:37599 [D loss: 0.357268, acc.: 82.03%] [G loss: 3.123728]\n",
      "epoch:48 step:37600 [D loss: 0.186621, acc.: 92.97%] [G loss: 3.303639]\n",
      "##############\n",
      "[0.84664268 0.84821441 0.77997852 0.85010398 0.79835717 0.85437534\n",
      " 0.86580109 0.85594457 0.80424463 0.82932023]\n",
      "##########\n",
      "epoch:48 step:37601 [D loss: 0.337796, acc.: 85.94%] [G loss: 2.661819]\n",
      "epoch:48 step:37602 [D loss: 0.368910, acc.: 79.69%] [G loss: 2.736513]\n",
      "epoch:48 step:37603 [D loss: 0.291438, acc.: 87.50%] [G loss: 2.730108]\n",
      "epoch:48 step:37604 [D loss: 0.291395, acc.: 89.06%] [G loss: 2.640804]\n",
      "epoch:48 step:37605 [D loss: 0.200861, acc.: 92.19%] [G loss: 2.788872]\n",
      "epoch:48 step:37606 [D loss: 0.279929, acc.: 85.94%] [G loss: 3.168276]\n",
      "epoch:48 step:37607 [D loss: 0.285488, acc.: 88.28%] [G loss: 4.072222]\n",
      "epoch:48 step:37608 [D loss: 0.266785, acc.: 88.28%] [G loss: 3.263264]\n",
      "epoch:48 step:37609 [D loss: 0.295760, acc.: 88.28%] [G loss: 2.756367]\n",
      "epoch:48 step:37610 [D loss: 0.364258, acc.: 84.38%] [G loss: 3.356453]\n",
      "epoch:48 step:37611 [D loss: 0.376711, acc.: 82.81%] [G loss: 2.836352]\n",
      "epoch:48 step:37612 [D loss: 0.366784, acc.: 85.16%] [G loss: 3.658298]\n",
      "epoch:48 step:37613 [D loss: 0.354784, acc.: 84.38%] [G loss: 3.073603]\n",
      "epoch:48 step:37614 [D loss: 0.296902, acc.: 85.16%] [G loss: 3.361947]\n",
      "epoch:48 step:37615 [D loss: 0.293907, acc.: 87.50%] [G loss: 3.124395]\n",
      "epoch:48 step:37616 [D loss: 0.249793, acc.: 89.84%] [G loss: 3.292825]\n",
      "epoch:48 step:37617 [D loss: 0.213450, acc.: 92.97%] [G loss: 3.485594]\n",
      "epoch:48 step:37618 [D loss: 0.253711, acc.: 88.28%] [G loss: 3.729648]\n",
      "epoch:48 step:37619 [D loss: 0.386665, acc.: 81.25%] [G loss: 2.521590]\n",
      "epoch:48 step:37620 [D loss: 0.307099, acc.: 83.59%] [G loss: 2.856977]\n",
      "epoch:48 step:37621 [D loss: 0.252732, acc.: 91.41%] [G loss: 3.042229]\n",
      "epoch:48 step:37622 [D loss: 0.243163, acc.: 89.84%] [G loss: 3.885777]\n",
      "epoch:48 step:37623 [D loss: 0.345119, acc.: 83.59%] [G loss: 3.359999]\n",
      "epoch:48 step:37624 [D loss: 0.253793, acc.: 86.72%] [G loss: 3.455478]\n",
      "epoch:48 step:37625 [D loss: 0.209770, acc.: 92.19%] [G loss: 3.020457]\n",
      "epoch:48 step:37626 [D loss: 0.275066, acc.: 85.94%] [G loss: 2.809701]\n",
      "epoch:48 step:37627 [D loss: 0.283242, acc.: 89.84%] [G loss: 2.894269]\n",
      "epoch:48 step:37628 [D loss: 0.233610, acc.: 89.06%] [G loss: 2.994510]\n",
      "epoch:48 step:37629 [D loss: 0.251662, acc.: 89.84%] [G loss: 2.835196]\n",
      "epoch:48 step:37630 [D loss: 0.269259, acc.: 85.94%] [G loss: 3.850608]\n",
      "epoch:48 step:37631 [D loss: 0.312001, acc.: 84.38%] [G loss: 2.732921]\n",
      "epoch:48 step:37632 [D loss: 0.350644, acc.: 78.12%] [G loss: 2.797828]\n",
      "epoch:48 step:37633 [D loss: 0.260337, acc.: 89.84%] [G loss: 3.646177]\n",
      "epoch:48 step:37634 [D loss: 0.371856, acc.: 81.25%] [G loss: 2.655272]\n",
      "epoch:48 step:37635 [D loss: 0.235340, acc.: 89.84%] [G loss: 3.077786]\n",
      "epoch:48 step:37636 [D loss: 0.304661, acc.: 84.38%] [G loss: 3.046252]\n",
      "epoch:48 step:37637 [D loss: 0.340875, acc.: 85.16%] [G loss: 4.025124]\n",
      "epoch:48 step:37638 [D loss: 0.330688, acc.: 85.94%] [G loss: 3.509233]\n",
      "epoch:48 step:37639 [D loss: 0.313296, acc.: 86.72%] [G loss: 3.662644]\n",
      "epoch:48 step:37640 [D loss: 0.322708, acc.: 83.59%] [G loss: 3.882857]\n",
      "epoch:48 step:37641 [D loss: 0.294421, acc.: 86.72%] [G loss: 3.939332]\n",
      "epoch:48 step:37642 [D loss: 0.261196, acc.: 87.50%] [G loss: 4.775556]\n",
      "epoch:48 step:37643 [D loss: 0.312023, acc.: 85.16%] [G loss: 3.682145]\n",
      "epoch:48 step:37644 [D loss: 0.247564, acc.: 89.84%] [G loss: 3.068276]\n",
      "epoch:48 step:37645 [D loss: 0.338247, acc.: 83.59%] [G loss: 3.715554]\n",
      "epoch:48 step:37646 [D loss: 0.312939, acc.: 84.38%] [G loss: 2.898206]\n",
      "epoch:48 step:37647 [D loss: 0.343660, acc.: 86.72%] [G loss: 3.379427]\n",
      "epoch:48 step:37648 [D loss: 0.423591, acc.: 77.34%] [G loss: 3.226044]\n",
      "epoch:48 step:37649 [D loss: 0.323396, acc.: 85.16%] [G loss: 3.478670]\n",
      "epoch:48 step:37650 [D loss: 0.330636, acc.: 89.06%] [G loss: 3.265469]\n",
      "epoch:48 step:37651 [D loss: 0.455249, acc.: 78.12%] [G loss: 2.614717]\n",
      "epoch:48 step:37652 [D loss: 0.352965, acc.: 79.69%] [G loss: 5.470611]\n",
      "epoch:48 step:37653 [D loss: 0.394180, acc.: 80.47%] [G loss: 2.670101]\n",
      "epoch:48 step:37654 [D loss: 0.324395, acc.: 86.72%] [G loss: 3.217687]\n",
      "epoch:48 step:37655 [D loss: 0.325712, acc.: 83.59%] [G loss: 3.401622]\n",
      "epoch:48 step:37656 [D loss: 0.377422, acc.: 85.16%] [G loss: 2.657425]\n",
      "epoch:48 step:37657 [D loss: 0.371691, acc.: 79.69%] [G loss: 3.470902]\n",
      "epoch:48 step:37658 [D loss: 0.264042, acc.: 88.28%] [G loss: 3.350997]\n",
      "epoch:48 step:37659 [D loss: 0.437008, acc.: 78.91%] [G loss: 2.995948]\n",
      "epoch:48 step:37660 [D loss: 0.353664, acc.: 87.50%] [G loss: 2.912214]\n",
      "epoch:48 step:37661 [D loss: 0.329655, acc.: 82.03%] [G loss: 2.957854]\n",
      "epoch:48 step:37662 [D loss: 0.427383, acc.: 82.03%] [G loss: 3.823744]\n",
      "epoch:48 step:37663 [D loss: 0.594034, acc.: 71.88%] [G loss: 3.329839]\n",
      "epoch:48 step:37664 [D loss: 0.292510, acc.: 89.84%] [G loss: 3.162788]\n",
      "epoch:48 step:37665 [D loss: 0.271799, acc.: 88.28%] [G loss: 3.182592]\n",
      "epoch:48 step:37666 [D loss: 0.302629, acc.: 85.94%] [G loss: 2.320208]\n",
      "epoch:48 step:37667 [D loss: 0.360840, acc.: 82.03%] [G loss: 3.732205]\n",
      "epoch:48 step:37668 [D loss: 0.418105, acc.: 84.38%] [G loss: 2.871933]\n",
      "epoch:48 step:37669 [D loss: 0.279973, acc.: 89.06%] [G loss: 3.685802]\n",
      "epoch:48 step:37670 [D loss: 0.301612, acc.: 85.94%] [G loss: 3.253178]\n",
      "epoch:48 step:37671 [D loss: 0.254602, acc.: 87.50%] [G loss: 2.347443]\n",
      "epoch:48 step:37672 [D loss: 0.418126, acc.: 78.91%] [G loss: 2.958216]\n",
      "epoch:48 step:37673 [D loss: 0.251187, acc.: 88.28%] [G loss: 3.030627]\n",
      "epoch:48 step:37674 [D loss: 0.228068, acc.: 93.75%] [G loss: 3.495935]\n",
      "epoch:48 step:37675 [D loss: 0.236217, acc.: 89.84%] [G loss: 3.555778]\n",
      "epoch:48 step:37676 [D loss: 0.292384, acc.: 90.62%] [G loss: 3.011771]\n",
      "epoch:48 step:37677 [D loss: 0.356070, acc.: 84.38%] [G loss: 2.922583]\n",
      "epoch:48 step:37678 [D loss: 0.279825, acc.: 90.62%] [G loss: 4.891866]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:48 step:37679 [D loss: 0.372597, acc.: 83.59%] [G loss: 3.596487]\n",
      "epoch:48 step:37680 [D loss: 0.252049, acc.: 90.62%] [G loss: 3.271536]\n",
      "epoch:48 step:37681 [D loss: 0.299838, acc.: 85.16%] [G loss: 3.081713]\n",
      "epoch:48 step:37682 [D loss: 0.266151, acc.: 89.84%] [G loss: 2.575693]\n",
      "epoch:48 step:37683 [D loss: 0.289886, acc.: 85.94%] [G loss: 2.999135]\n",
      "epoch:48 step:37684 [D loss: 0.250437, acc.: 89.84%] [G loss: 3.506718]\n",
      "epoch:48 step:37685 [D loss: 0.365822, acc.: 81.25%] [G loss: 5.412905]\n",
      "epoch:48 step:37686 [D loss: 0.472679, acc.: 82.03%] [G loss: 4.428265]\n",
      "epoch:48 step:37687 [D loss: 0.383078, acc.: 82.03%] [G loss: 2.751930]\n",
      "epoch:48 step:37688 [D loss: 0.258774, acc.: 87.50%] [G loss: 3.946789]\n",
      "epoch:48 step:37689 [D loss: 0.309708, acc.: 84.38%] [G loss: 3.665624]\n",
      "epoch:48 step:37690 [D loss: 0.354629, acc.: 82.81%] [G loss: 3.677191]\n",
      "epoch:48 step:37691 [D loss: 0.193266, acc.: 91.41%] [G loss: 3.631802]\n",
      "epoch:48 step:37692 [D loss: 0.297237, acc.: 87.50%] [G loss: 3.846504]\n",
      "epoch:48 step:37693 [D loss: 0.281119, acc.: 85.16%] [G loss: 3.569557]\n",
      "epoch:48 step:37694 [D loss: 0.315117, acc.: 83.59%] [G loss: 4.267003]\n",
      "epoch:48 step:37695 [D loss: 0.291442, acc.: 86.72%] [G loss: 3.611583]\n",
      "epoch:48 step:37696 [D loss: 0.499268, acc.: 75.78%] [G loss: 3.931658]\n",
      "epoch:48 step:37697 [D loss: 0.412909, acc.: 81.25%] [G loss: 4.986939]\n",
      "epoch:48 step:37698 [D loss: 0.394644, acc.: 82.03%] [G loss: 3.253706]\n",
      "epoch:48 step:37699 [D loss: 0.282822, acc.: 87.50%] [G loss: 6.163704]\n",
      "epoch:48 step:37700 [D loss: 0.410426, acc.: 85.94%] [G loss: 4.553851]\n",
      "epoch:48 step:37701 [D loss: 0.262759, acc.: 89.06%] [G loss: 5.082290]\n",
      "epoch:48 step:37702 [D loss: 0.727647, acc.: 67.97%] [G loss: 3.730061]\n",
      "epoch:48 step:37703 [D loss: 0.294376, acc.: 87.50%] [G loss: 3.797853]\n",
      "epoch:48 step:37704 [D loss: 0.284684, acc.: 84.38%] [G loss: 3.320051]\n",
      "epoch:48 step:37705 [D loss: 0.346618, acc.: 85.94%] [G loss: 3.740225]\n",
      "epoch:48 step:37706 [D loss: 0.255518, acc.: 91.41%] [G loss: 3.991609]\n",
      "epoch:48 step:37707 [D loss: 0.243043, acc.: 89.84%] [G loss: 3.752167]\n",
      "epoch:48 step:37708 [D loss: 0.241700, acc.: 90.62%] [G loss: 4.233435]\n",
      "epoch:48 step:37709 [D loss: 0.254039, acc.: 91.41%] [G loss: 2.806561]\n",
      "epoch:48 step:37710 [D loss: 0.316926, acc.: 85.94%] [G loss: 3.990435]\n",
      "epoch:48 step:37711 [D loss: 0.322564, acc.: 85.16%] [G loss: 2.530716]\n",
      "epoch:48 step:37712 [D loss: 0.302424, acc.: 83.59%] [G loss: 3.180095]\n",
      "epoch:48 step:37713 [D loss: 0.251363, acc.: 91.41%] [G loss: 3.074342]\n",
      "epoch:48 step:37714 [D loss: 0.231505, acc.: 92.19%] [G loss: 3.233761]\n",
      "epoch:48 step:37715 [D loss: 0.303409, acc.: 87.50%] [G loss: 3.711408]\n",
      "epoch:48 step:37716 [D loss: 0.284923, acc.: 86.72%] [G loss: 3.213759]\n",
      "epoch:48 step:37717 [D loss: 0.299847, acc.: 88.28%] [G loss: 3.037209]\n",
      "epoch:48 step:37718 [D loss: 0.372768, acc.: 81.25%] [G loss: 3.486779]\n",
      "epoch:48 step:37719 [D loss: 0.291413, acc.: 85.94%] [G loss: 3.310589]\n",
      "epoch:48 step:37720 [D loss: 0.237745, acc.: 89.84%] [G loss: 3.143245]\n",
      "epoch:48 step:37721 [D loss: 0.299585, acc.: 89.06%] [G loss: 2.281006]\n",
      "epoch:48 step:37722 [D loss: 0.336429, acc.: 82.03%] [G loss: 2.911914]\n",
      "epoch:48 step:37723 [D loss: 0.296891, acc.: 89.06%] [G loss: 2.948023]\n",
      "epoch:48 step:37724 [D loss: 0.336312, acc.: 85.16%] [G loss: 2.106389]\n",
      "epoch:48 step:37725 [D loss: 0.279612, acc.: 85.94%] [G loss: 3.091139]\n",
      "epoch:48 step:37726 [D loss: 0.361269, acc.: 82.03%] [G loss: 3.282370]\n",
      "epoch:48 step:37727 [D loss: 0.322608, acc.: 85.94%] [G loss: 4.026318]\n",
      "epoch:48 step:37728 [D loss: 0.340518, acc.: 81.25%] [G loss: 2.908262]\n",
      "epoch:48 step:37729 [D loss: 0.304343, acc.: 85.16%] [G loss: 4.293703]\n",
      "epoch:48 step:37730 [D loss: 0.308596, acc.: 85.94%] [G loss: 2.949034]\n",
      "epoch:48 step:37731 [D loss: 0.284225, acc.: 89.06%] [G loss: 2.670658]\n",
      "epoch:48 step:37732 [D loss: 0.274822, acc.: 87.50%] [G loss: 3.269393]\n",
      "epoch:48 step:37733 [D loss: 0.295767, acc.: 85.94%] [G loss: 3.326711]\n",
      "epoch:48 step:37734 [D loss: 0.294122, acc.: 89.06%] [G loss: 3.066189]\n",
      "epoch:48 step:37735 [D loss: 0.365550, acc.: 84.38%] [G loss: 2.565886]\n",
      "epoch:48 step:37736 [D loss: 0.277707, acc.: 88.28%] [G loss: 3.550425]\n",
      "epoch:48 step:37737 [D loss: 0.281974, acc.: 86.72%] [G loss: 3.244859]\n",
      "epoch:48 step:37738 [D loss: 0.271708, acc.: 86.72%] [G loss: 3.762156]\n",
      "epoch:48 step:37739 [D loss: 0.235780, acc.: 90.62%] [G loss: 3.701367]\n",
      "epoch:48 step:37740 [D loss: 0.294743, acc.: 85.16%] [G loss: 2.724850]\n",
      "epoch:48 step:37741 [D loss: 0.285116, acc.: 88.28%] [G loss: 2.943722]\n",
      "epoch:48 step:37742 [D loss: 0.263227, acc.: 90.62%] [G loss: 2.923898]\n",
      "epoch:48 step:37743 [D loss: 0.413071, acc.: 79.69%] [G loss: 2.521927]\n",
      "epoch:48 step:37744 [D loss: 0.293559, acc.: 85.16%] [G loss: 2.982585]\n",
      "epoch:48 step:37745 [D loss: 0.396282, acc.: 80.47%] [G loss: 2.253311]\n",
      "epoch:48 step:37746 [D loss: 0.268824, acc.: 92.19%] [G loss: 2.831792]\n",
      "epoch:48 step:37747 [D loss: 0.303963, acc.: 86.72%] [G loss: 2.644896]\n",
      "epoch:48 step:37748 [D loss: 0.368678, acc.: 85.94%] [G loss: 2.719229]\n",
      "epoch:48 step:37749 [D loss: 0.323814, acc.: 83.59%] [G loss: 2.969996]\n",
      "epoch:48 step:37750 [D loss: 0.475381, acc.: 75.78%] [G loss: 2.705099]\n",
      "epoch:48 step:37751 [D loss: 0.307586, acc.: 85.94%] [G loss: 5.289877]\n",
      "epoch:48 step:37752 [D loss: 0.209975, acc.: 89.06%] [G loss: 4.577409]\n",
      "epoch:48 step:37753 [D loss: 0.307281, acc.: 85.94%] [G loss: 2.886149]\n",
      "epoch:48 step:37754 [D loss: 0.355508, acc.: 81.25%] [G loss: 3.979457]\n",
      "epoch:48 step:37755 [D loss: 0.310318, acc.: 85.16%] [G loss: 3.507156]\n",
      "epoch:48 step:37756 [D loss: 0.258029, acc.: 88.28%] [G loss: 3.138747]\n",
      "epoch:48 step:37757 [D loss: 0.388931, acc.: 78.91%] [G loss: 3.254453]\n",
      "epoch:48 step:37758 [D loss: 0.381330, acc.: 83.59%] [G loss: 3.751610]\n",
      "epoch:48 step:37759 [D loss: 0.325384, acc.: 82.81%] [G loss: 3.380688]\n",
      "epoch:48 step:37760 [D loss: 0.318374, acc.: 87.50%] [G loss: 3.714793]\n",
      "epoch:48 step:37761 [D loss: 0.199315, acc.: 91.41%] [G loss: 3.376338]\n",
      "epoch:48 step:37762 [D loss: 0.285818, acc.: 88.28%] [G loss: 4.150795]\n",
      "epoch:48 step:37763 [D loss: 0.287777, acc.: 86.72%] [G loss: 3.743979]\n",
      "epoch:48 step:37764 [D loss: 0.372498, acc.: 83.59%] [G loss: 2.882267]\n",
      "epoch:48 step:37765 [D loss: 0.424979, acc.: 78.91%] [G loss: 2.785617]\n",
      "epoch:48 step:37766 [D loss: 0.287291, acc.: 90.62%] [G loss: 2.808873]\n",
      "epoch:48 step:37767 [D loss: 0.284847, acc.: 88.28%] [G loss: 2.415092]\n",
      "epoch:48 step:37768 [D loss: 0.289388, acc.: 88.28%] [G loss: 3.859043]\n",
      "epoch:48 step:37769 [D loss: 0.302315, acc.: 87.50%] [G loss: 3.430574]\n",
      "epoch:48 step:37770 [D loss: 0.300832, acc.: 83.59%] [G loss: 3.307494]\n",
      "epoch:48 step:37771 [D loss: 0.261684, acc.: 89.06%] [G loss: 3.691176]\n",
      "epoch:48 step:37772 [D loss: 0.280838, acc.: 86.72%] [G loss: 3.256841]\n",
      "epoch:48 step:37773 [D loss: 0.345061, acc.: 82.81%] [G loss: 3.542059]\n",
      "epoch:48 step:37774 [D loss: 0.343451, acc.: 86.72%] [G loss: 3.235410]\n",
      "epoch:48 step:37775 [D loss: 0.297214, acc.: 86.72%] [G loss: 3.962766]\n",
      "epoch:48 step:37776 [D loss: 0.313141, acc.: 87.50%] [G loss: 3.656436]\n",
      "epoch:48 step:37777 [D loss: 0.309346, acc.: 88.28%] [G loss: 3.440889]\n",
      "epoch:48 step:37778 [D loss: 0.245921, acc.: 89.06%] [G loss: 4.194235]\n",
      "epoch:48 step:37779 [D loss: 0.281059, acc.: 87.50%] [G loss: 2.701253]\n",
      "epoch:48 step:37780 [D loss: 0.255348, acc.: 89.84%] [G loss: 2.733733]\n",
      "epoch:48 step:37781 [D loss: 0.341093, acc.: 85.16%] [G loss: 2.555881]\n",
      "epoch:48 step:37782 [D loss: 0.297632, acc.: 87.50%] [G loss: 3.008790]\n",
      "epoch:48 step:37783 [D loss: 0.335415, acc.: 83.59%] [G loss: 2.730098]\n",
      "epoch:48 step:37784 [D loss: 0.413410, acc.: 80.47%] [G loss: 2.836120]\n",
      "epoch:48 step:37785 [D loss: 0.326198, acc.: 86.72%] [G loss: 2.896236]\n",
      "epoch:48 step:37786 [D loss: 0.361035, acc.: 85.16%] [G loss: 3.431963]\n",
      "epoch:48 step:37787 [D loss: 0.407029, acc.: 81.25%] [G loss: 2.455286]\n",
      "epoch:48 step:37788 [D loss: 0.380499, acc.: 79.69%] [G loss: 3.516011]\n",
      "epoch:48 step:37789 [D loss: 0.372935, acc.: 82.81%] [G loss: 3.482525]\n",
      "epoch:48 step:37790 [D loss: 0.365830, acc.: 82.81%] [G loss: 2.366657]\n",
      "epoch:48 step:37791 [D loss: 0.286963, acc.: 86.72%] [G loss: 3.220018]\n",
      "epoch:48 step:37792 [D loss: 0.345068, acc.: 82.03%] [G loss: 2.866411]\n",
      "epoch:48 step:37793 [D loss: 0.315852, acc.: 87.50%] [G loss: 3.528719]\n",
      "epoch:48 step:37794 [D loss: 0.252558, acc.: 91.41%] [G loss: 3.278602]\n",
      "epoch:48 step:37795 [D loss: 0.329706, acc.: 85.94%] [G loss: 3.717955]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:48 step:37796 [D loss: 0.206431, acc.: 93.75%] [G loss: 4.265955]\n",
      "epoch:48 step:37797 [D loss: 0.278352, acc.: 87.50%] [G loss: 4.398773]\n",
      "epoch:48 step:37798 [D loss: 0.224301, acc.: 93.75%] [G loss: 3.958547]\n",
      "epoch:48 step:37799 [D loss: 0.280632, acc.: 86.72%] [G loss: 4.031129]\n",
      "epoch:48 step:37800 [D loss: 0.307801, acc.: 85.16%] [G loss: 3.292736]\n",
      "##############\n",
      "[0.87920777 0.85882462 0.81075713 0.82113267 0.76601329 0.84465713\n",
      " 0.86455569 0.82643908 0.81899318 0.83164053]\n",
      "##########\n",
      "epoch:48 step:37801 [D loss: 0.312304, acc.: 86.72%] [G loss: 4.219542]\n",
      "epoch:48 step:37802 [D loss: 0.329428, acc.: 84.38%] [G loss: 3.420969]\n",
      "epoch:48 step:37803 [D loss: 0.553035, acc.: 70.31%] [G loss: 3.040552]\n",
      "epoch:48 step:37804 [D loss: 0.249481, acc.: 89.06%] [G loss: 3.539782]\n",
      "epoch:48 step:37805 [D loss: 0.374741, acc.: 86.72%] [G loss: 2.706918]\n",
      "epoch:48 step:37806 [D loss: 0.275687, acc.: 89.06%] [G loss: 4.356972]\n",
      "epoch:48 step:37807 [D loss: 0.271964, acc.: 87.50%] [G loss: 3.207059]\n",
      "epoch:48 step:37808 [D loss: 0.311368, acc.: 87.50%] [G loss: 2.715394]\n",
      "epoch:48 step:37809 [D loss: 0.430793, acc.: 77.34%] [G loss: 2.484885]\n",
      "epoch:48 step:37810 [D loss: 0.244196, acc.: 91.41%] [G loss: 3.675799]\n",
      "epoch:48 step:37811 [D loss: 0.382303, acc.: 83.59%] [G loss: 3.842434]\n",
      "epoch:48 step:37812 [D loss: 0.337906, acc.: 84.38%] [G loss: 2.959601]\n",
      "epoch:48 step:37813 [D loss: 0.435322, acc.: 80.47%] [G loss: 2.716143]\n",
      "epoch:48 step:37814 [D loss: 0.343583, acc.: 81.25%] [G loss: 3.731081]\n",
      "epoch:48 step:37815 [D loss: 0.284542, acc.: 88.28%] [G loss: 4.943357]\n",
      "epoch:48 step:37816 [D loss: 0.338778, acc.: 85.94%] [G loss: 3.247680]\n",
      "epoch:48 step:37817 [D loss: 0.281077, acc.: 86.72%] [G loss: 3.507779]\n",
      "epoch:48 step:37818 [D loss: 0.268417, acc.: 86.72%] [G loss: 3.272918]\n",
      "epoch:48 step:37819 [D loss: 0.333476, acc.: 84.38%] [G loss: 2.589717]\n",
      "epoch:48 step:37820 [D loss: 0.356568, acc.: 84.38%] [G loss: 2.871818]\n",
      "epoch:48 step:37821 [D loss: 0.365694, acc.: 79.69%] [G loss: 3.501875]\n",
      "epoch:48 step:37822 [D loss: 0.274063, acc.: 89.06%] [G loss: 3.593568]\n",
      "epoch:48 step:37823 [D loss: 0.306667, acc.: 89.06%] [G loss: 4.254608]\n",
      "epoch:48 step:37824 [D loss: 0.250252, acc.: 90.62%] [G loss: 2.980192]\n",
      "epoch:48 step:37825 [D loss: 0.296524, acc.: 87.50%] [G loss: 3.423365]\n",
      "epoch:48 step:37826 [D loss: 0.354575, acc.: 83.59%] [G loss: 3.451423]\n",
      "epoch:48 step:37827 [D loss: 0.277904, acc.: 89.06%] [G loss: 3.225256]\n",
      "epoch:48 step:37828 [D loss: 0.322072, acc.: 83.59%] [G loss: 3.222561]\n",
      "epoch:48 step:37829 [D loss: 0.257737, acc.: 89.84%] [G loss: 3.593152]\n",
      "epoch:48 step:37830 [D loss: 0.300584, acc.: 88.28%] [G loss: 4.016262]\n",
      "epoch:48 step:37831 [D loss: 0.370617, acc.: 82.81%] [G loss: 2.780519]\n",
      "epoch:48 step:37832 [D loss: 0.291812, acc.: 87.50%] [G loss: 3.140572]\n",
      "epoch:48 step:37833 [D loss: 0.417156, acc.: 82.81%] [G loss: 3.583274]\n",
      "epoch:48 step:37834 [D loss: 0.433417, acc.: 79.69%] [G loss: 2.666444]\n",
      "epoch:48 step:37835 [D loss: 0.273456, acc.: 89.06%] [G loss: 3.221951]\n",
      "epoch:48 step:37836 [D loss: 0.227792, acc.: 90.62%] [G loss: 3.067533]\n",
      "epoch:48 step:37837 [D loss: 0.312285, acc.: 86.72%] [G loss: 3.246262]\n",
      "epoch:48 step:37838 [D loss: 0.302305, acc.: 88.28%] [G loss: 4.257289]\n",
      "epoch:48 step:37839 [D loss: 0.380340, acc.: 87.50%] [G loss: 6.436527]\n",
      "epoch:48 step:37840 [D loss: 0.541297, acc.: 75.00%] [G loss: 4.174598]\n",
      "epoch:48 step:37841 [D loss: 0.319573, acc.: 84.38%] [G loss: 3.309792]\n",
      "epoch:48 step:37842 [D loss: 0.335884, acc.: 85.94%] [G loss: 4.515971]\n",
      "epoch:48 step:37843 [D loss: 0.252794, acc.: 88.28%] [G loss: 3.948917]\n",
      "epoch:48 step:37844 [D loss: 0.217020, acc.: 89.84%] [G loss: 4.331952]\n",
      "epoch:48 step:37845 [D loss: 0.218686, acc.: 92.97%] [G loss: 4.371757]\n",
      "epoch:48 step:37846 [D loss: 0.311285, acc.: 86.72%] [G loss: 3.084461]\n",
      "epoch:48 step:37847 [D loss: 0.225419, acc.: 88.28%] [G loss: 2.967388]\n",
      "epoch:48 step:37848 [D loss: 0.389034, acc.: 78.12%] [G loss: 4.506260]\n",
      "epoch:48 step:37849 [D loss: 0.320469, acc.: 84.38%] [G loss: 3.313428]\n",
      "epoch:48 step:37850 [D loss: 0.249875, acc.: 88.28%] [G loss: 5.050777]\n",
      "epoch:48 step:37851 [D loss: 0.364958, acc.: 82.03%] [G loss: 5.760400]\n",
      "epoch:48 step:37852 [D loss: 0.287886, acc.: 86.72%] [G loss: 3.970324]\n",
      "epoch:48 step:37853 [D loss: 0.418186, acc.: 78.91%] [G loss: 3.802144]\n",
      "epoch:48 step:37854 [D loss: 0.278698, acc.: 90.62%] [G loss: 3.425399]\n",
      "epoch:48 step:37855 [D loss: 0.310760, acc.: 87.50%] [G loss: 3.078774]\n",
      "epoch:48 step:37856 [D loss: 0.364347, acc.: 82.81%] [G loss: 3.289754]\n",
      "epoch:48 step:37857 [D loss: 0.312715, acc.: 87.50%] [G loss: 2.665148]\n",
      "epoch:48 step:37858 [D loss: 0.369480, acc.: 82.81%] [G loss: 3.557060]\n",
      "epoch:48 step:37859 [D loss: 0.320907, acc.: 85.94%] [G loss: 3.085002]\n",
      "epoch:48 step:37860 [D loss: 0.301006, acc.: 88.28%] [G loss: 3.329129]\n",
      "epoch:48 step:37861 [D loss: 0.304725, acc.: 86.72%] [G loss: 2.650572]\n",
      "epoch:48 step:37862 [D loss: 0.331018, acc.: 85.16%] [G loss: 2.950838]\n",
      "epoch:48 step:37863 [D loss: 0.243685, acc.: 88.28%] [G loss: 2.722420]\n",
      "epoch:48 step:37864 [D loss: 0.256316, acc.: 92.19%] [G loss: 2.868914]\n",
      "epoch:48 step:37865 [D loss: 0.327239, acc.: 85.94%] [G loss: 2.361692]\n",
      "epoch:48 step:37866 [D loss: 0.314926, acc.: 86.72%] [G loss: 2.109027]\n",
      "epoch:48 step:37867 [D loss: 0.360197, acc.: 82.81%] [G loss: 2.896321]\n",
      "epoch:48 step:37868 [D loss: 0.357524, acc.: 83.59%] [G loss: 2.721612]\n",
      "epoch:48 step:37869 [D loss: 0.373625, acc.: 80.47%] [G loss: 3.206970]\n",
      "epoch:48 step:37870 [D loss: 0.258041, acc.: 91.41%] [G loss: 2.605005]\n",
      "epoch:48 step:37871 [D loss: 0.271200, acc.: 87.50%] [G loss: 3.185592]\n",
      "epoch:48 step:37872 [D loss: 0.280463, acc.: 87.50%] [G loss: 2.952612]\n",
      "epoch:48 step:37873 [D loss: 0.259666, acc.: 89.06%] [G loss: 2.914810]\n",
      "epoch:48 step:37874 [D loss: 0.332349, acc.: 87.50%] [G loss: 2.915316]\n",
      "epoch:48 step:37875 [D loss: 0.310589, acc.: 83.59%] [G loss: 2.433879]\n",
      "epoch:48 step:37876 [D loss: 0.273442, acc.: 87.50%] [G loss: 2.853834]\n",
      "epoch:48 step:37877 [D loss: 0.322471, acc.: 82.03%] [G loss: 2.925936]\n",
      "epoch:48 step:37878 [D loss: 0.307973, acc.: 87.50%] [G loss: 2.783310]\n",
      "epoch:48 step:37879 [D loss: 0.407511, acc.: 79.69%] [G loss: 3.082403]\n",
      "epoch:48 step:37880 [D loss: 0.282103, acc.: 89.06%] [G loss: 3.058472]\n",
      "epoch:48 step:37881 [D loss: 0.309836, acc.: 88.28%] [G loss: 2.706219]\n",
      "epoch:48 step:37882 [D loss: 0.335049, acc.: 85.94%] [G loss: 2.878292]\n",
      "epoch:48 step:37883 [D loss: 0.341822, acc.: 89.06%] [G loss: 2.544192]\n",
      "epoch:48 step:37884 [D loss: 0.359696, acc.: 84.38%] [G loss: 3.888942]\n",
      "epoch:48 step:37885 [D loss: 0.325932, acc.: 84.38%] [G loss: 3.485493]\n",
      "epoch:48 step:37886 [D loss: 0.486427, acc.: 79.69%] [G loss: 2.492791]\n",
      "epoch:48 step:37887 [D loss: 0.341790, acc.: 85.16%] [G loss: 3.005455]\n",
      "epoch:48 step:37888 [D loss: 0.286064, acc.: 87.50%] [G loss: 3.531895]\n",
      "epoch:48 step:37889 [D loss: 0.277694, acc.: 89.06%] [G loss: 3.063490]\n",
      "epoch:48 step:37890 [D loss: 0.228389, acc.: 90.62%] [G loss: 3.284542]\n",
      "epoch:48 step:37891 [D loss: 0.310604, acc.: 85.16%] [G loss: 2.554076]\n",
      "epoch:48 step:37892 [D loss: 0.254952, acc.: 90.62%] [G loss: 3.291528]\n",
      "epoch:48 step:37893 [D loss: 0.267692, acc.: 87.50%] [G loss: 3.153240]\n",
      "epoch:48 step:37894 [D loss: 0.336007, acc.: 84.38%] [G loss: 3.035155]\n",
      "epoch:48 step:37895 [D loss: 0.335930, acc.: 82.03%] [G loss: 3.415555]\n",
      "epoch:48 step:37896 [D loss: 0.250157, acc.: 91.41%] [G loss: 3.148254]\n",
      "epoch:48 step:37897 [D loss: 0.251286, acc.: 87.50%] [G loss: 2.759467]\n",
      "epoch:48 step:37898 [D loss: 0.244596, acc.: 90.62%] [G loss: 4.049479]\n",
      "epoch:48 step:37899 [D loss: 0.292285, acc.: 89.06%] [G loss: 2.994596]\n",
      "epoch:48 step:37900 [D loss: 0.282119, acc.: 90.62%] [G loss: 3.682662]\n",
      "epoch:48 step:37901 [D loss: 0.236179, acc.: 89.06%] [G loss: 3.716484]\n",
      "epoch:48 step:37902 [D loss: 0.213413, acc.: 90.62%] [G loss: 4.656705]\n",
      "epoch:48 step:37903 [D loss: 0.260550, acc.: 89.84%] [G loss: 3.617899]\n",
      "epoch:48 step:37904 [D loss: 0.217354, acc.: 92.97%] [G loss: 3.302538]\n",
      "epoch:48 step:37905 [D loss: 0.233370, acc.: 91.41%] [G loss: 3.472829]\n",
      "epoch:48 step:37906 [D loss: 0.350465, acc.: 83.59%] [G loss: 3.865619]\n",
      "epoch:48 step:37907 [D loss: 0.415507, acc.: 84.38%] [G loss: 2.871626]\n",
      "epoch:48 step:37908 [D loss: 0.362968, acc.: 82.81%] [G loss: 2.700372]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:48 step:37909 [D loss: 0.235195, acc.: 89.06%] [G loss: 3.952747]\n",
      "epoch:48 step:37910 [D loss: 0.190064, acc.: 92.19%] [G loss: 3.850453]\n",
      "epoch:48 step:37911 [D loss: 0.218429, acc.: 90.62%] [G loss: 3.465446]\n",
      "epoch:48 step:37912 [D loss: 0.222035, acc.: 92.19%] [G loss: 3.075773]\n",
      "epoch:48 step:37913 [D loss: 0.270130, acc.: 89.06%] [G loss: 3.392882]\n",
      "epoch:48 step:37914 [D loss: 0.245061, acc.: 92.19%] [G loss: 3.290079]\n",
      "epoch:48 step:37915 [D loss: 0.286641, acc.: 85.16%] [G loss: 2.700143]\n",
      "epoch:48 step:37916 [D loss: 0.285073, acc.: 89.84%] [G loss: 3.303116]\n",
      "epoch:48 step:37917 [D loss: 0.387170, acc.: 82.81%] [G loss: 3.291630]\n",
      "epoch:48 step:37918 [D loss: 0.324919, acc.: 80.47%] [G loss: 3.122527]\n",
      "epoch:48 step:37919 [D loss: 0.326996, acc.: 85.94%] [G loss: 4.203974]\n",
      "epoch:48 step:37920 [D loss: 0.365588, acc.: 84.38%] [G loss: 3.147958]\n",
      "epoch:48 step:37921 [D loss: 0.189291, acc.: 92.19%] [G loss: 4.144454]\n",
      "epoch:48 step:37922 [D loss: 0.222678, acc.: 90.62%] [G loss: 3.827846]\n",
      "epoch:48 step:37923 [D loss: 0.316937, acc.: 88.28%] [G loss: 4.620697]\n",
      "epoch:48 step:37924 [D loss: 0.273732, acc.: 88.28%] [G loss: 3.957737]\n",
      "epoch:48 step:37925 [D loss: 0.325119, acc.: 85.94%] [G loss: 3.279810]\n",
      "epoch:48 step:37926 [D loss: 0.295359, acc.: 87.50%] [G loss: 3.323521]\n",
      "epoch:48 step:37927 [D loss: 0.231998, acc.: 92.97%] [G loss: 4.017250]\n",
      "epoch:48 step:37928 [D loss: 0.302252, acc.: 85.16%] [G loss: 4.007572]\n",
      "epoch:48 step:37929 [D loss: 0.368016, acc.: 82.81%] [G loss: 3.481520]\n",
      "epoch:48 step:37930 [D loss: 0.254986, acc.: 89.84%] [G loss: 3.236060]\n",
      "epoch:48 step:37931 [D loss: 0.233966, acc.: 89.84%] [G loss: 3.875130]\n",
      "epoch:48 step:37932 [D loss: 0.351895, acc.: 84.38%] [G loss: 3.067281]\n",
      "epoch:48 step:37933 [D loss: 0.315661, acc.: 85.16%] [G loss: 3.068887]\n",
      "epoch:48 step:37934 [D loss: 0.269805, acc.: 89.84%] [G loss: 2.789950]\n",
      "epoch:48 step:37935 [D loss: 0.282315, acc.: 85.94%] [G loss: 2.886551]\n",
      "epoch:48 step:37936 [D loss: 0.381538, acc.: 81.25%] [G loss: 3.296625]\n",
      "epoch:48 step:37937 [D loss: 0.312290, acc.: 85.16%] [G loss: 3.190027]\n",
      "epoch:48 step:37938 [D loss: 0.289110, acc.: 86.72%] [G loss: 2.701438]\n",
      "epoch:48 step:37939 [D loss: 0.247392, acc.: 89.06%] [G loss: 2.711382]\n",
      "epoch:48 step:37940 [D loss: 0.296939, acc.: 89.06%] [G loss: 3.073518]\n",
      "epoch:48 step:37941 [D loss: 0.211052, acc.: 90.62%] [G loss: 2.759727]\n",
      "epoch:48 step:37942 [D loss: 0.378531, acc.: 82.03%] [G loss: 2.840422]\n",
      "epoch:48 step:37943 [D loss: 0.327127, acc.: 85.16%] [G loss: 2.839539]\n",
      "epoch:48 step:37944 [D loss: 0.246149, acc.: 89.84%] [G loss: 2.915144]\n",
      "epoch:48 step:37945 [D loss: 0.246134, acc.: 91.41%] [G loss: 3.385360]\n",
      "epoch:48 step:37946 [D loss: 0.253711, acc.: 89.84%] [G loss: 2.766342]\n",
      "epoch:48 step:37947 [D loss: 0.235790, acc.: 91.41%] [G loss: 3.164269]\n",
      "epoch:48 step:37948 [D loss: 0.400546, acc.: 82.81%] [G loss: 3.046702]\n",
      "epoch:48 step:37949 [D loss: 0.289559, acc.: 92.19%] [G loss: 2.861040]\n",
      "epoch:48 step:37950 [D loss: 0.334409, acc.: 86.72%] [G loss: 3.162432]\n",
      "epoch:48 step:37951 [D loss: 0.266969, acc.: 85.94%] [G loss: 3.015748]\n",
      "epoch:48 step:37952 [D loss: 0.268804, acc.: 88.28%] [G loss: 3.764464]\n",
      "epoch:48 step:37953 [D loss: 0.355096, acc.: 82.81%] [G loss: 3.145897]\n",
      "epoch:48 step:37954 [D loss: 0.294066, acc.: 89.84%] [G loss: 3.394086]\n",
      "epoch:48 step:37955 [D loss: 0.277804, acc.: 85.94%] [G loss: 3.756622]\n",
      "epoch:48 step:37956 [D loss: 0.309312, acc.: 85.94%] [G loss: 4.059289]\n",
      "epoch:48 step:37957 [D loss: 0.235763, acc.: 89.84%] [G loss: 3.467036]\n",
      "epoch:48 step:37958 [D loss: 0.303340, acc.: 83.59%] [G loss: 3.943475]\n",
      "epoch:48 step:37959 [D loss: 0.288024, acc.: 84.38%] [G loss: 4.636812]\n",
      "epoch:48 step:37960 [D loss: 0.300907, acc.: 83.59%] [G loss: 3.743149]\n",
      "epoch:48 step:37961 [D loss: 0.417399, acc.: 80.47%] [G loss: 3.634782]\n",
      "epoch:48 step:37962 [D loss: 0.292217, acc.: 87.50%] [G loss: 5.929688]\n",
      "epoch:48 step:37963 [D loss: 0.522541, acc.: 81.25%] [G loss: 4.335294]\n",
      "epoch:48 step:37964 [D loss: 0.438859, acc.: 76.56%] [G loss: 4.202707]\n",
      "epoch:48 step:37965 [D loss: 0.385817, acc.: 81.25%] [G loss: 4.070964]\n",
      "epoch:48 step:37966 [D loss: 0.358578, acc.: 85.16%] [G loss: 3.275521]\n",
      "epoch:48 step:37967 [D loss: 0.304856, acc.: 84.38%] [G loss: 4.224333]\n",
      "epoch:48 step:37968 [D loss: 0.336813, acc.: 82.81%] [G loss: 5.173400]\n",
      "epoch:48 step:37969 [D loss: 0.247509, acc.: 89.06%] [G loss: 4.559074]\n",
      "epoch:48 step:37970 [D loss: 0.403794, acc.: 82.81%] [G loss: 3.260740]\n",
      "epoch:48 step:37971 [D loss: 0.365509, acc.: 82.81%] [G loss: 3.658668]\n",
      "epoch:48 step:37972 [D loss: 0.253973, acc.: 89.06%] [G loss: 4.642063]\n",
      "epoch:48 step:37973 [D loss: 0.262934, acc.: 85.94%] [G loss: 3.204303]\n",
      "epoch:48 step:37974 [D loss: 0.304947, acc.: 87.50%] [G loss: 2.787960]\n",
      "epoch:48 step:37975 [D loss: 0.286617, acc.: 85.16%] [G loss: 3.278330]\n",
      "epoch:48 step:37976 [D loss: 0.307200, acc.: 85.16%] [G loss: 2.318901]\n",
      "epoch:48 step:37977 [D loss: 0.382795, acc.: 88.28%] [G loss: 3.252013]\n",
      "epoch:48 step:37978 [D loss: 0.293136, acc.: 86.72%] [G loss: 3.709033]\n",
      "epoch:48 step:37979 [D loss: 0.370234, acc.: 79.69%] [G loss: 4.042896]\n",
      "epoch:48 step:37980 [D loss: 0.314455, acc.: 87.50%] [G loss: 3.188272]\n",
      "epoch:48 step:37981 [D loss: 0.209259, acc.: 92.19%] [G loss: 3.442687]\n",
      "epoch:48 step:37982 [D loss: 0.378294, acc.: 82.81%] [G loss: 2.985650]\n",
      "epoch:48 step:37983 [D loss: 0.255555, acc.: 88.28%] [G loss: 2.771696]\n",
      "epoch:48 step:37984 [D loss: 0.228056, acc.: 89.84%] [G loss: 3.063440]\n",
      "epoch:48 step:37985 [D loss: 0.288416, acc.: 88.28%] [G loss: 3.303805]\n",
      "epoch:48 step:37986 [D loss: 0.201831, acc.: 91.41%] [G loss: 3.964686]\n",
      "epoch:48 step:37987 [D loss: 0.299253, acc.: 85.94%] [G loss: 3.108731]\n",
      "epoch:48 step:37988 [D loss: 0.291563, acc.: 85.94%] [G loss: 3.429372]\n",
      "epoch:48 step:37989 [D loss: 0.250117, acc.: 90.62%] [G loss: 3.764694]\n",
      "epoch:48 step:37990 [D loss: 0.198765, acc.: 90.62%] [G loss: 3.442882]\n",
      "epoch:48 step:37991 [D loss: 0.255067, acc.: 89.06%] [G loss: 3.592773]\n",
      "epoch:48 step:37992 [D loss: 0.152366, acc.: 96.09%] [G loss: 3.651851]\n",
      "epoch:48 step:37993 [D loss: 0.367393, acc.: 82.03%] [G loss: 3.964273]\n",
      "epoch:48 step:37994 [D loss: 0.282966, acc.: 89.06%] [G loss: 3.888927]\n",
      "epoch:48 step:37995 [D loss: 0.332680, acc.: 85.94%] [G loss: 2.967537]\n",
      "epoch:48 step:37996 [D loss: 0.253409, acc.: 86.72%] [G loss: 2.788161]\n",
      "epoch:48 step:37997 [D loss: 0.285717, acc.: 88.28%] [G loss: 3.170191]\n",
      "epoch:48 step:37998 [D loss: 0.271274, acc.: 86.72%] [G loss: 3.133586]\n",
      "epoch:48 step:37999 [D loss: 0.266077, acc.: 88.28%] [G loss: 3.178320]\n",
      "epoch:48 step:38000 [D loss: 0.266797, acc.: 87.50%] [G loss: 3.505491]\n",
      "##############\n",
      "[0.870316   0.86349366 0.82370844 0.80420514 0.79546931 0.84004011\n",
      " 0.89630126 0.83740233 0.83521035 0.8231822 ]\n",
      "##########\n",
      "epoch:48 step:38001 [D loss: 0.235218, acc.: 90.62%] [G loss: 2.661046]\n",
      "epoch:48 step:38002 [D loss: 0.344339, acc.: 83.59%] [G loss: 2.839464]\n",
      "epoch:48 step:38003 [D loss: 0.352891, acc.: 85.94%] [G loss: 3.016061]\n",
      "epoch:48 step:38004 [D loss: 0.296900, acc.: 83.59%] [G loss: 2.841163]\n",
      "epoch:48 step:38005 [D loss: 0.283879, acc.: 85.94%] [G loss: 2.718051]\n",
      "epoch:48 step:38006 [D loss: 0.276453, acc.: 87.50%] [G loss: 3.646123]\n",
      "epoch:48 step:38007 [D loss: 0.287880, acc.: 86.72%] [G loss: 3.025560]\n",
      "epoch:48 step:38008 [D loss: 0.367461, acc.: 82.81%] [G loss: 3.358134]\n",
      "epoch:48 step:38009 [D loss: 0.325786, acc.: 87.50%] [G loss: 3.652008]\n",
      "epoch:48 step:38010 [D loss: 0.294115, acc.: 85.16%] [G loss: 3.701676]\n",
      "epoch:48 step:38011 [D loss: 0.302152, acc.: 92.19%] [G loss: 3.945199]\n",
      "epoch:48 step:38012 [D loss: 0.260677, acc.: 89.06%] [G loss: 2.880652]\n",
      "epoch:48 step:38013 [D loss: 0.282320, acc.: 88.28%] [G loss: 3.076020]\n",
      "epoch:48 step:38014 [D loss: 0.472100, acc.: 78.12%] [G loss: 4.038684]\n",
      "epoch:48 step:38015 [D loss: 0.268502, acc.: 86.72%] [G loss: 4.011425]\n",
      "epoch:48 step:38016 [D loss: 0.218794, acc.: 92.19%] [G loss: 3.549717]\n",
      "epoch:48 step:38017 [D loss: 0.419209, acc.: 80.47%] [G loss: 2.815357]\n",
      "epoch:48 step:38018 [D loss: 0.276866, acc.: 85.16%] [G loss: 3.717010]\n",
      "epoch:48 step:38019 [D loss: 0.228946, acc.: 89.84%] [G loss: 3.442652]\n",
      "epoch:48 step:38020 [D loss: 0.212500, acc.: 91.41%] [G loss: 3.812110]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:48 step:38021 [D loss: 0.304909, acc.: 89.06%] [G loss: 3.551062]\n",
      "epoch:48 step:38022 [D loss: 0.242034, acc.: 88.28%] [G loss: 4.351603]\n",
      "epoch:48 step:38023 [D loss: 0.298302, acc.: 86.72%] [G loss: 3.076491]\n",
      "epoch:48 step:38024 [D loss: 0.239024, acc.: 90.62%] [G loss: 3.965513]\n",
      "epoch:48 step:38025 [D loss: 0.345117, acc.: 85.16%] [G loss: 4.942550]\n",
      "epoch:48 step:38026 [D loss: 0.484268, acc.: 77.34%] [G loss: 3.119047]\n",
      "epoch:48 step:38027 [D loss: 0.255555, acc.: 90.62%] [G loss: 4.328571]\n",
      "epoch:48 step:38028 [D loss: 0.406766, acc.: 82.03%] [G loss: 5.048790]\n",
      "epoch:48 step:38029 [D loss: 0.471577, acc.: 82.81%] [G loss: 7.521313]\n",
      "epoch:48 step:38030 [D loss: 1.112277, acc.: 59.38%] [G loss: 7.725881]\n",
      "epoch:48 step:38031 [D loss: 2.187615, acc.: 58.59%] [G loss: 7.447301]\n",
      "epoch:48 step:38032 [D loss: 0.567975, acc.: 76.56%] [G loss: 3.682762]\n",
      "epoch:48 step:38033 [D loss: 0.294037, acc.: 90.62%] [G loss: 3.635416]\n",
      "epoch:48 step:38034 [D loss: 0.400781, acc.: 83.59%] [G loss: 4.516570]\n",
      "epoch:48 step:38035 [D loss: 0.345000, acc.: 84.38%] [G loss: 2.789546]\n",
      "epoch:48 step:38036 [D loss: 0.283994, acc.: 87.50%] [G loss: 2.936505]\n",
      "epoch:48 step:38037 [D loss: 0.311755, acc.: 84.38%] [G loss: 3.283031]\n",
      "epoch:48 step:38038 [D loss: 0.275282, acc.: 86.72%] [G loss: 3.769103]\n",
      "epoch:48 step:38039 [D loss: 0.280539, acc.: 86.72%] [G loss: 2.932854]\n",
      "epoch:48 step:38040 [D loss: 0.339122, acc.: 82.81%] [G loss: 6.255085]\n",
      "epoch:48 step:38041 [D loss: 0.326115, acc.: 85.94%] [G loss: 2.511497]\n",
      "epoch:48 step:38042 [D loss: 0.292388, acc.: 88.28%] [G loss: 4.027103]\n",
      "epoch:48 step:38043 [D loss: 0.313294, acc.: 86.72%] [G loss: 3.397760]\n",
      "epoch:48 step:38044 [D loss: 0.307702, acc.: 83.59%] [G loss: 3.420755]\n",
      "epoch:48 step:38045 [D loss: 0.203477, acc.: 92.97%] [G loss: 4.739229]\n",
      "epoch:48 step:38046 [D loss: 0.357689, acc.: 83.59%] [G loss: 3.915744]\n",
      "epoch:48 step:38047 [D loss: 0.317137, acc.: 85.94%] [G loss: 3.943646]\n",
      "epoch:48 step:38048 [D loss: 0.356138, acc.: 85.94%] [G loss: 4.375059]\n",
      "epoch:48 step:38049 [D loss: 0.330159, acc.: 85.94%] [G loss: 3.591180]\n",
      "epoch:48 step:38050 [D loss: 0.317196, acc.: 85.16%] [G loss: 3.091969]\n",
      "epoch:48 step:38051 [D loss: 0.293005, acc.: 85.94%] [G loss: 2.784864]\n",
      "epoch:48 step:38052 [D loss: 0.240408, acc.: 90.62%] [G loss: 3.264721]\n",
      "epoch:48 step:38053 [D loss: 0.301366, acc.: 88.28%] [G loss: 3.082207]\n",
      "epoch:48 step:38054 [D loss: 0.322774, acc.: 83.59%] [G loss: 2.948645]\n",
      "epoch:48 step:38055 [D loss: 0.301181, acc.: 87.50%] [G loss: 2.613913]\n",
      "epoch:48 step:38056 [D loss: 0.329041, acc.: 84.38%] [G loss: 2.833790]\n",
      "epoch:48 step:38057 [D loss: 0.256508, acc.: 91.41%] [G loss: 3.317458]\n",
      "epoch:48 step:38058 [D loss: 0.261607, acc.: 90.62%] [G loss: 2.748945]\n",
      "epoch:48 step:38059 [D loss: 0.373136, acc.: 83.59%] [G loss: 2.811329]\n",
      "epoch:48 step:38060 [D loss: 0.323605, acc.: 87.50%] [G loss: 2.168246]\n",
      "epoch:48 step:38061 [D loss: 0.263950, acc.: 89.84%] [G loss: 3.199670]\n",
      "epoch:48 step:38062 [D loss: 0.294559, acc.: 89.84%] [G loss: 3.002811]\n",
      "epoch:48 step:38063 [D loss: 0.295788, acc.: 85.94%] [G loss: 2.710128]\n",
      "epoch:48 step:38064 [D loss: 0.282624, acc.: 89.06%] [G loss: 2.698223]\n",
      "epoch:48 step:38065 [D loss: 0.309018, acc.: 88.28%] [G loss: 2.689132]\n",
      "epoch:48 step:38066 [D loss: 0.316659, acc.: 86.72%] [G loss: 2.849974]\n",
      "epoch:48 step:38067 [D loss: 0.285404, acc.: 85.94%] [G loss: 2.619059]\n",
      "epoch:48 step:38068 [D loss: 0.269565, acc.: 89.84%] [G loss: 2.449404]\n",
      "epoch:48 step:38069 [D loss: 0.268340, acc.: 89.06%] [G loss: 2.782389]\n",
      "epoch:48 step:38070 [D loss: 0.265979, acc.: 90.62%] [G loss: 3.114108]\n",
      "epoch:48 step:38071 [D loss: 0.385752, acc.: 81.25%] [G loss: 3.859316]\n",
      "epoch:48 step:38072 [D loss: 0.362004, acc.: 83.59%] [G loss: 3.430030]\n",
      "epoch:48 step:38073 [D loss: 0.247382, acc.: 89.84%] [G loss: 3.334098]\n",
      "epoch:48 step:38074 [D loss: 0.237148, acc.: 92.19%] [G loss: 3.636702]\n",
      "epoch:48 step:38075 [D loss: 0.275814, acc.: 86.72%] [G loss: 2.967244]\n",
      "epoch:48 step:38076 [D loss: 0.265451, acc.: 86.72%] [G loss: 2.823053]\n",
      "epoch:48 step:38077 [D loss: 0.223106, acc.: 91.41%] [G loss: 2.572594]\n",
      "epoch:48 step:38078 [D loss: 0.273874, acc.: 86.72%] [G loss: 2.376396]\n",
      "epoch:48 step:38079 [D loss: 0.348631, acc.: 82.03%] [G loss: 2.895142]\n",
      "epoch:48 step:38080 [D loss: 0.320761, acc.: 89.84%] [G loss: 2.959382]\n",
      "epoch:48 step:38081 [D loss: 0.315839, acc.: 85.94%] [G loss: 2.634263]\n",
      "epoch:48 step:38082 [D loss: 0.243606, acc.: 89.06%] [G loss: 2.962899]\n",
      "epoch:48 step:38083 [D loss: 0.311392, acc.: 86.72%] [G loss: 2.676960]\n",
      "epoch:48 step:38084 [D loss: 0.351276, acc.: 84.38%] [G loss: 2.796654]\n",
      "epoch:48 step:38085 [D loss: 0.288339, acc.: 89.84%] [G loss: 2.871713]\n",
      "epoch:48 step:38086 [D loss: 0.335714, acc.: 83.59%] [G loss: 2.814907]\n",
      "epoch:48 step:38087 [D loss: 0.237551, acc.: 93.75%] [G loss: 3.393291]\n",
      "epoch:48 step:38088 [D loss: 0.310629, acc.: 82.81%] [G loss: 3.315384]\n",
      "epoch:48 step:38089 [D loss: 0.267449, acc.: 87.50%] [G loss: 3.422925]\n",
      "epoch:48 step:38090 [D loss: 0.353895, acc.: 84.38%] [G loss: 2.880150]\n",
      "epoch:48 step:38091 [D loss: 0.311033, acc.: 87.50%] [G loss: 4.487772]\n",
      "epoch:48 step:38092 [D loss: 0.334751, acc.: 86.72%] [G loss: 2.874483]\n",
      "epoch:48 step:38093 [D loss: 0.280101, acc.: 87.50%] [G loss: 2.966769]\n",
      "epoch:48 step:38094 [D loss: 0.429878, acc.: 76.56%] [G loss: 3.303612]\n",
      "epoch:48 step:38095 [D loss: 0.262670, acc.: 85.94%] [G loss: 3.919875]\n",
      "epoch:48 step:38096 [D loss: 0.256273, acc.: 89.84%] [G loss: 3.563518]\n",
      "epoch:48 step:38097 [D loss: 0.392725, acc.: 81.25%] [G loss: 3.933149]\n",
      "epoch:48 step:38098 [D loss: 0.326335, acc.: 84.38%] [G loss: 2.544989]\n",
      "epoch:48 step:38099 [D loss: 0.327058, acc.: 82.03%] [G loss: 2.820657]\n",
      "epoch:48 step:38100 [D loss: 0.316404, acc.: 83.59%] [G loss: 3.845808]\n",
      "epoch:48 step:38101 [D loss: 0.316026, acc.: 88.28%] [G loss: 3.642942]\n",
      "epoch:48 step:38102 [D loss: 0.338385, acc.: 84.38%] [G loss: 3.651709]\n",
      "epoch:48 step:38103 [D loss: 0.249701, acc.: 89.06%] [G loss: 3.709400]\n",
      "epoch:48 step:38104 [D loss: 0.340254, acc.: 81.25%] [G loss: 4.404649]\n",
      "epoch:48 step:38105 [D loss: 0.270811, acc.: 89.06%] [G loss: 3.159487]\n",
      "epoch:48 step:38106 [D loss: 0.387330, acc.: 78.91%] [G loss: 6.162899]\n",
      "epoch:48 step:38107 [D loss: 0.254440, acc.: 89.06%] [G loss: 3.101755]\n",
      "epoch:48 step:38108 [D loss: 0.217937, acc.: 91.41%] [G loss: 3.760973]\n",
      "epoch:48 step:38109 [D loss: 0.297758, acc.: 86.72%] [G loss: 3.843503]\n",
      "epoch:48 step:38110 [D loss: 0.273205, acc.: 84.38%] [G loss: 3.306659]\n",
      "epoch:48 step:38111 [D loss: 0.345901, acc.: 85.16%] [G loss: 3.603777]\n",
      "epoch:48 step:38112 [D loss: 0.374387, acc.: 79.69%] [G loss: 3.473372]\n",
      "epoch:48 step:38113 [D loss: 0.413910, acc.: 75.78%] [G loss: 2.576708]\n",
      "epoch:48 step:38114 [D loss: 0.261613, acc.: 92.19%] [G loss: 3.501978]\n",
      "epoch:48 step:38115 [D loss: 0.322867, acc.: 85.94%] [G loss: 2.889591]\n",
      "epoch:48 step:38116 [D loss: 0.293628, acc.: 84.38%] [G loss: 3.569493]\n",
      "epoch:48 step:38117 [D loss: 0.326903, acc.: 83.59%] [G loss: 3.087933]\n",
      "epoch:48 step:38118 [D loss: 0.312693, acc.: 86.72%] [G loss: 3.400987]\n",
      "epoch:48 step:38119 [D loss: 0.355220, acc.: 84.38%] [G loss: 3.274577]\n",
      "epoch:48 step:38120 [D loss: 0.273675, acc.: 90.62%] [G loss: 3.127931]\n",
      "epoch:48 step:38121 [D loss: 0.347544, acc.: 84.38%] [G loss: 2.842794]\n",
      "epoch:48 step:38122 [D loss: 0.377183, acc.: 78.91%] [G loss: 2.979852]\n",
      "epoch:48 step:38123 [D loss: 0.386729, acc.: 83.59%] [G loss: 3.905303]\n",
      "epoch:48 step:38124 [D loss: 0.323503, acc.: 84.38%] [G loss: 3.399761]\n",
      "epoch:48 step:38125 [D loss: 0.368038, acc.: 85.94%] [G loss: 4.193594]\n",
      "epoch:48 step:38126 [D loss: 0.304794, acc.: 85.16%] [G loss: 4.036023]\n",
      "epoch:48 step:38127 [D loss: 0.230829, acc.: 90.62%] [G loss: 4.944130]\n",
      "epoch:48 step:38128 [D loss: 0.296468, acc.: 87.50%] [G loss: 3.814270]\n",
      "epoch:48 step:38129 [D loss: 0.293972, acc.: 87.50%] [G loss: 3.937187]\n",
      "epoch:48 step:38130 [D loss: 0.291744, acc.: 88.28%] [G loss: 4.282076]\n",
      "epoch:48 step:38131 [D loss: 0.287799, acc.: 84.38%] [G loss: 2.662384]\n",
      "epoch:48 step:38132 [D loss: 0.224627, acc.: 90.62%] [G loss: 2.684674]\n",
      "epoch:48 step:38133 [D loss: 0.299863, acc.: 85.94%] [G loss: 2.661534]\n",
      "epoch:48 step:38134 [D loss: 0.304152, acc.: 85.94%] [G loss: 2.941640]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:48 step:38135 [D loss: 0.278144, acc.: 86.72%] [G loss: 3.794394]\n",
      "epoch:48 step:38136 [D loss: 0.267670, acc.: 89.84%] [G loss: 3.255422]\n",
      "epoch:48 step:38137 [D loss: 0.359579, acc.: 80.47%] [G loss: 2.831507]\n",
      "epoch:48 step:38138 [D loss: 0.409426, acc.: 84.38%] [G loss: 3.159423]\n",
      "epoch:48 step:38139 [D loss: 0.324450, acc.: 83.59%] [G loss: 2.499792]\n",
      "epoch:48 step:38140 [D loss: 0.304702, acc.: 87.50%] [G loss: 3.040250]\n",
      "epoch:48 step:38141 [D loss: 0.342604, acc.: 87.50%] [G loss: 3.156487]\n",
      "epoch:48 step:38142 [D loss: 0.325081, acc.: 85.94%] [G loss: 2.936198]\n",
      "epoch:48 step:38143 [D loss: 0.321764, acc.: 87.50%] [G loss: 3.437516]\n",
      "epoch:48 step:38144 [D loss: 0.234322, acc.: 89.84%] [G loss: 3.283906]\n",
      "epoch:48 step:38145 [D loss: 0.336311, acc.: 84.38%] [G loss: 2.751421]\n",
      "epoch:48 step:38146 [D loss: 0.258329, acc.: 92.19%] [G loss: 3.206871]\n",
      "epoch:48 step:38147 [D loss: 0.330524, acc.: 84.38%] [G loss: 3.582586]\n",
      "epoch:48 step:38148 [D loss: 0.290141, acc.: 88.28%] [G loss: 5.205257]\n",
      "epoch:48 step:38149 [D loss: 0.321129, acc.: 85.16%] [G loss: 4.169551]\n",
      "epoch:48 step:38150 [D loss: 0.311930, acc.: 85.94%] [G loss: 3.294134]\n",
      "epoch:48 step:38151 [D loss: 0.228213, acc.: 90.62%] [G loss: 3.433095]\n",
      "epoch:48 step:38152 [D loss: 0.310381, acc.: 85.94%] [G loss: 3.578766]\n",
      "epoch:48 step:38153 [D loss: 0.324359, acc.: 86.72%] [G loss: 2.825680]\n",
      "epoch:48 step:38154 [D loss: 0.263354, acc.: 89.84%] [G loss: 3.271472]\n",
      "epoch:48 step:38155 [D loss: 0.283660, acc.: 86.72%] [G loss: 2.590105]\n",
      "epoch:48 step:38156 [D loss: 0.324086, acc.: 85.16%] [G loss: 3.315682]\n",
      "epoch:48 step:38157 [D loss: 0.310168, acc.: 87.50%] [G loss: 3.355118]\n",
      "epoch:48 step:38158 [D loss: 0.353885, acc.: 82.81%] [G loss: 4.451091]\n",
      "epoch:48 step:38159 [D loss: 0.380304, acc.: 82.81%] [G loss: 2.532881]\n",
      "epoch:48 step:38160 [D loss: 0.262165, acc.: 90.62%] [G loss: 3.730904]\n",
      "epoch:48 step:38161 [D loss: 0.245736, acc.: 89.06%] [G loss: 3.818191]\n",
      "epoch:48 step:38162 [D loss: 0.263845, acc.: 89.06%] [G loss: 3.396739]\n",
      "epoch:48 step:38163 [D loss: 0.272319, acc.: 85.94%] [G loss: 3.322254]\n",
      "epoch:48 step:38164 [D loss: 0.316643, acc.: 86.72%] [G loss: 2.631146]\n",
      "epoch:48 step:38165 [D loss: 0.298625, acc.: 90.62%] [G loss: 2.829337]\n",
      "epoch:48 step:38166 [D loss: 0.297488, acc.: 88.28%] [G loss: 3.327811]\n",
      "epoch:48 step:38167 [D loss: 0.266714, acc.: 89.06%] [G loss: 3.613533]\n",
      "epoch:48 step:38168 [D loss: 0.276328, acc.: 89.06%] [G loss: 4.082101]\n",
      "epoch:48 step:38169 [D loss: 0.332780, acc.: 86.72%] [G loss: 3.181237]\n",
      "epoch:48 step:38170 [D loss: 0.395373, acc.: 82.81%] [G loss: 4.156849]\n",
      "epoch:48 step:38171 [D loss: 0.275813, acc.: 84.38%] [G loss: 4.156111]\n",
      "epoch:48 step:38172 [D loss: 0.244909, acc.: 89.84%] [G loss: 4.545369]\n",
      "epoch:48 step:38173 [D loss: 0.267198, acc.: 89.06%] [G loss: 2.784127]\n",
      "epoch:48 step:38174 [D loss: 0.397574, acc.: 82.81%] [G loss: 3.841772]\n",
      "epoch:48 step:38175 [D loss: 0.297872, acc.: 86.72%] [G loss: 3.808057]\n",
      "epoch:48 step:38176 [D loss: 0.211924, acc.: 93.75%] [G loss: 2.768937]\n",
      "epoch:48 step:38177 [D loss: 0.273038, acc.: 86.72%] [G loss: 3.460449]\n",
      "epoch:48 step:38178 [D loss: 0.242926, acc.: 90.62%] [G loss: 4.039080]\n",
      "epoch:48 step:38179 [D loss: 0.417934, acc.: 80.47%] [G loss: 3.875747]\n",
      "epoch:48 step:38180 [D loss: 0.374481, acc.: 86.72%] [G loss: 4.171238]\n",
      "epoch:48 step:38181 [D loss: 0.265188, acc.: 87.50%] [G loss: 3.470764]\n",
      "epoch:48 step:38182 [D loss: 0.293266, acc.: 88.28%] [G loss: 3.420850]\n",
      "epoch:48 step:38183 [D loss: 0.230078, acc.: 89.06%] [G loss: 3.355113]\n",
      "epoch:48 step:38184 [D loss: 0.320774, acc.: 84.38%] [G loss: 3.769850]\n",
      "epoch:48 step:38185 [D loss: 0.440307, acc.: 77.34%] [G loss: 3.530358]\n",
      "epoch:48 step:38186 [D loss: 0.464994, acc.: 85.16%] [G loss: 3.154241]\n",
      "epoch:48 step:38187 [D loss: 0.393076, acc.: 83.59%] [G loss: 2.730446]\n",
      "epoch:48 step:38188 [D loss: 0.338984, acc.: 85.16%] [G loss: 3.417687]\n",
      "epoch:48 step:38189 [D loss: 0.360553, acc.: 82.03%] [G loss: 3.289492]\n",
      "epoch:48 step:38190 [D loss: 0.367893, acc.: 82.03%] [G loss: 3.481656]\n",
      "epoch:48 step:38191 [D loss: 0.323662, acc.: 85.16%] [G loss: 3.666538]\n",
      "epoch:48 step:38192 [D loss: 0.375856, acc.: 80.47%] [G loss: 3.874645]\n",
      "epoch:48 step:38193 [D loss: 0.263201, acc.: 89.06%] [G loss: 3.444146]\n",
      "epoch:48 step:38194 [D loss: 0.329515, acc.: 87.50%] [G loss: 5.014176]\n",
      "epoch:48 step:38195 [D loss: 0.441885, acc.: 82.03%] [G loss: 4.739434]\n",
      "epoch:48 step:38196 [D loss: 0.358188, acc.: 82.81%] [G loss: 3.189884]\n",
      "epoch:48 step:38197 [D loss: 0.359597, acc.: 85.16%] [G loss: 3.051982]\n",
      "epoch:48 step:38198 [D loss: 0.301931, acc.: 89.06%] [G loss: 3.761623]\n",
      "epoch:48 step:38199 [D loss: 0.253577, acc.: 90.62%] [G loss: 4.106730]\n",
      "epoch:48 step:38200 [D loss: 0.320618, acc.: 84.38%] [G loss: 3.749276]\n",
      "##############\n",
      "[0.87282841 0.85533775 0.81267491 0.83485411 0.77627135 0.82870453\n",
      " 0.86985933 0.85719953 0.80668188 0.84385185]\n",
      "##########\n",
      "epoch:48 step:38201 [D loss: 0.395004, acc.: 82.81%] [G loss: 3.646379]\n",
      "epoch:48 step:38202 [D loss: 0.283259, acc.: 85.94%] [G loss: 4.069791]\n",
      "epoch:48 step:38203 [D loss: 0.333434, acc.: 84.38%] [G loss: 5.547690]\n",
      "epoch:48 step:38204 [D loss: 0.289322, acc.: 83.59%] [G loss: 5.039082]\n",
      "epoch:48 step:38205 [D loss: 0.255749, acc.: 92.19%] [G loss: 3.557564]\n",
      "epoch:48 step:38206 [D loss: 0.355527, acc.: 80.47%] [G loss: 2.715510]\n",
      "epoch:48 step:38207 [D loss: 0.258480, acc.: 86.72%] [G loss: 3.117888]\n",
      "epoch:48 step:38208 [D loss: 0.331561, acc.: 86.72%] [G loss: 3.269876]\n",
      "epoch:48 step:38209 [D loss: 0.256528, acc.: 89.84%] [G loss: 3.697886]\n",
      "epoch:48 step:38210 [D loss: 0.298637, acc.: 86.72%] [G loss: 4.227361]\n",
      "epoch:48 step:38211 [D loss: 0.314604, acc.: 85.94%] [G loss: 3.382814]\n",
      "epoch:48 step:38212 [D loss: 0.242656, acc.: 91.41%] [G loss: 3.699365]\n",
      "epoch:48 step:38213 [D loss: 0.336830, acc.: 86.72%] [G loss: 3.001003]\n",
      "epoch:48 step:38214 [D loss: 0.299946, acc.: 86.72%] [G loss: 3.683498]\n",
      "epoch:48 step:38215 [D loss: 0.328987, acc.: 86.72%] [G loss: 3.702072]\n",
      "epoch:48 step:38216 [D loss: 0.299164, acc.: 86.72%] [G loss: 3.914506]\n",
      "epoch:48 step:38217 [D loss: 0.290061, acc.: 88.28%] [G loss: 4.359055]\n",
      "epoch:48 step:38218 [D loss: 0.261429, acc.: 87.50%] [G loss: 3.056235]\n",
      "epoch:48 step:38219 [D loss: 0.287250, acc.: 89.06%] [G loss: 3.255315]\n",
      "epoch:48 step:38220 [D loss: 0.307385, acc.: 85.94%] [G loss: 3.390902]\n",
      "epoch:48 step:38221 [D loss: 0.181365, acc.: 92.97%] [G loss: 4.094878]\n",
      "epoch:48 step:38222 [D loss: 0.363740, acc.: 84.38%] [G loss: 2.846267]\n",
      "epoch:48 step:38223 [D loss: 0.260747, acc.: 89.06%] [G loss: 4.073522]\n",
      "epoch:48 step:38224 [D loss: 0.298218, acc.: 86.72%] [G loss: 2.839912]\n",
      "epoch:48 step:38225 [D loss: 0.259102, acc.: 89.84%] [G loss: 4.937942]\n",
      "epoch:48 step:38226 [D loss: 0.535707, acc.: 78.91%] [G loss: 6.350312]\n",
      "epoch:48 step:38227 [D loss: 1.150558, acc.: 67.19%] [G loss: 7.209393]\n",
      "epoch:48 step:38228 [D loss: 1.477111, acc.: 64.06%] [G loss: 5.001104]\n",
      "epoch:48 step:38229 [D loss: 0.387234, acc.: 87.50%] [G loss: 5.564497]\n",
      "epoch:48 step:38230 [D loss: 0.404395, acc.: 89.06%] [G loss: 5.362315]\n",
      "epoch:48 step:38231 [D loss: 0.418013, acc.: 83.59%] [G loss: 4.677583]\n",
      "epoch:48 step:38232 [D loss: 0.313145, acc.: 89.06%] [G loss: 3.963408]\n",
      "epoch:48 step:38233 [D loss: 0.328686, acc.: 84.38%] [G loss: 3.414876]\n",
      "epoch:48 step:38234 [D loss: 0.303912, acc.: 87.50%] [G loss: 3.683854]\n",
      "epoch:48 step:38235 [D loss: 0.254843, acc.: 88.28%] [G loss: 3.466669]\n",
      "epoch:48 step:38236 [D loss: 0.294473, acc.: 85.94%] [G loss: 3.656817]\n",
      "epoch:48 step:38237 [D loss: 0.254554, acc.: 90.62%] [G loss: 3.227423]\n",
      "epoch:48 step:38238 [D loss: 0.285702, acc.: 86.72%] [G loss: 4.507710]\n",
      "epoch:48 step:38239 [D loss: 0.323040, acc.: 85.94%] [G loss: 3.332058]\n",
      "epoch:48 step:38240 [D loss: 0.269098, acc.: 89.06%] [G loss: 3.534050]\n",
      "epoch:48 step:38241 [D loss: 0.351437, acc.: 89.06%] [G loss: 3.616163]\n",
      "epoch:48 step:38242 [D loss: 0.437877, acc.: 86.72%] [G loss: 3.569335]\n",
      "epoch:48 step:38243 [D loss: 0.296536, acc.: 87.50%] [G loss: 2.835770]\n",
      "epoch:48 step:38244 [D loss: 0.324250, acc.: 85.94%] [G loss: 2.985547]\n",
      "epoch:48 step:38245 [D loss: 0.296027, acc.: 85.16%] [G loss: 3.147177]\n",
      "epoch:48 step:38246 [D loss: 0.384265, acc.: 80.47%] [G loss: 3.027610]\n",
      "epoch:48 step:38247 [D loss: 0.316717, acc.: 87.50%] [G loss: 3.743611]\n",
      "epoch:48 step:38248 [D loss: 0.328324, acc.: 84.38%] [G loss: 3.012011]\n",
      "epoch:48 step:38249 [D loss: 0.332759, acc.: 84.38%] [G loss: 3.013257]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:48 step:38250 [D loss: 0.396339, acc.: 82.81%] [G loss: 3.182664]\n",
      "epoch:48 step:38251 [D loss: 0.339669, acc.: 85.16%] [G loss: 3.282113]\n",
      "epoch:48 step:38252 [D loss: 0.368203, acc.: 84.38%] [G loss: 2.885814]\n",
      "epoch:48 step:38253 [D loss: 0.430483, acc.: 77.34%] [G loss: 2.796248]\n",
      "epoch:48 step:38254 [D loss: 0.279560, acc.: 88.28%] [G loss: 3.372228]\n",
      "epoch:48 step:38255 [D loss: 0.287010, acc.: 90.62%] [G loss: 3.178661]\n",
      "epoch:48 step:38256 [D loss: 0.397261, acc.: 78.91%] [G loss: 3.583766]\n",
      "epoch:48 step:38257 [D loss: 0.312091, acc.: 83.59%] [G loss: 3.006265]\n",
      "epoch:48 step:38258 [D loss: 0.234468, acc.: 91.41%] [G loss: 3.628275]\n",
      "epoch:48 step:38259 [D loss: 0.340028, acc.: 81.25%] [G loss: 3.836949]\n",
      "epoch:48 step:38260 [D loss: 0.278960, acc.: 85.94%] [G loss: 2.776843]\n",
      "epoch:48 step:38261 [D loss: 0.317331, acc.: 86.72%] [G loss: 3.396186]\n",
      "epoch:48 step:38262 [D loss: 0.262659, acc.: 89.84%] [G loss: 2.595867]\n",
      "epoch:48 step:38263 [D loss: 0.301127, acc.: 87.50%] [G loss: 3.646544]\n",
      "epoch:48 step:38264 [D loss: 0.403949, acc.: 82.81%] [G loss: 3.382151]\n",
      "epoch:48 step:38265 [D loss: 0.372232, acc.: 84.38%] [G loss: 6.159400]\n",
      "epoch:48 step:38266 [D loss: 0.372038, acc.: 83.59%] [G loss: 3.251592]\n",
      "epoch:48 step:38267 [D loss: 0.343942, acc.: 85.94%] [G loss: 4.007144]\n",
      "epoch:48 step:38268 [D loss: 0.360239, acc.: 82.03%] [G loss: 5.282059]\n",
      "epoch:48 step:38269 [D loss: 0.401360, acc.: 85.94%] [G loss: 3.859682]\n",
      "epoch:49 step:38270 [D loss: 0.366587, acc.: 82.81%] [G loss: 3.907892]\n",
      "epoch:49 step:38271 [D loss: 0.314791, acc.: 82.81%] [G loss: 4.305477]\n",
      "epoch:49 step:38272 [D loss: 0.476170, acc.: 78.12%] [G loss: 6.905629]\n",
      "epoch:49 step:38273 [D loss: 0.632253, acc.: 75.00%] [G loss: 3.643479]\n",
      "epoch:49 step:38274 [D loss: 0.314107, acc.: 88.28%] [G loss: 3.772788]\n",
      "epoch:49 step:38275 [D loss: 0.332585, acc.: 86.72%] [G loss: 2.753158]\n",
      "epoch:49 step:38276 [D loss: 0.362187, acc.: 86.72%] [G loss: 2.817354]\n",
      "epoch:49 step:38277 [D loss: 0.368855, acc.: 84.38%] [G loss: 3.124845]\n",
      "epoch:49 step:38278 [D loss: 0.427515, acc.: 82.03%] [G loss: 3.339906]\n",
      "epoch:49 step:38279 [D loss: 0.310473, acc.: 89.84%] [G loss: 2.740116]\n",
      "epoch:49 step:38280 [D loss: 0.371394, acc.: 82.03%] [G loss: 3.476717]\n",
      "epoch:49 step:38281 [D loss: 0.376327, acc.: 80.47%] [G loss: 3.420535]\n",
      "epoch:49 step:38282 [D loss: 0.320947, acc.: 85.16%] [G loss: 3.356490]\n",
      "epoch:49 step:38283 [D loss: 0.266177, acc.: 87.50%] [G loss: 2.978247]\n",
      "epoch:49 step:38284 [D loss: 0.308758, acc.: 87.50%] [G loss: 2.909492]\n",
      "epoch:49 step:38285 [D loss: 0.314327, acc.: 84.38%] [G loss: 3.495920]\n",
      "epoch:49 step:38286 [D loss: 0.238878, acc.: 89.84%] [G loss: 3.153405]\n",
      "epoch:49 step:38287 [D loss: 0.307479, acc.: 85.16%] [G loss: 3.095515]\n",
      "epoch:49 step:38288 [D loss: 0.313481, acc.: 85.94%] [G loss: 3.417830]\n",
      "epoch:49 step:38289 [D loss: 0.385447, acc.: 84.38%] [G loss: 2.722098]\n",
      "epoch:49 step:38290 [D loss: 0.339763, acc.: 84.38%] [G loss: 2.967044]\n",
      "epoch:49 step:38291 [D loss: 0.274835, acc.: 89.84%] [G loss: 2.995806]\n",
      "epoch:49 step:38292 [D loss: 0.443127, acc.: 79.69%] [G loss: 3.846726]\n",
      "epoch:49 step:38293 [D loss: 0.279612, acc.: 89.06%] [G loss: 4.634274]\n",
      "epoch:49 step:38294 [D loss: 0.276742, acc.: 85.16%] [G loss: 4.634865]\n",
      "epoch:49 step:38295 [D loss: 0.280387, acc.: 85.94%] [G loss: 3.464972]\n",
      "epoch:49 step:38296 [D loss: 0.333419, acc.: 85.94%] [G loss: 3.063569]\n",
      "epoch:49 step:38297 [D loss: 0.312441, acc.: 87.50%] [G loss: 2.985047]\n",
      "epoch:49 step:38298 [D loss: 0.307126, acc.: 86.72%] [G loss: 2.643319]\n",
      "epoch:49 step:38299 [D loss: 0.378165, acc.: 85.94%] [G loss: 2.654320]\n",
      "epoch:49 step:38300 [D loss: 0.364535, acc.: 81.25%] [G loss: 2.869786]\n",
      "epoch:49 step:38301 [D loss: 0.429832, acc.: 81.25%] [G loss: 3.131656]\n",
      "epoch:49 step:38302 [D loss: 0.288381, acc.: 86.72%] [G loss: 2.802780]\n",
      "epoch:49 step:38303 [D loss: 0.456721, acc.: 80.47%] [G loss: 4.821821]\n",
      "epoch:49 step:38304 [D loss: 0.462096, acc.: 77.34%] [G loss: 5.280092]\n",
      "epoch:49 step:38305 [D loss: 0.403838, acc.: 80.47%] [G loss: 3.349579]\n",
      "epoch:49 step:38306 [D loss: 0.364558, acc.: 84.38%] [G loss: 3.733012]\n",
      "epoch:49 step:38307 [D loss: 0.333051, acc.: 85.94%] [G loss: 3.303245]\n",
      "epoch:49 step:38308 [D loss: 0.296071, acc.: 85.94%] [G loss: 3.456935]\n",
      "epoch:49 step:38309 [D loss: 0.336629, acc.: 83.59%] [G loss: 4.046119]\n",
      "epoch:49 step:38310 [D loss: 0.381460, acc.: 80.47%] [G loss: 2.912213]\n",
      "epoch:49 step:38311 [D loss: 0.342482, acc.: 85.16%] [G loss: 3.090365]\n",
      "epoch:49 step:38312 [D loss: 0.413540, acc.: 79.69%] [G loss: 3.824889]\n",
      "epoch:49 step:38313 [D loss: 0.332010, acc.: 82.03%] [G loss: 2.797512]\n",
      "epoch:49 step:38314 [D loss: 0.219803, acc.: 89.84%] [G loss: 4.124802]\n",
      "epoch:49 step:38315 [D loss: 0.278175, acc.: 89.06%] [G loss: 3.109580]\n",
      "epoch:49 step:38316 [D loss: 0.324190, acc.: 84.38%] [G loss: 3.840970]\n",
      "epoch:49 step:38317 [D loss: 0.259668, acc.: 89.06%] [G loss: 3.119775]\n",
      "epoch:49 step:38318 [D loss: 0.285843, acc.: 89.06%] [G loss: 3.864298]\n",
      "epoch:49 step:38319 [D loss: 0.322351, acc.: 86.72%] [G loss: 2.841836]\n",
      "epoch:49 step:38320 [D loss: 0.368525, acc.: 81.25%] [G loss: 3.052977]\n",
      "epoch:49 step:38321 [D loss: 0.243089, acc.: 90.62%] [G loss: 2.703762]\n",
      "epoch:49 step:38322 [D loss: 0.321099, acc.: 83.59%] [G loss: 3.207934]\n",
      "epoch:49 step:38323 [D loss: 0.362180, acc.: 88.28%] [G loss: 2.769256]\n",
      "epoch:49 step:38324 [D loss: 0.246981, acc.: 90.62%] [G loss: 2.733565]\n",
      "epoch:49 step:38325 [D loss: 0.263032, acc.: 88.28%] [G loss: 3.921812]\n",
      "epoch:49 step:38326 [D loss: 0.321834, acc.: 87.50%] [G loss: 3.840958]\n",
      "epoch:49 step:38327 [D loss: 0.308780, acc.: 85.94%] [G loss: 3.693371]\n",
      "epoch:49 step:38328 [D loss: 0.393231, acc.: 80.47%] [G loss: 4.665831]\n",
      "epoch:49 step:38329 [D loss: 0.385003, acc.: 80.47%] [G loss: 3.536892]\n",
      "epoch:49 step:38330 [D loss: 0.385806, acc.: 85.94%] [G loss: 2.678504]\n",
      "epoch:49 step:38331 [D loss: 0.405306, acc.: 85.16%] [G loss: 4.181108]\n",
      "epoch:49 step:38332 [D loss: 0.436475, acc.: 79.69%] [G loss: 3.509356]\n",
      "epoch:49 step:38333 [D loss: 0.298885, acc.: 88.28%] [G loss: 3.406935]\n",
      "epoch:49 step:38334 [D loss: 0.477862, acc.: 78.12%] [G loss: 2.518383]\n",
      "epoch:49 step:38335 [D loss: 0.405587, acc.: 82.03%] [G loss: 2.814872]\n",
      "epoch:49 step:38336 [D loss: 0.284388, acc.: 86.72%] [G loss: 3.200861]\n",
      "epoch:49 step:38337 [D loss: 0.346020, acc.: 82.81%] [G loss: 2.403582]\n",
      "epoch:49 step:38338 [D loss: 0.354995, acc.: 85.94%] [G loss: 3.388967]\n",
      "epoch:49 step:38339 [D loss: 0.489238, acc.: 74.22%] [G loss: 4.584505]\n",
      "epoch:49 step:38340 [D loss: 0.343059, acc.: 85.16%] [G loss: 3.539672]\n",
      "epoch:49 step:38341 [D loss: 0.279305, acc.: 91.41%] [G loss: 5.945462]\n",
      "epoch:49 step:38342 [D loss: 0.387199, acc.: 85.16%] [G loss: 4.871071]\n",
      "epoch:49 step:38343 [D loss: 0.211004, acc.: 89.06%] [G loss: 5.817781]\n",
      "epoch:49 step:38344 [D loss: 0.312088, acc.: 85.94%] [G loss: 3.446443]\n",
      "epoch:49 step:38345 [D loss: 0.302097, acc.: 85.94%] [G loss: 4.375295]\n",
      "epoch:49 step:38346 [D loss: 0.270409, acc.: 92.97%] [G loss: 3.842594]\n",
      "epoch:49 step:38347 [D loss: 0.484875, acc.: 82.03%] [G loss: 3.804104]\n",
      "epoch:49 step:38348 [D loss: 0.406619, acc.: 81.25%] [G loss: 3.228744]\n",
      "epoch:49 step:38349 [D loss: 0.269704, acc.: 90.62%] [G loss: 3.266837]\n",
      "epoch:49 step:38350 [D loss: 0.337993, acc.: 82.03%] [G loss: 5.301247]\n",
      "epoch:49 step:38351 [D loss: 0.476418, acc.: 80.47%] [G loss: 3.989330]\n",
      "epoch:49 step:38352 [D loss: 0.367405, acc.: 81.25%] [G loss: 3.505988]\n",
      "epoch:49 step:38353 [D loss: 0.293449, acc.: 87.50%] [G loss: 5.060906]\n",
      "epoch:49 step:38354 [D loss: 0.368599, acc.: 82.03%] [G loss: 3.532393]\n",
      "epoch:49 step:38355 [D loss: 0.291226, acc.: 85.94%] [G loss: 5.229986]\n",
      "epoch:49 step:38356 [D loss: 0.370165, acc.: 81.25%] [G loss: 3.783743]\n",
      "epoch:49 step:38357 [D loss: 0.255791, acc.: 87.50%] [G loss: 4.318522]\n",
      "epoch:49 step:38358 [D loss: 0.423092, acc.: 82.81%] [G loss: 3.150562]\n",
      "epoch:49 step:38359 [D loss: 0.368165, acc.: 85.16%] [G loss: 3.631096]\n",
      "epoch:49 step:38360 [D loss: 0.329504, acc.: 85.94%] [G loss: 3.798252]\n",
      "epoch:49 step:38361 [D loss: 0.322514, acc.: 87.50%] [G loss: 3.286619]\n",
      "epoch:49 step:38362 [D loss: 0.380788, acc.: 84.38%] [G loss: 2.672318]\n",
      "epoch:49 step:38363 [D loss: 0.252926, acc.: 91.41%] [G loss: 3.988913]\n",
      "epoch:49 step:38364 [D loss: 0.319048, acc.: 88.28%] [G loss: 2.380286]\n",
      "epoch:49 step:38365 [D loss: 0.344851, acc.: 84.38%] [G loss: 3.795751]\n",
      "epoch:49 step:38366 [D loss: 0.347510, acc.: 84.38%] [G loss: 3.009765]\n",
      "epoch:49 step:38367 [D loss: 0.294647, acc.: 87.50%] [G loss: 3.193969]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:49 step:38368 [D loss: 0.410055, acc.: 78.91%] [G loss: 3.112909]\n",
      "epoch:49 step:38369 [D loss: 0.328479, acc.: 85.16%] [G loss: 3.525854]\n",
      "epoch:49 step:38370 [D loss: 0.273644, acc.: 89.06%] [G loss: 3.391294]\n",
      "epoch:49 step:38371 [D loss: 0.282616, acc.: 89.84%] [G loss: 2.411841]\n",
      "epoch:49 step:38372 [D loss: 0.300597, acc.: 83.59%] [G loss: 4.182209]\n",
      "epoch:49 step:38373 [D loss: 0.358720, acc.: 82.81%] [G loss: 4.635117]\n",
      "epoch:49 step:38374 [D loss: 0.401666, acc.: 83.59%] [G loss: 7.293904]\n",
      "epoch:49 step:38375 [D loss: 0.824286, acc.: 67.19%] [G loss: 5.128368]\n",
      "epoch:49 step:38376 [D loss: 0.956467, acc.: 64.84%] [G loss: 3.107810]\n",
      "epoch:49 step:38377 [D loss: 0.345774, acc.: 88.28%] [G loss: 4.464556]\n",
      "epoch:49 step:38378 [D loss: 0.310841, acc.: 82.81%] [G loss: 2.929215]\n",
      "epoch:49 step:38379 [D loss: 0.412981, acc.: 80.47%] [G loss: 3.121955]\n",
      "epoch:49 step:38380 [D loss: 0.381529, acc.: 82.81%] [G loss: 2.432879]\n",
      "epoch:49 step:38381 [D loss: 0.387730, acc.: 82.81%] [G loss: 2.632843]\n",
      "epoch:49 step:38382 [D loss: 0.349314, acc.: 85.94%] [G loss: 3.035802]\n",
      "epoch:49 step:38383 [D loss: 0.243949, acc.: 90.62%] [G loss: 2.981037]\n",
      "epoch:49 step:38384 [D loss: 0.355593, acc.: 85.16%] [G loss: 3.407374]\n",
      "epoch:49 step:38385 [D loss: 0.415410, acc.: 81.25%] [G loss: 3.986193]\n",
      "epoch:49 step:38386 [D loss: 0.397726, acc.: 81.25%] [G loss: 3.085823]\n",
      "epoch:49 step:38387 [D loss: 0.252914, acc.: 89.84%] [G loss: 3.431836]\n",
      "epoch:49 step:38388 [D loss: 0.344562, acc.: 89.06%] [G loss: 2.980969]\n",
      "epoch:49 step:38389 [D loss: 0.321394, acc.: 87.50%] [G loss: 3.176668]\n",
      "epoch:49 step:38390 [D loss: 0.329238, acc.: 88.28%] [G loss: 3.272053]\n",
      "epoch:49 step:38391 [D loss: 0.341605, acc.: 85.94%] [G loss: 3.264852]\n",
      "epoch:49 step:38392 [D loss: 0.310564, acc.: 83.59%] [G loss: 2.685184]\n",
      "epoch:49 step:38393 [D loss: 0.439990, acc.: 82.03%] [G loss: 3.527477]\n",
      "epoch:49 step:38394 [D loss: 0.462366, acc.: 76.56%] [G loss: 4.693659]\n",
      "epoch:49 step:38395 [D loss: 0.463950, acc.: 78.91%] [G loss: 3.168528]\n",
      "epoch:49 step:38396 [D loss: 0.288483, acc.: 85.94%] [G loss: 3.230889]\n",
      "epoch:49 step:38397 [D loss: 0.445956, acc.: 81.25%] [G loss: 2.909322]\n",
      "epoch:49 step:38398 [D loss: 0.235350, acc.: 91.41%] [G loss: 3.720233]\n",
      "epoch:49 step:38399 [D loss: 0.276478, acc.: 89.84%] [G loss: 3.811123]\n",
      "epoch:49 step:38400 [D loss: 0.307926, acc.: 83.59%] [G loss: 4.501493]\n",
      "##############\n",
      "[0.86613429 0.87011744 0.82942887 0.79327857 0.75248203 0.83069956\n",
      " 0.86703246 0.84192847 0.81816143 0.81927817]\n",
      "##########\n",
      "epoch:49 step:38401 [D loss: 0.465542, acc.: 81.25%] [G loss: 4.397311]\n",
      "epoch:49 step:38402 [D loss: 0.314117, acc.: 88.28%] [G loss: 3.902954]\n",
      "epoch:49 step:38403 [D loss: 0.235223, acc.: 88.28%] [G loss: 3.851038]\n",
      "epoch:49 step:38404 [D loss: 0.310280, acc.: 85.94%] [G loss: 5.135241]\n",
      "epoch:49 step:38405 [D loss: 0.257647, acc.: 89.84%] [G loss: 3.259232]\n",
      "epoch:49 step:38406 [D loss: 0.296536, acc.: 85.16%] [G loss: 3.840819]\n",
      "epoch:49 step:38407 [D loss: 0.369408, acc.: 77.34%] [G loss: 3.930529]\n",
      "epoch:49 step:38408 [D loss: 0.282151, acc.: 89.06%] [G loss: 3.442589]\n",
      "epoch:49 step:38409 [D loss: 0.314862, acc.: 85.16%] [G loss: 3.418806]\n",
      "epoch:49 step:38410 [D loss: 0.328385, acc.: 82.81%] [G loss: 3.571056]\n",
      "epoch:49 step:38411 [D loss: 0.339286, acc.: 82.81%] [G loss: 3.452372]\n",
      "epoch:49 step:38412 [D loss: 0.225247, acc.: 90.62%] [G loss: 4.408796]\n",
      "epoch:49 step:38413 [D loss: 0.323812, acc.: 85.16%] [G loss: 3.998864]\n",
      "epoch:49 step:38414 [D loss: 0.346639, acc.: 82.81%] [G loss: 2.913208]\n",
      "epoch:49 step:38415 [D loss: 0.321803, acc.: 85.16%] [G loss: 3.088880]\n",
      "epoch:49 step:38416 [D loss: 0.382833, acc.: 82.03%] [G loss: 3.269472]\n",
      "epoch:49 step:38417 [D loss: 0.359444, acc.: 87.50%] [G loss: 3.671926]\n",
      "epoch:49 step:38418 [D loss: 0.390187, acc.: 82.03%] [G loss: 3.317986]\n",
      "epoch:49 step:38419 [D loss: 0.324266, acc.: 85.94%] [G loss: 3.514390]\n",
      "epoch:49 step:38420 [D loss: 0.359394, acc.: 82.03%] [G loss: 3.383376]\n",
      "epoch:49 step:38421 [D loss: 0.382994, acc.: 84.38%] [G loss: 2.892117]\n",
      "epoch:49 step:38422 [D loss: 0.270734, acc.: 89.06%] [G loss: 3.633741]\n",
      "epoch:49 step:38423 [D loss: 0.363080, acc.: 84.38%] [G loss: 3.785191]\n",
      "epoch:49 step:38424 [D loss: 0.254347, acc.: 89.06%] [G loss: 2.916071]\n",
      "epoch:49 step:38425 [D loss: 0.232344, acc.: 88.28%] [G loss: 3.061751]\n",
      "epoch:49 step:38426 [D loss: 0.277539, acc.: 86.72%] [G loss: 5.654019]\n",
      "epoch:49 step:38427 [D loss: 0.344340, acc.: 84.38%] [G loss: 2.924323]\n",
      "epoch:49 step:38428 [D loss: 0.220349, acc.: 89.06%] [G loss: 4.032392]\n",
      "epoch:49 step:38429 [D loss: 0.353433, acc.: 85.16%] [G loss: 3.957210]\n",
      "epoch:49 step:38430 [D loss: 0.278496, acc.: 86.72%] [G loss: 2.897722]\n",
      "epoch:49 step:38431 [D loss: 0.333335, acc.: 82.81%] [G loss: 3.139938]\n",
      "epoch:49 step:38432 [D loss: 0.260887, acc.: 86.72%] [G loss: 3.143174]\n",
      "epoch:49 step:38433 [D loss: 0.365085, acc.: 84.38%] [G loss: 3.060004]\n",
      "epoch:49 step:38434 [D loss: 0.394812, acc.: 83.59%] [G loss: 2.768368]\n",
      "epoch:49 step:38435 [D loss: 0.300798, acc.: 86.72%] [G loss: 3.238235]\n",
      "epoch:49 step:38436 [D loss: 0.299253, acc.: 90.62%] [G loss: 3.799587]\n",
      "epoch:49 step:38437 [D loss: 0.269782, acc.: 88.28%] [G loss: 3.333201]\n",
      "epoch:49 step:38438 [D loss: 0.309000, acc.: 82.81%] [G loss: 3.522914]\n",
      "epoch:49 step:38439 [D loss: 0.346026, acc.: 83.59%] [G loss: 3.498052]\n",
      "epoch:49 step:38440 [D loss: 0.260486, acc.: 92.19%] [G loss: 3.405330]\n",
      "epoch:49 step:38441 [D loss: 0.336445, acc.: 87.50%] [G loss: 3.115489]\n",
      "epoch:49 step:38442 [D loss: 0.340353, acc.: 85.16%] [G loss: 3.702331]\n",
      "epoch:49 step:38443 [D loss: 0.335605, acc.: 85.94%] [G loss: 3.314037]\n",
      "epoch:49 step:38444 [D loss: 0.279689, acc.: 89.06%] [G loss: 2.636160]\n",
      "epoch:49 step:38445 [D loss: 0.277932, acc.: 85.94%] [G loss: 2.866352]\n",
      "epoch:49 step:38446 [D loss: 0.348873, acc.: 82.03%] [G loss: 2.814948]\n",
      "epoch:49 step:38447 [D loss: 0.359828, acc.: 84.38%] [G loss: 2.859921]\n",
      "epoch:49 step:38448 [D loss: 0.362805, acc.: 82.81%] [G loss: 2.868613]\n",
      "epoch:49 step:38449 [D loss: 0.345494, acc.: 86.72%] [G loss: 2.777424]\n",
      "epoch:49 step:38450 [D loss: 0.274044, acc.: 87.50%] [G loss: 3.201686]\n",
      "epoch:49 step:38451 [D loss: 0.365088, acc.: 83.59%] [G loss: 3.215558]\n",
      "epoch:49 step:38452 [D loss: 0.285280, acc.: 89.84%] [G loss: 3.024297]\n",
      "epoch:49 step:38453 [D loss: 0.330898, acc.: 84.38%] [G loss: 3.033609]\n",
      "epoch:49 step:38454 [D loss: 0.303698, acc.: 84.38%] [G loss: 2.777546]\n",
      "epoch:49 step:38455 [D loss: 0.345192, acc.: 85.94%] [G loss: 2.865422]\n",
      "epoch:49 step:38456 [D loss: 0.315750, acc.: 86.72%] [G loss: 3.022654]\n",
      "epoch:49 step:38457 [D loss: 0.321780, acc.: 85.16%] [G loss: 3.706195]\n",
      "epoch:49 step:38458 [D loss: 0.292199, acc.: 83.59%] [G loss: 3.248042]\n",
      "epoch:49 step:38459 [D loss: 0.252750, acc.: 86.72%] [G loss: 4.721228]\n",
      "epoch:49 step:38460 [D loss: 0.233998, acc.: 92.19%] [G loss: 3.834612]\n",
      "epoch:49 step:38461 [D loss: 0.239529, acc.: 88.28%] [G loss: 5.168481]\n",
      "epoch:49 step:38462 [D loss: 0.277397, acc.: 86.72%] [G loss: 4.019932]\n",
      "epoch:49 step:38463 [D loss: 0.208134, acc.: 91.41%] [G loss: 4.278404]\n",
      "epoch:49 step:38464 [D loss: 0.327335, acc.: 82.81%] [G loss: 3.910686]\n",
      "epoch:49 step:38465 [D loss: 0.292136, acc.: 91.41%] [G loss: 3.315507]\n",
      "epoch:49 step:38466 [D loss: 0.467011, acc.: 77.34%] [G loss: 5.590251]\n",
      "epoch:49 step:38467 [D loss: 0.656063, acc.: 71.88%] [G loss: 4.666189]\n",
      "epoch:49 step:38468 [D loss: 0.298674, acc.: 86.72%] [G loss: 4.502174]\n",
      "epoch:49 step:38469 [D loss: 0.492951, acc.: 77.34%] [G loss: 4.572491]\n",
      "epoch:49 step:38470 [D loss: 0.447716, acc.: 82.03%] [G loss: 3.157464]\n",
      "epoch:49 step:38471 [D loss: 0.336619, acc.: 84.38%] [G loss: 3.210848]\n",
      "epoch:49 step:38472 [D loss: 0.258523, acc.: 89.06%] [G loss: 6.588370]\n",
      "epoch:49 step:38473 [D loss: 0.208293, acc.: 91.41%] [G loss: 4.594775]\n",
      "epoch:49 step:38474 [D loss: 0.201083, acc.: 91.41%] [G loss: 4.965171]\n",
      "epoch:49 step:38475 [D loss: 0.262012, acc.: 87.50%] [G loss: 3.067248]\n",
      "epoch:49 step:38476 [D loss: 0.278977, acc.: 85.94%] [G loss: 3.721008]\n",
      "epoch:49 step:38477 [D loss: 0.211015, acc.: 92.19%] [G loss: 4.036422]\n",
      "epoch:49 step:38478 [D loss: 0.287452, acc.: 87.50%] [G loss: 2.849533]\n",
      "epoch:49 step:38479 [D loss: 0.274990, acc.: 88.28%] [G loss: 3.338706]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:49 step:38480 [D loss: 0.399281, acc.: 84.38%] [G loss: 3.214844]\n",
      "epoch:49 step:38481 [D loss: 0.266406, acc.: 89.06%] [G loss: 2.740890]\n",
      "epoch:49 step:38482 [D loss: 0.396214, acc.: 82.81%] [G loss: 2.595872]\n",
      "epoch:49 step:38483 [D loss: 0.344082, acc.: 85.16%] [G loss: 2.707899]\n",
      "epoch:49 step:38484 [D loss: 0.404009, acc.: 83.59%] [G loss: 2.258902]\n",
      "epoch:49 step:38485 [D loss: 0.325880, acc.: 80.47%] [G loss: 3.530309]\n",
      "epoch:49 step:38486 [D loss: 0.255660, acc.: 86.72%] [G loss: 5.348198]\n",
      "epoch:49 step:38487 [D loss: 0.290592, acc.: 87.50%] [G loss: 4.632569]\n",
      "epoch:49 step:38488 [D loss: 0.248187, acc.: 90.62%] [G loss: 2.994806]\n",
      "epoch:49 step:38489 [D loss: 0.354527, acc.: 86.72%] [G loss: 5.027160]\n",
      "epoch:49 step:38490 [D loss: 0.265020, acc.: 89.06%] [G loss: 4.471312]\n",
      "epoch:49 step:38491 [D loss: 0.291370, acc.: 87.50%] [G loss: 2.847691]\n",
      "epoch:49 step:38492 [D loss: 0.328054, acc.: 85.16%] [G loss: 2.800883]\n",
      "epoch:49 step:38493 [D loss: 0.249181, acc.: 89.84%] [G loss: 3.149659]\n",
      "epoch:49 step:38494 [D loss: 0.353933, acc.: 82.81%] [G loss: 2.892713]\n",
      "epoch:49 step:38495 [D loss: 0.363881, acc.: 80.47%] [G loss: 3.181591]\n",
      "epoch:49 step:38496 [D loss: 0.355970, acc.: 82.81%] [G loss: 3.229397]\n",
      "epoch:49 step:38497 [D loss: 0.355598, acc.: 81.25%] [G loss: 3.082616]\n",
      "epoch:49 step:38498 [D loss: 0.409218, acc.: 80.47%] [G loss: 2.460723]\n",
      "epoch:49 step:38499 [D loss: 0.235730, acc.: 91.41%] [G loss: 3.279260]\n",
      "epoch:49 step:38500 [D loss: 0.259513, acc.: 91.41%] [G loss: 2.909652]\n",
      "epoch:49 step:38501 [D loss: 0.282730, acc.: 84.38%] [G loss: 3.266582]\n",
      "epoch:49 step:38502 [D loss: 0.248560, acc.: 88.28%] [G loss: 3.258077]\n",
      "epoch:49 step:38503 [D loss: 0.368720, acc.: 80.47%] [G loss: 2.451301]\n",
      "epoch:49 step:38504 [D loss: 0.302715, acc.: 87.50%] [G loss: 2.946562]\n",
      "epoch:49 step:38505 [D loss: 0.380805, acc.: 85.16%] [G loss: 2.722588]\n",
      "epoch:49 step:38506 [D loss: 0.304074, acc.: 86.72%] [G loss: 3.131022]\n",
      "epoch:49 step:38507 [D loss: 0.350403, acc.: 84.38%] [G loss: 2.907946]\n",
      "epoch:49 step:38508 [D loss: 0.317496, acc.: 83.59%] [G loss: 3.703535]\n",
      "epoch:49 step:38509 [D loss: 0.361099, acc.: 83.59%] [G loss: 3.961933]\n",
      "epoch:49 step:38510 [D loss: 0.315897, acc.: 83.59%] [G loss: 3.174628]\n",
      "epoch:49 step:38511 [D loss: 0.264026, acc.: 89.06%] [G loss: 2.767006]\n",
      "epoch:49 step:38512 [D loss: 0.295602, acc.: 85.94%] [G loss: 3.150686]\n",
      "epoch:49 step:38513 [D loss: 0.227445, acc.: 87.50%] [G loss: 3.409643]\n",
      "epoch:49 step:38514 [D loss: 0.239607, acc.: 90.62%] [G loss: 3.335419]\n",
      "epoch:49 step:38515 [D loss: 0.247392, acc.: 89.84%] [G loss: 3.507508]\n",
      "epoch:49 step:38516 [D loss: 0.319987, acc.: 85.16%] [G loss: 3.220212]\n",
      "epoch:49 step:38517 [D loss: 0.279169, acc.: 86.72%] [G loss: 3.224283]\n",
      "epoch:49 step:38518 [D loss: 0.304597, acc.: 88.28%] [G loss: 3.073784]\n",
      "epoch:49 step:38519 [D loss: 0.314354, acc.: 84.38%] [G loss: 2.800115]\n",
      "epoch:49 step:38520 [D loss: 0.275188, acc.: 86.72%] [G loss: 2.947145]\n",
      "epoch:49 step:38521 [D loss: 0.265620, acc.: 88.28%] [G loss: 3.300526]\n",
      "epoch:49 step:38522 [D loss: 0.287506, acc.: 86.72%] [G loss: 4.061765]\n",
      "epoch:49 step:38523 [D loss: 0.250203, acc.: 90.62%] [G loss: 2.924129]\n",
      "epoch:49 step:38524 [D loss: 0.419290, acc.: 80.47%] [G loss: 4.064378]\n",
      "epoch:49 step:38525 [D loss: 0.275018, acc.: 84.38%] [G loss: 3.615193]\n",
      "epoch:49 step:38526 [D loss: 0.366567, acc.: 85.16%] [G loss: 2.773503]\n",
      "epoch:49 step:38527 [D loss: 0.235951, acc.: 88.28%] [G loss: 3.206135]\n",
      "epoch:49 step:38528 [D loss: 0.250418, acc.: 85.94%] [G loss: 2.765275]\n",
      "epoch:49 step:38529 [D loss: 0.271845, acc.: 89.84%] [G loss: 3.501111]\n",
      "epoch:49 step:38530 [D loss: 0.302795, acc.: 87.50%] [G loss: 3.047087]\n",
      "epoch:49 step:38531 [D loss: 0.290527, acc.: 87.50%] [G loss: 3.811026]\n",
      "epoch:49 step:38532 [D loss: 0.249735, acc.: 88.28%] [G loss: 3.302228]\n",
      "epoch:49 step:38533 [D loss: 0.307776, acc.: 84.38%] [G loss: 3.333887]\n",
      "epoch:49 step:38534 [D loss: 0.222363, acc.: 90.62%] [G loss: 3.228305]\n",
      "epoch:49 step:38535 [D loss: 0.277388, acc.: 88.28%] [G loss: 2.966296]\n",
      "epoch:49 step:38536 [D loss: 0.254051, acc.: 87.50%] [G loss: 2.617907]\n",
      "epoch:49 step:38537 [D loss: 0.261602, acc.: 89.84%] [G loss: 2.987202]\n",
      "epoch:49 step:38538 [D loss: 0.296282, acc.: 85.94%] [G loss: 2.981029]\n",
      "epoch:49 step:38539 [D loss: 0.336118, acc.: 83.59%] [G loss: 3.293667]\n",
      "epoch:49 step:38540 [D loss: 0.349797, acc.: 82.03%] [G loss: 4.043085]\n",
      "epoch:49 step:38541 [D loss: 0.238041, acc.: 89.84%] [G loss: 3.887084]\n",
      "epoch:49 step:38542 [D loss: 0.251425, acc.: 88.28%] [G loss: 4.438687]\n",
      "epoch:49 step:38543 [D loss: 0.315852, acc.: 82.03%] [G loss: 4.587467]\n",
      "epoch:49 step:38544 [D loss: 0.273536, acc.: 87.50%] [G loss: 4.213288]\n",
      "epoch:49 step:38545 [D loss: 0.359877, acc.: 78.91%] [G loss: 3.506137]\n",
      "epoch:49 step:38546 [D loss: 0.389151, acc.: 83.59%] [G loss: 3.362031]\n",
      "epoch:49 step:38547 [D loss: 0.331732, acc.: 83.59%] [G loss: 2.883210]\n",
      "epoch:49 step:38548 [D loss: 0.302067, acc.: 86.72%] [G loss: 3.099341]\n",
      "epoch:49 step:38549 [D loss: 0.353561, acc.: 78.91%] [G loss: 2.966531]\n",
      "epoch:49 step:38550 [D loss: 0.321925, acc.: 84.38%] [G loss: 2.946341]\n",
      "epoch:49 step:38551 [D loss: 0.362926, acc.: 84.38%] [G loss: 5.238886]\n",
      "epoch:49 step:38552 [D loss: 0.200975, acc.: 91.41%] [G loss: 4.493501]\n",
      "epoch:49 step:38553 [D loss: 0.215547, acc.: 89.06%] [G loss: 6.356698]\n",
      "epoch:49 step:38554 [D loss: 0.313525, acc.: 83.59%] [G loss: 4.879532]\n",
      "epoch:49 step:38555 [D loss: 0.287725, acc.: 86.72%] [G loss: 5.004600]\n",
      "epoch:49 step:38556 [D loss: 0.210441, acc.: 92.19%] [G loss: 3.532032]\n",
      "epoch:49 step:38557 [D loss: 0.285816, acc.: 89.06%] [G loss: 3.437047]\n",
      "epoch:49 step:38558 [D loss: 0.337830, acc.: 85.16%] [G loss: 3.303519]\n",
      "epoch:49 step:38559 [D loss: 0.266242, acc.: 87.50%] [G loss: 3.022991]\n",
      "epoch:49 step:38560 [D loss: 0.339674, acc.: 84.38%] [G loss: 3.131280]\n",
      "epoch:49 step:38561 [D loss: 0.236848, acc.: 91.41%] [G loss: 3.824440]\n",
      "epoch:49 step:38562 [D loss: 0.214577, acc.: 93.75%] [G loss: 2.829378]\n",
      "epoch:49 step:38563 [D loss: 0.324723, acc.: 85.16%] [G loss: 2.759106]\n",
      "epoch:49 step:38564 [D loss: 0.274014, acc.: 86.72%] [G loss: 3.010540]\n",
      "epoch:49 step:38565 [D loss: 0.316076, acc.: 88.28%] [G loss: 4.515624]\n",
      "epoch:49 step:38566 [D loss: 0.386575, acc.: 80.47%] [G loss: 2.380478]\n",
      "epoch:49 step:38567 [D loss: 0.234624, acc.: 92.97%] [G loss: 3.698379]\n",
      "epoch:49 step:38568 [D loss: 0.256797, acc.: 88.28%] [G loss: 3.697462]\n",
      "epoch:49 step:38569 [D loss: 0.421683, acc.: 80.47%] [G loss: 3.819736]\n",
      "epoch:49 step:38570 [D loss: 0.283661, acc.: 87.50%] [G loss: 4.040236]\n",
      "epoch:49 step:38571 [D loss: 0.297844, acc.: 84.38%] [G loss: 2.675445]\n",
      "epoch:49 step:38572 [D loss: 0.420978, acc.: 78.12%] [G loss: 3.918128]\n",
      "epoch:49 step:38573 [D loss: 0.359291, acc.: 86.72%] [G loss: 3.439849]\n",
      "epoch:49 step:38574 [D loss: 0.356299, acc.: 84.38%] [G loss: 4.480808]\n",
      "epoch:49 step:38575 [D loss: 0.274765, acc.: 86.72%] [G loss: 6.007490]\n",
      "epoch:49 step:38576 [D loss: 0.376360, acc.: 80.47%] [G loss: 4.122993]\n",
      "epoch:49 step:38577 [D loss: 0.261126, acc.: 90.62%] [G loss: 3.753846]\n",
      "epoch:49 step:38578 [D loss: 0.397653, acc.: 83.59%] [G loss: 3.087280]\n",
      "epoch:49 step:38579 [D loss: 0.318871, acc.: 85.16%] [G loss: 3.235501]\n",
      "epoch:49 step:38580 [D loss: 0.321906, acc.: 85.94%] [G loss: 3.094825]\n",
      "epoch:49 step:38581 [D loss: 0.316528, acc.: 85.16%] [G loss: 3.276974]\n",
      "epoch:49 step:38582 [D loss: 0.401756, acc.: 78.12%] [G loss: 5.636631]\n",
      "epoch:49 step:38583 [D loss: 0.491957, acc.: 79.69%] [G loss: 4.023417]\n",
      "epoch:49 step:38584 [D loss: 0.454224, acc.: 78.12%] [G loss: 2.687579]\n",
      "epoch:49 step:38585 [D loss: 0.285287, acc.: 85.94%] [G loss: 2.750062]\n",
      "epoch:49 step:38586 [D loss: 0.378201, acc.: 85.16%] [G loss: 5.386690]\n",
      "epoch:49 step:38587 [D loss: 0.410005, acc.: 82.81%] [G loss: 3.873077]\n",
      "epoch:49 step:38588 [D loss: 0.330773, acc.: 84.38%] [G loss: 3.475393]\n",
      "epoch:49 step:38589 [D loss: 0.412290, acc.: 78.91%] [G loss: 4.585311]\n",
      "epoch:49 step:38590 [D loss: 0.282826, acc.: 86.72%] [G loss: 3.812142]\n",
      "epoch:49 step:38591 [D loss: 0.330104, acc.: 84.38%] [G loss: 5.240169]\n",
      "epoch:49 step:38592 [D loss: 0.349024, acc.: 87.50%] [G loss: 4.488155]\n",
      "epoch:49 step:38593 [D loss: 0.318749, acc.: 85.16%] [G loss: 2.939330]\n",
      "epoch:49 step:38594 [D loss: 0.413706, acc.: 82.03%] [G loss: 3.172818]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:49 step:38595 [D loss: 0.279770, acc.: 88.28%] [G loss: 2.299343]\n",
      "epoch:49 step:38596 [D loss: 0.348635, acc.: 84.38%] [G loss: 3.525058]\n",
      "epoch:49 step:38597 [D loss: 0.360846, acc.: 85.16%] [G loss: 3.155296]\n",
      "epoch:49 step:38598 [D loss: 0.310789, acc.: 85.16%] [G loss: 3.170573]\n",
      "epoch:49 step:38599 [D loss: 0.244694, acc.: 89.84%] [G loss: 2.862066]\n",
      "epoch:49 step:38600 [D loss: 0.301055, acc.: 86.72%] [G loss: 2.359544]\n",
      "##############\n",
      "[0.85701823 0.86364706 0.8164577  0.81104416 0.7763444  0.83127803\n",
      " 0.87772699 0.80899449 0.8195672  0.81530491]\n",
      "##########\n",
      "epoch:49 step:38601 [D loss: 0.300656, acc.: 89.06%] [G loss: 3.335723]\n",
      "epoch:49 step:38602 [D loss: 0.396423, acc.: 82.03%] [G loss: 3.713251]\n",
      "epoch:49 step:38603 [D loss: 0.449398, acc.: 78.91%] [G loss: 3.169914]\n",
      "epoch:49 step:38604 [D loss: 0.299663, acc.: 86.72%] [G loss: 3.328427]\n",
      "epoch:49 step:38605 [D loss: 0.382566, acc.: 82.03%] [G loss: 3.255327]\n",
      "epoch:49 step:38606 [D loss: 0.343112, acc.: 83.59%] [G loss: 3.530518]\n",
      "epoch:49 step:38607 [D loss: 0.223911, acc.: 90.62%] [G loss: 3.755237]\n",
      "epoch:49 step:38608 [D loss: 0.296075, acc.: 86.72%] [G loss: 2.391952]\n",
      "epoch:49 step:38609 [D loss: 0.219176, acc.: 92.97%] [G loss: 3.596190]\n",
      "epoch:49 step:38610 [D loss: 0.304493, acc.: 87.50%] [G loss: 3.401796]\n",
      "epoch:49 step:38611 [D loss: 0.342429, acc.: 86.72%] [G loss: 3.460922]\n",
      "epoch:49 step:38612 [D loss: 0.288943, acc.: 89.06%] [G loss: 4.283042]\n",
      "epoch:49 step:38613 [D loss: 0.322549, acc.: 82.03%] [G loss: 3.573862]\n",
      "epoch:49 step:38614 [D loss: 0.362282, acc.: 81.25%] [G loss: 3.721975]\n",
      "epoch:49 step:38615 [D loss: 0.285002, acc.: 86.72%] [G loss: 2.937285]\n",
      "epoch:49 step:38616 [D loss: 0.308387, acc.: 88.28%] [G loss: 3.513767]\n",
      "epoch:49 step:38617 [D loss: 0.221987, acc.: 89.84%] [G loss: 3.034177]\n",
      "epoch:49 step:38618 [D loss: 0.374429, acc.: 85.16%] [G loss: 2.928895]\n",
      "epoch:49 step:38619 [D loss: 0.261383, acc.: 89.84%] [G loss: 2.726456]\n",
      "epoch:49 step:38620 [D loss: 0.296244, acc.: 86.72%] [G loss: 2.671448]\n",
      "epoch:49 step:38621 [D loss: 0.373872, acc.: 82.81%] [G loss: 2.925744]\n",
      "epoch:49 step:38622 [D loss: 0.264108, acc.: 90.62%] [G loss: 3.490492]\n",
      "epoch:49 step:38623 [D loss: 0.339896, acc.: 84.38%] [G loss: 3.090768]\n",
      "epoch:49 step:38624 [D loss: 0.255166, acc.: 89.84%] [G loss: 4.788343]\n",
      "epoch:49 step:38625 [D loss: 0.338121, acc.: 82.03%] [G loss: 4.402899]\n",
      "epoch:49 step:38626 [D loss: 0.332483, acc.: 85.94%] [G loss: 2.595093]\n",
      "epoch:49 step:38627 [D loss: 0.270873, acc.: 88.28%] [G loss: 2.781623]\n",
      "epoch:49 step:38628 [D loss: 0.245797, acc.: 88.28%] [G loss: 3.204486]\n",
      "epoch:49 step:38629 [D loss: 0.421120, acc.: 82.81%] [G loss: 2.612283]\n",
      "epoch:49 step:38630 [D loss: 0.307691, acc.: 84.38%] [G loss: 3.135734]\n",
      "epoch:49 step:38631 [D loss: 0.296252, acc.: 89.06%] [G loss: 2.902325]\n",
      "epoch:49 step:38632 [D loss: 0.319020, acc.: 85.94%] [G loss: 3.748173]\n",
      "epoch:49 step:38633 [D loss: 0.287713, acc.: 87.50%] [G loss: 3.324529]\n",
      "epoch:49 step:38634 [D loss: 0.184407, acc.: 94.53%] [G loss: 4.492266]\n",
      "epoch:49 step:38635 [D loss: 0.301120, acc.: 89.06%] [G loss: 3.819657]\n",
      "epoch:49 step:38636 [D loss: 0.284729, acc.: 85.16%] [G loss: 4.128138]\n",
      "epoch:49 step:38637 [D loss: 0.296830, acc.: 84.38%] [G loss: 6.533205]\n",
      "epoch:49 step:38638 [D loss: 0.286621, acc.: 82.81%] [G loss: 4.699863]\n",
      "epoch:49 step:38639 [D loss: 0.250502, acc.: 92.19%] [G loss: 6.703231]\n",
      "epoch:49 step:38640 [D loss: 0.217301, acc.: 91.41%] [G loss: 3.366370]\n",
      "epoch:49 step:38641 [D loss: 0.285734, acc.: 85.16%] [G loss: 2.975194]\n",
      "epoch:49 step:38642 [D loss: 0.263088, acc.: 86.72%] [G loss: 3.073686]\n",
      "epoch:49 step:38643 [D loss: 0.471582, acc.: 80.47%] [G loss: 3.199217]\n",
      "epoch:49 step:38644 [D loss: 0.282245, acc.: 87.50%] [G loss: 3.807285]\n",
      "epoch:49 step:38645 [D loss: 0.242594, acc.: 91.41%] [G loss: 3.257080]\n",
      "epoch:49 step:38646 [D loss: 0.299849, acc.: 84.38%] [G loss: 3.389658]\n",
      "epoch:49 step:38647 [D loss: 0.391826, acc.: 79.69%] [G loss: 3.427762]\n",
      "epoch:49 step:38648 [D loss: 0.378518, acc.: 82.03%] [G loss: 2.782761]\n",
      "epoch:49 step:38649 [D loss: 0.394204, acc.: 80.47%] [G loss: 3.184532]\n",
      "epoch:49 step:38650 [D loss: 0.346541, acc.: 82.81%] [G loss: 2.983585]\n",
      "epoch:49 step:38651 [D loss: 0.298904, acc.: 85.16%] [G loss: 3.453640]\n",
      "epoch:49 step:38652 [D loss: 0.272225, acc.: 89.84%] [G loss: 3.390206]\n",
      "epoch:49 step:38653 [D loss: 0.270296, acc.: 87.50%] [G loss: 3.690171]\n",
      "epoch:49 step:38654 [D loss: 0.256824, acc.: 89.06%] [G loss: 2.827869]\n",
      "epoch:49 step:38655 [D loss: 0.395797, acc.: 83.59%] [G loss: 3.051371]\n",
      "epoch:49 step:38656 [D loss: 0.223867, acc.: 90.62%] [G loss: 3.060257]\n",
      "epoch:49 step:38657 [D loss: 0.199242, acc.: 93.75%] [G loss: 3.258071]\n",
      "epoch:49 step:38658 [D loss: 0.315901, acc.: 84.38%] [G loss: 3.217519]\n",
      "epoch:49 step:38659 [D loss: 0.230553, acc.: 90.62%] [G loss: 3.237147]\n",
      "epoch:49 step:38660 [D loss: 0.335991, acc.: 86.72%] [G loss: 3.252305]\n",
      "epoch:49 step:38661 [D loss: 0.211203, acc.: 91.41%] [G loss: 4.626436]\n",
      "epoch:49 step:38662 [D loss: 0.321121, acc.: 86.72%] [G loss: 4.002680]\n",
      "epoch:49 step:38663 [D loss: 0.297154, acc.: 86.72%] [G loss: 2.974109]\n",
      "epoch:49 step:38664 [D loss: 0.297167, acc.: 87.50%] [G loss: 3.756695]\n",
      "epoch:49 step:38665 [D loss: 0.250161, acc.: 86.72%] [G loss: 3.871950]\n",
      "epoch:49 step:38666 [D loss: 0.267339, acc.: 88.28%] [G loss: 3.552704]\n",
      "epoch:49 step:38667 [D loss: 0.386524, acc.: 82.03%] [G loss: 3.615282]\n",
      "epoch:49 step:38668 [D loss: 0.250168, acc.: 90.62%] [G loss: 3.250417]\n",
      "epoch:49 step:38669 [D loss: 0.341523, acc.: 88.28%] [G loss: 3.340146]\n",
      "epoch:49 step:38670 [D loss: 0.312259, acc.: 88.28%] [G loss: 2.416411]\n",
      "epoch:49 step:38671 [D loss: 0.292926, acc.: 91.41%] [G loss: 2.936584]\n",
      "epoch:49 step:38672 [D loss: 0.285824, acc.: 89.84%] [G loss: 3.419128]\n",
      "epoch:49 step:38673 [D loss: 0.305082, acc.: 85.16%] [G loss: 3.570849]\n",
      "epoch:49 step:38674 [D loss: 0.413319, acc.: 78.91%] [G loss: 4.373692]\n",
      "epoch:49 step:38675 [D loss: 0.458343, acc.: 81.25%] [G loss: 3.818049]\n",
      "epoch:49 step:38676 [D loss: 0.297912, acc.: 87.50%] [G loss: 3.807789]\n",
      "epoch:49 step:38677 [D loss: 0.280040, acc.: 88.28%] [G loss: 2.825450]\n",
      "epoch:49 step:38678 [D loss: 0.295789, acc.: 89.06%] [G loss: 2.854612]\n",
      "epoch:49 step:38679 [D loss: 0.279355, acc.: 83.59%] [G loss: 4.617182]\n",
      "epoch:49 step:38680 [D loss: 0.361735, acc.: 85.16%] [G loss: 3.531016]\n",
      "epoch:49 step:38681 [D loss: 0.386860, acc.: 78.91%] [G loss: 4.429471]\n",
      "epoch:49 step:38682 [D loss: 0.338817, acc.: 83.59%] [G loss: 2.845549]\n",
      "epoch:49 step:38683 [D loss: 0.249435, acc.: 87.50%] [G loss: 3.434518]\n",
      "epoch:49 step:38684 [D loss: 0.328373, acc.: 85.94%] [G loss: 2.801740]\n",
      "epoch:49 step:38685 [D loss: 0.197652, acc.: 90.62%] [G loss: 3.359028]\n",
      "epoch:49 step:38686 [D loss: 0.515401, acc.: 78.12%] [G loss: 3.537477]\n",
      "epoch:49 step:38687 [D loss: 0.267548, acc.: 89.06%] [G loss: 2.823725]\n",
      "epoch:49 step:38688 [D loss: 0.356892, acc.: 85.94%] [G loss: 3.152191]\n",
      "epoch:49 step:38689 [D loss: 0.370332, acc.: 78.12%] [G loss: 2.561253]\n",
      "epoch:49 step:38690 [D loss: 0.334772, acc.: 85.16%] [G loss: 3.781004]\n",
      "epoch:49 step:38691 [D loss: 0.296793, acc.: 86.72%] [G loss: 2.990593]\n",
      "epoch:49 step:38692 [D loss: 0.401330, acc.: 81.25%] [G loss: 3.192133]\n",
      "epoch:49 step:38693 [D loss: 0.387691, acc.: 79.69%] [G loss: 3.313296]\n",
      "epoch:49 step:38694 [D loss: 0.293860, acc.: 88.28%] [G loss: 3.051051]\n",
      "epoch:49 step:38695 [D loss: 0.315668, acc.: 87.50%] [G loss: 2.526766]\n",
      "epoch:49 step:38696 [D loss: 0.218399, acc.: 92.19%] [G loss: 2.638115]\n",
      "epoch:49 step:38697 [D loss: 0.330209, acc.: 82.81%] [G loss: 2.618362]\n",
      "epoch:49 step:38698 [D loss: 0.308567, acc.: 84.38%] [G loss: 2.756721]\n",
      "epoch:49 step:38699 [D loss: 0.288144, acc.: 89.06%] [G loss: 2.592088]\n",
      "epoch:49 step:38700 [D loss: 0.250077, acc.: 91.41%] [G loss: 3.831079]\n",
      "epoch:49 step:38701 [D loss: 0.334894, acc.: 88.28%] [G loss: 3.899482]\n",
      "epoch:49 step:38702 [D loss: 0.228050, acc.: 93.75%] [G loss: 3.442765]\n",
      "epoch:49 step:38703 [D loss: 0.222523, acc.: 91.41%] [G loss: 4.108905]\n",
      "epoch:49 step:38704 [D loss: 0.307007, acc.: 85.16%] [G loss: 4.170266]\n",
      "epoch:49 step:38705 [D loss: 0.372637, acc.: 82.81%] [G loss: 3.438521]\n",
      "epoch:49 step:38706 [D loss: 0.290429, acc.: 88.28%] [G loss: 3.783523]\n",
      "epoch:49 step:38707 [D loss: 0.301350, acc.: 86.72%] [G loss: 3.089509]\n",
      "epoch:49 step:38708 [D loss: 0.289824, acc.: 87.50%] [G loss: 3.139718]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:49 step:38709 [D loss: 0.297910, acc.: 89.06%] [G loss: 3.329345]\n",
      "epoch:49 step:38710 [D loss: 0.357277, acc.: 82.03%] [G loss: 3.035904]\n",
      "epoch:49 step:38711 [D loss: 0.251900, acc.: 89.06%] [G loss: 3.158366]\n",
      "epoch:49 step:38712 [D loss: 0.330853, acc.: 86.72%] [G loss: 2.942727]\n",
      "epoch:49 step:38713 [D loss: 0.253985, acc.: 87.50%] [G loss: 2.958361]\n",
      "epoch:49 step:38714 [D loss: 0.292125, acc.: 86.72%] [G loss: 3.621078]\n",
      "epoch:49 step:38715 [D loss: 0.309042, acc.: 83.59%] [G loss: 2.551534]\n",
      "epoch:49 step:38716 [D loss: 0.317550, acc.: 85.16%] [G loss: 2.494732]\n",
      "epoch:49 step:38717 [D loss: 0.388467, acc.: 81.25%] [G loss: 3.135513]\n",
      "epoch:49 step:38718 [D loss: 0.234481, acc.: 90.62%] [G loss: 3.056872]\n",
      "epoch:49 step:38719 [D loss: 0.264555, acc.: 86.72%] [G loss: 3.268542]\n",
      "epoch:49 step:38720 [D loss: 0.231622, acc.: 89.06%] [G loss: 3.627468]\n",
      "epoch:49 step:38721 [D loss: 0.238967, acc.: 88.28%] [G loss: 3.022273]\n",
      "epoch:49 step:38722 [D loss: 0.292518, acc.: 85.16%] [G loss: 3.578598]\n",
      "epoch:49 step:38723 [D loss: 0.279795, acc.: 89.06%] [G loss: 3.770366]\n",
      "epoch:49 step:38724 [D loss: 0.296311, acc.: 87.50%] [G loss: 4.707033]\n",
      "epoch:49 step:38725 [D loss: 0.267738, acc.: 92.97%] [G loss: 3.991987]\n",
      "epoch:49 step:38726 [D loss: 0.430083, acc.: 83.59%] [G loss: 3.394060]\n",
      "epoch:49 step:38727 [D loss: 0.219530, acc.: 91.41%] [G loss: 3.353383]\n",
      "epoch:49 step:38728 [D loss: 0.311889, acc.: 83.59%] [G loss: 3.735762]\n",
      "epoch:49 step:38729 [D loss: 0.366437, acc.: 85.16%] [G loss: 4.314009]\n",
      "epoch:49 step:38730 [D loss: 0.264126, acc.: 87.50%] [G loss: 3.750290]\n",
      "epoch:49 step:38731 [D loss: 0.270843, acc.: 89.06%] [G loss: 3.832716]\n",
      "epoch:49 step:38732 [D loss: 0.293446, acc.: 87.50%] [G loss: 2.986358]\n",
      "epoch:49 step:38733 [D loss: 0.270540, acc.: 92.97%] [G loss: 4.216331]\n",
      "epoch:49 step:38734 [D loss: 0.278272, acc.: 85.94%] [G loss: 3.419861]\n",
      "epoch:49 step:38735 [D loss: 0.289853, acc.: 85.94%] [G loss: 3.685892]\n",
      "epoch:49 step:38736 [D loss: 0.287909, acc.: 86.72%] [G loss: 2.637325]\n",
      "epoch:49 step:38737 [D loss: 0.298303, acc.: 89.84%] [G loss: 3.534920]\n",
      "epoch:49 step:38738 [D loss: 0.329448, acc.: 82.81%] [G loss: 4.246224]\n",
      "epoch:49 step:38739 [D loss: 0.275744, acc.: 87.50%] [G loss: 2.504371]\n",
      "epoch:49 step:38740 [D loss: 0.441009, acc.: 78.91%] [G loss: 3.424665]\n",
      "epoch:49 step:38741 [D loss: 0.298227, acc.: 85.16%] [G loss: 3.690077]\n",
      "epoch:49 step:38742 [D loss: 0.282393, acc.: 87.50%] [G loss: 4.213491]\n",
      "epoch:49 step:38743 [D loss: 0.324630, acc.: 85.16%] [G loss: 3.483832]\n",
      "epoch:49 step:38744 [D loss: 0.297682, acc.: 88.28%] [G loss: 3.633208]\n",
      "epoch:49 step:38745 [D loss: 0.272584, acc.: 91.41%] [G loss: 3.281598]\n",
      "epoch:49 step:38746 [D loss: 0.237594, acc.: 90.62%] [G loss: 3.182938]\n",
      "epoch:49 step:38747 [D loss: 0.226179, acc.: 92.19%] [G loss: 3.573430]\n",
      "epoch:49 step:38748 [D loss: 0.255810, acc.: 89.84%] [G loss: 3.949057]\n",
      "epoch:49 step:38749 [D loss: 0.240245, acc.: 89.84%] [G loss: 3.108761]\n",
      "epoch:49 step:38750 [D loss: 0.430650, acc.: 80.47%] [G loss: 2.988181]\n",
      "epoch:49 step:38751 [D loss: 0.368547, acc.: 82.03%] [G loss: 2.528674]\n",
      "epoch:49 step:38752 [D loss: 0.375849, acc.: 82.03%] [G loss: 2.725580]\n",
      "epoch:49 step:38753 [D loss: 0.389571, acc.: 82.03%] [G loss: 3.205344]\n",
      "epoch:49 step:38754 [D loss: 0.283073, acc.: 91.41%] [G loss: 3.268975]\n",
      "epoch:49 step:38755 [D loss: 0.222615, acc.: 89.84%] [G loss: 3.469342]\n",
      "epoch:49 step:38756 [D loss: 0.291370, acc.: 85.16%] [G loss: 4.276306]\n",
      "epoch:49 step:38757 [D loss: 0.299571, acc.: 86.72%] [G loss: 3.526040]\n",
      "epoch:49 step:38758 [D loss: 0.359457, acc.: 85.94%] [G loss: 3.488686]\n",
      "epoch:49 step:38759 [D loss: 0.405370, acc.: 82.03%] [G loss: 3.984057]\n",
      "epoch:49 step:38760 [D loss: 0.356517, acc.: 82.03%] [G loss: 4.477564]\n",
      "epoch:49 step:38761 [D loss: 0.422461, acc.: 79.69%] [G loss: 3.660287]\n",
      "epoch:49 step:38762 [D loss: 0.230613, acc.: 89.84%] [G loss: 4.464837]\n",
      "epoch:49 step:38763 [D loss: 0.222168, acc.: 92.19%] [G loss: 4.534821]\n",
      "epoch:49 step:38764 [D loss: 0.365494, acc.: 83.59%] [G loss: 3.989071]\n",
      "epoch:49 step:38765 [D loss: 0.243962, acc.: 92.19%] [G loss: 3.386841]\n",
      "epoch:49 step:38766 [D loss: 0.321683, acc.: 85.94%] [G loss: 3.378272]\n",
      "epoch:49 step:38767 [D loss: 0.314485, acc.: 86.72%] [G loss: 2.908960]\n",
      "epoch:49 step:38768 [D loss: 0.326359, acc.: 83.59%] [G loss: 2.832211]\n",
      "epoch:49 step:38769 [D loss: 0.372187, acc.: 85.16%] [G loss: 3.214924]\n",
      "epoch:49 step:38770 [D loss: 0.409164, acc.: 82.03%] [G loss: 2.982864]\n",
      "epoch:49 step:38771 [D loss: 0.246745, acc.: 91.41%] [G loss: 4.052100]\n",
      "epoch:49 step:38772 [D loss: 0.326128, acc.: 87.50%] [G loss: 3.358742]\n",
      "epoch:49 step:38773 [D loss: 0.413777, acc.: 81.25%] [G loss: 2.829073]\n",
      "epoch:49 step:38774 [D loss: 0.251232, acc.: 88.28%] [G loss: 2.391945]\n",
      "epoch:49 step:38775 [D loss: 0.300610, acc.: 85.16%] [G loss: 4.765669]\n",
      "epoch:49 step:38776 [D loss: 0.386068, acc.: 78.91%] [G loss: 5.406857]\n",
      "epoch:49 step:38777 [D loss: 0.256933, acc.: 88.28%] [G loss: 3.547764]\n",
      "epoch:49 step:38778 [D loss: 0.319272, acc.: 84.38%] [G loss: 5.483515]\n",
      "epoch:49 step:38779 [D loss: 0.383969, acc.: 84.38%] [G loss: 4.115575]\n",
      "epoch:49 step:38780 [D loss: 0.218645, acc.: 92.97%] [G loss: 4.390195]\n",
      "epoch:49 step:38781 [D loss: 0.236850, acc.: 90.62%] [G loss: 3.592962]\n",
      "epoch:49 step:38782 [D loss: 0.287555, acc.: 85.94%] [G loss: 3.646539]\n",
      "epoch:49 step:38783 [D loss: 0.296089, acc.: 87.50%] [G loss: 3.116989]\n",
      "epoch:49 step:38784 [D loss: 0.316325, acc.: 88.28%] [G loss: 2.412950]\n",
      "epoch:49 step:38785 [D loss: 0.255023, acc.: 90.62%] [G loss: 3.073723]\n",
      "epoch:49 step:38786 [D loss: 0.338273, acc.: 84.38%] [G loss: 2.141430]\n",
      "epoch:49 step:38787 [D loss: 0.259485, acc.: 89.06%] [G loss: 3.920226]\n",
      "epoch:49 step:38788 [D loss: 0.324096, acc.: 83.59%] [G loss: 5.420460]\n",
      "epoch:49 step:38789 [D loss: 0.331143, acc.: 82.81%] [G loss: 4.356660]\n",
      "epoch:49 step:38790 [D loss: 0.243584, acc.: 89.06%] [G loss: 4.399831]\n",
      "epoch:49 step:38791 [D loss: 0.358427, acc.: 79.69%] [G loss: 3.454769]\n",
      "epoch:49 step:38792 [D loss: 0.249333, acc.: 91.41%] [G loss: 2.940165]\n",
      "epoch:49 step:38793 [D loss: 0.301962, acc.: 89.06%] [G loss: 6.055890]\n",
      "epoch:49 step:38794 [D loss: 0.356819, acc.: 88.28%] [G loss: 3.866355]\n",
      "epoch:49 step:38795 [D loss: 0.357399, acc.: 82.03%] [G loss: 2.716342]\n",
      "epoch:49 step:38796 [D loss: 0.399485, acc.: 82.03%] [G loss: 2.636953]\n",
      "epoch:49 step:38797 [D loss: 0.212281, acc.: 95.31%] [G loss: 2.979336]\n",
      "epoch:49 step:38798 [D loss: 0.281177, acc.: 87.50%] [G loss: 3.762716]\n",
      "epoch:49 step:38799 [D loss: 0.315615, acc.: 84.38%] [G loss: 3.735392]\n",
      "epoch:49 step:38800 [D loss: 0.300028, acc.: 85.94%] [G loss: 2.907256]\n",
      "##############\n",
      "[0.8709297  0.88051591 0.79660647 0.8196236  0.76299139 0.82724507\n",
      " 0.88034634 0.84063453 0.78321361 0.82419529]\n",
      "##########\n",
      "epoch:49 step:38801 [D loss: 0.284471, acc.: 85.94%] [G loss: 3.145797]\n",
      "epoch:49 step:38802 [D loss: 0.243511, acc.: 89.06%] [G loss: 3.210392]\n",
      "epoch:49 step:38803 [D loss: 0.231381, acc.: 91.41%] [G loss: 3.519781]\n",
      "epoch:49 step:38804 [D loss: 0.248126, acc.: 90.62%] [G loss: 3.013016]\n",
      "epoch:49 step:38805 [D loss: 0.330349, acc.: 82.81%] [G loss: 3.169042]\n",
      "epoch:49 step:38806 [D loss: 0.446909, acc.: 78.12%] [G loss: 3.418729]\n",
      "epoch:49 step:38807 [D loss: 0.321279, acc.: 82.03%] [G loss: 4.683027]\n",
      "epoch:49 step:38808 [D loss: 0.287536, acc.: 86.72%] [G loss: 3.714629]\n",
      "epoch:49 step:38809 [D loss: 0.323157, acc.: 84.38%] [G loss: 4.364873]\n",
      "epoch:49 step:38810 [D loss: 0.258400, acc.: 88.28%] [G loss: 4.793343]\n",
      "epoch:49 step:38811 [D loss: 0.212465, acc.: 89.06%] [G loss: 5.156024]\n",
      "epoch:49 step:38812 [D loss: 0.289605, acc.: 85.94%] [G loss: 6.581837]\n",
      "epoch:49 step:38813 [D loss: 0.219930, acc.: 88.28%] [G loss: 4.956800]\n",
      "epoch:49 step:38814 [D loss: 0.223599, acc.: 89.84%] [G loss: 5.109453]\n",
      "epoch:49 step:38815 [D loss: 0.298315, acc.: 83.59%] [G loss: 4.378601]\n",
      "epoch:49 step:38816 [D loss: 0.292653, acc.: 88.28%] [G loss: 4.371962]\n",
      "epoch:49 step:38817 [D loss: 0.262870, acc.: 89.84%] [G loss: 4.957454]\n",
      "epoch:49 step:38818 [D loss: 0.178215, acc.: 92.97%] [G loss: 6.225892]\n",
      "epoch:49 step:38819 [D loss: 0.220797, acc.: 90.62%] [G loss: 6.227446]\n",
      "epoch:49 step:38820 [D loss: 0.189553, acc.: 92.19%] [G loss: 5.162255]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:49 step:38821 [D loss: 0.328754, acc.: 87.50%] [G loss: 5.289224]\n",
      "epoch:49 step:38822 [D loss: 0.196828, acc.: 93.75%] [G loss: 4.058919]\n",
      "epoch:49 step:38823 [D loss: 0.240532, acc.: 91.41%] [G loss: 4.009463]\n",
      "epoch:49 step:38824 [D loss: 0.271904, acc.: 90.62%] [G loss: 4.550912]\n",
      "epoch:49 step:38825 [D loss: 0.279085, acc.: 85.94%] [G loss: 4.685321]\n",
      "epoch:49 step:38826 [D loss: 0.235435, acc.: 89.84%] [G loss: 4.906527]\n",
      "epoch:49 step:38827 [D loss: 0.215753, acc.: 92.97%] [G loss: 3.955300]\n",
      "epoch:49 step:38828 [D loss: 0.275441, acc.: 89.84%] [G loss: 3.748909]\n",
      "epoch:49 step:38829 [D loss: 0.304294, acc.: 88.28%] [G loss: 4.099949]\n",
      "epoch:49 step:38830 [D loss: 0.217989, acc.: 92.19%] [G loss: 3.219756]\n",
      "epoch:49 step:38831 [D loss: 0.359735, acc.: 82.81%] [G loss: 3.257181]\n",
      "epoch:49 step:38832 [D loss: 0.265051, acc.: 86.72%] [G loss: 3.370231]\n",
      "epoch:49 step:38833 [D loss: 0.290591, acc.: 90.62%] [G loss: 3.390096]\n",
      "epoch:49 step:38834 [D loss: 0.255071, acc.: 91.41%] [G loss: 2.905904]\n",
      "epoch:49 step:38835 [D loss: 0.282098, acc.: 86.72%] [G loss: 4.319438]\n",
      "epoch:49 step:38836 [D loss: 0.343844, acc.: 82.03%] [G loss: 2.711140]\n",
      "epoch:49 step:38837 [D loss: 0.173459, acc.: 95.31%] [G loss: 3.702863]\n",
      "epoch:49 step:38838 [D loss: 0.216369, acc.: 90.62%] [G loss: 2.811625]\n",
      "epoch:49 step:38839 [D loss: 0.308409, acc.: 84.38%] [G loss: 2.812059]\n",
      "epoch:49 step:38840 [D loss: 0.401913, acc.: 81.25%] [G loss: 2.908884]\n",
      "epoch:49 step:38841 [D loss: 0.278118, acc.: 91.41%] [G loss: 3.422619]\n",
      "epoch:49 step:38842 [D loss: 0.249848, acc.: 89.06%] [G loss: 3.298403]\n",
      "epoch:49 step:38843 [D loss: 0.332316, acc.: 84.38%] [G loss: 3.088823]\n",
      "epoch:49 step:38844 [D loss: 0.294871, acc.: 88.28%] [G loss: 3.378344]\n",
      "epoch:49 step:38845 [D loss: 0.355728, acc.: 84.38%] [G loss: 2.333716]\n",
      "epoch:49 step:38846 [D loss: 0.237109, acc.: 90.62%] [G loss: 3.890681]\n",
      "epoch:49 step:38847 [D loss: 0.266365, acc.: 89.06%] [G loss: 4.210935]\n",
      "epoch:49 step:38848 [D loss: 0.246736, acc.: 89.06%] [G loss: 3.783664]\n",
      "epoch:49 step:38849 [D loss: 0.295142, acc.: 87.50%] [G loss: 3.338568]\n",
      "epoch:49 step:38850 [D loss: 0.237105, acc.: 87.50%] [G loss: 2.734658]\n",
      "epoch:49 step:38851 [D loss: 0.318581, acc.: 87.50%] [G loss: 3.287954]\n",
      "epoch:49 step:38852 [D loss: 0.316381, acc.: 87.50%] [G loss: 5.617500]\n",
      "epoch:49 step:38853 [D loss: 0.360369, acc.: 82.81%] [G loss: 2.708368]\n",
      "epoch:49 step:38854 [D loss: 0.219340, acc.: 93.75%] [G loss: 4.065281]\n",
      "epoch:49 step:38855 [D loss: 0.364302, acc.: 83.59%] [G loss: 4.969090]\n",
      "epoch:49 step:38856 [D loss: 0.250103, acc.: 90.62%] [G loss: 4.710429]\n",
      "epoch:49 step:38857 [D loss: 0.336745, acc.: 89.84%] [G loss: 4.484739]\n",
      "epoch:49 step:38858 [D loss: 0.249605, acc.: 90.62%] [G loss: 4.147033]\n",
      "epoch:49 step:38859 [D loss: 0.287641, acc.: 86.72%] [G loss: 5.726511]\n",
      "epoch:49 step:38860 [D loss: 0.325728, acc.: 85.94%] [G loss: 4.182324]\n",
      "epoch:49 step:38861 [D loss: 0.291879, acc.: 87.50%] [G loss: 4.239705]\n",
      "epoch:49 step:38862 [D loss: 0.272475, acc.: 88.28%] [G loss: 3.302566]\n",
      "epoch:49 step:38863 [D loss: 0.263785, acc.: 89.84%] [G loss: 5.024277]\n",
      "epoch:49 step:38864 [D loss: 0.348316, acc.: 81.25%] [G loss: 3.636133]\n",
      "epoch:49 step:38865 [D loss: 0.431004, acc.: 81.25%] [G loss: 4.812838]\n",
      "epoch:49 step:38866 [D loss: 0.401252, acc.: 84.38%] [G loss: 4.875234]\n",
      "epoch:49 step:38867 [D loss: 0.389655, acc.: 84.38%] [G loss: 3.226201]\n",
      "epoch:49 step:38868 [D loss: 0.271745, acc.: 87.50%] [G loss: 3.265905]\n",
      "epoch:49 step:38869 [D loss: 0.293576, acc.: 85.94%] [G loss: 3.968673]\n",
      "epoch:49 step:38870 [D loss: 0.281320, acc.: 86.72%] [G loss: 3.097698]\n",
      "epoch:49 step:38871 [D loss: 0.236174, acc.: 89.84%] [G loss: 3.427873]\n",
      "epoch:49 step:38872 [D loss: 0.251092, acc.: 88.28%] [G loss: 3.985562]\n",
      "epoch:49 step:38873 [D loss: 0.392673, acc.: 80.47%] [G loss: 4.576545]\n",
      "epoch:49 step:38874 [D loss: 0.305321, acc.: 88.28%] [G loss: 3.241716]\n",
      "epoch:49 step:38875 [D loss: 0.384433, acc.: 82.81%] [G loss: 6.388198]\n",
      "epoch:49 step:38876 [D loss: 0.427195, acc.: 78.91%] [G loss: 3.135675]\n",
      "epoch:49 step:38877 [D loss: 0.239562, acc.: 92.97%] [G loss: 5.213160]\n",
      "epoch:49 step:38878 [D loss: 0.363949, acc.: 82.03%] [G loss: 3.602042]\n",
      "epoch:49 step:38879 [D loss: 0.243856, acc.: 91.41%] [G loss: 3.996222]\n",
      "epoch:49 step:38880 [D loss: 0.280352, acc.: 87.50%] [G loss: 2.577353]\n",
      "epoch:49 step:38881 [D loss: 0.315178, acc.: 89.06%] [G loss: 3.680286]\n",
      "epoch:49 step:38882 [D loss: 0.323302, acc.: 84.38%] [G loss: 2.843671]\n",
      "epoch:49 step:38883 [D loss: 0.363730, acc.: 85.16%] [G loss: 3.657943]\n",
      "epoch:49 step:38884 [D loss: 0.355927, acc.: 83.59%] [G loss: 3.581856]\n",
      "epoch:49 step:38885 [D loss: 0.287731, acc.: 86.72%] [G loss: 3.243151]\n",
      "epoch:49 step:38886 [D loss: 0.364512, acc.: 80.47%] [G loss: 4.105021]\n",
      "epoch:49 step:38887 [D loss: 0.278568, acc.: 88.28%] [G loss: 3.401587]\n",
      "epoch:49 step:38888 [D loss: 0.248860, acc.: 90.62%] [G loss: 3.059724]\n",
      "epoch:49 step:38889 [D loss: 0.279702, acc.: 89.84%] [G loss: 4.214905]\n",
      "epoch:49 step:38890 [D loss: 0.268898, acc.: 90.62%] [G loss: 2.938551]\n",
      "epoch:49 step:38891 [D loss: 0.257967, acc.: 87.50%] [G loss: 3.256727]\n",
      "epoch:49 step:38892 [D loss: 0.337286, acc.: 88.28%] [G loss: 3.388882]\n",
      "epoch:49 step:38893 [D loss: 0.348338, acc.: 83.59%] [G loss: 3.810980]\n",
      "epoch:49 step:38894 [D loss: 0.297654, acc.: 81.25%] [G loss: 3.653325]\n",
      "epoch:49 step:38895 [D loss: 0.217958, acc.: 87.50%] [G loss: 4.422461]\n",
      "epoch:49 step:38896 [D loss: 0.237536, acc.: 89.06%] [G loss: 5.073950]\n",
      "epoch:49 step:38897 [D loss: 0.329046, acc.: 84.38%] [G loss: 5.837234]\n",
      "epoch:49 step:38898 [D loss: 0.194780, acc.: 91.41%] [G loss: 4.456485]\n",
      "epoch:49 step:38899 [D loss: 0.311481, acc.: 87.50%] [G loss: 5.162840]\n",
      "epoch:49 step:38900 [D loss: 0.441976, acc.: 83.59%] [G loss: 4.151904]\n",
      "epoch:49 step:38901 [D loss: 0.274886, acc.: 88.28%] [G loss: 4.930337]\n",
      "epoch:49 step:38902 [D loss: 0.418151, acc.: 82.81%] [G loss: 4.053264]\n",
      "epoch:49 step:38903 [D loss: 0.277715, acc.: 85.94%] [G loss: 3.668231]\n",
      "epoch:49 step:38904 [D loss: 0.284506, acc.: 88.28%] [G loss: 3.259243]\n",
      "epoch:49 step:38905 [D loss: 0.214990, acc.: 89.84%] [G loss: 2.772781]\n",
      "epoch:49 step:38906 [D loss: 0.325464, acc.: 84.38%] [G loss: 3.424708]\n",
      "epoch:49 step:38907 [D loss: 0.229722, acc.: 89.84%] [G loss: 3.196718]\n",
      "epoch:49 step:38908 [D loss: 0.312856, acc.: 87.50%] [G loss: 2.986623]\n",
      "epoch:49 step:38909 [D loss: 0.230759, acc.: 89.84%] [G loss: 3.197095]\n",
      "epoch:49 step:38910 [D loss: 0.298304, acc.: 87.50%] [G loss: 3.885160]\n",
      "epoch:49 step:38911 [D loss: 0.272105, acc.: 89.06%] [G loss: 2.981795]\n",
      "epoch:49 step:38912 [D loss: 0.286817, acc.: 86.72%] [G loss: 3.417536]\n",
      "epoch:49 step:38913 [D loss: 0.259737, acc.: 91.41%] [G loss: 2.816633]\n",
      "epoch:49 step:38914 [D loss: 0.176511, acc.: 92.19%] [G loss: 4.226439]\n",
      "epoch:49 step:38915 [D loss: 0.270126, acc.: 90.62%] [G loss: 3.428716]\n",
      "epoch:49 step:38916 [D loss: 0.319532, acc.: 87.50%] [G loss: 3.045329]\n",
      "epoch:49 step:38917 [D loss: 0.296259, acc.: 87.50%] [G loss: 3.076820]\n",
      "epoch:49 step:38918 [D loss: 0.277091, acc.: 88.28%] [G loss: 3.272680]\n",
      "epoch:49 step:38919 [D loss: 0.297414, acc.: 86.72%] [G loss: 3.201414]\n",
      "epoch:49 step:38920 [D loss: 0.303510, acc.: 85.94%] [G loss: 3.042100]\n",
      "epoch:49 step:38921 [D loss: 0.296091, acc.: 85.16%] [G loss: 2.230465]\n",
      "epoch:49 step:38922 [D loss: 0.245143, acc.: 89.84%] [G loss: 3.277493]\n",
      "epoch:49 step:38923 [D loss: 0.377372, acc.: 85.94%] [G loss: 2.922469]\n",
      "epoch:49 step:38924 [D loss: 0.288946, acc.: 89.84%] [G loss: 3.446058]\n",
      "epoch:49 step:38925 [D loss: 0.309400, acc.: 86.72%] [G loss: 3.169899]\n",
      "epoch:49 step:38926 [D loss: 0.359441, acc.: 84.38%] [G loss: 4.775755]\n",
      "epoch:49 step:38927 [D loss: 0.346421, acc.: 85.16%] [G loss: 3.694563]\n",
      "epoch:49 step:38928 [D loss: 0.335827, acc.: 88.28%] [G loss: 4.560537]\n",
      "epoch:49 step:38929 [D loss: 0.361758, acc.: 88.28%] [G loss: 6.455373]\n",
      "epoch:49 step:38930 [D loss: 0.544483, acc.: 74.22%] [G loss: 8.821321]\n",
      "epoch:49 step:38931 [D loss: 1.279662, acc.: 57.81%] [G loss: 4.734013]\n",
      "epoch:49 step:38932 [D loss: 0.650016, acc.: 73.44%] [G loss: 3.981346]\n",
      "epoch:49 step:38933 [D loss: 0.478953, acc.: 78.12%] [G loss: 6.450183]\n",
      "epoch:49 step:38934 [D loss: 0.323987, acc.: 84.38%] [G loss: 3.174704]\n",
      "epoch:49 step:38935 [D loss: 0.278024, acc.: 85.94%] [G loss: 3.346785]\n",
      "epoch:49 step:38936 [D loss: 0.329992, acc.: 80.47%] [G loss: 4.107869]\n",
      "epoch:49 step:38937 [D loss: 0.252100, acc.: 92.19%] [G loss: 3.056955]\n",
      "epoch:49 step:38938 [D loss: 0.350284, acc.: 82.81%] [G loss: 3.229389]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:49 step:38939 [D loss: 0.239970, acc.: 92.19%] [G loss: 3.161451]\n",
      "epoch:49 step:38940 [D loss: 0.386057, acc.: 78.91%] [G loss: 3.214644]\n",
      "epoch:49 step:38941 [D loss: 0.296152, acc.: 81.25%] [G loss: 2.567756]\n",
      "epoch:49 step:38942 [D loss: 0.306257, acc.: 86.72%] [G loss: 3.440923]\n",
      "epoch:49 step:38943 [D loss: 0.360476, acc.: 84.38%] [G loss: 3.104663]\n",
      "epoch:49 step:38944 [D loss: 0.357382, acc.: 83.59%] [G loss: 3.026015]\n",
      "epoch:49 step:38945 [D loss: 0.326207, acc.: 85.16%] [G loss: 4.329074]\n",
      "epoch:49 step:38946 [D loss: 0.245013, acc.: 90.62%] [G loss: 3.404981]\n",
      "epoch:49 step:38947 [D loss: 0.311633, acc.: 87.50%] [G loss: 3.884705]\n",
      "epoch:49 step:38948 [D loss: 0.238808, acc.: 89.06%] [G loss: 3.958358]\n",
      "epoch:49 step:38949 [D loss: 0.288556, acc.: 87.50%] [G loss: 3.476619]\n",
      "epoch:49 step:38950 [D loss: 0.269903, acc.: 88.28%] [G loss: 2.731886]\n",
      "epoch:49 step:38951 [D loss: 0.287700, acc.: 85.16%] [G loss: 2.944839]\n",
      "epoch:49 step:38952 [D loss: 0.301604, acc.: 89.06%] [G loss: 2.596013]\n",
      "epoch:49 step:38953 [D loss: 0.233791, acc.: 92.19%] [G loss: 2.818316]\n",
      "epoch:49 step:38954 [D loss: 0.317849, acc.: 85.94%] [G loss: 2.539966]\n",
      "epoch:49 step:38955 [D loss: 0.363164, acc.: 78.91%] [G loss: 2.499505]\n",
      "epoch:49 step:38956 [D loss: 0.294795, acc.: 83.59%] [G loss: 3.208090]\n",
      "epoch:49 step:38957 [D loss: 0.272826, acc.: 87.50%] [G loss: 3.243309]\n",
      "epoch:49 step:38958 [D loss: 0.264134, acc.: 92.97%] [G loss: 2.683963]\n",
      "epoch:49 step:38959 [D loss: 0.294323, acc.: 85.16%] [G loss: 2.542885]\n",
      "epoch:49 step:38960 [D loss: 0.254098, acc.: 89.84%] [G loss: 3.564654]\n",
      "epoch:49 step:38961 [D loss: 0.417089, acc.: 80.47%] [G loss: 3.072768]\n",
      "epoch:49 step:38962 [D loss: 0.282495, acc.: 88.28%] [G loss: 2.633654]\n",
      "epoch:49 step:38963 [D loss: 0.285745, acc.: 88.28%] [G loss: 2.940105]\n",
      "epoch:49 step:38964 [D loss: 0.320726, acc.: 85.16%] [G loss: 2.860152]\n",
      "epoch:49 step:38965 [D loss: 0.323345, acc.: 82.81%] [G loss: 3.846615]\n",
      "epoch:49 step:38966 [D loss: 0.306376, acc.: 88.28%] [G loss: 2.721338]\n",
      "epoch:49 step:38967 [D loss: 0.286095, acc.: 87.50%] [G loss: 3.448797]\n",
      "epoch:49 step:38968 [D loss: 0.240845, acc.: 91.41%] [G loss: 2.078846]\n",
      "epoch:49 step:38969 [D loss: 0.376550, acc.: 87.50%] [G loss: 2.780209]\n",
      "epoch:49 step:38970 [D loss: 0.252733, acc.: 88.28%] [G loss: 3.128429]\n",
      "epoch:49 step:38971 [D loss: 0.233493, acc.: 89.06%] [G loss: 2.771045]\n",
      "epoch:49 step:38972 [D loss: 0.312325, acc.: 86.72%] [G loss: 2.540167]\n",
      "epoch:49 step:38973 [D loss: 0.278830, acc.: 89.06%] [G loss: 3.065269]\n",
      "epoch:49 step:38974 [D loss: 0.257293, acc.: 89.84%] [G loss: 3.200404]\n",
      "epoch:49 step:38975 [D loss: 0.388152, acc.: 82.03%] [G loss: 3.758749]\n",
      "epoch:49 step:38976 [D loss: 0.423761, acc.: 82.81%] [G loss: 4.486634]\n",
      "epoch:49 step:38977 [D loss: 0.380506, acc.: 83.59%] [G loss: 3.664993]\n",
      "epoch:49 step:38978 [D loss: 0.284151, acc.: 85.94%] [G loss: 4.323599]\n",
      "epoch:49 step:38979 [D loss: 0.293305, acc.: 89.06%] [G loss: 3.302115]\n",
      "epoch:49 step:38980 [D loss: 0.214550, acc.: 90.62%] [G loss: 3.097900]\n",
      "epoch:49 step:38981 [D loss: 0.478770, acc.: 79.69%] [G loss: 5.076811]\n",
      "epoch:49 step:38982 [D loss: 0.764257, acc.: 66.41%] [G loss: 3.962879]\n",
      "epoch:49 step:38983 [D loss: 0.494641, acc.: 75.78%] [G loss: 5.703553]\n",
      "epoch:49 step:38984 [D loss: 0.387390, acc.: 82.81%] [G loss: 4.936372]\n",
      "epoch:49 step:38985 [D loss: 0.421302, acc.: 81.25%] [G loss: 4.049508]\n",
      "epoch:49 step:38986 [D loss: 0.294425, acc.: 85.16%] [G loss: 4.468031]\n",
      "epoch:49 step:38987 [D loss: 0.350164, acc.: 85.16%] [G loss: 3.504146]\n",
      "epoch:49 step:38988 [D loss: 0.234531, acc.: 89.84%] [G loss: 3.370250]\n",
      "epoch:49 step:38989 [D loss: 0.263126, acc.: 89.06%] [G loss: 3.530176]\n",
      "epoch:49 step:38990 [D loss: 0.373077, acc.: 85.94%] [G loss: 3.151264]\n",
      "epoch:49 step:38991 [D loss: 0.238552, acc.: 88.28%] [G loss: 3.155993]\n",
      "epoch:49 step:38992 [D loss: 0.286258, acc.: 89.06%] [G loss: 2.472726]\n",
      "epoch:49 step:38993 [D loss: 0.352201, acc.: 84.38%] [G loss: 3.870491]\n",
      "epoch:49 step:38994 [D loss: 0.300768, acc.: 87.50%] [G loss: 2.744706]\n",
      "epoch:49 step:38995 [D loss: 0.310802, acc.: 88.28%] [G loss: 3.027784]\n",
      "epoch:49 step:38996 [D loss: 0.281225, acc.: 89.84%] [G loss: 2.768078]\n",
      "epoch:49 step:38997 [D loss: 0.305553, acc.: 88.28%] [G loss: 3.486427]\n",
      "epoch:49 step:38998 [D loss: 0.282388, acc.: 88.28%] [G loss: 2.971528]\n",
      "epoch:49 step:38999 [D loss: 0.272634, acc.: 89.06%] [G loss: 2.497812]\n",
      "epoch:49 step:39000 [D loss: 0.262320, acc.: 90.62%] [G loss: 2.660822]\n",
      "##############\n",
      "[0.88495781 0.85336784 0.80105548 0.80623889 0.77322339 0.85242625\n",
      " 0.86678664 0.83928973 0.79047342 0.80661302]\n",
      "##########\n",
      "epoch:49 step:39001 [D loss: 0.332955, acc.: 85.16%] [G loss: 3.263146]\n",
      "epoch:49 step:39002 [D loss: 0.276033, acc.: 85.16%] [G loss: 3.357967]\n",
      "epoch:49 step:39003 [D loss: 0.245353, acc.: 89.84%] [G loss: 3.226213]\n",
      "epoch:49 step:39004 [D loss: 0.513295, acc.: 79.69%] [G loss: 3.179763]\n",
      "epoch:49 step:39005 [D loss: 0.339684, acc.: 86.72%] [G loss: 3.527357]\n",
      "epoch:49 step:39006 [D loss: 0.263458, acc.: 89.06%] [G loss: 2.554852]\n",
      "epoch:49 step:39007 [D loss: 0.326698, acc.: 85.16%] [G loss: 3.507797]\n",
      "epoch:49 step:39008 [D loss: 0.317819, acc.: 85.94%] [G loss: 2.990868]\n",
      "epoch:49 step:39009 [D loss: 0.255157, acc.: 90.62%] [G loss: 4.020783]\n",
      "epoch:49 step:39010 [D loss: 0.288790, acc.: 83.59%] [G loss: 3.465668]\n",
      "epoch:49 step:39011 [D loss: 0.266728, acc.: 86.72%] [G loss: 3.862350]\n",
      "epoch:49 step:39012 [D loss: 0.241972, acc.: 88.28%] [G loss: 5.121339]\n",
      "epoch:49 step:39013 [D loss: 0.201992, acc.: 93.75%] [G loss: 2.953150]\n",
      "epoch:49 step:39014 [D loss: 0.259025, acc.: 89.84%] [G loss: 3.191216]\n",
      "epoch:49 step:39015 [D loss: 0.281400, acc.: 89.06%] [G loss: 3.899424]\n",
      "epoch:49 step:39016 [D loss: 0.258793, acc.: 87.50%] [G loss: 2.706434]\n",
      "epoch:49 step:39017 [D loss: 0.293753, acc.: 87.50%] [G loss: 2.568281]\n",
      "epoch:49 step:39018 [D loss: 0.297523, acc.: 85.94%] [G loss: 2.798755]\n",
      "epoch:49 step:39019 [D loss: 0.304505, acc.: 87.50%] [G loss: 3.402178]\n",
      "epoch:49 step:39020 [D loss: 0.268645, acc.: 87.50%] [G loss: 3.938285]\n",
      "epoch:49 step:39021 [D loss: 0.365367, acc.: 84.38%] [G loss: 3.150031]\n",
      "epoch:49 step:39022 [D loss: 0.291257, acc.: 87.50%] [G loss: 2.760567]\n",
      "epoch:49 step:39023 [D loss: 0.416208, acc.: 78.91%] [G loss: 2.589435]\n",
      "epoch:49 step:39024 [D loss: 0.385522, acc.: 82.81%] [G loss: 2.775521]\n",
      "epoch:49 step:39025 [D loss: 0.327784, acc.: 85.94%] [G loss: 3.158154]\n",
      "epoch:49 step:39026 [D loss: 0.262651, acc.: 91.41%] [G loss: 3.837131]\n",
      "epoch:49 step:39027 [D loss: 0.410129, acc.: 83.59%] [G loss: 2.133206]\n",
      "epoch:49 step:39028 [D loss: 0.365421, acc.: 82.81%] [G loss: 3.467999]\n",
      "epoch:49 step:39029 [D loss: 0.348869, acc.: 83.59%] [G loss: 3.013454]\n",
      "epoch:49 step:39030 [D loss: 0.269626, acc.: 89.84%] [G loss: 3.153177]\n",
      "epoch:49 step:39031 [D loss: 0.444494, acc.: 74.22%] [G loss: 3.836147]\n",
      "epoch:49 step:39032 [D loss: 0.512375, acc.: 75.00%] [G loss: 3.103751]\n",
      "epoch:49 step:39033 [D loss: 0.262987, acc.: 88.28%] [G loss: 3.883842]\n",
      "epoch:49 step:39034 [D loss: 0.320648, acc.: 85.94%] [G loss: 4.425938]\n",
      "epoch:49 step:39035 [D loss: 0.348265, acc.: 85.16%] [G loss: 3.339654]\n",
      "epoch:49 step:39036 [D loss: 0.248907, acc.: 90.62%] [G loss: 4.420316]\n",
      "epoch:49 step:39037 [D loss: 0.373527, acc.: 81.25%] [G loss: 3.778670]\n",
      "epoch:49 step:39038 [D loss: 0.230260, acc.: 90.62%] [G loss: 4.180971]\n",
      "epoch:49 step:39039 [D loss: 0.193179, acc.: 90.62%] [G loss: 4.483955]\n",
      "epoch:49 step:39040 [D loss: 0.330680, acc.: 85.16%] [G loss: 3.384523]\n",
      "epoch:49 step:39041 [D loss: 0.288304, acc.: 89.06%] [G loss: 3.740964]\n",
      "epoch:49 step:39042 [D loss: 0.223755, acc.: 91.41%] [G loss: 2.975910]\n",
      "epoch:49 step:39043 [D loss: 0.294476, acc.: 86.72%] [G loss: 4.970447]\n",
      "epoch:49 step:39044 [D loss: 0.339947, acc.: 85.94%] [G loss: 3.250659]\n",
      "epoch:49 step:39045 [D loss: 0.244721, acc.: 89.84%] [G loss: 4.358594]\n",
      "epoch:49 step:39046 [D loss: 0.377730, acc.: 81.25%] [G loss: 4.190180]\n",
      "epoch:49 step:39047 [D loss: 0.358504, acc.: 86.72%] [G loss: 3.905522]\n",
      "epoch:49 step:39048 [D loss: 0.233303, acc.: 92.97%] [G loss: 2.498420]\n",
      "epoch:49 step:39049 [D loss: 0.388357, acc.: 85.16%] [G loss: 5.570473]\n",
      "epoch:49 step:39050 [D loss: 0.454932, acc.: 82.03%] [G loss: 3.573967]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.datasets import cifar10\n",
    "from torch.utils.data import Dataset\n",
    "import torch.utils.data as Data\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, imgs, transform=None):\n",
    "        # super().__init__()\n",
    "        self.imgs = imgs\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.imgs)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img = self.imgs[index]\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "        else:\n",
    "            img = torch.from_numpy(img)\n",
    "        img=img.reshape([3,32,32])\n",
    "        return img\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import model\n",
    "import torch.nn.functional as F\n",
    "model = model.cifar10(128)\n",
    "model.load_state_dict(torch.load('./log/default/best-85.pth'))\n",
    "model.cuda()\n",
    "def EuclideanDistances(A, B):\n",
    "    BT = B.transpose()\n",
    "    # vecProd = A * BT\n",
    "    vecProd = np.dot(A, BT)\n",
    "    # print(vecProd)\n",
    "    SqA = A ** 2\n",
    "    # print(SqA)\n",
    "    sumSqA = np.matrix(np.sum(SqA, axis=1))\n",
    "    sumSqAEx = np.tile(sumSqA.transpose(), (1, vecProd.shape[1]))\n",
    "    # print(sumSqAEx)\n",
    "\n",
    "    SqB = B ** 2\n",
    "    sumSqB = np.sum(SqB, axis=1)\n",
    "    sumSqBEx = np.tile(sumSqB, (vecProd.shape[0], 1))\n",
    "    SqED = sumSqBEx + sumSqAEx - 2 * vecProd\n",
    "    SqED[SqED < 0] = 0.0\n",
    "    ED = np.sqrt(SqED)\n",
    "    return np.divide(ED.sum(), ED.shape[0] * ED.shape[1])\n",
    "\n",
    "\n",
    "def cal_distance_image_real(images, labels):\n",
    "    x_dataset = MyDataset(images)\n",
    "    # print(x_dataset[0].shape)\n",
    "    x_real_loader = Data.DataLoader(dataset=x_dataset, batch_size=200, shuffle=True)\n",
    "    y_logits = []\n",
    "    for i, data in enumerate(x_real_loader):\n",
    "        # indx_target = target.clone()\n",
    "        data = data.cuda()\n",
    "        data = Variable(data, volatile=True)\n",
    "        output = model(data)\n",
    "        pred = F.softmax(output).cpu().detach().numpy()\n",
    "        y_logits += [i for i in pred]\n",
    "    dict = {}\n",
    "    all_dis = []\n",
    "    for i in range(10):\n",
    "        dict[i] = []\n",
    "    for i in range(len(labels)):\n",
    "        dict[labels[i]].append(y_logits[i])\n",
    "    for i in range(10):\n",
    "        dict[i] = np.array(dict[i])\n",
    "        if len(dict[i]):\n",
    "            dis = EuclideanDistances(dict[i], dict[i])  # 生成图片的紧度\n",
    "        else:\n",
    "            dis = -1\n",
    "        all_dis.append(dis)\n",
    "    return np.array(all_dis)\n",
    "\n",
    "\n",
    "def cal_distance_image_fake(images):\n",
    "    x_dataset = MyDataset(images)\n",
    "    # print(x_dataset[0].shape)\n",
    "    x_real_loader = Data.DataLoader(dataset=x_dataset, batch_size=200, shuffle=True)\n",
    "    y_logits = []\n",
    "    labels=[]\n",
    "    for i, data in enumerate(x_real_loader):\n",
    "        # indx_target = target.clone()\n",
    "        data = data.cuda()\n",
    "        data = Variable(data, volatile=True)\n",
    "        output = model(data)\n",
    "        pred = output.data.max(1)[1]\n",
    "        labels += [i for i in pred.cpu().numpy()]\n",
    "        pred = F.softmax(output).cpu().detach().numpy()\n",
    "        y_logits += [i for i in pred]\n",
    "\n",
    "    dict = {}\n",
    "    all_dis = []\n",
    "    for i in range(10):\n",
    "        dict[i] = []\n",
    "    for i in range(len(labels)):\n",
    "        dict[labels[i]].append(y_logits[i])\n",
    "    for i in range(10):\n",
    "        dict[i] = np.array(dict[i])\n",
    "        if len(dict[i]):\n",
    "            dis = EuclideanDistances(dict[i], dict[i])  # 生成图片的紧度\n",
    "        else:\n",
    "            dis = -1\n",
    "        all_dis.append(dis)\n",
    "    return np.array(all_dis)\n",
    "\n",
    "import os\n",
    "if not os.path.isdir('saved_models_{}'.format('gan')):\n",
    "    os.mkdir('saved_models_{}'.format('gan'))\n",
    "f = open('saved_models_{}/log_collapse1.txt'.format('gan'), mode='w')\n",
    "import torch.utils.data as Data\n",
    "import cv2\n",
    "\n",
    "\n",
    "from keras.datasets import cifar10\n",
    "from keras.layers import Input, Dense, Reshape, Flatten, Dropout\n",
    "from keras.layers import BatchNormalization, Activation, ZeroPadding2D\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.layers.convolutional import UpSampling2D, Conv2D\n",
    "from keras.models import Sequential, Model\n",
    "from keras.optimizers import Adam #optimizer of keras\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "class GAN():\n",
    "    def __init__(self):\n",
    "        self.img_rows = 32\n",
    "        self.img_cols = 32\n",
    "        self.channels = 3\n",
    "        self.img_shape = (self.img_rows, self.img_cols, self.channels) #shape of image\n",
    "        self.latent_dim = 100\n",
    "        self.x = []\n",
    "        self.y = np.zeros((31, 1), dtype=np.int)\n",
    "        self.y = list(self.y)\n",
    "        for i in range(31):\n",
    "            self.y[i] = []\n",
    "\n",
    "        optimizer = Adam(0.0002, 0.5) #optimizer of gan\n",
    "\n",
    "        # Build and compile the discriminator,only to keras\n",
    "        self.discriminator = self.build_discriminator()\n",
    "        self.discriminator.compile(loss='binary_crossentropy',\n",
    "            optimizer=optimizer,\n",
    "            metrics=['accuracy'])\n",
    "\n",
    "        # Build the generator\n",
    "        self.generator = self.build_generator()\n",
    "\n",
    "        # The generator takes noise as input and generates imgs\n",
    "        z = Input(shape=(self.latent_dim,))  #Input():用来实例化一个keras张量\n",
    "        img = self.generator(z)\n",
    "\n",
    "        # For the combined model we will only train the generator\n",
    "        self.discriminator.trainable = False\n",
    "\n",
    "        # The discriminator takes generated images as input and determines validity\n",
    "        validity = self.discriminator(img)\n",
    "\n",
    "        # The combined model  (stacked generator and discriminator)\n",
    "        # Trains the generator to fool the discriminator\n",
    "        self.combined = Model(z, validity)\n",
    "        self.combined.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
    "\n",
    "\n",
    "    def build_generator(self):\n",
    "\n",
    "        model = Sequential()\n",
    "        model.add(Dense(256, input_dim=self.latent_dim))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(Dense(512))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(Dense(1024))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(Dense(np.prod(self.img_shape), activation='tanh'))\n",
    "        model.add(Reshape(self.img_shape))\n",
    "\n",
    "        model.summary()\n",
    "\n",
    "        noise = Input(shape=(self.latent_dim,))\n",
    "        img = model(noise)\n",
    "\n",
    "        return Model(noise, img)\n",
    "\n",
    "    def build_discriminator(self):\n",
    "\n",
    "        model = Sequential()\n",
    "\n",
    "        model.add(Flatten(input_shape=self.img_shape))\n",
    "        model.add(Dense(512))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dense(256))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dense(1, activation='sigmoid'))\n",
    "        model.summary()\n",
    "\n",
    "        img = Input(shape=self.img_shape)\n",
    "        validity = model(img)\n",
    "\n",
    "        return Model(img, validity)\n",
    "\n",
    "    def train(self, epochs, batch_size=128, sample_interval=50):\n",
    "\n",
    "        # Load the dataset\n",
    "        (X_train, _), (X_test, y_test) = cifar10.load_data()\n",
    "\n",
    "        # Rescale -1 to 1\n",
    "        X_train = (X_train.astype(np.float32) - 127.5) / 127.5\n",
    "        X_test = (X_test.astype(np.float32) - 127.5) / 127.5\n",
    "\n",
    "        # X_train = np.expand_dims(X_train, axis=3)  #expand_dims用于扩充数组形状\n",
    "        print(np.shape(X_train))\n",
    "\n",
    "        # Adversarial ground truths\n",
    "        valid = np.ones((batch_size, 1))\n",
    "        fake = np.zeros((batch_size, 1))\n",
    "\n",
    "        nb_batches = int(X_train.shape[0] / batch_size)\n",
    "        global_step = 0\n",
    "        steps = []\n",
    "        values = []\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "\n",
    "            for index in range(nb_batches):\n",
    "                global_step += 1\n",
    "                imgs = X_train[index * batch_size:(index + 1) * batch_size]\n",
    "                noise = np.random.normal(0, 1, (batch_size, self.latent_dim))\n",
    "\n",
    "                # Generate a batch of new images\n",
    "                gen_imgs = self.generator.predict(noise)\n",
    "\n",
    "                # Train the discriminator\n",
    "                d_loss_real = self.discriminator.train_on_batch(imgs, valid)\n",
    "                d_loss_fake = self.discriminator.train_on_batch(gen_imgs, fake)\n",
    "                d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "\n",
    "                # ---------------------\n",
    "                #  Train Generator\n",
    "                # ---------------------\n",
    "\n",
    "                noise = np.random.normal(0, 1, (batch_size, self.latent_dim))\n",
    "\n",
    "                # Train the generator (to have the discriminator label samples as valid)\n",
    "                g_loss = self.combined.train_on_batch(noise, valid)\n",
    "\n",
    "                # Plot the progress\n",
    "                print(\"epoch:%d step:%d [D loss: %f, acc.: %.2f%%] [G loss: %f]\" % (\n",
    "                    epoch, global_step, d_loss[0], 100 * d_loss[1], g_loss))\n",
    "                sample_num=5000\n",
    "\n",
    "                if global_step % sample_interval == 0:\n",
    "                    self.mode_drop(X_test,y_test,sample_num, global_step)\n",
    "\n",
    "\n",
    "    def mode_drop(self, x_test,y_test,sample_num, global_step):\n",
    "        r, c = 10, 1000\n",
    "        noise = np.random.normal(0, 1, (r * c, 100))\n",
    "        # sampled_labels = np.array([num for _ in range(r) for num in range(c)])\n",
    "        gen_imgs = self.generator.predict([noise])\n",
    "        # Rescale images 0 - 1\n",
    "        gen_imgs = 0.5 * gen_imgs + 0.5\n",
    "        labels = np.squeeze(y_test[:sample_num])\n",
    "        labels = np.squeeze(labels)\n",
    "        dis_real = cal_distance_image_real(x_test[:sample_num], labels)\n",
    "        dis_fake = cal_distance_image_fake(gen_imgs)\n",
    "        dis_cha = dis_real - dis_fake\n",
    "        print('##############')\n",
    "        # print(dis_real)\n",
    "        # print(dis_fake)\n",
    "        print(dis_cha)\n",
    "        print('##########')\n",
    "        f.writelines('\\n')\n",
    "        f.writelines('step:' + str(global_step))\n",
    "        f.writelines('\\n')\n",
    "        f.writelines('紧度')\n",
    "        f.writelines('\\n')\n",
    "        f.writelines(' %.8f ' % (i) for i in dis_cha)\n",
    "        f.writelines('\\n')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    gan = GAN()\n",
    "    gan.train(epochs=50, batch_size=64, sample_interval=200)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pppppppp [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
