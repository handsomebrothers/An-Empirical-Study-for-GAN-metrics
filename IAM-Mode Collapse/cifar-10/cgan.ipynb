{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.client.session.Session at 0x7ff010714208>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "# #指定使用那块GUP训练\n",
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "config = tf.ConfigProto()\n",
    "# 设置最大占有GPU不超过显存的70%\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.7 \n",
    "# # 重点：设置动态分配GPU\n",
    "config.gpu_options.allow_growth = True\n",
    "# 创建session时\n",
    "tf.Session(config=config) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "Sequential(\n",
      "  (0): Conv2d(3, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "  (2): ReLU()\n",
      "  (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "  (5): ReLU()\n",
      "  (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (7): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (8): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "  (9): ReLU()\n",
      "  (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (11): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "  (12): ReLU()\n",
      "  (13): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (14): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (15): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "  (16): ReLU()\n",
      "  (17): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (18): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "  (19): ReLU()\n",
      "  (20): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (21): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (22): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "  (23): ReLU()\n",
      "  (24): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      ")\n",
      "Sequential(\n",
      "  (0): Linear(in_features=1024, out_features=10, bias=True)\n",
      ")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 16, 16, 64)        4864      \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 16, 16, 64)        256       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)    (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 8, 8, 128)         204928    \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 8, 8, 128)         512       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)    (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 4, 4, 256)         819456    \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 4, 4, 256)         1024      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)    (None, 4, 4, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 2, 2, 512)         3277312   \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 2, 2, 512)         2048      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)    (None, 2, 2, 512)         0         \n",
      "=================================================================\n",
      "Total params: 4,310,400\n",
      "Trainable params: 4,308,480\n",
      "Non-trainable params: 1,920\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_3 (Dense)              (None, 2048)              206848    \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 2048)              8192      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)    (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "reshape_1 (Reshape)          (None, 2, 2, 512)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_1 (Conv2DTr (None, 4, 4, 256)         3277056   \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 4, 4, 256)         1024      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_6 (LeakyReLU)    (None, 4, 4, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_2 (Conv2DTr (None, 8, 8, 128)         819328    \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 8, 8, 128)         512       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_7 (LeakyReLU)    (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_3 (Conv2DTr (None, 16, 16, 64)        204864    \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 16, 16, 64)        256       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_8 (LeakyReLU)    (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_4 (Conv2DTr (None, 32, 32, 3)         4803      \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 32, 32, 3)         0         \n",
      "=================================================================\n",
      "Total params: 4,522,883\n",
      "Trainable params: 4,517,891\n",
      "Non-trainable params: 4,992\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/imi432_006/anaconda3/envs/tf/lib/python3.5/site-packages/keras/engine/training.py:490: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0 step:1 [D loss: 3.810548, acc.: 17.97%] [G loss: 0.267660]\n",
      "epoch:0 step:2 [D loss: 0.976476, acc.: 50.00%] [G loss: 3.399368]\n",
      "epoch:0 step:3 [D loss: 0.777092, acc.: 55.47%] [G loss: 2.964298]\n",
      "epoch:0 step:4 [D loss: 0.958809, acc.: 31.25%] [G loss: 3.593536]\n",
      "epoch:0 step:5 [D loss: 0.365670, acc.: 86.72%] [G loss: 3.247061]\n",
      "epoch:0 step:6 [D loss: 0.660181, acc.: 51.56%] [G loss: 4.713340]\n",
      "epoch:0 step:7 [D loss: 0.223886, acc.: 93.75%] [G loss: 4.124340]\n",
      "epoch:0 step:8 [D loss: 0.234675, acc.: 97.66%] [G loss: 4.287393]\n",
      "epoch:0 step:9 [D loss: 0.099042, acc.: 100.00%] [G loss: 4.902131]\n",
      "epoch:0 step:10 [D loss: 0.266006, acc.: 92.19%] [G loss: 7.048948]\n",
      "epoch:0 step:11 [D loss: 0.240475, acc.: 88.28%] [G loss: 5.662122]\n",
      "epoch:0 step:12 [D loss: 0.172468, acc.: 99.22%] [G loss: 5.737502]\n",
      "epoch:0 step:13 [D loss: 0.028799, acc.: 100.00%] [G loss: 4.366098]\n",
      "epoch:0 step:14 [D loss: 0.184768, acc.: 97.66%] [G loss: 5.860831]\n",
      "epoch:0 step:15 [D loss: 0.278675, acc.: 88.28%] [G loss: 0.362124]\n",
      "epoch:0 step:16 [D loss: 0.602330, acc.: 64.06%] [G loss: 7.639058]\n",
      "epoch:0 step:17 [D loss: 0.609069, acc.: 77.34%] [G loss: 4.820554]\n",
      "epoch:0 step:18 [D loss: 0.078739, acc.: 98.44%] [G loss: 0.828208]\n",
      "epoch:0 step:19 [D loss: 0.433176, acc.: 61.72%] [G loss: 5.252203]\n",
      "epoch:0 step:20 [D loss: 0.359570, acc.: 81.25%] [G loss: 3.852882]\n",
      "epoch:0 step:21 [D loss: 0.107229, acc.: 97.66%] [G loss: 0.756321]\n",
      "epoch:0 step:22 [D loss: 1.298501, acc.: 51.56%] [G loss: 8.035452]\n",
      "epoch:0 step:23 [D loss: 1.423134, acc.: 62.50%] [G loss: 4.309318]\n",
      "epoch:0 step:24 [D loss: 0.096815, acc.: 97.66%] [G loss: 2.476868]\n",
      "epoch:0 step:25 [D loss: 0.111786, acc.: 100.00%] [G loss: 3.383493]\n",
      "epoch:0 step:26 [D loss: 0.038476, acc.: 100.00%] [G loss: 3.263065]\n",
      "epoch:0 step:27 [D loss: 0.200251, acc.: 99.22%] [G loss: 4.895447]\n",
      "epoch:0 step:28 [D loss: 0.125052, acc.: 96.88%] [G loss: 5.500499]\n",
      "epoch:0 step:29 [D loss: 0.104869, acc.: 96.09%] [G loss: 3.873906]\n",
      "epoch:0 step:30 [D loss: 0.130696, acc.: 99.22%] [G loss: 4.310912]\n",
      "epoch:0 step:31 [D loss: 0.065249, acc.: 100.00%] [G loss: 4.912451]\n",
      "epoch:0 step:32 [D loss: 0.050508, acc.: 99.22%] [G loss: 1.454791]\n",
      "epoch:0 step:33 [D loss: 1.056480, acc.: 57.03%] [G loss: 9.107931]\n",
      "epoch:0 step:34 [D loss: 1.442248, acc.: 56.25%] [G loss: 5.496964]\n",
      "epoch:0 step:35 [D loss: 0.041593, acc.: 99.22%] [G loss: 4.380820]\n",
      "epoch:0 step:36 [D loss: 0.075438, acc.: 99.22%] [G loss: 4.003946]\n",
      "epoch:0 step:37 [D loss: 0.015534, acc.: 100.00%] [G loss: 3.254570]\n",
      "epoch:0 step:38 [D loss: 0.031454, acc.: 100.00%] [G loss: 2.917394]\n",
      "epoch:0 step:39 [D loss: 0.137125, acc.: 97.66%] [G loss: 5.127528]\n",
      "epoch:0 step:40 [D loss: 0.026287, acc.: 100.00%] [G loss: 5.378007]\n",
      "epoch:0 step:41 [D loss: 0.096635, acc.: 96.09%] [G loss: 3.168311]\n",
      "epoch:0 step:42 [D loss: 0.864204, acc.: 71.88%] [G loss: 6.603328]\n",
      "epoch:0 step:43 [D loss: 0.703598, acc.: 73.44%] [G loss: 4.749919]\n",
      "epoch:0 step:44 [D loss: 0.056360, acc.: 100.00%] [G loss: 4.288029]\n",
      "epoch:0 step:45 [D loss: 0.076042, acc.: 98.44%] [G loss: 3.615471]\n",
      "epoch:0 step:46 [D loss: 0.037125, acc.: 100.00%] [G loss: 3.683997]\n",
      "epoch:0 step:47 [D loss: 0.024243, acc.: 100.00%] [G loss: 3.583675]\n",
      "epoch:0 step:48 [D loss: 0.034850, acc.: 100.00%] [G loss: 3.849136]\n",
      "epoch:0 step:49 [D loss: 0.028971, acc.: 100.00%] [G loss: 3.536340]\n",
      "epoch:0 step:50 [D loss: 0.063233, acc.: 100.00%] [G loss: 4.737337]\n",
      "epoch:0 step:51 [D loss: 0.026065, acc.: 100.00%] [G loss: 5.267373]\n",
      "epoch:0 step:52 [D loss: 0.039594, acc.: 99.22%] [G loss: 4.286019]\n",
      "epoch:0 step:53 [D loss: 0.094671, acc.: 100.00%] [G loss: 5.499472]\n",
      "epoch:0 step:54 [D loss: 0.070712, acc.: 96.88%] [G loss: 5.393099]\n",
      "epoch:0 step:55 [D loss: 0.019117, acc.: 100.00%] [G loss: 4.784269]\n",
      "epoch:0 step:56 [D loss: 0.041050, acc.: 100.00%] [G loss: 3.918498]\n",
      "epoch:0 step:57 [D loss: 0.045843, acc.: 100.00%] [G loss: 4.642058]\n",
      "epoch:0 step:58 [D loss: 0.063994, acc.: 99.22%] [G loss: 4.638746]\n",
      "epoch:0 step:59 [D loss: 0.385462, acc.: 72.66%] [G loss: 8.512815]\n",
      "epoch:0 step:60 [D loss: 0.826925, acc.: 77.34%] [G loss: 5.871245]\n",
      "epoch:0 step:61 [D loss: 0.072770, acc.: 98.44%] [G loss: 4.600708]\n",
      "epoch:0 step:62 [D loss: 0.034682, acc.: 100.00%] [G loss: 4.022067]\n",
      "epoch:0 step:63 [D loss: 0.043678, acc.: 100.00%] [G loss: 4.117153]\n",
      "epoch:0 step:64 [D loss: 0.042860, acc.: 100.00%] [G loss: 4.538201]\n",
      "epoch:0 step:65 [D loss: 0.043189, acc.: 100.00%] [G loss: 3.929375]\n",
      "epoch:0 step:66 [D loss: 0.445063, acc.: 77.34%] [G loss: 7.739634]\n",
      "epoch:0 step:67 [D loss: 1.429464, acc.: 55.47%] [G loss: 3.894084]\n",
      "epoch:0 step:68 [D loss: 0.255998, acc.: 94.53%] [G loss: 3.003822]\n",
      "epoch:0 step:69 [D loss: 0.050418, acc.: 100.00%] [G loss: 2.978251]\n",
      "epoch:0 step:70 [D loss: 0.081422, acc.: 100.00%] [G loss: 4.071982]\n",
      "epoch:0 step:71 [D loss: 0.037325, acc.: 100.00%] [G loss: 3.951177]\n",
      "epoch:0 step:72 [D loss: 0.450550, acc.: 79.69%] [G loss: 6.867099]\n",
      "epoch:0 step:73 [D loss: 1.413089, acc.: 65.62%] [G loss: 2.493391]\n",
      "epoch:0 step:74 [D loss: 0.887566, acc.: 56.25%] [G loss: 3.402892]\n",
      "epoch:0 step:75 [D loss: 0.152116, acc.: 96.88%] [G loss: 3.642208]\n",
      "epoch:0 step:76 [D loss: 0.147590, acc.: 95.31%] [G loss: 2.865577]\n",
      "epoch:0 step:77 [D loss: 0.099267, acc.: 100.00%] [G loss: 2.981352]\n",
      "epoch:0 step:78 [D loss: 0.065699, acc.: 99.22%] [G loss: 3.289060]\n",
      "epoch:0 step:79 [D loss: 0.106161, acc.: 98.44%] [G loss: 3.739590]\n",
      "epoch:0 step:80 [D loss: 0.068314, acc.: 99.22%] [G loss: 3.588616]\n",
      "epoch:0 step:81 [D loss: 0.471033, acc.: 85.16%] [G loss: 4.885411]\n",
      "epoch:0 step:82 [D loss: 0.415139, acc.: 86.72%] [G loss: 4.453761]\n",
      "epoch:0 step:83 [D loss: 0.065275, acc.: 100.00%] [G loss: 3.933878]\n",
      "epoch:0 step:84 [D loss: 0.064130, acc.: 99.22%] [G loss: 3.557535]\n",
      "epoch:0 step:85 [D loss: 0.081655, acc.: 100.00%] [G loss: 4.737533]\n",
      "epoch:0 step:86 [D loss: 0.041683, acc.: 100.00%] [G loss: 4.768789]\n",
      "epoch:0 step:87 [D loss: 0.072386, acc.: 99.22%] [G loss: 4.365146]\n",
      "epoch:0 step:88 [D loss: 0.098036, acc.: 98.44%] [G loss: 5.402143]\n",
      "epoch:0 step:89 [D loss: 0.051043, acc.: 98.44%] [G loss: 4.605157]\n",
      "epoch:0 step:90 [D loss: 0.106563, acc.: 98.44%] [G loss: 5.462990]\n",
      "epoch:0 step:91 [D loss: 0.025752, acc.: 99.22%] [G loss: 4.999715]\n",
      "epoch:0 step:92 [D loss: 0.144804, acc.: 97.66%] [G loss: 6.109214]\n",
      "epoch:0 step:93 [D loss: 0.147344, acc.: 90.62%] [G loss: 4.177847]\n",
      "epoch:0 step:94 [D loss: 0.140145, acc.: 95.31%] [G loss: 4.643244]\n",
      "epoch:0 step:95 [D loss: 0.107391, acc.: 98.44%] [G loss: 3.674006]\n",
      "epoch:0 step:96 [D loss: 0.208271, acc.: 93.75%] [G loss: 1.904277]\n",
      "epoch:0 step:97 [D loss: 0.130967, acc.: 96.09%] [G loss: 2.666962]\n",
      "epoch:0 step:98 [D loss: 0.086452, acc.: 98.44%] [G loss: 1.358779]\n",
      "epoch:0 step:99 [D loss: 0.192805, acc.: 94.53%] [G loss: 2.232051]\n",
      "epoch:0 step:100 [D loss: 0.045400, acc.: 98.44%] [G loss: 1.781142]\n",
      "epoch:0 step:101 [D loss: 0.062468, acc.: 98.44%] [G loss: 0.458759]\n",
      "epoch:0 step:102 [D loss: 0.096581, acc.: 97.66%] [G loss: 2.605777]\n",
      "epoch:0 step:103 [D loss: 0.138575, acc.: 96.88%] [G loss: 3.338686]\n",
      "epoch:0 step:104 [D loss: 0.034809, acc.: 98.44%] [G loss: 1.750903]\n",
      "epoch:0 step:105 [D loss: 0.405203, acc.: 84.38%] [G loss: 8.058840]\n",
      "epoch:0 step:106 [D loss: 0.853699, acc.: 70.31%] [G loss: 3.311815]\n",
      "epoch:0 step:107 [D loss: 0.043320, acc.: 100.00%] [G loss: 2.980252]\n",
      "epoch:0 step:108 [D loss: 0.031251, acc.: 100.00%] [G loss: 3.424893]\n",
      "epoch:0 step:109 [D loss: 0.018189, acc.: 100.00%] [G loss: 3.019052]\n",
      "epoch:0 step:110 [D loss: 0.056776, acc.: 100.00%] [G loss: 3.433130]\n",
      "epoch:0 step:111 [D loss: 0.023056, acc.: 100.00%] [G loss: 3.595260]\n",
      "epoch:0 step:112 [D loss: 0.069446, acc.: 96.88%] [G loss: 3.856975]\n",
      "epoch:0 step:113 [D loss: 0.057061, acc.: 100.00%] [G loss: 4.460947]\n",
      "epoch:0 step:114 [D loss: 0.039761, acc.: 100.00%] [G loss: 4.783725]\n",
      "epoch:0 step:115 [D loss: 0.037985, acc.: 99.22%] [G loss: 3.517646]\n",
      "epoch:0 step:116 [D loss: 0.239760, acc.: 90.62%] [G loss: 7.246793]\n",
      "epoch:0 step:117 [D loss: 0.206509, acc.: 89.84%] [G loss: 6.700286]\n",
      "epoch:0 step:118 [D loss: 0.037182, acc.: 98.44%] [G loss: 5.558496]\n",
      "epoch:0 step:119 [D loss: 0.022414, acc.: 100.00%] [G loss: 4.983908]\n",
      "epoch:0 step:120 [D loss: 0.019882, acc.: 99.22%] [G loss: 4.563573]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0 step:121 [D loss: 0.031016, acc.: 100.00%] [G loss: 4.912096]\n",
      "epoch:0 step:122 [D loss: 0.010946, acc.: 100.00%] [G loss: 4.630763]\n",
      "epoch:0 step:123 [D loss: 0.143138, acc.: 92.97%] [G loss: 6.001534]\n",
      "epoch:0 step:124 [D loss: 0.026390, acc.: 99.22%] [G loss: 6.220888]\n",
      "epoch:0 step:125 [D loss: 0.088085, acc.: 96.88%] [G loss: 3.734549]\n",
      "epoch:0 step:126 [D loss: 0.277192, acc.: 88.28%] [G loss: 8.267740]\n",
      "epoch:0 step:127 [D loss: 0.297989, acc.: 89.84%] [G loss: 7.014802]\n",
      "epoch:0 step:128 [D loss: 0.058726, acc.: 98.44%] [G loss: 5.453012]\n",
      "epoch:0 step:129 [D loss: 0.027072, acc.: 99.22%] [G loss: 4.554230]\n",
      "epoch:0 step:130 [D loss: 0.027423, acc.: 100.00%] [G loss: 3.754258]\n",
      "epoch:0 step:131 [D loss: 0.091115, acc.: 96.09%] [G loss: 6.006870]\n",
      "epoch:0 step:132 [D loss: 0.080142, acc.: 96.88%] [G loss: 5.389741]\n",
      "epoch:0 step:133 [D loss: 0.077796, acc.: 99.22%] [G loss: 4.597342]\n",
      "epoch:0 step:134 [D loss: 0.041410, acc.: 100.00%] [G loss: 3.141474]\n",
      "epoch:0 step:135 [D loss: 0.098649, acc.: 99.22%] [G loss: 3.166698]\n",
      "epoch:0 step:136 [D loss: 0.071455, acc.: 97.66%] [G loss: 1.978015]\n",
      "epoch:0 step:137 [D loss: 0.254108, acc.: 92.97%] [G loss: 8.457098]\n",
      "epoch:0 step:138 [D loss: 0.313779, acc.: 84.38%] [G loss: 7.328792]\n",
      "epoch:0 step:139 [D loss: 0.049261, acc.: 97.66%] [G loss: 5.111082]\n",
      "epoch:0 step:140 [D loss: 0.043631, acc.: 100.00%] [G loss: 4.060061]\n",
      "epoch:0 step:141 [D loss: 0.029652, acc.: 100.00%] [G loss: 4.121359]\n",
      "epoch:0 step:142 [D loss: 0.166781, acc.: 97.66%] [G loss: 8.037046]\n",
      "epoch:0 step:143 [D loss: 0.215361, acc.: 92.19%] [G loss: 6.578711]\n",
      "epoch:0 step:144 [D loss: 0.387308, acc.: 82.81%] [G loss: 10.134293]\n",
      "epoch:0 step:145 [D loss: 1.694630, acc.: 63.28%] [G loss: 3.139387]\n",
      "epoch:0 step:146 [D loss: 0.678230, acc.: 75.00%] [G loss: 5.319061]\n",
      "epoch:0 step:147 [D loss: 0.065264, acc.: 98.44%] [G loss: 5.133335]\n",
      "epoch:0 step:148 [D loss: 0.589740, acc.: 85.94%] [G loss: 2.490700]\n",
      "epoch:0 step:149 [D loss: 0.049801, acc.: 99.22%] [G loss: 1.408434]\n",
      "epoch:0 step:150 [D loss: 0.204256, acc.: 91.41%] [G loss: 5.532570]\n",
      "epoch:0 step:151 [D loss: 0.099561, acc.: 93.75%] [G loss: 5.824287]\n",
      "epoch:0 step:152 [D loss: 0.109532, acc.: 93.75%] [G loss: 3.959960]\n",
      "epoch:0 step:153 [D loss: 0.147944, acc.: 96.88%] [G loss: 4.444429]\n",
      "epoch:0 step:154 [D loss: 0.073296, acc.: 98.44%] [G loss: 4.144922]\n",
      "epoch:0 step:155 [D loss: 0.028898, acc.: 100.00%] [G loss: 2.897172]\n",
      "epoch:0 step:156 [D loss: 0.090081, acc.: 100.00%] [G loss: 3.709848]\n",
      "epoch:0 step:157 [D loss: 0.050705, acc.: 99.22%] [G loss: 3.452105]\n",
      "epoch:0 step:158 [D loss: 0.101393, acc.: 96.09%] [G loss: 4.241460]\n",
      "epoch:0 step:159 [D loss: 0.071021, acc.: 96.88%] [G loss: 3.338021]\n",
      "epoch:0 step:160 [D loss: 0.095980, acc.: 99.22%] [G loss: 5.222631]\n",
      "epoch:0 step:161 [D loss: 0.033003, acc.: 100.00%] [G loss: 3.942550]\n",
      "epoch:0 step:162 [D loss: 0.030503, acc.: 99.22%] [G loss: 2.436178]\n",
      "epoch:0 step:163 [D loss: 0.216582, acc.: 88.28%] [G loss: 8.033115]\n",
      "epoch:0 step:164 [D loss: 0.420088, acc.: 79.69%] [G loss: 5.011455]\n",
      "epoch:0 step:165 [D loss: 0.057855, acc.: 99.22%] [G loss: 4.703278]\n",
      "epoch:0 step:166 [D loss: 0.006818, acc.: 100.00%] [G loss: 4.469715]\n",
      "epoch:0 step:167 [D loss: 0.014264, acc.: 100.00%] [G loss: 2.878950]\n",
      "epoch:0 step:168 [D loss: 0.092621, acc.: 98.44%] [G loss: 5.100529]\n",
      "epoch:0 step:169 [D loss: 0.017783, acc.: 100.00%] [G loss: 4.115993]\n",
      "epoch:0 step:170 [D loss: 0.309463, acc.: 88.28%] [G loss: 7.751911]\n",
      "epoch:0 step:171 [D loss: 0.511724, acc.: 85.16%] [G loss: 4.524137]\n",
      "epoch:0 step:172 [D loss: 0.031560, acc.: 100.00%] [G loss: 4.196597]\n",
      "epoch:0 step:173 [D loss: 0.012017, acc.: 100.00%] [G loss: 3.699643]\n",
      "epoch:0 step:174 [D loss: 0.022468, acc.: 100.00%] [G loss: 2.522733]\n",
      "epoch:0 step:175 [D loss: 0.082909, acc.: 98.44%] [G loss: 3.765977]\n",
      "epoch:0 step:176 [D loss: 0.027576, acc.: 99.22%] [G loss: 3.095907]\n",
      "epoch:0 step:177 [D loss: 0.064546, acc.: 98.44%] [G loss: 1.991875]\n",
      "epoch:0 step:178 [D loss: 0.481941, acc.: 67.19%] [G loss: 9.637531]\n",
      "epoch:0 step:179 [D loss: 1.905277, acc.: 55.47%] [G loss: 5.111115]\n",
      "epoch:0 step:180 [D loss: 0.185135, acc.: 92.19%] [G loss: 4.362468]\n",
      "epoch:0 step:181 [D loss: 0.226700, acc.: 85.94%] [G loss: 4.611304]\n",
      "epoch:0 step:182 [D loss: 0.072583, acc.: 98.44%] [G loss: 4.374334]\n",
      "epoch:0 step:183 [D loss: 0.083735, acc.: 99.22%] [G loss: 3.312229]\n",
      "epoch:0 step:184 [D loss: 0.210386, acc.: 89.84%] [G loss: 6.084794]\n",
      "epoch:0 step:185 [D loss: 0.107541, acc.: 97.66%] [G loss: 5.749679]\n",
      "epoch:0 step:186 [D loss: 0.263379, acc.: 86.72%] [G loss: 5.175730]\n",
      "epoch:0 step:187 [D loss: 0.039272, acc.: 100.00%] [G loss: 4.598866]\n",
      "epoch:0 step:188 [D loss: 0.061964, acc.: 100.00%] [G loss: 4.890603]\n",
      "epoch:0 step:189 [D loss: 0.163169, acc.: 95.31%] [G loss: 5.081612]\n",
      "epoch:0 step:190 [D loss: 0.033063, acc.: 100.00%] [G loss: 4.493485]\n",
      "epoch:0 step:191 [D loss: 0.072223, acc.: 100.00%] [G loss: 5.256913]\n",
      "epoch:0 step:192 [D loss: 0.033227, acc.: 100.00%] [G loss: 4.297361]\n",
      "epoch:0 step:193 [D loss: 0.253767, acc.: 92.19%] [G loss: 7.422857]\n",
      "epoch:0 step:194 [D loss: 0.181672, acc.: 91.41%] [G loss: 7.009022]\n",
      "epoch:0 step:195 [D loss: 0.087292, acc.: 98.44%] [G loss: 4.213292]\n",
      "epoch:0 step:196 [D loss: 0.268477, acc.: 88.28%] [G loss: 8.442226]\n",
      "epoch:0 step:197 [D loss: 0.325585, acc.: 86.72%] [G loss: 7.003408]\n",
      "epoch:0 step:198 [D loss: 0.048824, acc.: 100.00%] [G loss: 4.802922]\n",
      "epoch:0 step:199 [D loss: 1.316414, acc.: 56.25%] [G loss: 9.849151]\n",
      "epoch:0 step:200 [D loss: 2.993531, acc.: 50.78%] [G loss: 4.435122]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/imi432_006/anaconda3/envs/tf/lib/python3.5/site-packages/ipykernel_launcher.py:57: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "/home/imi432_006/anaconda3/envs/tf/lib/python3.5/site-packages/ipykernel_launcher.py:59: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "/home/imi432_006/anaconda3/envs/tf/lib/python3.5/site-packages/ipykernel_launcher.py:86: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "/home/imi432_006/anaconda3/envs/tf/lib/python3.5/site-packages/ipykernel_launcher.py:90: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##############\n",
      "[1.11019018 1.04926753 1.11360338 0.9492411  2.10781991 0.95266313\n",
      " 1.11615209 2.10410392 0.94774163 1.11593774]\n",
      "##########\n",
      "epoch:0 step:201 [D loss: 0.602038, acc.: 75.78%] [G loss: 2.067403]\n",
      "epoch:0 step:202 [D loss: 0.346389, acc.: 85.16%] [G loss: 3.458471]\n",
      "epoch:0 step:203 [D loss: 0.087569, acc.: 99.22%] [G loss: 3.735810]\n",
      "epoch:0 step:204 [D loss: 0.154613, acc.: 96.88%] [G loss: 3.000330]\n",
      "epoch:0 step:205 [D loss: 0.172799, acc.: 95.31%] [G loss: 3.716680]\n",
      "epoch:0 step:206 [D loss: 0.082339, acc.: 99.22%] [G loss: 3.945674]\n",
      "epoch:0 step:207 [D loss: 0.400504, acc.: 85.16%] [G loss: 3.176835]\n",
      "epoch:0 step:208 [D loss: 0.066748, acc.: 99.22%] [G loss: 3.543852]\n",
      "epoch:0 step:209 [D loss: 0.061352, acc.: 100.00%] [G loss: 3.318827]\n",
      "epoch:0 step:210 [D loss: 0.204347, acc.: 98.44%] [G loss: 4.114627]\n",
      "epoch:0 step:211 [D loss: 0.040251, acc.: 100.00%] [G loss: 4.178303]\n",
      "epoch:0 step:212 [D loss: 0.199081, acc.: 95.31%] [G loss: 3.028768]\n",
      "epoch:0 step:213 [D loss: 0.167280, acc.: 96.88%] [G loss: 4.785638]\n",
      "epoch:0 step:214 [D loss: 0.057370, acc.: 98.44%] [G loss: 5.190790]\n",
      "epoch:0 step:215 [D loss: 0.094999, acc.: 97.66%] [G loss: 4.139446]\n",
      "epoch:0 step:216 [D loss: 0.187974, acc.: 92.97%] [G loss: 5.011975]\n",
      "epoch:0 step:217 [D loss: 0.068146, acc.: 98.44%] [G loss: 5.211174]\n",
      "epoch:0 step:218 [D loss: 0.095951, acc.: 99.22%] [G loss: 4.293092]\n",
      "epoch:0 step:219 [D loss: 0.186190, acc.: 93.75%] [G loss: 5.295014]\n",
      "epoch:0 step:220 [D loss: 0.059856, acc.: 100.00%] [G loss: 5.749265]\n",
      "epoch:0 step:221 [D loss: 0.101511, acc.: 98.44%] [G loss: 4.001930]\n",
      "epoch:0 step:222 [D loss: 0.246507, acc.: 92.97%] [G loss: 6.747728]\n",
      "epoch:0 step:223 [D loss: 0.168227, acc.: 93.75%] [G loss: 6.374112]\n",
      "epoch:0 step:224 [D loss: 0.157950, acc.: 96.88%] [G loss: 3.360574]\n",
      "epoch:0 step:225 [D loss: 0.458257, acc.: 85.16%] [G loss: 6.808816]\n",
      "epoch:0 step:226 [D loss: 0.341744, acc.: 84.38%] [G loss: 5.933594]\n",
      "epoch:0 step:227 [D loss: 0.031304, acc.: 100.00%] [G loss: 4.715365]\n",
      "epoch:0 step:228 [D loss: 0.078643, acc.: 100.00%] [G loss: 4.518034]\n",
      "epoch:0 step:229 [D loss: 0.071541, acc.: 99.22%] [G loss: 4.811862]\n",
      "epoch:0 step:230 [D loss: 0.158110, acc.: 93.75%] [G loss: 5.811076]\n",
      "epoch:0 step:231 [D loss: 0.246412, acc.: 94.53%] [G loss: 4.475462]\n",
      "epoch:0 step:232 [D loss: 0.109797, acc.: 98.44%] [G loss: 5.246756]\n",
      "epoch:0 step:233 [D loss: 0.022744, acc.: 100.00%] [G loss: 4.405954]\n",
      "epoch:0 step:234 [D loss: 0.813424, acc.: 65.62%] [G loss: 9.308800]\n",
      "epoch:0 step:235 [D loss: 2.035911, acc.: 52.34%] [G loss: 4.290587]\n",
      "epoch:0 step:236 [D loss: 0.270454, acc.: 88.28%] [G loss: 4.377801]\n",
      "epoch:0 step:237 [D loss: 0.037347, acc.: 100.00%] [G loss: 4.581131]\n",
      "epoch:0 step:238 [D loss: 0.082866, acc.: 96.88%] [G loss: 3.428071]\n",
      "epoch:0 step:239 [D loss: 0.230182, acc.: 91.41%] [G loss: 4.593387]\n",
      "epoch:0 step:240 [D loss: 0.247731, acc.: 90.62%] [G loss: 4.052016]\n",
      "epoch:0 step:241 [D loss: 0.194038, acc.: 93.75%] [G loss: 4.455960]\n",
      "epoch:0 step:242 [D loss: 0.070071, acc.: 96.88%] [G loss: 4.042866]\n",
      "epoch:0 step:243 [D loss: 0.112279, acc.: 98.44%] [G loss: 5.053076]\n",
      "epoch:0 step:244 [D loss: 0.111905, acc.: 98.44%] [G loss: 5.101623]\n",
      "epoch:0 step:245 [D loss: 0.106971, acc.: 97.66%] [G loss: 4.653371]\n",
      "epoch:0 step:246 [D loss: 0.129868, acc.: 97.66%] [G loss: 4.739658]\n",
      "epoch:0 step:247 [D loss: 0.045207, acc.: 100.00%] [G loss: 4.429749]\n",
      "epoch:0 step:248 [D loss: 0.125667, acc.: 98.44%] [G loss: 6.856512]\n",
      "epoch:0 step:249 [D loss: 0.107405, acc.: 96.09%] [G loss: 6.657269]\n",
      "epoch:0 step:250 [D loss: 0.104551, acc.: 96.88%] [G loss: 3.865347]\n",
      "epoch:0 step:251 [D loss: 0.392032, acc.: 78.91%] [G loss: 8.150492]\n",
      "epoch:0 step:252 [D loss: 0.404047, acc.: 78.12%] [G loss: 6.563198]\n",
      "epoch:0 step:253 [D loss: 0.103642, acc.: 96.88%] [G loss: 5.094228]\n",
      "epoch:0 step:254 [D loss: 0.047769, acc.: 100.00%] [G loss: 4.041303]\n",
      "epoch:0 step:255 [D loss: 0.362049, acc.: 78.91%] [G loss: 7.788136]\n",
      "epoch:0 step:256 [D loss: 0.380349, acc.: 78.91%] [G loss: 6.484902]\n",
      "epoch:0 step:257 [D loss: 0.019024, acc.: 100.00%] [G loss: 5.322453]\n",
      "epoch:0 step:258 [D loss: 0.083623, acc.: 98.44%] [G loss: 3.885630]\n",
      "epoch:0 step:259 [D loss: 0.091293, acc.: 97.66%] [G loss: 4.248973]\n",
      "epoch:0 step:260 [D loss: 0.061803, acc.: 100.00%] [G loss: 4.247029]\n",
      "epoch:0 step:261 [D loss: 0.171888, acc.: 94.53%] [G loss: 5.041532]\n",
      "epoch:0 step:262 [D loss: 0.226532, acc.: 92.97%] [G loss: 6.123004]\n",
      "epoch:0 step:263 [D loss: 0.269031, acc.: 91.41%] [G loss: 5.946706]\n",
      "epoch:0 step:264 [D loss: 0.943892, acc.: 60.94%] [G loss: 6.996321]\n",
      "epoch:0 step:265 [D loss: 0.122695, acc.: 96.88%] [G loss: 6.709460]\n",
      "epoch:0 step:266 [D loss: 0.068621, acc.: 98.44%] [G loss: 3.457643]\n",
      "epoch:0 step:267 [D loss: 0.094568, acc.: 98.44%] [G loss: 1.050958]\n",
      "epoch:0 step:268 [D loss: 0.243511, acc.: 89.84%] [G loss: 3.637643]\n",
      "epoch:0 step:269 [D loss: 0.260943, acc.: 89.06%] [G loss: 0.390298]\n",
      "epoch:0 step:270 [D loss: 0.127612, acc.: 95.31%] [G loss: 0.003249]\n",
      "epoch:0 step:271 [D loss: 0.117114, acc.: 95.31%] [G loss: 2.690872]\n",
      "epoch:0 step:272 [D loss: 0.135834, acc.: 95.31%] [G loss: 0.414339]\n",
      "epoch:0 step:273 [D loss: 1.241742, acc.: 53.91%] [G loss: 9.060470]\n",
      "epoch:0 step:274 [D loss: 2.699804, acc.: 50.00%] [G loss: 3.013034]\n",
      "epoch:0 step:275 [D loss: 1.002722, acc.: 56.25%] [G loss: 1.735220]\n",
      "epoch:0 step:276 [D loss: 0.278602, acc.: 92.19%] [G loss: 2.246496]\n",
      "epoch:0 step:277 [D loss: 0.383314, acc.: 85.16%] [G loss: 1.900852]\n",
      "epoch:0 step:278 [D loss: 0.324586, acc.: 95.31%] [G loss: 2.849959]\n",
      "epoch:0 step:279 [D loss: 0.276436, acc.: 90.62%] [G loss: 2.479446]\n",
      "epoch:0 step:280 [D loss: 0.874660, acc.: 46.09%] [G loss: 3.971643]\n",
      "epoch:0 step:281 [D loss: 0.465898, acc.: 75.00%] [G loss: 3.489039]\n",
      "epoch:0 step:282 [D loss: 0.337403, acc.: 89.84%] [G loss: 2.466809]\n",
      "epoch:0 step:283 [D loss: 0.171649, acc.: 96.09%] [G loss: 2.123431]\n",
      "epoch:0 step:284 [D loss: 0.156467, acc.: 99.22%] [G loss: 2.976273]\n",
      "epoch:0 step:285 [D loss: 0.109386, acc.: 98.44%] [G loss: 3.252550]\n",
      "epoch:0 step:286 [D loss: 0.212641, acc.: 93.75%] [G loss: 3.138929]\n",
      "epoch:0 step:287 [D loss: 0.142245, acc.: 98.44%] [G loss: 2.665831]\n",
      "epoch:0 step:288 [D loss: 0.133168, acc.: 96.88%] [G loss: 3.340081]\n",
      "epoch:0 step:289 [D loss: 0.103646, acc.: 99.22%] [G loss: 3.327786]\n",
      "epoch:0 step:290 [D loss: 0.250867, acc.: 94.53%] [G loss: 4.531126]\n",
      "epoch:0 step:291 [D loss: 0.165844, acc.: 92.19%] [G loss: 4.120448]\n",
      "epoch:0 step:292 [D loss: 0.132400, acc.: 96.88%] [G loss: 3.903788]\n",
      "epoch:0 step:293 [D loss: 0.097201, acc.: 100.00%] [G loss: 3.962682]\n",
      "epoch:0 step:294 [D loss: 0.146142, acc.: 97.66%] [G loss: 4.623888]\n",
      "epoch:0 step:295 [D loss: 0.178206, acc.: 95.31%] [G loss: 3.665127]\n",
      "epoch:0 step:296 [D loss: 0.198787, acc.: 96.88%] [G loss: 5.767270]\n",
      "epoch:0 step:297 [D loss: 0.589549, acc.: 67.97%] [G loss: 2.733651]\n",
      "epoch:0 step:298 [D loss: 0.385100, acc.: 90.62%] [G loss: 4.363026]\n",
      "epoch:0 step:299 [D loss: 0.184425, acc.: 95.31%] [G loss: 4.630487]\n",
      "epoch:0 step:300 [D loss: 0.182983, acc.: 93.75%] [G loss: 3.555947]\n",
      "epoch:0 step:301 [D loss: 0.118869, acc.: 97.66%] [G loss: 4.047062]\n",
      "epoch:0 step:302 [D loss: 0.070410, acc.: 100.00%] [G loss: 4.162660]\n",
      "epoch:0 step:303 [D loss: 0.318197, acc.: 85.16%] [G loss: 7.164018]\n",
      "epoch:0 step:304 [D loss: 0.557232, acc.: 78.91%] [G loss: 4.852180]\n",
      "epoch:0 step:305 [D loss: 0.130369, acc.: 95.31%] [G loss: 4.172857]\n",
      "epoch:0 step:306 [D loss: 0.095734, acc.: 98.44%] [G loss: 4.473855]\n",
      "epoch:0 step:307 [D loss: 0.261220, acc.: 87.50%] [G loss: 5.410233]\n",
      "epoch:0 step:308 [D loss: 0.199121, acc.: 91.41%] [G loss: 4.530937]\n",
      "epoch:0 step:309 [D loss: 0.267562, acc.: 92.97%] [G loss: 5.721591]\n",
      "epoch:0 step:310 [D loss: 0.151313, acc.: 95.31%] [G loss: 4.992071]\n",
      "epoch:0 step:311 [D loss: 0.063995, acc.: 98.44%] [G loss: 3.679485]\n",
      "epoch:0 step:312 [D loss: 0.093166, acc.: 100.00%] [G loss: 4.660382]\n",
      "epoch:0 step:313 [D loss: 0.048076, acc.: 100.00%] [G loss: 4.684212]\n",
      "epoch:0 step:314 [D loss: 0.210419, acc.: 93.75%] [G loss: 6.683884]\n",
      "epoch:0 step:315 [D loss: 0.357798, acc.: 83.59%] [G loss: 4.875176]\n",
      "epoch:0 step:316 [D loss: 0.214648, acc.: 89.84%] [G loss: 6.679873]\n",
      "epoch:0 step:317 [D loss: 0.196985, acc.: 92.19%] [G loss: 5.534068]\n",
      "epoch:0 step:318 [D loss: 0.032271, acc.: 100.00%] [G loss: 4.743816]\n",
      "epoch:0 step:319 [D loss: 0.068146, acc.: 99.22%] [G loss: 5.583553]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0 step:320 [D loss: 0.066540, acc.: 97.66%] [G loss: 5.369120]\n",
      "epoch:0 step:321 [D loss: 0.066073, acc.: 98.44%] [G loss: 5.043574]\n",
      "epoch:0 step:322 [D loss: 0.271358, acc.: 88.28%] [G loss: 5.390779]\n",
      "epoch:0 step:323 [D loss: 0.028289, acc.: 99.22%] [G loss: 4.948421]\n",
      "epoch:0 step:324 [D loss: 0.113970, acc.: 98.44%] [G loss: 5.614791]\n",
      "epoch:0 step:325 [D loss: 0.081469, acc.: 97.66%] [G loss: 4.464667]\n",
      "epoch:0 step:326 [D loss: 0.288900, acc.: 89.84%] [G loss: 8.324209]\n",
      "epoch:0 step:327 [D loss: 0.497775, acc.: 81.25%] [G loss: 4.286788]\n",
      "epoch:0 step:328 [D loss: 0.085712, acc.: 96.88%] [G loss: 4.378011]\n",
      "epoch:0 step:329 [D loss: 0.072881, acc.: 96.88%] [G loss: 4.681573]\n",
      "epoch:0 step:330 [D loss: 0.117933, acc.: 97.66%] [G loss: 5.647578]\n",
      "epoch:0 step:331 [D loss: 0.097555, acc.: 97.66%] [G loss: 5.301248]\n",
      "epoch:0 step:332 [D loss: 0.369080, acc.: 84.38%] [G loss: 3.713830]\n",
      "epoch:0 step:333 [D loss: 0.030147, acc.: 100.00%] [G loss: 2.798184]\n",
      "epoch:0 step:334 [D loss: 0.321646, acc.: 82.03%] [G loss: 7.096303]\n",
      "epoch:0 step:335 [D loss: 0.560919, acc.: 77.34%] [G loss: 0.968466]\n",
      "epoch:0 step:336 [D loss: 0.486673, acc.: 73.44%] [G loss: 7.761087]\n",
      "epoch:0 step:337 [D loss: 0.296914, acc.: 89.84%] [G loss: 7.117831]\n",
      "epoch:0 step:338 [D loss: 0.211170, acc.: 89.06%] [G loss: 2.298730]\n",
      "epoch:0 step:339 [D loss: 0.324937, acc.: 81.25%] [G loss: 5.325795]\n",
      "epoch:0 step:340 [D loss: 0.052523, acc.: 97.66%] [G loss: 6.126318]\n",
      "epoch:0 step:341 [D loss: 0.144827, acc.: 95.31%] [G loss: 3.460056]\n",
      "epoch:0 step:342 [D loss: 0.210291, acc.: 89.06%] [G loss: 3.928010]\n",
      "epoch:0 step:343 [D loss: 0.151161, acc.: 95.31%] [G loss: 2.530027]\n",
      "epoch:0 step:344 [D loss: 0.143583, acc.: 95.31%] [G loss: 3.119131]\n",
      "epoch:0 step:345 [D loss: 0.073983, acc.: 96.09%] [G loss: 4.307799]\n",
      "epoch:0 step:346 [D loss: 0.234595, acc.: 92.19%] [G loss: 1.536332]\n",
      "epoch:0 step:347 [D loss: 0.604004, acc.: 68.75%] [G loss: 8.121418]\n",
      "epoch:0 step:348 [D loss: 2.183631, acc.: 51.56%] [G loss: 4.114098]\n",
      "epoch:0 step:349 [D loss: 0.145419, acc.: 95.31%] [G loss: 2.406651]\n",
      "epoch:0 step:350 [D loss: 0.238721, acc.: 92.19%] [G loss: 3.177146]\n",
      "epoch:0 step:351 [D loss: 0.051038, acc.: 99.22%] [G loss: 3.351249]\n",
      "epoch:0 step:352 [D loss: 0.266660, acc.: 88.28%] [G loss: 3.725879]\n",
      "epoch:0 step:353 [D loss: 0.089629, acc.: 99.22%] [G loss: 3.401101]\n",
      "epoch:0 step:354 [D loss: 0.262707, acc.: 92.19%] [G loss: 3.073941]\n",
      "epoch:0 step:355 [D loss: 1.630283, acc.: 35.94%] [G loss: 5.995951]\n",
      "epoch:0 step:356 [D loss: 1.578307, acc.: 50.78%] [G loss: 3.770918]\n",
      "epoch:0 step:357 [D loss: 0.522692, acc.: 76.56%] [G loss: 2.545712]\n",
      "epoch:0 step:358 [D loss: 0.178863, acc.: 93.75%] [G loss: 2.841115]\n",
      "epoch:0 step:359 [D loss: 0.144418, acc.: 97.66%] [G loss: 2.843343]\n",
      "epoch:0 step:360 [D loss: 0.124592, acc.: 99.22%] [G loss: 2.844810]\n",
      "epoch:0 step:361 [D loss: 0.103966, acc.: 98.44%] [G loss: 2.730094]\n",
      "epoch:0 step:362 [D loss: 0.098371, acc.: 100.00%] [G loss: 2.594442]\n",
      "epoch:0 step:363 [D loss: 0.181896, acc.: 96.09%] [G loss: 2.828196]\n",
      "epoch:0 step:364 [D loss: 0.159983, acc.: 98.44%] [G loss: 3.091953]\n",
      "epoch:0 step:365 [D loss: 0.140932, acc.: 97.66%] [G loss: 3.360927]\n",
      "epoch:0 step:366 [D loss: 0.147732, acc.: 97.66%] [G loss: 2.737930]\n",
      "epoch:0 step:367 [D loss: 0.201312, acc.: 98.44%] [G loss: 3.910794]\n",
      "epoch:0 step:368 [D loss: 0.195183, acc.: 93.75%] [G loss: 4.072462]\n",
      "epoch:0 step:369 [D loss: 0.097142, acc.: 99.22%] [G loss: 3.852157]\n",
      "epoch:0 step:370 [D loss: 0.124155, acc.: 98.44%] [G loss: 3.174680]\n",
      "epoch:0 step:371 [D loss: 0.137074, acc.: 97.66%] [G loss: 3.850258]\n",
      "epoch:0 step:372 [D loss: 0.217521, acc.: 95.31%] [G loss: 4.598284]\n",
      "epoch:0 step:373 [D loss: 0.114089, acc.: 96.88%] [G loss: 3.296789]\n",
      "epoch:0 step:374 [D loss: 0.588881, acc.: 68.75%] [G loss: 7.428363]\n",
      "epoch:0 step:375 [D loss: 1.037685, acc.: 63.28%] [G loss: 4.512332]\n",
      "epoch:0 step:376 [D loss: 0.088537, acc.: 100.00%] [G loss: 3.515234]\n",
      "epoch:0 step:377 [D loss: 0.173981, acc.: 95.31%] [G loss: 3.880877]\n",
      "epoch:0 step:378 [D loss: 0.151975, acc.: 96.88%] [G loss: 3.848777]\n",
      "epoch:0 step:379 [D loss: 0.073743, acc.: 100.00%] [G loss: 3.553360]\n",
      "epoch:0 step:380 [D loss: 0.199070, acc.: 95.31%] [G loss: 4.135926]\n",
      "epoch:0 step:381 [D loss: 0.366134, acc.: 82.03%] [G loss: 3.810628]\n",
      "epoch:0 step:382 [D loss: 0.203305, acc.: 95.31%] [G loss: 3.397233]\n",
      "epoch:0 step:383 [D loss: 0.160518, acc.: 96.88%] [G loss: 5.699382]\n",
      "epoch:0 step:384 [D loss: 0.400320, acc.: 82.81%] [G loss: 2.675504]\n",
      "epoch:0 step:385 [D loss: 0.683744, acc.: 57.81%] [G loss: 7.664554]\n",
      "epoch:0 step:386 [D loss: 1.163989, acc.: 54.69%] [G loss: 4.394529]\n",
      "epoch:0 step:387 [D loss: 0.221845, acc.: 91.41%] [G loss: 3.670205]\n",
      "epoch:0 step:388 [D loss: 0.079636, acc.: 100.00%] [G loss: 2.993515]\n",
      "epoch:0 step:389 [D loss: 0.214012, acc.: 89.06%] [G loss: 3.792075]\n",
      "epoch:0 step:390 [D loss: 0.126368, acc.: 97.66%] [G loss: 3.367250]\n",
      "epoch:0 step:391 [D loss: 0.306156, acc.: 90.62%] [G loss: 4.966785]\n",
      "epoch:0 step:392 [D loss: 0.149099, acc.: 95.31%] [G loss: 4.465396]\n",
      "epoch:0 step:393 [D loss: 0.299540, acc.: 85.16%] [G loss: 4.007552]\n",
      "epoch:0 step:394 [D loss: 0.173188, acc.: 93.75%] [G loss: 4.894272]\n",
      "epoch:0 step:395 [D loss: 0.172492, acc.: 92.97%] [G loss: 3.487194]\n",
      "epoch:0 step:396 [D loss: 0.427390, acc.: 84.38%] [G loss: 5.690786]\n",
      "epoch:0 step:397 [D loss: 0.450359, acc.: 80.47%] [G loss: 3.600736]\n",
      "epoch:0 step:398 [D loss: 0.485822, acc.: 78.91%] [G loss: 5.888058]\n",
      "epoch:0 step:399 [D loss: 0.572421, acc.: 75.00%] [G loss: 4.116169]\n",
      "epoch:0 step:400 [D loss: 0.173140, acc.: 95.31%] [G loss: 3.991164]\n",
      "##############\n",
      "[0.83274165 0.93417763 1.04135162 1.01371053 2.13241578 1.10431097\n",
      " 0.9641468  2.12178273 2.10941437 2.10937043]\n",
      "##########\n",
      "epoch:0 step:401 [D loss: 0.150809, acc.: 95.31%] [G loss: 4.814259]\n",
      "epoch:0 step:402 [D loss: 0.213681, acc.: 91.41%] [G loss: 4.048933]\n",
      "epoch:0 step:403 [D loss: 0.172663, acc.: 93.75%] [G loss: 4.311447]\n",
      "epoch:0 step:404 [D loss: 0.179221, acc.: 92.97%] [G loss: 3.020272]\n",
      "epoch:0 step:405 [D loss: 0.269267, acc.: 89.84%] [G loss: 5.543983]\n",
      "epoch:0 step:406 [D loss: 0.828238, acc.: 61.72%] [G loss: 1.888995]\n",
      "epoch:0 step:407 [D loss: 0.384909, acc.: 82.03%] [G loss: 5.225781]\n",
      "epoch:0 step:408 [D loss: 0.231151, acc.: 85.94%] [G loss: 4.773274]\n",
      "epoch:0 step:409 [D loss: 0.215326, acc.: 92.97%] [G loss: 3.267440]\n",
      "epoch:0 step:410 [D loss: 0.095642, acc.: 100.00%] [G loss: 2.991467]\n",
      "epoch:0 step:411 [D loss: 0.106721, acc.: 99.22%] [G loss: 3.302810]\n",
      "epoch:0 step:412 [D loss: 0.132740, acc.: 98.44%] [G loss: 3.598198]\n",
      "epoch:0 step:413 [D loss: 0.166168, acc.: 94.53%] [G loss: 2.943607]\n",
      "epoch:0 step:414 [D loss: 0.214889, acc.: 91.41%] [G loss: 5.744565]\n",
      "epoch:0 step:415 [D loss: 0.292320, acc.: 86.72%] [G loss: 4.289054]\n",
      "epoch:0 step:416 [D loss: 0.050845, acc.: 100.00%] [G loss: 3.904036]\n",
      "epoch:0 step:417 [D loss: 0.042205, acc.: 100.00%] [G loss: 3.335456]\n",
      "epoch:0 step:418 [D loss: 0.137987, acc.: 96.09%] [G loss: 5.549067]\n",
      "epoch:0 step:419 [D loss: 0.125084, acc.: 96.88%] [G loss: 4.019477]\n",
      "epoch:0 step:420 [D loss: 0.260523, acc.: 91.41%] [G loss: 5.300675]\n",
      "epoch:0 step:421 [D loss: 0.269920, acc.: 90.62%] [G loss: 3.299978]\n",
      "epoch:0 step:422 [D loss: 0.293646, acc.: 85.16%] [G loss: 6.808390]\n",
      "epoch:0 step:423 [D loss: 0.151206, acc.: 94.53%] [G loss: 6.608116]\n",
      "epoch:0 step:424 [D loss: 0.352783, acc.: 82.81%] [G loss: 2.387725]\n",
      "epoch:0 step:425 [D loss: 0.988462, acc.: 53.91%] [G loss: 8.098816]\n",
      "epoch:0 step:426 [D loss: 1.493408, acc.: 53.91%] [G loss: 5.415082]\n",
      "epoch:0 step:427 [D loss: 0.099682, acc.: 96.88%] [G loss: 4.277419]\n",
      "epoch:0 step:428 [D loss: 0.034810, acc.: 100.00%] [G loss: 3.067604]\n",
      "epoch:0 step:429 [D loss: 0.079537, acc.: 99.22%] [G loss: 3.133137]\n",
      "epoch:0 step:430 [D loss: 0.065508, acc.: 100.00%] [G loss: 3.263208]\n",
      "epoch:0 step:431 [D loss: 0.157595, acc.: 98.44%] [G loss: 3.413553]\n",
      "epoch:0 step:432 [D loss: 0.051705, acc.: 100.00%] [G loss: 4.165894]\n",
      "epoch:0 step:433 [D loss: 0.139334, acc.: 98.44%] [G loss: 3.418402]\n",
      "epoch:0 step:434 [D loss: 0.151461, acc.: 99.22%] [G loss: 4.179335]\n",
      "epoch:0 step:435 [D loss: 0.216125, acc.: 94.53%] [G loss: 4.315458]\n",
      "epoch:0 step:436 [D loss: 0.203990, acc.: 94.53%] [G loss: 4.030182]\n",
      "epoch:0 step:437 [D loss: 0.372057, acc.: 89.06%] [G loss: 6.588706]\n",
      "epoch:0 step:438 [D loss: 0.379790, acc.: 80.47%] [G loss: 5.394420]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0 step:439 [D loss: 0.046024, acc.: 99.22%] [G loss: 4.210947]\n",
      "epoch:0 step:440 [D loss: 0.073481, acc.: 99.22%] [G loss: 3.296036]\n",
      "epoch:0 step:441 [D loss: 0.231604, acc.: 92.19%] [G loss: 5.081691]\n",
      "epoch:0 step:442 [D loss: 0.193174, acc.: 91.41%] [G loss: 3.891845]\n",
      "epoch:0 step:443 [D loss: 1.063553, acc.: 50.00%] [G loss: 6.803382]\n",
      "epoch:0 step:444 [D loss: 1.427831, acc.: 54.69%] [G loss: 4.801135]\n",
      "epoch:0 step:445 [D loss: 0.304797, acc.: 85.94%] [G loss: 2.684734]\n",
      "epoch:0 step:446 [D loss: 0.160640, acc.: 95.31%] [G loss: 2.518856]\n",
      "epoch:0 step:447 [D loss: 0.249508, acc.: 92.19%] [G loss: 3.565414]\n",
      "epoch:0 step:448 [D loss: 0.187587, acc.: 95.31%] [G loss: 3.072853]\n",
      "epoch:0 step:449 [D loss: 0.529503, acc.: 68.75%] [G loss: 4.921705]\n",
      "epoch:0 step:450 [D loss: 0.207933, acc.: 89.06%] [G loss: 4.399304]\n",
      "epoch:0 step:451 [D loss: 0.171187, acc.: 92.97%] [G loss: 2.718205]\n",
      "epoch:0 step:452 [D loss: 0.120526, acc.: 100.00%] [G loss: 2.050800]\n",
      "epoch:0 step:453 [D loss: 0.129014, acc.: 96.88%] [G loss: 3.036164]\n",
      "epoch:0 step:454 [D loss: 0.040433, acc.: 100.00%] [G loss: 3.338528]\n",
      "epoch:0 step:455 [D loss: 0.541043, acc.: 67.19%] [G loss: 5.806447]\n",
      "epoch:0 step:456 [D loss: 1.027553, acc.: 59.38%] [G loss: 2.995116]\n",
      "epoch:0 step:457 [D loss: 0.159311, acc.: 92.97%] [G loss: 3.291541]\n",
      "epoch:0 step:458 [D loss: 0.071649, acc.: 98.44%] [G loss: 3.925362]\n",
      "epoch:0 step:459 [D loss: 0.130134, acc.: 96.09%] [G loss: 3.288564]\n",
      "epoch:0 step:460 [D loss: 0.257413, acc.: 90.62%] [G loss: 4.173472]\n",
      "epoch:0 step:461 [D loss: 0.160169, acc.: 96.09%] [G loss: 3.310801]\n",
      "epoch:0 step:462 [D loss: 0.328495, acc.: 87.50%] [G loss: 4.823902]\n",
      "epoch:0 step:463 [D loss: 0.116581, acc.: 97.66%] [G loss: 4.036448]\n",
      "epoch:0 step:464 [D loss: 0.174761, acc.: 96.09%] [G loss: 2.954990]\n",
      "epoch:0 step:465 [D loss: 0.713320, acc.: 64.06%] [G loss: 6.813414]\n",
      "epoch:0 step:466 [D loss: 1.159652, acc.: 60.94%] [G loss: 5.105210]\n",
      "epoch:0 step:467 [D loss: 0.299112, acc.: 85.16%] [G loss: 3.514690]\n",
      "epoch:0 step:468 [D loss: 0.076015, acc.: 99.22%] [G loss: 3.374274]\n",
      "epoch:0 step:469 [D loss: 0.062687, acc.: 100.00%] [G loss: 3.220337]\n",
      "epoch:0 step:470 [D loss: 0.189589, acc.: 94.53%] [G loss: 4.146834]\n",
      "epoch:0 step:471 [D loss: 0.070814, acc.: 99.22%] [G loss: 4.420712]\n",
      "epoch:0 step:472 [D loss: 0.153569, acc.: 95.31%] [G loss: 3.476854]\n",
      "epoch:0 step:473 [D loss: 0.254105, acc.: 93.75%] [G loss: 5.641864]\n",
      "epoch:0 step:474 [D loss: 0.191385, acc.: 90.62%] [G loss: 5.059452]\n",
      "epoch:0 step:475 [D loss: 0.101274, acc.: 99.22%] [G loss: 3.961945]\n",
      "epoch:0 step:476 [D loss: 0.311029, acc.: 89.06%] [G loss: 6.299998]\n",
      "epoch:0 step:477 [D loss: 0.130519, acc.: 94.53%] [G loss: 6.150342]\n",
      "epoch:0 step:478 [D loss: 0.253916, acc.: 87.50%] [G loss: 3.204680]\n",
      "epoch:0 step:479 [D loss: 0.701342, acc.: 67.19%] [G loss: 6.307108]\n",
      "epoch:0 step:480 [D loss: 0.409521, acc.: 78.12%] [G loss: 5.356845]\n",
      "epoch:0 step:481 [D loss: 0.178632, acc.: 93.75%] [G loss: 3.585100]\n",
      "epoch:0 step:482 [D loss: 0.140439, acc.: 96.88%] [G loss: 2.942774]\n",
      "epoch:0 step:483 [D loss: 0.127328, acc.: 96.88%] [G loss: 3.619605]\n",
      "epoch:0 step:484 [D loss: 0.191172, acc.: 93.75%] [G loss: 4.646578]\n",
      "epoch:0 step:485 [D loss: 0.146089, acc.: 94.53%] [G loss: 3.839283]\n",
      "epoch:0 step:486 [D loss: 0.562583, acc.: 70.31%] [G loss: 6.175557]\n",
      "epoch:0 step:487 [D loss: 0.617258, acc.: 74.22%] [G loss: 3.875726]\n",
      "epoch:0 step:488 [D loss: 0.252808, acc.: 92.19%] [G loss: 2.777743]\n",
      "epoch:0 step:489 [D loss: 0.391355, acc.: 82.81%] [G loss: 5.241420]\n",
      "epoch:0 step:490 [D loss: 0.336993, acc.: 83.59%] [G loss: 3.648102]\n",
      "epoch:0 step:491 [D loss: 0.340069, acc.: 87.50%] [G loss: 4.426759]\n",
      "epoch:0 step:492 [D loss: 0.201868, acc.: 95.31%] [G loss: 4.424971]\n",
      "epoch:0 step:493 [D loss: 0.203164, acc.: 94.53%] [G loss: 3.767535]\n",
      "epoch:0 step:494 [D loss: 0.168723, acc.: 93.75%] [G loss: 4.838727]\n",
      "epoch:0 step:495 [D loss: 0.122287, acc.: 97.66%] [G loss: 4.014236]\n",
      "epoch:0 step:496 [D loss: 0.158903, acc.: 96.88%] [G loss: 3.935652]\n",
      "epoch:0 step:497 [D loss: 0.067906, acc.: 100.00%] [G loss: 3.414443]\n",
      "epoch:0 step:498 [D loss: 0.156517, acc.: 97.66%] [G loss: 3.926857]\n",
      "epoch:0 step:499 [D loss: 0.113798, acc.: 99.22%] [G loss: 3.516539]\n",
      "epoch:0 step:500 [D loss: 0.391612, acc.: 85.16%] [G loss: 5.536400]\n",
      "epoch:0 step:501 [D loss: 0.274437, acc.: 88.28%] [G loss: 3.349525]\n",
      "epoch:0 step:502 [D loss: 0.075314, acc.: 100.00%] [G loss: 3.264498]\n",
      "epoch:0 step:503 [D loss: 0.035981, acc.: 100.00%] [G loss: 2.908542]\n",
      "epoch:0 step:504 [D loss: 0.202719, acc.: 95.31%] [G loss: 4.509262]\n",
      "epoch:0 step:505 [D loss: 0.152466, acc.: 95.31%] [G loss: 3.447199]\n",
      "epoch:0 step:506 [D loss: 0.254584, acc.: 91.41%] [G loss: 5.319265]\n",
      "epoch:0 step:507 [D loss: 0.341339, acc.: 82.03%] [G loss: 2.338561]\n",
      "epoch:0 step:508 [D loss: 0.907041, acc.: 60.94%] [G loss: 7.292558]\n",
      "epoch:0 step:509 [D loss: 1.172145, acc.: 51.56%] [G loss: 4.490286]\n",
      "epoch:0 step:510 [D loss: 0.172935, acc.: 94.53%] [G loss: 2.721178]\n",
      "epoch:0 step:511 [D loss: 0.203135, acc.: 92.19%] [G loss: 3.086602]\n",
      "epoch:0 step:512 [D loss: 0.131540, acc.: 98.44%] [G loss: 3.495456]\n",
      "epoch:0 step:513 [D loss: 0.233108, acc.: 93.75%] [G loss: 3.666800]\n",
      "epoch:0 step:514 [D loss: 0.169096, acc.: 93.75%] [G loss: 2.453484]\n",
      "epoch:0 step:515 [D loss: 0.740486, acc.: 65.62%] [G loss: 5.377866]\n",
      "epoch:0 step:516 [D loss: 0.594899, acc.: 75.00%] [G loss: 4.109453]\n",
      "epoch:0 step:517 [D loss: 0.167922, acc.: 93.75%] [G loss: 2.831331]\n",
      "epoch:0 step:518 [D loss: 0.215465, acc.: 92.19%] [G loss: 3.471067]\n",
      "epoch:0 step:519 [D loss: 0.109005, acc.: 98.44%] [G loss: 3.028903]\n",
      "epoch:0 step:520 [D loss: 0.122900, acc.: 99.22%] [G loss: 3.769655]\n",
      "epoch:0 step:521 [D loss: 0.081566, acc.: 99.22%] [G loss: 3.881302]\n",
      "epoch:0 step:522 [D loss: 0.110501, acc.: 99.22%] [G loss: 3.590576]\n",
      "epoch:0 step:523 [D loss: 0.142242, acc.: 96.09%] [G loss: 4.410480]\n",
      "epoch:0 step:524 [D loss: 0.113261, acc.: 96.88%] [G loss: 4.134337]\n",
      "epoch:0 step:525 [D loss: 0.133267, acc.: 96.88%] [G loss: 3.575123]\n",
      "epoch:0 step:526 [D loss: 0.109357, acc.: 98.44%] [G loss: 3.096490]\n",
      "epoch:0 step:527 [D loss: 0.068995, acc.: 99.22%] [G loss: 4.063350]\n",
      "epoch:0 step:528 [D loss: 0.084961, acc.: 98.44%] [G loss: 4.514465]\n",
      "epoch:0 step:529 [D loss: 0.429445, acc.: 78.12%] [G loss: 7.113901]\n",
      "epoch:0 step:530 [D loss: 0.489799, acc.: 80.47%] [G loss: 5.362035]\n",
      "epoch:0 step:531 [D loss: 0.048093, acc.: 100.00%] [G loss: 4.527162]\n",
      "epoch:0 step:532 [D loss: 0.097227, acc.: 99.22%] [G loss: 4.734834]\n",
      "epoch:0 step:533 [D loss: 0.078733, acc.: 97.66%] [G loss: 4.491950]\n",
      "epoch:0 step:534 [D loss: 0.123754, acc.: 97.66%] [G loss: 5.030515]\n",
      "epoch:0 step:535 [D loss: 0.178178, acc.: 92.97%] [G loss: 5.235291]\n",
      "epoch:0 step:536 [D loss: 0.098385, acc.: 96.88%] [G loss: 4.384403]\n",
      "epoch:0 step:537 [D loss: 0.090749, acc.: 98.44%] [G loss: 5.254498]\n",
      "epoch:0 step:538 [D loss: 0.052785, acc.: 99.22%] [G loss: 4.447497]\n",
      "epoch:0 step:539 [D loss: 0.122000, acc.: 96.88%] [G loss: 5.381652]\n",
      "epoch:0 step:540 [D loss: 0.560593, acc.: 72.66%] [G loss: 6.958381]\n",
      "epoch:0 step:541 [D loss: 0.160191, acc.: 92.19%] [G loss: 5.322963]\n",
      "epoch:0 step:542 [D loss: 0.096160, acc.: 95.31%] [G loss: 4.901269]\n",
      "epoch:0 step:543 [D loss: 0.043554, acc.: 99.22%] [G loss: 3.399054]\n",
      "epoch:0 step:544 [D loss: 0.243923, acc.: 87.50%] [G loss: 7.255911]\n",
      "epoch:0 step:545 [D loss: 0.437010, acc.: 76.56%] [G loss: 2.581244]\n",
      "epoch:0 step:546 [D loss: 0.692576, acc.: 71.88%] [G loss: 6.846526]\n",
      "epoch:0 step:547 [D loss: 0.384356, acc.: 77.34%] [G loss: 4.643285]\n",
      "epoch:0 step:548 [D loss: 0.078616, acc.: 97.66%] [G loss: 3.585799]\n",
      "epoch:0 step:549 [D loss: 0.094434, acc.: 99.22%] [G loss: 3.603590]\n",
      "epoch:0 step:550 [D loss: 0.187640, acc.: 92.97%] [G loss: 5.052377]\n",
      "epoch:0 step:551 [D loss: 0.170089, acc.: 95.31%] [G loss: 4.092619]\n",
      "epoch:0 step:552 [D loss: 0.402038, acc.: 78.12%] [G loss: 7.757523]\n",
      "epoch:0 step:553 [D loss: 0.989878, acc.: 60.94%] [G loss: 2.496642]\n",
      "epoch:0 step:554 [D loss: 0.169546, acc.: 96.09%] [G loss: 4.064779]\n",
      "epoch:0 step:555 [D loss: 0.033925, acc.: 99.22%] [G loss: 4.582937]\n",
      "epoch:0 step:556 [D loss: 0.118186, acc.: 96.88%] [G loss: 2.347630]\n",
      "epoch:0 step:557 [D loss: 0.517833, acc.: 71.88%] [G loss: 6.715250]\n",
      "epoch:0 step:558 [D loss: 1.092782, acc.: 52.34%] [G loss: 3.440776]\n",
      "epoch:0 step:559 [D loss: 0.162374, acc.: 96.88%] [G loss: 1.974313]\n",
      "epoch:0 step:560 [D loss: 0.129237, acc.: 97.66%] [G loss: 3.237240]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0 step:561 [D loss: 0.051690, acc.: 99.22%] [G loss: 3.633543]\n",
      "epoch:0 step:562 [D loss: 0.121096, acc.: 96.88%] [G loss: 3.463158]\n",
      "epoch:0 step:563 [D loss: 0.120673, acc.: 99.22%] [G loss: 3.158081]\n",
      "epoch:0 step:564 [D loss: 0.145686, acc.: 97.66%] [G loss: 3.643195]\n",
      "epoch:0 step:565 [D loss: 0.102181, acc.: 98.44%] [G loss: 4.723254]\n",
      "epoch:0 step:566 [D loss: 0.484857, acc.: 76.56%] [G loss: 5.646984]\n",
      "epoch:0 step:567 [D loss: 0.158925, acc.: 95.31%] [G loss: 4.986733]\n",
      "epoch:0 step:568 [D loss: 0.171283, acc.: 96.88%] [G loss: 4.018470]\n",
      "epoch:0 step:569 [D loss: 0.094606, acc.: 97.66%] [G loss: 4.956581]\n",
      "epoch:0 step:570 [D loss: 0.414760, acc.: 83.59%] [G loss: 7.359633]\n",
      "epoch:0 step:571 [D loss: 0.461241, acc.: 81.25%] [G loss: 6.077284]\n",
      "epoch:0 step:572 [D loss: 0.021508, acc.: 100.00%] [G loss: 5.006406]\n",
      "epoch:0 step:573 [D loss: 0.030274, acc.: 100.00%] [G loss: 4.152619]\n",
      "epoch:0 step:574 [D loss: 0.090852, acc.: 97.66%] [G loss: 4.678169]\n",
      "epoch:0 step:575 [D loss: 0.059295, acc.: 100.00%] [G loss: 4.648327]\n",
      "epoch:0 step:576 [D loss: 0.098592, acc.: 100.00%] [G loss: 5.301382]\n",
      "epoch:0 step:577 [D loss: 0.101094, acc.: 99.22%] [G loss: 5.172512]\n",
      "epoch:0 step:578 [D loss: 0.116055, acc.: 98.44%] [G loss: 5.889819]\n",
      "epoch:0 step:579 [D loss: 0.101427, acc.: 96.09%] [G loss: 4.190583]\n",
      "epoch:0 step:580 [D loss: 0.471214, acc.: 76.56%] [G loss: 8.307842]\n",
      "epoch:0 step:581 [D loss: 1.477899, acc.: 53.91%] [G loss: 4.690882]\n",
      "epoch:0 step:582 [D loss: 0.086615, acc.: 97.66%] [G loss: 4.008538]\n",
      "epoch:0 step:583 [D loss: 0.025895, acc.: 100.00%] [G loss: 3.762223]\n",
      "epoch:0 step:584 [D loss: 0.026923, acc.: 100.00%] [G loss: 3.567543]\n",
      "epoch:0 step:585 [D loss: 0.040352, acc.: 100.00%] [G loss: 3.528552]\n",
      "epoch:0 step:586 [D loss: 0.132869, acc.: 97.66%] [G loss: 4.649949]\n",
      "epoch:0 step:587 [D loss: 0.045389, acc.: 100.00%] [G loss: 4.362805]\n",
      "epoch:0 step:588 [D loss: 0.111964, acc.: 97.66%] [G loss: 5.057018]\n",
      "epoch:0 step:589 [D loss: 0.139234, acc.: 96.09%] [G loss: 3.481061]\n",
      "epoch:0 step:590 [D loss: 0.675541, acc.: 63.28%] [G loss: 7.112504]\n",
      "epoch:0 step:591 [D loss: 1.426089, acc.: 51.56%] [G loss: 4.692474]\n",
      "epoch:0 step:592 [D loss: 0.213424, acc.: 91.41%] [G loss: 3.362430]\n",
      "epoch:0 step:593 [D loss: 0.081440, acc.: 100.00%] [G loss: 2.878045]\n",
      "epoch:0 step:594 [D loss: 0.060708, acc.: 100.00%] [G loss: 2.992671]\n",
      "epoch:0 step:595 [D loss: 0.084102, acc.: 99.22%] [G loss: 3.452059]\n",
      "epoch:0 step:596 [D loss: 0.094685, acc.: 99.22%] [G loss: 2.992794]\n",
      "epoch:0 step:597 [D loss: 0.076993, acc.: 99.22%] [G loss: 2.500795]\n",
      "epoch:0 step:598 [D loss: 0.189737, acc.: 93.75%] [G loss: 3.690081]\n",
      "epoch:0 step:599 [D loss: 0.636472, acc.: 64.84%] [G loss: 5.917665]\n",
      "epoch:0 step:600 [D loss: 0.640364, acc.: 69.53%] [G loss: 4.190493]\n",
      "##############\n",
      "[0.99574045 0.81780366 1.01053322 0.95625295 0.86066044 2.09421453\n",
      " 2.11547273 2.11598383 2.1106371  2.11581852]\n",
      "##########\n",
      "epoch:0 step:601 [D loss: 0.108094, acc.: 96.88%] [G loss: 3.051902]\n",
      "epoch:0 step:602 [D loss: 0.130781, acc.: 97.66%] [G loss: 3.358900]\n",
      "epoch:0 step:603 [D loss: 0.123809, acc.: 98.44%] [G loss: 3.479772]\n",
      "epoch:0 step:604 [D loss: 0.125607, acc.: 96.88%] [G loss: 3.255597]\n",
      "epoch:0 step:605 [D loss: 0.284574, acc.: 90.62%] [G loss: 3.624923]\n",
      "epoch:0 step:606 [D loss: 0.103144, acc.: 96.88%] [G loss: 3.612081]\n",
      "epoch:0 step:607 [D loss: 0.126813, acc.: 100.00%] [G loss: 3.058969]\n",
      "epoch:0 step:608 [D loss: 0.070771, acc.: 100.00%] [G loss: 2.806166]\n",
      "epoch:0 step:609 [D loss: 0.246953, acc.: 91.41%] [G loss: 4.534707]\n",
      "epoch:0 step:610 [D loss: 0.176687, acc.: 92.19%] [G loss: 3.914807]\n",
      "epoch:0 step:611 [D loss: 0.234304, acc.: 90.62%] [G loss: 5.089255]\n",
      "epoch:0 step:612 [D loss: 0.028230, acc.: 100.00%] [G loss: 5.245459]\n",
      "epoch:0 step:613 [D loss: 0.151397, acc.: 95.31%] [G loss: 3.166607]\n",
      "epoch:0 step:614 [D loss: 0.342853, acc.: 84.38%] [G loss: 7.011611]\n",
      "epoch:0 step:615 [D loss: 0.397632, acc.: 72.66%] [G loss: 5.804397]\n",
      "epoch:0 step:616 [D loss: 0.084083, acc.: 97.66%] [G loss: 4.102177]\n",
      "epoch:0 step:617 [D loss: 0.100655, acc.: 97.66%] [G loss: 3.901372]\n",
      "epoch:0 step:618 [D loss: 0.029126, acc.: 100.00%] [G loss: 4.006894]\n",
      "epoch:0 step:619 [D loss: 0.043596, acc.: 100.00%] [G loss: 3.130809]\n",
      "epoch:0 step:620 [D loss: 0.260323, acc.: 90.62%] [G loss: 6.044931]\n",
      "epoch:0 step:621 [D loss: 0.279904, acc.: 86.72%] [G loss: 4.344414]\n",
      "epoch:0 step:622 [D loss: 0.129026, acc.: 97.66%] [G loss: 4.133112]\n",
      "epoch:0 step:623 [D loss: 0.114649, acc.: 99.22%] [G loss: 4.062562]\n",
      "epoch:0 step:624 [D loss: 0.030290, acc.: 99.22%] [G loss: 3.644766]\n",
      "epoch:0 step:625 [D loss: 0.140774, acc.: 97.66%] [G loss: 4.138314]\n",
      "epoch:0 step:626 [D loss: 0.290872, acc.: 88.28%] [G loss: 4.090246]\n",
      "epoch:0 step:627 [D loss: 0.044515, acc.: 100.00%] [G loss: 4.459371]\n",
      "epoch:0 step:628 [D loss: 0.079589, acc.: 99.22%] [G loss: 3.757233]\n",
      "epoch:0 step:629 [D loss: 0.156519, acc.: 95.31%] [G loss: 2.423404]\n",
      "epoch:0 step:630 [D loss: 0.083598, acc.: 100.00%] [G loss: 3.768145]\n",
      "epoch:0 step:631 [D loss: 0.162889, acc.: 96.09%] [G loss: 4.709750]\n",
      "epoch:0 step:632 [D loss: 0.124397, acc.: 96.88%] [G loss: 3.866766]\n",
      "epoch:0 step:633 [D loss: 0.730085, acc.: 70.31%] [G loss: 11.208453]\n",
      "epoch:0 step:634 [D loss: 1.813406, acc.: 54.69%] [G loss: 6.094319]\n",
      "epoch:0 step:635 [D loss: 0.027075, acc.: 99.22%] [G loss: 3.657007]\n",
      "epoch:0 step:636 [D loss: 0.138276, acc.: 93.75%] [G loss: 4.107809]\n",
      "epoch:0 step:637 [D loss: 0.026571, acc.: 100.00%] [G loss: 4.390325]\n",
      "epoch:0 step:638 [D loss: 0.029155, acc.: 100.00%] [G loss: 3.645903]\n",
      "epoch:0 step:639 [D loss: 0.144408, acc.: 93.75%] [G loss: 4.504633]\n",
      "epoch:0 step:640 [D loss: 0.049698, acc.: 100.00%] [G loss: 4.534299]\n",
      "epoch:0 step:641 [D loss: 0.035574, acc.: 99.22%] [G loss: 4.029698]\n",
      "epoch:0 step:642 [D loss: 0.222527, acc.: 92.97%] [G loss: 5.759541]\n",
      "epoch:0 step:643 [D loss: 0.199591, acc.: 92.19%] [G loss: 4.402067]\n",
      "epoch:0 step:644 [D loss: 0.048339, acc.: 100.00%] [G loss: 3.432033]\n",
      "epoch:0 step:645 [D loss: 0.188218, acc.: 92.97%] [G loss: 5.949211]\n",
      "epoch:0 step:646 [D loss: 0.230633, acc.: 88.28%] [G loss: 4.696686]\n",
      "epoch:0 step:647 [D loss: 0.111394, acc.: 97.66%] [G loss: 4.472463]\n",
      "epoch:0 step:648 [D loss: 0.108038, acc.: 98.44%] [G loss: 5.631499]\n",
      "epoch:0 step:649 [D loss: 0.089321, acc.: 96.09%] [G loss: 5.234545]\n",
      "epoch:0 step:650 [D loss: 0.226675, acc.: 89.84%] [G loss: 5.962899]\n",
      "epoch:0 step:651 [D loss: 0.261882, acc.: 86.72%] [G loss: 4.902023]\n",
      "epoch:0 step:652 [D loss: 0.171492, acc.: 92.19%] [G loss: 5.621117]\n",
      "epoch:0 step:653 [D loss: 0.066908, acc.: 96.88%] [G loss: 5.298418]\n",
      "epoch:0 step:654 [D loss: 0.150425, acc.: 96.88%] [G loss: 5.379010]\n",
      "epoch:0 step:655 [D loss: 0.261011, acc.: 89.06%] [G loss: 4.785960]\n",
      "epoch:0 step:656 [D loss: 0.045847, acc.: 99.22%] [G loss: 4.483206]\n",
      "epoch:0 step:657 [D loss: 0.024102, acc.: 100.00%] [G loss: 3.931814]\n",
      "epoch:0 step:658 [D loss: 0.098097, acc.: 99.22%] [G loss: 5.640963]\n",
      "epoch:0 step:659 [D loss: 0.122887, acc.: 96.09%] [G loss: 3.472113]\n",
      "epoch:0 step:660 [D loss: 0.797340, acc.: 63.28%] [G loss: 10.243750]\n",
      "epoch:0 step:661 [D loss: 2.783944, acc.: 50.00%] [G loss: 3.952363]\n",
      "epoch:0 step:662 [D loss: 0.443369, acc.: 84.38%] [G loss: 2.318060]\n",
      "epoch:0 step:663 [D loss: 0.162222, acc.: 96.88%] [G loss: 2.611112]\n",
      "epoch:0 step:664 [D loss: 0.104017, acc.: 99.22%] [G loss: 3.157131]\n",
      "epoch:0 step:665 [D loss: 0.167859, acc.: 95.31%] [G loss: 3.261445]\n",
      "epoch:0 step:666 [D loss: 0.058119, acc.: 100.00%] [G loss: 3.602345]\n",
      "epoch:0 step:667 [D loss: 0.097037, acc.: 97.66%] [G loss: 3.537424]\n",
      "epoch:0 step:668 [D loss: 0.107421, acc.: 99.22%] [G loss: 3.079661]\n",
      "epoch:0 step:669 [D loss: 0.206607, acc.: 95.31%] [G loss: 4.635535]\n",
      "epoch:0 step:670 [D loss: 0.194962, acc.: 92.19%] [G loss: 3.634763]\n",
      "epoch:0 step:671 [D loss: 0.179101, acc.: 95.31%] [G loss: 5.125539]\n",
      "epoch:0 step:672 [D loss: 0.097487, acc.: 96.09%] [G loss: 5.049278]\n",
      "epoch:0 step:673 [D loss: 0.200247, acc.: 93.75%] [G loss: 4.223196]\n",
      "epoch:0 step:674 [D loss: 0.070488, acc.: 100.00%] [G loss: 4.767261]\n",
      "epoch:0 step:675 [D loss: 0.050422, acc.: 100.00%] [G loss: 4.710218]\n",
      "epoch:0 step:676 [D loss: 0.115842, acc.: 98.44%] [G loss: 5.558740]\n",
      "epoch:0 step:677 [D loss: 0.102612, acc.: 96.88%] [G loss: 4.907965]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0 step:678 [D loss: 0.168592, acc.: 96.09%] [G loss: 6.409484]\n",
      "epoch:0 step:679 [D loss: 0.044840, acc.: 99.22%] [G loss: 6.796148]\n",
      "epoch:0 step:680 [D loss: 0.134482, acc.: 98.44%] [G loss: 4.335016]\n",
      "epoch:0 step:681 [D loss: 0.396975, acc.: 75.00%] [G loss: 7.351453]\n",
      "epoch:0 step:682 [D loss: 0.225361, acc.: 87.50%] [G loss: 6.352437]\n",
      "epoch:0 step:683 [D loss: 0.176586, acc.: 92.97%] [G loss: 4.743875]\n",
      "epoch:0 step:684 [D loss: 0.034565, acc.: 99.22%] [G loss: 4.567664]\n",
      "epoch:0 step:685 [D loss: 0.042501, acc.: 99.22%] [G loss: 3.971555]\n",
      "epoch:0 step:686 [D loss: 0.063098, acc.: 99.22%] [G loss: 4.446697]\n",
      "epoch:0 step:687 [D loss: 0.039421, acc.: 99.22%] [G loss: 3.958473]\n",
      "epoch:0 step:688 [D loss: 0.288471, acc.: 84.38%] [G loss: 7.552719]\n",
      "epoch:0 step:689 [D loss: 0.419195, acc.: 78.91%] [G loss: 5.384064]\n",
      "epoch:0 step:690 [D loss: 0.125791, acc.: 93.75%] [G loss: 4.643803]\n",
      "epoch:0 step:691 [D loss: 0.027933, acc.: 100.00%] [G loss: 4.221931]\n",
      "epoch:0 step:692 [D loss: 0.334649, acc.: 85.16%] [G loss: 6.619615]\n",
      "epoch:0 step:693 [D loss: 0.386538, acc.: 85.16%] [G loss: 4.522374]\n",
      "epoch:0 step:694 [D loss: 0.255896, acc.: 90.62%] [G loss: 6.127207]\n",
      "epoch:0 step:695 [D loss: 0.157185, acc.: 92.97%] [G loss: 4.713647]\n",
      "epoch:0 step:696 [D loss: 0.556620, acc.: 68.75%] [G loss: 8.177337]\n",
      "epoch:0 step:697 [D loss: 1.233504, acc.: 53.12%] [G loss: 4.985031]\n",
      "epoch:0 step:698 [D loss: 0.085835, acc.: 96.09%] [G loss: 4.775417]\n",
      "epoch:0 step:699 [D loss: 0.019610, acc.: 100.00%] [G loss: 4.173096]\n",
      "epoch:0 step:700 [D loss: 0.052380, acc.: 100.00%] [G loss: 4.169347]\n",
      "epoch:0 step:701 [D loss: 0.059973, acc.: 99.22%] [G loss: 3.820229]\n",
      "epoch:0 step:702 [D loss: 0.059537, acc.: 99.22%] [G loss: 3.592293]\n",
      "epoch:0 step:703 [D loss: 0.216026, acc.: 91.41%] [G loss: 4.928273]\n",
      "epoch:0 step:704 [D loss: 0.388039, acc.: 84.38%] [G loss: 3.818902]\n",
      "epoch:0 step:705 [D loss: 0.165733, acc.: 94.53%] [G loss: 4.305893]\n",
      "epoch:0 step:706 [D loss: 0.163412, acc.: 95.31%] [G loss: 3.844948]\n",
      "epoch:0 step:707 [D loss: 0.560717, acc.: 70.31%] [G loss: 6.054715]\n",
      "epoch:0 step:708 [D loss: 0.490474, acc.: 72.66%] [G loss: 5.065644]\n",
      "epoch:0 step:709 [D loss: 0.186287, acc.: 91.41%] [G loss: 2.934901]\n",
      "epoch:0 step:710 [D loss: 0.476841, acc.: 74.22%] [G loss: 5.422344]\n",
      "epoch:0 step:711 [D loss: 0.235467, acc.: 88.28%] [G loss: 5.894642]\n",
      "epoch:0 step:712 [D loss: 0.112624, acc.: 96.88%] [G loss: 4.954439]\n",
      "epoch:0 step:713 [D loss: 0.080469, acc.: 97.66%] [G loss: 3.795793]\n",
      "epoch:0 step:714 [D loss: 0.079963, acc.: 99.22%] [G loss: 3.817777]\n",
      "epoch:0 step:715 [D loss: 0.167298, acc.: 96.88%] [G loss: 4.438318]\n",
      "epoch:0 step:716 [D loss: 0.067523, acc.: 100.00%] [G loss: 4.564624]\n",
      "epoch:0 step:717 [D loss: 0.161264, acc.: 98.44%] [G loss: 4.635056]\n",
      "epoch:0 step:718 [D loss: 0.385199, acc.: 82.03%] [G loss: 3.859321]\n",
      "epoch:0 step:719 [D loss: 0.083603, acc.: 99.22%] [G loss: 4.273011]\n",
      "epoch:0 step:720 [D loss: 0.226931, acc.: 89.84%] [G loss: 3.298772]\n",
      "epoch:0 step:721 [D loss: 0.392192, acc.: 79.69%] [G loss: 7.480192]\n",
      "epoch:0 step:722 [D loss: 0.856496, acc.: 59.38%] [G loss: 4.827281]\n",
      "epoch:0 step:723 [D loss: 0.111761, acc.: 97.66%] [G loss: 3.549474]\n",
      "epoch:0 step:724 [D loss: 0.107133, acc.: 96.88%] [G loss: 4.449323]\n",
      "epoch:0 step:725 [D loss: 0.074349, acc.: 100.00%] [G loss: 4.885937]\n",
      "epoch:0 step:726 [D loss: 0.201807, acc.: 93.75%] [G loss: 5.759815]\n",
      "epoch:0 step:727 [D loss: 0.550515, acc.: 72.66%] [G loss: 4.074050]\n",
      "epoch:0 step:728 [D loss: 0.392900, acc.: 76.56%] [G loss: 7.078344]\n",
      "epoch:0 step:729 [D loss: 0.208764, acc.: 90.62%] [G loss: 7.137516]\n",
      "epoch:0 step:730 [D loss: 0.266235, acc.: 88.28%] [G loss: 4.256477]\n",
      "epoch:0 step:731 [D loss: 0.307664, acc.: 82.81%] [G loss: 5.257182]\n",
      "epoch:0 step:732 [D loss: 0.031415, acc.: 100.00%] [G loss: 5.675936]\n",
      "epoch:0 step:733 [D loss: 0.103158, acc.: 96.09%] [G loss: 4.188966]\n",
      "epoch:0 step:734 [D loss: 0.216348, acc.: 91.41%] [G loss: 3.514353]\n",
      "epoch:0 step:735 [D loss: 0.157641, acc.: 96.09%] [G loss: 4.287568]\n",
      "epoch:0 step:736 [D loss: 0.166486, acc.: 92.97%] [G loss: 3.335591]\n",
      "epoch:0 step:737 [D loss: 0.218935, acc.: 92.19%] [G loss: 5.096806]\n",
      "epoch:0 step:738 [D loss: 0.159213, acc.: 95.31%] [G loss: 4.400920]\n",
      "epoch:0 step:739 [D loss: 0.093744, acc.: 97.66%] [G loss: 3.922208]\n",
      "epoch:0 step:740 [D loss: 0.302944, acc.: 86.72%] [G loss: 5.874120]\n",
      "epoch:0 step:741 [D loss: 0.259691, acc.: 89.06%] [G loss: 5.536310]\n",
      "epoch:0 step:742 [D loss: 0.077557, acc.: 97.66%] [G loss: 3.937030]\n",
      "epoch:0 step:743 [D loss: 0.118138, acc.: 96.09%] [G loss: 4.758112]\n",
      "epoch:0 step:744 [D loss: 0.032287, acc.: 100.00%] [G loss: 4.312972]\n",
      "epoch:0 step:745 [D loss: 0.212952, acc.: 92.19%] [G loss: 6.395958]\n",
      "epoch:0 step:746 [D loss: 0.390840, acc.: 83.59%] [G loss: 3.797204]\n",
      "epoch:0 step:747 [D loss: 0.506318, acc.: 79.69%] [G loss: 6.984728]\n",
      "epoch:0 step:748 [D loss: 0.436696, acc.: 80.47%] [G loss: 6.001667]\n",
      "epoch:0 step:749 [D loss: 0.042745, acc.: 100.00%] [G loss: 5.026473]\n",
      "epoch:0 step:750 [D loss: 0.034620, acc.: 100.00%] [G loss: 4.549533]\n",
      "epoch:0 step:751 [D loss: 0.123411, acc.: 95.31%] [G loss: 5.131765]\n",
      "epoch:0 step:752 [D loss: 0.029728, acc.: 100.00%] [G loss: 5.000495]\n",
      "epoch:0 step:753 [D loss: 0.103960, acc.: 98.44%] [G loss: 4.235374]\n",
      "epoch:0 step:754 [D loss: 0.476589, acc.: 73.44%] [G loss: 7.679105]\n",
      "epoch:0 step:755 [D loss: 1.652767, acc.: 50.00%] [G loss: 3.548761]\n",
      "epoch:0 step:756 [D loss: 0.227798, acc.: 92.19%] [G loss: 3.702104]\n",
      "epoch:0 step:757 [D loss: 0.040692, acc.: 99.22%] [G loss: 3.877931]\n",
      "epoch:0 step:758 [D loss: 0.078096, acc.: 100.00%] [G loss: 3.636754]\n",
      "epoch:0 step:759 [D loss: 0.095187, acc.: 97.66%] [G loss: 3.822045]\n",
      "epoch:0 step:760 [D loss: 0.160705, acc.: 96.09%] [G loss: 3.640269]\n",
      "epoch:0 step:761 [D loss: 0.160987, acc.: 97.66%] [G loss: 3.962880]\n",
      "epoch:0 step:762 [D loss: 0.354240, acc.: 87.50%] [G loss: 3.315484]\n",
      "epoch:0 step:763 [D loss: 0.205440, acc.: 96.88%] [G loss: 4.868618]\n",
      "epoch:0 step:764 [D loss: 0.161613, acc.: 93.75%] [G loss: 4.454858]\n",
      "epoch:0 step:765 [D loss: 0.149453, acc.: 98.44%] [G loss: 3.777167]\n",
      "epoch:0 step:766 [D loss: 0.115368, acc.: 100.00%] [G loss: 4.836166]\n",
      "epoch:0 step:767 [D loss: 0.165732, acc.: 95.31%] [G loss: 4.470321]\n",
      "epoch:0 step:768 [D loss: 0.140073, acc.: 96.09%] [G loss: 3.705899]\n",
      "epoch:0 step:769 [D loss: 0.203579, acc.: 94.53%] [G loss: 6.289540]\n",
      "epoch:0 step:770 [D loss: 0.570216, acc.: 70.31%] [G loss: 2.218151]\n",
      "epoch:0 step:771 [D loss: 0.581976, acc.: 70.31%] [G loss: 6.858530]\n",
      "epoch:0 step:772 [D loss: 0.366753, acc.: 83.59%] [G loss: 6.589726]\n",
      "epoch:0 step:773 [D loss: 0.276853, acc.: 85.94%] [G loss: 5.107204]\n",
      "epoch:0 step:774 [D loss: 0.015459, acc.: 100.00%] [G loss: 3.846425]\n",
      "epoch:0 step:775 [D loss: 0.027596, acc.: 100.00%] [G loss: 3.192544]\n",
      "epoch:0 step:776 [D loss: 0.092101, acc.: 98.44%] [G loss: 2.576880]\n",
      "epoch:0 step:777 [D loss: 0.115918, acc.: 98.44%] [G loss: 2.690559]\n",
      "epoch:0 step:778 [D loss: 0.055620, acc.: 100.00%] [G loss: 3.055222]\n",
      "epoch:0 step:779 [D loss: 1.116150, acc.: 50.78%] [G loss: 6.978584]\n",
      "epoch:0 step:780 [D loss: 1.358129, acc.: 53.91%] [G loss: 4.625712]\n",
      "epoch:0 step:781 [D loss: 0.103839, acc.: 96.88%] [G loss: 3.208455]\n",
      "epoch:1 step:782 [D loss: 0.090724, acc.: 99.22%] [G loss: 3.012093]\n",
      "epoch:1 step:783 [D loss: 0.064731, acc.: 100.00%] [G loss: 2.887475]\n",
      "epoch:1 step:784 [D loss: 0.130057, acc.: 98.44%] [G loss: 3.411769]\n",
      "epoch:1 step:785 [D loss: 0.063135, acc.: 99.22%] [G loss: 3.834676]\n",
      "epoch:1 step:786 [D loss: 0.217286, acc.: 92.19%] [G loss: 3.116809]\n",
      "epoch:1 step:787 [D loss: 0.105509, acc.: 100.00%] [G loss: 3.204312]\n",
      "epoch:1 step:788 [D loss: 0.382723, acc.: 84.38%] [G loss: 4.297359]\n",
      "epoch:1 step:789 [D loss: 0.108248, acc.: 95.31%] [G loss: 4.425105]\n",
      "epoch:1 step:790 [D loss: 0.267668, acc.: 91.41%] [G loss: 3.004139]\n",
      "epoch:1 step:791 [D loss: 0.108292, acc.: 99.22%] [G loss: 3.975109]\n",
      "epoch:1 step:792 [D loss: 0.122193, acc.: 98.44%] [G loss: 4.312157]\n",
      "epoch:1 step:793 [D loss: 0.804618, acc.: 60.16%] [G loss: 6.244546]\n",
      "epoch:1 step:794 [D loss: 0.853969, acc.: 59.38%] [G loss: 4.402535]\n",
      "epoch:1 step:795 [D loss: 0.103572, acc.: 99.22%] [G loss: 2.908528]\n",
      "epoch:1 step:796 [D loss: 0.090787, acc.: 100.00%] [G loss: 3.432057]\n",
      "epoch:1 step:797 [D loss: 0.083465, acc.: 100.00%] [G loss: 3.552545]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:1 step:798 [D loss: 0.056354, acc.: 100.00%] [G loss: 3.331903]\n",
      "epoch:1 step:799 [D loss: 0.359093, acc.: 85.16%] [G loss: 4.530721]\n",
      "epoch:1 step:800 [D loss: 0.077706, acc.: 99.22%] [G loss: 4.920156]\n",
      "##############\n",
      "[0.94833503 1.0771438  0.90057123 2.11394714 2.12451639 2.11080704\n",
      " 1.1219012  2.11164027 2.11734226 0.99043769]\n",
      "##########\n",
      "epoch:1 step:801 [D loss: 0.178887, acc.: 92.97%] [G loss: 3.432657]\n",
      "epoch:1 step:802 [D loss: 0.162045, acc.: 92.97%] [G loss: 4.474724]\n",
      "epoch:1 step:803 [D loss: 0.156750, acc.: 94.53%] [G loss: 3.323503]\n",
      "epoch:1 step:804 [D loss: 0.242542, acc.: 88.28%] [G loss: 5.079261]\n",
      "epoch:1 step:805 [D loss: 0.057972, acc.: 97.66%] [G loss: 5.534047]\n",
      "epoch:1 step:806 [D loss: 0.318713, acc.: 85.16%] [G loss: 1.830837]\n",
      "epoch:1 step:807 [D loss: 0.712132, acc.: 66.41%] [G loss: 7.077585]\n",
      "epoch:1 step:808 [D loss: 0.587344, acc.: 69.53%] [G loss: 6.444857]\n",
      "epoch:1 step:809 [D loss: 0.072981, acc.: 98.44%] [G loss: 5.243210]\n",
      "epoch:1 step:810 [D loss: 0.065239, acc.: 98.44%] [G loss: 3.938805]\n",
      "epoch:1 step:811 [D loss: 0.147074, acc.: 93.75%] [G loss: 4.511172]\n",
      "epoch:1 step:812 [D loss: 0.041124, acc.: 100.00%] [G loss: 4.021578]\n",
      "epoch:1 step:813 [D loss: 0.430651, acc.: 78.12%] [G loss: 6.075039]\n",
      "epoch:1 step:814 [D loss: 0.724974, acc.: 67.19%] [G loss: 2.862493]\n",
      "epoch:1 step:815 [D loss: 0.226678, acc.: 92.97%] [G loss: 4.814764]\n",
      "epoch:1 step:816 [D loss: 0.067149, acc.: 99.22%] [G loss: 4.924587]\n",
      "epoch:1 step:817 [D loss: 0.551493, acc.: 71.88%] [G loss: 5.720818]\n",
      "epoch:1 step:818 [D loss: 0.064258, acc.: 98.44%] [G loss: 5.898395]\n",
      "epoch:1 step:819 [D loss: 0.317863, acc.: 87.50%] [G loss: 2.678087]\n",
      "epoch:1 step:820 [D loss: 0.357392, acc.: 85.94%] [G loss: 5.940835]\n",
      "epoch:1 step:821 [D loss: 0.428260, acc.: 80.47%] [G loss: 3.109579]\n",
      "epoch:1 step:822 [D loss: 0.447066, acc.: 78.12%] [G loss: 6.529306]\n",
      "epoch:1 step:823 [D loss: 0.241075, acc.: 88.28%] [G loss: 6.031942]\n",
      "epoch:1 step:824 [D loss: 0.104836, acc.: 96.09%] [G loss: 4.432847]\n",
      "epoch:1 step:825 [D loss: 0.135536, acc.: 96.88%] [G loss: 4.293336]\n",
      "epoch:1 step:826 [D loss: 0.140808, acc.: 95.31%] [G loss: 4.358639]\n",
      "epoch:1 step:827 [D loss: 0.084539, acc.: 99.22%] [G loss: 3.656455]\n",
      "epoch:1 step:828 [D loss: 0.505162, acc.: 72.66%] [G loss: 7.542474]\n",
      "epoch:1 step:829 [D loss: 0.969507, acc.: 57.81%] [G loss: 2.843896]\n",
      "epoch:1 step:830 [D loss: 0.219058, acc.: 94.53%] [G loss: 2.803799]\n",
      "epoch:1 step:831 [D loss: 0.091103, acc.: 100.00%] [G loss: 2.926571]\n",
      "epoch:1 step:832 [D loss: 0.180799, acc.: 92.97%] [G loss: 3.763012]\n",
      "epoch:1 step:833 [D loss: 0.217992, acc.: 92.97%] [G loss: 3.147163]\n",
      "epoch:1 step:834 [D loss: 0.162753, acc.: 95.31%] [G loss: 2.423578]\n",
      "epoch:1 step:835 [D loss: 0.600390, acc.: 68.75%] [G loss: 6.408710]\n",
      "epoch:1 step:836 [D loss: 0.831691, acc.: 63.28%] [G loss: 4.433409]\n",
      "epoch:1 step:837 [D loss: 0.106772, acc.: 95.31%] [G loss: 3.160355]\n",
      "epoch:1 step:838 [D loss: 0.063645, acc.: 100.00%] [G loss: 2.844941]\n",
      "epoch:1 step:839 [D loss: 0.080371, acc.: 99.22%] [G loss: 2.472586]\n",
      "epoch:1 step:840 [D loss: 0.077625, acc.: 99.22%] [G loss: 2.961839]\n",
      "epoch:1 step:841 [D loss: 0.079505, acc.: 99.22%] [G loss: 2.864856]\n",
      "epoch:1 step:842 [D loss: 0.441028, acc.: 78.12%] [G loss: 4.980632]\n",
      "epoch:1 step:843 [D loss: 0.229210, acc.: 92.19%] [G loss: 4.144308]\n",
      "epoch:1 step:844 [D loss: 0.223732, acc.: 93.75%] [G loss: 1.746548]\n",
      "epoch:1 step:845 [D loss: 0.281189, acc.: 89.84%] [G loss: 4.280503]\n",
      "epoch:1 step:846 [D loss: 0.249486, acc.: 90.62%] [G loss: 3.050921]\n",
      "epoch:1 step:847 [D loss: 0.280326, acc.: 88.28%] [G loss: 4.720102]\n",
      "epoch:1 step:848 [D loss: 0.683892, acc.: 68.75%] [G loss: 1.892908]\n",
      "epoch:1 step:849 [D loss: 0.322657, acc.: 85.94%] [G loss: 3.627610]\n",
      "epoch:1 step:850 [D loss: 0.130636, acc.: 96.09%] [G loss: 3.402382]\n",
      "epoch:1 step:851 [D loss: 0.348186, acc.: 83.59%] [G loss: 3.720885]\n",
      "epoch:1 step:852 [D loss: 0.348259, acc.: 84.38%] [G loss: 3.060558]\n",
      "epoch:1 step:853 [D loss: 0.489687, acc.: 78.91%] [G loss: 5.480332]\n",
      "epoch:1 step:854 [D loss: 0.551670, acc.: 70.31%] [G loss: 3.309065]\n",
      "epoch:1 step:855 [D loss: 0.315236, acc.: 85.16%] [G loss: 3.747256]\n",
      "epoch:1 step:856 [D loss: 0.070826, acc.: 99.22%] [G loss: 4.052978]\n",
      "epoch:1 step:857 [D loss: 0.107343, acc.: 97.66%] [G loss: 3.244555]\n",
      "epoch:1 step:858 [D loss: 0.387909, acc.: 84.38%] [G loss: 5.201469]\n",
      "epoch:1 step:859 [D loss: 0.163690, acc.: 92.97%] [G loss: 5.084132]\n",
      "epoch:1 step:860 [D loss: 0.275449, acc.: 89.06%] [G loss: 2.596166]\n",
      "epoch:1 step:861 [D loss: 0.338411, acc.: 83.59%] [G loss: 5.540903]\n",
      "epoch:1 step:862 [D loss: 0.208036, acc.: 87.50%] [G loss: 5.586203]\n",
      "epoch:1 step:863 [D loss: 0.216851, acc.: 92.97%] [G loss: 3.586688]\n",
      "epoch:1 step:864 [D loss: 0.050478, acc.: 99.22%] [G loss: 3.016295]\n",
      "epoch:1 step:865 [D loss: 0.125850, acc.: 96.88%] [G loss: 4.341198]\n",
      "epoch:1 step:866 [D loss: 0.065614, acc.: 99.22%] [G loss: 3.942597]\n",
      "epoch:1 step:867 [D loss: 0.191329, acc.: 92.97%] [G loss: 2.537395]\n",
      "epoch:1 step:868 [D loss: 0.092501, acc.: 98.44%] [G loss: 2.622609]\n",
      "epoch:1 step:869 [D loss: 0.050389, acc.: 100.00%] [G loss: 2.276480]\n",
      "epoch:1 step:870 [D loss: 0.571478, acc.: 74.22%] [G loss: 6.309024]\n",
      "epoch:1 step:871 [D loss: 0.950802, acc.: 58.59%] [G loss: 2.283752]\n",
      "epoch:1 step:872 [D loss: 0.061353, acc.: 97.66%] [G loss: 1.832007]\n",
      "epoch:1 step:873 [D loss: 0.093220, acc.: 97.66%] [G loss: 1.856068]\n",
      "epoch:1 step:874 [D loss: 0.075674, acc.: 99.22%] [G loss: 2.493709]\n",
      "epoch:1 step:875 [D loss: 0.143111, acc.: 95.31%] [G loss: 4.069999]\n",
      "epoch:1 step:876 [D loss: 0.094729, acc.: 98.44%] [G loss: 2.518325]\n",
      "epoch:1 step:877 [D loss: 0.220914, acc.: 96.09%] [G loss: 4.521625]\n",
      "epoch:1 step:878 [D loss: 0.338424, acc.: 82.03%] [G loss: 2.231056]\n",
      "epoch:1 step:879 [D loss: 0.236566, acc.: 86.72%] [G loss: 5.030540]\n",
      "epoch:1 step:880 [D loss: 0.161111, acc.: 92.19%] [G loss: 4.714250]\n",
      "epoch:1 step:881 [D loss: 0.164146, acc.: 95.31%] [G loss: 2.236537]\n",
      "epoch:1 step:882 [D loss: 0.170663, acc.: 94.53%] [G loss: 4.649334]\n",
      "epoch:1 step:883 [D loss: 0.074720, acc.: 98.44%] [G loss: 4.495640]\n",
      "epoch:1 step:884 [D loss: 0.455044, acc.: 77.34%] [G loss: 7.255881]\n",
      "epoch:1 step:885 [D loss: 0.375881, acc.: 78.91%] [G loss: 5.596670]\n",
      "epoch:1 step:886 [D loss: 0.120692, acc.: 95.31%] [G loss: 3.503936]\n",
      "epoch:1 step:887 [D loss: 0.243058, acc.: 88.28%] [G loss: 6.636006]\n",
      "epoch:1 step:888 [D loss: 0.231540, acc.: 90.62%] [G loss: 5.125427]\n",
      "epoch:1 step:889 [D loss: 0.039228, acc.: 100.00%] [G loss: 4.347414]\n",
      "epoch:1 step:890 [D loss: 0.295940, acc.: 82.81%] [G loss: 6.405714]\n",
      "epoch:1 step:891 [D loss: 0.289670, acc.: 82.81%] [G loss: 5.126719]\n",
      "epoch:1 step:892 [D loss: 0.094627, acc.: 98.44%] [G loss: 3.172037]\n",
      "epoch:1 step:893 [D loss: 0.196500, acc.: 90.62%] [G loss: 4.971165]\n",
      "epoch:1 step:894 [D loss: 0.129193, acc.: 96.88%] [G loss: 4.589920]\n",
      "epoch:1 step:895 [D loss: 0.083215, acc.: 96.88%] [G loss: 3.360041]\n",
      "epoch:1 step:896 [D loss: 0.249565, acc.: 89.06%] [G loss: 6.316876]\n",
      "epoch:1 step:897 [D loss: 0.488784, acc.: 77.34%] [G loss: 3.818615]\n",
      "epoch:1 step:898 [D loss: 0.240376, acc.: 89.84%] [G loss: 4.933223]\n",
      "epoch:1 step:899 [D loss: 0.015717, acc.: 100.00%] [G loss: 5.256565]\n",
      "epoch:1 step:900 [D loss: 0.076996, acc.: 97.66%] [G loss: 4.028332]\n",
      "epoch:1 step:901 [D loss: 0.110480, acc.: 97.66%] [G loss: 4.315045]\n",
      "epoch:1 step:902 [D loss: 0.075425, acc.: 100.00%] [G loss: 4.454774]\n",
      "epoch:1 step:903 [D loss: 0.491864, acc.: 75.78%] [G loss: 7.185684]\n",
      "epoch:1 step:904 [D loss: 0.668288, acc.: 69.53%] [G loss: 4.943940]\n",
      "epoch:1 step:905 [D loss: 0.072198, acc.: 98.44%] [G loss: 3.705152]\n",
      "epoch:1 step:906 [D loss: 0.035065, acc.: 100.00%] [G loss: 3.822176]\n",
      "epoch:1 step:907 [D loss: 0.063640, acc.: 99.22%] [G loss: 4.405056]\n",
      "epoch:1 step:908 [D loss: 0.032522, acc.: 100.00%] [G loss: 3.592643]\n",
      "epoch:1 step:909 [D loss: 0.151041, acc.: 96.09%] [G loss: 5.569979]\n",
      "epoch:1 step:910 [D loss: 0.072103, acc.: 98.44%] [G loss: 5.189533]\n",
      "epoch:1 step:911 [D loss: 0.469051, acc.: 80.47%] [G loss: 5.939506]\n",
      "epoch:1 step:912 [D loss: 0.126179, acc.: 95.31%] [G loss: 5.022156]\n",
      "epoch:1 step:913 [D loss: 0.148508, acc.: 95.31%] [G loss: 4.265336]\n",
      "epoch:1 step:914 [D loss: 0.059945, acc.: 100.00%] [G loss: 3.418611]\n",
      "epoch:1 step:915 [D loss: 0.509643, acc.: 80.47%] [G loss: 6.951907]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:1 step:916 [D loss: 0.839641, acc.: 59.38%] [G loss: 2.338353]\n",
      "epoch:1 step:917 [D loss: 0.322892, acc.: 83.59%] [G loss: 4.206781]\n",
      "epoch:1 step:918 [D loss: 0.098820, acc.: 98.44%] [G loss: 4.662638]\n",
      "epoch:1 step:919 [D loss: 0.110635, acc.: 97.66%] [G loss: 2.867876]\n",
      "epoch:1 step:920 [D loss: 0.218312, acc.: 90.62%] [G loss: 4.045401]\n",
      "epoch:1 step:921 [D loss: 0.188644, acc.: 92.97%] [G loss: 2.946625]\n",
      "epoch:1 step:922 [D loss: 0.539896, acc.: 71.09%] [G loss: 5.998225]\n",
      "epoch:1 step:923 [D loss: 0.530937, acc.: 78.12%] [G loss: 4.074949]\n",
      "epoch:1 step:924 [D loss: 0.053978, acc.: 100.00%] [G loss: 3.040787]\n",
      "epoch:1 step:925 [D loss: 0.157724, acc.: 95.31%] [G loss: 4.528968]\n",
      "epoch:1 step:926 [D loss: 0.107789, acc.: 100.00%] [G loss: 3.992976]\n",
      "epoch:1 step:927 [D loss: 0.140383, acc.: 97.66%] [G loss: 2.871835]\n",
      "epoch:1 step:928 [D loss: 0.138723, acc.: 97.66%] [G loss: 4.052493]\n",
      "epoch:1 step:929 [D loss: 0.539706, acc.: 71.88%] [G loss: 6.941387]\n",
      "epoch:1 step:930 [D loss: 0.652999, acc.: 73.44%] [G loss: 4.871026]\n",
      "epoch:1 step:931 [D loss: 0.063765, acc.: 100.00%] [G loss: 3.517820]\n",
      "epoch:1 step:932 [D loss: 0.027582, acc.: 100.00%] [G loss: 2.722289]\n",
      "epoch:1 step:933 [D loss: 0.092896, acc.: 98.44%] [G loss: 3.643189]\n",
      "epoch:1 step:934 [D loss: 0.045457, acc.: 99.22%] [G loss: 3.629671]\n",
      "epoch:1 step:935 [D loss: 0.137786, acc.: 97.66%] [G loss: 2.711121]\n",
      "epoch:1 step:936 [D loss: 0.310693, acc.: 84.38%] [G loss: 6.343326]\n",
      "epoch:1 step:937 [D loss: 1.290030, acc.: 46.09%] [G loss: 3.981830]\n",
      "epoch:1 step:938 [D loss: 0.056531, acc.: 99.22%] [G loss: 4.326324]\n",
      "epoch:1 step:939 [D loss: 0.099458, acc.: 96.88%] [G loss: 3.333447]\n",
      "epoch:1 step:940 [D loss: 0.113284, acc.: 96.88%] [G loss: 3.955895]\n",
      "epoch:1 step:941 [D loss: 0.087442, acc.: 98.44%] [G loss: 3.436808]\n",
      "epoch:1 step:942 [D loss: 0.105666, acc.: 96.88%] [G loss: 3.938199]\n",
      "epoch:1 step:943 [D loss: 0.057533, acc.: 100.00%] [G loss: 4.338287]\n",
      "epoch:1 step:944 [D loss: 0.133245, acc.: 98.44%] [G loss: 4.394428]\n",
      "epoch:1 step:945 [D loss: 0.148338, acc.: 96.88%] [G loss: 4.379882]\n",
      "epoch:1 step:946 [D loss: 0.167810, acc.: 96.88%] [G loss: 5.923538]\n",
      "epoch:1 step:947 [D loss: 0.183174, acc.: 92.19%] [G loss: 4.246760]\n",
      "epoch:1 step:948 [D loss: 0.701612, acc.: 71.09%] [G loss: 8.387527]\n",
      "epoch:1 step:949 [D loss: 1.393790, acc.: 57.81%] [G loss: 4.760663]\n",
      "epoch:1 step:950 [D loss: 0.212266, acc.: 88.28%] [G loss: 3.502718]\n",
      "epoch:1 step:951 [D loss: 0.137420, acc.: 97.66%] [G loss: 3.548832]\n",
      "epoch:1 step:952 [D loss: 0.072749, acc.: 100.00%] [G loss: 3.788222]\n",
      "epoch:1 step:953 [D loss: 0.121822, acc.: 98.44%] [G loss: 3.901865]\n",
      "epoch:1 step:954 [D loss: 0.147600, acc.: 96.88%] [G loss: 4.045687]\n",
      "epoch:1 step:955 [D loss: 0.330714, acc.: 88.28%] [G loss: 4.575595]\n",
      "epoch:1 step:956 [D loss: 0.131122, acc.: 96.09%] [G loss: 3.615019]\n",
      "epoch:1 step:957 [D loss: 0.188384, acc.: 92.19%] [G loss: 3.840219]\n",
      "epoch:1 step:958 [D loss: 0.053593, acc.: 99.22%] [G loss: 3.887996]\n",
      "epoch:1 step:959 [D loss: 0.139483, acc.: 96.88%] [G loss: 4.062747]\n",
      "epoch:1 step:960 [D loss: 0.129689, acc.: 97.66%] [G loss: 4.028187]\n",
      "epoch:1 step:961 [D loss: 0.324267, acc.: 89.84%] [G loss: 3.185119]\n",
      "epoch:1 step:962 [D loss: 0.129547, acc.: 96.88%] [G loss: 3.390930]\n",
      "epoch:1 step:963 [D loss: 0.102913, acc.: 97.66%] [G loss: 3.995619]\n",
      "epoch:1 step:964 [D loss: 0.125324, acc.: 98.44%] [G loss: 4.248642]\n",
      "epoch:1 step:965 [D loss: 0.467809, acc.: 75.00%] [G loss: 7.832445]\n",
      "epoch:1 step:966 [D loss: 0.315860, acc.: 85.94%] [G loss: 6.560790]\n",
      "epoch:1 step:967 [D loss: 0.040511, acc.: 100.00%] [G loss: 5.296959]\n",
      "epoch:1 step:968 [D loss: 0.025033, acc.: 100.00%] [G loss: 4.019313]\n",
      "epoch:1 step:969 [D loss: 0.264252, acc.: 85.94%] [G loss: 7.235120]\n",
      "epoch:1 step:970 [D loss: 0.322242, acc.: 88.28%] [G loss: 5.696358]\n",
      "epoch:1 step:971 [D loss: 0.043267, acc.: 99.22%] [G loss: 4.538804]\n",
      "epoch:1 step:972 [D loss: 0.141486, acc.: 95.31%] [G loss: 5.301779]\n",
      "epoch:1 step:973 [D loss: 0.070184, acc.: 98.44%] [G loss: 4.689421]\n",
      "epoch:1 step:974 [D loss: 0.415722, acc.: 78.12%] [G loss: 6.992095]\n",
      "epoch:1 step:975 [D loss: 0.956467, acc.: 60.16%] [G loss: 3.063643]\n",
      "epoch:1 step:976 [D loss: 0.133734, acc.: 98.44%] [G loss: 3.827240]\n",
      "epoch:1 step:977 [D loss: 0.041803, acc.: 100.00%] [G loss: 4.346600]\n",
      "epoch:1 step:978 [D loss: 0.101659, acc.: 99.22%] [G loss: 4.452269]\n",
      "epoch:1 step:979 [D loss: 0.150952, acc.: 96.09%] [G loss: 4.514916]\n",
      "epoch:1 step:980 [D loss: 0.083442, acc.: 97.66%] [G loss: 3.360395]\n",
      "epoch:1 step:981 [D loss: 0.097832, acc.: 99.22%] [G loss: 4.066777]\n",
      "epoch:1 step:982 [D loss: 0.062982, acc.: 99.22%] [G loss: 3.603946]\n",
      "epoch:1 step:983 [D loss: 0.246417, acc.: 92.19%] [G loss: 3.289597]\n",
      "epoch:1 step:984 [D loss: 0.054590, acc.: 99.22%] [G loss: 4.202035]\n",
      "epoch:1 step:985 [D loss: 0.063641, acc.: 99.22%] [G loss: 3.383439]\n",
      "epoch:1 step:986 [D loss: 0.356233, acc.: 85.94%] [G loss: 6.819012]\n",
      "epoch:1 step:987 [D loss: 0.681392, acc.: 67.19%] [G loss: 3.270514]\n",
      "epoch:1 step:988 [D loss: 0.234046, acc.: 88.28%] [G loss: 5.451327]\n",
      "epoch:1 step:989 [D loss: 0.025671, acc.: 99.22%] [G loss: 6.313339]\n",
      "epoch:1 step:990 [D loss: 0.257594, acc.: 89.84%] [G loss: 3.271023]\n",
      "epoch:1 step:991 [D loss: 1.233912, acc.: 54.69%] [G loss: 7.715360]\n",
      "epoch:1 step:992 [D loss: 1.472150, acc.: 50.00%] [G loss: 4.927001]\n",
      "epoch:1 step:993 [D loss: 0.275261, acc.: 89.06%] [G loss: 2.862538]\n",
      "epoch:1 step:994 [D loss: 0.117893, acc.: 97.66%] [G loss: 3.098358]\n",
      "epoch:1 step:995 [D loss: 0.121193, acc.: 96.88%] [G loss: 3.367860]\n",
      "epoch:1 step:996 [D loss: 0.061983, acc.: 100.00%] [G loss: 3.191506]\n",
      "epoch:1 step:997 [D loss: 0.206785, acc.: 92.19%] [G loss: 3.124141]\n",
      "epoch:1 step:998 [D loss: 0.204684, acc.: 93.75%] [G loss: 3.089272]\n",
      "epoch:1 step:999 [D loss: 0.325799, acc.: 87.50%] [G loss: 3.013009]\n",
      "epoch:1 step:1000 [D loss: 0.173103, acc.: 96.09%] [G loss: 3.837518]\n",
      "##############\n",
      "[0.95687632 1.02374688 1.10799552 0.88374062 1.05991371 0.96477691\n",
      " 2.12539924 2.1131044  0.88927932 1.07295763]\n",
      "##########\n",
      "epoch:1 step:1001 [D loss: 0.271777, acc.: 89.84%] [G loss: 3.680866]\n",
      "epoch:1 step:1002 [D loss: 0.116546, acc.: 99.22%] [G loss: 3.953053]\n",
      "epoch:1 step:1003 [D loss: 0.220025, acc.: 92.19%] [G loss: 4.304178]\n",
      "epoch:1 step:1004 [D loss: 0.161027, acc.: 95.31%] [G loss: 3.952874]\n",
      "epoch:1 step:1005 [D loss: 0.232547, acc.: 91.41%] [G loss: 4.961951]\n",
      "epoch:1 step:1006 [D loss: 0.103431, acc.: 98.44%] [G loss: 4.638453]\n",
      "epoch:1 step:1007 [D loss: 0.196498, acc.: 93.75%] [G loss: 4.043681]\n",
      "epoch:1 step:1008 [D loss: 0.138649, acc.: 95.31%] [G loss: 6.063746]\n",
      "epoch:1 step:1009 [D loss: 0.222328, acc.: 89.06%] [G loss: 4.082838]\n",
      "epoch:1 step:1010 [D loss: 0.217048, acc.: 89.84%] [G loss: 6.627750]\n",
      "epoch:1 step:1011 [D loss: 0.133662, acc.: 94.53%] [G loss: 6.139284]\n",
      "epoch:1 step:1012 [D loss: 0.066939, acc.: 98.44%] [G loss: 4.551770]\n",
      "epoch:1 step:1013 [D loss: 0.269484, acc.: 90.62%] [G loss: 8.020090]\n",
      "epoch:1 step:1014 [D loss: 0.489125, acc.: 76.56%] [G loss: 4.089767]\n",
      "epoch:1 step:1015 [D loss: 0.507418, acc.: 75.00%] [G loss: 6.728819]\n",
      "epoch:1 step:1016 [D loss: 0.161775, acc.: 92.19%] [G loss: 7.136408]\n",
      "epoch:1 step:1017 [D loss: 0.456685, acc.: 82.81%] [G loss: 3.276348]\n",
      "epoch:1 step:1018 [D loss: 0.491425, acc.: 78.12%] [G loss: 5.956153]\n",
      "epoch:1 step:1019 [D loss: 0.139400, acc.: 92.97%] [G loss: 6.244409]\n",
      "epoch:1 step:1020 [D loss: 0.188307, acc.: 92.97%] [G loss: 3.950913]\n",
      "epoch:1 step:1021 [D loss: 0.103561, acc.: 97.66%] [G loss: 3.140625]\n",
      "epoch:1 step:1022 [D loss: 0.127659, acc.: 96.88%] [G loss: 4.125685]\n",
      "epoch:1 step:1023 [D loss: 0.261058, acc.: 92.19%] [G loss: 3.673885]\n",
      "epoch:1 step:1024 [D loss: 0.230865, acc.: 92.97%] [G loss: 4.375214]\n",
      "epoch:1 step:1025 [D loss: 0.294963, acc.: 88.28%] [G loss: 3.263157]\n",
      "epoch:1 step:1026 [D loss: 0.248633, acc.: 92.97%] [G loss: 5.854915]\n",
      "epoch:1 step:1027 [D loss: 0.437659, acc.: 78.12%] [G loss: 2.998204]\n",
      "epoch:1 step:1028 [D loss: 0.343580, acc.: 84.38%] [G loss: 6.168227]\n",
      "epoch:1 step:1029 [D loss: 0.175172, acc.: 92.19%] [G loss: 5.543928]\n",
      "epoch:1 step:1030 [D loss: 0.295940, acc.: 88.28%] [G loss: 3.256806]\n",
      "epoch:1 step:1031 [D loss: 0.275544, acc.: 86.72%] [G loss: 5.227030]\n",
      "epoch:1 step:1032 [D loss: 0.103156, acc.: 96.88%] [G loss: 4.930390]\n",
      "epoch:1 step:1033 [D loss: 0.256088, acc.: 92.19%] [G loss: 3.692125]\n",
      "epoch:1 step:1034 [D loss: 0.241180, acc.: 89.06%] [G loss: 5.176112]\n",
      "epoch:1 step:1035 [D loss: 0.649460, acc.: 66.41%] [G loss: 4.200946]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:1 step:1036 [D loss: 0.130430, acc.: 96.88%] [G loss: 3.439244]\n",
      "epoch:1 step:1037 [D loss: 0.479356, acc.: 75.78%] [G loss: 6.715933]\n",
      "epoch:1 step:1038 [D loss: 0.651841, acc.: 72.66%] [G loss: 4.508881]\n",
      "epoch:1 step:1039 [D loss: 0.197729, acc.: 91.41%] [G loss: 4.442160]\n",
      "epoch:1 step:1040 [D loss: 0.049761, acc.: 100.00%] [G loss: 3.998069]\n",
      "epoch:1 step:1041 [D loss: 0.172194, acc.: 94.53%] [G loss: 3.573197]\n",
      "epoch:1 step:1042 [D loss: 0.287570, acc.: 91.41%] [G loss: 5.251657]\n",
      "epoch:1 step:1043 [D loss: 0.108580, acc.: 96.88%] [G loss: 4.893632]\n",
      "epoch:1 step:1044 [D loss: 0.124303, acc.: 96.88%] [G loss: 3.305042]\n",
      "epoch:1 step:1045 [D loss: 0.578462, acc.: 68.75%] [G loss: 6.265461]\n",
      "epoch:1 step:1046 [D loss: 0.538189, acc.: 78.91%] [G loss: 4.242538]\n",
      "epoch:1 step:1047 [D loss: 0.198700, acc.: 93.75%] [G loss: 3.817898]\n",
      "epoch:1 step:1048 [D loss: 0.137282, acc.: 96.88%] [G loss: 3.297782]\n",
      "epoch:1 step:1049 [D loss: 0.133583, acc.: 99.22%] [G loss: 4.191837]\n",
      "epoch:1 step:1050 [D loss: 0.232406, acc.: 91.41%] [G loss: 3.289293]\n",
      "epoch:1 step:1051 [D loss: 0.066141, acc.: 100.00%] [G loss: 3.279410]\n",
      "epoch:1 step:1052 [D loss: 0.691853, acc.: 64.84%] [G loss: 6.006469]\n",
      "epoch:1 step:1053 [D loss: 0.354041, acc.: 81.25%] [G loss: 4.833614]\n",
      "epoch:1 step:1054 [D loss: 0.114215, acc.: 96.09%] [G loss: 3.153270]\n",
      "epoch:1 step:1055 [D loss: 0.101126, acc.: 95.31%] [G loss: 3.449013]\n",
      "epoch:1 step:1056 [D loss: 0.113005, acc.: 98.44%] [G loss: 3.787941]\n",
      "epoch:1 step:1057 [D loss: 0.092049, acc.: 98.44%] [G loss: 3.135571]\n",
      "epoch:1 step:1058 [D loss: 0.459808, acc.: 79.69%] [G loss: 5.731211]\n",
      "epoch:1 step:1059 [D loss: 0.213003, acc.: 92.19%] [G loss: 4.654480]\n",
      "epoch:1 step:1060 [D loss: 0.079345, acc.: 99.22%] [G loss: 3.724141]\n",
      "epoch:1 step:1061 [D loss: 0.280941, acc.: 86.72%] [G loss: 6.691710]\n",
      "epoch:1 step:1062 [D loss: 0.496579, acc.: 78.12%] [G loss: 1.869604]\n",
      "epoch:1 step:1063 [D loss: 0.627330, acc.: 71.09%] [G loss: 6.402802]\n",
      "epoch:1 step:1064 [D loss: 0.392750, acc.: 81.25%] [G loss: 5.319801]\n",
      "epoch:1 step:1065 [D loss: 0.073268, acc.: 97.66%] [G loss: 4.019790]\n",
      "epoch:1 step:1066 [D loss: 0.098694, acc.: 97.66%] [G loss: 3.233885]\n",
      "epoch:1 step:1067 [D loss: 0.096136, acc.: 97.66%] [G loss: 3.633301]\n",
      "epoch:1 step:1068 [D loss: 0.084667, acc.: 99.22%] [G loss: 3.029849]\n",
      "epoch:1 step:1069 [D loss: 0.161403, acc.: 95.31%] [G loss: 2.371370]\n",
      "epoch:1 step:1070 [D loss: 0.235747, acc.: 91.41%] [G loss: 4.929893]\n",
      "epoch:1 step:1071 [D loss: 0.321614, acc.: 86.72%] [G loss: 3.203606]\n",
      "epoch:1 step:1072 [D loss: 0.383321, acc.: 82.81%] [G loss: 5.723825]\n",
      "epoch:1 step:1073 [D loss: 0.344963, acc.: 81.25%] [G loss: 3.253829]\n",
      "epoch:1 step:1074 [D loss: 0.235824, acc.: 89.84%] [G loss: 4.448493]\n",
      "epoch:1 step:1075 [D loss: 0.044183, acc.: 99.22%] [G loss: 4.923729]\n",
      "epoch:1 step:1076 [D loss: 0.235476, acc.: 91.41%] [G loss: 4.660740]\n",
      "epoch:1 step:1077 [D loss: 0.254903, acc.: 88.28%] [G loss: 4.145864]\n",
      "epoch:1 step:1078 [D loss: 0.125742, acc.: 96.88%] [G loss: 4.770409]\n",
      "epoch:1 step:1079 [D loss: 0.271360, acc.: 89.84%] [G loss: 3.843111]\n",
      "epoch:1 step:1080 [D loss: 0.071040, acc.: 99.22%] [G loss: 4.340126]\n",
      "epoch:1 step:1081 [D loss: 0.091682, acc.: 98.44%] [G loss: 3.914441]\n",
      "epoch:1 step:1082 [D loss: 0.380933, acc.: 85.16%] [G loss: 7.090446]\n",
      "epoch:1 step:1083 [D loss: 0.798210, acc.: 64.06%] [G loss: 3.436631]\n",
      "epoch:1 step:1084 [D loss: 0.382229, acc.: 78.91%] [G loss: 5.626225]\n",
      "epoch:1 step:1085 [D loss: 0.032602, acc.: 99.22%] [G loss: 6.455058]\n",
      "epoch:1 step:1086 [D loss: 0.150520, acc.: 93.75%] [G loss: 5.029974]\n",
      "epoch:1 step:1087 [D loss: 0.159754, acc.: 95.31%] [G loss: 3.411918]\n",
      "epoch:1 step:1088 [D loss: 0.285962, acc.: 87.50%] [G loss: 6.008018]\n",
      "epoch:1 step:1089 [D loss: 0.350423, acc.: 83.59%] [G loss: 4.144155]\n",
      "epoch:1 step:1090 [D loss: 0.155230, acc.: 95.31%] [G loss: 5.191443]\n",
      "epoch:1 step:1091 [D loss: 0.034563, acc.: 100.00%] [G loss: 5.157152]\n",
      "epoch:1 step:1092 [D loss: 0.065064, acc.: 98.44%] [G loss: 4.058294]\n",
      "epoch:1 step:1093 [D loss: 0.179157, acc.: 95.31%] [G loss: 4.278645]\n",
      "epoch:1 step:1094 [D loss: 0.091846, acc.: 96.88%] [G loss: 5.282248]\n",
      "epoch:1 step:1095 [D loss: 0.128494, acc.: 96.09%] [G loss: 3.689569]\n",
      "epoch:1 step:1096 [D loss: 1.283115, acc.: 47.66%] [G loss: 8.232172]\n",
      "epoch:1 step:1097 [D loss: 1.313857, acc.: 54.69%] [G loss: 5.662515]\n",
      "epoch:1 step:1098 [D loss: 0.219642, acc.: 88.28%] [G loss: 3.874216]\n",
      "epoch:1 step:1099 [D loss: 0.064140, acc.: 100.00%] [G loss: 3.334020]\n",
      "epoch:1 step:1100 [D loss: 0.065637, acc.: 98.44%] [G loss: 3.527898]\n",
      "epoch:1 step:1101 [D loss: 0.078669, acc.: 98.44%] [G loss: 3.359848]\n",
      "epoch:1 step:1102 [D loss: 0.050358, acc.: 100.00%] [G loss: 3.714530]\n",
      "epoch:1 step:1103 [D loss: 0.084849, acc.: 98.44%] [G loss: 3.283535]\n",
      "epoch:1 step:1104 [D loss: 0.143094, acc.: 98.44%] [G loss: 3.427401]\n",
      "epoch:1 step:1105 [D loss: 0.111381, acc.: 98.44%] [G loss: 3.479040]\n",
      "epoch:1 step:1106 [D loss: 0.287489, acc.: 89.06%] [G loss: 2.764561]\n",
      "epoch:1 step:1107 [D loss: 0.133915, acc.: 96.09%] [G loss: 2.168517]\n",
      "epoch:1 step:1108 [D loss: 0.131753, acc.: 98.44%] [G loss: 2.983377]\n",
      "epoch:1 step:1109 [D loss: 0.125007, acc.: 97.66%] [G loss: 3.504474]\n",
      "epoch:1 step:1110 [D loss: 0.242654, acc.: 92.19%] [G loss: 3.507230]\n",
      "epoch:1 step:1111 [D loss: 0.130551, acc.: 98.44%] [G loss: 4.207234]\n",
      "epoch:1 step:1112 [D loss: 0.072945, acc.: 98.44%] [G loss: 3.085675]\n",
      "epoch:1 step:1113 [D loss: 0.175755, acc.: 96.09%] [G loss: 5.489764]\n",
      "epoch:1 step:1114 [D loss: 0.104791, acc.: 95.31%] [G loss: 4.964787]\n",
      "epoch:1 step:1115 [D loss: 0.351434, acc.: 87.50%] [G loss: 7.100654]\n",
      "epoch:1 step:1116 [D loss: 0.211137, acc.: 91.41%] [G loss: 6.251046]\n",
      "epoch:1 step:1117 [D loss: 0.035335, acc.: 99.22%] [G loss: 4.795514]\n",
      "epoch:1 step:1118 [D loss: 0.049450, acc.: 99.22%] [G loss: 4.241061]\n",
      "epoch:1 step:1119 [D loss: 0.098615, acc.: 96.09%] [G loss: 5.312145]\n",
      "epoch:1 step:1120 [D loss: 0.043918, acc.: 99.22%] [G loss: 4.831826]\n",
      "epoch:1 step:1121 [D loss: 0.179775, acc.: 94.53%] [G loss: 4.193799]\n",
      "epoch:1 step:1122 [D loss: 0.094614, acc.: 98.44%] [G loss: 5.437492]\n",
      "epoch:1 step:1123 [D loss: 0.058137, acc.: 98.44%] [G loss: 4.571642]\n",
      "epoch:1 step:1124 [D loss: 0.245256, acc.: 92.97%] [G loss: 5.523888]\n",
      "epoch:1 step:1125 [D loss: 0.074136, acc.: 99.22%] [G loss: 5.507454]\n",
      "epoch:1 step:1126 [D loss: 0.172998, acc.: 93.75%] [G loss: 4.794721]\n",
      "epoch:1 step:1127 [D loss: 0.040856, acc.: 100.00%] [G loss: 4.203650]\n",
      "epoch:1 step:1128 [D loss: 0.143762, acc.: 96.09%] [G loss: 6.366820]\n",
      "epoch:1 step:1129 [D loss: 0.908538, acc.: 55.47%] [G loss: 8.658095]\n",
      "epoch:1 step:1130 [D loss: 0.709281, acc.: 73.44%] [G loss: 3.420860]\n",
      "epoch:1 step:1131 [D loss: 0.046700, acc.: 98.44%] [G loss: 1.849960]\n",
      "epoch:1 step:1132 [D loss: 0.060912, acc.: 97.66%] [G loss: 2.153857]\n",
      "epoch:1 step:1133 [D loss: 0.090936, acc.: 98.44%] [G loss: 3.889595]\n",
      "epoch:1 step:1134 [D loss: 0.146903, acc.: 95.31%] [G loss: 4.335204]\n",
      "epoch:1 step:1135 [D loss: 0.103389, acc.: 97.66%] [G loss: 2.601125]\n",
      "epoch:1 step:1136 [D loss: 0.149601, acc.: 95.31%] [G loss: 1.330737]\n",
      "epoch:1 step:1137 [D loss: 0.257138, acc.: 85.94%] [G loss: 6.089327]\n",
      "epoch:1 step:1138 [D loss: 0.642521, acc.: 76.56%] [G loss: 1.324546]\n",
      "epoch:1 step:1139 [D loss: 0.220172, acc.: 89.06%] [G loss: 3.253585]\n",
      "epoch:1 step:1140 [D loss: 0.043082, acc.: 98.44%] [G loss: 4.234314]\n",
      "epoch:1 step:1141 [D loss: 0.186021, acc.: 93.75%] [G loss: 4.185778]\n",
      "epoch:1 step:1142 [D loss: 0.074023, acc.: 99.22%] [G loss: 3.096577]\n",
      "epoch:1 step:1143 [D loss: 0.118799, acc.: 96.88%] [G loss: 3.253912]\n",
      "epoch:1 step:1144 [D loss: 0.107903, acc.: 98.44%] [G loss: 2.880931]\n",
      "epoch:1 step:1145 [D loss: 0.170892, acc.: 96.09%] [G loss: 5.185356]\n",
      "epoch:1 step:1146 [D loss: 0.538310, acc.: 71.88%] [G loss: 7.880192]\n",
      "epoch:1 step:1147 [D loss: 0.305518, acc.: 87.50%] [G loss: 6.346462]\n",
      "epoch:1 step:1148 [D loss: 0.062859, acc.: 96.88%] [G loss: 5.751477]\n",
      "epoch:1 step:1149 [D loss: 0.041386, acc.: 99.22%] [G loss: 4.795729]\n",
      "epoch:1 step:1150 [D loss: 0.038548, acc.: 99.22%] [G loss: 4.531308]\n",
      "epoch:1 step:1151 [D loss: 0.141391, acc.: 93.75%] [G loss: 5.597110]\n",
      "epoch:1 step:1152 [D loss: 0.081968, acc.: 97.66%] [G loss: 5.990814]\n",
      "epoch:1 step:1153 [D loss: 0.213391, acc.: 93.75%] [G loss: 5.688945]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:1 step:1154 [D loss: 0.059862, acc.: 99.22%] [G loss: 5.065054]\n",
      "epoch:1 step:1155 [D loss: 0.060054, acc.: 99.22%] [G loss: 3.795911]\n",
      "epoch:1 step:1156 [D loss: 0.113262, acc.: 97.66%] [G loss: 5.432567]\n",
      "epoch:1 step:1157 [D loss: 0.100645, acc.: 96.88%] [G loss: 4.230136]\n",
      "epoch:1 step:1158 [D loss: 0.339729, acc.: 85.94%] [G loss: 6.642646]\n",
      "epoch:1 step:1159 [D loss: 0.143268, acc.: 93.75%] [G loss: 6.172962]\n",
      "epoch:1 step:1160 [D loss: 0.054909, acc.: 98.44%] [G loss: 3.187152]\n",
      "epoch:1 step:1161 [D loss: 0.052846, acc.: 98.44%] [G loss: 2.101034]\n",
      "epoch:1 step:1162 [D loss: 0.047675, acc.: 100.00%] [G loss: 2.709905]\n",
      "epoch:1 step:1163 [D loss: 0.078813, acc.: 97.66%] [G loss: 3.028638]\n",
      "epoch:1 step:1164 [D loss: 0.218955, acc.: 89.84%] [G loss: 5.942294]\n",
      "epoch:1 step:1165 [D loss: 0.130865, acc.: 94.53%] [G loss: 3.623123]\n",
      "epoch:1 step:1166 [D loss: 0.314948, acc.: 87.50%] [G loss: 5.986681]\n",
      "epoch:1 step:1167 [D loss: 0.559424, acc.: 71.88%] [G loss: 5.264939]\n",
      "epoch:1 step:1168 [D loss: 0.239893, acc.: 92.19%] [G loss: 3.365700]\n",
      "epoch:1 step:1169 [D loss: 0.307498, acc.: 85.94%] [G loss: 6.165349]\n",
      "epoch:1 step:1170 [D loss: 0.114342, acc.: 96.88%] [G loss: 6.244574]\n",
      "epoch:1 step:1171 [D loss: 0.194505, acc.: 93.75%] [G loss: 3.941302]\n",
      "epoch:1 step:1172 [D loss: 0.135515, acc.: 94.53%] [G loss: 5.173833]\n",
      "epoch:1 step:1173 [D loss: 0.275355, acc.: 89.06%] [G loss: 3.925066]\n",
      "epoch:1 step:1174 [D loss: 0.116122, acc.: 96.88%] [G loss: 4.859944]\n",
      "epoch:1 step:1175 [D loss: 0.065957, acc.: 99.22%] [G loss: 4.528736]\n",
      "epoch:1 step:1176 [D loss: 0.515050, acc.: 69.53%] [G loss: 8.095184]\n",
      "epoch:1 step:1177 [D loss: 0.948198, acc.: 65.62%] [G loss: 3.088315]\n",
      "epoch:1 step:1178 [D loss: 0.503389, acc.: 75.78%] [G loss: 7.203689]\n",
      "epoch:1 step:1179 [D loss: 0.153525, acc.: 92.97%] [G loss: 6.969849]\n",
      "epoch:1 step:1180 [D loss: 0.087427, acc.: 97.66%] [G loss: 5.127513]\n",
      "epoch:1 step:1181 [D loss: 0.076383, acc.: 99.22%] [G loss: 5.030163]\n",
      "epoch:1 step:1182 [D loss: 0.020147, acc.: 100.00%] [G loss: 3.956788]\n",
      "epoch:1 step:1183 [D loss: 0.195066, acc.: 93.75%] [G loss: 5.485204]\n",
      "epoch:1 step:1184 [D loss: 0.206684, acc.: 91.41%] [G loss: 3.806931]\n",
      "epoch:1 step:1185 [D loss: 0.128152, acc.: 95.31%] [G loss: 4.536697]\n",
      "epoch:1 step:1186 [D loss: 0.054270, acc.: 100.00%] [G loss: 3.143406]\n",
      "epoch:1 step:1187 [D loss: 0.276377, acc.: 87.50%] [G loss: 5.572132]\n",
      "epoch:1 step:1188 [D loss: 0.325018, acc.: 85.94%] [G loss: 2.991098]\n",
      "epoch:1 step:1189 [D loss: 0.139905, acc.: 95.31%] [G loss: 2.293131]\n",
      "epoch:1 step:1190 [D loss: 0.078863, acc.: 100.00%] [G loss: 4.274905]\n",
      "epoch:1 step:1191 [D loss: 0.113051, acc.: 96.88%] [G loss: 1.977725]\n",
      "epoch:1 step:1192 [D loss: 0.147209, acc.: 96.88%] [G loss: 3.186409]\n",
      "epoch:1 step:1193 [D loss: 0.118656, acc.: 95.31%] [G loss: 3.306194]\n",
      "epoch:1 step:1194 [D loss: 0.126199, acc.: 97.66%] [G loss: 3.744689]\n",
      "epoch:1 step:1195 [D loss: 0.207033, acc.: 92.97%] [G loss: 3.200393]\n",
      "epoch:1 step:1196 [D loss: 0.048360, acc.: 99.22%] [G loss: 1.560841]\n",
      "epoch:1 step:1197 [D loss: 0.111170, acc.: 96.09%] [G loss: 3.134603]\n",
      "epoch:1 step:1198 [D loss: 0.461870, acc.: 76.56%] [G loss: 7.757281]\n",
      "epoch:1 step:1199 [D loss: 0.698887, acc.: 67.97%] [G loss: 3.626145]\n",
      "epoch:1 step:1200 [D loss: 0.036973, acc.: 99.22%] [G loss: 2.404982]\n",
      "##############\n",
      "[1.0540513  1.05306208 1.0587765  0.9987977  2.09609415 0.95131698\n",
      " 2.10927759 2.10850117 0.90382054 1.11404826]\n",
      "##########\n",
      "epoch:1 step:1201 [D loss: 0.423336, acc.: 80.47%] [G loss: 7.582143]\n",
      "epoch:1 step:1202 [D loss: 0.506258, acc.: 77.34%] [G loss: 5.291679]\n",
      "epoch:1 step:1203 [D loss: 0.017888, acc.: 100.00%] [G loss: 3.877033]\n",
      "epoch:1 step:1204 [D loss: 0.113458, acc.: 96.09%] [G loss: 5.465250]\n",
      "epoch:1 step:1205 [D loss: 0.030890, acc.: 100.00%] [G loss: 5.665404]\n",
      "epoch:1 step:1206 [D loss: 0.173570, acc.: 92.97%] [G loss: 4.171578]\n",
      "epoch:1 step:1207 [D loss: 0.121953, acc.: 98.44%] [G loss: 5.121949]\n",
      "epoch:1 step:1208 [D loss: 0.234481, acc.: 93.75%] [G loss: 6.071685]\n",
      "epoch:1 step:1209 [D loss: 0.122970, acc.: 96.09%] [G loss: 5.179719]\n",
      "epoch:1 step:1210 [D loss: 0.401215, acc.: 78.91%] [G loss: 7.728688]\n",
      "epoch:1 step:1211 [D loss: 0.279877, acc.: 86.72%] [G loss: 6.421252]\n",
      "epoch:1 step:1212 [D loss: 0.035743, acc.: 100.00%] [G loss: 5.484571]\n",
      "epoch:1 step:1213 [D loss: 0.036781, acc.: 100.00%] [G loss: 4.425220]\n",
      "epoch:1 step:1214 [D loss: 0.059846, acc.: 99.22%] [G loss: 5.101316]\n",
      "epoch:1 step:1215 [D loss: 0.062349, acc.: 98.44%] [G loss: 4.258909]\n",
      "epoch:1 step:1216 [D loss: 0.182506, acc.: 95.31%] [G loss: 5.655951]\n",
      "epoch:1 step:1217 [D loss: 0.160389, acc.: 93.75%] [G loss: 3.844041]\n",
      "epoch:1 step:1218 [D loss: 0.182956, acc.: 94.53%] [G loss: 5.991785]\n",
      "epoch:1 step:1219 [D loss: 0.113760, acc.: 96.09%] [G loss: 4.466871]\n",
      "epoch:1 step:1220 [D loss: 1.278732, acc.: 55.47%] [G loss: 10.163131]\n",
      "epoch:1 step:1221 [D loss: 2.480435, acc.: 50.00%] [G loss: 4.971335]\n",
      "epoch:1 step:1222 [D loss: 0.158125, acc.: 94.53%] [G loss: 2.889579]\n",
      "epoch:1 step:1223 [D loss: 0.092855, acc.: 97.66%] [G loss: 2.761536]\n",
      "epoch:1 step:1224 [D loss: 0.073752, acc.: 99.22%] [G loss: 2.554110]\n",
      "epoch:1 step:1225 [D loss: 0.276479, acc.: 85.94%] [G loss: 3.260125]\n",
      "epoch:1 step:1226 [D loss: 0.155907, acc.: 99.22%] [G loss: 3.716609]\n",
      "epoch:1 step:1227 [D loss: 0.182040, acc.: 94.53%] [G loss: 2.804172]\n",
      "epoch:1 step:1228 [D loss: 0.275679, acc.: 89.06%] [G loss: 4.629111]\n",
      "epoch:1 step:1229 [D loss: 0.546191, acc.: 73.44%] [G loss: 4.428585]\n",
      "epoch:1 step:1230 [D loss: 0.188759, acc.: 94.53%] [G loss: 3.431779]\n",
      "epoch:1 step:1231 [D loss: 0.184914, acc.: 93.75%] [G loss: 3.224298]\n",
      "epoch:1 step:1232 [D loss: 0.158422, acc.: 95.31%] [G loss: 3.093030]\n",
      "epoch:1 step:1233 [D loss: 0.207079, acc.: 96.09%] [G loss: 3.921051]\n",
      "epoch:1 step:1234 [D loss: 0.161717, acc.: 92.97%] [G loss: 3.787192]\n",
      "epoch:1 step:1235 [D loss: 0.294079, acc.: 85.94%] [G loss: 3.438620]\n",
      "epoch:1 step:1236 [D loss: 0.306633, acc.: 85.94%] [G loss: 3.114996]\n",
      "epoch:1 step:1237 [D loss: 0.282380, acc.: 89.06%] [G loss: 3.492603]\n",
      "epoch:1 step:1238 [D loss: 0.157837, acc.: 97.66%] [G loss: 3.479142]\n",
      "epoch:1 step:1239 [D loss: 0.299224, acc.: 85.94%] [G loss: 5.721148]\n",
      "epoch:1 step:1240 [D loss: 0.458032, acc.: 75.78%] [G loss: 3.685563]\n",
      "epoch:1 step:1241 [D loss: 0.078386, acc.: 100.00%] [G loss: 2.601006]\n",
      "epoch:1 step:1242 [D loss: 0.226032, acc.: 89.84%] [G loss: 4.916835]\n",
      "epoch:1 step:1243 [D loss: 0.222001, acc.: 88.28%] [G loss: 2.996815]\n",
      "epoch:1 step:1244 [D loss: 0.361694, acc.: 84.38%] [G loss: 5.824773]\n",
      "epoch:1 step:1245 [D loss: 0.276269, acc.: 86.72%] [G loss: 4.611528]\n",
      "epoch:1 step:1246 [D loss: 0.095894, acc.: 96.88%] [G loss: 2.094799]\n",
      "epoch:1 step:1247 [D loss: 0.136682, acc.: 95.31%] [G loss: 2.936952]\n",
      "epoch:1 step:1248 [D loss: 0.058225, acc.: 98.44%] [G loss: 4.161024]\n",
      "epoch:1 step:1249 [D loss: 0.119960, acc.: 96.09%] [G loss: 1.935897]\n",
      "epoch:1 step:1250 [D loss: 0.802783, acc.: 58.59%] [G loss: 7.574498]\n",
      "epoch:1 step:1251 [D loss: 1.406758, acc.: 50.78%] [G loss: 3.901484]\n",
      "epoch:1 step:1252 [D loss: 0.165590, acc.: 96.09%] [G loss: 3.552918]\n",
      "epoch:1 step:1253 [D loss: 0.068579, acc.: 99.22%] [G loss: 3.910384]\n",
      "epoch:1 step:1254 [D loss: 0.099467, acc.: 97.66%] [G loss: 4.194819]\n",
      "epoch:1 step:1255 [D loss: 0.158405, acc.: 93.75%] [G loss: 3.421494]\n",
      "epoch:1 step:1256 [D loss: 0.062052, acc.: 99.22%] [G loss: 2.981937]\n",
      "epoch:1 step:1257 [D loss: 0.090948, acc.: 99.22%] [G loss: 3.217559]\n",
      "epoch:1 step:1258 [D loss: 0.097581, acc.: 98.44%] [G loss: 3.119367]\n",
      "epoch:1 step:1259 [D loss: 0.163918, acc.: 96.88%] [G loss: 4.065213]\n",
      "epoch:1 step:1260 [D loss: 0.217472, acc.: 92.19%] [G loss: 2.958977]\n",
      "epoch:1 step:1261 [D loss: 0.318937, acc.: 87.50%] [G loss: 4.536722]\n",
      "epoch:1 step:1262 [D loss: 0.175063, acc.: 92.97%] [G loss: 3.983161]\n",
      "epoch:1 step:1263 [D loss: 0.149574, acc.: 95.31%] [G loss: 3.189076]\n",
      "epoch:1 step:1264 [D loss: 0.091492, acc.: 97.66%] [G loss: 3.028119]\n",
      "epoch:1 step:1265 [D loss: 0.150059, acc.: 95.31%] [G loss: 4.553653]\n",
      "epoch:1 step:1266 [D loss: 0.229684, acc.: 93.75%] [G loss: 2.774256]\n",
      "epoch:1 step:1267 [D loss: 0.132836, acc.: 96.88%] [G loss: 4.034760]\n",
      "epoch:1 step:1268 [D loss: 0.102435, acc.: 96.88%] [G loss: 3.400461]\n",
      "epoch:1 step:1269 [D loss: 0.305434, acc.: 88.28%] [G loss: 5.558901]\n",
      "epoch:1 step:1270 [D loss: 0.302357, acc.: 84.38%] [G loss: 3.337025]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:1 step:1271 [D loss: 0.114486, acc.: 97.66%] [G loss: 3.549084]\n",
      "epoch:1 step:1272 [D loss: 0.081820, acc.: 98.44%] [G loss: 3.737860]\n",
      "epoch:1 step:1273 [D loss: 0.154818, acc.: 93.75%] [G loss: 3.423451]\n",
      "epoch:1 step:1274 [D loss: 0.054170, acc.: 98.44%] [G loss: 3.975861]\n",
      "epoch:1 step:1275 [D loss: 0.090460, acc.: 98.44%] [G loss: 3.506581]\n",
      "epoch:1 step:1276 [D loss: 0.206293, acc.: 93.75%] [G loss: 5.089455]\n",
      "epoch:1 step:1277 [D loss: 0.115207, acc.: 96.88%] [G loss: 3.663217]\n",
      "epoch:1 step:1278 [D loss: 0.167585, acc.: 94.53%] [G loss: 5.107786]\n",
      "epoch:1 step:1279 [D loss: 0.164384, acc.: 91.41%] [G loss: 3.495088]\n",
      "epoch:1 step:1280 [D loss: 0.215813, acc.: 92.19%] [G loss: 5.929532]\n",
      "epoch:1 step:1281 [D loss: 0.264066, acc.: 87.50%] [G loss: 2.914908]\n",
      "epoch:1 step:1282 [D loss: 0.089566, acc.: 96.88%] [G loss: 4.274058]\n",
      "epoch:1 step:1283 [D loss: 0.033709, acc.: 99.22%] [G loss: 4.469147]\n",
      "epoch:1 step:1284 [D loss: 0.159535, acc.: 96.09%] [G loss: 4.019744]\n",
      "epoch:1 step:1285 [D loss: 0.286358, acc.: 85.16%] [G loss: 4.650921]\n",
      "epoch:1 step:1286 [D loss: 0.088408, acc.: 96.88%] [G loss: 3.181703]\n",
      "epoch:1 step:1287 [D loss: 0.101242, acc.: 98.44%] [G loss: 4.308099]\n",
      "epoch:1 step:1288 [D loss: 0.074556, acc.: 98.44%] [G loss: 2.833380]\n",
      "epoch:1 step:1289 [D loss: 1.541825, acc.: 41.41%] [G loss: 10.384657]\n",
      "epoch:1 step:1290 [D loss: 2.584629, acc.: 50.00%] [G loss: 4.334445]\n",
      "epoch:1 step:1291 [D loss: 0.324433, acc.: 85.94%] [G loss: 2.976504]\n",
      "epoch:1 step:1292 [D loss: 0.093624, acc.: 97.66%] [G loss: 3.098072]\n",
      "epoch:1 step:1293 [D loss: 0.081477, acc.: 98.44%] [G loss: 3.013547]\n",
      "epoch:1 step:1294 [D loss: 0.089825, acc.: 98.44%] [G loss: 2.937454]\n",
      "epoch:1 step:1295 [D loss: 0.179326, acc.: 93.75%] [G loss: 2.936266]\n",
      "epoch:1 step:1296 [D loss: 0.142592, acc.: 97.66%] [G loss: 3.813525]\n",
      "epoch:1 step:1297 [D loss: 0.096625, acc.: 99.22%] [G loss: 3.916519]\n",
      "epoch:1 step:1298 [D loss: 0.196450, acc.: 93.75%] [G loss: 3.419962]\n",
      "epoch:1 step:1299 [D loss: 0.108316, acc.: 97.66%] [G loss: 3.708985]\n",
      "epoch:1 step:1300 [D loss: 0.251926, acc.: 93.75%] [G loss: 3.471190]\n",
      "epoch:1 step:1301 [D loss: 0.084122, acc.: 100.00%] [G loss: 4.270623]\n",
      "epoch:1 step:1302 [D loss: 0.131941, acc.: 98.44%] [G loss: 2.881487]\n",
      "epoch:1 step:1303 [D loss: 0.087275, acc.: 98.44%] [G loss: 3.240013]\n",
      "epoch:1 step:1304 [D loss: 0.067184, acc.: 100.00%] [G loss: 3.729700]\n",
      "epoch:1 step:1305 [D loss: 0.072126, acc.: 99.22%] [G loss: 3.913290]\n",
      "epoch:1 step:1306 [D loss: 0.805755, acc.: 57.81%] [G loss: 6.632236]\n",
      "epoch:1 step:1307 [D loss: 1.234294, acc.: 47.66%] [G loss: 3.514357]\n",
      "epoch:1 step:1308 [D loss: 0.079357, acc.: 98.44%] [G loss: 4.017482]\n",
      "epoch:1 step:1309 [D loss: 0.035870, acc.: 100.00%] [G loss: 4.452723]\n",
      "epoch:1 step:1310 [D loss: 0.027657, acc.: 100.00%] [G loss: 4.113329]\n",
      "epoch:1 step:1311 [D loss: 0.038271, acc.: 100.00%] [G loss: 3.763901]\n",
      "epoch:1 step:1312 [D loss: 0.035380, acc.: 100.00%] [G loss: 4.247698]\n",
      "epoch:1 step:1313 [D loss: 0.028345, acc.: 100.00%] [G loss: 3.453057]\n",
      "epoch:1 step:1314 [D loss: 0.072205, acc.: 100.00%] [G loss: 3.689651]\n",
      "epoch:1 step:1315 [D loss: 0.061401, acc.: 99.22%] [G loss: 4.167513]\n",
      "epoch:1 step:1316 [D loss: 0.091133, acc.: 97.66%] [G loss: 3.667438]\n",
      "epoch:1 step:1317 [D loss: 0.054748, acc.: 99.22%] [G loss: 3.273946]\n",
      "epoch:1 step:1318 [D loss: 0.069521, acc.: 99.22%] [G loss: 3.440661]\n",
      "epoch:1 step:1319 [D loss: 0.124778, acc.: 96.88%] [G loss: 2.567292]\n",
      "epoch:1 step:1320 [D loss: 0.118894, acc.: 99.22%] [G loss: 3.597512]\n",
      "epoch:1 step:1321 [D loss: 0.099615, acc.: 96.88%] [G loss: 2.495902]\n",
      "epoch:1 step:1322 [D loss: 0.034136, acc.: 100.00%] [G loss: 1.738224]\n",
      "epoch:1 step:1323 [D loss: 0.053830, acc.: 100.00%] [G loss: 1.905718]\n",
      "epoch:1 step:1324 [D loss: 0.050377, acc.: 99.22%] [G loss: 1.279247]\n",
      "epoch:1 step:1325 [D loss: 0.120940, acc.: 96.88%] [G loss: 3.447993]\n",
      "epoch:1 step:1326 [D loss: 0.065656, acc.: 99.22%] [G loss: 2.652192]\n",
      "epoch:1 step:1327 [D loss: 0.162264, acc.: 96.09%] [G loss: 2.050206]\n",
      "epoch:1 step:1328 [D loss: 0.052575, acc.: 100.00%] [G loss: 2.493414]\n",
      "epoch:1 step:1329 [D loss: 0.117366, acc.: 96.88%] [G loss: 5.004466]\n",
      "epoch:1 step:1330 [D loss: 0.875196, acc.: 53.12%] [G loss: 8.506707]\n",
      "epoch:1 step:1331 [D loss: 0.586715, acc.: 70.31%] [G loss: 6.651226]\n",
      "epoch:1 step:1332 [D loss: 0.032363, acc.: 99.22%] [G loss: 6.067906]\n",
      "epoch:1 step:1333 [D loss: 0.038528, acc.: 99.22%] [G loss: 5.745845]\n",
      "epoch:1 step:1334 [D loss: 0.017574, acc.: 100.00%] [G loss: 5.552686]\n",
      "epoch:1 step:1335 [D loss: 0.032955, acc.: 100.00%] [G loss: 4.838861]\n",
      "epoch:1 step:1336 [D loss: 0.160719, acc.: 92.97%] [G loss: 6.188361]\n",
      "epoch:1 step:1337 [D loss: 0.015536, acc.: 100.00%] [G loss: 6.742607]\n",
      "epoch:1 step:1338 [D loss: 0.190897, acc.: 92.19%] [G loss: 3.848673]\n",
      "epoch:1 step:1339 [D loss: 0.371166, acc.: 79.69%] [G loss: 7.648682]\n",
      "epoch:1 step:1340 [D loss: 0.334103, acc.: 84.38%] [G loss: 6.644705]\n",
      "epoch:1 step:1341 [D loss: 0.016322, acc.: 100.00%] [G loss: 5.285605]\n",
      "epoch:1 step:1342 [D loss: 0.035660, acc.: 99.22%] [G loss: 4.500135]\n",
      "epoch:1 step:1343 [D loss: 0.062309, acc.: 97.66%] [G loss: 4.571430]\n",
      "epoch:1 step:1344 [D loss: 0.045349, acc.: 100.00%] [G loss: 4.413719]\n",
      "epoch:1 step:1345 [D loss: 0.144428, acc.: 96.09%] [G loss: 4.964285]\n",
      "epoch:1 step:1346 [D loss: 0.121374, acc.: 97.66%] [G loss: 4.621314]\n",
      "epoch:1 step:1347 [D loss: 0.118995, acc.: 96.09%] [G loss: 4.268614]\n",
      "epoch:1 step:1348 [D loss: 0.886950, acc.: 52.34%] [G loss: 8.438954]\n",
      "epoch:1 step:1349 [D loss: 1.460886, acc.: 51.56%] [G loss: 3.066382]\n",
      "epoch:1 step:1350 [D loss: 0.271385, acc.: 88.28%] [G loss: 4.611652]\n",
      "epoch:1 step:1351 [D loss: 0.037070, acc.: 99.22%] [G loss: 5.346409]\n",
      "epoch:1 step:1352 [D loss: 0.056484, acc.: 98.44%] [G loss: 5.013238]\n",
      "epoch:1 step:1353 [D loss: 0.059811, acc.: 99.22%] [G loss: 4.178617]\n",
      "epoch:1 step:1354 [D loss: 0.065074, acc.: 99.22%] [G loss: 3.942463]\n",
      "epoch:1 step:1355 [D loss: 0.070089, acc.: 99.22%] [G loss: 3.310385]\n",
      "epoch:1 step:1356 [D loss: 0.100673, acc.: 99.22%] [G loss: 4.132442]\n",
      "epoch:1 step:1357 [D loss: 0.093173, acc.: 98.44%] [G loss: 3.581671]\n",
      "epoch:1 step:1358 [D loss: 0.362943, acc.: 83.59%] [G loss: 4.925697]\n",
      "epoch:1 step:1359 [D loss: 0.461267, acc.: 75.78%] [G loss: 3.123857]\n",
      "epoch:1 step:1360 [D loss: 0.051919, acc.: 100.00%] [G loss: 2.603248]\n",
      "epoch:1 step:1361 [D loss: 0.074894, acc.: 100.00%] [G loss: 2.864326]\n",
      "epoch:1 step:1362 [D loss: 0.047298, acc.: 100.00%] [G loss: 3.125400]\n",
      "epoch:1 step:1363 [D loss: 0.051920, acc.: 100.00%] [G loss: 2.753786]\n",
      "epoch:1 step:1364 [D loss: 0.084918, acc.: 99.22%] [G loss: 3.117477]\n",
      "epoch:1 step:1365 [D loss: 0.094046, acc.: 100.00%] [G loss: 3.723387]\n",
      "epoch:1 step:1366 [D loss: 0.063718, acc.: 99.22%] [G loss: 2.433253]\n",
      "epoch:1 step:1367 [D loss: 0.059807, acc.: 100.00%] [G loss: 2.763208]\n",
      "epoch:1 step:1368 [D loss: 0.087653, acc.: 97.66%] [G loss: 2.843154]\n",
      "epoch:1 step:1369 [D loss: 0.364729, acc.: 83.59%] [G loss: 5.967274]\n",
      "epoch:1 step:1370 [D loss: 0.534027, acc.: 73.44%] [G loss: 2.889283]\n",
      "epoch:1 step:1371 [D loss: 0.090331, acc.: 96.88%] [G loss: 3.536275]\n",
      "epoch:1 step:1372 [D loss: 0.032621, acc.: 100.00%] [G loss: 4.147084]\n",
      "epoch:1 step:1373 [D loss: 0.041647, acc.: 100.00%] [G loss: 3.560504]\n",
      "epoch:1 step:1374 [D loss: 0.142817, acc.: 95.31%] [G loss: 2.255559]\n",
      "epoch:1 step:1375 [D loss: 0.288511, acc.: 85.94%] [G loss: 5.900256]\n",
      "epoch:1 step:1376 [D loss: 1.015862, acc.: 53.12%] [G loss: 3.952151]\n",
      "epoch:1 step:1377 [D loss: 0.026926, acc.: 100.00%] [G loss: 3.868341]\n",
      "epoch:1 step:1378 [D loss: 0.128327, acc.: 96.09%] [G loss: 4.543902]\n",
      "epoch:1 step:1379 [D loss: 0.058595, acc.: 98.44%] [G loss: 4.294292]\n",
      "epoch:1 step:1380 [D loss: 0.058789, acc.: 100.00%] [G loss: 4.185219]\n",
      "epoch:1 step:1381 [D loss: 0.179066, acc.: 94.53%] [G loss: 4.395328]\n",
      "epoch:1 step:1382 [D loss: 0.104403, acc.: 99.22%] [G loss: 4.054530]\n",
      "epoch:1 step:1383 [D loss: 0.244512, acc.: 94.53%] [G loss: 4.822995]\n",
      "epoch:1 step:1384 [D loss: 0.433784, acc.: 79.69%] [G loss: 2.482115]\n",
      "epoch:1 step:1385 [D loss: 0.376665, acc.: 85.16%] [G loss: 6.455778]\n",
      "epoch:1 step:1386 [D loss: 0.323628, acc.: 84.38%] [G loss: 5.581805]\n",
      "epoch:1 step:1387 [D loss: 0.046436, acc.: 100.00%] [G loss: 5.021081]\n",
      "epoch:1 step:1388 [D loss: 0.054579, acc.: 100.00%] [G loss: 4.454237]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:1 step:1389 [D loss: 0.162816, acc.: 92.97%] [G loss: 4.923860]\n",
      "epoch:1 step:1390 [D loss: 0.218771, acc.: 91.41%] [G loss: 4.117031]\n",
      "epoch:1 step:1391 [D loss: 0.110061, acc.: 98.44%] [G loss: 4.032839]\n",
      "epoch:1 step:1392 [D loss: 0.321490, acc.: 84.38%] [G loss: 5.758940]\n",
      "epoch:1 step:1393 [D loss: 0.218543, acc.: 89.84%] [G loss: 4.375588]\n",
      "epoch:1 step:1394 [D loss: 0.278816, acc.: 88.28%] [G loss: 4.733532]\n",
      "epoch:1 step:1395 [D loss: 0.061409, acc.: 99.22%] [G loss: 4.673333]\n",
      "epoch:1 step:1396 [D loss: 0.136020, acc.: 97.66%] [G loss: 3.351831]\n",
      "epoch:1 step:1397 [D loss: 0.316163, acc.: 84.38%] [G loss: 6.666860]\n",
      "epoch:1 step:1398 [D loss: 0.759384, acc.: 66.41%] [G loss: 2.534909]\n",
      "epoch:1 step:1399 [D loss: 0.228483, acc.: 89.84%] [G loss: 4.729737]\n",
      "epoch:1 step:1400 [D loss: 0.054634, acc.: 98.44%] [G loss: 5.211038]\n",
      "##############\n",
      "[0.93723935 1.07524647 0.96610293 0.92859587 2.10666546 2.13105752\n",
      " 2.09918217 2.10269794 0.94613561 2.11777247]\n",
      "##########\n",
      "epoch:1 step:1401 [D loss: 0.203136, acc.: 90.62%] [G loss: 3.741497]\n",
      "epoch:1 step:1402 [D loss: 0.194894, acc.: 94.53%] [G loss: 4.732385]\n",
      "epoch:1 step:1403 [D loss: 0.047411, acc.: 100.00%] [G loss: 4.402388]\n",
      "epoch:1 step:1404 [D loss: 0.340242, acc.: 85.16%] [G loss: 5.037760]\n",
      "epoch:1 step:1405 [D loss: 0.044214, acc.: 100.00%] [G loss: 5.433179]\n",
      "epoch:1 step:1406 [D loss: 0.190179, acc.: 93.75%] [G loss: 3.464866]\n",
      "epoch:1 step:1407 [D loss: 0.116256, acc.: 96.88%] [G loss: 4.223785]\n",
      "epoch:1 step:1408 [D loss: 0.154362, acc.: 95.31%] [G loss: 5.115771]\n",
      "epoch:1 step:1409 [D loss: 0.133582, acc.: 96.88%] [G loss: 4.379866]\n",
      "epoch:1 step:1410 [D loss: 0.218830, acc.: 94.53%] [G loss: 6.441361]\n",
      "epoch:1 step:1411 [D loss: 0.122380, acc.: 96.09%] [G loss: 5.060824]\n",
      "epoch:1 step:1412 [D loss: 0.096394, acc.: 96.88%] [G loss: 5.271305]\n",
      "epoch:1 step:1413 [D loss: 0.235591, acc.: 88.28%] [G loss: 4.602592]\n",
      "epoch:1 step:1414 [D loss: 0.043389, acc.: 100.00%] [G loss: 5.349430]\n",
      "epoch:1 step:1415 [D loss: 0.092226, acc.: 97.66%] [G loss: 4.573400]\n",
      "epoch:1 step:1416 [D loss: 0.121944, acc.: 96.88%] [G loss: 4.034753]\n",
      "epoch:1 step:1417 [D loss: 0.384949, acc.: 83.59%] [G loss: 8.432533]\n",
      "epoch:1 step:1418 [D loss: 0.742468, acc.: 64.84%] [G loss: 4.565745]\n",
      "epoch:1 step:1419 [D loss: 0.102712, acc.: 96.88%] [G loss: 4.364738]\n",
      "epoch:1 step:1420 [D loss: 0.013993, acc.: 100.00%] [G loss: 4.335389]\n",
      "epoch:1 step:1421 [D loss: 0.041743, acc.: 99.22%] [G loss: 3.103790]\n",
      "epoch:1 step:1422 [D loss: 0.119859, acc.: 96.88%] [G loss: 2.287632]\n",
      "epoch:1 step:1423 [D loss: 0.375632, acc.: 80.47%] [G loss: 7.259440]\n",
      "epoch:1 step:1424 [D loss: 1.057897, acc.: 55.47%] [G loss: 1.472361]\n",
      "epoch:1 step:1425 [D loss: 0.235352, acc.: 89.84%] [G loss: 5.118746]\n",
      "epoch:1 step:1426 [D loss: 0.248861, acc.: 86.72%] [G loss: 2.178400]\n",
      "epoch:1 step:1427 [D loss: 0.352904, acc.: 81.25%] [G loss: 5.387030]\n",
      "epoch:1 step:1428 [D loss: 0.181278, acc.: 89.84%] [G loss: 4.762232]\n",
      "epoch:1 step:1429 [D loss: 0.162958, acc.: 93.75%] [G loss: 2.762694]\n",
      "epoch:1 step:1430 [D loss: 0.103015, acc.: 98.44%] [G loss: 2.902253]\n",
      "epoch:1 step:1431 [D loss: 0.114878, acc.: 96.09%] [G loss: 3.127527]\n",
      "epoch:1 step:1432 [D loss: 0.159848, acc.: 96.09%] [G loss: 2.634833]\n",
      "epoch:1 step:1433 [D loss: 0.354819, acc.: 82.81%] [G loss: 6.488523]\n",
      "epoch:1 step:1434 [D loss: 0.466066, acc.: 78.12%] [G loss: 3.671362]\n",
      "epoch:1 step:1435 [D loss: 0.109502, acc.: 96.09%] [G loss: 3.513848]\n",
      "epoch:1 step:1436 [D loss: 0.054107, acc.: 99.22%] [G loss: 4.231201]\n",
      "epoch:1 step:1437 [D loss: 0.489693, acc.: 76.56%] [G loss: 5.298910]\n",
      "epoch:1 step:1438 [D loss: 0.088256, acc.: 98.44%] [G loss: 5.732786]\n",
      "epoch:1 step:1439 [D loss: 0.044061, acc.: 99.22%] [G loss: 4.712673]\n",
      "epoch:1 step:1440 [D loss: 0.115161, acc.: 96.88%] [G loss: 4.635562]\n",
      "epoch:1 step:1441 [D loss: 0.104430, acc.: 96.88%] [G loss: 5.862129]\n",
      "epoch:1 step:1442 [D loss: 0.200165, acc.: 92.97%] [G loss: 3.829612]\n",
      "epoch:1 step:1443 [D loss: 0.148825, acc.: 94.53%] [G loss: 5.857310]\n",
      "epoch:1 step:1444 [D loss: 0.039219, acc.: 100.00%] [G loss: 5.523554]\n",
      "epoch:1 step:1445 [D loss: 0.177226, acc.: 96.88%] [G loss: 4.098116]\n",
      "epoch:1 step:1446 [D loss: 0.173403, acc.: 94.53%] [G loss: 5.956968]\n",
      "epoch:1 step:1447 [D loss: 0.125652, acc.: 94.53%] [G loss: 4.986913]\n",
      "epoch:1 step:1448 [D loss: 0.073830, acc.: 99.22%] [G loss: 3.597168]\n",
      "epoch:1 step:1449 [D loss: 0.131102, acc.: 95.31%] [G loss: 4.200187]\n",
      "epoch:1 step:1450 [D loss: 0.094464, acc.: 97.66%] [G loss: 4.005156]\n",
      "epoch:1 step:1451 [D loss: 0.228974, acc.: 92.97%] [G loss: 6.874515]\n",
      "epoch:1 step:1452 [D loss: 0.587053, acc.: 73.44%] [G loss: 1.837692]\n",
      "epoch:1 step:1453 [D loss: 0.274089, acc.: 91.41%] [G loss: 6.474026]\n",
      "epoch:1 step:1454 [D loss: 0.036709, acc.: 98.44%] [G loss: 7.118233]\n",
      "epoch:1 step:1455 [D loss: 0.213082, acc.: 90.62%] [G loss: 4.000800]\n",
      "epoch:1 step:1456 [D loss: 0.105279, acc.: 96.09%] [G loss: 2.361886]\n",
      "epoch:1 step:1457 [D loss: 0.054918, acc.: 99.22%] [G loss: 2.720043]\n",
      "epoch:1 step:1458 [D loss: 0.046331, acc.: 100.00%] [G loss: 1.084689]\n",
      "epoch:1 step:1459 [D loss: 0.270366, acc.: 88.28%] [G loss: 3.415328]\n",
      "epoch:1 step:1460 [D loss: 0.094929, acc.: 96.09%] [G loss: 3.301604]\n",
      "epoch:1 step:1461 [D loss: 0.256430, acc.: 89.84%] [G loss: 0.781684]\n",
      "epoch:1 step:1462 [D loss: 0.111992, acc.: 96.09%] [G loss: 1.171620]\n",
      "epoch:1 step:1463 [D loss: 0.024758, acc.: 99.22%] [G loss: 2.057354]\n",
      "epoch:1 step:1464 [D loss: 0.030536, acc.: 99.22%] [G loss: 0.870695]\n",
      "epoch:1 step:1465 [D loss: 0.188429, acc.: 94.53%] [G loss: 5.255604]\n",
      "epoch:1 step:1466 [D loss: 0.253856, acc.: 85.16%] [G loss: 1.605704]\n",
      "epoch:1 step:1467 [D loss: 0.150755, acc.: 93.75%] [G loss: 2.870550]\n",
      "epoch:1 step:1468 [D loss: 0.022935, acc.: 100.00%] [G loss: 3.740567]\n",
      "epoch:1 step:1469 [D loss: 0.182527, acc.: 90.62%] [G loss: 2.983408]\n",
      "epoch:1 step:1470 [D loss: 0.148026, acc.: 94.53%] [G loss: 5.476771]\n",
      "epoch:1 step:1471 [D loss: 0.728375, acc.: 64.06%] [G loss: 6.443956]\n",
      "epoch:1 step:1472 [D loss: 0.038646, acc.: 100.00%] [G loss: 7.034179]\n",
      "epoch:1 step:1473 [D loss: 0.407360, acc.: 77.34%] [G loss: 2.615846]\n",
      "epoch:1 step:1474 [D loss: 0.813691, acc.: 62.50%] [G loss: 9.353109]\n",
      "epoch:1 step:1475 [D loss: 0.815036, acc.: 67.97%] [G loss: 7.277775]\n",
      "epoch:1 step:1476 [D loss: 0.043352, acc.: 100.00%] [G loss: 5.861673]\n",
      "epoch:1 step:1477 [D loss: 0.022810, acc.: 100.00%] [G loss: 4.714100]\n",
      "epoch:1 step:1478 [D loss: 0.014853, acc.: 100.00%] [G loss: 3.496879]\n",
      "epoch:1 step:1479 [D loss: 0.219044, acc.: 92.19%] [G loss: 5.004751]\n",
      "epoch:1 step:1480 [D loss: 0.049380, acc.: 99.22%] [G loss: 5.617345]\n",
      "epoch:1 step:1481 [D loss: 0.081238, acc.: 96.88%] [G loss: 3.764845]\n",
      "epoch:1 step:1482 [D loss: 0.095201, acc.: 99.22%] [G loss: 3.367269]\n",
      "epoch:1 step:1483 [D loss: 0.172131, acc.: 93.75%] [G loss: 5.646852]\n",
      "epoch:1 step:1484 [D loss: 0.087527, acc.: 94.53%] [G loss: 4.128756]\n",
      "epoch:1 step:1485 [D loss: 0.138262, acc.: 95.31%] [G loss: 4.153580]\n",
      "epoch:1 step:1486 [D loss: 0.080162, acc.: 99.22%] [G loss: 3.948348]\n",
      "epoch:1 step:1487 [D loss: 0.132228, acc.: 96.09%] [G loss: 4.752793]\n",
      "epoch:1 step:1488 [D loss: 0.205061, acc.: 93.75%] [G loss: 5.424338]\n",
      "epoch:1 step:1489 [D loss: 0.239133, acc.: 90.62%] [G loss: 4.174045]\n",
      "epoch:1 step:1490 [D loss: 0.052366, acc.: 99.22%] [G loss: 2.519491]\n",
      "epoch:1 step:1491 [D loss: 0.057334, acc.: 98.44%] [G loss: 2.809039]\n",
      "epoch:1 step:1492 [D loss: 0.077913, acc.: 99.22%] [G loss: 3.664246]\n",
      "epoch:1 step:1493 [D loss: 0.105864, acc.: 99.22%] [G loss: 3.431630]\n",
      "epoch:1 step:1494 [D loss: 0.203666, acc.: 90.62%] [G loss: 4.166128]\n",
      "epoch:1 step:1495 [D loss: 0.070019, acc.: 98.44%] [G loss: 1.905438]\n",
      "epoch:1 step:1496 [D loss: 0.089292, acc.: 98.44%] [G loss: 1.444278]\n",
      "epoch:1 step:1497 [D loss: 0.134411, acc.: 95.31%] [G loss: 3.431421]\n",
      "epoch:1 step:1498 [D loss: 0.146108, acc.: 96.88%] [G loss: 3.296242]\n",
      "epoch:1 step:1499 [D loss: 0.365144, acc.: 83.59%] [G loss: 6.482206]\n",
      "epoch:1 step:1500 [D loss: 0.152344, acc.: 92.97%] [G loss: 5.454741]\n",
      "epoch:1 step:1501 [D loss: 0.215133, acc.: 87.50%] [G loss: 0.743692]\n",
      "epoch:1 step:1502 [D loss: 0.127240, acc.: 96.88%] [G loss: 1.278816]\n",
      "epoch:1 step:1503 [D loss: 0.067806, acc.: 97.66%] [G loss: 3.493541]\n",
      "epoch:1 step:1504 [D loss: 0.237736, acc.: 90.62%] [G loss: 1.552596]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:1 step:1505 [D loss: 0.132875, acc.: 92.97%] [G loss: 2.296905]\n",
      "epoch:1 step:1506 [D loss: 0.205577, acc.: 92.97%] [G loss: 5.616494]\n",
      "epoch:1 step:1507 [D loss: 0.088922, acc.: 97.66%] [G loss: 3.333880]\n",
      "epoch:1 step:1508 [D loss: 0.707859, acc.: 64.06%] [G loss: 5.944750]\n",
      "epoch:1 step:1509 [D loss: 0.183456, acc.: 92.19%] [G loss: 4.617280]\n",
      "epoch:1 step:1510 [D loss: 0.047068, acc.: 100.00%] [G loss: 2.686963]\n",
      "epoch:1 step:1511 [D loss: 0.054586, acc.: 98.44%] [G loss: 3.194595]\n",
      "epoch:1 step:1512 [D loss: 0.124589, acc.: 96.88%] [G loss: 3.529834]\n",
      "epoch:1 step:1513 [D loss: 0.132076, acc.: 96.88%] [G loss: 5.509027]\n",
      "epoch:1 step:1514 [D loss: 0.215628, acc.: 92.19%] [G loss: 5.891298]\n",
      "epoch:1 step:1515 [D loss: 0.241766, acc.: 90.62%] [G loss: 7.021883]\n",
      "epoch:1 step:1516 [D loss: 0.125056, acc.: 97.66%] [G loss: 6.076233]\n",
      "epoch:1 step:1517 [D loss: 0.052944, acc.: 99.22%] [G loss: 4.815008]\n",
      "epoch:1 step:1518 [D loss: 0.106689, acc.: 96.88%] [G loss: 7.802606]\n",
      "epoch:1 step:1519 [D loss: 0.624221, acc.: 65.62%] [G loss: 9.383395]\n",
      "epoch:1 step:1520 [D loss: 0.129148, acc.: 95.31%] [G loss: 9.127753]\n",
      "epoch:1 step:1521 [D loss: 0.030287, acc.: 98.44%] [G loss: 7.542809]\n",
      "epoch:1 step:1522 [D loss: 0.010016, acc.: 100.00%] [G loss: 5.840258]\n",
      "epoch:1 step:1523 [D loss: 0.026344, acc.: 100.00%] [G loss: 5.798667]\n",
      "epoch:1 step:1524 [D loss: 0.064496, acc.: 100.00%] [G loss: 6.699870]\n",
      "epoch:1 step:1525 [D loss: 0.019126, acc.: 100.00%] [G loss: 6.404607]\n",
      "epoch:1 step:1526 [D loss: 0.103932, acc.: 99.22%] [G loss: 4.452985]\n",
      "epoch:1 step:1527 [D loss: 0.072780, acc.: 99.22%] [G loss: 2.700684]\n",
      "epoch:1 step:1528 [D loss: 0.083450, acc.: 96.88%] [G loss: 5.194936]\n",
      "epoch:1 step:1529 [D loss: 0.046560, acc.: 100.00%] [G loss: 3.567874]\n",
      "epoch:1 step:1530 [D loss: 0.917328, acc.: 58.59%] [G loss: 11.611470]\n",
      "epoch:1 step:1531 [D loss: 3.054265, acc.: 50.00%] [G loss: 5.528483]\n",
      "epoch:1 step:1532 [D loss: 0.377915, acc.: 82.03%] [G loss: 2.748172]\n",
      "epoch:1 step:1533 [D loss: 0.093781, acc.: 98.44%] [G loss: 3.058283]\n",
      "epoch:1 step:1534 [D loss: 0.112098, acc.: 96.09%] [G loss: 3.431946]\n",
      "epoch:1 step:1535 [D loss: 0.081244, acc.: 98.44%] [G loss: 2.476206]\n",
      "epoch:1 step:1536 [D loss: 0.080327, acc.: 99.22%] [G loss: 2.943042]\n",
      "epoch:1 step:1537 [D loss: 0.101981, acc.: 98.44%] [G loss: 3.635885]\n",
      "epoch:1 step:1538 [D loss: 0.099510, acc.: 96.09%] [G loss: 3.055955]\n",
      "epoch:1 step:1539 [D loss: 0.509780, acc.: 75.78%] [G loss: 5.446666]\n",
      "epoch:1 step:1540 [D loss: 0.903831, acc.: 61.72%] [G loss: 2.739636]\n",
      "epoch:1 step:1541 [D loss: 0.252033, acc.: 90.62%] [G loss: 2.446644]\n",
      "epoch:1 step:1542 [D loss: 0.089933, acc.: 95.31%] [G loss: 3.150528]\n",
      "epoch:1 step:1543 [D loss: 0.211498, acc.: 92.19%] [G loss: 1.581894]\n",
      "epoch:1 step:1544 [D loss: 0.174570, acc.: 96.88%] [G loss: 1.597122]\n",
      "epoch:1 step:1545 [D loss: 0.091679, acc.: 99.22%] [G loss: 1.779455]\n",
      "epoch:1 step:1546 [D loss: 0.254277, acc.: 91.41%] [G loss: 2.047056]\n",
      "epoch:1 step:1547 [D loss: 0.229499, acc.: 91.41%] [G loss: 3.876443]\n",
      "epoch:1 step:1548 [D loss: 0.118182, acc.: 96.88%] [G loss: 2.422866]\n",
      "epoch:1 step:1549 [D loss: 0.135244, acc.: 96.09%] [G loss: 1.312476]\n",
      "epoch:1 step:1550 [D loss: 0.056359, acc.: 100.00%] [G loss: 1.729052]\n",
      "epoch:1 step:1551 [D loss: 0.423375, acc.: 75.00%] [G loss: 6.181162]\n",
      "epoch:1 step:1552 [D loss: 0.276151, acc.: 84.38%] [G loss: 6.180119]\n",
      "epoch:1 step:1553 [D loss: 0.034530, acc.: 98.44%] [G loss: 5.303769]\n",
      "epoch:1 step:1554 [D loss: 0.031318, acc.: 99.22%] [G loss: 4.631376]\n",
      "epoch:1 step:1555 [D loss: 0.059689, acc.: 98.44%] [G loss: 3.992038]\n",
      "epoch:1 step:1556 [D loss: 0.113743, acc.: 96.88%] [G loss: 5.681790]\n",
      "epoch:1 step:1557 [D loss: 0.048975, acc.: 98.44%] [G loss: 5.511626]\n",
      "epoch:1 step:1558 [D loss: 0.198494, acc.: 92.19%] [G loss: 5.416669]\n",
      "epoch:1 step:1559 [D loss: 0.045841, acc.: 100.00%] [G loss: 5.143913]\n",
      "epoch:1 step:1560 [D loss: 0.143688, acc.: 97.66%] [G loss: 3.784003]\n",
      "epoch:1 step:1561 [D loss: 0.077875, acc.: 96.88%] [G loss: 5.046689]\n",
      "epoch:1 step:1562 [D loss: 0.037590, acc.: 100.00%] [G loss: 5.102092]\n",
      "epoch:2 step:1563 [D loss: 0.428275, acc.: 81.25%] [G loss: 6.813683]\n",
      "epoch:2 step:1564 [D loss: 0.196013, acc.: 92.19%] [G loss: 6.473844]\n",
      "epoch:2 step:1565 [D loss: 0.042735, acc.: 99.22%] [G loss: 5.203695]\n",
      "epoch:2 step:1566 [D loss: 0.040501, acc.: 99.22%] [G loss: 4.002966]\n",
      "epoch:2 step:1567 [D loss: 0.274876, acc.: 84.38%] [G loss: 7.248025]\n",
      "epoch:2 step:1568 [D loss: 0.221772, acc.: 89.84%] [G loss: 6.071783]\n",
      "epoch:2 step:1569 [D loss: 0.079992, acc.: 97.66%] [G loss: 3.701224]\n",
      "epoch:2 step:1570 [D loss: 0.101426, acc.: 97.66%] [G loss: 2.880485]\n",
      "epoch:2 step:1571 [D loss: 0.083292, acc.: 98.44%] [G loss: 3.912516]\n",
      "epoch:2 step:1572 [D loss: 0.088505, acc.: 96.88%] [G loss: 3.475852]\n",
      "epoch:2 step:1573 [D loss: 0.251411, acc.: 88.28%] [G loss: 5.539881]\n",
      "epoch:2 step:1574 [D loss: 0.236182, acc.: 89.84%] [G loss: 4.225163]\n",
      "epoch:2 step:1575 [D loss: 0.175176, acc.: 91.41%] [G loss: 5.881783]\n",
      "epoch:2 step:1576 [D loss: 0.077171, acc.: 97.66%] [G loss: 4.152554]\n",
      "epoch:2 step:1577 [D loss: 0.117994, acc.: 97.66%] [G loss: 4.608271]\n",
      "epoch:2 step:1578 [D loss: 0.090955, acc.: 96.88%] [G loss: 3.515146]\n",
      "epoch:2 step:1579 [D loss: 0.072067, acc.: 99.22%] [G loss: 1.840018]\n",
      "epoch:2 step:1580 [D loss: 0.217982, acc.: 93.75%] [G loss: 5.431394]\n",
      "epoch:2 step:1581 [D loss: 0.453733, acc.: 78.91%] [G loss: 2.593230]\n",
      "epoch:2 step:1582 [D loss: 0.123862, acc.: 96.88%] [G loss: 4.813825]\n",
      "epoch:2 step:1583 [D loss: 0.040705, acc.: 100.00%] [G loss: 3.571437]\n",
      "epoch:2 step:1584 [D loss: 0.302398, acc.: 87.50%] [G loss: 4.375956]\n",
      "epoch:2 step:1585 [D loss: 0.229527, acc.: 87.50%] [G loss: 3.573927]\n",
      "epoch:2 step:1586 [D loss: 0.104803, acc.: 96.09%] [G loss: 2.499977]\n",
      "epoch:2 step:1587 [D loss: 0.481658, acc.: 74.22%] [G loss: 5.579280]\n",
      "epoch:2 step:1588 [D loss: 2.142026, acc.: 26.56%] [G loss: 5.856418]\n",
      "epoch:2 step:1589 [D loss: 0.205630, acc.: 89.84%] [G loss: 6.158154]\n",
      "epoch:2 step:1590 [D loss: 0.101127, acc.: 96.09%] [G loss: 5.092695]\n",
      "epoch:2 step:1591 [D loss: 0.179729, acc.: 90.62%] [G loss: 4.718991]\n",
      "epoch:2 step:1592 [D loss: 0.201776, acc.: 95.31%] [G loss: 5.394229]\n",
      "epoch:2 step:1593 [D loss: 0.145227, acc.: 95.31%] [G loss: 4.348683]\n",
      "epoch:2 step:1594 [D loss: 0.550632, acc.: 73.44%] [G loss: 7.042778]\n",
      "epoch:2 step:1595 [D loss: 0.626419, acc.: 77.34%] [G loss: 5.826925]\n",
      "epoch:2 step:1596 [D loss: 0.166890, acc.: 92.19%] [G loss: 3.678270]\n",
      "epoch:2 step:1597 [D loss: 0.142796, acc.: 94.53%] [G loss: 4.371873]\n",
      "epoch:2 step:1598 [D loss: 0.062770, acc.: 99.22%] [G loss: 4.955921]\n",
      "epoch:2 step:1599 [D loss: 0.044536, acc.: 99.22%] [G loss: 4.590213]\n",
      "epoch:2 step:1600 [D loss: 0.128449, acc.: 97.66%] [G loss: 4.411880]\n",
      "##############\n",
      "[1.08749303 1.02067944 2.10377371 0.91116578 2.10256811 1.017476\n",
      " 2.10276214 0.8638342  0.85198988 1.07675929]\n",
      "##########\n",
      "epoch:2 step:1601 [D loss: 0.068837, acc.: 99.22%] [G loss: 4.709826]\n",
      "epoch:2 step:1602 [D loss: 0.087784, acc.: 97.66%] [G loss: 3.946737]\n",
      "epoch:2 step:1603 [D loss: 0.071162, acc.: 99.22%] [G loss: 3.858483]\n",
      "epoch:2 step:1604 [D loss: 0.092862, acc.: 99.22%] [G loss: 3.578177]\n",
      "epoch:2 step:1605 [D loss: 0.132002, acc.: 98.44%] [G loss: 3.512128]\n",
      "epoch:2 step:1606 [D loss: 0.239823, acc.: 90.62%] [G loss: 3.324125]\n",
      "epoch:2 step:1607 [D loss: 0.101772, acc.: 96.88%] [G loss: 1.780853]\n",
      "epoch:2 step:1608 [D loss: 0.167877, acc.: 91.41%] [G loss: 4.972299]\n",
      "epoch:2 step:1609 [D loss: 0.205019, acc.: 89.84%] [G loss: 2.109561]\n",
      "epoch:2 step:1610 [D loss: 0.125271, acc.: 96.09%] [G loss: 1.691265]\n",
      "epoch:2 step:1611 [D loss: 0.048464, acc.: 99.22%] [G loss: 2.921338]\n",
      "epoch:2 step:1612 [D loss: 0.123180, acc.: 93.75%] [G loss: 0.861978]\n",
      "epoch:2 step:1613 [D loss: 0.153824, acc.: 95.31%] [G loss: 4.428783]\n",
      "epoch:2 step:1614 [D loss: 0.196993, acc.: 89.84%] [G loss: 2.142642]\n",
      "epoch:2 step:1615 [D loss: 0.049302, acc.: 99.22%] [G loss: 1.789880]\n",
      "epoch:2 step:1616 [D loss: 0.132857, acc.: 95.31%] [G loss: 4.749396]\n",
      "epoch:2 step:1617 [D loss: 0.060156, acc.: 99.22%] [G loss: 4.097631]\n",
      "epoch:2 step:1618 [D loss: 2.022535, acc.: 30.47%] [G loss: 8.796043]\n",
      "epoch:2 step:1619 [D loss: 0.447026, acc.: 84.38%] [G loss: 8.310797]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:2 step:1620 [D loss: 0.513334, acc.: 72.66%] [G loss: 3.202659]\n",
      "epoch:2 step:1621 [D loss: 0.439890, acc.: 78.91%] [G loss: 4.271615]\n",
      "epoch:2 step:1622 [D loss: 0.126405, acc.: 94.53%] [G loss: 4.186179]\n",
      "epoch:2 step:1623 [D loss: 0.068384, acc.: 99.22%] [G loss: 3.322710]\n",
      "epoch:2 step:1624 [D loss: 0.242418, acc.: 91.41%] [G loss: 3.274579]\n",
      "epoch:2 step:1625 [D loss: 0.318743, acc.: 85.94%] [G loss: 5.494460]\n",
      "epoch:2 step:1626 [D loss: 1.027970, acc.: 50.78%] [G loss: 2.740236]\n",
      "epoch:2 step:1627 [D loss: 0.108442, acc.: 98.44%] [G loss: 3.461587]\n",
      "epoch:2 step:1628 [D loss: 0.187468, acc.: 95.31%] [G loss: 4.526480]\n",
      "epoch:2 step:1629 [D loss: 0.183720, acc.: 92.97%] [G loss: 3.747759]\n",
      "epoch:2 step:1630 [D loss: 0.158040, acc.: 96.09%] [G loss: 3.440553]\n",
      "epoch:2 step:1631 [D loss: 0.167575, acc.: 93.75%] [G loss: 3.923460]\n",
      "epoch:2 step:1632 [D loss: 0.246242, acc.: 92.97%] [G loss: 4.576267]\n",
      "epoch:2 step:1633 [D loss: 0.123913, acc.: 96.09%] [G loss: 3.456778]\n",
      "epoch:2 step:1634 [D loss: 0.312923, acc.: 85.94%] [G loss: 5.645604]\n",
      "epoch:2 step:1635 [D loss: 0.349837, acc.: 86.72%] [G loss: 3.874043]\n",
      "epoch:2 step:1636 [D loss: 0.216723, acc.: 92.19%] [G loss: 5.625545]\n",
      "epoch:2 step:1637 [D loss: 0.093107, acc.: 97.66%] [G loss: 5.270796]\n",
      "epoch:2 step:1638 [D loss: 0.390550, acc.: 82.81%] [G loss: 3.963634]\n",
      "epoch:2 step:1639 [D loss: 0.076233, acc.: 97.66%] [G loss: 5.037643]\n",
      "epoch:2 step:1640 [D loss: 0.045044, acc.: 100.00%] [G loss: 4.457306]\n",
      "epoch:2 step:1641 [D loss: 0.118221, acc.: 95.31%] [G loss: 3.854432]\n",
      "epoch:2 step:1642 [D loss: 0.105519, acc.: 96.88%] [G loss: 3.276227]\n",
      "epoch:2 step:1643 [D loss: 0.123692, acc.: 96.88%] [G loss: 4.471466]\n",
      "epoch:2 step:1644 [D loss: 0.093887, acc.: 98.44%] [G loss: 2.902854]\n",
      "epoch:2 step:1645 [D loss: 0.099641, acc.: 97.66%] [G loss: 3.345959]\n",
      "epoch:2 step:1646 [D loss: 0.157229, acc.: 95.31%] [G loss: 3.006497]\n",
      "epoch:2 step:1647 [D loss: 0.110069, acc.: 97.66%] [G loss: 2.538343]\n",
      "epoch:2 step:1648 [D loss: 0.171360, acc.: 92.19%] [G loss: 4.099348]\n",
      "epoch:2 step:1649 [D loss: 0.247565, acc.: 91.41%] [G loss: 2.295882]\n",
      "epoch:2 step:1650 [D loss: 0.227188, acc.: 89.06%] [G loss: 3.467854]\n",
      "epoch:2 step:1651 [D loss: 0.102496, acc.: 95.31%] [G loss: 2.523437]\n",
      "epoch:2 step:1652 [D loss: 0.354852, acc.: 82.03%] [G loss: 4.873098]\n",
      "epoch:2 step:1653 [D loss: 0.983095, acc.: 58.59%] [G loss: 3.778587]\n",
      "epoch:2 step:1654 [D loss: 0.395389, acc.: 83.59%] [G loss: 5.695926]\n",
      "epoch:2 step:1655 [D loss: 0.198734, acc.: 91.41%] [G loss: 4.552426]\n",
      "epoch:2 step:1656 [D loss: 0.188361, acc.: 91.41%] [G loss: 3.374876]\n",
      "epoch:2 step:1657 [D loss: 0.181740, acc.: 92.19%] [G loss: 4.791142]\n",
      "epoch:2 step:1658 [D loss: 0.220469, acc.: 92.19%] [G loss: 3.670753]\n",
      "epoch:2 step:1659 [D loss: 0.093279, acc.: 96.88%] [G loss: 4.812188]\n",
      "epoch:2 step:1660 [D loss: 0.055365, acc.: 100.00%] [G loss: 3.977288]\n",
      "epoch:2 step:1661 [D loss: 0.440552, acc.: 78.91%] [G loss: 6.476305]\n",
      "epoch:2 step:1662 [D loss: 0.301304, acc.: 85.16%] [G loss: 3.853195]\n",
      "epoch:2 step:1663 [D loss: 0.084520, acc.: 98.44%] [G loss: 4.361677]\n",
      "epoch:2 step:1664 [D loss: 0.029663, acc.: 100.00%] [G loss: 4.650757]\n",
      "epoch:2 step:1665 [D loss: 0.052707, acc.: 100.00%] [G loss: 3.031141]\n",
      "epoch:2 step:1666 [D loss: 0.132152, acc.: 95.31%] [G loss: 5.540168]\n",
      "epoch:2 step:1667 [D loss: 0.208634, acc.: 90.62%] [G loss: 3.425200]\n",
      "epoch:2 step:1668 [D loss: 0.111280, acc.: 96.88%] [G loss: 4.148378]\n",
      "epoch:2 step:1669 [D loss: 0.038334, acc.: 99.22%] [G loss: 4.088674]\n",
      "epoch:2 step:1670 [D loss: 0.126281, acc.: 95.31%] [G loss: 4.780258]\n",
      "epoch:2 step:1671 [D loss: 0.293245, acc.: 90.62%] [G loss: 3.804157]\n",
      "epoch:2 step:1672 [D loss: 0.024061, acc.: 100.00%] [G loss: 2.629332]\n",
      "epoch:2 step:1673 [D loss: 0.162476, acc.: 96.09%] [G loss: 0.083475]\n",
      "epoch:2 step:1674 [D loss: 0.035939, acc.: 100.00%] [G loss: 0.397368]\n",
      "epoch:2 step:1675 [D loss: 0.027803, acc.: 100.00%] [G loss: 0.979550]\n",
      "epoch:2 step:1676 [D loss: 0.085118, acc.: 97.66%] [G loss: 0.298766]\n",
      "epoch:2 step:1677 [D loss: 0.095357, acc.: 99.22%] [G loss: 2.065008]\n",
      "epoch:2 step:1678 [D loss: 0.351636, acc.: 83.59%] [G loss: 0.045926]\n",
      "epoch:2 step:1679 [D loss: 0.026460, acc.: 100.00%] [G loss: 0.195729]\n",
      "epoch:2 step:1680 [D loss: 0.024979, acc.: 100.00%] [G loss: 0.512773]\n",
      "epoch:2 step:1681 [D loss: 0.014314, acc.: 100.00%] [G loss: 0.534792]\n",
      "epoch:2 step:1682 [D loss: 0.023423, acc.: 100.00%] [G loss: 0.717620]\n",
      "epoch:2 step:1683 [D loss: 0.016530, acc.: 100.00%] [G loss: 0.665203]\n",
      "epoch:2 step:1684 [D loss: 0.122391, acc.: 97.66%] [G loss: 3.893925]\n",
      "epoch:2 step:1685 [D loss: 1.094572, acc.: 42.97%] [G loss: 9.653379]\n",
      "epoch:2 step:1686 [D loss: 1.317631, acc.: 50.78%] [G loss: 5.494127]\n",
      "epoch:2 step:1687 [D loss: 0.097901, acc.: 96.88%] [G loss: 3.846371]\n",
      "epoch:2 step:1688 [D loss: 0.068088, acc.: 98.44%] [G loss: 4.549367]\n",
      "epoch:2 step:1689 [D loss: 0.056860, acc.: 98.44%] [G loss: 4.400386]\n",
      "epoch:2 step:1690 [D loss: 0.076599, acc.: 97.66%] [G loss: 4.010342]\n",
      "epoch:2 step:1691 [D loss: 0.037858, acc.: 100.00%] [G loss: 3.707269]\n",
      "epoch:2 step:1692 [D loss: 0.195717, acc.: 92.97%] [G loss: 4.724495]\n",
      "epoch:2 step:1693 [D loss: 0.197461, acc.: 90.62%] [G loss: 3.407697]\n",
      "epoch:2 step:1694 [D loss: 0.258979, acc.: 87.50%] [G loss: 5.833335]\n",
      "epoch:2 step:1695 [D loss: 0.129593, acc.: 96.09%] [G loss: 5.168126]\n",
      "epoch:2 step:1696 [D loss: 0.123548, acc.: 96.09%] [G loss: 3.760419]\n",
      "epoch:2 step:1697 [D loss: 0.088259, acc.: 98.44%] [G loss: 2.884499]\n",
      "epoch:2 step:1698 [D loss: 0.048540, acc.: 99.22%] [G loss: 2.647678]\n",
      "epoch:2 step:1699 [D loss: 0.111443, acc.: 96.09%] [G loss: 3.151471]\n",
      "epoch:2 step:1700 [D loss: 0.625895, acc.: 65.62%] [G loss: 5.507627]\n",
      "epoch:2 step:1701 [D loss: 0.397337, acc.: 78.91%] [G loss: 3.118666]\n",
      "epoch:2 step:1702 [D loss: 0.027621, acc.: 100.00%] [G loss: 2.037015]\n",
      "epoch:2 step:1703 [D loss: 0.247026, acc.: 89.06%] [G loss: 4.602448]\n",
      "epoch:2 step:1704 [D loss: 0.051873, acc.: 100.00%] [G loss: 4.967611]\n",
      "epoch:2 step:1705 [D loss: 0.082990, acc.: 98.44%] [G loss: 2.348457]\n",
      "epoch:2 step:1706 [D loss: 0.185113, acc.: 92.97%] [G loss: 1.920394]\n",
      "epoch:2 step:1707 [D loss: 0.056749, acc.: 99.22%] [G loss: 1.893577]\n",
      "epoch:2 step:1708 [D loss: 0.265696, acc.: 91.41%] [G loss: 3.926851]\n",
      "epoch:2 step:1709 [D loss: 0.209344, acc.: 91.41%] [G loss: 1.909447]\n",
      "epoch:2 step:1710 [D loss: 0.207697, acc.: 92.19%] [G loss: 3.762368]\n",
      "epoch:2 step:1711 [D loss: 0.363468, acc.: 89.84%] [G loss: 1.014607]\n",
      "epoch:2 step:1712 [D loss: 0.166658, acc.: 93.75%] [G loss: 3.943272]\n",
      "epoch:2 step:1713 [D loss: 0.083271, acc.: 97.66%] [G loss: 4.200259]\n",
      "epoch:2 step:1714 [D loss: 0.054208, acc.: 99.22%] [G loss: 3.206287]\n",
      "epoch:2 step:1715 [D loss: 0.090851, acc.: 96.09%] [G loss: 1.284733]\n",
      "epoch:2 step:1716 [D loss: 0.343645, acc.: 84.38%] [G loss: 6.403727]\n",
      "epoch:2 step:1717 [D loss: 0.439155, acc.: 77.34%] [G loss: 1.939536]\n",
      "epoch:2 step:1718 [D loss: 0.388541, acc.: 84.38%] [G loss: 6.032073]\n",
      "epoch:2 step:1719 [D loss: 0.127026, acc.: 96.09%] [G loss: 5.674666]\n",
      "epoch:2 step:1720 [D loss: 0.153279, acc.: 92.19%] [G loss: 3.024248]\n",
      "epoch:2 step:1721 [D loss: 0.099785, acc.: 96.88%] [G loss: 4.183797]\n",
      "epoch:2 step:1722 [D loss: 0.018950, acc.: 100.00%] [G loss: 5.022497]\n",
      "epoch:2 step:1723 [D loss: 0.160186, acc.: 96.88%] [G loss: 3.802907]\n",
      "epoch:2 step:1724 [D loss: 0.057317, acc.: 100.00%] [G loss: 3.964784]\n",
      "epoch:2 step:1725 [D loss: 0.356730, acc.: 84.38%] [G loss: 7.063022]\n",
      "epoch:2 step:1726 [D loss: 0.638879, acc.: 71.09%] [G loss: 2.764604]\n",
      "epoch:2 step:1727 [D loss: 0.196139, acc.: 91.41%] [G loss: 4.959805]\n",
      "epoch:2 step:1728 [D loss: 0.014225, acc.: 100.00%] [G loss: 5.923035]\n",
      "epoch:2 step:1729 [D loss: 0.213027, acc.: 91.41%] [G loss: 3.194165]\n",
      "epoch:2 step:1730 [D loss: 0.192816, acc.: 92.97%] [G loss: 4.644532]\n",
      "epoch:2 step:1731 [D loss: 0.019708, acc.: 100.00%] [G loss: 5.196767]\n",
      "epoch:2 step:1732 [D loss: 0.120575, acc.: 95.31%] [G loss: 4.461943]\n",
      "epoch:2 step:1733 [D loss: 0.112200, acc.: 98.44%] [G loss: 3.834715]\n",
      "epoch:2 step:1734 [D loss: 0.042264, acc.: 99.22%] [G loss: 3.786824]\n",
      "epoch:2 step:1735 [D loss: 0.043763, acc.: 100.00%] [G loss: 2.776833]\n",
      "epoch:2 step:1736 [D loss: 0.310863, acc.: 84.38%] [G loss: 6.379910]\n",
      "epoch:2 step:1737 [D loss: 0.596017, acc.: 70.31%] [G loss: 3.972436]\n",
      "epoch:2 step:1738 [D loss: 0.069065, acc.: 99.22%] [G loss: 4.180175]\n",
      "epoch:2 step:1739 [D loss: 0.020736, acc.: 100.00%] [G loss: 4.784351]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:2 step:1740 [D loss: 0.071720, acc.: 97.66%] [G loss: 3.971215]\n",
      "epoch:2 step:1741 [D loss: 0.078199, acc.: 97.66%] [G loss: 3.821920]\n",
      "epoch:2 step:1742 [D loss: 0.187465, acc.: 96.09%] [G loss: 4.364683]\n",
      "epoch:2 step:1743 [D loss: 0.036124, acc.: 99.22%] [G loss: 4.453996]\n",
      "epoch:2 step:1744 [D loss: 0.373654, acc.: 80.47%] [G loss: 6.067312]\n",
      "epoch:2 step:1745 [D loss: 0.165594, acc.: 92.19%] [G loss: 4.962952]\n",
      "epoch:2 step:1746 [D loss: 0.106553, acc.: 97.66%] [G loss: 2.657126]\n",
      "epoch:2 step:1747 [D loss: 0.173112, acc.: 91.41%] [G loss: 5.843481]\n",
      "epoch:2 step:1748 [D loss: 0.189944, acc.: 92.97%] [G loss: 3.778165]\n",
      "epoch:2 step:1749 [D loss: 0.347333, acc.: 85.94%] [G loss: 5.693215]\n",
      "epoch:2 step:1750 [D loss: 0.391432, acc.: 80.47%] [G loss: 3.240924]\n",
      "epoch:2 step:1751 [D loss: 0.160142, acc.: 94.53%] [G loss: 4.681160]\n",
      "epoch:2 step:1752 [D loss: 0.044735, acc.: 99.22%] [G loss: 4.220358]\n",
      "epoch:2 step:1753 [D loss: 0.184590, acc.: 93.75%] [G loss: 4.135250]\n",
      "epoch:2 step:1754 [D loss: 0.229475, acc.: 91.41%] [G loss: 4.505005]\n",
      "epoch:2 step:1755 [D loss: 0.148940, acc.: 94.53%] [G loss: 3.776516]\n",
      "epoch:2 step:1756 [D loss: 0.430608, acc.: 79.69%] [G loss: 6.591663]\n",
      "epoch:2 step:1757 [D loss: 1.184921, acc.: 43.75%] [G loss: 5.984803]\n",
      "epoch:2 step:1758 [D loss: 0.028177, acc.: 99.22%] [G loss: 5.807311]\n",
      "epoch:2 step:1759 [D loss: 0.062048, acc.: 100.00%] [G loss: 4.347773]\n",
      "epoch:2 step:1760 [D loss: 0.096044, acc.: 97.66%] [G loss: 3.123219]\n",
      "epoch:2 step:1761 [D loss: 0.414241, acc.: 79.69%] [G loss: 7.225862]\n",
      "epoch:2 step:1762 [D loss: 0.682762, acc.: 69.53%] [G loss: 3.949028]\n",
      "epoch:2 step:1763 [D loss: 0.234333, acc.: 89.06%] [G loss: 5.011195]\n",
      "epoch:2 step:1764 [D loss: 0.102471, acc.: 96.88%] [G loss: 5.074296]\n",
      "epoch:2 step:1765 [D loss: 0.106146, acc.: 97.66%] [G loss: 4.420089]\n",
      "epoch:2 step:1766 [D loss: 0.084733, acc.: 99.22%] [G loss: 4.644310]\n",
      "epoch:2 step:1767 [D loss: 0.319815, acc.: 85.16%] [G loss: 6.743451]\n",
      "epoch:2 step:1768 [D loss: 0.264014, acc.: 85.16%] [G loss: 4.979993]\n",
      "epoch:2 step:1769 [D loss: 0.126794, acc.: 95.31%] [G loss: 4.978757]\n",
      "epoch:2 step:1770 [D loss: 0.108800, acc.: 96.88%] [G loss: 5.131056]\n",
      "epoch:2 step:1771 [D loss: 0.142255, acc.: 92.97%] [G loss: 3.688734]\n",
      "epoch:2 step:1772 [D loss: 0.174584, acc.: 96.88%] [G loss: 5.265802]\n",
      "epoch:2 step:1773 [D loss: 0.055596, acc.: 99.22%] [G loss: 4.164739]\n",
      "epoch:2 step:1774 [D loss: 0.247220, acc.: 91.41%] [G loss: 5.222663]\n",
      "epoch:2 step:1775 [D loss: 0.179587, acc.: 93.75%] [G loss: 3.386392]\n",
      "epoch:2 step:1776 [D loss: 0.166327, acc.: 92.97%] [G loss: 5.067227]\n",
      "epoch:2 step:1777 [D loss: 0.382702, acc.: 83.59%] [G loss: 5.298631]\n",
      "epoch:2 step:1778 [D loss: 0.163976, acc.: 96.88%] [G loss: 4.878344]\n",
      "epoch:2 step:1779 [D loss: 0.147009, acc.: 95.31%] [G loss: 5.164457]\n",
      "epoch:2 step:1780 [D loss: 0.772118, acc.: 62.50%] [G loss: 8.407478]\n",
      "epoch:2 step:1781 [D loss: 0.594001, acc.: 76.56%] [G loss: 5.027228]\n",
      "epoch:2 step:1782 [D loss: 0.114055, acc.: 96.88%] [G loss: 4.939344]\n",
      "epoch:2 step:1783 [D loss: 0.030464, acc.: 100.00%] [G loss: 4.422025]\n",
      "epoch:2 step:1784 [D loss: 0.166179, acc.: 92.97%] [G loss: 4.642137]\n",
      "epoch:2 step:1785 [D loss: 0.098391, acc.: 98.44%] [G loss: 4.122371]\n",
      "epoch:2 step:1786 [D loss: 0.090698, acc.: 96.88%] [G loss: 3.645180]\n",
      "epoch:2 step:1787 [D loss: 0.082702, acc.: 100.00%] [G loss: 3.407328]\n",
      "epoch:2 step:1788 [D loss: 0.293922, acc.: 87.50%] [G loss: 4.604740]\n",
      "epoch:2 step:1789 [D loss: 0.087331, acc.: 96.88%] [G loss: 4.455049]\n",
      "epoch:2 step:1790 [D loss: 0.227599, acc.: 92.97%] [G loss: 5.101142]\n",
      "epoch:2 step:1791 [D loss: 0.146766, acc.: 93.75%] [G loss: 2.832153]\n",
      "epoch:2 step:1792 [D loss: 0.032852, acc.: 100.00%] [G loss: 2.060637]\n",
      "epoch:2 step:1793 [D loss: 0.150166, acc.: 94.53%] [G loss: 4.786238]\n",
      "epoch:2 step:1794 [D loss: 0.434499, acc.: 78.12%] [G loss: 2.829214]\n",
      "epoch:2 step:1795 [D loss: 0.042755, acc.: 99.22%] [G loss: 2.895221]\n",
      "epoch:2 step:1796 [D loss: 0.182171, acc.: 93.75%] [G loss: 3.846007]\n",
      "epoch:2 step:1797 [D loss: 0.053159, acc.: 98.44%] [G loss: 2.852345]\n",
      "epoch:2 step:1798 [D loss: 0.071166, acc.: 98.44%] [G loss: 1.332981]\n",
      "epoch:2 step:1799 [D loss: 0.224769, acc.: 90.62%] [G loss: 3.272691]\n",
      "epoch:2 step:1800 [D loss: 0.311100, acc.: 85.16%] [G loss: 4.644467]\n",
      "##############\n",
      "[1.06443744 1.10778233 0.9592127  0.97218947 1.11922812 2.1146999\n",
      " 0.97263312 2.11253265 1.10392812 1.11685766]\n",
      "##########\n",
      "epoch:2 step:1801 [D loss: 0.275712, acc.: 88.28%] [G loss: 2.099401]\n",
      "epoch:2 step:1802 [D loss: 0.394981, acc.: 80.47%] [G loss: 6.858733]\n",
      "epoch:2 step:1803 [D loss: 1.012173, acc.: 64.84%] [G loss: 2.630171]\n",
      "epoch:2 step:1804 [D loss: 0.091712, acc.: 97.66%] [G loss: 3.681338]\n",
      "epoch:2 step:1805 [D loss: 0.077143, acc.: 97.66%] [G loss: 4.337586]\n",
      "epoch:2 step:1806 [D loss: 0.127747, acc.: 97.66%] [G loss: 4.487115]\n",
      "epoch:2 step:1807 [D loss: 0.078627, acc.: 99.22%] [G loss: 4.357464]\n",
      "epoch:2 step:1808 [D loss: 0.114634, acc.: 97.66%] [G loss: 4.241882]\n",
      "epoch:2 step:1809 [D loss: 0.224576, acc.: 92.19%] [G loss: 5.550138]\n",
      "epoch:2 step:1810 [D loss: 0.176894, acc.: 92.19%] [G loss: 4.423225]\n",
      "epoch:2 step:1811 [D loss: 0.318198, acc.: 86.72%] [G loss: 6.549158]\n",
      "epoch:2 step:1812 [D loss: 0.500760, acc.: 75.78%] [G loss: 5.238269]\n",
      "epoch:2 step:1813 [D loss: 0.049984, acc.: 99.22%] [G loss: 5.461287]\n",
      "epoch:2 step:1814 [D loss: 0.024319, acc.: 99.22%] [G loss: 4.623341]\n",
      "epoch:2 step:1815 [D loss: 0.047587, acc.: 99.22%] [G loss: 4.465525]\n",
      "epoch:2 step:1816 [D loss: 0.078041, acc.: 97.66%] [G loss: 4.739807]\n",
      "epoch:2 step:1817 [D loss: 0.231073, acc.: 91.41%] [G loss: 6.773741]\n",
      "epoch:2 step:1818 [D loss: 0.248496, acc.: 89.06%] [G loss: 4.778184]\n",
      "epoch:2 step:1819 [D loss: 0.139134, acc.: 96.09%] [G loss: 4.738493]\n",
      "epoch:2 step:1820 [D loss: 0.032509, acc.: 99.22%] [G loss: 4.302081]\n",
      "epoch:2 step:1821 [D loss: 0.027446, acc.: 100.00%] [G loss: 3.170053]\n",
      "epoch:2 step:1822 [D loss: 0.236404, acc.: 89.84%] [G loss: 6.057808]\n",
      "epoch:2 step:1823 [D loss: 0.913601, acc.: 58.59%] [G loss: 3.116808]\n",
      "epoch:2 step:1824 [D loss: 0.017596, acc.: 100.00%] [G loss: 3.966736]\n",
      "epoch:2 step:1825 [D loss: 0.037825, acc.: 99.22%] [G loss: 2.578615]\n",
      "epoch:2 step:1826 [D loss: 0.166658, acc.: 92.97%] [G loss: 4.066470]\n",
      "epoch:2 step:1827 [D loss: 0.256492, acc.: 89.84%] [G loss: 1.468200]\n",
      "epoch:2 step:1828 [D loss: 0.105326, acc.: 96.88%] [G loss: 1.527316]\n",
      "epoch:2 step:1829 [D loss: 0.054510, acc.: 100.00%] [G loss: 1.336876]\n",
      "epoch:2 step:1830 [D loss: 0.068425, acc.: 98.44%] [G loss: 0.595333]\n",
      "epoch:2 step:1831 [D loss: 0.104158, acc.: 97.66%] [G loss: 0.481879]\n",
      "epoch:2 step:1832 [D loss: 0.029520, acc.: 100.00%] [G loss: 0.303641]\n",
      "epoch:2 step:1833 [D loss: 0.050804, acc.: 100.00%] [G loss: 0.239257]\n",
      "epoch:2 step:1834 [D loss: 0.045196, acc.: 100.00%] [G loss: 0.778069]\n",
      "epoch:2 step:1835 [D loss: 0.072053, acc.: 98.44%] [G loss: 0.250645]\n",
      "epoch:2 step:1836 [D loss: 0.090181, acc.: 97.66%] [G loss: 0.943866]\n",
      "epoch:2 step:1837 [D loss: 0.303353, acc.: 85.94%] [G loss: 2.961735]\n",
      "epoch:2 step:1838 [D loss: 0.091194, acc.: 96.09%] [G loss: 2.087556]\n",
      "epoch:2 step:1839 [D loss: 1.244259, acc.: 50.78%] [G loss: 8.155827]\n",
      "epoch:2 step:1840 [D loss: 0.780204, acc.: 64.06%] [G loss: 5.495272]\n",
      "epoch:2 step:1841 [D loss: 0.027419, acc.: 100.00%] [G loss: 3.052648]\n",
      "epoch:2 step:1842 [D loss: 0.147526, acc.: 93.75%] [G loss: 4.188671]\n",
      "epoch:2 step:1843 [D loss: 0.056352, acc.: 97.66%] [G loss: 3.673237]\n",
      "epoch:2 step:1844 [D loss: 0.268230, acc.: 90.62%] [G loss: 3.494705]\n",
      "epoch:2 step:1845 [D loss: 0.135679, acc.: 93.75%] [G loss: 3.331756]\n",
      "epoch:2 step:1846 [D loss: 0.233157, acc.: 92.19%] [G loss: 2.861831]\n",
      "epoch:2 step:1847 [D loss: 0.240310, acc.: 90.62%] [G loss: 4.219288]\n",
      "epoch:2 step:1848 [D loss: 0.607189, acc.: 68.75%] [G loss: 3.885101]\n",
      "epoch:2 step:1849 [D loss: 0.202170, acc.: 93.75%] [G loss: 5.598635]\n",
      "epoch:2 step:1850 [D loss: 0.245794, acc.: 92.97%] [G loss: 4.536379]\n",
      "epoch:2 step:1851 [D loss: 0.254499, acc.: 90.62%] [G loss: 3.978622]\n",
      "epoch:2 step:1852 [D loss: 0.212542, acc.: 87.50%] [G loss: 5.563119]\n",
      "epoch:2 step:1853 [D loss: 0.170339, acc.: 91.41%] [G loss: 5.251103]\n",
      "epoch:2 step:1854 [D loss: 0.378336, acc.: 84.38%] [G loss: 4.192950]\n",
      "epoch:2 step:1855 [D loss: 0.131049, acc.: 95.31%] [G loss: 6.355186]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:2 step:1856 [D loss: 0.066359, acc.: 97.66%] [G loss: 5.820323]\n",
      "epoch:2 step:1857 [D loss: 0.131726, acc.: 96.09%] [G loss: 3.804782]\n",
      "epoch:2 step:1858 [D loss: 0.230940, acc.: 89.84%] [G loss: 6.943069]\n",
      "epoch:2 step:1859 [D loss: 0.112041, acc.: 94.53%] [G loss: 6.646440]\n",
      "epoch:2 step:1860 [D loss: 0.112340, acc.: 96.09%] [G loss: 5.185596]\n",
      "epoch:2 step:1861 [D loss: 0.077292, acc.: 98.44%] [G loss: 3.973032]\n",
      "epoch:2 step:1862 [D loss: 0.060652, acc.: 99.22%] [G loss: 3.942963]\n",
      "epoch:2 step:1863 [D loss: 0.078771, acc.: 99.22%] [G loss: 4.370595]\n",
      "epoch:2 step:1864 [D loss: 0.387046, acc.: 82.03%] [G loss: 4.607073]\n",
      "epoch:2 step:1865 [D loss: 0.244656, acc.: 89.06%] [G loss: 1.496443]\n",
      "epoch:2 step:1866 [D loss: 0.069617, acc.: 99.22%] [G loss: 1.255499]\n",
      "epoch:2 step:1867 [D loss: 0.029104, acc.: 100.00%] [G loss: 0.930452]\n",
      "epoch:2 step:1868 [D loss: 0.059678, acc.: 98.44%] [G loss: 0.544881]\n",
      "epoch:2 step:1869 [D loss: 0.167362, acc.: 94.53%] [G loss: 1.621581]\n",
      "epoch:2 step:1870 [D loss: 0.028684, acc.: 100.00%] [G loss: 2.496634]\n",
      "epoch:2 step:1871 [D loss: 0.719070, acc.: 67.97%] [G loss: 2.031905]\n",
      "epoch:2 step:1872 [D loss: 0.097009, acc.: 95.31%] [G loss: 2.087268]\n",
      "epoch:2 step:1873 [D loss: 0.039166, acc.: 100.00%] [G loss: 0.952518]\n",
      "epoch:2 step:1874 [D loss: 0.262273, acc.: 91.41%] [G loss: 5.255264]\n",
      "epoch:2 step:1875 [D loss: 0.214544, acc.: 91.41%] [G loss: 2.886676]\n",
      "epoch:2 step:1876 [D loss: 0.426544, acc.: 80.47%] [G loss: 1.121532]\n",
      "epoch:2 step:1877 [D loss: 0.019534, acc.: 100.00%] [G loss: 2.092024]\n",
      "epoch:2 step:1878 [D loss: 0.194072, acc.: 90.62%] [G loss: 1.294916]\n",
      "epoch:2 step:1879 [D loss: 0.054884, acc.: 99.22%] [G loss: 0.801520]\n",
      "epoch:2 step:1880 [D loss: 0.056362, acc.: 99.22%] [G loss: 0.530079]\n",
      "epoch:2 step:1881 [D loss: 0.288206, acc.: 85.94%] [G loss: 6.703306]\n",
      "epoch:2 step:1882 [D loss: 0.788026, acc.: 68.75%] [G loss: 2.767360]\n",
      "epoch:2 step:1883 [D loss: 0.083749, acc.: 96.88%] [G loss: 3.289613]\n",
      "epoch:2 step:1884 [D loss: 0.110223, acc.: 96.88%] [G loss: 4.327202]\n",
      "epoch:2 step:1885 [D loss: 0.043845, acc.: 98.44%] [G loss: 4.582616]\n",
      "epoch:2 step:1886 [D loss: 0.466149, acc.: 78.91%] [G loss: 5.368749]\n",
      "epoch:2 step:1887 [D loss: 0.182666, acc.: 94.53%] [G loss: 6.798745]\n",
      "epoch:2 step:1888 [D loss: 0.120952, acc.: 95.31%] [G loss: 5.449191]\n",
      "epoch:2 step:1889 [D loss: 0.386810, acc.: 82.81%] [G loss: 6.621772]\n",
      "epoch:2 step:1890 [D loss: 0.223746, acc.: 88.28%] [G loss: 5.510004]\n",
      "epoch:2 step:1891 [D loss: 0.094571, acc.: 96.88%] [G loss: 4.814238]\n",
      "epoch:2 step:1892 [D loss: 0.045525, acc.: 99.22%] [G loss: 4.664530]\n",
      "epoch:2 step:1893 [D loss: 0.074634, acc.: 97.66%] [G loss: 4.884577]\n",
      "epoch:2 step:1894 [D loss: 0.119502, acc.: 96.09%] [G loss: 5.954908]\n",
      "epoch:2 step:1895 [D loss: 0.069762, acc.: 98.44%] [G loss: 5.342212]\n",
      "epoch:2 step:1896 [D loss: 0.070122, acc.: 99.22%] [G loss: 4.635657]\n",
      "epoch:2 step:1897 [D loss: 0.107834, acc.: 97.66%] [G loss: 5.197282]\n",
      "epoch:2 step:1898 [D loss: 0.081386, acc.: 98.44%] [G loss: 3.491334]\n",
      "epoch:2 step:1899 [D loss: 0.064221, acc.: 99.22%] [G loss: 2.506874]\n",
      "epoch:2 step:1900 [D loss: 0.069750, acc.: 98.44%] [G loss: 2.458957]\n",
      "epoch:2 step:1901 [D loss: 0.027035, acc.: 99.22%] [G loss: 1.476586]\n",
      "epoch:2 step:1902 [D loss: 0.262496, acc.: 88.28%] [G loss: 3.619344]\n",
      "epoch:2 step:1903 [D loss: 0.062941, acc.: 96.09%] [G loss: 3.419534]\n",
      "epoch:2 step:1904 [D loss: 0.366701, acc.: 82.81%] [G loss: 1.331734]\n",
      "epoch:2 step:1905 [D loss: 0.012353, acc.: 100.00%] [G loss: 1.433559]\n",
      "epoch:2 step:1906 [D loss: 0.052141, acc.: 99.22%] [G loss: 1.206734]\n",
      "epoch:2 step:1907 [D loss: 0.013567, acc.: 100.00%] [G loss: 0.932736]\n",
      "epoch:2 step:1908 [D loss: 0.053799, acc.: 100.00%] [G loss: 0.759780]\n",
      "epoch:2 step:1909 [D loss: 0.081298, acc.: 98.44%] [G loss: 0.901233]\n",
      "epoch:2 step:1910 [D loss: 0.084821, acc.: 98.44%] [G loss: 0.880215]\n",
      "epoch:2 step:1911 [D loss: 0.179685, acc.: 92.19%] [G loss: 4.709421]\n",
      "epoch:2 step:1912 [D loss: 0.746197, acc.: 66.41%] [G loss: 5.686873]\n",
      "epoch:2 step:1913 [D loss: 0.026115, acc.: 100.00%] [G loss: 5.965921]\n",
      "epoch:2 step:1914 [D loss: 0.122447, acc.: 95.31%] [G loss: 4.034408]\n",
      "epoch:2 step:1915 [D loss: 0.115170, acc.: 96.09%] [G loss: 4.239500]\n",
      "epoch:2 step:1916 [D loss: 0.076089, acc.: 99.22%] [G loss: 3.583104]\n",
      "epoch:2 step:1917 [D loss: 0.309951, acc.: 88.28%] [G loss: 7.446700]\n",
      "epoch:2 step:1918 [D loss: 0.597569, acc.: 76.56%] [G loss: 3.956949]\n",
      "epoch:2 step:1919 [D loss: 0.112878, acc.: 96.09%] [G loss: 4.815460]\n",
      "epoch:2 step:1920 [D loss: 0.013408, acc.: 100.00%] [G loss: 5.150870]\n",
      "epoch:2 step:1921 [D loss: 0.118702, acc.: 96.88%] [G loss: 3.920862]\n",
      "epoch:2 step:1922 [D loss: 0.245624, acc.: 90.62%] [G loss: 7.168671]\n",
      "epoch:2 step:1923 [D loss: 0.440316, acc.: 78.12%] [G loss: 4.279296]\n",
      "epoch:2 step:1924 [D loss: 0.265756, acc.: 89.06%] [G loss: 7.334659]\n",
      "epoch:2 step:1925 [D loss: 0.092447, acc.: 96.09%] [G loss: 6.274542]\n",
      "epoch:2 step:1926 [D loss: 0.111284, acc.: 95.31%] [G loss: 4.775713]\n",
      "epoch:2 step:1927 [D loss: 0.098998, acc.: 97.66%] [G loss: 3.408927]\n",
      "epoch:2 step:1928 [D loss: 0.324226, acc.: 86.72%] [G loss: 6.616886]\n",
      "epoch:2 step:1929 [D loss: 0.252522, acc.: 89.06%] [G loss: 5.284868]\n",
      "epoch:2 step:1930 [D loss: 0.022483, acc.: 100.00%] [G loss: 4.581676]\n",
      "epoch:2 step:1931 [D loss: 0.065171, acc.: 98.44%] [G loss: 3.367704]\n",
      "epoch:2 step:1932 [D loss: 0.361684, acc.: 83.59%] [G loss: 4.220730]\n",
      "epoch:2 step:1933 [D loss: 0.319481, acc.: 87.50%] [G loss: 4.876493]\n",
      "epoch:2 step:1934 [D loss: 0.209811, acc.: 91.41%] [G loss: 1.374097]\n",
      "epoch:2 step:1935 [D loss: 0.174968, acc.: 92.97%] [G loss: 3.076906]\n",
      "epoch:2 step:1936 [D loss: 0.021955, acc.: 99.22%] [G loss: 4.929599]\n",
      "epoch:2 step:1937 [D loss: 0.067176, acc.: 96.09%] [G loss: 0.589038]\n",
      "epoch:2 step:1938 [D loss: 0.111676, acc.: 99.22%] [G loss: 1.155232]\n",
      "epoch:2 step:1939 [D loss: 0.122637, acc.: 96.09%] [G loss: 0.581743]\n",
      "epoch:2 step:1940 [D loss: 0.102962, acc.: 96.09%] [G loss: 2.890327]\n",
      "epoch:2 step:1941 [D loss: 0.253466, acc.: 88.28%] [G loss: 1.647629]\n",
      "epoch:2 step:1942 [D loss: 0.115399, acc.: 96.88%] [G loss: 4.297735]\n",
      "epoch:2 step:1943 [D loss: 0.484766, acc.: 75.78%] [G loss: 4.578565]\n",
      "epoch:2 step:1944 [D loss: 0.192851, acc.: 91.41%] [G loss: 4.137307]\n",
      "epoch:2 step:1945 [D loss: 0.993887, acc.: 59.38%] [G loss: 12.456359]\n",
      "epoch:2 step:1946 [D loss: 1.836489, acc.: 53.91%] [G loss: 4.153969]\n",
      "epoch:2 step:1947 [D loss: 0.152360, acc.: 96.88%] [G loss: 6.400479]\n",
      "epoch:2 step:1948 [D loss: 0.050218, acc.: 98.44%] [G loss: 6.046349]\n",
      "epoch:2 step:1949 [D loss: 0.149702, acc.: 93.75%] [G loss: 4.619829]\n",
      "epoch:2 step:1950 [D loss: 0.245878, acc.: 88.28%] [G loss: 5.901185]\n",
      "epoch:2 step:1951 [D loss: 0.128188, acc.: 93.75%] [G loss: 5.651713]\n",
      "epoch:2 step:1952 [D loss: 0.108078, acc.: 96.09%] [G loss: 4.705611]\n",
      "epoch:2 step:1953 [D loss: 0.427459, acc.: 80.47%] [G loss: 6.368284]\n",
      "epoch:2 step:1954 [D loss: 0.157361, acc.: 94.53%] [G loss: 6.575843]\n",
      "epoch:2 step:1955 [D loss: 0.233046, acc.: 89.06%] [G loss: 1.950556]\n",
      "epoch:2 step:1956 [D loss: 0.043729, acc.: 100.00%] [G loss: 1.306298]\n",
      "epoch:2 step:1957 [D loss: 0.210826, acc.: 92.97%] [G loss: 5.253058]\n",
      "epoch:2 step:1958 [D loss: 0.209538, acc.: 90.62%] [G loss: 2.798658]\n",
      "epoch:2 step:1959 [D loss: 0.038715, acc.: 100.00%] [G loss: 1.232001]\n",
      "epoch:2 step:1960 [D loss: 0.037182, acc.: 99.22%] [G loss: 0.497927]\n",
      "epoch:2 step:1961 [D loss: 0.086435, acc.: 96.09%] [G loss: 1.521843]\n",
      "epoch:2 step:1962 [D loss: 0.018527, acc.: 100.00%] [G loss: 1.506051]\n",
      "epoch:2 step:1963 [D loss: 0.033666, acc.: 99.22%] [G loss: 0.150635]\n",
      "epoch:2 step:1964 [D loss: 0.109520, acc.: 98.44%] [G loss: 0.825861]\n",
      "epoch:2 step:1965 [D loss: 0.061460, acc.: 98.44%] [G loss: 0.501001]\n",
      "epoch:2 step:1966 [D loss: 0.259557, acc.: 90.62%] [G loss: 0.588422]\n",
      "epoch:2 step:1967 [D loss: 0.109324, acc.: 96.09%] [G loss: 1.051259]\n",
      "epoch:2 step:1968 [D loss: 0.067183, acc.: 97.66%] [G loss: 1.476071]\n",
      "epoch:2 step:1969 [D loss: 0.053826, acc.: 98.44%] [G loss: 0.925877]\n",
      "epoch:2 step:1970 [D loss: 0.041504, acc.: 99.22%] [G loss: 0.255956]\n",
      "epoch:2 step:1971 [D loss: 0.066006, acc.: 99.22%] [G loss: 1.089878]\n",
      "epoch:2 step:1972 [D loss: 0.188737, acc.: 92.97%] [G loss: 4.563052]\n",
      "epoch:2 step:1973 [D loss: 1.799976, acc.: 27.34%] [G loss: 7.270211]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:2 step:1974 [D loss: 0.050972, acc.: 98.44%] [G loss: 8.371497]\n",
      "epoch:2 step:1975 [D loss: 0.252595, acc.: 89.06%] [G loss: 6.208777]\n",
      "epoch:2 step:1976 [D loss: 0.010851, acc.: 100.00%] [G loss: 4.657445]\n",
      "epoch:2 step:1977 [D loss: 0.180766, acc.: 89.84%] [G loss: 4.925597]\n",
      "epoch:2 step:1978 [D loss: 0.024750, acc.: 100.00%] [G loss: 4.952932]\n",
      "epoch:2 step:1979 [D loss: 0.211016, acc.: 90.62%] [G loss: 4.655065]\n",
      "epoch:2 step:1980 [D loss: 0.154582, acc.: 94.53%] [G loss: 4.854660]\n",
      "epoch:2 step:1981 [D loss: 0.304600, acc.: 90.62%] [G loss: 5.671929]\n",
      "epoch:2 step:1982 [D loss: 0.464628, acc.: 81.25%] [G loss: 3.224206]\n",
      "epoch:2 step:1983 [D loss: 0.081652, acc.: 97.66%] [G loss: 3.130819]\n",
      "epoch:2 step:1984 [D loss: 0.072137, acc.: 98.44%] [G loss: 2.289915]\n",
      "epoch:2 step:1985 [D loss: 0.123483, acc.: 96.09%] [G loss: 2.711664]\n",
      "epoch:2 step:1986 [D loss: 0.103231, acc.: 96.88%] [G loss: 1.492432]\n",
      "epoch:2 step:1987 [D loss: 0.143824, acc.: 94.53%] [G loss: 4.065094]\n",
      "epoch:2 step:1988 [D loss: 0.282117, acc.: 89.84%] [G loss: 0.465273]\n",
      "epoch:2 step:1989 [D loss: 0.081113, acc.: 99.22%] [G loss: 1.107235]\n",
      "epoch:2 step:1990 [D loss: 0.017647, acc.: 100.00%] [G loss: 1.377097]\n",
      "epoch:2 step:1991 [D loss: 0.021311, acc.: 100.00%] [G loss: 0.669462]\n",
      "epoch:2 step:1992 [D loss: 0.023640, acc.: 100.00%] [G loss: 0.164892]\n",
      "epoch:2 step:1993 [D loss: 0.053293, acc.: 99.22%] [G loss: 0.177766]\n",
      "epoch:2 step:1994 [D loss: 0.065246, acc.: 98.44%] [G loss: 0.828169]\n",
      "epoch:2 step:1995 [D loss: 0.084691, acc.: 96.09%] [G loss: 0.700129]\n",
      "epoch:2 step:1996 [D loss: 0.710069, acc.: 62.50%] [G loss: 8.763369]\n",
      "epoch:2 step:1997 [D loss: 0.908167, acc.: 68.75%] [G loss: 3.811136]\n",
      "epoch:2 step:1998 [D loss: 0.186124, acc.: 93.75%] [G loss: 3.023805]\n",
      "epoch:2 step:1999 [D loss: 0.111725, acc.: 95.31%] [G loss: 2.525755]\n",
      "epoch:2 step:2000 [D loss: 0.065101, acc.: 97.66%] [G loss: 1.275221]\n",
      "##############\n",
      "[0.96484662 0.83565476 0.97735893 1.09983938 2.11978638 0.94617907\n",
      " 2.09913473 0.81031505 0.73805017 0.91182563]\n",
      "##########\n",
      "epoch:2 step:2001 [D loss: 0.082278, acc.: 98.44%] [G loss: 1.017646]\n",
      "epoch:2 step:2002 [D loss: 0.267468, acc.: 89.06%] [G loss: 4.749559]\n",
      "epoch:2 step:2003 [D loss: 0.702516, acc.: 67.97%] [G loss: 4.572947]\n",
      "epoch:2 step:2004 [D loss: 0.079962, acc.: 98.44%] [G loss: 3.771619]\n",
      "epoch:2 step:2005 [D loss: 0.232456, acc.: 92.19%] [G loss: 4.245702]\n",
      "epoch:2 step:2006 [D loss: 0.091430, acc.: 97.66%] [G loss: 3.865906]\n",
      "epoch:2 step:2007 [D loss: 0.105979, acc.: 95.31%] [G loss: 3.586635]\n",
      "epoch:2 step:2008 [D loss: 0.558933, acc.: 74.22%] [G loss: 6.254430]\n",
      "epoch:2 step:2009 [D loss: 0.429045, acc.: 81.25%] [G loss: 3.900405]\n",
      "epoch:2 step:2010 [D loss: 0.126140, acc.: 97.66%] [G loss: 4.691055]\n",
      "epoch:2 step:2011 [D loss: 0.114363, acc.: 96.88%] [G loss: 4.432054]\n",
      "epoch:2 step:2012 [D loss: 0.050466, acc.: 100.00%] [G loss: 4.064414]\n",
      "epoch:2 step:2013 [D loss: 0.114330, acc.: 96.88%] [G loss: 3.212647]\n",
      "epoch:2 step:2014 [D loss: 0.101441, acc.: 98.44%] [G loss: 3.144656]\n",
      "epoch:2 step:2015 [D loss: 0.156498, acc.: 95.31%] [G loss: 3.641016]\n",
      "epoch:2 step:2016 [D loss: 0.137195, acc.: 96.09%] [G loss: 2.950219]\n",
      "epoch:2 step:2017 [D loss: 0.213680, acc.: 90.62%] [G loss: 5.343546]\n",
      "epoch:2 step:2018 [D loss: 0.203350, acc.: 90.62%] [G loss: 2.522377]\n",
      "epoch:2 step:2019 [D loss: 0.068456, acc.: 97.66%] [G loss: 1.386618]\n",
      "epoch:2 step:2020 [D loss: 0.145654, acc.: 95.31%] [G loss: 4.667948]\n",
      "epoch:2 step:2021 [D loss: 0.122150, acc.: 96.88%] [G loss: 2.890750]\n",
      "epoch:2 step:2022 [D loss: 0.229497, acc.: 92.97%] [G loss: 0.452583]\n",
      "epoch:2 step:2023 [D loss: 0.772454, acc.: 66.41%] [G loss: 9.838638]\n",
      "epoch:2 step:2024 [D loss: 1.727738, acc.: 51.56%] [G loss: 4.455700]\n",
      "epoch:2 step:2025 [D loss: 0.101406, acc.: 97.66%] [G loss: 3.241286]\n",
      "epoch:2 step:2026 [D loss: 0.071784, acc.: 99.22%] [G loss: 2.646781]\n",
      "epoch:2 step:2027 [D loss: 0.138544, acc.: 96.09%] [G loss: 3.545113]\n",
      "epoch:2 step:2028 [D loss: 0.156549, acc.: 95.31%] [G loss: 3.391034]\n",
      "epoch:2 step:2029 [D loss: 0.102747, acc.: 97.66%] [G loss: 3.175469]\n",
      "epoch:2 step:2030 [D loss: 0.093836, acc.: 100.00%] [G loss: 2.797473]\n",
      "epoch:2 step:2031 [D loss: 0.191917, acc.: 93.75%] [G loss: 2.447690]\n",
      "epoch:2 step:2032 [D loss: 0.235557, acc.: 91.41%] [G loss: 2.994010]\n",
      "epoch:2 step:2033 [D loss: 0.165028, acc.: 95.31%] [G loss: 2.556272]\n",
      "epoch:2 step:2034 [D loss: 0.285607, acc.: 85.94%] [G loss: 3.044539]\n",
      "epoch:2 step:2035 [D loss: 0.221557, acc.: 92.19%] [G loss: 2.552126]\n",
      "epoch:2 step:2036 [D loss: 0.355684, acc.: 82.81%] [G loss: 3.760804]\n",
      "epoch:2 step:2037 [D loss: 0.049797, acc.: 100.00%] [G loss: 4.213169]\n",
      "epoch:2 step:2038 [D loss: 0.107999, acc.: 98.44%] [G loss: 3.343670]\n",
      "epoch:2 step:2039 [D loss: 0.084238, acc.: 98.44%] [G loss: 3.070372]\n",
      "epoch:2 step:2040 [D loss: 0.168131, acc.: 96.09%] [G loss: 3.249402]\n",
      "epoch:2 step:2041 [D loss: 0.232094, acc.: 91.41%] [G loss: 5.305750]\n",
      "epoch:2 step:2042 [D loss: 0.320548, acc.: 82.81%] [G loss: 3.486001]\n",
      "epoch:2 step:2043 [D loss: 0.645605, acc.: 71.88%] [G loss: 6.749352]\n",
      "epoch:2 step:2044 [D loss: 0.379314, acc.: 78.12%] [G loss: 5.712333]\n",
      "epoch:2 step:2045 [D loss: 0.218670, acc.: 92.19%] [G loss: 3.725000]\n",
      "epoch:2 step:2046 [D loss: 0.209341, acc.: 89.84%] [G loss: 5.157387]\n",
      "epoch:2 step:2047 [D loss: 0.075506, acc.: 97.66%] [G loss: 5.319255]\n",
      "epoch:2 step:2048 [D loss: 0.068066, acc.: 98.44%] [G loss: 4.846794]\n",
      "epoch:2 step:2049 [D loss: 0.095893, acc.: 98.44%] [G loss: 4.592863]\n",
      "epoch:2 step:2050 [D loss: 0.196640, acc.: 92.19%] [G loss: 4.039906]\n",
      "epoch:2 step:2051 [D loss: 0.314881, acc.: 89.84%] [G loss: 4.144586]\n",
      "epoch:2 step:2052 [D loss: 0.104533, acc.: 97.66%] [G loss: 4.296871]\n",
      "epoch:2 step:2053 [D loss: 0.194855, acc.: 94.53%] [G loss: 3.269624]\n",
      "epoch:2 step:2054 [D loss: 0.223743, acc.: 92.97%] [G loss: 3.864341]\n",
      "epoch:2 step:2055 [D loss: 0.184764, acc.: 92.97%] [G loss: 4.290908]\n",
      "epoch:2 step:2056 [D loss: 0.217031, acc.: 94.53%] [G loss: 3.946744]\n",
      "epoch:2 step:2057 [D loss: 0.083484, acc.: 97.66%] [G loss: 2.082784]\n",
      "epoch:2 step:2058 [D loss: 0.207485, acc.: 92.19%] [G loss: 3.602335]\n",
      "epoch:2 step:2059 [D loss: 0.097025, acc.: 96.09%] [G loss: 3.316171]\n",
      "epoch:2 step:2060 [D loss: 0.150434, acc.: 96.88%] [G loss: 2.570641]\n",
      "epoch:2 step:2061 [D loss: 0.209193, acc.: 89.06%] [G loss: 5.516011]\n",
      "epoch:2 step:2062 [D loss: 1.891438, acc.: 31.25%] [G loss: 5.446327]\n",
      "epoch:2 step:2063 [D loss: 0.154064, acc.: 92.97%] [G loss: 4.219533]\n",
      "epoch:2 step:2064 [D loss: 0.066957, acc.: 98.44%] [G loss: 2.971058]\n",
      "epoch:2 step:2065 [D loss: 0.164302, acc.: 94.53%] [G loss: 3.206374]\n",
      "epoch:2 step:2066 [D loss: 0.144048, acc.: 95.31%] [G loss: 4.423956]\n",
      "epoch:2 step:2067 [D loss: 0.192515, acc.: 93.75%] [G loss: 3.065012]\n",
      "epoch:2 step:2068 [D loss: 0.544131, acc.: 76.56%] [G loss: 5.523518]\n",
      "epoch:2 step:2069 [D loss: 0.369914, acc.: 83.59%] [G loss: 4.311527]\n",
      "epoch:2 step:2070 [D loss: 0.172440, acc.: 95.31%] [G loss: 3.915421]\n",
      "epoch:2 step:2071 [D loss: 0.078150, acc.: 97.66%] [G loss: 4.246319]\n",
      "epoch:2 step:2072 [D loss: 0.157797, acc.: 95.31%] [G loss: 5.447169]\n",
      "epoch:2 step:2073 [D loss: 0.085690, acc.: 98.44%] [G loss: 4.446734]\n",
      "epoch:2 step:2074 [D loss: 0.196613, acc.: 92.19%] [G loss: 4.327422]\n",
      "epoch:2 step:2075 [D loss: 0.345985, acc.: 82.81%] [G loss: 2.949909]\n",
      "epoch:2 step:2076 [D loss: 0.177675, acc.: 93.75%] [G loss: 4.313667]\n",
      "epoch:2 step:2077 [D loss: 0.126875, acc.: 96.88%] [G loss: 3.388447]\n",
      "epoch:2 step:2078 [D loss: 0.390000, acc.: 82.03%] [G loss: 5.435042]\n",
      "epoch:2 step:2079 [D loss: 0.067586, acc.: 98.44%] [G loss: 5.393395]\n",
      "epoch:2 step:2080 [D loss: 0.304519, acc.: 87.50%] [G loss: 4.815073]\n",
      "epoch:2 step:2081 [D loss: 0.106994, acc.: 98.44%] [G loss: 3.207027]\n",
      "epoch:2 step:2082 [D loss: 0.197150, acc.: 92.97%] [G loss: 5.990035]\n",
      "epoch:2 step:2083 [D loss: 0.220961, acc.: 89.06%] [G loss: 3.743612]\n",
      "epoch:2 step:2084 [D loss: 0.313692, acc.: 87.50%] [G loss: 4.768259]\n",
      "epoch:2 step:2085 [D loss: 0.181469, acc.: 92.19%] [G loss: 3.697036]\n",
      "epoch:2 step:2086 [D loss: 0.150679, acc.: 96.88%] [G loss: 3.294187]\n",
      "epoch:2 step:2087 [D loss: 0.207923, acc.: 91.41%] [G loss: 4.629855]\n",
      "epoch:2 step:2088 [D loss: 1.515602, acc.: 37.50%] [G loss: 4.801183]\n",
      "epoch:2 step:2089 [D loss: 0.014540, acc.: 100.00%] [G loss: 6.716042]\n",
      "epoch:2 step:2090 [D loss: 0.267296, acc.: 85.94%] [G loss: 3.648424]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:2 step:2091 [D loss: 0.026921, acc.: 100.00%] [G loss: 2.163886]\n",
      "epoch:2 step:2092 [D loss: 0.108354, acc.: 95.31%] [G loss: 3.432518]\n",
      "epoch:2 step:2093 [D loss: 0.101864, acc.: 96.88%] [G loss: 3.638622]\n",
      "epoch:2 step:2094 [D loss: 0.149639, acc.: 93.75%] [G loss: 2.742829]\n",
      "epoch:2 step:2095 [D loss: 0.342576, acc.: 87.50%] [G loss: 4.435900]\n",
      "epoch:2 step:2096 [D loss: 0.431764, acc.: 81.25%] [G loss: 3.277098]\n",
      "epoch:2 step:2097 [D loss: 0.175558, acc.: 94.53%] [G loss: 3.574524]\n",
      "epoch:2 step:2098 [D loss: 0.069919, acc.: 97.66%] [G loss: 4.869003]\n",
      "epoch:2 step:2099 [D loss: 0.468614, acc.: 77.34%] [G loss: 3.793332]\n",
      "epoch:2 step:2100 [D loss: 0.086274, acc.: 97.66%] [G loss: 4.265391]\n",
      "epoch:2 step:2101 [D loss: 0.041480, acc.: 100.00%] [G loss: 3.986516]\n",
      "epoch:2 step:2102 [D loss: 0.086952, acc.: 96.09%] [G loss: 4.498789]\n",
      "epoch:2 step:2103 [D loss: 0.068749, acc.: 98.44%] [G loss: 3.775348]\n",
      "epoch:2 step:2104 [D loss: 0.156880, acc.: 96.09%] [G loss: 4.230304]\n",
      "epoch:2 step:2105 [D loss: 0.051830, acc.: 100.00%] [G loss: 3.888361]\n",
      "epoch:2 step:2106 [D loss: 0.172894, acc.: 95.31%] [G loss: 4.107229]\n",
      "epoch:2 step:2107 [D loss: 0.070680, acc.: 97.66%] [G loss: 2.635465]\n",
      "epoch:2 step:2108 [D loss: 0.069041, acc.: 98.44%] [G loss: 1.178087]\n",
      "epoch:2 step:2109 [D loss: 0.050996, acc.: 100.00%] [G loss: 0.780242]\n",
      "epoch:2 step:2110 [D loss: 0.030150, acc.: 99.22%] [G loss: 0.249878]\n",
      "epoch:2 step:2111 [D loss: 0.055030, acc.: 97.66%] [G loss: 0.076188]\n",
      "epoch:2 step:2112 [D loss: 0.061735, acc.: 98.44%] [G loss: 0.527688]\n",
      "epoch:2 step:2113 [D loss: 0.013061, acc.: 100.00%] [G loss: 0.386282]\n",
      "epoch:2 step:2114 [D loss: 0.099611, acc.: 96.88%] [G loss: 0.058291]\n",
      "epoch:2 step:2115 [D loss: 0.546388, acc.: 75.00%] [G loss: 8.499140]\n",
      "epoch:2 step:2116 [D loss: 1.156411, acc.: 57.03%] [G loss: 4.330046]\n",
      "epoch:2 step:2117 [D loss: 0.052594, acc.: 98.44%] [G loss: 2.421610]\n",
      "epoch:2 step:2118 [D loss: 0.057040, acc.: 98.44%] [G loss: 1.631555]\n",
      "epoch:2 step:2119 [D loss: 0.095480, acc.: 96.09%] [G loss: 2.567490]\n",
      "epoch:2 step:2120 [D loss: 0.108749, acc.: 96.09%] [G loss: 2.093045]\n",
      "epoch:2 step:2121 [D loss: 0.127628, acc.: 95.31%] [G loss: 1.820828]\n",
      "epoch:2 step:2122 [D loss: 0.263506, acc.: 90.62%] [G loss: 3.630883]\n",
      "epoch:2 step:2123 [D loss: 0.096785, acc.: 96.88%] [G loss: 3.391264]\n",
      "epoch:2 step:2124 [D loss: 0.189595, acc.: 92.19%] [G loss: 2.678528]\n",
      "epoch:2 step:2125 [D loss: 0.506814, acc.: 78.12%] [G loss: 3.451909]\n",
      "epoch:2 step:2126 [D loss: 0.087633, acc.: 97.66%] [G loss: 3.682724]\n",
      "epoch:2 step:2127 [D loss: 0.333376, acc.: 85.16%] [G loss: 4.246151]\n",
      "epoch:2 step:2128 [D loss: 0.155143, acc.: 91.41%] [G loss: 2.747579]\n",
      "epoch:2 step:2129 [D loss: 0.174203, acc.: 96.88%] [G loss: 4.656033]\n",
      "epoch:2 step:2130 [D loss: 0.282919, acc.: 89.06%] [G loss: 3.738401]\n",
      "epoch:2 step:2131 [D loss: 0.455724, acc.: 80.47%] [G loss: 5.254580]\n",
      "epoch:2 step:2132 [D loss: 0.167819, acc.: 94.53%] [G loss: 4.520305]\n",
      "epoch:2 step:2133 [D loss: 0.128382, acc.: 97.66%] [G loss: 3.322351]\n",
      "epoch:2 step:2134 [D loss: 0.188321, acc.: 93.75%] [G loss: 5.040790]\n",
      "epoch:2 step:2135 [D loss: 0.232642, acc.: 90.62%] [G loss: 4.089458]\n",
      "epoch:2 step:2136 [D loss: 0.349835, acc.: 83.59%] [G loss: 6.061043]\n",
      "epoch:2 step:2137 [D loss: 0.200435, acc.: 89.84%] [G loss: 4.778294]\n",
      "epoch:2 step:2138 [D loss: 0.182140, acc.: 91.41%] [G loss: 5.275040]\n",
      "epoch:2 step:2139 [D loss: 0.154578, acc.: 94.53%] [G loss: 6.053053]\n",
      "epoch:2 step:2140 [D loss: 0.110976, acc.: 96.09%] [G loss: 4.882874]\n",
      "epoch:2 step:2141 [D loss: 0.083896, acc.: 97.66%] [G loss: 5.628002]\n",
      "epoch:2 step:2142 [D loss: 0.254952, acc.: 90.62%] [G loss: 5.791584]\n",
      "epoch:2 step:2143 [D loss: 0.112151, acc.: 96.09%] [G loss: 5.020749]\n",
      "epoch:2 step:2144 [D loss: 0.099107, acc.: 96.09%] [G loss: 5.360876]\n",
      "epoch:2 step:2145 [D loss: 0.175538, acc.: 92.97%] [G loss: 3.632147]\n",
      "epoch:2 step:2146 [D loss: 0.056715, acc.: 99.22%] [G loss: 2.625279]\n",
      "epoch:2 step:2147 [D loss: 0.341177, acc.: 81.25%] [G loss: 8.348584]\n",
      "epoch:2 step:2148 [D loss: 0.647944, acc.: 70.31%] [G loss: 2.319174]\n",
      "epoch:2 step:2149 [D loss: 0.132742, acc.: 93.75%] [G loss: 3.266394]\n",
      "epoch:2 step:2150 [D loss: 0.005219, acc.: 100.00%] [G loss: 3.132629]\n",
      "epoch:2 step:2151 [D loss: 0.041297, acc.: 99.22%] [G loss: 1.230292]\n",
      "epoch:2 step:2152 [D loss: 0.108113, acc.: 96.09%] [G loss: 0.989814]\n",
      "epoch:2 step:2153 [D loss: 0.062596, acc.: 98.44%] [G loss: 0.837081]\n",
      "epoch:2 step:2154 [D loss: 0.173293, acc.: 92.97%] [G loss: 1.747959]\n",
      "epoch:2 step:2155 [D loss: 0.150874, acc.: 95.31%] [G loss: 1.555398]\n",
      "epoch:2 step:2156 [D loss: 0.148216, acc.: 96.09%] [G loss: 3.567177]\n",
      "epoch:2 step:2157 [D loss: 0.332020, acc.: 83.59%] [G loss: 5.953989]\n",
      "epoch:2 step:2158 [D loss: 0.566140, acc.: 77.34%] [G loss: 5.049374]\n",
      "epoch:2 step:2159 [D loss: 0.022071, acc.: 99.22%] [G loss: 5.155687]\n",
      "epoch:2 step:2160 [D loss: 0.041042, acc.: 99.22%] [G loss: 2.053836]\n",
      "epoch:2 step:2161 [D loss: 0.433558, acc.: 82.03%] [G loss: 7.374372]\n",
      "epoch:2 step:2162 [D loss: 0.766014, acc.: 64.84%] [G loss: 3.094050]\n",
      "epoch:2 step:2163 [D loss: 0.318407, acc.: 86.72%] [G loss: 5.859458]\n",
      "epoch:2 step:2164 [D loss: 0.061739, acc.: 97.66%] [G loss: 6.168866]\n",
      "epoch:2 step:2165 [D loss: 0.067961, acc.: 100.00%] [G loss: 4.049137]\n",
      "epoch:2 step:2166 [D loss: 0.696990, acc.: 68.75%] [G loss: 4.798128]\n",
      "epoch:2 step:2167 [D loss: 0.175191, acc.: 94.53%] [G loss: 2.283171]\n",
      "epoch:2 step:2168 [D loss: 0.304519, acc.: 86.72%] [G loss: 3.257159]\n",
      "epoch:2 step:2169 [D loss: 0.212280, acc.: 92.19%] [G loss: 2.108859]\n",
      "epoch:2 step:2170 [D loss: 0.087472, acc.: 100.00%] [G loss: 2.480526]\n",
      "epoch:2 step:2171 [D loss: 0.198884, acc.: 93.75%] [G loss: 2.287658]\n",
      "epoch:2 step:2172 [D loss: 0.166093, acc.: 94.53%] [G loss: 1.829614]\n",
      "epoch:2 step:2173 [D loss: 0.082583, acc.: 99.22%] [G loss: 1.983788]\n",
      "epoch:2 step:2174 [D loss: 0.117965, acc.: 96.09%] [G loss: 4.593902]\n",
      "epoch:2 step:2175 [D loss: 0.933958, acc.: 50.00%] [G loss: 7.848109]\n",
      "epoch:2 step:2176 [D loss: 0.333386, acc.: 85.94%] [G loss: 7.129992]\n",
      "epoch:2 step:2177 [D loss: 0.101694, acc.: 94.53%] [G loss: 5.116074]\n",
      "epoch:2 step:2178 [D loss: 0.035447, acc.: 100.00%] [G loss: 4.178298]\n",
      "epoch:2 step:2179 [D loss: 0.038697, acc.: 100.00%] [G loss: 4.494715]\n",
      "epoch:2 step:2180 [D loss: 0.050445, acc.: 99.22%] [G loss: 4.494819]\n",
      "epoch:2 step:2181 [D loss: 0.072055, acc.: 98.44%] [G loss: 3.762775]\n",
      "epoch:2 step:2182 [D loss: 0.050591, acc.: 100.00%] [G loss: 2.899181]\n",
      "epoch:2 step:2183 [D loss: 0.174199, acc.: 92.19%] [G loss: 4.241757]\n",
      "epoch:2 step:2184 [D loss: 0.170663, acc.: 92.97%] [G loss: 1.644214]\n",
      "epoch:2 step:2185 [D loss: 0.212843, acc.: 92.19%] [G loss: 1.448138]\n",
      "epoch:2 step:2186 [D loss: 0.023355, acc.: 100.00%] [G loss: 0.566008]\n",
      "epoch:2 step:2187 [D loss: 0.031886, acc.: 100.00%] [G loss: 0.369089]\n",
      "epoch:2 step:2188 [D loss: 0.069842, acc.: 99.22%] [G loss: 0.680885]\n",
      "epoch:2 step:2189 [D loss: 0.212400, acc.: 94.53%] [G loss: 0.196498]\n",
      "epoch:2 step:2190 [D loss: 0.009111, acc.: 100.00%] [G loss: 0.542330]\n",
      "epoch:2 step:2191 [D loss: 0.109224, acc.: 96.88%] [G loss: 0.019585]\n",
      "epoch:2 step:2192 [D loss: 0.024462, acc.: 100.00%] [G loss: 0.059931]\n",
      "epoch:2 step:2193 [D loss: 0.030536, acc.: 99.22%] [G loss: 0.162365]\n",
      "epoch:2 step:2194 [D loss: 0.015862, acc.: 100.00%] [G loss: 0.129332]\n",
      "epoch:2 step:2195 [D loss: 0.052824, acc.: 98.44%] [G loss: 0.549874]\n",
      "epoch:2 step:2196 [D loss: 0.056255, acc.: 99.22%] [G loss: 0.297380]\n",
      "epoch:2 step:2197 [D loss: 0.074793, acc.: 96.88%] [G loss: 0.090166]\n",
      "epoch:2 step:2198 [D loss: 0.502661, acc.: 75.78%] [G loss: 8.640087]\n",
      "epoch:2 step:2199 [D loss: 1.238658, acc.: 57.81%] [G loss: 1.977527]\n",
      "epoch:2 step:2200 [D loss: 0.121595, acc.: 96.88%] [G loss: 1.554132]\n",
      "##############\n",
      "[0.93567405 1.07457566 0.98666763 1.06893062 2.11540115 0.95426275\n",
      " 1.06953062 2.10564241 1.07879481 1.07451578]\n",
      "##########\n",
      "epoch:2 step:2201 [D loss: 0.076420, acc.: 97.66%] [G loss: 1.008791]\n",
      "epoch:2 step:2202 [D loss: 0.031121, acc.: 100.00%] [G loss: 0.564081]\n",
      "epoch:2 step:2203 [D loss: 0.087422, acc.: 98.44%] [G loss: 0.717962]\n",
      "epoch:2 step:2204 [D loss: 0.664811, acc.: 69.53%] [G loss: 7.193549]\n",
      "epoch:2 step:2205 [D loss: 1.298359, acc.: 57.03%] [G loss: 2.951231]\n",
      "epoch:2 step:2206 [D loss: 0.219523, acc.: 90.62%] [G loss: 3.021560]\n",
      "epoch:2 step:2207 [D loss: 0.187922, acc.: 94.53%] [G loss: 4.528252]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:2 step:2208 [D loss: 0.191590, acc.: 92.19%] [G loss: 3.326893]\n",
      "epoch:2 step:2209 [D loss: 0.071496, acc.: 98.44%] [G loss: 3.427217]\n",
      "epoch:2 step:2210 [D loss: 0.117533, acc.: 96.88%] [G loss: 4.066950]\n",
      "epoch:2 step:2211 [D loss: 0.090675, acc.: 97.66%] [G loss: 3.669339]\n",
      "epoch:2 step:2212 [D loss: 0.197459, acc.: 92.97%] [G loss: 3.215127]\n",
      "epoch:2 step:2213 [D loss: 0.102506, acc.: 96.88%] [G loss: 4.304935]\n",
      "epoch:2 step:2214 [D loss: 0.081934, acc.: 98.44%] [G loss: 2.709177]\n",
      "epoch:2 step:2215 [D loss: 0.136760, acc.: 94.53%] [G loss: 4.259594]\n",
      "epoch:2 step:2216 [D loss: 0.154604, acc.: 96.88%] [G loss: 4.317086]\n",
      "epoch:2 step:2217 [D loss: 0.155778, acc.: 94.53%] [G loss: 4.280139]\n",
      "epoch:2 step:2218 [D loss: 0.295949, acc.: 89.84%] [G loss: 5.099259]\n",
      "epoch:2 step:2219 [D loss: 0.140409, acc.: 95.31%] [G loss: 4.289160]\n",
      "epoch:2 step:2220 [D loss: 0.075282, acc.: 97.66%] [G loss: 3.943429]\n",
      "epoch:2 step:2221 [D loss: 0.094436, acc.: 97.66%] [G loss: 3.881269]\n",
      "epoch:2 step:2222 [D loss: 0.062499, acc.: 99.22%] [G loss: 3.679875]\n",
      "epoch:2 step:2223 [D loss: 0.452960, acc.: 80.47%] [G loss: 5.683083]\n",
      "epoch:2 step:2224 [D loss: 0.333894, acc.: 83.59%] [G loss: 3.072472]\n",
      "epoch:2 step:2225 [D loss: 0.042697, acc.: 100.00%] [G loss: 2.256807]\n",
      "epoch:2 step:2226 [D loss: 0.045719, acc.: 99.22%] [G loss: 1.891401]\n",
      "epoch:2 step:2227 [D loss: 0.116901, acc.: 99.22%] [G loss: 1.866679]\n",
      "epoch:2 step:2228 [D loss: 0.069371, acc.: 97.66%] [G loss: 1.310720]\n",
      "epoch:2 step:2229 [D loss: 0.130532, acc.: 96.88%] [G loss: 2.298047]\n",
      "epoch:2 step:2230 [D loss: 0.377242, acc.: 81.25%] [G loss: 1.161538]\n",
      "epoch:2 step:2231 [D loss: 0.046817, acc.: 100.00%] [G loss: 1.794125]\n",
      "epoch:2 step:2232 [D loss: 0.053143, acc.: 98.44%] [G loss: 0.632400]\n",
      "epoch:2 step:2233 [D loss: 0.176592, acc.: 93.75%] [G loss: 2.925528]\n",
      "epoch:2 step:2234 [D loss: 0.018808, acc.: 100.00%] [G loss: 4.640054]\n",
      "epoch:2 step:2235 [D loss: 0.728114, acc.: 64.06%] [G loss: 3.041857]\n",
      "epoch:2 step:2236 [D loss: 0.082570, acc.: 99.22%] [G loss: 3.788279]\n",
      "epoch:2 step:2237 [D loss: 0.096018, acc.: 98.44%] [G loss: 3.573991]\n",
      "epoch:2 step:2238 [D loss: 0.180234, acc.: 93.75%] [G loss: 4.622583]\n",
      "epoch:2 step:2239 [D loss: 0.176671, acc.: 92.19%] [G loss: 2.844203]\n",
      "epoch:2 step:2240 [D loss: 0.266613, acc.: 87.50%] [G loss: 5.468570]\n",
      "epoch:2 step:2241 [D loss: 0.174822, acc.: 93.75%] [G loss: 4.601747]\n",
      "epoch:2 step:2242 [D loss: 0.467634, acc.: 78.91%] [G loss: 7.371776]\n",
      "epoch:2 step:2243 [D loss: 0.304168, acc.: 81.25%] [G loss: 4.516594]\n",
      "epoch:2 step:2244 [D loss: 0.125137, acc.: 96.88%] [G loss: 3.307104]\n",
      "epoch:2 step:2245 [D loss: 0.129652, acc.: 96.88%] [G loss: 4.826646]\n",
      "epoch:2 step:2246 [D loss: 0.246847, acc.: 89.06%] [G loss: 5.605914]\n",
      "epoch:2 step:2247 [D loss: 0.275136, acc.: 86.72%] [G loss: 3.743015]\n",
      "epoch:2 step:2248 [D loss: 0.166043, acc.: 96.09%] [G loss: 4.037364]\n",
      "epoch:2 step:2249 [D loss: 0.196152, acc.: 94.53%] [G loss: 3.158153]\n",
      "epoch:2 step:2250 [D loss: 0.295136, acc.: 87.50%] [G loss: 1.254452]\n",
      "epoch:2 step:2251 [D loss: 0.034616, acc.: 99.22%] [G loss: 1.301939]\n",
      "epoch:2 step:2252 [D loss: 0.112123, acc.: 96.09%] [G loss: 2.766159]\n",
      "epoch:2 step:2253 [D loss: 0.132337, acc.: 96.09%] [G loss: 0.957771]\n",
      "epoch:2 step:2254 [D loss: 0.349190, acc.: 82.81%] [G loss: 6.738870]\n",
      "epoch:2 step:2255 [D loss: 0.661058, acc.: 71.88%] [G loss: 3.055185]\n",
      "epoch:2 step:2256 [D loss: 0.153034, acc.: 92.97%] [G loss: 2.819274]\n",
      "epoch:2 step:2257 [D loss: 0.034465, acc.: 99.22%] [G loss: 1.725197]\n",
      "epoch:2 step:2258 [D loss: 0.159744, acc.: 94.53%] [G loss: 3.402850]\n",
      "epoch:2 step:2259 [D loss: 0.257142, acc.: 90.62%] [G loss: 5.530011]\n",
      "epoch:2 step:2260 [D loss: 1.021773, acc.: 57.81%] [G loss: 4.567103]\n",
      "epoch:2 step:2261 [D loss: 0.196119, acc.: 91.41%] [G loss: 4.745324]\n",
      "epoch:2 step:2262 [D loss: 0.057878, acc.: 99.22%] [G loss: 4.285476]\n",
      "epoch:2 step:2263 [D loss: 0.067323, acc.: 100.00%] [G loss: 4.037863]\n",
      "epoch:2 step:2264 [D loss: 0.049007, acc.: 100.00%] [G loss: 4.436507]\n",
      "epoch:2 step:2265 [D loss: 0.148338, acc.: 95.31%] [G loss: 4.432639]\n",
      "epoch:2 step:2266 [D loss: 0.065424, acc.: 98.44%] [G loss: 3.118378]\n",
      "epoch:2 step:2267 [D loss: 0.556065, acc.: 71.09%] [G loss: 5.004274]\n",
      "epoch:2 step:2268 [D loss: 0.208317, acc.: 90.62%] [G loss: 3.920769]\n",
      "epoch:2 step:2269 [D loss: 0.258137, acc.: 88.28%] [G loss: 3.649030]\n",
      "epoch:2 step:2270 [D loss: 0.088744, acc.: 99.22%] [G loss: 2.105096]\n",
      "epoch:2 step:2271 [D loss: 0.070688, acc.: 98.44%] [G loss: 1.690950]\n",
      "epoch:2 step:2272 [D loss: 0.036558, acc.: 100.00%] [G loss: 1.540868]\n",
      "epoch:2 step:2273 [D loss: 0.085068, acc.: 98.44%] [G loss: 2.238162]\n",
      "epoch:2 step:2274 [D loss: 0.079511, acc.: 99.22%] [G loss: 0.732745]\n",
      "epoch:2 step:2275 [D loss: 0.387874, acc.: 82.03%] [G loss: 2.964973]\n",
      "epoch:2 step:2276 [D loss: 0.044586, acc.: 98.44%] [G loss: 4.211105]\n",
      "epoch:2 step:2277 [D loss: 1.032861, acc.: 53.12%] [G loss: 0.993163]\n",
      "epoch:2 step:2278 [D loss: 0.009321, acc.: 100.00%] [G loss: 2.477625]\n",
      "epoch:2 step:2279 [D loss: 0.067163, acc.: 96.88%] [G loss: 1.711160]\n",
      "epoch:2 step:2280 [D loss: 0.141337, acc.: 96.09%] [G loss: 2.748032]\n",
      "epoch:2 step:2281 [D loss: 0.251321, acc.: 92.97%] [G loss: 4.952384]\n",
      "epoch:2 step:2282 [D loss: 1.324309, acc.: 34.38%] [G loss: 7.696074]\n",
      "epoch:2 step:2283 [D loss: 0.474224, acc.: 77.34%] [G loss: 5.848071]\n",
      "epoch:2 step:2284 [D loss: 0.133209, acc.: 96.09%] [G loss: 3.635259]\n",
      "epoch:2 step:2285 [D loss: 0.104444, acc.: 96.09%] [G loss: 4.293069]\n",
      "epoch:2 step:2286 [D loss: 0.061652, acc.: 99.22%] [G loss: 3.594619]\n",
      "epoch:2 step:2287 [D loss: 0.239117, acc.: 91.41%] [G loss: 3.766326]\n",
      "epoch:2 step:2288 [D loss: 0.099334, acc.: 99.22%] [G loss: 2.724935]\n",
      "epoch:2 step:2289 [D loss: 0.137917, acc.: 96.09%] [G loss: 2.387567]\n",
      "epoch:2 step:2290 [D loss: 0.133279, acc.: 97.66%] [G loss: 2.317522]\n",
      "epoch:2 step:2291 [D loss: 0.111262, acc.: 97.66%] [G loss: 1.405039]\n",
      "epoch:2 step:2292 [D loss: 0.141220, acc.: 95.31%] [G loss: 3.042338]\n",
      "epoch:2 step:2293 [D loss: 0.207504, acc.: 91.41%] [G loss: 0.915544]\n",
      "epoch:2 step:2294 [D loss: 0.161512, acc.: 94.53%] [G loss: 3.529875]\n",
      "epoch:2 step:2295 [D loss: 0.085071, acc.: 96.88%] [G loss: 3.463732]\n",
      "epoch:2 step:2296 [D loss: 0.151316, acc.: 96.09%] [G loss: 1.774814]\n",
      "epoch:2 step:2297 [D loss: 0.432550, acc.: 78.12%] [G loss: 7.285243]\n",
      "epoch:2 step:2298 [D loss: 0.775115, acc.: 63.28%] [G loss: 1.874566]\n",
      "epoch:2 step:2299 [D loss: 0.605505, acc.: 72.66%] [G loss: 6.114888]\n",
      "epoch:2 step:2300 [D loss: 0.158542, acc.: 92.97%] [G loss: 6.099587]\n",
      "epoch:2 step:2301 [D loss: 0.178387, acc.: 92.97%] [G loss: 2.282423]\n",
      "epoch:2 step:2302 [D loss: 0.350138, acc.: 78.91%] [G loss: 5.121684]\n",
      "epoch:2 step:2303 [D loss: 0.197101, acc.: 91.41%] [G loss: 4.316020]\n",
      "epoch:2 step:2304 [D loss: 0.164189, acc.: 94.53%] [G loss: 2.218564]\n",
      "epoch:2 step:2305 [D loss: 0.189388, acc.: 93.75%] [G loss: 2.912267]\n",
      "epoch:2 step:2306 [D loss: 0.102359, acc.: 98.44%] [G loss: 4.293372]\n",
      "epoch:2 step:2307 [D loss: 0.431826, acc.: 82.03%] [G loss: 6.772034]\n",
      "epoch:2 step:2308 [D loss: 0.535039, acc.: 75.00%] [G loss: 4.244260]\n",
      "epoch:2 step:2309 [D loss: 0.155173, acc.: 94.53%] [G loss: 4.965158]\n",
      "epoch:2 step:2310 [D loss: 0.022463, acc.: 100.00%] [G loss: 5.746257]\n",
      "epoch:2 step:2311 [D loss: 0.086157, acc.: 97.66%] [G loss: 4.211836]\n",
      "epoch:2 step:2312 [D loss: 0.668788, acc.: 72.66%] [G loss: 7.558294]\n",
      "epoch:2 step:2313 [D loss: 0.368401, acc.: 79.69%] [G loss: 5.741629]\n",
      "epoch:2 step:2314 [D loss: 0.086828, acc.: 96.88%] [G loss: 2.938691]\n",
      "epoch:2 step:2315 [D loss: 0.226659, acc.: 90.62%] [G loss: 3.779620]\n",
      "epoch:2 step:2316 [D loss: 0.323744, acc.: 84.38%] [G loss: 1.547301]\n",
      "epoch:2 step:2317 [D loss: 0.131319, acc.: 93.75%] [G loss: 2.027692]\n",
      "epoch:2 step:2318 [D loss: 0.034453, acc.: 99.22%] [G loss: 3.462700]\n",
      "epoch:2 step:2319 [D loss: 0.108650, acc.: 96.88%] [G loss: 0.819835]\n",
      "epoch:2 step:2320 [D loss: 0.346996, acc.: 82.81%] [G loss: 5.563817]\n",
      "epoch:2 step:2321 [D loss: 1.017942, acc.: 57.81%] [G loss: 0.745362]\n",
      "epoch:2 step:2322 [D loss: 0.257618, acc.: 88.28%] [G loss: 2.072593]\n",
      "epoch:2 step:2323 [D loss: 0.015919, acc.: 100.00%] [G loss: 4.126664]\n",
      "epoch:2 step:2324 [D loss: 0.086337, acc.: 98.44%] [G loss: 1.329039]\n",
      "epoch:2 step:2325 [D loss: 0.554070, acc.: 80.47%] [G loss: 4.370164]\n",
      "epoch:2 step:2326 [D loss: 0.671175, acc.: 66.41%] [G loss: 3.113818]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:2 step:2327 [D loss: 0.143652, acc.: 97.66%] [G loss: 3.769382]\n",
      "epoch:2 step:2328 [D loss: 0.077047, acc.: 100.00%] [G loss: 2.417368]\n",
      "epoch:2 step:2329 [D loss: 0.620802, acc.: 66.41%] [G loss: 7.603289]\n",
      "epoch:2 step:2330 [D loss: 1.325808, acc.: 51.56%] [G loss: 3.901560]\n",
      "epoch:2 step:2331 [D loss: 0.085040, acc.: 96.88%] [G loss: 3.462561]\n",
      "epoch:2 step:2332 [D loss: 0.069876, acc.: 100.00%] [G loss: 3.387488]\n",
      "epoch:2 step:2333 [D loss: 0.079469, acc.: 97.66%] [G loss: 3.580978]\n",
      "epoch:2 step:2334 [D loss: 0.065356, acc.: 100.00%] [G loss: 2.777487]\n",
      "epoch:2 step:2335 [D loss: 0.105391, acc.: 96.09%] [G loss: 3.142524]\n",
      "epoch:2 step:2336 [D loss: 0.132145, acc.: 96.09%] [G loss: 3.170606]\n",
      "epoch:2 step:2337 [D loss: 0.160485, acc.: 96.09%] [G loss: 2.584416]\n",
      "epoch:2 step:2338 [D loss: 0.225953, acc.: 91.41%] [G loss: 4.586024]\n",
      "epoch:2 step:2339 [D loss: 0.416019, acc.: 78.91%] [G loss: 1.930903]\n",
      "epoch:2 step:2340 [D loss: 0.146681, acc.: 95.31%] [G loss: 3.458740]\n",
      "epoch:2 step:2341 [D loss: 0.544782, acc.: 71.09%] [G loss: 4.057051]\n",
      "epoch:2 step:2342 [D loss: 0.103272, acc.: 96.09%] [G loss: 3.563410]\n",
      "epoch:2 step:2343 [D loss: 0.141884, acc.: 96.09%] [G loss: 4.684396]\n",
      "epoch:3 step:2344 [D loss: 0.075585, acc.: 100.00%] [G loss: 4.273500]\n",
      "epoch:3 step:2345 [D loss: 0.191733, acc.: 92.19%] [G loss: 2.789111]\n",
      "epoch:3 step:2346 [D loss: 0.631437, acc.: 67.97%] [G loss: 6.692596]\n",
      "epoch:3 step:2347 [D loss: 0.553001, acc.: 72.66%] [G loss: 5.328797]\n",
      "epoch:3 step:2348 [D loss: 0.382069, acc.: 80.47%] [G loss: 3.629010]\n",
      "epoch:3 step:2349 [D loss: 0.072414, acc.: 98.44%] [G loss: 3.314670]\n",
      "epoch:3 step:2350 [D loss: 0.148406, acc.: 94.53%] [G loss: 2.969520]\n",
      "epoch:3 step:2351 [D loss: 0.047330, acc.: 98.44%] [G loss: 2.227879]\n",
      "epoch:3 step:2352 [D loss: 0.282761, acc.: 83.59%] [G loss: 3.880948]\n",
      "epoch:3 step:2353 [D loss: 0.416897, acc.: 82.03%] [G loss: 2.975340]\n",
      "epoch:3 step:2354 [D loss: 0.108689, acc.: 97.66%] [G loss: 1.740199]\n",
      "epoch:3 step:2355 [D loss: 0.057010, acc.: 99.22%] [G loss: 1.086834]\n",
      "epoch:3 step:2356 [D loss: 0.300584, acc.: 86.72%] [G loss: 4.079852]\n",
      "epoch:3 step:2357 [D loss: 0.206572, acc.: 89.84%] [G loss: 1.894249]\n",
      "epoch:3 step:2358 [D loss: 0.104507, acc.: 96.09%] [G loss: 0.711019]\n",
      "epoch:3 step:2359 [D loss: 0.349182, acc.: 82.03%] [G loss: 4.958260]\n",
      "epoch:3 step:2360 [D loss: 0.164936, acc.: 94.53%] [G loss: 5.540333]\n",
      "epoch:3 step:2361 [D loss: 0.243915, acc.: 88.28%] [G loss: 1.675947]\n",
      "epoch:3 step:2362 [D loss: 1.071610, acc.: 57.03%] [G loss: 6.715949]\n",
      "epoch:3 step:2363 [D loss: 0.434243, acc.: 80.47%] [G loss: 6.978955]\n",
      "epoch:3 step:2364 [D loss: 0.202875, acc.: 90.62%] [G loss: 4.326128]\n",
      "epoch:3 step:2365 [D loss: 0.056799, acc.: 99.22%] [G loss: 3.634936]\n",
      "epoch:3 step:2366 [D loss: 0.371872, acc.: 80.47%] [G loss: 5.300469]\n",
      "epoch:3 step:2367 [D loss: 0.119759, acc.: 95.31%] [G loss: 5.666463]\n",
      "epoch:3 step:2368 [D loss: 0.418004, acc.: 82.03%] [G loss: 2.862382]\n",
      "epoch:3 step:2369 [D loss: 0.386283, acc.: 82.03%] [G loss: 4.764507]\n",
      "epoch:3 step:2370 [D loss: 0.255352, acc.: 90.62%] [G loss: 4.406108]\n",
      "epoch:3 step:2371 [D loss: 0.143467, acc.: 97.66%] [G loss: 3.777155]\n",
      "epoch:3 step:2372 [D loss: 0.170823, acc.: 95.31%] [G loss: 4.210619]\n",
      "epoch:3 step:2373 [D loss: 0.228108, acc.: 92.97%] [G loss: 3.725163]\n",
      "epoch:3 step:2374 [D loss: 0.189721, acc.: 91.41%] [G loss: 4.150211]\n",
      "epoch:3 step:2375 [D loss: 0.105791, acc.: 98.44%] [G loss: 3.775483]\n",
      "epoch:3 step:2376 [D loss: 0.142832, acc.: 95.31%] [G loss: 3.453058]\n",
      "epoch:3 step:2377 [D loss: 0.208406, acc.: 92.97%] [G loss: 4.865179]\n",
      "epoch:3 step:2378 [D loss: 0.230717, acc.: 89.06%] [G loss: 3.070888]\n",
      "epoch:3 step:2379 [D loss: 0.254632, acc.: 89.06%] [G loss: 4.474511]\n",
      "epoch:3 step:2380 [D loss: 0.124297, acc.: 97.66%] [G loss: 4.548211]\n",
      "epoch:3 step:2381 [D loss: 0.293262, acc.: 89.84%] [G loss: 3.782398]\n",
      "epoch:3 step:2382 [D loss: 0.298380, acc.: 88.28%] [G loss: 4.790292]\n",
      "epoch:3 step:2383 [D loss: 0.158989, acc.: 93.75%] [G loss: 4.144094]\n",
      "epoch:3 step:2384 [D loss: 0.255585, acc.: 91.41%] [G loss: 4.485827]\n",
      "epoch:3 step:2385 [D loss: 0.107199, acc.: 95.31%] [G loss: 4.116958]\n",
      "epoch:3 step:2386 [D loss: 0.322126, acc.: 88.28%] [G loss: 5.059113]\n",
      "epoch:3 step:2387 [D loss: 0.353741, acc.: 82.81%] [G loss: 2.332149]\n",
      "epoch:3 step:2388 [D loss: 0.361525, acc.: 85.16%] [G loss: 5.269874]\n",
      "epoch:3 step:2389 [D loss: 0.279533, acc.: 85.94%] [G loss: 3.625728]\n",
      "epoch:3 step:2390 [D loss: 0.334861, acc.: 86.72%] [G loss: 5.556035]\n",
      "epoch:3 step:2391 [D loss: 0.331202, acc.: 83.59%] [G loss: 3.021118]\n",
      "epoch:3 step:2392 [D loss: 0.279143, acc.: 82.81%] [G loss: 4.501370]\n",
      "epoch:3 step:2393 [D loss: 0.197388, acc.: 92.19%] [G loss: 3.852315]\n",
      "epoch:3 step:2394 [D loss: 0.548777, acc.: 69.53%] [G loss: 5.156675]\n",
      "epoch:3 step:2395 [D loss: 0.201126, acc.: 90.62%] [G loss: 4.221313]\n",
      "epoch:3 step:2396 [D loss: 0.171537, acc.: 96.09%] [G loss: 3.585270]\n",
      "epoch:3 step:2397 [D loss: 0.174192, acc.: 93.75%] [G loss: 4.064943]\n",
      "epoch:3 step:2398 [D loss: 0.156174, acc.: 96.09%] [G loss: 4.648074]\n",
      "epoch:3 step:2399 [D loss: 0.283768, acc.: 89.84%] [G loss: 4.986908]\n",
      "epoch:3 step:2400 [D loss: 0.509475, acc.: 75.78%] [G loss: 4.127101]\n",
      "##############\n",
      "[0.79231381 0.85360494 0.92847802 0.9104121  0.95855911 0.91379171\n",
      " 0.97216914 2.11578716 0.86423382 1.04911181]\n",
      "##########\n",
      "epoch:3 step:2401 [D loss: 0.077800, acc.: 99.22%] [G loss: 5.143477]\n",
      "epoch:3 step:2402 [D loss: 0.091173, acc.: 98.44%] [G loss: 4.456387]\n",
      "epoch:3 step:2403 [D loss: 0.290357, acc.: 89.06%] [G loss: 5.849030]\n",
      "epoch:3 step:2404 [D loss: 0.230455, acc.: 88.28%] [G loss: 4.148126]\n",
      "epoch:3 step:2405 [D loss: 0.205714, acc.: 93.75%] [G loss: 3.456747]\n",
      "epoch:3 step:2406 [D loss: 0.111526, acc.: 96.09%] [G loss: 5.355701]\n",
      "epoch:3 step:2407 [D loss: 0.261729, acc.: 89.84%] [G loss: 3.934116]\n",
      "epoch:3 step:2408 [D loss: 0.603204, acc.: 69.53%] [G loss: 6.911002]\n",
      "epoch:3 step:2409 [D loss: 0.445976, acc.: 74.22%] [G loss: 4.368716]\n",
      "epoch:3 step:2410 [D loss: 0.379780, acc.: 82.03%] [G loss: 5.087149]\n",
      "epoch:3 step:2411 [D loss: 0.058442, acc.: 99.22%] [G loss: 5.383203]\n",
      "epoch:3 step:2412 [D loss: 0.071149, acc.: 98.44%] [G loss: 4.039754]\n",
      "epoch:3 step:2413 [D loss: 0.108475, acc.: 95.31%] [G loss: 3.905609]\n",
      "epoch:3 step:2414 [D loss: 0.512204, acc.: 74.22%] [G loss: 5.995849]\n",
      "epoch:3 step:2415 [D loss: 0.293739, acc.: 85.94%] [G loss: 3.622503]\n",
      "epoch:3 step:2416 [D loss: 0.151505, acc.: 94.53%] [G loss: 4.484214]\n",
      "epoch:3 step:2417 [D loss: 0.141184, acc.: 96.88%] [G loss: 4.264300]\n",
      "epoch:3 step:2418 [D loss: 0.214042, acc.: 92.19%] [G loss: 3.360666]\n",
      "epoch:3 step:2419 [D loss: 0.693596, acc.: 69.53%] [G loss: 7.124409]\n",
      "epoch:3 step:2420 [D loss: 1.199979, acc.: 54.69%] [G loss: 3.820013]\n",
      "epoch:3 step:2421 [D loss: 0.119868, acc.: 96.88%] [G loss: 3.113843]\n",
      "epoch:3 step:2422 [D loss: 0.106656, acc.: 96.88%] [G loss: 3.922249]\n",
      "epoch:3 step:2423 [D loss: 0.278894, acc.: 89.84%] [G loss: 4.070852]\n",
      "epoch:3 step:2424 [D loss: 0.175536, acc.: 93.75%] [G loss: 3.647691]\n",
      "epoch:3 step:2425 [D loss: 0.067833, acc.: 99.22%] [G loss: 3.361718]\n",
      "epoch:3 step:2426 [D loss: 0.156678, acc.: 96.09%] [G loss: 3.149931]\n",
      "epoch:3 step:2427 [D loss: 0.364601, acc.: 82.81%] [G loss: 3.204302]\n",
      "epoch:3 step:2428 [D loss: 0.091665, acc.: 99.22%] [G loss: 2.576665]\n",
      "epoch:3 step:2429 [D loss: 0.296997, acc.: 86.72%] [G loss: 3.575121]\n",
      "epoch:3 step:2430 [D loss: 0.110106, acc.: 96.88%] [G loss: 3.190722]\n",
      "epoch:3 step:2431 [D loss: 0.304078, acc.: 88.28%] [G loss: 5.558262]\n",
      "epoch:3 step:2432 [D loss: 0.619524, acc.: 70.31%] [G loss: 3.831464]\n",
      "epoch:3 step:2433 [D loss: 0.317614, acc.: 89.06%] [G loss: 4.641729]\n",
      "epoch:3 step:2434 [D loss: 0.077764, acc.: 98.44%] [G loss: 4.724738]\n",
      "epoch:3 step:2435 [D loss: 0.081087, acc.: 99.22%] [G loss: 2.841128]\n",
      "epoch:3 step:2436 [D loss: 0.146497, acc.: 97.66%] [G loss: 2.628789]\n",
      "epoch:3 step:2437 [D loss: 0.243140, acc.: 89.06%] [G loss: 4.893521]\n",
      "epoch:3 step:2438 [D loss: 0.290236, acc.: 85.16%] [G loss: 2.902470]\n",
      "epoch:3 step:2439 [D loss: 0.333619, acc.: 84.38%] [G loss: 4.682928]\n",
      "epoch:3 step:2440 [D loss: 0.332971, acc.: 86.72%] [G loss: 4.006456]\n",
      "epoch:3 step:2441 [D loss: 0.074852, acc.: 97.66%] [G loss: 2.035749]\n",
      "epoch:3 step:2442 [D loss: 0.282915, acc.: 86.72%] [G loss: 5.772025]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:3 step:2443 [D loss: 0.113041, acc.: 95.31%] [G loss: 4.815432]\n",
      "epoch:3 step:2444 [D loss: 0.163036, acc.: 92.97%] [G loss: 0.671930]\n",
      "epoch:3 step:2445 [D loss: 0.459476, acc.: 77.34%] [G loss: 7.074498]\n",
      "epoch:3 step:2446 [D loss: 0.905825, acc.: 67.19%] [G loss: 1.376287]\n",
      "epoch:3 step:2447 [D loss: 0.176862, acc.: 90.62%] [G loss: 3.280449]\n",
      "epoch:3 step:2448 [D loss: 0.037583, acc.: 100.00%] [G loss: 2.996639]\n",
      "epoch:3 step:2449 [D loss: 0.095547, acc.: 97.66%] [G loss: 0.961874]\n",
      "epoch:3 step:2450 [D loss: 0.179936, acc.: 93.75%] [G loss: 0.794293]\n",
      "epoch:3 step:2451 [D loss: 0.296221, acc.: 85.94%] [G loss: 5.000726]\n",
      "epoch:3 step:2452 [D loss: 0.814543, acc.: 54.69%] [G loss: 2.520555]\n",
      "epoch:3 step:2453 [D loss: 0.138677, acc.: 92.97%] [G loss: 4.403232]\n",
      "epoch:3 step:2454 [D loss: 0.408963, acc.: 81.25%] [G loss: 2.722379]\n",
      "epoch:3 step:2455 [D loss: 0.076238, acc.: 97.66%] [G loss: 3.751647]\n",
      "epoch:3 step:2456 [D loss: 0.103242, acc.: 98.44%] [G loss: 3.352519]\n",
      "epoch:3 step:2457 [D loss: 0.060616, acc.: 99.22%] [G loss: 2.285086]\n",
      "epoch:3 step:2458 [D loss: 0.509846, acc.: 74.22%] [G loss: 7.037533]\n",
      "epoch:3 step:2459 [D loss: 1.364969, acc.: 50.78%] [G loss: 2.716642]\n",
      "epoch:3 step:2460 [D loss: 0.233716, acc.: 87.50%] [G loss: 5.077982]\n",
      "epoch:3 step:2461 [D loss: 0.047439, acc.: 98.44%] [G loss: 5.494531]\n",
      "epoch:3 step:2462 [D loss: 0.077703, acc.: 98.44%] [G loss: 3.713291]\n",
      "epoch:3 step:2463 [D loss: 0.087807, acc.: 97.66%] [G loss: 3.376632]\n",
      "epoch:3 step:2464 [D loss: 0.066271, acc.: 100.00%] [G loss: 3.149450]\n",
      "epoch:3 step:2465 [D loss: 0.103690, acc.: 99.22%] [G loss: 3.524354]\n",
      "epoch:3 step:2466 [D loss: 0.177367, acc.: 96.88%] [G loss: 3.076302]\n",
      "epoch:3 step:2467 [D loss: 0.342591, acc.: 88.28%] [G loss: 3.943844]\n",
      "epoch:3 step:2468 [D loss: 0.070952, acc.: 98.44%] [G loss: 4.111655]\n",
      "epoch:3 step:2469 [D loss: 0.068454, acc.: 98.44%] [G loss: 2.372658]\n",
      "epoch:3 step:2470 [D loss: 0.054871, acc.: 98.44%] [G loss: 0.909334]\n",
      "epoch:3 step:2471 [D loss: 0.144339, acc.: 94.53%] [G loss: 2.065180]\n",
      "epoch:3 step:2472 [D loss: 0.051490, acc.: 100.00%] [G loss: 1.571803]\n",
      "epoch:3 step:2473 [D loss: 0.834230, acc.: 57.03%] [G loss: 5.055120]\n",
      "epoch:3 step:2474 [D loss: 0.244180, acc.: 89.06%] [G loss: 4.490139]\n",
      "epoch:3 step:2475 [D loss: 0.042770, acc.: 99.22%] [G loss: 3.534200]\n",
      "epoch:3 step:2476 [D loss: 0.046449, acc.: 100.00%] [G loss: 1.470431]\n",
      "epoch:3 step:2477 [D loss: 0.149132, acc.: 96.09%] [G loss: 1.010137]\n",
      "epoch:3 step:2478 [D loss: 0.109764, acc.: 97.66%] [G loss: 2.662221]\n",
      "epoch:3 step:2479 [D loss: 0.388401, acc.: 82.81%] [G loss: 2.933545]\n",
      "epoch:3 step:2480 [D loss: 0.143743, acc.: 96.09%] [G loss: 2.634988]\n",
      "epoch:3 step:2481 [D loss: 0.055251, acc.: 100.00%] [G loss: 1.916349]\n",
      "epoch:3 step:2482 [D loss: 0.083702, acc.: 98.44%] [G loss: 1.935720]\n",
      "epoch:3 step:2483 [D loss: 0.245385, acc.: 90.62%] [G loss: 2.931537]\n",
      "epoch:3 step:2484 [D loss: 0.568062, acc.: 75.78%] [G loss: 3.323320]\n",
      "epoch:3 step:2485 [D loss: 0.096193, acc.: 97.66%] [G loss: 3.029485]\n",
      "epoch:3 step:2486 [D loss: 0.064249, acc.: 99.22%] [G loss: 2.789131]\n",
      "epoch:3 step:2487 [D loss: 0.086223, acc.: 99.22%] [G loss: 3.272509]\n",
      "epoch:3 step:2488 [D loss: 0.362929, acc.: 82.03%] [G loss: 3.364394]\n",
      "epoch:3 step:2489 [D loss: 0.121381, acc.: 96.88%] [G loss: 3.711956]\n",
      "epoch:3 step:2490 [D loss: 0.109806, acc.: 98.44%] [G loss: 4.073748]\n",
      "epoch:3 step:2491 [D loss: 0.236786, acc.: 94.53%] [G loss: 3.056221]\n",
      "epoch:3 step:2492 [D loss: 0.097430, acc.: 98.44%] [G loss: 2.422782]\n",
      "epoch:3 step:2493 [D loss: 0.049797, acc.: 99.22%] [G loss: 2.824999]\n",
      "epoch:3 step:2494 [D loss: 0.085138, acc.: 99.22%] [G loss: 3.309191]\n",
      "epoch:3 step:2495 [D loss: 0.406648, acc.: 80.47%] [G loss: 5.519448]\n",
      "epoch:3 step:2496 [D loss: 0.370224, acc.: 79.69%] [G loss: 2.594316]\n",
      "epoch:3 step:2497 [D loss: 0.096004, acc.: 97.66%] [G loss: 3.375189]\n",
      "epoch:3 step:2498 [D loss: 0.038440, acc.: 98.44%] [G loss: 3.294079]\n",
      "epoch:3 step:2499 [D loss: 0.297475, acc.: 89.06%] [G loss: 2.566325]\n",
      "epoch:3 step:2500 [D loss: 0.034516, acc.: 99.22%] [G loss: 1.593298]\n",
      "epoch:3 step:2501 [D loss: 0.694355, acc.: 65.62%] [G loss: 7.742314]\n",
      "epoch:3 step:2502 [D loss: 0.314818, acc.: 88.28%] [G loss: 7.634209]\n",
      "epoch:3 step:2503 [D loss: 0.196899, acc.: 92.19%] [G loss: 3.369637]\n",
      "epoch:3 step:2504 [D loss: 0.066132, acc.: 98.44%] [G loss: 2.132764]\n",
      "epoch:3 step:2505 [D loss: 0.045679, acc.: 99.22%] [G loss: 2.393574]\n",
      "epoch:3 step:2506 [D loss: 0.070156, acc.: 99.22%] [G loss: 1.845516]\n",
      "epoch:3 step:2507 [D loss: 0.186216, acc.: 92.97%] [G loss: 5.720854]\n",
      "epoch:3 step:2508 [D loss: 0.171111, acc.: 92.97%] [G loss: 2.845495]\n",
      "epoch:3 step:2509 [D loss: 1.109098, acc.: 50.78%] [G loss: 7.458146]\n",
      "epoch:3 step:2510 [D loss: 0.764004, acc.: 67.19%] [G loss: 4.818523]\n",
      "epoch:3 step:2511 [D loss: 0.056965, acc.: 99.22%] [G loss: 3.034337]\n",
      "epoch:3 step:2512 [D loss: 0.090757, acc.: 97.66%] [G loss: 3.277749]\n",
      "epoch:3 step:2513 [D loss: 0.073087, acc.: 99.22%] [G loss: 3.954340]\n",
      "epoch:3 step:2514 [D loss: 0.128359, acc.: 96.09%] [G loss: 3.932806]\n",
      "epoch:3 step:2515 [D loss: 0.193819, acc.: 89.84%] [G loss: 4.895382]\n",
      "epoch:3 step:2516 [D loss: 0.195909, acc.: 92.19%] [G loss: 3.956999]\n",
      "epoch:3 step:2517 [D loss: 0.136240, acc.: 96.88%] [G loss: 3.215507]\n",
      "epoch:3 step:2518 [D loss: 0.177374, acc.: 94.53%] [G loss: 4.390641]\n",
      "epoch:3 step:2519 [D loss: 0.500247, acc.: 77.34%] [G loss: 2.051369]\n",
      "epoch:3 step:2520 [D loss: 0.374356, acc.: 78.91%] [G loss: 6.729377]\n",
      "epoch:3 step:2521 [D loss: 0.177989, acc.: 91.41%] [G loss: 5.926033]\n",
      "epoch:3 step:2522 [D loss: 0.234528, acc.: 91.41%] [G loss: 2.020399]\n",
      "epoch:3 step:2523 [D loss: 0.103484, acc.: 97.66%] [G loss: 3.443123]\n",
      "epoch:3 step:2524 [D loss: 0.015607, acc.: 100.00%] [G loss: 2.708359]\n",
      "epoch:3 step:2525 [D loss: 0.133429, acc.: 95.31%] [G loss: 3.089898]\n",
      "epoch:3 step:2526 [D loss: 0.199512, acc.: 91.41%] [G loss: 0.965267]\n",
      "epoch:3 step:2527 [D loss: 0.150717, acc.: 96.09%] [G loss: 3.686547]\n",
      "epoch:3 step:2528 [D loss: 1.995325, acc.: 22.66%] [G loss: 7.175054]\n",
      "epoch:3 step:2529 [D loss: 0.494429, acc.: 71.09%] [G loss: 5.962650]\n",
      "epoch:3 step:2530 [D loss: 0.125754, acc.: 93.75%] [G loss: 4.055610]\n",
      "epoch:3 step:2531 [D loss: 0.097165, acc.: 98.44%] [G loss: 2.670948]\n",
      "epoch:3 step:2532 [D loss: 0.141695, acc.: 96.09%] [G loss: 3.951787]\n",
      "epoch:3 step:2533 [D loss: 0.157719, acc.: 94.53%] [G loss: 3.851521]\n",
      "epoch:3 step:2534 [D loss: 0.140876, acc.: 95.31%] [G loss: 3.141571]\n",
      "epoch:3 step:2535 [D loss: 0.183894, acc.: 95.31%] [G loss: 2.801976]\n",
      "epoch:3 step:2536 [D loss: 0.093216, acc.: 97.66%] [G loss: 2.947349]\n",
      "epoch:3 step:2537 [D loss: 0.108637, acc.: 96.09%] [G loss: 3.049295]\n",
      "epoch:3 step:2538 [D loss: 0.300457, acc.: 89.84%] [G loss: 1.790113]\n",
      "epoch:3 step:2539 [D loss: 0.326897, acc.: 85.16%] [G loss: 5.179991]\n",
      "epoch:3 step:2540 [D loss: 0.141628, acc.: 92.97%] [G loss: 4.464465]\n",
      "epoch:3 step:2541 [D loss: 0.053697, acc.: 99.22%] [G loss: 1.917284]\n",
      "epoch:3 step:2542 [D loss: 0.097637, acc.: 99.22%] [G loss: 1.886438]\n",
      "epoch:3 step:2543 [D loss: 0.109885, acc.: 96.88%] [G loss: 1.781326]\n",
      "epoch:3 step:2544 [D loss: 0.620581, acc.: 68.75%] [G loss: 4.292124]\n",
      "epoch:3 step:2545 [D loss: 1.621008, acc.: 28.12%] [G loss: 4.247603]\n",
      "epoch:3 step:2546 [D loss: 0.222878, acc.: 91.41%] [G loss: 3.726037]\n",
      "epoch:3 step:2547 [D loss: 0.322798, acc.: 85.94%] [G loss: 3.450504]\n",
      "epoch:3 step:2548 [D loss: 0.264432, acc.: 89.06%] [G loss: 3.808055]\n",
      "epoch:3 step:2549 [D loss: 0.035406, acc.: 100.00%] [G loss: 4.402985]\n",
      "epoch:3 step:2550 [D loss: 0.096030, acc.: 96.88%] [G loss: 3.839016]\n",
      "epoch:3 step:2551 [D loss: 0.136975, acc.: 96.88%] [G loss: 3.479715]\n",
      "epoch:3 step:2552 [D loss: 0.097784, acc.: 97.66%] [G loss: 3.488470]\n",
      "epoch:3 step:2553 [D loss: 0.188327, acc.: 92.97%] [G loss: 4.646832]\n",
      "epoch:3 step:2554 [D loss: 0.093356, acc.: 98.44%] [G loss: 4.276800]\n",
      "epoch:3 step:2555 [D loss: 0.101326, acc.: 99.22%] [G loss: 3.907221]\n",
      "epoch:3 step:2556 [D loss: 0.462959, acc.: 77.34%] [G loss: 5.824070]\n",
      "epoch:3 step:2557 [D loss: 0.520074, acc.: 70.31%] [G loss: 2.140209]\n",
      "epoch:3 step:2558 [D loss: 0.196077, acc.: 91.41%] [G loss: 4.340331]\n",
      "epoch:3 step:2559 [D loss: 0.068326, acc.: 99.22%] [G loss: 3.884424]\n",
      "epoch:3 step:2560 [D loss: 0.120678, acc.: 98.44%] [G loss: 4.225151]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:3 step:2561 [D loss: 0.070411, acc.: 98.44%] [G loss: 2.318437]\n",
      "epoch:3 step:2562 [D loss: 0.154312, acc.: 96.09%] [G loss: 2.912397]\n",
      "epoch:3 step:2563 [D loss: 0.260240, acc.: 88.28%] [G loss: 4.879086]\n",
      "epoch:3 step:2564 [D loss: 0.150116, acc.: 93.75%] [G loss: 3.435961]\n",
      "epoch:3 step:2565 [D loss: 0.303868, acc.: 86.72%] [G loss: 4.188374]\n",
      "epoch:3 step:2566 [D loss: 0.179593, acc.: 92.19%] [G loss: 3.161895]\n",
      "epoch:3 step:2567 [D loss: 0.051105, acc.: 99.22%] [G loss: 2.792556]\n",
      "epoch:3 step:2568 [D loss: 0.212931, acc.: 94.53%] [G loss: 1.339803]\n",
      "epoch:3 step:2569 [D loss: 0.081979, acc.: 98.44%] [G loss: 1.089655]\n",
      "epoch:3 step:2570 [D loss: 0.092015, acc.: 96.09%] [G loss: 2.532885]\n",
      "epoch:3 step:2571 [D loss: 0.117502, acc.: 96.09%] [G loss: 0.442323]\n",
      "epoch:3 step:2572 [D loss: 0.160546, acc.: 95.31%] [G loss: 0.501003]\n",
      "epoch:3 step:2573 [D loss: 0.137865, acc.: 96.09%] [G loss: 0.515149]\n",
      "epoch:3 step:2574 [D loss: 0.181961, acc.: 89.84%] [G loss: 4.052411]\n",
      "epoch:3 step:2575 [D loss: 0.211408, acc.: 92.19%] [G loss: 2.207984]\n",
      "epoch:3 step:2576 [D loss: 0.041724, acc.: 99.22%] [G loss: 1.368892]\n",
      "epoch:3 step:2577 [D loss: 0.205371, acc.: 92.19%] [G loss: 4.576453]\n",
      "epoch:3 step:2578 [D loss: 0.361183, acc.: 84.38%] [G loss: 3.556450]\n",
      "epoch:3 step:2579 [D loss: 0.757917, acc.: 63.28%] [G loss: 5.859443]\n",
      "epoch:3 step:2580 [D loss: 0.059337, acc.: 99.22%] [G loss: 6.125050]\n",
      "epoch:3 step:2581 [D loss: 0.182651, acc.: 92.19%] [G loss: 3.294146]\n",
      "epoch:3 step:2582 [D loss: 0.537916, acc.: 78.12%] [G loss: 8.226375]\n",
      "epoch:3 step:2583 [D loss: 1.272777, acc.: 50.00%] [G loss: 4.221998]\n",
      "epoch:3 step:2584 [D loss: 0.470023, acc.: 77.34%] [G loss: 6.887084]\n",
      "epoch:3 step:2585 [D loss: 0.062925, acc.: 97.66%] [G loss: 7.445227]\n",
      "epoch:3 step:2586 [D loss: 0.058856, acc.: 98.44%] [G loss: 6.165314]\n",
      "epoch:3 step:2587 [D loss: 0.110693, acc.: 95.31%] [G loss: 3.643587]\n",
      "epoch:3 step:2588 [D loss: 0.142543, acc.: 93.75%] [G loss: 4.883321]\n",
      "epoch:3 step:2589 [D loss: 0.031547, acc.: 100.00%] [G loss: 3.263994]\n",
      "epoch:3 step:2590 [D loss: 0.112382, acc.: 96.09%] [G loss: 1.690926]\n",
      "epoch:3 step:2591 [D loss: 0.130370, acc.: 94.53%] [G loss: 0.996449]\n",
      "epoch:3 step:2592 [D loss: 0.019056, acc.: 100.00%] [G loss: 1.237477]\n",
      "epoch:3 step:2593 [D loss: 0.333234, acc.: 82.03%] [G loss: 0.001070]\n",
      "epoch:3 step:2594 [D loss: 0.287829, acc.: 85.94%] [G loss: 2.395991]\n",
      "epoch:3 step:2595 [D loss: 0.007944, acc.: 100.00%] [G loss: 5.073012]\n",
      "epoch:3 step:2596 [D loss: 0.144169, acc.: 94.53%] [G loss: 1.022213]\n",
      "epoch:3 step:2597 [D loss: 0.057051, acc.: 97.66%] [G loss: 0.065066]\n",
      "epoch:3 step:2598 [D loss: 0.808265, acc.: 66.41%] [G loss: 7.439096]\n",
      "epoch:3 step:2599 [D loss: 1.135670, acc.: 58.59%] [G loss: 3.000923]\n",
      "epoch:3 step:2600 [D loss: 0.081624, acc.: 97.66%] [G loss: 1.329169]\n",
      "##############\n",
      "[0.99833795 0.87025855 0.98667019 0.94811017 0.80384874 0.90207942\n",
      " 1.08684865 0.80893751 2.11496815 2.10912653]\n",
      "##########\n",
      "epoch:3 step:2601 [D loss: 0.277091, acc.: 87.50%] [G loss: 2.924924]\n",
      "epoch:3 step:2602 [D loss: 0.031971, acc.: 100.00%] [G loss: 4.380138]\n",
      "epoch:3 step:2603 [D loss: 0.072753, acc.: 98.44%] [G loss: 2.682956]\n",
      "epoch:3 step:2604 [D loss: 0.299660, acc.: 90.62%] [G loss: 3.834980]\n",
      "epoch:3 step:2605 [D loss: 0.089275, acc.: 99.22%] [G loss: 3.060123]\n",
      "epoch:3 step:2606 [D loss: 0.858099, acc.: 48.44%] [G loss: 4.696854]\n",
      "epoch:3 step:2607 [D loss: 0.316431, acc.: 84.38%] [G loss: 5.015887]\n",
      "epoch:3 step:2608 [D loss: 0.273215, acc.: 87.50%] [G loss: 3.791825]\n",
      "epoch:3 step:2609 [D loss: 0.117599, acc.: 97.66%] [G loss: 3.538521]\n",
      "epoch:3 step:2610 [D loss: 0.293070, acc.: 89.06%] [G loss: 4.282002]\n",
      "epoch:3 step:2611 [D loss: 0.061742, acc.: 100.00%] [G loss: 4.537718]\n",
      "epoch:3 step:2612 [D loss: 0.166575, acc.: 96.88%] [G loss: 3.737162]\n",
      "epoch:3 step:2613 [D loss: 0.110977, acc.: 97.66%] [G loss: 3.493685]\n",
      "epoch:3 step:2614 [D loss: 0.070336, acc.: 100.00%] [G loss: 2.777061]\n",
      "epoch:3 step:2615 [D loss: 0.067308, acc.: 100.00%] [G loss: 2.164372]\n",
      "epoch:3 step:2616 [D loss: 0.303878, acc.: 87.50%] [G loss: 4.784772]\n",
      "epoch:3 step:2617 [D loss: 0.287920, acc.: 85.16%] [G loss: 2.617635]\n",
      "epoch:3 step:2618 [D loss: 0.374901, acc.: 81.25%] [G loss: 0.805186]\n",
      "epoch:3 step:2619 [D loss: 0.079496, acc.: 98.44%] [G loss: 1.146447]\n",
      "epoch:3 step:2620 [D loss: 0.120638, acc.: 98.44%] [G loss: 1.406936]\n",
      "epoch:3 step:2621 [D loss: 0.026008, acc.: 100.00%] [G loss: 0.856712]\n",
      "epoch:3 step:2622 [D loss: 0.035969, acc.: 100.00%] [G loss: 0.347321]\n",
      "epoch:3 step:2623 [D loss: 0.024769, acc.: 100.00%] [G loss: 0.384279]\n",
      "epoch:3 step:2624 [D loss: 0.129066, acc.: 96.09%] [G loss: 0.743597]\n",
      "epoch:3 step:2625 [D loss: 0.355540, acc.: 86.72%] [G loss: 1.008237]\n",
      "epoch:3 step:2626 [D loss: 0.040726, acc.: 99.22%] [G loss: 1.830839]\n",
      "epoch:3 step:2627 [D loss: 0.257390, acc.: 91.41%] [G loss: 1.769528]\n",
      "epoch:3 step:2628 [D loss: 0.063028, acc.: 98.44%] [G loss: 3.284052]\n",
      "epoch:3 step:2629 [D loss: 0.715501, acc.: 64.06%] [G loss: 2.699910]\n",
      "epoch:3 step:2630 [D loss: 0.099629, acc.: 96.88%] [G loss: 3.259988]\n",
      "epoch:3 step:2631 [D loss: 0.216338, acc.: 92.97%] [G loss: 2.553706]\n",
      "epoch:3 step:2632 [D loss: 0.238407, acc.: 89.06%] [G loss: 4.121896]\n",
      "epoch:3 step:2633 [D loss: 0.160626, acc.: 96.09%] [G loss: 3.222669]\n",
      "epoch:3 step:2634 [D loss: 0.040367, acc.: 100.00%] [G loss: 2.127791]\n",
      "epoch:3 step:2635 [D loss: 0.478930, acc.: 79.69%] [G loss: 5.292953]\n",
      "epoch:3 step:2636 [D loss: 0.640908, acc.: 70.31%] [G loss: 2.119980]\n",
      "epoch:3 step:2637 [D loss: 0.262576, acc.: 91.41%] [G loss: 4.972384]\n",
      "epoch:3 step:2638 [D loss: 0.623468, acc.: 68.75%] [G loss: 5.051239]\n",
      "epoch:3 step:2639 [D loss: 0.079737, acc.: 96.88%] [G loss: 5.512969]\n",
      "epoch:3 step:2640 [D loss: 0.070117, acc.: 98.44%] [G loss: 3.977286]\n",
      "epoch:3 step:2641 [D loss: 0.265829, acc.: 89.06%] [G loss: 5.203858]\n",
      "epoch:3 step:2642 [D loss: 0.215030, acc.: 91.41%] [G loss: 4.228344]\n",
      "epoch:3 step:2643 [D loss: 0.221810, acc.: 89.84%] [G loss: 4.028381]\n",
      "epoch:3 step:2644 [D loss: 0.525702, acc.: 75.00%] [G loss: 7.137273]\n",
      "epoch:3 step:2645 [D loss: 0.290679, acc.: 86.72%] [G loss: 5.344138]\n",
      "epoch:3 step:2646 [D loss: 0.201650, acc.: 93.75%] [G loss: 3.979865]\n",
      "epoch:3 step:2647 [D loss: 0.197918, acc.: 92.19%] [G loss: 5.609731]\n",
      "epoch:3 step:2648 [D loss: 0.355936, acc.: 83.59%] [G loss: 3.107675]\n",
      "epoch:3 step:2649 [D loss: 0.118838, acc.: 96.88%] [G loss: 3.804405]\n",
      "epoch:3 step:2650 [D loss: 0.196453, acc.: 92.97%] [G loss: 3.191780]\n",
      "epoch:3 step:2651 [D loss: 0.068667, acc.: 99.22%] [G loss: 3.147786]\n",
      "epoch:3 step:2652 [D loss: 0.201338, acc.: 93.75%] [G loss: 3.105327]\n",
      "epoch:3 step:2653 [D loss: 0.796200, acc.: 66.41%] [G loss: 6.883878]\n",
      "epoch:3 step:2654 [D loss: 0.241805, acc.: 88.28%] [G loss: 6.363479]\n",
      "epoch:3 step:2655 [D loss: 0.210035, acc.: 91.41%] [G loss: 1.504845]\n",
      "epoch:3 step:2656 [D loss: 0.413249, acc.: 81.25%] [G loss: 4.596142]\n",
      "epoch:3 step:2657 [D loss: 0.260449, acc.: 89.84%] [G loss: 3.184883]\n",
      "epoch:3 step:2658 [D loss: 0.040064, acc.: 100.00%] [G loss: 2.409431]\n",
      "epoch:3 step:2659 [D loss: 0.213331, acc.: 91.41%] [G loss: 1.697940]\n",
      "epoch:3 step:2660 [D loss: 0.123275, acc.: 96.09%] [G loss: 1.142846]\n",
      "epoch:3 step:2661 [D loss: 0.437551, acc.: 78.12%] [G loss: 5.629967]\n",
      "epoch:3 step:2662 [D loss: 1.316883, acc.: 54.69%] [G loss: 1.722800]\n",
      "epoch:3 step:2663 [D loss: 0.276794, acc.: 87.50%] [G loss: 4.111094]\n",
      "epoch:3 step:2664 [D loss: 0.075847, acc.: 97.66%] [G loss: 4.458969]\n",
      "epoch:3 step:2665 [D loss: 0.286278, acc.: 87.50%] [G loss: 2.177851]\n",
      "epoch:3 step:2666 [D loss: 0.236364, acc.: 89.06%] [G loss: 4.278291]\n",
      "epoch:3 step:2667 [D loss: 0.268066, acc.: 87.50%] [G loss: 3.985004]\n",
      "epoch:3 step:2668 [D loss: 0.343669, acc.: 85.94%] [G loss: 4.766971]\n",
      "epoch:3 step:2669 [D loss: 0.253866, acc.: 89.84%] [G loss: 3.462300]\n",
      "epoch:3 step:2670 [D loss: 0.563752, acc.: 75.00%] [G loss: 5.228949]\n",
      "epoch:3 step:2671 [D loss: 0.244226, acc.: 89.06%] [G loss: 4.571629]\n",
      "epoch:3 step:2672 [D loss: 0.129777, acc.: 97.66%] [G loss: 4.590592]\n",
      "epoch:3 step:2673 [D loss: 0.039280, acc.: 100.00%] [G loss: 3.685561]\n",
      "epoch:3 step:2674 [D loss: 0.505464, acc.: 79.69%] [G loss: 6.401875]\n",
      "epoch:3 step:2675 [D loss: 0.148568, acc.: 95.31%] [G loss: 6.769207]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:3 step:2676 [D loss: 0.063689, acc.: 97.66%] [G loss: 5.250793]\n",
      "epoch:3 step:2677 [D loss: 0.107539, acc.: 96.09%] [G loss: 3.122178]\n",
      "epoch:3 step:2678 [D loss: 0.380059, acc.: 79.69%] [G loss: 5.779159]\n",
      "epoch:3 step:2679 [D loss: 0.133312, acc.: 95.31%] [G loss: 4.911655]\n",
      "epoch:3 step:2680 [D loss: 0.146630, acc.: 95.31%] [G loss: 3.146076]\n",
      "epoch:3 step:2681 [D loss: 0.348311, acc.: 84.38%] [G loss: 4.979338]\n",
      "epoch:3 step:2682 [D loss: 0.172114, acc.: 92.97%] [G loss: 4.545479]\n",
      "epoch:3 step:2683 [D loss: 0.749869, acc.: 62.50%] [G loss: 4.251506]\n",
      "epoch:3 step:2684 [D loss: 0.271291, acc.: 87.50%] [G loss: 3.656096]\n",
      "epoch:3 step:2685 [D loss: 0.127632, acc.: 95.31%] [G loss: 2.147034]\n",
      "epoch:3 step:2686 [D loss: 0.285767, acc.: 86.72%] [G loss: 3.606465]\n",
      "epoch:3 step:2687 [D loss: 0.160244, acc.: 94.53%] [G loss: 4.320179]\n",
      "epoch:3 step:2688 [D loss: 0.211196, acc.: 94.53%] [G loss: 3.612168]\n",
      "epoch:3 step:2689 [D loss: 0.106081, acc.: 97.66%] [G loss: 3.249170]\n",
      "epoch:3 step:2690 [D loss: 0.369649, acc.: 86.72%] [G loss: 4.808514]\n",
      "epoch:3 step:2691 [D loss: 0.078316, acc.: 96.88%] [G loss: 4.417777]\n",
      "epoch:3 step:2692 [D loss: 0.137970, acc.: 96.09%] [G loss: 2.347127]\n",
      "epoch:3 step:2693 [D loss: 0.476712, acc.: 75.00%] [G loss: 5.928230]\n",
      "epoch:3 step:2694 [D loss: 0.375815, acc.: 83.59%] [G loss: 4.416514]\n",
      "epoch:3 step:2695 [D loss: 0.249112, acc.: 92.19%] [G loss: 4.931320]\n",
      "epoch:3 step:2696 [D loss: 0.150009, acc.: 95.31%] [G loss: 4.398927]\n",
      "epoch:3 step:2697 [D loss: 0.178108, acc.: 92.97%] [G loss: 4.387185]\n",
      "epoch:3 step:2698 [D loss: 0.108118, acc.: 95.31%] [G loss: 3.572208]\n",
      "epoch:3 step:2699 [D loss: 0.074598, acc.: 99.22%] [G loss: 3.456880]\n",
      "epoch:3 step:2700 [D loss: 0.100487, acc.: 99.22%] [G loss: 4.247894]\n",
      "epoch:3 step:2701 [D loss: 0.224878, acc.: 91.41%] [G loss: 0.408043]\n",
      "epoch:3 step:2702 [D loss: 0.049222, acc.: 97.66%] [G loss: 0.873101]\n",
      "epoch:3 step:2703 [D loss: 0.028964, acc.: 100.00%] [G loss: 0.446931]\n",
      "epoch:3 step:2704 [D loss: 0.053837, acc.: 100.00%] [G loss: 2.126871]\n",
      "epoch:3 step:2705 [D loss: 0.020741, acc.: 100.00%] [G loss: 0.376875]\n",
      "epoch:3 step:2706 [D loss: 0.253561, acc.: 89.06%] [G loss: 3.682092]\n",
      "epoch:3 step:2707 [D loss: 0.357643, acc.: 82.03%] [G loss: 0.481596]\n",
      "epoch:3 step:2708 [D loss: 0.052103, acc.: 100.00%] [G loss: 0.106113]\n",
      "epoch:3 step:2709 [D loss: 0.065011, acc.: 96.88%] [G loss: 0.437495]\n",
      "epoch:3 step:2710 [D loss: 0.080123, acc.: 100.00%] [G loss: 0.401385]\n",
      "epoch:3 step:2711 [D loss: 0.147271, acc.: 96.09%] [G loss: 2.338393]\n",
      "epoch:3 step:2712 [D loss: 0.126889, acc.: 96.88%] [G loss: 1.365877]\n",
      "epoch:3 step:2713 [D loss: 1.507489, acc.: 48.44%] [G loss: 9.874918]\n",
      "epoch:3 step:2714 [D loss: 2.795201, acc.: 50.00%] [G loss: 4.379510]\n",
      "epoch:3 step:2715 [D loss: 0.701654, acc.: 67.19%] [G loss: 3.437969]\n",
      "epoch:3 step:2716 [D loss: 0.417943, acc.: 77.34%] [G loss: 3.195931]\n",
      "epoch:3 step:2717 [D loss: 0.151249, acc.: 95.31%] [G loss: 4.098972]\n",
      "epoch:3 step:2718 [D loss: 0.143625, acc.: 96.88%] [G loss: 4.377303]\n",
      "epoch:3 step:2719 [D loss: 0.077884, acc.: 98.44%] [G loss: 3.803410]\n",
      "epoch:3 step:2720 [D loss: 0.475899, acc.: 78.91%] [G loss: 3.871044]\n",
      "epoch:3 step:2721 [D loss: 0.159434, acc.: 92.97%] [G loss: 3.326872]\n",
      "epoch:3 step:2722 [D loss: 0.493551, acc.: 83.59%] [G loss: 3.811681]\n",
      "epoch:3 step:2723 [D loss: 0.084419, acc.: 98.44%] [G loss: 4.681637]\n",
      "epoch:3 step:2724 [D loss: 0.159041, acc.: 95.31%] [G loss: 2.797558]\n",
      "epoch:3 step:2725 [D loss: 0.216620, acc.: 92.19%] [G loss: 1.693629]\n",
      "epoch:3 step:2726 [D loss: 0.126417, acc.: 98.44%] [G loss: 2.051529]\n",
      "epoch:3 step:2727 [D loss: 0.085614, acc.: 100.00%] [G loss: 1.773080]\n",
      "epoch:3 step:2728 [D loss: 0.186860, acc.: 92.97%] [G loss: 0.624138]\n",
      "epoch:3 step:2729 [D loss: 0.065270, acc.: 98.44%] [G loss: 0.110708]\n",
      "epoch:3 step:2730 [D loss: 0.037463, acc.: 100.00%] [G loss: 0.200826]\n",
      "epoch:3 step:2731 [D loss: 0.298238, acc.: 87.50%] [G loss: 0.482082]\n",
      "epoch:3 step:2732 [D loss: 0.026455, acc.: 100.00%] [G loss: 0.416110]\n",
      "epoch:3 step:2733 [D loss: 0.023001, acc.: 100.00%] [G loss: 0.902890]\n",
      "epoch:3 step:2734 [D loss: 0.258913, acc.: 88.28%] [G loss: 0.354756]\n",
      "epoch:3 step:2735 [D loss: 0.029220, acc.: 100.00%] [G loss: 0.890173]\n",
      "epoch:3 step:2736 [D loss: 0.078639, acc.: 98.44%] [G loss: 0.546803]\n",
      "epoch:3 step:2737 [D loss: 0.045338, acc.: 99.22%] [G loss: 0.261499]\n",
      "epoch:3 step:2738 [D loss: 0.429101, acc.: 79.69%] [G loss: 5.043123]\n",
      "epoch:3 step:2739 [D loss: 0.850681, acc.: 64.06%] [G loss: 0.673341]\n",
      "epoch:3 step:2740 [D loss: 0.665852, acc.: 67.97%] [G loss: 5.918474]\n",
      "epoch:3 step:2741 [D loss: 0.271157, acc.: 87.50%] [G loss: 5.221788]\n",
      "epoch:3 step:2742 [D loss: 0.155214, acc.: 96.88%] [G loss: 2.679459]\n",
      "epoch:3 step:2743 [D loss: 0.241908, acc.: 89.06%] [G loss: 3.650491]\n",
      "epoch:3 step:2744 [D loss: 0.075282, acc.: 98.44%] [G loss: 2.805021]\n",
      "epoch:3 step:2745 [D loss: 0.395079, acc.: 83.59%] [G loss: 3.750476]\n",
      "epoch:3 step:2746 [D loss: 0.138528, acc.: 95.31%] [G loss: 3.308200]\n",
      "epoch:3 step:2747 [D loss: 0.258869, acc.: 91.41%] [G loss: 2.899259]\n",
      "epoch:3 step:2748 [D loss: 0.840445, acc.: 58.59%] [G loss: 3.995442]\n",
      "epoch:3 step:2749 [D loss: 0.093311, acc.: 96.88%] [G loss: 3.549078]\n",
      "epoch:3 step:2750 [D loss: 0.184445, acc.: 94.53%] [G loss: 3.051256]\n",
      "epoch:3 step:2751 [D loss: 0.185691, acc.: 94.53%] [G loss: 3.566182]\n",
      "epoch:3 step:2752 [D loss: 0.086309, acc.: 97.66%] [G loss: 3.002060]\n",
      "epoch:3 step:2753 [D loss: 0.203321, acc.: 96.09%] [G loss: 2.833177]\n",
      "epoch:3 step:2754 [D loss: 1.181629, acc.: 39.06%] [G loss: 5.856450]\n",
      "epoch:3 step:2755 [D loss: 0.318122, acc.: 82.81%] [G loss: 5.622801]\n",
      "epoch:3 step:2756 [D loss: 0.242241, acc.: 90.62%] [G loss: 2.964834]\n",
      "epoch:3 step:2757 [D loss: 0.089780, acc.: 96.09%] [G loss: 2.539964]\n",
      "epoch:3 step:2758 [D loss: 0.234749, acc.: 91.41%] [G loss: 4.325381]\n",
      "epoch:3 step:2759 [D loss: 0.149522, acc.: 96.09%] [G loss: 3.761593]\n",
      "epoch:3 step:2760 [D loss: 0.260200, acc.: 89.06%] [G loss: 2.349861]\n",
      "epoch:3 step:2761 [D loss: 0.250209, acc.: 89.84%] [G loss: 2.979679]\n",
      "epoch:3 step:2762 [D loss: 0.116976, acc.: 98.44%] [G loss: 3.857631]\n",
      "epoch:3 step:2763 [D loss: 0.270230, acc.: 91.41%] [G loss: 1.200460]\n",
      "epoch:3 step:2764 [D loss: 0.145842, acc.: 96.88%] [G loss: 1.944236]\n",
      "epoch:3 step:2765 [D loss: 0.103064, acc.: 98.44%] [G loss: 2.834285]\n",
      "epoch:3 step:2766 [D loss: 0.189518, acc.: 92.97%] [G loss: 1.189664]\n",
      "epoch:3 step:2767 [D loss: 0.271630, acc.: 90.62%] [G loss: 2.199269]\n",
      "epoch:3 step:2768 [D loss: 0.054501, acc.: 99.22%] [G loss: 2.737020]\n",
      "epoch:3 step:2769 [D loss: 0.315974, acc.: 89.06%] [G loss: 2.414178]\n",
      "epoch:3 step:2770 [D loss: 0.055013, acc.: 100.00%] [G loss: 1.452012]\n",
      "epoch:3 step:2771 [D loss: 0.333087, acc.: 84.38%] [G loss: 5.291346]\n",
      "epoch:3 step:2772 [D loss: 0.532883, acc.: 76.56%] [G loss: 2.413121]\n",
      "epoch:3 step:2773 [D loss: 0.210971, acc.: 88.28%] [G loss: 3.799216]\n",
      "epoch:3 step:2774 [D loss: 0.050350, acc.: 99.22%] [G loss: 3.312147]\n",
      "epoch:3 step:2775 [D loss: 0.071911, acc.: 99.22%] [G loss: 3.106692]\n",
      "epoch:3 step:2776 [D loss: 0.167910, acc.: 96.09%] [G loss: 1.181963]\n",
      "epoch:3 step:2777 [D loss: 1.216727, acc.: 49.22%] [G loss: 6.847353]\n",
      "epoch:3 step:2778 [D loss: 0.764021, acc.: 61.72%] [G loss: 5.411528]\n",
      "epoch:3 step:2779 [D loss: 0.121046, acc.: 95.31%] [G loss: 3.416056]\n",
      "epoch:3 step:2780 [D loss: 0.156497, acc.: 92.19%] [G loss: 4.007312]\n",
      "epoch:3 step:2781 [D loss: 0.112015, acc.: 97.66%] [G loss: 2.332543]\n",
      "epoch:3 step:2782 [D loss: 0.410059, acc.: 75.78%] [G loss: 5.330919]\n",
      "epoch:3 step:2783 [D loss: 0.422874, acc.: 78.91%] [G loss: 3.228496]\n",
      "epoch:3 step:2784 [D loss: 0.228861, acc.: 91.41%] [G loss: 4.626780]\n",
      "epoch:3 step:2785 [D loss: 0.117660, acc.: 97.66%] [G loss: 4.156415]\n",
      "epoch:3 step:2786 [D loss: 0.158470, acc.: 95.31%] [G loss: 3.652704]\n",
      "epoch:3 step:2787 [D loss: 0.282758, acc.: 87.50%] [G loss: 3.519652]\n",
      "epoch:3 step:2788 [D loss: 0.093911, acc.: 99.22%] [G loss: 4.202373]\n",
      "epoch:3 step:2789 [D loss: 0.151698, acc.: 96.09%] [G loss: 5.299965]\n",
      "epoch:3 step:2790 [D loss: 0.276441, acc.: 89.06%] [G loss: 2.267622]\n",
      "epoch:3 step:2791 [D loss: 0.080165, acc.: 99.22%] [G loss: 2.439449]\n",
      "epoch:3 step:2792 [D loss: 0.035254, acc.: 100.00%] [G loss: 2.807815]\n",
      "epoch:3 step:2793 [D loss: 0.150764, acc.: 94.53%] [G loss: 4.772934]\n",
      "epoch:3 step:2794 [D loss: 0.102942, acc.: 96.09%] [G loss: 2.953412]\n",
      "epoch:3 step:2795 [D loss: 0.110450, acc.: 96.09%] [G loss: 0.106011]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:3 step:2796 [D loss: 0.627073, acc.: 71.09%] [G loss: 7.876629]\n",
      "epoch:3 step:2797 [D loss: 1.399746, acc.: 53.12%] [G loss: 3.056632]\n",
      "epoch:3 step:2798 [D loss: 0.142971, acc.: 94.53%] [G loss: 2.388989]\n",
      "epoch:3 step:2799 [D loss: 0.367941, acc.: 80.47%] [G loss: 5.965110]\n",
      "epoch:3 step:2800 [D loss: 0.259912, acc.: 86.72%] [G loss: 4.913760]\n",
      "##############\n",
      "[1.02725629 1.08534653 0.91533116 0.99344678 2.11054617 2.11140778\n",
      " 0.81345881 2.10583818 2.11396046 2.1114392 ]\n",
      "##########\n",
      "epoch:3 step:2801 [D loss: 0.163051, acc.: 96.09%] [G loss: 2.294521]\n",
      "epoch:3 step:2802 [D loss: 0.351476, acc.: 82.03%] [G loss: 5.519481]\n",
      "epoch:3 step:2803 [D loss: 0.441350, acc.: 75.78%] [G loss: 5.381677]\n",
      "epoch:3 step:2804 [D loss: 0.435264, acc.: 82.81%] [G loss: 4.992230]\n",
      "epoch:3 step:2805 [D loss: 0.079222, acc.: 99.22%] [G loss: 4.938598]\n",
      "epoch:3 step:2806 [D loss: 0.316985, acc.: 85.94%] [G loss: 3.968953]\n",
      "epoch:3 step:2807 [D loss: 0.094206, acc.: 99.22%] [G loss: 4.428136]\n",
      "epoch:3 step:2808 [D loss: 0.141713, acc.: 97.66%] [G loss: 4.517300]\n",
      "epoch:3 step:2809 [D loss: 0.167393, acc.: 92.97%] [G loss: 3.880025]\n",
      "epoch:3 step:2810 [D loss: 0.099459, acc.: 99.22%] [G loss: 2.634714]\n",
      "epoch:3 step:2811 [D loss: 0.097699, acc.: 98.44%] [G loss: 3.813740]\n",
      "epoch:3 step:2812 [D loss: 0.139211, acc.: 96.88%] [G loss: 2.875422]\n",
      "epoch:3 step:2813 [D loss: 0.168382, acc.: 97.66%] [G loss: 4.559320]\n",
      "epoch:3 step:2814 [D loss: 0.495102, acc.: 78.91%] [G loss: 5.109382]\n",
      "epoch:3 step:2815 [D loss: 0.386193, acc.: 85.16%] [G loss: 4.229251]\n",
      "epoch:3 step:2816 [D loss: 0.119403, acc.: 99.22%] [G loss: 2.230978]\n",
      "epoch:3 step:2817 [D loss: 0.495698, acc.: 72.66%] [G loss: 5.240782]\n",
      "epoch:3 step:2818 [D loss: 0.352935, acc.: 86.72%] [G loss: 3.803912]\n",
      "epoch:3 step:2819 [D loss: 0.116591, acc.: 99.22%] [G loss: 3.329808]\n",
      "epoch:3 step:2820 [D loss: 0.073748, acc.: 99.22%] [G loss: 3.047242]\n",
      "epoch:3 step:2821 [D loss: 0.426585, acc.: 82.03%] [G loss: 4.700459]\n",
      "epoch:3 step:2822 [D loss: 0.432118, acc.: 82.03%] [G loss: 3.672123]\n",
      "epoch:3 step:2823 [D loss: 0.198965, acc.: 92.97%] [G loss: 4.266394]\n",
      "epoch:3 step:2824 [D loss: 0.171339, acc.: 93.75%] [G loss: 3.083541]\n",
      "epoch:3 step:2825 [D loss: 0.096267, acc.: 98.44%] [G loss: 3.045421]\n",
      "epoch:3 step:2826 [D loss: 0.182059, acc.: 94.53%] [G loss: 3.445790]\n",
      "epoch:3 step:2827 [D loss: 0.094775, acc.: 98.44%] [G loss: 1.986382]\n",
      "epoch:3 step:2828 [D loss: 0.647911, acc.: 64.84%] [G loss: 5.487193]\n",
      "epoch:3 step:2829 [D loss: 0.204580, acc.: 94.53%] [G loss: 5.125758]\n",
      "epoch:3 step:2830 [D loss: 0.502335, acc.: 75.78%] [G loss: 2.781869]\n",
      "epoch:3 step:2831 [D loss: 0.312470, acc.: 88.28%] [G loss: 3.098178]\n",
      "epoch:3 step:2832 [D loss: 0.060313, acc.: 99.22%] [G loss: 4.600059]\n",
      "epoch:3 step:2833 [D loss: 0.260443, acc.: 91.41%] [G loss: 2.944602]\n",
      "epoch:3 step:2834 [D loss: 0.181542, acc.: 93.75%] [G loss: 3.948827]\n",
      "epoch:3 step:2835 [D loss: 0.144024, acc.: 96.09%] [G loss: 2.733677]\n",
      "epoch:3 step:2836 [D loss: 0.160727, acc.: 94.53%] [G loss: 3.609992]\n",
      "epoch:3 step:2837 [D loss: 0.173522, acc.: 94.53%] [G loss: 4.524017]\n",
      "epoch:3 step:2838 [D loss: 0.276515, acc.: 89.84%] [G loss: 5.276206]\n",
      "epoch:3 step:2839 [D loss: 0.138582, acc.: 95.31%] [G loss: 2.109382]\n",
      "epoch:3 step:2840 [D loss: 0.407921, acc.: 77.34%] [G loss: 7.310789]\n",
      "epoch:3 step:2841 [D loss: 0.563902, acc.: 71.88%] [G loss: 3.361497]\n",
      "epoch:3 step:2842 [D loss: 0.354325, acc.: 82.81%] [G loss: 5.216963]\n",
      "epoch:3 step:2843 [D loss: 0.059122, acc.: 100.00%] [G loss: 5.400876]\n",
      "epoch:3 step:2844 [D loss: 0.371944, acc.: 82.03%] [G loss: 3.464221]\n",
      "epoch:3 step:2845 [D loss: 0.267237, acc.: 87.50%] [G loss: 6.237208]\n",
      "epoch:3 step:2846 [D loss: 0.388296, acc.: 79.69%] [G loss: 2.637715]\n",
      "epoch:3 step:2847 [D loss: 0.468585, acc.: 78.91%] [G loss: 7.151766]\n",
      "epoch:3 step:2848 [D loss: 0.331074, acc.: 85.16%] [G loss: 5.542180]\n",
      "epoch:3 step:2849 [D loss: 0.243211, acc.: 89.06%] [G loss: 4.313263]\n",
      "epoch:3 step:2850 [D loss: 0.106998, acc.: 96.88%] [G loss: 2.748949]\n",
      "epoch:3 step:2851 [D loss: 0.249029, acc.: 89.06%] [G loss: 5.765538]\n",
      "epoch:3 step:2852 [D loss: 0.879502, acc.: 58.59%] [G loss: 2.446580]\n",
      "epoch:3 step:2853 [D loss: 0.240293, acc.: 87.50%] [G loss: 5.351108]\n",
      "epoch:3 step:2854 [D loss: 0.243248, acc.: 90.62%] [G loss: 4.051535]\n",
      "epoch:3 step:2855 [D loss: 0.164955, acc.: 95.31%] [G loss: 4.128798]\n",
      "epoch:3 step:2856 [D loss: 0.146579, acc.: 94.53%] [G loss: 4.606349]\n",
      "epoch:3 step:2857 [D loss: 0.308309, acc.: 85.16%] [G loss: 4.098464]\n",
      "epoch:3 step:2858 [D loss: 0.137103, acc.: 94.53%] [G loss: 4.436486]\n",
      "epoch:3 step:2859 [D loss: 0.100132, acc.: 99.22%] [G loss: 3.697373]\n",
      "epoch:3 step:2860 [D loss: 0.225999, acc.: 92.19%] [G loss: 3.286824]\n",
      "epoch:3 step:2861 [D loss: 0.114420, acc.: 96.09%] [G loss: 3.668684]\n",
      "epoch:3 step:2862 [D loss: 0.394228, acc.: 80.47%] [G loss: 3.649927]\n",
      "epoch:3 step:2863 [D loss: 0.075393, acc.: 97.66%] [G loss: 3.378419]\n",
      "epoch:3 step:2864 [D loss: 0.208300, acc.: 92.19%] [G loss: 2.844526]\n",
      "epoch:3 step:2865 [D loss: 0.392817, acc.: 84.38%] [G loss: 5.659416]\n",
      "epoch:3 step:2866 [D loss: 0.950419, acc.: 57.81%] [G loss: 1.021516]\n",
      "epoch:3 step:2867 [D loss: 0.296357, acc.: 85.16%] [G loss: 4.188509]\n",
      "epoch:3 step:2868 [D loss: 0.060571, acc.: 97.66%] [G loss: 4.823880]\n",
      "epoch:3 step:2869 [D loss: 0.899887, acc.: 57.81%] [G loss: 2.487571]\n",
      "epoch:3 step:2870 [D loss: 0.188833, acc.: 92.97%] [G loss: 3.687464]\n",
      "epoch:3 step:2871 [D loss: 0.138813, acc.: 95.31%] [G loss: 2.928386]\n",
      "epoch:3 step:2872 [D loss: 0.441941, acc.: 79.69%] [G loss: 5.580208]\n",
      "epoch:3 step:2873 [D loss: 0.591620, acc.: 76.56%] [G loss: 2.449501]\n",
      "epoch:3 step:2874 [D loss: 0.448152, acc.: 77.34%] [G loss: 4.414757]\n",
      "epoch:3 step:2875 [D loss: 0.153210, acc.: 92.97%] [G loss: 4.532663]\n",
      "epoch:3 step:2876 [D loss: 0.377135, acc.: 83.59%] [G loss: 2.544301]\n",
      "epoch:3 step:2877 [D loss: 0.345962, acc.: 83.59%] [G loss: 5.860022]\n",
      "epoch:3 step:2878 [D loss: 0.340951, acc.: 83.59%] [G loss: 2.868178]\n",
      "epoch:3 step:2879 [D loss: 0.419522, acc.: 82.03%] [G loss: 4.055304]\n",
      "epoch:3 step:2880 [D loss: 0.149454, acc.: 92.97%] [G loss: 3.650706]\n",
      "epoch:3 step:2881 [D loss: 0.285098, acc.: 90.62%] [G loss: 4.250517]\n",
      "epoch:3 step:2882 [D loss: 0.245410, acc.: 92.97%] [G loss: 3.539148]\n",
      "epoch:3 step:2883 [D loss: 0.151316, acc.: 96.88%] [G loss: 4.720490]\n",
      "epoch:3 step:2884 [D loss: 0.290777, acc.: 89.06%] [G loss: 3.739268]\n",
      "epoch:3 step:2885 [D loss: 0.318625, acc.: 87.50%] [G loss: 4.442621]\n",
      "epoch:3 step:2886 [D loss: 0.100260, acc.: 98.44%] [G loss: 3.324287]\n",
      "epoch:3 step:2887 [D loss: 0.043801, acc.: 100.00%] [G loss: 1.555073]\n",
      "epoch:3 step:2888 [D loss: 0.293159, acc.: 86.72%] [G loss: 4.021546]\n",
      "epoch:3 step:2889 [D loss: 0.154313, acc.: 93.75%] [G loss: 4.268334]\n",
      "epoch:3 step:2890 [D loss: 0.360333, acc.: 83.59%] [G loss: 1.649545]\n",
      "epoch:3 step:2891 [D loss: 0.295175, acc.: 85.16%] [G loss: 3.687553]\n",
      "epoch:3 step:2892 [D loss: 0.079939, acc.: 97.66%] [G loss: 3.661922]\n",
      "epoch:3 step:2893 [D loss: 0.839634, acc.: 53.12%] [G loss: 5.143465]\n",
      "epoch:3 step:2894 [D loss: 0.120343, acc.: 96.09%] [G loss: 4.929605]\n",
      "epoch:3 step:2895 [D loss: 0.107941, acc.: 96.88%] [G loss: 3.397526]\n",
      "epoch:3 step:2896 [D loss: 0.152216, acc.: 93.75%] [G loss: 3.339904]\n",
      "epoch:3 step:2897 [D loss: 0.186603, acc.: 94.53%] [G loss: 3.811860]\n",
      "epoch:3 step:2898 [D loss: 0.346243, acc.: 85.16%] [G loss: 3.690279]\n",
      "epoch:3 step:2899 [D loss: 0.096189, acc.: 99.22%] [G loss: 4.155203]\n",
      "epoch:3 step:2900 [D loss: 0.330692, acc.: 83.59%] [G loss: 4.610670]\n",
      "epoch:3 step:2901 [D loss: 0.217879, acc.: 92.19%] [G loss: 3.701912]\n",
      "epoch:3 step:2902 [D loss: 0.156797, acc.: 96.09%] [G loss: 3.450136]\n",
      "epoch:3 step:2903 [D loss: 0.113144, acc.: 99.22%] [G loss: 4.155703]\n",
      "epoch:3 step:2904 [D loss: 0.241451, acc.: 92.19%] [G loss: 4.090478]\n",
      "epoch:3 step:2905 [D loss: 0.343607, acc.: 84.38%] [G loss: 4.706585]\n",
      "epoch:3 step:2906 [D loss: 0.147583, acc.: 94.53%] [G loss: 4.580444]\n",
      "epoch:3 step:2907 [D loss: 0.360719, acc.: 82.03%] [G loss: 4.251530]\n",
      "epoch:3 step:2908 [D loss: 0.059600, acc.: 99.22%] [G loss: 4.982642]\n",
      "epoch:3 step:2909 [D loss: 0.298534, acc.: 86.72%] [G loss: 4.333223]\n",
      "epoch:3 step:2910 [D loss: 0.059967, acc.: 98.44%] [G loss: 3.886680]\n",
      "epoch:3 step:2911 [D loss: 0.208974, acc.: 91.41%] [G loss: 4.958717]\n",
      "epoch:3 step:2912 [D loss: 0.215955, acc.: 92.19%] [G loss: 4.548935]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:3 step:2913 [D loss: 0.442922, acc.: 82.81%] [G loss: 4.408030]\n",
      "epoch:3 step:2914 [D loss: 0.238998, acc.: 89.84%] [G loss: 4.624546]\n",
      "epoch:3 step:2915 [D loss: 0.033046, acc.: 100.00%] [G loss: 3.606539]\n",
      "epoch:3 step:2916 [D loss: 0.224856, acc.: 91.41%] [G loss: 3.978608]\n",
      "epoch:3 step:2917 [D loss: 0.144561, acc.: 92.19%] [G loss: 2.268842]\n",
      "epoch:3 step:2918 [D loss: 0.500573, acc.: 75.00%] [G loss: 6.424181]\n",
      "epoch:3 step:2919 [D loss: 0.377833, acc.: 81.25%] [G loss: 3.240722]\n",
      "epoch:3 step:2920 [D loss: 0.160572, acc.: 94.53%] [G loss: 2.923484]\n",
      "epoch:3 step:2921 [D loss: 0.077897, acc.: 98.44%] [G loss: 3.980035]\n",
      "epoch:3 step:2922 [D loss: 0.083682, acc.: 100.00%] [G loss: 3.511827]\n",
      "epoch:3 step:2923 [D loss: 0.206774, acc.: 95.31%] [G loss: 5.848531]\n",
      "epoch:3 step:2924 [D loss: 0.249882, acc.: 89.84%] [G loss: 2.645126]\n",
      "epoch:3 step:2925 [D loss: 0.231404, acc.: 87.50%] [G loss: 4.647995]\n",
      "epoch:3 step:2926 [D loss: 0.113358, acc.: 95.31%] [G loss: 3.977228]\n",
      "epoch:3 step:2927 [D loss: 0.113465, acc.: 97.66%] [G loss: 1.990509]\n",
      "epoch:3 step:2928 [D loss: 0.400448, acc.: 82.81%] [G loss: 4.997678]\n",
      "epoch:3 step:2929 [D loss: 0.387856, acc.: 81.25%] [G loss: 2.068933]\n",
      "epoch:3 step:2930 [D loss: 0.286418, acc.: 88.28%] [G loss: 4.388492]\n",
      "epoch:3 step:2931 [D loss: 0.211466, acc.: 91.41%] [G loss: 2.840283]\n",
      "epoch:3 step:2932 [D loss: 0.833039, acc.: 67.19%] [G loss: 7.550498]\n",
      "epoch:3 step:2933 [D loss: 1.465508, acc.: 54.69%] [G loss: 4.492440]\n",
      "epoch:3 step:2934 [D loss: 0.171314, acc.: 92.19%] [G loss: 4.545207]\n",
      "epoch:3 step:2935 [D loss: 0.193293, acc.: 95.31%] [G loss: 3.532874]\n",
      "epoch:3 step:2936 [D loss: 0.191766, acc.: 95.31%] [G loss: 3.770514]\n",
      "epoch:3 step:2937 [D loss: 0.228235, acc.: 91.41%] [G loss: 4.435441]\n",
      "epoch:3 step:2938 [D loss: 0.179412, acc.: 96.09%] [G loss: 4.674119]\n",
      "epoch:3 step:2939 [D loss: 0.191350, acc.: 94.53%] [G loss: 4.217739]\n",
      "epoch:3 step:2940 [D loss: 0.164160, acc.: 93.75%] [G loss: 2.605778]\n",
      "epoch:3 step:2941 [D loss: 0.225359, acc.: 89.84%] [G loss: 4.785115]\n",
      "epoch:3 step:2942 [D loss: 0.072687, acc.: 98.44%] [G loss: 4.595332]\n",
      "epoch:3 step:2943 [D loss: 0.090541, acc.: 99.22%] [G loss: 3.004834]\n",
      "epoch:3 step:2944 [D loss: 0.157168, acc.: 94.53%] [G loss: 3.365479]\n",
      "epoch:3 step:2945 [D loss: 0.159596, acc.: 95.31%] [G loss: 2.577280]\n",
      "epoch:3 step:2946 [D loss: 0.112706, acc.: 96.88%] [G loss: 2.752085]\n",
      "epoch:3 step:2947 [D loss: 0.127336, acc.: 98.44%] [G loss: 1.292895]\n",
      "epoch:3 step:2948 [D loss: 0.327642, acc.: 84.38%] [G loss: 6.483836]\n",
      "epoch:3 step:2949 [D loss: 0.549538, acc.: 75.00%] [G loss: 2.407482]\n",
      "epoch:3 step:2950 [D loss: 0.311086, acc.: 85.16%] [G loss: 4.943930]\n",
      "epoch:3 step:2951 [D loss: 0.078469, acc.: 97.66%] [G loss: 4.988608]\n",
      "epoch:3 step:2952 [D loss: 0.662980, acc.: 68.75%] [G loss: 4.092329]\n",
      "epoch:3 step:2953 [D loss: 0.063808, acc.: 98.44%] [G loss: 3.856413]\n",
      "epoch:3 step:2954 [D loss: 0.192150, acc.: 93.75%] [G loss: 2.294021]\n",
      "epoch:3 step:2955 [D loss: 0.259965, acc.: 91.41%] [G loss: 3.353814]\n",
      "epoch:3 step:2956 [D loss: 1.013051, acc.: 45.31%] [G loss: 4.546012]\n",
      "epoch:3 step:2957 [D loss: 0.055133, acc.: 99.22%] [G loss: 5.512583]\n",
      "epoch:3 step:2958 [D loss: 0.202097, acc.: 93.75%] [G loss: 2.869852]\n",
      "epoch:3 step:2959 [D loss: 0.292977, acc.: 85.94%] [G loss: 4.923785]\n",
      "epoch:3 step:2960 [D loss: 0.407047, acc.: 78.91%] [G loss: 2.278276]\n",
      "epoch:3 step:2961 [D loss: 0.207393, acc.: 90.62%] [G loss: 4.090805]\n",
      "epoch:3 step:2962 [D loss: 0.287914, acc.: 89.06%] [G loss: 4.464911]\n",
      "epoch:3 step:2963 [D loss: 0.214787, acc.: 92.97%] [G loss: 4.335275]\n",
      "epoch:3 step:2964 [D loss: 0.140329, acc.: 96.88%] [G loss: 5.380263]\n",
      "epoch:3 step:2965 [D loss: 0.134361, acc.: 95.31%] [G loss: 5.092517]\n",
      "epoch:3 step:2966 [D loss: 0.904112, acc.: 53.91%] [G loss: 5.691508]\n",
      "epoch:3 step:2967 [D loss: 0.204915, acc.: 90.62%] [G loss: 5.185919]\n",
      "epoch:3 step:2968 [D loss: 0.071319, acc.: 98.44%] [G loss: 4.001510]\n",
      "epoch:3 step:2969 [D loss: 0.104558, acc.: 97.66%] [G loss: 2.854550]\n",
      "epoch:3 step:2970 [D loss: 0.179691, acc.: 96.88%] [G loss: 3.341825]\n",
      "epoch:3 step:2971 [D loss: 0.166080, acc.: 92.19%] [G loss: 1.795776]\n",
      "epoch:3 step:2972 [D loss: 0.491340, acc.: 75.78%] [G loss: 6.700407]\n",
      "epoch:3 step:2973 [D loss: 0.805291, acc.: 65.62%] [G loss: 2.586269]\n",
      "epoch:3 step:2974 [D loss: 0.118588, acc.: 96.09%] [G loss: 2.285194]\n",
      "epoch:3 step:2975 [D loss: 0.089264, acc.: 98.44%] [G loss: 2.693778]\n",
      "epoch:3 step:2976 [D loss: 0.104892, acc.: 98.44%] [G loss: 2.386271]\n",
      "epoch:3 step:2977 [D loss: 0.200407, acc.: 96.09%] [G loss: 2.852306]\n",
      "epoch:3 step:2978 [D loss: 0.429832, acc.: 78.91%] [G loss: 1.038313]\n",
      "epoch:3 step:2979 [D loss: 0.154607, acc.: 96.09%] [G loss: 1.758974]\n",
      "epoch:3 step:2980 [D loss: 0.042650, acc.: 99.22%] [G loss: 2.947170]\n",
      "epoch:3 step:2981 [D loss: 0.486717, acc.: 78.91%] [G loss: 0.476735]\n",
      "epoch:3 step:2982 [D loss: 0.360325, acc.: 80.47%] [G loss: 4.528071]\n",
      "epoch:3 step:2983 [D loss: 0.250319, acc.: 86.72%] [G loss: 3.514276]\n",
      "epoch:3 step:2984 [D loss: 0.339172, acc.: 85.16%] [G loss: 2.843616]\n",
      "epoch:3 step:2985 [D loss: 0.126254, acc.: 97.66%] [G loss: 1.791116]\n",
      "epoch:3 step:2986 [D loss: 0.255795, acc.: 88.28%] [G loss: 3.103599]\n",
      "epoch:3 step:2987 [D loss: 0.236174, acc.: 92.19%] [G loss: 3.681113]\n",
      "epoch:3 step:2988 [D loss: 0.825103, acc.: 57.03%] [G loss: 3.124323]\n",
      "epoch:3 step:2989 [D loss: 0.055326, acc.: 100.00%] [G loss: 4.171966]\n",
      "epoch:3 step:2990 [D loss: 0.796641, acc.: 65.62%] [G loss: 4.058945]\n",
      "epoch:3 step:2991 [D loss: 0.109193, acc.: 97.66%] [G loss: 5.064585]\n",
      "epoch:3 step:2992 [D loss: 0.137898, acc.: 97.66%] [G loss: 4.026884]\n",
      "epoch:3 step:2993 [D loss: 0.416530, acc.: 83.59%] [G loss: 5.340531]\n",
      "epoch:3 step:2994 [D loss: 0.202275, acc.: 90.62%] [G loss: 3.954219]\n",
      "epoch:3 step:2995 [D loss: 0.185647, acc.: 94.53%] [G loss: 4.474973]\n",
      "epoch:3 step:2996 [D loss: 0.025540, acc.: 100.00%] [G loss: 3.755223]\n",
      "epoch:3 step:2997 [D loss: 0.498549, acc.: 76.56%] [G loss: 4.751822]\n",
      "epoch:3 step:2998 [D loss: 0.188890, acc.: 91.41%] [G loss: 3.671481]\n",
      "epoch:3 step:2999 [D loss: 0.155901, acc.: 94.53%] [G loss: 3.087194]\n",
      "epoch:3 step:3000 [D loss: 0.112540, acc.: 95.31%] [G loss: 2.223490]\n",
      "##############\n",
      "[1.01554349 2.11352894 0.84311319 1.01612747 2.11273507 0.99197136\n",
      " 0.94297623 0.93347101 0.84624278 1.11659416]\n",
      "##########\n",
      "epoch:3 step:3001 [D loss: 0.150263, acc.: 96.88%] [G loss: 2.635536]\n",
      "epoch:3 step:3002 [D loss: 0.103963, acc.: 97.66%] [G loss: 1.875413]\n",
      "epoch:3 step:3003 [D loss: 0.063189, acc.: 99.22%] [G loss: 1.060808]\n",
      "epoch:3 step:3004 [D loss: 0.671340, acc.: 61.72%] [G loss: 5.788554]\n",
      "epoch:3 step:3005 [D loss: 1.511624, acc.: 51.56%] [G loss: 1.645619]\n",
      "epoch:3 step:3006 [D loss: 0.192835, acc.: 88.28%] [G loss: 2.427688]\n",
      "epoch:3 step:3007 [D loss: 0.048676, acc.: 100.00%] [G loss: 2.517550]\n",
      "epoch:3 step:3008 [D loss: 0.974002, acc.: 51.56%] [G loss: 1.605535]\n",
      "epoch:3 step:3009 [D loss: 0.074453, acc.: 97.66%] [G loss: 2.938425]\n",
      "epoch:3 step:3010 [D loss: 0.048574, acc.: 98.44%] [G loss: 2.665865]\n",
      "epoch:3 step:3011 [D loss: 0.151961, acc.: 97.66%] [G loss: 2.261029]\n",
      "epoch:3 step:3012 [D loss: 0.099934, acc.: 97.66%] [G loss: 1.318619]\n",
      "epoch:3 step:3013 [D loss: 0.381308, acc.: 84.38%] [G loss: 1.796470]\n",
      "epoch:3 step:3014 [D loss: 0.253716, acc.: 92.97%] [G loss: 2.293473]\n",
      "epoch:3 step:3015 [D loss: 0.177390, acc.: 92.97%] [G loss: 3.600759]\n",
      "epoch:3 step:3016 [D loss: 0.314965, acc.: 86.72%] [G loss: 1.836784]\n",
      "epoch:3 step:3017 [D loss: 0.218927, acc.: 91.41%] [G loss: 1.577745]\n",
      "epoch:3 step:3018 [D loss: 0.167222, acc.: 95.31%] [G loss: 3.597409]\n",
      "epoch:3 step:3019 [D loss: 1.071694, acc.: 42.97%] [G loss: 4.289149]\n",
      "epoch:3 step:3020 [D loss: 0.135611, acc.: 95.31%] [G loss: 4.297611]\n",
      "epoch:3 step:3021 [D loss: 0.311973, acc.: 85.94%] [G loss: 4.222889]\n",
      "epoch:3 step:3022 [D loss: 0.215697, acc.: 90.62%] [G loss: 5.183131]\n",
      "epoch:3 step:3023 [D loss: 0.159149, acc.: 93.75%] [G loss: 4.523248]\n",
      "epoch:3 step:3024 [D loss: 0.162944, acc.: 95.31%] [G loss: 3.281282]\n",
      "epoch:3 step:3025 [D loss: 0.232609, acc.: 89.06%] [G loss: 4.749352]\n",
      "epoch:3 step:3026 [D loss: 0.177033, acc.: 91.41%] [G loss: 3.878835]\n",
      "epoch:3 step:3027 [D loss: 0.210986, acc.: 92.97%] [G loss: 1.348032]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:3 step:3028 [D loss: 0.291819, acc.: 89.06%] [G loss: 4.320199]\n",
      "epoch:3 step:3029 [D loss: 0.097606, acc.: 96.09%] [G loss: 3.353305]\n",
      "epoch:3 step:3030 [D loss: 0.109022, acc.: 95.31%] [G loss: 0.611263]\n",
      "epoch:3 step:3031 [D loss: 0.359375, acc.: 81.25%] [G loss: 4.268942]\n",
      "epoch:3 step:3032 [D loss: 0.157052, acc.: 93.75%] [G loss: 4.143225]\n",
      "epoch:3 step:3033 [D loss: 0.083511, acc.: 96.09%] [G loss: 0.738569]\n",
      "epoch:3 step:3034 [D loss: 0.192788, acc.: 91.41%] [G loss: 2.927399]\n",
      "epoch:3 step:3035 [D loss: 0.025358, acc.: 100.00%] [G loss: 2.285050]\n",
      "epoch:3 step:3036 [D loss: 0.143148, acc.: 96.09%] [G loss: 0.201014]\n",
      "epoch:3 step:3037 [D loss: 0.455081, acc.: 72.66%] [G loss: 3.482336]\n",
      "epoch:3 step:3038 [D loss: 0.984260, acc.: 57.81%] [G loss: 0.295988]\n",
      "epoch:3 step:3039 [D loss: 1.120911, acc.: 57.03%] [G loss: 6.729932]\n",
      "epoch:3 step:3040 [D loss: 0.612260, acc.: 67.97%] [G loss: 3.900232]\n",
      "epoch:3 step:3041 [D loss: 0.297325, acc.: 85.94%] [G loss: 1.783337]\n",
      "epoch:3 step:3042 [D loss: 0.214220, acc.: 91.41%] [G loss: 4.142605]\n",
      "epoch:3 step:3043 [D loss: 0.130818, acc.: 97.66%] [G loss: 4.572776]\n",
      "epoch:3 step:3044 [D loss: 0.209322, acc.: 92.97%] [G loss: 2.942997]\n",
      "epoch:3 step:3045 [D loss: 0.164674, acc.: 93.75%] [G loss: 3.738530]\n",
      "epoch:3 step:3046 [D loss: 0.081233, acc.: 98.44%] [G loss: 3.677505]\n",
      "epoch:3 step:3047 [D loss: 0.137732, acc.: 97.66%] [G loss: 3.479560]\n",
      "epoch:3 step:3048 [D loss: 1.259841, acc.: 39.84%] [G loss: 5.539246]\n",
      "epoch:3 step:3049 [D loss: 0.498209, acc.: 75.00%] [G loss: 4.918689]\n",
      "epoch:3 step:3050 [D loss: 0.260150, acc.: 91.41%] [G loss: 3.437948]\n",
      "epoch:3 step:3051 [D loss: 0.101869, acc.: 97.66%] [G loss: 3.979830]\n",
      "epoch:3 step:3052 [D loss: 0.081534, acc.: 100.00%] [G loss: 3.092896]\n",
      "epoch:3 step:3053 [D loss: 0.058578, acc.: 100.00%] [G loss: 3.267901]\n",
      "epoch:3 step:3054 [D loss: 0.178637, acc.: 92.97%] [G loss: 4.105267]\n",
      "epoch:3 step:3055 [D loss: 0.102628, acc.: 97.66%] [G loss: 1.519792]\n",
      "epoch:3 step:3056 [D loss: 0.429642, acc.: 77.34%] [G loss: 3.236860]\n",
      "epoch:3 step:3057 [D loss: 0.040412, acc.: 99.22%] [G loss: 3.033550]\n",
      "epoch:3 step:3058 [D loss: 0.789067, acc.: 62.50%] [G loss: 0.264811]\n",
      "epoch:3 step:3059 [D loss: 0.036115, acc.: 100.00%] [G loss: 0.186199]\n",
      "epoch:3 step:3060 [D loss: 0.020141, acc.: 100.00%] [G loss: 0.430876]\n",
      "epoch:3 step:3061 [D loss: 0.106650, acc.: 95.31%] [G loss: 0.983303]\n",
      "epoch:3 step:3062 [D loss: 0.055253, acc.: 99.22%] [G loss: 1.006719]\n",
      "epoch:3 step:3063 [D loss: 0.544423, acc.: 71.88%] [G loss: 5.803308]\n",
      "epoch:3 step:3064 [D loss: 0.281328, acc.: 84.38%] [G loss: 3.966387]\n",
      "epoch:3 step:3065 [D loss: 0.262581, acc.: 91.41%] [G loss: 1.729668]\n",
      "epoch:3 step:3066 [D loss: 0.467496, acc.: 73.44%] [G loss: 5.486027]\n",
      "epoch:3 step:3067 [D loss: 0.253326, acc.: 87.50%] [G loss: 4.042344]\n",
      "epoch:3 step:3068 [D loss: 0.507933, acc.: 75.00%] [G loss: 4.559731]\n",
      "epoch:3 step:3069 [D loss: 0.069728, acc.: 98.44%] [G loss: 4.948051]\n",
      "epoch:3 step:3070 [D loss: 0.898494, acc.: 58.59%] [G loss: 5.013542]\n",
      "epoch:3 step:3071 [D loss: 0.142952, acc.: 94.53%] [G loss: 4.277796]\n",
      "epoch:3 step:3072 [D loss: 0.148360, acc.: 95.31%] [G loss: 3.438980]\n",
      "epoch:3 step:3073 [D loss: 0.234483, acc.: 89.06%] [G loss: 4.748639]\n",
      "epoch:3 step:3074 [D loss: 0.143457, acc.: 95.31%] [G loss: 4.248154]\n",
      "epoch:3 step:3075 [D loss: 0.513883, acc.: 74.22%] [G loss: 5.427366]\n",
      "epoch:3 step:3076 [D loss: 0.338886, acc.: 82.81%] [G loss: 4.353534]\n",
      "epoch:3 step:3077 [D loss: 0.468836, acc.: 82.81%] [G loss: 4.862928]\n",
      "epoch:3 step:3078 [D loss: 0.100162, acc.: 96.88%] [G loss: 4.074809]\n",
      "epoch:3 step:3079 [D loss: 0.076220, acc.: 98.44%] [G loss: 3.562777]\n",
      "epoch:3 step:3080 [D loss: 0.223362, acc.: 91.41%] [G loss: 4.171271]\n",
      "epoch:3 step:3081 [D loss: 0.261689, acc.: 88.28%] [G loss: 3.916060]\n",
      "epoch:3 step:3082 [D loss: 0.532798, acc.: 71.88%] [G loss: 3.828516]\n",
      "epoch:3 step:3083 [D loss: 0.153167, acc.: 97.66%] [G loss: 4.474956]\n",
      "epoch:3 step:3084 [D loss: 0.097082, acc.: 99.22%] [G loss: 3.823104]\n",
      "epoch:3 step:3085 [D loss: 0.254112, acc.: 89.06%] [G loss: 4.462491]\n",
      "epoch:3 step:3086 [D loss: 0.108363, acc.: 98.44%] [G loss: 3.617467]\n",
      "epoch:3 step:3087 [D loss: 0.273894, acc.: 93.75%] [G loss: 3.418960]\n",
      "epoch:3 step:3088 [D loss: 0.203561, acc.: 94.53%] [G loss: 2.427525]\n",
      "epoch:3 step:3089 [D loss: 0.130682, acc.: 97.66%] [G loss: 2.386580]\n",
      "epoch:3 step:3090 [D loss: 0.531489, acc.: 73.44%] [G loss: 4.749853]\n",
      "epoch:3 step:3091 [D loss: 0.191394, acc.: 89.06%] [G loss: 4.436012]\n",
      "epoch:3 step:3092 [D loss: 0.135164, acc.: 96.88%] [G loss: 2.666214]\n",
      "epoch:3 step:3093 [D loss: 0.172133, acc.: 93.75%] [G loss: 1.523094]\n",
      "epoch:3 step:3094 [D loss: 0.824722, acc.: 60.94%] [G loss: 7.459281]\n",
      "epoch:3 step:3095 [D loss: 1.125859, acc.: 56.25%] [G loss: 2.423102]\n",
      "epoch:3 step:3096 [D loss: 0.451540, acc.: 78.12%] [G loss: 5.148160]\n",
      "epoch:3 step:3097 [D loss: 0.191950, acc.: 92.97%] [G loss: 4.207629]\n",
      "epoch:3 step:3098 [D loss: 0.059246, acc.: 100.00%] [G loss: 3.829566]\n",
      "epoch:3 step:3099 [D loss: 0.182113, acc.: 95.31%] [G loss: 3.516937]\n",
      "epoch:3 step:3100 [D loss: 0.161890, acc.: 95.31%] [G loss: 4.301641]\n",
      "epoch:3 step:3101 [D loss: 0.306206, acc.: 87.50%] [G loss: 4.030545]\n",
      "epoch:3 step:3102 [D loss: 0.285317, acc.: 91.41%] [G loss: 4.263060]\n",
      "epoch:3 step:3103 [D loss: 0.427180, acc.: 78.12%] [G loss: 5.299444]\n",
      "epoch:3 step:3104 [D loss: 0.157828, acc.: 96.09%] [G loss: 5.036247]\n",
      "epoch:3 step:3105 [D loss: 1.040672, acc.: 43.75%] [G loss: 5.550162]\n",
      "epoch:3 step:3106 [D loss: 0.303286, acc.: 85.16%] [G loss: 4.169884]\n",
      "epoch:3 step:3107 [D loss: 0.214973, acc.: 95.31%] [G loss: 3.649243]\n",
      "epoch:3 step:3108 [D loss: 0.165773, acc.: 94.53%] [G loss: 4.781045]\n",
      "epoch:3 step:3109 [D loss: 0.117180, acc.: 97.66%] [G loss: 4.531208]\n",
      "epoch:3 step:3110 [D loss: 0.067774, acc.: 99.22%] [G loss: 3.352229]\n",
      "epoch:3 step:3111 [D loss: 0.206861, acc.: 92.97%] [G loss: 3.846086]\n",
      "epoch:3 step:3112 [D loss: 0.045241, acc.: 100.00%] [G loss: 3.701283]\n",
      "epoch:3 step:3113 [D loss: 0.097495, acc.: 97.66%] [G loss: 1.702888]\n",
      "epoch:3 step:3114 [D loss: 1.031493, acc.: 53.12%] [G loss: 6.436362]\n",
      "epoch:3 step:3115 [D loss: 0.723030, acc.: 65.62%] [G loss: 4.160151]\n",
      "epoch:3 step:3116 [D loss: 0.068868, acc.: 100.00%] [G loss: 2.723271]\n",
      "epoch:3 step:3117 [D loss: 0.126300, acc.: 96.88%] [G loss: 2.465971]\n",
      "epoch:3 step:3118 [D loss: 0.052578, acc.: 99.22%] [G loss: 2.570421]\n",
      "epoch:3 step:3119 [D loss: 0.071630, acc.: 99.22%] [G loss: 1.608537]\n",
      "epoch:3 step:3120 [D loss: 0.272779, acc.: 85.94%] [G loss: 0.532451]\n",
      "epoch:3 step:3121 [D loss: 0.186493, acc.: 92.97%] [G loss: 1.839815]\n",
      "epoch:3 step:3122 [D loss: 0.057767, acc.: 99.22%] [G loss: 2.400647]\n",
      "epoch:3 step:3123 [D loss: 0.147004, acc.: 95.31%] [G loss: 1.646366]\n",
      "epoch:3 step:3124 [D loss: 0.059164, acc.: 98.44%] [G loss: 1.045007]\n",
      "epoch:4 step:3125 [D loss: 0.164288, acc.: 96.88%] [G loss: 1.247488]\n",
      "epoch:4 step:3126 [D loss: 0.039121, acc.: 100.00%] [G loss: 1.235758]\n",
      "epoch:4 step:3127 [D loss: 0.492570, acc.: 75.78%] [G loss: 3.822030]\n",
      "epoch:4 step:3128 [D loss: 0.152043, acc.: 92.19%] [G loss: 3.694154]\n",
      "epoch:4 step:3129 [D loss: 0.187553, acc.: 92.19%] [G loss: 0.952200]\n",
      "epoch:4 step:3130 [D loss: 0.886523, acc.: 62.50%] [G loss: 6.976646]\n",
      "epoch:4 step:3131 [D loss: 0.532153, acc.: 70.31%] [G loss: 3.313261]\n",
      "epoch:4 step:3132 [D loss: 0.140110, acc.: 97.66%] [G loss: 3.387695]\n",
      "epoch:4 step:3133 [D loss: 0.140029, acc.: 95.31%] [G loss: 4.640806]\n",
      "epoch:4 step:3134 [D loss: 0.454194, acc.: 78.12%] [G loss: 4.178690]\n",
      "epoch:4 step:3135 [D loss: 0.068837, acc.: 99.22%] [G loss: 4.247946]\n",
      "epoch:4 step:3136 [D loss: 0.073498, acc.: 99.22%] [G loss: 3.452991]\n",
      "epoch:4 step:3137 [D loss: 0.257447, acc.: 91.41%] [G loss: 4.845623]\n",
      "epoch:4 step:3138 [D loss: 0.313779, acc.: 85.16%] [G loss: 3.187227]\n",
      "epoch:4 step:3139 [D loss: 0.077412, acc.: 100.00%] [G loss: 3.049498]\n",
      "epoch:4 step:3140 [D loss: 0.076400, acc.: 99.22%] [G loss: 2.757150]\n",
      "epoch:4 step:3141 [D loss: 0.063995, acc.: 99.22%] [G loss: 1.350737]\n",
      "epoch:4 step:3142 [D loss: 0.046810, acc.: 100.00%] [G loss: 0.569661]\n",
      "epoch:4 step:3143 [D loss: 0.066742, acc.: 99.22%] [G loss: 0.473858]\n",
      "epoch:4 step:3144 [D loss: 0.034761, acc.: 100.00%] [G loss: 0.636109]\n",
      "epoch:4 step:3145 [D loss: 0.043001, acc.: 100.00%] [G loss: 1.014184]\n",
      "epoch:4 step:3146 [D loss: 0.061998, acc.: 100.00%] [G loss: 0.505083]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:4 step:3147 [D loss: 0.059960, acc.: 99.22%] [G loss: 0.187270]\n",
      "epoch:4 step:3148 [D loss: 0.036000, acc.: 100.00%] [G loss: 0.146295]\n",
      "epoch:4 step:3149 [D loss: 0.108859, acc.: 98.44%] [G loss: 0.575445]\n",
      "epoch:4 step:3150 [D loss: 0.218200, acc.: 92.97%] [G loss: 0.467498]\n",
      "epoch:4 step:3151 [D loss: 0.021087, acc.: 100.00%] [G loss: 1.465430]\n",
      "epoch:4 step:3152 [D loss: 0.017091, acc.: 100.00%] [G loss: 0.346426]\n",
      "epoch:4 step:3153 [D loss: 0.014340, acc.: 100.00%] [G loss: 0.225741]\n",
      "epoch:4 step:3154 [D loss: 0.457355, acc.: 79.69%] [G loss: 5.247204]\n",
      "epoch:4 step:3155 [D loss: 0.771526, acc.: 63.28%] [G loss: 0.417094]\n",
      "epoch:4 step:3156 [D loss: 0.053359, acc.: 99.22%] [G loss: 0.361954]\n",
      "epoch:4 step:3157 [D loss: 0.503270, acc.: 75.00%] [G loss: 5.819351]\n",
      "epoch:4 step:3158 [D loss: 0.354152, acc.: 80.47%] [G loss: 3.638066]\n",
      "epoch:4 step:3159 [D loss: 0.055287, acc.: 100.00%] [G loss: 2.375936]\n",
      "epoch:4 step:3160 [D loss: 0.203229, acc.: 92.19%] [G loss: 3.326972]\n",
      "epoch:4 step:3161 [D loss: 0.155662, acc.: 92.97%] [G loss: 2.646474]\n",
      "epoch:4 step:3162 [D loss: 0.283249, acc.: 87.50%] [G loss: 3.996993]\n",
      "epoch:4 step:3163 [D loss: 0.320087, acc.: 87.50%] [G loss: 1.289546]\n",
      "epoch:4 step:3164 [D loss: 0.429156, acc.: 76.56%] [G loss: 6.060888]\n",
      "epoch:4 step:3165 [D loss: 0.557940, acc.: 79.69%] [G loss: 3.196196]\n",
      "epoch:4 step:3166 [D loss: 0.363731, acc.: 82.03%] [G loss: 6.397340]\n",
      "epoch:4 step:3167 [D loss: 0.311524, acc.: 84.38%] [G loss: 4.261482]\n",
      "epoch:4 step:3168 [D loss: 0.135478, acc.: 96.88%] [G loss: 3.175965]\n",
      "epoch:4 step:3169 [D loss: 0.123853, acc.: 96.88%] [G loss: 3.050899]\n",
      "epoch:4 step:3170 [D loss: 0.106679, acc.: 97.66%] [G loss: 3.864652]\n",
      "epoch:4 step:3171 [D loss: 0.169843, acc.: 95.31%] [G loss: 4.196507]\n",
      "epoch:4 step:3172 [D loss: 0.183783, acc.: 94.53%] [G loss: 3.410826]\n",
      "epoch:4 step:3173 [D loss: 0.360193, acc.: 82.03%] [G loss: 2.654742]\n",
      "epoch:4 step:3174 [D loss: 0.086952, acc.: 98.44%] [G loss: 1.931821]\n",
      "epoch:4 step:3175 [D loss: 0.079657, acc.: 99.22%] [G loss: 1.123355]\n",
      "epoch:4 step:3176 [D loss: 0.048177, acc.: 100.00%] [G loss: 0.558070]\n",
      "epoch:4 step:3177 [D loss: 0.154980, acc.: 93.75%] [G loss: 2.319342]\n",
      "epoch:4 step:3178 [D loss: 0.281110, acc.: 88.28%] [G loss: 1.018023]\n",
      "epoch:4 step:3179 [D loss: 0.030144, acc.: 100.00%] [G loss: 0.533985]\n",
      "epoch:4 step:3180 [D loss: 0.024870, acc.: 100.00%] [G loss: 0.501113]\n",
      "epoch:4 step:3181 [D loss: 0.083441, acc.: 97.66%] [G loss: 0.319022]\n",
      "epoch:4 step:3182 [D loss: 0.011501, acc.: 100.00%] [G loss: 0.789096]\n",
      "epoch:4 step:3183 [D loss: 0.232900, acc.: 91.41%] [G loss: 2.911137]\n",
      "epoch:4 step:3184 [D loss: 0.674537, acc.: 62.50%] [G loss: 2.335867]\n",
      "epoch:4 step:3185 [D loss: 0.020207, acc.: 100.00%] [G loss: 2.478614]\n",
      "epoch:4 step:3186 [D loss: 0.043928, acc.: 100.00%] [G loss: 2.180150]\n",
      "epoch:4 step:3187 [D loss: 0.076529, acc.: 96.88%] [G loss: 2.462149]\n",
      "epoch:4 step:3188 [D loss: 1.062482, acc.: 54.69%] [G loss: 9.233896]\n",
      "epoch:4 step:3189 [D loss: 3.069940, acc.: 50.00%] [G loss: 5.263407]\n",
      "epoch:4 step:3190 [D loss: 0.470067, acc.: 79.69%] [G loss: 2.545234]\n",
      "epoch:4 step:3191 [D loss: 0.547383, acc.: 75.00%] [G loss: 4.665448]\n",
      "epoch:4 step:3192 [D loss: 0.111297, acc.: 99.22%] [G loss: 4.460209]\n",
      "epoch:4 step:3193 [D loss: 0.270314, acc.: 91.41%] [G loss: 2.684171]\n",
      "epoch:4 step:3194 [D loss: 0.301580, acc.: 83.59%] [G loss: 3.518738]\n",
      "epoch:4 step:3195 [D loss: 0.083174, acc.: 98.44%] [G loss: 3.741935]\n",
      "epoch:4 step:3196 [D loss: 0.350549, acc.: 85.16%] [G loss: 3.047559]\n",
      "epoch:4 step:3197 [D loss: 0.281733, acc.: 90.62%] [G loss: 3.819565]\n",
      "epoch:4 step:3198 [D loss: 0.182321, acc.: 93.75%] [G loss: 3.756785]\n",
      "epoch:4 step:3199 [D loss: 0.107447, acc.: 100.00%] [G loss: 2.860579]\n",
      "epoch:4 step:3200 [D loss: 0.527591, acc.: 72.66%] [G loss: 4.146501]\n",
      "##############\n",
      "[0.93599637 0.96133538 1.04109115 0.90087475 1.06754912 0.98951083\n",
      " 1.04700176 0.98712873 2.10832302 0.91083023]\n",
      "##########\n",
      "epoch:4 step:3201 [D loss: 0.255276, acc.: 88.28%] [G loss: 3.605700]\n",
      "epoch:4 step:3202 [D loss: 0.200039, acc.: 92.97%] [G loss: 2.878596]\n",
      "epoch:4 step:3203 [D loss: 0.187553, acc.: 94.53%] [G loss: 3.210889]\n",
      "epoch:4 step:3204 [D loss: 0.082192, acc.: 99.22%] [G loss: 2.852448]\n",
      "epoch:4 step:3205 [D loss: 0.148895, acc.: 96.88%] [G loss: 2.883613]\n",
      "epoch:4 step:3206 [D loss: 0.381101, acc.: 85.94%] [G loss: 3.932468]\n",
      "epoch:4 step:3207 [D loss: 0.284060, acc.: 85.16%] [G loss: 1.716558]\n",
      "epoch:4 step:3208 [D loss: 0.277127, acc.: 89.06%] [G loss: 2.070814]\n",
      "epoch:4 step:3209 [D loss: 0.218979, acc.: 93.75%] [G loss: 3.539705]\n",
      "epoch:4 step:3210 [D loss: 0.483625, acc.: 76.56%] [G loss: 2.567805]\n",
      "epoch:4 step:3211 [D loss: 0.154795, acc.: 94.53%] [G loss: 3.237098]\n",
      "epoch:4 step:3212 [D loss: 0.307294, acc.: 89.06%] [G loss: 3.196995]\n",
      "epoch:4 step:3213 [D loss: 0.384292, acc.: 80.47%] [G loss: 2.690311]\n",
      "epoch:4 step:3214 [D loss: 0.241747, acc.: 89.06%] [G loss: 4.960052]\n",
      "epoch:4 step:3215 [D loss: 0.487510, acc.: 76.56%] [G loss: 3.390457]\n",
      "epoch:4 step:3216 [D loss: 0.187472, acc.: 93.75%] [G loss: 5.558708]\n",
      "epoch:4 step:3217 [D loss: 0.291304, acc.: 87.50%] [G loss: 4.370493]\n",
      "epoch:4 step:3218 [D loss: 0.425953, acc.: 80.47%] [G loss: 5.220620]\n",
      "epoch:4 step:3219 [D loss: 0.085122, acc.: 98.44%] [G loss: 4.317555]\n",
      "epoch:4 step:3220 [D loss: 0.335796, acc.: 83.59%] [G loss: 4.675666]\n",
      "epoch:4 step:3221 [D loss: 0.141430, acc.: 97.66%] [G loss: 3.721182]\n",
      "epoch:4 step:3222 [D loss: 0.093057, acc.: 99.22%] [G loss: 3.866513]\n",
      "epoch:4 step:3223 [D loss: 0.136602, acc.: 96.09%] [G loss: 3.246037]\n",
      "epoch:4 step:3224 [D loss: 0.243648, acc.: 92.19%] [G loss: 4.637641]\n",
      "epoch:4 step:3225 [D loss: 0.284557, acc.: 85.16%] [G loss: 3.043714]\n",
      "epoch:4 step:3226 [D loss: 0.155291, acc.: 96.09%] [G loss: 2.925638]\n",
      "epoch:4 step:3227 [D loss: 0.189292, acc.: 92.19%] [G loss: 4.011734]\n",
      "epoch:4 step:3228 [D loss: 0.140743, acc.: 93.75%] [G loss: 3.335895]\n",
      "epoch:4 step:3229 [D loss: 0.263108, acc.: 89.84%] [G loss: 2.097357]\n",
      "epoch:4 step:3230 [D loss: 0.060609, acc.: 99.22%] [G loss: 3.290560]\n",
      "epoch:4 step:3231 [D loss: 0.395910, acc.: 83.59%] [G loss: 0.532264]\n",
      "epoch:4 step:3232 [D loss: 0.056401, acc.: 100.00%] [G loss: 0.634467]\n",
      "epoch:4 step:3233 [D loss: 0.367079, acc.: 83.59%] [G loss: 5.735738]\n",
      "epoch:4 step:3234 [D loss: 0.172662, acc.: 92.97%] [G loss: 6.163157]\n",
      "epoch:4 step:3235 [D loss: 1.240387, acc.: 44.53%] [G loss: 2.278328]\n",
      "epoch:4 step:3236 [D loss: 0.155046, acc.: 92.97%] [G loss: 4.427428]\n",
      "epoch:4 step:3237 [D loss: 0.028766, acc.: 100.00%] [G loss: 4.553565]\n",
      "epoch:4 step:3238 [D loss: 0.067740, acc.: 99.22%] [G loss: 2.513194]\n",
      "epoch:4 step:3239 [D loss: 0.289104, acc.: 85.94%] [G loss: 4.373287]\n",
      "epoch:4 step:3240 [D loss: 0.234935, acc.: 90.62%] [G loss: 2.983974]\n",
      "epoch:4 step:3241 [D loss: 0.103339, acc.: 98.44%] [G loss: 3.040765]\n",
      "epoch:4 step:3242 [D loss: 0.228767, acc.: 91.41%] [G loss: 3.626654]\n",
      "epoch:4 step:3243 [D loss: 0.153403, acc.: 95.31%] [G loss: 1.969685]\n",
      "epoch:4 step:3244 [D loss: 0.147212, acc.: 96.88%] [G loss: 2.394783]\n",
      "epoch:4 step:3245 [D loss: 0.113978, acc.: 97.66%] [G loss: 1.915228]\n",
      "epoch:4 step:3246 [D loss: 0.320198, acc.: 85.94%] [G loss: 4.921759]\n",
      "epoch:4 step:3247 [D loss: 0.725486, acc.: 64.06%] [G loss: 0.982274]\n",
      "epoch:4 step:3248 [D loss: 0.068803, acc.: 100.00%] [G loss: 1.699128]\n",
      "epoch:4 step:3249 [D loss: 0.092635, acc.: 97.66%] [G loss: 3.638514]\n",
      "epoch:4 step:3250 [D loss: 0.498502, acc.: 78.91%] [G loss: 3.408358]\n",
      "epoch:4 step:3251 [D loss: 0.168511, acc.: 96.09%] [G loss: 2.319358]\n",
      "epoch:4 step:3252 [D loss: 0.231816, acc.: 92.19%] [G loss: 3.061660]\n",
      "epoch:4 step:3253 [D loss: 0.540971, acc.: 69.53%] [G loss: 4.186730]\n",
      "epoch:4 step:3254 [D loss: 0.675534, acc.: 70.31%] [G loss: 3.271724]\n",
      "epoch:4 step:3255 [D loss: 0.056269, acc.: 99.22%] [G loss: 4.182637]\n",
      "epoch:4 step:3256 [D loss: 0.123732, acc.: 98.44%] [G loss: 4.074215]\n",
      "epoch:4 step:3257 [D loss: 0.089199, acc.: 97.66%] [G loss: 3.261628]\n",
      "epoch:4 step:3258 [D loss: 0.130620, acc.: 98.44%] [G loss: 2.786338]\n",
      "epoch:4 step:3259 [D loss: 0.353915, acc.: 80.47%] [G loss: 5.650676]\n",
      "epoch:4 step:3260 [D loss: 0.461455, acc.: 77.34%] [G loss: 2.618660]\n",
      "epoch:4 step:3261 [D loss: 0.076078, acc.: 99.22%] [G loss: 3.020752]\n",
      "epoch:4 step:3262 [D loss: 0.139907, acc.: 96.88%] [G loss: 4.950401]\n",
      "epoch:4 step:3263 [D loss: 0.178998, acc.: 96.88%] [G loss: 4.204288]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:4 step:3264 [D loss: 0.363856, acc.: 82.81%] [G loss: 4.764619]\n",
      "epoch:4 step:3265 [D loss: 0.059775, acc.: 98.44%] [G loss: 4.632128]\n",
      "epoch:4 step:3266 [D loss: 0.077517, acc.: 99.22%] [G loss: 3.327119]\n",
      "epoch:4 step:3267 [D loss: 0.296962, acc.: 85.16%] [G loss: 5.604328]\n",
      "epoch:4 step:3268 [D loss: 0.143218, acc.: 93.75%] [G loss: 5.024350]\n",
      "epoch:4 step:3269 [D loss: 0.347155, acc.: 84.38%] [G loss: 1.766604]\n",
      "epoch:4 step:3270 [D loss: 0.845576, acc.: 60.16%] [G loss: 8.302285]\n",
      "epoch:4 step:3271 [D loss: 1.429486, acc.: 51.56%] [G loss: 5.324579]\n",
      "epoch:4 step:3272 [D loss: 0.121767, acc.: 97.66%] [G loss: 2.650917]\n",
      "epoch:4 step:3273 [D loss: 0.187293, acc.: 92.19%] [G loss: 2.263323]\n",
      "epoch:4 step:3274 [D loss: 0.025049, acc.: 100.00%] [G loss: 2.166273]\n",
      "epoch:4 step:3275 [D loss: 0.111606, acc.: 97.66%] [G loss: 2.942491]\n",
      "epoch:4 step:3276 [D loss: 0.187311, acc.: 95.31%] [G loss: 1.794485]\n",
      "epoch:4 step:3277 [D loss: 0.823253, acc.: 55.47%] [G loss: 5.535262]\n",
      "epoch:4 step:3278 [D loss: 0.313983, acc.: 82.81%] [G loss: 5.610898]\n",
      "epoch:4 step:3279 [D loss: 0.123380, acc.: 94.53%] [G loss: 3.293918]\n",
      "epoch:4 step:3280 [D loss: 0.040634, acc.: 100.00%] [G loss: 1.793193]\n",
      "epoch:4 step:3281 [D loss: 0.177255, acc.: 94.53%] [G loss: 3.368293]\n",
      "epoch:4 step:3282 [D loss: 0.121601, acc.: 96.09%] [G loss: 2.453655]\n",
      "epoch:4 step:3283 [D loss: 0.268424, acc.: 88.28%] [G loss: 3.950173]\n",
      "epoch:4 step:3284 [D loss: 0.769663, acc.: 63.28%] [G loss: 0.599789]\n",
      "epoch:4 step:3285 [D loss: 1.034189, acc.: 60.94%] [G loss: 4.486908]\n",
      "epoch:4 step:3286 [D loss: 0.231509, acc.: 88.28%] [G loss: 6.139343]\n",
      "epoch:4 step:3287 [D loss: 0.426552, acc.: 77.34%] [G loss: 3.052123]\n",
      "epoch:4 step:3288 [D loss: 0.037557, acc.: 100.00%] [G loss: 2.963039]\n",
      "epoch:4 step:3289 [D loss: 0.096313, acc.: 98.44%] [G loss: 1.744303]\n",
      "epoch:4 step:3290 [D loss: 0.059617, acc.: 99.22%] [G loss: 1.817821]\n",
      "epoch:4 step:3291 [D loss: 0.230624, acc.: 91.41%] [G loss: 2.470215]\n",
      "epoch:4 step:3292 [D loss: 0.104928, acc.: 99.22%] [G loss: 2.445600]\n",
      "epoch:4 step:3293 [D loss: 0.230848, acc.: 92.97%] [G loss: 2.178289]\n",
      "epoch:4 step:3294 [D loss: 0.538949, acc.: 72.66%] [G loss: 4.126169]\n",
      "epoch:4 step:3295 [D loss: 0.524827, acc.: 70.31%] [G loss: 3.221334]\n",
      "epoch:4 step:3296 [D loss: 0.042623, acc.: 100.00%] [G loss: 2.757819]\n",
      "epoch:4 step:3297 [D loss: 0.135245, acc.: 98.44%] [G loss: 3.741807]\n",
      "epoch:4 step:3298 [D loss: 0.096396, acc.: 96.88%] [G loss: 1.972353]\n",
      "epoch:4 step:3299 [D loss: 0.364462, acc.: 86.72%] [G loss: 4.998878]\n",
      "epoch:4 step:3300 [D loss: 0.577746, acc.: 66.41%] [G loss: 2.512095]\n",
      "epoch:4 step:3301 [D loss: 0.168669, acc.: 96.09%] [G loss: 2.063399]\n",
      "epoch:4 step:3302 [D loss: 0.083313, acc.: 98.44%] [G loss: 1.312784]\n",
      "epoch:4 step:3303 [D loss: 0.357087, acc.: 81.25%] [G loss: 4.443912]\n",
      "epoch:4 step:3304 [D loss: 0.433432, acc.: 76.56%] [G loss: 2.348021]\n",
      "epoch:4 step:3305 [D loss: 0.219083, acc.: 92.19%] [G loss: 2.417039]\n",
      "epoch:4 step:3306 [D loss: 0.168288, acc.: 94.53%] [G loss: 1.463080]\n",
      "epoch:4 step:3307 [D loss: 0.153770, acc.: 94.53%] [G loss: 0.549294]\n",
      "epoch:4 step:3308 [D loss: 0.019446, acc.: 100.00%] [G loss: 0.597531]\n",
      "epoch:4 step:3309 [D loss: 0.278602, acc.: 85.16%] [G loss: 1.874658]\n",
      "epoch:4 step:3310 [D loss: 0.383916, acc.: 84.38%] [G loss: 2.968993]\n",
      "epoch:4 step:3311 [D loss: 0.099436, acc.: 97.66%] [G loss: 3.153305]\n",
      "epoch:4 step:3312 [D loss: 0.289977, acc.: 85.94%] [G loss: 3.574131]\n",
      "epoch:4 step:3313 [D loss: 0.064173, acc.: 99.22%] [G loss: 4.321650]\n",
      "epoch:4 step:3314 [D loss: 0.222398, acc.: 93.75%] [G loss: 3.349475]\n",
      "epoch:4 step:3315 [D loss: 0.418445, acc.: 78.12%] [G loss: 5.318421]\n",
      "epoch:4 step:3316 [D loss: 0.339135, acc.: 84.38%] [G loss: 3.952577]\n",
      "epoch:4 step:3317 [D loss: 0.229873, acc.: 90.62%] [G loss: 4.786432]\n",
      "epoch:4 step:3318 [D loss: 0.077339, acc.: 99.22%] [G loss: 4.979065]\n",
      "epoch:4 step:3319 [D loss: 0.196926, acc.: 92.97%] [G loss: 4.168509]\n",
      "epoch:4 step:3320 [D loss: 0.174092, acc.: 96.09%] [G loss: 3.946273]\n",
      "epoch:4 step:3321 [D loss: 0.134765, acc.: 96.88%] [G loss: 5.280341]\n",
      "epoch:4 step:3322 [D loss: 0.239062, acc.: 92.19%] [G loss: 4.244877]\n",
      "epoch:4 step:3323 [D loss: 0.234057, acc.: 93.75%] [G loss: 3.454012]\n",
      "epoch:4 step:3324 [D loss: 0.046030, acc.: 100.00%] [G loss: 3.112980]\n",
      "epoch:4 step:3325 [D loss: 0.085281, acc.: 99.22%] [G loss: 2.316713]\n",
      "epoch:4 step:3326 [D loss: 0.595853, acc.: 71.09%] [G loss: 5.280821]\n",
      "epoch:4 step:3327 [D loss: 0.834365, acc.: 61.72%] [G loss: 2.498299]\n",
      "epoch:4 step:3328 [D loss: 0.526584, acc.: 74.22%] [G loss: 5.307474]\n",
      "epoch:4 step:3329 [D loss: 0.168875, acc.: 92.97%] [G loss: 5.988265]\n",
      "epoch:4 step:3330 [D loss: 0.402420, acc.: 82.03%] [G loss: 1.977068]\n",
      "epoch:4 step:3331 [D loss: 0.542949, acc.: 69.53%] [G loss: 5.087456]\n",
      "epoch:4 step:3332 [D loss: 0.433165, acc.: 77.34%] [G loss: 2.018673]\n",
      "epoch:4 step:3333 [D loss: 0.353694, acc.: 81.25%] [G loss: 2.374494]\n",
      "epoch:4 step:3334 [D loss: 0.325336, acc.: 89.06%] [G loss: 3.798995]\n",
      "epoch:4 step:3335 [D loss: 0.162038, acc.: 94.53%] [G loss: 3.935399]\n",
      "epoch:4 step:3336 [D loss: 0.178853, acc.: 93.75%] [G loss: 3.715563]\n",
      "epoch:4 step:3337 [D loss: 0.610697, acc.: 69.53%] [G loss: 5.308506]\n",
      "epoch:4 step:3338 [D loss: 0.778715, acc.: 64.06%] [G loss: 3.231521]\n",
      "epoch:4 step:3339 [D loss: 0.164885, acc.: 95.31%] [G loss: 4.133641]\n",
      "epoch:4 step:3340 [D loss: 0.147497, acc.: 94.53%] [G loss: 3.756601]\n",
      "epoch:4 step:3341 [D loss: 0.144848, acc.: 96.09%] [G loss: 4.557965]\n",
      "epoch:4 step:3342 [D loss: 0.223708, acc.: 93.75%] [G loss: 2.813362]\n",
      "epoch:4 step:3343 [D loss: 0.189934, acc.: 92.97%] [G loss: 4.742448]\n",
      "epoch:4 step:3344 [D loss: 0.278806, acc.: 88.28%] [G loss: 4.601820]\n",
      "epoch:4 step:3345 [D loss: 0.124921, acc.: 96.88%] [G loss: 5.763644]\n",
      "epoch:4 step:3346 [D loss: 0.263743, acc.: 86.72%] [G loss: 4.494777]\n",
      "epoch:4 step:3347 [D loss: 0.352818, acc.: 85.94%] [G loss: 4.694215]\n",
      "epoch:4 step:3348 [D loss: 0.140460, acc.: 95.31%] [G loss: 4.576327]\n",
      "epoch:4 step:3349 [D loss: 0.224204, acc.: 91.41%] [G loss: 4.150921]\n",
      "epoch:4 step:3350 [D loss: 0.191120, acc.: 94.53%] [G loss: 4.063416]\n",
      "epoch:4 step:3351 [D loss: 0.169067, acc.: 95.31%] [G loss: 5.353525]\n",
      "epoch:4 step:3352 [D loss: 0.113437, acc.: 96.88%] [G loss: 1.981365]\n",
      "epoch:4 step:3353 [D loss: 0.565646, acc.: 74.22%] [G loss: 9.340729]\n",
      "epoch:4 step:3354 [D loss: 1.445137, acc.: 54.69%] [G loss: 4.111240]\n",
      "epoch:4 step:3355 [D loss: 0.207707, acc.: 93.75%] [G loss: 4.080622]\n",
      "epoch:4 step:3356 [D loss: 0.161470, acc.: 93.75%] [G loss: 3.307980]\n",
      "epoch:4 step:3357 [D loss: 0.268827, acc.: 87.50%] [G loss: 4.945532]\n",
      "epoch:4 step:3358 [D loss: 0.463738, acc.: 78.91%] [G loss: 3.794021]\n",
      "epoch:4 step:3359 [D loss: 0.137248, acc.: 96.88%] [G loss: 4.215605]\n",
      "epoch:4 step:3360 [D loss: 0.287355, acc.: 89.06%] [G loss: 4.332951]\n",
      "epoch:4 step:3361 [D loss: 0.083308, acc.: 98.44%] [G loss: 3.265598]\n",
      "epoch:4 step:3362 [D loss: 0.107963, acc.: 96.88%] [G loss: 3.988666]\n",
      "epoch:4 step:3363 [D loss: 0.140354, acc.: 96.88%] [G loss: 3.824876]\n",
      "epoch:4 step:3364 [D loss: 0.881965, acc.: 53.12%] [G loss: 5.936249]\n",
      "epoch:4 step:3365 [D loss: 0.303557, acc.: 84.38%] [G loss: 4.437272]\n",
      "epoch:4 step:3366 [D loss: 0.080602, acc.: 98.44%] [G loss: 2.994215]\n",
      "epoch:4 step:3367 [D loss: 0.045636, acc.: 100.00%] [G loss: 2.348608]\n",
      "epoch:4 step:3368 [D loss: 0.153686, acc.: 97.66%] [G loss: 2.787256]\n",
      "epoch:4 step:3369 [D loss: 0.084810, acc.: 98.44%] [G loss: 2.222536]\n",
      "epoch:4 step:3370 [D loss: 0.269034, acc.: 92.19%] [G loss: 0.914872]\n",
      "epoch:4 step:3371 [D loss: 1.006688, acc.: 43.75%] [G loss: 5.375782]\n",
      "epoch:4 step:3372 [D loss: 0.256627, acc.: 84.38%] [G loss: 4.751450]\n",
      "epoch:4 step:3373 [D loss: 0.253616, acc.: 90.62%] [G loss: 2.749716]\n",
      "epoch:4 step:3374 [D loss: 0.213831, acc.: 93.75%] [G loss: 4.208856]\n",
      "epoch:4 step:3375 [D loss: 0.072219, acc.: 100.00%] [G loss: 3.201982]\n",
      "epoch:4 step:3376 [D loss: 0.517135, acc.: 73.44%] [G loss: 4.298479]\n",
      "epoch:4 step:3377 [D loss: 0.395741, acc.: 82.81%] [G loss: 3.591226]\n",
      "epoch:4 step:3378 [D loss: 0.082172, acc.: 99.22%] [G loss: 3.668700]\n",
      "epoch:4 step:3379 [D loss: 0.491724, acc.: 78.91%] [G loss: 5.550076]\n",
      "epoch:4 step:3380 [D loss: 0.458003, acc.: 75.00%] [G loss: 3.419001]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:4 step:3381 [D loss: 0.167606, acc.: 95.31%] [G loss: 4.741730]\n",
      "epoch:4 step:3382 [D loss: 0.056065, acc.: 100.00%] [G loss: 4.716840]\n",
      "epoch:4 step:3383 [D loss: 0.054362, acc.: 100.00%] [G loss: 3.084216]\n",
      "epoch:4 step:3384 [D loss: 0.260176, acc.: 92.19%] [G loss: 4.777596]\n",
      "epoch:4 step:3385 [D loss: 0.732660, acc.: 57.81%] [G loss: 4.465086]\n",
      "epoch:4 step:3386 [D loss: 0.034651, acc.: 100.00%] [G loss: 4.955658]\n",
      "epoch:4 step:3387 [D loss: 0.183697, acc.: 93.75%] [G loss: 4.460625]\n",
      "epoch:4 step:3388 [D loss: 0.266314, acc.: 90.62%] [G loss: 4.233864]\n",
      "epoch:4 step:3389 [D loss: 0.150675, acc.: 96.88%] [G loss: 3.016798]\n",
      "epoch:4 step:3390 [D loss: 0.501700, acc.: 70.31%] [G loss: 6.078959]\n",
      "epoch:4 step:3391 [D loss: 1.406325, acc.: 35.16%] [G loss: 4.960641]\n",
      "epoch:4 step:3392 [D loss: 0.027527, acc.: 100.00%] [G loss: 5.330032]\n",
      "epoch:4 step:3393 [D loss: 0.121731, acc.: 94.53%] [G loss: 3.509234]\n",
      "epoch:4 step:3394 [D loss: 0.220264, acc.: 92.97%] [G loss: 4.218210]\n",
      "epoch:4 step:3395 [D loss: 0.120656, acc.: 94.53%] [G loss: 3.372866]\n",
      "epoch:4 step:3396 [D loss: 0.105249, acc.: 99.22%] [G loss: 2.940297]\n",
      "epoch:4 step:3397 [D loss: 0.370343, acc.: 82.81%] [G loss: 4.313897]\n",
      "epoch:4 step:3398 [D loss: 0.158792, acc.: 93.75%] [G loss: 4.055854]\n",
      "epoch:4 step:3399 [D loss: 0.205153, acc.: 93.75%] [G loss: 2.142986]\n",
      "epoch:4 step:3400 [D loss: 0.108548, acc.: 96.09%] [G loss: 1.682194]\n",
      "##############\n",
      "[0.97586373 0.95287688 0.81062089 0.93604747 1.11989202 1.00876258\n",
      " 2.12881952 1.11224046 0.83935511 1.11004514]\n",
      "##########\n",
      "epoch:4 step:3401 [D loss: 0.542231, acc.: 75.00%] [G loss: 5.377206]\n",
      "epoch:4 step:3402 [D loss: 0.400267, acc.: 75.78%] [G loss: 3.283503]\n",
      "epoch:4 step:3403 [D loss: 0.114891, acc.: 96.88%] [G loss: 3.064833]\n",
      "epoch:4 step:3404 [D loss: 0.207418, acc.: 92.19%] [G loss: 3.835091]\n",
      "epoch:4 step:3405 [D loss: 0.205355, acc.: 92.97%] [G loss: 2.873562]\n",
      "epoch:4 step:3406 [D loss: 0.970953, acc.: 56.25%] [G loss: 5.799047]\n",
      "epoch:4 step:3407 [D loss: 0.608836, acc.: 67.97%] [G loss: 3.387367]\n",
      "epoch:4 step:3408 [D loss: 0.126770, acc.: 98.44%] [G loss: 3.454809]\n",
      "epoch:4 step:3409 [D loss: 0.323117, acc.: 88.28%] [G loss: 3.644471]\n",
      "epoch:4 step:3410 [D loss: 0.311721, acc.: 86.72%] [G loss: 5.143570]\n",
      "epoch:4 step:3411 [D loss: 0.656858, acc.: 71.09%] [G loss: 4.008495]\n",
      "epoch:4 step:3412 [D loss: 0.279410, acc.: 88.28%] [G loss: 4.498037]\n",
      "epoch:4 step:3413 [D loss: 0.179772, acc.: 92.97%] [G loss: 2.656497]\n",
      "epoch:4 step:3414 [D loss: 0.238716, acc.: 90.62%] [G loss: 4.149446]\n",
      "epoch:4 step:3415 [D loss: 0.532013, acc.: 71.88%] [G loss: 2.964720]\n",
      "epoch:4 step:3416 [D loss: 0.219561, acc.: 91.41%] [G loss: 3.937093]\n",
      "epoch:4 step:3417 [D loss: 0.070434, acc.: 100.00%] [G loss: 3.991614]\n",
      "epoch:4 step:3418 [D loss: 0.137398, acc.: 97.66%] [G loss: 3.188122]\n",
      "epoch:4 step:3419 [D loss: 0.281532, acc.: 85.94%] [G loss: 3.008406]\n",
      "epoch:4 step:3420 [D loss: 0.072748, acc.: 99.22%] [G loss: 2.597437]\n",
      "epoch:4 step:3421 [D loss: 0.295973, acc.: 89.84%] [G loss: 4.383609]\n",
      "epoch:4 step:3422 [D loss: 0.266321, acc.: 89.06%] [G loss: 3.723916]\n",
      "epoch:4 step:3423 [D loss: 0.177449, acc.: 96.09%] [G loss: 2.162665]\n",
      "epoch:4 step:3424 [D loss: 0.393366, acc.: 82.81%] [G loss: 5.956298]\n",
      "epoch:4 step:3425 [D loss: 0.339393, acc.: 82.81%] [G loss: 3.454037]\n",
      "epoch:4 step:3426 [D loss: 0.128771, acc.: 96.09%] [G loss: 3.428919]\n",
      "epoch:4 step:3427 [D loss: 0.154051, acc.: 93.75%] [G loss: 3.381903]\n",
      "epoch:4 step:3428 [D loss: 0.364986, acc.: 85.94%] [G loss: 7.421191]\n",
      "epoch:4 step:3429 [D loss: 0.848063, acc.: 65.62%] [G loss: 3.442523]\n",
      "epoch:4 step:3430 [D loss: 0.269716, acc.: 89.06%] [G loss: 5.551477]\n",
      "epoch:4 step:3431 [D loss: 0.146668, acc.: 95.31%] [G loss: 4.278740]\n",
      "epoch:4 step:3432 [D loss: 0.150589, acc.: 93.75%] [G loss: 4.133109]\n",
      "epoch:4 step:3433 [D loss: 0.156400, acc.: 95.31%] [G loss: 4.109757]\n",
      "epoch:4 step:3434 [D loss: 0.242197, acc.: 92.19%] [G loss: 5.195675]\n",
      "epoch:4 step:3435 [D loss: 0.116943, acc.: 96.88%] [G loss: 3.892879]\n",
      "epoch:4 step:3436 [D loss: 0.700769, acc.: 68.75%] [G loss: 5.969590]\n",
      "epoch:4 step:3437 [D loss: 0.440294, acc.: 80.47%] [G loss: 3.935275]\n",
      "epoch:4 step:3438 [D loss: 0.197697, acc.: 92.97%] [G loss: 4.205360]\n",
      "epoch:4 step:3439 [D loss: 0.978331, acc.: 50.78%] [G loss: 4.644047]\n",
      "epoch:4 step:3440 [D loss: 0.075244, acc.: 99.22%] [G loss: 5.155346]\n",
      "epoch:4 step:3441 [D loss: 0.284650, acc.: 85.94%] [G loss: 3.996417]\n",
      "epoch:4 step:3442 [D loss: 0.145991, acc.: 96.88%] [G loss: 3.768509]\n",
      "epoch:4 step:3443 [D loss: 0.223340, acc.: 91.41%] [G loss: 4.966685]\n",
      "epoch:4 step:3444 [D loss: 0.886154, acc.: 48.44%] [G loss: 5.718811]\n",
      "epoch:4 step:3445 [D loss: 0.524489, acc.: 74.22%] [G loss: 2.868529]\n",
      "epoch:4 step:3446 [D loss: 0.165909, acc.: 96.09%] [G loss: 3.764466]\n",
      "epoch:4 step:3447 [D loss: 0.140681, acc.: 96.09%] [G loss: 3.949184]\n",
      "epoch:4 step:3448 [D loss: 0.197958, acc.: 91.41%] [G loss: 4.118060]\n",
      "epoch:4 step:3449 [D loss: 0.256808, acc.: 90.62%] [G loss: 3.539949]\n",
      "epoch:4 step:3450 [D loss: 0.220922, acc.: 94.53%] [G loss: 4.346492]\n",
      "epoch:4 step:3451 [D loss: 0.674856, acc.: 57.81%] [G loss: 3.911926]\n",
      "epoch:4 step:3452 [D loss: 0.127238, acc.: 96.88%] [G loss: 4.102453]\n",
      "epoch:4 step:3453 [D loss: 0.230988, acc.: 89.84%] [G loss: 3.151295]\n",
      "epoch:4 step:3454 [D loss: 0.143886, acc.: 94.53%] [G loss: 3.268575]\n",
      "epoch:4 step:3455 [D loss: 0.194863, acc.: 95.31%] [G loss: 3.417590]\n",
      "epoch:4 step:3456 [D loss: 0.074517, acc.: 99.22%] [G loss: 4.531853]\n",
      "epoch:4 step:3457 [D loss: 0.387811, acc.: 82.81%] [G loss: 4.122941]\n",
      "epoch:4 step:3458 [D loss: 0.243712, acc.: 91.41%] [G loss: 3.494747]\n",
      "epoch:4 step:3459 [D loss: 0.441289, acc.: 75.78%] [G loss: 6.315833]\n",
      "epoch:4 step:3460 [D loss: 0.696074, acc.: 71.09%] [G loss: 2.827007]\n",
      "epoch:4 step:3461 [D loss: 0.522880, acc.: 73.44%] [G loss: 5.832787]\n",
      "epoch:4 step:3462 [D loss: 0.425067, acc.: 78.12%] [G loss: 4.418162]\n",
      "epoch:4 step:3463 [D loss: 0.197312, acc.: 93.75%] [G loss: 3.328998]\n",
      "epoch:4 step:3464 [D loss: 0.097201, acc.: 98.44%] [G loss: 3.158229]\n",
      "epoch:4 step:3465 [D loss: 0.086912, acc.: 98.44%] [G loss: 2.176007]\n",
      "epoch:4 step:3466 [D loss: 0.239150, acc.: 89.84%] [G loss: 3.347254]\n",
      "epoch:4 step:3467 [D loss: 0.302417, acc.: 88.28%] [G loss: 2.332981]\n",
      "epoch:4 step:3468 [D loss: 0.480195, acc.: 72.66%] [G loss: 6.437276]\n",
      "epoch:4 step:3469 [D loss: 0.784127, acc.: 63.28%] [G loss: 3.015685]\n",
      "epoch:4 step:3470 [D loss: 0.117498, acc.: 97.66%] [G loss: 3.197075]\n",
      "epoch:4 step:3471 [D loss: 0.172554, acc.: 94.53%] [G loss: 3.676842]\n",
      "epoch:4 step:3472 [D loss: 0.231743, acc.: 91.41%] [G loss: 3.443332]\n",
      "epoch:4 step:3473 [D loss: 0.073233, acc.: 98.44%] [G loss: 3.070048]\n",
      "epoch:4 step:3474 [D loss: 0.575119, acc.: 69.53%] [G loss: 5.599572]\n",
      "epoch:4 step:3475 [D loss: 0.371067, acc.: 77.34%] [G loss: 4.276087]\n",
      "epoch:4 step:3476 [D loss: 0.139899, acc.: 95.31%] [G loss: 4.270163]\n",
      "epoch:4 step:3477 [D loss: 0.077699, acc.: 99.22%] [G loss: 3.542739]\n",
      "epoch:4 step:3478 [D loss: 0.331150, acc.: 85.94%] [G loss: 5.480988]\n",
      "epoch:4 step:3479 [D loss: 0.238759, acc.: 89.06%] [G loss: 3.247907]\n",
      "epoch:4 step:3480 [D loss: 0.275861, acc.: 89.84%] [G loss: 4.304515]\n",
      "epoch:4 step:3481 [D loss: 0.114949, acc.: 96.09%] [G loss: 4.142606]\n",
      "epoch:4 step:3482 [D loss: 0.343402, acc.: 83.59%] [G loss: 2.153223]\n",
      "epoch:4 step:3483 [D loss: 0.207953, acc.: 92.97%] [G loss: 3.593643]\n",
      "epoch:4 step:3484 [D loss: 0.066142, acc.: 99.22%] [G loss: 3.473434]\n",
      "epoch:4 step:3485 [D loss: 0.589960, acc.: 71.09%] [G loss: 2.024685]\n",
      "epoch:4 step:3486 [D loss: 0.102688, acc.: 97.66%] [G loss: 2.289436]\n",
      "epoch:4 step:3487 [D loss: 0.267914, acc.: 86.72%] [G loss: 3.766410]\n",
      "epoch:4 step:3488 [D loss: 0.375123, acc.: 83.59%] [G loss: 3.600043]\n",
      "epoch:4 step:3489 [D loss: 0.237222, acc.: 90.62%] [G loss: 2.939668]\n",
      "epoch:4 step:3490 [D loss: 0.406772, acc.: 82.03%] [G loss: 6.787580]\n",
      "epoch:4 step:3491 [D loss: 0.596115, acc.: 71.09%] [G loss: 3.479645]\n",
      "epoch:4 step:3492 [D loss: 0.182366, acc.: 92.97%] [G loss: 5.141041]\n",
      "epoch:4 step:3493 [D loss: 0.029257, acc.: 100.00%] [G loss: 4.983594]\n",
      "epoch:4 step:3494 [D loss: 0.055360, acc.: 99.22%] [G loss: 3.700892]\n",
      "epoch:4 step:3495 [D loss: 0.225119, acc.: 90.62%] [G loss: 4.227420]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:4 step:3496 [D loss: 0.099586, acc.: 97.66%] [G loss: 4.956022]\n",
      "epoch:4 step:3497 [D loss: 0.231993, acc.: 92.19%] [G loss: 4.671092]\n",
      "epoch:4 step:3498 [D loss: 0.136657, acc.: 96.09%] [G loss: 3.806725]\n",
      "epoch:4 step:3499 [D loss: 0.863753, acc.: 53.91%] [G loss: 7.294371]\n",
      "epoch:4 step:3500 [D loss: 0.470046, acc.: 75.00%] [G loss: 4.396020]\n",
      "epoch:4 step:3501 [D loss: 0.047845, acc.: 99.22%] [G loss: 2.520757]\n",
      "epoch:4 step:3502 [D loss: 0.066823, acc.: 99.22%] [G loss: 1.785387]\n",
      "epoch:4 step:3503 [D loss: 0.482144, acc.: 76.56%] [G loss: 6.864316]\n",
      "epoch:4 step:3504 [D loss: 0.729822, acc.: 63.28%] [G loss: 2.645727]\n",
      "epoch:4 step:3505 [D loss: 0.147283, acc.: 94.53%] [G loss: 3.224071]\n",
      "epoch:4 step:3506 [D loss: 0.075207, acc.: 99.22%] [G loss: 2.073080]\n",
      "epoch:4 step:3507 [D loss: 0.181431, acc.: 95.31%] [G loss: 1.608641]\n",
      "epoch:4 step:3508 [D loss: 0.226018, acc.: 92.97%] [G loss: 4.159702]\n",
      "epoch:4 step:3509 [D loss: 1.098408, acc.: 47.66%] [G loss: 3.929837]\n",
      "epoch:4 step:3510 [D loss: 0.087211, acc.: 97.66%] [G loss: 4.154447]\n",
      "epoch:4 step:3511 [D loss: 0.467810, acc.: 77.34%] [G loss: 3.057394]\n",
      "epoch:4 step:3512 [D loss: 0.051635, acc.: 100.00%] [G loss: 4.191083]\n",
      "epoch:4 step:3513 [D loss: 0.151329, acc.: 96.88%] [G loss: 3.910572]\n",
      "epoch:4 step:3514 [D loss: 0.201830, acc.: 96.09%] [G loss: 3.159719]\n",
      "epoch:4 step:3515 [D loss: 0.230557, acc.: 93.75%] [G loss: 4.391735]\n",
      "epoch:4 step:3516 [D loss: 0.157168, acc.: 96.09%] [G loss: 3.886893]\n",
      "epoch:4 step:3517 [D loss: 0.178368, acc.: 96.88%] [G loss: 3.349948]\n",
      "epoch:4 step:3518 [D loss: 0.121037, acc.: 95.31%] [G loss: 4.547122]\n",
      "epoch:4 step:3519 [D loss: 0.254853, acc.: 90.62%] [G loss: 3.239955]\n",
      "epoch:4 step:3520 [D loss: 0.083711, acc.: 100.00%] [G loss: 2.373772]\n",
      "epoch:4 step:3521 [D loss: 0.203047, acc.: 92.19%] [G loss: 3.567642]\n",
      "epoch:4 step:3522 [D loss: 0.112345, acc.: 96.09%] [G loss: 3.043914]\n",
      "epoch:4 step:3523 [D loss: 0.230484, acc.: 92.97%] [G loss: 3.861975]\n",
      "epoch:4 step:3524 [D loss: 0.519076, acc.: 71.09%] [G loss: 5.009587]\n",
      "epoch:4 step:3525 [D loss: 0.555781, acc.: 74.22%] [G loss: 1.654634]\n",
      "epoch:4 step:3526 [D loss: 0.109997, acc.: 96.88%] [G loss: 1.516229]\n",
      "epoch:4 step:3527 [D loss: 0.269160, acc.: 86.72%] [G loss: 5.383057]\n",
      "epoch:4 step:3528 [D loss: 0.288003, acc.: 83.59%] [G loss: 3.498363]\n",
      "epoch:4 step:3529 [D loss: 0.145069, acc.: 96.88%] [G loss: 3.366274]\n",
      "epoch:4 step:3530 [D loss: 0.114310, acc.: 97.66%] [G loss: 4.414312]\n",
      "epoch:4 step:3531 [D loss: 0.104263, acc.: 98.44%] [G loss: 4.223232]\n",
      "epoch:4 step:3532 [D loss: 0.246798, acc.: 89.84%] [G loss: 2.382669]\n",
      "epoch:4 step:3533 [D loss: 0.323610, acc.: 86.72%] [G loss: 5.257477]\n",
      "epoch:4 step:3534 [D loss: 0.205435, acc.: 89.84%] [G loss: 3.721010]\n",
      "epoch:4 step:3535 [D loss: 1.337397, acc.: 35.94%] [G loss: 5.832430]\n",
      "epoch:4 step:3536 [D loss: 0.054899, acc.: 99.22%] [G loss: 6.635235]\n",
      "epoch:4 step:3537 [D loss: 0.367144, acc.: 82.81%] [G loss: 3.000504]\n",
      "epoch:4 step:3538 [D loss: 0.194915, acc.: 89.84%] [G loss: 3.598953]\n",
      "epoch:4 step:3539 [D loss: 0.111227, acc.: 98.44%] [G loss: 4.215048]\n",
      "epoch:4 step:3540 [D loss: 0.162880, acc.: 95.31%] [G loss: 2.288615]\n",
      "epoch:4 step:3541 [D loss: 0.288352, acc.: 89.06%] [G loss: 3.110650]\n",
      "epoch:4 step:3542 [D loss: 0.209626, acc.: 92.19%] [G loss: 3.388058]\n",
      "epoch:4 step:3543 [D loss: 0.482185, acc.: 75.00%] [G loss: 4.837423]\n",
      "epoch:4 step:3544 [D loss: 0.513334, acc.: 70.31%] [G loss: 1.742892]\n",
      "epoch:4 step:3545 [D loss: 0.421746, acc.: 79.69%] [G loss: 5.103615]\n",
      "epoch:4 step:3546 [D loss: 0.033619, acc.: 100.00%] [G loss: 5.974105]\n",
      "epoch:4 step:3547 [D loss: 0.364955, acc.: 84.38%] [G loss: 2.175673]\n",
      "epoch:4 step:3548 [D loss: 0.325102, acc.: 85.94%] [G loss: 4.798120]\n",
      "epoch:4 step:3549 [D loss: 0.092255, acc.: 96.88%] [G loss: 4.810549]\n",
      "epoch:4 step:3550 [D loss: 0.091449, acc.: 98.44%] [G loss: 3.380121]\n",
      "epoch:4 step:3551 [D loss: 0.268636, acc.: 89.06%] [G loss: 4.762275]\n",
      "epoch:4 step:3552 [D loss: 0.413384, acc.: 82.03%] [G loss: 2.855819]\n",
      "epoch:4 step:3553 [D loss: 0.122620, acc.: 97.66%] [G loss: 3.233079]\n",
      "epoch:4 step:3554 [D loss: 0.030073, acc.: 100.00%] [G loss: 2.487317]\n",
      "epoch:4 step:3555 [D loss: 0.065856, acc.: 99.22%] [G loss: 1.275128]\n",
      "epoch:4 step:3556 [D loss: 0.024354, acc.: 100.00%] [G loss: 0.506624]\n",
      "epoch:4 step:3557 [D loss: 0.454183, acc.: 76.56%] [G loss: 6.544147]\n",
      "epoch:4 step:3558 [D loss: 1.119474, acc.: 53.91%] [G loss: 1.314980]\n",
      "epoch:4 step:3559 [D loss: 0.306920, acc.: 81.25%] [G loss: 3.258182]\n",
      "epoch:4 step:3560 [D loss: 0.111023, acc.: 96.09%] [G loss: 2.795464]\n",
      "epoch:4 step:3561 [D loss: 0.187986, acc.: 94.53%] [G loss: 1.964491]\n",
      "epoch:4 step:3562 [D loss: 0.418491, acc.: 78.12%] [G loss: 4.094669]\n",
      "epoch:4 step:3563 [D loss: 0.364984, acc.: 84.38%] [G loss: 2.978997]\n",
      "epoch:4 step:3564 [D loss: 0.271797, acc.: 87.50%] [G loss: 0.826829]\n",
      "epoch:4 step:3565 [D loss: 0.130371, acc.: 96.09%] [G loss: 1.656596]\n",
      "epoch:4 step:3566 [D loss: 0.110884, acc.: 96.88%] [G loss: 2.383484]\n",
      "epoch:4 step:3567 [D loss: 0.340064, acc.: 89.84%] [G loss: 2.889565]\n",
      "epoch:4 step:3568 [D loss: 0.395697, acc.: 80.47%] [G loss: 2.969347]\n",
      "epoch:4 step:3569 [D loss: 0.294629, acc.: 89.84%] [G loss: 4.931839]\n",
      "epoch:4 step:3570 [D loss: 0.152654, acc.: 94.53%] [G loss: 4.050279]\n",
      "epoch:4 step:3571 [D loss: 0.108685, acc.: 98.44%] [G loss: 2.064433]\n",
      "epoch:4 step:3572 [D loss: 0.208981, acc.: 93.75%] [G loss: 3.760183]\n",
      "epoch:4 step:3573 [D loss: 0.163156, acc.: 93.75%] [G loss: 3.332901]\n",
      "epoch:4 step:3574 [D loss: 0.191119, acc.: 92.19%] [G loss: 2.481876]\n",
      "epoch:4 step:3575 [D loss: 0.085241, acc.: 98.44%] [G loss: 1.660053]\n",
      "epoch:4 step:3576 [D loss: 0.192223, acc.: 95.31%] [G loss: 1.331846]\n",
      "epoch:4 step:3577 [D loss: 0.091091, acc.: 96.88%] [G loss: 1.050858]\n",
      "epoch:4 step:3578 [D loss: 0.067632, acc.: 99.22%] [G loss: 0.161659]\n",
      "epoch:4 step:3579 [D loss: 1.197719, acc.: 53.91%] [G loss: 8.868268]\n",
      "epoch:4 step:3580 [D loss: 2.312138, acc.: 50.00%] [G loss: 5.545999]\n",
      "epoch:4 step:3581 [D loss: 0.506963, acc.: 73.44%] [G loss: 1.077222]\n",
      "epoch:4 step:3582 [D loss: 0.205992, acc.: 90.62%] [G loss: 2.006646]\n",
      "epoch:4 step:3583 [D loss: 0.211751, acc.: 90.62%] [G loss: 3.139194]\n",
      "epoch:4 step:3584 [D loss: 0.280925, acc.: 85.94%] [G loss: 1.881211]\n",
      "epoch:4 step:3585 [D loss: 0.154059, acc.: 96.88%] [G loss: 2.269896]\n",
      "epoch:4 step:3586 [D loss: 0.091300, acc.: 98.44%] [G loss: 1.867508]\n",
      "epoch:4 step:3587 [D loss: 0.442644, acc.: 83.59%] [G loss: 4.057304]\n",
      "epoch:4 step:3588 [D loss: 0.218183, acc.: 89.84%] [G loss: 3.432926]\n",
      "epoch:4 step:3589 [D loss: 0.153463, acc.: 97.66%] [G loss: 3.033496]\n",
      "epoch:4 step:3590 [D loss: 0.166011, acc.: 96.09%] [G loss: 3.156877]\n",
      "epoch:4 step:3591 [D loss: 0.305961, acc.: 89.84%] [G loss: 3.426979]\n",
      "epoch:4 step:3592 [D loss: 0.060587, acc.: 99.22%] [G loss: 4.362801]\n",
      "epoch:4 step:3593 [D loss: 0.277500, acc.: 92.19%] [G loss: 2.743918]\n",
      "epoch:4 step:3594 [D loss: 0.144972, acc.: 96.09%] [G loss: 3.122441]\n",
      "epoch:4 step:3595 [D loss: 0.200137, acc.: 94.53%] [G loss: 2.791862]\n",
      "epoch:4 step:3596 [D loss: 0.222656, acc.: 91.41%] [G loss: 3.442134]\n",
      "epoch:4 step:3597 [D loss: 0.211903, acc.: 92.97%] [G loss: 2.529896]\n",
      "epoch:4 step:3598 [D loss: 0.126644, acc.: 96.09%] [G loss: 3.004742]\n",
      "epoch:4 step:3599 [D loss: 0.281909, acc.: 88.28%] [G loss: 3.488801]\n",
      "epoch:4 step:3600 [D loss: 0.165324, acc.: 94.53%] [G loss: 2.895398]\n",
      "##############\n",
      "[0.97852203 0.9410108  0.78430161 1.00998218 0.88882772 0.95582315\n",
      " 0.83770897 1.11434101 2.10945054 1.03899701]\n",
      "##########\n",
      "epoch:4 step:3601 [D loss: 0.268567, acc.: 89.06%] [G loss: 3.504577]\n",
      "epoch:4 step:3602 [D loss: 0.351160, acc.: 86.72%] [G loss: 3.750432]\n",
      "epoch:4 step:3603 [D loss: 0.179492, acc.: 96.88%] [G loss: 4.350755]\n",
      "epoch:4 step:3604 [D loss: 0.117411, acc.: 97.66%] [G loss: 3.284059]\n",
      "epoch:4 step:3605 [D loss: 0.181607, acc.: 96.09%] [G loss: 3.333491]\n",
      "epoch:4 step:3606 [D loss: 0.146064, acc.: 94.53%] [G loss: 3.914313]\n",
      "epoch:4 step:3607 [D loss: 0.158982, acc.: 95.31%] [G loss: 1.807121]\n",
      "epoch:4 step:3608 [D loss: 0.100165, acc.: 98.44%] [G loss: 2.603933]\n",
      "epoch:4 step:3609 [D loss: 0.244136, acc.: 91.41%] [G loss: 2.889617]\n",
      "epoch:4 step:3610 [D loss: 0.173684, acc.: 92.19%] [G loss: 3.428499]\n",
      "epoch:4 step:3611 [D loss: 0.142187, acc.: 96.09%] [G loss: 1.860696]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:4 step:3612 [D loss: 0.626860, acc.: 71.88%] [G loss: 6.672963]\n",
      "epoch:4 step:3613 [D loss: 0.411339, acc.: 79.69%] [G loss: 3.049302]\n",
      "epoch:4 step:3614 [D loss: 0.037034, acc.: 99.22%] [G loss: 2.139234]\n",
      "epoch:4 step:3615 [D loss: 0.097752, acc.: 97.66%] [G loss: 1.027315]\n",
      "epoch:4 step:3616 [D loss: 1.141552, acc.: 54.69%] [G loss: 8.221634]\n",
      "epoch:4 step:3617 [D loss: 1.278453, acc.: 52.34%] [G loss: 4.882076]\n",
      "epoch:4 step:3618 [D loss: 0.118867, acc.: 95.31%] [G loss: 4.157528]\n",
      "epoch:4 step:3619 [D loss: 0.114314, acc.: 96.09%] [G loss: 4.238564]\n",
      "epoch:4 step:3620 [D loss: 0.027372, acc.: 100.00%] [G loss: 4.345758]\n",
      "epoch:4 step:3621 [D loss: 0.110054, acc.: 98.44%] [G loss: 3.230676]\n",
      "epoch:4 step:3622 [D loss: 0.249318, acc.: 91.41%] [G loss: 4.682019]\n",
      "epoch:4 step:3623 [D loss: 0.268490, acc.: 87.50%] [G loss: 3.506267]\n",
      "epoch:4 step:3624 [D loss: 0.133011, acc.: 96.88%] [G loss: 3.454523]\n",
      "epoch:4 step:3625 [D loss: 0.123943, acc.: 98.44%] [G loss: 2.971771]\n",
      "epoch:4 step:3626 [D loss: 0.441676, acc.: 81.25%] [G loss: 4.556276]\n",
      "epoch:4 step:3627 [D loss: 0.099113, acc.: 96.09%] [G loss: 3.538160]\n",
      "epoch:4 step:3628 [D loss: 0.209140, acc.: 93.75%] [G loss: 1.826369]\n",
      "epoch:4 step:3629 [D loss: 0.110532, acc.: 99.22%] [G loss: 1.816437]\n",
      "epoch:4 step:3630 [D loss: 0.146807, acc.: 97.66%] [G loss: 1.941800]\n",
      "epoch:4 step:3631 [D loss: 0.161711, acc.: 96.88%] [G loss: 1.582343]\n",
      "epoch:4 step:3632 [D loss: 0.175833, acc.: 94.53%] [G loss: 3.047400]\n",
      "epoch:4 step:3633 [D loss: 0.146886, acc.: 95.31%] [G loss: 2.192851]\n",
      "epoch:4 step:3634 [D loss: 0.079989, acc.: 100.00%] [G loss: 1.691381]\n",
      "epoch:4 step:3635 [D loss: 0.462686, acc.: 80.47%] [G loss: 5.634389]\n",
      "epoch:4 step:3636 [D loss: 0.341479, acc.: 82.81%] [G loss: 4.327088]\n",
      "epoch:4 step:3637 [D loss: 0.234884, acc.: 92.19%] [G loss: 2.340237]\n",
      "epoch:4 step:3638 [D loss: 0.432173, acc.: 75.78%] [G loss: 6.788333]\n",
      "epoch:4 step:3639 [D loss: 0.513858, acc.: 71.88%] [G loss: 3.728346]\n",
      "epoch:4 step:3640 [D loss: 0.153065, acc.: 95.31%] [G loss: 4.376305]\n",
      "epoch:4 step:3641 [D loss: 0.077533, acc.: 99.22%] [G loss: 4.323117]\n",
      "epoch:4 step:3642 [D loss: 0.114431, acc.: 96.09%] [G loss: 3.613577]\n",
      "epoch:4 step:3643 [D loss: 0.314817, acc.: 85.94%] [G loss: 4.082500]\n",
      "epoch:4 step:3644 [D loss: 0.076852, acc.: 99.22%] [G loss: 4.006327]\n",
      "epoch:4 step:3645 [D loss: 0.310912, acc.: 89.06%] [G loss: 3.044721]\n",
      "epoch:4 step:3646 [D loss: 0.084613, acc.: 99.22%] [G loss: 4.106065]\n",
      "epoch:4 step:3647 [D loss: 0.202938, acc.: 93.75%] [G loss: 3.439562]\n",
      "epoch:4 step:3648 [D loss: 0.274785, acc.: 87.50%] [G loss: 3.917962]\n",
      "epoch:4 step:3649 [D loss: 0.118098, acc.: 98.44%] [G loss: 3.007950]\n",
      "epoch:4 step:3650 [D loss: 0.161546, acc.: 96.09%] [G loss: 3.287530]\n",
      "epoch:4 step:3651 [D loss: 0.207309, acc.: 92.97%] [G loss: 5.662341]\n",
      "epoch:4 step:3652 [D loss: 0.088728, acc.: 99.22%] [G loss: 5.328103]\n",
      "epoch:4 step:3653 [D loss: 0.167023, acc.: 95.31%] [G loss: 2.750313]\n",
      "epoch:4 step:3654 [D loss: 0.173135, acc.: 95.31%] [G loss: 4.053681]\n",
      "epoch:4 step:3655 [D loss: 0.047724, acc.: 100.00%] [G loss: 5.419491]\n",
      "epoch:4 step:3656 [D loss: 0.178164, acc.: 96.09%] [G loss: 2.206746]\n",
      "epoch:4 step:3657 [D loss: 1.018824, acc.: 57.81%] [G loss: 9.385017]\n",
      "epoch:4 step:3658 [D loss: 2.185181, acc.: 50.00%] [G loss: 5.035929]\n",
      "epoch:4 step:3659 [D loss: 0.349486, acc.: 83.59%] [G loss: 2.498479]\n",
      "epoch:4 step:3660 [D loss: 0.259777, acc.: 90.62%] [G loss: 4.197774]\n",
      "epoch:4 step:3661 [D loss: 0.160125, acc.: 95.31%] [G loss: 3.447625]\n",
      "epoch:4 step:3662 [D loss: 0.100081, acc.: 96.88%] [G loss: 1.634350]\n",
      "epoch:4 step:3663 [D loss: 0.692516, acc.: 64.06%] [G loss: 5.668494]\n",
      "epoch:4 step:3664 [D loss: 0.926461, acc.: 57.03%] [G loss: 2.483031]\n",
      "epoch:4 step:3665 [D loss: 0.209791, acc.: 92.19%] [G loss: 3.573966]\n",
      "epoch:4 step:3666 [D loss: 0.307744, acc.: 85.94%] [G loss: 3.104455]\n",
      "epoch:4 step:3667 [D loss: 0.224865, acc.: 94.53%] [G loss: 3.604668]\n",
      "epoch:4 step:3668 [D loss: 0.178214, acc.: 96.09%] [G loss: 4.385555]\n",
      "epoch:4 step:3669 [D loss: 0.198454, acc.: 94.53%] [G loss: 2.535561]\n",
      "epoch:4 step:3670 [D loss: 0.555067, acc.: 75.00%] [G loss: 4.399256]\n",
      "epoch:4 step:3671 [D loss: 0.225859, acc.: 90.62%] [G loss: 4.331228]\n",
      "epoch:4 step:3672 [D loss: 0.169195, acc.: 95.31%] [G loss: 2.939215]\n",
      "epoch:4 step:3673 [D loss: 0.249002, acc.: 88.28%] [G loss: 4.383040]\n",
      "epoch:4 step:3674 [D loss: 0.393913, acc.: 82.03%] [G loss: 2.291020]\n",
      "epoch:4 step:3675 [D loss: 0.329698, acc.: 85.94%] [G loss: 5.186479]\n",
      "epoch:4 step:3676 [D loss: 0.363483, acc.: 82.81%] [G loss: 3.436417]\n",
      "epoch:4 step:3677 [D loss: 0.097948, acc.: 97.66%] [G loss: 4.411679]\n",
      "epoch:4 step:3678 [D loss: 0.314298, acc.: 85.94%] [G loss: 3.137945]\n",
      "epoch:4 step:3679 [D loss: 0.133152, acc.: 96.88%] [G loss: 4.103344]\n",
      "epoch:4 step:3680 [D loss: 0.153506, acc.: 96.09%] [G loss: 4.200158]\n",
      "epoch:4 step:3681 [D loss: 0.209085, acc.: 95.31%] [G loss: 4.310017]\n",
      "epoch:4 step:3682 [D loss: 0.761245, acc.: 61.72%] [G loss: 3.487426]\n",
      "epoch:4 step:3683 [D loss: 0.082110, acc.: 99.22%] [G loss: 4.896643]\n",
      "epoch:4 step:3684 [D loss: 0.505166, acc.: 75.78%] [G loss: 3.488832]\n",
      "epoch:4 step:3685 [D loss: 0.149978, acc.: 97.66%] [G loss: 3.674821]\n",
      "epoch:4 step:3686 [D loss: 0.169755, acc.: 96.09%] [G loss: 3.007182]\n",
      "epoch:4 step:3687 [D loss: 0.235475, acc.: 92.19%] [G loss: 4.398620]\n",
      "epoch:4 step:3688 [D loss: 0.189676, acc.: 94.53%] [G loss: 3.529006]\n",
      "epoch:4 step:3689 [D loss: 0.254487, acc.: 90.62%] [G loss: 6.186118]\n",
      "epoch:4 step:3690 [D loss: 0.348485, acc.: 81.25%] [G loss: 3.747816]\n",
      "epoch:4 step:3691 [D loss: 0.125213, acc.: 95.31%] [G loss: 3.788498]\n",
      "epoch:4 step:3692 [D loss: 0.023243, acc.: 100.00%] [G loss: 3.592137]\n",
      "epoch:4 step:3693 [D loss: 0.049126, acc.: 100.00%] [G loss: 3.051286]\n",
      "epoch:4 step:3694 [D loss: 0.131766, acc.: 94.53%] [G loss: 2.895531]\n",
      "epoch:4 step:3695 [D loss: 0.065984, acc.: 100.00%] [G loss: 3.216370]\n",
      "epoch:4 step:3696 [D loss: 0.088277, acc.: 100.00%] [G loss: 2.581607]\n",
      "epoch:4 step:3697 [D loss: 0.154828, acc.: 95.31%] [G loss: 1.550540]\n",
      "epoch:4 step:3698 [D loss: 1.023164, acc.: 49.22%] [G loss: 6.024612]\n",
      "epoch:4 step:3699 [D loss: 1.305080, acc.: 53.12%] [G loss: 2.728453]\n",
      "epoch:4 step:3700 [D loss: 0.233956, acc.: 88.28%] [G loss: 4.229378]\n",
      "epoch:4 step:3701 [D loss: 0.069591, acc.: 99.22%] [G loss: 4.147865]\n",
      "epoch:4 step:3702 [D loss: 0.098103, acc.: 99.22%] [G loss: 2.845380]\n",
      "epoch:4 step:3703 [D loss: 0.211298, acc.: 93.75%] [G loss: 3.136970]\n",
      "epoch:4 step:3704 [D loss: 0.333950, acc.: 85.16%] [G loss: 2.194823]\n",
      "epoch:4 step:3705 [D loss: 0.232553, acc.: 92.97%] [G loss: 2.994910]\n",
      "epoch:4 step:3706 [D loss: 0.330396, acc.: 88.28%] [G loss: 3.922785]\n",
      "epoch:4 step:3707 [D loss: 0.314828, acc.: 88.28%] [G loss: 2.431925]\n",
      "epoch:4 step:3708 [D loss: 0.106371, acc.: 96.88%] [G loss: 2.688213]\n",
      "epoch:4 step:3709 [D loss: 0.373796, acc.: 85.94%] [G loss: 4.118289]\n",
      "epoch:4 step:3710 [D loss: 0.120585, acc.: 97.66%] [G loss: 2.995514]\n",
      "epoch:4 step:3711 [D loss: 0.309124, acc.: 85.94%] [G loss: 3.454869]\n",
      "epoch:4 step:3712 [D loss: 0.149719, acc.: 97.66%] [G loss: 2.649259]\n",
      "epoch:4 step:3713 [D loss: 0.188020, acc.: 94.53%] [G loss: 4.310323]\n",
      "epoch:4 step:3714 [D loss: 0.063705, acc.: 99.22%] [G loss: 3.938943]\n",
      "epoch:4 step:3715 [D loss: 0.167342, acc.: 95.31%] [G loss: 3.454410]\n",
      "epoch:4 step:3716 [D loss: 0.134669, acc.: 97.66%] [G loss: 2.176920]\n",
      "epoch:4 step:3717 [D loss: 0.139034, acc.: 96.88%] [G loss: 3.183564]\n",
      "epoch:4 step:3718 [D loss: 0.045210, acc.: 100.00%] [G loss: 3.027308]\n",
      "epoch:4 step:3719 [D loss: 0.076390, acc.: 98.44%] [G loss: 1.395660]\n",
      "epoch:4 step:3720 [D loss: 0.147518, acc.: 97.66%] [G loss: 1.328224]\n",
      "epoch:4 step:3721 [D loss: 0.057149, acc.: 99.22%] [G loss: 2.712960]\n",
      "epoch:4 step:3722 [D loss: 0.460935, acc.: 74.22%] [G loss: 5.784884]\n",
      "epoch:4 step:3723 [D loss: 0.508618, acc.: 73.44%] [G loss: 1.289538]\n",
      "epoch:4 step:3724 [D loss: 0.047056, acc.: 99.22%] [G loss: 1.166141]\n",
      "epoch:4 step:3725 [D loss: 0.169732, acc.: 96.09%] [G loss: 4.957234]\n",
      "epoch:4 step:3726 [D loss: 0.470959, acc.: 75.78%] [G loss: 1.413672]\n",
      "epoch:4 step:3727 [D loss: 0.137225, acc.: 92.97%] [G loss: 3.022881]\n",
      "epoch:4 step:3728 [D loss: 0.060349, acc.: 99.22%] [G loss: 2.871226]\n",
      "epoch:4 step:3729 [D loss: 0.093834, acc.: 99.22%] [G loss: 0.838144]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:4 step:3730 [D loss: 0.399324, acc.: 79.69%] [G loss: 6.195562]\n",
      "epoch:4 step:3731 [D loss: 1.680248, acc.: 41.41%] [G loss: 3.840285]\n",
      "epoch:4 step:3732 [D loss: 0.016240, acc.: 100.00%] [G loss: 4.147688]\n",
      "epoch:4 step:3733 [D loss: 0.083895, acc.: 96.88%] [G loss: 2.419004]\n",
      "epoch:4 step:3734 [D loss: 0.121323, acc.: 96.88%] [G loss: 2.075218]\n",
      "epoch:4 step:3735 [D loss: 0.166440, acc.: 94.53%] [G loss: 4.450186]\n",
      "epoch:4 step:3736 [D loss: 0.113613, acc.: 96.88%] [G loss: 3.387525]\n",
      "epoch:4 step:3737 [D loss: 1.327779, acc.: 32.03%] [G loss: 4.773615]\n",
      "epoch:4 step:3738 [D loss: 0.011425, acc.: 100.00%] [G loss: 6.093832]\n",
      "epoch:4 step:3739 [D loss: 0.250983, acc.: 88.28%] [G loss: 3.633150]\n",
      "epoch:4 step:3740 [D loss: 0.125991, acc.: 97.66%] [G loss: 3.074288]\n",
      "epoch:4 step:3741 [D loss: 0.091326, acc.: 98.44%] [G loss: 2.898268]\n",
      "epoch:4 step:3742 [D loss: 0.096483, acc.: 99.22%] [G loss: 3.360995]\n",
      "epoch:4 step:3743 [D loss: 0.231493, acc.: 89.84%] [G loss: 2.940695]\n",
      "epoch:4 step:3744 [D loss: 0.096073, acc.: 99.22%] [G loss: 3.297323]\n",
      "epoch:4 step:3745 [D loss: 0.086460, acc.: 97.66%] [G loss: 3.469920]\n",
      "epoch:4 step:3746 [D loss: 0.102338, acc.: 97.66%] [G loss: 2.863385]\n",
      "epoch:4 step:3747 [D loss: 0.342704, acc.: 88.28%] [G loss: 0.838724]\n",
      "epoch:4 step:3748 [D loss: 0.029350, acc.: 100.00%] [G loss: 1.134317]\n",
      "epoch:4 step:3749 [D loss: 0.135532, acc.: 97.66%] [G loss: 2.534146]\n",
      "epoch:4 step:3750 [D loss: 0.099083, acc.: 99.22%] [G loss: 3.969168]\n",
      "epoch:4 step:3751 [D loss: 1.669339, acc.: 24.22%] [G loss: 6.105661]\n",
      "epoch:4 step:3752 [D loss: 0.438371, acc.: 75.00%] [G loss: 4.565216]\n",
      "epoch:4 step:3753 [D loss: 0.175338, acc.: 96.09%] [G loss: 3.419043]\n",
      "epoch:4 step:3754 [D loss: 0.123117, acc.: 96.88%] [G loss: 4.240345]\n",
      "epoch:4 step:3755 [D loss: 0.172158, acc.: 95.31%] [G loss: 3.479293]\n",
      "epoch:4 step:3756 [D loss: 0.541651, acc.: 73.44%] [G loss: 4.392211]\n",
      "epoch:4 step:3757 [D loss: 0.092319, acc.: 98.44%] [G loss: 3.613532]\n",
      "epoch:4 step:3758 [D loss: 0.252034, acc.: 91.41%] [G loss: 1.139169]\n",
      "epoch:4 step:3759 [D loss: 0.664256, acc.: 65.62%] [G loss: 5.247520]\n",
      "epoch:4 step:3760 [D loss: 0.290715, acc.: 87.50%] [G loss: 5.147815]\n",
      "epoch:4 step:3761 [D loss: 0.202809, acc.: 90.62%] [G loss: 3.379379]\n",
      "epoch:4 step:3762 [D loss: 0.047312, acc.: 100.00%] [G loss: 2.181851]\n",
      "epoch:4 step:3763 [D loss: 0.096986, acc.: 96.88%] [G loss: 1.382829]\n",
      "epoch:4 step:3764 [D loss: 0.086766, acc.: 98.44%] [G loss: 2.120587]\n",
      "epoch:4 step:3765 [D loss: 0.139227, acc.: 98.44%] [G loss: 2.554096]\n",
      "epoch:4 step:3766 [D loss: 0.085501, acc.: 99.22%] [G loss: 1.207474]\n",
      "epoch:4 step:3767 [D loss: 0.151996, acc.: 96.88%] [G loss: 1.125921]\n",
      "epoch:4 step:3768 [D loss: 0.145019, acc.: 97.66%] [G loss: 1.776232]\n",
      "epoch:4 step:3769 [D loss: 0.120021, acc.: 97.66%] [G loss: 2.681663]\n",
      "epoch:4 step:3770 [D loss: 1.048951, acc.: 42.97%] [G loss: 4.292275]\n",
      "epoch:4 step:3771 [D loss: 0.146744, acc.: 94.53%] [G loss: 4.319053]\n",
      "epoch:4 step:3772 [D loss: 0.086733, acc.: 98.44%] [G loss: 4.125894]\n",
      "epoch:4 step:3773 [D loss: 0.312701, acc.: 88.28%] [G loss: 3.260410]\n",
      "epoch:4 step:3774 [D loss: 0.209127, acc.: 92.19%] [G loss: 4.002400]\n",
      "epoch:4 step:3775 [D loss: 0.069449, acc.: 98.44%] [G loss: 3.783211]\n",
      "epoch:4 step:3776 [D loss: 0.174180, acc.: 96.88%] [G loss: 2.996467]\n",
      "epoch:4 step:3777 [D loss: 0.128477, acc.: 97.66%] [G loss: 3.269848]\n",
      "epoch:4 step:3778 [D loss: 0.212780, acc.: 92.19%] [G loss: 1.857947]\n",
      "epoch:4 step:3779 [D loss: 0.467822, acc.: 77.34%] [G loss: 5.589991]\n",
      "epoch:4 step:3780 [D loss: 0.795335, acc.: 65.62%] [G loss: 1.906536]\n",
      "epoch:4 step:3781 [D loss: 0.137893, acc.: 95.31%] [G loss: 3.273243]\n",
      "epoch:4 step:3782 [D loss: 0.036867, acc.: 100.00%] [G loss: 3.258065]\n",
      "epoch:4 step:3783 [D loss: 0.164437, acc.: 95.31%] [G loss: 3.874727]\n",
      "epoch:4 step:3784 [D loss: 0.208170, acc.: 93.75%] [G loss: 2.335516]\n",
      "epoch:4 step:3785 [D loss: 0.122667, acc.: 97.66%] [G loss: 2.267945]\n",
      "epoch:4 step:3786 [D loss: 0.148772, acc.: 97.66%] [G loss: 4.082176]\n",
      "epoch:4 step:3787 [D loss: 0.289042, acc.: 90.62%] [G loss: 3.034021]\n",
      "epoch:4 step:3788 [D loss: 0.213571, acc.: 93.75%] [G loss: 3.106724]\n",
      "epoch:4 step:3789 [D loss: 0.309163, acc.: 89.06%] [G loss: 3.865822]\n",
      "epoch:4 step:3790 [D loss: 0.324191, acc.: 87.50%] [G loss: 4.818155]\n",
      "epoch:4 step:3791 [D loss: 0.105765, acc.: 99.22%] [G loss: 4.090592]\n",
      "epoch:4 step:3792 [D loss: 0.175755, acc.: 97.66%] [G loss: 3.971173]\n",
      "epoch:4 step:3793 [D loss: 0.334643, acc.: 84.38%] [G loss: 4.983802]\n",
      "epoch:4 step:3794 [D loss: 0.287061, acc.: 88.28%] [G loss: 3.803051]\n",
      "epoch:4 step:3795 [D loss: 0.346797, acc.: 81.25%] [G loss: 5.433644]\n",
      "epoch:4 step:3796 [D loss: 0.044716, acc.: 100.00%] [G loss: 5.566101]\n",
      "epoch:4 step:3797 [D loss: 0.094971, acc.: 97.66%] [G loss: 4.550746]\n",
      "epoch:4 step:3798 [D loss: 0.717517, acc.: 67.97%] [G loss: 7.344282]\n",
      "epoch:4 step:3799 [D loss: 0.328406, acc.: 82.03%] [G loss: 5.825873]\n",
      "epoch:4 step:3800 [D loss: 0.158155, acc.: 94.53%] [G loss: 2.866642]\n",
      "##############\n",
      "[0.94256346 0.84391153 0.94083761 0.9316377  1.11821348 0.80600672\n",
      " 1.01450441 1.12057826 1.09808794 1.05729549]\n",
      "##########\n",
      "epoch:4 step:3801 [D loss: 0.216851, acc.: 92.19%] [G loss: 4.913328]\n",
      "epoch:4 step:3802 [D loss: 0.060791, acc.: 99.22%] [G loss: 5.068932]\n",
      "epoch:4 step:3803 [D loss: 0.214191, acc.: 90.62%] [G loss: 1.614355]\n",
      "epoch:4 step:3804 [D loss: 0.535365, acc.: 73.44%] [G loss: 6.805026]\n",
      "epoch:4 step:3805 [D loss: 0.713475, acc.: 65.62%] [G loss: 3.484089]\n",
      "epoch:4 step:3806 [D loss: 0.244275, acc.: 90.62%] [G loss: 2.678068]\n",
      "epoch:4 step:3807 [D loss: 0.066177, acc.: 98.44%] [G loss: 3.751077]\n",
      "epoch:4 step:3808 [D loss: 0.065241, acc.: 99.22%] [G loss: 1.525869]\n",
      "epoch:4 step:3809 [D loss: 0.259395, acc.: 90.62%] [G loss: 3.082685]\n",
      "epoch:4 step:3810 [D loss: 0.142037, acc.: 92.97%] [G loss: 1.828871]\n",
      "epoch:4 step:3811 [D loss: 0.163084, acc.: 96.09%] [G loss: 2.669678]\n",
      "epoch:4 step:3812 [D loss: 0.131546, acc.: 98.44%] [G loss: 3.329596]\n",
      "epoch:4 step:3813 [D loss: 0.198247, acc.: 92.97%] [G loss: 1.680484]\n",
      "epoch:4 step:3814 [D loss: 0.429897, acc.: 75.00%] [G loss: 6.057995]\n",
      "epoch:4 step:3815 [D loss: 0.434378, acc.: 78.12%] [G loss: 3.528260]\n",
      "epoch:4 step:3816 [D loss: 0.999855, acc.: 48.44%] [G loss: 5.225645]\n",
      "epoch:4 step:3817 [D loss: 0.085817, acc.: 97.66%] [G loss: 5.689346]\n",
      "epoch:4 step:3818 [D loss: 0.469491, acc.: 78.12%] [G loss: 3.501323]\n",
      "epoch:4 step:3819 [D loss: 0.141272, acc.: 96.88%] [G loss: 4.129541]\n",
      "epoch:4 step:3820 [D loss: 0.073405, acc.: 100.00%] [G loss: 4.183067]\n",
      "epoch:4 step:3821 [D loss: 0.333795, acc.: 85.16%] [G loss: 5.553732]\n",
      "epoch:4 step:3822 [D loss: 0.401221, acc.: 79.69%] [G loss: 2.962833]\n",
      "epoch:4 step:3823 [D loss: 0.251791, acc.: 89.06%] [G loss: 4.465301]\n",
      "epoch:4 step:3824 [D loss: 0.121227, acc.: 93.75%] [G loss: 4.758260]\n",
      "epoch:4 step:3825 [D loss: 0.231540, acc.: 89.84%] [G loss: 5.693955]\n",
      "epoch:4 step:3826 [D loss: 0.277363, acc.: 85.94%] [G loss: 2.860491]\n",
      "epoch:4 step:3827 [D loss: 0.232104, acc.: 88.28%] [G loss: 4.730516]\n",
      "epoch:4 step:3828 [D loss: 0.074869, acc.: 97.66%] [G loss: 5.681424]\n",
      "epoch:4 step:3829 [D loss: 0.718875, acc.: 64.06%] [G loss: 3.044837]\n",
      "epoch:4 step:3830 [D loss: 0.141585, acc.: 95.31%] [G loss: 4.386200]\n",
      "epoch:4 step:3831 [D loss: 0.440824, acc.: 75.78%] [G loss: 2.455665]\n",
      "epoch:4 step:3832 [D loss: 0.264106, acc.: 89.06%] [G loss: 5.163916]\n",
      "epoch:4 step:3833 [D loss: 0.398559, acc.: 80.47%] [G loss: 2.525666]\n",
      "epoch:4 step:3834 [D loss: 0.221619, acc.: 90.62%] [G loss: 4.321442]\n",
      "epoch:4 step:3835 [D loss: 0.221842, acc.: 92.97%] [G loss: 3.046237]\n",
      "epoch:4 step:3836 [D loss: 0.127526, acc.: 97.66%] [G loss: 2.531823]\n",
      "epoch:4 step:3837 [D loss: 0.147033, acc.: 94.53%] [G loss: 2.241671]\n",
      "epoch:4 step:3838 [D loss: 0.122374, acc.: 96.09%] [G loss: 0.913206]\n",
      "epoch:4 step:3839 [D loss: 0.513919, acc.: 77.34%] [G loss: 7.728907]\n",
      "epoch:4 step:3840 [D loss: 0.543023, acc.: 72.66%] [G loss: 4.452654]\n",
      "epoch:4 step:3841 [D loss: 0.044724, acc.: 99.22%] [G loss: 1.554534]\n",
      "epoch:4 step:3842 [D loss: 0.080563, acc.: 98.44%] [G loss: 2.714143]\n",
      "epoch:4 step:3843 [D loss: 0.314723, acc.: 83.59%] [G loss: 5.338282]\n",
      "epoch:4 step:3844 [D loss: 0.513372, acc.: 76.56%] [G loss: 2.556501]\n",
      "epoch:4 step:3845 [D loss: 0.291510, acc.: 90.62%] [G loss: 4.905648]\n",
      "epoch:4 step:3846 [D loss: 0.221448, acc.: 89.84%] [G loss: 2.798208]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:4 step:3847 [D loss: 0.331943, acc.: 81.25%] [G loss: 5.030103]\n",
      "epoch:4 step:3848 [D loss: 0.445196, acc.: 75.78%] [G loss: 4.052515]\n",
      "epoch:4 step:3849 [D loss: 0.169097, acc.: 94.53%] [G loss: 3.477237]\n",
      "epoch:4 step:3850 [D loss: 0.047843, acc.: 99.22%] [G loss: 3.845457]\n",
      "epoch:4 step:3851 [D loss: 0.114232, acc.: 96.88%] [G loss: 1.370643]\n",
      "epoch:4 step:3852 [D loss: 0.101959, acc.: 99.22%] [G loss: 1.412298]\n",
      "epoch:4 step:3853 [D loss: 0.052406, acc.: 100.00%] [G loss: 1.484064]\n",
      "epoch:4 step:3854 [D loss: 0.164851, acc.: 93.75%] [G loss: 0.786278]\n",
      "epoch:4 step:3855 [D loss: 0.093081, acc.: 99.22%] [G loss: 1.554220]\n",
      "epoch:4 step:3856 [D loss: 0.933316, acc.: 49.22%] [G loss: 4.231320]\n",
      "epoch:4 step:3857 [D loss: 0.130466, acc.: 95.31%] [G loss: 5.049067]\n",
      "epoch:4 step:3858 [D loss: 0.793787, acc.: 62.50%] [G loss: 0.910268]\n",
      "epoch:4 step:3859 [D loss: 0.166463, acc.: 94.53%] [G loss: 1.694687]\n",
      "epoch:4 step:3860 [D loss: 0.043710, acc.: 99.22%] [G loss: 2.789678]\n",
      "epoch:4 step:3861 [D loss: 0.111245, acc.: 96.88%] [G loss: 3.140418]\n",
      "epoch:4 step:3862 [D loss: 0.341540, acc.: 80.47%] [G loss: 3.392332]\n",
      "epoch:4 step:3863 [D loss: 0.798507, acc.: 56.25%] [G loss: 7.026488]\n",
      "epoch:4 step:3864 [D loss: 0.635668, acc.: 69.53%] [G loss: 5.099700]\n",
      "epoch:4 step:3865 [D loss: 0.130807, acc.: 94.53%] [G loss: 4.435991]\n",
      "epoch:4 step:3866 [D loss: 0.227535, acc.: 89.84%] [G loss: 4.514239]\n",
      "epoch:4 step:3867 [D loss: 0.049685, acc.: 99.22%] [G loss: 3.778767]\n",
      "epoch:4 step:3868 [D loss: 0.135657, acc.: 95.31%] [G loss: 3.482111]\n",
      "epoch:4 step:3869 [D loss: 0.106374, acc.: 97.66%] [G loss: 2.569805]\n",
      "epoch:4 step:3870 [D loss: 0.434296, acc.: 78.91%] [G loss: 4.847509]\n",
      "epoch:4 step:3871 [D loss: 0.278721, acc.: 84.38%] [G loss: 2.545712]\n",
      "epoch:4 step:3872 [D loss: 0.346329, acc.: 83.59%] [G loss: 5.249720]\n",
      "epoch:4 step:3873 [D loss: 0.422269, acc.: 77.34%] [G loss: 3.147652]\n",
      "epoch:4 step:3874 [D loss: 0.518740, acc.: 70.31%] [G loss: 5.189071]\n",
      "epoch:4 step:3875 [D loss: 0.166573, acc.: 94.53%] [G loss: 4.145503]\n",
      "epoch:4 step:3876 [D loss: 0.111998, acc.: 95.31%] [G loss: 2.970329]\n",
      "epoch:4 step:3877 [D loss: 0.207278, acc.: 94.53%] [G loss: 3.548068]\n",
      "epoch:4 step:3878 [D loss: 0.192361, acc.: 94.53%] [G loss: 4.344481]\n",
      "epoch:4 step:3879 [D loss: 0.076452, acc.: 98.44%] [G loss: 3.627262]\n",
      "epoch:4 step:3880 [D loss: 0.408679, acc.: 82.03%] [G loss: 3.118631]\n",
      "epoch:4 step:3881 [D loss: 0.059271, acc.: 98.44%] [G loss: 3.146170]\n",
      "epoch:4 step:3882 [D loss: 0.149492, acc.: 95.31%] [G loss: 2.526607]\n",
      "epoch:4 step:3883 [D loss: 0.135230, acc.: 95.31%] [G loss: 2.854073]\n",
      "epoch:4 step:3884 [D loss: 0.053487, acc.: 100.00%] [G loss: 2.700603]\n",
      "epoch:4 step:3885 [D loss: 0.074487, acc.: 98.44%] [G loss: 1.012828]\n",
      "epoch:4 step:3886 [D loss: 0.447432, acc.: 78.91%] [G loss: 3.922020]\n",
      "epoch:4 step:3887 [D loss: 0.788359, acc.: 64.84%] [G loss: 1.051518]\n",
      "epoch:4 step:3888 [D loss: 0.592081, acc.: 71.88%] [G loss: 5.398949]\n",
      "epoch:4 step:3889 [D loss: 0.294910, acc.: 84.38%] [G loss: 5.625151]\n",
      "epoch:4 step:3890 [D loss: 0.448714, acc.: 81.25%] [G loss: 1.491343]\n",
      "epoch:4 step:3891 [D loss: 0.218754, acc.: 92.97%] [G loss: 3.412566]\n",
      "epoch:4 step:3892 [D loss: 0.088661, acc.: 100.00%] [G loss: 4.535901]\n",
      "epoch:4 step:3893 [D loss: 0.051421, acc.: 100.00%] [G loss: 3.000196]\n",
      "epoch:4 step:3894 [D loss: 0.281826, acc.: 88.28%] [G loss: 4.692928]\n",
      "epoch:4 step:3895 [D loss: 0.732818, acc.: 65.62%] [G loss: 2.735093]\n",
      "epoch:4 step:3896 [D loss: 0.157143, acc.: 95.31%] [G loss: 3.681035]\n",
      "epoch:4 step:3897 [D loss: 0.088776, acc.: 97.66%] [G loss: 4.272157]\n",
      "epoch:4 step:3898 [D loss: 0.168228, acc.: 96.88%] [G loss: 2.702653]\n",
      "epoch:4 step:3899 [D loss: 0.117851, acc.: 96.88%] [G loss: 3.392018]\n",
      "epoch:4 step:3900 [D loss: 0.084051, acc.: 98.44%] [G loss: 3.418284]\n",
      "epoch:4 step:3901 [D loss: 0.256738, acc.: 89.84%] [G loss: 3.891235]\n",
      "epoch:4 step:3902 [D loss: 0.190301, acc.: 93.75%] [G loss: 3.737396]\n",
      "epoch:4 step:3903 [D loss: 0.457671, acc.: 78.91%] [G loss: 2.087325]\n",
      "epoch:4 step:3904 [D loss: 0.138527, acc.: 96.88%] [G loss: 3.754416]\n",
      "epoch:4 step:3905 [D loss: 0.092735, acc.: 98.44%] [G loss: 4.000510]\n",
      "epoch:5 step:3906 [D loss: 0.217951, acc.: 92.97%] [G loss: 1.061007]\n",
      "epoch:5 step:3907 [D loss: 0.287588, acc.: 83.59%] [G loss: 4.412570]\n",
      "epoch:5 step:3908 [D loss: 0.190319, acc.: 92.19%] [G loss: 3.878628]\n",
      "epoch:5 step:3909 [D loss: 0.094729, acc.: 100.00%] [G loss: 2.649939]\n",
      "epoch:5 step:3910 [D loss: 0.230198, acc.: 91.41%] [G loss: 3.388366]\n",
      "epoch:5 step:3911 [D loss: 0.133052, acc.: 96.88%] [G loss: 3.243655]\n",
      "epoch:5 step:3912 [D loss: 0.276185, acc.: 87.50%] [G loss: 4.502016]\n",
      "epoch:5 step:3913 [D loss: 0.177952, acc.: 94.53%] [G loss: 3.936442]\n",
      "epoch:5 step:3914 [D loss: 0.248764, acc.: 93.75%] [G loss: 3.473731]\n",
      "epoch:5 step:3915 [D loss: 0.177740, acc.: 94.53%] [G loss: 4.269771]\n",
      "epoch:5 step:3916 [D loss: 0.138854, acc.: 94.53%] [G loss: 3.378735]\n",
      "epoch:5 step:3917 [D loss: 0.394246, acc.: 81.25%] [G loss: 5.405556]\n",
      "epoch:5 step:3918 [D loss: 0.236220, acc.: 90.62%] [G loss: 4.061544]\n",
      "epoch:5 step:3919 [D loss: 0.109495, acc.: 98.44%] [G loss: 3.391987]\n",
      "epoch:5 step:3920 [D loss: 0.051672, acc.: 99.22%] [G loss: 2.185520]\n",
      "epoch:5 step:3921 [D loss: 0.230732, acc.: 92.19%] [G loss: 3.467318]\n",
      "epoch:5 step:3922 [D loss: 0.125403, acc.: 97.66%] [G loss: 4.542729]\n",
      "epoch:5 step:3923 [D loss: 0.172576, acc.: 92.19%] [G loss: 2.277213]\n",
      "epoch:5 step:3924 [D loss: 0.215825, acc.: 91.41%] [G loss: 3.715531]\n",
      "epoch:5 step:3925 [D loss: 0.118096, acc.: 96.09%] [G loss: 4.352959]\n",
      "epoch:5 step:3926 [D loss: 0.038332, acc.: 100.00%] [G loss: 2.619471]\n",
      "epoch:5 step:3927 [D loss: 0.404874, acc.: 79.69%] [G loss: 6.002892]\n",
      "epoch:5 step:3928 [D loss: 1.036553, acc.: 53.91%] [G loss: 3.694939]\n",
      "epoch:5 step:3929 [D loss: 0.048698, acc.: 98.44%] [G loss: 3.319726]\n",
      "epoch:5 step:3930 [D loss: 0.066212, acc.: 100.00%] [G loss: 2.661768]\n",
      "epoch:5 step:3931 [D loss: 0.325383, acc.: 81.25%] [G loss: 6.390887]\n",
      "epoch:5 step:3932 [D loss: 0.374026, acc.: 82.03%] [G loss: 4.478101]\n",
      "epoch:5 step:3933 [D loss: 0.767388, acc.: 59.38%] [G loss: 7.107429]\n",
      "epoch:5 step:3934 [D loss: 0.339712, acc.: 83.59%] [G loss: 5.327765]\n",
      "epoch:5 step:3935 [D loss: 0.127976, acc.: 96.09%] [G loss: 4.469946]\n",
      "epoch:5 step:3936 [D loss: 0.058286, acc.: 99.22%] [G loss: 4.444359]\n",
      "epoch:5 step:3937 [D loss: 0.244531, acc.: 89.06%] [G loss: 5.545021]\n",
      "epoch:5 step:3938 [D loss: 0.159890, acc.: 95.31%] [G loss: 3.894229]\n",
      "epoch:5 step:3939 [D loss: 0.122355, acc.: 98.44%] [G loss: 4.781008]\n",
      "epoch:5 step:3940 [D loss: 0.540814, acc.: 74.22%] [G loss: 6.397179]\n",
      "epoch:5 step:3941 [D loss: 0.758067, acc.: 63.28%] [G loss: 3.650354]\n",
      "epoch:5 step:3942 [D loss: 0.019882, acc.: 100.00%] [G loss: 1.632946]\n",
      "epoch:5 step:3943 [D loss: 0.100990, acc.: 99.22%] [G loss: 1.463110]\n",
      "epoch:5 step:3944 [D loss: 0.352985, acc.: 78.91%] [G loss: 5.651170]\n",
      "epoch:5 step:3945 [D loss: 0.570626, acc.: 71.09%] [G loss: 1.294039]\n",
      "epoch:5 step:3946 [D loss: 0.857474, acc.: 64.84%] [G loss: 6.622366]\n",
      "epoch:5 step:3947 [D loss: 1.547352, acc.: 52.34%] [G loss: 2.776565]\n",
      "epoch:5 step:3948 [D loss: 0.309367, acc.: 87.50%] [G loss: 3.253112]\n",
      "epoch:5 step:3949 [D loss: 0.089918, acc.: 98.44%] [G loss: 3.933171]\n",
      "epoch:5 step:3950 [D loss: 0.088783, acc.: 98.44%] [G loss: 3.268348]\n",
      "epoch:5 step:3951 [D loss: 0.240308, acc.: 92.97%] [G loss: 3.255306]\n",
      "epoch:5 step:3952 [D loss: 0.142783, acc.: 97.66%] [G loss: 3.076916]\n",
      "epoch:5 step:3953 [D loss: 0.164536, acc.: 96.09%] [G loss: 3.054842]\n",
      "epoch:5 step:3954 [D loss: 0.472431, acc.: 78.12%] [G loss: 3.646117]\n",
      "epoch:5 step:3955 [D loss: 0.419386, acc.: 85.16%] [G loss: 2.838882]\n",
      "epoch:5 step:3956 [D loss: 0.115258, acc.: 96.88%] [G loss: 3.368331]\n",
      "epoch:5 step:3957 [D loss: 0.106496, acc.: 97.66%] [G loss: 3.437553]\n",
      "epoch:5 step:3958 [D loss: 0.181570, acc.: 96.88%] [G loss: 1.995620]\n",
      "epoch:5 step:3959 [D loss: 0.336201, acc.: 84.38%] [G loss: 3.291853]\n",
      "epoch:5 step:3960 [D loss: 0.176217, acc.: 95.31%] [G loss: 3.731587]\n",
      "epoch:5 step:3961 [D loss: 0.262132, acc.: 89.06%] [G loss: 3.083598]\n",
      "epoch:5 step:3962 [D loss: 0.184521, acc.: 95.31%] [G loss: 3.508307]\n",
      "epoch:5 step:3963 [D loss: 0.111261, acc.: 97.66%] [G loss: 4.744276]\n",
      "epoch:5 step:3964 [D loss: 0.111172, acc.: 97.66%] [G loss: 3.639269]\n",
      "epoch:5 step:3965 [D loss: 0.241463, acc.: 90.62%] [G loss: 4.813029]\n",
      "epoch:5 step:3966 [D loss: 0.221377, acc.: 91.41%] [G loss: 3.785591]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:5 step:3967 [D loss: 0.320742, acc.: 89.06%] [G loss: 4.443684]\n",
      "epoch:5 step:3968 [D loss: 0.252769, acc.: 88.28%] [G loss: 4.440035]\n",
      "epoch:5 step:3969 [D loss: 0.135269, acc.: 96.88%] [G loss: 4.128630]\n",
      "epoch:5 step:3970 [D loss: 0.845165, acc.: 54.69%] [G loss: 5.973018]\n",
      "epoch:5 step:3971 [D loss: 0.133993, acc.: 93.75%] [G loss: 6.432811]\n",
      "epoch:5 step:3972 [D loss: 0.247554, acc.: 90.62%] [G loss: 2.843825]\n",
      "epoch:5 step:3973 [D loss: 0.411752, acc.: 78.91%] [G loss: 5.373394]\n",
      "epoch:5 step:3974 [D loss: 0.295236, acc.: 84.38%] [G loss: 4.006899]\n",
      "epoch:5 step:3975 [D loss: 0.328381, acc.: 85.94%] [G loss: 4.383876]\n",
      "epoch:5 step:3976 [D loss: 0.136863, acc.: 95.31%] [G loss: 4.155479]\n",
      "epoch:5 step:3977 [D loss: 0.188351, acc.: 92.97%] [G loss: 4.329050]\n",
      "epoch:5 step:3978 [D loss: 0.325199, acc.: 89.06%] [G loss: 3.757867]\n",
      "epoch:5 step:3979 [D loss: 0.220144, acc.: 92.19%] [G loss: 4.012064]\n",
      "epoch:5 step:3980 [D loss: 0.168233, acc.: 93.75%] [G loss: 3.139853]\n",
      "epoch:5 step:3981 [D loss: 0.500025, acc.: 74.22%] [G loss: 2.165658]\n",
      "epoch:5 step:3982 [D loss: 0.710078, acc.: 66.41%] [G loss: 5.444995]\n",
      "epoch:5 step:3983 [D loss: 0.516454, acc.: 71.09%] [G loss: 3.301468]\n",
      "epoch:5 step:3984 [D loss: 0.088801, acc.: 100.00%] [G loss: 2.100260]\n",
      "epoch:5 step:3985 [D loss: 0.053616, acc.: 99.22%] [G loss: 2.047534]\n",
      "epoch:5 step:3986 [D loss: 0.135249, acc.: 94.53%] [G loss: 3.576672]\n",
      "epoch:5 step:3987 [D loss: 0.091423, acc.: 97.66%] [G loss: 3.704852]\n",
      "epoch:5 step:3988 [D loss: 0.532131, acc.: 75.00%] [G loss: 4.941807]\n",
      "epoch:5 step:3989 [D loss: 0.247470, acc.: 87.50%] [G loss: 3.377882]\n",
      "epoch:5 step:3990 [D loss: 0.112105, acc.: 98.44%] [G loss: 1.738094]\n",
      "epoch:5 step:3991 [D loss: 0.439849, acc.: 79.69%] [G loss: 5.342901]\n",
      "epoch:5 step:3992 [D loss: 0.373748, acc.: 77.34%] [G loss: 4.154475]\n",
      "epoch:5 step:3993 [D loss: 0.269230, acc.: 88.28%] [G loss: 4.836785]\n",
      "epoch:5 step:3994 [D loss: 0.189475, acc.: 91.41%] [G loss: 4.112502]\n",
      "epoch:5 step:3995 [D loss: 1.153266, acc.: 45.31%] [G loss: 7.569757]\n",
      "epoch:5 step:3996 [D loss: 0.182662, acc.: 93.75%] [G loss: 7.558986]\n",
      "epoch:5 step:3997 [D loss: 0.437811, acc.: 77.34%] [G loss: 3.864427]\n",
      "epoch:5 step:3998 [D loss: 0.562273, acc.: 76.56%] [G loss: 6.012959]\n",
      "epoch:5 step:3999 [D loss: 0.085221, acc.: 98.44%] [G loss: 6.520627]\n",
      "epoch:5 step:4000 [D loss: 0.353269, acc.: 82.81%] [G loss: 3.504405]\n",
      "##############\n",
      "[1.05857695 0.92743257 0.96581771 0.86534632 2.10726311 0.9057198\n",
      " 2.10545898 2.11536897 0.94155978 1.08608224]\n",
      "##########\n",
      "epoch:5 step:4001 [D loss: 0.545877, acc.: 71.09%] [G loss: 5.397545]\n",
      "epoch:5 step:4002 [D loss: 0.305203, acc.: 82.81%] [G loss: 4.263038]\n",
      "epoch:5 step:4003 [D loss: 0.150785, acc.: 94.53%] [G loss: 3.608816]\n",
      "epoch:5 step:4004 [D loss: 0.163774, acc.: 96.09%] [G loss: 4.047265]\n",
      "epoch:5 step:4005 [D loss: 0.112645, acc.: 96.88%] [G loss: 3.986707]\n",
      "epoch:5 step:4006 [D loss: 0.179434, acc.: 96.09%] [G loss: 2.500315]\n",
      "epoch:5 step:4007 [D loss: 0.256824, acc.: 86.72%] [G loss: 5.395176]\n",
      "epoch:5 step:4008 [D loss: 0.171318, acc.: 92.19%] [G loss: 4.544143]\n",
      "epoch:5 step:4009 [D loss: 0.176009, acc.: 95.31%] [G loss: 3.706114]\n",
      "epoch:5 step:4010 [D loss: 0.205369, acc.: 92.97%] [G loss: 4.794559]\n",
      "epoch:5 step:4011 [D loss: 0.157317, acc.: 96.09%] [G loss: 3.875032]\n",
      "epoch:5 step:4012 [D loss: 0.275193, acc.: 89.84%] [G loss: 3.856965]\n",
      "epoch:5 step:4013 [D loss: 0.256684, acc.: 92.97%] [G loss: 1.942222]\n",
      "epoch:5 step:4014 [D loss: 0.034989, acc.: 100.00%] [G loss: 1.055638]\n",
      "epoch:5 step:4015 [D loss: 0.167033, acc.: 96.09%] [G loss: 2.114113]\n",
      "epoch:5 step:4016 [D loss: 0.030473, acc.: 100.00%] [G loss: 2.595353]\n",
      "epoch:5 step:4017 [D loss: 0.525048, acc.: 70.31%] [G loss: 2.703009]\n",
      "epoch:5 step:4018 [D loss: 0.100996, acc.: 96.09%] [G loss: 3.688936]\n",
      "epoch:5 step:4019 [D loss: 0.421419, acc.: 83.59%] [G loss: 2.767642]\n",
      "epoch:5 step:4020 [D loss: 0.031915, acc.: 99.22%] [G loss: 5.162357]\n",
      "epoch:5 step:4021 [D loss: 0.445015, acc.: 79.69%] [G loss: 2.035185]\n",
      "epoch:5 step:4022 [D loss: 0.444818, acc.: 78.91%] [G loss: 1.551449]\n",
      "epoch:5 step:4023 [D loss: 0.145162, acc.: 93.75%] [G loss: 3.349713]\n",
      "epoch:5 step:4024 [D loss: 0.151795, acc.: 94.53%] [G loss: 3.977130]\n",
      "epoch:5 step:4025 [D loss: 0.705011, acc.: 59.38%] [G loss: 5.007166]\n",
      "epoch:5 step:4026 [D loss: 0.118653, acc.: 95.31%] [G loss: 5.672189]\n",
      "epoch:5 step:4027 [D loss: 0.096561, acc.: 96.88%] [G loss: 2.900990]\n",
      "epoch:5 step:4028 [D loss: 0.022923, acc.: 100.00%] [G loss: 1.349913]\n",
      "epoch:5 step:4029 [D loss: 0.025919, acc.: 99.22%] [G loss: 0.387580]\n",
      "epoch:5 step:4030 [D loss: 0.146272, acc.: 96.09%] [G loss: 1.943207]\n",
      "epoch:5 step:4031 [D loss: 0.042909, acc.: 98.44%] [G loss: 1.736446]\n",
      "epoch:5 step:4032 [D loss: 0.045199, acc.: 100.00%] [G loss: 0.919310]\n",
      "epoch:5 step:4033 [D loss: 0.033325, acc.: 99.22%] [G loss: 0.499210]\n",
      "epoch:5 step:4034 [D loss: 0.074636, acc.: 100.00%] [G loss: 0.628912]\n",
      "epoch:5 step:4035 [D loss: 0.066153, acc.: 100.00%] [G loss: 1.308790]\n",
      "epoch:5 step:4036 [D loss: 0.078937, acc.: 99.22%] [G loss: 2.516005]\n",
      "epoch:5 step:4037 [D loss: 0.194509, acc.: 91.41%] [G loss: 0.650384]\n",
      "epoch:5 step:4038 [D loss: 0.072621, acc.: 99.22%] [G loss: 0.997204]\n",
      "epoch:5 step:4039 [D loss: 0.166677, acc.: 96.88%] [G loss: 4.210036]\n",
      "epoch:5 step:4040 [D loss: 2.175978, acc.: 10.94%] [G loss: 9.195926]\n",
      "epoch:5 step:4041 [D loss: 1.206745, acc.: 53.12%] [G loss: 6.815396]\n",
      "epoch:5 step:4042 [D loss: 0.232415, acc.: 86.72%] [G loss: 4.313207]\n",
      "epoch:5 step:4043 [D loss: 0.030261, acc.: 100.00%] [G loss: 2.872861]\n",
      "epoch:5 step:4044 [D loss: 0.256172, acc.: 89.84%] [G loss: 2.994389]\n",
      "epoch:5 step:4045 [D loss: 0.102132, acc.: 99.22%] [G loss: 2.432213]\n",
      "epoch:5 step:4046 [D loss: 0.263757, acc.: 87.50%] [G loss: 4.514377]\n",
      "epoch:5 step:4047 [D loss: 0.700032, acc.: 60.94%] [G loss: 4.886207]\n",
      "epoch:5 step:4048 [D loss: 0.211689, acc.: 89.84%] [G loss: 4.672818]\n",
      "epoch:5 step:4049 [D loss: 0.177414, acc.: 94.53%] [G loss: 3.509479]\n",
      "epoch:5 step:4050 [D loss: 0.151654, acc.: 95.31%] [G loss: 4.045565]\n",
      "epoch:5 step:4051 [D loss: 0.275428, acc.: 89.06%] [G loss: 4.504419]\n",
      "epoch:5 step:4052 [D loss: 0.126498, acc.: 96.88%] [G loss: 3.093600]\n",
      "epoch:5 step:4053 [D loss: 0.158892, acc.: 95.31%] [G loss: 2.009049]\n",
      "epoch:5 step:4054 [D loss: 0.165105, acc.: 96.09%] [G loss: 2.505093]\n",
      "epoch:5 step:4055 [D loss: 0.286926, acc.: 88.28%] [G loss: 3.066168]\n",
      "epoch:5 step:4056 [D loss: 0.244252, acc.: 85.94%] [G loss: 1.653236]\n",
      "epoch:5 step:4057 [D loss: 0.244912, acc.: 91.41%] [G loss: 2.589451]\n",
      "epoch:5 step:4058 [D loss: 0.295298, acc.: 86.72%] [G loss: 1.048578]\n",
      "epoch:5 step:4059 [D loss: 0.426332, acc.: 78.91%] [G loss: 3.966132]\n",
      "epoch:5 step:4060 [D loss: 0.763345, acc.: 62.50%] [G loss: 0.807122]\n",
      "epoch:5 step:4061 [D loss: 0.561478, acc.: 71.88%] [G loss: 3.335895]\n",
      "epoch:5 step:4062 [D loss: 0.243727, acc.: 88.28%] [G loss: 2.518901]\n",
      "epoch:5 step:4063 [D loss: 0.194960, acc.: 92.97%] [G loss: 2.545414]\n",
      "epoch:5 step:4064 [D loss: 0.158808, acc.: 93.75%] [G loss: 0.935649]\n",
      "epoch:5 step:4065 [D loss: 0.665406, acc.: 65.62%] [G loss: 4.779301]\n",
      "epoch:5 step:4066 [D loss: 0.693739, acc.: 65.62%] [G loss: 2.936621]\n",
      "epoch:5 step:4067 [D loss: 0.308595, acc.: 85.94%] [G loss: 3.914669]\n",
      "epoch:5 step:4068 [D loss: 0.266468, acc.: 88.28%] [G loss: 3.443978]\n",
      "epoch:5 step:4069 [D loss: 0.042983, acc.: 100.00%] [G loss: 2.624505]\n",
      "epoch:5 step:4070 [D loss: 0.138300, acc.: 98.44%] [G loss: 3.087284]\n",
      "epoch:5 step:4071 [D loss: 0.175626, acc.: 95.31%] [G loss: 3.859520]\n",
      "epoch:5 step:4072 [D loss: 0.324024, acc.: 85.94%] [G loss: 4.553049]\n",
      "epoch:5 step:4073 [D loss: 0.322465, acc.: 85.94%] [G loss: 2.929085]\n",
      "epoch:5 step:4074 [D loss: 0.246687, acc.: 85.16%] [G loss: 3.908883]\n",
      "epoch:5 step:4075 [D loss: 0.080257, acc.: 96.88%] [G loss: 4.278045]\n",
      "epoch:5 step:4076 [D loss: 0.153821, acc.: 96.88%] [G loss: 2.559832]\n",
      "epoch:5 step:4077 [D loss: 0.107470, acc.: 98.44%] [G loss: 2.275924]\n",
      "epoch:5 step:4078 [D loss: 0.169242, acc.: 94.53%] [G loss: 1.624233]\n",
      "epoch:5 step:4079 [D loss: 0.068259, acc.: 100.00%] [G loss: 0.451551]\n",
      "epoch:5 step:4080 [D loss: 0.043496, acc.: 100.00%] [G loss: 0.262040]\n",
      "epoch:5 step:4081 [D loss: 0.430932, acc.: 80.47%] [G loss: 4.136678]\n",
      "epoch:5 step:4082 [D loss: 0.158592, acc.: 95.31%] [G loss: 4.338191]\n",
      "epoch:5 step:4083 [D loss: 0.749577, acc.: 64.84%] [G loss: 0.143555]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:5 step:4084 [D loss: 1.046878, acc.: 60.94%] [G loss: 3.603248]\n",
      "epoch:5 step:4085 [D loss: 0.146118, acc.: 94.53%] [G loss: 3.485712]\n",
      "epoch:5 step:4086 [D loss: 0.179902, acc.: 92.97%] [G loss: 1.999107]\n",
      "epoch:5 step:4087 [D loss: 0.115506, acc.: 97.66%] [G loss: 1.858258]\n",
      "epoch:5 step:4088 [D loss: 0.073967, acc.: 99.22%] [G loss: 1.259786]\n",
      "epoch:5 step:4089 [D loss: 0.242388, acc.: 91.41%] [G loss: 2.727812]\n",
      "epoch:5 step:4090 [D loss: 0.058381, acc.: 100.00%] [G loss: 1.740760]\n",
      "epoch:5 step:4091 [D loss: 0.376319, acc.: 86.72%] [G loss: 2.727399]\n",
      "epoch:5 step:4092 [D loss: 0.654414, acc.: 70.31%] [G loss: 1.878635]\n",
      "epoch:5 step:4093 [D loss: 0.187372, acc.: 93.75%] [G loss: 3.600324]\n",
      "epoch:5 step:4094 [D loss: 0.051424, acc.: 100.00%] [G loss: 4.348461]\n",
      "epoch:5 step:4095 [D loss: 0.287354, acc.: 89.06%] [G loss: 3.250285]\n",
      "epoch:5 step:4096 [D loss: 0.074454, acc.: 99.22%] [G loss: 2.854353]\n",
      "epoch:5 step:4097 [D loss: 0.191497, acc.: 93.75%] [G loss: 3.951764]\n",
      "epoch:5 step:4098 [D loss: 0.662561, acc.: 64.84%] [G loss: 3.678626]\n",
      "epoch:5 step:4099 [D loss: 0.114921, acc.: 97.66%] [G loss: 3.388590]\n",
      "epoch:5 step:4100 [D loss: 0.126332, acc.: 96.88%] [G loss: 2.517865]\n",
      "epoch:5 step:4101 [D loss: 0.189565, acc.: 92.97%] [G loss: 0.504873]\n",
      "epoch:5 step:4102 [D loss: 0.331624, acc.: 84.38%] [G loss: 4.707577]\n",
      "epoch:5 step:4103 [D loss: 0.242913, acc.: 87.50%] [G loss: 4.042815]\n",
      "epoch:5 step:4104 [D loss: 0.093756, acc.: 98.44%] [G loss: 1.298680]\n",
      "epoch:5 step:4105 [D loss: 0.381942, acc.: 81.25%] [G loss: 3.888484]\n",
      "epoch:5 step:4106 [D loss: 0.650131, acc.: 67.97%] [G loss: 0.278497]\n",
      "epoch:5 step:4107 [D loss: 0.585776, acc.: 68.75%] [G loss: 4.542489]\n",
      "epoch:5 step:4108 [D loss: 0.118846, acc.: 97.66%] [G loss: 5.397346]\n",
      "epoch:5 step:4109 [D loss: 0.808415, acc.: 57.81%] [G loss: 1.962015]\n",
      "epoch:5 step:4110 [D loss: 0.251762, acc.: 93.75%] [G loss: 2.688655]\n",
      "epoch:5 step:4111 [D loss: 0.089857, acc.: 97.66%] [G loss: 2.988841]\n",
      "epoch:5 step:4112 [D loss: 0.303679, acc.: 85.16%] [G loss: 3.826293]\n",
      "epoch:5 step:4113 [D loss: 0.283681, acc.: 88.28%] [G loss: 2.576133]\n",
      "epoch:5 step:4114 [D loss: 0.227334, acc.: 93.75%] [G loss: 3.686103]\n",
      "epoch:5 step:4115 [D loss: 0.076454, acc.: 99.22%] [G loss: 4.469888]\n",
      "epoch:5 step:4116 [D loss: 0.191547, acc.: 95.31%] [G loss: 3.504161]\n",
      "epoch:5 step:4117 [D loss: 0.264409, acc.: 91.41%] [G loss: 2.305553]\n",
      "epoch:5 step:4118 [D loss: 0.694710, acc.: 61.72%] [G loss: 4.443458]\n",
      "epoch:5 step:4119 [D loss: 0.063842, acc.: 98.44%] [G loss: 6.164103]\n",
      "epoch:5 step:4120 [D loss: 0.427134, acc.: 78.91%] [G loss: 2.853209]\n",
      "epoch:5 step:4121 [D loss: 0.260868, acc.: 86.72%] [G loss: 4.383683]\n",
      "epoch:5 step:4122 [D loss: 0.053488, acc.: 100.00%] [G loss: 4.485548]\n",
      "epoch:5 step:4123 [D loss: 0.250353, acc.: 90.62%] [G loss: 3.370742]\n",
      "epoch:5 step:4124 [D loss: 0.063293, acc.: 99.22%] [G loss: 2.300877]\n",
      "epoch:5 step:4125 [D loss: 0.518977, acc.: 75.00%] [G loss: 5.667384]\n",
      "epoch:5 step:4126 [D loss: 0.249063, acc.: 89.84%] [G loss: 4.594234]\n",
      "epoch:5 step:4127 [D loss: 0.216161, acc.: 92.19%] [G loss: 3.086649]\n",
      "epoch:5 step:4128 [D loss: 0.206840, acc.: 95.31%] [G loss: 4.168456]\n",
      "epoch:5 step:4129 [D loss: 0.261711, acc.: 92.19%] [G loss: 4.229193]\n",
      "epoch:5 step:4130 [D loss: 0.210032, acc.: 91.41%] [G loss: 2.189342]\n",
      "epoch:5 step:4131 [D loss: 0.296822, acc.: 88.28%] [G loss: 4.718328]\n",
      "epoch:5 step:4132 [D loss: 0.129823, acc.: 96.09%] [G loss: 4.517941]\n",
      "epoch:5 step:4133 [D loss: 0.074134, acc.: 98.44%] [G loss: 2.719249]\n",
      "epoch:5 step:4134 [D loss: 1.213769, acc.: 45.31%] [G loss: 6.471875]\n",
      "epoch:5 step:4135 [D loss: 0.889753, acc.: 61.72%] [G loss: 3.404547]\n",
      "epoch:5 step:4136 [D loss: 0.252968, acc.: 89.06%] [G loss: 3.517172]\n",
      "epoch:5 step:4137 [D loss: 0.275269, acc.: 89.06%] [G loss: 1.951203]\n",
      "epoch:5 step:4138 [D loss: 0.327944, acc.: 83.59%] [G loss: 4.681326]\n",
      "epoch:5 step:4139 [D loss: 0.294502, acc.: 88.28%] [G loss: 3.041290]\n",
      "epoch:5 step:4140 [D loss: 0.126287, acc.: 98.44%] [G loss: 2.424054]\n",
      "epoch:5 step:4141 [D loss: 0.304647, acc.: 86.72%] [G loss: 1.808465]\n",
      "epoch:5 step:4142 [D loss: 0.123307, acc.: 96.88%] [G loss: 3.087389]\n",
      "epoch:5 step:4143 [D loss: 0.117601, acc.: 99.22%] [G loss: 3.233178]\n",
      "epoch:5 step:4144 [D loss: 0.094834, acc.: 97.66%] [G loss: 2.944317]\n",
      "epoch:5 step:4145 [D loss: 0.902115, acc.: 52.34%] [G loss: 4.468245]\n",
      "epoch:5 step:4146 [D loss: 0.203641, acc.: 90.62%] [G loss: 4.819641]\n",
      "epoch:5 step:4147 [D loss: 0.097211, acc.: 98.44%] [G loss: 3.152873]\n",
      "epoch:5 step:4148 [D loss: 0.151305, acc.: 95.31%] [G loss: 3.412838]\n",
      "epoch:5 step:4149 [D loss: 0.161416, acc.: 95.31%] [G loss: 3.256274]\n",
      "epoch:5 step:4150 [D loss: 0.082802, acc.: 99.22%] [G loss: 3.265540]\n",
      "epoch:5 step:4151 [D loss: 0.615579, acc.: 72.66%] [G loss: 3.314225]\n",
      "epoch:5 step:4152 [D loss: 0.549854, acc.: 70.31%] [G loss: 3.850103]\n",
      "epoch:5 step:4153 [D loss: 0.170566, acc.: 92.97%] [G loss: 2.357885]\n",
      "epoch:5 step:4154 [D loss: 0.066723, acc.: 98.44%] [G loss: 1.530690]\n",
      "epoch:5 step:4155 [D loss: 0.676111, acc.: 64.84%] [G loss: 5.541117]\n",
      "epoch:5 step:4156 [D loss: 0.141974, acc.: 95.31%] [G loss: 5.852189]\n",
      "epoch:5 step:4157 [D loss: 0.311686, acc.: 84.38%] [G loss: 1.017929]\n",
      "epoch:5 step:4158 [D loss: 0.354027, acc.: 84.38%] [G loss: 4.468195]\n",
      "epoch:5 step:4159 [D loss: 0.311548, acc.: 84.38%] [G loss: 2.348452]\n",
      "epoch:5 step:4160 [D loss: 0.067358, acc.: 99.22%] [G loss: 1.583684]\n",
      "epoch:5 step:4161 [D loss: 0.225230, acc.: 90.62%] [G loss: 3.200246]\n",
      "epoch:5 step:4162 [D loss: 0.341701, acc.: 86.72%] [G loss: 2.892384]\n",
      "epoch:5 step:4163 [D loss: 0.066486, acc.: 100.00%] [G loss: 3.312795]\n",
      "epoch:5 step:4164 [D loss: 0.076584, acc.: 99.22%] [G loss: 2.144804]\n",
      "epoch:5 step:4165 [D loss: 0.398720, acc.: 80.47%] [G loss: 3.746255]\n",
      "epoch:5 step:4166 [D loss: 0.386486, acc.: 82.03%] [G loss: 3.043106]\n",
      "epoch:5 step:4167 [D loss: 0.072974, acc.: 98.44%] [G loss: 3.280625]\n",
      "epoch:5 step:4168 [D loss: 0.361187, acc.: 82.81%] [G loss: 3.498825]\n",
      "epoch:5 step:4169 [D loss: 0.106741, acc.: 98.44%] [G loss: 3.194933]\n",
      "epoch:5 step:4170 [D loss: 0.105592, acc.: 97.66%] [G loss: 0.788351]\n",
      "epoch:5 step:4171 [D loss: 0.195589, acc.: 92.97%] [G loss: 1.195343]\n",
      "epoch:5 step:4172 [D loss: 0.183221, acc.: 91.41%] [G loss: 0.523818]\n",
      "epoch:5 step:4173 [D loss: 0.011364, acc.: 100.00%] [G loss: 0.526145]\n",
      "epoch:5 step:4174 [D loss: 0.017170, acc.: 100.00%] [G loss: 0.098670]\n",
      "epoch:5 step:4175 [D loss: 0.277872, acc.: 87.50%] [G loss: 1.965419]\n",
      "epoch:5 step:4176 [D loss: 0.090774, acc.: 96.88%] [G loss: 2.895452]\n",
      "epoch:5 step:4177 [D loss: 0.074627, acc.: 99.22%] [G loss: 0.776136]\n",
      "epoch:5 step:4178 [D loss: 0.163323, acc.: 93.75%] [G loss: 0.475313]\n",
      "epoch:5 step:4179 [D loss: 0.133562, acc.: 96.88%] [G loss: 1.587409]\n",
      "epoch:5 step:4180 [D loss: 0.442453, acc.: 75.00%] [G loss: 0.075734]\n",
      "epoch:5 step:4181 [D loss: 0.443184, acc.: 74.22%] [G loss: 4.593745]\n",
      "epoch:5 step:4182 [D loss: 1.276378, acc.: 51.56%] [G loss: 1.127304]\n",
      "epoch:5 step:4183 [D loss: 0.301903, acc.: 85.94%] [G loss: 3.506221]\n",
      "epoch:5 step:4184 [D loss: 0.115216, acc.: 94.53%] [G loss: 3.795323]\n",
      "epoch:5 step:4185 [D loss: 0.125782, acc.: 98.44%] [G loss: 2.866151]\n",
      "epoch:5 step:4186 [D loss: 0.104935, acc.: 100.00%] [G loss: 1.766165]\n",
      "epoch:5 step:4187 [D loss: 0.518796, acc.: 71.88%] [G loss: 4.643682]\n",
      "epoch:5 step:4188 [D loss: 0.118392, acc.: 95.31%] [G loss: 5.431627]\n",
      "epoch:5 step:4189 [D loss: 0.265269, acc.: 89.84%] [G loss: 3.308205]\n",
      "epoch:5 step:4190 [D loss: 0.175633, acc.: 93.75%] [G loss: 3.639358]\n",
      "epoch:5 step:4191 [D loss: 0.163178, acc.: 92.97%] [G loss: 2.402319]\n",
      "epoch:5 step:4192 [D loss: 0.152842, acc.: 96.88%] [G loss: 3.274306]\n",
      "epoch:5 step:4193 [D loss: 0.138930, acc.: 96.88%] [G loss: 1.838592]\n",
      "epoch:5 step:4194 [D loss: 0.195648, acc.: 93.75%] [G loss: 0.926671]\n",
      "epoch:5 step:4195 [D loss: 0.027909, acc.: 100.00%] [G loss: 1.459866]\n",
      "epoch:5 step:4196 [D loss: 0.483588, acc.: 77.34%] [G loss: 0.275861]\n",
      "epoch:5 step:4197 [D loss: 0.228485, acc.: 91.41%] [G loss: 2.407124]\n",
      "epoch:5 step:4198 [D loss: 0.086732, acc.: 96.09%] [G loss: 3.160799]\n",
      "epoch:5 step:4199 [D loss: 0.114836, acc.: 97.66%] [G loss: 0.504360]\n",
      "epoch:5 step:4200 [D loss: 0.245965, acc.: 91.41%] [G loss: 2.635582]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##############\n",
      "[1.00573876 1.06787884 0.92767588 1.00333208 0.68024949 0.95797408\n",
      " 1.11700658 2.11734581 1.00744276 0.95085076]\n",
      "##########\n",
      "epoch:5 step:4201 [D loss: 0.219764, acc.: 87.50%] [G loss: 1.110136]\n",
      "epoch:5 step:4202 [D loss: 0.429878, acc.: 78.91%] [G loss: 3.940764]\n",
      "epoch:5 step:4203 [D loss: 0.140584, acc.: 96.88%] [G loss: 4.357622]\n",
      "epoch:5 step:4204 [D loss: 0.263747, acc.: 87.50%] [G loss: 1.657725]\n",
      "epoch:5 step:4205 [D loss: 0.427619, acc.: 77.34%] [G loss: 5.165403]\n",
      "epoch:5 step:4206 [D loss: 0.124916, acc.: 96.09%] [G loss: 5.137410]\n",
      "epoch:5 step:4207 [D loss: 0.935750, acc.: 57.81%] [G loss: 2.499835]\n",
      "epoch:5 step:4208 [D loss: 0.066732, acc.: 99.22%] [G loss: 3.946384]\n",
      "epoch:5 step:4209 [D loss: 0.045757, acc.: 100.00%] [G loss: 3.536907]\n",
      "epoch:5 step:4210 [D loss: 0.139564, acc.: 96.09%] [G loss: 3.825881]\n",
      "epoch:5 step:4211 [D loss: 0.171454, acc.: 95.31%] [G loss: 3.918296]\n",
      "epoch:5 step:4212 [D loss: 0.184323, acc.: 96.09%] [G loss: 3.777965]\n",
      "epoch:5 step:4213 [D loss: 0.275971, acc.: 87.50%] [G loss: 1.538024]\n",
      "epoch:5 step:4214 [D loss: 0.136099, acc.: 95.31%] [G loss: 2.420748]\n",
      "epoch:5 step:4215 [D loss: 0.034219, acc.: 100.00%] [G loss: 1.863300]\n",
      "epoch:5 step:4216 [D loss: 0.168736, acc.: 93.75%] [G loss: 1.944848]\n",
      "epoch:5 step:4217 [D loss: 0.093476, acc.: 98.44%] [G loss: 2.278120]\n",
      "epoch:5 step:4218 [D loss: 0.137661, acc.: 96.09%] [G loss: 0.454745]\n",
      "epoch:5 step:4219 [D loss: 0.095811, acc.: 100.00%] [G loss: 0.515844]\n",
      "epoch:5 step:4220 [D loss: 1.382251, acc.: 25.00%] [G loss: 2.280972]\n",
      "epoch:5 step:4221 [D loss: 0.034625, acc.: 99.22%] [G loss: 5.221317]\n",
      "epoch:5 step:4222 [D loss: 0.173846, acc.: 92.97%] [G loss: 1.618209]\n",
      "epoch:5 step:4223 [D loss: 0.137035, acc.: 95.31%] [G loss: 1.842633]\n",
      "epoch:5 step:4224 [D loss: 0.009152, acc.: 100.00%] [G loss: 1.186269]\n",
      "epoch:5 step:4225 [D loss: 0.673297, acc.: 73.44%] [G loss: 4.677373]\n",
      "epoch:5 step:4226 [D loss: 0.830951, acc.: 61.72%] [G loss: 1.387750]\n",
      "epoch:5 step:4227 [D loss: 0.204191, acc.: 92.19%] [G loss: 2.031928]\n",
      "epoch:5 step:4228 [D loss: 0.037235, acc.: 99.22%] [G loss: 2.395446]\n",
      "epoch:5 step:4229 [D loss: 0.184277, acc.: 95.31%] [G loss: 2.524018]\n",
      "epoch:5 step:4230 [D loss: 0.147312, acc.: 97.66%] [G loss: 2.342215]\n",
      "epoch:5 step:4231 [D loss: 0.102803, acc.: 97.66%] [G loss: 0.722736]\n",
      "epoch:5 step:4232 [D loss: 0.544758, acc.: 68.75%] [G loss: 4.708169]\n",
      "epoch:5 step:4233 [D loss: 0.436055, acc.: 75.78%] [G loss: 3.688096]\n",
      "epoch:5 step:4234 [D loss: 0.386741, acc.: 78.12%] [G loss: 3.329736]\n",
      "epoch:5 step:4235 [D loss: 0.071872, acc.: 99.22%] [G loss: 3.548954]\n",
      "epoch:5 step:4236 [D loss: 0.219394, acc.: 94.53%] [G loss: 4.197993]\n",
      "epoch:5 step:4237 [D loss: 0.086430, acc.: 96.88%] [G loss: 3.757475]\n",
      "epoch:5 step:4238 [D loss: 0.055904, acc.: 99.22%] [G loss: 1.986955]\n",
      "epoch:5 step:4239 [D loss: 0.125870, acc.: 97.66%] [G loss: 3.047280]\n",
      "epoch:5 step:4240 [D loss: 0.109237, acc.: 97.66%] [G loss: 2.632513]\n",
      "epoch:5 step:4241 [D loss: 0.206299, acc.: 94.53%] [G loss: 2.636055]\n",
      "epoch:5 step:4242 [D loss: 0.112762, acc.: 97.66%] [G loss: 3.139962]\n",
      "epoch:5 step:4243 [D loss: 0.825009, acc.: 56.25%] [G loss: 6.434337]\n",
      "epoch:5 step:4244 [D loss: 0.936541, acc.: 60.94%] [G loss: 1.899408]\n",
      "epoch:5 step:4245 [D loss: 0.331154, acc.: 86.72%] [G loss: 4.107010]\n",
      "epoch:5 step:4246 [D loss: 0.048990, acc.: 100.00%] [G loss: 4.548515]\n",
      "epoch:5 step:4247 [D loss: 0.133212, acc.: 96.88%] [G loss: 3.504910]\n",
      "epoch:5 step:4248 [D loss: 0.136210, acc.: 98.44%] [G loss: 2.974203]\n",
      "epoch:5 step:4249 [D loss: 0.217826, acc.: 92.19%] [G loss: 4.869456]\n",
      "epoch:5 step:4250 [D loss: 0.786044, acc.: 57.81%] [G loss: 4.447627]\n",
      "epoch:5 step:4251 [D loss: 0.066166, acc.: 99.22%] [G loss: 4.372874]\n",
      "epoch:5 step:4252 [D loss: 0.099526, acc.: 96.09%] [G loss: 3.906531]\n",
      "epoch:5 step:4253 [D loss: 0.102911, acc.: 97.66%] [G loss: 2.666065]\n",
      "epoch:5 step:4254 [D loss: 0.149686, acc.: 97.66%] [G loss: 4.047585]\n",
      "epoch:5 step:4255 [D loss: 0.166175, acc.: 96.09%] [G loss: 3.796170]\n",
      "epoch:5 step:4256 [D loss: 0.155857, acc.: 96.88%] [G loss: 3.345043]\n",
      "epoch:5 step:4257 [D loss: 0.102274, acc.: 99.22%] [G loss: 3.942300]\n",
      "epoch:5 step:4258 [D loss: 0.533902, acc.: 75.00%] [G loss: 5.354357]\n",
      "epoch:5 step:4259 [D loss: 0.372946, acc.: 76.56%] [G loss: 2.601566]\n",
      "epoch:5 step:4260 [D loss: 0.402577, acc.: 82.03%] [G loss: 5.480486]\n",
      "epoch:5 step:4261 [D loss: 0.134121, acc.: 95.31%] [G loss: 5.365262]\n",
      "epoch:5 step:4262 [D loss: 0.220465, acc.: 91.41%] [G loss: 1.860820]\n",
      "epoch:5 step:4263 [D loss: 0.522792, acc.: 75.00%] [G loss: 6.079737]\n",
      "epoch:5 step:4264 [D loss: 0.486019, acc.: 71.88%] [G loss: 3.549932]\n",
      "epoch:5 step:4265 [D loss: 0.195504, acc.: 95.31%] [G loss: 4.058391]\n",
      "epoch:5 step:4266 [D loss: 0.365290, acc.: 82.81%] [G loss: 2.583874]\n",
      "epoch:5 step:4267 [D loss: 0.084996, acc.: 97.66%] [G loss: 3.472364]\n",
      "epoch:5 step:4268 [D loss: 0.176350, acc.: 95.31%] [G loss: 4.311390]\n",
      "epoch:5 step:4269 [D loss: 0.560900, acc.: 71.88%] [G loss: 3.884117]\n",
      "epoch:5 step:4270 [D loss: 0.132194, acc.: 99.22%] [G loss: 4.477728]\n",
      "epoch:5 step:4271 [D loss: 0.288049, acc.: 88.28%] [G loss: 3.150410]\n",
      "epoch:5 step:4272 [D loss: 0.209365, acc.: 94.53%] [G loss: 4.085844]\n",
      "epoch:5 step:4273 [D loss: 0.220157, acc.: 91.41%] [G loss: 3.087158]\n",
      "epoch:5 step:4274 [D loss: 0.129652, acc.: 96.88%] [G loss: 3.575780]\n",
      "epoch:5 step:4275 [D loss: 0.184924, acc.: 91.41%] [G loss: 3.169919]\n",
      "epoch:5 step:4276 [D loss: 0.113176, acc.: 96.88%] [G loss: 3.156028]\n",
      "epoch:5 step:4277 [D loss: 0.113349, acc.: 96.88%] [G loss: 1.885612]\n",
      "epoch:5 step:4278 [D loss: 0.271669, acc.: 92.97%] [G loss: 1.190211]\n",
      "epoch:5 step:4279 [D loss: 0.284594, acc.: 90.62%] [G loss: 4.820516]\n",
      "epoch:5 step:4280 [D loss: 0.426317, acc.: 76.56%] [G loss: 2.071398]\n",
      "epoch:5 step:4281 [D loss: 0.134795, acc.: 95.31%] [G loss: 2.291615]\n",
      "epoch:5 step:4282 [D loss: 0.030602, acc.: 100.00%] [G loss: 2.197805]\n",
      "epoch:5 step:4283 [D loss: 0.308404, acc.: 86.72%] [G loss: 2.561410]\n",
      "epoch:5 step:4284 [D loss: 0.797644, acc.: 62.50%] [G loss: 7.230001]\n",
      "epoch:5 step:4285 [D loss: 0.494459, acc.: 74.22%] [G loss: 5.585609]\n",
      "epoch:5 step:4286 [D loss: 0.085526, acc.: 96.88%] [G loss: 3.816840]\n",
      "epoch:5 step:4287 [D loss: 0.249804, acc.: 88.28%] [G loss: 5.030923]\n",
      "epoch:5 step:4288 [D loss: 0.089154, acc.: 98.44%] [G loss: 4.635899]\n",
      "epoch:5 step:4289 [D loss: 0.168415, acc.: 96.09%] [G loss: 3.493057]\n",
      "epoch:5 step:4290 [D loss: 0.554809, acc.: 74.22%] [G loss: 5.656904]\n",
      "epoch:5 step:4291 [D loss: 0.453533, acc.: 75.00%] [G loss: 4.475892]\n",
      "epoch:5 step:4292 [D loss: 0.183578, acc.: 95.31%] [G loss: 4.508528]\n",
      "epoch:5 step:4293 [D loss: 0.120081, acc.: 94.53%] [G loss: 3.509860]\n",
      "epoch:5 step:4294 [D loss: 0.128405, acc.: 96.09%] [G loss: 3.592356]\n",
      "epoch:5 step:4295 [D loss: 0.116589, acc.: 98.44%] [G loss: 2.463096]\n",
      "epoch:5 step:4296 [D loss: 0.202914, acc.: 93.75%] [G loss: 3.728267]\n",
      "epoch:5 step:4297 [D loss: 0.061480, acc.: 99.22%] [G loss: 3.645723]\n",
      "epoch:5 step:4298 [D loss: 0.217847, acc.: 92.97%] [G loss: 2.536795]\n",
      "epoch:5 step:4299 [D loss: 0.232370, acc.: 90.62%] [G loss: 4.103373]\n",
      "epoch:5 step:4300 [D loss: 0.511636, acc.: 73.44%] [G loss: 1.085510]\n",
      "epoch:5 step:4301 [D loss: 0.247547, acc.: 88.28%] [G loss: 2.469504]\n",
      "epoch:5 step:4302 [D loss: 0.099677, acc.: 96.09%] [G loss: 3.421950]\n",
      "epoch:5 step:4303 [D loss: 0.129135, acc.: 96.88%] [G loss: 1.646118]\n",
      "epoch:5 step:4304 [D loss: 0.150151, acc.: 94.53%] [G loss: 2.180312]\n",
      "epoch:5 step:4305 [D loss: 0.037584, acc.: 99.22%] [G loss: 2.224078]\n",
      "epoch:5 step:4306 [D loss: 0.293608, acc.: 89.84%] [G loss: 1.274814]\n",
      "epoch:5 step:4307 [D loss: 0.346604, acc.: 86.72%] [G loss: 3.692631]\n",
      "epoch:5 step:4308 [D loss: 0.260010, acc.: 89.84%] [G loss: 2.757086]\n",
      "epoch:5 step:4309 [D loss: 0.066154, acc.: 100.00%] [G loss: 2.293636]\n",
      "epoch:5 step:4310 [D loss: 0.193484, acc.: 92.97%] [G loss: 1.872755]\n",
      "epoch:5 step:4311 [D loss: 0.801606, acc.: 64.06%] [G loss: 7.101301]\n",
      "epoch:5 step:4312 [D loss: 0.895907, acc.: 61.72%] [G loss: 5.116815]\n",
      "epoch:5 step:4313 [D loss: 0.162100, acc.: 96.09%] [G loss: 3.264656]\n",
      "epoch:5 step:4314 [D loss: 0.212597, acc.: 93.75%] [G loss: 5.183838]\n",
      "epoch:5 step:4315 [D loss: 0.070872, acc.: 98.44%] [G loss: 5.180778]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:5 step:4316 [D loss: 1.629575, acc.: 21.09%] [G loss: 6.420593]\n",
      "epoch:5 step:4317 [D loss: 0.085357, acc.: 96.09%] [G loss: 6.894543]\n",
      "epoch:5 step:4318 [D loss: 0.410589, acc.: 78.12%] [G loss: 3.540723]\n",
      "epoch:5 step:4319 [D loss: 0.301542, acc.: 87.50%] [G loss: 4.241529]\n",
      "epoch:5 step:4320 [D loss: 0.039769, acc.: 100.00%] [G loss: 5.263953]\n",
      "epoch:5 step:4321 [D loss: 0.421801, acc.: 78.12%] [G loss: 3.989518]\n",
      "epoch:5 step:4322 [D loss: 0.124601, acc.: 96.09%] [G loss: 1.987896]\n",
      "epoch:5 step:4323 [D loss: 0.098430, acc.: 98.44%] [G loss: 2.167041]\n",
      "epoch:5 step:4324 [D loss: 0.062945, acc.: 99.22%] [G loss: 1.344415]\n",
      "epoch:5 step:4325 [D loss: 0.471088, acc.: 75.78%] [G loss: 3.785836]\n",
      "epoch:5 step:4326 [D loss: 0.253950, acc.: 86.72%] [G loss: 2.830903]\n",
      "epoch:5 step:4327 [D loss: 0.229720, acc.: 91.41%] [G loss: 2.622042]\n",
      "epoch:5 step:4328 [D loss: 0.020096, acc.: 100.00%] [G loss: 2.236857]\n",
      "epoch:5 step:4329 [D loss: 0.205899, acc.: 93.75%] [G loss: 0.726879]\n",
      "epoch:5 step:4330 [D loss: 0.113109, acc.: 96.09%] [G loss: 2.269141]\n",
      "epoch:5 step:4331 [D loss: 0.166133, acc.: 95.31%] [G loss: 2.955601]\n",
      "epoch:5 step:4332 [D loss: 0.116476, acc.: 98.44%] [G loss: 1.497800]\n",
      "epoch:5 step:4333 [D loss: 0.262813, acc.: 88.28%] [G loss: 3.295333]\n",
      "epoch:5 step:4334 [D loss: 0.359812, acc.: 84.38%] [G loss: 2.712451]\n",
      "epoch:5 step:4335 [D loss: 0.081645, acc.: 98.44%] [G loss: 1.293777]\n",
      "epoch:5 step:4336 [D loss: 0.670985, acc.: 67.19%] [G loss: 6.181808]\n",
      "epoch:5 step:4337 [D loss: 0.853810, acc.: 60.16%] [G loss: 1.242641]\n",
      "epoch:5 step:4338 [D loss: 0.679306, acc.: 65.62%] [G loss: 4.531513]\n",
      "epoch:5 step:4339 [D loss: 0.814152, acc.: 59.38%] [G loss: 3.298946]\n",
      "epoch:5 step:4340 [D loss: 0.097021, acc.: 97.66%] [G loss: 3.416164]\n",
      "epoch:5 step:4341 [D loss: 0.135249, acc.: 97.66%] [G loss: 3.921454]\n",
      "epoch:5 step:4342 [D loss: 0.191829, acc.: 93.75%] [G loss: 2.838952]\n",
      "epoch:5 step:4343 [D loss: 0.142513, acc.: 97.66%] [G loss: 1.891240]\n",
      "epoch:5 step:4344 [D loss: 0.143631, acc.: 96.09%] [G loss: 2.764395]\n",
      "epoch:5 step:4345 [D loss: 0.068003, acc.: 99.22%] [G loss: 2.350447]\n",
      "epoch:5 step:4346 [D loss: 0.152547, acc.: 96.09%] [G loss: 0.811557]\n",
      "epoch:5 step:4347 [D loss: 0.650121, acc.: 64.06%] [G loss: 4.957175]\n",
      "epoch:5 step:4348 [D loss: 0.344846, acc.: 82.03%] [G loss: 4.997690]\n",
      "epoch:5 step:4349 [D loss: 0.305169, acc.: 85.16%] [G loss: 0.874874]\n",
      "epoch:5 step:4350 [D loss: 0.072665, acc.: 100.00%] [G loss: 0.365644]\n",
      "epoch:5 step:4351 [D loss: 0.030133, acc.: 100.00%] [G loss: 0.318917]\n",
      "epoch:5 step:4352 [D loss: 0.028760, acc.: 100.00%] [G loss: 0.475940]\n",
      "epoch:5 step:4353 [D loss: 0.022400, acc.: 100.00%] [G loss: 0.351746]\n",
      "epoch:5 step:4354 [D loss: 0.158352, acc.: 96.88%] [G loss: 1.680127]\n",
      "epoch:5 step:4355 [D loss: 0.053160, acc.: 99.22%] [G loss: 1.770609]\n",
      "epoch:5 step:4356 [D loss: 0.063789, acc.: 98.44%] [G loss: 0.664939]\n",
      "epoch:5 step:4357 [D loss: 0.082251, acc.: 98.44%] [G loss: 0.441532]\n",
      "epoch:5 step:4358 [D loss: 0.260645, acc.: 88.28%] [G loss: 2.987649]\n",
      "epoch:5 step:4359 [D loss: 0.422121, acc.: 76.56%] [G loss: 0.737859]\n",
      "epoch:5 step:4360 [D loss: 0.086405, acc.: 99.22%] [G loss: 0.569177]\n",
      "epoch:5 step:4361 [D loss: 0.096692, acc.: 97.66%] [G loss: 0.765884]\n",
      "epoch:5 step:4362 [D loss: 0.041723, acc.: 99.22%] [G loss: 0.826860]\n",
      "epoch:5 step:4363 [D loss: 0.057667, acc.: 99.22%] [G loss: 0.871641]\n",
      "epoch:5 step:4364 [D loss: 0.108023, acc.: 99.22%] [G loss: 1.349731]\n",
      "epoch:5 step:4365 [D loss: 0.563658, acc.: 67.19%] [G loss: 5.123807]\n",
      "epoch:5 step:4366 [D loss: 0.140325, acc.: 95.31%] [G loss: 5.328534]\n",
      "epoch:5 step:4367 [D loss: 0.376946, acc.: 80.47%] [G loss: 1.882386]\n",
      "epoch:5 step:4368 [D loss: 0.104447, acc.: 97.66%] [G loss: 2.065666]\n",
      "epoch:5 step:4369 [D loss: 0.229194, acc.: 92.19%] [G loss: 4.133162]\n",
      "epoch:5 step:4370 [D loss: 0.216428, acc.: 92.19%] [G loss: 4.203619]\n",
      "epoch:5 step:4371 [D loss: 0.270237, acc.: 85.94%] [G loss: 3.730092]\n",
      "epoch:5 step:4372 [D loss: 0.218023, acc.: 94.53%] [G loss: 4.312969]\n",
      "epoch:5 step:4373 [D loss: 0.061515, acc.: 98.44%] [G loss: 4.613607]\n",
      "epoch:5 step:4374 [D loss: 0.077146, acc.: 97.66%] [G loss: 3.539199]\n",
      "epoch:5 step:4375 [D loss: 0.290121, acc.: 90.62%] [G loss: 4.563514]\n",
      "epoch:5 step:4376 [D loss: 0.194364, acc.: 93.75%] [G loss: 3.307083]\n",
      "epoch:5 step:4377 [D loss: 0.268823, acc.: 90.62%] [G loss: 3.606166]\n",
      "epoch:5 step:4378 [D loss: 0.105466, acc.: 95.31%] [G loss: 3.334018]\n",
      "epoch:5 step:4379 [D loss: 0.203363, acc.: 92.19%] [G loss: 1.801282]\n",
      "epoch:5 step:4380 [D loss: 0.187472, acc.: 95.31%] [G loss: 2.901047]\n",
      "epoch:5 step:4381 [D loss: 0.188864, acc.: 95.31%] [G loss: 5.018546]\n",
      "epoch:5 step:4382 [D loss: 0.877821, acc.: 52.34%] [G loss: 4.695461]\n",
      "epoch:5 step:4383 [D loss: 0.199275, acc.: 92.97%] [G loss: 4.982826]\n",
      "epoch:5 step:4384 [D loss: 0.118045, acc.: 96.88%] [G loss: 3.259846]\n",
      "epoch:5 step:4385 [D loss: 0.097675, acc.: 97.66%] [G loss: 3.772800]\n",
      "epoch:5 step:4386 [D loss: 0.358218, acc.: 84.38%] [G loss: 5.702074]\n",
      "epoch:5 step:4387 [D loss: 0.512918, acc.: 74.22%] [G loss: 2.285521]\n",
      "epoch:5 step:4388 [D loss: 0.462110, acc.: 75.78%] [G loss: 6.971123]\n",
      "epoch:5 step:4389 [D loss: 0.837455, acc.: 63.28%] [G loss: 2.127654]\n",
      "epoch:5 step:4390 [D loss: 0.283571, acc.: 87.50%] [G loss: 4.112066]\n",
      "epoch:5 step:4391 [D loss: 0.058644, acc.: 99.22%] [G loss: 4.860683]\n",
      "epoch:5 step:4392 [D loss: 0.166256, acc.: 96.09%] [G loss: 3.214359]\n",
      "epoch:5 step:4393 [D loss: 0.320852, acc.: 85.94%] [G loss: 3.986474]\n",
      "epoch:5 step:4394 [D loss: 0.071027, acc.: 99.22%] [G loss: 4.420867]\n",
      "epoch:5 step:4395 [D loss: 0.154460, acc.: 96.88%] [G loss: 3.979342]\n",
      "epoch:5 step:4396 [D loss: 1.007314, acc.: 44.53%] [G loss: 4.757001]\n",
      "epoch:5 step:4397 [D loss: 0.532607, acc.: 75.00%] [G loss: 3.229278]\n",
      "epoch:5 step:4398 [D loss: 0.063558, acc.: 98.44%] [G loss: 2.746766]\n",
      "epoch:5 step:4399 [D loss: 0.425192, acc.: 76.56%] [G loss: 5.411188]\n",
      "epoch:5 step:4400 [D loss: 0.513837, acc.: 75.00%] [G loss: 3.626638]\n",
      "##############\n",
      "[0.92381184 1.00927044 0.89211802 0.93621543 1.10942716 2.11298626\n",
      " 1.06570356 2.10000842 0.94773285 1.12011222]\n",
      "##########\n",
      "epoch:5 step:4401 [D loss: 0.233965, acc.: 88.28%] [G loss: 4.601620]\n",
      "epoch:5 step:4402 [D loss: 0.120805, acc.: 98.44%] [G loss: 4.816399]\n",
      "epoch:5 step:4403 [D loss: 0.200799, acc.: 95.31%] [G loss: 2.905442]\n",
      "epoch:5 step:4404 [D loss: 0.345531, acc.: 86.72%] [G loss: 4.707991]\n",
      "epoch:5 step:4405 [D loss: 0.137716, acc.: 96.09%] [G loss: 4.694424]\n",
      "epoch:5 step:4406 [D loss: 0.279879, acc.: 90.62%] [G loss: 4.293001]\n",
      "epoch:5 step:4407 [D loss: 0.119800, acc.: 96.09%] [G loss: 3.835939]\n",
      "epoch:5 step:4408 [D loss: 0.102522, acc.: 97.66%] [G loss: 3.708614]\n",
      "epoch:5 step:4409 [D loss: 0.191869, acc.: 92.97%] [G loss: 4.407694]\n",
      "epoch:5 step:4410 [D loss: 0.353158, acc.: 85.94%] [G loss: 3.519831]\n",
      "epoch:5 step:4411 [D loss: 0.169625, acc.: 94.53%] [G loss: 4.431779]\n",
      "epoch:5 step:4412 [D loss: 0.279610, acc.: 90.62%] [G loss: 2.999556]\n",
      "epoch:5 step:4413 [D loss: 0.336421, acc.: 87.50%] [G loss: 4.852951]\n",
      "epoch:5 step:4414 [D loss: 0.437370, acc.: 75.78%] [G loss: 3.515861]\n",
      "epoch:5 step:4415 [D loss: 0.088310, acc.: 97.66%] [G loss: 4.010185]\n",
      "epoch:5 step:4416 [D loss: 0.086991, acc.: 98.44%] [G loss: 4.481718]\n",
      "epoch:5 step:4417 [D loss: 0.142053, acc.: 95.31%] [G loss: 4.774581]\n",
      "epoch:5 step:4418 [D loss: 0.519496, acc.: 74.22%] [G loss: 4.660853]\n",
      "epoch:5 step:4419 [D loss: 0.086938, acc.: 98.44%] [G loss: 4.231272]\n",
      "epoch:5 step:4420 [D loss: 0.052758, acc.: 100.00%] [G loss: 3.879145]\n",
      "epoch:5 step:4421 [D loss: 0.158490, acc.: 95.31%] [G loss: 4.698004]\n",
      "epoch:5 step:4422 [D loss: 0.211170, acc.: 92.19%] [G loss: 4.601418]\n",
      "epoch:5 step:4423 [D loss: 0.480092, acc.: 76.56%] [G loss: 5.026200]\n",
      "epoch:5 step:4424 [D loss: 0.406881, acc.: 83.59%] [G loss: 4.489869]\n",
      "epoch:5 step:4425 [D loss: 0.040918, acc.: 100.00%] [G loss: 4.943224]\n",
      "epoch:5 step:4426 [D loss: 0.033912, acc.: 100.00%] [G loss: 4.276028]\n",
      "epoch:5 step:4427 [D loss: 0.406249, acc.: 79.69%] [G loss: 6.021860]\n",
      "epoch:5 step:4428 [D loss: 0.309699, acc.: 85.94%] [G loss: 3.999125]\n",
      "epoch:5 step:4429 [D loss: 0.372510, acc.: 83.59%] [G loss: 4.484861]\n",
      "epoch:5 step:4430 [D loss: 0.013244, acc.: 100.00%] [G loss: 5.224855]\n",
      "epoch:5 step:4431 [D loss: 2.000824, acc.: 18.75%] [G loss: 6.908930]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:5 step:4432 [D loss: 0.534926, acc.: 73.44%] [G loss: 3.756591]\n",
      "epoch:5 step:4433 [D loss: 0.240434, acc.: 89.84%] [G loss: 4.762993]\n",
      "epoch:5 step:4434 [D loss: 0.121006, acc.: 96.88%] [G loss: 3.686950]\n",
      "epoch:5 step:4435 [D loss: 0.144942, acc.: 95.31%] [G loss: 3.941988]\n",
      "epoch:5 step:4436 [D loss: 0.196389, acc.: 95.31%] [G loss: 4.603117]\n",
      "epoch:5 step:4437 [D loss: 0.559308, acc.: 75.00%] [G loss: 3.886972]\n",
      "epoch:5 step:4438 [D loss: 0.247072, acc.: 91.41%] [G loss: 3.120603]\n",
      "epoch:5 step:4439 [D loss: 0.214331, acc.: 94.53%] [G loss: 4.492914]\n",
      "epoch:5 step:4440 [D loss: 0.273820, acc.: 89.06%] [G loss: 3.015880]\n",
      "epoch:5 step:4441 [D loss: 0.220177, acc.: 90.62%] [G loss: 2.644095]\n",
      "epoch:5 step:4442 [D loss: 0.350283, acc.: 85.16%] [G loss: 4.313419]\n",
      "epoch:5 step:4443 [D loss: 0.255199, acc.: 89.06%] [G loss: 3.024691]\n",
      "epoch:5 step:4444 [D loss: 0.298707, acc.: 87.50%] [G loss: 3.128884]\n",
      "epoch:5 step:4445 [D loss: 0.068923, acc.: 98.44%] [G loss: 3.430708]\n",
      "epoch:5 step:4446 [D loss: 0.160045, acc.: 95.31%] [G loss: 4.136637]\n",
      "epoch:5 step:4447 [D loss: 0.459471, acc.: 78.12%] [G loss: 2.899775]\n",
      "epoch:5 step:4448 [D loss: 0.199382, acc.: 94.53%] [G loss: 1.417433]\n",
      "epoch:5 step:4449 [D loss: 0.397528, acc.: 80.47%] [G loss: 6.904543]\n",
      "epoch:5 step:4450 [D loss: 0.600404, acc.: 67.97%] [G loss: 3.826732]\n",
      "epoch:5 step:4451 [D loss: 0.131264, acc.: 96.09%] [G loss: 0.965235]\n",
      "epoch:5 step:4452 [D loss: 0.125385, acc.: 96.09%] [G loss: 1.800449]\n",
      "epoch:5 step:4453 [D loss: 0.135145, acc.: 96.88%] [G loss: 2.189793]\n",
      "epoch:5 step:4454 [D loss: 0.189041, acc.: 94.53%] [G loss: 1.206002]\n",
      "epoch:5 step:4455 [D loss: 0.093304, acc.: 96.88%] [G loss: 1.875077]\n",
      "epoch:5 step:4456 [D loss: 0.413502, acc.: 82.03%] [G loss: 5.169216]\n",
      "epoch:5 step:4457 [D loss: 1.848541, acc.: 48.44%] [G loss: 1.250416]\n",
      "epoch:5 step:4458 [D loss: 0.710197, acc.: 67.19%] [G loss: 5.520185]\n",
      "epoch:5 step:4459 [D loss: 0.435904, acc.: 76.56%] [G loss: 4.329565]\n",
      "epoch:5 step:4460 [D loss: 0.074028, acc.: 99.22%] [G loss: 2.631926]\n",
      "epoch:5 step:4461 [D loss: 0.328967, acc.: 88.28%] [G loss: 3.879110]\n",
      "epoch:5 step:4462 [D loss: 0.225444, acc.: 92.19%] [G loss: 2.793758]\n",
      "epoch:5 step:4463 [D loss: 0.074199, acc.: 99.22%] [G loss: 1.410776]\n",
      "epoch:5 step:4464 [D loss: 0.041820, acc.: 100.00%] [G loss: 1.086379]\n",
      "epoch:5 step:4465 [D loss: 0.034413, acc.: 100.00%] [G loss: 0.295372]\n",
      "epoch:5 step:4466 [D loss: 0.057268, acc.: 100.00%] [G loss: 0.594682]\n",
      "epoch:5 step:4467 [D loss: 0.049450, acc.: 99.22%] [G loss: 0.176130]\n",
      "epoch:5 step:4468 [D loss: 0.072282, acc.: 98.44%] [G loss: 0.055473]\n",
      "epoch:5 step:4469 [D loss: 0.053855, acc.: 99.22%] [G loss: 0.218072]\n",
      "epoch:5 step:4470 [D loss: 0.221289, acc.: 92.97%] [G loss: 1.418895]\n",
      "epoch:5 step:4471 [D loss: 0.249532, acc.: 88.28%] [G loss: 1.262942]\n",
      "epoch:5 step:4472 [D loss: 0.055411, acc.: 99.22%] [G loss: 0.274172]\n",
      "epoch:5 step:4473 [D loss: 0.032237, acc.: 99.22%] [G loss: 0.195837]\n",
      "epoch:5 step:4474 [D loss: 0.095146, acc.: 98.44%] [G loss: 1.180887]\n",
      "epoch:5 step:4475 [D loss: 0.102942, acc.: 99.22%] [G loss: 3.069532]\n",
      "epoch:5 step:4476 [D loss: 0.798361, acc.: 57.81%] [G loss: 4.241838]\n",
      "epoch:5 step:4477 [D loss: 0.106374, acc.: 98.44%] [G loss: 4.990216]\n",
      "epoch:5 step:4478 [D loss: 0.124435, acc.: 95.31%] [G loss: 2.993522]\n",
      "epoch:5 step:4479 [D loss: 0.270140, acc.: 85.16%] [G loss: 4.282434]\n",
      "epoch:5 step:4480 [D loss: 0.206804, acc.: 92.19%] [G loss: 3.547810]\n",
      "epoch:5 step:4481 [D loss: 0.093522, acc.: 97.66%] [G loss: 2.140974]\n",
      "epoch:5 step:4482 [D loss: 0.186083, acc.: 93.75%] [G loss: 2.297128]\n",
      "epoch:5 step:4483 [D loss: 0.089554, acc.: 99.22%] [G loss: 1.276860]\n",
      "epoch:5 step:4484 [D loss: 0.103089, acc.: 99.22%] [G loss: 0.560801]\n",
      "epoch:5 step:4485 [D loss: 0.331293, acc.: 87.50%] [G loss: 3.222284]\n",
      "epoch:5 step:4486 [D loss: 0.194713, acc.: 92.97%] [G loss: 3.105099]\n",
      "epoch:5 step:4487 [D loss: 0.117933, acc.: 97.66%] [G loss: 0.992401]\n",
      "epoch:5 step:4488 [D loss: 0.236977, acc.: 89.06%] [G loss: 4.016436]\n",
      "epoch:5 step:4489 [D loss: 0.168940, acc.: 92.97%] [G loss: 3.115589]\n",
      "epoch:5 step:4490 [D loss: 0.107775, acc.: 98.44%] [G loss: 3.447236]\n",
      "epoch:5 step:4491 [D loss: 0.065864, acc.: 98.44%] [G loss: 3.653797]\n",
      "epoch:5 step:4492 [D loss: 0.123160, acc.: 97.66%] [G loss: 1.672819]\n",
      "epoch:5 step:4493 [D loss: 1.501807, acc.: 45.31%] [G loss: 7.167310]\n",
      "epoch:5 step:4494 [D loss: 1.769844, acc.: 50.00%] [G loss: 3.093341]\n",
      "epoch:5 step:4495 [D loss: 0.331515, acc.: 84.38%] [G loss: 3.017286]\n",
      "epoch:5 step:4496 [D loss: 0.137533, acc.: 97.66%] [G loss: 3.679790]\n",
      "epoch:5 step:4497 [D loss: 0.233539, acc.: 92.19%] [G loss: 2.311071]\n",
      "epoch:5 step:4498 [D loss: 0.331792, acc.: 83.59%] [G loss: 2.085285]\n",
      "epoch:5 step:4499 [D loss: 0.098812, acc.: 99.22%] [G loss: 2.938639]\n",
      "epoch:5 step:4500 [D loss: 0.090770, acc.: 96.88%] [G loss: 1.460214]\n",
      "epoch:5 step:4501 [D loss: 0.576327, acc.: 70.31%] [G loss: 4.329991]\n",
      "epoch:5 step:4502 [D loss: 0.257497, acc.: 89.06%] [G loss: 3.683910]\n",
      "epoch:5 step:4503 [D loss: 0.468083, acc.: 78.12%] [G loss: 1.293333]\n",
      "epoch:5 step:4504 [D loss: 0.186296, acc.: 94.53%] [G loss: 3.109558]\n",
      "epoch:5 step:4505 [D loss: 0.090055, acc.: 97.66%] [G loss: 2.533259]\n",
      "epoch:5 step:4506 [D loss: 0.644047, acc.: 67.19%] [G loss: 3.888865]\n",
      "epoch:5 step:4507 [D loss: 0.164242, acc.: 95.31%] [G loss: 3.149121]\n",
      "epoch:5 step:4508 [D loss: 0.107553, acc.: 98.44%] [G loss: 1.786760]\n",
      "epoch:5 step:4509 [D loss: 0.314307, acc.: 86.72%] [G loss: 3.747960]\n",
      "epoch:5 step:4510 [D loss: 0.208823, acc.: 90.62%] [G loss: 2.735983]\n",
      "epoch:5 step:4511 [D loss: 0.094903, acc.: 99.22%] [G loss: 1.395707]\n",
      "epoch:5 step:4512 [D loss: 0.419668, acc.: 78.12%] [G loss: 5.749790]\n",
      "epoch:5 step:4513 [D loss: 0.592790, acc.: 70.31%] [G loss: 3.216589]\n",
      "epoch:5 step:4514 [D loss: 0.025930, acc.: 100.00%] [G loss: 2.272963]\n",
      "epoch:5 step:4515 [D loss: 0.215609, acc.: 92.19%] [G loss: 3.816509]\n",
      "epoch:5 step:4516 [D loss: 0.050369, acc.: 100.00%] [G loss: 3.470850]\n",
      "epoch:5 step:4517 [D loss: 0.147302, acc.: 99.22%] [G loss: 3.513215]\n",
      "epoch:5 step:4518 [D loss: 0.701951, acc.: 61.72%] [G loss: 2.708817]\n",
      "epoch:5 step:4519 [D loss: 0.069699, acc.: 98.44%] [G loss: 3.936749]\n",
      "epoch:5 step:4520 [D loss: 0.128509, acc.: 96.09%] [G loss: 3.464738]\n",
      "epoch:5 step:4521 [D loss: 0.480143, acc.: 78.91%] [G loss: 5.605260]\n",
      "epoch:5 step:4522 [D loss: 0.417412, acc.: 75.00%] [G loss: 3.046128]\n",
      "epoch:5 step:4523 [D loss: 0.259123, acc.: 91.41%] [G loss: 3.988561]\n",
      "epoch:5 step:4524 [D loss: 0.130889, acc.: 96.09%] [G loss: 3.861221]\n",
      "epoch:5 step:4525 [D loss: 0.086158, acc.: 100.00%] [G loss: 3.549541]\n",
      "epoch:5 step:4526 [D loss: 0.099388, acc.: 99.22%] [G loss: 4.144437]\n",
      "epoch:5 step:4527 [D loss: 0.157376, acc.: 97.66%] [G loss: 3.612376]\n",
      "epoch:5 step:4528 [D loss: 0.678023, acc.: 65.62%] [G loss: 5.547312]\n",
      "epoch:5 step:4529 [D loss: 0.097312, acc.: 95.31%] [G loss: 5.219536]\n",
      "epoch:5 step:4530 [D loss: 0.430354, acc.: 78.91%] [G loss: 1.701923]\n",
      "epoch:5 step:4531 [D loss: 0.162303, acc.: 94.53%] [G loss: 2.676122]\n",
      "epoch:5 step:4532 [D loss: 0.027159, acc.: 100.00%] [G loss: 3.138355]\n",
      "epoch:5 step:4533 [D loss: 0.059044, acc.: 99.22%] [G loss: 3.043220]\n",
      "epoch:5 step:4534 [D loss: 0.141751, acc.: 96.88%] [G loss: 1.685722]\n",
      "epoch:5 step:4535 [D loss: 0.041638, acc.: 100.00%] [G loss: 0.952673]\n",
      "epoch:5 step:4536 [D loss: 0.117980, acc.: 98.44%] [G loss: 0.903968]\n",
      "epoch:5 step:4537 [D loss: 0.221364, acc.: 92.97%] [G loss: 1.054083]\n",
      "epoch:5 step:4538 [D loss: 0.178943, acc.: 94.53%] [G loss: 3.391427]\n",
      "epoch:5 step:4539 [D loss: 0.103272, acc.: 96.88%] [G loss: 3.116617]\n",
      "epoch:5 step:4540 [D loss: 0.464431, acc.: 78.12%] [G loss: 1.277654]\n",
      "epoch:5 step:4541 [D loss: 0.061607, acc.: 99.22%] [G loss: 1.666054]\n",
      "epoch:5 step:4542 [D loss: 0.293260, acc.: 89.06%] [G loss: 4.232827]\n",
      "epoch:5 step:4543 [D loss: 0.426057, acc.: 77.34%] [G loss: 4.208887]\n",
      "epoch:5 step:4544 [D loss: 0.263232, acc.: 89.84%] [G loss: 4.716234]\n",
      "epoch:5 step:4545 [D loss: 0.087120, acc.: 99.22%] [G loss: 3.956676]\n",
      "epoch:5 step:4546 [D loss: 0.150431, acc.: 97.66%] [G loss: 3.559530]\n",
      "epoch:5 step:4547 [D loss: 0.638697, acc.: 64.84%] [G loss: 6.382578]\n",
      "epoch:5 step:4548 [D loss: 0.777592, acc.: 60.16%] [G loss: 3.376539]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:5 step:4549 [D loss: 0.118640, acc.: 94.53%] [G loss: 3.777933]\n",
      "epoch:5 step:4550 [D loss: 0.083415, acc.: 98.44%] [G loss: 3.462820]\n",
      "epoch:5 step:4551 [D loss: 0.305757, acc.: 86.72%] [G loss: 4.973458]\n",
      "epoch:5 step:4552 [D loss: 0.190335, acc.: 92.19%] [G loss: 3.515943]\n",
      "epoch:5 step:4553 [D loss: 0.157649, acc.: 97.66%] [G loss: 3.132733]\n",
      "epoch:5 step:4554 [D loss: 0.124452, acc.: 98.44%] [G loss: 2.183401]\n",
      "epoch:5 step:4555 [D loss: 0.404877, acc.: 80.47%] [G loss: 6.255923]\n",
      "epoch:5 step:4556 [D loss: 0.315831, acc.: 86.72%] [G loss: 3.822962]\n",
      "epoch:5 step:4557 [D loss: 0.102479, acc.: 96.88%] [G loss: 4.551311]\n",
      "epoch:5 step:4558 [D loss: 0.104281, acc.: 98.44%] [G loss: 4.590923]\n",
      "epoch:5 step:4559 [D loss: 0.177847, acc.: 95.31%] [G loss: 4.008280]\n",
      "epoch:5 step:4560 [D loss: 0.153272, acc.: 96.09%] [G loss: 3.158359]\n",
      "epoch:5 step:4561 [D loss: 0.229682, acc.: 89.84%] [G loss: 4.087824]\n",
      "epoch:5 step:4562 [D loss: 0.273662, acc.: 90.62%] [G loss: 2.953763]\n",
      "epoch:5 step:4563 [D loss: 0.315292, acc.: 88.28%] [G loss: 3.484532]\n",
      "epoch:5 step:4564 [D loss: 0.090745, acc.: 96.88%] [G loss: 3.685780]\n",
      "epoch:5 step:4565 [D loss: 0.089967, acc.: 98.44%] [G loss: 3.251977]\n",
      "epoch:5 step:4566 [D loss: 0.286316, acc.: 86.72%] [G loss: 6.752853]\n",
      "epoch:5 step:4567 [D loss: 0.568013, acc.: 68.75%] [G loss: 1.918595]\n",
      "epoch:5 step:4568 [D loss: 0.440946, acc.: 82.81%] [G loss: 5.610447]\n",
      "epoch:5 step:4569 [D loss: 0.039247, acc.: 99.22%] [G loss: 6.683177]\n",
      "epoch:5 step:4570 [D loss: 0.904334, acc.: 54.69%] [G loss: 2.083261]\n",
      "epoch:5 step:4571 [D loss: 0.320921, acc.: 82.03%] [G loss: 4.722507]\n",
      "epoch:5 step:4572 [D loss: 0.106326, acc.: 96.88%] [G loss: 4.760561]\n",
      "epoch:5 step:4573 [D loss: 0.300259, acc.: 89.84%] [G loss: 3.595809]\n",
      "epoch:5 step:4574 [D loss: 0.322908, acc.: 87.50%] [G loss: 4.790218]\n",
      "epoch:5 step:4575 [D loss: 0.104370, acc.: 96.09%] [G loss: 4.716951]\n",
      "epoch:5 step:4576 [D loss: 0.580976, acc.: 71.88%] [G loss: 5.814285]\n",
      "epoch:5 step:4577 [D loss: 0.371124, acc.: 79.69%] [G loss: 3.789459]\n",
      "epoch:5 step:4578 [D loss: 0.097768, acc.: 98.44%] [G loss: 2.518139]\n",
      "epoch:5 step:4579 [D loss: 0.168799, acc.: 94.53%] [G loss: 4.985069]\n",
      "epoch:5 step:4580 [D loss: 0.095974, acc.: 96.88%] [G loss: 4.157374]\n",
      "epoch:5 step:4581 [D loss: 0.906438, acc.: 56.25%] [G loss: 6.584494]\n",
      "epoch:5 step:4582 [D loss: 0.632929, acc.: 67.19%] [G loss: 3.791050]\n",
      "epoch:5 step:4583 [D loss: 0.076217, acc.: 99.22%] [G loss: 2.750332]\n",
      "epoch:5 step:4584 [D loss: 0.083028, acc.: 100.00%] [G loss: 1.856183]\n",
      "epoch:5 step:4585 [D loss: 0.070780, acc.: 98.44%] [G loss: 3.288172]\n",
      "epoch:5 step:4586 [D loss: 0.269468, acc.: 89.84%] [G loss: 2.999747]\n",
      "epoch:5 step:4587 [D loss: 0.105758, acc.: 97.66%] [G loss: 1.841695]\n",
      "epoch:5 step:4588 [D loss: 0.298038, acc.: 91.41%] [G loss: 1.159292]\n",
      "epoch:5 step:4589 [D loss: 0.168254, acc.: 94.53%] [G loss: 3.529202]\n",
      "epoch:5 step:4590 [D loss: 0.513034, acc.: 75.78%] [G loss: 2.412838]\n",
      "epoch:5 step:4591 [D loss: 0.355658, acc.: 84.38%] [G loss: 3.610325]\n",
      "epoch:5 step:4592 [D loss: 0.223737, acc.: 91.41%] [G loss: 4.153017]\n",
      "epoch:5 step:4593 [D loss: 0.847175, acc.: 53.12%] [G loss: 6.167017]\n",
      "epoch:5 step:4594 [D loss: 0.458626, acc.: 76.56%] [G loss: 3.947325]\n",
      "epoch:5 step:4595 [D loss: 0.106356, acc.: 98.44%] [G loss: 3.887973]\n",
      "epoch:5 step:4596 [D loss: 0.096889, acc.: 97.66%] [G loss: 4.707552]\n",
      "epoch:5 step:4597 [D loss: 0.159437, acc.: 96.09%] [G loss: 3.848955]\n",
      "epoch:5 step:4598 [D loss: 0.069134, acc.: 98.44%] [G loss: 3.336663]\n",
      "epoch:5 step:4599 [D loss: 0.261362, acc.: 92.19%] [G loss: 3.538693]\n",
      "epoch:5 step:4600 [D loss: 0.073577, acc.: 100.00%] [G loss: 3.211534]\n",
      "##############\n",
      "[1.06230637 0.90747104 1.1267947  0.94228043 1.10912705 1.07279145\n",
      " 1.10649105 1.10550888 0.93246231 0.88921415]\n",
      "##########\n",
      "epoch:5 step:4601 [D loss: 0.149163, acc.: 96.09%] [G loss: 1.878845]\n",
      "epoch:5 step:4602 [D loss: 0.115522, acc.: 97.66%] [G loss: 3.408754]\n",
      "epoch:5 step:4603 [D loss: 0.109260, acc.: 96.88%] [G loss: 2.340602]\n",
      "epoch:5 step:4604 [D loss: 0.046491, acc.: 99.22%] [G loss: 2.640131]\n",
      "epoch:5 step:4605 [D loss: 0.150465, acc.: 95.31%] [G loss: 1.213396]\n",
      "epoch:5 step:4606 [D loss: 0.059541, acc.: 99.22%] [G loss: 0.889537]\n",
      "epoch:5 step:4607 [D loss: 0.396403, acc.: 83.59%] [G loss: 2.923182]\n",
      "epoch:5 step:4608 [D loss: 0.255483, acc.: 86.72%] [G loss: 0.944133]\n",
      "epoch:5 step:4609 [D loss: 0.428969, acc.: 80.47%] [G loss: 4.546115]\n",
      "epoch:5 step:4610 [D loss: 0.104575, acc.: 95.31%] [G loss: 5.042008]\n",
      "epoch:5 step:4611 [D loss: 0.254621, acc.: 87.50%] [G loss: 1.535656]\n",
      "epoch:5 step:4612 [D loss: 0.174180, acc.: 93.75%] [G loss: 2.018679]\n",
      "epoch:5 step:4613 [D loss: 0.048725, acc.: 100.00%] [G loss: 2.298434]\n",
      "epoch:5 step:4614 [D loss: 0.521481, acc.: 71.88%] [G loss: 7.512918]\n",
      "epoch:5 step:4615 [D loss: 1.597342, acc.: 51.56%] [G loss: 2.341383]\n",
      "epoch:5 step:4616 [D loss: 0.178908, acc.: 92.97%] [G loss: 3.559878]\n",
      "epoch:5 step:4617 [D loss: 0.075840, acc.: 97.66%] [G loss: 3.662384]\n",
      "epoch:5 step:4618 [D loss: 0.960210, acc.: 53.12%] [G loss: 4.758825]\n",
      "epoch:5 step:4619 [D loss: 0.213520, acc.: 88.28%] [G loss: 4.639285]\n",
      "epoch:5 step:4620 [D loss: 0.128775, acc.: 95.31%] [G loss: 3.058755]\n",
      "epoch:5 step:4621 [D loss: 0.136109, acc.: 94.53%] [G loss: 3.280859]\n",
      "epoch:5 step:4622 [D loss: 0.022942, acc.: 100.00%] [G loss: 3.108407]\n",
      "epoch:5 step:4623 [D loss: 0.110738, acc.: 97.66%] [G loss: 1.974078]\n",
      "epoch:5 step:4624 [D loss: 0.197797, acc.: 94.53%] [G loss: 2.743497]\n",
      "epoch:5 step:4625 [D loss: 0.068395, acc.: 98.44%] [G loss: 3.058322]\n",
      "epoch:5 step:4626 [D loss: 0.092188, acc.: 96.88%] [G loss: 1.391686]\n",
      "epoch:5 step:4627 [D loss: 0.276470, acc.: 86.72%] [G loss: 4.045718]\n",
      "epoch:5 step:4628 [D loss: 0.064142, acc.: 98.44%] [G loss: 5.505485]\n",
      "epoch:5 step:4629 [D loss: 0.856466, acc.: 53.12%] [G loss: 4.758118]\n",
      "epoch:5 step:4630 [D loss: 0.182620, acc.: 92.19%] [G loss: 4.447045]\n",
      "epoch:5 step:4631 [D loss: 0.049007, acc.: 99.22%] [G loss: 3.394640]\n",
      "epoch:5 step:4632 [D loss: 0.211550, acc.: 91.41%] [G loss: 3.370505]\n",
      "epoch:5 step:4633 [D loss: 0.119294, acc.: 97.66%] [G loss: 4.234735]\n",
      "epoch:5 step:4634 [D loss: 0.152495, acc.: 96.88%] [G loss: 2.624542]\n",
      "epoch:5 step:4635 [D loss: 0.207478, acc.: 92.19%] [G loss: 2.220161]\n",
      "epoch:5 step:4636 [D loss: 0.237610, acc.: 89.06%] [G loss: 4.292411]\n",
      "epoch:5 step:4637 [D loss: 0.334055, acc.: 82.81%] [G loss: 3.777027]\n",
      "epoch:5 step:4638 [D loss: 0.489759, acc.: 78.12%] [G loss: 5.583386]\n",
      "epoch:5 step:4639 [D loss: 0.274242, acc.: 86.72%] [G loss: 4.147241]\n",
      "epoch:5 step:4640 [D loss: 0.650863, acc.: 69.53%] [G loss: 5.961985]\n",
      "epoch:5 step:4641 [D loss: 0.055179, acc.: 100.00%] [G loss: 6.227027]\n",
      "epoch:5 step:4642 [D loss: 0.376379, acc.: 79.69%] [G loss: 2.558690]\n",
      "epoch:5 step:4643 [D loss: 0.054384, acc.: 100.00%] [G loss: 2.489944]\n",
      "epoch:5 step:4644 [D loss: 0.247533, acc.: 86.72%] [G loss: 4.387372]\n",
      "epoch:5 step:4645 [D loss: 0.086247, acc.: 99.22%] [G loss: 3.977074]\n",
      "epoch:5 step:4646 [D loss: 0.178292, acc.: 95.31%] [G loss: 3.533290]\n",
      "epoch:5 step:4647 [D loss: 0.119021, acc.: 95.31%] [G loss: 2.339368]\n",
      "epoch:5 step:4648 [D loss: 0.178036, acc.: 94.53%] [G loss: 2.295714]\n",
      "epoch:5 step:4649 [D loss: 0.071549, acc.: 99.22%] [G loss: 2.521562]\n",
      "epoch:5 step:4650 [D loss: 0.240787, acc.: 91.41%] [G loss: 3.020485]\n",
      "epoch:5 step:4651 [D loss: 0.531178, acc.: 73.44%] [G loss: 3.374002]\n",
      "epoch:5 step:4652 [D loss: 0.139187, acc.: 96.09%] [G loss: 2.794846]\n",
      "epoch:5 step:4653 [D loss: 0.306439, acc.: 87.50%] [G loss: 2.157059]\n",
      "epoch:5 step:4654 [D loss: 0.103246, acc.: 97.66%] [G loss: 2.878943]\n",
      "epoch:5 step:4655 [D loss: 0.130773, acc.: 95.31%] [G loss: 1.769779]\n",
      "epoch:5 step:4656 [D loss: 0.272723, acc.: 88.28%] [G loss: 4.300766]\n",
      "epoch:5 step:4657 [D loss: 0.037250, acc.: 100.00%] [G loss: 5.520232]\n",
      "epoch:5 step:4658 [D loss: 1.232422, acc.: 37.50%] [G loss: 4.453136]\n",
      "epoch:5 step:4659 [D loss: 0.135953, acc.: 93.75%] [G loss: 4.317089]\n",
      "epoch:5 step:4660 [D loss: 0.037902, acc.: 99.22%] [G loss: 3.078556]\n",
      "epoch:5 step:4661 [D loss: 0.162906, acc.: 94.53%] [G loss: 3.657047]\n",
      "epoch:5 step:4662 [D loss: 0.062585, acc.: 100.00%] [G loss: 3.628218]\n",
      "epoch:5 step:4663 [D loss: 0.464204, acc.: 78.12%] [G loss: 4.211648]\n",
      "epoch:5 step:4664 [D loss: 0.116554, acc.: 96.09%] [G loss: 3.716353]\n",
      "epoch:5 step:4665 [D loss: 0.363094, acc.: 83.59%] [G loss: 3.344603]\n",
      "epoch:5 step:4666 [D loss: 0.052832, acc.: 99.22%] [G loss: 3.603445]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:5 step:4667 [D loss: 0.886803, acc.: 57.81%] [G loss: 4.998958]\n",
      "epoch:5 step:4668 [D loss: 0.161360, acc.: 92.19%] [G loss: 4.871502]\n",
      "epoch:5 step:4669 [D loss: 0.223834, acc.: 89.84%] [G loss: 2.814556]\n",
      "epoch:5 step:4670 [D loss: 0.098190, acc.: 98.44%] [G loss: 4.082486]\n",
      "epoch:5 step:4671 [D loss: 0.110137, acc.: 97.66%] [G loss: 3.097264]\n",
      "epoch:5 step:4672 [D loss: 0.287772, acc.: 87.50%] [G loss: 4.253530]\n",
      "epoch:5 step:4673 [D loss: 0.294802, acc.: 89.06%] [G loss: 2.867613]\n",
      "epoch:5 step:4674 [D loss: 0.081964, acc.: 98.44%] [G loss: 2.630695]\n",
      "epoch:5 step:4675 [D loss: 0.229184, acc.: 93.75%] [G loss: 2.881781]\n",
      "epoch:5 step:4676 [D loss: 0.422073, acc.: 81.25%] [G loss: 2.330269]\n",
      "epoch:5 step:4677 [D loss: 0.095017, acc.: 97.66%] [G loss: 2.085667]\n",
      "epoch:5 step:4678 [D loss: 0.051431, acc.: 100.00%] [G loss: 2.879676]\n",
      "epoch:5 step:4679 [D loss: 0.084073, acc.: 99.22%] [G loss: 2.022702]\n",
      "epoch:5 step:4680 [D loss: 0.222925, acc.: 91.41%] [G loss: 1.391926]\n",
      "epoch:5 step:4681 [D loss: 0.402574, acc.: 82.03%] [G loss: 4.141545]\n",
      "epoch:5 step:4682 [D loss: 0.226126, acc.: 90.62%] [G loss: 1.207978]\n",
      "epoch:5 step:4683 [D loss: 0.241021, acc.: 92.19%] [G loss: 3.602786]\n",
      "epoch:5 step:4684 [D loss: 1.051718, acc.: 55.47%] [G loss: 3.942448]\n",
      "epoch:5 step:4685 [D loss: 0.241518, acc.: 90.62%] [G loss: 3.604861]\n",
      "epoch:5 step:4686 [D loss: 0.077579, acc.: 98.44%] [G loss: 3.601426]\n",
      "epoch:6 step:4687 [D loss: 0.237084, acc.: 92.19%] [G loss: 4.685868]\n",
      "epoch:6 step:4688 [D loss: 0.273896, acc.: 88.28%] [G loss: 3.389585]\n",
      "epoch:6 step:4689 [D loss: 0.092342, acc.: 97.66%] [G loss: 2.817107]\n",
      "epoch:6 step:4690 [D loss: 0.270588, acc.: 90.62%] [G loss: 4.066451]\n",
      "epoch:6 step:4691 [D loss: 0.123542, acc.: 95.31%] [G loss: 4.476097]\n",
      "epoch:6 step:4692 [D loss: 0.290546, acc.: 89.84%] [G loss: 1.919118]\n",
      "epoch:6 step:4693 [D loss: 0.265363, acc.: 86.72%] [G loss: 4.513876]\n",
      "epoch:6 step:4694 [D loss: 0.185001, acc.: 92.97%] [G loss: 3.170068]\n",
      "epoch:6 step:4695 [D loss: 0.185793, acc.: 95.31%] [G loss: 4.054538]\n",
      "epoch:6 step:4696 [D loss: 0.183214, acc.: 94.53%] [G loss: 3.737081]\n",
      "epoch:6 step:4697 [D loss: 0.062366, acc.: 98.44%] [G loss: 2.774565]\n",
      "epoch:6 step:4698 [D loss: 0.095878, acc.: 98.44%] [G loss: 2.855598]\n",
      "epoch:6 step:4699 [D loss: 0.056715, acc.: 100.00%] [G loss: 2.859005]\n",
      "epoch:6 step:4700 [D loss: 0.233006, acc.: 92.97%] [G loss: 3.933665]\n",
      "epoch:6 step:4701 [D loss: 0.385207, acc.: 80.47%] [G loss: 3.515829]\n",
      "epoch:6 step:4702 [D loss: 0.146908, acc.: 96.09%] [G loss: 4.677638]\n",
      "epoch:6 step:4703 [D loss: 0.057607, acc.: 99.22%] [G loss: 3.436779]\n",
      "epoch:6 step:4704 [D loss: 0.095678, acc.: 97.66%] [G loss: 2.725196]\n",
      "epoch:6 step:4705 [D loss: 0.066942, acc.: 99.22%] [G loss: 1.748983]\n",
      "epoch:6 step:4706 [D loss: 0.241168, acc.: 91.41%] [G loss: 4.804698]\n",
      "epoch:6 step:4707 [D loss: 0.305633, acc.: 84.38%] [G loss: 1.999431]\n",
      "epoch:6 step:4708 [D loss: 0.114470, acc.: 96.88%] [G loss: 1.104915]\n",
      "epoch:6 step:4709 [D loss: 0.149937, acc.: 96.88%] [G loss: 4.167742]\n",
      "epoch:6 step:4710 [D loss: 0.118130, acc.: 95.31%] [G loss: 4.009500]\n",
      "epoch:6 step:4711 [D loss: 0.225345, acc.: 90.62%] [G loss: 2.701151]\n",
      "epoch:6 step:4712 [D loss: 0.061136, acc.: 99.22%] [G loss: 3.337119]\n",
      "epoch:6 step:4713 [D loss: 0.159128, acc.: 93.75%] [G loss: 5.282765]\n",
      "epoch:6 step:4714 [D loss: 1.182571, acc.: 42.97%] [G loss: 7.435085]\n",
      "epoch:6 step:4715 [D loss: 0.534510, acc.: 71.09%] [G loss: 4.646107]\n",
      "epoch:6 step:4716 [D loss: 0.049733, acc.: 99.22%] [G loss: 3.737410]\n",
      "epoch:6 step:4717 [D loss: 0.057735, acc.: 100.00%] [G loss: 3.073568]\n",
      "epoch:6 step:4718 [D loss: 0.050743, acc.: 99.22%] [G loss: 1.743939]\n",
      "epoch:6 step:4719 [D loss: 0.125901, acc.: 96.88%] [G loss: 1.202073]\n",
      "epoch:6 step:4720 [D loss: 0.691562, acc.: 66.41%] [G loss: 8.828676]\n",
      "epoch:6 step:4721 [D loss: 2.289229, acc.: 50.00%] [G loss: 3.021176]\n",
      "epoch:6 step:4722 [D loss: 0.252081, acc.: 92.97%] [G loss: 3.011074]\n",
      "epoch:6 step:4723 [D loss: 0.025717, acc.: 99.22%] [G loss: 2.812920]\n",
      "epoch:6 step:4724 [D loss: 0.169377, acc.: 92.19%] [G loss: 0.353416]\n",
      "epoch:6 step:4725 [D loss: 0.802493, acc.: 60.94%] [G loss: 4.799284]\n",
      "epoch:6 step:4726 [D loss: 0.635791, acc.: 67.97%] [G loss: 2.973437]\n",
      "epoch:6 step:4727 [D loss: 0.115990, acc.: 96.88%] [G loss: 2.656966]\n",
      "epoch:6 step:4728 [D loss: 0.107871, acc.: 96.88%] [G loss: 1.432456]\n",
      "epoch:6 step:4729 [D loss: 0.794019, acc.: 60.94%] [G loss: 6.149317]\n",
      "epoch:6 step:4730 [D loss: 0.817951, acc.: 60.94%] [G loss: 2.905162]\n",
      "epoch:6 step:4731 [D loss: 0.385055, acc.: 83.59%] [G loss: 3.211057]\n",
      "epoch:6 step:4732 [D loss: 0.135466, acc.: 95.31%] [G loss: 3.548241]\n",
      "epoch:6 step:4733 [D loss: 0.240417, acc.: 92.19%] [G loss: 2.522701]\n",
      "epoch:6 step:4734 [D loss: 0.158308, acc.: 95.31%] [G loss: 2.348112]\n",
      "epoch:6 step:4735 [D loss: 0.212317, acc.: 92.97%] [G loss: 2.329584]\n",
      "epoch:6 step:4736 [D loss: 0.154110, acc.: 98.44%] [G loss: 1.931804]\n",
      "epoch:6 step:4737 [D loss: 0.123890, acc.: 96.88%] [G loss: 2.059248]\n",
      "epoch:6 step:4738 [D loss: 0.191442, acc.: 95.31%] [G loss: 1.786515]\n",
      "epoch:6 step:4739 [D loss: 0.615605, acc.: 75.00%] [G loss: 4.721960]\n",
      "epoch:6 step:4740 [D loss: 0.983606, acc.: 54.69%] [G loss: 2.545532]\n",
      "epoch:6 step:4741 [D loss: 0.628798, acc.: 69.53%] [G loss: 4.808781]\n",
      "epoch:6 step:4742 [D loss: 0.332301, acc.: 81.25%] [G loss: 4.334633]\n",
      "epoch:6 step:4743 [D loss: 0.148433, acc.: 94.53%] [G loss: 2.551191]\n",
      "epoch:6 step:4744 [D loss: 0.070095, acc.: 98.44%] [G loss: 1.168207]\n",
      "epoch:6 step:4745 [D loss: 0.180513, acc.: 94.53%] [G loss: 3.475572]\n",
      "epoch:6 step:4746 [D loss: 0.119890, acc.: 99.22%] [G loss: 3.088769]\n",
      "epoch:6 step:4747 [D loss: 0.241985, acc.: 92.97%] [G loss: 2.000584]\n",
      "epoch:6 step:4748 [D loss: 0.180649, acc.: 95.31%] [G loss: 2.620750]\n",
      "epoch:6 step:4749 [D loss: 0.286089, acc.: 92.19%] [G loss: 3.783805]\n",
      "epoch:6 step:4750 [D loss: 0.226797, acc.: 91.41%] [G loss: 1.739058]\n",
      "epoch:6 step:4751 [D loss: 0.504914, acc.: 75.00%] [G loss: 2.767272]\n",
      "epoch:6 step:4752 [D loss: 0.151262, acc.: 95.31%] [G loss: 3.908093]\n",
      "epoch:6 step:4753 [D loss: 0.143468, acc.: 97.66%] [G loss: 2.539335]\n",
      "epoch:6 step:4754 [D loss: 0.244125, acc.: 91.41%] [G loss: 4.236298]\n",
      "epoch:6 step:4755 [D loss: 0.170079, acc.: 96.88%] [G loss: 3.109120]\n",
      "epoch:6 step:4756 [D loss: 0.089184, acc.: 100.00%] [G loss: 2.983222]\n",
      "epoch:6 step:4757 [D loss: 0.218905, acc.: 91.41%] [G loss: 4.146735]\n",
      "epoch:6 step:4758 [D loss: 0.061856, acc.: 100.00%] [G loss: 4.133729]\n",
      "epoch:6 step:4759 [D loss: 0.112079, acc.: 96.09%] [G loss: 2.019424]\n",
      "epoch:6 step:4760 [D loss: 0.027083, acc.: 100.00%] [G loss: 2.122494]\n",
      "epoch:6 step:4761 [D loss: 0.114127, acc.: 97.66%] [G loss: 2.303493]\n",
      "epoch:6 step:4762 [D loss: 0.139225, acc.: 96.88%] [G loss: 1.008059]\n",
      "epoch:6 step:4763 [D loss: 0.197898, acc.: 94.53%] [G loss: 0.993072]\n",
      "epoch:6 step:4764 [D loss: 0.022320, acc.: 100.00%] [G loss: 1.438095]\n",
      "epoch:6 step:4765 [D loss: 0.069480, acc.: 97.66%] [G loss: 0.530345]\n",
      "epoch:6 step:4766 [D loss: 0.431664, acc.: 76.56%] [G loss: 5.518548]\n",
      "epoch:6 step:4767 [D loss: 0.626956, acc.: 63.28%] [G loss: 3.188734]\n",
      "epoch:6 step:4768 [D loss: 0.069601, acc.: 99.22%] [G loss: 1.768546]\n",
      "epoch:6 step:4769 [D loss: 0.071939, acc.: 98.44%] [G loss: 2.562300]\n",
      "epoch:6 step:4770 [D loss: 0.297391, acc.: 91.41%] [G loss: 3.601513]\n",
      "epoch:6 step:4771 [D loss: 0.471680, acc.: 75.00%] [G loss: 1.607813]\n",
      "epoch:6 step:4772 [D loss: 0.192070, acc.: 92.97%] [G loss: 3.977736]\n",
      "epoch:6 step:4773 [D loss: 0.079758, acc.: 100.00%] [G loss: 4.033422]\n",
      "epoch:6 step:4774 [D loss: 1.441723, acc.: 28.91%] [G loss: 5.530097]\n",
      "epoch:6 step:4775 [D loss: 0.439464, acc.: 77.34%] [G loss: 4.241761]\n",
      "epoch:6 step:4776 [D loss: 0.172504, acc.: 93.75%] [G loss: 3.333383]\n",
      "epoch:6 step:4777 [D loss: 0.234633, acc.: 92.97%] [G loss: 3.813765]\n",
      "epoch:6 step:4778 [D loss: 0.095334, acc.: 98.44%] [G loss: 3.887002]\n",
      "epoch:6 step:4779 [D loss: 0.305744, acc.: 85.94%] [G loss: 2.969513]\n",
      "epoch:6 step:4780 [D loss: 0.357403, acc.: 82.03%] [G loss: 4.477604]\n",
      "epoch:6 step:4781 [D loss: 0.084570, acc.: 99.22%] [G loss: 3.964935]\n",
      "epoch:6 step:4782 [D loss: 0.152092, acc.: 96.88%] [G loss: 3.068957]\n",
      "epoch:6 step:4783 [D loss: 0.085362, acc.: 98.44%] [G loss: 1.989293]\n",
      "epoch:6 step:4784 [D loss: 0.072808, acc.: 99.22%] [G loss: 1.046423]\n",
      "epoch:6 step:4785 [D loss: 0.101873, acc.: 98.44%] [G loss: 0.633497]\n",
      "epoch:6 step:4786 [D loss: 0.060771, acc.: 99.22%] [G loss: 0.826498]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:6 step:4787 [D loss: 0.086436, acc.: 97.66%] [G loss: 0.208589]\n",
      "epoch:6 step:4788 [D loss: 0.082068, acc.: 100.00%] [G loss: 0.298619]\n",
      "epoch:6 step:4789 [D loss: 0.098337, acc.: 96.88%] [G loss: 0.098533]\n",
      "epoch:6 step:4790 [D loss: 0.339871, acc.: 83.59%] [G loss: 4.065224]\n",
      "epoch:6 step:4791 [D loss: 0.234998, acc.: 89.06%] [G loss: 3.260898]\n",
      "epoch:6 step:4792 [D loss: 0.357625, acc.: 85.94%] [G loss: 0.225103]\n",
      "epoch:6 step:4793 [D loss: 0.126995, acc.: 96.09%] [G loss: 0.313694]\n",
      "epoch:6 step:4794 [D loss: 0.065384, acc.: 100.00%] [G loss: 1.394245]\n",
      "epoch:6 step:4795 [D loss: 0.009785, acc.: 100.00%] [G loss: 1.197778]\n",
      "epoch:6 step:4796 [D loss: 0.018803, acc.: 100.00%] [G loss: 0.710894]\n",
      "epoch:6 step:4797 [D loss: 0.811718, acc.: 57.81%] [G loss: 3.054318]\n",
      "epoch:6 step:4798 [D loss: 0.082523, acc.: 97.66%] [G loss: 3.908611]\n",
      "epoch:6 step:4799 [D loss: 0.129276, acc.: 93.75%] [G loss: 1.754225]\n",
      "epoch:6 step:4800 [D loss: 0.401864, acc.: 85.16%] [G loss: 3.173396]\n",
      "##############\n",
      "[0.99730871 0.99773612 0.95469026 0.96084113 1.09683085 1.03467735\n",
      " 0.82757915 0.97021286 2.1159815  2.11914338]\n",
      "##########\n",
      "epoch:6 step:4801 [D loss: 1.127516, acc.: 42.97%] [G loss: 5.528951]\n",
      "epoch:6 step:4802 [D loss: 0.588901, acc.: 70.31%] [G loss: 3.815579]\n",
      "epoch:6 step:4803 [D loss: 0.188683, acc.: 93.75%] [G loss: 3.947937]\n",
      "epoch:6 step:4804 [D loss: 0.033126, acc.: 100.00%] [G loss: 3.895439]\n",
      "epoch:6 step:4805 [D loss: 0.055099, acc.: 100.00%] [G loss: 3.059403]\n",
      "epoch:6 step:4806 [D loss: 0.122069, acc.: 97.66%] [G loss: 3.230905]\n",
      "epoch:6 step:4807 [D loss: 0.069101, acc.: 99.22%] [G loss: 2.407214]\n",
      "epoch:6 step:4808 [D loss: 0.099982, acc.: 98.44%] [G loss: 2.644238]\n",
      "epoch:6 step:4809 [D loss: 0.087577, acc.: 97.66%] [G loss: 1.747439]\n",
      "epoch:6 step:4810 [D loss: 0.088302, acc.: 98.44%] [G loss: 1.181877]\n",
      "epoch:6 step:4811 [D loss: 0.047220, acc.: 100.00%] [G loss: 0.421698]\n",
      "epoch:6 step:4812 [D loss: 0.223857, acc.: 95.31%] [G loss: 0.884864]\n",
      "epoch:6 step:4813 [D loss: 0.037240, acc.: 98.44%] [G loss: 0.433257]\n",
      "epoch:6 step:4814 [D loss: 0.054666, acc.: 100.00%] [G loss: 0.643371]\n",
      "epoch:6 step:4815 [D loss: 0.149677, acc.: 97.66%] [G loss: 3.010556]\n",
      "epoch:6 step:4816 [D loss: 0.601064, acc.: 70.31%] [G loss: 0.378543]\n",
      "epoch:6 step:4817 [D loss: 0.159761, acc.: 94.53%] [G loss: 3.231577]\n",
      "epoch:6 step:4818 [D loss: 0.025284, acc.: 99.22%] [G loss: 4.959197]\n",
      "epoch:6 step:4819 [D loss: 0.113454, acc.: 95.31%] [G loss: 2.477374]\n",
      "epoch:6 step:4820 [D loss: 0.092875, acc.: 99.22%] [G loss: 1.160498]\n",
      "epoch:6 step:4821 [D loss: 0.231041, acc.: 88.28%] [G loss: 5.539095]\n",
      "epoch:6 step:4822 [D loss: 0.961932, acc.: 62.50%] [G loss: 1.771102]\n",
      "epoch:6 step:4823 [D loss: 0.327749, acc.: 85.16%] [G loss: 4.945067]\n",
      "epoch:6 step:4824 [D loss: 0.127031, acc.: 96.88%] [G loss: 3.725515]\n",
      "epoch:6 step:4825 [D loss: 0.064401, acc.: 100.00%] [G loss: 2.675832]\n",
      "epoch:6 step:4826 [D loss: 0.258534, acc.: 87.50%] [G loss: 4.808708]\n",
      "epoch:6 step:4827 [D loss: 0.248296, acc.: 89.84%] [G loss: 2.646393]\n",
      "epoch:6 step:4828 [D loss: 0.177382, acc.: 96.09%] [G loss: 3.325969]\n",
      "epoch:6 step:4829 [D loss: 0.078844, acc.: 99.22%] [G loss: 3.316367]\n",
      "epoch:6 step:4830 [D loss: 0.320597, acc.: 88.28%] [G loss: 4.222701]\n",
      "epoch:6 step:4831 [D loss: 0.192672, acc.: 92.97%] [G loss: 2.695738]\n",
      "epoch:6 step:4832 [D loss: 0.165015, acc.: 93.75%] [G loss: 3.617812]\n",
      "epoch:6 step:4833 [D loss: 0.179208, acc.: 92.19%] [G loss: 2.377396]\n",
      "epoch:6 step:4834 [D loss: 0.195691, acc.: 93.75%] [G loss: 3.922776]\n",
      "epoch:6 step:4835 [D loss: 0.051607, acc.: 100.00%] [G loss: 3.908049]\n",
      "epoch:6 step:4836 [D loss: 0.087052, acc.: 99.22%] [G loss: 3.652018]\n",
      "epoch:6 step:4837 [D loss: 0.117396, acc.: 95.31%] [G loss: 2.394399]\n",
      "epoch:6 step:4838 [D loss: 0.179539, acc.: 96.09%] [G loss: 4.010526]\n",
      "epoch:6 step:4839 [D loss: 0.486250, acc.: 76.56%] [G loss: 1.549495]\n",
      "epoch:6 step:4840 [D loss: 0.025663, acc.: 100.00%] [G loss: 3.006686]\n",
      "epoch:6 step:4841 [D loss: 0.135933, acc.: 95.31%] [G loss: 2.946039]\n",
      "epoch:6 step:4842 [D loss: 0.445511, acc.: 78.91%] [G loss: 2.358325]\n",
      "epoch:6 step:4843 [D loss: 0.447804, acc.: 74.22%] [G loss: 6.678503]\n",
      "epoch:6 step:4844 [D loss: 1.122175, acc.: 54.69%] [G loss: 2.214605]\n",
      "epoch:6 step:4845 [D loss: 0.611078, acc.: 71.88%] [G loss: 6.548800]\n",
      "epoch:6 step:4846 [D loss: 1.046649, acc.: 60.94%] [G loss: 3.991256]\n",
      "epoch:6 step:4847 [D loss: 0.104008, acc.: 97.66%] [G loss: 3.135183]\n",
      "epoch:6 step:4848 [D loss: 0.158145, acc.: 94.53%] [G loss: 4.478371]\n",
      "epoch:6 step:4849 [D loss: 0.065177, acc.: 99.22%] [G loss: 3.386409]\n",
      "epoch:6 step:4850 [D loss: 0.143370, acc.: 94.53%] [G loss: 3.642438]\n",
      "epoch:6 step:4851 [D loss: 0.111889, acc.: 96.88%] [G loss: 2.720833]\n",
      "epoch:6 step:4852 [D loss: 0.164541, acc.: 94.53%] [G loss: 3.812235]\n",
      "epoch:6 step:4853 [D loss: 0.148177, acc.: 96.09%] [G loss: 2.029135]\n",
      "epoch:6 step:4854 [D loss: 0.272934, acc.: 85.94%] [G loss: 5.105724]\n",
      "epoch:6 step:4855 [D loss: 0.163163, acc.: 92.19%] [G loss: 3.703337]\n",
      "epoch:6 step:4856 [D loss: 0.421889, acc.: 78.91%] [G loss: 3.256647]\n",
      "epoch:6 step:4857 [D loss: 0.081660, acc.: 100.00%] [G loss: 3.271312]\n",
      "epoch:6 step:4858 [D loss: 0.080073, acc.: 100.00%] [G loss: 3.394495]\n",
      "epoch:6 step:4859 [D loss: 0.337921, acc.: 85.94%] [G loss: 4.669704]\n",
      "epoch:6 step:4860 [D loss: 0.218718, acc.: 92.19%] [G loss: 4.033248]\n",
      "epoch:6 step:4861 [D loss: 0.077363, acc.: 98.44%] [G loss: 4.180948]\n",
      "epoch:6 step:4862 [D loss: 0.185748, acc.: 93.75%] [G loss: 3.412309]\n",
      "epoch:6 step:4863 [D loss: 0.079653, acc.: 99.22%] [G loss: 3.480869]\n",
      "epoch:6 step:4864 [D loss: 0.171774, acc.: 95.31%] [G loss: 1.913940]\n",
      "epoch:6 step:4865 [D loss: 0.505651, acc.: 72.66%] [G loss: 6.061029]\n",
      "epoch:6 step:4866 [D loss: 0.645880, acc.: 67.19%] [G loss: 1.885848]\n",
      "epoch:6 step:4867 [D loss: 0.305712, acc.: 88.28%] [G loss: 3.987662]\n",
      "epoch:6 step:4868 [D loss: 0.090732, acc.: 96.88%] [G loss: 4.580386]\n",
      "epoch:6 step:4869 [D loss: 0.211561, acc.: 92.19%] [G loss: 0.811102]\n",
      "epoch:6 step:4870 [D loss: 0.841009, acc.: 64.84%] [G loss: 6.447154]\n",
      "epoch:6 step:4871 [D loss: 0.961645, acc.: 61.72%] [G loss: 1.171736]\n",
      "epoch:6 step:4872 [D loss: 0.616295, acc.: 67.19%] [G loss: 5.210900]\n",
      "epoch:6 step:4873 [D loss: 0.534083, acc.: 72.66%] [G loss: 4.611421]\n",
      "epoch:6 step:4874 [D loss: 0.510004, acc.: 76.56%] [G loss: 2.765378]\n",
      "epoch:6 step:4875 [D loss: 0.253957, acc.: 86.72%] [G loss: 5.108120]\n",
      "epoch:6 step:4876 [D loss: 0.196370, acc.: 92.97%] [G loss: 4.929760]\n",
      "epoch:6 step:4877 [D loss: 0.129986, acc.: 98.44%] [G loss: 3.339549]\n",
      "epoch:6 step:4878 [D loss: 0.351405, acc.: 88.28%] [G loss: 4.222035]\n",
      "epoch:6 step:4879 [D loss: 0.146249, acc.: 96.88%] [G loss: 4.337576]\n",
      "epoch:6 step:4880 [D loss: 0.338425, acc.: 84.38%] [G loss: 3.463759]\n",
      "epoch:6 step:4881 [D loss: 0.094264, acc.: 98.44%] [G loss: 3.589373]\n",
      "epoch:6 step:4882 [D loss: 0.431976, acc.: 81.25%] [G loss: 3.907345]\n",
      "epoch:6 step:4883 [D loss: 0.280464, acc.: 87.50%] [G loss: 2.809163]\n",
      "epoch:6 step:4884 [D loss: 0.050126, acc.: 100.00%] [G loss: 2.528948]\n",
      "epoch:6 step:4885 [D loss: 0.102933, acc.: 97.66%] [G loss: 2.568334]\n",
      "epoch:6 step:4886 [D loss: 0.121175, acc.: 96.88%] [G loss: 1.635710]\n",
      "epoch:6 step:4887 [D loss: 0.149603, acc.: 96.09%] [G loss: 1.084649]\n",
      "epoch:6 step:4888 [D loss: 0.278069, acc.: 90.62%] [G loss: 1.450104]\n",
      "epoch:6 step:4889 [D loss: 0.198009, acc.: 88.28%] [G loss: 0.391546]\n",
      "epoch:6 step:4890 [D loss: 0.169594, acc.: 92.97%] [G loss: 1.460645]\n",
      "epoch:6 step:4891 [D loss: 0.069334, acc.: 96.88%] [G loss: 1.217147]\n",
      "epoch:6 step:4892 [D loss: 0.071364, acc.: 98.44%] [G loss: 0.466908]\n",
      "epoch:6 step:4893 [D loss: 0.468034, acc.: 79.69%] [G loss: 3.717318]\n",
      "epoch:6 step:4894 [D loss: 1.662689, acc.: 50.78%] [G loss: 1.141639]\n",
      "epoch:6 step:4895 [D loss: 0.703742, acc.: 69.53%] [G loss: 3.852846]\n",
      "epoch:6 step:4896 [D loss: 0.232887, acc.: 91.41%] [G loss: 4.132025]\n",
      "epoch:6 step:4897 [D loss: 0.093293, acc.: 96.88%] [G loss: 3.197227]\n",
      "epoch:6 step:4898 [D loss: 0.037511, acc.: 100.00%] [G loss: 2.762569]\n",
      "epoch:6 step:4899 [D loss: 0.250107, acc.: 89.06%] [G loss: 3.766451]\n",
      "epoch:6 step:4900 [D loss: 0.404337, acc.: 76.56%] [G loss: 3.109761]\n",
      "epoch:6 step:4901 [D loss: 0.196856, acc.: 92.19%] [G loss: 3.758442]\n",
      "epoch:6 step:4902 [D loss: 0.097450, acc.: 97.66%] [G loss: 4.385303]\n",
      "epoch:6 step:4903 [D loss: 0.079089, acc.: 98.44%] [G loss: 3.206599]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:6 step:4904 [D loss: 0.111773, acc.: 97.66%] [G loss: 2.864099]\n",
      "epoch:6 step:4905 [D loss: 0.092901, acc.: 99.22%] [G loss: 2.040161]\n",
      "epoch:6 step:4906 [D loss: 0.089207, acc.: 99.22%] [G loss: 1.445673]\n",
      "epoch:6 step:4907 [D loss: 0.035864, acc.: 100.00%] [G loss: 1.555600]\n",
      "epoch:6 step:4908 [D loss: 0.046665, acc.: 98.44%] [G loss: 1.174368]\n",
      "epoch:6 step:4909 [D loss: 0.080409, acc.: 98.44%] [G loss: 1.504628]\n",
      "epoch:6 step:4910 [D loss: 0.095341, acc.: 100.00%] [G loss: 0.890165]\n",
      "epoch:6 step:4911 [D loss: 0.149968, acc.: 96.09%] [G loss: 3.571342]\n",
      "epoch:6 step:4912 [D loss: 0.088201, acc.: 97.66%] [G loss: 2.366089]\n",
      "epoch:6 step:4913 [D loss: 0.321205, acc.: 87.50%] [G loss: 0.195499]\n",
      "epoch:6 step:4914 [D loss: 0.207141, acc.: 92.19%] [G loss: 2.840835]\n",
      "epoch:6 step:4915 [D loss: 0.054094, acc.: 99.22%] [G loss: 4.813975]\n",
      "epoch:6 step:4916 [D loss: 0.127270, acc.: 92.97%] [G loss: 1.739487]\n",
      "epoch:6 step:4917 [D loss: 0.167393, acc.: 95.31%] [G loss: 2.111592]\n",
      "epoch:6 step:4918 [D loss: 1.356691, acc.: 32.81%] [G loss: 5.427444]\n",
      "epoch:6 step:4919 [D loss: 0.250531, acc.: 83.59%] [G loss: 5.339978]\n",
      "epoch:6 step:4920 [D loss: 0.344245, acc.: 84.38%] [G loss: 3.245888]\n",
      "epoch:6 step:4921 [D loss: 0.120096, acc.: 96.09%] [G loss: 2.774129]\n",
      "epoch:6 step:4922 [D loss: 0.097432, acc.: 97.66%] [G loss: 3.415008]\n",
      "epoch:6 step:4923 [D loss: 0.125466, acc.: 96.88%] [G loss: 3.806784]\n",
      "epoch:6 step:4924 [D loss: 0.101673, acc.: 98.44%] [G loss: 3.509329]\n",
      "epoch:6 step:4925 [D loss: 0.306545, acc.: 89.06%] [G loss: 3.051404]\n",
      "epoch:6 step:4926 [D loss: 0.536118, acc.: 67.97%] [G loss: 3.147525]\n",
      "epoch:6 step:4927 [D loss: 0.060078, acc.: 99.22%] [G loss: 2.605444]\n",
      "epoch:6 step:4928 [D loss: 0.134878, acc.: 95.31%] [G loss: 2.856404]\n",
      "epoch:6 step:4929 [D loss: 0.114294, acc.: 99.22%] [G loss: 2.565347]\n",
      "epoch:6 step:4930 [D loss: 0.096454, acc.: 99.22%] [G loss: 1.524330]\n",
      "epoch:6 step:4931 [D loss: 0.726929, acc.: 65.62%] [G loss: 7.049158]\n",
      "epoch:6 step:4932 [D loss: 1.738211, acc.: 50.00%] [G loss: 3.861587]\n",
      "epoch:6 step:4933 [D loss: 0.485311, acc.: 75.78%] [G loss: 2.470917]\n",
      "epoch:6 step:4934 [D loss: 0.163407, acc.: 96.09%] [G loss: 3.819988]\n",
      "epoch:6 step:4935 [D loss: 0.136381, acc.: 96.88%] [G loss: 2.973848]\n",
      "epoch:6 step:4936 [D loss: 0.226986, acc.: 92.19%] [G loss: 1.603487]\n",
      "epoch:6 step:4937 [D loss: 0.122724, acc.: 97.66%] [G loss: 3.169243]\n",
      "epoch:6 step:4938 [D loss: 0.137303, acc.: 97.66%] [G loss: 3.471861]\n",
      "epoch:6 step:4939 [D loss: 0.165873, acc.: 96.09%] [G loss: 3.547828]\n",
      "epoch:6 step:4940 [D loss: 0.077264, acc.: 99.22%] [G loss: 3.392015]\n",
      "epoch:6 step:4941 [D loss: 0.239435, acc.: 92.19%] [G loss: 3.488644]\n",
      "epoch:6 step:4942 [D loss: 0.080530, acc.: 98.44%] [G loss: 3.661466]\n",
      "epoch:6 step:4943 [D loss: 0.315279, acc.: 89.06%] [G loss: 2.829365]\n",
      "epoch:6 step:4944 [D loss: 0.082667, acc.: 99.22%] [G loss: 3.954446]\n",
      "epoch:6 step:4945 [D loss: 0.108789, acc.: 99.22%] [G loss: 3.543859]\n",
      "epoch:6 step:4946 [D loss: 0.105771, acc.: 97.66%] [G loss: 1.996853]\n",
      "epoch:6 step:4947 [D loss: 0.319675, acc.: 89.84%] [G loss: 3.563910]\n",
      "epoch:6 step:4948 [D loss: 0.142817, acc.: 96.09%] [G loss: 3.150172]\n",
      "epoch:6 step:4949 [D loss: 0.034021, acc.: 100.00%] [G loss: 2.077035]\n",
      "epoch:6 step:4950 [D loss: 0.191581, acc.: 93.75%] [G loss: 1.756183]\n",
      "epoch:6 step:4951 [D loss: 0.293373, acc.: 91.41%] [G loss: 0.705873]\n",
      "epoch:6 step:4952 [D loss: 0.059075, acc.: 100.00%] [G loss: 1.575795]\n",
      "epoch:6 step:4953 [D loss: 0.386201, acc.: 85.16%] [G loss: 4.988998]\n",
      "epoch:6 step:4954 [D loss: 0.189733, acc.: 90.62%] [G loss: 4.031016]\n",
      "epoch:6 step:4955 [D loss: 0.087118, acc.: 97.66%] [G loss: 1.740227]\n",
      "epoch:6 step:4956 [D loss: 0.147602, acc.: 94.53%] [G loss: 1.073972]\n",
      "epoch:6 step:4957 [D loss: 0.565841, acc.: 73.44%] [G loss: 6.430053]\n",
      "epoch:6 step:4958 [D loss: 0.975261, acc.: 57.81%] [G loss: 2.522984]\n",
      "epoch:6 step:4959 [D loss: 0.315517, acc.: 85.94%] [G loss: 4.882849]\n",
      "epoch:6 step:4960 [D loss: 0.304248, acc.: 84.38%] [G loss: 3.313024]\n",
      "epoch:6 step:4961 [D loss: 0.238563, acc.: 93.75%] [G loss: 2.192294]\n",
      "epoch:6 step:4962 [D loss: 0.276300, acc.: 85.94%] [G loss: 5.893569]\n",
      "epoch:6 step:4963 [D loss: 0.946605, acc.: 56.25%] [G loss: 2.546275]\n",
      "epoch:6 step:4964 [D loss: 0.206964, acc.: 89.06%] [G loss: 4.342226]\n",
      "epoch:6 step:4965 [D loss: 0.080806, acc.: 98.44%] [G loss: 5.220778]\n",
      "epoch:6 step:4966 [D loss: 0.137755, acc.: 95.31%] [G loss: 3.467162]\n",
      "epoch:6 step:4967 [D loss: 0.071067, acc.: 99.22%] [G loss: 3.843699]\n",
      "epoch:6 step:4968 [D loss: 0.099031, acc.: 98.44%] [G loss: 4.212644]\n",
      "epoch:6 step:4969 [D loss: 0.166323, acc.: 92.97%] [G loss: 3.857322]\n",
      "epoch:6 step:4970 [D loss: 0.169814, acc.: 93.75%] [G loss: 3.375241]\n",
      "epoch:6 step:4971 [D loss: 0.114150, acc.: 96.88%] [G loss: 3.269122]\n",
      "epoch:6 step:4972 [D loss: 0.428870, acc.: 82.03%] [G loss: 2.304384]\n",
      "epoch:6 step:4973 [D loss: 0.059713, acc.: 100.00%] [G loss: 1.977749]\n",
      "epoch:6 step:4974 [D loss: 0.101024, acc.: 97.66%] [G loss: 2.081215]\n",
      "epoch:6 step:4975 [D loss: 0.165572, acc.: 97.66%] [G loss: 3.612189]\n",
      "epoch:6 step:4976 [D loss: 0.194979, acc.: 96.09%] [G loss: 2.689581]\n",
      "epoch:6 step:4977 [D loss: 1.866780, acc.: 30.47%] [G loss: 8.033720]\n",
      "epoch:6 step:4978 [D loss: 1.666064, acc.: 51.56%] [G loss: 5.152957]\n",
      "epoch:6 step:4979 [D loss: 0.362894, acc.: 82.81%] [G loss: 2.546492]\n",
      "epoch:6 step:4980 [D loss: 0.304710, acc.: 87.50%] [G loss: 3.083406]\n",
      "epoch:6 step:4981 [D loss: 0.087674, acc.: 98.44%] [G loss: 4.353648]\n",
      "epoch:6 step:4982 [D loss: 0.216194, acc.: 90.62%] [G loss: 3.393269]\n",
      "epoch:6 step:4983 [D loss: 0.242388, acc.: 92.19%] [G loss: 3.000044]\n",
      "epoch:6 step:4984 [D loss: 0.165914, acc.: 96.88%] [G loss: 2.695419]\n",
      "epoch:6 step:4985 [D loss: 0.347017, acc.: 86.72%] [G loss: 3.552677]\n",
      "epoch:6 step:4986 [D loss: 0.619713, acc.: 71.09%] [G loss: 2.475578]\n",
      "epoch:6 step:4987 [D loss: 0.397175, acc.: 80.47%] [G loss: 3.646394]\n",
      "epoch:6 step:4988 [D loss: 0.068528, acc.: 100.00%] [G loss: 3.703963]\n",
      "epoch:6 step:4989 [D loss: 0.324006, acc.: 86.72%] [G loss: 2.133537]\n",
      "epoch:6 step:4990 [D loss: 0.221101, acc.: 92.97%] [G loss: 3.161105]\n",
      "epoch:6 step:4991 [D loss: 0.124319, acc.: 98.44%] [G loss: 2.872313]\n",
      "epoch:6 step:4992 [D loss: 0.137923, acc.: 96.88%] [G loss: 1.391865]\n",
      "epoch:6 step:4993 [D loss: 0.163337, acc.: 94.53%] [G loss: 1.707041]\n",
      "epoch:6 step:4994 [D loss: 0.078338, acc.: 98.44%] [G loss: 1.471117]\n",
      "epoch:6 step:4995 [D loss: 0.182326, acc.: 94.53%] [G loss: 0.957856]\n",
      "epoch:6 step:4996 [D loss: 0.142085, acc.: 96.88%] [G loss: 1.870548]\n",
      "epoch:6 step:4997 [D loss: 0.147695, acc.: 95.31%] [G loss: 1.286447]\n",
      "epoch:6 step:4998 [D loss: 0.164899, acc.: 99.22%] [G loss: 1.451316]\n",
      "epoch:6 step:4999 [D loss: 0.632011, acc.: 68.75%] [G loss: 3.935318]\n",
      "epoch:6 step:5000 [D loss: 0.173829, acc.: 92.97%] [G loss: 3.826298]\n",
      "##############\n",
      "[1.04440107 1.05866437 0.98733111 0.94662059 2.10239487 2.11792156\n",
      " 2.10243523 2.11646847 2.10434001 0.97339279]\n",
      "##########\n",
      "epoch:6 step:5001 [D loss: 1.324430, acc.: 51.56%] [G loss: 0.738708]\n",
      "epoch:6 step:5002 [D loss: 0.676794, acc.: 64.06%] [G loss: 3.579669]\n",
      "epoch:6 step:5003 [D loss: 0.050204, acc.: 99.22%] [G loss: 4.447049]\n",
      "epoch:6 step:5004 [D loss: 0.280982, acc.: 85.16%] [G loss: 2.959758]\n",
      "epoch:6 step:5005 [D loss: 0.138617, acc.: 96.88%] [G loss: 2.814849]\n",
      "epoch:6 step:5006 [D loss: 0.445292, acc.: 80.47%] [G loss: 2.721044]\n",
      "epoch:6 step:5007 [D loss: 0.072841, acc.: 100.00%] [G loss: 3.125237]\n",
      "epoch:6 step:5008 [D loss: 0.096551, acc.: 100.00%] [G loss: 2.967225]\n",
      "epoch:6 step:5009 [D loss: 0.185784, acc.: 93.75%] [G loss: 3.115808]\n",
      "epoch:6 step:5010 [D loss: 0.081866, acc.: 99.22%] [G loss: 3.187116]\n",
      "epoch:6 step:5011 [D loss: 0.249282, acc.: 92.19%] [G loss: 3.326571]\n",
      "epoch:6 step:5012 [D loss: 0.257432, acc.: 91.41%] [G loss: 3.455254]\n",
      "epoch:6 step:5013 [D loss: 0.155168, acc.: 96.88%] [G loss: 2.991414]\n",
      "epoch:6 step:5014 [D loss: 0.331999, acc.: 85.94%] [G loss: 4.578575]\n",
      "epoch:6 step:5015 [D loss: 0.109177, acc.: 96.88%] [G loss: 4.145324]\n",
      "epoch:6 step:5016 [D loss: 0.065757, acc.: 98.44%] [G loss: 2.665630]\n",
      "epoch:6 step:5017 [D loss: 0.181407, acc.: 93.75%] [G loss: 0.965710]\n",
      "epoch:6 step:5018 [D loss: 0.070420, acc.: 99.22%] [G loss: 1.894144]\n",
      "epoch:6 step:5019 [D loss: 0.044136, acc.: 99.22%] [G loss: 1.072890]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:6 step:5020 [D loss: 0.113166, acc.: 99.22%] [G loss: 1.522144]\n",
      "epoch:6 step:5021 [D loss: 0.215821, acc.: 92.19%] [G loss: 1.767213]\n",
      "epoch:6 step:5022 [D loss: 0.266610, acc.: 85.94%] [G loss: 0.309602]\n",
      "epoch:6 step:5023 [D loss: 0.419445, acc.: 78.12%] [G loss: 5.496329]\n",
      "epoch:6 step:5024 [D loss: 0.301431, acc.: 83.59%] [G loss: 3.165626]\n",
      "epoch:6 step:5025 [D loss: 0.891192, acc.: 53.91%] [G loss: 2.247562]\n",
      "epoch:6 step:5026 [D loss: 0.027808, acc.: 100.00%] [G loss: 2.791461]\n",
      "epoch:6 step:5027 [D loss: 0.267938, acc.: 86.72%] [G loss: 3.637594]\n",
      "epoch:6 step:5028 [D loss: 0.228548, acc.: 89.84%] [G loss: 2.499038]\n",
      "epoch:6 step:5029 [D loss: 0.287892, acc.: 88.28%] [G loss: 5.052982]\n",
      "epoch:6 step:5030 [D loss: 0.226507, acc.: 89.06%] [G loss: 3.853148]\n",
      "epoch:6 step:5031 [D loss: 0.199450, acc.: 92.19%] [G loss: 3.385956]\n",
      "epoch:6 step:5032 [D loss: 0.143862, acc.: 97.66%] [G loss: 4.273698]\n",
      "epoch:6 step:5033 [D loss: 0.096333, acc.: 100.00%] [G loss: 4.215002]\n",
      "epoch:6 step:5034 [D loss: 0.956121, acc.: 51.56%] [G loss: 6.522975]\n",
      "epoch:6 step:5035 [D loss: 0.200055, acc.: 89.06%] [G loss: 5.930283]\n",
      "epoch:6 step:5036 [D loss: 0.106267, acc.: 95.31%] [G loss: 4.222429]\n",
      "epoch:6 step:5037 [D loss: 0.050430, acc.: 100.00%] [G loss: 3.511672]\n",
      "epoch:6 step:5038 [D loss: 0.060425, acc.: 100.00%] [G loss: 3.394736]\n",
      "epoch:6 step:5039 [D loss: 0.124104, acc.: 96.88%] [G loss: 3.659937]\n",
      "epoch:6 step:5040 [D loss: 0.112503, acc.: 98.44%] [G loss: 4.427729]\n",
      "epoch:6 step:5041 [D loss: 0.244949, acc.: 91.41%] [G loss: 2.627951]\n",
      "epoch:6 step:5042 [D loss: 0.079727, acc.: 98.44%] [G loss: 1.562917]\n",
      "epoch:6 step:5043 [D loss: 0.187540, acc.: 92.97%] [G loss: 3.447741]\n",
      "epoch:6 step:5044 [D loss: 0.193080, acc.: 89.84%] [G loss: 1.293659]\n",
      "epoch:6 step:5045 [D loss: 0.176926, acc.: 93.75%] [G loss: 1.407602]\n",
      "epoch:6 step:5046 [D loss: 0.035506, acc.: 99.22%] [G loss: 1.705831]\n",
      "epoch:6 step:5047 [D loss: 0.974465, acc.: 48.44%] [G loss: 5.326617]\n",
      "epoch:6 step:5048 [D loss: 0.569977, acc.: 71.88%] [G loss: 3.917196]\n",
      "epoch:6 step:5049 [D loss: 0.044841, acc.: 100.00%] [G loss: 3.192327]\n",
      "epoch:6 step:5050 [D loss: 0.100785, acc.: 97.66%] [G loss: 2.156586]\n",
      "epoch:6 step:5051 [D loss: 0.048104, acc.: 100.00%] [G loss: 2.572097]\n",
      "epoch:6 step:5052 [D loss: 0.161496, acc.: 94.53%] [G loss: 3.209861]\n",
      "epoch:6 step:5053 [D loss: 0.208161, acc.: 91.41%] [G loss: 1.941135]\n",
      "epoch:6 step:5054 [D loss: 0.136709, acc.: 96.88%] [G loss: 2.330617]\n",
      "epoch:6 step:5055 [D loss: 0.162522, acc.: 96.09%] [G loss: 3.113625]\n",
      "epoch:6 step:5056 [D loss: 0.146979, acc.: 98.44%] [G loss: 2.639239]\n",
      "epoch:6 step:5057 [D loss: 0.110082, acc.: 97.66%] [G loss: 2.469121]\n",
      "epoch:6 step:5058 [D loss: 0.223921, acc.: 92.19%] [G loss: 3.743124]\n",
      "epoch:6 step:5059 [D loss: 0.115922, acc.: 98.44%] [G loss: 3.821002]\n",
      "epoch:6 step:5060 [D loss: 0.787518, acc.: 58.59%] [G loss: 6.898662]\n",
      "epoch:6 step:5061 [D loss: 0.984883, acc.: 55.47%] [G loss: 4.276272]\n",
      "epoch:6 step:5062 [D loss: 0.061516, acc.: 99.22%] [G loss: 3.676384]\n",
      "epoch:6 step:5063 [D loss: 0.089327, acc.: 96.88%] [G loss: 4.254164]\n",
      "epoch:6 step:5064 [D loss: 0.035685, acc.: 100.00%] [G loss: 2.645404]\n",
      "epoch:6 step:5065 [D loss: 0.099275, acc.: 96.88%] [G loss: 3.131394]\n",
      "epoch:6 step:5066 [D loss: 0.150138, acc.: 96.09%] [G loss: 3.534248]\n",
      "epoch:6 step:5067 [D loss: 0.102369, acc.: 97.66%] [G loss: 2.284636]\n",
      "epoch:6 step:5068 [D loss: 1.288259, acc.: 48.44%] [G loss: 7.257591]\n",
      "epoch:6 step:5069 [D loss: 1.790940, acc.: 50.00%] [G loss: 4.994795]\n",
      "epoch:6 step:5070 [D loss: 0.136869, acc.: 96.09%] [G loss: 3.926909]\n",
      "epoch:6 step:5071 [D loss: 0.169817, acc.: 96.88%] [G loss: 3.907946]\n",
      "epoch:6 step:5072 [D loss: 0.092110, acc.: 98.44%] [G loss: 3.239165]\n",
      "epoch:6 step:5073 [D loss: 0.212109, acc.: 94.53%] [G loss: 3.290591]\n",
      "epoch:6 step:5074 [D loss: 0.315992, acc.: 85.94%] [G loss: 3.186746]\n",
      "epoch:6 step:5075 [D loss: 0.088146, acc.: 100.00%] [G loss: 3.598468]\n",
      "epoch:6 step:5076 [D loss: 0.454422, acc.: 76.56%] [G loss: 3.410271]\n",
      "epoch:6 step:5077 [D loss: 0.351413, acc.: 85.94%] [G loss: 2.618571]\n",
      "epoch:6 step:5078 [D loss: 0.068180, acc.: 100.00%] [G loss: 2.473000]\n",
      "epoch:6 step:5079 [D loss: 0.361716, acc.: 83.59%] [G loss: 4.724769]\n",
      "epoch:6 step:5080 [D loss: 0.248756, acc.: 90.62%] [G loss: 3.488862]\n",
      "epoch:6 step:5081 [D loss: 0.645081, acc.: 65.62%] [G loss: 4.658704]\n",
      "epoch:6 step:5082 [D loss: 0.213747, acc.: 89.84%] [G loss: 4.183565]\n",
      "epoch:6 step:5083 [D loss: 0.134573, acc.: 97.66%] [G loss: 2.920086]\n",
      "epoch:6 step:5084 [D loss: 0.044476, acc.: 100.00%] [G loss: 2.182634]\n",
      "epoch:6 step:5085 [D loss: 0.139927, acc.: 96.88%] [G loss: 3.515554]\n",
      "epoch:6 step:5086 [D loss: 0.179203, acc.: 95.31%] [G loss: 3.830558]\n",
      "epoch:6 step:5087 [D loss: 0.116427, acc.: 97.66%] [G loss: 3.630969]\n",
      "epoch:6 step:5088 [D loss: 0.908660, acc.: 47.66%] [G loss: 4.646650]\n",
      "epoch:6 step:5089 [D loss: 0.413062, acc.: 82.03%] [G loss: 3.513266]\n",
      "epoch:6 step:5090 [D loss: 0.097136, acc.: 98.44%] [G loss: 2.626865]\n",
      "epoch:6 step:5091 [D loss: 0.180047, acc.: 96.09%] [G loss: 2.622632]\n",
      "epoch:6 step:5092 [D loss: 0.173473, acc.: 94.53%] [G loss: 4.416641]\n",
      "epoch:6 step:5093 [D loss: 0.094995, acc.: 98.44%] [G loss: 3.906450]\n",
      "epoch:6 step:5094 [D loss: 0.159866, acc.: 94.53%] [G loss: 2.897441]\n",
      "epoch:6 step:5095 [D loss: 0.251403, acc.: 89.06%] [G loss: 4.064535]\n",
      "epoch:6 step:5096 [D loss: 0.056489, acc.: 98.44%] [G loss: 3.672731]\n",
      "epoch:6 step:5097 [D loss: 0.470916, acc.: 75.78%] [G loss: 0.805184]\n",
      "epoch:6 step:5098 [D loss: 0.493557, acc.: 74.22%] [G loss: 4.781200]\n",
      "epoch:6 step:5099 [D loss: 0.155091, acc.: 92.19%] [G loss: 5.221951]\n",
      "epoch:6 step:5100 [D loss: 0.416998, acc.: 78.91%] [G loss: 1.705298]\n",
      "epoch:6 step:5101 [D loss: 0.253444, acc.: 85.94%] [G loss: 3.754928]\n",
      "epoch:6 step:5102 [D loss: 0.026572, acc.: 100.00%] [G loss: 3.831657]\n",
      "epoch:6 step:5103 [D loss: 0.235481, acc.: 92.97%] [G loss: 3.370406]\n",
      "epoch:6 step:5104 [D loss: 0.091263, acc.: 98.44%] [G loss: 2.617538]\n",
      "epoch:6 step:5105 [D loss: 0.061782, acc.: 100.00%] [G loss: 2.308579]\n",
      "epoch:6 step:5106 [D loss: 0.716214, acc.: 65.62%] [G loss: 4.333098]\n",
      "epoch:6 step:5107 [D loss: 0.131628, acc.: 92.19%] [G loss: 4.316455]\n",
      "epoch:6 step:5108 [D loss: 0.186940, acc.: 93.75%] [G loss: 2.949463]\n",
      "epoch:6 step:5109 [D loss: 0.036119, acc.: 100.00%] [G loss: 1.795644]\n",
      "epoch:6 step:5110 [D loss: 0.113152, acc.: 99.22%] [G loss: 1.565323]\n",
      "epoch:6 step:5111 [D loss: 0.088114, acc.: 99.22%] [G loss: 1.771712]\n",
      "epoch:6 step:5112 [D loss: 0.041131, acc.: 100.00%] [G loss: 0.967479]\n",
      "epoch:6 step:5113 [D loss: 0.264867, acc.: 87.50%] [G loss: 4.938343]\n",
      "epoch:6 step:5114 [D loss: 0.898171, acc.: 57.81%] [G loss: 2.487714]\n",
      "epoch:6 step:5115 [D loss: 0.131255, acc.: 97.66%] [G loss: 2.644248]\n",
      "epoch:6 step:5116 [D loss: 0.327418, acc.: 87.50%] [G loss: 3.829589]\n",
      "epoch:6 step:5117 [D loss: 0.157700, acc.: 92.19%] [G loss: 2.398508]\n",
      "epoch:6 step:5118 [D loss: 0.103752, acc.: 98.44%] [G loss: 2.999096]\n",
      "epoch:6 step:5119 [D loss: 0.109386, acc.: 98.44%] [G loss: 3.191690]\n",
      "epoch:6 step:5120 [D loss: 0.162749, acc.: 96.09%] [G loss: 1.420922]\n",
      "epoch:6 step:5121 [D loss: 0.241784, acc.: 89.84%] [G loss: 2.775013]\n",
      "epoch:6 step:5122 [D loss: 0.089547, acc.: 98.44%] [G loss: 2.344697]\n",
      "epoch:6 step:5123 [D loss: 0.411188, acc.: 79.69%] [G loss: 0.036805]\n",
      "epoch:6 step:5124 [D loss: 1.160558, acc.: 53.91%] [G loss: 6.788544]\n",
      "epoch:6 step:5125 [D loss: 1.378050, acc.: 52.34%] [G loss: 2.815669]\n",
      "epoch:6 step:5126 [D loss: 0.243675, acc.: 89.84%] [G loss: 1.098587]\n",
      "epoch:6 step:5127 [D loss: 0.295218, acc.: 84.38%] [G loss: 2.733996]\n",
      "epoch:6 step:5128 [D loss: 0.075548, acc.: 98.44%] [G loss: 3.085895]\n",
      "epoch:6 step:5129 [D loss: 0.289990, acc.: 88.28%] [G loss: 1.671602]\n",
      "epoch:6 step:5130 [D loss: 0.129108, acc.: 96.88%] [G loss: 1.008569]\n",
      "epoch:6 step:5131 [D loss: 0.119306, acc.: 96.88%] [G loss: 2.072371]\n",
      "epoch:6 step:5132 [D loss: 0.163601, acc.: 97.66%] [G loss: 3.758258]\n",
      "epoch:6 step:5133 [D loss: 0.766241, acc.: 59.38%] [G loss: 2.134133]\n",
      "epoch:6 step:5134 [D loss: 0.383690, acc.: 80.47%] [G loss: 4.227458]\n",
      "epoch:6 step:5135 [D loss: 0.717148, acc.: 63.28%] [G loss: 3.099864]\n",
      "epoch:6 step:5136 [D loss: 0.207387, acc.: 91.41%] [G loss: 4.097160]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:6 step:5137 [D loss: 0.063193, acc.: 99.22%] [G loss: 4.172581]\n",
      "epoch:6 step:5138 [D loss: 0.074640, acc.: 98.44%] [G loss: 3.799434]\n",
      "epoch:6 step:5139 [D loss: 0.152331, acc.: 98.44%] [G loss: 3.453467]\n",
      "epoch:6 step:5140 [D loss: 0.091866, acc.: 97.66%] [G loss: 3.274682]\n",
      "epoch:6 step:5141 [D loss: 0.111118, acc.: 96.09%] [G loss: 2.317366]\n",
      "epoch:6 step:5142 [D loss: 0.087450, acc.: 99.22%] [G loss: 1.580740]\n",
      "epoch:6 step:5143 [D loss: 0.118143, acc.: 97.66%] [G loss: 2.306627]\n",
      "epoch:6 step:5144 [D loss: 0.061454, acc.: 100.00%] [G loss: 2.072609]\n",
      "epoch:6 step:5145 [D loss: 0.281371, acc.: 89.84%] [G loss: 2.090710]\n",
      "epoch:6 step:5146 [D loss: 0.201072, acc.: 89.06%] [G loss: 0.677606]\n",
      "epoch:6 step:5147 [D loss: 0.031414, acc.: 100.00%] [G loss: 0.300279]\n",
      "epoch:6 step:5148 [D loss: 0.319117, acc.: 84.38%] [G loss: 3.432816]\n",
      "epoch:6 step:5149 [D loss: 0.442165, acc.: 77.34%] [G loss: 1.216023]\n",
      "epoch:6 step:5150 [D loss: 0.109145, acc.: 95.31%] [G loss: 1.322545]\n",
      "epoch:6 step:5151 [D loss: 0.032490, acc.: 100.00%] [G loss: 1.933753]\n",
      "epoch:6 step:5152 [D loss: 0.066157, acc.: 100.00%] [G loss: 1.923384]\n",
      "epoch:6 step:5153 [D loss: 0.199107, acc.: 92.19%] [G loss: 0.484181]\n",
      "epoch:6 step:5154 [D loss: 0.221913, acc.: 91.41%] [G loss: 2.741694]\n",
      "epoch:6 step:5155 [D loss: 0.043547, acc.: 100.00%] [G loss: 3.981779]\n",
      "epoch:6 step:5156 [D loss: 0.288307, acc.: 88.28%] [G loss: 2.484952]\n",
      "epoch:6 step:5157 [D loss: 0.049211, acc.: 100.00%] [G loss: 1.342500]\n",
      "epoch:6 step:5158 [D loss: 0.362069, acc.: 83.59%] [G loss: 3.844135]\n",
      "epoch:6 step:5159 [D loss: 0.371974, acc.: 79.69%] [G loss: 2.336323]\n",
      "epoch:6 step:5160 [D loss: 0.070910, acc.: 100.00%] [G loss: 2.198749]\n",
      "epoch:6 step:5161 [D loss: 0.431278, acc.: 80.47%] [G loss: 3.649956]\n",
      "epoch:6 step:5162 [D loss: 0.109019, acc.: 98.44%] [G loss: 3.711863]\n",
      "epoch:6 step:5163 [D loss: 0.054776, acc.: 100.00%] [G loss: 2.480609]\n",
      "epoch:6 step:5164 [D loss: 0.085605, acc.: 99.22%] [G loss: 1.409534]\n",
      "epoch:6 step:5165 [D loss: 0.097658, acc.: 98.44%] [G loss: 2.075136]\n",
      "epoch:6 step:5166 [D loss: 0.338989, acc.: 82.81%] [G loss: 4.697072]\n",
      "epoch:6 step:5167 [D loss: 0.573051, acc.: 74.22%] [G loss: 4.769644]\n",
      "epoch:6 step:5168 [D loss: 0.629749, acc.: 64.84%] [G loss: 6.240149]\n",
      "epoch:6 step:5169 [D loss: 0.218583, acc.: 88.28%] [G loss: 6.021401]\n",
      "epoch:6 step:5170 [D loss: 0.052095, acc.: 100.00%] [G loss: 3.861055]\n",
      "epoch:6 step:5171 [D loss: 0.035691, acc.: 100.00%] [G loss: 3.530219]\n",
      "epoch:6 step:5172 [D loss: 0.039766, acc.: 100.00%] [G loss: 2.682431]\n",
      "epoch:6 step:5173 [D loss: 0.021048, acc.: 100.00%] [G loss: 2.582067]\n",
      "epoch:6 step:5174 [D loss: 0.073002, acc.: 100.00%] [G loss: 1.469519]\n",
      "epoch:6 step:5175 [D loss: 0.073013, acc.: 100.00%] [G loss: 1.679362]\n",
      "epoch:6 step:5176 [D loss: 0.042356, acc.: 100.00%] [G loss: 1.927220]\n",
      "epoch:6 step:5177 [D loss: 0.495444, acc.: 75.00%] [G loss: 6.241831]\n",
      "epoch:6 step:5178 [D loss: 1.091147, acc.: 54.69%] [G loss: 1.201953]\n",
      "epoch:6 step:5179 [D loss: 0.337302, acc.: 78.91%] [G loss: 4.236495]\n",
      "epoch:6 step:5180 [D loss: 0.188915, acc.: 91.41%] [G loss: 2.859282]\n",
      "epoch:6 step:5181 [D loss: 0.084349, acc.: 96.88%] [G loss: 1.420917]\n",
      "epoch:6 step:5182 [D loss: 0.240601, acc.: 89.84%] [G loss: 4.343440]\n",
      "epoch:6 step:5183 [D loss: 0.083821, acc.: 98.44%] [G loss: 4.041604]\n",
      "epoch:6 step:5184 [D loss: 0.190725, acc.: 92.19%] [G loss: 2.290412]\n",
      "epoch:6 step:5185 [D loss: 0.050540, acc.: 98.44%] [G loss: 1.416634]\n",
      "epoch:6 step:5186 [D loss: 0.312323, acc.: 86.72%] [G loss: 4.705225]\n",
      "epoch:6 step:5187 [D loss: 0.498145, acc.: 75.00%] [G loss: 3.073824]\n",
      "epoch:6 step:5188 [D loss: 0.093511, acc.: 97.66%] [G loss: 3.640058]\n",
      "epoch:6 step:5189 [D loss: 0.104602, acc.: 98.44%] [G loss: 3.773981]\n",
      "epoch:6 step:5190 [D loss: 0.107221, acc.: 98.44%] [G loss: 2.480062]\n",
      "epoch:6 step:5191 [D loss: 0.500529, acc.: 75.00%] [G loss: 6.702756]\n",
      "epoch:6 step:5192 [D loss: 1.058355, acc.: 57.81%] [G loss: 3.417968]\n",
      "epoch:6 step:5193 [D loss: 0.204173, acc.: 92.19%] [G loss: 4.454934]\n",
      "epoch:6 step:5194 [D loss: 0.065487, acc.: 98.44%] [G loss: 5.180098]\n",
      "epoch:6 step:5195 [D loss: 0.144329, acc.: 95.31%] [G loss: 4.032066]\n",
      "epoch:6 step:5196 [D loss: 0.139105, acc.: 96.09%] [G loss: 3.187282]\n",
      "epoch:6 step:5197 [D loss: 0.089685, acc.: 98.44%] [G loss: 2.604792]\n",
      "epoch:6 step:5198 [D loss: 0.137407, acc.: 94.53%] [G loss: 3.671982]\n",
      "epoch:6 step:5199 [D loss: 0.313266, acc.: 86.72%] [G loss: 2.668141]\n",
      "epoch:6 step:5200 [D loss: 0.078444, acc.: 98.44%] [G loss: 2.951243]\n",
      "##############\n",
      "[0.89766428 0.97369412 0.98563731 0.97724787 1.12120136 1.11960873\n",
      " 1.0741595  2.10856673 2.10617245 0.94401378]\n",
      "##########\n",
      "epoch:6 step:5201 [D loss: 0.164193, acc.: 95.31%] [G loss: 3.314347]\n",
      "epoch:6 step:5202 [D loss: 0.274472, acc.: 89.06%] [G loss: 2.994056]\n",
      "epoch:6 step:5203 [D loss: 0.049488, acc.: 100.00%] [G loss: 0.913218]\n",
      "epoch:6 step:5204 [D loss: 0.259729, acc.: 88.28%] [G loss: 3.382427]\n",
      "epoch:6 step:5205 [D loss: 0.202919, acc.: 92.19%] [G loss: 2.140029]\n",
      "epoch:6 step:5206 [D loss: 0.392739, acc.: 77.34%] [G loss: 3.175591]\n",
      "epoch:6 step:5207 [D loss: 0.034172, acc.: 99.22%] [G loss: 4.088187]\n",
      "epoch:6 step:5208 [D loss: 0.572013, acc.: 68.75%] [G loss: 1.500290]\n",
      "epoch:6 step:5209 [D loss: 0.086387, acc.: 99.22%] [G loss: 2.889039]\n",
      "epoch:6 step:5210 [D loss: 0.129571, acc.: 96.09%] [G loss: 2.374596]\n",
      "epoch:6 step:5211 [D loss: 0.162795, acc.: 94.53%] [G loss: 2.999502]\n",
      "epoch:6 step:5212 [D loss: 1.372022, acc.: 28.91%] [G loss: 5.938251]\n",
      "epoch:6 step:5213 [D loss: 0.176405, acc.: 92.97%] [G loss: 5.561388]\n",
      "epoch:6 step:5214 [D loss: 0.179623, acc.: 91.41%] [G loss: 2.292040]\n",
      "epoch:6 step:5215 [D loss: 0.671100, acc.: 68.75%] [G loss: 5.903288]\n",
      "epoch:6 step:5216 [D loss: 0.277822, acc.: 89.06%] [G loss: 5.367301]\n",
      "epoch:6 step:5217 [D loss: 0.283143, acc.: 88.28%] [G loss: 4.415872]\n",
      "epoch:6 step:5218 [D loss: 0.383327, acc.: 80.47%] [G loss: 3.381341]\n",
      "epoch:6 step:5219 [D loss: 0.391956, acc.: 80.47%] [G loss: 4.926717]\n",
      "epoch:6 step:5220 [D loss: 0.248574, acc.: 89.06%] [G loss: 3.281523]\n",
      "epoch:6 step:5221 [D loss: 0.159521, acc.: 95.31%] [G loss: 3.336469]\n",
      "epoch:6 step:5222 [D loss: 0.133141, acc.: 96.09%] [G loss: 4.802068]\n",
      "epoch:6 step:5223 [D loss: 0.656252, acc.: 63.28%] [G loss: 4.653483]\n",
      "epoch:6 step:5224 [D loss: 0.082178, acc.: 97.66%] [G loss: 4.039286]\n",
      "epoch:6 step:5225 [D loss: 0.255237, acc.: 85.94%] [G loss: 0.325227]\n",
      "epoch:6 step:5226 [D loss: 0.525294, acc.: 73.44%] [G loss: 4.129245]\n",
      "epoch:6 step:5227 [D loss: 0.051014, acc.: 99.22%] [G loss: 6.043460]\n",
      "epoch:6 step:5228 [D loss: 0.583925, acc.: 73.44%] [G loss: 0.701510]\n",
      "epoch:6 step:5229 [D loss: 0.495090, acc.: 74.22%] [G loss: 3.673881]\n",
      "epoch:6 step:5230 [D loss: 0.100461, acc.: 97.66%] [G loss: 3.387612]\n",
      "epoch:6 step:5231 [D loss: 0.074883, acc.: 98.44%] [G loss: 1.390249]\n",
      "epoch:6 step:5232 [D loss: 0.522844, acc.: 74.22%] [G loss: 4.471725]\n",
      "epoch:6 step:5233 [D loss: 0.589220, acc.: 68.75%] [G loss: 2.207978]\n",
      "epoch:6 step:5234 [D loss: 0.759709, acc.: 57.03%] [G loss: 2.921043]\n",
      "epoch:6 step:5235 [D loss: 0.071092, acc.: 99.22%] [G loss: 3.404746]\n",
      "epoch:6 step:5236 [D loss: 0.168461, acc.: 94.53%] [G loss: 2.433476]\n",
      "epoch:6 step:5237 [D loss: 0.018258, acc.: 100.00%] [G loss: 2.103797]\n",
      "epoch:6 step:5238 [D loss: 0.916711, acc.: 57.81%] [G loss: 4.691374]\n",
      "epoch:6 step:5239 [D loss: 0.097434, acc.: 98.44%] [G loss: 5.369528]\n",
      "epoch:6 step:5240 [D loss: 0.571970, acc.: 71.88%] [G loss: 2.809214]\n",
      "epoch:6 step:5241 [D loss: 0.151326, acc.: 94.53%] [G loss: 3.123479]\n",
      "epoch:6 step:5242 [D loss: 0.094959, acc.: 98.44%] [G loss: 3.282469]\n",
      "epoch:6 step:5243 [D loss: 0.190160, acc.: 94.53%] [G loss: 3.297879]\n",
      "epoch:6 step:5244 [D loss: 0.234043, acc.: 92.19%] [G loss: 3.468175]\n",
      "epoch:6 step:5245 [D loss: 0.062163, acc.: 100.00%] [G loss: 3.032489]\n",
      "epoch:6 step:5246 [D loss: 0.061428, acc.: 100.00%] [G loss: 2.431069]\n",
      "epoch:6 step:5247 [D loss: 0.244649, acc.: 93.75%] [G loss: 1.243648]\n",
      "epoch:6 step:5248 [D loss: 0.141413, acc.: 96.88%] [G loss: 0.749843]\n",
      "epoch:6 step:5249 [D loss: 0.049126, acc.: 100.00%] [G loss: 0.570482]\n",
      "epoch:6 step:5250 [D loss: 0.054904, acc.: 100.00%] [G loss: 0.404693]\n",
      "epoch:6 step:5251 [D loss: 0.238398, acc.: 88.28%] [G loss: 1.746691]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:6 step:5252 [D loss: 0.286413, acc.: 87.50%] [G loss: 0.525472]\n",
      "epoch:6 step:5253 [D loss: 0.369943, acc.: 84.38%] [G loss: 2.054419]\n",
      "epoch:6 step:5254 [D loss: 0.029251, acc.: 100.00%] [G loss: 3.144242]\n",
      "epoch:6 step:5255 [D loss: 0.025782, acc.: 100.00%] [G loss: 2.595816]\n",
      "epoch:6 step:5256 [D loss: 0.125679, acc.: 94.53%] [G loss: 0.552075]\n",
      "epoch:6 step:5257 [D loss: 0.470376, acc.: 75.00%] [G loss: 2.983325]\n",
      "epoch:6 step:5258 [D loss: 0.191585, acc.: 92.19%] [G loss: 3.325365]\n",
      "epoch:6 step:5259 [D loss: 0.391685, acc.: 82.03%] [G loss: 1.636579]\n",
      "epoch:6 step:5260 [D loss: 0.036782, acc.: 100.00%] [G loss: 1.020272]\n",
      "epoch:6 step:5261 [D loss: 0.514121, acc.: 69.53%] [G loss: 4.326166]\n",
      "epoch:6 step:5262 [D loss: 0.339629, acc.: 82.03%] [G loss: 3.873660]\n",
      "epoch:6 step:5263 [D loss: 0.215178, acc.: 93.75%] [G loss: 2.744510]\n",
      "epoch:6 step:5264 [D loss: 0.439060, acc.: 75.78%] [G loss: 4.558523]\n",
      "epoch:6 step:5265 [D loss: 0.151107, acc.: 94.53%] [G loss: 4.964206]\n",
      "epoch:6 step:5266 [D loss: 0.372227, acc.: 82.03%] [G loss: 2.409369]\n",
      "epoch:6 step:5267 [D loss: 0.348777, acc.: 81.25%] [G loss: 4.261780]\n",
      "epoch:6 step:5268 [D loss: 0.072368, acc.: 98.44%] [G loss: 4.493001]\n",
      "epoch:6 step:5269 [D loss: 0.320920, acc.: 88.28%] [G loss: 2.862616]\n",
      "epoch:6 step:5270 [D loss: 0.037306, acc.: 100.00%] [G loss: 2.680537]\n",
      "epoch:6 step:5271 [D loss: 0.062732, acc.: 99.22%] [G loss: 1.196056]\n",
      "epoch:6 step:5272 [D loss: 0.084409, acc.: 99.22%] [G loss: 1.319525]\n",
      "epoch:6 step:5273 [D loss: 0.041949, acc.: 100.00%] [G loss: 1.027052]\n",
      "epoch:6 step:5274 [D loss: 0.080809, acc.: 99.22%] [G loss: 1.498338]\n",
      "epoch:6 step:5275 [D loss: 0.034049, acc.: 100.00%] [G loss: 0.530230]\n",
      "epoch:6 step:5276 [D loss: 0.191225, acc.: 92.97%] [G loss: 0.843165]\n",
      "epoch:6 step:5277 [D loss: 0.048308, acc.: 99.22%] [G loss: 0.781085]\n",
      "epoch:6 step:5278 [D loss: 0.098659, acc.: 99.22%] [G loss: 0.936144]\n",
      "epoch:6 step:5279 [D loss: 0.078213, acc.: 99.22%] [G loss: 0.580523]\n",
      "epoch:6 step:5280 [D loss: 0.083874, acc.: 98.44%] [G loss: 0.290588]\n",
      "epoch:6 step:5281 [D loss: 0.081250, acc.: 98.44%] [G loss: 0.601175]\n",
      "epoch:6 step:5282 [D loss: 0.208145, acc.: 91.41%] [G loss: 0.118558]\n",
      "epoch:6 step:5283 [D loss: 0.660354, acc.: 65.62%] [G loss: 7.158714]\n",
      "epoch:6 step:5284 [D loss: 1.371437, acc.: 51.56%] [G loss: 4.314431]\n",
      "epoch:6 step:5285 [D loss: 0.199408, acc.: 92.19%] [G loss: 3.420180]\n",
      "epoch:6 step:5286 [D loss: 0.051151, acc.: 100.00%] [G loss: 3.565966]\n",
      "epoch:6 step:5287 [D loss: 0.087453, acc.: 99.22%] [G loss: 3.256790]\n",
      "epoch:6 step:5288 [D loss: 0.057668, acc.: 99.22%] [G loss: 3.601140]\n",
      "epoch:6 step:5289 [D loss: 0.251182, acc.: 94.53%] [G loss: 3.611552]\n",
      "epoch:6 step:5290 [D loss: 0.138693, acc.: 94.53%] [G loss: 3.088459]\n",
      "epoch:6 step:5291 [D loss: 0.120927, acc.: 97.66%] [G loss: 3.549375]\n",
      "epoch:6 step:5292 [D loss: 0.062554, acc.: 99.22%] [G loss: 3.299416]\n",
      "epoch:6 step:5293 [D loss: 0.215093, acc.: 92.19%] [G loss: 1.588985]\n",
      "epoch:6 step:5294 [D loss: 0.105333, acc.: 98.44%] [G loss: 2.134036]\n",
      "epoch:6 step:5295 [D loss: 0.098004, acc.: 98.44%] [G loss: 1.793906]\n",
      "epoch:6 step:5296 [D loss: 0.141265, acc.: 98.44%] [G loss: 0.756440]\n",
      "epoch:6 step:5297 [D loss: 0.039363, acc.: 100.00%] [G loss: 0.887949]\n",
      "epoch:6 step:5298 [D loss: 0.070346, acc.: 99.22%] [G loss: 0.604883]\n",
      "epoch:6 step:5299 [D loss: 1.481274, acc.: 46.88%] [G loss: 7.129967]\n",
      "epoch:6 step:5300 [D loss: 0.987805, acc.: 57.03%] [G loss: 4.906575]\n",
      "epoch:6 step:5301 [D loss: 0.075642, acc.: 98.44%] [G loss: 2.994868]\n",
      "epoch:6 step:5302 [D loss: 0.098604, acc.: 96.09%] [G loss: 2.358644]\n",
      "epoch:6 step:5303 [D loss: 0.156762, acc.: 95.31%] [G loss: 3.977078]\n",
      "epoch:6 step:5304 [D loss: 0.322419, acc.: 81.25%] [G loss: 1.639680]\n",
      "epoch:6 step:5305 [D loss: 0.787913, acc.: 63.28%] [G loss: 5.300775]\n",
      "epoch:6 step:5306 [D loss: 0.649116, acc.: 65.62%] [G loss: 4.048974]\n",
      "epoch:6 step:5307 [D loss: 0.178805, acc.: 92.19%] [G loss: 2.439370]\n",
      "epoch:6 step:5308 [D loss: 0.276063, acc.: 87.50%] [G loss: 3.195649]\n",
      "epoch:6 step:5309 [D loss: 0.214860, acc.: 89.84%] [G loss: 2.640121]\n",
      "epoch:6 step:5310 [D loss: 0.131338, acc.: 96.88%] [G loss: 2.551163]\n",
      "epoch:6 step:5311 [D loss: 0.151639, acc.: 95.31%] [G loss: 2.739118]\n",
      "epoch:6 step:5312 [D loss: 0.199744, acc.: 96.09%] [G loss: 2.615770]\n",
      "epoch:6 step:5313 [D loss: 0.177844, acc.: 93.75%] [G loss: 3.239904]\n",
      "epoch:6 step:5314 [D loss: 0.576981, acc.: 66.41%] [G loss: 1.239588]\n",
      "epoch:6 step:5315 [D loss: 0.316876, acc.: 82.03%] [G loss: 3.582244]\n",
      "epoch:6 step:5316 [D loss: 0.111104, acc.: 97.66%] [G loss: 3.581564]\n",
      "epoch:6 step:5317 [D loss: 0.233197, acc.: 91.41%] [G loss: 3.230536]\n",
      "epoch:6 step:5318 [D loss: 0.134408, acc.: 96.09%] [G loss: 2.970898]\n",
      "epoch:6 step:5319 [D loss: 0.101465, acc.: 97.66%] [G loss: 3.014565]\n",
      "epoch:6 step:5320 [D loss: 0.069522, acc.: 99.22%] [G loss: 2.787014]\n",
      "epoch:6 step:5321 [D loss: 0.133665, acc.: 96.88%] [G loss: 2.981255]\n",
      "epoch:6 step:5322 [D loss: 0.243395, acc.: 91.41%] [G loss: 3.904820]\n",
      "epoch:6 step:5323 [D loss: 0.497816, acc.: 78.12%] [G loss: 2.914651]\n",
      "epoch:6 step:5324 [D loss: 0.097139, acc.: 97.66%] [G loss: 3.732359]\n",
      "epoch:6 step:5325 [D loss: 0.093169, acc.: 99.22%] [G loss: 4.119364]\n",
      "epoch:6 step:5326 [D loss: 0.053944, acc.: 100.00%] [G loss: 3.286290]\n",
      "epoch:6 step:5327 [D loss: 0.089035, acc.: 98.44%] [G loss: 3.689742]\n",
      "epoch:6 step:5328 [D loss: 0.139441, acc.: 96.88%] [G loss: 2.596076]\n",
      "epoch:6 step:5329 [D loss: 0.168103, acc.: 94.53%] [G loss: 3.852165]\n",
      "epoch:6 step:5330 [D loss: 0.284703, acc.: 87.50%] [G loss: 2.273994]\n",
      "epoch:6 step:5331 [D loss: 0.238627, acc.: 91.41%] [G loss: 3.366240]\n",
      "epoch:6 step:5332 [D loss: 0.146143, acc.: 96.09%] [G loss: 3.126177]\n",
      "epoch:6 step:5333 [D loss: 0.426831, acc.: 82.81%] [G loss: 5.211070]\n",
      "epoch:6 step:5334 [D loss: 0.626865, acc.: 65.62%] [G loss: 3.066502]\n",
      "epoch:6 step:5335 [D loss: 0.418204, acc.: 78.91%] [G loss: 5.446131]\n",
      "epoch:6 step:5336 [D loss: 0.361309, acc.: 83.59%] [G loss: 3.581395]\n",
      "epoch:6 step:5337 [D loss: 0.089006, acc.: 99.22%] [G loss: 2.953914]\n",
      "epoch:6 step:5338 [D loss: 0.216195, acc.: 92.97%] [G loss: 4.217688]\n",
      "epoch:6 step:5339 [D loss: 0.065131, acc.: 99.22%] [G loss: 3.583517]\n",
      "epoch:6 step:5340 [D loss: 0.178961, acc.: 95.31%] [G loss: 2.123522]\n",
      "epoch:6 step:5341 [D loss: 0.270505, acc.: 88.28%] [G loss: 4.145538]\n",
      "epoch:6 step:5342 [D loss: 0.671337, acc.: 64.84%] [G loss: 2.567104]\n",
      "epoch:6 step:5343 [D loss: 0.254970, acc.: 88.28%] [G loss: 4.929290]\n",
      "epoch:6 step:5344 [D loss: 0.178586, acc.: 94.53%] [G loss: 3.550846]\n",
      "epoch:6 step:5345 [D loss: 0.116550, acc.: 95.31%] [G loss: 2.218641]\n",
      "epoch:6 step:5346 [D loss: 0.370876, acc.: 82.03%] [G loss: 5.361267]\n",
      "epoch:6 step:5347 [D loss: 0.520029, acc.: 71.09%] [G loss: 2.511021]\n",
      "epoch:6 step:5348 [D loss: 0.279256, acc.: 92.19%] [G loss: 4.185371]\n",
      "epoch:6 step:5349 [D loss: 0.118792, acc.: 96.09%] [G loss: 4.189889]\n",
      "epoch:6 step:5350 [D loss: 0.251046, acc.: 90.62%] [G loss: 2.705782]\n",
      "epoch:6 step:5351 [D loss: 0.435552, acc.: 83.59%] [G loss: 4.417815]\n",
      "epoch:6 step:5352 [D loss: 0.105358, acc.: 96.88%] [G loss: 4.920043]\n",
      "epoch:6 step:5353 [D loss: 0.198574, acc.: 95.31%] [G loss: 3.676460]\n",
      "epoch:6 step:5354 [D loss: 0.162076, acc.: 96.88%] [G loss: 2.329704]\n",
      "epoch:6 step:5355 [D loss: 0.248745, acc.: 90.62%] [G loss: 4.551437]\n",
      "epoch:6 step:5356 [D loss: 0.288796, acc.: 89.84%] [G loss: 2.904311]\n",
      "epoch:6 step:5357 [D loss: 0.227642, acc.: 89.84%] [G loss: 5.337255]\n",
      "epoch:6 step:5358 [D loss: 0.093894, acc.: 99.22%] [G loss: 4.951138]\n",
      "epoch:6 step:5359 [D loss: 0.124346, acc.: 96.09%] [G loss: 2.553488]\n",
      "epoch:6 step:5360 [D loss: 0.321910, acc.: 84.38%] [G loss: 5.621576]\n",
      "epoch:6 step:5361 [D loss: 0.104280, acc.: 98.44%] [G loss: 5.895935]\n",
      "epoch:6 step:5362 [D loss: 0.178181, acc.: 94.53%] [G loss: 3.271972]\n",
      "epoch:6 step:5363 [D loss: 0.122641, acc.: 96.09%] [G loss: 2.577322]\n",
      "epoch:6 step:5364 [D loss: 0.131113, acc.: 95.31%] [G loss: 3.737240]\n",
      "epoch:6 step:5365 [D loss: 0.139162, acc.: 96.88%] [G loss: 3.211330]\n",
      "epoch:6 step:5366 [D loss: 0.111678, acc.: 98.44%] [G loss: 3.294256]\n",
      "epoch:6 step:5367 [D loss: 0.563020, acc.: 71.09%] [G loss: 6.057938]\n",
      "epoch:6 step:5368 [D loss: 0.598780, acc.: 71.88%] [G loss: 1.311768]\n",
      "epoch:6 step:5369 [D loss: 0.287936, acc.: 85.16%] [G loss: 5.977167]\n",
      "epoch:6 step:5370 [D loss: 0.183267, acc.: 90.62%] [G loss: 4.069468]\n",
      "epoch:6 step:5371 [D loss: 0.318870, acc.: 87.50%] [G loss: 1.114913]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:6 step:5372 [D loss: 0.267370, acc.: 85.94%] [G loss: 4.530062]\n",
      "epoch:6 step:5373 [D loss: 0.121915, acc.: 96.88%] [G loss: 4.152359]\n",
      "epoch:6 step:5374 [D loss: 0.683787, acc.: 64.06%] [G loss: 3.031425]\n",
      "epoch:6 step:5375 [D loss: 0.089360, acc.: 98.44%] [G loss: 3.855504]\n",
      "epoch:6 step:5376 [D loss: 0.043655, acc.: 100.00%] [G loss: 2.299556]\n",
      "epoch:6 step:5377 [D loss: 0.287661, acc.: 87.50%] [G loss: 4.349060]\n",
      "epoch:6 step:5378 [D loss: 0.687189, acc.: 63.28%] [G loss: 5.040639]\n",
      "epoch:6 step:5379 [D loss: 0.266059, acc.: 85.16%] [G loss: 2.819276]\n",
      "epoch:6 step:5380 [D loss: 0.226710, acc.: 87.50%] [G loss: 4.899113]\n",
      "epoch:6 step:5381 [D loss: 0.282822, acc.: 85.94%] [G loss: 2.474493]\n",
      "epoch:6 step:5382 [D loss: 0.906228, acc.: 64.84%] [G loss: 7.919074]\n",
      "epoch:6 step:5383 [D loss: 1.146455, acc.: 53.12%] [G loss: 4.164005]\n",
      "epoch:6 step:5384 [D loss: 0.177215, acc.: 92.97%] [G loss: 4.161581]\n",
      "epoch:6 step:5385 [D loss: 0.127579, acc.: 93.75%] [G loss: 2.682765]\n",
      "epoch:6 step:5386 [D loss: 0.058767, acc.: 98.44%] [G loss: 2.681070]\n",
      "epoch:6 step:5387 [D loss: 0.136159, acc.: 96.88%] [G loss: 2.914086]\n",
      "epoch:6 step:5388 [D loss: 0.101345, acc.: 97.66%] [G loss: 2.731692]\n",
      "epoch:6 step:5389 [D loss: 0.237035, acc.: 91.41%] [G loss: 2.296577]\n",
      "epoch:6 step:5390 [D loss: 0.258931, acc.: 87.50%] [G loss: 0.358621]\n",
      "epoch:6 step:5391 [D loss: 0.336541, acc.: 80.47%] [G loss: 3.664802]\n",
      "epoch:6 step:5392 [D loss: 0.036275, acc.: 100.00%] [G loss: 4.700148]\n",
      "epoch:6 step:5393 [D loss: 0.646681, acc.: 70.31%] [G loss: 0.215389]\n",
      "epoch:6 step:5394 [D loss: 0.135983, acc.: 96.88%] [G loss: 0.366140]\n",
      "epoch:6 step:5395 [D loss: 0.157541, acc.: 93.75%] [G loss: 1.694550]\n",
      "epoch:6 step:5396 [D loss: 0.028686, acc.: 100.00%] [G loss: 2.274951]\n",
      "epoch:6 step:5397 [D loss: 0.045541, acc.: 100.00%] [G loss: 1.405927]\n",
      "epoch:6 step:5398 [D loss: 0.156204, acc.: 96.88%] [G loss: 1.355352]\n",
      "epoch:6 step:5399 [D loss: 0.353355, acc.: 82.81%] [G loss: 0.101781]\n",
      "epoch:6 step:5400 [D loss: 0.081008, acc.: 97.66%] [G loss: 1.375617]\n",
      "##############\n",
      "[0.94386834 0.87885213 0.88235906 0.96539576 2.1207919  0.97434095\n",
      " 0.77726215 0.95943561 0.82198284 0.86745193]\n",
      "##########\n",
      "epoch:6 step:5401 [D loss: 0.275207, acc.: 91.41%] [G loss: 1.064390]\n",
      "epoch:6 step:5402 [D loss: 0.171672, acc.: 95.31%] [G loss: 3.670914]\n",
      "epoch:6 step:5403 [D loss: 0.170745, acc.: 92.97%] [G loss: 2.706423]\n",
      "epoch:6 step:5404 [D loss: 0.038628, acc.: 99.22%] [G loss: 2.782776]\n",
      "epoch:6 step:5405 [D loss: 0.187839, acc.: 92.19%] [G loss: 4.437494]\n",
      "epoch:6 step:5406 [D loss: 0.525980, acc.: 75.78%] [G loss: 4.641925]\n",
      "epoch:6 step:5407 [D loss: 0.184593, acc.: 93.75%] [G loss: 3.272459]\n",
      "epoch:6 step:5408 [D loss: 0.144287, acc.: 95.31%] [G loss: 3.403112]\n",
      "epoch:6 step:5409 [D loss: 0.152191, acc.: 94.53%] [G loss: 4.431238]\n",
      "epoch:6 step:5410 [D loss: 0.245332, acc.: 89.84%] [G loss: 3.376767]\n",
      "epoch:6 step:5411 [D loss: 0.413823, acc.: 85.94%] [G loss: 3.625890]\n",
      "epoch:6 step:5412 [D loss: 0.026175, acc.: 100.00%] [G loss: 3.656708]\n",
      "epoch:6 step:5413 [D loss: 0.360740, acc.: 85.16%] [G loss: 3.606826]\n",
      "epoch:6 step:5414 [D loss: 0.165618, acc.: 94.53%] [G loss: 2.727846]\n",
      "epoch:6 step:5415 [D loss: 0.160130, acc.: 96.09%] [G loss: 3.662426]\n",
      "epoch:6 step:5416 [D loss: 0.075833, acc.: 97.66%] [G loss: 3.020365]\n",
      "epoch:6 step:5417 [D loss: 0.444469, acc.: 78.12%] [G loss: 4.080654]\n",
      "epoch:6 step:5418 [D loss: 0.132719, acc.: 96.88%] [G loss: 4.571219]\n",
      "epoch:6 step:5419 [D loss: 0.444453, acc.: 78.12%] [G loss: 3.144690]\n",
      "epoch:6 step:5420 [D loss: 0.072764, acc.: 98.44%] [G loss: 4.395463]\n",
      "epoch:6 step:5421 [D loss: 0.209469, acc.: 93.75%] [G loss: 2.042227]\n",
      "epoch:6 step:5422 [D loss: 0.289929, acc.: 87.50%] [G loss: 4.520921]\n",
      "epoch:6 step:5423 [D loss: 0.157086, acc.: 93.75%] [G loss: 3.885459]\n",
      "epoch:6 step:5424 [D loss: 0.192457, acc.: 93.75%] [G loss: 3.837035]\n",
      "epoch:6 step:5425 [D loss: 0.114966, acc.: 98.44%] [G loss: 4.130478]\n",
      "epoch:6 step:5426 [D loss: 0.185334, acc.: 94.53%] [G loss: 3.430631]\n",
      "epoch:6 step:5427 [D loss: 0.210094, acc.: 92.19%] [G loss: 4.281183]\n",
      "epoch:6 step:5428 [D loss: 0.793963, acc.: 54.69%] [G loss: 5.105582]\n",
      "epoch:6 step:5429 [D loss: 0.235072, acc.: 89.84%] [G loss: 3.514845]\n",
      "epoch:6 step:5430 [D loss: 0.167435, acc.: 92.19%] [G loss: 5.170878]\n",
      "epoch:6 step:5431 [D loss: 0.106876, acc.: 96.88%] [G loss: 3.675031]\n",
      "epoch:6 step:5432 [D loss: 0.333608, acc.: 85.16%] [G loss: 5.681640]\n",
      "epoch:6 step:5433 [D loss: 0.322411, acc.: 82.81%] [G loss: 5.008339]\n",
      "epoch:6 step:5434 [D loss: 0.061933, acc.: 100.00%] [G loss: 4.933059]\n",
      "epoch:6 step:5435 [D loss: 0.125805, acc.: 96.09%] [G loss: 2.322842]\n",
      "epoch:6 step:5436 [D loss: 0.471145, acc.: 79.69%] [G loss: 6.714245]\n",
      "epoch:6 step:5437 [D loss: 0.647876, acc.: 67.97%] [G loss: 4.060121]\n",
      "epoch:6 step:5438 [D loss: 0.084672, acc.: 99.22%] [G loss: 2.562761]\n",
      "epoch:6 step:5439 [D loss: 0.473441, acc.: 75.00%] [G loss: 6.559603]\n",
      "epoch:6 step:5440 [D loss: 1.006516, acc.: 54.69%] [G loss: 3.078771]\n",
      "epoch:6 step:5441 [D loss: 0.252202, acc.: 90.62%] [G loss: 4.732638]\n",
      "epoch:6 step:5442 [D loss: 0.092625, acc.: 96.88%] [G loss: 3.107641]\n",
      "epoch:6 step:5443 [D loss: 0.123084, acc.: 96.88%] [G loss: 3.394402]\n",
      "epoch:6 step:5444 [D loss: 0.221289, acc.: 92.19%] [G loss: 4.010285]\n",
      "epoch:6 step:5445 [D loss: 0.242901, acc.: 90.62%] [G loss: 2.230996]\n",
      "epoch:6 step:5446 [D loss: 0.277772, acc.: 89.06%] [G loss: 2.998483]\n",
      "epoch:6 step:5447 [D loss: 0.076885, acc.: 99.22%] [G loss: 4.198453]\n",
      "epoch:6 step:5448 [D loss: 1.180572, acc.: 35.94%] [G loss: 4.697121]\n",
      "epoch:6 step:5449 [D loss: 0.179250, acc.: 91.41%] [G loss: 5.084915]\n",
      "epoch:6 step:5450 [D loss: 0.199054, acc.: 89.84%] [G loss: 3.719185]\n",
      "epoch:6 step:5451 [D loss: 0.522010, acc.: 75.00%] [G loss: 5.669168]\n",
      "epoch:6 step:5452 [D loss: 0.225624, acc.: 87.50%] [G loss: 5.153921]\n",
      "epoch:6 step:5453 [D loss: 0.222405, acc.: 92.19%] [G loss: 2.249545]\n",
      "epoch:6 step:5454 [D loss: 0.612764, acc.: 70.31%] [G loss: 6.065149]\n",
      "epoch:6 step:5455 [D loss: 0.224751, acc.: 88.28%] [G loss: 4.705381]\n",
      "epoch:6 step:5456 [D loss: 0.263593, acc.: 89.06%] [G loss: 2.507863]\n",
      "epoch:6 step:5457 [D loss: 0.233615, acc.: 90.62%] [G loss: 3.648906]\n",
      "epoch:6 step:5458 [D loss: 0.080886, acc.: 98.44%] [G loss: 4.166732]\n",
      "epoch:6 step:5459 [D loss: 0.076683, acc.: 98.44%] [G loss: 3.912871]\n",
      "epoch:6 step:5460 [D loss: 0.400651, acc.: 82.81%] [G loss: 4.403006]\n",
      "epoch:6 step:5461 [D loss: 0.172313, acc.: 93.75%] [G loss: 3.630636]\n",
      "epoch:6 step:5462 [D loss: 0.197513, acc.: 94.53%] [G loss: 2.547529]\n",
      "epoch:6 step:5463 [D loss: 0.586032, acc.: 73.44%] [G loss: 4.466172]\n",
      "epoch:6 step:5464 [D loss: 0.359200, acc.: 80.47%] [G loss: 4.126064]\n",
      "epoch:6 step:5465 [D loss: 0.444995, acc.: 79.69%] [G loss: 4.450954]\n",
      "epoch:6 step:5466 [D loss: 0.128780, acc.: 96.09%] [G loss: 3.878646]\n",
      "epoch:6 step:5467 [D loss: 0.039241, acc.: 98.44%] [G loss: 3.693807]\n",
      "epoch:7 step:5468 [D loss: 0.059181, acc.: 99.22%] [G loss: 3.641679]\n",
      "epoch:7 step:5469 [D loss: 0.075326, acc.: 99.22%] [G loss: 3.691833]\n",
      "epoch:7 step:5470 [D loss: 0.135252, acc.: 94.53%] [G loss: 3.524064]\n",
      "epoch:7 step:5471 [D loss: 0.132145, acc.: 96.88%] [G loss: 3.263715]\n",
      "epoch:7 step:5472 [D loss: 0.174684, acc.: 93.75%] [G loss: 3.552351]\n",
      "epoch:7 step:5473 [D loss: 0.169443, acc.: 97.66%] [G loss: 5.007598]\n",
      "epoch:7 step:5474 [D loss: 0.368235, acc.: 80.47%] [G loss: 3.142479]\n",
      "epoch:7 step:5475 [D loss: 0.057185, acc.: 100.00%] [G loss: 4.181120]\n",
      "epoch:7 step:5476 [D loss: 0.070902, acc.: 98.44%] [G loss: 2.766870]\n",
      "epoch:7 step:5477 [D loss: 0.532278, acc.: 71.88%] [G loss: 5.997750]\n",
      "epoch:7 step:5478 [D loss: 0.390508, acc.: 76.56%] [G loss: 4.168113]\n",
      "epoch:7 step:5479 [D loss: 0.121769, acc.: 97.66%] [G loss: 3.634249]\n",
      "epoch:7 step:5480 [D loss: 0.081186, acc.: 99.22%] [G loss: 3.295765]\n",
      "epoch:7 step:5481 [D loss: 0.164155, acc.: 96.09%] [G loss: 3.720149]\n",
      "epoch:7 step:5482 [D loss: 0.140034, acc.: 97.66%] [G loss: 3.587621]\n",
      "epoch:7 step:5483 [D loss: 0.300925, acc.: 91.41%] [G loss: 3.778225]\n",
      "epoch:7 step:5484 [D loss: 0.098020, acc.: 96.88%] [G loss: 3.582051]\n",
      "epoch:7 step:5485 [D loss: 0.155351, acc.: 94.53%] [G loss: 4.257302]\n",
      "epoch:7 step:5486 [D loss: 0.162367, acc.: 96.88%] [G loss: 3.522635]\n",
      "epoch:7 step:5487 [D loss: 0.165885, acc.: 95.31%] [G loss: 3.984902]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:7 step:5488 [D loss: 0.066386, acc.: 99.22%] [G loss: 4.086614]\n",
      "epoch:7 step:5489 [D loss: 0.380495, acc.: 82.81%] [G loss: 2.888348]\n",
      "epoch:7 step:5490 [D loss: 0.269383, acc.: 92.19%] [G loss: 3.373698]\n",
      "epoch:7 step:5491 [D loss: 0.040745, acc.: 100.00%] [G loss: 4.926092]\n",
      "epoch:7 step:5492 [D loss: 0.115724, acc.: 96.09%] [G loss: 2.197992]\n",
      "epoch:7 step:5493 [D loss: 0.363411, acc.: 82.81%] [G loss: 6.117382]\n",
      "epoch:7 step:5494 [D loss: 0.251633, acc.: 90.62%] [G loss: 5.489966]\n",
      "epoch:7 step:5495 [D loss: 0.398619, acc.: 82.03%] [G loss: 4.545584]\n",
      "epoch:7 step:5496 [D loss: 0.040169, acc.: 99.22%] [G loss: 4.217130]\n",
      "epoch:7 step:5497 [D loss: 0.130889, acc.: 94.53%] [G loss: 4.218842]\n",
      "epoch:7 step:5498 [D loss: 0.447367, acc.: 78.12%] [G loss: 4.397596]\n",
      "epoch:7 step:5499 [D loss: 0.237706, acc.: 89.84%] [G loss: 2.714595]\n",
      "epoch:7 step:5500 [D loss: 0.236452, acc.: 89.06%] [G loss: 4.818634]\n",
      "epoch:7 step:5501 [D loss: 0.066119, acc.: 99.22%] [G loss: 4.548692]\n",
      "epoch:7 step:5502 [D loss: 0.218636, acc.: 90.62%] [G loss: 1.029971]\n",
      "epoch:7 step:5503 [D loss: 0.034169, acc.: 100.00%] [G loss: 1.196382]\n",
      "epoch:7 step:5504 [D loss: 0.043799, acc.: 100.00%] [G loss: 1.727049]\n",
      "epoch:7 step:5505 [D loss: 0.161775, acc.: 93.75%] [G loss: 2.134182]\n",
      "epoch:7 step:5506 [D loss: 0.194890, acc.: 89.84%] [G loss: 0.452069]\n",
      "epoch:7 step:5507 [D loss: 0.587562, acc.: 70.31%] [G loss: 7.589077]\n",
      "epoch:7 step:5508 [D loss: 1.425774, acc.: 52.34%] [G loss: 3.435197]\n",
      "epoch:7 step:5509 [D loss: 0.323534, acc.: 85.94%] [G loss: 4.951677]\n",
      "epoch:7 step:5510 [D loss: 0.191155, acc.: 90.62%] [G loss: 4.157316]\n",
      "epoch:7 step:5511 [D loss: 0.291543, acc.: 85.94%] [G loss: 5.462912]\n",
      "epoch:7 step:5512 [D loss: 0.341104, acc.: 82.81%] [G loss: 3.916185]\n",
      "epoch:7 step:5513 [D loss: 0.060614, acc.: 100.00%] [G loss: 3.500783]\n",
      "epoch:7 step:5514 [D loss: 0.076381, acc.: 97.66%] [G loss: 3.384445]\n",
      "epoch:7 step:5515 [D loss: 0.093530, acc.: 99.22%] [G loss: 3.734122]\n",
      "epoch:7 step:5516 [D loss: 0.133919, acc.: 96.09%] [G loss: 1.274042]\n",
      "epoch:7 step:5517 [D loss: 0.190862, acc.: 93.75%] [G loss: 2.356104]\n",
      "epoch:7 step:5518 [D loss: 0.018705, acc.: 100.00%] [G loss: 3.225249]\n",
      "epoch:7 step:5519 [D loss: 0.192267, acc.: 93.75%] [G loss: 1.338894]\n",
      "epoch:7 step:5520 [D loss: 0.344213, acc.: 81.25%] [G loss: 5.322057]\n",
      "epoch:7 step:5521 [D loss: 0.896504, acc.: 61.72%] [G loss: 1.182632]\n",
      "epoch:7 step:5522 [D loss: 0.205133, acc.: 90.62%] [G loss: 2.979172]\n",
      "epoch:7 step:5523 [D loss: 0.465195, acc.: 79.69%] [G loss: 0.844231]\n",
      "epoch:7 step:5524 [D loss: 0.023846, acc.: 100.00%] [G loss: 0.233216]\n",
      "epoch:7 step:5525 [D loss: 0.381404, acc.: 78.12%] [G loss: 4.617139]\n",
      "epoch:7 step:5526 [D loss: 0.029766, acc.: 99.22%] [G loss: 6.299719]\n",
      "epoch:7 step:5527 [D loss: 0.632365, acc.: 68.75%] [G loss: 1.328921]\n",
      "epoch:7 step:5528 [D loss: 0.579091, acc.: 71.88%] [G loss: 5.325365]\n",
      "epoch:7 step:5529 [D loss: 0.126452, acc.: 96.09%] [G loss: 6.197353]\n",
      "epoch:7 step:5530 [D loss: 0.152863, acc.: 93.75%] [G loss: 4.119533]\n",
      "epoch:7 step:5531 [D loss: 0.201555, acc.: 92.97%] [G loss: 2.923289]\n",
      "epoch:7 step:5532 [D loss: 0.269270, acc.: 87.50%] [G loss: 3.961522]\n",
      "epoch:7 step:5533 [D loss: 0.043125, acc.: 99.22%] [G loss: 4.537814]\n",
      "epoch:7 step:5534 [D loss: 0.070663, acc.: 99.22%] [G loss: 2.751464]\n",
      "epoch:7 step:5535 [D loss: 0.103756, acc.: 99.22%] [G loss: 2.821723]\n",
      "epoch:7 step:5536 [D loss: 0.159454, acc.: 95.31%] [G loss: 2.608655]\n",
      "epoch:7 step:5537 [D loss: 0.288145, acc.: 89.84%] [G loss: 3.585747]\n",
      "epoch:7 step:5538 [D loss: 0.123120, acc.: 97.66%] [G loss: 2.734393]\n",
      "epoch:7 step:5539 [D loss: 0.666017, acc.: 71.09%] [G loss: 6.042651]\n",
      "epoch:7 step:5540 [D loss: 0.770760, acc.: 63.28%] [G loss: 3.037495]\n",
      "epoch:7 step:5541 [D loss: 0.093878, acc.: 98.44%] [G loss: 2.075156]\n",
      "epoch:7 step:5542 [D loss: 0.076233, acc.: 99.22%] [G loss: 3.365145]\n",
      "epoch:7 step:5543 [D loss: 0.163067, acc.: 96.09%] [G loss: 1.500587]\n",
      "epoch:7 step:5544 [D loss: 0.727405, acc.: 62.50%] [G loss: 6.749171]\n",
      "epoch:7 step:5545 [D loss: 1.481611, acc.: 51.56%] [G loss: 3.147398]\n",
      "epoch:7 step:5546 [D loss: 0.190932, acc.: 95.31%] [G loss: 2.341509]\n",
      "epoch:7 step:5547 [D loss: 0.100051, acc.: 96.88%] [G loss: 3.296496]\n",
      "epoch:7 step:5548 [D loss: 0.181447, acc.: 92.97%] [G loss: 3.233069]\n",
      "epoch:7 step:5549 [D loss: 0.880363, acc.: 53.91%] [G loss: 4.815256]\n",
      "epoch:7 step:5550 [D loss: 0.296191, acc.: 85.94%] [G loss: 4.084578]\n",
      "epoch:7 step:5551 [D loss: 0.133772, acc.: 94.53%] [G loss: 2.478642]\n",
      "epoch:7 step:5552 [D loss: 0.137254, acc.: 96.09%] [G loss: 1.758008]\n",
      "epoch:7 step:5553 [D loss: 0.121338, acc.: 98.44%] [G loss: 2.307137]\n",
      "epoch:7 step:5554 [D loss: 0.075237, acc.: 99.22%] [G loss: 2.514806]\n",
      "epoch:7 step:5555 [D loss: 0.147623, acc.: 95.31%] [G loss: 0.596761]\n",
      "epoch:7 step:5556 [D loss: 0.078106, acc.: 98.44%] [G loss: 0.384208]\n",
      "epoch:7 step:5557 [D loss: 0.529285, acc.: 73.44%] [G loss: 1.415096]\n",
      "epoch:7 step:5558 [D loss: 0.032730, acc.: 100.00%] [G loss: 3.691175]\n",
      "epoch:7 step:5559 [D loss: 0.118829, acc.: 96.09%] [G loss: 1.734072]\n",
      "epoch:7 step:5560 [D loss: 0.128271, acc.: 96.09%] [G loss: 1.859687]\n",
      "epoch:7 step:5561 [D loss: 0.051293, acc.: 100.00%] [G loss: 2.000615]\n",
      "epoch:7 step:5562 [D loss: 0.054309, acc.: 99.22%] [G loss: 2.214159]\n",
      "epoch:7 step:5563 [D loss: 0.101571, acc.: 100.00%] [G loss: 1.451972]\n",
      "epoch:7 step:5564 [D loss: 0.757843, acc.: 60.16%] [G loss: 5.360833]\n",
      "epoch:7 step:5565 [D loss: 0.522850, acc.: 71.88%] [G loss: 4.179643]\n",
      "epoch:7 step:5566 [D loss: 0.176172, acc.: 94.53%] [G loss: 2.529063]\n",
      "epoch:7 step:5567 [D loss: 0.294007, acc.: 87.50%] [G loss: 4.138562]\n",
      "epoch:7 step:5568 [D loss: 0.088440, acc.: 97.66%] [G loss: 4.401669]\n",
      "epoch:7 step:5569 [D loss: 0.210934, acc.: 92.97%] [G loss: 2.373360]\n",
      "epoch:7 step:5570 [D loss: 0.251008, acc.: 91.41%] [G loss: 2.865198]\n",
      "epoch:7 step:5571 [D loss: 0.193330, acc.: 94.53%] [G loss: 2.656645]\n",
      "epoch:7 step:5572 [D loss: 0.140206, acc.: 96.09%] [G loss: 1.475690]\n",
      "epoch:7 step:5573 [D loss: 0.060918, acc.: 98.44%] [G loss: 1.179551]\n",
      "epoch:7 step:5574 [D loss: 0.222468, acc.: 94.53%] [G loss: 2.811945]\n",
      "epoch:7 step:5575 [D loss: 0.589395, acc.: 68.75%] [G loss: 4.880246]\n",
      "epoch:7 step:5576 [D loss: 0.199831, acc.: 89.84%] [G loss: 4.335560]\n",
      "epoch:7 step:5577 [D loss: 0.242332, acc.: 89.06%] [G loss: 0.485474]\n",
      "epoch:7 step:5578 [D loss: 0.391036, acc.: 82.81%] [G loss: 4.698512]\n",
      "epoch:7 step:5579 [D loss: 0.322123, acc.: 82.03%] [G loss: 3.026425]\n",
      "epoch:7 step:5580 [D loss: 0.050816, acc.: 99.22%] [G loss: 2.470250]\n",
      "epoch:7 step:5581 [D loss: 0.092783, acc.: 97.66%] [G loss: 3.455397]\n",
      "epoch:7 step:5582 [D loss: 0.020530, acc.: 100.00%] [G loss: 4.243152]\n",
      "epoch:7 step:5583 [D loss: 0.200709, acc.: 92.97%] [G loss: 3.287252]\n",
      "epoch:7 step:5584 [D loss: 0.371468, acc.: 85.16%] [G loss: 4.835146]\n",
      "epoch:7 step:5585 [D loss: 0.270021, acc.: 87.50%] [G loss: 3.329022]\n",
      "epoch:7 step:5586 [D loss: 0.180776, acc.: 93.75%] [G loss: 4.612337]\n",
      "epoch:7 step:5587 [D loss: 0.043854, acc.: 100.00%] [G loss: 4.768702]\n",
      "epoch:7 step:5588 [D loss: 0.116636, acc.: 97.66%] [G loss: 3.163889]\n",
      "epoch:7 step:5589 [D loss: 0.162006, acc.: 96.09%] [G loss: 3.540936]\n",
      "epoch:7 step:5590 [D loss: 0.049856, acc.: 100.00%] [G loss: 2.849725]\n",
      "epoch:7 step:5591 [D loss: 0.242530, acc.: 92.19%] [G loss: 2.626802]\n",
      "epoch:7 step:5592 [D loss: 0.166553, acc.: 96.09%] [G loss: 1.759614]\n",
      "epoch:7 step:5593 [D loss: 0.379623, acc.: 81.25%] [G loss: 3.600109]\n",
      "epoch:7 step:5594 [D loss: 0.076121, acc.: 98.44%] [G loss: 3.615161]\n",
      "epoch:7 step:5595 [D loss: 0.123023, acc.: 96.88%] [G loss: 2.194636]\n",
      "epoch:7 step:5596 [D loss: 0.085016, acc.: 99.22%] [G loss: 1.308176]\n",
      "epoch:7 step:5597 [D loss: 0.044005, acc.: 98.44%] [G loss: 1.381413]\n",
      "epoch:7 step:5598 [D loss: 0.020753, acc.: 100.00%] [G loss: 1.237799]\n",
      "epoch:7 step:5599 [D loss: 0.281108, acc.: 89.84%] [G loss: 1.510271]\n",
      "epoch:7 step:5600 [D loss: 0.028373, acc.: 100.00%] [G loss: 2.530184]\n",
      "##############\n",
      "[0.82977452 0.92531125 1.10661745 0.91163454 0.93426034 0.84625372\n",
      " 0.9836106  1.12273212 1.10998458 1.10831995]\n",
      "##########\n",
      "epoch:7 step:5601 [D loss: 0.073582, acc.: 97.66%] [G loss: 0.683250]\n",
      "epoch:7 step:5602 [D loss: 0.124564, acc.: 97.66%] [G loss: 1.523125]\n",
      "epoch:7 step:5603 [D loss: 0.008530, acc.: 100.00%] [G loss: 2.050493]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:7 step:5604 [D loss: 0.200306, acc.: 91.41%] [G loss: 0.055082]\n",
      "epoch:7 step:5605 [D loss: 0.663054, acc.: 67.97%] [G loss: 7.001770]\n",
      "epoch:7 step:5606 [D loss: 0.459356, acc.: 76.56%] [G loss: 6.150241]\n",
      "epoch:7 step:5607 [D loss: 0.314802, acc.: 86.72%] [G loss: 4.305618]\n",
      "epoch:7 step:5608 [D loss: 0.108528, acc.: 96.88%] [G loss: 4.754851]\n",
      "epoch:7 step:5609 [D loss: 0.149928, acc.: 93.75%] [G loss: 2.639155]\n",
      "epoch:7 step:5610 [D loss: 0.809885, acc.: 63.28%] [G loss: 8.020514]\n",
      "epoch:7 step:5611 [D loss: 0.798611, acc.: 60.16%] [G loss: 3.348345]\n",
      "epoch:7 step:5612 [D loss: 0.022533, acc.: 99.22%] [G loss: 1.644314]\n",
      "epoch:7 step:5613 [D loss: 0.217934, acc.: 89.84%] [G loss: 3.924285]\n",
      "epoch:7 step:5614 [D loss: 0.105966, acc.: 97.66%] [G loss: 4.399306]\n",
      "epoch:7 step:5615 [D loss: 0.428862, acc.: 85.16%] [G loss: 2.790614]\n",
      "epoch:7 step:5616 [D loss: 0.125554, acc.: 94.53%] [G loss: 3.701885]\n",
      "epoch:7 step:5617 [D loss: 0.074738, acc.: 98.44%] [G loss: 3.517041]\n",
      "epoch:7 step:5618 [D loss: 0.206670, acc.: 93.75%] [G loss: 3.618794]\n",
      "epoch:7 step:5619 [D loss: 0.267467, acc.: 88.28%] [G loss: 4.335149]\n",
      "epoch:7 step:5620 [D loss: 0.686577, acc.: 62.50%] [G loss: 6.259387]\n",
      "epoch:7 step:5621 [D loss: 0.105085, acc.: 96.09%] [G loss: 5.394299]\n",
      "epoch:7 step:5622 [D loss: 0.137113, acc.: 94.53%] [G loss: 3.082882]\n",
      "epoch:7 step:5623 [D loss: 0.078018, acc.: 98.44%] [G loss: 2.051072]\n",
      "epoch:7 step:5624 [D loss: 0.044500, acc.: 99.22%] [G loss: 1.482930]\n",
      "epoch:7 step:5625 [D loss: 0.343303, acc.: 83.59%] [G loss: 4.627769]\n",
      "epoch:7 step:5626 [D loss: 0.227431, acc.: 90.62%] [G loss: 4.279433]\n",
      "epoch:7 step:5627 [D loss: 0.118324, acc.: 94.53%] [G loss: 1.229546]\n",
      "epoch:7 step:5628 [D loss: 0.048166, acc.: 100.00%] [G loss: 0.479890]\n",
      "epoch:7 step:5629 [D loss: 0.018772, acc.: 100.00%] [G loss: 0.542945]\n",
      "epoch:7 step:5630 [D loss: 0.124264, acc.: 98.44%] [G loss: 1.347298]\n",
      "epoch:7 step:5631 [D loss: 0.063288, acc.: 98.44%] [G loss: 1.881410]\n",
      "epoch:7 step:5632 [D loss: 0.074183, acc.: 100.00%] [G loss: 0.531479]\n",
      "epoch:7 step:5633 [D loss: 0.019885, acc.: 99.22%] [G loss: 0.789464]\n",
      "epoch:7 step:5634 [D loss: 0.022654, acc.: 100.00%] [G loss: 0.330732]\n",
      "epoch:7 step:5635 [D loss: 0.471113, acc.: 74.22%] [G loss: 6.541547]\n",
      "epoch:7 step:5636 [D loss: 0.725853, acc.: 64.06%] [G loss: 2.904433]\n",
      "epoch:7 step:5637 [D loss: 0.068175, acc.: 99.22%] [G loss: 2.190138]\n",
      "epoch:7 step:5638 [D loss: 0.046071, acc.: 100.00%] [G loss: 2.171756]\n",
      "epoch:7 step:5639 [D loss: 0.089404, acc.: 99.22%] [G loss: 1.824209]\n",
      "epoch:7 step:5640 [D loss: 0.398347, acc.: 85.16%] [G loss: 2.366893]\n",
      "epoch:7 step:5641 [D loss: 0.249863, acc.: 92.19%] [G loss: 4.020399]\n",
      "epoch:7 step:5642 [D loss: 0.084751, acc.: 97.66%] [G loss: 3.109813]\n",
      "epoch:7 step:5643 [D loss: 0.168879, acc.: 96.09%] [G loss: 2.335008]\n",
      "epoch:7 step:5644 [D loss: 0.067705, acc.: 98.44%] [G loss: 2.063858]\n",
      "epoch:7 step:5645 [D loss: 0.138782, acc.: 96.88%] [G loss: 2.647537]\n",
      "epoch:7 step:5646 [D loss: 0.103151, acc.: 97.66%] [G loss: 3.595522]\n",
      "epoch:7 step:5647 [D loss: 0.137120, acc.: 94.53%] [G loss: 2.818963]\n",
      "epoch:7 step:5648 [D loss: 0.721620, acc.: 61.72%] [G loss: 7.983187]\n",
      "epoch:7 step:5649 [D loss: 1.301124, acc.: 53.91%] [G loss: 4.019038]\n",
      "epoch:7 step:5650 [D loss: 0.244651, acc.: 89.06%] [G loss: 4.336937]\n",
      "epoch:7 step:5651 [D loss: 0.237468, acc.: 88.28%] [G loss: 2.427418]\n",
      "epoch:7 step:5652 [D loss: 0.221128, acc.: 92.19%] [G loss: 4.070155]\n",
      "epoch:7 step:5653 [D loss: 0.083670, acc.: 99.22%] [G loss: 3.413569]\n",
      "epoch:7 step:5654 [D loss: 0.206034, acc.: 92.19%] [G loss: 3.927603]\n",
      "epoch:7 step:5655 [D loss: 0.065148, acc.: 96.88%] [G loss: 3.253320]\n",
      "epoch:7 step:5656 [D loss: 0.207987, acc.: 95.31%] [G loss: 5.260180]\n",
      "epoch:7 step:5657 [D loss: 0.645842, acc.: 64.06%] [G loss: 4.194118]\n",
      "epoch:7 step:5658 [D loss: 0.110102, acc.: 97.66%] [G loss: 4.427144]\n",
      "epoch:7 step:5659 [D loss: 0.045845, acc.: 100.00%] [G loss: 3.780494]\n",
      "epoch:7 step:5660 [D loss: 0.085575, acc.: 98.44%] [G loss: 3.282537]\n",
      "epoch:7 step:5661 [D loss: 0.084375, acc.: 97.66%] [G loss: 3.299382]\n",
      "epoch:7 step:5662 [D loss: 0.153107, acc.: 94.53%] [G loss: 4.446686]\n",
      "epoch:7 step:5663 [D loss: 0.554434, acc.: 75.00%] [G loss: 5.774888]\n",
      "epoch:7 step:5664 [D loss: 0.172321, acc.: 94.53%] [G loss: 4.859107]\n",
      "epoch:7 step:5665 [D loss: 0.092374, acc.: 99.22%] [G loss: 3.213249]\n",
      "epoch:7 step:5666 [D loss: 0.110286, acc.: 96.09%] [G loss: 3.723088]\n",
      "epoch:7 step:5667 [D loss: 0.099779, acc.: 100.00%] [G loss: 3.938404]\n",
      "epoch:7 step:5668 [D loss: 0.066912, acc.: 99.22%] [G loss: 4.035829]\n",
      "epoch:7 step:5669 [D loss: 0.157054, acc.: 95.31%] [G loss: 2.559673]\n",
      "epoch:7 step:5670 [D loss: 0.410866, acc.: 79.69%] [G loss: 5.549827]\n",
      "epoch:7 step:5671 [D loss: 0.387822, acc.: 81.25%] [G loss: 3.030264]\n",
      "epoch:7 step:5672 [D loss: 0.153386, acc.: 96.09%] [G loss: 4.032557]\n",
      "epoch:7 step:5673 [D loss: 0.094004, acc.: 96.09%] [G loss: 2.816482]\n",
      "epoch:7 step:5674 [D loss: 0.105136, acc.: 96.88%] [G loss: 2.415615]\n",
      "epoch:7 step:5675 [D loss: 0.153415, acc.: 96.09%] [G loss: 1.948542]\n",
      "epoch:7 step:5676 [D loss: 0.914137, acc.: 57.81%] [G loss: 6.923737]\n",
      "epoch:7 step:5677 [D loss: 0.803896, acc.: 59.38%] [G loss: 4.071915]\n",
      "epoch:7 step:5678 [D loss: 0.064162, acc.: 100.00%] [G loss: 3.594340]\n",
      "epoch:7 step:5679 [D loss: 0.023947, acc.: 100.00%] [G loss: 2.451932]\n",
      "epoch:7 step:5680 [D loss: 0.287150, acc.: 85.16%] [G loss: 4.159718]\n",
      "epoch:7 step:5681 [D loss: 0.337645, acc.: 83.59%] [G loss: 1.598609]\n",
      "epoch:7 step:5682 [D loss: 0.158129, acc.: 94.53%] [G loss: 2.933145]\n",
      "epoch:7 step:5683 [D loss: 0.123224, acc.: 97.66%] [G loss: 3.405344]\n",
      "epoch:7 step:5684 [D loss: 0.065283, acc.: 98.44%] [G loss: 2.601080]\n",
      "epoch:7 step:5685 [D loss: 0.487261, acc.: 78.12%] [G loss: 2.752291]\n",
      "epoch:7 step:5686 [D loss: 0.072242, acc.: 98.44%] [G loss: 3.422794]\n",
      "epoch:7 step:5687 [D loss: 0.974061, acc.: 54.69%] [G loss: 6.333927]\n",
      "epoch:7 step:5688 [D loss: 0.196912, acc.: 89.84%] [G loss: 6.933764]\n",
      "epoch:7 step:5689 [D loss: 0.531346, acc.: 71.88%] [G loss: 2.498150]\n",
      "epoch:7 step:5690 [D loss: 0.197522, acc.: 90.62%] [G loss: 4.377554]\n",
      "epoch:7 step:5691 [D loss: 0.026777, acc.: 100.00%] [G loss: 5.081636]\n",
      "epoch:7 step:5692 [D loss: 0.067330, acc.: 99.22%] [G loss: 3.525673]\n",
      "epoch:7 step:5693 [D loss: 0.054761, acc.: 100.00%] [G loss: 2.310551]\n",
      "epoch:7 step:5694 [D loss: 0.133069, acc.: 95.31%] [G loss: 2.496647]\n",
      "epoch:7 step:5695 [D loss: 0.030416, acc.: 100.00%] [G loss: 1.810431]\n",
      "epoch:7 step:5696 [D loss: 0.162898, acc.: 96.09%] [G loss: 1.500789]\n",
      "epoch:7 step:5697 [D loss: 0.008377, acc.: 100.00%] [G loss: 1.365251]\n",
      "epoch:7 step:5698 [D loss: 0.142135, acc.: 96.09%] [G loss: 1.110337]\n",
      "epoch:7 step:5699 [D loss: 0.200758, acc.: 92.97%] [G loss: 0.098677]\n",
      "epoch:7 step:5700 [D loss: 0.124523, acc.: 96.09%] [G loss: 2.029484]\n",
      "epoch:7 step:5701 [D loss: 0.069271, acc.: 99.22%] [G loss: 0.522623]\n",
      "epoch:7 step:5702 [D loss: 0.011694, acc.: 100.00%] [G loss: 0.182333]\n",
      "epoch:7 step:5703 [D loss: 0.226217, acc.: 92.97%] [G loss: 3.956256]\n",
      "epoch:7 step:5704 [D loss: 0.403850, acc.: 81.25%] [G loss: 1.370882]\n",
      "epoch:7 step:5705 [D loss: 0.148897, acc.: 96.09%] [G loss: 3.669128]\n",
      "epoch:7 step:5706 [D loss: 0.101953, acc.: 98.44%] [G loss: 2.977501]\n",
      "epoch:7 step:5707 [D loss: 0.392362, acc.: 84.38%] [G loss: 5.090482]\n",
      "epoch:7 step:5708 [D loss: 0.307520, acc.: 83.59%] [G loss: 4.608932]\n",
      "epoch:7 step:5709 [D loss: 0.155314, acc.: 94.53%] [G loss: 3.968801]\n",
      "epoch:7 step:5710 [D loss: 0.046445, acc.: 100.00%] [G loss: 2.603432]\n",
      "epoch:7 step:5711 [D loss: 0.068766, acc.: 97.66%] [G loss: 2.917047]\n",
      "epoch:7 step:5712 [D loss: 0.050136, acc.: 99.22%] [G loss: 1.937096]\n",
      "epoch:7 step:5713 [D loss: 0.483886, acc.: 73.44%] [G loss: 5.847324]\n",
      "epoch:7 step:5714 [D loss: 1.986546, acc.: 50.00%] [G loss: 1.024322]\n",
      "epoch:7 step:5715 [D loss: 0.311798, acc.: 88.28%] [G loss: 1.982810]\n",
      "epoch:7 step:5716 [D loss: 0.043781, acc.: 100.00%] [G loss: 2.743384]\n",
      "epoch:7 step:5717 [D loss: 0.158850, acc.: 95.31%] [G loss: 0.750309]\n",
      "epoch:7 step:5718 [D loss: 0.043855, acc.: 99.22%] [G loss: 0.589872]\n",
      "epoch:7 step:5719 [D loss: 0.230593, acc.: 92.19%] [G loss: 2.058291]\n",
      "epoch:7 step:5720 [D loss: 0.111028, acc.: 97.66%] [G loss: 3.499423]\n",
      "epoch:7 step:5721 [D loss: 0.055685, acc.: 99.22%] [G loss: 3.027676]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:7 step:5722 [D loss: 0.139291, acc.: 96.09%] [G loss: 3.128000]\n",
      "epoch:7 step:5723 [D loss: 0.127824, acc.: 97.66%] [G loss: 2.351594]\n",
      "epoch:7 step:5724 [D loss: 0.334492, acc.: 85.16%] [G loss: 4.666773]\n",
      "epoch:7 step:5725 [D loss: 0.361048, acc.: 82.03%] [G loss: 2.690400]\n",
      "epoch:7 step:5726 [D loss: 0.125454, acc.: 96.88%] [G loss: 4.831079]\n",
      "epoch:7 step:5727 [D loss: 0.061264, acc.: 100.00%] [G loss: 4.230949]\n",
      "epoch:7 step:5728 [D loss: 0.097141, acc.: 97.66%] [G loss: 3.365602]\n",
      "epoch:7 step:5729 [D loss: 0.093885, acc.: 97.66%] [G loss: 2.985687]\n",
      "epoch:7 step:5730 [D loss: 0.018058, acc.: 100.00%] [G loss: 3.158276]\n",
      "epoch:7 step:5731 [D loss: 0.102443, acc.: 97.66%] [G loss: 2.001395]\n",
      "epoch:7 step:5732 [D loss: 0.029356, acc.: 99.22%] [G loss: 1.303124]\n",
      "epoch:7 step:5733 [D loss: 0.146759, acc.: 95.31%] [G loss: 2.580314]\n",
      "epoch:7 step:5734 [D loss: 2.006177, acc.: 19.53%] [G loss: 6.908154]\n",
      "epoch:7 step:5735 [D loss: 0.637852, acc.: 64.84%] [G loss: 4.864314]\n",
      "epoch:7 step:5736 [D loss: 0.105122, acc.: 95.31%] [G loss: 3.208268]\n",
      "epoch:7 step:5737 [D loss: 0.164438, acc.: 95.31%] [G loss: 3.217860]\n",
      "epoch:7 step:5738 [D loss: 0.071839, acc.: 99.22%] [G loss: 2.897310]\n",
      "epoch:7 step:5739 [D loss: 0.059697, acc.: 100.00%] [G loss: 2.771329]\n",
      "epoch:7 step:5740 [D loss: 0.244741, acc.: 92.19%] [G loss: 2.706176]\n",
      "epoch:7 step:5741 [D loss: 0.399615, acc.: 79.69%] [G loss: 4.179123]\n",
      "epoch:7 step:5742 [D loss: 0.169349, acc.: 92.97%] [G loss: 3.499610]\n",
      "epoch:7 step:5743 [D loss: 0.251925, acc.: 91.41%] [G loss: 4.116028]\n",
      "epoch:7 step:5744 [D loss: 0.162053, acc.: 93.75%] [G loss: 2.229331]\n",
      "epoch:7 step:5745 [D loss: 0.335993, acc.: 85.94%] [G loss: 5.140683]\n",
      "epoch:7 step:5746 [D loss: 0.222538, acc.: 91.41%] [G loss: 3.963304]\n",
      "epoch:7 step:5747 [D loss: 0.644886, acc.: 63.28%] [G loss: 4.239019]\n",
      "epoch:7 step:5748 [D loss: 0.246894, acc.: 87.50%] [G loss: 4.589220]\n",
      "epoch:7 step:5749 [D loss: 0.042663, acc.: 100.00%] [G loss: 2.434567]\n",
      "epoch:7 step:5750 [D loss: 0.125298, acc.: 98.44%] [G loss: 4.427330]\n",
      "epoch:7 step:5751 [D loss: 0.086113, acc.: 99.22%] [G loss: 4.096029]\n",
      "epoch:7 step:5752 [D loss: 0.076913, acc.: 100.00%] [G loss: 3.743900]\n",
      "epoch:7 step:5753 [D loss: 0.418925, acc.: 83.59%] [G loss: 4.205288]\n",
      "epoch:7 step:5754 [D loss: 0.308955, acc.: 84.38%] [G loss: 1.556180]\n",
      "epoch:7 step:5755 [D loss: 0.328487, acc.: 82.81%] [G loss: 4.549658]\n",
      "epoch:7 step:5756 [D loss: 0.157448, acc.: 94.53%] [G loss: 4.336682]\n",
      "epoch:7 step:5757 [D loss: 0.101510, acc.: 96.88%] [G loss: 3.018788]\n",
      "epoch:7 step:5758 [D loss: 0.174109, acc.: 92.97%] [G loss: 3.686630]\n",
      "epoch:7 step:5759 [D loss: 0.132641, acc.: 96.88%] [G loss: 2.990027]\n",
      "epoch:7 step:5760 [D loss: 0.180722, acc.: 93.75%] [G loss: 2.078866]\n",
      "epoch:7 step:5761 [D loss: 0.160050, acc.: 92.97%] [G loss: 3.977314]\n",
      "epoch:7 step:5762 [D loss: 0.998099, acc.: 44.53%] [G loss: 3.318555]\n",
      "epoch:7 step:5763 [D loss: 0.079807, acc.: 97.66%] [G loss: 4.699829]\n",
      "epoch:7 step:5764 [D loss: 0.134174, acc.: 96.09%] [G loss: 2.014669]\n",
      "epoch:7 step:5765 [D loss: 0.109568, acc.: 95.31%] [G loss: 2.887198]\n",
      "epoch:7 step:5766 [D loss: 0.270108, acc.: 89.06%] [G loss: 4.837845]\n",
      "epoch:7 step:5767 [D loss: 0.624673, acc.: 68.75%] [G loss: 2.941195]\n",
      "epoch:7 step:5768 [D loss: 0.272607, acc.: 87.50%] [G loss: 5.482985]\n",
      "epoch:7 step:5769 [D loss: 0.045550, acc.: 98.44%] [G loss: 5.645112]\n",
      "epoch:7 step:5770 [D loss: 0.522595, acc.: 77.34%] [G loss: 3.189387]\n",
      "epoch:7 step:5771 [D loss: 0.079029, acc.: 97.66%] [G loss: 4.408219]\n",
      "epoch:7 step:5772 [D loss: 0.082279, acc.: 97.66%] [G loss: 3.546420]\n",
      "epoch:7 step:5773 [D loss: 0.077408, acc.: 99.22%] [G loss: 3.376286]\n",
      "epoch:7 step:5774 [D loss: 0.233298, acc.: 89.06%] [G loss: 5.190616]\n",
      "epoch:7 step:5775 [D loss: 0.434239, acc.: 79.69%] [G loss: 3.276680]\n",
      "epoch:7 step:5776 [D loss: 0.302466, acc.: 87.50%] [G loss: 5.538162]\n",
      "epoch:7 step:5777 [D loss: 0.263948, acc.: 85.16%] [G loss: 3.577967]\n",
      "epoch:7 step:5778 [D loss: 0.194695, acc.: 92.97%] [G loss: 4.772971]\n",
      "epoch:7 step:5779 [D loss: 0.095231, acc.: 98.44%] [G loss: 4.020924]\n",
      "epoch:7 step:5780 [D loss: 0.237830, acc.: 92.19%] [G loss: 4.193827]\n",
      "epoch:7 step:5781 [D loss: 0.040063, acc.: 100.00%] [G loss: 4.241437]\n",
      "epoch:7 step:5782 [D loss: 0.489826, acc.: 77.34%] [G loss: 4.714622]\n",
      "epoch:7 step:5783 [D loss: 0.395696, acc.: 77.34%] [G loss: 2.371195]\n",
      "epoch:7 step:5784 [D loss: 0.162377, acc.: 91.41%] [G loss: 2.454489]\n",
      "epoch:7 step:5785 [D loss: 0.072153, acc.: 99.22%] [G loss: 4.210956]\n",
      "epoch:7 step:5786 [D loss: 0.794704, acc.: 56.25%] [G loss: 4.758618]\n",
      "epoch:7 step:5787 [D loss: 0.308927, acc.: 81.25%] [G loss: 2.272797]\n",
      "epoch:7 step:5788 [D loss: 0.438678, acc.: 79.69%] [G loss: 5.220194]\n",
      "epoch:7 step:5789 [D loss: 0.505911, acc.: 76.56%] [G loss: 2.312877]\n",
      "epoch:7 step:5790 [D loss: 0.295791, acc.: 82.81%] [G loss: 5.048665]\n",
      "epoch:7 step:5791 [D loss: 0.622220, acc.: 60.16%] [G loss: 3.368295]\n",
      "epoch:7 step:5792 [D loss: 0.085538, acc.: 97.66%] [G loss: 3.843171]\n",
      "epoch:7 step:5793 [D loss: 0.068845, acc.: 100.00%] [G loss: 4.406015]\n",
      "epoch:7 step:5794 [D loss: 0.145861, acc.: 96.88%] [G loss: 3.955172]\n",
      "epoch:7 step:5795 [D loss: 0.294975, acc.: 89.06%] [G loss: 4.176062]\n",
      "epoch:7 step:5796 [D loss: 0.094614, acc.: 97.66%] [G loss: 3.898397]\n",
      "epoch:7 step:5797 [D loss: 0.210770, acc.: 93.75%] [G loss: 4.088764]\n",
      "epoch:7 step:5798 [D loss: 0.082872, acc.: 99.22%] [G loss: 4.184800]\n",
      "epoch:7 step:5799 [D loss: 0.171996, acc.: 92.97%] [G loss: 4.357134]\n",
      "epoch:7 step:5800 [D loss: 0.031689, acc.: 100.00%] [G loss: 3.570208]\n",
      "##############\n",
      "[1.00686466 0.99134184 0.84501674 0.94934567 0.97224419 0.97181166\n",
      " 1.11270672 0.96521716 0.95302349 2.11179936]\n",
      "##########\n",
      "epoch:7 step:5801 [D loss: 0.058553, acc.: 98.44%] [G loss: 2.317775]\n",
      "epoch:7 step:5802 [D loss: 0.326496, acc.: 83.59%] [G loss: 5.345864]\n",
      "epoch:7 step:5803 [D loss: 0.707868, acc.: 64.84%] [G loss: 2.244687]\n",
      "epoch:7 step:5804 [D loss: 0.074682, acc.: 97.66%] [G loss: 0.821799]\n",
      "epoch:7 step:5805 [D loss: 0.055918, acc.: 100.00%] [G loss: 0.761425]\n",
      "epoch:7 step:5806 [D loss: 0.117976, acc.: 95.31%] [G loss: 1.748663]\n",
      "epoch:7 step:5807 [D loss: 0.124653, acc.: 97.66%] [G loss: 0.433101]\n",
      "epoch:7 step:5808 [D loss: 0.144912, acc.: 96.09%] [G loss: 1.888160]\n",
      "epoch:7 step:5809 [D loss: 0.220762, acc.: 92.19%] [G loss: 0.479490]\n",
      "epoch:7 step:5810 [D loss: 0.041458, acc.: 99.22%] [G loss: 0.598142]\n",
      "epoch:7 step:5811 [D loss: 0.366321, acc.: 81.25%] [G loss: 3.840566]\n",
      "epoch:7 step:5812 [D loss: 0.160882, acc.: 93.75%] [G loss: 2.895659]\n",
      "epoch:7 step:5813 [D loss: 0.550947, acc.: 77.34%] [G loss: 1.519750]\n",
      "epoch:7 step:5814 [D loss: 0.051825, acc.: 99.22%] [G loss: 2.844012]\n",
      "epoch:7 step:5815 [D loss: 0.093289, acc.: 96.88%] [G loss: 2.506659]\n",
      "epoch:7 step:5816 [D loss: 0.183629, acc.: 91.41%] [G loss: 1.346075]\n",
      "epoch:7 step:5817 [D loss: 0.443913, acc.: 75.78%] [G loss: 3.405829]\n",
      "epoch:7 step:5818 [D loss: 0.025531, acc.: 100.00%] [G loss: 4.081866]\n",
      "epoch:7 step:5819 [D loss: 0.709046, acc.: 61.72%] [G loss: 4.640035]\n",
      "epoch:7 step:5820 [D loss: 0.369209, acc.: 80.47%] [G loss: 2.605103]\n",
      "epoch:7 step:5821 [D loss: 0.182560, acc.: 88.28%] [G loss: 5.250710]\n",
      "epoch:7 step:5822 [D loss: 0.079444, acc.: 96.88%] [G loss: 4.682208]\n",
      "epoch:7 step:5823 [D loss: 0.157361, acc.: 96.88%] [G loss: 3.742048]\n",
      "epoch:7 step:5824 [D loss: 0.060389, acc.: 100.00%] [G loss: 3.344285]\n",
      "epoch:7 step:5825 [D loss: 0.353966, acc.: 84.38%] [G loss: 1.369927]\n",
      "epoch:7 step:5826 [D loss: 0.388987, acc.: 77.34%] [G loss: 5.308408]\n",
      "epoch:7 step:5827 [D loss: 0.254872, acc.: 89.84%] [G loss: 4.770498]\n",
      "epoch:7 step:5828 [D loss: 0.277018, acc.: 88.28%] [G loss: 1.050193]\n",
      "epoch:7 step:5829 [D loss: 0.567200, acc.: 73.44%] [G loss: 6.038648]\n",
      "epoch:7 step:5830 [D loss: 0.324200, acc.: 81.25%] [G loss: 4.161564]\n",
      "epoch:7 step:5831 [D loss: 0.213548, acc.: 92.19%] [G loss: 2.120923]\n",
      "epoch:7 step:5832 [D loss: 0.084840, acc.: 98.44%] [G loss: 2.242472]\n",
      "epoch:7 step:5833 [D loss: 0.053720, acc.: 99.22%] [G loss: 3.056378]\n",
      "epoch:7 step:5834 [D loss: 0.107536, acc.: 97.66%] [G loss: 1.717350]\n",
      "epoch:7 step:5835 [D loss: 0.145682, acc.: 96.09%] [G loss: 4.770606]\n",
      "epoch:7 step:5836 [D loss: 0.131558, acc.: 96.09%] [G loss: 3.379064]\n",
      "epoch:7 step:5837 [D loss: 0.345094, acc.: 82.81%] [G loss: 4.023295]\n",
      "epoch:7 step:5838 [D loss: 0.055200, acc.: 97.66%] [G loss: 4.630946]\n",
      "epoch:7 step:5839 [D loss: 0.241871, acc.: 89.84%] [G loss: 1.020266]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:7 step:5840 [D loss: 0.346536, acc.: 85.94%] [G loss: 1.257920]\n",
      "epoch:7 step:5841 [D loss: 0.017948, acc.: 100.00%] [G loss: 1.852752]\n",
      "epoch:7 step:5842 [D loss: 0.076383, acc.: 100.00%] [G loss: 2.920979]\n",
      "epoch:7 step:5843 [D loss: 0.035752, acc.: 99.22%] [G loss: 1.925239]\n",
      "epoch:7 step:5844 [D loss: 0.137903, acc.: 94.53%] [G loss: 0.331299]\n",
      "epoch:7 step:5845 [D loss: 0.578250, acc.: 71.88%] [G loss: 5.768087]\n",
      "epoch:7 step:5846 [D loss: 1.490661, acc.: 50.00%] [G loss: 1.642746]\n",
      "epoch:7 step:5847 [D loss: 0.088527, acc.: 97.66%] [G loss: 3.033878]\n",
      "epoch:7 step:5848 [D loss: 0.057630, acc.: 100.00%] [G loss: 4.611170]\n",
      "epoch:7 step:5849 [D loss: 0.447373, acc.: 77.34%] [G loss: 4.532383]\n",
      "epoch:7 step:5850 [D loss: 0.229287, acc.: 92.19%] [G loss: 2.716943]\n",
      "epoch:7 step:5851 [D loss: 0.154874, acc.: 97.66%] [G loss: 4.243712]\n",
      "epoch:7 step:5852 [D loss: 0.270989, acc.: 91.41%] [G loss: 4.496711]\n",
      "epoch:7 step:5853 [D loss: 0.109498, acc.: 99.22%] [G loss: 3.814494]\n",
      "epoch:7 step:5854 [D loss: 0.193782, acc.: 95.31%] [G loss: 2.610538]\n",
      "epoch:7 step:5855 [D loss: 0.170789, acc.: 92.97%] [G loss: 1.986453]\n",
      "epoch:7 step:5856 [D loss: 0.070806, acc.: 100.00%] [G loss: 2.025340]\n",
      "epoch:7 step:5857 [D loss: 0.075191, acc.: 98.44%] [G loss: 1.626051]\n",
      "epoch:7 step:5858 [D loss: 0.236712, acc.: 92.19%] [G loss: 4.230017]\n",
      "epoch:7 step:5859 [D loss: 0.131712, acc.: 96.09%] [G loss: 2.937344]\n",
      "epoch:7 step:5860 [D loss: 0.185526, acc.: 92.97%] [G loss: 2.541934]\n",
      "epoch:7 step:5861 [D loss: 0.060912, acc.: 100.00%] [G loss: 1.175632]\n",
      "epoch:7 step:5862 [D loss: 0.069039, acc.: 98.44%] [G loss: 0.653033]\n",
      "epoch:7 step:5863 [D loss: 0.096003, acc.: 96.09%] [G loss: 0.179404]\n",
      "epoch:7 step:5864 [D loss: 0.206234, acc.: 93.75%] [G loss: 2.240698]\n",
      "epoch:7 step:5865 [D loss: 0.271690, acc.: 89.84%] [G loss: 2.251066]\n",
      "epoch:7 step:5866 [D loss: 0.039755, acc.: 99.22%] [G loss: 3.213693]\n",
      "epoch:7 step:5867 [D loss: 0.163186, acc.: 93.75%] [G loss: 3.239367]\n",
      "epoch:7 step:5868 [D loss: 0.192590, acc.: 90.62%] [G loss: 1.476577]\n",
      "epoch:7 step:5869 [D loss: 0.566507, acc.: 71.88%] [G loss: 6.343050]\n",
      "epoch:7 step:5870 [D loss: 0.774335, acc.: 65.62%] [G loss: 3.657265]\n",
      "epoch:7 step:5871 [D loss: 0.174023, acc.: 92.97%] [G loss: 4.293670]\n",
      "epoch:7 step:5872 [D loss: 0.330076, acc.: 89.84%] [G loss: 4.630851]\n",
      "epoch:7 step:5873 [D loss: 0.303195, acc.: 89.06%] [G loss: 3.586274]\n",
      "epoch:7 step:5874 [D loss: 0.046740, acc.: 100.00%] [G loss: 3.587670]\n",
      "epoch:7 step:5875 [D loss: 0.144710, acc.: 96.88%] [G loss: 3.489458]\n",
      "epoch:7 step:5876 [D loss: 0.149554, acc.: 97.66%] [G loss: 3.820786]\n",
      "epoch:7 step:5877 [D loss: 0.116164, acc.: 96.09%] [G loss: 2.594694]\n",
      "epoch:7 step:5878 [D loss: 0.257425, acc.: 92.19%] [G loss: 4.915931]\n",
      "epoch:7 step:5879 [D loss: 0.156462, acc.: 96.09%] [G loss: 4.235179]\n",
      "epoch:7 step:5880 [D loss: 0.117722, acc.: 97.66%] [G loss: 3.375923]\n",
      "epoch:7 step:5881 [D loss: 0.200612, acc.: 93.75%] [G loss: 6.016858]\n",
      "epoch:7 step:5882 [D loss: 0.215864, acc.: 91.41%] [G loss: 3.938811]\n",
      "epoch:7 step:5883 [D loss: 0.103081, acc.: 99.22%] [G loss: 4.462251]\n",
      "epoch:7 step:5884 [D loss: 0.120839, acc.: 98.44%] [G loss: 2.793664]\n",
      "epoch:7 step:5885 [D loss: 0.143891, acc.: 96.09%] [G loss: 3.132996]\n",
      "epoch:7 step:5886 [D loss: 0.032958, acc.: 100.00%] [G loss: 3.333790]\n",
      "epoch:7 step:5887 [D loss: 0.891286, acc.: 55.47%] [G loss: 7.027356]\n",
      "epoch:7 step:5888 [D loss: 0.749220, acc.: 64.84%] [G loss: 3.271482]\n",
      "epoch:7 step:5889 [D loss: 0.176008, acc.: 92.97%] [G loss: 5.131699]\n",
      "epoch:7 step:5890 [D loss: 0.164471, acc.: 93.75%] [G loss: 5.413495]\n",
      "epoch:7 step:5891 [D loss: 0.470193, acc.: 79.69%] [G loss: 6.060836]\n",
      "epoch:7 step:5892 [D loss: 0.369599, acc.: 79.69%] [G loss: 4.425216]\n",
      "epoch:7 step:5893 [D loss: 0.119445, acc.: 96.88%] [G loss: 4.022047]\n",
      "epoch:7 step:5894 [D loss: 0.010280, acc.: 100.00%] [G loss: 2.737698]\n",
      "epoch:7 step:5895 [D loss: 0.121243, acc.: 96.88%] [G loss: 1.643940]\n",
      "epoch:7 step:5896 [D loss: 0.134546, acc.: 96.09%] [G loss: 2.299327]\n",
      "epoch:7 step:5897 [D loss: 0.541769, acc.: 71.88%] [G loss: 0.397415]\n",
      "epoch:7 step:5898 [D loss: 0.050083, acc.: 100.00%] [G loss: 0.612241]\n",
      "epoch:7 step:5899 [D loss: 0.009355, acc.: 100.00%] [G loss: 1.310668]\n",
      "epoch:7 step:5900 [D loss: 0.072515, acc.: 98.44%] [G loss: 0.030743]\n",
      "epoch:7 step:5901 [D loss: 0.060033, acc.: 99.22%] [G loss: 0.038066]\n",
      "epoch:7 step:5902 [D loss: 0.346797, acc.: 80.47%] [G loss: 4.884600]\n",
      "epoch:7 step:5903 [D loss: 0.811419, acc.: 66.41%] [G loss: 0.332442]\n",
      "epoch:7 step:5904 [D loss: 1.526869, acc.: 53.91%] [G loss: 7.983430]\n",
      "epoch:7 step:5905 [D loss: 1.821057, acc.: 51.56%] [G loss: 3.584803]\n",
      "epoch:7 step:5906 [D loss: 0.203340, acc.: 94.53%] [G loss: 2.972481]\n",
      "epoch:7 step:5907 [D loss: 0.130147, acc.: 96.09%] [G loss: 3.172527]\n",
      "epoch:7 step:5908 [D loss: 0.289906, acc.: 90.62%] [G loss: 3.600084]\n",
      "epoch:7 step:5909 [D loss: 0.284574, acc.: 89.84%] [G loss: 4.186669]\n",
      "epoch:7 step:5910 [D loss: 0.252984, acc.: 89.84%] [G loss: 3.520423]\n",
      "epoch:7 step:5911 [D loss: 0.117157, acc.: 96.88%] [G loss: 3.102815]\n",
      "epoch:7 step:5912 [D loss: 0.142015, acc.: 99.22%] [G loss: 3.391279]\n",
      "epoch:7 step:5913 [D loss: 0.148806, acc.: 96.88%] [G loss: 4.392823]\n",
      "epoch:7 step:5914 [D loss: 0.309758, acc.: 85.94%] [G loss: 1.887375]\n",
      "epoch:7 step:5915 [D loss: 0.303666, acc.: 89.06%] [G loss: 3.949546]\n",
      "epoch:7 step:5916 [D loss: 0.077174, acc.: 98.44%] [G loss: 3.372427]\n",
      "epoch:7 step:5917 [D loss: 0.085094, acc.: 97.66%] [G loss: 1.184212]\n",
      "epoch:7 step:5918 [D loss: 0.029460, acc.: 100.00%] [G loss: 0.605837]\n",
      "epoch:7 step:5919 [D loss: 0.136310, acc.: 95.31%] [G loss: 1.174628]\n",
      "epoch:7 step:5920 [D loss: 0.243168, acc.: 87.50%] [G loss: 0.365876]\n",
      "epoch:7 step:5921 [D loss: 0.061155, acc.: 99.22%] [G loss: 0.199866]\n",
      "epoch:7 step:5922 [D loss: 0.155344, acc.: 93.75%] [G loss: 1.871845]\n",
      "epoch:7 step:5923 [D loss: 0.200656, acc.: 95.31%] [G loss: 2.283319]\n",
      "epoch:7 step:5924 [D loss: 0.145906, acc.: 94.53%] [G loss: 0.571554]\n",
      "epoch:7 step:5925 [D loss: 0.317908, acc.: 82.03%] [G loss: 3.666828]\n",
      "epoch:7 step:5926 [D loss: 0.220008, acc.: 89.84%] [G loss: 3.050667]\n",
      "epoch:7 step:5927 [D loss: 0.189037, acc.: 92.97%] [G loss: 0.302506]\n",
      "epoch:7 step:5928 [D loss: 0.053813, acc.: 100.00%] [G loss: 0.372878]\n",
      "epoch:7 step:5929 [D loss: 0.209793, acc.: 90.62%] [G loss: 2.719567]\n",
      "epoch:7 step:5930 [D loss: 0.059874, acc.: 99.22%] [G loss: 3.036887]\n",
      "epoch:7 step:5931 [D loss: 0.615350, acc.: 72.66%] [G loss: 5.972738]\n",
      "epoch:7 step:5932 [D loss: 0.077173, acc.: 97.66%] [G loss: 6.518090]\n",
      "epoch:7 step:5933 [D loss: 0.186929, acc.: 92.97%] [G loss: 3.505201]\n",
      "epoch:7 step:5934 [D loss: 0.142487, acc.: 95.31%] [G loss: 3.087778]\n",
      "epoch:7 step:5935 [D loss: 0.050312, acc.: 100.00%] [G loss: 4.554608]\n",
      "epoch:7 step:5936 [D loss: 0.042725, acc.: 100.00%] [G loss: 3.432670]\n",
      "epoch:7 step:5937 [D loss: 0.088577, acc.: 99.22%] [G loss: 2.653843]\n",
      "epoch:7 step:5938 [D loss: 0.188881, acc.: 96.09%] [G loss: 2.910387]\n",
      "epoch:7 step:5939 [D loss: 0.482541, acc.: 72.66%] [G loss: 5.093569]\n",
      "epoch:7 step:5940 [D loss: 0.099356, acc.: 95.31%] [G loss: 5.120471]\n",
      "epoch:7 step:5941 [D loss: 0.316650, acc.: 86.72%] [G loss: 2.127344]\n",
      "epoch:7 step:5942 [D loss: 0.425666, acc.: 76.56%] [G loss: 6.222554]\n",
      "epoch:7 step:5943 [D loss: 0.332037, acc.: 83.59%] [G loss: 5.074459]\n",
      "epoch:7 step:5944 [D loss: 0.046429, acc.: 98.44%] [G loss: 3.262826]\n",
      "epoch:7 step:5945 [D loss: 0.017894, acc.: 100.00%] [G loss: 2.969448]\n",
      "epoch:7 step:5946 [D loss: 0.285375, acc.: 85.94%] [G loss: 5.562891]\n",
      "epoch:7 step:5947 [D loss: 0.190609, acc.: 92.19%] [G loss: 5.152974]\n",
      "epoch:7 step:5948 [D loss: 0.163335, acc.: 94.53%] [G loss: 2.610442]\n",
      "epoch:7 step:5949 [D loss: 0.217559, acc.: 87.50%] [G loss: 4.129398]\n",
      "epoch:7 step:5950 [D loss: 0.091900, acc.: 97.66%] [G loss: 4.877821]\n",
      "epoch:7 step:5951 [D loss: 0.120403, acc.: 96.09%] [G loss: 3.064881]\n",
      "epoch:7 step:5952 [D loss: 0.050784, acc.: 99.22%] [G loss: 2.126472]\n",
      "epoch:7 step:5953 [D loss: 0.239021, acc.: 91.41%] [G loss: 5.054200]\n",
      "epoch:7 step:5954 [D loss: 0.273942, acc.: 84.38%] [G loss: 2.459195]\n",
      "epoch:7 step:5955 [D loss: 1.482540, acc.: 44.53%] [G loss: 6.879322]\n",
      "epoch:7 step:5956 [D loss: 0.818464, acc.: 64.06%] [G loss: 5.445721]\n",
      "epoch:7 step:5957 [D loss: 0.131430, acc.: 92.19%] [G loss: 2.352361]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:7 step:5958 [D loss: 0.107974, acc.: 97.66%] [G loss: 2.657670]\n",
      "epoch:7 step:5959 [D loss: 0.143786, acc.: 97.66%] [G loss: 1.908107]\n",
      "epoch:7 step:5960 [D loss: 0.037208, acc.: 100.00%] [G loss: 2.571700]\n",
      "epoch:7 step:5961 [D loss: 0.044150, acc.: 99.22%] [G loss: 0.886827]\n",
      "epoch:7 step:5962 [D loss: 0.399010, acc.: 82.81%] [G loss: 4.202151]\n",
      "epoch:7 step:5963 [D loss: 0.392955, acc.: 78.12%] [G loss: 1.531049]\n",
      "epoch:7 step:5964 [D loss: 0.049192, acc.: 100.00%] [G loss: 1.297283]\n",
      "epoch:7 step:5965 [D loss: 0.036951, acc.: 100.00%] [G loss: 0.679612]\n",
      "epoch:7 step:5966 [D loss: 0.131323, acc.: 94.53%] [G loss: 3.074087]\n",
      "epoch:7 step:5967 [D loss: 0.296610, acc.: 89.06%] [G loss: 2.542868]\n",
      "epoch:7 step:5968 [D loss: 0.211332, acc.: 93.75%] [G loss: 2.944373]\n",
      "epoch:7 step:5969 [D loss: 0.212246, acc.: 88.28%] [G loss: 5.340298]\n",
      "epoch:7 step:5970 [D loss: 0.408823, acc.: 82.03%] [G loss: 2.869299]\n",
      "epoch:7 step:5971 [D loss: 0.163827, acc.: 92.97%] [G loss: 4.022092]\n",
      "epoch:7 step:5972 [D loss: 0.124022, acc.: 96.88%] [G loss: 3.797955]\n",
      "epoch:7 step:5973 [D loss: 0.083203, acc.: 97.66%] [G loss: 2.340227]\n",
      "epoch:7 step:5974 [D loss: 0.087605, acc.: 99.22%] [G loss: 1.028886]\n",
      "epoch:7 step:5975 [D loss: 0.228599, acc.: 90.62%] [G loss: 3.775193]\n",
      "epoch:7 step:5976 [D loss: 0.432649, acc.: 79.69%] [G loss: 0.176340]\n",
      "epoch:7 step:5977 [D loss: 0.146906, acc.: 92.97%] [G loss: 0.258233]\n",
      "epoch:7 step:5978 [D loss: 0.004314, acc.: 100.00%] [G loss: 1.072985]\n",
      "epoch:7 step:5979 [D loss: 0.005776, acc.: 100.00%] [G loss: 0.822348]\n",
      "epoch:7 step:5980 [D loss: 0.020395, acc.: 100.00%] [G loss: 0.516239]\n",
      "epoch:7 step:5981 [D loss: 0.196429, acc.: 91.41%] [G loss: 3.781370]\n",
      "epoch:7 step:5982 [D loss: 0.179842, acc.: 92.19%] [G loss: 1.286476]\n",
      "epoch:7 step:5983 [D loss: 0.029766, acc.: 100.00%] [G loss: 0.400510]\n",
      "epoch:7 step:5984 [D loss: 0.019230, acc.: 100.00%] [G loss: 0.448263]\n",
      "epoch:7 step:5985 [D loss: 0.642961, acc.: 69.53%] [G loss: 6.051665]\n",
      "epoch:7 step:5986 [D loss: 0.727695, acc.: 66.41%] [G loss: 4.716455]\n",
      "epoch:7 step:5987 [D loss: 0.456755, acc.: 74.22%] [G loss: 1.290165]\n",
      "epoch:7 step:5988 [D loss: 0.124304, acc.: 94.53%] [G loss: 1.608718]\n",
      "epoch:7 step:5989 [D loss: 0.157078, acc.: 95.31%] [G loss: 3.873994]\n",
      "epoch:7 step:5990 [D loss: 0.099615, acc.: 98.44%] [G loss: 3.634593]\n",
      "epoch:7 step:5991 [D loss: 0.959480, acc.: 47.66%] [G loss: 4.890225]\n",
      "epoch:7 step:5992 [D loss: 0.012109, acc.: 100.00%] [G loss: 6.084222]\n",
      "epoch:7 step:5993 [D loss: 0.977205, acc.: 56.25%] [G loss: 3.230942]\n",
      "epoch:7 step:5994 [D loss: 0.324354, acc.: 82.81%] [G loss: 4.883480]\n",
      "epoch:7 step:5995 [D loss: 0.013285, acc.: 100.00%] [G loss: 5.819195]\n",
      "epoch:7 step:5996 [D loss: 0.223750, acc.: 89.06%] [G loss: 4.017577]\n",
      "epoch:7 step:5997 [D loss: 0.069359, acc.: 98.44%] [G loss: 2.823265]\n",
      "epoch:7 step:5998 [D loss: 0.094785, acc.: 97.66%] [G loss: 3.469511]\n",
      "epoch:7 step:5999 [D loss: 0.110596, acc.: 98.44%] [G loss: 2.420365]\n",
      "epoch:7 step:6000 [D loss: 0.284585, acc.: 85.16%] [G loss: 3.795260]\n",
      "##############\n",
      "[0.98033426 0.85123053 0.95053783 1.00039969 2.11331594 1.00846316\n",
      " 2.09533116 1.00386787 1.11166927 1.10994824]\n",
      "##########\n",
      "epoch:7 step:6001 [D loss: 0.065297, acc.: 100.00%] [G loss: 3.545295]\n",
      "epoch:7 step:6002 [D loss: 0.109326, acc.: 96.88%] [G loss: 1.214643]\n",
      "epoch:7 step:6003 [D loss: 0.178504, acc.: 93.75%] [G loss: 1.365380]\n",
      "epoch:7 step:6004 [D loss: 0.083188, acc.: 97.66%] [G loss: 2.131349]\n",
      "epoch:7 step:6005 [D loss: 0.081329, acc.: 98.44%] [G loss: 0.765894]\n",
      "epoch:7 step:6006 [D loss: 0.242281, acc.: 90.62%] [G loss: 2.306420]\n",
      "epoch:7 step:6007 [D loss: 0.169232, acc.: 94.53%] [G loss: 3.256753]\n",
      "epoch:7 step:6008 [D loss: 0.032127, acc.: 100.00%] [G loss: 0.894688]\n",
      "epoch:7 step:6009 [D loss: 0.051548, acc.: 100.00%] [G loss: 0.351676]\n",
      "epoch:7 step:6010 [D loss: 0.103112, acc.: 96.88%] [G loss: 0.902057]\n",
      "epoch:7 step:6011 [D loss: 0.049109, acc.: 99.22%] [G loss: 1.975005]\n",
      "epoch:7 step:6012 [D loss: 0.110813, acc.: 98.44%] [G loss: 3.136327]\n",
      "epoch:7 step:6013 [D loss: 0.776652, acc.: 61.72%] [G loss: 4.848297]\n",
      "epoch:7 step:6014 [D loss: 0.453281, acc.: 74.22%] [G loss: 4.315000]\n",
      "epoch:7 step:6015 [D loss: 0.134158, acc.: 95.31%] [G loss: 1.920103]\n",
      "epoch:7 step:6016 [D loss: 0.131195, acc.: 93.75%] [G loss: 2.784712]\n",
      "epoch:7 step:6017 [D loss: 0.054581, acc.: 99.22%] [G loss: 3.416478]\n",
      "epoch:7 step:6018 [D loss: 0.106717, acc.: 97.66%] [G loss: 2.742518]\n",
      "epoch:7 step:6019 [D loss: 0.240067, acc.: 86.72%] [G loss: 2.274957]\n",
      "epoch:7 step:6020 [D loss: 0.073564, acc.: 100.00%] [G loss: 4.052022]\n",
      "epoch:7 step:6021 [D loss: 0.134848, acc.: 94.53%] [G loss: 2.720819]\n",
      "epoch:7 step:6022 [D loss: 0.033560, acc.: 100.00%] [G loss: 1.707377]\n",
      "epoch:7 step:6023 [D loss: 0.481074, acc.: 73.44%] [G loss: 5.965482]\n",
      "epoch:7 step:6024 [D loss: 1.151641, acc.: 55.47%] [G loss: 2.810227]\n",
      "epoch:7 step:6025 [D loss: 0.056153, acc.: 99.22%] [G loss: 0.886961]\n",
      "epoch:7 step:6026 [D loss: 0.014040, acc.: 100.00%] [G loss: 0.410795]\n",
      "epoch:7 step:6027 [D loss: 0.024271, acc.: 100.00%] [G loss: 0.227184]\n",
      "epoch:7 step:6028 [D loss: 0.571364, acc.: 72.66%] [G loss: 4.163514]\n",
      "epoch:7 step:6029 [D loss: 0.601321, acc.: 71.09%] [G loss: 2.133799]\n",
      "epoch:7 step:6030 [D loss: 0.077676, acc.: 97.66%] [G loss: 1.893084]\n",
      "epoch:7 step:6031 [D loss: 0.045552, acc.: 100.00%] [G loss: 2.413777]\n",
      "epoch:7 step:6032 [D loss: 0.225838, acc.: 90.62%] [G loss: 1.476021]\n",
      "epoch:7 step:6033 [D loss: 0.183689, acc.: 92.97%] [G loss: 1.390394]\n",
      "epoch:7 step:6034 [D loss: 0.687382, acc.: 65.62%] [G loss: 5.086189]\n",
      "epoch:7 step:6035 [D loss: 0.448326, acc.: 75.78%] [G loss: 4.403916]\n",
      "epoch:7 step:6036 [D loss: 0.035063, acc.: 100.00%] [G loss: 2.653365]\n",
      "epoch:7 step:6037 [D loss: 0.056532, acc.: 98.44%] [G loss: 2.623341]\n",
      "epoch:7 step:6038 [D loss: 0.149762, acc.: 95.31%] [G loss: 1.124088]\n",
      "epoch:7 step:6039 [D loss: 0.038787, acc.: 100.00%] [G loss: 1.500639]\n",
      "epoch:7 step:6040 [D loss: 0.226745, acc.: 91.41%] [G loss: 2.407140]\n",
      "epoch:7 step:6041 [D loss: 0.092672, acc.: 100.00%] [G loss: 3.541875]\n",
      "epoch:7 step:6042 [D loss: 1.091075, acc.: 46.88%] [G loss: 3.131747]\n",
      "epoch:7 step:6043 [D loss: 0.466196, acc.: 79.69%] [G loss: 2.233587]\n",
      "epoch:7 step:6044 [D loss: 0.110399, acc.: 97.66%] [G loss: 3.228161]\n",
      "epoch:7 step:6045 [D loss: 0.076561, acc.: 97.66%] [G loss: 3.689803]\n",
      "epoch:7 step:6046 [D loss: 0.132685, acc.: 96.09%] [G loss: 3.898377]\n",
      "epoch:7 step:6047 [D loss: 0.111759, acc.: 99.22%] [G loss: 3.581790]\n",
      "epoch:7 step:6048 [D loss: 0.162257, acc.: 96.09%] [G loss: 2.956023]\n",
      "epoch:7 step:6049 [D loss: 0.167317, acc.: 96.88%] [G loss: 3.157175]\n",
      "epoch:7 step:6050 [D loss: 0.100483, acc.: 99.22%] [G loss: 3.353482]\n",
      "epoch:7 step:6051 [D loss: 0.119469, acc.: 97.66%] [G loss: 1.125889]\n",
      "epoch:7 step:6052 [D loss: 0.262073, acc.: 89.84%] [G loss: 3.618977]\n",
      "epoch:7 step:6053 [D loss: 0.064234, acc.: 98.44%] [G loss: 4.245668]\n",
      "epoch:7 step:6054 [D loss: 0.407381, acc.: 78.91%] [G loss: 1.668282]\n",
      "epoch:7 step:6055 [D loss: 0.441981, acc.: 79.69%] [G loss: 4.820624]\n",
      "epoch:7 step:6056 [D loss: 0.997043, acc.: 54.69%] [G loss: 2.058913]\n",
      "epoch:7 step:6057 [D loss: 0.140044, acc.: 96.88%] [G loss: 2.542885]\n",
      "epoch:7 step:6058 [D loss: 0.099175, acc.: 99.22%] [G loss: 3.379283]\n",
      "epoch:7 step:6059 [D loss: 0.297170, acc.: 88.28%] [G loss: 4.321517]\n",
      "epoch:7 step:6060 [D loss: 0.529046, acc.: 74.22%] [G loss: 1.152757]\n",
      "epoch:7 step:6061 [D loss: 0.319837, acc.: 84.38%] [G loss: 3.517661]\n",
      "epoch:7 step:6062 [D loss: 0.035178, acc.: 99.22%] [G loss: 4.673974]\n",
      "epoch:7 step:6063 [D loss: 0.446269, acc.: 81.25%] [G loss: 3.086329]\n",
      "epoch:7 step:6064 [D loss: 0.048932, acc.: 99.22%] [G loss: 4.400868]\n",
      "epoch:7 step:6065 [D loss: 0.180473, acc.: 93.75%] [G loss: 2.882449]\n",
      "epoch:7 step:6066 [D loss: 0.181490, acc.: 92.97%] [G loss: 3.699827]\n",
      "epoch:7 step:6067 [D loss: 0.106338, acc.: 96.88%] [G loss: 3.888328]\n",
      "epoch:7 step:6068 [D loss: 0.112123, acc.: 99.22%] [G loss: 2.468246]\n",
      "epoch:7 step:6069 [D loss: 0.225137, acc.: 92.19%] [G loss: 3.779008]\n",
      "epoch:7 step:6070 [D loss: 0.331796, acc.: 84.38%] [G loss: 3.911995]\n",
      "epoch:7 step:6071 [D loss: 0.366104, acc.: 80.47%] [G loss: 2.450679]\n",
      "epoch:7 step:6072 [D loss: 0.164929, acc.: 93.75%] [G loss: 4.043744]\n",
      "epoch:7 step:6073 [D loss: 0.054863, acc.: 99.22%] [G loss: 4.627188]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:7 step:6074 [D loss: 0.454829, acc.: 81.25%] [G loss: 2.237305]\n",
      "epoch:7 step:6075 [D loss: 0.203505, acc.: 94.53%] [G loss: 3.878519]\n",
      "epoch:7 step:6076 [D loss: 0.203536, acc.: 93.75%] [G loss: 3.922051]\n",
      "epoch:7 step:6077 [D loss: 0.369060, acc.: 83.59%] [G loss: 4.124649]\n",
      "epoch:7 step:6078 [D loss: 0.047145, acc.: 99.22%] [G loss: 4.419594]\n",
      "epoch:7 step:6079 [D loss: 0.233233, acc.: 92.97%] [G loss: 4.259564]\n",
      "epoch:7 step:6080 [D loss: 0.915468, acc.: 50.78%] [G loss: 3.932698]\n",
      "epoch:7 step:6081 [D loss: 0.081009, acc.: 98.44%] [G loss: 4.844547]\n",
      "epoch:7 step:6082 [D loss: 0.328008, acc.: 81.25%] [G loss: 2.286874]\n",
      "epoch:7 step:6083 [D loss: 0.422693, acc.: 81.25%] [G loss: 5.218268]\n",
      "epoch:7 step:6084 [D loss: 0.223620, acc.: 92.19%] [G loss: 4.386996]\n",
      "epoch:7 step:6085 [D loss: 0.177720, acc.: 92.97%] [G loss: 1.816507]\n",
      "epoch:7 step:6086 [D loss: 0.221037, acc.: 89.06%] [G loss: 2.961842]\n",
      "epoch:7 step:6087 [D loss: 0.045963, acc.: 99.22%] [G loss: 3.566066]\n",
      "epoch:7 step:6088 [D loss: 0.043398, acc.: 99.22%] [G loss: 2.344696]\n",
      "epoch:7 step:6089 [D loss: 0.108658, acc.: 96.88%] [G loss: 1.602116]\n",
      "epoch:7 step:6090 [D loss: 1.059143, acc.: 50.78%] [G loss: 5.231395]\n",
      "epoch:7 step:6091 [D loss: 0.447576, acc.: 77.34%] [G loss: 4.094581]\n",
      "epoch:7 step:6092 [D loss: 0.421150, acc.: 79.69%] [G loss: 0.504688]\n",
      "epoch:7 step:6093 [D loss: 0.328029, acc.: 79.69%] [G loss: 3.402511]\n",
      "epoch:7 step:6094 [D loss: 0.061623, acc.: 99.22%] [G loss: 4.171168]\n",
      "epoch:7 step:6095 [D loss: 0.214141, acc.: 91.41%] [G loss: 1.492661]\n",
      "epoch:7 step:6096 [D loss: 0.125421, acc.: 97.66%] [G loss: 2.785307]\n",
      "epoch:7 step:6097 [D loss: 0.027844, acc.: 100.00%] [G loss: 2.347364]\n",
      "epoch:7 step:6098 [D loss: 0.207410, acc.: 92.97%] [G loss: 2.674215]\n",
      "epoch:7 step:6099 [D loss: 0.026250, acc.: 100.00%] [G loss: 3.019137]\n",
      "epoch:7 step:6100 [D loss: 0.219214, acc.: 91.41%] [G loss: 2.867418]\n",
      "epoch:7 step:6101 [D loss: 0.043715, acc.: 100.00%] [G loss: 2.773099]\n",
      "epoch:7 step:6102 [D loss: 0.266496, acc.: 86.72%] [G loss: 3.952759]\n",
      "epoch:7 step:6103 [D loss: 0.286384, acc.: 86.72%] [G loss: 3.389001]\n",
      "epoch:7 step:6104 [D loss: 0.341441, acc.: 86.72%] [G loss: 3.693894]\n",
      "epoch:7 step:6105 [D loss: 0.058218, acc.: 100.00%] [G loss: 4.075572]\n",
      "epoch:7 step:6106 [D loss: 0.334106, acc.: 82.81%] [G loss: 1.273652]\n",
      "epoch:7 step:6107 [D loss: 0.319471, acc.: 82.81%] [G loss: 4.964696]\n",
      "epoch:7 step:6108 [D loss: 0.110034, acc.: 97.66%] [G loss: 5.333111]\n",
      "epoch:7 step:6109 [D loss: 0.980370, acc.: 46.88%] [G loss: 3.644131]\n",
      "epoch:7 step:6110 [D loss: 0.035106, acc.: 100.00%] [G loss: 4.397796]\n",
      "epoch:7 step:6111 [D loss: 0.101633, acc.: 98.44%] [G loss: 3.151844]\n",
      "epoch:7 step:6112 [D loss: 0.097129, acc.: 96.88%] [G loss: 2.301888]\n",
      "epoch:7 step:6113 [D loss: 0.248147, acc.: 88.28%] [G loss: 3.571776]\n",
      "epoch:7 step:6114 [D loss: 0.113097, acc.: 97.66%] [G loss: 4.025391]\n",
      "epoch:7 step:6115 [D loss: 0.156647, acc.: 95.31%] [G loss: 1.799911]\n",
      "epoch:7 step:6116 [D loss: 0.231540, acc.: 92.97%] [G loss: 1.283919]\n",
      "epoch:7 step:6117 [D loss: 0.130415, acc.: 96.88%] [G loss: 4.298590]\n",
      "epoch:7 step:6118 [D loss: 0.030317, acc.: 100.00%] [G loss: 4.531543]\n",
      "epoch:7 step:6119 [D loss: 0.249697, acc.: 87.50%] [G loss: 0.796341]\n",
      "epoch:7 step:6120 [D loss: 0.603083, acc.: 65.62%] [G loss: 5.937405]\n",
      "epoch:7 step:6121 [D loss: 0.546908, acc.: 71.88%] [G loss: 4.583096]\n",
      "epoch:7 step:6122 [D loss: 0.095968, acc.: 96.88%] [G loss: 2.760393]\n",
      "epoch:7 step:6123 [D loss: 0.142013, acc.: 95.31%] [G loss: 1.436565]\n",
      "epoch:7 step:6124 [D loss: 0.198058, acc.: 92.19%] [G loss: 3.378664]\n",
      "epoch:7 step:6125 [D loss: 0.295762, acc.: 85.94%] [G loss: 2.613421]\n",
      "epoch:7 step:6126 [D loss: 0.070224, acc.: 99.22%] [G loss: 3.515378]\n",
      "epoch:7 step:6127 [D loss: 0.119710, acc.: 96.09%] [G loss: 2.736473]\n",
      "epoch:7 step:6128 [D loss: 0.194369, acc.: 96.88%] [G loss: 4.265688]\n",
      "epoch:7 step:6129 [D loss: 0.150146, acc.: 94.53%] [G loss: 3.636394]\n",
      "epoch:7 step:6130 [D loss: 0.225261, acc.: 92.97%] [G loss: 5.113951]\n",
      "epoch:7 step:6131 [D loss: 0.204033, acc.: 91.41%] [G loss: 3.752037]\n",
      "epoch:7 step:6132 [D loss: 0.315121, acc.: 85.94%] [G loss: 4.927060]\n",
      "epoch:7 step:6133 [D loss: 0.122123, acc.: 96.09%] [G loss: 4.676620]\n",
      "epoch:7 step:6134 [D loss: 0.364629, acc.: 83.59%] [G loss: 2.301591]\n",
      "epoch:7 step:6135 [D loss: 0.259360, acc.: 87.50%] [G loss: 4.541073]\n",
      "epoch:7 step:6136 [D loss: 0.254018, acc.: 89.06%] [G loss: 3.774711]\n",
      "epoch:7 step:6137 [D loss: 0.412530, acc.: 80.47%] [G loss: 6.353700]\n",
      "epoch:7 step:6138 [D loss: 0.233910, acc.: 88.28%] [G loss: 4.804052]\n",
      "epoch:7 step:6139 [D loss: 0.020596, acc.: 100.00%] [G loss: 4.749995]\n",
      "epoch:7 step:6140 [D loss: 0.117059, acc.: 96.88%] [G loss: 3.433520]\n",
      "epoch:7 step:6141 [D loss: 0.058314, acc.: 99.22%] [G loss: 3.560081]\n",
      "epoch:7 step:6142 [D loss: 0.078971, acc.: 98.44%] [G loss: 3.100484]\n",
      "epoch:7 step:6143 [D loss: 0.117415, acc.: 96.88%] [G loss: 3.308530]\n",
      "epoch:7 step:6144 [D loss: 0.065779, acc.: 100.00%] [G loss: 2.271050]\n",
      "epoch:7 step:6145 [D loss: 0.184951, acc.: 93.75%] [G loss: 2.553643]\n",
      "epoch:7 step:6146 [D loss: 0.088628, acc.: 96.88%] [G loss: 2.635031]\n",
      "epoch:7 step:6147 [D loss: 0.080581, acc.: 98.44%] [G loss: 1.512390]\n",
      "epoch:7 step:6148 [D loss: 0.057638, acc.: 100.00%] [G loss: 1.402967]\n",
      "epoch:7 step:6149 [D loss: 1.168021, acc.: 50.78%] [G loss: 8.021165]\n",
      "epoch:7 step:6150 [D loss: 2.405985, acc.: 50.00%] [G loss: 3.378251]\n",
      "epoch:7 step:6151 [D loss: 0.187914, acc.: 94.53%] [G loss: 1.747093]\n",
      "epoch:7 step:6152 [D loss: 0.243961, acc.: 93.75%] [G loss: 2.957108]\n",
      "epoch:7 step:6153 [D loss: 0.467931, acc.: 78.12%] [G loss: 3.635697]\n",
      "epoch:7 step:6154 [D loss: 0.514069, acc.: 76.56%] [G loss: 3.002813]\n",
      "epoch:7 step:6155 [D loss: 0.091389, acc.: 99.22%] [G loss: 3.659019]\n",
      "epoch:7 step:6156 [D loss: 0.214530, acc.: 96.88%] [G loss: 3.362206]\n",
      "epoch:7 step:6157 [D loss: 0.303726, acc.: 85.94%] [G loss: 3.577271]\n",
      "epoch:7 step:6158 [D loss: 0.175451, acc.: 94.53%] [G loss: 3.576952]\n",
      "epoch:7 step:6159 [D loss: 0.200017, acc.: 97.66%] [G loss: 3.036144]\n",
      "epoch:7 step:6160 [D loss: 0.064683, acc.: 100.00%] [G loss: 3.446124]\n",
      "epoch:7 step:6161 [D loss: 0.279056, acc.: 87.50%] [G loss: 5.155912]\n",
      "epoch:7 step:6162 [D loss: 0.365435, acc.: 81.25%] [G loss: 3.275888]\n",
      "epoch:7 step:6163 [D loss: 0.276319, acc.: 87.50%] [G loss: 4.703366]\n",
      "epoch:7 step:6164 [D loss: 0.193174, acc.: 90.62%] [G loss: 4.160683]\n",
      "epoch:7 step:6165 [D loss: 0.168343, acc.: 93.75%] [G loss: 3.485206]\n",
      "epoch:7 step:6166 [D loss: 0.046739, acc.: 100.00%] [G loss: 4.036176]\n",
      "epoch:7 step:6167 [D loss: 0.093952, acc.: 98.44%] [G loss: 3.316183]\n",
      "epoch:7 step:6168 [D loss: 0.062454, acc.: 98.44%] [G loss: 3.731527]\n",
      "epoch:7 step:6169 [D loss: 0.053528, acc.: 100.00%] [G loss: 2.759500]\n",
      "epoch:7 step:6170 [D loss: 0.291789, acc.: 87.50%] [G loss: 4.491671]\n",
      "epoch:7 step:6171 [D loss: 0.541467, acc.: 73.44%] [G loss: 2.410345]\n",
      "epoch:7 step:6172 [D loss: 0.082074, acc.: 98.44%] [G loss: 4.595319]\n",
      "epoch:7 step:6173 [D loss: 0.042713, acc.: 100.00%] [G loss: 4.257090]\n",
      "epoch:7 step:6174 [D loss: 0.194366, acc.: 95.31%] [G loss: 2.136831]\n",
      "epoch:7 step:6175 [D loss: 0.070430, acc.: 99.22%] [G loss: 3.411857]\n",
      "epoch:7 step:6176 [D loss: 0.129332, acc.: 96.88%] [G loss: 3.917147]\n",
      "epoch:7 step:6177 [D loss: 0.222390, acc.: 92.97%] [G loss: 2.403718]\n",
      "epoch:7 step:6178 [D loss: 0.100931, acc.: 98.44%] [G loss: 2.538186]\n",
      "epoch:7 step:6179 [D loss: 0.166157, acc.: 92.97%] [G loss: 3.166632]\n",
      "epoch:7 step:6180 [D loss: 1.132532, acc.: 42.19%] [G loss: 3.684570]\n",
      "epoch:7 step:6181 [D loss: 0.169431, acc.: 92.19%] [G loss: 4.116876]\n",
      "epoch:7 step:6182 [D loss: 0.485773, acc.: 73.44%] [G loss: 3.361152]\n",
      "epoch:7 step:6183 [D loss: 0.063152, acc.: 99.22%] [G loss: 3.090173]\n",
      "epoch:7 step:6184 [D loss: 0.046467, acc.: 99.22%] [G loss: 3.449924]\n",
      "epoch:7 step:6185 [D loss: 0.082058, acc.: 99.22%] [G loss: 3.340766]\n",
      "epoch:7 step:6186 [D loss: 0.137633, acc.: 94.53%] [G loss: 2.376543]\n",
      "epoch:7 step:6187 [D loss: 0.082228, acc.: 98.44%] [G loss: 1.937022]\n",
      "epoch:7 step:6188 [D loss: 0.668975, acc.: 67.19%] [G loss: 4.354746]\n",
      "epoch:7 step:6189 [D loss: 0.131479, acc.: 93.75%] [G loss: 5.477448]\n",
      "epoch:7 step:6190 [D loss: 0.108135, acc.: 96.09%] [G loss: 2.815118]\n",
      "epoch:7 step:6191 [D loss: 0.133352, acc.: 96.09%] [G loss: 1.597508]\n",
      "epoch:7 step:6192 [D loss: 0.182525, acc.: 92.97%] [G loss: 1.532320]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:7 step:6193 [D loss: 0.023672, acc.: 100.00%] [G loss: 1.990464]\n",
      "epoch:7 step:6194 [D loss: 0.063805, acc.: 99.22%] [G loss: 0.725843]\n",
      "epoch:7 step:6195 [D loss: 0.445156, acc.: 80.47%] [G loss: 6.478448]\n",
      "epoch:7 step:6196 [D loss: 1.267198, acc.: 45.31%] [G loss: 5.681965]\n",
      "epoch:7 step:6197 [D loss: 0.160087, acc.: 91.41%] [G loss: 5.226913]\n",
      "epoch:7 step:6198 [D loss: 0.250017, acc.: 89.06%] [G loss: 2.424584]\n",
      "epoch:7 step:6199 [D loss: 0.313611, acc.: 82.03%] [G loss: 4.557491]\n",
      "epoch:7 step:6200 [D loss: 0.126686, acc.: 96.09%] [G loss: 4.907515]\n",
      "##############\n",
      "[0.95809417 0.93853533 0.99479456 0.91166001 1.10154218 2.09284515\n",
      " 0.99969827 0.83144027 2.1096192  1.10888809]\n",
      "##########\n",
      "epoch:7 step:6201 [D loss: 0.104746, acc.: 96.09%] [G loss: 3.837539]\n",
      "epoch:7 step:6202 [D loss: 0.050323, acc.: 100.00%] [G loss: 1.724309]\n",
      "epoch:7 step:6203 [D loss: 0.073066, acc.: 100.00%] [G loss: 2.455519]\n",
      "epoch:7 step:6204 [D loss: 0.036695, acc.: 100.00%] [G loss: 1.701478]\n",
      "epoch:7 step:6205 [D loss: 0.234956, acc.: 89.84%] [G loss: 3.184681]\n",
      "epoch:7 step:6206 [D loss: 0.111445, acc.: 96.09%] [G loss: 3.302127]\n",
      "epoch:7 step:6207 [D loss: 0.098592, acc.: 99.22%] [G loss: 1.446561]\n",
      "epoch:7 step:6208 [D loss: 0.051659, acc.: 100.00%] [G loss: 1.022984]\n",
      "epoch:7 step:6209 [D loss: 0.096557, acc.: 95.31%] [G loss: 0.256539]\n",
      "epoch:7 step:6210 [D loss: 0.176324, acc.: 93.75%] [G loss: 1.968785]\n",
      "epoch:7 step:6211 [D loss: 0.102115, acc.: 98.44%] [G loss: 1.439357]\n",
      "epoch:7 step:6212 [D loss: 0.139765, acc.: 97.66%] [G loss: 1.427837]\n",
      "epoch:7 step:6213 [D loss: 0.075890, acc.: 99.22%] [G loss: 1.221313]\n",
      "epoch:7 step:6214 [D loss: 0.091614, acc.: 99.22%] [G loss: 1.871223]\n",
      "epoch:7 step:6215 [D loss: 0.088891, acc.: 97.66%] [G loss: 0.652593]\n",
      "epoch:7 step:6216 [D loss: 0.069822, acc.: 100.00%] [G loss: 0.615346]\n",
      "epoch:7 step:6217 [D loss: 0.478898, acc.: 76.56%] [G loss: 5.858352]\n",
      "epoch:7 step:6218 [D loss: 0.669329, acc.: 70.31%] [G loss: 2.384844]\n",
      "epoch:7 step:6219 [D loss: 0.332544, acc.: 85.94%] [G loss: 4.479403]\n",
      "epoch:7 step:6220 [D loss: 0.826053, acc.: 59.38%] [G loss: 2.202472]\n",
      "epoch:7 step:6221 [D loss: 0.147080, acc.: 95.31%] [G loss: 3.588464]\n",
      "epoch:7 step:6222 [D loss: 0.108444, acc.: 96.88%] [G loss: 3.578257]\n",
      "epoch:7 step:6223 [D loss: 0.193149, acc.: 95.31%] [G loss: 4.856836]\n",
      "epoch:7 step:6224 [D loss: 0.343493, acc.: 83.59%] [G loss: 2.817664]\n",
      "epoch:7 step:6225 [D loss: 0.048640, acc.: 100.00%] [G loss: 3.730569]\n",
      "epoch:7 step:6226 [D loss: 0.178393, acc.: 95.31%] [G loss: 3.866511]\n",
      "epoch:7 step:6227 [D loss: 0.074995, acc.: 99.22%] [G loss: 4.209688]\n",
      "epoch:7 step:6228 [D loss: 0.202378, acc.: 92.97%] [G loss: 3.092331]\n",
      "epoch:7 step:6229 [D loss: 1.138282, acc.: 49.22%] [G loss: 7.005193]\n",
      "epoch:7 step:6230 [D loss: 0.982576, acc.: 57.81%] [G loss: 5.375167]\n",
      "epoch:7 step:6231 [D loss: 0.415737, acc.: 83.59%] [G loss: 2.385525]\n",
      "epoch:7 step:6232 [D loss: 0.713025, acc.: 64.06%] [G loss: 6.359985]\n",
      "epoch:7 step:6233 [D loss: 0.511427, acc.: 74.22%] [G loss: 6.083791]\n",
      "epoch:7 step:6234 [D loss: 0.139334, acc.: 95.31%] [G loss: 3.809170]\n",
      "epoch:7 step:6235 [D loss: 0.078205, acc.: 99.22%] [G loss: 3.318052]\n",
      "epoch:7 step:6236 [D loss: 0.044987, acc.: 100.00%] [G loss: 3.218218]\n",
      "epoch:7 step:6237 [D loss: 0.092419, acc.: 99.22%] [G loss: 3.079719]\n",
      "epoch:7 step:6238 [D loss: 0.088608, acc.: 98.44%] [G loss: 3.535282]\n",
      "epoch:7 step:6239 [D loss: 0.118833, acc.: 96.09%] [G loss: 2.980986]\n",
      "epoch:7 step:6240 [D loss: 0.068265, acc.: 100.00%] [G loss: 3.288908]\n",
      "epoch:7 step:6241 [D loss: 0.146450, acc.: 96.88%] [G loss: 2.669546]\n",
      "epoch:7 step:6242 [D loss: 0.157838, acc.: 96.88%] [G loss: 1.773809]\n",
      "epoch:7 step:6243 [D loss: 0.112213, acc.: 97.66%] [G loss: 4.241964]\n",
      "epoch:7 step:6244 [D loss: 1.050256, acc.: 45.31%] [G loss: 6.216727]\n",
      "epoch:7 step:6245 [D loss: 0.188701, acc.: 90.62%] [G loss: 6.074732]\n",
      "epoch:7 step:6246 [D loss: 1.567952, acc.: 28.91%] [G loss: 4.565591]\n",
      "epoch:7 step:6247 [D loss: 0.086425, acc.: 96.09%] [G loss: 5.150002]\n",
      "epoch:7 step:6248 [D loss: 0.092595, acc.: 96.88%] [G loss: 3.589720]\n",
      "epoch:8 step:6249 [D loss: 0.227541, acc.: 92.19%] [G loss: 3.041766]\n",
      "epoch:8 step:6250 [D loss: 0.158783, acc.: 94.53%] [G loss: 3.826931]\n",
      "epoch:8 step:6251 [D loss: 0.253577, acc.: 95.31%] [G loss: 4.167509]\n",
      "epoch:8 step:6252 [D loss: 0.103630, acc.: 97.66%] [G loss: 5.070159]\n",
      "epoch:8 step:6253 [D loss: 0.143112, acc.: 96.88%] [G loss: 3.437866]\n",
      "epoch:8 step:6254 [D loss: 0.135467, acc.: 97.66%] [G loss: 3.505229]\n",
      "epoch:8 step:6255 [D loss: 0.388442, acc.: 82.03%] [G loss: 4.539630]\n",
      "epoch:8 step:6256 [D loss: 0.128587, acc.: 97.66%] [G loss: 4.718020]\n",
      "epoch:8 step:6257 [D loss: 0.690184, acc.: 59.38%] [G loss: 3.956575]\n",
      "epoch:8 step:6258 [D loss: 0.125302, acc.: 96.88%] [G loss: 4.293361]\n",
      "epoch:8 step:6259 [D loss: 0.064932, acc.: 98.44%] [G loss: 3.459497]\n",
      "epoch:8 step:6260 [D loss: 0.098930, acc.: 99.22%] [G loss: 1.498371]\n",
      "epoch:8 step:6261 [D loss: 0.074041, acc.: 98.44%] [G loss: 2.206463]\n",
      "epoch:8 step:6262 [D loss: 0.351326, acc.: 86.72%] [G loss: 5.191875]\n",
      "epoch:8 step:6263 [D loss: 0.511969, acc.: 77.34%] [G loss: 4.072163]\n",
      "epoch:8 step:6264 [D loss: 0.203545, acc.: 94.53%] [G loss: 3.572370]\n",
      "epoch:8 step:6265 [D loss: 0.192860, acc.: 92.97%] [G loss: 3.823046]\n",
      "epoch:8 step:6266 [D loss: 0.102130, acc.: 97.66%] [G loss: 4.087905]\n",
      "epoch:8 step:6267 [D loss: 0.322807, acc.: 84.38%] [G loss: 1.618404]\n",
      "epoch:8 step:6268 [D loss: 0.263663, acc.: 87.50%] [G loss: 4.464736]\n",
      "epoch:8 step:6269 [D loss: 0.069736, acc.: 99.22%] [G loss: 5.308358]\n",
      "epoch:8 step:6270 [D loss: 0.418680, acc.: 80.47%] [G loss: 3.773019]\n",
      "epoch:8 step:6271 [D loss: 0.163180, acc.: 95.31%] [G loss: 3.333870]\n",
      "epoch:8 step:6272 [D loss: 0.143623, acc.: 96.09%] [G loss: 4.198521]\n",
      "epoch:8 step:6273 [D loss: 0.135153, acc.: 95.31%] [G loss: 3.698176]\n",
      "epoch:8 step:6274 [D loss: 0.049299, acc.: 100.00%] [G loss: 2.579839]\n",
      "epoch:8 step:6275 [D loss: 0.218699, acc.: 92.19%] [G loss: 5.281580]\n",
      "epoch:8 step:6276 [D loss: 0.571582, acc.: 73.44%] [G loss: 3.157495]\n",
      "epoch:8 step:6277 [D loss: 0.146870, acc.: 96.88%] [G loss: 4.772912]\n",
      "epoch:8 step:6278 [D loss: 0.250560, acc.: 91.41%] [G loss: 3.701620]\n",
      "epoch:8 step:6279 [D loss: 0.112898, acc.: 98.44%] [G loss: 4.406444]\n",
      "epoch:8 step:6280 [D loss: 0.081064, acc.: 99.22%] [G loss: 4.099833]\n",
      "epoch:8 step:6281 [D loss: 0.159208, acc.: 96.09%] [G loss: 3.039994]\n",
      "epoch:8 step:6282 [D loss: 0.110324, acc.: 98.44%] [G loss: 3.960194]\n",
      "epoch:8 step:6283 [D loss: 0.106121, acc.: 99.22%] [G loss: 3.325760]\n",
      "epoch:8 step:6284 [D loss: 0.138372, acc.: 95.31%] [G loss: 1.796131]\n",
      "epoch:8 step:6285 [D loss: 0.137723, acc.: 96.09%] [G loss: 3.514815]\n",
      "epoch:8 step:6286 [D loss: 0.129141, acc.: 95.31%] [G loss: 3.465259]\n",
      "epoch:8 step:6287 [D loss: 0.104548, acc.: 97.66%] [G loss: 2.493111]\n",
      "epoch:8 step:6288 [D loss: 0.171407, acc.: 92.19%] [G loss: 4.629541]\n",
      "epoch:8 step:6289 [D loss: 0.070407, acc.: 100.00%] [G loss: 4.735563]\n",
      "epoch:8 step:6290 [D loss: 0.209569, acc.: 92.19%] [G loss: 5.139645]\n",
      "epoch:8 step:6291 [D loss: 0.105696, acc.: 96.09%] [G loss: 3.959081]\n",
      "epoch:8 step:6292 [D loss: 0.265142, acc.: 89.06%] [G loss: 2.728579]\n",
      "epoch:8 step:6293 [D loss: 0.094055, acc.: 96.88%] [G loss: 4.441106]\n",
      "epoch:8 step:6294 [D loss: 0.132561, acc.: 98.44%] [G loss: 4.338577]\n",
      "epoch:8 step:6295 [D loss: 0.212504, acc.: 92.19%] [G loss: 4.063169]\n",
      "epoch:8 step:6296 [D loss: 0.077903, acc.: 99.22%] [G loss: 3.381615]\n",
      "epoch:8 step:6297 [D loss: 0.187799, acc.: 95.31%] [G loss: 3.424633]\n",
      "epoch:8 step:6298 [D loss: 0.120668, acc.: 98.44%] [G loss: 3.065279]\n",
      "epoch:8 step:6299 [D loss: 0.018859, acc.: 100.00%] [G loss: 2.304143]\n",
      "epoch:8 step:6300 [D loss: 0.060711, acc.: 99.22%] [G loss: 1.638252]\n",
      "epoch:8 step:6301 [D loss: 0.060107, acc.: 100.00%] [G loss: 0.537749]\n",
      "epoch:8 step:6302 [D loss: 0.029611, acc.: 100.00%] [G loss: 0.626063]\n",
      "epoch:8 step:6303 [D loss: 0.372648, acc.: 78.91%] [G loss: 6.864601]\n",
      "epoch:8 step:6304 [D loss: 2.043273, acc.: 49.22%] [G loss: 1.449851]\n",
      "epoch:8 step:6305 [D loss: 0.652794, acc.: 71.88%] [G loss: 5.918255]\n",
      "epoch:8 step:6306 [D loss: 0.265654, acc.: 89.06%] [G loss: 6.076276]\n",
      "epoch:8 step:6307 [D loss: 0.064994, acc.: 98.44%] [G loss: 4.799105]\n",
      "epoch:8 step:6308 [D loss: 0.011903, acc.: 100.00%] [G loss: 4.466119]\n",
      "epoch:8 step:6309 [D loss: 0.090862, acc.: 98.44%] [G loss: 3.668226]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:8 step:6310 [D loss: 0.016851, acc.: 100.00%] [G loss: 3.892113]\n",
      "epoch:8 step:6311 [D loss: 0.085773, acc.: 97.66%] [G loss: 2.414445]\n",
      "epoch:8 step:6312 [D loss: 0.104094, acc.: 97.66%] [G loss: 2.026011]\n",
      "epoch:8 step:6313 [D loss: 0.447177, acc.: 79.69%] [G loss: 3.796598]\n",
      "epoch:8 step:6314 [D loss: 0.074900, acc.: 98.44%] [G loss: 3.249363]\n",
      "epoch:8 step:6315 [D loss: 0.073905, acc.: 97.66%] [G loss: 3.507543]\n",
      "epoch:8 step:6316 [D loss: 0.304016, acc.: 84.38%] [G loss: 4.063414]\n",
      "epoch:8 step:6317 [D loss: 0.133563, acc.: 96.09%] [G loss: 3.613557]\n",
      "epoch:8 step:6318 [D loss: 0.093318, acc.: 96.88%] [G loss: 2.619634]\n",
      "epoch:8 step:6319 [D loss: 0.123862, acc.: 96.88%] [G loss: 3.187438]\n",
      "epoch:8 step:6320 [D loss: 0.039045, acc.: 100.00%] [G loss: 2.823853]\n",
      "epoch:8 step:6321 [D loss: 0.096859, acc.: 96.88%] [G loss: 1.207193]\n",
      "epoch:8 step:6322 [D loss: 0.147685, acc.: 94.53%] [G loss: 3.655174]\n",
      "epoch:8 step:6323 [D loss: 0.135375, acc.: 95.31%] [G loss: 2.520722]\n",
      "epoch:8 step:6324 [D loss: 0.350288, acc.: 86.72%] [G loss: 5.143574]\n",
      "epoch:8 step:6325 [D loss: 0.404132, acc.: 78.91%] [G loss: 2.997345]\n",
      "epoch:8 step:6326 [D loss: 0.068350, acc.: 100.00%] [G loss: 2.298702]\n",
      "epoch:8 step:6327 [D loss: 0.093022, acc.: 100.00%] [G loss: 2.872769]\n",
      "epoch:8 step:6328 [D loss: 0.012160, acc.: 100.00%] [G loss: 1.902736]\n",
      "epoch:8 step:6329 [D loss: 0.419751, acc.: 78.91%] [G loss: 5.284978]\n",
      "epoch:8 step:6330 [D loss: 0.336240, acc.: 85.94%] [G loss: 3.866031]\n",
      "epoch:8 step:6331 [D loss: 0.126241, acc.: 96.88%] [G loss: 3.161664]\n",
      "epoch:8 step:6332 [D loss: 0.174511, acc.: 93.75%] [G loss: 1.765346]\n",
      "epoch:8 step:6333 [D loss: 0.101748, acc.: 96.88%] [G loss: 3.458949]\n",
      "epoch:8 step:6334 [D loss: 0.024455, acc.: 100.00%] [G loss: 4.255396]\n",
      "epoch:8 step:6335 [D loss: 0.032052, acc.: 99.22%] [G loss: 2.838839]\n",
      "epoch:8 step:6336 [D loss: 0.372507, acc.: 82.03%] [G loss: 6.604561]\n",
      "epoch:8 step:6337 [D loss: 0.417877, acc.: 77.34%] [G loss: 4.288805]\n",
      "epoch:8 step:6338 [D loss: 1.471430, acc.: 39.84%] [G loss: 7.588806]\n",
      "epoch:8 step:6339 [D loss: 0.908014, acc.: 60.94%] [G loss: 5.411450]\n",
      "epoch:8 step:6340 [D loss: 0.015984, acc.: 100.00%] [G loss: 3.827526]\n",
      "epoch:8 step:6341 [D loss: 0.105749, acc.: 96.88%] [G loss: 3.883655]\n",
      "epoch:8 step:6342 [D loss: 0.041691, acc.: 99.22%] [G loss: 2.834372]\n",
      "epoch:8 step:6343 [D loss: 0.335004, acc.: 85.16%] [G loss: 4.393371]\n",
      "epoch:8 step:6344 [D loss: 0.152643, acc.: 94.53%] [G loss: 2.802417]\n",
      "epoch:8 step:6345 [D loss: 0.117483, acc.: 96.88%] [G loss: 2.745002]\n",
      "epoch:8 step:6346 [D loss: 0.104586, acc.: 98.44%] [G loss: 1.699265]\n",
      "epoch:8 step:6347 [D loss: 0.396276, acc.: 78.91%] [G loss: 5.145366]\n",
      "epoch:8 step:6348 [D loss: 0.601670, acc.: 69.53%] [G loss: 4.303991]\n",
      "epoch:8 step:6349 [D loss: 0.107029, acc.: 99.22%] [G loss: 1.873364]\n",
      "epoch:8 step:6350 [D loss: 0.255171, acc.: 85.94%] [G loss: 4.360177]\n",
      "epoch:8 step:6351 [D loss: 0.092444, acc.: 96.09%] [G loss: 4.523335]\n",
      "epoch:8 step:6352 [D loss: 0.049912, acc.: 99.22%] [G loss: 3.421801]\n",
      "epoch:8 step:6353 [D loss: 0.106757, acc.: 96.88%] [G loss: 2.974768]\n",
      "epoch:8 step:6354 [D loss: 0.049182, acc.: 99.22%] [G loss: 3.252098]\n",
      "epoch:8 step:6355 [D loss: 0.381274, acc.: 81.25%] [G loss: 5.022296]\n",
      "epoch:8 step:6356 [D loss: 0.465940, acc.: 76.56%] [G loss: 2.363495]\n",
      "epoch:8 step:6357 [D loss: 0.310005, acc.: 85.16%] [G loss: 4.902524]\n",
      "epoch:8 step:6358 [D loss: 0.121529, acc.: 95.31%] [G loss: 5.332114]\n",
      "epoch:8 step:6359 [D loss: 0.410861, acc.: 80.47%] [G loss: 1.832666]\n",
      "epoch:8 step:6360 [D loss: 0.326125, acc.: 80.47%] [G loss: 4.891284]\n",
      "epoch:8 step:6361 [D loss: 0.065391, acc.: 99.22%] [G loss: 5.391528]\n",
      "epoch:8 step:6362 [D loss: 0.448512, acc.: 80.47%] [G loss: 1.519968]\n",
      "epoch:8 step:6363 [D loss: 0.084258, acc.: 99.22%] [G loss: 2.247321]\n",
      "epoch:8 step:6364 [D loss: 0.049330, acc.: 100.00%] [G loss: 2.986499]\n",
      "epoch:8 step:6365 [D loss: 0.106028, acc.: 98.44%] [G loss: 2.055179]\n",
      "epoch:8 step:6366 [D loss: 0.098466, acc.: 98.44%] [G loss: 3.807509]\n",
      "epoch:8 step:6367 [D loss: 0.117134, acc.: 97.66%] [G loss: 2.638343]\n",
      "epoch:8 step:6368 [D loss: 0.191454, acc.: 91.41%] [G loss: 4.437934]\n",
      "epoch:8 step:6369 [D loss: 0.143567, acc.: 94.53%] [G loss: 4.422772]\n",
      "epoch:8 step:6370 [D loss: 0.116722, acc.: 96.88%] [G loss: 2.266491]\n",
      "epoch:8 step:6371 [D loss: 0.404020, acc.: 82.03%] [G loss: 6.715604]\n",
      "epoch:8 step:6372 [D loss: 0.681644, acc.: 72.66%] [G loss: 3.813570]\n",
      "epoch:8 step:6373 [D loss: 0.050242, acc.: 99.22%] [G loss: 1.916059]\n",
      "epoch:8 step:6374 [D loss: 0.199210, acc.: 92.19%] [G loss: 4.729069]\n",
      "epoch:8 step:6375 [D loss: 0.105148, acc.: 96.88%] [G loss: 4.820822]\n",
      "epoch:8 step:6376 [D loss: 0.103121, acc.: 96.88%] [G loss: 2.611595]\n",
      "epoch:8 step:6377 [D loss: 0.157250, acc.: 93.75%] [G loss: 4.600834]\n",
      "epoch:8 step:6378 [D loss: 0.206539, acc.: 92.97%] [G loss: 3.251882]\n",
      "epoch:8 step:6379 [D loss: 0.056994, acc.: 99.22%] [G loss: 3.953675]\n",
      "epoch:8 step:6380 [D loss: 0.266344, acc.: 92.19%] [G loss: 5.135535]\n",
      "epoch:8 step:6381 [D loss: 0.231093, acc.: 90.62%] [G loss: 3.646444]\n",
      "epoch:8 step:6382 [D loss: 0.121553, acc.: 96.88%] [G loss: 3.159721]\n",
      "epoch:8 step:6383 [D loss: 0.081695, acc.: 98.44%] [G loss: 4.589618]\n",
      "epoch:8 step:6384 [D loss: 0.150274, acc.: 96.09%] [G loss: 4.422036]\n",
      "epoch:8 step:6385 [D loss: 0.080680, acc.: 98.44%] [G loss: 2.869980]\n",
      "epoch:8 step:6386 [D loss: 0.171609, acc.: 93.75%] [G loss: 6.196082]\n",
      "epoch:8 step:6387 [D loss: 0.322367, acc.: 85.16%] [G loss: 3.408239]\n",
      "epoch:8 step:6388 [D loss: 0.134757, acc.: 95.31%] [G loss: 4.060148]\n",
      "epoch:8 step:6389 [D loss: 0.040283, acc.: 99.22%] [G loss: 3.783376]\n",
      "epoch:8 step:6390 [D loss: 0.625067, acc.: 66.41%] [G loss: 7.704904]\n",
      "epoch:8 step:6391 [D loss: 1.513919, acc.: 50.00%] [G loss: 4.015422]\n",
      "epoch:8 step:6392 [D loss: 0.087754, acc.: 99.22%] [G loss: 4.450547]\n",
      "epoch:8 step:6393 [D loss: 0.027922, acc.: 100.00%] [G loss: 4.899417]\n",
      "epoch:8 step:6394 [D loss: 0.056288, acc.: 99.22%] [G loss: 3.428736]\n",
      "epoch:8 step:6395 [D loss: 0.191179, acc.: 91.41%] [G loss: 4.452495]\n",
      "epoch:8 step:6396 [D loss: 0.155769, acc.: 96.09%] [G loss: 3.248344]\n",
      "epoch:8 step:6397 [D loss: 0.171410, acc.: 96.09%] [G loss: 3.498259]\n",
      "epoch:8 step:6398 [D loss: 0.143533, acc.: 95.31%] [G loss: 4.211265]\n",
      "epoch:8 step:6399 [D loss: 0.052608, acc.: 98.44%] [G loss: 3.920888]\n",
      "epoch:8 step:6400 [D loss: 0.063151, acc.: 99.22%] [G loss: 3.150633]\n",
      "##############\n",
      "[1.09504913 0.97444072 0.87408905 0.98458784 0.88707061 1.09822159\n",
      " 2.10625423 0.98164287 2.10300016 1.06006175]\n",
      "##########\n",
      "epoch:8 step:6401 [D loss: 0.265407, acc.: 90.62%] [G loss: 4.893815]\n",
      "epoch:8 step:6402 [D loss: 0.092187, acc.: 96.88%] [G loss: 3.885423]\n",
      "epoch:8 step:6403 [D loss: 0.276137, acc.: 89.84%] [G loss: 4.366442]\n",
      "epoch:8 step:6404 [D loss: 0.283402, acc.: 86.72%] [G loss: 1.353431]\n",
      "epoch:8 step:6405 [D loss: 0.180751, acc.: 92.19%] [G loss: 3.482190]\n",
      "epoch:8 step:6406 [D loss: 0.057333, acc.: 98.44%] [G loss: 4.293618]\n",
      "epoch:8 step:6407 [D loss: 0.011835, acc.: 100.00%] [G loss: 2.748534]\n",
      "epoch:8 step:6408 [D loss: 0.299126, acc.: 87.50%] [G loss: 3.776785]\n",
      "epoch:8 step:6409 [D loss: 0.158777, acc.: 94.53%] [G loss: 2.114185]\n",
      "epoch:8 step:6410 [D loss: 0.280731, acc.: 88.28%] [G loss: 6.271769]\n",
      "epoch:8 step:6411 [D loss: 0.291585, acc.: 86.72%] [G loss: 3.866901]\n",
      "epoch:8 step:6412 [D loss: 0.043468, acc.: 99.22%] [G loss: 2.487598]\n",
      "epoch:8 step:6413 [D loss: 0.053353, acc.: 100.00%] [G loss: 2.984397]\n",
      "epoch:8 step:6414 [D loss: 0.161143, acc.: 95.31%] [G loss: 5.918331]\n",
      "epoch:8 step:6415 [D loss: 0.183140, acc.: 90.62%] [G loss: 3.634129]\n",
      "epoch:8 step:6416 [D loss: 0.097451, acc.: 96.88%] [G loss: 3.453461]\n",
      "epoch:8 step:6417 [D loss: 0.063206, acc.: 99.22%] [G loss: 4.215427]\n",
      "epoch:8 step:6418 [D loss: 0.082233, acc.: 99.22%] [G loss: 4.470004]\n",
      "epoch:8 step:6419 [D loss: 0.253739, acc.: 92.19%] [G loss: 4.400053]\n",
      "epoch:8 step:6420 [D loss: 0.104274, acc.: 96.88%] [G loss: 3.553818]\n",
      "epoch:8 step:6421 [D loss: 0.159311, acc.: 94.53%] [G loss: 3.998049]\n",
      "epoch:8 step:6422 [D loss: 0.405849, acc.: 81.25%] [G loss: 5.823623]\n",
      "epoch:8 step:6423 [D loss: 0.131177, acc.: 92.97%] [G loss: 5.499783]\n",
      "epoch:8 step:6424 [D loss: 1.820324, acc.: 27.34%] [G loss: 7.013871]\n",
      "epoch:8 step:6425 [D loss: 1.089591, acc.: 53.12%] [G loss: 2.824359]\n",
      "epoch:8 step:6426 [D loss: 0.149086, acc.: 94.53%] [G loss: 2.905970]\n",
      "epoch:8 step:6427 [D loss: 0.243123, acc.: 92.19%] [G loss: 4.766020]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:8 step:6428 [D loss: 0.344360, acc.: 87.50%] [G loss: 3.211948]\n",
      "epoch:8 step:6429 [D loss: 0.432250, acc.: 77.34%] [G loss: 5.406205]\n",
      "epoch:8 step:6430 [D loss: 0.181848, acc.: 90.62%] [G loss: 5.260866]\n",
      "epoch:8 step:6431 [D loss: 0.196126, acc.: 91.41%] [G loss: 4.270061]\n",
      "epoch:8 step:6432 [D loss: 0.103980, acc.: 95.31%] [G loss: 4.471350]\n",
      "epoch:8 step:6433 [D loss: 0.124277, acc.: 97.66%] [G loss: 3.777973]\n",
      "epoch:8 step:6434 [D loss: 0.176498, acc.: 95.31%] [G loss: 3.600463]\n",
      "epoch:8 step:6435 [D loss: 0.185437, acc.: 92.97%] [G loss: 2.896297]\n",
      "epoch:8 step:6436 [D loss: 0.190505, acc.: 90.62%] [G loss: 4.588066]\n",
      "epoch:8 step:6437 [D loss: 0.185113, acc.: 92.97%] [G loss: 3.108424]\n",
      "epoch:8 step:6438 [D loss: 0.125807, acc.: 96.88%] [G loss: 3.205477]\n",
      "epoch:8 step:6439 [D loss: 0.292933, acc.: 85.94%] [G loss: 4.478242]\n",
      "epoch:8 step:6440 [D loss: 0.300073, acc.: 85.94%] [G loss: 2.063296]\n",
      "epoch:8 step:6441 [D loss: 0.356337, acc.: 83.59%] [G loss: 5.638938]\n",
      "epoch:8 step:6442 [D loss: 0.312045, acc.: 81.25%] [G loss: 3.565405]\n",
      "epoch:8 step:6443 [D loss: 0.086937, acc.: 99.22%] [G loss: 2.632170]\n",
      "epoch:8 step:6444 [D loss: 0.091412, acc.: 98.44%] [G loss: 2.131678]\n",
      "epoch:8 step:6445 [D loss: 0.032010, acc.: 100.00%] [G loss: 2.994991]\n",
      "epoch:8 step:6446 [D loss: 0.083025, acc.: 97.66%] [G loss: 3.864956]\n",
      "epoch:8 step:6447 [D loss: 0.089143, acc.: 99.22%] [G loss: 3.283159]\n",
      "epoch:8 step:6448 [D loss: 0.752044, acc.: 62.50%] [G loss: 5.414721]\n",
      "epoch:8 step:6449 [D loss: 0.180416, acc.: 91.41%] [G loss: 4.791523]\n",
      "epoch:8 step:6450 [D loss: 0.472082, acc.: 74.22%] [G loss: 3.255549]\n",
      "epoch:8 step:6451 [D loss: 0.059270, acc.: 99.22%] [G loss: 3.095749]\n",
      "epoch:8 step:6452 [D loss: 0.083560, acc.: 96.88%] [G loss: 3.707975]\n",
      "epoch:8 step:6453 [D loss: 0.142614, acc.: 94.53%] [G loss: 3.751584]\n",
      "epoch:8 step:6454 [D loss: 0.071095, acc.: 99.22%] [G loss: 3.392536]\n",
      "epoch:8 step:6455 [D loss: 0.158815, acc.: 94.53%] [G loss: 5.274071]\n",
      "epoch:8 step:6456 [D loss: 0.460208, acc.: 75.78%] [G loss: 2.742852]\n",
      "epoch:8 step:6457 [D loss: 0.343590, acc.: 80.47%] [G loss: 6.481194]\n",
      "epoch:8 step:6458 [D loss: 0.567502, acc.: 70.31%] [G loss: 2.974618]\n",
      "epoch:8 step:6459 [D loss: 0.117826, acc.: 93.75%] [G loss: 3.357762]\n",
      "epoch:8 step:6460 [D loss: 0.022033, acc.: 100.00%] [G loss: 3.023844]\n",
      "epoch:8 step:6461 [D loss: 0.436607, acc.: 82.81%] [G loss: 4.934612]\n",
      "epoch:8 step:6462 [D loss: 0.169293, acc.: 92.97%] [G loss: 4.257145]\n",
      "epoch:8 step:6463 [D loss: 0.166169, acc.: 94.53%] [G loss: 3.413043]\n",
      "epoch:8 step:6464 [D loss: 0.032481, acc.: 100.00%] [G loss: 1.715349]\n",
      "epoch:8 step:6465 [D loss: 0.005392, acc.: 100.00%] [G loss: 1.413388]\n",
      "epoch:8 step:6466 [D loss: 0.079107, acc.: 99.22%] [G loss: 0.540489]\n",
      "epoch:8 step:6467 [D loss: 0.136770, acc.: 94.53%] [G loss: 2.016291]\n",
      "epoch:8 step:6468 [D loss: 0.179585, acc.: 90.62%] [G loss: 0.829600]\n",
      "epoch:8 step:6469 [D loss: 0.039853, acc.: 100.00%] [G loss: 0.839233]\n",
      "epoch:8 step:6470 [D loss: 0.034648, acc.: 100.00%] [G loss: 0.690158]\n",
      "epoch:8 step:6471 [D loss: 0.018949, acc.: 100.00%] [G loss: 0.822713]\n",
      "epoch:8 step:6472 [D loss: 0.464370, acc.: 78.12%] [G loss: 6.140756]\n",
      "epoch:8 step:6473 [D loss: 0.771045, acc.: 63.28%] [G loss: 3.164782]\n",
      "epoch:8 step:6474 [D loss: 0.182184, acc.: 93.75%] [G loss: 4.555199]\n",
      "epoch:8 step:6475 [D loss: 0.031076, acc.: 99.22%] [G loss: 4.666667]\n",
      "epoch:8 step:6476 [D loss: 0.515643, acc.: 76.56%] [G loss: 4.368800]\n",
      "epoch:8 step:6477 [D loss: 0.189347, acc.: 92.97%] [G loss: 3.720508]\n",
      "epoch:8 step:6478 [D loss: 0.059831, acc.: 99.22%] [G loss: 4.309353]\n",
      "epoch:8 step:6479 [D loss: 0.047244, acc.: 100.00%] [G loss: 3.399009]\n",
      "epoch:8 step:6480 [D loss: 0.183087, acc.: 94.53%] [G loss: 2.963963]\n",
      "epoch:8 step:6481 [D loss: 0.110006, acc.: 96.09%] [G loss: 4.258411]\n",
      "epoch:8 step:6482 [D loss: 0.448383, acc.: 78.12%] [G loss: 3.066106]\n",
      "epoch:8 step:6483 [D loss: 0.137226, acc.: 95.31%] [G loss: 5.392997]\n",
      "epoch:8 step:6484 [D loss: 0.914211, acc.: 58.59%] [G loss: 2.329717]\n",
      "epoch:8 step:6485 [D loss: 0.211191, acc.: 89.84%] [G loss: 5.514586]\n",
      "epoch:8 step:6486 [D loss: 0.123628, acc.: 95.31%] [G loss: 5.253289]\n",
      "epoch:8 step:6487 [D loss: 0.066982, acc.: 98.44%] [G loss: 3.565906]\n",
      "epoch:8 step:6488 [D loss: 0.103275, acc.: 96.88%] [G loss: 2.495558]\n",
      "epoch:8 step:6489 [D loss: 0.148144, acc.: 96.09%] [G loss: 4.024735]\n",
      "epoch:8 step:6490 [D loss: 0.066694, acc.: 99.22%] [G loss: 2.887023]\n",
      "epoch:8 step:6491 [D loss: 0.167274, acc.: 94.53%] [G loss: 2.492024]\n",
      "epoch:8 step:6492 [D loss: 0.010385, acc.: 100.00%] [G loss: 2.250966]\n",
      "epoch:8 step:6493 [D loss: 0.032871, acc.: 99.22%] [G loss: 2.072681]\n",
      "epoch:8 step:6494 [D loss: 0.157503, acc.: 96.09%] [G loss: 0.873655]\n",
      "epoch:8 step:6495 [D loss: 0.100915, acc.: 97.66%] [G loss: 1.261235]\n",
      "epoch:8 step:6496 [D loss: 0.094138, acc.: 96.88%] [G loss: 1.250490]\n",
      "epoch:8 step:6497 [D loss: 0.025497, acc.: 100.00%] [G loss: 0.370119]\n",
      "epoch:8 step:6498 [D loss: 1.231809, acc.: 39.84%] [G loss: 8.185250]\n",
      "epoch:8 step:6499 [D loss: 1.643964, acc.: 50.00%] [G loss: 4.226395]\n",
      "epoch:8 step:6500 [D loss: 0.023896, acc.: 100.00%] [G loss: 3.014531]\n",
      "epoch:8 step:6501 [D loss: 0.112124, acc.: 95.31%] [G loss: 3.234770]\n",
      "epoch:8 step:6502 [D loss: 0.121789, acc.: 97.66%] [G loss: 4.517930]\n",
      "epoch:8 step:6503 [D loss: 0.568532, acc.: 73.44%] [G loss: 2.583898]\n",
      "epoch:8 step:6504 [D loss: 0.249924, acc.: 89.06%] [G loss: 5.258737]\n",
      "epoch:8 step:6505 [D loss: 0.244611, acc.: 90.62%] [G loss: 3.388067]\n",
      "epoch:8 step:6506 [D loss: 0.060007, acc.: 100.00%] [G loss: 1.869644]\n",
      "epoch:8 step:6507 [D loss: 0.228796, acc.: 89.06%] [G loss: 4.164900]\n",
      "epoch:8 step:6508 [D loss: 0.254900, acc.: 89.84%] [G loss: 4.235371]\n",
      "epoch:8 step:6509 [D loss: 0.266714, acc.: 88.28%] [G loss: 4.856818]\n",
      "epoch:8 step:6510 [D loss: 0.065772, acc.: 99.22%] [G loss: 4.823947]\n",
      "epoch:8 step:6511 [D loss: 0.097464, acc.: 97.66%] [G loss: 4.225063]\n",
      "epoch:8 step:6512 [D loss: 0.299232, acc.: 86.72%] [G loss: 3.998597]\n",
      "epoch:8 step:6513 [D loss: 0.101690, acc.: 98.44%] [G loss: 2.978008]\n",
      "epoch:8 step:6514 [D loss: 0.324610, acc.: 85.16%] [G loss: 4.189326]\n",
      "epoch:8 step:6515 [D loss: 0.386939, acc.: 78.91%] [G loss: 2.377256]\n",
      "epoch:8 step:6516 [D loss: 0.662632, acc.: 67.97%] [G loss: 6.426169]\n",
      "epoch:8 step:6517 [D loss: 0.571713, acc.: 72.66%] [G loss: 5.160787]\n",
      "epoch:8 step:6518 [D loss: 0.367707, acc.: 82.81%] [G loss: 1.239831]\n",
      "epoch:8 step:6519 [D loss: 0.089229, acc.: 99.22%] [G loss: 2.078524]\n",
      "epoch:8 step:6520 [D loss: 0.213712, acc.: 91.41%] [G loss: 5.086363]\n",
      "epoch:8 step:6521 [D loss: 0.481119, acc.: 72.66%] [G loss: 2.813639]\n",
      "epoch:8 step:6522 [D loss: 0.116990, acc.: 96.88%] [G loss: 3.717843]\n",
      "epoch:8 step:6523 [D loss: 0.154243, acc.: 96.09%] [G loss: 3.554056]\n",
      "epoch:8 step:6524 [D loss: 0.061443, acc.: 100.00%] [G loss: 3.072030]\n",
      "epoch:8 step:6525 [D loss: 0.509915, acc.: 68.75%] [G loss: 4.752429]\n",
      "epoch:8 step:6526 [D loss: 0.110526, acc.: 96.09%] [G loss: 4.825015]\n",
      "epoch:8 step:6527 [D loss: 0.368814, acc.: 80.47%] [G loss: 3.063245]\n",
      "epoch:8 step:6528 [D loss: 0.124737, acc.: 96.88%] [G loss: 4.000495]\n",
      "epoch:8 step:6529 [D loss: 0.043904, acc.: 99.22%] [G loss: 4.131905]\n",
      "epoch:8 step:6530 [D loss: 0.264631, acc.: 92.97%] [G loss: 3.485966]\n",
      "epoch:8 step:6531 [D loss: 0.025771, acc.: 100.00%] [G loss: 4.068906]\n",
      "epoch:8 step:6532 [D loss: 0.126137, acc.: 97.66%] [G loss: 2.943821]\n",
      "epoch:8 step:6533 [D loss: 0.116952, acc.: 98.44%] [G loss: 3.943954]\n",
      "epoch:8 step:6534 [D loss: 0.426655, acc.: 78.91%] [G loss: 2.427476]\n",
      "epoch:8 step:6535 [D loss: 0.196447, acc.: 92.97%] [G loss: 4.144493]\n",
      "epoch:8 step:6536 [D loss: 0.083109, acc.: 98.44%] [G loss: 3.852240]\n",
      "epoch:8 step:6537 [D loss: 0.205735, acc.: 92.19%] [G loss: 3.295779]\n",
      "epoch:8 step:6538 [D loss: 0.306799, acc.: 88.28%] [G loss: 5.294627]\n",
      "epoch:8 step:6539 [D loss: 0.537137, acc.: 71.09%] [G loss: 1.570025]\n",
      "epoch:8 step:6540 [D loss: 0.732664, acc.: 68.75%] [G loss: 7.290375]\n",
      "epoch:8 step:6541 [D loss: 0.651241, acc.: 64.06%] [G loss: 5.232300]\n",
      "epoch:8 step:6542 [D loss: 0.046994, acc.: 100.00%] [G loss: 3.781920]\n",
      "epoch:8 step:6543 [D loss: 0.066524, acc.: 99.22%] [G loss: 4.228149]\n",
      "epoch:8 step:6544 [D loss: 0.188409, acc.: 95.31%] [G loss: 5.129967]\n",
      "epoch:8 step:6545 [D loss: 0.140598, acc.: 96.88%] [G loss: 3.693182]\n",
      "epoch:8 step:6546 [D loss: 0.101907, acc.: 99.22%] [G loss: 2.460108]\n",
      "epoch:8 step:6547 [D loss: 0.178574, acc.: 92.97%] [G loss: 4.500052]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:8 step:6548 [D loss: 0.075803, acc.: 99.22%] [G loss: 3.986823]\n",
      "epoch:8 step:6549 [D loss: 0.077145, acc.: 99.22%] [G loss: 3.495622]\n",
      "epoch:8 step:6550 [D loss: 0.076754, acc.: 100.00%] [G loss: 1.860835]\n",
      "epoch:8 step:6551 [D loss: 0.283817, acc.: 87.50%] [G loss: 3.472452]\n",
      "epoch:8 step:6552 [D loss: 0.104429, acc.: 98.44%] [G loss: 4.715793]\n",
      "epoch:8 step:6553 [D loss: 0.266176, acc.: 87.50%] [G loss: 0.298145]\n",
      "epoch:8 step:6554 [D loss: 1.196369, acc.: 55.47%] [G loss: 7.954206]\n",
      "epoch:8 step:6555 [D loss: 2.524001, acc.: 50.00%] [G loss: 4.144568]\n",
      "epoch:8 step:6556 [D loss: 1.113795, acc.: 44.53%] [G loss: 1.396336]\n",
      "epoch:8 step:6557 [D loss: 0.366755, acc.: 82.03%] [G loss: 2.686193]\n",
      "epoch:8 step:6558 [D loss: 0.356970, acc.: 88.28%] [G loss: 2.976682]\n",
      "epoch:8 step:6559 [D loss: 0.271877, acc.: 87.50%] [G loss: 2.181132]\n",
      "epoch:8 step:6560 [D loss: 0.245380, acc.: 93.75%] [G loss: 3.006711]\n",
      "epoch:8 step:6561 [D loss: 0.139862, acc.: 98.44%] [G loss: 3.665726]\n",
      "epoch:8 step:6562 [D loss: 0.396961, acc.: 82.81%] [G loss: 3.068347]\n",
      "epoch:8 step:6563 [D loss: 0.355037, acc.: 85.16%] [G loss: 2.981122]\n",
      "epoch:8 step:6564 [D loss: 0.120828, acc.: 97.66%] [G loss: 2.776186]\n",
      "epoch:8 step:6565 [D loss: 0.189007, acc.: 96.09%] [G loss: 3.623522]\n",
      "epoch:8 step:6566 [D loss: 0.211060, acc.: 92.19%] [G loss: 3.733329]\n",
      "epoch:8 step:6567 [D loss: 0.324399, acc.: 85.94%] [G loss: 3.593618]\n",
      "epoch:8 step:6568 [D loss: 0.568890, acc.: 66.41%] [G loss: 4.968819]\n",
      "epoch:8 step:6569 [D loss: 0.658958, acc.: 62.50%] [G loss: 2.789217]\n",
      "epoch:8 step:6570 [D loss: 0.146665, acc.: 95.31%] [G loss: 3.712821]\n",
      "epoch:8 step:6571 [D loss: 0.065432, acc.: 99.22%] [G loss: 3.538536]\n",
      "epoch:8 step:6572 [D loss: 0.118871, acc.: 96.88%] [G loss: 3.278459]\n",
      "epoch:8 step:6573 [D loss: 0.115832, acc.: 98.44%] [G loss: 2.769399]\n",
      "epoch:8 step:6574 [D loss: 0.075212, acc.: 99.22%] [G loss: 2.595984]\n",
      "epoch:8 step:6575 [D loss: 0.162482, acc.: 95.31%] [G loss: 2.673725]\n",
      "epoch:8 step:6576 [D loss: 0.082756, acc.: 99.22%] [G loss: 3.118155]\n",
      "epoch:8 step:6577 [D loss: 0.097414, acc.: 97.66%] [G loss: 1.959408]\n",
      "epoch:8 step:6578 [D loss: 0.097177, acc.: 99.22%] [G loss: 1.645158]\n",
      "epoch:8 step:6579 [D loss: 0.193061, acc.: 94.53%] [G loss: 2.791931]\n",
      "epoch:8 step:6580 [D loss: 0.075914, acc.: 96.88%] [G loss: 3.489287]\n",
      "epoch:8 step:6581 [D loss: 0.096263, acc.: 98.44%] [G loss: 2.505810]\n",
      "epoch:8 step:6582 [D loss: 0.193079, acc.: 95.31%] [G loss: 1.193999]\n",
      "epoch:8 step:6583 [D loss: 0.042198, acc.: 99.22%] [G loss: 1.578454]\n",
      "epoch:8 step:6584 [D loss: 0.792494, acc.: 64.84%] [G loss: 4.915671]\n",
      "epoch:8 step:6585 [D loss: 0.374327, acc.: 78.91%] [G loss: 3.721962]\n",
      "epoch:8 step:6586 [D loss: 0.293520, acc.: 85.16%] [G loss: 0.910438]\n",
      "epoch:8 step:6587 [D loss: 1.258994, acc.: 54.69%] [G loss: 6.088875]\n",
      "epoch:8 step:6588 [D loss: 1.239901, acc.: 51.56%] [G loss: 4.032802]\n",
      "epoch:8 step:6589 [D loss: 0.058510, acc.: 98.44%] [G loss: 3.495549]\n",
      "epoch:8 step:6590 [D loss: 0.053886, acc.: 99.22%] [G loss: 2.662563]\n",
      "epoch:8 step:6591 [D loss: 0.098298, acc.: 98.44%] [G loss: 2.254211]\n",
      "epoch:8 step:6592 [D loss: 0.099417, acc.: 100.00%] [G loss: 3.597513]\n",
      "epoch:8 step:6593 [D loss: 0.141891, acc.: 96.09%] [G loss: 3.090253]\n",
      "epoch:8 step:6594 [D loss: 0.295260, acc.: 92.19%] [G loss: 2.469907]\n",
      "epoch:8 step:6595 [D loss: 0.151518, acc.: 97.66%] [G loss: 2.509179]\n",
      "epoch:8 step:6596 [D loss: 0.078940, acc.: 99.22%] [G loss: 2.758181]\n",
      "epoch:8 step:6597 [D loss: 0.352847, acc.: 81.25%] [G loss: 3.089572]\n",
      "epoch:8 step:6598 [D loss: 0.351901, acc.: 82.03%] [G loss: 3.082123]\n",
      "epoch:8 step:6599 [D loss: 0.130500, acc.: 97.66%] [G loss: 2.340537]\n",
      "epoch:8 step:6600 [D loss: 0.105636, acc.: 97.66%] [G loss: 3.012177]\n",
      "##############\n",
      "[0.92323831 0.91228128 1.06120954 0.90912932 2.11456279 1.01092621\n",
      " 1.02116119 2.10460871 1.12041317 1.00657391]\n",
      "##########\n",
      "epoch:8 step:6601 [D loss: 0.081367, acc.: 100.00%] [G loss: 2.514333]\n",
      "epoch:8 step:6602 [D loss: 0.186336, acc.: 94.53%] [G loss: 2.668466]\n",
      "epoch:8 step:6603 [D loss: 0.762604, acc.: 53.12%] [G loss: 3.452571]\n",
      "epoch:8 step:6604 [D loss: 0.061289, acc.: 100.00%] [G loss: 4.053499]\n",
      "epoch:8 step:6605 [D loss: 0.103686, acc.: 97.66%] [G loss: 3.703942]\n",
      "epoch:8 step:6606 [D loss: 0.261302, acc.: 90.62%] [G loss: 2.263259]\n",
      "epoch:8 step:6607 [D loss: 0.046010, acc.: 99.22%] [G loss: 1.535793]\n",
      "epoch:8 step:6608 [D loss: 0.141738, acc.: 92.97%] [G loss: 2.802423]\n",
      "epoch:8 step:6609 [D loss: 0.115146, acc.: 98.44%] [G loss: 2.574516]\n",
      "epoch:8 step:6610 [D loss: 0.104616, acc.: 100.00%] [G loss: 1.464184]\n",
      "epoch:8 step:6611 [D loss: 0.075807, acc.: 99.22%] [G loss: 1.890176]\n",
      "epoch:8 step:6612 [D loss: 0.114805, acc.: 98.44%] [G loss: 2.077342]\n",
      "epoch:8 step:6613 [D loss: 0.110227, acc.: 98.44%] [G loss: 3.117581]\n",
      "epoch:8 step:6614 [D loss: 0.699742, acc.: 65.62%] [G loss: 4.053956]\n",
      "epoch:8 step:6615 [D loss: 0.311952, acc.: 84.38%] [G loss: 2.735575]\n",
      "epoch:8 step:6616 [D loss: 0.095356, acc.: 97.66%] [G loss: 3.232606]\n",
      "epoch:8 step:6617 [D loss: 0.081238, acc.: 99.22%] [G loss: 3.092906]\n",
      "epoch:8 step:6618 [D loss: 0.509848, acc.: 72.66%] [G loss: 5.091026]\n",
      "epoch:8 step:6619 [D loss: 0.039932, acc.: 99.22%] [G loss: 4.951330]\n",
      "epoch:8 step:6620 [D loss: 0.183859, acc.: 92.97%] [G loss: 3.647470]\n",
      "epoch:8 step:6621 [D loss: 0.345813, acc.: 85.16%] [G loss: 4.982164]\n",
      "epoch:8 step:6622 [D loss: 0.180015, acc.: 94.53%] [G loss: 4.444453]\n",
      "epoch:8 step:6623 [D loss: 0.071549, acc.: 98.44%] [G loss: 3.700062]\n",
      "epoch:8 step:6624 [D loss: 0.076805, acc.: 99.22%] [G loss: 3.222847]\n",
      "epoch:8 step:6625 [D loss: 0.044384, acc.: 100.00%] [G loss: 4.000422]\n",
      "epoch:8 step:6626 [D loss: 0.102940, acc.: 99.22%] [G loss: 2.892970]\n",
      "epoch:8 step:6627 [D loss: 0.057245, acc.: 99.22%] [G loss: 3.105526]\n",
      "epoch:8 step:6628 [D loss: 0.107440, acc.: 98.44%] [G loss: 3.094049]\n",
      "epoch:8 step:6629 [D loss: 0.108478, acc.: 96.88%] [G loss: 3.366264]\n",
      "epoch:8 step:6630 [D loss: 0.194552, acc.: 93.75%] [G loss: 2.891709]\n",
      "epoch:8 step:6631 [D loss: 0.095954, acc.: 99.22%] [G loss: 2.906623]\n",
      "epoch:8 step:6632 [D loss: 0.079339, acc.: 99.22%] [G loss: 2.398085]\n",
      "epoch:8 step:6633 [D loss: 0.284118, acc.: 86.72%] [G loss: 4.125637]\n",
      "epoch:8 step:6634 [D loss: 0.245717, acc.: 91.41%] [G loss: 2.008177]\n",
      "epoch:8 step:6635 [D loss: 0.173386, acc.: 96.88%] [G loss: 3.717030]\n",
      "epoch:8 step:6636 [D loss: 0.110884, acc.: 97.66%] [G loss: 3.409327]\n",
      "epoch:8 step:6637 [D loss: 0.247720, acc.: 90.62%] [G loss: 4.835670]\n",
      "epoch:8 step:6638 [D loss: 0.118241, acc.: 96.88%] [G loss: 4.772031]\n",
      "epoch:8 step:6639 [D loss: 0.616142, acc.: 69.53%] [G loss: 5.947746]\n",
      "epoch:8 step:6640 [D loss: 0.015151, acc.: 100.00%] [G loss: 6.891865]\n",
      "epoch:8 step:6641 [D loss: 0.272599, acc.: 89.84%] [G loss: 4.839829]\n",
      "epoch:8 step:6642 [D loss: 0.032138, acc.: 100.00%] [G loss: 4.218186]\n",
      "epoch:8 step:6643 [D loss: 0.123762, acc.: 96.88%] [G loss: 4.818624]\n",
      "epoch:8 step:6644 [D loss: 0.015469, acc.: 100.00%] [G loss: 6.307138]\n",
      "epoch:8 step:6645 [D loss: 0.094431, acc.: 98.44%] [G loss: 4.442203]\n",
      "epoch:8 step:6646 [D loss: 0.096958, acc.: 96.88%] [G loss: 4.138207]\n",
      "epoch:8 step:6647 [D loss: 0.034910, acc.: 100.00%] [G loss: 4.561444]\n",
      "epoch:8 step:6648 [D loss: 0.092928, acc.: 98.44%] [G loss: 3.850434]\n",
      "epoch:8 step:6649 [D loss: 0.187091, acc.: 94.53%] [G loss: 3.570500]\n",
      "epoch:8 step:6650 [D loss: 0.284245, acc.: 88.28%] [G loss: 2.567222]\n",
      "epoch:8 step:6651 [D loss: 0.024269, acc.: 100.00%] [G loss: 2.562088]\n",
      "epoch:8 step:6652 [D loss: 0.153396, acc.: 94.53%] [G loss: 4.350271]\n",
      "epoch:8 step:6653 [D loss: 0.920716, acc.: 53.12%] [G loss: 6.634254]\n",
      "epoch:8 step:6654 [D loss: 0.370297, acc.: 81.25%] [G loss: 5.254550]\n",
      "epoch:8 step:6655 [D loss: 0.024743, acc.: 99.22%] [G loss: 4.118294]\n",
      "epoch:8 step:6656 [D loss: 0.055369, acc.: 99.22%] [G loss: 5.334176]\n",
      "epoch:8 step:6657 [D loss: 0.051722, acc.: 99.22%] [G loss: 4.581647]\n",
      "epoch:8 step:6658 [D loss: 0.075033, acc.: 97.66%] [G loss: 3.683862]\n",
      "epoch:8 step:6659 [D loss: 0.322979, acc.: 86.72%] [G loss: 3.453239]\n",
      "epoch:8 step:6660 [D loss: 0.012583, acc.: 100.00%] [G loss: 4.180257]\n",
      "epoch:8 step:6661 [D loss: 0.102668, acc.: 99.22%] [G loss: 3.333814]\n",
      "epoch:8 step:6662 [D loss: 0.059913, acc.: 99.22%] [G loss: 3.690016]\n",
      "epoch:8 step:6663 [D loss: 0.036123, acc.: 99.22%] [G loss: 3.635833]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:8 step:6664 [D loss: 0.144689, acc.: 93.75%] [G loss: 3.940141]\n",
      "epoch:8 step:6665 [D loss: 0.171115, acc.: 94.53%] [G loss: 2.810426]\n",
      "epoch:8 step:6666 [D loss: 0.031690, acc.: 100.00%] [G loss: 2.829855]\n",
      "epoch:8 step:6667 [D loss: 0.093482, acc.: 97.66%] [G loss: 1.818860]\n",
      "epoch:8 step:6668 [D loss: 0.719635, acc.: 60.94%] [G loss: 7.347226]\n",
      "epoch:8 step:6669 [D loss: 1.363530, acc.: 57.03%] [G loss: 2.544801]\n",
      "epoch:8 step:6670 [D loss: 0.075645, acc.: 98.44%] [G loss: 1.857617]\n",
      "epoch:8 step:6671 [D loss: 0.143443, acc.: 95.31%] [G loss: 3.997639]\n",
      "epoch:8 step:6672 [D loss: 0.098739, acc.: 97.66%] [G loss: 4.371520]\n",
      "epoch:8 step:6673 [D loss: 0.042798, acc.: 100.00%] [G loss: 3.159405]\n",
      "epoch:8 step:6674 [D loss: 0.254125, acc.: 88.28%] [G loss: 4.600553]\n",
      "epoch:8 step:6675 [D loss: 0.153914, acc.: 93.75%] [G loss: 3.988026]\n",
      "epoch:8 step:6676 [D loss: 0.143859, acc.: 95.31%] [G loss: 4.681363]\n",
      "epoch:8 step:6677 [D loss: 0.197283, acc.: 92.19%] [G loss: 3.877512]\n",
      "epoch:8 step:6678 [D loss: 0.424524, acc.: 82.03%] [G loss: 3.604395]\n",
      "epoch:8 step:6679 [D loss: 0.289226, acc.: 88.28%] [G loss: 4.151321]\n",
      "epoch:8 step:6680 [D loss: 0.018527, acc.: 100.00%] [G loss: 4.294972]\n",
      "epoch:8 step:6681 [D loss: 0.064978, acc.: 99.22%] [G loss: 3.694548]\n",
      "epoch:8 step:6682 [D loss: 0.330037, acc.: 89.84%] [G loss: 4.871834]\n",
      "epoch:8 step:6683 [D loss: 0.157263, acc.: 93.75%] [G loss: 3.667340]\n",
      "epoch:8 step:6684 [D loss: 0.056115, acc.: 99.22%] [G loss: 3.470050]\n",
      "epoch:8 step:6685 [D loss: 0.223882, acc.: 92.19%] [G loss: 4.514012]\n",
      "epoch:8 step:6686 [D loss: 0.139270, acc.: 93.75%] [G loss: 3.990024]\n",
      "epoch:8 step:6687 [D loss: 0.052815, acc.: 99.22%] [G loss: 2.805127]\n",
      "epoch:8 step:6688 [D loss: 0.202768, acc.: 91.41%] [G loss: 4.714558]\n",
      "epoch:8 step:6689 [D loss: 0.354053, acc.: 80.47%] [G loss: 1.686875]\n",
      "epoch:8 step:6690 [D loss: 0.341237, acc.: 84.38%] [G loss: 5.590449]\n",
      "epoch:8 step:6691 [D loss: 0.337697, acc.: 83.59%] [G loss: 3.405204]\n",
      "epoch:8 step:6692 [D loss: 0.060844, acc.: 100.00%] [G loss: 2.076272]\n",
      "epoch:8 step:6693 [D loss: 0.084361, acc.: 99.22%] [G loss: 1.606933]\n",
      "epoch:8 step:6694 [D loss: 0.067682, acc.: 98.44%] [G loss: 2.753729]\n",
      "epoch:8 step:6695 [D loss: 0.081630, acc.: 98.44%] [G loss: 3.933470]\n",
      "epoch:8 step:6696 [D loss: 1.228216, acc.: 39.84%] [G loss: 7.101346]\n",
      "epoch:8 step:6697 [D loss: 0.268956, acc.: 85.16%] [G loss: 5.089941]\n",
      "epoch:8 step:6698 [D loss: 0.133659, acc.: 96.09%] [G loss: 4.694734]\n",
      "epoch:8 step:6699 [D loss: 0.139425, acc.: 96.88%] [G loss: 4.236681]\n",
      "epoch:8 step:6700 [D loss: 0.039328, acc.: 100.00%] [G loss: 3.637384]\n",
      "epoch:8 step:6701 [D loss: 0.569428, acc.: 68.75%] [G loss: 6.036809]\n",
      "epoch:8 step:6702 [D loss: 0.386606, acc.: 79.69%] [G loss: 4.208665]\n",
      "epoch:8 step:6703 [D loss: 0.200787, acc.: 90.62%] [G loss: 3.779637]\n",
      "epoch:8 step:6704 [D loss: 0.057048, acc.: 98.44%] [G loss: 4.533190]\n",
      "epoch:8 step:6705 [D loss: 0.044230, acc.: 99.22%] [G loss: 3.113050]\n",
      "epoch:8 step:6706 [D loss: 0.185043, acc.: 95.31%] [G loss: 3.792129]\n",
      "epoch:8 step:6707 [D loss: 0.206441, acc.: 92.19%] [G loss: 4.866403]\n",
      "epoch:8 step:6708 [D loss: 1.047587, acc.: 51.56%] [G loss: 6.377846]\n",
      "epoch:8 step:6709 [D loss: 0.428473, acc.: 78.12%] [G loss: 3.385734]\n",
      "epoch:8 step:6710 [D loss: 0.126422, acc.: 95.31%] [G loss: 4.030060]\n",
      "epoch:8 step:6711 [D loss: 0.028260, acc.: 100.00%] [G loss: 4.557919]\n",
      "epoch:8 step:6712 [D loss: 0.094791, acc.: 96.09%] [G loss: 2.958766]\n",
      "epoch:8 step:6713 [D loss: 0.100655, acc.: 96.88%] [G loss: 3.848633]\n",
      "epoch:8 step:6714 [D loss: 0.070834, acc.: 98.44%] [G loss: 4.371096]\n",
      "epoch:8 step:6715 [D loss: 0.129093, acc.: 96.09%] [G loss: 1.388949]\n",
      "epoch:8 step:6716 [D loss: 0.096686, acc.: 98.44%] [G loss: 2.795717]\n",
      "epoch:8 step:6717 [D loss: 0.128987, acc.: 96.09%] [G loss: 4.741602]\n",
      "epoch:8 step:6718 [D loss: 0.117642, acc.: 95.31%] [G loss: 3.422394]\n",
      "epoch:8 step:6719 [D loss: 0.137454, acc.: 94.53%] [G loss: 0.575795]\n",
      "epoch:8 step:6720 [D loss: 0.943709, acc.: 58.59%] [G loss: 8.424314]\n",
      "epoch:8 step:6721 [D loss: 2.417705, acc.: 50.00%] [G loss: 5.591040]\n",
      "epoch:8 step:6722 [D loss: 0.864744, acc.: 53.91%] [G loss: 1.870824]\n",
      "epoch:8 step:6723 [D loss: 0.235494, acc.: 90.62%] [G loss: 3.123793]\n",
      "epoch:8 step:6724 [D loss: 0.062845, acc.: 100.00%] [G loss: 3.835684]\n",
      "epoch:8 step:6725 [D loss: 0.091781, acc.: 97.66%] [G loss: 3.441881]\n",
      "epoch:8 step:6726 [D loss: 0.088264, acc.: 99.22%] [G loss: 3.367531]\n",
      "epoch:8 step:6727 [D loss: 0.067309, acc.: 99.22%] [G loss: 3.454735]\n",
      "epoch:8 step:6728 [D loss: 0.052053, acc.: 100.00%] [G loss: 3.271146]\n",
      "epoch:8 step:6729 [D loss: 0.092186, acc.: 99.22%] [G loss: 3.230411]\n",
      "epoch:8 step:6730 [D loss: 0.237555, acc.: 94.53%] [G loss: 3.976098]\n",
      "epoch:8 step:6731 [D loss: 0.134125, acc.: 96.09%] [G loss: 3.405082]\n",
      "epoch:8 step:6732 [D loss: 0.217682, acc.: 93.75%] [G loss: 4.261392]\n",
      "epoch:8 step:6733 [D loss: 0.161531, acc.: 96.09%] [G loss: 3.417318]\n",
      "epoch:8 step:6734 [D loss: 0.238706, acc.: 89.06%] [G loss: 4.645483]\n",
      "epoch:8 step:6735 [D loss: 0.245859, acc.: 90.62%] [G loss: 4.045794]\n",
      "epoch:8 step:6736 [D loss: 0.323290, acc.: 86.72%] [G loss: 3.499346]\n",
      "epoch:8 step:6737 [D loss: 0.128615, acc.: 96.09%] [G loss: 4.027983]\n",
      "epoch:8 step:6738 [D loss: 0.061992, acc.: 99.22%] [G loss: 3.876267]\n",
      "epoch:8 step:6739 [D loss: 0.692710, acc.: 63.28%] [G loss: 5.612799]\n",
      "epoch:8 step:6740 [D loss: 0.313701, acc.: 85.16%] [G loss: 3.881726]\n",
      "epoch:8 step:6741 [D loss: 0.100909, acc.: 97.66%] [G loss: 3.897124]\n",
      "epoch:8 step:6742 [D loss: 0.067590, acc.: 99.22%] [G loss: 4.143412]\n",
      "epoch:8 step:6743 [D loss: 0.060599, acc.: 99.22%] [G loss: 3.603649]\n",
      "epoch:8 step:6744 [D loss: 0.078001, acc.: 98.44%] [G loss: 2.789120]\n",
      "epoch:8 step:6745 [D loss: 0.083885, acc.: 99.22%] [G loss: 3.593000]\n",
      "epoch:8 step:6746 [D loss: 0.063836, acc.: 100.00%] [G loss: 2.569612]\n",
      "epoch:8 step:6747 [D loss: 0.170740, acc.: 96.88%] [G loss: 2.866127]\n",
      "epoch:8 step:6748 [D loss: 0.028159, acc.: 99.22%] [G loss: 4.163629]\n",
      "epoch:8 step:6749 [D loss: 0.231423, acc.: 92.19%] [G loss: 1.724459]\n",
      "epoch:8 step:6750 [D loss: 0.027532, acc.: 100.00%] [G loss: 1.620780]\n",
      "epoch:8 step:6751 [D loss: 0.068704, acc.: 96.88%] [G loss: 1.542048]\n",
      "epoch:8 step:6752 [D loss: 0.041435, acc.: 100.00%] [G loss: 1.650080]\n",
      "epoch:8 step:6753 [D loss: 0.178139, acc.: 93.75%] [G loss: 4.053902]\n",
      "epoch:8 step:6754 [D loss: 0.241065, acc.: 89.84%] [G loss: 3.470268]\n",
      "epoch:8 step:6755 [D loss: 0.060837, acc.: 98.44%] [G loss: 4.059649]\n",
      "epoch:8 step:6756 [D loss: 0.063250, acc.: 99.22%] [G loss: 3.802769]\n",
      "epoch:8 step:6757 [D loss: 0.705836, acc.: 63.28%] [G loss: 6.856901]\n",
      "epoch:8 step:6758 [D loss: 0.577896, acc.: 67.97%] [G loss: 4.636814]\n",
      "epoch:8 step:6759 [D loss: 0.079566, acc.: 99.22%] [G loss: 3.342750]\n",
      "epoch:8 step:6760 [D loss: 0.131924, acc.: 95.31%] [G loss: 4.495224]\n",
      "epoch:8 step:6761 [D loss: 0.028876, acc.: 100.00%] [G loss: 4.823215]\n",
      "epoch:8 step:6762 [D loss: 0.054476, acc.: 100.00%] [G loss: 4.959697]\n",
      "epoch:8 step:6763 [D loss: 0.047003, acc.: 99.22%] [G loss: 4.124149]\n",
      "epoch:8 step:6764 [D loss: 0.069154, acc.: 100.00%] [G loss: 3.822790]\n",
      "epoch:8 step:6765 [D loss: 0.148966, acc.: 95.31%] [G loss: 4.299758]\n",
      "epoch:8 step:6766 [D loss: 0.101091, acc.: 96.09%] [G loss: 3.872411]\n",
      "epoch:8 step:6767 [D loss: 0.220579, acc.: 94.53%] [G loss: 4.049288]\n",
      "epoch:8 step:6768 [D loss: 0.063997, acc.: 99.22%] [G loss: 4.730280]\n",
      "epoch:8 step:6769 [D loss: 0.332036, acc.: 85.94%] [G loss: 4.450261]\n",
      "epoch:8 step:6770 [D loss: 0.172793, acc.: 95.31%] [G loss: 3.286136]\n",
      "epoch:8 step:6771 [D loss: 0.067892, acc.: 99.22%] [G loss: 3.410611]\n",
      "epoch:8 step:6772 [D loss: 0.198916, acc.: 93.75%] [G loss: 5.547413]\n",
      "epoch:8 step:6773 [D loss: 0.068668, acc.: 99.22%] [G loss: 5.715121]\n",
      "epoch:8 step:6774 [D loss: 0.155808, acc.: 95.31%] [G loss: 3.198003]\n",
      "epoch:8 step:6775 [D loss: 0.190881, acc.: 94.53%] [G loss: 5.487048]\n",
      "epoch:8 step:6776 [D loss: 0.091548, acc.: 97.66%] [G loss: 4.908347]\n",
      "epoch:8 step:6777 [D loss: 0.091245, acc.: 97.66%] [G loss: 4.898468]\n",
      "epoch:8 step:6778 [D loss: 0.124428, acc.: 99.22%] [G loss: 4.502803]\n",
      "epoch:8 step:6779 [D loss: 0.040670, acc.: 100.00%] [G loss: 4.065752]\n",
      "epoch:8 step:6780 [D loss: 0.757757, acc.: 57.81%] [G loss: 7.877639]\n",
      "epoch:8 step:6781 [D loss: 1.418261, acc.: 52.34%] [G loss: 2.783659]\n",
      "epoch:8 step:6782 [D loss: 0.357951, acc.: 83.59%] [G loss: 5.522560]\n",
      "epoch:8 step:6783 [D loss: 0.044607, acc.: 98.44%] [G loss: 6.385646]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:8 step:6784 [D loss: 0.364373, acc.: 85.16%] [G loss: 3.579420]\n",
      "epoch:8 step:6785 [D loss: 0.041117, acc.: 100.00%] [G loss: 3.437253]\n",
      "epoch:8 step:6786 [D loss: 0.060219, acc.: 99.22%] [G loss: 3.362227]\n",
      "epoch:8 step:6787 [D loss: 0.025260, acc.: 100.00%] [G loss: 3.575637]\n",
      "epoch:8 step:6788 [D loss: 0.044798, acc.: 99.22%] [G loss: 2.418115]\n",
      "epoch:8 step:6789 [D loss: 0.048636, acc.: 99.22%] [G loss: 3.354510]\n",
      "epoch:8 step:6790 [D loss: 1.047248, acc.: 51.56%] [G loss: 5.483348]\n",
      "epoch:8 step:6791 [D loss: 0.590428, acc.: 71.09%] [G loss: 2.846228]\n",
      "epoch:8 step:6792 [D loss: 0.050690, acc.: 99.22%] [G loss: 2.445226]\n",
      "epoch:8 step:6793 [D loss: 0.091786, acc.: 98.44%] [G loss: 2.741325]\n",
      "epoch:8 step:6794 [D loss: 0.048398, acc.: 99.22%] [G loss: 4.195738]\n",
      "epoch:8 step:6795 [D loss: 0.440738, acc.: 76.56%] [G loss: 0.406219]\n",
      "epoch:8 step:6796 [D loss: 0.421432, acc.: 78.91%] [G loss: 4.569893]\n",
      "epoch:8 step:6797 [D loss: 0.075967, acc.: 98.44%] [G loss: 5.301495]\n",
      "epoch:8 step:6798 [D loss: 1.318433, acc.: 40.62%] [G loss: 3.414519]\n",
      "epoch:8 step:6799 [D loss: 0.028431, acc.: 100.00%] [G loss: 4.570182]\n",
      "epoch:8 step:6800 [D loss: 0.382129, acc.: 82.81%] [G loss: 2.749660]\n",
      "##############\n",
      "[0.98663842 0.99916527 1.0583923  0.96437968 0.87508777 0.99879684\n",
      " 0.92616169 1.04599722 2.10432205 1.02250898]\n",
      "##########\n",
      "epoch:8 step:6801 [D loss: 0.038063, acc.: 99.22%] [G loss: 3.786039]\n",
      "epoch:8 step:6802 [D loss: 0.063879, acc.: 99.22%] [G loss: 2.907644]\n",
      "epoch:8 step:6803 [D loss: 0.069596, acc.: 97.66%] [G loss: 2.847577]\n",
      "epoch:8 step:6804 [D loss: 0.064187, acc.: 100.00%] [G loss: 1.950733]\n",
      "epoch:8 step:6805 [D loss: 0.069277, acc.: 100.00%] [G loss: 1.294460]\n",
      "epoch:8 step:6806 [D loss: 0.279207, acc.: 88.28%] [G loss: 3.967727]\n",
      "epoch:8 step:6807 [D loss: 0.222158, acc.: 89.84%] [G loss: 3.695148]\n",
      "epoch:8 step:6808 [D loss: 0.162216, acc.: 96.09%] [G loss: 2.381395]\n",
      "epoch:8 step:6809 [D loss: 0.235757, acc.: 87.50%] [G loss: 4.063183]\n",
      "epoch:8 step:6810 [D loss: 0.646402, acc.: 64.84%] [G loss: 4.570855]\n",
      "epoch:8 step:6811 [D loss: 0.311529, acc.: 85.94%] [G loss: 2.854290]\n",
      "epoch:8 step:6812 [D loss: 0.188402, acc.: 92.19%] [G loss: 5.099028]\n",
      "epoch:8 step:6813 [D loss: 0.086976, acc.: 97.66%] [G loss: 3.364823]\n",
      "epoch:8 step:6814 [D loss: 0.132514, acc.: 96.09%] [G loss: 3.638552]\n",
      "epoch:8 step:6815 [D loss: 0.098941, acc.: 96.09%] [G loss: 4.179061]\n",
      "epoch:8 step:6816 [D loss: 0.064487, acc.: 100.00%] [G loss: 4.328934]\n",
      "epoch:8 step:6817 [D loss: 0.081866, acc.: 99.22%] [G loss: 3.743520]\n",
      "epoch:8 step:6818 [D loss: 0.165702, acc.: 95.31%] [G loss: 2.950656]\n",
      "epoch:8 step:6819 [D loss: 0.086981, acc.: 97.66%] [G loss: 3.671289]\n",
      "epoch:8 step:6820 [D loss: 0.086714, acc.: 99.22%] [G loss: 3.785771]\n",
      "epoch:8 step:6821 [D loss: 0.188779, acc.: 96.09%] [G loss: 3.365786]\n",
      "epoch:8 step:6822 [D loss: 0.148646, acc.: 94.53%] [G loss: 2.465944]\n",
      "epoch:8 step:6823 [D loss: 0.165449, acc.: 96.09%] [G loss: 4.836700]\n",
      "epoch:8 step:6824 [D loss: 0.044685, acc.: 99.22%] [G loss: 3.674768]\n",
      "epoch:8 step:6825 [D loss: 0.296187, acc.: 86.72%] [G loss: 2.243915]\n",
      "epoch:8 step:6826 [D loss: 0.149940, acc.: 94.53%] [G loss: 5.077151]\n",
      "epoch:8 step:6827 [D loss: 0.047384, acc.: 99.22%] [G loss: 4.975467]\n",
      "epoch:8 step:6828 [D loss: 0.278639, acc.: 88.28%] [G loss: 2.657574]\n",
      "epoch:8 step:6829 [D loss: 0.302817, acc.: 84.38%] [G loss: 5.329749]\n",
      "epoch:8 step:6830 [D loss: 0.187260, acc.: 92.19%] [G loss: 3.067877]\n",
      "epoch:8 step:6831 [D loss: 0.162445, acc.: 95.31%] [G loss: 4.570557]\n",
      "epoch:8 step:6832 [D loss: 0.029723, acc.: 100.00%] [G loss: 4.346710]\n",
      "epoch:8 step:6833 [D loss: 0.056001, acc.: 99.22%] [G loss: 4.196495]\n",
      "epoch:8 step:6834 [D loss: 0.062752, acc.: 99.22%] [G loss: 1.827434]\n",
      "epoch:8 step:6835 [D loss: 0.266583, acc.: 92.19%] [G loss: 4.642616]\n",
      "epoch:8 step:6836 [D loss: 0.444338, acc.: 79.69%] [G loss: 1.617268]\n",
      "epoch:8 step:6837 [D loss: 0.172899, acc.: 92.97%] [G loss: 3.586083]\n",
      "epoch:8 step:6838 [D loss: 0.042160, acc.: 99.22%] [G loss: 3.519947]\n",
      "epoch:8 step:6839 [D loss: 0.112203, acc.: 95.31%] [G loss: 1.483490]\n",
      "epoch:8 step:6840 [D loss: 0.092282, acc.: 97.66%] [G loss: 1.467241]\n",
      "epoch:8 step:6841 [D loss: 0.024978, acc.: 100.00%] [G loss: 0.949142]\n",
      "epoch:8 step:6842 [D loss: 0.031202, acc.: 99.22%] [G loss: 1.592843]\n",
      "epoch:8 step:6843 [D loss: 0.151953, acc.: 95.31%] [G loss: 4.100119]\n",
      "epoch:8 step:6844 [D loss: 0.081750, acc.: 97.66%] [G loss: 3.633335]\n",
      "epoch:8 step:6845 [D loss: 0.099202, acc.: 96.88%] [G loss: 3.702171]\n",
      "epoch:8 step:6846 [D loss: 0.039987, acc.: 100.00%] [G loss: 3.253973]\n",
      "epoch:8 step:6847 [D loss: 0.909495, acc.: 51.56%] [G loss: 6.457592]\n",
      "epoch:8 step:6848 [D loss: 1.031123, acc.: 60.16%] [G loss: 1.797208]\n",
      "epoch:8 step:6849 [D loss: 0.206182, acc.: 92.19%] [G loss: 4.129102]\n",
      "epoch:8 step:6850 [D loss: 0.110590, acc.: 95.31%] [G loss: 3.620891]\n",
      "epoch:8 step:6851 [D loss: 0.070199, acc.: 99.22%] [G loss: 4.129384]\n",
      "epoch:8 step:6852 [D loss: 0.618982, acc.: 67.97%] [G loss: 4.201373]\n",
      "epoch:8 step:6853 [D loss: 0.014713, acc.: 100.00%] [G loss: 5.743489]\n",
      "epoch:8 step:6854 [D loss: 0.128590, acc.: 95.31%] [G loss: 3.503321]\n",
      "epoch:8 step:6855 [D loss: 0.136964, acc.: 94.53%] [G loss: 3.865134]\n",
      "epoch:8 step:6856 [D loss: 0.006459, acc.: 100.00%] [G loss: 4.376795]\n",
      "epoch:8 step:6857 [D loss: 0.019034, acc.: 100.00%] [G loss: 3.167193]\n",
      "epoch:8 step:6858 [D loss: 0.099803, acc.: 97.66%] [G loss: 3.327477]\n",
      "epoch:8 step:6859 [D loss: 0.008527, acc.: 100.00%] [G loss: 3.558035]\n",
      "epoch:8 step:6860 [D loss: 0.022470, acc.: 100.00%] [G loss: 3.237390]\n",
      "epoch:8 step:6861 [D loss: 0.390052, acc.: 84.38%] [G loss: 5.163329]\n",
      "epoch:8 step:6862 [D loss: 0.102060, acc.: 95.31%] [G loss: 5.146626]\n",
      "epoch:8 step:6863 [D loss: 0.130557, acc.: 95.31%] [G loss: 2.328306]\n",
      "epoch:8 step:6864 [D loss: 0.283939, acc.: 88.28%] [G loss: 3.030441]\n",
      "epoch:8 step:6865 [D loss: 0.027573, acc.: 100.00%] [G loss: 5.409067]\n",
      "epoch:8 step:6866 [D loss: 0.209457, acc.: 91.41%] [G loss: 2.877579]\n",
      "epoch:8 step:6867 [D loss: 0.302400, acc.: 87.50%] [G loss: 3.286809]\n",
      "epoch:8 step:6868 [D loss: 0.032805, acc.: 100.00%] [G loss: 3.119045]\n",
      "epoch:8 step:6869 [D loss: 0.058665, acc.: 97.66%] [G loss: 1.246701]\n",
      "epoch:8 step:6870 [D loss: 0.015307, acc.: 100.00%] [G loss: 1.141245]\n",
      "epoch:8 step:6871 [D loss: 0.433493, acc.: 77.34%] [G loss: 6.215553]\n",
      "epoch:8 step:6872 [D loss: 1.544881, acc.: 37.50%] [G loss: 5.620742]\n",
      "epoch:8 step:6873 [D loss: 0.094142, acc.: 97.66%] [G loss: 4.648704]\n",
      "epoch:8 step:6874 [D loss: 0.154089, acc.: 94.53%] [G loss: 2.516606]\n",
      "epoch:8 step:6875 [D loss: 0.261035, acc.: 88.28%] [G loss: 4.782910]\n",
      "epoch:8 step:6876 [D loss: 0.059747, acc.: 99.22%] [G loss: 5.361064]\n",
      "epoch:8 step:6877 [D loss: 0.392613, acc.: 81.25%] [G loss: 3.540400]\n",
      "epoch:8 step:6878 [D loss: 0.039062, acc.: 100.00%] [G loss: 3.652391]\n",
      "epoch:8 step:6879 [D loss: 0.224436, acc.: 94.53%] [G loss: 4.097444]\n",
      "epoch:8 step:6880 [D loss: 0.133075, acc.: 98.44%] [G loss: 4.651025]\n",
      "epoch:8 step:6881 [D loss: 0.078929, acc.: 97.66%] [G loss: 4.149071]\n",
      "epoch:8 step:6882 [D loss: 0.024533, acc.: 100.00%] [G loss: 2.422519]\n",
      "epoch:8 step:6883 [D loss: 0.134205, acc.: 94.53%] [G loss: 3.472248]\n",
      "epoch:8 step:6884 [D loss: 0.128701, acc.: 96.88%] [G loss: 2.768957]\n",
      "epoch:8 step:6885 [D loss: 0.097060, acc.: 99.22%] [G loss: 1.679320]\n",
      "epoch:8 step:6886 [D loss: 0.071587, acc.: 99.22%] [G loss: 1.236279]\n",
      "epoch:8 step:6887 [D loss: 0.231410, acc.: 92.97%] [G loss: 1.710158]\n",
      "epoch:8 step:6888 [D loss: 0.297592, acc.: 87.50%] [G loss: 6.374339]\n",
      "epoch:8 step:6889 [D loss: 0.592220, acc.: 68.75%] [G loss: 2.077972]\n",
      "epoch:8 step:6890 [D loss: 0.419006, acc.: 79.69%] [G loss: 6.612306]\n",
      "epoch:8 step:6891 [D loss: 0.349642, acc.: 78.12%] [G loss: 4.206767]\n",
      "epoch:8 step:6892 [D loss: 0.086795, acc.: 98.44%] [G loss: 3.813176]\n",
      "epoch:8 step:6893 [D loss: 0.058989, acc.: 99.22%] [G loss: 4.841844]\n",
      "epoch:8 step:6894 [D loss: 0.675479, acc.: 64.84%] [G loss: 3.937517]\n",
      "epoch:8 step:6895 [D loss: 0.027887, acc.: 100.00%] [G loss: 5.201965]\n",
      "epoch:8 step:6896 [D loss: 0.137764, acc.: 95.31%] [G loss: 3.757845]\n",
      "epoch:8 step:6897 [D loss: 0.120460, acc.: 99.22%] [G loss: 3.480520]\n",
      "epoch:8 step:6898 [D loss: 0.039925, acc.: 100.00%] [G loss: 3.395673]\n",
      "epoch:8 step:6899 [D loss: 0.039014, acc.: 99.22%] [G loss: 3.616042]\n",
      "epoch:8 step:6900 [D loss: 0.098621, acc.: 97.66%] [G loss: 3.965086]\n",
      "epoch:8 step:6901 [D loss: 0.054271, acc.: 99.22%] [G loss: 3.488213]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:8 step:6902 [D loss: 0.186265, acc.: 93.75%] [G loss: 4.056028]\n",
      "epoch:8 step:6903 [D loss: 0.044213, acc.: 99.22%] [G loss: 3.777742]\n",
      "epoch:8 step:6904 [D loss: 0.358629, acc.: 82.81%] [G loss: 6.122661]\n",
      "epoch:8 step:6905 [D loss: 0.391833, acc.: 78.91%] [G loss: 3.348804]\n",
      "epoch:8 step:6906 [D loss: 0.070219, acc.: 99.22%] [G loss: 3.833995]\n",
      "epoch:8 step:6907 [D loss: 0.068534, acc.: 98.44%] [G loss: 4.488691]\n",
      "epoch:8 step:6908 [D loss: 0.021484, acc.: 100.00%] [G loss: 4.593927]\n",
      "epoch:8 step:6909 [D loss: 0.042048, acc.: 100.00%] [G loss: 3.453047]\n",
      "epoch:8 step:6910 [D loss: 0.155251, acc.: 95.31%] [G loss: 4.056158]\n",
      "epoch:8 step:6911 [D loss: 0.063782, acc.: 100.00%] [G loss: 4.018244]\n",
      "epoch:8 step:6912 [D loss: 0.092363, acc.: 98.44%] [G loss: 3.938822]\n",
      "epoch:8 step:6913 [D loss: 0.743572, acc.: 62.50%] [G loss: 6.227497]\n",
      "epoch:8 step:6914 [D loss: 0.429308, acc.: 78.91%] [G loss: 3.992574]\n",
      "epoch:8 step:6915 [D loss: 0.225502, acc.: 89.84%] [G loss: 5.233118]\n",
      "epoch:8 step:6916 [D loss: 0.284741, acc.: 85.94%] [G loss: 3.007882]\n",
      "epoch:8 step:6917 [D loss: 0.078198, acc.: 98.44%] [G loss: 4.160818]\n",
      "epoch:8 step:6918 [D loss: 0.205782, acc.: 93.75%] [G loss: 4.063851]\n",
      "epoch:8 step:6919 [D loss: 0.036425, acc.: 100.00%] [G loss: 3.937231]\n",
      "epoch:8 step:6920 [D loss: 0.065420, acc.: 99.22%] [G loss: 2.808609]\n",
      "epoch:8 step:6921 [D loss: 0.390476, acc.: 85.94%] [G loss: 4.704404]\n",
      "epoch:8 step:6922 [D loss: 0.093875, acc.: 96.88%] [G loss: 4.743834]\n",
      "epoch:8 step:6923 [D loss: 0.146928, acc.: 98.44%] [G loss: 2.335938]\n",
      "epoch:8 step:6924 [D loss: 0.121615, acc.: 95.31%] [G loss: 4.641516]\n",
      "epoch:8 step:6925 [D loss: 0.099789, acc.: 98.44%] [G loss: 3.651977]\n",
      "epoch:8 step:6926 [D loss: 1.377535, acc.: 41.41%] [G loss: 7.954223]\n",
      "epoch:8 step:6927 [D loss: 2.100698, acc.: 50.00%] [G loss: 4.179592]\n",
      "epoch:8 step:6928 [D loss: 0.303107, acc.: 86.72%] [G loss: 3.825472]\n",
      "epoch:8 step:6929 [D loss: 0.066375, acc.: 99.22%] [G loss: 3.054329]\n",
      "epoch:8 step:6930 [D loss: 0.132596, acc.: 96.09%] [G loss: 2.124694]\n",
      "epoch:8 step:6931 [D loss: 0.250928, acc.: 87.50%] [G loss: 5.068106]\n",
      "epoch:8 step:6932 [D loss: 0.319171, acc.: 83.59%] [G loss: 3.168973]\n",
      "epoch:8 step:6933 [D loss: 0.202839, acc.: 93.75%] [G loss: 1.926085]\n",
      "epoch:8 step:6934 [D loss: 0.251574, acc.: 88.28%] [G loss: 4.056566]\n",
      "epoch:8 step:6935 [D loss: 0.128456, acc.: 96.88%] [G loss: 4.006737]\n",
      "epoch:8 step:6936 [D loss: 0.155000, acc.: 97.66%] [G loss: 3.631238]\n",
      "epoch:8 step:6937 [D loss: 0.126331, acc.: 97.66%] [G loss: 2.616763]\n",
      "epoch:8 step:6938 [D loss: 0.250594, acc.: 91.41%] [G loss: 4.631698]\n",
      "epoch:8 step:6939 [D loss: 0.091229, acc.: 98.44%] [G loss: 4.237363]\n",
      "epoch:8 step:6940 [D loss: 0.231554, acc.: 92.19%] [G loss: 2.306315]\n",
      "epoch:8 step:6941 [D loss: 0.298798, acc.: 85.16%] [G loss: 4.533624]\n",
      "epoch:8 step:6942 [D loss: 0.189949, acc.: 92.19%] [G loss: 4.946367]\n",
      "epoch:8 step:6943 [D loss: 0.017626, acc.: 100.00%] [G loss: 3.548413]\n",
      "epoch:8 step:6944 [D loss: 0.093391, acc.: 99.22%] [G loss: 4.587756]\n",
      "epoch:8 step:6945 [D loss: 0.019225, acc.: 100.00%] [G loss: 3.571700]\n",
      "epoch:8 step:6946 [D loss: 0.066111, acc.: 99.22%] [G loss: 3.531132]\n",
      "epoch:8 step:6947 [D loss: 0.147550, acc.: 94.53%] [G loss: 3.784848]\n",
      "epoch:8 step:6948 [D loss: 0.065407, acc.: 99.22%] [G loss: 4.205449]\n",
      "epoch:8 step:6949 [D loss: 0.061816, acc.: 100.00%] [G loss: 3.310038]\n",
      "epoch:8 step:6950 [D loss: 0.292355, acc.: 84.38%] [G loss: 4.522543]\n",
      "epoch:8 step:6951 [D loss: 0.211062, acc.: 89.84%] [G loss: 3.763061]\n",
      "epoch:8 step:6952 [D loss: 0.126046, acc.: 97.66%] [G loss: 4.529064]\n",
      "epoch:8 step:6953 [D loss: 0.015220, acc.: 100.00%] [G loss: 4.397105]\n",
      "epoch:8 step:6954 [D loss: 0.029358, acc.: 100.00%] [G loss: 3.986808]\n",
      "epoch:8 step:6955 [D loss: 0.084894, acc.: 100.00%] [G loss: 2.373113]\n",
      "epoch:8 step:6956 [D loss: 0.067953, acc.: 98.44%] [G loss: 3.922581]\n",
      "epoch:8 step:6957 [D loss: 0.049015, acc.: 97.66%] [G loss: 2.372546]\n",
      "epoch:8 step:6958 [D loss: 0.048560, acc.: 100.00%] [G loss: 2.754521]\n",
      "epoch:8 step:6959 [D loss: 0.230423, acc.: 89.84%] [G loss: 2.947777]\n",
      "epoch:8 step:6960 [D loss: 0.103715, acc.: 96.88%] [G loss: 3.194001]\n",
      "epoch:8 step:6961 [D loss: 0.802951, acc.: 57.81%] [G loss: 6.086957]\n",
      "epoch:8 step:6962 [D loss: 1.320837, acc.: 50.78%] [G loss: 2.601386]\n",
      "epoch:8 step:6963 [D loss: 0.645944, acc.: 75.00%] [G loss: 4.779642]\n",
      "epoch:8 step:6964 [D loss: 0.326696, acc.: 83.59%] [G loss: 4.475476]\n",
      "epoch:8 step:6965 [D loss: 0.098346, acc.: 95.31%] [G loss: 2.946463]\n",
      "epoch:8 step:6966 [D loss: 0.037977, acc.: 99.22%] [G loss: 2.015887]\n",
      "epoch:8 step:6967 [D loss: 0.204777, acc.: 91.41%] [G loss: 3.532337]\n",
      "epoch:8 step:6968 [D loss: 0.365141, acc.: 84.38%] [G loss: 3.965603]\n",
      "epoch:8 step:6969 [D loss: 0.383233, acc.: 83.59%] [G loss: 1.346451]\n",
      "epoch:8 step:6970 [D loss: 0.411032, acc.: 80.47%] [G loss: 5.272486]\n",
      "epoch:8 step:6971 [D loss: 0.285420, acc.: 85.94%] [G loss: 4.655255]\n",
      "epoch:8 step:6972 [D loss: 0.169428, acc.: 94.53%] [G loss: 2.291794]\n",
      "epoch:8 step:6973 [D loss: 0.182551, acc.: 96.09%] [G loss: 2.212765]\n",
      "epoch:8 step:6974 [D loss: 0.087456, acc.: 97.66%] [G loss: 3.589861]\n",
      "epoch:8 step:6975 [D loss: 0.067043, acc.: 98.44%] [G loss: 3.518023]\n",
      "epoch:8 step:6976 [D loss: 0.059095, acc.: 99.22%] [G loss: 2.311110]\n",
      "epoch:8 step:6977 [D loss: 0.032118, acc.: 100.00%] [G loss: 1.130085]\n",
      "epoch:8 step:6978 [D loss: 0.097261, acc.: 98.44%] [G loss: 1.394154]\n",
      "epoch:8 step:6979 [D loss: 0.116807, acc.: 96.88%] [G loss: 3.452533]\n",
      "epoch:8 step:6980 [D loss: 0.216102, acc.: 93.75%] [G loss: 1.933571]\n",
      "epoch:8 step:6981 [D loss: 0.127264, acc.: 96.88%] [G loss: 2.579406]\n",
      "epoch:8 step:6982 [D loss: 0.090654, acc.: 97.66%] [G loss: 2.129365]\n",
      "epoch:8 step:6983 [D loss: 0.059822, acc.: 99.22%] [G loss: 1.896297]\n",
      "epoch:8 step:6984 [D loss: 0.082855, acc.: 99.22%] [G loss: 3.456600]\n",
      "epoch:8 step:6985 [D loss: 0.212120, acc.: 92.19%] [G loss: 3.487139]\n",
      "epoch:8 step:6986 [D loss: 0.078726, acc.: 97.66%] [G loss: 3.948161]\n",
      "epoch:8 step:6987 [D loss: 0.205565, acc.: 93.75%] [G loss: 2.625410]\n",
      "epoch:8 step:6988 [D loss: 0.071486, acc.: 98.44%] [G loss: 4.152788]\n",
      "epoch:8 step:6989 [D loss: 0.031210, acc.: 100.00%] [G loss: 3.373039]\n",
      "epoch:8 step:6990 [D loss: 0.105974, acc.: 98.44%] [G loss: 4.734971]\n",
      "epoch:8 step:6991 [D loss: 0.171104, acc.: 94.53%] [G loss: 3.247561]\n",
      "epoch:8 step:6992 [D loss: 0.057090, acc.: 100.00%] [G loss: 4.297959]\n",
      "epoch:8 step:6993 [D loss: 0.080354, acc.: 98.44%] [G loss: 4.419549]\n",
      "epoch:8 step:6994 [D loss: 0.089536, acc.: 98.44%] [G loss: 3.724849]\n",
      "epoch:8 step:6995 [D loss: 0.111197, acc.: 96.88%] [G loss: 2.155119]\n",
      "epoch:8 step:6996 [D loss: 0.265409, acc.: 85.94%] [G loss: 7.076814]\n",
      "epoch:8 step:6997 [D loss: 0.417346, acc.: 78.91%] [G loss: 4.431544]\n",
      "epoch:8 step:6998 [D loss: 0.042942, acc.: 100.00%] [G loss: 3.564782]\n",
      "epoch:8 step:6999 [D loss: 0.064282, acc.: 99.22%] [G loss: 5.203230]\n",
      "epoch:8 step:7000 [D loss: 0.012921, acc.: 100.00%] [G loss: 5.175278]\n",
      "##############\n",
      "[0.95542545 0.94645562 0.8376678  0.96115298 1.11065838 0.99654429\n",
      " 0.77261481 2.10845602 1.11646375 0.88617348]\n",
      "##########\n",
      "epoch:8 step:7001 [D loss: 0.129439, acc.: 96.88%] [G loss: 3.516787]\n",
      "epoch:8 step:7002 [D loss: 0.065113, acc.: 98.44%] [G loss: 5.099373]\n",
      "epoch:8 step:7003 [D loss: 0.012984, acc.: 100.00%] [G loss: 5.023010]\n",
      "epoch:8 step:7004 [D loss: 0.136189, acc.: 95.31%] [G loss: 4.492180]\n",
      "epoch:8 step:7005 [D loss: 0.060227, acc.: 100.00%] [G loss: 3.690088]\n",
      "epoch:8 step:7006 [D loss: 0.038429, acc.: 100.00%] [G loss: 3.627766]\n",
      "epoch:8 step:7007 [D loss: 0.035902, acc.: 98.44%] [G loss: 2.613184]\n",
      "epoch:8 step:7008 [D loss: 0.342595, acc.: 82.81%] [G loss: 7.348730]\n",
      "epoch:8 step:7009 [D loss: 0.539092, acc.: 76.56%] [G loss: 3.139315]\n",
      "epoch:8 step:7010 [D loss: 0.600247, acc.: 78.12%] [G loss: 8.094768]\n",
      "epoch:8 step:7011 [D loss: 1.139417, acc.: 56.25%] [G loss: 3.638271]\n",
      "epoch:8 step:7012 [D loss: 0.514253, acc.: 78.91%] [G loss: 6.669291]\n",
      "epoch:8 step:7013 [D loss: 0.091662, acc.: 96.88%] [G loss: 6.806159]\n",
      "epoch:8 step:7014 [D loss: 0.414783, acc.: 74.22%] [G loss: 3.080479]\n",
      "epoch:8 step:7015 [D loss: 0.368162, acc.: 81.25%] [G loss: 4.968822]\n",
      "epoch:8 step:7016 [D loss: 0.037341, acc.: 99.22%] [G loss: 6.243886]\n",
      "epoch:8 step:7017 [D loss: 0.168685, acc.: 94.53%] [G loss: 3.263480]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:8 step:7018 [D loss: 0.062282, acc.: 99.22%] [G loss: 4.024971]\n",
      "epoch:8 step:7019 [D loss: 0.124379, acc.: 95.31%] [G loss: 2.795928]\n",
      "epoch:8 step:7020 [D loss: 0.025720, acc.: 100.00%] [G loss: 2.369361]\n",
      "epoch:8 step:7021 [D loss: 0.108144, acc.: 97.66%] [G loss: 1.865642]\n",
      "epoch:8 step:7022 [D loss: 0.190541, acc.: 92.97%] [G loss: 4.969261]\n",
      "epoch:8 step:7023 [D loss: 0.478816, acc.: 78.91%] [G loss: 0.802852]\n",
      "epoch:8 step:7024 [D loss: 0.035302, acc.: 99.22%] [G loss: 0.910095]\n",
      "epoch:8 step:7025 [D loss: 0.324387, acc.: 83.59%] [G loss: 5.735541]\n",
      "epoch:8 step:7026 [D loss: 0.799245, acc.: 64.06%] [G loss: 1.468572]\n",
      "epoch:8 step:7027 [D loss: 0.216609, acc.: 91.41%] [G loss: 3.984368]\n",
      "epoch:8 step:7028 [D loss: 0.006225, acc.: 100.00%] [G loss: 5.084846]\n",
      "epoch:8 step:7029 [D loss: 0.050954, acc.: 96.88%] [G loss: 3.433689]\n",
      "epoch:9 step:7030 [D loss: 0.068834, acc.: 99.22%] [G loss: 2.351826]\n",
      "epoch:9 step:7031 [D loss: 0.021948, acc.: 100.00%] [G loss: 1.351826]\n",
      "epoch:9 step:7032 [D loss: 0.107481, acc.: 96.88%] [G loss: 2.267858]\n",
      "epoch:9 step:7033 [D loss: 0.013978, acc.: 100.00%] [G loss: 2.606727]\n",
      "epoch:9 step:7034 [D loss: 0.041735, acc.: 99.22%] [G loss: 1.923215]\n",
      "epoch:9 step:7035 [D loss: 0.362476, acc.: 83.59%] [G loss: 4.975881]\n",
      "epoch:9 step:7036 [D loss: 0.547381, acc.: 75.78%] [G loss: 2.636646]\n",
      "epoch:9 step:7037 [D loss: 0.108977, acc.: 94.53%] [G loss: 3.782856]\n",
      "epoch:9 step:7038 [D loss: 0.020764, acc.: 100.00%] [G loss: 4.207520]\n",
      "epoch:9 step:7039 [D loss: 0.143527, acc.: 94.53%] [G loss: 3.003843]\n",
      "epoch:9 step:7040 [D loss: 0.362715, acc.: 81.25%] [G loss: 6.347625]\n",
      "epoch:9 step:7041 [D loss: 0.244412, acc.: 87.50%] [G loss: 5.281645]\n",
      "epoch:9 step:7042 [D loss: 0.026007, acc.: 100.00%] [G loss: 4.070079]\n",
      "epoch:9 step:7043 [D loss: 0.086615, acc.: 97.66%] [G loss: 2.374299]\n",
      "epoch:9 step:7044 [D loss: 0.100849, acc.: 97.66%] [G loss: 1.657133]\n",
      "epoch:9 step:7045 [D loss: 0.037915, acc.: 99.22%] [G loss: 2.569088]\n",
      "epoch:9 step:7046 [D loss: 0.031909, acc.: 100.00%] [G loss: 2.549792]\n",
      "epoch:9 step:7047 [D loss: 0.075474, acc.: 99.22%] [G loss: 1.072554]\n",
      "epoch:9 step:7048 [D loss: 0.122049, acc.: 96.09%] [G loss: 3.364766]\n",
      "epoch:9 step:7049 [D loss: 0.055157, acc.: 99.22%] [G loss: 2.133337]\n",
      "epoch:9 step:7050 [D loss: 0.156951, acc.: 92.97%] [G loss: 2.635554]\n",
      "epoch:9 step:7051 [D loss: 0.094510, acc.: 97.66%] [G loss: 2.010349]\n",
      "epoch:9 step:7052 [D loss: 0.655344, acc.: 64.84%] [G loss: 8.324919]\n",
      "epoch:9 step:7053 [D loss: 1.769411, acc.: 50.78%] [G loss: 4.776786]\n",
      "epoch:9 step:7054 [D loss: 0.127128, acc.: 96.09%] [G loss: 3.553480]\n",
      "epoch:9 step:7055 [D loss: 0.266771, acc.: 87.50%] [G loss: 5.109077]\n",
      "epoch:9 step:7056 [D loss: 0.083821, acc.: 97.66%] [G loss: 5.257397]\n",
      "epoch:9 step:7057 [D loss: 0.184248, acc.: 93.75%] [G loss: 3.082303]\n",
      "epoch:9 step:7058 [D loss: 0.093786, acc.: 97.66%] [G loss: 2.369120]\n",
      "epoch:9 step:7059 [D loss: 0.015374, acc.: 100.00%] [G loss: 3.423741]\n",
      "epoch:9 step:7060 [D loss: 0.070247, acc.: 99.22%] [G loss: 1.636228]\n",
      "epoch:9 step:7061 [D loss: 0.101330, acc.: 98.44%] [G loss: 1.949649]\n",
      "epoch:9 step:7062 [D loss: 0.048651, acc.: 100.00%] [G loss: 2.194752]\n",
      "epoch:9 step:7063 [D loss: 0.062102, acc.: 97.66%] [G loss: 1.126020]\n",
      "epoch:9 step:7064 [D loss: 0.793343, acc.: 58.59%] [G loss: 6.474032]\n",
      "epoch:9 step:7065 [D loss: 1.473583, acc.: 50.78%] [G loss: 2.567425]\n",
      "epoch:9 step:7066 [D loss: 0.250645, acc.: 91.41%] [G loss: 5.481165]\n",
      "epoch:9 step:7067 [D loss: 0.342376, acc.: 82.81%] [G loss: 4.125626]\n",
      "epoch:9 step:7068 [D loss: 0.068175, acc.: 99.22%] [G loss: 2.876670]\n",
      "epoch:9 step:7069 [D loss: 0.032366, acc.: 100.00%] [G loss: 3.775426]\n",
      "epoch:9 step:7070 [D loss: 0.059998, acc.: 100.00%] [G loss: 3.538751]\n",
      "epoch:9 step:7071 [D loss: 0.096172, acc.: 97.66%] [G loss: 3.553546]\n",
      "epoch:9 step:7072 [D loss: 0.156045, acc.: 96.88%] [G loss: 2.783050]\n",
      "epoch:9 step:7073 [D loss: 0.065412, acc.: 99.22%] [G loss: 3.729318]\n",
      "epoch:9 step:7074 [D loss: 0.201107, acc.: 96.09%] [G loss: 2.020715]\n",
      "epoch:9 step:7075 [D loss: 0.302053, acc.: 85.94%] [G loss: 4.493023]\n",
      "epoch:9 step:7076 [D loss: 0.484177, acc.: 71.88%] [G loss: 2.284297]\n",
      "epoch:9 step:7077 [D loss: 0.068217, acc.: 99.22%] [G loss: 2.681880]\n",
      "epoch:9 step:7078 [D loss: 0.045164, acc.: 100.00%] [G loss: 3.066771]\n",
      "epoch:9 step:7079 [D loss: 0.147255, acc.: 99.22%] [G loss: 2.851423]\n",
      "epoch:9 step:7080 [D loss: 0.112657, acc.: 98.44%] [G loss: 3.824044]\n",
      "epoch:9 step:7081 [D loss: 0.142560, acc.: 98.44%] [G loss: 3.981520]\n",
      "epoch:9 step:7082 [D loss: 0.137439, acc.: 97.66%] [G loss: 2.854109]\n",
      "epoch:9 step:7083 [D loss: 0.239140, acc.: 92.97%] [G loss: 4.287806]\n",
      "epoch:9 step:7084 [D loss: 0.131377, acc.: 94.53%] [G loss: 3.943173]\n",
      "epoch:9 step:7085 [D loss: 0.059233, acc.: 99.22%] [G loss: 3.349633]\n",
      "epoch:9 step:7086 [D loss: 0.107290, acc.: 96.88%] [G loss: 3.364346]\n",
      "epoch:9 step:7087 [D loss: 0.048848, acc.: 100.00%] [G loss: 3.956491]\n",
      "epoch:9 step:7088 [D loss: 0.132454, acc.: 96.09%] [G loss: 4.655008]\n",
      "epoch:9 step:7089 [D loss: 0.374428, acc.: 85.16%] [G loss: 3.813177]\n",
      "epoch:9 step:7090 [D loss: 0.092121, acc.: 98.44%] [G loss: 3.777661]\n",
      "epoch:9 step:7091 [D loss: 0.224844, acc.: 90.62%] [G loss: 5.143665]\n",
      "epoch:9 step:7092 [D loss: 0.184915, acc.: 90.62%] [G loss: 3.920133]\n",
      "epoch:9 step:7093 [D loss: 0.063159, acc.: 99.22%] [G loss: 4.698957]\n",
      "epoch:9 step:7094 [D loss: 0.117582, acc.: 96.88%] [G loss: 4.531253]\n",
      "epoch:9 step:7095 [D loss: 0.027184, acc.: 100.00%] [G loss: 4.508552]\n",
      "epoch:9 step:7096 [D loss: 0.026058, acc.: 100.00%] [G loss: 4.047955]\n",
      "epoch:9 step:7097 [D loss: 0.047521, acc.: 100.00%] [G loss: 4.134343]\n",
      "epoch:9 step:7098 [D loss: 0.112290, acc.: 96.88%] [G loss: 4.343357]\n",
      "epoch:9 step:7099 [D loss: 0.090865, acc.: 100.00%] [G loss: 4.228113]\n",
      "epoch:9 step:7100 [D loss: 0.127704, acc.: 96.88%] [G loss: 3.104983]\n",
      "epoch:9 step:7101 [D loss: 0.058472, acc.: 98.44%] [G loss: 3.493876]\n",
      "epoch:9 step:7102 [D loss: 0.083147, acc.: 99.22%] [G loss: 4.664165]\n",
      "epoch:9 step:7103 [D loss: 0.065650, acc.: 100.00%] [G loss: 3.263572]\n",
      "epoch:9 step:7104 [D loss: 0.064569, acc.: 99.22%] [G loss: 3.713351]\n",
      "epoch:9 step:7105 [D loss: 1.429113, acc.: 37.50%] [G loss: 8.608486]\n",
      "epoch:9 step:7106 [D loss: 2.014832, acc.: 50.78%] [G loss: 5.256838]\n",
      "epoch:9 step:7107 [D loss: 0.038554, acc.: 100.00%] [G loss: 3.506615]\n",
      "epoch:9 step:7108 [D loss: 0.184153, acc.: 95.31%] [G loss: 3.707500]\n",
      "epoch:9 step:7109 [D loss: 0.259269, acc.: 88.28%] [G loss: 3.163497]\n",
      "epoch:9 step:7110 [D loss: 0.475989, acc.: 75.78%] [G loss: 5.096625]\n",
      "epoch:9 step:7111 [D loss: 1.102124, acc.: 48.44%] [G loss: 2.994227]\n",
      "epoch:9 step:7112 [D loss: 0.807460, acc.: 58.59%] [G loss: 4.772817]\n",
      "epoch:9 step:7113 [D loss: 0.861120, acc.: 57.81%] [G loss: 2.358151]\n",
      "epoch:9 step:7114 [D loss: 0.256330, acc.: 88.28%] [G loss: 3.985494]\n",
      "epoch:9 step:7115 [D loss: 0.140049, acc.: 94.53%] [G loss: 2.865713]\n",
      "epoch:9 step:7116 [D loss: 0.114874, acc.: 98.44%] [G loss: 3.011527]\n",
      "epoch:9 step:7117 [D loss: 0.246804, acc.: 90.62%] [G loss: 3.195620]\n",
      "epoch:9 step:7118 [D loss: 0.099078, acc.: 98.44%] [G loss: 3.423348]\n",
      "epoch:9 step:7119 [D loss: 0.241765, acc.: 96.09%] [G loss: 3.998745]\n",
      "epoch:9 step:7120 [D loss: 0.220452, acc.: 94.53%] [G loss: 4.020587]\n",
      "epoch:9 step:7121 [D loss: 0.220406, acc.: 92.19%] [G loss: 3.191421]\n",
      "epoch:9 step:7122 [D loss: 0.181653, acc.: 96.09%] [G loss: 4.074096]\n",
      "epoch:9 step:7123 [D loss: 0.116822, acc.: 95.31%] [G loss: 3.646041]\n",
      "epoch:9 step:7124 [D loss: 0.200891, acc.: 91.41%] [G loss: 3.781130]\n",
      "epoch:9 step:7125 [D loss: 0.070735, acc.: 100.00%] [G loss: 3.916686]\n",
      "epoch:9 step:7126 [D loss: 0.181350, acc.: 96.09%] [G loss: 2.697430]\n",
      "epoch:9 step:7127 [D loss: 0.159216, acc.: 94.53%] [G loss: 4.009522]\n",
      "epoch:9 step:7128 [D loss: 0.306542, acc.: 88.28%] [G loss: 2.994998]\n",
      "epoch:9 step:7129 [D loss: 0.138431, acc.: 96.09%] [G loss: 3.799881]\n",
      "epoch:9 step:7130 [D loss: 0.081367, acc.: 100.00%] [G loss: 3.424272]\n",
      "epoch:9 step:7131 [D loss: 0.042413, acc.: 100.00%] [G loss: 3.116396]\n",
      "epoch:9 step:7132 [D loss: 0.812058, acc.: 57.81%] [G loss: 6.688012]\n",
      "epoch:9 step:7133 [D loss: 1.112186, acc.: 53.91%] [G loss: 2.830507]\n",
      "epoch:9 step:7134 [D loss: 0.533835, acc.: 75.78%] [G loss: 4.632705]\n",
      "epoch:9 step:7135 [D loss: 0.161589, acc.: 94.53%] [G loss: 4.811633]\n",
      "epoch:9 step:7136 [D loss: 0.362616, acc.: 82.81%] [G loss: 2.875218]\n",
      "epoch:9 step:7137 [D loss: 0.176810, acc.: 94.53%] [G loss: 2.333694]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:9 step:7138 [D loss: 0.049687, acc.: 100.00%] [G loss: 3.472201]\n",
      "epoch:9 step:7139 [D loss: 0.186565, acc.: 92.97%] [G loss: 4.230136]\n",
      "epoch:9 step:7140 [D loss: 0.084125, acc.: 99.22%] [G loss: 4.538268]\n",
      "epoch:9 step:7141 [D loss: 0.095382, acc.: 99.22%] [G loss: 4.170666]\n",
      "epoch:9 step:7142 [D loss: 0.061566, acc.: 100.00%] [G loss: 3.561175]\n",
      "epoch:9 step:7143 [D loss: 0.087202, acc.: 98.44%] [G loss: 3.828897]\n",
      "epoch:9 step:7144 [D loss: 0.044468, acc.: 99.22%] [G loss: 2.893896]\n",
      "epoch:9 step:7145 [D loss: 0.158518, acc.: 96.88%] [G loss: 3.818044]\n",
      "epoch:9 step:7146 [D loss: 0.097998, acc.: 97.66%] [G loss: 3.291492]\n",
      "epoch:9 step:7147 [D loss: 0.106915, acc.: 97.66%] [G loss: 2.849750]\n",
      "epoch:9 step:7148 [D loss: 0.063086, acc.: 98.44%] [G loss: 2.416659]\n",
      "epoch:9 step:7149 [D loss: 0.285924, acc.: 85.94%] [G loss: 5.189148]\n",
      "epoch:9 step:7150 [D loss: 0.463088, acc.: 74.22%] [G loss: 2.764382]\n",
      "epoch:9 step:7151 [D loss: 0.078620, acc.: 100.00%] [G loss: 3.107589]\n",
      "epoch:9 step:7152 [D loss: 0.065134, acc.: 100.00%] [G loss: 3.880320]\n",
      "epoch:9 step:7153 [D loss: 0.059527, acc.: 98.44%] [G loss: 3.560744]\n",
      "epoch:9 step:7154 [D loss: 0.114926, acc.: 97.66%] [G loss: 3.115773]\n",
      "epoch:9 step:7155 [D loss: 0.057172, acc.: 99.22%] [G loss: 2.409787]\n",
      "epoch:9 step:7156 [D loss: 0.044256, acc.: 100.00%] [G loss: 3.884815]\n",
      "epoch:9 step:7157 [D loss: 0.111698, acc.: 98.44%] [G loss: 2.438330]\n",
      "epoch:9 step:7158 [D loss: 0.129818, acc.: 96.88%] [G loss: 2.223875]\n",
      "epoch:9 step:7159 [D loss: 0.126001, acc.: 96.88%] [G loss: 1.713209]\n",
      "epoch:9 step:7160 [D loss: 0.075945, acc.: 98.44%] [G loss: 3.904778]\n",
      "epoch:9 step:7161 [D loss: 0.938956, acc.: 52.34%] [G loss: 6.107985]\n",
      "epoch:9 step:7162 [D loss: 0.270839, acc.: 85.94%] [G loss: 4.544238]\n",
      "epoch:9 step:7163 [D loss: 0.129829, acc.: 94.53%] [G loss: 3.283666]\n",
      "epoch:9 step:7164 [D loss: 0.349232, acc.: 85.16%] [G loss: 6.174084]\n",
      "epoch:9 step:7165 [D loss: 0.134009, acc.: 92.97%] [G loss: 5.324321]\n",
      "epoch:9 step:7166 [D loss: 0.064562, acc.: 99.22%] [G loss: 4.160298]\n",
      "epoch:9 step:7167 [D loss: 0.040141, acc.: 100.00%] [G loss: 3.746681]\n",
      "epoch:9 step:7168 [D loss: 0.095783, acc.: 98.44%] [G loss: 3.446787]\n",
      "epoch:9 step:7169 [D loss: 0.230079, acc.: 92.19%] [G loss: 4.762910]\n",
      "epoch:9 step:7170 [D loss: 0.142658, acc.: 94.53%] [G loss: 4.015560]\n",
      "epoch:9 step:7171 [D loss: 0.113665, acc.: 96.88%] [G loss: 4.449378]\n",
      "epoch:9 step:7172 [D loss: 0.076798, acc.: 98.44%] [G loss: 4.939509]\n",
      "epoch:9 step:7173 [D loss: 0.114553, acc.: 96.88%] [G loss: 3.600002]\n",
      "epoch:9 step:7174 [D loss: 0.047471, acc.: 99.22%] [G loss: 3.710541]\n",
      "epoch:9 step:7175 [D loss: 0.031601, acc.: 100.00%] [G loss: 3.405680]\n",
      "epoch:9 step:7176 [D loss: 0.080118, acc.: 100.00%] [G loss: 2.951681]\n",
      "epoch:9 step:7177 [D loss: 0.093866, acc.: 98.44%] [G loss: 2.333519]\n",
      "epoch:9 step:7178 [D loss: 0.063652, acc.: 99.22%] [G loss: 3.191138]\n",
      "epoch:9 step:7179 [D loss: 0.395582, acc.: 84.38%] [G loss: 5.575047]\n",
      "epoch:9 step:7180 [D loss: 0.469403, acc.: 76.56%] [G loss: 3.865662]\n",
      "epoch:9 step:7181 [D loss: 0.146716, acc.: 94.53%] [G loss: 5.158151]\n",
      "epoch:9 step:7182 [D loss: 0.128165, acc.: 95.31%] [G loss: 3.793431]\n",
      "epoch:9 step:7183 [D loss: 0.044765, acc.: 100.00%] [G loss: 4.475083]\n",
      "epoch:9 step:7184 [D loss: 0.046195, acc.: 100.00%] [G loss: 3.646156]\n",
      "epoch:9 step:7185 [D loss: 0.235983, acc.: 89.06%] [G loss: 3.627130]\n",
      "epoch:9 step:7186 [D loss: 0.129141, acc.: 96.88%] [G loss: 5.448136]\n",
      "epoch:9 step:7187 [D loss: 0.272048, acc.: 86.72%] [G loss: 1.972653]\n",
      "epoch:9 step:7188 [D loss: 0.343351, acc.: 83.59%] [G loss: 6.200116]\n",
      "epoch:9 step:7189 [D loss: 0.299972, acc.: 85.94%] [G loss: 4.175760]\n",
      "epoch:9 step:7190 [D loss: 0.079272, acc.: 98.44%] [G loss: 3.099497]\n",
      "epoch:9 step:7191 [D loss: 0.080289, acc.: 98.44%] [G loss: 4.501449]\n",
      "epoch:9 step:7192 [D loss: 1.057124, acc.: 53.12%] [G loss: 7.502457]\n",
      "epoch:9 step:7193 [D loss: 1.163254, acc.: 57.81%] [G loss: 4.834029]\n",
      "epoch:9 step:7194 [D loss: 0.217858, acc.: 87.50%] [G loss: 3.795745]\n",
      "epoch:9 step:7195 [D loss: 0.028107, acc.: 100.00%] [G loss: 4.261143]\n",
      "epoch:9 step:7196 [D loss: 0.100146, acc.: 97.66%] [G loss: 2.342953]\n",
      "epoch:9 step:7197 [D loss: 0.131244, acc.: 97.66%] [G loss: 3.217756]\n",
      "epoch:9 step:7198 [D loss: 0.156060, acc.: 93.75%] [G loss: 5.538765]\n",
      "epoch:9 step:7199 [D loss: 0.481579, acc.: 79.69%] [G loss: 1.249492]\n",
      "epoch:9 step:7200 [D loss: 0.161504, acc.: 94.53%] [G loss: 1.545120]\n",
      "##############\n",
      "[0.96692225 0.83152771 0.75297871 1.00293124 2.10945785 0.95145007\n",
      " 2.10913915 2.11077148 2.11108515 2.10846452]\n",
      "##########\n",
      "epoch:9 step:7201 [D loss: 0.030598, acc.: 100.00%] [G loss: 2.556911]\n",
      "epoch:9 step:7202 [D loss: 0.194934, acc.: 92.97%] [G loss: 4.106539]\n",
      "epoch:9 step:7203 [D loss: 0.778606, acc.: 65.62%] [G loss: 6.118490]\n",
      "epoch:9 step:7204 [D loss: 0.311067, acc.: 85.16%] [G loss: 4.856052]\n",
      "epoch:9 step:7205 [D loss: 0.155975, acc.: 94.53%] [G loss: 2.067482]\n",
      "epoch:9 step:7206 [D loss: 0.239009, acc.: 87.50%] [G loss: 5.507279]\n",
      "epoch:9 step:7207 [D loss: 0.215238, acc.: 89.06%] [G loss: 3.273066]\n",
      "epoch:9 step:7208 [D loss: 0.123940, acc.: 94.53%] [G loss: 2.293986]\n",
      "epoch:9 step:7209 [D loss: 0.018846, acc.: 100.00%] [G loss: 1.767331]\n",
      "epoch:9 step:7210 [D loss: 0.358140, acc.: 82.81%] [G loss: 6.471964]\n",
      "epoch:9 step:7211 [D loss: 0.148922, acc.: 92.97%] [G loss: 4.941616]\n",
      "epoch:9 step:7212 [D loss: 0.071295, acc.: 96.88%] [G loss: 3.751375]\n",
      "epoch:9 step:7213 [D loss: 0.186230, acc.: 90.62%] [G loss: 1.530796]\n",
      "epoch:9 step:7214 [D loss: 0.352096, acc.: 82.03%] [G loss: 4.594625]\n",
      "epoch:9 step:7215 [D loss: 0.113154, acc.: 96.88%] [G loss: 5.958537]\n",
      "epoch:9 step:7216 [D loss: 1.106883, acc.: 50.00%] [G loss: 6.341092]\n",
      "epoch:9 step:7217 [D loss: 0.169181, acc.: 91.41%] [G loss: 5.811398]\n",
      "epoch:9 step:7218 [D loss: 0.092356, acc.: 98.44%] [G loss: 4.707616]\n",
      "epoch:9 step:7219 [D loss: 0.151743, acc.: 95.31%] [G loss: 3.732844]\n",
      "epoch:9 step:7220 [D loss: 0.321797, acc.: 84.38%] [G loss: 6.802224]\n",
      "epoch:9 step:7221 [D loss: 0.077991, acc.: 98.44%] [G loss: 6.269701]\n",
      "epoch:9 step:7222 [D loss: 0.305194, acc.: 83.59%] [G loss: 2.928681]\n",
      "epoch:9 step:7223 [D loss: 0.031242, acc.: 100.00%] [G loss: 3.273784]\n",
      "epoch:9 step:7224 [D loss: 0.089084, acc.: 98.44%] [G loss: 4.382546]\n",
      "epoch:9 step:7225 [D loss: 0.038983, acc.: 100.00%] [G loss: 3.967673]\n",
      "epoch:9 step:7226 [D loss: 0.050338, acc.: 98.44%] [G loss: 2.895142]\n",
      "epoch:9 step:7227 [D loss: 0.054313, acc.: 99.22%] [G loss: 3.119098]\n",
      "epoch:9 step:7228 [D loss: 0.063225, acc.: 98.44%] [G loss: 4.555696]\n",
      "epoch:9 step:7229 [D loss: 0.095112, acc.: 98.44%] [G loss: 3.607602]\n",
      "epoch:9 step:7230 [D loss: 0.036419, acc.: 100.00%] [G loss: 3.429321]\n",
      "epoch:9 step:7231 [D loss: 0.123818, acc.: 95.31%] [G loss: 3.894942]\n",
      "epoch:9 step:7232 [D loss: 0.149254, acc.: 95.31%] [G loss: 2.316223]\n",
      "epoch:9 step:7233 [D loss: 0.185264, acc.: 94.53%] [G loss: 4.187555]\n",
      "epoch:9 step:7234 [D loss: 0.363319, acc.: 85.16%] [G loss: 3.254866]\n",
      "epoch:9 step:7235 [D loss: 0.087692, acc.: 100.00%] [G loss: 3.099311]\n",
      "epoch:9 step:7236 [D loss: 0.010141, acc.: 100.00%] [G loss: 3.621806]\n",
      "epoch:9 step:7237 [D loss: 0.414424, acc.: 81.25%] [G loss: 5.663502]\n",
      "epoch:9 step:7238 [D loss: 0.380205, acc.: 81.25%] [G loss: 2.989138]\n",
      "epoch:9 step:7239 [D loss: 0.131257, acc.: 94.53%] [G loss: 3.930757]\n",
      "epoch:9 step:7240 [D loss: 0.007671, acc.: 100.00%] [G loss: 4.840827]\n",
      "epoch:9 step:7241 [D loss: 0.014559, acc.: 100.00%] [G loss: 4.950329]\n",
      "epoch:9 step:7242 [D loss: 0.051023, acc.: 99.22%] [G loss: 3.330037]\n",
      "epoch:9 step:7243 [D loss: 0.282385, acc.: 89.84%] [G loss: 1.536219]\n",
      "epoch:9 step:7244 [D loss: 0.008275, acc.: 100.00%] [G loss: 3.299071]\n",
      "epoch:9 step:7245 [D loss: 0.024543, acc.: 100.00%] [G loss: 3.212563]\n",
      "epoch:9 step:7246 [D loss: 0.027204, acc.: 99.22%] [G loss: 1.731688]\n",
      "epoch:9 step:7247 [D loss: 0.108773, acc.: 97.66%] [G loss: 0.277140]\n",
      "epoch:9 step:7248 [D loss: 0.014062, acc.: 100.00%] [G loss: 0.964366]\n",
      "epoch:9 step:7249 [D loss: 0.037733, acc.: 99.22%] [G loss: 0.237126]\n",
      "epoch:9 step:7250 [D loss: 0.029335, acc.: 100.00%] [G loss: 1.571976]\n",
      "epoch:9 step:7251 [D loss: 0.137509, acc.: 94.53%] [G loss: 4.334137]\n",
      "epoch:9 step:7252 [D loss: 0.447749, acc.: 80.47%] [G loss: 3.427866]\n",
      "epoch:9 step:7253 [D loss: 0.025728, acc.: 100.00%] [G loss: 3.729223]\n",
      "epoch:9 step:7254 [D loss: 0.043470, acc.: 99.22%] [G loss: 3.589490]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:9 step:7255 [D loss: 0.158850, acc.: 96.09%] [G loss: 4.903471]\n",
      "epoch:9 step:7256 [D loss: 0.594331, acc.: 68.75%] [G loss: 6.112799]\n",
      "epoch:9 step:7257 [D loss: 0.024961, acc.: 100.00%] [G loss: 7.054609]\n",
      "epoch:9 step:7258 [D loss: 0.170613, acc.: 92.97%] [G loss: 3.711132]\n",
      "epoch:9 step:7259 [D loss: 0.090098, acc.: 97.66%] [G loss: 4.337643]\n",
      "epoch:9 step:7260 [D loss: 0.016028, acc.: 100.00%] [G loss: 4.577325]\n",
      "epoch:9 step:7261 [D loss: 1.734489, acc.: 37.50%] [G loss: 7.694622]\n",
      "epoch:9 step:7262 [D loss: 0.999051, acc.: 57.03%] [G loss: 4.294372]\n",
      "epoch:9 step:7263 [D loss: 0.127177, acc.: 96.09%] [G loss: 3.010771]\n",
      "epoch:9 step:7264 [D loss: 0.489520, acc.: 78.12%] [G loss: 5.272022]\n",
      "epoch:9 step:7265 [D loss: 0.659280, acc.: 68.75%] [G loss: 1.931931]\n",
      "epoch:9 step:7266 [D loss: 0.165039, acc.: 90.62%] [G loss: 2.816116]\n",
      "epoch:9 step:7267 [D loss: 0.029256, acc.: 100.00%] [G loss: 3.225214]\n",
      "epoch:9 step:7268 [D loss: 0.065812, acc.: 97.66%] [G loss: 2.338346]\n",
      "epoch:9 step:7269 [D loss: 1.593608, acc.: 27.34%] [G loss: 5.384061]\n",
      "epoch:9 step:7270 [D loss: 0.140048, acc.: 92.19%] [G loss: 6.195614]\n",
      "epoch:9 step:7271 [D loss: 0.287068, acc.: 88.28%] [G loss: 3.502187]\n",
      "epoch:9 step:7272 [D loss: 0.022968, acc.: 100.00%] [G loss: 2.088400]\n",
      "epoch:9 step:7273 [D loss: 0.145426, acc.: 96.09%] [G loss: 3.915425]\n",
      "epoch:9 step:7274 [D loss: 0.015455, acc.: 100.00%] [G loss: 4.632800]\n",
      "epoch:9 step:7275 [D loss: 0.102897, acc.: 96.09%] [G loss: 2.723525]\n",
      "epoch:9 step:7276 [D loss: 0.208313, acc.: 92.97%] [G loss: 2.981201]\n",
      "epoch:9 step:7277 [D loss: 0.036300, acc.: 99.22%] [G loss: 3.786465]\n",
      "epoch:9 step:7278 [D loss: 0.297581, acc.: 89.06%] [G loss: 4.217783]\n",
      "epoch:9 step:7279 [D loss: 0.265419, acc.: 85.94%] [G loss: 2.795703]\n",
      "epoch:9 step:7280 [D loss: 0.251576, acc.: 88.28%] [G loss: 4.954607]\n",
      "epoch:9 step:7281 [D loss: 0.122389, acc.: 95.31%] [G loss: 3.605269]\n",
      "epoch:9 step:7282 [D loss: 0.050446, acc.: 100.00%] [G loss: 2.869014]\n",
      "epoch:9 step:7283 [D loss: 0.084223, acc.: 98.44%] [G loss: 2.654818]\n",
      "epoch:9 step:7284 [D loss: 0.187161, acc.: 96.09%] [G loss: 3.046933]\n",
      "epoch:9 step:7285 [D loss: 0.053412, acc.: 99.22%] [G loss: 3.061709]\n",
      "epoch:9 step:7286 [D loss: 0.206116, acc.: 95.31%] [G loss: 3.160426]\n",
      "epoch:9 step:7287 [D loss: 0.139613, acc.: 97.66%] [G loss: 2.351164]\n",
      "epoch:9 step:7288 [D loss: 0.120145, acc.: 96.09%] [G loss: 0.509525]\n",
      "epoch:9 step:7289 [D loss: 0.099160, acc.: 99.22%] [G loss: 1.569863]\n",
      "epoch:9 step:7290 [D loss: 0.041255, acc.: 99.22%] [G loss: 1.142261]\n",
      "epoch:9 step:7291 [D loss: 0.015782, acc.: 100.00%] [G loss: 1.002064]\n",
      "epoch:9 step:7292 [D loss: 0.056613, acc.: 97.66%] [G loss: 0.486273]\n",
      "epoch:9 step:7293 [D loss: 0.574397, acc.: 74.22%] [G loss: 5.232633]\n",
      "epoch:9 step:7294 [D loss: 0.480126, acc.: 75.00%] [G loss: 3.829168]\n",
      "epoch:9 step:7295 [D loss: 0.045374, acc.: 100.00%] [G loss: 1.278904]\n",
      "epoch:9 step:7296 [D loss: 0.481847, acc.: 75.00%] [G loss: 3.702749]\n",
      "epoch:9 step:7297 [D loss: 0.018289, acc.: 100.00%] [G loss: 6.452778]\n",
      "epoch:9 step:7298 [D loss: 0.557536, acc.: 74.22%] [G loss: 1.546679]\n",
      "epoch:9 step:7299 [D loss: 0.349194, acc.: 80.47%] [G loss: 4.476147]\n",
      "epoch:9 step:7300 [D loss: 0.109869, acc.: 96.88%] [G loss: 5.191920]\n",
      "epoch:9 step:7301 [D loss: 0.108136, acc.: 96.09%] [G loss: 2.931487]\n",
      "epoch:9 step:7302 [D loss: 0.046207, acc.: 99.22%] [G loss: 3.044999]\n",
      "epoch:9 step:7303 [D loss: 0.201148, acc.: 93.75%] [G loss: 4.807328]\n",
      "epoch:9 step:7304 [D loss: 0.087863, acc.: 97.66%] [G loss: 4.737744]\n",
      "epoch:9 step:7305 [D loss: 0.137895, acc.: 93.75%] [G loss: 3.028528]\n",
      "epoch:9 step:7306 [D loss: 0.155292, acc.: 95.31%] [G loss: 4.166094]\n",
      "epoch:9 step:7307 [D loss: 0.105190, acc.: 98.44%] [G loss: 3.648597]\n",
      "epoch:9 step:7308 [D loss: 0.182190, acc.: 92.97%] [G loss: 1.559376]\n",
      "epoch:9 step:7309 [D loss: 0.435733, acc.: 77.34%] [G loss: 5.163920]\n",
      "epoch:9 step:7310 [D loss: 0.153858, acc.: 93.75%] [G loss: 5.833921]\n",
      "epoch:9 step:7311 [D loss: 0.284609, acc.: 84.38%] [G loss: 2.862287]\n",
      "epoch:9 step:7312 [D loss: 0.062820, acc.: 97.66%] [G loss: 1.172278]\n",
      "epoch:9 step:7313 [D loss: 0.551846, acc.: 71.88%] [G loss: 6.176287]\n",
      "epoch:9 step:7314 [D loss: 0.456128, acc.: 77.34%] [G loss: 5.720143]\n",
      "epoch:9 step:7315 [D loss: 0.290756, acc.: 84.38%] [G loss: 2.176654]\n",
      "epoch:9 step:7316 [D loss: 0.224213, acc.: 89.06%] [G loss: 4.345398]\n",
      "epoch:9 step:7317 [D loss: 0.021567, acc.: 100.00%] [G loss: 4.503715]\n",
      "epoch:9 step:7318 [D loss: 0.076622, acc.: 97.66%] [G loss: 3.160654]\n",
      "epoch:9 step:7319 [D loss: 0.025800, acc.: 100.00%] [G loss: 2.702134]\n",
      "epoch:9 step:7320 [D loss: 0.025493, acc.: 100.00%] [G loss: 1.473948]\n",
      "epoch:9 step:7321 [D loss: 0.085675, acc.: 99.22%] [G loss: 2.102529]\n",
      "epoch:9 step:7322 [D loss: 0.290918, acc.: 85.16%] [G loss: 4.162259]\n",
      "epoch:9 step:7323 [D loss: 0.155832, acc.: 94.53%] [G loss: 4.214936]\n",
      "epoch:9 step:7324 [D loss: 0.735835, acc.: 61.72%] [G loss: 4.057693]\n",
      "epoch:9 step:7325 [D loss: 0.086246, acc.: 96.88%] [G loss: 4.126021]\n",
      "epoch:9 step:7326 [D loss: 0.132380, acc.: 97.66%] [G loss: 4.622329]\n",
      "epoch:9 step:7327 [D loss: 0.131496, acc.: 96.88%] [G loss: 4.437494]\n",
      "epoch:9 step:7328 [D loss: 0.155495, acc.: 96.88%] [G loss: 3.961386]\n",
      "epoch:9 step:7329 [D loss: 0.105394, acc.: 98.44%] [G loss: 3.557382]\n",
      "epoch:9 step:7330 [D loss: 0.165995, acc.: 94.53%] [G loss: 5.248368]\n",
      "epoch:9 step:7331 [D loss: 0.228223, acc.: 91.41%] [G loss: 3.944661]\n",
      "epoch:9 step:7332 [D loss: 0.112220, acc.: 96.88%] [G loss: 4.473794]\n",
      "epoch:9 step:7333 [D loss: 0.018739, acc.: 100.00%] [G loss: 4.846241]\n",
      "epoch:9 step:7334 [D loss: 0.143602, acc.: 96.09%] [G loss: 4.749113]\n",
      "epoch:9 step:7335 [D loss: 0.049583, acc.: 100.00%] [G loss: 3.725373]\n",
      "epoch:9 step:7336 [D loss: 0.239203, acc.: 92.97%] [G loss: 5.429209]\n",
      "epoch:9 step:7337 [D loss: 0.110172, acc.: 93.75%] [G loss: 4.502196]\n",
      "epoch:9 step:7338 [D loss: 0.312367, acc.: 89.84%] [G loss: 4.235552]\n",
      "epoch:9 step:7339 [D loss: 0.022829, acc.: 100.00%] [G loss: 4.191575]\n",
      "epoch:9 step:7340 [D loss: 0.033287, acc.: 100.00%] [G loss: 4.096227]\n",
      "epoch:9 step:7341 [D loss: 0.080477, acc.: 99.22%] [G loss: 3.231749]\n",
      "epoch:9 step:7342 [D loss: 0.083795, acc.: 99.22%] [G loss: 3.588787]\n",
      "epoch:9 step:7343 [D loss: 0.095942, acc.: 98.44%] [G loss: 4.013223]\n",
      "epoch:9 step:7344 [D loss: 0.855704, acc.: 53.12%] [G loss: 6.582052]\n",
      "epoch:9 step:7345 [D loss: 0.306177, acc.: 82.81%] [G loss: 4.660849]\n",
      "epoch:9 step:7346 [D loss: 0.029200, acc.: 100.00%] [G loss: 3.645111]\n",
      "epoch:9 step:7347 [D loss: 0.018901, acc.: 100.00%] [G loss: 3.922648]\n",
      "epoch:9 step:7348 [D loss: 0.027136, acc.: 100.00%] [G loss: 4.042956]\n",
      "epoch:9 step:7349 [D loss: 0.867914, acc.: 54.69%] [G loss: 5.778662]\n",
      "epoch:9 step:7350 [D loss: 0.471867, acc.: 73.44%] [G loss: 4.642971]\n",
      "epoch:9 step:7351 [D loss: 0.031590, acc.: 100.00%] [G loss: 3.371753]\n",
      "epoch:9 step:7352 [D loss: 0.137371, acc.: 96.09%] [G loss: 3.964953]\n",
      "epoch:9 step:7353 [D loss: 0.131946, acc.: 95.31%] [G loss: 3.957884]\n",
      "epoch:9 step:7354 [D loss: 0.084778, acc.: 98.44%] [G loss: 2.990121]\n",
      "epoch:9 step:7355 [D loss: 0.030757, acc.: 100.00%] [G loss: 3.989267]\n",
      "epoch:9 step:7356 [D loss: 0.096310, acc.: 97.66%] [G loss: 3.882665]\n",
      "epoch:9 step:7357 [D loss: 0.033713, acc.: 99.22%] [G loss: 4.136518]\n",
      "epoch:9 step:7358 [D loss: 0.039483, acc.: 99.22%] [G loss: 3.557214]\n",
      "epoch:9 step:7359 [D loss: 0.171137, acc.: 92.97%] [G loss: 4.433840]\n",
      "epoch:9 step:7360 [D loss: 1.577799, acc.: 34.38%] [G loss: 6.513500]\n",
      "epoch:9 step:7361 [D loss: 0.693711, acc.: 67.97%] [G loss: 5.396744]\n",
      "epoch:9 step:7362 [D loss: 0.031893, acc.: 100.00%] [G loss: 3.958091]\n",
      "epoch:9 step:7363 [D loss: 0.052065, acc.: 99.22%] [G loss: 4.876781]\n",
      "epoch:9 step:7364 [D loss: 0.047507, acc.: 100.00%] [G loss: 4.946983]\n",
      "epoch:9 step:7365 [D loss: 0.248991, acc.: 89.06%] [G loss: 1.894784]\n",
      "epoch:9 step:7366 [D loss: 0.330946, acc.: 83.59%] [G loss: 5.379458]\n",
      "epoch:9 step:7367 [D loss: 0.261923, acc.: 88.28%] [G loss: 3.868219]\n",
      "epoch:9 step:7368 [D loss: 0.043135, acc.: 100.00%] [G loss: 3.379426]\n",
      "epoch:9 step:7369 [D loss: 0.140895, acc.: 95.31%] [G loss: 2.129228]\n",
      "epoch:9 step:7370 [D loss: 0.329510, acc.: 84.38%] [G loss: 4.697286]\n",
      "epoch:9 step:7371 [D loss: 0.541379, acc.: 77.34%] [G loss: 3.012092]\n",
      "epoch:9 step:7372 [D loss: 0.062680, acc.: 99.22%] [G loss: 1.989395]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:9 step:7373 [D loss: 0.068274, acc.: 99.22%] [G loss: 2.681621]\n",
      "epoch:9 step:7374 [D loss: 0.028933, acc.: 100.00%] [G loss: 3.744289]\n",
      "epoch:9 step:7375 [D loss: 0.128015, acc.: 96.88%] [G loss: 3.462229]\n",
      "epoch:9 step:7376 [D loss: 0.028411, acc.: 99.22%] [G loss: 3.364881]\n",
      "epoch:9 step:7377 [D loss: 0.046299, acc.: 100.00%] [G loss: 2.568302]\n",
      "epoch:9 step:7378 [D loss: 0.048784, acc.: 100.00%] [G loss: 2.062685]\n",
      "epoch:9 step:7379 [D loss: 0.176633, acc.: 97.66%] [G loss: 3.343558]\n",
      "epoch:9 step:7380 [D loss: 0.031205, acc.: 99.22%] [G loss: 2.600654]\n",
      "epoch:9 step:7381 [D loss: 0.044832, acc.: 99.22%] [G loss: 2.436070]\n",
      "epoch:9 step:7382 [D loss: 0.018833, acc.: 100.00%] [G loss: 1.256492]\n",
      "epoch:9 step:7383 [D loss: 0.046652, acc.: 99.22%] [G loss: 1.382534]\n",
      "epoch:9 step:7384 [D loss: 0.053449, acc.: 99.22%] [G loss: 1.492421]\n",
      "epoch:9 step:7385 [D loss: 0.187062, acc.: 93.75%] [G loss: 4.756641]\n",
      "epoch:9 step:7386 [D loss: 0.238421, acc.: 92.19%] [G loss: 3.301407]\n",
      "epoch:9 step:7387 [D loss: 0.074759, acc.: 99.22%] [G loss: 0.499815]\n",
      "epoch:9 step:7388 [D loss: 0.711160, acc.: 64.06%] [G loss: 7.412161]\n",
      "epoch:9 step:7389 [D loss: 1.060486, acc.: 56.25%] [G loss: 5.701057]\n",
      "epoch:9 step:7390 [D loss: 0.091794, acc.: 96.88%] [G loss: 2.657579]\n",
      "epoch:9 step:7391 [D loss: 0.129809, acc.: 96.09%] [G loss: 4.065687]\n",
      "epoch:9 step:7392 [D loss: 0.012107, acc.: 100.00%] [G loss: 5.180288]\n",
      "epoch:9 step:7393 [D loss: 0.150684, acc.: 96.09%] [G loss: 2.097662]\n",
      "epoch:9 step:7394 [D loss: 0.175843, acc.: 93.75%] [G loss: 3.955152]\n",
      "epoch:9 step:7395 [D loss: 0.034584, acc.: 100.00%] [G loss: 5.227993]\n",
      "epoch:9 step:7396 [D loss: 0.172332, acc.: 93.75%] [G loss: 3.241421]\n",
      "epoch:9 step:7397 [D loss: 0.162046, acc.: 93.75%] [G loss: 5.460845]\n",
      "epoch:9 step:7398 [D loss: 0.121019, acc.: 94.53%] [G loss: 4.355947]\n",
      "epoch:9 step:7399 [D loss: 0.513550, acc.: 79.69%] [G loss: 5.954407]\n",
      "epoch:9 step:7400 [D loss: 0.053691, acc.: 98.44%] [G loss: 6.834724]\n",
      "##############\n",
      "[1.05022234 0.95936439 0.76885068 1.11247076 2.10846971 0.91043276\n",
      " 1.11770227 2.12155533 0.99667226 1.00212538]\n",
      "##########\n",
      "epoch:9 step:7401 [D loss: 0.268386, acc.: 84.38%] [G loss: 1.964341]\n",
      "epoch:9 step:7402 [D loss: 0.291509, acc.: 84.38%] [G loss: 6.209637]\n",
      "epoch:9 step:7403 [D loss: 0.162340, acc.: 93.75%] [G loss: 4.818159]\n",
      "epoch:9 step:7404 [D loss: 0.153390, acc.: 95.31%] [G loss: 2.180412]\n",
      "epoch:9 step:7405 [D loss: 0.042623, acc.: 98.44%] [G loss: 1.835631]\n",
      "epoch:9 step:7406 [D loss: 0.020943, acc.: 100.00%] [G loss: 1.321068]\n",
      "epoch:9 step:7407 [D loss: 0.095237, acc.: 97.66%] [G loss: 2.593699]\n",
      "epoch:9 step:7408 [D loss: 0.044344, acc.: 99.22%] [G loss: 2.150928]\n",
      "epoch:9 step:7409 [D loss: 0.053010, acc.: 99.22%] [G loss: 0.526132]\n",
      "epoch:9 step:7410 [D loss: 0.206935, acc.: 91.41%] [G loss: 2.954395]\n",
      "epoch:9 step:7411 [D loss: 0.273896, acc.: 86.72%] [G loss: 2.379167]\n",
      "epoch:9 step:7412 [D loss: 0.050588, acc.: 99.22%] [G loss: 1.611180]\n",
      "epoch:9 step:7413 [D loss: 0.014843, acc.: 100.00%] [G loss: 0.767704]\n",
      "epoch:9 step:7414 [D loss: 0.117091, acc.: 98.44%] [G loss: 0.071270]\n",
      "epoch:9 step:7415 [D loss: 1.029342, acc.: 58.59%] [G loss: 9.293345]\n",
      "epoch:9 step:7416 [D loss: 2.272949, acc.: 50.00%] [G loss: 7.459506]\n",
      "epoch:9 step:7417 [D loss: 0.411391, acc.: 78.12%] [G loss: 2.426637]\n",
      "epoch:9 step:7418 [D loss: 0.176046, acc.: 92.97%] [G loss: 3.527805]\n",
      "epoch:9 step:7419 [D loss: 0.037786, acc.: 100.00%] [G loss: 2.869412]\n",
      "epoch:9 step:7420 [D loss: 0.075478, acc.: 98.44%] [G loss: 2.159610]\n",
      "epoch:9 step:7421 [D loss: 0.247068, acc.: 90.62%] [G loss: 4.273063]\n",
      "epoch:9 step:7422 [D loss: 0.326585, acc.: 85.94%] [G loss: 4.173155]\n",
      "epoch:9 step:7423 [D loss: 0.128489, acc.: 98.44%] [G loss: 3.544534]\n",
      "epoch:9 step:7424 [D loss: 0.078651, acc.: 99.22%] [G loss: 3.226304]\n",
      "epoch:9 step:7425 [D loss: 0.061605, acc.: 100.00%] [G loss: 2.642735]\n",
      "epoch:9 step:7426 [D loss: 0.174406, acc.: 92.97%] [G loss: 0.781317]\n",
      "epoch:9 step:7427 [D loss: 0.017032, acc.: 100.00%] [G loss: 1.450329]\n",
      "epoch:9 step:7428 [D loss: 0.207336, acc.: 89.84%] [G loss: 4.957134]\n",
      "epoch:9 step:7429 [D loss: 0.095659, acc.: 96.09%] [G loss: 4.245537]\n",
      "epoch:9 step:7430 [D loss: 0.423591, acc.: 84.38%] [G loss: 1.190210]\n",
      "epoch:9 step:7431 [D loss: 0.191009, acc.: 93.75%] [G loss: 1.911684]\n",
      "epoch:9 step:7432 [D loss: 0.031651, acc.: 99.22%] [G loss: 3.139634]\n",
      "epoch:9 step:7433 [D loss: 0.250318, acc.: 91.41%] [G loss: 2.293295]\n",
      "epoch:9 step:7434 [D loss: 0.672214, acc.: 64.84%] [G loss: 3.045138]\n",
      "epoch:9 step:7435 [D loss: 0.074332, acc.: 99.22%] [G loss: 2.646681]\n",
      "epoch:9 step:7436 [D loss: 0.088312, acc.: 98.44%] [G loss: 3.138317]\n",
      "epoch:9 step:7437 [D loss: 0.148598, acc.: 97.66%] [G loss: 2.708461]\n",
      "epoch:9 step:7438 [D loss: 0.345152, acc.: 85.94%] [G loss: 5.026442]\n",
      "epoch:9 step:7439 [D loss: 0.047977, acc.: 98.44%] [G loss: 5.446656]\n",
      "epoch:9 step:7440 [D loss: 0.657245, acc.: 67.97%] [G loss: 4.699887]\n",
      "epoch:9 step:7441 [D loss: 0.008670, acc.: 100.00%] [G loss: 5.520895]\n",
      "epoch:9 step:7442 [D loss: 0.028686, acc.: 100.00%] [G loss: 4.728189]\n",
      "epoch:9 step:7443 [D loss: 0.023308, acc.: 100.00%] [G loss: 3.604560]\n",
      "epoch:9 step:7444 [D loss: 0.024618, acc.: 100.00%] [G loss: 3.109417]\n",
      "epoch:9 step:7445 [D loss: 0.054986, acc.: 99.22%] [G loss: 3.782792]\n",
      "epoch:9 step:7446 [D loss: 0.051924, acc.: 99.22%] [G loss: 3.543941]\n",
      "epoch:9 step:7447 [D loss: 0.054844, acc.: 99.22%] [G loss: 4.004922]\n",
      "epoch:9 step:7448 [D loss: 0.081221, acc.: 99.22%] [G loss: 4.116228]\n",
      "epoch:9 step:7449 [D loss: 0.168150, acc.: 94.53%] [G loss: 2.640044]\n",
      "epoch:9 step:7450 [D loss: 0.082717, acc.: 99.22%] [G loss: 3.712696]\n",
      "epoch:9 step:7451 [D loss: 0.229211, acc.: 94.53%] [G loss: 4.628945]\n",
      "epoch:9 step:7452 [D loss: 0.037477, acc.: 100.00%] [G loss: 4.470089]\n",
      "epoch:9 step:7453 [D loss: 0.592045, acc.: 72.66%] [G loss: 4.393376]\n",
      "epoch:9 step:7454 [D loss: 0.057362, acc.: 99.22%] [G loss: 5.598687]\n",
      "epoch:9 step:7455 [D loss: 0.205933, acc.: 91.41%] [G loss: 4.266819]\n",
      "epoch:9 step:7456 [D loss: 0.019983, acc.: 100.00%] [G loss: 4.464700]\n",
      "epoch:9 step:7457 [D loss: 0.037049, acc.: 99.22%] [G loss: 4.010072]\n",
      "epoch:9 step:7458 [D loss: 0.044765, acc.: 99.22%] [G loss: 4.717751]\n",
      "epoch:9 step:7459 [D loss: 0.187833, acc.: 95.31%] [G loss: 5.364722]\n",
      "epoch:9 step:7460 [D loss: 1.370253, acc.: 31.25%] [G loss: 5.147018]\n",
      "epoch:9 step:7461 [D loss: 0.060021, acc.: 97.66%] [G loss: 6.124639]\n",
      "epoch:9 step:7462 [D loss: 0.260411, acc.: 87.50%] [G loss: 3.277806]\n",
      "epoch:9 step:7463 [D loss: 0.048044, acc.: 99.22%] [G loss: 1.750208]\n",
      "epoch:9 step:7464 [D loss: 0.132450, acc.: 96.88%] [G loss: 3.144351]\n",
      "epoch:9 step:7465 [D loss: 0.049308, acc.: 100.00%] [G loss: 3.586576]\n",
      "epoch:9 step:7466 [D loss: 0.669218, acc.: 64.06%] [G loss: 5.017842]\n",
      "epoch:9 step:7467 [D loss: 0.139524, acc.: 94.53%] [G loss: 5.302653]\n",
      "epoch:9 step:7468 [D loss: 0.422494, acc.: 80.47%] [G loss: 4.471142]\n",
      "epoch:9 step:7469 [D loss: 0.034262, acc.: 100.00%] [G loss: 4.849273]\n",
      "epoch:9 step:7470 [D loss: 0.119551, acc.: 96.09%] [G loss: 4.772742]\n",
      "epoch:9 step:7471 [D loss: 0.054751, acc.: 99.22%] [G loss: 3.515587]\n",
      "epoch:9 step:7472 [D loss: 0.152488, acc.: 95.31%] [G loss: 4.672479]\n",
      "epoch:9 step:7473 [D loss: 0.248298, acc.: 89.06%] [G loss: 2.454911]\n",
      "epoch:9 step:7474 [D loss: 0.133572, acc.: 96.09%] [G loss: 3.509333]\n",
      "epoch:9 step:7475 [D loss: 0.027947, acc.: 100.00%] [G loss: 4.468277]\n",
      "epoch:9 step:7476 [D loss: 0.140129, acc.: 96.88%] [G loss: 1.963631]\n",
      "epoch:9 step:7477 [D loss: 0.059520, acc.: 98.44%] [G loss: 1.744678]\n",
      "epoch:9 step:7478 [D loss: 0.018297, acc.: 100.00%] [G loss: 2.941179]\n",
      "epoch:9 step:7479 [D loss: 0.014284, acc.: 100.00%] [G loss: 1.563311]\n",
      "epoch:9 step:7480 [D loss: 0.116236, acc.: 96.88%] [G loss: 3.839128]\n",
      "epoch:9 step:7481 [D loss: 0.105800, acc.: 96.88%] [G loss: 3.829416]\n",
      "epoch:9 step:7482 [D loss: 0.876299, acc.: 56.25%] [G loss: 5.278272]\n",
      "epoch:9 step:7483 [D loss: 0.115789, acc.: 95.31%] [G loss: 5.781037]\n",
      "epoch:9 step:7484 [D loss: 0.133613, acc.: 93.75%] [G loss: 4.537345]\n",
      "epoch:9 step:7485 [D loss: 0.035129, acc.: 100.00%] [G loss: 3.273019]\n",
      "epoch:9 step:7486 [D loss: 0.013022, acc.: 100.00%] [G loss: 2.627183]\n",
      "epoch:9 step:7487 [D loss: 0.049978, acc.: 99.22%] [G loss: 4.118784]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:9 step:7488 [D loss: 0.082103, acc.: 98.44%] [G loss: 3.158248]\n",
      "epoch:9 step:7489 [D loss: 0.270525, acc.: 86.72%] [G loss: 4.600814]\n",
      "epoch:9 step:7490 [D loss: 0.017353, acc.: 100.00%] [G loss: 6.106552]\n",
      "epoch:9 step:7491 [D loss: 0.237467, acc.: 90.62%] [G loss: 2.271804]\n",
      "epoch:9 step:7492 [D loss: 0.225198, acc.: 90.62%] [G loss: 5.290103]\n",
      "epoch:9 step:7493 [D loss: 0.108542, acc.: 96.88%] [G loss: 5.283390]\n",
      "epoch:9 step:7494 [D loss: 0.066816, acc.: 98.44%] [G loss: 4.939473]\n",
      "epoch:9 step:7495 [D loss: 0.054834, acc.: 100.00%] [G loss: 5.393391]\n",
      "epoch:9 step:7496 [D loss: 0.156073, acc.: 93.75%] [G loss: 3.243366]\n",
      "epoch:9 step:7497 [D loss: 0.031449, acc.: 99.22%] [G loss: 4.073136]\n",
      "epoch:9 step:7498 [D loss: 0.068046, acc.: 98.44%] [G loss: 5.700891]\n",
      "epoch:9 step:7499 [D loss: 0.057870, acc.: 98.44%] [G loss: 3.601524]\n",
      "epoch:9 step:7500 [D loss: 0.031957, acc.: 99.22%] [G loss: 2.971660]\n",
      "epoch:9 step:7501 [D loss: 0.015881, acc.: 100.00%] [G loss: 2.711903]\n",
      "epoch:9 step:7502 [D loss: 0.039480, acc.: 100.00%] [G loss: 2.079740]\n",
      "epoch:9 step:7503 [D loss: 0.209505, acc.: 92.19%] [G loss: 0.262253]\n",
      "epoch:9 step:7504 [D loss: 0.365989, acc.: 82.03%] [G loss: 6.337832]\n",
      "epoch:9 step:7505 [D loss: 0.426093, acc.: 77.34%] [G loss: 2.136997]\n",
      "epoch:9 step:7506 [D loss: 0.091174, acc.: 96.88%] [G loss: 1.514939]\n",
      "epoch:9 step:7507 [D loss: 0.030646, acc.: 100.00%] [G loss: 3.777860]\n",
      "epoch:9 step:7508 [D loss: 0.117636, acc.: 97.66%] [G loss: 2.590909]\n",
      "epoch:9 step:7509 [D loss: 0.055643, acc.: 99.22%] [G loss: 4.281775]\n",
      "epoch:9 step:7510 [D loss: 0.047731, acc.: 98.44%] [G loss: 2.381316]\n",
      "epoch:9 step:7511 [D loss: 0.110203, acc.: 97.66%] [G loss: 2.757943]\n",
      "epoch:9 step:7512 [D loss: 0.042693, acc.: 100.00%] [G loss: 3.246116]\n",
      "epoch:9 step:7513 [D loss: 0.013009, acc.: 100.00%] [G loss: 2.466267]\n",
      "epoch:9 step:7514 [D loss: 0.132494, acc.: 96.88%] [G loss: 0.912400]\n",
      "epoch:9 step:7515 [D loss: 0.109896, acc.: 97.66%] [G loss: 3.856598]\n",
      "epoch:9 step:7516 [D loss: 0.045617, acc.: 99.22%] [G loss: 3.024120]\n",
      "epoch:9 step:7517 [D loss: 0.208067, acc.: 92.19%] [G loss: 0.929608]\n",
      "epoch:9 step:7518 [D loss: 0.097268, acc.: 97.66%] [G loss: 2.714751]\n",
      "epoch:9 step:7519 [D loss: 0.051340, acc.: 98.44%] [G loss: 2.017021]\n",
      "epoch:9 step:7520 [D loss: 0.594957, acc.: 66.41%] [G loss: 4.936763]\n",
      "epoch:9 step:7521 [D loss: 0.111983, acc.: 95.31%] [G loss: 5.747675]\n",
      "epoch:9 step:7522 [D loss: 0.115829, acc.: 96.09%] [G loss: 1.295618]\n",
      "epoch:9 step:7523 [D loss: 0.247789, acc.: 86.72%] [G loss: 5.321712]\n",
      "epoch:9 step:7524 [D loss: 0.179928, acc.: 93.75%] [G loss: 4.036771]\n",
      "epoch:9 step:7525 [D loss: 0.081017, acc.: 96.09%] [G loss: 1.831590]\n",
      "epoch:9 step:7526 [D loss: 0.006971, acc.: 100.00%] [G loss: 0.312734]\n",
      "epoch:9 step:7527 [D loss: 0.068585, acc.: 99.22%] [G loss: 0.767194]\n",
      "epoch:9 step:7528 [D loss: 0.059069, acc.: 98.44%] [G loss: 0.262867]\n",
      "epoch:9 step:7529 [D loss: 0.026519, acc.: 100.00%] [G loss: 0.765845]\n",
      "epoch:9 step:7530 [D loss: 0.025053, acc.: 100.00%] [G loss: 0.970843]\n",
      "epoch:9 step:7531 [D loss: 0.294417, acc.: 84.38%] [G loss: 9.029595]\n",
      "epoch:9 step:7532 [D loss: 1.050171, acc.: 60.94%] [G loss: 1.557091]\n",
      "epoch:9 step:7533 [D loss: 0.723299, acc.: 71.09%] [G loss: 7.342524]\n",
      "epoch:9 step:7534 [D loss: 1.989150, acc.: 42.19%] [G loss: 2.465940]\n",
      "epoch:9 step:7535 [D loss: 0.219478, acc.: 91.41%] [G loss: 6.017148]\n",
      "epoch:9 step:7536 [D loss: 0.129003, acc.: 95.31%] [G loss: 5.222921]\n",
      "epoch:9 step:7537 [D loss: 0.044325, acc.: 100.00%] [G loss: 3.668922]\n",
      "epoch:9 step:7538 [D loss: 0.075834, acc.: 97.66%] [G loss: 2.944553]\n",
      "epoch:9 step:7539 [D loss: 0.102068, acc.: 96.09%] [G loss: 3.645287]\n",
      "epoch:9 step:7540 [D loss: 0.174848, acc.: 95.31%] [G loss: 2.613892]\n",
      "epoch:9 step:7541 [D loss: 0.160283, acc.: 96.88%] [G loss: 4.715226]\n",
      "epoch:9 step:7542 [D loss: 0.658508, acc.: 62.50%] [G loss: 3.541350]\n",
      "epoch:9 step:7543 [D loss: 0.022611, acc.: 100.00%] [G loss: 4.550976]\n",
      "epoch:9 step:7544 [D loss: 0.123332, acc.: 95.31%] [G loss: 2.803714]\n",
      "epoch:9 step:7545 [D loss: 0.092250, acc.: 96.88%] [G loss: 3.185011]\n",
      "epoch:9 step:7546 [D loss: 0.016808, acc.: 100.00%] [G loss: 2.616474]\n",
      "epoch:9 step:7547 [D loss: 0.122663, acc.: 96.88%] [G loss: 3.882712]\n",
      "epoch:9 step:7548 [D loss: 0.074766, acc.: 99.22%] [G loss: 2.206337]\n",
      "epoch:9 step:7549 [D loss: 0.075634, acc.: 100.00%] [G loss: 1.554396]\n",
      "epoch:9 step:7550 [D loss: 0.131614, acc.: 96.09%] [G loss: 4.241906]\n",
      "epoch:9 step:7551 [D loss: 0.111762, acc.: 96.09%] [G loss: 3.313583]\n",
      "epoch:9 step:7552 [D loss: 0.035135, acc.: 99.22%] [G loss: 1.712957]\n",
      "epoch:9 step:7553 [D loss: 0.042148, acc.: 100.00%] [G loss: 0.287516]\n",
      "epoch:9 step:7554 [D loss: 0.051975, acc.: 97.66%] [G loss: 0.839343]\n",
      "epoch:9 step:7555 [D loss: 0.036822, acc.: 100.00%] [G loss: 0.709741]\n",
      "epoch:9 step:7556 [D loss: 0.045122, acc.: 99.22%] [G loss: 0.726546]\n",
      "epoch:9 step:7557 [D loss: 0.008178, acc.: 100.00%] [G loss: 0.598338]\n",
      "epoch:9 step:7558 [D loss: 0.096910, acc.: 98.44%] [G loss: 2.441712]\n",
      "epoch:9 step:7559 [D loss: 0.118247, acc.: 94.53%] [G loss: 0.367930]\n",
      "epoch:9 step:7560 [D loss: 0.058436, acc.: 98.44%] [G loss: 0.644274]\n",
      "epoch:9 step:7561 [D loss: 0.014156, acc.: 100.00%] [G loss: 0.596534]\n",
      "epoch:9 step:7562 [D loss: 0.061708, acc.: 98.44%] [G loss: 0.160936]\n",
      "epoch:9 step:7563 [D loss: 0.037313, acc.: 100.00%] [G loss: 0.904236]\n",
      "epoch:9 step:7564 [D loss: 0.013396, acc.: 100.00%] [G loss: 0.813915]\n",
      "epoch:9 step:7565 [D loss: 0.359535, acc.: 83.59%] [G loss: 3.429436]\n",
      "epoch:9 step:7566 [D loss: 0.259579, acc.: 85.16%] [G loss: 1.270752]\n",
      "epoch:9 step:7567 [D loss: 0.070429, acc.: 98.44%] [G loss: 0.343239]\n",
      "epoch:9 step:7568 [D loss: 0.018015, acc.: 99.22%] [G loss: 0.304771]\n",
      "epoch:9 step:7569 [D loss: 0.017625, acc.: 100.00%] [G loss: 0.360362]\n",
      "epoch:9 step:7570 [D loss: 0.002415, acc.: 100.00%] [G loss: 0.274157]\n",
      "epoch:9 step:7571 [D loss: 0.699094, acc.: 72.66%] [G loss: 9.447121]\n",
      "epoch:9 step:7572 [D loss: 2.220865, acc.: 50.78%] [G loss: 3.863719]\n",
      "epoch:9 step:7573 [D loss: 0.450613, acc.: 82.81%] [G loss: 3.091534]\n",
      "epoch:9 step:7574 [D loss: 0.068657, acc.: 98.44%] [G loss: 4.382280]\n",
      "epoch:9 step:7575 [D loss: 0.154786, acc.: 95.31%] [G loss: 3.442088]\n",
      "epoch:9 step:7576 [D loss: 0.082856, acc.: 98.44%] [G loss: 3.588259]\n",
      "epoch:9 step:7577 [D loss: 0.076143, acc.: 98.44%] [G loss: 3.948477]\n",
      "epoch:9 step:7578 [D loss: 0.016348, acc.: 100.00%] [G loss: 3.559729]\n",
      "epoch:9 step:7579 [D loss: 0.120884, acc.: 99.22%] [G loss: 3.386199]\n",
      "epoch:9 step:7580 [D loss: 0.033111, acc.: 100.00%] [G loss: 2.913463]\n",
      "epoch:9 step:7581 [D loss: 0.087096, acc.: 98.44%] [G loss: 2.681517]\n",
      "epoch:9 step:7582 [D loss: 0.043516, acc.: 100.00%] [G loss: 2.321025]\n",
      "epoch:9 step:7583 [D loss: 0.088734, acc.: 99.22%] [G loss: 1.184461]\n",
      "epoch:9 step:7584 [D loss: 0.055070, acc.: 100.00%] [G loss: 0.602789]\n",
      "epoch:9 step:7585 [D loss: 0.038637, acc.: 100.00%] [G loss: 0.531946]\n",
      "epoch:9 step:7586 [D loss: 0.088699, acc.: 99.22%] [G loss: 0.926464]\n",
      "epoch:9 step:7587 [D loss: 0.069898, acc.: 98.44%] [G loss: 0.372933]\n",
      "epoch:9 step:7588 [D loss: 0.038467, acc.: 99.22%] [G loss: 0.265804]\n",
      "epoch:9 step:7589 [D loss: 0.053708, acc.: 100.00%] [G loss: 0.221975]\n",
      "epoch:9 step:7590 [D loss: 0.018032, acc.: 100.00%] [G loss: 0.660471]\n",
      "epoch:9 step:7591 [D loss: 0.071688, acc.: 99.22%] [G loss: 0.216633]\n",
      "epoch:9 step:7592 [D loss: 0.029857, acc.: 100.00%] [G loss: 0.169043]\n",
      "epoch:9 step:7593 [D loss: 0.058976, acc.: 99.22%] [G loss: 0.270815]\n",
      "epoch:9 step:7594 [D loss: 0.473248, acc.: 77.34%] [G loss: 6.742817]\n",
      "epoch:9 step:7595 [D loss: 1.680815, acc.: 50.00%] [G loss: 3.160846]\n",
      "epoch:9 step:7596 [D loss: 0.095023, acc.: 98.44%] [G loss: 2.054055]\n",
      "epoch:9 step:7597 [D loss: 0.179755, acc.: 92.19%] [G loss: 4.015841]\n",
      "epoch:9 step:7598 [D loss: 0.153855, acc.: 96.09%] [G loss: 4.130954]\n",
      "epoch:9 step:7599 [D loss: 0.164771, acc.: 92.97%] [G loss: 2.994790]\n",
      "epoch:9 step:7600 [D loss: 0.263415, acc.: 89.06%] [G loss: 5.047660]\n",
      "##############\n",
      "[1.03294763 0.91829928 1.10339618 0.99190673 2.11680701 2.12453718\n",
      " 2.1077383  1.11143008 2.11302204 2.09239453]\n",
      "##########\n",
      "epoch:9 step:7601 [D loss: 0.103330, acc.: 97.66%] [G loss: 5.157875]\n",
      "epoch:9 step:7602 [D loss: 0.217565, acc.: 90.62%] [G loss: 3.786316]\n",
      "epoch:9 step:7603 [D loss: 0.183437, acc.: 91.41%] [G loss: 5.695765]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:9 step:7604 [D loss: 0.169669, acc.: 94.53%] [G loss: 3.810169]\n",
      "epoch:9 step:7605 [D loss: 0.030504, acc.: 100.00%] [G loss: 3.757972]\n",
      "epoch:9 step:7606 [D loss: 0.040935, acc.: 100.00%] [G loss: 2.826523]\n",
      "epoch:9 step:7607 [D loss: 0.026896, acc.: 100.00%] [G loss: 2.437521]\n",
      "epoch:9 step:7608 [D loss: 0.210571, acc.: 90.62%] [G loss: 5.504385]\n",
      "epoch:9 step:7609 [D loss: 0.583928, acc.: 71.88%] [G loss: 1.174387]\n",
      "epoch:9 step:7610 [D loss: 0.372383, acc.: 82.81%] [G loss: 5.166791]\n",
      "epoch:9 step:7611 [D loss: 0.030485, acc.: 100.00%] [G loss: 6.144447]\n",
      "epoch:9 step:7612 [D loss: 0.254569, acc.: 89.84%] [G loss: 3.184713]\n",
      "epoch:9 step:7613 [D loss: 0.070325, acc.: 97.66%] [G loss: 3.375383]\n",
      "epoch:9 step:7614 [D loss: 0.015458, acc.: 100.00%] [G loss: 2.678099]\n",
      "epoch:9 step:7615 [D loss: 0.066559, acc.: 99.22%] [G loss: 3.669563]\n",
      "epoch:9 step:7616 [D loss: 0.310539, acc.: 86.72%] [G loss: 4.535350]\n",
      "epoch:9 step:7617 [D loss: 0.361319, acc.: 83.59%] [G loss: 4.290169]\n",
      "epoch:9 step:7618 [D loss: 0.040315, acc.: 100.00%] [G loss: 4.743637]\n",
      "epoch:9 step:7619 [D loss: 0.068326, acc.: 99.22%] [G loss: 3.059793]\n",
      "epoch:9 step:7620 [D loss: 0.132010, acc.: 96.09%] [G loss: 4.059545]\n",
      "epoch:9 step:7621 [D loss: 0.052996, acc.: 99.22%] [G loss: 4.543472]\n",
      "epoch:9 step:7622 [D loss: 0.184308, acc.: 94.53%] [G loss: 3.216361]\n",
      "epoch:9 step:7623 [D loss: 0.090260, acc.: 97.66%] [G loss: 3.904332]\n",
      "epoch:9 step:7624 [D loss: 0.086063, acc.: 97.66%] [G loss: 3.348482]\n",
      "epoch:9 step:7625 [D loss: 0.066726, acc.: 100.00%] [G loss: 1.854780]\n",
      "epoch:9 step:7626 [D loss: 0.090279, acc.: 97.66%] [G loss: 2.698319]\n",
      "epoch:9 step:7627 [D loss: 0.064575, acc.: 97.66%] [G loss: 1.778738]\n",
      "epoch:9 step:7628 [D loss: 0.054059, acc.: 99.22%] [G loss: 1.728798]\n",
      "epoch:9 step:7629 [D loss: 0.073267, acc.: 98.44%] [G loss: 2.924335]\n",
      "epoch:9 step:7630 [D loss: 0.197222, acc.: 92.97%] [G loss: 2.045061]\n",
      "epoch:9 step:7631 [D loss: 0.155304, acc.: 93.75%] [G loss: 4.312969]\n",
      "epoch:9 step:7632 [D loss: 0.184965, acc.: 90.62%] [G loss: 0.948524]\n",
      "epoch:9 step:7633 [D loss: 4.230470, acc.: 31.25%] [G loss: 7.305711]\n",
      "epoch:9 step:7634 [D loss: 2.093008, acc.: 50.00%] [G loss: 4.511663]\n",
      "epoch:9 step:7635 [D loss: 0.566083, acc.: 75.00%] [G loss: 2.147887]\n",
      "epoch:9 step:7636 [D loss: 0.330492, acc.: 84.38%] [G loss: 3.561675]\n",
      "epoch:9 step:7637 [D loss: 0.099268, acc.: 97.66%] [G loss: 3.838193]\n",
      "epoch:9 step:7638 [D loss: 0.242929, acc.: 90.62%] [G loss: 2.364497]\n",
      "epoch:9 step:7639 [D loss: 0.585482, acc.: 68.75%] [G loss: 4.300701]\n",
      "epoch:9 step:7640 [D loss: 0.383790, acc.: 76.56%] [G loss: 4.193611]\n",
      "epoch:9 step:7641 [D loss: 0.190852, acc.: 96.09%] [G loss: 2.658943]\n",
      "epoch:9 step:7642 [D loss: 0.137202, acc.: 97.66%] [G loss: 2.638652]\n",
      "epoch:9 step:7643 [D loss: 0.203476, acc.: 94.53%] [G loss: 3.757376]\n",
      "epoch:9 step:7644 [D loss: 0.063127, acc.: 99.22%] [G loss: 3.103500]\n",
      "epoch:9 step:7645 [D loss: 0.525771, acc.: 71.09%] [G loss: 1.302186]\n",
      "epoch:9 step:7646 [D loss: 0.098375, acc.: 98.44%] [G loss: 2.534803]\n",
      "epoch:9 step:7647 [D loss: 0.313373, acc.: 88.28%] [G loss: 1.875265]\n",
      "epoch:9 step:7648 [D loss: 0.241654, acc.: 90.62%] [G loss: 3.355032]\n",
      "epoch:9 step:7649 [D loss: 0.300761, acc.: 86.72%] [G loss: 2.681737]\n",
      "epoch:9 step:7650 [D loss: 0.103629, acc.: 100.00%] [G loss: 2.762991]\n",
      "epoch:9 step:7651 [D loss: 0.156243, acc.: 95.31%] [G loss: 2.800764]\n",
      "epoch:9 step:7652 [D loss: 0.821488, acc.: 48.44%] [G loss: 2.963184]\n",
      "epoch:9 step:7653 [D loss: 0.056580, acc.: 98.44%] [G loss: 3.950656]\n",
      "epoch:9 step:7654 [D loss: 0.504822, acc.: 78.91%] [G loss: 1.328112]\n",
      "epoch:9 step:7655 [D loss: 0.056999, acc.: 100.00%] [G loss: 2.304910]\n",
      "epoch:9 step:7656 [D loss: 0.245164, acc.: 91.41%] [G loss: 3.485514]\n",
      "epoch:9 step:7657 [D loss: 0.060195, acc.: 99.22%] [G loss: 3.419987]\n",
      "epoch:9 step:7658 [D loss: 0.212792, acc.: 91.41%] [G loss: 1.967909]\n",
      "epoch:9 step:7659 [D loss: 0.197979, acc.: 96.09%] [G loss: 2.943025]\n",
      "epoch:9 step:7660 [D loss: 0.069978, acc.: 100.00%] [G loss: 1.765224]\n",
      "epoch:9 step:7661 [D loss: 0.186692, acc.: 94.53%] [G loss: 2.804474]\n",
      "epoch:9 step:7662 [D loss: 0.164138, acc.: 96.88%] [G loss: 2.097640]\n",
      "epoch:9 step:7663 [D loss: 0.043394, acc.: 100.00%] [G loss: 1.906532]\n",
      "epoch:9 step:7664 [D loss: 0.315139, acc.: 88.28%] [G loss: 3.320822]\n",
      "epoch:9 step:7665 [D loss: 0.183131, acc.: 95.31%] [G loss: 1.735796]\n",
      "epoch:9 step:7666 [D loss: 0.232475, acc.: 93.75%] [G loss: 2.172266]\n",
      "epoch:9 step:7667 [D loss: 0.035829, acc.: 100.00%] [G loss: 2.298780]\n",
      "epoch:9 step:7668 [D loss: 0.435848, acc.: 78.12%] [G loss: 5.896485]\n",
      "epoch:9 step:7669 [D loss: 0.748851, acc.: 63.28%] [G loss: 2.730732]\n",
      "epoch:9 step:7670 [D loss: 0.040274, acc.: 100.00%] [G loss: 1.642238]\n",
      "epoch:9 step:7671 [D loss: 0.106339, acc.: 98.44%] [G loss: 2.734741]\n",
      "epoch:9 step:7672 [D loss: 0.031976, acc.: 100.00%] [G loss: 3.135666]\n",
      "epoch:9 step:7673 [D loss: 0.106911, acc.: 97.66%] [G loss: 3.635779]\n",
      "epoch:9 step:7674 [D loss: 0.291975, acc.: 87.50%] [G loss: 1.769348]\n",
      "epoch:9 step:7675 [D loss: 0.326902, acc.: 83.59%] [G loss: 5.598904]\n",
      "epoch:9 step:7676 [D loss: 0.308996, acc.: 83.59%] [G loss: 3.761073]\n",
      "epoch:9 step:7677 [D loss: 0.085938, acc.: 99.22%] [G loss: 3.130119]\n",
      "epoch:9 step:7678 [D loss: 0.127398, acc.: 96.09%] [G loss: 3.153196]\n",
      "epoch:9 step:7679 [D loss: 0.094717, acc.: 96.88%] [G loss: 3.519568]\n",
      "epoch:9 step:7680 [D loss: 0.046791, acc.: 100.00%] [G loss: 3.850328]\n",
      "epoch:9 step:7681 [D loss: 0.217447, acc.: 92.19%] [G loss: 2.472756]\n",
      "epoch:9 step:7682 [D loss: 0.092172, acc.: 99.22%] [G loss: 3.621555]\n",
      "epoch:9 step:7683 [D loss: 0.035475, acc.: 100.00%] [G loss: 3.225409]\n",
      "epoch:9 step:7684 [D loss: 0.268752, acc.: 89.84%] [G loss: 2.350319]\n",
      "epoch:9 step:7685 [D loss: 0.110189, acc.: 96.88%] [G loss: 2.957856]\n",
      "epoch:9 step:7686 [D loss: 0.095088, acc.: 98.44%] [G loss: 2.486867]\n",
      "epoch:9 step:7687 [D loss: 0.142889, acc.: 95.31%] [G loss: 3.897468]\n",
      "epoch:9 step:7688 [D loss: 0.107450, acc.: 96.09%] [G loss: 2.799501]\n",
      "epoch:9 step:7689 [D loss: 0.161469, acc.: 99.22%] [G loss: 3.852916]\n",
      "epoch:9 step:7690 [D loss: 0.113891, acc.: 97.66%] [G loss: 2.624282]\n",
      "epoch:9 step:7691 [D loss: 0.311564, acc.: 88.28%] [G loss: 4.941302]\n",
      "epoch:9 step:7692 [D loss: 0.153725, acc.: 92.97%] [G loss: 5.115778]\n",
      "epoch:9 step:7693 [D loss: 0.067551, acc.: 98.44%] [G loss: 4.726439]\n",
      "epoch:9 step:7694 [D loss: 0.136432, acc.: 94.53%] [G loss: 3.456674]\n",
      "epoch:9 step:7695 [D loss: 0.114506, acc.: 96.88%] [G loss: 5.890804]\n",
      "epoch:9 step:7696 [D loss: 0.035018, acc.: 100.00%] [G loss: 4.989478]\n",
      "epoch:9 step:7697 [D loss: 0.235855, acc.: 89.06%] [G loss: 2.584746]\n",
      "epoch:9 step:7698 [D loss: 0.140832, acc.: 96.09%] [G loss: 3.742162]\n",
      "epoch:9 step:7699 [D loss: 0.026418, acc.: 100.00%] [G loss: 4.188660]\n",
      "epoch:9 step:7700 [D loss: 0.033661, acc.: 100.00%] [G loss: 3.916660]\n",
      "epoch:9 step:7701 [D loss: 0.044006, acc.: 100.00%] [G loss: 3.456240]\n",
      "epoch:9 step:7702 [D loss: 0.167495, acc.: 95.31%] [G loss: 2.391354]\n",
      "epoch:9 step:7703 [D loss: 0.117424, acc.: 97.66%] [G loss: 4.543716]\n",
      "epoch:9 step:7704 [D loss: 0.021641, acc.: 100.00%] [G loss: 4.790524]\n",
      "epoch:9 step:7705 [D loss: 0.093645, acc.: 97.66%] [G loss: 3.080840]\n",
      "epoch:9 step:7706 [D loss: 0.167970, acc.: 92.19%] [G loss: 4.196063]\n",
      "epoch:9 step:7707 [D loss: 0.093408, acc.: 97.66%] [G loss: 4.485916]\n",
      "epoch:9 step:7708 [D loss: 0.049380, acc.: 100.00%] [G loss: 4.376602]\n",
      "epoch:9 step:7709 [D loss: 0.021406, acc.: 100.00%] [G loss: 4.708000]\n",
      "epoch:9 step:7710 [D loss: 0.154130, acc.: 94.53%] [G loss: 3.190496]\n",
      "epoch:9 step:7711 [D loss: 0.055461, acc.: 100.00%] [G loss: 3.189119]\n",
      "epoch:9 step:7712 [D loss: 0.159684, acc.: 95.31%] [G loss: 4.469772]\n",
      "epoch:9 step:7713 [D loss: 0.024723, acc.: 100.00%] [G loss: 4.816893]\n",
      "epoch:9 step:7714 [D loss: 1.541348, acc.: 28.91%] [G loss: 6.356884]\n",
      "epoch:9 step:7715 [D loss: 1.151165, acc.: 52.34%] [G loss: 3.032867]\n",
      "epoch:9 step:7716 [D loss: 0.071144, acc.: 100.00%] [G loss: 1.840134]\n",
      "epoch:9 step:7717 [D loss: 0.203365, acc.: 93.75%] [G loss: 2.634266]\n",
      "epoch:9 step:7718 [D loss: 0.061528, acc.: 99.22%] [G loss: 3.850341]\n",
      "epoch:9 step:7719 [D loss: 0.182734, acc.: 95.31%] [G loss: 3.397390]\n",
      "epoch:9 step:7720 [D loss: 0.115189, acc.: 95.31%] [G loss: 4.413329]\n",
      "epoch:9 step:7721 [D loss: 0.277036, acc.: 87.50%] [G loss: 2.857450]\n",
      "epoch:9 step:7722 [D loss: 0.093292, acc.: 97.66%] [G loss: 4.129993]\n",
      "epoch:9 step:7723 [D loss: 0.078354, acc.: 99.22%] [G loss: 4.729697]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:9 step:7724 [D loss: 0.047494, acc.: 100.00%] [G loss: 4.995495]\n",
      "epoch:9 step:7725 [D loss: 0.100824, acc.: 97.66%] [G loss: 3.462841]\n",
      "epoch:9 step:7726 [D loss: 0.214226, acc.: 90.62%] [G loss: 5.386317]\n",
      "epoch:9 step:7727 [D loss: 0.121725, acc.: 96.09%] [G loss: 4.979895]\n",
      "epoch:9 step:7728 [D loss: 0.805941, acc.: 55.47%] [G loss: 6.076072]\n",
      "epoch:9 step:7729 [D loss: 0.201624, acc.: 86.72%] [G loss: 5.896634]\n",
      "epoch:9 step:7730 [D loss: 0.039203, acc.: 97.66%] [G loss: 4.712722]\n",
      "epoch:9 step:7731 [D loss: 0.014196, acc.: 100.00%] [G loss: 4.398636]\n",
      "epoch:9 step:7732 [D loss: 0.025626, acc.: 100.00%] [G loss: 4.427981]\n",
      "epoch:9 step:7733 [D loss: 0.037311, acc.: 100.00%] [G loss: 4.100169]\n",
      "epoch:9 step:7734 [D loss: 0.217142, acc.: 91.41%] [G loss: 5.810165]\n",
      "epoch:9 step:7735 [D loss: 0.072292, acc.: 98.44%] [G loss: 5.082445]\n",
      "epoch:9 step:7736 [D loss: 1.134326, acc.: 50.00%] [G loss: 2.442714]\n",
      "epoch:9 step:7737 [D loss: 0.108524, acc.: 96.09%] [G loss: 3.107698]\n",
      "epoch:9 step:7738 [D loss: 0.010565, acc.: 100.00%] [G loss: 4.176599]\n",
      "epoch:9 step:7739 [D loss: 0.071865, acc.: 98.44%] [G loss: 2.104060]\n",
      "epoch:9 step:7740 [D loss: 0.165213, acc.: 96.09%] [G loss: 3.461298]\n",
      "epoch:9 step:7741 [D loss: 0.103941, acc.: 98.44%] [G loss: 3.325386]\n",
      "epoch:9 step:7742 [D loss: 0.325726, acc.: 84.38%] [G loss: 0.442993]\n",
      "epoch:9 step:7743 [D loss: 0.911942, acc.: 59.38%] [G loss: 6.737832]\n",
      "epoch:9 step:7744 [D loss: 1.605791, acc.: 50.78%] [G loss: 3.142175]\n",
      "epoch:9 step:7745 [D loss: 1.083743, acc.: 57.81%] [G loss: 4.016732]\n",
      "epoch:9 step:7746 [D loss: 0.244985, acc.: 89.06%] [G loss: 4.178186]\n",
      "epoch:9 step:7747 [D loss: 0.403526, acc.: 83.59%] [G loss: 3.163941]\n",
      "epoch:9 step:7748 [D loss: 0.155075, acc.: 96.09%] [G loss: 2.523500]\n",
      "epoch:9 step:7749 [D loss: 0.499951, acc.: 73.44%] [G loss: 2.136466]\n",
      "epoch:9 step:7750 [D loss: 0.509988, acc.: 76.56%] [G loss: 4.538011]\n",
      "epoch:9 step:7751 [D loss: 0.357829, acc.: 82.81%] [G loss: 2.794261]\n",
      "epoch:9 step:7752 [D loss: 0.174960, acc.: 93.75%] [G loss: 1.445032]\n",
      "epoch:9 step:7753 [D loss: 0.115423, acc.: 95.31%] [G loss: 1.870826]\n",
      "epoch:9 step:7754 [D loss: 0.154205, acc.: 96.88%] [G loss: 1.678980]\n",
      "epoch:9 step:7755 [D loss: 0.142761, acc.: 97.66%] [G loss: 2.804690]\n",
      "epoch:9 step:7756 [D loss: 0.231695, acc.: 91.41%] [G loss: 2.739497]\n",
      "epoch:9 step:7757 [D loss: 0.054005, acc.: 100.00%] [G loss: 2.472966]\n",
      "epoch:9 step:7758 [D loss: 0.228688, acc.: 89.06%] [G loss: 0.466749]\n",
      "epoch:9 step:7759 [D loss: 0.297929, acc.: 84.38%] [G loss: 2.546856]\n",
      "epoch:9 step:7760 [D loss: 0.054496, acc.: 99.22%] [G loss: 3.232632]\n",
      "epoch:9 step:7761 [D loss: 0.513424, acc.: 71.09%] [G loss: 0.199743]\n",
      "epoch:9 step:7762 [D loss: 0.655646, acc.: 65.62%] [G loss: 3.098484]\n",
      "epoch:9 step:7763 [D loss: 0.229156, acc.: 84.38%] [G loss: 3.999412]\n",
      "epoch:9 step:7764 [D loss: 0.110651, acc.: 93.75%] [G loss: 3.077851]\n",
      "epoch:9 step:7765 [D loss: 0.030602, acc.: 100.00%] [G loss: 2.075568]\n",
      "epoch:9 step:7766 [D loss: 0.067562, acc.: 99.22%] [G loss: 1.857848]\n",
      "epoch:9 step:7767 [D loss: 0.112219, acc.: 98.44%] [G loss: 1.926702]\n",
      "epoch:9 step:7768 [D loss: 0.561021, acc.: 72.66%] [G loss: 1.899262]\n",
      "epoch:9 step:7769 [D loss: 0.046155, acc.: 98.44%] [G loss: 1.800243]\n",
      "epoch:9 step:7770 [D loss: 0.110054, acc.: 97.66%] [G loss: 2.019907]\n",
      "epoch:9 step:7771 [D loss: 0.332920, acc.: 88.28%] [G loss: 1.817560]\n",
      "epoch:9 step:7772 [D loss: 0.047896, acc.: 100.00%] [G loss: 1.757800]\n",
      "epoch:9 step:7773 [D loss: 0.094109, acc.: 99.22%] [G loss: 2.568595]\n",
      "epoch:9 step:7774 [D loss: 0.145495, acc.: 96.09%] [G loss: 2.849407]\n",
      "epoch:9 step:7775 [D loss: 0.162190, acc.: 93.75%] [G loss: 2.531030]\n",
      "epoch:9 step:7776 [D loss: 0.231459, acc.: 92.97%] [G loss: 3.144648]\n",
      "epoch:9 step:7777 [D loss: 0.049357, acc.: 100.00%] [G loss: 3.554370]\n",
      "epoch:9 step:7778 [D loss: 0.045562, acc.: 99.22%] [G loss: 3.642472]\n",
      "epoch:9 step:7779 [D loss: 0.675233, acc.: 67.19%] [G loss: 5.239265]\n",
      "epoch:9 step:7780 [D loss: 0.141706, acc.: 94.53%] [G loss: 4.764758]\n",
      "epoch:9 step:7781 [D loss: 0.190456, acc.: 90.62%] [G loss: 2.824034]\n",
      "epoch:9 step:7782 [D loss: 0.269118, acc.: 90.62%] [G loss: 3.003320]\n",
      "epoch:9 step:7783 [D loss: 0.028681, acc.: 100.00%] [G loss: 2.942094]\n",
      "epoch:9 step:7784 [D loss: 0.117844, acc.: 96.88%] [G loss: 3.524780]\n",
      "epoch:9 step:7785 [D loss: 0.174488, acc.: 91.41%] [G loss: 3.865314]\n",
      "epoch:9 step:7786 [D loss: 0.110896, acc.: 98.44%] [G loss: 3.835256]\n",
      "epoch:9 step:7787 [D loss: 0.067655, acc.: 99.22%] [G loss: 4.419478]\n",
      "epoch:9 step:7788 [D loss: 0.071818, acc.: 99.22%] [G loss: 4.329999]\n",
      "epoch:9 step:7789 [D loss: 0.355719, acc.: 82.03%] [G loss: 6.393993]\n",
      "epoch:9 step:7790 [D loss: 0.056612, acc.: 99.22%] [G loss: 6.612789]\n",
      "epoch:9 step:7791 [D loss: 1.169219, acc.: 47.66%] [G loss: 2.953219]\n",
      "epoch:9 step:7792 [D loss: 0.011035, acc.: 100.00%] [G loss: 2.799569]\n",
      "epoch:9 step:7793 [D loss: 0.072225, acc.: 98.44%] [G loss: 3.078377]\n",
      "epoch:9 step:7794 [D loss: 0.205292, acc.: 90.62%] [G loss: 4.893611]\n",
      "epoch:9 step:7795 [D loss: 0.182502, acc.: 90.62%] [G loss: 3.044966]\n",
      "epoch:9 step:7796 [D loss: 0.054238, acc.: 100.00%] [G loss: 2.732379]\n",
      "epoch:9 step:7797 [D loss: 0.050119, acc.: 98.44%] [G loss: 2.335397]\n",
      "epoch:9 step:7798 [D loss: 0.022695, acc.: 100.00%] [G loss: 1.955979]\n",
      "epoch:9 step:7799 [D loss: 0.035581, acc.: 100.00%] [G loss: 1.620115]\n",
      "epoch:9 step:7800 [D loss: 0.046792, acc.: 100.00%] [G loss: 1.359251]\n",
      "##############\n",
      "[1.04804307 1.00596947 1.04062854 0.96896837 1.10348112 1.10678557\n",
      " 2.10859913 2.11518748 1.10706658 1.13133534]\n",
      "##########\n",
      "epoch:9 step:7801 [D loss: 0.356909, acc.: 82.81%] [G loss: 5.198078]\n",
      "epoch:9 step:7802 [D loss: 0.614399, acc.: 65.62%] [G loss: 2.442581]\n",
      "epoch:9 step:7803 [D loss: 0.020732, acc.: 100.00%] [G loss: 0.475982]\n",
      "epoch:9 step:7804 [D loss: 0.187980, acc.: 89.06%] [G loss: 3.044244]\n",
      "epoch:9 step:7805 [D loss: 0.057635, acc.: 100.00%] [G loss: 3.145639]\n",
      "epoch:9 step:7806 [D loss: 0.167365, acc.: 95.31%] [G loss: 0.742235]\n",
      "epoch:9 step:7807 [D loss: 0.497192, acc.: 77.34%] [G loss: 5.783792]\n",
      "epoch:9 step:7808 [D loss: 1.851469, acc.: 49.22%] [G loss: 3.225629]\n",
      "epoch:9 step:7809 [D loss: 0.274596, acc.: 89.06%] [G loss: 3.539558]\n",
      "epoch:9 step:7810 [D loss: 0.078962, acc.: 99.22%] [G loss: 3.543082]\n",
      "epoch:10 step:7811 [D loss: 0.139913, acc.: 94.53%] [G loss: 3.532629]\n",
      "epoch:10 step:7812 [D loss: 0.079667, acc.: 99.22%] [G loss: 2.903705]\n",
      "epoch:10 step:7813 [D loss: 0.080388, acc.: 98.44%] [G loss: 3.541723]\n",
      "epoch:10 step:7814 [D loss: 0.057554, acc.: 100.00%] [G loss: 3.035928]\n",
      "epoch:10 step:7815 [D loss: 0.026593, acc.: 100.00%] [G loss: 2.873171]\n",
      "epoch:10 step:7816 [D loss: 0.045268, acc.: 100.00%] [G loss: 1.876750]\n",
      "epoch:10 step:7817 [D loss: 0.116798, acc.: 99.22%] [G loss: 2.735079]\n",
      "epoch:10 step:7818 [D loss: 0.080907, acc.: 99.22%] [G loss: 3.804732]\n",
      "epoch:10 step:7819 [D loss: 0.034536, acc.: 100.00%] [G loss: 2.183324]\n",
      "epoch:10 step:7820 [D loss: 0.116237, acc.: 96.88%] [G loss: 1.660761]\n",
      "epoch:10 step:7821 [D loss: 0.013698, acc.: 100.00%] [G loss: 1.906908]\n",
      "epoch:10 step:7822 [D loss: 0.111987, acc.: 96.88%] [G loss: 1.151513]\n",
      "epoch:10 step:7823 [D loss: 0.248950, acc.: 88.28%] [G loss: 4.799495]\n",
      "epoch:10 step:7824 [D loss: 0.579725, acc.: 71.09%] [G loss: 1.713936]\n",
      "epoch:10 step:7825 [D loss: 0.068491, acc.: 99.22%] [G loss: 0.859425]\n",
      "epoch:10 step:7826 [D loss: 0.226437, acc.: 88.28%] [G loss: 4.244539]\n",
      "epoch:10 step:7827 [D loss: 0.057946, acc.: 98.44%] [G loss: 4.545103]\n",
      "epoch:10 step:7828 [D loss: 0.079004, acc.: 97.66%] [G loss: 3.122934]\n",
      "epoch:10 step:7829 [D loss: 0.042523, acc.: 100.00%] [G loss: 2.560291]\n",
      "epoch:10 step:7830 [D loss: 0.220842, acc.: 92.19%] [G loss: 4.592045]\n",
      "epoch:10 step:7831 [D loss: 0.134375, acc.: 92.97%] [G loss: 4.119864]\n",
      "epoch:10 step:7832 [D loss: 0.048111, acc.: 100.00%] [G loss: 4.274309]\n",
      "epoch:10 step:7833 [D loss: 0.112750, acc.: 95.31%] [G loss: 3.328822]\n",
      "epoch:10 step:7834 [D loss: 0.048451, acc.: 100.00%] [G loss: 3.140904]\n",
      "epoch:10 step:7835 [D loss: 0.071645, acc.: 99.22%] [G loss: 3.292476]\n",
      "epoch:10 step:7836 [D loss: 0.081766, acc.: 98.44%] [G loss: 3.186298]\n",
      "epoch:10 step:7837 [D loss: 0.043109, acc.: 99.22%] [G loss: 3.595663]\n",
      "epoch:10 step:7838 [D loss: 0.428084, acc.: 80.47%] [G loss: 3.501994]\n",
      "epoch:10 step:7839 [D loss: 0.035085, acc.: 100.00%] [G loss: 4.018226]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:10 step:7840 [D loss: 0.069930, acc.: 98.44%] [G loss: 2.657134]\n",
      "epoch:10 step:7841 [D loss: 0.045554, acc.: 100.00%] [G loss: 3.051594]\n",
      "epoch:10 step:7842 [D loss: 0.055535, acc.: 99.22%] [G loss: 3.617122]\n",
      "epoch:10 step:7843 [D loss: 0.252552, acc.: 91.41%] [G loss: 4.287679]\n",
      "epoch:10 step:7844 [D loss: 0.019060, acc.: 100.00%] [G loss: 4.930504]\n",
      "epoch:10 step:7845 [D loss: 0.505806, acc.: 75.00%] [G loss: 5.026420]\n",
      "epoch:10 step:7846 [D loss: 0.209318, acc.: 89.06%] [G loss: 4.190164]\n",
      "epoch:10 step:7847 [D loss: 0.064161, acc.: 96.88%] [G loss: 4.024862]\n",
      "epoch:10 step:7848 [D loss: 0.066339, acc.: 99.22%] [G loss: 2.837592]\n",
      "epoch:10 step:7849 [D loss: 0.265251, acc.: 86.72%] [G loss: 5.364794]\n",
      "epoch:10 step:7850 [D loss: 0.460469, acc.: 75.00%] [G loss: 2.665650]\n",
      "epoch:10 step:7851 [D loss: 0.180356, acc.: 92.19%] [G loss: 4.838350]\n",
      "epoch:10 step:7852 [D loss: 0.027461, acc.: 100.00%] [G loss: 4.762910]\n",
      "epoch:10 step:7853 [D loss: 0.062890, acc.: 97.66%] [G loss: 4.247859]\n",
      "epoch:10 step:7854 [D loss: 0.656343, acc.: 66.41%] [G loss: 6.022212]\n",
      "epoch:10 step:7855 [D loss: 0.215123, acc.: 89.84%] [G loss: 5.137789]\n",
      "epoch:10 step:7856 [D loss: 0.107793, acc.: 96.09%] [G loss: 4.158043]\n",
      "epoch:10 step:7857 [D loss: 0.048116, acc.: 99.22%] [G loss: 4.173062]\n",
      "epoch:10 step:7858 [D loss: 0.070259, acc.: 98.44%] [G loss: 4.604972]\n",
      "epoch:10 step:7859 [D loss: 0.114151, acc.: 96.88%] [G loss: 4.420716]\n",
      "epoch:10 step:7860 [D loss: 0.140784, acc.: 98.44%] [G loss: 4.501614]\n",
      "epoch:10 step:7861 [D loss: 0.034334, acc.: 99.22%] [G loss: 4.534614]\n",
      "epoch:10 step:7862 [D loss: 0.067828, acc.: 98.44%] [G loss: 4.141149]\n",
      "epoch:10 step:7863 [D loss: 0.075334, acc.: 97.66%] [G loss: 3.533695]\n",
      "epoch:10 step:7864 [D loss: 0.155806, acc.: 95.31%] [G loss: 4.034925]\n",
      "epoch:10 step:7865 [D loss: 0.017777, acc.: 100.00%] [G loss: 4.304243]\n",
      "epoch:10 step:7866 [D loss: 0.378793, acc.: 80.47%] [G loss: 4.280685]\n",
      "epoch:10 step:7867 [D loss: 0.031287, acc.: 99.22%] [G loss: 4.735915]\n",
      "epoch:10 step:7868 [D loss: 0.016223, acc.: 100.00%] [G loss: 4.806679]\n",
      "epoch:10 step:7869 [D loss: 0.054341, acc.: 99.22%] [G loss: 4.206499]\n",
      "epoch:10 step:7870 [D loss: 0.104688, acc.: 96.88%] [G loss: 5.258312]\n",
      "epoch:10 step:7871 [D loss: 0.363367, acc.: 82.81%] [G loss: 4.463770]\n",
      "epoch:10 step:7872 [D loss: 0.032562, acc.: 99.22%] [G loss: 4.711681]\n",
      "epoch:10 step:7873 [D loss: 0.288234, acc.: 88.28%] [G loss: 3.789608]\n",
      "epoch:10 step:7874 [D loss: 0.074435, acc.: 99.22%] [G loss: 4.307429]\n",
      "epoch:10 step:7875 [D loss: 0.725309, acc.: 64.84%] [G loss: 6.282124]\n",
      "epoch:10 step:7876 [D loss: 0.053552, acc.: 98.44%] [G loss: 6.933716]\n",
      "epoch:10 step:7877 [D loss: 0.534317, acc.: 75.78%] [G loss: 3.641010]\n",
      "epoch:10 step:7878 [D loss: 0.130101, acc.: 96.09%] [G loss: 5.583934]\n",
      "epoch:10 step:7879 [D loss: 0.030117, acc.: 100.00%] [G loss: 5.367425]\n",
      "epoch:10 step:7880 [D loss: 0.021565, acc.: 100.00%] [G loss: 4.252020]\n",
      "epoch:10 step:7881 [D loss: 0.064607, acc.: 98.44%] [G loss: 2.914271]\n",
      "epoch:10 step:7882 [D loss: 0.067978, acc.: 99.22%] [G loss: 3.738966]\n",
      "epoch:10 step:7883 [D loss: 0.081178, acc.: 96.09%] [G loss: 1.415073]\n",
      "epoch:10 step:7884 [D loss: 0.015868, acc.: 100.00%] [G loss: 1.245095]\n",
      "epoch:10 step:7885 [D loss: 0.024507, acc.: 100.00%] [G loss: 1.554205]\n",
      "epoch:10 step:7886 [D loss: 0.031488, acc.: 100.00%] [G loss: 0.701189]\n",
      "epoch:10 step:7887 [D loss: 0.439870, acc.: 82.03%] [G loss: 5.920944]\n",
      "epoch:10 step:7888 [D loss: 0.671516, acc.: 67.97%] [G loss: 1.715237]\n",
      "epoch:10 step:7889 [D loss: 0.209894, acc.: 91.41%] [G loss: 4.155781]\n",
      "epoch:10 step:7890 [D loss: 0.041967, acc.: 100.00%] [G loss: 4.813265]\n",
      "epoch:10 step:7891 [D loss: 0.773611, acc.: 59.38%] [G loss: 6.720579]\n",
      "epoch:10 step:7892 [D loss: 0.338421, acc.: 81.25%] [G loss: 3.354771]\n",
      "epoch:10 step:7893 [D loss: 0.190539, acc.: 91.41%] [G loss: 3.884350]\n",
      "epoch:10 step:7894 [D loss: 0.048911, acc.: 99.22%] [G loss: 5.381294]\n",
      "epoch:10 step:7895 [D loss: 0.143003, acc.: 96.09%] [G loss: 3.406860]\n",
      "epoch:10 step:7896 [D loss: 0.019759, acc.: 100.00%] [G loss: 3.946201]\n",
      "epoch:10 step:7897 [D loss: 0.008767, acc.: 100.00%] [G loss: 2.202740]\n",
      "epoch:10 step:7898 [D loss: 0.141774, acc.: 94.53%] [G loss: 3.625869]\n",
      "epoch:10 step:7899 [D loss: 0.100134, acc.: 96.88%] [G loss: 3.203592]\n",
      "epoch:10 step:7900 [D loss: 1.297179, acc.: 35.16%] [G loss: 1.872349]\n",
      "epoch:10 step:7901 [D loss: 0.022827, acc.: 100.00%] [G loss: 3.518917]\n",
      "epoch:10 step:7902 [D loss: 0.062913, acc.: 98.44%] [G loss: 2.808168]\n",
      "epoch:10 step:7903 [D loss: 0.618912, acc.: 73.44%] [G loss: 4.287427]\n",
      "epoch:10 step:7904 [D loss: 0.077711, acc.: 97.66%] [G loss: 4.860682]\n",
      "epoch:10 step:7905 [D loss: 0.289352, acc.: 85.94%] [G loss: 0.375412]\n",
      "epoch:10 step:7906 [D loss: 0.483638, acc.: 75.00%] [G loss: 5.551790]\n",
      "epoch:10 step:7907 [D loss: 0.229555, acc.: 86.72%] [G loss: 4.226049]\n",
      "epoch:10 step:7908 [D loss: 0.062699, acc.: 100.00%] [G loss: 3.303619]\n",
      "epoch:10 step:7909 [D loss: 0.025480, acc.: 99.22%] [G loss: 3.553029]\n",
      "epoch:10 step:7910 [D loss: 0.063087, acc.: 98.44%] [G loss: 2.671561]\n",
      "epoch:10 step:7911 [D loss: 0.060137, acc.: 99.22%] [G loss: 0.690612]\n",
      "epoch:10 step:7912 [D loss: 0.273779, acc.: 84.38%] [G loss: 6.091399]\n",
      "epoch:10 step:7913 [D loss: 1.926787, acc.: 45.31%] [G loss: 1.996557]\n",
      "epoch:10 step:7914 [D loss: 0.083298, acc.: 98.44%] [G loss: 3.069330]\n",
      "epoch:10 step:7915 [D loss: 0.110132, acc.: 97.66%] [G loss: 3.622943]\n",
      "epoch:10 step:7916 [D loss: 0.009388, acc.: 100.00%] [G loss: 4.634478]\n",
      "epoch:10 step:7917 [D loss: 0.118203, acc.: 98.44%] [G loss: 2.433879]\n",
      "epoch:10 step:7918 [D loss: 0.152256, acc.: 96.88%] [G loss: 3.770255]\n",
      "epoch:10 step:7919 [D loss: 0.090305, acc.: 98.44%] [G loss: 4.030223]\n",
      "epoch:10 step:7920 [D loss: 0.168218, acc.: 94.53%] [G loss: 3.007490]\n",
      "epoch:10 step:7921 [D loss: 0.184041, acc.: 95.31%] [G loss: 3.541529]\n",
      "epoch:10 step:7922 [D loss: 0.016997, acc.: 100.00%] [G loss: 3.818227]\n",
      "epoch:10 step:7923 [D loss: 0.067735, acc.: 100.00%] [G loss: 3.747470]\n",
      "epoch:10 step:7924 [D loss: 0.384053, acc.: 83.59%] [G loss: 3.804099]\n",
      "epoch:10 step:7925 [D loss: 0.046929, acc.: 98.44%] [G loss: 3.930021]\n",
      "epoch:10 step:7926 [D loss: 0.210535, acc.: 92.97%] [G loss: 1.814365]\n",
      "epoch:10 step:7927 [D loss: 0.028410, acc.: 100.00%] [G loss: 0.445347]\n",
      "epoch:10 step:7928 [D loss: 0.019049, acc.: 100.00%] [G loss: 0.541181]\n",
      "epoch:10 step:7929 [D loss: 0.059646, acc.: 99.22%] [G loss: 1.084223]\n",
      "epoch:10 step:7930 [D loss: 0.029691, acc.: 100.00%] [G loss: 2.021076]\n",
      "epoch:10 step:7931 [D loss: 0.008438, acc.: 100.00%] [G loss: 2.363922]\n",
      "epoch:10 step:7932 [D loss: 0.017004, acc.: 100.00%] [G loss: 0.994602]\n",
      "epoch:10 step:7933 [D loss: 0.088390, acc.: 97.66%] [G loss: 1.828179]\n",
      "epoch:10 step:7934 [D loss: 0.048225, acc.: 99.22%] [G loss: 3.138215]\n",
      "epoch:10 step:7935 [D loss: 0.239470, acc.: 92.19%] [G loss: 1.096434]\n",
      "epoch:10 step:7936 [D loss: 0.368611, acc.: 83.59%] [G loss: 4.211778]\n",
      "epoch:10 step:7937 [D loss: 0.101191, acc.: 96.88%] [G loss: 4.273808]\n",
      "epoch:10 step:7938 [D loss: 0.157282, acc.: 95.31%] [G loss: 1.431315]\n",
      "epoch:10 step:7939 [D loss: 0.464345, acc.: 78.91%] [G loss: 6.804195]\n",
      "epoch:10 step:7940 [D loss: 1.256343, acc.: 54.69%] [G loss: 3.440696]\n",
      "epoch:10 step:7941 [D loss: 0.317584, acc.: 82.81%] [G loss: 4.910742]\n",
      "epoch:10 step:7942 [D loss: 0.076308, acc.: 98.44%] [G loss: 5.340713]\n",
      "epoch:10 step:7943 [D loss: 0.180942, acc.: 94.53%] [G loss: 3.697515]\n",
      "epoch:10 step:7944 [D loss: 0.080370, acc.: 97.66%] [G loss: 3.242700]\n",
      "epoch:10 step:7945 [D loss: 0.097605, acc.: 98.44%] [G loss: 3.702970]\n",
      "epoch:10 step:7946 [D loss: 0.026969, acc.: 100.00%] [G loss: 3.763084]\n",
      "epoch:10 step:7947 [D loss: 0.056228, acc.: 100.00%] [G loss: 3.982498]\n",
      "epoch:10 step:7948 [D loss: 0.060637, acc.: 98.44%] [G loss: 3.787783]\n",
      "epoch:10 step:7949 [D loss: 0.120406, acc.: 99.22%] [G loss: 3.478464]\n",
      "epoch:10 step:7950 [D loss: 0.627891, acc.: 71.88%] [G loss: 5.948214]\n",
      "epoch:10 step:7951 [D loss: 0.796841, acc.: 60.94%] [G loss: 2.415826]\n",
      "epoch:10 step:7952 [D loss: 0.229619, acc.: 89.84%] [G loss: 4.263600]\n",
      "epoch:10 step:7953 [D loss: 0.102850, acc.: 96.09%] [G loss: 4.202634]\n",
      "epoch:10 step:7954 [D loss: 0.027629, acc.: 100.00%] [G loss: 2.786044]\n",
      "epoch:10 step:7955 [D loss: 0.153204, acc.: 95.31%] [G loss: 2.257118]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:10 step:7956 [D loss: 0.053971, acc.: 99.22%] [G loss: 3.299902]\n",
      "epoch:10 step:7957 [D loss: 0.607201, acc.: 67.19%] [G loss: 4.598399]\n",
      "epoch:10 step:7958 [D loss: 0.109992, acc.: 96.09%] [G loss: 4.451975]\n",
      "epoch:10 step:7959 [D loss: 0.090627, acc.: 99.22%] [G loss: 2.689110]\n",
      "epoch:10 step:7960 [D loss: 0.017756, acc.: 100.00%] [G loss: 1.378479]\n",
      "epoch:10 step:7961 [D loss: 0.618107, acc.: 72.66%] [G loss: 6.509929]\n",
      "epoch:10 step:7962 [D loss: 1.043586, acc.: 56.25%] [G loss: 3.247032]\n",
      "epoch:10 step:7963 [D loss: 0.202519, acc.: 94.53%] [G loss: 1.402005]\n",
      "epoch:10 step:7964 [D loss: 0.098038, acc.: 95.31%] [G loss: 2.037764]\n",
      "epoch:10 step:7965 [D loss: 0.038331, acc.: 100.00%] [G loss: 1.475429]\n",
      "epoch:10 step:7966 [D loss: 0.204605, acc.: 94.53%] [G loss: 2.088621]\n",
      "epoch:10 step:7967 [D loss: 0.100609, acc.: 99.22%] [G loss: 2.568340]\n",
      "epoch:10 step:7968 [D loss: 1.378186, acc.: 35.16%] [G loss: 4.690330]\n",
      "epoch:10 step:7969 [D loss: 0.379384, acc.: 81.25%] [G loss: 4.529947]\n",
      "epoch:10 step:7970 [D loss: 0.702993, acc.: 64.06%] [G loss: 3.225776]\n",
      "epoch:10 step:7971 [D loss: 0.206216, acc.: 92.19%] [G loss: 4.435684]\n",
      "epoch:10 step:7972 [D loss: 0.037212, acc.: 99.22%] [G loss: 5.451358]\n",
      "epoch:10 step:7973 [D loss: 0.144417, acc.: 92.97%] [G loss: 4.623721]\n",
      "epoch:10 step:7974 [D loss: 0.092901, acc.: 100.00%] [G loss: 3.864647]\n",
      "epoch:10 step:7975 [D loss: 0.054449, acc.: 100.00%] [G loss: 3.874208]\n",
      "epoch:10 step:7976 [D loss: 0.088985, acc.: 97.66%] [G loss: 2.867056]\n",
      "epoch:10 step:7977 [D loss: 0.102507, acc.: 96.09%] [G loss: 3.801187]\n",
      "epoch:10 step:7978 [D loss: 0.056674, acc.: 99.22%] [G loss: 3.771051]\n",
      "epoch:10 step:7979 [D loss: 0.093193, acc.: 97.66%] [G loss: 3.648837]\n",
      "epoch:10 step:7980 [D loss: 0.195300, acc.: 94.53%] [G loss: 4.019157]\n",
      "epoch:10 step:7981 [D loss: 0.045623, acc.: 100.00%] [G loss: 4.426878]\n",
      "epoch:10 step:7982 [D loss: 0.083291, acc.: 98.44%] [G loss: 3.325876]\n",
      "epoch:10 step:7983 [D loss: 0.121613, acc.: 97.66%] [G loss: 2.677616]\n",
      "epoch:10 step:7984 [D loss: 0.078642, acc.: 99.22%] [G loss: 2.976282]\n",
      "epoch:10 step:7985 [D loss: 0.037828, acc.: 100.00%] [G loss: 1.939736]\n",
      "epoch:10 step:7986 [D loss: 0.357728, acc.: 82.81%] [G loss: 4.092519]\n",
      "epoch:10 step:7987 [D loss: 0.194330, acc.: 91.41%] [G loss: 3.535940]\n",
      "epoch:10 step:7988 [D loss: 0.175248, acc.: 97.66%] [G loss: 3.806139]\n",
      "epoch:10 step:7989 [D loss: 0.296026, acc.: 88.28%] [G loss: 2.087175]\n",
      "epoch:10 step:7990 [D loss: 0.041261, acc.: 100.00%] [G loss: 2.000999]\n",
      "epoch:10 step:7991 [D loss: 0.094600, acc.: 99.22%] [G loss: 3.809867]\n",
      "epoch:10 step:7992 [D loss: 0.055680, acc.: 100.00%] [G loss: 3.638474]\n",
      "epoch:10 step:7993 [D loss: 0.110030, acc.: 96.88%] [G loss: 2.548317]\n",
      "epoch:10 step:7994 [D loss: 0.242715, acc.: 90.62%] [G loss: 5.139691]\n",
      "epoch:10 step:7995 [D loss: 0.309060, acc.: 88.28%] [G loss: 2.804391]\n",
      "epoch:10 step:7996 [D loss: 0.039988, acc.: 100.00%] [G loss: 3.670335]\n",
      "epoch:10 step:7997 [D loss: 0.227415, acc.: 92.97%] [G loss: 4.027065]\n",
      "epoch:10 step:7998 [D loss: 0.117280, acc.: 96.09%] [G loss: 3.569822]\n",
      "epoch:10 step:7999 [D loss: 0.076974, acc.: 97.66%] [G loss: 3.766283]\n",
      "epoch:10 step:8000 [D loss: 0.101373, acc.: 96.09%] [G loss: 3.300493]\n",
      "##############\n",
      "[1.02635009 0.91832663 0.91490001 0.98960826 0.9041794  1.02627167\n",
      " 2.12095727 0.82445121 0.98195795 0.98619853]\n",
      "##########\n",
      "epoch:10 step:8001 [D loss: 0.225984, acc.: 93.75%] [G loss: 4.097575]\n",
      "epoch:10 step:8002 [D loss: 0.075895, acc.: 98.44%] [G loss: 4.509525]\n",
      "epoch:10 step:8003 [D loss: 0.985119, acc.: 45.31%] [G loss: 5.791962]\n",
      "epoch:10 step:8004 [D loss: 0.103371, acc.: 95.31%] [G loss: 6.699785]\n",
      "epoch:10 step:8005 [D loss: 0.504567, acc.: 75.00%] [G loss: 1.869089]\n",
      "epoch:10 step:8006 [D loss: 0.284520, acc.: 84.38%] [G loss: 5.042673]\n",
      "epoch:10 step:8007 [D loss: 0.129621, acc.: 95.31%] [G loss: 5.057059]\n",
      "epoch:10 step:8008 [D loss: 0.028600, acc.: 99.22%] [G loss: 4.997334]\n",
      "epoch:10 step:8009 [D loss: 0.053990, acc.: 99.22%] [G loss: 3.352865]\n",
      "epoch:10 step:8010 [D loss: 0.048551, acc.: 100.00%] [G loss: 2.741018]\n",
      "epoch:10 step:8011 [D loss: 0.022790, acc.: 100.00%] [G loss: 2.573604]\n",
      "epoch:10 step:8012 [D loss: 0.211599, acc.: 93.75%] [G loss: 3.915329]\n",
      "epoch:10 step:8013 [D loss: 0.060322, acc.: 97.66%] [G loss: 4.836734]\n",
      "epoch:10 step:8014 [D loss: 1.053418, acc.: 45.31%] [G loss: 6.264257]\n",
      "epoch:10 step:8015 [D loss: 1.043016, acc.: 55.47%] [G loss: 3.986710]\n",
      "epoch:10 step:8016 [D loss: 0.053416, acc.: 99.22%] [G loss: 1.740783]\n",
      "epoch:10 step:8017 [D loss: 0.451653, acc.: 77.34%] [G loss: 5.766160]\n",
      "epoch:10 step:8018 [D loss: 0.581575, acc.: 74.22%] [G loss: 3.600022]\n",
      "epoch:10 step:8019 [D loss: 0.101139, acc.: 99.22%] [G loss: 2.626904]\n",
      "epoch:10 step:8020 [D loss: 0.110530, acc.: 97.66%] [G loss: 4.479376]\n",
      "epoch:10 step:8021 [D loss: 0.015753, acc.: 100.00%] [G loss: 4.724379]\n",
      "epoch:10 step:8022 [D loss: 0.035078, acc.: 99.22%] [G loss: 4.297781]\n",
      "epoch:10 step:8023 [D loss: 0.264046, acc.: 88.28%] [G loss: 3.407444]\n",
      "epoch:10 step:8024 [D loss: 0.627790, acc.: 67.19%] [G loss: 4.541590]\n",
      "epoch:10 step:8025 [D loss: 0.011397, acc.: 100.00%] [G loss: 3.712796]\n",
      "epoch:10 step:8026 [D loss: 0.097794, acc.: 97.66%] [G loss: 2.173505]\n",
      "epoch:10 step:8027 [D loss: 0.017740, acc.: 100.00%] [G loss: 2.790939]\n",
      "epoch:10 step:8028 [D loss: 0.075966, acc.: 99.22%] [G loss: 1.852978]\n",
      "epoch:10 step:8029 [D loss: 0.042614, acc.: 99.22%] [G loss: 2.177192]\n",
      "epoch:10 step:8030 [D loss: 0.142567, acc.: 95.31%] [G loss: 2.795427]\n",
      "epoch:10 step:8031 [D loss: 0.022235, acc.: 100.00%] [G loss: 2.888887]\n",
      "epoch:10 step:8032 [D loss: 0.382669, acc.: 83.59%] [G loss: 4.892986]\n",
      "epoch:10 step:8033 [D loss: 0.198079, acc.: 90.62%] [G loss: 4.665807]\n",
      "epoch:10 step:8034 [D loss: 0.051872, acc.: 100.00%] [G loss: 3.312647]\n",
      "epoch:10 step:8035 [D loss: 0.028739, acc.: 100.00%] [G loss: 3.493793]\n",
      "epoch:10 step:8036 [D loss: 0.029990, acc.: 100.00%] [G loss: 2.474991]\n",
      "epoch:10 step:8037 [D loss: 0.127588, acc.: 94.53%] [G loss: 4.711043]\n",
      "epoch:10 step:8038 [D loss: 0.031875, acc.: 100.00%] [G loss: 4.703962]\n",
      "epoch:10 step:8039 [D loss: 0.268592, acc.: 89.06%] [G loss: 2.648640]\n",
      "epoch:10 step:8040 [D loss: 0.126047, acc.: 97.66%] [G loss: 4.236554]\n",
      "epoch:10 step:8041 [D loss: 0.014593, acc.: 100.00%] [G loss: 4.568034]\n",
      "epoch:10 step:8042 [D loss: 0.078810, acc.: 99.22%] [G loss: 4.309578]\n",
      "epoch:10 step:8043 [D loss: 0.028336, acc.: 100.00%] [G loss: 4.011381]\n",
      "epoch:10 step:8044 [D loss: 0.084142, acc.: 99.22%] [G loss: 4.313948]\n",
      "epoch:10 step:8045 [D loss: 0.025864, acc.: 100.00%] [G loss: 4.365528]\n",
      "epoch:10 step:8046 [D loss: 0.106868, acc.: 97.66%] [G loss: 3.226285]\n",
      "epoch:10 step:8047 [D loss: 0.253058, acc.: 89.84%] [G loss: 5.730065]\n",
      "epoch:10 step:8048 [D loss: 0.240310, acc.: 89.84%] [G loss: 5.013295]\n",
      "epoch:10 step:8049 [D loss: 0.013482, acc.: 100.00%] [G loss: 4.465338]\n",
      "epoch:10 step:8050 [D loss: 0.452691, acc.: 75.00%] [G loss: 5.251813]\n",
      "epoch:10 step:8051 [D loss: 0.022589, acc.: 100.00%] [G loss: 6.209512]\n",
      "epoch:10 step:8052 [D loss: 0.159782, acc.: 93.75%] [G loss: 4.068207]\n",
      "epoch:10 step:8053 [D loss: 0.026165, acc.: 100.00%] [G loss: 2.991654]\n",
      "epoch:10 step:8054 [D loss: 0.056600, acc.: 99.22%] [G loss: 4.068764]\n",
      "epoch:10 step:8055 [D loss: 0.008735, acc.: 100.00%] [G loss: 4.552500]\n",
      "epoch:10 step:8056 [D loss: 0.058986, acc.: 100.00%] [G loss: 3.769441]\n",
      "epoch:10 step:8057 [D loss: 0.158965, acc.: 95.31%] [G loss: 5.293747]\n",
      "epoch:10 step:8058 [D loss: 0.161022, acc.: 93.75%] [G loss: 3.062424]\n",
      "epoch:10 step:8059 [D loss: 0.018246, acc.: 100.00%] [G loss: 1.988984]\n",
      "epoch:10 step:8060 [D loss: 0.188633, acc.: 95.31%] [G loss: 4.677527]\n",
      "epoch:10 step:8061 [D loss: 0.018207, acc.: 99.22%] [G loss: 4.399979]\n",
      "epoch:10 step:8062 [D loss: 0.327344, acc.: 88.28%] [G loss: 2.499544]\n",
      "epoch:10 step:8063 [D loss: 0.193118, acc.: 90.62%] [G loss: 6.194864]\n",
      "epoch:10 step:8064 [D loss: 0.394217, acc.: 78.91%] [G loss: 2.615307]\n",
      "epoch:10 step:8065 [D loss: 0.130525, acc.: 94.53%] [G loss: 3.125330]\n",
      "epoch:10 step:8066 [D loss: 0.026039, acc.: 100.00%] [G loss: 5.528522]\n",
      "epoch:10 step:8067 [D loss: 0.066252, acc.: 97.66%] [G loss: 2.699989]\n",
      "epoch:10 step:8068 [D loss: 0.047686, acc.: 98.44%] [G loss: 1.610913]\n",
      "epoch:10 step:8069 [D loss: 0.085936, acc.: 98.44%] [G loss: 5.504750]\n",
      "epoch:10 step:8070 [D loss: 0.125260, acc.: 97.66%] [G loss: 1.961983]\n",
      "epoch:10 step:8071 [D loss: 0.294215, acc.: 86.72%] [G loss: 4.754648]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:10 step:8072 [D loss: 0.050845, acc.: 99.22%] [G loss: 5.112671]\n",
      "epoch:10 step:8073 [D loss: 0.152862, acc.: 94.53%] [G loss: 2.402350]\n",
      "epoch:10 step:8074 [D loss: 0.350333, acc.: 84.38%] [G loss: 4.024466]\n",
      "epoch:10 step:8075 [D loss: 0.204517, acc.: 92.97%] [G loss: 2.446655]\n",
      "epoch:10 step:8076 [D loss: 0.056960, acc.: 98.44%] [G loss: 1.324522]\n",
      "epoch:10 step:8077 [D loss: 0.248660, acc.: 92.19%] [G loss: 2.172410]\n",
      "epoch:10 step:8078 [D loss: 0.015524, acc.: 100.00%] [G loss: 3.294354]\n",
      "epoch:10 step:8079 [D loss: 0.081429, acc.: 99.22%] [G loss: 2.757872]\n",
      "epoch:10 step:8080 [D loss: 1.138568, acc.: 47.66%] [G loss: 8.868923]\n",
      "epoch:10 step:8081 [D loss: 1.478234, acc.: 53.91%] [G loss: 6.770714]\n",
      "epoch:10 step:8082 [D loss: 0.041602, acc.: 98.44%] [G loss: 5.314936]\n",
      "epoch:10 step:8083 [D loss: 0.071212, acc.: 96.88%] [G loss: 3.086999]\n",
      "epoch:10 step:8084 [D loss: 0.360086, acc.: 84.38%] [G loss: 5.255557]\n",
      "epoch:10 step:8085 [D loss: 0.057214, acc.: 99.22%] [G loss: 5.421554]\n",
      "epoch:10 step:8086 [D loss: 0.240793, acc.: 91.41%] [G loss: 3.718042]\n",
      "epoch:10 step:8087 [D loss: 0.135040, acc.: 97.66%] [G loss: 3.085280]\n",
      "epoch:10 step:8088 [D loss: 0.035700, acc.: 99.22%] [G loss: 3.699151]\n",
      "epoch:10 step:8089 [D loss: 0.191581, acc.: 93.75%] [G loss: 3.953740]\n",
      "epoch:10 step:8090 [D loss: 0.169548, acc.: 94.53%] [G loss: 3.378675]\n",
      "epoch:10 step:8091 [D loss: 0.029382, acc.: 100.00%] [G loss: 3.276521]\n",
      "epoch:10 step:8092 [D loss: 0.237989, acc.: 91.41%] [G loss: 5.811943]\n",
      "epoch:10 step:8093 [D loss: 0.220075, acc.: 91.41%] [G loss: 3.525308]\n",
      "epoch:10 step:8094 [D loss: 0.056227, acc.: 98.44%] [G loss: 3.580308]\n",
      "epoch:10 step:8095 [D loss: 0.062531, acc.: 99.22%] [G loss: 4.516871]\n",
      "epoch:10 step:8096 [D loss: 0.890929, acc.: 54.69%] [G loss: 5.720391]\n",
      "epoch:10 step:8097 [D loss: 0.170656, acc.: 92.97%] [G loss: 5.828406]\n",
      "epoch:10 step:8098 [D loss: 0.055709, acc.: 99.22%] [G loss: 4.372808]\n",
      "epoch:10 step:8099 [D loss: 0.147425, acc.: 96.09%] [G loss: 3.447960]\n",
      "epoch:10 step:8100 [D loss: 0.083048, acc.: 98.44%] [G loss: 3.669889]\n",
      "epoch:10 step:8101 [D loss: 0.113827, acc.: 96.88%] [G loss: 4.714416]\n",
      "epoch:10 step:8102 [D loss: 0.045788, acc.: 98.44%] [G loss: 3.117047]\n",
      "epoch:10 step:8103 [D loss: 0.112702, acc.: 97.66%] [G loss: 2.853798]\n",
      "epoch:10 step:8104 [D loss: 0.147875, acc.: 99.22%] [G loss: 2.075371]\n",
      "epoch:10 step:8105 [D loss: 0.085192, acc.: 97.66%] [G loss: 1.258497]\n",
      "epoch:10 step:8106 [D loss: 0.932762, acc.: 58.59%] [G loss: 7.395638]\n",
      "epoch:10 step:8107 [D loss: 2.409445, acc.: 50.00%] [G loss: 4.167567]\n",
      "epoch:10 step:8108 [D loss: 0.776350, acc.: 58.59%] [G loss: 2.613155]\n",
      "epoch:10 step:8109 [D loss: 0.082161, acc.: 99.22%] [G loss: 2.860843]\n",
      "epoch:10 step:8110 [D loss: 0.065790, acc.: 100.00%] [G loss: 3.395457]\n",
      "epoch:10 step:8111 [D loss: 0.036673, acc.: 100.00%] [G loss: 2.454926]\n",
      "epoch:10 step:8112 [D loss: 0.180842, acc.: 96.88%] [G loss: 3.271790]\n",
      "epoch:10 step:8113 [D loss: 0.071344, acc.: 98.44%] [G loss: 2.791055]\n",
      "epoch:10 step:8114 [D loss: 0.069754, acc.: 99.22%] [G loss: 3.103636]\n",
      "epoch:10 step:8115 [D loss: 0.280381, acc.: 93.75%] [G loss: 3.944595]\n",
      "epoch:10 step:8116 [D loss: 0.052553, acc.: 98.44%] [G loss: 3.801227]\n",
      "epoch:10 step:8117 [D loss: 0.060465, acc.: 100.00%] [G loss: 3.371881]\n",
      "epoch:10 step:8118 [D loss: 0.109106, acc.: 97.66%] [G loss: 3.847783]\n",
      "epoch:10 step:8119 [D loss: 0.364382, acc.: 88.28%] [G loss: 2.699830]\n",
      "epoch:10 step:8120 [D loss: 0.042555, acc.: 100.00%] [G loss: 3.710381]\n",
      "epoch:10 step:8121 [D loss: 0.069734, acc.: 100.00%] [G loss: 3.602108]\n",
      "epoch:10 step:8122 [D loss: 0.206130, acc.: 94.53%] [G loss: 3.056000]\n",
      "epoch:10 step:8123 [D loss: 0.204150, acc.: 92.97%] [G loss: 3.002357]\n",
      "epoch:10 step:8124 [D loss: 0.050641, acc.: 99.22%] [G loss: 3.911114]\n",
      "epoch:10 step:8125 [D loss: 1.419327, acc.: 26.56%] [G loss: 5.833004]\n",
      "epoch:10 step:8126 [D loss: 0.332188, acc.: 82.81%] [G loss: 4.886294]\n",
      "epoch:10 step:8127 [D loss: 0.210673, acc.: 92.97%] [G loss: 3.951397]\n",
      "epoch:10 step:8128 [D loss: 0.048291, acc.: 99.22%] [G loss: 3.476676]\n",
      "epoch:10 step:8129 [D loss: 0.060155, acc.: 98.44%] [G loss: 3.985075]\n",
      "epoch:10 step:8130 [D loss: 0.172703, acc.: 96.09%] [G loss: 3.603255]\n",
      "epoch:10 step:8131 [D loss: 0.023871, acc.: 100.00%] [G loss: 3.822741]\n",
      "epoch:10 step:8132 [D loss: 0.029588, acc.: 100.00%] [G loss: 3.360380]\n",
      "epoch:10 step:8133 [D loss: 0.036534, acc.: 100.00%] [G loss: 2.458993]\n",
      "epoch:10 step:8134 [D loss: 0.247447, acc.: 92.97%] [G loss: 3.926402]\n",
      "epoch:10 step:8135 [D loss: 0.232275, acc.: 88.28%] [G loss: 3.003856]\n",
      "epoch:10 step:8136 [D loss: 0.100770, acc.: 97.66%] [G loss: 3.767363]\n",
      "epoch:10 step:8137 [D loss: 0.017070, acc.: 100.00%] [G loss: 3.636140]\n",
      "epoch:10 step:8138 [D loss: 0.082182, acc.: 97.66%] [G loss: 1.850502]\n",
      "epoch:10 step:8139 [D loss: 0.064175, acc.: 99.22%] [G loss: 3.187791]\n",
      "epoch:10 step:8140 [D loss: 0.166656, acc.: 96.09%] [G loss: 4.546546]\n",
      "epoch:10 step:8141 [D loss: 0.397621, acc.: 80.47%] [G loss: 2.749998]\n",
      "epoch:10 step:8142 [D loss: 0.317044, acc.: 82.03%] [G loss: 5.211838]\n",
      "epoch:10 step:8143 [D loss: 0.225637, acc.: 89.06%] [G loss: 5.028684]\n",
      "epoch:10 step:8144 [D loss: 0.129803, acc.: 96.09%] [G loss: 3.317804]\n",
      "epoch:10 step:8145 [D loss: 0.041012, acc.: 99.22%] [G loss: 3.296221]\n",
      "epoch:10 step:8146 [D loss: 0.081827, acc.: 97.66%] [G loss: 3.379583]\n",
      "epoch:10 step:8147 [D loss: 0.054615, acc.: 100.00%] [G loss: 4.104486]\n",
      "epoch:10 step:8148 [D loss: 0.397642, acc.: 78.91%] [G loss: 2.784718]\n",
      "epoch:10 step:8149 [D loss: 0.036834, acc.: 100.00%] [G loss: 3.092808]\n",
      "epoch:10 step:8150 [D loss: 0.152191, acc.: 94.53%] [G loss: 3.762650]\n",
      "epoch:10 step:8151 [D loss: 0.047262, acc.: 100.00%] [G loss: 3.629114]\n",
      "epoch:10 step:8152 [D loss: 0.060552, acc.: 99.22%] [G loss: 3.499348]\n",
      "epoch:10 step:8153 [D loss: 0.379787, acc.: 84.38%] [G loss: 3.975034]\n",
      "epoch:10 step:8154 [D loss: 0.025564, acc.: 100.00%] [G loss: 4.909375]\n",
      "epoch:10 step:8155 [D loss: 0.103264, acc.: 96.09%] [G loss: 3.276852]\n",
      "epoch:10 step:8156 [D loss: 0.267297, acc.: 88.28%] [G loss: 4.890228]\n",
      "epoch:10 step:8157 [D loss: 0.066196, acc.: 98.44%] [G loss: 5.379983]\n",
      "epoch:10 step:8158 [D loss: 0.014943, acc.: 100.00%] [G loss: 4.472881]\n",
      "epoch:10 step:8159 [D loss: 0.037517, acc.: 100.00%] [G loss: 3.833590]\n",
      "epoch:10 step:8160 [D loss: 0.106659, acc.: 98.44%] [G loss: 3.742895]\n",
      "epoch:10 step:8161 [D loss: 0.034186, acc.: 100.00%] [G loss: 4.154430]\n",
      "epoch:10 step:8162 [D loss: 0.092496, acc.: 98.44%] [G loss: 3.908397]\n",
      "epoch:10 step:8163 [D loss: 0.076785, acc.: 99.22%] [G loss: 3.355155]\n",
      "epoch:10 step:8164 [D loss: 0.039919, acc.: 99.22%] [G loss: 2.405912]\n",
      "epoch:10 step:8165 [D loss: 0.192637, acc.: 93.75%] [G loss: 2.353522]\n",
      "epoch:10 step:8166 [D loss: 0.027942, acc.: 99.22%] [G loss: 3.825404]\n",
      "epoch:10 step:8167 [D loss: 0.065258, acc.: 99.22%] [G loss: 2.545849]\n",
      "epoch:10 step:8168 [D loss: 1.264004, acc.: 46.09%] [G loss: 7.925982]\n",
      "epoch:10 step:8169 [D loss: 2.557842, acc.: 50.00%] [G loss: 5.983382]\n",
      "epoch:10 step:8170 [D loss: 0.560910, acc.: 71.09%] [G loss: 3.055786]\n",
      "epoch:10 step:8171 [D loss: 0.084284, acc.: 97.66%] [G loss: 2.904393]\n",
      "epoch:10 step:8172 [D loss: 0.056441, acc.: 99.22%] [G loss: 3.162333]\n",
      "epoch:10 step:8173 [D loss: 0.033267, acc.: 100.00%] [G loss: 3.093387]\n",
      "epoch:10 step:8174 [D loss: 0.085437, acc.: 99.22%] [G loss: 2.979377]\n",
      "epoch:10 step:8175 [D loss: 0.071065, acc.: 99.22%] [G loss: 3.574190]\n",
      "epoch:10 step:8176 [D loss: 0.053254, acc.: 100.00%] [G loss: 3.538444]\n",
      "epoch:10 step:8177 [D loss: 0.046479, acc.: 100.00%] [G loss: 3.485262]\n",
      "epoch:10 step:8178 [D loss: 0.035406, acc.: 100.00%] [G loss: 2.985913]\n",
      "epoch:10 step:8179 [D loss: 0.087758, acc.: 100.00%] [G loss: 3.409417]\n",
      "epoch:10 step:8180 [D loss: 0.053898, acc.: 100.00%] [G loss: 3.042168]\n",
      "epoch:10 step:8181 [D loss: 0.078741, acc.: 99.22%] [G loss: 3.477550]\n",
      "epoch:10 step:8182 [D loss: 0.046057, acc.: 100.00%] [G loss: 3.056061]\n",
      "epoch:10 step:8183 [D loss: 0.300337, acc.: 88.28%] [G loss: 3.800459]\n",
      "epoch:10 step:8184 [D loss: 0.179748, acc.: 92.97%] [G loss: 2.104330]\n",
      "epoch:10 step:8185 [D loss: 0.371190, acc.: 82.03%] [G loss: 3.498437]\n",
      "epoch:10 step:8186 [D loss: 0.077679, acc.: 99.22%] [G loss: 4.166886]\n",
      "epoch:10 step:8187 [D loss: 0.157862, acc.: 94.53%] [G loss: 1.410195]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:10 step:8188 [D loss: 0.086774, acc.: 98.44%] [G loss: 1.594438]\n",
      "epoch:10 step:8189 [D loss: 0.042287, acc.: 100.00%] [G loss: 2.050928]\n",
      "epoch:10 step:8190 [D loss: 0.029249, acc.: 100.00%] [G loss: 2.758663]\n",
      "epoch:10 step:8191 [D loss: 0.022171, acc.: 100.00%] [G loss: 2.129170]\n",
      "epoch:10 step:8192 [D loss: 0.177849, acc.: 96.88%] [G loss: 1.085301]\n",
      "epoch:10 step:8193 [D loss: 0.104563, acc.: 99.22%] [G loss: 3.356447]\n",
      "epoch:10 step:8194 [D loss: 0.064064, acc.: 99.22%] [G loss: 3.246076]\n",
      "epoch:10 step:8195 [D loss: 0.292335, acc.: 89.84%] [G loss: 2.037470]\n",
      "epoch:10 step:8196 [D loss: 0.051683, acc.: 99.22%] [G loss: 2.085384]\n",
      "epoch:10 step:8197 [D loss: 0.020415, acc.: 100.00%] [G loss: 1.874178]\n",
      "epoch:10 step:8198 [D loss: 0.132257, acc.: 96.88%] [G loss: 4.876084]\n",
      "epoch:10 step:8199 [D loss: 0.135400, acc.: 95.31%] [G loss: 2.198999]\n",
      "epoch:10 step:8200 [D loss: 0.051013, acc.: 99.22%] [G loss: 2.340829]\n",
      "##############\n",
      "[0.91925811 0.95100556 1.00301635 0.93675631 0.97794665 0.98375975\n",
      " 1.02346996 0.83071874 0.95941634 2.12453179]\n",
      "##########\n",
      "epoch:10 step:8201 [D loss: 0.248657, acc.: 92.19%] [G loss: 5.271082]\n",
      "epoch:10 step:8202 [D loss: 0.102679, acc.: 97.66%] [G loss: 5.351960]\n",
      "epoch:10 step:8203 [D loss: 0.200391, acc.: 91.41%] [G loss: 2.241112]\n",
      "epoch:10 step:8204 [D loss: 0.119573, acc.: 96.09%] [G loss: 3.938359]\n",
      "epoch:10 step:8205 [D loss: 0.078659, acc.: 99.22%] [G loss: 2.812786]\n",
      "epoch:10 step:8206 [D loss: 0.281939, acc.: 90.62%] [G loss: 2.582673]\n",
      "epoch:10 step:8207 [D loss: 0.027445, acc.: 100.00%] [G loss: 2.568363]\n",
      "epoch:10 step:8208 [D loss: 0.014919, acc.: 100.00%] [G loss: 2.431123]\n",
      "epoch:10 step:8209 [D loss: 0.115298, acc.: 94.53%] [G loss: 4.160391]\n",
      "epoch:10 step:8210 [D loss: 0.055836, acc.: 99.22%] [G loss: 3.384455]\n",
      "epoch:10 step:8211 [D loss: 0.677219, acc.: 65.62%] [G loss: 1.067783]\n",
      "epoch:10 step:8212 [D loss: 0.032436, acc.: 100.00%] [G loss: 1.742862]\n",
      "epoch:10 step:8213 [D loss: 0.204896, acc.: 92.97%] [G loss: 4.666149]\n",
      "epoch:10 step:8214 [D loss: 0.422013, acc.: 80.47%] [G loss: 2.686118]\n",
      "epoch:10 step:8215 [D loss: 0.049644, acc.: 99.22%] [G loss: 3.122648]\n",
      "epoch:10 step:8216 [D loss: 0.408087, acc.: 80.47%] [G loss: 5.179485]\n",
      "epoch:10 step:8217 [D loss: 0.164098, acc.: 95.31%] [G loss: 4.882746]\n",
      "epoch:10 step:8218 [D loss: 0.135180, acc.: 96.88%] [G loss: 4.240493]\n",
      "epoch:10 step:8219 [D loss: 0.029083, acc.: 99.22%] [G loss: 4.134899]\n",
      "epoch:10 step:8220 [D loss: 0.013070, acc.: 100.00%] [G loss: 3.442692]\n",
      "epoch:10 step:8221 [D loss: 0.112502, acc.: 97.66%] [G loss: 1.991349]\n",
      "epoch:10 step:8222 [D loss: 0.146236, acc.: 96.09%] [G loss: 4.236703]\n",
      "epoch:10 step:8223 [D loss: 0.221575, acc.: 89.06%] [G loss: 3.213556]\n",
      "epoch:10 step:8224 [D loss: 0.183509, acc.: 96.09%] [G loss: 5.376875]\n",
      "epoch:10 step:8225 [D loss: 0.086034, acc.: 96.88%] [G loss: 4.676697]\n",
      "epoch:10 step:8226 [D loss: 0.079172, acc.: 98.44%] [G loss: 4.102335]\n",
      "epoch:10 step:8227 [D loss: 0.097364, acc.: 96.88%] [G loss: 4.284385]\n",
      "epoch:10 step:8228 [D loss: 0.026331, acc.: 100.00%] [G loss: 4.506915]\n",
      "epoch:10 step:8229 [D loss: 0.048761, acc.: 99.22%] [G loss: 4.517652]\n",
      "epoch:10 step:8230 [D loss: 2.956265, acc.: 24.22%] [G loss: 8.124632]\n",
      "epoch:10 step:8231 [D loss: 2.407352, acc.: 50.78%] [G loss: 5.438765]\n",
      "epoch:10 step:8232 [D loss: 1.183609, acc.: 53.91%] [G loss: 2.679274]\n",
      "epoch:10 step:8233 [D loss: 0.337645, acc.: 85.94%] [G loss: 2.006007]\n",
      "epoch:10 step:8234 [D loss: 0.159238, acc.: 98.44%] [G loss: 1.732533]\n",
      "epoch:10 step:8235 [D loss: 0.235639, acc.: 91.41%] [G loss: 2.527923]\n",
      "epoch:10 step:8236 [D loss: 0.249009, acc.: 91.41%] [G loss: 1.887088]\n",
      "epoch:10 step:8237 [D loss: 0.108206, acc.: 100.00%] [G loss: 2.460724]\n",
      "epoch:10 step:8238 [D loss: 0.172525, acc.: 96.09%] [G loss: 2.801415]\n",
      "epoch:10 step:8239 [D loss: 0.220479, acc.: 94.53%] [G loss: 3.253270]\n",
      "epoch:10 step:8240 [D loss: 0.231194, acc.: 92.97%] [G loss: 2.809409]\n",
      "epoch:10 step:8241 [D loss: 0.180401, acc.: 92.97%] [G loss: 3.178865]\n",
      "epoch:10 step:8242 [D loss: 0.060071, acc.: 100.00%] [G loss: 3.924724]\n",
      "epoch:10 step:8243 [D loss: 0.130989, acc.: 95.31%] [G loss: 3.211766]\n",
      "epoch:10 step:8244 [D loss: 0.242107, acc.: 89.84%] [G loss: 4.668977]\n",
      "epoch:10 step:8245 [D loss: 0.221389, acc.: 89.06%] [G loss: 3.583416]\n",
      "epoch:10 step:8246 [D loss: 0.079949, acc.: 100.00%] [G loss: 2.416265]\n",
      "epoch:10 step:8247 [D loss: 0.140159, acc.: 97.66%] [G loss: 2.343439]\n",
      "epoch:10 step:8248 [D loss: 0.044223, acc.: 100.00%] [G loss: 2.785941]\n",
      "epoch:10 step:8249 [D loss: 0.114775, acc.: 98.44%] [G loss: 2.095186]\n",
      "epoch:10 step:8250 [D loss: 0.206173, acc.: 94.53%] [G loss: 1.672212]\n",
      "epoch:10 step:8251 [D loss: 0.088042, acc.: 99.22%] [G loss: 3.075978]\n",
      "epoch:10 step:8252 [D loss: 0.101041, acc.: 96.88%] [G loss: 2.421000]\n",
      "epoch:10 step:8253 [D loss: 0.179485, acc.: 94.53%] [G loss: 3.403096]\n",
      "epoch:10 step:8254 [D loss: 0.071045, acc.: 98.44%] [G loss: 2.822515]\n",
      "epoch:10 step:8255 [D loss: 0.400665, acc.: 77.34%] [G loss: 3.782305]\n",
      "epoch:10 step:8256 [D loss: 0.056777, acc.: 99.22%] [G loss: 4.846883]\n",
      "epoch:10 step:8257 [D loss: 0.739651, acc.: 60.94%] [G loss: 3.232876]\n",
      "epoch:10 step:8258 [D loss: 0.275228, acc.: 89.06%] [G loss: 2.203062]\n",
      "epoch:10 step:8259 [D loss: 0.035415, acc.: 100.00%] [G loss: 2.496199]\n",
      "epoch:10 step:8260 [D loss: 0.079405, acc.: 99.22%] [G loss: 2.133949]\n",
      "epoch:10 step:8261 [D loss: 0.099134, acc.: 97.66%] [G loss: 1.693183]\n",
      "epoch:10 step:8262 [D loss: 0.130516, acc.: 98.44%] [G loss: 2.418649]\n",
      "epoch:10 step:8263 [D loss: 0.923559, acc.: 50.78%] [G loss: 5.766333]\n",
      "epoch:10 step:8264 [D loss: 0.801610, acc.: 64.06%] [G loss: 4.286435]\n",
      "epoch:10 step:8265 [D loss: 0.158721, acc.: 97.66%] [G loss: 2.292729]\n",
      "epoch:10 step:8266 [D loss: 0.176973, acc.: 92.97%] [G loss: 3.425177]\n",
      "epoch:10 step:8267 [D loss: 0.015516, acc.: 100.00%] [G loss: 3.494847]\n",
      "epoch:10 step:8268 [D loss: 0.113967, acc.: 95.31%] [G loss: 3.301954]\n",
      "epoch:10 step:8269 [D loss: 0.116791, acc.: 96.88%] [G loss: 1.588037]\n",
      "epoch:10 step:8270 [D loss: 0.218775, acc.: 89.84%] [G loss: 4.246312]\n",
      "epoch:10 step:8271 [D loss: 0.174505, acc.: 92.97%] [G loss: 2.981918]\n",
      "epoch:10 step:8272 [D loss: 0.067038, acc.: 98.44%] [G loss: 3.102957]\n",
      "epoch:10 step:8273 [D loss: 0.053375, acc.: 99.22%] [G loss: 2.238530]\n",
      "epoch:10 step:8274 [D loss: 0.084888, acc.: 96.88%] [G loss: 3.477851]\n",
      "epoch:10 step:8275 [D loss: 0.082233, acc.: 99.22%] [G loss: 3.511968]\n",
      "epoch:10 step:8276 [D loss: 0.217924, acc.: 92.19%] [G loss: 4.766678]\n",
      "epoch:10 step:8277 [D loss: 0.249057, acc.: 90.62%] [G loss: 2.534171]\n",
      "epoch:10 step:8278 [D loss: 0.223511, acc.: 91.41%] [G loss: 4.642875]\n",
      "epoch:10 step:8279 [D loss: 0.040866, acc.: 99.22%] [G loss: 4.868175]\n",
      "epoch:10 step:8280 [D loss: 0.186575, acc.: 92.19%] [G loss: 1.359494]\n",
      "epoch:10 step:8281 [D loss: 0.114019, acc.: 99.22%] [G loss: 2.938348]\n",
      "epoch:10 step:8282 [D loss: 0.038814, acc.: 100.00%] [G loss: 3.771233]\n",
      "epoch:10 step:8283 [D loss: 0.053346, acc.: 99.22%] [G loss: 3.355660]\n",
      "epoch:10 step:8284 [D loss: 0.138744, acc.: 95.31%] [G loss: 3.205642]\n",
      "epoch:10 step:8285 [D loss: 0.299197, acc.: 88.28%] [G loss: 2.901910]\n",
      "epoch:10 step:8286 [D loss: 0.081877, acc.: 99.22%] [G loss: 4.878788]\n",
      "epoch:10 step:8287 [D loss: 0.068220, acc.: 99.22%] [G loss: 2.557706]\n",
      "epoch:10 step:8288 [D loss: 0.145619, acc.: 96.09%] [G loss: 2.923757]\n",
      "epoch:10 step:8289 [D loss: 0.106271, acc.: 98.44%] [G loss: 3.202184]\n",
      "epoch:10 step:8290 [D loss: 0.027455, acc.: 100.00%] [G loss: 2.953757]\n",
      "epoch:10 step:8291 [D loss: 0.082676, acc.: 97.66%] [G loss: 1.900738]\n",
      "epoch:10 step:8292 [D loss: 0.739695, acc.: 60.94%] [G loss: 6.799971]\n",
      "epoch:10 step:8293 [D loss: 0.984739, acc.: 63.28%] [G loss: 5.197364]\n",
      "epoch:10 step:8294 [D loss: 0.022559, acc.: 100.00%] [G loss: 3.097504]\n",
      "epoch:10 step:8295 [D loss: 0.014206, acc.: 100.00%] [G loss: 1.137725]\n",
      "epoch:10 step:8296 [D loss: 0.048394, acc.: 100.00%] [G loss: 1.683603]\n",
      "epoch:10 step:8297 [D loss: 0.038810, acc.: 100.00%] [G loss: 2.027609]\n",
      "epoch:10 step:8298 [D loss: 0.011527, acc.: 100.00%] [G loss: 1.919560]\n",
      "epoch:10 step:8299 [D loss: 0.029496, acc.: 100.00%] [G loss: 1.341450]\n",
      "epoch:10 step:8300 [D loss: 0.075877, acc.: 97.66%] [G loss: 1.550114]\n",
      "epoch:10 step:8301 [D loss: 0.108068, acc.: 96.88%] [G loss: 0.814275]\n",
      "epoch:10 step:8302 [D loss: 0.035152, acc.: 100.00%] [G loss: 1.291064]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:10 step:8303 [D loss: 0.190805, acc.: 93.75%] [G loss: 2.961164]\n",
      "epoch:10 step:8304 [D loss: 0.040257, acc.: 99.22%] [G loss: 3.967714]\n",
      "epoch:10 step:8305 [D loss: 0.173794, acc.: 91.41%] [G loss: 1.748948]\n",
      "epoch:10 step:8306 [D loss: 0.025964, acc.: 99.22%] [G loss: 2.003227]\n",
      "epoch:10 step:8307 [D loss: 0.076358, acc.: 99.22%] [G loss: 1.644614]\n",
      "epoch:10 step:8308 [D loss: 0.402613, acc.: 81.25%] [G loss: 5.630873]\n",
      "epoch:10 step:8309 [D loss: 1.361306, acc.: 54.69%] [G loss: 2.519586]\n",
      "epoch:10 step:8310 [D loss: 0.620953, acc.: 70.31%] [G loss: 6.099047]\n",
      "epoch:10 step:8311 [D loss: 1.367826, acc.: 52.34%] [G loss: 4.420900]\n",
      "epoch:10 step:8312 [D loss: 0.192801, acc.: 91.41%] [G loss: 3.265458]\n",
      "epoch:10 step:8313 [D loss: 0.085464, acc.: 99.22%] [G loss: 3.175487]\n",
      "epoch:10 step:8314 [D loss: 0.075861, acc.: 99.22%] [G loss: 3.252964]\n",
      "epoch:10 step:8315 [D loss: 0.021050, acc.: 100.00%] [G loss: 3.369205]\n",
      "epoch:10 step:8316 [D loss: 0.092311, acc.: 99.22%] [G loss: 4.058179]\n",
      "epoch:10 step:8317 [D loss: 0.046453, acc.: 100.00%] [G loss: 3.697524]\n",
      "epoch:10 step:8318 [D loss: 0.042008, acc.: 100.00%] [G loss: 2.943722]\n",
      "epoch:10 step:8319 [D loss: 0.120642, acc.: 96.88%] [G loss: 2.564618]\n",
      "epoch:10 step:8320 [D loss: 0.026705, acc.: 100.00%] [G loss: 2.885349]\n",
      "epoch:10 step:8321 [D loss: 0.134706, acc.: 96.88%] [G loss: 2.769797]\n",
      "epoch:10 step:8322 [D loss: 0.135700, acc.: 96.09%] [G loss: 1.980245]\n",
      "epoch:10 step:8323 [D loss: 0.257809, acc.: 88.28%] [G loss: 2.705560]\n",
      "epoch:10 step:8324 [D loss: 0.103434, acc.: 97.66%] [G loss: 3.332797]\n",
      "epoch:10 step:8325 [D loss: 0.065571, acc.: 99.22%] [G loss: 1.041747]\n",
      "epoch:10 step:8326 [D loss: 0.129529, acc.: 95.31%] [G loss: 1.763018]\n",
      "epoch:10 step:8327 [D loss: 0.016974, acc.: 100.00%] [G loss: 1.451787]\n",
      "epoch:10 step:8328 [D loss: 0.092332, acc.: 98.44%] [G loss: 0.530294]\n",
      "epoch:10 step:8329 [D loss: 0.141609, acc.: 96.88%] [G loss: 0.288885]\n",
      "epoch:10 step:8330 [D loss: 0.059768, acc.: 100.00%] [G loss: 0.570587]\n",
      "epoch:10 step:8331 [D loss: 0.293728, acc.: 84.38%] [G loss: 4.848650]\n",
      "epoch:10 step:8332 [D loss: 0.544808, acc.: 70.31%] [G loss: 2.341595]\n",
      "epoch:10 step:8333 [D loss: 0.159407, acc.: 93.75%] [G loss: 3.927202]\n",
      "epoch:10 step:8334 [D loss: 0.048685, acc.: 97.66%] [G loss: 3.195932]\n",
      "epoch:10 step:8335 [D loss: 0.076392, acc.: 98.44%] [G loss: 3.820576]\n",
      "epoch:10 step:8336 [D loss: 0.129813, acc.: 97.66%] [G loss: 2.776224]\n",
      "epoch:10 step:8337 [D loss: 0.173077, acc.: 94.53%] [G loss: 3.384615]\n",
      "epoch:10 step:8338 [D loss: 0.570733, acc.: 73.44%] [G loss: 1.995916]\n",
      "epoch:10 step:8339 [D loss: 0.107051, acc.: 96.09%] [G loss: 3.995449]\n",
      "epoch:10 step:8340 [D loss: 0.138305, acc.: 96.09%] [G loss: 3.055467]\n",
      "epoch:10 step:8341 [D loss: 0.340840, acc.: 82.03%] [G loss: 6.244752]\n",
      "epoch:10 step:8342 [D loss: 1.339173, acc.: 51.56%] [G loss: 2.724709]\n",
      "epoch:10 step:8343 [D loss: 0.263746, acc.: 86.72%] [G loss: 4.626888]\n",
      "epoch:10 step:8344 [D loss: 0.028003, acc.: 100.00%] [G loss: 4.739377]\n",
      "epoch:10 step:8345 [D loss: 0.168063, acc.: 93.75%] [G loss: 2.600580]\n",
      "epoch:10 step:8346 [D loss: 0.040598, acc.: 99.22%] [G loss: 2.196524]\n",
      "epoch:10 step:8347 [D loss: 0.108752, acc.: 96.09%] [G loss: 2.699258]\n",
      "epoch:10 step:8348 [D loss: 0.043432, acc.: 99.22%] [G loss: 3.024740]\n",
      "epoch:10 step:8349 [D loss: 0.059208, acc.: 99.22%] [G loss: 2.947638]\n",
      "epoch:10 step:8350 [D loss: 0.076733, acc.: 97.66%] [G loss: 2.812775]\n",
      "epoch:10 step:8351 [D loss: 0.035290, acc.: 100.00%] [G loss: 2.288877]\n",
      "epoch:10 step:8352 [D loss: 0.429226, acc.: 79.69%] [G loss: 4.447639]\n",
      "epoch:10 step:8353 [D loss: 0.045922, acc.: 99.22%] [G loss: 5.842649]\n",
      "epoch:10 step:8354 [D loss: 0.400291, acc.: 75.00%] [G loss: 1.224345]\n",
      "epoch:10 step:8355 [D loss: 0.540118, acc.: 71.09%] [G loss: 4.984529]\n",
      "epoch:10 step:8356 [D loss: 0.102616, acc.: 96.88%] [G loss: 5.954876]\n",
      "epoch:10 step:8357 [D loss: 0.628277, acc.: 70.31%] [G loss: 2.812368]\n",
      "epoch:10 step:8358 [D loss: 0.066783, acc.: 99.22%] [G loss: 1.002635]\n",
      "epoch:10 step:8359 [D loss: 0.082739, acc.: 96.88%] [G loss: 2.889457]\n",
      "epoch:10 step:8360 [D loss: 0.060799, acc.: 98.44%] [G loss: 3.317603]\n",
      "epoch:10 step:8361 [D loss: 0.030375, acc.: 100.00%] [G loss: 2.287662]\n",
      "epoch:10 step:8362 [D loss: 0.040145, acc.: 100.00%] [G loss: 2.720413]\n",
      "epoch:10 step:8363 [D loss: 0.020673, acc.: 100.00%] [G loss: 2.000268]\n",
      "epoch:10 step:8364 [D loss: 0.054363, acc.: 99.22%] [G loss: 1.053972]\n",
      "epoch:10 step:8365 [D loss: 0.029848, acc.: 100.00%] [G loss: 0.635976]\n",
      "epoch:10 step:8366 [D loss: 0.244345, acc.: 89.84%] [G loss: 3.344789]\n",
      "epoch:10 step:8367 [D loss: 0.213444, acc.: 89.84%] [G loss: 2.359432]\n",
      "epoch:10 step:8368 [D loss: 0.285019, acc.: 85.16%] [G loss: 0.338563]\n",
      "epoch:10 step:8369 [D loss: 0.720974, acc.: 69.53%] [G loss: 5.512418]\n",
      "epoch:10 step:8370 [D loss: 0.449580, acc.: 80.47%] [G loss: 4.834248]\n",
      "epoch:10 step:8371 [D loss: 0.090585, acc.: 99.22%] [G loss: 3.721386]\n",
      "epoch:10 step:8372 [D loss: 0.023508, acc.: 100.00%] [G loss: 2.927092]\n",
      "epoch:10 step:8373 [D loss: 0.057946, acc.: 100.00%] [G loss: 4.068067]\n",
      "epoch:10 step:8374 [D loss: 0.017672, acc.: 100.00%] [G loss: 4.597483]\n",
      "epoch:10 step:8375 [D loss: 0.073280, acc.: 99.22%] [G loss: 3.733072]\n",
      "epoch:10 step:8376 [D loss: 0.034080, acc.: 100.00%] [G loss: 3.366758]\n",
      "epoch:10 step:8377 [D loss: 0.078706, acc.: 100.00%] [G loss: 2.734986]\n",
      "epoch:10 step:8378 [D loss: 0.114347, acc.: 97.66%] [G loss: 3.964409]\n",
      "epoch:10 step:8379 [D loss: 0.042966, acc.: 99.22%] [G loss: 3.679217]\n",
      "epoch:10 step:8380 [D loss: 0.051234, acc.: 99.22%] [G loss: 3.514310]\n",
      "epoch:10 step:8381 [D loss: 0.155982, acc.: 94.53%] [G loss: 3.114002]\n",
      "epoch:10 step:8382 [D loss: 0.011961, acc.: 100.00%] [G loss: 4.101124]\n",
      "epoch:10 step:8383 [D loss: 0.203057, acc.: 90.62%] [G loss: 1.693823]\n",
      "epoch:10 step:8384 [D loss: 0.319504, acc.: 82.81%] [G loss: 4.648656]\n",
      "epoch:10 step:8385 [D loss: 0.660323, acc.: 64.84%] [G loss: 3.645784]\n",
      "epoch:10 step:8386 [D loss: 0.088658, acc.: 98.44%] [G loss: 2.965321]\n",
      "epoch:10 step:8387 [D loss: 0.082272, acc.: 99.22%] [G loss: 3.100029]\n",
      "epoch:10 step:8388 [D loss: 0.095395, acc.: 96.88%] [G loss: 3.946312]\n",
      "epoch:10 step:8389 [D loss: 0.050365, acc.: 99.22%] [G loss: 4.012366]\n",
      "epoch:10 step:8390 [D loss: 0.235921, acc.: 90.62%] [G loss: 1.806741]\n",
      "epoch:10 step:8391 [D loss: 0.300032, acc.: 85.16%] [G loss: 4.977521]\n",
      "epoch:10 step:8392 [D loss: 0.027598, acc.: 100.00%] [G loss: 6.203230]\n",
      "epoch:10 step:8393 [D loss: 0.334698, acc.: 83.59%] [G loss: 3.524559]\n",
      "epoch:10 step:8394 [D loss: 0.020307, acc.: 100.00%] [G loss: 3.050985]\n",
      "epoch:10 step:8395 [D loss: 0.029931, acc.: 100.00%] [G loss: 1.180475]\n",
      "epoch:10 step:8396 [D loss: 0.030083, acc.: 99.22%] [G loss: 0.975302]\n",
      "epoch:10 step:8397 [D loss: 0.019023, acc.: 100.00%] [G loss: 0.479231]\n",
      "epoch:10 step:8398 [D loss: 0.069638, acc.: 98.44%] [G loss: 1.986511]\n",
      "epoch:10 step:8399 [D loss: 0.038540, acc.: 100.00%] [G loss: 0.858604]\n",
      "epoch:10 step:8400 [D loss: 0.092521, acc.: 99.22%] [G loss: 0.197984]\n",
      "##############\n",
      "[0.82264861 0.94895449 0.99719925 0.90367007 2.10682679 0.94027753\n",
      " 1.08345201 2.11233879 2.10972107 0.99381471]\n",
      "##########\n",
      "epoch:10 step:8401 [D loss: 0.058979, acc.: 99.22%] [G loss: 0.454057]\n",
      "epoch:10 step:8402 [D loss: 0.071685, acc.: 98.44%] [G loss: 1.989520]\n",
      "epoch:10 step:8403 [D loss: 0.044776, acc.: 99.22%] [G loss: 1.805758]\n",
      "epoch:10 step:8404 [D loss: 0.184443, acc.: 96.09%] [G loss: 0.726535]\n",
      "epoch:10 step:8405 [D loss: 0.036488, acc.: 100.00%] [G loss: 1.204233]\n",
      "epoch:10 step:8406 [D loss: 0.104725, acc.: 96.88%] [G loss: 1.392092]\n",
      "epoch:10 step:8407 [D loss: 0.011735, acc.: 100.00%] [G loss: 1.123237]\n",
      "epoch:10 step:8408 [D loss: 0.152725, acc.: 95.31%] [G loss: 2.100445]\n",
      "epoch:10 step:8409 [D loss: 0.158711, acc.: 94.53%] [G loss: 1.348622]\n",
      "epoch:10 step:8410 [D loss: 0.620319, acc.: 65.62%] [G loss: 7.449086]\n",
      "epoch:10 step:8411 [D loss: 1.477627, acc.: 53.12%] [G loss: 3.105131]\n",
      "epoch:10 step:8412 [D loss: 0.197333, acc.: 93.75%] [G loss: 3.376460]\n",
      "epoch:10 step:8413 [D loss: 0.083407, acc.: 99.22%] [G loss: 3.375093]\n",
      "epoch:10 step:8414 [D loss: 0.302599, acc.: 86.72%] [G loss: 3.559594]\n",
      "epoch:10 step:8415 [D loss: 0.029759, acc.: 99.22%] [G loss: 3.526881]\n",
      "epoch:10 step:8416 [D loss: 0.106246, acc.: 95.31%] [G loss: 3.792593]\n",
      "epoch:10 step:8417 [D loss: 0.238727, acc.: 91.41%] [G loss: 2.714716]\n",
      "epoch:10 step:8418 [D loss: 0.031593, acc.: 99.22%] [G loss: 2.713419]\n",
      "epoch:10 step:8419 [D loss: 0.057555, acc.: 100.00%] [G loss: 1.978920]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:10 step:8420 [D loss: 0.157970, acc.: 93.75%] [G loss: 2.913564]\n",
      "epoch:10 step:8421 [D loss: 0.069172, acc.: 100.00%] [G loss: 2.828822]\n",
      "epoch:10 step:8422 [D loss: 0.144415, acc.: 95.31%] [G loss: 1.016176]\n",
      "epoch:10 step:8423 [D loss: 0.561997, acc.: 72.66%] [G loss: 5.347671]\n",
      "epoch:10 step:8424 [D loss: 0.246754, acc.: 87.50%] [G loss: 6.118139]\n",
      "epoch:10 step:8425 [D loss: 0.240639, acc.: 87.50%] [G loss: 1.621934]\n",
      "epoch:10 step:8426 [D loss: 0.106034, acc.: 96.88%] [G loss: 2.360636]\n",
      "epoch:10 step:8427 [D loss: 0.018248, acc.: 100.00%] [G loss: 2.920435]\n",
      "epoch:10 step:8428 [D loss: 0.181634, acc.: 92.97%] [G loss: 1.516941]\n",
      "epoch:10 step:8429 [D loss: 0.056131, acc.: 100.00%] [G loss: 2.631514]\n",
      "epoch:10 step:8430 [D loss: 0.016049, acc.: 100.00%] [G loss: 2.129629]\n",
      "epoch:10 step:8431 [D loss: 0.014963, acc.: 100.00%] [G loss: 1.248700]\n",
      "epoch:10 step:8432 [D loss: 0.085466, acc.: 97.66%] [G loss: 2.566190]\n",
      "epoch:10 step:8433 [D loss: 0.516140, acc.: 72.66%] [G loss: 2.119560]\n",
      "epoch:10 step:8434 [D loss: 0.040874, acc.: 100.00%] [G loss: 2.785155]\n",
      "epoch:10 step:8435 [D loss: 0.180010, acc.: 94.53%] [G loss: 1.702392]\n",
      "epoch:10 step:8436 [D loss: 0.128975, acc.: 96.09%] [G loss: 2.464321]\n",
      "epoch:10 step:8437 [D loss: 0.146568, acc.: 95.31%] [G loss: 2.106673]\n",
      "epoch:10 step:8438 [D loss: 0.279008, acc.: 88.28%] [G loss: 4.064527]\n",
      "epoch:10 step:8439 [D loss: 0.130732, acc.: 95.31%] [G loss: 4.904402]\n",
      "epoch:10 step:8440 [D loss: 0.074241, acc.: 97.66%] [G loss: 3.887046]\n",
      "epoch:10 step:8441 [D loss: 0.205001, acc.: 92.97%] [G loss: 5.160200]\n",
      "epoch:10 step:8442 [D loss: 0.026223, acc.: 99.22%] [G loss: 5.599602]\n",
      "epoch:10 step:8443 [D loss: 0.430588, acc.: 73.44%] [G loss: 2.510838]\n",
      "epoch:10 step:8444 [D loss: 0.352727, acc.: 84.38%] [G loss: 5.651399]\n",
      "epoch:10 step:8445 [D loss: 0.018173, acc.: 100.00%] [G loss: 6.246506]\n",
      "epoch:10 step:8446 [D loss: 0.248393, acc.: 87.50%] [G loss: 4.622676]\n",
      "epoch:10 step:8447 [D loss: 0.103923, acc.: 96.09%] [G loss: 4.334645]\n",
      "epoch:10 step:8448 [D loss: 0.008994, acc.: 100.00%] [G loss: 4.295364]\n",
      "epoch:10 step:8449 [D loss: 0.022903, acc.: 100.00%] [G loss: 4.410042]\n",
      "epoch:10 step:8450 [D loss: 0.021361, acc.: 100.00%] [G loss: 4.493085]\n",
      "epoch:10 step:8451 [D loss: 0.032730, acc.: 100.00%] [G loss: 4.374028]\n",
      "epoch:10 step:8452 [D loss: 0.184882, acc.: 92.97%] [G loss: 1.379337]\n",
      "epoch:10 step:8453 [D loss: 0.101050, acc.: 98.44%] [G loss: 3.289881]\n",
      "epoch:10 step:8454 [D loss: 0.012330, acc.: 100.00%] [G loss: 3.980259]\n",
      "epoch:10 step:8455 [D loss: 0.064687, acc.: 98.44%] [G loss: 2.212295]\n",
      "epoch:10 step:8456 [D loss: 0.469530, acc.: 74.22%] [G loss: 6.637729]\n",
      "epoch:10 step:8457 [D loss: 1.123394, acc.: 53.91%] [G loss: 3.486096]\n",
      "epoch:10 step:8458 [D loss: 0.199493, acc.: 92.97%] [G loss: 3.477838]\n",
      "epoch:10 step:8459 [D loss: 0.039704, acc.: 100.00%] [G loss: 4.611697]\n",
      "epoch:10 step:8460 [D loss: 0.110148, acc.: 96.09%] [G loss: 3.500828]\n",
      "epoch:10 step:8461 [D loss: 0.025230, acc.: 100.00%] [G loss: 3.284926]\n",
      "epoch:10 step:8462 [D loss: 0.053692, acc.: 100.00%] [G loss: 2.996169]\n",
      "epoch:10 step:8463 [D loss: 0.057260, acc.: 99.22%] [G loss: 2.602226]\n",
      "epoch:10 step:8464 [D loss: 0.064858, acc.: 100.00%] [G loss: 3.442070]\n",
      "epoch:10 step:8465 [D loss: 0.153352, acc.: 94.53%] [G loss: 4.021547]\n",
      "epoch:10 step:8466 [D loss: 0.878826, acc.: 55.47%] [G loss: 4.548100]\n",
      "epoch:10 step:8467 [D loss: 0.020556, acc.: 100.00%] [G loss: 5.981416]\n",
      "epoch:10 step:8468 [D loss: 0.296248, acc.: 85.94%] [G loss: 1.203074]\n",
      "epoch:10 step:8469 [D loss: 1.152622, acc.: 60.94%] [G loss: 7.342050]\n",
      "epoch:10 step:8470 [D loss: 1.070693, acc.: 53.91%] [G loss: 4.678908]\n",
      "epoch:10 step:8471 [D loss: 0.096629, acc.: 96.88%] [G loss: 3.651757]\n",
      "epoch:10 step:8472 [D loss: 0.178663, acc.: 93.75%] [G loss: 2.886940]\n",
      "epoch:10 step:8473 [D loss: 0.063397, acc.: 99.22%] [G loss: 3.580890]\n",
      "epoch:10 step:8474 [D loss: 0.103245, acc.: 97.66%] [G loss: 2.326382]\n",
      "epoch:10 step:8475 [D loss: 0.511216, acc.: 80.47%] [G loss: 5.471123]\n",
      "epoch:10 step:8476 [D loss: 0.115783, acc.: 96.88%] [G loss: 5.739973]\n",
      "epoch:10 step:8477 [D loss: 0.282246, acc.: 89.06%] [G loss: 2.727872]\n",
      "epoch:10 step:8478 [D loss: 0.176352, acc.: 92.97%] [G loss: 3.557149]\n",
      "epoch:10 step:8479 [D loss: 0.049081, acc.: 99.22%] [G loss: 3.749601]\n",
      "epoch:10 step:8480 [D loss: 0.362558, acc.: 84.38%] [G loss: 2.097296]\n",
      "epoch:10 step:8481 [D loss: 0.081734, acc.: 97.66%] [G loss: 3.180059]\n",
      "epoch:10 step:8482 [D loss: 0.040329, acc.: 100.00%] [G loss: 3.657318]\n",
      "epoch:10 step:8483 [D loss: 0.215180, acc.: 93.75%] [G loss: 2.135022]\n",
      "epoch:10 step:8484 [D loss: 0.059677, acc.: 99.22%] [G loss: 3.627309]\n",
      "epoch:10 step:8485 [D loss: 0.034325, acc.: 100.00%] [G loss: 3.548354]\n",
      "epoch:10 step:8486 [D loss: 0.072711, acc.: 99.22%] [G loss: 2.364502]\n",
      "epoch:10 step:8487 [D loss: 0.056464, acc.: 100.00%] [G loss: 2.650346]\n",
      "epoch:10 step:8488 [D loss: 0.550337, acc.: 75.00%] [G loss: 4.770833]\n",
      "epoch:10 step:8489 [D loss: 0.364661, acc.: 82.03%] [G loss: 3.775259]\n",
      "epoch:10 step:8490 [D loss: 0.082499, acc.: 98.44%] [G loss: 3.172969]\n",
      "epoch:10 step:8491 [D loss: 0.251630, acc.: 92.97%] [G loss: 5.181424]\n",
      "epoch:10 step:8492 [D loss: 0.053754, acc.: 98.44%] [G loss: 4.858113]\n",
      "epoch:10 step:8493 [D loss: 0.275849, acc.: 89.84%] [G loss: 3.129945]\n",
      "epoch:10 step:8494 [D loss: 0.073924, acc.: 97.66%] [G loss: 4.222472]\n",
      "epoch:10 step:8495 [D loss: 0.134429, acc.: 94.53%] [G loss: 2.868852]\n",
      "epoch:10 step:8496 [D loss: 0.067140, acc.: 98.44%] [G loss: 3.575900]\n",
      "epoch:10 step:8497 [D loss: 0.143387, acc.: 95.31%] [G loss: 4.440471]\n",
      "epoch:10 step:8498 [D loss: 0.104926, acc.: 96.88%] [G loss: 3.438104]\n",
      "epoch:10 step:8499 [D loss: 0.088825, acc.: 98.44%] [G loss: 4.074885]\n",
      "epoch:10 step:8500 [D loss: 0.198657, acc.: 92.97%] [G loss: 4.336797]\n",
      "epoch:10 step:8501 [D loss: 0.039613, acc.: 100.00%] [G loss: 4.268120]\n",
      "epoch:10 step:8502 [D loss: 0.419918, acc.: 82.81%] [G loss: 4.509067]\n",
      "epoch:10 step:8503 [D loss: 0.111857, acc.: 96.09%] [G loss: 4.231187]\n",
      "epoch:10 step:8504 [D loss: 0.030618, acc.: 99.22%] [G loss: 3.023239]\n",
      "epoch:10 step:8505 [D loss: 0.029663, acc.: 99.22%] [G loss: 3.530245]\n",
      "epoch:10 step:8506 [D loss: 0.030699, acc.: 100.00%] [G loss: 2.529031]\n",
      "epoch:10 step:8507 [D loss: 0.016316, acc.: 100.00%] [G loss: 2.223176]\n",
      "epoch:10 step:8508 [D loss: 0.080747, acc.: 98.44%] [G loss: 2.031082]\n",
      "epoch:10 step:8509 [D loss: 0.080448, acc.: 99.22%] [G loss: 2.151708]\n",
      "epoch:10 step:8510 [D loss: 0.325401, acc.: 85.16%] [G loss: 6.325844]\n",
      "epoch:10 step:8511 [D loss: 1.237788, acc.: 56.25%] [G loss: 1.149439]\n",
      "epoch:10 step:8512 [D loss: 0.608686, acc.: 71.88%] [G loss: 6.281678]\n",
      "epoch:10 step:8513 [D loss: 0.811095, acc.: 60.94%] [G loss: 4.184330]\n",
      "epoch:10 step:8514 [D loss: 0.024306, acc.: 100.00%] [G loss: 2.802807]\n",
      "epoch:10 step:8515 [D loss: 0.106098, acc.: 97.66%] [G loss: 4.139746]\n",
      "epoch:10 step:8516 [D loss: 0.007750, acc.: 100.00%] [G loss: 3.776608]\n",
      "epoch:10 step:8517 [D loss: 0.038338, acc.: 100.00%] [G loss: 3.166615]\n",
      "epoch:10 step:8518 [D loss: 0.046207, acc.: 100.00%] [G loss: 3.015420]\n",
      "epoch:10 step:8519 [D loss: 0.125156, acc.: 96.09%] [G loss: 2.116888]\n",
      "epoch:10 step:8520 [D loss: 0.189143, acc.: 90.62%] [G loss: 3.999843]\n",
      "epoch:10 step:8521 [D loss: 0.284970, acc.: 89.06%] [G loss: 3.045525]\n",
      "epoch:10 step:8522 [D loss: 0.176815, acc.: 92.97%] [G loss: 4.020164]\n",
      "epoch:10 step:8523 [D loss: 0.517794, acc.: 75.00%] [G loss: 2.733328]\n",
      "epoch:10 step:8524 [D loss: 0.115396, acc.: 98.44%] [G loss: 2.919782]\n",
      "epoch:10 step:8525 [D loss: 0.113343, acc.: 98.44%] [G loss: 2.686656]\n",
      "epoch:10 step:8526 [D loss: 0.078148, acc.: 98.44%] [G loss: 2.787769]\n",
      "epoch:10 step:8527 [D loss: 0.039906, acc.: 100.00%] [G loss: 2.453647]\n",
      "epoch:10 step:8528 [D loss: 0.175681, acc.: 96.09%] [G loss: 4.301947]\n",
      "epoch:10 step:8529 [D loss: 0.235313, acc.: 89.84%] [G loss: 2.975633]\n",
      "epoch:10 step:8530 [D loss: 0.045415, acc.: 100.00%] [G loss: 1.474934]\n",
      "epoch:10 step:8531 [D loss: 0.078805, acc.: 99.22%] [G loss: 1.004826]\n",
      "epoch:10 step:8532 [D loss: 0.021266, acc.: 100.00%] [G loss: 1.363658]\n",
      "epoch:10 step:8533 [D loss: 0.131839, acc.: 96.88%] [G loss: 2.948749]\n",
      "epoch:10 step:8534 [D loss: 0.174369, acc.: 94.53%] [G loss: 0.418200]\n",
      "epoch:10 step:8535 [D loss: 0.243225, acc.: 92.97%] [G loss: 0.404908]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:10 step:8536 [D loss: 0.018331, acc.: 100.00%] [G loss: 1.045289]\n",
      "epoch:10 step:8537 [D loss: 0.146726, acc.: 96.88%] [G loss: 2.598873]\n",
      "epoch:10 step:8538 [D loss: 0.190086, acc.: 95.31%] [G loss: 4.654838]\n",
      "epoch:10 step:8539 [D loss: 0.978902, acc.: 57.81%] [G loss: 7.057252]\n",
      "epoch:10 step:8540 [D loss: 0.534725, acc.: 73.44%] [G loss: 4.726116]\n",
      "epoch:10 step:8541 [D loss: 0.151586, acc.: 93.75%] [G loss: 2.641575]\n",
      "epoch:10 step:8542 [D loss: 0.064505, acc.: 98.44%] [G loss: 3.915369]\n",
      "epoch:10 step:8543 [D loss: 0.028931, acc.: 100.00%] [G loss: 3.748610]\n",
      "epoch:10 step:8544 [D loss: 0.079298, acc.: 98.44%] [G loss: 4.742885]\n",
      "epoch:10 step:8545 [D loss: 0.269739, acc.: 90.62%] [G loss: 3.034884]\n",
      "epoch:10 step:8546 [D loss: 0.112081, acc.: 96.88%] [G loss: 4.831963]\n",
      "epoch:10 step:8547 [D loss: 0.058040, acc.: 97.66%] [G loss: 4.596220]\n",
      "epoch:10 step:8548 [D loss: 0.101800, acc.: 96.88%] [G loss: 4.047396]\n",
      "epoch:10 step:8549 [D loss: 0.124750, acc.: 95.31%] [G loss: 2.897392]\n",
      "epoch:10 step:8550 [D loss: 0.062056, acc.: 98.44%] [G loss: 3.730387]\n",
      "epoch:10 step:8551 [D loss: 0.094379, acc.: 99.22%] [G loss: 3.650893]\n",
      "epoch:10 step:8552 [D loss: 0.040374, acc.: 100.00%] [G loss: 2.232258]\n",
      "epoch:10 step:8553 [D loss: 0.041426, acc.: 99.22%] [G loss: 2.492912]\n",
      "epoch:10 step:8554 [D loss: 0.082570, acc.: 99.22%] [G loss: 4.770516]\n",
      "epoch:10 step:8555 [D loss: 0.052037, acc.: 98.44%] [G loss: 3.018922]\n",
      "epoch:10 step:8556 [D loss: 0.523427, acc.: 73.44%] [G loss: 6.009803]\n",
      "epoch:10 step:8557 [D loss: 1.235459, acc.: 52.34%] [G loss: 1.395706]\n",
      "epoch:10 step:8558 [D loss: 0.112469, acc.: 97.66%] [G loss: 4.299168]\n",
      "epoch:10 step:8559 [D loss: 0.012737, acc.: 100.00%] [G loss: 3.551902]\n",
      "epoch:10 step:8560 [D loss: 0.072428, acc.: 99.22%] [G loss: 2.721912]\n",
      "epoch:10 step:8561 [D loss: 0.025418, acc.: 99.22%] [G loss: 2.090658]\n",
      "epoch:10 step:8562 [D loss: 0.102681, acc.: 96.09%] [G loss: 3.221093]\n",
      "epoch:10 step:8563 [D loss: 0.926511, acc.: 53.91%] [G loss: 6.376461]\n",
      "epoch:10 step:8564 [D loss: 1.469066, acc.: 50.78%] [G loss: 3.664219]\n",
      "epoch:10 step:8565 [D loss: 0.542527, acc.: 71.09%] [G loss: 5.010904]\n",
      "epoch:10 step:8566 [D loss: 0.522276, acc.: 77.34%] [G loss: 4.390365]\n",
      "epoch:10 step:8567 [D loss: 0.105531, acc.: 97.66%] [G loss: 3.647279]\n",
      "epoch:10 step:8568 [D loss: 0.103358, acc.: 97.66%] [G loss: 3.618759]\n",
      "epoch:10 step:8569 [D loss: 0.029373, acc.: 100.00%] [G loss: 3.907265]\n",
      "epoch:10 step:8570 [D loss: 0.074219, acc.: 98.44%] [G loss: 3.197978]\n",
      "epoch:10 step:8571 [D loss: 0.040604, acc.: 100.00%] [G loss: 2.604196]\n",
      "epoch:10 step:8572 [D loss: 0.117411, acc.: 99.22%] [G loss: 3.075077]\n",
      "epoch:10 step:8573 [D loss: 0.097493, acc.: 98.44%] [G loss: 3.905664]\n",
      "epoch:10 step:8574 [D loss: 0.137050, acc.: 97.66%] [G loss: 4.758694]\n",
      "epoch:10 step:8575 [D loss: 0.208893, acc.: 91.41%] [G loss: 3.230316]\n",
      "epoch:10 step:8576 [D loss: 0.161349, acc.: 95.31%] [G loss: 3.770854]\n",
      "epoch:10 step:8577 [D loss: 0.107711, acc.: 96.88%] [G loss: 3.463223]\n",
      "epoch:10 step:8578 [D loss: 0.110363, acc.: 97.66%] [G loss: 2.290658]\n",
      "epoch:10 step:8579 [D loss: 0.124112, acc.: 95.31%] [G loss: 3.378015]\n",
      "epoch:10 step:8580 [D loss: 0.114074, acc.: 96.88%] [G loss: 4.608688]\n",
      "epoch:10 step:8581 [D loss: 0.079621, acc.: 98.44%] [G loss: 4.495569]\n",
      "epoch:10 step:8582 [D loss: 0.076033, acc.: 98.44%] [G loss: 3.232048]\n",
      "epoch:10 step:8583 [D loss: 0.161754, acc.: 92.19%] [G loss: 4.345081]\n",
      "epoch:10 step:8584 [D loss: 0.118267, acc.: 95.31%] [G loss: 3.968004]\n",
      "epoch:10 step:8585 [D loss: 0.026471, acc.: 100.00%] [G loss: 3.068662]\n",
      "epoch:10 step:8586 [D loss: 0.085569, acc.: 97.66%] [G loss: 4.124178]\n",
      "epoch:10 step:8587 [D loss: 0.099118, acc.: 98.44%] [G loss: 2.892506]\n",
      "epoch:10 step:8588 [D loss: 0.029944, acc.: 100.00%] [G loss: 1.681424]\n",
      "epoch:10 step:8589 [D loss: 1.446850, acc.: 40.62%] [G loss: 6.339513]\n",
      "epoch:10 step:8590 [D loss: 1.901281, acc.: 50.00%] [G loss: 2.912783]\n",
      "epoch:10 step:8591 [D loss: 0.470045, acc.: 75.78%] [G loss: 2.617737]\n",
      "epoch:11 step:8592 [D loss: 0.452088, acc.: 78.91%] [G loss: 2.134932]\n",
      "epoch:11 step:8593 [D loss: 0.182993, acc.: 95.31%] [G loss: 2.631000]\n",
      "epoch:11 step:8594 [D loss: 0.130582, acc.: 96.88%] [G loss: 1.947483]\n",
      "epoch:11 step:8595 [D loss: 0.212161, acc.: 93.75%] [G loss: 2.033154]\n",
      "epoch:11 step:8596 [D loss: 0.268267, acc.: 89.84%] [G loss: 2.861077]\n",
      "epoch:11 step:8597 [D loss: 0.152653, acc.: 94.53%] [G loss: 2.314844]\n",
      "epoch:11 step:8598 [D loss: 0.529029, acc.: 75.00%] [G loss: 5.016754]\n",
      "epoch:11 step:8599 [D loss: 0.837857, acc.: 56.25%] [G loss: 3.800088]\n",
      "epoch:11 step:8600 [D loss: 0.203510, acc.: 93.75%] [G loss: 2.340584]\n",
      "##############\n",
      "[0.9063396  1.0158425  0.94098886 0.85861947 0.99790753 2.10889997\n",
      " 0.93411634 0.83170164 1.10058311 2.11613664]\n",
      "##########\n",
      "epoch:11 step:8601 [D loss: 0.153631, acc.: 95.31%] [G loss: 2.260276]\n",
      "epoch:11 step:8602 [D loss: 0.102688, acc.: 98.44%] [G loss: 2.826598]\n",
      "epoch:11 step:8603 [D loss: 0.229021, acc.: 92.97%] [G loss: 1.722351]\n",
      "epoch:11 step:8604 [D loss: 0.159259, acc.: 95.31%] [G loss: 2.681306]\n",
      "epoch:11 step:8605 [D loss: 0.053393, acc.: 100.00%] [G loss: 2.316331]\n",
      "epoch:11 step:8606 [D loss: 0.318526, acc.: 87.50%] [G loss: 2.177714]\n",
      "epoch:11 step:8607 [D loss: 0.137723, acc.: 95.31%] [G loss: 2.592484]\n",
      "epoch:11 step:8608 [D loss: 0.037978, acc.: 99.22%] [G loss: 1.870724]\n",
      "epoch:11 step:8609 [D loss: 0.222874, acc.: 91.41%] [G loss: 3.586915]\n",
      "epoch:11 step:8610 [D loss: 0.045023, acc.: 100.00%] [G loss: 2.318310]\n",
      "epoch:11 step:8611 [D loss: 0.039717, acc.: 100.00%] [G loss: 1.967035]\n",
      "epoch:11 step:8612 [D loss: 0.576534, acc.: 71.09%] [G loss: 5.935794]\n",
      "epoch:11 step:8613 [D loss: 1.246070, acc.: 53.12%] [G loss: 3.919277]\n",
      "epoch:11 step:8614 [D loss: 0.170951, acc.: 94.53%] [G loss: 2.264685]\n",
      "epoch:11 step:8615 [D loss: 0.199676, acc.: 92.97%] [G loss: 3.142457]\n",
      "epoch:11 step:8616 [D loss: 0.027332, acc.: 100.00%] [G loss: 3.620772]\n",
      "epoch:11 step:8617 [D loss: 0.054769, acc.: 100.00%] [G loss: 3.517837]\n",
      "epoch:11 step:8618 [D loss: 0.044603, acc.: 100.00%] [G loss: 3.783475]\n",
      "epoch:11 step:8619 [D loss: 0.245846, acc.: 94.53%] [G loss: 2.113972]\n",
      "epoch:11 step:8620 [D loss: 0.287826, acc.: 85.94%] [G loss: 4.250560]\n",
      "epoch:11 step:8621 [D loss: 0.108751, acc.: 96.88%] [G loss: 3.934626]\n",
      "epoch:11 step:8622 [D loss: 0.297980, acc.: 86.72%] [G loss: 2.849617]\n",
      "epoch:11 step:8623 [D loss: 0.136278, acc.: 95.31%] [G loss: 2.243911]\n",
      "epoch:11 step:8624 [D loss: 0.085143, acc.: 99.22%] [G loss: 4.070169]\n",
      "epoch:11 step:8625 [D loss: 0.017754, acc.: 100.00%] [G loss: 4.159265]\n",
      "epoch:11 step:8626 [D loss: 0.035257, acc.: 100.00%] [G loss: 3.935810]\n",
      "epoch:11 step:8627 [D loss: 0.147001, acc.: 96.88%] [G loss: 3.587014]\n",
      "epoch:11 step:8628 [D loss: 0.055432, acc.: 100.00%] [G loss: 3.918643]\n",
      "epoch:11 step:8629 [D loss: 0.173437, acc.: 92.19%] [G loss: 3.255127]\n",
      "epoch:11 step:8630 [D loss: 0.131324, acc.: 98.44%] [G loss: 3.874000]\n",
      "epoch:11 step:8631 [D loss: 0.017761, acc.: 100.00%] [G loss: 4.344469]\n",
      "epoch:11 step:8632 [D loss: 0.043990, acc.: 100.00%] [G loss: 3.433460]\n",
      "epoch:11 step:8633 [D loss: 0.060351, acc.: 98.44%] [G loss: 3.317117]\n",
      "epoch:11 step:8634 [D loss: 0.058509, acc.: 99.22%] [G loss: 3.808786]\n",
      "epoch:11 step:8635 [D loss: 0.107588, acc.: 96.88%] [G loss: 2.461498]\n",
      "epoch:11 step:8636 [D loss: 0.285699, acc.: 87.50%] [G loss: 4.869823]\n",
      "epoch:11 step:8637 [D loss: 0.294557, acc.: 84.38%] [G loss: 3.346341]\n",
      "epoch:11 step:8638 [D loss: 0.071283, acc.: 99.22%] [G loss: 3.568357]\n",
      "epoch:11 step:8639 [D loss: 0.022187, acc.: 100.00%] [G loss: 3.182651]\n",
      "epoch:11 step:8640 [D loss: 0.031915, acc.: 99.22%] [G loss: 3.488111]\n",
      "epoch:11 step:8641 [D loss: 0.139088, acc.: 97.66%] [G loss: 3.606449]\n",
      "epoch:11 step:8642 [D loss: 0.047776, acc.: 99.22%] [G loss: 3.522544]\n",
      "epoch:11 step:8643 [D loss: 0.075505, acc.: 99.22%] [G loss: 3.131524]\n",
      "epoch:11 step:8644 [D loss: 0.029609, acc.: 100.00%] [G loss: 3.013935]\n",
      "epoch:11 step:8645 [D loss: 0.244669, acc.: 93.75%] [G loss: 4.081648]\n",
      "epoch:11 step:8646 [D loss: 0.074692, acc.: 98.44%] [G loss: 4.298734]\n",
      "epoch:11 step:8647 [D loss: 0.121021, acc.: 100.00%] [G loss: 2.498770]\n",
      "epoch:11 step:8648 [D loss: 0.074189, acc.: 100.00%] [G loss: 2.926765]\n",
      "epoch:11 step:8649 [D loss: 0.080699, acc.: 97.66%] [G loss: 3.713032]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:11 step:8650 [D loss: 0.090905, acc.: 98.44%] [G loss: 2.840816]\n",
      "epoch:11 step:8651 [D loss: 0.026391, acc.: 99.22%] [G loss: 3.791540]\n",
      "epoch:11 step:8652 [D loss: 0.129418, acc.: 96.88%] [G loss: 1.655406]\n",
      "epoch:11 step:8653 [D loss: 0.223978, acc.: 91.41%] [G loss: 4.945556]\n",
      "epoch:11 step:8654 [D loss: 0.510159, acc.: 74.22%] [G loss: 1.285829]\n",
      "epoch:11 step:8655 [D loss: 0.672464, acc.: 73.44%] [G loss: 6.570219]\n",
      "epoch:11 step:8656 [D loss: 1.534014, acc.: 50.00%] [G loss: 4.685221]\n",
      "epoch:11 step:8657 [D loss: 0.106460, acc.: 95.31%] [G loss: 3.112004]\n",
      "epoch:11 step:8658 [D loss: 0.076007, acc.: 97.66%] [G loss: 3.296401]\n",
      "epoch:11 step:8659 [D loss: 0.029493, acc.: 99.22%] [G loss: 2.347615]\n",
      "epoch:11 step:8660 [D loss: 0.156120, acc.: 95.31%] [G loss: 2.257929]\n",
      "epoch:11 step:8661 [D loss: 0.045904, acc.: 100.00%] [G loss: 3.529611]\n",
      "epoch:11 step:8662 [D loss: 0.149888, acc.: 96.88%] [G loss: 3.624117]\n",
      "epoch:11 step:8663 [D loss: 0.107136, acc.: 98.44%] [G loss: 3.290839]\n",
      "epoch:11 step:8664 [D loss: 0.034377, acc.: 100.00%] [G loss: 2.833489]\n",
      "epoch:11 step:8665 [D loss: 0.062314, acc.: 98.44%] [G loss: 3.731185]\n",
      "epoch:11 step:8666 [D loss: 0.071541, acc.: 98.44%] [G loss: 2.932560]\n",
      "epoch:11 step:8667 [D loss: 0.077099, acc.: 100.00%] [G loss: 2.450166]\n",
      "epoch:11 step:8668 [D loss: 0.297409, acc.: 88.28%] [G loss: 3.640078]\n",
      "epoch:11 step:8669 [D loss: 0.105187, acc.: 96.88%] [G loss: 3.191460]\n",
      "epoch:11 step:8670 [D loss: 0.372512, acc.: 83.59%] [G loss: 6.230133]\n",
      "epoch:11 step:8671 [D loss: 0.413360, acc.: 78.91%] [G loss: 3.678948]\n",
      "epoch:11 step:8672 [D loss: 0.013559, acc.: 100.00%] [G loss: 2.971878]\n",
      "epoch:11 step:8673 [D loss: 0.031873, acc.: 100.00%] [G loss: 3.215845]\n",
      "epoch:11 step:8674 [D loss: 0.038986, acc.: 100.00%] [G loss: 3.676596]\n",
      "epoch:11 step:8675 [D loss: 0.037258, acc.: 100.00%] [G loss: 3.698054]\n",
      "epoch:11 step:8676 [D loss: 0.083841, acc.: 98.44%] [G loss: 2.462475]\n",
      "epoch:11 step:8677 [D loss: 0.113599, acc.: 97.66%] [G loss: 3.763085]\n",
      "epoch:11 step:8678 [D loss: 0.007372, acc.: 100.00%] [G loss: 4.006870]\n",
      "epoch:11 step:8679 [D loss: 0.361085, acc.: 82.03%] [G loss: 0.633783]\n",
      "epoch:11 step:8680 [D loss: 0.860277, acc.: 60.94%] [G loss: 7.472281]\n",
      "epoch:11 step:8681 [D loss: 2.474051, acc.: 50.00%] [G loss: 5.753294]\n",
      "epoch:11 step:8682 [D loss: 1.684996, acc.: 50.00%] [G loss: 2.921190]\n",
      "epoch:11 step:8683 [D loss: 0.307147, acc.: 87.50%] [G loss: 2.231128]\n",
      "epoch:11 step:8684 [D loss: 0.112629, acc.: 98.44%] [G loss: 2.635493]\n",
      "epoch:11 step:8685 [D loss: 0.070465, acc.: 99.22%] [G loss: 2.834433]\n",
      "epoch:11 step:8686 [D loss: 0.049594, acc.: 100.00%] [G loss: 2.919267]\n",
      "epoch:11 step:8687 [D loss: 0.064337, acc.: 100.00%] [G loss: 2.290819]\n",
      "epoch:11 step:8688 [D loss: 0.098163, acc.: 98.44%] [G loss: 2.619204]\n",
      "epoch:11 step:8689 [D loss: 0.147345, acc.: 98.44%] [G loss: 2.561164]\n",
      "epoch:11 step:8690 [D loss: 0.104707, acc.: 100.00%] [G loss: 2.313120]\n",
      "epoch:11 step:8691 [D loss: 0.086968, acc.: 99.22%] [G loss: 3.054826]\n",
      "epoch:11 step:8692 [D loss: 0.092724, acc.: 99.22%] [G loss: 2.481099]\n",
      "epoch:11 step:8693 [D loss: 0.040274, acc.: 100.00%] [G loss: 3.106343]\n",
      "epoch:11 step:8694 [D loss: 0.049590, acc.: 100.00%] [G loss: 2.543445]\n",
      "epoch:11 step:8695 [D loss: 0.066576, acc.: 99.22%] [G loss: 2.629073]\n",
      "epoch:11 step:8696 [D loss: 0.060371, acc.: 100.00%] [G loss: 2.768763]\n",
      "epoch:11 step:8697 [D loss: 0.041398, acc.: 100.00%] [G loss: 2.911129]\n",
      "epoch:11 step:8698 [D loss: 0.138792, acc.: 97.66%] [G loss: 2.483298]\n",
      "epoch:11 step:8699 [D loss: 0.137029, acc.: 96.09%] [G loss: 2.637280]\n",
      "epoch:11 step:8700 [D loss: 0.027907, acc.: 100.00%] [G loss: 3.139774]\n",
      "epoch:11 step:8701 [D loss: 0.102935, acc.: 97.66%] [G loss: 2.477241]\n",
      "epoch:11 step:8702 [D loss: 0.076501, acc.: 98.44%] [G loss: 2.081474]\n",
      "epoch:11 step:8703 [D loss: 0.076094, acc.: 98.44%] [G loss: 2.495406]\n",
      "epoch:11 step:8704 [D loss: 0.028526, acc.: 100.00%] [G loss: 3.209950]\n",
      "epoch:11 step:8705 [D loss: 0.036432, acc.: 100.00%] [G loss: 3.053436]\n",
      "epoch:11 step:8706 [D loss: 0.076465, acc.: 100.00%] [G loss: 2.079941]\n",
      "epoch:11 step:8707 [D loss: 0.233016, acc.: 92.19%] [G loss: 2.703855]\n",
      "epoch:11 step:8708 [D loss: 0.019125, acc.: 100.00%] [G loss: 3.798460]\n",
      "epoch:11 step:8709 [D loss: 0.022993, acc.: 100.00%] [G loss: 4.031656]\n",
      "epoch:11 step:8710 [D loss: 0.018447, acc.: 100.00%] [G loss: 3.542702]\n",
      "epoch:11 step:8711 [D loss: 0.030168, acc.: 100.00%] [G loss: 3.636501]\n",
      "epoch:11 step:8712 [D loss: 0.014621, acc.: 100.00%] [G loss: 3.576894]\n",
      "epoch:11 step:8713 [D loss: 0.021134, acc.: 100.00%] [G loss: 3.744544]\n",
      "epoch:11 step:8714 [D loss: 0.074041, acc.: 97.66%] [G loss: 3.348958]\n",
      "epoch:11 step:8715 [D loss: 0.094359, acc.: 96.88%] [G loss: 3.781497]\n",
      "epoch:11 step:8716 [D loss: 0.015975, acc.: 100.00%] [G loss: 4.519359]\n",
      "epoch:11 step:8717 [D loss: 0.076246, acc.: 97.66%] [G loss: 3.083064]\n",
      "epoch:11 step:8718 [D loss: 0.040762, acc.: 100.00%] [G loss: 4.009511]\n",
      "epoch:11 step:8719 [D loss: 0.034958, acc.: 100.00%] [G loss: 4.077330]\n",
      "epoch:11 step:8720 [D loss: 0.056929, acc.: 98.44%] [G loss: 3.814954]\n",
      "epoch:11 step:8721 [D loss: 0.016944, acc.: 100.00%] [G loss: 3.879255]\n",
      "epoch:11 step:8722 [D loss: 0.021975, acc.: 100.00%] [G loss: 2.847007]\n",
      "epoch:11 step:8723 [D loss: 0.496861, acc.: 74.22%] [G loss: 4.898768]\n",
      "epoch:11 step:8724 [D loss: 0.009020, acc.: 100.00%] [G loss: 6.450651]\n",
      "epoch:11 step:8725 [D loss: 0.183088, acc.: 92.97%] [G loss: 1.353510]\n",
      "epoch:11 step:8726 [D loss: 0.206635, acc.: 89.84%] [G loss: 5.111500]\n",
      "epoch:11 step:8727 [D loss: 0.285754, acc.: 86.72%] [G loss: 2.025512]\n",
      "epoch:11 step:8728 [D loss: 0.069706, acc.: 98.44%] [G loss: 1.698071]\n",
      "epoch:11 step:8729 [D loss: 0.005262, acc.: 100.00%] [G loss: 3.823563]\n",
      "epoch:11 step:8730 [D loss: 0.012814, acc.: 100.00%] [G loss: 2.333172]\n",
      "epoch:11 step:8731 [D loss: 0.036332, acc.: 99.22%] [G loss: 1.779142]\n",
      "epoch:11 step:8732 [D loss: 0.013362, acc.: 100.00%] [G loss: 1.809440]\n",
      "epoch:11 step:8733 [D loss: 0.238257, acc.: 89.84%] [G loss: 5.542573]\n",
      "epoch:11 step:8734 [D loss: 0.647002, acc.: 67.19%] [G loss: 0.418648]\n",
      "epoch:11 step:8735 [D loss: 0.540515, acc.: 71.88%] [G loss: 6.048843]\n",
      "epoch:11 step:8736 [D loss: 0.999164, acc.: 55.47%] [G loss: 3.287999]\n",
      "epoch:11 step:8737 [D loss: 0.257449, acc.: 88.28%] [G loss: 3.870172]\n",
      "epoch:11 step:8738 [D loss: 0.030063, acc.: 100.00%] [G loss: 5.070959]\n",
      "epoch:11 step:8739 [D loss: 0.113816, acc.: 94.53%] [G loss: 3.740964]\n",
      "epoch:11 step:8740 [D loss: 0.046295, acc.: 100.00%] [G loss: 3.193759]\n",
      "epoch:11 step:8741 [D loss: 0.076491, acc.: 99.22%] [G loss: 4.556178]\n",
      "epoch:11 step:8742 [D loss: 0.022256, acc.: 100.00%] [G loss: 2.895212]\n",
      "epoch:11 step:8743 [D loss: 0.040446, acc.: 99.22%] [G loss: 3.243072]\n",
      "epoch:11 step:8744 [D loss: 0.221451, acc.: 93.75%] [G loss: 4.778802]\n",
      "epoch:11 step:8745 [D loss: 0.019506, acc.: 100.00%] [G loss: 4.511631]\n",
      "epoch:11 step:8746 [D loss: 0.236609, acc.: 91.41%] [G loss: 4.419312]\n",
      "epoch:11 step:8747 [D loss: 0.129675, acc.: 96.09%] [G loss: 3.929627]\n",
      "epoch:11 step:8748 [D loss: 0.073312, acc.: 98.44%] [G loss: 2.753421]\n",
      "epoch:11 step:8749 [D loss: 0.210540, acc.: 92.97%] [G loss: 4.192609]\n",
      "epoch:11 step:8750 [D loss: 0.046841, acc.: 100.00%] [G loss: 4.989336]\n",
      "epoch:11 step:8751 [D loss: 0.291272, acc.: 88.28%] [G loss: 3.360051]\n",
      "epoch:11 step:8752 [D loss: 0.225146, acc.: 90.62%] [G loss: 4.738455]\n",
      "epoch:11 step:8753 [D loss: 0.124683, acc.: 94.53%] [G loss: 3.894459]\n",
      "epoch:11 step:8754 [D loss: 0.041232, acc.: 99.22%] [G loss: 2.384817]\n",
      "epoch:11 step:8755 [D loss: 0.050655, acc.: 99.22%] [G loss: 1.932979]\n",
      "epoch:11 step:8756 [D loss: 0.052665, acc.: 100.00%] [G loss: 3.182972]\n",
      "epoch:11 step:8757 [D loss: 0.128001, acc.: 95.31%] [G loss: 4.211486]\n",
      "epoch:11 step:8758 [D loss: 0.016486, acc.: 100.00%] [G loss: 3.097867]\n",
      "epoch:11 step:8759 [D loss: 0.009863, acc.: 100.00%] [G loss: 2.266832]\n",
      "epoch:11 step:8760 [D loss: 0.021861, acc.: 100.00%] [G loss: 1.458519]\n",
      "epoch:11 step:8761 [D loss: 0.030717, acc.: 100.00%] [G loss: 1.163733]\n",
      "epoch:11 step:8762 [D loss: 0.038829, acc.: 99.22%] [G loss: 0.991094]\n",
      "epoch:11 step:8763 [D loss: 0.253595, acc.: 86.72%] [G loss: 6.047130]\n",
      "epoch:11 step:8764 [D loss: 1.072835, acc.: 54.69%] [G loss: 1.256861]\n",
      "epoch:11 step:8765 [D loss: 0.008851, acc.: 100.00%] [G loss: 2.032703]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:11 step:8766 [D loss: 0.257670, acc.: 89.84%] [G loss: 5.962118]\n",
      "epoch:11 step:8767 [D loss: 2.115789, acc.: 18.75%] [G loss: 6.254898]\n",
      "epoch:11 step:8768 [D loss: 0.189384, acc.: 91.41%] [G loss: 6.473917]\n",
      "epoch:11 step:8769 [D loss: 0.546824, acc.: 68.75%] [G loss: 3.232778]\n",
      "epoch:11 step:8770 [D loss: 0.381186, acc.: 82.81%] [G loss: 4.866048]\n",
      "epoch:11 step:8771 [D loss: 0.014782, acc.: 100.00%] [G loss: 5.008531]\n",
      "epoch:11 step:8772 [D loss: 0.056497, acc.: 98.44%] [G loss: 4.068168]\n",
      "epoch:11 step:8773 [D loss: 0.073973, acc.: 99.22%] [G loss: 3.746741]\n",
      "epoch:11 step:8774 [D loss: 0.100414, acc.: 97.66%] [G loss: 3.912183]\n",
      "epoch:11 step:8775 [D loss: 0.133367, acc.: 94.53%] [G loss: 4.240244]\n",
      "epoch:11 step:8776 [D loss: 0.032554, acc.: 99.22%] [G loss: 4.028197]\n",
      "epoch:11 step:8777 [D loss: 0.063456, acc.: 97.66%] [G loss: 3.479869]\n",
      "epoch:11 step:8778 [D loss: 0.454185, acc.: 77.34%] [G loss: 3.778634]\n",
      "epoch:11 step:8779 [D loss: 0.018256, acc.: 100.00%] [G loss: 4.429758]\n",
      "epoch:11 step:8780 [D loss: 0.024549, acc.: 99.22%] [G loss: 5.143035]\n",
      "epoch:11 step:8781 [D loss: 0.196743, acc.: 92.19%] [G loss: 1.527912]\n",
      "epoch:11 step:8782 [D loss: 0.087541, acc.: 98.44%] [G loss: 1.095629]\n",
      "epoch:11 step:8783 [D loss: 0.400836, acc.: 75.78%] [G loss: 6.138593]\n",
      "epoch:11 step:8784 [D loss: 0.279833, acc.: 85.16%] [G loss: 5.353863]\n",
      "epoch:11 step:8785 [D loss: 0.220342, acc.: 91.41%] [G loss: 0.770207]\n",
      "epoch:11 step:8786 [D loss: 0.184328, acc.: 94.53%] [G loss: 2.944750]\n",
      "epoch:11 step:8787 [D loss: 0.105453, acc.: 96.88%] [G loss: 3.461303]\n",
      "epoch:11 step:8788 [D loss: 0.022070, acc.: 100.00%] [G loss: 3.694745]\n",
      "epoch:11 step:8789 [D loss: 0.020550, acc.: 100.00%] [G loss: 2.564077]\n",
      "epoch:11 step:8790 [D loss: 0.053218, acc.: 100.00%] [G loss: 1.585095]\n",
      "epoch:11 step:8791 [D loss: 0.058303, acc.: 98.44%] [G loss: 2.157752]\n",
      "epoch:11 step:8792 [D loss: 0.026382, acc.: 100.00%] [G loss: 1.195296]\n",
      "epoch:11 step:8793 [D loss: 0.318506, acc.: 88.28%] [G loss: 2.559149]\n",
      "epoch:11 step:8794 [D loss: 0.046810, acc.: 100.00%] [G loss: 3.972980]\n",
      "epoch:11 step:8795 [D loss: 0.554531, acc.: 73.44%] [G loss: 0.338990]\n",
      "epoch:11 step:8796 [D loss: 2.284721, acc.: 51.56%] [G loss: 7.381217]\n",
      "epoch:11 step:8797 [D loss: 2.278455, acc.: 50.00%] [G loss: 4.655836]\n",
      "epoch:11 step:8798 [D loss: 1.400616, acc.: 50.00%] [G loss: 1.953760]\n",
      "epoch:11 step:8799 [D loss: 0.390347, acc.: 84.38%] [G loss: 1.700189]\n",
      "epoch:11 step:8800 [D loss: 0.280418, acc.: 93.75%] [G loss: 2.216284]\n",
      "##############\n",
      "[2.11215509 1.07130146 1.09987516 0.99729521 2.10583464 1.12358516\n",
      " 2.12285083 1.11443437 2.10234026 0.91030948]\n",
      "##########\n",
      "epoch:11 step:8801 [D loss: 0.217118, acc.: 96.09%] [G loss: 2.247130]\n",
      "epoch:11 step:8802 [D loss: 0.088298, acc.: 100.00%] [G loss: 2.367848]\n",
      "epoch:11 step:8803 [D loss: 0.207703, acc.: 92.97%] [G loss: 2.031584]\n",
      "epoch:11 step:8804 [D loss: 0.795540, acc.: 57.03%] [G loss: 4.044914]\n",
      "epoch:11 step:8805 [D loss: 0.537464, acc.: 69.53%] [G loss: 3.893664]\n",
      "epoch:11 step:8806 [D loss: 0.444172, acc.: 75.78%] [G loss: 2.404878]\n",
      "epoch:11 step:8807 [D loss: 0.121985, acc.: 97.66%] [G loss: 1.460518]\n",
      "epoch:11 step:8808 [D loss: 0.113619, acc.: 98.44%] [G loss: 2.177009]\n",
      "epoch:11 step:8809 [D loss: 0.082861, acc.: 100.00%] [G loss: 2.536837]\n",
      "epoch:11 step:8810 [D loss: 0.132137, acc.: 97.66%] [G loss: 2.150065]\n",
      "epoch:11 step:8811 [D loss: 0.272106, acc.: 91.41%] [G loss: 3.263318]\n",
      "epoch:11 step:8812 [D loss: 0.118413, acc.: 96.88%] [G loss: 3.260579]\n",
      "epoch:11 step:8813 [D loss: 0.473688, acc.: 74.22%] [G loss: 2.298392]\n",
      "epoch:11 step:8814 [D loss: 0.061980, acc.: 100.00%] [G loss: 1.982658]\n",
      "epoch:11 step:8815 [D loss: 0.133283, acc.: 97.66%] [G loss: 1.972214]\n",
      "epoch:11 step:8816 [D loss: 0.080278, acc.: 100.00%] [G loss: 1.466241]\n",
      "epoch:11 step:8817 [D loss: 0.142046, acc.: 97.66%] [G loss: 2.262550]\n",
      "epoch:11 step:8818 [D loss: 0.207678, acc.: 92.97%] [G loss: 3.365851]\n",
      "epoch:11 step:8819 [D loss: 0.186488, acc.: 92.19%] [G loss: 2.770943]\n",
      "epoch:11 step:8820 [D loss: 0.345632, acc.: 87.50%] [G loss: 2.213630]\n",
      "epoch:11 step:8821 [D loss: 0.115248, acc.: 97.66%] [G loss: 1.499632]\n",
      "epoch:11 step:8822 [D loss: 0.228768, acc.: 89.84%] [G loss: 3.663479]\n",
      "epoch:11 step:8823 [D loss: 0.189582, acc.: 90.62%] [G loss: 3.316827]\n",
      "epoch:11 step:8824 [D loss: 0.181498, acc.: 92.97%] [G loss: 2.960893]\n",
      "epoch:11 step:8825 [D loss: 0.180341, acc.: 96.09%] [G loss: 2.844565]\n",
      "epoch:11 step:8826 [D loss: 0.058224, acc.: 99.22%] [G loss: 3.199325]\n",
      "epoch:11 step:8827 [D loss: 0.466104, acc.: 75.00%] [G loss: 3.809994]\n",
      "epoch:11 step:8828 [D loss: 0.085064, acc.: 98.44%] [G loss: 4.515933]\n",
      "epoch:11 step:8829 [D loss: 0.136315, acc.: 95.31%] [G loss: 3.779613]\n",
      "epoch:11 step:8830 [D loss: 0.050294, acc.: 100.00%] [G loss: 2.874373]\n",
      "epoch:11 step:8831 [D loss: 0.207032, acc.: 92.19%] [G loss: 3.750593]\n",
      "epoch:11 step:8832 [D loss: 0.094942, acc.: 98.44%] [G loss: 2.683793]\n",
      "epoch:11 step:8833 [D loss: 0.048304, acc.: 100.00%] [G loss: 2.736356]\n",
      "epoch:11 step:8834 [D loss: 0.086080, acc.: 100.00%] [G loss: 2.852424]\n",
      "epoch:11 step:8835 [D loss: 0.048220, acc.: 100.00%] [G loss: 2.986635]\n",
      "epoch:11 step:8836 [D loss: 0.196426, acc.: 92.97%] [G loss: 3.469558]\n",
      "epoch:11 step:8837 [D loss: 0.183045, acc.: 92.97%] [G loss: 3.139416]\n",
      "epoch:11 step:8838 [D loss: 0.129039, acc.: 96.88%] [G loss: 2.537714]\n",
      "epoch:11 step:8839 [D loss: 0.055969, acc.: 100.00%] [G loss: 3.104007]\n",
      "epoch:11 step:8840 [D loss: 0.030451, acc.: 100.00%] [G loss: 3.929181]\n",
      "epoch:11 step:8841 [D loss: 0.879433, acc.: 51.56%] [G loss: 5.327038]\n",
      "epoch:11 step:8842 [D loss: 0.047648, acc.: 98.44%] [G loss: 6.063068]\n",
      "epoch:11 step:8843 [D loss: 0.562033, acc.: 72.66%] [G loss: 2.563520]\n",
      "epoch:11 step:8844 [D loss: 0.217818, acc.: 92.97%] [G loss: 3.856568]\n",
      "epoch:11 step:8845 [D loss: 0.019586, acc.: 100.00%] [G loss: 4.417566]\n",
      "epoch:11 step:8846 [D loss: 0.090426, acc.: 96.88%] [G loss: 3.689020]\n",
      "epoch:11 step:8847 [D loss: 0.065791, acc.: 100.00%] [G loss: 3.511546]\n",
      "epoch:11 step:8848 [D loss: 0.031804, acc.: 99.22%] [G loss: 3.597739]\n",
      "epoch:11 step:8849 [D loss: 0.042733, acc.: 99.22%] [G loss: 3.101254]\n",
      "epoch:11 step:8850 [D loss: 0.027494, acc.: 100.00%] [G loss: 3.074454]\n",
      "epoch:11 step:8851 [D loss: 0.048437, acc.: 99.22%] [G loss: 3.007293]\n",
      "epoch:11 step:8852 [D loss: 0.118934, acc.: 96.88%] [G loss: 3.733974]\n",
      "epoch:11 step:8853 [D loss: 0.025378, acc.: 100.00%] [G loss: 3.326748]\n",
      "epoch:11 step:8854 [D loss: 0.198803, acc.: 93.75%] [G loss: 2.842510]\n",
      "epoch:11 step:8855 [D loss: 0.312952, acc.: 86.72%] [G loss: 2.823227]\n",
      "epoch:11 step:8856 [D loss: 0.051123, acc.: 100.00%] [G loss: 2.961586]\n",
      "epoch:11 step:8857 [D loss: 0.100809, acc.: 99.22%] [G loss: 1.147352]\n",
      "epoch:11 step:8858 [D loss: 0.421273, acc.: 80.47%] [G loss: 4.681561]\n",
      "epoch:11 step:8859 [D loss: 0.183164, acc.: 91.41%] [G loss: 3.223192]\n",
      "epoch:11 step:8860 [D loss: 0.294815, acc.: 85.16%] [G loss: 3.212166]\n",
      "epoch:11 step:8861 [D loss: 0.131656, acc.: 94.53%] [G loss: 4.558272]\n",
      "epoch:11 step:8862 [D loss: 0.068762, acc.: 98.44%] [G loss: 4.969989]\n",
      "epoch:11 step:8863 [D loss: 0.043507, acc.: 99.22%] [G loss: 3.763875]\n",
      "epoch:11 step:8864 [D loss: 0.101381, acc.: 97.66%] [G loss: 2.843517]\n",
      "epoch:11 step:8865 [D loss: 0.029609, acc.: 100.00%] [G loss: 2.895614]\n",
      "epoch:11 step:8866 [D loss: 0.067941, acc.: 99.22%] [G loss: 2.121569]\n",
      "epoch:11 step:8867 [D loss: 0.048195, acc.: 100.00%] [G loss: 2.880719]\n",
      "epoch:11 step:8868 [D loss: 0.362010, acc.: 84.38%] [G loss: 4.671274]\n",
      "epoch:11 step:8869 [D loss: 0.152747, acc.: 91.41%] [G loss: 3.776642]\n",
      "epoch:11 step:8870 [D loss: 0.083637, acc.: 94.53%] [G loss: 2.238585]\n",
      "epoch:11 step:8871 [D loss: 0.067871, acc.: 98.44%] [G loss: 1.988775]\n",
      "epoch:11 step:8872 [D loss: 0.091421, acc.: 97.66%] [G loss: 1.908656]\n",
      "epoch:11 step:8873 [D loss: 0.085940, acc.: 98.44%] [G loss: 0.888900]\n",
      "epoch:11 step:8874 [D loss: 0.072505, acc.: 99.22%] [G loss: 2.429157]\n",
      "epoch:11 step:8875 [D loss: 0.025057, acc.: 99.22%] [G loss: 2.281268]\n",
      "epoch:11 step:8876 [D loss: 0.035513, acc.: 100.00%] [G loss: 2.170689]\n",
      "epoch:11 step:8877 [D loss: 0.236447, acc.: 94.53%] [G loss: 1.779597]\n",
      "epoch:11 step:8878 [D loss: 0.113882, acc.: 95.31%] [G loss: 1.300991]\n",
      "epoch:11 step:8879 [D loss: 0.458535, acc.: 75.00%] [G loss: 6.757281]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:11 step:8880 [D loss: 1.182120, acc.: 50.78%] [G loss: 2.284199]\n",
      "epoch:11 step:8881 [D loss: 0.330388, acc.: 85.16%] [G loss: 4.745292]\n",
      "epoch:11 step:8882 [D loss: 0.901839, acc.: 58.59%] [G loss: 1.772919]\n",
      "epoch:11 step:8883 [D loss: 0.111532, acc.: 96.09%] [G loss: 2.667406]\n",
      "epoch:11 step:8884 [D loss: 0.098833, acc.: 98.44%] [G loss: 3.780983]\n",
      "epoch:11 step:8885 [D loss: 0.036384, acc.: 100.00%] [G loss: 3.271142]\n",
      "epoch:11 step:8886 [D loss: 0.273097, acc.: 91.41%] [G loss: 2.982924]\n",
      "epoch:11 step:8887 [D loss: 0.250831, acc.: 89.06%] [G loss: 4.786464]\n",
      "epoch:11 step:8888 [D loss: 0.091092, acc.: 97.66%] [G loss: 4.862961]\n",
      "epoch:11 step:8889 [D loss: 0.129853, acc.: 92.97%] [G loss: 2.564974]\n",
      "epoch:11 step:8890 [D loss: 0.065689, acc.: 100.00%] [G loss: 1.934245]\n",
      "epoch:11 step:8891 [D loss: 0.069491, acc.: 99.22%] [G loss: 3.545033]\n",
      "epoch:11 step:8892 [D loss: 0.018000, acc.: 100.00%] [G loss: 3.821452]\n",
      "epoch:11 step:8893 [D loss: 0.231541, acc.: 92.19%] [G loss: 4.080519]\n",
      "epoch:11 step:8894 [D loss: 0.205958, acc.: 94.53%] [G loss: 2.690030]\n",
      "epoch:11 step:8895 [D loss: 0.046029, acc.: 99.22%] [G loss: 2.611830]\n",
      "epoch:11 step:8896 [D loss: 0.083117, acc.: 98.44%] [G loss: 4.087826]\n",
      "epoch:11 step:8897 [D loss: 0.036394, acc.: 100.00%] [G loss: 3.926741]\n",
      "epoch:11 step:8898 [D loss: 0.213528, acc.: 92.97%] [G loss: 2.071764]\n",
      "epoch:11 step:8899 [D loss: 0.102917, acc.: 96.88%] [G loss: 3.894044]\n",
      "epoch:11 step:8900 [D loss: 0.350304, acc.: 83.59%] [G loss: 2.314386]\n",
      "epoch:11 step:8901 [D loss: 0.139380, acc.: 95.31%] [G loss: 4.094706]\n",
      "epoch:11 step:8902 [D loss: 0.050854, acc.: 98.44%] [G loss: 4.898714]\n",
      "epoch:11 step:8903 [D loss: 0.468574, acc.: 78.91%] [G loss: 0.677673]\n",
      "epoch:11 step:8904 [D loss: 0.982582, acc.: 57.03%] [G loss: 6.563004]\n",
      "epoch:11 step:8905 [D loss: 1.920010, acc.: 50.00%] [G loss: 4.290379]\n",
      "epoch:11 step:8906 [D loss: 0.971520, acc.: 55.47%] [G loss: 0.989705]\n",
      "epoch:11 step:8907 [D loss: 0.283495, acc.: 86.72%] [G loss: 2.069463]\n",
      "epoch:11 step:8908 [D loss: 0.133956, acc.: 95.31%] [G loss: 2.999965]\n",
      "epoch:11 step:8909 [D loss: 0.285026, acc.: 89.84%] [G loss: 3.330403]\n",
      "epoch:11 step:8910 [D loss: 0.870071, acc.: 53.12%] [G loss: 1.397055]\n",
      "epoch:11 step:8911 [D loss: 0.510244, acc.: 75.78%] [G loss: 3.221841]\n",
      "epoch:11 step:8912 [D loss: 0.435459, acc.: 75.00%] [G loss: 2.026361]\n",
      "epoch:11 step:8913 [D loss: 0.186156, acc.: 95.31%] [G loss: 2.363276]\n",
      "epoch:11 step:8914 [D loss: 0.477502, acc.: 73.44%] [G loss: 3.334475]\n",
      "epoch:11 step:8915 [D loss: 0.266038, acc.: 87.50%] [G loss: 2.510868]\n",
      "epoch:11 step:8916 [D loss: 0.462898, acc.: 76.56%] [G loss: 2.645965]\n",
      "epoch:11 step:8917 [D loss: 0.182125, acc.: 92.97%] [G loss: 2.177197]\n",
      "epoch:11 step:8918 [D loss: 0.205905, acc.: 96.09%] [G loss: 3.353792]\n",
      "epoch:11 step:8919 [D loss: 0.325326, acc.: 88.28%] [G loss: 4.250713]\n",
      "epoch:11 step:8920 [D loss: 0.183660, acc.: 94.53%] [G loss: 2.657512]\n",
      "epoch:11 step:8921 [D loss: 0.120468, acc.: 96.88%] [G loss: 3.703335]\n",
      "epoch:11 step:8922 [D loss: 0.124779, acc.: 96.09%] [G loss: 2.598403]\n",
      "epoch:11 step:8923 [D loss: 0.178753, acc.: 96.88%] [G loss: 3.753612]\n",
      "epoch:11 step:8924 [D loss: 0.243922, acc.: 88.28%] [G loss: 2.655686]\n",
      "epoch:11 step:8925 [D loss: 0.211065, acc.: 92.19%] [G loss: 4.127128]\n",
      "epoch:11 step:8926 [D loss: 0.112239, acc.: 96.88%] [G loss: 4.176457]\n",
      "epoch:11 step:8927 [D loss: 0.146662, acc.: 95.31%] [G loss: 3.196299]\n",
      "epoch:11 step:8928 [D loss: 0.162283, acc.: 96.09%] [G loss: 4.641954]\n",
      "epoch:11 step:8929 [D loss: 0.176296, acc.: 93.75%] [G loss: 3.555973]\n",
      "epoch:11 step:8930 [D loss: 0.058066, acc.: 99.22%] [G loss: 2.705900]\n",
      "epoch:11 step:8931 [D loss: 0.045689, acc.: 100.00%] [G loss: 3.429411]\n",
      "epoch:11 step:8932 [D loss: 0.044644, acc.: 99.22%] [G loss: 3.303768]\n",
      "epoch:11 step:8933 [D loss: 0.052589, acc.: 99.22%] [G loss: 3.364058]\n",
      "epoch:11 step:8934 [D loss: 0.080050, acc.: 99.22%] [G loss: 2.436487]\n",
      "epoch:11 step:8935 [D loss: 0.025746, acc.: 100.00%] [G loss: 2.239231]\n",
      "epoch:11 step:8936 [D loss: 0.080269, acc.: 97.66%] [G loss: 2.885269]\n",
      "epoch:11 step:8937 [D loss: 0.117114, acc.: 96.88%] [G loss: 2.065626]\n",
      "epoch:11 step:8938 [D loss: 0.092691, acc.: 96.88%] [G loss: 3.207119]\n",
      "epoch:11 step:8939 [D loss: 0.049068, acc.: 100.00%] [G loss: 2.577558]\n",
      "epoch:11 step:8940 [D loss: 0.084796, acc.: 97.66%] [G loss: 1.719478]\n",
      "epoch:11 step:8941 [D loss: 0.173152, acc.: 93.75%] [G loss: 2.173440]\n",
      "epoch:11 step:8942 [D loss: 0.034581, acc.: 100.00%] [G loss: 3.429854]\n",
      "epoch:11 step:8943 [D loss: 0.091504, acc.: 98.44%] [G loss: 3.231127]\n",
      "epoch:11 step:8944 [D loss: 0.043775, acc.: 99.22%] [G loss: 3.718542]\n",
      "epoch:11 step:8945 [D loss: 0.018160, acc.: 100.00%] [G loss: 3.160935]\n",
      "epoch:11 step:8946 [D loss: 0.043126, acc.: 100.00%] [G loss: 3.372349]\n",
      "epoch:11 step:8947 [D loss: 0.056086, acc.: 99.22%] [G loss: 2.934835]\n",
      "epoch:11 step:8948 [D loss: 0.054570, acc.: 100.00%] [G loss: 4.267287]\n",
      "epoch:11 step:8949 [D loss: 0.164077, acc.: 94.53%] [G loss: 2.731915]\n",
      "epoch:11 step:8950 [D loss: 0.029421, acc.: 100.00%] [G loss: 2.368899]\n",
      "epoch:11 step:8951 [D loss: 0.011634, acc.: 100.00%] [G loss: 1.936870]\n",
      "epoch:11 step:8952 [D loss: 0.213912, acc.: 90.62%] [G loss: 5.391964]\n",
      "epoch:11 step:8953 [D loss: 0.038097, acc.: 100.00%] [G loss: 5.794628]\n",
      "epoch:11 step:8954 [D loss: 0.212060, acc.: 92.19%] [G loss: 3.784211]\n",
      "epoch:11 step:8955 [D loss: 0.200125, acc.: 89.06%] [G loss: 6.011416]\n",
      "epoch:11 step:8956 [D loss: 0.024956, acc.: 100.00%] [G loss: 6.373002]\n",
      "epoch:11 step:8957 [D loss: 0.242718, acc.: 91.41%] [G loss: 3.211878]\n",
      "epoch:11 step:8958 [D loss: 0.077742, acc.: 98.44%] [G loss: 4.626983]\n",
      "epoch:11 step:8959 [D loss: 0.009874, acc.: 100.00%] [G loss: 4.872603]\n",
      "epoch:11 step:8960 [D loss: 0.008395, acc.: 100.00%] [G loss: 5.279898]\n",
      "epoch:11 step:8961 [D loss: 0.036656, acc.: 99.22%] [G loss: 4.810898]\n",
      "epoch:11 step:8962 [D loss: 0.020192, acc.: 100.00%] [G loss: 5.008091]\n",
      "epoch:11 step:8963 [D loss: 0.008333, acc.: 100.00%] [G loss: 4.856500]\n",
      "epoch:11 step:8964 [D loss: 0.045181, acc.: 99.22%] [G loss: 4.763160]\n",
      "epoch:11 step:8965 [D loss: 0.094170, acc.: 98.44%] [G loss: 4.719055]\n",
      "epoch:11 step:8966 [D loss: 0.090765, acc.: 98.44%] [G loss: 3.680436]\n",
      "epoch:11 step:8967 [D loss: 0.161739, acc.: 94.53%] [G loss: 2.909008]\n",
      "epoch:11 step:8968 [D loss: 0.019650, acc.: 100.00%] [G loss: 4.543542]\n",
      "epoch:11 step:8969 [D loss: 0.192557, acc.: 92.19%] [G loss: 0.299715]\n",
      "epoch:11 step:8970 [D loss: 0.583331, acc.: 72.66%] [G loss: 8.578506]\n",
      "epoch:11 step:8971 [D loss: 1.854135, acc.: 51.56%] [G loss: 2.681329]\n",
      "epoch:11 step:8972 [D loss: 0.036983, acc.: 100.00%] [G loss: 1.176045]\n",
      "epoch:11 step:8973 [D loss: 0.054392, acc.: 100.00%] [G loss: 2.197114]\n",
      "epoch:11 step:8974 [D loss: 0.104174, acc.: 96.09%] [G loss: 3.707082]\n",
      "epoch:11 step:8975 [D loss: 0.076174, acc.: 98.44%] [G loss: 2.630068]\n",
      "epoch:11 step:8976 [D loss: 0.264396, acc.: 89.84%] [G loss: 2.310461]\n",
      "epoch:11 step:8977 [D loss: 0.057555, acc.: 99.22%] [G loss: 3.060824]\n",
      "epoch:11 step:8978 [D loss: 0.196440, acc.: 92.97%] [G loss: 2.121981]\n",
      "epoch:11 step:8979 [D loss: 0.063353, acc.: 100.00%] [G loss: 3.860093]\n",
      "epoch:11 step:8980 [D loss: 0.030201, acc.: 100.00%] [G loss: 3.700301]\n",
      "epoch:11 step:8981 [D loss: 0.827414, acc.: 60.16%] [G loss: 6.398750]\n",
      "epoch:11 step:8982 [D loss: 0.379924, acc.: 83.59%] [G loss: 4.052041]\n",
      "epoch:11 step:8983 [D loss: 0.067685, acc.: 97.66%] [G loss: 5.093401]\n",
      "epoch:11 step:8984 [D loss: 0.018483, acc.: 100.00%] [G loss: 5.552762]\n",
      "epoch:11 step:8985 [D loss: 0.021157, acc.: 100.00%] [G loss: 4.124778]\n",
      "epoch:11 step:8986 [D loss: 0.141241, acc.: 96.88%] [G loss: 2.763394]\n",
      "epoch:11 step:8987 [D loss: 0.051433, acc.: 99.22%] [G loss: 3.920412]\n",
      "epoch:11 step:8988 [D loss: 0.033331, acc.: 100.00%] [G loss: 3.287821]\n",
      "epoch:11 step:8989 [D loss: 0.020716, acc.: 100.00%] [G loss: 3.441221]\n",
      "epoch:11 step:8990 [D loss: 0.108233, acc.: 97.66%] [G loss: 4.770648]\n",
      "epoch:11 step:8991 [D loss: 0.096232, acc.: 96.09%] [G loss: 3.604590]\n",
      "epoch:11 step:8992 [D loss: 0.047211, acc.: 100.00%] [G loss: 1.460558]\n",
      "epoch:11 step:8993 [D loss: 0.279449, acc.: 85.94%] [G loss: 6.289038]\n",
      "epoch:11 step:8994 [D loss: 1.244071, acc.: 55.47%] [G loss: 0.815187]\n",
      "epoch:11 step:8995 [D loss: 0.198554, acc.: 92.19%] [G loss: 4.186966]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:11 step:8996 [D loss: 0.029012, acc.: 99.22%] [G loss: 3.991729]\n",
      "epoch:11 step:8997 [D loss: 0.111156, acc.: 97.66%] [G loss: 0.885570]\n",
      "epoch:11 step:8998 [D loss: 0.064904, acc.: 98.44%] [G loss: 1.388305]\n",
      "epoch:11 step:8999 [D loss: 0.012847, acc.: 100.00%] [G loss: 1.227771]\n",
      "epoch:11 step:9000 [D loss: 0.127790, acc.: 96.09%] [G loss: 1.300195]\n",
      "##############\n",
      "[1.11049236 0.94567374 0.90358555 1.00294958 0.92106513 1.04812975\n",
      " 1.11920553 0.81364028 2.10314139 1.07381596]\n",
      "##########\n",
      "epoch:11 step:9001 [D loss: 0.256514, acc.: 90.62%] [G loss: 5.859852]\n",
      "epoch:11 step:9002 [D loss: 1.527012, acc.: 42.19%] [G loss: 4.232225]\n",
      "epoch:11 step:9003 [D loss: 0.011706, acc.: 100.00%] [G loss: 5.036508]\n",
      "epoch:11 step:9004 [D loss: 0.021829, acc.: 100.00%] [G loss: 5.301262]\n",
      "epoch:11 step:9005 [D loss: 0.022904, acc.: 100.00%] [G loss: 4.355838]\n",
      "epoch:11 step:9006 [D loss: 0.043414, acc.: 100.00%] [G loss: 4.723188]\n",
      "epoch:11 step:9007 [D loss: 0.062379, acc.: 99.22%] [G loss: 4.286506]\n",
      "epoch:11 step:9008 [D loss: 0.134223, acc.: 96.88%] [G loss: 4.935788]\n",
      "epoch:11 step:9009 [D loss: 0.020626, acc.: 100.00%] [G loss: 5.243282]\n",
      "epoch:11 step:9010 [D loss: 0.027803, acc.: 100.00%] [G loss: 3.857893]\n",
      "epoch:11 step:9011 [D loss: 0.297843, acc.: 87.50%] [G loss: 4.134188]\n",
      "epoch:11 step:9012 [D loss: 0.012153, acc.: 100.00%] [G loss: 4.062040]\n",
      "epoch:11 step:9013 [D loss: 0.152312, acc.: 95.31%] [G loss: 2.406181]\n",
      "epoch:11 step:9014 [D loss: 0.099685, acc.: 97.66%] [G loss: 2.317046]\n",
      "epoch:11 step:9015 [D loss: 0.247201, acc.: 89.84%] [G loss: 4.373957]\n",
      "epoch:11 step:9016 [D loss: 0.053979, acc.: 99.22%] [G loss: 4.386156]\n",
      "epoch:11 step:9017 [D loss: 0.071139, acc.: 98.44%] [G loss: 2.411382]\n",
      "epoch:11 step:9018 [D loss: 0.043093, acc.: 100.00%] [G loss: 1.826819]\n",
      "epoch:11 step:9019 [D loss: 0.018834, acc.: 100.00%] [G loss: 3.352228]\n",
      "epoch:11 step:9020 [D loss: 0.021487, acc.: 100.00%] [G loss: 0.183307]\n",
      "epoch:11 step:9021 [D loss: 0.105912, acc.: 97.66%] [G loss: 2.228435]\n",
      "epoch:11 step:9022 [D loss: 0.985361, acc.: 52.34%] [G loss: 6.441877]\n",
      "epoch:11 step:9023 [D loss: 0.877735, acc.: 61.72%] [G loss: 1.839060]\n",
      "epoch:11 step:9024 [D loss: 0.528140, acc.: 82.03%] [G loss: 4.644598]\n",
      "epoch:11 step:9025 [D loss: 0.352275, acc.: 81.25%] [G loss: 2.562354]\n",
      "epoch:11 step:9026 [D loss: 0.177445, acc.: 92.97%] [G loss: 4.130998]\n",
      "epoch:11 step:9027 [D loss: 0.036188, acc.: 100.00%] [G loss: 3.124018]\n",
      "epoch:11 step:9028 [D loss: 3.184457, acc.: 12.50%] [G loss: 5.895998]\n",
      "epoch:11 step:9029 [D loss: 1.046539, acc.: 53.91%] [G loss: 5.193858]\n",
      "epoch:11 step:9030 [D loss: 0.561214, acc.: 71.88%] [G loss: 3.461138]\n",
      "epoch:11 step:9031 [D loss: 0.209554, acc.: 94.53%] [G loss: 3.207256]\n",
      "epoch:11 step:9032 [D loss: 0.108497, acc.: 97.66%] [G loss: 3.094831]\n",
      "epoch:11 step:9033 [D loss: 0.134406, acc.: 96.88%] [G loss: 2.247991]\n",
      "epoch:11 step:9034 [D loss: 0.271381, acc.: 85.94%] [G loss: 3.679232]\n",
      "epoch:11 step:9035 [D loss: 0.056264, acc.: 97.66%] [G loss: 4.731249]\n",
      "epoch:11 step:9036 [D loss: 0.305828, acc.: 83.59%] [G loss: 2.925668]\n",
      "epoch:11 step:9037 [D loss: 0.047793, acc.: 100.00%] [G loss: 2.285178]\n",
      "epoch:11 step:9038 [D loss: 0.143382, acc.: 98.44%] [G loss: 2.450661]\n",
      "epoch:11 step:9039 [D loss: 0.032107, acc.: 100.00%] [G loss: 3.149696]\n",
      "epoch:11 step:9040 [D loss: 0.030009, acc.: 100.00%] [G loss: 2.227798]\n",
      "epoch:11 step:9041 [D loss: 0.048476, acc.: 100.00%] [G loss: 1.391878]\n",
      "epoch:11 step:9042 [D loss: 0.157757, acc.: 96.88%] [G loss: 2.802547]\n",
      "epoch:11 step:9043 [D loss: 0.058066, acc.: 100.00%] [G loss: 2.235493]\n",
      "epoch:11 step:9044 [D loss: 0.192791, acc.: 93.75%] [G loss: 0.709872]\n",
      "epoch:11 step:9045 [D loss: 0.021219, acc.: 100.00%] [G loss: 0.438594]\n",
      "epoch:11 step:9046 [D loss: 0.403006, acc.: 77.34%] [G loss: 4.023608]\n",
      "epoch:11 step:9047 [D loss: 0.232816, acc.: 88.28%] [G loss: 4.223396]\n",
      "epoch:11 step:9048 [D loss: 0.041287, acc.: 99.22%] [G loss: 3.379011]\n",
      "epoch:11 step:9049 [D loss: 0.017872, acc.: 100.00%] [G loss: 3.255459]\n",
      "epoch:11 step:9050 [D loss: 0.117832, acc.: 97.66%] [G loss: 3.461322]\n",
      "epoch:11 step:9051 [D loss: 0.220335, acc.: 88.28%] [G loss: 1.729620]\n",
      "epoch:11 step:9052 [D loss: 0.082289, acc.: 98.44%] [G loss: 2.654664]\n",
      "epoch:11 step:9053 [D loss: 0.038094, acc.: 100.00%] [G loss: 3.242655]\n",
      "epoch:11 step:9054 [D loss: 0.097190, acc.: 98.44%] [G loss: 3.777531]\n",
      "epoch:11 step:9055 [D loss: 0.064965, acc.: 98.44%] [G loss: 3.570506]\n",
      "epoch:11 step:9056 [D loss: 0.084847, acc.: 97.66%] [G loss: 2.977185]\n",
      "epoch:11 step:9057 [D loss: 0.050021, acc.: 99.22%] [G loss: 2.857152]\n",
      "epoch:11 step:9058 [D loss: 0.068103, acc.: 99.22%] [G loss: 3.141359]\n",
      "epoch:11 step:9059 [D loss: 0.116419, acc.: 96.88%] [G loss: 3.771143]\n",
      "epoch:11 step:9060 [D loss: 0.031912, acc.: 100.00%] [G loss: 4.379768]\n",
      "epoch:11 step:9061 [D loss: 0.256418, acc.: 91.41%] [G loss: 2.746252]\n",
      "epoch:11 step:9062 [D loss: 0.081704, acc.: 98.44%] [G loss: 3.090642]\n",
      "epoch:11 step:9063 [D loss: 0.125978, acc.: 96.88%] [G loss: 4.193327]\n",
      "epoch:11 step:9064 [D loss: 0.116828, acc.: 97.66%] [G loss: 4.477607]\n",
      "epoch:11 step:9065 [D loss: 0.348834, acc.: 84.38%] [G loss: 3.162229]\n",
      "epoch:11 step:9066 [D loss: 0.227256, acc.: 91.41%] [G loss: 4.990976]\n",
      "epoch:11 step:9067 [D loss: 0.158749, acc.: 92.97%] [G loss: 4.597533]\n",
      "epoch:11 step:9068 [D loss: 0.030501, acc.: 100.00%] [G loss: 3.180082]\n",
      "epoch:11 step:9069 [D loss: 0.028851, acc.: 100.00%] [G loss: 3.502869]\n",
      "epoch:11 step:9070 [D loss: 0.038364, acc.: 100.00%] [G loss: 2.773318]\n",
      "epoch:11 step:9071 [D loss: 0.055137, acc.: 99.22%] [G loss: 2.567918]\n",
      "epoch:11 step:9072 [D loss: 0.038886, acc.: 100.00%] [G loss: 3.783589]\n",
      "epoch:11 step:9073 [D loss: 0.051482, acc.: 99.22%] [G loss: 2.755429]\n",
      "epoch:11 step:9074 [D loss: 0.070120, acc.: 99.22%] [G loss: 2.682502]\n",
      "epoch:11 step:9075 [D loss: 0.023638, acc.: 100.00%] [G loss: 2.499892]\n",
      "epoch:11 step:9076 [D loss: 0.220438, acc.: 94.53%] [G loss: 3.203704]\n",
      "epoch:11 step:9077 [D loss: 0.087890, acc.: 96.88%] [G loss: 2.940191]\n",
      "epoch:11 step:9078 [D loss: 0.024145, acc.: 99.22%] [G loss: 2.141334]\n",
      "epoch:11 step:9079 [D loss: 0.097443, acc.: 97.66%] [G loss: 1.876415]\n",
      "epoch:11 step:9080 [D loss: 0.040175, acc.: 100.00%] [G loss: 3.220750]\n",
      "epoch:11 step:9081 [D loss: 0.028020, acc.: 100.00%] [G loss: 2.472391]\n",
      "epoch:11 step:9082 [D loss: 0.295108, acc.: 89.06%] [G loss: 1.384975]\n",
      "epoch:11 step:9083 [D loss: 0.151968, acc.: 95.31%] [G loss: 1.125605]\n",
      "epoch:11 step:9084 [D loss: 0.012211, acc.: 100.00%] [G loss: 2.444432]\n",
      "epoch:11 step:9085 [D loss: 0.036806, acc.: 100.00%] [G loss: 2.813816]\n",
      "epoch:11 step:9086 [D loss: 0.103505, acc.: 98.44%] [G loss: 3.478162]\n",
      "epoch:11 step:9087 [D loss: 0.034987, acc.: 100.00%] [G loss: 2.860485]\n",
      "epoch:11 step:9088 [D loss: 0.172325, acc.: 94.53%] [G loss: 3.997527]\n",
      "epoch:11 step:9089 [D loss: 0.090752, acc.: 97.66%] [G loss: 3.643701]\n",
      "epoch:11 step:9090 [D loss: 0.100094, acc.: 99.22%] [G loss: 4.700478]\n",
      "epoch:11 step:9091 [D loss: 0.259598, acc.: 91.41%] [G loss: 3.572085]\n",
      "epoch:11 step:9092 [D loss: 0.024624, acc.: 100.00%] [G loss: 5.082594]\n",
      "epoch:11 step:9093 [D loss: 0.014192, acc.: 100.00%] [G loss: 4.177623]\n",
      "epoch:11 step:9094 [D loss: 0.028206, acc.: 100.00%] [G loss: 4.592430]\n",
      "epoch:11 step:9095 [D loss: 0.062364, acc.: 99.22%] [G loss: 5.315415]\n",
      "epoch:11 step:9096 [D loss: 0.071458, acc.: 98.44%] [G loss: 5.583741]\n",
      "epoch:11 step:9097 [D loss: 0.037080, acc.: 100.00%] [G loss: 3.655771]\n",
      "epoch:11 step:9098 [D loss: 0.102737, acc.: 96.88%] [G loss: 4.957311]\n",
      "epoch:11 step:9099 [D loss: 0.223418, acc.: 89.06%] [G loss: 3.397982]\n",
      "epoch:11 step:9100 [D loss: 0.112023, acc.: 96.88%] [G loss: 4.981535]\n",
      "epoch:11 step:9101 [D loss: 0.015804, acc.: 100.00%] [G loss: 5.259071]\n",
      "epoch:11 step:9102 [D loss: 0.363908, acc.: 85.94%] [G loss: 5.701050]\n",
      "epoch:11 step:9103 [D loss: 0.014283, acc.: 99.22%] [G loss: 5.510982]\n",
      "epoch:11 step:9104 [D loss: 0.057416, acc.: 98.44%] [G loss: 4.116025]\n",
      "epoch:11 step:9105 [D loss: 0.256558, acc.: 87.50%] [G loss: 7.226460]\n",
      "epoch:11 step:9106 [D loss: 1.088895, acc.: 57.03%] [G loss: 2.151686]\n",
      "epoch:11 step:9107 [D loss: 0.201205, acc.: 92.19%] [G loss: 4.835458]\n",
      "epoch:11 step:9108 [D loss: 0.002871, acc.: 100.00%] [G loss: 6.205363]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:11 step:9109 [D loss: 0.015389, acc.: 100.00%] [G loss: 4.704724]\n",
      "epoch:11 step:9110 [D loss: 0.046814, acc.: 97.66%] [G loss: 3.417588]\n",
      "epoch:11 step:9111 [D loss: 0.046507, acc.: 100.00%] [G loss: 3.806804]\n",
      "epoch:11 step:9112 [D loss: 0.011773, acc.: 100.00%] [G loss: 3.160597]\n",
      "epoch:11 step:9113 [D loss: 0.360180, acc.: 82.81%] [G loss: 4.245605]\n",
      "epoch:11 step:9114 [D loss: 0.115918, acc.: 96.09%] [G loss: 4.304249]\n",
      "epoch:11 step:9115 [D loss: 2.305327, acc.: 27.34%] [G loss: 6.174502]\n",
      "epoch:11 step:9116 [D loss: 0.174682, acc.: 90.62%] [G loss: 7.003462]\n",
      "epoch:11 step:9117 [D loss: 2.203974, acc.: 49.22%] [G loss: 3.075287]\n",
      "epoch:11 step:9118 [D loss: 0.414991, acc.: 82.03%] [G loss: 2.579434]\n",
      "epoch:11 step:9119 [D loss: 0.031289, acc.: 100.00%] [G loss: 3.609171]\n",
      "epoch:11 step:9120 [D loss: 0.107250, acc.: 96.09%] [G loss: 2.966684]\n",
      "epoch:11 step:9121 [D loss: 0.105039, acc.: 97.66%] [G loss: 2.844723]\n",
      "epoch:11 step:9122 [D loss: 0.023774, acc.: 100.00%] [G loss: 3.039894]\n",
      "epoch:11 step:9123 [D loss: 0.286416, acc.: 88.28%] [G loss: 2.955781]\n",
      "epoch:11 step:9124 [D loss: 0.101221, acc.: 99.22%] [G loss: 3.582694]\n",
      "epoch:11 step:9125 [D loss: 0.313675, acc.: 91.41%] [G loss: 4.725277]\n",
      "epoch:11 step:9126 [D loss: 0.020153, acc.: 99.22%] [G loss: 5.249792]\n",
      "epoch:11 step:9127 [D loss: 0.274850, acc.: 88.28%] [G loss: 2.076877]\n",
      "epoch:11 step:9128 [D loss: 0.406900, acc.: 82.03%] [G loss: 4.770026]\n",
      "epoch:11 step:9129 [D loss: 0.042905, acc.: 99.22%] [G loss: 5.202495]\n",
      "epoch:11 step:9130 [D loss: 0.205338, acc.: 92.97%] [G loss: 2.772416]\n",
      "epoch:11 step:9131 [D loss: 0.211111, acc.: 89.84%] [G loss: 4.770646]\n",
      "epoch:11 step:9132 [D loss: 0.020005, acc.: 100.00%] [G loss: 5.039486]\n",
      "epoch:11 step:9133 [D loss: 0.316705, acc.: 84.38%] [G loss: 2.543889]\n",
      "epoch:11 step:9134 [D loss: 0.412015, acc.: 80.47%] [G loss: 5.036082]\n",
      "epoch:11 step:9135 [D loss: 0.046854, acc.: 98.44%] [G loss: 6.056600]\n",
      "epoch:11 step:9136 [D loss: 0.572197, acc.: 71.88%] [G loss: 2.794837]\n",
      "epoch:11 step:9137 [D loss: 0.570301, acc.: 74.22%] [G loss: 5.748647]\n",
      "epoch:11 step:9138 [D loss: 0.358724, acc.: 82.81%] [G loss: 5.204043]\n",
      "epoch:11 step:9139 [D loss: 0.222891, acc.: 89.84%] [G loss: 2.740650]\n",
      "epoch:11 step:9140 [D loss: 0.162197, acc.: 92.97%] [G loss: 4.383861]\n",
      "epoch:11 step:9141 [D loss: 0.024781, acc.: 100.00%] [G loss: 4.623998]\n",
      "epoch:11 step:9142 [D loss: 0.016731, acc.: 100.00%] [G loss: 4.805174]\n",
      "epoch:11 step:9143 [D loss: 0.241209, acc.: 91.41%] [G loss: 2.314583]\n",
      "epoch:11 step:9144 [D loss: 0.062050, acc.: 98.44%] [G loss: 1.733732]\n",
      "epoch:11 step:9145 [D loss: 0.075046, acc.: 99.22%] [G loss: 2.620297]\n",
      "epoch:11 step:9146 [D loss: 0.039859, acc.: 100.00%] [G loss: 3.849781]\n",
      "epoch:11 step:9147 [D loss: 0.038777, acc.: 100.00%] [G loss: 3.364377]\n",
      "epoch:11 step:9148 [D loss: 0.230657, acc.: 88.28%] [G loss: 3.438172]\n",
      "epoch:11 step:9149 [D loss: 0.078540, acc.: 99.22%] [G loss: 3.316795]\n",
      "epoch:11 step:9150 [D loss: 0.081218, acc.: 97.66%] [G loss: 3.423023]\n",
      "epoch:11 step:9151 [D loss: 0.191966, acc.: 93.75%] [G loss: 4.529353]\n",
      "epoch:11 step:9152 [D loss: 0.113487, acc.: 96.88%] [G loss: 3.745800]\n",
      "epoch:11 step:9153 [D loss: 0.350813, acc.: 84.38%] [G loss: 4.009510]\n",
      "epoch:11 step:9154 [D loss: 0.030750, acc.: 100.00%] [G loss: 4.952589]\n",
      "epoch:11 step:9155 [D loss: 0.059286, acc.: 98.44%] [G loss: 3.703916]\n",
      "epoch:11 step:9156 [D loss: 0.168439, acc.: 93.75%] [G loss: 3.819062]\n",
      "epoch:11 step:9157 [D loss: 0.159528, acc.: 96.09%] [G loss: 3.018334]\n",
      "epoch:11 step:9158 [D loss: 0.101719, acc.: 96.88%] [G loss: 4.367664]\n",
      "epoch:11 step:9159 [D loss: 0.018763, acc.: 100.00%] [G loss: 4.928467]\n",
      "epoch:11 step:9160 [D loss: 0.036782, acc.: 100.00%] [G loss: 3.893822]\n",
      "epoch:11 step:9161 [D loss: 0.216651, acc.: 89.06%] [G loss: 2.980420]\n",
      "epoch:11 step:9162 [D loss: 0.040258, acc.: 100.00%] [G loss: 3.413598]\n",
      "epoch:11 step:9163 [D loss: 0.038221, acc.: 100.00%] [G loss: 3.257074]\n",
      "epoch:11 step:9164 [D loss: 0.100093, acc.: 96.88%] [G loss: 1.560519]\n",
      "epoch:11 step:9165 [D loss: 0.045813, acc.: 99.22%] [G loss: 1.033450]\n",
      "epoch:11 step:9166 [D loss: 0.204667, acc.: 94.53%] [G loss: 3.598860]\n",
      "epoch:11 step:9167 [D loss: 0.066571, acc.: 99.22%] [G loss: 3.581783]\n",
      "epoch:11 step:9168 [D loss: 0.142632, acc.: 96.88%] [G loss: 0.396613]\n",
      "epoch:11 step:9169 [D loss: 0.128007, acc.: 96.09%] [G loss: 2.444175]\n",
      "epoch:11 step:9170 [D loss: 0.016747, acc.: 100.00%] [G loss: 3.435183]\n",
      "epoch:11 step:9171 [D loss: 0.197268, acc.: 94.53%] [G loss: 3.089377]\n",
      "epoch:11 step:9172 [D loss: 0.053732, acc.: 100.00%] [G loss: 2.844215]\n",
      "epoch:11 step:9173 [D loss: 0.089830, acc.: 99.22%] [G loss: 3.750350]\n",
      "epoch:11 step:9174 [D loss: 0.153308, acc.: 94.53%] [G loss: 1.812614]\n",
      "epoch:11 step:9175 [D loss: 0.316117, acc.: 85.16%] [G loss: 6.875667]\n",
      "epoch:11 step:9176 [D loss: 0.859730, acc.: 60.16%] [G loss: 1.832685]\n",
      "epoch:11 step:9177 [D loss: 0.368248, acc.: 85.94%] [G loss: 4.419020]\n",
      "epoch:11 step:9178 [D loss: 0.016482, acc.: 100.00%] [G loss: 6.289544]\n",
      "epoch:11 step:9179 [D loss: 0.641747, acc.: 69.53%] [G loss: 0.772582]\n",
      "epoch:11 step:9180 [D loss: 0.598790, acc.: 70.31%] [G loss: 5.529357]\n",
      "epoch:11 step:9181 [D loss: 0.353996, acc.: 81.25%] [G loss: 4.996451]\n",
      "epoch:11 step:9182 [D loss: 0.081715, acc.: 96.09%] [G loss: 4.304792]\n",
      "epoch:11 step:9183 [D loss: 0.083819, acc.: 97.66%] [G loss: 4.267709]\n",
      "epoch:11 step:9184 [D loss: 0.030439, acc.: 100.00%] [G loss: 4.494116]\n",
      "epoch:11 step:9185 [D loss: 0.038918, acc.: 100.00%] [G loss: 3.475689]\n",
      "epoch:11 step:9186 [D loss: 0.018876, acc.: 100.00%] [G loss: 3.294731]\n",
      "epoch:11 step:9187 [D loss: 0.078196, acc.: 99.22%] [G loss: 4.026509]\n",
      "epoch:11 step:9188 [D loss: 0.029534, acc.: 100.00%] [G loss: 3.373745]\n",
      "epoch:11 step:9189 [D loss: 0.091455, acc.: 100.00%] [G loss: 3.388132]\n",
      "epoch:11 step:9190 [D loss: 0.527075, acc.: 71.09%] [G loss: 6.028625]\n",
      "epoch:11 step:9191 [D loss: 0.489231, acc.: 72.66%] [G loss: 3.389598]\n",
      "epoch:11 step:9192 [D loss: 0.083731, acc.: 96.88%] [G loss: 3.615621]\n",
      "epoch:11 step:9193 [D loss: 0.041590, acc.: 100.00%] [G loss: 4.587411]\n",
      "epoch:11 step:9194 [D loss: 0.045791, acc.: 100.00%] [G loss: 3.585657]\n",
      "epoch:11 step:9195 [D loss: 0.094903, acc.: 99.22%] [G loss: 2.750741]\n",
      "epoch:11 step:9196 [D loss: 0.226858, acc.: 91.41%] [G loss: 4.532985]\n",
      "epoch:11 step:9197 [D loss: 0.087383, acc.: 98.44%] [G loss: 5.303077]\n",
      "epoch:11 step:9198 [D loss: 0.257578, acc.: 89.06%] [G loss: 1.687825]\n",
      "epoch:11 step:9199 [D loss: 0.441948, acc.: 75.00%] [G loss: 6.643880]\n",
      "epoch:11 step:9200 [D loss: 0.323614, acc.: 84.38%] [G loss: 5.091105]\n",
      "##############\n",
      "[0.99965357 0.94460235 1.03502289 0.96513383 1.00815682 2.11689842\n",
      " 2.12903975 0.87096683 2.10713092 2.09742326]\n",
      "##########\n",
      "epoch:11 step:9201 [D loss: 0.171700, acc.: 93.75%] [G loss: 2.198822]\n",
      "epoch:11 step:9202 [D loss: 0.052033, acc.: 99.22%] [G loss: 3.923717]\n",
      "epoch:11 step:9203 [D loss: 0.047189, acc.: 99.22%] [G loss: 3.995930]\n",
      "epoch:11 step:9204 [D loss: 0.041513, acc.: 99.22%] [G loss: 3.845840]\n",
      "epoch:11 step:9205 [D loss: 0.036295, acc.: 100.00%] [G loss: 5.473053]\n",
      "epoch:11 step:9206 [D loss: 0.084556, acc.: 98.44%] [G loss: 2.169711]\n",
      "epoch:11 step:9207 [D loss: 0.113894, acc.: 97.66%] [G loss: 3.585474]\n",
      "epoch:11 step:9208 [D loss: 0.050041, acc.: 100.00%] [G loss: 2.901508]\n",
      "epoch:11 step:9209 [D loss: 0.037955, acc.: 99.22%] [G loss: 3.461146]\n",
      "epoch:11 step:9210 [D loss: 0.614817, acc.: 69.53%] [G loss: 5.501690]\n",
      "epoch:11 step:9211 [D loss: 0.098559, acc.: 96.88%] [G loss: 5.802008]\n",
      "epoch:11 step:9212 [D loss: 0.065680, acc.: 96.88%] [G loss: 4.086931]\n",
      "epoch:11 step:9213 [D loss: 0.027547, acc.: 100.00%] [G loss: 3.842104]\n",
      "epoch:11 step:9214 [D loss: 0.240934, acc.: 92.97%] [G loss: 4.154022]\n",
      "epoch:11 step:9215 [D loss: 0.012756, acc.: 100.00%] [G loss: 4.374032]\n",
      "epoch:11 step:9216 [D loss: 0.037725, acc.: 99.22%] [G loss: 4.983168]\n",
      "epoch:11 step:9217 [D loss: 0.067287, acc.: 96.88%] [G loss: 4.605357]\n",
      "epoch:11 step:9218 [D loss: 0.527324, acc.: 74.22%] [G loss: 6.213606]\n",
      "epoch:11 step:9219 [D loss: 0.100223, acc.: 98.44%] [G loss: 6.647956]\n",
      "epoch:11 step:9220 [D loss: 0.320388, acc.: 85.16%] [G loss: 1.270435]\n",
      "epoch:11 step:9221 [D loss: 0.132495, acc.: 95.31%] [G loss: 4.092578]\n",
      "epoch:11 step:9222 [D loss: 0.024846, acc.: 100.00%] [G loss: 4.701649]\n",
      "epoch:11 step:9223 [D loss: 0.013165, acc.: 100.00%] [G loss: 5.291893]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:11 step:9224 [D loss: 0.038876, acc.: 100.00%] [G loss: 4.640282]\n",
      "epoch:11 step:9225 [D loss: 0.019186, acc.: 100.00%] [G loss: 4.182114]\n",
      "epoch:11 step:9226 [D loss: 0.198839, acc.: 92.19%] [G loss: 4.511600]\n",
      "epoch:11 step:9227 [D loss: 0.041478, acc.: 100.00%] [G loss: 4.057034]\n",
      "epoch:11 step:9228 [D loss: 0.034210, acc.: 100.00%] [G loss: 3.922435]\n",
      "epoch:11 step:9229 [D loss: 0.035455, acc.: 100.00%] [G loss: 4.375186]\n",
      "epoch:11 step:9230 [D loss: 0.102770, acc.: 98.44%] [G loss: 3.017817]\n",
      "epoch:11 step:9231 [D loss: 0.116785, acc.: 96.88%] [G loss: 5.184865]\n",
      "epoch:11 step:9232 [D loss: 0.025853, acc.: 100.00%] [G loss: 5.749423]\n",
      "epoch:11 step:9233 [D loss: 0.431325, acc.: 78.12%] [G loss: 2.984336]\n",
      "epoch:11 step:9234 [D loss: 0.049511, acc.: 99.22%] [G loss: 2.821057]\n",
      "epoch:11 step:9235 [D loss: 0.033701, acc.: 100.00%] [G loss: 4.071512]\n",
      "epoch:11 step:9236 [D loss: 0.037322, acc.: 100.00%] [G loss: 4.512277]\n",
      "epoch:11 step:9237 [D loss: 0.163738, acc.: 96.09%] [G loss: 2.005595]\n",
      "epoch:11 step:9238 [D loss: 0.351282, acc.: 84.38%] [G loss: 5.610090]\n",
      "epoch:11 step:9239 [D loss: 0.174132, acc.: 89.84%] [G loss: 5.625071]\n",
      "epoch:11 step:9240 [D loss: 0.207294, acc.: 92.97%] [G loss: 2.148672]\n",
      "epoch:11 step:9241 [D loss: 0.360217, acc.: 83.59%] [G loss: 7.065628]\n",
      "epoch:11 step:9242 [D loss: 0.112427, acc.: 96.09%] [G loss: 7.099607]\n",
      "epoch:11 step:9243 [D loss: 0.300892, acc.: 84.38%] [G loss: 0.988916]\n",
      "epoch:11 step:9244 [D loss: 0.834158, acc.: 59.38%] [G loss: 7.750070]\n",
      "epoch:11 step:9245 [D loss: 1.477040, acc.: 52.34%] [G loss: 4.569827]\n",
      "epoch:11 step:9246 [D loss: 0.080627, acc.: 98.44%] [G loss: 2.207390]\n",
      "epoch:11 step:9247 [D loss: 0.069616, acc.: 96.88%] [G loss: 2.743490]\n",
      "epoch:11 step:9248 [D loss: 0.035956, acc.: 99.22%] [G loss: 3.023528]\n",
      "epoch:11 step:9249 [D loss: 0.104643, acc.: 97.66%] [G loss: 2.129628]\n",
      "epoch:11 step:9250 [D loss: 0.032706, acc.: 100.00%] [G loss: 1.027038]\n",
      "epoch:11 step:9251 [D loss: 0.475462, acc.: 75.00%] [G loss: 4.058883]\n",
      "epoch:11 step:9252 [D loss: 1.208341, acc.: 53.91%] [G loss: 2.005989]\n",
      "epoch:11 step:9253 [D loss: 0.187184, acc.: 94.53%] [G loss: 0.867300]\n",
      "epoch:11 step:9254 [D loss: 0.031237, acc.: 100.00%] [G loss: 1.497645]\n",
      "epoch:11 step:9255 [D loss: 0.017425, acc.: 100.00%] [G loss: 1.177400]\n",
      "epoch:11 step:9256 [D loss: 0.779100, acc.: 61.72%] [G loss: 4.539110]\n",
      "epoch:11 step:9257 [D loss: 0.096383, acc.: 96.88%] [G loss: 6.139024]\n",
      "epoch:11 step:9258 [D loss: 0.154401, acc.: 92.19%] [G loss: 4.407632]\n",
      "epoch:11 step:9259 [D loss: 0.080973, acc.: 97.66%] [G loss: 4.063948]\n",
      "epoch:11 step:9260 [D loss: 0.021353, acc.: 100.00%] [G loss: 4.496714]\n",
      "epoch:11 step:9261 [D loss: 0.189374, acc.: 91.41%] [G loss: 5.284021]\n",
      "epoch:11 step:9262 [D loss: 0.021099, acc.: 100.00%] [G loss: 5.533091]\n",
      "epoch:11 step:9263 [D loss: 0.048769, acc.: 99.22%] [G loss: 4.259970]\n",
      "epoch:11 step:9264 [D loss: 0.098433, acc.: 97.66%] [G loss: 3.548723]\n",
      "epoch:11 step:9265 [D loss: 0.070165, acc.: 99.22%] [G loss: 4.092573]\n",
      "epoch:11 step:9266 [D loss: 0.013508, acc.: 100.00%] [G loss: 4.499780]\n",
      "epoch:11 step:9267 [D loss: 0.017563, acc.: 100.00%] [G loss: 3.883388]\n",
      "epoch:11 step:9268 [D loss: 0.092469, acc.: 98.44%] [G loss: 1.857296]\n",
      "epoch:11 step:9269 [D loss: 0.103313, acc.: 98.44%] [G loss: 2.277428]\n",
      "epoch:11 step:9270 [D loss: 0.026050, acc.: 100.00%] [G loss: 3.567266]\n",
      "epoch:11 step:9271 [D loss: 0.011719, acc.: 100.00%] [G loss: 3.537876]\n",
      "epoch:11 step:9272 [D loss: 0.029695, acc.: 100.00%] [G loss: 3.084389]\n",
      "epoch:11 step:9273 [D loss: 0.095673, acc.: 98.44%] [G loss: 1.699579]\n",
      "epoch:11 step:9274 [D loss: 0.068793, acc.: 98.44%] [G loss: 1.010628]\n",
      "epoch:11 step:9275 [D loss: 0.060181, acc.: 100.00%] [G loss: 1.525648]\n",
      "epoch:11 step:9276 [D loss: 1.024643, acc.: 45.31%] [G loss: 5.408591]\n",
      "epoch:11 step:9277 [D loss: 0.220007, acc.: 89.06%] [G loss: 4.903649]\n",
      "epoch:11 step:9278 [D loss: 0.068220, acc.: 97.66%] [G loss: 3.815546]\n",
      "epoch:11 step:9279 [D loss: 0.065242, acc.: 99.22%] [G loss: 2.516326]\n",
      "epoch:11 step:9280 [D loss: 0.310263, acc.: 85.94%] [G loss: 4.180910]\n",
      "epoch:11 step:9281 [D loss: 0.536543, acc.: 71.09%] [G loss: 1.282665]\n",
      "epoch:11 step:9282 [D loss: 0.490239, acc.: 75.78%] [G loss: 4.071862]\n",
      "epoch:11 step:9283 [D loss: 0.137761, acc.: 95.31%] [G loss: 4.951338]\n",
      "epoch:11 step:9284 [D loss: 0.266998, acc.: 86.72%] [G loss: 3.108775]\n",
      "epoch:11 step:9285 [D loss: 0.169883, acc.: 93.75%] [G loss: 2.792257]\n",
      "epoch:11 step:9286 [D loss: 0.385775, acc.: 82.81%] [G loss: 5.856049]\n",
      "epoch:11 step:9287 [D loss: 1.188889, acc.: 51.56%] [G loss: 2.998867]\n",
      "epoch:11 step:9288 [D loss: 0.111126, acc.: 97.66%] [G loss: 2.670008]\n",
      "epoch:11 step:9289 [D loss: 0.031018, acc.: 100.00%] [G loss: 3.241967]\n",
      "epoch:11 step:9290 [D loss: 0.030825, acc.: 100.00%] [G loss: 2.713558]\n",
      "epoch:11 step:9291 [D loss: 0.062035, acc.: 98.44%] [G loss: 2.351623]\n",
      "epoch:11 step:9292 [D loss: 0.129490, acc.: 96.88%] [G loss: 4.313372]\n",
      "epoch:11 step:9293 [D loss: 0.049685, acc.: 99.22%] [G loss: 3.747567]\n",
      "epoch:11 step:9294 [D loss: 0.158221, acc.: 91.41%] [G loss: 1.514462]\n",
      "epoch:11 step:9295 [D loss: 0.443940, acc.: 75.78%] [G loss: 5.054463]\n",
      "epoch:11 step:9296 [D loss: 0.243865, acc.: 87.50%] [G loss: 5.064642]\n",
      "epoch:11 step:9297 [D loss: 0.019504, acc.: 100.00%] [G loss: 4.526594]\n",
      "epoch:11 step:9298 [D loss: 0.299326, acc.: 85.16%] [G loss: 2.254747]\n",
      "epoch:11 step:9299 [D loss: 0.056345, acc.: 100.00%] [G loss: 3.617275]\n",
      "epoch:11 step:9300 [D loss: 0.020243, acc.: 100.00%] [G loss: 3.349883]\n",
      "epoch:11 step:9301 [D loss: 0.022263, acc.: 100.00%] [G loss: 3.415243]\n",
      "epoch:11 step:9302 [D loss: 0.088428, acc.: 98.44%] [G loss: 3.074206]\n",
      "epoch:11 step:9303 [D loss: 0.050775, acc.: 99.22%] [G loss: 2.824865]\n",
      "epoch:11 step:9304 [D loss: 0.448290, acc.: 83.59%] [G loss: 1.120028]\n",
      "epoch:11 step:9305 [D loss: 0.353667, acc.: 85.16%] [G loss: 5.189044]\n",
      "epoch:11 step:9306 [D loss: 0.983370, acc.: 55.47%] [G loss: 2.854844]\n",
      "epoch:11 step:9307 [D loss: 0.164102, acc.: 92.97%] [G loss: 3.874759]\n",
      "epoch:11 step:9308 [D loss: 0.073494, acc.: 97.66%] [G loss: 4.747309]\n",
      "epoch:11 step:9309 [D loss: 0.114757, acc.: 94.53%] [G loss: 2.986654]\n",
      "epoch:11 step:9310 [D loss: 0.042384, acc.: 99.22%] [G loss: 3.389361]\n",
      "epoch:11 step:9311 [D loss: 0.057317, acc.: 98.44%] [G loss: 3.688324]\n",
      "epoch:11 step:9312 [D loss: 0.023458, acc.: 100.00%] [G loss: 3.578754]\n",
      "epoch:11 step:9313 [D loss: 0.033837, acc.: 100.00%] [G loss: 2.789440]\n",
      "epoch:11 step:9314 [D loss: 0.058516, acc.: 100.00%] [G loss: 2.599387]\n",
      "epoch:11 step:9315 [D loss: 0.101681, acc.: 97.66%] [G loss: 3.483860]\n",
      "epoch:11 step:9316 [D loss: 0.548065, acc.: 75.00%] [G loss: 3.410048]\n",
      "epoch:11 step:9317 [D loss: 0.020827, acc.: 100.00%] [G loss: 5.256473]\n",
      "epoch:11 step:9318 [D loss: 0.109387, acc.: 96.88%] [G loss: 3.259128]\n",
      "epoch:11 step:9319 [D loss: 0.253092, acc.: 89.84%] [G loss: 5.580047]\n",
      "epoch:11 step:9320 [D loss: 0.035462, acc.: 99.22%] [G loss: 5.613402]\n",
      "epoch:11 step:9321 [D loss: 0.137220, acc.: 92.97%] [G loss: 4.512168]\n",
      "epoch:11 step:9322 [D loss: 0.029160, acc.: 100.00%] [G loss: 4.101761]\n",
      "epoch:11 step:9323 [D loss: 0.038800, acc.: 100.00%] [G loss: 3.753510]\n",
      "epoch:11 step:9324 [D loss: 0.020380, acc.: 100.00%] [G loss: 3.486658]\n",
      "epoch:11 step:9325 [D loss: 0.019682, acc.: 100.00%] [G loss: 4.262985]\n",
      "epoch:11 step:9326 [D loss: 0.088162, acc.: 98.44%] [G loss: 3.172113]\n",
      "epoch:11 step:9327 [D loss: 0.070702, acc.: 99.22%] [G loss: 3.751889]\n",
      "epoch:11 step:9328 [D loss: 0.008301, acc.: 100.00%] [G loss: 4.255997]\n",
      "epoch:11 step:9329 [D loss: 0.035715, acc.: 100.00%] [G loss: 4.217980]\n",
      "epoch:11 step:9330 [D loss: 0.269363, acc.: 87.50%] [G loss: 2.813652]\n",
      "epoch:11 step:9331 [D loss: 0.066317, acc.: 98.44%] [G loss: 3.652756]\n",
      "epoch:11 step:9332 [D loss: 0.012252, acc.: 100.00%] [G loss: 3.788446]\n",
      "epoch:11 step:9333 [D loss: 0.027626, acc.: 100.00%] [G loss: 3.070109]\n",
      "epoch:11 step:9334 [D loss: 0.064037, acc.: 99.22%] [G loss: 2.558912]\n",
      "epoch:11 step:9335 [D loss: 0.044015, acc.: 99.22%] [G loss: 3.422361]\n",
      "epoch:11 step:9336 [D loss: 0.159809, acc.: 93.75%] [G loss: 3.239621]\n",
      "epoch:11 step:9337 [D loss: 0.029609, acc.: 100.00%] [G loss: 3.965028]\n",
      "epoch:11 step:9338 [D loss: 1.142555, acc.: 45.31%] [G loss: 6.904613]\n",
      "epoch:11 step:9339 [D loss: 1.363927, acc.: 53.12%] [G loss: 2.366513]\n",
      "epoch:11 step:9340 [D loss: 0.308282, acc.: 86.72%] [G loss: 3.800908]\n",
      "epoch:11 step:9341 [D loss: 0.138116, acc.: 95.31%] [G loss: 4.276608]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:11 step:9342 [D loss: 0.243681, acc.: 90.62%] [G loss: 3.891461]\n",
      "epoch:11 step:9343 [D loss: 0.014310, acc.: 100.00%] [G loss: 4.975571]\n",
      "epoch:11 step:9344 [D loss: 0.072126, acc.: 99.22%] [G loss: 3.563910]\n",
      "epoch:11 step:9345 [D loss: 0.036531, acc.: 100.00%] [G loss: 3.657212]\n",
      "epoch:11 step:9346 [D loss: 0.088471, acc.: 97.66%] [G loss: 3.228013]\n",
      "epoch:11 step:9347 [D loss: 0.036453, acc.: 100.00%] [G loss: 3.363411]\n",
      "epoch:11 step:9348 [D loss: 0.094505, acc.: 98.44%] [G loss: 4.126753]\n",
      "epoch:11 step:9349 [D loss: 0.078140, acc.: 99.22%] [G loss: 2.413741]\n",
      "epoch:11 step:9350 [D loss: 0.083829, acc.: 98.44%] [G loss: 2.872632]\n",
      "epoch:11 step:9351 [D loss: 0.241161, acc.: 90.62%] [G loss: 2.446699]\n",
      "epoch:11 step:9352 [D loss: 0.011212, acc.: 100.00%] [G loss: 4.297425]\n",
      "epoch:11 step:9353 [D loss: 0.312524, acc.: 89.06%] [G loss: 1.398286]\n",
      "epoch:11 step:9354 [D loss: 0.038363, acc.: 99.22%] [G loss: 2.490906]\n",
      "epoch:11 step:9355 [D loss: 0.021224, acc.: 100.00%] [G loss: 3.271101]\n",
      "epoch:11 step:9356 [D loss: 0.023712, acc.: 100.00%] [G loss: 2.780280]\n",
      "epoch:11 step:9357 [D loss: 0.018407, acc.: 100.00%] [G loss: 1.842903]\n",
      "epoch:11 step:9358 [D loss: 0.033911, acc.: 100.00%] [G loss: 1.644290]\n",
      "epoch:11 step:9359 [D loss: 0.173960, acc.: 94.53%] [G loss: 3.029126]\n",
      "epoch:11 step:9360 [D loss: 0.022807, acc.: 100.00%] [G loss: 4.814484]\n",
      "epoch:11 step:9361 [D loss: 0.316899, acc.: 85.94%] [G loss: 2.280647]\n",
      "epoch:11 step:9362 [D loss: 0.224752, acc.: 91.41%] [G loss: 5.009791]\n",
      "epoch:11 step:9363 [D loss: 0.028886, acc.: 100.00%] [G loss: 6.269366]\n",
      "epoch:11 step:9364 [D loss: 0.225359, acc.: 90.62%] [G loss: 3.419136]\n",
      "epoch:11 step:9365 [D loss: 0.089950, acc.: 97.66%] [G loss: 4.481674]\n",
      "epoch:11 step:9366 [D loss: 0.011155, acc.: 100.00%] [G loss: 4.435651]\n",
      "epoch:11 step:9367 [D loss: 0.019446, acc.: 100.00%] [G loss: 3.103142]\n",
      "epoch:11 step:9368 [D loss: 0.222345, acc.: 94.53%] [G loss: 3.530294]\n",
      "epoch:11 step:9369 [D loss: 0.012723, acc.: 100.00%] [G loss: 3.045185]\n",
      "epoch:11 step:9370 [D loss: 2.632113, acc.: 11.72%] [G loss: 7.053504]\n",
      "epoch:11 step:9371 [D loss: 0.997032, acc.: 58.59%] [G loss: 3.801663]\n",
      "epoch:11 step:9372 [D loss: 0.290875, acc.: 89.06%] [G loss: 4.955208]\n",
      "epoch:12 step:9373 [D loss: 0.143370, acc.: 96.88%] [G loss: 4.101567]\n",
      "epoch:12 step:9374 [D loss: 0.273343, acc.: 87.50%] [G loss: 2.749531]\n",
      "epoch:12 step:9375 [D loss: 0.326001, acc.: 85.94%] [G loss: 4.184613]\n",
      "epoch:12 step:9376 [D loss: 0.051723, acc.: 98.44%] [G loss: 4.750984]\n",
      "epoch:12 step:9377 [D loss: 0.284535, acc.: 89.06%] [G loss: 2.495557]\n",
      "epoch:12 step:9378 [D loss: 0.089139, acc.: 96.09%] [G loss: 3.426506]\n",
      "epoch:12 step:9379 [D loss: 0.142854, acc.: 96.09%] [G loss: 3.414221]\n",
      "epoch:12 step:9380 [D loss: 0.031899, acc.: 100.00%] [G loss: 3.172146]\n",
      "epoch:12 step:9381 [D loss: 0.250810, acc.: 88.28%] [G loss: 0.844536]\n",
      "epoch:12 step:9382 [D loss: 0.847670, acc.: 64.06%] [G loss: 6.367289]\n",
      "epoch:12 step:9383 [D loss: 0.908628, acc.: 60.94%] [G loss: 5.032702]\n",
      "epoch:12 step:9384 [D loss: 0.170363, acc.: 96.09%] [G loss: 3.766362]\n",
      "epoch:12 step:9385 [D loss: 0.089349, acc.: 98.44%] [G loss: 3.832855]\n",
      "epoch:12 step:9386 [D loss: 0.064737, acc.: 99.22%] [G loss: 3.883396]\n",
      "epoch:12 step:9387 [D loss: 0.045725, acc.: 100.00%] [G loss: 3.589022]\n",
      "epoch:12 step:9388 [D loss: 0.047238, acc.: 100.00%] [G loss: 3.609229]\n",
      "epoch:12 step:9389 [D loss: 0.104626, acc.: 98.44%] [G loss: 3.200539]\n",
      "epoch:12 step:9390 [D loss: 0.050599, acc.: 99.22%] [G loss: 3.748579]\n",
      "epoch:12 step:9391 [D loss: 0.063587, acc.: 99.22%] [G loss: 3.336567]\n",
      "epoch:12 step:9392 [D loss: 0.186529, acc.: 95.31%] [G loss: 3.193804]\n",
      "epoch:12 step:9393 [D loss: 0.064515, acc.: 99.22%] [G loss: 3.519850]\n",
      "epoch:12 step:9394 [D loss: 0.629807, acc.: 71.88%] [G loss: 5.334779]\n",
      "epoch:12 step:9395 [D loss: 0.713287, acc.: 64.84%] [G loss: 2.810681]\n",
      "epoch:12 step:9396 [D loss: 0.091953, acc.: 97.66%] [G loss: 2.614924]\n",
      "epoch:12 step:9397 [D loss: 0.030768, acc.: 100.00%] [G loss: 2.938782]\n",
      "epoch:12 step:9398 [D loss: 0.175685, acc.: 96.09%] [G loss: 4.224842]\n",
      "epoch:12 step:9399 [D loss: 0.013530, acc.: 100.00%] [G loss: 4.187222]\n",
      "epoch:12 step:9400 [D loss: 0.199661, acc.: 89.84%] [G loss: 2.242022]\n",
      "##############\n",
      "[1.11700364 1.03292861 1.03300521 0.9121842  2.11066835 1.04224442\n",
      " 1.05623053 1.10822289 2.12234293 1.05720171]\n",
      "##########\n",
      "epoch:12 step:9401 [D loss: 0.044190, acc.: 100.00%] [G loss: 1.207615]\n",
      "epoch:12 step:9402 [D loss: 0.086732, acc.: 98.44%] [G loss: 2.469127]\n",
      "epoch:12 step:9403 [D loss: 0.063497, acc.: 99.22%] [G loss: 3.261547]\n",
      "epoch:12 step:9404 [D loss: 0.061927, acc.: 98.44%] [G loss: 2.997706]\n",
      "epoch:12 step:9405 [D loss: 0.189744, acc.: 95.31%] [G loss: 3.526393]\n",
      "epoch:12 step:9406 [D loss: 0.006574, acc.: 100.00%] [G loss: 3.663324]\n",
      "epoch:12 step:9407 [D loss: 0.112785, acc.: 96.09%] [G loss: 1.898980]\n",
      "epoch:12 step:9408 [D loss: 0.283639, acc.: 89.06%] [G loss: 5.132699]\n",
      "epoch:12 step:9409 [D loss: 0.247779, acc.: 85.94%] [G loss: 3.627911]\n",
      "epoch:12 step:9410 [D loss: 0.024709, acc.: 100.00%] [G loss: 3.550687]\n",
      "epoch:12 step:9411 [D loss: 0.026393, acc.: 100.00%] [G loss: 1.295066]\n",
      "epoch:12 step:9412 [D loss: 0.099332, acc.: 96.88%] [G loss: 3.103142]\n",
      "epoch:12 step:9413 [D loss: 0.020901, acc.: 100.00%] [G loss: 3.575902]\n",
      "epoch:12 step:9414 [D loss: 0.261509, acc.: 91.41%] [G loss: 4.186353]\n",
      "epoch:12 step:9415 [D loss: 0.022806, acc.: 100.00%] [G loss: 5.078107]\n",
      "epoch:12 step:9416 [D loss: 0.285713, acc.: 86.72%] [G loss: 2.628516]\n",
      "epoch:12 step:9417 [D loss: 0.155621, acc.: 96.09%] [G loss: 3.999994]\n",
      "epoch:12 step:9418 [D loss: 0.078752, acc.: 98.44%] [G loss: 4.633881]\n",
      "epoch:12 step:9419 [D loss: 0.017639, acc.: 100.00%] [G loss: 2.934762]\n",
      "epoch:12 step:9420 [D loss: 0.055803, acc.: 98.44%] [G loss: 3.692972]\n",
      "epoch:12 step:9421 [D loss: 0.065059, acc.: 99.22%] [G loss: 2.548739]\n",
      "epoch:12 step:9422 [D loss: 0.309674, acc.: 85.16%] [G loss: 5.715845]\n",
      "epoch:12 step:9423 [D loss: 0.207812, acc.: 91.41%] [G loss: 4.327308]\n",
      "epoch:12 step:9424 [D loss: 0.086041, acc.: 97.66%] [G loss: 1.957045]\n",
      "epoch:12 step:9425 [D loss: 0.247661, acc.: 89.06%] [G loss: 5.823686]\n",
      "epoch:12 step:9426 [D loss: 0.356112, acc.: 79.69%] [G loss: 3.047023]\n",
      "epoch:12 step:9427 [D loss: 0.254463, acc.: 89.84%] [G loss: 6.028783]\n",
      "epoch:12 step:9428 [D loss: 0.102449, acc.: 96.09%] [G loss: 5.896985]\n",
      "epoch:12 step:9429 [D loss: 0.103562, acc.: 95.31%] [G loss: 4.632214]\n",
      "epoch:12 step:9430 [D loss: 0.020090, acc.: 100.00%] [G loss: 3.064761]\n",
      "epoch:12 step:9431 [D loss: 0.036438, acc.: 100.00%] [G loss: 4.625503]\n",
      "epoch:12 step:9432 [D loss: 0.011884, acc.: 100.00%] [G loss: 4.800019]\n",
      "epoch:12 step:9433 [D loss: 0.029482, acc.: 100.00%] [G loss: 3.924691]\n",
      "epoch:12 step:9434 [D loss: 0.043634, acc.: 100.00%] [G loss: 2.928717]\n",
      "epoch:12 step:9435 [D loss: 0.041276, acc.: 99.22%] [G loss: 3.657901]\n",
      "epoch:12 step:9436 [D loss: 0.222684, acc.: 92.19%] [G loss: 5.005120]\n",
      "epoch:12 step:9437 [D loss: 0.506731, acc.: 73.44%] [G loss: 3.016253]\n",
      "epoch:12 step:9438 [D loss: 0.030130, acc.: 100.00%] [G loss: 3.786291]\n",
      "epoch:12 step:9439 [D loss: 0.013686, acc.: 100.00%] [G loss: 3.433684]\n",
      "epoch:12 step:9440 [D loss: 0.089686, acc.: 99.22%] [G loss: 4.248995]\n",
      "epoch:12 step:9441 [D loss: 0.032424, acc.: 100.00%] [G loss: 4.783411]\n",
      "epoch:12 step:9442 [D loss: 0.034036, acc.: 100.00%] [G loss: 4.813780]\n",
      "epoch:12 step:9443 [D loss: 0.051658, acc.: 99.22%] [G loss: 4.301794]\n",
      "epoch:12 step:9444 [D loss: 0.023479, acc.: 100.00%] [G loss: 4.347133]\n",
      "epoch:12 step:9445 [D loss: 0.071840, acc.: 99.22%] [G loss: 3.999043]\n",
      "epoch:12 step:9446 [D loss: 0.013987, acc.: 100.00%] [G loss: 4.815854]\n",
      "epoch:12 step:9447 [D loss: 0.073973, acc.: 99.22%] [G loss: 3.157845]\n",
      "epoch:12 step:9448 [D loss: 0.059370, acc.: 99.22%] [G loss: 3.516722]\n",
      "epoch:12 step:9449 [D loss: 0.541437, acc.: 69.53%] [G loss: 5.545032]\n",
      "epoch:12 step:9450 [D loss: 0.258795, acc.: 86.72%] [G loss: 4.677983]\n",
      "epoch:12 step:9451 [D loss: 0.029690, acc.: 100.00%] [G loss: 4.730203]\n",
      "epoch:12 step:9452 [D loss: 0.021710, acc.: 100.00%] [G loss: 3.675822]\n",
      "epoch:12 step:9453 [D loss: 0.039127, acc.: 99.22%] [G loss: 3.149591]\n",
      "epoch:12 step:9454 [D loss: 0.025575, acc.: 100.00%] [G loss: 4.152996]\n",
      "epoch:12 step:9455 [D loss: 0.131985, acc.: 96.88%] [G loss: 4.332999]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:12 step:9456 [D loss: 0.974675, acc.: 53.91%] [G loss: 8.223286]\n",
      "epoch:12 step:9457 [D loss: 2.101096, acc.: 50.00%] [G loss: 4.146598]\n",
      "epoch:12 step:9458 [D loss: 0.393175, acc.: 85.94%] [G loss: 4.645777]\n",
      "epoch:12 step:9459 [D loss: 0.232076, acc.: 92.19%] [G loss: 3.926474]\n",
      "epoch:12 step:9460 [D loss: 0.107166, acc.: 98.44%] [G loss: 2.147188]\n",
      "epoch:12 step:9461 [D loss: 0.187111, acc.: 90.62%] [G loss: 4.820476]\n",
      "epoch:12 step:9462 [D loss: 0.996351, acc.: 55.47%] [G loss: 2.966401]\n",
      "epoch:12 step:9463 [D loss: 0.182072, acc.: 92.19%] [G loss: 3.567706]\n",
      "epoch:12 step:9464 [D loss: 0.111134, acc.: 97.66%] [G loss: 4.191742]\n",
      "epoch:12 step:9465 [D loss: 0.424999, acc.: 79.69%] [G loss: 4.181536]\n",
      "epoch:12 step:9466 [D loss: 0.084842, acc.: 96.09%] [G loss: 5.207016]\n",
      "epoch:12 step:9467 [D loss: 0.035381, acc.: 100.00%] [G loss: 4.543272]\n",
      "epoch:12 step:9468 [D loss: 0.088129, acc.: 97.66%] [G loss: 3.569848]\n",
      "epoch:12 step:9469 [D loss: 0.063734, acc.: 99.22%] [G loss: 3.198370]\n",
      "epoch:12 step:9470 [D loss: 0.062928, acc.: 99.22%] [G loss: 3.751665]\n",
      "epoch:12 step:9471 [D loss: 0.095291, acc.: 99.22%] [G loss: 3.249457]\n",
      "epoch:12 step:9472 [D loss: 0.160787, acc.: 95.31%] [G loss: 4.635960]\n",
      "epoch:12 step:9473 [D loss: 0.067201, acc.: 98.44%] [G loss: 4.377633]\n",
      "epoch:12 step:9474 [D loss: 0.044862, acc.: 100.00%] [G loss: 3.192731]\n",
      "epoch:12 step:9475 [D loss: 0.196620, acc.: 94.53%] [G loss: 3.274009]\n",
      "epoch:12 step:9476 [D loss: 0.063458, acc.: 99.22%] [G loss: 3.342600]\n",
      "epoch:12 step:9477 [D loss: 0.066952, acc.: 98.44%] [G loss: 3.309637]\n",
      "epoch:12 step:9478 [D loss: 0.022822, acc.: 100.00%] [G loss: 2.418736]\n",
      "epoch:12 step:9479 [D loss: 0.204294, acc.: 95.31%] [G loss: 4.143807]\n",
      "epoch:12 step:9480 [D loss: 0.292247, acc.: 87.50%] [G loss: 2.355550]\n",
      "epoch:12 step:9481 [D loss: 0.096377, acc.: 97.66%] [G loss: 4.186371]\n",
      "epoch:12 step:9482 [D loss: 0.048360, acc.: 99.22%] [G loss: 3.409079]\n",
      "epoch:12 step:9483 [D loss: 0.128790, acc.: 96.88%] [G loss: 2.042495]\n",
      "epoch:12 step:9484 [D loss: 0.098480, acc.: 98.44%] [G loss: 4.171580]\n",
      "epoch:12 step:9485 [D loss: 0.015069, acc.: 100.00%] [G loss: 5.106727]\n",
      "epoch:12 step:9486 [D loss: 0.067746, acc.: 97.66%] [G loss: 2.225764]\n",
      "epoch:12 step:9487 [D loss: 0.029998, acc.: 100.00%] [G loss: 2.320305]\n",
      "epoch:12 step:9488 [D loss: 0.754220, acc.: 63.28%] [G loss: 7.623178]\n",
      "epoch:12 step:9489 [D loss: 0.789824, acc.: 62.50%] [G loss: 4.893102]\n",
      "epoch:12 step:9490 [D loss: 0.073551, acc.: 98.44%] [G loss: 4.628718]\n",
      "epoch:12 step:9491 [D loss: 0.020784, acc.: 100.00%] [G loss: 4.946321]\n",
      "epoch:12 step:9492 [D loss: 0.018374, acc.: 100.00%] [G loss: 4.729307]\n",
      "epoch:12 step:9493 [D loss: 0.035984, acc.: 99.22%] [G loss: 3.669229]\n",
      "epoch:12 step:9494 [D loss: 0.117917, acc.: 98.44%] [G loss: 4.803285]\n",
      "epoch:12 step:9495 [D loss: 0.085629, acc.: 97.66%] [G loss: 2.804580]\n",
      "epoch:12 step:9496 [D loss: 0.068350, acc.: 98.44%] [G loss: 3.907538]\n",
      "epoch:12 step:9497 [D loss: 0.040671, acc.: 100.00%] [G loss: 4.481440]\n",
      "epoch:12 step:9498 [D loss: 0.017998, acc.: 100.00%] [G loss: 3.823781]\n",
      "epoch:12 step:9499 [D loss: 0.033461, acc.: 100.00%] [G loss: 3.282965]\n",
      "epoch:12 step:9500 [D loss: 0.597514, acc.: 69.53%] [G loss: 6.064126]\n",
      "epoch:12 step:9501 [D loss: 0.811442, acc.: 65.62%] [G loss: 3.318348]\n",
      "epoch:12 step:9502 [D loss: 0.077449, acc.: 96.88%] [G loss: 4.227250]\n",
      "epoch:12 step:9503 [D loss: 0.010228, acc.: 100.00%] [G loss: 4.622557]\n",
      "epoch:12 step:9504 [D loss: 1.165968, acc.: 46.88%] [G loss: 5.141715]\n",
      "epoch:12 step:9505 [D loss: 0.006176, acc.: 100.00%] [G loss: 7.479734]\n",
      "epoch:12 step:9506 [D loss: 0.426624, acc.: 82.03%] [G loss: 4.533637]\n",
      "epoch:12 step:9507 [D loss: 0.034146, acc.: 100.00%] [G loss: 1.857866]\n",
      "epoch:12 step:9508 [D loss: 0.060176, acc.: 97.66%] [G loss: 3.303186]\n",
      "epoch:12 step:9509 [D loss: 0.037080, acc.: 100.00%] [G loss: 3.047595]\n",
      "epoch:12 step:9510 [D loss: 0.020951, acc.: 100.00%] [G loss: 3.247748]\n",
      "epoch:12 step:9511 [D loss: 0.020090, acc.: 100.00%] [G loss: 3.446171]\n",
      "epoch:12 step:9512 [D loss: 0.016041, acc.: 100.00%] [G loss: 2.725991]\n",
      "epoch:12 step:9513 [D loss: 0.056208, acc.: 98.44%] [G loss: 2.277515]\n",
      "epoch:12 step:9514 [D loss: 0.008991, acc.: 100.00%] [G loss: 2.259873]\n",
      "epoch:12 step:9515 [D loss: 0.011637, acc.: 100.00%] [G loss: 0.360370]\n",
      "epoch:12 step:9516 [D loss: 0.227349, acc.: 91.41%] [G loss: 3.415881]\n",
      "epoch:12 step:9517 [D loss: 0.149408, acc.: 92.19%] [G loss: 2.724217]\n",
      "epoch:12 step:9518 [D loss: 0.080737, acc.: 99.22%] [G loss: 0.880142]\n",
      "epoch:12 step:9519 [D loss: 0.482649, acc.: 73.44%] [G loss: 6.414914]\n",
      "epoch:12 step:9520 [D loss: 1.261974, acc.: 53.12%] [G loss: 2.983858]\n",
      "epoch:12 step:9521 [D loss: 0.064495, acc.: 97.66%] [G loss: 1.937446]\n",
      "epoch:12 step:9522 [D loss: 0.580324, acc.: 68.75%] [G loss: 5.723692]\n",
      "epoch:12 step:9523 [D loss: 0.723896, acc.: 60.94%] [G loss: 4.311936]\n",
      "epoch:12 step:9524 [D loss: 0.067079, acc.: 97.66%] [G loss: 3.832686]\n",
      "epoch:12 step:9525 [D loss: 0.112941, acc.: 96.09%] [G loss: 4.520412]\n",
      "epoch:12 step:9526 [D loss: 0.010340, acc.: 100.00%] [G loss: 4.986542]\n",
      "epoch:12 step:9527 [D loss: 0.054593, acc.: 98.44%] [G loss: 4.153195]\n",
      "epoch:12 step:9528 [D loss: 0.120390, acc.: 97.66%] [G loss: 4.360115]\n",
      "epoch:12 step:9529 [D loss: 0.080728, acc.: 98.44%] [G loss: 3.388474]\n",
      "epoch:12 step:9530 [D loss: 0.348208, acc.: 84.38%] [G loss: 4.784752]\n",
      "epoch:12 step:9531 [D loss: 0.009931, acc.: 100.00%] [G loss: 4.580430]\n",
      "epoch:12 step:9532 [D loss: 0.125082, acc.: 96.09%] [G loss: 2.930252]\n",
      "epoch:12 step:9533 [D loss: 0.132056, acc.: 97.66%] [G loss: 3.096117]\n",
      "epoch:12 step:9534 [D loss: 0.066584, acc.: 98.44%] [G loss: 4.671001]\n",
      "epoch:12 step:9535 [D loss: 0.037973, acc.: 99.22%] [G loss: 5.118143]\n",
      "epoch:12 step:9536 [D loss: 0.046929, acc.: 100.00%] [G loss: 3.881059]\n",
      "epoch:12 step:9537 [D loss: 0.028006, acc.: 100.00%] [G loss: 3.691575]\n",
      "epoch:12 step:9538 [D loss: 0.028465, acc.: 100.00%] [G loss: 2.908536]\n",
      "epoch:12 step:9539 [D loss: 0.044985, acc.: 99.22%] [G loss: 1.829359]\n",
      "epoch:12 step:9540 [D loss: 0.037857, acc.: 100.00%] [G loss: 2.451094]\n",
      "epoch:12 step:9541 [D loss: 0.027952, acc.: 100.00%] [G loss: 3.359884]\n",
      "epoch:12 step:9542 [D loss: 0.113135, acc.: 97.66%] [G loss: 1.938836]\n",
      "epoch:12 step:9543 [D loss: 0.057577, acc.: 99.22%] [G loss: 0.985083]\n",
      "epoch:12 step:9544 [D loss: 0.016270, acc.: 100.00%] [G loss: 0.695020]\n",
      "epoch:12 step:9545 [D loss: 0.572421, acc.: 71.09%] [G loss: 6.208453]\n",
      "epoch:12 step:9546 [D loss: 0.940243, acc.: 59.38%] [G loss: 3.996838]\n",
      "epoch:12 step:9547 [D loss: 0.130157, acc.: 96.88%] [G loss: 3.046578]\n",
      "epoch:12 step:9548 [D loss: 0.158394, acc.: 95.31%] [G loss: 4.481303]\n",
      "epoch:12 step:9549 [D loss: 0.094331, acc.: 96.88%] [G loss: 4.291325]\n",
      "epoch:12 step:9550 [D loss: 0.048777, acc.: 100.00%] [G loss: 3.336265]\n",
      "epoch:12 step:9551 [D loss: 0.212897, acc.: 91.41%] [G loss: 0.779065]\n",
      "epoch:12 step:9552 [D loss: 0.569996, acc.: 70.31%] [G loss: 5.516175]\n",
      "epoch:12 step:9553 [D loss: 0.126294, acc.: 96.09%] [G loss: 4.613767]\n",
      "epoch:12 step:9554 [D loss: 0.426351, acc.: 79.69%] [G loss: 0.653723]\n",
      "epoch:12 step:9555 [D loss: 0.712543, acc.: 69.53%] [G loss: 6.029688]\n",
      "epoch:12 step:9556 [D loss: 0.764075, acc.: 64.06%] [G loss: 4.149236]\n",
      "epoch:12 step:9557 [D loss: 0.041747, acc.: 99.22%] [G loss: 3.738863]\n",
      "epoch:12 step:9558 [D loss: 0.044104, acc.: 100.00%] [G loss: 2.401089]\n",
      "epoch:12 step:9559 [D loss: 0.266592, acc.: 89.84%] [G loss: 3.540151]\n",
      "epoch:12 step:9560 [D loss: 0.039751, acc.: 99.22%] [G loss: 4.027786]\n",
      "epoch:12 step:9561 [D loss: 0.059109, acc.: 98.44%] [G loss: 4.190372]\n",
      "epoch:12 step:9562 [D loss: 0.182531, acc.: 94.53%] [G loss: 3.676450]\n",
      "epoch:12 step:9563 [D loss: 0.109095, acc.: 97.66%] [G loss: 2.487811]\n",
      "epoch:12 step:9564 [D loss: 0.109661, acc.: 96.88%] [G loss: 4.237463]\n",
      "epoch:12 step:9565 [D loss: 0.068102, acc.: 99.22%] [G loss: 4.229815]\n",
      "epoch:12 step:9566 [D loss: 0.060675, acc.: 99.22%] [G loss: 2.774192]\n",
      "epoch:12 step:9567 [D loss: 0.185338, acc.: 92.97%] [G loss: 4.133535]\n",
      "epoch:12 step:9568 [D loss: 0.085460, acc.: 96.88%] [G loss: 3.412379]\n",
      "epoch:12 step:9569 [D loss: 0.047224, acc.: 100.00%] [G loss: 2.864664]\n",
      "epoch:12 step:9570 [D loss: 0.049060, acc.: 99.22%] [G loss: 2.298773]\n",
      "epoch:12 step:9571 [D loss: 0.037109, acc.: 99.22%] [G loss: 2.895785]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:12 step:9572 [D loss: 0.051969, acc.: 100.00%] [G loss: 0.953634]\n",
      "epoch:12 step:9573 [D loss: 0.092663, acc.: 98.44%] [G loss: 2.275170]\n",
      "epoch:12 step:9574 [D loss: 0.039801, acc.: 100.00%] [G loss: 0.589757]\n",
      "epoch:12 step:9575 [D loss: 0.058926, acc.: 100.00%] [G loss: 1.576221]\n",
      "epoch:12 step:9576 [D loss: 0.049725, acc.: 98.44%] [G loss: 0.891056]\n",
      "epoch:12 step:9577 [D loss: 1.363788, acc.: 43.75%] [G loss: 7.000128]\n",
      "epoch:12 step:9578 [D loss: 1.348737, acc.: 53.12%] [G loss: 5.483297]\n",
      "epoch:12 step:9579 [D loss: 0.449771, acc.: 75.00%] [G loss: 2.038668]\n",
      "epoch:12 step:9580 [D loss: 0.201783, acc.: 92.97%] [G loss: 2.159053]\n",
      "epoch:12 step:9581 [D loss: 0.056997, acc.: 99.22%] [G loss: 3.091763]\n",
      "epoch:12 step:9582 [D loss: 0.032355, acc.: 100.00%] [G loss: 3.908586]\n",
      "epoch:12 step:9583 [D loss: 0.045081, acc.: 100.00%] [G loss: 2.727308]\n",
      "epoch:12 step:9584 [D loss: 0.247017, acc.: 90.62%] [G loss: 4.137096]\n",
      "epoch:12 step:9585 [D loss: 0.891403, acc.: 50.00%] [G loss: 4.002127]\n",
      "epoch:12 step:9586 [D loss: 0.119395, acc.: 96.09%] [G loss: 4.010921]\n",
      "epoch:12 step:9587 [D loss: 0.078433, acc.: 99.22%] [G loss: 3.642008]\n",
      "epoch:12 step:9588 [D loss: 0.085481, acc.: 100.00%] [G loss: 3.916278]\n",
      "epoch:12 step:9589 [D loss: 0.106341, acc.: 99.22%] [G loss: 3.564651]\n",
      "epoch:12 step:9590 [D loss: 0.072581, acc.: 97.66%] [G loss: 3.091712]\n",
      "epoch:12 step:9591 [D loss: 0.121383, acc.: 96.09%] [G loss: 3.457691]\n",
      "epoch:12 step:9592 [D loss: 0.129768, acc.: 96.09%] [G loss: 3.352184]\n",
      "epoch:12 step:9593 [D loss: 0.047004, acc.: 99.22%] [G loss: 3.091868]\n",
      "epoch:12 step:9594 [D loss: 0.058880, acc.: 99.22%] [G loss: 2.771800]\n",
      "epoch:12 step:9595 [D loss: 0.085251, acc.: 98.44%] [G loss: 2.810766]\n",
      "epoch:12 step:9596 [D loss: 0.173415, acc.: 94.53%] [G loss: 4.445398]\n",
      "epoch:12 step:9597 [D loss: 0.061017, acc.: 99.22%] [G loss: 4.396841]\n",
      "epoch:12 step:9598 [D loss: 0.263101, acc.: 86.72%] [G loss: 1.843576]\n",
      "epoch:12 step:9599 [D loss: 0.144060, acc.: 96.09%] [G loss: 4.743541]\n",
      "epoch:12 step:9600 [D loss: 0.104464, acc.: 97.66%] [G loss: 4.558794]\n",
      "##############\n",
      "[1.09089807 0.90969596 1.10393943 0.88122267 2.11231334 0.91721484\n",
      " 2.11435783 1.11898097 1.10562757 1.04857224]\n",
      "##########\n",
      "epoch:12 step:9601 [D loss: 0.153642, acc.: 96.09%] [G loss: 3.039968]\n",
      "epoch:12 step:9602 [D loss: 0.060374, acc.: 100.00%] [G loss: 3.406879]\n",
      "epoch:12 step:9603 [D loss: 0.080234, acc.: 98.44%] [G loss: 4.487329]\n",
      "epoch:12 step:9604 [D loss: 0.215492, acc.: 92.19%] [G loss: 4.250023]\n",
      "epoch:12 step:9605 [D loss: 0.077388, acc.: 97.66%] [G loss: 4.103221]\n",
      "epoch:12 step:9606 [D loss: 0.275293, acc.: 89.06%] [G loss: 4.178013]\n",
      "epoch:12 step:9607 [D loss: 0.063693, acc.: 98.44%] [G loss: 4.899734]\n",
      "epoch:12 step:9608 [D loss: 0.150315, acc.: 94.53%] [G loss: 3.874464]\n",
      "epoch:12 step:9609 [D loss: 0.070278, acc.: 100.00%] [G loss: 3.862068]\n",
      "epoch:12 step:9610 [D loss: 0.041008, acc.: 100.00%] [G loss: 4.830006]\n",
      "epoch:12 step:9611 [D loss: 0.017075, acc.: 100.00%] [G loss: 4.827752]\n",
      "epoch:12 step:9612 [D loss: 0.905950, acc.: 56.25%] [G loss: 4.889872]\n",
      "epoch:12 step:9613 [D loss: 0.034547, acc.: 100.00%] [G loss: 5.757031]\n",
      "epoch:12 step:9614 [D loss: 0.525564, acc.: 76.56%] [G loss: 2.313450]\n",
      "epoch:12 step:9615 [D loss: 0.238237, acc.: 89.06%] [G loss: 4.942863]\n",
      "epoch:12 step:9616 [D loss: 0.010817, acc.: 100.00%] [G loss: 5.752200]\n",
      "epoch:12 step:9617 [D loss: 0.111696, acc.: 96.09%] [G loss: 4.717352]\n",
      "epoch:12 step:9618 [D loss: 0.048741, acc.: 98.44%] [G loss: 3.275303]\n",
      "epoch:12 step:9619 [D loss: 0.076964, acc.: 97.66%] [G loss: 3.174477]\n",
      "epoch:12 step:9620 [D loss: 0.048030, acc.: 100.00%] [G loss: 4.554674]\n",
      "epoch:12 step:9621 [D loss: 0.047537, acc.: 99.22%] [G loss: 4.083245]\n",
      "epoch:12 step:9622 [D loss: 0.183416, acc.: 90.62%] [G loss: 2.946183]\n",
      "epoch:12 step:9623 [D loss: 0.017502, acc.: 100.00%] [G loss: 4.070091]\n",
      "epoch:12 step:9624 [D loss: 0.128433, acc.: 97.66%] [G loss: 3.778048]\n",
      "epoch:12 step:9625 [D loss: 0.055615, acc.: 97.66%] [G loss: 3.061165]\n",
      "epoch:12 step:9626 [D loss: 0.032471, acc.: 100.00%] [G loss: 4.070736]\n",
      "epoch:12 step:9627 [D loss: 0.051518, acc.: 100.00%] [G loss: 3.791314]\n",
      "epoch:12 step:9628 [D loss: 0.123170, acc.: 96.88%] [G loss: 2.881886]\n",
      "epoch:12 step:9629 [D loss: 0.044950, acc.: 100.00%] [G loss: 3.981678]\n",
      "epoch:12 step:9630 [D loss: 0.041044, acc.: 100.00%] [G loss: 3.817009]\n",
      "epoch:12 step:9631 [D loss: 0.064676, acc.: 99.22%] [G loss: 3.365735]\n",
      "epoch:12 step:9632 [D loss: 0.131838, acc.: 97.66%] [G loss: 2.795125]\n",
      "epoch:12 step:9633 [D loss: 0.141320, acc.: 96.09%] [G loss: 3.402522]\n",
      "epoch:12 step:9634 [D loss: 0.034486, acc.: 100.00%] [G loss: 4.114635]\n",
      "epoch:12 step:9635 [D loss: 0.043443, acc.: 99.22%] [G loss: 3.815529]\n",
      "epoch:12 step:9636 [D loss: 3.200797, acc.: 23.44%] [G loss: 7.004526]\n",
      "epoch:12 step:9637 [D loss: 2.293549, acc.: 50.00%] [G loss: 5.545506]\n",
      "epoch:12 step:9638 [D loss: 1.408038, acc.: 50.78%] [G loss: 2.346845]\n",
      "epoch:12 step:9639 [D loss: 0.169385, acc.: 92.97%] [G loss: 2.184822]\n",
      "epoch:12 step:9640 [D loss: 0.095220, acc.: 99.22%] [G loss: 2.279763]\n",
      "epoch:12 step:9641 [D loss: 0.059484, acc.: 100.00%] [G loss: 3.111409]\n",
      "epoch:12 step:9642 [D loss: 0.238128, acc.: 92.97%] [G loss: 2.646858]\n",
      "epoch:12 step:9643 [D loss: 0.094294, acc.: 98.44%] [G loss: 2.825852]\n",
      "epoch:12 step:9644 [D loss: 0.062097, acc.: 100.00%] [G loss: 3.052410]\n",
      "epoch:12 step:9645 [D loss: 0.240961, acc.: 92.97%] [G loss: 2.382551]\n",
      "epoch:12 step:9646 [D loss: 0.101749, acc.: 99.22%] [G loss: 2.553527]\n",
      "epoch:12 step:9647 [D loss: 0.068755, acc.: 99.22%] [G loss: 2.415183]\n",
      "epoch:12 step:9648 [D loss: 0.137868, acc.: 96.88%] [G loss: 2.690250]\n",
      "epoch:12 step:9649 [D loss: 0.159362, acc.: 97.66%] [G loss: 2.209311]\n",
      "epoch:12 step:9650 [D loss: 0.178791, acc.: 92.97%] [G loss: 3.122627]\n",
      "epoch:12 step:9651 [D loss: 0.187287, acc.: 94.53%] [G loss: 2.924300]\n",
      "epoch:12 step:9652 [D loss: 0.166187, acc.: 96.09%] [G loss: 2.920455]\n",
      "epoch:12 step:9653 [D loss: 0.149532, acc.: 97.66%] [G loss: 2.994751]\n",
      "epoch:12 step:9654 [D loss: 0.111548, acc.: 97.66%] [G loss: 3.200591]\n",
      "epoch:12 step:9655 [D loss: 0.067006, acc.: 100.00%] [G loss: 3.573210]\n",
      "epoch:12 step:9656 [D loss: 0.100554, acc.: 98.44%] [G loss: 3.214948]\n",
      "epoch:12 step:9657 [D loss: 0.095579, acc.: 96.09%] [G loss: 4.162329]\n",
      "epoch:12 step:9658 [D loss: 0.616020, acc.: 67.19%] [G loss: 2.987258]\n",
      "epoch:12 step:9659 [D loss: 0.041925, acc.: 100.00%] [G loss: 3.591490]\n",
      "epoch:12 step:9660 [D loss: 0.081236, acc.: 99.22%] [G loss: 3.557664]\n",
      "epoch:12 step:9661 [D loss: 0.078111, acc.: 99.22%] [G loss: 2.609807]\n",
      "epoch:12 step:9662 [D loss: 0.082853, acc.: 98.44%] [G loss: 3.027809]\n",
      "epoch:12 step:9663 [D loss: 0.038274, acc.: 100.00%] [G loss: 3.382407]\n",
      "epoch:12 step:9664 [D loss: 0.064470, acc.: 100.00%] [G loss: 2.083972]\n",
      "epoch:12 step:9665 [D loss: 0.169834, acc.: 93.75%] [G loss: 4.078714]\n",
      "epoch:12 step:9666 [D loss: 0.068311, acc.: 99.22%] [G loss: 3.777107]\n",
      "epoch:12 step:9667 [D loss: 0.255743, acc.: 89.84%] [G loss: 3.152957]\n",
      "epoch:12 step:9668 [D loss: 0.038889, acc.: 100.00%] [G loss: 2.644632]\n",
      "epoch:12 step:9669 [D loss: 0.045613, acc.: 100.00%] [G loss: 3.490417]\n",
      "epoch:12 step:9670 [D loss: 0.060654, acc.: 99.22%] [G loss: 3.375314]\n",
      "epoch:12 step:9671 [D loss: 0.152882, acc.: 96.88%] [G loss: 4.006758]\n",
      "epoch:12 step:9672 [D loss: 0.196047, acc.: 91.41%] [G loss: 1.185309]\n",
      "epoch:12 step:9673 [D loss: 0.159390, acc.: 94.53%] [G loss: 4.398704]\n",
      "epoch:12 step:9674 [D loss: 0.029734, acc.: 100.00%] [G loss: 5.222854]\n",
      "epoch:12 step:9675 [D loss: 0.223985, acc.: 91.41%] [G loss: 1.760129]\n",
      "epoch:12 step:9676 [D loss: 0.142352, acc.: 96.09%] [G loss: 4.281319]\n",
      "epoch:12 step:9677 [D loss: 0.045979, acc.: 100.00%] [G loss: 5.100365]\n",
      "epoch:12 step:9678 [D loss: 0.021205, acc.: 100.00%] [G loss: 4.489928]\n",
      "epoch:12 step:9679 [D loss: 0.061164, acc.: 99.22%] [G loss: 4.337105]\n",
      "epoch:12 step:9680 [D loss: 0.019728, acc.: 100.00%] [G loss: 3.848760]\n",
      "epoch:12 step:9681 [D loss: 0.101956, acc.: 97.66%] [G loss: 2.242546]\n",
      "epoch:12 step:9682 [D loss: 0.237449, acc.: 89.84%] [G loss: 4.135239]\n",
      "epoch:12 step:9683 [D loss: 0.020326, acc.: 100.00%] [G loss: 5.023209]\n",
      "epoch:12 step:9684 [D loss: 0.726422, acc.: 60.94%] [G loss: 4.970120]\n",
      "epoch:12 step:9685 [D loss: 0.037236, acc.: 99.22%] [G loss: 5.343860]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:12 step:9686 [D loss: 0.057098, acc.: 99.22%] [G loss: 4.468087]\n",
      "epoch:12 step:9687 [D loss: 0.070410, acc.: 98.44%] [G loss: 4.071799]\n",
      "epoch:12 step:9688 [D loss: 0.017069, acc.: 100.00%] [G loss: 3.884234]\n",
      "epoch:12 step:9689 [D loss: 0.034967, acc.: 100.00%] [G loss: 3.630480]\n",
      "epoch:12 step:9690 [D loss: 0.049268, acc.: 99.22%] [G loss: 2.961931]\n",
      "epoch:12 step:9691 [D loss: 0.075931, acc.: 99.22%] [G loss: 2.135489]\n",
      "epoch:12 step:9692 [D loss: 1.750261, acc.: 36.72%] [G loss: 6.885063]\n",
      "epoch:12 step:9693 [D loss: 1.649869, acc.: 51.56%] [G loss: 5.254746]\n",
      "epoch:12 step:9694 [D loss: 0.792692, acc.: 64.06%] [G loss: 1.980219]\n",
      "epoch:12 step:9695 [D loss: 0.218678, acc.: 92.19%] [G loss: 3.066754]\n",
      "epoch:12 step:9696 [D loss: 0.052806, acc.: 100.00%] [G loss: 2.136343]\n",
      "epoch:12 step:9697 [D loss: 0.098974, acc.: 98.44%] [G loss: 2.035681]\n",
      "epoch:12 step:9698 [D loss: 0.100566, acc.: 99.22%] [G loss: 2.531476]\n",
      "epoch:12 step:9699 [D loss: 0.059804, acc.: 100.00%] [G loss: 2.749761]\n",
      "epoch:12 step:9700 [D loss: 0.298712, acc.: 85.94%] [G loss: 4.425902]\n",
      "epoch:12 step:9701 [D loss: 0.165129, acc.: 95.31%] [G loss: 3.581034]\n",
      "epoch:12 step:9702 [D loss: 0.221993, acc.: 91.41%] [G loss: 2.877694]\n",
      "epoch:12 step:9703 [D loss: 0.125801, acc.: 98.44%] [G loss: 3.196761]\n",
      "epoch:12 step:9704 [D loss: 0.055913, acc.: 100.00%] [G loss: 2.653585]\n",
      "epoch:12 step:9705 [D loss: 0.080129, acc.: 99.22%] [G loss: 3.325021]\n",
      "epoch:12 step:9706 [D loss: 0.102252, acc.: 97.66%] [G loss: 2.863297]\n",
      "epoch:12 step:9707 [D loss: 0.078031, acc.: 99.22%] [G loss: 2.031810]\n",
      "epoch:12 step:9708 [D loss: 0.137788, acc.: 96.09%] [G loss: 2.427581]\n",
      "epoch:12 step:9709 [D loss: 0.145806, acc.: 94.53%] [G loss: 1.690776]\n",
      "epoch:12 step:9710 [D loss: 1.091453, acc.: 57.03%] [G loss: 5.896349]\n",
      "epoch:12 step:9711 [D loss: 1.833752, acc.: 50.00%] [G loss: 5.047976]\n",
      "epoch:12 step:9712 [D loss: 1.298163, acc.: 50.78%] [G loss: 1.792808]\n",
      "epoch:12 step:9713 [D loss: 0.385795, acc.: 78.91%] [G loss: 2.018226]\n",
      "epoch:12 step:9714 [D loss: 0.359631, acc.: 85.94%] [G loss: 2.378012]\n",
      "epoch:12 step:9715 [D loss: 0.259478, acc.: 87.50%] [G loss: 1.868590]\n",
      "epoch:12 step:9716 [D loss: 0.178228, acc.: 96.88%] [G loss: 2.586181]\n",
      "epoch:12 step:9717 [D loss: 0.112324, acc.: 100.00%] [G loss: 2.420672]\n",
      "epoch:12 step:9718 [D loss: 0.242776, acc.: 94.53%] [G loss: 2.640047]\n",
      "epoch:12 step:9719 [D loss: 0.254467, acc.: 92.19%] [G loss: 2.988032]\n",
      "epoch:12 step:9720 [D loss: 0.179194, acc.: 96.09%] [G loss: 2.717445]\n",
      "epoch:12 step:9721 [D loss: 0.333968, acc.: 87.50%] [G loss: 2.021194]\n",
      "epoch:12 step:9722 [D loss: 0.374412, acc.: 82.81%] [G loss: 1.837080]\n",
      "epoch:12 step:9723 [D loss: 0.324347, acc.: 85.16%] [G loss: 4.346696]\n",
      "epoch:12 step:9724 [D loss: 0.333818, acc.: 84.38%] [G loss: 3.553123]\n",
      "epoch:12 step:9725 [D loss: 0.255857, acc.: 89.84%] [G loss: 2.618111]\n",
      "epoch:12 step:9726 [D loss: 0.105255, acc.: 98.44%] [G loss: 2.480172]\n",
      "epoch:12 step:9727 [D loss: 0.103813, acc.: 99.22%] [G loss: 2.630945]\n",
      "epoch:12 step:9728 [D loss: 0.051020, acc.: 100.00%] [G loss: 2.817276]\n",
      "epoch:12 step:9729 [D loss: 0.083362, acc.: 99.22%] [G loss: 3.728311]\n",
      "epoch:12 step:9730 [D loss: 0.076815, acc.: 97.66%] [G loss: 2.572320]\n",
      "epoch:12 step:9731 [D loss: 0.087036, acc.: 99.22%] [G loss: 2.247283]\n",
      "epoch:12 step:9732 [D loss: 0.180032, acc.: 94.53%] [G loss: 3.656862]\n",
      "epoch:12 step:9733 [D loss: 0.243077, acc.: 90.62%] [G loss: 3.272436]\n",
      "epoch:12 step:9734 [D loss: 0.092564, acc.: 98.44%] [G loss: 3.091322]\n",
      "epoch:12 step:9735 [D loss: 0.031525, acc.: 100.00%] [G loss: 2.940598]\n",
      "epoch:12 step:9736 [D loss: 0.361641, acc.: 84.38%] [G loss: 3.813417]\n",
      "epoch:12 step:9737 [D loss: 0.017728, acc.: 100.00%] [G loss: 4.253685]\n",
      "epoch:12 step:9738 [D loss: 0.046340, acc.: 99.22%] [G loss: 3.757772]\n",
      "epoch:12 step:9739 [D loss: 0.124917, acc.: 96.09%] [G loss: 2.948700]\n",
      "epoch:12 step:9740 [D loss: 0.086788, acc.: 97.66%] [G loss: 3.024472]\n",
      "epoch:12 step:9741 [D loss: 0.053018, acc.: 99.22%] [G loss: 4.075632]\n",
      "epoch:12 step:9742 [D loss: 0.219805, acc.: 92.97%] [G loss: 3.582910]\n",
      "epoch:12 step:9743 [D loss: 0.062330, acc.: 99.22%] [G loss: 3.663815]\n",
      "epoch:12 step:9744 [D loss: 0.013572, acc.: 100.00%] [G loss: 4.191871]\n",
      "epoch:12 step:9745 [D loss: 0.359794, acc.: 82.81%] [G loss: 3.145964]\n",
      "epoch:12 step:9746 [D loss: 0.054539, acc.: 100.00%] [G loss: 3.841733]\n",
      "epoch:12 step:9747 [D loss: 0.055395, acc.: 100.00%] [G loss: 1.486778]\n",
      "epoch:12 step:9748 [D loss: 0.060563, acc.: 100.00%] [G loss: 3.854995]\n",
      "epoch:12 step:9749 [D loss: 0.018304, acc.: 100.00%] [G loss: 2.149616]\n",
      "epoch:12 step:9750 [D loss: 0.050345, acc.: 99.22%] [G loss: 2.747214]\n",
      "epoch:12 step:9751 [D loss: 0.022458, acc.: 100.00%] [G loss: 1.483701]\n",
      "epoch:12 step:9752 [D loss: 0.090728, acc.: 98.44%] [G loss: 4.387444]\n",
      "epoch:12 step:9753 [D loss: 0.012078, acc.: 100.00%] [G loss: 4.872570]\n",
      "epoch:12 step:9754 [D loss: 0.309187, acc.: 89.06%] [G loss: 1.896015]\n",
      "epoch:12 step:9755 [D loss: 0.240907, acc.: 88.28%] [G loss: 4.436649]\n",
      "epoch:12 step:9756 [D loss: 0.078764, acc.: 96.88%] [G loss: 4.192303]\n",
      "epoch:12 step:9757 [D loss: 0.240127, acc.: 89.84%] [G loss: 1.306883]\n",
      "epoch:12 step:9758 [D loss: 0.103910, acc.: 96.88%] [G loss: 1.366184]\n",
      "epoch:12 step:9759 [D loss: 0.086746, acc.: 96.88%] [G loss: 3.493782]\n",
      "epoch:12 step:9760 [D loss: 0.014306, acc.: 100.00%] [G loss: 3.843372]\n",
      "epoch:12 step:9761 [D loss: 0.030042, acc.: 99.22%] [G loss: 2.950575]\n",
      "epoch:12 step:9762 [D loss: 0.018276, acc.: 99.22%] [G loss: 2.424992]\n",
      "epoch:12 step:9763 [D loss: 0.091696, acc.: 97.66%] [G loss: 2.819779]\n",
      "epoch:12 step:9764 [D loss: 0.024068, acc.: 100.00%] [G loss: 2.586690]\n",
      "epoch:12 step:9765 [D loss: 0.027533, acc.: 100.00%] [G loss: 2.161007]\n",
      "epoch:12 step:9766 [D loss: 0.060180, acc.: 100.00%] [G loss: 3.332282]\n",
      "epoch:12 step:9767 [D loss: 1.151400, acc.: 47.66%] [G loss: 7.308462]\n",
      "epoch:12 step:9768 [D loss: 2.432047, acc.: 50.00%] [G loss: 5.299836]\n",
      "epoch:12 step:9769 [D loss: 0.552421, acc.: 71.88%] [G loss: 2.190602]\n",
      "epoch:12 step:9770 [D loss: 0.163095, acc.: 91.41%] [G loss: 1.971910]\n",
      "epoch:12 step:9771 [D loss: 0.213192, acc.: 87.50%] [G loss: 3.441772]\n",
      "epoch:12 step:9772 [D loss: 0.008017, acc.: 100.00%] [G loss: 4.317414]\n",
      "epoch:12 step:9773 [D loss: 0.221772, acc.: 89.06%] [G loss: 3.886204]\n",
      "epoch:12 step:9774 [D loss: 0.032314, acc.: 100.00%] [G loss: 3.549310]\n",
      "epoch:12 step:9775 [D loss: 0.051032, acc.: 100.00%] [G loss: 3.062238]\n",
      "epoch:12 step:9776 [D loss: 0.045399, acc.: 99.22%] [G loss: 2.437676]\n",
      "epoch:12 step:9777 [D loss: 0.065817, acc.: 99.22%] [G loss: 3.147656]\n",
      "epoch:12 step:9778 [D loss: 0.044265, acc.: 100.00%] [G loss: 2.444636]\n",
      "epoch:12 step:9779 [D loss: 0.021848, acc.: 100.00%] [G loss: 3.305932]\n",
      "epoch:12 step:9780 [D loss: 0.046321, acc.: 100.00%] [G loss: 2.753521]\n",
      "epoch:12 step:9781 [D loss: 0.036414, acc.: 100.00%] [G loss: 2.235590]\n",
      "epoch:12 step:9782 [D loss: 0.077667, acc.: 99.22%] [G loss: 3.434244]\n",
      "epoch:12 step:9783 [D loss: 0.135508, acc.: 94.53%] [G loss: 1.588007]\n",
      "epoch:12 step:9784 [D loss: 0.083063, acc.: 98.44%] [G loss: 2.799583]\n",
      "epoch:12 step:9785 [D loss: 0.062152, acc.: 98.44%] [G loss: 0.996802]\n",
      "epoch:12 step:9786 [D loss: 0.410545, acc.: 77.34%] [G loss: 5.601640]\n",
      "epoch:12 step:9787 [D loss: 0.772291, acc.: 64.84%] [G loss: 4.001791]\n",
      "epoch:12 step:9788 [D loss: 0.068553, acc.: 100.00%] [G loss: 3.614893]\n",
      "epoch:12 step:9789 [D loss: 0.017290, acc.: 100.00%] [G loss: 3.100971]\n",
      "epoch:12 step:9790 [D loss: 0.029366, acc.: 100.00%] [G loss: 4.014361]\n",
      "epoch:12 step:9791 [D loss: 0.032643, acc.: 100.00%] [G loss: 2.725318]\n",
      "epoch:12 step:9792 [D loss: 0.547985, acc.: 72.66%] [G loss: 5.322454]\n",
      "epoch:12 step:9793 [D loss: 0.127654, acc.: 96.09%] [G loss: 5.371704]\n",
      "epoch:12 step:9794 [D loss: 0.154734, acc.: 94.53%] [G loss: 4.317711]\n",
      "epoch:12 step:9795 [D loss: 0.022224, acc.: 99.22%] [G loss: 3.816594]\n",
      "epoch:12 step:9796 [D loss: 0.021613, acc.: 100.00%] [G loss: 2.941336]\n",
      "epoch:12 step:9797 [D loss: 0.025208, acc.: 100.00%] [G loss: 2.970980]\n",
      "epoch:12 step:9798 [D loss: 0.053241, acc.: 98.44%] [G loss: 3.653206]\n",
      "epoch:12 step:9799 [D loss: 0.036470, acc.: 99.22%] [G loss: 3.796933]\n",
      "epoch:12 step:9800 [D loss: 0.098647, acc.: 96.88%] [G loss: 3.280469]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##############\n",
      "[0.9182552  0.87050754 0.81267801 1.03249126 2.1198068  1.02138015\n",
      " 2.12017486 1.11966866 2.11312083 1.11602205]\n",
      "##########\n",
      "epoch:12 step:9801 [D loss: 0.013830, acc.: 100.00%] [G loss: 1.610649]\n",
      "epoch:12 step:9802 [D loss: 0.072807, acc.: 100.00%] [G loss: 2.197270]\n",
      "epoch:12 step:9803 [D loss: 0.114180, acc.: 97.66%] [G loss: 2.154243]\n",
      "epoch:12 step:9804 [D loss: 0.026950, acc.: 100.00%] [G loss: 0.655973]\n",
      "epoch:12 step:9805 [D loss: 0.023361, acc.: 100.00%] [G loss: 0.943886]\n",
      "epoch:12 step:9806 [D loss: 0.036759, acc.: 100.00%] [G loss: 2.425179]\n",
      "epoch:12 step:9807 [D loss: 0.026137, acc.: 100.00%] [G loss: 1.489375]\n",
      "epoch:12 step:9808 [D loss: 0.031049, acc.: 100.00%] [G loss: 1.625357]\n",
      "epoch:12 step:9809 [D loss: 0.120227, acc.: 97.66%] [G loss: 0.544343]\n",
      "epoch:12 step:9810 [D loss: 0.059510, acc.: 100.00%] [G loss: 2.447141]\n",
      "epoch:12 step:9811 [D loss: 0.106373, acc.: 98.44%] [G loss: 4.000458]\n",
      "epoch:12 step:9812 [D loss: 0.145851, acc.: 92.97%] [G loss: 2.350466]\n",
      "epoch:12 step:9813 [D loss: 0.154693, acc.: 97.66%] [G loss: 3.285552]\n",
      "epoch:12 step:9814 [D loss: 0.050927, acc.: 100.00%] [G loss: 2.426343]\n",
      "epoch:12 step:9815 [D loss: 0.186070, acc.: 95.31%] [G loss: 4.761063]\n",
      "epoch:12 step:9816 [D loss: 0.104529, acc.: 97.66%] [G loss: 3.388395]\n",
      "epoch:12 step:9817 [D loss: 0.076871, acc.: 99.22%] [G loss: 2.164935]\n",
      "epoch:12 step:9818 [D loss: 0.043776, acc.: 100.00%] [G loss: 2.653632]\n",
      "epoch:12 step:9819 [D loss: 0.096479, acc.: 100.00%] [G loss: 3.820505]\n",
      "epoch:12 step:9820 [D loss: 0.358148, acc.: 86.72%] [G loss: 4.462903]\n",
      "epoch:12 step:9821 [D loss: 0.058893, acc.: 100.00%] [G loss: 4.303956]\n",
      "epoch:12 step:9822 [D loss: 0.041519, acc.: 100.00%] [G loss: 3.523153]\n",
      "epoch:12 step:9823 [D loss: 0.022059, acc.: 100.00%] [G loss: 4.502465]\n",
      "epoch:12 step:9824 [D loss: 0.155325, acc.: 96.09%] [G loss: 5.403101]\n",
      "epoch:12 step:9825 [D loss: 0.271369, acc.: 83.59%] [G loss: 3.450639]\n",
      "epoch:12 step:9826 [D loss: 0.104348, acc.: 98.44%] [G loss: 3.563294]\n",
      "epoch:12 step:9827 [D loss: 0.008015, acc.: 100.00%] [G loss: 4.554708]\n",
      "epoch:12 step:9828 [D loss: 0.043125, acc.: 100.00%] [G loss: 2.527752]\n",
      "epoch:12 step:9829 [D loss: 0.011960, acc.: 100.00%] [G loss: 2.246486]\n",
      "epoch:12 step:9830 [D loss: 0.014127, acc.: 100.00%] [G loss: 2.826469]\n",
      "epoch:12 step:9831 [D loss: 0.208449, acc.: 95.31%] [G loss: 2.229679]\n",
      "epoch:12 step:9832 [D loss: 0.061803, acc.: 99.22%] [G loss: 0.243348]\n",
      "epoch:12 step:9833 [D loss: 0.308758, acc.: 81.25%] [G loss: 5.318428]\n",
      "epoch:12 step:9834 [D loss: 0.244284, acc.: 89.06%] [G loss: 5.113668]\n",
      "epoch:12 step:9835 [D loss: 0.105492, acc.: 96.09%] [G loss: 1.634247]\n",
      "epoch:12 step:9836 [D loss: 0.035173, acc.: 100.00%] [G loss: 1.827162]\n",
      "epoch:12 step:9837 [D loss: 0.136292, acc.: 94.53%] [G loss: 3.938010]\n",
      "epoch:12 step:9838 [D loss: 0.020855, acc.: 100.00%] [G loss: 5.066839]\n",
      "epoch:12 step:9839 [D loss: 0.305852, acc.: 87.50%] [G loss: 3.558024]\n",
      "epoch:12 step:9840 [D loss: 0.026920, acc.: 100.00%] [G loss: 2.667179]\n",
      "epoch:12 step:9841 [D loss: 0.027006, acc.: 99.22%] [G loss: 4.135612]\n",
      "epoch:12 step:9842 [D loss: 0.011955, acc.: 100.00%] [G loss: 2.696923]\n",
      "epoch:12 step:9843 [D loss: 0.188101, acc.: 94.53%] [G loss: 1.393604]\n",
      "epoch:12 step:9844 [D loss: 0.249971, acc.: 85.16%] [G loss: 5.796717]\n",
      "epoch:12 step:9845 [D loss: 0.818363, acc.: 60.16%] [G loss: 1.861770]\n",
      "epoch:12 step:9846 [D loss: 0.347418, acc.: 83.59%] [G loss: 5.559740]\n",
      "epoch:12 step:9847 [D loss: 1.315244, acc.: 53.12%] [G loss: 3.890569]\n",
      "epoch:12 step:9848 [D loss: 0.109778, acc.: 97.66%] [G loss: 3.130582]\n",
      "epoch:12 step:9849 [D loss: 0.238034, acc.: 93.75%] [G loss: 5.062233]\n",
      "epoch:12 step:9850 [D loss: 0.169832, acc.: 94.53%] [G loss: 4.109718]\n",
      "epoch:12 step:9851 [D loss: 0.059128, acc.: 98.44%] [G loss: 3.707831]\n",
      "epoch:12 step:9852 [D loss: 0.047152, acc.: 100.00%] [G loss: 3.282872]\n",
      "epoch:12 step:9853 [D loss: 0.064678, acc.: 99.22%] [G loss: 3.630697]\n",
      "epoch:12 step:9854 [D loss: 0.071167, acc.: 99.22%] [G loss: 4.403628]\n",
      "epoch:12 step:9855 [D loss: 0.055845, acc.: 99.22%] [G loss: 3.625830]\n",
      "epoch:12 step:9856 [D loss: 0.064795, acc.: 98.44%] [G loss: 3.971080]\n",
      "epoch:12 step:9857 [D loss: 0.331878, acc.: 85.94%] [G loss: 4.554856]\n",
      "epoch:12 step:9858 [D loss: 0.164544, acc.: 93.75%] [G loss: 3.905917]\n",
      "epoch:12 step:9859 [D loss: 0.085593, acc.: 97.66%] [G loss: 5.285952]\n",
      "epoch:12 step:9860 [D loss: 0.235869, acc.: 92.19%] [G loss: 2.314374]\n",
      "epoch:12 step:9861 [D loss: 0.052176, acc.: 99.22%] [G loss: 3.263599]\n",
      "epoch:12 step:9862 [D loss: 0.015710, acc.: 100.00%] [G loss: 3.915124]\n",
      "epoch:12 step:9863 [D loss: 0.118420, acc.: 94.53%] [G loss: 3.765839]\n",
      "epoch:12 step:9864 [D loss: 0.139570, acc.: 96.88%] [G loss: 2.824812]\n",
      "epoch:12 step:9865 [D loss: 0.060763, acc.: 99.22%] [G loss: 3.103524]\n",
      "epoch:12 step:9866 [D loss: 0.016164, acc.: 100.00%] [G loss: 4.107831]\n",
      "epoch:12 step:9867 [D loss: 0.095998, acc.: 95.31%] [G loss: 3.092034]\n",
      "epoch:12 step:9868 [D loss: 0.025263, acc.: 100.00%] [G loss: 2.927545]\n",
      "epoch:12 step:9869 [D loss: 0.082900, acc.: 98.44%] [G loss: 2.250638]\n",
      "epoch:12 step:9870 [D loss: 0.065648, acc.: 99.22%] [G loss: 1.835303]\n",
      "epoch:12 step:9871 [D loss: 0.024191, acc.: 100.00%] [G loss: 1.950055]\n",
      "epoch:12 step:9872 [D loss: 0.033051, acc.: 100.00%] [G loss: 2.572525]\n",
      "epoch:12 step:9873 [D loss: 1.019583, acc.: 52.34%] [G loss: 8.459561]\n",
      "epoch:12 step:9874 [D loss: 1.811347, acc.: 50.00%] [G loss: 2.565982]\n",
      "epoch:12 step:9875 [D loss: 0.152565, acc.: 96.09%] [G loss: 2.479370]\n",
      "epoch:12 step:9876 [D loss: 0.050288, acc.: 99.22%] [G loss: 2.686858]\n",
      "epoch:12 step:9877 [D loss: 0.040126, acc.: 100.00%] [G loss: 3.191700]\n",
      "epoch:12 step:9878 [D loss: 0.156883, acc.: 96.09%] [G loss: 1.386107]\n",
      "epoch:12 step:9879 [D loss: 0.249850, acc.: 89.06%] [G loss: 4.229269]\n",
      "epoch:12 step:9880 [D loss: 0.401077, acc.: 83.59%] [G loss: 2.390200]\n",
      "epoch:12 step:9881 [D loss: 1.862929, acc.: 40.62%] [G loss: 6.516986]\n",
      "epoch:12 step:9882 [D loss: 1.831684, acc.: 50.00%] [G loss: 5.274435]\n",
      "epoch:12 step:9883 [D loss: 1.157902, acc.: 53.12%] [G loss: 2.664433]\n",
      "epoch:12 step:9884 [D loss: 0.544564, acc.: 72.66%] [G loss: 2.759947]\n",
      "epoch:12 step:9885 [D loss: 0.172470, acc.: 95.31%] [G loss: 2.733369]\n",
      "epoch:12 step:9886 [D loss: 0.075751, acc.: 99.22%] [G loss: 2.178235]\n",
      "epoch:12 step:9887 [D loss: 0.183471, acc.: 95.31%] [G loss: 2.282817]\n",
      "epoch:12 step:9888 [D loss: 0.129972, acc.: 97.66%] [G loss: 2.974816]\n",
      "epoch:12 step:9889 [D loss: 0.265354, acc.: 89.06%] [G loss: 2.692516]\n",
      "epoch:12 step:9890 [D loss: 0.157091, acc.: 94.53%] [G loss: 2.711781]\n",
      "epoch:12 step:9891 [D loss: 0.054147, acc.: 100.00%] [G loss: 3.837690]\n",
      "epoch:12 step:9892 [D loss: 0.056748, acc.: 99.22%] [G loss: 3.306637]\n",
      "epoch:12 step:9893 [D loss: 0.015270, acc.: 100.00%] [G loss: 1.546585]\n",
      "epoch:12 step:9894 [D loss: 0.064181, acc.: 100.00%] [G loss: 1.534234]\n",
      "epoch:12 step:9895 [D loss: 0.071053, acc.: 100.00%] [G loss: 1.721515]\n",
      "epoch:12 step:9896 [D loss: 0.097043, acc.: 98.44%] [G loss: 0.309792]\n",
      "epoch:12 step:9897 [D loss: 0.167686, acc.: 94.53%] [G loss: 1.882365]\n",
      "epoch:12 step:9898 [D loss: 0.175894, acc.: 93.75%] [G loss: 0.627888]\n",
      "epoch:12 step:9899 [D loss: 0.009571, acc.: 100.00%] [G loss: 1.068001]\n",
      "epoch:12 step:9900 [D loss: 0.089881, acc.: 97.66%] [G loss: 1.712466]\n",
      "epoch:12 step:9901 [D loss: 0.029690, acc.: 99.22%] [G loss: 1.577878]\n",
      "epoch:12 step:9902 [D loss: 0.064009, acc.: 100.00%] [G loss: 1.512311]\n",
      "epoch:12 step:9903 [D loss: 0.228377, acc.: 91.41%] [G loss: 4.218356]\n",
      "epoch:12 step:9904 [D loss: 0.787602, acc.: 63.28%] [G loss: 3.019554]\n",
      "epoch:12 step:9905 [D loss: 0.070916, acc.: 97.66%] [G loss: 3.447204]\n",
      "epoch:12 step:9906 [D loss: 0.028282, acc.: 100.00%] [G loss: 4.066508]\n",
      "epoch:12 step:9907 [D loss: 0.021688, acc.: 100.00%] [G loss: 3.594642]\n",
      "epoch:12 step:9908 [D loss: 0.098071, acc.: 98.44%] [G loss: 2.502009]\n",
      "epoch:12 step:9909 [D loss: 0.249025, acc.: 89.84%] [G loss: 3.656211]\n",
      "epoch:12 step:9910 [D loss: 0.043454, acc.: 99.22%] [G loss: 3.624645]\n",
      "epoch:12 step:9911 [D loss: 0.056341, acc.: 99.22%] [G loss: 3.513299]\n",
      "epoch:12 step:9912 [D loss: 0.063474, acc.: 98.44%] [G loss: 3.869828]\n",
      "epoch:12 step:9913 [D loss: 0.017383, acc.: 100.00%] [G loss: 3.970245]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:12 step:9914 [D loss: 0.111218, acc.: 97.66%] [G loss: 3.044367]\n",
      "epoch:12 step:9915 [D loss: 0.070709, acc.: 98.44%] [G loss: 3.600936]\n",
      "epoch:12 step:9916 [D loss: 0.023759, acc.: 100.00%] [G loss: 4.114129]\n",
      "epoch:12 step:9917 [D loss: 0.043605, acc.: 99.22%] [G loss: 4.036023]\n",
      "epoch:12 step:9918 [D loss: 0.097434, acc.: 98.44%] [G loss: 4.274352]\n",
      "epoch:12 step:9919 [D loss: 0.045852, acc.: 100.00%] [G loss: 3.930446]\n",
      "epoch:12 step:9920 [D loss: 0.029414, acc.: 100.00%] [G loss: 3.549190]\n",
      "epoch:12 step:9921 [D loss: 0.024545, acc.: 100.00%] [G loss: 4.134531]\n",
      "epoch:12 step:9922 [D loss: 0.067595, acc.: 99.22%] [G loss: 4.193769]\n",
      "epoch:12 step:9923 [D loss: 0.035105, acc.: 100.00%] [G loss: 3.243586]\n",
      "epoch:12 step:9924 [D loss: 0.096917, acc.: 100.00%] [G loss: 1.137597]\n",
      "epoch:12 step:9925 [D loss: 0.026454, acc.: 100.00%] [G loss: 3.950295]\n",
      "epoch:12 step:9926 [D loss: 0.060204, acc.: 99.22%] [G loss: 2.406937]\n",
      "epoch:12 step:9927 [D loss: 0.039959, acc.: 100.00%] [G loss: 2.689219]\n",
      "epoch:12 step:9928 [D loss: 0.052707, acc.: 100.00%] [G loss: 2.190053]\n",
      "epoch:12 step:9929 [D loss: 0.075754, acc.: 99.22%] [G loss: 4.150853]\n",
      "epoch:12 step:9930 [D loss: 0.370806, acc.: 84.38%] [G loss: 4.454015]\n",
      "epoch:12 step:9931 [D loss: 0.031272, acc.: 99.22%] [G loss: 4.497652]\n",
      "epoch:12 step:9932 [D loss: 0.044991, acc.: 100.00%] [G loss: 2.930713]\n",
      "epoch:12 step:9933 [D loss: 0.079308, acc.: 98.44%] [G loss: 1.223058]\n",
      "epoch:12 step:9934 [D loss: 0.108211, acc.: 96.09%] [G loss: 2.207763]\n",
      "epoch:12 step:9935 [D loss: 0.033191, acc.: 99.22%] [G loss: 1.062255]\n",
      "epoch:12 step:9936 [D loss: 0.057157, acc.: 98.44%] [G loss: 2.407612]\n",
      "epoch:12 step:9937 [D loss: 0.224565, acc.: 92.97%] [G loss: 3.345886]\n",
      "epoch:12 step:9938 [D loss: 0.603259, acc.: 73.44%] [G loss: 5.729823]\n",
      "epoch:12 step:9939 [D loss: 0.136149, acc.: 93.75%] [G loss: 5.729846]\n",
      "epoch:12 step:9940 [D loss: 0.135278, acc.: 95.31%] [G loss: 3.534596]\n",
      "epoch:12 step:9941 [D loss: 0.079919, acc.: 97.66%] [G loss: 3.533661]\n",
      "epoch:12 step:9942 [D loss: 0.051054, acc.: 100.00%] [G loss: 3.699687]\n",
      "epoch:12 step:9943 [D loss: 0.041067, acc.: 99.22%] [G loss: 3.558150]\n",
      "epoch:12 step:9944 [D loss: 0.057387, acc.: 99.22%] [G loss: 4.091547]\n",
      "epoch:12 step:9945 [D loss: 0.115001, acc.: 96.88%] [G loss: 3.922588]\n",
      "epoch:12 step:9946 [D loss: 0.017873, acc.: 100.00%] [G loss: 3.756277]\n",
      "epoch:12 step:9947 [D loss: 0.211054, acc.: 93.75%] [G loss: 3.449332]\n",
      "epoch:12 step:9948 [D loss: 0.024420, acc.: 100.00%] [G loss: 4.171326]\n",
      "epoch:12 step:9949 [D loss: 0.284616, acc.: 88.28%] [G loss: 4.401226]\n",
      "epoch:12 step:9950 [D loss: 0.011275, acc.: 100.00%] [G loss: 5.453857]\n",
      "epoch:12 step:9951 [D loss: 0.030588, acc.: 99.22%] [G loss: 5.134430]\n",
      "epoch:12 step:9952 [D loss: 0.115184, acc.: 97.66%] [G loss: 4.878827]\n",
      "epoch:12 step:9953 [D loss: 0.034941, acc.: 100.00%] [G loss: 4.765066]\n",
      "epoch:12 step:9954 [D loss: 0.026643, acc.: 100.00%] [G loss: 4.942202]\n",
      "epoch:12 step:9955 [D loss: 0.022700, acc.: 100.00%] [G loss: 4.686222]\n",
      "epoch:12 step:9956 [D loss: 0.042485, acc.: 100.00%] [G loss: 4.254193]\n",
      "epoch:12 step:9957 [D loss: 0.025901, acc.: 99.22%] [G loss: 4.665059]\n",
      "epoch:12 step:9958 [D loss: 0.032256, acc.: 99.22%] [G loss: 5.077985]\n",
      "epoch:12 step:9959 [D loss: 0.046643, acc.: 99.22%] [G loss: 3.658279]\n",
      "epoch:12 step:9960 [D loss: 0.096864, acc.: 99.22%] [G loss: 4.891602]\n",
      "epoch:12 step:9961 [D loss: 0.049859, acc.: 99.22%] [G loss: 5.196420]\n",
      "epoch:12 step:9962 [D loss: 0.609848, acc.: 70.31%] [G loss: 8.585176]\n",
      "epoch:12 step:9963 [D loss: 1.550153, acc.: 52.34%] [G loss: 3.671625]\n",
      "epoch:12 step:9964 [D loss: 0.265391, acc.: 89.06%] [G loss: 5.681994]\n",
      "epoch:12 step:9965 [D loss: 0.031226, acc.: 100.00%] [G loss: 5.359526]\n",
      "epoch:12 step:9966 [D loss: 0.323733, acc.: 85.94%] [G loss: 3.313570]\n",
      "epoch:12 step:9967 [D loss: 0.254369, acc.: 93.75%] [G loss: 4.723366]\n",
      "epoch:12 step:9968 [D loss: 0.009737, acc.: 100.00%] [G loss: 5.285093]\n",
      "epoch:12 step:9969 [D loss: 0.007332, acc.: 100.00%] [G loss: 4.270541]\n",
      "epoch:12 step:9970 [D loss: 0.046906, acc.: 98.44%] [G loss: 2.655267]\n",
      "epoch:12 step:9971 [D loss: 0.036966, acc.: 100.00%] [G loss: 2.705429]\n",
      "epoch:12 step:9972 [D loss: 0.186146, acc.: 92.97%] [G loss: 4.347490]\n",
      "epoch:12 step:9973 [D loss: 0.204619, acc.: 91.41%] [G loss: 2.411846]\n",
      "epoch:12 step:9974 [D loss: 0.020194, acc.: 100.00%] [G loss: 0.903577]\n",
      "epoch:12 step:9975 [D loss: 0.179848, acc.: 93.75%] [G loss: 0.243719]\n",
      "epoch:12 step:9976 [D loss: 0.674767, acc.: 71.88%] [G loss: 7.354640]\n",
      "epoch:12 step:9977 [D loss: 1.302721, acc.: 53.12%] [G loss: 4.901758]\n",
      "epoch:12 step:9978 [D loss: 0.022208, acc.: 100.00%] [G loss: 3.657117]\n",
      "epoch:12 step:9979 [D loss: 0.053747, acc.: 99.22%] [G loss: 2.894814]\n",
      "epoch:12 step:9980 [D loss: 0.085650, acc.: 96.88%] [G loss: 4.432819]\n",
      "epoch:12 step:9981 [D loss: 0.389672, acc.: 81.25%] [G loss: 4.502433]\n",
      "epoch:12 step:9982 [D loss: 0.049229, acc.: 99.22%] [G loss: 3.646724]\n",
      "epoch:12 step:9983 [D loss: 0.018377, acc.: 100.00%] [G loss: 3.558294]\n",
      "epoch:12 step:9984 [D loss: 0.025881, acc.: 100.00%] [G loss: 2.505124]\n",
      "epoch:12 step:9985 [D loss: 0.415171, acc.: 80.47%] [G loss: 5.220712]\n",
      "epoch:12 step:9986 [D loss: 0.228616, acc.: 89.84%] [G loss: 4.867623]\n",
      "epoch:12 step:9987 [D loss: 0.089380, acc.: 97.66%] [G loss: 2.874646]\n",
      "epoch:12 step:9988 [D loss: 0.073564, acc.: 96.88%] [G loss: 3.559962]\n",
      "epoch:12 step:9989 [D loss: 0.041018, acc.: 99.22%] [G loss: 3.442159]\n",
      "epoch:12 step:9990 [D loss: 0.073327, acc.: 99.22%] [G loss: 4.257472]\n",
      "epoch:12 step:9991 [D loss: 0.212466, acc.: 96.09%] [G loss: 3.293077]\n",
      "epoch:12 step:9992 [D loss: 0.036549, acc.: 100.00%] [G loss: 4.982686]\n",
      "epoch:12 step:9993 [D loss: 0.094518, acc.: 98.44%] [G loss: 4.313354]\n",
      "epoch:12 step:9994 [D loss: 0.067523, acc.: 98.44%] [G loss: 4.329389]\n",
      "epoch:12 step:9995 [D loss: 0.804808, acc.: 59.38%] [G loss: 6.392551]\n",
      "epoch:12 step:9996 [D loss: 0.600253, acc.: 73.44%] [G loss: 3.285743]\n",
      "epoch:12 step:9997 [D loss: 0.041569, acc.: 99.22%] [G loss: 3.451137]\n",
      "epoch:12 step:9998 [D loss: 0.124861, acc.: 96.09%] [G loss: 4.583052]\n",
      "epoch:12 step:9999 [D loss: 0.082751, acc.: 97.66%] [G loss: 3.503370]\n",
      "epoch:12 step:10000 [D loss: 0.046471, acc.: 100.00%] [G loss: 3.402945]\n",
      "##############\n",
      "[0.84124475 0.9854463  2.11852109 0.98474112 1.12633292 0.96133113\n",
      " 2.09666694 0.95074876 2.11420368 1.1168003 ]\n",
      "##########\n",
      "epoch:12 step:10001 [D loss: 0.164341, acc.: 92.97%] [G loss: 5.129191]\n",
      "epoch:12 step:10002 [D loss: 0.036134, acc.: 99.22%] [G loss: 4.744068]\n",
      "epoch:12 step:10003 [D loss: 1.264885, acc.: 46.88%] [G loss: 7.081053]\n",
      "epoch:12 step:10004 [D loss: 1.404270, acc.: 52.34%] [G loss: 4.202629]\n",
      "epoch:12 step:10005 [D loss: 0.112690, acc.: 96.88%] [G loss: 4.622005]\n",
      "epoch:12 step:10006 [D loss: 0.031509, acc.: 99.22%] [G loss: 4.055942]\n",
      "epoch:12 step:10007 [D loss: 0.087050, acc.: 100.00%] [G loss: 3.815477]\n",
      "epoch:12 step:10008 [D loss: 0.067105, acc.: 98.44%] [G loss: 4.271340]\n",
      "epoch:12 step:10009 [D loss: 0.018506, acc.: 100.00%] [G loss: 3.638101]\n",
      "epoch:12 step:10010 [D loss: 0.037013, acc.: 100.00%] [G loss: 3.762037]\n",
      "epoch:12 step:10011 [D loss: 0.068928, acc.: 97.66%] [G loss: 3.553735]\n",
      "epoch:12 step:10012 [D loss: 0.041979, acc.: 100.00%] [G loss: 3.758528]\n",
      "epoch:12 step:10013 [D loss: 0.022776, acc.: 100.00%] [G loss: 3.505317]\n",
      "epoch:12 step:10014 [D loss: 0.218004, acc.: 93.75%] [G loss: 5.069126]\n",
      "epoch:12 step:10015 [D loss: 0.100990, acc.: 96.09%] [G loss: 4.385009]\n",
      "epoch:12 step:10016 [D loss: 0.018113, acc.: 100.00%] [G loss: 3.413772]\n",
      "epoch:12 step:10017 [D loss: 0.052667, acc.: 100.00%] [G loss: 3.932402]\n",
      "epoch:12 step:10018 [D loss: 0.927693, acc.: 58.59%] [G loss: 5.279883]\n",
      "epoch:12 step:10019 [D loss: 0.598780, acc.: 72.66%] [G loss: 3.580478]\n",
      "epoch:12 step:10020 [D loss: 0.076308, acc.: 98.44%] [G loss: 3.575868]\n",
      "epoch:12 step:10021 [D loss: 0.029775, acc.: 100.00%] [G loss: 3.666296]\n",
      "epoch:12 step:10022 [D loss: 0.039329, acc.: 100.00%] [G loss: 4.092917]\n",
      "epoch:12 step:10023 [D loss: 0.010120, acc.: 100.00%] [G loss: 4.384401]\n",
      "epoch:12 step:10024 [D loss: 0.031427, acc.: 100.00%] [G loss: 3.696270]\n",
      "epoch:12 step:10025 [D loss: 0.039583, acc.: 100.00%] [G loss: 3.789100]\n",
      "epoch:12 step:10026 [D loss: 0.050595, acc.: 99.22%] [G loss: 4.017215]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:12 step:10027 [D loss: 0.014656, acc.: 100.00%] [G loss: 4.017166]\n",
      "epoch:12 step:10028 [D loss: 0.066518, acc.: 99.22%] [G loss: 3.772414]\n",
      "epoch:12 step:10029 [D loss: 0.041436, acc.: 100.00%] [G loss: 4.243756]\n",
      "epoch:12 step:10030 [D loss: 0.054711, acc.: 99.22%] [G loss: 4.267335]\n",
      "epoch:12 step:10031 [D loss: 0.057042, acc.: 98.44%] [G loss: 3.563088]\n",
      "epoch:12 step:10032 [D loss: 0.081273, acc.: 99.22%] [G loss: 2.848798]\n",
      "epoch:12 step:10033 [D loss: 0.058078, acc.: 100.00%] [G loss: 4.114707]\n",
      "epoch:12 step:10034 [D loss: 0.039148, acc.: 100.00%] [G loss: 3.301800]\n",
      "epoch:12 step:10035 [D loss: 0.027108, acc.: 100.00%] [G loss: 1.507824]\n",
      "epoch:12 step:10036 [D loss: 0.046033, acc.: 99.22%] [G loss: 3.948564]\n",
      "epoch:12 step:10037 [D loss: 0.093102, acc.: 98.44%] [G loss: 1.185763]\n",
      "epoch:12 step:10038 [D loss: 0.083219, acc.: 98.44%] [G loss: 4.837058]\n",
      "epoch:12 step:10039 [D loss: 0.170412, acc.: 96.88%] [G loss: 4.209847]\n",
      "epoch:12 step:10040 [D loss: 0.064324, acc.: 99.22%] [G loss: 3.313435]\n",
      "epoch:12 step:10041 [D loss: 0.019008, acc.: 100.00%] [G loss: 3.348135]\n",
      "epoch:12 step:10042 [D loss: 0.140903, acc.: 97.66%] [G loss: 5.271932]\n",
      "epoch:12 step:10043 [D loss: 0.009496, acc.: 100.00%] [G loss: 5.564809]\n",
      "epoch:12 step:10044 [D loss: 0.371210, acc.: 82.81%] [G loss: 0.477485]\n",
      "epoch:12 step:10045 [D loss: 0.257042, acc.: 88.28%] [G loss: 7.337739]\n",
      "epoch:12 step:10046 [D loss: 0.698308, acc.: 67.97%] [G loss: 0.536134]\n",
      "epoch:12 step:10047 [D loss: 0.480648, acc.: 72.66%] [G loss: 7.616975]\n",
      "epoch:12 step:10048 [D loss: 0.889395, acc.: 62.50%] [G loss: 1.462280]\n",
      "epoch:12 step:10049 [D loss: 0.402974, acc.: 80.47%] [G loss: 6.706171]\n",
      "epoch:12 step:10050 [D loss: 0.104821, acc.: 95.31%] [G loss: 6.367734]\n",
      "epoch:12 step:10051 [D loss: 0.020715, acc.: 100.00%] [G loss: 5.191179]\n",
      "epoch:12 step:10052 [D loss: 0.019316, acc.: 100.00%] [G loss: 4.818174]\n",
      "epoch:12 step:10053 [D loss: 0.039893, acc.: 100.00%] [G loss: 4.406357]\n",
      "epoch:12 step:10054 [D loss: 0.037296, acc.: 98.44%] [G loss: 2.974191]\n",
      "epoch:12 step:10055 [D loss: 0.115523, acc.: 95.31%] [G loss: 3.544254]\n",
      "epoch:12 step:10056 [D loss: 0.039777, acc.: 99.22%] [G loss: 5.075867]\n",
      "epoch:12 step:10057 [D loss: 0.329260, acc.: 82.03%] [G loss: 1.194450]\n",
      "epoch:12 step:10058 [D loss: 0.047337, acc.: 100.00%] [G loss: 0.319483]\n",
      "epoch:12 step:10059 [D loss: 0.044564, acc.: 100.00%] [G loss: 0.579110]\n",
      "epoch:12 step:10060 [D loss: 0.203977, acc.: 92.19%] [G loss: 4.950077]\n",
      "epoch:12 step:10061 [D loss: 0.124551, acc.: 94.53%] [G loss: 4.665059]\n",
      "epoch:12 step:10062 [D loss: 0.014291, acc.: 100.00%] [G loss: 3.721791]\n",
      "epoch:12 step:10063 [D loss: 0.014117, acc.: 100.00%] [G loss: 2.507021]\n",
      "epoch:12 step:10064 [D loss: 0.064980, acc.: 98.44%] [G loss: 1.559389]\n",
      "epoch:12 step:10065 [D loss: 0.228534, acc.: 86.72%] [G loss: 5.675140]\n",
      "epoch:12 step:10066 [D loss: 0.207086, acc.: 89.06%] [G loss: 3.994872]\n",
      "epoch:12 step:10067 [D loss: 0.011341, acc.: 100.00%] [G loss: 2.837061]\n",
      "epoch:12 step:10068 [D loss: 0.026762, acc.: 99.22%] [G loss: 1.517560]\n",
      "epoch:12 step:10069 [D loss: 0.068546, acc.: 98.44%] [G loss: 2.802947]\n",
      "epoch:12 step:10070 [D loss: 0.046615, acc.: 99.22%] [G loss: 3.495945]\n",
      "epoch:12 step:10071 [D loss: 0.031668, acc.: 99.22%] [G loss: 2.850678]\n",
      "epoch:12 step:10072 [D loss: 1.662059, acc.: 45.31%] [G loss: 8.543539]\n",
      "epoch:12 step:10073 [D loss: 2.023876, acc.: 50.00%] [G loss: 2.519989]\n",
      "epoch:12 step:10074 [D loss: 0.334192, acc.: 84.38%] [G loss: 4.111243]\n",
      "epoch:12 step:10075 [D loss: 0.139308, acc.: 94.53%] [G loss: 5.488676]\n",
      "epoch:12 step:10076 [D loss: 0.389956, acc.: 82.81%] [G loss: 3.547633]\n",
      "epoch:12 step:10077 [D loss: 0.146256, acc.: 94.53%] [G loss: 4.718246]\n",
      "epoch:12 step:10078 [D loss: 0.024872, acc.: 100.00%] [G loss: 4.579362]\n",
      "epoch:12 step:10079 [D loss: 0.496779, acc.: 74.22%] [G loss: 2.828268]\n",
      "epoch:12 step:10080 [D loss: 0.054687, acc.: 99.22%] [G loss: 3.399328]\n",
      "epoch:12 step:10081 [D loss: 0.133595, acc.: 97.66%] [G loss: 2.309178]\n",
      "epoch:12 step:10082 [D loss: 0.346773, acc.: 86.72%] [G loss: 4.231159]\n",
      "epoch:12 step:10083 [D loss: 0.116560, acc.: 96.09%] [G loss: 3.512630]\n",
      "epoch:12 step:10084 [D loss: 0.081522, acc.: 99.22%] [G loss: 3.458711]\n",
      "epoch:12 step:10085 [D loss: 1.084584, acc.: 43.75%] [G loss: 4.599955]\n",
      "epoch:12 step:10086 [D loss: 0.145177, acc.: 93.75%] [G loss: 4.091133]\n",
      "epoch:12 step:10087 [D loss: 0.631683, acc.: 66.41%] [G loss: 3.913537]\n",
      "epoch:12 step:10088 [D loss: 0.020495, acc.: 100.00%] [G loss: 4.642809]\n",
      "epoch:12 step:10089 [D loss: 0.030961, acc.: 99.22%] [G loss: 4.219984]\n",
      "epoch:12 step:10090 [D loss: 0.059475, acc.: 97.66%] [G loss: 3.139879]\n",
      "epoch:12 step:10091 [D loss: 0.055623, acc.: 99.22%] [G loss: 2.734052]\n",
      "epoch:12 step:10092 [D loss: 0.057402, acc.: 100.00%] [G loss: 2.663011]\n",
      "epoch:12 step:10093 [D loss: 0.252201, acc.: 91.41%] [G loss: 3.921483]\n",
      "epoch:12 step:10094 [D loss: 0.055501, acc.: 98.44%] [G loss: 3.712974]\n",
      "epoch:12 step:10095 [D loss: 0.063513, acc.: 98.44%] [G loss: 2.732606]\n",
      "epoch:12 step:10096 [D loss: 0.125069, acc.: 97.66%] [G loss: 3.722766]\n",
      "epoch:12 step:10097 [D loss: 0.761185, acc.: 56.25%] [G loss: 3.445817]\n",
      "epoch:12 step:10098 [D loss: 0.089908, acc.: 95.31%] [G loss: 3.359978]\n",
      "epoch:12 step:10099 [D loss: 0.042328, acc.: 100.00%] [G loss: 2.530667]\n",
      "epoch:12 step:10100 [D loss: 0.088441, acc.: 98.44%] [G loss: 1.277546]\n",
      "epoch:12 step:10101 [D loss: 0.050040, acc.: 99.22%] [G loss: 1.755861]\n",
      "epoch:12 step:10102 [D loss: 0.020846, acc.: 100.00%] [G loss: 1.115227]\n",
      "epoch:12 step:10103 [D loss: 0.081082, acc.: 99.22%] [G loss: 1.289098]\n",
      "epoch:12 step:10104 [D loss: 0.267605, acc.: 87.50%] [G loss: 5.110981]\n",
      "epoch:12 step:10105 [D loss: 0.644917, acc.: 67.19%] [G loss: 3.315534]\n",
      "epoch:12 step:10106 [D loss: 0.092686, acc.: 99.22%] [G loss: 2.929595]\n",
      "epoch:12 step:10107 [D loss: 0.135502, acc.: 96.88%] [G loss: 3.221396]\n",
      "epoch:12 step:10108 [D loss: 0.089777, acc.: 98.44%] [G loss: 3.769754]\n",
      "epoch:12 step:10109 [D loss: 0.102640, acc.: 96.88%] [G loss: 2.929310]\n",
      "epoch:12 step:10110 [D loss: 0.092856, acc.: 97.66%] [G loss: 3.528967]\n",
      "epoch:12 step:10111 [D loss: 0.058140, acc.: 99.22%] [G loss: 3.564572]\n",
      "epoch:12 step:10112 [D loss: 0.066864, acc.: 98.44%] [G loss: 3.569726]\n",
      "epoch:12 step:10113 [D loss: 0.078898, acc.: 99.22%] [G loss: 1.531899]\n",
      "epoch:12 step:10114 [D loss: 0.169389, acc.: 94.53%] [G loss: 4.619730]\n",
      "epoch:12 step:10115 [D loss: 0.110878, acc.: 95.31%] [G loss: 3.350480]\n",
      "epoch:12 step:10116 [D loss: 0.071727, acc.: 99.22%] [G loss: 3.444255]\n",
      "epoch:12 step:10117 [D loss: 0.052090, acc.: 99.22%] [G loss: 3.898592]\n",
      "epoch:12 step:10118 [D loss: 0.054422, acc.: 99.22%] [G loss: 3.743443]\n",
      "epoch:12 step:10119 [D loss: 0.121702, acc.: 96.09%] [G loss: 2.280896]\n",
      "epoch:12 step:10120 [D loss: 0.157666, acc.: 96.88%] [G loss: 3.650181]\n",
      "epoch:12 step:10121 [D loss: 0.069163, acc.: 97.66%] [G loss: 3.305448]\n",
      "epoch:12 step:10122 [D loss: 0.565232, acc.: 72.66%] [G loss: 5.417516]\n",
      "epoch:12 step:10123 [D loss: 0.151860, acc.: 92.19%] [G loss: 5.004860]\n",
      "epoch:12 step:10124 [D loss: 0.065329, acc.: 97.66%] [G loss: 3.644725]\n",
      "epoch:12 step:10125 [D loss: 0.214816, acc.: 93.75%] [G loss: 3.845775]\n",
      "epoch:12 step:10126 [D loss: 0.057358, acc.: 98.44%] [G loss: 4.668882]\n",
      "epoch:12 step:10127 [D loss: 0.014160, acc.: 100.00%] [G loss: 3.114578]\n",
      "epoch:12 step:10128 [D loss: 0.032361, acc.: 100.00%] [G loss: 4.279340]\n",
      "epoch:12 step:10129 [D loss: 0.088792, acc.: 97.66%] [G loss: 3.466352]\n",
      "epoch:12 step:10130 [D loss: 0.023848, acc.: 100.00%] [G loss: 4.629708]\n",
      "epoch:12 step:10131 [D loss: 0.068007, acc.: 100.00%] [G loss: 3.740353]\n",
      "epoch:12 step:10132 [D loss: 0.071305, acc.: 99.22%] [G loss: 3.404748]\n",
      "epoch:12 step:10133 [D loss: 0.029345, acc.: 100.00%] [G loss: 3.989816]\n",
      "epoch:12 step:10134 [D loss: 2.456863, acc.: 14.84%] [G loss: 6.000664]\n",
      "epoch:12 step:10135 [D loss: 1.258339, acc.: 52.34%] [G loss: 4.204223]\n",
      "epoch:12 step:10136 [D loss: 0.274397, acc.: 88.28%] [G loss: 2.889145]\n",
      "epoch:12 step:10137 [D loss: 0.068432, acc.: 99.22%] [G loss: 3.263893]\n",
      "epoch:12 step:10138 [D loss: 0.094543, acc.: 97.66%] [G loss: 3.701514]\n",
      "epoch:12 step:10139 [D loss: 0.054972, acc.: 100.00%] [G loss: 3.751966]\n",
      "epoch:12 step:10140 [D loss: 0.081700, acc.: 98.44%] [G loss: 3.355196]\n",
      "epoch:12 step:10141 [D loss: 0.111870, acc.: 97.66%] [G loss: 4.171747]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:12 step:10142 [D loss: 0.020119, acc.: 100.00%] [G loss: 4.336401]\n",
      "epoch:12 step:10143 [D loss: 0.468646, acc.: 77.34%] [G loss: 3.232929]\n",
      "epoch:12 step:10144 [D loss: 0.031472, acc.: 100.00%] [G loss: 2.329315]\n",
      "epoch:12 step:10145 [D loss: 0.042614, acc.: 99.22%] [G loss: 3.723660]\n",
      "epoch:12 step:10146 [D loss: 0.054469, acc.: 99.22%] [G loss: 3.713206]\n",
      "epoch:12 step:10147 [D loss: 0.045018, acc.: 100.00%] [G loss: 3.911515]\n",
      "epoch:12 step:10148 [D loss: 0.102912, acc.: 98.44%] [G loss: 3.347743]\n",
      "epoch:12 step:10149 [D loss: 0.082445, acc.: 99.22%] [G loss: 3.422389]\n",
      "epoch:12 step:10150 [D loss: 0.071870, acc.: 98.44%] [G loss: 2.297424]\n",
      "epoch:12 step:10151 [D loss: 0.702064, acc.: 62.50%] [G loss: 4.797626]\n",
      "epoch:12 step:10152 [D loss: 0.327480, acc.: 83.59%] [G loss: 3.829237]\n",
      "epoch:12 step:10153 [D loss: 0.151032, acc.: 95.31%] [G loss: 2.361725]\n",
      "epoch:13 step:10154 [D loss: 0.613878, acc.: 71.09%] [G loss: 3.968667]\n",
      "epoch:13 step:10155 [D loss: 0.112147, acc.: 95.31%] [G loss: 4.508059]\n",
      "epoch:13 step:10156 [D loss: 0.369765, acc.: 81.25%] [G loss: 3.337594]\n",
      "epoch:13 step:10157 [D loss: 0.038641, acc.: 100.00%] [G loss: 2.427119]\n",
      "epoch:13 step:10158 [D loss: 0.110158, acc.: 95.31%] [G loss: 3.841907]\n",
      "epoch:13 step:10159 [D loss: 0.013018, acc.: 100.00%] [G loss: 5.039060]\n",
      "epoch:13 step:10160 [D loss: 0.087462, acc.: 96.88%] [G loss: 3.370171]\n",
      "epoch:13 step:10161 [D loss: 0.044474, acc.: 98.44%] [G loss: 3.382678]\n",
      "epoch:13 step:10162 [D loss: 0.049596, acc.: 99.22%] [G loss: 3.533619]\n",
      "epoch:13 step:10163 [D loss: 0.043764, acc.: 100.00%] [G loss: 2.930272]\n",
      "epoch:13 step:10164 [D loss: 0.031518, acc.: 100.00%] [G loss: 3.169506]\n",
      "epoch:13 step:10165 [D loss: 0.050561, acc.: 98.44%] [G loss: 2.681326]\n",
      "epoch:13 step:10166 [D loss: 0.042941, acc.: 99.22%] [G loss: 2.283451]\n",
      "epoch:13 step:10167 [D loss: 0.085741, acc.: 98.44%] [G loss: 2.355366]\n",
      "epoch:13 step:10168 [D loss: 0.211218, acc.: 89.84%] [G loss: 5.169312]\n",
      "epoch:13 step:10169 [D loss: 0.235471, acc.: 89.84%] [G loss: 3.960049]\n",
      "epoch:13 step:10170 [D loss: 0.058346, acc.: 98.44%] [G loss: 4.419279]\n",
      "epoch:13 step:10171 [D loss: 0.011420, acc.: 100.00%] [G loss: 3.494636]\n",
      "epoch:13 step:10172 [D loss: 0.029753, acc.: 100.00%] [G loss: 3.972138]\n",
      "epoch:13 step:10173 [D loss: 0.022696, acc.: 100.00%] [G loss: 4.323601]\n",
      "epoch:13 step:10174 [D loss: 0.035070, acc.: 100.00%] [G loss: 3.500072]\n",
      "epoch:13 step:10175 [D loss: 0.046627, acc.: 100.00%] [G loss: 3.234788]\n",
      "epoch:13 step:10176 [D loss: 0.118678, acc.: 97.66%] [G loss: 4.413291]\n",
      "epoch:13 step:10177 [D loss: 0.064740, acc.: 97.66%] [G loss: 3.880693]\n",
      "epoch:13 step:10178 [D loss: 0.066230, acc.: 97.66%] [G loss: 1.281009]\n",
      "epoch:13 step:10179 [D loss: 0.094574, acc.: 99.22%] [G loss: 3.263088]\n",
      "epoch:13 step:10180 [D loss: 0.015386, acc.: 100.00%] [G loss: 3.072468]\n",
      "epoch:13 step:10181 [D loss: 0.269261, acc.: 86.72%] [G loss: 3.528632]\n",
      "epoch:13 step:10182 [D loss: 0.019672, acc.: 100.00%] [G loss: 4.615890]\n",
      "epoch:13 step:10183 [D loss: 0.163565, acc.: 93.75%] [G loss: 2.964514]\n",
      "epoch:13 step:10184 [D loss: 0.018062, acc.: 100.00%] [G loss: 2.521364]\n",
      "epoch:13 step:10185 [D loss: 0.016422, acc.: 100.00%] [G loss: 2.250408]\n",
      "epoch:13 step:10186 [D loss: 0.358569, acc.: 86.72%] [G loss: 6.445922]\n",
      "epoch:13 step:10187 [D loss: 0.095102, acc.: 96.09%] [G loss: 5.487168]\n",
      "epoch:13 step:10188 [D loss: 0.504191, acc.: 70.31%] [G loss: 4.629510]\n",
      "epoch:13 step:10189 [D loss: 0.573122, acc.: 75.78%] [G loss: 4.359811]\n",
      "epoch:13 step:10190 [D loss: 0.004591, acc.: 100.00%] [G loss: 4.873445]\n",
      "epoch:13 step:10191 [D loss: 0.062258, acc.: 98.44%] [G loss: 4.220547]\n",
      "epoch:13 step:10192 [D loss: 0.028112, acc.: 100.00%] [G loss: 3.540596]\n",
      "epoch:13 step:10193 [D loss: 0.104795, acc.: 97.66%] [G loss: 5.652278]\n",
      "epoch:13 step:10194 [D loss: 0.045901, acc.: 98.44%] [G loss: 5.800996]\n",
      "epoch:13 step:10195 [D loss: 0.119272, acc.: 96.09%] [G loss: 4.260456]\n",
      "epoch:13 step:10196 [D loss: 0.046740, acc.: 100.00%] [G loss: 3.282680]\n",
      "epoch:13 step:10197 [D loss: 0.197055, acc.: 90.62%] [G loss: 5.248412]\n",
      "epoch:13 step:10198 [D loss: 0.202126, acc.: 91.41%] [G loss: 1.241332]\n",
      "epoch:13 step:10199 [D loss: 0.470783, acc.: 81.25%] [G loss: 8.108532]\n",
      "epoch:13 step:10200 [D loss: 0.933099, acc.: 62.50%] [G loss: 1.770163]\n",
      "##############\n",
      "[0.93569874 0.91648372 0.94293137 0.95912759 2.11262626 0.93632082\n",
      " 0.98116125 2.10725105 0.91332055 1.09660422]\n",
      "##########\n",
      "epoch:13 step:10201 [D loss: 1.404277, acc.: 51.56%] [G loss: 7.984504]\n",
      "epoch:13 step:10202 [D loss: 2.388506, acc.: 50.00%] [G loss: 4.766642]\n",
      "epoch:13 step:10203 [D loss: 0.922976, acc.: 54.69%] [G loss: 1.419049]\n",
      "epoch:13 step:10204 [D loss: 0.453701, acc.: 81.25%] [G loss: 1.441620]\n",
      "epoch:13 step:10205 [D loss: 0.129827, acc.: 96.88%] [G loss: 2.056176]\n",
      "epoch:13 step:10206 [D loss: 0.391692, acc.: 82.03%] [G loss: 2.555893]\n",
      "epoch:13 step:10207 [D loss: 0.381010, acc.: 83.59%] [G loss: 2.950474]\n",
      "epoch:13 step:10208 [D loss: 0.285655, acc.: 92.97%] [G loss: 2.173166]\n",
      "epoch:13 step:10209 [D loss: 1.425232, acc.: 24.22%] [G loss: 3.124900]\n",
      "epoch:13 step:10210 [D loss: 0.384818, acc.: 78.12%] [G loss: 2.516988]\n",
      "epoch:13 step:10211 [D loss: 0.189065, acc.: 93.75%] [G loss: 1.459449]\n",
      "epoch:13 step:10212 [D loss: 0.411661, acc.: 76.56%] [G loss: 2.647605]\n",
      "epoch:13 step:10213 [D loss: 0.319452, acc.: 84.38%] [G loss: 2.085496]\n",
      "epoch:13 step:10214 [D loss: 0.348236, acc.: 85.94%] [G loss: 1.939197]\n",
      "epoch:13 step:10215 [D loss: 0.196918, acc.: 94.53%] [G loss: 1.444853]\n",
      "epoch:13 step:10216 [D loss: 0.135639, acc.: 96.09%] [G loss: 1.394567]\n",
      "epoch:13 step:10217 [D loss: 0.597521, acc.: 67.97%] [G loss: 2.930340]\n",
      "epoch:13 step:10218 [D loss: 0.754450, acc.: 63.28%] [G loss: 1.354684]\n",
      "epoch:13 step:10219 [D loss: 0.312780, acc.: 86.72%] [G loss: 2.686540]\n",
      "epoch:13 step:10220 [D loss: 0.115051, acc.: 96.88%] [G loss: 2.382698]\n",
      "epoch:13 step:10221 [D loss: 0.484931, acc.: 76.56%] [G loss: 1.872571]\n",
      "epoch:13 step:10222 [D loss: 0.097419, acc.: 100.00%] [G loss: 2.259842]\n",
      "epoch:13 step:10223 [D loss: 0.392358, acc.: 80.47%] [G loss: 2.644514]\n",
      "epoch:13 step:10224 [D loss: 0.338514, acc.: 84.38%] [G loss: 3.211626]\n",
      "epoch:13 step:10225 [D loss: 0.126201, acc.: 97.66%] [G loss: 2.883394]\n",
      "epoch:13 step:10226 [D loss: 0.426708, acc.: 82.81%] [G loss: 1.501169]\n",
      "epoch:13 step:10227 [D loss: 0.226945, acc.: 91.41%] [G loss: 3.579072]\n",
      "epoch:13 step:10228 [D loss: 0.112095, acc.: 97.66%] [G loss: 3.618543]\n",
      "epoch:13 step:10229 [D loss: 0.365067, acc.: 84.38%] [G loss: 2.089643]\n",
      "epoch:13 step:10230 [D loss: 0.188537, acc.: 94.53%] [G loss: 3.916323]\n",
      "epoch:13 step:10231 [D loss: 0.105064, acc.: 98.44%] [G loss: 3.896568]\n",
      "epoch:13 step:10232 [D loss: 0.228498, acc.: 92.97%] [G loss: 2.957482]\n",
      "epoch:13 step:10233 [D loss: 0.128771, acc.: 97.66%] [G loss: 3.116590]\n",
      "epoch:13 step:10234 [D loss: 0.204086, acc.: 92.19%] [G loss: 3.654039]\n",
      "epoch:13 step:10235 [D loss: 0.177108, acc.: 92.97%] [G loss: 4.164012]\n",
      "epoch:13 step:10236 [D loss: 0.215633, acc.: 90.62%] [G loss: 2.669118]\n",
      "epoch:13 step:10237 [D loss: 0.360526, acc.: 84.38%] [G loss: 3.418944]\n",
      "epoch:13 step:10238 [D loss: 0.085920, acc.: 96.88%] [G loss: 3.734700]\n",
      "epoch:13 step:10239 [D loss: 0.140244, acc.: 95.31%] [G loss: 1.495668]\n",
      "epoch:13 step:10240 [D loss: 0.021181, acc.: 100.00%] [G loss: 1.079821]\n",
      "epoch:13 step:10241 [D loss: 0.051807, acc.: 100.00%] [G loss: 0.504077]\n",
      "epoch:13 step:10242 [D loss: 0.422167, acc.: 78.12%] [G loss: 4.869740]\n",
      "epoch:13 step:10243 [D loss: 1.085941, acc.: 60.16%] [G loss: 2.696671]\n",
      "epoch:13 step:10244 [D loss: 0.244702, acc.: 92.97%] [G loss: 0.857729]\n",
      "epoch:13 step:10245 [D loss: 0.050058, acc.: 100.00%] [G loss: 0.502838]\n",
      "epoch:13 step:10246 [D loss: 0.085344, acc.: 100.00%] [G loss: 0.895800]\n",
      "epoch:13 step:10247 [D loss: 0.086395, acc.: 99.22%] [G loss: 1.006038]\n",
      "epoch:13 step:10248 [D loss: 0.055769, acc.: 100.00%] [G loss: 0.884513]\n",
      "epoch:13 step:10249 [D loss: 0.158931, acc.: 95.31%] [G loss: 1.813307]\n",
      "epoch:13 step:10250 [D loss: 0.102859, acc.: 97.66%] [G loss: 1.513478]\n",
      "epoch:13 step:10251 [D loss: 0.076430, acc.: 99.22%] [G loss: 1.877305]\n",
      "epoch:13 step:10252 [D loss: 0.191416, acc.: 92.97%] [G loss: 2.745480]\n",
      "epoch:13 step:10253 [D loss: 0.215300, acc.: 92.97%] [G loss: 0.882047]\n",
      "epoch:13 step:10254 [D loss: 0.357675, acc.: 83.59%] [G loss: 4.381919]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:13 step:10255 [D loss: 0.820550, acc.: 59.38%] [G loss: 1.522965]\n",
      "epoch:13 step:10256 [D loss: 0.176752, acc.: 93.75%] [G loss: 1.793496]\n",
      "epoch:13 step:10257 [D loss: 0.072262, acc.: 98.44%] [G loss: 2.950924]\n",
      "epoch:13 step:10258 [D loss: 0.060387, acc.: 99.22%] [G loss: 2.535947]\n",
      "epoch:13 step:10259 [D loss: 0.042797, acc.: 100.00%] [G loss: 2.638070]\n",
      "epoch:13 step:10260 [D loss: 0.124877, acc.: 96.09%] [G loss: 1.795637]\n",
      "epoch:13 step:10261 [D loss: 0.216814, acc.: 91.41%] [G loss: 2.690868]\n",
      "epoch:13 step:10262 [D loss: 0.066817, acc.: 98.44%] [G loss: 3.407250]\n",
      "epoch:13 step:10263 [D loss: 0.342818, acc.: 87.50%] [G loss: 4.163870]\n",
      "epoch:13 step:10264 [D loss: 0.086822, acc.: 96.88%] [G loss: 4.314703]\n",
      "epoch:13 step:10265 [D loss: 0.220833, acc.: 89.84%] [G loss: 2.647979]\n",
      "epoch:13 step:10266 [D loss: 0.136093, acc.: 95.31%] [G loss: 4.446949]\n",
      "epoch:13 step:10267 [D loss: 0.040207, acc.: 100.00%] [G loss: 4.542285]\n",
      "epoch:13 step:10268 [D loss: 0.020307, acc.: 100.00%] [G loss: 3.976892]\n",
      "epoch:13 step:10269 [D loss: 0.189638, acc.: 92.97%] [G loss: 3.506443]\n",
      "epoch:13 step:10270 [D loss: 0.028882, acc.: 100.00%] [G loss: 3.378818]\n",
      "epoch:13 step:10271 [D loss: 0.026774, acc.: 100.00%] [G loss: 4.098407]\n",
      "epoch:13 step:10272 [D loss: 0.147317, acc.: 96.88%] [G loss: 4.436260]\n",
      "epoch:13 step:10273 [D loss: 0.055672, acc.: 99.22%] [G loss: 4.422228]\n",
      "epoch:13 step:10274 [D loss: 0.047769, acc.: 100.00%] [G loss: 4.570018]\n",
      "epoch:13 step:10275 [D loss: 0.053069, acc.: 100.00%] [G loss: 4.017101]\n",
      "epoch:13 step:10276 [D loss: 0.054330, acc.: 99.22%] [G loss: 3.908027]\n",
      "epoch:13 step:10277 [D loss: 0.024006, acc.: 100.00%] [G loss: 4.056014]\n",
      "epoch:13 step:10278 [D loss: 0.065102, acc.: 100.00%] [G loss: 3.581988]\n",
      "epoch:13 step:10279 [D loss: 0.040055, acc.: 100.00%] [G loss: 3.251811]\n",
      "epoch:13 step:10280 [D loss: 0.085304, acc.: 96.09%] [G loss: 3.881754]\n",
      "epoch:13 step:10281 [D loss: 0.094187, acc.: 96.88%] [G loss: 2.990489]\n",
      "epoch:13 step:10282 [D loss: 0.107035, acc.: 96.88%] [G loss: 4.599177]\n",
      "epoch:13 step:10283 [D loss: 0.057167, acc.: 99.22%] [G loss: 3.953254]\n",
      "epoch:13 step:10284 [D loss: 0.017038, acc.: 100.00%] [G loss: 3.796255]\n",
      "epoch:13 step:10285 [D loss: 0.516574, acc.: 69.53%] [G loss: 6.240227]\n",
      "epoch:13 step:10286 [D loss: 0.882047, acc.: 60.94%] [G loss: 2.130107]\n",
      "epoch:13 step:10287 [D loss: 0.827636, acc.: 63.28%] [G loss: 6.367866]\n",
      "epoch:13 step:10288 [D loss: 1.109678, acc.: 56.25%] [G loss: 4.666541]\n",
      "epoch:13 step:10289 [D loss: 0.067324, acc.: 100.00%] [G loss: 4.196216]\n",
      "epoch:13 step:10290 [D loss: 0.134663, acc.: 92.97%] [G loss: 4.701361]\n",
      "epoch:13 step:10291 [D loss: 0.015701, acc.: 100.00%] [G loss: 4.884706]\n",
      "epoch:13 step:10292 [D loss: 0.063658, acc.: 99.22%] [G loss: 4.520071]\n",
      "epoch:13 step:10293 [D loss: 0.033442, acc.: 99.22%] [G loss: 2.761161]\n",
      "epoch:13 step:10294 [D loss: 0.051320, acc.: 97.66%] [G loss: 3.121405]\n",
      "epoch:13 step:10295 [D loss: 0.038222, acc.: 99.22%] [G loss: 2.974020]\n",
      "epoch:13 step:10296 [D loss: 0.024559, acc.: 100.00%] [G loss: 3.239548]\n",
      "epoch:13 step:10297 [D loss: 0.047410, acc.: 100.00%] [G loss: 2.584022]\n",
      "epoch:13 step:10298 [D loss: 0.091322, acc.: 99.22%] [G loss: 2.027135]\n",
      "epoch:13 step:10299 [D loss: 0.072095, acc.: 100.00%] [G loss: 1.556164]\n",
      "epoch:13 step:10300 [D loss: 0.158374, acc.: 93.75%] [G loss: 3.119190]\n",
      "epoch:13 step:10301 [D loss: 0.062686, acc.: 100.00%] [G loss: 3.374982]\n",
      "epoch:13 step:10302 [D loss: 0.163336, acc.: 93.75%] [G loss: 1.771887]\n",
      "epoch:13 step:10303 [D loss: 0.241552, acc.: 88.28%] [G loss: 4.577876]\n",
      "epoch:13 step:10304 [D loss: 0.128963, acc.: 95.31%] [G loss: 4.496543]\n",
      "epoch:13 step:10305 [D loss: 0.198812, acc.: 93.75%] [G loss: 1.426975]\n",
      "epoch:13 step:10306 [D loss: 0.043895, acc.: 100.00%] [G loss: 1.477115]\n",
      "epoch:13 step:10307 [D loss: 0.149834, acc.: 94.53%] [G loss: 2.317090]\n",
      "epoch:13 step:10308 [D loss: 0.019621, acc.: 100.00%] [G loss: 2.896041]\n",
      "epoch:13 step:10309 [D loss: 0.058293, acc.: 99.22%] [G loss: 2.920253]\n",
      "epoch:13 step:10310 [D loss: 0.116889, acc.: 98.44%] [G loss: 2.349825]\n",
      "epoch:13 step:10311 [D loss: 0.337871, acc.: 85.16%] [G loss: 3.894812]\n",
      "epoch:13 step:10312 [D loss: 0.008832, acc.: 100.00%] [G loss: 5.268689]\n",
      "epoch:13 step:10313 [D loss: 0.191233, acc.: 91.41%] [G loss: 2.260144]\n",
      "epoch:13 step:10314 [D loss: 0.164947, acc.: 95.31%] [G loss: 3.683422]\n",
      "epoch:13 step:10315 [D loss: 0.008780, acc.: 100.00%] [G loss: 5.235010]\n",
      "epoch:13 step:10316 [D loss: 0.040046, acc.: 99.22%] [G loss: 3.815534]\n",
      "epoch:13 step:10317 [D loss: 0.023558, acc.: 100.00%] [G loss: 3.489974]\n",
      "epoch:13 step:10318 [D loss: 0.069916, acc.: 99.22%] [G loss: 3.605764]\n",
      "epoch:13 step:10319 [D loss: 0.024989, acc.: 100.00%] [G loss: 2.709064]\n",
      "epoch:13 step:10320 [D loss: 0.052474, acc.: 98.44%] [G loss: 3.035583]\n",
      "epoch:13 step:10321 [D loss: 0.061766, acc.: 100.00%] [G loss: 4.230964]\n",
      "epoch:13 step:10322 [D loss: 0.086761, acc.: 98.44%] [G loss: 3.848755]\n",
      "epoch:13 step:10323 [D loss: 0.774397, acc.: 59.38%] [G loss: 6.788971]\n",
      "epoch:13 step:10324 [D loss: 0.797881, acc.: 64.84%] [G loss: 3.812997]\n",
      "epoch:13 step:10325 [D loss: 0.009773, acc.: 100.00%] [G loss: 2.919495]\n",
      "epoch:13 step:10326 [D loss: 0.200769, acc.: 90.62%] [G loss: 4.193300]\n",
      "epoch:13 step:10327 [D loss: 0.005882, acc.: 100.00%] [G loss: 5.316136]\n",
      "epoch:13 step:10328 [D loss: 0.033768, acc.: 100.00%] [G loss: 4.626449]\n",
      "epoch:13 step:10329 [D loss: 0.272250, acc.: 88.28%] [G loss: 2.014665]\n",
      "epoch:13 step:10330 [D loss: 0.107411, acc.: 99.22%] [G loss: 2.597154]\n",
      "epoch:13 step:10331 [D loss: 0.018374, acc.: 100.00%] [G loss: 4.186686]\n",
      "epoch:13 step:10332 [D loss: 0.045011, acc.: 100.00%] [G loss: 3.657969]\n",
      "epoch:13 step:10333 [D loss: 0.030877, acc.: 100.00%] [G loss: 2.000687]\n",
      "epoch:13 step:10334 [D loss: 0.091905, acc.: 97.66%] [G loss: 1.665551]\n",
      "epoch:13 step:10335 [D loss: 0.130341, acc.: 96.09%] [G loss: 4.329793]\n",
      "epoch:13 step:10336 [D loss: 0.486366, acc.: 76.56%] [G loss: 2.636975]\n",
      "epoch:13 step:10337 [D loss: 0.231325, acc.: 91.41%] [G loss: 6.523686]\n",
      "epoch:13 step:10338 [D loss: 0.112168, acc.: 94.53%] [G loss: 6.325170]\n",
      "epoch:13 step:10339 [D loss: 0.030080, acc.: 99.22%] [G loss: 5.088335]\n",
      "epoch:13 step:10340 [D loss: 0.258322, acc.: 89.06%] [G loss: 3.081822]\n",
      "epoch:13 step:10341 [D loss: 0.059649, acc.: 99.22%] [G loss: 4.046614]\n",
      "epoch:13 step:10342 [D loss: 0.034959, acc.: 99.22%] [G loss: 5.342999]\n",
      "epoch:13 step:10343 [D loss: 0.113209, acc.: 96.88%] [G loss: 1.889079]\n",
      "epoch:13 step:10344 [D loss: 0.743327, acc.: 59.38%] [G loss: 8.132931]\n",
      "epoch:13 step:10345 [D loss: 1.142551, acc.: 54.69%] [G loss: 5.728194]\n",
      "epoch:13 step:10346 [D loss: 0.130986, acc.: 96.09%] [G loss: 3.155899]\n",
      "epoch:13 step:10347 [D loss: 0.034695, acc.: 100.00%] [G loss: 3.078432]\n",
      "epoch:13 step:10348 [D loss: 0.115667, acc.: 96.88%] [G loss: 4.458413]\n",
      "epoch:13 step:10349 [D loss: 0.022161, acc.: 100.00%] [G loss: 5.106151]\n",
      "epoch:13 step:10350 [D loss: 0.323109, acc.: 85.94%] [G loss: 2.008555]\n",
      "epoch:13 step:10351 [D loss: 0.240289, acc.: 89.06%] [G loss: 4.493188]\n",
      "epoch:13 step:10352 [D loss: 0.168858, acc.: 94.53%] [G loss: 4.989527]\n",
      "epoch:13 step:10353 [D loss: 0.106869, acc.: 98.44%] [G loss: 3.141100]\n",
      "epoch:13 step:10354 [D loss: 0.027931, acc.: 100.00%] [G loss: 3.697156]\n",
      "epoch:13 step:10355 [D loss: 0.186088, acc.: 93.75%] [G loss: 3.641119]\n",
      "epoch:13 step:10356 [D loss: 0.089088, acc.: 98.44%] [G loss: 6.376703]\n",
      "epoch:13 step:10357 [D loss: 0.142346, acc.: 91.41%] [G loss: 4.179129]\n",
      "epoch:13 step:10358 [D loss: 0.094140, acc.: 96.88%] [G loss: 2.240219]\n",
      "epoch:13 step:10359 [D loss: 0.006478, acc.: 100.00%] [G loss: 1.466884]\n",
      "epoch:13 step:10360 [D loss: 0.145591, acc.: 94.53%] [G loss: 2.375595]\n",
      "epoch:13 step:10361 [D loss: 0.163476, acc.: 92.97%] [G loss: 1.347377]\n",
      "epoch:13 step:10362 [D loss: 0.406462, acc.: 78.91%] [G loss: 5.655620]\n",
      "epoch:13 step:10363 [D loss: 0.303961, acc.: 83.59%] [G loss: 2.877817]\n",
      "epoch:13 step:10364 [D loss: 0.113372, acc.: 95.31%] [G loss: 3.742992]\n",
      "epoch:13 step:10365 [D loss: 0.005448, acc.: 100.00%] [G loss: 2.623112]\n",
      "epoch:13 step:10366 [D loss: 0.271425, acc.: 86.72%] [G loss: 4.508615]\n",
      "epoch:13 step:10367 [D loss: 0.564203, acc.: 71.88%] [G loss: 2.342972]\n",
      "epoch:13 step:10368 [D loss: 0.042054, acc.: 100.00%] [G loss: 2.031334]\n",
      "epoch:13 step:10369 [D loss: 0.075115, acc.: 98.44%] [G loss: 3.811334]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:13 step:10370 [D loss: 0.019247, acc.: 100.00%] [G loss: 4.264224]\n",
      "epoch:13 step:10371 [D loss: 0.326508, acc.: 86.72%] [G loss: 3.114167]\n",
      "epoch:13 step:10372 [D loss: 0.017860, acc.: 100.00%] [G loss: 4.196172]\n",
      "epoch:13 step:10373 [D loss: 0.016185, acc.: 100.00%] [G loss: 4.649347]\n",
      "epoch:13 step:10374 [D loss: 0.028475, acc.: 100.00%] [G loss: 4.347106]\n",
      "epoch:13 step:10375 [D loss: 0.032822, acc.: 100.00%] [G loss: 4.534432]\n",
      "epoch:13 step:10376 [D loss: 0.013569, acc.: 100.00%] [G loss: 3.810365]\n",
      "epoch:13 step:10377 [D loss: 0.042260, acc.: 100.00%] [G loss: 3.720348]\n",
      "epoch:13 step:10378 [D loss: 0.008189, acc.: 100.00%] [G loss: 3.728949]\n",
      "epoch:13 step:10379 [D loss: 0.034512, acc.: 100.00%] [G loss: 3.782856]\n",
      "epoch:13 step:10380 [D loss: 0.031330, acc.: 100.00%] [G loss: 3.852418]\n",
      "epoch:13 step:10381 [D loss: 0.037251, acc.: 100.00%] [G loss: 4.022817]\n",
      "epoch:13 step:10382 [D loss: 0.295368, acc.: 89.84%] [G loss: 5.876666]\n",
      "epoch:13 step:10383 [D loss: 0.005683, acc.: 100.00%] [G loss: 6.999553]\n",
      "epoch:13 step:10384 [D loss: 0.125202, acc.: 93.75%] [G loss: 5.507524]\n",
      "epoch:13 step:10385 [D loss: 0.012782, acc.: 100.00%] [G loss: 4.701620]\n",
      "epoch:13 step:10386 [D loss: 0.021530, acc.: 100.00%] [G loss: 3.798531]\n",
      "epoch:13 step:10387 [D loss: 0.084887, acc.: 97.66%] [G loss: 4.622828]\n",
      "epoch:13 step:10388 [D loss: 0.007580, acc.: 100.00%] [G loss: 6.112875]\n",
      "epoch:13 step:10389 [D loss: 0.096572, acc.: 96.09%] [G loss: 5.039656]\n",
      "epoch:13 step:10390 [D loss: 0.069089, acc.: 100.00%] [G loss: 5.667495]\n",
      "epoch:13 step:10391 [D loss: 0.001988, acc.: 100.00%] [G loss: 6.867859]\n",
      "epoch:13 step:10392 [D loss: 0.085459, acc.: 97.66%] [G loss: 4.035091]\n",
      "epoch:13 step:10393 [D loss: 0.833854, acc.: 61.72%] [G loss: 7.927380]\n",
      "epoch:13 step:10394 [D loss: 2.615943, acc.: 50.00%] [G loss: 3.262948]\n",
      "epoch:13 step:10395 [D loss: 1.031286, acc.: 46.88%] [G loss: 3.695471]\n",
      "epoch:13 step:10396 [D loss: 0.139571, acc.: 95.31%] [G loss: 4.364240]\n",
      "epoch:13 step:10397 [D loss: 0.359133, acc.: 83.59%] [G loss: 2.863396]\n",
      "epoch:13 step:10398 [D loss: 0.063812, acc.: 99.22%] [G loss: 2.477476]\n",
      "epoch:13 step:10399 [D loss: 0.181481, acc.: 93.75%] [G loss: 3.451139]\n",
      "epoch:13 step:10400 [D loss: 0.078694, acc.: 98.44%] [G loss: 4.073866]\n",
      "##############\n",
      "[1.05888671 0.92312971 0.88532643 1.06013531 1.12499562 0.95599727\n",
      " 2.10604929 0.90041245 0.94024349 2.12683319]\n",
      "##########\n",
      "epoch:13 step:10401 [D loss: 0.020828, acc.: 100.00%] [G loss: 3.507366]\n",
      "epoch:13 step:10402 [D loss: 0.055787, acc.: 100.00%] [G loss: 3.295375]\n",
      "epoch:13 step:10403 [D loss: 0.055860, acc.: 98.44%] [G loss: 3.278455]\n",
      "epoch:13 step:10404 [D loss: 0.064294, acc.: 99.22%] [G loss: 3.334962]\n",
      "epoch:13 step:10405 [D loss: 0.051302, acc.: 100.00%] [G loss: 3.174321]\n",
      "epoch:13 step:10406 [D loss: 0.022788, acc.: 100.00%] [G loss: 3.655419]\n",
      "epoch:13 step:10407 [D loss: 0.060119, acc.: 100.00%] [G loss: 3.718966]\n",
      "epoch:13 step:10408 [D loss: 0.022840, acc.: 100.00%] [G loss: 3.407634]\n",
      "epoch:13 step:10409 [D loss: 0.109384, acc.: 97.66%] [G loss: 3.383733]\n",
      "epoch:13 step:10410 [D loss: 0.067385, acc.: 99.22%] [G loss: 4.166811]\n",
      "epoch:13 step:10411 [D loss: 0.031148, acc.: 100.00%] [G loss: 3.484057]\n",
      "epoch:13 step:10412 [D loss: 0.037996, acc.: 100.00%] [G loss: 3.635729]\n",
      "epoch:13 step:10413 [D loss: 0.031456, acc.: 100.00%] [G loss: 3.642252]\n",
      "epoch:13 step:10414 [D loss: 0.137502, acc.: 96.88%] [G loss: 2.476020]\n",
      "epoch:13 step:10415 [D loss: 0.013908, acc.: 100.00%] [G loss: 3.187624]\n",
      "epoch:13 step:10416 [D loss: 0.042565, acc.: 100.00%] [G loss: 2.804317]\n",
      "epoch:13 step:10417 [D loss: 0.747991, acc.: 58.59%] [G loss: 3.895518]\n",
      "epoch:13 step:10418 [D loss: 0.096712, acc.: 98.44%] [G loss: 3.810913]\n",
      "epoch:13 step:10419 [D loss: 0.212088, acc.: 89.06%] [G loss: 0.250235]\n",
      "epoch:13 step:10420 [D loss: 0.212486, acc.: 89.84%] [G loss: 0.519256]\n",
      "epoch:13 step:10421 [D loss: 0.011013, acc.: 100.00%] [G loss: 2.738499]\n",
      "epoch:13 step:10422 [D loss: 0.242822, acc.: 90.62%] [G loss: 0.330964]\n",
      "epoch:13 step:10423 [D loss: 0.125862, acc.: 94.53%] [G loss: 1.629132]\n",
      "epoch:13 step:10424 [D loss: 0.069376, acc.: 99.22%] [G loss: 2.010516]\n",
      "epoch:13 step:10425 [D loss: 0.020871, acc.: 100.00%] [G loss: 1.295699]\n",
      "epoch:13 step:10426 [D loss: 0.159591, acc.: 94.53%] [G loss: 1.312682]\n",
      "epoch:13 step:10427 [D loss: 0.164261, acc.: 92.97%] [G loss: 2.413723]\n",
      "epoch:13 step:10428 [D loss: 0.059137, acc.: 97.66%] [G loss: 4.924126]\n",
      "epoch:13 step:10429 [D loss: 0.240495, acc.: 90.62%] [G loss: 1.178873]\n",
      "epoch:13 step:10430 [D loss: 0.707573, acc.: 65.62%] [G loss: 7.104560]\n",
      "epoch:13 step:10431 [D loss: 1.815326, acc.: 51.56%] [G loss: 5.672063]\n",
      "epoch:13 step:10432 [D loss: 0.333179, acc.: 84.38%] [G loss: 2.632098]\n",
      "epoch:13 step:10433 [D loss: 0.556011, acc.: 73.44%] [G loss: 3.578352]\n",
      "epoch:13 step:10434 [D loss: 0.037121, acc.: 100.00%] [G loss: 4.589031]\n",
      "epoch:13 step:10435 [D loss: 0.171534, acc.: 93.75%] [G loss: 2.145804]\n",
      "epoch:13 step:10436 [D loss: 0.078804, acc.: 99.22%] [G loss: 2.330096]\n",
      "epoch:13 step:10437 [D loss: 0.052537, acc.: 100.00%] [G loss: 2.816993]\n",
      "epoch:13 step:10438 [D loss: 0.349580, acc.: 84.38%] [G loss: 4.510584]\n",
      "epoch:13 step:10439 [D loss: 0.418740, acc.: 75.78%] [G loss: 3.997154]\n",
      "epoch:13 step:10440 [D loss: 0.052883, acc.: 100.00%] [G loss: 2.927605]\n",
      "epoch:13 step:10441 [D loss: 0.020976, acc.: 100.00%] [G loss: 1.236506]\n",
      "epoch:13 step:10442 [D loss: 0.333771, acc.: 85.16%] [G loss: 3.265031]\n",
      "epoch:13 step:10443 [D loss: 0.126839, acc.: 95.31%] [G loss: 3.443483]\n",
      "epoch:13 step:10444 [D loss: 0.102260, acc.: 97.66%] [G loss: 1.316002]\n",
      "epoch:13 step:10445 [D loss: 0.127429, acc.: 95.31%] [G loss: 2.038609]\n",
      "epoch:13 step:10446 [D loss: 0.089917, acc.: 99.22%] [G loss: 3.542910]\n",
      "epoch:13 step:10447 [D loss: 0.240615, acc.: 92.97%] [G loss: 1.604245]\n",
      "epoch:13 step:10448 [D loss: 0.414303, acc.: 76.56%] [G loss: 5.409374]\n",
      "epoch:13 step:10449 [D loss: 0.528259, acc.: 73.44%] [G loss: 3.232422]\n",
      "epoch:13 step:10450 [D loss: 0.059378, acc.: 99.22%] [G loss: 2.621434]\n",
      "epoch:13 step:10451 [D loss: 0.110866, acc.: 96.88%] [G loss: 3.555988]\n",
      "epoch:13 step:10452 [D loss: 0.019933, acc.: 100.00%] [G loss: 2.529812]\n",
      "epoch:13 step:10453 [D loss: 0.215165, acc.: 92.97%] [G loss: 2.421514]\n",
      "epoch:13 step:10454 [D loss: 0.024989, acc.: 100.00%] [G loss: 4.000595]\n",
      "epoch:13 step:10455 [D loss: 0.048787, acc.: 99.22%] [G loss: 4.294664]\n",
      "epoch:13 step:10456 [D loss: 0.124590, acc.: 96.09%] [G loss: 3.148455]\n",
      "epoch:13 step:10457 [D loss: 0.050016, acc.: 100.00%] [G loss: 3.958159]\n",
      "epoch:13 step:10458 [D loss: 0.063356, acc.: 100.00%] [G loss: 3.904949]\n",
      "epoch:13 step:10459 [D loss: 0.012574, acc.: 100.00%] [G loss: 4.156731]\n",
      "epoch:13 step:10460 [D loss: 0.040578, acc.: 100.00%] [G loss: 4.234516]\n",
      "epoch:13 step:10461 [D loss: 0.050209, acc.: 99.22%] [G loss: 4.270424]\n",
      "epoch:13 step:10462 [D loss: 0.207582, acc.: 92.97%] [G loss: 3.279994]\n",
      "epoch:13 step:10463 [D loss: 0.040506, acc.: 99.22%] [G loss: 3.591482]\n",
      "epoch:13 step:10464 [D loss: 0.022707, acc.: 100.00%] [G loss: 4.058567]\n",
      "epoch:13 step:10465 [D loss: 0.040081, acc.: 100.00%] [G loss: 4.113957]\n",
      "epoch:13 step:10466 [D loss: 0.041556, acc.: 100.00%] [G loss: 4.023305]\n",
      "epoch:13 step:10467 [D loss: 0.024357, acc.: 100.00%] [G loss: 4.461904]\n",
      "epoch:13 step:10468 [D loss: 1.408490, acc.: 46.09%] [G loss: 7.345361]\n",
      "epoch:13 step:10469 [D loss: 1.936151, acc.: 50.78%] [G loss: 5.044788]\n",
      "epoch:13 step:10470 [D loss: 0.386581, acc.: 78.91%] [G loss: 2.259535]\n",
      "epoch:13 step:10471 [D loss: 0.162588, acc.: 94.53%] [G loss: 2.383090]\n",
      "epoch:13 step:10472 [D loss: 0.092570, acc.: 98.44%] [G loss: 2.354379]\n",
      "epoch:13 step:10473 [D loss: 0.140313, acc.: 95.31%] [G loss: 2.560001]\n",
      "epoch:13 step:10474 [D loss: 0.118933, acc.: 98.44%] [G loss: 3.226634]\n",
      "epoch:13 step:10475 [D loss: 0.041693, acc.: 100.00%] [G loss: 3.111225]\n",
      "epoch:13 step:10476 [D loss: 0.125482, acc.: 99.22%] [G loss: 1.078971]\n",
      "epoch:13 step:10477 [D loss: 0.097698, acc.: 97.66%] [G loss: 0.685385]\n",
      "epoch:13 step:10478 [D loss: 0.117819, acc.: 96.88%] [G loss: 2.180422]\n",
      "epoch:13 step:10479 [D loss: 0.090922, acc.: 98.44%] [G loss: 0.875688]\n",
      "epoch:13 step:10480 [D loss: 0.095559, acc.: 100.00%] [G loss: 0.189645]\n",
      "epoch:13 step:10481 [D loss: 0.209639, acc.: 92.19%] [G loss: 3.340105]\n",
      "epoch:13 step:10482 [D loss: 0.104965, acc.: 96.09%] [G loss: 2.105438]\n",
      "epoch:13 step:10483 [D loss: 0.397126, acc.: 82.03%] [G loss: 1.728219]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:13 step:10484 [D loss: 0.293163, acc.: 85.16%] [G loss: 0.226109]\n",
      "epoch:13 step:10485 [D loss: 0.174956, acc.: 91.41%] [G loss: 1.855670]\n",
      "epoch:13 step:10486 [D loss: 0.038710, acc.: 100.00%] [G loss: 2.200806]\n",
      "epoch:13 step:10487 [D loss: 0.474858, acc.: 77.34%] [G loss: 4.991112]\n",
      "epoch:13 step:10488 [D loss: 0.641224, acc.: 70.31%] [G loss: 3.752668]\n",
      "epoch:13 step:10489 [D loss: 0.400759, acc.: 82.03%] [G loss: 5.798668]\n",
      "epoch:13 step:10490 [D loss: 0.306913, acc.: 83.59%] [G loss: 5.012514]\n",
      "epoch:13 step:10491 [D loss: 0.218799, acc.: 92.19%] [G loss: 2.818108]\n",
      "epoch:13 step:10492 [D loss: 0.137692, acc.: 96.09%] [G loss: 4.983209]\n",
      "epoch:13 step:10493 [D loss: 0.034113, acc.: 100.00%] [G loss: 5.360155]\n",
      "epoch:13 step:10494 [D loss: 0.051754, acc.: 100.00%] [G loss: 4.807672]\n",
      "epoch:13 step:10495 [D loss: 0.077118, acc.: 98.44%] [G loss: 4.273477]\n",
      "epoch:13 step:10496 [D loss: 0.076848, acc.: 98.44%] [G loss: 2.896747]\n",
      "epoch:13 step:10497 [D loss: 0.119995, acc.: 97.66%] [G loss: 4.078294]\n",
      "epoch:13 step:10498 [D loss: 0.169472, acc.: 96.09%] [G loss: 2.751781]\n",
      "epoch:13 step:10499 [D loss: 0.437465, acc.: 80.47%] [G loss: 7.029706]\n",
      "epoch:13 step:10500 [D loss: 0.581663, acc.: 72.66%] [G loss: 5.745854]\n",
      "epoch:13 step:10501 [D loss: 0.101335, acc.: 96.09%] [G loss: 3.095399]\n",
      "epoch:13 step:10502 [D loss: 0.019000, acc.: 100.00%] [G loss: 2.165045]\n",
      "epoch:13 step:10503 [D loss: 0.015890, acc.: 100.00%] [G loss: 2.151918]\n",
      "epoch:13 step:10504 [D loss: 0.122621, acc.: 99.22%] [G loss: 3.490696]\n",
      "epoch:13 step:10505 [D loss: 0.016093, acc.: 100.00%] [G loss: 3.945700]\n",
      "epoch:13 step:10506 [D loss: 0.133634, acc.: 94.53%] [G loss: 2.595806]\n",
      "epoch:13 step:10507 [D loss: 0.120713, acc.: 96.88%] [G loss: 3.056827]\n",
      "epoch:13 step:10508 [D loss: 0.034634, acc.: 100.00%] [G loss: 4.242551]\n",
      "epoch:13 step:10509 [D loss: 0.083077, acc.: 98.44%] [G loss: 3.402996]\n",
      "epoch:13 step:10510 [D loss: 0.048263, acc.: 98.44%] [G loss: 3.628814]\n",
      "epoch:13 step:10511 [D loss: 0.097179, acc.: 98.44%] [G loss: 3.017882]\n",
      "epoch:13 step:10512 [D loss: 0.031960, acc.: 100.00%] [G loss: 3.334643]\n",
      "epoch:13 step:10513 [D loss: 0.037065, acc.: 100.00%] [G loss: 3.647199]\n",
      "epoch:13 step:10514 [D loss: 0.362226, acc.: 84.38%] [G loss: 1.508824]\n",
      "epoch:13 step:10515 [D loss: 0.263074, acc.: 89.84%] [G loss: 5.660751]\n",
      "epoch:13 step:10516 [D loss: 0.053048, acc.: 98.44%] [G loss: 5.562168]\n",
      "epoch:13 step:10517 [D loss: 1.055032, acc.: 56.25%] [G loss: 1.980819]\n",
      "epoch:13 step:10518 [D loss: 0.066988, acc.: 98.44%] [G loss: 2.184042]\n",
      "epoch:13 step:10519 [D loss: 0.031991, acc.: 100.00%] [G loss: 2.611189]\n",
      "epoch:13 step:10520 [D loss: 0.009381, acc.: 100.00%] [G loss: 2.814091]\n",
      "epoch:13 step:10521 [D loss: 0.031049, acc.: 100.00%] [G loss: 2.114044]\n",
      "epoch:13 step:10522 [D loss: 0.023103, acc.: 100.00%] [G loss: 2.196191]\n",
      "epoch:13 step:10523 [D loss: 0.036312, acc.: 98.44%] [G loss: 1.001720]\n",
      "epoch:13 step:10524 [D loss: 0.151052, acc.: 95.31%] [G loss: 3.219654]\n",
      "epoch:13 step:10525 [D loss: 0.045771, acc.: 98.44%] [G loss: 3.004368]\n",
      "epoch:13 step:10526 [D loss: 0.050477, acc.: 99.22%] [G loss: 2.418392]\n",
      "epoch:13 step:10527 [D loss: 0.203988, acc.: 92.97%] [G loss: 0.269981]\n",
      "epoch:13 step:10528 [D loss: 0.255735, acc.: 87.50%] [G loss: 2.124292]\n",
      "epoch:13 step:10529 [D loss: 0.098416, acc.: 96.09%] [G loss: 3.240079]\n",
      "epoch:13 step:10530 [D loss: 0.042161, acc.: 99.22%] [G loss: 2.690641]\n",
      "epoch:13 step:10531 [D loss: 0.053596, acc.: 100.00%] [G loss: 3.561169]\n",
      "epoch:13 step:10532 [D loss: 0.010273, acc.: 100.00%] [G loss: 2.014991]\n",
      "epoch:13 step:10533 [D loss: 0.066006, acc.: 99.22%] [G loss: 2.074315]\n",
      "epoch:13 step:10534 [D loss: 0.068749, acc.: 98.44%] [G loss: 3.784710]\n",
      "epoch:13 step:10535 [D loss: 0.157365, acc.: 96.09%] [G loss: 4.280342]\n",
      "epoch:13 step:10536 [D loss: 0.033339, acc.: 99.22%] [G loss: 4.637176]\n",
      "epoch:13 step:10537 [D loss: 0.070818, acc.: 99.22%] [G loss: 3.717272]\n",
      "epoch:13 step:10538 [D loss: 0.171547, acc.: 91.41%] [G loss: 5.393143]\n",
      "epoch:13 step:10539 [D loss: 0.199681, acc.: 93.75%] [G loss: 4.395451]\n",
      "epoch:13 step:10540 [D loss: 0.026392, acc.: 100.00%] [G loss: 4.120660]\n",
      "epoch:13 step:10541 [D loss: 0.097715, acc.: 98.44%] [G loss: 4.740014]\n",
      "epoch:13 step:10542 [D loss: 0.009681, acc.: 100.00%] [G loss: 5.200037]\n",
      "epoch:13 step:10543 [D loss: 0.027303, acc.: 100.00%] [G loss: 4.769302]\n",
      "epoch:13 step:10544 [D loss: 0.071470, acc.: 99.22%] [G loss: 3.479419]\n",
      "epoch:13 step:10545 [D loss: 0.014181, acc.: 100.00%] [G loss: 2.715476]\n",
      "epoch:13 step:10546 [D loss: 0.117069, acc.: 97.66%] [G loss: 3.762649]\n",
      "epoch:13 step:10547 [D loss: 0.037740, acc.: 99.22%] [G loss: 3.325948]\n",
      "epoch:13 step:10548 [D loss: 0.113132, acc.: 95.31%] [G loss: 0.807478]\n",
      "epoch:13 step:10549 [D loss: 0.135469, acc.: 96.09%] [G loss: 4.734679]\n",
      "epoch:13 step:10550 [D loss: 0.050465, acc.: 98.44%] [G loss: 4.298797]\n",
      "epoch:13 step:10551 [D loss: 0.014291, acc.: 100.00%] [G loss: 3.608334]\n",
      "epoch:13 step:10552 [D loss: 0.039995, acc.: 100.00%] [G loss: 3.153768]\n",
      "epoch:13 step:10553 [D loss: 0.127852, acc.: 97.66%] [G loss: 4.918996]\n",
      "epoch:13 step:10554 [D loss: 0.706086, acc.: 64.06%] [G loss: 1.022618]\n",
      "epoch:13 step:10555 [D loss: 0.032536, acc.: 100.00%] [G loss: 1.101952]\n",
      "epoch:13 step:10556 [D loss: 0.370326, acc.: 82.81%] [G loss: 5.610173]\n",
      "epoch:13 step:10557 [D loss: 0.698921, acc.: 69.53%] [G loss: 2.031159]\n",
      "epoch:13 step:10558 [D loss: 0.582354, acc.: 75.00%] [G loss: 5.997312]\n",
      "epoch:13 step:10559 [D loss: 0.276500, acc.: 85.94%] [G loss: 6.105436]\n",
      "epoch:13 step:10560 [D loss: 0.051452, acc.: 98.44%] [G loss: 5.095472]\n",
      "epoch:13 step:10561 [D loss: 0.023752, acc.: 100.00%] [G loss: 4.323099]\n",
      "epoch:13 step:10562 [D loss: 0.021146, acc.: 100.00%] [G loss: 2.971122]\n",
      "epoch:13 step:10563 [D loss: 0.017409, acc.: 100.00%] [G loss: 2.742846]\n",
      "epoch:13 step:10564 [D loss: 0.053020, acc.: 100.00%] [G loss: 3.141817]\n",
      "epoch:13 step:10565 [D loss: 0.052504, acc.: 99.22%] [G loss: 3.902990]\n",
      "epoch:13 step:10566 [D loss: 0.140512, acc.: 97.66%] [G loss: 4.008858]\n",
      "epoch:13 step:10567 [D loss: 0.042336, acc.: 100.00%] [G loss: 4.368476]\n",
      "epoch:13 step:10568 [D loss: 0.079336, acc.: 98.44%] [G loss: 4.366859]\n",
      "epoch:13 step:10569 [D loss: 0.031404, acc.: 99.22%] [G loss: 2.461961]\n",
      "epoch:13 step:10570 [D loss: 0.123913, acc.: 94.53%] [G loss: 0.667573]\n",
      "epoch:13 step:10571 [D loss: 0.830510, acc.: 62.50%] [G loss: 8.259009]\n",
      "epoch:13 step:10572 [D loss: 2.582379, acc.: 50.00%] [G loss: 5.898212]\n",
      "epoch:13 step:10573 [D loss: 1.751287, acc.: 50.00%] [G loss: 2.841512]\n",
      "epoch:13 step:10574 [D loss: 0.923726, acc.: 58.59%] [G loss: 1.611334]\n",
      "epoch:13 step:10575 [D loss: 0.559955, acc.: 76.56%] [G loss: 3.430892]\n",
      "epoch:13 step:10576 [D loss: 0.303362, acc.: 82.81%] [G loss: 3.123173]\n",
      "epoch:13 step:10577 [D loss: 0.312257, acc.: 83.59%] [G loss: 2.328277]\n",
      "epoch:13 step:10578 [D loss: 0.170441, acc.: 96.09%] [G loss: 2.531156]\n",
      "epoch:13 step:10579 [D loss: 0.089600, acc.: 99.22%] [G loss: 2.335384]\n",
      "epoch:13 step:10580 [D loss: 0.141277, acc.: 97.66%] [G loss: 2.098329]\n",
      "epoch:13 step:10581 [D loss: 0.133060, acc.: 96.09%] [G loss: 2.463790]\n",
      "epoch:13 step:10582 [D loss: 0.091494, acc.: 100.00%] [G loss: 2.542322]\n",
      "epoch:13 step:10583 [D loss: 0.311418, acc.: 85.16%] [G loss: 0.964308]\n",
      "epoch:13 step:10584 [D loss: 0.502482, acc.: 74.22%] [G loss: 3.533349]\n",
      "epoch:13 step:10585 [D loss: 0.176594, acc.: 92.97%] [G loss: 3.602193]\n",
      "epoch:13 step:10586 [D loss: 0.070344, acc.: 100.00%] [G loss: 2.831958]\n",
      "epoch:13 step:10587 [D loss: 0.102796, acc.: 99.22%] [G loss: 3.214777]\n",
      "epoch:13 step:10588 [D loss: 0.031689, acc.: 100.00%] [G loss: 2.730313]\n",
      "epoch:13 step:10589 [D loss: 0.132820, acc.: 98.44%] [G loss: 2.106566]\n",
      "epoch:13 step:10590 [D loss: 0.114373, acc.: 99.22%] [G loss: 2.581234]\n",
      "epoch:13 step:10591 [D loss: 0.066532, acc.: 100.00%] [G loss: 2.674916]\n",
      "epoch:13 step:10592 [D loss: 0.121414, acc.: 96.88%] [G loss: 2.477028]\n",
      "epoch:13 step:10593 [D loss: 0.077307, acc.: 99.22%] [G loss: 2.903006]\n",
      "epoch:13 step:10594 [D loss: 0.071908, acc.: 100.00%] [G loss: 2.953794]\n",
      "epoch:13 step:10595 [D loss: 0.091738, acc.: 99.22%] [G loss: 2.166370]\n",
      "epoch:13 step:10596 [D loss: 0.157894, acc.: 96.88%] [G loss: 3.006850]\n",
      "epoch:13 step:10597 [D loss: 0.105709, acc.: 96.88%] [G loss: 2.925904]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:13 step:10598 [D loss: 0.069992, acc.: 100.00%] [G loss: 1.754498]\n",
      "epoch:13 step:10599 [D loss: 0.315730, acc.: 87.50%] [G loss: 4.186627]\n",
      "epoch:13 step:10600 [D loss: 0.247096, acc.: 86.72%] [G loss: 3.554744]\n",
      "##############\n",
      "[0.85145699 1.08800212 0.97825676 1.00197246 0.8850924  1.11301775\n",
      " 0.99006329 1.11483484 2.11342076 1.06438551]\n",
      "##########\n",
      "epoch:13 step:10601 [D loss: 0.445023, acc.: 82.03%] [G loss: 3.161828]\n",
      "epoch:13 step:10602 [D loss: 0.029795, acc.: 100.00%] [G loss: 3.516772]\n",
      "epoch:13 step:10603 [D loss: 0.052573, acc.: 97.66%] [G loss: 2.560117]\n",
      "epoch:13 step:10604 [D loss: 0.085908, acc.: 96.09%] [G loss: 3.399903]\n",
      "epoch:13 step:10605 [D loss: 0.027031, acc.: 100.00%] [G loss: 3.844273]\n",
      "epoch:13 step:10606 [D loss: 0.087275, acc.: 98.44%] [G loss: 3.839654]\n",
      "epoch:13 step:10607 [D loss: 0.029029, acc.: 100.00%] [G loss: 3.819098]\n",
      "epoch:13 step:10608 [D loss: 0.058852, acc.: 100.00%] [G loss: 3.359325]\n",
      "epoch:13 step:10609 [D loss: 0.047080, acc.: 100.00%] [G loss: 3.324899]\n",
      "epoch:13 step:10610 [D loss: 0.067310, acc.: 100.00%] [G loss: 3.567281]\n",
      "epoch:13 step:10611 [D loss: 0.065108, acc.: 97.66%] [G loss: 2.193038]\n",
      "epoch:13 step:10612 [D loss: 0.229033, acc.: 90.62%] [G loss: 4.394496]\n",
      "epoch:13 step:10613 [D loss: 0.331192, acc.: 82.81%] [G loss: 1.778430]\n",
      "epoch:13 step:10614 [D loss: 0.021322, acc.: 100.00%] [G loss: 1.933853]\n",
      "epoch:13 step:10615 [D loss: 0.072445, acc.: 98.44%] [G loss: 2.924921]\n",
      "epoch:13 step:10616 [D loss: 0.022060, acc.: 100.00%] [G loss: 3.581041]\n",
      "epoch:13 step:10617 [D loss: 0.039316, acc.: 100.00%] [G loss: 3.710537]\n",
      "epoch:13 step:10618 [D loss: 0.042111, acc.: 99.22%] [G loss: 3.871620]\n",
      "epoch:13 step:10619 [D loss: 0.086000, acc.: 98.44%] [G loss: 3.228078]\n",
      "epoch:13 step:10620 [D loss: 0.057247, acc.: 99.22%] [G loss: 3.660860]\n",
      "epoch:13 step:10621 [D loss: 0.232794, acc.: 89.84%] [G loss: 5.316720]\n",
      "epoch:13 step:10622 [D loss: 0.027790, acc.: 100.00%] [G loss: 6.170135]\n",
      "epoch:13 step:10623 [D loss: 0.175351, acc.: 92.97%] [G loss: 3.209272]\n",
      "epoch:13 step:10624 [D loss: 0.205693, acc.: 92.19%] [G loss: 5.845874]\n",
      "epoch:13 step:10625 [D loss: 0.049064, acc.: 100.00%] [G loss: 6.367761]\n",
      "epoch:13 step:10626 [D loss: 0.028966, acc.: 100.00%] [G loss: 5.627965]\n",
      "epoch:13 step:10627 [D loss: 0.178516, acc.: 93.75%] [G loss: 3.950738]\n",
      "epoch:13 step:10628 [D loss: 0.032791, acc.: 100.00%] [G loss: 4.068756]\n",
      "epoch:13 step:10629 [D loss: 0.020660, acc.: 100.00%] [G loss: 3.103027]\n",
      "epoch:13 step:10630 [D loss: 0.020256, acc.: 100.00%] [G loss: 4.693941]\n",
      "epoch:13 step:10631 [D loss: 0.057103, acc.: 99.22%] [G loss: 4.383980]\n",
      "epoch:13 step:10632 [D loss: 0.030629, acc.: 100.00%] [G loss: 3.537051]\n",
      "epoch:13 step:10633 [D loss: 0.058672, acc.: 100.00%] [G loss: 4.217750]\n",
      "epoch:13 step:10634 [D loss: 0.055944, acc.: 99.22%] [G loss: 3.767952]\n",
      "epoch:13 step:10635 [D loss: 0.291301, acc.: 85.16%] [G loss: 5.587447]\n",
      "epoch:13 step:10636 [D loss: 0.262341, acc.: 87.50%] [G loss: 3.582391]\n",
      "epoch:13 step:10637 [D loss: 0.179815, acc.: 92.97%] [G loss: 5.144443]\n",
      "epoch:13 step:10638 [D loss: 0.108283, acc.: 95.31%] [G loss: 4.299555]\n",
      "epoch:13 step:10639 [D loss: 0.029744, acc.: 99.22%] [G loss: 3.970428]\n",
      "epoch:13 step:10640 [D loss: 0.022047, acc.: 100.00%] [G loss: 2.563068]\n",
      "epoch:13 step:10641 [D loss: 0.126096, acc.: 95.31%] [G loss: 5.536789]\n",
      "epoch:13 step:10642 [D loss: 0.078298, acc.: 96.09%] [G loss: 4.524513]\n",
      "epoch:13 step:10643 [D loss: 0.014200, acc.: 100.00%] [G loss: 3.978229]\n",
      "epoch:13 step:10644 [D loss: 0.117438, acc.: 99.22%] [G loss: 4.509933]\n",
      "epoch:13 step:10645 [D loss: 0.206620, acc.: 91.41%] [G loss: 2.037448]\n",
      "epoch:13 step:10646 [D loss: 0.020896, acc.: 99.22%] [G loss: 2.839708]\n",
      "epoch:13 step:10647 [D loss: 0.008021, acc.: 100.00%] [G loss: 1.730936]\n",
      "epoch:13 step:10648 [D loss: 0.018880, acc.: 100.00%] [G loss: 2.244582]\n",
      "epoch:13 step:10649 [D loss: 0.070223, acc.: 99.22%] [G loss: 4.401227]\n",
      "epoch:13 step:10650 [D loss: 0.158446, acc.: 91.41%] [G loss: 1.863649]\n",
      "epoch:13 step:10651 [D loss: 0.514695, acc.: 76.56%] [G loss: 7.346263]\n",
      "epoch:13 step:10652 [D loss: 2.766590, acc.: 50.00%] [G loss: 4.209881]\n",
      "epoch:13 step:10653 [D loss: 0.480095, acc.: 78.91%] [G loss: 2.846488]\n",
      "epoch:13 step:10654 [D loss: 0.153947, acc.: 97.66%] [G loss: 3.551831]\n",
      "epoch:13 step:10655 [D loss: 0.148319, acc.: 95.31%] [G loss: 3.543176]\n",
      "epoch:13 step:10656 [D loss: 0.051578, acc.: 100.00%] [G loss: 3.338051]\n",
      "epoch:13 step:10657 [D loss: 0.087781, acc.: 99.22%] [G loss: 2.908597]\n",
      "epoch:13 step:10658 [D loss: 0.038041, acc.: 99.22%] [G loss: 3.782411]\n",
      "epoch:13 step:10659 [D loss: 0.039801, acc.: 100.00%] [G loss: 3.127646]\n",
      "epoch:13 step:10660 [D loss: 0.238776, acc.: 90.62%] [G loss: 5.127260]\n",
      "epoch:13 step:10661 [D loss: 0.198855, acc.: 90.62%] [G loss: 4.008920]\n",
      "epoch:13 step:10662 [D loss: 0.530167, acc.: 77.34%] [G loss: 5.002345]\n",
      "epoch:13 step:10663 [D loss: 0.012190, acc.: 100.00%] [G loss: 5.759957]\n",
      "epoch:13 step:10664 [D loss: 0.190192, acc.: 90.62%] [G loss: 3.868705]\n",
      "epoch:13 step:10665 [D loss: 0.149805, acc.: 96.88%] [G loss: 3.821972]\n",
      "epoch:13 step:10666 [D loss: 0.021192, acc.: 100.00%] [G loss: 4.247223]\n",
      "epoch:13 step:10667 [D loss: 0.016438, acc.: 100.00%] [G loss: 4.848352]\n",
      "epoch:13 step:10668 [D loss: 0.024491, acc.: 100.00%] [G loss: 3.830716]\n",
      "epoch:13 step:10669 [D loss: 0.161522, acc.: 94.53%] [G loss: 4.377949]\n",
      "epoch:13 step:10670 [D loss: 0.038096, acc.: 98.44%] [G loss: 4.584907]\n",
      "epoch:13 step:10671 [D loss: 0.033669, acc.: 100.00%] [G loss: 3.544407]\n",
      "epoch:13 step:10672 [D loss: 0.397287, acc.: 79.69%] [G loss: 5.261142]\n",
      "epoch:13 step:10673 [D loss: 0.019464, acc.: 100.00%] [G loss: 5.729420]\n",
      "epoch:13 step:10674 [D loss: 0.355112, acc.: 85.16%] [G loss: 2.644780]\n",
      "epoch:13 step:10675 [D loss: 0.736132, acc.: 70.31%] [G loss: 6.745866]\n",
      "epoch:13 step:10676 [D loss: 1.072397, acc.: 55.47%] [G loss: 5.202693]\n",
      "epoch:13 step:10677 [D loss: 0.350826, acc.: 83.59%] [G loss: 1.397893]\n",
      "epoch:13 step:10678 [D loss: 1.654536, acc.: 50.78%] [G loss: 5.288368]\n",
      "epoch:13 step:10679 [D loss: 1.121971, acc.: 55.47%] [G loss: 3.860281]\n",
      "epoch:13 step:10680 [D loss: 0.819454, acc.: 61.72%] [G loss: 2.190094]\n",
      "epoch:13 step:10681 [D loss: 0.432631, acc.: 82.03%] [G loss: 1.966202]\n",
      "epoch:13 step:10682 [D loss: 0.141409, acc.: 98.44%] [G loss: 2.292325]\n",
      "epoch:13 step:10683 [D loss: 0.144130, acc.: 99.22%] [G loss: 2.126468]\n",
      "epoch:13 step:10684 [D loss: 0.172632, acc.: 96.88%] [G loss: 1.894217]\n",
      "epoch:13 step:10685 [D loss: 0.191654, acc.: 97.66%] [G loss: 2.164469]\n",
      "epoch:13 step:10686 [D loss: 0.359857, acc.: 85.16%] [G loss: 1.164888]\n",
      "epoch:13 step:10687 [D loss: 0.221710, acc.: 92.97%] [G loss: 2.172980]\n",
      "epoch:13 step:10688 [D loss: 0.144472, acc.: 96.09%] [G loss: 2.271167]\n",
      "epoch:13 step:10689 [D loss: 0.381865, acc.: 83.59%] [G loss: 1.830926]\n",
      "epoch:13 step:10690 [D loss: 0.118805, acc.: 96.09%] [G loss: 1.102695]\n",
      "epoch:13 step:10691 [D loss: 0.283158, acc.: 92.19%] [G loss: 2.159175]\n",
      "epoch:13 step:10692 [D loss: 0.595127, acc.: 72.66%] [G loss: 1.507675]\n",
      "epoch:13 step:10693 [D loss: 0.254586, acc.: 93.75%] [G loss: 3.204876]\n",
      "epoch:13 step:10694 [D loss: 0.137825, acc.: 93.75%] [G loss: 2.672344]\n",
      "epoch:13 step:10695 [D loss: 0.112145, acc.: 98.44%] [G loss: 1.481902]\n",
      "epoch:13 step:10696 [D loss: 0.224489, acc.: 90.62%] [G loss: 3.749503]\n",
      "epoch:13 step:10697 [D loss: 0.728551, acc.: 61.72%] [G loss: 3.149984]\n",
      "epoch:13 step:10698 [D loss: 0.077415, acc.: 98.44%] [G loss: 3.355021]\n",
      "epoch:13 step:10699 [D loss: 0.119640, acc.: 98.44%] [G loss: 3.672825]\n",
      "epoch:13 step:10700 [D loss: 0.163838, acc.: 96.09%] [G loss: 3.776605]\n",
      "epoch:13 step:10701 [D loss: 0.084196, acc.: 99.22%] [G loss: 4.258780]\n",
      "epoch:13 step:10702 [D loss: 0.020958, acc.: 100.00%] [G loss: 3.274677]\n",
      "epoch:13 step:10703 [D loss: 0.335178, acc.: 87.50%] [G loss: 3.685883]\n",
      "epoch:13 step:10704 [D loss: 0.020823, acc.: 100.00%] [G loss: 4.072808]\n",
      "epoch:13 step:10705 [D loss: 0.492801, acc.: 75.00%] [G loss: 2.081755]\n",
      "epoch:13 step:10706 [D loss: 0.023029, acc.: 100.00%] [G loss: 4.251068]\n",
      "epoch:13 step:10707 [D loss: 0.052745, acc.: 100.00%] [G loss: 4.385143]\n",
      "epoch:13 step:10708 [D loss: 0.025600, acc.: 100.00%] [G loss: 3.100729]\n",
      "epoch:13 step:10709 [D loss: 0.026760, acc.: 100.00%] [G loss: 3.476304]\n",
      "epoch:13 step:10710 [D loss: 0.068409, acc.: 98.44%] [G loss: 2.278226]\n",
      "epoch:13 step:10711 [D loss: 0.067736, acc.: 99.22%] [G loss: 3.214948]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:13 step:10712 [D loss: 0.162787, acc.: 96.09%] [G loss: 3.900753]\n",
      "epoch:13 step:10713 [D loss: 0.076121, acc.: 97.66%] [G loss: 3.245450]\n",
      "epoch:13 step:10714 [D loss: 0.163001, acc.: 92.97%] [G loss: 3.638954]\n",
      "epoch:13 step:10715 [D loss: 0.398418, acc.: 82.81%] [G loss: 3.170350]\n",
      "epoch:13 step:10716 [D loss: 0.009762, acc.: 100.00%] [G loss: 3.740231]\n",
      "epoch:13 step:10717 [D loss: 0.056105, acc.: 99.22%] [G loss: 2.580528]\n",
      "epoch:13 step:10718 [D loss: 0.108094, acc.: 99.22%] [G loss: 4.022424]\n",
      "epoch:13 step:10719 [D loss: 0.061794, acc.: 99.22%] [G loss: 3.893055]\n",
      "epoch:13 step:10720 [D loss: 0.030634, acc.: 100.00%] [G loss: 3.459368]\n",
      "epoch:13 step:10721 [D loss: 0.016386, acc.: 100.00%] [G loss: 3.411432]\n",
      "epoch:13 step:10722 [D loss: 0.026779, acc.: 100.00%] [G loss: 3.249944]\n",
      "epoch:13 step:10723 [D loss: 0.033654, acc.: 100.00%] [G loss: 3.538972]\n",
      "epoch:13 step:10724 [D loss: 0.083958, acc.: 99.22%] [G loss: 3.714494]\n",
      "epoch:13 step:10725 [D loss: 0.038718, acc.: 99.22%] [G loss: 4.423162]\n",
      "epoch:13 step:10726 [D loss: 0.244174, acc.: 90.62%] [G loss: 3.126005]\n",
      "epoch:13 step:10727 [D loss: 0.030807, acc.: 100.00%] [G loss: 3.953900]\n",
      "epoch:13 step:10728 [D loss: 0.800010, acc.: 62.50%] [G loss: 6.139774]\n",
      "epoch:13 step:10729 [D loss: 1.695015, acc.: 50.00%] [G loss: 4.032560]\n",
      "epoch:13 step:10730 [D loss: 0.689103, acc.: 71.09%] [G loss: 1.964961]\n",
      "epoch:13 step:10731 [D loss: 0.236508, acc.: 89.84%] [G loss: 3.417379]\n",
      "epoch:13 step:10732 [D loss: 0.031861, acc.: 100.00%] [G loss: 4.100588]\n",
      "epoch:13 step:10733 [D loss: 0.124974, acc.: 96.09%] [G loss: 3.110130]\n",
      "epoch:13 step:10734 [D loss: 0.026358, acc.: 100.00%] [G loss: 3.469958]\n",
      "epoch:13 step:10735 [D loss: 0.070711, acc.: 98.44%] [G loss: 3.387325]\n",
      "epoch:13 step:10736 [D loss: 0.091790, acc.: 98.44%] [G loss: 3.270832]\n",
      "epoch:13 step:10737 [D loss: 0.068369, acc.: 99.22%] [G loss: 3.579367]\n",
      "epoch:13 step:10738 [D loss: 0.022526, acc.: 100.00%] [G loss: 4.409948]\n",
      "epoch:13 step:10739 [D loss: 0.028001, acc.: 100.00%] [G loss: 4.051037]\n",
      "epoch:13 step:10740 [D loss: 0.043650, acc.: 98.44%] [G loss: 3.798328]\n",
      "epoch:13 step:10741 [D loss: 0.043183, acc.: 100.00%] [G loss: 3.869616]\n",
      "epoch:13 step:10742 [D loss: 0.047069, acc.: 100.00%] [G loss: 3.545824]\n",
      "epoch:13 step:10743 [D loss: 0.039595, acc.: 100.00%] [G loss: 3.104856]\n",
      "epoch:13 step:10744 [D loss: 0.033982, acc.: 100.00%] [G loss: 2.900706]\n",
      "epoch:13 step:10745 [D loss: 0.052845, acc.: 100.00%] [G loss: 3.578426]\n",
      "epoch:13 step:10746 [D loss: 0.061129, acc.: 100.00%] [G loss: 3.491899]\n",
      "epoch:13 step:10747 [D loss: 0.055986, acc.: 100.00%] [G loss: 3.949860]\n",
      "epoch:13 step:10748 [D loss: 0.031417, acc.: 100.00%] [G loss: 3.560013]\n",
      "epoch:13 step:10749 [D loss: 0.076839, acc.: 98.44%] [G loss: 3.022035]\n",
      "epoch:13 step:10750 [D loss: 0.025184, acc.: 100.00%] [G loss: 2.610832]\n",
      "epoch:13 step:10751 [D loss: 0.063102, acc.: 99.22%] [G loss: 3.660415]\n",
      "epoch:13 step:10752 [D loss: 0.330784, acc.: 84.38%] [G loss: 3.593083]\n",
      "epoch:13 step:10753 [D loss: 0.055772, acc.: 99.22%] [G loss: 3.547987]\n",
      "epoch:13 step:10754 [D loss: 0.042683, acc.: 100.00%] [G loss: 1.949162]\n",
      "epoch:13 step:10755 [D loss: 0.037595, acc.: 100.00%] [G loss: 1.611081]\n",
      "epoch:13 step:10756 [D loss: 0.061382, acc.: 100.00%] [G loss: 3.355993]\n",
      "epoch:13 step:10757 [D loss: 1.098634, acc.: 41.41%] [G loss: 6.297148]\n",
      "epoch:13 step:10758 [D loss: 0.344554, acc.: 78.12%] [G loss: 5.440321]\n",
      "epoch:13 step:10759 [D loss: 0.039432, acc.: 99.22%] [G loss: 4.829601]\n",
      "epoch:13 step:10760 [D loss: 0.026944, acc.: 100.00%] [G loss: 3.589182]\n",
      "epoch:13 step:10761 [D loss: 0.046065, acc.: 100.00%] [G loss: 3.121316]\n",
      "epoch:13 step:10762 [D loss: 0.051735, acc.: 99.22%] [G loss: 3.733529]\n",
      "epoch:13 step:10763 [D loss: 0.395799, acc.: 84.38%] [G loss: 4.074782]\n",
      "epoch:13 step:10764 [D loss: 0.022449, acc.: 100.00%] [G loss: 4.813373]\n",
      "epoch:13 step:10765 [D loss: 0.125231, acc.: 96.88%] [G loss: 4.031916]\n",
      "epoch:13 step:10766 [D loss: 0.078394, acc.: 99.22%] [G loss: 2.791004]\n",
      "epoch:13 step:10767 [D loss: 0.037224, acc.: 100.00%] [G loss: 3.872361]\n",
      "epoch:13 step:10768 [D loss: 0.034547, acc.: 100.00%] [G loss: 3.727705]\n",
      "epoch:13 step:10769 [D loss: 0.290492, acc.: 88.28%] [G loss: 3.992867]\n",
      "epoch:13 step:10770 [D loss: 0.021096, acc.: 100.00%] [G loss: 5.487732]\n",
      "epoch:13 step:10771 [D loss: 0.120258, acc.: 97.66%] [G loss: 2.785014]\n",
      "epoch:13 step:10772 [D loss: 0.097512, acc.: 100.00%] [G loss: 3.525116]\n",
      "epoch:13 step:10773 [D loss: 0.020221, acc.: 100.00%] [G loss: 3.768004]\n",
      "epoch:13 step:10774 [D loss: 0.053536, acc.: 100.00%] [G loss: 3.485160]\n",
      "epoch:13 step:10775 [D loss: 0.036954, acc.: 99.22%] [G loss: 2.640424]\n",
      "epoch:13 step:10776 [D loss: 0.231039, acc.: 90.62%] [G loss: 1.154299]\n",
      "epoch:13 step:10777 [D loss: 0.109822, acc.: 96.88%] [G loss: 3.735884]\n",
      "epoch:13 step:10778 [D loss: 0.037095, acc.: 100.00%] [G loss: 3.778379]\n",
      "epoch:13 step:10779 [D loss: 0.206687, acc.: 91.41%] [G loss: 0.291607]\n",
      "epoch:13 step:10780 [D loss: 0.344677, acc.: 80.47%] [G loss: 5.969390]\n",
      "epoch:13 step:10781 [D loss: 0.420534, acc.: 79.69%] [G loss: 4.185198]\n",
      "epoch:13 step:10782 [D loss: 0.073306, acc.: 98.44%] [G loss: 1.944177]\n",
      "epoch:13 step:10783 [D loss: 0.207503, acc.: 89.06%] [G loss: 5.014864]\n",
      "epoch:13 step:10784 [D loss: 0.023368, acc.: 99.22%] [G loss: 5.074520]\n",
      "epoch:13 step:10785 [D loss: 0.081016, acc.: 96.88%] [G loss: 4.282325]\n",
      "epoch:13 step:10786 [D loss: 0.073585, acc.: 98.44%] [G loss: 3.642009]\n",
      "epoch:13 step:10787 [D loss: 0.017976, acc.: 100.00%] [G loss: 3.497995]\n",
      "epoch:13 step:10788 [D loss: 0.059158, acc.: 100.00%] [G loss: 4.272844]\n",
      "epoch:13 step:10789 [D loss: 0.087847, acc.: 100.00%] [G loss: 4.200234]\n",
      "epoch:13 step:10790 [D loss: 0.230476, acc.: 92.19%] [G loss: 4.348096]\n",
      "epoch:13 step:10791 [D loss: 0.030579, acc.: 100.00%] [G loss: 4.795111]\n",
      "epoch:13 step:10792 [D loss: 0.673621, acc.: 65.62%] [G loss: 5.742607]\n",
      "epoch:13 step:10793 [D loss: 0.192300, acc.: 91.41%] [G loss: 3.687739]\n",
      "epoch:13 step:10794 [D loss: 0.019108, acc.: 100.00%] [G loss: 4.810665]\n",
      "epoch:13 step:10795 [D loss: 0.080471, acc.: 96.88%] [G loss: 2.972101]\n",
      "epoch:13 step:10796 [D loss: 0.074265, acc.: 98.44%] [G loss: 3.344422]\n",
      "epoch:13 step:10797 [D loss: 0.022866, acc.: 99.22%] [G loss: 3.180841]\n",
      "epoch:13 step:10798 [D loss: 0.057524, acc.: 100.00%] [G loss: 4.644149]\n",
      "epoch:13 step:10799 [D loss: 2.067761, acc.: 24.22%] [G loss: 8.299904]\n",
      "epoch:13 step:10800 [D loss: 2.153751, acc.: 50.00%] [G loss: 5.980031]\n",
      "##############\n",
      "[0.97868934 0.98345506 0.94965578 0.84179019 1.10506606 0.9944802\n",
      " 2.1149016  2.11643799 1.11707678 2.0987057 ]\n",
      "##########\n",
      "epoch:13 step:10801 [D loss: 0.953224, acc.: 60.94%] [G loss: 3.250136]\n",
      "epoch:13 step:10802 [D loss: 0.218712, acc.: 91.41%] [G loss: 3.000519]\n",
      "epoch:13 step:10803 [D loss: 0.040170, acc.: 100.00%] [G loss: 2.995121]\n",
      "epoch:13 step:10804 [D loss: 0.048678, acc.: 100.00%] [G loss: 3.375217]\n",
      "epoch:13 step:10805 [D loss: 0.051369, acc.: 100.00%] [G loss: 2.855422]\n",
      "epoch:13 step:10806 [D loss: 0.080359, acc.: 99.22%] [G loss: 3.479618]\n",
      "epoch:13 step:10807 [D loss: 0.186996, acc.: 95.31%] [G loss: 3.199750]\n",
      "epoch:13 step:10808 [D loss: 0.040799, acc.: 100.00%] [G loss: 3.879150]\n",
      "epoch:13 step:10809 [D loss: 0.417790, acc.: 78.91%] [G loss: 3.834653]\n",
      "epoch:13 step:10810 [D loss: 0.080764, acc.: 97.66%] [G loss: 3.916014]\n",
      "epoch:13 step:10811 [D loss: 0.118238, acc.: 96.88%] [G loss: 3.307662]\n",
      "epoch:13 step:10812 [D loss: 0.080679, acc.: 98.44%] [G loss: 2.796146]\n",
      "epoch:13 step:10813 [D loss: 0.069505, acc.: 99.22%] [G loss: 3.506051]\n",
      "epoch:13 step:10814 [D loss: 0.103896, acc.: 98.44%] [G loss: 3.697196]\n",
      "epoch:13 step:10815 [D loss: 0.042589, acc.: 100.00%] [G loss: 3.410882]\n",
      "epoch:13 step:10816 [D loss: 0.093585, acc.: 98.44%] [G loss: 3.907060]\n",
      "epoch:13 step:10817 [D loss: 0.050092, acc.: 99.22%] [G loss: 3.393343]\n",
      "epoch:13 step:10818 [D loss: 0.105687, acc.: 96.88%] [G loss: 3.013415]\n",
      "epoch:13 step:10819 [D loss: 0.024777, acc.: 100.00%] [G loss: 2.973371]\n",
      "epoch:13 step:10820 [D loss: 0.134744, acc.: 94.53%] [G loss: 3.993024]\n",
      "epoch:13 step:10821 [D loss: 0.274197, acc.: 91.41%] [G loss: 3.192508]\n",
      "epoch:13 step:10822 [D loss: 0.080949, acc.: 98.44%] [G loss: 4.163082]\n",
      "epoch:13 step:10823 [D loss: 0.391717, acc.: 81.25%] [G loss: 1.492751]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:13 step:10824 [D loss: 0.105630, acc.: 98.44%] [G loss: 3.672968]\n",
      "epoch:13 step:10825 [D loss: 0.027181, acc.: 100.00%] [G loss: 4.865148]\n",
      "epoch:13 step:10826 [D loss: 0.105753, acc.: 96.88%] [G loss: 2.671325]\n",
      "epoch:13 step:10827 [D loss: 0.032207, acc.: 100.00%] [G loss: 1.881611]\n",
      "epoch:13 step:10828 [D loss: 0.067048, acc.: 97.66%] [G loss: 2.453945]\n",
      "epoch:13 step:10829 [D loss: 0.029143, acc.: 100.00%] [G loss: 2.473151]\n",
      "epoch:13 step:10830 [D loss: 0.222585, acc.: 92.97%] [G loss: 2.809347]\n",
      "epoch:13 step:10831 [D loss: 0.088336, acc.: 97.66%] [G loss: 1.792403]\n",
      "epoch:13 step:10832 [D loss: 0.188371, acc.: 91.41%] [G loss: 3.621533]\n",
      "epoch:13 step:10833 [D loss: 0.117583, acc.: 97.66%] [G loss: 2.921780]\n",
      "epoch:13 step:10834 [D loss: 0.488037, acc.: 75.78%] [G loss: 1.033911]\n",
      "epoch:13 step:10835 [D loss: 0.030290, acc.: 100.00%] [G loss: 3.346389]\n",
      "epoch:13 step:10836 [D loss: 0.042403, acc.: 99.22%] [G loss: 3.530269]\n",
      "epoch:13 step:10837 [D loss: 0.105305, acc.: 98.44%] [G loss: 3.660515]\n",
      "epoch:13 step:10838 [D loss: 1.076837, acc.: 44.53%] [G loss: 5.181877]\n",
      "epoch:13 step:10839 [D loss: 0.262287, acc.: 85.94%] [G loss: 4.719242]\n",
      "epoch:13 step:10840 [D loss: 0.078901, acc.: 97.66%] [G loss: 3.504053]\n",
      "epoch:13 step:10841 [D loss: 0.054775, acc.: 99.22%] [G loss: 3.742887]\n",
      "epoch:13 step:10842 [D loss: 0.016331, acc.: 100.00%] [G loss: 3.918854]\n",
      "epoch:13 step:10843 [D loss: 0.023129, acc.: 100.00%] [G loss: 3.015159]\n",
      "epoch:13 step:10844 [D loss: 0.090664, acc.: 97.66%] [G loss: 3.715508]\n",
      "epoch:13 step:10845 [D loss: 0.061190, acc.: 99.22%] [G loss: 3.456770]\n",
      "epoch:13 step:10846 [D loss: 0.028647, acc.: 99.22%] [G loss: 2.797752]\n",
      "epoch:13 step:10847 [D loss: 0.025236, acc.: 100.00%] [G loss: 1.906465]\n",
      "epoch:13 step:10848 [D loss: 0.062620, acc.: 99.22%] [G loss: 1.798558]\n",
      "epoch:13 step:10849 [D loss: 0.017396, acc.: 100.00%] [G loss: 1.777151]\n",
      "epoch:13 step:10850 [D loss: 0.025400, acc.: 100.00%] [G loss: 2.025511]\n",
      "epoch:13 step:10851 [D loss: 0.038382, acc.: 98.44%] [G loss: 2.136980]\n",
      "epoch:13 step:10852 [D loss: 0.114546, acc.: 97.66%] [G loss: 3.642619]\n",
      "epoch:13 step:10853 [D loss: 0.116661, acc.: 96.88%] [G loss: 2.257936]\n",
      "epoch:13 step:10854 [D loss: 0.005279, acc.: 100.00%] [G loss: 1.694234]\n",
      "epoch:13 step:10855 [D loss: 0.046578, acc.: 99.22%] [G loss: 3.173667]\n",
      "epoch:13 step:10856 [D loss: 0.029344, acc.: 100.00%] [G loss: 2.308794]\n",
      "epoch:13 step:10857 [D loss: 0.334638, acc.: 85.16%] [G loss: 5.513377]\n",
      "epoch:13 step:10858 [D loss: 0.198049, acc.: 91.41%] [G loss: 2.559720]\n",
      "epoch:13 step:10859 [D loss: 0.001555, acc.: 100.00%] [G loss: 2.701970]\n",
      "epoch:13 step:10860 [D loss: 0.014165, acc.: 100.00%] [G loss: 2.178933]\n",
      "epoch:13 step:10861 [D loss: 0.046222, acc.: 100.00%] [G loss: 1.279455]\n",
      "epoch:13 step:10862 [D loss: 0.204607, acc.: 90.62%] [G loss: 6.586731]\n",
      "epoch:13 step:10863 [D loss: 0.685112, acc.: 63.28%] [G loss: 2.010723]\n",
      "epoch:13 step:10864 [D loss: 0.231325, acc.: 87.50%] [G loss: 4.818015]\n",
      "epoch:13 step:10865 [D loss: 0.020570, acc.: 99.22%] [G loss: 5.931221]\n",
      "epoch:13 step:10866 [D loss: 2.215425, acc.: 15.62%] [G loss: 5.402247]\n",
      "epoch:13 step:10867 [D loss: 0.384085, acc.: 81.25%] [G loss: 5.228656]\n",
      "epoch:13 step:10868 [D loss: 0.223155, acc.: 86.72%] [G loss: 2.526009]\n",
      "epoch:13 step:10869 [D loss: 0.185606, acc.: 92.19%] [G loss: 3.839611]\n",
      "epoch:13 step:10870 [D loss: 0.020929, acc.: 100.00%] [G loss: 4.049450]\n",
      "epoch:13 step:10871 [D loss: 0.061381, acc.: 100.00%] [G loss: 3.478713]\n",
      "epoch:13 step:10872 [D loss: 0.086469, acc.: 96.88%] [G loss: 3.513312]\n",
      "epoch:13 step:10873 [D loss: 0.137623, acc.: 97.66%] [G loss: 3.055520]\n",
      "epoch:13 step:10874 [D loss: 0.051126, acc.: 99.22%] [G loss: 4.107790]\n",
      "epoch:13 step:10875 [D loss: 0.047534, acc.: 100.00%] [G loss: 3.909438]\n",
      "epoch:13 step:10876 [D loss: 0.101499, acc.: 97.66%] [G loss: 3.911569]\n",
      "epoch:13 step:10877 [D loss: 0.082265, acc.: 96.09%] [G loss: 4.312357]\n",
      "epoch:13 step:10878 [D loss: 0.216355, acc.: 89.06%] [G loss: 2.321169]\n",
      "epoch:13 step:10879 [D loss: 0.021921, acc.: 99.22%] [G loss: 3.326993]\n",
      "epoch:13 step:10880 [D loss: 0.333353, acc.: 83.59%] [G loss: 5.749163]\n",
      "epoch:13 step:10881 [D loss: 0.170186, acc.: 91.41%] [G loss: 3.368937]\n",
      "epoch:13 step:10882 [D loss: 0.052398, acc.: 99.22%] [G loss: 4.371139]\n",
      "epoch:13 step:10883 [D loss: 0.005244, acc.: 100.00%] [G loss: 4.454058]\n",
      "epoch:13 step:10884 [D loss: 0.007583, acc.: 100.00%] [G loss: 3.914306]\n",
      "epoch:13 step:10885 [D loss: 0.039201, acc.: 98.44%] [G loss: 2.520493]\n",
      "epoch:13 step:10886 [D loss: 0.022341, acc.: 100.00%] [G loss: 2.917489]\n",
      "epoch:13 step:10887 [D loss: 0.046739, acc.: 99.22%] [G loss: 2.789271]\n",
      "epoch:13 step:10888 [D loss: 1.696001, acc.: 35.94%] [G loss: 6.842204]\n",
      "epoch:13 step:10889 [D loss: 0.504927, acc.: 73.44%] [G loss: 5.738951]\n",
      "epoch:13 step:10890 [D loss: 0.601498, acc.: 69.53%] [G loss: 1.487262]\n",
      "epoch:13 step:10891 [D loss: 0.588982, acc.: 77.34%] [G loss: 3.810936]\n",
      "epoch:13 step:10892 [D loss: 0.031343, acc.: 99.22%] [G loss: 5.420336]\n",
      "epoch:13 step:10893 [D loss: 0.346751, acc.: 87.50%] [G loss: 3.620929]\n",
      "epoch:13 step:10894 [D loss: 0.062944, acc.: 99.22%] [G loss: 2.284700]\n",
      "epoch:13 step:10895 [D loss: 0.152556, acc.: 96.88%] [G loss: 2.851549]\n",
      "epoch:13 step:10896 [D loss: 0.049608, acc.: 99.22%] [G loss: 2.230074]\n",
      "epoch:13 step:10897 [D loss: 0.130190, acc.: 96.88%] [G loss: 2.125484]\n",
      "epoch:13 step:10898 [D loss: 0.094554, acc.: 97.66%] [G loss: 2.752528]\n",
      "epoch:13 step:10899 [D loss: 0.394495, acc.: 87.50%] [G loss: 4.778764]\n",
      "epoch:13 step:10900 [D loss: 0.988842, acc.: 57.03%] [G loss: 2.042615]\n",
      "epoch:13 step:10901 [D loss: 0.517960, acc.: 75.00%] [G loss: 6.455452]\n",
      "epoch:13 step:10902 [D loss: 0.504222, acc.: 76.56%] [G loss: 5.535234]\n",
      "epoch:13 step:10903 [D loss: 0.069763, acc.: 98.44%] [G loss: 3.860339]\n",
      "epoch:13 step:10904 [D loss: 0.026751, acc.: 99.22%] [G loss: 4.318811]\n",
      "epoch:13 step:10905 [D loss: 0.038358, acc.: 99.22%] [G loss: 4.497741]\n",
      "epoch:13 step:10906 [D loss: 0.058038, acc.: 100.00%] [G loss: 3.762572]\n",
      "epoch:13 step:10907 [D loss: 0.165065, acc.: 96.09%] [G loss: 3.266108]\n",
      "epoch:13 step:10908 [D loss: 0.009703, acc.: 100.00%] [G loss: 3.846735]\n",
      "epoch:13 step:10909 [D loss: 0.071978, acc.: 99.22%] [G loss: 2.809947]\n",
      "epoch:13 step:10910 [D loss: 0.125429, acc.: 95.31%] [G loss: 4.054160]\n",
      "epoch:13 step:10911 [D loss: 0.015063, acc.: 100.00%] [G loss: 3.619657]\n",
      "epoch:13 step:10912 [D loss: 0.047547, acc.: 99.22%] [G loss: 2.466865]\n",
      "epoch:13 step:10913 [D loss: 0.474280, acc.: 75.78%] [G loss: 3.673072]\n",
      "epoch:13 step:10914 [D loss: 0.011589, acc.: 100.00%] [G loss: 4.690598]\n",
      "epoch:13 step:10915 [D loss: 1.050074, acc.: 53.12%] [G loss: 5.020350]\n",
      "epoch:13 step:10916 [D loss: 0.022861, acc.: 100.00%] [G loss: 5.710347]\n",
      "epoch:13 step:10917 [D loss: 0.081525, acc.: 98.44%] [G loss: 5.186045]\n",
      "epoch:13 step:10918 [D loss: 0.020821, acc.: 100.00%] [G loss: 4.753329]\n",
      "epoch:13 step:10919 [D loss: 0.021947, acc.: 100.00%] [G loss: 4.123533]\n",
      "epoch:13 step:10920 [D loss: 0.024170, acc.: 100.00%] [G loss: 3.590176]\n",
      "epoch:13 step:10921 [D loss: 0.036798, acc.: 100.00%] [G loss: 3.154537]\n",
      "epoch:13 step:10922 [D loss: 0.037617, acc.: 100.00%] [G loss: 3.531096]\n",
      "epoch:13 step:10923 [D loss: 0.063696, acc.: 99.22%] [G loss: 3.441106]\n",
      "epoch:13 step:10924 [D loss: 0.037938, acc.: 100.00%] [G loss: 2.724515]\n",
      "epoch:13 step:10925 [D loss: 0.046956, acc.: 100.00%] [G loss: 3.419463]\n",
      "epoch:13 step:10926 [D loss: 0.063954, acc.: 99.22%] [G loss: 4.107615]\n",
      "epoch:13 step:10927 [D loss: 0.122044, acc.: 96.09%] [G loss: 1.317419]\n",
      "epoch:13 step:10928 [D loss: 0.155092, acc.: 93.75%] [G loss: 4.395918]\n",
      "epoch:13 step:10929 [D loss: 0.309374, acc.: 85.16%] [G loss: 1.358469]\n",
      "epoch:13 step:10930 [D loss: 0.080532, acc.: 98.44%] [G loss: 2.546874]\n",
      "epoch:13 step:10931 [D loss: 0.046367, acc.: 99.22%] [G loss: 2.106391]\n",
      "epoch:13 step:10932 [D loss: 0.058773, acc.: 98.44%] [G loss: 0.786047]\n",
      "epoch:13 step:10933 [D loss: 0.304274, acc.: 84.38%] [G loss: 6.649236]\n",
      "epoch:13 step:10934 [D loss: 0.843021, acc.: 64.06%] [G loss: 2.299210]\n",
      "epoch:14 step:10935 [D loss: 0.154339, acc.: 95.31%] [G loss: 2.159057]\n",
      "epoch:14 step:10936 [D loss: 0.018002, acc.: 100.00%] [G loss: 3.011148]\n",
      "epoch:14 step:10937 [D loss: 0.027896, acc.: 100.00%] [G loss: 2.799069]\n",
      "epoch:14 step:10938 [D loss: 0.099315, acc.: 98.44%] [G loss: 2.612171]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:14 step:10939 [D loss: 0.246406, acc.: 90.62%] [G loss: 3.836381]\n",
      "epoch:14 step:10940 [D loss: 0.099865, acc.: 96.09%] [G loss: 3.392451]\n",
      "epoch:14 step:10941 [D loss: 0.685835, acc.: 66.41%] [G loss: 6.784477]\n",
      "epoch:14 step:10942 [D loss: 0.598561, acc.: 70.31%] [G loss: 5.171904]\n",
      "epoch:14 step:10943 [D loss: 0.121105, acc.: 97.66%] [G loss: 3.830410]\n",
      "epoch:14 step:10944 [D loss: 0.034652, acc.: 100.00%] [G loss: 2.882661]\n",
      "epoch:14 step:10945 [D loss: 0.013657, acc.: 100.00%] [G loss: 2.915924]\n",
      "epoch:14 step:10946 [D loss: 0.133898, acc.: 95.31%] [G loss: 3.637766]\n",
      "epoch:14 step:10947 [D loss: 0.042050, acc.: 99.22%] [G loss: 4.761716]\n",
      "epoch:14 step:10948 [D loss: 0.350763, acc.: 83.59%] [G loss: 1.086952]\n",
      "epoch:14 step:10949 [D loss: 0.272565, acc.: 85.94%] [G loss: 3.757599]\n",
      "epoch:14 step:10950 [D loss: 0.024235, acc.: 100.00%] [G loss: 5.695705]\n",
      "epoch:14 step:10951 [D loss: 0.260967, acc.: 86.72%] [G loss: 2.005491]\n",
      "epoch:14 step:10952 [D loss: 0.079229, acc.: 99.22%] [G loss: 3.439304]\n",
      "epoch:14 step:10953 [D loss: 0.051343, acc.: 100.00%] [G loss: 4.667087]\n",
      "epoch:14 step:10954 [D loss: 0.028062, acc.: 99.22%] [G loss: 3.843329]\n",
      "epoch:14 step:10955 [D loss: 0.025512, acc.: 100.00%] [G loss: 3.972085]\n",
      "epoch:14 step:10956 [D loss: 0.084438, acc.: 98.44%] [G loss: 3.354200]\n",
      "epoch:14 step:10957 [D loss: 0.121725, acc.: 97.66%] [G loss: 3.667931]\n",
      "epoch:14 step:10958 [D loss: 0.019862, acc.: 100.00%] [G loss: 4.574992]\n",
      "epoch:14 step:10959 [D loss: 0.258105, acc.: 87.50%] [G loss: 1.860719]\n",
      "epoch:14 step:10960 [D loss: 0.455463, acc.: 78.91%] [G loss: 6.486088]\n",
      "epoch:14 step:10961 [D loss: 0.351696, acc.: 81.25%] [G loss: 5.636281]\n",
      "epoch:14 step:10962 [D loss: 0.085831, acc.: 96.88%] [G loss: 4.332077]\n",
      "epoch:14 step:10963 [D loss: 0.054826, acc.: 99.22%] [G loss: 3.963926]\n",
      "epoch:14 step:10964 [D loss: 0.025885, acc.: 100.00%] [G loss: 3.427191]\n",
      "epoch:14 step:10965 [D loss: 0.028843, acc.: 100.00%] [G loss: 3.586465]\n",
      "epoch:14 step:10966 [D loss: 0.016593, acc.: 100.00%] [G loss: 4.223111]\n",
      "epoch:14 step:10967 [D loss: 0.025232, acc.: 100.00%] [G loss: 3.473809]\n",
      "epoch:14 step:10968 [D loss: 0.016597, acc.: 100.00%] [G loss: 3.405731]\n",
      "epoch:14 step:10969 [D loss: 0.029766, acc.: 100.00%] [G loss: 3.574975]\n",
      "epoch:14 step:10970 [D loss: 0.453918, acc.: 79.69%] [G loss: 4.566913]\n",
      "epoch:14 step:10971 [D loss: 0.058122, acc.: 98.44%] [G loss: 4.683112]\n",
      "epoch:14 step:10972 [D loss: 0.077131, acc.: 97.66%] [G loss: 2.769406]\n",
      "epoch:14 step:10973 [D loss: 0.043000, acc.: 100.00%] [G loss: 2.493319]\n",
      "epoch:14 step:10974 [D loss: 0.037701, acc.: 100.00%] [G loss: 1.923459]\n",
      "epoch:14 step:10975 [D loss: 0.050621, acc.: 100.00%] [G loss: 3.430863]\n",
      "epoch:14 step:10976 [D loss: 0.050328, acc.: 99.22%] [G loss: 2.392171]\n",
      "epoch:14 step:10977 [D loss: 0.032464, acc.: 100.00%] [G loss: 2.330618]\n",
      "epoch:14 step:10978 [D loss: 1.107718, acc.: 43.75%] [G loss: 7.066528]\n",
      "epoch:14 step:10979 [D loss: 1.259658, acc.: 52.34%] [G loss: 5.413922]\n",
      "epoch:14 step:10980 [D loss: 0.348244, acc.: 86.72%] [G loss: 2.228537]\n",
      "epoch:14 step:10981 [D loss: 0.261692, acc.: 89.84%] [G loss: 4.189095]\n",
      "epoch:14 step:10982 [D loss: 0.053195, acc.: 99.22%] [G loss: 4.649016]\n",
      "epoch:14 step:10983 [D loss: 0.289046, acc.: 88.28%] [G loss: 1.607580]\n",
      "epoch:14 step:10984 [D loss: 0.428838, acc.: 74.22%] [G loss: 4.838515]\n",
      "epoch:14 step:10985 [D loss: 0.416139, acc.: 79.69%] [G loss: 4.507038]\n",
      "epoch:14 step:10986 [D loss: 0.037721, acc.: 98.44%] [G loss: 3.118248]\n",
      "epoch:14 step:10987 [D loss: 0.097227, acc.: 98.44%] [G loss: 3.730590]\n",
      "epoch:14 step:10988 [D loss: 0.130516, acc.: 97.66%] [G loss: 3.670703]\n",
      "epoch:14 step:10989 [D loss: 0.047512, acc.: 98.44%] [G loss: 4.208103]\n",
      "epoch:14 step:10990 [D loss: 0.049251, acc.: 100.00%] [G loss: 3.074014]\n",
      "epoch:14 step:10991 [D loss: 0.124250, acc.: 97.66%] [G loss: 3.393075]\n",
      "epoch:14 step:10992 [D loss: 0.045518, acc.: 99.22%] [G loss: 3.110474]\n",
      "epoch:14 step:10993 [D loss: 0.083650, acc.: 100.00%] [G loss: 3.607873]\n",
      "epoch:14 step:10994 [D loss: 0.017568, acc.: 100.00%] [G loss: 1.870918]\n",
      "epoch:14 step:10995 [D loss: 0.100046, acc.: 96.88%] [G loss: 1.969175]\n",
      "epoch:14 step:10996 [D loss: 0.112381, acc.: 96.88%] [G loss: 1.723055]\n",
      "epoch:14 step:10997 [D loss: 0.035447, acc.: 100.00%] [G loss: 2.480986]\n",
      "epoch:14 step:10998 [D loss: 0.061468, acc.: 99.22%] [G loss: 1.348546]\n",
      "epoch:14 step:10999 [D loss: 0.219772, acc.: 93.75%] [G loss: 1.448366]\n",
      "epoch:14 step:11000 [D loss: 0.016615, acc.: 100.00%] [G loss: 2.178876]\n",
      "##############\n",
      "[0.97179667 0.9692656  0.95761085 0.8853919  1.07381193 2.11634925\n",
      " 1.00332233 2.10454954 1.11316524 2.11163763]\n",
      "##########\n",
      "epoch:14 step:11001 [D loss: 0.035902, acc.: 100.00%] [G loss: 3.070012]\n",
      "epoch:14 step:11002 [D loss: 0.055471, acc.: 98.44%] [G loss: 2.694065]\n",
      "epoch:14 step:11003 [D loss: 0.039982, acc.: 100.00%] [G loss: 2.812065]\n",
      "epoch:14 step:11004 [D loss: 0.109409, acc.: 96.88%] [G loss: 4.314540]\n",
      "epoch:14 step:11005 [D loss: 0.165407, acc.: 94.53%] [G loss: 3.350856]\n",
      "epoch:14 step:11006 [D loss: 0.064100, acc.: 99.22%] [G loss: 3.958744]\n",
      "epoch:14 step:11007 [D loss: 0.009785, acc.: 100.00%] [G loss: 5.434639]\n",
      "epoch:14 step:11008 [D loss: 0.011605, acc.: 100.00%] [G loss: 4.286803]\n",
      "epoch:14 step:11009 [D loss: 0.025750, acc.: 100.00%] [G loss: 3.827404]\n",
      "epoch:14 step:11010 [D loss: 0.079728, acc.: 97.66%] [G loss: 3.251902]\n",
      "epoch:14 step:11011 [D loss: 1.288716, acc.: 42.19%] [G loss: 7.010719]\n",
      "epoch:14 step:11012 [D loss: 0.597160, acc.: 70.31%] [G loss: 5.565794]\n",
      "epoch:14 step:11013 [D loss: 0.011900, acc.: 100.00%] [G loss: 4.617507]\n",
      "epoch:14 step:11014 [D loss: 0.015602, acc.: 100.00%] [G loss: 2.569773]\n",
      "epoch:14 step:11015 [D loss: 0.057779, acc.: 99.22%] [G loss: 2.179312]\n",
      "epoch:14 step:11016 [D loss: 0.064086, acc.: 100.00%] [G loss: 3.692597]\n",
      "epoch:14 step:11017 [D loss: 0.044257, acc.: 100.00%] [G loss: 4.804526]\n",
      "epoch:14 step:11018 [D loss: 0.384304, acc.: 80.47%] [G loss: 3.655944]\n",
      "epoch:14 step:11019 [D loss: 0.067370, acc.: 98.44%] [G loss: 3.205559]\n",
      "epoch:14 step:11020 [D loss: 0.032395, acc.: 100.00%] [G loss: 4.632594]\n",
      "epoch:14 step:11021 [D loss: 0.018965, acc.: 100.00%] [G loss: 4.046068]\n",
      "epoch:14 step:11022 [D loss: 0.208740, acc.: 91.41%] [G loss: 3.787790]\n",
      "epoch:14 step:11023 [D loss: 0.042863, acc.: 100.00%] [G loss: 5.310100]\n",
      "epoch:14 step:11024 [D loss: 0.849898, acc.: 52.34%] [G loss: 5.932425]\n",
      "epoch:14 step:11025 [D loss: 0.704309, acc.: 66.41%] [G loss: 3.237413]\n",
      "epoch:14 step:11026 [D loss: 0.104532, acc.: 94.53%] [G loss: 4.518067]\n",
      "epoch:14 step:11027 [D loss: 0.020416, acc.: 100.00%] [G loss: 4.565985]\n",
      "epoch:14 step:11028 [D loss: 0.097066, acc.: 97.66%] [G loss: 4.362226]\n",
      "epoch:14 step:11029 [D loss: 0.023963, acc.: 100.00%] [G loss: 4.346965]\n",
      "epoch:14 step:11030 [D loss: 0.057849, acc.: 100.00%] [G loss: 2.145817]\n",
      "epoch:14 step:11031 [D loss: 0.132428, acc.: 94.53%] [G loss: 4.079993]\n",
      "epoch:14 step:11032 [D loss: 0.020995, acc.: 100.00%] [G loss: 4.026229]\n",
      "epoch:14 step:11033 [D loss: 0.770669, acc.: 61.72%] [G loss: 4.569090]\n",
      "epoch:14 step:11034 [D loss: 0.035887, acc.: 100.00%] [G loss: 5.470207]\n",
      "epoch:14 step:11035 [D loss: 0.117232, acc.: 96.88%] [G loss: 3.265236]\n",
      "epoch:14 step:11036 [D loss: 0.022019, acc.: 100.00%] [G loss: 2.789664]\n",
      "epoch:14 step:11037 [D loss: 0.027844, acc.: 100.00%] [G loss: 2.875083]\n",
      "epoch:14 step:11038 [D loss: 0.076169, acc.: 98.44%] [G loss: 2.994482]\n",
      "epoch:14 step:11039 [D loss: 0.128625, acc.: 96.09%] [G loss: 3.750220]\n",
      "epoch:14 step:11040 [D loss: 0.028806, acc.: 100.00%] [G loss: 3.416862]\n",
      "epoch:14 step:11041 [D loss: 0.295109, acc.: 87.50%] [G loss: 4.689448]\n",
      "epoch:14 step:11042 [D loss: 0.134816, acc.: 96.09%] [G loss: 3.461864]\n",
      "epoch:14 step:11043 [D loss: 0.009863, acc.: 100.00%] [G loss: 3.537044]\n",
      "epoch:14 step:11044 [D loss: 0.115805, acc.: 97.66%] [G loss: 3.841537]\n",
      "epoch:14 step:11045 [D loss: 0.346655, acc.: 86.72%] [G loss: 4.347533]\n",
      "epoch:14 step:11046 [D loss: 0.020432, acc.: 99.22%] [G loss: 4.987443]\n",
      "epoch:14 step:11047 [D loss: 0.016314, acc.: 100.00%] [G loss: 5.009167]\n",
      "epoch:14 step:11048 [D loss: 0.114059, acc.: 98.44%] [G loss: 3.959450]\n",
      "epoch:14 step:11049 [D loss: 0.016352, acc.: 100.00%] [G loss: 3.971178]\n",
      "epoch:14 step:11050 [D loss: 0.234299, acc.: 92.97%] [G loss: 4.492491]\n",
      "epoch:14 step:11051 [D loss: 0.020190, acc.: 100.00%] [G loss: 5.155003]\n",
      "epoch:14 step:11052 [D loss: 0.096257, acc.: 96.09%] [G loss: 3.883114]\n",
      "epoch:14 step:11053 [D loss: 0.034783, acc.: 100.00%] [G loss: 2.921775]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:14 step:11054 [D loss: 0.041726, acc.: 100.00%] [G loss: 4.184379]\n",
      "epoch:14 step:11055 [D loss: 0.026524, acc.: 100.00%] [G loss: 4.263596]\n",
      "epoch:14 step:11056 [D loss: 0.030152, acc.: 99.22%] [G loss: 3.991280]\n",
      "epoch:14 step:11057 [D loss: 0.031027, acc.: 100.00%] [G loss: 3.727612]\n",
      "epoch:14 step:11058 [D loss: 0.026929, acc.: 100.00%] [G loss: 3.584526]\n",
      "epoch:14 step:11059 [D loss: 0.056389, acc.: 98.44%] [G loss: 4.223904]\n",
      "epoch:14 step:11060 [D loss: 0.063671, acc.: 99.22%] [G loss: 3.379079]\n",
      "epoch:14 step:11061 [D loss: 0.058224, acc.: 99.22%] [G loss: 3.371942]\n",
      "epoch:14 step:11062 [D loss: 0.050635, acc.: 99.22%] [G loss: 1.887990]\n",
      "epoch:14 step:11063 [D loss: 0.035682, acc.: 100.00%] [G loss: 1.779941]\n",
      "epoch:14 step:11064 [D loss: 0.168383, acc.: 94.53%] [G loss: 4.946732]\n",
      "epoch:14 step:11065 [D loss: 0.033900, acc.: 99.22%] [G loss: 5.651046]\n",
      "epoch:14 step:11066 [D loss: 4.187470, acc.: 2.34%] [G loss: 6.485196]\n",
      "epoch:14 step:11067 [D loss: 1.060250, acc.: 53.91%] [G loss: 3.615998]\n",
      "epoch:14 step:11068 [D loss: 0.697781, acc.: 67.97%] [G loss: 1.054968]\n",
      "epoch:14 step:11069 [D loss: 1.232391, acc.: 55.47%] [G loss: 4.630684]\n",
      "epoch:14 step:11070 [D loss: 0.312136, acc.: 84.38%] [G loss: 4.695659]\n",
      "epoch:14 step:11071 [D loss: 0.713635, acc.: 60.94%] [G loss: 3.074078]\n",
      "epoch:14 step:11072 [D loss: 0.249613, acc.: 91.41%] [G loss: 2.282807]\n",
      "epoch:14 step:11073 [D loss: 0.099933, acc.: 99.22%] [G loss: 2.234028]\n",
      "epoch:14 step:11074 [D loss: 0.099763, acc.: 98.44%] [G loss: 2.436710]\n",
      "epoch:14 step:11075 [D loss: 0.312987, acc.: 88.28%] [G loss: 2.285074]\n",
      "epoch:14 step:11076 [D loss: 0.357028, acc.: 82.03%] [G loss: 2.724141]\n",
      "epoch:14 step:11077 [D loss: 0.238474, acc.: 93.75%] [G loss: 3.185591]\n",
      "epoch:14 step:11078 [D loss: 0.207789, acc.: 95.31%] [G loss: 2.291291]\n",
      "epoch:14 step:11079 [D loss: 0.248388, acc.: 92.97%] [G loss: 2.755886]\n",
      "epoch:14 step:11080 [D loss: 0.165997, acc.: 99.22%] [G loss: 3.299708]\n",
      "epoch:14 step:11081 [D loss: 0.416090, acc.: 82.81%] [G loss: 3.610260]\n",
      "epoch:14 step:11082 [D loss: 0.258444, acc.: 88.28%] [G loss: 2.882389]\n",
      "epoch:14 step:11083 [D loss: 0.065487, acc.: 99.22%] [G loss: 1.522282]\n",
      "epoch:14 step:11084 [D loss: 0.741179, acc.: 62.50%] [G loss: 3.978316]\n",
      "epoch:14 step:11085 [D loss: 0.380586, acc.: 75.78%] [G loss: 4.020704]\n",
      "epoch:14 step:11086 [D loss: 0.274025, acc.: 87.50%] [G loss: 3.116762]\n",
      "epoch:14 step:11087 [D loss: 0.036353, acc.: 100.00%] [G loss: 1.181329]\n",
      "epoch:14 step:11088 [D loss: 0.070174, acc.: 98.44%] [G loss: 1.488775]\n",
      "epoch:14 step:11089 [D loss: 0.216870, acc.: 91.41%] [G loss: 2.897795]\n",
      "epoch:14 step:11090 [D loss: 0.241669, acc.: 90.62%] [G loss: 1.335992]\n",
      "epoch:14 step:11091 [D loss: 0.185737, acc.: 95.31%] [G loss: 1.846017]\n",
      "epoch:14 step:11092 [D loss: 0.107497, acc.: 96.09%] [G loss: 1.361566]\n",
      "epoch:14 step:11093 [D loss: 0.210247, acc.: 94.53%] [G loss: 2.598027]\n",
      "epoch:14 step:11094 [D loss: 0.576476, acc.: 68.75%] [G loss: 0.395976]\n",
      "epoch:14 step:11095 [D loss: 0.711161, acc.: 59.38%] [G loss: 4.012043]\n",
      "epoch:14 step:11096 [D loss: 0.242675, acc.: 87.50%] [G loss: 4.876374]\n",
      "epoch:14 step:11097 [D loss: 0.506748, acc.: 75.00%] [G loss: 1.561137]\n",
      "epoch:14 step:11098 [D loss: 0.087367, acc.: 99.22%] [G loss: 1.216813]\n",
      "epoch:14 step:11099 [D loss: 0.315465, acc.: 85.16%] [G loss: 2.193140]\n",
      "epoch:14 step:11100 [D loss: 0.189246, acc.: 92.97%] [G loss: 3.198021]\n",
      "epoch:14 step:11101 [D loss: 0.187644, acc.: 96.09%] [G loss: 1.590133]\n",
      "epoch:14 step:11102 [D loss: 0.198176, acc.: 92.19%] [G loss: 1.999750]\n",
      "epoch:14 step:11103 [D loss: 0.098786, acc.: 97.66%] [G loss: 2.590780]\n",
      "epoch:14 step:11104 [D loss: 0.219248, acc.: 92.97%] [G loss: 2.851249]\n",
      "epoch:14 step:11105 [D loss: 0.230081, acc.: 92.19%] [G loss: 2.628409]\n",
      "epoch:14 step:11106 [D loss: 0.350729, acc.: 84.38%] [G loss: 4.173077]\n",
      "epoch:14 step:11107 [D loss: 0.305468, acc.: 84.38%] [G loss: 3.776815]\n",
      "epoch:14 step:11108 [D loss: 0.264405, acc.: 91.41%] [G loss: 3.266213]\n",
      "epoch:14 step:11109 [D loss: 0.097109, acc.: 98.44%] [G loss: 3.771583]\n",
      "epoch:14 step:11110 [D loss: 0.201298, acc.: 96.09%] [G loss: 2.872618]\n",
      "epoch:14 step:11111 [D loss: 0.052833, acc.: 99.22%] [G loss: 3.188334]\n",
      "epoch:14 step:11112 [D loss: 0.148980, acc.: 94.53%] [G loss: 3.519510]\n",
      "epoch:14 step:11113 [D loss: 0.347595, acc.: 83.59%] [G loss: 3.814738]\n",
      "epoch:14 step:11114 [D loss: 0.427027, acc.: 80.47%] [G loss: 2.920671]\n",
      "epoch:14 step:11115 [D loss: 0.105576, acc.: 96.88%] [G loss: 3.783588]\n",
      "epoch:14 step:11116 [D loss: 0.033060, acc.: 100.00%] [G loss: 3.025720]\n",
      "epoch:14 step:11117 [D loss: 0.079301, acc.: 98.44%] [G loss: 2.441325]\n",
      "epoch:14 step:11118 [D loss: 0.107525, acc.: 97.66%] [G loss: 2.604614]\n",
      "epoch:14 step:11119 [D loss: 0.077874, acc.: 100.00%] [G loss: 2.962583]\n",
      "epoch:14 step:11120 [D loss: 0.167335, acc.: 94.53%] [G loss: 2.573394]\n",
      "epoch:14 step:11121 [D loss: 0.260060, acc.: 89.84%] [G loss: 1.590977]\n",
      "epoch:14 step:11122 [D loss: 0.075868, acc.: 98.44%] [G loss: 3.236589]\n",
      "epoch:14 step:11123 [D loss: 0.061578, acc.: 97.66%] [G loss: 2.648072]\n",
      "epoch:14 step:11124 [D loss: 0.255025, acc.: 88.28%] [G loss: 3.824778]\n",
      "epoch:14 step:11125 [D loss: 0.081550, acc.: 98.44%] [G loss: 4.253349]\n",
      "epoch:14 step:11126 [D loss: 0.125569, acc.: 93.75%] [G loss: 2.050738]\n",
      "epoch:14 step:11127 [D loss: 0.342337, acc.: 85.94%] [G loss: 4.682612]\n",
      "epoch:14 step:11128 [D loss: 0.517193, acc.: 76.56%] [G loss: 1.700669]\n",
      "epoch:14 step:11129 [D loss: 0.142752, acc.: 96.09%] [G loss: 3.716110]\n",
      "epoch:14 step:11130 [D loss: 0.303649, acc.: 84.38%] [G loss: 4.512273]\n",
      "epoch:14 step:11131 [D loss: 0.017216, acc.: 100.00%] [G loss: 5.010494]\n",
      "epoch:14 step:11132 [D loss: 0.075879, acc.: 97.66%] [G loss: 3.717807]\n",
      "epoch:14 step:11133 [D loss: 0.076851, acc.: 99.22%] [G loss: 3.471066]\n",
      "epoch:14 step:11134 [D loss: 0.036214, acc.: 100.00%] [G loss: 4.071360]\n",
      "epoch:14 step:11135 [D loss: 0.021454, acc.: 100.00%] [G loss: 3.452989]\n",
      "epoch:14 step:11136 [D loss: 0.295533, acc.: 86.72%] [G loss: 3.010092]\n",
      "epoch:14 step:11137 [D loss: 0.054336, acc.: 99.22%] [G loss: 3.219118]\n",
      "epoch:14 step:11138 [D loss: 0.043265, acc.: 99.22%] [G loss: 3.300376]\n",
      "epoch:14 step:11139 [D loss: 0.679222, acc.: 64.06%] [G loss: 6.823913]\n",
      "epoch:14 step:11140 [D loss: 0.292250, acc.: 81.25%] [G loss: 5.796342]\n",
      "epoch:14 step:11141 [D loss: 0.017452, acc.: 100.00%] [G loss: 4.632763]\n",
      "epoch:14 step:11142 [D loss: 0.075883, acc.: 99.22%] [G loss: 4.139254]\n",
      "epoch:14 step:11143 [D loss: 0.082726, acc.: 97.66%] [G loss: 4.600561]\n",
      "epoch:14 step:11144 [D loss: 0.041614, acc.: 100.00%] [G loss: 4.171508]\n",
      "epoch:14 step:11145 [D loss: 0.037473, acc.: 100.00%] [G loss: 4.296781]\n",
      "epoch:14 step:11146 [D loss: 0.016125, acc.: 100.00%] [G loss: 4.152027]\n",
      "epoch:14 step:11147 [D loss: 0.159443, acc.: 97.66%] [G loss: 4.197996]\n",
      "epoch:14 step:11148 [D loss: 0.305549, acc.: 85.16%] [G loss: 3.888880]\n",
      "epoch:14 step:11149 [D loss: 0.016895, acc.: 100.00%] [G loss: 4.028412]\n",
      "epoch:14 step:11150 [D loss: 0.083522, acc.: 99.22%] [G loss: 4.759078]\n",
      "epoch:14 step:11151 [D loss: 0.024518, acc.: 100.00%] [G loss: 3.547814]\n",
      "epoch:14 step:11152 [D loss: 0.321541, acc.: 88.28%] [G loss: 4.062058]\n",
      "epoch:14 step:11153 [D loss: 0.008735, acc.: 100.00%] [G loss: 4.614286]\n",
      "epoch:14 step:11154 [D loss: 0.023069, acc.: 100.00%] [G loss: 3.206326]\n",
      "epoch:14 step:11155 [D loss: 0.019089, acc.: 100.00%] [G loss: 4.358998]\n",
      "epoch:14 step:11156 [D loss: 0.089825, acc.: 97.66%] [G loss: 4.439740]\n",
      "epoch:14 step:11157 [D loss: 0.063950, acc.: 99.22%] [G loss: 5.465713]\n",
      "epoch:14 step:11158 [D loss: 0.141385, acc.: 94.53%] [G loss: 3.592125]\n",
      "epoch:14 step:11159 [D loss: 0.048505, acc.: 99.22%] [G loss: 3.985291]\n",
      "epoch:14 step:11160 [D loss: 0.029839, acc.: 100.00%] [G loss: 4.889348]\n",
      "epoch:14 step:11161 [D loss: 0.029800, acc.: 99.22%] [G loss: 4.852538]\n",
      "epoch:14 step:11162 [D loss: 0.028176, acc.: 100.00%] [G loss: 4.105374]\n",
      "epoch:14 step:11163 [D loss: 0.055429, acc.: 98.44%] [G loss: 2.739680]\n",
      "epoch:14 step:11164 [D loss: 0.130428, acc.: 95.31%] [G loss: 4.556902]\n",
      "epoch:14 step:11165 [D loss: 0.077837, acc.: 97.66%] [G loss: 4.720507]\n",
      "epoch:14 step:11166 [D loss: 1.557432, acc.: 34.38%] [G loss: 6.984527]\n",
      "epoch:14 step:11167 [D loss: 1.405559, acc.: 54.69%] [G loss: 3.019325]\n",
      "epoch:14 step:11168 [D loss: 0.041101, acc.: 100.00%] [G loss: 1.130132]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:14 step:11169 [D loss: 0.107923, acc.: 97.66%] [G loss: 1.616449]\n",
      "epoch:14 step:11170 [D loss: 0.046033, acc.: 99.22%] [G loss: 1.030526]\n",
      "epoch:14 step:11171 [D loss: 0.126690, acc.: 95.31%] [G loss: 2.189540]\n",
      "epoch:14 step:11172 [D loss: 0.143118, acc.: 93.75%] [G loss: 0.908203]\n",
      "epoch:14 step:11173 [D loss: 0.302911, acc.: 82.81%] [G loss: 3.879447]\n",
      "epoch:14 step:11174 [D loss: 0.577936, acc.: 69.53%] [G loss: 2.920295]\n",
      "epoch:14 step:11175 [D loss: 0.294777, acc.: 85.94%] [G loss: 5.247226]\n",
      "epoch:14 step:11176 [D loss: 0.031467, acc.: 100.00%] [G loss: 5.192994]\n",
      "epoch:14 step:11177 [D loss: 0.110227, acc.: 97.66%] [G loss: 4.887724]\n",
      "epoch:14 step:11178 [D loss: 0.006923, acc.: 100.00%] [G loss: 3.473909]\n",
      "epoch:14 step:11179 [D loss: 0.009576, acc.: 100.00%] [G loss: 3.521132]\n",
      "epoch:14 step:11180 [D loss: 0.089072, acc.: 97.66%] [G loss: 4.188315]\n",
      "epoch:14 step:11181 [D loss: 0.241842, acc.: 94.53%] [G loss: 4.052268]\n",
      "epoch:14 step:11182 [D loss: 0.054984, acc.: 98.44%] [G loss: 4.923291]\n",
      "epoch:14 step:11183 [D loss: 0.091472, acc.: 98.44%] [G loss: 3.370841]\n",
      "epoch:14 step:11184 [D loss: 0.123955, acc.: 95.31%] [G loss: 4.339551]\n",
      "epoch:14 step:11185 [D loss: 0.014209, acc.: 100.00%] [G loss: 3.467083]\n",
      "epoch:14 step:11186 [D loss: 0.290464, acc.: 89.06%] [G loss: 3.694458]\n",
      "epoch:14 step:11187 [D loss: 0.058092, acc.: 99.22%] [G loss: 4.399071]\n",
      "epoch:14 step:11188 [D loss: 0.027805, acc.: 100.00%] [G loss: 3.196517]\n",
      "epoch:14 step:11189 [D loss: 0.083185, acc.: 98.44%] [G loss: 2.224810]\n",
      "epoch:14 step:11190 [D loss: 0.085004, acc.: 98.44%] [G loss: 3.027075]\n",
      "epoch:14 step:11191 [D loss: 0.043230, acc.: 99.22%] [G loss: 2.180529]\n",
      "epoch:14 step:11192 [D loss: 0.040386, acc.: 99.22%] [G loss: 0.939317]\n",
      "epoch:14 step:11193 [D loss: 0.053718, acc.: 100.00%] [G loss: 0.851252]\n",
      "epoch:14 step:11194 [D loss: 0.029178, acc.: 100.00%] [G loss: 0.784175]\n",
      "epoch:14 step:11195 [D loss: 0.261888, acc.: 90.62%] [G loss: 4.354200]\n",
      "epoch:14 step:11196 [D loss: 0.089630, acc.: 98.44%] [G loss: 5.255171]\n",
      "epoch:14 step:11197 [D loss: 0.050187, acc.: 100.00%] [G loss: 3.329109]\n",
      "epoch:14 step:11198 [D loss: 0.176028, acc.: 98.44%] [G loss: 1.697133]\n",
      "epoch:14 step:11199 [D loss: 0.124979, acc.: 97.66%] [G loss: 4.963569]\n",
      "epoch:14 step:11200 [D loss: 0.078812, acc.: 97.66%] [G loss: 3.740490]\n",
      "##############\n",
      "[0.99690045 1.02628567 0.97272305 0.9319752  1.1249132  1.03325927\n",
      " 0.87010105 0.80641037 2.11480478 0.99456423]\n",
      "##########\n",
      "epoch:14 step:11201 [D loss: 0.467746, acc.: 77.34%] [G loss: 4.645604]\n",
      "epoch:14 step:11202 [D loss: 0.010864, acc.: 100.00%] [G loss: 5.713516]\n",
      "epoch:14 step:11203 [D loss: 0.043280, acc.: 100.00%] [G loss: 4.518689]\n",
      "epoch:14 step:11204 [D loss: 0.160002, acc.: 96.88%] [G loss: 3.232039]\n",
      "epoch:14 step:11205 [D loss: 0.019209, acc.: 100.00%] [G loss: 2.859465]\n",
      "epoch:14 step:11206 [D loss: 0.076740, acc.: 97.66%] [G loss: 5.286975]\n",
      "epoch:14 step:11207 [D loss: 0.189442, acc.: 92.19%] [G loss: 1.679571]\n",
      "epoch:14 step:11208 [D loss: 0.463714, acc.: 76.56%] [G loss: 7.120337]\n",
      "epoch:14 step:11209 [D loss: 1.472056, acc.: 51.56%] [G loss: 3.641716]\n",
      "epoch:14 step:11210 [D loss: 0.030389, acc.: 100.00%] [G loss: 1.662679]\n",
      "epoch:14 step:11211 [D loss: 0.029970, acc.: 100.00%] [G loss: 2.353228]\n",
      "epoch:14 step:11212 [D loss: 0.030272, acc.: 100.00%] [G loss: 1.329449]\n",
      "epoch:14 step:11213 [D loss: 0.125056, acc.: 96.88%] [G loss: 3.071890]\n",
      "epoch:14 step:11214 [D loss: 0.047304, acc.: 99.22%] [G loss: 2.822480]\n",
      "epoch:14 step:11215 [D loss: 0.052411, acc.: 97.66%] [G loss: 1.749096]\n",
      "epoch:14 step:11216 [D loss: 0.110504, acc.: 96.88%] [G loss: 2.224152]\n",
      "epoch:14 step:11217 [D loss: 0.015460, acc.: 100.00%] [G loss: 2.432770]\n",
      "epoch:14 step:11218 [D loss: 0.070135, acc.: 98.44%] [G loss: 0.824284]\n",
      "epoch:14 step:11219 [D loss: 0.486507, acc.: 75.00%] [G loss: 6.704762]\n",
      "epoch:14 step:11220 [D loss: 1.722751, acc.: 50.78%] [G loss: 1.793378]\n",
      "epoch:14 step:11221 [D loss: 0.354099, acc.: 82.81%] [G loss: 2.478276]\n",
      "epoch:14 step:11222 [D loss: 0.104194, acc.: 97.66%] [G loss: 3.433035]\n",
      "epoch:14 step:11223 [D loss: 0.314677, acc.: 83.59%] [G loss: 2.391095]\n",
      "epoch:14 step:11224 [D loss: 0.125835, acc.: 99.22%] [G loss: 2.779015]\n",
      "epoch:14 step:11225 [D loss: 0.089208, acc.: 96.88%] [G loss: 2.768147]\n",
      "epoch:14 step:11226 [D loss: 0.127429, acc.: 98.44%] [G loss: 2.877329]\n",
      "epoch:14 step:11227 [D loss: 0.131617, acc.: 96.09%] [G loss: 3.155533]\n",
      "epoch:14 step:11228 [D loss: 0.195056, acc.: 92.97%] [G loss: 3.524772]\n",
      "epoch:14 step:11229 [D loss: 0.360508, acc.: 83.59%] [G loss: 1.849185]\n",
      "epoch:14 step:11230 [D loss: 0.233182, acc.: 90.62%] [G loss: 4.390413]\n",
      "epoch:14 step:11231 [D loss: 0.343933, acc.: 85.94%] [G loss: 3.521551]\n",
      "epoch:14 step:11232 [D loss: 0.047591, acc.: 99.22%] [G loss: 3.033783]\n",
      "epoch:14 step:11233 [D loss: 0.055699, acc.: 100.00%] [G loss: 2.114630]\n",
      "epoch:14 step:11234 [D loss: 0.040208, acc.: 100.00%] [G loss: 1.312223]\n",
      "epoch:14 step:11235 [D loss: 0.095472, acc.: 96.88%] [G loss: 2.607289]\n",
      "epoch:14 step:11236 [D loss: 0.078596, acc.: 96.88%] [G loss: 1.716357]\n",
      "epoch:14 step:11237 [D loss: 0.043719, acc.: 99.22%] [G loss: 0.987293]\n",
      "epoch:14 step:11238 [D loss: 0.046620, acc.: 100.00%] [G loss: 1.711417]\n",
      "epoch:14 step:11239 [D loss: 0.098946, acc.: 99.22%] [G loss: 3.912109]\n",
      "epoch:14 step:11240 [D loss: 0.185575, acc.: 92.19%] [G loss: 1.492208]\n",
      "epoch:14 step:11241 [D loss: 0.021262, acc.: 100.00%] [G loss: 1.183728]\n",
      "epoch:14 step:11242 [D loss: 0.143817, acc.: 93.75%] [G loss: 3.545009]\n",
      "epoch:14 step:11243 [D loss: 0.343268, acc.: 89.06%] [G loss: 0.957025]\n",
      "epoch:14 step:11244 [D loss: 0.140699, acc.: 95.31%] [G loss: 3.597255]\n",
      "epoch:14 step:11245 [D loss: 0.002793, acc.: 100.00%] [G loss: 4.761492]\n",
      "epoch:14 step:11246 [D loss: 0.136959, acc.: 95.31%] [G loss: 2.455940]\n",
      "epoch:14 step:11247 [D loss: 0.031279, acc.: 99.22%] [G loss: 2.739714]\n",
      "epoch:14 step:11248 [D loss: 0.013322, acc.: 100.00%] [G loss: 2.070101]\n",
      "epoch:14 step:11249 [D loss: 0.047444, acc.: 100.00%] [G loss: 2.067852]\n",
      "epoch:14 step:11250 [D loss: 0.027670, acc.: 99.22%] [G loss: 3.028288]\n",
      "epoch:14 step:11251 [D loss: 0.012100, acc.: 100.00%] [G loss: 2.235689]\n",
      "epoch:14 step:11252 [D loss: 0.101661, acc.: 97.66%] [G loss: 3.938416]\n",
      "epoch:14 step:11253 [D loss: 0.052297, acc.: 99.22%] [G loss: 3.074443]\n",
      "epoch:14 step:11254 [D loss: 0.226932, acc.: 89.06%] [G loss: 0.458440]\n",
      "epoch:14 step:11255 [D loss: 0.123393, acc.: 96.09%] [G loss: 1.954445]\n",
      "epoch:14 step:11256 [D loss: 0.033127, acc.: 99.22%] [G loss: 5.055239]\n",
      "epoch:14 step:11257 [D loss: 0.007269, acc.: 100.00%] [G loss: 4.696856]\n",
      "epoch:14 step:11258 [D loss: 1.688403, acc.: 32.03%] [G loss: 7.111911]\n",
      "epoch:14 step:11259 [D loss: 1.608382, acc.: 51.56%] [G loss: 5.249935]\n",
      "epoch:14 step:11260 [D loss: 0.145523, acc.: 93.75%] [G loss: 3.603620]\n",
      "epoch:14 step:11261 [D loss: 0.091268, acc.: 98.44%] [G loss: 3.634555]\n",
      "epoch:14 step:11262 [D loss: 0.061547, acc.: 100.00%] [G loss: 3.447656]\n",
      "epoch:14 step:11263 [D loss: 0.071321, acc.: 98.44%] [G loss: 3.403656]\n",
      "epoch:14 step:11264 [D loss: 0.052908, acc.: 100.00%] [G loss: 4.014081]\n",
      "epoch:14 step:11265 [D loss: 0.042677, acc.: 99.22%] [G loss: 3.371535]\n",
      "epoch:14 step:11266 [D loss: 0.068680, acc.: 98.44%] [G loss: 4.006186]\n",
      "epoch:14 step:11267 [D loss: 0.067730, acc.: 100.00%] [G loss: 3.204773]\n",
      "epoch:14 step:11268 [D loss: 0.144278, acc.: 95.31%] [G loss: 4.976902]\n",
      "epoch:14 step:11269 [D loss: 0.068681, acc.: 96.88%] [G loss: 5.008009]\n",
      "epoch:14 step:11270 [D loss: 0.570504, acc.: 71.88%] [G loss: 1.892629]\n",
      "epoch:14 step:11271 [D loss: 0.505931, acc.: 72.66%] [G loss: 5.673196]\n",
      "epoch:14 step:11272 [D loss: 0.202479, acc.: 91.41%] [G loss: 5.362640]\n",
      "epoch:14 step:11273 [D loss: 0.067116, acc.: 96.88%] [G loss: 4.518989]\n",
      "epoch:14 step:11274 [D loss: 0.128929, acc.: 95.31%] [G loss: 2.597218]\n",
      "epoch:14 step:11275 [D loss: 0.262674, acc.: 86.72%] [G loss: 4.427749]\n",
      "epoch:14 step:11276 [D loss: 0.009297, acc.: 100.00%] [G loss: 5.255880]\n",
      "epoch:14 step:11277 [D loss: 0.285452, acc.: 85.16%] [G loss: 2.861880]\n",
      "epoch:14 step:11278 [D loss: 0.023417, acc.: 99.22%] [G loss: 2.175342]\n",
      "epoch:14 step:11279 [D loss: 0.028516, acc.: 99.22%] [G loss: 3.005509]\n",
      "epoch:14 step:11280 [D loss: 0.042511, acc.: 100.00%] [G loss: 2.385286]\n",
      "epoch:14 step:11281 [D loss: 0.013727, acc.: 100.00%] [G loss: 2.174096]\n",
      "epoch:14 step:11282 [D loss: 0.051770, acc.: 99.22%] [G loss: 3.559838]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:14 step:11283 [D loss: 0.024326, acc.: 100.00%] [G loss: 1.016672]\n",
      "epoch:14 step:11284 [D loss: 0.052601, acc.: 99.22%] [G loss: 2.816379]\n",
      "epoch:14 step:11285 [D loss: 0.015053, acc.: 100.00%] [G loss: 2.742882]\n",
      "epoch:14 step:11286 [D loss: 0.012988, acc.: 100.00%] [G loss: 2.629945]\n",
      "epoch:14 step:11287 [D loss: 0.013523, acc.: 100.00%] [G loss: 1.506225]\n",
      "epoch:14 step:11288 [D loss: 0.025255, acc.: 100.00%] [G loss: 1.857488]\n",
      "epoch:14 step:11289 [D loss: 0.114610, acc.: 96.88%] [G loss: 1.723851]\n",
      "epoch:14 step:11290 [D loss: 0.027090, acc.: 100.00%] [G loss: 2.737650]\n",
      "epoch:14 step:11291 [D loss: 0.052873, acc.: 99.22%] [G loss: 3.123385]\n",
      "epoch:14 step:11292 [D loss: 0.228818, acc.: 91.41%] [G loss: 1.282661]\n",
      "epoch:14 step:11293 [D loss: 0.111003, acc.: 97.66%] [G loss: 2.766731]\n",
      "epoch:14 step:11294 [D loss: 0.009446, acc.: 100.00%] [G loss: 3.741319]\n",
      "epoch:14 step:11295 [D loss: 1.255309, acc.: 32.81%] [G loss: 5.190030]\n",
      "epoch:14 step:11296 [D loss: 0.030689, acc.: 99.22%] [G loss: 6.525948]\n",
      "epoch:14 step:11297 [D loss: 0.731015, acc.: 65.62%] [G loss: 3.812786]\n",
      "epoch:14 step:11298 [D loss: 0.113202, acc.: 96.09%] [G loss: 3.076129]\n",
      "epoch:14 step:11299 [D loss: 0.075305, acc.: 97.66%] [G loss: 4.523466]\n",
      "epoch:14 step:11300 [D loss: 0.011777, acc.: 100.00%] [G loss: 3.964733]\n",
      "epoch:14 step:11301 [D loss: 0.034870, acc.: 99.22%] [G loss: 4.603601]\n",
      "epoch:14 step:11302 [D loss: 0.016386, acc.: 100.00%] [G loss: 4.286022]\n",
      "epoch:14 step:11303 [D loss: 0.035114, acc.: 100.00%] [G loss: 2.791532]\n",
      "epoch:14 step:11304 [D loss: 0.075092, acc.: 98.44%] [G loss: 3.486846]\n",
      "epoch:14 step:11305 [D loss: 0.050789, acc.: 99.22%] [G loss: 3.898724]\n",
      "epoch:14 step:11306 [D loss: 0.026632, acc.: 100.00%] [G loss: 2.231033]\n",
      "epoch:14 step:11307 [D loss: 0.012054, acc.: 100.00%] [G loss: 1.547209]\n",
      "epoch:14 step:11308 [D loss: 0.081136, acc.: 100.00%] [G loss: 2.804783]\n",
      "epoch:14 step:11309 [D loss: 0.244607, acc.: 91.41%] [G loss: 0.449940]\n",
      "epoch:14 step:11310 [D loss: 0.045763, acc.: 100.00%] [G loss: 1.116814]\n",
      "epoch:14 step:11311 [D loss: 0.005430, acc.: 100.00%] [G loss: 1.208023]\n",
      "epoch:14 step:11312 [D loss: 0.015039, acc.: 100.00%] [G loss: 1.143944]\n",
      "epoch:14 step:11313 [D loss: 0.321490, acc.: 83.59%] [G loss: 5.880409]\n",
      "epoch:14 step:11314 [D loss: 0.847521, acc.: 57.03%] [G loss: 3.257741]\n",
      "epoch:14 step:11315 [D loss: 0.077148, acc.: 98.44%] [G loss: 2.915089]\n",
      "epoch:14 step:11316 [D loss: 0.204939, acc.: 94.53%] [G loss: 4.213567]\n",
      "epoch:14 step:11317 [D loss: 0.022282, acc.: 100.00%] [G loss: 4.906678]\n",
      "epoch:14 step:11318 [D loss: 0.246838, acc.: 88.28%] [G loss: 2.615507]\n",
      "epoch:14 step:11319 [D loss: 0.045060, acc.: 100.00%] [G loss: 2.201018]\n",
      "epoch:14 step:11320 [D loss: 0.484304, acc.: 71.88%] [G loss: 5.597029]\n",
      "epoch:14 step:11321 [D loss: 0.299321, acc.: 80.47%] [G loss: 5.579855]\n",
      "epoch:14 step:11322 [D loss: 0.142942, acc.: 93.75%] [G loss: 3.116257]\n",
      "epoch:14 step:11323 [D loss: 0.029467, acc.: 100.00%] [G loss: 2.900894]\n",
      "epoch:14 step:11324 [D loss: 0.035173, acc.: 100.00%] [G loss: 2.650975]\n",
      "epoch:14 step:11325 [D loss: 0.024300, acc.: 100.00%] [G loss: 2.215395]\n",
      "epoch:14 step:11326 [D loss: 0.067040, acc.: 97.66%] [G loss: 3.559740]\n",
      "epoch:14 step:11327 [D loss: 0.040180, acc.: 100.00%] [G loss: 3.317602]\n",
      "epoch:14 step:11328 [D loss: 0.013483, acc.: 100.00%] [G loss: 3.792202]\n",
      "epoch:14 step:11329 [D loss: 0.192022, acc.: 94.53%] [G loss: 3.654773]\n",
      "epoch:14 step:11330 [D loss: 0.029678, acc.: 100.00%] [G loss: 3.393465]\n",
      "epoch:14 step:11331 [D loss: 0.064910, acc.: 98.44%] [G loss: 3.110368]\n",
      "epoch:14 step:11332 [D loss: 0.044801, acc.: 99.22%] [G loss: 2.865354]\n",
      "epoch:14 step:11333 [D loss: 0.022769, acc.: 100.00%] [G loss: 3.297785]\n",
      "epoch:14 step:11334 [D loss: 0.028190, acc.: 100.00%] [G loss: 3.222689]\n",
      "epoch:14 step:11335 [D loss: 0.100576, acc.: 98.44%] [G loss: 2.037256]\n",
      "epoch:14 step:11336 [D loss: 0.066766, acc.: 99.22%] [G loss: 2.334940]\n",
      "epoch:14 step:11337 [D loss: 0.404097, acc.: 81.25%] [G loss: 4.449884]\n",
      "epoch:14 step:11338 [D loss: 0.072492, acc.: 97.66%] [G loss: 4.246880]\n",
      "epoch:14 step:11339 [D loss: 2.217121, acc.: 24.22%] [G loss: 7.565894]\n",
      "epoch:14 step:11340 [D loss: 1.164106, acc.: 52.34%] [G loss: 5.185024]\n",
      "epoch:14 step:11341 [D loss: 0.026294, acc.: 99.22%] [G loss: 5.170679]\n",
      "epoch:14 step:11342 [D loss: 0.011698, acc.: 100.00%] [G loss: 4.805480]\n",
      "epoch:14 step:11343 [D loss: 0.021990, acc.: 100.00%] [G loss: 3.601517]\n",
      "epoch:14 step:11344 [D loss: 0.038344, acc.: 99.22%] [G loss: 3.905527]\n",
      "epoch:14 step:11345 [D loss: 0.229451, acc.: 91.41%] [G loss: 1.044897]\n",
      "epoch:14 step:11346 [D loss: 1.441138, acc.: 56.25%] [G loss: 6.850689]\n",
      "epoch:14 step:11347 [D loss: 0.928195, acc.: 57.81%] [G loss: 6.275321]\n",
      "epoch:14 step:11348 [D loss: 0.358791, acc.: 81.25%] [G loss: 3.485789]\n",
      "epoch:14 step:11349 [D loss: 0.122167, acc.: 96.09%] [G loss: 3.331489]\n",
      "epoch:14 step:11350 [D loss: 0.020805, acc.: 100.00%] [G loss: 3.005198]\n",
      "epoch:14 step:11351 [D loss: 0.040705, acc.: 99.22%] [G loss: 2.858758]\n",
      "epoch:14 step:11352 [D loss: 0.054425, acc.: 98.44%] [G loss: 3.189861]\n",
      "epoch:14 step:11353 [D loss: 0.059152, acc.: 99.22%] [G loss: 3.285666]\n",
      "epoch:14 step:11354 [D loss: 0.165668, acc.: 94.53%] [G loss: 3.066121]\n",
      "epoch:14 step:11355 [D loss: 0.080547, acc.: 99.22%] [G loss: 2.997277]\n",
      "epoch:14 step:11356 [D loss: 0.125323, acc.: 98.44%] [G loss: 3.686437]\n",
      "epoch:14 step:11357 [D loss: 0.032525, acc.: 99.22%] [G loss: 4.203411]\n",
      "epoch:14 step:11358 [D loss: 0.493145, acc.: 75.78%] [G loss: 2.279154]\n",
      "epoch:14 step:11359 [D loss: 0.102160, acc.: 97.66%] [G loss: 4.128657]\n",
      "epoch:14 step:11360 [D loss: 0.038649, acc.: 100.00%] [G loss: 4.149476]\n",
      "epoch:14 step:11361 [D loss: 0.058171, acc.: 100.00%] [G loss: 3.489683]\n",
      "epoch:14 step:11362 [D loss: 0.205746, acc.: 96.09%] [G loss: 2.647283]\n",
      "epoch:14 step:11363 [D loss: 0.107023, acc.: 98.44%] [G loss: 2.923361]\n",
      "epoch:14 step:11364 [D loss: 0.327641, acc.: 89.84%] [G loss: 3.758557]\n",
      "epoch:14 step:11365 [D loss: 0.554766, acc.: 76.56%] [G loss: 2.518027]\n",
      "epoch:14 step:11366 [D loss: 0.067583, acc.: 99.22%] [G loss: 3.385309]\n",
      "epoch:14 step:11367 [D loss: 0.049774, acc.: 99.22%] [G loss: 4.100186]\n",
      "epoch:14 step:11368 [D loss: 0.110458, acc.: 98.44%] [G loss: 3.465460]\n",
      "epoch:14 step:11369 [D loss: 0.133476, acc.: 96.88%] [G loss: 2.892904]\n",
      "epoch:14 step:11370 [D loss: 0.110290, acc.: 98.44%] [G loss: 3.339701]\n",
      "epoch:14 step:11371 [D loss: 0.180960, acc.: 95.31%] [G loss: 3.751722]\n",
      "epoch:14 step:11372 [D loss: 0.227692, acc.: 92.97%] [G loss: 2.763853]\n",
      "epoch:14 step:11373 [D loss: 0.059426, acc.: 100.00%] [G loss: 3.187113]\n",
      "epoch:14 step:11374 [D loss: 0.064701, acc.: 99.22%] [G loss: 3.893341]\n",
      "epoch:14 step:11375 [D loss: 0.086007, acc.: 98.44%] [G loss: 3.307086]\n",
      "epoch:14 step:11376 [D loss: 0.072171, acc.: 99.22%] [G loss: 3.443332]\n",
      "epoch:14 step:11377 [D loss: 0.097820, acc.: 97.66%] [G loss: 3.186989]\n",
      "epoch:14 step:11378 [D loss: 0.055210, acc.: 100.00%] [G loss: 4.395194]\n",
      "epoch:14 step:11379 [D loss: 0.141118, acc.: 96.09%] [G loss: 3.499250]\n",
      "epoch:14 step:11380 [D loss: 0.038757, acc.: 100.00%] [G loss: 3.919165]\n",
      "epoch:14 step:11381 [D loss: 0.118464, acc.: 97.66%] [G loss: 4.687930]\n",
      "epoch:14 step:11382 [D loss: 0.220997, acc.: 89.84%] [G loss: 3.257219]\n",
      "epoch:14 step:11383 [D loss: 0.059611, acc.: 100.00%] [G loss: 3.823734]\n",
      "epoch:14 step:11384 [D loss: 0.031321, acc.: 100.00%] [G loss: 3.013813]\n",
      "epoch:14 step:11385 [D loss: 0.155341, acc.: 96.09%] [G loss: 4.259155]\n",
      "epoch:14 step:11386 [D loss: 0.067722, acc.: 99.22%] [G loss: 3.922545]\n",
      "epoch:14 step:11387 [D loss: 0.255461, acc.: 92.97%] [G loss: 3.273949]\n",
      "epoch:14 step:11388 [D loss: 0.036068, acc.: 100.00%] [G loss: 3.953443]\n",
      "epoch:14 step:11389 [D loss: 0.029324, acc.: 100.00%] [G loss: 4.293904]\n",
      "epoch:14 step:11390 [D loss: 0.119744, acc.: 94.53%] [G loss: 4.484772]\n",
      "epoch:14 step:11391 [D loss: 0.035619, acc.: 99.22%] [G loss: 4.514597]\n",
      "epoch:14 step:11392 [D loss: 0.187473, acc.: 95.31%] [G loss: 4.329051]\n",
      "epoch:14 step:11393 [D loss: 0.080117, acc.: 98.44%] [G loss: 3.542522]\n",
      "epoch:14 step:11394 [D loss: 0.110289, acc.: 98.44%] [G loss: 3.575262]\n",
      "epoch:14 step:11395 [D loss: 0.021103, acc.: 100.00%] [G loss: 4.138477]\n",
      "epoch:14 step:11396 [D loss: 0.014325, acc.: 100.00%] [G loss: 4.174456]\n",
      "epoch:14 step:11397 [D loss: 0.026296, acc.: 100.00%] [G loss: 4.171424]\n",
      "epoch:14 step:11398 [D loss: 0.153066, acc.: 96.88%] [G loss: 4.461028]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:14 step:11399 [D loss: 0.012505, acc.: 100.00%] [G loss: 5.295767]\n",
      "epoch:14 step:11400 [D loss: 0.110666, acc.: 97.66%] [G loss: 4.236924]\n",
      "##############\n",
      "[0.85459958 0.95961057 0.89522029 0.97581361 1.09176116 2.10051383\n",
      " 1.02840436 1.10062761 0.95835271 2.11842663]\n",
      "##########\n",
      "epoch:14 step:11401 [D loss: 0.076488, acc.: 100.00%] [G loss: 4.049175]\n",
      "epoch:14 step:11402 [D loss: 0.046806, acc.: 99.22%] [G loss: 4.284340]\n",
      "epoch:14 step:11403 [D loss: 0.041630, acc.: 100.00%] [G loss: 4.696516]\n",
      "epoch:14 step:11404 [D loss: 0.023931, acc.: 100.00%] [G loss: 4.233037]\n",
      "epoch:14 step:11405 [D loss: 0.209059, acc.: 92.97%] [G loss: 3.699158]\n",
      "epoch:14 step:11406 [D loss: 0.047697, acc.: 100.00%] [G loss: 4.593019]\n",
      "epoch:14 step:11407 [D loss: 0.008066, acc.: 100.00%] [G loss: 4.707047]\n",
      "epoch:14 step:11408 [D loss: 0.667585, acc.: 66.41%] [G loss: 6.653487]\n",
      "epoch:14 step:11409 [D loss: 1.121361, acc.: 52.34%] [G loss: 1.759909]\n",
      "epoch:14 step:11410 [D loss: 0.618266, acc.: 78.91%] [G loss: 6.382599]\n",
      "epoch:14 step:11411 [D loss: 0.938144, acc.: 59.38%] [G loss: 3.484600]\n",
      "epoch:14 step:11412 [D loss: 0.073213, acc.: 99.22%] [G loss: 4.524154]\n",
      "epoch:14 step:11413 [D loss: 0.020058, acc.: 100.00%] [G loss: 4.905692]\n",
      "epoch:14 step:11414 [D loss: 0.064984, acc.: 96.88%] [G loss: 5.289759]\n",
      "epoch:14 step:11415 [D loss: 0.030217, acc.: 99.22%] [G loss: 4.991250]\n",
      "epoch:14 step:11416 [D loss: 0.176546, acc.: 95.31%] [G loss: 3.525716]\n",
      "epoch:14 step:11417 [D loss: 0.037858, acc.: 99.22%] [G loss: 4.113458]\n",
      "epoch:14 step:11418 [D loss: 0.022999, acc.: 100.00%] [G loss: 4.095362]\n",
      "epoch:14 step:11419 [D loss: 0.187665, acc.: 95.31%] [G loss: 3.087157]\n",
      "epoch:14 step:11420 [D loss: 0.035713, acc.: 99.22%] [G loss: 4.830683]\n",
      "epoch:14 step:11421 [D loss: 0.030271, acc.: 99.22%] [G loss: 3.360690]\n",
      "epoch:14 step:11422 [D loss: 0.056515, acc.: 100.00%] [G loss: 2.846779]\n",
      "epoch:14 step:11423 [D loss: 0.116632, acc.: 96.09%] [G loss: 3.679243]\n",
      "epoch:14 step:11424 [D loss: 0.002760, acc.: 100.00%] [G loss: 3.665273]\n",
      "epoch:14 step:11425 [D loss: 0.315878, acc.: 85.94%] [G loss: 5.696720]\n",
      "epoch:14 step:11426 [D loss: 0.474360, acc.: 82.81%] [G loss: 1.356630]\n",
      "epoch:14 step:11427 [D loss: 0.460987, acc.: 79.69%] [G loss: 6.316045]\n",
      "epoch:14 step:11428 [D loss: 0.360319, acc.: 84.38%] [G loss: 4.149485]\n",
      "epoch:14 step:11429 [D loss: 0.121260, acc.: 96.09%] [G loss: 4.055078]\n",
      "epoch:14 step:11430 [D loss: 0.120939, acc.: 98.44%] [G loss: 3.142874]\n",
      "epoch:14 step:11431 [D loss: 0.052996, acc.: 100.00%] [G loss: 2.993529]\n",
      "epoch:14 step:11432 [D loss: 0.450422, acc.: 75.78%] [G loss: 6.170487]\n",
      "epoch:14 step:11433 [D loss: 0.438753, acc.: 82.81%] [G loss: 4.151172]\n",
      "epoch:14 step:11434 [D loss: 0.113126, acc.: 95.31%] [G loss: 4.552637]\n",
      "epoch:14 step:11435 [D loss: 0.036944, acc.: 100.00%] [G loss: 4.217794]\n",
      "epoch:14 step:11436 [D loss: 0.013378, acc.: 100.00%] [G loss: 3.876819]\n",
      "epoch:14 step:11437 [D loss: 0.027370, acc.: 100.00%] [G loss: 2.708888]\n",
      "epoch:14 step:11438 [D loss: 0.057423, acc.: 100.00%] [G loss: 3.541036]\n",
      "epoch:14 step:11439 [D loss: 0.284206, acc.: 87.50%] [G loss: 5.680760]\n",
      "epoch:14 step:11440 [D loss: 0.193292, acc.: 92.97%] [G loss: 4.396057]\n",
      "epoch:14 step:11441 [D loss: 0.068748, acc.: 99.22%] [G loss: 5.186071]\n",
      "epoch:14 step:11442 [D loss: 0.039126, acc.: 100.00%] [G loss: 3.916858]\n",
      "epoch:14 step:11443 [D loss: 0.136685, acc.: 95.31%] [G loss: 5.513398]\n",
      "epoch:14 step:11444 [D loss: 0.018730, acc.: 100.00%] [G loss: 5.930636]\n",
      "epoch:14 step:11445 [D loss: 0.207867, acc.: 92.97%] [G loss: 3.341545]\n",
      "epoch:14 step:11446 [D loss: 0.148313, acc.: 95.31%] [G loss: 4.744382]\n",
      "epoch:14 step:11447 [D loss: 0.057300, acc.: 97.66%] [G loss: 5.844184]\n",
      "epoch:14 step:11448 [D loss: 0.078391, acc.: 99.22%] [G loss: 3.548762]\n",
      "epoch:14 step:11449 [D loss: 0.089279, acc.: 98.44%] [G loss: 4.409941]\n",
      "epoch:14 step:11450 [D loss: 0.039945, acc.: 99.22%] [G loss: 4.008554]\n",
      "epoch:14 step:11451 [D loss: 0.027943, acc.: 100.00%] [G loss: 4.536111]\n",
      "epoch:14 step:11452 [D loss: 0.102397, acc.: 98.44%] [G loss: 3.719628]\n",
      "epoch:14 step:11453 [D loss: 0.204796, acc.: 96.09%] [G loss: 4.978795]\n",
      "epoch:14 step:11454 [D loss: 0.028839, acc.: 100.00%] [G loss: 4.817527]\n",
      "epoch:14 step:11455 [D loss: 0.193155, acc.: 94.53%] [G loss: 2.376610]\n",
      "epoch:14 step:11456 [D loss: 0.105844, acc.: 96.09%] [G loss: 5.357463]\n",
      "epoch:14 step:11457 [D loss: 0.159332, acc.: 94.53%] [G loss: 5.199651]\n",
      "epoch:14 step:11458 [D loss: 0.035269, acc.: 100.00%] [G loss: 4.281737]\n",
      "epoch:14 step:11459 [D loss: 0.016576, acc.: 100.00%] [G loss: 5.147134]\n",
      "epoch:14 step:11460 [D loss: 0.640405, acc.: 67.97%] [G loss: 6.494978]\n",
      "epoch:14 step:11461 [D loss: 0.454423, acc.: 73.44%] [G loss: 4.394764]\n",
      "epoch:14 step:11462 [D loss: 0.064440, acc.: 97.66%] [G loss: 3.339027]\n",
      "epoch:14 step:11463 [D loss: 0.141246, acc.: 92.97%] [G loss: 4.289306]\n",
      "epoch:14 step:11464 [D loss: 0.015405, acc.: 100.00%] [G loss: 5.669452]\n",
      "epoch:14 step:11465 [D loss: 0.046878, acc.: 99.22%] [G loss: 5.862041]\n",
      "epoch:14 step:11466 [D loss: 0.071339, acc.: 97.66%] [G loss: 3.154298]\n",
      "epoch:14 step:11467 [D loss: 0.105863, acc.: 97.66%] [G loss: 2.475287]\n",
      "epoch:14 step:11468 [D loss: 0.034235, acc.: 100.00%] [G loss: 2.640121]\n",
      "epoch:14 step:11469 [D loss: 0.464419, acc.: 82.03%] [G loss: 6.915696]\n",
      "epoch:14 step:11470 [D loss: 1.571022, acc.: 51.56%] [G loss: 0.867421]\n",
      "epoch:14 step:11471 [D loss: 0.579512, acc.: 73.44%] [G loss: 4.508155]\n",
      "epoch:14 step:11472 [D loss: 0.076914, acc.: 96.88%] [G loss: 6.316820]\n",
      "epoch:14 step:11473 [D loss: 0.428109, acc.: 81.25%] [G loss: 3.354107]\n",
      "epoch:14 step:11474 [D loss: 0.201419, acc.: 92.97%] [G loss: 2.056088]\n",
      "epoch:14 step:11475 [D loss: 0.140680, acc.: 95.31%] [G loss: 3.899029]\n",
      "epoch:14 step:11476 [D loss: 0.912254, acc.: 57.81%] [G loss: 4.693826]\n",
      "epoch:14 step:11477 [D loss: 0.382964, acc.: 82.03%] [G loss: 5.099467]\n",
      "epoch:14 step:11478 [D loss: 0.097533, acc.: 96.09%] [G loss: 5.604472]\n",
      "epoch:14 step:11479 [D loss: 0.009254, acc.: 100.00%] [G loss: 5.743686]\n",
      "epoch:14 step:11480 [D loss: 0.021968, acc.: 99.22%] [G loss: 5.079961]\n",
      "epoch:14 step:11481 [D loss: 0.062441, acc.: 98.44%] [G loss: 3.926701]\n",
      "epoch:14 step:11482 [D loss: 0.031133, acc.: 100.00%] [G loss: 3.910887]\n",
      "epoch:14 step:11483 [D loss: 0.021915, acc.: 100.00%] [G loss: 3.295843]\n",
      "epoch:14 step:11484 [D loss: 0.050003, acc.: 99.22%] [G loss: 2.616807]\n",
      "epoch:14 step:11485 [D loss: 0.079236, acc.: 98.44%] [G loss: 2.602784]\n",
      "epoch:14 step:11486 [D loss: 0.195159, acc.: 94.53%] [G loss: 3.647200]\n",
      "epoch:14 step:11487 [D loss: 0.067686, acc.: 97.66%] [G loss: 3.479442]\n",
      "epoch:14 step:11488 [D loss: 0.124302, acc.: 96.09%] [G loss: 3.633809]\n",
      "epoch:14 step:11489 [D loss: 0.066979, acc.: 99.22%] [G loss: 4.638855]\n",
      "epoch:14 step:11490 [D loss: 0.039340, acc.: 99.22%] [G loss: 4.411459]\n",
      "epoch:14 step:11491 [D loss: 0.083955, acc.: 98.44%] [G loss: 3.982833]\n",
      "epoch:14 step:11492 [D loss: 0.081370, acc.: 98.44%] [G loss: 4.006241]\n",
      "epoch:14 step:11493 [D loss: 0.034915, acc.: 100.00%] [G loss: 3.652630]\n",
      "epoch:14 step:11494 [D loss: 0.050100, acc.: 98.44%] [G loss: 4.531358]\n",
      "epoch:14 step:11495 [D loss: 0.035888, acc.: 100.00%] [G loss: 3.983180]\n",
      "epoch:14 step:11496 [D loss: 0.222684, acc.: 92.19%] [G loss: 2.170217]\n",
      "epoch:14 step:11497 [D loss: 0.102488, acc.: 96.09%] [G loss: 4.137424]\n",
      "epoch:14 step:11498 [D loss: 0.033485, acc.: 100.00%] [G loss: 4.269479]\n",
      "epoch:14 step:11499 [D loss: 0.178484, acc.: 92.97%] [G loss: 3.134836]\n",
      "epoch:14 step:11500 [D loss: 0.278276, acc.: 89.06%] [G loss: 5.572095]\n",
      "epoch:14 step:11501 [D loss: 0.078131, acc.: 97.66%] [G loss: 5.937912]\n",
      "epoch:14 step:11502 [D loss: 0.185373, acc.: 89.84%] [G loss: 2.022192]\n",
      "epoch:14 step:11503 [D loss: 0.157001, acc.: 94.53%] [G loss: 5.158104]\n",
      "epoch:14 step:11504 [D loss: 0.050575, acc.: 98.44%] [G loss: 5.465963]\n",
      "epoch:14 step:11505 [D loss: 0.080269, acc.: 99.22%] [G loss: 3.708310]\n",
      "epoch:14 step:11506 [D loss: 0.016379, acc.: 100.00%] [G loss: 2.869591]\n",
      "epoch:14 step:11507 [D loss: 0.050726, acc.: 98.44%] [G loss: 3.310445]\n",
      "epoch:14 step:11508 [D loss: 0.145780, acc.: 96.09%] [G loss: 3.126689]\n",
      "epoch:14 step:11509 [D loss: 0.121547, acc.: 97.66%] [G loss: 4.347767]\n",
      "epoch:14 step:11510 [D loss: 0.039851, acc.: 99.22%] [G loss: 5.235117]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:14 step:11511 [D loss: 0.649189, acc.: 65.62%] [G loss: 5.988241]\n",
      "epoch:14 step:11512 [D loss: 0.303364, acc.: 86.72%] [G loss: 4.843075]\n",
      "epoch:14 step:11513 [D loss: 0.025361, acc.: 99.22%] [G loss: 4.854072]\n",
      "epoch:14 step:11514 [D loss: 0.065501, acc.: 99.22%] [G loss: 3.481474]\n",
      "epoch:14 step:11515 [D loss: 0.038676, acc.: 100.00%] [G loss: 4.115047]\n",
      "epoch:14 step:11516 [D loss: 0.023443, acc.: 99.22%] [G loss: 4.204138]\n",
      "epoch:14 step:11517 [D loss: 0.022384, acc.: 100.00%] [G loss: 4.386895]\n",
      "epoch:14 step:11518 [D loss: 0.037676, acc.: 100.00%] [G loss: 4.281224]\n",
      "epoch:14 step:11519 [D loss: 0.013984, acc.: 100.00%] [G loss: 3.845436]\n",
      "epoch:14 step:11520 [D loss: 0.028562, acc.: 100.00%] [G loss: 3.878887]\n",
      "epoch:14 step:11521 [D loss: 0.104019, acc.: 97.66%] [G loss: 3.665125]\n",
      "epoch:14 step:11522 [D loss: 0.028513, acc.: 100.00%] [G loss: 3.542391]\n",
      "epoch:14 step:11523 [D loss: 0.063397, acc.: 99.22%] [G loss: 3.342445]\n",
      "epoch:14 step:11524 [D loss: 0.077138, acc.: 98.44%] [G loss: 2.114583]\n",
      "epoch:14 step:11525 [D loss: 0.074934, acc.: 97.66%] [G loss: 3.977759]\n",
      "epoch:14 step:11526 [D loss: 0.030476, acc.: 99.22%] [G loss: 3.850335]\n",
      "epoch:14 step:11527 [D loss: 0.893364, acc.: 53.91%] [G loss: 8.083635]\n",
      "epoch:14 step:11528 [D loss: 2.740852, acc.: 50.78%] [G loss: 5.658001]\n",
      "epoch:14 step:11529 [D loss: 1.089696, acc.: 58.59%] [G loss: 0.916789]\n",
      "epoch:14 step:11530 [D loss: 1.292630, acc.: 54.69%] [G loss: 5.093565]\n",
      "epoch:14 step:11531 [D loss: 0.338849, acc.: 82.81%] [G loss: 5.481690]\n",
      "epoch:14 step:11532 [D loss: 0.383985, acc.: 77.34%] [G loss: 3.353995]\n",
      "epoch:14 step:11533 [D loss: 0.205106, acc.: 92.19%] [G loss: 3.677369]\n",
      "epoch:14 step:11534 [D loss: 0.025933, acc.: 100.00%] [G loss: 4.340801]\n",
      "epoch:14 step:11535 [D loss: 0.171599, acc.: 93.75%] [G loss: 2.924892]\n",
      "epoch:14 step:11536 [D loss: 0.120657, acc.: 96.09%] [G loss: 3.546657]\n",
      "epoch:14 step:11537 [D loss: 0.054054, acc.: 99.22%] [G loss: 3.601161]\n",
      "epoch:14 step:11538 [D loss: 0.413865, acc.: 81.25%] [G loss: 2.541454]\n",
      "epoch:14 step:11539 [D loss: 0.039883, acc.: 100.00%] [G loss: 2.915042]\n",
      "epoch:14 step:11540 [D loss: 0.064043, acc.: 100.00%] [G loss: 2.778327]\n",
      "epoch:14 step:11541 [D loss: 0.032955, acc.: 100.00%] [G loss: 2.465069]\n",
      "epoch:14 step:11542 [D loss: 0.038910, acc.: 100.00%] [G loss: 2.368628]\n",
      "epoch:14 step:11543 [D loss: 0.161810, acc.: 96.09%] [G loss: 3.759814]\n",
      "epoch:14 step:11544 [D loss: 0.198726, acc.: 91.41%] [G loss: 2.012893]\n",
      "epoch:14 step:11545 [D loss: 0.110857, acc.: 96.09%] [G loss: 3.903643]\n",
      "epoch:14 step:11546 [D loss: 0.029252, acc.: 100.00%] [G loss: 3.700732]\n",
      "epoch:14 step:11547 [D loss: 2.557691, acc.: 7.81%] [G loss: 5.506159]\n",
      "epoch:14 step:11548 [D loss: 0.801509, acc.: 60.94%] [G loss: 4.579266]\n",
      "epoch:14 step:11549 [D loss: 0.332571, acc.: 83.59%] [G loss: 2.809821]\n",
      "epoch:14 step:11550 [D loss: 0.102796, acc.: 96.88%] [G loss: 2.266453]\n",
      "epoch:14 step:11551 [D loss: 0.112350, acc.: 96.88%] [G loss: 3.099451]\n",
      "epoch:14 step:11552 [D loss: 0.088946, acc.: 98.44%] [G loss: 2.993190]\n",
      "epoch:14 step:11553 [D loss: 0.094916, acc.: 98.44%] [G loss: 3.126829]\n",
      "epoch:14 step:11554 [D loss: 0.110544, acc.: 99.22%] [G loss: 3.588791]\n",
      "epoch:14 step:11555 [D loss: 0.035426, acc.: 100.00%] [G loss: 3.533482]\n",
      "epoch:14 step:11556 [D loss: 0.055332, acc.: 99.22%] [G loss: 3.862421]\n",
      "epoch:14 step:11557 [D loss: 0.078037, acc.: 98.44%] [G loss: 2.943994]\n",
      "epoch:14 step:11558 [D loss: 0.055419, acc.: 99.22%] [G loss: 3.457345]\n",
      "epoch:14 step:11559 [D loss: 0.063177, acc.: 100.00%] [G loss: 3.846944]\n",
      "epoch:14 step:11560 [D loss: 0.051685, acc.: 100.00%] [G loss: 2.862039]\n",
      "epoch:14 step:11561 [D loss: 0.214438, acc.: 95.31%] [G loss: 2.583405]\n",
      "epoch:14 step:11562 [D loss: 0.030162, acc.: 100.00%] [G loss: 3.011900]\n",
      "epoch:14 step:11563 [D loss: 0.134113, acc.: 96.88%] [G loss: 2.165800]\n",
      "epoch:14 step:11564 [D loss: 0.087871, acc.: 99.22%] [G loss: 2.179109]\n",
      "epoch:14 step:11565 [D loss: 0.102150, acc.: 96.88%] [G loss: 1.127857]\n",
      "epoch:14 step:11566 [D loss: 0.117625, acc.: 98.44%] [G loss: 2.065773]\n",
      "epoch:14 step:11567 [D loss: 0.097061, acc.: 99.22%] [G loss: 1.821408]\n",
      "epoch:14 step:11568 [D loss: 0.033933, acc.: 100.00%] [G loss: 2.181585]\n",
      "epoch:14 step:11569 [D loss: 0.282507, acc.: 87.50%] [G loss: 0.386408]\n",
      "epoch:14 step:11570 [D loss: 0.070943, acc.: 99.22%] [G loss: 1.522981]\n",
      "epoch:14 step:11571 [D loss: 0.034764, acc.: 99.22%] [G loss: 1.599500]\n",
      "epoch:14 step:11572 [D loss: 0.075654, acc.: 99.22%] [G loss: 3.191678]\n",
      "epoch:14 step:11573 [D loss: 0.032955, acc.: 100.00%] [G loss: 2.456966]\n",
      "epoch:14 step:11574 [D loss: 0.020457, acc.: 100.00%] [G loss: 0.945052]\n",
      "epoch:14 step:11575 [D loss: 0.129626, acc.: 96.88%] [G loss: 4.435793]\n",
      "epoch:14 step:11576 [D loss: 0.694223, acc.: 68.75%] [G loss: 2.509272]\n",
      "epoch:14 step:11577 [D loss: 0.035876, acc.: 100.00%] [G loss: 3.580137]\n",
      "epoch:14 step:11578 [D loss: 0.020489, acc.: 100.00%] [G loss: 3.610062]\n",
      "epoch:14 step:11579 [D loss: 0.051417, acc.: 98.44%] [G loss: 2.589588]\n",
      "epoch:14 step:11580 [D loss: 0.121572, acc.: 96.09%] [G loss: 3.895149]\n",
      "epoch:14 step:11581 [D loss: 0.092060, acc.: 97.66%] [G loss: 3.452153]\n",
      "epoch:14 step:11582 [D loss: 0.095559, acc.: 97.66%] [G loss: 3.093699]\n",
      "epoch:14 step:11583 [D loss: 0.193730, acc.: 93.75%] [G loss: 4.853288]\n",
      "epoch:14 step:11584 [D loss: 0.136040, acc.: 95.31%] [G loss: 4.325333]\n",
      "epoch:14 step:11585 [D loss: 0.017852, acc.: 100.00%] [G loss: 3.420667]\n",
      "epoch:14 step:11586 [D loss: 0.096658, acc.: 98.44%] [G loss: 3.619475]\n",
      "epoch:14 step:11587 [D loss: 0.046506, acc.: 100.00%] [G loss: 4.941837]\n",
      "epoch:14 step:11588 [D loss: 0.067688, acc.: 99.22%] [G loss: 3.230099]\n",
      "epoch:14 step:11589 [D loss: 0.042935, acc.: 100.00%] [G loss: 3.209525]\n",
      "epoch:14 step:11590 [D loss: 0.037508, acc.: 99.22%] [G loss: 3.360630]\n",
      "epoch:14 step:11591 [D loss: 0.037669, acc.: 100.00%] [G loss: 4.050914]\n",
      "epoch:14 step:11592 [D loss: 0.105389, acc.: 96.09%] [G loss: 3.706448]\n",
      "epoch:14 step:11593 [D loss: 0.106140, acc.: 98.44%] [G loss: 4.051685]\n",
      "epoch:14 step:11594 [D loss: 0.028022, acc.: 100.00%] [G loss: 4.664805]\n",
      "epoch:14 step:11595 [D loss: 0.162023, acc.: 95.31%] [G loss: 3.118307]\n",
      "epoch:14 step:11596 [D loss: 0.066191, acc.: 98.44%] [G loss: 3.248264]\n",
      "epoch:14 step:11597 [D loss: 0.034119, acc.: 100.00%] [G loss: 3.663428]\n",
      "epoch:14 step:11598 [D loss: 0.076988, acc.: 97.66%] [G loss: 3.335640]\n",
      "epoch:14 step:11599 [D loss: 0.147532, acc.: 95.31%] [G loss: 3.660948]\n",
      "epoch:14 step:11600 [D loss: 0.016443, acc.: 100.00%] [G loss: 4.122178]\n",
      "##############\n",
      "[0.96457637 0.87957958 0.90673409 0.88420028 0.81501336 0.99172761\n",
      " 1.012173   2.11982183 1.1102214  0.96413195]\n",
      "##########\n",
      "epoch:14 step:11601 [D loss: 0.025338, acc.: 99.22%] [G loss: 3.414814]\n",
      "epoch:14 step:11602 [D loss: 0.183394, acc.: 95.31%] [G loss: 2.663364]\n",
      "epoch:14 step:11603 [D loss: 0.035854, acc.: 100.00%] [G loss: 3.592203]\n",
      "epoch:14 step:11604 [D loss: 0.156550, acc.: 95.31%] [G loss: 3.553210]\n",
      "epoch:14 step:11605 [D loss: 0.015862, acc.: 100.00%] [G loss: 4.375750]\n",
      "epoch:14 step:11606 [D loss: 0.023540, acc.: 100.00%] [G loss: 5.084271]\n",
      "epoch:14 step:11607 [D loss: 0.043482, acc.: 100.00%] [G loss: 3.614881]\n",
      "epoch:14 step:11608 [D loss: 0.041268, acc.: 100.00%] [G loss: 3.185602]\n",
      "epoch:14 step:11609 [D loss: 0.016183, acc.: 100.00%] [G loss: 4.038994]\n",
      "epoch:14 step:11610 [D loss: 0.014119, acc.: 100.00%] [G loss: 4.752728]\n",
      "epoch:14 step:11611 [D loss: 0.101943, acc.: 97.66%] [G loss: 4.630392]\n",
      "epoch:14 step:11612 [D loss: 0.034513, acc.: 100.00%] [G loss: 4.011474]\n",
      "epoch:14 step:11613 [D loss: 0.010621, acc.: 100.00%] [G loss: 3.390466]\n",
      "epoch:14 step:11614 [D loss: 0.007425, acc.: 100.00%] [G loss: 3.081002]\n",
      "epoch:14 step:11615 [D loss: 0.032085, acc.: 99.22%] [G loss: 2.936061]\n",
      "epoch:14 step:11616 [D loss: 0.073656, acc.: 99.22%] [G loss: 2.244743]\n",
      "epoch:14 step:11617 [D loss: 0.113743, acc.: 98.44%] [G loss: 4.890310]\n",
      "epoch:14 step:11618 [D loss: 0.365678, acc.: 87.50%] [G loss: 1.918918]\n",
      "epoch:14 step:11619 [D loss: 1.753990, acc.: 47.66%] [G loss: 8.438015]\n",
      "epoch:14 step:11620 [D loss: 2.839380, acc.: 50.00%] [G loss: 5.928619]\n",
      "epoch:14 step:11621 [D loss: 2.022732, acc.: 50.00%] [G loss: 2.886667]\n",
      "epoch:14 step:11622 [D loss: 0.491808, acc.: 74.22%] [G loss: 1.884231]\n",
      "epoch:14 step:11623 [D loss: 0.420358, acc.: 84.38%] [G loss: 2.209135]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:14 step:11624 [D loss: 0.168300, acc.: 97.66%] [G loss: 2.372976]\n",
      "epoch:14 step:11625 [D loss: 0.112042, acc.: 100.00%] [G loss: 2.778377]\n",
      "epoch:14 step:11626 [D loss: 0.256250, acc.: 88.28%] [G loss: 1.950661]\n",
      "epoch:14 step:11627 [D loss: 0.476071, acc.: 78.12%] [G loss: 3.716293]\n",
      "epoch:14 step:11628 [D loss: 0.185969, acc.: 92.19%] [G loss: 3.788469]\n",
      "epoch:14 step:11629 [D loss: 0.247403, acc.: 91.41%] [G loss: 2.536154]\n",
      "epoch:14 step:11630 [D loss: 0.135573, acc.: 98.44%] [G loss: 2.430546]\n",
      "epoch:14 step:11631 [D loss: 0.106903, acc.: 99.22%] [G loss: 2.276260]\n",
      "epoch:14 step:11632 [D loss: 0.351675, acc.: 89.06%] [G loss: 3.188018]\n",
      "epoch:14 step:11633 [D loss: 0.055964, acc.: 100.00%] [G loss: 3.802687]\n",
      "epoch:14 step:11634 [D loss: 0.375529, acc.: 84.38%] [G loss: 2.708623]\n",
      "epoch:14 step:11635 [D loss: 0.301821, acc.: 87.50%] [G loss: 3.639400]\n",
      "epoch:14 step:11636 [D loss: 0.116539, acc.: 98.44%] [G loss: 4.316595]\n",
      "epoch:14 step:11637 [D loss: 0.324041, acc.: 84.38%] [G loss: 2.371291]\n",
      "epoch:14 step:11638 [D loss: 0.225870, acc.: 92.19%] [G loss: 3.716102]\n",
      "epoch:14 step:11639 [D loss: 0.044002, acc.: 99.22%] [G loss: 4.144724]\n",
      "epoch:14 step:11640 [D loss: 0.391976, acc.: 79.69%] [G loss: 2.385334]\n",
      "epoch:14 step:11641 [D loss: 0.124829, acc.: 97.66%] [G loss: 2.208397]\n",
      "epoch:14 step:11642 [D loss: 0.096990, acc.: 98.44%] [G loss: 2.939590]\n",
      "epoch:14 step:11643 [D loss: 0.054956, acc.: 100.00%] [G loss: 2.481116]\n",
      "epoch:14 step:11644 [D loss: 0.091843, acc.: 98.44%] [G loss: 2.719263]\n",
      "epoch:14 step:11645 [D loss: 0.175258, acc.: 95.31%] [G loss: 2.741926]\n",
      "epoch:14 step:11646 [D loss: 0.230341, acc.: 92.97%] [G loss: 1.902977]\n",
      "epoch:14 step:11647 [D loss: 0.132501, acc.: 98.44%] [G loss: 2.549158]\n",
      "epoch:14 step:11648 [D loss: 0.097210, acc.: 98.44%] [G loss: 2.868150]\n",
      "epoch:14 step:11649 [D loss: 0.231624, acc.: 92.97%] [G loss: 1.168420]\n",
      "epoch:14 step:11650 [D loss: 0.156509, acc.: 96.88%] [G loss: 2.255775]\n",
      "epoch:14 step:11651 [D loss: 0.061929, acc.: 100.00%] [G loss: 2.953881]\n",
      "epoch:14 step:11652 [D loss: 0.421513, acc.: 76.56%] [G loss: 4.185772]\n",
      "epoch:14 step:11653 [D loss: 0.250584, acc.: 89.06%] [G loss: 3.428324]\n",
      "epoch:14 step:11654 [D loss: 0.208871, acc.: 94.53%] [G loss: 2.561732]\n",
      "epoch:14 step:11655 [D loss: 0.172181, acc.: 94.53%] [G loss: 2.878433]\n",
      "epoch:14 step:11656 [D loss: 0.118564, acc.: 96.88%] [G loss: 2.016189]\n",
      "epoch:14 step:11657 [D loss: 0.084821, acc.: 99.22%] [G loss: 3.765047]\n",
      "epoch:14 step:11658 [D loss: 0.194152, acc.: 90.62%] [G loss: 2.268665]\n",
      "epoch:14 step:11659 [D loss: 0.296929, acc.: 89.06%] [G loss: 4.890836]\n",
      "epoch:14 step:11660 [D loss: 0.089281, acc.: 98.44%] [G loss: 5.515054]\n",
      "epoch:14 step:11661 [D loss: 1.339412, acc.: 35.94%] [G loss: 5.310970]\n",
      "epoch:14 step:11662 [D loss: 0.188060, acc.: 90.62%] [G loss: 4.484382]\n",
      "epoch:14 step:11663 [D loss: 0.089038, acc.: 98.44%] [G loss: 4.054325]\n",
      "epoch:14 step:11664 [D loss: 0.041064, acc.: 99.22%] [G loss: 3.784679]\n",
      "epoch:14 step:11665 [D loss: 0.031485, acc.: 100.00%] [G loss: 3.687154]\n",
      "epoch:14 step:11666 [D loss: 0.073025, acc.: 99.22%] [G loss: 3.157181]\n",
      "epoch:14 step:11667 [D loss: 0.063551, acc.: 98.44%] [G loss: 3.874369]\n",
      "epoch:14 step:11668 [D loss: 0.070865, acc.: 99.22%] [G loss: 2.642808]\n",
      "epoch:14 step:11669 [D loss: 0.064092, acc.: 99.22%] [G loss: 2.351489]\n",
      "epoch:14 step:11670 [D loss: 0.104119, acc.: 97.66%] [G loss: 3.445947]\n",
      "epoch:14 step:11671 [D loss: 0.077385, acc.: 98.44%] [G loss: 3.786808]\n",
      "epoch:14 step:11672 [D loss: 0.062555, acc.: 98.44%] [G loss: 2.462384]\n",
      "epoch:14 step:11673 [D loss: 0.078722, acc.: 98.44%] [G loss: 1.361039]\n",
      "epoch:14 step:11674 [D loss: 0.235414, acc.: 90.62%] [G loss: 2.606671]\n",
      "epoch:14 step:11675 [D loss: 0.077929, acc.: 97.66%] [G loss: 4.300670]\n",
      "epoch:14 step:11676 [D loss: 0.120916, acc.: 94.53%] [G loss: 3.156002]\n",
      "epoch:14 step:11677 [D loss: 0.625516, acc.: 67.97%] [G loss: 5.378024]\n",
      "epoch:14 step:11678 [D loss: 0.200160, acc.: 89.84%] [G loss: 5.044021]\n",
      "epoch:14 step:11679 [D loss: 0.091235, acc.: 96.88%] [G loss: 3.680661]\n",
      "epoch:14 step:11680 [D loss: 0.060223, acc.: 98.44%] [G loss: 2.364895]\n",
      "epoch:14 step:11681 [D loss: 0.092390, acc.: 98.44%] [G loss: 3.278065]\n",
      "epoch:14 step:11682 [D loss: 0.018629, acc.: 100.00%] [G loss: 3.756376]\n",
      "epoch:14 step:11683 [D loss: 0.065242, acc.: 97.66%] [G loss: 3.139043]\n",
      "epoch:14 step:11684 [D loss: 0.269681, acc.: 89.06%] [G loss: 4.281920]\n",
      "epoch:14 step:11685 [D loss: 0.264911, acc.: 87.50%] [G loss: 2.794020]\n",
      "epoch:14 step:11686 [D loss: 0.088061, acc.: 97.66%] [G loss: 4.228224]\n",
      "epoch:14 step:11687 [D loss: 0.041753, acc.: 100.00%] [G loss: 4.161758]\n",
      "epoch:14 step:11688 [D loss: 0.066938, acc.: 100.00%] [G loss: 3.568892]\n",
      "epoch:14 step:11689 [D loss: 0.056024, acc.: 100.00%] [G loss: 3.361418]\n",
      "epoch:14 step:11690 [D loss: 0.147228, acc.: 95.31%] [G loss: 4.151796]\n",
      "epoch:14 step:11691 [D loss: 0.024946, acc.: 100.00%] [G loss: 4.162522]\n",
      "epoch:14 step:11692 [D loss: 0.047382, acc.: 100.00%] [G loss: 3.777289]\n",
      "epoch:14 step:11693 [D loss: 0.119767, acc.: 96.09%] [G loss: 2.537631]\n",
      "epoch:14 step:11694 [D loss: 0.166954, acc.: 92.19%] [G loss: 4.921159]\n",
      "epoch:14 step:11695 [D loss: 0.044634, acc.: 97.66%] [G loss: 5.297264]\n",
      "epoch:14 step:11696 [D loss: 0.271276, acc.: 89.06%] [G loss: 2.726889]\n",
      "epoch:14 step:11697 [D loss: 0.219712, acc.: 91.41%] [G loss: 6.188077]\n",
      "epoch:14 step:11698 [D loss: 0.033830, acc.: 99.22%] [G loss: 6.747463]\n",
      "epoch:14 step:11699 [D loss: 0.250258, acc.: 87.50%] [G loss: 3.726582]\n",
      "epoch:14 step:11700 [D loss: 0.133595, acc.: 95.31%] [G loss: 4.869749]\n",
      "epoch:14 step:11701 [D loss: 0.013354, acc.: 100.00%] [G loss: 5.650140]\n",
      "epoch:14 step:11702 [D loss: 0.300752, acc.: 85.16%] [G loss: 4.973673]\n",
      "epoch:14 step:11703 [D loss: 0.010887, acc.: 100.00%] [G loss: 5.510735]\n",
      "epoch:14 step:11704 [D loss: 0.052867, acc.: 98.44%] [G loss: 4.458255]\n",
      "epoch:14 step:11705 [D loss: 0.069145, acc.: 100.00%] [G loss: 4.811860]\n",
      "epoch:14 step:11706 [D loss: 0.016947, acc.: 100.00%] [G loss: 5.117493]\n",
      "epoch:14 step:11707 [D loss: 0.030526, acc.: 100.00%] [G loss: 4.741544]\n",
      "epoch:14 step:11708 [D loss: 0.022656, acc.: 100.00%] [G loss: 5.443743]\n",
      "epoch:14 step:11709 [D loss: 0.027818, acc.: 100.00%] [G loss: 4.825397]\n",
      "epoch:14 step:11710 [D loss: 0.023459, acc.: 100.00%] [G loss: 3.354311]\n",
      "epoch:14 step:11711 [D loss: 0.174259, acc.: 92.19%] [G loss: 6.104615]\n",
      "epoch:14 step:11712 [D loss: 0.109046, acc.: 96.88%] [G loss: 5.506767]\n",
      "epoch:14 step:11713 [D loss: 0.071557, acc.: 98.44%] [G loss: 5.027189]\n",
      "epoch:14 step:11714 [D loss: 0.064438, acc.: 97.66%] [G loss: 5.563256]\n",
      "epoch:14 step:11715 [D loss: 0.003572, acc.: 100.00%] [G loss: 5.799345]\n",
      "epoch:15 step:11716 [D loss: 0.107048, acc.: 96.88%] [G loss: 2.758818]\n",
      "epoch:15 step:11717 [D loss: 0.078953, acc.: 97.66%] [G loss: 5.378571]\n",
      "epoch:15 step:11718 [D loss: 0.079852, acc.: 96.88%] [G loss: 4.392835]\n",
      "epoch:15 step:11719 [D loss: 0.018455, acc.: 100.00%] [G loss: 4.664084]\n",
      "epoch:15 step:11720 [D loss: 0.031455, acc.: 99.22%] [G loss: 5.424826]\n",
      "epoch:15 step:11721 [D loss: 0.065377, acc.: 99.22%] [G loss: 3.145031]\n",
      "epoch:15 step:11722 [D loss: 0.052806, acc.: 98.44%] [G loss: 1.743572]\n",
      "epoch:15 step:11723 [D loss: 0.533813, acc.: 72.66%] [G loss: 8.337600]\n",
      "epoch:15 step:11724 [D loss: 3.050586, acc.: 50.00%] [G loss: 4.813590]\n",
      "epoch:15 step:11725 [D loss: 0.858558, acc.: 63.28%] [G loss: 0.748625]\n",
      "epoch:15 step:11726 [D loss: 0.523233, acc.: 78.91%] [G loss: 3.774968]\n",
      "epoch:15 step:11727 [D loss: 0.548213, acc.: 73.44%] [G loss: 2.902719]\n",
      "epoch:15 step:11728 [D loss: 0.205836, acc.: 92.19%] [G loss: 3.001951]\n",
      "epoch:15 step:11729 [D loss: 0.112661, acc.: 97.66%] [G loss: 3.012915]\n",
      "epoch:15 step:11730 [D loss: 0.220719, acc.: 94.53%] [G loss: 2.436913]\n",
      "epoch:15 step:11731 [D loss: 0.302955, acc.: 86.72%] [G loss: 2.620182]\n",
      "epoch:15 step:11732 [D loss: 0.084499, acc.: 100.00%] [G loss: 2.391586]\n",
      "epoch:15 step:11733 [D loss: 0.031252, acc.: 100.00%] [G loss: 2.253713]\n",
      "epoch:15 step:11734 [D loss: 0.122416, acc.: 96.88%] [G loss: 1.916410]\n",
      "epoch:15 step:11735 [D loss: 0.174116, acc.: 95.31%] [G loss: 3.641122]\n",
      "epoch:15 step:11736 [D loss: 0.208570, acc.: 89.84%] [G loss: 2.679549]\n",
      "epoch:15 step:11737 [D loss: 0.191217, acc.: 92.19%] [G loss: 3.930487]\n",
      "epoch:15 step:11738 [D loss: 0.465218, acc.: 75.78%] [G loss: 1.895484]\n",
      "epoch:15 step:11739 [D loss: 0.336550, acc.: 83.59%] [G loss: 5.167194]\n",
      "epoch:15 step:11740 [D loss: 0.500014, acc.: 75.78%] [G loss: 4.351935]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:15 step:11741 [D loss: 0.061926, acc.: 100.00%] [G loss: 4.288280]\n",
      "epoch:15 step:11742 [D loss: 0.084538, acc.: 97.66%] [G loss: 4.508718]\n",
      "epoch:15 step:11743 [D loss: 0.031618, acc.: 100.00%] [G loss: 4.078597]\n",
      "epoch:15 step:11744 [D loss: 0.028329, acc.: 100.00%] [G loss: 3.917451]\n",
      "epoch:15 step:11745 [D loss: 0.040311, acc.: 99.22%] [G loss: 3.569756]\n",
      "epoch:15 step:11746 [D loss: 0.035118, acc.: 100.00%] [G loss: 3.468063]\n",
      "epoch:15 step:11747 [D loss: 0.049302, acc.: 100.00%] [G loss: 3.286850]\n",
      "epoch:15 step:11748 [D loss: 0.118793, acc.: 96.09%] [G loss: 3.074908]\n",
      "epoch:15 step:11749 [D loss: 0.021127, acc.: 100.00%] [G loss: 3.861867]\n",
      "epoch:15 step:11750 [D loss: 0.084434, acc.: 98.44%] [G loss: 3.727244]\n",
      "epoch:15 step:11751 [D loss: 0.541990, acc.: 77.34%] [G loss: 4.548470]\n",
      "epoch:15 step:11752 [D loss: 0.008993, acc.: 100.00%] [G loss: 4.137703]\n",
      "epoch:15 step:11753 [D loss: 0.212190, acc.: 90.62%] [G loss: 3.556131]\n",
      "epoch:15 step:11754 [D loss: 0.035753, acc.: 100.00%] [G loss: 2.857002]\n",
      "epoch:15 step:11755 [D loss: 0.052281, acc.: 98.44%] [G loss: 3.078363]\n",
      "epoch:15 step:11756 [D loss: 0.031709, acc.: 100.00%] [G loss: 3.936331]\n",
      "epoch:15 step:11757 [D loss: 0.041507, acc.: 100.00%] [G loss: 3.020920]\n",
      "epoch:15 step:11758 [D loss: 0.049989, acc.: 98.44%] [G loss: 4.388624]\n",
      "epoch:15 step:11759 [D loss: 0.188649, acc.: 94.53%] [G loss: 3.376129]\n",
      "epoch:15 step:11760 [D loss: 0.168287, acc.: 96.09%] [G loss: 3.738434]\n",
      "epoch:15 step:11761 [D loss: 0.064587, acc.: 99.22%] [G loss: 3.424013]\n",
      "epoch:15 step:11762 [D loss: 0.018971, acc.: 100.00%] [G loss: 3.175926]\n",
      "epoch:15 step:11763 [D loss: 0.086249, acc.: 98.44%] [G loss: 3.993081]\n",
      "epoch:15 step:11764 [D loss: 0.030027, acc.: 100.00%] [G loss: 4.054040]\n",
      "epoch:15 step:11765 [D loss: 0.200253, acc.: 93.75%] [G loss: 2.126773]\n",
      "epoch:15 step:11766 [D loss: 0.077019, acc.: 98.44%] [G loss: 4.179580]\n",
      "epoch:15 step:11767 [D loss: 0.272638, acc.: 89.06%] [G loss: 2.705981]\n",
      "epoch:15 step:11768 [D loss: 0.015740, acc.: 100.00%] [G loss: 2.497791]\n",
      "epoch:15 step:11769 [D loss: 0.256784, acc.: 89.84%] [G loss: 5.301265]\n",
      "epoch:15 step:11770 [D loss: 0.076267, acc.: 97.66%] [G loss: 5.162363]\n",
      "epoch:15 step:11771 [D loss: 0.024606, acc.: 100.00%] [G loss: 5.147520]\n",
      "epoch:15 step:11772 [D loss: 0.042290, acc.: 99.22%] [G loss: 3.894181]\n",
      "epoch:15 step:11773 [D loss: 0.021706, acc.: 100.00%] [G loss: 3.011994]\n",
      "epoch:15 step:11774 [D loss: 0.059890, acc.: 98.44%] [G loss: 4.579853]\n",
      "epoch:15 step:11775 [D loss: 0.010082, acc.: 100.00%] [G loss: 4.655762]\n",
      "epoch:15 step:11776 [D loss: 1.712692, acc.: 33.59%] [G loss: 6.923544]\n",
      "epoch:15 step:11777 [D loss: 1.697087, acc.: 51.56%] [G loss: 4.574997]\n",
      "epoch:15 step:11778 [D loss: 0.436877, acc.: 79.69%] [G loss: 1.837216]\n",
      "epoch:15 step:11779 [D loss: 0.161979, acc.: 95.31%] [G loss: 2.781940]\n",
      "epoch:15 step:11780 [D loss: 0.211979, acc.: 90.62%] [G loss: 3.633167]\n",
      "epoch:15 step:11781 [D loss: 0.170998, acc.: 92.97%] [G loss: 3.307994]\n",
      "epoch:15 step:11782 [D loss: 0.153801, acc.: 96.88%] [G loss: 3.024259]\n",
      "epoch:15 step:11783 [D loss: 0.278546, acc.: 91.41%] [G loss: 3.068032]\n",
      "epoch:15 step:11784 [D loss: 0.106827, acc.: 96.09%] [G loss: 3.395697]\n",
      "epoch:15 step:11785 [D loss: 0.148082, acc.: 95.31%] [G loss: 2.548428]\n",
      "epoch:15 step:11786 [D loss: 0.475335, acc.: 73.44%] [G loss: 3.575898]\n",
      "epoch:15 step:11787 [D loss: 0.050999, acc.: 99.22%] [G loss: 4.396399]\n",
      "epoch:15 step:11788 [D loss: 0.338174, acc.: 82.81%] [G loss: 2.414403]\n",
      "epoch:15 step:11789 [D loss: 0.073225, acc.: 99.22%] [G loss: 3.238210]\n",
      "epoch:15 step:11790 [D loss: 0.090718, acc.: 99.22%] [G loss: 3.068563]\n",
      "epoch:15 step:11791 [D loss: 0.285994, acc.: 87.50%] [G loss: 4.473614]\n",
      "epoch:15 step:11792 [D loss: 0.189953, acc.: 92.97%] [G loss: 2.965812]\n",
      "epoch:15 step:11793 [D loss: 0.173667, acc.: 92.97%] [G loss: 3.588345]\n",
      "epoch:15 step:11794 [D loss: 0.204922, acc.: 92.97%] [G loss: 1.794947]\n",
      "epoch:15 step:11795 [D loss: 0.074786, acc.: 99.22%] [G loss: 3.296496]\n",
      "epoch:15 step:11796 [D loss: 0.086281, acc.: 98.44%] [G loss: 3.821440]\n",
      "epoch:15 step:11797 [D loss: 0.096908, acc.: 99.22%] [G loss: 3.530888]\n",
      "epoch:15 step:11798 [D loss: 0.919391, acc.: 53.12%] [G loss: 6.259401]\n",
      "epoch:15 step:11799 [D loss: 1.318745, acc.: 51.56%] [G loss: 4.936798]\n",
      "epoch:15 step:11800 [D loss: 0.217804, acc.: 91.41%] [G loss: 2.321782]\n",
      "##############\n",
      "[0.94333213 0.94197739 1.1170876  1.03431604 0.91218318 0.96600192\n",
      " 1.09221592 0.80601139 2.10172809 2.11018443]\n",
      "##########\n",
      "epoch:15 step:11801 [D loss: 0.365997, acc.: 86.72%] [G loss: 4.514349]\n",
      "epoch:15 step:11802 [D loss: 0.170385, acc.: 91.41%] [G loss: 3.883397]\n",
      "epoch:15 step:11803 [D loss: 0.224563, acc.: 88.28%] [G loss: 2.309058]\n",
      "epoch:15 step:11804 [D loss: 0.157116, acc.: 95.31%] [G loss: 3.354506]\n",
      "epoch:15 step:11805 [D loss: 0.272628, acc.: 89.84%] [G loss: 3.351902]\n",
      "epoch:15 step:11806 [D loss: 0.052223, acc.: 100.00%] [G loss: 3.315024]\n",
      "epoch:15 step:11807 [D loss: 0.042258, acc.: 99.22%] [G loss: 3.245781]\n",
      "epoch:15 step:11808 [D loss: 0.201928, acc.: 92.97%] [G loss: 1.784219]\n",
      "epoch:15 step:11809 [D loss: 0.043343, acc.: 100.00%] [G loss: 2.233374]\n",
      "epoch:15 step:11810 [D loss: 0.250549, acc.: 90.62%] [G loss: 3.709244]\n",
      "epoch:15 step:11811 [D loss: 0.137183, acc.: 94.53%] [G loss: 4.097389]\n",
      "epoch:15 step:11812 [D loss: 0.227637, acc.: 91.41%] [G loss: 2.001326]\n",
      "epoch:15 step:11813 [D loss: 0.188212, acc.: 92.19%] [G loss: 2.714051]\n",
      "epoch:15 step:11814 [D loss: 0.057519, acc.: 98.44%] [G loss: 3.575719]\n",
      "epoch:15 step:11815 [D loss: 0.074697, acc.: 99.22%] [G loss: 2.231895]\n",
      "epoch:15 step:11816 [D loss: 0.048962, acc.: 98.44%] [G loss: 2.225751]\n",
      "epoch:15 step:11817 [D loss: 0.053433, acc.: 100.00%] [G loss: 1.777416]\n",
      "epoch:15 step:11818 [D loss: 0.165621, acc.: 94.53%] [G loss: 3.210174]\n",
      "epoch:15 step:11819 [D loss: 0.260866, acc.: 89.06%] [G loss: 1.923051]\n",
      "epoch:15 step:11820 [D loss: 0.070119, acc.: 100.00%] [G loss: 2.730362]\n",
      "epoch:15 step:11821 [D loss: 0.088352, acc.: 98.44%] [G loss: 3.716101]\n",
      "epoch:15 step:11822 [D loss: 0.159327, acc.: 93.75%] [G loss: 2.347845]\n",
      "epoch:15 step:11823 [D loss: 0.094306, acc.: 97.66%] [G loss: 2.415163]\n",
      "epoch:15 step:11824 [D loss: 0.022087, acc.: 100.00%] [G loss: 2.797653]\n",
      "epoch:15 step:11825 [D loss: 0.091608, acc.: 97.66%] [G loss: 4.211265]\n",
      "epoch:15 step:11826 [D loss: 0.074062, acc.: 99.22%] [G loss: 3.312597]\n",
      "epoch:15 step:11827 [D loss: 0.181439, acc.: 93.75%] [G loss: 3.856377]\n",
      "epoch:15 step:11828 [D loss: 0.026483, acc.: 100.00%] [G loss: 4.766103]\n",
      "epoch:15 step:11829 [D loss: 0.129746, acc.: 96.88%] [G loss: 3.547628]\n",
      "epoch:15 step:11830 [D loss: 0.025510, acc.: 100.00%] [G loss: 4.357121]\n",
      "epoch:15 step:11831 [D loss: 0.119746, acc.: 96.88%] [G loss: 2.422749]\n",
      "epoch:15 step:11832 [D loss: 0.015578, acc.: 100.00%] [G loss: 2.609652]\n",
      "epoch:15 step:11833 [D loss: 0.037947, acc.: 100.00%] [G loss: 3.555714]\n",
      "epoch:15 step:11834 [D loss: 0.030851, acc.: 99.22%] [G loss: 3.732229]\n",
      "epoch:15 step:11835 [D loss: 0.028324, acc.: 100.00%] [G loss: 2.898473]\n",
      "epoch:15 step:11836 [D loss: 0.085420, acc.: 97.66%] [G loss: 4.700247]\n",
      "epoch:15 step:11837 [D loss: 0.142254, acc.: 95.31%] [G loss: 2.578766]\n",
      "epoch:15 step:11838 [D loss: 0.280594, acc.: 85.94%] [G loss: 7.650780]\n",
      "epoch:15 step:11839 [D loss: 0.885545, acc.: 62.50%] [G loss: 3.113121]\n",
      "epoch:15 step:11840 [D loss: 0.135775, acc.: 94.53%] [G loss: 4.985787]\n",
      "epoch:15 step:11841 [D loss: 0.006865, acc.: 100.00%] [G loss: 5.921774]\n",
      "epoch:15 step:11842 [D loss: 0.008847, acc.: 100.00%] [G loss: 5.403508]\n",
      "epoch:15 step:11843 [D loss: 0.038951, acc.: 98.44%] [G loss: 4.454967]\n",
      "epoch:15 step:11844 [D loss: 0.013215, acc.: 99.22%] [G loss: 3.459074]\n",
      "epoch:15 step:11845 [D loss: 0.033055, acc.: 100.00%] [G loss: 3.619800]\n",
      "epoch:15 step:11846 [D loss: 0.009649, acc.: 100.00%] [G loss: 3.362599]\n",
      "epoch:15 step:11847 [D loss: 0.140451, acc.: 96.88%] [G loss: 2.913229]\n",
      "epoch:15 step:11848 [D loss: 0.002796, acc.: 100.00%] [G loss: 2.409594]\n",
      "epoch:15 step:11849 [D loss: 0.016124, acc.: 100.00%] [G loss: 2.282640]\n",
      "epoch:15 step:11850 [D loss: 0.010911, acc.: 100.00%] [G loss: 1.655050]\n",
      "epoch:15 step:11851 [D loss: 0.028841, acc.: 100.00%] [G loss: 1.813920]\n",
      "epoch:15 step:11852 [D loss: 0.027955, acc.: 100.00%] [G loss: 1.611652]\n",
      "epoch:15 step:11853 [D loss: 0.009048, acc.: 100.00%] [G loss: 1.140081]\n",
      "epoch:15 step:11854 [D loss: 0.020619, acc.: 100.00%] [G loss: 0.670466]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:15 step:11855 [D loss: 0.148960, acc.: 97.66%] [G loss: 3.706846]\n",
      "epoch:15 step:11856 [D loss: 0.005494, acc.: 100.00%] [G loss: 5.093960]\n",
      "epoch:15 step:11857 [D loss: 0.090476, acc.: 96.88%] [G loss: 2.919488]\n",
      "epoch:15 step:11858 [D loss: 0.408262, acc.: 81.25%] [G loss: 7.666696]\n",
      "epoch:15 step:11859 [D loss: 2.430610, acc.: 48.44%] [G loss: 1.922642]\n",
      "epoch:15 step:11860 [D loss: 0.157608, acc.: 94.53%] [G loss: 1.329621]\n",
      "epoch:15 step:11861 [D loss: 0.075064, acc.: 98.44%] [G loss: 3.080127]\n",
      "epoch:15 step:11862 [D loss: 0.097495, acc.: 98.44%] [G loss: 1.707429]\n",
      "epoch:15 step:11863 [D loss: 0.282934, acc.: 84.38%] [G loss: 2.750603]\n",
      "epoch:15 step:11864 [D loss: 0.039822, acc.: 100.00%] [G loss: 4.756038]\n",
      "epoch:15 step:11865 [D loss: 0.367675, acc.: 79.69%] [G loss: 2.981129]\n",
      "epoch:15 step:11866 [D loss: 0.040288, acc.: 100.00%] [G loss: 3.519525]\n",
      "epoch:15 step:11867 [D loss: 0.078584, acc.: 98.44%] [G loss: 4.184499]\n",
      "epoch:15 step:11868 [D loss: 0.062478, acc.: 100.00%] [G loss: 3.557252]\n",
      "epoch:15 step:11869 [D loss: 0.029227, acc.: 100.00%] [G loss: 3.256721]\n",
      "epoch:15 step:11870 [D loss: 0.021528, acc.: 100.00%] [G loss: 3.410634]\n",
      "epoch:15 step:11871 [D loss: 0.029712, acc.: 100.00%] [G loss: 3.612506]\n",
      "epoch:15 step:11872 [D loss: 0.142237, acc.: 96.09%] [G loss: 4.618045]\n",
      "epoch:15 step:11873 [D loss: 0.066698, acc.: 97.66%] [G loss: 4.107616]\n",
      "epoch:15 step:11874 [D loss: 0.041476, acc.: 98.44%] [G loss: 2.381649]\n",
      "epoch:15 step:11875 [D loss: 0.015024, acc.: 100.00%] [G loss: 1.683209]\n",
      "epoch:15 step:11876 [D loss: 0.063372, acc.: 97.66%] [G loss: 1.480285]\n",
      "epoch:15 step:11877 [D loss: 0.013886, acc.: 100.00%] [G loss: 1.311148]\n",
      "epoch:15 step:11878 [D loss: 0.106920, acc.: 95.31%] [G loss: 1.729828]\n",
      "epoch:15 step:11879 [D loss: 0.244759, acc.: 90.62%] [G loss: 1.914636]\n",
      "epoch:15 step:11880 [D loss: 0.015909, acc.: 100.00%] [G loss: 2.408536]\n",
      "epoch:15 step:11881 [D loss: 0.023596, acc.: 99.22%] [G loss: 2.141190]\n",
      "epoch:15 step:11882 [D loss: 0.041093, acc.: 99.22%] [G loss: 0.967768]\n",
      "epoch:15 step:11883 [D loss: 0.041572, acc.: 99.22%] [G loss: 2.310589]\n",
      "epoch:15 step:11884 [D loss: 0.008429, acc.: 100.00%] [G loss: 1.173867]\n",
      "epoch:15 step:11885 [D loss: 0.301580, acc.: 87.50%] [G loss: 0.569385]\n",
      "epoch:15 step:11886 [D loss: 0.055769, acc.: 98.44%] [G loss: 2.716454]\n",
      "epoch:15 step:11887 [D loss: 0.088293, acc.: 95.31%] [G loss: 1.890427]\n",
      "epoch:15 step:11888 [D loss: 0.022282, acc.: 100.00%] [G loss: 2.301134]\n",
      "epoch:15 step:11889 [D loss: 0.152395, acc.: 93.75%] [G loss: 5.938959]\n",
      "epoch:15 step:11890 [D loss: 0.363240, acc.: 85.16%] [G loss: 2.543838]\n",
      "epoch:15 step:11891 [D loss: 0.433820, acc.: 78.91%] [G loss: 6.549086]\n",
      "epoch:15 step:11892 [D loss: 0.522025, acc.: 71.88%] [G loss: 4.604216]\n",
      "epoch:15 step:11893 [D loss: 0.081373, acc.: 98.44%] [G loss: 3.050530]\n",
      "epoch:15 step:11894 [D loss: 0.014674, acc.: 100.00%] [G loss: 3.652304]\n",
      "epoch:15 step:11895 [D loss: 0.248567, acc.: 87.50%] [G loss: 5.895101]\n",
      "epoch:15 step:11896 [D loss: 0.238587, acc.: 87.50%] [G loss: 4.594772]\n",
      "epoch:15 step:11897 [D loss: 0.007947, acc.: 100.00%] [G loss: 3.784792]\n",
      "epoch:15 step:11898 [D loss: 0.021531, acc.: 99.22%] [G loss: 3.644457]\n",
      "epoch:15 step:11899 [D loss: 0.010346, acc.: 100.00%] [G loss: 2.863965]\n",
      "epoch:15 step:11900 [D loss: 0.018813, acc.: 100.00%] [G loss: 2.471698]\n",
      "epoch:15 step:11901 [D loss: 0.025379, acc.: 100.00%] [G loss: 1.671652]\n",
      "epoch:15 step:11902 [D loss: 0.178200, acc.: 95.31%] [G loss: 2.804283]\n",
      "epoch:15 step:11903 [D loss: 0.055191, acc.: 98.44%] [G loss: 3.727757]\n",
      "epoch:15 step:11904 [D loss: 0.102147, acc.: 96.88%] [G loss: 4.613802]\n",
      "epoch:15 step:11905 [D loss: 0.200452, acc.: 91.41%] [G loss: 2.245511]\n",
      "epoch:15 step:11906 [D loss: 1.217110, acc.: 55.47%] [G loss: 7.755257]\n",
      "epoch:15 step:11907 [D loss: 2.090909, acc.: 50.78%] [G loss: 5.189095]\n",
      "epoch:15 step:11908 [D loss: 1.209936, acc.: 57.03%] [G loss: 0.676779]\n",
      "epoch:15 step:11909 [D loss: 0.719357, acc.: 65.62%] [G loss: 5.090589]\n",
      "epoch:15 step:11910 [D loss: 0.292980, acc.: 84.38%] [G loss: 4.640834]\n",
      "epoch:15 step:11911 [D loss: 0.219930, acc.: 88.28%] [G loss: 3.510917]\n",
      "epoch:15 step:11912 [D loss: 0.060185, acc.: 100.00%] [G loss: 3.199388]\n",
      "epoch:15 step:11913 [D loss: 0.052382, acc.: 99.22%] [G loss: 3.206000]\n",
      "epoch:15 step:11914 [D loss: 0.152715, acc.: 96.09%] [G loss: 3.793434]\n",
      "epoch:15 step:11915 [D loss: 0.065988, acc.: 100.00%] [G loss: 3.538308]\n",
      "epoch:15 step:11916 [D loss: 0.161894, acc.: 94.53%] [G loss: 2.647148]\n",
      "epoch:15 step:11917 [D loss: 0.074838, acc.: 99.22%] [G loss: 3.554838]\n",
      "epoch:15 step:11918 [D loss: 0.020079, acc.: 100.00%] [G loss: 3.709849]\n",
      "epoch:15 step:11919 [D loss: 0.080233, acc.: 98.44%] [G loss: 3.892821]\n",
      "epoch:15 step:11920 [D loss: 0.221725, acc.: 89.84%] [G loss: 2.689782]\n",
      "epoch:15 step:11921 [D loss: 0.105190, acc.: 99.22%] [G loss: 2.951231]\n",
      "epoch:15 step:11922 [D loss: 0.058086, acc.: 100.00%] [G loss: 3.985466]\n",
      "epoch:15 step:11923 [D loss: 0.045031, acc.: 100.00%] [G loss: 4.098951]\n",
      "epoch:15 step:11924 [D loss: 0.129280, acc.: 94.53%] [G loss: 1.487337]\n",
      "epoch:15 step:11925 [D loss: 0.247683, acc.: 90.62%] [G loss: 4.195562]\n",
      "epoch:15 step:11926 [D loss: 0.069399, acc.: 97.66%] [G loss: 4.982832]\n",
      "epoch:15 step:11927 [D loss: 0.208329, acc.: 92.97%] [G loss: 2.672148]\n",
      "epoch:15 step:11928 [D loss: 0.079816, acc.: 97.66%] [G loss: 3.116355]\n",
      "epoch:15 step:11929 [D loss: 0.278219, acc.: 88.28%] [G loss: 1.850649]\n",
      "epoch:15 step:11930 [D loss: 0.062042, acc.: 99.22%] [G loss: 2.732965]\n",
      "epoch:15 step:11931 [D loss: 0.056694, acc.: 99.22%] [G loss: 4.213948]\n",
      "epoch:15 step:11932 [D loss: 0.073036, acc.: 99.22%] [G loss: 3.964173]\n",
      "epoch:15 step:11933 [D loss: 0.130498, acc.: 96.88%] [G loss: 2.102779]\n",
      "epoch:15 step:11934 [D loss: 0.057648, acc.: 99.22%] [G loss: 3.725593]\n",
      "epoch:15 step:11935 [D loss: 0.065987, acc.: 98.44%] [G loss: 3.091273]\n",
      "epoch:15 step:11936 [D loss: 0.158604, acc.: 96.09%] [G loss: 5.505957]\n",
      "epoch:15 step:11937 [D loss: 0.319399, acc.: 82.81%] [G loss: 2.996909]\n",
      "epoch:15 step:11938 [D loss: 0.096376, acc.: 97.66%] [G loss: 2.940696]\n",
      "epoch:15 step:11939 [D loss: 0.029327, acc.: 100.00%] [G loss: 3.797498]\n",
      "epoch:15 step:11940 [D loss: 0.010454, acc.: 100.00%] [G loss: 3.866587]\n",
      "epoch:15 step:11941 [D loss: 0.103800, acc.: 97.66%] [G loss: 4.277946]\n",
      "epoch:15 step:11942 [D loss: 0.122015, acc.: 96.88%] [G loss: 3.927204]\n",
      "epoch:15 step:11943 [D loss: 0.024847, acc.: 100.00%] [G loss: 3.653794]\n",
      "epoch:15 step:11944 [D loss: 0.075886, acc.: 100.00%] [G loss: 2.934432]\n",
      "epoch:15 step:11945 [D loss: 0.033223, acc.: 100.00%] [G loss: 4.044635]\n",
      "epoch:15 step:11946 [D loss: 0.050770, acc.: 98.44%] [G loss: 3.832648]\n",
      "epoch:15 step:11947 [D loss: 0.362555, acc.: 85.94%] [G loss: 5.479663]\n",
      "epoch:15 step:11948 [D loss: 0.018533, acc.: 99.22%] [G loss: 5.752690]\n",
      "epoch:15 step:11949 [D loss: 1.281241, acc.: 52.34%] [G loss: 1.485277]\n",
      "epoch:15 step:11950 [D loss: 1.321711, acc.: 54.69%] [G loss: 6.001542]\n",
      "epoch:15 step:11951 [D loss: 0.987473, acc.: 56.25%] [G loss: 5.234921]\n",
      "epoch:15 step:11952 [D loss: 0.043325, acc.: 99.22%] [G loss: 4.343306]\n",
      "epoch:15 step:11953 [D loss: 0.057256, acc.: 100.00%] [G loss: 4.115897]\n",
      "epoch:15 step:11954 [D loss: 0.019783, acc.: 100.00%] [G loss: 3.697519]\n",
      "epoch:15 step:11955 [D loss: 0.128661, acc.: 95.31%] [G loss: 3.569559]\n",
      "epoch:15 step:11956 [D loss: 0.051252, acc.: 100.00%] [G loss: 2.946390]\n",
      "epoch:15 step:11957 [D loss: 0.021323, acc.: 100.00%] [G loss: 3.286886]\n",
      "epoch:15 step:11958 [D loss: 0.040974, acc.: 100.00%] [G loss: 3.237264]\n",
      "epoch:15 step:11959 [D loss: 0.026387, acc.: 100.00%] [G loss: 3.137191]\n",
      "epoch:15 step:11960 [D loss: 0.069074, acc.: 99.22%] [G loss: 3.923503]\n",
      "epoch:15 step:11961 [D loss: 0.223809, acc.: 89.06%] [G loss: 4.181373]\n",
      "epoch:15 step:11962 [D loss: 0.159449, acc.: 93.75%] [G loss: 3.036102]\n",
      "epoch:15 step:11963 [D loss: 0.121643, acc.: 96.09%] [G loss: 4.654393]\n",
      "epoch:15 step:11964 [D loss: 0.029520, acc.: 100.00%] [G loss: 4.527203]\n",
      "epoch:15 step:11965 [D loss: 0.158498, acc.: 96.88%] [G loss: 3.208117]\n",
      "epoch:15 step:11966 [D loss: 0.057237, acc.: 99.22%] [G loss: 3.289598]\n",
      "epoch:15 step:11967 [D loss: 0.043927, acc.: 99.22%] [G loss: 3.617465]\n",
      "epoch:15 step:11968 [D loss: 0.021088, acc.: 100.00%] [G loss: 4.182042]\n",
      "epoch:15 step:11969 [D loss: 0.023496, acc.: 100.00%] [G loss: 4.207118]\n",
      "epoch:15 step:11970 [D loss: 0.074888, acc.: 100.00%] [G loss: 4.019818]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:15 step:11971 [D loss: 0.072025, acc.: 99.22%] [G loss: 3.724377]\n",
      "epoch:15 step:11972 [D loss: 0.036942, acc.: 100.00%] [G loss: 3.386096]\n",
      "epoch:15 step:11973 [D loss: 0.054631, acc.: 100.00%] [G loss: 3.696246]\n",
      "epoch:15 step:11974 [D loss: 0.066723, acc.: 99.22%] [G loss: 2.914735]\n",
      "epoch:15 step:11975 [D loss: 0.117186, acc.: 96.88%] [G loss: 3.028581]\n",
      "epoch:15 step:11976 [D loss: 0.152580, acc.: 95.31%] [G loss: 4.790065]\n",
      "epoch:15 step:11977 [D loss: 0.028235, acc.: 99.22%] [G loss: 4.797825]\n",
      "epoch:15 step:11978 [D loss: 0.069689, acc.: 97.66%] [G loss: 4.063418]\n",
      "epoch:15 step:11979 [D loss: 0.084223, acc.: 100.00%] [G loss: 1.867146]\n",
      "epoch:15 step:11980 [D loss: 0.102793, acc.: 97.66%] [G loss: 4.630709]\n",
      "epoch:15 step:11981 [D loss: 0.028378, acc.: 100.00%] [G loss: 4.335037]\n",
      "epoch:15 step:11982 [D loss: 0.154488, acc.: 93.75%] [G loss: 2.287828]\n",
      "epoch:15 step:11983 [D loss: 0.159255, acc.: 93.75%] [G loss: 5.399416]\n",
      "epoch:15 step:11984 [D loss: 0.094935, acc.: 96.88%] [G loss: 4.804185]\n",
      "epoch:15 step:11985 [D loss: 0.131200, acc.: 95.31%] [G loss: 2.007729]\n",
      "epoch:15 step:11986 [D loss: 0.080814, acc.: 98.44%] [G loss: 4.481443]\n",
      "epoch:15 step:11987 [D loss: 0.007338, acc.: 100.00%] [G loss: 5.558397]\n",
      "epoch:15 step:11988 [D loss: 0.048418, acc.: 100.00%] [G loss: 4.156632]\n",
      "epoch:15 step:11989 [D loss: 0.012122, acc.: 100.00%] [G loss: 2.051736]\n",
      "epoch:15 step:11990 [D loss: 0.181210, acc.: 92.19%] [G loss: 6.038963]\n",
      "epoch:15 step:11991 [D loss: 0.134632, acc.: 97.66%] [G loss: 4.729746]\n",
      "epoch:15 step:11992 [D loss: 0.646030, acc.: 68.75%] [G loss: 3.577885]\n",
      "epoch:15 step:11993 [D loss: 0.005320, acc.: 100.00%] [G loss: 5.338497]\n",
      "epoch:15 step:11994 [D loss: 0.045371, acc.: 99.22%] [G loss: 4.370128]\n",
      "epoch:15 step:11995 [D loss: 0.080314, acc.: 99.22%] [G loss: 4.075031]\n",
      "epoch:15 step:11996 [D loss: 0.036849, acc.: 99.22%] [G loss: 4.009883]\n",
      "epoch:15 step:11997 [D loss: 0.014190, acc.: 100.00%] [G loss: 3.093506]\n",
      "epoch:15 step:11998 [D loss: 0.076115, acc.: 99.22%] [G loss: 4.221167]\n",
      "epoch:15 step:11999 [D loss: 0.054124, acc.: 99.22%] [G loss: 2.942968]\n",
      "epoch:15 step:12000 [D loss: 0.039459, acc.: 100.00%] [G loss: 3.695543]\n",
      "##############\n",
      "[0.9610171  1.10212708 0.96998862 0.89934746 2.116373   0.94094164\n",
      " 1.11052561 0.86768154 2.11242156 1.0405581 ]\n",
      "##########\n",
      "epoch:15 step:12001 [D loss: 0.789529, acc.: 61.72%] [G loss: 7.459912]\n",
      "epoch:15 step:12002 [D loss: 1.655665, acc.: 50.00%] [G loss: 3.520000]\n",
      "epoch:15 step:12003 [D loss: 0.127134, acc.: 95.31%] [G loss: 4.275857]\n",
      "epoch:15 step:12004 [D loss: 0.025739, acc.: 100.00%] [G loss: 3.492707]\n",
      "epoch:15 step:12005 [D loss: 0.087162, acc.: 99.22%] [G loss: 3.310774]\n",
      "epoch:15 step:12006 [D loss: 1.078039, acc.: 53.12%] [G loss: 6.746851]\n",
      "epoch:15 step:12007 [D loss: 1.908520, acc.: 50.00%] [G loss: 4.227241]\n",
      "epoch:15 step:12008 [D loss: 0.842106, acc.: 59.38%] [G loss: 1.355979]\n",
      "epoch:15 step:12009 [D loss: 0.146580, acc.: 95.31%] [G loss: 2.404652]\n",
      "epoch:15 step:12010 [D loss: 0.127525, acc.: 97.66%] [G loss: 3.413753]\n",
      "epoch:15 step:12011 [D loss: 0.080982, acc.: 98.44%] [G loss: 3.444360]\n",
      "epoch:15 step:12012 [D loss: 0.061006, acc.: 98.44%] [G loss: 2.950320]\n",
      "epoch:15 step:12013 [D loss: 0.083724, acc.: 98.44%] [G loss: 2.977978]\n",
      "epoch:15 step:12014 [D loss: 0.107719, acc.: 96.88%] [G loss: 3.187587]\n",
      "epoch:15 step:12015 [D loss: 0.123345, acc.: 97.66%] [G loss: 3.754714]\n",
      "epoch:15 step:12016 [D loss: 0.148417, acc.: 96.09%] [G loss: 3.423888]\n",
      "epoch:15 step:12017 [D loss: 0.034756, acc.: 100.00%] [G loss: 3.176046]\n",
      "epoch:15 step:12018 [D loss: 0.148783, acc.: 97.66%] [G loss: 3.070459]\n",
      "epoch:15 step:12019 [D loss: 0.036472, acc.: 100.00%] [G loss: 3.376637]\n",
      "epoch:15 step:12020 [D loss: 0.036245, acc.: 100.00%] [G loss: 3.644874]\n",
      "epoch:15 step:12021 [D loss: 0.086880, acc.: 98.44%] [G loss: 3.401474]\n",
      "epoch:15 step:12022 [D loss: 0.027644, acc.: 100.00%] [G loss: 4.282816]\n",
      "epoch:15 step:12023 [D loss: 0.123002, acc.: 97.66%] [G loss: 2.951506]\n",
      "epoch:15 step:12024 [D loss: 0.188685, acc.: 93.75%] [G loss: 2.967883]\n",
      "epoch:15 step:12025 [D loss: 0.038329, acc.: 100.00%] [G loss: 3.583133]\n",
      "epoch:15 step:12026 [D loss: 0.022529, acc.: 100.00%] [G loss: 3.711781]\n",
      "epoch:15 step:12027 [D loss: 0.193016, acc.: 95.31%] [G loss: 3.364919]\n",
      "epoch:15 step:12028 [D loss: 0.026542, acc.: 100.00%] [G loss: 3.889446]\n",
      "epoch:15 step:12029 [D loss: 0.028458, acc.: 100.00%] [G loss: 4.220606]\n",
      "epoch:15 step:12030 [D loss: 0.243852, acc.: 89.06%] [G loss: 3.940591]\n",
      "epoch:15 step:12031 [D loss: 0.023894, acc.: 100.00%] [G loss: 5.041473]\n",
      "epoch:15 step:12032 [D loss: 0.020531, acc.: 100.00%] [G loss: 4.528494]\n",
      "epoch:15 step:12033 [D loss: 0.020974, acc.: 100.00%] [G loss: 4.534727]\n",
      "epoch:15 step:12034 [D loss: 0.013095, acc.: 100.00%] [G loss: 4.214431]\n",
      "epoch:15 step:12035 [D loss: 2.028550, acc.: 28.12%] [G loss: 6.187487]\n",
      "epoch:15 step:12036 [D loss: 2.151523, acc.: 50.00%] [G loss: 4.150158]\n",
      "epoch:15 step:12037 [D loss: 1.182818, acc.: 52.34%] [G loss: 2.038177]\n",
      "epoch:15 step:12038 [D loss: 0.816141, acc.: 53.91%] [G loss: 2.798102]\n",
      "epoch:15 step:12039 [D loss: 0.237715, acc.: 91.41%] [G loss: 2.798136]\n",
      "epoch:15 step:12040 [D loss: 0.260182, acc.: 90.62%] [G loss: 2.438862]\n",
      "epoch:15 step:12041 [D loss: 0.214906, acc.: 93.75%] [G loss: 2.030941]\n",
      "epoch:15 step:12042 [D loss: 0.124349, acc.: 98.44%] [G loss: 3.046726]\n",
      "epoch:15 step:12043 [D loss: 0.292343, acc.: 89.84%] [G loss: 2.538723]\n",
      "epoch:15 step:12044 [D loss: 0.306430, acc.: 87.50%] [G loss: 3.557499]\n",
      "epoch:15 step:12045 [D loss: 0.252017, acc.: 90.62%] [G loss: 3.126912]\n",
      "epoch:15 step:12046 [D loss: 0.126098, acc.: 98.44%] [G loss: 3.006692]\n",
      "epoch:15 step:12047 [D loss: 0.189332, acc.: 93.75%] [G loss: 2.253184]\n",
      "epoch:15 step:12048 [D loss: 0.194821, acc.: 96.09%] [G loss: 3.081553]\n",
      "epoch:15 step:12049 [D loss: 0.394863, acc.: 81.25%] [G loss: 3.119584]\n",
      "epoch:15 step:12050 [D loss: 0.064319, acc.: 100.00%] [G loss: 2.672601]\n",
      "epoch:15 step:12051 [D loss: 0.187911, acc.: 96.09%] [G loss: 2.901908]\n",
      "epoch:15 step:12052 [D loss: 0.343896, acc.: 84.38%] [G loss: 2.855808]\n",
      "epoch:15 step:12053 [D loss: 0.163894, acc.: 95.31%] [G loss: 2.582145]\n",
      "epoch:15 step:12054 [D loss: 0.195589, acc.: 90.62%] [G loss: 1.838879]\n",
      "epoch:15 step:12055 [D loss: 0.277918, acc.: 86.72%] [G loss: 3.083429]\n",
      "epoch:15 step:12056 [D loss: 0.169406, acc.: 94.53%] [G loss: 2.760143]\n",
      "epoch:15 step:12057 [D loss: 0.404310, acc.: 79.69%] [G loss: 2.444186]\n",
      "epoch:15 step:12058 [D loss: 0.079699, acc.: 100.00%] [G loss: 2.909653]\n",
      "epoch:15 step:12059 [D loss: 0.082915, acc.: 96.88%] [G loss: 2.911169]\n",
      "epoch:15 step:12060 [D loss: 0.111266, acc.: 98.44%] [G loss: 2.774817]\n",
      "epoch:15 step:12061 [D loss: 0.067926, acc.: 100.00%] [G loss: 2.497301]\n",
      "epoch:15 step:12062 [D loss: 0.120997, acc.: 97.66%] [G loss: 3.753521]\n",
      "epoch:15 step:12063 [D loss: 0.096706, acc.: 96.88%] [G loss: 2.911282]\n",
      "epoch:15 step:12064 [D loss: 0.611458, acc.: 71.88%] [G loss: 4.486743]\n",
      "epoch:15 step:12065 [D loss: 1.035013, acc.: 56.25%] [G loss: 3.536651]\n",
      "epoch:15 step:12066 [D loss: 0.351469, acc.: 81.25%] [G loss: 3.301370]\n",
      "epoch:15 step:12067 [D loss: 0.172035, acc.: 95.31%] [G loss: 3.585217]\n",
      "epoch:15 step:12068 [D loss: 0.122801, acc.: 96.09%] [G loss: 3.737826]\n",
      "epoch:15 step:12069 [D loss: 0.067477, acc.: 98.44%] [G loss: 3.565501]\n",
      "epoch:15 step:12070 [D loss: 0.221257, acc.: 90.62%] [G loss: 2.970740]\n",
      "epoch:15 step:12071 [D loss: 0.246058, acc.: 92.19%] [G loss: 3.691952]\n",
      "epoch:15 step:12072 [D loss: 0.082661, acc.: 98.44%] [G loss: 3.925007]\n",
      "epoch:15 step:12073 [D loss: 0.063585, acc.: 99.22%] [G loss: 3.442020]\n",
      "epoch:15 step:12074 [D loss: 0.217665, acc.: 92.19%] [G loss: 2.684705]\n",
      "epoch:15 step:12075 [D loss: 0.095444, acc.: 98.44%] [G loss: 3.035813]\n",
      "epoch:15 step:12076 [D loss: 0.070315, acc.: 98.44%] [G loss: 3.342619]\n",
      "epoch:15 step:12077 [D loss: 0.073975, acc.: 100.00%] [G loss: 3.459431]\n",
      "epoch:15 step:12078 [D loss: 0.037316, acc.: 100.00%] [G loss: 4.022195]\n",
      "epoch:15 step:12079 [D loss: 0.247045, acc.: 88.28%] [G loss: 2.546789]\n",
      "epoch:15 step:12080 [D loss: 0.070821, acc.: 98.44%] [G loss: 3.523622]\n",
      "epoch:15 step:12081 [D loss: 0.094649, acc.: 98.44%] [G loss: 3.122979]\n",
      "epoch:15 step:12082 [D loss: 0.276648, acc.: 89.06%] [G loss: 3.959138]\n",
      "epoch:15 step:12083 [D loss: 0.078909, acc.: 97.66%] [G loss: 3.781176]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:15 step:12084 [D loss: 0.078231, acc.: 98.44%] [G loss: 2.265439]\n",
      "epoch:15 step:12085 [D loss: 0.088056, acc.: 99.22%] [G loss: 2.703422]\n",
      "epoch:15 step:12086 [D loss: 0.089374, acc.: 98.44%] [G loss: 2.749444]\n",
      "epoch:15 step:12087 [D loss: 0.057640, acc.: 100.00%] [G loss: 2.756480]\n",
      "epoch:15 step:12088 [D loss: 2.519722, acc.: 23.44%] [G loss: 5.413710]\n",
      "epoch:15 step:12089 [D loss: 0.500013, acc.: 76.56%] [G loss: 5.291706]\n",
      "epoch:15 step:12090 [D loss: 0.292620, acc.: 85.94%] [G loss: 3.733789]\n",
      "epoch:15 step:12091 [D loss: 0.046421, acc.: 99.22%] [G loss: 3.131094]\n",
      "epoch:15 step:12092 [D loss: 0.044248, acc.: 99.22%] [G loss: 3.597484]\n",
      "epoch:15 step:12093 [D loss: 0.039196, acc.: 100.00%] [G loss: 3.025642]\n",
      "epoch:15 step:12094 [D loss: 0.089086, acc.: 98.44%] [G loss: 3.415933]\n",
      "epoch:15 step:12095 [D loss: 0.020333, acc.: 100.00%] [G loss: 3.162361]\n",
      "epoch:15 step:12096 [D loss: 0.079363, acc.: 99.22%] [G loss: 2.717433]\n",
      "epoch:15 step:12097 [D loss: 0.110450, acc.: 98.44%] [G loss: 2.945667]\n",
      "epoch:15 step:12098 [D loss: 0.072714, acc.: 100.00%] [G loss: 3.562559]\n",
      "epoch:15 step:12099 [D loss: 0.163527, acc.: 94.53%] [G loss: 3.384802]\n",
      "epoch:15 step:12100 [D loss: 0.505267, acc.: 73.44%] [G loss: 2.397834]\n",
      "epoch:15 step:12101 [D loss: 0.141704, acc.: 96.09%] [G loss: 3.798008]\n",
      "epoch:15 step:12102 [D loss: 0.028100, acc.: 100.00%] [G loss: 4.314829]\n",
      "epoch:15 step:12103 [D loss: 0.051058, acc.: 99.22%] [G loss: 3.941489]\n",
      "epoch:15 step:12104 [D loss: 0.062352, acc.: 98.44%] [G loss: 3.684365]\n",
      "epoch:15 step:12105 [D loss: 0.060317, acc.: 98.44%] [G loss: 3.751712]\n",
      "epoch:15 step:12106 [D loss: 0.091785, acc.: 97.66%] [G loss: 3.348469]\n",
      "epoch:15 step:12107 [D loss: 0.045226, acc.: 100.00%] [G loss: 3.551424]\n",
      "epoch:15 step:12108 [D loss: 0.083904, acc.: 98.44%] [G loss: 4.000319]\n",
      "epoch:15 step:12109 [D loss: 0.029705, acc.: 100.00%] [G loss: 3.877423]\n",
      "epoch:15 step:12110 [D loss: 0.078075, acc.: 100.00%] [G loss: 3.667222]\n",
      "epoch:15 step:12111 [D loss: 0.119927, acc.: 96.88%] [G loss: 3.311290]\n",
      "epoch:15 step:12112 [D loss: 0.096392, acc.: 97.66%] [G loss: 3.246473]\n",
      "epoch:15 step:12113 [D loss: 0.054594, acc.: 98.44%] [G loss: 4.086398]\n",
      "epoch:15 step:12114 [D loss: 0.022488, acc.: 100.00%] [G loss: 3.726176]\n",
      "epoch:15 step:12115 [D loss: 0.041878, acc.: 100.00%] [G loss: 3.257738]\n",
      "epoch:15 step:12116 [D loss: 0.023403, acc.: 100.00%] [G loss: 3.485399]\n",
      "epoch:15 step:12117 [D loss: 0.073530, acc.: 98.44%] [G loss: 3.216412]\n",
      "epoch:15 step:12118 [D loss: 0.142417, acc.: 96.09%] [G loss: 3.373224]\n",
      "epoch:15 step:12119 [D loss: 0.042499, acc.: 99.22%] [G loss: 3.309232]\n",
      "epoch:15 step:12120 [D loss: 0.572513, acc.: 67.97%] [G loss: 4.759482]\n",
      "epoch:15 step:12121 [D loss: 0.166747, acc.: 91.41%] [G loss: 3.949793]\n",
      "epoch:15 step:12122 [D loss: 0.026483, acc.: 100.00%] [G loss: 4.444027]\n",
      "epoch:15 step:12123 [D loss: 0.029205, acc.: 100.00%] [G loss: 3.770047]\n",
      "epoch:15 step:12124 [D loss: 0.093010, acc.: 99.22%] [G loss: 2.783106]\n",
      "epoch:15 step:12125 [D loss: 0.078175, acc.: 96.88%] [G loss: 3.646972]\n",
      "epoch:15 step:12126 [D loss: 0.072428, acc.: 97.66%] [G loss: 3.524511]\n",
      "epoch:15 step:12127 [D loss: 0.020469, acc.: 99.22%] [G loss: 2.915060]\n",
      "epoch:15 step:12128 [D loss: 0.059805, acc.: 100.00%] [G loss: 2.160239]\n",
      "epoch:15 step:12129 [D loss: 0.024951, acc.: 100.00%] [G loss: 4.079379]\n",
      "epoch:15 step:12130 [D loss: 0.025317, acc.: 100.00%] [G loss: 2.203445]\n",
      "epoch:15 step:12131 [D loss: 0.221183, acc.: 92.19%] [G loss: 5.863950]\n",
      "epoch:15 step:12132 [D loss: 0.839048, acc.: 62.50%] [G loss: 0.591092]\n",
      "epoch:15 step:12133 [D loss: 0.747524, acc.: 66.41%] [G loss: 6.215033]\n",
      "epoch:15 step:12134 [D loss: 0.880677, acc.: 59.38%] [G loss: 4.529211]\n",
      "epoch:15 step:12135 [D loss: 0.539562, acc.: 73.44%] [G loss: 0.641151]\n",
      "epoch:15 step:12136 [D loss: 1.244670, acc.: 60.16%] [G loss: 5.291334]\n",
      "epoch:15 step:12137 [D loss: 0.381422, acc.: 76.56%] [G loss: 5.548689]\n",
      "epoch:15 step:12138 [D loss: 0.173363, acc.: 91.41%] [G loss: 5.102097]\n",
      "epoch:15 step:12139 [D loss: 0.019968, acc.: 100.00%] [G loss: 4.523626]\n",
      "epoch:15 step:12140 [D loss: 0.027279, acc.: 100.00%] [G loss: 4.025637]\n",
      "epoch:15 step:12141 [D loss: 0.018327, acc.: 100.00%] [G loss: 3.790427]\n",
      "epoch:15 step:12142 [D loss: 0.010625, acc.: 100.00%] [G loss: 4.488296]\n",
      "epoch:15 step:12143 [D loss: 0.029817, acc.: 99.22%] [G loss: 2.916892]\n",
      "epoch:15 step:12144 [D loss: 0.033119, acc.: 100.00%] [G loss: 3.491606]\n",
      "epoch:15 step:12145 [D loss: 0.030919, acc.: 100.00%] [G loss: 3.616219]\n",
      "epoch:15 step:12146 [D loss: 0.115593, acc.: 97.66%] [G loss: 2.706159]\n",
      "epoch:15 step:12147 [D loss: 0.097395, acc.: 98.44%] [G loss: 2.808366]\n",
      "epoch:15 step:12148 [D loss: 0.032734, acc.: 100.00%] [G loss: 3.891305]\n",
      "epoch:15 step:12149 [D loss: 0.036416, acc.: 100.00%] [G loss: 3.179754]\n",
      "epoch:15 step:12150 [D loss: 0.058733, acc.: 100.00%] [G loss: 3.513263]\n",
      "epoch:15 step:12151 [D loss: 0.108959, acc.: 96.88%] [G loss: 3.013221]\n",
      "epoch:15 step:12152 [D loss: 0.053910, acc.: 100.00%] [G loss: 4.177488]\n",
      "epoch:15 step:12153 [D loss: 0.039021, acc.: 100.00%] [G loss: 3.619335]\n",
      "epoch:15 step:12154 [D loss: 0.175705, acc.: 95.31%] [G loss: 3.818642]\n",
      "epoch:15 step:12155 [D loss: 0.018582, acc.: 100.00%] [G loss: 4.046037]\n",
      "epoch:15 step:12156 [D loss: 0.108298, acc.: 97.66%] [G loss: 2.832243]\n",
      "epoch:15 step:12157 [D loss: 0.072814, acc.: 100.00%] [G loss: 2.733255]\n",
      "epoch:15 step:12158 [D loss: 0.046007, acc.: 100.00%] [G loss: 2.758720]\n",
      "epoch:15 step:12159 [D loss: 0.068549, acc.: 99.22%] [G loss: 3.008401]\n",
      "epoch:15 step:12160 [D loss: 0.066089, acc.: 100.00%] [G loss: 3.550097]\n",
      "epoch:15 step:12161 [D loss: 0.064336, acc.: 100.00%] [G loss: 3.157632]\n",
      "epoch:15 step:12162 [D loss: 0.161344, acc.: 96.88%] [G loss: 4.625228]\n",
      "epoch:15 step:12163 [D loss: 0.178182, acc.: 92.97%] [G loss: 2.998065]\n",
      "epoch:15 step:12164 [D loss: 0.036280, acc.: 100.00%] [G loss: 2.050940]\n",
      "epoch:15 step:12165 [D loss: 0.135526, acc.: 95.31%] [G loss: 4.929465]\n",
      "epoch:15 step:12166 [D loss: 0.204373, acc.: 89.84%] [G loss: 3.212036]\n",
      "epoch:15 step:12167 [D loss: 0.125661, acc.: 98.44%] [G loss: 3.494645]\n",
      "epoch:15 step:12168 [D loss: 0.038495, acc.: 99.22%] [G loss: 4.230941]\n",
      "epoch:15 step:12169 [D loss: 0.022255, acc.: 100.00%] [G loss: 4.056519]\n",
      "epoch:15 step:12170 [D loss: 0.060624, acc.: 99.22%] [G loss: 4.203346]\n",
      "epoch:15 step:12171 [D loss: 0.128708, acc.: 98.44%] [G loss: 3.374392]\n",
      "epoch:15 step:12172 [D loss: 0.032446, acc.: 100.00%] [G loss: 4.559420]\n",
      "epoch:15 step:12173 [D loss: 0.046424, acc.: 100.00%] [G loss: 4.109326]\n",
      "epoch:15 step:12174 [D loss: 0.561830, acc.: 67.19%] [G loss: 6.840151]\n",
      "epoch:15 step:12175 [D loss: 0.826893, acc.: 59.38%] [G loss: 3.737602]\n",
      "epoch:15 step:12176 [D loss: 0.424562, acc.: 81.25%] [G loss: 5.267011]\n",
      "epoch:15 step:12177 [D loss: 0.124138, acc.: 95.31%] [G loss: 6.230236]\n",
      "epoch:15 step:12178 [D loss: 0.278158, acc.: 85.94%] [G loss: 4.707672]\n",
      "epoch:15 step:12179 [D loss: 0.055612, acc.: 96.88%] [G loss: 3.459754]\n",
      "epoch:15 step:12180 [D loss: 0.035206, acc.: 99.22%] [G loss: 3.883843]\n",
      "epoch:15 step:12181 [D loss: 0.034961, acc.: 100.00%] [G loss: 3.898100]\n",
      "epoch:15 step:12182 [D loss: 0.053383, acc.: 99.22%] [G loss: 3.176747]\n",
      "epoch:15 step:12183 [D loss: 0.113889, acc.: 97.66%] [G loss: 2.317920]\n",
      "epoch:15 step:12184 [D loss: 0.076832, acc.: 98.44%] [G loss: 4.432110]\n",
      "epoch:15 step:12185 [D loss: 0.012563, acc.: 100.00%] [G loss: 3.310097]\n",
      "epoch:15 step:12186 [D loss: 0.251283, acc.: 90.62%] [G loss: 3.577073]\n",
      "epoch:15 step:12187 [D loss: 0.013508, acc.: 100.00%] [G loss: 4.207313]\n",
      "epoch:15 step:12188 [D loss: 0.023105, acc.: 100.00%] [G loss: 4.163981]\n",
      "epoch:15 step:12189 [D loss: 0.046592, acc.: 100.00%] [G loss: 4.565173]\n",
      "epoch:15 step:12190 [D loss: 0.040560, acc.: 100.00%] [G loss: 3.208773]\n",
      "epoch:15 step:12191 [D loss: 0.021948, acc.: 100.00%] [G loss: 3.961936]\n",
      "epoch:15 step:12192 [D loss: 0.078784, acc.: 98.44%] [G loss: 4.861261]\n",
      "epoch:15 step:12193 [D loss: 0.124912, acc.: 95.31%] [G loss: 4.122420]\n",
      "epoch:15 step:12194 [D loss: 0.119384, acc.: 95.31%] [G loss: 4.711064]\n",
      "epoch:15 step:12195 [D loss: 0.006820, acc.: 100.00%] [G loss: 5.728069]\n",
      "epoch:15 step:12196 [D loss: 0.043800, acc.: 100.00%] [G loss: 4.945055]\n",
      "epoch:15 step:12197 [D loss: 0.039557, acc.: 99.22%] [G loss: 4.301216]\n",
      "epoch:15 step:12198 [D loss: 0.044794, acc.: 99.22%] [G loss: 4.445215]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:15 step:12199 [D loss: 0.021127, acc.: 100.00%] [G loss: 4.813447]\n",
      "epoch:15 step:12200 [D loss: 0.154864, acc.: 93.75%] [G loss: 3.638034]\n",
      "##############\n",
      "[1.01451238 2.11351963 2.11427927 0.96285288 0.86628536 0.88056778\n",
      " 2.10323968 0.87268434 2.12152163 2.11786497]\n",
      "##########\n",
      "epoch:15 step:12201 [D loss: 0.015800, acc.: 100.00%] [G loss: 4.017229]\n",
      "epoch:15 step:12202 [D loss: 0.015569, acc.: 100.00%] [G loss: 4.841237]\n",
      "epoch:15 step:12203 [D loss: 0.017562, acc.: 100.00%] [G loss: 3.446229]\n",
      "epoch:15 step:12204 [D loss: 0.035352, acc.: 99.22%] [G loss: 4.719934]\n",
      "epoch:15 step:12205 [D loss: 0.018603, acc.: 100.00%] [G loss: 4.268716]\n",
      "epoch:15 step:12206 [D loss: 0.070992, acc.: 98.44%] [G loss: 4.107347]\n",
      "epoch:15 step:12207 [D loss: 0.472037, acc.: 77.34%] [G loss: 6.941422]\n",
      "epoch:15 step:12208 [D loss: 0.286086, acc.: 89.06%] [G loss: 6.118420]\n",
      "epoch:15 step:12209 [D loss: 0.041920, acc.: 99.22%] [G loss: 4.940570]\n",
      "epoch:15 step:12210 [D loss: 0.037544, acc.: 99.22%] [G loss: 4.555605]\n",
      "epoch:15 step:12211 [D loss: 0.028013, acc.: 100.00%] [G loss: 4.879838]\n",
      "epoch:15 step:12212 [D loss: 0.043591, acc.: 100.00%] [G loss: 5.406818]\n",
      "epoch:15 step:12213 [D loss: 0.036424, acc.: 100.00%] [G loss: 5.247079]\n",
      "epoch:15 step:12214 [D loss: 0.044210, acc.: 99.22%] [G loss: 4.213154]\n",
      "epoch:15 step:12215 [D loss: 0.040281, acc.: 100.00%] [G loss: 5.437245]\n",
      "epoch:15 step:12216 [D loss: 1.732491, acc.: 35.94%] [G loss: 7.620512]\n",
      "epoch:15 step:12217 [D loss: 0.650402, acc.: 68.75%] [G loss: 6.736776]\n",
      "epoch:15 step:12218 [D loss: 0.017034, acc.: 100.00%] [G loss: 5.811955]\n",
      "epoch:15 step:12219 [D loss: 0.003762, acc.: 100.00%] [G loss: 5.141960]\n",
      "epoch:15 step:12220 [D loss: 0.017817, acc.: 100.00%] [G loss: 4.867194]\n",
      "epoch:15 step:12221 [D loss: 0.011039, acc.: 100.00%] [G loss: 4.055936]\n",
      "epoch:15 step:12222 [D loss: 0.053214, acc.: 98.44%] [G loss: 4.466349]\n",
      "epoch:15 step:12223 [D loss: 0.099038, acc.: 99.22%] [G loss: 4.038634]\n",
      "epoch:15 step:12224 [D loss: 0.031240, acc.: 100.00%] [G loss: 4.302871]\n",
      "epoch:15 step:12225 [D loss: 0.061003, acc.: 97.66%] [G loss: 4.758590]\n",
      "epoch:15 step:12226 [D loss: 0.089251, acc.: 96.88%] [G loss: 5.270224]\n",
      "epoch:15 step:12227 [D loss: 0.286993, acc.: 87.50%] [G loss: 5.105567]\n",
      "epoch:15 step:12228 [D loss: 0.020746, acc.: 100.00%] [G loss: 5.494352]\n",
      "epoch:15 step:12229 [D loss: 0.038564, acc.: 98.44%] [G loss: 4.800609]\n",
      "epoch:15 step:12230 [D loss: 0.058467, acc.: 98.44%] [G loss: 3.780483]\n",
      "epoch:15 step:12231 [D loss: 0.040318, acc.: 100.00%] [G loss: 4.463560]\n",
      "epoch:15 step:12232 [D loss: 0.036221, acc.: 100.00%] [G loss: 5.033414]\n",
      "epoch:15 step:12233 [D loss: 0.016286, acc.: 100.00%] [G loss: 5.230135]\n",
      "epoch:15 step:12234 [D loss: 0.384786, acc.: 80.47%] [G loss: 4.888471]\n",
      "epoch:15 step:12235 [D loss: 0.006143, acc.: 100.00%] [G loss: 5.486749]\n",
      "epoch:15 step:12236 [D loss: 0.015891, acc.: 100.00%] [G loss: 4.693810]\n",
      "epoch:15 step:12237 [D loss: 0.050613, acc.: 96.88%] [G loss: 2.422157]\n",
      "epoch:15 step:12238 [D loss: 0.017357, acc.: 100.00%] [G loss: 1.588315]\n",
      "epoch:15 step:12239 [D loss: 0.028465, acc.: 100.00%] [G loss: 0.530147]\n",
      "epoch:15 step:12240 [D loss: 0.035634, acc.: 100.00%] [G loss: 0.259150]\n",
      "epoch:15 step:12241 [D loss: 0.089946, acc.: 100.00%] [G loss: 1.256218]\n",
      "epoch:15 step:12242 [D loss: 0.009084, acc.: 100.00%] [G loss: 2.644279]\n",
      "epoch:15 step:12243 [D loss: 0.195645, acc.: 92.19%] [G loss: 1.190347]\n",
      "epoch:15 step:12244 [D loss: 0.003857, acc.: 100.00%] [G loss: 2.540461]\n",
      "epoch:15 step:12245 [D loss: 0.003254, acc.: 100.00%] [G loss: 0.884179]\n",
      "epoch:15 step:12246 [D loss: 0.006807, acc.: 100.00%] [G loss: 0.533557]\n",
      "epoch:15 step:12247 [D loss: 0.199348, acc.: 91.41%] [G loss: 1.870818]\n",
      "epoch:15 step:12248 [D loss: 1.259851, acc.: 46.09%] [G loss: 6.088233]\n",
      "epoch:15 step:12249 [D loss: 1.037433, acc.: 58.59%] [G loss: 2.560378]\n",
      "epoch:15 step:12250 [D loss: 0.137387, acc.: 92.97%] [G loss: 3.241768]\n",
      "epoch:15 step:12251 [D loss: 0.075415, acc.: 99.22%] [G loss: 3.770380]\n",
      "epoch:15 step:12252 [D loss: 0.281269, acc.: 86.72%] [G loss: 1.392099]\n",
      "epoch:15 step:12253 [D loss: 0.177872, acc.: 92.19%] [G loss: 3.484765]\n",
      "epoch:15 step:12254 [D loss: 0.081578, acc.: 98.44%] [G loss: 5.455509]\n",
      "epoch:15 step:12255 [D loss: 0.077559, acc.: 97.66%] [G loss: 5.192027]\n",
      "epoch:15 step:12256 [D loss: 0.026650, acc.: 100.00%] [G loss: 5.016735]\n",
      "epoch:15 step:12257 [D loss: 0.053408, acc.: 98.44%] [G loss: 3.891852]\n",
      "epoch:15 step:12258 [D loss: 0.021146, acc.: 100.00%] [G loss: 4.185952]\n",
      "epoch:15 step:12259 [D loss: 0.100728, acc.: 96.88%] [G loss: 4.792332]\n",
      "epoch:15 step:12260 [D loss: 0.013888, acc.: 100.00%] [G loss: 4.720012]\n",
      "epoch:15 step:12261 [D loss: 0.051967, acc.: 98.44%] [G loss: 3.404229]\n",
      "epoch:15 step:12262 [D loss: 0.256780, acc.: 88.28%] [G loss: 5.529490]\n",
      "epoch:15 step:12263 [D loss: 0.044178, acc.: 99.22%] [G loss: 6.265281]\n",
      "epoch:15 step:12264 [D loss: 0.039827, acc.: 99.22%] [G loss: 5.052942]\n",
      "epoch:15 step:12265 [D loss: 0.136619, acc.: 95.31%] [G loss: 3.537514]\n",
      "epoch:15 step:12266 [D loss: 0.047316, acc.: 99.22%] [G loss: 4.044291]\n",
      "epoch:15 step:12267 [D loss: 0.009715, acc.: 100.00%] [G loss: 3.581501]\n",
      "epoch:15 step:12268 [D loss: 0.011205, acc.: 100.00%] [G loss: 4.232857]\n",
      "epoch:15 step:12269 [D loss: 0.193569, acc.: 94.53%] [G loss: 4.505406]\n",
      "epoch:15 step:12270 [D loss: 0.005635, acc.: 100.00%] [G loss: 5.190979]\n",
      "epoch:15 step:12271 [D loss: 0.082208, acc.: 99.22%] [G loss: 4.308475]\n",
      "epoch:15 step:12272 [D loss: 0.099287, acc.: 97.66%] [G loss: 2.563223]\n",
      "epoch:15 step:12273 [D loss: 0.079817, acc.: 99.22%] [G loss: 3.977941]\n",
      "epoch:15 step:12274 [D loss: 0.076900, acc.: 98.44%] [G loss: 3.685487]\n",
      "epoch:15 step:12275 [D loss: 0.077877, acc.: 98.44%] [G loss: 5.350299]\n",
      "epoch:15 step:12276 [D loss: 0.135123, acc.: 96.88%] [G loss: 3.443980]\n",
      "epoch:15 step:12277 [D loss: 0.085015, acc.: 96.88%] [G loss: 6.309854]\n",
      "epoch:15 step:12278 [D loss: 0.089612, acc.: 96.88%] [G loss: 3.838492]\n",
      "epoch:15 step:12279 [D loss: 0.075053, acc.: 98.44%] [G loss: 5.544938]\n",
      "epoch:15 step:12280 [D loss: 0.428142, acc.: 80.47%] [G loss: 6.882326]\n",
      "epoch:15 step:12281 [D loss: 0.323101, acc.: 82.81%] [G loss: 4.219980]\n",
      "epoch:15 step:12282 [D loss: 0.056796, acc.: 99.22%] [G loss: 4.262432]\n",
      "epoch:15 step:12283 [D loss: 0.015332, acc.: 100.00%] [G loss: 3.953395]\n",
      "epoch:15 step:12284 [D loss: 0.016898, acc.: 100.00%] [G loss: 3.431966]\n",
      "epoch:15 step:12285 [D loss: 0.109572, acc.: 96.88%] [G loss: 5.222522]\n",
      "epoch:15 step:12286 [D loss: 0.048550, acc.: 99.22%] [G loss: 5.113836]\n",
      "epoch:15 step:12287 [D loss: 0.083455, acc.: 98.44%] [G loss: 5.533017]\n",
      "epoch:15 step:12288 [D loss: 0.017714, acc.: 100.00%] [G loss: 4.917931]\n",
      "epoch:15 step:12289 [D loss: 0.713491, acc.: 67.97%] [G loss: 8.490715]\n",
      "epoch:15 step:12290 [D loss: 1.671798, acc.: 50.78%] [G loss: 3.757293]\n",
      "epoch:15 step:12291 [D loss: 0.124407, acc.: 94.53%] [G loss: 4.588044]\n",
      "epoch:15 step:12292 [D loss: 0.014146, acc.: 100.00%] [G loss: 4.522106]\n",
      "epoch:15 step:12293 [D loss: 0.016701, acc.: 100.00%] [G loss: 3.867075]\n",
      "epoch:15 step:12294 [D loss: 0.063453, acc.: 98.44%] [G loss: 2.232151]\n",
      "epoch:15 step:12295 [D loss: 0.239936, acc.: 88.28%] [G loss: 4.148480]\n",
      "epoch:15 step:12296 [D loss: 0.037373, acc.: 99.22%] [G loss: 4.944694]\n",
      "epoch:15 step:12297 [D loss: 0.604520, acc.: 73.44%] [G loss: 1.847664]\n",
      "epoch:15 step:12298 [D loss: 0.004017, acc.: 100.00%] [G loss: 3.326724]\n",
      "epoch:15 step:12299 [D loss: 0.185450, acc.: 89.84%] [G loss: 5.508506]\n",
      "epoch:15 step:12300 [D loss: 0.358702, acc.: 83.59%] [G loss: 4.111051]\n",
      "epoch:15 step:12301 [D loss: 0.009376, acc.: 100.00%] [G loss: 3.076914]\n",
      "epoch:15 step:12302 [D loss: 0.062436, acc.: 99.22%] [G loss: 2.390302]\n",
      "epoch:15 step:12303 [D loss: 0.018615, acc.: 100.00%] [G loss: 2.957498]\n",
      "epoch:15 step:12304 [D loss: 0.065076, acc.: 97.66%] [G loss: 3.957477]\n",
      "epoch:15 step:12305 [D loss: 0.032847, acc.: 100.00%] [G loss: 3.981229]\n",
      "epoch:15 step:12306 [D loss: 0.153111, acc.: 94.53%] [G loss: 4.354337]\n",
      "epoch:15 step:12307 [D loss: 0.031701, acc.: 99.22%] [G loss: 4.529907]\n",
      "epoch:15 step:12308 [D loss: 0.101033, acc.: 97.66%] [G loss: 3.392040]\n",
      "epoch:15 step:12309 [D loss: 0.185934, acc.: 92.19%] [G loss: 5.947156]\n",
      "epoch:15 step:12310 [D loss: 0.093771, acc.: 97.66%] [G loss: 6.191442]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:15 step:12311 [D loss: 0.229179, acc.: 90.62%] [G loss: 2.432916]\n",
      "epoch:15 step:12312 [D loss: 0.041773, acc.: 100.00%] [G loss: 2.439069]\n",
      "epoch:15 step:12313 [D loss: 0.039832, acc.: 100.00%] [G loss: 2.596642]\n",
      "epoch:15 step:12314 [D loss: 0.016188, acc.: 100.00%] [G loss: 2.811297]\n",
      "epoch:15 step:12315 [D loss: 0.028561, acc.: 100.00%] [G loss: 3.357444]\n",
      "epoch:15 step:12316 [D loss: 0.013178, acc.: 100.00%] [G loss: 3.040739]\n",
      "epoch:15 step:12317 [D loss: 0.021952, acc.: 100.00%] [G loss: 1.995341]\n",
      "epoch:15 step:12318 [D loss: 0.012653, acc.: 100.00%] [G loss: 1.146683]\n",
      "epoch:15 step:12319 [D loss: 1.934766, acc.: 42.97%] [G loss: 8.464689]\n",
      "epoch:15 step:12320 [D loss: 2.211177, acc.: 50.00%] [G loss: 5.971231]\n",
      "epoch:15 step:12321 [D loss: 1.246285, acc.: 53.12%] [G loss: 0.595335]\n",
      "epoch:15 step:12322 [D loss: 0.628662, acc.: 72.66%] [G loss: 3.127131]\n",
      "epoch:15 step:12323 [D loss: 0.022365, acc.: 99.22%] [G loss: 4.561341]\n",
      "epoch:15 step:12324 [D loss: 0.386361, acc.: 81.25%] [G loss: 3.372040]\n",
      "epoch:15 step:12325 [D loss: 0.118670, acc.: 98.44%] [G loss: 2.030771]\n",
      "epoch:15 step:12326 [D loss: 0.090015, acc.: 98.44%] [G loss: 2.961221]\n",
      "epoch:15 step:12327 [D loss: 0.069177, acc.: 98.44%] [G loss: 3.186448]\n",
      "epoch:15 step:12328 [D loss: 0.083494, acc.: 97.66%] [G loss: 2.447247]\n",
      "epoch:15 step:12329 [D loss: 0.055331, acc.: 98.44%] [G loss: 3.545737]\n",
      "epoch:15 step:12330 [D loss: 0.068511, acc.: 98.44%] [G loss: 2.350254]\n",
      "epoch:15 step:12331 [D loss: 0.720799, acc.: 64.06%] [G loss: 4.878462]\n",
      "epoch:15 step:12332 [D loss: 0.665533, acc.: 68.75%] [G loss: 3.974431]\n",
      "epoch:15 step:12333 [D loss: 0.129392, acc.: 95.31%] [G loss: 2.784210]\n",
      "epoch:15 step:12334 [D loss: 0.095890, acc.: 97.66%] [G loss: 3.827604]\n",
      "epoch:15 step:12335 [D loss: 0.049121, acc.: 100.00%] [G loss: 3.912562]\n",
      "epoch:15 step:12336 [D loss: 0.026132, acc.: 100.00%] [G loss: 3.855251]\n",
      "epoch:15 step:12337 [D loss: 0.055751, acc.: 100.00%] [G loss: 3.034243]\n",
      "epoch:15 step:12338 [D loss: 0.127073, acc.: 96.88%] [G loss: 3.098368]\n",
      "epoch:15 step:12339 [D loss: 0.104504, acc.: 96.88%] [G loss: 3.226442]\n",
      "epoch:15 step:12340 [D loss: 0.035313, acc.: 100.00%] [G loss: 3.523209]\n",
      "epoch:15 step:12341 [D loss: 0.084822, acc.: 98.44%] [G loss: 3.802673]\n",
      "epoch:15 step:12342 [D loss: 0.533555, acc.: 71.88%] [G loss: 3.910437]\n",
      "epoch:15 step:12343 [D loss: 0.016601, acc.: 100.00%] [G loss: 4.641753]\n",
      "epoch:15 step:12344 [D loss: 0.453653, acc.: 79.69%] [G loss: 1.618843]\n",
      "epoch:15 step:12345 [D loss: 0.448945, acc.: 71.88%] [G loss: 5.260319]\n",
      "epoch:15 step:12346 [D loss: 0.747799, acc.: 60.16%] [G loss: 3.354651]\n",
      "epoch:15 step:12347 [D loss: 0.020655, acc.: 100.00%] [G loss: 2.603863]\n",
      "epoch:15 step:12348 [D loss: 0.136373, acc.: 96.88%] [G loss: 3.093907]\n",
      "epoch:15 step:12349 [D loss: 0.022834, acc.: 100.00%] [G loss: 3.107583]\n",
      "epoch:15 step:12350 [D loss: 0.032061, acc.: 100.00%] [G loss: 3.477639]\n",
      "epoch:15 step:12351 [D loss: 0.049085, acc.: 100.00%] [G loss: 2.927207]\n",
      "epoch:15 step:12352 [D loss: 0.056534, acc.: 100.00%] [G loss: 2.518140]\n",
      "epoch:15 step:12353 [D loss: 0.080810, acc.: 98.44%] [G loss: 2.684650]\n",
      "epoch:15 step:12354 [D loss: 0.028029, acc.: 99.22%] [G loss: 3.034891]\n",
      "epoch:15 step:12355 [D loss: 0.033518, acc.: 100.00%] [G loss: 2.353172]\n",
      "epoch:15 step:12356 [D loss: 0.142000, acc.: 95.31%] [G loss: 3.433670]\n",
      "epoch:15 step:12357 [D loss: 0.324790, acc.: 83.59%] [G loss: 1.617983]\n",
      "epoch:15 step:12358 [D loss: 0.106430, acc.: 98.44%] [G loss: 2.435087]\n",
      "epoch:15 step:12359 [D loss: 0.029357, acc.: 100.00%] [G loss: 3.163295]\n",
      "epoch:15 step:12360 [D loss: 0.047564, acc.: 100.00%] [G loss: 3.086502]\n",
      "epoch:15 step:12361 [D loss: 0.503828, acc.: 76.56%] [G loss: 1.494848]\n",
      "epoch:15 step:12362 [D loss: 0.162834, acc.: 92.19%] [G loss: 3.955505]\n",
      "epoch:15 step:12363 [D loss: 0.062851, acc.: 96.88%] [G loss: 4.912573]\n",
      "epoch:15 step:12364 [D loss: 0.184078, acc.: 92.97%] [G loss: 3.143177]\n",
      "epoch:15 step:12365 [D loss: 0.064508, acc.: 100.00%] [G loss: 3.321009]\n",
      "epoch:15 step:12366 [D loss: 0.014094, acc.: 100.00%] [G loss: 3.894846]\n",
      "epoch:15 step:12367 [D loss: 0.053839, acc.: 99.22%] [G loss: 3.817079]\n",
      "epoch:15 step:12368 [D loss: 0.049460, acc.: 98.44%] [G loss: 4.148365]\n",
      "epoch:15 step:12369 [D loss: 0.055028, acc.: 100.00%] [G loss: 3.427978]\n",
      "epoch:15 step:12370 [D loss: 0.029406, acc.: 100.00%] [G loss: 3.605095]\n",
      "epoch:15 step:12371 [D loss: 0.055025, acc.: 100.00%] [G loss: 3.548355]\n",
      "epoch:15 step:12372 [D loss: 0.040123, acc.: 100.00%] [G loss: 3.784718]\n",
      "epoch:15 step:12373 [D loss: 0.253924, acc.: 92.97%] [G loss: 4.633525]\n",
      "epoch:15 step:12374 [D loss: 0.050958, acc.: 98.44%] [G loss: 4.884777]\n",
      "epoch:15 step:12375 [D loss: 0.168895, acc.: 96.09%] [G loss: 2.756392]\n",
      "epoch:15 step:12376 [D loss: 0.086837, acc.: 100.00%] [G loss: 3.941236]\n",
      "epoch:15 step:12377 [D loss: 0.021288, acc.: 100.00%] [G loss: 4.512665]\n",
      "epoch:15 step:12378 [D loss: 0.024148, acc.: 100.00%] [G loss: 4.121052]\n",
      "epoch:15 step:12379 [D loss: 0.032775, acc.: 100.00%] [G loss: 3.804683]\n",
      "epoch:15 step:12380 [D loss: 0.122394, acc.: 97.66%] [G loss: 3.173746]\n",
      "epoch:15 step:12381 [D loss: 0.009468, acc.: 100.00%] [G loss: 4.022280]\n",
      "epoch:15 step:12382 [D loss: 0.038558, acc.: 100.00%] [G loss: 4.421539]\n",
      "epoch:15 step:12383 [D loss: 0.290993, acc.: 89.84%] [G loss: 5.296886]\n",
      "epoch:15 step:12384 [D loss: 0.008789, acc.: 100.00%] [G loss: 6.199280]\n",
      "epoch:15 step:12385 [D loss: 0.284413, acc.: 82.81%] [G loss: 3.471354]\n",
      "epoch:15 step:12386 [D loss: 0.018247, acc.: 100.00%] [G loss: 2.973194]\n",
      "epoch:15 step:12387 [D loss: 0.122030, acc.: 98.44%] [G loss: 4.556354]\n",
      "epoch:15 step:12388 [D loss: 0.007424, acc.: 100.00%] [G loss: 5.219209]\n",
      "epoch:15 step:12389 [D loss: 0.013054, acc.: 100.00%] [G loss: 5.111062]\n",
      "epoch:15 step:12390 [D loss: 0.014695, acc.: 100.00%] [G loss: 4.963768]\n",
      "epoch:15 step:12391 [D loss: 0.013633, acc.: 100.00%] [G loss: 4.532346]\n",
      "epoch:15 step:12392 [D loss: 0.021431, acc.: 100.00%] [G loss: 4.212998]\n",
      "epoch:15 step:12393 [D loss: 0.031640, acc.: 100.00%] [G loss: 4.179546]\n",
      "epoch:15 step:12394 [D loss: 0.034746, acc.: 100.00%] [G loss: 3.416042]\n",
      "epoch:15 step:12395 [D loss: 0.040690, acc.: 100.00%] [G loss: 3.821846]\n",
      "epoch:15 step:12396 [D loss: 0.085221, acc.: 97.66%] [G loss: 3.084744]\n",
      "epoch:15 step:12397 [D loss: 0.047241, acc.: 100.00%] [G loss: 3.476167]\n",
      "epoch:15 step:12398 [D loss: 0.041910, acc.: 99.22%] [G loss: 3.823794]\n",
      "epoch:15 step:12399 [D loss: 0.040115, acc.: 100.00%] [G loss: 4.349960]\n",
      "epoch:15 step:12400 [D loss: 0.648592, acc.: 69.53%] [G loss: 5.451471]\n",
      "##############\n",
      "[1.01016658 0.74874378 0.90675409 0.98830237 0.94398132 0.92731453\n",
      " 2.11061415 2.11021171 2.11363617 2.11045255]\n",
      "##########\n",
      "epoch:15 step:12401 [D loss: 0.083781, acc.: 97.66%] [G loss: 4.855005]\n",
      "epoch:15 step:12402 [D loss: 0.286103, acc.: 86.72%] [G loss: 2.007617]\n",
      "epoch:15 step:12403 [D loss: 0.332892, acc.: 83.59%] [G loss: 6.115269]\n",
      "epoch:15 step:12404 [D loss: 0.636258, acc.: 70.31%] [G loss: 3.555414]\n",
      "epoch:15 step:12405 [D loss: 0.085745, acc.: 98.44%] [G loss: 4.989450]\n",
      "epoch:15 step:12406 [D loss: 0.025033, acc.: 99.22%] [G loss: 5.235125]\n",
      "epoch:15 step:12407 [D loss: 0.083733, acc.: 97.66%] [G loss: 4.159770]\n",
      "epoch:15 step:12408 [D loss: 0.056008, acc.: 98.44%] [G loss: 4.308976]\n",
      "epoch:15 step:12409 [D loss: 0.008636, acc.: 100.00%] [G loss: 4.207937]\n",
      "epoch:15 step:12410 [D loss: 0.010037, acc.: 100.00%] [G loss: 4.188158]\n",
      "epoch:15 step:12411 [D loss: 0.063617, acc.: 98.44%] [G loss: 3.540801]\n",
      "epoch:15 step:12412 [D loss: 0.028041, acc.: 100.00%] [G loss: 3.794349]\n",
      "epoch:15 step:12413 [D loss: 0.156705, acc.: 95.31%] [G loss: 5.834549]\n",
      "epoch:15 step:12414 [D loss: 0.160594, acc.: 94.53%] [G loss: 3.825609]\n",
      "epoch:15 step:12415 [D loss: 0.083683, acc.: 98.44%] [G loss: 4.434354]\n",
      "epoch:15 step:12416 [D loss: 0.005116, acc.: 100.00%] [G loss: 5.089374]\n",
      "epoch:15 step:12417 [D loss: 0.021282, acc.: 100.00%] [G loss: 4.747609]\n",
      "epoch:15 step:12418 [D loss: 0.077193, acc.: 98.44%] [G loss: 2.879144]\n",
      "epoch:15 step:12419 [D loss: 0.052555, acc.: 99.22%] [G loss: 3.964275]\n",
      "epoch:15 step:12420 [D loss: 0.043017, acc.: 98.44%] [G loss: 5.094908]\n",
      "epoch:15 step:12421 [D loss: 0.014187, acc.: 100.00%] [G loss: 5.726645]\n",
      "epoch:15 step:12422 [D loss: 0.169941, acc.: 95.31%] [G loss: 2.626454]\n",
      "epoch:15 step:12423 [D loss: 0.064427, acc.: 99.22%] [G loss: 2.881237]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:15 step:12424 [D loss: 0.013007, acc.: 100.00%] [G loss: 5.388985]\n",
      "epoch:15 step:12425 [D loss: 0.029041, acc.: 100.00%] [G loss: 5.151536]\n",
      "epoch:15 step:12426 [D loss: 0.453917, acc.: 79.69%] [G loss: 5.425492]\n",
      "epoch:15 step:12427 [D loss: 1.837393, acc.: 14.84%] [G loss: 7.017487]\n",
      "epoch:15 step:12428 [D loss: 0.555504, acc.: 74.22%] [G loss: 4.046059]\n",
      "epoch:15 step:12429 [D loss: 0.072273, acc.: 99.22%] [G loss: 3.663483]\n",
      "epoch:15 step:12430 [D loss: 0.233354, acc.: 92.97%] [G loss: 4.808402]\n",
      "epoch:15 step:12431 [D loss: 0.018743, acc.: 100.00%] [G loss: 6.044622]\n",
      "epoch:15 step:12432 [D loss: 0.028464, acc.: 99.22%] [G loss: 4.984297]\n",
      "epoch:15 step:12433 [D loss: 0.088951, acc.: 97.66%] [G loss: 4.159460]\n",
      "epoch:15 step:12434 [D loss: 0.098474, acc.: 96.88%] [G loss: 5.174463]\n",
      "epoch:15 step:12435 [D loss: 0.013111, acc.: 100.00%] [G loss: 5.000172]\n",
      "epoch:15 step:12436 [D loss: 0.062610, acc.: 99.22%] [G loss: 3.305520]\n",
      "epoch:15 step:12437 [D loss: 0.027773, acc.: 100.00%] [G loss: 3.201629]\n",
      "epoch:15 step:12438 [D loss: 0.009227, acc.: 100.00%] [G loss: 2.775128]\n",
      "epoch:15 step:12439 [D loss: 0.051184, acc.: 100.00%] [G loss: 3.945417]\n",
      "epoch:15 step:12440 [D loss: 0.850196, acc.: 63.28%] [G loss: 6.711918]\n",
      "epoch:15 step:12441 [D loss: 0.035751, acc.: 100.00%] [G loss: 7.038299]\n",
      "epoch:15 step:12442 [D loss: 0.531660, acc.: 70.31%] [G loss: 1.266552]\n",
      "epoch:15 step:12443 [D loss: 0.988363, acc.: 60.16%] [G loss: 7.440357]\n",
      "epoch:15 step:12444 [D loss: 1.149418, acc.: 57.03%] [G loss: 4.503308]\n",
      "epoch:15 step:12445 [D loss: 0.088125, acc.: 97.66%] [G loss: 3.735816]\n",
      "epoch:15 step:12446 [D loss: 0.013735, acc.: 100.00%] [G loss: 3.680145]\n",
      "epoch:15 step:12447 [D loss: 0.089970, acc.: 97.66%] [G loss: 3.902000]\n",
      "epoch:15 step:12448 [D loss: 0.018415, acc.: 100.00%] [G loss: 3.730710]\n",
      "epoch:15 step:12449 [D loss: 0.050952, acc.: 100.00%] [G loss: 3.143518]\n",
      "epoch:15 step:12450 [D loss: 0.174166, acc.: 93.75%] [G loss: 3.102451]\n",
      "epoch:15 step:12451 [D loss: 0.034012, acc.: 100.00%] [G loss: 3.430871]\n",
      "epoch:15 step:12452 [D loss: 0.247346, acc.: 89.06%] [G loss: 4.671633]\n",
      "epoch:15 step:12453 [D loss: 0.192589, acc.: 91.41%] [G loss: 3.519195]\n",
      "epoch:15 step:12454 [D loss: 0.413972, acc.: 78.12%] [G loss: 5.983419]\n",
      "epoch:15 step:12455 [D loss: 0.425616, acc.: 78.12%] [G loss: 4.440930]\n",
      "epoch:15 step:12456 [D loss: 0.084333, acc.: 98.44%] [G loss: 3.411085]\n",
      "epoch:15 step:12457 [D loss: 0.201838, acc.: 90.62%] [G loss: 5.005795]\n",
      "epoch:15 step:12458 [D loss: 0.046711, acc.: 97.66%] [G loss: 5.168118]\n",
      "epoch:15 step:12459 [D loss: 0.016018, acc.: 100.00%] [G loss: 4.693115]\n",
      "epoch:15 step:12460 [D loss: 0.036043, acc.: 99.22%] [G loss: 4.610894]\n",
      "epoch:15 step:12461 [D loss: 0.073580, acc.: 99.22%] [G loss: 3.784209]\n",
      "epoch:15 step:12462 [D loss: 0.027158, acc.: 100.00%] [G loss: 3.204799]\n",
      "epoch:15 step:12463 [D loss: 0.042613, acc.: 100.00%] [G loss: 4.306866]\n",
      "epoch:15 step:12464 [D loss: 0.026086, acc.: 100.00%] [G loss: 3.456151]\n",
      "epoch:15 step:12465 [D loss: 0.074427, acc.: 97.66%] [G loss: 1.904791]\n",
      "epoch:15 step:12466 [D loss: 0.031785, acc.: 100.00%] [G loss: 2.069576]\n",
      "epoch:15 step:12467 [D loss: 0.078443, acc.: 100.00%] [G loss: 3.955788]\n",
      "epoch:15 step:12468 [D loss: 0.433805, acc.: 82.81%] [G loss: 3.071200]\n",
      "epoch:15 step:12469 [D loss: 0.031708, acc.: 100.00%] [G loss: 4.038423]\n",
      "epoch:15 step:12470 [D loss: 0.008051, acc.: 100.00%] [G loss: 3.701079]\n",
      "epoch:15 step:12471 [D loss: 0.082089, acc.: 99.22%] [G loss: 4.501453]\n",
      "epoch:15 step:12472 [D loss: 0.023360, acc.: 99.22%] [G loss: 4.290086]\n",
      "epoch:15 step:12473 [D loss: 0.052143, acc.: 100.00%] [G loss: 3.719199]\n",
      "epoch:15 step:12474 [D loss: 0.046469, acc.: 100.00%] [G loss: 3.545602]\n",
      "epoch:15 step:12475 [D loss: 0.201362, acc.: 94.53%] [G loss: 3.948260]\n",
      "epoch:15 step:12476 [D loss: 0.015678, acc.: 100.00%] [G loss: 4.176099]\n",
      "epoch:15 step:12477 [D loss: 0.583170, acc.: 66.41%] [G loss: 4.567222]\n",
      "epoch:15 step:12478 [D loss: 0.011437, acc.: 100.00%] [G loss: 5.574639]\n",
      "epoch:15 step:12479 [D loss: 0.128718, acc.: 95.31%] [G loss: 4.824202]\n",
      "epoch:15 step:12480 [D loss: 0.019014, acc.: 100.00%] [G loss: 4.113679]\n",
      "epoch:15 step:12481 [D loss: 0.010150, acc.: 100.00%] [G loss: 4.235723]\n",
      "epoch:15 step:12482 [D loss: 0.012624, acc.: 100.00%] [G loss: 4.383518]\n",
      "epoch:15 step:12483 [D loss: 0.031101, acc.: 100.00%] [G loss: 4.081345]\n",
      "epoch:15 step:12484 [D loss: 0.028475, acc.: 100.00%] [G loss: 4.633486]\n",
      "epoch:15 step:12485 [D loss: 0.078044, acc.: 98.44%] [G loss: 4.306481]\n",
      "epoch:15 step:12486 [D loss: 0.078921, acc.: 96.09%] [G loss: 4.418285]\n",
      "epoch:15 step:12487 [D loss: 0.025853, acc.: 100.00%] [G loss: 4.352821]\n",
      "epoch:15 step:12488 [D loss: 0.008241, acc.: 100.00%] [G loss: 4.353081]\n",
      "epoch:15 step:12489 [D loss: 0.026485, acc.: 100.00%] [G loss: 4.458992]\n",
      "epoch:15 step:12490 [D loss: 0.016803, acc.: 100.00%] [G loss: 3.377525]\n",
      "epoch:15 step:12491 [D loss: 0.112257, acc.: 96.09%] [G loss: 1.884926]\n",
      "epoch:15 step:12492 [D loss: 0.581041, acc.: 74.22%] [G loss: 6.983023]\n",
      "epoch:15 step:12493 [D loss: 2.060267, acc.: 50.00%] [G loss: 4.307311]\n",
      "epoch:15 step:12494 [D loss: 0.093611, acc.: 98.44%] [G loss: 2.736814]\n",
      "epoch:15 step:12495 [D loss: 0.225457, acc.: 89.06%] [G loss: 4.633401]\n",
      "epoch:15 step:12496 [D loss: 0.035181, acc.: 100.00%] [G loss: 5.113331]\n",
      "epoch:16 step:12497 [D loss: 0.159384, acc.: 92.19%] [G loss: 3.219684]\n",
      "epoch:16 step:12498 [D loss: 0.029522, acc.: 100.00%] [G loss: 2.656057]\n",
      "epoch:16 step:12499 [D loss: 0.100416, acc.: 96.88%] [G loss: 3.829192]\n",
      "epoch:16 step:12500 [D loss: 0.008523, acc.: 100.00%] [G loss: 4.497539]\n",
      "epoch:16 step:12501 [D loss: 0.014963, acc.: 100.00%] [G loss: 3.688273]\n",
      "epoch:16 step:12502 [D loss: 0.052979, acc.: 99.22%] [G loss: 2.365647]\n",
      "epoch:16 step:12503 [D loss: 0.039052, acc.: 99.22%] [G loss: 2.476460]\n",
      "epoch:16 step:12504 [D loss: 0.036894, acc.: 99.22%] [G loss: 2.352280]\n",
      "epoch:16 step:12505 [D loss: 0.188965, acc.: 93.75%] [G loss: 3.999297]\n",
      "epoch:16 step:12506 [D loss: 0.183584, acc.: 92.19%] [G loss: 2.536889]\n",
      "epoch:16 step:12507 [D loss: 0.325830, acc.: 84.38%] [G loss: 5.697696]\n",
      "epoch:16 step:12508 [D loss: 0.397084, acc.: 76.56%] [G loss: 4.430244]\n",
      "epoch:16 step:12509 [D loss: 0.013818, acc.: 100.00%] [G loss: 4.384116]\n",
      "epoch:16 step:12510 [D loss: 0.023898, acc.: 100.00%] [G loss: 4.005799]\n",
      "epoch:16 step:12511 [D loss: 0.015593, acc.: 100.00%] [G loss: 3.688938]\n",
      "epoch:16 step:12512 [D loss: 0.010926, acc.: 100.00%] [G loss: 3.245373]\n",
      "epoch:16 step:12513 [D loss: 0.034155, acc.: 100.00%] [G loss: 3.100748]\n",
      "epoch:16 step:12514 [D loss: 0.028913, acc.: 100.00%] [G loss: 4.168964]\n",
      "epoch:16 step:12515 [D loss: 0.036700, acc.: 100.00%] [G loss: 3.904683]\n",
      "epoch:16 step:12516 [D loss: 0.025759, acc.: 100.00%] [G loss: 4.239920]\n",
      "epoch:16 step:12517 [D loss: 0.030771, acc.: 100.00%] [G loss: 3.369801]\n",
      "epoch:16 step:12518 [D loss: 0.040327, acc.: 100.00%] [G loss: 3.919826]\n",
      "epoch:16 step:12519 [D loss: 0.089623, acc.: 100.00%] [G loss: 3.306657]\n",
      "epoch:16 step:12520 [D loss: 0.058383, acc.: 99.22%] [G loss: 3.588979]\n",
      "epoch:16 step:12521 [D loss: 0.109685, acc.: 97.66%] [G loss: 4.573051]\n",
      "epoch:16 step:12522 [D loss: 0.132658, acc.: 96.88%] [G loss: 2.735314]\n",
      "epoch:16 step:12523 [D loss: 0.134918, acc.: 94.53%] [G loss: 5.171268]\n",
      "epoch:16 step:12524 [D loss: 0.283492, acc.: 85.16%] [G loss: 1.959628]\n",
      "epoch:16 step:12525 [D loss: 0.100477, acc.: 97.66%] [G loss: 4.112365]\n",
      "epoch:16 step:12526 [D loss: 0.041970, acc.: 100.00%] [G loss: 4.297249]\n",
      "epoch:16 step:12527 [D loss: 0.021783, acc.: 100.00%] [G loss: 3.935623]\n",
      "epoch:16 step:12528 [D loss: 0.025793, acc.: 100.00%] [G loss: 3.601127]\n",
      "epoch:16 step:12529 [D loss: 0.357715, acc.: 88.28%] [G loss: 5.090172]\n",
      "epoch:16 step:12530 [D loss: 0.028582, acc.: 99.22%] [G loss: 5.879865]\n",
      "epoch:16 step:12531 [D loss: 0.327218, acc.: 82.03%] [G loss: 1.438995]\n",
      "epoch:16 step:12532 [D loss: 0.657381, acc.: 68.75%] [G loss: 7.295537]\n",
      "epoch:16 step:12533 [D loss: 0.580327, acc.: 69.53%] [G loss: 4.717306]\n",
      "epoch:16 step:12534 [D loss: 0.131716, acc.: 92.97%] [G loss: 2.761004]\n",
      "epoch:16 step:12535 [D loss: 0.540001, acc.: 75.00%] [G loss: 6.834345]\n",
      "epoch:16 step:12536 [D loss: 1.004389, acc.: 55.47%] [G loss: 4.356215]\n",
      "epoch:16 step:12537 [D loss: 0.039907, acc.: 100.00%] [G loss: 3.441636]\n",
      "epoch:16 step:12538 [D loss: 0.019335, acc.: 100.00%] [G loss: 2.853660]\n",
      "epoch:16 step:12539 [D loss: 0.223900, acc.: 86.72%] [G loss: 4.478405]\n",
      "epoch:16 step:12540 [D loss: 0.101033, acc.: 96.88%] [G loss: 5.387122]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:16 step:12541 [D loss: 0.136959, acc.: 95.31%] [G loss: 3.148923]\n",
      "epoch:16 step:12542 [D loss: 0.048218, acc.: 100.00%] [G loss: 4.113790]\n",
      "epoch:16 step:12543 [D loss: 0.029325, acc.: 100.00%] [G loss: 3.176217]\n",
      "epoch:16 step:12544 [D loss: 0.038362, acc.: 100.00%] [G loss: 3.933450]\n",
      "epoch:16 step:12545 [D loss: 0.079091, acc.: 98.44%] [G loss: 2.710597]\n",
      "epoch:16 step:12546 [D loss: 0.057569, acc.: 98.44%] [G loss: 1.358286]\n",
      "epoch:16 step:12547 [D loss: 0.049432, acc.: 100.00%] [G loss: 3.151712]\n",
      "epoch:16 step:12548 [D loss: 0.008174, acc.: 100.00%] [G loss: 3.439662]\n",
      "epoch:16 step:12549 [D loss: 0.091252, acc.: 99.22%] [G loss: 3.080645]\n",
      "epoch:16 step:12550 [D loss: 0.106372, acc.: 98.44%] [G loss: 2.538525]\n",
      "epoch:16 step:12551 [D loss: 0.199044, acc.: 91.41%] [G loss: 3.510278]\n",
      "epoch:16 step:12552 [D loss: 0.198561, acc.: 90.62%] [G loss: 4.323828]\n",
      "epoch:16 step:12553 [D loss: 0.025835, acc.: 100.00%] [G loss: 2.453830]\n",
      "epoch:16 step:12554 [D loss: 0.058001, acc.: 100.00%] [G loss: 3.355552]\n",
      "epoch:16 step:12555 [D loss: 0.013534, acc.: 100.00%] [G loss: 3.122402]\n",
      "epoch:16 step:12556 [D loss: 0.031208, acc.: 100.00%] [G loss: 2.680641]\n",
      "epoch:16 step:12557 [D loss: 0.234356, acc.: 92.19%] [G loss: 4.782948]\n",
      "epoch:16 step:12558 [D loss: 0.020197, acc.: 100.00%] [G loss: 4.778924]\n",
      "epoch:16 step:12559 [D loss: 0.132092, acc.: 95.31%] [G loss: 4.731894]\n",
      "epoch:16 step:12560 [D loss: 0.101681, acc.: 98.44%] [G loss: 5.007798]\n",
      "epoch:16 step:12561 [D loss: 0.230421, acc.: 91.41%] [G loss: 4.185950]\n",
      "epoch:16 step:12562 [D loss: 0.014491, acc.: 100.00%] [G loss: 5.055614]\n",
      "epoch:16 step:12563 [D loss: 0.012406, acc.: 100.00%] [G loss: 4.707325]\n",
      "epoch:16 step:12564 [D loss: 0.007007, acc.: 100.00%] [G loss: 4.873072]\n",
      "epoch:16 step:12565 [D loss: 0.013094, acc.: 100.00%] [G loss: 4.277856]\n",
      "epoch:16 step:12566 [D loss: 0.019651, acc.: 100.00%] [G loss: 4.437350]\n",
      "epoch:16 step:12567 [D loss: 0.049055, acc.: 98.44%] [G loss: 4.018312]\n",
      "epoch:16 step:12568 [D loss: 0.007378, acc.: 100.00%] [G loss: 4.369181]\n",
      "epoch:16 step:12569 [D loss: 0.009569, acc.: 100.00%] [G loss: 4.246925]\n",
      "epoch:16 step:12570 [D loss: 0.032805, acc.: 99.22%] [G loss: 4.048713]\n",
      "epoch:16 step:12571 [D loss: 0.027310, acc.: 100.00%] [G loss: 4.253745]\n",
      "epoch:16 step:12572 [D loss: 0.057418, acc.: 100.00%] [G loss: 2.798382]\n",
      "epoch:16 step:12573 [D loss: 1.070791, acc.: 55.47%] [G loss: 7.469496]\n",
      "epoch:16 step:12574 [D loss: 2.822480, acc.: 50.00%] [G loss: 5.922274]\n",
      "epoch:16 step:12575 [D loss: 1.310439, acc.: 50.00%] [G loss: 2.124064]\n",
      "epoch:16 step:12576 [D loss: 0.105786, acc.: 98.44%] [G loss: 1.682872]\n",
      "epoch:16 step:12577 [D loss: 0.423658, acc.: 75.78%] [G loss: 4.228193]\n",
      "epoch:16 step:12578 [D loss: 0.180937, acc.: 89.84%] [G loss: 4.042555]\n",
      "epoch:16 step:12579 [D loss: 0.517487, acc.: 74.22%] [G loss: 2.650802]\n",
      "epoch:16 step:12580 [D loss: 0.258621, acc.: 89.06%] [G loss: 2.938943]\n",
      "epoch:16 step:12581 [D loss: 0.089035, acc.: 98.44%] [G loss: 3.934115]\n",
      "epoch:16 step:12582 [D loss: 0.101511, acc.: 96.09%] [G loss: 4.120492]\n",
      "epoch:16 step:12583 [D loss: 0.159621, acc.: 93.75%] [G loss: 3.577519]\n",
      "epoch:16 step:12584 [D loss: 0.289526, acc.: 89.84%] [G loss: 2.825209]\n",
      "epoch:16 step:12585 [D loss: 0.070161, acc.: 100.00%] [G loss: 3.344014]\n",
      "epoch:16 step:12586 [D loss: 0.420812, acc.: 82.03%] [G loss: 3.697437]\n",
      "epoch:16 step:12587 [D loss: 0.256535, acc.: 92.19%] [G loss: 4.525240]\n",
      "epoch:16 step:12588 [D loss: 0.089980, acc.: 96.88%] [G loss: 4.067188]\n",
      "epoch:16 step:12589 [D loss: 0.074773, acc.: 98.44%] [G loss: 3.520590]\n",
      "epoch:16 step:12590 [D loss: 0.392475, acc.: 83.59%] [G loss: 4.275416]\n",
      "epoch:16 step:12591 [D loss: 0.066940, acc.: 96.88%] [G loss: 3.736598]\n",
      "epoch:16 step:12592 [D loss: 0.060990, acc.: 100.00%] [G loss: 2.887402]\n",
      "epoch:16 step:12593 [D loss: 0.114564, acc.: 96.88%] [G loss: 4.310529]\n",
      "epoch:16 step:12594 [D loss: 0.017579, acc.: 100.00%] [G loss: 3.708763]\n",
      "epoch:16 step:12595 [D loss: 0.308566, acc.: 85.16%] [G loss: 4.120138]\n",
      "epoch:16 step:12596 [D loss: 0.037649, acc.: 99.22%] [G loss: 4.290363]\n",
      "epoch:16 step:12597 [D loss: 0.045414, acc.: 99.22%] [G loss: 2.839296]\n",
      "epoch:16 step:12598 [D loss: 0.050403, acc.: 99.22%] [G loss: 3.009550]\n",
      "epoch:16 step:12599 [D loss: 0.035874, acc.: 100.00%] [G loss: 2.635490]\n",
      "epoch:16 step:12600 [D loss: 0.364882, acc.: 85.16%] [G loss: 5.531340]\n",
      "##############\n",
      "[0.99961096 0.90885794 0.96399307 0.97728039 2.11606578 1.06135162\n",
      " 1.1088204  0.93773824 0.89184029 1.11706095]\n",
      "##########\n",
      "epoch:16 step:12601 [D loss: 0.649667, acc.: 67.97%] [G loss: 2.212910]\n",
      "epoch:16 step:12602 [D loss: 0.055230, acc.: 99.22%] [G loss: 3.482596]\n",
      "epoch:16 step:12603 [D loss: 0.058560, acc.: 99.22%] [G loss: 3.335723]\n",
      "epoch:16 step:12604 [D loss: 0.015949, acc.: 100.00%] [G loss: 3.857104]\n",
      "epoch:16 step:12605 [D loss: 0.056171, acc.: 100.00%] [G loss: 4.280195]\n",
      "epoch:16 step:12606 [D loss: 0.166174, acc.: 94.53%] [G loss: 3.194143]\n",
      "epoch:16 step:12607 [D loss: 0.135536, acc.: 93.75%] [G loss: 5.433671]\n",
      "epoch:16 step:12608 [D loss: 0.080132, acc.: 97.66%] [G loss: 5.488960]\n",
      "epoch:16 step:12609 [D loss: 0.030351, acc.: 99.22%] [G loss: 3.913376]\n",
      "epoch:16 step:12610 [D loss: 0.023885, acc.: 100.00%] [G loss: 4.042588]\n",
      "epoch:16 step:12611 [D loss: 0.087986, acc.: 97.66%] [G loss: 4.306792]\n",
      "epoch:16 step:12612 [D loss: 0.105226, acc.: 96.88%] [G loss: 3.333334]\n",
      "epoch:16 step:12613 [D loss: 0.092826, acc.: 99.22%] [G loss: 4.550958]\n",
      "epoch:16 step:12614 [D loss: 0.012799, acc.: 100.00%] [G loss: 5.044960]\n",
      "epoch:16 step:12615 [D loss: 0.036111, acc.: 99.22%] [G loss: 4.157654]\n",
      "epoch:16 step:12616 [D loss: 0.030445, acc.: 100.00%] [G loss: 3.919141]\n",
      "epoch:16 step:12617 [D loss: 0.027205, acc.: 100.00%] [G loss: 4.413475]\n",
      "epoch:16 step:12618 [D loss: 0.036924, acc.: 100.00%] [G loss: 4.411589]\n",
      "epoch:16 step:12619 [D loss: 0.017982, acc.: 100.00%] [G loss: 4.023345]\n",
      "epoch:16 step:12620 [D loss: 0.012815, acc.: 100.00%] [G loss: 3.859874]\n",
      "epoch:16 step:12621 [D loss: 0.040341, acc.: 100.00%] [G loss: 4.861067]\n",
      "epoch:16 step:12622 [D loss: 0.046843, acc.: 98.44%] [G loss: 3.761189]\n",
      "epoch:16 step:12623 [D loss: 0.013001, acc.: 100.00%] [G loss: 2.764931]\n",
      "epoch:16 step:12624 [D loss: 0.058936, acc.: 100.00%] [G loss: 3.052817]\n",
      "epoch:16 step:12625 [D loss: 0.024813, acc.: 100.00%] [G loss: 3.865729]\n",
      "epoch:16 step:12626 [D loss: 0.026756, acc.: 100.00%] [G loss: 3.801363]\n",
      "epoch:16 step:12627 [D loss: 0.027650, acc.: 100.00%] [G loss: 4.904248]\n",
      "epoch:16 step:12628 [D loss: 1.160447, acc.: 47.66%] [G loss: 5.807719]\n",
      "epoch:16 step:12629 [D loss: 0.550706, acc.: 71.88%] [G loss: 4.621982]\n",
      "epoch:16 step:12630 [D loss: 0.499809, acc.: 78.91%] [G loss: 1.082909]\n",
      "epoch:16 step:12631 [D loss: 0.899951, acc.: 62.50%] [G loss: 4.831660]\n",
      "epoch:16 step:12632 [D loss: 0.141769, acc.: 95.31%] [G loss: 5.712138]\n",
      "epoch:16 step:12633 [D loss: 0.998206, acc.: 57.03%] [G loss: 2.706118]\n",
      "epoch:16 step:12634 [D loss: 0.151556, acc.: 93.75%] [G loss: 3.349135]\n",
      "epoch:16 step:12635 [D loss: 0.021619, acc.: 100.00%] [G loss: 3.834117]\n",
      "epoch:16 step:12636 [D loss: 0.043975, acc.: 100.00%] [G loss: 2.940070]\n",
      "epoch:16 step:12637 [D loss: 0.088956, acc.: 100.00%] [G loss: 1.960038]\n",
      "epoch:16 step:12638 [D loss: 0.118307, acc.: 96.88%] [G loss: 2.276950]\n",
      "epoch:16 step:12639 [D loss: 0.101892, acc.: 98.44%] [G loss: 2.659758]\n",
      "epoch:16 step:12640 [D loss: 0.025048, acc.: 100.00%] [G loss: 2.098472]\n",
      "epoch:16 step:12641 [D loss: 0.058831, acc.: 99.22%] [G loss: 1.928024]\n",
      "epoch:16 step:12642 [D loss: 0.143800, acc.: 96.88%] [G loss: 1.738531]\n",
      "epoch:16 step:12643 [D loss: 0.056050, acc.: 99.22%] [G loss: 2.727853]\n",
      "epoch:16 step:12644 [D loss: 0.198676, acc.: 94.53%] [G loss: 5.648993]\n",
      "epoch:16 step:12645 [D loss: 0.214686, acc.: 90.62%] [G loss: 4.365791]\n",
      "epoch:16 step:12646 [D loss: 0.024972, acc.: 100.00%] [G loss: 2.752654]\n",
      "epoch:16 step:12647 [D loss: 0.100999, acc.: 98.44%] [G loss: 4.775614]\n",
      "epoch:16 step:12648 [D loss: 0.022464, acc.: 99.22%] [G loss: 5.344916]\n",
      "epoch:16 step:12649 [D loss: 0.326211, acc.: 85.16%] [G loss: 3.188599]\n",
      "epoch:16 step:12650 [D loss: 0.026354, acc.: 99.22%] [G loss: 3.048525]\n",
      "epoch:16 step:12651 [D loss: 0.019778, acc.: 100.00%] [G loss: 2.441545]\n",
      "epoch:16 step:12652 [D loss: 0.017120, acc.: 100.00%] [G loss: 2.950488]\n",
      "epoch:16 step:12653 [D loss: 0.025540, acc.: 100.00%] [G loss: 2.127407]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:16 step:12654 [D loss: 0.056036, acc.: 99.22%] [G loss: 2.160170]\n",
      "epoch:16 step:12655 [D loss: 0.058323, acc.: 99.22%] [G loss: 1.468423]\n",
      "epoch:16 step:12656 [D loss: 0.022240, acc.: 100.00%] [G loss: 0.922811]\n",
      "epoch:16 step:12657 [D loss: 0.261291, acc.: 88.28%] [G loss: 2.040094]\n",
      "epoch:16 step:12658 [D loss: 0.005512, acc.: 100.00%] [G loss: 1.712288]\n",
      "epoch:16 step:12659 [D loss: 0.082541, acc.: 98.44%] [G loss: 1.507542]\n",
      "epoch:16 step:12660 [D loss: 0.065735, acc.: 99.22%] [G loss: 0.690570]\n",
      "epoch:16 step:12661 [D loss: 0.109608, acc.: 97.66%] [G loss: 3.378937]\n",
      "epoch:16 step:12662 [D loss: 0.018591, acc.: 100.00%] [G loss: 4.032578]\n",
      "epoch:16 step:12663 [D loss: 0.147585, acc.: 94.53%] [G loss: 0.944198]\n",
      "epoch:16 step:12664 [D loss: 0.423142, acc.: 75.00%] [G loss: 6.578260]\n",
      "epoch:16 step:12665 [D loss: 1.274679, acc.: 52.34%] [G loss: 3.139778]\n",
      "epoch:16 step:12666 [D loss: 0.772255, acc.: 67.97%] [G loss: 5.514344]\n",
      "epoch:16 step:12667 [D loss: 0.659907, acc.: 66.41%] [G loss: 4.924473]\n",
      "epoch:16 step:12668 [D loss: 0.311087, acc.: 86.72%] [G loss: 3.325919]\n",
      "epoch:16 step:12669 [D loss: 0.186366, acc.: 92.97%] [G loss: 4.404198]\n",
      "epoch:16 step:12670 [D loss: 0.087010, acc.: 97.66%] [G loss: 4.431779]\n",
      "epoch:16 step:12671 [D loss: 0.129390, acc.: 96.88%] [G loss: 2.704876]\n",
      "epoch:16 step:12672 [D loss: 0.281686, acc.: 84.38%] [G loss: 5.274001]\n",
      "epoch:16 step:12673 [D loss: 0.181743, acc.: 91.41%] [G loss: 3.740942]\n",
      "epoch:16 step:12674 [D loss: 0.104622, acc.: 98.44%] [G loss: 2.371850]\n",
      "epoch:16 step:12675 [D loss: 0.100244, acc.: 97.66%] [G loss: 3.640238]\n",
      "epoch:16 step:12676 [D loss: 0.086563, acc.: 100.00%] [G loss: 4.163530]\n",
      "epoch:16 step:12677 [D loss: 0.142511, acc.: 95.31%] [G loss: 2.296489]\n",
      "epoch:16 step:12678 [D loss: 0.170671, acc.: 93.75%] [G loss: 4.126798]\n",
      "epoch:16 step:12679 [D loss: 0.201563, acc.: 93.75%] [G loss: 4.586106]\n",
      "epoch:16 step:12680 [D loss: 0.075341, acc.: 98.44%] [G loss: 3.490828]\n",
      "epoch:16 step:12681 [D loss: 0.047381, acc.: 99.22%] [G loss: 3.006785]\n",
      "epoch:16 step:12682 [D loss: 0.058732, acc.: 99.22%] [G loss: 3.769256]\n",
      "epoch:16 step:12683 [D loss: 1.192779, acc.: 46.88%] [G loss: 7.375156]\n",
      "epoch:16 step:12684 [D loss: 1.289737, acc.: 54.69%] [G loss: 5.880690]\n",
      "epoch:16 step:12685 [D loss: 0.040366, acc.: 100.00%] [G loss: 4.313948]\n",
      "epoch:16 step:12686 [D loss: 0.037379, acc.: 100.00%] [G loss: 3.758845]\n",
      "epoch:16 step:12687 [D loss: 0.144038, acc.: 96.09%] [G loss: 3.412013]\n",
      "epoch:16 step:12688 [D loss: 0.019924, acc.: 100.00%] [G loss: 4.154652]\n",
      "epoch:16 step:12689 [D loss: 0.035437, acc.: 99.22%] [G loss: 4.241569]\n",
      "epoch:16 step:12690 [D loss: 0.033300, acc.: 100.00%] [G loss: 3.549422]\n",
      "epoch:16 step:12691 [D loss: 0.017537, acc.: 100.00%] [G loss: 2.625132]\n",
      "epoch:16 step:12692 [D loss: 0.087493, acc.: 100.00%] [G loss: 3.079817]\n",
      "epoch:16 step:12693 [D loss: 0.020734, acc.: 100.00%] [G loss: 4.115630]\n",
      "epoch:16 step:12694 [D loss: 0.083940, acc.: 97.66%] [G loss: 4.347995]\n",
      "epoch:16 step:12695 [D loss: 0.030706, acc.: 100.00%] [G loss: 2.992663]\n",
      "epoch:16 step:12696 [D loss: 0.047402, acc.: 100.00%] [G loss: 3.342017]\n",
      "epoch:16 step:12697 [D loss: 0.028369, acc.: 100.00%] [G loss: 3.414404]\n",
      "epoch:16 step:12698 [D loss: 0.042624, acc.: 100.00%] [G loss: 3.713768]\n",
      "epoch:16 step:12699 [D loss: 0.027212, acc.: 100.00%] [G loss: 3.837552]\n",
      "epoch:16 step:12700 [D loss: 0.049593, acc.: 100.00%] [G loss: 3.277565]\n",
      "epoch:16 step:12701 [D loss: 0.123770, acc.: 96.88%] [G loss: 3.991284]\n",
      "epoch:16 step:12702 [D loss: 0.078672, acc.: 99.22%] [G loss: 4.163201]\n",
      "epoch:16 step:12703 [D loss: 0.015977, acc.: 100.00%] [G loss: 3.522474]\n",
      "epoch:16 step:12704 [D loss: 0.062340, acc.: 100.00%] [G loss: 3.410319]\n",
      "epoch:16 step:12705 [D loss: 0.229953, acc.: 89.84%] [G loss: 4.915676]\n",
      "epoch:16 step:12706 [D loss: 0.429163, acc.: 77.34%] [G loss: 2.735632]\n",
      "epoch:16 step:12707 [D loss: 0.165822, acc.: 96.09%] [G loss: 3.776421]\n",
      "epoch:16 step:12708 [D loss: 0.029965, acc.: 99.22%] [G loss: 4.800207]\n",
      "epoch:16 step:12709 [D loss: 0.256162, acc.: 89.84%] [G loss: 3.486116]\n",
      "epoch:16 step:12710 [D loss: 0.066112, acc.: 100.00%] [G loss: 2.938826]\n",
      "epoch:16 step:12711 [D loss: 0.180235, acc.: 92.19%] [G loss: 5.599439]\n",
      "epoch:16 step:12712 [D loss: 0.028974, acc.: 99.22%] [G loss: 5.954393]\n",
      "epoch:16 step:12713 [D loss: 0.067378, acc.: 97.66%] [G loss: 5.509489]\n",
      "epoch:16 step:12714 [D loss: 0.057778, acc.: 99.22%] [G loss: 4.531984]\n",
      "epoch:16 step:12715 [D loss: 0.019615, acc.: 99.22%] [G loss: 3.924512]\n",
      "epoch:16 step:12716 [D loss: 0.049204, acc.: 99.22%] [G loss: 4.116369]\n",
      "epoch:16 step:12717 [D loss: 0.007937, acc.: 100.00%] [G loss: 4.739844]\n",
      "epoch:16 step:12718 [D loss: 0.009096, acc.: 100.00%] [G loss: 4.494762]\n",
      "epoch:16 step:12719 [D loss: 0.013959, acc.: 100.00%] [G loss: 3.666768]\n",
      "epoch:16 step:12720 [D loss: 0.052193, acc.: 100.00%] [G loss: 2.417108]\n",
      "epoch:16 step:12721 [D loss: 0.023944, acc.: 100.00%] [G loss: 2.721437]\n",
      "epoch:16 step:12722 [D loss: 0.143988, acc.: 95.31%] [G loss: 5.207659]\n",
      "epoch:16 step:12723 [D loss: 0.062185, acc.: 98.44%] [G loss: 5.862566]\n",
      "epoch:16 step:12724 [D loss: 0.044103, acc.: 99.22%] [G loss: 4.681341]\n",
      "epoch:16 step:12725 [D loss: 0.105940, acc.: 96.88%] [G loss: 5.591973]\n",
      "epoch:16 step:12726 [D loss: 0.019028, acc.: 100.00%] [G loss: 6.266428]\n",
      "epoch:16 step:12727 [D loss: 0.100365, acc.: 96.09%] [G loss: 3.270965]\n",
      "epoch:16 step:12728 [D loss: 0.055549, acc.: 100.00%] [G loss: 3.666249]\n",
      "epoch:16 step:12729 [D loss: 0.012813, acc.: 100.00%] [G loss: 4.853120]\n",
      "epoch:16 step:12730 [D loss: 0.037169, acc.: 100.00%] [G loss: 3.668170]\n",
      "epoch:16 step:12731 [D loss: 0.030288, acc.: 100.00%] [G loss: 5.079041]\n",
      "epoch:16 step:12732 [D loss: 0.052541, acc.: 99.22%] [G loss: 4.181047]\n",
      "epoch:16 step:12733 [D loss: 0.035760, acc.: 100.00%] [G loss: 5.709895]\n",
      "epoch:16 step:12734 [D loss: 0.003798, acc.: 100.00%] [G loss: 4.549487]\n",
      "epoch:16 step:12735 [D loss: 0.012347, acc.: 100.00%] [G loss: 3.459826]\n",
      "epoch:16 step:12736 [D loss: 5.971232, acc.: 21.88%] [G loss: 7.136826]\n",
      "epoch:16 step:12737 [D loss: 1.789436, acc.: 50.00%] [G loss: 4.662051]\n",
      "epoch:16 step:12738 [D loss: 0.448634, acc.: 79.69%] [G loss: 3.016442]\n",
      "epoch:16 step:12739 [D loss: 0.244850, acc.: 88.28%] [G loss: 2.660767]\n",
      "epoch:16 step:12740 [D loss: 0.036347, acc.: 100.00%] [G loss: 2.991637]\n",
      "epoch:16 step:12741 [D loss: 0.065332, acc.: 99.22%] [G loss: 3.179670]\n",
      "epoch:16 step:12742 [D loss: 0.174862, acc.: 94.53%] [G loss: 2.190757]\n",
      "epoch:16 step:12743 [D loss: 0.160733, acc.: 96.09%] [G loss: 2.169000]\n",
      "epoch:16 step:12744 [D loss: 0.052857, acc.: 100.00%] [G loss: 2.856493]\n",
      "epoch:16 step:12745 [D loss: 0.153757, acc.: 96.09%] [G loss: 1.485681]\n",
      "epoch:16 step:12746 [D loss: 0.157199, acc.: 93.75%] [G loss: 0.561764]\n",
      "epoch:16 step:12747 [D loss: 0.159195, acc.: 93.75%] [G loss: 2.398055]\n",
      "epoch:16 step:12748 [D loss: 0.097738, acc.: 97.66%] [G loss: 2.064691]\n",
      "epoch:16 step:12749 [D loss: 0.059634, acc.: 98.44%] [G loss: 2.047169]\n",
      "epoch:16 step:12750 [D loss: 0.072514, acc.: 100.00%] [G loss: 2.785975]\n",
      "epoch:16 step:12751 [D loss: 0.024769, acc.: 100.00%] [G loss: 2.641005]\n",
      "epoch:16 step:12752 [D loss: 0.117516, acc.: 99.22%] [G loss: 1.115665]\n",
      "epoch:16 step:12753 [D loss: 0.025997, acc.: 100.00%] [G loss: 1.307407]\n",
      "epoch:16 step:12754 [D loss: 0.189199, acc.: 95.31%] [G loss: 3.127100]\n",
      "epoch:16 step:12755 [D loss: 0.252661, acc.: 90.62%] [G loss: 1.584751]\n",
      "epoch:16 step:12756 [D loss: 1.072051, acc.: 53.91%] [G loss: 6.456599]\n",
      "epoch:16 step:12757 [D loss: 2.366258, acc.: 50.00%] [G loss: 4.813915]\n",
      "epoch:16 step:12758 [D loss: 1.401206, acc.: 50.78%] [G loss: 1.906246]\n",
      "epoch:16 step:12759 [D loss: 0.401413, acc.: 79.69%] [G loss: 1.287787]\n",
      "epoch:16 step:12760 [D loss: 0.548413, acc.: 76.56%] [G loss: 1.870369]\n",
      "epoch:16 step:12761 [D loss: 0.239148, acc.: 90.62%] [G loss: 2.320617]\n",
      "epoch:16 step:12762 [D loss: 0.146003, acc.: 97.66%] [G loss: 1.769382]\n",
      "epoch:16 step:12763 [D loss: 0.336490, acc.: 85.94%] [G loss: 1.228209]\n",
      "epoch:16 step:12764 [D loss: 0.247493, acc.: 90.62%] [G loss: 2.208734]\n",
      "epoch:16 step:12765 [D loss: 0.380687, acc.: 84.38%] [G loss: 2.484318]\n",
      "epoch:16 step:12766 [D loss: 0.148581, acc.: 96.09%] [G loss: 2.675186]\n",
      "epoch:16 step:12767 [D loss: 0.248190, acc.: 92.19%] [G loss: 2.262195]\n",
      "epoch:16 step:12768 [D loss: 0.108049, acc.: 99.22%] [G loss: 3.516172]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:16 step:12769 [D loss: 0.096723, acc.: 98.44%] [G loss: 2.406011]\n",
      "epoch:16 step:12770 [D loss: 0.148411, acc.: 97.66%] [G loss: 2.170921]\n",
      "epoch:16 step:12771 [D loss: 0.155020, acc.: 98.44%] [G loss: 2.692992]\n",
      "epoch:16 step:12772 [D loss: 0.112775, acc.: 98.44%] [G loss: 2.834293]\n",
      "epoch:16 step:12773 [D loss: 0.094362, acc.: 99.22%] [G loss: 3.017766]\n",
      "epoch:16 step:12774 [D loss: 0.050458, acc.: 100.00%] [G loss: 1.390282]\n",
      "epoch:16 step:12775 [D loss: 0.095036, acc.: 97.66%] [G loss: 0.522825]\n",
      "epoch:16 step:12776 [D loss: 0.076478, acc.: 99.22%] [G loss: 1.150222]\n",
      "epoch:16 step:12777 [D loss: 0.116912, acc.: 98.44%] [G loss: 1.526994]\n",
      "epoch:16 step:12778 [D loss: 0.029060, acc.: 100.00%] [G loss: 3.368888]\n",
      "epoch:16 step:12779 [D loss: 0.287297, acc.: 85.94%] [G loss: 0.228751]\n",
      "epoch:16 step:12780 [D loss: 0.077815, acc.: 100.00%] [G loss: 0.512410]\n",
      "epoch:16 step:12781 [D loss: 0.047722, acc.: 100.00%] [G loss: 1.537087]\n",
      "epoch:16 step:12782 [D loss: 0.067356, acc.: 99.22%] [G loss: 1.435088]\n",
      "epoch:16 step:12783 [D loss: 0.232331, acc.: 89.84%] [G loss: 3.256830]\n",
      "epoch:16 step:12784 [D loss: 0.057994, acc.: 99.22%] [G loss: 4.441263]\n",
      "epoch:16 step:12785 [D loss: 0.172399, acc.: 92.97%] [G loss: 3.232389]\n",
      "epoch:16 step:12786 [D loss: 0.109416, acc.: 100.00%] [G loss: 1.718574]\n",
      "epoch:16 step:12787 [D loss: 0.101830, acc.: 98.44%] [G loss: 1.106019]\n",
      "epoch:16 step:12788 [D loss: 0.056327, acc.: 100.00%] [G loss: 1.567701]\n",
      "epoch:16 step:12789 [D loss: 0.023789, acc.: 100.00%] [G loss: 1.949806]\n",
      "epoch:16 step:12790 [D loss: 0.021180, acc.: 100.00%] [G loss: 1.340994]\n",
      "epoch:16 step:12791 [D loss: 0.378532, acc.: 84.38%] [G loss: 4.199025]\n",
      "epoch:16 step:12792 [D loss: 0.455696, acc.: 72.66%] [G loss: 2.565019]\n",
      "epoch:16 step:12793 [D loss: 0.244912, acc.: 87.50%] [G loss: 4.137450]\n",
      "epoch:16 step:12794 [D loss: 0.027443, acc.: 99.22%] [G loss: 5.829739]\n",
      "epoch:16 step:12795 [D loss: 0.149818, acc.: 95.31%] [G loss: 3.848487]\n",
      "epoch:16 step:12796 [D loss: 0.023903, acc.: 100.00%] [G loss: 3.918045]\n",
      "epoch:16 step:12797 [D loss: 0.049135, acc.: 100.00%] [G loss: 4.076259]\n",
      "epoch:16 step:12798 [D loss: 0.016863, acc.: 100.00%] [G loss: 4.114426]\n",
      "epoch:16 step:12799 [D loss: 0.067635, acc.: 100.00%] [G loss: 3.876046]\n",
      "epoch:16 step:12800 [D loss: 0.057629, acc.: 100.00%] [G loss: 3.707650]\n",
      "##############\n",
      "[1.0005735  0.99415312 0.996659   1.03878281 2.11681066 0.71534297\n",
      " 2.11179178 1.03188598 1.064777   0.94723728]\n",
      "##########\n",
      "epoch:16 step:12801 [D loss: 0.069291, acc.: 98.44%] [G loss: 3.133151]\n",
      "epoch:16 step:12802 [D loss: 0.031643, acc.: 100.00%] [G loss: 3.350059]\n",
      "epoch:16 step:12803 [D loss: 0.066393, acc.: 99.22%] [G loss: 3.453722]\n",
      "epoch:16 step:12804 [D loss: 0.022249, acc.: 100.00%] [G loss: 2.480815]\n",
      "epoch:16 step:12805 [D loss: 0.245454, acc.: 91.41%] [G loss: 4.231462]\n",
      "epoch:16 step:12806 [D loss: 0.129008, acc.: 95.31%] [G loss: 4.552111]\n",
      "epoch:16 step:12807 [D loss: 0.021122, acc.: 100.00%] [G loss: 2.984000]\n",
      "epoch:16 step:12808 [D loss: 0.078840, acc.: 98.44%] [G loss: 2.244334]\n",
      "epoch:16 step:12809 [D loss: 0.060812, acc.: 100.00%] [G loss: 2.581883]\n",
      "epoch:16 step:12810 [D loss: 0.050811, acc.: 99.22%] [G loss: 2.557116]\n",
      "epoch:16 step:12811 [D loss: 0.248501, acc.: 92.97%] [G loss: 2.766392]\n",
      "epoch:16 step:12812 [D loss: 0.047961, acc.: 100.00%] [G loss: 3.468865]\n",
      "epoch:16 step:12813 [D loss: 0.014860, acc.: 100.00%] [G loss: 4.232676]\n",
      "epoch:16 step:12814 [D loss: 0.024104, acc.: 100.00%] [G loss: 3.763212]\n",
      "epoch:16 step:12815 [D loss: 0.013624, acc.: 100.00%] [G loss: 3.971965]\n",
      "epoch:16 step:12816 [D loss: 1.144746, acc.: 42.97%] [G loss: 5.982127]\n",
      "epoch:16 step:12817 [D loss: 1.702427, acc.: 50.00%] [G loss: 4.426137]\n",
      "epoch:16 step:12818 [D loss: 0.522992, acc.: 71.88%] [G loss: 1.633058]\n",
      "epoch:16 step:12819 [D loss: 0.348470, acc.: 82.81%] [G loss: 3.584673]\n",
      "epoch:16 step:12820 [D loss: 0.045363, acc.: 100.00%] [G loss: 3.178716]\n",
      "epoch:16 step:12821 [D loss: 0.182540, acc.: 95.31%] [G loss: 3.056974]\n",
      "epoch:16 step:12822 [D loss: 0.086978, acc.: 99.22%] [G loss: 2.283834]\n",
      "epoch:16 step:12823 [D loss: 0.061795, acc.: 100.00%] [G loss: 2.144462]\n",
      "epoch:16 step:12824 [D loss: 0.065753, acc.: 100.00%] [G loss: 2.568468]\n",
      "epoch:16 step:12825 [D loss: 0.094372, acc.: 98.44%] [G loss: 3.129528]\n",
      "epoch:16 step:12826 [D loss: 0.089866, acc.: 98.44%] [G loss: 3.190995]\n",
      "epoch:16 step:12827 [D loss: 0.353016, acc.: 85.94%] [G loss: 3.757488]\n",
      "epoch:16 step:12828 [D loss: 0.057755, acc.: 99.22%] [G loss: 3.742345]\n",
      "epoch:16 step:12829 [D loss: 0.037718, acc.: 100.00%] [G loss: 2.703966]\n",
      "epoch:16 step:12830 [D loss: 0.049079, acc.: 100.00%] [G loss: 2.788379]\n",
      "epoch:16 step:12831 [D loss: 0.078337, acc.: 100.00%] [G loss: 2.786820]\n",
      "epoch:16 step:12832 [D loss: 0.165178, acc.: 94.53%] [G loss: 2.470951]\n",
      "epoch:16 step:12833 [D loss: 0.257605, acc.: 87.50%] [G loss: 4.552940]\n",
      "epoch:16 step:12834 [D loss: 0.365689, acc.: 78.91%] [G loss: 2.367846]\n",
      "epoch:16 step:12835 [D loss: 0.126508, acc.: 96.88%] [G loss: 3.552238]\n",
      "epoch:16 step:12836 [D loss: 0.053331, acc.: 99.22%] [G loss: 3.769982]\n",
      "epoch:16 step:12837 [D loss: 0.021386, acc.: 100.00%] [G loss: 4.309617]\n",
      "epoch:16 step:12838 [D loss: 0.021698, acc.: 100.00%] [G loss: 3.815531]\n",
      "epoch:16 step:12839 [D loss: 0.132113, acc.: 96.09%] [G loss: 3.003223]\n",
      "epoch:16 step:12840 [D loss: 0.063626, acc.: 100.00%] [G loss: 3.230967]\n",
      "epoch:16 step:12841 [D loss: 0.054284, acc.: 100.00%] [G loss: 4.119684]\n",
      "epoch:16 step:12842 [D loss: 0.069411, acc.: 100.00%] [G loss: 3.779287]\n",
      "epoch:16 step:12843 [D loss: 0.025987, acc.: 100.00%] [G loss: 3.385846]\n",
      "epoch:16 step:12844 [D loss: 0.043152, acc.: 99.22%] [G loss: 3.637206]\n",
      "epoch:16 step:12845 [D loss: 0.021450, acc.: 100.00%] [G loss: 3.866507]\n",
      "epoch:16 step:12846 [D loss: 0.144856, acc.: 96.09%] [G loss: 3.667873]\n",
      "epoch:16 step:12847 [D loss: 0.046142, acc.: 100.00%] [G loss: 3.977834]\n",
      "epoch:16 step:12848 [D loss: 0.027225, acc.: 100.00%] [G loss: 4.102297]\n",
      "epoch:16 step:12849 [D loss: 0.036616, acc.: 100.00%] [G loss: 4.294071]\n",
      "epoch:16 step:12850 [D loss: 0.064992, acc.: 100.00%] [G loss: 3.032782]\n",
      "epoch:16 step:12851 [D loss: 0.148216, acc.: 96.09%] [G loss: 3.346170]\n",
      "epoch:16 step:12852 [D loss: 0.244244, acc.: 89.84%] [G loss: 5.344949]\n",
      "epoch:16 step:12853 [D loss: 0.065300, acc.: 99.22%] [G loss: 6.189778]\n",
      "epoch:16 step:12854 [D loss: 0.232411, acc.: 88.28%] [G loss: 4.043640]\n",
      "epoch:16 step:12855 [D loss: 0.052372, acc.: 99.22%] [G loss: 3.542809]\n",
      "epoch:16 step:12856 [D loss: 0.047019, acc.: 100.00%] [G loss: 3.854107]\n",
      "epoch:16 step:12857 [D loss: 0.020027, acc.: 100.00%] [G loss: 4.457300]\n",
      "epoch:16 step:12858 [D loss: 0.039423, acc.: 100.00%] [G loss: 4.417831]\n",
      "epoch:16 step:12859 [D loss: 0.018955, acc.: 100.00%] [G loss: 3.842347]\n",
      "epoch:16 step:12860 [D loss: 0.078723, acc.: 97.66%] [G loss: 3.918674]\n",
      "epoch:16 step:12861 [D loss: 0.064813, acc.: 98.44%] [G loss: 3.874009]\n",
      "epoch:16 step:12862 [D loss: 0.040392, acc.: 99.22%] [G loss: 4.179446]\n",
      "epoch:16 step:12863 [D loss: 0.019410, acc.: 100.00%] [G loss: 4.132028]\n",
      "epoch:16 step:12864 [D loss: 0.014529, acc.: 100.00%] [G loss: 4.377525]\n",
      "epoch:16 step:12865 [D loss: 0.029711, acc.: 100.00%] [G loss: 4.052547]\n",
      "epoch:16 step:12866 [D loss: 0.657359, acc.: 61.72%] [G loss: 6.263957]\n",
      "epoch:16 step:12867 [D loss: 0.020030, acc.: 100.00%] [G loss: 6.857704]\n",
      "epoch:16 step:12868 [D loss: 0.297657, acc.: 85.16%] [G loss: 2.650115]\n",
      "epoch:16 step:12869 [D loss: 0.136752, acc.: 94.53%] [G loss: 4.220644]\n",
      "epoch:16 step:12870 [D loss: 0.021656, acc.: 100.00%] [G loss: 4.734378]\n",
      "epoch:16 step:12871 [D loss: 0.172681, acc.: 95.31%] [G loss: 3.463022]\n",
      "epoch:16 step:12872 [D loss: 0.080499, acc.: 99.22%] [G loss: 3.370766]\n",
      "epoch:16 step:12873 [D loss: 0.149718, acc.: 92.97%] [G loss: 5.508707]\n",
      "epoch:16 step:12874 [D loss: 0.020886, acc.: 100.00%] [G loss: 6.400421]\n",
      "epoch:16 step:12875 [D loss: 0.172219, acc.: 90.62%] [G loss: 1.735326]\n",
      "epoch:16 step:12876 [D loss: 0.228058, acc.: 91.41%] [G loss: 5.928372]\n",
      "epoch:16 step:12877 [D loss: 0.006079, acc.: 100.00%] [G loss: 6.983530]\n",
      "epoch:16 step:12878 [D loss: 0.641203, acc.: 70.31%] [G loss: 2.763297]\n",
      "epoch:16 step:12879 [D loss: 0.276138, acc.: 86.72%] [G loss: 4.232574]\n",
      "epoch:16 step:12880 [D loss: 0.006918, acc.: 100.00%] [G loss: 5.702197]\n",
      "epoch:16 step:12881 [D loss: 0.078856, acc.: 97.66%] [G loss: 5.161683]\n",
      "epoch:16 step:12882 [D loss: 0.052952, acc.: 98.44%] [G loss: 4.418033]\n",
      "epoch:16 step:12883 [D loss: 0.044984, acc.: 99.22%] [G loss: 3.874530]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:16 step:12884 [D loss: 0.028999, acc.: 100.00%] [G loss: 4.719973]\n",
      "epoch:16 step:12885 [D loss: 0.008101, acc.: 100.00%] [G loss: 4.328243]\n",
      "epoch:16 step:12886 [D loss: 0.018430, acc.: 100.00%] [G loss: 3.480838]\n",
      "epoch:16 step:12887 [D loss: 0.047036, acc.: 100.00%] [G loss: 3.299630]\n",
      "epoch:16 step:12888 [D loss: 0.012955, acc.: 100.00%] [G loss: 2.863484]\n",
      "epoch:16 step:12889 [D loss: 0.134859, acc.: 96.88%] [G loss: 4.633989]\n",
      "epoch:16 step:12890 [D loss: 0.051719, acc.: 99.22%] [G loss: 4.932275]\n",
      "epoch:16 step:12891 [D loss: 0.718082, acc.: 67.19%] [G loss: 4.682239]\n",
      "epoch:16 step:12892 [D loss: 0.079086, acc.: 96.88%] [G loss: 4.288439]\n",
      "epoch:16 step:12893 [D loss: 0.083419, acc.: 97.66%] [G loss: 4.407283]\n",
      "epoch:16 step:12894 [D loss: 0.026796, acc.: 99.22%] [G loss: 3.818922]\n",
      "epoch:16 step:12895 [D loss: 0.136485, acc.: 96.88%] [G loss: 3.860163]\n",
      "epoch:16 step:12896 [D loss: 0.024190, acc.: 100.00%] [G loss: 4.899972]\n",
      "epoch:16 step:12897 [D loss: 0.350712, acc.: 83.59%] [G loss: 1.845808]\n",
      "epoch:16 step:12898 [D loss: 0.268113, acc.: 92.97%] [G loss: 5.814946]\n",
      "epoch:16 step:12899 [D loss: 0.415080, acc.: 76.56%] [G loss: 2.545356]\n",
      "epoch:16 step:12900 [D loss: 0.876414, acc.: 64.06%] [G loss: 7.319724]\n",
      "epoch:16 step:12901 [D loss: 1.398001, acc.: 53.91%] [G loss: 5.265300]\n",
      "epoch:16 step:12902 [D loss: 0.465915, acc.: 82.81%] [G loss: 1.906444]\n",
      "epoch:16 step:12903 [D loss: 0.833853, acc.: 60.94%] [G loss: 5.398978]\n",
      "epoch:16 step:12904 [D loss: 0.315186, acc.: 83.59%] [G loss: 5.329862]\n",
      "epoch:16 step:12905 [D loss: 0.918458, acc.: 57.81%] [G loss: 2.127471]\n",
      "epoch:16 step:12906 [D loss: 0.243341, acc.: 85.16%] [G loss: 3.588472]\n",
      "epoch:16 step:12907 [D loss: 0.102696, acc.: 98.44%] [G loss: 3.516814]\n",
      "epoch:16 step:12908 [D loss: 0.039486, acc.: 100.00%] [G loss: 3.550527]\n",
      "epoch:16 step:12909 [D loss: 0.500776, acc.: 73.44%] [G loss: 0.885360]\n",
      "epoch:16 step:12910 [D loss: 0.674618, acc.: 66.41%] [G loss: 4.411074]\n",
      "epoch:16 step:12911 [D loss: 0.113046, acc.: 96.09%] [G loss: 5.153400]\n",
      "epoch:16 step:12912 [D loss: 0.276510, acc.: 83.59%] [G loss: 2.493812]\n",
      "epoch:16 step:12913 [D loss: 0.049446, acc.: 100.00%] [G loss: 1.627399]\n",
      "epoch:16 step:12914 [D loss: 0.671316, acc.: 72.66%] [G loss: 4.092060]\n",
      "epoch:16 step:12915 [D loss: 0.475892, acc.: 75.78%] [G loss: 3.979889]\n",
      "epoch:16 step:12916 [D loss: 0.284205, acc.: 85.94%] [G loss: 2.187637]\n",
      "epoch:16 step:12917 [D loss: 0.130065, acc.: 95.31%] [G loss: 0.669248]\n",
      "epoch:16 step:12918 [D loss: 0.317220, acc.: 87.50%] [G loss: 3.406329]\n",
      "epoch:16 step:12919 [D loss: 0.153604, acc.: 92.19%] [G loss: 3.334164]\n",
      "epoch:16 step:12920 [D loss: 0.176645, acc.: 93.75%] [G loss: 0.894472]\n",
      "epoch:16 step:12921 [D loss: 0.133487, acc.: 95.31%] [G loss: 2.313931]\n",
      "epoch:16 step:12922 [D loss: 0.027029, acc.: 100.00%] [G loss: 2.054307]\n",
      "epoch:16 step:12923 [D loss: 0.109452, acc.: 98.44%] [G loss: 2.580438]\n",
      "epoch:16 step:12924 [D loss: 0.493854, acc.: 73.44%] [G loss: 3.779226]\n",
      "epoch:16 step:12925 [D loss: 0.277922, acc.: 85.94%] [G loss: 3.433210]\n",
      "epoch:16 step:12926 [D loss: 0.460172, acc.: 79.69%] [G loss: 2.326341]\n",
      "epoch:16 step:12927 [D loss: 0.202733, acc.: 90.62%] [G loss: 3.811199]\n",
      "epoch:16 step:12928 [D loss: 0.059891, acc.: 100.00%] [G loss: 4.101611]\n",
      "epoch:16 step:12929 [D loss: 0.026370, acc.: 100.00%] [G loss: 4.327700]\n",
      "epoch:16 step:12930 [D loss: 0.193175, acc.: 92.97%] [G loss: 3.217274]\n",
      "epoch:16 step:12931 [D loss: 0.035654, acc.: 100.00%] [G loss: 2.987469]\n",
      "epoch:16 step:12932 [D loss: 0.173395, acc.: 94.53%] [G loss: 3.973345]\n",
      "epoch:16 step:12933 [D loss: 0.053479, acc.: 100.00%] [G loss: 4.249932]\n",
      "epoch:16 step:12934 [D loss: 0.081633, acc.: 97.66%] [G loss: 3.507075]\n",
      "epoch:16 step:12935 [D loss: 0.062805, acc.: 100.00%] [G loss: 3.507299]\n",
      "epoch:16 step:12936 [D loss: 0.110368, acc.: 98.44%] [G loss: 3.620306]\n",
      "epoch:16 step:12937 [D loss: 0.177016, acc.: 94.53%] [G loss: 3.791075]\n",
      "epoch:16 step:12938 [D loss: 0.015569, acc.: 100.00%] [G loss: 4.281641]\n",
      "epoch:16 step:12939 [D loss: 0.098490, acc.: 98.44%] [G loss: 2.943143]\n",
      "epoch:16 step:12940 [D loss: 0.021473, acc.: 100.00%] [G loss: 2.058769]\n",
      "epoch:16 step:12941 [D loss: 0.031804, acc.: 100.00%] [G loss: 2.917120]\n",
      "epoch:16 step:12942 [D loss: 0.054694, acc.: 99.22%] [G loss: 1.448838]\n",
      "epoch:16 step:12943 [D loss: 0.062759, acc.: 99.22%] [G loss: 0.942881]\n",
      "epoch:16 step:12944 [D loss: 0.021055, acc.: 100.00%] [G loss: 1.734555]\n",
      "epoch:16 step:12945 [D loss: 0.015288, acc.: 100.00%] [G loss: 1.276100]\n",
      "epoch:16 step:12946 [D loss: 0.019065, acc.: 100.00%] [G loss: 2.100434]\n",
      "epoch:16 step:12947 [D loss: 0.067410, acc.: 100.00%] [G loss: 0.776335]\n",
      "epoch:16 step:12948 [D loss: 0.013908, acc.: 100.00%] [G loss: 0.586125]\n",
      "epoch:16 step:12949 [D loss: 0.091040, acc.: 96.88%] [G loss: 1.567597]\n",
      "epoch:16 step:12950 [D loss: 0.016204, acc.: 100.00%] [G loss: 3.727018]\n",
      "epoch:16 step:12951 [D loss: 0.076791, acc.: 97.66%] [G loss: 1.442495]\n",
      "epoch:16 step:12952 [D loss: 0.030358, acc.: 100.00%] [G loss: 2.045907]\n",
      "epoch:16 step:12953 [D loss: 0.053138, acc.: 100.00%] [G loss: 3.304673]\n",
      "epoch:16 step:12954 [D loss: 0.020853, acc.: 100.00%] [G loss: 2.233022]\n",
      "epoch:16 step:12955 [D loss: 0.815982, acc.: 52.34%] [G loss: 5.766694]\n",
      "epoch:16 step:12956 [D loss: 0.423368, acc.: 79.69%] [G loss: 4.735054]\n",
      "epoch:16 step:12957 [D loss: 0.088538, acc.: 98.44%] [G loss: 3.392639]\n",
      "epoch:16 step:12958 [D loss: 0.050309, acc.: 100.00%] [G loss: 2.948759]\n",
      "epoch:16 step:12959 [D loss: 0.063089, acc.: 98.44%] [G loss: 3.612972]\n",
      "epoch:16 step:12960 [D loss: 0.057150, acc.: 99.22%] [G loss: 4.833906]\n",
      "epoch:16 step:12961 [D loss: 0.128970, acc.: 96.88%] [G loss: 4.819558]\n",
      "epoch:16 step:12962 [D loss: 0.024698, acc.: 100.00%] [G loss: 3.636206]\n",
      "epoch:16 step:12963 [D loss: 0.045500, acc.: 100.00%] [G loss: 3.671814]\n",
      "epoch:16 step:12964 [D loss: 0.231840, acc.: 92.19%] [G loss: 4.461086]\n",
      "epoch:16 step:12965 [D loss: 0.033516, acc.: 99.22%] [G loss: 4.879852]\n",
      "epoch:16 step:12966 [D loss: 0.010445, acc.: 100.00%] [G loss: 4.859991]\n",
      "epoch:16 step:12967 [D loss: 0.058353, acc.: 99.22%] [G loss: 3.825992]\n",
      "epoch:16 step:12968 [D loss: 0.013777, acc.: 100.00%] [G loss: 2.854952]\n",
      "epoch:16 step:12969 [D loss: 0.019566, acc.: 100.00%] [G loss: 3.108286]\n",
      "epoch:16 step:12970 [D loss: 0.068881, acc.: 99.22%] [G loss: 4.580033]\n",
      "epoch:16 step:12971 [D loss: 0.048016, acc.: 99.22%] [G loss: 2.528961]\n",
      "epoch:16 step:12972 [D loss: 0.053407, acc.: 99.22%] [G loss: 4.424304]\n",
      "epoch:16 step:12973 [D loss: 0.007362, acc.: 100.00%] [G loss: 3.963232]\n",
      "epoch:16 step:12974 [D loss: 0.034165, acc.: 99.22%] [G loss: 3.583297]\n",
      "epoch:16 step:12975 [D loss: 0.083583, acc.: 97.66%] [G loss: 4.493773]\n",
      "epoch:16 step:12976 [D loss: 0.021589, acc.: 99.22%] [G loss: 4.462020]\n",
      "epoch:16 step:12977 [D loss: 0.029075, acc.: 100.00%] [G loss: 4.144290]\n",
      "epoch:16 step:12978 [D loss: 0.210091, acc.: 91.41%] [G loss: 6.506011]\n",
      "epoch:16 step:12979 [D loss: 0.330847, acc.: 83.59%] [G loss: 3.287016]\n",
      "epoch:16 step:12980 [D loss: 0.046520, acc.: 99.22%] [G loss: 2.251922]\n",
      "epoch:16 step:12981 [D loss: 0.145551, acc.: 95.31%] [G loss: 5.573646]\n",
      "epoch:16 step:12982 [D loss: 0.099239, acc.: 96.88%] [G loss: 4.287805]\n",
      "epoch:16 step:12983 [D loss: 0.013392, acc.: 100.00%] [G loss: 3.880411]\n",
      "epoch:16 step:12984 [D loss: 0.080666, acc.: 98.44%] [G loss: 5.689065]\n",
      "epoch:16 step:12985 [D loss: 0.051087, acc.: 98.44%] [G loss: 5.096593]\n",
      "epoch:16 step:12986 [D loss: 0.018569, acc.: 100.00%] [G loss: 4.127059]\n",
      "epoch:16 step:12987 [D loss: 0.224793, acc.: 91.41%] [G loss: 1.686455]\n",
      "epoch:16 step:12988 [D loss: 0.084430, acc.: 97.66%] [G loss: 5.201437]\n",
      "epoch:16 step:12989 [D loss: 0.010713, acc.: 100.00%] [G loss: 6.459476]\n",
      "epoch:16 step:12990 [D loss: 0.008887, acc.: 100.00%] [G loss: 5.385915]\n",
      "epoch:16 step:12991 [D loss: 0.048853, acc.: 98.44%] [G loss: 4.922829]\n",
      "epoch:16 step:12992 [D loss: 0.030021, acc.: 98.44%] [G loss: 4.899701]\n",
      "epoch:16 step:12993 [D loss: 0.025608, acc.: 100.00%] [G loss: 4.861801]\n",
      "epoch:16 step:12994 [D loss: 0.075566, acc.: 98.44%] [G loss: 3.741912]\n",
      "epoch:16 step:12995 [D loss: 0.084978, acc.: 98.44%] [G loss: 5.132446]\n",
      "epoch:16 step:12996 [D loss: 0.008732, acc.: 100.00%] [G loss: 4.629459]\n",
      "epoch:16 step:12997 [D loss: 0.489873, acc.: 75.78%] [G loss: 7.036898]\n",
      "epoch:16 step:12998 [D loss: 0.110586, acc.: 96.88%] [G loss: 7.133800]\n",
      "epoch:16 step:12999 [D loss: 0.032930, acc.: 99.22%] [G loss: 6.072479]\n",
      "epoch:16 step:13000 [D loss: 0.007536, acc.: 100.00%] [G loss: 5.054502]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##############\n",
      "[0.85529759 1.03239604 1.07359221 1.12069069 0.78929051 0.93404192\n",
      " 2.11003239 2.11602171 0.98327565 0.96919332]\n",
      "##########\n",
      "epoch:16 step:13001 [D loss: 0.007144, acc.: 100.00%] [G loss: 2.546522]\n",
      "epoch:16 step:13002 [D loss: 0.117794, acc.: 93.75%] [G loss: 6.254948]\n",
      "epoch:16 step:13003 [D loss: 0.157012, acc.: 96.88%] [G loss: 5.949559]\n",
      "epoch:16 step:13004 [D loss: 0.110119, acc.: 96.88%] [G loss: 3.974336]\n",
      "epoch:16 step:13005 [D loss: 0.395976, acc.: 83.59%] [G loss: 7.906406]\n",
      "epoch:16 step:13006 [D loss: 0.498605, acc.: 73.44%] [G loss: 3.506652]\n",
      "epoch:16 step:13007 [D loss: 0.208334, acc.: 89.06%] [G loss: 5.771963]\n",
      "epoch:16 step:13008 [D loss: 0.013707, acc.: 100.00%] [G loss: 6.916680]\n",
      "epoch:16 step:13009 [D loss: 0.518891, acc.: 77.34%] [G loss: 4.117775]\n",
      "epoch:16 step:13010 [D loss: 0.017759, acc.: 100.00%] [G loss: 5.420127]\n",
      "epoch:16 step:13011 [D loss: 0.040834, acc.: 99.22%] [G loss: 6.219513]\n",
      "epoch:16 step:13012 [D loss: 0.093975, acc.: 96.88%] [G loss: 6.276601]\n",
      "epoch:16 step:13013 [D loss: 0.017352, acc.: 100.00%] [G loss: 6.183468]\n",
      "epoch:16 step:13014 [D loss: 0.126146, acc.: 95.31%] [G loss: 2.244178]\n",
      "epoch:16 step:13015 [D loss: 0.180808, acc.: 91.41%] [G loss: 5.244796]\n",
      "epoch:16 step:13016 [D loss: 0.150317, acc.: 91.41%] [G loss: 4.668211]\n",
      "epoch:16 step:13017 [D loss: 0.055960, acc.: 98.44%] [G loss: 4.459147]\n",
      "epoch:16 step:13018 [D loss: 0.017127, acc.: 100.00%] [G loss: 3.865283]\n",
      "epoch:16 step:13019 [D loss: 0.198004, acc.: 92.97%] [G loss: 4.056926]\n",
      "epoch:16 step:13020 [D loss: 0.029325, acc.: 100.00%] [G loss: 5.186429]\n",
      "epoch:16 step:13021 [D loss: 0.061204, acc.: 98.44%] [G loss: 3.381039]\n",
      "epoch:16 step:13022 [D loss: 0.085417, acc.: 98.44%] [G loss: 2.948700]\n",
      "epoch:16 step:13023 [D loss: 0.098562, acc.: 96.88%] [G loss: 6.045093]\n",
      "epoch:16 step:13024 [D loss: 0.074490, acc.: 98.44%] [G loss: 3.703342]\n",
      "epoch:16 step:13025 [D loss: 0.018526, acc.: 100.00%] [G loss: 3.184472]\n",
      "epoch:16 step:13026 [D loss: 0.010912, acc.: 100.00%] [G loss: 2.652731]\n",
      "epoch:16 step:13027 [D loss: 0.002905, acc.: 100.00%] [G loss: 2.913848]\n",
      "epoch:16 step:13028 [D loss: 0.753542, acc.: 64.84%] [G loss: 9.664503]\n",
      "epoch:16 step:13029 [D loss: 2.775547, acc.: 50.00%] [G loss: 5.696148]\n",
      "epoch:16 step:13030 [D loss: 0.695009, acc.: 78.91%] [G loss: 0.807998]\n",
      "epoch:16 step:13031 [D loss: 0.380031, acc.: 84.38%] [G loss: 3.051639]\n",
      "epoch:16 step:13032 [D loss: 0.069918, acc.: 98.44%] [G loss: 2.675536]\n",
      "epoch:16 step:13033 [D loss: 0.443375, acc.: 75.78%] [G loss: 2.558213]\n",
      "epoch:16 step:13034 [D loss: 0.062947, acc.: 99.22%] [G loss: 2.212192]\n",
      "epoch:16 step:13035 [D loss: 0.041385, acc.: 100.00%] [G loss: 3.600687]\n",
      "epoch:16 step:13036 [D loss: 0.126274, acc.: 98.44%] [G loss: 3.084083]\n",
      "epoch:16 step:13037 [D loss: 0.111736, acc.: 99.22%] [G loss: 3.285958]\n",
      "epoch:16 step:13038 [D loss: 0.887768, acc.: 53.12%] [G loss: 2.376754]\n",
      "epoch:16 step:13039 [D loss: 0.118816, acc.: 96.88%] [G loss: 3.661224]\n",
      "epoch:16 step:13040 [D loss: 0.053677, acc.: 99.22%] [G loss: 4.812176]\n",
      "epoch:16 step:13041 [D loss: 0.027000, acc.: 100.00%] [G loss: 3.481854]\n",
      "epoch:16 step:13042 [D loss: 0.043946, acc.: 100.00%] [G loss: 3.275011]\n",
      "epoch:16 step:13043 [D loss: 0.093468, acc.: 98.44%] [G loss: 1.136742]\n",
      "epoch:16 step:13044 [D loss: 0.326142, acc.: 86.72%] [G loss: 5.980237]\n",
      "epoch:16 step:13045 [D loss: 0.209187, acc.: 89.84%] [G loss: 5.819603]\n",
      "epoch:16 step:13046 [D loss: 0.247999, acc.: 89.06%] [G loss: 2.335237]\n",
      "epoch:16 step:13047 [D loss: 0.187625, acc.: 90.62%] [G loss: 4.446767]\n",
      "epoch:16 step:13048 [D loss: 0.004298, acc.: 100.00%] [G loss: 5.541307]\n",
      "epoch:16 step:13049 [D loss: 0.003866, acc.: 100.00%] [G loss: 5.867063]\n",
      "epoch:16 step:13050 [D loss: 0.056402, acc.: 98.44%] [G loss: 4.817209]\n",
      "epoch:16 step:13051 [D loss: 0.015923, acc.: 100.00%] [G loss: 4.168329]\n",
      "epoch:16 step:13052 [D loss: 0.008504, acc.: 100.00%] [G loss: 3.768881]\n",
      "epoch:16 step:13053 [D loss: 0.013439, acc.: 100.00%] [G loss: 3.507402]\n",
      "epoch:16 step:13054 [D loss: 0.025704, acc.: 100.00%] [G loss: 3.119405]\n",
      "epoch:16 step:13055 [D loss: 0.028623, acc.: 100.00%] [G loss: 2.332371]\n",
      "epoch:16 step:13056 [D loss: 0.050903, acc.: 99.22%] [G loss: 3.273708]\n",
      "epoch:16 step:13057 [D loss: 0.008793, acc.: 100.00%] [G loss: 3.483370]\n",
      "epoch:16 step:13058 [D loss: 0.045088, acc.: 99.22%] [G loss: 2.501152]\n",
      "epoch:16 step:13059 [D loss: 0.007647, acc.: 100.00%] [G loss: 2.620069]\n",
      "epoch:16 step:13060 [D loss: 0.044704, acc.: 100.00%] [G loss: 2.268344]\n",
      "epoch:16 step:13061 [D loss: 0.041218, acc.: 100.00%] [G loss: 2.076470]\n",
      "epoch:16 step:13062 [D loss: 0.228682, acc.: 92.19%] [G loss: 4.594677]\n",
      "epoch:16 step:13063 [D loss: 0.016127, acc.: 100.00%] [G loss: 6.091824]\n",
      "epoch:16 step:13064 [D loss: 1.421324, acc.: 35.94%] [G loss: 6.250874]\n",
      "epoch:16 step:13065 [D loss: 0.718691, acc.: 65.62%] [G loss: 3.650463]\n",
      "epoch:16 step:13066 [D loss: 0.009253, acc.: 100.00%] [G loss: 2.147547]\n",
      "epoch:16 step:13067 [D loss: 0.038032, acc.: 99.22%] [G loss: 2.644435]\n",
      "epoch:16 step:13068 [D loss: 0.259750, acc.: 85.94%] [G loss: 5.472882]\n",
      "epoch:16 step:13069 [D loss: 0.039537, acc.: 100.00%] [G loss: 5.785843]\n",
      "epoch:16 step:13070 [D loss: 0.056036, acc.: 98.44%] [G loss: 5.248390]\n",
      "epoch:16 step:13071 [D loss: 0.033291, acc.: 100.00%] [G loss: 5.064562]\n",
      "epoch:16 step:13072 [D loss: 0.006283, acc.: 100.00%] [G loss: 4.814428]\n",
      "epoch:16 step:13073 [D loss: 0.024586, acc.: 99.22%] [G loss: 4.365763]\n",
      "epoch:16 step:13074 [D loss: 0.012766, acc.: 100.00%] [G loss: 3.138642]\n",
      "epoch:16 step:13075 [D loss: 0.007816, acc.: 100.00%] [G loss: 4.014410]\n",
      "epoch:16 step:13076 [D loss: 0.024166, acc.: 100.00%] [G loss: 3.907003]\n",
      "epoch:16 step:13077 [D loss: 0.018241, acc.: 100.00%] [G loss: 3.228400]\n",
      "epoch:16 step:13078 [D loss: 0.021071, acc.: 100.00%] [G loss: 3.380272]\n",
      "epoch:16 step:13079 [D loss: 0.015601, acc.: 100.00%] [G loss: 2.817092]\n",
      "epoch:16 step:13080 [D loss: 0.053610, acc.: 99.22%] [G loss: 2.761047]\n",
      "epoch:16 step:13081 [D loss: 0.076645, acc.: 99.22%] [G loss: 2.740358]\n",
      "epoch:16 step:13082 [D loss: 0.053355, acc.: 99.22%] [G loss: 3.881653]\n",
      "epoch:16 step:13083 [D loss: 0.119553, acc.: 99.22%] [G loss: 4.921720]\n",
      "epoch:16 step:13084 [D loss: 0.013556, acc.: 100.00%] [G loss: 2.875908]\n",
      "epoch:16 step:13085 [D loss: 0.088450, acc.: 97.66%] [G loss: 3.961140]\n",
      "epoch:16 step:13086 [D loss: 0.104985, acc.: 95.31%] [G loss: 3.591366]\n",
      "epoch:16 step:13087 [D loss: 0.074703, acc.: 100.00%] [G loss: 1.741775]\n",
      "epoch:16 step:13088 [D loss: 0.007841, acc.: 100.00%] [G loss: 2.539638]\n",
      "epoch:16 step:13089 [D loss: 0.055552, acc.: 100.00%] [G loss: 1.791907]\n",
      "epoch:16 step:13090 [D loss: 0.466625, acc.: 75.78%] [G loss: 6.992902]\n",
      "epoch:16 step:13091 [D loss: 2.205053, acc.: 50.00%] [G loss: 3.949694]\n",
      "epoch:16 step:13092 [D loss: 1.152517, acc.: 50.00%] [G loss: 2.378427]\n",
      "epoch:16 step:13093 [D loss: 0.162104, acc.: 96.88%] [G loss: 2.198380]\n",
      "epoch:16 step:13094 [D loss: 0.209833, acc.: 92.97%] [G loss: 2.777373]\n",
      "epoch:16 step:13095 [D loss: 0.034641, acc.: 100.00%] [G loss: 3.580703]\n",
      "epoch:16 step:13096 [D loss: 0.152019, acc.: 96.88%] [G loss: 2.613737]\n",
      "epoch:16 step:13097 [D loss: 0.146933, acc.: 96.09%] [G loss: 1.367100]\n",
      "epoch:16 step:13098 [D loss: 0.159997, acc.: 93.75%] [G loss: 1.679678]\n",
      "epoch:16 step:13099 [D loss: 0.055513, acc.: 99.22%] [G loss: 3.030049]\n",
      "epoch:16 step:13100 [D loss: 0.221565, acc.: 87.50%] [G loss: 1.122981]\n",
      "epoch:16 step:13101 [D loss: 0.417226, acc.: 82.81%] [G loss: 3.765028]\n",
      "epoch:16 step:13102 [D loss: 0.134947, acc.: 96.88%] [G loss: 3.586790]\n",
      "epoch:16 step:13103 [D loss: 0.593699, acc.: 64.84%] [G loss: 3.077501]\n",
      "epoch:16 step:13104 [D loss: 0.086835, acc.: 97.66%] [G loss: 3.725759]\n",
      "epoch:16 step:13105 [D loss: 0.148222, acc.: 93.75%] [G loss: 2.129626]\n",
      "epoch:16 step:13106 [D loss: 0.138095, acc.: 96.09%] [G loss: 2.642481]\n",
      "epoch:16 step:13107 [D loss: 0.027819, acc.: 99.22%] [G loss: 3.617586]\n",
      "epoch:16 step:13108 [D loss: 0.049641, acc.: 99.22%] [G loss: 2.611453]\n",
      "epoch:16 step:13109 [D loss: 0.920496, acc.: 53.91%] [G loss: 5.690814]\n",
      "epoch:16 step:13110 [D loss: 0.245488, acc.: 89.84%] [G loss: 5.738224]\n",
      "epoch:16 step:13111 [D loss: 0.067105, acc.: 97.66%] [G loss: 5.481418]\n",
      "epoch:16 step:13112 [D loss: 0.042846, acc.: 98.44%] [G loss: 5.123652]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:16 step:13113 [D loss: 0.004963, acc.: 100.00%] [G loss: 3.348112]\n",
      "epoch:16 step:13114 [D loss: 0.019596, acc.: 100.00%] [G loss: 3.728246]\n",
      "epoch:16 step:13115 [D loss: 0.025925, acc.: 100.00%] [G loss: 2.225791]\n",
      "epoch:16 step:13116 [D loss: 0.038514, acc.: 100.00%] [G loss: 2.448451]\n",
      "epoch:16 step:13117 [D loss: 0.043256, acc.: 100.00%] [G loss: 3.329535]\n",
      "epoch:16 step:13118 [D loss: 0.061692, acc.: 99.22%] [G loss: 3.776594]\n",
      "epoch:16 step:13119 [D loss: 0.201577, acc.: 93.75%] [G loss: 2.292433]\n",
      "epoch:16 step:13120 [D loss: 0.020990, acc.: 100.00%] [G loss: 2.103455]\n",
      "epoch:16 step:13121 [D loss: 0.036525, acc.: 100.00%] [G loss: 2.383555]\n",
      "epoch:16 step:13122 [D loss: 0.045481, acc.: 99.22%] [G loss: 2.997298]\n",
      "epoch:16 step:13123 [D loss: 0.519047, acc.: 74.22%] [G loss: 2.306595]\n",
      "epoch:16 step:13124 [D loss: 0.048262, acc.: 100.00%] [G loss: 3.528402]\n",
      "epoch:16 step:13125 [D loss: 0.034496, acc.: 100.00%] [G loss: 3.918721]\n",
      "epoch:16 step:13126 [D loss: 0.040048, acc.: 100.00%] [G loss: 3.498560]\n",
      "epoch:16 step:13127 [D loss: 0.165266, acc.: 96.09%] [G loss: 2.640370]\n",
      "epoch:16 step:13128 [D loss: 0.033883, acc.: 100.00%] [G loss: 3.197524]\n",
      "epoch:16 step:13129 [D loss: 0.014860, acc.: 100.00%] [G loss: 1.797887]\n",
      "epoch:16 step:13130 [D loss: 0.027057, acc.: 99.22%] [G loss: 2.021811]\n",
      "epoch:16 step:13131 [D loss: 0.024183, acc.: 100.00%] [G loss: 1.355936]\n",
      "epoch:16 step:13132 [D loss: 0.237240, acc.: 92.97%] [G loss: 4.140100]\n",
      "epoch:16 step:13133 [D loss: 0.484474, acc.: 72.66%] [G loss: 2.127897]\n",
      "epoch:16 step:13134 [D loss: 0.251985, acc.: 86.72%] [G loss: 4.214374]\n",
      "epoch:16 step:13135 [D loss: 0.022138, acc.: 99.22%] [G loss: 5.330869]\n",
      "epoch:16 step:13136 [D loss: 0.069523, acc.: 99.22%] [G loss: 4.037067]\n",
      "epoch:16 step:13137 [D loss: 0.012896, acc.: 100.00%] [G loss: 3.506193]\n",
      "epoch:16 step:13138 [D loss: 0.169359, acc.: 92.97%] [G loss: 1.684820]\n",
      "epoch:16 step:13139 [D loss: 0.024469, acc.: 100.00%] [G loss: 2.968025]\n",
      "epoch:16 step:13140 [D loss: 0.006677, acc.: 100.00%] [G loss: 3.241989]\n",
      "epoch:16 step:13141 [D loss: 0.018453, acc.: 100.00%] [G loss: 3.019429]\n",
      "epoch:16 step:13142 [D loss: 0.057088, acc.: 100.00%] [G loss: 3.507030]\n",
      "epoch:16 step:13143 [D loss: 0.107253, acc.: 97.66%] [G loss: 4.174424]\n",
      "epoch:16 step:13144 [D loss: 0.046980, acc.: 100.00%] [G loss: 4.370494]\n",
      "epoch:16 step:13145 [D loss: 0.086020, acc.: 97.66%] [G loss: 3.775826]\n",
      "epoch:16 step:13146 [D loss: 0.022649, acc.: 100.00%] [G loss: 4.021662]\n",
      "epoch:16 step:13147 [D loss: 0.017646, acc.: 100.00%] [G loss: 3.780117]\n",
      "epoch:16 step:13148 [D loss: 0.091978, acc.: 98.44%] [G loss: 4.129127]\n",
      "epoch:16 step:13149 [D loss: 0.012540, acc.: 100.00%] [G loss: 4.120241]\n",
      "epoch:16 step:13150 [D loss: 0.028559, acc.: 100.00%] [G loss: 3.612417]\n",
      "epoch:16 step:13151 [D loss: 0.090911, acc.: 99.22%] [G loss: 4.347878]\n",
      "epoch:16 step:13152 [D loss: 0.103789, acc.: 96.88%] [G loss: 3.743361]\n",
      "epoch:16 step:13153 [D loss: 0.021495, acc.: 100.00%] [G loss: 3.283707]\n",
      "epoch:16 step:13154 [D loss: 0.034540, acc.: 100.00%] [G loss: 3.068205]\n",
      "epoch:16 step:13155 [D loss: 0.021598, acc.: 100.00%] [G loss: 2.259094]\n",
      "epoch:16 step:13156 [D loss: 0.008545, acc.: 100.00%] [G loss: 2.823380]\n",
      "epoch:16 step:13157 [D loss: 0.034659, acc.: 100.00%] [G loss: 2.678353]\n",
      "epoch:16 step:13158 [D loss: 0.064132, acc.: 99.22%] [G loss: 2.397830]\n",
      "epoch:16 step:13159 [D loss: 0.020424, acc.: 100.00%] [G loss: 3.525624]\n",
      "epoch:16 step:13160 [D loss: 0.092127, acc.: 98.44%] [G loss: 3.080866]\n",
      "epoch:16 step:13161 [D loss: 0.687520, acc.: 63.28%] [G loss: 5.765035]\n",
      "epoch:16 step:13162 [D loss: 0.008865, acc.: 100.00%] [G loss: 7.156826]\n",
      "epoch:16 step:13163 [D loss: 0.483720, acc.: 75.00%] [G loss: 2.056888]\n",
      "epoch:16 step:13164 [D loss: 0.205886, acc.: 89.84%] [G loss: 4.510734]\n",
      "epoch:16 step:13165 [D loss: 0.003971, acc.: 100.00%] [G loss: 6.245335]\n",
      "epoch:16 step:13166 [D loss: 0.358398, acc.: 82.03%] [G loss: 3.970721]\n",
      "epoch:16 step:13167 [D loss: 0.093238, acc.: 97.66%] [G loss: 1.726824]\n",
      "epoch:16 step:13168 [D loss: 0.604772, acc.: 68.75%] [G loss: 6.843192]\n",
      "epoch:16 step:13169 [D loss: 0.770418, acc.: 64.06%] [G loss: 5.191309]\n",
      "epoch:16 step:13170 [D loss: 0.006939, acc.: 100.00%] [G loss: 2.734106]\n",
      "epoch:16 step:13171 [D loss: 0.018524, acc.: 100.00%] [G loss: 3.403513]\n",
      "epoch:16 step:13172 [D loss: 0.014245, acc.: 100.00%] [G loss: 2.322896]\n",
      "epoch:16 step:13173 [D loss: 0.124356, acc.: 95.31%] [G loss: 3.692060]\n",
      "epoch:16 step:13174 [D loss: 0.020154, acc.: 99.22%] [G loss: 3.423135]\n",
      "epoch:16 step:13175 [D loss: 0.029080, acc.: 100.00%] [G loss: 3.956771]\n",
      "epoch:16 step:13176 [D loss: 0.212588, acc.: 92.97%] [G loss: 0.850774]\n",
      "epoch:16 step:13177 [D loss: 0.223083, acc.: 90.62%] [G loss: 4.711047]\n",
      "epoch:16 step:13178 [D loss: 0.173400, acc.: 92.19%] [G loss: 4.186939]\n",
      "epoch:16 step:13179 [D loss: 0.327966, acc.: 83.59%] [G loss: 7.298553]\n",
      "epoch:16 step:13180 [D loss: 1.190602, acc.: 52.34%] [G loss: 3.714180]\n",
      "epoch:16 step:13181 [D loss: 0.122757, acc.: 96.09%] [G loss: 2.903201]\n",
      "epoch:16 step:13182 [D loss: 0.082268, acc.: 97.66%] [G loss: 3.660892]\n",
      "epoch:16 step:13183 [D loss: 0.048923, acc.: 100.00%] [G loss: 3.077641]\n",
      "epoch:16 step:13184 [D loss: 0.045433, acc.: 99.22%] [G loss: 3.430783]\n",
      "epoch:16 step:13185 [D loss: 0.016794, acc.: 100.00%] [G loss: 3.053264]\n",
      "epoch:16 step:13186 [D loss: 0.066519, acc.: 99.22%] [G loss: 3.784664]\n",
      "epoch:16 step:13187 [D loss: 0.057605, acc.: 99.22%] [G loss: 3.608503]\n",
      "epoch:16 step:13188 [D loss: 0.107725, acc.: 96.88%] [G loss: 3.675818]\n",
      "epoch:16 step:13189 [D loss: 0.019744, acc.: 100.00%] [G loss: 3.958798]\n",
      "epoch:16 step:13190 [D loss: 0.058374, acc.: 98.44%] [G loss: 4.528097]\n",
      "epoch:16 step:13191 [D loss: 0.006996, acc.: 100.00%] [G loss: 4.487863]\n",
      "epoch:16 step:13192 [D loss: 0.084339, acc.: 97.66%] [G loss: 3.986926]\n",
      "epoch:16 step:13193 [D loss: 0.009570, acc.: 100.00%] [G loss: 3.863665]\n",
      "epoch:16 step:13194 [D loss: 0.025726, acc.: 100.00%] [G loss: 2.911472]\n",
      "epoch:16 step:13195 [D loss: 0.005514, acc.: 100.00%] [G loss: 2.515478]\n",
      "epoch:16 step:13196 [D loss: 0.038983, acc.: 100.00%] [G loss: 2.031559]\n",
      "epoch:16 step:13197 [D loss: 0.040539, acc.: 100.00%] [G loss: 1.207411]\n",
      "epoch:16 step:13198 [D loss: 0.010544, acc.: 100.00%] [G loss: 1.062714]\n",
      "epoch:16 step:13199 [D loss: 0.020057, acc.: 99.22%] [G loss: 1.570639]\n",
      "epoch:16 step:13200 [D loss: 0.046577, acc.: 99.22%] [G loss: 0.160461]\n",
      "##############\n",
      "[0.98969515 1.01730987 0.79850024 0.96042097 2.11640777 1.11326668\n",
      " 1.10112369 2.11172291 0.94872637 1.007708  ]\n",
      "##########\n",
      "epoch:16 step:13201 [D loss: 0.033190, acc.: 100.00%] [G loss: 0.084980]\n",
      "epoch:16 step:13202 [D loss: 0.039910, acc.: 100.00%] [G loss: 1.262034]\n",
      "epoch:16 step:13203 [D loss: 0.060953, acc.: 98.44%] [G loss: 0.235582]\n",
      "epoch:16 step:13204 [D loss: 0.027715, acc.: 99.22%] [G loss: 0.724264]\n",
      "epoch:16 step:13205 [D loss: 0.065705, acc.: 99.22%] [G loss: 3.131708]\n",
      "epoch:16 step:13206 [D loss: 0.132238, acc.: 95.31%] [G loss: 0.890139]\n",
      "epoch:16 step:13207 [D loss: 0.111985, acc.: 96.88%] [G loss: 2.813110]\n",
      "epoch:16 step:13208 [D loss: 0.049290, acc.: 99.22%] [G loss: 4.779464]\n",
      "epoch:16 step:13209 [D loss: 0.672590, acc.: 64.84%] [G loss: 7.140440]\n",
      "epoch:16 step:13210 [D loss: 1.187048, acc.: 56.25%] [G loss: 3.423151]\n",
      "epoch:16 step:13211 [D loss: 0.352250, acc.: 86.72%] [G loss: 4.278245]\n",
      "epoch:16 step:13212 [D loss: 0.013019, acc.: 100.00%] [G loss: 5.048819]\n",
      "epoch:16 step:13213 [D loss: 0.039770, acc.: 100.00%] [G loss: 4.532871]\n",
      "epoch:16 step:13214 [D loss: 0.030905, acc.: 99.22%] [G loss: 2.595991]\n",
      "epoch:16 step:13215 [D loss: 0.030621, acc.: 100.00%] [G loss: 3.768966]\n",
      "epoch:16 step:13216 [D loss: 0.343687, acc.: 82.03%] [G loss: 0.607802]\n",
      "epoch:16 step:13217 [D loss: 0.210292, acc.: 90.62%] [G loss: 4.204152]\n",
      "epoch:16 step:13218 [D loss: 0.051051, acc.: 98.44%] [G loss: 4.142934]\n",
      "epoch:16 step:13219 [D loss: 0.020012, acc.: 100.00%] [G loss: 2.559026]\n",
      "epoch:16 step:13220 [D loss: 0.014694, acc.: 100.00%] [G loss: 1.584845]\n",
      "epoch:16 step:13221 [D loss: 0.196975, acc.: 90.62%] [G loss: 0.122219]\n",
      "epoch:16 step:13222 [D loss: 0.514325, acc.: 78.12%] [G loss: 2.929682]\n",
      "epoch:16 step:13223 [D loss: 0.157392, acc.: 93.75%] [G loss: 4.666503]\n",
      "epoch:16 step:13224 [D loss: 0.235795, acc.: 89.06%] [G loss: 1.646417]\n",
      "epoch:16 step:13225 [D loss: 0.051482, acc.: 99.22%] [G loss: 0.584155]\n",
      "epoch:16 step:13226 [D loss: 0.314724, acc.: 86.72%] [G loss: 2.817280]\n",
      "epoch:16 step:13227 [D loss: 0.004151, acc.: 100.00%] [G loss: 5.319953]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:16 step:13228 [D loss: 0.349789, acc.: 79.69%] [G loss: 0.862900]\n",
      "epoch:16 step:13229 [D loss: 0.251376, acc.: 86.72%] [G loss: 4.462880]\n",
      "epoch:16 step:13230 [D loss: 0.006107, acc.: 100.00%] [G loss: 5.859881]\n",
      "epoch:16 step:13231 [D loss: 0.044707, acc.: 99.22%] [G loss: 5.178972]\n",
      "epoch:16 step:13232 [D loss: 0.026816, acc.: 99.22%] [G loss: 4.139967]\n",
      "epoch:16 step:13233 [D loss: 0.022376, acc.: 100.00%] [G loss: 3.780610]\n",
      "epoch:16 step:13234 [D loss: 0.004262, acc.: 100.00%] [G loss: 4.465963]\n",
      "epoch:16 step:13235 [D loss: 0.406035, acc.: 81.25%] [G loss: 5.228682]\n",
      "epoch:16 step:13236 [D loss: 0.010255, acc.: 100.00%] [G loss: 6.272361]\n",
      "epoch:16 step:13237 [D loss: 0.419115, acc.: 78.91%] [G loss: 3.031368]\n",
      "epoch:16 step:13238 [D loss: 0.107239, acc.: 97.66%] [G loss: 4.882209]\n",
      "epoch:16 step:13239 [D loss: 0.032095, acc.: 99.22%] [G loss: 5.345158]\n",
      "epoch:16 step:13240 [D loss: 0.002607, acc.: 100.00%] [G loss: 5.284519]\n",
      "epoch:16 step:13241 [D loss: 0.003195, acc.: 100.00%] [G loss: 4.722058]\n",
      "epoch:16 step:13242 [D loss: 0.007431, acc.: 100.00%] [G loss: 3.609322]\n",
      "epoch:16 step:13243 [D loss: 0.033965, acc.: 100.00%] [G loss: 3.383382]\n",
      "epoch:16 step:13244 [D loss: 0.009688, acc.: 100.00%] [G loss: 2.866762]\n",
      "epoch:16 step:13245 [D loss: 0.008952, acc.: 100.00%] [G loss: 2.704118]\n",
      "epoch:16 step:13246 [D loss: 0.034402, acc.: 100.00%] [G loss: 1.977263]\n",
      "epoch:16 step:13247 [D loss: 0.015577, acc.: 100.00%] [G loss: 2.169361]\n",
      "epoch:16 step:13248 [D loss: 0.029322, acc.: 100.00%] [G loss: 2.801521]\n",
      "epoch:16 step:13249 [D loss: 0.177325, acc.: 93.75%] [G loss: 2.549263]\n",
      "epoch:16 step:13250 [D loss: 0.106881, acc.: 95.31%] [G loss: 4.442384]\n",
      "epoch:16 step:13251 [D loss: 0.015151, acc.: 100.00%] [G loss: 2.509545]\n",
      "epoch:16 step:13252 [D loss: 0.037687, acc.: 100.00%] [G loss: 1.390557]\n",
      "epoch:16 step:13253 [D loss: 0.094265, acc.: 98.44%] [G loss: 4.038300]\n",
      "epoch:16 step:13254 [D loss: 0.015462, acc.: 100.00%] [G loss: 2.913910]\n",
      "epoch:16 step:13255 [D loss: 0.082080, acc.: 97.66%] [G loss: 1.217701]\n",
      "epoch:16 step:13256 [D loss: 0.066756, acc.: 98.44%] [G loss: 2.659610]\n",
      "epoch:16 step:13257 [D loss: 0.001592, acc.: 100.00%] [G loss: 3.004099]\n",
      "epoch:16 step:13258 [D loss: 0.279583, acc.: 87.50%] [G loss: 1.730078]\n",
      "epoch:16 step:13259 [D loss: 0.004651, acc.: 100.00%] [G loss: 2.189270]\n",
      "epoch:16 step:13260 [D loss: 0.013209, acc.: 100.00%] [G loss: 1.018647]\n",
      "epoch:16 step:13261 [D loss: 0.050342, acc.: 100.00%] [G loss: 3.048765]\n",
      "epoch:16 step:13262 [D loss: 0.070151, acc.: 98.44%] [G loss: 1.953778]\n",
      "epoch:16 step:13263 [D loss: 0.103629, acc.: 96.88%] [G loss: 5.469890]\n",
      "epoch:16 step:13264 [D loss: 0.490767, acc.: 74.22%] [G loss: 6.812179]\n",
      "epoch:16 step:13265 [D loss: 0.279059, acc.: 83.59%] [G loss: 5.705609]\n",
      "epoch:16 step:13266 [D loss: 0.014332, acc.: 100.00%] [G loss: 4.199475]\n",
      "epoch:16 step:13267 [D loss: 0.010252, acc.: 100.00%] [G loss: 3.539152]\n",
      "epoch:16 step:13268 [D loss: 0.037696, acc.: 99.22%] [G loss: 3.872355]\n",
      "epoch:16 step:13269 [D loss: 0.004468, acc.: 100.00%] [G loss: 4.370840]\n",
      "epoch:16 step:13270 [D loss: 0.020502, acc.: 100.00%] [G loss: 4.175895]\n",
      "epoch:16 step:13271 [D loss: 0.015289, acc.: 100.00%] [G loss: 4.677202]\n",
      "epoch:16 step:13272 [D loss: 0.033463, acc.: 100.00%] [G loss: 4.067720]\n",
      "epoch:16 step:13273 [D loss: 0.017784, acc.: 100.00%] [G loss: 4.022586]\n",
      "epoch:16 step:13274 [D loss: 0.034544, acc.: 100.00%] [G loss: 2.927605]\n",
      "epoch:16 step:13275 [D loss: 0.034163, acc.: 100.00%] [G loss: 3.252022]\n",
      "epoch:16 step:13276 [D loss: 0.075002, acc.: 97.66%] [G loss: 3.799141]\n",
      "epoch:16 step:13277 [D loss: 0.009512, acc.: 100.00%] [G loss: 4.061972]\n",
      "epoch:17 step:13278 [D loss: 0.150975, acc.: 95.31%] [G loss: 0.840582]\n",
      "epoch:17 step:13279 [D loss: 0.014834, acc.: 100.00%] [G loss: 0.426151]\n",
      "epoch:17 step:13280 [D loss: 0.285780, acc.: 87.50%] [G loss: 7.077014]\n",
      "epoch:17 step:13281 [D loss: 0.200851, acc.: 92.19%] [G loss: 6.519082]\n",
      "epoch:17 step:13282 [D loss: 0.006699, acc.: 100.00%] [G loss: 3.987050]\n",
      "epoch:17 step:13283 [D loss: 0.009893, acc.: 100.00%] [G loss: 3.642063]\n",
      "epoch:17 step:13284 [D loss: 0.009832, acc.: 100.00%] [G loss: 3.152515]\n",
      "epoch:17 step:13285 [D loss: 0.158720, acc.: 95.31%] [G loss: 5.952604]\n",
      "epoch:17 step:13286 [D loss: 0.233709, acc.: 89.06%] [G loss: 4.131372]\n",
      "epoch:17 step:13287 [D loss: 0.118316, acc.: 96.09%] [G loss: 6.252608]\n",
      "epoch:17 step:13288 [D loss: 0.165401, acc.: 92.97%] [G loss: 3.644996]\n",
      "epoch:17 step:13289 [D loss: 0.097738, acc.: 96.88%] [G loss: 4.036836]\n",
      "epoch:17 step:13290 [D loss: 0.005825, acc.: 100.00%] [G loss: 7.116940]\n",
      "epoch:17 step:13291 [D loss: 0.141977, acc.: 96.09%] [G loss: 4.471520]\n",
      "epoch:17 step:13292 [D loss: 0.004136, acc.: 100.00%] [G loss: 1.230272]\n",
      "epoch:17 step:13293 [D loss: 0.079644, acc.: 96.88%] [G loss: 5.702227]\n",
      "epoch:17 step:13294 [D loss: 0.013177, acc.: 100.00%] [G loss: 4.757939]\n",
      "epoch:17 step:13295 [D loss: 0.011107, acc.: 100.00%] [G loss: 4.907086]\n",
      "epoch:17 step:13296 [D loss: 0.015261, acc.: 100.00%] [G loss: 5.138654]\n",
      "epoch:17 step:13297 [D loss: 0.024158, acc.: 100.00%] [G loss: 5.513834]\n",
      "epoch:17 step:13298 [D loss: 0.016262, acc.: 100.00%] [G loss: 5.156460]\n",
      "epoch:17 step:13299 [D loss: 0.068024, acc.: 100.00%] [G loss: 3.690476]\n",
      "epoch:17 step:13300 [D loss: 0.411749, acc.: 79.69%] [G loss: 6.985883]\n",
      "epoch:17 step:13301 [D loss: 1.573015, acc.: 48.44%] [G loss: 4.473277]\n",
      "epoch:17 step:13302 [D loss: 0.048694, acc.: 98.44%] [G loss: 4.535853]\n",
      "epoch:17 step:13303 [D loss: 0.017187, acc.: 100.00%] [G loss: 4.715520]\n",
      "epoch:17 step:13304 [D loss: 0.007750, acc.: 100.00%] [G loss: 3.870258]\n",
      "epoch:17 step:13305 [D loss: 0.025549, acc.: 100.00%] [G loss: 4.009716]\n",
      "epoch:17 step:13306 [D loss: 0.018760, acc.: 100.00%] [G loss: 3.009852]\n",
      "epoch:17 step:13307 [D loss: 0.065454, acc.: 99.22%] [G loss: 3.532818]\n",
      "epoch:17 step:13308 [D loss: 0.015558, acc.: 100.00%] [G loss: 3.158073]\n",
      "epoch:17 step:13309 [D loss: 0.019520, acc.: 100.00%] [G loss: 3.051873]\n",
      "epoch:17 step:13310 [D loss: 0.024796, acc.: 100.00%] [G loss: 1.023234]\n",
      "epoch:17 step:13311 [D loss: 0.068142, acc.: 98.44%] [G loss: 2.406543]\n",
      "epoch:17 step:13312 [D loss: 0.020241, acc.: 100.00%] [G loss: 2.793153]\n",
      "epoch:17 step:13313 [D loss: 0.173567, acc.: 96.09%] [G loss: 1.668492]\n",
      "epoch:17 step:13314 [D loss: 0.361484, acc.: 83.59%] [G loss: 7.582777]\n",
      "epoch:17 step:13315 [D loss: 1.426923, acc.: 51.56%] [G loss: 2.494563]\n",
      "epoch:17 step:13316 [D loss: 0.443774, acc.: 80.47%] [G loss: 5.596343]\n",
      "epoch:17 step:13317 [D loss: 0.018996, acc.: 99.22%] [G loss: 6.059345]\n",
      "epoch:17 step:13318 [D loss: 0.196327, acc.: 89.06%] [G loss: 3.517678]\n",
      "epoch:17 step:13319 [D loss: 0.103911, acc.: 96.88%] [G loss: 3.721827]\n",
      "epoch:17 step:13320 [D loss: 0.022421, acc.: 100.00%] [G loss: 5.156353]\n",
      "epoch:17 step:13321 [D loss: 0.208007, acc.: 92.19%] [G loss: 3.387292]\n",
      "epoch:17 step:13322 [D loss: 0.029470, acc.: 100.00%] [G loss: 3.271331]\n",
      "epoch:17 step:13323 [D loss: 0.043237, acc.: 99.22%] [G loss: 3.621624]\n",
      "epoch:17 step:13324 [D loss: 0.014082, acc.: 100.00%] [G loss: 3.623019]\n",
      "epoch:17 step:13325 [D loss: 0.134630, acc.: 93.75%] [G loss: 4.220105]\n",
      "epoch:17 step:13326 [D loss: 0.030268, acc.: 100.00%] [G loss: 4.244300]\n",
      "epoch:17 step:13327 [D loss: 0.034688, acc.: 99.22%] [G loss: 4.650129]\n",
      "epoch:17 step:13328 [D loss: 0.010839, acc.: 100.00%] [G loss: 3.433861]\n",
      "epoch:17 step:13329 [D loss: 0.172085, acc.: 94.53%] [G loss: 5.538484]\n",
      "epoch:17 step:13330 [D loss: 0.089335, acc.: 95.31%] [G loss: 4.395866]\n",
      "epoch:17 step:13331 [D loss: 0.092006, acc.: 98.44%] [G loss: 4.451056]\n",
      "epoch:17 step:13332 [D loss: 0.015971, acc.: 100.00%] [G loss: 3.284669]\n",
      "epoch:17 step:13333 [D loss: 0.079810, acc.: 99.22%] [G loss: 4.671429]\n",
      "epoch:17 step:13334 [D loss: 0.013281, acc.: 100.00%] [G loss: 5.145944]\n",
      "epoch:17 step:13335 [D loss: 0.063313, acc.: 98.44%] [G loss: 3.296341]\n",
      "epoch:17 step:13336 [D loss: 0.013947, acc.: 100.00%] [G loss: 2.342661]\n",
      "epoch:17 step:13337 [D loss: 0.116696, acc.: 97.66%] [G loss: 5.353195]\n",
      "epoch:17 step:13338 [D loss: 0.282664, acc.: 88.28%] [G loss: 2.089649]\n",
      "epoch:17 step:13339 [D loss: 0.197170, acc.: 90.62%] [G loss: 5.637483]\n",
      "epoch:17 step:13340 [D loss: 0.042660, acc.: 98.44%] [G loss: 6.473640]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:17 step:13341 [D loss: 0.330864, acc.: 85.94%] [G loss: 2.548178]\n",
      "epoch:17 step:13342 [D loss: 0.049569, acc.: 100.00%] [G loss: 1.873765]\n",
      "epoch:17 step:13343 [D loss: 0.289963, acc.: 87.50%] [G loss: 6.347760]\n",
      "epoch:17 step:13344 [D loss: 0.027002, acc.: 99.22%] [G loss: 7.356182]\n",
      "epoch:17 step:13345 [D loss: 0.966920, acc.: 54.69%] [G loss: 4.796159]\n",
      "epoch:17 step:13346 [D loss: 0.030776, acc.: 99.22%] [G loss: 6.084830]\n",
      "epoch:17 step:13347 [D loss: 0.034851, acc.: 99.22%] [G loss: 6.336648]\n",
      "epoch:17 step:13348 [D loss: 0.013199, acc.: 99.22%] [G loss: 5.480951]\n",
      "epoch:17 step:13349 [D loss: 0.003136, acc.: 100.00%] [G loss: 5.539654]\n",
      "epoch:17 step:13350 [D loss: 0.034067, acc.: 99.22%] [G loss: 4.027661]\n",
      "epoch:17 step:13351 [D loss: 0.022077, acc.: 100.00%] [G loss: 3.506577]\n",
      "epoch:17 step:13352 [D loss: 0.008375, acc.: 100.00%] [G loss: 3.880425]\n",
      "epoch:17 step:13353 [D loss: 0.010070, acc.: 100.00%] [G loss: 4.126469]\n",
      "epoch:17 step:13354 [D loss: 0.143086, acc.: 95.31%] [G loss: 5.157606]\n",
      "epoch:17 step:13355 [D loss: 0.198299, acc.: 92.97%] [G loss: 2.854812]\n",
      "epoch:17 step:13356 [D loss: 0.058711, acc.: 98.44%] [G loss: 2.636240]\n",
      "epoch:17 step:13357 [D loss: 0.066005, acc.: 97.66%] [G loss: 5.135394]\n",
      "epoch:17 step:13358 [D loss: 0.062365, acc.: 98.44%] [G loss: 3.447280]\n",
      "epoch:17 step:13359 [D loss: 0.013403, acc.: 100.00%] [G loss: 3.973909]\n",
      "epoch:17 step:13360 [D loss: 0.023966, acc.: 100.00%] [G loss: 4.508912]\n",
      "epoch:17 step:13361 [D loss: 0.025284, acc.: 100.00%] [G loss: 4.159421]\n",
      "epoch:17 step:13362 [D loss: 0.020504, acc.: 100.00%] [G loss: 2.706526]\n",
      "epoch:17 step:13363 [D loss: 0.033428, acc.: 99.22%] [G loss: 3.675196]\n",
      "epoch:17 step:13364 [D loss: 0.104831, acc.: 99.22%] [G loss: 5.956222]\n",
      "epoch:17 step:13365 [D loss: 4.199732, acc.: 9.38%] [G loss: 6.957394]\n",
      "epoch:17 step:13366 [D loss: 1.882268, acc.: 50.78%] [G loss: 4.882590]\n",
      "epoch:17 step:13367 [D loss: 1.792986, acc.: 45.31%] [G loss: 2.906678]\n",
      "epoch:17 step:13368 [D loss: 0.239076, acc.: 91.41%] [G loss: 1.674459]\n",
      "epoch:17 step:13369 [D loss: 0.224002, acc.: 92.19%] [G loss: 2.909710]\n",
      "epoch:17 step:13370 [D loss: 0.209656, acc.: 93.75%] [G loss: 2.625411]\n",
      "epoch:17 step:13371 [D loss: 0.203720, acc.: 96.88%] [G loss: 2.968100]\n",
      "epoch:17 step:13372 [D loss: 0.219521, acc.: 93.75%] [G loss: 3.465602]\n",
      "epoch:17 step:13373 [D loss: 0.196402, acc.: 92.19%] [G loss: 3.229038]\n",
      "epoch:17 step:13374 [D loss: 0.200708, acc.: 93.75%] [G loss: 2.792521]\n",
      "epoch:17 step:13375 [D loss: 0.211379, acc.: 93.75%] [G loss: 2.886745]\n",
      "epoch:17 step:13376 [D loss: 0.310720, acc.: 87.50%] [G loss: 3.393286]\n",
      "epoch:17 step:13377 [D loss: 0.131434, acc.: 96.09%] [G loss: 3.352599]\n",
      "epoch:17 step:13378 [D loss: 0.153376, acc.: 97.66%] [G loss: 1.963226]\n",
      "epoch:17 step:13379 [D loss: 0.118080, acc.: 97.66%] [G loss: 2.516840]\n",
      "epoch:17 step:13380 [D loss: 0.197307, acc.: 92.97%] [G loss: 2.231818]\n",
      "epoch:17 step:13381 [D loss: 0.146369, acc.: 96.09%] [G loss: 0.799434]\n",
      "epoch:17 step:13382 [D loss: 0.514609, acc.: 73.44%] [G loss: 4.707101]\n",
      "epoch:17 step:13383 [D loss: 0.188921, acc.: 93.75%] [G loss: 4.960113]\n",
      "epoch:17 step:13384 [D loss: 0.480769, acc.: 76.56%] [G loss: 1.428323]\n",
      "epoch:17 step:13385 [D loss: 0.530208, acc.: 73.44%] [G loss: 3.964211]\n",
      "epoch:17 step:13386 [D loss: 0.106589, acc.: 95.31%] [G loss: 4.281812]\n",
      "epoch:17 step:13387 [D loss: 0.381388, acc.: 80.47%] [G loss: 2.355669]\n",
      "epoch:17 step:13388 [D loss: 0.115493, acc.: 97.66%] [G loss: 2.027075]\n",
      "epoch:17 step:13389 [D loss: 0.065963, acc.: 99.22%] [G loss: 2.432392]\n",
      "epoch:17 step:13390 [D loss: 0.077379, acc.: 99.22%] [G loss: 2.741813]\n",
      "epoch:17 step:13391 [D loss: 0.088503, acc.: 98.44%] [G loss: 2.069577]\n",
      "epoch:17 step:13392 [D loss: 0.145435, acc.: 95.31%] [G loss: 1.901357]\n",
      "epoch:17 step:13393 [D loss: 0.157343, acc.: 96.88%] [G loss: 2.168197]\n",
      "epoch:17 step:13394 [D loss: 0.072145, acc.: 98.44%] [G loss: 2.927295]\n",
      "epoch:17 step:13395 [D loss: 0.132724, acc.: 96.88%] [G loss: 3.063664]\n",
      "epoch:17 step:13396 [D loss: 0.131616, acc.: 96.09%] [G loss: 2.359134]\n",
      "epoch:17 step:13397 [D loss: 0.169189, acc.: 95.31%] [G loss: 2.202884]\n",
      "epoch:17 step:13398 [D loss: 0.111882, acc.: 97.66%] [G loss: 3.162215]\n",
      "epoch:17 step:13399 [D loss: 0.066063, acc.: 99.22%] [G loss: 2.114580]\n",
      "epoch:17 step:13400 [D loss: 0.674240, acc.: 65.62%] [G loss: 4.797709]\n",
      "##############\n",
      "[1.02308918 1.07689023 0.9238889  0.96657535 0.92924846 2.11128587\n",
      " 0.8205286  0.83559947 1.1262076  1.12302147]\n",
      "##########\n",
      "epoch:17 step:13401 [D loss: 0.729836, acc.: 63.28%] [G loss: 4.314231]\n",
      "epoch:17 step:13402 [D loss: 0.229108, acc.: 92.97%] [G loss: 2.885819]\n",
      "epoch:17 step:13403 [D loss: 0.070454, acc.: 98.44%] [G loss: 3.228272]\n",
      "epoch:17 step:13404 [D loss: 0.059768, acc.: 99.22%] [G loss: 3.384518]\n",
      "epoch:17 step:13405 [D loss: 0.115249, acc.: 99.22%] [G loss: 3.266926]\n",
      "epoch:17 step:13406 [D loss: 0.068057, acc.: 98.44%] [G loss: 3.098130]\n",
      "epoch:17 step:13407 [D loss: 0.123716, acc.: 95.31%] [G loss: 3.872800]\n",
      "epoch:17 step:13408 [D loss: 0.037480, acc.: 100.00%] [G loss: 3.823282]\n",
      "epoch:17 step:13409 [D loss: 0.335131, acc.: 85.16%] [G loss: 3.532253]\n",
      "epoch:17 step:13410 [D loss: 0.038824, acc.: 99.22%] [G loss: 3.363198]\n",
      "epoch:17 step:13411 [D loss: 0.122923, acc.: 97.66%] [G loss: 3.322376]\n",
      "epoch:17 step:13412 [D loss: 0.107313, acc.: 99.22%] [G loss: 4.306701]\n",
      "epoch:17 step:13413 [D loss: 0.110022, acc.: 96.09%] [G loss: 2.955538]\n",
      "epoch:17 step:13414 [D loss: 0.040725, acc.: 99.22%] [G loss: 2.348037]\n",
      "epoch:17 step:13415 [D loss: 0.113219, acc.: 96.88%] [G loss: 4.177234]\n",
      "epoch:17 step:13416 [D loss: 0.109155, acc.: 97.66%] [G loss: 4.633478]\n",
      "epoch:17 step:13417 [D loss: 0.147012, acc.: 95.31%] [G loss: 3.572474]\n",
      "epoch:17 step:13418 [D loss: 0.085700, acc.: 97.66%] [G loss: 4.061016]\n",
      "epoch:17 step:13419 [D loss: 0.123262, acc.: 97.66%] [G loss: 3.520582]\n",
      "epoch:17 step:13420 [D loss: 0.097342, acc.: 99.22%] [G loss: 2.575784]\n",
      "epoch:17 step:13421 [D loss: 0.031320, acc.: 100.00%] [G loss: 3.354675]\n",
      "epoch:17 step:13422 [D loss: 0.036245, acc.: 100.00%] [G loss: 2.295261]\n",
      "epoch:17 step:13423 [D loss: 0.254099, acc.: 92.97%] [G loss: 3.459921]\n",
      "epoch:17 step:13424 [D loss: 0.095779, acc.: 98.44%] [G loss: 3.683747]\n",
      "epoch:17 step:13425 [D loss: 0.041789, acc.: 99.22%] [G loss: 4.325588]\n",
      "epoch:17 step:13426 [D loss: 0.032800, acc.: 100.00%] [G loss: 2.910315]\n",
      "epoch:17 step:13427 [D loss: 0.153031, acc.: 96.88%] [G loss: 4.219152]\n",
      "epoch:17 step:13428 [D loss: 0.077373, acc.: 96.09%] [G loss: 3.462857]\n",
      "epoch:17 step:13429 [D loss: 0.071486, acc.: 99.22%] [G loss: 3.400172]\n",
      "epoch:17 step:13430 [D loss: 0.112588, acc.: 98.44%] [G loss: 4.523466]\n",
      "epoch:17 step:13431 [D loss: 0.021265, acc.: 100.00%] [G loss: 3.709923]\n",
      "epoch:17 step:13432 [D loss: 0.067798, acc.: 98.44%] [G loss: 3.292690]\n",
      "epoch:17 step:13433 [D loss: 0.110426, acc.: 99.22%] [G loss: 4.176451]\n",
      "epoch:17 step:13434 [D loss: 0.027868, acc.: 100.00%] [G loss: 4.321670]\n",
      "epoch:17 step:13435 [D loss: 0.271676, acc.: 91.41%] [G loss: 4.673981]\n",
      "epoch:17 step:13436 [D loss: 0.010850, acc.: 100.00%] [G loss: 4.991445]\n",
      "epoch:17 step:13437 [D loss: 0.038684, acc.: 99.22%] [G loss: 3.897228]\n",
      "epoch:17 step:13438 [D loss: 0.140137, acc.: 96.09%] [G loss: 3.119976]\n",
      "epoch:17 step:13439 [D loss: 0.105571, acc.: 98.44%] [G loss: 4.843308]\n",
      "epoch:17 step:13440 [D loss: 0.024715, acc.: 100.00%] [G loss: 6.432096]\n",
      "epoch:17 step:13441 [D loss: 0.161321, acc.: 92.97%] [G loss: 2.981530]\n",
      "epoch:17 step:13442 [D loss: 0.136635, acc.: 96.09%] [G loss: 5.259635]\n",
      "epoch:17 step:13443 [D loss: 0.011413, acc.: 100.00%] [G loss: 6.147556]\n",
      "epoch:17 step:13444 [D loss: 0.020969, acc.: 100.00%] [G loss: 6.334334]\n",
      "epoch:17 step:13445 [D loss: 0.005873, acc.: 100.00%] [G loss: 5.058037]\n",
      "epoch:17 step:13446 [D loss: 0.005704, acc.: 100.00%] [G loss: 5.237323]\n",
      "epoch:17 step:13447 [D loss: 0.033691, acc.: 100.00%] [G loss: 4.396449]\n",
      "epoch:17 step:13448 [D loss: 0.041329, acc.: 100.00%] [G loss: 4.542195]\n",
      "epoch:17 step:13449 [D loss: 0.009041, acc.: 100.00%] [G loss: 5.055318]\n",
      "epoch:17 step:13450 [D loss: 0.018500, acc.: 100.00%] [G loss: 4.450008]\n",
      "epoch:17 step:13451 [D loss: 0.028990, acc.: 100.00%] [G loss: 4.414082]\n",
      "epoch:17 step:13452 [D loss: 0.017657, acc.: 100.00%] [G loss: 3.593120]\n",
      "epoch:17 step:13453 [D loss: 1.068019, acc.: 47.66%] [G loss: 7.510386]\n",
      "epoch:17 step:13454 [D loss: 1.975525, acc.: 51.56%] [G loss: 4.440311]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:17 step:13455 [D loss: 0.075245, acc.: 97.66%] [G loss: 3.500588]\n",
      "epoch:17 step:13456 [D loss: 0.073260, acc.: 99.22%] [G loss: 3.184930]\n",
      "epoch:17 step:13457 [D loss: 0.060724, acc.: 99.22%] [G loss: 3.688004]\n",
      "epoch:17 step:13458 [D loss: 0.218927, acc.: 92.97%] [G loss: 4.213214]\n",
      "epoch:17 step:13459 [D loss: 0.029270, acc.: 100.00%] [G loss: 4.103224]\n",
      "epoch:17 step:13460 [D loss: 0.125185, acc.: 97.66%] [G loss: 3.217804]\n",
      "epoch:17 step:13461 [D loss: 0.046113, acc.: 100.00%] [G loss: 3.364187]\n",
      "epoch:17 step:13462 [D loss: 0.066396, acc.: 99.22%] [G loss: 3.766000]\n",
      "epoch:17 step:13463 [D loss: 0.064403, acc.: 98.44%] [G loss: 4.025570]\n",
      "epoch:17 step:13464 [D loss: 0.135447, acc.: 96.09%] [G loss: 2.368439]\n",
      "epoch:17 step:13465 [D loss: 0.117298, acc.: 98.44%] [G loss: 4.361352]\n",
      "epoch:17 step:13466 [D loss: 0.032782, acc.: 100.00%] [G loss: 4.281727]\n",
      "epoch:17 step:13467 [D loss: 0.062212, acc.: 99.22%] [G loss: 4.452238]\n",
      "epoch:17 step:13468 [D loss: 0.054592, acc.: 100.00%] [G loss: 3.697568]\n",
      "epoch:17 step:13469 [D loss: 0.015936, acc.: 100.00%] [G loss: 3.740611]\n",
      "epoch:17 step:13470 [D loss: 0.078355, acc.: 100.00%] [G loss: 2.877665]\n",
      "epoch:17 step:13471 [D loss: 0.025092, acc.: 100.00%] [G loss: 4.331246]\n",
      "epoch:17 step:13472 [D loss: 0.026690, acc.: 100.00%] [G loss: 3.927617]\n",
      "epoch:17 step:13473 [D loss: 0.026029, acc.: 100.00%] [G loss: 3.172668]\n",
      "epoch:17 step:13474 [D loss: 0.020278, acc.: 100.00%] [G loss: 3.792000]\n",
      "epoch:17 step:13475 [D loss: 0.068310, acc.: 98.44%] [G loss: 3.802547]\n",
      "epoch:17 step:13476 [D loss: 0.192939, acc.: 96.09%] [G loss: 2.315529]\n",
      "epoch:17 step:13477 [D loss: 0.055136, acc.: 100.00%] [G loss: 1.967776]\n",
      "epoch:17 step:13478 [D loss: 0.028606, acc.: 100.00%] [G loss: 2.639059]\n",
      "epoch:17 step:13479 [D loss: 0.022656, acc.: 100.00%] [G loss: 4.141846]\n",
      "epoch:17 step:13480 [D loss: 0.040342, acc.: 99.22%] [G loss: 2.550060]\n",
      "epoch:17 step:13481 [D loss: 0.065787, acc.: 99.22%] [G loss: 3.358625]\n",
      "epoch:17 step:13482 [D loss: 1.695632, acc.: 28.91%] [G loss: 6.659215]\n",
      "epoch:17 step:13483 [D loss: 1.921812, acc.: 50.78%] [G loss: 3.272154]\n",
      "epoch:17 step:13484 [D loss: 0.273880, acc.: 86.72%] [G loss: 2.503155]\n",
      "epoch:17 step:13485 [D loss: 0.112714, acc.: 98.44%] [G loss: 1.724358]\n",
      "epoch:17 step:13486 [D loss: 0.595830, acc.: 67.97%] [G loss: 4.720668]\n",
      "epoch:17 step:13487 [D loss: 0.366486, acc.: 78.91%] [G loss: 3.405414]\n",
      "epoch:17 step:13488 [D loss: 0.284992, acc.: 88.28%] [G loss: 1.922191]\n",
      "epoch:17 step:13489 [D loss: 0.073458, acc.: 99.22%] [G loss: 2.115111]\n",
      "epoch:17 step:13490 [D loss: 0.337259, acc.: 82.81%] [G loss: 4.297458]\n",
      "epoch:17 step:13491 [D loss: 0.950083, acc.: 60.94%] [G loss: 2.245255]\n",
      "epoch:17 step:13492 [D loss: 0.148558, acc.: 96.09%] [G loss: 2.249973]\n",
      "epoch:17 step:13493 [D loss: 0.137705, acc.: 96.09%] [G loss: 1.402267]\n",
      "epoch:17 step:13494 [D loss: 0.150248, acc.: 95.31%] [G loss: 3.618378]\n",
      "epoch:17 step:13495 [D loss: 0.162827, acc.: 93.75%] [G loss: 2.831565]\n",
      "epoch:17 step:13496 [D loss: 0.051803, acc.: 99.22%] [G loss: 2.427142]\n",
      "epoch:17 step:13497 [D loss: 0.263999, acc.: 88.28%] [G loss: 5.048617]\n",
      "epoch:17 step:13498 [D loss: 0.054546, acc.: 98.44%] [G loss: 4.731653]\n",
      "epoch:17 step:13499 [D loss: 0.129246, acc.: 96.09%] [G loss: 3.381692]\n",
      "epoch:17 step:13500 [D loss: 0.058071, acc.: 99.22%] [G loss: 3.454959]\n",
      "epoch:17 step:13501 [D loss: 0.030256, acc.: 100.00%] [G loss: 4.135534]\n",
      "epoch:17 step:13502 [D loss: 0.054087, acc.: 99.22%] [G loss: 3.919685]\n",
      "epoch:17 step:13503 [D loss: 0.030186, acc.: 100.00%] [G loss: 2.663457]\n",
      "epoch:17 step:13504 [D loss: 0.060875, acc.: 99.22%] [G loss: 3.015179]\n",
      "epoch:17 step:13505 [D loss: 0.065694, acc.: 100.00%] [G loss: 3.036206]\n",
      "epoch:17 step:13506 [D loss: 0.164827, acc.: 95.31%] [G loss: 3.611020]\n",
      "epoch:17 step:13507 [D loss: 0.061629, acc.: 98.44%] [G loss: 4.254577]\n",
      "epoch:17 step:13508 [D loss: 0.050583, acc.: 100.00%] [G loss: 2.926309]\n",
      "epoch:17 step:13509 [D loss: 0.297845, acc.: 89.84%] [G loss: 5.258022]\n",
      "epoch:17 step:13510 [D loss: 0.068873, acc.: 98.44%] [G loss: 5.777568]\n",
      "epoch:17 step:13511 [D loss: 0.321851, acc.: 87.50%] [G loss: 2.824786]\n",
      "epoch:17 step:13512 [D loss: 0.018472, acc.: 100.00%] [G loss: 3.386428]\n",
      "epoch:17 step:13513 [D loss: 0.067285, acc.: 100.00%] [G loss: 4.747856]\n",
      "epoch:17 step:13514 [D loss: 0.029767, acc.: 100.00%] [G loss: 5.063685]\n",
      "epoch:17 step:13515 [D loss: 0.010490, acc.: 100.00%] [G loss: 4.193750]\n",
      "epoch:17 step:13516 [D loss: 0.017627, acc.: 100.00%] [G loss: 4.226361]\n",
      "epoch:17 step:13517 [D loss: 1.570754, acc.: 28.91%] [G loss: 6.104447]\n",
      "epoch:17 step:13518 [D loss: 0.974869, acc.: 56.25%] [G loss: 4.898036]\n",
      "epoch:17 step:13519 [D loss: 0.325872, acc.: 85.16%] [G loss: 3.465181]\n",
      "epoch:17 step:13520 [D loss: 0.024935, acc.: 100.00%] [G loss: 2.544132]\n",
      "epoch:17 step:13521 [D loss: 0.181337, acc.: 94.53%] [G loss: 3.522238]\n",
      "epoch:17 step:13522 [D loss: 0.022121, acc.: 100.00%] [G loss: 3.968164]\n",
      "epoch:17 step:13523 [D loss: 0.055015, acc.: 100.00%] [G loss: 4.003398]\n",
      "epoch:17 step:13524 [D loss: 0.031203, acc.: 100.00%] [G loss: 3.984111]\n",
      "epoch:17 step:13525 [D loss: 0.033237, acc.: 100.00%] [G loss: 3.235816]\n",
      "epoch:17 step:13526 [D loss: 0.038896, acc.: 100.00%] [G loss: 3.428445]\n",
      "epoch:17 step:13527 [D loss: 0.129623, acc.: 96.09%] [G loss: 3.283595]\n",
      "epoch:17 step:13528 [D loss: 0.039360, acc.: 100.00%] [G loss: 3.001510]\n",
      "epoch:17 step:13529 [D loss: 0.139254, acc.: 96.09%] [G loss: 3.862806]\n",
      "epoch:17 step:13530 [D loss: 0.029872, acc.: 100.00%] [G loss: 4.351033]\n",
      "epoch:17 step:13531 [D loss: 0.030486, acc.: 100.00%] [G loss: 4.360319]\n",
      "epoch:17 step:13532 [D loss: 0.151132, acc.: 96.09%] [G loss: 2.056164]\n",
      "epoch:17 step:13533 [D loss: 0.080529, acc.: 96.88%] [G loss: 3.639209]\n",
      "epoch:17 step:13534 [D loss: 0.036097, acc.: 100.00%] [G loss: 4.223048]\n",
      "epoch:17 step:13535 [D loss: 0.075020, acc.: 99.22%] [G loss: 3.702654]\n",
      "epoch:17 step:13536 [D loss: 0.032515, acc.: 100.00%] [G loss: 3.885354]\n",
      "epoch:17 step:13537 [D loss: 0.078544, acc.: 99.22%] [G loss: 3.606160]\n",
      "epoch:17 step:13538 [D loss: 0.045924, acc.: 100.00%] [G loss: 3.502986]\n",
      "epoch:17 step:13539 [D loss: 0.101818, acc.: 97.66%] [G loss: 5.034559]\n",
      "epoch:17 step:13540 [D loss: 0.043676, acc.: 100.00%] [G loss: 3.191621]\n",
      "epoch:17 step:13541 [D loss: 0.502816, acc.: 75.00%] [G loss: 6.048032]\n",
      "epoch:17 step:13542 [D loss: 0.450501, acc.: 77.34%] [G loss: 4.296856]\n",
      "epoch:17 step:13543 [D loss: 0.174163, acc.: 93.75%] [G loss: 5.614689]\n",
      "epoch:17 step:13544 [D loss: 0.124493, acc.: 94.53%] [G loss: 4.554562]\n",
      "epoch:17 step:13545 [D loss: 0.005071, acc.: 100.00%] [G loss: 4.312525]\n",
      "epoch:17 step:13546 [D loss: 0.024625, acc.: 100.00%] [G loss: 4.508459]\n",
      "epoch:17 step:13547 [D loss: 0.051961, acc.: 98.44%] [G loss: 4.455013]\n",
      "epoch:17 step:13548 [D loss: 0.017688, acc.: 99.22%] [G loss: 3.369161]\n",
      "epoch:17 step:13549 [D loss: 0.029549, acc.: 100.00%] [G loss: 3.230196]\n",
      "epoch:17 step:13550 [D loss: 0.032995, acc.: 100.00%] [G loss: 4.438959]\n",
      "epoch:17 step:13551 [D loss: 0.021385, acc.: 100.00%] [G loss: 3.085557]\n",
      "epoch:17 step:13552 [D loss: 0.020760, acc.: 100.00%] [G loss: 3.017552]\n",
      "epoch:17 step:13553 [D loss: 0.014167, acc.: 100.00%] [G loss: 2.455049]\n",
      "epoch:17 step:13554 [D loss: 0.039790, acc.: 99.22%] [G loss: 4.034910]\n",
      "epoch:17 step:13555 [D loss: 0.022802, acc.: 100.00%] [G loss: 1.586219]\n",
      "epoch:17 step:13556 [D loss: 0.047976, acc.: 99.22%] [G loss: 3.781843]\n",
      "epoch:17 step:13557 [D loss: 0.106566, acc.: 96.09%] [G loss: 2.440265]\n",
      "epoch:17 step:13558 [D loss: 0.015250, acc.: 100.00%] [G loss: 2.803566]\n",
      "epoch:17 step:13559 [D loss: 0.022655, acc.: 99.22%] [G loss: 1.320603]\n",
      "epoch:17 step:13560 [D loss: 0.308140, acc.: 82.03%] [G loss: 5.930332]\n",
      "epoch:17 step:13561 [D loss: 0.940564, acc.: 57.81%] [G loss: 0.565380]\n",
      "epoch:17 step:13562 [D loss: 1.661634, acc.: 54.69%] [G loss: 6.321042]\n",
      "epoch:17 step:13563 [D loss: 1.104920, acc.: 54.69%] [G loss: 4.428566]\n",
      "epoch:17 step:13564 [D loss: 0.312866, acc.: 83.59%] [G loss: 3.496444]\n",
      "epoch:17 step:13565 [D loss: 0.336253, acc.: 78.12%] [G loss: 4.460811]\n",
      "epoch:17 step:13566 [D loss: 0.167541, acc.: 93.75%] [G loss: 3.924850]\n",
      "epoch:17 step:13567 [D loss: 0.226715, acc.: 91.41%] [G loss: 2.764868]\n",
      "epoch:17 step:13568 [D loss: 0.076164, acc.: 99.22%] [G loss: 3.435756]\n",
      "epoch:17 step:13569 [D loss: 0.078200, acc.: 99.22%] [G loss: 3.313125]\n",
      "epoch:17 step:13570 [D loss: 0.034159, acc.: 100.00%] [G loss: 3.867900]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:17 step:13571 [D loss: 0.096573, acc.: 100.00%] [G loss: 3.187448]\n",
      "epoch:17 step:13572 [D loss: 0.082737, acc.: 99.22%] [G loss: 4.027683]\n",
      "epoch:17 step:13573 [D loss: 0.139315, acc.: 94.53%] [G loss: 2.725599]\n",
      "epoch:17 step:13574 [D loss: 0.233829, acc.: 93.75%] [G loss: 3.568709]\n",
      "epoch:17 step:13575 [D loss: 0.305363, acc.: 87.50%] [G loss: 4.129225]\n",
      "epoch:17 step:13576 [D loss: 0.116755, acc.: 96.09%] [G loss: 2.512396]\n",
      "epoch:17 step:13577 [D loss: 0.046652, acc.: 99.22%] [G loss: 1.236082]\n",
      "epoch:17 step:13578 [D loss: 0.288730, acc.: 85.94%] [G loss: 4.379864]\n",
      "epoch:17 step:13579 [D loss: 0.795497, acc.: 60.16%] [G loss: 0.792799]\n",
      "epoch:17 step:13580 [D loss: 0.826434, acc.: 66.41%] [G loss: 5.123897]\n",
      "epoch:17 step:13581 [D loss: 0.293490, acc.: 82.81%] [G loss: 6.186879]\n",
      "epoch:17 step:13582 [D loss: 0.329414, acc.: 82.03%] [G loss: 2.589061]\n",
      "epoch:17 step:13583 [D loss: 0.094435, acc.: 96.88%] [G loss: 1.585297]\n",
      "epoch:17 step:13584 [D loss: 0.147069, acc.: 96.88%] [G loss: 2.591478]\n",
      "epoch:17 step:13585 [D loss: 0.114589, acc.: 97.66%] [G loss: 5.505869]\n",
      "epoch:17 step:13586 [D loss: 0.096209, acc.: 97.66%] [G loss: 2.599936]\n",
      "epoch:17 step:13587 [D loss: 0.449516, acc.: 79.69%] [G loss: 4.098051]\n",
      "epoch:17 step:13588 [D loss: 0.088699, acc.: 96.88%] [G loss: 4.360218]\n",
      "epoch:17 step:13589 [D loss: 0.166824, acc.: 94.53%] [G loss: 3.000362]\n",
      "epoch:17 step:13590 [D loss: 0.076987, acc.: 99.22%] [G loss: 3.492321]\n",
      "epoch:17 step:13591 [D loss: 0.050745, acc.: 98.44%] [G loss: 2.146705]\n",
      "epoch:17 step:13592 [D loss: 0.301458, acc.: 88.28%] [G loss: 2.938060]\n",
      "epoch:17 step:13593 [D loss: 0.093872, acc.: 97.66%] [G loss: 2.735910]\n",
      "epoch:17 step:13594 [D loss: 0.276602, acc.: 91.41%] [G loss: 2.634661]\n",
      "epoch:17 step:13595 [D loss: 0.053739, acc.: 100.00%] [G loss: 3.088205]\n",
      "epoch:17 step:13596 [D loss: 0.225977, acc.: 92.19%] [G loss: 2.675508]\n",
      "epoch:17 step:13597 [D loss: 0.224910, acc.: 91.41%] [G loss: 3.242842]\n",
      "epoch:17 step:13598 [D loss: 0.048193, acc.: 100.00%] [G loss: 5.063485]\n",
      "epoch:17 step:13599 [D loss: 0.044646, acc.: 100.00%] [G loss: 4.091354]\n",
      "epoch:17 step:13600 [D loss: 0.270104, acc.: 90.62%] [G loss: 3.736259]\n",
      "##############\n",
      "[0.95300494 0.92943372 0.9478296  0.92274433 1.10527627 1.11912685\n",
      " 1.00911691 1.10044536 0.93501449 0.90759337]\n",
      "##########\n",
      "epoch:17 step:13601 [D loss: 0.092659, acc.: 96.09%] [G loss: 4.257599]\n",
      "epoch:17 step:13602 [D loss: 0.043596, acc.: 98.44%] [G loss: 4.863105]\n",
      "epoch:17 step:13603 [D loss: 0.064933, acc.: 99.22%] [G loss: 3.070103]\n",
      "epoch:17 step:13604 [D loss: 0.059140, acc.: 100.00%] [G loss: 3.822179]\n",
      "epoch:17 step:13605 [D loss: 0.023184, acc.: 100.00%] [G loss: 4.062101]\n",
      "epoch:17 step:13606 [D loss: 0.038946, acc.: 100.00%] [G loss: 3.485569]\n",
      "epoch:17 step:13607 [D loss: 0.393674, acc.: 82.81%] [G loss: 6.425209]\n",
      "epoch:17 step:13608 [D loss: 1.132571, acc.: 58.59%] [G loss: 2.655855]\n",
      "epoch:17 step:13609 [D loss: 0.365600, acc.: 78.91%] [G loss: 6.179983]\n",
      "epoch:17 step:13610 [D loss: 0.104003, acc.: 95.31%] [G loss: 6.200145]\n",
      "epoch:17 step:13611 [D loss: 0.153570, acc.: 92.97%] [G loss: 4.457667]\n",
      "epoch:17 step:13612 [D loss: 0.009502, acc.: 100.00%] [G loss: 3.948486]\n",
      "epoch:17 step:13613 [D loss: 0.152839, acc.: 95.31%] [G loss: 4.505903]\n",
      "epoch:17 step:13614 [D loss: 0.011587, acc.: 100.00%] [G loss: 5.504287]\n",
      "epoch:17 step:13615 [D loss: 0.055617, acc.: 98.44%] [G loss: 4.339651]\n",
      "epoch:17 step:13616 [D loss: 0.016259, acc.: 100.00%] [G loss: 3.663410]\n",
      "epoch:17 step:13617 [D loss: 0.163269, acc.: 92.19%] [G loss: 4.461745]\n",
      "epoch:17 step:13618 [D loss: 0.013054, acc.: 100.00%] [G loss: 5.191899]\n",
      "epoch:17 step:13619 [D loss: 0.085955, acc.: 96.09%] [G loss: 3.901956]\n",
      "epoch:17 step:13620 [D loss: 0.139486, acc.: 96.88%] [G loss: 3.241003]\n",
      "epoch:17 step:13621 [D loss: 0.012852, acc.: 100.00%] [G loss: 4.527246]\n",
      "epoch:17 step:13622 [D loss: 0.086497, acc.: 97.66%] [G loss: 3.158291]\n",
      "epoch:17 step:13623 [D loss: 0.136522, acc.: 96.88%] [G loss: 4.287102]\n",
      "epoch:17 step:13624 [D loss: 0.013955, acc.: 100.00%] [G loss: 5.072018]\n",
      "epoch:17 step:13625 [D loss: 0.084185, acc.: 98.44%] [G loss: 4.426840]\n",
      "epoch:17 step:13626 [D loss: 0.019279, acc.: 100.00%] [G loss: 4.489455]\n",
      "epoch:17 step:13627 [D loss: 0.259445, acc.: 88.28%] [G loss: 5.797138]\n",
      "epoch:17 step:13628 [D loss: 0.019978, acc.: 100.00%] [G loss: 7.028282]\n",
      "epoch:17 step:13629 [D loss: 0.425364, acc.: 78.91%] [G loss: 3.305553]\n",
      "epoch:17 step:13630 [D loss: 0.119954, acc.: 96.09%] [G loss: 4.506984]\n",
      "epoch:17 step:13631 [D loss: 0.005497, acc.: 100.00%] [G loss: 5.881254]\n",
      "epoch:17 step:13632 [D loss: 0.084183, acc.: 95.31%] [G loss: 4.414915]\n",
      "epoch:17 step:13633 [D loss: 0.014385, acc.: 100.00%] [G loss: 3.889609]\n",
      "epoch:17 step:13634 [D loss: 0.105112, acc.: 96.88%] [G loss: 5.567674]\n",
      "epoch:17 step:13635 [D loss: 0.051116, acc.: 98.44%] [G loss: 5.212406]\n",
      "epoch:17 step:13636 [D loss: 0.006057, acc.: 100.00%] [G loss: 5.402425]\n",
      "epoch:17 step:13637 [D loss: 0.019606, acc.: 100.00%] [G loss: 4.809231]\n",
      "epoch:17 step:13638 [D loss: 0.077304, acc.: 97.66%] [G loss: 2.732402]\n",
      "epoch:17 step:13639 [D loss: 0.130577, acc.: 92.19%] [G loss: 4.661551]\n",
      "epoch:17 step:13640 [D loss: 0.016217, acc.: 100.00%] [G loss: 4.685354]\n",
      "epoch:17 step:13641 [D loss: 0.226801, acc.: 90.62%] [G loss: 3.448858]\n",
      "epoch:17 step:13642 [D loss: 0.084341, acc.: 97.66%] [G loss: 4.738508]\n",
      "epoch:17 step:13643 [D loss: 0.072714, acc.: 99.22%] [G loss: 4.997703]\n",
      "epoch:17 step:13644 [D loss: 0.020549, acc.: 100.00%] [G loss: 4.875262]\n",
      "epoch:17 step:13645 [D loss: 0.019639, acc.: 99.22%] [G loss: 4.891258]\n",
      "epoch:17 step:13646 [D loss: 0.043217, acc.: 100.00%] [G loss: 5.239686]\n",
      "epoch:17 step:13647 [D loss: 0.150359, acc.: 93.75%] [G loss: 4.944022]\n",
      "epoch:17 step:13648 [D loss: 0.014519, acc.: 100.00%] [G loss: 4.413874]\n",
      "epoch:17 step:13649 [D loss: 0.018369, acc.: 100.00%] [G loss: 4.385197]\n",
      "epoch:17 step:13650 [D loss: 0.015669, acc.: 100.00%] [G loss: 4.501073]\n",
      "epoch:17 step:13651 [D loss: 0.717521, acc.: 65.62%] [G loss: 6.695996]\n",
      "epoch:17 step:13652 [D loss: 1.923277, acc.: 50.78%] [G loss: 4.126455]\n",
      "epoch:17 step:13653 [D loss: 0.309205, acc.: 89.06%] [G loss: 1.881752]\n",
      "epoch:17 step:13654 [D loss: 0.181436, acc.: 92.97%] [G loss: 3.369108]\n",
      "epoch:17 step:13655 [D loss: 0.039585, acc.: 100.00%] [G loss: 3.778369]\n",
      "epoch:17 step:13656 [D loss: 0.073364, acc.: 100.00%] [G loss: 3.503474]\n",
      "epoch:17 step:13657 [D loss: 0.139264, acc.: 95.31%] [G loss: 4.255048]\n",
      "epoch:17 step:13658 [D loss: 0.018283, acc.: 100.00%] [G loss: 4.297530]\n",
      "epoch:17 step:13659 [D loss: 0.076959, acc.: 98.44%] [G loss: 4.259823]\n",
      "epoch:17 step:13660 [D loss: 0.017782, acc.: 100.00%] [G loss: 4.013442]\n",
      "epoch:17 step:13661 [D loss: 0.027171, acc.: 100.00%] [G loss: 3.175714]\n",
      "epoch:17 step:13662 [D loss: 0.179642, acc.: 95.31%] [G loss: 2.852416]\n",
      "epoch:17 step:13663 [D loss: 0.105531, acc.: 97.66%] [G loss: 4.473013]\n",
      "epoch:17 step:13664 [D loss: 0.024949, acc.: 100.00%] [G loss: 4.425138]\n",
      "epoch:17 step:13665 [D loss: 0.009411, acc.: 100.00%] [G loss: 4.563637]\n",
      "epoch:17 step:13666 [D loss: 0.016482, acc.: 100.00%] [G loss: 4.742872]\n",
      "epoch:17 step:13667 [D loss: 0.035336, acc.: 99.22%] [G loss: 3.885987]\n",
      "epoch:17 step:13668 [D loss: 0.026731, acc.: 99.22%] [G loss: 3.978293]\n",
      "epoch:17 step:13669 [D loss: 0.035900, acc.: 100.00%] [G loss: 3.934324]\n",
      "epoch:17 step:13670 [D loss: 0.128698, acc.: 96.88%] [G loss: 2.992350]\n",
      "epoch:17 step:13671 [D loss: 0.071983, acc.: 97.66%] [G loss: 3.934424]\n",
      "epoch:17 step:13672 [D loss: 0.035310, acc.: 99.22%] [G loss: 4.248538]\n",
      "epoch:17 step:13673 [D loss: 0.052772, acc.: 99.22%] [G loss: 2.810190]\n",
      "epoch:17 step:13674 [D loss: 0.044186, acc.: 100.00%] [G loss: 4.134521]\n",
      "epoch:17 step:13675 [D loss: 0.008327, acc.: 100.00%] [G loss: 4.310896]\n",
      "epoch:17 step:13676 [D loss: 0.006592, acc.: 100.00%] [G loss: 3.673587]\n",
      "epoch:17 step:13677 [D loss: 0.014710, acc.: 100.00%] [G loss: 4.327800]\n",
      "epoch:17 step:13678 [D loss: 0.090410, acc.: 99.22%] [G loss: 4.673567]\n",
      "epoch:17 step:13679 [D loss: 0.072454, acc.: 98.44%] [G loss: 4.935273]\n",
      "epoch:17 step:13680 [D loss: 0.274410, acc.: 92.19%] [G loss: 4.779440]\n",
      "epoch:17 step:13681 [D loss: 0.016100, acc.: 100.00%] [G loss: 5.674589]\n",
      "epoch:17 step:13682 [D loss: 0.179589, acc.: 92.97%] [G loss: 3.088662]\n",
      "epoch:17 step:13683 [D loss: 0.065571, acc.: 99.22%] [G loss: 3.311508]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:17 step:13684 [D loss: 0.014429, acc.: 100.00%] [G loss: 4.182127]\n",
      "epoch:17 step:13685 [D loss: 0.010118, acc.: 100.00%] [G loss: 4.417997]\n",
      "epoch:17 step:13686 [D loss: 0.087056, acc.: 98.44%] [G loss: 3.606854]\n",
      "epoch:17 step:13687 [D loss: 0.007144, acc.: 100.00%] [G loss: 2.048048]\n",
      "epoch:17 step:13688 [D loss: 0.040512, acc.: 100.00%] [G loss: 1.840742]\n",
      "epoch:17 step:13689 [D loss: 0.076554, acc.: 100.00%] [G loss: 3.368009]\n",
      "epoch:17 step:13690 [D loss: 0.052197, acc.: 99.22%] [G loss: 4.616581]\n",
      "epoch:17 step:13691 [D loss: 0.015460, acc.: 100.00%] [G loss: 4.163480]\n",
      "epoch:17 step:13692 [D loss: 0.031670, acc.: 99.22%] [G loss: 3.733701]\n",
      "epoch:17 step:13693 [D loss: 0.037832, acc.: 99.22%] [G loss: 2.937445]\n",
      "epoch:17 step:13694 [D loss: 0.051042, acc.: 99.22%] [G loss: 2.662516]\n",
      "epoch:17 step:13695 [D loss: 0.074289, acc.: 99.22%] [G loss: 4.207434]\n",
      "epoch:17 step:13696 [D loss: 0.027420, acc.: 100.00%] [G loss: 5.465581]\n",
      "epoch:17 step:13697 [D loss: 2.322906, acc.: 14.84%] [G loss: 6.876336]\n",
      "epoch:17 step:13698 [D loss: 1.222759, acc.: 53.12%] [G loss: 4.309378]\n",
      "epoch:17 step:13699 [D loss: 0.198453, acc.: 93.75%] [G loss: 4.419097]\n",
      "epoch:17 step:13700 [D loss: 0.025497, acc.: 100.00%] [G loss: 4.287035]\n",
      "epoch:17 step:13701 [D loss: 0.049224, acc.: 98.44%] [G loss: 3.985189]\n",
      "epoch:17 step:13702 [D loss: 0.059518, acc.: 99.22%] [G loss: 3.375914]\n",
      "epoch:17 step:13703 [D loss: 0.049354, acc.: 100.00%] [G loss: 2.635825]\n",
      "epoch:17 step:13704 [D loss: 0.015503, acc.: 100.00%] [G loss: 3.435991]\n",
      "epoch:17 step:13705 [D loss: 0.456042, acc.: 78.91%] [G loss: 5.168473]\n",
      "epoch:17 step:13706 [D loss: 0.330596, acc.: 83.59%] [G loss: 4.173076]\n",
      "epoch:17 step:13707 [D loss: 0.068558, acc.: 98.44%] [G loss: 2.051050]\n",
      "epoch:17 step:13708 [D loss: 0.904137, acc.: 62.50%] [G loss: 5.883556]\n",
      "epoch:17 step:13709 [D loss: 1.222019, acc.: 54.69%] [G loss: 4.028544]\n",
      "epoch:17 step:13710 [D loss: 0.203447, acc.: 93.75%] [G loss: 2.106410]\n",
      "epoch:17 step:13711 [D loss: 0.133448, acc.: 96.09%] [G loss: 2.647005]\n",
      "epoch:17 step:13712 [D loss: 0.201712, acc.: 91.41%] [G loss: 4.015491]\n",
      "epoch:17 step:13713 [D loss: 0.103707, acc.: 97.66%] [G loss: 3.917113]\n",
      "epoch:17 step:13714 [D loss: 0.426603, acc.: 78.12%] [G loss: 1.451253]\n",
      "epoch:17 step:13715 [D loss: 0.145720, acc.: 97.66%] [G loss: 3.063261]\n",
      "epoch:17 step:13716 [D loss: 0.061870, acc.: 99.22%] [G loss: 3.919600]\n",
      "epoch:17 step:13717 [D loss: 0.078030, acc.: 99.22%] [G loss: 3.947539]\n",
      "epoch:17 step:13718 [D loss: 0.297670, acc.: 89.84%] [G loss: 1.870265]\n",
      "epoch:17 step:13719 [D loss: 0.142900, acc.: 93.75%] [G loss: 2.915768]\n",
      "epoch:17 step:13720 [D loss: 0.078696, acc.: 98.44%] [G loss: 2.513451]\n",
      "epoch:17 step:13721 [D loss: 0.051192, acc.: 98.44%] [G loss: 4.069911]\n",
      "epoch:17 step:13722 [D loss: 0.052845, acc.: 98.44%] [G loss: 2.016046]\n",
      "epoch:17 step:13723 [D loss: 0.110835, acc.: 97.66%] [G loss: 4.257648]\n",
      "epoch:17 step:13724 [D loss: 0.078237, acc.: 98.44%] [G loss: 2.935754]\n",
      "epoch:17 step:13725 [D loss: 0.171128, acc.: 95.31%] [G loss: 3.271820]\n",
      "epoch:17 step:13726 [D loss: 0.025192, acc.: 100.00%] [G loss: 3.339924]\n",
      "epoch:17 step:13727 [D loss: 0.057685, acc.: 100.00%] [G loss: 4.128197]\n",
      "epoch:17 step:13728 [D loss: 0.204835, acc.: 90.62%] [G loss: 2.656300]\n",
      "epoch:17 step:13729 [D loss: 0.067207, acc.: 100.00%] [G loss: 3.311036]\n",
      "epoch:17 step:13730 [D loss: 0.055058, acc.: 99.22%] [G loss: 3.437572]\n",
      "epoch:17 step:13731 [D loss: 0.014680, acc.: 100.00%] [G loss: 3.501876]\n",
      "epoch:17 step:13732 [D loss: 0.210534, acc.: 93.75%] [G loss: 4.971776]\n",
      "epoch:17 step:13733 [D loss: 0.103035, acc.: 96.88%] [G loss: 4.073165]\n",
      "epoch:17 step:13734 [D loss: 0.013194, acc.: 100.00%] [G loss: 4.190525]\n",
      "epoch:17 step:13735 [D loss: 0.015642, acc.: 100.00%] [G loss: 3.368935]\n",
      "epoch:17 step:13736 [D loss: 0.230565, acc.: 92.19%] [G loss: 4.321607]\n",
      "epoch:17 step:13737 [D loss: 0.213872, acc.: 92.19%] [G loss: 3.708813]\n",
      "epoch:17 step:13738 [D loss: 0.032132, acc.: 100.00%] [G loss: 3.414298]\n",
      "epoch:17 step:13739 [D loss: 0.013238, acc.: 100.00%] [G loss: 3.704054]\n",
      "epoch:17 step:13740 [D loss: 0.035742, acc.: 100.00%] [G loss: 3.843086]\n",
      "epoch:17 step:13741 [D loss: 0.052139, acc.: 99.22%] [G loss: 4.203521]\n",
      "epoch:17 step:13742 [D loss: 0.018327, acc.: 100.00%] [G loss: 3.768399]\n",
      "epoch:17 step:13743 [D loss: 0.025609, acc.: 100.00%] [G loss: 3.364221]\n",
      "epoch:17 step:13744 [D loss: 0.047560, acc.: 98.44%] [G loss: 3.349909]\n",
      "epoch:17 step:13745 [D loss: 0.037365, acc.: 100.00%] [G loss: 3.412656]\n",
      "epoch:17 step:13746 [D loss: 0.054227, acc.: 100.00%] [G loss: 4.246905]\n",
      "epoch:17 step:13747 [D loss: 0.022266, acc.: 100.00%] [G loss: 4.667082]\n",
      "epoch:17 step:13748 [D loss: 0.067579, acc.: 98.44%] [G loss: 2.568076]\n",
      "epoch:17 step:13749 [D loss: 0.025976, acc.: 100.00%] [G loss: 3.206000]\n",
      "epoch:17 step:13750 [D loss: 0.105688, acc.: 96.09%] [G loss: 4.589844]\n",
      "epoch:17 step:13751 [D loss: 0.669783, acc.: 66.41%] [G loss: 2.749481]\n",
      "epoch:17 step:13752 [D loss: 0.036434, acc.: 100.00%] [G loss: 3.998043]\n",
      "epoch:17 step:13753 [D loss: 0.057209, acc.: 99.22%] [G loss: 4.367158]\n",
      "epoch:17 step:13754 [D loss: 0.026109, acc.: 100.00%] [G loss: 3.980068]\n",
      "epoch:17 step:13755 [D loss: 0.101710, acc.: 98.44%] [G loss: 3.721222]\n",
      "epoch:17 step:13756 [D loss: 0.043822, acc.: 100.00%] [G loss: 5.354260]\n",
      "epoch:17 step:13757 [D loss: 0.021350, acc.: 100.00%] [G loss: 4.406607]\n",
      "epoch:17 step:13758 [D loss: 0.039897, acc.: 100.00%] [G loss: 4.509119]\n",
      "epoch:17 step:13759 [D loss: 0.145083, acc.: 97.66%] [G loss: 5.909197]\n",
      "epoch:17 step:13760 [D loss: 0.012593, acc.: 100.00%] [G loss: 6.242134]\n",
      "epoch:17 step:13761 [D loss: 0.023948, acc.: 99.22%] [G loss: 4.528296]\n",
      "epoch:17 step:13762 [D loss: 0.131957, acc.: 95.31%] [G loss: 1.718191]\n",
      "epoch:17 step:13763 [D loss: 0.254076, acc.: 87.50%] [G loss: 6.065976]\n",
      "epoch:17 step:13764 [D loss: 0.018723, acc.: 100.00%] [G loss: 7.634018]\n",
      "epoch:17 step:13765 [D loss: 1.410160, acc.: 50.78%] [G loss: 0.072701]\n",
      "epoch:17 step:13766 [D loss: 1.864389, acc.: 53.12%] [G loss: 7.288574]\n",
      "epoch:17 step:13767 [D loss: 2.215671, acc.: 50.00%] [G loss: 5.781621]\n",
      "epoch:17 step:13768 [D loss: 1.495540, acc.: 50.78%] [G loss: 3.098401]\n",
      "epoch:17 step:13769 [D loss: 0.170450, acc.: 93.75%] [G loss: 2.722353]\n",
      "epoch:17 step:13770 [D loss: 0.172810, acc.: 93.75%] [G loss: 3.007482]\n",
      "epoch:17 step:13771 [D loss: 0.056746, acc.: 100.00%] [G loss: 3.573890]\n",
      "epoch:17 step:13772 [D loss: 0.194155, acc.: 94.53%] [G loss: 3.563327]\n",
      "epoch:17 step:13773 [D loss: 0.040555, acc.: 100.00%] [G loss: 3.253114]\n",
      "epoch:17 step:13774 [D loss: 0.145773, acc.: 98.44%] [G loss: 1.893425]\n",
      "epoch:17 step:13775 [D loss: 0.142822, acc.: 98.44%] [G loss: 1.489955]\n",
      "epoch:17 step:13776 [D loss: 0.111810, acc.: 98.44%] [G loss: 1.457643]\n",
      "epoch:17 step:13777 [D loss: 0.062774, acc.: 100.00%] [G loss: 1.630096]\n",
      "epoch:17 step:13778 [D loss: 0.942116, acc.: 53.12%] [G loss: 4.791285]\n",
      "epoch:17 step:13779 [D loss: 0.632883, acc.: 67.19%] [G loss: 4.229922]\n",
      "epoch:17 step:13780 [D loss: 0.264188, acc.: 89.84%] [G loss: 2.765936]\n",
      "epoch:17 step:13781 [D loss: 0.064425, acc.: 99.22%] [G loss: 2.587578]\n",
      "epoch:17 step:13782 [D loss: 0.364417, acc.: 82.81%] [G loss: 2.289704]\n",
      "epoch:17 step:13783 [D loss: 0.048491, acc.: 100.00%] [G loss: 4.149804]\n",
      "epoch:17 step:13784 [D loss: 0.124229, acc.: 97.66%] [G loss: 2.172718]\n",
      "epoch:17 step:13785 [D loss: 0.183843, acc.: 97.66%] [G loss: 2.282671]\n",
      "epoch:17 step:13786 [D loss: 0.313693, acc.: 85.16%] [G loss: 2.175807]\n",
      "epoch:17 step:13787 [D loss: 0.069451, acc.: 98.44%] [G loss: 2.340192]\n",
      "epoch:17 step:13788 [D loss: 0.131226, acc.: 98.44%] [G loss: 2.461624]\n",
      "epoch:17 step:13789 [D loss: 0.247103, acc.: 89.06%] [G loss: 4.823294]\n",
      "epoch:17 step:13790 [D loss: 0.437558, acc.: 75.78%] [G loss: 2.791924]\n",
      "epoch:17 step:13791 [D loss: 0.083939, acc.: 99.22%] [G loss: 3.919291]\n",
      "epoch:17 step:13792 [D loss: 0.057030, acc.: 99.22%] [G loss: 3.418238]\n",
      "epoch:17 step:13793 [D loss: 0.059395, acc.: 99.22%] [G loss: 3.616419]\n",
      "epoch:17 step:13794 [D loss: 0.053783, acc.: 99.22%] [G loss: 3.441150]\n",
      "epoch:17 step:13795 [D loss: 0.086224, acc.: 99.22%] [G loss: 3.974455]\n",
      "epoch:17 step:13796 [D loss: 0.081603, acc.: 98.44%] [G loss: 3.764228]\n",
      "epoch:17 step:13797 [D loss: 0.108969, acc.: 97.66%] [G loss: 3.661465]\n",
      "epoch:17 step:13798 [D loss: 0.054004, acc.: 99.22%] [G loss: 3.251642]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:17 step:13799 [D loss: 0.180433, acc.: 95.31%] [G loss: 2.931766]\n",
      "epoch:17 step:13800 [D loss: 0.039135, acc.: 100.00%] [G loss: 2.862865]\n",
      "##############\n",
      "[1.09764263 0.87618297 0.97754229 0.89548301 1.02436912 0.99740268\n",
      " 1.11694681 1.01963619 1.11122648 1.06610291]\n",
      "##########\n",
      "epoch:17 step:13801 [D loss: 0.022383, acc.: 100.00%] [G loss: 2.458374]\n",
      "epoch:17 step:13802 [D loss: 0.028295, acc.: 100.00%] [G loss: 3.208597]\n",
      "epoch:17 step:13803 [D loss: 0.374124, acc.: 83.59%] [G loss: 5.068559]\n",
      "epoch:17 step:13804 [D loss: 0.054462, acc.: 100.00%] [G loss: 4.969620]\n",
      "epoch:17 step:13805 [D loss: 0.151539, acc.: 93.75%] [G loss: 3.740306]\n",
      "epoch:17 step:13806 [D loss: 0.054668, acc.: 99.22%] [G loss: 2.906786]\n",
      "epoch:17 step:13807 [D loss: 0.048288, acc.: 100.00%] [G loss: 4.286934]\n",
      "epoch:17 step:13808 [D loss: 0.024508, acc.: 100.00%] [G loss: 3.870111]\n",
      "epoch:17 step:13809 [D loss: 0.054811, acc.: 99.22%] [G loss: 2.760250]\n",
      "epoch:17 step:13810 [D loss: 0.108547, acc.: 99.22%] [G loss: 2.498610]\n",
      "epoch:17 step:13811 [D loss: 0.017871, acc.: 100.00%] [G loss: 2.200787]\n",
      "epoch:17 step:13812 [D loss: 0.015839, acc.: 100.00%] [G loss: 1.703298]\n",
      "epoch:17 step:13813 [D loss: 0.282165, acc.: 89.06%] [G loss: 4.223910]\n",
      "epoch:17 step:13814 [D loss: 0.662571, acc.: 72.66%] [G loss: 1.247328]\n",
      "epoch:17 step:13815 [D loss: 0.317658, acc.: 82.81%] [G loss: 4.403961]\n",
      "epoch:17 step:13816 [D loss: 0.073185, acc.: 96.88%] [G loss: 4.625095]\n",
      "epoch:17 step:13817 [D loss: 0.069431, acc.: 97.66%] [G loss: 3.823179]\n",
      "epoch:17 step:13818 [D loss: 0.050209, acc.: 99.22%] [G loss: 3.400840]\n",
      "epoch:17 step:13819 [D loss: 0.023312, acc.: 100.00%] [G loss: 2.815862]\n",
      "epoch:17 step:13820 [D loss: 0.074218, acc.: 99.22%] [G loss: 2.743276]\n",
      "epoch:17 step:13821 [D loss: 0.056238, acc.: 100.00%] [G loss: 2.945841]\n",
      "epoch:17 step:13822 [D loss: 0.057375, acc.: 100.00%] [G loss: 3.446661]\n",
      "epoch:17 step:13823 [D loss: 0.214466, acc.: 91.41%] [G loss: 1.966186]\n",
      "epoch:17 step:13824 [D loss: 0.226086, acc.: 90.62%] [G loss: 4.880681]\n",
      "epoch:17 step:13825 [D loss: 0.337050, acc.: 82.03%] [G loss: 3.532560]\n",
      "epoch:17 step:13826 [D loss: 0.070693, acc.: 99.22%] [G loss: 3.980774]\n",
      "epoch:17 step:13827 [D loss: 0.033138, acc.: 100.00%] [G loss: 4.674506]\n",
      "epoch:17 step:13828 [D loss: 0.013294, acc.: 100.00%] [G loss: 4.221703]\n",
      "epoch:17 step:13829 [D loss: 0.021822, acc.: 100.00%] [G loss: 4.109839]\n",
      "epoch:17 step:13830 [D loss: 0.014735, acc.: 100.00%] [G loss: 3.285309]\n",
      "epoch:17 step:13831 [D loss: 0.032064, acc.: 100.00%] [G loss: 2.894391]\n",
      "epoch:17 step:13832 [D loss: 0.009349, acc.: 100.00%] [G loss: 3.709020]\n",
      "epoch:17 step:13833 [D loss: 0.048124, acc.: 100.00%] [G loss: 3.559874]\n",
      "epoch:17 step:13834 [D loss: 0.122464, acc.: 98.44%] [G loss: 3.068751]\n",
      "epoch:17 step:13835 [D loss: 0.062804, acc.: 100.00%] [G loss: 3.261580]\n",
      "epoch:17 step:13836 [D loss: 0.027576, acc.: 100.00%] [G loss: 2.404356]\n",
      "epoch:17 step:13837 [D loss: 0.028527, acc.: 100.00%] [G loss: 2.778429]\n",
      "epoch:17 step:13838 [D loss: 0.035392, acc.: 99.22%] [G loss: 3.777401]\n",
      "epoch:17 step:13839 [D loss: 0.379547, acc.: 79.69%] [G loss: 6.387664]\n",
      "epoch:17 step:13840 [D loss: 1.121997, acc.: 53.12%] [G loss: 2.251430]\n",
      "epoch:17 step:13841 [D loss: 0.376596, acc.: 82.03%] [G loss: 5.799450]\n",
      "epoch:17 step:13842 [D loss: 0.082160, acc.: 97.66%] [G loss: 5.639652]\n",
      "epoch:17 step:13843 [D loss: 0.275844, acc.: 85.16%] [G loss: 4.492707]\n",
      "epoch:17 step:13844 [D loss: 0.037105, acc.: 99.22%] [G loss: 3.150304]\n",
      "epoch:17 step:13845 [D loss: 0.045669, acc.: 98.44%] [G loss: 3.830715]\n",
      "epoch:17 step:13846 [D loss: 0.022452, acc.: 100.00%] [G loss: 3.865696]\n",
      "epoch:17 step:13847 [D loss: 0.037262, acc.: 100.00%] [G loss: 3.911171]\n",
      "epoch:17 step:13848 [D loss: 0.013068, acc.: 100.00%] [G loss: 4.219940]\n",
      "epoch:17 step:13849 [D loss: 0.041246, acc.: 100.00%] [G loss: 3.669918]\n",
      "epoch:17 step:13850 [D loss: 0.038467, acc.: 100.00%] [G loss: 2.374364]\n",
      "epoch:17 step:13851 [D loss: 0.056287, acc.: 99.22%] [G loss: 2.529995]\n",
      "epoch:17 step:13852 [D loss: 0.194311, acc.: 93.75%] [G loss: 5.293043]\n",
      "epoch:17 step:13853 [D loss: 0.242732, acc.: 91.41%] [G loss: 3.300256]\n",
      "epoch:17 step:13854 [D loss: 0.188136, acc.: 93.75%] [G loss: 2.433804]\n",
      "epoch:17 step:13855 [D loss: 0.022061, acc.: 100.00%] [G loss: 2.729212]\n",
      "epoch:17 step:13856 [D loss: 0.050553, acc.: 99.22%] [G loss: 3.772930]\n",
      "epoch:17 step:13857 [D loss: 0.073330, acc.: 99.22%] [G loss: 3.963413]\n",
      "epoch:17 step:13858 [D loss: 0.030878, acc.: 100.00%] [G loss: 2.690216]\n",
      "epoch:17 step:13859 [D loss: 0.012694, acc.: 100.00%] [G loss: 2.114075]\n",
      "epoch:17 step:13860 [D loss: 0.183494, acc.: 92.19%] [G loss: 4.252127]\n",
      "epoch:17 step:13861 [D loss: 0.061498, acc.: 98.44%] [G loss: 4.563909]\n",
      "epoch:17 step:13862 [D loss: 0.100768, acc.: 98.44%] [G loss: 2.294840]\n",
      "epoch:17 step:13863 [D loss: 0.022251, acc.: 100.00%] [G loss: 3.008964]\n",
      "epoch:17 step:13864 [D loss: 0.135038, acc.: 96.88%] [G loss: 4.890317]\n",
      "epoch:17 step:13865 [D loss: 0.178748, acc.: 92.19%] [G loss: 2.022012]\n",
      "epoch:17 step:13866 [D loss: 0.046088, acc.: 100.00%] [G loss: 2.569474]\n",
      "epoch:17 step:13867 [D loss: 0.068859, acc.: 97.66%] [G loss: 3.972007]\n",
      "epoch:17 step:13868 [D loss: 0.032356, acc.: 100.00%] [G loss: 4.960117]\n",
      "epoch:17 step:13869 [D loss: 0.090930, acc.: 96.09%] [G loss: 3.099615]\n",
      "epoch:17 step:13870 [D loss: 0.067985, acc.: 100.00%] [G loss: 2.113230]\n",
      "epoch:17 step:13871 [D loss: 0.085221, acc.: 97.66%] [G loss: 4.151758]\n",
      "epoch:17 step:13872 [D loss: 0.012841, acc.: 100.00%] [G loss: 5.626215]\n",
      "epoch:17 step:13873 [D loss: 0.200964, acc.: 90.62%] [G loss: 1.644922]\n",
      "epoch:17 step:13874 [D loss: 0.402810, acc.: 78.91%] [G loss: 6.685742]\n",
      "epoch:17 step:13875 [D loss: 0.142056, acc.: 95.31%] [G loss: 7.050689]\n",
      "epoch:17 step:13876 [D loss: 0.780468, acc.: 65.62%] [G loss: 0.347686]\n",
      "epoch:17 step:13877 [D loss: 0.259161, acc.: 86.72%] [G loss: 1.971844]\n",
      "epoch:17 step:13878 [D loss: 0.009266, acc.: 100.00%] [G loss: 4.112074]\n",
      "epoch:17 step:13879 [D loss: 0.078007, acc.: 97.66%] [G loss: 3.126653]\n",
      "epoch:17 step:13880 [D loss: 0.187801, acc.: 91.41%] [G loss: 4.713535]\n",
      "epoch:17 step:13881 [D loss: 0.122551, acc.: 95.31%] [G loss: 4.905313]\n",
      "epoch:17 step:13882 [D loss: 0.088365, acc.: 98.44%] [G loss: 5.246933]\n",
      "epoch:17 step:13883 [D loss: 0.115408, acc.: 98.44%] [G loss: 4.585847]\n",
      "epoch:17 step:13884 [D loss: 0.061024, acc.: 100.00%] [G loss: 5.529603]\n",
      "epoch:17 step:13885 [D loss: 0.005499, acc.: 100.00%] [G loss: 5.869286]\n",
      "epoch:17 step:13886 [D loss: 0.016367, acc.: 100.00%] [G loss: 5.209905]\n",
      "epoch:17 step:13887 [D loss: 0.016259, acc.: 100.00%] [G loss: 4.906584]\n",
      "epoch:17 step:13888 [D loss: 0.014652, acc.: 100.00%] [G loss: 4.594525]\n",
      "epoch:17 step:13889 [D loss: 0.029719, acc.: 99.22%] [G loss: 4.789865]\n",
      "epoch:17 step:13890 [D loss: 0.076973, acc.: 99.22%] [G loss: 3.698966]\n",
      "epoch:17 step:13891 [D loss: 0.056262, acc.: 97.66%] [G loss: 5.151888]\n",
      "epoch:17 step:13892 [D loss: 0.023135, acc.: 99.22%] [G loss: 4.802512]\n",
      "epoch:17 step:13893 [D loss: 0.043110, acc.: 98.44%] [G loss: 2.436736]\n",
      "epoch:17 step:13894 [D loss: 0.051727, acc.: 99.22%] [G loss: 4.199533]\n",
      "epoch:17 step:13895 [D loss: 0.048713, acc.: 98.44%] [G loss: 4.200840]\n",
      "epoch:17 step:13896 [D loss: 2.938834, acc.: 28.91%] [G loss: 7.539913]\n",
      "epoch:17 step:13897 [D loss: 2.642030, acc.: 50.00%] [G loss: 5.689525]\n",
      "epoch:17 step:13898 [D loss: 1.968441, acc.: 50.00%] [G loss: 3.570770]\n",
      "epoch:17 step:13899 [D loss: 1.070384, acc.: 54.69%] [G loss: 1.978688]\n",
      "epoch:17 step:13900 [D loss: 0.945303, acc.: 47.66%] [G loss: 1.893499]\n",
      "epoch:17 step:13901 [D loss: 0.472310, acc.: 72.66%] [G loss: 2.161054]\n",
      "epoch:17 step:13902 [D loss: 0.356814, acc.: 84.38%] [G loss: 1.640905]\n",
      "epoch:17 step:13903 [D loss: 0.380305, acc.: 87.50%] [G loss: 1.571065]\n",
      "epoch:17 step:13904 [D loss: 0.273954, acc.: 90.62%] [G loss: 2.890904]\n",
      "epoch:17 step:13905 [D loss: 0.174671, acc.: 96.09%] [G loss: 2.233048]\n",
      "epoch:17 step:13906 [D loss: 0.332429, acc.: 88.28%] [G loss: 2.349876]\n",
      "epoch:17 step:13907 [D loss: 0.333099, acc.: 85.94%] [G loss: 2.476464]\n",
      "epoch:17 step:13908 [D loss: 0.277064, acc.: 92.19%] [G loss: 2.373635]\n",
      "epoch:17 step:13909 [D loss: 0.191071, acc.: 95.31%] [G loss: 3.028878]\n",
      "epoch:17 step:13910 [D loss: 0.322673, acc.: 85.94%] [G loss: 2.264090]\n",
      "epoch:17 step:13911 [D loss: 0.312759, acc.: 88.28%] [G loss: 2.897028]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:17 step:13912 [D loss: 0.258040, acc.: 93.75%] [G loss: 3.448792]\n",
      "epoch:17 step:13913 [D loss: 0.247784, acc.: 92.97%] [G loss: 1.942866]\n",
      "epoch:17 step:13914 [D loss: 0.168159, acc.: 94.53%] [G loss: 2.627316]\n",
      "epoch:17 step:13915 [D loss: 0.124424, acc.: 97.66%] [G loss: 2.322280]\n",
      "epoch:17 step:13916 [D loss: 0.215982, acc.: 92.97%] [G loss: 1.569764]\n",
      "epoch:17 step:13917 [D loss: 0.178117, acc.: 96.88%] [G loss: 2.775426]\n",
      "epoch:17 step:13918 [D loss: 0.124862, acc.: 96.09%] [G loss: 2.212582]\n",
      "epoch:17 step:13919 [D loss: 0.149471, acc.: 97.66%] [G loss: 1.327439]\n",
      "epoch:17 step:13920 [D loss: 0.184108, acc.: 94.53%] [G loss: 1.245785]\n",
      "epoch:17 step:13921 [D loss: 0.213082, acc.: 92.97%] [G loss: 3.001349]\n",
      "epoch:17 step:13922 [D loss: 0.423240, acc.: 82.81%] [G loss: 3.353418]\n",
      "epoch:17 step:13923 [D loss: 0.280841, acc.: 85.94%] [G loss: 1.535367]\n",
      "epoch:17 step:13924 [D loss: 0.149554, acc.: 96.09%] [G loss: 1.483512]\n",
      "epoch:17 step:13925 [D loss: 0.135178, acc.: 96.09%] [G loss: 2.134893]\n",
      "epoch:17 step:13926 [D loss: 0.380840, acc.: 80.47%] [G loss: 3.912944]\n",
      "epoch:17 step:13927 [D loss: 0.146033, acc.: 93.75%] [G loss: 3.861658]\n",
      "epoch:17 step:13928 [D loss: 0.366013, acc.: 78.91%] [G loss: 2.239080]\n",
      "epoch:17 step:13929 [D loss: 0.148433, acc.: 96.88%] [G loss: 1.160397]\n",
      "epoch:17 step:13930 [D loss: 0.077939, acc.: 100.00%] [G loss: 2.970200]\n",
      "epoch:17 step:13931 [D loss: 0.247011, acc.: 89.06%] [G loss: 2.346835]\n",
      "epoch:17 step:13932 [D loss: 0.044622, acc.: 100.00%] [G loss: 2.291291]\n",
      "epoch:17 step:13933 [D loss: 0.227203, acc.: 90.62%] [G loss: 3.324416]\n",
      "epoch:17 step:13934 [D loss: 0.429440, acc.: 82.03%] [G loss: 2.485684]\n",
      "epoch:17 step:13935 [D loss: 0.150578, acc.: 95.31%] [G loss: 2.505257]\n",
      "epoch:17 step:13936 [D loss: 0.311124, acc.: 85.16%] [G loss: 4.249594]\n",
      "epoch:17 step:13937 [D loss: 0.170034, acc.: 91.41%] [G loss: 3.909432]\n",
      "epoch:17 step:13938 [D loss: 0.127506, acc.: 96.09%] [G loss: 3.479994]\n",
      "epoch:17 step:13939 [D loss: 0.122556, acc.: 97.66%] [G loss: 2.593382]\n",
      "epoch:17 step:13940 [D loss: 0.267935, acc.: 86.72%] [G loss: 4.439632]\n",
      "epoch:17 step:13941 [D loss: 0.163560, acc.: 91.41%] [G loss: 3.939066]\n",
      "epoch:17 step:13942 [D loss: 0.317977, acc.: 85.94%] [G loss: 3.977713]\n",
      "epoch:17 step:13943 [D loss: 0.044536, acc.: 99.22%] [G loss: 4.047288]\n",
      "epoch:17 step:13944 [D loss: 0.160487, acc.: 95.31%] [G loss: 3.294523]\n",
      "epoch:17 step:13945 [D loss: 0.081119, acc.: 98.44%] [G loss: 3.917914]\n",
      "epoch:17 step:13946 [D loss: 0.062304, acc.: 100.00%] [G loss: 3.908241]\n",
      "epoch:17 step:13947 [D loss: 0.060520, acc.: 99.22%] [G loss: 4.038989]\n",
      "epoch:17 step:13948 [D loss: 0.066925, acc.: 99.22%] [G loss: 3.544461]\n",
      "epoch:17 step:13949 [D loss: 0.110913, acc.: 96.88%] [G loss: 3.849176]\n",
      "epoch:17 step:13950 [D loss: 0.039895, acc.: 100.00%] [G loss: 4.130472]\n",
      "epoch:17 step:13951 [D loss: 0.145301, acc.: 94.53%] [G loss: 3.288573]\n",
      "epoch:17 step:13952 [D loss: 0.107691, acc.: 97.66%] [G loss: 4.303296]\n",
      "epoch:17 step:13953 [D loss: 0.176473, acc.: 92.97%] [G loss: 3.455474]\n",
      "epoch:17 step:13954 [D loss: 0.085822, acc.: 96.88%] [G loss: 3.778947]\n",
      "epoch:17 step:13955 [D loss: 0.082790, acc.: 99.22%] [G loss: 3.235664]\n",
      "epoch:17 step:13956 [D loss: 0.062980, acc.: 98.44%] [G loss: 2.189122]\n",
      "epoch:17 step:13957 [D loss: 0.087501, acc.: 96.88%] [G loss: 3.030106]\n",
      "epoch:17 step:13958 [D loss: 0.112334, acc.: 96.88%] [G loss: 4.096634]\n",
      "epoch:17 step:13959 [D loss: 0.107674, acc.: 96.88%] [G loss: 3.039520]\n",
      "epoch:17 step:13960 [D loss: 0.030096, acc.: 100.00%] [G loss: 1.760033]\n",
      "epoch:17 step:13961 [D loss: 0.130077, acc.: 96.09%] [G loss: 3.599726]\n",
      "epoch:17 step:13962 [D loss: 0.395965, acc.: 82.81%] [G loss: 1.380116]\n",
      "epoch:17 step:13963 [D loss: 0.178682, acc.: 92.19%] [G loss: 2.803500]\n",
      "epoch:17 step:13964 [D loss: 0.033462, acc.: 100.00%] [G loss: 3.471398]\n",
      "epoch:17 step:13965 [D loss: 0.316205, acc.: 85.94%] [G loss: 0.627123]\n",
      "epoch:17 step:13966 [D loss: 0.402222, acc.: 82.03%] [G loss: 5.324749]\n",
      "epoch:17 step:13967 [D loss: 0.421742, acc.: 81.25%] [G loss: 3.268203]\n",
      "epoch:17 step:13968 [D loss: 0.073914, acc.: 99.22%] [G loss: 4.062025]\n",
      "epoch:17 step:13969 [D loss: 0.049010, acc.: 100.00%] [G loss: 3.546238]\n",
      "epoch:17 step:13970 [D loss: 0.032237, acc.: 100.00%] [G loss: 3.032048]\n",
      "epoch:17 step:13971 [D loss: 0.033012, acc.: 100.00%] [G loss: 3.880751]\n",
      "epoch:17 step:13972 [D loss: 0.084822, acc.: 98.44%] [G loss: 3.216110]\n",
      "epoch:17 step:13973 [D loss: 0.135540, acc.: 98.44%] [G loss: 2.910620]\n",
      "epoch:17 step:13974 [D loss: 0.143018, acc.: 95.31%] [G loss: 4.544852]\n",
      "epoch:17 step:13975 [D loss: 0.832778, acc.: 61.72%] [G loss: 5.503144]\n",
      "epoch:17 step:13976 [D loss: 0.107566, acc.: 95.31%] [G loss: 5.552254]\n",
      "epoch:17 step:13977 [D loss: 0.355488, acc.: 85.16%] [G loss: 3.637137]\n",
      "epoch:17 step:13978 [D loss: 0.182076, acc.: 89.84%] [G loss: 4.761480]\n",
      "epoch:17 step:13979 [D loss: 0.007790, acc.: 100.00%] [G loss: 5.435585]\n",
      "epoch:17 step:13980 [D loss: 0.446743, acc.: 79.69%] [G loss: 2.105323]\n",
      "epoch:17 step:13981 [D loss: 0.202542, acc.: 91.41%] [G loss: 4.766881]\n",
      "epoch:17 step:13982 [D loss: 0.016314, acc.: 100.00%] [G loss: 5.523017]\n",
      "epoch:17 step:13983 [D loss: 0.064039, acc.: 99.22%] [G loss: 4.734450]\n",
      "epoch:17 step:13984 [D loss: 0.159184, acc.: 94.53%] [G loss: 2.873516]\n",
      "epoch:17 step:13985 [D loss: 0.047344, acc.: 100.00%] [G loss: 3.887555]\n",
      "epoch:17 step:13986 [D loss: 0.015762, acc.: 100.00%] [G loss: 3.843604]\n",
      "epoch:17 step:13987 [D loss: 0.040570, acc.: 99.22%] [G loss: 2.672415]\n",
      "epoch:17 step:13988 [D loss: 0.201538, acc.: 92.19%] [G loss: 3.606256]\n",
      "epoch:17 step:13989 [D loss: 0.055547, acc.: 98.44%] [G loss: 3.078588]\n",
      "epoch:17 step:13990 [D loss: 0.156294, acc.: 96.09%] [G loss: 2.984300]\n",
      "epoch:17 step:13991 [D loss: 0.078868, acc.: 99.22%] [G loss: 4.362154]\n",
      "epoch:17 step:13992 [D loss: 0.888869, acc.: 48.44%] [G loss: 4.624438]\n",
      "epoch:17 step:13993 [D loss: 0.031990, acc.: 98.44%] [G loss: 5.822116]\n",
      "epoch:17 step:13994 [D loss: 0.125936, acc.: 96.88%] [G loss: 4.476941]\n",
      "epoch:17 step:13995 [D loss: 0.023833, acc.: 100.00%] [G loss: 3.708496]\n",
      "epoch:17 step:13996 [D loss: 0.033485, acc.: 99.22%] [G loss: 3.921887]\n",
      "epoch:17 step:13997 [D loss: 0.008338, acc.: 100.00%] [G loss: 4.038072]\n",
      "epoch:17 step:13998 [D loss: 0.030495, acc.: 99.22%] [G loss: 3.938601]\n",
      "epoch:17 step:13999 [D loss: 0.014087, acc.: 100.00%] [G loss: 3.406992]\n",
      "epoch:17 step:14000 [D loss: 0.038336, acc.: 98.44%] [G loss: 4.107426]\n",
      "##############\n",
      "[1.00758093 0.98362494 0.77363433 0.94780259 1.09947787 0.95471483\n",
      " 2.12002644 0.88185103 2.10583105 1.0152232 ]\n",
      "##########\n",
      "epoch:17 step:14001 [D loss: 0.066387, acc.: 100.00%] [G loss: 2.741478]\n",
      "epoch:17 step:14002 [D loss: 1.020986, acc.: 46.88%] [G loss: 5.211317]\n",
      "epoch:17 step:14003 [D loss: 0.076636, acc.: 99.22%] [G loss: 4.568450]\n",
      "epoch:17 step:14004 [D loss: 0.414742, acc.: 82.81%] [G loss: 2.105489]\n",
      "epoch:17 step:14005 [D loss: 0.191357, acc.: 90.62%] [G loss: 3.794960]\n",
      "epoch:17 step:14006 [D loss: 0.088032, acc.: 97.66%] [G loss: 2.887739]\n",
      "epoch:17 step:14007 [D loss: 0.099966, acc.: 96.88%] [G loss: 2.694740]\n",
      "epoch:17 step:14008 [D loss: 0.012154, acc.: 100.00%] [G loss: 4.234469]\n",
      "epoch:17 step:14009 [D loss: 0.061331, acc.: 100.00%] [G loss: 4.691234]\n",
      "epoch:17 step:14010 [D loss: 0.016931, acc.: 100.00%] [G loss: 4.412661]\n",
      "epoch:17 step:14011 [D loss: 0.020700, acc.: 100.00%] [G loss: 2.786485]\n",
      "epoch:17 step:14012 [D loss: 0.140127, acc.: 95.31%] [G loss: 3.488608]\n",
      "epoch:17 step:14013 [D loss: 0.023031, acc.: 100.00%] [G loss: 5.114515]\n",
      "epoch:17 step:14014 [D loss: 0.075475, acc.: 97.66%] [G loss: 3.665359]\n",
      "epoch:17 step:14015 [D loss: 0.043794, acc.: 99.22%] [G loss: 4.113517]\n",
      "epoch:17 step:14016 [D loss: 0.116918, acc.: 96.88%] [G loss: 2.795032]\n",
      "epoch:17 step:14017 [D loss: 0.097285, acc.: 97.66%] [G loss: 4.704051]\n",
      "epoch:17 step:14018 [D loss: 0.068854, acc.: 99.22%] [G loss: 4.592691]\n",
      "epoch:17 step:14019 [D loss: 0.028558, acc.: 100.00%] [G loss: 3.976977]\n",
      "epoch:17 step:14020 [D loss: 0.023655, acc.: 99.22%] [G loss: 4.368970]\n",
      "epoch:17 step:14021 [D loss: 0.032498, acc.: 100.00%] [G loss: 4.498487]\n",
      "epoch:17 step:14022 [D loss: 0.040443, acc.: 99.22%] [G loss: 3.762084]\n",
      "epoch:17 step:14023 [D loss: 0.106605, acc.: 96.88%] [G loss: 3.880553]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:17 step:14024 [D loss: 0.022721, acc.: 100.00%] [G loss: 3.076174]\n",
      "epoch:17 step:14025 [D loss: 0.095786, acc.: 98.44%] [G loss: 3.988918]\n",
      "epoch:17 step:14026 [D loss: 0.021421, acc.: 100.00%] [G loss: 5.176256]\n",
      "epoch:17 step:14027 [D loss: 0.643275, acc.: 66.41%] [G loss: 7.497965]\n",
      "epoch:17 step:14028 [D loss: 1.406153, acc.: 56.25%] [G loss: 4.984363]\n",
      "epoch:17 step:14029 [D loss: 0.181273, acc.: 92.19%] [G loss: 4.175339]\n",
      "epoch:17 step:14030 [D loss: 0.048004, acc.: 99.22%] [G loss: 5.080691]\n",
      "epoch:17 step:14031 [D loss: 0.109115, acc.: 97.66%] [G loss: 3.702142]\n",
      "epoch:17 step:14032 [D loss: 0.030656, acc.: 100.00%] [G loss: 3.341368]\n",
      "epoch:17 step:14033 [D loss: 0.047899, acc.: 99.22%] [G loss: 3.958982]\n",
      "epoch:17 step:14034 [D loss: 0.039931, acc.: 100.00%] [G loss: 4.799197]\n",
      "epoch:17 step:14035 [D loss: 0.016325, acc.: 100.00%] [G loss: 2.613412]\n",
      "epoch:17 step:14036 [D loss: 0.052847, acc.: 99.22%] [G loss: 2.423783]\n",
      "epoch:17 step:14037 [D loss: 0.145103, acc.: 95.31%] [G loss: 4.371866]\n",
      "epoch:17 step:14038 [D loss: 0.020036, acc.: 100.00%] [G loss: 5.347775]\n",
      "epoch:17 step:14039 [D loss: 1.430370, acc.: 29.69%] [G loss: 6.234412]\n",
      "epoch:17 step:14040 [D loss: 0.116339, acc.: 96.88%] [G loss: 6.205093]\n",
      "epoch:17 step:14041 [D loss: 0.388231, acc.: 85.94%] [G loss: 3.838975]\n",
      "epoch:17 step:14042 [D loss: 0.101336, acc.: 97.66%] [G loss: 4.041927]\n",
      "epoch:17 step:14043 [D loss: 0.022833, acc.: 100.00%] [G loss: 4.410982]\n",
      "epoch:17 step:14044 [D loss: 0.012743, acc.: 100.00%] [G loss: 5.275355]\n",
      "epoch:17 step:14045 [D loss: 0.042092, acc.: 99.22%] [G loss: 3.702588]\n",
      "epoch:17 step:14046 [D loss: 0.057732, acc.: 99.22%] [G loss: 4.217187]\n",
      "epoch:17 step:14047 [D loss: 0.044150, acc.: 98.44%] [G loss: 4.286402]\n",
      "epoch:17 step:14048 [D loss: 0.119540, acc.: 98.44%] [G loss: 2.514905]\n",
      "epoch:17 step:14049 [D loss: 0.099781, acc.: 99.22%] [G loss: 4.227192]\n",
      "epoch:17 step:14050 [D loss: 0.031882, acc.: 99.22%] [G loss: 4.568460]\n",
      "epoch:17 step:14051 [D loss: 0.040684, acc.: 100.00%] [G loss: 4.322400]\n",
      "epoch:17 step:14052 [D loss: 0.075432, acc.: 96.88%] [G loss: 3.588721]\n",
      "epoch:17 step:14053 [D loss: 0.251554, acc.: 85.94%] [G loss: 5.686987]\n",
      "epoch:17 step:14054 [D loss: 0.400652, acc.: 81.25%] [G loss: 3.856089]\n",
      "epoch:17 step:14055 [D loss: 0.032728, acc.: 100.00%] [G loss: 2.958592]\n",
      "epoch:17 step:14056 [D loss: 0.016773, acc.: 100.00%] [G loss: 4.467953]\n",
      "epoch:17 step:14057 [D loss: 0.014787, acc.: 100.00%] [G loss: 3.121036]\n",
      "epoch:17 step:14058 [D loss: 0.017641, acc.: 100.00%] [G loss: 3.993411]\n",
      "epoch:18 step:14059 [D loss: 0.030089, acc.: 100.00%] [G loss: 2.897902]\n",
      "epoch:18 step:14060 [D loss: 0.018265, acc.: 100.00%] [G loss: 3.209929]\n",
      "epoch:18 step:14061 [D loss: 0.035261, acc.: 100.00%] [G loss: 3.137175]\n",
      "epoch:18 step:14062 [D loss: 0.082422, acc.: 100.00%] [G loss: 2.959139]\n",
      "epoch:18 step:14063 [D loss: 0.066559, acc.: 96.88%] [G loss: 2.612742]\n",
      "epoch:18 step:14064 [D loss: 0.062567, acc.: 100.00%] [G loss: 4.716831]\n",
      "epoch:18 step:14065 [D loss: 0.034190, acc.: 100.00%] [G loss: 4.124636]\n",
      "epoch:18 step:14066 [D loss: 0.017430, acc.: 100.00%] [G loss: 3.459088]\n",
      "epoch:18 step:14067 [D loss: 0.045013, acc.: 100.00%] [G loss: 3.391675]\n",
      "epoch:18 step:14068 [D loss: 0.044572, acc.: 99.22%] [G loss: 2.914177]\n",
      "epoch:18 step:14069 [D loss: 0.028673, acc.: 100.00%] [G loss: 3.971287]\n",
      "epoch:18 step:14070 [D loss: 0.064070, acc.: 99.22%] [G loss: 2.969014]\n",
      "epoch:18 step:14071 [D loss: 0.107732, acc.: 96.88%] [G loss: 5.518405]\n",
      "epoch:18 step:14072 [D loss: 0.210558, acc.: 90.62%] [G loss: 3.502540]\n",
      "epoch:18 step:14073 [D loss: 0.030401, acc.: 100.00%] [G loss: 3.054224]\n",
      "epoch:18 step:14074 [D loss: 0.014141, acc.: 100.00%] [G loss: 4.601920]\n",
      "epoch:18 step:14075 [D loss: 0.004926, acc.: 100.00%] [G loss: 3.200097]\n",
      "epoch:18 step:14076 [D loss: 0.033459, acc.: 99.22%] [G loss: 3.513578]\n",
      "epoch:18 step:14077 [D loss: 0.005854, acc.: 100.00%] [G loss: 3.665564]\n",
      "epoch:18 step:14078 [D loss: 0.010616, acc.: 100.00%] [G loss: 4.297445]\n",
      "epoch:18 step:14079 [D loss: 0.040343, acc.: 99.22%] [G loss: 3.886486]\n",
      "epoch:18 step:14080 [D loss: 0.048402, acc.: 99.22%] [G loss: 3.955157]\n",
      "epoch:18 step:14081 [D loss: 0.096940, acc.: 96.09%] [G loss: 3.356748]\n",
      "epoch:18 step:14082 [D loss: 0.045068, acc.: 100.00%] [G loss: 3.790362]\n",
      "epoch:18 step:14083 [D loss: 0.016129, acc.: 100.00%] [G loss: 3.424831]\n",
      "epoch:18 step:14084 [D loss: 0.036524, acc.: 100.00%] [G loss: 3.703397]\n",
      "epoch:18 step:14085 [D loss: 0.014111, acc.: 100.00%] [G loss: 4.447666]\n",
      "epoch:18 step:14086 [D loss: 0.053555, acc.: 100.00%] [G loss: 3.810969]\n",
      "epoch:18 step:14087 [D loss: 0.023850, acc.: 100.00%] [G loss: 3.911253]\n",
      "epoch:18 step:14088 [D loss: 0.082941, acc.: 99.22%] [G loss: 3.391267]\n",
      "epoch:18 step:14089 [D loss: 0.045979, acc.: 100.00%] [G loss: 3.520476]\n",
      "epoch:18 step:14090 [D loss: 0.030050, acc.: 100.00%] [G loss: 4.136158]\n",
      "epoch:18 step:14091 [D loss: 0.306966, acc.: 87.50%] [G loss: 6.020179]\n",
      "epoch:18 step:14092 [D loss: 0.009110, acc.: 100.00%] [G loss: 7.266721]\n",
      "epoch:18 step:14093 [D loss: 3.064003, acc.: 18.75%] [G loss: 8.895752]\n",
      "epoch:18 step:14094 [D loss: 2.745742, acc.: 50.00%] [G loss: 5.216860]\n",
      "epoch:18 step:14095 [D loss: 0.934046, acc.: 60.94%] [G loss: 1.488696]\n",
      "epoch:18 step:14096 [D loss: 0.452637, acc.: 75.78%] [G loss: 3.198697]\n",
      "epoch:18 step:14097 [D loss: 0.142531, acc.: 92.97%] [G loss: 3.747195]\n",
      "epoch:18 step:14098 [D loss: 0.166506, acc.: 94.53%] [G loss: 3.845443]\n",
      "epoch:18 step:14099 [D loss: 0.471698, acc.: 78.91%] [G loss: 2.930978]\n",
      "epoch:18 step:14100 [D loss: 0.300210, acc.: 90.62%] [G loss: 2.882151]\n",
      "epoch:18 step:14101 [D loss: 0.150755, acc.: 96.09%] [G loss: 2.490949]\n",
      "epoch:18 step:14102 [D loss: 0.087438, acc.: 100.00%] [G loss: 2.944092]\n",
      "epoch:18 step:14103 [D loss: 0.125883, acc.: 96.88%] [G loss: 3.443436]\n",
      "epoch:18 step:14104 [D loss: 0.085887, acc.: 98.44%] [G loss: 3.820177]\n",
      "epoch:18 step:14105 [D loss: 0.104626, acc.: 97.66%] [G loss: 2.973171]\n",
      "epoch:18 step:14106 [D loss: 0.120813, acc.: 97.66%] [G loss: 3.625079]\n",
      "epoch:18 step:14107 [D loss: 0.257574, acc.: 89.84%] [G loss: 2.073925]\n",
      "epoch:18 step:14108 [D loss: 0.180463, acc.: 93.75%] [G loss: 3.588024]\n",
      "epoch:18 step:14109 [D loss: 0.121487, acc.: 96.88%] [G loss: 4.008630]\n",
      "epoch:18 step:14110 [D loss: 0.080389, acc.: 99.22%] [G loss: 3.677239]\n",
      "epoch:18 step:14111 [D loss: 0.105573, acc.: 98.44%] [G loss: 2.437335]\n",
      "epoch:18 step:14112 [D loss: 0.164790, acc.: 97.66%] [G loss: 2.334280]\n",
      "epoch:18 step:14113 [D loss: 0.105147, acc.: 98.44%] [G loss: 1.324464]\n",
      "epoch:18 step:14114 [D loss: 0.189819, acc.: 93.75%] [G loss: 2.925906]\n",
      "epoch:18 step:14115 [D loss: 0.355602, acc.: 83.59%] [G loss: 4.945020]\n",
      "epoch:18 step:14116 [D loss: 0.521871, acc.: 73.44%] [G loss: 2.745371]\n",
      "epoch:18 step:14117 [D loss: 0.129383, acc.: 96.09%] [G loss: 2.915330]\n",
      "epoch:18 step:14118 [D loss: 0.137977, acc.: 98.44%] [G loss: 2.681116]\n",
      "epoch:18 step:14119 [D loss: 0.085193, acc.: 98.44%] [G loss: 3.580503]\n",
      "epoch:18 step:14120 [D loss: 0.110896, acc.: 96.88%] [G loss: 2.745104]\n",
      "epoch:18 step:14121 [D loss: 0.503665, acc.: 74.22%] [G loss: 3.785029]\n",
      "epoch:18 step:14122 [D loss: 0.105313, acc.: 96.88%] [G loss: 3.468788]\n",
      "epoch:18 step:14123 [D loss: 0.159712, acc.: 92.19%] [G loss: 1.091974]\n",
      "epoch:18 step:14124 [D loss: 0.221553, acc.: 92.97%] [G loss: 3.214001]\n",
      "epoch:18 step:14125 [D loss: 0.089271, acc.: 98.44%] [G loss: 3.210244]\n",
      "epoch:18 step:14126 [D loss: 0.197221, acc.: 92.19%] [G loss: 1.721305]\n",
      "epoch:18 step:14127 [D loss: 0.097910, acc.: 97.66%] [G loss: 3.055970]\n",
      "epoch:18 step:14128 [D loss: 0.029102, acc.: 100.00%] [G loss: 2.077802]\n",
      "epoch:18 step:14129 [D loss: 0.154122, acc.: 98.44%] [G loss: 2.316679]\n",
      "epoch:18 step:14130 [D loss: 0.172935, acc.: 92.97%] [G loss: 2.145094]\n",
      "epoch:18 step:14131 [D loss: 0.043623, acc.: 99.22%] [G loss: 3.172271]\n",
      "epoch:18 step:14132 [D loss: 0.154238, acc.: 96.09%] [G loss: 2.670694]\n",
      "epoch:18 step:14133 [D loss: 0.307942, acc.: 87.50%] [G loss: 1.268061]\n",
      "epoch:18 step:14134 [D loss: 0.119604, acc.: 98.44%] [G loss: 3.283396]\n",
      "epoch:18 step:14135 [D loss: 0.289800, acc.: 85.94%] [G loss: 5.120274]\n",
      "epoch:18 step:14136 [D loss: 0.080630, acc.: 96.09%] [G loss: 4.970493]\n",
      "epoch:18 step:14137 [D loss: 0.474068, acc.: 78.91%] [G loss: 1.331823]\n",
      "epoch:18 step:14138 [D loss: 0.152772, acc.: 94.53%] [G loss: 3.277883]\n",
      "epoch:18 step:14139 [D loss: 0.018951, acc.: 100.00%] [G loss: 4.279167]\n",
      "epoch:18 step:14140 [D loss: 0.034321, acc.: 100.00%] [G loss: 3.097874]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:18 step:14141 [D loss: 0.050271, acc.: 99.22%] [G loss: 2.917987]\n",
      "epoch:18 step:14142 [D loss: 0.079813, acc.: 100.00%] [G loss: 2.955328]\n",
      "epoch:18 step:14143 [D loss: 0.069365, acc.: 99.22%] [G loss: 4.051156]\n",
      "epoch:18 step:14144 [D loss: 0.025441, acc.: 100.00%] [G loss: 4.163359]\n",
      "epoch:18 step:14145 [D loss: 0.054731, acc.: 100.00%] [G loss: 3.730184]\n",
      "epoch:18 step:14146 [D loss: 0.123090, acc.: 98.44%] [G loss: 3.020981]\n",
      "epoch:18 step:14147 [D loss: 0.065310, acc.: 99.22%] [G loss: 3.800356]\n",
      "epoch:18 step:14148 [D loss: 2.093083, acc.: 15.62%] [G loss: 6.461269]\n",
      "epoch:18 step:14149 [D loss: 0.381232, acc.: 78.91%] [G loss: 6.275623]\n",
      "epoch:18 step:14150 [D loss: 0.035890, acc.: 100.00%] [G loss: 5.675852]\n",
      "epoch:18 step:14151 [D loss: 0.015613, acc.: 99.22%] [G loss: 5.023151]\n",
      "epoch:18 step:14152 [D loss: 0.006666, acc.: 100.00%] [G loss: 5.576691]\n",
      "epoch:18 step:14153 [D loss: 0.015506, acc.: 100.00%] [G loss: 3.935109]\n",
      "epoch:18 step:14154 [D loss: 0.022652, acc.: 100.00%] [G loss: 4.359797]\n",
      "epoch:18 step:14155 [D loss: 0.014235, acc.: 100.00%] [G loss: 3.555884]\n",
      "epoch:18 step:14156 [D loss: 0.041380, acc.: 100.00%] [G loss: 4.206243]\n",
      "epoch:18 step:14157 [D loss: 0.141649, acc.: 96.09%] [G loss: 2.133976]\n",
      "epoch:18 step:14158 [D loss: 0.069219, acc.: 99.22%] [G loss: 3.196332]\n",
      "epoch:18 step:14159 [D loss: 0.237504, acc.: 91.41%] [G loss: 3.991285]\n",
      "epoch:18 step:14160 [D loss: 0.023506, acc.: 99.22%] [G loss: 3.845474]\n",
      "epoch:18 step:14161 [D loss: 0.517274, acc.: 74.22%] [G loss: 0.823923]\n",
      "epoch:18 step:14162 [D loss: 0.674167, acc.: 74.22%] [G loss: 6.699789]\n",
      "epoch:18 step:14163 [D loss: 1.416357, acc.: 53.12%] [G loss: 5.140689]\n",
      "epoch:18 step:14164 [D loss: 0.245410, acc.: 91.41%] [G loss: 3.571836]\n",
      "epoch:18 step:14165 [D loss: 0.071494, acc.: 98.44%] [G loss: 3.743993]\n",
      "epoch:18 step:14166 [D loss: 0.027736, acc.: 100.00%] [G loss: 2.752292]\n",
      "epoch:18 step:14167 [D loss: 0.075088, acc.: 96.09%] [G loss: 3.751423]\n",
      "epoch:18 step:14168 [D loss: 0.056262, acc.: 98.44%] [G loss: 3.355675]\n",
      "epoch:18 step:14169 [D loss: 0.063757, acc.: 99.22%] [G loss: 2.627903]\n",
      "epoch:18 step:14170 [D loss: 0.025857, acc.: 100.00%] [G loss: 4.612347]\n",
      "epoch:18 step:14171 [D loss: 0.021801, acc.: 100.00%] [G loss: 3.974621]\n",
      "epoch:18 step:14172 [D loss: 0.087864, acc.: 97.66%] [G loss: 4.136253]\n",
      "epoch:18 step:14173 [D loss: 0.028696, acc.: 100.00%] [G loss: 2.856988]\n",
      "epoch:18 step:14174 [D loss: 0.150303, acc.: 93.75%] [G loss: 2.252683]\n",
      "epoch:18 step:14175 [D loss: 0.096857, acc.: 96.88%] [G loss: 2.896808]\n",
      "epoch:18 step:14176 [D loss: 0.011252, acc.: 100.00%] [G loss: 2.863977]\n",
      "epoch:18 step:14177 [D loss: 0.039625, acc.: 100.00%] [G loss: 2.704444]\n",
      "epoch:18 step:14178 [D loss: 0.019311, acc.: 100.00%] [G loss: 2.892876]\n",
      "epoch:18 step:14179 [D loss: 0.149163, acc.: 93.75%] [G loss: 3.694620]\n",
      "epoch:18 step:14180 [D loss: 0.038603, acc.: 99.22%] [G loss: 4.285766]\n",
      "epoch:18 step:14181 [D loss: 0.045578, acc.: 99.22%] [G loss: 3.308962]\n",
      "epoch:18 step:14182 [D loss: 0.029688, acc.: 100.00%] [G loss: 3.871669]\n",
      "epoch:18 step:14183 [D loss: 0.024106, acc.: 100.00%] [G loss: 3.170643]\n",
      "epoch:18 step:14184 [D loss: 0.041434, acc.: 99.22%] [G loss: 3.433229]\n",
      "epoch:18 step:14185 [D loss: 0.025492, acc.: 100.00%] [G loss: 4.510015]\n",
      "epoch:18 step:14186 [D loss: 0.102939, acc.: 97.66%] [G loss: 3.122905]\n",
      "epoch:18 step:14187 [D loss: 0.039106, acc.: 100.00%] [G loss: 4.198287]\n",
      "epoch:18 step:14188 [D loss: 0.025732, acc.: 100.00%] [G loss: 4.054929]\n",
      "epoch:18 step:14189 [D loss: 0.011274, acc.: 100.00%] [G loss: 4.662809]\n",
      "epoch:18 step:14190 [D loss: 0.170950, acc.: 93.75%] [G loss: 2.706352]\n",
      "epoch:18 step:14191 [D loss: 0.021785, acc.: 100.00%] [G loss: 3.509404]\n",
      "epoch:18 step:14192 [D loss: 0.094286, acc.: 97.66%] [G loss: 5.371603]\n",
      "epoch:18 step:14193 [D loss: 0.019287, acc.: 100.00%] [G loss: 5.308856]\n",
      "epoch:18 step:14194 [D loss: 0.121312, acc.: 96.88%] [G loss: 4.330194]\n",
      "epoch:18 step:14195 [D loss: 0.029364, acc.: 100.00%] [G loss: 3.803699]\n",
      "epoch:18 step:14196 [D loss: 0.007810, acc.: 100.00%] [G loss: 3.572402]\n",
      "epoch:18 step:14197 [D loss: 0.017213, acc.: 100.00%] [G loss: 4.172476]\n",
      "epoch:18 step:14198 [D loss: 0.044124, acc.: 98.44%] [G loss: 3.448975]\n",
      "epoch:18 step:14199 [D loss: 0.024566, acc.: 100.00%] [G loss: 3.693659]\n",
      "epoch:18 step:14200 [D loss: 0.013075, acc.: 100.00%] [G loss: 3.238524]\n",
      "##############\n",
      "[1.00697496 1.03026389 2.11671436 0.96539679 0.87800531 0.94192789\n",
      " 2.11638684 2.11571064 1.0239065  0.87758637]\n",
      "##########\n",
      "epoch:18 step:14201 [D loss: 0.077433, acc.: 99.22%] [G loss: 4.591115]\n",
      "epoch:18 step:14202 [D loss: 0.046755, acc.: 99.22%] [G loss: 4.623619]\n",
      "epoch:18 step:14203 [D loss: 0.164550, acc.: 96.09%] [G loss: 4.978848]\n",
      "epoch:18 step:14204 [D loss: 0.020739, acc.: 100.00%] [G loss: 4.941402]\n",
      "epoch:18 step:14205 [D loss: 0.026600, acc.: 100.00%] [G loss: 4.366237]\n",
      "epoch:18 step:14206 [D loss: 0.016728, acc.: 100.00%] [G loss: 4.686578]\n",
      "epoch:18 step:14207 [D loss: 0.006208, acc.: 100.00%] [G loss: 5.055893]\n",
      "epoch:18 step:14208 [D loss: 0.033769, acc.: 100.00%] [G loss: 5.513502]\n",
      "epoch:18 step:14209 [D loss: 0.048247, acc.: 100.00%] [G loss: 5.243313]\n",
      "epoch:18 step:14210 [D loss: 0.050156, acc.: 99.22%] [G loss: 4.712416]\n",
      "epoch:18 step:14211 [D loss: 1.121700, acc.: 52.34%] [G loss: 6.754665]\n",
      "epoch:18 step:14212 [D loss: 0.637193, acc.: 70.31%] [G loss: 4.934953]\n",
      "epoch:18 step:14213 [D loss: 0.063090, acc.: 97.66%] [G loss: 4.437727]\n",
      "epoch:18 step:14214 [D loss: 0.151603, acc.: 94.53%] [G loss: 3.908693]\n",
      "epoch:18 step:14215 [D loss: 0.113441, acc.: 95.31%] [G loss: 4.286523]\n",
      "epoch:18 step:14216 [D loss: 0.265362, acc.: 87.50%] [G loss: 5.404751]\n",
      "epoch:18 step:14217 [D loss: 0.005463, acc.: 100.00%] [G loss: 6.328573]\n",
      "epoch:18 step:14218 [D loss: 0.159072, acc.: 94.53%] [G loss: 4.928271]\n",
      "epoch:18 step:14219 [D loss: 0.212035, acc.: 89.84%] [G loss: 4.052662]\n",
      "epoch:18 step:14220 [D loss: 0.012673, acc.: 100.00%] [G loss: 3.764696]\n",
      "epoch:18 step:14221 [D loss: 0.022421, acc.: 100.00%] [G loss: 3.505420]\n",
      "epoch:18 step:14222 [D loss: 0.011526, acc.: 100.00%] [G loss: 5.269591]\n",
      "epoch:18 step:14223 [D loss: 0.050387, acc.: 97.66%] [G loss: 3.075888]\n",
      "epoch:18 step:14224 [D loss: 0.055057, acc.: 99.22%] [G loss: 4.902383]\n",
      "epoch:18 step:14225 [D loss: 0.039460, acc.: 100.00%] [G loss: 3.928763]\n",
      "epoch:18 step:14226 [D loss: 0.005383, acc.: 100.00%] [G loss: 3.968382]\n",
      "epoch:18 step:14227 [D loss: 0.029465, acc.: 100.00%] [G loss: 4.106792]\n",
      "epoch:18 step:14228 [D loss: 1.699703, acc.: 35.16%] [G loss: 7.670824]\n",
      "epoch:18 step:14229 [D loss: 2.424119, acc.: 50.00%] [G loss: 5.681595]\n",
      "epoch:18 step:14230 [D loss: 1.131874, acc.: 54.69%] [G loss: 1.281178]\n",
      "epoch:18 step:14231 [D loss: 0.975013, acc.: 64.84%] [G loss: 4.271178]\n",
      "epoch:18 step:14232 [D loss: 0.551260, acc.: 75.00%] [G loss: 4.571064]\n",
      "epoch:18 step:14233 [D loss: 0.294526, acc.: 86.72%] [G loss: 2.833162]\n",
      "epoch:18 step:14234 [D loss: 0.154525, acc.: 94.53%] [G loss: 2.721134]\n",
      "epoch:18 step:14235 [D loss: 0.148886, acc.: 96.88%] [G loss: 2.986825]\n",
      "epoch:18 step:14236 [D loss: 0.133844, acc.: 97.66%] [G loss: 3.808707]\n",
      "epoch:18 step:14237 [D loss: 0.222506, acc.: 93.75%] [G loss: 3.616828]\n",
      "epoch:18 step:14238 [D loss: 0.367837, acc.: 85.16%] [G loss: 3.158493]\n",
      "epoch:18 step:14239 [D loss: 0.226358, acc.: 92.19%] [G loss: 2.642869]\n",
      "epoch:18 step:14240 [D loss: 0.347602, acc.: 85.94%] [G loss: 4.165234]\n",
      "epoch:18 step:14241 [D loss: 0.423792, acc.: 79.69%] [G loss: 3.653569]\n",
      "epoch:18 step:14242 [D loss: 0.098054, acc.: 99.22%] [G loss: 2.795428]\n",
      "epoch:18 step:14243 [D loss: 0.086474, acc.: 100.00%] [G loss: 2.081682]\n",
      "epoch:18 step:14244 [D loss: 0.189044, acc.: 92.97%] [G loss: 3.327733]\n",
      "epoch:18 step:14245 [D loss: 0.287935, acc.: 86.72%] [G loss: 3.280620]\n",
      "epoch:18 step:14246 [D loss: 0.076638, acc.: 98.44%] [G loss: 2.975997]\n",
      "epoch:18 step:14247 [D loss: 0.097860, acc.: 98.44%] [G loss: 2.618735]\n",
      "epoch:18 step:14248 [D loss: 0.173763, acc.: 95.31%] [G loss: 1.559371]\n",
      "epoch:18 step:14249 [D loss: 0.070163, acc.: 100.00%] [G loss: 2.906809]\n",
      "epoch:18 step:14250 [D loss: 0.127896, acc.: 98.44%] [G loss: 1.863184]\n",
      "epoch:18 step:14251 [D loss: 0.125924, acc.: 99.22%] [G loss: 3.906096]\n",
      "epoch:18 step:14252 [D loss: 0.205534, acc.: 94.53%] [G loss: 2.325353]\n",
      "epoch:18 step:14253 [D loss: 0.197250, acc.: 96.09%] [G loss: 2.915133]\n",
      "epoch:18 step:14254 [D loss: 0.394063, acc.: 80.47%] [G loss: 3.273978]\n",
      "epoch:18 step:14255 [D loss: 0.057036, acc.: 98.44%] [G loss: 3.929427]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:18 step:14256 [D loss: 0.038647, acc.: 100.00%] [G loss: 4.421184]\n",
      "epoch:18 step:14257 [D loss: 0.071099, acc.: 99.22%] [G loss: 2.665118]\n",
      "epoch:18 step:14258 [D loss: 0.123088, acc.: 96.88%] [G loss: 4.032360]\n",
      "epoch:18 step:14259 [D loss: 0.031440, acc.: 100.00%] [G loss: 3.962983]\n",
      "epoch:18 step:14260 [D loss: 0.471652, acc.: 77.34%] [G loss: 3.801476]\n",
      "epoch:18 step:14261 [D loss: 0.115029, acc.: 95.31%] [G loss: 4.507468]\n",
      "epoch:18 step:14262 [D loss: 0.067550, acc.: 97.66%] [G loss: 3.696532]\n",
      "epoch:18 step:14263 [D loss: 0.084328, acc.: 96.88%] [G loss: 3.483433]\n",
      "epoch:18 step:14264 [D loss: 0.098454, acc.: 97.66%] [G loss: 3.750240]\n",
      "epoch:18 step:14265 [D loss: 0.032695, acc.: 99.22%] [G loss: 4.537257]\n",
      "epoch:18 step:14266 [D loss: 0.303406, acc.: 90.62%] [G loss: 2.073764]\n",
      "epoch:18 step:14267 [D loss: 0.170822, acc.: 92.97%] [G loss: 4.077849]\n",
      "epoch:18 step:14268 [D loss: 0.033542, acc.: 99.22%] [G loss: 4.885045]\n",
      "epoch:18 step:14269 [D loss: 0.057855, acc.: 99.22%] [G loss: 4.192493]\n",
      "epoch:18 step:14270 [D loss: 0.080741, acc.: 99.22%] [G loss: 3.757222]\n",
      "epoch:18 step:14271 [D loss: 0.058668, acc.: 100.00%] [G loss: 3.061792]\n",
      "epoch:18 step:14272 [D loss: 0.217026, acc.: 94.53%] [G loss: 4.331965]\n",
      "epoch:18 step:14273 [D loss: 0.024459, acc.: 100.00%] [G loss: 5.105999]\n",
      "epoch:18 step:14274 [D loss: 0.241744, acc.: 90.62%] [G loss: 2.541519]\n",
      "epoch:18 step:14275 [D loss: 0.159376, acc.: 96.09%] [G loss: 4.468729]\n",
      "epoch:18 step:14276 [D loss: 0.058075, acc.: 99.22%] [G loss: 4.046126]\n",
      "epoch:18 step:14277 [D loss: 0.015819, acc.: 100.00%] [G loss: 4.515812]\n",
      "epoch:18 step:14278 [D loss: 0.039446, acc.: 99.22%] [G loss: 3.862382]\n",
      "epoch:18 step:14279 [D loss: 0.025472, acc.: 100.00%] [G loss: 3.868912]\n",
      "epoch:18 step:14280 [D loss: 0.021609, acc.: 100.00%] [G loss: 3.479142]\n",
      "epoch:18 step:14281 [D loss: 0.055741, acc.: 98.44%] [G loss: 3.639337]\n",
      "epoch:18 step:14282 [D loss: 0.042317, acc.: 99.22%] [G loss: 2.738510]\n",
      "epoch:18 step:14283 [D loss: 0.017763, acc.: 100.00%] [G loss: 3.455677]\n",
      "epoch:18 step:14284 [D loss: 0.056130, acc.: 100.00%] [G loss: 4.437919]\n",
      "epoch:18 step:14285 [D loss: 0.019323, acc.: 100.00%] [G loss: 4.785764]\n",
      "epoch:18 step:14286 [D loss: 0.020874, acc.: 100.00%] [G loss: 4.152476]\n",
      "epoch:18 step:14287 [D loss: 0.142822, acc.: 96.88%] [G loss: 3.681611]\n",
      "epoch:18 step:14288 [D loss: 0.006826, acc.: 100.00%] [G loss: 4.803075]\n",
      "epoch:18 step:14289 [D loss: 0.019743, acc.: 100.00%] [G loss: 4.722183]\n",
      "epoch:18 step:14290 [D loss: 0.372444, acc.: 82.03%] [G loss: 5.125128]\n",
      "epoch:18 step:14291 [D loss: 0.058336, acc.: 98.44%] [G loss: 5.216393]\n",
      "epoch:18 step:14292 [D loss: 0.234196, acc.: 92.19%] [G loss: 3.220094]\n",
      "epoch:18 step:14293 [D loss: 0.099077, acc.: 97.66%] [G loss: 3.146476]\n",
      "epoch:18 step:14294 [D loss: 0.017596, acc.: 100.00%] [G loss: 4.460971]\n",
      "epoch:18 step:14295 [D loss: 0.007047, acc.: 100.00%] [G loss: 3.956099]\n",
      "epoch:18 step:14296 [D loss: 0.025984, acc.: 100.00%] [G loss: 3.036906]\n",
      "epoch:18 step:14297 [D loss: 0.018868, acc.: 100.00%] [G loss: 3.099341]\n",
      "epoch:18 step:14298 [D loss: 1.870675, acc.: 39.84%] [G loss: 7.632698]\n",
      "epoch:18 step:14299 [D loss: 2.083867, acc.: 50.00%] [G loss: 4.479764]\n",
      "epoch:18 step:14300 [D loss: 0.249668, acc.: 88.28%] [G loss: 2.423475]\n",
      "epoch:18 step:14301 [D loss: 0.302575, acc.: 83.59%] [G loss: 3.403239]\n",
      "epoch:18 step:14302 [D loss: 0.015744, acc.: 100.00%] [G loss: 4.472577]\n",
      "epoch:18 step:14303 [D loss: 0.029967, acc.: 100.00%] [G loss: 3.922063]\n",
      "epoch:18 step:14304 [D loss: 0.272755, acc.: 89.06%] [G loss: 2.890954]\n",
      "epoch:18 step:14305 [D loss: 0.199163, acc.: 92.19%] [G loss: 4.044168]\n",
      "epoch:18 step:14306 [D loss: 0.037244, acc.: 100.00%] [G loss: 4.147794]\n",
      "epoch:18 step:14307 [D loss: 0.075911, acc.: 97.66%] [G loss: 3.761597]\n",
      "epoch:18 step:14308 [D loss: 0.438711, acc.: 79.69%] [G loss: 3.326255]\n",
      "epoch:18 step:14309 [D loss: 0.080204, acc.: 99.22%] [G loss: 1.855241]\n",
      "epoch:18 step:14310 [D loss: 0.080819, acc.: 98.44%] [G loss: 2.108693]\n",
      "epoch:18 step:14311 [D loss: 0.138311, acc.: 96.88%] [G loss: 1.892290]\n",
      "epoch:18 step:14312 [D loss: 0.083883, acc.: 99.22%] [G loss: 0.907963]\n",
      "epoch:18 step:14313 [D loss: 0.109703, acc.: 96.88%] [G loss: 1.123774]\n",
      "epoch:18 step:14314 [D loss: 0.049740, acc.: 100.00%] [G loss: 4.178652]\n",
      "epoch:18 step:14315 [D loss: 0.117611, acc.: 97.66%] [G loss: 2.505390]\n",
      "epoch:18 step:14316 [D loss: 0.252646, acc.: 91.41%] [G loss: 2.280074]\n",
      "epoch:18 step:14317 [D loss: 0.207078, acc.: 91.41%] [G loss: 3.480260]\n",
      "epoch:18 step:14318 [D loss: 0.164219, acc.: 95.31%] [G loss: 0.356105]\n",
      "epoch:18 step:14319 [D loss: 0.020709, acc.: 100.00%] [G loss: 1.567183]\n",
      "epoch:18 step:14320 [D loss: 0.011672, acc.: 100.00%] [G loss: 1.429232]\n",
      "epoch:18 step:14321 [D loss: 0.039406, acc.: 100.00%] [G loss: 2.249706]\n",
      "epoch:18 step:14322 [D loss: 0.216993, acc.: 91.41%] [G loss: 0.683127]\n",
      "epoch:18 step:14323 [D loss: 0.029178, acc.: 100.00%] [G loss: 2.361889]\n",
      "epoch:18 step:14324 [D loss: 0.022549, acc.: 100.00%] [G loss: 0.260316]\n",
      "epoch:18 step:14325 [D loss: 0.017596, acc.: 100.00%] [G loss: 1.634852]\n",
      "epoch:18 step:14326 [D loss: 0.026926, acc.: 100.00%] [G loss: 0.357266]\n",
      "epoch:18 step:14327 [D loss: 0.084509, acc.: 99.22%] [G loss: 1.343588]\n",
      "epoch:18 step:14328 [D loss: 0.114504, acc.: 96.88%] [G loss: 0.098207]\n",
      "epoch:18 step:14329 [D loss: 0.079890, acc.: 98.44%] [G loss: 0.219104]\n",
      "epoch:18 step:14330 [D loss: 0.007564, acc.: 100.00%] [G loss: 0.456102]\n",
      "epoch:18 step:14331 [D loss: 0.159448, acc.: 94.53%] [G loss: 1.118439]\n",
      "epoch:18 step:14332 [D loss: 0.006266, acc.: 100.00%] [G loss: 2.403595]\n",
      "epoch:18 step:14333 [D loss: 0.117884, acc.: 95.31%] [G loss: 0.854857]\n",
      "epoch:18 step:14334 [D loss: 0.167410, acc.: 92.97%] [G loss: 4.290504]\n",
      "epoch:18 step:14335 [D loss: 0.347409, acc.: 81.25%] [G loss: 0.505939]\n",
      "epoch:18 step:14336 [D loss: 0.448011, acc.: 77.34%] [G loss: 5.441967]\n",
      "epoch:18 step:14337 [D loss: 0.224087, acc.: 91.41%] [G loss: 5.773211]\n",
      "epoch:18 step:14338 [D loss: 0.102802, acc.: 94.53%] [G loss: 4.551973]\n",
      "epoch:18 step:14339 [D loss: 0.016672, acc.: 100.00%] [G loss: 3.115718]\n",
      "epoch:18 step:14340 [D loss: 0.013192, acc.: 100.00%] [G loss: 2.535805]\n",
      "epoch:18 step:14341 [D loss: 0.023393, acc.: 100.00%] [G loss: 2.744090]\n",
      "epoch:18 step:14342 [D loss: 0.149861, acc.: 95.31%] [G loss: 5.053249]\n",
      "epoch:18 step:14343 [D loss: 0.063639, acc.: 97.66%] [G loss: 5.017460]\n",
      "epoch:18 step:14344 [D loss: 0.615919, acc.: 66.41%] [G loss: 0.813020]\n",
      "epoch:18 step:14345 [D loss: 0.921331, acc.: 60.16%] [G loss: 6.915143]\n",
      "epoch:18 step:14346 [D loss: 0.863731, acc.: 62.50%] [G loss: 5.894280]\n",
      "epoch:18 step:14347 [D loss: 0.043717, acc.: 98.44%] [G loss: 5.579210]\n",
      "epoch:18 step:14348 [D loss: 0.037189, acc.: 99.22%] [G loss: 4.486471]\n",
      "epoch:18 step:14349 [D loss: 0.013764, acc.: 100.00%] [G loss: 4.571262]\n",
      "epoch:18 step:14350 [D loss: 0.098847, acc.: 96.88%] [G loss: 4.441710]\n",
      "epoch:18 step:14351 [D loss: 0.027809, acc.: 100.00%] [G loss: 4.416219]\n",
      "epoch:18 step:14352 [D loss: 0.046558, acc.: 99.22%] [G loss: 4.217339]\n",
      "epoch:18 step:14353 [D loss: 0.120913, acc.: 96.09%] [G loss: 3.189363]\n",
      "epoch:18 step:14354 [D loss: 0.098852, acc.: 96.88%] [G loss: 3.276535]\n",
      "epoch:18 step:14355 [D loss: 0.049451, acc.: 100.00%] [G loss: 3.451663]\n",
      "epoch:18 step:14356 [D loss: 0.067912, acc.: 97.66%] [G loss: 4.122493]\n",
      "epoch:18 step:14357 [D loss: 0.098239, acc.: 98.44%] [G loss: 3.262456]\n",
      "epoch:18 step:14358 [D loss: 0.141507, acc.: 96.88%] [G loss: 4.259107]\n",
      "epoch:18 step:14359 [D loss: 0.048323, acc.: 100.00%] [G loss: 3.742955]\n",
      "epoch:18 step:14360 [D loss: 0.284659, acc.: 89.06%] [G loss: 4.347469]\n",
      "epoch:18 step:14361 [D loss: 0.104186, acc.: 98.44%] [G loss: 4.265462]\n",
      "epoch:18 step:14362 [D loss: 0.024637, acc.: 100.00%] [G loss: 4.792707]\n",
      "epoch:18 step:14363 [D loss: 0.030029, acc.: 100.00%] [G loss: 3.566972]\n",
      "epoch:18 step:14364 [D loss: 0.018003, acc.: 100.00%] [G loss: 2.756095]\n",
      "epoch:18 step:14365 [D loss: 0.138350, acc.: 93.75%] [G loss: 4.287172]\n",
      "epoch:18 step:14366 [D loss: 0.075801, acc.: 96.88%] [G loss: 4.848592]\n",
      "epoch:18 step:14367 [D loss: 0.498846, acc.: 73.44%] [G loss: 6.027710]\n",
      "epoch:18 step:14368 [D loss: 0.326231, acc.: 83.59%] [G loss: 2.957389]\n",
      "epoch:18 step:14369 [D loss: 0.016774, acc.: 100.00%] [G loss: 5.079334]\n",
      "epoch:18 step:14370 [D loss: 0.062765, acc.: 98.44%] [G loss: 2.006608]\n",
      "epoch:18 step:14371 [D loss: 0.037566, acc.: 100.00%] [G loss: 3.292541]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:18 step:14372 [D loss: 0.042010, acc.: 97.66%] [G loss: 4.213704]\n",
      "epoch:18 step:14373 [D loss: 0.121636, acc.: 96.09%] [G loss: 4.014896]\n",
      "epoch:18 step:14374 [D loss: 0.019639, acc.: 100.00%] [G loss: 4.032348]\n",
      "epoch:18 step:14375 [D loss: 0.034602, acc.: 100.00%] [G loss: 3.552884]\n",
      "epoch:18 step:14376 [D loss: 0.014412, acc.: 100.00%] [G loss: 2.908679]\n",
      "epoch:18 step:14377 [D loss: 0.064656, acc.: 99.22%] [G loss: 3.528523]\n",
      "epoch:18 step:14378 [D loss: 2.344636, acc.: 22.66%] [G loss: 6.284960]\n",
      "epoch:18 step:14379 [D loss: 1.609354, acc.: 50.78%] [G loss: 3.891246]\n",
      "epoch:18 step:14380 [D loss: 1.188664, acc.: 45.31%] [G loss: 1.973347]\n",
      "epoch:18 step:14381 [D loss: 0.408046, acc.: 82.81%] [G loss: 3.012416]\n",
      "epoch:18 step:14382 [D loss: 0.176478, acc.: 96.88%] [G loss: 2.759792]\n",
      "epoch:18 step:14383 [D loss: 0.126460, acc.: 96.09%] [G loss: 3.143755]\n",
      "epoch:18 step:14384 [D loss: 0.260555, acc.: 89.06%] [G loss: 2.925078]\n",
      "epoch:18 step:14385 [D loss: 0.111621, acc.: 98.44%] [G loss: 3.070149]\n",
      "epoch:18 step:14386 [D loss: 0.322107, acc.: 85.94%] [G loss: 3.499514]\n",
      "epoch:18 step:14387 [D loss: 0.187167, acc.: 95.31%] [G loss: 3.068363]\n",
      "epoch:18 step:14388 [D loss: 0.122323, acc.: 98.44%] [G loss: 2.553239]\n",
      "epoch:18 step:14389 [D loss: 0.418652, acc.: 82.81%] [G loss: 3.907991]\n",
      "epoch:18 step:14390 [D loss: 0.569435, acc.: 68.75%] [G loss: 2.766542]\n",
      "epoch:18 step:14391 [D loss: 0.384634, acc.: 82.03%] [G loss: 3.809405]\n",
      "epoch:18 step:14392 [D loss: 0.452474, acc.: 79.69%] [G loss: 3.288702]\n",
      "epoch:18 step:14393 [D loss: 0.262165, acc.: 88.28%] [G loss: 2.895400]\n",
      "epoch:18 step:14394 [D loss: 0.136853, acc.: 96.88%] [G loss: 2.751932]\n",
      "epoch:18 step:14395 [D loss: 0.348079, acc.: 85.16%] [G loss: 3.085292]\n",
      "epoch:18 step:14396 [D loss: 0.134969, acc.: 97.66%] [G loss: 3.295105]\n",
      "epoch:18 step:14397 [D loss: 0.328786, acc.: 85.16%] [G loss: 3.248816]\n",
      "epoch:18 step:14398 [D loss: 0.275322, acc.: 90.62%] [G loss: 3.759531]\n",
      "epoch:18 step:14399 [D loss: 0.119634, acc.: 98.44%] [G loss: 3.806712]\n",
      "epoch:18 step:14400 [D loss: 0.291962, acc.: 89.06%] [G loss: 2.157933]\n",
      "##############\n",
      "[0.90734908 0.89107651 0.89801887 0.96859051 2.11524955 0.98921084\n",
      " 1.0114795  1.11289307 1.10466393 0.91277901]\n",
      "##########\n",
      "epoch:18 step:14401 [D loss: 0.148719, acc.: 96.09%] [G loss: 2.749121]\n",
      "epoch:18 step:14402 [D loss: 0.127228, acc.: 100.00%] [G loss: 2.063443]\n",
      "epoch:18 step:14403 [D loss: 0.120766, acc.: 97.66%] [G loss: 3.146592]\n",
      "epoch:18 step:14404 [D loss: 0.108695, acc.: 96.88%] [G loss: 2.405802]\n",
      "epoch:18 step:14405 [D loss: 0.096911, acc.: 98.44%] [G loss: 2.714134]\n",
      "epoch:18 step:14406 [D loss: 0.091896, acc.: 98.44%] [G loss: 2.336657]\n",
      "epoch:18 step:14407 [D loss: 0.408284, acc.: 78.91%] [G loss: 4.052120]\n",
      "epoch:18 step:14408 [D loss: 0.537442, acc.: 74.22%] [G loss: 1.653041]\n",
      "epoch:18 step:14409 [D loss: 0.159747, acc.: 92.97%] [G loss: 2.396569]\n",
      "epoch:18 step:14410 [D loss: 0.294551, acc.: 89.84%] [G loss: 3.809778]\n",
      "epoch:18 step:14411 [D loss: 0.416314, acc.: 78.12%] [G loss: 3.091434]\n",
      "epoch:18 step:14412 [D loss: 0.342755, acc.: 84.38%] [G loss: 4.086883]\n",
      "epoch:18 step:14413 [D loss: 0.213977, acc.: 90.62%] [G loss: 3.640230]\n",
      "epoch:18 step:14414 [D loss: 0.067863, acc.: 99.22%] [G loss: 2.975613]\n",
      "epoch:18 step:14415 [D loss: 0.088553, acc.: 100.00%] [G loss: 3.826205]\n",
      "epoch:18 step:14416 [D loss: 0.059555, acc.: 98.44%] [G loss: 3.413231]\n",
      "epoch:18 step:14417 [D loss: 0.184644, acc.: 92.19%] [G loss: 3.386107]\n",
      "epoch:18 step:14418 [D loss: 0.223715, acc.: 93.75%] [G loss: 4.240697]\n",
      "epoch:18 step:14419 [D loss: 0.254711, acc.: 91.41%] [G loss: 2.450320]\n",
      "epoch:18 step:14420 [D loss: 0.223997, acc.: 91.41%] [G loss: 3.832356]\n",
      "epoch:18 step:14421 [D loss: 0.299378, acc.: 90.62%] [G loss: 3.217883]\n",
      "epoch:18 step:14422 [D loss: 0.301284, acc.: 88.28%] [G loss: 3.081749]\n",
      "epoch:18 step:14423 [D loss: 0.262700, acc.: 89.06%] [G loss: 4.224389]\n",
      "epoch:18 step:14424 [D loss: 0.072200, acc.: 100.00%] [G loss: 4.130589]\n",
      "epoch:18 step:14425 [D loss: 0.195442, acc.: 94.53%] [G loss: 2.963733]\n",
      "epoch:18 step:14426 [D loss: 0.142976, acc.: 96.09%] [G loss: 3.831510]\n",
      "epoch:18 step:14427 [D loss: 0.374635, acc.: 84.38%] [G loss: 2.206728]\n",
      "epoch:18 step:14428 [D loss: 0.232285, acc.: 88.28%] [G loss: 4.715485]\n",
      "epoch:18 step:14429 [D loss: 0.270123, acc.: 86.72%] [G loss: 3.663421]\n",
      "epoch:18 step:14430 [D loss: 0.109255, acc.: 96.88%] [G loss: 4.240759]\n",
      "epoch:18 step:14431 [D loss: 0.371072, acc.: 85.94%] [G loss: 2.792764]\n",
      "epoch:18 step:14432 [D loss: 0.331500, acc.: 85.16%] [G loss: 5.214589]\n",
      "epoch:18 step:14433 [D loss: 0.102413, acc.: 96.88%] [G loss: 6.118098]\n",
      "epoch:18 step:14434 [D loss: 0.582230, acc.: 72.66%] [G loss: 2.354753]\n",
      "epoch:18 step:14435 [D loss: 0.203860, acc.: 89.84%] [G loss: 3.127194]\n",
      "epoch:18 step:14436 [D loss: 0.051309, acc.: 99.22%] [G loss: 3.656560]\n",
      "epoch:18 step:14437 [D loss: 0.080321, acc.: 96.88%] [G loss: 2.421164]\n",
      "epoch:18 step:14438 [D loss: 0.034542, acc.: 100.00%] [G loss: 2.549624]\n",
      "epoch:18 step:14439 [D loss: 0.043626, acc.: 100.00%] [G loss: 2.547963]\n",
      "epoch:18 step:14440 [D loss: 0.063857, acc.: 99.22%] [G loss: 2.741643]\n",
      "epoch:18 step:14441 [D loss: 0.095942, acc.: 98.44%] [G loss: 1.035476]\n",
      "epoch:18 step:14442 [D loss: 0.055133, acc.: 99.22%] [G loss: 0.457809]\n",
      "epoch:18 step:14443 [D loss: 0.342269, acc.: 85.94%] [G loss: 1.266798]\n",
      "epoch:18 step:14444 [D loss: 0.073801, acc.: 98.44%] [G loss: 3.171430]\n",
      "epoch:18 step:14445 [D loss: 0.126176, acc.: 97.66%] [G loss: 1.709714]\n",
      "epoch:18 step:14446 [D loss: 0.163782, acc.: 94.53%] [G loss: 2.482878]\n",
      "epoch:18 step:14447 [D loss: 0.322360, acc.: 86.72%] [G loss: 2.615715]\n",
      "epoch:18 step:14448 [D loss: 0.235775, acc.: 91.41%] [G loss: 3.120668]\n",
      "epoch:18 step:14449 [D loss: 0.338991, acc.: 85.94%] [G loss: 2.602959]\n",
      "epoch:18 step:14450 [D loss: 0.195278, acc.: 90.62%] [G loss: 3.612496]\n",
      "epoch:18 step:14451 [D loss: 0.433169, acc.: 82.03%] [G loss: 3.611174]\n",
      "epoch:18 step:14452 [D loss: 0.051399, acc.: 99.22%] [G loss: 5.074384]\n",
      "epoch:18 step:14453 [D loss: 0.113804, acc.: 96.88%] [G loss: 4.137560]\n",
      "epoch:18 step:14454 [D loss: 0.159442, acc.: 94.53%] [G loss: 3.598619]\n",
      "epoch:18 step:14455 [D loss: 0.066687, acc.: 99.22%] [G loss: 4.306159]\n",
      "epoch:18 step:14456 [D loss: 0.134129, acc.: 96.88%] [G loss: 3.791575]\n",
      "epoch:18 step:14457 [D loss: 0.071422, acc.: 98.44%] [G loss: 3.819051]\n",
      "epoch:18 step:14458 [D loss: 0.079398, acc.: 98.44%] [G loss: 3.431917]\n",
      "epoch:18 step:14459 [D loss: 0.019508, acc.: 100.00%] [G loss: 3.247079]\n",
      "epoch:18 step:14460 [D loss: 0.071228, acc.: 98.44%] [G loss: 2.861804]\n",
      "epoch:18 step:14461 [D loss: 0.132363, acc.: 96.09%] [G loss: 3.753990]\n",
      "epoch:18 step:14462 [D loss: 0.088867, acc.: 96.88%] [G loss: 3.073165]\n",
      "epoch:18 step:14463 [D loss: 0.133980, acc.: 97.66%] [G loss: 4.006618]\n",
      "epoch:18 step:14464 [D loss: 0.045155, acc.: 100.00%] [G loss: 3.862913]\n",
      "epoch:18 step:14465 [D loss: 0.029664, acc.: 100.00%] [G loss: 3.608784]\n",
      "epoch:18 step:14466 [D loss: 0.070417, acc.: 99.22%] [G loss: 2.099341]\n",
      "epoch:18 step:14467 [D loss: 0.446995, acc.: 78.91%] [G loss: 6.189823]\n",
      "epoch:18 step:14468 [D loss: 0.275793, acc.: 88.28%] [G loss: 4.491486]\n",
      "epoch:18 step:14469 [D loss: 0.088338, acc.: 96.09%] [G loss: 1.073595]\n",
      "epoch:18 step:14470 [D loss: 0.187924, acc.: 92.19%] [G loss: 4.015347]\n",
      "epoch:18 step:14471 [D loss: 0.019137, acc.: 100.00%] [G loss: 4.917009]\n",
      "epoch:18 step:14472 [D loss: 0.062520, acc.: 98.44%] [G loss: 2.562434]\n",
      "epoch:18 step:14473 [D loss: 0.034881, acc.: 100.00%] [G loss: 2.766219]\n",
      "epoch:18 step:14474 [D loss: 0.063751, acc.: 99.22%] [G loss: 3.184974]\n",
      "epoch:18 step:14475 [D loss: 0.087486, acc.: 98.44%] [G loss: 3.738198]\n",
      "epoch:18 step:14476 [D loss: 0.096981, acc.: 97.66%] [G loss: 1.782958]\n",
      "epoch:18 step:14477 [D loss: 0.111140, acc.: 97.66%] [G loss: 1.873515]\n",
      "epoch:18 step:14478 [D loss: 0.185568, acc.: 92.97%] [G loss: 1.490524]\n",
      "epoch:18 step:14479 [D loss: 0.213806, acc.: 92.97%] [G loss: 4.067039]\n",
      "epoch:18 step:14480 [D loss: 0.090001, acc.: 97.66%] [G loss: 2.640385]\n",
      "epoch:18 step:14481 [D loss: 0.029136, acc.: 100.00%] [G loss: 2.982389]\n",
      "epoch:18 step:14482 [D loss: 0.410989, acc.: 80.47%] [G loss: 5.722174]\n",
      "epoch:18 step:14483 [D loss: 0.360781, acc.: 82.03%] [G loss: 1.054319]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:18 step:14484 [D loss: 0.317020, acc.: 83.59%] [G loss: 6.587222]\n",
      "epoch:18 step:14485 [D loss: 0.009349, acc.: 100.00%] [G loss: 7.345661]\n",
      "epoch:18 step:14486 [D loss: 0.556491, acc.: 75.00%] [G loss: 1.578849]\n",
      "epoch:18 step:14487 [D loss: 0.514952, acc.: 79.69%] [G loss: 8.034073]\n",
      "epoch:18 step:14488 [D loss: 0.238225, acc.: 89.84%] [G loss: 6.707692]\n",
      "epoch:18 step:14489 [D loss: 0.776421, acc.: 68.75%] [G loss: 2.300074]\n",
      "epoch:18 step:14490 [D loss: 0.072878, acc.: 100.00%] [G loss: 3.244569]\n",
      "epoch:18 step:14491 [D loss: 0.004725, acc.: 100.00%] [G loss: 4.356124]\n",
      "epoch:18 step:14492 [D loss: 0.284076, acc.: 88.28%] [G loss: 5.983847]\n",
      "epoch:18 step:14493 [D loss: 0.197434, acc.: 91.41%] [G loss: 2.439248]\n",
      "epoch:18 step:14494 [D loss: 0.093356, acc.: 93.75%] [G loss: 3.748417]\n",
      "epoch:18 step:14495 [D loss: 0.075666, acc.: 99.22%] [G loss: 3.745370]\n",
      "epoch:18 step:14496 [D loss: 0.030478, acc.: 100.00%] [G loss: 3.017707]\n",
      "epoch:18 step:14497 [D loss: 0.057848, acc.: 100.00%] [G loss: 2.606014]\n",
      "epoch:18 step:14498 [D loss: 0.144944, acc.: 97.66%] [G loss: 5.586272]\n",
      "epoch:18 step:14499 [D loss: 0.081161, acc.: 97.66%] [G loss: 5.164610]\n",
      "epoch:18 step:14500 [D loss: 0.020551, acc.: 100.00%] [G loss: 3.806124]\n",
      "epoch:18 step:14501 [D loss: 0.090118, acc.: 97.66%] [G loss: 2.874392]\n",
      "epoch:18 step:14502 [D loss: 0.070671, acc.: 98.44%] [G loss: 4.528255]\n",
      "epoch:18 step:14503 [D loss: 0.019851, acc.: 100.00%] [G loss: 3.846219]\n",
      "epoch:18 step:14504 [D loss: 0.016184, acc.: 100.00%] [G loss: 3.568379]\n",
      "epoch:18 step:14505 [D loss: 0.036803, acc.: 100.00%] [G loss: 2.542664]\n",
      "epoch:18 step:14506 [D loss: 0.289527, acc.: 88.28%] [G loss: 4.953230]\n",
      "epoch:18 step:14507 [D loss: 0.118884, acc.: 92.97%] [G loss: 4.604865]\n",
      "epoch:18 step:14508 [D loss: 0.011375, acc.: 100.00%] [G loss: 4.316214]\n",
      "epoch:18 step:14509 [D loss: 0.045035, acc.: 99.22%] [G loss: 2.349933]\n",
      "epoch:18 step:14510 [D loss: 0.325675, acc.: 82.81%] [G loss: 6.749819]\n",
      "epoch:18 step:14511 [D loss: 0.633920, acc.: 71.88%] [G loss: 3.789304]\n",
      "epoch:18 step:14512 [D loss: 0.073029, acc.: 96.88%] [G loss: 2.361546]\n",
      "epoch:18 step:14513 [D loss: 0.004927, acc.: 100.00%] [G loss: 3.555974]\n",
      "epoch:18 step:14514 [D loss: 0.015782, acc.: 99.22%] [G loss: 1.714194]\n",
      "epoch:18 step:14515 [D loss: 0.022537, acc.: 100.00%] [G loss: 2.256496]\n",
      "epoch:18 step:14516 [D loss: 0.022688, acc.: 100.00%] [G loss: 1.886614]\n",
      "epoch:18 step:14517 [D loss: 0.086550, acc.: 97.66%] [G loss: 3.671410]\n",
      "epoch:18 step:14518 [D loss: 0.074490, acc.: 97.66%] [G loss: 0.859332]\n",
      "epoch:18 step:14519 [D loss: 0.435028, acc.: 81.25%] [G loss: 6.147901]\n",
      "epoch:18 step:14520 [D loss: 0.359004, acc.: 78.91%] [G loss: 5.731966]\n",
      "epoch:18 step:14521 [D loss: 0.113127, acc.: 96.09%] [G loss: 3.515292]\n",
      "epoch:18 step:14522 [D loss: 0.048548, acc.: 99.22%] [G loss: 3.224424]\n",
      "epoch:18 step:14523 [D loss: 0.021739, acc.: 100.00%] [G loss: 1.682724]\n",
      "epoch:18 step:14524 [D loss: 0.017942, acc.: 100.00%] [G loss: 2.555960]\n",
      "epoch:18 step:14525 [D loss: 0.100282, acc.: 97.66%] [G loss: 2.607950]\n",
      "epoch:18 step:14526 [D loss: 0.009546, acc.: 100.00%] [G loss: 4.474293]\n",
      "epoch:18 step:14527 [D loss: 0.010945, acc.: 99.22%] [G loss: 4.351832]\n",
      "epoch:18 step:14528 [D loss: 0.045981, acc.: 100.00%] [G loss: 4.296114]\n",
      "epoch:18 step:14529 [D loss: 0.025592, acc.: 100.00%] [G loss: 3.509104]\n",
      "epoch:18 step:14530 [D loss: 0.036436, acc.: 100.00%] [G loss: 2.595667]\n",
      "epoch:18 step:14531 [D loss: 0.018092, acc.: 100.00%] [G loss: 0.857810]\n",
      "epoch:18 step:14532 [D loss: 0.301294, acc.: 88.28%] [G loss: 5.721583]\n",
      "epoch:18 step:14533 [D loss: 1.248855, acc.: 46.88%] [G loss: 4.675862]\n",
      "epoch:18 step:14534 [D loss: 0.007298, acc.: 100.00%] [G loss: 5.406970]\n",
      "epoch:18 step:14535 [D loss: 0.118594, acc.: 96.88%] [G loss: 3.534385]\n",
      "epoch:18 step:14536 [D loss: 0.306897, acc.: 86.72%] [G loss: 6.349903]\n",
      "epoch:18 step:14537 [D loss: 0.831629, acc.: 61.72%] [G loss: 1.371527]\n",
      "epoch:18 step:14538 [D loss: 0.710172, acc.: 66.41%] [G loss: 6.947154]\n",
      "epoch:18 step:14539 [D loss: 1.714825, acc.: 50.78%] [G loss: 4.372978]\n",
      "epoch:18 step:14540 [D loss: 0.347508, acc.: 85.16%] [G loss: 2.610506]\n",
      "epoch:18 step:14541 [D loss: 0.078358, acc.: 97.66%] [G loss: 2.597247]\n",
      "epoch:18 step:14542 [D loss: 0.167130, acc.: 92.97%] [G loss: 4.427634]\n",
      "epoch:18 step:14543 [D loss: 0.053403, acc.: 98.44%] [G loss: 4.916695]\n",
      "epoch:18 step:14544 [D loss: 0.084415, acc.: 96.88%] [G loss: 3.327801]\n",
      "epoch:18 step:14545 [D loss: 0.033200, acc.: 100.00%] [G loss: 3.567017]\n",
      "epoch:18 step:14546 [D loss: 0.028695, acc.: 100.00%] [G loss: 4.122673]\n",
      "epoch:18 step:14547 [D loss: 0.108337, acc.: 99.22%] [G loss: 2.807052]\n",
      "epoch:18 step:14548 [D loss: 0.040160, acc.: 99.22%] [G loss: 3.878304]\n",
      "epoch:18 step:14549 [D loss: 0.201509, acc.: 92.19%] [G loss: 3.794795]\n",
      "epoch:18 step:14550 [D loss: 0.150035, acc.: 96.88%] [G loss: 1.106255]\n",
      "epoch:18 step:14551 [D loss: 0.092059, acc.: 100.00%] [G loss: 4.090441]\n",
      "epoch:18 step:14552 [D loss: 0.030646, acc.: 100.00%] [G loss: 3.275592]\n",
      "epoch:18 step:14553 [D loss: 0.031055, acc.: 99.22%] [G loss: 4.297833]\n",
      "epoch:18 step:14554 [D loss: 0.029431, acc.: 100.00%] [G loss: 2.511166]\n",
      "epoch:18 step:14555 [D loss: 0.063145, acc.: 99.22%] [G loss: 3.470264]\n",
      "epoch:18 step:14556 [D loss: 0.119212, acc.: 96.88%] [G loss: 2.629964]\n",
      "epoch:18 step:14557 [D loss: 0.103632, acc.: 98.44%] [G loss: 3.398981]\n",
      "epoch:18 step:14558 [D loss: 0.021436, acc.: 100.00%] [G loss: 3.916471]\n",
      "epoch:18 step:14559 [D loss: 0.018683, acc.: 100.00%] [G loss: 4.025854]\n",
      "epoch:18 step:14560 [D loss: 0.059133, acc.: 98.44%] [G loss: 3.546962]\n",
      "epoch:18 step:14561 [D loss: 0.051312, acc.: 99.22%] [G loss: 3.795687]\n",
      "epoch:18 step:14562 [D loss: 0.057810, acc.: 98.44%] [G loss: 4.731660]\n",
      "epoch:18 step:14563 [D loss: 0.115043, acc.: 96.09%] [G loss: 2.745157]\n",
      "epoch:18 step:14564 [D loss: 0.033349, acc.: 100.00%] [G loss: 4.430655]\n",
      "epoch:18 step:14565 [D loss: 0.019759, acc.: 100.00%] [G loss: 3.664222]\n",
      "epoch:18 step:14566 [D loss: 0.105954, acc.: 96.88%] [G loss: 2.868368]\n",
      "epoch:18 step:14567 [D loss: 0.081850, acc.: 99.22%] [G loss: 4.456046]\n",
      "epoch:18 step:14568 [D loss: 0.031220, acc.: 100.00%] [G loss: 3.684194]\n",
      "epoch:18 step:14569 [D loss: 0.075461, acc.: 99.22%] [G loss: 3.601344]\n",
      "epoch:18 step:14570 [D loss: 0.077296, acc.: 98.44%] [G loss: 4.639023]\n",
      "epoch:18 step:14571 [D loss: 0.036203, acc.: 100.00%] [G loss: 4.023935]\n",
      "epoch:18 step:14572 [D loss: 0.011234, acc.: 100.00%] [G loss: 4.546846]\n",
      "epoch:18 step:14573 [D loss: 0.016791, acc.: 100.00%] [G loss: 3.256873]\n",
      "epoch:18 step:14574 [D loss: 0.036174, acc.: 99.22%] [G loss: 3.799159]\n",
      "epoch:18 step:14575 [D loss: 0.037107, acc.: 99.22%] [G loss: 4.323939]\n",
      "epoch:18 step:14576 [D loss: 0.097533, acc.: 99.22%] [G loss: 4.536150]\n",
      "epoch:18 step:14577 [D loss: 0.050477, acc.: 97.66%] [G loss: 3.032187]\n",
      "epoch:18 step:14578 [D loss: 0.041717, acc.: 100.00%] [G loss: 3.326170]\n",
      "epoch:18 step:14579 [D loss: 0.024650, acc.: 100.00%] [G loss: 3.150377]\n",
      "epoch:18 step:14580 [D loss: 0.023345, acc.: 100.00%] [G loss: 3.029946]\n",
      "epoch:18 step:14581 [D loss: 0.029287, acc.: 99.22%] [G loss: 0.723542]\n",
      "epoch:18 step:14582 [D loss: 0.171602, acc.: 97.66%] [G loss: 6.787163]\n",
      "epoch:18 step:14583 [D loss: 0.408020, acc.: 79.69%] [G loss: 0.061938]\n",
      "epoch:18 step:14584 [D loss: 0.680229, acc.: 64.84%] [G loss: 7.512092]\n",
      "epoch:18 step:14585 [D loss: 2.110676, acc.: 50.00%] [G loss: 4.956685]\n",
      "epoch:18 step:14586 [D loss: 1.062619, acc.: 52.34%] [G loss: 3.620238]\n",
      "epoch:18 step:14587 [D loss: 0.015462, acc.: 100.00%] [G loss: 4.657332]\n",
      "epoch:18 step:14588 [D loss: 0.138709, acc.: 93.75%] [G loss: 2.478471]\n",
      "epoch:18 step:14589 [D loss: 0.103936, acc.: 93.75%] [G loss: 2.703948]\n",
      "epoch:18 step:14590 [D loss: 0.047634, acc.: 97.66%] [G loss: 3.514746]\n",
      "epoch:18 step:14591 [D loss: 0.096917, acc.: 97.66%] [G loss: 2.856074]\n",
      "epoch:18 step:14592 [D loss: 0.046809, acc.: 99.22%] [G loss: 3.967687]\n",
      "epoch:18 step:14593 [D loss: 0.064105, acc.: 100.00%] [G loss: 3.493070]\n",
      "epoch:18 step:14594 [D loss: 0.576135, acc.: 71.88%] [G loss: 4.438231]\n",
      "epoch:18 step:14595 [D loss: 0.136301, acc.: 94.53%] [G loss: 4.258471]\n",
      "epoch:18 step:14596 [D loss: 0.022142, acc.: 100.00%] [G loss: 3.714576]\n",
      "epoch:18 step:14597 [D loss: 0.053089, acc.: 100.00%] [G loss: 3.508492]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:18 step:14598 [D loss: 0.010588, acc.: 100.00%] [G loss: 2.633991]\n",
      "epoch:18 step:14599 [D loss: 0.037051, acc.: 100.00%] [G loss: 3.067490]\n",
      "epoch:18 step:14600 [D loss: 0.112635, acc.: 98.44%] [G loss: 2.045974]\n",
      "##############\n",
      "[1.04765759 0.95951559 1.0442178  0.97856167 1.10552572 0.99136885\n",
      " 2.10414381 1.00885919 1.12199514 2.11032704]\n",
      "##########\n",
      "epoch:18 step:14601 [D loss: 0.078074, acc.: 97.66%] [G loss: 0.559284]\n",
      "epoch:18 step:14602 [D loss: 0.041188, acc.: 100.00%] [G loss: 2.218598]\n",
      "epoch:18 step:14603 [D loss: 0.038908, acc.: 100.00%] [G loss: 1.434307]\n",
      "epoch:18 step:14604 [D loss: 0.016591, acc.: 100.00%] [G loss: 1.353866]\n",
      "epoch:18 step:14605 [D loss: 0.224954, acc.: 94.53%] [G loss: 3.373949]\n",
      "epoch:18 step:14606 [D loss: 0.088405, acc.: 97.66%] [G loss: 2.697220]\n",
      "epoch:18 step:14607 [D loss: 0.018566, acc.: 100.00%] [G loss: 2.566977]\n",
      "epoch:18 step:14608 [D loss: 0.022779, acc.: 100.00%] [G loss: 0.814499]\n",
      "epoch:18 step:14609 [D loss: 0.145462, acc.: 95.31%] [G loss: 2.987840]\n",
      "epoch:18 step:14610 [D loss: 0.223569, acc.: 91.41%] [G loss: 4.792994]\n",
      "epoch:18 step:14611 [D loss: 0.011695, acc.: 100.00%] [G loss: 4.839760]\n",
      "epoch:18 step:14612 [D loss: 0.045960, acc.: 99.22%] [G loss: 3.974504]\n",
      "epoch:18 step:14613 [D loss: 0.034222, acc.: 100.00%] [G loss: 2.643837]\n",
      "epoch:18 step:14614 [D loss: 0.017158, acc.: 100.00%] [G loss: 2.584177]\n",
      "epoch:18 step:14615 [D loss: 0.045186, acc.: 99.22%] [G loss: 1.305062]\n",
      "epoch:18 step:14616 [D loss: 0.527457, acc.: 71.88%] [G loss: 5.421632]\n",
      "epoch:18 step:14617 [D loss: 0.432289, acc.: 79.69%] [G loss: 5.076987]\n",
      "epoch:18 step:14618 [D loss: 0.037965, acc.: 99.22%] [G loss: 4.359099]\n",
      "epoch:18 step:14619 [D loss: 0.101463, acc.: 97.66%] [G loss: 3.501348]\n",
      "epoch:18 step:14620 [D loss: 0.066297, acc.: 99.22%] [G loss: 5.096371]\n",
      "epoch:18 step:14621 [D loss: 0.020934, acc.: 100.00%] [G loss: 4.886701]\n",
      "epoch:18 step:14622 [D loss: 0.034107, acc.: 100.00%] [G loss: 4.372107]\n",
      "epoch:18 step:14623 [D loss: 0.034418, acc.: 100.00%] [G loss: 4.066249]\n",
      "epoch:18 step:14624 [D loss: 0.054599, acc.: 99.22%] [G loss: 2.444586]\n",
      "epoch:18 step:14625 [D loss: 0.076226, acc.: 99.22%] [G loss: 4.110036]\n",
      "epoch:18 step:14626 [D loss: 0.020627, acc.: 100.00%] [G loss: 3.945726]\n",
      "epoch:18 step:14627 [D loss: 0.060356, acc.: 98.44%] [G loss: 4.351540]\n",
      "epoch:18 step:14628 [D loss: 0.013551, acc.: 100.00%] [G loss: 3.954184]\n",
      "epoch:18 step:14629 [D loss: 0.091699, acc.: 97.66%] [G loss: 4.068167]\n",
      "epoch:18 step:14630 [D loss: 0.006273, acc.: 100.00%] [G loss: 4.943969]\n",
      "epoch:18 step:14631 [D loss: 0.028072, acc.: 100.00%] [G loss: 4.785059]\n",
      "epoch:18 step:14632 [D loss: 0.021023, acc.: 100.00%] [G loss: 2.443667]\n",
      "epoch:18 step:14633 [D loss: 0.294588, acc.: 85.94%] [G loss: 3.715574]\n",
      "epoch:18 step:14634 [D loss: 0.018643, acc.: 100.00%] [G loss: 3.824093]\n",
      "epoch:18 step:14635 [D loss: 0.151293, acc.: 94.53%] [G loss: 1.579793]\n",
      "epoch:18 step:14636 [D loss: 0.138569, acc.: 95.31%] [G loss: 3.945721]\n",
      "epoch:18 step:14637 [D loss: 0.020254, acc.: 100.00%] [G loss: 4.274019]\n",
      "epoch:18 step:14638 [D loss: 0.186301, acc.: 94.53%] [G loss: 0.949795]\n",
      "epoch:18 step:14639 [D loss: 0.071243, acc.: 98.44%] [G loss: 2.317072]\n",
      "epoch:18 step:14640 [D loss: 0.010274, acc.: 100.00%] [G loss: 3.815520]\n",
      "epoch:18 step:14641 [D loss: 0.022395, acc.: 100.00%] [G loss: 2.465796]\n",
      "epoch:18 step:14642 [D loss: 0.068387, acc.: 99.22%] [G loss: 3.860047]\n",
      "epoch:18 step:14643 [D loss: 0.036505, acc.: 100.00%] [G loss: 2.092718]\n",
      "epoch:18 step:14644 [D loss: 0.128060, acc.: 96.09%] [G loss: 4.903338]\n",
      "epoch:18 step:14645 [D loss: 0.185528, acc.: 92.97%] [G loss: 1.934930]\n",
      "epoch:18 step:14646 [D loss: 0.368223, acc.: 80.47%] [G loss: 8.303175]\n",
      "epoch:18 step:14647 [D loss: 0.598490, acc.: 71.09%] [G loss: 4.600921]\n",
      "epoch:18 step:14648 [D loss: 0.076673, acc.: 98.44%] [G loss: 3.754146]\n",
      "epoch:18 step:14649 [D loss: 0.142358, acc.: 92.97%] [G loss: 5.684305]\n",
      "epoch:18 step:14650 [D loss: 0.039473, acc.: 98.44%] [G loss: 5.230907]\n",
      "epoch:18 step:14651 [D loss: 0.350092, acc.: 83.59%] [G loss: 1.030224]\n",
      "epoch:18 step:14652 [D loss: 0.050940, acc.: 99.22%] [G loss: 1.590714]\n",
      "epoch:18 step:14653 [D loss: 0.050050, acc.: 98.44%] [G loss: 5.262704]\n",
      "epoch:18 step:14654 [D loss: 0.003058, acc.: 100.00%] [G loss: 5.142177]\n",
      "epoch:18 step:14655 [D loss: 0.005235, acc.: 100.00%] [G loss: 3.515455]\n",
      "epoch:18 step:14656 [D loss: 0.017120, acc.: 100.00%] [G loss: 3.877502]\n",
      "epoch:18 step:14657 [D loss: 0.041970, acc.: 99.22%] [G loss: 1.862245]\n",
      "epoch:18 step:14658 [D loss: 0.073433, acc.: 97.66%] [G loss: 3.734651]\n",
      "epoch:18 step:14659 [D loss: 0.074795, acc.: 96.88%] [G loss: 3.349594]\n",
      "epoch:18 step:14660 [D loss: 0.407224, acc.: 80.47%] [G loss: 8.323336]\n",
      "epoch:18 step:14661 [D loss: 0.914989, acc.: 63.28%] [G loss: 3.744154]\n",
      "epoch:18 step:14662 [D loss: 0.133885, acc.: 95.31%] [G loss: 1.181635]\n",
      "epoch:18 step:14663 [D loss: 1.150576, acc.: 59.38%] [G loss: 7.979511]\n",
      "epoch:18 step:14664 [D loss: 2.597248, acc.: 50.78%] [G loss: 4.939541]\n",
      "epoch:18 step:14665 [D loss: 1.496164, acc.: 32.81%] [G loss: 3.187210]\n",
      "epoch:18 step:14666 [D loss: 0.081949, acc.: 98.44%] [G loss: 3.643587]\n",
      "epoch:18 step:14667 [D loss: 0.435453, acc.: 85.94%] [G loss: 2.313896]\n",
      "epoch:18 step:14668 [D loss: 0.310185, acc.: 88.28%] [G loss: 3.456902]\n",
      "epoch:18 step:14669 [D loss: 0.037405, acc.: 100.00%] [G loss: 3.556977]\n",
      "epoch:18 step:14670 [D loss: 0.267298, acc.: 89.84%] [G loss: 1.935144]\n",
      "epoch:18 step:14671 [D loss: 0.251070, acc.: 91.41%] [G loss: 2.032306]\n",
      "epoch:18 step:14672 [D loss: 0.080961, acc.: 99.22%] [G loss: 2.925301]\n",
      "epoch:18 step:14673 [D loss: 0.166245, acc.: 93.75%] [G loss: 1.978390]\n",
      "epoch:18 step:14674 [D loss: 0.236662, acc.: 90.62%] [G loss: 1.500185]\n",
      "epoch:18 step:14675 [D loss: 0.071045, acc.: 99.22%] [G loss: 1.936138]\n",
      "epoch:18 step:14676 [D loss: 0.131111, acc.: 95.31%] [G loss: 0.149492]\n",
      "epoch:18 step:14677 [D loss: 0.233034, acc.: 91.41%] [G loss: 0.862215]\n",
      "epoch:18 step:14678 [D loss: 0.053917, acc.: 99.22%] [G loss: 2.159801]\n",
      "epoch:18 step:14679 [D loss: 0.313107, acc.: 85.94%] [G loss: 1.501748]\n",
      "epoch:18 step:14680 [D loss: 0.104975, acc.: 96.88%] [G loss: 0.363099]\n",
      "epoch:18 step:14681 [D loss: 0.689563, acc.: 64.06%] [G loss: 2.019851]\n",
      "epoch:18 step:14682 [D loss: 0.225721, acc.: 90.62%] [G loss: 3.981492]\n",
      "epoch:18 step:14683 [D loss: 1.086146, acc.: 48.44%] [G loss: 4.012363]\n",
      "epoch:18 step:14684 [D loss: 0.451818, acc.: 76.56%] [G loss: 3.470420]\n",
      "epoch:18 step:14685 [D loss: 0.332474, acc.: 83.59%] [G loss: 2.602452]\n",
      "epoch:18 step:14686 [D loss: 0.240192, acc.: 89.06%] [G loss: 4.373049]\n",
      "epoch:18 step:14687 [D loss: 0.402825, acc.: 78.91%] [G loss: 3.079103]\n",
      "epoch:18 step:14688 [D loss: 0.153237, acc.: 96.88%] [G loss: 2.023449]\n",
      "epoch:18 step:14689 [D loss: 0.327647, acc.: 86.72%] [G loss: 3.081541]\n",
      "epoch:18 step:14690 [D loss: 0.106189, acc.: 99.22%] [G loss: 3.197346]\n",
      "epoch:18 step:14691 [D loss: 0.254775, acc.: 92.97%] [G loss: 3.236999]\n",
      "epoch:18 step:14692 [D loss: 0.087240, acc.: 99.22%] [G loss: 3.035431]\n",
      "epoch:18 step:14693 [D loss: 0.191961, acc.: 90.62%] [G loss: 2.401131]\n",
      "epoch:18 step:14694 [D loss: 0.120503, acc.: 96.88%] [G loss: 2.460723]\n",
      "epoch:18 step:14695 [D loss: 0.079994, acc.: 99.22%] [G loss: 2.668298]\n",
      "epoch:18 step:14696 [D loss: 0.112037, acc.: 97.66%] [G loss: 2.045500]\n",
      "epoch:18 step:14697 [D loss: 0.133949, acc.: 97.66%] [G loss: 2.551603]\n",
      "epoch:18 step:14698 [D loss: 0.034885, acc.: 100.00%] [G loss: 2.273154]\n",
      "epoch:18 step:14699 [D loss: 0.096902, acc.: 100.00%] [G loss: 2.443878]\n",
      "epoch:18 step:14700 [D loss: 0.108011, acc.: 97.66%] [G loss: 1.701501]\n",
      "epoch:18 step:14701 [D loss: 0.144256, acc.: 96.88%] [G loss: 2.611613]\n",
      "epoch:18 step:14702 [D loss: 0.081009, acc.: 99.22%] [G loss: 2.423377]\n",
      "epoch:18 step:14703 [D loss: 0.179041, acc.: 92.19%] [G loss: 1.340785]\n",
      "epoch:18 step:14704 [D loss: 0.449931, acc.: 75.00%] [G loss: 4.445682]\n",
      "epoch:18 step:14705 [D loss: 0.562769, acc.: 71.09%] [G loss: 3.475307]\n",
      "epoch:18 step:14706 [D loss: 0.093401, acc.: 97.66%] [G loss: 2.065117]\n",
      "epoch:18 step:14707 [D loss: 0.086426, acc.: 98.44%] [G loss: 1.634351]\n",
      "epoch:18 step:14708 [D loss: 0.036669, acc.: 100.00%] [G loss: 1.997254]\n",
      "epoch:18 step:14709 [D loss: 0.060972, acc.: 100.00%] [G loss: 2.262231]\n",
      "epoch:18 step:14710 [D loss: 0.091987, acc.: 98.44%] [G loss: 1.499281]\n",
      "epoch:18 step:14711 [D loss: 0.035895, acc.: 100.00%] [G loss: 2.458982]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:18 step:14712 [D loss: 0.127404, acc.: 96.88%] [G loss: 1.271460]\n",
      "epoch:18 step:14713 [D loss: 0.144318, acc.: 96.88%] [G loss: 0.851421]\n",
      "epoch:18 step:14714 [D loss: 0.350377, acc.: 87.50%] [G loss: 2.875258]\n",
      "epoch:18 step:14715 [D loss: 0.109046, acc.: 98.44%] [G loss: 3.087394]\n",
      "epoch:18 step:14716 [D loss: 0.214786, acc.: 92.97%] [G loss: 2.352076]\n",
      "epoch:18 step:14717 [D loss: 0.274382, acc.: 85.16%] [G loss: 3.557766]\n",
      "epoch:18 step:14718 [D loss: 0.130726, acc.: 93.75%] [G loss: 4.309040]\n",
      "epoch:18 step:14719 [D loss: 0.054663, acc.: 100.00%] [G loss: 3.720743]\n",
      "epoch:18 step:14720 [D loss: 0.208430, acc.: 93.75%] [G loss: 3.039962]\n",
      "epoch:18 step:14721 [D loss: 0.049641, acc.: 100.00%] [G loss: 3.635043]\n",
      "epoch:18 step:14722 [D loss: 0.027644, acc.: 100.00%] [G loss: 3.929911]\n",
      "epoch:18 step:14723 [D loss: 0.176034, acc.: 95.31%] [G loss: 3.526762]\n",
      "epoch:18 step:14724 [D loss: 0.061773, acc.: 99.22%] [G loss: 3.837363]\n",
      "epoch:18 step:14725 [D loss: 0.703775, acc.: 67.97%] [G loss: 3.405724]\n",
      "epoch:18 step:14726 [D loss: 0.051083, acc.: 98.44%] [G loss: 4.429407]\n",
      "epoch:18 step:14727 [D loss: 0.162248, acc.: 92.97%] [G loss: 2.834533]\n",
      "epoch:18 step:14728 [D loss: 0.045152, acc.: 99.22%] [G loss: 3.560488]\n",
      "epoch:18 step:14729 [D loss: 0.071127, acc.: 99.22%] [G loss: 2.692746]\n",
      "epoch:18 step:14730 [D loss: 0.053859, acc.: 100.00%] [G loss: 4.035660]\n",
      "epoch:18 step:14731 [D loss: 0.042054, acc.: 100.00%] [G loss: 3.803016]\n",
      "epoch:18 step:14732 [D loss: 0.134725, acc.: 96.09%] [G loss: 3.387926]\n",
      "epoch:18 step:14733 [D loss: 0.094493, acc.: 98.44%] [G loss: 3.580952]\n",
      "epoch:18 step:14734 [D loss: 0.145068, acc.: 99.22%] [G loss: 3.348080]\n",
      "epoch:18 step:14735 [D loss: 0.028097, acc.: 100.00%] [G loss: 3.450604]\n",
      "epoch:18 step:14736 [D loss: 0.097915, acc.: 97.66%] [G loss: 4.425323]\n",
      "epoch:18 step:14737 [D loss: 0.015592, acc.: 100.00%] [G loss: 4.746006]\n",
      "epoch:18 step:14738 [D loss: 0.056573, acc.: 99.22%] [G loss: 4.321667]\n",
      "epoch:18 step:14739 [D loss: 0.025811, acc.: 100.00%] [G loss: 3.739632]\n",
      "epoch:18 step:14740 [D loss: 0.062956, acc.: 98.44%] [G loss: 3.596895]\n",
      "epoch:18 step:14741 [D loss: 0.073274, acc.: 99.22%] [G loss: 4.500113]\n",
      "epoch:18 step:14742 [D loss: 0.008832, acc.: 100.00%] [G loss: 3.955489]\n",
      "epoch:18 step:14743 [D loss: 0.536535, acc.: 76.56%] [G loss: 3.981174]\n",
      "epoch:18 step:14744 [D loss: 0.031344, acc.: 99.22%] [G loss: 4.794447]\n",
      "epoch:18 step:14745 [D loss: 0.039281, acc.: 99.22%] [G loss: 3.104100]\n",
      "epoch:18 step:14746 [D loss: 0.120935, acc.: 98.44%] [G loss: 2.871530]\n",
      "epoch:18 step:14747 [D loss: 0.160371, acc.: 92.97%] [G loss: 6.340295]\n",
      "epoch:18 step:14748 [D loss: 0.147830, acc.: 92.97%] [G loss: 2.660653]\n",
      "epoch:18 step:14749 [D loss: 0.101350, acc.: 97.66%] [G loss: 4.327999]\n",
      "epoch:18 step:14750 [D loss: 0.058768, acc.: 97.66%] [G loss: 2.944943]\n",
      "epoch:18 step:14751 [D loss: 0.048207, acc.: 98.44%] [G loss: 4.555862]\n",
      "epoch:18 step:14752 [D loss: 0.067204, acc.: 98.44%] [G loss: 5.212038]\n",
      "epoch:18 step:14753 [D loss: 0.014727, acc.: 100.00%] [G loss: 5.556630]\n",
      "epoch:18 step:14754 [D loss: 0.220410, acc.: 92.19%] [G loss: 2.013359]\n",
      "epoch:18 step:14755 [D loss: 0.010476, acc.: 100.00%] [G loss: 2.856714]\n",
      "epoch:18 step:14756 [D loss: 0.137211, acc.: 94.53%] [G loss: 4.428830]\n",
      "epoch:18 step:14757 [D loss: 0.017311, acc.: 99.22%] [G loss: 6.633483]\n",
      "epoch:18 step:14758 [D loss: 0.242126, acc.: 91.41%] [G loss: 2.148102]\n",
      "epoch:18 step:14759 [D loss: 0.008913, acc.: 100.00%] [G loss: 3.321946]\n",
      "epoch:18 step:14760 [D loss: 0.105600, acc.: 97.66%] [G loss: 4.119301]\n",
      "epoch:18 step:14761 [D loss: 0.032324, acc.: 100.00%] [G loss: 4.812608]\n",
      "epoch:18 step:14762 [D loss: 0.337745, acc.: 87.50%] [G loss: 1.395502]\n",
      "epoch:18 step:14763 [D loss: 0.318101, acc.: 81.25%] [G loss: 6.184909]\n",
      "epoch:18 step:14764 [D loss: 0.101336, acc.: 96.88%] [G loss: 8.220042]\n",
      "epoch:18 step:14765 [D loss: 0.365455, acc.: 79.69%] [G loss: 2.572314]\n",
      "epoch:18 step:14766 [D loss: 0.768590, acc.: 64.84%] [G loss: 8.328313]\n",
      "epoch:18 step:14767 [D loss: 1.145139, acc.: 58.59%] [G loss: 4.283566]\n",
      "epoch:18 step:14768 [D loss: 0.022062, acc.: 100.00%] [G loss: 2.782814]\n",
      "epoch:18 step:14769 [D loss: 0.140624, acc.: 94.53%] [G loss: 3.422399]\n",
      "epoch:18 step:14770 [D loss: 0.019528, acc.: 100.00%] [G loss: 3.873491]\n",
      "epoch:18 step:14771 [D loss: 0.194958, acc.: 90.62%] [G loss: 1.597234]\n",
      "epoch:18 step:14772 [D loss: 0.226040, acc.: 90.62%] [G loss: 3.493348]\n",
      "epoch:18 step:14773 [D loss: 0.805401, acc.: 64.84%] [G loss: 3.423798]\n",
      "epoch:18 step:14774 [D loss: 0.070342, acc.: 98.44%] [G loss: 2.929634]\n",
      "epoch:18 step:14775 [D loss: 0.022870, acc.: 100.00%] [G loss: 4.110926]\n",
      "epoch:18 step:14776 [D loss: 0.033657, acc.: 100.00%] [G loss: 4.582134]\n",
      "epoch:18 step:14777 [D loss: 0.017920, acc.: 100.00%] [G loss: 3.422740]\n",
      "epoch:18 step:14778 [D loss: 0.044983, acc.: 99.22%] [G loss: 5.400943]\n",
      "epoch:18 step:14779 [D loss: 0.024426, acc.: 99.22%] [G loss: 4.399556]\n",
      "epoch:18 step:14780 [D loss: 0.080587, acc.: 99.22%] [G loss: 4.373396]\n",
      "epoch:18 step:14781 [D loss: 0.056325, acc.: 99.22%] [G loss: 5.012613]\n",
      "epoch:18 step:14782 [D loss: 0.084856, acc.: 100.00%] [G loss: 2.743864]\n",
      "epoch:18 step:14783 [D loss: 0.375129, acc.: 82.81%] [G loss: 5.592802]\n",
      "epoch:18 step:14784 [D loss: 0.019296, acc.: 100.00%] [G loss: 6.337228]\n",
      "epoch:18 step:14785 [D loss: 0.154939, acc.: 92.97%] [G loss: 3.743751]\n",
      "epoch:18 step:14786 [D loss: 0.016674, acc.: 100.00%] [G loss: 2.864410]\n",
      "epoch:18 step:14787 [D loss: 0.021640, acc.: 100.00%] [G loss: 2.734952]\n",
      "epoch:18 step:14788 [D loss: 0.047706, acc.: 99.22%] [G loss: 3.424953]\n",
      "epoch:18 step:14789 [D loss: 0.037013, acc.: 99.22%] [G loss: 3.600417]\n",
      "epoch:18 step:14790 [D loss: 0.028728, acc.: 100.00%] [G loss: 4.227510]\n",
      "epoch:18 step:14791 [D loss: 0.033584, acc.: 100.00%] [G loss: 3.441725]\n",
      "epoch:18 step:14792 [D loss: 0.053028, acc.: 98.44%] [G loss: 2.953885]\n",
      "epoch:18 step:14793 [D loss: 1.747403, acc.: 37.50%] [G loss: 6.516324]\n",
      "epoch:18 step:14794 [D loss: 0.403788, acc.: 77.34%] [G loss: 5.488108]\n",
      "epoch:18 step:14795 [D loss: 1.980825, acc.: 29.69%] [G loss: 3.182547]\n",
      "epoch:18 step:14796 [D loss: 0.245873, acc.: 88.28%] [G loss: 2.813485]\n",
      "epoch:18 step:14797 [D loss: 0.310934, acc.: 86.72%] [G loss: 2.104087]\n",
      "epoch:18 step:14798 [D loss: 0.069939, acc.: 99.22%] [G loss: 1.066204]\n",
      "epoch:18 step:14799 [D loss: 1.200167, acc.: 56.25%] [G loss: 6.164325]\n",
      "epoch:18 step:14800 [D loss: 1.460150, acc.: 50.78%] [G loss: 4.942503]\n",
      "##############\n",
      "[0.8985976  1.05213236 0.93137102 1.11346654 0.97239755 0.90455514\n",
      " 1.10908315 0.84365931 0.76749145 2.10719811]\n",
      "##########\n",
      "epoch:18 step:14801 [D loss: 1.136354, acc.: 58.59%] [G loss: 3.048733]\n",
      "epoch:18 step:14802 [D loss: 0.555056, acc.: 78.12%] [G loss: 3.320044]\n",
      "epoch:18 step:14803 [D loss: 0.216279, acc.: 92.19%] [G loss: 3.173005]\n",
      "epoch:18 step:14804 [D loss: 0.157701, acc.: 96.88%] [G loss: 3.266887]\n",
      "epoch:18 step:14805 [D loss: 0.272006, acc.: 89.84%] [G loss: 2.840231]\n",
      "epoch:18 step:14806 [D loss: 0.164838, acc.: 96.88%] [G loss: 2.412945]\n",
      "epoch:18 step:14807 [D loss: 0.264824, acc.: 89.06%] [G loss: 2.917895]\n",
      "epoch:18 step:14808 [D loss: 0.327405, acc.: 85.94%] [G loss: 3.277997]\n",
      "epoch:18 step:14809 [D loss: 0.107856, acc.: 99.22%] [G loss: 2.533226]\n",
      "epoch:18 step:14810 [D loss: 0.176941, acc.: 94.53%] [G loss: 3.137103]\n",
      "epoch:18 step:14811 [D loss: 0.239571, acc.: 92.97%] [G loss: 2.735018]\n",
      "epoch:18 step:14812 [D loss: 0.226449, acc.: 90.62%] [G loss: 2.328822]\n",
      "epoch:18 step:14813 [D loss: 0.125104, acc.: 99.22%] [G loss: 2.454269]\n",
      "epoch:18 step:14814 [D loss: 0.097968, acc.: 99.22%] [G loss: 2.779575]\n",
      "epoch:18 step:14815 [D loss: 0.131613, acc.: 96.09%] [G loss: 3.160403]\n",
      "epoch:18 step:14816 [D loss: 0.125870, acc.: 96.09%] [G loss: 3.320590]\n",
      "epoch:18 step:14817 [D loss: 0.093523, acc.: 98.44%] [G loss: 3.592869]\n",
      "epoch:18 step:14818 [D loss: 0.115953, acc.: 96.09%] [G loss: 1.229977]\n",
      "epoch:18 step:14819 [D loss: 0.319700, acc.: 82.81%] [G loss: 4.274624]\n",
      "epoch:18 step:14820 [D loss: 0.291061, acc.: 89.06%] [G loss: 3.520347]\n",
      "epoch:18 step:14821 [D loss: 0.189072, acc.: 95.31%] [G loss: 3.375285]\n",
      "epoch:18 step:14822 [D loss: 0.060643, acc.: 100.00%] [G loss: 3.163505]\n",
      "epoch:18 step:14823 [D loss: 0.108830, acc.: 96.88%] [G loss: 3.221176]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:18 step:14824 [D loss: 0.174183, acc.: 96.09%] [G loss: 3.278607]\n",
      "epoch:18 step:14825 [D loss: 0.263070, acc.: 90.62%] [G loss: 3.681175]\n",
      "epoch:18 step:14826 [D loss: 0.138695, acc.: 96.09%] [G loss: 3.717285]\n",
      "epoch:18 step:14827 [D loss: 0.350256, acc.: 85.94%] [G loss: 3.052392]\n",
      "epoch:18 step:14828 [D loss: 0.051267, acc.: 99.22%] [G loss: 1.969268]\n",
      "epoch:18 step:14829 [D loss: 0.507333, acc.: 71.88%] [G loss: 4.487658]\n",
      "epoch:18 step:14830 [D loss: 0.273472, acc.: 89.06%] [G loss: 4.206072]\n",
      "epoch:18 step:14831 [D loss: 0.166640, acc.: 93.75%] [G loss: 2.246145]\n",
      "epoch:18 step:14832 [D loss: 0.030174, acc.: 100.00%] [G loss: 1.385676]\n",
      "epoch:18 step:14833 [D loss: 0.082325, acc.: 100.00%] [G loss: 3.498093]\n",
      "epoch:18 step:14834 [D loss: 0.023169, acc.: 100.00%] [G loss: 2.400978]\n",
      "epoch:18 step:14835 [D loss: 0.050834, acc.: 99.22%] [G loss: 1.983679]\n",
      "epoch:18 step:14836 [D loss: 0.026543, acc.: 100.00%] [G loss: 2.338604]\n",
      "epoch:18 step:14837 [D loss: 0.574301, acc.: 69.53%] [G loss: 3.796320]\n",
      "epoch:18 step:14838 [D loss: 0.426838, acc.: 77.34%] [G loss: 2.817970]\n",
      "epoch:18 step:14839 [D loss: 0.224533, acc.: 88.28%] [G loss: 3.635289]\n",
      "epoch:19 step:14840 [D loss: 0.037395, acc.: 98.44%] [G loss: 3.261096]\n",
      "epoch:19 step:14841 [D loss: 0.211463, acc.: 94.53%] [G loss: 1.699644]\n",
      "epoch:19 step:14842 [D loss: 0.082486, acc.: 98.44%] [G loss: 2.612387]\n",
      "epoch:19 step:14843 [D loss: 0.020948, acc.: 100.00%] [G loss: 2.935360]\n",
      "epoch:19 step:14844 [D loss: 0.050672, acc.: 100.00%] [G loss: 2.204156]\n",
      "epoch:19 step:14845 [D loss: 0.189122, acc.: 96.88%] [G loss: 4.163471]\n",
      "epoch:19 step:14846 [D loss: 0.077379, acc.: 98.44%] [G loss: 3.408205]\n",
      "epoch:19 step:14847 [D loss: 0.031453, acc.: 100.00%] [G loss: 3.307564]\n",
      "epoch:19 step:14848 [D loss: 0.499951, acc.: 75.78%] [G loss: 4.582053]\n",
      "epoch:19 step:14849 [D loss: 0.126296, acc.: 95.31%] [G loss: 4.323552]\n",
      "epoch:19 step:14850 [D loss: 0.322620, acc.: 82.03%] [G loss: 1.097656]\n",
      "epoch:19 step:14851 [D loss: 0.292404, acc.: 85.94%] [G loss: 3.760423]\n",
      "epoch:19 step:14852 [D loss: 0.020342, acc.: 100.00%] [G loss: 4.572101]\n",
      "epoch:19 step:14853 [D loss: 0.139002, acc.: 93.75%] [G loss: 3.217939]\n",
      "epoch:19 step:14854 [D loss: 0.050255, acc.: 100.00%] [G loss: 2.472109]\n",
      "epoch:19 step:14855 [D loss: 0.121611, acc.: 96.09%] [G loss: 2.800460]\n",
      "epoch:19 step:14856 [D loss: 0.020968, acc.: 100.00%] [G loss: 2.713504]\n",
      "epoch:19 step:14857 [D loss: 0.045030, acc.: 100.00%] [G loss: 2.315730]\n",
      "epoch:19 step:14858 [D loss: 0.278653, acc.: 86.72%] [G loss: 3.931951]\n",
      "epoch:19 step:14859 [D loss: 0.342110, acc.: 86.72%] [G loss: 2.992789]\n",
      "epoch:19 step:14860 [D loss: 0.144846, acc.: 96.09%] [G loss: 3.245645]\n",
      "epoch:19 step:14861 [D loss: 0.057489, acc.: 99.22%] [G loss: 3.984912]\n",
      "epoch:19 step:14862 [D loss: 0.127561, acc.: 97.66%] [G loss: 2.064105]\n",
      "epoch:19 step:14863 [D loss: 0.127723, acc.: 99.22%] [G loss: 3.483156]\n",
      "epoch:19 step:14864 [D loss: 0.115180, acc.: 95.31%] [G loss: 3.191216]\n",
      "epoch:19 step:14865 [D loss: 0.118255, acc.: 96.88%] [G loss: 4.008037]\n",
      "epoch:19 step:14866 [D loss: 0.030364, acc.: 100.00%] [G loss: 4.500877]\n",
      "epoch:19 step:14867 [D loss: 0.415444, acc.: 82.03%] [G loss: 4.303308]\n",
      "epoch:19 step:14868 [D loss: 0.153154, acc.: 94.53%] [G loss: 4.141824]\n",
      "epoch:19 step:14869 [D loss: 0.047589, acc.: 99.22%] [G loss: 4.253077]\n",
      "epoch:19 step:14870 [D loss: 0.075874, acc.: 97.66%] [G loss: 3.541302]\n",
      "epoch:19 step:14871 [D loss: 0.070993, acc.: 97.66%] [G loss: 4.152821]\n",
      "epoch:19 step:14872 [D loss: 0.066541, acc.: 98.44%] [G loss: 4.131805]\n",
      "epoch:19 step:14873 [D loss: 0.018671, acc.: 100.00%] [G loss: 4.330866]\n",
      "epoch:19 step:14874 [D loss: 0.109431, acc.: 97.66%] [G loss: 4.125774]\n",
      "epoch:19 step:14875 [D loss: 0.054002, acc.: 99.22%] [G loss: 3.968625]\n",
      "epoch:19 step:14876 [D loss: 0.017443, acc.: 100.00%] [G loss: 4.388035]\n",
      "epoch:19 step:14877 [D loss: 0.044236, acc.: 100.00%] [G loss: 3.332341]\n",
      "epoch:19 step:14878 [D loss: 0.079918, acc.: 99.22%] [G loss: 2.430777]\n",
      "epoch:19 step:14879 [D loss: 0.020293, acc.: 100.00%] [G loss: 3.071973]\n",
      "epoch:19 step:14880 [D loss: 0.452555, acc.: 76.56%] [G loss: 8.096901]\n",
      "epoch:19 step:14881 [D loss: 1.478549, acc.: 52.34%] [G loss: 2.192476]\n",
      "epoch:19 step:14882 [D loss: 0.484911, acc.: 78.91%] [G loss: 5.951779]\n",
      "epoch:19 step:14883 [D loss: 0.189397, acc.: 91.41%] [G loss: 5.959796]\n",
      "epoch:19 step:14884 [D loss: 0.250469, acc.: 88.28%] [G loss: 2.978310]\n",
      "epoch:19 step:14885 [D loss: 0.061552, acc.: 99.22%] [G loss: 2.771089]\n",
      "epoch:19 step:14886 [D loss: 0.026219, acc.: 99.22%] [G loss: 4.111371]\n",
      "epoch:19 step:14887 [D loss: 0.035160, acc.: 99.22%] [G loss: 4.005894]\n",
      "epoch:19 step:14888 [D loss: 0.018354, acc.: 100.00%] [G loss: 4.192117]\n",
      "epoch:19 step:14889 [D loss: 0.217124, acc.: 89.84%] [G loss: 3.785744]\n",
      "epoch:19 step:14890 [D loss: 0.020022, acc.: 100.00%] [G loss: 4.249960]\n",
      "epoch:19 step:14891 [D loss: 0.077947, acc.: 98.44%] [G loss: 3.759357]\n",
      "epoch:19 step:14892 [D loss: 0.008476, acc.: 100.00%] [G loss: 4.403985]\n",
      "epoch:19 step:14893 [D loss: 0.213870, acc.: 89.06%] [G loss: 4.791739]\n",
      "epoch:19 step:14894 [D loss: 0.030763, acc.: 98.44%] [G loss: 5.015489]\n",
      "epoch:19 step:14895 [D loss: 0.200318, acc.: 93.75%] [G loss: 3.620280]\n",
      "epoch:19 step:14896 [D loss: 0.133010, acc.: 94.53%] [G loss: 5.231681]\n",
      "epoch:19 step:14897 [D loss: 0.019353, acc.: 100.00%] [G loss: 5.781374]\n",
      "epoch:19 step:14898 [D loss: 0.108387, acc.: 97.66%] [G loss: 3.472115]\n",
      "epoch:19 step:14899 [D loss: 0.082877, acc.: 96.88%] [G loss: 4.269484]\n",
      "epoch:19 step:14900 [D loss: 0.044408, acc.: 99.22%] [G loss: 4.760777]\n",
      "epoch:19 step:14901 [D loss: 0.027027, acc.: 100.00%] [G loss: 4.380409]\n",
      "epoch:19 step:14902 [D loss: 0.030360, acc.: 100.00%] [G loss: 4.532376]\n",
      "epoch:19 step:14903 [D loss: 0.042744, acc.: 99.22%] [G loss: 4.231132]\n",
      "epoch:19 step:14904 [D loss: 0.203199, acc.: 91.41%] [G loss: 4.679550]\n",
      "epoch:19 step:14905 [D loss: 0.023549, acc.: 100.00%] [G loss: 4.784906]\n",
      "epoch:19 step:14906 [D loss: 0.012286, acc.: 100.00%] [G loss: 4.887298]\n",
      "epoch:19 step:14907 [D loss: 0.042901, acc.: 99.22%] [G loss: 4.490753]\n",
      "epoch:19 step:14908 [D loss: 0.052526, acc.: 98.44%] [G loss: 3.142896]\n",
      "epoch:19 step:14909 [D loss: 0.054292, acc.: 99.22%] [G loss: 3.744592]\n",
      "epoch:19 step:14910 [D loss: 0.035903, acc.: 100.00%] [G loss: 4.447733]\n",
      "epoch:19 step:14911 [D loss: 0.051531, acc.: 99.22%] [G loss: 4.344932]\n",
      "epoch:19 step:14912 [D loss: 0.055581, acc.: 98.44%] [G loss: 3.784254]\n",
      "epoch:19 step:14913 [D loss: 0.037568, acc.: 98.44%] [G loss: 3.284084]\n",
      "epoch:19 step:14914 [D loss: 0.033809, acc.: 100.00%] [G loss: 4.747644]\n",
      "epoch:19 step:14915 [D loss: 0.050904, acc.: 100.00%] [G loss: 4.139601]\n",
      "epoch:19 step:14916 [D loss: 1.451097, acc.: 33.59%] [G loss: 6.831162]\n",
      "epoch:19 step:14917 [D loss: 1.477761, acc.: 53.91%] [G loss: 1.840574]\n",
      "epoch:19 step:14918 [D loss: 0.324534, acc.: 82.81%] [G loss: 1.775505]\n",
      "epoch:19 step:14919 [D loss: 0.721814, acc.: 64.06%] [G loss: 5.472945]\n",
      "epoch:19 step:14920 [D loss: 1.112161, acc.: 57.03%] [G loss: 2.840874]\n",
      "epoch:19 step:14921 [D loss: 0.079429, acc.: 96.88%] [G loss: 2.096199]\n",
      "epoch:19 step:14922 [D loss: 0.361840, acc.: 80.47%] [G loss: 3.987120]\n",
      "epoch:19 step:14923 [D loss: 0.217615, acc.: 88.28%] [G loss: 3.792266]\n",
      "epoch:19 step:14924 [D loss: 0.048137, acc.: 99.22%] [G loss: 3.135901]\n",
      "epoch:19 step:14925 [D loss: 0.046892, acc.: 100.00%] [G loss: 3.231324]\n",
      "epoch:19 step:14926 [D loss: 0.113290, acc.: 96.88%] [G loss: 2.986539]\n",
      "epoch:19 step:14927 [D loss: 0.029117, acc.: 100.00%] [G loss: 3.014375]\n",
      "epoch:19 step:14928 [D loss: 0.030171, acc.: 100.00%] [G loss: 2.479614]\n",
      "epoch:19 step:14929 [D loss: 1.269456, acc.: 41.41%] [G loss: 5.141277]\n",
      "epoch:19 step:14930 [D loss: 0.419661, acc.: 75.00%] [G loss: 5.005183]\n",
      "epoch:19 step:14931 [D loss: 0.316176, acc.: 84.38%] [G loss: 2.363089]\n",
      "epoch:19 step:14932 [D loss: 0.058064, acc.: 99.22%] [G loss: 1.827969]\n",
      "epoch:19 step:14933 [D loss: 0.307147, acc.: 85.16%] [G loss: 3.915717]\n",
      "epoch:19 step:14934 [D loss: 0.196545, acc.: 92.97%] [G loss: 3.791789]\n",
      "epoch:19 step:14935 [D loss: 0.111346, acc.: 95.31%] [G loss: 2.558254]\n",
      "epoch:19 step:14936 [D loss: 0.257734, acc.: 87.50%] [G loss: 3.901439]\n",
      "epoch:19 step:14937 [D loss: 0.154898, acc.: 94.53%] [G loss: 3.505204]\n",
      "epoch:19 step:14938 [D loss: 0.123406, acc.: 97.66%] [G loss: 2.931483]\n",
      "epoch:19 step:14939 [D loss: 0.122236, acc.: 96.09%] [G loss: 3.354540]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:19 step:14940 [D loss: 0.047831, acc.: 99.22%] [G loss: 3.694413]\n",
      "epoch:19 step:14941 [D loss: 0.047536, acc.: 100.00%] [G loss: 3.543818]\n",
      "epoch:19 step:14942 [D loss: 0.218712, acc.: 92.97%] [G loss: 3.798242]\n",
      "epoch:19 step:14943 [D loss: 0.036313, acc.: 100.00%] [G loss: 4.152636]\n",
      "epoch:19 step:14944 [D loss: 0.047881, acc.: 100.00%] [G loss: 3.628301]\n",
      "epoch:19 step:14945 [D loss: 0.075795, acc.: 99.22%] [G loss: 3.305419]\n",
      "epoch:19 step:14946 [D loss: 0.053314, acc.: 100.00%] [G loss: 3.046010]\n",
      "epoch:19 step:14947 [D loss: 0.091539, acc.: 95.31%] [G loss: 4.397414]\n",
      "epoch:19 step:14948 [D loss: 0.040011, acc.: 100.00%] [G loss: 4.772001]\n",
      "epoch:19 step:14949 [D loss: 0.080730, acc.: 99.22%] [G loss: 3.530976]\n",
      "epoch:19 step:14950 [D loss: 0.112681, acc.: 98.44%] [G loss: 3.738987]\n",
      "epoch:19 step:14951 [D loss: 0.061270, acc.: 97.66%] [G loss: 3.204404]\n",
      "epoch:19 step:14952 [D loss: 0.021827, acc.: 100.00%] [G loss: 3.857415]\n",
      "epoch:19 step:14953 [D loss: 0.043814, acc.: 100.00%] [G loss: 2.196848]\n",
      "epoch:19 step:14954 [D loss: 0.026715, acc.: 100.00%] [G loss: 2.592923]\n",
      "epoch:19 step:14955 [D loss: 0.363566, acc.: 84.38%] [G loss: 1.755471]\n",
      "epoch:19 step:14956 [D loss: 0.018963, acc.: 100.00%] [G loss: 4.246932]\n",
      "epoch:19 step:14957 [D loss: 0.017270, acc.: 100.00%] [G loss: 4.258854]\n",
      "epoch:19 step:14958 [D loss: 0.062712, acc.: 98.44%] [G loss: 3.974479]\n",
      "epoch:19 step:14959 [D loss: 0.022287, acc.: 100.00%] [G loss: 2.567170]\n",
      "epoch:19 step:14960 [D loss: 0.124568, acc.: 98.44%] [G loss: 3.867640]\n",
      "epoch:19 step:14961 [D loss: 0.030224, acc.: 100.00%] [G loss: 4.679699]\n",
      "epoch:19 step:14962 [D loss: 0.040041, acc.: 100.00%] [G loss: 4.050377]\n",
      "epoch:19 step:14963 [D loss: 0.018064, acc.: 100.00%] [G loss: 4.588822]\n",
      "epoch:19 step:14964 [D loss: 0.033624, acc.: 100.00%] [G loss: 2.856380]\n",
      "epoch:19 step:14965 [D loss: 0.036472, acc.: 100.00%] [G loss: 1.477104]\n",
      "epoch:19 step:14966 [D loss: 0.051966, acc.: 98.44%] [G loss: 2.895349]\n",
      "epoch:19 step:14967 [D loss: 0.142243, acc.: 96.88%] [G loss: 4.073542]\n",
      "epoch:19 step:14968 [D loss: 0.036980, acc.: 100.00%] [G loss: 3.244851]\n",
      "epoch:19 step:14969 [D loss: 0.052539, acc.: 99.22%] [G loss: 5.473168]\n",
      "epoch:19 step:14970 [D loss: 0.021636, acc.: 100.00%] [G loss: 5.421911]\n",
      "epoch:19 step:14971 [D loss: 2.202602, acc.: 19.53%] [G loss: 6.694924]\n",
      "epoch:19 step:14972 [D loss: 0.504324, acc.: 75.00%] [G loss: 6.149634]\n",
      "epoch:19 step:14973 [D loss: 0.841548, acc.: 64.06%] [G loss: 2.423535]\n",
      "epoch:19 step:14974 [D loss: 0.072266, acc.: 98.44%] [G loss: 1.245528]\n",
      "epoch:19 step:14975 [D loss: 0.312324, acc.: 84.38%] [G loss: 4.401776]\n",
      "epoch:19 step:14976 [D loss: 0.012180, acc.: 100.00%] [G loss: 4.939076]\n",
      "epoch:19 step:14977 [D loss: 0.081538, acc.: 98.44%] [G loss: 4.332691]\n",
      "epoch:19 step:14978 [D loss: 0.025945, acc.: 100.00%] [G loss: 3.939021]\n",
      "epoch:19 step:14979 [D loss: 0.027209, acc.: 99.22%] [G loss: 3.617757]\n",
      "epoch:19 step:14980 [D loss: 0.033259, acc.: 99.22%] [G loss: 2.579740]\n",
      "epoch:19 step:14981 [D loss: 0.045317, acc.: 100.00%] [G loss: 2.422320]\n",
      "epoch:19 step:14982 [D loss: 0.253093, acc.: 91.41%] [G loss: 4.631480]\n",
      "epoch:19 step:14983 [D loss: 0.042051, acc.: 99.22%] [G loss: 5.174219]\n",
      "epoch:19 step:14984 [D loss: 0.042750, acc.: 100.00%] [G loss: 4.576838]\n",
      "epoch:19 step:14985 [D loss: 0.046663, acc.: 100.00%] [G loss: 3.275931]\n",
      "epoch:19 step:14986 [D loss: 0.131917, acc.: 96.09%] [G loss: 4.211863]\n",
      "epoch:19 step:14987 [D loss: 0.028008, acc.: 99.22%] [G loss: 4.547766]\n",
      "epoch:19 step:14988 [D loss: 0.015819, acc.: 100.00%] [G loss: 4.810118]\n",
      "epoch:19 step:14989 [D loss: 0.053382, acc.: 99.22%] [G loss: 3.900903]\n",
      "epoch:19 step:14990 [D loss: 0.033855, acc.: 100.00%] [G loss: 3.293203]\n",
      "epoch:19 step:14991 [D loss: 0.032535, acc.: 100.00%] [G loss: 3.397556]\n",
      "epoch:19 step:14992 [D loss: 0.438953, acc.: 77.34%] [G loss: 4.332230]\n",
      "epoch:19 step:14993 [D loss: 0.023170, acc.: 100.00%] [G loss: 5.453248]\n",
      "epoch:19 step:14994 [D loss: 0.101304, acc.: 96.09%] [G loss: 4.127214]\n",
      "epoch:19 step:14995 [D loss: 0.039187, acc.: 100.00%] [G loss: 2.513510]\n",
      "epoch:19 step:14996 [D loss: 0.037140, acc.: 100.00%] [G loss: 3.068742]\n",
      "epoch:19 step:14997 [D loss: 0.141181, acc.: 96.88%] [G loss: 1.847609]\n",
      "epoch:19 step:14998 [D loss: 0.140630, acc.: 93.75%] [G loss: 4.908303]\n",
      "epoch:19 step:14999 [D loss: 0.166239, acc.: 93.75%] [G loss: 3.475314]\n",
      "epoch:19 step:15000 [D loss: 0.199153, acc.: 91.41%] [G loss: 5.112664]\n",
      "##############\n",
      "[0.97022197 0.92436709 0.97820751 0.97751131 1.01548123 0.92565672\n",
      " 1.01227042 2.1257725  1.11183424 1.02019738]\n",
      "##########\n",
      "epoch:19 step:15001 [D loss: 0.003872, acc.: 100.00%] [G loss: 5.396468]\n",
      "epoch:19 step:15002 [D loss: 0.065455, acc.: 99.22%] [G loss: 4.212442]\n",
      "epoch:19 step:15003 [D loss: 0.058305, acc.: 99.22%] [G loss: 4.393337]\n",
      "epoch:19 step:15004 [D loss: 0.009757, acc.: 100.00%] [G loss: 4.380019]\n",
      "epoch:19 step:15005 [D loss: 0.130878, acc.: 96.88%] [G loss: 4.844615]\n",
      "epoch:19 step:15006 [D loss: 0.014295, acc.: 100.00%] [G loss: 5.629608]\n",
      "epoch:19 step:15007 [D loss: 0.061164, acc.: 99.22%] [G loss: 3.676756]\n",
      "epoch:19 step:15008 [D loss: 0.026311, acc.: 100.00%] [G loss: 3.513278]\n",
      "epoch:19 step:15009 [D loss: 0.028156, acc.: 100.00%] [G loss: 4.502472]\n",
      "epoch:19 step:15010 [D loss: 0.024722, acc.: 100.00%] [G loss: 4.766766]\n",
      "epoch:19 step:15011 [D loss: 0.008763, acc.: 100.00%] [G loss: 4.898302]\n",
      "epoch:19 step:15012 [D loss: 0.022705, acc.: 100.00%] [G loss: 3.898008]\n",
      "epoch:19 step:15013 [D loss: 0.047125, acc.: 100.00%] [G loss: 2.930677]\n",
      "epoch:19 step:15014 [D loss: 0.020270, acc.: 100.00%] [G loss: 3.892530]\n",
      "epoch:19 step:15015 [D loss: 0.430104, acc.: 78.91%] [G loss: 5.048443]\n",
      "epoch:19 step:15016 [D loss: 0.122271, acc.: 95.31%] [G loss: 3.886707]\n",
      "epoch:19 step:15017 [D loss: 0.107414, acc.: 96.88%] [G loss: 2.763837]\n",
      "epoch:19 step:15018 [D loss: 0.204282, acc.: 90.62%] [G loss: 5.478175]\n",
      "epoch:19 step:15019 [D loss: 0.074543, acc.: 97.66%] [G loss: 5.264889]\n",
      "epoch:19 step:15020 [D loss: 0.316040, acc.: 86.72%] [G loss: 2.517422]\n",
      "epoch:19 step:15021 [D loss: 0.160712, acc.: 94.53%] [G loss: 5.931965]\n",
      "epoch:19 step:15022 [D loss: 0.014446, acc.: 100.00%] [G loss: 6.629854]\n",
      "epoch:19 step:15023 [D loss: 0.085484, acc.: 98.44%] [G loss: 3.944749]\n",
      "epoch:19 step:15024 [D loss: 0.040272, acc.: 100.00%] [G loss: 4.079569]\n",
      "epoch:19 step:15025 [D loss: 0.042914, acc.: 100.00%] [G loss: 4.887115]\n",
      "epoch:19 step:15026 [D loss: 0.497282, acc.: 76.56%] [G loss: 6.628348]\n",
      "epoch:19 step:15027 [D loss: 0.056408, acc.: 98.44%] [G loss: 6.013954]\n",
      "epoch:19 step:15028 [D loss: 0.068820, acc.: 98.44%] [G loss: 5.639598]\n",
      "epoch:19 step:15029 [D loss: 0.029048, acc.: 100.00%] [G loss: 4.697493]\n",
      "epoch:19 step:15030 [D loss: 0.031691, acc.: 100.00%] [G loss: 4.389904]\n",
      "epoch:19 step:15031 [D loss: 0.015138, acc.: 100.00%] [G loss: 4.420286]\n",
      "epoch:19 step:15032 [D loss: 0.050721, acc.: 99.22%] [G loss: 3.168362]\n",
      "epoch:19 step:15033 [D loss: 0.076403, acc.: 97.66%] [G loss: 4.909050]\n",
      "epoch:19 step:15034 [D loss: 0.010092, acc.: 100.00%] [G loss: 5.489100]\n",
      "epoch:19 step:15035 [D loss: 0.019432, acc.: 100.00%] [G loss: 4.940966]\n",
      "epoch:19 step:15036 [D loss: 0.036640, acc.: 100.00%] [G loss: 3.297696]\n",
      "epoch:19 step:15037 [D loss: 0.037007, acc.: 99.22%] [G loss: 3.836690]\n",
      "epoch:19 step:15038 [D loss: 0.009418, acc.: 100.00%] [G loss: 3.549833]\n",
      "epoch:19 step:15039 [D loss: 0.155659, acc.: 94.53%] [G loss: 5.074315]\n",
      "epoch:19 step:15040 [D loss: 0.100611, acc.: 96.88%] [G loss: 5.295599]\n",
      "epoch:19 step:15041 [D loss: 0.018357, acc.: 100.00%] [G loss: 3.238676]\n",
      "epoch:19 step:15042 [D loss: 0.005863, acc.: 100.00%] [G loss: 4.773314]\n",
      "epoch:19 step:15043 [D loss: 0.037517, acc.: 100.00%] [G loss: 4.864957]\n",
      "epoch:19 step:15044 [D loss: 0.329194, acc.: 89.06%] [G loss: 7.230751]\n",
      "epoch:19 step:15045 [D loss: 0.289952, acc.: 84.38%] [G loss: 3.231551]\n",
      "epoch:19 step:15046 [D loss: 0.612121, acc.: 70.31%] [G loss: 8.524908]\n",
      "epoch:19 step:15047 [D loss: 2.821029, acc.: 50.00%] [G loss: 5.790936]\n",
      "epoch:19 step:15048 [D loss: 1.192683, acc.: 50.78%] [G loss: 1.483567]\n",
      "epoch:19 step:15049 [D loss: 0.483625, acc.: 71.88%] [G loss: 3.836498]\n",
      "epoch:19 step:15050 [D loss: 0.015578, acc.: 100.00%] [G loss: 5.202539]\n",
      "epoch:19 step:15051 [D loss: 0.506139, acc.: 74.22%] [G loss: 3.592429]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:19 step:15052 [D loss: 0.146726, acc.: 93.75%] [G loss: 2.718433]\n",
      "epoch:19 step:15053 [D loss: 0.081094, acc.: 98.44%] [G loss: 3.042175]\n",
      "epoch:19 step:15054 [D loss: 0.029628, acc.: 100.00%] [G loss: 3.683722]\n",
      "epoch:19 step:15055 [D loss: 0.036797, acc.: 100.00%] [G loss: 3.443487]\n",
      "epoch:19 step:15056 [D loss: 0.150401, acc.: 96.09%] [G loss: 4.054221]\n",
      "epoch:19 step:15057 [D loss: 0.772076, acc.: 58.59%] [G loss: 3.594352]\n",
      "epoch:19 step:15058 [D loss: 0.028787, acc.: 100.00%] [G loss: 5.118974]\n",
      "epoch:19 step:15059 [D loss: 0.541054, acc.: 72.66%] [G loss: 1.844583]\n",
      "epoch:19 step:15060 [D loss: 0.089312, acc.: 97.66%] [G loss: 1.866939]\n",
      "epoch:19 step:15061 [D loss: 0.035302, acc.: 100.00%] [G loss: 3.486094]\n",
      "epoch:19 step:15062 [D loss: 0.010375, acc.: 100.00%] [G loss: 3.286156]\n",
      "epoch:19 step:15063 [D loss: 0.032883, acc.: 100.00%] [G loss: 3.081723]\n",
      "epoch:19 step:15064 [D loss: 0.102208, acc.: 97.66%] [G loss: 3.250571]\n",
      "epoch:19 step:15065 [D loss: 0.040461, acc.: 100.00%] [G loss: 2.318649]\n",
      "epoch:19 step:15066 [D loss: 0.094565, acc.: 99.22%] [G loss: 3.504354]\n",
      "epoch:19 step:15067 [D loss: 0.022210, acc.: 100.00%] [G loss: 3.722231]\n",
      "epoch:19 step:15068 [D loss: 0.148698, acc.: 96.88%] [G loss: 2.349926]\n",
      "epoch:19 step:15069 [D loss: 0.071941, acc.: 100.00%] [G loss: 2.187075]\n",
      "epoch:19 step:15070 [D loss: 0.050975, acc.: 100.00%] [G loss: 2.716637]\n",
      "epoch:19 step:15071 [D loss: 0.876328, acc.: 54.69%] [G loss: 6.127602]\n",
      "epoch:19 step:15072 [D loss: 1.025707, acc.: 55.47%] [G loss: 4.975513]\n",
      "epoch:19 step:15073 [D loss: 0.548728, acc.: 73.44%] [G loss: 2.475337]\n",
      "epoch:19 step:15074 [D loss: 0.376825, acc.: 84.38%] [G loss: 4.018736]\n",
      "epoch:19 step:15075 [D loss: 0.060190, acc.: 98.44%] [G loss: 5.336557]\n",
      "epoch:19 step:15076 [D loss: 0.026374, acc.: 100.00%] [G loss: 4.671336]\n",
      "epoch:19 step:15077 [D loss: 0.012984, acc.: 100.00%] [G loss: 4.950994]\n",
      "epoch:19 step:15078 [D loss: 0.019861, acc.: 100.00%] [G loss: 4.626557]\n",
      "epoch:19 step:15079 [D loss: 0.175437, acc.: 93.75%] [G loss: 2.431377]\n",
      "epoch:19 step:15080 [D loss: 0.026725, acc.: 100.00%] [G loss: 3.273504]\n",
      "epoch:19 step:15081 [D loss: 0.015155, acc.: 100.00%] [G loss: 2.841153]\n",
      "epoch:19 step:15082 [D loss: 0.032165, acc.: 100.00%] [G loss: 2.908724]\n",
      "epoch:19 step:15083 [D loss: 0.049794, acc.: 100.00%] [G loss: 3.399444]\n",
      "epoch:19 step:15084 [D loss: 0.016589, acc.: 100.00%] [G loss: 2.608855]\n",
      "epoch:19 step:15085 [D loss: 0.037240, acc.: 99.22%] [G loss: 3.024004]\n",
      "epoch:19 step:15086 [D loss: 0.065096, acc.: 98.44%] [G loss: 2.092876]\n",
      "epoch:19 step:15087 [D loss: 0.058105, acc.: 98.44%] [G loss: 2.206844]\n",
      "epoch:19 step:15088 [D loss: 0.049843, acc.: 100.00%] [G loss: 2.710594]\n",
      "epoch:19 step:15089 [D loss: 0.356714, acc.: 85.16%] [G loss: 2.805187]\n",
      "epoch:19 step:15090 [D loss: 0.024529, acc.: 100.00%] [G loss: 3.988662]\n",
      "epoch:19 step:15091 [D loss: 0.087559, acc.: 97.66%] [G loss: 2.288188]\n",
      "epoch:19 step:15092 [D loss: 0.038005, acc.: 100.00%] [G loss: 2.425601]\n",
      "epoch:19 step:15093 [D loss: 0.021226, acc.: 100.00%] [G loss: 3.072083]\n",
      "epoch:19 step:15094 [D loss: 0.111317, acc.: 97.66%] [G loss: 3.693897]\n",
      "epoch:19 step:15095 [D loss: 0.075364, acc.: 96.88%] [G loss: 3.589302]\n",
      "epoch:19 step:15096 [D loss: 0.041163, acc.: 100.00%] [G loss: 2.676871]\n",
      "epoch:19 step:15097 [D loss: 0.022394, acc.: 100.00%] [G loss: 2.597211]\n",
      "epoch:19 step:15098 [D loss: 0.064901, acc.: 99.22%] [G loss: 3.360294]\n",
      "epoch:19 step:15099 [D loss: 0.049566, acc.: 99.22%] [G loss: 2.999128]\n",
      "epoch:19 step:15100 [D loss: 0.110765, acc.: 98.44%] [G loss: 2.534981]\n",
      "epoch:19 step:15101 [D loss: 0.036440, acc.: 100.00%] [G loss: 2.974793]\n",
      "epoch:19 step:15102 [D loss: 0.035427, acc.: 100.00%] [G loss: 3.910631]\n",
      "epoch:19 step:15103 [D loss: 0.100251, acc.: 99.22%] [G loss: 2.902929]\n",
      "epoch:19 step:15104 [D loss: 0.056678, acc.: 100.00%] [G loss: 4.085370]\n",
      "epoch:19 step:15105 [D loss: 0.059091, acc.: 100.00%] [G loss: 3.395103]\n",
      "epoch:19 step:15106 [D loss: 0.594845, acc.: 67.19%] [G loss: 3.656046]\n",
      "epoch:19 step:15107 [D loss: 0.005946, acc.: 100.00%] [G loss: 5.295062]\n",
      "epoch:19 step:15108 [D loss: 0.228035, acc.: 92.97%] [G loss: 3.083300]\n",
      "epoch:19 step:15109 [D loss: 0.082987, acc.: 99.22%] [G loss: 3.781353]\n",
      "epoch:19 step:15110 [D loss: 0.016499, acc.: 100.00%] [G loss: 4.248234]\n",
      "epoch:19 step:15111 [D loss: 0.013102, acc.: 100.00%] [G loss: 4.084361]\n",
      "epoch:19 step:15112 [D loss: 0.180488, acc.: 89.84%] [G loss: 3.739079]\n",
      "epoch:19 step:15113 [D loss: 1.063366, acc.: 60.16%] [G loss: 6.622903]\n",
      "epoch:19 step:15114 [D loss: 1.760399, acc.: 50.00%] [G loss: 4.854580]\n",
      "epoch:19 step:15115 [D loss: 0.297010, acc.: 82.81%] [G loss: 3.547716]\n",
      "epoch:19 step:15116 [D loss: 0.190029, acc.: 93.75%] [G loss: 2.076413]\n",
      "epoch:19 step:15117 [D loss: 0.088839, acc.: 98.44%] [G loss: 1.642325]\n",
      "epoch:19 step:15118 [D loss: 0.033134, acc.: 100.00%] [G loss: 3.337092]\n",
      "epoch:19 step:15119 [D loss: 0.075704, acc.: 98.44%] [G loss: 3.040092]\n",
      "epoch:19 step:15120 [D loss: 0.019040, acc.: 100.00%] [G loss: 2.814814]\n",
      "epoch:19 step:15121 [D loss: 0.175505, acc.: 92.97%] [G loss: 1.322508]\n",
      "epoch:19 step:15122 [D loss: 0.311443, acc.: 85.94%] [G loss: 4.301328]\n",
      "epoch:19 step:15123 [D loss: 0.042687, acc.: 99.22%] [G loss: 4.812607]\n",
      "epoch:19 step:15124 [D loss: 0.187141, acc.: 92.19%] [G loss: 2.723643]\n",
      "epoch:19 step:15125 [D loss: 0.040563, acc.: 100.00%] [G loss: 1.992071]\n",
      "epoch:19 step:15126 [D loss: 0.043793, acc.: 99.22%] [G loss: 1.632158]\n",
      "epoch:19 step:15127 [D loss: 0.049410, acc.: 100.00%] [G loss: 1.070150]\n",
      "epoch:19 step:15128 [D loss: 0.084549, acc.: 98.44%] [G loss: 1.223500]\n",
      "epoch:19 step:15129 [D loss: 0.067769, acc.: 99.22%] [G loss: 1.995332]\n",
      "epoch:19 step:15130 [D loss: 0.034678, acc.: 100.00%] [G loss: 2.366422]\n",
      "epoch:19 step:15131 [D loss: 0.184374, acc.: 93.75%] [G loss: 1.393416]\n",
      "epoch:19 step:15132 [D loss: 0.057193, acc.: 100.00%] [G loss: 2.400863]\n",
      "epoch:19 step:15133 [D loss: 0.068901, acc.: 100.00%] [G loss: 2.825783]\n",
      "epoch:19 step:15134 [D loss: 1.007329, acc.: 45.31%] [G loss: 5.229601]\n",
      "epoch:19 step:15135 [D loss: 0.090933, acc.: 97.66%] [G loss: 5.875328]\n",
      "epoch:19 step:15136 [D loss: 0.511016, acc.: 77.34%] [G loss: 3.188066]\n",
      "epoch:19 step:15137 [D loss: 0.420220, acc.: 79.69%] [G loss: 5.450418]\n",
      "epoch:19 step:15138 [D loss: 0.090905, acc.: 97.66%] [G loss: 5.323252]\n",
      "epoch:19 step:15139 [D loss: 0.273268, acc.: 87.50%] [G loss: 4.225090]\n",
      "epoch:19 step:15140 [D loss: 0.018710, acc.: 100.00%] [G loss: 3.352787]\n",
      "epoch:19 step:15141 [D loss: 0.102776, acc.: 95.31%] [G loss: 4.397708]\n",
      "epoch:19 step:15142 [D loss: 0.035346, acc.: 99.22%] [G loss: 4.290269]\n",
      "epoch:19 step:15143 [D loss: 0.041872, acc.: 100.00%] [G loss: 4.279563]\n",
      "epoch:19 step:15144 [D loss: 0.018550, acc.: 100.00%] [G loss: 4.367949]\n",
      "epoch:19 step:15145 [D loss: 0.009950, acc.: 100.00%] [G loss: 4.129329]\n",
      "epoch:19 step:15146 [D loss: 0.017902, acc.: 100.00%] [G loss: 3.590239]\n",
      "epoch:19 step:15147 [D loss: 0.014582, acc.: 100.00%] [G loss: 3.451265]\n",
      "epoch:19 step:15148 [D loss: 0.142354, acc.: 95.31%] [G loss: 1.244713]\n",
      "epoch:19 step:15149 [D loss: 0.643262, acc.: 64.06%] [G loss: 6.191939]\n",
      "epoch:19 step:15150 [D loss: 0.782433, acc.: 60.94%] [G loss: 4.482111]\n",
      "epoch:19 step:15151 [D loss: 0.206843, acc.: 89.84%] [G loss: 2.687689]\n",
      "epoch:19 step:15152 [D loss: 0.100241, acc.: 96.09%] [G loss: 3.172400]\n",
      "epoch:19 step:15153 [D loss: 0.041076, acc.: 100.00%] [G loss: 3.662301]\n",
      "epoch:19 step:15154 [D loss: 0.094559, acc.: 99.22%] [G loss: 2.863499]\n",
      "epoch:19 step:15155 [D loss: 0.026413, acc.: 100.00%] [G loss: 2.553654]\n",
      "epoch:19 step:15156 [D loss: 0.022699, acc.: 100.00%] [G loss: 2.502328]\n",
      "epoch:19 step:15157 [D loss: 0.017833, acc.: 100.00%] [G loss: 2.655360]\n",
      "epoch:19 step:15158 [D loss: 0.022570, acc.: 100.00%] [G loss: 2.572052]\n",
      "epoch:19 step:15159 [D loss: 0.193982, acc.: 93.75%] [G loss: 2.401479]\n",
      "epoch:19 step:15160 [D loss: 0.010676, acc.: 100.00%] [G loss: 3.529144]\n",
      "epoch:19 step:15161 [D loss: 0.011005, acc.: 100.00%] [G loss: 3.152750]\n",
      "epoch:19 step:15162 [D loss: 0.034810, acc.: 100.00%] [G loss: 1.915770]\n",
      "epoch:19 step:15163 [D loss: 0.158978, acc.: 93.75%] [G loss: 3.549533]\n",
      "epoch:19 step:15164 [D loss: 0.063051, acc.: 99.22%] [G loss: 3.258280]\n",
      "epoch:19 step:15165 [D loss: 0.026438, acc.: 99.22%] [G loss: 2.180296]\n",
      "epoch:19 step:15166 [D loss: 0.022891, acc.: 100.00%] [G loss: 1.867778]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:19 step:15167 [D loss: 0.063649, acc.: 99.22%] [G loss: 3.232868]\n",
      "epoch:19 step:15168 [D loss: 0.025238, acc.: 100.00%] [G loss: 3.861804]\n",
      "epoch:19 step:15169 [D loss: 0.163117, acc.: 95.31%] [G loss: 2.042367]\n",
      "epoch:19 step:15170 [D loss: 0.164643, acc.: 96.88%] [G loss: 4.984522]\n",
      "epoch:19 step:15171 [D loss: 0.100753, acc.: 97.66%] [G loss: 4.661576]\n",
      "epoch:19 step:15172 [D loss: 0.040969, acc.: 99.22%] [G loss: 3.810627]\n",
      "epoch:19 step:15173 [D loss: 0.061770, acc.: 99.22%] [G loss: 4.368204]\n",
      "epoch:19 step:15174 [D loss: 0.043562, acc.: 99.22%] [G loss: 3.497174]\n",
      "epoch:19 step:15175 [D loss: 1.496263, acc.: 39.06%] [G loss: 7.146671]\n",
      "epoch:19 step:15176 [D loss: 1.284712, acc.: 50.78%] [G loss: 4.536873]\n",
      "epoch:19 step:15177 [D loss: 1.055032, acc.: 50.00%] [G loss: 4.044204]\n",
      "epoch:19 step:15178 [D loss: 0.035313, acc.: 99.22%] [G loss: 5.005609]\n",
      "epoch:19 step:15179 [D loss: 0.452213, acc.: 76.56%] [G loss: 2.619108]\n",
      "epoch:19 step:15180 [D loss: 0.201645, acc.: 91.41%] [G loss: 3.484642]\n",
      "epoch:19 step:15181 [D loss: 0.025287, acc.: 100.00%] [G loss: 3.507255]\n",
      "epoch:19 step:15182 [D loss: 0.073728, acc.: 96.88%] [G loss: 2.558839]\n",
      "epoch:19 step:15183 [D loss: 0.028929, acc.: 100.00%] [G loss: 3.229391]\n",
      "epoch:19 step:15184 [D loss: 0.006905, acc.: 100.00%] [G loss: 1.761523]\n",
      "epoch:19 step:15185 [D loss: 0.040887, acc.: 99.22%] [G loss: 1.794243]\n",
      "epoch:19 step:15186 [D loss: 0.050572, acc.: 99.22%] [G loss: 1.641439]\n",
      "epoch:19 step:15187 [D loss: 0.140247, acc.: 95.31%] [G loss: 2.989036]\n",
      "epoch:19 step:15188 [D loss: 0.168721, acc.: 93.75%] [G loss: 1.609760]\n",
      "epoch:19 step:15189 [D loss: 0.142854, acc.: 96.09%] [G loss: 0.427495]\n",
      "epoch:19 step:15190 [D loss: 0.433882, acc.: 74.22%] [G loss: 3.877987]\n",
      "epoch:19 step:15191 [D loss: 0.344963, acc.: 85.16%] [G loss: 4.278786]\n",
      "epoch:19 step:15192 [D loss: 0.290648, acc.: 87.50%] [G loss: 3.516444]\n",
      "epoch:19 step:15193 [D loss: 0.061605, acc.: 98.44%] [G loss: 3.353648]\n",
      "epoch:19 step:15194 [D loss: 0.084427, acc.: 98.44%] [G loss: 4.186790]\n",
      "epoch:19 step:15195 [D loss: 0.037773, acc.: 99.22%] [G loss: 3.726148]\n",
      "epoch:19 step:15196 [D loss: 0.018274, acc.: 100.00%] [G loss: 3.548645]\n",
      "epoch:19 step:15197 [D loss: 0.021663, acc.: 100.00%] [G loss: 3.217504]\n",
      "epoch:19 step:15198 [D loss: 0.067427, acc.: 99.22%] [G loss: 3.468799]\n",
      "epoch:19 step:15199 [D loss: 0.020986, acc.: 100.00%] [G loss: 3.321780]\n",
      "epoch:19 step:15200 [D loss: 0.042332, acc.: 99.22%] [G loss: 3.574142]\n",
      "##############\n",
      "[0.99815077 1.05918328 1.11506454 1.00517249 2.10440154 0.99676054\n",
      " 1.09457078 1.0377573  0.93540107 0.98243784]\n",
      "##########\n",
      "epoch:19 step:15201 [D loss: 0.022261, acc.: 100.00%] [G loss: 2.888362]\n",
      "epoch:19 step:15202 [D loss: 0.042398, acc.: 100.00%] [G loss: 3.305735]\n",
      "epoch:19 step:15203 [D loss: 0.591571, acc.: 70.31%] [G loss: 3.442332]\n",
      "epoch:19 step:15204 [D loss: 0.013772, acc.: 100.00%] [G loss: 4.477214]\n",
      "epoch:19 step:15205 [D loss: 0.027433, acc.: 100.00%] [G loss: 4.718062]\n",
      "epoch:19 step:15206 [D loss: 0.017754, acc.: 100.00%] [G loss: 4.219466]\n",
      "epoch:19 step:15207 [D loss: 0.025114, acc.: 100.00%] [G loss: 4.033558]\n",
      "epoch:19 step:15208 [D loss: 0.036116, acc.: 100.00%] [G loss: 4.650663]\n",
      "epoch:19 step:15209 [D loss: 0.039039, acc.: 100.00%] [G loss: 4.121069]\n",
      "epoch:19 step:15210 [D loss: 0.013003, acc.: 100.00%] [G loss: 3.342844]\n",
      "epoch:19 step:15211 [D loss: 0.019976, acc.: 100.00%] [G loss: 3.623638]\n",
      "epoch:19 step:15212 [D loss: 0.017703, acc.: 100.00%] [G loss: 1.958925]\n",
      "epoch:19 step:15213 [D loss: 0.066206, acc.: 99.22%] [G loss: 3.879194]\n",
      "epoch:19 step:15214 [D loss: 0.041075, acc.: 99.22%] [G loss: 3.308590]\n",
      "epoch:19 step:15215 [D loss: 0.042307, acc.: 100.00%] [G loss: 2.115149]\n",
      "epoch:19 step:15216 [D loss: 0.021750, acc.: 100.00%] [G loss: 2.745170]\n",
      "epoch:19 step:15217 [D loss: 0.050470, acc.: 100.00%] [G loss: 2.046091]\n",
      "epoch:19 step:15218 [D loss: 0.013794, acc.: 100.00%] [G loss: 4.397471]\n",
      "epoch:19 step:15219 [D loss: 0.028313, acc.: 100.00%] [G loss: 1.128437]\n",
      "epoch:19 step:15220 [D loss: 0.043549, acc.: 100.00%] [G loss: 1.985852]\n",
      "epoch:19 step:15221 [D loss: 0.215878, acc.: 89.84%] [G loss: 0.411401]\n",
      "epoch:19 step:15222 [D loss: 1.145275, acc.: 54.69%] [G loss: 6.740911]\n",
      "epoch:19 step:15223 [D loss: 2.334116, acc.: 50.00%] [G loss: 5.605257]\n",
      "epoch:19 step:15224 [D loss: 1.377622, acc.: 51.56%] [G loss: 3.567785]\n",
      "epoch:19 step:15225 [D loss: 0.204289, acc.: 92.19%] [G loss: 2.221518]\n",
      "epoch:19 step:15226 [D loss: 0.096659, acc.: 98.44%] [G loss: 2.410339]\n",
      "epoch:19 step:15227 [D loss: 0.088514, acc.: 100.00%] [G loss: 2.600650]\n",
      "epoch:19 step:15228 [D loss: 0.069032, acc.: 100.00%] [G loss: 2.635730]\n",
      "epoch:19 step:15229 [D loss: 0.060157, acc.: 100.00%] [G loss: 2.702846]\n",
      "epoch:19 step:15230 [D loss: 0.131777, acc.: 98.44%] [G loss: 2.729389]\n",
      "epoch:19 step:15231 [D loss: 0.056172, acc.: 100.00%] [G loss: 2.512072]\n",
      "epoch:19 step:15232 [D loss: 0.096631, acc.: 98.44%] [G loss: 2.111040]\n",
      "epoch:19 step:15233 [D loss: 0.110369, acc.: 97.66%] [G loss: 2.218951]\n",
      "epoch:19 step:15234 [D loss: 0.077929, acc.: 100.00%] [G loss: 2.169801]\n",
      "epoch:19 step:15235 [D loss: 0.438389, acc.: 79.69%] [G loss: 3.358400]\n",
      "epoch:19 step:15236 [D loss: 0.082692, acc.: 99.22%] [G loss: 3.131969]\n",
      "epoch:19 step:15237 [D loss: 0.038591, acc.: 100.00%] [G loss: 2.951859]\n",
      "epoch:19 step:15238 [D loss: 0.099384, acc.: 97.66%] [G loss: 2.308778]\n",
      "epoch:19 step:15239 [D loss: 0.063763, acc.: 100.00%] [G loss: 3.989697]\n",
      "epoch:19 step:15240 [D loss: 0.084970, acc.: 98.44%] [G loss: 3.206923]\n",
      "epoch:19 step:15241 [D loss: 0.076336, acc.: 100.00%] [G loss: 3.231772]\n",
      "epoch:19 step:15242 [D loss: 0.142120, acc.: 96.88%] [G loss: 1.789551]\n",
      "epoch:19 step:15243 [D loss: 0.045096, acc.: 100.00%] [G loss: 2.020966]\n",
      "epoch:19 step:15244 [D loss: 0.030242, acc.: 100.00%] [G loss: 1.826404]\n",
      "epoch:19 step:15245 [D loss: 0.049063, acc.: 100.00%] [G loss: 1.753808]\n",
      "epoch:19 step:15246 [D loss: 0.033540, acc.: 100.00%] [G loss: 2.831023]\n",
      "epoch:19 step:15247 [D loss: 0.057845, acc.: 99.22%] [G loss: 2.326670]\n",
      "epoch:19 step:15248 [D loss: 0.215953, acc.: 92.97%] [G loss: 1.176231]\n",
      "epoch:19 step:15249 [D loss: 0.015608, acc.: 100.00%] [G loss: 1.666171]\n",
      "epoch:19 step:15250 [D loss: 0.086178, acc.: 99.22%] [G loss: 2.009025]\n",
      "epoch:19 step:15251 [D loss: 0.045077, acc.: 100.00%] [G loss: 2.912215]\n",
      "epoch:19 step:15252 [D loss: 0.061002, acc.: 99.22%] [G loss: 1.991088]\n",
      "epoch:19 step:15253 [D loss: 0.036416, acc.: 100.00%] [G loss: 2.154524]\n",
      "epoch:19 step:15254 [D loss: 0.057289, acc.: 98.44%] [G loss: 3.017829]\n",
      "epoch:19 step:15255 [D loss: 0.042087, acc.: 100.00%] [G loss: 4.108690]\n",
      "epoch:19 step:15256 [D loss: 0.102651, acc.: 96.09%] [G loss: 2.918342]\n",
      "epoch:19 step:15257 [D loss: 0.059841, acc.: 98.44%] [G loss: 3.796825]\n",
      "epoch:19 step:15258 [D loss: 0.021184, acc.: 99.22%] [G loss: 4.151311]\n",
      "epoch:19 step:15259 [D loss: 0.416404, acc.: 80.47%] [G loss: 2.525966]\n",
      "epoch:19 step:15260 [D loss: 0.111639, acc.: 96.88%] [G loss: 3.903614]\n",
      "epoch:19 step:15261 [D loss: 0.010288, acc.: 100.00%] [G loss: 4.553467]\n",
      "epoch:19 step:15262 [D loss: 0.023340, acc.: 100.00%] [G loss: 4.942043]\n",
      "epoch:19 step:15263 [D loss: 0.114466, acc.: 95.31%] [G loss: 3.522048]\n",
      "epoch:19 step:15264 [D loss: 0.039022, acc.: 100.00%] [G loss: 3.468772]\n",
      "epoch:19 step:15265 [D loss: 0.027207, acc.: 100.00%] [G loss: 3.255669]\n",
      "epoch:19 step:15266 [D loss: 0.024367, acc.: 100.00%] [G loss: 3.753966]\n",
      "epoch:19 step:15267 [D loss: 0.037937, acc.: 100.00%] [G loss: 3.109093]\n",
      "epoch:19 step:15268 [D loss: 0.021186, acc.: 100.00%] [G loss: 4.088748]\n",
      "epoch:19 step:15269 [D loss: 0.046504, acc.: 100.00%] [G loss: 4.279114]\n",
      "epoch:19 step:15270 [D loss: 0.661530, acc.: 64.06%] [G loss: 5.214585]\n",
      "epoch:19 step:15271 [D loss: 0.037227, acc.: 99.22%] [G loss: 5.738839]\n",
      "epoch:19 step:15272 [D loss: 0.750780, acc.: 64.84%] [G loss: 3.021169]\n",
      "epoch:19 step:15273 [D loss: 0.091114, acc.: 98.44%] [G loss: 2.247278]\n",
      "epoch:19 step:15274 [D loss: 0.023664, acc.: 100.00%] [G loss: 4.259547]\n",
      "epoch:19 step:15275 [D loss: 0.017790, acc.: 100.00%] [G loss: 2.101699]\n",
      "epoch:19 step:15276 [D loss: 0.025719, acc.: 100.00%] [G loss: 3.653560]\n",
      "epoch:19 step:15277 [D loss: 0.079315, acc.: 97.66%] [G loss: 4.271645]\n",
      "epoch:19 step:15278 [D loss: 0.022605, acc.: 100.00%] [G loss: 3.313401]\n",
      "epoch:19 step:15279 [D loss: 0.032509, acc.: 100.00%] [G loss: 4.680315]\n",
      "epoch:19 step:15280 [D loss: 0.078292, acc.: 99.22%] [G loss: 3.249799]\n",
      "epoch:19 step:15281 [D loss: 0.032634, acc.: 100.00%] [G loss: 4.533860]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:19 step:15282 [D loss: 0.037409, acc.: 100.00%] [G loss: 2.973965]\n",
      "epoch:19 step:15283 [D loss: 0.018522, acc.: 100.00%] [G loss: 3.514227]\n",
      "epoch:19 step:15284 [D loss: 0.025771, acc.: 100.00%] [G loss: 2.166578]\n",
      "epoch:19 step:15285 [D loss: 0.027194, acc.: 100.00%] [G loss: 2.426475]\n",
      "epoch:19 step:15286 [D loss: 0.103141, acc.: 98.44%] [G loss: 2.737190]\n",
      "epoch:19 step:15287 [D loss: 0.508698, acc.: 76.56%] [G loss: 6.023208]\n",
      "epoch:19 step:15288 [D loss: 0.073286, acc.: 96.88%] [G loss: 6.030662]\n",
      "epoch:19 step:15289 [D loss: 0.031892, acc.: 99.22%] [G loss: 5.557093]\n",
      "epoch:19 step:15290 [D loss: 0.125988, acc.: 94.53%] [G loss: 4.201800]\n",
      "epoch:19 step:15291 [D loss: 0.078223, acc.: 96.88%] [G loss: 2.871337]\n",
      "epoch:19 step:15292 [D loss: 0.029736, acc.: 100.00%] [G loss: 4.416366]\n",
      "epoch:19 step:15293 [D loss: 0.010729, acc.: 100.00%] [G loss: 4.828485]\n",
      "epoch:19 step:15294 [D loss: 0.004481, acc.: 100.00%] [G loss: 4.992444]\n",
      "epoch:19 step:15295 [D loss: 0.020663, acc.: 100.00%] [G loss: 3.534400]\n",
      "epoch:19 step:15296 [D loss: 0.011570, acc.: 100.00%] [G loss: 3.963063]\n",
      "epoch:19 step:15297 [D loss: 0.009741, acc.: 100.00%] [G loss: 4.600782]\n",
      "epoch:19 step:15298 [D loss: 0.163478, acc.: 95.31%] [G loss: 1.174165]\n",
      "epoch:19 step:15299 [D loss: 0.137252, acc.: 96.09%] [G loss: 2.931197]\n",
      "epoch:19 step:15300 [D loss: 0.015625, acc.: 100.00%] [G loss: 4.032787]\n",
      "epoch:19 step:15301 [D loss: 0.082776, acc.: 98.44%] [G loss: 4.499435]\n",
      "epoch:19 step:15302 [D loss: 0.041630, acc.: 99.22%] [G loss: 4.589214]\n",
      "epoch:19 step:15303 [D loss: 0.098110, acc.: 98.44%] [G loss: 4.008375]\n",
      "epoch:19 step:15304 [D loss: 0.050650, acc.: 100.00%] [G loss: 3.500760]\n",
      "epoch:19 step:15305 [D loss: 0.011822, acc.: 100.00%] [G loss: 3.667425]\n",
      "epoch:19 step:15306 [D loss: 0.149138, acc.: 96.09%] [G loss: 4.080337]\n",
      "epoch:19 step:15307 [D loss: 0.044603, acc.: 99.22%] [G loss: 4.131773]\n",
      "epoch:19 step:15308 [D loss: 0.008780, acc.: 100.00%] [G loss: 5.580241]\n",
      "epoch:19 step:15309 [D loss: 0.015039, acc.: 100.00%] [G loss: 4.186726]\n",
      "epoch:19 step:15310 [D loss: 0.085529, acc.: 98.44%] [G loss: 3.701591]\n",
      "epoch:19 step:15311 [D loss: 0.044685, acc.: 99.22%] [G loss: 3.739096]\n",
      "epoch:19 step:15312 [D loss: 0.015479, acc.: 100.00%] [G loss: 5.037773]\n",
      "epoch:19 step:15313 [D loss: 0.092130, acc.: 98.44%] [G loss: 3.166211]\n",
      "epoch:19 step:15314 [D loss: 0.030915, acc.: 100.00%] [G loss: 4.415762]\n",
      "epoch:19 step:15315 [D loss: 0.015870, acc.: 100.00%] [G loss: 4.162564]\n",
      "epoch:19 step:15316 [D loss: 0.018797, acc.: 100.00%] [G loss: 4.685215]\n",
      "epoch:19 step:15317 [D loss: 0.033856, acc.: 100.00%] [G loss: 4.223493]\n",
      "epoch:19 step:15318 [D loss: 0.015439, acc.: 100.00%] [G loss: 3.436003]\n",
      "epoch:19 step:15319 [D loss: 0.055447, acc.: 98.44%] [G loss: 2.954548]\n",
      "epoch:19 step:15320 [D loss: 0.045017, acc.: 100.00%] [G loss: 4.455501]\n",
      "epoch:19 step:15321 [D loss: 0.115661, acc.: 99.22%] [G loss: 3.438029]\n",
      "epoch:19 step:15322 [D loss: 0.128262, acc.: 96.09%] [G loss: 6.808659]\n",
      "epoch:19 step:15323 [D loss: 0.180806, acc.: 90.62%] [G loss: 3.161514]\n",
      "epoch:19 step:15324 [D loss: 0.402165, acc.: 78.91%] [G loss: 8.704332]\n",
      "epoch:19 step:15325 [D loss: 1.620951, acc.: 52.34%] [G loss: 4.098663]\n",
      "epoch:19 step:15326 [D loss: 0.053113, acc.: 98.44%] [G loss: 3.200042]\n",
      "epoch:19 step:15327 [D loss: 0.038541, acc.: 99.22%] [G loss: 2.553272]\n",
      "epoch:19 step:15328 [D loss: 0.048711, acc.: 99.22%] [G loss: 3.908646]\n",
      "epoch:19 step:15329 [D loss: 0.045215, acc.: 100.00%] [G loss: 3.562906]\n",
      "epoch:19 step:15330 [D loss: 0.059000, acc.: 100.00%] [G loss: 3.674873]\n",
      "epoch:19 step:15331 [D loss: 3.867006, acc.: 25.78%] [G loss: 7.626410]\n",
      "epoch:19 step:15332 [D loss: 1.169162, acc.: 56.25%] [G loss: 6.252985]\n",
      "epoch:19 step:15333 [D loss: 0.215104, acc.: 89.84%] [G loss: 4.632378]\n",
      "epoch:19 step:15334 [D loss: 0.032407, acc.: 100.00%] [G loss: 4.394323]\n",
      "epoch:19 step:15335 [D loss: 0.010042, acc.: 100.00%] [G loss: 3.693923]\n",
      "epoch:19 step:15336 [D loss: 0.081292, acc.: 96.88%] [G loss: 4.035546]\n",
      "epoch:19 step:15337 [D loss: 0.035156, acc.: 100.00%] [G loss: 3.995191]\n",
      "epoch:19 step:15338 [D loss: 0.091709, acc.: 98.44%] [G loss: 2.406284]\n",
      "epoch:19 step:15339 [D loss: 0.056536, acc.: 100.00%] [G loss: 2.885230]\n",
      "epoch:19 step:15340 [D loss: 0.079679, acc.: 99.22%] [G loss: 4.019730]\n",
      "epoch:19 step:15341 [D loss: 0.025658, acc.: 100.00%] [G loss: 4.160855]\n",
      "epoch:19 step:15342 [D loss: 0.014756, acc.: 100.00%] [G loss: 3.267364]\n",
      "epoch:19 step:15343 [D loss: 0.035238, acc.: 99.22%] [G loss: 3.284629]\n",
      "epoch:19 step:15344 [D loss: 0.046790, acc.: 100.00%] [G loss: 3.173841]\n",
      "epoch:19 step:15345 [D loss: 0.029821, acc.: 100.00%] [G loss: 2.971445]\n",
      "epoch:19 step:15346 [D loss: 0.096441, acc.: 99.22%] [G loss: 3.827746]\n",
      "epoch:19 step:15347 [D loss: 0.247337, acc.: 92.19%] [G loss: 2.193939]\n",
      "epoch:19 step:15348 [D loss: 0.791729, acc.: 61.72%] [G loss: 4.558577]\n",
      "epoch:19 step:15349 [D loss: 0.017900, acc.: 100.00%] [G loss: 6.095567]\n",
      "epoch:19 step:15350 [D loss: 0.788425, acc.: 59.38%] [G loss: 1.137544]\n",
      "epoch:19 step:15351 [D loss: 0.859822, acc.: 65.62%] [G loss: 6.314121]\n",
      "epoch:19 step:15352 [D loss: 0.176583, acc.: 92.19%] [G loss: 6.488081]\n",
      "epoch:19 step:15353 [D loss: 0.652142, acc.: 68.75%] [G loss: 2.818465]\n",
      "epoch:19 step:15354 [D loss: 0.219505, acc.: 89.84%] [G loss: 2.767376]\n",
      "epoch:19 step:15355 [D loss: 0.019160, acc.: 100.00%] [G loss: 3.962439]\n",
      "epoch:19 step:15356 [D loss: 0.033564, acc.: 100.00%] [G loss: 3.633675]\n",
      "epoch:19 step:15357 [D loss: 0.254264, acc.: 87.50%] [G loss: 1.843065]\n",
      "epoch:19 step:15358 [D loss: 0.037230, acc.: 99.22%] [G loss: 0.519088]\n",
      "epoch:19 step:15359 [D loss: 0.157824, acc.: 96.09%] [G loss: 3.567431]\n",
      "epoch:19 step:15360 [D loss: 0.027467, acc.: 100.00%] [G loss: 4.229618]\n",
      "epoch:19 step:15361 [D loss: 0.114251, acc.: 97.66%] [G loss: 2.662028]\n",
      "epoch:19 step:15362 [D loss: 0.233091, acc.: 92.19%] [G loss: 4.113753]\n",
      "epoch:19 step:15363 [D loss: 0.045252, acc.: 100.00%] [G loss: 4.703685]\n",
      "epoch:19 step:15364 [D loss: 0.136504, acc.: 95.31%] [G loss: 2.179946]\n",
      "epoch:19 step:15365 [D loss: 0.201628, acc.: 94.53%] [G loss: 1.849252]\n",
      "epoch:19 step:15366 [D loss: 0.065499, acc.: 99.22%] [G loss: 4.326933]\n",
      "epoch:19 step:15367 [D loss: 0.032855, acc.: 100.00%] [G loss: 4.034074]\n",
      "epoch:19 step:15368 [D loss: 0.057576, acc.: 99.22%] [G loss: 2.709210]\n",
      "epoch:19 step:15369 [D loss: 0.071674, acc.: 99.22%] [G loss: 3.798009]\n",
      "epoch:19 step:15370 [D loss: 0.082177, acc.: 99.22%] [G loss: 4.481719]\n",
      "epoch:19 step:15371 [D loss: 0.121100, acc.: 97.66%] [G loss: 3.550470]\n",
      "epoch:19 step:15372 [D loss: 0.248335, acc.: 92.19%] [G loss: 4.024293]\n",
      "epoch:19 step:15373 [D loss: 0.041110, acc.: 100.00%] [G loss: 5.178024]\n",
      "epoch:19 step:15374 [D loss: 0.235163, acc.: 89.06%] [G loss: 2.157192]\n",
      "epoch:19 step:15375 [D loss: 0.175250, acc.: 92.19%] [G loss: 4.380638]\n",
      "epoch:19 step:15376 [D loss: 0.032570, acc.: 99.22%] [G loss: 4.827140]\n",
      "epoch:19 step:15377 [D loss: 0.139134, acc.: 95.31%] [G loss: 3.160169]\n",
      "epoch:19 step:15378 [D loss: 0.048919, acc.: 99.22%] [G loss: 3.375492]\n",
      "epoch:19 step:15379 [D loss: 0.012248, acc.: 100.00%] [G loss: 3.900236]\n",
      "epoch:19 step:15380 [D loss: 0.033034, acc.: 99.22%] [G loss: 2.910208]\n",
      "epoch:19 step:15381 [D loss: 0.076138, acc.: 98.44%] [G loss: 3.579925]\n",
      "epoch:19 step:15382 [D loss: 0.022592, acc.: 100.00%] [G loss: 2.941170]\n",
      "epoch:19 step:15383 [D loss: 0.019385, acc.: 100.00%] [G loss: 2.154812]\n",
      "epoch:19 step:15384 [D loss: 0.033044, acc.: 100.00%] [G loss: 3.809093]\n",
      "epoch:19 step:15385 [D loss: 0.016616, acc.: 100.00%] [G loss: 3.431494]\n",
      "epoch:19 step:15386 [D loss: 0.986685, acc.: 53.91%] [G loss: 6.883211]\n",
      "epoch:19 step:15387 [D loss: 0.917122, acc.: 60.16%] [G loss: 4.978652]\n",
      "epoch:19 step:15388 [D loss: 0.028291, acc.: 100.00%] [G loss: 4.901750]\n",
      "epoch:19 step:15389 [D loss: 0.057984, acc.: 99.22%] [G loss: 3.389051]\n",
      "epoch:19 step:15390 [D loss: 0.034006, acc.: 100.00%] [G loss: 4.114591]\n",
      "epoch:19 step:15391 [D loss: 0.065662, acc.: 100.00%] [G loss: 3.011074]\n",
      "epoch:19 step:15392 [D loss: 0.015507, acc.: 100.00%] [G loss: 3.025023]\n",
      "epoch:19 step:15393 [D loss: 0.100754, acc.: 96.88%] [G loss: 2.247781]\n",
      "epoch:19 step:15394 [D loss: 0.050032, acc.: 100.00%] [G loss: 4.126661]\n",
      "epoch:19 step:15395 [D loss: 0.024689, acc.: 100.00%] [G loss: 2.602629]\n",
      "epoch:19 step:15396 [D loss: 0.026178, acc.: 100.00%] [G loss: 3.475703]\n",
      "epoch:19 step:15397 [D loss: 0.051599, acc.: 100.00%] [G loss: 4.063806]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:19 step:15398 [D loss: 0.057803, acc.: 99.22%] [G loss: 4.639698]\n",
      "epoch:19 step:15399 [D loss: 0.009726, acc.: 100.00%] [G loss: 3.103047]\n",
      "epoch:19 step:15400 [D loss: 0.023113, acc.: 100.00%] [G loss: 3.846010]\n",
      "##############\n",
      "[0.86996811 0.96560591 0.99248501 1.01738855 2.1059945  1.06563621\n",
      " 0.77789493 2.10505956 1.09844606 1.05836735]\n",
      "##########\n",
      "epoch:19 step:15401 [D loss: 0.305349, acc.: 88.28%] [G loss: 5.395786]\n",
      "epoch:19 step:15402 [D loss: 0.021616, acc.: 100.00%] [G loss: 5.855059]\n",
      "epoch:19 step:15403 [D loss: 0.056773, acc.: 99.22%] [G loss: 5.207626]\n",
      "epoch:19 step:15404 [D loss: 0.026370, acc.: 99.22%] [G loss: 3.956970]\n",
      "epoch:19 step:15405 [D loss: 0.125237, acc.: 98.44%] [G loss: 3.476576]\n",
      "epoch:19 step:15406 [D loss: 0.015965, acc.: 100.00%] [G loss: 4.022555]\n",
      "epoch:19 step:15407 [D loss: 0.016105, acc.: 100.00%] [G loss: 4.664990]\n",
      "epoch:19 step:15408 [D loss: 0.048911, acc.: 97.66%] [G loss: 2.879169]\n",
      "epoch:19 step:15409 [D loss: 0.039523, acc.: 100.00%] [G loss: 3.648081]\n",
      "epoch:19 step:15410 [D loss: 0.037950, acc.: 100.00%] [G loss: 4.082817]\n",
      "epoch:19 step:15411 [D loss: 0.006447, acc.: 100.00%] [G loss: 4.255863]\n",
      "epoch:19 step:15412 [D loss: 0.030471, acc.: 99.22%] [G loss: 3.987755]\n",
      "epoch:19 step:15413 [D loss: 0.046238, acc.: 99.22%] [G loss: 4.372923]\n",
      "epoch:19 step:15414 [D loss: 0.257733, acc.: 91.41%] [G loss: 4.963249]\n",
      "epoch:19 step:15415 [D loss: 0.012468, acc.: 100.00%] [G loss: 5.723608]\n",
      "epoch:19 step:15416 [D loss: 0.502645, acc.: 75.78%] [G loss: 2.540442]\n",
      "epoch:19 step:15417 [D loss: 0.158172, acc.: 96.09%] [G loss: 3.933508]\n",
      "epoch:19 step:15418 [D loss: 0.003783, acc.: 100.00%] [G loss: 5.569746]\n",
      "epoch:19 step:15419 [D loss: 0.062373, acc.: 97.66%] [G loss: 4.415644]\n",
      "epoch:19 step:15420 [D loss: 0.013419, acc.: 100.00%] [G loss: 4.175322]\n",
      "epoch:19 step:15421 [D loss: 0.007893, acc.: 100.00%] [G loss: 4.291922]\n",
      "epoch:19 step:15422 [D loss: 0.024254, acc.: 100.00%] [G loss: 3.952999]\n",
      "epoch:19 step:15423 [D loss: 0.028940, acc.: 100.00%] [G loss: 4.267491]\n",
      "epoch:19 step:15424 [D loss: 0.007334, acc.: 100.00%] [G loss: 3.242758]\n",
      "epoch:19 step:15425 [D loss: 0.015075, acc.: 99.22%] [G loss: 4.339799]\n",
      "epoch:19 step:15426 [D loss: 0.036823, acc.: 100.00%] [G loss: 3.498615]\n",
      "epoch:19 step:15427 [D loss: 0.061357, acc.: 99.22%] [G loss: 4.070297]\n",
      "epoch:19 step:15428 [D loss: 0.077518, acc.: 97.66%] [G loss: 2.755521]\n",
      "epoch:19 step:15429 [D loss: 0.102487, acc.: 98.44%] [G loss: 3.886005]\n",
      "epoch:19 step:15430 [D loss: 0.057891, acc.: 98.44%] [G loss: 4.073496]\n",
      "epoch:19 step:15431 [D loss: 0.025954, acc.: 100.00%] [G loss: 4.649368]\n",
      "epoch:19 step:15432 [D loss: 0.123897, acc.: 96.88%] [G loss: 3.220075]\n",
      "epoch:19 step:15433 [D loss: 0.034869, acc.: 99.22%] [G loss: 4.552309]\n",
      "epoch:19 step:15434 [D loss: 0.007457, acc.: 100.00%] [G loss: 3.689118]\n",
      "epoch:19 step:15435 [D loss: 0.042517, acc.: 99.22%] [G loss: 3.451763]\n",
      "epoch:19 step:15436 [D loss: 0.047240, acc.: 100.00%] [G loss: 5.348588]\n",
      "epoch:19 step:15437 [D loss: 0.025432, acc.: 98.44%] [G loss: 4.446817]\n",
      "epoch:19 step:15438 [D loss: 0.542205, acc.: 75.00%] [G loss: 6.758994]\n",
      "epoch:19 step:15439 [D loss: 0.722573, acc.: 66.41%] [G loss: 3.009930]\n",
      "epoch:19 step:15440 [D loss: 0.065904, acc.: 98.44%] [G loss: 4.186020]\n",
      "epoch:19 step:15441 [D loss: 0.012480, acc.: 100.00%] [G loss: 4.541422]\n",
      "epoch:19 step:15442 [D loss: 0.024399, acc.: 100.00%] [G loss: 5.016371]\n",
      "epoch:19 step:15443 [D loss: 0.573241, acc.: 68.75%] [G loss: 5.803616]\n",
      "epoch:19 step:15444 [D loss: 0.125453, acc.: 93.75%] [G loss: 7.242682]\n",
      "epoch:19 step:15445 [D loss: 0.609015, acc.: 70.31%] [G loss: 6.405065]\n",
      "epoch:19 step:15446 [D loss: 0.022027, acc.: 100.00%] [G loss: 7.217010]\n",
      "epoch:19 step:15447 [D loss: 0.129934, acc.: 94.53%] [G loss: 4.488894]\n",
      "epoch:19 step:15448 [D loss: 0.041562, acc.: 100.00%] [G loss: 4.249907]\n",
      "epoch:19 step:15449 [D loss: 0.023741, acc.: 100.00%] [G loss: 4.138421]\n",
      "epoch:19 step:15450 [D loss: 0.007433, acc.: 100.00%] [G loss: 3.439763]\n",
      "epoch:19 step:15451 [D loss: 0.033180, acc.: 99.22%] [G loss: 3.262156]\n",
      "epoch:19 step:15452 [D loss: 0.164699, acc.: 95.31%] [G loss: 5.925586]\n",
      "epoch:19 step:15453 [D loss: 0.059145, acc.: 98.44%] [G loss: 6.471498]\n",
      "epoch:19 step:15454 [D loss: 0.015344, acc.: 100.00%] [G loss: 5.426867]\n",
      "epoch:19 step:15455 [D loss: 0.012359, acc.: 100.00%] [G loss: 4.742014]\n",
      "epoch:19 step:15456 [D loss: 0.060533, acc.: 98.44%] [G loss: 3.500668]\n",
      "epoch:19 step:15457 [D loss: 0.019211, acc.: 100.00%] [G loss: 4.139981]\n",
      "epoch:19 step:15458 [D loss: 0.092471, acc.: 99.22%] [G loss: 4.232368]\n",
      "epoch:19 step:15459 [D loss: 0.020780, acc.: 100.00%] [G loss: 3.852014]\n",
      "epoch:19 step:15460 [D loss: 0.016387, acc.: 100.00%] [G loss: 4.086393]\n",
      "epoch:19 step:15461 [D loss: 0.007305, acc.: 100.00%] [G loss: 4.406776]\n",
      "epoch:19 step:15462 [D loss: 0.239654, acc.: 92.19%] [G loss: 5.453737]\n",
      "epoch:19 step:15463 [D loss: 0.027481, acc.: 99.22%] [G loss: 6.313118]\n",
      "epoch:19 step:15464 [D loss: 0.053555, acc.: 99.22%] [G loss: 4.898688]\n",
      "epoch:19 step:15465 [D loss: 0.021609, acc.: 100.00%] [G loss: 4.679383]\n",
      "epoch:19 step:15466 [D loss: 0.585036, acc.: 71.09%] [G loss: 7.267245]\n",
      "epoch:19 step:15467 [D loss: 0.923275, acc.: 60.94%] [G loss: 3.492302]\n",
      "epoch:19 step:15468 [D loss: 0.396960, acc.: 82.03%] [G loss: 3.880756]\n",
      "epoch:19 step:15469 [D loss: 0.013591, acc.: 100.00%] [G loss: 5.534482]\n",
      "epoch:19 step:15470 [D loss: 0.701765, acc.: 66.41%] [G loss: 4.957439]\n",
      "epoch:19 step:15471 [D loss: 0.054462, acc.: 98.44%] [G loss: 4.982815]\n",
      "epoch:19 step:15472 [D loss: 0.606659, acc.: 70.31%] [G loss: 2.653179]\n",
      "epoch:19 step:15473 [D loss: 0.013250, acc.: 100.00%] [G loss: 4.369452]\n",
      "epoch:19 step:15474 [D loss: 0.032743, acc.: 99.22%] [G loss: 3.940413]\n",
      "epoch:19 step:15475 [D loss: 0.041100, acc.: 99.22%] [G loss: 2.704504]\n",
      "epoch:19 step:15476 [D loss: 0.028069, acc.: 100.00%] [G loss: 3.479437]\n",
      "epoch:19 step:15477 [D loss: 0.014908, acc.: 100.00%] [G loss: 2.628084]\n",
      "epoch:19 step:15478 [D loss: 0.058786, acc.: 99.22%] [G loss: 2.326691]\n",
      "epoch:19 step:15479 [D loss: 0.025290, acc.: 100.00%] [G loss: 2.536245]\n",
      "epoch:19 step:15480 [D loss: 0.081316, acc.: 96.09%] [G loss: 2.439325]\n",
      "epoch:19 step:15481 [D loss: 0.202442, acc.: 93.75%] [G loss: 1.401611]\n",
      "epoch:19 step:15482 [D loss: 0.173451, acc.: 94.53%] [G loss: 2.575531]\n",
      "epoch:19 step:15483 [D loss: 0.004355, acc.: 100.00%] [G loss: 2.020764]\n",
      "epoch:19 step:15484 [D loss: 0.047403, acc.: 100.00%] [G loss: 3.525014]\n",
      "epoch:19 step:15485 [D loss: 4.553387, acc.: 21.09%] [G loss: 8.767675]\n",
      "epoch:19 step:15486 [D loss: 0.767318, acc.: 67.97%] [G loss: 8.185371]\n",
      "epoch:19 step:15487 [D loss: 0.217665, acc.: 91.41%] [G loss: 6.310558]\n",
      "epoch:19 step:15488 [D loss: 0.025636, acc.: 99.22%] [G loss: 5.200584]\n",
      "epoch:19 step:15489 [D loss: 0.014132, acc.: 100.00%] [G loss: 4.163619]\n",
      "epoch:19 step:15490 [D loss: 0.011280, acc.: 100.00%] [G loss: 4.190485]\n",
      "epoch:19 step:15491 [D loss: 0.065532, acc.: 98.44%] [G loss: 3.559346]\n",
      "epoch:19 step:15492 [D loss: 0.048788, acc.: 99.22%] [G loss: 3.299480]\n",
      "epoch:19 step:15493 [D loss: 0.085035, acc.: 96.09%] [G loss: 5.120010]\n",
      "epoch:19 step:15494 [D loss: 0.019300, acc.: 100.00%] [G loss: 5.181616]\n",
      "epoch:19 step:15495 [D loss: 0.019165, acc.: 100.00%] [G loss: 3.323167]\n",
      "epoch:19 step:15496 [D loss: 0.119078, acc.: 95.31%] [G loss: 5.111495]\n",
      "epoch:19 step:15497 [D loss: 0.268713, acc.: 88.28%] [G loss: 3.934393]\n",
      "epoch:19 step:15498 [D loss: 0.052556, acc.: 100.00%] [G loss: 2.785191]\n",
      "epoch:19 step:15499 [D loss: 0.099617, acc.: 97.66%] [G loss: 3.809512]\n",
      "epoch:19 step:15500 [D loss: 0.607123, acc.: 67.19%] [G loss: 6.357124]\n",
      "epoch:19 step:15501 [D loss: 0.229972, acc.: 88.28%] [G loss: 5.087462]\n",
      "epoch:19 step:15502 [D loss: 0.037554, acc.: 100.00%] [G loss: 2.538736]\n",
      "epoch:19 step:15503 [D loss: 0.116703, acc.: 96.09%] [G loss: 4.847958]\n",
      "epoch:19 step:15504 [D loss: 0.116712, acc.: 96.09%] [G loss: 4.030116]\n",
      "epoch:19 step:15505 [D loss: 0.019162, acc.: 100.00%] [G loss: 3.012482]\n",
      "epoch:19 step:15506 [D loss: 0.040505, acc.: 100.00%] [G loss: 4.300744]\n",
      "epoch:19 step:15507 [D loss: 0.066245, acc.: 99.22%] [G loss: 3.724708]\n",
      "epoch:19 step:15508 [D loss: 0.035846, acc.: 100.00%] [G loss: 2.363728]\n",
      "epoch:19 step:15509 [D loss: 0.106644, acc.: 98.44%] [G loss: 3.574162]\n",
      "epoch:19 step:15510 [D loss: 0.021487, acc.: 100.00%] [G loss: 4.640411]\n",
      "epoch:19 step:15511 [D loss: 0.390724, acc.: 85.94%] [G loss: 5.709547]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:19 step:15512 [D loss: 0.270053, acc.: 92.19%] [G loss: 3.057731]\n",
      "epoch:19 step:15513 [D loss: 0.019375, acc.: 100.00%] [G loss: 2.153502]\n",
      "epoch:19 step:15514 [D loss: 0.104961, acc.: 96.88%] [G loss: 4.942595]\n",
      "epoch:19 step:15515 [D loss: 0.010395, acc.: 100.00%] [G loss: 5.416030]\n",
      "epoch:19 step:15516 [D loss: 0.129405, acc.: 94.53%] [G loss: 3.024601]\n",
      "epoch:19 step:15517 [D loss: 0.037931, acc.: 99.22%] [G loss: 3.332294]\n",
      "epoch:19 step:15518 [D loss: 0.020126, acc.: 100.00%] [G loss: 2.859173]\n",
      "epoch:19 step:15519 [D loss: 0.032868, acc.: 99.22%] [G loss: 4.537992]\n",
      "epoch:19 step:15520 [D loss: 0.019685, acc.: 100.00%] [G loss: 5.679526]\n",
      "epoch:19 step:15521 [D loss: 0.042604, acc.: 98.44%] [G loss: 3.261877]\n",
      "epoch:19 step:15522 [D loss: 0.056721, acc.: 100.00%] [G loss: 4.309543]\n",
      "epoch:19 step:15523 [D loss: 0.053005, acc.: 100.00%] [G loss: 4.287227]\n",
      "epoch:19 step:15524 [D loss: 0.650128, acc.: 65.62%] [G loss: 3.153573]\n",
      "epoch:19 step:15525 [D loss: 0.024791, acc.: 100.00%] [G loss: 4.597372]\n",
      "epoch:19 step:15526 [D loss: 0.160132, acc.: 93.75%] [G loss: 3.315589]\n",
      "epoch:19 step:15527 [D loss: 0.070228, acc.: 100.00%] [G loss: 4.594796]\n",
      "epoch:19 step:15528 [D loss: 0.025628, acc.: 99.22%] [G loss: 4.015284]\n",
      "epoch:19 step:15529 [D loss: 0.157778, acc.: 93.75%] [G loss: 2.770834]\n",
      "epoch:19 step:15530 [D loss: 0.021601, acc.: 100.00%] [G loss: 3.191335]\n",
      "epoch:19 step:15531 [D loss: 0.091615, acc.: 99.22%] [G loss: 1.449800]\n",
      "epoch:19 step:15532 [D loss: 0.048141, acc.: 100.00%] [G loss: 3.475846]\n",
      "epoch:19 step:15533 [D loss: 0.014112, acc.: 100.00%] [G loss: 2.212445]\n",
      "epoch:19 step:15534 [D loss: 0.049072, acc.: 100.00%] [G loss: 3.371650]\n",
      "epoch:19 step:15535 [D loss: 0.063484, acc.: 99.22%] [G loss: 2.902557]\n",
      "epoch:19 step:15536 [D loss: 0.005593, acc.: 100.00%] [G loss: 3.309898]\n",
      "epoch:19 step:15537 [D loss: 0.018722, acc.: 100.00%] [G loss: 1.778438]\n",
      "epoch:19 step:15538 [D loss: 0.030224, acc.: 100.00%] [G loss: 2.562614]\n",
      "epoch:19 step:15539 [D loss: 0.117003, acc.: 96.88%] [G loss: 4.611478]\n",
      "epoch:19 step:15540 [D loss: 0.075391, acc.: 97.66%] [G loss: 4.060115]\n",
      "epoch:19 step:15541 [D loss: 0.011152, acc.: 100.00%] [G loss: 3.663184]\n",
      "epoch:19 step:15542 [D loss: 0.013728, acc.: 100.00%] [G loss: 3.500146]\n",
      "epoch:19 step:15543 [D loss: 0.072126, acc.: 98.44%] [G loss: 1.492512]\n",
      "epoch:19 step:15544 [D loss: 0.555704, acc.: 68.75%] [G loss: 8.232823]\n",
      "epoch:19 step:15545 [D loss: 1.167513, acc.: 56.25%] [G loss: 2.273607]\n",
      "epoch:19 step:15546 [D loss: 0.393303, acc.: 79.69%] [G loss: 6.819388]\n",
      "epoch:19 step:15547 [D loss: 0.147876, acc.: 96.09%] [G loss: 6.539846]\n",
      "epoch:19 step:15548 [D loss: 0.078444, acc.: 97.66%] [G loss: 4.222261]\n",
      "epoch:19 step:15549 [D loss: 0.090758, acc.: 97.66%] [G loss: 3.497329]\n",
      "epoch:19 step:15550 [D loss: 0.030065, acc.: 100.00%] [G loss: 4.371280]\n",
      "epoch:19 step:15551 [D loss: 0.018047, acc.: 100.00%] [G loss: 2.500188]\n",
      "epoch:19 step:15552 [D loss: 1.831224, acc.: 31.25%] [G loss: 6.045809]\n",
      "epoch:19 step:15553 [D loss: 0.356127, acc.: 78.91%] [G loss: 5.036127]\n",
      "epoch:19 step:15554 [D loss: 0.841668, acc.: 60.16%] [G loss: 3.406374]\n",
      "epoch:19 step:15555 [D loss: 0.047564, acc.: 100.00%] [G loss: 4.068529]\n",
      "epoch:19 step:15556 [D loss: 0.017997, acc.: 100.00%] [G loss: 4.869205]\n",
      "epoch:19 step:15557 [D loss: 0.053834, acc.: 100.00%] [G loss: 4.108514]\n",
      "epoch:19 step:15558 [D loss: 0.072693, acc.: 98.44%] [G loss: 3.246086]\n",
      "epoch:19 step:15559 [D loss: 0.078521, acc.: 97.66%] [G loss: 4.370956]\n",
      "epoch:19 step:15560 [D loss: 0.071454, acc.: 99.22%] [G loss: 3.963138]\n",
      "epoch:19 step:15561 [D loss: 0.159747, acc.: 93.75%] [G loss: 3.852955]\n",
      "epoch:19 step:15562 [D loss: 0.021543, acc.: 100.00%] [G loss: 3.858714]\n",
      "epoch:19 step:15563 [D loss: 0.161149, acc.: 95.31%] [G loss: 2.177163]\n",
      "epoch:19 step:15564 [D loss: 2.076308, acc.: 25.00%] [G loss: 7.927711]\n",
      "epoch:19 step:15565 [D loss: 0.974385, acc.: 61.72%] [G loss: 5.311861]\n",
      "epoch:19 step:15566 [D loss: 0.201824, acc.: 92.19%] [G loss: 3.527148]\n",
      "epoch:19 step:15567 [D loss: 0.084232, acc.: 96.09%] [G loss: 3.591089]\n",
      "epoch:19 step:15568 [D loss: 0.018414, acc.: 100.00%] [G loss: 4.559812]\n",
      "epoch:19 step:15569 [D loss: 0.033009, acc.: 100.00%] [G loss: 4.239980]\n",
      "epoch:19 step:15570 [D loss: 0.037307, acc.: 99.22%] [G loss: 4.240365]\n",
      "epoch:19 step:15571 [D loss: 0.063871, acc.: 99.22%] [G loss: 3.316383]\n",
      "epoch:19 step:15572 [D loss: 0.082099, acc.: 100.00%] [G loss: 3.608313]\n",
      "epoch:19 step:15573 [D loss: 0.027845, acc.: 100.00%] [G loss: 4.417399]\n",
      "epoch:19 step:15574 [D loss: 0.102198, acc.: 98.44%] [G loss: 3.930683]\n",
      "epoch:19 step:15575 [D loss: 0.305775, acc.: 87.50%] [G loss: 4.960678]\n",
      "epoch:19 step:15576 [D loss: 0.086399, acc.: 96.88%] [G loss: 4.613928]\n",
      "epoch:19 step:15577 [D loss: 0.065936, acc.: 97.66%] [G loss: 3.424450]\n",
      "epoch:19 step:15578 [D loss: 0.190966, acc.: 93.75%] [G loss: 3.920717]\n",
      "epoch:19 step:15579 [D loss: 0.034926, acc.: 100.00%] [G loss: 4.284932]\n",
      "epoch:19 step:15580 [D loss: 0.221016, acc.: 91.41%] [G loss: 2.553992]\n",
      "epoch:19 step:15581 [D loss: 0.104015, acc.: 98.44%] [G loss: 3.819504]\n",
      "epoch:19 step:15582 [D loss: 0.054947, acc.: 100.00%] [G loss: 3.392704]\n",
      "epoch:19 step:15583 [D loss: 0.134906, acc.: 96.09%] [G loss: 5.032575]\n",
      "epoch:19 step:15584 [D loss: 0.021729, acc.: 100.00%] [G loss: 5.042278]\n",
      "epoch:19 step:15585 [D loss: 0.090585, acc.: 98.44%] [G loss: 3.593034]\n",
      "epoch:19 step:15586 [D loss: 0.101052, acc.: 97.66%] [G loss: 4.295597]\n",
      "epoch:19 step:15587 [D loss: 0.092724, acc.: 99.22%] [G loss: 4.710261]\n",
      "epoch:19 step:15588 [D loss: 0.012565, acc.: 100.00%] [G loss: 3.915601]\n",
      "epoch:19 step:15589 [D loss: 0.022228, acc.: 100.00%] [G loss: 3.871668]\n",
      "epoch:19 step:15590 [D loss: 0.017690, acc.: 100.00%] [G loss: 3.202862]\n",
      "epoch:19 step:15591 [D loss: 0.062781, acc.: 98.44%] [G loss: 4.056519]\n",
      "epoch:19 step:15592 [D loss: 0.232321, acc.: 92.97%] [G loss: 4.058949]\n",
      "epoch:19 step:15593 [D loss: 0.089861, acc.: 97.66%] [G loss: 3.665930]\n",
      "epoch:19 step:15594 [D loss: 0.017397, acc.: 100.00%] [G loss: 3.346819]\n",
      "epoch:19 step:15595 [D loss: 0.029208, acc.: 100.00%] [G loss: 4.102211]\n",
      "epoch:19 step:15596 [D loss: 0.016995, acc.: 100.00%] [G loss: 3.947450]\n",
      "epoch:19 step:15597 [D loss: 0.022322, acc.: 100.00%] [G loss: 3.295353]\n",
      "epoch:19 step:15598 [D loss: 0.042869, acc.: 100.00%] [G loss: 4.170426]\n",
      "epoch:19 step:15599 [D loss: 0.259765, acc.: 89.06%] [G loss: 1.921017]\n",
      "epoch:19 step:15600 [D loss: 0.015914, acc.: 100.00%] [G loss: 2.723216]\n",
      "##############\n",
      "[0.96202343 0.88269979 0.89543562 0.96540232 2.11187957 2.12336719\n",
      " 2.10638515 0.86805718 0.98212808 2.11280477]\n",
      "##########\n",
      "epoch:19 step:15601 [D loss: 0.150579, acc.: 95.31%] [G loss: 4.257111]\n",
      "epoch:19 step:15602 [D loss: 0.027824, acc.: 99.22%] [G loss: 6.125124]\n",
      "epoch:19 step:15603 [D loss: 0.148963, acc.: 92.19%] [G loss: 4.183662]\n",
      "epoch:19 step:15604 [D loss: 0.053048, acc.: 98.44%] [G loss: 3.642351]\n",
      "epoch:19 step:15605 [D loss: 0.014136, acc.: 100.00%] [G loss: 4.191557]\n",
      "epoch:19 step:15606 [D loss: 0.012457, acc.: 100.00%] [G loss: 3.600718]\n",
      "epoch:19 step:15607 [D loss: 0.071748, acc.: 98.44%] [G loss: 4.379968]\n",
      "epoch:19 step:15608 [D loss: 0.008176, acc.: 100.00%] [G loss: 5.225907]\n",
      "epoch:19 step:15609 [D loss: 0.070143, acc.: 98.44%] [G loss: 3.324628]\n",
      "epoch:19 step:15610 [D loss: 0.106191, acc.: 98.44%] [G loss: 3.322609]\n",
      "epoch:19 step:15611 [D loss: 0.020180, acc.: 100.00%] [G loss: 3.945589]\n",
      "epoch:19 step:15612 [D loss: 0.011987, acc.: 100.00%] [G loss: 4.383190]\n",
      "epoch:19 step:15613 [D loss: 0.031486, acc.: 100.00%] [G loss: 4.319880]\n",
      "epoch:19 step:15614 [D loss: 0.008611, acc.: 100.00%] [G loss: 3.634862]\n",
      "epoch:19 step:15615 [D loss: 0.187458, acc.: 92.97%] [G loss: 4.181014]\n",
      "epoch:19 step:15616 [D loss: 0.078744, acc.: 98.44%] [G loss: 4.656816]\n",
      "epoch:19 step:15617 [D loss: 0.037621, acc.: 100.00%] [G loss: 4.701820]\n",
      "epoch:19 step:15618 [D loss: 0.070094, acc.: 98.44%] [G loss: 3.898082]\n",
      "epoch:19 step:15619 [D loss: 0.095468, acc.: 95.31%] [G loss: 5.436776]\n",
      "epoch:19 step:15620 [D loss: 0.006481, acc.: 100.00%] [G loss: 6.061091]\n",
      "epoch:20 step:15621 [D loss: 0.335030, acc.: 83.59%] [G loss: 1.707605]\n",
      "epoch:20 step:15622 [D loss: 0.075423, acc.: 100.00%] [G loss: 4.256192]\n",
      "epoch:20 step:15623 [D loss: 0.015093, acc.: 99.22%] [G loss: 5.359175]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:20 step:15624 [D loss: 0.005668, acc.: 100.00%] [G loss: 5.259881]\n",
      "epoch:20 step:15625 [D loss: 0.014291, acc.: 100.00%] [G loss: 4.721572]\n",
      "epoch:20 step:15626 [D loss: 0.025252, acc.: 100.00%] [G loss: 3.860487]\n",
      "epoch:20 step:15627 [D loss: 0.061615, acc.: 97.66%] [G loss: 3.246263]\n",
      "epoch:20 step:15628 [D loss: 0.022011, acc.: 100.00%] [G loss: 3.882555]\n",
      "epoch:20 step:15629 [D loss: 0.048811, acc.: 99.22%] [G loss: 3.739728]\n",
      "epoch:20 step:15630 [D loss: 0.065612, acc.: 97.66%] [G loss: 3.154469]\n",
      "epoch:20 step:15631 [D loss: 0.025760, acc.: 100.00%] [G loss: 2.847436]\n",
      "epoch:20 step:15632 [D loss: 0.037190, acc.: 100.00%] [G loss: 3.856275]\n",
      "epoch:20 step:15633 [D loss: 0.023599, acc.: 100.00%] [G loss: 6.448383]\n",
      "epoch:20 step:15634 [D loss: 0.132709, acc.: 96.88%] [G loss: 4.113082]\n",
      "epoch:20 step:15635 [D loss: 0.012972, acc.: 100.00%] [G loss: 4.289484]\n",
      "epoch:20 step:15636 [D loss: 0.008719, acc.: 100.00%] [G loss: 3.254333]\n",
      "epoch:20 step:15637 [D loss: 0.056373, acc.: 99.22%] [G loss: 4.593985]\n",
      "epoch:20 step:15638 [D loss: 0.018177, acc.: 100.00%] [G loss: 5.442706]\n",
      "epoch:20 step:15639 [D loss: 0.060073, acc.: 98.44%] [G loss: 5.357436]\n",
      "epoch:20 step:15640 [D loss: 0.007770, acc.: 100.00%] [G loss: 5.029073]\n",
      "epoch:20 step:15641 [D loss: 0.039989, acc.: 100.00%] [G loss: 4.993946]\n",
      "epoch:20 step:15642 [D loss: 0.055032, acc.: 98.44%] [G loss: 5.132001]\n",
      "epoch:20 step:15643 [D loss: 0.476020, acc.: 78.12%] [G loss: 8.737362]\n",
      "epoch:20 step:15644 [D loss: 2.285797, acc.: 50.00%] [G loss: 2.629479]\n",
      "epoch:20 step:15645 [D loss: 0.096954, acc.: 96.88%] [G loss: 2.737477]\n",
      "epoch:20 step:15646 [D loss: 0.134392, acc.: 95.31%] [G loss: 4.724528]\n",
      "epoch:20 step:15647 [D loss: 0.045927, acc.: 99.22%] [G loss: 4.687439]\n",
      "epoch:20 step:15648 [D loss: 0.805776, acc.: 63.28%] [G loss: 4.894374]\n",
      "epoch:20 step:15649 [D loss: 0.005026, acc.: 100.00%] [G loss: 6.878798]\n",
      "epoch:20 step:15650 [D loss: 0.620806, acc.: 71.88%] [G loss: 1.048070]\n",
      "epoch:20 step:15651 [D loss: 0.493697, acc.: 74.22%] [G loss: 5.475533]\n",
      "epoch:20 step:15652 [D loss: 0.083032, acc.: 98.44%] [G loss: 6.165530]\n",
      "epoch:20 step:15653 [D loss: 0.335743, acc.: 82.03%] [G loss: 3.682109]\n",
      "epoch:20 step:15654 [D loss: 0.008128, acc.: 100.00%] [G loss: 3.152686]\n",
      "epoch:20 step:15655 [D loss: 0.144498, acc.: 93.75%] [G loss: 4.079498]\n",
      "epoch:20 step:15656 [D loss: 0.032022, acc.: 100.00%] [G loss: 5.066396]\n",
      "epoch:20 step:15657 [D loss: 0.008630, acc.: 100.00%] [G loss: 4.245927]\n",
      "epoch:20 step:15658 [D loss: 0.026059, acc.: 100.00%] [G loss: 4.323723]\n",
      "epoch:20 step:15659 [D loss: 0.017973, acc.: 100.00%] [G loss: 3.694922]\n",
      "epoch:20 step:15660 [D loss: 0.013083, acc.: 100.00%] [G loss: 3.887451]\n",
      "epoch:20 step:15661 [D loss: 0.012045, acc.: 100.00%] [G loss: 4.288724]\n",
      "epoch:20 step:15662 [D loss: 0.012617, acc.: 100.00%] [G loss: 4.064027]\n",
      "epoch:20 step:15663 [D loss: 0.033925, acc.: 99.22%] [G loss: 2.865700]\n",
      "epoch:20 step:15664 [D loss: 0.151518, acc.: 96.88%] [G loss: 3.593859]\n",
      "epoch:20 step:15665 [D loss: 0.121003, acc.: 97.66%] [G loss: 3.207910]\n",
      "epoch:20 step:15666 [D loss: 0.011446, acc.: 100.00%] [G loss: 3.628847]\n",
      "epoch:20 step:15667 [D loss: 0.037931, acc.: 100.00%] [G loss: 3.712796]\n",
      "epoch:20 step:15668 [D loss: 0.015826, acc.: 100.00%] [G loss: 4.297725]\n",
      "epoch:20 step:15669 [D loss: 0.017796, acc.: 100.00%] [G loss: 4.993138]\n",
      "epoch:20 step:15670 [D loss: 0.038513, acc.: 100.00%] [G loss: 4.552648]\n",
      "epoch:20 step:15671 [D loss: 0.011427, acc.: 100.00%] [G loss: 3.658737]\n",
      "epoch:20 step:15672 [D loss: 0.030605, acc.: 100.00%] [G loss: 3.063815]\n",
      "epoch:20 step:15673 [D loss: 0.036766, acc.: 99.22%] [G loss: 4.092494]\n",
      "epoch:20 step:15674 [D loss: 0.085796, acc.: 99.22%] [G loss: 3.205688]\n",
      "epoch:20 step:15675 [D loss: 0.026430, acc.: 100.00%] [G loss: 4.565044]\n",
      "epoch:20 step:15676 [D loss: 0.020717, acc.: 100.00%] [G loss: 4.717382]\n",
      "epoch:20 step:15677 [D loss: 0.025051, acc.: 100.00%] [G loss: 4.118790]\n",
      "epoch:20 step:15678 [D loss: 0.057232, acc.: 99.22%] [G loss: 4.886338]\n",
      "epoch:20 step:15679 [D loss: 0.012318, acc.: 100.00%] [G loss: 4.694135]\n",
      "epoch:20 step:15680 [D loss: 0.004490, acc.: 100.00%] [G loss: 5.020499]\n",
      "epoch:20 step:15681 [D loss: 0.140061, acc.: 95.31%] [G loss: 2.678025]\n",
      "epoch:20 step:15682 [D loss: 0.101117, acc.: 96.88%] [G loss: 4.398226]\n",
      "epoch:20 step:15683 [D loss: 0.042653, acc.: 99.22%] [G loss: 4.604242]\n",
      "epoch:20 step:15684 [D loss: 0.017677, acc.: 100.00%] [G loss: 4.850303]\n",
      "epoch:20 step:15685 [D loss: 0.143698, acc.: 95.31%] [G loss: 2.161758]\n",
      "epoch:20 step:15686 [D loss: 0.102220, acc.: 96.09%] [G loss: 4.432972]\n",
      "epoch:20 step:15687 [D loss: 0.003115, acc.: 100.00%] [G loss: 5.758096]\n",
      "epoch:20 step:15688 [D loss: 0.012761, acc.: 100.00%] [G loss: 5.571391]\n",
      "epoch:20 step:15689 [D loss: 0.086657, acc.: 97.66%] [G loss: 3.972235]\n",
      "epoch:20 step:15690 [D loss: 0.053016, acc.: 99.22%] [G loss: 3.977199]\n",
      "epoch:20 step:15691 [D loss: 0.009235, acc.: 100.00%] [G loss: 3.525338]\n",
      "epoch:20 step:15692 [D loss: 0.007397, acc.: 100.00%] [G loss: 4.809631]\n",
      "epoch:20 step:15693 [D loss: 0.035889, acc.: 100.00%] [G loss: 3.305068]\n",
      "epoch:20 step:15694 [D loss: 0.011100, acc.: 100.00%] [G loss: 3.667913]\n",
      "epoch:20 step:15695 [D loss: 0.127521, acc.: 96.09%] [G loss: 6.052939]\n",
      "epoch:20 step:15696 [D loss: 0.091600, acc.: 96.09%] [G loss: 5.786131]\n",
      "epoch:20 step:15697 [D loss: 0.754925, acc.: 63.28%] [G loss: 5.166980]\n",
      "epoch:20 step:15698 [D loss: 0.007119, acc.: 100.00%] [G loss: 6.688528]\n",
      "epoch:20 step:15699 [D loss: 0.411213, acc.: 81.25%] [G loss: 2.992161]\n",
      "epoch:20 step:15700 [D loss: 0.007526, acc.: 100.00%] [G loss: 2.600464]\n",
      "epoch:20 step:15701 [D loss: 0.044210, acc.: 100.00%] [G loss: 3.221948]\n",
      "epoch:20 step:15702 [D loss: 0.003347, acc.: 100.00%] [G loss: 4.601269]\n",
      "epoch:20 step:15703 [D loss: 0.013690, acc.: 100.00%] [G loss: 2.859213]\n",
      "epoch:20 step:15704 [D loss: 0.024845, acc.: 100.00%] [G loss: 1.873130]\n",
      "epoch:20 step:15705 [D loss: 0.268081, acc.: 85.16%] [G loss: 6.968232]\n",
      "epoch:20 step:15706 [D loss: 0.224490, acc.: 91.41%] [G loss: 6.612005]\n",
      "epoch:20 step:15707 [D loss: 0.006898, acc.: 100.00%] [G loss: 5.940471]\n",
      "epoch:20 step:15708 [D loss: 0.024812, acc.: 100.00%] [G loss: 4.842967]\n",
      "epoch:20 step:15709 [D loss: 0.003484, acc.: 100.00%] [G loss: 4.837106]\n",
      "epoch:20 step:15710 [D loss: 0.272111, acc.: 88.28%] [G loss: 0.180293]\n",
      "epoch:20 step:15711 [D loss: 0.527326, acc.: 76.56%] [G loss: 6.620093]\n",
      "epoch:20 step:15712 [D loss: 0.002881, acc.: 100.00%] [G loss: 7.741892]\n",
      "epoch:20 step:15713 [D loss: 0.281302, acc.: 85.16%] [G loss: 2.557487]\n",
      "epoch:20 step:15714 [D loss: 0.236528, acc.: 89.06%] [G loss: 7.480271]\n",
      "epoch:20 step:15715 [D loss: 0.013503, acc.: 100.00%] [G loss: 7.106286]\n",
      "epoch:20 step:15716 [D loss: 0.886515, acc.: 60.16%] [G loss: 0.397736]\n",
      "epoch:20 step:15717 [D loss: 0.855535, acc.: 63.28%] [G loss: 6.968134]\n",
      "epoch:20 step:15718 [D loss: 0.362458, acc.: 78.91%] [G loss: 6.629157]\n",
      "epoch:20 step:15719 [D loss: 1.220985, acc.: 56.25%] [G loss: 0.403486]\n",
      "epoch:20 step:15720 [D loss: 0.546496, acc.: 75.00%] [G loss: 3.435668]\n",
      "epoch:20 step:15721 [D loss: 0.004709, acc.: 100.00%] [G loss: 5.907323]\n",
      "epoch:20 step:15722 [D loss: 0.015956, acc.: 100.00%] [G loss: 3.187294]\n",
      "epoch:20 step:15723 [D loss: 0.075983, acc.: 96.88%] [G loss: 1.861160]\n",
      "epoch:20 step:15724 [D loss: 0.058994, acc.: 98.44%] [G loss: 1.414280]\n",
      "epoch:20 step:15725 [D loss: 0.072406, acc.: 99.22%] [G loss: 1.142053]\n",
      "epoch:20 step:15726 [D loss: 0.010968, acc.: 100.00%] [G loss: 1.428070]\n",
      "epoch:20 step:15727 [D loss: 0.077138, acc.: 97.66%] [G loss: 2.499168]\n",
      "epoch:20 step:15728 [D loss: 0.074169, acc.: 98.44%] [G loss: 3.222419]\n",
      "epoch:20 step:15729 [D loss: 0.144255, acc.: 96.09%] [G loss: 5.317371]\n",
      "epoch:20 step:15730 [D loss: 0.518778, acc.: 75.78%] [G loss: 0.938675]\n",
      "epoch:20 step:15731 [D loss: 0.334105, acc.: 86.72%] [G loss: 5.284482]\n",
      "epoch:20 step:15732 [D loss: 0.021706, acc.: 100.00%] [G loss: 5.515440]\n",
      "epoch:20 step:15733 [D loss: 0.086092, acc.: 97.66%] [G loss: 5.580177]\n",
      "epoch:20 step:15734 [D loss: 0.048203, acc.: 99.22%] [G loss: 5.888618]\n",
      "epoch:20 step:15735 [D loss: 0.036975, acc.: 100.00%] [G loss: 5.864397]\n",
      "epoch:20 step:15736 [D loss: 0.092647, acc.: 99.22%] [G loss: 4.186263]\n",
      "epoch:20 step:15737 [D loss: 0.028294, acc.: 100.00%] [G loss: 4.599200]\n",
      "epoch:20 step:15738 [D loss: 0.009141, acc.: 100.00%] [G loss: 4.153894]\n",
      "epoch:20 step:15739 [D loss: 0.038261, acc.: 100.00%] [G loss: 4.084540]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:20 step:15740 [D loss: 0.012738, acc.: 100.00%] [G loss: 4.132707]\n",
      "epoch:20 step:15741 [D loss: 0.039456, acc.: 99.22%] [G loss: 4.257093]\n",
      "epoch:20 step:15742 [D loss: 0.021008, acc.: 100.00%] [G loss: 2.855219]\n",
      "epoch:20 step:15743 [D loss: 0.075997, acc.: 98.44%] [G loss: 5.252548]\n",
      "epoch:20 step:15744 [D loss: 0.070235, acc.: 100.00%] [G loss: 4.260298]\n",
      "epoch:20 step:15745 [D loss: 0.069222, acc.: 100.00%] [G loss: 3.139594]\n",
      "epoch:20 step:15746 [D loss: 0.072793, acc.: 98.44%] [G loss: 3.953222]\n",
      "epoch:20 step:15747 [D loss: 0.007157, acc.: 100.00%] [G loss: 3.690086]\n",
      "epoch:20 step:15748 [D loss: 0.095112, acc.: 97.66%] [G loss: 4.199452]\n",
      "epoch:20 step:15749 [D loss: 0.039907, acc.: 98.44%] [G loss: 4.278047]\n",
      "epoch:20 step:15750 [D loss: 0.024477, acc.: 100.00%] [G loss: 4.294176]\n",
      "epoch:20 step:15751 [D loss: 0.023464, acc.: 99.22%] [G loss: 4.345628]\n",
      "epoch:20 step:15752 [D loss: 0.323562, acc.: 85.94%] [G loss: 3.004450]\n",
      "epoch:20 step:15753 [D loss: 0.015432, acc.: 100.00%] [G loss: 4.390301]\n",
      "epoch:20 step:15754 [D loss: 0.047269, acc.: 100.00%] [G loss: 4.097288]\n",
      "epoch:20 step:15755 [D loss: 0.023358, acc.: 100.00%] [G loss: 4.213346]\n",
      "epoch:20 step:15756 [D loss: 0.170712, acc.: 95.31%] [G loss: 3.526011]\n",
      "epoch:20 step:15757 [D loss: 0.023170, acc.: 100.00%] [G loss: 3.524553]\n",
      "epoch:20 step:15758 [D loss: 0.027183, acc.: 99.22%] [G loss: 4.065679]\n",
      "epoch:20 step:15759 [D loss: 0.019061, acc.: 100.00%] [G loss: 5.358652]\n",
      "epoch:20 step:15760 [D loss: 0.055593, acc.: 99.22%] [G loss: 3.077874]\n",
      "epoch:20 step:15761 [D loss: 0.037253, acc.: 100.00%] [G loss: 3.667286]\n",
      "epoch:20 step:15762 [D loss: 0.011790, acc.: 100.00%] [G loss: 4.740750]\n",
      "epoch:20 step:15763 [D loss: 0.037050, acc.: 100.00%] [G loss: 3.267348]\n",
      "epoch:20 step:15764 [D loss: 0.024240, acc.: 100.00%] [G loss: 3.007257]\n",
      "epoch:20 step:15765 [D loss: 0.081342, acc.: 98.44%] [G loss: 3.180285]\n",
      "epoch:20 step:15766 [D loss: 0.059095, acc.: 99.22%] [G loss: 6.191678]\n",
      "epoch:20 step:15767 [D loss: 0.046854, acc.: 98.44%] [G loss: 5.321126]\n",
      "epoch:20 step:15768 [D loss: 0.007242, acc.: 100.00%] [G loss: 3.268102]\n",
      "epoch:20 step:15769 [D loss: 0.013877, acc.: 100.00%] [G loss: 5.058712]\n",
      "epoch:20 step:15770 [D loss: 0.010917, acc.: 100.00%] [G loss: 5.231217]\n",
      "epoch:20 step:15771 [D loss: 0.032361, acc.: 100.00%] [G loss: 2.603358]\n",
      "epoch:20 step:15772 [D loss: 0.046301, acc.: 100.00%] [G loss: 4.651671]\n",
      "epoch:20 step:15773 [D loss: 0.161536, acc.: 96.88%] [G loss: 2.888475]\n",
      "epoch:20 step:15774 [D loss: 0.008628, acc.: 100.00%] [G loss: 4.393503]\n",
      "epoch:20 step:15775 [D loss: 0.027517, acc.: 100.00%] [G loss: 4.329724]\n",
      "epoch:20 step:15776 [D loss: 0.334243, acc.: 84.38%] [G loss: 8.281139]\n",
      "epoch:20 step:15777 [D loss: 0.110699, acc.: 96.09%] [G loss: 9.145691]\n",
      "epoch:20 step:15778 [D loss: 0.160892, acc.: 92.19%] [G loss: 6.328817]\n",
      "epoch:20 step:15779 [D loss: 0.014552, acc.: 100.00%] [G loss: 4.878504]\n",
      "epoch:20 step:15780 [D loss: 0.012917, acc.: 100.00%] [G loss: 5.053654]\n",
      "epoch:20 step:15781 [D loss: 0.003320, acc.: 100.00%] [G loss: 4.556320]\n",
      "epoch:20 step:15782 [D loss: 0.014308, acc.: 100.00%] [G loss: 5.628264]\n",
      "epoch:20 step:15783 [D loss: 0.005927, acc.: 100.00%] [G loss: 4.144240]\n",
      "epoch:20 step:15784 [D loss: 0.002456, acc.: 100.00%] [G loss: 3.445909]\n",
      "epoch:20 step:15785 [D loss: 0.069186, acc.: 96.88%] [G loss: 5.180806]\n",
      "epoch:20 step:15786 [D loss: 0.003912, acc.: 100.00%] [G loss: 6.452502]\n",
      "epoch:20 step:15787 [D loss: 0.007224, acc.: 100.00%] [G loss: 5.472486]\n",
      "epoch:20 step:15788 [D loss: 0.008247, acc.: 100.00%] [G loss: 5.744806]\n",
      "epoch:20 step:15789 [D loss: 0.006969, acc.: 100.00%] [G loss: 4.171706]\n",
      "epoch:20 step:15790 [D loss: 0.022154, acc.: 100.00%] [G loss: 2.657789]\n",
      "epoch:20 step:15791 [D loss: 0.026394, acc.: 99.22%] [G loss: 5.055437]\n",
      "epoch:20 step:15792 [D loss: 0.020687, acc.: 100.00%] [G loss: 6.006700]\n",
      "epoch:20 step:15793 [D loss: 0.013434, acc.: 100.00%] [G loss: 4.536001]\n",
      "epoch:20 step:15794 [D loss: 0.280341, acc.: 88.28%] [G loss: 5.913159]\n",
      "epoch:20 step:15795 [D loss: 0.036237, acc.: 99.22%] [G loss: 7.394110]\n",
      "epoch:20 step:15796 [D loss: 3.526872, acc.: 4.69%] [G loss: 8.217506]\n",
      "epoch:20 step:15797 [D loss: 0.965180, acc.: 60.94%] [G loss: 5.459615]\n",
      "epoch:20 step:15798 [D loss: 0.265472, acc.: 90.62%] [G loss: 2.485436]\n",
      "epoch:20 step:15799 [D loss: 0.076311, acc.: 100.00%] [G loss: 2.467241]\n",
      "epoch:20 step:15800 [D loss: 0.081709, acc.: 98.44%] [G loss: 3.044765]\n",
      "##############\n",
      "[1.05219197 1.10809406 1.04922861 0.90705878 2.13009205 1.02563314\n",
      " 0.8106093  1.12187519 1.10412841 1.03513842]\n",
      "##########\n",
      "epoch:20 step:15801 [D loss: 0.054816, acc.: 99.22%] [G loss: 2.776315]\n",
      "epoch:20 step:15802 [D loss: 0.095636, acc.: 96.88%] [G loss: 1.189929]\n",
      "epoch:20 step:15803 [D loss: 0.178244, acc.: 96.09%] [G loss: 1.907166]\n",
      "epoch:20 step:15804 [D loss: 0.209122, acc.: 92.19%] [G loss: 1.436465]\n",
      "epoch:20 step:15805 [D loss: 0.070745, acc.: 100.00%] [G loss: 2.035876]\n",
      "epoch:20 step:15806 [D loss: 0.041703, acc.: 100.00%] [G loss: 0.878545]\n",
      "epoch:20 step:15807 [D loss: 0.158188, acc.: 96.88%] [G loss: 1.594359]\n",
      "epoch:20 step:15808 [D loss: 0.071963, acc.: 98.44%] [G loss: 1.671610]\n",
      "epoch:20 step:15809 [D loss: 0.610952, acc.: 64.06%] [G loss: 4.934546]\n",
      "epoch:20 step:15810 [D loss: 0.607773, acc.: 70.31%] [G loss: 2.954356]\n",
      "epoch:20 step:15811 [D loss: 0.154301, acc.: 94.53%] [G loss: 2.784652]\n",
      "epoch:20 step:15812 [D loss: 0.027500, acc.: 100.00%] [G loss: 3.608771]\n",
      "epoch:20 step:15813 [D loss: 0.244608, acc.: 89.06%] [G loss: 3.325926]\n",
      "epoch:20 step:15814 [D loss: 0.092305, acc.: 96.88%] [G loss: 3.705744]\n",
      "epoch:20 step:15815 [D loss: 0.096248, acc.: 97.66%] [G loss: 3.870367]\n",
      "epoch:20 step:15816 [D loss: 0.084950, acc.: 98.44%] [G loss: 3.841892]\n",
      "epoch:20 step:15817 [D loss: 0.265955, acc.: 88.28%] [G loss: 2.388450]\n",
      "epoch:20 step:15818 [D loss: 0.108621, acc.: 97.66%] [G loss: 4.512728]\n",
      "epoch:20 step:15819 [D loss: 0.117026, acc.: 97.66%] [G loss: 3.949702]\n",
      "epoch:20 step:15820 [D loss: 0.105230, acc.: 96.09%] [G loss: 3.348582]\n",
      "epoch:20 step:15821 [D loss: 0.027025, acc.: 100.00%] [G loss: 3.716387]\n",
      "epoch:20 step:15822 [D loss: 0.048495, acc.: 100.00%] [G loss: 4.386124]\n",
      "epoch:20 step:15823 [D loss: 0.022228, acc.: 100.00%] [G loss: 3.625881]\n",
      "epoch:20 step:15824 [D loss: 0.123096, acc.: 97.66%] [G loss: 2.275660]\n",
      "epoch:20 step:15825 [D loss: 0.626933, acc.: 71.09%] [G loss: 6.847213]\n",
      "epoch:20 step:15826 [D loss: 0.499858, acc.: 75.78%] [G loss: 4.914397]\n",
      "epoch:20 step:15827 [D loss: 0.082529, acc.: 96.88%] [G loss: 4.480120]\n",
      "epoch:20 step:15828 [D loss: 0.005718, acc.: 100.00%] [G loss: 4.650556]\n",
      "epoch:20 step:15829 [D loss: 0.054418, acc.: 99.22%] [G loss: 5.148843]\n",
      "epoch:20 step:15830 [D loss: 0.136836, acc.: 93.75%] [G loss: 5.350248]\n",
      "epoch:20 step:15831 [D loss: 0.016030, acc.: 100.00%] [G loss: 4.685483]\n",
      "epoch:20 step:15832 [D loss: 0.019419, acc.: 100.00%] [G loss: 4.697652]\n",
      "epoch:20 step:15833 [D loss: 0.066292, acc.: 99.22%] [G loss: 3.237725]\n",
      "epoch:20 step:15834 [D loss: 0.992680, acc.: 53.12%] [G loss: 6.857342]\n",
      "epoch:20 step:15835 [D loss: 0.233737, acc.: 85.94%] [G loss: 6.944906]\n",
      "epoch:20 step:15836 [D loss: 0.204382, acc.: 91.41%] [G loss: 3.974494]\n",
      "epoch:20 step:15837 [D loss: 0.139256, acc.: 91.41%] [G loss: 3.993123]\n",
      "epoch:20 step:15838 [D loss: 0.015866, acc.: 100.00%] [G loss: 5.086709]\n",
      "epoch:20 step:15839 [D loss: 0.006438, acc.: 100.00%] [G loss: 4.574027]\n",
      "epoch:20 step:15840 [D loss: 0.043239, acc.: 100.00%] [G loss: 3.763117]\n",
      "epoch:20 step:15841 [D loss: 0.083610, acc.: 96.88%] [G loss: 4.840152]\n",
      "epoch:20 step:15842 [D loss: 0.028625, acc.: 100.00%] [G loss: 3.788577]\n",
      "epoch:20 step:15843 [D loss: 0.128336, acc.: 96.88%] [G loss: 3.355728]\n",
      "epoch:20 step:15844 [D loss: 0.162651, acc.: 96.88%] [G loss: 3.771324]\n",
      "epoch:20 step:15845 [D loss: 0.026724, acc.: 100.00%] [G loss: 4.257335]\n",
      "epoch:20 step:15846 [D loss: 0.055380, acc.: 100.00%] [G loss: 4.236409]\n",
      "epoch:20 step:15847 [D loss: 0.060711, acc.: 100.00%] [G loss: 4.421322]\n",
      "epoch:20 step:15848 [D loss: 0.067698, acc.: 97.66%] [G loss: 4.520051]\n",
      "epoch:20 step:15849 [D loss: 0.123580, acc.: 95.31%] [G loss: 3.783089]\n",
      "epoch:20 step:15850 [D loss: 0.058308, acc.: 98.44%] [G loss: 5.155761]\n",
      "epoch:20 step:15851 [D loss: 0.044561, acc.: 100.00%] [G loss: 4.427255]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:20 step:15852 [D loss: 0.841793, acc.: 60.94%] [G loss: 7.117559]\n",
      "epoch:20 step:15853 [D loss: 1.284894, acc.: 55.47%] [G loss: 4.514361]\n",
      "epoch:20 step:15854 [D loss: 0.132554, acc.: 96.09%] [G loss: 3.571123]\n",
      "epoch:20 step:15855 [D loss: 0.122956, acc.: 93.75%] [G loss: 3.986243]\n",
      "epoch:20 step:15856 [D loss: 0.029068, acc.: 100.00%] [G loss: 4.156106]\n",
      "epoch:20 step:15857 [D loss: 0.034694, acc.: 100.00%] [G loss: 4.238686]\n",
      "epoch:20 step:15858 [D loss: 0.033125, acc.: 100.00%] [G loss: 4.338833]\n",
      "epoch:20 step:15859 [D loss: 0.022012, acc.: 100.00%] [G loss: 3.752554]\n",
      "epoch:20 step:15860 [D loss: 0.133766, acc.: 96.88%] [G loss: 3.286159]\n",
      "epoch:20 step:15861 [D loss: 0.113135, acc.: 96.88%] [G loss: 4.129655]\n",
      "epoch:20 step:15862 [D loss: 0.013424, acc.: 100.00%] [G loss: 3.986119]\n",
      "epoch:20 step:15863 [D loss: 0.026372, acc.: 100.00%] [G loss: 4.207503]\n",
      "epoch:20 step:15864 [D loss: 0.024280, acc.: 100.00%] [G loss: 3.895653]\n",
      "epoch:20 step:15865 [D loss: 0.060496, acc.: 99.22%] [G loss: 3.308568]\n",
      "epoch:20 step:15866 [D loss: 0.029992, acc.: 100.00%] [G loss: 4.225802]\n",
      "epoch:20 step:15867 [D loss: 0.676879, acc.: 60.94%] [G loss: 5.541915]\n",
      "epoch:20 step:15868 [D loss: 0.096865, acc.: 96.09%] [G loss: 5.884493]\n",
      "epoch:20 step:15869 [D loss: 0.177867, acc.: 92.19%] [G loss: 3.435103]\n",
      "epoch:20 step:15870 [D loss: 0.157386, acc.: 95.31%] [G loss: 4.092958]\n",
      "epoch:20 step:15871 [D loss: 0.017587, acc.: 100.00%] [G loss: 5.344162]\n",
      "epoch:20 step:15872 [D loss: 0.023125, acc.: 99.22%] [G loss: 3.858125]\n",
      "epoch:20 step:15873 [D loss: 0.007326, acc.: 100.00%] [G loss: 5.423415]\n",
      "epoch:20 step:15874 [D loss: 0.004745, acc.: 100.00%] [G loss: 4.600485]\n",
      "epoch:20 step:15875 [D loss: 0.080302, acc.: 99.22%] [G loss: 3.806379]\n",
      "epoch:20 step:15876 [D loss: 0.014279, acc.: 100.00%] [G loss: 3.680275]\n",
      "epoch:20 step:15877 [D loss: 0.073543, acc.: 99.22%] [G loss: 4.354672]\n",
      "epoch:20 step:15878 [D loss: 0.032461, acc.: 99.22%] [G loss: 4.372040]\n",
      "epoch:20 step:15879 [D loss: 0.059907, acc.: 99.22%] [G loss: 4.778165]\n",
      "epoch:20 step:15880 [D loss: 0.197741, acc.: 90.62%] [G loss: 4.718567]\n",
      "epoch:20 step:15881 [D loss: 0.057923, acc.: 99.22%] [G loss: 5.114860]\n",
      "epoch:20 step:15882 [D loss: 0.007868, acc.: 100.00%] [G loss: 4.823509]\n",
      "epoch:20 step:15883 [D loss: 0.032652, acc.: 100.00%] [G loss: 4.370757]\n",
      "epoch:20 step:15884 [D loss: 0.174621, acc.: 92.19%] [G loss: 4.462893]\n",
      "epoch:20 step:15885 [D loss: 0.020711, acc.: 100.00%] [G loss: 3.619182]\n",
      "epoch:20 step:15886 [D loss: 0.036202, acc.: 100.00%] [G loss: 4.395255]\n",
      "epoch:20 step:15887 [D loss: 0.052432, acc.: 99.22%] [G loss: 2.314626]\n",
      "epoch:20 step:15888 [D loss: 0.030882, acc.: 100.00%] [G loss: 2.041832]\n",
      "epoch:20 step:15889 [D loss: 0.138573, acc.: 96.88%] [G loss: 5.730846]\n",
      "epoch:20 step:15890 [D loss: 0.156234, acc.: 91.41%] [G loss: 3.720075]\n",
      "epoch:20 step:15891 [D loss: 0.043860, acc.: 99.22%] [G loss: 3.757316]\n",
      "epoch:20 step:15892 [D loss: 0.072732, acc.: 96.88%] [G loss: 6.153001]\n",
      "epoch:20 step:15893 [D loss: 0.023547, acc.: 100.00%] [G loss: 4.043707]\n",
      "epoch:20 step:15894 [D loss: 0.040659, acc.: 100.00%] [G loss: 3.235615]\n",
      "epoch:20 step:15895 [D loss: 0.010207, acc.: 100.00%] [G loss: 3.893992]\n",
      "epoch:20 step:15896 [D loss: 0.105461, acc.: 96.88%] [G loss: 4.282409]\n",
      "epoch:20 step:15897 [D loss: 0.079826, acc.: 98.44%] [G loss: 5.569294]\n",
      "epoch:20 step:15898 [D loss: 0.055737, acc.: 99.22%] [G loss: 4.721362]\n",
      "epoch:20 step:15899 [D loss: 0.038279, acc.: 99.22%] [G loss: 5.044718]\n",
      "epoch:20 step:15900 [D loss: 0.023993, acc.: 100.00%] [G loss: 5.526064]\n",
      "epoch:20 step:15901 [D loss: 0.005668, acc.: 100.00%] [G loss: 4.353909]\n",
      "epoch:20 step:15902 [D loss: 0.012282, acc.: 100.00%] [G loss: 4.878988]\n",
      "epoch:20 step:15903 [D loss: 0.014974, acc.: 100.00%] [G loss: 5.923868]\n",
      "epoch:20 step:15904 [D loss: 0.006071, acc.: 100.00%] [G loss: 4.426250]\n",
      "epoch:20 step:15905 [D loss: 0.019078, acc.: 99.22%] [G loss: 4.831486]\n",
      "epoch:20 step:15906 [D loss: 1.412575, acc.: 39.84%] [G loss: 8.831066]\n",
      "epoch:20 step:15907 [D loss: 1.927400, acc.: 50.78%] [G loss: 5.869403]\n",
      "epoch:20 step:15908 [D loss: 0.107743, acc.: 96.09%] [G loss: 4.943324]\n",
      "epoch:20 step:15909 [D loss: 0.059244, acc.: 99.22%] [G loss: 5.119788]\n",
      "epoch:20 step:15910 [D loss: 0.033232, acc.: 100.00%] [G loss: 3.585426]\n",
      "epoch:20 step:15911 [D loss: 0.322737, acc.: 85.94%] [G loss: 6.310348]\n",
      "epoch:20 step:15912 [D loss: 0.275431, acc.: 85.94%] [G loss: 4.392671]\n",
      "epoch:20 step:15913 [D loss: 0.026187, acc.: 100.00%] [G loss: 4.048968]\n",
      "epoch:20 step:15914 [D loss: 0.037357, acc.: 100.00%] [G loss: 4.231157]\n",
      "epoch:20 step:15915 [D loss: 0.140320, acc.: 96.88%] [G loss: 3.787720]\n",
      "epoch:20 step:15916 [D loss: 0.015452, acc.: 100.00%] [G loss: 3.042104]\n",
      "epoch:20 step:15917 [D loss: 0.026117, acc.: 100.00%] [G loss: 3.465954]\n",
      "epoch:20 step:15918 [D loss: 0.026116, acc.: 100.00%] [G loss: 3.464045]\n",
      "epoch:20 step:15919 [D loss: 0.032043, acc.: 100.00%] [G loss: 4.577689]\n",
      "epoch:20 step:15920 [D loss: 0.012593, acc.: 100.00%] [G loss: 3.861774]\n",
      "epoch:20 step:15921 [D loss: 0.025412, acc.: 100.00%] [G loss: 2.496349]\n",
      "epoch:20 step:15922 [D loss: 0.023417, acc.: 100.00%] [G loss: 2.089407]\n",
      "epoch:20 step:15923 [D loss: 0.284561, acc.: 89.06%] [G loss: 4.512620]\n",
      "epoch:20 step:15924 [D loss: 0.024104, acc.: 100.00%] [G loss: 5.030961]\n",
      "epoch:20 step:15925 [D loss: 0.176951, acc.: 94.53%] [G loss: 1.328274]\n",
      "epoch:20 step:15926 [D loss: 0.103656, acc.: 96.88%] [G loss: 1.479981]\n",
      "epoch:20 step:15927 [D loss: 0.008039, acc.: 100.00%] [G loss: 1.789009]\n",
      "epoch:20 step:15928 [D loss: 0.008398, acc.: 100.00%] [G loss: 3.277154]\n",
      "epoch:20 step:15929 [D loss: 0.149198, acc.: 96.88%] [G loss: 5.587042]\n",
      "epoch:20 step:15930 [D loss: 0.133941, acc.: 93.75%] [G loss: 5.059309]\n",
      "epoch:20 step:15931 [D loss: 0.012153, acc.: 100.00%] [G loss: 4.842521]\n",
      "epoch:20 step:15932 [D loss: 0.065345, acc.: 100.00%] [G loss: 3.810191]\n",
      "epoch:20 step:15933 [D loss: 0.025713, acc.: 100.00%] [G loss: 4.176313]\n",
      "epoch:20 step:15934 [D loss: 0.004231, acc.: 100.00%] [G loss: 3.320365]\n",
      "epoch:20 step:15935 [D loss: 2.610435, acc.: 26.56%] [G loss: 8.361217]\n",
      "epoch:20 step:15936 [D loss: 1.620026, acc.: 52.34%] [G loss: 5.733384]\n",
      "epoch:20 step:15937 [D loss: 0.532262, acc.: 76.56%] [G loss: 2.664607]\n",
      "epoch:20 step:15938 [D loss: 0.372630, acc.: 84.38%] [G loss: 3.533897]\n",
      "epoch:20 step:15939 [D loss: 0.021990, acc.: 100.00%] [G loss: 4.852348]\n",
      "epoch:20 step:15940 [D loss: 0.165681, acc.: 89.84%] [G loss: 4.156621]\n",
      "epoch:20 step:15941 [D loss: 0.131001, acc.: 94.53%] [G loss: 2.293661]\n",
      "epoch:20 step:15942 [D loss: 0.070064, acc.: 98.44%] [G loss: 1.946379]\n",
      "epoch:20 step:15943 [D loss: 0.106158, acc.: 95.31%] [G loss: 3.471807]\n",
      "epoch:20 step:15944 [D loss: 0.031279, acc.: 100.00%] [G loss: 3.877333]\n",
      "epoch:20 step:15945 [D loss: 0.050451, acc.: 100.00%] [G loss: 3.713548]\n",
      "epoch:20 step:15946 [D loss: 0.039119, acc.: 99.22%] [G loss: 2.757947]\n",
      "epoch:20 step:15947 [D loss: 0.075512, acc.: 98.44%] [G loss: 3.223886]\n",
      "epoch:20 step:15948 [D loss: 0.013941, acc.: 100.00%] [G loss: 3.122111]\n",
      "epoch:20 step:15949 [D loss: 0.063578, acc.: 99.22%] [G loss: 1.710473]\n",
      "epoch:20 step:15950 [D loss: 0.034303, acc.: 100.00%] [G loss: 1.923638]\n",
      "epoch:20 step:15951 [D loss: 0.069617, acc.: 98.44%] [G loss: 2.643174]\n",
      "epoch:20 step:15952 [D loss: 0.044783, acc.: 100.00%] [G loss: 3.141591]\n",
      "epoch:20 step:15953 [D loss: 0.047432, acc.: 100.00%] [G loss: 2.640489]\n",
      "epoch:20 step:15954 [D loss: 0.460163, acc.: 77.34%] [G loss: 6.372837]\n",
      "epoch:20 step:15955 [D loss: 0.336851, acc.: 80.47%] [G loss: 5.111791]\n",
      "epoch:20 step:15956 [D loss: 0.146122, acc.: 94.53%] [G loss: 3.260296]\n",
      "epoch:20 step:15957 [D loss: 0.237445, acc.: 89.06%] [G loss: 5.499063]\n",
      "epoch:20 step:15958 [D loss: 0.329128, acc.: 85.16%] [G loss: 4.309361]\n",
      "epoch:20 step:15959 [D loss: 0.046033, acc.: 100.00%] [G loss: 4.695088]\n",
      "epoch:20 step:15960 [D loss: 0.059500, acc.: 99.22%] [G loss: 3.379628]\n",
      "epoch:20 step:15961 [D loss: 0.015854, acc.: 100.00%] [G loss: 3.814375]\n",
      "epoch:20 step:15962 [D loss: 0.025885, acc.: 100.00%] [G loss: 4.607672]\n",
      "epoch:20 step:15963 [D loss: 0.057720, acc.: 98.44%] [G loss: 3.461374]\n",
      "epoch:20 step:15964 [D loss: 0.022220, acc.: 100.00%] [G loss: 4.068243]\n",
      "epoch:20 step:15965 [D loss: 0.032221, acc.: 99.22%] [G loss: 3.605578]\n",
      "epoch:20 step:15966 [D loss: 0.089715, acc.: 100.00%] [G loss: 3.630172]\n",
      "epoch:20 step:15967 [D loss: 0.020662, acc.: 100.00%] [G loss: 4.318064]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:20 step:15968 [D loss: 0.019030, acc.: 100.00%] [G loss: 5.138014]\n",
      "epoch:20 step:15969 [D loss: 0.016430, acc.: 100.00%] [G loss: 4.368761]\n",
      "epoch:20 step:15970 [D loss: 0.066421, acc.: 98.44%] [G loss: 2.990033]\n",
      "epoch:20 step:15971 [D loss: 0.106226, acc.: 96.09%] [G loss: 4.134088]\n",
      "epoch:20 step:15972 [D loss: 0.028550, acc.: 100.00%] [G loss: 5.148658]\n",
      "epoch:20 step:15973 [D loss: 0.048879, acc.: 99.22%] [G loss: 3.141740]\n",
      "epoch:20 step:15974 [D loss: 0.024932, acc.: 100.00%] [G loss: 2.777440]\n",
      "epoch:20 step:15975 [D loss: 0.117799, acc.: 97.66%] [G loss: 1.851133]\n",
      "epoch:20 step:15976 [D loss: 0.106002, acc.: 97.66%] [G loss: 5.135583]\n",
      "epoch:20 step:15977 [D loss: 0.016127, acc.: 100.00%] [G loss: 5.240484]\n",
      "epoch:20 step:15978 [D loss: 0.256550, acc.: 89.84%] [G loss: 2.607183]\n",
      "epoch:20 step:15979 [D loss: 0.363288, acc.: 79.69%] [G loss: 5.958700]\n",
      "epoch:20 step:15980 [D loss: 0.118878, acc.: 96.09%] [G loss: 6.308254]\n",
      "epoch:20 step:15981 [D loss: 0.398300, acc.: 84.38%] [G loss: 1.937900]\n",
      "epoch:20 step:15982 [D loss: 0.418434, acc.: 80.47%] [G loss: 6.534235]\n",
      "epoch:20 step:15983 [D loss: 0.059105, acc.: 98.44%] [G loss: 6.387075]\n",
      "epoch:20 step:15984 [D loss: 1.112902, acc.: 58.59%] [G loss: 2.474001]\n",
      "epoch:20 step:15985 [D loss: 0.532918, acc.: 72.66%] [G loss: 5.461514]\n",
      "epoch:20 step:15986 [D loss: 0.027548, acc.: 98.44%] [G loss: 6.451150]\n",
      "epoch:20 step:15987 [D loss: 0.295103, acc.: 83.59%] [G loss: 4.458784]\n",
      "epoch:20 step:15988 [D loss: 0.036517, acc.: 100.00%] [G loss: 4.324628]\n",
      "epoch:20 step:15989 [D loss: 0.044799, acc.: 99.22%] [G loss: 4.533260]\n",
      "epoch:20 step:15990 [D loss: 0.084878, acc.: 98.44%] [G loss: 3.411467]\n",
      "epoch:20 step:15991 [D loss: 0.031852, acc.: 100.00%] [G loss: 3.626152]\n",
      "epoch:20 step:15992 [D loss: 0.010686, acc.: 100.00%] [G loss: 3.899182]\n",
      "epoch:20 step:15993 [D loss: 0.017253, acc.: 100.00%] [G loss: 3.262058]\n",
      "epoch:20 step:15994 [D loss: 0.141266, acc.: 96.88%] [G loss: 2.946252]\n",
      "epoch:20 step:15995 [D loss: 0.029901, acc.: 100.00%] [G loss: 2.977976]\n",
      "epoch:20 step:15996 [D loss: 0.034228, acc.: 100.00%] [G loss: 1.833599]\n",
      "epoch:20 step:15997 [D loss: 0.019906, acc.: 99.22%] [G loss: 2.986266]\n",
      "epoch:20 step:15998 [D loss: 0.132270, acc.: 97.66%] [G loss: 5.314600]\n",
      "epoch:20 step:15999 [D loss: 0.056231, acc.: 97.66%] [G loss: 5.723695]\n",
      "epoch:20 step:16000 [D loss: 0.062302, acc.: 98.44%] [G loss: 4.351590]\n",
      "##############\n",
      "[1.02252758 0.98046613 2.10759735 0.91832102 0.9714113  1.02496878\n",
      " 2.12674565 1.11056818 2.11587668 2.1022981 ]\n",
      "##########\n",
      "epoch:20 step:16001 [D loss: 0.011372, acc.: 100.00%] [G loss: 4.025208]\n",
      "epoch:20 step:16002 [D loss: 0.044739, acc.: 99.22%] [G loss: 4.865713]\n",
      "epoch:20 step:16003 [D loss: 0.012968, acc.: 100.00%] [G loss: 4.587146]\n",
      "epoch:20 step:16004 [D loss: 0.024763, acc.: 100.00%] [G loss: 4.552313]\n",
      "epoch:20 step:16005 [D loss: 0.198194, acc.: 92.19%] [G loss: 0.885706]\n",
      "epoch:20 step:16006 [D loss: 0.248883, acc.: 87.50%] [G loss: 6.079095]\n",
      "epoch:20 step:16007 [D loss: 0.177203, acc.: 92.19%] [G loss: 5.610203]\n",
      "epoch:20 step:16008 [D loss: 0.019292, acc.: 100.00%] [G loss: 4.560206]\n",
      "epoch:20 step:16009 [D loss: 0.016939, acc.: 99.22%] [G loss: 4.680397]\n",
      "epoch:20 step:16010 [D loss: 0.057178, acc.: 98.44%] [G loss: 2.964128]\n",
      "epoch:20 step:16011 [D loss: 0.074384, acc.: 97.66%] [G loss: 5.301103]\n",
      "epoch:20 step:16012 [D loss: 0.003788, acc.: 100.00%] [G loss: 5.503689]\n",
      "epoch:20 step:16013 [D loss: 0.090998, acc.: 96.09%] [G loss: 4.230202]\n",
      "epoch:20 step:16014 [D loss: 0.030037, acc.: 100.00%] [G loss: 3.582633]\n",
      "epoch:20 step:16015 [D loss: 0.047712, acc.: 99.22%] [G loss: 4.339349]\n",
      "epoch:20 step:16016 [D loss: 0.009471, acc.: 100.00%] [G loss: 4.592900]\n",
      "epoch:20 step:16017 [D loss: 0.011076, acc.: 100.00%] [G loss: 3.874431]\n",
      "epoch:20 step:16018 [D loss: 0.007362, acc.: 100.00%] [G loss: 3.986438]\n",
      "epoch:20 step:16019 [D loss: 0.020621, acc.: 100.00%] [G loss: 3.732539]\n",
      "epoch:20 step:16020 [D loss: 0.012088, acc.: 100.00%] [G loss: 4.138865]\n",
      "epoch:20 step:16021 [D loss: 0.058997, acc.: 100.00%] [G loss: 3.926090]\n",
      "epoch:20 step:16022 [D loss: 0.040818, acc.: 99.22%] [G loss: 3.802176]\n",
      "epoch:20 step:16023 [D loss: 0.070949, acc.: 99.22%] [G loss: 3.430045]\n",
      "epoch:20 step:16024 [D loss: 0.022777, acc.: 100.00%] [G loss: 3.370417]\n",
      "epoch:20 step:16025 [D loss: 0.175858, acc.: 95.31%] [G loss: 2.512953]\n",
      "epoch:20 step:16026 [D loss: 0.031067, acc.: 100.00%] [G loss: 3.950097]\n",
      "epoch:20 step:16027 [D loss: 0.059717, acc.: 99.22%] [G loss: 5.487649]\n",
      "epoch:20 step:16028 [D loss: 0.008948, acc.: 100.00%] [G loss: 4.935373]\n",
      "epoch:20 step:16029 [D loss: 0.042956, acc.: 99.22%] [G loss: 4.058474]\n",
      "epoch:20 step:16030 [D loss: 0.009220, acc.: 100.00%] [G loss: 3.391508]\n",
      "epoch:20 step:16031 [D loss: 0.087304, acc.: 98.44%] [G loss: 1.476233]\n",
      "epoch:20 step:16032 [D loss: 0.201604, acc.: 92.19%] [G loss: 6.159971]\n",
      "epoch:20 step:16033 [D loss: 0.618339, acc.: 68.75%] [G loss: 2.468116]\n",
      "epoch:20 step:16034 [D loss: 0.377737, acc.: 82.03%] [G loss: 6.093357]\n",
      "epoch:20 step:16035 [D loss: 0.043786, acc.: 99.22%] [G loss: 6.416587]\n",
      "epoch:20 step:16036 [D loss: 0.208208, acc.: 90.62%] [G loss: 4.130638]\n",
      "epoch:20 step:16037 [D loss: 0.133062, acc.: 93.75%] [G loss: 4.703192]\n",
      "epoch:20 step:16038 [D loss: 0.004995, acc.: 100.00%] [G loss: 5.490860]\n",
      "epoch:20 step:16039 [D loss: 0.011485, acc.: 100.00%] [G loss: 5.504422]\n",
      "epoch:20 step:16040 [D loss: 0.469310, acc.: 78.91%] [G loss: 0.986958]\n",
      "epoch:20 step:16041 [D loss: 0.025788, acc.: 100.00%] [G loss: 3.109066]\n",
      "epoch:20 step:16042 [D loss: 0.080229, acc.: 97.66%] [G loss: 3.804978]\n",
      "epoch:20 step:16043 [D loss: 0.003365, acc.: 100.00%] [G loss: 5.455690]\n",
      "epoch:20 step:16044 [D loss: 0.195366, acc.: 92.19%] [G loss: 2.120316]\n",
      "epoch:20 step:16045 [D loss: 0.112200, acc.: 98.44%] [G loss: 2.430643]\n",
      "epoch:20 step:16046 [D loss: 0.005700, acc.: 100.00%] [G loss: 5.256455]\n",
      "epoch:20 step:16047 [D loss: 0.002838, acc.: 100.00%] [G loss: 4.121484]\n",
      "epoch:20 step:16048 [D loss: 0.110285, acc.: 96.88%] [G loss: 4.161439]\n",
      "epoch:20 step:16049 [D loss: 0.118719, acc.: 97.66%] [G loss: 5.208922]\n",
      "epoch:20 step:16050 [D loss: 0.081464, acc.: 96.88%] [G loss: 2.652458]\n",
      "epoch:20 step:16051 [D loss: 1.250450, acc.: 45.31%] [G loss: 6.404697]\n",
      "epoch:20 step:16052 [D loss: 0.658584, acc.: 66.41%] [G loss: 3.830066]\n",
      "epoch:20 step:16053 [D loss: 0.097949, acc.: 99.22%] [G loss: 4.268426]\n",
      "epoch:20 step:16054 [D loss: 0.097297, acc.: 96.88%] [G loss: 2.954498]\n",
      "epoch:20 step:16055 [D loss: 0.094561, acc.: 99.22%] [G loss: 3.813035]\n",
      "epoch:20 step:16056 [D loss: 0.273001, acc.: 89.84%] [G loss: 3.011893]\n",
      "epoch:20 step:16057 [D loss: 0.091564, acc.: 99.22%] [G loss: 3.164939]\n",
      "epoch:20 step:16058 [D loss: 0.275262, acc.: 86.72%] [G loss: 6.397425]\n",
      "epoch:20 step:16059 [D loss: 0.126540, acc.: 96.09%] [G loss: 5.655517]\n",
      "epoch:20 step:16060 [D loss: 0.296289, acc.: 88.28%] [G loss: 4.711735]\n",
      "epoch:20 step:16061 [D loss: 0.080274, acc.: 100.00%] [G loss: 3.845650]\n",
      "epoch:20 step:16062 [D loss: 0.063884, acc.: 97.66%] [G loss: 5.235829]\n",
      "epoch:20 step:16063 [D loss: 0.040716, acc.: 97.66%] [G loss: 4.737711]\n",
      "epoch:20 step:16064 [D loss: 0.031667, acc.: 99.22%] [G loss: 5.699263]\n",
      "epoch:20 step:16065 [D loss: 0.027149, acc.: 100.00%] [G loss: 4.041070]\n",
      "epoch:20 step:16066 [D loss: 0.050829, acc.: 99.22%] [G loss: 5.157746]\n",
      "epoch:20 step:16067 [D loss: 0.020221, acc.: 100.00%] [G loss: 5.139529]\n",
      "epoch:20 step:16068 [D loss: 0.190480, acc.: 95.31%] [G loss: 2.608332]\n",
      "epoch:20 step:16069 [D loss: 0.017168, acc.: 100.00%] [G loss: 2.764046]\n",
      "epoch:20 step:16070 [D loss: 0.029896, acc.: 100.00%] [G loss: 4.422937]\n",
      "epoch:20 step:16071 [D loss: 0.026896, acc.: 100.00%] [G loss: 4.065711]\n",
      "epoch:20 step:16072 [D loss: 0.052276, acc.: 99.22%] [G loss: 4.149781]\n",
      "epoch:20 step:16073 [D loss: 0.062273, acc.: 99.22%] [G loss: 3.520950]\n",
      "epoch:20 step:16074 [D loss: 0.011582, acc.: 100.00%] [G loss: 4.273247]\n",
      "epoch:20 step:16075 [D loss: 0.026351, acc.: 100.00%] [G loss: 4.996413]\n",
      "epoch:20 step:16076 [D loss: 0.005608, acc.: 100.00%] [G loss: 4.248823]\n",
      "epoch:20 step:16077 [D loss: 0.007596, acc.: 100.00%] [G loss: 4.761110]\n",
      "epoch:20 step:16078 [D loss: 0.046669, acc.: 99.22%] [G loss: 4.347496]\n",
      "epoch:20 step:16079 [D loss: 0.361921, acc.: 82.81%] [G loss: 6.771076]\n",
      "epoch:20 step:16080 [D loss: 1.745722, acc.: 46.09%] [G loss: 3.779862]\n",
      "epoch:20 step:16081 [D loss: 0.111345, acc.: 97.66%] [G loss: 5.364646]\n",
      "epoch:20 step:16082 [D loss: 0.028662, acc.: 99.22%] [G loss: 5.487308]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:20 step:16083 [D loss: 0.062194, acc.: 97.66%] [G loss: 4.716282]\n",
      "epoch:20 step:16084 [D loss: 0.024486, acc.: 100.00%] [G loss: 4.497625]\n",
      "epoch:20 step:16085 [D loss: 0.023566, acc.: 100.00%] [G loss: 3.679344]\n",
      "epoch:20 step:16086 [D loss: 0.038205, acc.: 99.22%] [G loss: 4.432992]\n",
      "epoch:20 step:16087 [D loss: 0.055633, acc.: 100.00%] [G loss: 3.874324]\n",
      "epoch:20 step:16088 [D loss: 0.050280, acc.: 100.00%] [G loss: 3.941987]\n",
      "epoch:20 step:16089 [D loss: 0.010199, acc.: 100.00%] [G loss: 4.543259]\n",
      "epoch:20 step:16090 [D loss: 0.018442, acc.: 99.22%] [G loss: 4.013932]\n",
      "epoch:20 step:16091 [D loss: 0.024791, acc.: 100.00%] [G loss: 4.251401]\n",
      "epoch:20 step:16092 [D loss: 0.025371, acc.: 100.00%] [G loss: 3.681482]\n",
      "epoch:20 step:16093 [D loss: 0.021698, acc.: 100.00%] [G loss: 4.476051]\n",
      "epoch:20 step:16094 [D loss: 0.102320, acc.: 96.88%] [G loss: 4.211920]\n",
      "epoch:20 step:16095 [D loss: 0.027608, acc.: 99.22%] [G loss: 4.527329]\n",
      "epoch:20 step:16096 [D loss: 0.013602, acc.: 100.00%] [G loss: 4.465254]\n",
      "epoch:20 step:16097 [D loss: 0.023970, acc.: 100.00%] [G loss: 3.707207]\n",
      "epoch:20 step:16098 [D loss: 0.020899, acc.: 100.00%] [G loss: 5.244772]\n",
      "epoch:20 step:16099 [D loss: 0.005908, acc.: 100.00%] [G loss: 4.497721]\n",
      "epoch:20 step:16100 [D loss: 0.013952, acc.: 100.00%] [G loss: 4.042418]\n",
      "epoch:20 step:16101 [D loss: 0.024957, acc.: 100.00%] [G loss: 3.534091]\n",
      "epoch:20 step:16102 [D loss: 0.038062, acc.: 100.00%] [G loss: 2.732276]\n",
      "epoch:20 step:16103 [D loss: 0.012426, acc.: 100.00%] [G loss: 4.067020]\n",
      "epoch:20 step:16104 [D loss: 0.024383, acc.: 100.00%] [G loss: 3.519159]\n",
      "epoch:20 step:16105 [D loss: 0.076674, acc.: 97.66%] [G loss: 3.550339]\n",
      "epoch:20 step:16106 [D loss: 0.052270, acc.: 99.22%] [G loss: 4.334220]\n",
      "epoch:20 step:16107 [D loss: 0.028662, acc.: 99.22%] [G loss: 3.491169]\n",
      "epoch:20 step:16108 [D loss: 0.007917, acc.: 100.00%] [G loss: 4.158053]\n",
      "epoch:20 step:16109 [D loss: 0.019815, acc.: 100.00%] [G loss: 3.224054]\n",
      "epoch:20 step:16110 [D loss: 0.013869, acc.: 100.00%] [G loss: 1.989560]\n",
      "epoch:20 step:16111 [D loss: 0.150449, acc.: 93.75%] [G loss: 4.345813]\n",
      "epoch:20 step:16112 [D loss: 0.211766, acc.: 92.97%] [G loss: 3.532226]\n",
      "epoch:20 step:16113 [D loss: 0.039547, acc.: 100.00%] [G loss: 4.031873]\n",
      "epoch:20 step:16114 [D loss: 0.012283, acc.: 100.00%] [G loss: 4.920833]\n",
      "epoch:20 step:16115 [D loss: 0.008544, acc.: 100.00%] [G loss: 3.892988]\n",
      "epoch:20 step:16116 [D loss: 0.010574, acc.: 100.00%] [G loss: 4.460822]\n",
      "epoch:20 step:16117 [D loss: 0.045681, acc.: 100.00%] [G loss: 4.100477]\n",
      "epoch:20 step:16118 [D loss: 0.062552, acc.: 98.44%] [G loss: 3.920785]\n",
      "epoch:20 step:16119 [D loss: 0.038722, acc.: 99.22%] [G loss: 4.039522]\n",
      "epoch:20 step:16120 [D loss: 0.017260, acc.: 100.00%] [G loss: 4.193933]\n",
      "epoch:20 step:16121 [D loss: 4.599426, acc.: 15.62%] [G loss: 7.447424]\n",
      "epoch:20 step:16122 [D loss: 1.832515, acc.: 50.78%] [G loss: 4.812678]\n",
      "epoch:20 step:16123 [D loss: 0.567813, acc.: 74.22%] [G loss: 1.958161]\n",
      "epoch:20 step:16124 [D loss: 0.422805, acc.: 78.12%] [G loss: 2.839070]\n",
      "epoch:20 step:16125 [D loss: 0.067925, acc.: 99.22%] [G loss: 3.285147]\n",
      "epoch:20 step:16126 [D loss: 0.349211, acc.: 82.03%] [G loss: 2.530513]\n",
      "epoch:20 step:16127 [D loss: 0.305297, acc.: 90.62%] [G loss: 2.235974]\n",
      "epoch:20 step:16128 [D loss: 0.198574, acc.: 94.53%] [G loss: 3.462967]\n",
      "epoch:20 step:16129 [D loss: 0.708527, acc.: 63.28%] [G loss: 2.287940]\n",
      "epoch:20 step:16130 [D loss: 0.361474, acc.: 86.72%] [G loss: 2.927877]\n",
      "epoch:20 step:16131 [D loss: 0.310934, acc.: 84.38%] [G loss: 3.242847]\n",
      "epoch:20 step:16132 [D loss: 0.229625, acc.: 89.06%] [G loss: 1.372600]\n",
      "epoch:20 step:16133 [D loss: 0.406870, acc.: 80.47%] [G loss: 4.033386]\n",
      "epoch:20 step:16134 [D loss: 0.456411, acc.: 77.34%] [G loss: 2.975686]\n",
      "epoch:20 step:16135 [D loss: 0.219894, acc.: 95.31%] [G loss: 2.631465]\n",
      "epoch:20 step:16136 [D loss: 0.116920, acc.: 98.44%] [G loss: 3.684675]\n",
      "epoch:20 step:16137 [D loss: 0.452640, acc.: 77.34%] [G loss: 4.107788]\n",
      "epoch:20 step:16138 [D loss: 0.150221, acc.: 97.66%] [G loss: 4.056450]\n",
      "epoch:20 step:16139 [D loss: 0.842228, acc.: 55.47%] [G loss: 2.899251]\n",
      "epoch:20 step:16140 [D loss: 0.065539, acc.: 99.22%] [G loss: 3.248607]\n",
      "epoch:20 step:16141 [D loss: 0.157939, acc.: 96.09%] [G loss: 2.940405]\n",
      "epoch:20 step:16142 [D loss: 0.234502, acc.: 91.41%] [G loss: 2.780559]\n",
      "epoch:20 step:16143 [D loss: 0.280358, acc.: 87.50%] [G loss: 3.719996]\n",
      "epoch:20 step:16144 [D loss: 0.262218, acc.: 89.84%] [G loss: 2.782607]\n",
      "epoch:20 step:16145 [D loss: 0.488197, acc.: 77.34%] [G loss: 4.810792]\n",
      "epoch:20 step:16146 [D loss: 0.545051, acc.: 75.00%] [G loss: 3.979079]\n",
      "epoch:20 step:16147 [D loss: 0.113227, acc.: 96.88%] [G loss: 3.595870]\n",
      "epoch:20 step:16148 [D loss: 0.285135, acc.: 87.50%] [G loss: 3.012998]\n",
      "epoch:20 step:16149 [D loss: 0.057088, acc.: 99.22%] [G loss: 3.540652]\n",
      "epoch:20 step:16150 [D loss: 0.264883, acc.: 92.97%] [G loss: 3.219298]\n",
      "epoch:20 step:16151 [D loss: 0.107262, acc.: 98.44%] [G loss: 3.582342]\n",
      "epoch:20 step:16152 [D loss: 0.102098, acc.: 98.44%] [G loss: 2.478638]\n",
      "epoch:20 step:16153 [D loss: 0.122304, acc.: 96.88%] [G loss: 3.093915]\n",
      "epoch:20 step:16154 [D loss: 0.050135, acc.: 100.00%] [G loss: 3.335051]\n",
      "epoch:20 step:16155 [D loss: 0.269987, acc.: 89.84%] [G loss: 3.876954]\n",
      "epoch:20 step:16156 [D loss: 0.133317, acc.: 93.75%] [G loss: 3.768792]\n",
      "epoch:20 step:16157 [D loss: 0.141657, acc.: 95.31%] [G loss: 3.238384]\n",
      "epoch:20 step:16158 [D loss: 0.267446, acc.: 88.28%] [G loss: 2.306797]\n",
      "epoch:20 step:16159 [D loss: 0.033052, acc.: 99.22%] [G loss: 2.200865]\n",
      "epoch:20 step:16160 [D loss: 0.117348, acc.: 96.88%] [G loss: 1.895792]\n",
      "epoch:20 step:16161 [D loss: 0.089947, acc.: 99.22%] [G loss: 1.819365]\n",
      "epoch:20 step:16162 [D loss: 0.111636, acc.: 96.09%] [G loss: 2.269379]\n",
      "epoch:20 step:16163 [D loss: 0.089886, acc.: 97.66%] [G loss: 0.540808]\n",
      "epoch:20 step:16164 [D loss: 0.355179, acc.: 86.72%] [G loss: 3.556067]\n",
      "epoch:20 step:16165 [D loss: 0.150906, acc.: 92.97%] [G loss: 2.933890]\n",
      "epoch:20 step:16166 [D loss: 1.132556, acc.: 50.78%] [G loss: 5.400098]\n",
      "epoch:20 step:16167 [D loss: 0.258551, acc.: 88.28%] [G loss: 4.799106]\n",
      "epoch:20 step:16168 [D loss: 0.369605, acc.: 78.91%] [G loss: 1.930685]\n",
      "epoch:20 step:16169 [D loss: 0.507062, acc.: 75.78%] [G loss: 4.586876]\n",
      "epoch:20 step:16170 [D loss: 0.068183, acc.: 98.44%] [G loss: 5.835650]\n",
      "epoch:20 step:16171 [D loss: 0.282633, acc.: 89.06%] [G loss: 4.096782]\n",
      "epoch:20 step:16172 [D loss: 0.295418, acc.: 89.84%] [G loss: 4.177827]\n",
      "epoch:20 step:16173 [D loss: 0.113346, acc.: 95.31%] [G loss: 3.522480]\n",
      "epoch:20 step:16174 [D loss: 0.126799, acc.: 98.44%] [G loss: 3.693366]\n",
      "epoch:20 step:16175 [D loss: 0.075111, acc.: 99.22%] [G loss: 3.806137]\n",
      "epoch:20 step:16176 [D loss: 0.291602, acc.: 86.72%] [G loss: 3.663506]\n",
      "epoch:20 step:16177 [D loss: 0.093204, acc.: 98.44%] [G loss: 4.181150]\n",
      "epoch:20 step:16178 [D loss: 0.149874, acc.: 95.31%] [G loss: 3.621457]\n",
      "epoch:20 step:16179 [D loss: 0.033869, acc.: 100.00%] [G loss: 3.849725]\n",
      "epoch:20 step:16180 [D loss: 0.143144, acc.: 96.88%] [G loss: 2.711012]\n",
      "epoch:20 step:16181 [D loss: 0.107904, acc.: 99.22%] [G loss: 4.069705]\n",
      "epoch:20 step:16182 [D loss: 0.119056, acc.: 94.53%] [G loss: 2.853577]\n",
      "epoch:20 step:16183 [D loss: 0.065742, acc.: 99.22%] [G loss: 3.187792]\n",
      "epoch:20 step:16184 [D loss: 0.095510, acc.: 96.88%] [G loss: 3.199425]\n",
      "epoch:20 step:16185 [D loss: 0.126922, acc.: 95.31%] [G loss: 3.643870]\n",
      "epoch:20 step:16186 [D loss: 0.078933, acc.: 99.22%] [G loss: 3.403877]\n",
      "epoch:20 step:16187 [D loss: 0.120643, acc.: 97.66%] [G loss: 2.606805]\n",
      "epoch:20 step:16188 [D loss: 0.117761, acc.: 97.66%] [G loss: 2.916626]\n",
      "epoch:20 step:16189 [D loss: 0.118097, acc.: 99.22%] [G loss: 3.479889]\n",
      "epoch:20 step:16190 [D loss: 0.114183, acc.: 98.44%] [G loss: 3.271919]\n",
      "epoch:20 step:16191 [D loss: 0.395970, acc.: 85.16%] [G loss: 3.682948]\n",
      "epoch:20 step:16192 [D loss: 0.032630, acc.: 100.00%] [G loss: 4.092495]\n",
      "epoch:20 step:16193 [D loss: 0.250891, acc.: 88.28%] [G loss: 1.562704]\n",
      "epoch:20 step:16194 [D loss: 0.106424, acc.: 97.66%] [G loss: 2.811857]\n",
      "epoch:20 step:16195 [D loss: 0.091749, acc.: 98.44%] [G loss: 5.226091]\n",
      "epoch:20 step:16196 [D loss: 0.052392, acc.: 100.00%] [G loss: 4.129349]\n",
      "epoch:20 step:16197 [D loss: 0.236559, acc.: 91.41%] [G loss: 1.988082]\n",
      "epoch:20 step:16198 [D loss: 0.187026, acc.: 93.75%] [G loss: 4.735917]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:20 step:16199 [D loss: 0.061201, acc.: 98.44%] [G loss: 5.226067]\n",
      "epoch:20 step:16200 [D loss: 0.291120, acc.: 86.72%] [G loss: 2.032560]\n",
      "##############\n",
      "[1.09628481 0.83358481 1.12313219 0.941874   0.80748026 0.98846778\n",
      " 1.02253662 0.86749016 2.11206467 1.01540041]\n",
      "##########\n",
      "epoch:20 step:16201 [D loss: 0.125041, acc.: 95.31%] [G loss: 2.870107]\n",
      "epoch:20 step:16202 [D loss: 0.023705, acc.: 100.00%] [G loss: 4.194210]\n",
      "epoch:20 step:16203 [D loss: 0.099524, acc.: 96.88%] [G loss: 2.578331]\n",
      "epoch:20 step:16204 [D loss: 0.103132, acc.: 97.66%] [G loss: 4.085372]\n",
      "epoch:20 step:16205 [D loss: 0.011864, acc.: 100.00%] [G loss: 4.434832]\n",
      "epoch:20 step:16206 [D loss: 0.050743, acc.: 99.22%] [G loss: 2.970811]\n",
      "epoch:20 step:16207 [D loss: 0.032712, acc.: 100.00%] [G loss: 2.506554]\n",
      "epoch:20 step:16208 [D loss: 0.072275, acc.: 97.66%] [G loss: 2.624669]\n",
      "epoch:20 step:16209 [D loss: 0.079928, acc.: 98.44%] [G loss: 2.377308]\n",
      "epoch:20 step:16210 [D loss: 0.042579, acc.: 99.22%] [G loss: 2.613958]\n",
      "epoch:20 step:16211 [D loss: 0.188091, acc.: 92.97%] [G loss: 3.518816]\n",
      "epoch:20 step:16212 [D loss: 0.052789, acc.: 99.22%] [G loss: 2.871901]\n",
      "epoch:20 step:16213 [D loss: 0.643484, acc.: 70.31%] [G loss: 5.983537]\n",
      "epoch:20 step:16214 [D loss: 0.219848, acc.: 90.62%] [G loss: 5.042686]\n",
      "epoch:20 step:16215 [D loss: 0.072061, acc.: 97.66%] [G loss: 3.502404]\n",
      "epoch:20 step:16216 [D loss: 0.035065, acc.: 100.00%] [G loss: 2.064713]\n",
      "epoch:20 step:16217 [D loss: 0.162129, acc.: 91.41%] [G loss: 5.015816]\n",
      "epoch:20 step:16218 [D loss: 0.068236, acc.: 97.66%] [G loss: 5.626483]\n",
      "epoch:20 step:16219 [D loss: 0.070733, acc.: 98.44%] [G loss: 3.382087]\n",
      "epoch:20 step:16220 [D loss: 0.043800, acc.: 100.00%] [G loss: 4.334886]\n",
      "epoch:20 step:16221 [D loss: 0.102704, acc.: 98.44%] [G loss: 4.138003]\n",
      "epoch:20 step:16222 [D loss: 0.039565, acc.: 99.22%] [G loss: 4.006008]\n",
      "epoch:20 step:16223 [D loss: 0.060260, acc.: 99.22%] [G loss: 3.987743]\n",
      "epoch:20 step:16224 [D loss: 1.294634, acc.: 33.59%] [G loss: 6.804785]\n",
      "epoch:20 step:16225 [D loss: 0.226472, acc.: 89.06%] [G loss: 7.346912]\n",
      "epoch:20 step:16226 [D loss: 0.216362, acc.: 91.41%] [G loss: 6.185740]\n",
      "epoch:20 step:16227 [D loss: 0.008946, acc.: 100.00%] [G loss: 4.424258]\n",
      "epoch:20 step:16228 [D loss: 0.021478, acc.: 100.00%] [G loss: 4.724359]\n",
      "epoch:20 step:16229 [D loss: 0.050486, acc.: 100.00%] [G loss: 5.385930]\n",
      "epoch:20 step:16230 [D loss: 0.064596, acc.: 97.66%] [G loss: 5.452394]\n",
      "epoch:20 step:16231 [D loss: 0.006781, acc.: 100.00%] [G loss: 5.596926]\n",
      "epoch:20 step:16232 [D loss: 0.011978, acc.: 100.00%] [G loss: 4.530613]\n",
      "epoch:20 step:16233 [D loss: 0.184285, acc.: 94.53%] [G loss: 3.304894]\n",
      "epoch:20 step:16234 [D loss: 0.019923, acc.: 100.00%] [G loss: 3.126379]\n",
      "epoch:20 step:16235 [D loss: 0.027865, acc.: 100.00%] [G loss: 4.050301]\n",
      "epoch:20 step:16236 [D loss: 0.081776, acc.: 98.44%] [G loss: 4.305647]\n",
      "epoch:20 step:16237 [D loss: 0.025971, acc.: 100.00%] [G loss: 5.239931]\n",
      "epoch:20 step:16238 [D loss: 0.243167, acc.: 91.41%] [G loss: 5.631411]\n",
      "epoch:20 step:16239 [D loss: 0.152581, acc.: 92.97%] [G loss: 3.886545]\n",
      "epoch:20 step:16240 [D loss: 0.065812, acc.: 100.00%] [G loss: 5.329432]\n",
      "epoch:20 step:16241 [D loss: 0.004045, acc.: 100.00%] [G loss: 5.587214]\n",
      "epoch:20 step:16242 [D loss: 0.032000, acc.: 100.00%] [G loss: 5.034339]\n",
      "epoch:20 step:16243 [D loss: 0.128307, acc.: 96.09%] [G loss: 3.795518]\n",
      "epoch:20 step:16244 [D loss: 0.114032, acc.: 96.88%] [G loss: 5.944444]\n",
      "epoch:20 step:16245 [D loss: 0.021738, acc.: 100.00%] [G loss: 6.161758]\n",
      "epoch:20 step:16246 [D loss: 0.268302, acc.: 88.28%] [G loss: 1.151044]\n",
      "epoch:20 step:16247 [D loss: 0.288866, acc.: 87.50%] [G loss: 7.288698]\n",
      "epoch:20 step:16248 [D loss: 0.038013, acc.: 98.44%] [G loss: 7.640387]\n",
      "epoch:20 step:16249 [D loss: 0.728640, acc.: 67.97%] [G loss: 2.793019]\n",
      "epoch:20 step:16250 [D loss: 0.157905, acc.: 92.97%] [G loss: 5.405722]\n",
      "epoch:20 step:16251 [D loss: 0.009657, acc.: 100.00%] [G loss: 5.811222]\n",
      "epoch:20 step:16252 [D loss: 0.045100, acc.: 98.44%] [G loss: 5.420491]\n",
      "epoch:20 step:16253 [D loss: 0.020554, acc.: 100.00%] [G loss: 4.328298]\n",
      "epoch:20 step:16254 [D loss: 0.038756, acc.: 99.22%] [G loss: 4.731581]\n",
      "epoch:20 step:16255 [D loss: 0.055711, acc.: 98.44%] [G loss: 4.240644]\n",
      "epoch:20 step:16256 [D loss: 0.203881, acc.: 92.19%] [G loss: 7.084947]\n",
      "epoch:20 step:16257 [D loss: 0.158045, acc.: 92.97%] [G loss: 6.310874]\n",
      "epoch:20 step:16258 [D loss: 0.047554, acc.: 99.22%] [G loss: 5.590217]\n",
      "epoch:20 step:16259 [D loss: 0.025655, acc.: 100.00%] [G loss: 4.521656]\n",
      "epoch:20 step:16260 [D loss: 0.017685, acc.: 100.00%] [G loss: 3.692314]\n",
      "epoch:20 step:16261 [D loss: 0.037166, acc.: 100.00%] [G loss: 3.855715]\n",
      "epoch:20 step:16262 [D loss: 0.047965, acc.: 99.22%] [G loss: 4.041680]\n",
      "epoch:20 step:16263 [D loss: 0.048305, acc.: 100.00%] [G loss: 3.884703]\n",
      "epoch:20 step:16264 [D loss: 0.020301, acc.: 100.00%] [G loss: 4.559462]\n",
      "epoch:20 step:16265 [D loss: 0.077362, acc.: 98.44%] [G loss: 3.391197]\n",
      "epoch:20 step:16266 [D loss: 0.105387, acc.: 99.22%] [G loss: 2.664673]\n",
      "epoch:20 step:16267 [D loss: 0.063113, acc.: 97.66%] [G loss: 4.178571]\n",
      "epoch:20 step:16268 [D loss: 0.022123, acc.: 100.00%] [G loss: 4.662843]\n",
      "epoch:20 step:16269 [D loss: 0.778272, acc.: 57.81%] [G loss: 5.169704]\n",
      "epoch:20 step:16270 [D loss: 0.036908, acc.: 100.00%] [G loss: 6.001055]\n",
      "epoch:20 step:16271 [D loss: 0.044884, acc.: 99.22%] [G loss: 5.956628]\n",
      "epoch:20 step:16272 [D loss: 0.079839, acc.: 97.66%] [G loss: 3.962409]\n",
      "epoch:20 step:16273 [D loss: 0.042495, acc.: 98.44%] [G loss: 3.192491]\n",
      "epoch:20 step:16274 [D loss: 0.010584, acc.: 100.00%] [G loss: 3.510844]\n",
      "epoch:20 step:16275 [D loss: 0.100308, acc.: 96.88%] [G loss: 4.830675]\n",
      "epoch:20 step:16276 [D loss: 0.007317, acc.: 100.00%] [G loss: 5.185695]\n",
      "epoch:20 step:16277 [D loss: 0.012764, acc.: 100.00%] [G loss: 5.118709]\n",
      "epoch:20 step:16278 [D loss: 0.153712, acc.: 93.75%] [G loss: 2.679396]\n",
      "epoch:20 step:16279 [D loss: 0.018091, acc.: 100.00%] [G loss: 2.208528]\n",
      "epoch:20 step:16280 [D loss: 0.173263, acc.: 94.53%] [G loss: 5.927351]\n",
      "epoch:20 step:16281 [D loss: 0.096748, acc.: 96.88%] [G loss: 6.029363]\n",
      "epoch:20 step:16282 [D loss: 0.042678, acc.: 99.22%] [G loss: 4.909713]\n",
      "epoch:20 step:16283 [D loss: 0.031472, acc.: 100.00%] [G loss: 4.242586]\n",
      "epoch:20 step:16284 [D loss: 0.035674, acc.: 100.00%] [G loss: 4.682453]\n",
      "epoch:20 step:16285 [D loss: 0.134046, acc.: 97.66%] [G loss: 5.855143]\n",
      "epoch:20 step:16286 [D loss: 0.004200, acc.: 100.00%] [G loss: 5.241757]\n",
      "epoch:20 step:16287 [D loss: 0.007141, acc.: 100.00%] [G loss: 5.219965]\n",
      "epoch:20 step:16288 [D loss: 0.097040, acc.: 96.88%] [G loss: 4.082848]\n",
      "epoch:20 step:16289 [D loss: 0.007834, acc.: 100.00%] [G loss: 3.370340]\n",
      "epoch:20 step:16290 [D loss: 0.077213, acc.: 99.22%] [G loss: 5.239970]\n",
      "epoch:20 step:16291 [D loss: 0.009242, acc.: 100.00%] [G loss: 6.346879]\n",
      "epoch:20 step:16292 [D loss: 0.015711, acc.: 100.00%] [G loss: 4.662851]\n",
      "epoch:20 step:16293 [D loss: 0.042437, acc.: 99.22%] [G loss: 4.689229]\n",
      "epoch:20 step:16294 [D loss: 0.028011, acc.: 100.00%] [G loss: 3.386597]\n",
      "epoch:20 step:16295 [D loss: 0.025971, acc.: 100.00%] [G loss: 5.411190]\n",
      "epoch:20 step:16296 [D loss: 0.010961, acc.: 100.00%] [G loss: 6.014734]\n",
      "epoch:20 step:16297 [D loss: 0.047704, acc.: 100.00%] [G loss: 2.525030]\n",
      "epoch:20 step:16298 [D loss: 0.120683, acc.: 96.88%] [G loss: 2.597471]\n",
      "epoch:20 step:16299 [D loss: 0.027586, acc.: 100.00%] [G loss: 5.303071]\n",
      "epoch:20 step:16300 [D loss: 0.014624, acc.: 100.00%] [G loss: 5.798879]\n",
      "epoch:20 step:16301 [D loss: 0.078580, acc.: 97.66%] [G loss: 4.918150]\n",
      "epoch:20 step:16302 [D loss: 0.078955, acc.: 99.22%] [G loss: 4.459431]\n",
      "epoch:20 step:16303 [D loss: 0.019596, acc.: 100.00%] [G loss: 5.631149]\n",
      "epoch:20 step:16304 [D loss: 0.141027, acc.: 96.09%] [G loss: 2.075700]\n",
      "epoch:20 step:16305 [D loss: 0.689487, acc.: 65.62%] [G loss: 8.645597]\n",
      "epoch:20 step:16306 [D loss: 1.642371, acc.: 51.56%] [G loss: 1.812479]\n",
      "epoch:20 step:16307 [D loss: 0.698208, acc.: 67.97%] [G loss: 7.011814]\n",
      "epoch:20 step:16308 [D loss: 0.222583, acc.: 89.84%] [G loss: 6.649314]\n",
      "epoch:20 step:16309 [D loss: 0.507253, acc.: 71.88%] [G loss: 0.804134]\n",
      "epoch:20 step:16310 [D loss: 0.381737, acc.: 81.25%] [G loss: 4.534511]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:20 step:16311 [D loss: 0.010412, acc.: 100.00%] [G loss: 5.229709]\n",
      "epoch:20 step:16312 [D loss: 0.027548, acc.: 100.00%] [G loss: 4.105352]\n",
      "epoch:20 step:16313 [D loss: 0.028965, acc.: 99.22%] [G loss: 2.224160]\n",
      "epoch:20 step:16314 [D loss: 0.038145, acc.: 99.22%] [G loss: 2.751980]\n",
      "epoch:20 step:16315 [D loss: 0.038580, acc.: 99.22%] [G loss: 1.185463]\n",
      "epoch:20 step:16316 [D loss: 0.046516, acc.: 100.00%] [G loss: 1.151495]\n",
      "epoch:20 step:16317 [D loss: 0.043456, acc.: 100.00%] [G loss: 3.241986]\n",
      "epoch:20 step:16318 [D loss: 0.494139, acc.: 75.78%] [G loss: 5.912523]\n",
      "epoch:20 step:16319 [D loss: 0.227289, acc.: 86.72%] [G loss: 4.295235]\n",
      "epoch:20 step:16320 [D loss: 0.169656, acc.: 92.97%] [G loss: 0.906421]\n",
      "epoch:20 step:16321 [D loss: 0.065410, acc.: 97.66%] [G loss: 2.713654]\n",
      "epoch:20 step:16322 [D loss: 0.032935, acc.: 100.00%] [G loss: 3.488656]\n",
      "epoch:20 step:16323 [D loss: 0.063858, acc.: 99.22%] [G loss: 3.798805]\n",
      "epoch:20 step:16324 [D loss: 0.057169, acc.: 97.66%] [G loss: 3.581248]\n",
      "epoch:20 step:16325 [D loss: 0.086867, acc.: 99.22%] [G loss: 4.589765]\n",
      "epoch:20 step:16326 [D loss: 0.009182, acc.: 100.00%] [G loss: 5.104897]\n",
      "epoch:20 step:16327 [D loss: 0.464723, acc.: 79.69%] [G loss: 5.147833]\n",
      "epoch:20 step:16328 [D loss: 0.005612, acc.: 100.00%] [G loss: 5.795788]\n",
      "epoch:20 step:16329 [D loss: 0.020978, acc.: 100.00%] [G loss: 6.259543]\n",
      "epoch:20 step:16330 [D loss: 0.135550, acc.: 96.09%] [G loss: 5.352463]\n",
      "epoch:20 step:16331 [D loss: 0.045693, acc.: 99.22%] [G loss: 4.059322]\n",
      "epoch:20 step:16332 [D loss: 0.124020, acc.: 95.31%] [G loss: 6.012776]\n",
      "epoch:20 step:16333 [D loss: 1.783664, acc.: 32.81%] [G loss: 3.722919]\n",
      "epoch:20 step:16334 [D loss: 0.047355, acc.: 98.44%] [G loss: 4.647138]\n",
      "epoch:20 step:16335 [D loss: 0.384631, acc.: 85.16%] [G loss: 3.823490]\n",
      "epoch:20 step:16336 [D loss: 0.165851, acc.: 92.97%] [G loss: 4.840186]\n",
      "epoch:20 step:16337 [D loss: 0.007538, acc.: 100.00%] [G loss: 6.295614]\n",
      "epoch:20 step:16338 [D loss: 0.024024, acc.: 99.22%] [G loss: 6.184396]\n",
      "epoch:20 step:16339 [D loss: 0.015489, acc.: 100.00%] [G loss: 4.051110]\n",
      "epoch:20 step:16340 [D loss: 0.028895, acc.: 100.00%] [G loss: 3.284670]\n",
      "epoch:20 step:16341 [D loss: 0.029898, acc.: 100.00%] [G loss: 5.186434]\n",
      "epoch:20 step:16342 [D loss: 0.053617, acc.: 100.00%] [G loss: 3.459860]\n",
      "epoch:20 step:16343 [D loss: 0.067431, acc.: 99.22%] [G loss: 4.070827]\n",
      "epoch:20 step:16344 [D loss: 0.098419, acc.: 96.88%] [G loss: 3.338248]\n",
      "epoch:20 step:16345 [D loss: 0.439205, acc.: 80.47%] [G loss: 5.158533]\n",
      "epoch:20 step:16346 [D loss: 0.023739, acc.: 100.00%] [G loss: 7.304132]\n",
      "epoch:20 step:16347 [D loss: 0.147783, acc.: 92.19%] [G loss: 3.293178]\n",
      "epoch:20 step:16348 [D loss: 0.096672, acc.: 96.88%] [G loss: 3.844603]\n",
      "epoch:20 step:16349 [D loss: 0.004905, acc.: 100.00%] [G loss: 4.997175]\n",
      "epoch:20 step:16350 [D loss: 0.007952, acc.: 100.00%] [G loss: 4.448702]\n",
      "epoch:20 step:16351 [D loss: 0.042096, acc.: 98.44%] [G loss: 4.908522]\n",
      "epoch:20 step:16352 [D loss: 0.035026, acc.: 100.00%] [G loss: 4.193015]\n",
      "epoch:20 step:16353 [D loss: 0.123755, acc.: 96.88%] [G loss: 5.495734]\n",
      "epoch:20 step:16354 [D loss: 0.012441, acc.: 100.00%] [G loss: 5.576225]\n",
      "epoch:20 step:16355 [D loss: 1.107527, acc.: 47.66%] [G loss: 7.742445]\n",
      "epoch:20 step:16356 [D loss: 0.058911, acc.: 98.44%] [G loss: 8.073054]\n",
      "epoch:20 step:16357 [D loss: 1.478656, acc.: 51.56%] [G loss: 2.944304]\n",
      "epoch:20 step:16358 [D loss: 0.838543, acc.: 71.88%] [G loss: 5.496233]\n",
      "epoch:20 step:16359 [D loss: 0.142465, acc.: 93.75%] [G loss: 6.447236]\n",
      "epoch:20 step:16360 [D loss: 0.275893, acc.: 87.50%] [G loss: 4.584116]\n",
      "epoch:20 step:16361 [D loss: 0.053268, acc.: 98.44%] [G loss: 5.200186]\n",
      "epoch:20 step:16362 [D loss: 0.036312, acc.: 100.00%] [G loss: 4.056605]\n",
      "epoch:20 step:16363 [D loss: 0.028635, acc.: 100.00%] [G loss: 3.602170]\n",
      "epoch:20 step:16364 [D loss: 0.058027, acc.: 99.22%] [G loss: 3.371248]\n",
      "epoch:20 step:16365 [D loss: 0.038131, acc.: 100.00%] [G loss: 4.182487]\n",
      "epoch:20 step:16366 [D loss: 0.048466, acc.: 99.22%] [G loss: 4.033659]\n",
      "epoch:20 step:16367 [D loss: 0.020667, acc.: 100.00%] [G loss: 2.160190]\n",
      "epoch:20 step:16368 [D loss: 0.015929, acc.: 100.00%] [G loss: 2.385873]\n",
      "epoch:20 step:16369 [D loss: 0.123202, acc.: 98.44%] [G loss: 3.976450]\n",
      "epoch:20 step:16370 [D loss: 0.142730, acc.: 94.53%] [G loss: 2.948081]\n",
      "epoch:20 step:16371 [D loss: 0.023524, acc.: 100.00%] [G loss: 2.617126]\n",
      "epoch:20 step:16372 [D loss: 0.104103, acc.: 97.66%] [G loss: 4.962792]\n",
      "epoch:20 step:16373 [D loss: 0.053215, acc.: 99.22%] [G loss: 4.258718]\n",
      "epoch:20 step:16374 [D loss: 0.058133, acc.: 98.44%] [G loss: 3.352005]\n",
      "epoch:20 step:16375 [D loss: 0.042013, acc.: 99.22%] [G loss: 3.483154]\n",
      "epoch:20 step:16376 [D loss: 0.009729, acc.: 100.00%] [G loss: 3.091165]\n",
      "epoch:20 step:16377 [D loss: 0.018479, acc.: 100.00%] [G loss: 3.247733]\n",
      "epoch:20 step:16378 [D loss: 0.020654, acc.: 100.00%] [G loss: 3.592248]\n",
      "epoch:20 step:16379 [D loss: 0.023262, acc.: 100.00%] [G loss: 3.461016]\n",
      "epoch:20 step:16380 [D loss: 0.098100, acc.: 98.44%] [G loss: 2.807300]\n",
      "epoch:20 step:16381 [D loss: 0.016975, acc.: 100.00%] [G loss: 3.756444]\n",
      "epoch:20 step:16382 [D loss: 0.358673, acc.: 87.50%] [G loss: 2.570130]\n",
      "epoch:20 step:16383 [D loss: 0.008910, acc.: 100.00%] [G loss: 3.571018]\n",
      "epoch:20 step:16384 [D loss: 0.157565, acc.: 94.53%] [G loss: 5.143025]\n",
      "epoch:20 step:16385 [D loss: 0.225769, acc.: 91.41%] [G loss: 4.571213]\n",
      "epoch:20 step:16386 [D loss: 0.027554, acc.: 100.00%] [G loss: 4.289990]\n",
      "epoch:20 step:16387 [D loss: 0.021363, acc.: 100.00%] [G loss: 3.424341]\n",
      "epoch:20 step:16388 [D loss: 0.107899, acc.: 98.44%] [G loss: 3.952594]\n",
      "epoch:20 step:16389 [D loss: 0.012123, acc.: 100.00%] [G loss: 4.392913]\n",
      "epoch:20 step:16390 [D loss: 0.060070, acc.: 98.44%] [G loss: 2.671690]\n",
      "epoch:20 step:16391 [D loss: 0.123248, acc.: 96.09%] [G loss: 5.320461]\n",
      "epoch:20 step:16392 [D loss: 0.029143, acc.: 99.22%] [G loss: 5.759531]\n",
      "epoch:20 step:16393 [D loss: 0.021429, acc.: 100.00%] [G loss: 4.076015]\n",
      "epoch:20 step:16394 [D loss: 0.008958, acc.: 100.00%] [G loss: 4.457482]\n",
      "epoch:20 step:16395 [D loss: 0.008535, acc.: 100.00%] [G loss: 2.859947]\n",
      "epoch:20 step:16396 [D loss: 0.028779, acc.: 100.00%] [G loss: 4.689161]\n",
      "epoch:20 step:16397 [D loss: 0.393469, acc.: 79.69%] [G loss: 7.757966]\n",
      "epoch:20 step:16398 [D loss: 1.841166, acc.: 50.78%] [G loss: 2.930311]\n",
      "epoch:20 step:16399 [D loss: 0.067569, acc.: 99.22%] [G loss: 3.110179]\n",
      "epoch:20 step:16400 [D loss: 0.721562, acc.: 67.97%] [G loss: 7.202847]\n",
      "##############\n",
      "[0.96756582 0.88894441 0.65720065 1.05767088 0.82292615 0.90363375\n",
      " 0.97072374 2.11664524 1.10299573 1.01218622]\n",
      "##########\n",
      "epoch:20 step:16401 [D loss: 2.147728, acc.: 50.00%] [G loss: 5.234585]\n",
      "epoch:21 step:16402 [D loss: 1.061980, acc.: 53.91%] [G loss: 2.186813]\n",
      "epoch:21 step:16403 [D loss: 0.195967, acc.: 92.97%] [G loss: 1.767932]\n",
      "epoch:21 step:16404 [D loss: 0.715750, acc.: 64.84%] [G loss: 4.251655]\n",
      "epoch:21 step:16405 [D loss: 0.453907, acc.: 70.31%] [G loss: 3.968588]\n",
      "epoch:21 step:16406 [D loss: 0.544818, acc.: 68.75%] [G loss: 2.020212]\n",
      "epoch:21 step:16407 [D loss: 0.386433, acc.: 81.25%] [G loss: 3.192284]\n",
      "epoch:21 step:16408 [D loss: 0.105686, acc.: 98.44%] [G loss: 3.029843]\n",
      "epoch:21 step:16409 [D loss: 0.094381, acc.: 98.44%] [G loss: 3.000841]\n",
      "epoch:21 step:16410 [D loss: 0.138433, acc.: 97.66%] [G loss: 2.575359]\n",
      "epoch:21 step:16411 [D loss: 0.129536, acc.: 96.88%] [G loss: 2.802899]\n",
      "epoch:21 step:16412 [D loss: 0.235672, acc.: 93.75%] [G loss: 1.851640]\n",
      "epoch:21 step:16413 [D loss: 0.194908, acc.: 93.75%] [G loss: 2.575633]\n",
      "epoch:21 step:16414 [D loss: 0.097134, acc.: 98.44%] [G loss: 2.435557]\n",
      "epoch:21 step:16415 [D loss: 0.144984, acc.: 96.09%] [G loss: 2.545798]\n",
      "epoch:21 step:16416 [D loss: 0.214399, acc.: 92.19%] [G loss: 3.035807]\n",
      "epoch:21 step:16417 [D loss: 0.219798, acc.: 92.19%] [G loss: 1.807906]\n",
      "epoch:21 step:16418 [D loss: 0.140096, acc.: 97.66%] [G loss: 2.564295]\n",
      "epoch:21 step:16419 [D loss: 0.150338, acc.: 96.88%] [G loss: 3.093909]\n",
      "epoch:21 step:16420 [D loss: 0.205006, acc.: 92.19%] [G loss: 2.735711]\n",
      "epoch:21 step:16421 [D loss: 0.114747, acc.: 98.44%] [G loss: 3.487746]\n",
      "epoch:21 step:16422 [D loss: 0.172637, acc.: 94.53%] [G loss: 2.994429]\n",
      "epoch:21 step:16423 [D loss: 0.051720, acc.: 99.22%] [G loss: 2.434615]\n",
      "epoch:21 step:16424 [D loss: 0.232614, acc.: 93.75%] [G loss: 2.472550]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:21 step:16425 [D loss: 0.076474, acc.: 99.22%] [G loss: 2.008384]\n",
      "epoch:21 step:16426 [D loss: 0.099586, acc.: 97.66%] [G loss: 2.895033]\n",
      "epoch:21 step:16427 [D loss: 0.120616, acc.: 97.66%] [G loss: 3.473998]\n",
      "epoch:21 step:16428 [D loss: 0.105280, acc.: 98.44%] [G loss: 2.978864]\n",
      "epoch:21 step:16429 [D loss: 0.070183, acc.: 99.22%] [G loss: 2.656927]\n",
      "epoch:21 step:16430 [D loss: 0.207105, acc.: 91.41%] [G loss: 4.253552]\n",
      "epoch:21 step:16431 [D loss: 0.028701, acc.: 100.00%] [G loss: 4.540015]\n",
      "epoch:21 step:16432 [D loss: 0.138862, acc.: 95.31%] [G loss: 3.190414]\n",
      "epoch:21 step:16433 [D loss: 0.611911, acc.: 70.31%] [G loss: 6.279425]\n",
      "epoch:21 step:16434 [D loss: 0.701607, acc.: 68.75%] [G loss: 4.700482]\n",
      "epoch:21 step:16435 [D loss: 0.116707, acc.: 96.09%] [G loss: 3.044991]\n",
      "epoch:21 step:16436 [D loss: 0.075496, acc.: 99.22%] [G loss: 2.643319]\n",
      "epoch:21 step:16437 [D loss: 0.082428, acc.: 96.88%] [G loss: 3.368016]\n",
      "epoch:21 step:16438 [D loss: 0.060230, acc.: 98.44%] [G loss: 3.866549]\n",
      "epoch:21 step:16439 [D loss: 0.080334, acc.: 97.66%] [G loss: 3.529602]\n",
      "epoch:21 step:16440 [D loss: 0.076991, acc.: 99.22%] [G loss: 2.614219]\n",
      "epoch:21 step:16441 [D loss: 0.029062, acc.: 100.00%] [G loss: 2.856646]\n",
      "epoch:21 step:16442 [D loss: 0.463257, acc.: 78.12%] [G loss: 5.768824]\n",
      "epoch:21 step:16443 [D loss: 0.912642, acc.: 64.06%] [G loss: 4.315272]\n",
      "epoch:21 step:16444 [D loss: 0.023972, acc.: 100.00%] [G loss: 3.174701]\n",
      "epoch:21 step:16445 [D loss: 0.086934, acc.: 98.44%] [G loss: 3.299799]\n",
      "epoch:21 step:16446 [D loss: 0.085512, acc.: 99.22%] [G loss: 2.876040]\n",
      "epoch:21 step:16447 [D loss: 0.049792, acc.: 99.22%] [G loss: 3.195361]\n",
      "epoch:21 step:16448 [D loss: 0.053679, acc.: 98.44%] [G loss: 3.387314]\n",
      "epoch:21 step:16449 [D loss: 0.077359, acc.: 100.00%] [G loss: 3.432950]\n",
      "epoch:21 step:16450 [D loss: 0.060261, acc.: 99.22%] [G loss: 3.258070]\n",
      "epoch:21 step:16451 [D loss: 0.296482, acc.: 89.06%] [G loss: 3.892503]\n",
      "epoch:21 step:16452 [D loss: 0.021660, acc.: 99.22%] [G loss: 4.934975]\n",
      "epoch:21 step:16453 [D loss: 0.187177, acc.: 93.75%] [G loss: 3.233199]\n",
      "epoch:21 step:16454 [D loss: 0.040668, acc.: 100.00%] [G loss: 3.094611]\n",
      "epoch:21 step:16455 [D loss: 0.063878, acc.: 99.22%] [G loss: 3.231039]\n",
      "epoch:21 step:16456 [D loss: 0.046957, acc.: 99.22%] [G loss: 3.600394]\n",
      "epoch:21 step:16457 [D loss: 0.043439, acc.: 100.00%] [G loss: 2.811721]\n",
      "epoch:21 step:16458 [D loss: 0.038783, acc.: 100.00%] [G loss: 3.254988]\n",
      "epoch:21 step:16459 [D loss: 0.054649, acc.: 100.00%] [G loss: 3.852741]\n",
      "epoch:21 step:16460 [D loss: 0.058276, acc.: 99.22%] [G loss: 4.509501]\n",
      "epoch:21 step:16461 [D loss: 0.046587, acc.: 100.00%] [G loss: 4.670576]\n",
      "epoch:21 step:16462 [D loss: 0.019782, acc.: 100.00%] [G loss: 3.963421]\n",
      "epoch:21 step:16463 [D loss: 0.079360, acc.: 98.44%] [G loss: 2.928788]\n",
      "epoch:21 step:16464 [D loss: 0.140946, acc.: 96.88%] [G loss: 3.732473]\n",
      "epoch:21 step:16465 [D loss: 0.025233, acc.: 100.00%] [G loss: 3.868927]\n",
      "epoch:21 step:16466 [D loss: 0.105766, acc.: 98.44%] [G loss: 2.515473]\n",
      "epoch:21 step:16467 [D loss: 0.090977, acc.: 98.44%] [G loss: 4.464874]\n",
      "epoch:21 step:16468 [D loss: 0.011530, acc.: 100.00%] [G loss: 4.861663]\n",
      "epoch:21 step:16469 [D loss: 0.016284, acc.: 100.00%] [G loss: 4.960390]\n",
      "epoch:21 step:16470 [D loss: 0.023100, acc.: 100.00%] [G loss: 5.132497]\n",
      "epoch:21 step:16471 [D loss: 0.020918, acc.: 100.00%] [G loss: 4.377437]\n",
      "epoch:21 step:16472 [D loss: 0.027585, acc.: 100.00%] [G loss: 3.868030]\n",
      "epoch:21 step:16473 [D loss: 0.015469, acc.: 100.00%] [G loss: 3.713115]\n",
      "epoch:21 step:16474 [D loss: 0.043084, acc.: 99.22%] [G loss: 3.662353]\n",
      "epoch:21 step:16475 [D loss: 0.020948, acc.: 100.00%] [G loss: 4.008537]\n",
      "epoch:21 step:16476 [D loss: 0.030423, acc.: 100.00%] [G loss: 4.596052]\n",
      "epoch:21 step:16477 [D loss: 0.039084, acc.: 100.00%] [G loss: 3.526404]\n",
      "epoch:21 step:16478 [D loss: 0.057669, acc.: 99.22%] [G loss: 3.611368]\n",
      "epoch:21 step:16479 [D loss: 0.052367, acc.: 100.00%] [G loss: 5.388015]\n",
      "epoch:21 step:16480 [D loss: 0.032067, acc.: 100.00%] [G loss: 5.813544]\n",
      "epoch:21 step:16481 [D loss: 0.033082, acc.: 99.22%] [G loss: 3.702527]\n",
      "epoch:21 step:16482 [D loss: 0.022108, acc.: 100.00%] [G loss: 5.166815]\n",
      "epoch:21 step:16483 [D loss: 0.004031, acc.: 100.00%] [G loss: 5.916005]\n",
      "epoch:21 step:16484 [D loss: 0.033773, acc.: 99.22%] [G loss: 3.555191]\n",
      "epoch:21 step:16485 [D loss: 0.007911, acc.: 100.00%] [G loss: 3.474102]\n",
      "epoch:21 step:16486 [D loss: 0.159310, acc.: 94.53%] [G loss: 5.712988]\n",
      "epoch:21 step:16487 [D loss: 0.123492, acc.: 94.53%] [G loss: 4.813492]\n",
      "epoch:21 step:16488 [D loss: 0.037780, acc.: 98.44%] [G loss: 5.324086]\n",
      "epoch:21 step:16489 [D loss: 0.025746, acc.: 99.22%] [G loss: 5.238802]\n",
      "epoch:21 step:16490 [D loss: 0.022113, acc.: 100.00%] [G loss: 5.907439]\n",
      "epoch:21 step:16491 [D loss: 4.581737, acc.: 14.06%] [G loss: 7.851687]\n",
      "epoch:21 step:16492 [D loss: 2.280511, acc.: 50.78%] [G loss: 5.120149]\n",
      "epoch:21 step:16493 [D loss: 0.666736, acc.: 67.19%] [G loss: 2.768773]\n",
      "epoch:21 step:16494 [D loss: 0.289212, acc.: 89.84%] [G loss: 2.263782]\n",
      "epoch:21 step:16495 [D loss: 0.115188, acc.: 98.44%] [G loss: 2.602512]\n",
      "epoch:21 step:16496 [D loss: 0.059101, acc.: 98.44%] [G loss: 2.963497]\n",
      "epoch:21 step:16497 [D loss: 0.224990, acc.: 94.53%] [G loss: 2.628192]\n",
      "epoch:21 step:16498 [D loss: 0.294212, acc.: 90.62%] [G loss: 1.657011]\n",
      "epoch:21 step:16499 [D loss: 0.113297, acc.: 97.66%] [G loss: 2.912021]\n",
      "epoch:21 step:16500 [D loss: 0.145543, acc.: 98.44%] [G loss: 3.319203]\n",
      "epoch:21 step:16501 [D loss: 0.093741, acc.: 98.44%] [G loss: 3.142922]\n",
      "epoch:21 step:16502 [D loss: 0.063563, acc.: 100.00%] [G loss: 3.372499]\n",
      "epoch:21 step:16503 [D loss: 0.305483, acc.: 88.28%] [G loss: 2.123938]\n",
      "epoch:21 step:16504 [D loss: 0.168871, acc.: 95.31%] [G loss: 2.028928]\n",
      "epoch:21 step:16505 [D loss: 0.105168, acc.: 98.44%] [G loss: 3.461359]\n",
      "epoch:21 step:16506 [D loss: 0.050859, acc.: 100.00%] [G loss: 2.702625]\n",
      "epoch:21 step:16507 [D loss: 0.062733, acc.: 100.00%] [G loss: 3.301633]\n",
      "epoch:21 step:16508 [D loss: 0.375026, acc.: 83.59%] [G loss: 4.637556]\n",
      "epoch:21 step:16509 [D loss: 0.282455, acc.: 88.28%] [G loss: 1.924498]\n",
      "epoch:21 step:16510 [D loss: 0.132444, acc.: 95.31%] [G loss: 3.074770]\n",
      "epoch:21 step:16511 [D loss: 0.044926, acc.: 99.22%] [G loss: 2.688377]\n",
      "epoch:21 step:16512 [D loss: 0.151670, acc.: 95.31%] [G loss: 2.799499]\n",
      "epoch:21 step:16513 [D loss: 0.094475, acc.: 98.44%] [G loss: 3.862239]\n",
      "epoch:21 step:16514 [D loss: 0.050539, acc.: 100.00%] [G loss: 3.325424]\n",
      "epoch:21 step:16515 [D loss: 0.235853, acc.: 90.62%] [G loss: 3.627031]\n",
      "epoch:21 step:16516 [D loss: 0.242102, acc.: 89.84%] [G loss: 3.292017]\n",
      "epoch:21 step:16517 [D loss: 0.238890, acc.: 92.19%] [G loss: 2.629594]\n",
      "epoch:21 step:16518 [D loss: 0.049595, acc.: 99.22%] [G loss: 3.683505]\n",
      "epoch:21 step:16519 [D loss: 0.043715, acc.: 100.00%] [G loss: 4.027934]\n",
      "epoch:21 step:16520 [D loss: 0.060279, acc.: 99.22%] [G loss: 3.332790]\n",
      "epoch:21 step:16521 [D loss: 0.034345, acc.: 99.22%] [G loss: 3.080560]\n",
      "epoch:21 step:16522 [D loss: 0.042735, acc.: 100.00%] [G loss: 3.051197]\n",
      "epoch:21 step:16523 [D loss: 0.059226, acc.: 99.22%] [G loss: 2.860584]\n",
      "epoch:21 step:16524 [D loss: 0.080085, acc.: 98.44%] [G loss: 3.258235]\n",
      "epoch:21 step:16525 [D loss: 0.093706, acc.: 99.22%] [G loss: 3.325018]\n",
      "epoch:21 step:16526 [D loss: 0.063311, acc.: 99.22%] [G loss: 3.202854]\n",
      "epoch:21 step:16527 [D loss: 0.066842, acc.: 99.22%] [G loss: 1.938175]\n",
      "epoch:21 step:16528 [D loss: 0.071550, acc.: 98.44%] [G loss: 3.424906]\n",
      "epoch:21 step:16529 [D loss: 0.036792, acc.: 100.00%] [G loss: 4.313071]\n",
      "epoch:21 step:16530 [D loss: 0.207365, acc.: 91.41%] [G loss: 3.797037]\n",
      "epoch:21 step:16531 [D loss: 0.032548, acc.: 99.22%] [G loss: 4.282779]\n",
      "epoch:21 step:16532 [D loss: 0.115765, acc.: 97.66%] [G loss: 4.663208]\n",
      "epoch:21 step:16533 [D loss: 0.211846, acc.: 92.97%] [G loss: 4.222738]\n",
      "epoch:21 step:16534 [D loss: 0.006357, acc.: 100.00%] [G loss: 3.324778]\n",
      "epoch:21 step:16535 [D loss: 0.053768, acc.: 98.44%] [G loss: 4.598665]\n",
      "epoch:21 step:16536 [D loss: 0.017860, acc.: 100.00%] [G loss: 3.497029]\n",
      "epoch:21 step:16537 [D loss: 0.043290, acc.: 99.22%] [G loss: 2.531906]\n",
      "epoch:21 step:16538 [D loss: 0.025194, acc.: 100.00%] [G loss: 3.612433]\n",
      "epoch:21 step:16539 [D loss: 0.041999, acc.: 98.44%] [G loss: 2.901788]\n",
      "epoch:21 step:16540 [D loss: 0.151208, acc.: 95.31%] [G loss: 6.004460]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:21 step:16541 [D loss: 2.132130, acc.: 20.31%] [G loss: 8.706856]\n",
      "epoch:21 step:16542 [D loss: 1.279412, acc.: 53.91%] [G loss: 3.879931]\n",
      "epoch:21 step:16543 [D loss: 0.275349, acc.: 86.72%] [G loss: 4.012383]\n",
      "epoch:21 step:16544 [D loss: 0.155316, acc.: 92.97%] [G loss: 3.694161]\n",
      "epoch:21 step:16545 [D loss: 0.043456, acc.: 100.00%] [G loss: 4.665282]\n",
      "epoch:21 step:16546 [D loss: 0.109017, acc.: 98.44%] [G loss: 3.673534]\n",
      "epoch:21 step:16547 [D loss: 0.091410, acc.: 98.44%] [G loss: 4.796793]\n",
      "epoch:21 step:16548 [D loss: 0.024617, acc.: 100.00%] [G loss: 4.276133]\n",
      "epoch:21 step:16549 [D loss: 0.047984, acc.: 99.22%] [G loss: 4.103315]\n",
      "epoch:21 step:16550 [D loss: 0.048217, acc.: 98.44%] [G loss: 3.678587]\n",
      "epoch:21 step:16551 [D loss: 0.116144, acc.: 96.09%] [G loss: 3.175191]\n",
      "epoch:21 step:16552 [D loss: 0.032227, acc.: 100.00%] [G loss: 4.022956]\n",
      "epoch:21 step:16553 [D loss: 0.105630, acc.: 97.66%] [G loss: 2.632351]\n",
      "epoch:21 step:16554 [D loss: 0.437317, acc.: 78.91%] [G loss: 5.169792]\n",
      "epoch:21 step:16555 [D loss: 0.095395, acc.: 95.31%] [G loss: 5.608056]\n",
      "epoch:21 step:16556 [D loss: 0.067147, acc.: 98.44%] [G loss: 5.832015]\n",
      "epoch:21 step:16557 [D loss: 0.049669, acc.: 98.44%] [G loss: 4.398729]\n",
      "epoch:21 step:16558 [D loss: 0.056258, acc.: 97.66%] [G loss: 3.665803]\n",
      "epoch:21 step:16559 [D loss: 0.164151, acc.: 96.09%] [G loss: 3.047299]\n",
      "epoch:21 step:16560 [D loss: 0.031643, acc.: 100.00%] [G loss: 3.220467]\n",
      "epoch:21 step:16561 [D loss: 0.021910, acc.: 100.00%] [G loss: 3.898572]\n",
      "epoch:21 step:16562 [D loss: 0.114378, acc.: 96.88%] [G loss: 2.226687]\n",
      "epoch:21 step:16563 [D loss: 0.088018, acc.: 96.09%] [G loss: 3.586952]\n",
      "epoch:21 step:16564 [D loss: 0.015882, acc.: 100.00%] [G loss: 5.157637]\n",
      "epoch:21 step:16565 [D loss: 0.043776, acc.: 99.22%] [G loss: 3.412808]\n",
      "epoch:21 step:16566 [D loss: 0.038748, acc.: 100.00%] [G loss: 3.195395]\n",
      "epoch:21 step:16567 [D loss: 0.040053, acc.: 98.44%] [G loss: 2.475796]\n",
      "epoch:21 step:16568 [D loss: 0.157602, acc.: 93.75%] [G loss: 4.975128]\n",
      "epoch:21 step:16569 [D loss: 0.041048, acc.: 98.44%] [G loss: 6.039921]\n",
      "epoch:21 step:16570 [D loss: 0.055318, acc.: 99.22%] [G loss: 4.440709]\n",
      "epoch:21 step:16571 [D loss: 0.340872, acc.: 85.94%] [G loss: 3.647166]\n",
      "epoch:21 step:16572 [D loss: 0.012654, acc.: 100.00%] [G loss: 4.872396]\n",
      "epoch:21 step:16573 [D loss: 0.054162, acc.: 99.22%] [G loss: 5.126947]\n",
      "epoch:21 step:16574 [D loss: 0.025604, acc.: 100.00%] [G loss: 3.433141]\n",
      "epoch:21 step:16575 [D loss: 0.053364, acc.: 98.44%] [G loss: 4.492364]\n",
      "epoch:21 step:16576 [D loss: 0.017280, acc.: 100.00%] [G loss: 4.002306]\n",
      "epoch:21 step:16577 [D loss: 0.844313, acc.: 53.12%] [G loss: 5.389285]\n",
      "epoch:21 step:16578 [D loss: 0.042434, acc.: 100.00%] [G loss: 7.049616]\n",
      "epoch:21 step:16579 [D loss: 0.546939, acc.: 77.34%] [G loss: 2.372676]\n",
      "epoch:21 step:16580 [D loss: 0.488609, acc.: 77.34%] [G loss: 6.906607]\n",
      "epoch:21 step:16581 [D loss: 0.004413, acc.: 100.00%] [G loss: 8.937222]\n",
      "epoch:21 step:16582 [D loss: 0.477879, acc.: 78.12%] [G loss: 4.913162]\n",
      "epoch:21 step:16583 [D loss: 0.056673, acc.: 96.88%] [G loss: 5.436842]\n",
      "epoch:21 step:16584 [D loss: 0.037577, acc.: 100.00%] [G loss: 4.755692]\n",
      "epoch:21 step:16585 [D loss: 0.004191, acc.: 100.00%] [G loss: 4.773755]\n",
      "epoch:21 step:16586 [D loss: 0.005339, acc.: 100.00%] [G loss: 3.668559]\n",
      "epoch:21 step:16587 [D loss: 0.057343, acc.: 98.44%] [G loss: 5.140985]\n",
      "epoch:21 step:16588 [D loss: 0.060248, acc.: 97.66%] [G loss: 4.318255]\n",
      "epoch:21 step:16589 [D loss: 0.140514, acc.: 96.09%] [G loss: 4.692005]\n",
      "epoch:21 step:16590 [D loss: 0.004667, acc.: 100.00%] [G loss: 6.398773]\n",
      "epoch:21 step:16591 [D loss: 0.089851, acc.: 96.88%] [G loss: 3.571186]\n",
      "epoch:21 step:16592 [D loss: 0.257595, acc.: 93.75%] [G loss: 3.341247]\n",
      "epoch:21 step:16593 [D loss: 0.019751, acc.: 100.00%] [G loss: 4.076295]\n",
      "epoch:21 step:16594 [D loss: 0.027143, acc.: 99.22%] [G loss: 4.052711]\n",
      "epoch:21 step:16595 [D loss: 0.016907, acc.: 100.00%] [G loss: 2.683719]\n",
      "epoch:21 step:16596 [D loss: 0.044810, acc.: 99.22%] [G loss: 3.414809]\n",
      "epoch:21 step:16597 [D loss: 0.025744, acc.: 100.00%] [G loss: 3.987322]\n",
      "epoch:21 step:16598 [D loss: 0.039409, acc.: 100.00%] [G loss: 3.221416]\n",
      "epoch:21 step:16599 [D loss: 0.016149, acc.: 100.00%] [G loss: 3.656719]\n",
      "epoch:21 step:16600 [D loss: 0.047035, acc.: 100.00%] [G loss: 3.039698]\n",
      "##############\n",
      "[0.97402323 1.06639049 2.11179578 0.91592236 1.02950354 1.11653493\n",
      " 2.10262111 2.11224339 2.10479729 0.89420418]\n",
      "##########\n",
      "epoch:21 step:16601 [D loss: 0.114587, acc.: 99.22%] [G loss: 2.992055]\n",
      "epoch:21 step:16602 [D loss: 0.007192, acc.: 100.00%] [G loss: 3.487616]\n",
      "epoch:21 step:16603 [D loss: 0.020319, acc.: 100.00%] [G loss: 3.623152]\n",
      "epoch:21 step:16604 [D loss: 0.013987, acc.: 100.00%] [G loss: 4.574532]\n",
      "epoch:21 step:16605 [D loss: 0.033060, acc.: 100.00%] [G loss: 3.700740]\n",
      "epoch:21 step:16606 [D loss: 0.079887, acc.: 98.44%] [G loss: 2.208261]\n",
      "epoch:21 step:16607 [D loss: 0.059764, acc.: 99.22%] [G loss: 2.332016]\n",
      "epoch:21 step:16608 [D loss: 0.014382, acc.: 100.00%] [G loss: 4.297196]\n",
      "epoch:21 step:16609 [D loss: 0.057841, acc.: 100.00%] [G loss: 2.412855]\n",
      "epoch:21 step:16610 [D loss: 0.042587, acc.: 98.44%] [G loss: 3.031446]\n",
      "epoch:21 step:16611 [D loss: 0.186267, acc.: 93.75%] [G loss: 4.279886]\n",
      "epoch:21 step:16612 [D loss: 0.002404, acc.: 100.00%] [G loss: 5.969080]\n",
      "epoch:21 step:16613 [D loss: 0.114318, acc.: 96.09%] [G loss: 1.305226]\n",
      "epoch:21 step:16614 [D loss: 0.017345, acc.: 100.00%] [G loss: 5.412129]\n",
      "epoch:21 step:16615 [D loss: 0.616547, acc.: 74.22%] [G loss: 2.490496]\n",
      "epoch:21 step:16616 [D loss: 0.005283, acc.: 100.00%] [G loss: 6.996993]\n",
      "epoch:21 step:16617 [D loss: 0.019533, acc.: 99.22%] [G loss: 6.844895]\n",
      "epoch:21 step:16618 [D loss: 0.119996, acc.: 95.31%] [G loss: 2.174242]\n",
      "epoch:21 step:16619 [D loss: 0.071712, acc.: 99.22%] [G loss: 6.526937]\n",
      "epoch:21 step:16620 [D loss: 0.004632, acc.: 100.00%] [G loss: 7.129917]\n",
      "epoch:21 step:16621 [D loss: 0.148011, acc.: 93.75%] [G loss: 2.544444]\n",
      "epoch:21 step:16622 [D loss: 0.191691, acc.: 90.62%] [G loss: 7.775915]\n",
      "epoch:21 step:16623 [D loss: 0.022682, acc.: 100.00%] [G loss: 7.815576]\n",
      "epoch:21 step:16624 [D loss: 0.558416, acc.: 74.22%] [G loss: 1.224962]\n",
      "epoch:21 step:16625 [D loss: 1.009801, acc.: 58.59%] [G loss: 8.271495]\n",
      "epoch:21 step:16626 [D loss: 0.259337, acc.: 88.28%] [G loss: 8.277672]\n",
      "epoch:21 step:16627 [D loss: 0.173094, acc.: 90.62%] [G loss: 6.166783]\n",
      "epoch:21 step:16628 [D loss: 0.008661, acc.: 100.00%] [G loss: 6.473378]\n",
      "epoch:21 step:16629 [D loss: 0.011810, acc.: 100.00%] [G loss: 4.596585]\n",
      "epoch:21 step:16630 [D loss: 0.018564, acc.: 100.00%] [G loss: 3.401459]\n",
      "epoch:21 step:16631 [D loss: 0.017296, acc.: 100.00%] [G loss: 3.490581]\n",
      "epoch:21 step:16632 [D loss: 0.027984, acc.: 100.00%] [G loss: 3.431192]\n",
      "epoch:21 step:16633 [D loss: 0.024264, acc.: 100.00%] [G loss: 5.165548]\n",
      "epoch:21 step:16634 [D loss: 0.044518, acc.: 98.44%] [G loss: 5.281376]\n",
      "epoch:21 step:16635 [D loss: 0.027923, acc.: 100.00%] [G loss: 4.736713]\n",
      "epoch:21 step:16636 [D loss: 0.020767, acc.: 100.00%] [G loss: 3.882068]\n",
      "epoch:21 step:16637 [D loss: 0.098881, acc.: 95.31%] [G loss: 4.067140]\n",
      "epoch:21 step:16638 [D loss: 0.113720, acc.: 96.09%] [G loss: 4.525708]\n",
      "epoch:21 step:16639 [D loss: 0.010628, acc.: 100.00%] [G loss: 3.857191]\n",
      "epoch:21 step:16640 [D loss: 0.082329, acc.: 95.31%] [G loss: 4.466720]\n",
      "epoch:21 step:16641 [D loss: 1.108800, acc.: 42.97%] [G loss: 6.698301]\n",
      "epoch:21 step:16642 [D loss: 0.856297, acc.: 62.50%] [G loss: 5.791273]\n",
      "epoch:21 step:16643 [D loss: 0.018007, acc.: 100.00%] [G loss: 3.372466]\n",
      "epoch:21 step:16644 [D loss: 0.064999, acc.: 98.44%] [G loss: 2.707301]\n",
      "epoch:21 step:16645 [D loss: 0.015575, acc.: 100.00%] [G loss: 3.849036]\n",
      "epoch:21 step:16646 [D loss: 0.007292, acc.: 100.00%] [G loss: 3.109875]\n",
      "epoch:21 step:16647 [D loss: 0.117923, acc.: 96.09%] [G loss: 2.962111]\n",
      "epoch:21 step:16648 [D loss: 0.157150, acc.: 95.31%] [G loss: 2.782291]\n",
      "epoch:21 step:16649 [D loss: 0.027041, acc.: 100.00%] [G loss: 2.193851]\n",
      "epoch:21 step:16650 [D loss: 0.060695, acc.: 99.22%] [G loss: 4.402889]\n",
      "epoch:21 step:16651 [D loss: 0.061636, acc.: 97.66%] [G loss: 3.239177]\n",
      "epoch:21 step:16652 [D loss: 0.015374, acc.: 100.00%] [G loss: 3.602329]\n",
      "epoch:21 step:16653 [D loss: 0.098102, acc.: 97.66%] [G loss: 3.807317]\n",
      "epoch:21 step:16654 [D loss: 0.013357, acc.: 100.00%] [G loss: 4.150800]\n",
      "epoch:21 step:16655 [D loss: 0.017919, acc.: 100.00%] [G loss: 5.287410]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:21 step:16656 [D loss: 0.016666, acc.: 100.00%] [G loss: 4.580659]\n",
      "epoch:21 step:16657 [D loss: 0.016835, acc.: 100.00%] [G loss: 4.033434]\n",
      "epoch:21 step:16658 [D loss: 0.030003, acc.: 100.00%] [G loss: 2.957677]\n",
      "epoch:21 step:16659 [D loss: 0.109712, acc.: 97.66%] [G loss: 5.345338]\n",
      "epoch:21 step:16660 [D loss: 0.014429, acc.: 100.00%] [G loss: 5.427037]\n",
      "epoch:21 step:16661 [D loss: 0.106822, acc.: 96.09%] [G loss: 2.555319]\n",
      "epoch:21 step:16662 [D loss: 0.407330, acc.: 78.12%] [G loss: 5.924068]\n",
      "epoch:21 step:16663 [D loss: 0.033285, acc.: 99.22%] [G loss: 7.769937]\n",
      "epoch:21 step:16664 [D loss: 0.152629, acc.: 91.41%] [G loss: 4.930586]\n",
      "epoch:21 step:16665 [D loss: 0.014502, acc.: 100.00%] [G loss: 4.708065]\n",
      "epoch:21 step:16666 [D loss: 0.033712, acc.: 99.22%] [G loss: 3.321249]\n",
      "epoch:21 step:16667 [D loss: 0.050742, acc.: 99.22%] [G loss: 4.879515]\n",
      "epoch:21 step:16668 [D loss: 0.025989, acc.: 99.22%] [G loss: 4.314174]\n",
      "epoch:21 step:16669 [D loss: 0.016334, acc.: 100.00%] [G loss: 4.050621]\n",
      "epoch:21 step:16670 [D loss: 0.011518, acc.: 100.00%] [G loss: 3.969690]\n",
      "epoch:21 step:16671 [D loss: 0.237916, acc.: 90.62%] [G loss: 5.000920]\n",
      "epoch:21 step:16672 [D loss: 0.025225, acc.: 100.00%] [G loss: 5.962611]\n",
      "epoch:21 step:16673 [D loss: 0.012656, acc.: 100.00%] [G loss: 4.721303]\n",
      "epoch:21 step:16674 [D loss: 0.155884, acc.: 94.53%] [G loss: 5.079381]\n",
      "epoch:21 step:16675 [D loss: 0.013244, acc.: 100.00%] [G loss: 6.506559]\n",
      "epoch:21 step:16676 [D loss: 0.013211, acc.: 100.00%] [G loss: 4.911094]\n",
      "epoch:21 step:16677 [D loss: 0.015668, acc.: 100.00%] [G loss: 5.132489]\n",
      "epoch:21 step:16678 [D loss: 0.026827, acc.: 100.00%] [G loss: 5.171371]\n",
      "epoch:21 step:16679 [D loss: 0.011825, acc.: 100.00%] [G loss: 5.724400]\n",
      "epoch:21 step:16680 [D loss: 0.070951, acc.: 98.44%] [G loss: 3.448337]\n",
      "epoch:21 step:16681 [D loss: 0.134113, acc.: 94.53%] [G loss: 3.979331]\n",
      "epoch:21 step:16682 [D loss: 0.011401, acc.: 100.00%] [G loss: 7.189937]\n",
      "epoch:21 step:16683 [D loss: 0.111465, acc.: 97.66%] [G loss: 5.133985]\n",
      "epoch:21 step:16684 [D loss: 0.032623, acc.: 100.00%] [G loss: 5.261152]\n",
      "epoch:21 step:16685 [D loss: 0.008929, acc.: 100.00%] [G loss: 4.730428]\n",
      "epoch:21 step:16686 [D loss: 0.013614, acc.: 100.00%] [G loss: 3.531824]\n",
      "epoch:21 step:16687 [D loss: 1.968498, acc.: 32.81%] [G loss: 8.686695]\n",
      "epoch:21 step:16688 [D loss: 2.398823, acc.: 50.00%] [G loss: 5.088082]\n",
      "epoch:21 step:16689 [D loss: 0.615104, acc.: 71.88%] [G loss: 2.361020]\n",
      "epoch:21 step:16690 [D loss: 0.307117, acc.: 85.94%] [G loss: 2.995317]\n",
      "epoch:21 step:16691 [D loss: 0.125083, acc.: 96.09%] [G loss: 3.111093]\n",
      "epoch:21 step:16692 [D loss: 0.199809, acc.: 92.97%] [G loss: 3.300678]\n",
      "epoch:21 step:16693 [D loss: 0.068367, acc.: 100.00%] [G loss: 3.536810]\n",
      "epoch:21 step:16694 [D loss: 0.130376, acc.: 96.88%] [G loss: 3.121659]\n",
      "epoch:21 step:16695 [D loss: 0.055476, acc.: 100.00%] [G loss: 3.144566]\n",
      "epoch:21 step:16696 [D loss: 0.099782, acc.: 99.22%] [G loss: 2.664647]\n",
      "epoch:21 step:16697 [D loss: 0.240779, acc.: 94.53%] [G loss: 4.311652]\n",
      "epoch:21 step:16698 [D loss: 0.451883, acc.: 78.12%] [G loss: 2.669168]\n",
      "epoch:21 step:16699 [D loss: 0.193872, acc.: 95.31%] [G loss: 3.025815]\n",
      "epoch:21 step:16700 [D loss: 0.201878, acc.: 92.19%] [G loss: 4.117858]\n",
      "epoch:21 step:16701 [D loss: 0.164850, acc.: 95.31%] [G loss: 2.280839]\n",
      "epoch:21 step:16702 [D loss: 0.065569, acc.: 98.44%] [G loss: 3.033276]\n",
      "epoch:21 step:16703 [D loss: 0.267816, acc.: 91.41%] [G loss: 2.837643]\n",
      "epoch:21 step:16704 [D loss: 0.091445, acc.: 96.09%] [G loss: 3.306040]\n",
      "epoch:21 step:16705 [D loss: 0.089866, acc.: 97.66%] [G loss: 1.836772]\n",
      "epoch:21 step:16706 [D loss: 0.306280, acc.: 85.16%] [G loss: 4.503750]\n",
      "epoch:21 step:16707 [D loss: 0.130749, acc.: 96.88%] [G loss: 3.404488]\n",
      "epoch:21 step:16708 [D loss: 0.159033, acc.: 95.31%] [G loss: 1.984071]\n",
      "epoch:21 step:16709 [D loss: 0.081040, acc.: 98.44%] [G loss: 1.345664]\n",
      "epoch:21 step:16710 [D loss: 0.055135, acc.: 100.00%] [G loss: 2.213781]\n",
      "epoch:21 step:16711 [D loss: 0.106219, acc.: 96.88%] [G loss: 3.621279]\n",
      "epoch:21 step:16712 [D loss: 0.168973, acc.: 90.62%] [G loss: 1.636928]\n",
      "epoch:21 step:16713 [D loss: 0.416047, acc.: 79.69%] [G loss: 5.028686]\n",
      "epoch:21 step:16714 [D loss: 0.255576, acc.: 87.50%] [G loss: 4.915396]\n",
      "epoch:21 step:16715 [D loss: 0.047573, acc.: 100.00%] [G loss: 3.099483]\n",
      "epoch:21 step:16716 [D loss: 0.183765, acc.: 92.97%] [G loss: 2.113479]\n",
      "epoch:21 step:16717 [D loss: 0.282763, acc.: 86.72%] [G loss: 6.034343]\n",
      "epoch:21 step:16718 [D loss: 0.088766, acc.: 96.88%] [G loss: 6.631170]\n",
      "epoch:21 step:16719 [D loss: 0.022771, acc.: 100.00%] [G loss: 6.003789]\n",
      "epoch:21 step:16720 [D loss: 0.024528, acc.: 100.00%] [G loss: 5.191140]\n",
      "epoch:21 step:16721 [D loss: 0.073334, acc.: 99.22%] [G loss: 4.058095]\n",
      "epoch:21 step:16722 [D loss: 0.042590, acc.: 98.44%] [G loss: 3.330490]\n",
      "epoch:21 step:16723 [D loss: 0.016625, acc.: 100.00%] [G loss: 3.243667]\n",
      "epoch:21 step:16724 [D loss: 0.022894, acc.: 100.00%] [G loss: 4.456398]\n",
      "epoch:21 step:16725 [D loss: 0.147954, acc.: 94.53%] [G loss: 4.472645]\n",
      "epoch:21 step:16726 [D loss: 0.079274, acc.: 98.44%] [G loss: 4.499396]\n",
      "epoch:21 step:16727 [D loss: 0.023964, acc.: 100.00%] [G loss: 3.577828]\n",
      "epoch:21 step:16728 [D loss: 0.023579, acc.: 100.00%] [G loss: 4.679068]\n",
      "epoch:21 step:16729 [D loss: 0.017500, acc.: 100.00%] [G loss: 4.233950]\n",
      "epoch:21 step:16730 [D loss: 0.011400, acc.: 100.00%] [G loss: 3.879334]\n",
      "epoch:21 step:16731 [D loss: 0.017872, acc.: 100.00%] [G loss: 3.676822]\n",
      "epoch:21 step:16732 [D loss: 0.102243, acc.: 97.66%] [G loss: 4.097278]\n",
      "epoch:21 step:16733 [D loss: 0.032613, acc.: 99.22%] [G loss: 4.926142]\n",
      "epoch:21 step:16734 [D loss: 0.021213, acc.: 100.00%] [G loss: 4.109093]\n",
      "epoch:21 step:16735 [D loss: 0.018827, acc.: 100.00%] [G loss: 3.654775]\n",
      "epoch:21 step:16736 [D loss: 0.027024, acc.: 100.00%] [G loss: 3.484418]\n",
      "epoch:21 step:16737 [D loss: 2.136946, acc.: 35.16%] [G loss: 7.157224]\n",
      "epoch:21 step:16738 [D loss: 1.646432, acc.: 50.78%] [G loss: 3.873639]\n",
      "epoch:21 step:16739 [D loss: 0.222578, acc.: 89.84%] [G loss: 3.482671]\n",
      "epoch:21 step:16740 [D loss: 0.233439, acc.: 92.97%] [G loss: 3.098252]\n",
      "epoch:21 step:16741 [D loss: 0.097275, acc.: 98.44%] [G loss: 3.844573]\n",
      "epoch:21 step:16742 [D loss: 0.193749, acc.: 93.75%] [G loss: 3.585586]\n",
      "epoch:21 step:16743 [D loss: 0.177244, acc.: 95.31%] [G loss: 3.994977]\n",
      "epoch:21 step:16744 [D loss: 0.195151, acc.: 92.19%] [G loss: 3.056466]\n",
      "epoch:21 step:16745 [D loss: 0.094436, acc.: 97.66%] [G loss: 3.511826]\n",
      "epoch:21 step:16746 [D loss: 0.219660, acc.: 92.19%] [G loss: 3.292996]\n",
      "epoch:21 step:16747 [D loss: 0.220389, acc.: 93.75%] [G loss: 2.757847]\n",
      "epoch:21 step:16748 [D loss: 0.134384, acc.: 96.09%] [G loss: 5.018378]\n",
      "epoch:21 step:16749 [D loss: 0.185105, acc.: 92.97%] [G loss: 2.338987]\n",
      "epoch:21 step:16750 [D loss: 0.676949, acc.: 63.28%] [G loss: 6.719631]\n",
      "epoch:21 step:16751 [D loss: 0.825441, acc.: 59.38%] [G loss: 4.787184]\n",
      "epoch:21 step:16752 [D loss: 0.246087, acc.: 87.50%] [G loss: 4.209749]\n",
      "epoch:21 step:16753 [D loss: 0.029176, acc.: 100.00%] [G loss: 4.584229]\n",
      "epoch:21 step:16754 [D loss: 0.077575, acc.: 99.22%] [G loss: 3.267465]\n",
      "epoch:21 step:16755 [D loss: 0.103090, acc.: 96.88%] [G loss: 3.515624]\n",
      "epoch:21 step:16756 [D loss: 0.133065, acc.: 96.88%] [G loss: 3.081866]\n",
      "epoch:21 step:16757 [D loss: 0.130898, acc.: 96.88%] [G loss: 4.494918]\n",
      "epoch:21 step:16758 [D loss: 0.117845, acc.: 95.31%] [G loss: 3.622893]\n",
      "epoch:21 step:16759 [D loss: 0.352360, acc.: 84.38%] [G loss: 4.049270]\n",
      "epoch:21 step:16760 [D loss: 0.109205, acc.: 98.44%] [G loss: 3.880118]\n",
      "epoch:21 step:16761 [D loss: 0.103366, acc.: 97.66%] [G loss: 2.314310]\n",
      "epoch:21 step:16762 [D loss: 0.149887, acc.: 97.66%] [G loss: 4.172940]\n",
      "epoch:21 step:16763 [D loss: 0.122367, acc.: 94.53%] [G loss: 3.225812]\n",
      "epoch:21 step:16764 [D loss: 0.031450, acc.: 100.00%] [G loss: 2.682398]\n",
      "epoch:21 step:16765 [D loss: 0.285003, acc.: 87.50%] [G loss: 4.179305]\n",
      "epoch:21 step:16766 [D loss: 0.014283, acc.: 100.00%] [G loss: 5.000058]\n",
      "epoch:21 step:16767 [D loss: 0.282977, acc.: 85.94%] [G loss: 1.675432]\n",
      "epoch:21 step:16768 [D loss: 0.025695, acc.: 100.00%] [G loss: 1.455359]\n",
      "epoch:21 step:16769 [D loss: 0.038915, acc.: 100.00%] [G loss: 1.348984]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:21 step:16770 [D loss: 0.079794, acc.: 98.44%] [G loss: 4.007204]\n",
      "epoch:21 step:16771 [D loss: 0.166416, acc.: 93.75%] [G loss: 1.197826]\n",
      "epoch:21 step:16772 [D loss: 0.422655, acc.: 77.34%] [G loss: 5.872578]\n",
      "epoch:21 step:16773 [D loss: 0.387596, acc.: 84.38%] [G loss: 5.569228]\n",
      "epoch:21 step:16774 [D loss: 0.652927, acc.: 72.66%] [G loss: 1.559458]\n",
      "epoch:21 step:16775 [D loss: 0.919571, acc.: 61.72%] [G loss: 6.112901]\n",
      "epoch:21 step:16776 [D loss: 0.167403, acc.: 90.62%] [G loss: 7.444142]\n",
      "epoch:21 step:16777 [D loss: 0.712403, acc.: 68.75%] [G loss: 3.846493]\n",
      "epoch:21 step:16778 [D loss: 0.039938, acc.: 100.00%] [G loss: 3.411026]\n",
      "epoch:21 step:16779 [D loss: 0.115360, acc.: 96.09%] [G loss: 3.917560]\n",
      "epoch:21 step:16780 [D loss: 0.020403, acc.: 100.00%] [G loss: 3.630463]\n",
      "epoch:21 step:16781 [D loss: 0.054723, acc.: 98.44%] [G loss: 3.812718]\n",
      "epoch:21 step:16782 [D loss: 0.056652, acc.: 99.22%] [G loss: 4.860583]\n",
      "epoch:21 step:16783 [D loss: 0.158226, acc.: 92.97%] [G loss: 2.262763]\n",
      "epoch:21 step:16784 [D loss: 0.127246, acc.: 96.88%] [G loss: 3.388650]\n",
      "epoch:21 step:16785 [D loss: 0.080926, acc.: 97.66%] [G loss: 3.490968]\n",
      "epoch:21 step:16786 [D loss: 0.290897, acc.: 89.84%] [G loss: 4.067145]\n",
      "epoch:21 step:16787 [D loss: 0.220809, acc.: 93.75%] [G loss: 2.979876]\n",
      "epoch:21 step:16788 [D loss: 0.061736, acc.: 98.44%] [G loss: 3.067125]\n",
      "epoch:21 step:16789 [D loss: 0.071559, acc.: 100.00%] [G loss: 2.770962]\n",
      "epoch:21 step:16790 [D loss: 0.070938, acc.: 99.22%] [G loss: 2.369722]\n",
      "epoch:21 step:16791 [D loss: 0.056073, acc.: 98.44%] [G loss: 3.463083]\n",
      "epoch:21 step:16792 [D loss: 0.137086, acc.: 97.66%] [G loss: 1.945694]\n",
      "epoch:21 step:16793 [D loss: 0.023357, acc.: 100.00%] [G loss: 3.253345]\n",
      "epoch:21 step:16794 [D loss: 0.209936, acc.: 93.75%] [G loss: 2.870913]\n",
      "epoch:21 step:16795 [D loss: 0.026480, acc.: 100.00%] [G loss: 3.319781]\n",
      "epoch:21 step:16796 [D loss: 0.465950, acc.: 75.78%] [G loss: 5.155467]\n",
      "epoch:21 step:16797 [D loss: 0.134049, acc.: 92.97%] [G loss: 4.366580]\n",
      "epoch:21 step:16798 [D loss: 0.045028, acc.: 99.22%] [G loss: 4.305877]\n",
      "epoch:21 step:16799 [D loss: 0.027876, acc.: 100.00%] [G loss: 3.845708]\n",
      "epoch:21 step:16800 [D loss: 0.006027, acc.: 100.00%] [G loss: 4.767620]\n",
      "##############\n",
      "[0.92411001 1.06409964 1.05114378 1.00765531 0.75467478 0.96251445\n",
      " 0.889557   0.95640756 2.09444939 0.98317337]\n",
      "##########\n",
      "epoch:21 step:16801 [D loss: 0.017742, acc.: 100.00%] [G loss: 3.901326]\n",
      "epoch:21 step:16802 [D loss: 0.021027, acc.: 100.00%] [G loss: 3.309442]\n",
      "epoch:21 step:16803 [D loss: 0.141429, acc.: 94.53%] [G loss: 5.112700]\n",
      "epoch:21 step:16804 [D loss: 0.129110, acc.: 96.09%] [G loss: 4.842438]\n",
      "epoch:21 step:16805 [D loss: 0.010735, acc.: 100.00%] [G loss: 4.459832]\n",
      "epoch:21 step:16806 [D loss: 0.042405, acc.: 99.22%] [G loss: 3.233416]\n",
      "epoch:21 step:16807 [D loss: 0.137229, acc.: 95.31%] [G loss: 5.275728]\n",
      "epoch:21 step:16808 [D loss: 0.013984, acc.: 100.00%] [G loss: 5.485483]\n",
      "epoch:21 step:16809 [D loss: 0.127408, acc.: 94.53%] [G loss: 3.534239]\n",
      "epoch:21 step:16810 [D loss: 0.024465, acc.: 100.00%] [G loss: 2.830055]\n",
      "epoch:21 step:16811 [D loss: 0.037115, acc.: 99.22%] [G loss: 3.492649]\n",
      "epoch:21 step:16812 [D loss: 0.371557, acc.: 81.25%] [G loss: 5.537689]\n",
      "epoch:21 step:16813 [D loss: 0.116242, acc.: 95.31%] [G loss: 6.181598]\n",
      "epoch:21 step:16814 [D loss: 0.084728, acc.: 97.66%] [G loss: 4.377017]\n",
      "epoch:21 step:16815 [D loss: 0.008591, acc.: 100.00%] [G loss: 4.383284]\n",
      "epoch:21 step:16816 [D loss: 0.023310, acc.: 99.22%] [G loss: 4.472047]\n",
      "epoch:21 step:16817 [D loss: 0.023875, acc.: 100.00%] [G loss: 3.961759]\n",
      "epoch:21 step:16818 [D loss: 0.048493, acc.: 100.00%] [G loss: 3.603052]\n",
      "epoch:21 step:16819 [D loss: 0.054500, acc.: 98.44%] [G loss: 4.152997]\n",
      "epoch:21 step:16820 [D loss: 0.012700, acc.: 100.00%] [G loss: 5.063167]\n",
      "epoch:21 step:16821 [D loss: 0.191576, acc.: 92.97%] [G loss: 3.854044]\n",
      "epoch:21 step:16822 [D loss: 0.031914, acc.: 100.00%] [G loss: 4.210809]\n",
      "epoch:21 step:16823 [D loss: 0.029900, acc.: 100.00%] [G loss: 5.035577]\n",
      "epoch:21 step:16824 [D loss: 0.019672, acc.: 100.00%] [G loss: 4.173623]\n",
      "epoch:21 step:16825 [D loss: 0.252824, acc.: 86.72%] [G loss: 3.245921]\n",
      "epoch:21 step:16826 [D loss: 0.037344, acc.: 98.44%] [G loss: 4.944653]\n",
      "epoch:21 step:16827 [D loss: 0.018238, acc.: 100.00%] [G loss: 3.826784]\n",
      "epoch:21 step:16828 [D loss: 0.018059, acc.: 100.00%] [G loss: 4.487898]\n",
      "epoch:21 step:16829 [D loss: 0.028878, acc.: 99.22%] [G loss: 3.057297]\n",
      "epoch:21 step:16830 [D loss: 0.211915, acc.: 92.19%] [G loss: 6.037453]\n",
      "epoch:21 step:16831 [D loss: 0.760924, acc.: 61.72%] [G loss: 5.595493]\n",
      "epoch:21 step:16832 [D loss: 0.116299, acc.: 93.75%] [G loss: 5.176557]\n",
      "epoch:21 step:16833 [D loss: 0.008239, acc.: 100.00%] [G loss: 4.227811]\n",
      "epoch:21 step:16834 [D loss: 0.066651, acc.: 96.09%] [G loss: 3.925432]\n",
      "epoch:21 step:16835 [D loss: 0.072827, acc.: 98.44%] [G loss: 4.326748]\n",
      "epoch:21 step:16836 [D loss: 0.043196, acc.: 99.22%] [G loss: 3.962815]\n",
      "epoch:21 step:16837 [D loss: 0.021563, acc.: 100.00%] [G loss: 2.751190]\n",
      "epoch:21 step:16838 [D loss: 0.062375, acc.: 100.00%] [G loss: 3.599454]\n",
      "epoch:21 step:16839 [D loss: 0.087620, acc.: 97.66%] [G loss: 2.983924]\n",
      "epoch:21 step:16840 [D loss: 0.039284, acc.: 100.00%] [G loss: 3.929526]\n",
      "epoch:21 step:16841 [D loss: 0.072377, acc.: 97.66%] [G loss: 3.378571]\n",
      "epoch:21 step:16842 [D loss: 0.194728, acc.: 93.75%] [G loss: 5.582723]\n",
      "epoch:21 step:16843 [D loss: 0.077034, acc.: 98.44%] [G loss: 5.905099]\n",
      "epoch:21 step:16844 [D loss: 0.075611, acc.: 96.88%] [G loss: 5.128065]\n",
      "epoch:21 step:16845 [D loss: 0.032732, acc.: 100.00%] [G loss: 4.304464]\n",
      "epoch:21 step:16846 [D loss: 0.010843, acc.: 100.00%] [G loss: 4.739114]\n",
      "epoch:21 step:16847 [D loss: 0.012134, acc.: 100.00%] [G loss: 4.909754]\n",
      "epoch:21 step:16848 [D loss: 0.034319, acc.: 100.00%] [G loss: 3.809213]\n",
      "epoch:21 step:16849 [D loss: 0.253737, acc.: 87.50%] [G loss: 6.489240]\n",
      "epoch:21 step:16850 [D loss: 0.019581, acc.: 99.22%] [G loss: 7.319335]\n",
      "epoch:21 step:16851 [D loss: 0.182291, acc.: 93.75%] [G loss: 4.311006]\n",
      "epoch:21 step:16852 [D loss: 0.018089, acc.: 99.22%] [G loss: 2.609661]\n",
      "epoch:21 step:16853 [D loss: 0.193093, acc.: 89.84%] [G loss: 6.384353]\n",
      "epoch:21 step:16854 [D loss: 0.145991, acc.: 93.75%] [G loss: 6.084081]\n",
      "epoch:21 step:16855 [D loss: 0.005619, acc.: 100.00%] [G loss: 5.474556]\n",
      "epoch:21 step:16856 [D loss: 0.003275, acc.: 100.00%] [G loss: 4.822868]\n",
      "epoch:21 step:16857 [D loss: 0.010226, acc.: 100.00%] [G loss: 4.690031]\n",
      "epoch:21 step:16858 [D loss: 0.004540, acc.: 100.00%] [G loss: 5.029903]\n",
      "epoch:21 step:16859 [D loss: 0.034635, acc.: 100.00%] [G loss: 5.460725]\n",
      "epoch:21 step:16860 [D loss: 0.063906, acc.: 99.22%] [G loss: 4.126871]\n",
      "epoch:21 step:16861 [D loss: 0.069105, acc.: 97.66%] [G loss: 3.291372]\n",
      "epoch:21 step:16862 [D loss: 0.103870, acc.: 96.09%] [G loss: 5.087851]\n",
      "epoch:21 step:16863 [D loss: 0.005087, acc.: 100.00%] [G loss: 5.679913]\n",
      "epoch:21 step:16864 [D loss: 0.006921, acc.: 100.00%] [G loss: 6.160987]\n",
      "epoch:21 step:16865 [D loss: 0.158855, acc.: 91.41%] [G loss: 3.555928]\n",
      "epoch:21 step:16866 [D loss: 0.060521, acc.: 98.44%] [G loss: 3.463117]\n",
      "epoch:21 step:16867 [D loss: 0.017993, acc.: 100.00%] [G loss: 4.506298]\n",
      "epoch:21 step:16868 [D loss: 0.010975, acc.: 100.00%] [G loss: 5.580547]\n",
      "epoch:21 step:16869 [D loss: 0.027785, acc.: 99.22%] [G loss: 4.567241]\n",
      "epoch:21 step:16870 [D loss: 0.007338, acc.: 100.00%] [G loss: 5.133532]\n",
      "epoch:21 step:16871 [D loss: 0.018945, acc.: 99.22%] [G loss: 4.739684]\n",
      "epoch:21 step:16872 [D loss: 0.012069, acc.: 100.00%] [G loss: 3.784288]\n",
      "epoch:21 step:16873 [D loss: 0.033957, acc.: 100.00%] [G loss: 3.581052]\n",
      "epoch:21 step:16874 [D loss: 0.006078, acc.: 100.00%] [G loss: 5.900415]\n",
      "epoch:21 step:16875 [D loss: 0.204526, acc.: 89.84%] [G loss: 5.513806]\n",
      "epoch:21 step:16876 [D loss: 0.428335, acc.: 78.91%] [G loss: 5.675105]\n",
      "epoch:21 step:16877 [D loss: 0.026913, acc.: 99.22%] [G loss: 7.439389]\n",
      "epoch:21 step:16878 [D loss: 0.412408, acc.: 79.69%] [G loss: 3.232814]\n",
      "epoch:21 step:16879 [D loss: 0.019619, acc.: 100.00%] [G loss: 4.003405]\n",
      "epoch:21 step:16880 [D loss: 0.022293, acc.: 100.00%] [G loss: 4.191192]\n",
      "epoch:21 step:16881 [D loss: 0.008382, acc.: 100.00%] [G loss: 3.203619]\n",
      "epoch:21 step:16882 [D loss: 0.050358, acc.: 100.00%] [G loss: 3.742749]\n",
      "epoch:21 step:16883 [D loss: 0.355473, acc.: 83.59%] [G loss: 8.241009]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:21 step:16884 [D loss: 2.338037, acc.: 50.00%] [G loss: 1.851555]\n",
      "epoch:21 step:16885 [D loss: 0.956598, acc.: 65.62%] [G loss: 6.365487]\n",
      "epoch:21 step:16886 [D loss: 0.830175, acc.: 66.41%] [G loss: 5.945383]\n",
      "epoch:21 step:16887 [D loss: 0.278213, acc.: 85.94%] [G loss: 3.939063]\n",
      "epoch:21 step:16888 [D loss: 0.060406, acc.: 97.66%] [G loss: 2.752323]\n",
      "epoch:21 step:16889 [D loss: 0.135032, acc.: 96.09%] [G loss: 5.160392]\n",
      "epoch:21 step:16890 [D loss: 0.026031, acc.: 100.00%] [G loss: 3.820657]\n",
      "epoch:21 step:16891 [D loss: 0.043585, acc.: 100.00%] [G loss: 2.190825]\n",
      "epoch:21 step:16892 [D loss: 0.154147, acc.: 94.53%] [G loss: 3.953918]\n",
      "epoch:21 step:16893 [D loss: 0.115111, acc.: 96.88%] [G loss: 3.441919]\n",
      "epoch:21 step:16894 [D loss: 0.045067, acc.: 99.22%] [G loss: 1.683494]\n",
      "epoch:21 step:16895 [D loss: 0.031498, acc.: 100.00%] [G loss: 3.994059]\n",
      "epoch:21 step:16896 [D loss: 0.080218, acc.: 97.66%] [G loss: 2.423978]\n",
      "epoch:21 step:16897 [D loss: 0.022945, acc.: 100.00%] [G loss: 4.824749]\n",
      "epoch:21 step:16898 [D loss: 0.083423, acc.: 98.44%] [G loss: 2.360453]\n",
      "epoch:21 step:16899 [D loss: 0.126192, acc.: 96.09%] [G loss: 3.807932]\n",
      "epoch:21 step:16900 [D loss: 0.148840, acc.: 96.09%] [G loss: 2.705525]\n",
      "epoch:21 step:16901 [D loss: 0.081893, acc.: 99.22%] [G loss: 0.694618]\n",
      "epoch:21 step:16902 [D loss: 0.039679, acc.: 100.00%] [G loss: 1.840128]\n",
      "epoch:21 step:16903 [D loss: 0.027585, acc.: 100.00%] [G loss: 0.931131]\n",
      "epoch:21 step:16904 [D loss: 0.028084, acc.: 100.00%] [G loss: 1.988166]\n",
      "epoch:21 step:16905 [D loss: 0.022488, acc.: 100.00%] [G loss: 4.380941]\n",
      "epoch:21 step:16906 [D loss: 0.066827, acc.: 98.44%] [G loss: 0.605077]\n",
      "epoch:21 step:16907 [D loss: 0.037796, acc.: 99.22%] [G loss: 1.373183]\n",
      "epoch:21 step:16908 [D loss: 0.097298, acc.: 99.22%] [G loss: 5.456961]\n",
      "epoch:21 step:16909 [D loss: 0.577720, acc.: 75.00%] [G loss: 2.992631]\n",
      "epoch:21 step:16910 [D loss: 0.013845, acc.: 100.00%] [G loss: 2.016380]\n",
      "epoch:21 step:16911 [D loss: 0.019515, acc.: 100.00%] [G loss: 3.054725]\n",
      "epoch:21 step:16912 [D loss: 0.030079, acc.: 100.00%] [G loss: 1.878653]\n",
      "epoch:21 step:16913 [D loss: 0.036076, acc.: 99.22%] [G loss: 2.692578]\n",
      "epoch:21 step:16914 [D loss: 0.039314, acc.: 99.22%] [G loss: 3.001605]\n",
      "epoch:21 step:16915 [D loss: 0.009791, acc.: 100.00%] [G loss: 2.807731]\n",
      "epoch:21 step:16916 [D loss: 0.025045, acc.: 100.00%] [G loss: 1.493270]\n",
      "epoch:21 step:16917 [D loss: 0.128316, acc.: 97.66%] [G loss: 4.535695]\n",
      "epoch:21 step:16918 [D loss: 0.018657, acc.: 100.00%] [G loss: 5.235820]\n",
      "epoch:21 step:16919 [D loss: 0.074224, acc.: 98.44%] [G loss: 3.853766]\n",
      "epoch:21 step:16920 [D loss: 0.102021, acc.: 98.44%] [G loss: 4.739250]\n",
      "epoch:21 step:16921 [D loss: 0.005629, acc.: 100.00%] [G loss: 4.643954]\n",
      "epoch:21 step:16922 [D loss: 0.010884, acc.: 100.00%] [G loss: 3.042537]\n",
      "epoch:21 step:16923 [D loss: 0.022643, acc.: 100.00%] [G loss: 4.469069]\n",
      "epoch:21 step:16924 [D loss: 0.079831, acc.: 98.44%] [G loss: 3.688552]\n",
      "epoch:21 step:16925 [D loss: 0.057925, acc.: 99.22%] [G loss: 5.352168]\n",
      "epoch:21 step:16926 [D loss: 0.015857, acc.: 100.00%] [G loss: 5.264547]\n",
      "epoch:21 step:16927 [D loss: 0.892729, acc.: 55.47%] [G loss: 8.032951]\n",
      "epoch:21 step:16928 [D loss: 0.384723, acc.: 82.03%] [G loss: 6.600677]\n",
      "epoch:21 step:16929 [D loss: 0.816681, acc.: 65.62%] [G loss: 4.605296]\n",
      "epoch:21 step:16930 [D loss: 0.018533, acc.: 100.00%] [G loss: 4.800700]\n",
      "epoch:21 step:16931 [D loss: 0.029733, acc.: 100.00%] [G loss: 3.954906]\n",
      "epoch:21 step:16932 [D loss: 0.062966, acc.: 97.66%] [G loss: 4.974345]\n",
      "epoch:21 step:16933 [D loss: 0.076103, acc.: 97.66%] [G loss: 3.356139]\n",
      "epoch:21 step:16934 [D loss: 0.121370, acc.: 97.66%] [G loss: 4.067661]\n",
      "epoch:21 step:16935 [D loss: 0.028415, acc.: 100.00%] [G loss: 5.584001]\n",
      "epoch:21 step:16936 [D loss: 0.005207, acc.: 100.00%] [G loss: 4.376473]\n",
      "epoch:21 step:16937 [D loss: 0.071118, acc.: 99.22%] [G loss: 2.390486]\n",
      "epoch:21 step:16938 [D loss: 0.205366, acc.: 91.41%] [G loss: 2.563836]\n",
      "epoch:21 step:16939 [D loss: 0.003308, acc.: 100.00%] [G loss: 4.673784]\n",
      "epoch:21 step:16940 [D loss: 0.005874, acc.: 100.00%] [G loss: 4.163634]\n",
      "epoch:21 step:16941 [D loss: 0.010592, acc.: 100.00%] [G loss: 3.453544]\n",
      "epoch:21 step:16942 [D loss: 0.003446, acc.: 100.00%] [G loss: 3.507404]\n",
      "epoch:21 step:16943 [D loss: 0.404025, acc.: 79.69%] [G loss: 4.418393]\n",
      "epoch:21 step:16944 [D loss: 0.061213, acc.: 96.88%] [G loss: 5.768883]\n",
      "epoch:21 step:16945 [D loss: 0.011163, acc.: 100.00%] [G loss: 4.605098]\n",
      "epoch:21 step:16946 [D loss: 0.060298, acc.: 99.22%] [G loss: 5.428442]\n",
      "epoch:21 step:16947 [D loss: 0.109797, acc.: 93.75%] [G loss: 6.718707]\n",
      "epoch:21 step:16948 [D loss: 0.527879, acc.: 71.09%] [G loss: 7.368809]\n",
      "epoch:21 step:16949 [D loss: 0.014602, acc.: 99.22%] [G loss: 5.865621]\n",
      "epoch:21 step:16950 [D loss: 0.029868, acc.: 99.22%] [G loss: 5.582579]\n",
      "epoch:21 step:16951 [D loss: 0.018970, acc.: 100.00%] [G loss: 6.791407]\n",
      "epoch:21 step:16952 [D loss: 0.006582, acc.: 100.00%] [G loss: 3.481330]\n",
      "epoch:21 step:16953 [D loss: 0.040018, acc.: 99.22%] [G loss: 5.957977]\n",
      "epoch:21 step:16954 [D loss: 0.008187, acc.: 100.00%] [G loss: 5.698141]\n",
      "epoch:21 step:16955 [D loss: 0.037422, acc.: 100.00%] [G loss: 6.091279]\n",
      "epoch:21 step:16956 [D loss: 0.003089, acc.: 100.00%] [G loss: 4.776117]\n",
      "epoch:21 step:16957 [D loss: 0.011746, acc.: 100.00%] [G loss: 6.093363]\n",
      "epoch:21 step:16958 [D loss: 0.016080, acc.: 100.00%] [G loss: 5.359036]\n",
      "epoch:21 step:16959 [D loss: 0.176674, acc.: 94.53%] [G loss: 5.922859]\n",
      "epoch:21 step:16960 [D loss: 0.027576, acc.: 100.00%] [G loss: 6.667026]\n",
      "epoch:21 step:16961 [D loss: 0.025990, acc.: 99.22%] [G loss: 5.693338]\n",
      "epoch:21 step:16962 [D loss: 0.022120, acc.: 100.00%] [G loss: 4.630365]\n",
      "epoch:21 step:16963 [D loss: 0.056799, acc.: 97.66%] [G loss: 5.406591]\n",
      "epoch:21 step:16964 [D loss: 0.008089, acc.: 100.00%] [G loss: 4.911592]\n",
      "epoch:21 step:16965 [D loss: 0.016175, acc.: 100.00%] [G loss: 3.259856]\n",
      "epoch:21 step:16966 [D loss: 0.058024, acc.: 98.44%] [G loss: 1.526745]\n",
      "epoch:21 step:16967 [D loss: 0.250497, acc.: 90.62%] [G loss: 5.580156]\n",
      "epoch:21 step:16968 [D loss: 0.068980, acc.: 97.66%] [G loss: 7.300747]\n",
      "epoch:21 step:16969 [D loss: 0.318177, acc.: 79.69%] [G loss: 1.880130]\n",
      "epoch:21 step:16970 [D loss: 0.571131, acc.: 76.56%] [G loss: 8.960472]\n",
      "epoch:21 step:16971 [D loss: 1.425513, acc.: 56.25%] [G loss: 4.121499]\n",
      "epoch:21 step:16972 [D loss: 0.070979, acc.: 99.22%] [G loss: 3.387883]\n",
      "epoch:21 step:16973 [D loss: 0.025972, acc.: 100.00%] [G loss: 4.111408]\n",
      "epoch:21 step:16974 [D loss: 0.057404, acc.: 99.22%] [G loss: 3.159700]\n",
      "epoch:21 step:16975 [D loss: 0.067882, acc.: 99.22%] [G loss: 3.604678]\n",
      "epoch:21 step:16976 [D loss: 0.068366, acc.: 98.44%] [G loss: 2.949012]\n",
      "epoch:21 step:16977 [D loss: 0.095985, acc.: 99.22%] [G loss: 3.894044]\n",
      "epoch:21 step:16978 [D loss: 0.125266, acc.: 97.66%] [G loss: 3.938515]\n",
      "epoch:21 step:16979 [D loss: 0.019507, acc.: 100.00%] [G loss: 4.351064]\n",
      "epoch:21 step:16980 [D loss: 0.208639, acc.: 89.84%] [G loss: 3.177765]\n",
      "epoch:21 step:16981 [D loss: 0.085084, acc.: 97.66%] [G loss: 4.811309]\n",
      "epoch:21 step:16982 [D loss: 0.041738, acc.: 98.44%] [G loss: 5.285762]\n",
      "epoch:21 step:16983 [D loss: 0.030328, acc.: 99.22%] [G loss: 4.315800]\n",
      "epoch:21 step:16984 [D loss: 0.057187, acc.: 100.00%] [G loss: 3.118677]\n",
      "epoch:21 step:16985 [D loss: 0.043089, acc.: 100.00%] [G loss: 4.901727]\n",
      "epoch:21 step:16986 [D loss: 0.068541, acc.: 98.44%] [G loss: 4.444196]\n",
      "epoch:21 step:16987 [D loss: 0.033430, acc.: 99.22%] [G loss: 4.006670]\n",
      "epoch:21 step:16988 [D loss: 0.017612, acc.: 99.22%] [G loss: 4.741858]\n",
      "epoch:21 step:16989 [D loss: 0.004027, acc.: 100.00%] [G loss: 2.410388]\n",
      "epoch:21 step:16990 [D loss: 0.191002, acc.: 95.31%] [G loss: 5.995262]\n",
      "epoch:21 step:16991 [D loss: 0.260489, acc.: 88.28%] [G loss: 6.071149]\n",
      "epoch:21 step:16992 [D loss: 0.012242, acc.: 100.00%] [G loss: 5.804181]\n",
      "epoch:21 step:16993 [D loss: 0.009028, acc.: 100.00%] [G loss: 5.276141]\n",
      "epoch:21 step:16994 [D loss: 0.033684, acc.: 100.00%] [G loss: 5.441544]\n",
      "epoch:21 step:16995 [D loss: 0.011628, acc.: 100.00%] [G loss: 4.541222]\n",
      "epoch:21 step:16996 [D loss: 0.032369, acc.: 100.00%] [G loss: 5.742799]\n",
      "epoch:21 step:16997 [D loss: 0.075812, acc.: 99.22%] [G loss: 4.887729]\n",
      "epoch:21 step:16998 [D loss: 0.065652, acc.: 100.00%] [G loss: 5.623653]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:21 step:16999 [D loss: 0.016673, acc.: 100.00%] [G loss: 7.222594]\n",
      "epoch:21 step:17000 [D loss: 2.461181, acc.: 28.12%] [G loss: 9.450411]\n",
      "##############\n",
      "[0.90171859 0.9335683  0.82133567 0.98709677 0.83591961 1.1090804\n",
      " 2.11041219 1.10422138 2.10881254 2.10426582]\n",
      "##########\n",
      "epoch:21 step:17001 [D loss: 2.526626, acc.: 50.00%] [G loss: 6.828182]\n",
      "epoch:21 step:17002 [D loss: 0.179919, acc.: 91.41%] [G loss: 5.768257]\n",
      "epoch:21 step:17003 [D loss: 0.070023, acc.: 96.09%] [G loss: 3.201424]\n",
      "epoch:21 step:17004 [D loss: 0.054136, acc.: 98.44%] [G loss: 3.104486]\n",
      "epoch:21 step:17005 [D loss: 0.121738, acc.: 96.88%] [G loss: 4.523871]\n",
      "epoch:21 step:17006 [D loss: 0.022120, acc.: 100.00%] [G loss: 3.824381]\n",
      "epoch:21 step:17007 [D loss: 0.025272, acc.: 100.00%] [G loss: 3.035931]\n",
      "epoch:21 step:17008 [D loss: 0.083817, acc.: 98.44%] [G loss: 3.303069]\n",
      "epoch:21 step:17009 [D loss: 0.053800, acc.: 98.44%] [G loss: 4.512236]\n",
      "epoch:21 step:17010 [D loss: 0.163003, acc.: 94.53%] [G loss: 4.734044]\n",
      "epoch:21 step:17011 [D loss: 0.100823, acc.: 96.88%] [G loss: 4.934507]\n",
      "epoch:21 step:17012 [D loss: 0.032823, acc.: 99.22%] [G loss: 4.582369]\n",
      "epoch:21 step:17013 [D loss: 0.069206, acc.: 97.66%] [G loss: 3.724946]\n",
      "epoch:21 step:17014 [D loss: 0.880512, acc.: 57.03%] [G loss: 7.292199]\n",
      "epoch:21 step:17015 [D loss: 1.108112, acc.: 56.25%] [G loss: 3.839762]\n",
      "epoch:21 step:17016 [D loss: 0.338350, acc.: 85.16%] [G loss: 3.131137]\n",
      "epoch:21 step:17017 [D loss: 0.035855, acc.: 99.22%] [G loss: 4.002896]\n",
      "epoch:21 step:17018 [D loss: 0.112168, acc.: 95.31%] [G loss: 3.700811]\n",
      "epoch:21 step:17019 [D loss: 0.120732, acc.: 97.66%] [G loss: 3.945760]\n",
      "epoch:21 step:17020 [D loss: 0.118912, acc.: 99.22%] [G loss: 3.796103]\n",
      "epoch:21 step:17021 [D loss: 0.052765, acc.: 99.22%] [G loss: 3.637079]\n",
      "epoch:21 step:17022 [D loss: 0.053520, acc.: 99.22%] [G loss: 4.436134]\n",
      "epoch:21 step:17023 [D loss: 0.085078, acc.: 97.66%] [G loss: 3.900058]\n",
      "epoch:21 step:17024 [D loss: 0.248471, acc.: 91.41%] [G loss: 4.323854]\n",
      "epoch:21 step:17025 [D loss: 0.039387, acc.: 100.00%] [G loss: 4.720516]\n",
      "epoch:21 step:17026 [D loss: 0.043530, acc.: 100.00%] [G loss: 4.364623]\n",
      "epoch:21 step:17027 [D loss: 0.124016, acc.: 96.88%] [G loss: 3.608877]\n",
      "epoch:21 step:17028 [D loss: 0.413219, acc.: 75.78%] [G loss: 6.401352]\n",
      "epoch:21 step:17029 [D loss: 0.087820, acc.: 99.22%] [G loss: 6.925973]\n",
      "epoch:21 step:17030 [D loss: 0.235371, acc.: 84.38%] [G loss: 4.917523]\n",
      "epoch:21 step:17031 [D loss: 0.012482, acc.: 100.00%] [G loss: 3.568755]\n",
      "epoch:21 step:17032 [D loss: 0.107436, acc.: 97.66%] [G loss: 4.886197]\n",
      "epoch:21 step:17033 [D loss: 0.004121, acc.: 100.00%] [G loss: 5.323839]\n",
      "epoch:21 step:17034 [D loss: 0.006586, acc.: 100.00%] [G loss: 5.306098]\n",
      "epoch:21 step:17035 [D loss: 0.032097, acc.: 100.00%] [G loss: 4.829533]\n",
      "epoch:21 step:17036 [D loss: 0.020066, acc.: 100.00%] [G loss: 4.062577]\n",
      "epoch:21 step:17037 [D loss: 0.078386, acc.: 99.22%] [G loss: 4.199499]\n",
      "epoch:21 step:17038 [D loss: 0.014603, acc.: 100.00%] [G loss: 4.337045]\n",
      "epoch:21 step:17039 [D loss: 0.030367, acc.: 99.22%] [G loss: 3.544501]\n",
      "epoch:21 step:17040 [D loss: 0.050070, acc.: 99.22%] [G loss: 3.545899]\n",
      "epoch:21 step:17041 [D loss: 0.040142, acc.: 100.00%] [G loss: 3.868908]\n",
      "epoch:21 step:17042 [D loss: 0.040085, acc.: 99.22%] [G loss: 4.960382]\n",
      "epoch:21 step:17043 [D loss: 0.481209, acc.: 74.22%] [G loss: 5.245375]\n",
      "epoch:21 step:17044 [D loss: 0.270201, acc.: 87.50%] [G loss: 5.043518]\n",
      "epoch:21 step:17045 [D loss: 0.011135, acc.: 100.00%] [G loss: 4.852518]\n",
      "epoch:21 step:17046 [D loss: 0.013497, acc.: 100.00%] [G loss: 4.842915]\n",
      "epoch:21 step:17047 [D loss: 0.077946, acc.: 98.44%] [G loss: 3.792654]\n",
      "epoch:21 step:17048 [D loss: 0.031203, acc.: 100.00%] [G loss: 3.632774]\n",
      "epoch:21 step:17049 [D loss: 0.038113, acc.: 98.44%] [G loss: 4.729454]\n",
      "epoch:21 step:17050 [D loss: 0.012481, acc.: 100.00%] [G loss: 3.932480]\n",
      "epoch:21 step:17051 [D loss: 0.007128, acc.: 100.00%] [G loss: 3.826682]\n",
      "epoch:21 step:17052 [D loss: 0.010437, acc.: 100.00%] [G loss: 3.898842]\n",
      "epoch:21 step:17053 [D loss: 0.032275, acc.: 100.00%] [G loss: 4.693822]\n",
      "epoch:21 step:17054 [D loss: 0.021374, acc.: 100.00%] [G loss: 4.422966]\n",
      "epoch:21 step:17055 [D loss: 0.034809, acc.: 100.00%] [G loss: 3.659346]\n",
      "epoch:21 step:17056 [D loss: 0.032254, acc.: 100.00%] [G loss: 4.222380]\n",
      "epoch:21 step:17057 [D loss: 0.010375, acc.: 100.00%] [G loss: 4.447495]\n",
      "epoch:21 step:17058 [D loss: 0.015880, acc.: 100.00%] [G loss: 4.683132]\n",
      "epoch:21 step:17059 [D loss: 0.075457, acc.: 97.66%] [G loss: 3.557651]\n",
      "epoch:21 step:17060 [D loss: 0.048004, acc.: 100.00%] [G loss: 4.604246]\n",
      "epoch:21 step:17061 [D loss: 0.011413, acc.: 100.00%] [G loss: 4.274804]\n",
      "epoch:21 step:17062 [D loss: 0.039500, acc.: 100.00%] [G loss: 3.922682]\n",
      "epoch:21 step:17063 [D loss: 0.033094, acc.: 100.00%] [G loss: 2.701661]\n",
      "epoch:21 step:17064 [D loss: 0.012947, acc.: 100.00%] [G loss: 3.615629]\n",
      "epoch:21 step:17065 [D loss: 0.020714, acc.: 100.00%] [G loss: 3.631896]\n",
      "epoch:21 step:17066 [D loss: 0.042208, acc.: 100.00%] [G loss: 3.725132]\n",
      "epoch:21 step:17067 [D loss: 0.020560, acc.: 100.00%] [G loss: 3.985155]\n",
      "epoch:21 step:17068 [D loss: 0.013115, acc.: 100.00%] [G loss: 5.241791]\n",
      "epoch:21 step:17069 [D loss: 0.045351, acc.: 100.00%] [G loss: 4.595922]\n",
      "epoch:21 step:17070 [D loss: 0.026260, acc.: 99.22%] [G loss: 1.873347]\n",
      "epoch:21 step:17071 [D loss: 0.133857, acc.: 99.22%] [G loss: 5.239486]\n",
      "epoch:21 step:17072 [D loss: 0.005071, acc.: 100.00%] [G loss: 5.751578]\n",
      "epoch:21 step:17073 [D loss: 0.689659, acc.: 62.50%] [G loss: 6.404424]\n",
      "epoch:21 step:17074 [D loss: 1.376172, acc.: 50.00%] [G loss: 2.483330]\n",
      "epoch:21 step:17075 [D loss: 0.317978, acc.: 85.94%] [G loss: 4.901350]\n",
      "epoch:21 step:17076 [D loss: 0.106759, acc.: 95.31%] [G loss: 4.193638]\n",
      "epoch:21 step:17077 [D loss: 0.411716, acc.: 79.69%] [G loss: 3.311059]\n",
      "epoch:21 step:17078 [D loss: 0.021642, acc.: 100.00%] [G loss: 3.896036]\n",
      "epoch:21 step:17079 [D loss: 0.078904, acc.: 97.66%] [G loss: 4.202175]\n",
      "epoch:21 step:17080 [D loss: 0.010064, acc.: 100.00%] [G loss: 4.101004]\n",
      "epoch:21 step:17081 [D loss: 0.049332, acc.: 99.22%] [G loss: 2.754975]\n",
      "epoch:21 step:17082 [D loss: 0.034286, acc.: 100.00%] [G loss: 1.462345]\n",
      "epoch:21 step:17083 [D loss: 0.158988, acc.: 92.97%] [G loss: 3.142760]\n",
      "epoch:21 step:17084 [D loss: 0.044369, acc.: 99.22%] [G loss: 4.485356]\n",
      "epoch:21 step:17085 [D loss: 0.015590, acc.: 100.00%] [G loss: 4.658863]\n",
      "epoch:21 step:17086 [D loss: 0.256947, acc.: 92.97%] [G loss: 2.068686]\n",
      "epoch:21 step:17087 [D loss: 0.082247, acc.: 96.88%] [G loss: 2.918330]\n",
      "epoch:21 step:17088 [D loss: 0.211080, acc.: 92.97%] [G loss: 3.309064]\n",
      "epoch:21 step:17089 [D loss: 0.004320, acc.: 100.00%] [G loss: 2.511471]\n",
      "epoch:21 step:17090 [D loss: 0.135530, acc.: 94.53%] [G loss: 3.733474]\n",
      "epoch:21 step:17091 [D loss: 0.017721, acc.: 100.00%] [G loss: 3.166369]\n",
      "epoch:21 step:17092 [D loss: 0.031961, acc.: 100.00%] [G loss: 4.853350]\n",
      "epoch:21 step:17093 [D loss: 0.077373, acc.: 97.66%] [G loss: 1.166778]\n",
      "epoch:21 step:17094 [D loss: 0.315424, acc.: 85.94%] [G loss: 6.238608]\n",
      "epoch:21 step:17095 [D loss: 0.638075, acc.: 71.88%] [G loss: 3.564030]\n",
      "epoch:21 step:17096 [D loss: 0.195369, acc.: 92.19%] [G loss: 6.061767]\n",
      "epoch:21 step:17097 [D loss: 0.002383, acc.: 100.00%] [G loss: 6.200405]\n",
      "epoch:21 step:17098 [D loss: 0.049059, acc.: 98.44%] [G loss: 5.714409]\n",
      "epoch:21 step:17099 [D loss: 0.019276, acc.: 100.00%] [G loss: 5.689286]\n",
      "epoch:21 step:17100 [D loss: 0.007992, acc.: 100.00%] [G loss: 3.905233]\n",
      "epoch:21 step:17101 [D loss: 0.095662, acc.: 96.88%] [G loss: 3.156750]\n",
      "epoch:21 step:17102 [D loss: 0.137478, acc.: 93.75%] [G loss: 4.395722]\n",
      "epoch:21 step:17103 [D loss: 0.002723, acc.: 100.00%] [G loss: 4.651914]\n",
      "epoch:21 step:17104 [D loss: 0.014149, acc.: 99.22%] [G loss: 5.011154]\n",
      "epoch:21 step:17105 [D loss: 0.032045, acc.: 99.22%] [G loss: 3.134652]\n",
      "epoch:21 step:17106 [D loss: 0.009515, acc.: 100.00%] [G loss: 3.878236]\n",
      "epoch:21 step:17107 [D loss: 0.107295, acc.: 95.31%] [G loss: 5.134543]\n",
      "epoch:21 step:17108 [D loss: 0.127467, acc.: 94.53%] [G loss: 5.103323]\n",
      "epoch:21 step:17109 [D loss: 0.018630, acc.: 100.00%] [G loss: 4.117842]\n",
      "epoch:21 step:17110 [D loss: 0.010212, acc.: 100.00%] [G loss: 4.201409]\n",
      "epoch:21 step:17111 [D loss: 0.024741, acc.: 100.00%] [G loss: 3.672476]\n",
      "epoch:21 step:17112 [D loss: 0.058811, acc.: 99.22%] [G loss: 3.983987]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:21 step:17113 [D loss: 0.009387, acc.: 100.00%] [G loss: 3.731124]\n",
      "epoch:21 step:17114 [D loss: 0.214736, acc.: 93.75%] [G loss: 5.896351]\n",
      "epoch:21 step:17115 [D loss: 0.053211, acc.: 100.00%] [G loss: 5.569180]\n",
      "epoch:21 step:17116 [D loss: 0.425884, acc.: 78.12%] [G loss: 5.867544]\n",
      "epoch:21 step:17117 [D loss: 0.002785, acc.: 100.00%] [G loss: 6.934222]\n",
      "epoch:21 step:17118 [D loss: 0.134199, acc.: 92.97%] [G loss: 4.570916]\n",
      "epoch:21 step:17119 [D loss: 0.006994, acc.: 100.00%] [G loss: 4.679501]\n",
      "epoch:21 step:17120 [D loss: 0.009001, acc.: 100.00%] [G loss: 5.062220]\n",
      "epoch:21 step:17121 [D loss: 0.004701, acc.: 100.00%] [G loss: 4.532213]\n",
      "epoch:21 step:17122 [D loss: 0.004155, acc.: 100.00%] [G loss: 4.356666]\n",
      "epoch:21 step:17123 [D loss: 0.043944, acc.: 100.00%] [G loss: 4.092528]\n",
      "epoch:21 step:17124 [D loss: 0.025712, acc.: 100.00%] [G loss: 3.239245]\n",
      "epoch:21 step:17125 [D loss: 0.034640, acc.: 99.22%] [G loss: 4.127697]\n",
      "epoch:21 step:17126 [D loss: 1.193186, acc.: 43.75%] [G loss: 8.650395]\n",
      "epoch:21 step:17127 [D loss: 0.849676, acc.: 62.50%] [G loss: 6.975629]\n",
      "epoch:21 step:17128 [D loss: 0.007555, acc.: 100.00%] [G loss: 6.472808]\n",
      "epoch:21 step:17129 [D loss: 0.021617, acc.: 99.22%] [G loss: 5.352586]\n",
      "epoch:21 step:17130 [D loss: 0.008977, acc.: 100.00%] [G loss: 4.785516]\n",
      "epoch:21 step:17131 [D loss: 0.040325, acc.: 99.22%] [G loss: 5.412982]\n",
      "epoch:21 step:17132 [D loss: 0.011031, acc.: 100.00%] [G loss: 4.604765]\n",
      "epoch:21 step:17133 [D loss: 0.031616, acc.: 100.00%] [G loss: 4.932559]\n",
      "epoch:21 step:17134 [D loss: 0.061812, acc.: 98.44%] [G loss: 5.579246]\n",
      "epoch:21 step:17135 [D loss: 0.056638, acc.: 98.44%] [G loss: 3.799052]\n",
      "epoch:21 step:17136 [D loss: 0.039093, acc.: 100.00%] [G loss: 3.291473]\n",
      "epoch:21 step:17137 [D loss: 0.009150, acc.: 100.00%] [G loss: 5.116285]\n",
      "epoch:21 step:17138 [D loss: 0.083710, acc.: 98.44%] [G loss: 3.669433]\n",
      "epoch:21 step:17139 [D loss: 0.029682, acc.: 100.00%] [G loss: 5.203803]\n",
      "epoch:21 step:17140 [D loss: 2.869859, acc.: 26.56%] [G loss: 8.534903]\n",
      "epoch:21 step:17141 [D loss: 2.353307, acc.: 50.78%] [G loss: 6.291193]\n",
      "epoch:21 step:17142 [D loss: 0.626688, acc.: 68.75%] [G loss: 3.476544]\n",
      "epoch:21 step:17143 [D loss: 0.237753, acc.: 89.84%] [G loss: 3.046676]\n",
      "epoch:21 step:17144 [D loss: 0.115585, acc.: 95.31%] [G loss: 3.506233]\n",
      "epoch:21 step:17145 [D loss: 0.038406, acc.: 99.22%] [G loss: 3.769413]\n",
      "epoch:21 step:17146 [D loss: 0.069187, acc.: 98.44%] [G loss: 3.140572]\n",
      "epoch:21 step:17147 [D loss: 0.069430, acc.: 100.00%] [G loss: 2.905481]\n",
      "epoch:21 step:17148 [D loss: 0.145987, acc.: 96.09%] [G loss: 3.501377]\n",
      "epoch:21 step:17149 [D loss: 0.163917, acc.: 95.31%] [G loss: 3.534371]\n",
      "epoch:21 step:17150 [D loss: 0.317266, acc.: 87.50%] [G loss: 2.891454]\n",
      "epoch:21 step:17151 [D loss: 0.070300, acc.: 99.22%] [G loss: 3.437432]\n",
      "epoch:21 step:17152 [D loss: 0.072718, acc.: 100.00%] [G loss: 3.115369]\n",
      "epoch:21 step:17153 [D loss: 0.046473, acc.: 100.00%] [G loss: 3.455442]\n",
      "epoch:21 step:17154 [D loss: 0.165968, acc.: 95.31%] [G loss: 2.591192]\n",
      "epoch:21 step:17155 [D loss: 0.162467, acc.: 96.09%] [G loss: 3.083607]\n",
      "epoch:21 step:17156 [D loss: 0.088165, acc.: 98.44%] [G loss: 4.448570]\n",
      "epoch:21 step:17157 [D loss: 1.410156, acc.: 40.62%] [G loss: 5.515046]\n",
      "epoch:21 step:17158 [D loss: 0.353959, acc.: 83.59%] [G loss: 5.418597]\n",
      "epoch:21 step:17159 [D loss: 0.191605, acc.: 92.19%] [G loss: 4.004704]\n",
      "epoch:21 step:17160 [D loss: 0.139925, acc.: 93.75%] [G loss: 4.151791]\n",
      "epoch:21 step:17161 [D loss: 0.146116, acc.: 93.75%] [G loss: 3.511430]\n",
      "epoch:21 step:17162 [D loss: 0.052897, acc.: 99.22%] [G loss: 2.931578]\n",
      "epoch:21 step:17163 [D loss: 0.063544, acc.: 100.00%] [G loss: 3.598956]\n",
      "epoch:21 step:17164 [D loss: 0.024365, acc.: 100.00%] [G loss: 3.059886]\n",
      "epoch:21 step:17165 [D loss: 0.116267, acc.: 97.66%] [G loss: 4.002236]\n",
      "epoch:21 step:17166 [D loss: 0.068440, acc.: 97.66%] [G loss: 3.336702]\n",
      "epoch:21 step:17167 [D loss: 0.552622, acc.: 72.66%] [G loss: 4.636065]\n",
      "epoch:21 step:17168 [D loss: 0.390298, acc.: 81.25%] [G loss: 3.873660]\n",
      "epoch:21 step:17169 [D loss: 0.072976, acc.: 98.44%] [G loss: 4.092789]\n",
      "epoch:21 step:17170 [D loss: 0.046255, acc.: 100.00%] [G loss: 3.584210]\n",
      "epoch:21 step:17171 [D loss: 0.059263, acc.: 100.00%] [G loss: 2.929160]\n",
      "epoch:21 step:17172 [D loss: 0.059165, acc.: 98.44%] [G loss: 3.714817]\n",
      "epoch:21 step:17173 [D loss: 0.030061, acc.: 100.00%] [G loss: 3.387920]\n",
      "epoch:21 step:17174 [D loss: 0.029655, acc.: 100.00%] [G loss: 3.462994]\n",
      "epoch:21 step:17175 [D loss: 0.041511, acc.: 100.00%] [G loss: 3.199872]\n",
      "epoch:21 step:17176 [D loss: 0.027410, acc.: 100.00%] [G loss: 3.374505]\n",
      "epoch:21 step:17177 [D loss: 0.083364, acc.: 100.00%] [G loss: 3.209330]\n",
      "epoch:21 step:17178 [D loss: 0.176382, acc.: 92.97%] [G loss: 2.155391]\n",
      "epoch:21 step:17179 [D loss: 0.037147, acc.: 100.00%] [G loss: 3.493944]\n",
      "epoch:21 step:17180 [D loss: 0.652519, acc.: 67.97%] [G loss: 4.395887]\n",
      "epoch:21 step:17181 [D loss: 0.272311, acc.: 89.06%] [G loss: 3.840283]\n",
      "epoch:21 step:17182 [D loss: 0.027623, acc.: 100.00%] [G loss: 3.244183]\n",
      "epoch:22 step:17183 [D loss: 0.032910, acc.: 100.00%] [G loss: 4.093449]\n",
      "epoch:22 step:17184 [D loss: 0.032509, acc.: 100.00%] [G loss: 4.490461]\n",
      "epoch:22 step:17185 [D loss: 0.044932, acc.: 100.00%] [G loss: 4.402937]\n",
      "epoch:22 step:17186 [D loss: 0.041922, acc.: 100.00%] [G loss: 2.513745]\n",
      "epoch:22 step:17187 [D loss: 0.042013, acc.: 99.22%] [G loss: 2.890606]\n",
      "epoch:22 step:17188 [D loss: 0.070735, acc.: 99.22%] [G loss: 3.242758]\n",
      "epoch:22 step:17189 [D loss: 0.029768, acc.: 100.00%] [G loss: 4.322093]\n",
      "epoch:22 step:17190 [D loss: 0.022740, acc.: 100.00%] [G loss: 4.491864]\n",
      "epoch:22 step:17191 [D loss: 0.069119, acc.: 99.22%] [G loss: 3.933978]\n",
      "epoch:22 step:17192 [D loss: 0.038552, acc.: 100.00%] [G loss: 3.465463]\n",
      "epoch:22 step:17193 [D loss: 0.041608, acc.: 100.00%] [G loss: 3.897082]\n",
      "epoch:22 step:17194 [D loss: 0.067322, acc.: 99.22%] [G loss: 4.301253]\n",
      "epoch:22 step:17195 [D loss: 0.025653, acc.: 99.22%] [G loss: 5.500529]\n",
      "epoch:22 step:17196 [D loss: 0.302949, acc.: 85.94%] [G loss: 2.480864]\n",
      "epoch:22 step:17197 [D loss: 0.060121, acc.: 100.00%] [G loss: 4.557151]\n",
      "epoch:22 step:17198 [D loss: 0.012027, acc.: 100.00%] [G loss: 5.452480]\n",
      "epoch:22 step:17199 [D loss: 0.050811, acc.: 99.22%] [G loss: 4.939993]\n",
      "epoch:22 step:17200 [D loss: 0.008809, acc.: 100.00%] [G loss: 4.156815]\n",
      "##############\n",
      "[0.96719472 0.88929148 1.09866694 0.93528988 0.97559451 1.10501425\n",
      " 0.78377785 0.89735406 1.10955461 0.94793161]\n",
      "##########\n",
      "epoch:22 step:17201 [D loss: 0.009071, acc.: 100.00%] [G loss: 2.537068]\n",
      "epoch:22 step:17202 [D loss: 0.051023, acc.: 98.44%] [G loss: 4.601420]\n",
      "epoch:22 step:17203 [D loss: 0.006759, acc.: 100.00%] [G loss: 4.552175]\n",
      "epoch:22 step:17204 [D loss: 0.076661, acc.: 97.66%] [G loss: 2.344818]\n",
      "epoch:22 step:17205 [D loss: 0.223691, acc.: 90.62%] [G loss: 3.465581]\n",
      "epoch:22 step:17206 [D loss: 0.008386, acc.: 100.00%] [G loss: 5.399908]\n",
      "epoch:22 step:17207 [D loss: 0.564944, acc.: 76.56%] [G loss: 1.328316]\n",
      "epoch:22 step:17208 [D loss: 0.238403, acc.: 89.06%] [G loss: 4.933496]\n",
      "epoch:22 step:17209 [D loss: 0.022871, acc.: 100.00%] [G loss: 5.913921]\n",
      "epoch:22 step:17210 [D loss: 0.369907, acc.: 82.81%] [G loss: 2.494938]\n",
      "epoch:22 step:17211 [D loss: 0.513636, acc.: 76.56%] [G loss: 5.927681]\n",
      "epoch:22 step:17212 [D loss: 0.095276, acc.: 96.88%] [G loss: 6.795941]\n",
      "epoch:22 step:17213 [D loss: 0.499380, acc.: 76.56%] [G loss: 3.414975]\n",
      "epoch:22 step:17214 [D loss: 0.364458, acc.: 85.16%] [G loss: 4.584958]\n",
      "epoch:22 step:17215 [D loss: 0.005257, acc.: 100.00%] [G loss: 6.282763]\n",
      "epoch:22 step:17216 [D loss: 0.002323, acc.: 100.00%] [G loss: 6.320915]\n",
      "epoch:22 step:17217 [D loss: 0.046001, acc.: 99.22%] [G loss: 5.239484]\n",
      "epoch:22 step:17218 [D loss: 0.084727, acc.: 97.66%] [G loss: 4.420381]\n",
      "epoch:22 step:17219 [D loss: 0.004924, acc.: 100.00%] [G loss: 4.567678]\n",
      "epoch:22 step:17220 [D loss: 0.019062, acc.: 100.00%] [G loss: 3.845047]\n",
      "epoch:22 step:17221 [D loss: 0.081788, acc.: 98.44%] [G loss: 5.326454]\n",
      "epoch:22 step:17222 [D loss: 0.010704, acc.: 100.00%] [G loss: 5.475002]\n",
      "epoch:22 step:17223 [D loss: 0.011242, acc.: 100.00%] [G loss: 5.558358]\n",
      "epoch:22 step:17224 [D loss: 0.007095, acc.: 100.00%] [G loss: 5.116635]\n",
      "epoch:22 step:17225 [D loss: 0.017787, acc.: 100.00%] [G loss: 5.181993]\n",
      "epoch:22 step:17226 [D loss: 0.063487, acc.: 97.66%] [G loss: 2.811784]\n",
      "epoch:22 step:17227 [D loss: 0.165564, acc.: 92.97%] [G loss: 4.245807]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:22 step:17228 [D loss: 0.021800, acc.: 100.00%] [G loss: 4.672882]\n",
      "epoch:22 step:17229 [D loss: 0.011096, acc.: 100.00%] [G loss: 4.731106]\n",
      "epoch:22 step:17230 [D loss: 0.012863, acc.: 100.00%] [G loss: 4.430357]\n",
      "epoch:22 step:17231 [D loss: 0.004834, acc.: 100.00%] [G loss: 3.919819]\n",
      "epoch:22 step:17232 [D loss: 0.015626, acc.: 100.00%] [G loss: 4.699328]\n",
      "epoch:22 step:17233 [D loss: 0.021346, acc.: 100.00%] [G loss: 3.720531]\n",
      "epoch:22 step:17234 [D loss: 0.048839, acc.: 100.00%] [G loss: 5.087313]\n",
      "epoch:22 step:17235 [D loss: 0.004008, acc.: 100.00%] [G loss: 4.969064]\n",
      "epoch:22 step:17236 [D loss: 0.072862, acc.: 99.22%] [G loss: 0.928223]\n",
      "epoch:22 step:17237 [D loss: 0.173490, acc.: 93.75%] [G loss: 6.925358]\n",
      "epoch:22 step:17238 [D loss: 0.058579, acc.: 98.44%] [G loss: 6.457188]\n",
      "epoch:22 step:17239 [D loss: 0.718399, acc.: 68.75%] [G loss: 3.554411]\n",
      "epoch:22 step:17240 [D loss: 0.069539, acc.: 98.44%] [G loss: 1.622532]\n",
      "epoch:22 step:17241 [D loss: 0.048275, acc.: 98.44%] [G loss: 3.317474]\n",
      "epoch:22 step:17242 [D loss: 0.009870, acc.: 100.00%] [G loss: 3.674488]\n",
      "epoch:22 step:17243 [D loss: 0.052916, acc.: 100.00%] [G loss: 3.448450]\n",
      "epoch:22 step:17244 [D loss: 0.020137, acc.: 100.00%] [G loss: 4.277912]\n",
      "epoch:22 step:17245 [D loss: 0.028435, acc.: 100.00%] [G loss: 4.015763]\n",
      "epoch:22 step:17246 [D loss: 0.040030, acc.: 99.22%] [G loss: 4.873429]\n",
      "epoch:22 step:17247 [D loss: 0.133722, acc.: 97.66%] [G loss: 2.170223]\n",
      "epoch:22 step:17248 [D loss: 0.038450, acc.: 99.22%] [G loss: 3.372147]\n",
      "epoch:22 step:17249 [D loss: 0.007681, acc.: 100.00%] [G loss: 2.980442]\n",
      "epoch:22 step:17250 [D loss: 0.006467, acc.: 100.00%] [G loss: 4.871798]\n",
      "epoch:22 step:17251 [D loss: 0.009796, acc.: 100.00%] [G loss: 4.005308]\n",
      "epoch:22 step:17252 [D loss: 0.020444, acc.: 100.00%] [G loss: 4.815833]\n",
      "epoch:22 step:17253 [D loss: 0.046760, acc.: 99.22%] [G loss: 3.331852]\n",
      "epoch:22 step:17254 [D loss: 0.024773, acc.: 100.00%] [G loss: 2.552876]\n",
      "epoch:22 step:17255 [D loss: 0.012418, acc.: 100.00%] [G loss: 2.411936]\n",
      "epoch:22 step:17256 [D loss: 0.019449, acc.: 100.00%] [G loss: 3.254122]\n",
      "epoch:22 step:17257 [D loss: 0.029309, acc.: 100.00%] [G loss: 4.292502]\n",
      "epoch:22 step:17258 [D loss: 0.045816, acc.: 100.00%] [G loss: 2.561903]\n",
      "epoch:22 step:17259 [D loss: 1.705335, acc.: 36.72%] [G loss: 8.557812]\n",
      "epoch:22 step:17260 [D loss: 1.536963, acc.: 51.56%] [G loss: 5.473768]\n",
      "epoch:22 step:17261 [D loss: 0.031402, acc.: 99.22%] [G loss: 4.723239]\n",
      "epoch:22 step:17262 [D loss: 0.074932, acc.: 96.88%] [G loss: 4.021483]\n",
      "epoch:22 step:17263 [D loss: 0.026214, acc.: 100.00%] [G loss: 2.837877]\n",
      "epoch:22 step:17264 [D loss: 0.058112, acc.: 98.44%] [G loss: 3.952588]\n",
      "epoch:22 step:17265 [D loss: 0.071309, acc.: 98.44%] [G loss: 5.294726]\n",
      "epoch:22 step:17266 [D loss: 0.462337, acc.: 75.78%] [G loss: 4.628782]\n",
      "epoch:22 step:17267 [D loss: 0.048960, acc.: 98.44%] [G loss: 3.524747]\n",
      "epoch:22 step:17268 [D loss: 0.012061, acc.: 100.00%] [G loss: 3.597323]\n",
      "epoch:22 step:17269 [D loss: 0.015795, acc.: 100.00%] [G loss: 3.907821]\n",
      "epoch:22 step:17270 [D loss: 0.050332, acc.: 100.00%] [G loss: 3.157542]\n",
      "epoch:22 step:17271 [D loss: 0.132970, acc.: 94.53%] [G loss: 3.611740]\n",
      "epoch:22 step:17272 [D loss: 1.162025, acc.: 48.44%] [G loss: 1.088991]\n",
      "epoch:22 step:17273 [D loss: 0.070784, acc.: 100.00%] [G loss: 5.151701]\n",
      "epoch:22 step:17274 [D loss: 0.007191, acc.: 100.00%] [G loss: 4.639273]\n",
      "epoch:22 step:17275 [D loss: 0.004869, acc.: 100.00%] [G loss: 4.553280]\n",
      "epoch:22 step:17276 [D loss: 0.008675, acc.: 100.00%] [G loss: 2.975888]\n",
      "epoch:22 step:17277 [D loss: 0.021695, acc.: 99.22%] [G loss: 4.602222]\n",
      "epoch:22 step:17278 [D loss: 0.015208, acc.: 100.00%] [G loss: 4.153095]\n",
      "epoch:22 step:17279 [D loss: 0.067531, acc.: 99.22%] [G loss: 3.422732]\n",
      "epoch:22 step:17280 [D loss: 0.014672, acc.: 100.00%] [G loss: 4.352472]\n",
      "epoch:22 step:17281 [D loss: 0.603177, acc.: 70.31%] [G loss: 5.342552]\n",
      "epoch:22 step:17282 [D loss: 0.339290, acc.: 82.81%] [G loss: 3.734425]\n",
      "epoch:22 step:17283 [D loss: 0.018212, acc.: 100.00%] [G loss: 4.379286]\n",
      "epoch:22 step:17284 [D loss: 0.021492, acc.: 100.00%] [G loss: 2.692617]\n",
      "epoch:22 step:17285 [D loss: 0.144508, acc.: 96.09%] [G loss: 4.727219]\n",
      "epoch:22 step:17286 [D loss: 0.017767, acc.: 100.00%] [G loss: 5.639708]\n",
      "epoch:22 step:17287 [D loss: 0.036875, acc.: 98.44%] [G loss: 4.881080]\n",
      "epoch:22 step:17288 [D loss: 0.009521, acc.: 100.00%] [G loss: 4.786386]\n",
      "epoch:22 step:17289 [D loss: 0.075140, acc.: 99.22%] [G loss: 3.623905]\n",
      "epoch:22 step:17290 [D loss: 0.017902, acc.: 100.00%] [G loss: 4.106155]\n",
      "epoch:22 step:17291 [D loss: 0.105987, acc.: 97.66%] [G loss: 4.978746]\n",
      "epoch:22 step:17292 [D loss: 0.090214, acc.: 96.88%] [G loss: 4.858075]\n",
      "epoch:22 step:17293 [D loss: 0.024177, acc.: 99.22%] [G loss: 4.510251]\n",
      "epoch:22 step:17294 [D loss: 0.080255, acc.: 98.44%] [G loss: 4.601527]\n",
      "epoch:22 step:17295 [D loss: 0.017547, acc.: 100.00%] [G loss: 4.993455]\n",
      "epoch:22 step:17296 [D loss: 0.031035, acc.: 99.22%] [G loss: 4.228858]\n",
      "epoch:22 step:17297 [D loss: 0.029991, acc.: 100.00%] [G loss: 4.666235]\n",
      "epoch:22 step:17298 [D loss: 0.140677, acc.: 94.53%] [G loss: 1.884011]\n",
      "epoch:22 step:17299 [D loss: 0.292168, acc.: 86.72%] [G loss: 6.276525]\n",
      "epoch:22 step:17300 [D loss: 0.039178, acc.: 98.44%] [G loss: 7.722534]\n",
      "epoch:22 step:17301 [D loss: 0.546430, acc.: 71.09%] [G loss: 0.774449]\n",
      "epoch:22 step:17302 [D loss: 1.649777, acc.: 50.78%] [G loss: 8.717951]\n",
      "epoch:22 step:17303 [D loss: 1.479711, acc.: 52.34%] [G loss: 6.982225]\n",
      "epoch:22 step:17304 [D loss: 0.266369, acc.: 86.72%] [G loss: 4.935927]\n",
      "epoch:22 step:17305 [D loss: 0.014324, acc.: 100.00%] [G loss: 4.035281]\n",
      "epoch:22 step:17306 [D loss: 0.038389, acc.: 99.22%] [G loss: 4.296352]\n",
      "epoch:22 step:17307 [D loss: 0.081206, acc.: 97.66%] [G loss: 2.523894]\n",
      "epoch:22 step:17308 [D loss: 0.039169, acc.: 100.00%] [G loss: 2.957708]\n",
      "epoch:22 step:17309 [D loss: 0.019784, acc.: 100.00%] [G loss: 2.605232]\n",
      "epoch:22 step:17310 [D loss: 0.049992, acc.: 98.44%] [G loss: 3.179928]\n",
      "epoch:22 step:17311 [D loss: 0.042231, acc.: 99.22%] [G loss: 2.111706]\n",
      "epoch:22 step:17312 [D loss: 0.182480, acc.: 94.53%] [G loss: 2.891109]\n",
      "epoch:22 step:17313 [D loss: 0.036801, acc.: 100.00%] [G loss: 1.846823]\n",
      "epoch:22 step:17314 [D loss: 0.051035, acc.: 100.00%] [G loss: 2.410351]\n",
      "epoch:22 step:17315 [D loss: 0.076361, acc.: 99.22%] [G loss: 3.077497]\n",
      "epoch:22 step:17316 [D loss: 0.134273, acc.: 97.66%] [G loss: 3.067004]\n",
      "epoch:22 step:17317 [D loss: 0.124323, acc.: 99.22%] [G loss: 3.132519]\n",
      "epoch:22 step:17318 [D loss: 0.103778, acc.: 97.66%] [G loss: 2.483873]\n",
      "epoch:22 step:17319 [D loss: 0.140107, acc.: 96.88%] [G loss: 1.958577]\n",
      "epoch:22 step:17320 [D loss: 0.032714, acc.: 100.00%] [G loss: 2.971486]\n",
      "epoch:22 step:17321 [D loss: 0.081233, acc.: 98.44%] [G loss: 2.520479]\n",
      "epoch:22 step:17322 [D loss: 0.054801, acc.: 100.00%] [G loss: 2.304261]\n",
      "epoch:22 step:17323 [D loss: 0.119853, acc.: 98.44%] [G loss: 4.194237]\n",
      "epoch:22 step:17324 [D loss: 0.016548, acc.: 99.22%] [G loss: 4.782923]\n",
      "epoch:22 step:17325 [D loss: 0.119697, acc.: 96.09%] [G loss: 3.654700]\n",
      "epoch:22 step:17326 [D loss: 0.022808, acc.: 100.00%] [G loss: 1.985469]\n",
      "epoch:22 step:17327 [D loss: 0.199337, acc.: 92.19%] [G loss: 6.005481]\n",
      "epoch:22 step:17328 [D loss: 0.301929, acc.: 85.16%] [G loss: 4.631125]\n",
      "epoch:22 step:17329 [D loss: 0.024952, acc.: 100.00%] [G loss: 3.591355]\n",
      "epoch:22 step:17330 [D loss: 0.065099, acc.: 98.44%] [G loss: 3.986251]\n",
      "epoch:22 step:17331 [D loss: 0.016524, acc.: 99.22%] [G loss: 4.434913]\n",
      "epoch:22 step:17332 [D loss: 0.023260, acc.: 100.00%] [G loss: 4.264683]\n",
      "epoch:22 step:17333 [D loss: 0.019418, acc.: 100.00%] [G loss: 4.841579]\n",
      "epoch:22 step:17334 [D loss: 0.072085, acc.: 98.44%] [G loss: 3.586128]\n",
      "epoch:22 step:17335 [D loss: 0.037094, acc.: 99.22%] [G loss: 3.055603]\n",
      "epoch:22 step:17336 [D loss: 0.066511, acc.: 99.22%] [G loss: 4.198230]\n",
      "epoch:22 step:17337 [D loss: 0.019301, acc.: 100.00%] [G loss: 5.379975]\n",
      "epoch:22 step:17338 [D loss: 0.030553, acc.: 100.00%] [G loss: 3.737218]\n",
      "epoch:22 step:17339 [D loss: 0.045089, acc.: 99.22%] [G loss: 2.568234]\n",
      "epoch:22 step:17340 [D loss: 0.116728, acc.: 98.44%] [G loss: 4.661093]\n",
      "epoch:22 step:17341 [D loss: 0.028655, acc.: 99.22%] [G loss: 5.482780]\n",
      "epoch:22 step:17342 [D loss: 0.028567, acc.: 100.00%] [G loss: 5.471232]\n",
      "epoch:22 step:17343 [D loss: 0.472172, acc.: 77.34%] [G loss: 5.382912]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:22 step:17344 [D loss: 0.016889, acc.: 99.22%] [G loss: 7.104207]\n",
      "epoch:22 step:17345 [D loss: 0.388418, acc.: 79.69%] [G loss: 2.791187]\n",
      "epoch:22 step:17346 [D loss: 0.270444, acc.: 89.06%] [G loss: 4.613032]\n",
      "epoch:22 step:17347 [D loss: 0.007143, acc.: 100.00%] [G loss: 6.344731]\n",
      "epoch:22 step:17348 [D loss: 0.824071, acc.: 56.25%] [G loss: 3.670941]\n",
      "epoch:22 step:17349 [D loss: 0.027207, acc.: 99.22%] [G loss: 4.488263]\n",
      "epoch:22 step:17350 [D loss: 0.015887, acc.: 99.22%] [G loss: 4.616554]\n",
      "epoch:22 step:17351 [D loss: 0.010080, acc.: 100.00%] [G loss: 4.624355]\n",
      "epoch:22 step:17352 [D loss: 0.020763, acc.: 100.00%] [G loss: 3.590132]\n",
      "epoch:22 step:17353 [D loss: 0.044035, acc.: 100.00%] [G loss: 4.243522]\n",
      "epoch:22 step:17354 [D loss: 0.023525, acc.: 100.00%] [G loss: 4.173831]\n",
      "epoch:22 step:17355 [D loss: 0.026080, acc.: 100.00%] [G loss: 4.957512]\n",
      "epoch:22 step:17356 [D loss: 0.035589, acc.: 99.22%] [G loss: 4.622965]\n",
      "epoch:22 step:17357 [D loss: 0.014971, acc.: 100.00%] [G loss: 4.806998]\n",
      "epoch:22 step:17358 [D loss: 0.668185, acc.: 67.97%] [G loss: 4.739795]\n",
      "epoch:22 step:17359 [D loss: 0.009955, acc.: 100.00%] [G loss: 6.063715]\n",
      "epoch:22 step:17360 [D loss: 0.062299, acc.: 98.44%] [G loss: 5.410740]\n",
      "epoch:22 step:17361 [D loss: 0.131265, acc.: 96.09%] [G loss: 3.124054]\n",
      "epoch:22 step:17362 [D loss: 0.127397, acc.: 94.53%] [G loss: 4.681692]\n",
      "epoch:22 step:17363 [D loss: 0.019345, acc.: 100.00%] [G loss: 5.178214]\n",
      "epoch:22 step:17364 [D loss: 0.048190, acc.: 98.44%] [G loss: 4.549516]\n",
      "epoch:22 step:17365 [D loss: 0.065323, acc.: 98.44%] [G loss: 4.517706]\n",
      "epoch:22 step:17366 [D loss: 0.027217, acc.: 100.00%] [G loss: 4.632225]\n",
      "epoch:22 step:17367 [D loss: 0.029935, acc.: 100.00%] [G loss: 4.504784]\n",
      "epoch:22 step:17368 [D loss: 0.018816, acc.: 100.00%] [G loss: 4.788880]\n",
      "epoch:22 step:17369 [D loss: 0.175610, acc.: 94.53%] [G loss: 3.839214]\n",
      "epoch:22 step:17370 [D loss: 0.015451, acc.: 100.00%] [G loss: 4.045184]\n",
      "epoch:22 step:17371 [D loss: 0.022238, acc.: 99.22%] [G loss: 4.064449]\n",
      "epoch:22 step:17372 [D loss: 0.029212, acc.: 100.00%] [G loss: 4.518865]\n",
      "epoch:22 step:17373 [D loss: 0.026887, acc.: 100.00%] [G loss: 4.067704]\n",
      "epoch:22 step:17374 [D loss: 0.011757, acc.: 100.00%] [G loss: 4.518204]\n",
      "epoch:22 step:17375 [D loss: 0.013288, acc.: 100.00%] [G loss: 3.834091]\n",
      "epoch:22 step:17376 [D loss: 0.035132, acc.: 100.00%] [G loss: 3.853439]\n",
      "epoch:22 step:17377 [D loss: 0.026262, acc.: 100.00%] [G loss: 4.240657]\n",
      "epoch:22 step:17378 [D loss: 0.015665, acc.: 100.00%] [G loss: 3.541938]\n",
      "epoch:22 step:17379 [D loss: 0.069486, acc.: 99.22%] [G loss: 3.814663]\n",
      "epoch:22 step:17380 [D loss: 0.030046, acc.: 100.00%] [G loss: 3.346676]\n",
      "epoch:22 step:17381 [D loss: 0.033333, acc.: 100.00%] [G loss: 3.607685]\n",
      "epoch:22 step:17382 [D loss: 0.100969, acc.: 96.88%] [G loss: 3.101262]\n",
      "epoch:22 step:17383 [D loss: 0.019498, acc.: 100.00%] [G loss: 3.877602]\n",
      "epoch:22 step:17384 [D loss: 0.049084, acc.: 100.00%] [G loss: 4.235810]\n",
      "epoch:22 step:17385 [D loss: 0.027980, acc.: 100.00%] [G loss: 4.679907]\n",
      "epoch:22 step:17386 [D loss: 0.039484, acc.: 99.22%] [G loss: 4.265073]\n",
      "epoch:22 step:17387 [D loss: 0.129455, acc.: 94.53%] [G loss: 3.994938]\n",
      "epoch:22 step:17388 [D loss: 0.013551, acc.: 100.00%] [G loss: 4.701844]\n",
      "epoch:22 step:17389 [D loss: 0.018315, acc.: 100.00%] [G loss: 4.605109]\n",
      "epoch:22 step:17390 [D loss: 0.041510, acc.: 98.44%] [G loss: 3.955155]\n",
      "epoch:22 step:17391 [D loss: 0.126491, acc.: 96.88%] [G loss: 4.289246]\n",
      "epoch:22 step:17392 [D loss: 0.339682, acc.: 82.81%] [G loss: 4.466189]\n",
      "epoch:22 step:17393 [D loss: 0.015284, acc.: 100.00%] [G loss: 3.481246]\n",
      "epoch:22 step:17394 [D loss: 0.082037, acc.: 98.44%] [G loss: 4.976910]\n",
      "epoch:22 step:17395 [D loss: 0.056177, acc.: 98.44%] [G loss: 3.530535]\n",
      "epoch:22 step:17396 [D loss: 0.897081, acc.: 59.38%] [G loss: 8.476369]\n",
      "epoch:22 step:17397 [D loss: 0.764659, acc.: 62.50%] [G loss: 6.315093]\n",
      "epoch:22 step:17398 [D loss: 0.004293, acc.: 100.00%] [G loss: 4.918637]\n",
      "epoch:22 step:17399 [D loss: 0.093034, acc.: 96.88%] [G loss: 5.677884]\n",
      "epoch:22 step:17400 [D loss: 0.039983, acc.: 99.22%] [G loss: 4.433973]\n",
      "##############\n",
      "[0.87579509 0.89010258 0.87662864 0.93363695 0.87887456 0.9844506\n",
      " 2.11402988 2.11034435 1.00366594 1.09186576]\n",
      "##########\n",
      "epoch:22 step:17401 [D loss: 0.006599, acc.: 100.00%] [G loss: 5.177229]\n",
      "epoch:22 step:17402 [D loss: 0.078077, acc.: 98.44%] [G loss: 4.474110]\n",
      "epoch:22 step:17403 [D loss: 0.088764, acc.: 96.88%] [G loss: 5.139891]\n",
      "epoch:22 step:17404 [D loss: 0.191134, acc.: 94.53%] [G loss: 4.106544]\n",
      "epoch:22 step:17405 [D loss: 0.056085, acc.: 100.00%] [G loss: 4.984177]\n",
      "epoch:22 step:17406 [D loss: 0.031025, acc.: 100.00%] [G loss: 5.830778]\n",
      "epoch:22 step:17407 [D loss: 0.010950, acc.: 100.00%] [G loss: 4.399282]\n",
      "epoch:22 step:17408 [D loss: 0.143555, acc.: 96.88%] [G loss: 4.728073]\n",
      "epoch:22 step:17409 [D loss: 0.021590, acc.: 100.00%] [G loss: 4.156974]\n",
      "epoch:22 step:17410 [D loss: 0.042284, acc.: 99.22%] [G loss: 5.046328]\n",
      "epoch:22 step:17411 [D loss: 0.131696, acc.: 96.88%] [G loss: 3.254924]\n",
      "epoch:22 step:17412 [D loss: 0.130807, acc.: 97.66%] [G loss: 6.317966]\n",
      "epoch:22 step:17413 [D loss: 0.088649, acc.: 96.09%] [G loss: 5.856274]\n",
      "epoch:22 step:17414 [D loss: 0.241933, acc.: 89.84%] [G loss: 5.342815]\n",
      "epoch:22 step:17415 [D loss: 0.004609, acc.: 100.00%] [G loss: 7.366366]\n",
      "epoch:22 step:17416 [D loss: 0.150605, acc.: 91.41%] [G loss: 5.114499]\n",
      "epoch:22 step:17417 [D loss: 0.017839, acc.: 100.00%] [G loss: 4.215301]\n",
      "epoch:22 step:17418 [D loss: 0.009086, acc.: 100.00%] [G loss: 3.946143]\n",
      "epoch:22 step:17419 [D loss: 0.017861, acc.: 100.00%] [G loss: 3.742790]\n",
      "epoch:22 step:17420 [D loss: 0.023347, acc.: 100.00%] [G loss: 5.536129]\n",
      "epoch:22 step:17421 [D loss: 0.003553, acc.: 100.00%] [G loss: 5.042053]\n",
      "epoch:22 step:17422 [D loss: 2.918633, acc.: 34.38%] [G loss: 10.594165]\n",
      "epoch:22 step:17423 [D loss: 2.546767, acc.: 50.00%] [G loss: 6.835232]\n",
      "epoch:22 step:17424 [D loss: 0.715697, acc.: 66.41%] [G loss: 1.295327]\n",
      "epoch:22 step:17425 [D loss: 0.577587, acc.: 71.88%] [G loss: 4.251875]\n",
      "epoch:22 step:17426 [D loss: 0.041480, acc.: 98.44%] [G loss: 6.529731]\n",
      "epoch:22 step:17427 [D loss: 0.317331, acc.: 85.16%] [G loss: 5.266216]\n",
      "epoch:22 step:17428 [D loss: 0.044643, acc.: 100.00%] [G loss: 3.608094]\n",
      "epoch:22 step:17429 [D loss: 0.102882, acc.: 97.66%] [G loss: 3.790498]\n",
      "epoch:22 step:17430 [D loss: 0.030865, acc.: 99.22%] [G loss: 3.095287]\n",
      "epoch:22 step:17431 [D loss: 0.070450, acc.: 98.44%] [G loss: 3.635585]\n",
      "epoch:22 step:17432 [D loss: 0.116122, acc.: 99.22%] [G loss: 2.985145]\n",
      "epoch:22 step:17433 [D loss: 0.044815, acc.: 99.22%] [G loss: 2.566684]\n",
      "epoch:22 step:17434 [D loss: 0.201469, acc.: 92.97%] [G loss: 5.383813]\n",
      "epoch:22 step:17435 [D loss: 0.241430, acc.: 89.06%] [G loss: 3.813582]\n",
      "epoch:22 step:17436 [D loss: 0.042287, acc.: 100.00%] [G loss: 3.797096]\n",
      "epoch:22 step:17437 [D loss: 0.047951, acc.: 100.00%] [G loss: 4.047377]\n",
      "epoch:22 step:17438 [D loss: 0.041357, acc.: 100.00%] [G loss: 3.963325]\n",
      "epoch:22 step:17439 [D loss: 0.045617, acc.: 100.00%] [G loss: 3.784872]\n",
      "epoch:22 step:17440 [D loss: 0.099655, acc.: 97.66%] [G loss: 3.756078]\n",
      "epoch:22 step:17441 [D loss: 0.017271, acc.: 100.00%] [G loss: 4.610253]\n",
      "epoch:22 step:17442 [D loss: 0.038383, acc.: 99.22%] [G loss: 4.145042]\n",
      "epoch:22 step:17443 [D loss: 0.184132, acc.: 96.09%] [G loss: 5.534057]\n",
      "epoch:22 step:17444 [D loss: 0.017340, acc.: 100.00%] [G loss: 5.624048]\n",
      "epoch:22 step:17445 [D loss: 0.011064, acc.: 100.00%] [G loss: 5.120323]\n",
      "epoch:22 step:17446 [D loss: 0.381588, acc.: 82.81%] [G loss: 2.567037]\n",
      "epoch:22 step:17447 [D loss: 0.044250, acc.: 100.00%] [G loss: 3.135604]\n",
      "epoch:22 step:17448 [D loss: 0.056798, acc.: 98.44%] [G loss: 4.959182]\n",
      "epoch:22 step:17449 [D loss: 0.018475, acc.: 100.00%] [G loss: 4.295191]\n",
      "epoch:22 step:17450 [D loss: 0.011942, acc.: 100.00%] [G loss: 4.335928]\n",
      "epoch:22 step:17451 [D loss: 0.019447, acc.: 100.00%] [G loss: 4.274322]\n",
      "epoch:22 step:17452 [D loss: 0.083818, acc.: 97.66%] [G loss: 3.745793]\n",
      "epoch:22 step:17453 [D loss: 0.054807, acc.: 99.22%] [G loss: 3.981886]\n",
      "epoch:22 step:17454 [D loss: 0.006823, acc.: 100.00%] [G loss: 4.482752]\n",
      "epoch:22 step:17455 [D loss: 0.060574, acc.: 99.22%] [G loss: 4.344999]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:22 step:17456 [D loss: 0.014201, acc.: 100.00%] [G loss: 4.085492]\n",
      "epoch:22 step:17457 [D loss: 0.024153, acc.: 100.00%] [G loss: 4.146419]\n",
      "epoch:22 step:17458 [D loss: 0.017002, acc.: 100.00%] [G loss: 3.825386]\n",
      "epoch:22 step:17459 [D loss: 0.107456, acc.: 97.66%] [G loss: 4.192326]\n",
      "epoch:22 step:17460 [D loss: 0.026369, acc.: 100.00%] [G loss: 4.119608]\n",
      "epoch:22 step:17461 [D loss: 0.023567, acc.: 100.00%] [G loss: 3.936787]\n",
      "epoch:22 step:17462 [D loss: 0.026945, acc.: 99.22%] [G loss: 3.427660]\n",
      "epoch:22 step:17463 [D loss: 0.017296, acc.: 100.00%] [G loss: 3.444497]\n",
      "epoch:22 step:17464 [D loss: 0.028888, acc.: 100.00%] [G loss: 3.347780]\n",
      "epoch:22 step:17465 [D loss: 0.033554, acc.: 99.22%] [G loss: 2.951957]\n",
      "epoch:22 step:17466 [D loss: 0.013762, acc.: 100.00%] [G loss: 3.354954]\n",
      "epoch:22 step:17467 [D loss: 0.076758, acc.: 97.66%] [G loss: 4.368529]\n",
      "epoch:22 step:17468 [D loss: 0.486471, acc.: 78.91%] [G loss: 2.477836]\n",
      "epoch:22 step:17469 [D loss: 0.074562, acc.: 97.66%] [G loss: 5.190352]\n",
      "epoch:22 step:17470 [D loss: 0.006874, acc.: 100.00%] [G loss: 5.910131]\n",
      "epoch:22 step:17471 [D loss: 0.202276, acc.: 93.75%] [G loss: 5.397627]\n",
      "epoch:22 step:17472 [D loss: 0.010739, acc.: 100.00%] [G loss: 4.977537]\n",
      "epoch:22 step:17473 [D loss: 0.005844, acc.: 100.00%] [G loss: 4.154132]\n",
      "epoch:22 step:17474 [D loss: 0.012758, acc.: 100.00%] [G loss: 5.056637]\n",
      "epoch:22 step:17475 [D loss: 0.014070, acc.: 100.00%] [G loss: 4.919643]\n",
      "epoch:22 step:17476 [D loss: 0.025422, acc.: 99.22%] [G loss: 3.432300]\n",
      "epoch:22 step:17477 [D loss: 0.176633, acc.: 93.75%] [G loss: 3.638719]\n",
      "epoch:22 step:17478 [D loss: 0.008568, acc.: 100.00%] [G loss: 5.432564]\n",
      "epoch:22 step:17479 [D loss: 0.020599, acc.: 100.00%] [G loss: 3.176074]\n",
      "epoch:22 step:17480 [D loss: 0.019210, acc.: 100.00%] [G loss: 3.223279]\n",
      "epoch:22 step:17481 [D loss: 0.025717, acc.: 99.22%] [G loss: 3.656210]\n",
      "epoch:22 step:17482 [D loss: 0.026941, acc.: 99.22%] [G loss: 3.997507]\n",
      "epoch:22 step:17483 [D loss: 0.024422, acc.: 100.00%] [G loss: 5.072147]\n",
      "epoch:22 step:17484 [D loss: 0.022036, acc.: 100.00%] [G loss: 5.515083]\n",
      "epoch:22 step:17485 [D loss: 0.068929, acc.: 99.22%] [G loss: 4.466756]\n",
      "epoch:22 step:17486 [D loss: 0.027413, acc.: 99.22%] [G loss: 3.977340]\n",
      "epoch:22 step:17487 [D loss: 0.023103, acc.: 100.00%] [G loss: 5.175900]\n",
      "epoch:22 step:17488 [D loss: 0.012677, acc.: 100.00%] [G loss: 5.363941]\n",
      "epoch:22 step:17489 [D loss: 0.016310, acc.: 100.00%] [G loss: 4.410377]\n",
      "epoch:22 step:17490 [D loss: 0.008983, acc.: 100.00%] [G loss: 4.501431]\n",
      "epoch:22 step:17491 [D loss: 0.089132, acc.: 97.66%] [G loss: 4.206438]\n",
      "epoch:22 step:17492 [D loss: 0.030772, acc.: 100.00%] [G loss: 4.372322]\n",
      "epoch:22 step:17493 [D loss: 0.015896, acc.: 100.00%] [G loss: 4.276146]\n",
      "epoch:22 step:17494 [D loss: 0.030535, acc.: 100.00%] [G loss: 4.191906]\n",
      "epoch:22 step:17495 [D loss: 0.045271, acc.: 99.22%] [G loss: 4.740819]\n",
      "epoch:22 step:17496 [D loss: 0.009262, acc.: 100.00%] [G loss: 4.710117]\n",
      "epoch:22 step:17497 [D loss: 0.038122, acc.: 99.22%] [G loss: 4.414809]\n",
      "epoch:22 step:17498 [D loss: 0.005560, acc.: 100.00%] [G loss: 4.696781]\n",
      "epoch:22 step:17499 [D loss: 0.014176, acc.: 100.00%] [G loss: 4.495309]\n",
      "epoch:22 step:17500 [D loss: 0.009823, acc.: 100.00%] [G loss: 4.997385]\n",
      "epoch:22 step:17501 [D loss: 0.021302, acc.: 100.00%] [G loss: 4.429165]\n",
      "epoch:22 step:17502 [D loss: 0.196089, acc.: 92.97%] [G loss: 3.456215]\n",
      "epoch:22 step:17503 [D loss: 0.081664, acc.: 96.88%] [G loss: 5.013048]\n",
      "epoch:22 step:17504 [D loss: 0.003777, acc.: 100.00%] [G loss: 6.622402]\n",
      "epoch:22 step:17505 [D loss: 0.003720, acc.: 100.00%] [G loss: 7.514624]\n",
      "epoch:22 step:17506 [D loss: 0.173191, acc.: 94.53%] [G loss: 3.174946]\n",
      "epoch:22 step:17507 [D loss: 0.056436, acc.: 99.22%] [G loss: 6.203148]\n",
      "epoch:22 step:17508 [D loss: 0.003730, acc.: 100.00%] [G loss: 5.256150]\n",
      "epoch:22 step:17509 [D loss: 0.018102, acc.: 100.00%] [G loss: 6.268237]\n",
      "epoch:22 step:17510 [D loss: 0.005717, acc.: 100.00%] [G loss: 4.728340]\n",
      "epoch:22 step:17511 [D loss: 0.005780, acc.: 100.00%] [G loss: 5.130573]\n",
      "epoch:22 step:17512 [D loss: 0.007661, acc.: 100.00%] [G loss: 5.895984]\n",
      "epoch:22 step:17513 [D loss: 0.013524, acc.: 100.00%] [G loss: 5.431489]\n",
      "epoch:22 step:17514 [D loss: 0.018561, acc.: 99.22%] [G loss: 5.190574]\n",
      "epoch:22 step:17515 [D loss: 0.021527, acc.: 100.00%] [G loss: 5.633817]\n",
      "epoch:22 step:17516 [D loss: 0.023797, acc.: 100.00%] [G loss: 5.411280]\n",
      "epoch:22 step:17517 [D loss: 0.008460, acc.: 100.00%] [G loss: 4.768781]\n",
      "epoch:22 step:17518 [D loss: 1.203773, acc.: 49.22%] [G loss: 9.488868]\n",
      "epoch:22 step:17519 [D loss: 3.086850, acc.: 50.00%] [G loss: 5.154893]\n",
      "epoch:22 step:17520 [D loss: 0.813952, acc.: 65.62%] [G loss: 1.394230]\n",
      "epoch:22 step:17521 [D loss: 0.777838, acc.: 68.75%] [G loss: 4.259521]\n",
      "epoch:22 step:17522 [D loss: 0.155430, acc.: 93.75%] [G loss: 4.590941]\n",
      "epoch:22 step:17523 [D loss: 0.308865, acc.: 85.94%] [G loss: 2.725888]\n",
      "epoch:22 step:17524 [D loss: 0.233608, acc.: 90.62%] [G loss: 3.842421]\n",
      "epoch:22 step:17525 [D loss: 0.087243, acc.: 99.22%] [G loss: 3.922635]\n",
      "epoch:22 step:17526 [D loss: 0.063306, acc.: 99.22%] [G loss: 3.943725]\n",
      "epoch:22 step:17527 [D loss: 0.037447, acc.: 99.22%] [G loss: 3.945290]\n",
      "epoch:22 step:17528 [D loss: 0.774664, acc.: 61.72%] [G loss: 6.053755]\n",
      "epoch:22 step:17529 [D loss: 0.343717, acc.: 80.47%] [G loss: 5.085316]\n",
      "epoch:22 step:17530 [D loss: 0.071910, acc.: 97.66%] [G loss: 3.854091]\n",
      "epoch:22 step:17531 [D loss: 0.113116, acc.: 97.66%] [G loss: 2.778971]\n",
      "epoch:22 step:17532 [D loss: 0.039158, acc.: 100.00%] [G loss: 1.470514]\n",
      "epoch:22 step:17533 [D loss: 0.115958, acc.: 98.44%] [G loss: 3.235121]\n",
      "epoch:22 step:17534 [D loss: 0.142592, acc.: 96.09%] [G loss: 3.930573]\n",
      "epoch:22 step:17535 [D loss: 0.456739, acc.: 80.47%] [G loss: 3.636341]\n",
      "epoch:22 step:17536 [D loss: 0.071081, acc.: 97.66%] [G loss: 4.088823]\n",
      "epoch:22 step:17537 [D loss: 0.116439, acc.: 96.09%] [G loss: 3.565529]\n",
      "epoch:22 step:17538 [D loss: 0.138486, acc.: 92.97%] [G loss: 4.379823]\n",
      "epoch:22 step:17539 [D loss: 0.025782, acc.: 100.00%] [G loss: 4.866856]\n",
      "epoch:22 step:17540 [D loss: 0.074732, acc.: 97.66%] [G loss: 4.267842]\n",
      "epoch:22 step:17541 [D loss: 0.201141, acc.: 92.19%] [G loss: 3.945813]\n",
      "epoch:22 step:17542 [D loss: 0.026366, acc.: 100.00%] [G loss: 3.991015]\n",
      "epoch:22 step:17543 [D loss: 0.181211, acc.: 93.75%] [G loss: 3.569931]\n",
      "epoch:22 step:17544 [D loss: 0.073145, acc.: 100.00%] [G loss: 5.239122]\n",
      "epoch:22 step:17545 [D loss: 0.041180, acc.: 100.00%] [G loss: 4.259062]\n",
      "epoch:22 step:17546 [D loss: 0.283105, acc.: 88.28%] [G loss: 5.534288]\n",
      "epoch:22 step:17547 [D loss: 0.018992, acc.: 100.00%] [G loss: 5.887093]\n",
      "epoch:22 step:17548 [D loss: 0.053419, acc.: 99.22%] [G loss: 5.008076]\n",
      "epoch:22 step:17549 [D loss: 0.019055, acc.: 100.00%] [G loss: 4.247014]\n",
      "epoch:22 step:17550 [D loss: 0.047673, acc.: 100.00%] [G loss: 4.946870]\n",
      "epoch:22 step:17551 [D loss: 0.006290, acc.: 100.00%] [G loss: 4.853311]\n",
      "epoch:22 step:17552 [D loss: 0.078121, acc.: 97.66%] [G loss: 3.887366]\n",
      "epoch:22 step:17553 [D loss: 0.021052, acc.: 100.00%] [G loss: 3.553458]\n",
      "epoch:22 step:17554 [D loss: 0.044544, acc.: 98.44%] [G loss: 3.947952]\n",
      "epoch:22 step:17555 [D loss: 0.130563, acc.: 93.75%] [G loss: 2.215023]\n",
      "epoch:22 step:17556 [D loss: 0.316568, acc.: 82.03%] [G loss: 6.735795]\n",
      "epoch:22 step:17557 [D loss: 0.887875, acc.: 57.03%] [G loss: 3.194978]\n",
      "epoch:22 step:17558 [D loss: 0.139871, acc.: 93.75%] [G loss: 4.016153]\n",
      "epoch:22 step:17559 [D loss: 0.017259, acc.: 100.00%] [G loss: 4.476123]\n",
      "epoch:22 step:17560 [D loss: 0.028727, acc.: 99.22%] [G loss: 3.908277]\n",
      "epoch:22 step:17561 [D loss: 0.035938, acc.: 99.22%] [G loss: 3.534566]\n",
      "epoch:22 step:17562 [D loss: 0.012846, acc.: 100.00%] [G loss: 3.753869]\n",
      "epoch:22 step:17563 [D loss: 0.009690, acc.: 100.00%] [G loss: 3.253105]\n",
      "epoch:22 step:17564 [D loss: 0.022478, acc.: 100.00%] [G loss: 3.021547]\n",
      "epoch:22 step:17565 [D loss: 0.048637, acc.: 99.22%] [G loss: 3.510874]\n",
      "epoch:22 step:17566 [D loss: 0.046062, acc.: 100.00%] [G loss: 3.614528]\n",
      "epoch:22 step:17567 [D loss: 0.125222, acc.: 100.00%] [G loss: 1.960085]\n",
      "epoch:22 step:17568 [D loss: 0.134200, acc.: 96.09%] [G loss: 3.298214]\n",
      "epoch:22 step:17569 [D loss: 0.011214, acc.: 100.00%] [G loss: 4.451841]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:22 step:17570 [D loss: 0.037038, acc.: 100.00%] [G loss: 4.629366]\n",
      "epoch:22 step:17571 [D loss: 0.005619, acc.: 100.00%] [G loss: 4.471516]\n",
      "epoch:22 step:17572 [D loss: 0.159478, acc.: 96.09%] [G loss: 4.571958]\n",
      "epoch:22 step:17573 [D loss: 0.019685, acc.: 100.00%] [G loss: 5.311230]\n",
      "epoch:22 step:17574 [D loss: 0.003717, acc.: 100.00%] [G loss: 4.729420]\n",
      "epoch:22 step:17575 [D loss: 0.363047, acc.: 83.59%] [G loss: 5.979751]\n",
      "epoch:22 step:17576 [D loss: 0.014091, acc.: 100.00%] [G loss: 6.015495]\n",
      "epoch:22 step:17577 [D loss: 0.264780, acc.: 89.06%] [G loss: 2.886268]\n",
      "epoch:22 step:17578 [D loss: 0.331056, acc.: 85.16%] [G loss: 6.822069]\n",
      "epoch:22 step:17579 [D loss: 0.019035, acc.: 100.00%] [G loss: 7.599493]\n",
      "epoch:22 step:17580 [D loss: 0.156493, acc.: 89.84%] [G loss: 6.604991]\n",
      "epoch:22 step:17581 [D loss: 0.005462, acc.: 100.00%] [G loss: 5.695474]\n",
      "epoch:22 step:17582 [D loss: 0.003907, acc.: 100.00%] [G loss: 4.916490]\n",
      "epoch:22 step:17583 [D loss: 0.033534, acc.: 99.22%] [G loss: 3.350787]\n",
      "epoch:22 step:17584 [D loss: 0.115092, acc.: 96.09%] [G loss: 5.238546]\n",
      "epoch:22 step:17585 [D loss: 0.035897, acc.: 99.22%] [G loss: 5.568188]\n",
      "epoch:22 step:17586 [D loss: 0.005784, acc.: 100.00%] [G loss: 5.148382]\n",
      "epoch:22 step:17587 [D loss: 0.192490, acc.: 93.75%] [G loss: 4.801281]\n",
      "epoch:22 step:17588 [D loss: 0.010029, acc.: 100.00%] [G loss: 3.381492]\n",
      "epoch:22 step:17589 [D loss: 0.040653, acc.: 100.00%] [G loss: 4.559631]\n",
      "epoch:22 step:17590 [D loss: 0.003903, acc.: 100.00%] [G loss: 4.192430]\n",
      "epoch:22 step:17591 [D loss: 0.004048, acc.: 100.00%] [G loss: 3.585093]\n",
      "epoch:22 step:17592 [D loss: 0.026561, acc.: 100.00%] [G loss: 4.198595]\n",
      "epoch:22 step:17593 [D loss: 0.034192, acc.: 100.00%] [G loss: 3.272377]\n",
      "epoch:22 step:17594 [D loss: 0.035198, acc.: 100.00%] [G loss: 3.507848]\n",
      "epoch:22 step:17595 [D loss: 0.018168, acc.: 100.00%] [G loss: 3.982627]\n",
      "epoch:22 step:17596 [D loss: 0.030421, acc.: 99.22%] [G loss: 3.633044]\n",
      "epoch:22 step:17597 [D loss: 0.007651, acc.: 100.00%] [G loss: 3.716207]\n",
      "epoch:22 step:17598 [D loss: 0.043264, acc.: 100.00%] [G loss: 3.817052]\n",
      "epoch:22 step:17599 [D loss: 0.045540, acc.: 100.00%] [G loss: 3.807360]\n",
      "epoch:22 step:17600 [D loss: 0.024373, acc.: 100.00%] [G loss: 2.972179]\n",
      "##############\n",
      "[1.08580388 0.88814093 2.10445642 0.95421811 0.89328014 0.94486587\n",
      " 2.12565033 1.00313755 1.046314   2.11215595]\n",
      "##########\n",
      "epoch:22 step:17601 [D loss: 0.080211, acc.: 98.44%] [G loss: 5.203411]\n",
      "epoch:22 step:17602 [D loss: 1.007041, acc.: 50.00%] [G loss: 6.175457]\n",
      "epoch:22 step:17603 [D loss: 0.047915, acc.: 98.44%] [G loss: 6.515643]\n",
      "epoch:22 step:17604 [D loss: 0.020407, acc.: 100.00%] [G loss: 4.717060]\n",
      "epoch:22 step:17605 [D loss: 0.016386, acc.: 100.00%] [G loss: 4.634789]\n",
      "epoch:22 step:17606 [D loss: 0.077148, acc.: 97.66%] [G loss: 4.403200]\n",
      "epoch:22 step:17607 [D loss: 0.033675, acc.: 99.22%] [G loss: 3.878110]\n",
      "epoch:22 step:17608 [D loss: 0.030873, acc.: 100.00%] [G loss: 5.157601]\n",
      "epoch:22 step:17609 [D loss: 0.008639, acc.: 100.00%] [G loss: 4.905535]\n",
      "epoch:22 step:17610 [D loss: 0.039786, acc.: 100.00%] [G loss: 4.364933]\n",
      "epoch:22 step:17611 [D loss: 0.042758, acc.: 98.44%] [G loss: 2.675278]\n",
      "epoch:22 step:17612 [D loss: 0.298254, acc.: 83.59%] [G loss: 6.519689]\n",
      "epoch:22 step:17613 [D loss: 1.133963, acc.: 56.25%] [G loss: 1.993967]\n",
      "epoch:22 step:17614 [D loss: 1.061894, acc.: 60.16%] [G loss: 7.553805]\n",
      "epoch:22 step:17615 [D loss: 0.788097, acc.: 63.28%] [G loss: 5.227187]\n",
      "epoch:22 step:17616 [D loss: 0.218926, acc.: 92.19%] [G loss: 4.240209]\n",
      "epoch:22 step:17617 [D loss: 0.011893, acc.: 100.00%] [G loss: 2.483849]\n",
      "epoch:22 step:17618 [D loss: 0.323372, acc.: 85.16%] [G loss: 5.159518]\n",
      "epoch:22 step:17619 [D loss: 0.272183, acc.: 85.16%] [G loss: 3.852928]\n",
      "epoch:22 step:17620 [D loss: 0.069115, acc.: 99.22%] [G loss: 1.048790]\n",
      "epoch:22 step:17621 [D loss: 0.032779, acc.: 100.00%] [G loss: 2.635335]\n",
      "epoch:22 step:17622 [D loss: 0.094259, acc.: 97.66%] [G loss: 3.854582]\n",
      "epoch:22 step:17623 [D loss: 0.176965, acc.: 95.31%] [G loss: 2.370626]\n",
      "epoch:22 step:17624 [D loss: 0.016284, acc.: 100.00%] [G loss: 2.117067]\n",
      "epoch:22 step:17625 [D loss: 0.068461, acc.: 99.22%] [G loss: 2.858334]\n",
      "epoch:22 step:17626 [D loss: 0.107100, acc.: 98.44%] [G loss: 4.186152]\n",
      "epoch:22 step:17627 [D loss: 0.101448, acc.: 98.44%] [G loss: 1.760895]\n",
      "epoch:22 step:17628 [D loss: 0.096108, acc.: 96.09%] [G loss: 3.777812]\n",
      "epoch:22 step:17629 [D loss: 0.243055, acc.: 90.62%] [G loss: 3.306953]\n",
      "epoch:22 step:17630 [D loss: 0.766250, acc.: 66.41%] [G loss: 7.355440]\n",
      "epoch:22 step:17631 [D loss: 0.253938, acc.: 87.50%] [G loss: 5.553567]\n",
      "epoch:22 step:17632 [D loss: 0.011414, acc.: 100.00%] [G loss: 4.432744]\n",
      "epoch:22 step:17633 [D loss: 0.174326, acc.: 91.41%] [G loss: 4.190905]\n",
      "epoch:22 step:17634 [D loss: 0.016058, acc.: 100.00%] [G loss: 2.783581]\n",
      "epoch:22 step:17635 [D loss: 0.063255, acc.: 98.44%] [G loss: 5.204024]\n",
      "epoch:22 step:17636 [D loss: 0.007828, acc.: 100.00%] [G loss: 4.626590]\n",
      "epoch:22 step:17637 [D loss: 0.012187, acc.: 100.00%] [G loss: 5.175557]\n",
      "epoch:22 step:17638 [D loss: 0.027708, acc.: 100.00%] [G loss: 3.893120]\n",
      "epoch:22 step:17639 [D loss: 0.037100, acc.: 99.22%] [G loss: 4.635228]\n",
      "epoch:22 step:17640 [D loss: 0.007758, acc.: 100.00%] [G loss: 4.240932]\n",
      "epoch:22 step:17641 [D loss: 0.112766, acc.: 96.88%] [G loss: 3.957946]\n",
      "epoch:22 step:17642 [D loss: 0.214088, acc.: 92.97%] [G loss: 3.755941]\n",
      "epoch:22 step:17643 [D loss: 0.032412, acc.: 100.00%] [G loss: 4.207917]\n",
      "epoch:22 step:17644 [D loss: 0.007780, acc.: 100.00%] [G loss: 3.676631]\n",
      "epoch:22 step:17645 [D loss: 0.013549, acc.: 100.00%] [G loss: 3.664320]\n",
      "epoch:22 step:17646 [D loss: 0.018000, acc.: 100.00%] [G loss: 4.267736]\n",
      "epoch:22 step:17647 [D loss: 0.015792, acc.: 100.00%] [G loss: 3.916458]\n",
      "epoch:22 step:17648 [D loss: 0.013748, acc.: 100.00%] [G loss: 4.289202]\n",
      "epoch:22 step:17649 [D loss: 0.119326, acc.: 96.88%] [G loss: 5.351874]\n",
      "epoch:22 step:17650 [D loss: 0.022068, acc.: 100.00%] [G loss: 5.552104]\n",
      "epoch:22 step:17651 [D loss: 0.002515, acc.: 100.00%] [G loss: 5.671397]\n",
      "epoch:22 step:17652 [D loss: 0.006776, acc.: 100.00%] [G loss: 5.675657]\n",
      "epoch:22 step:17653 [D loss: 0.022975, acc.: 99.22%] [G loss: 4.693165]\n",
      "epoch:22 step:17654 [D loss: 0.014063, acc.: 100.00%] [G loss: 4.170165]\n",
      "epoch:22 step:17655 [D loss: 0.015825, acc.: 100.00%] [G loss: 4.034373]\n",
      "epoch:22 step:17656 [D loss: 0.051801, acc.: 99.22%] [G loss: 4.305029]\n",
      "epoch:22 step:17657 [D loss: 0.102784, acc.: 96.88%] [G loss: 5.034561]\n",
      "epoch:22 step:17658 [D loss: 0.008616, acc.: 100.00%] [G loss: 5.736372]\n",
      "epoch:22 step:17659 [D loss: 0.012915, acc.: 100.00%] [G loss: 5.311375]\n",
      "epoch:22 step:17660 [D loss: 0.018531, acc.: 100.00%] [G loss: 4.771371]\n",
      "epoch:22 step:17661 [D loss: 0.076086, acc.: 98.44%] [G loss: 3.676221]\n",
      "epoch:22 step:17662 [D loss: 0.033005, acc.: 100.00%] [G loss: 3.785334]\n",
      "epoch:22 step:17663 [D loss: 0.049481, acc.: 98.44%] [G loss: 5.186968]\n",
      "epoch:22 step:17664 [D loss: 0.015809, acc.: 100.00%] [G loss: 5.204820]\n",
      "epoch:22 step:17665 [D loss: 0.010111, acc.: 100.00%] [G loss: 5.180254]\n",
      "epoch:22 step:17666 [D loss: 0.001973, acc.: 100.00%] [G loss: 4.132482]\n",
      "epoch:22 step:17667 [D loss: 0.029858, acc.: 100.00%] [G loss: 4.949873]\n",
      "epoch:22 step:17668 [D loss: 0.014995, acc.: 100.00%] [G loss: 5.012738]\n",
      "epoch:22 step:17669 [D loss: 0.032151, acc.: 99.22%] [G loss: 5.295608]\n",
      "epoch:22 step:17670 [D loss: 0.013416, acc.: 100.00%] [G loss: 4.881971]\n",
      "epoch:22 step:17671 [D loss: 0.018328, acc.: 99.22%] [G loss: 4.875271]\n",
      "epoch:22 step:17672 [D loss: 0.011493, acc.: 100.00%] [G loss: 5.563982]\n",
      "epoch:22 step:17673 [D loss: 0.006921, acc.: 100.00%] [G loss: 4.857677]\n",
      "epoch:22 step:17674 [D loss: 0.056702, acc.: 100.00%] [G loss: 4.772405]\n",
      "epoch:22 step:17675 [D loss: 0.021218, acc.: 100.00%] [G loss: 3.621745]\n",
      "epoch:22 step:17676 [D loss: 0.025924, acc.: 100.00%] [G loss: 4.613147]\n",
      "epoch:22 step:17677 [D loss: 0.023655, acc.: 100.00%] [G loss: 5.880648]\n",
      "epoch:22 step:17678 [D loss: 0.008369, acc.: 100.00%] [G loss: 5.182044]\n",
      "epoch:22 step:17679 [D loss: 0.007203, acc.: 100.00%] [G loss: 5.445952]\n",
      "epoch:22 step:17680 [D loss: 0.017180, acc.: 100.00%] [G loss: 4.722806]\n",
      "epoch:22 step:17681 [D loss: 0.036937, acc.: 100.00%] [G loss: 5.623721]\n",
      "epoch:22 step:17682 [D loss: 0.004103, acc.: 100.00%] [G loss: 6.301372]\n",
      "epoch:22 step:17683 [D loss: 0.492927, acc.: 75.78%] [G loss: 3.001794]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:22 step:17684 [D loss: 0.036889, acc.: 99.22%] [G loss: 5.011372]\n",
      "epoch:22 step:17685 [D loss: 0.013843, acc.: 100.00%] [G loss: 5.711797]\n",
      "epoch:22 step:17686 [D loss: 0.002206, acc.: 100.00%] [G loss: 4.530936]\n",
      "epoch:22 step:17687 [D loss: 0.007564, acc.: 100.00%] [G loss: 6.047131]\n",
      "epoch:22 step:17688 [D loss: 0.024753, acc.: 99.22%] [G loss: 4.375286]\n",
      "epoch:22 step:17689 [D loss: 0.003616, acc.: 100.00%] [G loss: 4.207541]\n",
      "epoch:22 step:17690 [D loss: 0.018484, acc.: 100.00%] [G loss: 3.253617]\n",
      "epoch:22 step:17691 [D loss: 0.014006, acc.: 100.00%] [G loss: 1.286021]\n",
      "epoch:22 step:17692 [D loss: 0.026422, acc.: 99.22%] [G loss: 2.438262]\n",
      "epoch:22 step:17693 [D loss: 0.006056, acc.: 100.00%] [G loss: 0.893101]\n",
      "epoch:22 step:17694 [D loss: 0.012841, acc.: 100.00%] [G loss: 3.197907]\n",
      "epoch:22 step:17695 [D loss: 0.349978, acc.: 83.59%] [G loss: 10.040556]\n",
      "epoch:22 step:17696 [D loss: 1.862926, acc.: 51.56%] [G loss: 4.576710]\n",
      "epoch:22 step:17697 [D loss: 0.102944, acc.: 95.31%] [G loss: 5.025687]\n",
      "epoch:22 step:17698 [D loss: 0.006080, acc.: 100.00%] [G loss: 3.943680]\n",
      "epoch:22 step:17699 [D loss: 0.019153, acc.: 100.00%] [G loss: 3.122230]\n",
      "epoch:22 step:17700 [D loss: 0.126622, acc.: 96.88%] [G loss: 5.930696]\n",
      "epoch:22 step:17701 [D loss: 0.023776, acc.: 99.22%] [G loss: 5.193732]\n",
      "epoch:22 step:17702 [D loss: 0.032973, acc.: 100.00%] [G loss: 2.839770]\n",
      "epoch:22 step:17703 [D loss: 0.066593, acc.: 97.66%] [G loss: 3.573280]\n",
      "epoch:22 step:17704 [D loss: 0.056949, acc.: 99.22%] [G loss: 3.371904]\n",
      "epoch:22 step:17705 [D loss: 0.411175, acc.: 78.91%] [G loss: 6.861904]\n",
      "epoch:22 step:17706 [D loss: 1.502939, acc.: 52.34%] [G loss: 1.591971]\n",
      "epoch:22 step:17707 [D loss: 0.332632, acc.: 81.25%] [G loss: 4.589165]\n",
      "epoch:22 step:17708 [D loss: 0.011195, acc.: 100.00%] [G loss: 4.029898]\n",
      "epoch:22 step:17709 [D loss: 0.024082, acc.: 100.00%] [G loss: 3.464349]\n",
      "epoch:22 step:17710 [D loss: 0.137538, acc.: 92.97%] [G loss: 2.160843]\n",
      "epoch:22 step:17711 [D loss: 0.056939, acc.: 100.00%] [G loss: 1.291207]\n",
      "epoch:22 step:17712 [D loss: 0.227363, acc.: 91.41%] [G loss: 4.591541]\n",
      "epoch:22 step:17713 [D loss: 0.107250, acc.: 96.88%] [G loss: 3.699591]\n",
      "epoch:22 step:17714 [D loss: 0.134264, acc.: 96.88%] [G loss: 3.028034]\n",
      "epoch:22 step:17715 [D loss: 0.162373, acc.: 94.53%] [G loss: 1.160391]\n",
      "epoch:22 step:17716 [D loss: 0.691906, acc.: 74.22%] [G loss: 7.317102]\n",
      "epoch:22 step:17717 [D loss: 0.520762, acc.: 75.78%] [G loss: 4.340309]\n",
      "epoch:22 step:17718 [D loss: 0.094001, acc.: 97.66%] [G loss: 4.050467]\n",
      "epoch:22 step:17719 [D loss: 0.032650, acc.: 100.00%] [G loss: 4.773167]\n",
      "epoch:22 step:17720 [D loss: 0.027464, acc.: 99.22%] [G loss: 4.346138]\n",
      "epoch:22 step:17721 [D loss: 0.014788, acc.: 100.00%] [G loss: 4.243824]\n",
      "epoch:22 step:17722 [D loss: 0.079322, acc.: 100.00%] [G loss: 4.255523]\n",
      "epoch:22 step:17723 [D loss: 0.017554, acc.: 99.22%] [G loss: 4.401598]\n",
      "epoch:22 step:17724 [D loss: 0.132311, acc.: 97.66%] [G loss: 4.851422]\n",
      "epoch:22 step:17725 [D loss: 0.016457, acc.: 100.00%] [G loss: 3.888169]\n",
      "epoch:22 step:17726 [D loss: 0.003671, acc.: 100.00%] [G loss: 4.152698]\n",
      "epoch:22 step:17727 [D loss: 0.054092, acc.: 99.22%] [G loss: 2.896817]\n",
      "epoch:22 step:17728 [D loss: 0.052060, acc.: 99.22%] [G loss: 3.583610]\n",
      "epoch:22 step:17729 [D loss: 0.322045, acc.: 88.28%] [G loss: 1.606697]\n",
      "epoch:22 step:17730 [D loss: 0.059363, acc.: 99.22%] [G loss: 3.564646]\n",
      "epoch:22 step:17731 [D loss: 0.008491, acc.: 100.00%] [G loss: 4.819182]\n",
      "epoch:22 step:17732 [D loss: 0.031289, acc.: 100.00%] [G loss: 4.975944]\n",
      "epoch:22 step:17733 [D loss: 0.024242, acc.: 100.00%] [G loss: 4.084141]\n",
      "epoch:22 step:17734 [D loss: 0.029403, acc.: 99.22%] [G loss: 5.447114]\n",
      "epoch:22 step:17735 [D loss: 0.008861, acc.: 100.00%] [G loss: 3.969669]\n",
      "epoch:22 step:17736 [D loss: 0.138659, acc.: 96.88%] [G loss: 4.495485]\n",
      "epoch:22 step:17737 [D loss: 0.008855, acc.: 100.00%] [G loss: 5.308384]\n",
      "epoch:22 step:17738 [D loss: 0.031083, acc.: 99.22%] [G loss: 4.895697]\n",
      "epoch:22 step:17739 [D loss: 0.124417, acc.: 96.88%] [G loss: 4.477011]\n",
      "epoch:22 step:17740 [D loss: 0.041810, acc.: 100.00%] [G loss: 4.065554]\n",
      "epoch:22 step:17741 [D loss: 0.056903, acc.: 100.00%] [G loss: 5.059077]\n",
      "epoch:22 step:17742 [D loss: 0.009182, acc.: 100.00%] [G loss: 3.732876]\n",
      "epoch:22 step:17743 [D loss: 0.034860, acc.: 100.00%] [G loss: 5.008737]\n",
      "epoch:22 step:17744 [D loss: 0.315115, acc.: 87.50%] [G loss: 7.373378]\n",
      "epoch:22 step:17745 [D loss: 0.413097, acc.: 80.47%] [G loss: 2.730258]\n",
      "epoch:22 step:17746 [D loss: 0.106188, acc.: 96.09%] [G loss: 2.786130]\n",
      "epoch:22 step:17747 [D loss: 0.015827, acc.: 99.22%] [G loss: 5.248089]\n",
      "epoch:22 step:17748 [D loss: 0.190340, acc.: 95.31%] [G loss: 2.043468]\n",
      "epoch:22 step:17749 [D loss: 0.083029, acc.: 100.00%] [G loss: 4.398312]\n",
      "epoch:22 step:17750 [D loss: 0.003425, acc.: 100.00%] [G loss: 6.759363]\n",
      "epoch:22 step:17751 [D loss: 0.010163, acc.: 100.00%] [G loss: 5.070507]\n",
      "epoch:22 step:17752 [D loss: 0.021637, acc.: 98.44%] [G loss: 6.303512]\n",
      "epoch:22 step:17753 [D loss: 0.019695, acc.: 100.00%] [G loss: 4.406020]\n",
      "epoch:22 step:17754 [D loss: 0.005388, acc.: 100.00%] [G loss: 4.130049]\n",
      "epoch:22 step:17755 [D loss: 0.011176, acc.: 100.00%] [G loss: 4.065525]\n",
      "epoch:22 step:17756 [D loss: 0.014859, acc.: 100.00%] [G loss: 4.277641]\n",
      "epoch:22 step:17757 [D loss: 0.107637, acc.: 96.88%] [G loss: 3.400684]\n",
      "epoch:22 step:17758 [D loss: 0.059469, acc.: 99.22%] [G loss: 4.763863]\n",
      "epoch:22 step:17759 [D loss: 0.066528, acc.: 99.22%] [G loss: 4.591178]\n",
      "epoch:22 step:17760 [D loss: 0.006269, acc.: 100.00%] [G loss: 4.610668]\n",
      "epoch:22 step:17761 [D loss: 0.078408, acc.: 97.66%] [G loss: 5.087113]\n",
      "epoch:22 step:17762 [D loss: 0.020700, acc.: 100.00%] [G loss: 4.216725]\n",
      "epoch:22 step:17763 [D loss: 0.036554, acc.: 98.44%] [G loss: 4.395618]\n",
      "epoch:22 step:17764 [D loss: 0.015514, acc.: 100.00%] [G loss: 4.712209]\n",
      "epoch:22 step:17765 [D loss: 0.008300, acc.: 100.00%] [G loss: 3.736407]\n",
      "epoch:22 step:17766 [D loss: 0.018004, acc.: 100.00%] [G loss: 4.018307]\n",
      "epoch:22 step:17767 [D loss: 0.013308, acc.: 100.00%] [G loss: 4.212315]\n",
      "epoch:22 step:17768 [D loss: 0.030735, acc.: 99.22%] [G loss: 5.283367]\n",
      "epoch:22 step:17769 [D loss: 0.145226, acc.: 96.09%] [G loss: 4.774680]\n",
      "epoch:22 step:17770 [D loss: 0.004893, acc.: 100.00%] [G loss: 5.596488]\n",
      "epoch:22 step:17771 [D loss: 0.057113, acc.: 99.22%] [G loss: 2.875314]\n",
      "epoch:22 step:17772 [D loss: 0.045080, acc.: 99.22%] [G loss: 4.418049]\n",
      "epoch:22 step:17773 [D loss: 0.014495, acc.: 100.00%] [G loss: 4.572679]\n",
      "epoch:22 step:17774 [D loss: 0.004234, acc.: 100.00%] [G loss: 4.582596]\n",
      "epoch:22 step:17775 [D loss: 0.017828, acc.: 100.00%] [G loss: 4.988510]\n",
      "epoch:22 step:17776 [D loss: 0.011057, acc.: 100.00%] [G loss: 3.427560]\n",
      "epoch:22 step:17777 [D loss: 0.038492, acc.: 99.22%] [G loss: 5.348495]\n",
      "epoch:22 step:17778 [D loss: 1.125954, acc.: 50.00%] [G loss: 8.877655]\n",
      "epoch:22 step:17779 [D loss: 3.277672, acc.: 50.00%] [G loss: 5.381617]\n",
      "epoch:22 step:17780 [D loss: 1.432518, acc.: 49.22%] [G loss: 2.198294]\n",
      "epoch:22 step:17781 [D loss: 0.404266, acc.: 80.47%] [G loss: 3.590513]\n",
      "epoch:22 step:17782 [D loss: 0.176303, acc.: 93.75%] [G loss: 3.620816]\n",
      "epoch:22 step:17783 [D loss: 0.244300, acc.: 88.28%] [G loss: 3.040300]\n",
      "epoch:22 step:17784 [D loss: 0.122146, acc.: 98.44%] [G loss: 3.067444]\n",
      "epoch:22 step:17785 [D loss: 0.070737, acc.: 100.00%] [G loss: 3.123475]\n",
      "epoch:22 step:17786 [D loss: 0.083000, acc.: 99.22%] [G loss: 3.263746]\n",
      "epoch:22 step:17787 [D loss: 0.067563, acc.: 99.22%] [G loss: 2.682696]\n",
      "epoch:22 step:17788 [D loss: 0.213723, acc.: 92.97%] [G loss: 4.433751]\n",
      "epoch:22 step:17789 [D loss: 0.083993, acc.: 98.44%] [G loss: 3.588097]\n",
      "epoch:22 step:17790 [D loss: 0.052464, acc.: 99.22%] [G loss: 3.599755]\n",
      "epoch:22 step:17791 [D loss: 0.140153, acc.: 96.88%] [G loss: 2.281991]\n",
      "epoch:22 step:17792 [D loss: 0.230965, acc.: 89.84%] [G loss: 4.463859]\n",
      "epoch:22 step:17793 [D loss: 0.062231, acc.: 99.22%] [G loss: 4.308619]\n",
      "epoch:22 step:17794 [D loss: 0.083620, acc.: 97.66%] [G loss: 3.715469]\n",
      "epoch:22 step:17795 [D loss: 0.077987, acc.: 97.66%] [G loss: 3.495778]\n",
      "epoch:22 step:17796 [D loss: 0.085136, acc.: 97.66%] [G loss: 3.320442]\n",
      "epoch:22 step:17797 [D loss: 0.082279, acc.: 98.44%] [G loss: 3.357888]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:22 step:17798 [D loss: 0.071823, acc.: 99.22%] [G loss: 2.304390]\n",
      "epoch:22 step:17799 [D loss: 0.064012, acc.: 100.00%] [G loss: 2.960048]\n",
      "epoch:22 step:17800 [D loss: 0.337837, acc.: 85.94%] [G loss: 3.723855]\n",
      "##############\n",
      "[0.89213284 0.97796744 1.01576805 1.0283865  0.90296517 0.92574056\n",
      " 0.8797836  0.88683171 1.02026241 0.95939802]\n",
      "##########\n",
      "epoch:22 step:17801 [D loss: 0.086325, acc.: 96.88%] [G loss: 4.289750]\n",
      "epoch:22 step:17802 [D loss: 0.223029, acc.: 91.41%] [G loss: 2.428121]\n",
      "epoch:22 step:17803 [D loss: 0.522190, acc.: 77.34%] [G loss: 6.647112]\n",
      "epoch:22 step:17804 [D loss: 1.041999, acc.: 57.81%] [G loss: 4.785467]\n",
      "epoch:22 step:17805 [D loss: 0.098304, acc.: 98.44%] [G loss: 4.363368]\n",
      "epoch:22 step:17806 [D loss: 0.021723, acc.: 100.00%] [G loss: 4.371682]\n",
      "epoch:22 step:17807 [D loss: 0.068452, acc.: 96.88%] [G loss: 4.319172]\n",
      "epoch:22 step:17808 [D loss: 0.070184, acc.: 97.66%] [G loss: 4.511528]\n",
      "epoch:22 step:17809 [D loss: 0.121016, acc.: 96.88%] [G loss: 3.474637]\n",
      "epoch:22 step:17810 [D loss: 0.025520, acc.: 100.00%] [G loss: 3.272155]\n",
      "epoch:22 step:17811 [D loss: 0.056589, acc.: 100.00%] [G loss: 3.083895]\n",
      "epoch:22 step:17812 [D loss: 0.061959, acc.: 99.22%] [G loss: 3.389127]\n",
      "epoch:22 step:17813 [D loss: 0.123711, acc.: 95.31%] [G loss: 2.787055]\n",
      "epoch:22 step:17814 [D loss: 0.093173, acc.: 98.44%] [G loss: 2.151151]\n",
      "epoch:22 step:17815 [D loss: 0.074293, acc.: 100.00%] [G loss: 3.810201]\n",
      "epoch:22 step:17816 [D loss: 0.069968, acc.: 98.44%] [G loss: 3.008231]\n",
      "epoch:22 step:17817 [D loss: 0.230706, acc.: 92.19%] [G loss: 1.114173]\n",
      "epoch:22 step:17818 [D loss: 0.102094, acc.: 97.66%] [G loss: 2.885865]\n",
      "epoch:22 step:17819 [D loss: 0.014782, acc.: 100.00%] [G loss: 4.165530]\n",
      "epoch:22 step:17820 [D loss: 0.012414, acc.: 100.00%] [G loss: 3.485513]\n",
      "epoch:22 step:17821 [D loss: 0.152862, acc.: 95.31%] [G loss: 1.980895]\n",
      "epoch:22 step:17822 [D loss: 0.055068, acc.: 100.00%] [G loss: 4.468933]\n",
      "epoch:22 step:17823 [D loss: 0.019678, acc.: 100.00%] [G loss: 3.789064]\n",
      "epoch:22 step:17824 [D loss: 0.117346, acc.: 96.88%] [G loss: 3.415380]\n",
      "epoch:22 step:17825 [D loss: 0.021053, acc.: 100.00%] [G loss: 2.586452]\n",
      "epoch:22 step:17826 [D loss: 0.012570, acc.: 100.00%] [G loss: 2.819323]\n",
      "epoch:22 step:17827 [D loss: 0.127704, acc.: 95.31%] [G loss: 0.281755]\n",
      "epoch:22 step:17828 [D loss: 1.491463, acc.: 55.47%] [G loss: 8.490372]\n",
      "epoch:22 step:17829 [D loss: 2.279704, acc.: 50.00%] [G loss: 5.804152]\n",
      "epoch:22 step:17830 [D loss: 1.171511, acc.: 60.94%] [G loss: 3.846058]\n",
      "epoch:22 step:17831 [D loss: 0.540311, acc.: 71.09%] [G loss: 2.377179]\n",
      "epoch:22 step:17832 [D loss: 0.216335, acc.: 89.06%] [G loss: 3.588677]\n",
      "epoch:22 step:17833 [D loss: 0.139232, acc.: 94.53%] [G loss: 2.806812]\n",
      "epoch:22 step:17834 [D loss: 0.101432, acc.: 99.22%] [G loss: 2.271572]\n",
      "epoch:22 step:17835 [D loss: 0.115811, acc.: 96.88%] [G loss: 4.234756]\n",
      "epoch:22 step:17836 [D loss: 0.227077, acc.: 93.75%] [G loss: 3.245675]\n",
      "epoch:22 step:17837 [D loss: 0.084133, acc.: 98.44%] [G loss: 3.786330]\n",
      "epoch:22 step:17838 [D loss: 0.234076, acc.: 91.41%] [G loss: 2.807364]\n",
      "epoch:22 step:17839 [D loss: 0.065462, acc.: 98.44%] [G loss: 3.461389]\n",
      "epoch:22 step:17840 [D loss: 0.133355, acc.: 96.88%] [G loss: 2.587703]\n",
      "epoch:22 step:17841 [D loss: 0.095828, acc.: 98.44%] [G loss: 2.181502]\n",
      "epoch:22 step:17842 [D loss: 0.065261, acc.: 99.22%] [G loss: 3.132555]\n",
      "epoch:22 step:17843 [D loss: 0.029431, acc.: 100.00%] [G loss: 2.358802]\n",
      "epoch:22 step:17844 [D loss: 0.108589, acc.: 99.22%] [G loss: 1.414566]\n",
      "epoch:22 step:17845 [D loss: 0.061532, acc.: 100.00%] [G loss: 2.425901]\n",
      "epoch:22 step:17846 [D loss: 0.035673, acc.: 100.00%] [G loss: 1.945837]\n",
      "epoch:22 step:17847 [D loss: 0.636795, acc.: 67.97%] [G loss: 5.260661]\n",
      "epoch:22 step:17848 [D loss: 0.312392, acc.: 82.81%] [G loss: 4.979434]\n",
      "epoch:22 step:17849 [D loss: 0.075929, acc.: 96.88%] [G loss: 3.301310]\n",
      "epoch:22 step:17850 [D loss: 0.024246, acc.: 100.00%] [G loss: 2.558670]\n",
      "epoch:22 step:17851 [D loss: 0.036020, acc.: 100.00%] [G loss: 2.003061]\n",
      "epoch:22 step:17852 [D loss: 0.039475, acc.: 100.00%] [G loss: 2.048394]\n",
      "epoch:22 step:17853 [D loss: 0.207848, acc.: 92.19%] [G loss: 4.002950]\n",
      "epoch:22 step:17854 [D loss: 0.077991, acc.: 98.44%] [G loss: 4.820219]\n",
      "epoch:22 step:17855 [D loss: 0.123335, acc.: 96.09%] [G loss: 3.348857]\n",
      "epoch:22 step:17856 [D loss: 0.146007, acc.: 96.09%] [G loss: 3.468594]\n",
      "epoch:22 step:17857 [D loss: 0.030051, acc.: 100.00%] [G loss: 3.863126]\n",
      "epoch:22 step:17858 [D loss: 0.229246, acc.: 89.84%] [G loss: 1.494372]\n",
      "epoch:22 step:17859 [D loss: 0.146545, acc.: 93.75%] [G loss: 3.345771]\n",
      "epoch:22 step:17860 [D loss: 0.051409, acc.: 99.22%] [G loss: 4.238966]\n",
      "epoch:22 step:17861 [D loss: 0.024471, acc.: 99.22%] [G loss: 4.989398]\n",
      "epoch:22 step:17862 [D loss: 0.041808, acc.: 99.22%] [G loss: 4.389850]\n",
      "epoch:22 step:17863 [D loss: 0.029245, acc.: 100.00%] [G loss: 4.357138]\n",
      "epoch:22 step:17864 [D loss: 0.045558, acc.: 98.44%] [G loss: 4.291085]\n",
      "epoch:22 step:17865 [D loss: 0.022781, acc.: 100.00%] [G loss: 3.196023]\n",
      "epoch:22 step:17866 [D loss: 0.120098, acc.: 95.31%] [G loss: 4.947890]\n",
      "epoch:22 step:17867 [D loss: 0.783973, acc.: 62.50%] [G loss: 2.818187]\n",
      "epoch:22 step:17868 [D loss: 0.020321, acc.: 100.00%] [G loss: 4.313697]\n",
      "epoch:22 step:17869 [D loss: 0.049637, acc.: 97.66%] [G loss: 3.725016]\n",
      "epoch:22 step:17870 [D loss: 0.031811, acc.: 100.00%] [G loss: 4.742205]\n",
      "epoch:22 step:17871 [D loss: 0.014550, acc.: 100.00%] [G loss: 4.463159]\n",
      "epoch:22 step:17872 [D loss: 0.010654, acc.: 100.00%] [G loss: 4.340715]\n",
      "epoch:22 step:17873 [D loss: 0.016906, acc.: 100.00%] [G loss: 4.311231]\n",
      "epoch:22 step:17874 [D loss: 0.019829, acc.: 100.00%] [G loss: 4.272096]\n",
      "epoch:22 step:17875 [D loss: 0.009390, acc.: 100.00%] [G loss: 3.947658]\n",
      "epoch:22 step:17876 [D loss: 0.009465, acc.: 100.00%] [G loss: 3.488493]\n",
      "epoch:22 step:17877 [D loss: 0.010974, acc.: 100.00%] [G loss: 3.405699]\n",
      "epoch:22 step:17878 [D loss: 0.018637, acc.: 100.00%] [G loss: 3.381748]\n",
      "epoch:22 step:17879 [D loss: 0.036802, acc.: 100.00%] [G loss: 3.129140]\n",
      "epoch:22 step:17880 [D loss: 0.020482, acc.: 100.00%] [G loss: 3.260678]\n",
      "epoch:22 step:17881 [D loss: 0.030439, acc.: 100.00%] [G loss: 2.178671]\n",
      "epoch:22 step:17882 [D loss: 0.021393, acc.: 100.00%] [G loss: 2.098290]\n",
      "epoch:22 step:17883 [D loss: 0.122952, acc.: 98.44%] [G loss: 4.136509]\n",
      "epoch:22 step:17884 [D loss: 0.015235, acc.: 100.00%] [G loss: 5.094110]\n",
      "epoch:22 step:17885 [D loss: 0.107952, acc.: 96.09%] [G loss: 2.319145]\n",
      "epoch:22 step:17886 [D loss: 0.049216, acc.: 98.44%] [G loss: 3.397087]\n",
      "epoch:22 step:17887 [D loss: 0.004940, acc.: 100.00%] [G loss: 3.415193]\n",
      "epoch:22 step:17888 [D loss: 0.016369, acc.: 100.00%] [G loss: 2.938060]\n",
      "epoch:22 step:17889 [D loss: 0.027931, acc.: 100.00%] [G loss: 2.923708]\n",
      "epoch:22 step:17890 [D loss: 0.032116, acc.: 100.00%] [G loss: 3.684471]\n",
      "epoch:22 step:17891 [D loss: 0.033579, acc.: 99.22%] [G loss: 3.645646]\n",
      "epoch:22 step:17892 [D loss: 0.056932, acc.: 99.22%] [G loss: 2.599201]\n",
      "epoch:22 step:17893 [D loss: 0.067949, acc.: 98.44%] [G loss: 2.721497]\n",
      "epoch:22 step:17894 [D loss: 0.051310, acc.: 99.22%] [G loss: 4.456196]\n",
      "epoch:22 step:17895 [D loss: 0.279769, acc.: 89.06%] [G loss: 2.118511]\n",
      "epoch:22 step:17896 [D loss: 0.279051, acc.: 84.38%] [G loss: 6.310226]\n",
      "epoch:22 step:17897 [D loss: 0.341583, acc.: 82.03%] [G loss: 4.023050]\n",
      "epoch:22 step:17898 [D loss: 0.097103, acc.: 96.09%] [G loss: 4.847970]\n",
      "epoch:22 step:17899 [D loss: 0.006252, acc.: 100.00%] [G loss: 4.893600]\n",
      "epoch:22 step:17900 [D loss: 0.017574, acc.: 100.00%] [G loss: 4.495967]\n",
      "epoch:22 step:17901 [D loss: 0.005435, acc.: 100.00%] [G loss: 4.883218]\n",
      "epoch:22 step:17902 [D loss: 0.022498, acc.: 99.22%] [G loss: 5.074514]\n",
      "epoch:22 step:17903 [D loss: 0.030351, acc.: 100.00%] [G loss: 4.504035]\n",
      "epoch:22 step:17904 [D loss: 0.024600, acc.: 100.00%] [G loss: 2.788879]\n",
      "epoch:22 step:17905 [D loss: 0.029467, acc.: 100.00%] [G loss: 2.448136]\n",
      "epoch:22 step:17906 [D loss: 0.079786, acc.: 99.22%] [G loss: 3.408541]\n",
      "epoch:22 step:17907 [D loss: 0.258217, acc.: 89.84%] [G loss: 1.189562]\n",
      "epoch:22 step:17908 [D loss: 0.069127, acc.: 99.22%] [G loss: 2.780994]\n",
      "epoch:22 step:17909 [D loss: 0.024751, acc.: 100.00%] [G loss: 5.034810]\n",
      "epoch:22 step:17910 [D loss: 0.033655, acc.: 99.22%] [G loss: 4.437560]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:22 step:17911 [D loss: 0.037617, acc.: 98.44%] [G loss: 4.179139]\n",
      "epoch:22 step:17912 [D loss: 0.021037, acc.: 100.00%] [G loss: 4.659537]\n",
      "epoch:22 step:17913 [D loss: 0.014067, acc.: 100.00%] [G loss: 4.707040]\n",
      "epoch:22 step:17914 [D loss: 0.164889, acc.: 94.53%] [G loss: 1.878756]\n",
      "epoch:22 step:17915 [D loss: 0.090066, acc.: 97.66%] [G loss: 3.895711]\n",
      "epoch:22 step:17916 [D loss: 0.007064, acc.: 100.00%] [G loss: 4.635812]\n",
      "epoch:22 step:17917 [D loss: 0.098243, acc.: 97.66%] [G loss: 3.310797]\n",
      "epoch:22 step:17918 [D loss: 0.056539, acc.: 100.00%] [G loss: 4.196432]\n",
      "epoch:22 step:17919 [D loss: 0.008149, acc.: 100.00%] [G loss: 4.851148]\n",
      "epoch:22 step:17920 [D loss: 0.011459, acc.: 100.00%] [G loss: 5.284464]\n",
      "epoch:22 step:17921 [D loss: 0.207410, acc.: 92.97%] [G loss: 2.374721]\n",
      "epoch:22 step:17922 [D loss: 0.103451, acc.: 94.53%] [G loss: 5.836941]\n",
      "epoch:22 step:17923 [D loss: 0.089708, acc.: 96.88%] [G loss: 4.234411]\n",
      "epoch:22 step:17924 [D loss: 0.082793, acc.: 97.66%] [G loss: 4.808988]\n",
      "epoch:22 step:17925 [D loss: 0.009999, acc.: 100.00%] [G loss: 4.985586]\n",
      "epoch:22 step:17926 [D loss: 0.031552, acc.: 100.00%] [G loss: 5.597413]\n",
      "epoch:22 step:17927 [D loss: 0.012677, acc.: 100.00%] [G loss: 5.345086]\n",
      "epoch:22 step:17928 [D loss: 0.029755, acc.: 100.00%] [G loss: 4.570313]\n",
      "epoch:22 step:17929 [D loss: 0.025160, acc.: 100.00%] [G loss: 3.065405]\n",
      "epoch:22 step:17930 [D loss: 0.171089, acc.: 92.19%] [G loss: 5.569189]\n",
      "epoch:22 step:17931 [D loss: 0.033981, acc.: 100.00%] [G loss: 6.645604]\n",
      "epoch:22 step:17932 [D loss: 1.206945, acc.: 39.84%] [G loss: 6.798137]\n",
      "epoch:22 step:17933 [D loss: 0.802223, acc.: 60.94%] [G loss: 3.478166]\n",
      "epoch:22 step:17934 [D loss: 0.030083, acc.: 99.22%] [G loss: 3.067926]\n",
      "epoch:22 step:17935 [D loss: 0.037527, acc.: 99.22%] [G loss: 2.067180]\n",
      "epoch:22 step:17936 [D loss: 0.284631, acc.: 84.38%] [G loss: 5.523660]\n",
      "epoch:22 step:17937 [D loss: 0.356896, acc.: 78.91%] [G loss: 3.619620]\n",
      "epoch:22 step:17938 [D loss: 0.023209, acc.: 100.00%] [G loss: 4.875069]\n",
      "epoch:22 step:17939 [D loss: 0.045269, acc.: 99.22%] [G loss: 3.347876]\n",
      "epoch:22 step:17940 [D loss: 0.018040, acc.: 100.00%] [G loss: 4.708999]\n",
      "epoch:22 step:17941 [D loss: 0.018690, acc.: 100.00%] [G loss: 4.320500]\n",
      "epoch:22 step:17942 [D loss: 0.120923, acc.: 96.09%] [G loss: 3.404509]\n",
      "epoch:22 step:17943 [D loss: 0.153682, acc.: 94.53%] [G loss: 6.344199]\n",
      "epoch:22 step:17944 [D loss: 0.310923, acc.: 83.59%] [G loss: 2.599239]\n",
      "epoch:22 step:17945 [D loss: 0.720132, acc.: 68.75%] [G loss: 7.868846]\n",
      "epoch:22 step:17946 [D loss: 0.908015, acc.: 61.72%] [G loss: 5.991412]\n",
      "epoch:22 step:17947 [D loss: 0.143627, acc.: 93.75%] [G loss: 4.784874]\n",
      "epoch:22 step:17948 [D loss: 0.035436, acc.: 99.22%] [G loss: 3.947861]\n",
      "epoch:22 step:17949 [D loss: 0.013365, acc.: 100.00%] [G loss: 4.041235]\n",
      "epoch:22 step:17950 [D loss: 0.028990, acc.: 99.22%] [G loss: 3.323844]\n",
      "epoch:22 step:17951 [D loss: 0.048678, acc.: 99.22%] [G loss: 3.273653]\n",
      "epoch:22 step:17952 [D loss: 0.010268, acc.: 100.00%] [G loss: 3.091344]\n",
      "epoch:22 step:17953 [D loss: 0.055823, acc.: 100.00%] [G loss: 2.739694]\n",
      "epoch:22 step:17954 [D loss: 0.025142, acc.: 99.22%] [G loss: 3.482870]\n",
      "epoch:22 step:17955 [D loss: 0.010500, acc.: 100.00%] [G loss: 3.297656]\n",
      "epoch:22 step:17956 [D loss: 0.185255, acc.: 94.53%] [G loss: 6.125747]\n",
      "epoch:22 step:17957 [D loss: 0.041709, acc.: 98.44%] [G loss: 5.993602]\n",
      "epoch:22 step:17958 [D loss: 0.299509, acc.: 85.16%] [G loss: 1.447248]\n",
      "epoch:22 step:17959 [D loss: 0.571928, acc.: 70.31%] [G loss: 6.988109]\n",
      "epoch:22 step:17960 [D loss: 0.699884, acc.: 66.41%] [G loss: 5.132323]\n",
      "epoch:22 step:17961 [D loss: 0.039609, acc.: 97.66%] [G loss: 4.530400]\n",
      "epoch:22 step:17962 [D loss: 0.015882, acc.: 100.00%] [G loss: 4.742721]\n",
      "epoch:22 step:17963 [D loss: 0.038334, acc.: 99.22%] [G loss: 4.393927]\n",
      "epoch:23 step:17964 [D loss: 0.015880, acc.: 100.00%] [G loss: 5.544868]\n",
      "epoch:23 step:17965 [D loss: 0.015549, acc.: 100.00%] [G loss: 4.560725]\n",
      "epoch:23 step:17966 [D loss: 0.025780, acc.: 100.00%] [G loss: 4.344099]\n",
      "epoch:23 step:17967 [D loss: 0.039046, acc.: 100.00%] [G loss: 4.461878]\n",
      "epoch:23 step:17968 [D loss: 0.009190, acc.: 100.00%] [G loss: 4.870339]\n",
      "epoch:23 step:17969 [D loss: 0.007480, acc.: 100.00%] [G loss: 4.786396]\n",
      "epoch:23 step:17970 [D loss: 0.034464, acc.: 99.22%] [G loss: 4.489309]\n",
      "epoch:23 step:17971 [D loss: 0.023498, acc.: 100.00%] [G loss: 4.278453]\n",
      "epoch:23 step:17972 [D loss: 0.037114, acc.: 100.00%] [G loss: 4.207086]\n",
      "epoch:23 step:17973 [D loss: 0.026988, acc.: 100.00%] [G loss: 4.056904]\n",
      "epoch:23 step:17974 [D loss: 0.007275, acc.: 100.00%] [G loss: 5.038985]\n",
      "epoch:23 step:17975 [D loss: 0.043459, acc.: 100.00%] [G loss: 2.608025]\n",
      "epoch:23 step:17976 [D loss: 0.056603, acc.: 99.22%] [G loss: 3.746885]\n",
      "epoch:23 step:17977 [D loss: 0.015686, acc.: 100.00%] [G loss: 4.434332]\n",
      "epoch:23 step:17978 [D loss: 0.007249, acc.: 100.00%] [G loss: 4.581649]\n",
      "epoch:23 step:17979 [D loss: 0.025886, acc.: 100.00%] [G loss: 2.984809]\n",
      "epoch:23 step:17980 [D loss: 0.029433, acc.: 100.00%] [G loss: 4.032874]\n",
      "epoch:23 step:17981 [D loss: 0.019228, acc.: 100.00%] [G loss: 3.956577]\n",
      "epoch:23 step:17982 [D loss: 0.010976, acc.: 100.00%] [G loss: 2.939947]\n",
      "epoch:23 step:17983 [D loss: 0.022527, acc.: 100.00%] [G loss: 3.468436]\n",
      "epoch:23 step:17984 [D loss: 0.005806, acc.: 100.00%] [G loss: 2.880111]\n",
      "epoch:23 step:17985 [D loss: 0.101420, acc.: 98.44%] [G loss: 2.701204]\n",
      "epoch:23 step:17986 [D loss: 0.109109, acc.: 97.66%] [G loss: 2.615529]\n",
      "epoch:23 step:17987 [D loss: 0.012955, acc.: 100.00%] [G loss: 2.989441]\n",
      "epoch:23 step:17988 [D loss: 0.052180, acc.: 99.22%] [G loss: 2.251617]\n",
      "epoch:23 step:17989 [D loss: 0.032162, acc.: 100.00%] [G loss: 2.179311]\n",
      "epoch:23 step:17990 [D loss: 0.048961, acc.: 100.00%] [G loss: 3.762269]\n",
      "epoch:23 step:17991 [D loss: 0.105681, acc.: 97.66%] [G loss: 1.433691]\n",
      "epoch:23 step:17992 [D loss: 0.125558, acc.: 96.09%] [G loss: 4.559420]\n",
      "epoch:23 step:17993 [D loss: 0.043143, acc.: 100.00%] [G loss: 5.535007]\n",
      "epoch:23 step:17994 [D loss: 0.047700, acc.: 98.44%] [G loss: 4.012961]\n",
      "epoch:23 step:17995 [D loss: 0.030764, acc.: 98.44%] [G loss: 3.763558]\n",
      "epoch:23 step:17996 [D loss: 0.016929, acc.: 100.00%] [G loss: 4.323812]\n",
      "epoch:23 step:17997 [D loss: 0.009736, acc.: 100.00%] [G loss: 3.783703]\n",
      "epoch:23 step:17998 [D loss: 0.013368, acc.: 100.00%] [G loss: 3.465201]\n",
      "epoch:23 step:17999 [D loss: 0.412122, acc.: 83.59%] [G loss: 6.675670]\n",
      "epoch:23 step:18000 [D loss: 0.253332, acc.: 87.50%] [G loss: 5.593946]\n",
      "##############\n",
      "[0.99457611 0.93778731 2.12073147 0.92917145 1.11182474 1.00867922\n",
      " 1.10811045 1.1103265  0.88363244 0.92963366]\n",
      "##########\n",
      "epoch:23 step:18001 [D loss: 0.019075, acc.: 100.00%] [G loss: 4.651504]\n",
      "epoch:23 step:18002 [D loss: 0.014888, acc.: 100.00%] [G loss: 3.687844]\n",
      "epoch:23 step:18003 [D loss: 0.009902, acc.: 100.00%] [G loss: 4.522055]\n",
      "epoch:23 step:18004 [D loss: 0.028750, acc.: 100.00%] [G loss: 4.153300]\n",
      "epoch:23 step:18005 [D loss: 0.007475, acc.: 100.00%] [G loss: 5.267538]\n",
      "epoch:23 step:18006 [D loss: 0.011584, acc.: 100.00%] [G loss: 5.651498]\n",
      "epoch:23 step:18007 [D loss: 0.018816, acc.: 100.00%] [G loss: 4.118057]\n",
      "epoch:23 step:18008 [D loss: 0.057145, acc.: 99.22%] [G loss: 3.754676]\n",
      "epoch:23 step:18009 [D loss: 0.021432, acc.: 100.00%] [G loss: 4.529143]\n",
      "epoch:23 step:18010 [D loss: 0.011891, acc.: 100.00%] [G loss: 4.455036]\n",
      "epoch:23 step:18011 [D loss: 0.007089, acc.: 100.00%] [G loss: 5.103487]\n",
      "epoch:23 step:18012 [D loss: 0.014434, acc.: 100.00%] [G loss: 5.083280]\n",
      "epoch:23 step:18013 [D loss: 0.048258, acc.: 100.00%] [G loss: 3.443318]\n",
      "epoch:23 step:18014 [D loss: 0.035313, acc.: 100.00%] [G loss: 4.638646]\n",
      "epoch:23 step:18015 [D loss: 0.018160, acc.: 100.00%] [G loss: 4.818324]\n",
      "epoch:23 step:18016 [D loss: 0.006696, acc.: 100.00%] [G loss: 4.506472]\n",
      "epoch:23 step:18017 [D loss: 0.133474, acc.: 95.31%] [G loss: 2.883151]\n",
      "epoch:23 step:18018 [D loss: 0.121226, acc.: 95.31%] [G loss: 6.387450]\n",
      "epoch:23 step:18019 [D loss: 0.075050, acc.: 96.09%] [G loss: 5.950532]\n",
      "epoch:23 step:18020 [D loss: 0.003000, acc.: 100.00%] [G loss: 5.242826]\n",
      "epoch:23 step:18021 [D loss: 0.003533, acc.: 100.00%] [G loss: 4.621720]\n",
      "epoch:23 step:18022 [D loss: 0.035471, acc.: 98.44%] [G loss: 5.618639]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:23 step:18023 [D loss: 0.004811, acc.: 100.00%] [G loss: 5.872997]\n",
      "epoch:23 step:18024 [D loss: 0.110548, acc.: 96.09%] [G loss: 3.845174]\n",
      "epoch:23 step:18025 [D loss: 0.056425, acc.: 100.00%] [G loss: 3.350752]\n",
      "epoch:23 step:18026 [D loss: 0.022701, acc.: 100.00%] [G loss: 5.060802]\n",
      "epoch:23 step:18027 [D loss: 0.013787, acc.: 100.00%] [G loss: 3.804383]\n",
      "epoch:23 step:18028 [D loss: 0.179442, acc.: 94.53%] [G loss: 1.118054]\n",
      "epoch:23 step:18029 [D loss: 0.148901, acc.: 94.53%] [G loss: 6.844995]\n",
      "epoch:23 step:18030 [D loss: 0.007367, acc.: 100.00%] [G loss: 8.124891]\n",
      "epoch:23 step:18031 [D loss: 1.073321, acc.: 50.78%] [G loss: 7.819230]\n",
      "epoch:23 step:18032 [D loss: 0.330377, acc.: 83.59%] [G loss: 5.615313]\n",
      "epoch:23 step:18033 [D loss: 0.017454, acc.: 100.00%] [G loss: 4.656918]\n",
      "epoch:23 step:18034 [D loss: 0.017056, acc.: 100.00%] [G loss: 4.103466]\n",
      "epoch:23 step:18035 [D loss: 0.033496, acc.: 100.00%] [G loss: 3.687234]\n",
      "epoch:23 step:18036 [D loss: 0.026125, acc.: 100.00%] [G loss: 3.373745]\n",
      "epoch:23 step:18037 [D loss: 0.010107, acc.: 100.00%] [G loss: 3.853183]\n",
      "epoch:23 step:18038 [D loss: 0.043556, acc.: 99.22%] [G loss: 3.349227]\n",
      "epoch:23 step:18039 [D loss: 0.062268, acc.: 97.66%] [G loss: 4.701827]\n",
      "epoch:23 step:18040 [D loss: 1.012577, acc.: 54.69%] [G loss: 6.566208]\n",
      "epoch:23 step:18041 [D loss: 0.065570, acc.: 96.88%] [G loss: 8.336311]\n",
      "epoch:23 step:18042 [D loss: 1.281671, acc.: 53.12%] [G loss: 3.675503]\n",
      "epoch:23 step:18043 [D loss: 0.030437, acc.: 98.44%] [G loss: 4.045601]\n",
      "epoch:23 step:18044 [D loss: 0.135986, acc.: 94.53%] [G loss: 6.061933]\n",
      "epoch:23 step:18045 [D loss: 0.024445, acc.: 99.22%] [G loss: 6.205760]\n",
      "epoch:23 step:18046 [D loss: 0.078666, acc.: 98.44%] [G loss: 5.205636]\n",
      "epoch:23 step:18047 [D loss: 0.021349, acc.: 100.00%] [G loss: 4.340036]\n",
      "epoch:23 step:18048 [D loss: 0.031177, acc.: 99.22%] [G loss: 4.972878]\n",
      "epoch:23 step:18049 [D loss: 0.011982, acc.: 100.00%] [G loss: 5.236517]\n",
      "epoch:23 step:18050 [D loss: 0.020258, acc.: 100.00%] [G loss: 4.639716]\n",
      "epoch:23 step:18051 [D loss: 0.095268, acc.: 98.44%] [G loss: 5.809905]\n",
      "epoch:23 step:18052 [D loss: 0.005862, acc.: 100.00%] [G loss: 5.430977]\n",
      "epoch:23 step:18053 [D loss: 0.369185, acc.: 83.59%] [G loss: 7.355542]\n",
      "epoch:23 step:18054 [D loss: 0.086544, acc.: 96.09%] [G loss: 6.862715]\n",
      "epoch:23 step:18055 [D loss: 0.023541, acc.: 99.22%] [G loss: 6.994214]\n",
      "epoch:23 step:18056 [D loss: 0.005539, acc.: 100.00%] [G loss: 5.464678]\n",
      "epoch:23 step:18057 [D loss: 0.007217, acc.: 100.00%] [G loss: 4.463274]\n",
      "epoch:23 step:18058 [D loss: 0.027497, acc.: 100.00%] [G loss: 4.408349]\n",
      "epoch:23 step:18059 [D loss: 0.018950, acc.: 100.00%] [G loss: 4.695313]\n",
      "epoch:23 step:18060 [D loss: 0.036007, acc.: 99.22%] [G loss: 5.863381]\n",
      "epoch:23 step:18061 [D loss: 0.012770, acc.: 100.00%] [G loss: 6.549816]\n",
      "epoch:23 step:18062 [D loss: 0.401231, acc.: 82.03%] [G loss: 7.538493]\n",
      "epoch:23 step:18063 [D loss: 0.111448, acc.: 96.09%] [G loss: 7.377278]\n",
      "epoch:23 step:18064 [D loss: 0.007508, acc.: 100.00%] [G loss: 6.515458]\n",
      "epoch:23 step:18065 [D loss: 0.001362, acc.: 100.00%] [G loss: 6.570617]\n",
      "epoch:23 step:18066 [D loss: 0.006009, acc.: 100.00%] [G loss: 5.729290]\n",
      "epoch:23 step:18067 [D loss: 0.005489, acc.: 100.00%] [G loss: 4.974916]\n",
      "epoch:23 step:18068 [D loss: 0.010789, acc.: 100.00%] [G loss: 4.924883]\n",
      "epoch:23 step:18069 [D loss: 0.029885, acc.: 98.44%] [G loss: 5.277153]\n",
      "epoch:23 step:18070 [D loss: 0.006915, acc.: 100.00%] [G loss: 4.630274]\n",
      "epoch:23 step:18071 [D loss: 0.007941, acc.: 100.00%] [G loss: 4.665834]\n",
      "epoch:23 step:18072 [D loss: 0.008508, acc.: 100.00%] [G loss: 3.866649]\n",
      "epoch:23 step:18073 [D loss: 0.634589, acc.: 64.84%] [G loss: 7.300389]\n",
      "epoch:23 step:18074 [D loss: 0.150055, acc.: 93.75%] [G loss: 7.785136]\n",
      "epoch:23 step:18075 [D loss: 0.015478, acc.: 100.00%] [G loss: 6.567407]\n",
      "epoch:23 step:18076 [D loss: 0.001453, acc.: 100.00%] [G loss: 6.433233]\n",
      "epoch:23 step:18077 [D loss: 0.019108, acc.: 99.22%] [G loss: 5.556545]\n",
      "epoch:23 step:18078 [D loss: 0.005676, acc.: 100.00%] [G loss: 5.694486]\n",
      "epoch:23 step:18079 [D loss: 0.054500, acc.: 99.22%] [G loss: 3.302184]\n",
      "epoch:23 step:18080 [D loss: 0.030848, acc.: 100.00%] [G loss: 5.369840]\n",
      "epoch:23 step:18081 [D loss: 0.004149, acc.: 100.00%] [G loss: 6.042390]\n",
      "epoch:23 step:18082 [D loss: 0.019246, acc.: 100.00%] [G loss: 5.732261]\n",
      "epoch:23 step:18083 [D loss: 0.028859, acc.: 99.22%] [G loss: 5.863220]\n",
      "epoch:23 step:18084 [D loss: 0.017621, acc.: 100.00%] [G loss: 4.850524]\n",
      "epoch:23 step:18085 [D loss: 0.025736, acc.: 100.00%] [G loss: 5.259377]\n",
      "epoch:23 step:18086 [D loss: 0.023516, acc.: 100.00%] [G loss: 4.847758]\n",
      "epoch:23 step:18087 [D loss: 0.009481, acc.: 100.00%] [G loss: 4.044742]\n",
      "epoch:23 step:18088 [D loss: 0.016567, acc.: 100.00%] [G loss: 3.151305]\n",
      "epoch:23 step:18089 [D loss: 0.014222, acc.: 100.00%] [G loss: 5.468451]\n",
      "epoch:23 step:18090 [D loss: 0.021475, acc.: 100.00%] [G loss: 4.742707]\n",
      "epoch:23 step:18091 [D loss: 0.121671, acc.: 97.66%] [G loss: 5.790696]\n",
      "epoch:23 step:18092 [D loss: 0.038888, acc.: 99.22%] [G loss: 5.456178]\n",
      "epoch:23 step:18093 [D loss: 0.023141, acc.: 100.00%] [G loss: 5.839207]\n",
      "epoch:23 step:18094 [D loss: 0.001804, acc.: 100.00%] [G loss: 3.649641]\n",
      "epoch:23 step:18095 [D loss: 6.335888, acc.: 18.75%] [G loss: 7.455626]\n",
      "epoch:23 step:18096 [D loss: 0.948813, acc.: 67.97%] [G loss: 5.765273]\n",
      "epoch:23 step:18097 [D loss: 0.286535, acc.: 85.16%] [G loss: 1.853840]\n",
      "epoch:23 step:18098 [D loss: 0.218320, acc.: 89.84%] [G loss: 3.397012]\n",
      "epoch:23 step:18099 [D loss: 0.050545, acc.: 99.22%] [G loss: 4.582596]\n",
      "epoch:23 step:18100 [D loss: 0.092515, acc.: 98.44%] [G loss: 4.824121]\n",
      "epoch:23 step:18101 [D loss: 0.055611, acc.: 98.44%] [G loss: 3.648083]\n",
      "epoch:23 step:18102 [D loss: 0.330208, acc.: 87.50%] [G loss: 3.047453]\n",
      "epoch:23 step:18103 [D loss: 0.047328, acc.: 100.00%] [G loss: 3.927520]\n",
      "epoch:23 step:18104 [D loss: 0.096200, acc.: 96.09%] [G loss: 3.965450]\n",
      "epoch:23 step:18105 [D loss: 0.079958, acc.: 98.44%] [G loss: 3.323287]\n",
      "epoch:23 step:18106 [D loss: 0.292986, acc.: 89.06%] [G loss: 2.818457]\n",
      "epoch:23 step:18107 [D loss: 0.098432, acc.: 97.66%] [G loss: 4.863873]\n",
      "epoch:23 step:18108 [D loss: 0.139611, acc.: 92.97%] [G loss: 1.223450]\n",
      "epoch:23 step:18109 [D loss: 0.264970, acc.: 86.72%] [G loss: 4.378115]\n",
      "epoch:23 step:18110 [D loss: 0.303122, acc.: 84.38%] [G loss: 3.501330]\n",
      "epoch:23 step:18111 [D loss: 0.148442, acc.: 95.31%] [G loss: 2.746183]\n",
      "epoch:23 step:18112 [D loss: 0.062468, acc.: 98.44%] [G loss: 2.815392]\n",
      "epoch:23 step:18113 [D loss: 0.236169, acc.: 90.62%] [G loss: 3.070511]\n",
      "epoch:23 step:18114 [D loss: 0.211628, acc.: 91.41%] [G loss: 5.930091]\n",
      "epoch:23 step:18115 [D loss: 2.044321, acc.: 25.78%] [G loss: 7.454609]\n",
      "epoch:23 step:18116 [D loss: 0.978438, acc.: 58.59%] [G loss: 6.808277]\n",
      "epoch:23 step:18117 [D loss: 0.258797, acc.: 85.94%] [G loss: 4.365207]\n",
      "epoch:23 step:18118 [D loss: 0.050224, acc.: 97.66%] [G loss: 2.964189]\n",
      "epoch:23 step:18119 [D loss: 0.097778, acc.: 96.88%] [G loss: 3.919852]\n",
      "epoch:23 step:18120 [D loss: 0.088066, acc.: 99.22%] [G loss: 4.224685]\n",
      "epoch:23 step:18121 [D loss: 0.039696, acc.: 98.44%] [G loss: 3.706475]\n",
      "epoch:23 step:18122 [D loss: 0.115103, acc.: 96.09%] [G loss: 2.680872]\n",
      "epoch:23 step:18123 [D loss: 0.060930, acc.: 98.44%] [G loss: 2.267271]\n",
      "epoch:23 step:18124 [D loss: 0.129413, acc.: 96.09%] [G loss: 4.022336]\n",
      "epoch:23 step:18125 [D loss: 0.020879, acc.: 100.00%] [G loss: 4.394541]\n",
      "epoch:23 step:18126 [D loss: 0.266089, acc.: 88.28%] [G loss: 3.438415]\n",
      "epoch:23 step:18127 [D loss: 0.166619, acc.: 96.09%] [G loss: 4.904994]\n",
      "epoch:23 step:18128 [D loss: 0.121973, acc.: 96.09%] [G loss: 3.071035]\n",
      "epoch:23 step:18129 [D loss: 0.035149, acc.: 100.00%] [G loss: 3.249904]\n",
      "epoch:23 step:18130 [D loss: 0.052955, acc.: 99.22%] [G loss: 4.170835]\n",
      "epoch:23 step:18131 [D loss: 0.029811, acc.: 100.00%] [G loss: 3.955400]\n",
      "epoch:23 step:18132 [D loss: 0.033108, acc.: 100.00%] [G loss: 3.935449]\n",
      "epoch:23 step:18133 [D loss: 0.131276, acc.: 96.88%] [G loss: 2.459067]\n",
      "epoch:23 step:18134 [D loss: 0.037116, acc.: 100.00%] [G loss: 3.031765]\n",
      "epoch:23 step:18135 [D loss: 0.022082, acc.: 100.00%] [G loss: 3.716739]\n",
      "epoch:23 step:18136 [D loss: 0.081663, acc.: 99.22%] [G loss: 4.705172]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:23 step:18137 [D loss: 0.104987, acc.: 96.88%] [G loss: 3.970132]\n",
      "epoch:23 step:18138 [D loss: 0.039034, acc.: 100.00%] [G loss: 3.929437]\n",
      "epoch:23 step:18139 [D loss: 0.128963, acc.: 97.66%] [G loss: 3.342381]\n",
      "epoch:23 step:18140 [D loss: 0.045945, acc.: 100.00%] [G loss: 4.344199]\n",
      "epoch:23 step:18141 [D loss: 0.094995, acc.: 97.66%] [G loss: 2.718918]\n",
      "epoch:23 step:18142 [D loss: 0.064002, acc.: 100.00%] [G loss: 3.245380]\n",
      "epoch:23 step:18143 [D loss: 0.051377, acc.: 100.00%] [G loss: 4.249788]\n",
      "epoch:23 step:18144 [D loss: 0.055779, acc.: 100.00%] [G loss: 3.153302]\n",
      "epoch:23 step:18145 [D loss: 0.055615, acc.: 99.22%] [G loss: 3.597012]\n",
      "epoch:23 step:18146 [D loss: 0.047879, acc.: 99.22%] [G loss: 3.727337]\n",
      "epoch:23 step:18147 [D loss: 0.044897, acc.: 100.00%] [G loss: 3.096106]\n",
      "epoch:23 step:18148 [D loss: 0.031337, acc.: 100.00%] [G loss: 4.193972]\n",
      "epoch:23 step:18149 [D loss: 0.031364, acc.: 100.00%] [G loss: 4.279663]\n",
      "epoch:23 step:18150 [D loss: 0.083254, acc.: 99.22%] [G loss: 3.355808]\n",
      "epoch:23 step:18151 [D loss: 0.172129, acc.: 94.53%] [G loss: 3.872634]\n",
      "epoch:23 step:18152 [D loss: 0.096731, acc.: 96.88%] [G loss: 3.433378]\n",
      "epoch:23 step:18153 [D loss: 0.023421, acc.: 100.00%] [G loss: 3.233491]\n",
      "epoch:23 step:18154 [D loss: 0.008347, acc.: 100.00%] [G loss: 4.453224]\n",
      "epoch:23 step:18155 [D loss: 0.024441, acc.: 100.00%] [G loss: 4.265000]\n",
      "epoch:23 step:18156 [D loss: 0.024750, acc.: 100.00%] [G loss: 3.780377]\n",
      "epoch:23 step:18157 [D loss: 0.012863, acc.: 100.00%] [G loss: 3.462178]\n",
      "epoch:23 step:18158 [D loss: 0.042859, acc.: 100.00%] [G loss: 3.179312]\n",
      "epoch:23 step:18159 [D loss: 0.071599, acc.: 97.66%] [G loss: 2.432045]\n",
      "epoch:23 step:18160 [D loss: 0.036820, acc.: 100.00%] [G loss: 2.578436]\n",
      "epoch:23 step:18161 [D loss: 0.039252, acc.: 99.22%] [G loss: 3.528220]\n",
      "epoch:23 step:18162 [D loss: 0.025905, acc.: 99.22%] [G loss: 3.658860]\n",
      "epoch:23 step:18163 [D loss: 0.014289, acc.: 100.00%] [G loss: 2.959621]\n",
      "epoch:23 step:18164 [D loss: 0.010478, acc.: 100.00%] [G loss: 3.767410]\n",
      "epoch:23 step:18165 [D loss: 0.030462, acc.: 100.00%] [G loss: 2.646595]\n",
      "epoch:23 step:18166 [D loss: 0.045900, acc.: 100.00%] [G loss: 4.467778]\n",
      "epoch:23 step:18167 [D loss: 0.025660, acc.: 100.00%] [G loss: 4.178568]\n",
      "epoch:23 step:18168 [D loss: 0.713985, acc.: 67.19%] [G loss: 7.271209]\n",
      "epoch:23 step:18169 [D loss: 1.486647, acc.: 54.69%] [G loss: 3.696917]\n",
      "epoch:23 step:18170 [D loss: 0.142850, acc.: 92.97%] [G loss: 3.862170]\n",
      "epoch:23 step:18171 [D loss: 0.058057, acc.: 97.66%] [G loss: 4.146830]\n",
      "epoch:23 step:18172 [D loss: 0.054164, acc.: 98.44%] [G loss: 2.934188]\n",
      "epoch:23 step:18173 [D loss: 0.117092, acc.: 95.31%] [G loss: 4.883767]\n",
      "epoch:23 step:18174 [D loss: 0.011158, acc.: 100.00%] [G loss: 4.924555]\n",
      "epoch:23 step:18175 [D loss: 0.080497, acc.: 96.88%] [G loss: 4.727057]\n",
      "epoch:23 step:18176 [D loss: 0.067079, acc.: 99.22%] [G loss: 3.342404]\n",
      "epoch:23 step:18177 [D loss: 0.049028, acc.: 100.00%] [G loss: 3.603399]\n",
      "epoch:23 step:18178 [D loss: 0.085636, acc.: 97.66%] [G loss: 4.217448]\n",
      "epoch:23 step:18179 [D loss: 0.029733, acc.: 100.00%] [G loss: 4.488704]\n",
      "epoch:23 step:18180 [D loss: 0.014378, acc.: 100.00%] [G loss: 4.565861]\n",
      "epoch:23 step:18181 [D loss: 0.171741, acc.: 92.97%] [G loss: 2.332963]\n",
      "epoch:23 step:18182 [D loss: 0.062238, acc.: 99.22%] [G loss: 2.837222]\n",
      "epoch:23 step:18183 [D loss: 0.017858, acc.: 100.00%] [G loss: 3.257798]\n",
      "epoch:23 step:18184 [D loss: 0.007991, acc.: 100.00%] [G loss: 3.173528]\n",
      "epoch:23 step:18185 [D loss: 0.083743, acc.: 98.44%] [G loss: 4.516779]\n",
      "epoch:23 step:18186 [D loss: 0.023857, acc.: 100.00%] [G loss: 5.203377]\n",
      "epoch:23 step:18187 [D loss: 0.020520, acc.: 99.22%] [G loss: 4.715746]\n",
      "epoch:23 step:18188 [D loss: 0.015611, acc.: 100.00%] [G loss: 4.180184]\n",
      "epoch:23 step:18189 [D loss: 0.011572, acc.: 100.00%] [G loss: 4.114003]\n",
      "epoch:23 step:18190 [D loss: 0.011552, acc.: 100.00%] [G loss: 4.125210]\n",
      "epoch:23 step:18191 [D loss: 0.028521, acc.: 100.00%] [G loss: 3.753347]\n",
      "epoch:23 step:18192 [D loss: 0.029195, acc.: 100.00%] [G loss: 3.755446]\n",
      "epoch:23 step:18193 [D loss: 0.037258, acc.: 99.22%] [G loss: 3.715778]\n",
      "epoch:23 step:18194 [D loss: 0.010678, acc.: 100.00%] [G loss: 4.212421]\n",
      "epoch:23 step:18195 [D loss: 0.212604, acc.: 92.19%] [G loss: 1.854460]\n",
      "epoch:23 step:18196 [D loss: 0.218791, acc.: 91.41%] [G loss: 6.993225]\n",
      "epoch:23 step:18197 [D loss: 0.696085, acc.: 67.97%] [G loss: 3.791429]\n",
      "epoch:23 step:18198 [D loss: 0.128196, acc.: 92.97%] [G loss: 3.469853]\n",
      "epoch:23 step:18199 [D loss: 0.002557, acc.: 100.00%] [G loss: 5.682311]\n",
      "epoch:23 step:18200 [D loss: 0.006079, acc.: 100.00%] [G loss: 6.288263]\n",
      "##############\n",
      "[0.89385346 0.95070851 0.98139093 0.86041211 0.98482788 0.99777221\n",
      " 2.11526987 1.11171233 1.04674005 1.03049241]\n",
      "##########\n",
      "epoch:23 step:18201 [D loss: 0.002569, acc.: 100.00%] [G loss: 5.350454]\n",
      "epoch:23 step:18202 [D loss: 0.002093, acc.: 100.00%] [G loss: 6.009940]\n",
      "epoch:23 step:18203 [D loss: 0.034141, acc.: 99.22%] [G loss: 5.004905]\n",
      "epoch:23 step:18204 [D loss: 0.008458, acc.: 100.00%] [G loss: 3.967616]\n",
      "epoch:23 step:18205 [D loss: 0.013779, acc.: 100.00%] [G loss: 4.668930]\n",
      "epoch:23 step:18206 [D loss: 0.008749, acc.: 100.00%] [G loss: 3.724855]\n",
      "epoch:23 step:18207 [D loss: 0.007962, acc.: 100.00%] [G loss: 3.834022]\n",
      "epoch:23 step:18208 [D loss: 0.007636, acc.: 100.00%] [G loss: 4.060789]\n",
      "epoch:23 step:18209 [D loss: 0.017558, acc.: 100.00%] [G loss: 3.637203]\n",
      "epoch:23 step:18210 [D loss: 0.033810, acc.: 100.00%] [G loss: 3.056330]\n",
      "epoch:23 step:18211 [D loss: 0.191016, acc.: 92.19%] [G loss: 6.566689]\n",
      "epoch:23 step:18212 [D loss: 0.051281, acc.: 98.44%] [G loss: 7.337554]\n",
      "epoch:23 step:18213 [D loss: 1.081147, acc.: 42.97%] [G loss: 5.532331]\n",
      "epoch:23 step:18214 [D loss: 0.002848, acc.: 100.00%] [G loss: 7.052557]\n",
      "epoch:23 step:18215 [D loss: 0.825037, acc.: 56.25%] [G loss: 3.024028]\n",
      "epoch:23 step:18216 [D loss: 0.242099, acc.: 90.62%] [G loss: 4.686511]\n",
      "epoch:23 step:18217 [D loss: 0.003627, acc.: 100.00%] [G loss: 6.018949]\n",
      "epoch:23 step:18218 [D loss: 0.022041, acc.: 99.22%] [G loss: 6.354530]\n",
      "epoch:23 step:18219 [D loss: 0.025938, acc.: 100.00%] [G loss: 5.124656]\n",
      "epoch:23 step:18220 [D loss: 0.010560, acc.: 100.00%] [G loss: 4.866076]\n",
      "epoch:23 step:18221 [D loss: 0.014841, acc.: 100.00%] [G loss: 4.715763]\n",
      "epoch:23 step:18222 [D loss: 0.002396, acc.: 100.00%] [G loss: 3.852209]\n",
      "epoch:23 step:18223 [D loss: 0.063075, acc.: 99.22%] [G loss: 4.396692]\n",
      "epoch:23 step:18224 [D loss: 0.039014, acc.: 100.00%] [G loss: 3.162708]\n",
      "epoch:23 step:18225 [D loss: 0.060131, acc.: 99.22%] [G loss: 5.581066]\n",
      "epoch:23 step:18226 [D loss: 0.003882, acc.: 100.00%] [G loss: 5.552723]\n",
      "epoch:23 step:18227 [D loss: 0.133942, acc.: 94.53%] [G loss: 2.364248]\n",
      "epoch:23 step:18228 [D loss: 0.018738, acc.: 100.00%] [G loss: 2.393287]\n",
      "epoch:23 step:18229 [D loss: 0.286010, acc.: 86.72%] [G loss: 6.435787]\n",
      "epoch:23 step:18230 [D loss: 0.229987, acc.: 90.62%] [G loss: 4.735633]\n",
      "epoch:23 step:18231 [D loss: 0.005182, acc.: 100.00%] [G loss: 4.540416]\n",
      "epoch:23 step:18232 [D loss: 0.038888, acc.: 100.00%] [G loss: 4.084323]\n",
      "epoch:23 step:18233 [D loss: 0.038157, acc.: 100.00%] [G loss: 2.558470]\n",
      "epoch:23 step:18234 [D loss: 0.098645, acc.: 96.88%] [G loss: 4.614198]\n",
      "epoch:23 step:18235 [D loss: 0.011068, acc.: 100.00%] [G loss: 6.570309]\n",
      "epoch:23 step:18236 [D loss: 0.755686, acc.: 57.03%] [G loss: 1.034853]\n",
      "epoch:23 step:18237 [D loss: 0.203158, acc.: 92.19%] [G loss: 6.519181]\n",
      "epoch:23 step:18238 [D loss: 0.502331, acc.: 75.78%] [G loss: 0.165057]\n",
      "epoch:23 step:18239 [D loss: 1.536689, acc.: 57.03%] [G loss: 9.138809]\n",
      "epoch:23 step:18240 [D loss: 1.699722, acc.: 50.00%] [G loss: 7.279362]\n",
      "epoch:23 step:18241 [D loss: 0.057293, acc.: 97.66%] [G loss: 5.360523]\n",
      "epoch:23 step:18242 [D loss: 0.036702, acc.: 99.22%] [G loss: 3.969192]\n",
      "epoch:23 step:18243 [D loss: 0.022274, acc.: 100.00%] [G loss: 4.838067]\n",
      "epoch:23 step:18244 [D loss: 0.009763, acc.: 100.00%] [G loss: 4.276960]\n",
      "epoch:23 step:18245 [D loss: 0.047492, acc.: 99.22%] [G loss: 4.310739]\n",
      "epoch:23 step:18246 [D loss: 0.013080, acc.: 100.00%] [G loss: 4.883526]\n",
      "epoch:23 step:18247 [D loss: 0.021091, acc.: 100.00%] [G loss: 3.749109]\n",
      "epoch:23 step:18248 [D loss: 0.026708, acc.: 100.00%] [G loss: 4.687905]\n",
      "epoch:23 step:18249 [D loss: 0.143706, acc.: 95.31%] [G loss: 3.001821]\n",
      "epoch:23 step:18250 [D loss: 0.072902, acc.: 99.22%] [G loss: 3.610637]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:23 step:18251 [D loss: 0.006738, acc.: 100.00%] [G loss: 4.062473]\n",
      "epoch:23 step:18252 [D loss: 0.027317, acc.: 99.22%] [G loss: 3.426963]\n",
      "epoch:23 step:18253 [D loss: 0.026009, acc.: 100.00%] [G loss: 3.622460]\n",
      "epoch:23 step:18254 [D loss: 0.131746, acc.: 96.09%] [G loss: 3.154606]\n",
      "epoch:23 step:18255 [D loss: 0.020316, acc.: 100.00%] [G loss: 3.495225]\n",
      "epoch:23 step:18256 [D loss: 0.032562, acc.: 100.00%] [G loss: 2.812882]\n",
      "epoch:23 step:18257 [D loss: 0.118958, acc.: 96.88%] [G loss: 2.120097]\n",
      "epoch:23 step:18258 [D loss: 0.098590, acc.: 98.44%] [G loss: 3.733653]\n",
      "epoch:23 step:18259 [D loss: 0.158470, acc.: 96.09%] [G loss: 3.399662]\n",
      "epoch:23 step:18260 [D loss: 0.024696, acc.: 99.22%] [G loss: 4.118974]\n",
      "epoch:23 step:18261 [D loss: 0.035484, acc.: 100.00%] [G loss: 4.170427]\n",
      "epoch:23 step:18262 [D loss: 0.064335, acc.: 98.44%] [G loss: 2.769066]\n",
      "epoch:23 step:18263 [D loss: 0.123161, acc.: 98.44%] [G loss: 5.542349]\n",
      "epoch:23 step:18264 [D loss: 0.031396, acc.: 99.22%] [G loss: 5.733922]\n",
      "epoch:23 step:18265 [D loss: 0.142435, acc.: 95.31%] [G loss: 4.311202]\n",
      "epoch:23 step:18266 [D loss: 0.023878, acc.: 100.00%] [G loss: 3.943878]\n",
      "epoch:23 step:18267 [D loss: 0.015023, acc.: 100.00%] [G loss: 3.714209]\n",
      "epoch:23 step:18268 [D loss: 0.061716, acc.: 97.66%] [G loss: 4.988470]\n",
      "epoch:23 step:18269 [D loss: 0.014047, acc.: 100.00%] [G loss: 5.619550]\n",
      "epoch:23 step:18270 [D loss: 0.030819, acc.: 100.00%] [G loss: 4.904920]\n",
      "epoch:23 step:18271 [D loss: 0.019279, acc.: 100.00%] [G loss: 4.530126]\n",
      "epoch:23 step:18272 [D loss: 0.128815, acc.: 96.88%] [G loss: 5.119044]\n",
      "epoch:23 step:18273 [D loss: 0.014164, acc.: 100.00%] [G loss: 5.191151]\n",
      "epoch:23 step:18274 [D loss: 0.056990, acc.: 100.00%] [G loss: 3.465877]\n",
      "epoch:23 step:18275 [D loss: 0.082407, acc.: 96.88%] [G loss: 3.467840]\n",
      "epoch:23 step:18276 [D loss: 0.021418, acc.: 100.00%] [G loss: 4.744789]\n",
      "epoch:23 step:18277 [D loss: 0.055489, acc.: 99.22%] [G loss: 4.416133]\n",
      "epoch:23 step:18278 [D loss: 0.164797, acc.: 93.75%] [G loss: 3.591193]\n",
      "epoch:23 step:18279 [D loss: 0.016170, acc.: 100.00%] [G loss: 3.949569]\n",
      "epoch:23 step:18280 [D loss: 0.008890, acc.: 100.00%] [G loss: 3.931672]\n",
      "epoch:23 step:18281 [D loss: 0.008227, acc.: 100.00%] [G loss: 3.831597]\n",
      "epoch:23 step:18282 [D loss: 0.011304, acc.: 100.00%] [G loss: 4.995619]\n",
      "epoch:23 step:18283 [D loss: 0.673122, acc.: 65.62%] [G loss: 7.221437]\n",
      "epoch:23 step:18284 [D loss: 0.349161, acc.: 82.03%] [G loss: 5.722634]\n",
      "epoch:23 step:18285 [D loss: 0.004074, acc.: 100.00%] [G loss: 5.924124]\n",
      "epoch:23 step:18286 [D loss: 0.007365, acc.: 100.00%] [G loss: 4.902802]\n",
      "epoch:23 step:18287 [D loss: 0.027757, acc.: 100.00%] [G loss: 5.126215]\n",
      "epoch:23 step:18288 [D loss: 0.014856, acc.: 100.00%] [G loss: 4.142428]\n",
      "epoch:23 step:18289 [D loss: 0.015511, acc.: 100.00%] [G loss: 4.752939]\n",
      "epoch:23 step:18290 [D loss: 0.012691, acc.: 100.00%] [G loss: 3.569312]\n",
      "epoch:23 step:18291 [D loss: 0.004903, acc.: 100.00%] [G loss: 4.356091]\n",
      "epoch:23 step:18292 [D loss: 0.019631, acc.: 100.00%] [G loss: 3.324933]\n",
      "epoch:23 step:18293 [D loss: 0.058571, acc.: 99.22%] [G loss: 4.842546]\n",
      "epoch:23 step:18294 [D loss: 0.007715, acc.: 100.00%] [G loss: 6.046885]\n",
      "epoch:23 step:18295 [D loss: 0.004372, acc.: 100.00%] [G loss: 5.539976]\n",
      "epoch:23 step:18296 [D loss: 0.010154, acc.: 100.00%] [G loss: 4.247300]\n",
      "epoch:23 step:18297 [D loss: 0.013913, acc.: 100.00%] [G loss: 4.349800]\n",
      "epoch:23 step:18298 [D loss: 0.013120, acc.: 100.00%] [G loss: 4.594753]\n",
      "epoch:23 step:18299 [D loss: 1.845522, acc.: 28.91%] [G loss: 8.160334]\n",
      "epoch:23 step:18300 [D loss: 0.018772, acc.: 99.22%] [G loss: 8.979623]\n",
      "epoch:23 step:18301 [D loss: 1.815453, acc.: 50.78%] [G loss: 5.893819]\n",
      "epoch:23 step:18302 [D loss: 0.021408, acc.: 100.00%] [G loss: 2.480962]\n",
      "epoch:23 step:18303 [D loss: 0.022646, acc.: 100.00%] [G loss: 1.394611]\n",
      "epoch:23 step:18304 [D loss: 0.122633, acc.: 93.75%] [G loss: 3.915448]\n",
      "epoch:23 step:18305 [D loss: 0.002579, acc.: 100.00%] [G loss: 4.423630]\n",
      "epoch:23 step:18306 [D loss: 0.193776, acc.: 92.19%] [G loss: 5.787974]\n",
      "epoch:23 step:18307 [D loss: 0.016158, acc.: 100.00%] [G loss: 5.823090]\n",
      "epoch:23 step:18308 [D loss: 0.345108, acc.: 82.81%] [G loss: 3.756822]\n",
      "epoch:23 step:18309 [D loss: 0.180318, acc.: 92.97%] [G loss: 3.948020]\n",
      "epoch:23 step:18310 [D loss: 0.008040, acc.: 100.00%] [G loss: 5.440630]\n",
      "epoch:23 step:18311 [D loss: 0.006441, acc.: 100.00%] [G loss: 4.290040]\n",
      "epoch:23 step:18312 [D loss: 0.011162, acc.: 100.00%] [G loss: 4.494573]\n",
      "epoch:23 step:18313 [D loss: 0.050428, acc.: 99.22%] [G loss: 3.914084]\n",
      "epoch:23 step:18314 [D loss: 0.034774, acc.: 99.22%] [G loss: 3.610400]\n",
      "epoch:23 step:18315 [D loss: 0.039261, acc.: 99.22%] [G loss: 3.806065]\n",
      "epoch:23 step:18316 [D loss: 0.051663, acc.: 99.22%] [G loss: 4.292737]\n",
      "epoch:23 step:18317 [D loss: 0.153869, acc.: 94.53%] [G loss: 4.256473]\n",
      "epoch:23 step:18318 [D loss: 0.072406, acc.: 98.44%] [G loss: 2.574742]\n",
      "epoch:23 step:18319 [D loss: 0.058924, acc.: 99.22%] [G loss: 3.949769]\n",
      "epoch:23 step:18320 [D loss: 0.015204, acc.: 100.00%] [G loss: 4.009885]\n",
      "epoch:23 step:18321 [D loss: 0.164351, acc.: 96.88%] [G loss: 2.971895]\n",
      "epoch:23 step:18322 [D loss: 0.015013, acc.: 100.00%] [G loss: 2.065796]\n",
      "epoch:23 step:18323 [D loss: 0.010373, acc.: 100.00%] [G loss: 2.499173]\n",
      "epoch:23 step:18324 [D loss: 0.092247, acc.: 99.22%] [G loss: 2.125342]\n",
      "epoch:23 step:18325 [D loss: 0.014443, acc.: 100.00%] [G loss: 2.701522]\n",
      "epoch:23 step:18326 [D loss: 0.021402, acc.: 100.00%] [G loss: 3.439193]\n",
      "epoch:23 step:18327 [D loss: 0.149145, acc.: 96.88%] [G loss: 4.661610]\n",
      "epoch:23 step:18328 [D loss: 0.025722, acc.: 99.22%] [G loss: 4.431800]\n",
      "epoch:23 step:18329 [D loss: 0.055241, acc.: 100.00%] [G loss: 3.381601]\n",
      "epoch:23 step:18330 [D loss: 0.025993, acc.: 100.00%] [G loss: 3.012304]\n",
      "epoch:23 step:18331 [D loss: 0.024247, acc.: 100.00%] [G loss: 3.213682]\n",
      "epoch:23 step:18332 [D loss: 0.029403, acc.: 100.00%] [G loss: 2.656313]\n",
      "epoch:23 step:18333 [D loss: 0.222614, acc.: 93.75%] [G loss: 5.211798]\n",
      "epoch:23 step:18334 [D loss: 0.025742, acc.: 100.00%] [G loss: 6.739892]\n",
      "epoch:23 step:18335 [D loss: 0.279604, acc.: 85.94%] [G loss: 1.086357]\n",
      "epoch:23 step:18336 [D loss: 0.526805, acc.: 72.66%] [G loss: 7.476577]\n",
      "epoch:23 step:18337 [D loss: 1.235375, acc.: 55.47%] [G loss: 5.332252]\n",
      "epoch:23 step:18338 [D loss: 0.009883, acc.: 100.00%] [G loss: 4.143089]\n",
      "epoch:23 step:18339 [D loss: 0.029116, acc.: 100.00%] [G loss: 4.182139]\n",
      "epoch:23 step:18340 [D loss: 0.011030, acc.: 100.00%] [G loss: 3.753451]\n",
      "epoch:23 step:18341 [D loss: 0.086207, acc.: 96.09%] [G loss: 4.519037]\n",
      "epoch:23 step:18342 [D loss: 0.005075, acc.: 100.00%] [G loss: 5.390149]\n",
      "epoch:23 step:18343 [D loss: 0.010042, acc.: 100.00%] [G loss: 5.392842]\n",
      "epoch:23 step:18344 [D loss: 0.011019, acc.: 100.00%] [G loss: 4.125416]\n",
      "epoch:23 step:18345 [D loss: 0.007657, acc.: 100.00%] [G loss: 4.119502]\n",
      "epoch:23 step:18346 [D loss: 0.019391, acc.: 100.00%] [G loss: 4.451140]\n",
      "epoch:23 step:18347 [D loss: 0.010102, acc.: 100.00%] [G loss: 4.136344]\n",
      "epoch:23 step:18348 [D loss: 0.033244, acc.: 100.00%] [G loss: 3.385478]\n",
      "epoch:23 step:18349 [D loss: 0.034972, acc.: 100.00%] [G loss: 3.282462]\n",
      "epoch:23 step:18350 [D loss: 0.034083, acc.: 99.22%] [G loss: 3.928863]\n",
      "epoch:23 step:18351 [D loss: 0.019745, acc.: 100.00%] [G loss: 3.955286]\n",
      "epoch:23 step:18352 [D loss: 0.017337, acc.: 100.00%] [G loss: 3.712708]\n",
      "epoch:23 step:18353 [D loss: 0.048297, acc.: 100.00%] [G loss: 3.364455]\n",
      "epoch:23 step:18354 [D loss: 0.020690, acc.: 100.00%] [G loss: 4.547928]\n",
      "epoch:23 step:18355 [D loss: 0.068283, acc.: 97.66%] [G loss: 3.685188]\n",
      "epoch:23 step:18356 [D loss: 0.044513, acc.: 98.44%] [G loss: 3.343760]\n",
      "epoch:23 step:18357 [D loss: 0.025259, acc.: 100.00%] [G loss: 2.131333]\n",
      "epoch:23 step:18358 [D loss: 0.016248, acc.: 100.00%] [G loss: 2.192760]\n",
      "epoch:23 step:18359 [D loss: 0.151270, acc.: 92.97%] [G loss: 5.738626]\n",
      "epoch:23 step:18360 [D loss: 0.057161, acc.: 98.44%] [G loss: 5.683276]\n",
      "epoch:23 step:18361 [D loss: 0.008029, acc.: 100.00%] [G loss: 5.553551]\n",
      "epoch:23 step:18362 [D loss: 0.021280, acc.: 100.00%] [G loss: 4.829943]\n",
      "epoch:23 step:18363 [D loss: 0.012428, acc.: 100.00%] [G loss: 3.945428]\n",
      "epoch:23 step:18364 [D loss: 0.930331, acc.: 53.12%] [G loss: 7.068185]\n",
      "epoch:23 step:18365 [D loss: 0.400494, acc.: 78.91%] [G loss: 5.917336]\n",
      "epoch:23 step:18366 [D loss: 0.140683, acc.: 92.97%] [G loss: 4.348738]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:23 step:18367 [D loss: 0.200379, acc.: 90.62%] [G loss: 4.668587]\n",
      "epoch:23 step:18368 [D loss: 0.009941, acc.: 100.00%] [G loss: 5.369128]\n",
      "epoch:23 step:18369 [D loss: 0.010350, acc.: 100.00%] [G loss: 4.543521]\n",
      "epoch:23 step:18370 [D loss: 0.010899, acc.: 100.00%] [G loss: 4.284405]\n",
      "epoch:23 step:18371 [D loss: 0.034115, acc.: 100.00%] [G loss: 4.191243]\n",
      "epoch:23 step:18372 [D loss: 0.178636, acc.: 94.53%] [G loss: 3.127708]\n",
      "epoch:23 step:18373 [D loss: 0.099727, acc.: 97.66%] [G loss: 4.252857]\n",
      "epoch:23 step:18374 [D loss: 0.040636, acc.: 99.22%] [G loss: 4.745290]\n",
      "epoch:23 step:18375 [D loss: 0.023514, acc.: 100.00%] [G loss: 4.354020]\n",
      "epoch:23 step:18376 [D loss: 0.039307, acc.: 100.00%] [G loss: 3.011947]\n",
      "epoch:23 step:18377 [D loss: 0.047396, acc.: 99.22%] [G loss: 4.188387]\n",
      "epoch:23 step:18378 [D loss: 0.007828, acc.: 100.00%] [G loss: 4.139572]\n",
      "epoch:23 step:18379 [D loss: 0.030827, acc.: 100.00%] [G loss: 3.629442]\n",
      "epoch:23 step:18380 [D loss: 0.029230, acc.: 99.22%] [G loss: 3.392151]\n",
      "epoch:23 step:18381 [D loss: 0.007530, acc.: 100.00%] [G loss: 2.276051]\n",
      "epoch:23 step:18382 [D loss: 0.086142, acc.: 99.22%] [G loss: 4.834051]\n",
      "epoch:23 step:18383 [D loss: 0.124834, acc.: 94.53%] [G loss: 3.809966]\n",
      "epoch:23 step:18384 [D loss: 0.011388, acc.: 100.00%] [G loss: 1.356942]\n",
      "epoch:23 step:18385 [D loss: 0.151432, acc.: 96.09%] [G loss: 4.948159]\n",
      "epoch:23 step:18386 [D loss: 0.081670, acc.: 97.66%] [G loss: 5.993526]\n",
      "epoch:23 step:18387 [D loss: 1.858281, acc.: 22.66%] [G loss: 8.077223]\n",
      "epoch:23 step:18388 [D loss: 2.321625, acc.: 50.00%] [G loss: 5.440009]\n",
      "epoch:23 step:18389 [D loss: 0.886547, acc.: 58.59%] [G loss: 3.008740]\n",
      "epoch:23 step:18390 [D loss: 0.230615, acc.: 92.19%] [G loss: 2.917849]\n",
      "epoch:23 step:18391 [D loss: 0.106217, acc.: 97.66%] [G loss: 3.877714]\n",
      "epoch:23 step:18392 [D loss: 0.099562, acc.: 99.22%] [G loss: 3.776534]\n",
      "epoch:23 step:18393 [D loss: 0.298985, acc.: 86.72%] [G loss: 3.609293]\n",
      "epoch:23 step:18394 [D loss: 0.234405, acc.: 92.97%] [G loss: 1.614462]\n",
      "epoch:23 step:18395 [D loss: 0.210120, acc.: 92.19%] [G loss: 4.283395]\n",
      "epoch:23 step:18396 [D loss: 0.028486, acc.: 100.00%] [G loss: 4.497037]\n",
      "epoch:23 step:18397 [D loss: 0.371586, acc.: 84.38%] [G loss: 1.926241]\n",
      "epoch:23 step:18398 [D loss: 0.638312, acc.: 65.62%] [G loss: 5.566917]\n",
      "epoch:23 step:18399 [D loss: 1.138529, acc.: 55.47%] [G loss: 3.328535]\n",
      "epoch:23 step:18400 [D loss: 0.151381, acc.: 96.09%] [G loss: 3.530229]\n",
      "##############\n",
      "[0.94326671 1.12244829 2.10268879 0.97431572 0.9349306  1.08512945\n",
      " 2.11401044 2.10630306 2.1169481  0.82505398]\n",
      "##########\n",
      "epoch:23 step:18401 [D loss: 0.131702, acc.: 96.09%] [G loss: 2.949346]\n",
      "epoch:23 step:18402 [D loss: 0.063061, acc.: 99.22%] [G loss: 2.096279]\n",
      "epoch:23 step:18403 [D loss: 0.304681, acc.: 88.28%] [G loss: 3.011987]\n",
      "epoch:23 step:18404 [D loss: 0.108570, acc.: 98.44%] [G loss: 2.784674]\n",
      "epoch:23 step:18405 [D loss: 0.246858, acc.: 91.41%] [G loss: 2.057078]\n",
      "epoch:23 step:18406 [D loss: 0.103458, acc.: 96.88%] [G loss: 2.141951]\n",
      "epoch:23 step:18407 [D loss: 0.037008, acc.: 100.00%] [G loss: 1.551470]\n",
      "epoch:23 step:18408 [D loss: 0.130195, acc.: 98.44%] [G loss: 1.987140]\n",
      "epoch:23 step:18409 [D loss: 0.085587, acc.: 96.88%] [G loss: 1.536696]\n",
      "epoch:23 step:18410 [D loss: 0.106668, acc.: 96.88%] [G loss: 1.253377]\n",
      "epoch:23 step:18411 [D loss: 0.109300, acc.: 97.66%] [G loss: 1.817906]\n",
      "epoch:23 step:18412 [D loss: 0.282986, acc.: 88.28%] [G loss: 4.154230]\n",
      "epoch:23 step:18413 [D loss: 0.166758, acc.: 92.97%] [G loss: 3.007616]\n",
      "epoch:23 step:18414 [D loss: 0.160802, acc.: 94.53%] [G loss: 2.359076]\n",
      "epoch:23 step:18415 [D loss: 0.210169, acc.: 92.97%] [G loss: 3.370203]\n",
      "epoch:23 step:18416 [D loss: 0.191861, acc.: 91.41%] [G loss: 5.310038]\n",
      "epoch:23 step:18417 [D loss: 0.305004, acc.: 84.38%] [G loss: 3.410325]\n",
      "epoch:23 step:18418 [D loss: 0.100272, acc.: 96.88%] [G loss: 3.619245]\n",
      "epoch:23 step:18419 [D loss: 0.025126, acc.: 100.00%] [G loss: 4.542904]\n",
      "epoch:23 step:18420 [D loss: 0.034821, acc.: 100.00%] [G loss: 4.577486]\n",
      "epoch:23 step:18421 [D loss: 0.057319, acc.: 100.00%] [G loss: 3.423040]\n",
      "epoch:23 step:18422 [D loss: 0.156741, acc.: 96.09%] [G loss: 3.470014]\n",
      "epoch:23 step:18423 [D loss: 0.125094, acc.: 96.09%] [G loss: 4.214872]\n",
      "epoch:23 step:18424 [D loss: 0.069182, acc.: 98.44%] [G loss: 3.343452]\n",
      "epoch:23 step:18425 [D loss: 0.020854, acc.: 100.00%] [G loss: 2.336721]\n",
      "epoch:23 step:18426 [D loss: 0.021374, acc.: 100.00%] [G loss: 2.386998]\n",
      "epoch:23 step:18427 [D loss: 0.060435, acc.: 100.00%] [G loss: 2.133400]\n",
      "epoch:23 step:18428 [D loss: 0.034305, acc.: 100.00%] [G loss: 2.747389]\n",
      "epoch:23 step:18429 [D loss: 0.106935, acc.: 98.44%] [G loss: 2.129146]\n",
      "epoch:23 step:18430 [D loss: 0.065573, acc.: 98.44%] [G loss: 1.032714]\n",
      "epoch:23 step:18431 [D loss: 0.115484, acc.: 96.88%] [G loss: 0.799466]\n",
      "epoch:23 step:18432 [D loss: 0.290786, acc.: 86.72%] [G loss: 4.633460]\n",
      "epoch:23 step:18433 [D loss: 0.579813, acc.: 74.22%] [G loss: 1.322128]\n",
      "epoch:23 step:18434 [D loss: 0.410261, acc.: 82.81%] [G loss: 5.683021]\n",
      "epoch:23 step:18435 [D loss: 0.029813, acc.: 100.00%] [G loss: 7.266599]\n",
      "epoch:23 step:18436 [D loss: 0.338451, acc.: 82.81%] [G loss: 4.734636]\n",
      "epoch:23 step:18437 [D loss: 0.012747, acc.: 100.00%] [G loss: 2.855282]\n",
      "epoch:23 step:18438 [D loss: 0.132292, acc.: 95.31%] [G loss: 4.365874]\n",
      "epoch:23 step:18439 [D loss: 0.005409, acc.: 100.00%] [G loss: 5.398273]\n",
      "epoch:23 step:18440 [D loss: 0.022834, acc.: 100.00%] [G loss: 4.992150]\n",
      "epoch:23 step:18441 [D loss: 0.016362, acc.: 100.00%] [G loss: 4.736086]\n",
      "epoch:23 step:18442 [D loss: 0.045255, acc.: 99.22%] [G loss: 3.924982]\n",
      "epoch:23 step:18443 [D loss: 0.033764, acc.: 99.22%] [G loss: 4.131986]\n",
      "epoch:23 step:18444 [D loss: 0.069132, acc.: 97.66%] [G loss: 4.703635]\n",
      "epoch:23 step:18445 [D loss: 0.076307, acc.: 97.66%] [G loss: 3.573303]\n",
      "epoch:23 step:18446 [D loss: 0.101366, acc.: 96.88%] [G loss: 3.309644]\n",
      "epoch:23 step:18447 [D loss: 0.035912, acc.: 100.00%] [G loss: 4.177390]\n",
      "epoch:23 step:18448 [D loss: 0.075921, acc.: 100.00%] [G loss: 3.982184]\n",
      "epoch:23 step:18449 [D loss: 0.032470, acc.: 100.00%] [G loss: 4.147166]\n",
      "epoch:23 step:18450 [D loss: 0.032672, acc.: 99.22%] [G loss: 3.816290]\n",
      "epoch:23 step:18451 [D loss: 0.036435, acc.: 99.22%] [G loss: 3.584700]\n",
      "epoch:23 step:18452 [D loss: 0.034217, acc.: 100.00%] [G loss: 4.643085]\n",
      "epoch:23 step:18453 [D loss: 0.056642, acc.: 99.22%] [G loss: 4.821731]\n",
      "epoch:23 step:18454 [D loss: 0.220810, acc.: 89.84%] [G loss: 3.314511]\n",
      "epoch:23 step:18455 [D loss: 0.429346, acc.: 72.66%] [G loss: 6.180673]\n",
      "epoch:23 step:18456 [D loss: 0.004788, acc.: 100.00%] [G loss: 7.153215]\n",
      "epoch:23 step:18457 [D loss: 0.289646, acc.: 82.81%] [G loss: 4.957617]\n",
      "epoch:23 step:18458 [D loss: 0.059141, acc.: 99.22%] [G loss: 5.541484]\n",
      "epoch:23 step:18459 [D loss: 0.004475, acc.: 100.00%] [G loss: 5.596451]\n",
      "epoch:23 step:18460 [D loss: 0.006852, acc.: 100.00%] [G loss: 5.153719]\n",
      "epoch:23 step:18461 [D loss: 0.010433, acc.: 100.00%] [G loss: 4.800792]\n",
      "epoch:23 step:18462 [D loss: 0.026803, acc.: 100.00%] [G loss: 3.966497]\n",
      "epoch:23 step:18463 [D loss: 0.010528, acc.: 100.00%] [G loss: 4.672378]\n",
      "epoch:23 step:18464 [D loss: 0.040468, acc.: 100.00%] [G loss: 4.237845]\n",
      "epoch:23 step:18465 [D loss: 0.011074, acc.: 100.00%] [G loss: 4.224773]\n",
      "epoch:23 step:18466 [D loss: 0.006445, acc.: 100.00%] [G loss: 4.692202]\n",
      "epoch:23 step:18467 [D loss: 0.008710, acc.: 100.00%] [G loss: 4.163777]\n",
      "epoch:23 step:18468 [D loss: 0.061505, acc.: 99.22%] [G loss: 3.780038]\n",
      "epoch:23 step:18469 [D loss: 0.010814, acc.: 100.00%] [G loss: 3.572724]\n",
      "epoch:23 step:18470 [D loss: 0.039031, acc.: 100.00%] [G loss: 2.957638]\n",
      "epoch:23 step:18471 [D loss: 0.107669, acc.: 98.44%] [G loss: 4.382969]\n",
      "epoch:23 step:18472 [D loss: 0.071293, acc.: 100.00%] [G loss: 4.360252]\n",
      "epoch:23 step:18473 [D loss: 0.048941, acc.: 97.66%] [G loss: 4.828182]\n",
      "epoch:23 step:18474 [D loss: 0.020750, acc.: 100.00%] [G loss: 5.165732]\n",
      "epoch:23 step:18475 [D loss: 0.015111, acc.: 100.00%] [G loss: 4.805525]\n",
      "epoch:23 step:18476 [D loss: 0.042333, acc.: 99.22%] [G loss: 4.984930]\n",
      "epoch:23 step:18477 [D loss: 0.008835, acc.: 100.00%] [G loss: 4.839715]\n",
      "epoch:23 step:18478 [D loss: 0.017746, acc.: 100.00%] [G loss: 5.243509]\n",
      "epoch:23 step:18479 [D loss: 0.038806, acc.: 99.22%] [G loss: 3.570973]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:23 step:18480 [D loss: 0.070442, acc.: 99.22%] [G loss: 4.977915]\n",
      "epoch:23 step:18481 [D loss: 0.091046, acc.: 96.88%] [G loss: 6.048192]\n",
      "epoch:23 step:18482 [D loss: 0.145480, acc.: 96.88%] [G loss: 5.619031]\n",
      "epoch:23 step:18483 [D loss: 0.020197, acc.: 99.22%] [G loss: 5.831189]\n",
      "epoch:23 step:18484 [D loss: 0.007927, acc.: 100.00%] [G loss: 6.092746]\n",
      "epoch:23 step:18485 [D loss: 0.012647, acc.: 100.00%] [G loss: 5.066459]\n",
      "epoch:23 step:18486 [D loss: 0.028358, acc.: 100.00%] [G loss: 4.962128]\n",
      "epoch:23 step:18487 [D loss: 0.051800, acc.: 99.22%] [G loss: 4.326625]\n",
      "epoch:23 step:18488 [D loss: 0.015144, acc.: 100.00%] [G loss: 5.143983]\n",
      "epoch:23 step:18489 [D loss: 0.125337, acc.: 96.09%] [G loss: 5.729512]\n",
      "epoch:23 step:18490 [D loss: 0.008622, acc.: 100.00%] [G loss: 5.469155]\n",
      "epoch:23 step:18491 [D loss: 0.006653, acc.: 100.00%] [G loss: 5.787489]\n",
      "epoch:23 step:18492 [D loss: 0.007404, acc.: 100.00%] [G loss: 5.180385]\n",
      "epoch:23 step:18493 [D loss: 0.005013, acc.: 100.00%] [G loss: 5.779006]\n",
      "epoch:23 step:18494 [D loss: 0.007199, acc.: 100.00%] [G loss: 5.259711]\n",
      "epoch:23 step:18495 [D loss: 0.065432, acc.: 98.44%] [G loss: 5.325876]\n",
      "epoch:23 step:18496 [D loss: 0.110988, acc.: 97.66%] [G loss: 3.059886]\n",
      "epoch:23 step:18497 [D loss: 0.021244, acc.: 99.22%] [G loss: 3.518721]\n",
      "epoch:23 step:18498 [D loss: 0.010281, acc.: 100.00%] [G loss: 4.158228]\n",
      "epoch:23 step:18499 [D loss: 0.153328, acc.: 96.09%] [G loss: 6.337381]\n",
      "epoch:23 step:18500 [D loss: 0.689236, acc.: 69.53%] [G loss: 7.784778]\n",
      "epoch:23 step:18501 [D loss: 0.113491, acc.: 93.75%] [G loss: 7.832596]\n",
      "epoch:23 step:18502 [D loss: 0.044973, acc.: 99.22%] [G loss: 5.747466]\n",
      "epoch:23 step:18503 [D loss: 0.021412, acc.: 100.00%] [G loss: 5.333338]\n",
      "epoch:23 step:18504 [D loss: 0.010301, acc.: 100.00%] [G loss: 3.933737]\n",
      "epoch:23 step:18505 [D loss: 0.025406, acc.: 100.00%] [G loss: 4.100365]\n",
      "epoch:23 step:18506 [D loss: 0.042254, acc.: 99.22%] [G loss: 2.769151]\n",
      "epoch:23 step:18507 [D loss: 0.012977, acc.: 100.00%] [G loss: 1.899379]\n",
      "epoch:23 step:18508 [D loss: 0.022763, acc.: 99.22%] [G loss: 1.087268]\n",
      "epoch:23 step:18509 [D loss: 0.040226, acc.: 100.00%] [G loss: 1.769449]\n",
      "epoch:23 step:18510 [D loss: 0.037010, acc.: 99.22%] [G loss: 1.556382]\n",
      "epoch:23 step:18511 [D loss: 0.024154, acc.: 100.00%] [G loss: 1.634315]\n",
      "epoch:23 step:18512 [D loss: 0.017935, acc.: 100.00%] [G loss: 1.436559]\n",
      "epoch:23 step:18513 [D loss: 0.146570, acc.: 97.66%] [G loss: 2.914693]\n",
      "epoch:23 step:18514 [D loss: 0.019985, acc.: 100.00%] [G loss: 3.978488]\n",
      "epoch:23 step:18515 [D loss: 0.106068, acc.: 96.88%] [G loss: 7.998694]\n",
      "epoch:23 step:18516 [D loss: 0.036964, acc.: 97.66%] [G loss: 7.195018]\n",
      "epoch:23 step:18517 [D loss: 0.075305, acc.: 95.31%] [G loss: 3.916169]\n",
      "epoch:23 step:18518 [D loss: 0.001962, acc.: 100.00%] [G loss: 2.881788]\n",
      "epoch:23 step:18519 [D loss: 0.374605, acc.: 75.78%] [G loss: 10.003972]\n",
      "epoch:23 step:18520 [D loss: 1.353892, acc.: 53.91%] [G loss: 3.459079]\n",
      "epoch:23 step:18521 [D loss: 1.079439, acc.: 68.75%] [G loss: 8.561111]\n",
      "epoch:23 step:18522 [D loss: 2.252118, acc.: 50.78%] [G loss: 6.107562]\n",
      "epoch:23 step:18523 [D loss: 0.673132, acc.: 71.88%] [G loss: 2.491800]\n",
      "epoch:23 step:18524 [D loss: 0.078988, acc.: 97.66%] [G loss: 2.162616]\n",
      "epoch:23 step:18525 [D loss: 0.041907, acc.: 100.00%] [G loss: 2.577373]\n",
      "epoch:23 step:18526 [D loss: 0.335108, acc.: 82.81%] [G loss: 4.679346]\n",
      "epoch:23 step:18527 [D loss: 0.010152, acc.: 100.00%] [G loss: 5.892785]\n",
      "epoch:23 step:18528 [D loss: 0.435613, acc.: 78.12%] [G loss: 3.679287]\n",
      "epoch:23 step:18529 [D loss: 0.294764, acc.: 85.94%] [G loss: 5.345247]\n",
      "epoch:23 step:18530 [D loss: 0.009675, acc.: 100.00%] [G loss: 5.279160]\n",
      "epoch:23 step:18531 [D loss: 0.081798, acc.: 96.88%] [G loss: 4.469522]\n",
      "epoch:23 step:18532 [D loss: 0.016007, acc.: 100.00%] [G loss: 4.271128]\n",
      "epoch:23 step:18533 [D loss: 0.015678, acc.: 100.00%] [G loss: 3.618003]\n",
      "epoch:23 step:18534 [D loss: 0.033713, acc.: 100.00%] [G loss: 2.961546]\n",
      "epoch:23 step:18535 [D loss: 0.046609, acc.: 99.22%] [G loss: 4.315193]\n",
      "epoch:23 step:18536 [D loss: 0.014318, acc.: 100.00%] [G loss: 5.453405]\n",
      "epoch:23 step:18537 [D loss: 0.034563, acc.: 100.00%] [G loss: 3.952689]\n",
      "epoch:23 step:18538 [D loss: 0.050127, acc.: 99.22%] [G loss: 2.495602]\n",
      "epoch:23 step:18539 [D loss: 0.084638, acc.: 97.66%] [G loss: 3.387957]\n",
      "epoch:23 step:18540 [D loss: 0.155121, acc.: 93.75%] [G loss: 3.277250]\n",
      "epoch:23 step:18541 [D loss: 0.069011, acc.: 99.22%] [G loss: 2.486378]\n",
      "epoch:23 step:18542 [D loss: 0.019409, acc.: 100.00%] [G loss: 4.233151]\n",
      "epoch:23 step:18543 [D loss: 0.030590, acc.: 100.00%] [G loss: 3.921006]\n",
      "epoch:23 step:18544 [D loss: 0.018102, acc.: 100.00%] [G loss: 3.833152]\n",
      "epoch:23 step:18545 [D loss: 0.016221, acc.: 100.00%] [G loss: 3.629938]\n",
      "epoch:23 step:18546 [D loss: 0.026297, acc.: 100.00%] [G loss: 3.283297]\n",
      "epoch:23 step:18547 [D loss: 0.045689, acc.: 100.00%] [G loss: 3.041970]\n",
      "epoch:23 step:18548 [D loss: 0.012011, acc.: 99.22%] [G loss: 3.384458]\n",
      "epoch:23 step:18549 [D loss: 0.030679, acc.: 100.00%] [G loss: 1.988816]\n",
      "epoch:23 step:18550 [D loss: 0.047584, acc.: 99.22%] [G loss: 1.802677]\n",
      "epoch:23 step:18551 [D loss: 0.257233, acc.: 89.06%] [G loss: 5.601878]\n",
      "epoch:23 step:18552 [D loss: 0.333759, acc.: 84.38%] [G loss: 1.317550]\n",
      "epoch:23 step:18553 [D loss: 0.422394, acc.: 78.91%] [G loss: 6.643661]\n",
      "epoch:23 step:18554 [D loss: 0.280476, acc.: 85.94%] [G loss: 6.472728]\n",
      "epoch:23 step:18555 [D loss: 0.028714, acc.: 98.44%] [G loss: 6.272328]\n",
      "epoch:23 step:18556 [D loss: 0.003133, acc.: 100.00%] [G loss: 6.223587]\n",
      "epoch:23 step:18557 [D loss: 0.012599, acc.: 100.00%] [G loss: 5.076963]\n",
      "epoch:23 step:18558 [D loss: 0.005009, acc.: 100.00%] [G loss: 4.739293]\n",
      "epoch:23 step:18559 [D loss: 0.005559, acc.: 100.00%] [G loss: 4.875193]\n",
      "epoch:23 step:18560 [D loss: 0.006282, acc.: 100.00%] [G loss: 4.274392]\n",
      "epoch:23 step:18561 [D loss: 0.010591, acc.: 100.00%] [G loss: 4.273431]\n",
      "epoch:23 step:18562 [D loss: 0.053209, acc.: 98.44%] [G loss: 4.635537]\n",
      "epoch:23 step:18563 [D loss: 0.011714, acc.: 100.00%] [G loss: 5.399084]\n",
      "epoch:23 step:18564 [D loss: 0.027186, acc.: 100.00%] [G loss: 4.274392]\n",
      "epoch:23 step:18565 [D loss: 0.010076, acc.: 100.00%] [G loss: 4.089093]\n",
      "epoch:23 step:18566 [D loss: 0.088868, acc.: 97.66%] [G loss: 5.087810]\n",
      "epoch:23 step:18567 [D loss: 0.425293, acc.: 79.69%] [G loss: 3.896519]\n",
      "epoch:23 step:18568 [D loss: 0.020899, acc.: 100.00%] [G loss: 5.171281]\n",
      "epoch:23 step:18569 [D loss: 0.015102, acc.: 100.00%] [G loss: 4.942575]\n",
      "epoch:23 step:18570 [D loss: 0.027274, acc.: 100.00%] [G loss: 3.894391]\n",
      "epoch:23 step:18571 [D loss: 0.004131, acc.: 100.00%] [G loss: 3.496781]\n",
      "epoch:23 step:18572 [D loss: 0.029399, acc.: 99.22%] [G loss: 3.887401]\n",
      "epoch:23 step:18573 [D loss: 0.051501, acc.: 100.00%] [G loss: 4.325900]\n",
      "epoch:23 step:18574 [D loss: 0.008167, acc.: 100.00%] [G loss: 4.452418]\n",
      "epoch:23 step:18575 [D loss: 0.035615, acc.: 98.44%] [G loss: 4.350368]\n",
      "epoch:23 step:18576 [D loss: 0.539558, acc.: 74.22%] [G loss: 4.997821]\n",
      "epoch:23 step:18577 [D loss: 0.003513, acc.: 100.00%] [G loss: 6.026282]\n",
      "epoch:23 step:18578 [D loss: 0.020994, acc.: 100.00%] [G loss: 5.531330]\n",
      "epoch:23 step:18579 [D loss: 0.084324, acc.: 97.66%] [G loss: 4.957181]\n",
      "epoch:23 step:18580 [D loss: 0.055855, acc.: 98.44%] [G loss: 5.151682]\n",
      "epoch:23 step:18581 [D loss: 0.009876, acc.: 100.00%] [G loss: 5.354627]\n",
      "epoch:23 step:18582 [D loss: 0.021427, acc.: 100.00%] [G loss: 4.047893]\n",
      "epoch:23 step:18583 [D loss: 0.012513, acc.: 100.00%] [G loss: 4.443925]\n",
      "epoch:23 step:18584 [D loss: 0.025759, acc.: 100.00%] [G loss: 4.248007]\n",
      "epoch:23 step:18585 [D loss: 0.008569, acc.: 100.00%] [G loss: 4.607803]\n",
      "epoch:23 step:18586 [D loss: 0.296934, acc.: 86.72%] [G loss: 5.484142]\n",
      "epoch:23 step:18587 [D loss: 0.006860, acc.: 100.00%] [G loss: 6.261096]\n",
      "epoch:23 step:18588 [D loss: 0.471472, acc.: 75.78%] [G loss: 3.561831]\n",
      "epoch:23 step:18589 [D loss: 0.013636, acc.: 100.00%] [G loss: 3.709207]\n",
      "epoch:23 step:18590 [D loss: 0.005960, acc.: 100.00%] [G loss: 2.700887]\n",
      "epoch:23 step:18591 [D loss: 0.166601, acc.: 91.41%] [G loss: 7.162343]\n",
      "epoch:23 step:18592 [D loss: 0.021226, acc.: 100.00%] [G loss: 7.804532]\n",
      "epoch:23 step:18593 [D loss: 0.205654, acc.: 89.84%] [G loss: 3.886571]\n",
      "epoch:23 step:18594 [D loss: 0.153642, acc.: 92.97%] [G loss: 6.971355]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:23 step:18595 [D loss: 0.000568, acc.: 100.00%] [G loss: 7.387513]\n",
      "epoch:23 step:18596 [D loss: 0.061671, acc.: 99.22%] [G loss: 6.806911]\n",
      "epoch:23 step:18597 [D loss: 0.027932, acc.: 99.22%] [G loss: 5.943429]\n",
      "epoch:23 step:18598 [D loss: 0.008119, acc.: 100.00%] [G loss: 4.444574]\n",
      "epoch:23 step:18599 [D loss: 0.031253, acc.: 100.00%] [G loss: 3.856386]\n",
      "epoch:23 step:18600 [D loss: 0.031941, acc.: 100.00%] [G loss: 5.118906]\n",
      "##############\n",
      "[0.93038043 0.87549917 1.02836957 0.93588383 2.11711653 1.1178112\n",
      " 0.98011602 2.11412713 1.11319673 1.10513076]\n",
      "##########\n",
      "epoch:23 step:18601 [D loss: 0.005250, acc.: 100.00%] [G loss: 4.624306]\n",
      "epoch:23 step:18602 [D loss: 0.017435, acc.: 100.00%] [G loss: 3.996543]\n",
      "epoch:23 step:18603 [D loss: 0.010720, acc.: 100.00%] [G loss: 4.583613]\n",
      "epoch:23 step:18604 [D loss: 0.009255, acc.: 100.00%] [G loss: 4.288399]\n",
      "epoch:23 step:18605 [D loss: 0.099224, acc.: 97.66%] [G loss: 4.433573]\n",
      "epoch:23 step:18606 [D loss: 0.010551, acc.: 100.00%] [G loss: 4.471936]\n",
      "epoch:23 step:18607 [D loss: 0.016476, acc.: 100.00%] [G loss: 4.854882]\n",
      "epoch:23 step:18608 [D loss: 0.024341, acc.: 100.00%] [G loss: 3.647687]\n",
      "epoch:23 step:18609 [D loss: 0.383619, acc.: 81.25%] [G loss: 7.758349]\n",
      "epoch:23 step:18610 [D loss: 0.577843, acc.: 76.56%] [G loss: 3.146829]\n",
      "epoch:23 step:18611 [D loss: 0.248412, acc.: 89.84%] [G loss: 6.501440]\n",
      "epoch:23 step:18612 [D loss: 0.063508, acc.: 96.88%] [G loss: 6.569013]\n",
      "epoch:23 step:18613 [D loss: 0.077401, acc.: 97.66%] [G loss: 5.497572]\n",
      "epoch:23 step:18614 [D loss: 0.007788, acc.: 100.00%] [G loss: 5.266212]\n",
      "epoch:23 step:18615 [D loss: 0.034941, acc.: 100.00%] [G loss: 4.783853]\n",
      "epoch:23 step:18616 [D loss: 0.042289, acc.: 99.22%] [G loss: 5.267462]\n",
      "epoch:23 step:18617 [D loss: 0.053013, acc.: 99.22%] [G loss: 4.459055]\n",
      "epoch:23 step:18618 [D loss: 0.048074, acc.: 98.44%] [G loss: 5.801426]\n",
      "epoch:23 step:18619 [D loss: 0.012923, acc.: 100.00%] [G loss: 4.834563]\n",
      "epoch:23 step:18620 [D loss: 0.023769, acc.: 99.22%] [G loss: 5.066567]\n",
      "epoch:23 step:18621 [D loss: 0.150324, acc.: 92.97%] [G loss: 3.992585]\n",
      "epoch:23 step:18622 [D loss: 0.024487, acc.: 100.00%] [G loss: 4.504493]\n",
      "epoch:23 step:18623 [D loss: 0.035680, acc.: 100.00%] [G loss: 4.552199]\n",
      "epoch:23 step:18624 [D loss: 0.093043, acc.: 96.09%] [G loss: 3.260715]\n",
      "epoch:23 step:18625 [D loss: 0.036254, acc.: 99.22%] [G loss: 3.166547]\n",
      "epoch:23 step:18626 [D loss: 0.036019, acc.: 100.00%] [G loss: 5.532019]\n",
      "epoch:23 step:18627 [D loss: 0.024643, acc.: 100.00%] [G loss: 5.502627]\n",
      "epoch:23 step:18628 [D loss: 0.175332, acc.: 95.31%] [G loss: 4.469833]\n",
      "epoch:23 step:18629 [D loss: 0.043052, acc.: 99.22%] [G loss: 5.652773]\n",
      "epoch:23 step:18630 [D loss: 0.006859, acc.: 100.00%] [G loss: 6.343995]\n",
      "epoch:23 step:18631 [D loss: 0.082384, acc.: 97.66%] [G loss: 2.662373]\n",
      "epoch:23 step:18632 [D loss: 0.095314, acc.: 96.09%] [G loss: 6.587113]\n",
      "epoch:23 step:18633 [D loss: 0.002060, acc.: 100.00%] [G loss: 7.258085]\n",
      "epoch:23 step:18634 [D loss: 0.014820, acc.: 100.00%] [G loss: 6.194862]\n",
      "epoch:23 step:18635 [D loss: 0.006661, acc.: 100.00%] [G loss: 6.269183]\n",
      "epoch:23 step:18636 [D loss: 0.019940, acc.: 100.00%] [G loss: 6.187832]\n",
      "epoch:23 step:18637 [D loss: 0.001149, acc.: 100.00%] [G loss: 4.752607]\n",
      "epoch:23 step:18638 [D loss: 0.006574, acc.: 100.00%] [G loss: 5.118037]\n",
      "epoch:23 step:18639 [D loss: 0.027203, acc.: 100.00%] [G loss: 5.008959]\n",
      "epoch:23 step:18640 [D loss: 0.005214, acc.: 100.00%] [G loss: 5.006683]\n",
      "epoch:23 step:18641 [D loss: 0.018049, acc.: 100.00%] [G loss: 4.735999]\n",
      "epoch:23 step:18642 [D loss: 0.027271, acc.: 98.44%] [G loss: 5.070866]\n",
      "epoch:23 step:18643 [D loss: 0.009616, acc.: 100.00%] [G loss: 5.660940]\n",
      "epoch:23 step:18644 [D loss: 0.016778, acc.: 100.00%] [G loss: 5.092876]\n",
      "epoch:23 step:18645 [D loss: 0.024273, acc.: 100.00%] [G loss: 4.650564]\n",
      "epoch:23 step:18646 [D loss: 0.023666, acc.: 100.00%] [G loss: 3.991703]\n",
      "epoch:23 step:18647 [D loss: 0.096709, acc.: 96.88%] [G loss: 5.249543]\n",
      "epoch:23 step:18648 [D loss: 0.969747, acc.: 53.12%] [G loss: 7.146056]\n",
      "epoch:23 step:18649 [D loss: 0.083177, acc.: 96.88%] [G loss: 7.901912]\n",
      "epoch:23 step:18650 [D loss: 0.179022, acc.: 91.41%] [G loss: 5.068916]\n",
      "epoch:23 step:18651 [D loss: 0.036255, acc.: 100.00%] [G loss: 4.352589]\n",
      "epoch:23 step:18652 [D loss: 0.027182, acc.: 100.00%] [G loss: 2.275178]\n",
      "epoch:23 step:18653 [D loss: 0.179534, acc.: 95.31%] [G loss: 3.052202]\n",
      "epoch:23 step:18654 [D loss: 0.081322, acc.: 97.66%] [G loss: 5.876170]\n",
      "epoch:23 step:18655 [D loss: 0.582063, acc.: 70.31%] [G loss: 4.230988]\n",
      "epoch:23 step:18656 [D loss: 0.016354, acc.: 99.22%] [G loss: 6.474629]\n",
      "epoch:23 step:18657 [D loss: 0.437407, acc.: 79.69%] [G loss: 6.840353]\n",
      "epoch:23 step:18658 [D loss: 0.010605, acc.: 99.22%] [G loss: 7.453118]\n",
      "epoch:23 step:18659 [D loss: 0.249519, acc.: 92.19%] [G loss: 1.989120]\n",
      "epoch:23 step:18660 [D loss: 0.413770, acc.: 79.69%] [G loss: 8.708278]\n",
      "epoch:23 step:18661 [D loss: 0.845171, acc.: 64.06%] [G loss: 4.214665]\n",
      "epoch:23 step:18662 [D loss: 0.741922, acc.: 73.44%] [G loss: 8.696320]\n",
      "epoch:23 step:18663 [D loss: 0.328216, acc.: 82.81%] [G loss: 6.388556]\n",
      "epoch:23 step:18664 [D loss: 0.023419, acc.: 99.22%] [G loss: 5.032924]\n",
      "epoch:23 step:18665 [D loss: 0.027829, acc.: 99.22%] [G loss: 5.631891]\n",
      "epoch:23 step:18666 [D loss: 0.014964, acc.: 100.00%] [G loss: 5.370063]\n",
      "epoch:23 step:18667 [D loss: 0.229026, acc.: 89.84%] [G loss: 2.713340]\n",
      "epoch:23 step:18668 [D loss: 0.177547, acc.: 92.97%] [G loss: 4.809500]\n",
      "epoch:23 step:18669 [D loss: 0.002867, acc.: 100.00%] [G loss: 6.527177]\n",
      "epoch:23 step:18670 [D loss: 0.180079, acc.: 91.41%] [G loss: 2.794013]\n",
      "epoch:23 step:18671 [D loss: 0.160768, acc.: 92.19%] [G loss: 6.218469]\n",
      "epoch:23 step:18672 [D loss: 0.008237, acc.: 100.00%] [G loss: 6.428736]\n",
      "epoch:23 step:18673 [D loss: 0.033208, acc.: 99.22%] [G loss: 5.178401]\n",
      "epoch:23 step:18674 [D loss: 0.076880, acc.: 97.66%] [G loss: 2.207777]\n",
      "epoch:23 step:18675 [D loss: 0.248066, acc.: 89.06%] [G loss: 7.351418]\n",
      "epoch:23 step:18676 [D loss: 0.677621, acc.: 70.31%] [G loss: 4.369313]\n",
      "epoch:23 step:18677 [D loss: 0.078848, acc.: 97.66%] [G loss: 3.649756]\n",
      "epoch:23 step:18678 [D loss: 0.024759, acc.: 100.00%] [G loss: 3.574069]\n",
      "epoch:23 step:18679 [D loss: 0.050789, acc.: 98.44%] [G loss: 4.700510]\n",
      "epoch:23 step:18680 [D loss: 0.004136, acc.: 100.00%] [G loss: 3.672857]\n",
      "epoch:23 step:18681 [D loss: 0.095092, acc.: 97.66%] [G loss: 4.170288]\n",
      "epoch:23 step:18682 [D loss: 0.018263, acc.: 100.00%] [G loss: 5.089135]\n",
      "epoch:23 step:18683 [D loss: 0.220976, acc.: 92.97%] [G loss: 3.646833]\n",
      "epoch:23 step:18684 [D loss: 0.136259, acc.: 95.31%] [G loss: 6.161422]\n",
      "epoch:23 step:18685 [D loss: 0.002898, acc.: 100.00%] [G loss: 7.494418]\n",
      "epoch:23 step:18686 [D loss: 0.059303, acc.: 98.44%] [G loss: 6.522323]\n",
      "epoch:23 step:18687 [D loss: 0.006521, acc.: 100.00%] [G loss: 5.410581]\n",
      "epoch:23 step:18688 [D loss: 0.358643, acc.: 82.03%] [G loss: 5.220707]\n",
      "epoch:23 step:18689 [D loss: 0.015724, acc.: 100.00%] [G loss: 3.874815]\n",
      "epoch:23 step:18690 [D loss: 0.025382, acc.: 100.00%] [G loss: 5.249929]\n",
      "epoch:23 step:18691 [D loss: 0.071774, acc.: 98.44%] [G loss: 6.490111]\n",
      "epoch:23 step:18692 [D loss: 0.009621, acc.: 100.00%] [G loss: 5.409010]\n",
      "epoch:23 step:18693 [D loss: 0.006379, acc.: 100.00%] [G loss: 4.360403]\n",
      "epoch:23 step:18694 [D loss: 0.027065, acc.: 99.22%] [G loss: 4.356193]\n",
      "epoch:23 step:18695 [D loss: 0.120397, acc.: 96.09%] [G loss: 5.642345]\n",
      "epoch:23 step:18696 [D loss: 0.038972, acc.: 99.22%] [G loss: 5.626618]\n",
      "epoch:23 step:18697 [D loss: 0.013073, acc.: 100.00%] [G loss: 4.921700]\n",
      "epoch:23 step:18698 [D loss: 0.144096, acc.: 95.31%] [G loss: 1.684131]\n",
      "epoch:23 step:18699 [D loss: 0.457209, acc.: 81.25%] [G loss: 7.516115]\n",
      "epoch:23 step:18700 [D loss: 0.220489, acc.: 91.41%] [G loss: 7.623089]\n",
      "epoch:23 step:18701 [D loss: 0.023695, acc.: 100.00%] [G loss: 6.436548]\n",
      "epoch:23 step:18702 [D loss: 0.106896, acc.: 96.88%] [G loss: 2.949223]\n",
      "epoch:23 step:18703 [D loss: 0.053128, acc.: 99.22%] [G loss: 3.281591]\n",
      "epoch:23 step:18704 [D loss: 0.007482, acc.: 100.00%] [G loss: 3.697593]\n",
      "epoch:23 step:18705 [D loss: 0.077904, acc.: 96.88%] [G loss: 4.106499]\n",
      "epoch:23 step:18706 [D loss: 0.004903, acc.: 100.00%] [G loss: 5.965670]\n",
      "epoch:23 step:18707 [D loss: 0.045320, acc.: 98.44%] [G loss: 3.990139]\n",
      "epoch:23 step:18708 [D loss: 0.014759, acc.: 100.00%] [G loss: 4.425071]\n",
      "epoch:23 step:18709 [D loss: 0.022608, acc.: 99.22%] [G loss: 3.600261]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:23 step:18710 [D loss: 1.242045, acc.: 50.78%] [G loss: 9.685810]\n",
      "epoch:23 step:18711 [D loss: 3.142600, acc.: 50.00%] [G loss: 6.435661]\n",
      "epoch:23 step:18712 [D loss: 1.604310, acc.: 49.22%] [G loss: 2.864620]\n",
      "epoch:23 step:18713 [D loss: 0.674116, acc.: 69.53%] [G loss: 3.724585]\n",
      "epoch:23 step:18714 [D loss: 0.068111, acc.: 99.22%] [G loss: 4.302972]\n",
      "epoch:23 step:18715 [D loss: 0.218203, acc.: 88.28%] [G loss: 3.145250]\n",
      "epoch:23 step:18716 [D loss: 0.118922, acc.: 96.09%] [G loss: 3.216249]\n",
      "epoch:23 step:18717 [D loss: 0.051389, acc.: 100.00%] [G loss: 3.533388]\n",
      "epoch:23 step:18718 [D loss: 0.048018, acc.: 100.00%] [G loss: 3.253603]\n",
      "epoch:23 step:18719 [D loss: 0.081029, acc.: 98.44%] [G loss: 3.641770]\n",
      "epoch:23 step:18720 [D loss: 0.052160, acc.: 100.00%] [G loss: 3.405986]\n",
      "epoch:23 step:18721 [D loss: 0.039152, acc.: 100.00%] [G loss: 3.295483]\n",
      "epoch:23 step:18722 [D loss: 0.128396, acc.: 97.66%] [G loss: 3.601220]\n",
      "epoch:23 step:18723 [D loss: 0.102283, acc.: 97.66%] [G loss: 2.592791]\n",
      "epoch:23 step:18724 [D loss: 0.165887, acc.: 94.53%] [G loss: 4.360175]\n",
      "epoch:23 step:18725 [D loss: 0.194050, acc.: 91.41%] [G loss: 2.810721]\n",
      "epoch:23 step:18726 [D loss: 0.184810, acc.: 95.31%] [G loss: 3.374965]\n",
      "epoch:23 step:18727 [D loss: 0.043498, acc.: 99.22%] [G loss: 3.851283]\n",
      "epoch:23 step:18728 [D loss: 0.074832, acc.: 100.00%] [G loss: 2.976022]\n",
      "epoch:23 step:18729 [D loss: 0.221298, acc.: 92.97%] [G loss: 2.801395]\n",
      "epoch:23 step:18730 [D loss: 0.056781, acc.: 99.22%] [G loss: 3.467010]\n",
      "epoch:23 step:18731 [D loss: 0.190179, acc.: 94.53%] [G loss: 3.000959]\n",
      "epoch:23 step:18732 [D loss: 0.106395, acc.: 96.88%] [G loss: 2.397712]\n",
      "epoch:23 step:18733 [D loss: 0.055087, acc.: 100.00%] [G loss: 2.277376]\n",
      "epoch:23 step:18734 [D loss: 0.104585, acc.: 100.00%] [G loss: 3.150562]\n",
      "epoch:23 step:18735 [D loss: 0.070419, acc.: 99.22%] [G loss: 3.796512]\n",
      "epoch:23 step:18736 [D loss: 0.021424, acc.: 100.00%] [G loss: 4.435655]\n",
      "epoch:23 step:18737 [D loss: 0.055487, acc.: 100.00%] [G loss: 4.566831]\n",
      "epoch:23 step:18738 [D loss: 0.147818, acc.: 94.53%] [G loss: 3.307757]\n",
      "epoch:23 step:18739 [D loss: 0.051330, acc.: 99.22%] [G loss: 3.689570]\n",
      "epoch:23 step:18740 [D loss: 0.057128, acc.: 100.00%] [G loss: 2.824335]\n",
      "epoch:23 step:18741 [D loss: 0.055668, acc.: 99.22%] [G loss: 3.924869]\n",
      "epoch:23 step:18742 [D loss: 0.636625, acc.: 67.97%] [G loss: 5.241067]\n",
      "epoch:23 step:18743 [D loss: 0.043567, acc.: 100.00%] [G loss: 6.482346]\n",
      "epoch:23 step:18744 [D loss: 0.179940, acc.: 92.19%] [G loss: 4.305512]\n",
      "epoch:24 step:18745 [D loss: 0.074587, acc.: 96.88%] [G loss: 3.470896]\n",
      "epoch:24 step:18746 [D loss: 0.033873, acc.: 99.22%] [G loss: 3.738172]\n",
      "epoch:24 step:18747 [D loss: 0.051469, acc.: 99.22%] [G loss: 4.458736]\n",
      "epoch:24 step:18748 [D loss: 0.023734, acc.: 100.00%] [G loss: 4.211486]\n",
      "epoch:24 step:18749 [D loss: 0.030766, acc.: 100.00%] [G loss: 3.341915]\n",
      "epoch:24 step:18750 [D loss: 0.022089, acc.: 100.00%] [G loss: 3.808626]\n",
      "epoch:24 step:18751 [D loss: 0.040092, acc.: 100.00%] [G loss: 3.297592]\n",
      "epoch:24 step:18752 [D loss: 0.036161, acc.: 100.00%] [G loss: 3.551870]\n",
      "epoch:24 step:18753 [D loss: 0.223174, acc.: 92.97%] [G loss: 3.575543]\n",
      "epoch:24 step:18754 [D loss: 0.024926, acc.: 100.00%] [G loss: 4.064520]\n",
      "epoch:24 step:18755 [D loss: 0.009107, acc.: 100.00%] [G loss: 4.591594]\n",
      "epoch:24 step:18756 [D loss: 0.029703, acc.: 100.00%] [G loss: 4.269886]\n",
      "epoch:24 step:18757 [D loss: 0.014907, acc.: 99.22%] [G loss: 4.148704]\n",
      "epoch:24 step:18758 [D loss: 0.099304, acc.: 98.44%] [G loss: 4.302019]\n",
      "epoch:24 step:18759 [D loss: 0.019797, acc.: 100.00%] [G loss: 4.841887]\n",
      "epoch:24 step:18760 [D loss: 0.013314, acc.: 100.00%] [G loss: 4.854459]\n",
      "epoch:24 step:18761 [D loss: 0.027305, acc.: 100.00%] [G loss: 3.893611]\n",
      "epoch:24 step:18762 [D loss: 0.013581, acc.: 100.00%] [G loss: 3.645559]\n",
      "epoch:24 step:18763 [D loss: 0.012620, acc.: 100.00%] [G loss: 4.862737]\n",
      "epoch:24 step:18764 [D loss: 0.008911, acc.: 100.00%] [G loss: 4.338087]\n",
      "epoch:24 step:18765 [D loss: 0.009100, acc.: 100.00%] [G loss: 4.333221]\n",
      "epoch:24 step:18766 [D loss: 0.014143, acc.: 100.00%] [G loss: 3.704341]\n",
      "epoch:24 step:18767 [D loss: 0.220918, acc.: 92.19%] [G loss: 4.206786]\n",
      "epoch:24 step:18768 [D loss: 0.004859, acc.: 100.00%] [G loss: 5.374524]\n",
      "epoch:24 step:18769 [D loss: 0.071355, acc.: 96.88%] [G loss: 4.430215]\n",
      "epoch:24 step:18770 [D loss: 0.054274, acc.: 99.22%] [G loss: 4.759459]\n",
      "epoch:24 step:18771 [D loss: 0.003468, acc.: 100.00%] [G loss: 4.727533]\n",
      "epoch:24 step:18772 [D loss: 0.013031, acc.: 100.00%] [G loss: 5.068461]\n",
      "epoch:24 step:18773 [D loss: 0.011198, acc.: 100.00%] [G loss: 4.904809]\n",
      "epoch:24 step:18774 [D loss: 0.019677, acc.: 99.22%] [G loss: 3.946157]\n",
      "epoch:24 step:18775 [D loss: 0.023848, acc.: 99.22%] [G loss: 4.326539]\n",
      "epoch:24 step:18776 [D loss: 0.008125, acc.: 100.00%] [G loss: 4.502457]\n",
      "epoch:24 step:18777 [D loss: 0.054579, acc.: 99.22%] [G loss: 3.449550]\n",
      "epoch:24 step:18778 [D loss: 0.050197, acc.: 100.00%] [G loss: 4.328380]\n",
      "epoch:24 step:18779 [D loss: 0.009376, acc.: 100.00%] [G loss: 4.313823]\n",
      "epoch:24 step:18780 [D loss: 1.050164, acc.: 47.66%] [G loss: 6.965424]\n",
      "epoch:24 step:18781 [D loss: 0.551190, acc.: 74.22%] [G loss: 5.422383]\n",
      "epoch:24 step:18782 [D loss: 0.039117, acc.: 99.22%] [G loss: 3.742605]\n",
      "epoch:24 step:18783 [D loss: 0.032935, acc.: 99.22%] [G loss: 4.840547]\n",
      "epoch:24 step:18784 [D loss: 0.014797, acc.: 100.00%] [G loss: 4.535185]\n",
      "epoch:24 step:18785 [D loss: 0.017597, acc.: 100.00%] [G loss: 3.910204]\n",
      "epoch:24 step:18786 [D loss: 0.043715, acc.: 99.22%] [G loss: 4.351168]\n",
      "epoch:24 step:18787 [D loss: 0.012827, acc.: 100.00%] [G loss: 4.254270]\n",
      "epoch:24 step:18788 [D loss: 0.040850, acc.: 99.22%] [G loss: 3.387229]\n",
      "epoch:24 step:18789 [D loss: 0.281214, acc.: 88.28%] [G loss: 5.163584]\n",
      "epoch:24 step:18790 [D loss: 0.314760, acc.: 85.16%] [G loss: 3.155747]\n",
      "epoch:24 step:18791 [D loss: 0.137360, acc.: 95.31%] [G loss: 5.902050]\n",
      "epoch:24 step:18792 [D loss: 0.002914, acc.: 100.00%] [G loss: 7.090602]\n",
      "epoch:24 step:18793 [D loss: 0.290690, acc.: 86.72%] [G loss: 1.334223]\n",
      "epoch:24 step:18794 [D loss: 1.039822, acc.: 62.50%] [G loss: 8.442503]\n",
      "epoch:24 step:18795 [D loss: 1.603156, acc.: 54.69%] [G loss: 6.606269]\n",
      "epoch:24 step:18796 [D loss: 0.052932, acc.: 98.44%] [G loss: 5.430553]\n",
      "epoch:24 step:18797 [D loss: 0.028431, acc.: 99.22%] [G loss: 3.967587]\n",
      "epoch:24 step:18798 [D loss: 0.038660, acc.: 99.22%] [G loss: 4.061573]\n",
      "epoch:24 step:18799 [D loss: 0.055761, acc.: 98.44%] [G loss: 3.643038]\n",
      "epoch:24 step:18800 [D loss: 0.102739, acc.: 96.09%] [G loss: 3.544882]\n",
      "##############\n",
      "[0.81341339 0.86927003 1.0926768  0.8819405  2.10451084 1.12182598\n",
      " 2.12238262 2.10673401 2.10360777 1.0610016 ]\n",
      "##########\n",
      "epoch:24 step:18801 [D loss: 0.056822, acc.: 97.66%] [G loss: 3.021982]\n",
      "epoch:24 step:18802 [D loss: 0.447898, acc.: 81.25%] [G loss: 4.981717]\n",
      "epoch:24 step:18803 [D loss: 0.040437, acc.: 98.44%] [G loss: 6.570367]\n",
      "epoch:24 step:18804 [D loss: 0.117165, acc.: 96.09%] [G loss: 5.059686]\n",
      "epoch:24 step:18805 [D loss: 0.040964, acc.: 100.00%] [G loss: 4.661540]\n",
      "epoch:24 step:18806 [D loss: 0.032010, acc.: 100.00%] [G loss: 4.891590]\n",
      "epoch:24 step:18807 [D loss: 0.101307, acc.: 98.44%] [G loss: 3.994265]\n",
      "epoch:24 step:18808 [D loss: 0.036370, acc.: 100.00%] [G loss: 5.181869]\n",
      "epoch:24 step:18809 [D loss: 0.080544, acc.: 98.44%] [G loss: 3.690890]\n",
      "epoch:24 step:18810 [D loss: 0.016881, acc.: 100.00%] [G loss: 4.656190]\n",
      "epoch:24 step:18811 [D loss: 0.012143, acc.: 100.00%] [G loss: 4.547686]\n",
      "epoch:24 step:18812 [D loss: 0.027706, acc.: 100.00%] [G loss: 4.346185]\n",
      "epoch:24 step:18813 [D loss: 0.073805, acc.: 98.44%] [G loss: 4.208017]\n",
      "epoch:24 step:18814 [D loss: 0.068218, acc.: 100.00%] [G loss: 4.666921]\n",
      "epoch:24 step:18815 [D loss: 0.026929, acc.: 100.00%] [G loss: 4.853178]\n",
      "epoch:24 step:18816 [D loss: 0.051348, acc.: 99.22%] [G loss: 3.872115]\n",
      "epoch:24 step:18817 [D loss: 0.060176, acc.: 100.00%] [G loss: 4.315516]\n",
      "epoch:24 step:18818 [D loss: 0.016553, acc.: 100.00%] [G loss: 3.700435]\n",
      "epoch:24 step:18819 [D loss: 0.060609, acc.: 99.22%] [G loss: 2.493186]\n",
      "epoch:24 step:18820 [D loss: 0.080217, acc.: 97.66%] [G loss: 3.722593]\n",
      "epoch:24 step:18821 [D loss: 0.497895, acc.: 77.34%] [G loss: 7.611711]\n",
      "epoch:24 step:18822 [D loss: 0.255466, acc.: 85.94%] [G loss: 6.468933]\n",
      "epoch:24 step:18823 [D loss: 0.067765, acc.: 99.22%] [G loss: 5.500916]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:24 step:18824 [D loss: 0.021784, acc.: 100.00%] [G loss: 4.345883]\n",
      "epoch:24 step:18825 [D loss: 0.016031, acc.: 100.00%] [G loss: 4.790213]\n",
      "epoch:24 step:18826 [D loss: 0.017758, acc.: 100.00%] [G loss: 4.194884]\n",
      "epoch:24 step:18827 [D loss: 0.020271, acc.: 100.00%] [G loss: 5.354761]\n",
      "epoch:24 step:18828 [D loss: 0.014514, acc.: 100.00%] [G loss: 3.596178]\n",
      "epoch:24 step:18829 [D loss: 0.027743, acc.: 100.00%] [G loss: 4.466102]\n",
      "epoch:24 step:18830 [D loss: 0.013738, acc.: 100.00%] [G loss: 4.094271]\n",
      "epoch:24 step:18831 [D loss: 0.014609, acc.: 100.00%] [G loss: 4.363585]\n",
      "epoch:24 step:18832 [D loss: 0.032307, acc.: 100.00%] [G loss: 5.163194]\n",
      "epoch:24 step:18833 [D loss: 0.015770, acc.: 100.00%] [G loss: 4.934846]\n",
      "epoch:24 step:18834 [D loss: 0.652975, acc.: 66.41%] [G loss: 6.566584]\n",
      "epoch:24 step:18835 [D loss: 0.348817, acc.: 85.16%] [G loss: 5.668012]\n",
      "epoch:24 step:18836 [D loss: 0.007582, acc.: 100.00%] [G loss: 5.618628]\n",
      "epoch:24 step:18837 [D loss: 0.007671, acc.: 100.00%] [G loss: 5.544165]\n",
      "epoch:24 step:18838 [D loss: 0.005837, acc.: 100.00%] [G loss: 4.599538]\n",
      "epoch:24 step:18839 [D loss: 0.013442, acc.: 100.00%] [G loss: 5.202680]\n",
      "epoch:24 step:18840 [D loss: 0.006243, acc.: 100.00%] [G loss: 5.494600]\n",
      "epoch:24 step:18841 [D loss: 0.008421, acc.: 100.00%] [G loss: 3.765146]\n",
      "epoch:24 step:18842 [D loss: 0.011124, acc.: 100.00%] [G loss: 3.672761]\n",
      "epoch:24 step:18843 [D loss: 0.239428, acc.: 89.06%] [G loss: 6.571070]\n",
      "epoch:24 step:18844 [D loss: 0.010788, acc.: 100.00%] [G loss: 6.686668]\n",
      "epoch:24 step:18845 [D loss: 0.199726, acc.: 86.72%] [G loss: 4.009159]\n",
      "epoch:24 step:18846 [D loss: 0.093902, acc.: 95.31%] [G loss: 5.169403]\n",
      "epoch:24 step:18847 [D loss: 0.008146, acc.: 100.00%] [G loss: 5.540022]\n",
      "epoch:24 step:18848 [D loss: 0.007241, acc.: 100.00%] [G loss: 4.139911]\n",
      "epoch:24 step:18849 [D loss: 0.028897, acc.: 100.00%] [G loss: 4.017220]\n",
      "epoch:24 step:18850 [D loss: 0.015098, acc.: 100.00%] [G loss: 3.991970]\n",
      "epoch:24 step:18851 [D loss: 0.046539, acc.: 100.00%] [G loss: 5.315161]\n",
      "epoch:24 step:18852 [D loss: 0.008219, acc.: 100.00%] [G loss: 6.001704]\n",
      "epoch:24 step:18853 [D loss: 0.004339, acc.: 100.00%] [G loss: 4.889864]\n",
      "epoch:24 step:18854 [D loss: 0.093821, acc.: 98.44%] [G loss: 1.583848]\n",
      "epoch:24 step:18855 [D loss: 0.174670, acc.: 91.41%] [G loss: 6.526019]\n",
      "epoch:24 step:18856 [D loss: 0.014067, acc.: 100.00%] [G loss: 8.488184]\n",
      "epoch:24 step:18857 [D loss: 0.178958, acc.: 92.97%] [G loss: 4.560373]\n",
      "epoch:24 step:18858 [D loss: 0.201590, acc.: 89.84%] [G loss: 6.391466]\n",
      "epoch:24 step:18859 [D loss: 0.012959, acc.: 100.00%] [G loss: 7.485316]\n",
      "epoch:24 step:18860 [D loss: 1.582896, acc.: 46.09%] [G loss: 2.535474]\n",
      "epoch:24 step:18861 [D loss: 0.216869, acc.: 91.41%] [G loss: 4.945997]\n",
      "epoch:24 step:18862 [D loss: 0.002137, acc.: 100.00%] [G loss: 6.072578]\n",
      "epoch:24 step:18863 [D loss: 0.085725, acc.: 98.44%] [G loss: 4.691257]\n",
      "epoch:24 step:18864 [D loss: 0.013997, acc.: 100.00%] [G loss: 4.949667]\n",
      "epoch:24 step:18865 [D loss: 0.015216, acc.: 100.00%] [G loss: 5.027740]\n",
      "epoch:24 step:18866 [D loss: 0.017356, acc.: 100.00%] [G loss: 3.944495]\n",
      "epoch:24 step:18867 [D loss: 0.019312, acc.: 100.00%] [G loss: 3.947103]\n",
      "epoch:24 step:18868 [D loss: 0.017997, acc.: 100.00%] [G loss: 4.346542]\n",
      "epoch:24 step:18869 [D loss: 0.025070, acc.: 100.00%] [G loss: 3.178385]\n",
      "epoch:24 step:18870 [D loss: 0.018897, acc.: 100.00%] [G loss: 3.721121]\n",
      "epoch:24 step:18871 [D loss: 0.063084, acc.: 99.22%] [G loss: 4.529124]\n",
      "epoch:24 step:18872 [D loss: 0.015840, acc.: 100.00%] [G loss: 4.875998]\n",
      "epoch:24 step:18873 [D loss: 0.192167, acc.: 97.66%] [G loss: 3.145019]\n",
      "epoch:24 step:18874 [D loss: 0.009583, acc.: 100.00%] [G loss: 4.243825]\n",
      "epoch:24 step:18875 [D loss: 0.017656, acc.: 100.00%] [G loss: 4.719720]\n",
      "epoch:24 step:18876 [D loss: 0.098663, acc.: 96.88%] [G loss: 1.923467]\n",
      "epoch:24 step:18877 [D loss: 0.474071, acc.: 71.09%] [G loss: 7.600679]\n",
      "epoch:24 step:18878 [D loss: 1.653127, acc.: 53.12%] [G loss: 5.721899]\n",
      "epoch:24 step:18879 [D loss: 0.109495, acc.: 95.31%] [G loss: 4.554336]\n",
      "epoch:24 step:18880 [D loss: 0.022424, acc.: 100.00%] [G loss: 3.562999]\n",
      "epoch:24 step:18881 [D loss: 0.076836, acc.: 98.44%] [G loss: 4.011808]\n",
      "epoch:24 step:18882 [D loss: 0.004678, acc.: 100.00%] [G loss: 4.480207]\n",
      "epoch:24 step:18883 [D loss: 0.020916, acc.: 100.00%] [G loss: 5.026415]\n",
      "epoch:24 step:18884 [D loss: 0.021326, acc.: 100.00%] [G loss: 4.220854]\n",
      "epoch:24 step:18885 [D loss: 0.006005, acc.: 100.00%] [G loss: 4.365905]\n",
      "epoch:24 step:18886 [D loss: 0.018365, acc.: 100.00%] [G loss: 4.023494]\n",
      "epoch:24 step:18887 [D loss: 0.014055, acc.: 100.00%] [G loss: 3.486708]\n",
      "epoch:24 step:18888 [D loss: 0.023734, acc.: 100.00%] [G loss: 3.959304]\n",
      "epoch:24 step:18889 [D loss: 0.030474, acc.: 100.00%] [G loss: 3.305182]\n",
      "epoch:24 step:18890 [D loss: 0.010347, acc.: 100.00%] [G loss: 3.972476]\n",
      "epoch:24 step:18891 [D loss: 0.032375, acc.: 100.00%] [G loss: 3.631016]\n",
      "epoch:24 step:18892 [D loss: 0.019035, acc.: 100.00%] [G loss: 3.755443]\n",
      "epoch:24 step:18893 [D loss: 0.015058, acc.: 100.00%] [G loss: 4.307722]\n",
      "epoch:24 step:18894 [D loss: 0.019055, acc.: 100.00%] [G loss: 4.034977]\n",
      "epoch:24 step:18895 [D loss: 0.025148, acc.: 100.00%] [G loss: 3.828545]\n",
      "epoch:24 step:18896 [D loss: 0.011039, acc.: 100.00%] [G loss: 3.569746]\n",
      "epoch:24 step:18897 [D loss: 0.346307, acc.: 86.72%] [G loss: 2.416125]\n",
      "epoch:24 step:18898 [D loss: 0.022181, acc.: 100.00%] [G loss: 3.621150]\n",
      "epoch:24 step:18899 [D loss: 0.007952, acc.: 100.00%] [G loss: 3.945414]\n",
      "epoch:24 step:18900 [D loss: 0.031513, acc.: 100.00%] [G loss: 3.425789]\n",
      "epoch:24 step:18901 [D loss: 0.019800, acc.: 100.00%] [G loss: 3.550120]\n",
      "epoch:24 step:18902 [D loss: 0.072316, acc.: 100.00%] [G loss: 4.355879]\n",
      "epoch:24 step:18903 [D loss: 0.009007, acc.: 100.00%] [G loss: 4.818068]\n",
      "epoch:24 step:18904 [D loss: 0.036612, acc.: 98.44%] [G loss: 4.361249]\n",
      "epoch:24 step:18905 [D loss: 0.051871, acc.: 98.44%] [G loss: 3.408338]\n",
      "epoch:24 step:18906 [D loss: 0.020991, acc.: 100.00%] [G loss: 3.683908]\n",
      "epoch:24 step:18907 [D loss: 0.015799, acc.: 100.00%] [G loss: 4.142628]\n",
      "epoch:24 step:18908 [D loss: 0.011167, acc.: 100.00%] [G loss: 3.631133]\n",
      "epoch:24 step:18909 [D loss: 0.006012, acc.: 100.00%] [G loss: 3.622845]\n",
      "epoch:24 step:18910 [D loss: 0.030523, acc.: 100.00%] [G loss: 3.434969]\n",
      "epoch:24 step:18911 [D loss: 0.018205, acc.: 100.00%] [G loss: 3.852108]\n",
      "epoch:24 step:18912 [D loss: 0.013441, acc.: 100.00%] [G loss: 4.395539]\n",
      "epoch:24 step:18913 [D loss: 0.003148, acc.: 100.00%] [G loss: 4.174746]\n",
      "epoch:24 step:18914 [D loss: 0.020396, acc.: 100.00%] [G loss: 4.232581]\n",
      "epoch:24 step:18915 [D loss: 0.022768, acc.: 100.00%] [G loss: 4.667039]\n",
      "epoch:24 step:18916 [D loss: 0.005266, acc.: 100.00%] [G loss: 4.320896]\n",
      "epoch:24 step:18917 [D loss: 0.007973, acc.: 100.00%] [G loss: 4.425596]\n",
      "epoch:24 step:18918 [D loss: 0.015190, acc.: 100.00%] [G loss: 3.863036]\n",
      "epoch:24 step:18919 [D loss: 0.024362, acc.: 100.00%] [G loss: 3.837947]\n",
      "epoch:24 step:18920 [D loss: 0.241744, acc.: 91.41%] [G loss: 2.444757]\n",
      "epoch:24 step:18921 [D loss: 0.055434, acc.: 99.22%] [G loss: 3.407942]\n",
      "epoch:24 step:18922 [D loss: 0.011279, acc.: 100.00%] [G loss: 4.363480]\n",
      "epoch:24 step:18923 [D loss: 0.070682, acc.: 99.22%] [G loss: 2.464392]\n",
      "epoch:24 step:18924 [D loss: 0.067891, acc.: 98.44%] [G loss: 3.443517]\n",
      "epoch:24 step:18925 [D loss: 0.006858, acc.: 100.00%] [G loss: 4.457708]\n",
      "epoch:24 step:18926 [D loss: 0.044976, acc.: 99.22%] [G loss: 2.157807]\n",
      "epoch:24 step:18927 [D loss: 0.087704, acc.: 96.88%] [G loss: 4.194865]\n",
      "epoch:24 step:18928 [D loss: 0.008669, acc.: 100.00%] [G loss: 4.074950]\n",
      "epoch:24 step:18929 [D loss: 0.196727, acc.: 91.41%] [G loss: 4.693746]\n",
      "epoch:24 step:18930 [D loss: 0.023464, acc.: 100.00%] [G loss: 5.044015]\n",
      "epoch:24 step:18931 [D loss: 0.132972, acc.: 97.66%] [G loss: 3.762620]\n",
      "epoch:24 step:18932 [D loss: 0.174577, acc.: 91.41%] [G loss: 7.269878]\n",
      "epoch:24 step:18933 [D loss: 0.147637, acc.: 93.75%] [G loss: 6.210736]\n",
      "epoch:24 step:18934 [D loss: 0.029435, acc.: 100.00%] [G loss: 4.318706]\n",
      "epoch:24 step:18935 [D loss: 0.120397, acc.: 96.88%] [G loss: 6.024162]\n",
      "epoch:24 step:18936 [D loss: 0.028404, acc.: 99.22%] [G loss: 6.610362]\n",
      "epoch:24 step:18937 [D loss: 0.328152, acc.: 82.03%] [G loss: 0.267538]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:24 step:18938 [D loss: 0.895774, acc.: 64.84%] [G loss: 6.704251]\n",
      "epoch:24 step:18939 [D loss: 0.177930, acc.: 92.19%] [G loss: 6.933592]\n",
      "epoch:24 step:18940 [D loss: 0.004173, acc.: 100.00%] [G loss: 6.678148]\n",
      "epoch:24 step:18941 [D loss: 0.622081, acc.: 71.09%] [G loss: 0.337733]\n",
      "epoch:24 step:18942 [D loss: 0.413331, acc.: 81.25%] [G loss: 3.424986]\n",
      "epoch:24 step:18943 [D loss: 0.019385, acc.: 99.22%] [G loss: 5.881824]\n",
      "epoch:24 step:18944 [D loss: 0.161084, acc.: 93.75%] [G loss: 4.296626]\n",
      "epoch:24 step:18945 [D loss: 0.014189, acc.: 100.00%] [G loss: 2.832092]\n",
      "epoch:24 step:18946 [D loss: 0.008643, acc.: 100.00%] [G loss: 1.760053]\n",
      "epoch:24 step:18947 [D loss: 0.016615, acc.: 100.00%] [G loss: 1.544948]\n",
      "epoch:24 step:18948 [D loss: 0.012960, acc.: 100.00%] [G loss: 2.452947]\n",
      "epoch:24 step:18949 [D loss: 0.222194, acc.: 90.62%] [G loss: 5.870275]\n",
      "epoch:24 step:18950 [D loss: 0.147102, acc.: 95.31%] [G loss: 4.294776]\n",
      "epoch:24 step:18951 [D loss: 0.002118, acc.: 100.00%] [G loss: 4.252801]\n",
      "epoch:24 step:18952 [D loss: 0.118538, acc.: 98.44%] [G loss: 3.649909]\n",
      "epoch:24 step:18953 [D loss: 0.106984, acc.: 95.31%] [G loss: 2.129463]\n",
      "epoch:24 step:18954 [D loss: 0.347768, acc.: 81.25%] [G loss: 7.876617]\n",
      "epoch:24 step:18955 [D loss: 0.625410, acc.: 67.97%] [G loss: 4.923966]\n",
      "epoch:24 step:18956 [D loss: 0.014678, acc.: 100.00%] [G loss: 3.769867]\n",
      "epoch:24 step:18957 [D loss: 0.025113, acc.: 100.00%] [G loss: 2.532440]\n",
      "epoch:24 step:18958 [D loss: 0.695375, acc.: 71.09%] [G loss: 6.996302]\n",
      "epoch:24 step:18959 [D loss: 0.477139, acc.: 78.12%] [G loss: 4.354685]\n",
      "epoch:24 step:18960 [D loss: 0.206596, acc.: 91.41%] [G loss: 6.608207]\n",
      "epoch:24 step:18961 [D loss: 0.008265, acc.: 100.00%] [G loss: 7.181677]\n",
      "epoch:24 step:18962 [D loss: 0.528604, acc.: 75.00%] [G loss: 1.123381]\n",
      "epoch:24 step:18963 [D loss: 0.912496, acc.: 64.06%] [G loss: 8.626688]\n",
      "epoch:24 step:18964 [D loss: 0.833063, acc.: 62.50%] [G loss: 4.758580]\n",
      "epoch:24 step:18965 [D loss: 0.087337, acc.: 95.31%] [G loss: 3.797035]\n",
      "epoch:24 step:18966 [D loss: 0.100445, acc.: 95.31%] [G loss: 5.967216]\n",
      "epoch:24 step:18967 [D loss: 0.011744, acc.: 99.22%] [G loss: 6.605736]\n",
      "epoch:24 step:18968 [D loss: 0.107519, acc.: 95.31%] [G loss: 4.632020]\n",
      "epoch:24 step:18969 [D loss: 0.041625, acc.: 100.00%] [G loss: 4.265094]\n",
      "epoch:24 step:18970 [D loss: 0.022453, acc.: 99.22%] [G loss: 4.240426]\n",
      "epoch:24 step:18971 [D loss: 0.041504, acc.: 100.00%] [G loss: 4.657924]\n",
      "epoch:24 step:18972 [D loss: 0.021579, acc.: 100.00%] [G loss: 4.323002]\n",
      "epoch:24 step:18973 [D loss: 0.039432, acc.: 100.00%] [G loss: 4.797725]\n",
      "epoch:24 step:18974 [D loss: 0.038518, acc.: 100.00%] [G loss: 5.424956]\n",
      "epoch:24 step:18975 [D loss: 0.014113, acc.: 100.00%] [G loss: 4.450287]\n",
      "epoch:24 step:18976 [D loss: 0.313224, acc.: 86.72%] [G loss: 3.910019]\n",
      "epoch:24 step:18977 [D loss: 0.015907, acc.: 100.00%] [G loss: 4.718192]\n",
      "epoch:24 step:18978 [D loss: 0.055536, acc.: 98.44%] [G loss: 4.210578]\n",
      "epoch:24 step:18979 [D loss: 0.024022, acc.: 100.00%] [G loss: 4.150185]\n",
      "epoch:24 step:18980 [D loss: 0.032573, acc.: 99.22%] [G loss: 3.355067]\n",
      "epoch:24 step:18981 [D loss: 0.174389, acc.: 93.75%] [G loss: 6.614502]\n",
      "epoch:24 step:18982 [D loss: 0.045564, acc.: 99.22%] [G loss: 6.995019]\n",
      "epoch:24 step:18983 [D loss: 0.027824, acc.: 100.00%] [G loss: 6.383648]\n",
      "epoch:24 step:18984 [D loss: 1.017843, acc.: 47.66%] [G loss: 3.602356]\n",
      "epoch:24 step:18985 [D loss: 0.003707, acc.: 100.00%] [G loss: 4.687918]\n",
      "epoch:24 step:18986 [D loss: 0.060407, acc.: 98.44%] [G loss: 3.573095]\n",
      "epoch:24 step:18987 [D loss: 0.039328, acc.: 99.22%] [G loss: 3.476421]\n",
      "epoch:24 step:18988 [D loss: 0.013762, acc.: 100.00%] [G loss: 4.312269]\n",
      "epoch:24 step:18989 [D loss: 0.016094, acc.: 100.00%] [G loss: 3.298108]\n",
      "epoch:24 step:18990 [D loss: 0.014029, acc.: 100.00%] [G loss: 4.133708]\n",
      "epoch:24 step:18991 [D loss: 0.034836, acc.: 99.22%] [G loss: 2.229664]\n",
      "epoch:24 step:18992 [D loss: 0.224965, acc.: 89.84%] [G loss: 5.609972]\n",
      "epoch:24 step:18993 [D loss: 0.161653, acc.: 94.53%] [G loss: 4.731510]\n",
      "epoch:24 step:18994 [D loss: 0.057466, acc.: 100.00%] [G loss: 4.321388]\n",
      "epoch:24 step:18995 [D loss: 0.032715, acc.: 99.22%] [G loss: 4.339582]\n",
      "epoch:24 step:18996 [D loss: 0.049141, acc.: 100.00%] [G loss: 4.156356]\n",
      "epoch:24 step:18997 [D loss: 0.035944, acc.: 100.00%] [G loss: 4.201614]\n",
      "epoch:24 step:18998 [D loss: 0.013658, acc.: 100.00%] [G loss: 4.187471]\n",
      "epoch:24 step:18999 [D loss: 0.062553, acc.: 98.44%] [G loss: 3.950490]\n",
      "epoch:24 step:19000 [D loss: 0.050369, acc.: 100.00%] [G loss: 3.808902]\n",
      "##############\n",
      "[1.10688857 0.93773588 0.88290133 0.97110828 2.1135896  0.76184604\n",
      " 0.80673761 1.10611723 2.11705574 0.92695123]\n",
      "##########\n",
      "epoch:24 step:19001 [D loss: 0.012997, acc.: 100.00%] [G loss: 2.935351]\n",
      "epoch:24 step:19002 [D loss: 0.073766, acc.: 100.00%] [G loss: 3.948204]\n",
      "epoch:24 step:19003 [D loss: 0.016844, acc.: 100.00%] [G loss: 4.381752]\n",
      "epoch:24 step:19004 [D loss: 0.117984, acc.: 96.88%] [G loss: 3.390275]\n",
      "epoch:24 step:19005 [D loss: 0.067613, acc.: 99.22%] [G loss: 2.813630]\n",
      "epoch:24 step:19006 [D loss: 0.050493, acc.: 100.00%] [G loss: 4.312305]\n",
      "epoch:24 step:19007 [D loss: 0.009798, acc.: 100.00%] [G loss: 4.841146]\n",
      "epoch:24 step:19008 [D loss: 0.192146, acc.: 90.62%] [G loss: 2.105862]\n",
      "epoch:24 step:19009 [D loss: 0.437404, acc.: 74.22%] [G loss: 7.616508]\n",
      "epoch:24 step:19010 [D loss: 0.891863, acc.: 59.38%] [G loss: 4.293801]\n",
      "epoch:24 step:19011 [D loss: 0.309001, acc.: 85.16%] [G loss: 7.016107]\n",
      "epoch:24 step:19012 [D loss: 0.001142, acc.: 100.00%] [G loss: 7.378701]\n",
      "epoch:24 step:19013 [D loss: 0.283417, acc.: 84.38%] [G loss: 6.182087]\n",
      "epoch:24 step:19014 [D loss: 0.003223, acc.: 100.00%] [G loss: 5.133900]\n",
      "epoch:24 step:19015 [D loss: 0.005065, acc.: 100.00%] [G loss: 4.705152]\n",
      "epoch:24 step:19016 [D loss: 0.033021, acc.: 100.00%] [G loss: 4.478045]\n",
      "epoch:24 step:19017 [D loss: 0.003428, acc.: 100.00%] [G loss: 4.536339]\n",
      "epoch:24 step:19018 [D loss: 0.014390, acc.: 100.00%] [G loss: 3.783528]\n",
      "epoch:24 step:19019 [D loss: 0.003584, acc.: 100.00%] [G loss: 3.858009]\n",
      "epoch:24 step:19020 [D loss: 0.020802, acc.: 100.00%] [G loss: 3.700747]\n",
      "epoch:24 step:19021 [D loss: 0.015076, acc.: 100.00%] [G loss: 3.345342]\n",
      "epoch:24 step:19022 [D loss: 0.075960, acc.: 99.22%] [G loss: 4.365873]\n",
      "epoch:24 step:19023 [D loss: 0.036777, acc.: 100.00%] [G loss: 4.104027]\n",
      "epoch:24 step:19024 [D loss: 0.013217, acc.: 100.00%] [G loss: 5.360614]\n",
      "epoch:24 step:19025 [D loss: 0.039842, acc.: 99.22%] [G loss: 5.932519]\n",
      "epoch:24 step:19026 [D loss: 0.026043, acc.: 100.00%] [G loss: 4.868726]\n",
      "epoch:24 step:19027 [D loss: 0.016323, acc.: 100.00%] [G loss: 3.473310]\n",
      "epoch:24 step:19028 [D loss: 0.008745, acc.: 100.00%] [G loss: 3.020670]\n",
      "epoch:24 step:19029 [D loss: 0.024608, acc.: 100.00%] [G loss: 2.882200]\n",
      "epoch:24 step:19030 [D loss: 0.495967, acc.: 76.56%] [G loss: 6.368878]\n",
      "epoch:24 step:19031 [D loss: 0.039473, acc.: 99.22%] [G loss: 7.295670]\n",
      "epoch:24 step:19032 [D loss: 0.081241, acc.: 97.66%] [G loss: 6.311409]\n",
      "epoch:24 step:19033 [D loss: 0.041640, acc.: 98.44%] [G loss: 5.393878]\n",
      "epoch:24 step:19034 [D loss: 0.003636, acc.: 100.00%] [G loss: 5.132323]\n",
      "epoch:24 step:19035 [D loss: 0.005625, acc.: 100.00%] [G loss: 4.862007]\n",
      "epoch:24 step:19036 [D loss: 0.008183, acc.: 100.00%] [G loss: 4.582070]\n",
      "epoch:24 step:19037 [D loss: 0.017245, acc.: 100.00%] [G loss: 4.532177]\n",
      "epoch:24 step:19038 [D loss: 0.064285, acc.: 98.44%] [G loss: 5.251216]\n",
      "epoch:24 step:19039 [D loss: 0.082876, acc.: 98.44%] [G loss: 4.212175]\n",
      "epoch:24 step:19040 [D loss: 0.008226, acc.: 100.00%] [G loss: 3.980130]\n",
      "epoch:24 step:19041 [D loss: 0.037987, acc.: 100.00%] [G loss: 4.621817]\n",
      "epoch:24 step:19042 [D loss: 0.012809, acc.: 100.00%] [G loss: 5.059796]\n",
      "epoch:24 step:19043 [D loss: 0.008983, acc.: 100.00%] [G loss: 4.960045]\n",
      "epoch:24 step:19044 [D loss: 0.004478, acc.: 100.00%] [G loss: 4.973737]\n",
      "epoch:24 step:19045 [D loss: 0.005008, acc.: 100.00%] [G loss: 4.339103]\n",
      "epoch:24 step:19046 [D loss: 0.056470, acc.: 98.44%] [G loss: 5.293828]\n",
      "epoch:24 step:19047 [D loss: 0.147174, acc.: 93.75%] [G loss: 2.550666]\n",
      "epoch:24 step:19048 [D loss: 0.077226, acc.: 99.22%] [G loss: 3.825056]\n",
      "epoch:24 step:19049 [D loss: 0.008172, acc.: 100.00%] [G loss: 5.267499]\n",
      "epoch:24 step:19050 [D loss: 0.010837, acc.: 100.00%] [G loss: 4.482718]\n",
      "epoch:24 step:19051 [D loss: 0.021867, acc.: 100.00%] [G loss: 4.284186]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:24 step:19052 [D loss: 0.014187, acc.: 100.00%] [G loss: 4.355464]\n",
      "epoch:24 step:19053 [D loss: 0.044574, acc.: 100.00%] [G loss: 3.419897]\n",
      "epoch:24 step:19054 [D loss: 0.038576, acc.: 99.22%] [G loss: 2.440760]\n",
      "epoch:24 step:19055 [D loss: 0.021440, acc.: 100.00%] [G loss: 4.402072]\n",
      "epoch:24 step:19056 [D loss: 0.198610, acc.: 89.84%] [G loss: 5.148471]\n",
      "epoch:24 step:19057 [D loss: 0.031916, acc.: 100.00%] [G loss: 5.554063]\n",
      "epoch:24 step:19058 [D loss: 0.067703, acc.: 96.88%] [G loss: 3.425502]\n",
      "epoch:24 step:19059 [D loss: 0.355462, acc.: 81.25%] [G loss: 7.618169]\n",
      "epoch:24 step:19060 [D loss: 1.328588, acc.: 55.47%] [G loss: 2.973205]\n",
      "epoch:24 step:19061 [D loss: 0.556206, acc.: 71.88%] [G loss: 6.945068]\n",
      "epoch:24 step:19062 [D loss: 0.578480, acc.: 73.44%] [G loss: 5.036752]\n",
      "epoch:24 step:19063 [D loss: 0.024986, acc.: 100.00%] [G loss: 3.778338]\n",
      "epoch:24 step:19064 [D loss: 0.659041, acc.: 64.06%] [G loss: 4.336134]\n",
      "epoch:24 step:19065 [D loss: 0.008026, acc.: 100.00%] [G loss: 5.756354]\n",
      "epoch:24 step:19066 [D loss: 0.024164, acc.: 99.22%] [G loss: 5.188853]\n",
      "epoch:24 step:19067 [D loss: 0.011535, acc.: 100.00%] [G loss: 5.157898]\n",
      "epoch:24 step:19068 [D loss: 0.094147, acc.: 96.88%] [G loss: 5.193650]\n",
      "epoch:24 step:19069 [D loss: 0.043473, acc.: 100.00%] [G loss: 4.470965]\n",
      "epoch:24 step:19070 [D loss: 0.010508, acc.: 100.00%] [G loss: 4.037250]\n",
      "epoch:24 step:19071 [D loss: 0.101325, acc.: 98.44%] [G loss: 5.883440]\n",
      "epoch:24 step:19072 [D loss: 0.026900, acc.: 99.22%] [G loss: 5.759327]\n",
      "epoch:24 step:19073 [D loss: 0.073267, acc.: 96.88%] [G loss: 4.763052]\n",
      "epoch:24 step:19074 [D loss: 0.015305, acc.: 100.00%] [G loss: 4.287743]\n",
      "epoch:24 step:19075 [D loss: 0.028483, acc.: 100.00%] [G loss: 3.305681]\n",
      "epoch:24 step:19076 [D loss: 0.113985, acc.: 96.09%] [G loss: 5.225353]\n",
      "epoch:24 step:19077 [D loss: 0.012340, acc.: 100.00%] [G loss: 5.983079]\n",
      "epoch:24 step:19078 [D loss: 0.121728, acc.: 94.53%] [G loss: 4.112244]\n",
      "epoch:24 step:19079 [D loss: 0.037142, acc.: 100.00%] [G loss: 3.906728]\n",
      "epoch:24 step:19080 [D loss: 0.017738, acc.: 100.00%] [G loss: 5.111001]\n",
      "epoch:24 step:19081 [D loss: 0.009753, acc.: 100.00%] [G loss: 4.798308]\n",
      "epoch:24 step:19082 [D loss: 0.118521, acc.: 96.88%] [G loss: 3.659315]\n",
      "epoch:24 step:19083 [D loss: 0.015200, acc.: 100.00%] [G loss: 4.833293]\n",
      "epoch:24 step:19084 [D loss: 1.490029, acc.: 39.84%] [G loss: 7.395725]\n",
      "epoch:24 step:19085 [D loss: 1.283366, acc.: 54.69%] [G loss: 2.587027]\n",
      "epoch:24 step:19086 [D loss: 0.491197, acc.: 74.22%] [G loss: 6.173067]\n",
      "epoch:24 step:19087 [D loss: 0.226150, acc.: 86.72%] [G loss: 5.084512]\n",
      "epoch:24 step:19088 [D loss: 0.062692, acc.: 98.44%] [G loss: 3.785392]\n",
      "epoch:24 step:19089 [D loss: 0.023926, acc.: 100.00%] [G loss: 3.091573]\n",
      "epoch:24 step:19090 [D loss: 0.048649, acc.: 100.00%] [G loss: 2.760971]\n",
      "epoch:24 step:19091 [D loss: 0.021665, acc.: 100.00%] [G loss: 3.027009]\n",
      "epoch:24 step:19092 [D loss: 0.080201, acc.: 96.88%] [G loss: 4.091742]\n",
      "epoch:24 step:19093 [D loss: 0.132512, acc.: 95.31%] [G loss: 3.619717]\n",
      "epoch:24 step:19094 [D loss: 0.049916, acc.: 100.00%] [G loss: 2.201883]\n",
      "epoch:24 step:19095 [D loss: 0.102674, acc.: 97.66%] [G loss: 3.495211]\n",
      "epoch:24 step:19096 [D loss: 0.041545, acc.: 99.22%] [G loss: 5.331699]\n",
      "epoch:24 step:19097 [D loss: 0.016431, acc.: 100.00%] [G loss: 4.186738]\n",
      "epoch:24 step:19098 [D loss: 0.023780, acc.: 100.00%] [G loss: 3.917511]\n",
      "epoch:24 step:19099 [D loss: 0.018441, acc.: 100.00%] [G loss: 2.421996]\n",
      "epoch:24 step:19100 [D loss: 0.057758, acc.: 100.00%] [G loss: 3.149844]\n",
      "epoch:24 step:19101 [D loss: 0.023701, acc.: 100.00%] [G loss: 2.969089]\n",
      "epoch:24 step:19102 [D loss: 0.163376, acc.: 94.53%] [G loss: 2.873075]\n",
      "epoch:24 step:19103 [D loss: 0.030880, acc.: 100.00%] [G loss: 4.597873]\n",
      "epoch:24 step:19104 [D loss: 0.019447, acc.: 100.00%] [G loss: 3.854034]\n",
      "epoch:24 step:19105 [D loss: 0.282917, acc.: 89.06%] [G loss: 3.990732]\n",
      "epoch:24 step:19106 [D loss: 0.004306, acc.: 100.00%] [G loss: 6.026320]\n",
      "epoch:24 step:19107 [D loss: 0.051723, acc.: 98.44%] [G loss: 4.160826]\n",
      "epoch:24 step:19108 [D loss: 0.118046, acc.: 96.09%] [G loss: 5.111496]\n",
      "epoch:24 step:19109 [D loss: 0.003935, acc.: 100.00%] [G loss: 4.517657]\n",
      "epoch:24 step:19110 [D loss: 0.052607, acc.: 100.00%] [G loss: 3.173640]\n",
      "epoch:24 step:19111 [D loss: 0.078529, acc.: 97.66%] [G loss: 5.028256]\n",
      "epoch:24 step:19112 [D loss: 0.002337, acc.: 100.00%] [G loss: 5.376556]\n",
      "epoch:24 step:19113 [D loss: 0.005961, acc.: 100.00%] [G loss: 4.957796]\n",
      "epoch:24 step:19114 [D loss: 0.095494, acc.: 98.44%] [G loss: 3.562339]\n",
      "epoch:24 step:19115 [D loss: 0.054163, acc.: 99.22%] [G loss: 4.822711]\n",
      "epoch:24 step:19116 [D loss: 0.006216, acc.: 100.00%] [G loss: 5.656064]\n",
      "epoch:24 step:19117 [D loss: 0.092119, acc.: 96.88%] [G loss: 2.341081]\n",
      "epoch:24 step:19118 [D loss: 0.110674, acc.: 97.66%] [G loss: 6.054610]\n",
      "epoch:24 step:19119 [D loss: 0.110008, acc.: 96.88%] [G loss: 4.846174]\n",
      "epoch:24 step:19120 [D loss: 0.054321, acc.: 99.22%] [G loss: 4.266354]\n",
      "epoch:24 step:19121 [D loss: 0.009103, acc.: 100.00%] [G loss: 4.414475]\n",
      "epoch:24 step:19122 [D loss: 0.016704, acc.: 100.00%] [G loss: 3.958048]\n",
      "epoch:24 step:19123 [D loss: 0.011214, acc.: 100.00%] [G loss: 4.323451]\n",
      "epoch:24 step:19124 [D loss: 0.004089, acc.: 100.00%] [G loss: 3.751972]\n",
      "epoch:24 step:19125 [D loss: 0.010559, acc.: 100.00%] [G loss: 4.422380]\n",
      "epoch:24 step:19126 [D loss: 0.037816, acc.: 99.22%] [G loss: 3.050650]\n",
      "epoch:24 step:19127 [D loss: 0.013016, acc.: 100.00%] [G loss: 4.445560]\n",
      "epoch:24 step:19128 [D loss: 0.028195, acc.: 99.22%] [G loss: 3.536950]\n",
      "epoch:24 step:19129 [D loss: 0.292802, acc.: 89.06%] [G loss: 5.558233]\n",
      "epoch:24 step:19130 [D loss: 0.761778, acc.: 63.28%] [G loss: 6.370625]\n",
      "epoch:24 step:19131 [D loss: 0.030876, acc.: 99.22%] [G loss: 7.346163]\n",
      "epoch:24 step:19132 [D loss: 0.009494, acc.: 100.00%] [G loss: 7.603605]\n",
      "epoch:24 step:19133 [D loss: 0.018526, acc.: 100.00%] [G loss: 7.338887]\n",
      "epoch:24 step:19134 [D loss: 0.108872, acc.: 96.09%] [G loss: 4.153916]\n",
      "epoch:24 step:19135 [D loss: 0.014801, acc.: 100.00%] [G loss: 3.334949]\n",
      "epoch:24 step:19136 [D loss: 0.061435, acc.: 99.22%] [G loss: 5.053086]\n",
      "epoch:24 step:19137 [D loss: 0.003368, acc.: 100.00%] [G loss: 5.733214]\n",
      "epoch:24 step:19138 [D loss: 0.001347, acc.: 100.00%] [G loss: 4.796206]\n",
      "epoch:24 step:19139 [D loss: 0.008142, acc.: 100.00%] [G loss: 4.733575]\n",
      "epoch:24 step:19140 [D loss: 0.006710, acc.: 100.00%] [G loss: 4.206088]\n",
      "epoch:24 step:19141 [D loss: 0.006033, acc.: 100.00%] [G loss: 2.674531]\n",
      "epoch:24 step:19142 [D loss: 0.003416, acc.: 100.00%] [G loss: 4.407548]\n",
      "epoch:24 step:19143 [D loss: 0.013125, acc.: 100.00%] [G loss: 3.355292]\n",
      "epoch:24 step:19144 [D loss: 0.031464, acc.: 100.00%] [G loss: 4.207146]\n",
      "epoch:24 step:19145 [D loss: 1.056165, acc.: 51.56%] [G loss: 7.727224]\n",
      "epoch:24 step:19146 [D loss: 0.173948, acc.: 92.19%] [G loss: 7.407411]\n",
      "epoch:24 step:19147 [D loss: 1.107244, acc.: 56.25%] [G loss: 2.107831]\n",
      "epoch:24 step:19148 [D loss: 1.309425, acc.: 56.25%] [G loss: 6.807104]\n",
      "epoch:24 step:19149 [D loss: 0.103348, acc.: 96.09%] [G loss: 7.226660]\n",
      "epoch:24 step:19150 [D loss: 1.541370, acc.: 50.78%] [G loss: 4.530702]\n",
      "epoch:24 step:19151 [D loss: 0.007413, acc.: 100.00%] [G loss: 2.646423]\n",
      "epoch:24 step:19152 [D loss: 0.125304, acc.: 94.53%] [G loss: 3.304224]\n",
      "epoch:24 step:19153 [D loss: 0.021043, acc.: 99.22%] [G loss: 4.611526]\n",
      "epoch:24 step:19154 [D loss: 0.014183, acc.: 100.00%] [G loss: 3.126409]\n",
      "epoch:24 step:19155 [D loss: 0.029957, acc.: 99.22%] [G loss: 3.790066]\n",
      "epoch:24 step:19156 [D loss: 0.007386, acc.: 100.00%] [G loss: 4.313219]\n",
      "epoch:24 step:19157 [D loss: 0.063208, acc.: 98.44%] [G loss: 3.456560]\n",
      "epoch:24 step:19158 [D loss: 0.008822, acc.: 100.00%] [G loss: 4.016327]\n",
      "epoch:24 step:19159 [D loss: 0.008511, acc.: 100.00%] [G loss: 3.361457]\n",
      "epoch:24 step:19160 [D loss: 0.036911, acc.: 100.00%] [G loss: 1.922937]\n",
      "epoch:24 step:19161 [D loss: 0.005437, acc.: 100.00%] [G loss: 3.236871]\n",
      "epoch:24 step:19162 [D loss: 0.032060, acc.: 99.22%] [G loss: 1.957161]\n",
      "epoch:24 step:19163 [D loss: 0.057358, acc.: 99.22%] [G loss: 1.709944]\n",
      "epoch:24 step:19164 [D loss: 0.372578, acc.: 86.72%] [G loss: 1.609780]\n",
      "epoch:24 step:19165 [D loss: 0.008814, acc.: 100.00%] [G loss: 3.115352]\n",
      "epoch:24 step:19166 [D loss: 0.067106, acc.: 99.22%] [G loss: 2.371401]\n",
      "epoch:24 step:19167 [D loss: 0.043611, acc.: 99.22%] [G loss: 1.646487]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:24 step:19168 [D loss: 0.015266, acc.: 100.00%] [G loss: 1.773121]\n",
      "epoch:24 step:19169 [D loss: 0.051291, acc.: 99.22%] [G loss: 2.147840]\n",
      "epoch:24 step:19170 [D loss: 0.007322, acc.: 100.00%] [G loss: 2.389889]\n",
      "epoch:24 step:19171 [D loss: 0.033838, acc.: 100.00%] [G loss: 3.174334]\n",
      "epoch:24 step:19172 [D loss: 0.125709, acc.: 93.75%] [G loss: 2.821891]\n",
      "epoch:24 step:19173 [D loss: 0.192362, acc.: 92.19%] [G loss: 3.936631]\n",
      "epoch:24 step:19174 [D loss: 0.068487, acc.: 99.22%] [G loss: 5.161087]\n",
      "epoch:24 step:19175 [D loss: 0.181243, acc.: 92.19%] [G loss: 2.133336]\n",
      "epoch:24 step:19176 [D loss: 0.484630, acc.: 71.88%] [G loss: 7.001360]\n",
      "epoch:24 step:19177 [D loss: 0.010052, acc.: 100.00%] [G loss: 8.425576]\n",
      "epoch:24 step:19178 [D loss: 0.363724, acc.: 81.25%] [G loss: 6.990486]\n",
      "epoch:24 step:19179 [D loss: 0.013064, acc.: 100.00%] [G loss: 5.926716]\n",
      "epoch:24 step:19180 [D loss: 0.016183, acc.: 100.00%] [G loss: 5.503195]\n",
      "epoch:24 step:19181 [D loss: 0.022596, acc.: 100.00%] [G loss: 4.902948]\n",
      "epoch:24 step:19182 [D loss: 0.006164, acc.: 100.00%] [G loss: 5.402250]\n",
      "epoch:24 step:19183 [D loss: 0.008938, acc.: 100.00%] [G loss: 3.904624]\n",
      "epoch:24 step:19184 [D loss: 0.025181, acc.: 100.00%] [G loss: 4.453285]\n",
      "epoch:24 step:19185 [D loss: 0.020300, acc.: 100.00%] [G loss: 4.058363]\n",
      "epoch:24 step:19186 [D loss: 0.017712, acc.: 99.22%] [G loss: 3.177042]\n",
      "epoch:24 step:19187 [D loss: 0.022836, acc.: 100.00%] [G loss: 3.947141]\n",
      "epoch:24 step:19188 [D loss: 0.020452, acc.: 99.22%] [G loss: 3.669437]\n",
      "epoch:24 step:19189 [D loss: 0.007283, acc.: 100.00%] [G loss: 2.248337]\n",
      "epoch:24 step:19190 [D loss: 0.049248, acc.: 100.00%] [G loss: 3.966622]\n",
      "epoch:24 step:19191 [D loss: 0.018430, acc.: 100.00%] [G loss: 3.244579]\n",
      "epoch:24 step:19192 [D loss: 1.867290, acc.: 41.41%] [G loss: 9.062534]\n",
      "epoch:24 step:19193 [D loss: 2.531259, acc.: 50.00%] [G loss: 6.172726]\n",
      "epoch:24 step:19194 [D loss: 0.181266, acc.: 89.84%] [G loss: 4.069778]\n",
      "epoch:24 step:19195 [D loss: 0.149089, acc.: 94.53%] [G loss: 1.379863]\n",
      "epoch:24 step:19196 [D loss: 0.186091, acc.: 89.84%] [G loss: 3.402005]\n",
      "epoch:24 step:19197 [D loss: 0.080120, acc.: 98.44%] [G loss: 3.194869]\n",
      "epoch:24 step:19198 [D loss: 0.012051, acc.: 100.00%] [G loss: 3.115303]\n",
      "epoch:24 step:19199 [D loss: 0.046414, acc.: 99.22%] [G loss: 3.202510]\n",
      "epoch:24 step:19200 [D loss: 0.022495, acc.: 100.00%] [G loss: 2.924118]\n",
      "##############\n",
      "[1.04277267 0.9926985  1.02054253 0.9589126  2.10787267 2.0942142\n",
      " 1.08297601 2.12784129 1.10289755 0.9842565 ]\n",
      "##########\n",
      "epoch:24 step:19201 [D loss: 0.021320, acc.: 100.00%] [G loss: 2.401840]\n",
      "epoch:24 step:19202 [D loss: 0.028538, acc.: 100.00%] [G loss: 2.734637]\n",
      "epoch:24 step:19203 [D loss: 0.164360, acc.: 97.66%] [G loss: 2.363019]\n",
      "epoch:24 step:19204 [D loss: 1.414266, acc.: 41.41%] [G loss: 7.362569]\n",
      "epoch:24 step:19205 [D loss: 1.999952, acc.: 50.00%] [G loss: 5.857629]\n",
      "epoch:24 step:19206 [D loss: 0.385369, acc.: 78.12%] [G loss: 3.193856]\n",
      "epoch:24 step:19207 [D loss: 0.090725, acc.: 96.88%] [G loss: 3.537325]\n",
      "epoch:24 step:19208 [D loss: 0.069739, acc.: 99.22%] [G loss: 3.354712]\n",
      "epoch:24 step:19209 [D loss: 0.051371, acc.: 100.00%] [G loss: 3.633497]\n",
      "epoch:24 step:19210 [D loss: 0.043050, acc.: 100.00%] [G loss: 3.794637]\n",
      "epoch:24 step:19211 [D loss: 0.141368, acc.: 93.75%] [G loss: 3.048717]\n",
      "epoch:24 step:19212 [D loss: 0.134764, acc.: 96.88%] [G loss: 4.625813]\n",
      "epoch:24 step:19213 [D loss: 0.014363, acc.: 100.00%] [G loss: 4.772198]\n",
      "epoch:24 step:19214 [D loss: 0.027779, acc.: 100.00%] [G loss: 5.002872]\n",
      "epoch:24 step:19215 [D loss: 0.037275, acc.: 100.00%] [G loss: 3.807224]\n",
      "epoch:24 step:19216 [D loss: 0.019288, acc.: 100.00%] [G loss: 3.762242]\n",
      "epoch:24 step:19217 [D loss: 0.026356, acc.: 100.00%] [G loss: 3.848688]\n",
      "epoch:24 step:19218 [D loss: 0.078174, acc.: 98.44%] [G loss: 3.353941]\n",
      "epoch:24 step:19219 [D loss: 0.051237, acc.: 100.00%] [G loss: 3.341124]\n",
      "epoch:24 step:19220 [D loss: 0.022281, acc.: 100.00%] [G loss: 3.745130]\n",
      "epoch:24 step:19221 [D loss: 0.012274, acc.: 100.00%] [G loss: 4.130555]\n",
      "epoch:24 step:19222 [D loss: 0.041320, acc.: 99.22%] [G loss: 3.829450]\n",
      "epoch:24 step:19223 [D loss: 0.045836, acc.: 99.22%] [G loss: 3.213155]\n",
      "epoch:24 step:19224 [D loss: 0.052169, acc.: 99.22%] [G loss: 4.233746]\n",
      "epoch:24 step:19225 [D loss: 0.012006, acc.: 100.00%] [G loss: 4.101317]\n",
      "epoch:24 step:19226 [D loss: 0.121442, acc.: 96.88%] [G loss: 2.592361]\n",
      "epoch:24 step:19227 [D loss: 0.056480, acc.: 99.22%] [G loss: 2.803682]\n",
      "epoch:24 step:19228 [D loss: 0.010112, acc.: 100.00%] [G loss: 3.382725]\n",
      "epoch:24 step:19229 [D loss: 0.046204, acc.: 100.00%] [G loss: 2.677358]\n",
      "epoch:24 step:19230 [D loss: 0.015989, acc.: 100.00%] [G loss: 3.421706]\n",
      "epoch:24 step:19231 [D loss: 0.025792, acc.: 100.00%] [G loss: 2.258226]\n",
      "epoch:24 step:19232 [D loss: 0.067215, acc.: 99.22%] [G loss: 3.312963]\n",
      "epoch:24 step:19233 [D loss: 0.021967, acc.: 100.00%] [G loss: 4.613491]\n",
      "epoch:24 step:19234 [D loss: 0.039987, acc.: 100.00%] [G loss: 3.698715]\n",
      "epoch:24 step:19235 [D loss: 0.155546, acc.: 98.44%] [G loss: 4.433515]\n",
      "epoch:24 step:19236 [D loss: 0.352115, acc.: 84.38%] [G loss: 3.026763]\n",
      "epoch:24 step:19237 [D loss: 0.005134, acc.: 100.00%] [G loss: 4.475423]\n",
      "epoch:24 step:19238 [D loss: 0.008233, acc.: 100.00%] [G loss: 3.884387]\n",
      "epoch:24 step:19239 [D loss: 0.012545, acc.: 100.00%] [G loss: 4.778184]\n",
      "epoch:24 step:19240 [D loss: 0.023335, acc.: 100.00%] [G loss: 3.629663]\n",
      "epoch:24 step:19241 [D loss: 0.027888, acc.: 100.00%] [G loss: 3.818689]\n",
      "epoch:24 step:19242 [D loss: 0.029761, acc.: 99.22%] [G loss: 3.558272]\n",
      "epoch:24 step:19243 [D loss: 0.070067, acc.: 99.22%] [G loss: 3.584548]\n",
      "epoch:24 step:19244 [D loss: 0.008229, acc.: 100.00%] [G loss: 4.226960]\n",
      "epoch:24 step:19245 [D loss: 0.059468, acc.: 99.22%] [G loss: 4.069356]\n",
      "epoch:24 step:19246 [D loss: 0.027851, acc.: 100.00%] [G loss: 4.324299]\n",
      "epoch:24 step:19247 [D loss: 0.007782, acc.: 100.00%] [G loss: 4.018934]\n",
      "epoch:24 step:19248 [D loss: 0.012281, acc.: 100.00%] [G loss: 4.344679]\n",
      "epoch:24 step:19249 [D loss: 0.033917, acc.: 100.00%] [G loss: 3.630672]\n",
      "epoch:24 step:19250 [D loss: 0.039267, acc.: 100.00%] [G loss: 4.862379]\n",
      "epoch:24 step:19251 [D loss: 0.026199, acc.: 100.00%] [G loss: 4.789499]\n",
      "epoch:24 step:19252 [D loss: 0.064972, acc.: 100.00%] [G loss: 3.751681]\n",
      "epoch:24 step:19253 [D loss: 0.124634, acc.: 98.44%] [G loss: 3.501663]\n",
      "epoch:24 step:19254 [D loss: 0.011531, acc.: 100.00%] [G loss: 4.173696]\n",
      "epoch:24 step:19255 [D loss: 0.030401, acc.: 100.00%] [G loss: 3.421079]\n",
      "epoch:24 step:19256 [D loss: 0.013420, acc.: 100.00%] [G loss: 3.922051]\n",
      "epoch:24 step:19257 [D loss: 0.040441, acc.: 100.00%] [G loss: 3.636861]\n",
      "epoch:24 step:19258 [D loss: 0.036307, acc.: 100.00%] [G loss: 5.443969]\n",
      "epoch:24 step:19259 [D loss: 0.019087, acc.: 99.22%] [G loss: 5.324268]\n",
      "epoch:24 step:19260 [D loss: 0.202449, acc.: 94.53%] [G loss: 4.682899]\n",
      "epoch:24 step:19261 [D loss: 0.009053, acc.: 100.00%] [G loss: 5.462387]\n",
      "epoch:24 step:19262 [D loss: 0.022486, acc.: 100.00%] [G loss: 4.195015]\n",
      "epoch:24 step:19263 [D loss: 0.010098, acc.: 100.00%] [G loss: 4.646268]\n",
      "epoch:24 step:19264 [D loss: 0.005812, acc.: 100.00%] [G loss: 4.498527]\n",
      "epoch:24 step:19265 [D loss: 0.016144, acc.: 100.00%] [G loss: 3.153895]\n",
      "epoch:24 step:19266 [D loss: 0.030723, acc.: 100.00%] [G loss: 3.618375]\n",
      "epoch:24 step:19267 [D loss: 0.107922, acc.: 97.66%] [G loss: 4.814922]\n",
      "epoch:24 step:19268 [D loss: 0.113144, acc.: 96.09%] [G loss: 3.194597]\n",
      "epoch:24 step:19269 [D loss: 0.040342, acc.: 99.22%] [G loss: 4.201904]\n",
      "epoch:24 step:19270 [D loss: 0.074156, acc.: 99.22%] [G loss: 5.383729]\n",
      "epoch:24 step:19271 [D loss: 0.007218, acc.: 100.00%] [G loss: 5.275559]\n",
      "epoch:24 step:19272 [D loss: 0.014768, acc.: 100.00%] [G loss: 5.142663]\n",
      "epoch:24 step:19273 [D loss: 0.013190, acc.: 100.00%] [G loss: 5.356339]\n",
      "epoch:24 step:19274 [D loss: 0.009420, acc.: 100.00%] [G loss: 4.838710]\n",
      "epoch:24 step:19275 [D loss: 0.008833, acc.: 100.00%] [G loss: 4.289545]\n",
      "epoch:24 step:19276 [D loss: 0.322300, acc.: 86.72%] [G loss: 6.631721]\n",
      "epoch:24 step:19277 [D loss: 1.992765, acc.: 30.47%] [G loss: 8.761318]\n",
      "epoch:24 step:19278 [D loss: 2.880749, acc.: 50.00%] [G loss: 5.868358]\n",
      "epoch:24 step:19279 [D loss: 1.851182, acc.: 50.00%] [G loss: 2.283509]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:24 step:19280 [D loss: 0.280620, acc.: 90.62%] [G loss: 2.459880]\n",
      "epoch:24 step:19281 [D loss: 0.092246, acc.: 98.44%] [G loss: 2.516599]\n",
      "epoch:24 step:19282 [D loss: 0.166913, acc.: 93.75%] [G loss: 3.266165]\n",
      "epoch:24 step:19283 [D loss: 0.116411, acc.: 96.09%] [G loss: 3.413786]\n",
      "epoch:24 step:19284 [D loss: 0.226851, acc.: 90.62%] [G loss: 2.451287]\n",
      "epoch:24 step:19285 [D loss: 0.141454, acc.: 99.22%] [G loss: 2.418168]\n",
      "epoch:24 step:19286 [D loss: 0.168100, acc.: 96.88%] [G loss: 3.060538]\n",
      "epoch:24 step:19287 [D loss: 0.209517, acc.: 93.75%] [G loss: 3.251123]\n",
      "epoch:24 step:19288 [D loss: 0.503693, acc.: 71.88%] [G loss: 3.131783]\n",
      "epoch:24 step:19289 [D loss: 0.166400, acc.: 93.75%] [G loss: 3.834042]\n",
      "epoch:24 step:19290 [D loss: 0.346701, acc.: 82.03%] [G loss: 2.727217]\n",
      "epoch:24 step:19291 [D loss: 0.089719, acc.: 98.44%] [G loss: 2.872385]\n",
      "epoch:24 step:19292 [D loss: 0.081216, acc.: 99.22%] [G loss: 3.414964]\n",
      "epoch:24 step:19293 [D loss: 0.088888, acc.: 98.44%] [G loss: 3.541893]\n",
      "epoch:24 step:19294 [D loss: 0.107661, acc.: 96.88%] [G loss: 3.349309]\n",
      "epoch:24 step:19295 [D loss: 0.134848, acc.: 96.88%] [G loss: 3.236822]\n",
      "epoch:24 step:19296 [D loss: 0.144403, acc.: 92.97%] [G loss: 1.531541]\n",
      "epoch:24 step:19297 [D loss: 0.100772, acc.: 99.22%] [G loss: 2.659051]\n",
      "epoch:24 step:19298 [D loss: 0.262248, acc.: 88.28%] [G loss: 4.003527]\n",
      "epoch:24 step:19299 [D loss: 0.323073, acc.: 87.50%] [G loss: 3.784560]\n",
      "epoch:24 step:19300 [D loss: 0.163891, acc.: 96.88%] [G loss: 2.202064]\n",
      "epoch:24 step:19301 [D loss: 0.216674, acc.: 92.97%] [G loss: 3.975132]\n",
      "epoch:24 step:19302 [D loss: 0.169624, acc.: 93.75%] [G loss: 3.828768]\n",
      "epoch:24 step:19303 [D loss: 0.034546, acc.: 100.00%] [G loss: 3.378662]\n",
      "epoch:24 step:19304 [D loss: 0.554177, acc.: 71.09%] [G loss: 3.831306]\n",
      "epoch:24 step:19305 [D loss: 0.074328, acc.: 96.88%] [G loss: 4.911742]\n",
      "epoch:24 step:19306 [D loss: 0.340779, acc.: 83.59%] [G loss: 3.435127]\n",
      "epoch:24 step:19307 [D loss: 0.054041, acc.: 99.22%] [G loss: 3.241858]\n",
      "epoch:24 step:19308 [D loss: 0.027399, acc.: 100.00%] [G loss: 3.370504]\n",
      "epoch:24 step:19309 [D loss: 0.074848, acc.: 98.44%] [G loss: 3.383938]\n",
      "epoch:24 step:19310 [D loss: 0.086743, acc.: 99.22%] [G loss: 3.325288]\n",
      "epoch:24 step:19311 [D loss: 0.018429, acc.: 100.00%] [G loss: 3.541300]\n",
      "epoch:24 step:19312 [D loss: 0.056866, acc.: 99.22%] [G loss: 2.649916]\n",
      "epoch:24 step:19313 [D loss: 0.150031, acc.: 95.31%] [G loss: 4.472478]\n",
      "epoch:24 step:19314 [D loss: 0.155736, acc.: 96.09%] [G loss: 3.948058]\n",
      "epoch:24 step:19315 [D loss: 0.209485, acc.: 91.41%] [G loss: 2.940427]\n",
      "epoch:24 step:19316 [D loss: 0.137175, acc.: 95.31%] [G loss: 4.587423]\n",
      "epoch:24 step:19317 [D loss: 0.072154, acc.: 99.22%] [G loss: 4.355852]\n",
      "epoch:24 step:19318 [D loss: 0.121982, acc.: 96.88%] [G loss: 3.189171]\n",
      "epoch:24 step:19319 [D loss: 0.025884, acc.: 100.00%] [G loss: 1.958408]\n",
      "epoch:24 step:19320 [D loss: 0.043856, acc.: 100.00%] [G loss: 2.671751]\n",
      "epoch:24 step:19321 [D loss: 0.024303, acc.: 100.00%] [G loss: 3.151982]\n",
      "epoch:24 step:19322 [D loss: 0.031786, acc.: 100.00%] [G loss: 2.313252]\n",
      "epoch:24 step:19323 [D loss: 0.028824, acc.: 100.00%] [G loss: 1.989849]\n",
      "epoch:24 step:19324 [D loss: 0.140751, acc.: 96.09%] [G loss: 2.007882]\n",
      "epoch:24 step:19325 [D loss: 0.120136, acc.: 97.66%] [G loss: 3.183370]\n",
      "epoch:24 step:19326 [D loss: 0.229583, acc.: 89.84%] [G loss: 2.183497]\n",
      "epoch:24 step:19327 [D loss: 0.048666, acc.: 99.22%] [G loss: 1.673520]\n",
      "epoch:24 step:19328 [D loss: 0.070678, acc.: 98.44%] [G loss: 2.670210]\n",
      "epoch:24 step:19329 [D loss: 0.028271, acc.: 100.00%] [G loss: 2.913904]\n",
      "epoch:24 step:19330 [D loss: 0.015110, acc.: 99.22%] [G loss: 2.249546]\n",
      "epoch:24 step:19331 [D loss: 0.218913, acc.: 92.97%] [G loss: 1.787239]\n",
      "epoch:24 step:19332 [D loss: 0.446192, acc.: 76.56%] [G loss: 7.339248]\n",
      "epoch:24 step:19333 [D loss: 1.577920, acc.: 52.34%] [G loss: 3.354828]\n",
      "epoch:24 step:19334 [D loss: 0.530982, acc.: 81.25%] [G loss: 6.079482]\n",
      "epoch:24 step:19335 [D loss: 0.052262, acc.: 98.44%] [G loss: 6.848523]\n",
      "epoch:24 step:19336 [D loss: 0.283630, acc.: 86.72%] [G loss: 4.352331]\n",
      "epoch:24 step:19337 [D loss: 0.028325, acc.: 99.22%] [G loss: 4.350209]\n",
      "epoch:24 step:19338 [D loss: 0.077660, acc.: 97.66%] [G loss: 4.220238]\n",
      "epoch:24 step:19339 [D loss: 0.039053, acc.: 100.00%] [G loss: 4.970100]\n",
      "epoch:24 step:19340 [D loss: 0.018258, acc.: 100.00%] [G loss: 4.824765]\n",
      "epoch:24 step:19341 [D loss: 0.030283, acc.: 100.00%] [G loss: 3.921011]\n",
      "epoch:24 step:19342 [D loss: 0.015789, acc.: 100.00%] [G loss: 3.475963]\n",
      "epoch:24 step:19343 [D loss: 0.037038, acc.: 100.00%] [G loss: 4.118081]\n",
      "epoch:24 step:19344 [D loss: 0.043206, acc.: 99.22%] [G loss: 3.922109]\n",
      "epoch:24 step:19345 [D loss: 0.172678, acc.: 96.09%] [G loss: 2.659995]\n",
      "epoch:24 step:19346 [D loss: 0.077239, acc.: 100.00%] [G loss: 3.619599]\n",
      "epoch:24 step:19347 [D loss: 0.019833, acc.: 100.00%] [G loss: 4.406383]\n",
      "epoch:24 step:19348 [D loss: 0.120113, acc.: 97.66%] [G loss: 3.493823]\n",
      "epoch:24 step:19349 [D loss: 0.023748, acc.: 100.00%] [G loss: 3.374005]\n",
      "epoch:24 step:19350 [D loss: 0.145028, acc.: 96.09%] [G loss: 4.629507]\n",
      "epoch:24 step:19351 [D loss: 0.060161, acc.: 98.44%] [G loss: 4.394051]\n",
      "epoch:24 step:19352 [D loss: 0.007924, acc.: 100.00%] [G loss: 3.745808]\n",
      "epoch:24 step:19353 [D loss: 0.044625, acc.: 100.00%] [G loss: 3.546666]\n",
      "epoch:24 step:19354 [D loss: 0.126388, acc.: 98.44%] [G loss: 3.612494]\n",
      "epoch:24 step:19355 [D loss: 0.025250, acc.: 99.22%] [G loss: 4.291083]\n",
      "epoch:24 step:19356 [D loss: 0.008215, acc.: 100.00%] [G loss: 3.940664]\n",
      "epoch:24 step:19357 [D loss: 0.959185, acc.: 50.00%] [G loss: 5.679602]\n",
      "epoch:24 step:19358 [D loss: 0.001017, acc.: 100.00%] [G loss: 7.063931]\n",
      "epoch:24 step:19359 [D loss: 0.038506, acc.: 99.22%] [G loss: 6.928600]\n",
      "epoch:24 step:19360 [D loss: 0.015131, acc.: 100.00%] [G loss: 5.778329]\n",
      "epoch:24 step:19361 [D loss: 0.016622, acc.: 100.00%] [G loss: 6.381721]\n",
      "epoch:24 step:19362 [D loss: 0.024368, acc.: 100.00%] [G loss: 5.718577]\n",
      "epoch:24 step:19363 [D loss: 0.010231, acc.: 100.00%] [G loss: 5.469624]\n",
      "epoch:24 step:19364 [D loss: 0.006899, acc.: 100.00%] [G loss: 4.534731]\n",
      "epoch:24 step:19365 [D loss: 0.030291, acc.: 98.44%] [G loss: 4.863281]\n",
      "epoch:24 step:19366 [D loss: 0.004770, acc.: 100.00%] [G loss: 5.133270]\n",
      "epoch:24 step:19367 [D loss: 0.039592, acc.: 99.22%] [G loss: 4.943735]\n",
      "epoch:24 step:19368 [D loss: 0.036739, acc.: 100.00%] [G loss: 4.112669]\n",
      "epoch:24 step:19369 [D loss: 0.010073, acc.: 100.00%] [G loss: 5.250822]\n",
      "epoch:24 step:19370 [D loss: 0.010706, acc.: 100.00%] [G loss: 5.195233]\n",
      "epoch:24 step:19371 [D loss: 0.182753, acc.: 96.09%] [G loss: 2.841998]\n",
      "epoch:24 step:19372 [D loss: 0.040536, acc.: 99.22%] [G loss: 4.535518]\n",
      "epoch:24 step:19373 [D loss: 0.011315, acc.: 100.00%] [G loss: 4.584372]\n",
      "epoch:24 step:19374 [D loss: 0.006108, acc.: 100.00%] [G loss: 4.951617]\n",
      "epoch:24 step:19375 [D loss: 0.063320, acc.: 99.22%] [G loss: 3.306195]\n",
      "epoch:24 step:19376 [D loss: 0.033086, acc.: 100.00%] [G loss: 3.844865]\n",
      "epoch:24 step:19377 [D loss: 0.010345, acc.: 100.00%] [G loss: 4.483759]\n",
      "epoch:24 step:19378 [D loss: 0.026948, acc.: 100.00%] [G loss: 4.586116]\n",
      "epoch:24 step:19379 [D loss: 0.050113, acc.: 99.22%] [G loss: 3.770909]\n",
      "epoch:24 step:19380 [D loss: 0.057119, acc.: 99.22%] [G loss: 3.091627]\n",
      "epoch:24 step:19381 [D loss: 0.039587, acc.: 99.22%] [G loss: 3.957731]\n",
      "epoch:24 step:19382 [D loss: 0.021945, acc.: 100.00%] [G loss: 4.406674]\n",
      "epoch:24 step:19383 [D loss: 0.054821, acc.: 99.22%] [G loss: 4.296711]\n",
      "epoch:24 step:19384 [D loss: 0.013652, acc.: 100.00%] [G loss: 5.179224]\n",
      "epoch:24 step:19385 [D loss: 0.010616, acc.: 100.00%] [G loss: 5.226281]\n",
      "epoch:24 step:19386 [D loss: 1.071322, acc.: 46.09%] [G loss: 6.606271]\n",
      "epoch:24 step:19387 [D loss: 1.278316, acc.: 59.38%] [G loss: 3.877843]\n",
      "epoch:24 step:19388 [D loss: 0.215362, acc.: 91.41%] [G loss: 4.357898]\n",
      "epoch:24 step:19389 [D loss: 0.117147, acc.: 96.88%] [G loss: 4.641408]\n",
      "epoch:24 step:19390 [D loss: 0.121461, acc.: 96.88%] [G loss: 4.017620]\n",
      "epoch:24 step:19391 [D loss: 0.059399, acc.: 98.44%] [G loss: 3.233342]\n",
      "epoch:24 step:19392 [D loss: 0.038033, acc.: 98.44%] [G loss: 3.878687]\n",
      "epoch:24 step:19393 [D loss: 0.201994, acc.: 93.75%] [G loss: 4.008909]\n",
      "epoch:24 step:19394 [D loss: 0.035727, acc.: 100.00%] [G loss: 4.118937]\n",
      "epoch:24 step:19395 [D loss: 0.041606, acc.: 100.00%] [G loss: 3.872204]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:24 step:19396 [D loss: 0.068550, acc.: 99.22%] [G loss: 3.634641]\n",
      "epoch:24 step:19397 [D loss: 0.138392, acc.: 96.09%] [G loss: 4.334143]\n",
      "epoch:24 step:19398 [D loss: 0.247880, acc.: 90.62%] [G loss: 4.042413]\n",
      "epoch:24 step:19399 [D loss: 0.028744, acc.: 98.44%] [G loss: 4.598482]\n",
      "epoch:24 step:19400 [D loss: 0.012203, acc.: 100.00%] [G loss: 4.508724]\n",
      "##############\n",
      "[0.98541691 1.10704909 1.10645974 0.89159551 2.09985146 0.96885304\n",
      " 1.10303215 0.79430475 2.10662739 1.105681  ]\n",
      "##########\n",
      "epoch:24 step:19401 [D loss: 0.021293, acc.: 100.00%] [G loss: 3.390923]\n",
      "epoch:24 step:19402 [D loss: 0.148855, acc.: 96.88%] [G loss: 3.974837]\n",
      "epoch:24 step:19403 [D loss: 0.053785, acc.: 100.00%] [G loss: 5.003552]\n",
      "epoch:24 step:19404 [D loss: 0.048800, acc.: 98.44%] [G loss: 4.197192]\n",
      "epoch:24 step:19405 [D loss: 0.088438, acc.: 97.66%] [G loss: 3.214656]\n",
      "epoch:24 step:19406 [D loss: 0.193163, acc.: 90.62%] [G loss: 6.076622]\n",
      "epoch:24 step:19407 [D loss: 0.033435, acc.: 100.00%] [G loss: 6.688438]\n",
      "epoch:24 step:19408 [D loss: 0.127268, acc.: 95.31%] [G loss: 4.294798]\n",
      "epoch:24 step:19409 [D loss: 0.023015, acc.: 100.00%] [G loss: 3.991491]\n",
      "epoch:24 step:19410 [D loss: 0.013335, acc.: 100.00%] [G loss: 3.319874]\n",
      "epoch:24 step:19411 [D loss: 0.021151, acc.: 100.00%] [G loss: 3.636042]\n",
      "epoch:24 step:19412 [D loss: 0.039398, acc.: 100.00%] [G loss: 4.222097]\n",
      "epoch:24 step:19413 [D loss: 0.009789, acc.: 100.00%] [G loss: 4.366078]\n",
      "epoch:24 step:19414 [D loss: 0.011929, acc.: 100.00%] [G loss: 4.605638]\n",
      "epoch:24 step:19415 [D loss: 0.012138, acc.: 100.00%] [G loss: 3.641277]\n",
      "epoch:24 step:19416 [D loss: 0.029158, acc.: 99.22%] [G loss: 4.016377]\n",
      "epoch:24 step:19417 [D loss: 0.121071, acc.: 96.88%] [G loss: 5.653237]\n",
      "epoch:24 step:19418 [D loss: 0.256724, acc.: 90.62%] [G loss: 2.941917]\n",
      "epoch:24 step:19419 [D loss: 0.146944, acc.: 94.53%] [G loss: 3.830394]\n",
      "epoch:24 step:19420 [D loss: 0.005462, acc.: 100.00%] [G loss: 5.676733]\n",
      "epoch:24 step:19421 [D loss: 0.045473, acc.: 99.22%] [G loss: 5.645627]\n",
      "epoch:24 step:19422 [D loss: 0.023050, acc.: 99.22%] [G loss: 4.622106]\n",
      "epoch:24 step:19423 [D loss: 0.006998, acc.: 100.00%] [G loss: 5.104869]\n",
      "epoch:24 step:19424 [D loss: 0.013427, acc.: 100.00%] [G loss: 5.071057]\n",
      "epoch:24 step:19425 [D loss: 0.009071, acc.: 100.00%] [G loss: 4.622171]\n",
      "epoch:24 step:19426 [D loss: 0.014484, acc.: 100.00%] [G loss: 4.500555]\n",
      "epoch:24 step:19427 [D loss: 0.046156, acc.: 100.00%] [G loss: 3.500671]\n",
      "epoch:24 step:19428 [D loss: 0.048651, acc.: 100.00%] [G loss: 4.789268]\n",
      "epoch:24 step:19429 [D loss: 0.057895, acc.: 99.22%] [G loss: 4.165725]\n",
      "epoch:24 step:19430 [D loss: 0.029010, acc.: 100.00%] [G loss: 3.935356]\n",
      "epoch:24 step:19431 [D loss: 0.118174, acc.: 97.66%] [G loss: 4.319880]\n",
      "epoch:24 step:19432 [D loss: 0.018644, acc.: 100.00%] [G loss: 5.635929]\n",
      "epoch:24 step:19433 [D loss: 0.009721, acc.: 100.00%] [G loss: 4.249413]\n",
      "epoch:24 step:19434 [D loss: 0.055914, acc.: 98.44%] [G loss: 3.171191]\n",
      "epoch:24 step:19435 [D loss: 0.140114, acc.: 96.09%] [G loss: 6.508350]\n",
      "epoch:24 step:19436 [D loss: 0.130575, acc.: 94.53%] [G loss: 4.932840]\n",
      "epoch:24 step:19437 [D loss: 0.027893, acc.: 100.00%] [G loss: 3.158999]\n",
      "epoch:24 step:19438 [D loss: 0.014487, acc.: 100.00%] [G loss: 4.648319]\n",
      "epoch:24 step:19439 [D loss: 0.005228, acc.: 100.00%] [G loss: 4.706392]\n",
      "epoch:24 step:19440 [D loss: 0.007010, acc.: 100.00%] [G loss: 5.104225]\n",
      "epoch:24 step:19441 [D loss: 0.009622, acc.: 100.00%] [G loss: 4.218732]\n",
      "epoch:24 step:19442 [D loss: 0.032193, acc.: 99.22%] [G loss: 4.612000]\n",
      "epoch:24 step:19443 [D loss: 0.049973, acc.: 99.22%] [G loss: 4.149301]\n",
      "epoch:24 step:19444 [D loss: 0.028904, acc.: 99.22%] [G loss: 5.056155]\n",
      "epoch:24 step:19445 [D loss: 0.042105, acc.: 100.00%] [G loss: 6.020732]\n",
      "epoch:24 step:19446 [D loss: 0.014982, acc.: 100.00%] [G loss: 5.971165]\n",
      "epoch:24 step:19447 [D loss: 0.020172, acc.: 100.00%] [G loss: 4.313395]\n",
      "epoch:24 step:19448 [D loss: 0.043773, acc.: 100.00%] [G loss: 4.288311]\n",
      "epoch:24 step:19449 [D loss: 0.014035, acc.: 100.00%] [G loss: 4.449129]\n",
      "epoch:24 step:19450 [D loss: 0.004010, acc.: 100.00%] [G loss: 5.426523]\n",
      "epoch:24 step:19451 [D loss: 0.013277, acc.: 100.00%] [G loss: 4.874835]\n",
      "epoch:24 step:19452 [D loss: 0.028815, acc.: 100.00%] [G loss: 5.866798]\n",
      "epoch:24 step:19453 [D loss: 0.007978, acc.: 100.00%] [G loss: 5.992326]\n",
      "epoch:24 step:19454 [D loss: 0.651777, acc.: 63.28%] [G loss: 8.818313]\n",
      "epoch:24 step:19455 [D loss: 0.893969, acc.: 60.94%] [G loss: 3.878644]\n",
      "epoch:24 step:19456 [D loss: 0.082990, acc.: 96.88%] [G loss: 4.829802]\n",
      "epoch:24 step:19457 [D loss: 0.027762, acc.: 99.22%] [G loss: 5.323048]\n",
      "epoch:24 step:19458 [D loss: 0.013790, acc.: 100.00%] [G loss: 5.371877]\n",
      "epoch:24 step:19459 [D loss: 0.482810, acc.: 78.91%] [G loss: 8.029914]\n",
      "epoch:24 step:19460 [D loss: 0.018570, acc.: 100.00%] [G loss: 9.419486]\n",
      "epoch:24 step:19461 [D loss: 0.481896, acc.: 77.34%] [G loss: 3.648926]\n",
      "epoch:24 step:19462 [D loss: 0.182325, acc.: 91.41%] [G loss: 5.229648]\n",
      "epoch:24 step:19463 [D loss: 0.052258, acc.: 99.22%] [G loss: 7.448817]\n",
      "epoch:24 step:19464 [D loss: 0.020397, acc.: 99.22%] [G loss: 7.735043]\n",
      "epoch:24 step:19465 [D loss: 0.013597, acc.: 100.00%] [G loss: 7.140364]\n",
      "epoch:24 step:19466 [D loss: 0.021226, acc.: 100.00%] [G loss: 3.669787]\n",
      "epoch:24 step:19467 [D loss: 0.017939, acc.: 100.00%] [G loss: 5.197193]\n",
      "epoch:24 step:19468 [D loss: 0.079464, acc.: 97.66%] [G loss: 5.598848]\n",
      "epoch:24 step:19469 [D loss: 2.085556, acc.: 23.44%] [G loss: 8.316210]\n",
      "epoch:24 step:19470 [D loss: 0.718249, acc.: 65.62%] [G loss: 5.229630]\n",
      "epoch:24 step:19471 [D loss: 0.120317, acc.: 95.31%] [G loss: 2.664790]\n",
      "epoch:24 step:19472 [D loss: 0.183910, acc.: 91.41%] [G loss: 3.464185]\n",
      "epoch:24 step:19473 [D loss: 0.085392, acc.: 97.66%] [G loss: 4.070335]\n",
      "epoch:24 step:19474 [D loss: 0.255693, acc.: 92.97%] [G loss: 4.138714]\n",
      "epoch:24 step:19475 [D loss: 0.104831, acc.: 97.66%] [G loss: 5.196040]\n",
      "epoch:24 step:19476 [D loss: 0.267980, acc.: 91.41%] [G loss: 4.292589]\n",
      "epoch:24 step:19477 [D loss: 0.113396, acc.: 97.66%] [G loss: 3.485506]\n",
      "epoch:24 step:19478 [D loss: 0.246508, acc.: 91.41%] [G loss: 6.080889]\n",
      "epoch:24 step:19479 [D loss: 0.079315, acc.: 96.88%] [G loss: 6.422225]\n",
      "epoch:24 step:19480 [D loss: 0.170616, acc.: 91.41%] [G loss: 2.807979]\n",
      "epoch:24 step:19481 [D loss: 0.065030, acc.: 100.00%] [G loss: 4.486559]\n",
      "epoch:24 step:19482 [D loss: 0.048413, acc.: 100.00%] [G loss: 4.557628]\n",
      "epoch:24 step:19483 [D loss: 0.029681, acc.: 100.00%] [G loss: 4.809256]\n",
      "epoch:24 step:19484 [D loss: 0.610599, acc.: 71.88%] [G loss: 7.014359]\n",
      "epoch:24 step:19485 [D loss: 0.829627, acc.: 64.84%] [G loss: 2.895519]\n",
      "epoch:24 step:19486 [D loss: 0.086675, acc.: 98.44%] [G loss: 4.249182]\n",
      "epoch:24 step:19487 [D loss: 0.096763, acc.: 97.66%] [G loss: 3.472853]\n",
      "epoch:24 step:19488 [D loss: 0.023441, acc.: 100.00%] [G loss: 3.631375]\n",
      "epoch:24 step:19489 [D loss: 0.071353, acc.: 97.66%] [G loss: 3.736427]\n",
      "epoch:24 step:19490 [D loss: 0.036757, acc.: 100.00%] [G loss: 3.997583]\n",
      "epoch:24 step:19491 [D loss: 0.023490, acc.: 100.00%] [G loss: 3.148623]\n",
      "epoch:24 step:19492 [D loss: 0.045089, acc.: 99.22%] [G loss: 4.029365]\n",
      "epoch:24 step:19493 [D loss: 0.067973, acc.: 99.22%] [G loss: 2.900198]\n",
      "epoch:24 step:19494 [D loss: 0.056190, acc.: 100.00%] [G loss: 2.025006]\n",
      "epoch:24 step:19495 [D loss: 0.179155, acc.: 94.53%] [G loss: 5.112641]\n",
      "epoch:24 step:19496 [D loss: 0.209957, acc.: 91.41%] [G loss: 3.710727]\n",
      "epoch:24 step:19497 [D loss: 0.057288, acc.: 99.22%] [G loss: 3.687595]\n",
      "epoch:24 step:19498 [D loss: 0.075755, acc.: 98.44%] [G loss: 4.543010]\n",
      "epoch:24 step:19499 [D loss: 0.023212, acc.: 98.44%] [G loss: 3.845632]\n",
      "epoch:24 step:19500 [D loss: 0.046260, acc.: 99.22%] [G loss: 4.166276]\n",
      "epoch:24 step:19501 [D loss: 0.018719, acc.: 99.22%] [G loss: 3.405974]\n",
      "epoch:24 step:19502 [D loss: 0.025190, acc.: 100.00%] [G loss: 4.416512]\n",
      "epoch:24 step:19503 [D loss: 0.027879, acc.: 99.22%] [G loss: 4.649719]\n",
      "epoch:24 step:19504 [D loss: 0.044281, acc.: 99.22%] [G loss: 3.050462]\n",
      "epoch:24 step:19505 [D loss: 0.008796, acc.: 100.00%] [G loss: 4.307208]\n",
      "epoch:24 step:19506 [D loss: 0.481497, acc.: 75.00%] [G loss: 7.613519]\n",
      "epoch:24 step:19507 [D loss: 0.401973, acc.: 77.34%] [G loss: 6.115605]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:24 step:19508 [D loss: 0.002882, acc.: 100.00%] [G loss: 5.328607]\n",
      "epoch:24 step:19509 [D loss: 0.019534, acc.: 100.00%] [G loss: 5.394446]\n",
      "epoch:24 step:19510 [D loss: 0.006335, acc.: 100.00%] [G loss: 4.919032]\n",
      "epoch:24 step:19511 [D loss: 0.012391, acc.: 100.00%] [G loss: 3.511659]\n",
      "epoch:24 step:19512 [D loss: 0.082212, acc.: 98.44%] [G loss: 5.033588]\n",
      "epoch:24 step:19513 [D loss: 0.005054, acc.: 100.00%] [G loss: 4.546418]\n",
      "epoch:24 step:19514 [D loss: 0.022531, acc.: 100.00%] [G loss: 4.242754]\n",
      "epoch:24 step:19515 [D loss: 0.184096, acc.: 93.75%] [G loss: 6.632717]\n",
      "epoch:24 step:19516 [D loss: 0.003374, acc.: 100.00%] [G loss: 7.368670]\n",
      "epoch:24 step:19517 [D loss: 0.012228, acc.: 100.00%] [G loss: 7.143093]\n",
      "epoch:24 step:19518 [D loss: 0.026211, acc.: 99.22%] [G loss: 5.393442]\n",
      "epoch:24 step:19519 [D loss: 0.005994, acc.: 100.00%] [G loss: 4.836001]\n",
      "epoch:24 step:19520 [D loss: 0.033173, acc.: 100.00%] [G loss: 4.618541]\n",
      "epoch:24 step:19521 [D loss: 0.126687, acc.: 95.31%] [G loss: 4.370724]\n",
      "epoch:24 step:19522 [D loss: 0.002040, acc.: 100.00%] [G loss: 6.207703]\n",
      "epoch:24 step:19523 [D loss: 0.127405, acc.: 96.88%] [G loss: 5.007395]\n",
      "epoch:24 step:19524 [D loss: 0.022778, acc.: 100.00%] [G loss: 4.436502]\n",
      "epoch:24 step:19525 [D loss: 0.008782, acc.: 100.00%] [G loss: 4.006408]\n",
      "epoch:25 step:19526 [D loss: 0.003397, acc.: 100.00%] [G loss: 3.411711]\n",
      "epoch:25 step:19527 [D loss: 0.081323, acc.: 99.22%] [G loss: 4.080514]\n",
      "epoch:25 step:19528 [D loss: 0.005524, acc.: 100.00%] [G loss: 4.777067]\n",
      "epoch:25 step:19529 [D loss: 0.031068, acc.: 100.00%] [G loss: 5.175792]\n",
      "epoch:25 step:19530 [D loss: 0.005923, acc.: 100.00%] [G loss: 5.038851]\n",
      "epoch:25 step:19531 [D loss: 0.016229, acc.: 100.00%] [G loss: 5.713463]\n",
      "epoch:25 step:19532 [D loss: 0.013027, acc.: 100.00%] [G loss: 4.496148]\n",
      "epoch:25 step:19533 [D loss: 0.020466, acc.: 100.00%] [G loss: 3.826351]\n",
      "epoch:25 step:19534 [D loss: 0.067405, acc.: 98.44%] [G loss: 3.919139]\n",
      "epoch:25 step:19535 [D loss: 0.057053, acc.: 98.44%] [G loss: 5.481074]\n",
      "epoch:25 step:19536 [D loss: 0.005191, acc.: 100.00%] [G loss: 5.597457]\n",
      "epoch:25 step:19537 [D loss: 0.213096, acc.: 92.97%] [G loss: 2.786944]\n",
      "epoch:25 step:19538 [D loss: 0.032707, acc.: 98.44%] [G loss: 4.843258]\n",
      "epoch:25 step:19539 [D loss: 0.012565, acc.: 100.00%] [G loss: 6.098991]\n",
      "epoch:25 step:19540 [D loss: 0.170276, acc.: 96.09%] [G loss: 6.805745]\n",
      "epoch:25 step:19541 [D loss: 0.007876, acc.: 100.00%] [G loss: 6.936181]\n",
      "epoch:25 step:19542 [D loss: 0.017591, acc.: 100.00%] [G loss: 6.923783]\n",
      "epoch:25 step:19543 [D loss: 0.010406, acc.: 100.00%] [G loss: 5.860502]\n",
      "epoch:25 step:19544 [D loss: 0.006692, acc.: 100.00%] [G loss: 5.808250]\n",
      "epoch:25 step:19545 [D loss: 0.002719, acc.: 100.00%] [G loss: 5.383676]\n",
      "epoch:25 step:19546 [D loss: 0.009981, acc.: 100.00%] [G loss: 5.943921]\n",
      "epoch:25 step:19547 [D loss: 0.028809, acc.: 99.22%] [G loss: 3.793474]\n",
      "epoch:25 step:19548 [D loss: 0.025534, acc.: 100.00%] [G loss: 5.008240]\n",
      "epoch:25 step:19549 [D loss: 0.007577, acc.: 100.00%] [G loss: 5.118327]\n",
      "epoch:25 step:19550 [D loss: 0.061959, acc.: 98.44%] [G loss: 4.597655]\n",
      "epoch:25 step:19551 [D loss: 0.028564, acc.: 100.00%] [G loss: 4.177829]\n",
      "epoch:25 step:19552 [D loss: 0.023644, acc.: 100.00%] [G loss: 4.200736]\n",
      "epoch:25 step:19553 [D loss: 0.103704, acc.: 96.88%] [G loss: 5.954935]\n",
      "epoch:25 step:19554 [D loss: 0.011225, acc.: 100.00%] [G loss: 7.063302]\n",
      "epoch:25 step:19555 [D loss: 0.116427, acc.: 96.09%] [G loss: 2.317602]\n",
      "epoch:25 step:19556 [D loss: 0.306599, acc.: 85.94%] [G loss: 8.958128]\n",
      "epoch:25 step:19557 [D loss: 0.729348, acc.: 67.97%] [G loss: 3.019673]\n",
      "epoch:25 step:19558 [D loss: 0.395466, acc.: 84.38%] [G loss: 8.175316]\n",
      "epoch:25 step:19559 [D loss: 0.091363, acc.: 97.66%] [G loss: 8.658878]\n",
      "epoch:25 step:19560 [D loss: 0.177958, acc.: 93.75%] [G loss: 5.577296]\n",
      "epoch:25 step:19561 [D loss: 0.067357, acc.: 99.22%] [G loss: 5.696192]\n",
      "epoch:25 step:19562 [D loss: 0.004040, acc.: 100.00%] [G loss: 5.641212]\n",
      "epoch:25 step:19563 [D loss: 0.039309, acc.: 98.44%] [G loss: 7.144078]\n",
      "epoch:25 step:19564 [D loss: 0.001478, acc.: 100.00%] [G loss: 6.459320]\n",
      "epoch:25 step:19565 [D loss: 0.004758, acc.: 100.00%] [G loss: 5.243415]\n",
      "epoch:25 step:19566 [D loss: 0.009036, acc.: 100.00%] [G loss: 5.872858]\n",
      "epoch:25 step:19567 [D loss: 0.006777, acc.: 100.00%] [G loss: 6.371316]\n",
      "epoch:25 step:19568 [D loss: 0.005988, acc.: 100.00%] [G loss: 4.834634]\n",
      "epoch:25 step:19569 [D loss: 0.289853, acc.: 85.94%] [G loss: 8.355551]\n",
      "epoch:25 step:19570 [D loss: 1.276947, acc.: 51.56%] [G loss: 3.167187]\n",
      "epoch:25 step:19571 [D loss: 0.028996, acc.: 100.00%] [G loss: 5.001708]\n",
      "epoch:25 step:19572 [D loss: 0.013390, acc.: 100.00%] [G loss: 5.132438]\n",
      "epoch:25 step:19573 [D loss: 0.020736, acc.: 100.00%] [G loss: 4.035637]\n",
      "epoch:25 step:19574 [D loss: 0.048903, acc.: 99.22%] [G loss: 3.770116]\n",
      "epoch:25 step:19575 [D loss: 0.177387, acc.: 92.97%] [G loss: 5.643017]\n",
      "epoch:25 step:19576 [D loss: 0.055484, acc.: 97.66%] [G loss: 5.098640]\n",
      "epoch:25 step:19577 [D loss: 0.043310, acc.: 98.44%] [G loss: 4.161451]\n",
      "epoch:25 step:19578 [D loss: 0.028427, acc.: 100.00%] [G loss: 3.851920]\n",
      "epoch:25 step:19579 [D loss: 0.057421, acc.: 99.22%] [G loss: 3.393278]\n",
      "epoch:25 step:19580 [D loss: 0.026701, acc.: 100.00%] [G loss: 3.604170]\n",
      "epoch:25 step:19581 [D loss: 0.022177, acc.: 100.00%] [G loss: 3.643553]\n",
      "epoch:25 step:19582 [D loss: 0.052633, acc.: 99.22%] [G loss: 3.222208]\n",
      "epoch:25 step:19583 [D loss: 0.181854, acc.: 96.88%] [G loss: 5.908495]\n",
      "epoch:25 step:19584 [D loss: 0.055901, acc.: 97.66%] [G loss: 5.403524]\n",
      "epoch:25 step:19585 [D loss: 0.033726, acc.: 99.22%] [G loss: 5.255783]\n",
      "epoch:25 step:19586 [D loss: 0.077876, acc.: 97.66%] [G loss: 3.154996]\n",
      "epoch:25 step:19587 [D loss: 0.115901, acc.: 97.66%] [G loss: 5.643970]\n",
      "epoch:25 step:19588 [D loss: 0.052355, acc.: 97.66%] [G loss: 5.964537]\n",
      "epoch:25 step:19589 [D loss: 0.041671, acc.: 99.22%] [G loss: 4.075725]\n",
      "epoch:25 step:19590 [D loss: 0.285874, acc.: 86.72%] [G loss: 6.499471]\n",
      "epoch:25 step:19591 [D loss: 0.014018, acc.: 100.00%] [G loss: 7.639889]\n",
      "epoch:25 step:19592 [D loss: 0.013713, acc.: 99.22%] [G loss: 7.529665]\n",
      "epoch:25 step:19593 [D loss: 0.011351, acc.: 100.00%] [G loss: 6.646280]\n",
      "epoch:25 step:19594 [D loss: 0.011097, acc.: 100.00%] [G loss: 6.247912]\n",
      "epoch:25 step:19595 [D loss: 0.002724, acc.: 100.00%] [G loss: 6.103933]\n",
      "epoch:25 step:19596 [D loss: 0.030800, acc.: 100.00%] [G loss: 4.763594]\n",
      "epoch:25 step:19597 [D loss: 0.004680, acc.: 100.00%] [G loss: 5.255692]\n",
      "epoch:25 step:19598 [D loss: 0.016467, acc.: 100.00%] [G loss: 4.740563]\n",
      "epoch:25 step:19599 [D loss: 0.019011, acc.: 100.00%] [G loss: 4.021700]\n",
      "epoch:25 step:19600 [D loss: 0.017938, acc.: 100.00%] [G loss: 4.037767]\n",
      "##############\n",
      "[0.93604616 0.82718487 1.00235019 0.95286531 2.12915765 0.98493633\n",
      " 2.11533364 1.024481   2.10889484 1.11054434]\n",
      "##########\n",
      "epoch:25 step:19601 [D loss: 0.040634, acc.: 99.22%] [G loss: 4.368231]\n",
      "epoch:25 step:19602 [D loss: 0.482454, acc.: 79.69%] [G loss: 7.663457]\n",
      "epoch:25 step:19603 [D loss: 0.169826, acc.: 92.19%] [G loss: 7.451633]\n",
      "epoch:25 step:19604 [D loss: 0.053752, acc.: 98.44%] [G loss: 5.960295]\n",
      "epoch:25 step:19605 [D loss: 0.045140, acc.: 99.22%] [G loss: 5.417701]\n",
      "epoch:25 step:19606 [D loss: 0.002391, acc.: 100.00%] [G loss: 6.602067]\n",
      "epoch:25 step:19607 [D loss: 0.004888, acc.: 100.00%] [G loss: 6.222692]\n",
      "epoch:25 step:19608 [D loss: 0.002792, acc.: 100.00%] [G loss: 5.841739]\n",
      "epoch:25 step:19609 [D loss: 0.020138, acc.: 100.00%] [G loss: 5.330291]\n",
      "epoch:25 step:19610 [D loss: 0.121920, acc.: 96.88%] [G loss: 3.158107]\n",
      "epoch:25 step:19611 [D loss: 0.238330, acc.: 87.50%] [G loss: 8.878154]\n",
      "epoch:25 step:19612 [D loss: 0.736911, acc.: 67.97%] [G loss: 2.522615]\n",
      "epoch:25 step:19613 [D loss: 0.460226, acc.: 78.91%] [G loss: 8.860491]\n",
      "epoch:25 step:19614 [D loss: 2.326996, acc.: 50.78%] [G loss: 3.039207]\n",
      "epoch:25 step:19615 [D loss: 0.962464, acc.: 61.72%] [G loss: 6.757725]\n",
      "epoch:25 step:19616 [D loss: 1.069664, acc.: 58.59%] [G loss: 4.190084]\n",
      "epoch:25 step:19617 [D loss: 0.139459, acc.: 96.88%] [G loss: 5.152495]\n",
      "epoch:25 step:19618 [D loss: 0.190457, acc.: 91.41%] [G loss: 2.767085]\n",
      "epoch:25 step:19619 [D loss: 0.375672, acc.: 80.47%] [G loss: 4.954704]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:25 step:19620 [D loss: 0.015367, acc.: 100.00%] [G loss: 5.912149]\n",
      "epoch:25 step:19621 [D loss: 0.751127, acc.: 69.53%] [G loss: 2.450681]\n",
      "epoch:25 step:19622 [D loss: 0.418540, acc.: 80.47%] [G loss: 5.158771]\n",
      "epoch:25 step:19623 [D loss: 0.077704, acc.: 97.66%] [G loss: 5.407792]\n",
      "epoch:25 step:19624 [D loss: 0.335994, acc.: 85.16%] [G loss: 3.249340]\n",
      "epoch:25 step:19625 [D loss: 0.209256, acc.: 92.97%] [G loss: 4.130852]\n",
      "epoch:25 step:19626 [D loss: 0.078234, acc.: 97.66%] [G loss: 4.598493]\n",
      "epoch:25 step:19627 [D loss: 0.075457, acc.: 97.66%] [G loss: 3.695651]\n",
      "epoch:25 step:19628 [D loss: 0.154492, acc.: 92.97%] [G loss: 3.028542]\n",
      "epoch:25 step:19629 [D loss: 0.053711, acc.: 100.00%] [G loss: 2.408078]\n",
      "epoch:25 step:19630 [D loss: 0.155756, acc.: 95.31%] [G loss: 3.806614]\n",
      "epoch:25 step:19631 [D loss: 0.013770, acc.: 100.00%] [G loss: 3.762945]\n",
      "epoch:25 step:19632 [D loss: 0.091681, acc.: 97.66%] [G loss: 3.516560]\n",
      "epoch:25 step:19633 [D loss: 0.801645, acc.: 57.03%] [G loss: 2.180892]\n",
      "epoch:25 step:19634 [D loss: 0.023389, acc.: 100.00%] [G loss: 4.439280]\n",
      "epoch:25 step:19635 [D loss: 0.115785, acc.: 96.09%] [G loss: 4.180037]\n",
      "epoch:25 step:19636 [D loss: 0.271052, acc.: 90.62%] [G loss: 4.503173]\n",
      "epoch:25 step:19637 [D loss: 0.014288, acc.: 100.00%] [G loss: 5.252698]\n",
      "epoch:25 step:19638 [D loss: 0.053961, acc.: 98.44%] [G loss: 4.835499]\n",
      "epoch:25 step:19639 [D loss: 0.096922, acc.: 96.09%] [G loss: 3.447996]\n",
      "epoch:25 step:19640 [D loss: 0.050609, acc.: 99.22%] [G loss: 3.242144]\n",
      "epoch:25 step:19641 [D loss: 0.073668, acc.: 98.44%] [G loss: 4.059500]\n",
      "epoch:25 step:19642 [D loss: 0.032622, acc.: 100.00%] [G loss: 4.980054]\n",
      "epoch:25 step:19643 [D loss: 0.019559, acc.: 100.00%] [G loss: 4.660641]\n",
      "epoch:25 step:19644 [D loss: 0.017046, acc.: 100.00%] [G loss: 3.832949]\n",
      "epoch:25 step:19645 [D loss: 0.035559, acc.: 99.22%] [G loss: 3.710771]\n",
      "epoch:25 step:19646 [D loss: 0.023489, acc.: 100.00%] [G loss: 2.979911]\n",
      "epoch:25 step:19647 [D loss: 0.079366, acc.: 100.00%] [G loss: 4.548607]\n",
      "epoch:25 step:19648 [D loss: 0.121270, acc.: 95.31%] [G loss: 4.387013]\n",
      "epoch:25 step:19649 [D loss: 0.021342, acc.: 100.00%] [G loss: 4.587353]\n",
      "epoch:25 step:19650 [D loss: 0.014326, acc.: 100.00%] [G loss: 4.426774]\n",
      "epoch:25 step:19651 [D loss: 0.065209, acc.: 98.44%] [G loss: 4.754814]\n",
      "epoch:25 step:19652 [D loss: 0.006018, acc.: 100.00%] [G loss: 5.403291]\n",
      "epoch:25 step:19653 [D loss: 0.044638, acc.: 99.22%] [G loss: 4.317052]\n",
      "epoch:25 step:19654 [D loss: 0.025707, acc.: 99.22%] [G loss: 4.223658]\n",
      "epoch:25 step:19655 [D loss: 0.013996, acc.: 100.00%] [G loss: 4.138229]\n",
      "epoch:25 step:19656 [D loss: 0.044324, acc.: 99.22%] [G loss: 4.092520]\n",
      "epoch:25 step:19657 [D loss: 0.141838, acc.: 93.75%] [G loss: 2.915911]\n",
      "epoch:25 step:19658 [D loss: 0.028953, acc.: 100.00%] [G loss: 2.630930]\n",
      "epoch:25 step:19659 [D loss: 0.019246, acc.: 100.00%] [G loss: 3.520755]\n",
      "epoch:25 step:19660 [D loss: 0.027839, acc.: 100.00%] [G loss: 2.505998]\n",
      "epoch:25 step:19661 [D loss: 0.040454, acc.: 100.00%] [G loss: 1.781037]\n",
      "epoch:25 step:19662 [D loss: 0.060420, acc.: 100.00%] [G loss: 3.827209]\n",
      "epoch:25 step:19663 [D loss: 0.005597, acc.: 100.00%] [G loss: 5.323334]\n",
      "epoch:25 step:19664 [D loss: 0.133572, acc.: 96.09%] [G loss: 0.301761]\n",
      "epoch:25 step:19665 [D loss: 0.116790, acc.: 97.66%] [G loss: 4.257379]\n",
      "epoch:25 step:19666 [D loss: 0.003971, acc.: 100.00%] [G loss: 6.098705]\n",
      "epoch:25 step:19667 [D loss: 0.051319, acc.: 99.22%] [G loss: 4.447532]\n",
      "epoch:25 step:19668 [D loss: 0.024349, acc.: 100.00%] [G loss: 3.843909]\n",
      "epoch:25 step:19669 [D loss: 0.068436, acc.: 98.44%] [G loss: 5.253438]\n",
      "epoch:25 step:19670 [D loss: 0.053010, acc.: 97.66%] [G loss: 5.536708]\n",
      "epoch:25 step:19671 [D loss: 0.021989, acc.: 99.22%] [G loss: 4.016154]\n",
      "epoch:25 step:19672 [D loss: 0.015193, acc.: 100.00%] [G loss: 3.669239]\n",
      "epoch:25 step:19673 [D loss: 0.009906, acc.: 100.00%] [G loss: 4.238158]\n",
      "epoch:25 step:19674 [D loss: 0.031944, acc.: 99.22%] [G loss: 5.787522]\n",
      "epoch:25 step:19675 [D loss: 0.038780, acc.: 99.22%] [G loss: 4.596747]\n",
      "epoch:25 step:19676 [D loss: 0.027535, acc.: 99.22%] [G loss: 5.817080]\n",
      "epoch:25 step:19677 [D loss: 0.011988, acc.: 100.00%] [G loss: 4.606937]\n",
      "epoch:25 step:19678 [D loss: 0.111919, acc.: 96.88%] [G loss: 6.565202]\n",
      "epoch:25 step:19679 [D loss: 0.017423, acc.: 100.00%] [G loss: 6.513221]\n",
      "epoch:25 step:19680 [D loss: 0.048275, acc.: 99.22%] [G loss: 4.946518]\n",
      "epoch:25 step:19681 [D loss: 0.060337, acc.: 99.22%] [G loss: 5.167006]\n",
      "epoch:25 step:19682 [D loss: 0.026147, acc.: 98.44%] [G loss: 4.963569]\n",
      "epoch:25 step:19683 [D loss: 0.191777, acc.: 92.97%] [G loss: 3.481873]\n",
      "epoch:25 step:19684 [D loss: 0.018189, acc.: 100.00%] [G loss: 3.869980]\n",
      "epoch:25 step:19685 [D loss: 0.004884, acc.: 100.00%] [G loss: 3.312766]\n",
      "epoch:25 step:19686 [D loss: 0.150485, acc.: 95.31%] [G loss: 4.402074]\n",
      "epoch:25 step:19687 [D loss: 0.001731, acc.: 100.00%] [G loss: 6.769276]\n",
      "epoch:25 step:19688 [D loss: 0.040425, acc.: 98.44%] [G loss: 4.957990]\n",
      "epoch:25 step:19689 [D loss: 0.044386, acc.: 99.22%] [G loss: 4.247841]\n",
      "epoch:25 step:19690 [D loss: 0.012564, acc.: 100.00%] [G loss: 3.537426]\n",
      "epoch:25 step:19691 [D loss: 0.019042, acc.: 100.00%] [G loss: 4.536169]\n",
      "epoch:25 step:19692 [D loss: 0.010033, acc.: 100.00%] [G loss: 4.520548]\n",
      "epoch:25 step:19693 [D loss: 0.005733, acc.: 100.00%] [G loss: 4.373999]\n",
      "epoch:25 step:19694 [D loss: 0.015415, acc.: 100.00%] [G loss: 5.081012]\n",
      "epoch:25 step:19695 [D loss: 0.054407, acc.: 100.00%] [G loss: 3.075922]\n",
      "epoch:25 step:19696 [D loss: 0.050721, acc.: 99.22%] [G loss: 4.075987]\n",
      "epoch:25 step:19697 [D loss: 0.018988, acc.: 99.22%] [G loss: 5.024995]\n",
      "epoch:25 step:19698 [D loss: 0.008822, acc.: 100.00%] [G loss: 4.379223]\n",
      "epoch:25 step:19699 [D loss: 0.034168, acc.: 100.00%] [G loss: 3.829155]\n",
      "epoch:25 step:19700 [D loss: 0.026385, acc.: 100.00%] [G loss: 4.610226]\n",
      "epoch:25 step:19701 [D loss: 3.793287, acc.: 28.12%] [G loss: 9.006416]\n",
      "epoch:25 step:19702 [D loss: 2.787821, acc.: 50.00%] [G loss: 5.418901]\n",
      "epoch:25 step:19703 [D loss: 1.090219, acc.: 53.12%] [G loss: 3.388014]\n",
      "epoch:25 step:19704 [D loss: 0.250080, acc.: 88.28%] [G loss: 3.858605]\n",
      "epoch:25 step:19705 [D loss: 0.058262, acc.: 100.00%] [G loss: 3.433601]\n",
      "epoch:25 step:19706 [D loss: 0.040352, acc.: 100.00%] [G loss: 4.023907]\n",
      "epoch:25 step:19707 [D loss: 0.120837, acc.: 95.31%] [G loss: 3.582452]\n",
      "epoch:25 step:19708 [D loss: 0.063106, acc.: 99.22%] [G loss: 3.771673]\n",
      "epoch:25 step:19709 [D loss: 0.042711, acc.: 99.22%] [G loss: 3.440485]\n",
      "epoch:25 step:19710 [D loss: 0.125801, acc.: 95.31%] [G loss: 3.760573]\n",
      "epoch:25 step:19711 [D loss: 0.075580, acc.: 98.44%] [G loss: 3.107073]\n",
      "epoch:25 step:19712 [D loss: 0.058969, acc.: 98.44%] [G loss: 2.397102]\n",
      "epoch:25 step:19713 [D loss: 0.079276, acc.: 99.22%] [G loss: 2.233305]\n",
      "epoch:25 step:19714 [D loss: 0.028327, acc.: 100.00%] [G loss: 2.504850]\n",
      "epoch:25 step:19715 [D loss: 0.090655, acc.: 98.44%] [G loss: 2.618464]\n",
      "epoch:25 step:19716 [D loss: 0.567207, acc.: 71.88%] [G loss: 6.110703]\n",
      "epoch:25 step:19717 [D loss: 0.406619, acc.: 81.25%] [G loss: 5.482687]\n",
      "epoch:25 step:19718 [D loss: 0.235245, acc.: 90.62%] [G loss: 3.056895]\n",
      "epoch:25 step:19719 [D loss: 0.061581, acc.: 100.00%] [G loss: 2.680454]\n",
      "epoch:25 step:19720 [D loss: 0.032177, acc.: 99.22%] [G loss: 3.731948]\n",
      "epoch:25 step:19721 [D loss: 0.021171, acc.: 100.00%] [G loss: 4.309349]\n",
      "epoch:25 step:19722 [D loss: 0.022173, acc.: 100.00%] [G loss: 4.141568]\n",
      "epoch:25 step:19723 [D loss: 0.019966, acc.: 100.00%] [G loss: 4.113780]\n",
      "epoch:25 step:19724 [D loss: 0.030295, acc.: 100.00%] [G loss: 4.471400]\n",
      "epoch:25 step:19725 [D loss: 0.045830, acc.: 100.00%] [G loss: 4.838070]\n",
      "epoch:25 step:19726 [D loss: 0.022225, acc.: 100.00%] [G loss: 4.252654]\n",
      "epoch:25 step:19727 [D loss: 0.050974, acc.: 99.22%] [G loss: 3.281117]\n",
      "epoch:25 step:19728 [D loss: 0.105660, acc.: 98.44%] [G loss: 4.914454]\n",
      "epoch:25 step:19729 [D loss: 0.024125, acc.: 100.00%] [G loss: 5.378649]\n",
      "epoch:25 step:19730 [D loss: 0.735141, acc.: 59.38%] [G loss: 3.703092]\n",
      "epoch:25 step:19731 [D loss: 0.028481, acc.: 100.00%] [G loss: 4.766233]\n",
      "epoch:25 step:19732 [D loss: 0.023841, acc.: 100.00%] [G loss: 5.018842]\n",
      "epoch:25 step:19733 [D loss: 0.006952, acc.: 100.00%] [G loss: 3.882600]\n",
      "epoch:25 step:19734 [D loss: 0.032655, acc.: 100.00%] [G loss: 4.491011]\n",
      "epoch:25 step:19735 [D loss: 0.031173, acc.: 100.00%] [G loss: 4.108537]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:25 step:19736 [D loss: 0.013061, acc.: 100.00%] [G loss: 4.459270]\n",
      "epoch:25 step:19737 [D loss: 0.048527, acc.: 99.22%] [G loss: 3.036719]\n",
      "epoch:25 step:19738 [D loss: 0.097001, acc.: 97.66%] [G loss: 4.404700]\n",
      "epoch:25 step:19739 [D loss: 0.248886, acc.: 91.41%] [G loss: 2.491572]\n",
      "epoch:25 step:19740 [D loss: 0.024504, acc.: 100.00%] [G loss: 1.998135]\n",
      "epoch:25 step:19741 [D loss: 0.017309, acc.: 100.00%] [G loss: 2.497181]\n",
      "epoch:25 step:19742 [D loss: 0.049160, acc.: 100.00%] [G loss: 3.174251]\n",
      "epoch:25 step:19743 [D loss: 0.123561, acc.: 97.66%] [G loss: 1.457930]\n",
      "epoch:25 step:19744 [D loss: 0.033477, acc.: 100.00%] [G loss: 2.486920]\n",
      "epoch:25 step:19745 [D loss: 0.020116, acc.: 100.00%] [G loss: 3.107666]\n",
      "epoch:25 step:19746 [D loss: 0.035230, acc.: 100.00%] [G loss: 3.659716]\n",
      "epoch:25 step:19747 [D loss: 0.023515, acc.: 99.22%] [G loss: 2.810570]\n",
      "epoch:25 step:19748 [D loss: 0.018347, acc.: 100.00%] [G loss: 3.349753]\n",
      "epoch:25 step:19749 [D loss: 0.061548, acc.: 100.00%] [G loss: 4.182150]\n",
      "epoch:25 step:19750 [D loss: 0.010482, acc.: 100.00%] [G loss: 3.745935]\n",
      "epoch:25 step:19751 [D loss: 0.043927, acc.: 100.00%] [G loss: 3.744829]\n",
      "epoch:25 step:19752 [D loss: 0.023018, acc.: 100.00%] [G loss: 4.106166]\n",
      "epoch:25 step:19753 [D loss: 0.027966, acc.: 100.00%] [G loss: 4.001137]\n",
      "epoch:25 step:19754 [D loss: 0.122114, acc.: 96.88%] [G loss: 4.678283]\n",
      "epoch:25 step:19755 [D loss: 0.020127, acc.: 100.00%] [G loss: 5.611337]\n",
      "epoch:25 step:19756 [D loss: 0.018996, acc.: 100.00%] [G loss: 5.424942]\n",
      "epoch:25 step:19757 [D loss: 0.124300, acc.: 96.09%] [G loss: 3.576504]\n",
      "epoch:25 step:19758 [D loss: 0.058423, acc.: 97.66%] [G loss: 4.744775]\n",
      "epoch:25 step:19759 [D loss: 0.015535, acc.: 100.00%] [G loss: 5.297500]\n",
      "epoch:25 step:19760 [D loss: 0.009036, acc.: 100.00%] [G loss: 5.694502]\n",
      "epoch:25 step:19761 [D loss: 0.038622, acc.: 99.22%] [G loss: 4.702541]\n",
      "epoch:25 step:19762 [D loss: 0.007492, acc.: 100.00%] [G loss: 4.223170]\n",
      "epoch:25 step:19763 [D loss: 0.016776, acc.: 100.00%] [G loss: 4.929632]\n",
      "epoch:25 step:19764 [D loss: 0.003301, acc.: 100.00%] [G loss: 3.672318]\n",
      "epoch:25 step:19765 [D loss: 1.464332, acc.: 42.97%] [G loss: 8.409679]\n",
      "epoch:25 step:19766 [D loss: 2.561009, acc.: 50.00%] [G loss: 7.126794]\n",
      "epoch:25 step:19767 [D loss: 1.750881, acc.: 53.91%] [G loss: 4.651921]\n",
      "epoch:25 step:19768 [D loss: 0.152795, acc.: 96.09%] [G loss: 2.822895]\n",
      "epoch:25 step:19769 [D loss: 0.209443, acc.: 89.06%] [G loss: 2.991817]\n",
      "epoch:25 step:19770 [D loss: 0.011247, acc.: 100.00%] [G loss: 4.478042]\n",
      "epoch:25 step:19771 [D loss: 0.079505, acc.: 96.88%] [G loss: 3.169626]\n",
      "epoch:25 step:19772 [D loss: 0.063261, acc.: 99.22%] [G loss: 2.604911]\n",
      "epoch:25 step:19773 [D loss: 0.090491, acc.: 98.44%] [G loss: 3.360401]\n",
      "epoch:25 step:19774 [D loss: 0.018450, acc.: 100.00%] [G loss: 3.780319]\n",
      "epoch:25 step:19775 [D loss: 0.045582, acc.: 100.00%] [G loss: 2.939499]\n",
      "epoch:25 step:19776 [D loss: 0.034232, acc.: 100.00%] [G loss: 3.491227]\n",
      "epoch:25 step:19777 [D loss: 0.032190, acc.: 100.00%] [G loss: 2.870621]\n",
      "epoch:25 step:19778 [D loss: 0.054675, acc.: 99.22%] [G loss: 1.693696]\n",
      "epoch:25 step:19779 [D loss: 0.089292, acc.: 99.22%] [G loss: 2.277739]\n",
      "epoch:25 step:19780 [D loss: 0.083660, acc.: 100.00%] [G loss: 3.491551]\n",
      "epoch:25 step:19781 [D loss: 0.087085, acc.: 100.00%] [G loss: 3.422248]\n",
      "epoch:25 step:19782 [D loss: 0.091204, acc.: 99.22%] [G loss: 3.050115]\n",
      "epoch:25 step:19783 [D loss: 0.075128, acc.: 98.44%] [G loss: 3.339522]\n",
      "epoch:25 step:19784 [D loss: 0.029713, acc.: 100.00%] [G loss: 3.247653]\n",
      "epoch:25 step:19785 [D loss: 0.094273, acc.: 97.66%] [G loss: 2.397534]\n",
      "epoch:25 step:19786 [D loss: 0.066853, acc.: 100.00%] [G loss: 3.528307]\n",
      "epoch:25 step:19787 [D loss: 0.020879, acc.: 100.00%] [G loss: 2.659056]\n",
      "epoch:25 step:19788 [D loss: 0.075748, acc.: 99.22%] [G loss: 2.736318]\n",
      "epoch:25 step:19789 [D loss: 0.264215, acc.: 90.62%] [G loss: 4.319916]\n",
      "epoch:25 step:19790 [D loss: 0.071326, acc.: 97.66%] [G loss: 4.135263]\n",
      "epoch:25 step:19791 [D loss: 0.070065, acc.: 98.44%] [G loss: 2.378948]\n",
      "epoch:25 step:19792 [D loss: 0.136007, acc.: 96.09%] [G loss: 2.742949]\n",
      "epoch:25 step:19793 [D loss: 0.010858, acc.: 100.00%] [G loss: 3.836824]\n",
      "epoch:25 step:19794 [D loss: 0.019235, acc.: 100.00%] [G loss: 3.799911]\n",
      "epoch:25 step:19795 [D loss: 0.098055, acc.: 98.44%] [G loss: 1.668677]\n",
      "epoch:25 step:19796 [D loss: 0.349266, acc.: 80.47%] [G loss: 6.382463]\n",
      "epoch:25 step:19797 [D loss: 0.765544, acc.: 60.16%] [G loss: 3.618885]\n",
      "epoch:25 step:19798 [D loss: 0.016569, acc.: 100.00%] [G loss: 3.055516]\n",
      "epoch:25 step:19799 [D loss: 0.072621, acc.: 96.88%] [G loss: 3.140500]\n",
      "epoch:25 step:19800 [D loss: 0.036719, acc.: 100.00%] [G loss: 4.120664]\n",
      "##############\n",
      "[0.80181545 1.00658083 1.03833458 0.97221007 2.11038314 0.89256767\n",
      " 2.11297772 2.10526388 1.11090025 1.04138393]\n",
      "##########\n",
      "epoch:25 step:19801 [D loss: 0.010992, acc.: 100.00%] [G loss: 4.472085]\n",
      "epoch:25 step:19802 [D loss: 0.024552, acc.: 100.00%] [G loss: 4.545489]\n",
      "epoch:25 step:19803 [D loss: 0.018700, acc.: 100.00%] [G loss: 4.191387]\n",
      "epoch:25 step:19804 [D loss: 0.039000, acc.: 100.00%] [G loss: 4.249672]\n",
      "epoch:25 step:19805 [D loss: 0.023385, acc.: 99.22%] [G loss: 3.115225]\n",
      "epoch:25 step:19806 [D loss: 0.011902, acc.: 100.00%] [G loss: 2.140854]\n",
      "epoch:25 step:19807 [D loss: 0.056435, acc.: 99.22%] [G loss: 3.007690]\n",
      "epoch:25 step:19808 [D loss: 0.008394, acc.: 100.00%] [G loss: 4.199386]\n",
      "epoch:25 step:19809 [D loss: 0.013591, acc.: 100.00%] [G loss: 4.061656]\n",
      "epoch:25 step:19810 [D loss: 0.029850, acc.: 99.22%] [G loss: 3.000989]\n",
      "epoch:25 step:19811 [D loss: 0.268153, acc.: 85.94%] [G loss: 2.144944]\n",
      "epoch:25 step:19812 [D loss: 0.074842, acc.: 99.22%] [G loss: 3.454057]\n",
      "epoch:25 step:19813 [D loss: 0.007991, acc.: 100.00%] [G loss: 5.022943]\n",
      "epoch:25 step:19814 [D loss: 0.094044, acc.: 98.44%] [G loss: 3.167112]\n",
      "epoch:25 step:19815 [D loss: 0.033080, acc.: 100.00%] [G loss: 3.227427]\n",
      "epoch:25 step:19816 [D loss: 0.040475, acc.: 100.00%] [G loss: 3.974016]\n",
      "epoch:25 step:19817 [D loss: 0.018951, acc.: 100.00%] [G loss: 4.308004]\n",
      "epoch:25 step:19818 [D loss: 0.010482, acc.: 100.00%] [G loss: 4.349836]\n",
      "epoch:25 step:19819 [D loss: 0.029244, acc.: 100.00%] [G loss: 4.245649]\n",
      "epoch:25 step:19820 [D loss: 0.296275, acc.: 90.62%] [G loss: 4.919981]\n",
      "epoch:25 step:19821 [D loss: 0.003706, acc.: 100.00%] [G loss: 6.093834]\n",
      "epoch:25 step:19822 [D loss: 0.002836, acc.: 100.00%] [G loss: 5.918440]\n",
      "epoch:25 step:19823 [D loss: 0.003459, acc.: 100.00%] [G loss: 5.988486]\n",
      "epoch:25 step:19824 [D loss: 0.009517, acc.: 100.00%] [G loss: 5.942554]\n",
      "epoch:25 step:19825 [D loss: 0.004652, acc.: 100.00%] [G loss: 5.517746]\n",
      "epoch:25 step:19826 [D loss: 0.003305, acc.: 100.00%] [G loss: 4.955738]\n",
      "epoch:25 step:19827 [D loss: 0.005997, acc.: 100.00%] [G loss: 5.192188]\n",
      "epoch:25 step:19828 [D loss: 0.012078, acc.: 100.00%] [G loss: 5.038035]\n",
      "epoch:25 step:19829 [D loss: 0.004694, acc.: 100.00%] [G loss: 4.968615]\n",
      "epoch:25 step:19830 [D loss: 0.009745, acc.: 100.00%] [G loss: 5.175912]\n",
      "epoch:25 step:19831 [D loss: 0.008497, acc.: 100.00%] [G loss: 4.689775]\n",
      "epoch:25 step:19832 [D loss: 0.007803, acc.: 100.00%] [G loss: 4.658706]\n",
      "epoch:25 step:19833 [D loss: 0.007721, acc.: 100.00%] [G loss: 4.193511]\n",
      "epoch:25 step:19834 [D loss: 0.034272, acc.: 100.00%] [G loss: 4.101686]\n",
      "epoch:25 step:19835 [D loss: 0.024163, acc.: 100.00%] [G loss: 4.037466]\n",
      "epoch:25 step:19836 [D loss: 0.007929, acc.: 100.00%] [G loss: 3.864536]\n",
      "epoch:25 step:19837 [D loss: 0.073041, acc.: 99.22%] [G loss: 2.707635]\n",
      "epoch:25 step:19838 [D loss: 0.029573, acc.: 100.00%] [G loss: 3.808649]\n",
      "epoch:25 step:19839 [D loss: 0.007396, acc.: 100.00%] [G loss: 3.114678]\n",
      "epoch:25 step:19840 [D loss: 0.087296, acc.: 100.00%] [G loss: 3.611459]\n",
      "epoch:25 step:19841 [D loss: 0.010574, acc.: 100.00%] [G loss: 3.554934]\n",
      "epoch:25 step:19842 [D loss: 0.019588, acc.: 100.00%] [G loss: 4.336017]\n",
      "epoch:25 step:19843 [D loss: 0.008681, acc.: 100.00%] [G loss: 4.265649]\n",
      "epoch:25 step:19844 [D loss: 0.010521, acc.: 100.00%] [G loss: 4.261394]\n",
      "epoch:25 step:19845 [D loss: 0.449540, acc.: 73.44%] [G loss: 7.761847]\n",
      "epoch:25 step:19846 [D loss: 0.888591, acc.: 56.25%] [G loss: 2.811700]\n",
      "epoch:25 step:19847 [D loss: 0.368889, acc.: 82.03%] [G loss: 6.678869]\n",
      "epoch:25 step:19848 [D loss: 0.010403, acc.: 100.00%] [G loss: 7.350542]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:25 step:19849 [D loss: 0.454776, acc.: 75.00%] [G loss: 3.955517]\n",
      "epoch:25 step:19850 [D loss: 0.083369, acc.: 97.66%] [G loss: 2.440701]\n",
      "epoch:25 step:19851 [D loss: 0.032290, acc.: 99.22%] [G loss: 3.177762]\n",
      "epoch:25 step:19852 [D loss: 0.052688, acc.: 98.44%] [G loss: 2.318382]\n",
      "epoch:25 step:19853 [D loss: 0.023475, acc.: 100.00%] [G loss: 5.094330]\n",
      "epoch:25 step:19854 [D loss: 0.147409, acc.: 93.75%] [G loss: 5.919453]\n",
      "epoch:25 step:19855 [D loss: 0.146634, acc.: 92.19%] [G loss: 4.402617]\n",
      "epoch:25 step:19856 [D loss: 0.040425, acc.: 100.00%] [G loss: 2.606195]\n",
      "epoch:25 step:19857 [D loss: 0.023251, acc.: 100.00%] [G loss: 3.978586]\n",
      "epoch:25 step:19858 [D loss: 0.087860, acc.: 97.66%] [G loss: 4.374646]\n",
      "epoch:25 step:19859 [D loss: 0.314749, acc.: 86.72%] [G loss: 3.813776]\n",
      "epoch:25 step:19860 [D loss: 0.006639, acc.: 100.00%] [G loss: 5.515688]\n",
      "epoch:25 step:19861 [D loss: 0.477222, acc.: 77.34%] [G loss: 5.148328]\n",
      "epoch:25 step:19862 [D loss: 0.001427, acc.: 100.00%] [G loss: 7.120893]\n",
      "epoch:25 step:19863 [D loss: 0.194613, acc.: 91.41%] [G loss: 4.224710]\n",
      "epoch:25 step:19864 [D loss: 0.005999, acc.: 100.00%] [G loss: 2.911038]\n",
      "epoch:25 step:19865 [D loss: 0.093155, acc.: 97.66%] [G loss: 3.575260]\n",
      "epoch:25 step:19866 [D loss: 0.004060, acc.: 100.00%] [G loss: 5.223805]\n",
      "epoch:25 step:19867 [D loss: 0.008072, acc.: 100.00%] [G loss: 4.835039]\n",
      "epoch:25 step:19868 [D loss: 0.052699, acc.: 100.00%] [G loss: 3.675737]\n",
      "epoch:25 step:19869 [D loss: 0.118533, acc.: 95.31%] [G loss: 5.619080]\n",
      "epoch:25 step:19870 [D loss: 0.033494, acc.: 99.22%] [G loss: 6.943543]\n",
      "epoch:25 step:19871 [D loss: 0.154172, acc.: 91.41%] [G loss: 3.850033]\n",
      "epoch:25 step:19872 [D loss: 0.085896, acc.: 97.66%] [G loss: 4.997681]\n",
      "epoch:25 step:19873 [D loss: 0.012107, acc.: 100.00%] [G loss: 6.107040]\n",
      "epoch:25 step:19874 [D loss: 0.005292, acc.: 100.00%] [G loss: 5.930092]\n",
      "epoch:25 step:19875 [D loss: 0.406590, acc.: 81.25%] [G loss: 4.860923]\n",
      "epoch:25 step:19876 [D loss: 0.006120, acc.: 100.00%] [G loss: 5.661539]\n",
      "epoch:25 step:19877 [D loss: 0.005717, acc.: 100.00%] [G loss: 5.130249]\n",
      "epoch:25 step:19878 [D loss: 0.025788, acc.: 99.22%] [G loss: 4.940365]\n",
      "epoch:25 step:19879 [D loss: 0.017484, acc.: 100.00%] [G loss: 5.647870]\n",
      "epoch:25 step:19880 [D loss: 0.178577, acc.: 93.75%] [G loss: 3.605629]\n",
      "epoch:25 step:19881 [D loss: 0.130356, acc.: 95.31%] [G loss: 6.960451]\n",
      "epoch:25 step:19882 [D loss: 0.003408, acc.: 100.00%] [G loss: 7.115352]\n",
      "epoch:25 step:19883 [D loss: 0.231828, acc.: 88.28%] [G loss: 4.123469]\n",
      "epoch:25 step:19884 [D loss: 0.102402, acc.: 93.75%] [G loss: 5.145754]\n",
      "epoch:25 step:19885 [D loss: 0.014209, acc.: 100.00%] [G loss: 5.184780]\n",
      "epoch:25 step:19886 [D loss: 0.056841, acc.: 98.44%] [G loss: 4.394491]\n",
      "epoch:25 step:19887 [D loss: 0.004669, acc.: 100.00%] [G loss: 4.496003]\n",
      "epoch:25 step:19888 [D loss: 0.024410, acc.: 100.00%] [G loss: 4.790248]\n",
      "epoch:25 step:19889 [D loss: 0.013369, acc.: 100.00%] [G loss: 4.185695]\n",
      "epoch:25 step:19890 [D loss: 0.001811, acc.: 100.00%] [G loss: 3.876903]\n",
      "epoch:25 step:19891 [D loss: 0.005178, acc.: 100.00%] [G loss: 2.109770]\n",
      "epoch:25 step:19892 [D loss: 0.037917, acc.: 99.22%] [G loss: 3.202771]\n",
      "epoch:25 step:19893 [D loss: 0.002422, acc.: 100.00%] [G loss: 3.194693]\n",
      "epoch:25 step:19894 [D loss: 0.015144, acc.: 99.22%] [G loss: 3.152827]\n",
      "epoch:25 step:19895 [D loss: 0.066559, acc.: 98.44%] [G loss: 3.779566]\n",
      "epoch:25 step:19896 [D loss: 0.003382, acc.: 100.00%] [G loss: 3.482232]\n",
      "epoch:25 step:19897 [D loss: 0.005161, acc.: 100.00%] [G loss: 2.942702]\n",
      "epoch:25 step:19898 [D loss: 0.018509, acc.: 99.22%] [G loss: 3.667327]\n",
      "epoch:25 step:19899 [D loss: 0.047360, acc.: 100.00%] [G loss: 3.453887]\n",
      "epoch:25 step:19900 [D loss: 0.041572, acc.: 99.22%] [G loss: 3.096532]\n",
      "epoch:25 step:19901 [D loss: 0.041617, acc.: 100.00%] [G loss: 3.733624]\n",
      "epoch:25 step:19902 [D loss: 0.004860, acc.: 100.00%] [G loss: 4.009821]\n",
      "epoch:25 step:19903 [D loss: 0.013880, acc.: 100.00%] [G loss: 4.231439]\n",
      "epoch:25 step:19904 [D loss: 0.010195, acc.: 100.00%] [G loss: 3.192586]\n",
      "epoch:25 step:19905 [D loss: 0.009746, acc.: 100.00%] [G loss: 4.787971]\n",
      "epoch:25 step:19906 [D loss: 0.029935, acc.: 100.00%] [G loss: 3.871739]\n",
      "epoch:25 step:19907 [D loss: 0.413650, acc.: 84.38%] [G loss: 7.240020]\n",
      "epoch:25 step:19908 [D loss: 0.037201, acc.: 100.00%] [G loss: 7.830884]\n",
      "epoch:25 step:19909 [D loss: 0.464505, acc.: 76.56%] [G loss: 3.989525]\n",
      "epoch:25 step:19910 [D loss: 0.175222, acc.: 90.62%] [G loss: 6.759202]\n",
      "epoch:25 step:19911 [D loss: 0.001451, acc.: 100.00%] [G loss: 7.027008]\n",
      "epoch:25 step:19912 [D loss: 0.002473, acc.: 100.00%] [G loss: 7.102134]\n",
      "epoch:25 step:19913 [D loss: 0.012698, acc.: 100.00%] [G loss: 6.778858]\n",
      "epoch:25 step:19914 [D loss: 0.002533, acc.: 100.00%] [G loss: 6.628544]\n",
      "epoch:25 step:19915 [D loss: 0.003638, acc.: 100.00%] [G loss: 6.417281]\n",
      "epoch:25 step:19916 [D loss: 0.003758, acc.: 100.00%] [G loss: 5.836732]\n",
      "epoch:25 step:19917 [D loss: 0.014578, acc.: 99.22%] [G loss: 4.959016]\n",
      "epoch:25 step:19918 [D loss: 0.056438, acc.: 99.22%] [G loss: 5.276701]\n",
      "epoch:25 step:19919 [D loss: 0.016785, acc.: 100.00%] [G loss: 5.530200]\n",
      "epoch:25 step:19920 [D loss: 0.026118, acc.: 99.22%] [G loss: 6.123178]\n",
      "epoch:25 step:19921 [D loss: 0.010103, acc.: 100.00%] [G loss: 4.262106]\n",
      "epoch:25 step:19922 [D loss: 0.004722, acc.: 100.00%] [G loss: 4.108278]\n",
      "epoch:25 step:19923 [D loss: 0.033381, acc.: 98.44%] [G loss: 4.339003]\n",
      "epoch:25 step:19924 [D loss: 0.009116, acc.: 100.00%] [G loss: 5.145044]\n",
      "epoch:25 step:19925 [D loss: 0.010091, acc.: 100.00%] [G loss: 4.997037]\n",
      "epoch:25 step:19926 [D loss: 1.889030, acc.: 28.91%] [G loss: 8.580095]\n",
      "epoch:25 step:19927 [D loss: 2.937943, acc.: 50.00%] [G loss: 7.107729]\n",
      "epoch:25 step:19928 [D loss: 2.386400, acc.: 50.00%] [G loss: 4.417112]\n",
      "epoch:25 step:19929 [D loss: 0.449958, acc.: 78.91%] [G loss: 1.912093]\n",
      "epoch:25 step:19930 [D loss: 0.402710, acc.: 80.47%] [G loss: 3.477559]\n",
      "epoch:25 step:19931 [D loss: 0.121912, acc.: 96.09%] [G loss: 3.938842]\n",
      "epoch:25 step:19932 [D loss: 0.110426, acc.: 99.22%] [G loss: 3.456210]\n",
      "epoch:25 step:19933 [D loss: 0.147457, acc.: 95.31%] [G loss: 3.150096]\n",
      "epoch:25 step:19934 [D loss: 0.169268, acc.: 95.31%] [G loss: 2.357319]\n",
      "epoch:25 step:19935 [D loss: 0.140237, acc.: 96.09%] [G loss: 3.067771]\n",
      "epoch:25 step:19936 [D loss: 0.050447, acc.: 100.00%] [G loss: 3.387022]\n",
      "epoch:25 step:19937 [D loss: 0.099647, acc.: 100.00%] [G loss: 2.677123]\n",
      "epoch:25 step:19938 [D loss: 0.132657, acc.: 96.09%] [G loss: 1.726522]\n",
      "epoch:25 step:19939 [D loss: 0.105314, acc.: 98.44%] [G loss: 2.773547]\n",
      "epoch:25 step:19940 [D loss: 0.063619, acc.: 100.00%] [G loss: 3.553506]\n",
      "epoch:25 step:19941 [D loss: 0.151218, acc.: 94.53%] [G loss: 1.911996]\n",
      "epoch:25 step:19942 [D loss: 0.109533, acc.: 98.44%] [G loss: 1.196928]\n",
      "epoch:25 step:19943 [D loss: 0.112845, acc.: 99.22%] [G loss: 4.022366]\n",
      "epoch:25 step:19944 [D loss: 0.027768, acc.: 100.00%] [G loss: 3.934007]\n",
      "epoch:25 step:19945 [D loss: 0.156572, acc.: 93.75%] [G loss: 2.629058]\n",
      "epoch:25 step:19946 [D loss: 0.191868, acc.: 91.41%] [G loss: 4.197086]\n",
      "epoch:25 step:19947 [D loss: 0.020830, acc.: 100.00%] [G loss: 5.009050]\n",
      "epoch:25 step:19948 [D loss: 0.111801, acc.: 95.31%] [G loss: 3.997050]\n",
      "epoch:25 step:19949 [D loss: 0.085950, acc.: 99.22%] [G loss: 2.724064]\n",
      "epoch:25 step:19950 [D loss: 0.123735, acc.: 96.88%] [G loss: 4.132361]\n",
      "epoch:25 step:19951 [D loss: 0.024103, acc.: 100.00%] [G loss: 5.188498]\n",
      "epoch:25 step:19952 [D loss: 0.050030, acc.: 98.44%] [G loss: 4.007036]\n",
      "epoch:25 step:19953 [D loss: 0.066094, acc.: 99.22%] [G loss: 2.257546]\n",
      "epoch:25 step:19954 [D loss: 0.104258, acc.: 97.66%] [G loss: 3.795053]\n",
      "epoch:25 step:19955 [D loss: 0.109522, acc.: 96.88%] [G loss: 3.890019]\n",
      "epoch:25 step:19956 [D loss: 0.141486, acc.: 98.44%] [G loss: 3.761294]\n",
      "epoch:25 step:19957 [D loss: 0.037190, acc.: 100.00%] [G loss: 4.775774]\n",
      "epoch:25 step:19958 [D loss: 0.017680, acc.: 100.00%] [G loss: 4.845486]\n",
      "epoch:25 step:19959 [D loss: 0.022735, acc.: 100.00%] [G loss: 3.719107]\n",
      "epoch:25 step:19960 [D loss: 0.029801, acc.: 100.00%] [G loss: 3.870676]\n",
      "epoch:25 step:19961 [D loss: 0.046743, acc.: 100.00%] [G loss: 3.613447]\n",
      "epoch:25 step:19962 [D loss: 0.156102, acc.: 96.09%] [G loss: 3.005718]\n",
      "epoch:25 step:19963 [D loss: 0.020845, acc.: 100.00%] [G loss: 3.872909]\n",
      "epoch:25 step:19964 [D loss: 0.036290, acc.: 98.44%] [G loss: 3.094829]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:25 step:19965 [D loss: 0.036574, acc.: 100.00%] [G loss: 3.938566]\n",
      "epoch:25 step:19966 [D loss: 0.034906, acc.: 100.00%] [G loss: 4.613799]\n",
      "epoch:25 step:19967 [D loss: 0.023200, acc.: 100.00%] [G loss: 4.141313]\n",
      "epoch:25 step:19968 [D loss: 0.133805, acc.: 97.66%] [G loss: 4.412986]\n",
      "epoch:25 step:19969 [D loss: 0.043105, acc.: 99.22%] [G loss: 4.690002]\n",
      "epoch:25 step:19970 [D loss: 0.093115, acc.: 96.88%] [G loss: 1.761226]\n",
      "epoch:25 step:19971 [D loss: 0.130967, acc.: 96.09%] [G loss: 5.497954]\n",
      "epoch:25 step:19972 [D loss: 0.066674, acc.: 96.88%] [G loss: 4.881717]\n",
      "epoch:25 step:19973 [D loss: 0.099484, acc.: 98.44%] [G loss: 3.482487]\n",
      "epoch:25 step:19974 [D loss: 0.033956, acc.: 100.00%] [G loss: 3.523647]\n",
      "epoch:25 step:19975 [D loss: 0.020451, acc.: 100.00%] [G loss: 4.472489]\n",
      "epoch:25 step:19976 [D loss: 0.071949, acc.: 98.44%] [G loss: 3.701175]\n",
      "epoch:25 step:19977 [D loss: 0.033430, acc.: 100.00%] [G loss: 3.711481]\n",
      "epoch:25 step:19978 [D loss: 0.257813, acc.: 92.19%] [G loss: 4.820271]\n",
      "epoch:25 step:19979 [D loss: 0.079086, acc.: 98.44%] [G loss: 4.800095]\n",
      "epoch:25 step:19980 [D loss: 0.011726, acc.: 100.00%] [G loss: 4.644631]\n",
      "epoch:25 step:19981 [D loss: 0.016431, acc.: 100.00%] [G loss: 3.723455]\n",
      "epoch:25 step:19982 [D loss: 0.023818, acc.: 100.00%] [G loss: 4.069746]\n",
      "epoch:25 step:19983 [D loss: 0.088786, acc.: 99.22%] [G loss: 4.484965]\n",
      "epoch:25 step:19984 [D loss: 0.116172, acc.: 95.31%] [G loss: 3.772013]\n",
      "epoch:25 step:19985 [D loss: 0.070925, acc.: 99.22%] [G loss: 3.629532]\n",
      "epoch:25 step:19986 [D loss: 0.017823, acc.: 100.00%] [G loss: 5.083180]\n",
      "epoch:25 step:19987 [D loss: 0.007813, acc.: 100.00%] [G loss: 4.319107]\n",
      "epoch:25 step:19988 [D loss: 0.057709, acc.: 97.66%] [G loss: 5.018735]\n",
      "epoch:25 step:19989 [D loss: 0.022482, acc.: 100.00%] [G loss: 5.225323]\n",
      "epoch:25 step:19990 [D loss: 0.014923, acc.: 100.00%] [G loss: 4.728541]\n",
      "epoch:25 step:19991 [D loss: 0.068360, acc.: 99.22%] [G loss: 3.212561]\n",
      "epoch:25 step:19992 [D loss: 0.135512, acc.: 96.88%] [G loss: 6.593451]\n",
      "epoch:25 step:19993 [D loss: 3.139662, acc.: 25.78%] [G loss: 8.534498]\n",
      "epoch:25 step:19994 [D loss: 2.609091, acc.: 50.00%] [G loss: 6.952038]\n",
      "epoch:25 step:19995 [D loss: 1.690493, acc.: 50.00%] [G loss: 3.538714]\n",
      "epoch:25 step:19996 [D loss: 0.223144, acc.: 90.62%] [G loss: 2.130597]\n",
      "epoch:25 step:19997 [D loss: 0.082917, acc.: 98.44%] [G loss: 2.524534]\n",
      "epoch:25 step:19998 [D loss: 0.127222, acc.: 96.09%] [G loss: 3.506851]\n",
      "epoch:25 step:19999 [D loss: 0.037136, acc.: 100.00%] [G loss: 3.587304]\n",
      "epoch:25 step:20000 [D loss: 0.256278, acc.: 91.41%] [G loss: 2.514481]\n",
      "##############\n",
      "[0.93403927 0.95749086 1.10497093 1.01661143 2.11563566 2.10452591\n",
      " 2.10867341 2.13085948 2.10803343 1.10664701]\n",
      "##########\n",
      "epoch:25 step:20001 [D loss: 0.131903, acc.: 97.66%] [G loss: 3.750599]\n",
      "epoch:25 step:20002 [D loss: 0.020018, acc.: 100.00%] [G loss: 3.421653]\n",
      "epoch:25 step:20003 [D loss: 0.154815, acc.: 96.09%] [G loss: 3.265048]\n",
      "epoch:25 step:20004 [D loss: 0.039218, acc.: 100.00%] [G loss: 3.279903]\n",
      "epoch:25 step:20005 [D loss: 0.075567, acc.: 99.22%] [G loss: 2.917266]\n",
      "epoch:25 step:20006 [D loss: 0.066193, acc.: 100.00%] [G loss: 3.838028]\n",
      "epoch:25 step:20007 [D loss: 0.142962, acc.: 96.09%] [G loss: 2.327707]\n",
      "epoch:25 step:20008 [D loss: 0.414105, acc.: 82.03%] [G loss: 4.739709]\n",
      "epoch:25 step:20009 [D loss: 0.453642, acc.: 73.44%] [G loss: 4.112186]\n",
      "epoch:25 step:20010 [D loss: 0.159292, acc.: 95.31%] [G loss: 2.442403]\n",
      "epoch:25 step:20011 [D loss: 0.071469, acc.: 99.22%] [G loss: 2.303288]\n",
      "epoch:25 step:20012 [D loss: 0.055837, acc.: 100.00%] [G loss: 2.728409]\n",
      "epoch:25 step:20013 [D loss: 0.037089, acc.: 100.00%] [G loss: 3.039700]\n",
      "epoch:25 step:20014 [D loss: 0.040900, acc.: 100.00%] [G loss: 2.062608]\n",
      "epoch:25 step:20015 [D loss: 0.041480, acc.: 100.00%] [G loss: 2.488232]\n",
      "epoch:25 step:20016 [D loss: 0.080899, acc.: 100.00%] [G loss: 1.840231]\n",
      "epoch:25 step:20017 [D loss: 0.203524, acc.: 94.53%] [G loss: 1.993939]\n",
      "epoch:25 step:20018 [D loss: 0.009475, acc.: 100.00%] [G loss: 2.803019]\n",
      "epoch:25 step:20019 [D loss: 0.035307, acc.: 100.00%] [G loss: 2.239137]\n",
      "epoch:25 step:20020 [D loss: 0.127503, acc.: 96.88%] [G loss: 3.645503]\n",
      "epoch:25 step:20021 [D loss: 0.198515, acc.: 90.62%] [G loss: 2.208245]\n",
      "epoch:25 step:20022 [D loss: 0.114923, acc.: 97.66%] [G loss: 2.438566]\n",
      "epoch:25 step:20023 [D loss: 0.164830, acc.: 94.53%] [G loss: 2.618730]\n",
      "epoch:25 step:20024 [D loss: 0.060213, acc.: 99.22%] [G loss: 4.486489]\n",
      "epoch:25 step:20025 [D loss: 0.071182, acc.: 97.66%] [G loss: 2.472455]\n",
      "epoch:25 step:20026 [D loss: 0.112209, acc.: 96.09%] [G loss: 3.421763]\n",
      "epoch:25 step:20027 [D loss: 0.131315, acc.: 95.31%] [G loss: 3.058740]\n",
      "epoch:25 step:20028 [D loss: 0.158481, acc.: 92.97%] [G loss: 4.172193]\n",
      "epoch:25 step:20029 [D loss: 0.109240, acc.: 97.66%] [G loss: 4.052341]\n",
      "epoch:25 step:20030 [D loss: 0.035298, acc.: 100.00%] [G loss: 3.520145]\n",
      "epoch:25 step:20031 [D loss: 0.123664, acc.: 96.88%] [G loss: 4.457592]\n",
      "epoch:25 step:20032 [D loss: 0.021063, acc.: 100.00%] [G loss: 4.278323]\n",
      "epoch:25 step:20033 [D loss: 0.170866, acc.: 96.09%] [G loss: 2.613055]\n",
      "epoch:25 step:20034 [D loss: 0.437751, acc.: 82.81%] [G loss: 5.552623]\n",
      "epoch:25 step:20035 [D loss: 0.276271, acc.: 84.38%] [G loss: 4.048526]\n",
      "epoch:25 step:20036 [D loss: 0.015761, acc.: 100.00%] [G loss: 3.992410]\n",
      "epoch:25 step:20037 [D loss: 0.026107, acc.: 100.00%] [G loss: 4.231101]\n",
      "epoch:25 step:20038 [D loss: 0.022378, acc.: 100.00%] [G loss: 4.818287]\n",
      "epoch:25 step:20039 [D loss: 0.007345, acc.: 100.00%] [G loss: 4.128002]\n",
      "epoch:25 step:20040 [D loss: 0.031710, acc.: 100.00%] [G loss: 3.522920]\n",
      "epoch:25 step:20041 [D loss: 0.027193, acc.: 100.00%] [G loss: 5.024256]\n",
      "epoch:25 step:20042 [D loss: 0.013641, acc.: 100.00%] [G loss: 4.093590]\n",
      "epoch:25 step:20043 [D loss: 0.050492, acc.: 99.22%] [G loss: 3.651212]\n",
      "epoch:25 step:20044 [D loss: 0.201707, acc.: 95.31%] [G loss: 4.808468]\n",
      "epoch:25 step:20045 [D loss: 0.007277, acc.: 100.00%] [G loss: 4.344269]\n",
      "epoch:25 step:20046 [D loss: 0.025406, acc.: 100.00%] [G loss: 4.256083]\n",
      "epoch:25 step:20047 [D loss: 0.021430, acc.: 100.00%] [G loss: 3.314481]\n",
      "epoch:25 step:20048 [D loss: 0.037629, acc.: 100.00%] [G loss: 2.905744]\n",
      "epoch:25 step:20049 [D loss: 0.039600, acc.: 100.00%] [G loss: 3.884468]\n",
      "epoch:25 step:20050 [D loss: 0.023024, acc.: 100.00%] [G loss: 4.579333]\n",
      "epoch:25 step:20051 [D loss: 0.040919, acc.: 100.00%] [G loss: 4.175610]\n",
      "epoch:25 step:20052 [D loss: 0.017157, acc.: 100.00%] [G loss: 3.688430]\n",
      "epoch:25 step:20053 [D loss: 0.031386, acc.: 100.00%] [G loss: 3.561034]\n",
      "epoch:25 step:20054 [D loss: 0.056109, acc.: 99.22%] [G loss: 5.129690]\n",
      "epoch:25 step:20055 [D loss: 0.014954, acc.: 100.00%] [G loss: 5.572814]\n",
      "epoch:25 step:20056 [D loss: 0.097622, acc.: 96.09%] [G loss: 3.309542]\n",
      "epoch:25 step:20057 [D loss: 0.029653, acc.: 100.00%] [G loss: 2.648976]\n",
      "epoch:25 step:20058 [D loss: 0.168400, acc.: 94.53%] [G loss: 5.552027]\n",
      "epoch:25 step:20059 [D loss: 0.105198, acc.: 94.53%] [G loss: 5.760533]\n",
      "epoch:25 step:20060 [D loss: 0.014788, acc.: 100.00%] [G loss: 5.418152]\n",
      "epoch:25 step:20061 [D loss: 0.094557, acc.: 97.66%] [G loss: 3.394229]\n",
      "epoch:25 step:20062 [D loss: 0.706394, acc.: 68.75%] [G loss: 8.041835]\n",
      "epoch:25 step:20063 [D loss: 1.436846, acc.: 57.03%] [G loss: 3.676530]\n",
      "epoch:25 step:20064 [D loss: 0.173934, acc.: 91.41%] [G loss: 4.103014]\n",
      "epoch:25 step:20065 [D loss: 0.056557, acc.: 99.22%] [G loss: 4.740675]\n",
      "epoch:25 step:20066 [D loss: 0.061477, acc.: 99.22%] [G loss: 4.544731]\n",
      "epoch:25 step:20067 [D loss: 0.177255, acc.: 94.53%] [G loss: 1.737226]\n",
      "epoch:25 step:20068 [D loss: 0.355148, acc.: 80.47%] [G loss: 5.851748]\n",
      "epoch:25 step:20069 [D loss: 0.199584, acc.: 91.41%] [G loss: 5.356110]\n",
      "epoch:25 step:20070 [D loss: 0.139340, acc.: 94.53%] [G loss: 4.164710]\n",
      "epoch:25 step:20071 [D loss: 0.029619, acc.: 100.00%] [G loss: 4.028151]\n",
      "epoch:25 step:20072 [D loss: 0.074513, acc.: 98.44%] [G loss: 4.074778]\n",
      "epoch:25 step:20073 [D loss: 0.067645, acc.: 99.22%] [G loss: 3.935183]\n",
      "epoch:25 step:20074 [D loss: 0.013802, acc.: 100.00%] [G loss: 4.135869]\n",
      "epoch:25 step:20075 [D loss: 0.098024, acc.: 97.66%] [G loss: 2.555097]\n",
      "epoch:25 step:20076 [D loss: 0.029721, acc.: 100.00%] [G loss: 2.273176]\n",
      "epoch:25 step:20077 [D loss: 0.094456, acc.: 96.09%] [G loss: 3.112608]\n",
      "epoch:25 step:20078 [D loss: 0.040205, acc.: 100.00%] [G loss: 4.523562]\n",
      "epoch:25 step:20079 [D loss: 0.096329, acc.: 96.88%] [G loss: 2.258499]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:25 step:20080 [D loss: 0.052180, acc.: 98.44%] [G loss: 2.568647]\n",
      "epoch:25 step:20081 [D loss: 0.027539, acc.: 99.22%] [G loss: 2.002189]\n",
      "epoch:25 step:20082 [D loss: 0.053255, acc.: 99.22%] [G loss: 2.156433]\n",
      "epoch:25 step:20083 [D loss: 1.028450, acc.: 50.78%] [G loss: 5.852322]\n",
      "epoch:25 step:20084 [D loss: 0.258846, acc.: 86.72%] [G loss: 7.259914]\n",
      "epoch:25 step:20085 [D loss: 0.671507, acc.: 65.62%] [G loss: 1.303996]\n",
      "epoch:25 step:20086 [D loss: 0.631748, acc.: 71.09%] [G loss: 5.953106]\n",
      "epoch:25 step:20087 [D loss: 0.023638, acc.: 100.00%] [G loss: 6.597837]\n",
      "epoch:25 step:20088 [D loss: 0.223384, acc.: 90.62%] [G loss: 5.998096]\n",
      "epoch:25 step:20089 [D loss: 0.006441, acc.: 100.00%] [G loss: 6.018211]\n",
      "epoch:25 step:20090 [D loss: 0.037370, acc.: 99.22%] [G loss: 5.054843]\n",
      "epoch:25 step:20091 [D loss: 0.031160, acc.: 100.00%] [G loss: 5.472253]\n",
      "epoch:25 step:20092 [D loss: 0.040380, acc.: 98.44%] [G loss: 3.544964]\n",
      "epoch:25 step:20093 [D loss: 0.012138, acc.: 100.00%] [G loss: 4.513660]\n",
      "epoch:25 step:20094 [D loss: 0.007068, acc.: 100.00%] [G loss: 4.091722]\n",
      "epoch:25 step:20095 [D loss: 0.013101, acc.: 100.00%] [G loss: 5.266051]\n",
      "epoch:25 step:20096 [D loss: 0.008563, acc.: 100.00%] [G loss: 3.846438]\n",
      "epoch:25 step:20097 [D loss: 0.024998, acc.: 99.22%] [G loss: 3.682774]\n",
      "epoch:25 step:20098 [D loss: 0.026027, acc.: 100.00%] [G loss: 3.326346]\n",
      "epoch:25 step:20099 [D loss: 0.005067, acc.: 100.00%] [G loss: 4.481090]\n",
      "epoch:25 step:20100 [D loss: 0.024198, acc.: 100.00%] [G loss: 5.167446]\n",
      "epoch:25 step:20101 [D loss: 0.056577, acc.: 98.44%] [G loss: 5.617307]\n",
      "epoch:25 step:20102 [D loss: 0.140020, acc.: 95.31%] [G loss: 2.513753]\n",
      "epoch:25 step:20103 [D loss: 0.129727, acc.: 95.31%] [G loss: 3.764486]\n",
      "epoch:25 step:20104 [D loss: 0.012838, acc.: 100.00%] [G loss: 5.388794]\n",
      "epoch:25 step:20105 [D loss: 0.113283, acc.: 94.53%] [G loss: 4.470719]\n",
      "epoch:25 step:20106 [D loss: 0.004459, acc.: 100.00%] [G loss: 2.727993]\n",
      "epoch:25 step:20107 [D loss: 0.042155, acc.: 100.00%] [G loss: 3.023226]\n",
      "epoch:25 step:20108 [D loss: 0.009444, acc.: 100.00%] [G loss: 3.964673]\n",
      "epoch:25 step:20109 [D loss: 0.007278, acc.: 100.00%] [G loss: 4.051097]\n",
      "epoch:25 step:20110 [D loss: 0.036070, acc.: 100.00%] [G loss: 3.279804]\n",
      "epoch:25 step:20111 [D loss: 0.014220, acc.: 100.00%] [G loss: 3.432649]\n",
      "epoch:25 step:20112 [D loss: 0.015714, acc.: 100.00%] [G loss: 4.451847]\n",
      "epoch:25 step:20113 [D loss: 0.049944, acc.: 98.44%] [G loss: 3.842514]\n",
      "epoch:25 step:20114 [D loss: 0.046061, acc.: 99.22%] [G loss: 2.894533]\n",
      "epoch:25 step:20115 [D loss: 0.169362, acc.: 94.53%] [G loss: 4.702238]\n",
      "epoch:25 step:20116 [D loss: 0.015407, acc.: 100.00%] [G loss: 4.522825]\n",
      "epoch:25 step:20117 [D loss: 0.010567, acc.: 100.00%] [G loss: 3.948057]\n",
      "epoch:25 step:20118 [D loss: 0.034099, acc.: 100.00%] [G loss: 2.420042]\n",
      "epoch:25 step:20119 [D loss: 0.025771, acc.: 100.00%] [G loss: 3.392242]\n",
      "epoch:25 step:20120 [D loss: 0.114919, acc.: 96.09%] [G loss: 5.163754]\n",
      "epoch:25 step:20121 [D loss: 0.258307, acc.: 90.62%] [G loss: 3.337592]\n",
      "epoch:25 step:20122 [D loss: 0.045217, acc.: 100.00%] [G loss: 4.390084]\n",
      "epoch:25 step:20123 [D loss: 0.005233, acc.: 100.00%] [G loss: 4.682930]\n",
      "epoch:25 step:20124 [D loss: 1.281385, acc.: 39.84%] [G loss: 7.255956]\n",
      "epoch:25 step:20125 [D loss: 1.444505, acc.: 52.34%] [G loss: 5.378523]\n",
      "epoch:25 step:20126 [D loss: 0.052414, acc.: 100.00%] [G loss: 3.824553]\n",
      "epoch:25 step:20127 [D loss: 0.019522, acc.: 100.00%] [G loss: 4.207335]\n",
      "epoch:25 step:20128 [D loss: 0.068547, acc.: 98.44%] [G loss: 3.529832]\n",
      "epoch:25 step:20129 [D loss: 0.069525, acc.: 99.22%] [G loss: 3.363153]\n",
      "epoch:25 step:20130 [D loss: 0.033961, acc.: 100.00%] [G loss: 3.082719]\n",
      "epoch:25 step:20131 [D loss: 0.063896, acc.: 98.44%] [G loss: 4.113160]\n",
      "epoch:25 step:20132 [D loss: 0.008588, acc.: 100.00%] [G loss: 4.337642]\n",
      "epoch:25 step:20133 [D loss: 0.053405, acc.: 99.22%] [G loss: 4.160601]\n",
      "epoch:25 step:20134 [D loss: 0.034078, acc.: 100.00%] [G loss: 4.197040]\n",
      "epoch:25 step:20135 [D loss: 0.057664, acc.: 99.22%] [G loss: 4.003928]\n",
      "epoch:25 step:20136 [D loss: 0.047413, acc.: 100.00%] [G loss: 4.556418]\n",
      "epoch:25 step:20137 [D loss: 0.010556, acc.: 100.00%] [G loss: 4.387356]\n",
      "epoch:25 step:20138 [D loss: 0.059378, acc.: 100.00%] [G loss: 3.499126]\n",
      "epoch:25 step:20139 [D loss: 0.026217, acc.: 100.00%] [G loss: 4.338908]\n",
      "epoch:25 step:20140 [D loss: 0.046193, acc.: 100.00%] [G loss: 4.175362]\n",
      "epoch:25 step:20141 [D loss: 0.031141, acc.: 100.00%] [G loss: 4.082253]\n",
      "epoch:25 step:20142 [D loss: 0.091622, acc.: 100.00%] [G loss: 3.412485]\n",
      "epoch:25 step:20143 [D loss: 0.031726, acc.: 100.00%] [G loss: 3.891299]\n",
      "epoch:25 step:20144 [D loss: 0.134033, acc.: 96.88%] [G loss: 3.678304]\n",
      "epoch:25 step:20145 [D loss: 0.051244, acc.: 99.22%] [G loss: 5.135707]\n",
      "epoch:25 step:20146 [D loss: 0.006152, acc.: 100.00%] [G loss: 4.685347]\n",
      "epoch:25 step:20147 [D loss: 0.019453, acc.: 100.00%] [G loss: 3.637302]\n",
      "epoch:25 step:20148 [D loss: 0.672753, acc.: 67.97%] [G loss: 6.383806]\n",
      "epoch:25 step:20149 [D loss: 0.085647, acc.: 96.88%] [G loss: 6.763238]\n",
      "epoch:25 step:20150 [D loss: 0.160330, acc.: 94.53%] [G loss: 4.888007]\n",
      "epoch:25 step:20151 [D loss: 0.032237, acc.: 100.00%] [G loss: 4.137819]\n",
      "epoch:25 step:20152 [D loss: 0.013715, acc.: 100.00%] [G loss: 3.960824]\n",
      "epoch:25 step:20153 [D loss: 0.021067, acc.: 100.00%] [G loss: 4.061579]\n",
      "epoch:25 step:20154 [D loss: 0.013751, acc.: 100.00%] [G loss: 4.324461]\n",
      "epoch:25 step:20155 [D loss: 0.010184, acc.: 100.00%] [G loss: 4.454408]\n",
      "epoch:25 step:20156 [D loss: 0.055060, acc.: 99.22%] [G loss: 2.393618]\n",
      "epoch:25 step:20157 [D loss: 0.053522, acc.: 99.22%] [G loss: 3.380674]\n",
      "epoch:25 step:20158 [D loss: 0.019004, acc.: 100.00%] [G loss: 3.872056]\n",
      "epoch:25 step:20159 [D loss: 0.030690, acc.: 99.22%] [G loss: 4.438459]\n",
      "epoch:25 step:20160 [D loss: 0.032467, acc.: 100.00%] [G loss: 2.748884]\n",
      "epoch:25 step:20161 [D loss: 0.030123, acc.: 100.00%] [G loss: 3.611657]\n",
      "epoch:25 step:20162 [D loss: 0.240808, acc.: 90.62%] [G loss: 7.401029]\n",
      "epoch:25 step:20163 [D loss: 0.978103, acc.: 60.94%] [G loss: 1.064281]\n",
      "epoch:25 step:20164 [D loss: 0.571291, acc.: 75.00%] [G loss: 7.867015]\n",
      "epoch:25 step:20165 [D loss: 0.260214, acc.: 90.62%] [G loss: 6.947499]\n",
      "epoch:25 step:20166 [D loss: 0.078241, acc.: 96.88%] [G loss: 6.521398]\n",
      "epoch:25 step:20167 [D loss: 0.534914, acc.: 81.25%] [G loss: 4.500948]\n",
      "epoch:25 step:20168 [D loss: 0.006534, acc.: 100.00%] [G loss: 4.813000]\n",
      "epoch:25 step:20169 [D loss: 0.026744, acc.: 100.00%] [G loss: 5.246387]\n",
      "epoch:25 step:20170 [D loss: 0.072703, acc.: 97.66%] [G loss: 4.664971]\n",
      "epoch:25 step:20171 [D loss: 0.013556, acc.: 100.00%] [G loss: 3.840896]\n",
      "epoch:25 step:20172 [D loss: 0.057186, acc.: 99.22%] [G loss: 4.096314]\n",
      "epoch:25 step:20173 [D loss: 0.029384, acc.: 99.22%] [G loss: 4.383203]\n",
      "epoch:25 step:20174 [D loss: 0.376783, acc.: 85.16%] [G loss: 6.334290]\n",
      "epoch:25 step:20175 [D loss: 0.019299, acc.: 99.22%] [G loss: 6.312473]\n",
      "epoch:25 step:20176 [D loss: 0.157064, acc.: 93.75%] [G loss: 5.110148]\n",
      "epoch:25 step:20177 [D loss: 0.027761, acc.: 100.00%] [G loss: 3.254570]\n",
      "epoch:25 step:20178 [D loss: 0.148803, acc.: 93.75%] [G loss: 5.141166]\n",
      "epoch:25 step:20179 [D loss: 0.033634, acc.: 99.22%] [G loss: 5.278841]\n",
      "epoch:25 step:20180 [D loss: 0.006023, acc.: 100.00%] [G loss: 4.870242]\n",
      "epoch:25 step:20181 [D loss: 0.080916, acc.: 97.66%] [G loss: 4.663455]\n",
      "epoch:25 step:20182 [D loss: 0.032486, acc.: 99.22%] [G loss: 2.455655]\n",
      "epoch:25 step:20183 [D loss: 0.363263, acc.: 83.59%] [G loss: 5.394638]\n",
      "epoch:25 step:20184 [D loss: 0.040711, acc.: 98.44%] [G loss: 6.185281]\n",
      "epoch:25 step:20185 [D loss: 0.482841, acc.: 77.34%] [G loss: 1.191094]\n",
      "epoch:25 step:20186 [D loss: 1.089819, acc.: 59.38%] [G loss: 7.625084]\n",
      "epoch:25 step:20187 [D loss: 0.186823, acc.: 92.19%] [G loss: 7.757731]\n",
      "epoch:25 step:20188 [D loss: 0.085157, acc.: 96.88%] [G loss: 6.027089]\n",
      "epoch:25 step:20189 [D loss: 0.009099, acc.: 100.00%] [G loss: 5.079025]\n",
      "epoch:25 step:20190 [D loss: 0.011762, acc.: 100.00%] [G loss: 4.546538]\n",
      "epoch:25 step:20191 [D loss: 0.044858, acc.: 99.22%] [G loss: 4.707889]\n",
      "epoch:25 step:20192 [D loss: 0.011388, acc.: 100.00%] [G loss: 4.837355]\n",
      "epoch:25 step:20193 [D loss: 0.037666, acc.: 100.00%] [G loss: 5.185770]\n",
      "epoch:25 step:20194 [D loss: 0.032247, acc.: 100.00%] [G loss: 5.719426]\n",
      "epoch:25 step:20195 [D loss: 0.108937, acc.: 96.88%] [G loss: 1.860211]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:25 step:20196 [D loss: 0.393507, acc.: 81.25%] [G loss: 8.452606]\n",
      "epoch:25 step:20197 [D loss: 1.321497, acc.: 53.91%] [G loss: 2.114461]\n",
      "epoch:25 step:20198 [D loss: 0.328282, acc.: 85.16%] [G loss: 5.721821]\n",
      "epoch:25 step:20199 [D loss: 0.125843, acc.: 92.97%] [G loss: 5.737063]\n",
      "epoch:25 step:20200 [D loss: 0.066071, acc.: 99.22%] [G loss: 4.539336]\n",
      "##############\n",
      "[1.10765075 0.92611148 1.11725259 0.95109113 2.10920377 1.09699045\n",
      " 1.10132551 0.95209225 2.10705895 0.97728833]\n",
      "##########\n",
      "epoch:25 step:20201 [D loss: 0.007581, acc.: 100.00%] [G loss: 4.578923]\n",
      "epoch:25 step:20202 [D loss: 0.036174, acc.: 99.22%] [G loss: 4.515002]\n",
      "epoch:25 step:20203 [D loss: 0.013194, acc.: 100.00%] [G loss: 3.142433]\n",
      "epoch:25 step:20204 [D loss: 0.015659, acc.: 100.00%] [G loss: 3.013076]\n",
      "epoch:25 step:20205 [D loss: 0.081189, acc.: 97.66%] [G loss: 5.257343]\n",
      "epoch:25 step:20206 [D loss: 0.014551, acc.: 100.00%] [G loss: 4.853256]\n",
      "epoch:25 step:20207 [D loss: 1.495438, acc.: 39.06%] [G loss: 7.501508]\n",
      "epoch:25 step:20208 [D loss: 1.071364, acc.: 59.38%] [G loss: 5.642639]\n",
      "epoch:25 step:20209 [D loss: 0.049326, acc.: 99.22%] [G loss: 4.255692]\n",
      "epoch:25 step:20210 [D loss: 0.114229, acc.: 96.88%] [G loss: 3.722163]\n",
      "epoch:25 step:20211 [D loss: 0.093273, acc.: 96.88%] [G loss: 4.307622]\n",
      "epoch:25 step:20212 [D loss: 0.085501, acc.: 97.66%] [G loss: 3.284569]\n",
      "epoch:25 step:20213 [D loss: 0.058135, acc.: 100.00%] [G loss: 2.174257]\n",
      "epoch:25 step:20214 [D loss: 0.057692, acc.: 100.00%] [G loss: 2.688771]\n",
      "epoch:25 step:20215 [D loss: 0.126019, acc.: 96.09%] [G loss: 3.742642]\n",
      "epoch:25 step:20216 [D loss: 0.017100, acc.: 100.00%] [G loss: 4.434493]\n",
      "epoch:25 step:20217 [D loss: 0.222339, acc.: 92.19%] [G loss: 2.017345]\n",
      "epoch:25 step:20218 [D loss: 0.051990, acc.: 100.00%] [G loss: 1.610050]\n",
      "epoch:25 step:20219 [D loss: 0.237660, acc.: 91.41%] [G loss: 5.373635]\n",
      "epoch:25 step:20220 [D loss: 0.432863, acc.: 77.34%] [G loss: 4.034713]\n",
      "epoch:25 step:20221 [D loss: 0.099531, acc.: 96.88%] [G loss: 4.228559]\n",
      "epoch:25 step:20222 [D loss: 0.017608, acc.: 100.00%] [G loss: 4.884705]\n",
      "epoch:25 step:20223 [D loss: 0.029507, acc.: 100.00%] [G loss: 4.142840]\n",
      "epoch:25 step:20224 [D loss: 0.019661, acc.: 100.00%] [G loss: 4.253579]\n",
      "epoch:25 step:20225 [D loss: 0.060126, acc.: 99.22%] [G loss: 3.611459]\n",
      "epoch:25 step:20226 [D loss: 0.022127, acc.: 100.00%] [G loss: 3.084210]\n",
      "epoch:25 step:20227 [D loss: 0.067538, acc.: 98.44%] [G loss: 3.440498]\n",
      "epoch:25 step:20228 [D loss: 0.050841, acc.: 99.22%] [G loss: 4.436618]\n",
      "epoch:25 step:20229 [D loss: 0.036734, acc.: 100.00%] [G loss: 3.780291]\n",
      "epoch:25 step:20230 [D loss: 0.045658, acc.: 99.22%] [G loss: 4.259256]\n",
      "epoch:25 step:20231 [D loss: 0.021843, acc.: 100.00%] [G loss: 3.393806]\n",
      "epoch:25 step:20232 [D loss: 0.059438, acc.: 100.00%] [G loss: 4.055737]\n",
      "epoch:25 step:20233 [D loss: 0.047604, acc.: 100.00%] [G loss: 3.849235]\n",
      "epoch:25 step:20234 [D loss: 0.111813, acc.: 96.09%] [G loss: 4.241153]\n",
      "epoch:25 step:20235 [D loss: 0.074925, acc.: 97.66%] [G loss: 3.557057]\n",
      "epoch:25 step:20236 [D loss: 0.039136, acc.: 100.00%] [G loss: 2.724602]\n",
      "epoch:25 step:20237 [D loss: 0.073860, acc.: 98.44%] [G loss: 3.502571]\n",
      "epoch:25 step:20238 [D loss: 0.073162, acc.: 97.66%] [G loss: 3.293055]\n",
      "epoch:25 step:20239 [D loss: 0.049600, acc.: 100.00%] [G loss: 2.377857]\n",
      "epoch:25 step:20240 [D loss: 0.817019, acc.: 57.81%] [G loss: 6.669212]\n",
      "epoch:25 step:20241 [D loss: 0.527751, acc.: 75.00%] [G loss: 5.692634]\n",
      "epoch:25 step:20242 [D loss: 0.062869, acc.: 98.44%] [G loss: 4.375114]\n",
      "epoch:25 step:20243 [D loss: 0.096739, acc.: 96.09%] [G loss: 4.543872]\n",
      "epoch:25 step:20244 [D loss: 0.015230, acc.: 100.00%] [G loss: 4.076851]\n",
      "epoch:25 step:20245 [D loss: 0.038780, acc.: 100.00%] [G loss: 3.925516]\n",
      "epoch:25 step:20246 [D loss: 0.063020, acc.: 100.00%] [G loss: 3.712980]\n",
      "epoch:25 step:20247 [D loss: 0.037308, acc.: 100.00%] [G loss: 3.953271]\n",
      "epoch:25 step:20248 [D loss: 0.027945, acc.: 99.22%] [G loss: 4.172251]\n",
      "epoch:25 step:20249 [D loss: 0.053714, acc.: 100.00%] [G loss: 4.314832]\n",
      "epoch:25 step:20250 [D loss: 0.152563, acc.: 95.31%] [G loss: 4.125516]\n",
      "epoch:25 step:20251 [D loss: 0.012836, acc.: 100.00%] [G loss: 4.354623]\n",
      "epoch:25 step:20252 [D loss: 0.020374, acc.: 100.00%] [G loss: 4.575280]\n",
      "epoch:25 step:20253 [D loss: 0.027278, acc.: 99.22%] [G loss: 4.481629]\n",
      "epoch:25 step:20254 [D loss: 0.037630, acc.: 99.22%] [G loss: 4.045304]\n",
      "epoch:25 step:20255 [D loss: 0.068764, acc.: 98.44%] [G loss: 4.216434]\n",
      "epoch:25 step:20256 [D loss: 0.010559, acc.: 100.00%] [G loss: 5.177750]\n",
      "epoch:25 step:20257 [D loss: 0.021816, acc.: 100.00%] [G loss: 4.305960]\n",
      "epoch:25 step:20258 [D loss: 0.045492, acc.: 99.22%] [G loss: 3.900965]\n",
      "epoch:25 step:20259 [D loss: 0.064338, acc.: 98.44%] [G loss: 3.958841]\n",
      "epoch:25 step:20260 [D loss: 0.039169, acc.: 100.00%] [G loss: 3.753401]\n",
      "epoch:25 step:20261 [D loss: 0.017337, acc.: 100.00%] [G loss: 4.231206]\n",
      "epoch:25 step:20262 [D loss: 0.064132, acc.: 99.22%] [G loss: 4.168564]\n",
      "epoch:25 step:20263 [D loss: 0.007475, acc.: 100.00%] [G loss: 4.321646]\n",
      "epoch:25 step:20264 [D loss: 0.175181, acc.: 95.31%] [G loss: 1.637252]\n",
      "epoch:25 step:20265 [D loss: 0.365850, acc.: 77.34%] [G loss: 6.652420]\n",
      "epoch:25 step:20266 [D loss: 0.135242, acc.: 93.75%] [G loss: 6.680749]\n",
      "epoch:25 step:20267 [D loss: 0.007261, acc.: 100.00%] [G loss: 6.691932]\n",
      "epoch:25 step:20268 [D loss: 0.087843, acc.: 96.88%] [G loss: 5.569922]\n",
      "epoch:25 step:20269 [D loss: 0.017356, acc.: 100.00%] [G loss: 5.438119]\n",
      "epoch:25 step:20270 [D loss: 0.009686, acc.: 100.00%] [G loss: 4.364164]\n",
      "epoch:25 step:20271 [D loss: 0.009826, acc.: 100.00%] [G loss: 4.200289]\n",
      "epoch:25 step:20272 [D loss: 0.014685, acc.: 100.00%] [G loss: 3.637261]\n",
      "epoch:25 step:20273 [D loss: 0.020316, acc.: 100.00%] [G loss: 4.273508]\n",
      "epoch:25 step:20274 [D loss: 0.015606, acc.: 100.00%] [G loss: 4.272057]\n",
      "epoch:25 step:20275 [D loss: 0.023572, acc.: 100.00%] [G loss: 4.374258]\n",
      "epoch:25 step:20276 [D loss: 0.009106, acc.: 100.00%] [G loss: 4.527022]\n",
      "epoch:25 step:20277 [D loss: 0.008320, acc.: 100.00%] [G loss: 3.952967]\n",
      "epoch:25 step:20278 [D loss: 0.040520, acc.: 100.00%] [G loss: 4.782348]\n",
      "epoch:25 step:20279 [D loss: 0.071573, acc.: 98.44%] [G loss: 4.264119]\n",
      "epoch:25 step:20280 [D loss: 0.006900, acc.: 100.00%] [G loss: 4.051848]\n",
      "epoch:25 step:20281 [D loss: 0.031527, acc.: 99.22%] [G loss: 3.901669]\n",
      "epoch:25 step:20282 [D loss: 0.018114, acc.: 100.00%] [G loss: 3.757656]\n",
      "epoch:25 step:20283 [D loss: 0.027798, acc.: 99.22%] [G loss: 4.143879]\n",
      "epoch:25 step:20284 [D loss: 0.022089, acc.: 100.00%] [G loss: 4.340942]\n",
      "epoch:25 step:20285 [D loss: 0.248587, acc.: 92.19%] [G loss: 5.919868]\n",
      "epoch:25 step:20286 [D loss: 0.008939, acc.: 100.00%] [G loss: 6.186477]\n",
      "epoch:25 step:20287 [D loss: 0.516012, acc.: 71.09%] [G loss: 0.497965]\n",
      "epoch:25 step:20288 [D loss: 0.705426, acc.: 67.97%] [G loss: 7.162680]\n",
      "epoch:25 step:20289 [D loss: 0.285702, acc.: 85.16%] [G loss: 6.169175]\n",
      "epoch:25 step:20290 [D loss: 0.242281, acc.: 88.28%] [G loss: 5.128651]\n",
      "epoch:25 step:20291 [D loss: 0.011718, acc.: 100.00%] [G loss: 4.858499]\n",
      "epoch:25 step:20292 [D loss: 0.011316, acc.: 100.00%] [G loss: 5.009531]\n",
      "epoch:25 step:20293 [D loss: 0.096890, acc.: 97.66%] [G loss: 4.786452]\n",
      "epoch:25 step:20294 [D loss: 0.002469, acc.: 100.00%] [G loss: 5.539648]\n",
      "epoch:25 step:20295 [D loss: 0.002185, acc.: 100.00%] [G loss: 6.074929]\n",
      "epoch:25 step:20296 [D loss: 0.006118, acc.: 100.00%] [G loss: 5.316432]\n",
      "epoch:25 step:20297 [D loss: 0.004456, acc.: 100.00%] [G loss: 4.937554]\n",
      "epoch:25 step:20298 [D loss: 0.005680, acc.: 100.00%] [G loss: 5.177784]\n",
      "epoch:25 step:20299 [D loss: 0.008995, acc.: 100.00%] [G loss: 5.594216]\n",
      "epoch:25 step:20300 [D loss: 0.005825, acc.: 100.00%] [G loss: 5.270200]\n",
      "epoch:25 step:20301 [D loss: 0.028819, acc.: 99.22%] [G loss: 4.821795]\n",
      "epoch:25 step:20302 [D loss: 0.016691, acc.: 100.00%] [G loss: 5.164127]\n",
      "epoch:25 step:20303 [D loss: 0.003926, acc.: 100.00%] [G loss: 4.895000]\n",
      "epoch:25 step:20304 [D loss: 0.017743, acc.: 100.00%] [G loss: 3.844679]\n",
      "epoch:25 step:20305 [D loss: 0.026633, acc.: 99.22%] [G loss: 3.838817]\n",
      "epoch:25 step:20306 [D loss: 0.015755, acc.: 100.00%] [G loss: 3.868851]\n",
      "epoch:26 step:20307 [D loss: 0.010890, acc.: 100.00%] [G loss: 4.493824]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:26 step:20308 [D loss: 0.037840, acc.: 99.22%] [G loss: 2.704019]\n",
      "epoch:26 step:20309 [D loss: 0.057305, acc.: 99.22%] [G loss: 4.313790]\n",
      "epoch:26 step:20310 [D loss: 0.013198, acc.: 100.00%] [G loss: 4.888288]\n",
      "epoch:26 step:20311 [D loss: 0.004897, acc.: 100.00%] [G loss: 4.673284]\n",
      "epoch:26 step:20312 [D loss: 0.001739, acc.: 100.00%] [G loss: 5.173983]\n",
      "epoch:26 step:20313 [D loss: 0.035227, acc.: 100.00%] [G loss: 4.532175]\n",
      "epoch:26 step:20314 [D loss: 0.007097, acc.: 100.00%] [G loss: 4.543735]\n",
      "epoch:26 step:20315 [D loss: 0.059806, acc.: 97.66%] [G loss: 2.525722]\n",
      "epoch:26 step:20316 [D loss: 0.071905, acc.: 97.66%] [G loss: 4.325835]\n",
      "epoch:26 step:20317 [D loss: 0.012601, acc.: 100.00%] [G loss: 4.703265]\n",
      "epoch:26 step:20318 [D loss: 0.093534, acc.: 96.09%] [G loss: 2.617749]\n",
      "epoch:26 step:20319 [D loss: 0.027525, acc.: 100.00%] [G loss: 2.916725]\n",
      "epoch:26 step:20320 [D loss: 0.029312, acc.: 100.00%] [G loss: 4.724730]\n",
      "epoch:26 step:20321 [D loss: 0.011160, acc.: 100.00%] [G loss: 4.378959]\n",
      "epoch:26 step:20322 [D loss: 0.008492, acc.: 100.00%] [G loss: 5.114964]\n",
      "epoch:26 step:20323 [D loss: 0.022688, acc.: 100.00%] [G loss: 2.859325]\n",
      "epoch:26 step:20324 [D loss: 0.025509, acc.: 100.00%] [G loss: 3.155898]\n",
      "epoch:26 step:20325 [D loss: 0.021618, acc.: 100.00%] [G loss: 5.036242]\n",
      "epoch:26 step:20326 [D loss: 0.029775, acc.: 100.00%] [G loss: 6.283282]\n",
      "epoch:26 step:20327 [D loss: 0.011355, acc.: 100.00%] [G loss: 5.591113]\n",
      "epoch:26 step:20328 [D loss: 0.063469, acc.: 99.22%] [G loss: 3.042690]\n",
      "epoch:26 step:20329 [D loss: 0.178599, acc.: 94.53%] [G loss: 5.153555]\n",
      "epoch:26 step:20330 [D loss: 0.008362, acc.: 100.00%] [G loss: 7.439210]\n",
      "epoch:26 step:20331 [D loss: 0.102903, acc.: 97.66%] [G loss: 5.584014]\n",
      "epoch:26 step:20332 [D loss: 0.149959, acc.: 93.75%] [G loss: 6.152944]\n",
      "epoch:26 step:20333 [D loss: 0.010768, acc.: 100.00%] [G loss: 7.249172]\n",
      "epoch:26 step:20334 [D loss: 0.414469, acc.: 83.59%] [G loss: 2.137982]\n",
      "epoch:26 step:20335 [D loss: 1.592760, acc.: 52.34%] [G loss: 9.796305]\n",
      "epoch:26 step:20336 [D loss: 3.055827, acc.: 50.00%] [G loss: 7.178473]\n",
      "epoch:26 step:20337 [D loss: 2.173714, acc.: 50.00%] [G loss: 4.226671]\n",
      "epoch:26 step:20338 [D loss: 0.952298, acc.: 56.25%] [G loss: 2.796597]\n",
      "epoch:26 step:20339 [D loss: 0.225263, acc.: 91.41%] [G loss: 2.585459]\n",
      "epoch:26 step:20340 [D loss: 0.142335, acc.: 96.88%] [G loss: 3.000608]\n",
      "epoch:26 step:20341 [D loss: 0.109095, acc.: 99.22%] [G loss: 2.596371]\n",
      "epoch:26 step:20342 [D loss: 0.221035, acc.: 92.19%] [G loss: 3.179902]\n",
      "epoch:26 step:20343 [D loss: 0.051499, acc.: 100.00%] [G loss: 3.694338]\n",
      "epoch:26 step:20344 [D loss: 0.185861, acc.: 92.19%] [G loss: 2.831645]\n",
      "epoch:26 step:20345 [D loss: 0.149047, acc.: 93.75%] [G loss: 2.888889]\n",
      "epoch:26 step:20346 [D loss: 0.037233, acc.: 100.00%] [G loss: 3.556105]\n",
      "epoch:26 step:20347 [D loss: 0.119356, acc.: 97.66%] [G loss: 2.792864]\n",
      "epoch:26 step:20348 [D loss: 0.169718, acc.: 98.44%] [G loss: 2.989522]\n",
      "epoch:26 step:20349 [D loss: 0.158078, acc.: 94.53%] [G loss: 3.541270]\n",
      "epoch:26 step:20350 [D loss: 0.368057, acc.: 83.59%] [G loss: 3.395500]\n",
      "epoch:26 step:20351 [D loss: 0.105709, acc.: 98.44%] [G loss: 3.599545]\n",
      "epoch:26 step:20352 [D loss: 0.160468, acc.: 96.09%] [G loss: 3.674020]\n",
      "epoch:26 step:20353 [D loss: 0.071894, acc.: 100.00%] [G loss: 3.232783]\n",
      "epoch:26 step:20354 [D loss: 0.041806, acc.: 99.22%] [G loss: 4.193868]\n",
      "epoch:26 step:20355 [D loss: 0.234669, acc.: 89.06%] [G loss: 2.610659]\n",
      "epoch:26 step:20356 [D loss: 0.093142, acc.: 98.44%] [G loss: 2.080724]\n",
      "epoch:26 step:20357 [D loss: 0.126766, acc.: 96.09%] [G loss: 3.956758]\n",
      "epoch:26 step:20358 [D loss: 0.068034, acc.: 99.22%] [G loss: 3.437141]\n",
      "epoch:26 step:20359 [D loss: 0.039949, acc.: 99.22%] [G loss: 3.368700]\n",
      "epoch:26 step:20360 [D loss: 0.329122, acc.: 86.72%] [G loss: 3.451775]\n",
      "epoch:26 step:20361 [D loss: 0.168953, acc.: 93.75%] [G loss: 3.916166]\n",
      "epoch:26 step:20362 [D loss: 0.083400, acc.: 98.44%] [G loss: 4.080638]\n",
      "epoch:26 step:20363 [D loss: 0.071350, acc.: 99.22%] [G loss: 3.269830]\n",
      "epoch:26 step:20364 [D loss: 0.085421, acc.: 100.00%] [G loss: 3.928029]\n",
      "epoch:26 step:20365 [D loss: 0.024737, acc.: 100.00%] [G loss: 3.597814]\n",
      "epoch:26 step:20366 [D loss: 0.039010, acc.: 100.00%] [G loss: 3.711779]\n",
      "epoch:26 step:20367 [D loss: 0.099305, acc.: 96.09%] [G loss: 3.561023]\n",
      "epoch:26 step:20368 [D loss: 0.081087, acc.: 98.44%] [G loss: 3.661375]\n",
      "epoch:26 step:20369 [D loss: 0.182488, acc.: 96.09%] [G loss: 3.053548]\n",
      "epoch:26 step:20370 [D loss: 0.050692, acc.: 100.00%] [G loss: 3.195642]\n",
      "epoch:26 step:20371 [D loss: 0.155862, acc.: 96.88%] [G loss: 3.189670]\n",
      "epoch:26 step:20372 [D loss: 0.176645, acc.: 95.31%] [G loss: 2.580016]\n",
      "epoch:26 step:20373 [D loss: 0.088021, acc.: 98.44%] [G loss: 4.802583]\n",
      "epoch:26 step:20374 [D loss: 0.174436, acc.: 92.19%] [G loss: 2.342118]\n",
      "epoch:26 step:20375 [D loss: 0.081864, acc.: 97.66%] [G loss: 2.541595]\n",
      "epoch:26 step:20376 [D loss: 0.023742, acc.: 100.00%] [G loss: 4.147336]\n",
      "epoch:26 step:20377 [D loss: 0.049646, acc.: 98.44%] [G loss: 4.099124]\n",
      "epoch:26 step:20378 [D loss: 0.058838, acc.: 100.00%] [G loss: 3.787957]\n",
      "epoch:26 step:20379 [D loss: 0.076061, acc.: 98.44%] [G loss: 3.884251]\n",
      "epoch:26 step:20380 [D loss: 0.009951, acc.: 100.00%] [G loss: 3.906732]\n",
      "epoch:26 step:20381 [D loss: 0.038716, acc.: 100.00%] [G loss: 3.310865]\n",
      "epoch:26 step:20382 [D loss: 0.046848, acc.: 100.00%] [G loss: 3.074521]\n",
      "epoch:26 step:20383 [D loss: 0.083226, acc.: 99.22%] [G loss: 2.318156]\n",
      "epoch:26 step:20384 [D loss: 0.028905, acc.: 100.00%] [G loss: 2.543366]\n",
      "epoch:26 step:20385 [D loss: 0.047338, acc.: 99.22%] [G loss: 2.424061]\n",
      "epoch:26 step:20386 [D loss: 0.100288, acc.: 98.44%] [G loss: 3.261895]\n",
      "epoch:26 step:20387 [D loss: 0.195643, acc.: 92.97%] [G loss: 6.462325]\n",
      "epoch:26 step:20388 [D loss: 0.055329, acc.: 99.22%] [G loss: 6.525529]\n",
      "epoch:26 step:20389 [D loss: 0.152726, acc.: 94.53%] [G loss: 3.407370]\n",
      "epoch:26 step:20390 [D loss: 0.028650, acc.: 100.00%] [G loss: 3.007797]\n",
      "epoch:26 step:20391 [D loss: 0.033416, acc.: 99.22%] [G loss: 4.194383]\n",
      "epoch:26 step:20392 [D loss: 0.014592, acc.: 100.00%] [G loss: 4.224846]\n",
      "epoch:26 step:20393 [D loss: 0.074545, acc.: 99.22%] [G loss: 4.806337]\n",
      "epoch:26 step:20394 [D loss: 0.012591, acc.: 100.00%] [G loss: 6.065382]\n",
      "epoch:26 step:20395 [D loss: 0.013793, acc.: 100.00%] [G loss: 4.921770]\n",
      "epoch:26 step:20396 [D loss: 0.367581, acc.: 81.25%] [G loss: 4.723603]\n",
      "epoch:26 step:20397 [D loss: 0.004438, acc.: 100.00%] [G loss: 5.896595]\n",
      "epoch:26 step:20398 [D loss: 0.002339, acc.: 100.00%] [G loss: 5.893408]\n",
      "epoch:26 step:20399 [D loss: 0.004741, acc.: 100.00%] [G loss: 6.130395]\n",
      "epoch:26 step:20400 [D loss: 0.003032, acc.: 100.00%] [G loss: 5.974077]\n",
      "##############\n",
      "[0.96339179 1.0067743  1.01926492 0.97505128 2.10534105 1.11471702\n",
      " 2.11654226 2.09735397 0.8607287  1.08164946]\n",
      "##########\n",
      "epoch:26 step:20401 [D loss: 0.003048, acc.: 100.00%] [G loss: 5.443647]\n",
      "epoch:26 step:20402 [D loss: 0.004970, acc.: 100.00%] [G loss: 5.767168]\n",
      "epoch:26 step:20403 [D loss: 0.002649, acc.: 100.00%] [G loss: 4.962790]\n",
      "epoch:26 step:20404 [D loss: 0.006918, acc.: 100.00%] [G loss: 4.507787]\n",
      "epoch:26 step:20405 [D loss: 0.020359, acc.: 100.00%] [G loss: 4.384405]\n",
      "epoch:26 step:20406 [D loss: 0.006570, acc.: 100.00%] [G loss: 4.249158]\n",
      "epoch:26 step:20407 [D loss: 0.011759, acc.: 100.00%] [G loss: 4.095314]\n",
      "epoch:26 step:20408 [D loss: 0.014930, acc.: 100.00%] [G loss: 4.570531]\n",
      "epoch:26 step:20409 [D loss: 0.028403, acc.: 100.00%] [G loss: 3.890810]\n",
      "epoch:26 step:20410 [D loss: 0.007163, acc.: 100.00%] [G loss: 4.472188]\n",
      "epoch:26 step:20411 [D loss: 0.061793, acc.: 98.44%] [G loss: 4.650902]\n",
      "epoch:26 step:20412 [D loss: 0.013464, acc.: 100.00%] [G loss: 4.584285]\n",
      "epoch:26 step:20413 [D loss: 0.067507, acc.: 98.44%] [G loss: 3.845095]\n",
      "epoch:26 step:20414 [D loss: 0.035379, acc.: 100.00%] [G loss: 3.527470]\n",
      "epoch:26 step:20415 [D loss: 0.032617, acc.: 100.00%] [G loss: 4.999417]\n",
      "epoch:26 step:20416 [D loss: 0.028199, acc.: 99.22%] [G loss: 5.231996]\n",
      "epoch:26 step:20417 [D loss: 0.006097, acc.: 100.00%] [G loss: 5.231622]\n",
      "epoch:26 step:20418 [D loss: 0.002639, acc.: 100.00%] [G loss: 4.754136]\n",
      "epoch:26 step:20419 [D loss: 0.011689, acc.: 100.00%] [G loss: 4.925904]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:26 step:20420 [D loss: 0.007265, acc.: 100.00%] [G loss: 4.726746]\n",
      "epoch:26 step:20421 [D loss: 0.006326, acc.: 100.00%] [G loss: 4.292336]\n",
      "epoch:26 step:20422 [D loss: 0.459387, acc.: 84.38%] [G loss: 6.963909]\n",
      "epoch:26 step:20423 [D loss: 0.033693, acc.: 98.44%] [G loss: 8.168856]\n",
      "epoch:26 step:20424 [D loss: 0.155493, acc.: 92.97%] [G loss: 6.692348]\n",
      "epoch:26 step:20425 [D loss: 0.003260, acc.: 100.00%] [G loss: 5.340436]\n",
      "epoch:26 step:20426 [D loss: 0.021810, acc.: 99.22%] [G loss: 5.738618]\n",
      "epoch:26 step:20427 [D loss: 0.004535, acc.: 100.00%] [G loss: 4.576474]\n",
      "epoch:26 step:20428 [D loss: 0.006954, acc.: 100.00%] [G loss: 5.457427]\n",
      "epoch:26 step:20429 [D loss: 0.017955, acc.: 100.00%] [G loss: 4.982323]\n",
      "epoch:26 step:20430 [D loss: 0.017254, acc.: 100.00%] [G loss: 6.841059]\n",
      "epoch:26 step:20431 [D loss: 0.009970, acc.: 100.00%] [G loss: 4.957025]\n",
      "epoch:26 step:20432 [D loss: 0.006355, acc.: 100.00%] [G loss: 5.364587]\n",
      "epoch:26 step:20433 [D loss: 0.003875, acc.: 100.00%] [G loss: 5.499194]\n",
      "epoch:26 step:20434 [D loss: 0.014314, acc.: 100.00%] [G loss: 4.745846]\n",
      "epoch:26 step:20435 [D loss: 0.018225, acc.: 100.00%] [G loss: 5.824483]\n",
      "epoch:26 step:20436 [D loss: 0.008168, acc.: 100.00%] [G loss: 3.698499]\n",
      "epoch:26 step:20437 [D loss: 0.005489, acc.: 100.00%] [G loss: 5.189911]\n",
      "epoch:26 step:20438 [D loss: 1.267548, acc.: 45.31%] [G loss: 10.080003]\n",
      "epoch:26 step:20439 [D loss: 3.907584, acc.: 50.00%] [G loss: 7.035745]\n",
      "epoch:26 step:20440 [D loss: 2.470932, acc.: 50.78%] [G loss: 3.170351]\n",
      "epoch:26 step:20441 [D loss: 0.257828, acc.: 86.72%] [G loss: 2.434407]\n",
      "epoch:26 step:20442 [D loss: 0.179678, acc.: 95.31%] [G loss: 3.319582]\n",
      "epoch:26 step:20443 [D loss: 0.086596, acc.: 97.66%] [G loss: 3.231728]\n",
      "epoch:26 step:20444 [D loss: 0.125572, acc.: 97.66%] [G loss: 2.928828]\n",
      "epoch:26 step:20445 [D loss: 0.068855, acc.: 99.22%] [G loss: 3.245459]\n",
      "epoch:26 step:20446 [D loss: 0.093369, acc.: 98.44%] [G loss: 3.608644]\n",
      "epoch:26 step:20447 [D loss: 0.198483, acc.: 93.75%] [G loss: 3.066925]\n",
      "epoch:26 step:20448 [D loss: 0.059314, acc.: 100.00%] [G loss: 2.397378]\n",
      "epoch:26 step:20449 [D loss: 0.081367, acc.: 99.22%] [G loss: 2.551923]\n",
      "epoch:26 step:20450 [D loss: 0.060850, acc.: 98.44%] [G loss: 3.082922]\n",
      "epoch:26 step:20451 [D loss: 0.071670, acc.: 98.44%] [G loss: 2.561578]\n",
      "epoch:26 step:20452 [D loss: 0.053750, acc.: 99.22%] [G loss: 2.767387]\n",
      "epoch:26 step:20453 [D loss: 0.038803, acc.: 100.00%] [G loss: 3.493825]\n",
      "epoch:26 step:20454 [D loss: 0.118293, acc.: 99.22%] [G loss: 3.546772]\n",
      "epoch:26 step:20455 [D loss: 0.084820, acc.: 97.66%] [G loss: 3.059072]\n",
      "epoch:26 step:20456 [D loss: 0.677477, acc.: 64.84%] [G loss: 4.639985]\n",
      "epoch:26 step:20457 [D loss: 0.099863, acc.: 95.31%] [G loss: 4.830891]\n",
      "epoch:26 step:20458 [D loss: 0.433952, acc.: 78.12%] [G loss: 2.422909]\n",
      "epoch:26 step:20459 [D loss: 0.103786, acc.: 97.66%] [G loss: 2.175077]\n",
      "epoch:26 step:20460 [D loss: 0.036463, acc.: 100.00%] [G loss: 3.692361]\n",
      "epoch:26 step:20461 [D loss: 0.042096, acc.: 99.22%] [G loss: 3.608770]\n",
      "epoch:26 step:20462 [D loss: 0.046815, acc.: 100.00%] [G loss: 4.728892]\n",
      "epoch:26 step:20463 [D loss: 0.093988, acc.: 97.66%] [G loss: 3.001575]\n",
      "epoch:26 step:20464 [D loss: 0.044186, acc.: 100.00%] [G loss: 3.952344]\n",
      "epoch:26 step:20465 [D loss: 0.014605, acc.: 100.00%] [G loss: 4.085793]\n",
      "epoch:26 step:20466 [D loss: 0.081149, acc.: 99.22%] [G loss: 3.667828]\n",
      "epoch:26 step:20467 [D loss: 0.028213, acc.: 100.00%] [G loss: 3.808800]\n",
      "epoch:26 step:20468 [D loss: 0.018615, acc.: 100.00%] [G loss: 3.518160]\n",
      "epoch:26 step:20469 [D loss: 0.021874, acc.: 100.00%] [G loss: 3.659765]\n",
      "epoch:26 step:20470 [D loss: 0.029136, acc.: 100.00%] [G loss: 3.015209]\n",
      "epoch:26 step:20471 [D loss: 0.038042, acc.: 99.22%] [G loss: 2.397980]\n",
      "epoch:26 step:20472 [D loss: 0.094887, acc.: 100.00%] [G loss: 3.785871]\n",
      "epoch:26 step:20473 [D loss: 0.060635, acc.: 100.00%] [G loss: 3.839307]\n",
      "epoch:26 step:20474 [D loss: 0.016403, acc.: 100.00%] [G loss: 3.937841]\n",
      "epoch:26 step:20475 [D loss: 0.035734, acc.: 100.00%] [G loss: 3.773240]\n",
      "epoch:26 step:20476 [D loss: 0.017918, acc.: 100.00%] [G loss: 3.705311]\n",
      "epoch:26 step:20477 [D loss: 0.044102, acc.: 100.00%] [G loss: 3.917453]\n",
      "epoch:26 step:20478 [D loss: 0.090278, acc.: 97.66%] [G loss: 3.068824]\n",
      "epoch:26 step:20479 [D loss: 0.035237, acc.: 100.00%] [G loss: 3.002976]\n",
      "epoch:26 step:20480 [D loss: 0.044672, acc.: 99.22%] [G loss: 2.636123]\n",
      "epoch:26 step:20481 [D loss: 0.043632, acc.: 100.00%] [G loss: 3.818997]\n",
      "epoch:26 step:20482 [D loss: 0.106478, acc.: 98.44%] [G loss: 2.973743]\n",
      "epoch:26 step:20483 [D loss: 0.032996, acc.: 100.00%] [G loss: 3.047636]\n",
      "epoch:26 step:20484 [D loss: 0.030106, acc.: 100.00%] [G loss: 3.375063]\n",
      "epoch:26 step:20485 [D loss: 0.226417, acc.: 90.62%] [G loss: 4.490883]\n",
      "epoch:26 step:20486 [D loss: 0.062110, acc.: 97.66%] [G loss: 4.249497]\n",
      "epoch:26 step:20487 [D loss: 0.080700, acc.: 97.66%] [G loss: 3.274587]\n",
      "epoch:26 step:20488 [D loss: 0.014222, acc.: 100.00%] [G loss: 3.385686]\n",
      "epoch:26 step:20489 [D loss: 0.074861, acc.: 98.44%] [G loss: 4.873256]\n",
      "epoch:26 step:20490 [D loss: 0.051905, acc.: 99.22%] [G loss: 4.830513]\n",
      "epoch:26 step:20491 [D loss: 0.047259, acc.: 99.22%] [G loss: 4.309148]\n",
      "epoch:26 step:20492 [D loss: 0.008774, acc.: 100.00%] [G loss: 4.124559]\n",
      "epoch:26 step:20493 [D loss: 0.119613, acc.: 96.88%] [G loss: 4.023615]\n",
      "epoch:26 step:20494 [D loss: 0.083784, acc.: 98.44%] [G loss: 4.087117]\n",
      "epoch:26 step:20495 [D loss: 0.013801, acc.: 100.00%] [G loss: 4.572018]\n",
      "epoch:26 step:20496 [D loss: 0.013580, acc.: 100.00%] [G loss: 4.382648]\n",
      "epoch:26 step:20497 [D loss: 0.059638, acc.: 99.22%] [G loss: 4.577902]\n",
      "epoch:26 step:20498 [D loss: 0.020472, acc.: 100.00%] [G loss: 3.565979]\n",
      "epoch:26 step:20499 [D loss: 0.032811, acc.: 100.00%] [G loss: 4.313322]\n",
      "epoch:26 step:20500 [D loss: 0.045526, acc.: 99.22%] [G loss: 5.091301]\n",
      "epoch:26 step:20501 [D loss: 0.024608, acc.: 100.00%] [G loss: 4.442345]\n",
      "epoch:26 step:20502 [D loss: 0.090042, acc.: 96.88%] [G loss: 3.117757]\n",
      "epoch:26 step:20503 [D loss: 0.088792, acc.: 96.09%] [G loss: 4.768195]\n",
      "epoch:26 step:20504 [D loss: 0.247252, acc.: 88.28%] [G loss: 2.545504]\n",
      "epoch:26 step:20505 [D loss: 0.027551, acc.: 100.00%] [G loss: 3.000430]\n",
      "epoch:26 step:20506 [D loss: 0.014952, acc.: 100.00%] [G loss: 3.945743]\n",
      "epoch:26 step:20507 [D loss: 0.022265, acc.: 100.00%] [G loss: 3.958366]\n",
      "epoch:26 step:20508 [D loss: 0.019840, acc.: 100.00%] [G loss: 4.588896]\n",
      "epoch:26 step:20509 [D loss: 0.014456, acc.: 100.00%] [G loss: 4.601778]\n",
      "epoch:26 step:20510 [D loss: 0.034593, acc.: 100.00%] [G loss: 4.596879]\n",
      "epoch:26 step:20511 [D loss: 0.101190, acc.: 99.22%] [G loss: 2.369235]\n",
      "epoch:26 step:20512 [D loss: 0.053428, acc.: 99.22%] [G loss: 3.941629]\n",
      "epoch:26 step:20513 [D loss: 0.005768, acc.: 100.00%] [G loss: 4.441002]\n",
      "epoch:26 step:20514 [D loss: 0.016609, acc.: 100.00%] [G loss: 4.395139]\n",
      "epoch:26 step:20515 [D loss: 0.393525, acc.: 82.81%] [G loss: 5.283710]\n",
      "epoch:26 step:20516 [D loss: 0.060643, acc.: 97.66%] [G loss: 5.811696]\n",
      "epoch:26 step:20517 [D loss: 0.005491, acc.: 100.00%] [G loss: 5.770550]\n",
      "epoch:26 step:20518 [D loss: 0.134655, acc.: 95.31%] [G loss: 3.085173]\n",
      "epoch:26 step:20519 [D loss: 0.324673, acc.: 84.38%] [G loss: 7.691549]\n",
      "epoch:26 step:20520 [D loss: 1.594025, acc.: 50.78%] [G loss: 1.717070]\n",
      "epoch:26 step:20521 [D loss: 0.525735, acc.: 75.00%] [G loss: 5.318483]\n",
      "epoch:26 step:20522 [D loss: 0.502488, acc.: 74.22%] [G loss: 4.414336]\n",
      "epoch:26 step:20523 [D loss: 0.104216, acc.: 97.66%] [G loss: 3.624724]\n",
      "epoch:26 step:20524 [D loss: 0.072075, acc.: 97.66%] [G loss: 4.533549]\n",
      "epoch:26 step:20525 [D loss: 0.021501, acc.: 99.22%] [G loss: 4.799339]\n",
      "epoch:26 step:20526 [D loss: 0.017924, acc.: 100.00%] [G loss: 4.727353]\n",
      "epoch:26 step:20527 [D loss: 0.009684, acc.: 100.00%] [G loss: 4.163257]\n",
      "epoch:26 step:20528 [D loss: 0.011139, acc.: 100.00%] [G loss: 3.950869]\n",
      "epoch:26 step:20529 [D loss: 0.039275, acc.: 100.00%] [G loss: 5.079410]\n",
      "epoch:26 step:20530 [D loss: 0.045243, acc.: 100.00%] [G loss: 4.113301]\n",
      "epoch:26 step:20531 [D loss: 0.010717, acc.: 100.00%] [G loss: 4.454349]\n",
      "epoch:26 step:20532 [D loss: 0.019167, acc.: 100.00%] [G loss: 5.156713]\n",
      "epoch:26 step:20533 [D loss: 0.038793, acc.: 100.00%] [G loss: 3.435161]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:26 step:20534 [D loss: 0.023017, acc.: 100.00%] [G loss: 4.261293]\n",
      "epoch:26 step:20535 [D loss: 0.048674, acc.: 99.22%] [G loss: 3.822643]\n",
      "epoch:26 step:20536 [D loss: 0.027748, acc.: 100.00%] [G loss: 3.293748]\n",
      "epoch:26 step:20537 [D loss: 0.031495, acc.: 100.00%] [G loss: 4.079093]\n",
      "epoch:26 step:20538 [D loss: 0.111587, acc.: 98.44%] [G loss: 2.254798]\n",
      "epoch:26 step:20539 [D loss: 0.196095, acc.: 92.97%] [G loss: 5.113545]\n",
      "epoch:26 step:20540 [D loss: 0.180923, acc.: 91.41%] [G loss: 4.540600]\n",
      "epoch:26 step:20541 [D loss: 0.015105, acc.: 100.00%] [G loss: 4.001455]\n",
      "epoch:26 step:20542 [D loss: 0.006770, acc.: 100.00%] [G loss: 4.184248]\n",
      "epoch:26 step:20543 [D loss: 0.043887, acc.: 100.00%] [G loss: 4.438373]\n",
      "epoch:26 step:20544 [D loss: 0.005234, acc.: 100.00%] [G loss: 4.402870]\n",
      "epoch:26 step:20545 [D loss: 0.006898, acc.: 100.00%] [G loss: 4.999867]\n",
      "epoch:26 step:20546 [D loss: 0.305817, acc.: 91.41%] [G loss: 1.408935]\n",
      "epoch:26 step:20547 [D loss: 0.322451, acc.: 83.59%] [G loss: 7.040751]\n",
      "epoch:26 step:20548 [D loss: 0.099590, acc.: 96.88%] [G loss: 7.118594]\n",
      "epoch:26 step:20549 [D loss: 0.116890, acc.: 95.31%] [G loss: 6.179994]\n",
      "epoch:26 step:20550 [D loss: 0.002580, acc.: 100.00%] [G loss: 5.629269]\n",
      "epoch:26 step:20551 [D loss: 0.003939, acc.: 100.00%] [G loss: 5.010878]\n",
      "epoch:26 step:20552 [D loss: 0.004638, acc.: 100.00%] [G loss: 4.894118]\n",
      "epoch:26 step:20553 [D loss: 0.003009, acc.: 100.00%] [G loss: 4.082023]\n",
      "epoch:26 step:20554 [D loss: 0.016842, acc.: 99.22%] [G loss: 4.989402]\n",
      "epoch:26 step:20555 [D loss: 0.006705, acc.: 100.00%] [G loss: 4.925479]\n",
      "epoch:26 step:20556 [D loss: 0.170169, acc.: 96.88%] [G loss: 3.148277]\n",
      "epoch:26 step:20557 [D loss: 0.038922, acc.: 99.22%] [G loss: 3.722501]\n",
      "epoch:26 step:20558 [D loss: 0.027233, acc.: 100.00%] [G loss: 5.006294]\n",
      "epoch:26 step:20559 [D loss: 0.006755, acc.: 100.00%] [G loss: 5.141585]\n",
      "epoch:26 step:20560 [D loss: 0.002060, acc.: 100.00%] [G loss: 5.621321]\n",
      "epoch:26 step:20561 [D loss: 0.002481, acc.: 100.00%] [G loss: 5.299103]\n",
      "epoch:26 step:20562 [D loss: 0.004808, acc.: 100.00%] [G loss: 4.648964]\n",
      "epoch:26 step:20563 [D loss: 0.005617, acc.: 100.00%] [G loss: 4.154469]\n",
      "epoch:26 step:20564 [D loss: 0.004210, acc.: 100.00%] [G loss: 4.020461]\n",
      "epoch:26 step:20565 [D loss: 0.003299, acc.: 100.00%] [G loss: 3.777285]\n",
      "epoch:26 step:20566 [D loss: 0.034718, acc.: 100.00%] [G loss: 4.992670]\n",
      "epoch:26 step:20567 [D loss: 0.059831, acc.: 100.00%] [G loss: 3.278725]\n",
      "epoch:26 step:20568 [D loss: 0.039979, acc.: 100.00%] [G loss: 4.508539]\n",
      "epoch:26 step:20569 [D loss: 0.004735, acc.: 100.00%] [G loss: 4.285544]\n",
      "epoch:26 step:20570 [D loss: 0.027384, acc.: 99.22%] [G loss: 4.609658]\n",
      "epoch:26 step:20571 [D loss: 0.024642, acc.: 100.00%] [G loss: 3.794575]\n",
      "epoch:26 step:20572 [D loss: 0.011117, acc.: 100.00%] [G loss: 3.774710]\n",
      "epoch:26 step:20573 [D loss: 0.068577, acc.: 99.22%] [G loss: 5.013717]\n",
      "epoch:26 step:20574 [D loss: 0.008434, acc.: 100.00%] [G loss: 4.903342]\n",
      "epoch:26 step:20575 [D loss: 0.042477, acc.: 98.44%] [G loss: 4.297478]\n",
      "epoch:26 step:20576 [D loss: 0.039414, acc.: 99.22%] [G loss: 3.580637]\n",
      "epoch:26 step:20577 [D loss: 0.010780, acc.: 100.00%] [G loss: 2.945536]\n",
      "epoch:26 step:20578 [D loss: 0.028591, acc.: 100.00%] [G loss: 3.664009]\n",
      "epoch:26 step:20579 [D loss: 0.066231, acc.: 100.00%] [G loss: 4.820781]\n",
      "epoch:26 step:20580 [D loss: 0.034746, acc.: 100.00%] [G loss: 5.213313]\n",
      "epoch:26 step:20581 [D loss: 0.011542, acc.: 100.00%] [G loss: 4.783777]\n",
      "epoch:26 step:20582 [D loss: 0.012105, acc.: 100.00%] [G loss: 4.951527]\n",
      "epoch:26 step:20583 [D loss: 0.010358, acc.: 100.00%] [G loss: 3.677026]\n",
      "epoch:26 step:20584 [D loss: 0.080417, acc.: 98.44%] [G loss: 5.754658]\n",
      "epoch:26 step:20585 [D loss: 0.242685, acc.: 90.62%] [G loss: 1.671774]\n",
      "epoch:26 step:20586 [D loss: 0.096740, acc.: 98.44%] [G loss: 4.399726]\n",
      "epoch:26 step:20587 [D loss: 0.005016, acc.: 100.00%] [G loss: 5.166172]\n",
      "epoch:26 step:20588 [D loss: 0.005283, acc.: 100.00%] [G loss: 4.898333]\n",
      "epoch:26 step:20589 [D loss: 0.016941, acc.: 99.22%] [G loss: 4.347063]\n",
      "epoch:26 step:20590 [D loss: 0.030844, acc.: 99.22%] [G loss: 5.109409]\n",
      "epoch:26 step:20591 [D loss: 0.008916, acc.: 100.00%] [G loss: 4.332511]\n",
      "epoch:26 step:20592 [D loss: 0.023211, acc.: 100.00%] [G loss: 3.586221]\n",
      "epoch:26 step:20593 [D loss: 0.007424, acc.: 100.00%] [G loss: 3.240386]\n",
      "epoch:26 step:20594 [D loss: 0.013379, acc.: 100.00%] [G loss: 3.504718]\n",
      "epoch:26 step:20595 [D loss: 0.012630, acc.: 100.00%] [G loss: 3.038668]\n",
      "epoch:26 step:20596 [D loss: 0.007281, acc.: 100.00%] [G loss: 3.505403]\n",
      "epoch:26 step:20597 [D loss: 0.048553, acc.: 99.22%] [G loss: 4.378074]\n",
      "epoch:26 step:20598 [D loss: 0.011773, acc.: 100.00%] [G loss: 4.833824]\n",
      "epoch:26 step:20599 [D loss: 0.010087, acc.: 100.00%] [G loss: 5.320191]\n",
      "epoch:26 step:20600 [D loss: 0.076016, acc.: 98.44%] [G loss: 3.196985]\n",
      "##############\n",
      "[0.98911863 1.00459623 0.85363501 1.01163086 1.11473092 2.12595412\n",
      " 0.95487504 2.11233943 1.0312547  0.99234526]\n",
      "##########\n",
      "epoch:26 step:20601 [D loss: 0.257596, acc.: 85.16%] [G loss: 8.333809]\n",
      "epoch:26 step:20602 [D loss: 1.699499, acc.: 50.00%] [G loss: 2.138773]\n",
      "epoch:26 step:20603 [D loss: 2.700346, acc.: 50.78%] [G loss: 8.513684]\n",
      "epoch:26 step:20604 [D loss: 2.957278, acc.: 50.00%] [G loss: 6.302623]\n",
      "epoch:26 step:20605 [D loss: 2.368398, acc.: 50.00%] [G loss: 4.026596]\n",
      "epoch:26 step:20606 [D loss: 1.330339, acc.: 51.56%] [G loss: 1.901636]\n",
      "epoch:26 step:20607 [D loss: 0.385713, acc.: 82.81%] [G loss: 1.490114]\n",
      "epoch:26 step:20608 [D loss: 0.471622, acc.: 76.56%] [G loss: 2.778098]\n",
      "epoch:26 step:20609 [D loss: 0.260121, acc.: 89.06%] [G loss: 2.272759]\n",
      "epoch:26 step:20610 [D loss: 0.339589, acc.: 85.16%] [G loss: 1.644877]\n",
      "epoch:26 step:20611 [D loss: 0.237898, acc.: 93.75%] [G loss: 1.775020]\n",
      "epoch:26 step:20612 [D loss: 0.327944, acc.: 88.28%] [G loss: 2.474844]\n",
      "epoch:26 step:20613 [D loss: 0.093906, acc.: 100.00%] [G loss: 2.246657]\n",
      "epoch:26 step:20614 [D loss: 0.207822, acc.: 94.53%] [G loss: 1.824436]\n",
      "epoch:26 step:20615 [D loss: 0.280463, acc.: 89.84%] [G loss: 2.559365]\n",
      "epoch:26 step:20616 [D loss: 0.186106, acc.: 95.31%] [G loss: 2.711498]\n",
      "epoch:26 step:20617 [D loss: 0.391377, acc.: 87.50%] [G loss: 1.897972]\n",
      "epoch:26 step:20618 [D loss: 0.315555, acc.: 90.62%] [G loss: 2.247866]\n",
      "epoch:26 step:20619 [D loss: 0.216748, acc.: 95.31%] [G loss: 2.465055]\n",
      "epoch:26 step:20620 [D loss: 0.126613, acc.: 99.22%] [G loss: 2.320704]\n",
      "epoch:26 step:20621 [D loss: 0.382775, acc.: 85.16%] [G loss: 1.409970]\n",
      "epoch:26 step:20622 [D loss: 0.341810, acc.: 84.38%] [G loss: 2.451868]\n",
      "epoch:26 step:20623 [D loss: 0.505938, acc.: 75.78%] [G loss: 2.667517]\n",
      "epoch:26 step:20624 [D loss: 0.128165, acc.: 98.44%] [G loss: 1.863081]\n",
      "epoch:26 step:20625 [D loss: 0.400711, acc.: 82.81%] [G loss: 2.178919]\n",
      "epoch:26 step:20626 [D loss: 1.136208, acc.: 35.94%] [G loss: 4.009747]\n",
      "epoch:26 step:20627 [D loss: 0.428570, acc.: 74.22%] [G loss: 4.336957]\n",
      "epoch:26 step:20628 [D loss: 0.271520, acc.: 87.50%] [G loss: 2.844503]\n",
      "epoch:26 step:20629 [D loss: 0.270866, acc.: 88.28%] [G loss: 2.867613]\n",
      "epoch:26 step:20630 [D loss: 0.144015, acc.: 96.88%] [G loss: 3.229320]\n",
      "epoch:26 step:20631 [D loss: 0.203887, acc.: 92.97%] [G loss: 2.616963]\n",
      "epoch:26 step:20632 [D loss: 0.075008, acc.: 100.00%] [G loss: 2.631160]\n",
      "epoch:26 step:20633 [D loss: 0.045417, acc.: 100.00%] [G loss: 1.935651]\n",
      "epoch:26 step:20634 [D loss: 0.110977, acc.: 99.22%] [G loss: 3.289514]\n",
      "epoch:26 step:20635 [D loss: 0.047953, acc.: 100.00%] [G loss: 3.037978]\n",
      "epoch:26 step:20636 [D loss: 0.094429, acc.: 99.22%] [G loss: 2.868295]\n",
      "epoch:26 step:20637 [D loss: 0.127847, acc.: 96.09%] [G loss: 1.985840]\n",
      "epoch:26 step:20638 [D loss: 0.114274, acc.: 96.88%] [G loss: 2.063250]\n",
      "epoch:26 step:20639 [D loss: 0.089547, acc.: 98.44%] [G loss: 2.361767]\n",
      "epoch:26 step:20640 [D loss: 0.077843, acc.: 99.22%] [G loss: 1.688534]\n",
      "epoch:26 step:20641 [D loss: 0.180347, acc.: 96.09%] [G loss: 2.795007]\n",
      "epoch:26 step:20642 [D loss: 0.070540, acc.: 98.44%] [G loss: 2.211477]\n",
      "epoch:26 step:20643 [D loss: 0.078911, acc.: 99.22%] [G loss: 1.221899]\n",
      "epoch:26 step:20644 [D loss: 0.080761, acc.: 99.22%] [G loss: 0.950169]\n",
      "epoch:26 step:20645 [D loss: 0.084108, acc.: 97.66%] [G loss: 1.473634]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:26 step:20646 [D loss: 0.480551, acc.: 73.44%] [G loss: 4.352040]\n",
      "epoch:26 step:20647 [D loss: 0.456009, acc.: 74.22%] [G loss: 3.909015]\n",
      "epoch:26 step:20648 [D loss: 0.331407, acc.: 84.38%] [G loss: 2.441310]\n",
      "epoch:26 step:20649 [D loss: 0.120447, acc.: 95.31%] [G loss: 1.784049]\n",
      "epoch:26 step:20650 [D loss: 0.116340, acc.: 97.66%] [G loss: 2.560298]\n",
      "epoch:26 step:20651 [D loss: 0.078382, acc.: 99.22%] [G loss: 3.091480]\n",
      "epoch:26 step:20652 [D loss: 0.128360, acc.: 96.88%] [G loss: 1.732936]\n",
      "epoch:26 step:20653 [D loss: 0.041119, acc.: 100.00%] [G loss: 1.928721]\n",
      "epoch:26 step:20654 [D loss: 0.129249, acc.: 97.66%] [G loss: 3.166134]\n",
      "epoch:26 step:20655 [D loss: 0.099254, acc.: 96.88%] [G loss: 3.185803]\n",
      "epoch:26 step:20656 [D loss: 0.190070, acc.: 93.75%] [G loss: 2.806497]\n",
      "epoch:26 step:20657 [D loss: 0.212693, acc.: 93.75%] [G loss: 2.717049]\n",
      "epoch:26 step:20658 [D loss: 0.053943, acc.: 100.00%] [G loss: 3.830441]\n",
      "epoch:26 step:20659 [D loss: 0.100637, acc.: 97.66%] [G loss: 3.612579]\n",
      "epoch:26 step:20660 [D loss: 0.044408, acc.: 99.22%] [G loss: 2.404583]\n",
      "epoch:26 step:20661 [D loss: 0.068081, acc.: 99.22%] [G loss: 3.234942]\n",
      "epoch:26 step:20662 [D loss: 0.036471, acc.: 100.00%] [G loss: 2.507925]\n",
      "epoch:26 step:20663 [D loss: 0.050919, acc.: 98.44%] [G loss: 2.990732]\n",
      "epoch:26 step:20664 [D loss: 0.025286, acc.: 100.00%] [G loss: 3.373240]\n",
      "epoch:26 step:20665 [D loss: 0.889358, acc.: 60.16%] [G loss: 6.252628]\n",
      "epoch:26 step:20666 [D loss: 0.484246, acc.: 74.22%] [G loss: 5.551381]\n",
      "epoch:26 step:20667 [D loss: 0.177506, acc.: 92.97%] [G loss: 3.761963]\n",
      "epoch:26 step:20668 [D loss: 0.031186, acc.: 99.22%] [G loss: 2.980398]\n",
      "epoch:26 step:20669 [D loss: 0.086653, acc.: 96.88%] [G loss: 3.289481]\n",
      "epoch:26 step:20670 [D loss: 0.027577, acc.: 100.00%] [G loss: 3.477221]\n",
      "epoch:26 step:20671 [D loss: 0.083017, acc.: 98.44%] [G loss: 4.009557]\n",
      "epoch:26 step:20672 [D loss: 0.009024, acc.: 100.00%] [G loss: 3.778093]\n",
      "epoch:26 step:20673 [D loss: 0.022333, acc.: 100.00%] [G loss: 3.679389]\n",
      "epoch:26 step:20674 [D loss: 0.140268, acc.: 93.75%] [G loss: 2.950269]\n",
      "epoch:26 step:20675 [D loss: 0.107988, acc.: 98.44%] [G loss: 3.742582]\n",
      "epoch:26 step:20676 [D loss: 0.212222, acc.: 92.97%] [G loss: 2.958026]\n",
      "epoch:26 step:20677 [D loss: 0.023267, acc.: 100.00%] [G loss: 2.990456]\n",
      "epoch:26 step:20678 [D loss: 0.097261, acc.: 97.66%] [G loss: 4.443796]\n",
      "epoch:26 step:20679 [D loss: 0.046703, acc.: 100.00%] [G loss: 4.669149]\n",
      "epoch:26 step:20680 [D loss: 0.545011, acc.: 71.09%] [G loss: 3.299152]\n",
      "epoch:26 step:20681 [D loss: 0.061915, acc.: 98.44%] [G loss: 4.348197]\n",
      "epoch:26 step:20682 [D loss: 0.037317, acc.: 100.00%] [G loss: 3.213440]\n",
      "epoch:26 step:20683 [D loss: 0.040148, acc.: 100.00%] [G loss: 3.603459]\n",
      "epoch:26 step:20684 [D loss: 0.091071, acc.: 97.66%] [G loss: 3.102611]\n",
      "epoch:26 step:20685 [D loss: 0.015897, acc.: 100.00%] [G loss: 3.853194]\n",
      "epoch:26 step:20686 [D loss: 0.047894, acc.: 99.22%] [G loss: 2.936771]\n",
      "epoch:26 step:20687 [D loss: 0.050496, acc.: 100.00%] [G loss: 3.138927]\n",
      "epoch:26 step:20688 [D loss: 0.159666, acc.: 97.66%] [G loss: 3.516448]\n",
      "epoch:26 step:20689 [D loss: 0.027167, acc.: 100.00%] [G loss: 3.542243]\n",
      "epoch:26 step:20690 [D loss: 0.034095, acc.: 100.00%] [G loss: 2.678511]\n",
      "epoch:26 step:20691 [D loss: 1.015390, acc.: 53.12%] [G loss: 6.247333]\n",
      "epoch:26 step:20692 [D loss: 1.117453, acc.: 57.81%] [G loss: 3.655627]\n",
      "epoch:26 step:20693 [D loss: 0.130796, acc.: 94.53%] [G loss: 4.395634]\n",
      "epoch:26 step:20694 [D loss: 0.032066, acc.: 99.22%] [G loss: 5.316993]\n",
      "epoch:26 step:20695 [D loss: 0.126099, acc.: 95.31%] [G loss: 4.781559]\n",
      "epoch:26 step:20696 [D loss: 0.085260, acc.: 98.44%] [G loss: 4.907463]\n",
      "epoch:26 step:20697 [D loss: 0.057683, acc.: 99.22%] [G loss: 4.649314]\n",
      "epoch:26 step:20698 [D loss: 0.005811, acc.: 100.00%] [G loss: 4.120436]\n",
      "epoch:26 step:20699 [D loss: 0.027190, acc.: 100.00%] [G loss: 3.666065]\n",
      "epoch:26 step:20700 [D loss: 0.022022, acc.: 100.00%] [G loss: 3.138026]\n",
      "epoch:26 step:20701 [D loss: 0.079073, acc.: 98.44%] [G loss: 3.905787]\n",
      "epoch:26 step:20702 [D loss: 0.020602, acc.: 100.00%] [G loss: 3.997308]\n",
      "epoch:26 step:20703 [D loss: 0.029361, acc.: 100.00%] [G loss: 4.411406]\n",
      "epoch:26 step:20704 [D loss: 0.009693, acc.: 100.00%] [G loss: 3.229926]\n",
      "epoch:26 step:20705 [D loss: 0.029797, acc.: 99.22%] [G loss: 1.453250]\n",
      "epoch:26 step:20706 [D loss: 0.023791, acc.: 100.00%] [G loss: 1.580678]\n",
      "epoch:26 step:20707 [D loss: 0.172587, acc.: 94.53%] [G loss: 2.666214]\n",
      "epoch:26 step:20708 [D loss: 0.047996, acc.: 98.44%] [G loss: 2.855376]\n",
      "epoch:26 step:20709 [D loss: 1.250176, acc.: 38.28%] [G loss: 4.427476]\n",
      "epoch:26 step:20710 [D loss: 0.095571, acc.: 95.31%] [G loss: 4.858257]\n",
      "epoch:26 step:20711 [D loss: 0.335375, acc.: 85.16%] [G loss: 2.608651]\n",
      "epoch:26 step:20712 [D loss: 0.083269, acc.: 96.88%] [G loss: 2.778466]\n",
      "epoch:26 step:20713 [D loss: 0.010047, acc.: 100.00%] [G loss: 3.330222]\n",
      "epoch:26 step:20714 [D loss: 0.028553, acc.: 100.00%] [G loss: 3.237366]\n",
      "epoch:26 step:20715 [D loss: 0.249453, acc.: 89.84%] [G loss: 4.474655]\n",
      "epoch:26 step:20716 [D loss: 0.020494, acc.: 100.00%] [G loss: 4.881168]\n",
      "epoch:26 step:20717 [D loss: 0.079378, acc.: 96.88%] [G loss: 4.342879]\n",
      "epoch:26 step:20718 [D loss: 0.018467, acc.: 100.00%] [G loss: 3.643934]\n",
      "epoch:26 step:20719 [D loss: 0.069586, acc.: 99.22%] [G loss: 3.827689]\n",
      "epoch:26 step:20720 [D loss: 0.046882, acc.: 100.00%] [G loss: 4.664021]\n",
      "epoch:26 step:20721 [D loss: 0.052075, acc.: 100.00%] [G loss: 4.628509]\n",
      "epoch:26 step:20722 [D loss: 0.038592, acc.: 98.44%] [G loss: 3.888257]\n",
      "epoch:26 step:20723 [D loss: 0.047808, acc.: 100.00%] [G loss: 4.005623]\n",
      "epoch:26 step:20724 [D loss: 0.088102, acc.: 97.66%] [G loss: 4.005129]\n",
      "epoch:26 step:20725 [D loss: 0.038389, acc.: 99.22%] [G loss: 3.950170]\n",
      "epoch:26 step:20726 [D loss: 0.172302, acc.: 96.09%] [G loss: 3.189342]\n",
      "epoch:26 step:20727 [D loss: 0.049251, acc.: 99.22%] [G loss: 3.777643]\n",
      "epoch:26 step:20728 [D loss: 0.041332, acc.: 100.00%] [G loss: 3.737497]\n",
      "epoch:26 step:20729 [D loss: 0.010389, acc.: 100.00%] [G loss: 3.696153]\n",
      "epoch:26 step:20730 [D loss: 0.059026, acc.: 97.66%] [G loss: 3.029275]\n",
      "epoch:26 step:20731 [D loss: 0.070189, acc.: 98.44%] [G loss: 3.478384]\n",
      "epoch:26 step:20732 [D loss: 0.006190, acc.: 100.00%] [G loss: 3.989712]\n",
      "epoch:26 step:20733 [D loss: 0.008099, acc.: 100.00%] [G loss: 3.805650]\n",
      "epoch:26 step:20734 [D loss: 0.109111, acc.: 96.88%] [G loss: 1.972798]\n",
      "epoch:26 step:20735 [D loss: 0.107857, acc.: 96.88%] [G loss: 3.424113]\n",
      "epoch:26 step:20736 [D loss: 0.047254, acc.: 100.00%] [G loss: 3.509000]\n",
      "epoch:26 step:20737 [D loss: 0.369113, acc.: 82.03%] [G loss: 3.724610]\n",
      "epoch:26 step:20738 [D loss: 0.005081, acc.: 100.00%] [G loss: 5.751845]\n",
      "epoch:26 step:20739 [D loss: 0.006666, acc.: 100.00%] [G loss: 4.818759]\n",
      "epoch:26 step:20740 [D loss: 0.012690, acc.: 100.00%] [G loss: 4.509632]\n",
      "epoch:26 step:20741 [D loss: 0.025511, acc.: 99.22%] [G loss: 4.362294]\n",
      "epoch:26 step:20742 [D loss: 0.051671, acc.: 99.22%] [G loss: 3.780892]\n",
      "epoch:26 step:20743 [D loss: 0.017764, acc.: 99.22%] [G loss: 4.592191]\n",
      "epoch:26 step:20744 [D loss: 0.084347, acc.: 99.22%] [G loss: 3.989403]\n",
      "epoch:26 step:20745 [D loss: 0.016591, acc.: 100.00%] [G loss: 3.582392]\n",
      "epoch:26 step:20746 [D loss: 0.010292, acc.: 100.00%] [G loss: 4.196526]\n",
      "epoch:26 step:20747 [D loss: 0.008679, acc.: 100.00%] [G loss: 4.744427]\n",
      "epoch:26 step:20748 [D loss: 0.015207, acc.: 100.00%] [G loss: 3.295108]\n",
      "epoch:26 step:20749 [D loss: 0.037639, acc.: 100.00%] [G loss: 3.679470]\n",
      "epoch:26 step:20750 [D loss: 0.086430, acc.: 97.66%] [G loss: 4.179837]\n",
      "epoch:26 step:20751 [D loss: 0.039079, acc.: 98.44%] [G loss: 4.155768]\n",
      "epoch:26 step:20752 [D loss: 0.025042, acc.: 100.00%] [G loss: 5.341146]\n",
      "epoch:26 step:20753 [D loss: 0.041810, acc.: 99.22%] [G loss: 3.575633]\n",
      "epoch:26 step:20754 [D loss: 0.227036, acc.: 91.41%] [G loss: 2.155412]\n",
      "epoch:26 step:20755 [D loss: 0.040243, acc.: 99.22%] [G loss: 4.195907]\n",
      "epoch:26 step:20756 [D loss: 0.015796, acc.: 100.00%] [G loss: 4.293433]\n",
      "epoch:26 step:20757 [D loss: 0.091024, acc.: 97.66%] [G loss: 3.749056]\n",
      "epoch:26 step:20758 [D loss: 0.043508, acc.: 100.00%] [G loss: 4.721800]\n",
      "epoch:26 step:20759 [D loss: 0.173449, acc.: 93.75%] [G loss: 0.906190]\n",
      "epoch:26 step:20760 [D loss: 0.369150, acc.: 81.25%] [G loss: 6.991720]\n",
      "epoch:26 step:20761 [D loss: 1.349172, acc.: 53.12%] [G loss: 1.299054]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:26 step:20762 [D loss: 1.007363, acc.: 66.41%] [G loss: 6.791468]\n",
      "epoch:26 step:20763 [D loss: 0.695549, acc.: 68.75%] [G loss: 6.203716]\n",
      "epoch:26 step:20764 [D loss: 0.064542, acc.: 97.66%] [G loss: 5.459610]\n",
      "epoch:26 step:20765 [D loss: 0.009434, acc.: 100.00%] [G loss: 5.003521]\n",
      "epoch:26 step:20766 [D loss: 0.047384, acc.: 99.22%] [G loss: 4.282507]\n",
      "epoch:26 step:20767 [D loss: 0.035231, acc.: 98.44%] [G loss: 4.421155]\n",
      "epoch:26 step:20768 [D loss: 0.014263, acc.: 100.00%] [G loss: 4.142892]\n",
      "epoch:26 step:20769 [D loss: 0.028825, acc.: 100.00%] [G loss: 4.185843]\n",
      "epoch:26 step:20770 [D loss: 0.034555, acc.: 100.00%] [G loss: 4.154742]\n",
      "epoch:26 step:20771 [D loss: 0.017193, acc.: 100.00%] [G loss: 4.540916]\n",
      "epoch:26 step:20772 [D loss: 0.026213, acc.: 100.00%] [G loss: 4.209869]\n",
      "epoch:26 step:20773 [D loss: 0.059891, acc.: 100.00%] [G loss: 3.800435]\n",
      "epoch:26 step:20774 [D loss: 0.174304, acc.: 96.09%] [G loss: 4.334904]\n",
      "epoch:26 step:20775 [D loss: 0.009333, acc.: 100.00%] [G loss: 4.772571]\n",
      "epoch:26 step:20776 [D loss: 0.018820, acc.: 100.00%] [G loss: 5.163611]\n",
      "epoch:26 step:20777 [D loss: 0.368696, acc.: 82.03%] [G loss: 5.904160]\n",
      "epoch:26 step:20778 [D loss: 0.181492, acc.: 90.62%] [G loss: 5.451537]\n",
      "epoch:26 step:20779 [D loss: 0.005029, acc.: 100.00%] [G loss: 5.384708]\n",
      "epoch:26 step:20780 [D loss: 0.012693, acc.: 100.00%] [G loss: 5.007190]\n",
      "epoch:26 step:20781 [D loss: 0.007582, acc.: 100.00%] [G loss: 4.822378]\n",
      "epoch:26 step:20782 [D loss: 0.012520, acc.: 100.00%] [G loss: 4.635604]\n",
      "epoch:26 step:20783 [D loss: 0.011806, acc.: 100.00%] [G loss: 5.023760]\n",
      "epoch:26 step:20784 [D loss: 0.011774, acc.: 100.00%] [G loss: 4.281007]\n",
      "epoch:26 step:20785 [D loss: 0.015625, acc.: 100.00%] [G loss: 4.634744]\n",
      "epoch:26 step:20786 [D loss: 0.015311, acc.: 100.00%] [G loss: 3.946475]\n",
      "epoch:26 step:20787 [D loss: 0.007917, acc.: 100.00%] [G loss: 4.676673]\n",
      "epoch:26 step:20788 [D loss: 0.039413, acc.: 100.00%] [G loss: 4.228865]\n",
      "epoch:26 step:20789 [D loss: 0.014274, acc.: 100.00%] [G loss: 4.308078]\n",
      "epoch:26 step:20790 [D loss: 0.009988, acc.: 100.00%] [G loss: 4.020793]\n",
      "epoch:26 step:20791 [D loss: 0.344530, acc.: 83.59%] [G loss: 5.174009]\n",
      "epoch:26 step:20792 [D loss: 0.006713, acc.: 100.00%] [G loss: 6.095674]\n",
      "epoch:26 step:20793 [D loss: 0.052504, acc.: 99.22%] [G loss: 5.071721]\n",
      "epoch:26 step:20794 [D loss: 0.016873, acc.: 100.00%] [G loss: 4.523208]\n",
      "epoch:26 step:20795 [D loss: 0.005981, acc.: 100.00%] [G loss: 4.994245]\n",
      "epoch:26 step:20796 [D loss: 0.006167, acc.: 100.00%] [G loss: 4.676966]\n",
      "epoch:26 step:20797 [D loss: 0.050790, acc.: 100.00%] [G loss: 4.331174]\n",
      "epoch:26 step:20798 [D loss: 0.897217, acc.: 54.69%] [G loss: 7.664823]\n",
      "epoch:26 step:20799 [D loss: 0.218805, acc.: 89.06%] [G loss: 7.060991]\n",
      "epoch:26 step:20800 [D loss: 0.243155, acc.: 88.28%] [G loss: 4.202967]\n",
      "##############\n",
      "[0.95538044 0.9869715  1.04614738 0.99297366 0.88165987 2.10831708\n",
      " 1.10565186 2.11305135 2.11616062 0.80698194]\n",
      "##########\n",
      "epoch:26 step:20801 [D loss: 0.106462, acc.: 96.09%] [G loss: 4.146813]\n",
      "epoch:26 step:20802 [D loss: 0.006945, acc.: 100.00%] [G loss: 5.016504]\n",
      "epoch:26 step:20803 [D loss: 0.009376, acc.: 100.00%] [G loss: 4.751457]\n",
      "epoch:26 step:20804 [D loss: 0.026571, acc.: 100.00%] [G loss: 4.668154]\n",
      "epoch:26 step:20805 [D loss: 0.057046, acc.: 96.88%] [G loss: 4.873622]\n",
      "epoch:26 step:20806 [D loss: 0.029136, acc.: 100.00%] [G loss: 5.184607]\n",
      "epoch:26 step:20807 [D loss: 0.026373, acc.: 99.22%] [G loss: 5.038579]\n",
      "epoch:26 step:20808 [D loss: 0.007979, acc.: 100.00%] [G loss: 4.518223]\n",
      "epoch:26 step:20809 [D loss: 0.012694, acc.: 100.00%] [G loss: 4.000585]\n",
      "epoch:26 step:20810 [D loss: 0.010787, acc.: 100.00%] [G loss: 4.404458]\n",
      "epoch:26 step:20811 [D loss: 0.101598, acc.: 96.09%] [G loss: 4.931988]\n",
      "epoch:26 step:20812 [D loss: 0.019958, acc.: 100.00%] [G loss: 4.926896]\n",
      "epoch:26 step:20813 [D loss: 0.008954, acc.: 100.00%] [G loss: 4.458589]\n",
      "epoch:26 step:20814 [D loss: 0.154412, acc.: 96.88%] [G loss: 3.393073]\n",
      "epoch:26 step:20815 [D loss: 0.013885, acc.: 100.00%] [G loss: 2.877748]\n",
      "epoch:26 step:20816 [D loss: 0.074313, acc.: 99.22%] [G loss: 5.310527]\n",
      "epoch:26 step:20817 [D loss: 0.064771, acc.: 98.44%] [G loss: 5.374998]\n",
      "epoch:26 step:20818 [D loss: 0.013105, acc.: 100.00%] [G loss: 5.349383]\n",
      "epoch:26 step:20819 [D loss: 0.008838, acc.: 100.00%] [G loss: 4.843506]\n",
      "epoch:26 step:20820 [D loss: 0.008979, acc.: 100.00%] [G loss: 4.830112]\n",
      "epoch:26 step:20821 [D loss: 0.007868, acc.: 100.00%] [G loss: 3.532848]\n",
      "epoch:26 step:20822 [D loss: 0.071444, acc.: 98.44%] [G loss: 5.250821]\n",
      "epoch:26 step:20823 [D loss: 0.027314, acc.: 100.00%] [G loss: 5.925661]\n",
      "epoch:26 step:20824 [D loss: 0.079574, acc.: 98.44%] [G loss: 4.481506]\n",
      "epoch:26 step:20825 [D loss: 0.053459, acc.: 98.44%] [G loss: 4.810324]\n",
      "epoch:26 step:20826 [D loss: 0.016786, acc.: 99.22%] [G loss: 3.594648]\n",
      "epoch:26 step:20827 [D loss: 0.028454, acc.: 100.00%] [G loss: 4.928755]\n",
      "epoch:26 step:20828 [D loss: 0.182762, acc.: 95.31%] [G loss: 6.006810]\n",
      "epoch:26 step:20829 [D loss: 1.739789, acc.: 33.59%] [G loss: 9.191128]\n",
      "epoch:26 step:20830 [D loss: 3.274875, acc.: 50.00%] [G loss: 6.585972]\n",
      "epoch:26 step:20831 [D loss: 2.742412, acc.: 50.00%] [G loss: 4.631860]\n",
      "epoch:26 step:20832 [D loss: 1.552563, acc.: 50.78%] [G loss: 2.535785]\n",
      "epoch:26 step:20833 [D loss: 0.716835, acc.: 68.75%] [G loss: 1.636881]\n",
      "epoch:26 step:20834 [D loss: 0.295608, acc.: 88.28%] [G loss: 2.210960]\n",
      "epoch:26 step:20835 [D loss: 0.170836, acc.: 94.53%] [G loss: 2.522539]\n",
      "epoch:26 step:20836 [D loss: 0.111667, acc.: 98.44%] [G loss: 2.153152]\n",
      "epoch:26 step:20837 [D loss: 0.104151, acc.: 99.22%] [G loss: 1.880319]\n",
      "epoch:26 step:20838 [D loss: 0.289400, acc.: 88.28%] [G loss: 2.485521]\n",
      "epoch:26 step:20839 [D loss: 0.227290, acc.: 91.41%] [G loss: 2.114855]\n",
      "epoch:26 step:20840 [D loss: 0.123095, acc.: 99.22%] [G loss: 2.695407]\n",
      "epoch:26 step:20841 [D loss: 0.087619, acc.: 98.44%] [G loss: 2.727774]\n",
      "epoch:26 step:20842 [D loss: 0.060378, acc.: 100.00%] [G loss: 2.658255]\n",
      "epoch:26 step:20843 [D loss: 0.327175, acc.: 88.28%] [G loss: 2.285101]\n",
      "epoch:26 step:20844 [D loss: 0.094818, acc.: 99.22%] [G loss: 2.619343]\n",
      "epoch:26 step:20845 [D loss: 0.050706, acc.: 100.00%] [G loss: 3.067297]\n",
      "epoch:26 step:20846 [D loss: 0.065047, acc.: 100.00%] [G loss: 2.861773]\n",
      "epoch:26 step:20847 [D loss: 0.057285, acc.: 98.44%] [G loss: 3.164379]\n",
      "epoch:26 step:20848 [D loss: 0.090932, acc.: 96.88%] [G loss: 3.186764]\n",
      "epoch:26 step:20849 [D loss: 0.101286, acc.: 99.22%] [G loss: 2.672488]\n",
      "epoch:26 step:20850 [D loss: 0.074924, acc.: 99.22%] [G loss: 3.274320]\n",
      "epoch:26 step:20851 [D loss: 0.046550, acc.: 100.00%] [G loss: 3.459502]\n",
      "epoch:26 step:20852 [D loss: 0.073806, acc.: 100.00%] [G loss: 2.830491]\n",
      "epoch:26 step:20853 [D loss: 0.046058, acc.: 100.00%] [G loss: 3.585939]\n",
      "epoch:26 step:20854 [D loss: 0.038950, acc.: 100.00%] [G loss: 3.590611]\n",
      "epoch:26 step:20855 [D loss: 0.072257, acc.: 98.44%] [G loss: 2.159563]\n",
      "epoch:26 step:20856 [D loss: 0.261899, acc.: 88.28%] [G loss: 4.718993]\n",
      "epoch:26 step:20857 [D loss: 0.271763, acc.: 86.72%] [G loss: 3.911895]\n",
      "epoch:26 step:20858 [D loss: 0.025347, acc.: 100.00%] [G loss: 3.704139]\n",
      "epoch:26 step:20859 [D loss: 0.055705, acc.: 98.44%] [G loss: 3.745153]\n",
      "epoch:26 step:20860 [D loss: 0.030496, acc.: 99.22%] [G loss: 3.819831]\n",
      "epoch:26 step:20861 [D loss: 0.025766, acc.: 100.00%] [G loss: 3.647769]\n",
      "epoch:26 step:20862 [D loss: 0.025537, acc.: 100.00%] [G loss: 3.054173]\n",
      "epoch:26 step:20863 [D loss: 0.057101, acc.: 99.22%] [G loss: 2.945681]\n",
      "epoch:26 step:20864 [D loss: 0.110072, acc.: 99.22%] [G loss: 3.685042]\n",
      "epoch:26 step:20865 [D loss: 0.038705, acc.: 100.00%] [G loss: 3.356548]\n",
      "epoch:26 step:20866 [D loss: 0.029699, acc.: 100.00%] [G loss: 4.097436]\n",
      "epoch:26 step:20867 [D loss: 0.027919, acc.: 100.00%] [G loss: 3.562884]\n",
      "epoch:26 step:20868 [D loss: 0.078653, acc.: 99.22%] [G loss: 3.720899]\n",
      "epoch:26 step:20869 [D loss: 0.014691, acc.: 100.00%] [G loss: 3.282331]\n",
      "epoch:26 step:20870 [D loss: 0.026640, acc.: 100.00%] [G loss: 4.041788]\n",
      "epoch:26 step:20871 [D loss: 0.087342, acc.: 100.00%] [G loss: 3.917409]\n",
      "epoch:26 step:20872 [D loss: 0.154114, acc.: 95.31%] [G loss: 3.022280]\n",
      "epoch:26 step:20873 [D loss: 0.097801, acc.: 96.88%] [G loss: 4.174019]\n",
      "epoch:26 step:20874 [D loss: 0.030880, acc.: 99.22%] [G loss: 4.491629]\n",
      "epoch:26 step:20875 [D loss: 0.032467, acc.: 100.00%] [G loss: 4.001449]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:26 step:20876 [D loss: 0.010386, acc.: 100.00%] [G loss: 3.252836]\n",
      "epoch:26 step:20877 [D loss: 0.057917, acc.: 99.22%] [G loss: 4.588133]\n",
      "epoch:26 step:20878 [D loss: 0.017676, acc.: 100.00%] [G loss: 3.237457]\n",
      "epoch:26 step:20879 [D loss: 0.062726, acc.: 98.44%] [G loss: 3.421171]\n",
      "epoch:26 step:20880 [D loss: 0.016064, acc.: 100.00%] [G loss: 3.738497]\n",
      "epoch:26 step:20881 [D loss: 1.810235, acc.: 35.94%] [G loss: 6.730905]\n",
      "epoch:26 step:20882 [D loss: 1.247474, acc.: 53.12%] [G loss: 4.386681]\n",
      "epoch:26 step:20883 [D loss: 0.133720, acc.: 96.09%] [G loss: 3.251722]\n",
      "epoch:26 step:20884 [D loss: 0.030540, acc.: 100.00%] [G loss: 3.150013]\n",
      "epoch:26 step:20885 [D loss: 0.026269, acc.: 100.00%] [G loss: 3.527449]\n",
      "epoch:26 step:20886 [D loss: 0.043210, acc.: 100.00%] [G loss: 3.424630]\n",
      "epoch:26 step:20887 [D loss: 0.017908, acc.: 100.00%] [G loss: 3.325971]\n",
      "epoch:26 step:20888 [D loss: 0.083084, acc.: 100.00%] [G loss: 3.755934]\n",
      "epoch:26 step:20889 [D loss: 0.015015, acc.: 100.00%] [G loss: 3.856563]\n",
      "epoch:26 step:20890 [D loss: 0.024112, acc.: 100.00%] [G loss: 4.195548]\n",
      "epoch:26 step:20891 [D loss: 0.032594, acc.: 100.00%] [G loss: 4.358839]\n",
      "epoch:26 step:20892 [D loss: 0.028867, acc.: 100.00%] [G loss: 4.062037]\n",
      "epoch:26 step:20893 [D loss: 0.024895, acc.: 100.00%] [G loss: 2.938332]\n",
      "epoch:26 step:20894 [D loss: 0.029353, acc.: 100.00%] [G loss: 3.311369]\n",
      "epoch:26 step:20895 [D loss: 0.028565, acc.: 100.00%] [G loss: 2.914357]\n",
      "epoch:26 step:20896 [D loss: 0.085935, acc.: 99.22%] [G loss: 3.656563]\n",
      "epoch:26 step:20897 [D loss: 0.029914, acc.: 100.00%] [G loss: 3.306085]\n",
      "epoch:26 step:20898 [D loss: 0.028735, acc.: 100.00%] [G loss: 3.996687]\n",
      "epoch:26 step:20899 [D loss: 0.030699, acc.: 100.00%] [G loss: 3.627419]\n",
      "epoch:26 step:20900 [D loss: 0.020492, acc.: 100.00%] [G loss: 3.562799]\n",
      "epoch:26 step:20901 [D loss: 0.028286, acc.: 100.00%] [G loss: 3.326147]\n",
      "epoch:26 step:20902 [D loss: 0.040512, acc.: 100.00%] [G loss: 3.119198]\n",
      "epoch:26 step:20903 [D loss: 0.051738, acc.: 100.00%] [G loss: 3.567939]\n",
      "epoch:26 step:20904 [D loss: 0.013278, acc.: 100.00%] [G loss: 4.222656]\n",
      "epoch:26 step:20905 [D loss: 0.150123, acc.: 94.53%] [G loss: 2.232430]\n",
      "epoch:26 step:20906 [D loss: 0.364470, acc.: 82.81%] [G loss: 6.530504]\n",
      "epoch:26 step:20907 [D loss: 1.165867, acc.: 54.69%] [G loss: 4.211247]\n",
      "epoch:26 step:20908 [D loss: 0.035886, acc.: 100.00%] [G loss: 3.191137]\n",
      "epoch:26 step:20909 [D loss: 0.020089, acc.: 100.00%] [G loss: 3.655158]\n",
      "epoch:26 step:20910 [D loss: 0.067483, acc.: 100.00%] [G loss: 3.369298]\n",
      "epoch:26 step:20911 [D loss: 0.085486, acc.: 99.22%] [G loss: 2.858945]\n",
      "epoch:26 step:20912 [D loss: 0.029530, acc.: 100.00%] [G loss: 4.679301]\n",
      "epoch:26 step:20913 [D loss: 0.043522, acc.: 99.22%] [G loss: 3.046356]\n",
      "epoch:26 step:20914 [D loss: 0.048639, acc.: 99.22%] [G loss: 3.757301]\n",
      "epoch:26 step:20915 [D loss: 0.323294, acc.: 89.06%] [G loss: 4.095911]\n",
      "epoch:26 step:20916 [D loss: 0.018717, acc.: 100.00%] [G loss: 4.038122]\n",
      "epoch:26 step:20917 [D loss: 0.010245, acc.: 100.00%] [G loss: 4.735226]\n",
      "epoch:26 step:20918 [D loss: 0.006647, acc.: 100.00%] [G loss: 4.382361]\n",
      "epoch:26 step:20919 [D loss: 0.052089, acc.: 100.00%] [G loss: 3.156663]\n",
      "epoch:26 step:20920 [D loss: 0.075151, acc.: 98.44%] [G loss: 3.833101]\n",
      "epoch:26 step:20921 [D loss: 0.006786, acc.: 100.00%] [G loss: 3.931167]\n",
      "epoch:26 step:20922 [D loss: 0.013967, acc.: 100.00%] [G loss: 4.694931]\n",
      "epoch:26 step:20923 [D loss: 0.023574, acc.: 100.00%] [G loss: 3.935668]\n",
      "epoch:26 step:20924 [D loss: 0.027737, acc.: 100.00%] [G loss: 3.992082]\n",
      "epoch:26 step:20925 [D loss: 0.038659, acc.: 100.00%] [G loss: 4.348994]\n",
      "epoch:26 step:20926 [D loss: 0.055771, acc.: 100.00%] [G loss: 3.681176]\n",
      "epoch:26 step:20927 [D loss: 0.026601, acc.: 100.00%] [G loss: 3.896348]\n",
      "epoch:26 step:20928 [D loss: 0.008943, acc.: 100.00%] [G loss: 3.558782]\n",
      "epoch:26 step:20929 [D loss: 0.021321, acc.: 99.22%] [G loss: 3.231137]\n",
      "epoch:26 step:20930 [D loss: 0.107186, acc.: 98.44%] [G loss: 4.724984]\n",
      "epoch:26 step:20931 [D loss: 0.077133, acc.: 96.88%] [G loss: 4.793872]\n",
      "epoch:26 step:20932 [D loss: 0.073454, acc.: 97.66%] [G loss: 2.659116]\n",
      "epoch:26 step:20933 [D loss: 0.046609, acc.: 100.00%] [G loss: 2.587573]\n",
      "epoch:26 step:20934 [D loss: 0.096026, acc.: 96.88%] [G loss: 4.666800]\n",
      "epoch:26 step:20935 [D loss: 0.017348, acc.: 100.00%] [G loss: 5.805069]\n",
      "epoch:26 step:20936 [D loss: 0.036268, acc.: 99.22%] [G loss: 5.177164]\n",
      "epoch:26 step:20937 [D loss: 0.043459, acc.: 99.22%] [G loss: 3.699497]\n",
      "epoch:26 step:20938 [D loss: 0.004908, acc.: 100.00%] [G loss: 3.506869]\n",
      "epoch:26 step:20939 [D loss: 0.032538, acc.: 99.22%] [G loss: 4.343052]\n",
      "epoch:26 step:20940 [D loss: 0.009162, acc.: 100.00%] [G loss: 4.526022]\n",
      "epoch:26 step:20941 [D loss: 0.025581, acc.: 100.00%] [G loss: 4.723498]\n",
      "epoch:26 step:20942 [D loss: 0.105836, acc.: 96.88%] [G loss: 3.881080]\n",
      "epoch:26 step:20943 [D loss: 0.079855, acc.: 100.00%] [G loss: 4.331309]\n",
      "epoch:26 step:20944 [D loss: 0.006992, acc.: 100.00%] [G loss: 5.067088]\n",
      "epoch:26 step:20945 [D loss: 0.026165, acc.: 99.22%] [G loss: 4.364055]\n",
      "epoch:26 step:20946 [D loss: 0.018977, acc.: 100.00%] [G loss: 4.839572]\n",
      "epoch:26 step:20947 [D loss: 0.011836, acc.: 100.00%] [G loss: 4.722778]\n",
      "epoch:26 step:20948 [D loss: 0.036366, acc.: 99.22%] [G loss: 4.380135]\n",
      "epoch:26 step:20949 [D loss: 0.004512, acc.: 100.00%] [G loss: 3.650595]\n",
      "epoch:26 step:20950 [D loss: 0.010148, acc.: 100.00%] [G loss: 4.436711]\n",
      "epoch:26 step:20951 [D loss: 0.019189, acc.: 100.00%] [G loss: 4.846238]\n",
      "epoch:26 step:20952 [D loss: 0.189108, acc.: 92.97%] [G loss: 5.795507]\n",
      "epoch:26 step:20953 [D loss: 0.143172, acc.: 94.53%] [G loss: 3.971687]\n",
      "epoch:26 step:20954 [D loss: 0.037533, acc.: 100.00%] [G loss: 3.719034]\n",
      "epoch:26 step:20955 [D loss: 0.015592, acc.: 100.00%] [G loss: 3.888278]\n",
      "epoch:26 step:20956 [D loss: 0.016922, acc.: 100.00%] [G loss: 4.968536]\n",
      "epoch:26 step:20957 [D loss: 0.013070, acc.: 100.00%] [G loss: 5.054965]\n",
      "epoch:26 step:20958 [D loss: 0.027568, acc.: 100.00%] [G loss: 4.435645]\n",
      "epoch:26 step:20959 [D loss: 0.026531, acc.: 99.22%] [G loss: 5.073172]\n",
      "epoch:26 step:20960 [D loss: 0.103442, acc.: 98.44%] [G loss: 4.292464]\n",
      "epoch:26 step:20961 [D loss: 0.016453, acc.: 100.00%] [G loss: 3.654030]\n",
      "epoch:26 step:20962 [D loss: 0.024136, acc.: 100.00%] [G loss: 3.787995]\n",
      "epoch:26 step:20963 [D loss: 0.007555, acc.: 100.00%] [G loss: 4.285783]\n",
      "epoch:26 step:20964 [D loss: 0.573289, acc.: 71.09%] [G loss: 7.439566]\n",
      "epoch:26 step:20965 [D loss: 1.365177, acc.: 50.00%] [G loss: 1.983775]\n",
      "epoch:26 step:20966 [D loss: 0.142998, acc.: 95.31%] [G loss: 5.190857]\n",
      "epoch:26 step:20967 [D loss: 0.022163, acc.: 100.00%] [G loss: 5.838253]\n",
      "epoch:26 step:20968 [D loss: 0.015557, acc.: 100.00%] [G loss: 5.691645]\n",
      "epoch:26 step:20969 [D loss: 0.010707, acc.: 100.00%] [G loss: 5.847954]\n",
      "epoch:26 step:20970 [D loss: 0.011636, acc.: 100.00%] [G loss: 5.046882]\n",
      "epoch:26 step:20971 [D loss: 0.014499, acc.: 100.00%] [G loss: 4.365106]\n",
      "epoch:26 step:20972 [D loss: 0.006400, acc.: 100.00%] [G loss: 3.653162]\n",
      "epoch:26 step:20973 [D loss: 0.032921, acc.: 100.00%] [G loss: 4.792151]\n",
      "epoch:26 step:20974 [D loss: 0.022098, acc.: 100.00%] [G loss: 4.278727]\n",
      "epoch:26 step:20975 [D loss: 0.015212, acc.: 100.00%] [G loss: 4.331105]\n",
      "epoch:26 step:20976 [D loss: 0.025171, acc.: 100.00%] [G loss: 3.369738]\n",
      "epoch:26 step:20977 [D loss: 0.078012, acc.: 99.22%] [G loss: 4.453459]\n",
      "epoch:26 step:20978 [D loss: 0.015969, acc.: 100.00%] [G loss: 4.882279]\n",
      "epoch:26 step:20979 [D loss: 0.038762, acc.: 100.00%] [G loss: 4.201986]\n",
      "epoch:26 step:20980 [D loss: 0.025620, acc.: 100.00%] [G loss: 3.813916]\n",
      "epoch:26 step:20981 [D loss: 0.014114, acc.: 100.00%] [G loss: 4.022431]\n",
      "epoch:26 step:20982 [D loss: 0.037371, acc.: 100.00%] [G loss: 4.521298]\n",
      "epoch:26 step:20983 [D loss: 0.024192, acc.: 100.00%] [G loss: 4.586389]\n",
      "epoch:26 step:20984 [D loss: 0.052665, acc.: 98.44%] [G loss: 3.213618]\n",
      "epoch:26 step:20985 [D loss: 0.010614, acc.: 100.00%] [G loss: 3.755026]\n",
      "epoch:26 step:20986 [D loss: 0.037892, acc.: 100.00%] [G loss: 3.393900]\n",
      "epoch:26 step:20987 [D loss: 0.010478, acc.: 100.00%] [G loss: 3.386539]\n",
      "epoch:26 step:20988 [D loss: 0.047773, acc.: 99.22%] [G loss: 3.757303]\n",
      "epoch:26 step:20989 [D loss: 0.027175, acc.: 100.00%] [G loss: 3.010544]\n",
      "epoch:26 step:20990 [D loss: 0.049221, acc.: 100.00%] [G loss: 4.871807]\n",
      "epoch:26 step:20991 [D loss: 3.395015, acc.: 15.62%] [G loss: 8.044859]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:26 step:20992 [D loss: 2.401156, acc.: 50.00%] [G loss: 4.652564]\n",
      "epoch:26 step:20993 [D loss: 0.226661, acc.: 88.28%] [G loss: 2.645246]\n",
      "epoch:26 step:20994 [D loss: 0.087019, acc.: 98.44%] [G loss: 2.177901]\n",
      "epoch:26 step:20995 [D loss: 0.323996, acc.: 87.50%] [G loss: 3.976729]\n",
      "epoch:26 step:20996 [D loss: 0.281138, acc.: 87.50%] [G loss: 2.891430]\n",
      "epoch:26 step:20997 [D loss: 0.026484, acc.: 100.00%] [G loss: 2.631634]\n",
      "epoch:26 step:20998 [D loss: 0.112312, acc.: 96.09%] [G loss: 2.758321]\n",
      "epoch:26 step:20999 [D loss: 0.062183, acc.: 100.00%] [G loss: 3.604274]\n",
      "epoch:26 step:21000 [D loss: 0.035975, acc.: 99.22%] [G loss: 3.216650]\n",
      "##############\n",
      "[0.97603468 0.97120963 1.00254714 1.02107876 2.12020434 2.11267933\n",
      " 1.11154744 2.11469327 0.95494458 0.83106377]\n",
      "##########\n",
      "epoch:26 step:21001 [D loss: 0.056662, acc.: 100.00%] [G loss: 2.072820]\n",
      "epoch:26 step:21002 [D loss: 0.211489, acc.: 92.97%] [G loss: 3.539640]\n",
      "epoch:26 step:21003 [D loss: 0.158926, acc.: 93.75%] [G loss: 3.370940]\n",
      "epoch:26 step:21004 [D loss: 1.220019, acc.: 42.97%] [G loss: 5.318707]\n",
      "epoch:26 step:21005 [D loss: 0.300828, acc.: 82.81%] [G loss: 4.823395]\n",
      "epoch:26 step:21006 [D loss: 0.707606, acc.: 67.19%] [G loss: 1.645454]\n",
      "epoch:26 step:21007 [D loss: 0.116035, acc.: 96.88%] [G loss: 3.340380]\n",
      "epoch:26 step:21008 [D loss: 0.139496, acc.: 94.53%] [G loss: 3.506012]\n",
      "epoch:26 step:21009 [D loss: 0.382443, acc.: 82.81%] [G loss: 2.121631]\n",
      "epoch:26 step:21010 [D loss: 0.196430, acc.: 92.97%] [G loss: 3.368492]\n",
      "epoch:26 step:21011 [D loss: 0.078580, acc.: 98.44%] [G loss: 3.639755]\n",
      "epoch:26 step:21012 [D loss: 0.108793, acc.: 96.09%] [G loss: 2.035640]\n",
      "epoch:26 step:21013 [D loss: 0.179028, acc.: 94.53%] [G loss: 2.657916]\n",
      "epoch:26 step:21014 [D loss: 0.047527, acc.: 100.00%] [G loss: 3.279423]\n",
      "epoch:26 step:21015 [D loss: 0.032523, acc.: 100.00%] [G loss: 4.091476]\n",
      "epoch:26 step:21016 [D loss: 0.085171, acc.: 98.44%] [G loss: 3.124655]\n",
      "epoch:26 step:21017 [D loss: 0.088779, acc.: 100.00%] [G loss: 1.993142]\n",
      "epoch:26 step:21018 [D loss: 0.042884, acc.: 100.00%] [G loss: 2.795475]\n",
      "epoch:26 step:21019 [D loss: 0.296140, acc.: 88.28%] [G loss: 4.243367]\n",
      "epoch:26 step:21020 [D loss: 0.282082, acc.: 86.72%] [G loss: 3.539201]\n",
      "epoch:26 step:21021 [D loss: 1.418559, acc.: 38.28%] [G loss: 6.144318]\n",
      "epoch:26 step:21022 [D loss: 0.238514, acc.: 90.62%] [G loss: 6.512106]\n",
      "epoch:26 step:21023 [D loss: 0.190160, acc.: 90.62%] [G loss: 5.102707]\n",
      "epoch:26 step:21024 [D loss: 0.042328, acc.: 99.22%] [G loss: 4.784914]\n",
      "epoch:26 step:21025 [D loss: 0.014464, acc.: 100.00%] [G loss: 3.818542]\n",
      "epoch:26 step:21026 [D loss: 0.021783, acc.: 100.00%] [G loss: 4.202728]\n",
      "epoch:26 step:21027 [D loss: 0.045005, acc.: 100.00%] [G loss: 3.794872]\n",
      "epoch:26 step:21028 [D loss: 0.058048, acc.: 98.44%] [G loss: 3.694108]\n",
      "epoch:26 step:21029 [D loss: 0.074453, acc.: 98.44%] [G loss: 3.306174]\n",
      "epoch:26 step:21030 [D loss: 0.060052, acc.: 99.22%] [G loss: 4.201090]\n",
      "epoch:26 step:21031 [D loss: 0.429451, acc.: 80.47%] [G loss: 4.429859]\n",
      "epoch:26 step:21032 [D loss: 0.011544, acc.: 100.00%] [G loss: 5.666491]\n",
      "epoch:26 step:21033 [D loss: 0.338200, acc.: 84.38%] [G loss: 2.819563]\n",
      "epoch:26 step:21034 [D loss: 0.143211, acc.: 94.53%] [G loss: 4.239240]\n",
      "epoch:26 step:21035 [D loss: 0.032846, acc.: 100.00%] [G loss: 4.248870]\n",
      "epoch:26 step:21036 [D loss: 0.013888, acc.: 100.00%] [G loss: 4.694018]\n",
      "epoch:26 step:21037 [D loss: 0.022002, acc.: 100.00%] [G loss: 3.700065]\n",
      "epoch:26 step:21038 [D loss: 0.037425, acc.: 100.00%] [G loss: 4.056247]\n",
      "epoch:26 step:21039 [D loss: 0.038650, acc.: 99.22%] [G loss: 3.674706]\n",
      "epoch:26 step:21040 [D loss: 0.070738, acc.: 99.22%] [G loss: 3.379765]\n",
      "epoch:26 step:21041 [D loss: 0.064764, acc.: 100.00%] [G loss: 3.007001]\n",
      "epoch:26 step:21042 [D loss: 0.027315, acc.: 100.00%] [G loss: 2.984733]\n",
      "epoch:26 step:21043 [D loss: 0.043304, acc.: 100.00%] [G loss: 3.193680]\n",
      "epoch:26 step:21044 [D loss: 0.042131, acc.: 100.00%] [G loss: 2.820679]\n",
      "epoch:26 step:21045 [D loss: 0.171349, acc.: 94.53%] [G loss: 1.819424]\n",
      "epoch:26 step:21046 [D loss: 0.171914, acc.: 92.19%] [G loss: 3.654159]\n",
      "epoch:26 step:21047 [D loss: 0.009209, acc.: 100.00%] [G loss: 5.212996]\n",
      "epoch:26 step:21048 [D loss: 0.218997, acc.: 90.62%] [G loss: 2.627709]\n",
      "epoch:26 step:21049 [D loss: 0.019276, acc.: 100.00%] [G loss: 1.228534]\n",
      "epoch:26 step:21050 [D loss: 0.140107, acc.: 95.31%] [G loss: 4.308774]\n",
      "epoch:26 step:21051 [D loss: 0.007448, acc.: 100.00%] [G loss: 5.661475]\n",
      "epoch:26 step:21052 [D loss: 0.142087, acc.: 92.97%] [G loss: 3.529662]\n",
      "epoch:26 step:21053 [D loss: 0.037459, acc.: 100.00%] [G loss: 2.407213]\n",
      "epoch:26 step:21054 [D loss: 0.074044, acc.: 99.22%] [G loss: 3.043033]\n",
      "epoch:26 step:21055 [D loss: 0.016121, acc.: 100.00%] [G loss: 3.591925]\n",
      "epoch:26 step:21056 [D loss: 0.119742, acc.: 97.66%] [G loss: 3.296001]\n",
      "epoch:26 step:21057 [D loss: 0.013238, acc.: 100.00%] [G loss: 3.511261]\n",
      "epoch:26 step:21058 [D loss: 0.016204, acc.: 100.00%] [G loss: 3.764285]\n",
      "epoch:26 step:21059 [D loss: 0.031163, acc.: 100.00%] [G loss: 3.469736]\n",
      "epoch:26 step:21060 [D loss: 0.014619, acc.: 100.00%] [G loss: 2.883387]\n",
      "epoch:26 step:21061 [D loss: 0.014434, acc.: 100.00%] [G loss: 2.690335]\n",
      "epoch:26 step:21062 [D loss: 0.113100, acc.: 96.09%] [G loss: 1.087525]\n",
      "epoch:26 step:21063 [D loss: 0.032890, acc.: 100.00%] [G loss: 1.625039]\n",
      "epoch:26 step:21064 [D loss: 0.109522, acc.: 96.88%] [G loss: 4.205935]\n",
      "epoch:26 step:21065 [D loss: 0.013734, acc.: 100.00%] [G loss: 5.304178]\n",
      "epoch:26 step:21066 [D loss: 1.330565, acc.: 35.16%] [G loss: 5.229675]\n",
      "epoch:26 step:21067 [D loss: 0.017380, acc.: 100.00%] [G loss: 5.537471]\n",
      "epoch:26 step:21068 [D loss: 0.514283, acc.: 75.00%] [G loss: 3.166760]\n",
      "epoch:26 step:21069 [D loss: 0.236343, acc.: 91.41%] [G loss: 3.911879]\n",
      "epoch:26 step:21070 [D loss: 0.015536, acc.: 100.00%] [G loss: 4.917655]\n",
      "epoch:26 step:21071 [D loss: 0.135377, acc.: 96.09%] [G loss: 4.612669]\n",
      "epoch:26 step:21072 [D loss: 0.027439, acc.: 100.00%] [G loss: 3.958506]\n",
      "epoch:26 step:21073 [D loss: 0.032405, acc.: 99.22%] [G loss: 2.869145]\n",
      "epoch:26 step:21074 [D loss: 0.043331, acc.: 100.00%] [G loss: 3.486383]\n",
      "epoch:26 step:21075 [D loss: 0.020624, acc.: 99.22%] [G loss: 3.349600]\n",
      "epoch:26 step:21076 [D loss: 0.014864, acc.: 100.00%] [G loss: 3.206007]\n",
      "epoch:26 step:21077 [D loss: 0.028248, acc.: 100.00%] [G loss: 3.178334]\n",
      "epoch:26 step:21078 [D loss: 0.056630, acc.: 99.22%] [G loss: 3.369159]\n",
      "epoch:26 step:21079 [D loss: 0.009515, acc.: 100.00%] [G loss: 3.790153]\n",
      "epoch:26 step:21080 [D loss: 0.029518, acc.: 99.22%] [G loss: 3.017224]\n",
      "epoch:26 step:21081 [D loss: 0.039210, acc.: 99.22%] [G loss: 2.873221]\n",
      "epoch:26 step:21082 [D loss: 0.009850, acc.: 100.00%] [G loss: 3.249943]\n",
      "epoch:26 step:21083 [D loss: 0.279493, acc.: 84.38%] [G loss: 5.710517]\n",
      "epoch:26 step:21084 [D loss: 0.114388, acc.: 94.53%] [G loss: 5.781585]\n",
      "epoch:26 step:21085 [D loss: 0.134791, acc.: 94.53%] [G loss: 2.548707]\n",
      "epoch:26 step:21086 [D loss: 0.054113, acc.: 98.44%] [G loss: 3.982998]\n",
      "epoch:26 step:21087 [D loss: 0.007008, acc.: 100.00%] [G loss: 4.844346]\n",
      "epoch:27 step:21088 [D loss: 0.013318, acc.: 100.00%] [G loss: 4.962635]\n",
      "epoch:27 step:21089 [D loss: 0.013487, acc.: 100.00%] [G loss: 4.693547]\n",
      "epoch:27 step:21090 [D loss: 0.012607, acc.: 100.00%] [G loss: 4.153486]\n",
      "epoch:27 step:21091 [D loss: 0.016788, acc.: 100.00%] [G loss: 4.033586]\n",
      "epoch:27 step:21092 [D loss: 0.006916, acc.: 100.00%] [G loss: 3.703831]\n",
      "epoch:27 step:21093 [D loss: 0.009519, acc.: 100.00%] [G loss: 4.008183]\n",
      "epoch:27 step:21094 [D loss: 0.027442, acc.: 99.22%] [G loss: 3.181777]\n",
      "epoch:27 step:21095 [D loss: 0.042740, acc.: 99.22%] [G loss: 4.395218]\n",
      "epoch:27 step:21096 [D loss: 0.051651, acc.: 100.00%] [G loss: 3.820521]\n",
      "epoch:27 step:21097 [D loss: 0.013114, acc.: 100.00%] [G loss: 2.597515]\n",
      "epoch:27 step:21098 [D loss: 0.024707, acc.: 100.00%] [G loss: 2.518502]\n",
      "epoch:27 step:21099 [D loss: 0.033106, acc.: 100.00%] [G loss: 3.505441]\n",
      "epoch:27 step:21100 [D loss: 0.025138, acc.: 100.00%] [G loss: 3.952388]\n",
      "epoch:27 step:21101 [D loss: 0.799482, acc.: 58.59%] [G loss: 6.370337]\n",
      "epoch:27 step:21102 [D loss: 0.669781, acc.: 71.88%] [G loss: 3.643191]\n",
      "epoch:27 step:21103 [D loss: 0.362997, acc.: 82.81%] [G loss: 6.708210]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:27 step:21104 [D loss: 0.184905, acc.: 91.41%] [G loss: 5.693536]\n",
      "epoch:27 step:21105 [D loss: 0.051986, acc.: 99.22%] [G loss: 5.319974]\n",
      "epoch:27 step:21106 [D loss: 0.006402, acc.: 100.00%] [G loss: 4.230145]\n",
      "epoch:27 step:21107 [D loss: 0.058054, acc.: 98.44%] [G loss: 5.005974]\n",
      "epoch:27 step:21108 [D loss: 0.016566, acc.: 100.00%] [G loss: 4.751740]\n",
      "epoch:27 step:21109 [D loss: 0.012387, acc.: 100.00%] [G loss: 4.933355]\n",
      "epoch:27 step:21110 [D loss: 0.127400, acc.: 95.31%] [G loss: 1.715925]\n",
      "epoch:27 step:21111 [D loss: 0.218043, acc.: 90.62%] [G loss: 4.208393]\n",
      "epoch:27 step:21112 [D loss: 0.020812, acc.: 100.00%] [G loss: 5.437800]\n",
      "epoch:27 step:21113 [D loss: 0.031098, acc.: 99.22%] [G loss: 5.253078]\n",
      "epoch:27 step:21114 [D loss: 0.010722, acc.: 100.00%] [G loss: 4.484999]\n",
      "epoch:27 step:21115 [D loss: 0.017010, acc.: 100.00%] [G loss: 3.121458]\n",
      "epoch:27 step:21116 [D loss: 0.036864, acc.: 98.44%] [G loss: 4.561707]\n",
      "epoch:27 step:21117 [D loss: 0.053722, acc.: 99.22%] [G loss: 4.508224]\n",
      "epoch:27 step:21118 [D loss: 0.007974, acc.: 100.00%] [G loss: 5.476374]\n",
      "epoch:27 step:21119 [D loss: 0.003915, acc.: 100.00%] [G loss: 5.295683]\n",
      "epoch:27 step:21120 [D loss: 0.199541, acc.: 93.75%] [G loss: 3.604668]\n",
      "epoch:27 step:21121 [D loss: 0.020095, acc.: 100.00%] [G loss: 4.306620]\n",
      "epoch:27 step:21122 [D loss: 0.016294, acc.: 100.00%] [G loss: 4.516692]\n",
      "epoch:27 step:21123 [D loss: 0.429825, acc.: 79.69%] [G loss: 6.165598]\n",
      "epoch:27 step:21124 [D loss: 0.004113, acc.: 100.00%] [G loss: 7.865582]\n",
      "epoch:27 step:21125 [D loss: 0.839142, acc.: 59.38%] [G loss: 1.949686]\n",
      "epoch:27 step:21126 [D loss: 0.221971, acc.: 89.84%] [G loss: 5.596341]\n",
      "epoch:27 step:21127 [D loss: 0.010779, acc.: 100.00%] [G loss: 5.902050]\n",
      "epoch:27 step:21128 [D loss: 0.372120, acc.: 78.91%] [G loss: 2.333030]\n",
      "epoch:27 step:21129 [D loss: 0.191082, acc.: 92.19%] [G loss: 3.554845]\n",
      "epoch:27 step:21130 [D loss: 0.003112, acc.: 100.00%] [G loss: 4.996395]\n",
      "epoch:27 step:21131 [D loss: 0.373716, acc.: 85.16%] [G loss: 1.041195]\n",
      "epoch:27 step:21132 [D loss: 1.024402, acc.: 57.81%] [G loss: 7.400470]\n",
      "epoch:27 step:21133 [D loss: 1.084886, acc.: 58.59%] [G loss: 5.911220]\n",
      "epoch:27 step:21134 [D loss: 0.433440, acc.: 82.03%] [G loss: 3.419947]\n",
      "epoch:27 step:21135 [D loss: 0.060948, acc.: 99.22%] [G loss: 3.532747]\n",
      "epoch:27 step:21136 [D loss: 0.116089, acc.: 96.09%] [G loss: 3.867041]\n",
      "epoch:27 step:21137 [D loss: 0.020651, acc.: 100.00%] [G loss: 4.017392]\n",
      "epoch:27 step:21138 [D loss: 0.032380, acc.: 100.00%] [G loss: 4.022200]\n",
      "epoch:27 step:21139 [D loss: 0.036769, acc.: 99.22%] [G loss: 4.297842]\n",
      "epoch:27 step:21140 [D loss: 0.105653, acc.: 97.66%] [G loss: 3.131402]\n",
      "epoch:27 step:21141 [D loss: 0.057087, acc.: 100.00%] [G loss: 4.315063]\n",
      "epoch:27 step:21142 [D loss: 0.035682, acc.: 100.00%] [G loss: 4.203849]\n",
      "epoch:27 step:21143 [D loss: 0.068569, acc.: 98.44%] [G loss: 3.650315]\n",
      "epoch:27 step:21144 [D loss: 0.145337, acc.: 96.09%] [G loss: 4.820810]\n",
      "epoch:27 step:21145 [D loss: 0.073934, acc.: 99.22%] [G loss: 5.000618]\n",
      "epoch:27 step:21146 [D loss: 0.034401, acc.: 99.22%] [G loss: 3.929085]\n",
      "epoch:27 step:21147 [D loss: 0.074583, acc.: 97.66%] [G loss: 3.654267]\n",
      "epoch:27 step:21148 [D loss: 0.101441, acc.: 99.22%] [G loss: 4.741759]\n",
      "epoch:27 step:21149 [D loss: 0.096898, acc.: 94.53%] [G loss: 4.267384]\n",
      "epoch:27 step:21150 [D loss: 0.208300, acc.: 91.41%] [G loss: 3.216529]\n",
      "epoch:27 step:21151 [D loss: 0.030046, acc.: 100.00%] [G loss: 3.382051]\n",
      "epoch:27 step:21152 [D loss: 0.023667, acc.: 100.00%] [G loss: 4.265121]\n",
      "epoch:27 step:21153 [D loss: 0.035598, acc.: 100.00%] [G loss: 4.072263]\n",
      "epoch:27 step:21154 [D loss: 0.017164, acc.: 100.00%] [G loss: 4.659806]\n",
      "epoch:27 step:21155 [D loss: 0.036824, acc.: 99.22%] [G loss: 3.611067]\n",
      "epoch:27 step:21156 [D loss: 0.024042, acc.: 100.00%] [G loss: 3.860625]\n",
      "epoch:27 step:21157 [D loss: 0.021880, acc.: 100.00%] [G loss: 3.996502]\n",
      "epoch:27 step:21158 [D loss: 0.033395, acc.: 100.00%] [G loss: 3.747227]\n",
      "epoch:27 step:21159 [D loss: 0.022755, acc.: 100.00%] [G loss: 3.920559]\n",
      "epoch:27 step:21160 [D loss: 0.321200, acc.: 83.59%] [G loss: 6.369173]\n",
      "epoch:27 step:21161 [D loss: 0.095247, acc.: 96.09%] [G loss: 5.983232]\n",
      "epoch:27 step:21162 [D loss: 0.053365, acc.: 97.66%] [G loss: 4.981152]\n",
      "epoch:27 step:21163 [D loss: 0.007214, acc.: 100.00%] [G loss: 4.859886]\n",
      "epoch:27 step:21164 [D loss: 0.073410, acc.: 96.88%] [G loss: 2.111694]\n",
      "epoch:27 step:21165 [D loss: 0.113626, acc.: 96.88%] [G loss: 4.704233]\n",
      "epoch:27 step:21166 [D loss: 0.004648, acc.: 100.00%] [G loss: 5.307212]\n",
      "epoch:27 step:21167 [D loss: 0.067587, acc.: 98.44%] [G loss: 4.500506]\n",
      "epoch:27 step:21168 [D loss: 0.022945, acc.: 100.00%] [G loss: 4.878233]\n",
      "epoch:27 step:21169 [D loss: 0.005767, acc.: 100.00%] [G loss: 5.451660]\n",
      "epoch:27 step:21170 [D loss: 0.013430, acc.: 100.00%] [G loss: 4.468095]\n",
      "epoch:27 step:21171 [D loss: 0.006279, acc.: 100.00%] [G loss: 4.363003]\n",
      "epoch:27 step:21172 [D loss: 0.075693, acc.: 99.22%] [G loss: 3.120630]\n",
      "epoch:27 step:21173 [D loss: 0.022366, acc.: 100.00%] [G loss: 4.012685]\n",
      "epoch:27 step:21174 [D loss: 0.008666, acc.: 100.00%] [G loss: 3.948920]\n",
      "epoch:27 step:21175 [D loss: 0.050463, acc.: 99.22%] [G loss: 3.772072]\n",
      "epoch:27 step:21176 [D loss: 0.036322, acc.: 100.00%] [G loss: 4.463843]\n",
      "epoch:27 step:21177 [D loss: 2.995646, acc.: 28.12%] [G loss: 7.368693]\n",
      "epoch:27 step:21178 [D loss: 2.300580, acc.: 50.00%] [G loss: 5.002893]\n",
      "epoch:27 step:21179 [D loss: 0.835138, acc.: 58.59%] [G loss: 2.818680]\n",
      "epoch:27 step:21180 [D loss: 0.341942, acc.: 79.69%] [G loss: 1.514331]\n",
      "epoch:27 step:21181 [D loss: 0.220599, acc.: 92.19%] [G loss: 3.060278]\n",
      "epoch:27 step:21182 [D loss: 0.052119, acc.: 100.00%] [G loss: 3.494508]\n",
      "epoch:27 step:21183 [D loss: 0.425734, acc.: 82.81%] [G loss: 2.326479]\n",
      "epoch:27 step:21184 [D loss: 0.335203, acc.: 87.50%] [G loss: 1.806041]\n",
      "epoch:27 step:21185 [D loss: 0.124662, acc.: 97.66%] [G loss: 3.098179]\n",
      "epoch:27 step:21186 [D loss: 0.320871, acc.: 89.84%] [G loss: 3.456432]\n",
      "epoch:27 step:21187 [D loss: 0.107988, acc.: 98.44%] [G loss: 3.070185]\n",
      "epoch:27 step:21188 [D loss: 0.198937, acc.: 94.53%] [G loss: 2.704028]\n",
      "epoch:27 step:21189 [D loss: 0.106431, acc.: 97.66%] [G loss: 2.319085]\n",
      "epoch:27 step:21190 [D loss: 0.199308, acc.: 92.19%] [G loss: 1.651518]\n",
      "epoch:27 step:21191 [D loss: 1.015660, acc.: 55.47%] [G loss: 5.215710]\n",
      "epoch:27 step:21192 [D loss: 0.848893, acc.: 61.72%] [G loss: 4.755561]\n",
      "epoch:27 step:21193 [D loss: 0.180661, acc.: 92.97%] [G loss: 3.865952]\n",
      "epoch:27 step:21194 [D loss: 0.152455, acc.: 95.31%] [G loss: 2.661991]\n",
      "epoch:27 step:21195 [D loss: 0.299573, acc.: 84.38%] [G loss: 3.089167]\n",
      "epoch:27 step:21196 [D loss: 0.139816, acc.: 97.66%] [G loss: 3.666667]\n",
      "epoch:27 step:21197 [D loss: 0.076934, acc.: 99.22%] [G loss: 2.812612]\n",
      "epoch:27 step:21198 [D loss: 0.144853, acc.: 95.31%] [G loss: 3.261418]\n",
      "epoch:27 step:21199 [D loss: 0.083001, acc.: 100.00%] [G loss: 2.603139]\n",
      "epoch:27 step:21200 [D loss: 0.184517, acc.: 95.31%] [G loss: 2.788705]\n",
      "##############\n",
      "[0.93647268 1.04133572 0.9646513  0.88067866 2.11143486 1.11607971\n",
      " 1.09423635 1.01187564 1.11153751 1.10702012]\n",
      "##########\n",
      "epoch:27 step:21201 [D loss: 0.172044, acc.: 95.31%] [G loss: 2.694782]\n",
      "epoch:27 step:21202 [D loss: 0.092690, acc.: 98.44%] [G loss: 2.783455]\n",
      "epoch:27 step:21203 [D loss: 0.164014, acc.: 96.09%] [G loss: 3.712569]\n",
      "epoch:27 step:21204 [D loss: 0.353540, acc.: 82.81%] [G loss: 2.198175]\n",
      "epoch:27 step:21205 [D loss: 0.092821, acc.: 98.44%] [G loss: 2.278017]\n",
      "epoch:27 step:21206 [D loss: 0.284997, acc.: 87.50%] [G loss: 2.622300]\n",
      "epoch:27 step:21207 [D loss: 0.238449, acc.: 91.41%] [G loss: 3.166451]\n",
      "epoch:27 step:21208 [D loss: 0.033714, acc.: 100.00%] [G loss: 3.517188]\n",
      "epoch:27 step:21209 [D loss: 0.388309, acc.: 79.69%] [G loss: 2.665321]\n",
      "epoch:27 step:21210 [D loss: 0.183927, acc.: 93.75%] [G loss: 3.553609]\n",
      "epoch:27 step:21211 [D loss: 0.148312, acc.: 96.09%] [G loss: 3.339358]\n",
      "epoch:27 step:21212 [D loss: 0.234237, acc.: 92.97%] [G loss: 3.921562]\n",
      "epoch:27 step:21213 [D loss: 0.371936, acc.: 79.69%] [G loss: 2.905336]\n",
      "epoch:27 step:21214 [D loss: 0.150359, acc.: 96.88%] [G loss: 2.713154]\n",
      "epoch:27 step:21215 [D loss: 0.078144, acc.: 99.22%] [G loss: 3.812063]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:27 step:21216 [D loss: 0.377596, acc.: 82.81%] [G loss: 3.458455]\n",
      "epoch:27 step:21217 [D loss: 0.187446, acc.: 91.41%] [G loss: 2.597597]\n",
      "epoch:27 step:21218 [D loss: 0.035261, acc.: 100.00%] [G loss: 3.261668]\n",
      "epoch:27 step:21219 [D loss: 0.124496, acc.: 96.88%] [G loss: 4.048125]\n",
      "epoch:27 step:21220 [D loss: 0.060452, acc.: 99.22%] [G loss: 4.296834]\n",
      "epoch:27 step:21221 [D loss: 0.330430, acc.: 84.38%] [G loss: 3.223152]\n",
      "epoch:27 step:21222 [D loss: 0.060041, acc.: 99.22%] [G loss: 3.861851]\n",
      "epoch:27 step:21223 [D loss: 0.152361, acc.: 96.09%] [G loss: 2.479471]\n",
      "epoch:27 step:21224 [D loss: 0.085536, acc.: 96.09%] [G loss: 2.632526]\n",
      "epoch:27 step:21225 [D loss: 0.184061, acc.: 95.31%] [G loss: 3.924796]\n",
      "epoch:27 step:21226 [D loss: 0.077647, acc.: 98.44%] [G loss: 3.828873]\n",
      "epoch:27 step:21227 [D loss: 0.137662, acc.: 96.88%] [G loss: 3.871293]\n",
      "epoch:27 step:21228 [D loss: 0.290016, acc.: 90.62%] [G loss: 2.060565]\n",
      "epoch:27 step:21229 [D loss: 0.159673, acc.: 95.31%] [G loss: 4.203403]\n",
      "epoch:27 step:21230 [D loss: 0.092386, acc.: 96.09%] [G loss: 4.464323]\n",
      "epoch:27 step:21231 [D loss: 0.171339, acc.: 94.53%] [G loss: 2.291174]\n",
      "epoch:27 step:21232 [D loss: 0.119452, acc.: 97.66%] [G loss: 3.929227]\n",
      "epoch:27 step:21233 [D loss: 0.054823, acc.: 99.22%] [G loss: 4.025650]\n",
      "epoch:27 step:21234 [D loss: 0.122227, acc.: 97.66%] [G loss: 3.563340]\n",
      "epoch:27 step:21235 [D loss: 0.080487, acc.: 100.00%] [G loss: 3.729672]\n",
      "epoch:27 step:21236 [D loss: 0.021413, acc.: 100.00%] [G loss: 4.456865]\n",
      "epoch:27 step:21237 [D loss: 0.075669, acc.: 96.88%] [G loss: 3.594325]\n",
      "epoch:27 step:21238 [D loss: 0.109753, acc.: 98.44%] [G loss: 3.487561]\n",
      "epoch:27 step:21239 [D loss: 0.042276, acc.: 99.22%] [G loss: 3.744035]\n",
      "epoch:27 step:21240 [D loss: 0.009982, acc.: 100.00%] [G loss: 3.500676]\n",
      "epoch:27 step:21241 [D loss: 0.074968, acc.: 99.22%] [G loss: 3.885660]\n",
      "epoch:27 step:21242 [D loss: 0.102143, acc.: 99.22%] [G loss: 1.616835]\n",
      "epoch:27 step:21243 [D loss: 0.451738, acc.: 80.47%] [G loss: 5.447690]\n",
      "epoch:27 step:21244 [D loss: 0.411504, acc.: 82.03%] [G loss: 3.053216]\n",
      "epoch:27 step:21245 [D loss: 0.119243, acc.: 98.44%] [G loss: 3.987764]\n",
      "epoch:27 step:21246 [D loss: 0.010744, acc.: 100.00%] [G loss: 5.103106]\n",
      "epoch:27 step:21247 [D loss: 0.023779, acc.: 100.00%] [G loss: 4.477742]\n",
      "epoch:27 step:21248 [D loss: 0.350577, acc.: 83.59%] [G loss: 2.873519]\n",
      "epoch:27 step:21249 [D loss: 0.025704, acc.: 100.00%] [G loss: 3.692745]\n",
      "epoch:27 step:21250 [D loss: 0.050189, acc.: 99.22%] [G loss: 3.319036]\n",
      "epoch:27 step:21251 [D loss: 0.038981, acc.: 99.22%] [G loss: 5.038089]\n",
      "epoch:27 step:21252 [D loss: 0.272060, acc.: 89.06%] [G loss: 3.351355]\n",
      "epoch:27 step:21253 [D loss: 0.032089, acc.: 100.00%] [G loss: 3.305375]\n",
      "epoch:27 step:21254 [D loss: 0.016495, acc.: 100.00%] [G loss: 3.945373]\n",
      "epoch:27 step:21255 [D loss: 0.096691, acc.: 98.44%] [G loss: 5.171627]\n",
      "epoch:27 step:21256 [D loss: 0.013323, acc.: 99.22%] [G loss: 5.336338]\n",
      "epoch:27 step:21257 [D loss: 0.078601, acc.: 97.66%] [G loss: 4.414757]\n",
      "epoch:27 step:21258 [D loss: 0.043276, acc.: 99.22%] [G loss: 3.602966]\n",
      "epoch:27 step:21259 [D loss: 0.033241, acc.: 100.00%] [G loss: 3.568456]\n",
      "epoch:27 step:21260 [D loss: 0.066732, acc.: 99.22%] [G loss: 4.165309]\n",
      "epoch:27 step:21261 [D loss: 0.036034, acc.: 99.22%] [G loss: 2.546645]\n",
      "epoch:27 step:21262 [D loss: 0.058609, acc.: 99.22%] [G loss: 4.909940]\n",
      "epoch:27 step:21263 [D loss: 0.068877, acc.: 99.22%] [G loss: 4.115960]\n",
      "epoch:27 step:21264 [D loss: 0.042040, acc.: 99.22%] [G loss: 3.052983]\n",
      "epoch:27 step:21265 [D loss: 0.064167, acc.: 100.00%] [G loss: 4.395715]\n",
      "epoch:27 step:21266 [D loss: 0.104698, acc.: 96.88%] [G loss: 3.244252]\n",
      "epoch:27 step:21267 [D loss: 0.063474, acc.: 100.00%] [G loss: 5.316448]\n",
      "epoch:27 step:21268 [D loss: 0.187891, acc.: 92.19%] [G loss: 3.298816]\n",
      "epoch:27 step:21269 [D loss: 0.104591, acc.: 97.66%] [G loss: 3.521499]\n",
      "epoch:27 step:21270 [D loss: 0.029943, acc.: 99.22%] [G loss: 4.331636]\n",
      "epoch:27 step:21271 [D loss: 0.075084, acc.: 96.88%] [G loss: 3.726381]\n",
      "epoch:27 step:21272 [D loss: 0.033245, acc.: 99.22%] [G loss: 4.281430]\n",
      "epoch:27 step:21273 [D loss: 0.029137, acc.: 100.00%] [G loss: 4.414921]\n",
      "epoch:27 step:21274 [D loss: 0.163230, acc.: 94.53%] [G loss: 4.814789]\n",
      "epoch:27 step:21275 [D loss: 0.032207, acc.: 99.22%] [G loss: 5.319035]\n",
      "epoch:27 step:21276 [D loss: 0.013722, acc.: 100.00%] [G loss: 4.588895]\n",
      "epoch:27 step:21277 [D loss: 0.088329, acc.: 99.22%] [G loss: 6.223629]\n",
      "epoch:27 step:21278 [D loss: 0.045275, acc.: 99.22%] [G loss: 5.860826]\n",
      "epoch:27 step:21279 [D loss: 0.030797, acc.: 100.00%] [G loss: 5.171140]\n",
      "epoch:27 step:21280 [D loss: 0.131822, acc.: 94.53%] [G loss: 2.873828]\n",
      "epoch:27 step:21281 [D loss: 0.057877, acc.: 99.22%] [G loss: 4.635628]\n",
      "epoch:27 step:21282 [D loss: 0.020435, acc.: 100.00%] [G loss: 5.041574]\n",
      "epoch:27 step:21283 [D loss: 0.242514, acc.: 89.06%] [G loss: 4.419087]\n",
      "epoch:27 step:21284 [D loss: 0.093634, acc.: 98.44%] [G loss: 4.383385]\n",
      "epoch:27 step:21285 [D loss: 0.021351, acc.: 100.00%] [G loss: 4.457527]\n",
      "epoch:27 step:21286 [D loss: 0.097614, acc.: 97.66%] [G loss: 4.497234]\n",
      "epoch:27 step:21287 [D loss: 0.030375, acc.: 99.22%] [G loss: 4.237393]\n",
      "epoch:27 step:21288 [D loss: 0.020368, acc.: 100.00%] [G loss: 4.205309]\n",
      "epoch:27 step:21289 [D loss: 0.071091, acc.: 98.44%] [G loss: 3.338863]\n",
      "epoch:27 step:21290 [D loss: 0.022847, acc.: 100.00%] [G loss: 4.578961]\n",
      "epoch:27 step:21291 [D loss: 0.029252, acc.: 100.00%] [G loss: 3.949745]\n",
      "epoch:27 step:21292 [D loss: 0.081206, acc.: 99.22%] [G loss: 4.483750]\n",
      "epoch:27 step:21293 [D loss: 0.012574, acc.: 100.00%] [G loss: 2.963738]\n",
      "epoch:27 step:21294 [D loss: 0.004090, acc.: 100.00%] [G loss: 3.554339]\n",
      "epoch:27 step:21295 [D loss: 0.018822, acc.: 100.00%] [G loss: 4.269093]\n",
      "epoch:27 step:21296 [D loss: 0.200431, acc.: 92.19%] [G loss: 6.474712]\n",
      "epoch:27 step:21297 [D loss: 0.820680, acc.: 60.16%] [G loss: 4.876012]\n",
      "epoch:27 step:21298 [D loss: 0.001364, acc.: 100.00%] [G loss: 5.929066]\n",
      "epoch:27 step:21299 [D loss: 0.075083, acc.: 96.88%] [G loss: 4.245093]\n",
      "epoch:27 step:21300 [D loss: 0.015520, acc.: 99.22%] [G loss: 1.002872]\n",
      "epoch:27 step:21301 [D loss: 1.148434, acc.: 57.03%] [G loss: 9.150902]\n",
      "epoch:27 step:21302 [D loss: 2.215112, acc.: 50.78%] [G loss: 6.746784]\n",
      "epoch:27 step:21303 [D loss: 0.039478, acc.: 98.44%] [G loss: 4.669351]\n",
      "epoch:27 step:21304 [D loss: 0.102801, acc.: 97.66%] [G loss: 3.977562]\n",
      "epoch:27 step:21305 [D loss: 0.120211, acc.: 96.09%] [G loss: 3.622156]\n",
      "epoch:27 step:21306 [D loss: 0.036236, acc.: 99.22%] [G loss: 3.140460]\n",
      "epoch:27 step:21307 [D loss: 0.069615, acc.: 96.88%] [G loss: 5.248258]\n",
      "epoch:27 step:21308 [D loss: 0.013694, acc.: 100.00%] [G loss: 4.198236]\n",
      "epoch:27 step:21309 [D loss: 0.036327, acc.: 98.44%] [G loss: 1.213775]\n",
      "epoch:27 step:21310 [D loss: 0.334408, acc.: 85.94%] [G loss: 6.268490]\n",
      "epoch:27 step:21311 [D loss: 1.063945, acc.: 54.69%] [G loss: 2.738918]\n",
      "epoch:27 step:21312 [D loss: 0.080012, acc.: 98.44%] [G loss: 1.783497]\n",
      "epoch:27 step:21313 [D loss: 0.030400, acc.: 100.00%] [G loss: 3.042901]\n",
      "epoch:27 step:21314 [D loss: 0.072225, acc.: 97.66%] [G loss: 3.720101]\n",
      "epoch:27 step:21315 [D loss: 0.030050, acc.: 99.22%] [G loss: 4.806338]\n",
      "epoch:27 step:21316 [D loss: 0.041979, acc.: 99.22%] [G loss: 4.023522]\n",
      "epoch:27 step:21317 [D loss: 0.103613, acc.: 96.88%] [G loss: 3.866693]\n",
      "epoch:27 step:21318 [D loss: 0.132056, acc.: 93.75%] [G loss: 2.871131]\n",
      "epoch:27 step:21319 [D loss: 0.054310, acc.: 99.22%] [G loss: 2.205182]\n",
      "epoch:27 step:21320 [D loss: 0.010511, acc.: 100.00%] [G loss: 2.923961]\n",
      "epoch:27 step:21321 [D loss: 0.086246, acc.: 100.00%] [G loss: 3.821809]\n",
      "epoch:27 step:21322 [D loss: 0.013229, acc.: 100.00%] [G loss: 3.917080]\n",
      "epoch:27 step:21323 [D loss: 0.132705, acc.: 95.31%] [G loss: 1.077940]\n",
      "epoch:27 step:21324 [D loss: 0.286088, acc.: 82.81%] [G loss: 5.683769]\n",
      "epoch:27 step:21325 [D loss: 0.043912, acc.: 99.22%] [G loss: 7.087193]\n",
      "epoch:27 step:21326 [D loss: 0.130684, acc.: 96.88%] [G loss: 5.304616]\n",
      "epoch:27 step:21327 [D loss: 0.946960, acc.: 63.28%] [G loss: 6.351032]\n",
      "epoch:27 step:21328 [D loss: 0.029255, acc.: 99.22%] [G loss: 7.683081]\n",
      "epoch:27 step:21329 [D loss: 0.235465, acc.: 95.31%] [G loss: 6.501374]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:27 step:21330 [D loss: 0.009216, acc.: 100.00%] [G loss: 4.872721]\n",
      "epoch:27 step:21331 [D loss: 0.006007, acc.: 100.00%] [G loss: 5.386814]\n",
      "epoch:27 step:21332 [D loss: 0.006899, acc.: 100.00%] [G loss: 5.596849]\n",
      "epoch:27 step:21333 [D loss: 0.035665, acc.: 99.22%] [G loss: 4.275376]\n",
      "epoch:27 step:21334 [D loss: 0.030057, acc.: 100.00%] [G loss: 3.501993]\n",
      "epoch:27 step:21335 [D loss: 0.012972, acc.: 100.00%] [G loss: 3.883338]\n",
      "epoch:27 step:21336 [D loss: 0.009455, acc.: 100.00%] [G loss: 3.355385]\n",
      "epoch:27 step:21337 [D loss: 0.170886, acc.: 92.97%] [G loss: 4.917614]\n",
      "epoch:27 step:21338 [D loss: 0.040220, acc.: 99.22%] [G loss: 4.748896]\n",
      "epoch:27 step:21339 [D loss: 0.042547, acc.: 99.22%] [G loss: 4.381380]\n",
      "epoch:27 step:21340 [D loss: 0.022916, acc.: 100.00%] [G loss: 2.915105]\n",
      "epoch:27 step:21341 [D loss: 0.041993, acc.: 99.22%] [G loss: 3.552140]\n",
      "epoch:27 step:21342 [D loss: 0.013023, acc.: 100.00%] [G loss: 4.052840]\n",
      "epoch:27 step:21343 [D loss: 0.108607, acc.: 96.88%] [G loss: 5.794575]\n",
      "epoch:27 step:21344 [D loss: 0.049851, acc.: 98.44%] [G loss: 6.737376]\n",
      "epoch:27 step:21345 [D loss: 0.270255, acc.: 89.06%] [G loss: 2.860449]\n",
      "epoch:27 step:21346 [D loss: 0.280768, acc.: 86.72%] [G loss: 6.525261]\n",
      "epoch:27 step:21347 [D loss: 0.149624, acc.: 91.41%] [G loss: 5.975403]\n",
      "epoch:27 step:21348 [D loss: 0.206245, acc.: 91.41%] [G loss: 4.220784]\n",
      "epoch:27 step:21349 [D loss: 0.037135, acc.: 100.00%] [G loss: 4.353062]\n",
      "epoch:27 step:21350 [D loss: 0.011860, acc.: 100.00%] [G loss: 5.084147]\n",
      "epoch:27 step:21351 [D loss: 0.014479, acc.: 100.00%] [G loss: 4.302095]\n",
      "epoch:27 step:21352 [D loss: 0.030013, acc.: 99.22%] [G loss: 5.184157]\n",
      "epoch:27 step:21353 [D loss: 0.008453, acc.: 100.00%] [G loss: 4.862713]\n",
      "epoch:27 step:21354 [D loss: 0.063238, acc.: 99.22%] [G loss: 4.419795]\n",
      "epoch:27 step:21355 [D loss: 0.010495, acc.: 100.00%] [G loss: 4.459541]\n",
      "epoch:27 step:21356 [D loss: 0.010849, acc.: 100.00%] [G loss: 4.411380]\n",
      "epoch:27 step:21357 [D loss: 0.029752, acc.: 100.00%] [G loss: 4.425639]\n",
      "epoch:27 step:21358 [D loss: 0.031752, acc.: 100.00%] [G loss: 4.194321]\n",
      "epoch:27 step:21359 [D loss: 0.008662, acc.: 100.00%] [G loss: 5.077676]\n",
      "epoch:27 step:21360 [D loss: 0.034360, acc.: 99.22%] [G loss: 4.033713]\n",
      "epoch:27 step:21361 [D loss: 0.025808, acc.: 100.00%] [G loss: 4.190138]\n",
      "epoch:27 step:21362 [D loss: 0.010173, acc.: 100.00%] [G loss: 4.235644]\n",
      "epoch:27 step:21363 [D loss: 0.006777, acc.: 100.00%] [G loss: 4.188317]\n",
      "epoch:27 step:21364 [D loss: 0.209544, acc.: 94.53%] [G loss: 5.796841]\n",
      "epoch:27 step:21365 [D loss: 0.141775, acc.: 94.53%] [G loss: 4.626019]\n",
      "epoch:27 step:21366 [D loss: 0.018532, acc.: 100.00%] [G loss: 4.081070]\n",
      "epoch:27 step:21367 [D loss: 0.024499, acc.: 100.00%] [G loss: 5.177511]\n",
      "epoch:27 step:21368 [D loss: 0.008543, acc.: 100.00%] [G loss: 4.812630]\n",
      "epoch:27 step:21369 [D loss: 0.006631, acc.: 100.00%] [G loss: 5.053905]\n",
      "epoch:27 step:21370 [D loss: 0.008165, acc.: 100.00%] [G loss: 4.549496]\n",
      "epoch:27 step:21371 [D loss: 0.011997, acc.: 100.00%] [G loss: 4.621546]\n",
      "epoch:27 step:21372 [D loss: 0.041914, acc.: 99.22%] [G loss: 5.047135]\n",
      "epoch:27 step:21373 [D loss: 1.067567, acc.: 51.56%] [G loss: 7.350161]\n",
      "epoch:27 step:21374 [D loss: 1.366968, acc.: 55.47%] [G loss: 3.573684]\n",
      "epoch:27 step:21375 [D loss: 0.069132, acc.: 99.22%] [G loss: 2.525333]\n",
      "epoch:27 step:21376 [D loss: 0.079461, acc.: 97.66%] [G loss: 4.179900]\n",
      "epoch:27 step:21377 [D loss: 0.013775, acc.: 100.00%] [G loss: 4.870599]\n",
      "epoch:27 step:21378 [D loss: 0.012709, acc.: 100.00%] [G loss: 3.298671]\n",
      "epoch:27 step:21379 [D loss: 0.019424, acc.: 100.00%] [G loss: 4.616542]\n",
      "epoch:27 step:21380 [D loss: 0.028639, acc.: 100.00%] [G loss: 4.379205]\n",
      "epoch:27 step:21381 [D loss: 0.055359, acc.: 98.44%] [G loss: 3.242410]\n",
      "epoch:27 step:21382 [D loss: 0.105774, acc.: 96.09%] [G loss: 4.667089]\n",
      "epoch:27 step:21383 [D loss: 0.023523, acc.: 100.00%] [G loss: 5.169237]\n",
      "epoch:27 step:21384 [D loss: 0.062754, acc.: 98.44%] [G loss: 3.561282]\n",
      "epoch:27 step:21385 [D loss: 0.015134, acc.: 100.00%] [G loss: 2.667431]\n",
      "epoch:27 step:21386 [D loss: 0.030031, acc.: 100.00%] [G loss: 3.496193]\n",
      "epoch:27 step:21387 [D loss: 0.023360, acc.: 100.00%] [G loss: 3.784778]\n",
      "epoch:27 step:21388 [D loss: 0.006752, acc.: 100.00%] [G loss: 3.709868]\n",
      "epoch:27 step:21389 [D loss: 0.179522, acc.: 93.75%] [G loss: 6.321007]\n",
      "epoch:27 step:21390 [D loss: 0.397716, acc.: 80.47%] [G loss: 2.989737]\n",
      "epoch:27 step:21391 [D loss: 0.066293, acc.: 99.22%] [G loss: 4.322726]\n",
      "epoch:27 step:21392 [D loss: 0.018775, acc.: 100.00%] [G loss: 3.868187]\n",
      "epoch:27 step:21393 [D loss: 0.010318, acc.: 100.00%] [G loss: 4.190828]\n",
      "epoch:27 step:21394 [D loss: 0.011285, acc.: 100.00%] [G loss: 3.933733]\n",
      "epoch:27 step:21395 [D loss: 0.010264, acc.: 100.00%] [G loss: 3.919712]\n",
      "epoch:27 step:21396 [D loss: 0.178976, acc.: 93.75%] [G loss: 4.963430]\n",
      "epoch:27 step:21397 [D loss: 0.037224, acc.: 99.22%] [G loss: 5.354829]\n",
      "epoch:27 step:21398 [D loss: 0.015024, acc.: 100.00%] [G loss: 5.219272]\n",
      "epoch:27 step:21399 [D loss: 0.167600, acc.: 92.97%] [G loss: 2.459635]\n",
      "epoch:27 step:21400 [D loss: 0.229561, acc.: 89.06%] [G loss: 6.064599]\n",
      "##############\n",
      "[0.99253634 1.04065283 2.10803294 0.95381575 2.11141492 1.11650455\n",
      " 2.10659734 0.99444254 1.12471247 1.07177293]\n",
      "##########\n",
      "epoch:27 step:21401 [D loss: 0.022298, acc.: 100.00%] [G loss: 7.353414]\n",
      "epoch:27 step:21402 [D loss: 1.690989, acc.: 36.72%] [G loss: 6.238400]\n",
      "epoch:27 step:21403 [D loss: 0.004841, acc.: 100.00%] [G loss: 6.722674]\n",
      "epoch:27 step:21404 [D loss: 0.039556, acc.: 98.44%] [G loss: 6.107896]\n",
      "epoch:27 step:21405 [D loss: 0.004322, acc.: 100.00%] [G loss: 6.009591]\n",
      "epoch:27 step:21406 [D loss: 0.006038, acc.: 100.00%] [G loss: 5.877569]\n",
      "epoch:27 step:21407 [D loss: 0.132583, acc.: 96.09%] [G loss: 3.748115]\n",
      "epoch:27 step:21408 [D loss: 0.222851, acc.: 87.50%] [G loss: 5.503355]\n",
      "epoch:27 step:21409 [D loss: 0.005391, acc.: 100.00%] [G loss: 6.409826]\n",
      "epoch:27 step:21410 [D loss: 0.358879, acc.: 84.38%] [G loss: 4.383405]\n",
      "epoch:27 step:21411 [D loss: 0.291593, acc.: 84.38%] [G loss: 5.547224]\n",
      "epoch:27 step:21412 [D loss: 0.008805, acc.: 100.00%] [G loss: 6.291519]\n",
      "epoch:27 step:21413 [D loss: 0.088013, acc.: 96.09%] [G loss: 5.094096]\n",
      "epoch:27 step:21414 [D loss: 0.030946, acc.: 100.00%] [G loss: 3.881721]\n",
      "epoch:27 step:21415 [D loss: 0.008902, acc.: 100.00%] [G loss: 4.720784]\n",
      "epoch:27 step:21416 [D loss: 0.034481, acc.: 98.44%] [G loss: 4.178547]\n",
      "epoch:27 step:21417 [D loss: 0.024761, acc.: 99.22%] [G loss: 4.537327]\n",
      "epoch:27 step:21418 [D loss: 0.073617, acc.: 97.66%] [G loss: 2.714644]\n",
      "epoch:27 step:21419 [D loss: 0.022114, acc.: 100.00%] [G loss: 1.373798]\n",
      "epoch:27 step:21420 [D loss: 0.079942, acc.: 98.44%] [G loss: 2.901596]\n",
      "epoch:27 step:21421 [D loss: 0.025605, acc.: 100.00%] [G loss: 2.987121]\n",
      "epoch:27 step:21422 [D loss: 0.029542, acc.: 100.00%] [G loss: 4.209873]\n",
      "epoch:27 step:21423 [D loss: 0.286450, acc.: 89.84%] [G loss: 3.998531]\n",
      "epoch:27 step:21424 [D loss: 0.023553, acc.: 99.22%] [G loss: 4.697970]\n",
      "epoch:27 step:21425 [D loss: 0.182745, acc.: 92.97%] [G loss: 0.703835]\n",
      "epoch:27 step:21426 [D loss: 0.068421, acc.: 100.00%] [G loss: 3.248296]\n",
      "epoch:27 step:21427 [D loss: 0.019367, acc.: 100.00%] [G loss: 4.693989]\n",
      "epoch:27 step:21428 [D loss: 0.010316, acc.: 100.00%] [G loss: 3.280348]\n",
      "epoch:27 step:21429 [D loss: 0.006047, acc.: 100.00%] [G loss: 3.304736]\n",
      "epoch:27 step:21430 [D loss: 0.140674, acc.: 96.88%] [G loss: 2.590844]\n",
      "epoch:27 step:21431 [D loss: 0.008141, acc.: 100.00%] [G loss: 4.694885]\n",
      "epoch:27 step:21432 [D loss: 0.012966, acc.: 100.00%] [G loss: 4.780626]\n",
      "epoch:27 step:21433 [D loss: 0.066146, acc.: 100.00%] [G loss: 2.551782]\n",
      "epoch:27 step:21434 [D loss: 0.031341, acc.: 100.00%] [G loss: 3.273427]\n",
      "epoch:27 step:21435 [D loss: 0.012058, acc.: 100.00%] [G loss: 4.089019]\n",
      "epoch:27 step:21436 [D loss: 0.046936, acc.: 100.00%] [G loss: 5.597872]\n",
      "epoch:27 step:21437 [D loss: 0.136659, acc.: 96.88%] [G loss: 4.574438]\n",
      "epoch:27 step:21438 [D loss: 0.012203, acc.: 100.00%] [G loss: 3.801008]\n",
      "epoch:27 step:21439 [D loss: 0.014021, acc.: 100.00%] [G loss: 4.185827]\n",
      "epoch:27 step:21440 [D loss: 0.047892, acc.: 100.00%] [G loss: 5.013120]\n",
      "epoch:27 step:21441 [D loss: 0.013042, acc.: 100.00%] [G loss: 5.250466]\n",
      "epoch:27 step:21442 [D loss: 0.030737, acc.: 99.22%] [G loss: 4.038488]\n",
      "epoch:27 step:21443 [D loss: 0.058219, acc.: 100.00%] [G loss: 5.982040]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:27 step:21444 [D loss: 0.003352, acc.: 100.00%] [G loss: 6.615316]\n",
      "epoch:27 step:21445 [D loss: 0.047915, acc.: 98.44%] [G loss: 5.566166]\n",
      "epoch:27 step:21446 [D loss: 0.016732, acc.: 100.00%] [G loss: 4.970909]\n",
      "epoch:27 step:21447 [D loss: 0.006393, acc.: 100.00%] [G loss: 4.334694]\n",
      "epoch:27 step:21448 [D loss: 0.031719, acc.: 99.22%] [G loss: 3.982692]\n",
      "epoch:27 step:21449 [D loss: 0.007576, acc.: 100.00%] [G loss: 3.688056]\n",
      "epoch:27 step:21450 [D loss: 0.040608, acc.: 100.00%] [G loss: 4.249873]\n",
      "epoch:27 step:21451 [D loss: 0.740684, acc.: 60.94%] [G loss: 8.657223]\n",
      "epoch:27 step:21452 [D loss: 0.184139, acc.: 92.97%] [G loss: 8.560083]\n",
      "epoch:27 step:21453 [D loss: 0.151112, acc.: 93.75%] [G loss: 6.936170]\n",
      "epoch:27 step:21454 [D loss: 0.001955, acc.: 100.00%] [G loss: 5.827263]\n",
      "epoch:27 step:21455 [D loss: 0.001324, acc.: 100.00%] [G loss: 4.313008]\n",
      "epoch:27 step:21456 [D loss: 0.021164, acc.: 100.00%] [G loss: 4.232947]\n",
      "epoch:27 step:21457 [D loss: 0.014391, acc.: 99.22%] [G loss: 5.073354]\n",
      "epoch:27 step:21458 [D loss: 0.003690, acc.: 100.00%] [G loss: 2.954494]\n",
      "epoch:27 step:21459 [D loss: 0.156351, acc.: 92.97%] [G loss: 7.315425]\n",
      "epoch:27 step:21460 [D loss: 0.015992, acc.: 100.00%] [G loss: 8.323833]\n",
      "epoch:27 step:21461 [D loss: 2.310063, acc.: 50.00%] [G loss: 0.675157]\n",
      "epoch:27 step:21462 [D loss: 1.670564, acc.: 56.25%] [G loss: 6.528948]\n",
      "epoch:27 step:21463 [D loss: 2.142835, acc.: 50.00%] [G loss: 4.817426]\n",
      "epoch:27 step:21464 [D loss: 0.951769, acc.: 54.69%] [G loss: 1.566614]\n",
      "epoch:27 step:21465 [D loss: 0.438765, acc.: 78.12%] [G loss: 3.202354]\n",
      "epoch:27 step:21466 [D loss: 0.197038, acc.: 92.19%] [G loss: 3.343940]\n",
      "epoch:27 step:21467 [D loss: 0.316407, acc.: 85.94%] [G loss: 1.653776]\n",
      "epoch:27 step:21468 [D loss: 0.264713, acc.: 92.19%] [G loss: 3.076927]\n",
      "epoch:27 step:21469 [D loss: 0.089077, acc.: 99.22%] [G loss: 3.187306]\n",
      "epoch:27 step:21470 [D loss: 0.203894, acc.: 91.41%] [G loss: 2.922867]\n",
      "epoch:27 step:21471 [D loss: 0.157195, acc.: 97.66%] [G loss: 2.665701]\n",
      "epoch:27 step:21472 [D loss: 0.221701, acc.: 93.75%] [G loss: 2.617829]\n",
      "epoch:27 step:21473 [D loss: 0.041689, acc.: 100.00%] [G loss: 2.497208]\n",
      "epoch:27 step:21474 [D loss: 0.150741, acc.: 96.88%] [G loss: 2.865313]\n",
      "epoch:27 step:21475 [D loss: 0.174529, acc.: 97.66%] [G loss: 1.809002]\n",
      "epoch:27 step:21476 [D loss: 0.470720, acc.: 78.12%] [G loss: 2.492185]\n",
      "epoch:27 step:21477 [D loss: 0.158803, acc.: 94.53%] [G loss: 3.184229]\n",
      "epoch:27 step:21478 [D loss: 0.291203, acc.: 86.72%] [G loss: 3.704056]\n",
      "epoch:27 step:21479 [D loss: 0.115494, acc.: 96.09%] [G loss: 3.661637]\n",
      "epoch:27 step:21480 [D loss: 0.281779, acc.: 89.84%] [G loss: 1.031020]\n",
      "epoch:27 step:21481 [D loss: 0.599705, acc.: 75.00%] [G loss: 4.157632]\n",
      "epoch:27 step:21482 [D loss: 0.264913, acc.: 88.28%] [G loss: 3.987021]\n",
      "epoch:27 step:21483 [D loss: 0.553940, acc.: 71.88%] [G loss: 2.044958]\n",
      "epoch:27 step:21484 [D loss: 0.211493, acc.: 91.41%] [G loss: 3.998105]\n",
      "epoch:27 step:21485 [D loss: 0.056036, acc.: 100.00%] [G loss: 2.881057]\n",
      "epoch:27 step:21486 [D loss: 0.229009, acc.: 91.41%] [G loss: 2.135007]\n",
      "epoch:27 step:21487 [D loss: 0.192724, acc.: 92.97%] [G loss: 1.914127]\n",
      "epoch:27 step:21488 [D loss: 0.051165, acc.: 100.00%] [G loss: 3.712456]\n",
      "epoch:27 step:21489 [D loss: 0.112048, acc.: 96.09%] [G loss: 2.299675]\n",
      "epoch:27 step:21490 [D loss: 0.152892, acc.: 94.53%] [G loss: 2.533371]\n",
      "epoch:27 step:21491 [D loss: 0.187851, acc.: 93.75%] [G loss: 3.439136]\n",
      "epoch:27 step:21492 [D loss: 0.449796, acc.: 80.47%] [G loss: 3.488442]\n",
      "epoch:27 step:21493 [D loss: 0.316038, acc.: 86.72%] [G loss: 2.354100]\n",
      "epoch:27 step:21494 [D loss: 0.074517, acc.: 100.00%] [G loss: 3.644814]\n",
      "epoch:27 step:21495 [D loss: 0.171156, acc.: 92.19%] [G loss: 1.878870]\n",
      "epoch:27 step:21496 [D loss: 0.294228, acc.: 87.50%] [G loss: 0.964455]\n",
      "epoch:27 step:21497 [D loss: 0.328273, acc.: 85.94%] [G loss: 3.519490]\n",
      "epoch:27 step:21498 [D loss: 0.238467, acc.: 91.41%] [G loss: 4.273881]\n",
      "epoch:27 step:21499 [D loss: 0.074966, acc.: 99.22%] [G loss: 3.468666]\n",
      "epoch:27 step:21500 [D loss: 0.122657, acc.: 97.66%] [G loss: 2.584296]\n",
      "epoch:27 step:21501 [D loss: 0.139925, acc.: 96.88%] [G loss: 3.774841]\n",
      "epoch:27 step:21502 [D loss: 0.101319, acc.: 98.44%] [G loss: 3.127411]\n",
      "epoch:27 step:21503 [D loss: 0.181854, acc.: 93.75%] [G loss: 2.213274]\n",
      "epoch:27 step:21504 [D loss: 0.197355, acc.: 92.19%] [G loss: 3.805341]\n",
      "epoch:27 step:21505 [D loss: 0.258648, acc.: 91.41%] [G loss: 3.151410]\n",
      "epoch:27 step:21506 [D loss: 0.106303, acc.: 98.44%] [G loss: 3.696528]\n",
      "epoch:27 step:21507 [D loss: 0.201351, acc.: 92.97%] [G loss: 4.328353]\n",
      "epoch:27 step:21508 [D loss: 1.052164, acc.: 51.56%] [G loss: 6.135479]\n",
      "epoch:27 step:21509 [D loss: 0.110142, acc.: 94.53%] [G loss: 6.639115]\n",
      "epoch:27 step:21510 [D loss: 0.075356, acc.: 97.66%] [G loss: 6.132324]\n",
      "epoch:27 step:21511 [D loss: 0.176750, acc.: 90.62%] [G loss: 3.960750]\n",
      "epoch:27 step:21512 [D loss: 0.123957, acc.: 95.31%] [G loss: 3.762417]\n",
      "epoch:27 step:21513 [D loss: 0.029403, acc.: 99.22%] [G loss: 4.782887]\n",
      "epoch:27 step:21514 [D loss: 0.008811, acc.: 100.00%] [G loss: 3.632594]\n",
      "epoch:27 step:21515 [D loss: 0.042970, acc.: 99.22%] [G loss: 2.998828]\n",
      "epoch:27 step:21516 [D loss: 0.165124, acc.: 92.19%] [G loss: 3.926183]\n",
      "epoch:27 step:21517 [D loss: 0.067689, acc.: 98.44%] [G loss: 4.657586]\n",
      "epoch:27 step:21518 [D loss: 0.630676, acc.: 64.84%] [G loss: 5.070892]\n",
      "epoch:27 step:21519 [D loss: 0.107009, acc.: 96.09%] [G loss: 5.049809]\n",
      "epoch:27 step:21520 [D loss: 0.055858, acc.: 98.44%] [G loss: 5.186126]\n",
      "epoch:27 step:21521 [D loss: 0.041181, acc.: 100.00%] [G loss: 3.825856]\n",
      "epoch:27 step:21522 [D loss: 0.115867, acc.: 96.09%] [G loss: 5.880607]\n",
      "epoch:27 step:21523 [D loss: 0.054810, acc.: 100.00%] [G loss: 4.435294]\n",
      "epoch:27 step:21524 [D loss: 0.315832, acc.: 88.28%] [G loss: 4.587175]\n",
      "epoch:27 step:21525 [D loss: 0.036903, acc.: 99.22%] [G loss: 4.901042]\n",
      "epoch:27 step:21526 [D loss: 0.034074, acc.: 100.00%] [G loss: 4.492230]\n",
      "epoch:27 step:21527 [D loss: 0.024141, acc.: 100.00%] [G loss: 4.727153]\n",
      "epoch:27 step:21528 [D loss: 0.388751, acc.: 80.47%] [G loss: 5.428463]\n",
      "epoch:27 step:21529 [D loss: 0.056058, acc.: 97.66%] [G loss: 6.162860]\n",
      "epoch:27 step:21530 [D loss: 0.211801, acc.: 90.62%] [G loss: 3.704130]\n",
      "epoch:27 step:21531 [D loss: 0.287492, acc.: 83.59%] [G loss: 5.738090]\n",
      "epoch:27 step:21532 [D loss: 0.051484, acc.: 96.88%] [G loss: 6.553727]\n",
      "epoch:27 step:21533 [D loss: 0.344306, acc.: 85.94%] [G loss: 3.689923]\n",
      "epoch:27 step:21534 [D loss: 0.114870, acc.: 96.88%] [G loss: 4.851480]\n",
      "epoch:27 step:21535 [D loss: 0.029350, acc.: 99.22%] [G loss: 4.425619]\n",
      "epoch:27 step:21536 [D loss: 0.031302, acc.: 99.22%] [G loss: 4.475796]\n",
      "epoch:27 step:21537 [D loss: 0.010416, acc.: 100.00%] [G loss: 4.079458]\n",
      "epoch:27 step:21538 [D loss: 0.076517, acc.: 98.44%] [G loss: 4.138171]\n",
      "epoch:27 step:21539 [D loss: 0.018621, acc.: 100.00%] [G loss: 4.199146]\n",
      "epoch:27 step:21540 [D loss: 0.051964, acc.: 98.44%] [G loss: 2.961183]\n",
      "epoch:27 step:21541 [D loss: 0.040267, acc.: 99.22%] [G loss: 2.909194]\n",
      "epoch:27 step:21542 [D loss: 0.035604, acc.: 99.22%] [G loss: 4.153902]\n",
      "epoch:27 step:21543 [D loss: 0.065722, acc.: 100.00%] [G loss: 4.693399]\n",
      "epoch:27 step:21544 [D loss: 0.067203, acc.: 96.88%] [G loss: 3.823391]\n",
      "epoch:27 step:21545 [D loss: 0.058101, acc.: 100.00%] [G loss: 4.040452]\n",
      "epoch:27 step:21546 [D loss: 0.219057, acc.: 91.41%] [G loss: 5.164656]\n",
      "epoch:27 step:21547 [D loss: 0.690499, acc.: 67.19%] [G loss: 2.391976]\n",
      "epoch:27 step:21548 [D loss: 0.101855, acc.: 96.09%] [G loss: 3.562623]\n",
      "epoch:27 step:21549 [D loss: 0.009166, acc.: 100.00%] [G loss: 5.298493]\n",
      "epoch:27 step:21550 [D loss: 0.075929, acc.: 97.66%] [G loss: 3.447769]\n",
      "epoch:27 step:21551 [D loss: 0.119346, acc.: 96.09%] [G loss: 4.833050]\n",
      "epoch:27 step:21552 [D loss: 0.003440, acc.: 100.00%] [G loss: 5.306128]\n",
      "epoch:27 step:21553 [D loss: 0.088781, acc.: 99.22%] [G loss: 3.505610]\n",
      "epoch:27 step:21554 [D loss: 0.026193, acc.: 100.00%] [G loss: 3.564778]\n",
      "epoch:27 step:21555 [D loss: 0.048243, acc.: 100.00%] [G loss: 3.487781]\n",
      "epoch:27 step:21556 [D loss: 0.060456, acc.: 100.00%] [G loss: 3.704371]\n",
      "epoch:27 step:21557 [D loss: 0.006796, acc.: 100.00%] [G loss: 5.006242]\n",
      "epoch:27 step:21558 [D loss: 0.134741, acc.: 96.09%] [G loss: 3.863138]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:27 step:21559 [D loss: 0.010751, acc.: 100.00%] [G loss: 4.501523]\n",
      "epoch:27 step:21560 [D loss: 0.028374, acc.: 100.00%] [G loss: 3.918402]\n",
      "epoch:27 step:21561 [D loss: 0.014192, acc.: 100.00%] [G loss: 4.051848]\n",
      "epoch:27 step:21562 [D loss: 0.714494, acc.: 62.50%] [G loss: 6.517684]\n",
      "epoch:27 step:21563 [D loss: 0.137084, acc.: 94.53%] [G loss: 6.895469]\n",
      "epoch:27 step:21564 [D loss: 0.023133, acc.: 100.00%] [G loss: 5.742490]\n",
      "epoch:27 step:21565 [D loss: 0.027740, acc.: 99.22%] [G loss: 5.789029]\n",
      "epoch:27 step:21566 [D loss: 0.014741, acc.: 100.00%] [G loss: 5.704266]\n",
      "epoch:27 step:21567 [D loss: 0.007767, acc.: 100.00%] [G loss: 4.375985]\n",
      "epoch:27 step:21568 [D loss: 0.032182, acc.: 100.00%] [G loss: 4.592814]\n",
      "epoch:27 step:21569 [D loss: 0.050537, acc.: 98.44%] [G loss: 3.565235]\n",
      "epoch:27 step:21570 [D loss: 0.016816, acc.: 100.00%] [G loss: 3.245143]\n",
      "epoch:27 step:21571 [D loss: 0.068914, acc.: 98.44%] [G loss: 4.667283]\n",
      "epoch:27 step:21572 [D loss: 0.069639, acc.: 99.22%] [G loss: 4.063192]\n",
      "epoch:27 step:21573 [D loss: 0.009897, acc.: 100.00%] [G loss: 4.590705]\n",
      "epoch:27 step:21574 [D loss: 0.030050, acc.: 99.22%] [G loss: 2.870214]\n",
      "epoch:27 step:21575 [D loss: 0.043106, acc.: 100.00%] [G loss: 3.952719]\n",
      "epoch:27 step:21576 [D loss: 0.026532, acc.: 100.00%] [G loss: 5.390280]\n",
      "epoch:27 step:21577 [D loss: 0.015910, acc.: 100.00%] [G loss: 5.123526]\n",
      "epoch:27 step:21578 [D loss: 0.116883, acc.: 96.09%] [G loss: 2.672111]\n",
      "epoch:27 step:21579 [D loss: 0.592131, acc.: 71.88%] [G loss: 7.173958]\n",
      "epoch:27 step:21580 [D loss: 0.365961, acc.: 78.91%] [G loss: 5.744818]\n",
      "epoch:27 step:21581 [D loss: 0.077603, acc.: 96.88%] [G loss: 4.625129]\n",
      "epoch:27 step:21582 [D loss: 0.036133, acc.: 99.22%] [G loss: 4.733165]\n",
      "epoch:27 step:21583 [D loss: 0.014340, acc.: 100.00%] [G loss: 5.421875]\n",
      "epoch:27 step:21584 [D loss: 0.021592, acc.: 100.00%] [G loss: 5.143806]\n",
      "epoch:27 step:21585 [D loss: 0.053363, acc.: 99.22%] [G loss: 4.840521]\n",
      "epoch:27 step:21586 [D loss: 0.006091, acc.: 100.00%] [G loss: 5.001873]\n",
      "epoch:27 step:21587 [D loss: 0.044123, acc.: 98.44%] [G loss: 4.776909]\n",
      "epoch:27 step:21588 [D loss: 0.015850, acc.: 100.00%] [G loss: 4.525905]\n",
      "epoch:27 step:21589 [D loss: 0.008186, acc.: 100.00%] [G loss: 4.367188]\n",
      "epoch:27 step:21590 [D loss: 0.024421, acc.: 99.22%] [G loss: 4.676123]\n",
      "epoch:27 step:21591 [D loss: 0.012279, acc.: 100.00%] [G loss: 5.286229]\n",
      "epoch:27 step:21592 [D loss: 0.020201, acc.: 100.00%] [G loss: 5.512051]\n",
      "epoch:27 step:21593 [D loss: 0.024021, acc.: 98.44%] [G loss: 5.168221]\n",
      "epoch:27 step:21594 [D loss: 0.046593, acc.: 99.22%] [G loss: 3.457018]\n",
      "epoch:27 step:21595 [D loss: 0.023274, acc.: 100.00%] [G loss: 4.327925]\n",
      "epoch:27 step:21596 [D loss: 0.036691, acc.: 99.22%] [G loss: 2.479080]\n",
      "epoch:27 step:21597 [D loss: 0.026209, acc.: 100.00%] [G loss: 4.120809]\n",
      "epoch:27 step:21598 [D loss: 0.108587, acc.: 98.44%] [G loss: 3.262747]\n",
      "epoch:27 step:21599 [D loss: 0.012253, acc.: 100.00%] [G loss: 3.030544]\n",
      "epoch:27 step:21600 [D loss: 0.009414, acc.: 100.00%] [G loss: 1.572592]\n",
      "##############\n",
      "[0.96752363 0.89887706 0.90308525 0.98916517 2.10531881 1.00912445\n",
      " 1.08318824 2.10738547 0.9057974  0.99719163]\n",
      "##########\n",
      "epoch:27 step:21601 [D loss: 0.016538, acc.: 100.00%] [G loss: 1.433774]\n",
      "epoch:27 step:21602 [D loss: 0.024509, acc.: 100.00%] [G loss: 1.152134]\n",
      "epoch:27 step:21603 [D loss: 0.074559, acc.: 100.00%] [G loss: 2.305094]\n",
      "epoch:27 step:21604 [D loss: 0.009193, acc.: 100.00%] [G loss: 3.620851]\n",
      "epoch:27 step:21605 [D loss: 0.048129, acc.: 98.44%] [G loss: 1.266898]\n",
      "epoch:27 step:21606 [D loss: 0.069027, acc.: 98.44%] [G loss: 4.369197]\n",
      "epoch:27 step:21607 [D loss: 0.019089, acc.: 100.00%] [G loss: 4.952362]\n",
      "epoch:27 step:21608 [D loss: 0.036787, acc.: 98.44%] [G loss: 4.004071]\n",
      "epoch:27 step:21609 [D loss: 0.190967, acc.: 92.97%] [G loss: 5.403976]\n",
      "epoch:27 step:21610 [D loss: 0.051598, acc.: 98.44%] [G loss: 5.354135]\n",
      "epoch:27 step:21611 [D loss: 0.012776, acc.: 100.00%] [G loss: 4.391019]\n",
      "epoch:27 step:21612 [D loss: 0.019807, acc.: 100.00%] [G loss: 5.553262]\n",
      "epoch:27 step:21613 [D loss: 0.089170, acc.: 96.88%] [G loss: 2.300181]\n",
      "epoch:27 step:21614 [D loss: 0.004979, acc.: 100.00%] [G loss: 3.211512]\n",
      "epoch:27 step:21615 [D loss: 0.008348, acc.: 100.00%] [G loss: 1.334374]\n",
      "epoch:27 step:21616 [D loss: 0.012403, acc.: 100.00%] [G loss: 2.841307]\n",
      "epoch:27 step:21617 [D loss: 0.064044, acc.: 99.22%] [G loss: 6.588518]\n",
      "epoch:27 step:21618 [D loss: 0.019809, acc.: 100.00%] [G loss: 6.854072]\n",
      "epoch:27 step:21619 [D loss: 2.856854, acc.: 25.00%] [G loss: 10.528782]\n",
      "epoch:27 step:21620 [D loss: 3.230799, acc.: 50.00%] [G loss: 7.850058]\n",
      "epoch:27 step:21621 [D loss: 0.675306, acc.: 69.53%] [G loss: 5.731400]\n",
      "epoch:27 step:21622 [D loss: 0.163407, acc.: 93.75%] [G loss: 4.178029]\n",
      "epoch:27 step:21623 [D loss: 0.030165, acc.: 100.00%] [G loss: 4.608117]\n",
      "epoch:27 step:21624 [D loss: 0.027708, acc.: 100.00%] [G loss: 3.905049]\n",
      "epoch:27 step:21625 [D loss: 0.071593, acc.: 99.22%] [G loss: 4.045456]\n",
      "epoch:27 step:21626 [D loss: 0.027553, acc.: 100.00%] [G loss: 4.120873]\n",
      "epoch:27 step:21627 [D loss: 0.068522, acc.: 98.44%] [G loss: 4.778680]\n",
      "epoch:27 step:21628 [D loss: 0.018483, acc.: 100.00%] [G loss: 3.954294]\n",
      "epoch:27 step:21629 [D loss: 0.081080, acc.: 99.22%] [G loss: 3.838339]\n",
      "epoch:27 step:21630 [D loss: 0.022395, acc.: 100.00%] [G loss: 3.784848]\n",
      "epoch:27 step:21631 [D loss: 0.015523, acc.: 100.00%] [G loss: 4.086449]\n",
      "epoch:27 step:21632 [D loss: 0.041895, acc.: 99.22%] [G loss: 2.933954]\n",
      "epoch:27 step:21633 [D loss: 0.069775, acc.: 100.00%] [G loss: 3.750705]\n",
      "epoch:27 step:21634 [D loss: 0.858079, acc.: 50.78%] [G loss: 7.200365]\n",
      "epoch:27 step:21635 [D loss: 1.356542, acc.: 54.69%] [G loss: 5.253089]\n",
      "epoch:27 step:21636 [D loss: 0.041302, acc.: 98.44%] [G loss: 4.332272]\n",
      "epoch:27 step:21637 [D loss: 0.180738, acc.: 95.31%] [G loss: 3.465178]\n",
      "epoch:27 step:21638 [D loss: 0.015474, acc.: 100.00%] [G loss: 4.098852]\n",
      "epoch:27 step:21639 [D loss: 0.028439, acc.: 100.00%] [G loss: 4.243061]\n",
      "epoch:27 step:21640 [D loss: 0.031927, acc.: 100.00%] [G loss: 3.759334]\n",
      "epoch:27 step:21641 [D loss: 0.050337, acc.: 99.22%] [G loss: 4.003194]\n",
      "epoch:27 step:21642 [D loss: 0.012349, acc.: 100.00%] [G loss: 4.256487]\n",
      "epoch:27 step:21643 [D loss: 0.020687, acc.: 100.00%] [G loss: 3.959447]\n",
      "epoch:27 step:21644 [D loss: 0.021363, acc.: 100.00%] [G loss: 4.334267]\n",
      "epoch:27 step:21645 [D loss: 0.124492, acc.: 98.44%] [G loss: 3.806381]\n",
      "epoch:27 step:21646 [D loss: 0.083947, acc.: 96.88%] [G loss: 4.489041]\n",
      "epoch:27 step:21647 [D loss: 0.016844, acc.: 100.00%] [G loss: 4.099288]\n",
      "epoch:27 step:21648 [D loss: 0.016288, acc.: 100.00%] [G loss: 5.070746]\n",
      "epoch:27 step:21649 [D loss: 0.083866, acc.: 99.22%] [G loss: 2.759087]\n",
      "epoch:27 step:21650 [D loss: 0.148050, acc.: 96.88%] [G loss: 4.937779]\n",
      "epoch:27 step:21651 [D loss: 0.008926, acc.: 100.00%] [G loss: 5.843441]\n",
      "epoch:27 step:21652 [D loss: 0.598897, acc.: 68.75%] [G loss: 0.213953]\n",
      "epoch:27 step:21653 [D loss: 1.389204, acc.: 53.91%] [G loss: 6.981330]\n",
      "epoch:27 step:21654 [D loss: 1.259368, acc.: 52.34%] [G loss: 6.228453]\n",
      "epoch:27 step:21655 [D loss: 0.560097, acc.: 73.44%] [G loss: 4.166782]\n",
      "epoch:27 step:21656 [D loss: 0.117279, acc.: 96.88%] [G loss: 2.553950]\n",
      "epoch:27 step:21657 [D loss: 0.047455, acc.: 100.00%] [G loss: 2.530431]\n",
      "epoch:27 step:21658 [D loss: 0.027433, acc.: 100.00%] [G loss: 2.289180]\n",
      "epoch:27 step:21659 [D loss: 0.068350, acc.: 100.00%] [G loss: 2.206212]\n",
      "epoch:27 step:21660 [D loss: 0.048698, acc.: 100.00%] [G loss: 3.651358]\n",
      "epoch:27 step:21661 [D loss: 0.053238, acc.: 99.22%] [G loss: 3.096137]\n",
      "epoch:27 step:21662 [D loss: 0.085590, acc.: 98.44%] [G loss: 3.875465]\n",
      "epoch:27 step:21663 [D loss: 0.011638, acc.: 100.00%] [G loss: 3.466609]\n",
      "epoch:27 step:21664 [D loss: 0.031464, acc.: 100.00%] [G loss: 1.875227]\n",
      "epoch:27 step:21665 [D loss: 0.018405, acc.: 100.00%] [G loss: 1.169973]\n",
      "epoch:27 step:21666 [D loss: 0.020062, acc.: 100.00%] [G loss: 0.374967]\n",
      "epoch:27 step:21667 [D loss: 0.156464, acc.: 96.09%] [G loss: 4.339113]\n",
      "epoch:27 step:21668 [D loss: 0.079984, acc.: 96.88%] [G loss: 3.229605]\n",
      "epoch:27 step:21669 [D loss: 0.044371, acc.: 100.00%] [G loss: 3.250954]\n",
      "epoch:27 step:21670 [D loss: 0.027603, acc.: 100.00%] [G loss: 2.694393]\n",
      "epoch:27 step:21671 [D loss: 0.028076, acc.: 100.00%] [G loss: 2.595949]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:27 step:21672 [D loss: 0.027922, acc.: 100.00%] [G loss: 1.903593]\n",
      "epoch:27 step:21673 [D loss: 0.100185, acc.: 98.44%] [G loss: 3.705360]\n",
      "epoch:27 step:21674 [D loss: 0.171559, acc.: 91.41%] [G loss: 1.255748]\n",
      "epoch:27 step:21675 [D loss: 0.287118, acc.: 86.72%] [G loss: 5.534268]\n",
      "epoch:27 step:21676 [D loss: 0.614526, acc.: 69.53%] [G loss: 3.416548]\n",
      "epoch:27 step:21677 [D loss: 0.044322, acc.: 98.44%] [G loss: 3.445126]\n",
      "epoch:27 step:21678 [D loss: 0.067425, acc.: 96.88%] [G loss: 3.882991]\n",
      "epoch:27 step:21679 [D loss: 0.010397, acc.: 100.00%] [G loss: 4.992755]\n",
      "epoch:27 step:21680 [D loss: 0.014803, acc.: 100.00%] [G loss: 4.755417]\n",
      "epoch:27 step:21681 [D loss: 0.031744, acc.: 99.22%] [G loss: 4.462762]\n",
      "epoch:27 step:21682 [D loss: 0.019276, acc.: 100.00%] [G loss: 4.132328]\n",
      "epoch:27 step:21683 [D loss: 0.010336, acc.: 100.00%] [G loss: 4.442605]\n",
      "epoch:27 step:21684 [D loss: 0.017440, acc.: 100.00%] [G loss: 3.903586]\n",
      "epoch:27 step:21685 [D loss: 0.019552, acc.: 100.00%] [G loss: 4.702903]\n",
      "epoch:27 step:21686 [D loss: 0.028918, acc.: 100.00%] [G loss: 4.248424]\n",
      "epoch:27 step:21687 [D loss: 0.052584, acc.: 100.00%] [G loss: 4.455153]\n",
      "epoch:27 step:21688 [D loss: 0.026743, acc.: 100.00%] [G loss: 4.050758]\n",
      "epoch:27 step:21689 [D loss: 0.035806, acc.: 99.22%] [G loss: 4.369813]\n",
      "epoch:27 step:21690 [D loss: 0.025813, acc.: 100.00%] [G loss: 3.925719]\n",
      "epoch:27 step:21691 [D loss: 0.310838, acc.: 86.72%] [G loss: 4.164177]\n",
      "epoch:27 step:21692 [D loss: 0.007479, acc.: 100.00%] [G loss: 4.956900]\n",
      "epoch:27 step:21693 [D loss: 0.069143, acc.: 99.22%] [G loss: 4.767062]\n",
      "epoch:27 step:21694 [D loss: 0.019603, acc.: 100.00%] [G loss: 4.309314]\n",
      "epoch:27 step:21695 [D loss: 0.015782, acc.: 100.00%] [G loss: 4.258825]\n",
      "epoch:27 step:21696 [D loss: 0.027604, acc.: 100.00%] [G loss: 4.287125]\n",
      "epoch:27 step:21697 [D loss: 0.020331, acc.: 100.00%] [G loss: 3.662849]\n",
      "epoch:27 step:21698 [D loss: 0.023254, acc.: 100.00%] [G loss: 3.684865]\n",
      "epoch:27 step:21699 [D loss: 0.026069, acc.: 100.00%] [G loss: 4.746731]\n",
      "epoch:27 step:21700 [D loss: 0.023865, acc.: 100.00%] [G loss: 3.819827]\n",
      "epoch:27 step:21701 [D loss: 0.010291, acc.: 100.00%] [G loss: 3.637130]\n",
      "epoch:27 step:21702 [D loss: 0.011372, acc.: 100.00%] [G loss: 3.847428]\n",
      "epoch:27 step:21703 [D loss: 0.049572, acc.: 100.00%] [G loss: 3.777374]\n",
      "epoch:27 step:21704 [D loss: 0.025152, acc.: 100.00%] [G loss: 4.039692]\n",
      "epoch:27 step:21705 [D loss: 0.101352, acc.: 98.44%] [G loss: 4.300931]\n",
      "epoch:27 step:21706 [D loss: 0.052635, acc.: 98.44%] [G loss: 5.072637]\n",
      "epoch:27 step:21707 [D loss: 0.018043, acc.: 100.00%] [G loss: 3.900299]\n",
      "epoch:27 step:21708 [D loss: 0.024557, acc.: 99.22%] [G loss: 4.281483]\n",
      "epoch:27 step:21709 [D loss: 0.025509, acc.: 100.00%] [G loss: 3.831956]\n",
      "epoch:27 step:21710 [D loss: 0.029179, acc.: 100.00%] [G loss: 3.774128]\n",
      "epoch:27 step:21711 [D loss: 0.014816, acc.: 100.00%] [G loss: 3.853271]\n",
      "epoch:27 step:21712 [D loss: 0.026190, acc.: 100.00%] [G loss: 4.229073]\n",
      "epoch:27 step:21713 [D loss: 0.028722, acc.: 100.00%] [G loss: 4.783846]\n",
      "epoch:27 step:21714 [D loss: 0.136186, acc.: 97.66%] [G loss: 3.371535]\n",
      "epoch:27 step:21715 [D loss: 0.046584, acc.: 99.22%] [G loss: 3.655010]\n",
      "epoch:27 step:21716 [D loss: 0.016623, acc.: 100.00%] [G loss: 3.907468]\n",
      "epoch:27 step:21717 [D loss: 0.015187, acc.: 100.00%] [G loss: 3.490004]\n",
      "epoch:27 step:21718 [D loss: 0.028384, acc.: 100.00%] [G loss: 2.047378]\n",
      "epoch:27 step:21719 [D loss: 0.026839, acc.: 100.00%] [G loss: 2.889443]\n",
      "epoch:27 step:21720 [D loss: 0.010555, acc.: 100.00%] [G loss: 2.735296]\n",
      "epoch:27 step:21721 [D loss: 0.040225, acc.: 98.44%] [G loss: 2.322428]\n",
      "epoch:27 step:21722 [D loss: 0.043215, acc.: 99.22%] [G loss: 3.030908]\n",
      "epoch:27 step:21723 [D loss: 1.138736, acc.: 50.78%] [G loss: 8.816445]\n",
      "epoch:27 step:21724 [D loss: 2.497361, acc.: 50.78%] [G loss: 5.286404]\n",
      "epoch:27 step:21725 [D loss: 0.048782, acc.: 98.44%] [G loss: 4.304715]\n",
      "epoch:27 step:21726 [D loss: 0.043361, acc.: 100.00%] [G loss: 3.304152]\n",
      "epoch:27 step:21727 [D loss: 0.021930, acc.: 100.00%] [G loss: 2.783450]\n",
      "epoch:27 step:21728 [D loss: 0.017484, acc.: 100.00%] [G loss: 1.882903]\n",
      "epoch:27 step:21729 [D loss: 0.325122, acc.: 88.28%] [G loss: 3.752279]\n",
      "epoch:27 step:21730 [D loss: 0.009411, acc.: 100.00%] [G loss: 5.178198]\n",
      "epoch:27 step:21731 [D loss: 0.135783, acc.: 95.31%] [G loss: 3.202565]\n",
      "epoch:27 step:21732 [D loss: 0.047118, acc.: 100.00%] [G loss: 4.022197]\n",
      "epoch:27 step:21733 [D loss: 0.374440, acc.: 77.34%] [G loss: 2.691859]\n",
      "epoch:27 step:21734 [D loss: 0.071448, acc.: 99.22%] [G loss: 4.527794]\n",
      "epoch:27 step:21735 [D loss: 0.033606, acc.: 100.00%] [G loss: 3.742496]\n",
      "epoch:27 step:21736 [D loss: 0.057474, acc.: 99.22%] [G loss: 2.928899]\n",
      "epoch:27 step:21737 [D loss: 0.051539, acc.: 99.22%] [G loss: 3.802842]\n",
      "epoch:27 step:21738 [D loss: 0.009424, acc.: 100.00%] [G loss: 4.154390]\n",
      "epoch:27 step:21739 [D loss: 0.029610, acc.: 100.00%] [G loss: 4.002899]\n",
      "epoch:27 step:21740 [D loss: 0.041757, acc.: 100.00%] [G loss: 3.557145]\n",
      "epoch:27 step:21741 [D loss: 0.151692, acc.: 96.88%] [G loss: 3.810127]\n",
      "epoch:27 step:21742 [D loss: 0.013572, acc.: 100.00%] [G loss: 5.010839]\n",
      "epoch:27 step:21743 [D loss: 0.013004, acc.: 100.00%] [G loss: 4.559639]\n",
      "epoch:27 step:21744 [D loss: 0.044767, acc.: 100.00%] [G loss: 4.319207]\n",
      "epoch:27 step:21745 [D loss: 0.169644, acc.: 96.09%] [G loss: 3.113710]\n",
      "epoch:27 step:21746 [D loss: 0.006987, acc.: 100.00%] [G loss: 3.986947]\n",
      "epoch:27 step:21747 [D loss: 0.029501, acc.: 100.00%] [G loss: 3.402823]\n",
      "epoch:27 step:21748 [D loss: 0.130325, acc.: 96.88%] [G loss: 0.826672]\n",
      "epoch:27 step:21749 [D loss: 0.209398, acc.: 91.41%] [G loss: 5.647182]\n",
      "epoch:27 step:21750 [D loss: 0.151720, acc.: 92.97%] [G loss: 3.641382]\n",
      "epoch:27 step:21751 [D loss: 0.065416, acc.: 100.00%] [G loss: 5.115720]\n",
      "epoch:27 step:21752 [D loss: 0.070318, acc.: 99.22%] [G loss: 3.406985]\n",
      "epoch:27 step:21753 [D loss: 0.022170, acc.: 100.00%] [G loss: 3.872960]\n",
      "epoch:27 step:21754 [D loss: 0.015520, acc.: 100.00%] [G loss: 4.476763]\n",
      "epoch:27 step:21755 [D loss: 0.020235, acc.: 100.00%] [G loss: 4.172275]\n",
      "epoch:27 step:21756 [D loss: 0.015755, acc.: 100.00%] [G loss: 3.578015]\n",
      "epoch:27 step:21757 [D loss: 0.041598, acc.: 100.00%] [G loss: 4.845981]\n",
      "epoch:27 step:21758 [D loss: 0.026114, acc.: 99.22%] [G loss: 4.615101]\n",
      "epoch:27 step:21759 [D loss: 0.042231, acc.: 99.22%] [G loss: 4.620817]\n",
      "epoch:27 step:21760 [D loss: 0.239332, acc.: 89.84%] [G loss: 6.328867]\n",
      "epoch:27 step:21761 [D loss: 0.030494, acc.: 99.22%] [G loss: 6.933751]\n",
      "epoch:27 step:21762 [D loss: 0.030848, acc.: 100.00%] [G loss: 6.265810]\n",
      "epoch:27 step:21763 [D loss: 0.051117, acc.: 99.22%] [G loss: 4.711008]\n",
      "epoch:27 step:21764 [D loss: 0.029087, acc.: 99.22%] [G loss: 4.393813]\n",
      "epoch:27 step:21765 [D loss: 0.007275, acc.: 100.00%] [G loss: 5.552279]\n",
      "epoch:27 step:21766 [D loss: 0.004617, acc.: 100.00%] [G loss: 4.903898]\n",
      "epoch:27 step:21767 [D loss: 0.008229, acc.: 100.00%] [G loss: 4.806169]\n",
      "epoch:27 step:21768 [D loss: 0.015122, acc.: 99.22%] [G loss: 4.825754]\n",
      "epoch:27 step:21769 [D loss: 0.011434, acc.: 100.00%] [G loss: 3.583944]\n",
      "epoch:27 step:21770 [D loss: 0.026719, acc.: 100.00%] [G loss: 3.530670]\n",
      "epoch:27 step:21771 [D loss: 0.033197, acc.: 100.00%] [G loss: 4.239537]\n",
      "epoch:27 step:21772 [D loss: 2.561103, acc.: 28.91%] [G loss: 9.059616]\n",
      "epoch:27 step:21773 [D loss: 3.035372, acc.: 50.00%] [G loss: 6.980284]\n",
      "epoch:27 step:21774 [D loss: 2.535241, acc.: 50.00%] [G loss: 4.557270]\n",
      "epoch:27 step:21775 [D loss: 1.497842, acc.: 51.56%] [G loss: 2.824877]\n",
      "epoch:27 step:21776 [D loss: 0.805173, acc.: 58.59%] [G loss: 1.747249]\n",
      "epoch:27 step:21777 [D loss: 0.393393, acc.: 84.38%] [G loss: 2.080175]\n",
      "epoch:27 step:21778 [D loss: 0.194128, acc.: 96.88%] [G loss: 2.296705]\n",
      "epoch:27 step:21779 [D loss: 0.175007, acc.: 96.88%] [G loss: 2.496289]\n",
      "epoch:27 step:21780 [D loss: 0.161692, acc.: 96.88%] [G loss: 2.143778]\n",
      "epoch:27 step:21781 [D loss: 0.096732, acc.: 100.00%] [G loss: 2.566184]\n",
      "epoch:27 step:21782 [D loss: 0.105045, acc.: 99.22%] [G loss: 2.601092]\n",
      "epoch:27 step:21783 [D loss: 0.128969, acc.: 100.00%] [G loss: 2.405512]\n",
      "epoch:27 step:21784 [D loss: 0.132876, acc.: 97.66%] [G loss: 2.456890]\n",
      "epoch:27 step:21785 [D loss: 0.145924, acc.: 96.88%] [G loss: 2.395761]\n",
      "epoch:27 step:21786 [D loss: 0.112471, acc.: 99.22%] [G loss: 2.852582]\n",
      "epoch:27 step:21787 [D loss: 0.114090, acc.: 100.00%] [G loss: 3.059635]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:27 step:21788 [D loss: 0.155502, acc.: 98.44%] [G loss: 1.939151]\n",
      "epoch:27 step:21789 [D loss: 0.071485, acc.: 100.00%] [G loss: 2.693813]\n",
      "epoch:27 step:21790 [D loss: 0.649541, acc.: 63.28%] [G loss: 3.933654]\n",
      "epoch:27 step:21791 [D loss: 0.179493, acc.: 93.75%] [G loss: 3.498109]\n",
      "epoch:27 step:21792 [D loss: 0.223595, acc.: 90.62%] [G loss: 2.368491]\n",
      "epoch:27 step:21793 [D loss: 0.160408, acc.: 96.88%] [G loss: 3.135123]\n",
      "epoch:27 step:21794 [D loss: 0.098068, acc.: 97.66%] [G loss: 3.195205]\n",
      "epoch:27 step:21795 [D loss: 0.078435, acc.: 98.44%] [G loss: 3.124637]\n",
      "epoch:27 step:21796 [D loss: 0.068011, acc.: 100.00%] [G loss: 2.993611]\n",
      "epoch:27 step:21797 [D loss: 0.092779, acc.: 99.22%] [G loss: 3.293938]\n",
      "epoch:27 step:21798 [D loss: 0.109795, acc.: 97.66%] [G loss: 2.792566]\n",
      "epoch:27 step:21799 [D loss: 0.096186, acc.: 98.44%] [G loss: 2.786515]\n",
      "epoch:27 step:21800 [D loss: 0.113310, acc.: 97.66%] [G loss: 3.217582]\n",
      "##############\n",
      "[1.026025   1.04003198 0.95255144 1.09795611 1.10335165 0.96112735\n",
      " 2.10642805 2.11929191 1.12548557 1.12020998]\n",
      "##########\n",
      "epoch:27 step:21801 [D loss: 0.157091, acc.: 97.66%] [G loss: 3.366501]\n",
      "epoch:27 step:21802 [D loss: 0.198588, acc.: 93.75%] [G loss: 2.423254]\n",
      "epoch:27 step:21803 [D loss: 0.157713, acc.: 93.75%] [G loss: 3.219323]\n",
      "epoch:27 step:21804 [D loss: 0.027276, acc.: 100.00%] [G loss: 4.106261]\n",
      "epoch:27 step:21805 [D loss: 0.182202, acc.: 91.41%] [G loss: 3.058409]\n",
      "epoch:27 step:21806 [D loss: 0.088000, acc.: 99.22%] [G loss: 3.182833]\n",
      "epoch:27 step:21807 [D loss: 0.196933, acc.: 96.09%] [G loss: 2.404035]\n",
      "epoch:27 step:21808 [D loss: 0.152131, acc.: 97.66%] [G loss: 2.681378]\n",
      "epoch:27 step:21809 [D loss: 0.112113, acc.: 97.66%] [G loss: 2.735706]\n",
      "epoch:27 step:21810 [D loss: 0.242221, acc.: 93.75%] [G loss: 2.162617]\n",
      "epoch:27 step:21811 [D loss: 0.062805, acc.: 100.00%] [G loss: 3.903883]\n",
      "epoch:27 step:21812 [D loss: 0.122056, acc.: 96.09%] [G loss: 2.440743]\n",
      "epoch:27 step:21813 [D loss: 0.087461, acc.: 99.22%] [G loss: 2.778960]\n",
      "epoch:27 step:21814 [D loss: 0.136872, acc.: 98.44%] [G loss: 1.747880]\n",
      "epoch:27 step:21815 [D loss: 0.043651, acc.: 100.00%] [G loss: 2.890836]\n",
      "epoch:27 step:21816 [D loss: 0.128265, acc.: 96.09%] [G loss: 2.916329]\n",
      "epoch:27 step:21817 [D loss: 0.049293, acc.: 100.00%] [G loss: 2.434587]\n",
      "epoch:27 step:21818 [D loss: 0.203672, acc.: 93.75%] [G loss: 3.094539]\n",
      "epoch:27 step:21819 [D loss: 0.115229, acc.: 96.88%] [G loss: 2.167962]\n",
      "epoch:27 step:21820 [D loss: 0.306628, acc.: 89.84%] [G loss: 2.816263]\n",
      "epoch:27 step:21821 [D loss: 0.177203, acc.: 95.31%] [G loss: 2.911198]\n",
      "epoch:27 step:21822 [D loss: 0.076817, acc.: 99.22%] [G loss: 2.146080]\n",
      "epoch:27 step:21823 [D loss: 0.089907, acc.: 98.44%] [G loss: 2.887389]\n",
      "epoch:27 step:21824 [D loss: 0.062432, acc.: 99.22%] [G loss: 4.090102]\n",
      "epoch:27 step:21825 [D loss: 0.070040, acc.: 99.22%] [G loss: 2.416698]\n",
      "epoch:27 step:21826 [D loss: 0.029166, acc.: 100.00%] [G loss: 2.193617]\n",
      "epoch:27 step:21827 [D loss: 0.214765, acc.: 91.41%] [G loss: 3.249901]\n",
      "epoch:27 step:21828 [D loss: 0.041691, acc.: 100.00%] [G loss: 4.054031]\n",
      "epoch:27 step:21829 [D loss: 0.171760, acc.: 92.19%] [G loss: 4.069533]\n",
      "epoch:27 step:21830 [D loss: 0.526484, acc.: 72.66%] [G loss: 5.984296]\n",
      "epoch:27 step:21831 [D loss: 0.334225, acc.: 82.81%] [G loss: 5.438861]\n",
      "epoch:27 step:21832 [D loss: 0.072821, acc.: 97.66%] [G loss: 4.782503]\n",
      "epoch:27 step:21833 [D loss: 0.063306, acc.: 100.00%] [G loss: 3.703770]\n",
      "epoch:27 step:21834 [D loss: 0.093058, acc.: 97.66%] [G loss: 5.242709]\n",
      "epoch:27 step:21835 [D loss: 0.030927, acc.: 100.00%] [G loss: 4.006349]\n",
      "epoch:27 step:21836 [D loss: 0.052533, acc.: 99.22%] [G loss: 4.190073]\n",
      "epoch:27 step:21837 [D loss: 0.624234, acc.: 73.44%] [G loss: 5.357226]\n",
      "epoch:27 step:21838 [D loss: 0.071109, acc.: 95.31%] [G loss: 6.329052]\n",
      "epoch:27 step:21839 [D loss: 0.071981, acc.: 99.22%] [G loss: 5.519053]\n",
      "epoch:27 step:21840 [D loss: 0.033890, acc.: 99.22%] [G loss: 3.983316]\n",
      "epoch:27 step:21841 [D loss: 0.018621, acc.: 100.00%] [G loss: 3.680917]\n",
      "epoch:27 step:21842 [D loss: 0.096251, acc.: 95.31%] [G loss: 4.733801]\n",
      "epoch:27 step:21843 [D loss: 0.045083, acc.: 98.44%] [G loss: 5.066243]\n",
      "epoch:27 step:21844 [D loss: 0.025861, acc.: 100.00%] [G loss: 4.772857]\n",
      "epoch:27 step:21845 [D loss: 0.020773, acc.: 99.22%] [G loss: 4.831306]\n",
      "epoch:27 step:21846 [D loss: 0.031827, acc.: 100.00%] [G loss: 4.078379]\n",
      "epoch:27 step:21847 [D loss: 0.027981, acc.: 100.00%] [G loss: 3.488948]\n",
      "epoch:27 step:21848 [D loss: 0.101763, acc.: 97.66%] [G loss: 4.102152]\n",
      "epoch:27 step:21849 [D loss: 0.099127, acc.: 99.22%] [G loss: 4.525382]\n",
      "epoch:27 step:21850 [D loss: 0.121408, acc.: 97.66%] [G loss: 3.005413]\n",
      "epoch:27 step:21851 [D loss: 0.084108, acc.: 97.66%] [G loss: 5.751885]\n",
      "epoch:27 step:21852 [D loss: 0.094092, acc.: 96.88%] [G loss: 4.339708]\n",
      "epoch:27 step:21853 [D loss: 0.570467, acc.: 68.75%] [G loss: 6.889157]\n",
      "epoch:27 step:21854 [D loss: 0.941914, acc.: 64.84%] [G loss: 5.044633]\n",
      "epoch:27 step:21855 [D loss: 0.048225, acc.: 100.00%] [G loss: 3.298644]\n",
      "epoch:27 step:21856 [D loss: 0.065581, acc.: 96.88%] [G loss: 3.133742]\n",
      "epoch:27 step:21857 [D loss: 0.052713, acc.: 100.00%] [G loss: 2.991022]\n",
      "epoch:27 step:21858 [D loss: 0.131244, acc.: 96.88%] [G loss: 3.660501]\n",
      "epoch:27 step:21859 [D loss: 0.011284, acc.: 100.00%] [G loss: 3.908232]\n",
      "epoch:27 step:21860 [D loss: 0.023535, acc.: 100.00%] [G loss: 3.838669]\n",
      "epoch:27 step:21861 [D loss: 0.014537, acc.: 100.00%] [G loss: 3.757614]\n",
      "epoch:27 step:21862 [D loss: 0.040483, acc.: 100.00%] [G loss: 3.606118]\n",
      "epoch:27 step:21863 [D loss: 0.016702, acc.: 100.00%] [G loss: 3.667743]\n",
      "epoch:27 step:21864 [D loss: 0.200725, acc.: 96.09%] [G loss: 2.119419]\n",
      "epoch:27 step:21865 [D loss: 0.042606, acc.: 99.22%] [G loss: 4.139138]\n",
      "epoch:27 step:21866 [D loss: 0.054771, acc.: 99.22%] [G loss: 3.319267]\n",
      "epoch:27 step:21867 [D loss: 0.060208, acc.: 99.22%] [G loss: 3.800472]\n",
      "epoch:27 step:21868 [D loss: 0.017873, acc.: 100.00%] [G loss: 3.453240]\n",
      "epoch:28 step:21869 [D loss: 0.011579, acc.: 100.00%] [G loss: 4.209964]\n",
      "epoch:28 step:21870 [D loss: 0.080039, acc.: 97.66%] [G loss: 3.677389]\n",
      "epoch:28 step:21871 [D loss: 0.021585, acc.: 100.00%] [G loss: 3.797352]\n",
      "epoch:28 step:21872 [D loss: 0.261455, acc.: 88.28%] [G loss: 3.873474]\n",
      "epoch:28 step:21873 [D loss: 0.063405, acc.: 99.22%] [G loss: 5.041124]\n",
      "epoch:28 step:21874 [D loss: 0.067608, acc.: 98.44%] [G loss: 5.316775]\n",
      "epoch:28 step:21875 [D loss: 0.032115, acc.: 100.00%] [G loss: 4.850898]\n",
      "epoch:28 step:21876 [D loss: 0.025464, acc.: 100.00%] [G loss: 4.743393]\n",
      "epoch:28 step:21877 [D loss: 0.169292, acc.: 94.53%] [G loss: 3.360988]\n",
      "epoch:28 step:21878 [D loss: 0.050079, acc.: 98.44%] [G loss: 4.852939]\n",
      "epoch:28 step:21879 [D loss: 0.025718, acc.: 99.22%] [G loss: 5.085891]\n",
      "epoch:28 step:21880 [D loss: 0.085528, acc.: 98.44%] [G loss: 4.078852]\n",
      "epoch:28 step:21881 [D loss: 0.036410, acc.: 99.22%] [G loss: 4.198970]\n",
      "epoch:28 step:21882 [D loss: 0.017370, acc.: 100.00%] [G loss: 4.271946]\n",
      "epoch:28 step:21883 [D loss: 0.020929, acc.: 100.00%] [G loss: 3.964997]\n",
      "epoch:28 step:21884 [D loss: 0.038433, acc.: 99.22%] [G loss: 3.657881]\n",
      "epoch:28 step:21885 [D loss: 0.039504, acc.: 99.22%] [G loss: 5.043202]\n",
      "epoch:28 step:21886 [D loss: 0.019605, acc.: 99.22%] [G loss: 5.549500]\n",
      "epoch:28 step:21887 [D loss: 0.064339, acc.: 98.44%] [G loss: 5.375172]\n",
      "epoch:28 step:21888 [D loss: 0.027447, acc.: 99.22%] [G loss: 4.572961]\n",
      "epoch:28 step:21889 [D loss: 0.017719, acc.: 100.00%] [G loss: 5.348782]\n",
      "epoch:28 step:21890 [D loss: 0.010628, acc.: 100.00%] [G loss: 5.383744]\n",
      "epoch:28 step:21891 [D loss: 0.082511, acc.: 97.66%] [G loss: 3.848079]\n",
      "epoch:28 step:21892 [D loss: 0.007586, acc.: 100.00%] [G loss: 4.900840]\n",
      "epoch:28 step:21893 [D loss: 0.037708, acc.: 100.00%] [G loss: 4.110743]\n",
      "epoch:28 step:21894 [D loss: 0.087087, acc.: 96.88%] [G loss: 4.546810]\n",
      "epoch:28 step:21895 [D loss: 0.036547, acc.: 99.22%] [G loss: 4.450690]\n",
      "epoch:28 step:21896 [D loss: 0.013962, acc.: 100.00%] [G loss: 6.267196]\n",
      "epoch:28 step:21897 [D loss: 0.044186, acc.: 99.22%] [G loss: 5.581628]\n",
      "epoch:28 step:21898 [D loss: 0.055088, acc.: 97.66%] [G loss: 6.759502]\n",
      "epoch:28 step:21899 [D loss: 0.004378, acc.: 100.00%] [G loss: 6.259490]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:28 step:21900 [D loss: 0.055576, acc.: 99.22%] [G loss: 6.396094]\n",
      "epoch:28 step:21901 [D loss: 0.039902, acc.: 99.22%] [G loss: 3.294445]\n",
      "epoch:28 step:21902 [D loss: 0.221883, acc.: 91.41%] [G loss: 9.174425]\n",
      "epoch:28 step:21903 [D loss: 0.282375, acc.: 85.16%] [G loss: 7.916402]\n",
      "epoch:28 step:21904 [D loss: 0.040693, acc.: 100.00%] [G loss: 6.233548]\n",
      "epoch:28 step:21905 [D loss: 0.049100, acc.: 97.66%] [G loss: 6.769211]\n",
      "epoch:28 step:21906 [D loss: 0.004334, acc.: 100.00%] [G loss: 7.242075]\n",
      "epoch:28 step:21907 [D loss: 0.002465, acc.: 100.00%] [G loss: 7.020948]\n",
      "epoch:28 step:21908 [D loss: 0.022264, acc.: 99.22%] [G loss: 7.330290]\n",
      "epoch:28 step:21909 [D loss: 0.005697, acc.: 100.00%] [G loss: 4.543794]\n",
      "epoch:28 step:21910 [D loss: 0.005586, acc.: 100.00%] [G loss: 5.556402]\n",
      "epoch:28 step:21911 [D loss: 0.005513, acc.: 100.00%] [G loss: 4.993344]\n",
      "epoch:28 step:21912 [D loss: 1.141643, acc.: 57.81%] [G loss: 9.722703]\n",
      "epoch:28 step:21913 [D loss: 2.389566, acc.: 51.56%] [G loss: 3.556271]\n",
      "epoch:28 step:21914 [D loss: 0.159625, acc.: 93.75%] [G loss: 3.584149]\n",
      "epoch:28 step:21915 [D loss: 0.068073, acc.: 97.66%] [G loss: 4.263401]\n",
      "epoch:28 step:21916 [D loss: 0.063098, acc.: 100.00%] [G loss: 4.387140]\n",
      "epoch:28 step:21917 [D loss: 1.279848, acc.: 55.47%] [G loss: 5.731299]\n",
      "epoch:28 step:21918 [D loss: 0.705449, acc.: 68.75%] [G loss: 2.664611]\n",
      "epoch:28 step:21919 [D loss: 0.040843, acc.: 99.22%] [G loss: 1.994850]\n",
      "epoch:28 step:21920 [D loss: 0.179319, acc.: 94.53%] [G loss: 3.718427]\n",
      "epoch:28 step:21921 [D loss: 0.024027, acc.: 100.00%] [G loss: 4.460692]\n",
      "epoch:28 step:21922 [D loss: 0.095469, acc.: 97.66%] [G loss: 3.284829]\n",
      "epoch:28 step:21923 [D loss: 0.031574, acc.: 100.00%] [G loss: 2.414911]\n",
      "epoch:28 step:21924 [D loss: 0.044023, acc.: 99.22%] [G loss: 1.217629]\n",
      "epoch:28 step:21925 [D loss: 0.103124, acc.: 98.44%] [G loss: 2.882181]\n",
      "epoch:28 step:21926 [D loss: 0.312218, acc.: 87.50%] [G loss: 3.120652]\n",
      "epoch:28 step:21927 [D loss: 0.044858, acc.: 100.00%] [G loss: 4.702065]\n",
      "epoch:28 step:21928 [D loss: 0.080191, acc.: 98.44%] [G loss: 2.507795]\n",
      "epoch:28 step:21929 [D loss: 0.198974, acc.: 89.06%] [G loss: 4.713710]\n",
      "epoch:28 step:21930 [D loss: 0.132874, acc.: 95.31%] [G loss: 3.840216]\n",
      "epoch:28 step:21931 [D loss: 0.654804, acc.: 62.50%] [G loss: 4.083030]\n",
      "epoch:28 step:21932 [D loss: 0.117541, acc.: 96.88%] [G loss: 5.232970]\n",
      "epoch:28 step:21933 [D loss: 0.160041, acc.: 92.19%] [G loss: 3.454109]\n",
      "epoch:28 step:21934 [D loss: 0.160021, acc.: 94.53%] [G loss: 4.942465]\n",
      "epoch:28 step:21935 [D loss: 0.005303, acc.: 100.00%] [G loss: 5.702404]\n",
      "epoch:28 step:21936 [D loss: 0.032643, acc.: 100.00%] [G loss: 5.356010]\n",
      "epoch:28 step:21937 [D loss: 0.131643, acc.: 94.53%] [G loss: 1.954186]\n",
      "epoch:28 step:21938 [D loss: 0.169602, acc.: 92.97%] [G loss: 5.193819]\n",
      "epoch:28 step:21939 [D loss: 0.017405, acc.: 100.00%] [G loss: 5.680529]\n",
      "epoch:28 step:21940 [D loss: 0.136228, acc.: 96.88%] [G loss: 4.396719]\n",
      "epoch:28 step:21941 [D loss: 0.006878, acc.: 100.00%] [G loss: 3.403716]\n",
      "epoch:28 step:21942 [D loss: 0.014419, acc.: 100.00%] [G loss: 3.837670]\n",
      "epoch:28 step:21943 [D loss: 0.018611, acc.: 100.00%] [G loss: 2.467384]\n",
      "epoch:28 step:21944 [D loss: 0.027114, acc.: 100.00%] [G loss: 3.553568]\n",
      "epoch:28 step:21945 [D loss: 0.127170, acc.: 96.88%] [G loss: 3.044155]\n",
      "epoch:28 step:21946 [D loss: 0.023006, acc.: 100.00%] [G loss: 2.885399]\n",
      "epoch:28 step:21947 [D loss: 0.012375, acc.: 100.00%] [G loss: 3.430759]\n",
      "epoch:28 step:21948 [D loss: 0.014316, acc.: 100.00%] [G loss: 3.352444]\n",
      "epoch:28 step:21949 [D loss: 0.030799, acc.: 100.00%] [G loss: 3.391238]\n",
      "epoch:28 step:21950 [D loss: 0.004490, acc.: 100.00%] [G loss: 2.663071]\n",
      "epoch:28 step:21951 [D loss: 0.024364, acc.: 100.00%] [G loss: 3.012638]\n",
      "epoch:28 step:21952 [D loss: 0.020492, acc.: 100.00%] [G loss: 2.786337]\n",
      "epoch:28 step:21953 [D loss: 0.083517, acc.: 99.22%] [G loss: 3.183218]\n",
      "epoch:28 step:21954 [D loss: 0.016042, acc.: 100.00%] [G loss: 3.116166]\n",
      "epoch:28 step:21955 [D loss: 0.022621, acc.: 100.00%] [G loss: 4.214358]\n",
      "epoch:28 step:21956 [D loss: 0.136708, acc.: 97.66%] [G loss: 2.704437]\n",
      "epoch:28 step:21957 [D loss: 0.032447, acc.: 100.00%] [G loss: 4.304608]\n",
      "epoch:28 step:21958 [D loss: 0.250043, acc.: 92.97%] [G loss: 4.364644]\n",
      "epoch:28 step:21959 [D loss: 0.003010, acc.: 100.00%] [G loss: 6.166626]\n",
      "epoch:28 step:21960 [D loss: 0.003433, acc.: 100.00%] [G loss: 6.067087]\n",
      "epoch:28 step:21961 [D loss: 0.003632, acc.: 100.00%] [G loss: 5.797868]\n",
      "epoch:28 step:21962 [D loss: 0.014991, acc.: 100.00%] [G loss: 5.042983]\n",
      "epoch:28 step:21963 [D loss: 0.003457, acc.: 100.00%] [G loss: 5.095246]\n",
      "epoch:28 step:21964 [D loss: 0.003075, acc.: 100.00%] [G loss: 4.769845]\n",
      "epoch:28 step:21965 [D loss: 0.012379, acc.: 100.00%] [G loss: 4.412012]\n",
      "epoch:28 step:21966 [D loss: 0.012385, acc.: 100.00%] [G loss: 5.301146]\n",
      "epoch:28 step:21967 [D loss: 0.284569, acc.: 92.19%] [G loss: 5.618235]\n",
      "epoch:28 step:21968 [D loss: 0.005509, acc.: 100.00%] [G loss: 6.251333]\n",
      "epoch:28 step:21969 [D loss: 0.018259, acc.: 100.00%] [G loss: 6.363258]\n",
      "epoch:28 step:21970 [D loss: 0.009182, acc.: 100.00%] [G loss: 5.237223]\n",
      "epoch:28 step:21971 [D loss: 0.100709, acc.: 97.66%] [G loss: 3.010249]\n",
      "epoch:28 step:21972 [D loss: 0.078875, acc.: 99.22%] [G loss: 3.834778]\n",
      "epoch:28 step:21973 [D loss: 0.017125, acc.: 100.00%] [G loss: 5.017416]\n",
      "epoch:28 step:21974 [D loss: 0.003451, acc.: 100.00%] [G loss: 4.872293]\n",
      "epoch:28 step:21975 [D loss: 0.027776, acc.: 99.22%] [G loss: 4.784011]\n",
      "epoch:28 step:21976 [D loss: 0.009841, acc.: 100.00%] [G loss: 5.162845]\n",
      "epoch:28 step:21977 [D loss: 0.002860, acc.: 100.00%] [G loss: 5.263770]\n",
      "epoch:28 step:21978 [D loss: 0.056057, acc.: 99.22%] [G loss: 3.393617]\n",
      "epoch:28 step:21979 [D loss: 0.004131, acc.: 100.00%] [G loss: 4.103126]\n",
      "epoch:28 step:21980 [D loss: 0.010471, acc.: 100.00%] [G loss: 4.131959]\n",
      "epoch:28 step:21981 [D loss: 0.014239, acc.: 100.00%] [G loss: 3.814172]\n",
      "epoch:28 step:21982 [D loss: 0.018084, acc.: 100.00%] [G loss: 4.482766]\n",
      "epoch:28 step:21983 [D loss: 0.002459, acc.: 100.00%] [G loss: 4.642011]\n",
      "epoch:28 step:21984 [D loss: 0.149941, acc.: 97.66%] [G loss: 5.642690]\n",
      "epoch:28 step:21985 [D loss: 0.008431, acc.: 100.00%] [G loss: 5.574663]\n",
      "epoch:28 step:21986 [D loss: 0.003135, acc.: 100.00%] [G loss: 6.207143]\n",
      "epoch:28 step:21987 [D loss: 0.027551, acc.: 98.44%] [G loss: 5.151407]\n",
      "epoch:28 step:21988 [D loss: 0.003704, acc.: 100.00%] [G loss: 4.180490]\n",
      "epoch:28 step:21989 [D loss: 0.006243, acc.: 100.00%] [G loss: 4.141102]\n",
      "epoch:28 step:21990 [D loss: 0.020762, acc.: 100.00%] [G loss: 5.706676]\n",
      "epoch:28 step:21991 [D loss: 0.006127, acc.: 100.00%] [G loss: 5.538619]\n",
      "epoch:28 step:21992 [D loss: 0.002802, acc.: 100.00%] [G loss: 4.841964]\n",
      "epoch:28 step:21993 [D loss: 0.014002, acc.: 100.00%] [G loss: 4.850766]\n",
      "epoch:28 step:21994 [D loss: 0.013882, acc.: 100.00%] [G loss: 4.150398]\n",
      "epoch:28 step:21995 [D loss: 0.031733, acc.: 100.00%] [G loss: 5.532170]\n",
      "epoch:28 step:21996 [D loss: 0.142261, acc.: 94.53%] [G loss: 3.724955]\n",
      "epoch:28 step:21997 [D loss: 0.006679, acc.: 100.00%] [G loss: 3.437205]\n",
      "epoch:28 step:21998 [D loss: 0.095201, acc.: 96.09%] [G loss: 7.295158]\n",
      "epoch:28 step:21999 [D loss: 0.006196, acc.: 100.00%] [G loss: 8.182507]\n",
      "epoch:28 step:22000 [D loss: 4.330562, acc.: 0.00%] [G loss: 8.242987]\n",
      "##############\n",
      "[1.01111527 0.90913348 0.98043955 1.06190136 2.1018867  1.11432841\n",
      " 2.09989355 2.10678094 2.11638428 1.02088611]\n",
      "##########\n",
      "epoch:28 step:22001 [D loss: 2.752677, acc.: 50.00%] [G loss: 6.457428]\n",
      "epoch:28 step:22002 [D loss: 1.974695, acc.: 50.00%] [G loss: 3.009357]\n",
      "epoch:28 step:22003 [D loss: 0.506276, acc.: 75.00%] [G loss: 2.266150]\n",
      "epoch:28 step:22004 [D loss: 0.151459, acc.: 97.66%] [G loss: 2.854451]\n",
      "epoch:28 step:22005 [D loss: 0.260732, acc.: 89.84%] [G loss: 2.907572]\n",
      "epoch:28 step:22006 [D loss: 0.182650, acc.: 97.66%] [G loss: 2.020417]\n",
      "epoch:28 step:22007 [D loss: 0.091299, acc.: 99.22%] [G loss: 2.146978]\n",
      "epoch:28 step:22008 [D loss: 0.230665, acc.: 90.62%] [G loss: 3.233143]\n",
      "epoch:28 step:22009 [D loss: 0.289546, acc.: 87.50%] [G loss: 2.922921]\n",
      "epoch:28 step:22010 [D loss: 0.485832, acc.: 76.56%] [G loss: 3.748919]\n",
      "epoch:28 step:22011 [D loss: 0.405429, acc.: 79.69%] [G loss: 2.139725]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:28 step:22012 [D loss: 0.092159, acc.: 98.44%] [G loss: 1.391127]\n",
      "epoch:28 step:22013 [D loss: 0.292743, acc.: 83.59%] [G loss: 3.181358]\n",
      "epoch:28 step:22014 [D loss: 0.148537, acc.: 94.53%] [G loss: 3.746996]\n",
      "epoch:28 step:22015 [D loss: 0.325904, acc.: 86.72%] [G loss: 1.335089]\n",
      "epoch:28 step:22016 [D loss: 0.152026, acc.: 95.31%] [G loss: 2.285184]\n",
      "epoch:28 step:22017 [D loss: 0.070667, acc.: 98.44%] [G loss: 2.982184]\n",
      "epoch:28 step:22018 [D loss: 0.300352, acc.: 88.28%] [G loss: 1.720908]\n",
      "epoch:28 step:22019 [D loss: 0.157218, acc.: 95.31%] [G loss: 3.777510]\n",
      "epoch:28 step:22020 [D loss: 0.385770, acc.: 82.81%] [G loss: 3.077976]\n",
      "epoch:28 step:22021 [D loss: 0.168795, acc.: 93.75%] [G loss: 2.210784]\n",
      "epoch:28 step:22022 [D loss: 0.072621, acc.: 99.22%] [G loss: 2.584731]\n",
      "epoch:28 step:22023 [D loss: 0.268485, acc.: 89.84%] [G loss: 4.326589]\n",
      "epoch:28 step:22024 [D loss: 0.430208, acc.: 75.00%] [G loss: 1.818361]\n",
      "epoch:28 step:22025 [D loss: 0.474786, acc.: 76.56%] [G loss: 4.369181]\n",
      "epoch:28 step:22026 [D loss: 0.123552, acc.: 95.31%] [G loss: 4.807133]\n",
      "epoch:28 step:22027 [D loss: 0.167646, acc.: 94.53%] [G loss: 3.339647]\n",
      "epoch:28 step:22028 [D loss: 0.147703, acc.: 95.31%] [G loss: 3.261807]\n",
      "epoch:28 step:22029 [D loss: 0.105812, acc.: 96.88%] [G loss: 4.056484]\n",
      "epoch:28 step:22030 [D loss: 0.026771, acc.: 99.22%] [G loss: 4.115828]\n",
      "epoch:28 step:22031 [D loss: 0.073362, acc.: 97.66%] [G loss: 3.497011]\n",
      "epoch:28 step:22032 [D loss: 0.270054, acc.: 89.06%] [G loss: 3.959940]\n",
      "epoch:28 step:22033 [D loss: 0.126595, acc.: 96.09%] [G loss: 2.890982]\n",
      "epoch:28 step:22034 [D loss: 0.089319, acc.: 98.44%] [G loss: 3.696060]\n",
      "epoch:28 step:22035 [D loss: 0.042264, acc.: 100.00%] [G loss: 3.879008]\n",
      "epoch:28 step:22036 [D loss: 0.039017, acc.: 100.00%] [G loss: 2.435128]\n",
      "epoch:28 step:22037 [D loss: 0.065122, acc.: 99.22%] [G loss: 2.857761]\n",
      "epoch:28 step:22038 [D loss: 0.036555, acc.: 100.00%] [G loss: 3.411937]\n",
      "epoch:28 step:22039 [D loss: 0.386693, acc.: 85.94%] [G loss: 5.414032]\n",
      "epoch:28 step:22040 [D loss: 0.788831, acc.: 63.28%] [G loss: 2.724482]\n",
      "epoch:28 step:22041 [D loss: 0.079042, acc.: 97.66%] [G loss: 4.222623]\n",
      "epoch:28 step:22042 [D loss: 0.111389, acc.: 98.44%] [G loss: 3.790695]\n",
      "epoch:28 step:22043 [D loss: 0.051119, acc.: 99.22%] [G loss: 3.489690]\n",
      "epoch:28 step:22044 [D loss: 0.026140, acc.: 100.00%] [G loss: 2.454612]\n",
      "epoch:28 step:22045 [D loss: 0.065144, acc.: 99.22%] [G loss: 2.387735]\n",
      "epoch:28 step:22046 [D loss: 0.153490, acc.: 98.44%] [G loss: 3.785755]\n",
      "epoch:28 step:22047 [D loss: 0.097792, acc.: 95.31%] [G loss: 3.190363]\n",
      "epoch:28 step:22048 [D loss: 0.054202, acc.: 99.22%] [G loss: 2.259285]\n",
      "epoch:28 step:22049 [D loss: 0.171964, acc.: 93.75%] [G loss: 3.781551]\n",
      "epoch:28 step:22050 [D loss: 0.119337, acc.: 94.53%] [G loss: 3.165717]\n",
      "epoch:28 step:22051 [D loss: 0.047532, acc.: 99.22%] [G loss: 4.255675]\n",
      "epoch:28 step:22052 [D loss: 0.063358, acc.: 98.44%] [G loss: 2.475408]\n",
      "epoch:28 step:22053 [D loss: 0.053239, acc.: 99.22%] [G loss: 3.382566]\n",
      "epoch:28 step:22054 [D loss: 0.056679, acc.: 100.00%] [G loss: 3.036940]\n",
      "epoch:28 step:22055 [D loss: 0.064144, acc.: 99.22%] [G loss: 4.251316]\n",
      "epoch:28 step:22056 [D loss: 0.128946, acc.: 96.88%] [G loss: 4.033136]\n",
      "epoch:28 step:22057 [D loss: 0.026716, acc.: 100.00%] [G loss: 3.985123]\n",
      "epoch:28 step:22058 [D loss: 0.053069, acc.: 99.22%] [G loss: 3.391087]\n",
      "epoch:28 step:22059 [D loss: 0.024531, acc.: 100.00%] [G loss: 3.764964]\n",
      "epoch:28 step:22060 [D loss: 0.036851, acc.: 100.00%] [G loss: 3.487640]\n",
      "epoch:28 step:22061 [D loss: 0.039640, acc.: 100.00%] [G loss: 3.408254]\n",
      "epoch:28 step:22062 [D loss: 0.049462, acc.: 99.22%] [G loss: 3.158768]\n",
      "epoch:28 step:22063 [D loss: 0.075573, acc.: 100.00%] [G loss: 3.038123]\n",
      "epoch:28 step:22064 [D loss: 0.245766, acc.: 93.75%] [G loss: 4.116469]\n",
      "epoch:28 step:22065 [D loss: 0.037392, acc.: 99.22%] [G loss: 4.741558]\n",
      "epoch:28 step:22066 [D loss: 0.044167, acc.: 100.00%] [G loss: 4.480340]\n",
      "epoch:28 step:22067 [D loss: 0.498644, acc.: 77.34%] [G loss: 6.191073]\n",
      "epoch:28 step:22068 [D loss: 0.033852, acc.: 100.00%] [G loss: 6.639372]\n",
      "epoch:28 step:22069 [D loss: 0.717833, acc.: 66.41%] [G loss: 1.761165]\n",
      "epoch:28 step:22070 [D loss: 0.684401, acc.: 70.31%] [G loss: 7.070939]\n",
      "epoch:28 step:22071 [D loss: 0.105029, acc.: 93.75%] [G loss: 7.145214]\n",
      "epoch:28 step:22072 [D loss: 0.323859, acc.: 85.94%] [G loss: 5.242769]\n",
      "epoch:28 step:22073 [D loss: 0.034063, acc.: 100.00%] [G loss: 4.245896]\n",
      "epoch:28 step:22074 [D loss: 0.011382, acc.: 100.00%] [G loss: 3.992276]\n",
      "epoch:28 step:22075 [D loss: 0.069582, acc.: 98.44%] [G loss: 4.733834]\n",
      "epoch:28 step:22076 [D loss: 0.005883, acc.: 100.00%] [G loss: 4.281273]\n",
      "epoch:28 step:22077 [D loss: 0.019522, acc.: 100.00%] [G loss: 3.578386]\n",
      "epoch:28 step:22078 [D loss: 0.046244, acc.: 99.22%] [G loss: 2.800766]\n",
      "epoch:28 step:22079 [D loss: 0.020623, acc.: 100.00%] [G loss: 3.527885]\n",
      "epoch:28 step:22080 [D loss: 0.040629, acc.: 99.22%] [G loss: 2.705896]\n",
      "epoch:28 step:22081 [D loss: 0.024715, acc.: 100.00%] [G loss: 3.052782]\n",
      "epoch:28 step:22082 [D loss: 0.189401, acc.: 93.75%] [G loss: 4.230684]\n",
      "epoch:28 step:22083 [D loss: 0.012968, acc.: 99.22%] [G loss: 5.447195]\n",
      "epoch:28 step:22084 [D loss: 0.349548, acc.: 84.38%] [G loss: 0.295995]\n",
      "epoch:28 step:22085 [D loss: 0.511234, acc.: 74.22%] [G loss: 6.225934]\n",
      "epoch:28 step:22086 [D loss: 0.662783, acc.: 72.66%] [G loss: 4.548668]\n",
      "epoch:28 step:22087 [D loss: 0.004249, acc.: 100.00%] [G loss: 2.269269]\n",
      "epoch:28 step:22088 [D loss: 0.044492, acc.: 100.00%] [G loss: 2.907689]\n",
      "epoch:28 step:22089 [D loss: 0.092602, acc.: 97.66%] [G loss: 4.242520]\n",
      "epoch:28 step:22090 [D loss: 0.040055, acc.: 98.44%] [G loss: 4.394255]\n",
      "epoch:28 step:22091 [D loss: 0.102192, acc.: 97.66%] [G loss: 4.044685]\n",
      "epoch:28 step:22092 [D loss: 0.027203, acc.: 100.00%] [G loss: 3.176598]\n",
      "epoch:28 step:22093 [D loss: 0.035542, acc.: 100.00%] [G loss: 3.780255]\n",
      "epoch:28 step:22094 [D loss: 0.092671, acc.: 98.44%] [G loss: 3.863763]\n",
      "epoch:28 step:22095 [D loss: 0.037326, acc.: 100.00%] [G loss: 4.832199]\n",
      "epoch:28 step:22096 [D loss: 0.020203, acc.: 100.00%] [G loss: 4.824626]\n",
      "epoch:28 step:22097 [D loss: 0.262488, acc.: 90.62%] [G loss: 4.714281]\n",
      "epoch:28 step:22098 [D loss: 0.050463, acc.: 98.44%] [G loss: 4.829410]\n",
      "epoch:28 step:22099 [D loss: 0.040736, acc.: 99.22%] [G loss: 4.910665]\n",
      "epoch:28 step:22100 [D loss: 0.050757, acc.: 99.22%] [G loss: 3.514626]\n",
      "epoch:28 step:22101 [D loss: 0.006423, acc.: 100.00%] [G loss: 4.374830]\n",
      "epoch:28 step:22102 [D loss: 0.026734, acc.: 100.00%] [G loss: 3.747222]\n",
      "epoch:28 step:22103 [D loss: 0.046785, acc.: 100.00%] [G loss: 4.501066]\n",
      "epoch:28 step:22104 [D loss: 0.027520, acc.: 100.00%] [G loss: 4.818251]\n",
      "epoch:28 step:22105 [D loss: 0.069787, acc.: 99.22%] [G loss: 4.213305]\n",
      "epoch:28 step:22106 [D loss: 0.015583, acc.: 100.00%] [G loss: 4.447645]\n",
      "epoch:28 step:22107 [D loss: 0.010965, acc.: 100.00%] [G loss: 4.337566]\n",
      "epoch:28 step:22108 [D loss: 1.082954, acc.: 39.06%] [G loss: 2.535574]\n",
      "epoch:28 step:22109 [D loss: 0.020126, acc.: 100.00%] [G loss: 4.237174]\n",
      "epoch:28 step:22110 [D loss: 0.006663, acc.: 100.00%] [G loss: 4.787024]\n",
      "epoch:28 step:22111 [D loss: 0.006678, acc.: 100.00%] [G loss: 4.273746]\n",
      "epoch:28 step:22112 [D loss: 0.011047, acc.: 100.00%] [G loss: 4.390293]\n",
      "epoch:28 step:22113 [D loss: 0.008124, acc.: 100.00%] [G loss: 3.561089]\n",
      "epoch:28 step:22114 [D loss: 0.136070, acc.: 96.88%] [G loss: 2.138746]\n",
      "epoch:28 step:22115 [D loss: 0.059646, acc.: 100.00%] [G loss: 3.112980]\n",
      "epoch:28 step:22116 [D loss: 0.014552, acc.: 100.00%] [G loss: 3.937056]\n",
      "epoch:28 step:22117 [D loss: 0.021827, acc.: 99.22%] [G loss: 3.640773]\n",
      "epoch:28 step:22118 [D loss: 1.788865, acc.: 37.50%] [G loss: 7.612984]\n",
      "epoch:28 step:22119 [D loss: 2.058371, acc.: 50.00%] [G loss: 5.411584]\n",
      "epoch:28 step:22120 [D loss: 0.820859, acc.: 61.72%] [G loss: 1.727535]\n",
      "epoch:28 step:22121 [D loss: 0.300851, acc.: 85.16%] [G loss: 1.914510]\n",
      "epoch:28 step:22122 [D loss: 0.250239, acc.: 89.06%] [G loss: 4.365041]\n",
      "epoch:28 step:22123 [D loss: 0.047818, acc.: 100.00%] [G loss: 5.004044]\n",
      "epoch:28 step:22124 [D loss: 0.426481, acc.: 78.12%] [G loss: 3.113877]\n",
      "epoch:28 step:22125 [D loss: 0.132227, acc.: 94.53%] [G loss: 3.355171]\n",
      "epoch:28 step:22126 [D loss: 0.052186, acc.: 99.22%] [G loss: 3.120260]\n",
      "epoch:28 step:22127 [D loss: 0.082690, acc.: 99.22%] [G loss: 3.358264]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:28 step:22128 [D loss: 0.192501, acc.: 92.19%] [G loss: 2.384990]\n",
      "epoch:28 step:22129 [D loss: 0.077845, acc.: 99.22%] [G loss: 3.021083]\n",
      "epoch:28 step:22130 [D loss: 0.099319, acc.: 98.44%] [G loss: 3.460056]\n",
      "epoch:28 step:22131 [D loss: 0.057533, acc.: 100.00%] [G loss: 3.492274]\n",
      "epoch:28 step:22132 [D loss: 0.802093, acc.: 66.41%] [G loss: 2.050187]\n",
      "epoch:28 step:22133 [D loss: 0.167341, acc.: 94.53%] [G loss: 4.028996]\n",
      "epoch:28 step:22134 [D loss: 0.081587, acc.: 97.66%] [G loss: 4.209658]\n",
      "epoch:28 step:22135 [D loss: 0.364895, acc.: 80.47%] [G loss: 2.977066]\n",
      "epoch:28 step:22136 [D loss: 0.160923, acc.: 93.75%] [G loss: 3.387097]\n",
      "epoch:28 step:22137 [D loss: 0.115726, acc.: 98.44%] [G loss: 2.899412]\n",
      "epoch:28 step:22138 [D loss: 0.130080, acc.: 96.09%] [G loss: 3.750846]\n",
      "epoch:28 step:22139 [D loss: 0.028961, acc.: 100.00%] [G loss: 3.964664]\n",
      "epoch:28 step:22140 [D loss: 0.163390, acc.: 93.75%] [G loss: 2.912764]\n",
      "epoch:28 step:22141 [D loss: 0.387465, acc.: 84.38%] [G loss: 4.289819]\n",
      "epoch:28 step:22142 [D loss: 0.394988, acc.: 81.25%] [G loss: 3.974355]\n",
      "epoch:28 step:22143 [D loss: 0.062014, acc.: 99.22%] [G loss: 4.064932]\n",
      "epoch:28 step:22144 [D loss: 0.050885, acc.: 98.44%] [G loss: 3.255636]\n",
      "epoch:28 step:22145 [D loss: 0.286634, acc.: 88.28%] [G loss: 3.114883]\n",
      "epoch:28 step:22146 [D loss: 0.056199, acc.: 99.22%] [G loss: 3.723178]\n",
      "epoch:28 step:22147 [D loss: 0.053912, acc.: 99.22%] [G loss: 3.923131]\n",
      "epoch:28 step:22148 [D loss: 0.664982, acc.: 61.72%] [G loss: 5.523607]\n",
      "epoch:28 step:22149 [D loss: 0.266220, acc.: 86.72%] [G loss: 5.082685]\n",
      "epoch:28 step:22150 [D loss: 0.075436, acc.: 97.66%] [G loss: 4.422656]\n",
      "epoch:28 step:22151 [D loss: 0.076367, acc.: 96.88%] [G loss: 3.892624]\n",
      "epoch:28 step:22152 [D loss: 0.050455, acc.: 98.44%] [G loss: 3.916669]\n",
      "epoch:28 step:22153 [D loss: 0.075722, acc.: 98.44%] [G loss: 3.690776]\n",
      "epoch:28 step:22154 [D loss: 0.022630, acc.: 100.00%] [G loss: 4.071617]\n",
      "epoch:28 step:22155 [D loss: 0.047566, acc.: 99.22%] [G loss: 2.840688]\n",
      "epoch:28 step:22156 [D loss: 0.057940, acc.: 100.00%] [G loss: 3.820592]\n",
      "epoch:28 step:22157 [D loss: 0.131044, acc.: 95.31%] [G loss: 2.836180]\n",
      "epoch:28 step:22158 [D loss: 0.023319, acc.: 100.00%] [G loss: 2.973897]\n",
      "epoch:28 step:22159 [D loss: 0.099165, acc.: 99.22%] [G loss: 3.752847]\n",
      "epoch:28 step:22160 [D loss: 0.017650, acc.: 100.00%] [G loss: 4.024619]\n",
      "epoch:28 step:22161 [D loss: 0.146304, acc.: 94.53%] [G loss: 2.796546]\n",
      "epoch:28 step:22162 [D loss: 0.035102, acc.: 100.00%] [G loss: 2.888192]\n",
      "epoch:28 step:22163 [D loss: 0.030901, acc.: 99.22%] [G loss: 3.104574]\n",
      "epoch:28 step:22164 [D loss: 0.071078, acc.: 99.22%] [G loss: 2.319253]\n",
      "epoch:28 step:22165 [D loss: 0.141468, acc.: 95.31%] [G loss: 1.708997]\n",
      "epoch:28 step:22166 [D loss: 0.207633, acc.: 92.97%] [G loss: 3.966878]\n",
      "epoch:28 step:22167 [D loss: 0.670672, acc.: 68.75%] [G loss: 5.444438]\n",
      "epoch:28 step:22168 [D loss: 0.068529, acc.: 97.66%] [G loss: 6.363695]\n",
      "epoch:28 step:22169 [D loss: 0.339954, acc.: 84.38%] [G loss: 2.660459]\n",
      "epoch:28 step:22170 [D loss: 0.522148, acc.: 74.22%] [G loss: 6.102703]\n",
      "epoch:28 step:22171 [D loss: 0.101455, acc.: 96.88%] [G loss: 7.098452]\n",
      "epoch:28 step:22172 [D loss: 0.258284, acc.: 86.72%] [G loss: 5.017182]\n",
      "epoch:28 step:22173 [D loss: 0.013383, acc.: 100.00%] [G loss: 3.736444]\n",
      "epoch:28 step:22174 [D loss: 0.087996, acc.: 98.44%] [G loss: 4.600023]\n",
      "epoch:28 step:22175 [D loss: 0.007960, acc.: 100.00%] [G loss: 5.055578]\n",
      "epoch:28 step:22176 [D loss: 0.012580, acc.: 100.00%] [G loss: 4.605469]\n",
      "epoch:28 step:22177 [D loss: 0.067287, acc.: 99.22%] [G loss: 4.555943]\n",
      "epoch:28 step:22178 [D loss: 0.085158, acc.: 96.88%] [G loss: 3.683094]\n",
      "epoch:28 step:22179 [D loss: 0.051055, acc.: 98.44%] [G loss: 3.628396]\n",
      "epoch:28 step:22180 [D loss: 0.050288, acc.: 99.22%] [G loss: 4.391772]\n",
      "epoch:28 step:22181 [D loss: 0.021694, acc.: 100.00%] [G loss: 3.776876]\n",
      "epoch:28 step:22182 [D loss: 0.029662, acc.: 100.00%] [G loss: 3.650541]\n",
      "epoch:28 step:22183 [D loss: 0.121057, acc.: 96.88%] [G loss: 3.302666]\n",
      "epoch:28 step:22184 [D loss: 0.025662, acc.: 100.00%] [G loss: 3.712252]\n",
      "epoch:28 step:22185 [D loss: 0.121388, acc.: 96.88%] [G loss: 5.042667]\n",
      "epoch:28 step:22186 [D loss: 0.079407, acc.: 97.66%] [G loss: 4.809904]\n",
      "epoch:28 step:22187 [D loss: 0.240728, acc.: 92.19%] [G loss: 3.274154]\n",
      "epoch:28 step:22188 [D loss: 0.139931, acc.: 96.09%] [G loss: 3.697191]\n",
      "epoch:28 step:22189 [D loss: 0.016137, acc.: 100.00%] [G loss: 3.800135]\n",
      "epoch:28 step:22190 [D loss: 0.023067, acc.: 100.00%] [G loss: 3.786721]\n",
      "epoch:28 step:22191 [D loss: 0.011838, acc.: 100.00%] [G loss: 3.999981]\n",
      "epoch:28 step:22192 [D loss: 0.020818, acc.: 100.00%] [G loss: 4.259180]\n",
      "epoch:28 step:22193 [D loss: 0.056108, acc.: 99.22%] [G loss: 4.167982]\n",
      "epoch:28 step:22194 [D loss: 0.025562, acc.: 100.00%] [G loss: 3.820495]\n",
      "epoch:28 step:22195 [D loss: 0.048110, acc.: 100.00%] [G loss: 4.442764]\n",
      "epoch:28 step:22196 [D loss: 0.008124, acc.: 100.00%] [G loss: 4.982239]\n",
      "epoch:28 step:22197 [D loss: 0.026043, acc.: 100.00%] [G loss: 4.284743]\n",
      "epoch:28 step:22198 [D loss: 0.019333, acc.: 100.00%] [G loss: 3.848249]\n",
      "epoch:28 step:22199 [D loss: 0.090060, acc.: 97.66%] [G loss: 2.445889]\n",
      "epoch:28 step:22200 [D loss: 0.092692, acc.: 97.66%] [G loss: 3.676082]\n",
      "##############\n",
      "[0.94987735 1.10589575 1.05332827 0.946304   1.0055421  1.00043881\n",
      " 2.10739985 1.02354562 2.1095818  0.99498312]\n",
      "##########\n",
      "epoch:28 step:22201 [D loss: 0.005105, acc.: 100.00%] [G loss: 4.336629]\n",
      "epoch:28 step:22202 [D loss: 0.047332, acc.: 100.00%] [G loss: 3.762869]\n",
      "epoch:28 step:22203 [D loss: 0.009060, acc.: 100.00%] [G loss: 3.382366]\n",
      "epoch:28 step:22204 [D loss: 0.498903, acc.: 77.34%] [G loss: 6.758193]\n",
      "epoch:28 step:22205 [D loss: 0.360630, acc.: 79.69%] [G loss: 4.701868]\n",
      "epoch:28 step:22206 [D loss: 0.070729, acc.: 96.88%] [G loss: 5.412161]\n",
      "epoch:28 step:22207 [D loss: 0.003498, acc.: 100.00%] [G loss: 5.577918]\n",
      "epoch:28 step:22208 [D loss: 0.341402, acc.: 85.16%] [G loss: 6.032703]\n",
      "epoch:28 step:22209 [D loss: 0.001355, acc.: 100.00%] [G loss: 7.406645]\n",
      "epoch:28 step:22210 [D loss: 0.121644, acc.: 96.88%] [G loss: 5.603383]\n",
      "epoch:28 step:22211 [D loss: 0.071593, acc.: 98.44%] [G loss: 5.065668]\n",
      "epoch:28 step:22212 [D loss: 0.003002, acc.: 100.00%] [G loss: 5.072183]\n",
      "epoch:28 step:22213 [D loss: 0.002905, acc.: 100.00%] [G loss: 5.495905]\n",
      "epoch:28 step:22214 [D loss: 0.017703, acc.: 100.00%] [G loss: 4.907884]\n",
      "epoch:28 step:22215 [D loss: 0.031791, acc.: 99.22%] [G loss: 4.824828]\n",
      "epoch:28 step:22216 [D loss: 0.004499, acc.: 100.00%] [G loss: 5.344965]\n",
      "epoch:28 step:22217 [D loss: 0.005785, acc.: 100.00%] [G loss: 5.575032]\n",
      "epoch:28 step:22218 [D loss: 0.038381, acc.: 99.22%] [G loss: 3.982375]\n",
      "epoch:28 step:22219 [D loss: 0.007046, acc.: 100.00%] [G loss: 3.933056]\n",
      "epoch:28 step:22220 [D loss: 0.038003, acc.: 100.00%] [G loss: 4.491632]\n",
      "epoch:28 step:22221 [D loss: 0.012231, acc.: 100.00%] [G loss: 5.056347]\n",
      "epoch:28 step:22222 [D loss: 0.007934, acc.: 100.00%] [G loss: 4.113714]\n",
      "epoch:28 step:22223 [D loss: 0.050266, acc.: 100.00%] [G loss: 2.968008]\n",
      "epoch:28 step:22224 [D loss: 0.061471, acc.: 99.22%] [G loss: 3.380797]\n",
      "epoch:28 step:22225 [D loss: 0.005601, acc.: 100.00%] [G loss: 3.780122]\n",
      "epoch:28 step:22226 [D loss: 0.093299, acc.: 96.09%] [G loss: 1.796618]\n",
      "epoch:28 step:22227 [D loss: 0.091019, acc.: 97.66%] [G loss: 2.505433]\n",
      "epoch:28 step:22228 [D loss: 0.005839, acc.: 100.00%] [G loss: 4.578814]\n",
      "epoch:28 step:22229 [D loss: 0.295452, acc.: 86.72%] [G loss: 3.661268]\n",
      "epoch:28 step:22230 [D loss: 0.006266, acc.: 100.00%] [G loss: 4.069944]\n",
      "epoch:28 step:22231 [D loss: 0.066914, acc.: 99.22%] [G loss: 3.119253]\n",
      "epoch:28 step:22232 [D loss: 0.057713, acc.: 100.00%] [G loss: 2.478459]\n",
      "epoch:28 step:22233 [D loss: 0.006669, acc.: 100.00%] [G loss: 3.681990]\n",
      "epoch:28 step:22234 [D loss: 0.006140, acc.: 100.00%] [G loss: 2.491878]\n",
      "epoch:28 step:22235 [D loss: 0.010661, acc.: 100.00%] [G loss: 3.314770]\n",
      "epoch:28 step:22236 [D loss: 0.019168, acc.: 100.00%] [G loss: 2.793924]\n",
      "epoch:28 step:22237 [D loss: 0.012807, acc.: 100.00%] [G loss: 3.625537]\n",
      "epoch:28 step:22238 [D loss: 0.102342, acc.: 97.66%] [G loss: 3.117467]\n",
      "epoch:28 step:22239 [D loss: 0.013223, acc.: 100.00%] [G loss: 3.206007]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:28 step:22240 [D loss: 0.014415, acc.: 100.00%] [G loss: 4.892575]\n",
      "epoch:28 step:22241 [D loss: 0.028738, acc.: 100.00%] [G loss: 4.079387]\n",
      "epoch:28 step:22242 [D loss: 0.142234, acc.: 96.88%] [G loss: 2.045483]\n",
      "epoch:28 step:22243 [D loss: 1.122045, acc.: 53.91%] [G loss: 9.303370]\n",
      "epoch:28 step:22244 [D loss: 3.455391, acc.: 50.00%] [G loss: 6.034886]\n",
      "epoch:28 step:22245 [D loss: 2.222822, acc.: 50.00%] [G loss: 2.941901]\n",
      "epoch:28 step:22246 [D loss: 1.083373, acc.: 46.88%] [G loss: 2.026830]\n",
      "epoch:28 step:22247 [D loss: 0.403043, acc.: 84.38%] [G loss: 2.588560]\n",
      "epoch:28 step:22248 [D loss: 0.360338, acc.: 81.25%] [G loss: 2.670516]\n",
      "epoch:28 step:22249 [D loss: 0.280808, acc.: 89.84%] [G loss: 2.862470]\n",
      "epoch:28 step:22250 [D loss: 0.351321, acc.: 85.94%] [G loss: 2.605140]\n",
      "epoch:28 step:22251 [D loss: 0.191168, acc.: 96.09%] [G loss: 2.666126]\n",
      "epoch:28 step:22252 [D loss: 0.137292, acc.: 100.00%] [G loss: 2.445688]\n",
      "epoch:28 step:22253 [D loss: 0.184373, acc.: 95.31%] [G loss: 1.803875]\n",
      "epoch:28 step:22254 [D loss: 0.117589, acc.: 97.66%] [G loss: 2.794078]\n",
      "epoch:28 step:22255 [D loss: 0.246160, acc.: 93.75%] [G loss: 2.063325]\n",
      "epoch:28 step:22256 [D loss: 0.277378, acc.: 89.84%] [G loss: 2.748478]\n",
      "epoch:28 step:22257 [D loss: 0.247898, acc.: 92.97%] [G loss: 2.577433]\n",
      "epoch:28 step:22258 [D loss: 0.437487, acc.: 82.81%] [G loss: 2.353060]\n",
      "epoch:28 step:22259 [D loss: 0.165238, acc.: 96.09%] [G loss: 2.850628]\n",
      "epoch:28 step:22260 [D loss: 0.104871, acc.: 96.88%] [G loss: 2.848220]\n",
      "epoch:28 step:22261 [D loss: 0.404653, acc.: 79.69%] [G loss: 3.857410]\n",
      "epoch:28 step:22262 [D loss: 0.149265, acc.: 95.31%] [G loss: 3.826315]\n",
      "epoch:28 step:22263 [D loss: 0.764130, acc.: 60.94%] [G loss: 1.854132]\n",
      "epoch:28 step:22264 [D loss: 0.173891, acc.: 95.31%] [G loss: 2.755897]\n",
      "epoch:28 step:22265 [D loss: 0.115715, acc.: 99.22%] [G loss: 2.629115]\n",
      "epoch:28 step:22266 [D loss: 0.066762, acc.: 99.22%] [G loss: 2.809901]\n",
      "epoch:28 step:22267 [D loss: 0.150131, acc.: 96.88%] [G loss: 3.530906]\n",
      "epoch:28 step:22268 [D loss: 0.393145, acc.: 82.81%] [G loss: 2.749088]\n",
      "epoch:28 step:22269 [D loss: 0.219836, acc.: 92.19%] [G loss: 3.129967]\n",
      "epoch:28 step:22270 [D loss: 0.288607, acc.: 89.06%] [G loss: 2.834424]\n",
      "epoch:28 step:22271 [D loss: 0.188541, acc.: 96.88%] [G loss: 3.379587]\n",
      "epoch:28 step:22272 [D loss: 0.104979, acc.: 98.44%] [G loss: 3.066404]\n",
      "epoch:28 step:22273 [D loss: 0.336980, acc.: 86.72%] [G loss: 4.272423]\n",
      "epoch:28 step:22274 [D loss: 0.507318, acc.: 78.12%] [G loss: 3.083127]\n",
      "epoch:28 step:22275 [D loss: 0.122901, acc.: 96.09%] [G loss: 3.172068]\n",
      "epoch:28 step:22276 [D loss: 0.124820, acc.: 96.09%] [G loss: 2.736892]\n",
      "epoch:28 step:22277 [D loss: 0.422959, acc.: 80.47%] [G loss: 3.850732]\n",
      "epoch:28 step:22278 [D loss: 0.203193, acc.: 92.19%] [G loss: 3.671576]\n",
      "epoch:28 step:22279 [D loss: 0.237752, acc.: 90.62%] [G loss: 2.871235]\n",
      "epoch:28 step:22280 [D loss: 0.060240, acc.: 97.66%] [G loss: 3.024353]\n",
      "epoch:28 step:22281 [D loss: 0.101526, acc.: 98.44%] [G loss: 2.331714]\n",
      "epoch:28 step:22282 [D loss: 0.123347, acc.: 97.66%] [G loss: 3.125648]\n",
      "epoch:28 step:22283 [D loss: 0.129782, acc.: 97.66%] [G loss: 3.988161]\n",
      "epoch:28 step:22284 [D loss: 0.162427, acc.: 91.41%] [G loss: 2.728559]\n",
      "epoch:28 step:22285 [D loss: 0.127744, acc.: 96.09%] [G loss: 3.222092]\n",
      "epoch:28 step:22286 [D loss: 0.117014, acc.: 98.44%] [G loss: 2.704584]\n",
      "epoch:28 step:22287 [D loss: 0.316942, acc.: 82.03%] [G loss: 4.908822]\n",
      "epoch:28 step:22288 [D loss: 0.233848, acc.: 89.84%] [G loss: 4.439339]\n",
      "epoch:28 step:22289 [D loss: 0.471670, acc.: 74.22%] [G loss: 4.664544]\n",
      "epoch:28 step:22290 [D loss: 0.064136, acc.: 98.44%] [G loss: 5.296431]\n",
      "epoch:28 step:22291 [D loss: 0.198963, acc.: 92.97%] [G loss: 3.610599]\n",
      "epoch:28 step:22292 [D loss: 0.157515, acc.: 96.09%] [G loss: 3.360271]\n",
      "epoch:28 step:22293 [D loss: 0.075473, acc.: 97.66%] [G loss: 3.742309]\n",
      "epoch:28 step:22294 [D loss: 0.039357, acc.: 100.00%] [G loss: 3.644832]\n",
      "epoch:28 step:22295 [D loss: 0.063178, acc.: 99.22%] [G loss: 3.419722]\n",
      "epoch:28 step:22296 [D loss: 0.236536, acc.: 95.31%] [G loss: 3.032805]\n",
      "epoch:28 step:22297 [D loss: 0.250467, acc.: 92.19%] [G loss: 2.693397]\n",
      "epoch:28 step:22298 [D loss: 0.528666, acc.: 76.56%] [G loss: 4.381763]\n",
      "epoch:28 step:22299 [D loss: 0.037697, acc.: 100.00%] [G loss: 4.677972]\n",
      "epoch:28 step:22300 [D loss: 0.075363, acc.: 96.88%] [G loss: 4.263782]\n",
      "epoch:28 step:22301 [D loss: 0.045819, acc.: 99.22%] [G loss: 3.872518]\n",
      "epoch:28 step:22302 [D loss: 0.172774, acc.: 94.53%] [G loss: 3.165004]\n",
      "epoch:28 step:22303 [D loss: 0.048802, acc.: 100.00%] [G loss: 4.490948]\n",
      "epoch:28 step:22304 [D loss: 0.071454, acc.: 100.00%] [G loss: 3.019609]\n",
      "epoch:28 step:22305 [D loss: 0.254551, acc.: 88.28%] [G loss: 3.531617]\n",
      "epoch:28 step:22306 [D loss: 0.090384, acc.: 98.44%] [G loss: 4.077314]\n",
      "epoch:28 step:22307 [D loss: 0.074109, acc.: 97.66%] [G loss: 3.661702]\n",
      "epoch:28 step:22308 [D loss: 0.191880, acc.: 93.75%] [G loss: 2.823146]\n",
      "epoch:28 step:22309 [D loss: 0.251238, acc.: 89.84%] [G loss: 4.647188]\n",
      "epoch:28 step:22310 [D loss: 0.106972, acc.: 96.88%] [G loss: 5.484632]\n",
      "epoch:28 step:22311 [D loss: 0.035012, acc.: 100.00%] [G loss: 4.472674]\n",
      "epoch:28 step:22312 [D loss: 0.033298, acc.: 99.22%] [G loss: 4.637527]\n",
      "epoch:28 step:22313 [D loss: 0.121334, acc.: 97.66%] [G loss: 3.328052]\n",
      "epoch:28 step:22314 [D loss: 0.198654, acc.: 92.19%] [G loss: 0.683236]\n",
      "epoch:28 step:22315 [D loss: 0.491418, acc.: 74.22%] [G loss: 7.376436]\n",
      "epoch:28 step:22316 [D loss: 1.639223, acc.: 52.34%] [G loss: 5.202997]\n",
      "epoch:28 step:22317 [D loss: 0.177090, acc.: 92.19%] [G loss: 1.992805]\n",
      "epoch:28 step:22318 [D loss: 0.137662, acc.: 95.31%] [G loss: 3.230966]\n",
      "epoch:28 step:22319 [D loss: 0.060777, acc.: 99.22%] [G loss: 3.834109]\n",
      "epoch:28 step:22320 [D loss: 0.019501, acc.: 99.22%] [G loss: 4.124065]\n",
      "epoch:28 step:22321 [D loss: 0.036785, acc.: 100.00%] [G loss: 2.667614]\n",
      "epoch:28 step:22322 [D loss: 0.012267, acc.: 100.00%] [G loss: 3.902646]\n",
      "epoch:28 step:22323 [D loss: 0.048461, acc.: 100.00%] [G loss: 3.727796]\n",
      "epoch:28 step:22324 [D loss: 0.048273, acc.: 100.00%] [G loss: 2.492807]\n",
      "epoch:28 step:22325 [D loss: 0.040908, acc.: 100.00%] [G loss: 3.405425]\n",
      "epoch:28 step:22326 [D loss: 0.086388, acc.: 98.44%] [G loss: 4.034473]\n",
      "epoch:28 step:22327 [D loss: 0.208771, acc.: 89.84%] [G loss: 4.068626]\n",
      "epoch:28 step:22328 [D loss: 0.127889, acc.: 96.88%] [G loss: 4.190362]\n",
      "epoch:28 step:22329 [D loss: 0.091816, acc.: 97.66%] [G loss: 3.676604]\n",
      "epoch:28 step:22330 [D loss: 0.116722, acc.: 96.88%] [G loss: 4.385793]\n",
      "epoch:28 step:22331 [D loss: 0.114184, acc.: 95.31%] [G loss: 4.008508]\n",
      "epoch:28 step:22332 [D loss: 0.037837, acc.: 100.00%] [G loss: 3.713453]\n",
      "epoch:28 step:22333 [D loss: 0.029849, acc.: 100.00%] [G loss: 3.537382]\n",
      "epoch:28 step:22334 [D loss: 0.039646, acc.: 100.00%] [G loss: 3.870171]\n",
      "epoch:28 step:22335 [D loss: 0.115155, acc.: 98.44%] [G loss: 3.873522]\n",
      "epoch:28 step:22336 [D loss: 0.038261, acc.: 99.22%] [G loss: 3.547087]\n",
      "epoch:28 step:22337 [D loss: 0.069989, acc.: 100.00%] [G loss: 4.023879]\n",
      "epoch:28 step:22338 [D loss: 0.020407, acc.: 100.00%] [G loss: 4.413078]\n",
      "epoch:28 step:22339 [D loss: 0.018155, acc.: 100.00%] [G loss: 4.049387]\n",
      "epoch:28 step:22340 [D loss: 0.056468, acc.: 99.22%] [G loss: 3.725312]\n",
      "epoch:28 step:22341 [D loss: 0.021134, acc.: 100.00%] [G loss: 4.319844]\n",
      "epoch:28 step:22342 [D loss: 0.055724, acc.: 97.66%] [G loss: 3.159519]\n",
      "epoch:28 step:22343 [D loss: 0.244964, acc.: 90.62%] [G loss: 5.644633]\n",
      "epoch:28 step:22344 [D loss: 0.062573, acc.: 98.44%] [G loss: 5.929351]\n",
      "epoch:28 step:22345 [D loss: 0.133314, acc.: 93.75%] [G loss: 4.223516]\n",
      "epoch:28 step:22346 [D loss: 0.088178, acc.: 96.09%] [G loss: 4.927122]\n",
      "epoch:28 step:22347 [D loss: 0.011628, acc.: 100.00%] [G loss: 4.922190]\n",
      "epoch:28 step:22348 [D loss: 0.015344, acc.: 100.00%] [G loss: 5.454284]\n",
      "epoch:28 step:22349 [D loss: 0.036111, acc.: 100.00%] [G loss: 5.494382]\n",
      "epoch:28 step:22350 [D loss: 0.092391, acc.: 97.66%] [G loss: 4.943509]\n",
      "epoch:28 step:22351 [D loss: 0.029530, acc.: 100.00%] [G loss: 4.620975]\n",
      "epoch:28 step:22352 [D loss: 0.009707, acc.: 100.00%] [G loss: 4.675992]\n",
      "epoch:28 step:22353 [D loss: 0.129991, acc.: 96.09%] [G loss: 3.773196]\n",
      "epoch:28 step:22354 [D loss: 0.016509, acc.: 100.00%] [G loss: 5.093554]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:28 step:22355 [D loss: 0.023864, acc.: 100.00%] [G loss: 4.872455]\n",
      "epoch:28 step:22356 [D loss: 0.019834, acc.: 100.00%] [G loss: 4.311138]\n",
      "epoch:28 step:22357 [D loss: 0.021548, acc.: 100.00%] [G loss: 4.109123]\n",
      "epoch:28 step:22358 [D loss: 0.006684, acc.: 100.00%] [G loss: 4.165699]\n",
      "epoch:28 step:22359 [D loss: 0.262932, acc.: 89.06%] [G loss: 4.483763]\n",
      "epoch:28 step:22360 [D loss: 0.048168, acc.: 100.00%] [G loss: 5.113603]\n",
      "epoch:28 step:22361 [D loss: 0.014118, acc.: 100.00%] [G loss: 5.147522]\n",
      "epoch:28 step:22362 [D loss: 0.009715, acc.: 100.00%] [G loss: 4.666318]\n",
      "epoch:28 step:22363 [D loss: 0.036631, acc.: 99.22%] [G loss: 3.692170]\n",
      "epoch:28 step:22364 [D loss: 0.017745, acc.: 100.00%] [G loss: 3.328509]\n",
      "epoch:28 step:22365 [D loss: 0.115229, acc.: 96.09%] [G loss: 6.430946]\n",
      "epoch:28 step:22366 [D loss: 0.475408, acc.: 80.47%] [G loss: 6.271251]\n",
      "epoch:28 step:22367 [D loss: 0.125091, acc.: 92.97%] [G loss: 4.654131]\n",
      "epoch:28 step:22368 [D loss: 0.046416, acc.: 98.44%] [G loss: 4.991324]\n",
      "epoch:28 step:22369 [D loss: 0.117401, acc.: 96.09%] [G loss: 2.439541]\n",
      "epoch:28 step:22370 [D loss: 0.171948, acc.: 93.75%] [G loss: 6.024495]\n",
      "epoch:28 step:22371 [D loss: 0.041329, acc.: 100.00%] [G loss: 6.783805]\n",
      "epoch:28 step:22372 [D loss: 0.028027, acc.: 99.22%] [G loss: 5.504195]\n",
      "epoch:28 step:22373 [D loss: 0.005840, acc.: 100.00%] [G loss: 4.423235]\n",
      "epoch:28 step:22374 [D loss: 0.007777, acc.: 100.00%] [G loss: 4.224283]\n",
      "epoch:28 step:22375 [D loss: 0.006175, acc.: 100.00%] [G loss: 3.520806]\n",
      "epoch:28 step:22376 [D loss: 0.031810, acc.: 100.00%] [G loss: 2.464583]\n",
      "epoch:28 step:22377 [D loss: 0.078984, acc.: 98.44%] [G loss: 2.270105]\n",
      "epoch:28 step:22378 [D loss: 0.008477, acc.: 100.00%] [G loss: 1.540943]\n",
      "epoch:28 step:22379 [D loss: 0.023341, acc.: 100.00%] [G loss: 0.688158]\n",
      "epoch:28 step:22380 [D loss: 0.083388, acc.: 100.00%] [G loss: 3.073967]\n",
      "epoch:28 step:22381 [D loss: 0.026466, acc.: 99.22%] [G loss: 5.797668]\n",
      "epoch:28 step:22382 [D loss: 0.027880, acc.: 100.00%] [G loss: 5.358370]\n",
      "epoch:28 step:22383 [D loss: 0.037033, acc.: 100.00%] [G loss: 2.532244]\n",
      "epoch:28 step:22384 [D loss: 0.049407, acc.: 99.22%] [G loss: 4.458879]\n",
      "epoch:28 step:22385 [D loss: 0.035038, acc.: 100.00%] [G loss: 4.948212]\n",
      "epoch:28 step:22386 [D loss: 0.032526, acc.: 98.44%] [G loss: 4.855229]\n",
      "epoch:28 step:22387 [D loss: 0.038972, acc.: 98.44%] [G loss: 4.552727]\n",
      "epoch:28 step:22388 [D loss: 0.077084, acc.: 97.66%] [G loss: 5.999327]\n",
      "epoch:28 step:22389 [D loss: 0.074912, acc.: 98.44%] [G loss: 5.643149]\n",
      "epoch:28 step:22390 [D loss: 0.011006, acc.: 100.00%] [G loss: 4.439902]\n",
      "epoch:28 step:22391 [D loss: 0.011173, acc.: 100.00%] [G loss: 4.802907]\n",
      "epoch:28 step:22392 [D loss: 0.019532, acc.: 100.00%] [G loss: 4.843503]\n",
      "epoch:28 step:22393 [D loss: 0.003329, acc.: 100.00%] [G loss: 5.120534]\n",
      "epoch:28 step:22394 [D loss: 0.015914, acc.: 100.00%] [G loss: 4.635797]\n",
      "epoch:28 step:22395 [D loss: 0.008358, acc.: 100.00%] [G loss: 4.950337]\n",
      "epoch:28 step:22396 [D loss: 0.007545, acc.: 100.00%] [G loss: 5.015220]\n",
      "epoch:28 step:22397 [D loss: 0.008492, acc.: 100.00%] [G loss: 4.660053]\n",
      "epoch:28 step:22398 [D loss: 0.016127, acc.: 100.00%] [G loss: 4.657348]\n",
      "epoch:28 step:22399 [D loss: 0.006274, acc.: 100.00%] [G loss: 5.635897]\n",
      "epoch:28 step:22400 [D loss: 0.066862, acc.: 97.66%] [G loss: 4.707923]\n",
      "##############\n",
      "[0.84171858 1.03167339 0.98224898 1.01387301 2.10790136 1.03002162\n",
      " 1.10951765 2.12262167 0.95468192 1.11044151]\n",
      "##########\n",
      "epoch:28 step:22401 [D loss: 0.623633, acc.: 70.31%] [G loss: 8.797987]\n",
      "epoch:28 step:22402 [D loss: 1.954192, acc.: 49.22%] [G loss: 3.613459]\n",
      "epoch:28 step:22403 [D loss: 0.037819, acc.: 100.00%] [G loss: 1.428469]\n",
      "epoch:28 step:22404 [D loss: 0.238482, acc.: 86.72%] [G loss: 5.644012]\n",
      "epoch:28 step:22405 [D loss: 0.113666, acc.: 95.31%] [G loss: 4.629573]\n",
      "epoch:28 step:22406 [D loss: 0.009941, acc.: 100.00%] [G loss: 4.741606]\n",
      "epoch:28 step:22407 [D loss: 0.002156, acc.: 100.00%] [G loss: 4.169979]\n",
      "epoch:28 step:22408 [D loss: 0.015254, acc.: 100.00%] [G loss: 3.129853]\n",
      "epoch:28 step:22409 [D loss: 0.011966, acc.: 100.00%] [G loss: 2.332935]\n",
      "epoch:28 step:22410 [D loss: 1.431540, acc.: 46.09%] [G loss: 8.355864]\n",
      "epoch:28 step:22411 [D loss: 1.809805, acc.: 50.78%] [G loss: 5.599454]\n",
      "epoch:28 step:22412 [D loss: 0.037671, acc.: 99.22%] [G loss: 3.496524]\n",
      "epoch:28 step:22413 [D loss: 0.060396, acc.: 99.22%] [G loss: 3.820525]\n",
      "epoch:28 step:22414 [D loss: 0.039266, acc.: 100.00%] [G loss: 3.186235]\n",
      "epoch:28 step:22415 [D loss: 0.645095, acc.: 65.62%] [G loss: 4.737482]\n",
      "epoch:28 step:22416 [D loss: 0.014689, acc.: 100.00%] [G loss: 5.850921]\n",
      "epoch:28 step:22417 [D loss: 0.224440, acc.: 89.84%] [G loss: 2.607865]\n",
      "epoch:28 step:22418 [D loss: 1.013732, acc.: 63.28%] [G loss: 6.429817]\n",
      "epoch:28 step:22419 [D loss: 1.003579, acc.: 56.25%] [G loss: 5.394721]\n",
      "epoch:28 step:22420 [D loss: 0.179882, acc.: 92.19%] [G loss: 4.151050]\n",
      "epoch:28 step:22421 [D loss: 0.015203, acc.: 100.00%] [G loss: 2.760063]\n",
      "epoch:28 step:22422 [D loss: 0.056033, acc.: 98.44%] [G loss: 3.466680]\n",
      "epoch:28 step:22423 [D loss: 0.017539, acc.: 100.00%] [G loss: 3.602503]\n",
      "epoch:28 step:22424 [D loss: 0.015146, acc.: 100.00%] [G loss: 3.878387]\n",
      "epoch:28 step:22425 [D loss: 0.027954, acc.: 100.00%] [G loss: 3.591493]\n",
      "epoch:28 step:22426 [D loss: 0.078584, acc.: 99.22%] [G loss: 3.717135]\n",
      "epoch:28 step:22427 [D loss: 0.049845, acc.: 100.00%] [G loss: 3.561484]\n",
      "epoch:28 step:22428 [D loss: 0.014564, acc.: 100.00%] [G loss: 3.808982]\n",
      "epoch:28 step:22429 [D loss: 0.022614, acc.: 100.00%] [G loss: 3.460195]\n",
      "epoch:28 step:22430 [D loss: 0.032200, acc.: 100.00%] [G loss: 3.372089]\n",
      "epoch:28 step:22431 [D loss: 0.031015, acc.: 100.00%] [G loss: 3.361652]\n",
      "epoch:28 step:22432 [D loss: 0.047609, acc.: 100.00%] [G loss: 4.308303]\n",
      "epoch:28 step:22433 [D loss: 0.204362, acc.: 91.41%] [G loss: 3.411714]\n",
      "epoch:28 step:22434 [D loss: 0.074038, acc.: 99.22%] [G loss: 3.984672]\n",
      "epoch:28 step:22435 [D loss: 0.023038, acc.: 100.00%] [G loss: 3.917830]\n",
      "epoch:28 step:22436 [D loss: 0.030598, acc.: 99.22%] [G loss: 4.186151]\n",
      "epoch:28 step:22437 [D loss: 0.092257, acc.: 99.22%] [G loss: 3.807843]\n",
      "epoch:28 step:22438 [D loss: 0.013825, acc.: 100.00%] [G loss: 4.363822]\n",
      "epoch:28 step:22439 [D loss: 0.123650, acc.: 99.22%] [G loss: 4.677467]\n",
      "epoch:28 step:22440 [D loss: 0.031650, acc.: 100.00%] [G loss: 5.126194]\n",
      "epoch:28 step:22441 [D loss: 0.115090, acc.: 96.88%] [G loss: 2.050539]\n",
      "epoch:28 step:22442 [D loss: 0.072746, acc.: 99.22%] [G loss: 2.962632]\n",
      "epoch:28 step:22443 [D loss: 0.037742, acc.: 100.00%] [G loss: 4.502888]\n",
      "epoch:28 step:22444 [D loss: 0.007852, acc.: 100.00%] [G loss: 4.309312]\n",
      "epoch:28 step:22445 [D loss: 0.218006, acc.: 94.53%] [G loss: 5.228085]\n",
      "epoch:28 step:22446 [D loss: 0.031773, acc.: 100.00%] [G loss: 5.250517]\n",
      "epoch:28 step:22447 [D loss: 0.047874, acc.: 99.22%] [G loss: 4.694102]\n",
      "epoch:28 step:22448 [D loss: 0.081112, acc.: 99.22%] [G loss: 3.619442]\n",
      "epoch:28 step:22449 [D loss: 0.038148, acc.: 100.00%] [G loss: 3.656410]\n",
      "epoch:28 step:22450 [D loss: 0.043886, acc.: 100.00%] [G loss: 5.188517]\n",
      "epoch:28 step:22451 [D loss: 0.018229, acc.: 100.00%] [G loss: 5.269907]\n",
      "epoch:28 step:22452 [D loss: 0.013514, acc.: 100.00%] [G loss: 4.348143]\n",
      "epoch:28 step:22453 [D loss: 0.010400, acc.: 100.00%] [G loss: 4.709241]\n",
      "epoch:28 step:22454 [D loss: 0.017071, acc.: 100.00%] [G loss: 4.131429]\n",
      "epoch:28 step:22455 [D loss: 0.017355, acc.: 100.00%] [G loss: 4.194576]\n",
      "epoch:28 step:22456 [D loss: 0.073126, acc.: 100.00%] [G loss: 4.791571]\n",
      "epoch:28 step:22457 [D loss: 0.035995, acc.: 100.00%] [G loss: 3.722821]\n",
      "epoch:28 step:22458 [D loss: 0.065757, acc.: 100.00%] [G loss: 5.114672]\n",
      "epoch:28 step:22459 [D loss: 0.078161, acc.: 97.66%] [G loss: 4.318899]\n",
      "epoch:28 step:22460 [D loss: 0.021077, acc.: 100.00%] [G loss: 4.465907]\n",
      "epoch:28 step:22461 [D loss: 0.020692, acc.: 100.00%] [G loss: 4.563882]\n",
      "epoch:28 step:22462 [D loss: 0.023110, acc.: 100.00%] [G loss: 4.004180]\n",
      "epoch:28 step:22463 [D loss: 0.030558, acc.: 100.00%] [G loss: 3.705059]\n",
      "epoch:28 step:22464 [D loss: 0.118257, acc.: 99.22%] [G loss: 4.851513]\n",
      "epoch:28 step:22465 [D loss: 0.014933, acc.: 100.00%] [G loss: 5.390707]\n",
      "epoch:28 step:22466 [D loss: 0.016551, acc.: 100.00%] [G loss: 5.435964]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:28 step:22467 [D loss: 0.118024, acc.: 96.88%] [G loss: 3.421039]\n",
      "epoch:28 step:22468 [D loss: 0.049432, acc.: 100.00%] [G loss: 4.215884]\n",
      "epoch:28 step:22469 [D loss: 0.006051, acc.: 100.00%] [G loss: 4.579323]\n",
      "epoch:28 step:22470 [D loss: 0.058938, acc.: 100.00%] [G loss: 4.845009]\n",
      "epoch:28 step:22471 [D loss: 0.022840, acc.: 99.22%] [G loss: 4.278359]\n",
      "epoch:28 step:22472 [D loss: 0.515525, acc.: 66.41%] [G loss: 7.294835]\n",
      "epoch:28 step:22473 [D loss: 0.841210, acc.: 60.16%] [G loss: 3.870382]\n",
      "epoch:28 step:22474 [D loss: 0.103064, acc.: 96.88%] [G loss: 3.174652]\n",
      "epoch:28 step:22475 [D loss: 0.016829, acc.: 100.00%] [G loss: 5.424145]\n",
      "epoch:28 step:22476 [D loss: 0.016216, acc.: 100.00%] [G loss: 5.159570]\n",
      "epoch:28 step:22477 [D loss: 0.069281, acc.: 97.66%] [G loss: 2.731550]\n",
      "epoch:28 step:22478 [D loss: 0.172862, acc.: 93.75%] [G loss: 5.219567]\n",
      "epoch:28 step:22479 [D loss: 0.044735, acc.: 98.44%] [G loss: 4.327885]\n",
      "epoch:28 step:22480 [D loss: 0.209345, acc.: 93.75%] [G loss: 0.666152]\n",
      "epoch:28 step:22481 [D loss: 0.781837, acc.: 63.28%] [G loss: 8.499676]\n",
      "epoch:28 step:22482 [D loss: 2.167657, acc.: 50.00%] [G loss: 5.341199]\n",
      "epoch:28 step:22483 [D loss: 0.206352, acc.: 93.75%] [G loss: 2.395306]\n",
      "epoch:28 step:22484 [D loss: 0.170791, acc.: 92.97%] [G loss: 3.463833]\n",
      "epoch:28 step:22485 [D loss: 0.078206, acc.: 100.00%] [G loss: 4.598591]\n",
      "epoch:28 step:22486 [D loss: 0.069153, acc.: 97.66%] [G loss: 2.706503]\n",
      "epoch:28 step:22487 [D loss: 0.253032, acc.: 90.62%] [G loss: 4.979993]\n",
      "epoch:28 step:22488 [D loss: 0.374550, acc.: 78.91%] [G loss: 0.916251]\n",
      "epoch:28 step:22489 [D loss: 0.442076, acc.: 74.22%] [G loss: 6.684828]\n",
      "epoch:28 step:22490 [D loss: 0.540862, acc.: 67.97%] [G loss: 4.899405]\n",
      "epoch:28 step:22491 [D loss: 0.183466, acc.: 94.53%] [G loss: 1.896651]\n",
      "epoch:28 step:22492 [D loss: 0.656007, acc.: 71.09%] [G loss: 6.831616]\n",
      "epoch:28 step:22493 [D loss: 0.087060, acc.: 96.09%] [G loss: 7.321917]\n",
      "epoch:28 step:22494 [D loss: 1.019100, acc.: 58.59%] [G loss: 4.369403]\n",
      "epoch:28 step:22495 [D loss: 0.028682, acc.: 99.22%] [G loss: 3.798341]\n",
      "epoch:28 step:22496 [D loss: 0.072275, acc.: 99.22%] [G loss: 4.074240]\n",
      "epoch:28 step:22497 [D loss: 0.013106, acc.: 100.00%] [G loss: 4.581426]\n",
      "epoch:28 step:22498 [D loss: 0.006989, acc.: 100.00%] [G loss: 4.043107]\n",
      "epoch:28 step:22499 [D loss: 0.026874, acc.: 99.22%] [G loss: 3.310686]\n",
      "epoch:28 step:22500 [D loss: 0.089218, acc.: 96.09%] [G loss: 5.338990]\n",
      "epoch:28 step:22501 [D loss: 0.006755, acc.: 100.00%] [G loss: 5.064616]\n",
      "epoch:28 step:22502 [D loss: 0.017400, acc.: 100.00%] [G loss: 4.567373]\n",
      "epoch:28 step:22503 [D loss: 0.039972, acc.: 99.22%] [G loss: 3.734345]\n",
      "epoch:28 step:22504 [D loss: 0.015599, acc.: 100.00%] [G loss: 3.207901]\n",
      "epoch:28 step:22505 [D loss: 0.119985, acc.: 94.53%] [G loss: 4.540700]\n",
      "epoch:28 step:22506 [D loss: 0.010054, acc.: 100.00%] [G loss: 5.175059]\n",
      "epoch:28 step:22507 [D loss: 0.032973, acc.: 100.00%] [G loss: 4.798553]\n",
      "epoch:28 step:22508 [D loss: 0.028782, acc.: 100.00%] [G loss: 3.576562]\n",
      "epoch:28 step:22509 [D loss: 0.014216, acc.: 100.00%] [G loss: 3.232710]\n",
      "epoch:28 step:22510 [D loss: 0.163322, acc.: 95.31%] [G loss: 4.111205]\n",
      "epoch:28 step:22511 [D loss: 0.006897, acc.: 100.00%] [G loss: 4.383849]\n",
      "epoch:28 step:22512 [D loss: 0.041509, acc.: 99.22%] [G loss: 3.871723]\n",
      "epoch:28 step:22513 [D loss: 0.035672, acc.: 100.00%] [G loss: 2.873279]\n",
      "epoch:28 step:22514 [D loss: 0.585779, acc.: 73.44%] [G loss: 6.586646]\n",
      "epoch:28 step:22515 [D loss: 0.562713, acc.: 68.75%] [G loss: 4.939449]\n",
      "epoch:28 step:22516 [D loss: 0.020446, acc.: 100.00%] [G loss: 4.436568]\n",
      "epoch:28 step:22517 [D loss: 0.020578, acc.: 100.00%] [G loss: 3.687529]\n",
      "epoch:28 step:22518 [D loss: 0.015650, acc.: 100.00%] [G loss: 4.707074]\n",
      "epoch:28 step:22519 [D loss: 0.018855, acc.: 100.00%] [G loss: 4.400341]\n",
      "epoch:28 step:22520 [D loss: 0.033956, acc.: 100.00%] [G loss: 3.653229]\n",
      "epoch:28 step:22521 [D loss: 0.018371, acc.: 100.00%] [G loss: 4.411090]\n",
      "epoch:28 step:22522 [D loss: 0.022321, acc.: 100.00%] [G loss: 4.622970]\n",
      "epoch:28 step:22523 [D loss: 0.009288, acc.: 100.00%] [G loss: 4.635663]\n",
      "epoch:28 step:22524 [D loss: 0.008216, acc.: 100.00%] [G loss: 4.298358]\n",
      "epoch:28 step:22525 [D loss: 0.020840, acc.: 100.00%] [G loss: 4.187010]\n",
      "epoch:28 step:22526 [D loss: 0.024272, acc.: 100.00%] [G loss: 3.540022]\n",
      "epoch:28 step:22527 [D loss: 0.086177, acc.: 97.66%] [G loss: 4.615227]\n",
      "epoch:28 step:22528 [D loss: 0.012372, acc.: 100.00%] [G loss: 4.517557]\n",
      "epoch:28 step:22529 [D loss: 0.058849, acc.: 100.00%] [G loss: 3.887362]\n",
      "epoch:28 step:22530 [D loss: 0.018849, acc.: 100.00%] [G loss: 4.057938]\n",
      "epoch:28 step:22531 [D loss: 0.032716, acc.: 99.22%] [G loss: 4.510197]\n",
      "epoch:28 step:22532 [D loss: 0.023505, acc.: 100.00%] [G loss: 4.532311]\n",
      "epoch:28 step:22533 [D loss: 0.121130, acc.: 96.88%] [G loss: 3.183102]\n",
      "epoch:28 step:22534 [D loss: 0.018986, acc.: 100.00%] [G loss: 3.471745]\n",
      "epoch:28 step:22535 [D loss: 0.014492, acc.: 100.00%] [G loss: 4.363396]\n",
      "epoch:28 step:22536 [D loss: 0.022627, acc.: 100.00%] [G loss: 4.057549]\n",
      "epoch:28 step:22537 [D loss: 0.011447, acc.: 100.00%] [G loss: 4.358043]\n",
      "epoch:28 step:22538 [D loss: 0.041194, acc.: 99.22%] [G loss: 3.869407]\n",
      "epoch:28 step:22539 [D loss: 0.039317, acc.: 98.44%] [G loss: 4.590301]\n",
      "epoch:28 step:22540 [D loss: 0.036772, acc.: 99.22%] [G loss: 4.643640]\n",
      "epoch:28 step:22541 [D loss: 0.022145, acc.: 100.00%] [G loss: 3.883451]\n",
      "epoch:28 step:22542 [D loss: 0.020662, acc.: 100.00%] [G loss: 4.560700]\n",
      "epoch:28 step:22543 [D loss: 0.009880, acc.: 100.00%] [G loss: 3.769611]\n",
      "epoch:28 step:22544 [D loss: 0.027161, acc.: 100.00%] [G loss: 3.295516]\n",
      "epoch:28 step:22545 [D loss: 0.052923, acc.: 100.00%] [G loss: 4.697565]\n",
      "epoch:28 step:22546 [D loss: 0.030916, acc.: 100.00%] [G loss: 3.535293]\n",
      "epoch:28 step:22547 [D loss: 0.050680, acc.: 100.00%] [G loss: 4.510804]\n",
      "epoch:28 step:22548 [D loss: 0.011031, acc.: 100.00%] [G loss: 4.907752]\n",
      "epoch:28 step:22549 [D loss: 0.066144, acc.: 99.22%] [G loss: 2.731452]\n",
      "epoch:28 step:22550 [D loss: 0.049234, acc.: 100.00%] [G loss: 3.288862]\n",
      "epoch:28 step:22551 [D loss: 0.034004, acc.: 100.00%] [G loss: 4.488246]\n",
      "epoch:28 step:22552 [D loss: 0.010601, acc.: 100.00%] [G loss: 4.676966]\n",
      "epoch:28 step:22553 [D loss: 0.640514, acc.: 66.41%] [G loss: 5.387349]\n",
      "epoch:28 step:22554 [D loss: 0.003475, acc.: 100.00%] [G loss: 6.699094]\n",
      "epoch:28 step:22555 [D loss: 0.647085, acc.: 73.44%] [G loss: 2.344316]\n",
      "epoch:28 step:22556 [D loss: 0.764503, acc.: 67.19%] [G loss: 7.127754]\n",
      "epoch:28 step:22557 [D loss: 1.214142, acc.: 56.25%] [G loss: 4.601378]\n",
      "epoch:28 step:22558 [D loss: 0.141896, acc.: 95.31%] [G loss: 3.818599]\n",
      "epoch:28 step:22559 [D loss: 0.005770, acc.: 100.00%] [G loss: 4.970805]\n",
      "epoch:28 step:22560 [D loss: 0.023745, acc.: 99.22%] [G loss: 4.287588]\n",
      "epoch:28 step:22561 [D loss: 0.052897, acc.: 98.44%] [G loss: 4.421564]\n",
      "epoch:28 step:22562 [D loss: 0.006898, acc.: 100.00%] [G loss: 3.667571]\n",
      "epoch:28 step:22563 [D loss: 0.021625, acc.: 100.00%] [G loss: 4.379376]\n",
      "epoch:28 step:22564 [D loss: 0.071360, acc.: 99.22%] [G loss: 1.750003]\n",
      "epoch:28 step:22565 [D loss: 0.153007, acc.: 95.31%] [G loss: 5.199785]\n",
      "epoch:28 step:22566 [D loss: 0.095479, acc.: 97.66%] [G loss: 3.711570]\n",
      "epoch:28 step:22567 [D loss: 0.015427, acc.: 100.00%] [G loss: 3.671932]\n",
      "epoch:28 step:22568 [D loss: 0.185415, acc.: 95.31%] [G loss: 4.069685]\n",
      "epoch:28 step:22569 [D loss: 0.008593, acc.: 100.00%] [G loss: 5.697201]\n",
      "epoch:28 step:22570 [D loss: 0.065565, acc.: 98.44%] [G loss: 2.296503]\n",
      "epoch:28 step:22571 [D loss: 0.023953, acc.: 100.00%] [G loss: 2.048778]\n",
      "epoch:28 step:22572 [D loss: 0.179085, acc.: 91.41%] [G loss: 4.732658]\n",
      "epoch:28 step:22573 [D loss: 0.052407, acc.: 99.22%] [G loss: 5.701261]\n",
      "epoch:28 step:22574 [D loss: 0.004604, acc.: 100.00%] [G loss: 4.859215]\n",
      "epoch:28 step:22575 [D loss: 0.110549, acc.: 95.31%] [G loss: 1.422616]\n",
      "epoch:28 step:22576 [D loss: 0.107810, acc.: 97.66%] [G loss: 4.462379]\n",
      "epoch:28 step:22577 [D loss: 0.004178, acc.: 100.00%] [G loss: 5.698286]\n",
      "epoch:28 step:22578 [D loss: 0.042752, acc.: 98.44%] [G loss: 5.166050]\n",
      "epoch:28 step:22579 [D loss: 0.038670, acc.: 100.00%] [G loss: 3.284772]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:28 step:22580 [D loss: 0.012828, acc.: 100.00%] [G loss: 4.722197]\n",
      "epoch:28 step:22581 [D loss: 0.926533, acc.: 56.25%] [G loss: 5.363132]\n",
      "epoch:28 step:22582 [D loss: 0.009658, acc.: 100.00%] [G loss: 6.384054]\n",
      "epoch:28 step:22583 [D loss: 0.444009, acc.: 80.47%] [G loss: 3.022048]\n",
      "epoch:28 step:22584 [D loss: 0.428076, acc.: 79.69%] [G loss: 5.579247]\n",
      "epoch:28 step:22585 [D loss: 0.001886, acc.: 100.00%] [G loss: 7.046705]\n",
      "epoch:28 step:22586 [D loss: 0.660674, acc.: 69.53%] [G loss: 2.625767]\n",
      "epoch:28 step:22587 [D loss: 0.769133, acc.: 67.19%] [G loss: 6.973957]\n",
      "epoch:28 step:22588 [D loss: 0.364350, acc.: 85.16%] [G loss: 6.888125]\n",
      "epoch:28 step:22589 [D loss: 0.314995, acc.: 84.38%] [G loss: 4.817492]\n",
      "epoch:28 step:22590 [D loss: 0.022482, acc.: 100.00%] [G loss: 3.471575]\n",
      "epoch:28 step:22591 [D loss: 0.077507, acc.: 97.66%] [G loss: 4.379846]\n",
      "epoch:28 step:22592 [D loss: 0.010680, acc.: 100.00%] [G loss: 4.430300]\n",
      "epoch:28 step:22593 [D loss: 0.036776, acc.: 100.00%] [G loss: 4.300213]\n",
      "epoch:28 step:22594 [D loss: 0.016012, acc.: 100.00%] [G loss: 5.218711]\n",
      "epoch:28 step:22595 [D loss: 0.024700, acc.: 100.00%] [G loss: 4.304768]\n",
      "epoch:28 step:22596 [D loss: 0.013802, acc.: 100.00%] [G loss: 5.312831]\n",
      "epoch:28 step:22597 [D loss: 0.012292, acc.: 100.00%] [G loss: 4.310622]\n",
      "epoch:28 step:22598 [D loss: 0.097024, acc.: 97.66%] [G loss: 4.622642]\n",
      "epoch:28 step:22599 [D loss: 0.056400, acc.: 98.44%] [G loss: 4.161297]\n",
      "epoch:28 step:22600 [D loss: 0.039173, acc.: 100.00%] [G loss: 4.730120]\n",
      "##############\n",
      "[1.10407254 0.96806398 0.68670792 0.97660354 2.1158168  0.98860969\n",
      " 2.11168445 0.91495808 0.91894066 0.98860663]\n",
      "##########\n",
      "epoch:28 step:22601 [D loss: 0.010823, acc.: 100.00%] [G loss: 4.646502]\n",
      "epoch:28 step:22602 [D loss: 0.036581, acc.: 100.00%] [G loss: 4.192811]\n",
      "epoch:28 step:22603 [D loss: 0.030073, acc.: 100.00%] [G loss: 3.385492]\n",
      "epoch:28 step:22604 [D loss: 0.011518, acc.: 100.00%] [G loss: 4.053220]\n",
      "epoch:28 step:22605 [D loss: 0.071237, acc.: 99.22%] [G loss: 3.837125]\n",
      "epoch:28 step:22606 [D loss: 0.010694, acc.: 100.00%] [G loss: 4.721363]\n",
      "epoch:28 step:22607 [D loss: 0.235787, acc.: 91.41%] [G loss: 4.871840]\n",
      "epoch:28 step:22608 [D loss: 0.034050, acc.: 100.00%] [G loss: 4.642665]\n",
      "epoch:28 step:22609 [D loss: 0.289302, acc.: 87.50%] [G loss: 4.022507]\n",
      "epoch:28 step:22610 [D loss: 0.008324, acc.: 100.00%] [G loss: 4.409954]\n",
      "epoch:28 step:22611 [D loss: 0.021870, acc.: 100.00%] [G loss: 4.519789]\n",
      "epoch:28 step:22612 [D loss: 0.015544, acc.: 100.00%] [G loss: 4.870511]\n",
      "epoch:28 step:22613 [D loss: 0.023335, acc.: 99.22%] [G loss: 4.172177]\n",
      "epoch:28 step:22614 [D loss: 0.039540, acc.: 99.22%] [G loss: 4.337382]\n",
      "epoch:28 step:22615 [D loss: 0.031718, acc.: 100.00%] [G loss: 3.719778]\n",
      "epoch:28 step:22616 [D loss: 0.082578, acc.: 99.22%] [G loss: 4.966575]\n",
      "epoch:28 step:22617 [D loss: 0.009178, acc.: 100.00%] [G loss: 5.021906]\n",
      "epoch:28 step:22618 [D loss: 0.197240, acc.: 95.31%] [G loss: 4.632202]\n",
      "epoch:28 step:22619 [D loss: 0.019676, acc.: 100.00%] [G loss: 4.838683]\n",
      "epoch:28 step:22620 [D loss: 0.004840, acc.: 100.00%] [G loss: 5.711959]\n",
      "epoch:28 step:22621 [D loss: 0.139028, acc.: 93.75%] [G loss: 3.689604]\n",
      "epoch:28 step:22622 [D loss: 0.094440, acc.: 96.88%] [G loss: 4.591740]\n",
      "epoch:28 step:22623 [D loss: 0.003172, acc.: 100.00%] [G loss: 5.772806]\n",
      "epoch:28 step:22624 [D loss: 0.008986, acc.: 100.00%] [G loss: 5.802395]\n",
      "epoch:28 step:22625 [D loss: 0.022020, acc.: 100.00%] [G loss: 4.937566]\n",
      "epoch:28 step:22626 [D loss: 0.006116, acc.: 100.00%] [G loss: 5.257331]\n",
      "epoch:28 step:22627 [D loss: 0.016679, acc.: 99.22%] [G loss: 4.035393]\n",
      "epoch:28 step:22628 [D loss: 0.008849, acc.: 100.00%] [G loss: 4.746378]\n",
      "epoch:28 step:22629 [D loss: 0.009082, acc.: 100.00%] [G loss: 4.673004]\n",
      "epoch:28 step:22630 [D loss: 0.069038, acc.: 99.22%] [G loss: 4.133749]\n",
      "epoch:28 step:22631 [D loss: 0.023524, acc.: 100.00%] [G loss: 4.148955]\n",
      "epoch:28 step:22632 [D loss: 0.027647, acc.: 99.22%] [G loss: 4.609391]\n",
      "epoch:28 step:22633 [D loss: 0.004611, acc.: 100.00%] [G loss: 5.364836]\n",
      "epoch:28 step:22634 [D loss: 0.035235, acc.: 99.22%] [G loss: 4.537120]\n",
      "epoch:28 step:22635 [D loss: 0.015327, acc.: 100.00%] [G loss: 4.424021]\n",
      "epoch:28 step:22636 [D loss: 0.080034, acc.: 98.44%] [G loss: 4.058894]\n",
      "epoch:28 step:22637 [D loss: 0.006450, acc.: 100.00%] [G loss: 4.113832]\n",
      "epoch:28 step:22638 [D loss: 0.032057, acc.: 100.00%] [G loss: 3.879784]\n",
      "epoch:28 step:22639 [D loss: 0.041215, acc.: 100.00%] [G loss: 4.629980]\n",
      "epoch:28 step:22640 [D loss: 0.008021, acc.: 100.00%] [G loss: 4.350810]\n",
      "epoch:28 step:22641 [D loss: 0.012088, acc.: 100.00%] [G loss: 5.042652]\n",
      "epoch:28 step:22642 [D loss: 0.004784, acc.: 100.00%] [G loss: 4.117104]\n",
      "epoch:28 step:22643 [D loss: 0.024195, acc.: 100.00%] [G loss: 4.182185]\n",
      "epoch:28 step:22644 [D loss: 0.017245, acc.: 100.00%] [G loss: 3.730083]\n",
      "epoch:28 step:22645 [D loss: 0.018797, acc.: 100.00%] [G loss: 4.634988]\n",
      "epoch:28 step:22646 [D loss: 0.011969, acc.: 100.00%] [G loss: 4.884358]\n",
      "epoch:28 step:22647 [D loss: 0.925424, acc.: 57.81%] [G loss: 7.494763]\n",
      "epoch:28 step:22648 [D loss: 2.626341, acc.: 50.00%] [G loss: 5.168301]\n",
      "epoch:28 step:22649 [D loss: 0.503709, acc.: 75.78%] [G loss: 4.932998]\n",
      "epoch:29 step:22650 [D loss: 0.591891, acc.: 69.53%] [G loss: 0.854728]\n",
      "epoch:29 step:22651 [D loss: 0.550230, acc.: 71.88%] [G loss: 5.308561]\n",
      "epoch:29 step:22652 [D loss: 0.037696, acc.: 98.44%] [G loss: 6.165518]\n",
      "epoch:29 step:22653 [D loss: 1.349047, acc.: 53.91%] [G loss: 3.945326]\n",
      "epoch:29 step:22654 [D loss: 0.178932, acc.: 93.75%] [G loss: 3.209001]\n",
      "epoch:29 step:22655 [D loss: 0.101585, acc.: 96.09%] [G loss: 3.786550]\n",
      "epoch:29 step:22656 [D loss: 0.014215, acc.: 100.00%] [G loss: 3.991501]\n",
      "epoch:29 step:22657 [D loss: 0.033392, acc.: 100.00%] [G loss: 3.856429]\n",
      "epoch:29 step:22658 [D loss: 0.267140, acc.: 90.62%] [G loss: 3.524147]\n",
      "epoch:29 step:22659 [D loss: 0.050812, acc.: 100.00%] [G loss: 3.130225]\n",
      "epoch:29 step:22660 [D loss: 0.036640, acc.: 100.00%] [G loss: 3.143572]\n",
      "epoch:29 step:22661 [D loss: 0.122365, acc.: 96.88%] [G loss: 2.958031]\n",
      "epoch:29 step:22662 [D loss: 0.039211, acc.: 100.00%] [G loss: 3.838097]\n",
      "epoch:29 step:22663 [D loss: 0.048199, acc.: 98.44%] [G loss: 3.906756]\n",
      "epoch:29 step:22664 [D loss: 0.024910, acc.: 100.00%] [G loss: 4.210081]\n",
      "epoch:29 step:22665 [D loss: 0.178022, acc.: 92.97%] [G loss: 3.006674]\n",
      "epoch:29 step:22666 [D loss: 0.073261, acc.: 100.00%] [G loss: 4.035090]\n",
      "epoch:29 step:22667 [D loss: 0.043754, acc.: 99.22%] [G loss: 3.811759]\n",
      "epoch:29 step:22668 [D loss: 0.056961, acc.: 98.44%] [G loss: 3.868744]\n",
      "epoch:29 step:22669 [D loss: 0.102819, acc.: 100.00%] [G loss: 4.845370]\n",
      "epoch:29 step:22670 [D loss: 0.046115, acc.: 98.44%] [G loss: 4.728862]\n",
      "epoch:29 step:22671 [D loss: 0.014075, acc.: 100.00%] [G loss: 4.460325]\n",
      "epoch:29 step:22672 [D loss: 0.597623, acc.: 66.41%] [G loss: 4.919579]\n",
      "epoch:29 step:22673 [D loss: 0.128548, acc.: 94.53%] [G loss: 5.015575]\n",
      "epoch:29 step:22674 [D loss: 0.105063, acc.: 96.88%] [G loss: 2.896530]\n",
      "epoch:29 step:22675 [D loss: 0.038028, acc.: 100.00%] [G loss: 2.509369]\n",
      "epoch:29 step:22676 [D loss: 0.070465, acc.: 97.66%] [G loss: 3.410190]\n",
      "epoch:29 step:22677 [D loss: 0.028728, acc.: 100.00%] [G loss: 4.547287]\n",
      "epoch:29 step:22678 [D loss: 0.032174, acc.: 99.22%] [G loss: 4.664764]\n",
      "epoch:29 step:22679 [D loss: 0.065191, acc.: 98.44%] [G loss: 4.379672]\n",
      "epoch:29 step:22680 [D loss: 0.012347, acc.: 100.00%] [G loss: 4.061963]\n",
      "epoch:29 step:22681 [D loss: 0.147234, acc.: 96.88%] [G loss: 4.978979]\n",
      "epoch:29 step:22682 [D loss: 0.156361, acc.: 94.53%] [G loss: 4.263326]\n",
      "epoch:29 step:22683 [D loss: 0.016367, acc.: 100.00%] [G loss: 4.221539]\n",
      "epoch:29 step:22684 [D loss: 0.021896, acc.: 100.00%] [G loss: 3.100843]\n",
      "epoch:29 step:22685 [D loss: 0.104280, acc.: 97.66%] [G loss: 3.373178]\n",
      "epoch:29 step:22686 [D loss: 0.075834, acc.: 98.44%] [G loss: 4.869593]\n",
      "epoch:29 step:22687 [D loss: 0.052960, acc.: 98.44%] [G loss: 4.867369]\n",
      "epoch:29 step:22688 [D loss: 0.015659, acc.: 100.00%] [G loss: 4.904556]\n",
      "epoch:29 step:22689 [D loss: 0.017213, acc.: 100.00%] [G loss: 4.535070]\n",
      "epoch:29 step:22690 [D loss: 0.025347, acc.: 100.00%] [G loss: 4.512699]\n",
      "epoch:29 step:22691 [D loss: 0.136409, acc.: 96.88%] [G loss: 4.195306]\n",
      "epoch:29 step:22692 [D loss: 0.038201, acc.: 99.22%] [G loss: 4.424602]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:29 step:22693 [D loss: 0.043536, acc.: 100.00%] [G loss: 3.520052]\n",
      "epoch:29 step:22694 [D loss: 0.482644, acc.: 74.22%] [G loss: 6.732494]\n",
      "epoch:29 step:22695 [D loss: 1.064369, acc.: 57.03%] [G loss: 5.163025]\n",
      "epoch:29 step:22696 [D loss: 0.050241, acc.: 99.22%] [G loss: 3.687508]\n",
      "epoch:29 step:22697 [D loss: 0.102369, acc.: 97.66%] [G loss: 4.093878]\n",
      "epoch:29 step:22698 [D loss: 0.029614, acc.: 100.00%] [G loss: 5.295665]\n",
      "epoch:29 step:22699 [D loss: 0.045834, acc.: 99.22%] [G loss: 4.111749]\n",
      "epoch:29 step:22700 [D loss: 0.014801, acc.: 100.00%] [G loss: 4.292354]\n",
      "epoch:29 step:22701 [D loss: 0.017542, acc.: 100.00%] [G loss: 3.883466]\n",
      "epoch:29 step:22702 [D loss: 0.016545, acc.: 100.00%] [G loss: 3.884468]\n",
      "epoch:29 step:22703 [D loss: 0.037601, acc.: 99.22%] [G loss: 4.038688]\n",
      "epoch:29 step:22704 [D loss: 0.017584, acc.: 100.00%] [G loss: 4.214692]\n",
      "epoch:29 step:22705 [D loss: 0.077898, acc.: 96.09%] [G loss: 3.462381]\n",
      "epoch:29 step:22706 [D loss: 0.009356, acc.: 100.00%] [G loss: 3.214438]\n",
      "epoch:29 step:22707 [D loss: 0.094343, acc.: 96.09%] [G loss: 4.437985]\n",
      "epoch:29 step:22708 [D loss: 0.004841, acc.: 100.00%] [G loss: 5.192779]\n",
      "epoch:29 step:22709 [D loss: 0.004434, acc.: 100.00%] [G loss: 5.055422]\n",
      "epoch:29 step:22710 [D loss: 0.157854, acc.: 92.97%] [G loss: 2.481258]\n",
      "epoch:29 step:22711 [D loss: 0.046310, acc.: 100.00%] [G loss: 2.187466]\n",
      "epoch:29 step:22712 [D loss: 0.095145, acc.: 97.66%] [G loss: 5.236140]\n",
      "epoch:29 step:22713 [D loss: 0.007184, acc.: 100.00%] [G loss: 5.459592]\n",
      "epoch:29 step:22714 [D loss: 0.306426, acc.: 88.28%] [G loss: 2.438565]\n",
      "epoch:29 step:22715 [D loss: 0.138949, acc.: 94.53%] [G loss: 4.000995]\n",
      "epoch:29 step:22716 [D loss: 0.003786, acc.: 100.00%] [G loss: 5.296975]\n",
      "epoch:29 step:22717 [D loss: 0.005628, acc.: 100.00%] [G loss: 4.779473]\n",
      "epoch:29 step:22718 [D loss: 0.011698, acc.: 100.00%] [G loss: 5.015860]\n",
      "epoch:29 step:22719 [D loss: 0.008719, acc.: 100.00%] [G loss: 4.523376]\n",
      "epoch:29 step:22720 [D loss: 0.013364, acc.: 100.00%] [G loss: 3.854206]\n",
      "epoch:29 step:22721 [D loss: 0.014587, acc.: 100.00%] [G loss: 4.164074]\n",
      "epoch:29 step:22722 [D loss: 0.013473, acc.: 100.00%] [G loss: 3.347590]\n",
      "epoch:29 step:22723 [D loss: 0.038365, acc.: 100.00%] [G loss: 4.805859]\n",
      "epoch:29 step:22724 [D loss: 0.020725, acc.: 100.00%] [G loss: 4.951655]\n",
      "epoch:29 step:22725 [D loss: 0.037206, acc.: 100.00%] [G loss: 5.391178]\n",
      "epoch:29 step:22726 [D loss: 0.086507, acc.: 97.66%] [G loss: 3.617125]\n",
      "epoch:29 step:22727 [D loss: 0.140140, acc.: 95.31%] [G loss: 5.399163]\n",
      "epoch:29 step:22728 [D loss: 0.016769, acc.: 99.22%] [G loss: 5.904530]\n",
      "epoch:29 step:22729 [D loss: 0.107263, acc.: 96.88%] [G loss: 4.945925]\n",
      "epoch:29 step:22730 [D loss: 0.011674, acc.: 100.00%] [G loss: 4.109151]\n",
      "epoch:29 step:22731 [D loss: 0.011416, acc.: 100.00%] [G loss: 4.630940]\n",
      "epoch:29 step:22732 [D loss: 0.041243, acc.: 99.22%] [G loss: 4.278985]\n",
      "epoch:29 step:22733 [D loss: 0.007089, acc.: 100.00%] [G loss: 5.530983]\n",
      "epoch:29 step:22734 [D loss: 0.043022, acc.: 98.44%] [G loss: 4.191169]\n",
      "epoch:29 step:22735 [D loss: 0.012463, acc.: 100.00%] [G loss: 4.119212]\n",
      "epoch:29 step:22736 [D loss: 0.005386, acc.: 100.00%] [G loss: 4.418986]\n",
      "epoch:29 step:22737 [D loss: 0.008220, acc.: 100.00%] [G loss: 4.018291]\n",
      "epoch:29 step:22738 [D loss: 0.016024, acc.: 100.00%] [G loss: 4.527666]\n",
      "epoch:29 step:22739 [D loss: 0.193307, acc.: 90.62%] [G loss: 5.011827]\n",
      "epoch:29 step:22740 [D loss: 0.006596, acc.: 100.00%] [G loss: 5.779944]\n",
      "epoch:29 step:22741 [D loss: 0.002838, acc.: 100.00%] [G loss: 6.558213]\n",
      "epoch:29 step:22742 [D loss: 0.010801, acc.: 100.00%] [G loss: 5.707894]\n",
      "epoch:29 step:22743 [D loss: 0.029520, acc.: 100.00%] [G loss: 4.884440]\n",
      "epoch:29 step:22744 [D loss: 0.014085, acc.: 100.00%] [G loss: 5.441288]\n",
      "epoch:29 step:22745 [D loss: 0.010840, acc.: 100.00%] [G loss: 4.884218]\n",
      "epoch:29 step:22746 [D loss: 0.005269, acc.: 100.00%] [G loss: 5.081958]\n",
      "epoch:29 step:22747 [D loss: 0.015103, acc.: 100.00%] [G loss: 5.499067]\n",
      "epoch:29 step:22748 [D loss: 0.015915, acc.: 100.00%] [G loss: 5.096913]\n",
      "epoch:29 step:22749 [D loss: 0.016854, acc.: 99.22%] [G loss: 4.798288]\n",
      "epoch:29 step:22750 [D loss: 0.010432, acc.: 100.00%] [G loss: 5.107286]\n",
      "epoch:29 step:22751 [D loss: 0.003800, acc.: 100.00%] [G loss: 4.965395]\n",
      "epoch:29 step:22752 [D loss: 0.017461, acc.: 100.00%] [G loss: 4.287783]\n",
      "epoch:29 step:22753 [D loss: 0.012132, acc.: 100.00%] [G loss: 4.220762]\n",
      "epoch:29 step:22754 [D loss: 0.014025, acc.: 100.00%] [G loss: 3.617023]\n",
      "epoch:29 step:22755 [D loss: 0.046764, acc.: 99.22%] [G loss: 5.033056]\n",
      "epoch:29 step:22756 [D loss: 0.117487, acc.: 95.31%] [G loss: 3.012156]\n",
      "epoch:29 step:22757 [D loss: 0.047975, acc.: 100.00%] [G loss: 4.360977]\n",
      "epoch:29 step:22758 [D loss: 0.014875, acc.: 100.00%] [G loss: 4.996084]\n",
      "epoch:29 step:22759 [D loss: 0.084581, acc.: 99.22%] [G loss: 4.558533]\n",
      "epoch:29 step:22760 [D loss: 0.018131, acc.: 100.00%] [G loss: 4.833018]\n",
      "epoch:29 step:22761 [D loss: 0.003428, acc.: 100.00%] [G loss: 5.161765]\n",
      "epoch:29 step:22762 [D loss: 0.010557, acc.: 100.00%] [G loss: 4.968686]\n",
      "epoch:29 step:22763 [D loss: 0.006075, acc.: 100.00%] [G loss: 5.102297]\n",
      "epoch:29 step:22764 [D loss: 0.002234, acc.: 100.00%] [G loss: 4.963993]\n",
      "epoch:29 step:22765 [D loss: 0.136506, acc.: 97.66%] [G loss: 3.094584]\n",
      "epoch:29 step:22766 [D loss: 0.084873, acc.: 98.44%] [G loss: 4.732888]\n",
      "epoch:29 step:22767 [D loss: 0.001672, acc.: 100.00%] [G loss: 6.952929]\n",
      "epoch:29 step:22768 [D loss: 0.025837, acc.: 100.00%] [G loss: 5.611335]\n",
      "epoch:29 step:22769 [D loss: 0.001674, acc.: 100.00%] [G loss: 6.201525]\n",
      "epoch:29 step:22770 [D loss: 0.019558, acc.: 100.00%] [G loss: 4.652586]\n",
      "epoch:29 step:22771 [D loss: 0.005689, acc.: 100.00%] [G loss: 6.071954]\n",
      "epoch:29 step:22772 [D loss: 0.009629, acc.: 100.00%] [G loss: 4.647990]\n",
      "epoch:29 step:22773 [D loss: 0.004680, acc.: 100.00%] [G loss: 5.573162]\n",
      "epoch:29 step:22774 [D loss: 0.009525, acc.: 100.00%] [G loss: 6.300494]\n",
      "epoch:29 step:22775 [D loss: 0.016813, acc.: 100.00%] [G loss: 5.014680]\n",
      "epoch:29 step:22776 [D loss: 0.005272, acc.: 100.00%] [G loss: 4.593211]\n",
      "epoch:29 step:22777 [D loss: 0.022232, acc.: 100.00%] [G loss: 5.655379]\n",
      "epoch:29 step:22778 [D loss: 0.016123, acc.: 100.00%] [G loss: 6.054732]\n",
      "epoch:29 step:22779 [D loss: 0.015560, acc.: 100.00%] [G loss: 6.012335]\n",
      "epoch:29 step:22780 [D loss: 0.006930, acc.: 100.00%] [G loss: 5.485198]\n",
      "epoch:29 step:22781 [D loss: 0.199158, acc.: 94.53%] [G loss: 7.076964]\n",
      "epoch:29 step:22782 [D loss: 0.009375, acc.: 100.00%] [G loss: 7.937257]\n",
      "epoch:29 step:22783 [D loss: 5.093421, acc.: 14.06%] [G loss: 8.345391]\n",
      "epoch:29 step:22784 [D loss: 3.156871, acc.: 50.00%] [G loss: 5.990866]\n",
      "epoch:29 step:22785 [D loss: 2.314749, acc.: 50.00%] [G loss: 3.864100]\n",
      "epoch:29 step:22786 [D loss: 1.155846, acc.: 52.34%] [G loss: 1.541990]\n",
      "epoch:29 step:22787 [D loss: 0.514776, acc.: 69.53%] [G loss: 1.728684]\n",
      "epoch:29 step:22788 [D loss: 0.179641, acc.: 95.31%] [G loss: 1.898427]\n",
      "epoch:29 step:22789 [D loss: 0.304205, acc.: 88.28%] [G loss: 1.945984]\n",
      "epoch:29 step:22790 [D loss: 0.371797, acc.: 85.16%] [G loss: 2.458321]\n",
      "epoch:29 step:22791 [D loss: 0.276061, acc.: 89.84%] [G loss: 1.719179]\n",
      "epoch:29 step:22792 [D loss: 0.330572, acc.: 86.72%] [G loss: 2.468067]\n",
      "epoch:29 step:22793 [D loss: 0.123813, acc.: 98.44%] [G loss: 2.602270]\n",
      "epoch:29 step:22794 [D loss: 0.224959, acc.: 93.75%] [G loss: 2.753795]\n",
      "epoch:29 step:22795 [D loss: 0.063416, acc.: 99.22%] [G loss: 2.703716]\n",
      "epoch:29 step:22796 [D loss: 0.177027, acc.: 95.31%] [G loss: 3.073464]\n",
      "epoch:29 step:22797 [D loss: 0.040281, acc.: 100.00%] [G loss: 3.054486]\n",
      "epoch:29 step:22798 [D loss: 0.055938, acc.: 100.00%] [G loss: 3.479984]\n",
      "epoch:29 step:22799 [D loss: 0.353951, acc.: 85.94%] [G loss: 2.261353]\n",
      "epoch:29 step:22800 [D loss: 0.183288, acc.: 95.31%] [G loss: 3.234628]\n",
      "##############\n",
      "[1.00037921 1.00647053 0.79954973 0.99385742 2.10624701 1.12248275\n",
      " 0.9794968  1.12258609 2.10991815 0.96837952]\n",
      "##########\n",
      "epoch:29 step:22801 [D loss: 0.131546, acc.: 97.66%] [G loss: 3.649468]\n",
      "epoch:29 step:22802 [D loss: 0.149390, acc.: 96.09%] [G loss: 2.500678]\n",
      "epoch:29 step:22803 [D loss: 0.058027, acc.: 100.00%] [G loss: 2.020943]\n",
      "epoch:29 step:22804 [D loss: 0.441871, acc.: 78.91%] [G loss: 3.993183]\n",
      "epoch:29 step:22805 [D loss: 0.687588, acc.: 66.41%] [G loss: 2.114732]\n",
      "epoch:29 step:22806 [D loss: 0.129101, acc.: 98.44%] [G loss: 2.466012]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:29 step:22807 [D loss: 0.220423, acc.: 90.62%] [G loss: 3.302132]\n",
      "epoch:29 step:22808 [D loss: 0.132938, acc.: 96.88%] [G loss: 3.457458]\n",
      "epoch:29 step:22809 [D loss: 0.427285, acc.: 79.69%] [G loss: 3.922066]\n",
      "epoch:29 step:22810 [D loss: 0.320833, acc.: 85.16%] [G loss: 3.081861]\n",
      "epoch:29 step:22811 [D loss: 0.029627, acc.: 100.00%] [G loss: 3.566758]\n",
      "epoch:29 step:22812 [D loss: 0.164968, acc.: 94.53%] [G loss: 3.043505]\n",
      "epoch:29 step:22813 [D loss: 0.071347, acc.: 99.22%] [G loss: 3.153041]\n",
      "epoch:29 step:22814 [D loss: 0.093702, acc.: 98.44%] [G loss: 2.587763]\n",
      "epoch:29 step:22815 [D loss: 0.108245, acc.: 97.66%] [G loss: 3.426937]\n",
      "epoch:29 step:22816 [D loss: 0.133266, acc.: 96.09%] [G loss: 3.582238]\n",
      "epoch:29 step:22817 [D loss: 0.113489, acc.: 97.66%] [G loss: 2.620797]\n",
      "epoch:29 step:22818 [D loss: 0.020504, acc.: 100.00%] [G loss: 2.501155]\n",
      "epoch:29 step:22819 [D loss: 0.143144, acc.: 97.66%] [G loss: 2.284029]\n",
      "epoch:29 step:22820 [D loss: 0.044027, acc.: 100.00%] [G loss: 2.943390]\n",
      "epoch:29 step:22821 [D loss: 0.061094, acc.: 98.44%] [G loss: 2.472028]\n",
      "epoch:29 step:22822 [D loss: 0.101903, acc.: 99.22%] [G loss: 3.330389]\n",
      "epoch:29 step:22823 [D loss: 0.095731, acc.: 98.44%] [G loss: 3.428768]\n",
      "epoch:29 step:22824 [D loss: 0.089223, acc.: 98.44%] [G loss: 3.696800]\n",
      "epoch:29 step:22825 [D loss: 0.119529, acc.: 96.09%] [G loss: 2.669967]\n",
      "epoch:29 step:22826 [D loss: 0.073207, acc.: 99.22%] [G loss: 2.644446]\n",
      "epoch:29 step:22827 [D loss: 0.103057, acc.: 97.66%] [G loss: 2.601161]\n",
      "epoch:29 step:22828 [D loss: 0.498083, acc.: 75.00%] [G loss: 5.691871]\n",
      "epoch:29 step:22829 [D loss: 0.919587, acc.: 58.59%] [G loss: 3.116069]\n",
      "epoch:29 step:22830 [D loss: 0.044463, acc.: 99.22%] [G loss: 1.345152]\n",
      "epoch:29 step:22831 [D loss: 0.257335, acc.: 87.50%] [G loss: 5.194303]\n",
      "epoch:29 step:22832 [D loss: 0.111872, acc.: 95.31%] [G loss: 5.554375]\n",
      "epoch:29 step:22833 [D loss: 0.199406, acc.: 92.19%] [G loss: 3.597349]\n",
      "epoch:29 step:22834 [D loss: 0.065353, acc.: 99.22%] [G loss: 4.367742]\n",
      "epoch:29 step:22835 [D loss: 0.029392, acc.: 100.00%] [G loss: 4.422733]\n",
      "epoch:29 step:22836 [D loss: 0.043743, acc.: 100.00%] [G loss: 3.835975]\n",
      "epoch:29 step:22837 [D loss: 0.020806, acc.: 100.00%] [G loss: 3.329112]\n",
      "epoch:29 step:22838 [D loss: 0.029007, acc.: 100.00%] [G loss: 3.391343]\n",
      "epoch:29 step:22839 [D loss: 0.040710, acc.: 100.00%] [G loss: 3.792916]\n",
      "epoch:29 step:22840 [D loss: 0.111091, acc.: 97.66%] [G loss: 3.805303]\n",
      "epoch:29 step:22841 [D loss: 0.011908, acc.: 100.00%] [G loss: 4.582579]\n",
      "epoch:29 step:22842 [D loss: 0.116454, acc.: 96.88%] [G loss: 2.738956]\n",
      "epoch:29 step:22843 [D loss: 0.042756, acc.: 100.00%] [G loss: 2.725910]\n",
      "epoch:29 step:22844 [D loss: 0.034901, acc.: 100.00%] [G loss: 3.087269]\n",
      "epoch:29 step:22845 [D loss: 0.067758, acc.: 100.00%] [G loss: 3.044297]\n",
      "epoch:29 step:22846 [D loss: 0.036196, acc.: 100.00%] [G loss: 3.167365]\n",
      "epoch:29 step:22847 [D loss: 0.047722, acc.: 100.00%] [G loss: 2.872881]\n",
      "epoch:29 step:22848 [D loss: 0.261307, acc.: 89.84%] [G loss: 4.687520]\n",
      "epoch:29 step:22849 [D loss: 0.733750, acc.: 65.62%] [G loss: 5.181408]\n",
      "epoch:29 step:22850 [D loss: 0.030238, acc.: 99.22%] [G loss: 5.986777]\n",
      "epoch:29 step:22851 [D loss: 0.090804, acc.: 96.88%] [G loss: 4.874940]\n",
      "epoch:29 step:22852 [D loss: 0.005423, acc.: 100.00%] [G loss: 4.765082]\n",
      "epoch:29 step:22853 [D loss: 0.031275, acc.: 99.22%] [G loss: 4.800555]\n",
      "epoch:29 step:22854 [D loss: 0.012082, acc.: 100.00%] [G loss: 4.800944]\n",
      "epoch:29 step:22855 [D loss: 0.003866, acc.: 100.00%] [G loss: 4.563593]\n",
      "epoch:29 step:22856 [D loss: 0.008449, acc.: 100.00%] [G loss: 4.018138]\n",
      "epoch:29 step:22857 [D loss: 0.012324, acc.: 100.00%] [G loss: 3.768187]\n",
      "epoch:29 step:22858 [D loss: 0.072747, acc.: 98.44%] [G loss: 3.827175]\n",
      "epoch:29 step:22859 [D loss: 0.019143, acc.: 100.00%] [G loss: 4.370455]\n",
      "epoch:29 step:22860 [D loss: 0.011615, acc.: 100.00%] [G loss: 4.491251]\n",
      "epoch:29 step:22861 [D loss: 0.107119, acc.: 97.66%] [G loss: 3.583409]\n",
      "epoch:29 step:22862 [D loss: 0.018547, acc.: 100.00%] [G loss: 3.600558]\n",
      "epoch:29 step:22863 [D loss: 0.994775, acc.: 56.25%] [G loss: 6.757294]\n",
      "epoch:29 step:22864 [D loss: 1.568430, acc.: 51.56%] [G loss: 4.335824]\n",
      "epoch:29 step:22865 [D loss: 0.191921, acc.: 93.75%] [G loss: 2.957663]\n",
      "epoch:29 step:22866 [D loss: 0.071217, acc.: 97.66%] [G loss: 1.888568]\n",
      "epoch:29 step:22867 [D loss: 0.217896, acc.: 88.28%] [G loss: 3.483375]\n",
      "epoch:29 step:22868 [D loss: 0.026666, acc.: 100.00%] [G loss: 3.943848]\n",
      "epoch:29 step:22869 [D loss: 0.201275, acc.: 92.97%] [G loss: 3.113779]\n",
      "epoch:29 step:22870 [D loss: 0.087368, acc.: 96.09%] [G loss: 2.992836]\n",
      "epoch:29 step:22871 [D loss: 0.019456, acc.: 100.00%] [G loss: 3.694706]\n",
      "epoch:29 step:22872 [D loss: 0.060395, acc.: 99.22%] [G loss: 3.141778]\n",
      "epoch:29 step:22873 [D loss: 0.085392, acc.: 97.66%] [G loss: 3.297864]\n",
      "epoch:29 step:22874 [D loss: 0.012402, acc.: 100.00%] [G loss: 4.403616]\n",
      "epoch:29 step:22875 [D loss: 0.035747, acc.: 100.00%] [G loss: 4.036933]\n",
      "epoch:29 step:22876 [D loss: 0.081602, acc.: 100.00%] [G loss: 3.951233]\n",
      "epoch:29 step:22877 [D loss: 0.027176, acc.: 100.00%] [G loss: 3.746394]\n",
      "epoch:29 step:22878 [D loss: 0.028255, acc.: 100.00%] [G loss: 4.044598]\n",
      "epoch:29 step:22879 [D loss: 0.019330, acc.: 100.00%] [G loss: 4.201141]\n",
      "epoch:29 step:22880 [D loss: 0.031551, acc.: 100.00%] [G loss: 3.994470]\n",
      "epoch:29 step:22881 [D loss: 0.079731, acc.: 100.00%] [G loss: 4.157677]\n",
      "epoch:29 step:22882 [D loss: 0.025490, acc.: 100.00%] [G loss: 3.859324]\n",
      "epoch:29 step:22883 [D loss: 0.508792, acc.: 75.78%] [G loss: 6.362518]\n",
      "epoch:29 step:22884 [D loss: 0.368181, acc.: 78.91%] [G loss: 6.419282]\n",
      "epoch:29 step:22885 [D loss: 0.145099, acc.: 92.97%] [G loss: 3.801037]\n",
      "epoch:29 step:22886 [D loss: 0.029785, acc.: 100.00%] [G loss: 3.964433]\n",
      "epoch:29 step:22887 [D loss: 0.058902, acc.: 98.44%] [G loss: 3.205678]\n",
      "epoch:29 step:22888 [D loss: 0.026515, acc.: 100.00%] [G loss: 3.719345]\n",
      "epoch:29 step:22889 [D loss: 0.082854, acc.: 96.88%] [G loss: 1.792278]\n",
      "epoch:29 step:22890 [D loss: 0.015227, acc.: 100.00%] [G loss: 3.656304]\n",
      "epoch:29 step:22891 [D loss: 0.025314, acc.: 100.00%] [G loss: 1.088486]\n",
      "epoch:29 step:22892 [D loss: 0.090211, acc.: 98.44%] [G loss: 2.442443]\n",
      "epoch:29 step:22893 [D loss: 0.007274, acc.: 100.00%] [G loss: 4.080764]\n",
      "epoch:29 step:22894 [D loss: 0.018001, acc.: 100.00%] [G loss: 4.045520]\n",
      "epoch:29 step:22895 [D loss: 0.209668, acc.: 93.75%] [G loss: 2.718559]\n",
      "epoch:29 step:22896 [D loss: 0.021683, acc.: 100.00%] [G loss: 1.691988]\n",
      "epoch:29 step:22897 [D loss: 0.014505, acc.: 100.00%] [G loss: 1.846126]\n",
      "epoch:29 step:22898 [D loss: 0.050281, acc.: 99.22%] [G loss: 2.361675]\n",
      "epoch:29 step:22899 [D loss: 0.028316, acc.: 100.00%] [G loss: 2.619417]\n",
      "epoch:29 step:22900 [D loss: 0.009060, acc.: 100.00%] [G loss: 2.357724]\n",
      "epoch:29 step:22901 [D loss: 0.110460, acc.: 95.31%] [G loss: 4.707163]\n",
      "epoch:29 step:22902 [D loss: 0.026000, acc.: 100.00%] [G loss: 5.288177]\n",
      "epoch:29 step:22903 [D loss: 0.009798, acc.: 100.00%] [G loss: 5.691283]\n",
      "epoch:29 step:22904 [D loss: 0.013995, acc.: 100.00%] [G loss: 4.690471]\n",
      "epoch:29 step:22905 [D loss: 0.031552, acc.: 100.00%] [G loss: 3.878129]\n",
      "epoch:29 step:22906 [D loss: 0.007029, acc.: 100.00%] [G loss: 3.754797]\n",
      "epoch:29 step:22907 [D loss: 0.013328, acc.: 100.00%] [G loss: 3.489634]\n",
      "epoch:29 step:22908 [D loss: 0.017544, acc.: 100.00%] [G loss: 3.724337]\n",
      "epoch:29 step:22909 [D loss: 0.058969, acc.: 99.22%] [G loss: 4.703128]\n",
      "epoch:29 step:22910 [D loss: 0.079740, acc.: 99.22%] [G loss: 4.086841]\n",
      "epoch:29 step:22911 [D loss: 0.038580, acc.: 100.00%] [G loss: 5.032644]\n",
      "epoch:29 step:22912 [D loss: 0.004507, acc.: 100.00%] [G loss: 5.201280]\n",
      "epoch:29 step:22913 [D loss: 0.472264, acc.: 76.56%] [G loss: 4.595026]\n",
      "epoch:29 step:22914 [D loss: 0.005209, acc.: 100.00%] [G loss: 5.399314]\n",
      "epoch:29 step:22915 [D loss: 0.006151, acc.: 100.00%] [G loss: 5.622012]\n",
      "epoch:29 step:22916 [D loss: 0.002766, acc.: 100.00%] [G loss: 5.428499]\n",
      "epoch:29 step:22917 [D loss: 0.005525, acc.: 100.00%] [G loss: 5.592422]\n",
      "epoch:29 step:22918 [D loss: 0.018424, acc.: 100.00%] [G loss: 4.664629]\n",
      "epoch:29 step:22919 [D loss: 0.086726, acc.: 98.44%] [G loss: 4.629081]\n",
      "epoch:29 step:22920 [D loss: 0.023061, acc.: 100.00%] [G loss: 5.469240]\n",
      "epoch:29 step:22921 [D loss: 0.005936, acc.: 100.00%] [G loss: 4.552905]\n",
      "epoch:29 step:22922 [D loss: 0.009963, acc.: 100.00%] [G loss: 4.170406]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:29 step:22923 [D loss: 0.017810, acc.: 100.00%] [G loss: 4.963658]\n",
      "epoch:29 step:22924 [D loss: 0.012039, acc.: 100.00%] [G loss: 5.007822]\n",
      "epoch:29 step:22925 [D loss: 0.011787, acc.: 100.00%] [G loss: 3.568145]\n",
      "epoch:29 step:22926 [D loss: 0.035197, acc.: 100.00%] [G loss: 4.658988]\n",
      "epoch:29 step:22927 [D loss: 0.027807, acc.: 100.00%] [G loss: 5.197337]\n",
      "epoch:29 step:22928 [D loss: 0.177602, acc.: 93.75%] [G loss: 3.444210]\n",
      "epoch:29 step:22929 [D loss: 0.084520, acc.: 96.88%] [G loss: 5.259523]\n",
      "epoch:29 step:22930 [D loss: 0.005963, acc.: 100.00%] [G loss: 6.079239]\n",
      "epoch:29 step:22931 [D loss: 0.074106, acc.: 97.66%] [G loss: 4.881371]\n",
      "epoch:29 step:22932 [D loss: 0.024525, acc.: 100.00%] [G loss: 4.818178]\n",
      "epoch:29 step:22933 [D loss: 0.004840, acc.: 100.00%] [G loss: 4.715883]\n",
      "epoch:29 step:22934 [D loss: 0.018700, acc.: 100.00%] [G loss: 4.650049]\n",
      "epoch:29 step:22935 [D loss: 0.140992, acc.: 95.31%] [G loss: 3.060189]\n",
      "epoch:29 step:22936 [D loss: 0.282155, acc.: 85.16%] [G loss: 7.289742]\n",
      "epoch:29 step:22937 [D loss: 0.268328, acc.: 85.94%] [G loss: 5.716458]\n",
      "epoch:29 step:22938 [D loss: 0.020071, acc.: 100.00%] [G loss: 4.463541]\n",
      "epoch:29 step:22939 [D loss: 0.080026, acc.: 96.88%] [G loss: 5.578281]\n",
      "epoch:29 step:22940 [D loss: 0.008842, acc.: 100.00%] [G loss: 6.633678]\n",
      "epoch:29 step:22941 [D loss: 0.041015, acc.: 97.66%] [G loss: 5.496375]\n",
      "epoch:29 step:22942 [D loss: 0.007211, acc.: 100.00%] [G loss: 4.886065]\n",
      "epoch:29 step:22943 [D loss: 0.010139, acc.: 100.00%] [G loss: 4.095964]\n",
      "epoch:29 step:22944 [D loss: 0.015784, acc.: 100.00%] [G loss: 4.586520]\n",
      "epoch:29 step:22945 [D loss: 0.035460, acc.: 99.22%] [G loss: 5.288333]\n",
      "epoch:29 step:22946 [D loss: 0.013014, acc.: 100.00%] [G loss: 4.592476]\n",
      "epoch:29 step:22947 [D loss: 0.019502, acc.: 100.00%] [G loss: 4.892713]\n",
      "epoch:29 step:22948 [D loss: 0.009678, acc.: 100.00%] [G loss: 4.387149]\n",
      "epoch:29 step:22949 [D loss: 0.017572, acc.: 100.00%] [G loss: 4.662271]\n",
      "epoch:29 step:22950 [D loss: 0.002886, acc.: 100.00%] [G loss: 4.299057]\n",
      "epoch:29 step:22951 [D loss: 0.028139, acc.: 100.00%] [G loss: 4.054173]\n",
      "epoch:29 step:22952 [D loss: 0.049152, acc.: 100.00%] [G loss: 4.475244]\n",
      "epoch:29 step:22953 [D loss: 0.005661, acc.: 100.00%] [G loss: 4.359772]\n",
      "epoch:29 step:22954 [D loss: 0.334165, acc.: 86.72%] [G loss: 7.508548]\n",
      "epoch:29 step:22955 [D loss: 1.916507, acc.: 43.75%] [G loss: 1.672855]\n",
      "epoch:29 step:22956 [D loss: 1.433083, acc.: 57.03%] [G loss: 7.822899]\n",
      "epoch:29 step:22957 [D loss: 3.096235, acc.: 50.00%] [G loss: 6.557270]\n",
      "epoch:29 step:22958 [D loss: 2.465256, acc.: 50.00%] [G loss: 4.007221]\n",
      "epoch:29 step:22959 [D loss: 1.566327, acc.: 51.56%] [G loss: 2.715304]\n",
      "epoch:29 step:22960 [D loss: 0.909258, acc.: 57.03%] [G loss: 1.362310]\n",
      "epoch:29 step:22961 [D loss: 0.799147, acc.: 60.94%] [G loss: 1.726000]\n",
      "epoch:29 step:22962 [D loss: 0.513816, acc.: 76.56%] [G loss: 1.996467]\n",
      "epoch:29 step:22963 [D loss: 0.237215, acc.: 93.75%] [G loss: 2.367260]\n",
      "epoch:29 step:22964 [D loss: 0.564604, acc.: 67.97%] [G loss: 1.795276]\n",
      "epoch:29 step:22965 [D loss: 0.227563, acc.: 92.97%] [G loss: 2.116032]\n",
      "epoch:29 step:22966 [D loss: 0.178686, acc.: 99.22%] [G loss: 2.432724]\n",
      "epoch:29 step:22967 [D loss: 0.126184, acc.: 100.00%] [G loss: 2.965167]\n",
      "epoch:29 step:22968 [D loss: 0.309611, acc.: 87.50%] [G loss: 2.632688]\n",
      "epoch:29 step:22969 [D loss: 0.209105, acc.: 94.53%] [G loss: 2.684297]\n",
      "epoch:29 step:22970 [D loss: 0.148449, acc.: 99.22%] [G loss: 2.461216]\n",
      "epoch:29 step:22971 [D loss: 0.213486, acc.: 93.75%] [G loss: 2.577845]\n",
      "epoch:29 step:22972 [D loss: 0.131899, acc.: 99.22%] [G loss: 2.821283]\n",
      "epoch:29 step:22973 [D loss: 0.088261, acc.: 100.00%] [G loss: 2.952872]\n",
      "epoch:29 step:22974 [D loss: 0.245782, acc.: 91.41%] [G loss: 3.196230]\n",
      "epoch:29 step:22975 [D loss: 0.086672, acc.: 98.44%] [G loss: 3.308558]\n",
      "epoch:29 step:22976 [D loss: 0.210550, acc.: 90.62%] [G loss: 2.668349]\n",
      "epoch:29 step:22977 [D loss: 0.085633, acc.: 100.00%] [G loss: 2.959995]\n",
      "epoch:29 step:22978 [D loss: 0.053518, acc.: 100.00%] [G loss: 3.272083]\n",
      "epoch:29 step:22979 [D loss: 0.159371, acc.: 97.66%] [G loss: 2.640842]\n",
      "epoch:29 step:22980 [D loss: 0.155566, acc.: 98.44%] [G loss: 2.688231]\n",
      "epoch:29 step:22981 [D loss: 0.105472, acc.: 98.44%] [G loss: 3.170304]\n",
      "epoch:29 step:22982 [D loss: 0.164919, acc.: 92.97%] [G loss: 2.225475]\n",
      "epoch:29 step:22983 [D loss: 0.459574, acc.: 80.47%] [G loss: 4.985461]\n",
      "epoch:29 step:22984 [D loss: 0.796565, acc.: 63.28%] [G loss: 3.653958]\n",
      "epoch:29 step:22985 [D loss: 0.143671, acc.: 95.31%] [G loss: 3.393889]\n",
      "epoch:29 step:22986 [D loss: 0.107901, acc.: 96.88%] [G loss: 3.076724]\n",
      "epoch:29 step:22987 [D loss: 0.097832, acc.: 97.66%] [G loss: 3.238429]\n",
      "epoch:29 step:22988 [D loss: 0.049923, acc.: 100.00%] [G loss: 3.311651]\n",
      "epoch:29 step:22989 [D loss: 0.051800, acc.: 100.00%] [G loss: 3.440731]\n",
      "epoch:29 step:22990 [D loss: 0.031841, acc.: 100.00%] [G loss: 3.508168]\n",
      "epoch:29 step:22991 [D loss: 0.096296, acc.: 98.44%] [G loss: 2.865058]\n",
      "epoch:29 step:22992 [D loss: 0.088417, acc.: 97.66%] [G loss: 3.123273]\n",
      "epoch:29 step:22993 [D loss: 0.060563, acc.: 99.22%] [G loss: 3.683245]\n",
      "epoch:29 step:22994 [D loss: 0.049290, acc.: 100.00%] [G loss: 3.610567]\n",
      "epoch:29 step:22995 [D loss: 0.117410, acc.: 99.22%] [G loss: 3.253519]\n",
      "epoch:29 step:22996 [D loss: 0.047866, acc.: 100.00%] [G loss: 3.473521]\n",
      "epoch:29 step:22997 [D loss: 0.062714, acc.: 98.44%] [G loss: 3.342490]\n",
      "epoch:29 step:22998 [D loss: 0.218586, acc.: 92.97%] [G loss: 4.119557]\n",
      "epoch:29 step:22999 [D loss: 0.324508, acc.: 85.16%] [G loss: 2.435355]\n",
      "epoch:29 step:23000 [D loss: 0.209423, acc.: 91.41%] [G loss: 4.125891]\n",
      "##############\n",
      "[0.98961877 0.99676796 1.00368698 0.92438385 1.10625862 1.00140585\n",
      " 0.83040068 1.07682673 1.04251574 1.11376785]\n",
      "##########\n",
      "epoch:29 step:23001 [D loss: 0.060110, acc.: 98.44%] [G loss: 4.844302]\n",
      "epoch:29 step:23002 [D loss: 0.358796, acc.: 81.25%] [G loss: 2.449281]\n",
      "epoch:29 step:23003 [D loss: 0.073285, acc.: 98.44%] [G loss: 2.264805]\n",
      "epoch:29 step:23004 [D loss: 0.036643, acc.: 100.00%] [G loss: 3.299891]\n",
      "epoch:29 step:23005 [D loss: 0.023182, acc.: 100.00%] [G loss: 3.522116]\n",
      "epoch:29 step:23006 [D loss: 0.026386, acc.: 100.00%] [G loss: 3.672497]\n",
      "epoch:29 step:23007 [D loss: 0.020113, acc.: 100.00%] [G loss: 3.969624]\n",
      "epoch:29 step:23008 [D loss: 0.038130, acc.: 100.00%] [G loss: 3.662510]\n",
      "epoch:29 step:23009 [D loss: 0.020370, acc.: 100.00%] [G loss: 3.624494]\n",
      "epoch:29 step:23010 [D loss: 0.051137, acc.: 100.00%] [G loss: 3.994848]\n",
      "epoch:29 step:23011 [D loss: 0.033719, acc.: 100.00%] [G loss: 4.375468]\n",
      "epoch:29 step:23012 [D loss: 0.052047, acc.: 100.00%] [G loss: 4.257021]\n",
      "epoch:29 step:23013 [D loss: 0.156972, acc.: 97.66%] [G loss: 3.170646]\n",
      "epoch:29 step:23014 [D loss: 0.045137, acc.: 100.00%] [G loss: 3.715101]\n",
      "epoch:29 step:23015 [D loss: 0.020206, acc.: 100.00%] [G loss: 4.687676]\n",
      "epoch:29 step:23016 [D loss: 0.016894, acc.: 100.00%] [G loss: 4.653373]\n",
      "epoch:29 step:23017 [D loss: 0.010268, acc.: 100.00%] [G loss: 5.141803]\n",
      "epoch:29 step:23018 [D loss: 0.024477, acc.: 99.22%] [G loss: 4.517163]\n",
      "epoch:29 step:23019 [D loss: 0.035766, acc.: 100.00%] [G loss: 4.345612]\n",
      "epoch:29 step:23020 [D loss: 0.033876, acc.: 100.00%] [G loss: 3.684858]\n",
      "epoch:29 step:23021 [D loss: 0.012767, acc.: 100.00%] [G loss: 4.252377]\n",
      "epoch:29 step:23022 [D loss: 0.021348, acc.: 100.00%] [G loss: 4.492167]\n",
      "epoch:29 step:23023 [D loss: 0.037942, acc.: 100.00%] [G loss: 4.238017]\n",
      "epoch:29 step:23024 [D loss: 0.078572, acc.: 97.66%] [G loss: 4.250617]\n",
      "epoch:29 step:23025 [D loss: 0.029906, acc.: 100.00%] [G loss: 4.201994]\n",
      "epoch:29 step:23026 [D loss: 0.016055, acc.: 100.00%] [G loss: 3.902729]\n",
      "epoch:29 step:23027 [D loss: 0.010308, acc.: 100.00%] [G loss: 4.322949]\n",
      "epoch:29 step:23028 [D loss: 0.027022, acc.: 100.00%] [G loss: 4.686026]\n",
      "epoch:29 step:23029 [D loss: 0.013531, acc.: 100.00%] [G loss: 5.142695]\n",
      "epoch:29 step:23030 [D loss: 0.012885, acc.: 99.22%] [G loss: 4.575696]\n",
      "epoch:29 step:23031 [D loss: 0.458930, acc.: 78.12%] [G loss: 5.739054]\n",
      "epoch:29 step:23032 [D loss: 0.002704, acc.: 100.00%] [G loss: 7.203409]\n",
      "epoch:29 step:23033 [D loss: 0.098267, acc.: 97.66%] [G loss: 6.150803]\n",
      "epoch:29 step:23034 [D loss: 0.073108, acc.: 98.44%] [G loss: 4.519094]\n",
      "epoch:29 step:23035 [D loss: 0.152820, acc.: 93.75%] [G loss: 6.625636]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:29 step:23036 [D loss: 0.001034, acc.: 100.00%] [G loss: 7.622493]\n",
      "epoch:29 step:23037 [D loss: 0.128777, acc.: 95.31%] [G loss: 5.701517]\n",
      "epoch:29 step:23038 [D loss: 0.026422, acc.: 99.22%] [G loss: 5.207627]\n",
      "epoch:29 step:23039 [D loss: 0.003861, acc.: 100.00%] [G loss: 5.094466]\n",
      "epoch:29 step:23040 [D loss: 0.011153, acc.: 100.00%] [G loss: 4.973671]\n",
      "epoch:29 step:23041 [D loss: 0.009480, acc.: 100.00%] [G loss: 4.963751]\n",
      "epoch:29 step:23042 [D loss: 0.004914, acc.: 100.00%] [G loss: 4.899068]\n",
      "epoch:29 step:23043 [D loss: 0.012861, acc.: 100.00%] [G loss: 5.138627]\n",
      "epoch:29 step:23044 [D loss: 0.023563, acc.: 99.22%] [G loss: 5.252221]\n",
      "epoch:29 step:23045 [D loss: 0.018154, acc.: 100.00%] [G loss: 4.723493]\n",
      "epoch:29 step:23046 [D loss: 0.026852, acc.: 100.00%] [G loss: 4.577758]\n",
      "epoch:29 step:23047 [D loss: 0.015483, acc.: 100.00%] [G loss: 5.040370]\n",
      "epoch:29 step:23048 [D loss: 0.006229, acc.: 100.00%] [G loss: 6.241203]\n",
      "epoch:29 step:23049 [D loss: 0.016035, acc.: 99.22%] [G loss: 4.055247]\n",
      "epoch:29 step:23050 [D loss: 0.011670, acc.: 100.00%] [G loss: 5.049303]\n",
      "epoch:29 step:23051 [D loss: 0.039444, acc.: 99.22%] [G loss: 6.406325]\n",
      "epoch:29 step:23052 [D loss: 2.951943, acc.: 28.91%] [G loss: 7.622506]\n",
      "epoch:29 step:23053 [D loss: 1.705653, acc.: 50.78%] [G loss: 2.910749]\n",
      "epoch:29 step:23054 [D loss: 0.265316, acc.: 87.50%] [G loss: 2.590512]\n",
      "epoch:29 step:23055 [D loss: 0.242746, acc.: 89.06%] [G loss: 4.687802]\n",
      "epoch:29 step:23056 [D loss: 0.073671, acc.: 97.66%] [G loss: 3.874184]\n",
      "epoch:29 step:23057 [D loss: 0.309293, acc.: 88.28%] [G loss: 2.792191]\n",
      "epoch:29 step:23058 [D loss: 0.960127, acc.: 55.47%] [G loss: 5.726076]\n",
      "epoch:29 step:23059 [D loss: 0.416970, acc.: 75.78%] [G loss: 5.104104]\n",
      "epoch:29 step:23060 [D loss: 0.202691, acc.: 92.97%] [G loss: 3.512450]\n",
      "epoch:29 step:23061 [D loss: 0.192799, acc.: 92.97%] [G loss: 3.667753]\n",
      "epoch:29 step:23062 [D loss: 0.174876, acc.: 92.97%] [G loss: 3.911003]\n",
      "epoch:29 step:23063 [D loss: 0.137715, acc.: 95.31%] [G loss: 3.930636]\n",
      "epoch:29 step:23064 [D loss: 0.059296, acc.: 99.22%] [G loss: 3.713960]\n",
      "epoch:29 step:23065 [D loss: 0.048485, acc.: 100.00%] [G loss: 3.617791]\n",
      "epoch:29 step:23066 [D loss: 0.250769, acc.: 89.06%] [G loss: 5.102975]\n",
      "epoch:29 step:23067 [D loss: 0.381758, acc.: 80.47%] [G loss: 3.303451]\n",
      "epoch:29 step:23068 [D loss: 0.161328, acc.: 95.31%] [G loss: 4.489879]\n",
      "epoch:29 step:23069 [D loss: 0.024247, acc.: 100.00%] [G loss: 4.684765]\n",
      "epoch:29 step:23070 [D loss: 0.547134, acc.: 72.66%] [G loss: 0.748547]\n",
      "epoch:29 step:23071 [D loss: 0.306248, acc.: 82.81%] [G loss: 5.692146]\n",
      "epoch:29 step:23072 [D loss: 0.125893, acc.: 92.97%] [G loss: 5.719251]\n",
      "epoch:29 step:23073 [D loss: 0.419839, acc.: 78.12%] [G loss: 1.747015]\n",
      "epoch:29 step:23074 [D loss: 0.674731, acc.: 66.41%] [G loss: 5.796932]\n",
      "epoch:29 step:23075 [D loss: 0.139686, acc.: 95.31%] [G loss: 6.840473]\n",
      "epoch:29 step:23076 [D loss: 0.575432, acc.: 77.34%] [G loss: 4.012466]\n",
      "epoch:29 step:23077 [D loss: 0.070580, acc.: 96.88%] [G loss: 2.532321]\n",
      "epoch:29 step:23078 [D loss: 0.087275, acc.: 100.00%] [G loss: 3.672763]\n",
      "epoch:29 step:23079 [D loss: 0.042761, acc.: 99.22%] [G loss: 3.394564]\n",
      "epoch:29 step:23080 [D loss: 0.065315, acc.: 99.22%] [G loss: 3.624243]\n",
      "epoch:29 step:23081 [D loss: 0.087228, acc.: 99.22%] [G loss: 3.075459]\n",
      "epoch:29 step:23082 [D loss: 0.017306, acc.: 100.00%] [G loss: 3.138399]\n",
      "epoch:29 step:23083 [D loss: 0.256944, acc.: 91.41%] [G loss: 4.470263]\n",
      "epoch:29 step:23084 [D loss: 0.184518, acc.: 92.97%] [G loss: 3.575175]\n",
      "epoch:29 step:23085 [D loss: 0.053039, acc.: 100.00%] [G loss: 2.536422]\n",
      "epoch:29 step:23086 [D loss: 0.210232, acc.: 92.97%] [G loss: 3.937180]\n",
      "epoch:29 step:23087 [D loss: 0.052524, acc.: 99.22%] [G loss: 3.428305]\n",
      "epoch:29 step:23088 [D loss: 0.043599, acc.: 100.00%] [G loss: 3.574103]\n",
      "epoch:29 step:23089 [D loss: 0.117679, acc.: 96.88%] [G loss: 2.757834]\n",
      "epoch:29 step:23090 [D loss: 0.129553, acc.: 96.09%] [G loss: 3.892366]\n",
      "epoch:29 step:23091 [D loss: 0.150669, acc.: 95.31%] [G loss: 3.247042]\n",
      "epoch:29 step:23092 [D loss: 0.106123, acc.: 98.44%] [G loss: 4.011177]\n",
      "epoch:29 step:23093 [D loss: 0.023946, acc.: 100.00%] [G loss: 3.310130]\n",
      "epoch:29 step:23094 [D loss: 0.048473, acc.: 100.00%] [G loss: 4.098778]\n",
      "epoch:29 step:23095 [D loss: 0.265271, acc.: 89.84%] [G loss: 5.042551]\n",
      "epoch:29 step:23096 [D loss: 0.166405, acc.: 92.19%] [G loss: 4.416858]\n",
      "epoch:29 step:23097 [D loss: 0.031953, acc.: 100.00%] [G loss: 3.745650]\n",
      "epoch:29 step:23098 [D loss: 0.039117, acc.: 100.00%] [G loss: 3.319590]\n",
      "epoch:29 step:23099 [D loss: 0.117808, acc.: 96.09%] [G loss: 4.480364]\n",
      "epoch:29 step:23100 [D loss: 0.051816, acc.: 98.44%] [G loss: 3.654729]\n",
      "epoch:29 step:23101 [D loss: 0.077452, acc.: 98.44%] [G loss: 2.898082]\n",
      "epoch:29 step:23102 [D loss: 0.079374, acc.: 99.22%] [G loss: 3.824252]\n",
      "epoch:29 step:23103 [D loss: 0.091018, acc.: 96.88%] [G loss: 2.826588]\n",
      "epoch:29 step:23104 [D loss: 0.054451, acc.: 99.22%] [G loss: 3.909787]\n",
      "epoch:29 step:23105 [D loss: 0.046455, acc.: 99.22%] [G loss: 3.586875]\n",
      "epoch:29 step:23106 [D loss: 0.024126, acc.: 100.00%] [G loss: 4.006295]\n",
      "epoch:29 step:23107 [D loss: 0.020453, acc.: 100.00%] [G loss: 3.894528]\n",
      "epoch:29 step:23108 [D loss: 1.262403, acc.: 46.09%] [G loss: 8.892321]\n",
      "epoch:29 step:23109 [D loss: 2.424429, acc.: 50.00%] [G loss: 7.108663]\n",
      "epoch:29 step:23110 [D loss: 1.046608, acc.: 60.16%] [G loss: 3.618825]\n",
      "epoch:29 step:23111 [D loss: 0.242248, acc.: 89.06%] [G loss: 3.526803]\n",
      "epoch:29 step:23112 [D loss: 0.031637, acc.: 100.00%] [G loss: 4.080896]\n",
      "epoch:29 step:23113 [D loss: 0.050169, acc.: 100.00%] [G loss: 3.985591]\n",
      "epoch:29 step:23114 [D loss: 0.039310, acc.: 99.22%] [G loss: 4.519920]\n",
      "epoch:29 step:23115 [D loss: 0.059616, acc.: 97.66%] [G loss: 2.716941]\n",
      "epoch:29 step:23116 [D loss: 0.031580, acc.: 100.00%] [G loss: 2.929007]\n",
      "epoch:29 step:23117 [D loss: 0.076324, acc.: 100.00%] [G loss: 2.392236]\n",
      "epoch:29 step:23118 [D loss: 0.041980, acc.: 100.00%] [G loss: 2.996950]\n",
      "epoch:29 step:23119 [D loss: 0.031359, acc.: 100.00%] [G loss: 2.201786]\n",
      "epoch:29 step:23120 [D loss: 0.060439, acc.: 99.22%] [G loss: 2.012632]\n",
      "epoch:29 step:23121 [D loss: 0.041702, acc.: 100.00%] [G loss: 2.674576]\n",
      "epoch:29 step:23122 [D loss: 0.099292, acc.: 98.44%] [G loss: 2.958735]\n",
      "epoch:29 step:23123 [D loss: 0.086000, acc.: 97.66%] [G loss: 3.843708]\n",
      "epoch:29 step:23124 [D loss: 0.116715, acc.: 96.88%] [G loss: 2.254605]\n",
      "epoch:29 step:23125 [D loss: 0.061466, acc.: 100.00%] [G loss: 2.357526]\n",
      "epoch:29 step:23126 [D loss: 0.021110, acc.: 100.00%] [G loss: 2.782486]\n",
      "epoch:29 step:23127 [D loss: 0.090322, acc.: 99.22%] [G loss: 3.752409]\n",
      "epoch:29 step:23128 [D loss: 0.043997, acc.: 99.22%] [G loss: 3.758174]\n",
      "epoch:29 step:23129 [D loss: 0.045542, acc.: 99.22%] [G loss: 3.199978]\n",
      "epoch:29 step:23130 [D loss: 0.112005, acc.: 96.88%] [G loss: 4.085145]\n",
      "epoch:29 step:23131 [D loss: 0.053978, acc.: 100.00%] [G loss: 4.201034]\n",
      "epoch:29 step:23132 [D loss: 0.308272, acc.: 85.94%] [G loss: 3.664251]\n",
      "epoch:29 step:23133 [D loss: 0.012648, acc.: 100.00%] [G loss: 4.664174]\n",
      "epoch:29 step:23134 [D loss: 0.080666, acc.: 99.22%] [G loss: 3.106724]\n",
      "epoch:29 step:23135 [D loss: 0.047564, acc.: 100.00%] [G loss: 4.247046]\n",
      "epoch:29 step:23136 [D loss: 0.047240, acc.: 99.22%] [G loss: 3.300538]\n",
      "epoch:29 step:23137 [D loss: 0.031412, acc.: 100.00%] [G loss: 3.525626]\n",
      "epoch:29 step:23138 [D loss: 0.037796, acc.: 100.00%] [G loss: 3.826408]\n",
      "epoch:29 step:23139 [D loss: 0.022679, acc.: 99.22%] [G loss: 2.888428]\n",
      "epoch:29 step:23140 [D loss: 0.018586, acc.: 100.00%] [G loss: 2.811515]\n",
      "epoch:29 step:23141 [D loss: 0.272045, acc.: 89.84%] [G loss: 3.623889]\n",
      "epoch:29 step:23142 [D loss: 0.004421, acc.: 100.00%] [G loss: 4.215334]\n",
      "epoch:29 step:23143 [D loss: 0.034039, acc.: 98.44%] [G loss: 4.439027]\n",
      "epoch:29 step:23144 [D loss: 0.021731, acc.: 100.00%] [G loss: 2.792008]\n",
      "epoch:29 step:23145 [D loss: 0.059046, acc.: 99.22%] [G loss: 3.384018]\n",
      "epoch:29 step:23146 [D loss: 0.023887, acc.: 100.00%] [G loss: 3.268690]\n",
      "epoch:29 step:23147 [D loss: 0.041252, acc.: 99.22%] [G loss: 3.675622]\n",
      "epoch:29 step:23148 [D loss: 0.019907, acc.: 100.00%] [G loss: 3.562484]\n",
      "epoch:29 step:23149 [D loss: 0.018920, acc.: 100.00%] [G loss: 3.774136]\n",
      "epoch:29 step:23150 [D loss: 0.403203, acc.: 81.25%] [G loss: 5.320164]\n",
      "epoch:29 step:23151 [D loss: 0.025356, acc.: 99.22%] [G loss: 6.337360]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:29 step:23152 [D loss: 0.227989, acc.: 91.41%] [G loss: 4.083402]\n",
      "epoch:29 step:23153 [D loss: 0.166380, acc.: 92.97%] [G loss: 4.638550]\n",
      "epoch:29 step:23154 [D loss: 0.008100, acc.: 100.00%] [G loss: 5.868760]\n",
      "epoch:29 step:23155 [D loss: 0.099017, acc.: 97.66%] [G loss: 4.592974]\n",
      "epoch:29 step:23156 [D loss: 0.010013, acc.: 100.00%] [G loss: 4.355663]\n",
      "epoch:29 step:23157 [D loss: 0.037652, acc.: 99.22%] [G loss: 4.304302]\n",
      "epoch:29 step:23158 [D loss: 0.160085, acc.: 93.75%] [G loss: 2.543434]\n",
      "epoch:29 step:23159 [D loss: 0.050314, acc.: 100.00%] [G loss: 4.067332]\n",
      "epoch:29 step:23160 [D loss: 0.031800, acc.: 100.00%] [G loss: 4.150174]\n",
      "epoch:29 step:23161 [D loss: 0.028187, acc.: 99.22%] [G loss: 3.610121]\n",
      "epoch:29 step:23162 [D loss: 0.014671, acc.: 100.00%] [G loss: 2.926740]\n",
      "epoch:29 step:23163 [D loss: 0.009049, acc.: 100.00%] [G loss: 2.563122]\n",
      "epoch:29 step:23164 [D loss: 0.013927, acc.: 100.00%] [G loss: 2.534782]\n",
      "epoch:29 step:23165 [D loss: 0.066076, acc.: 98.44%] [G loss: 3.976576]\n",
      "epoch:29 step:23166 [D loss: 0.013796, acc.: 100.00%] [G loss: 4.032802]\n",
      "epoch:29 step:23167 [D loss: 0.020667, acc.: 100.00%] [G loss: 3.847717]\n",
      "epoch:29 step:23168 [D loss: 0.071866, acc.: 99.22%] [G loss: 1.960393]\n",
      "epoch:29 step:23169 [D loss: 0.017240, acc.: 100.00%] [G loss: 2.812919]\n",
      "epoch:29 step:23170 [D loss: 0.025597, acc.: 100.00%] [G loss: 2.872783]\n",
      "epoch:29 step:23171 [D loss: 0.022191, acc.: 100.00%] [G loss: 3.599579]\n",
      "epoch:29 step:23172 [D loss: 0.035392, acc.: 99.22%] [G loss: 2.847154]\n",
      "epoch:29 step:23173 [D loss: 0.081432, acc.: 99.22%] [G loss: 3.524461]\n",
      "epoch:29 step:23174 [D loss: 0.079327, acc.: 99.22%] [G loss: 5.514863]\n",
      "epoch:29 step:23175 [D loss: 3.158572, acc.: 13.28%] [G loss: 7.699256]\n",
      "epoch:29 step:23176 [D loss: 0.809573, acc.: 58.59%] [G loss: 6.116278]\n",
      "epoch:29 step:23177 [D loss: 0.011961, acc.: 100.00%] [G loss: 5.061676]\n",
      "epoch:29 step:23178 [D loss: 0.016562, acc.: 100.00%] [G loss: 4.131431]\n",
      "epoch:29 step:23179 [D loss: 0.095500, acc.: 96.88%] [G loss: 4.542248]\n",
      "epoch:29 step:23180 [D loss: 0.053894, acc.: 100.00%] [G loss: 3.702142]\n",
      "epoch:29 step:23181 [D loss: 0.159914, acc.: 96.88%] [G loss: 2.663331]\n",
      "epoch:29 step:23182 [D loss: 0.728767, acc.: 68.75%] [G loss: 6.091596]\n",
      "epoch:29 step:23183 [D loss: 0.149296, acc.: 94.53%] [G loss: 6.240160]\n",
      "epoch:29 step:23184 [D loss: 0.972676, acc.: 58.59%] [G loss: 2.401467]\n",
      "epoch:29 step:23185 [D loss: 0.195169, acc.: 91.41%] [G loss: 2.230169]\n",
      "epoch:29 step:23186 [D loss: 0.060345, acc.: 98.44%] [G loss: 1.744416]\n",
      "epoch:29 step:23187 [D loss: 0.193647, acc.: 93.75%] [G loss: 3.791089]\n",
      "epoch:29 step:23188 [D loss: 0.028445, acc.: 100.00%] [G loss: 3.678195]\n",
      "epoch:29 step:23189 [D loss: 0.093851, acc.: 100.00%] [G loss: 2.744877]\n",
      "epoch:29 step:23190 [D loss: 0.019918, acc.: 100.00%] [G loss: 2.477020]\n",
      "epoch:29 step:23191 [D loss: 0.083381, acc.: 100.00%] [G loss: 2.655655]\n",
      "epoch:29 step:23192 [D loss: 0.224569, acc.: 91.41%] [G loss: 4.030684]\n",
      "epoch:29 step:23193 [D loss: 0.085235, acc.: 96.88%] [G loss: 4.905591]\n",
      "epoch:29 step:23194 [D loss: 0.211696, acc.: 90.62%] [G loss: 2.154440]\n",
      "epoch:29 step:23195 [D loss: 0.117247, acc.: 95.31%] [G loss: 3.944924]\n",
      "epoch:29 step:23196 [D loss: 2.601476, acc.: 17.97%] [G loss: 6.575476]\n",
      "epoch:29 step:23197 [D loss: 0.887101, acc.: 60.16%] [G loss: 5.790825]\n",
      "epoch:29 step:23198 [D loss: 0.975499, acc.: 55.47%] [G loss: 3.347878]\n",
      "epoch:29 step:23199 [D loss: 0.270938, acc.: 88.28%] [G loss: 3.087477]\n",
      "epoch:29 step:23200 [D loss: 0.061930, acc.: 99.22%] [G loss: 3.113016]\n",
      "##############\n",
      "[1.09966065 0.91716225 0.84693868 1.030073   2.10079182 2.10586137\n",
      " 0.9384791  0.91964383 2.11375821 2.11306886]\n",
      "##########\n",
      "epoch:29 step:23201 [D loss: 0.068887, acc.: 100.00%] [G loss: 2.885336]\n",
      "epoch:29 step:23202 [D loss: 0.035341, acc.: 100.00%] [G loss: 2.930621]\n",
      "epoch:29 step:23203 [D loss: 0.045509, acc.: 100.00%] [G loss: 3.176351]\n",
      "epoch:29 step:23204 [D loss: 0.043150, acc.: 100.00%] [G loss: 2.896373]\n",
      "epoch:29 step:23205 [D loss: 0.062613, acc.: 100.00%] [G loss: 2.734065]\n",
      "epoch:29 step:23206 [D loss: 0.072693, acc.: 99.22%] [G loss: 3.136837]\n",
      "epoch:29 step:23207 [D loss: 0.100732, acc.: 97.66%] [G loss: 2.793843]\n",
      "epoch:29 step:23208 [D loss: 0.076274, acc.: 99.22%] [G loss: 2.987600]\n",
      "epoch:29 step:23209 [D loss: 0.082115, acc.: 98.44%] [G loss: 3.053802]\n",
      "epoch:29 step:23210 [D loss: 0.055281, acc.: 99.22%] [G loss: 2.748264]\n",
      "epoch:29 step:23211 [D loss: 0.076738, acc.: 100.00%] [G loss: 3.355248]\n",
      "epoch:29 step:23212 [D loss: 0.089686, acc.: 99.22%] [G loss: 3.426254]\n",
      "epoch:29 step:23213 [D loss: 0.044846, acc.: 100.00%] [G loss: 3.977524]\n",
      "epoch:29 step:23214 [D loss: 0.143044, acc.: 96.88%] [G loss: 3.172342]\n",
      "epoch:29 step:23215 [D loss: 0.223555, acc.: 93.75%] [G loss: 3.580142]\n",
      "epoch:29 step:23216 [D loss: 0.074069, acc.: 97.66%] [G loss: 3.505336]\n",
      "epoch:29 step:23217 [D loss: 0.027654, acc.: 100.00%] [G loss: 3.705187]\n",
      "epoch:29 step:23218 [D loss: 0.050943, acc.: 100.00%] [G loss: 3.755003]\n",
      "epoch:29 step:23219 [D loss: 0.031897, acc.: 100.00%] [G loss: 3.642847]\n",
      "epoch:29 step:23220 [D loss: 0.083949, acc.: 98.44%] [G loss: 2.381322]\n",
      "epoch:29 step:23221 [D loss: 0.063669, acc.: 100.00%] [G loss: 3.027095]\n",
      "epoch:29 step:23222 [D loss: 0.063751, acc.: 100.00%] [G loss: 2.957822]\n",
      "epoch:29 step:23223 [D loss: 0.025862, acc.: 100.00%] [G loss: 2.450956]\n",
      "epoch:29 step:23224 [D loss: 0.133164, acc.: 96.88%] [G loss: 1.620587]\n",
      "epoch:29 step:23225 [D loss: 0.064158, acc.: 99.22%] [G loss: 2.661919]\n",
      "epoch:29 step:23226 [D loss: 0.298579, acc.: 87.50%] [G loss: 4.215422]\n",
      "epoch:29 step:23227 [D loss: 0.067869, acc.: 96.88%] [G loss: 4.664131]\n",
      "epoch:29 step:23228 [D loss: 0.226007, acc.: 89.06%] [G loss: 0.339230]\n",
      "epoch:29 step:23229 [D loss: 1.178013, acc.: 53.12%] [G loss: 6.661760]\n",
      "epoch:29 step:23230 [D loss: 1.645334, acc.: 51.56%] [G loss: 5.695732]\n",
      "epoch:29 step:23231 [D loss: 0.939632, acc.: 58.59%] [G loss: 2.583153]\n",
      "epoch:29 step:23232 [D loss: 0.124543, acc.: 96.09%] [G loss: 2.508610]\n",
      "epoch:29 step:23233 [D loss: 0.062040, acc.: 100.00%] [G loss: 2.268906]\n",
      "epoch:29 step:23234 [D loss: 0.045695, acc.: 100.00%] [G loss: 3.154272]\n",
      "epoch:29 step:23235 [D loss: 0.031317, acc.: 100.00%] [G loss: 3.385782]\n",
      "epoch:29 step:23236 [D loss: 0.023509, acc.: 100.00%] [G loss: 3.158295]\n",
      "epoch:29 step:23237 [D loss: 0.057207, acc.: 98.44%] [G loss: 3.176996]\n",
      "epoch:29 step:23238 [D loss: 0.108615, acc.: 96.88%] [G loss: 3.148212]\n",
      "epoch:29 step:23239 [D loss: 0.124873, acc.: 96.09%] [G loss: 2.891512]\n",
      "epoch:29 step:23240 [D loss: 0.056327, acc.: 98.44%] [G loss: 2.540924]\n",
      "epoch:29 step:23241 [D loss: 0.073399, acc.: 98.44%] [G loss: 3.342133]\n",
      "epoch:29 step:23242 [D loss: 0.079813, acc.: 99.22%] [G loss: 2.937525]\n",
      "epoch:29 step:23243 [D loss: 0.101420, acc.: 97.66%] [G loss: 2.370203]\n",
      "epoch:29 step:23244 [D loss: 0.041242, acc.: 100.00%] [G loss: 2.211754]\n",
      "epoch:29 step:23245 [D loss: 0.099451, acc.: 98.44%] [G loss: 3.347615]\n",
      "epoch:29 step:23246 [D loss: 0.013372, acc.: 100.00%] [G loss: 4.073987]\n",
      "epoch:29 step:23247 [D loss: 0.028388, acc.: 100.00%] [G loss: 3.408640]\n",
      "epoch:29 step:23248 [D loss: 0.390661, acc.: 83.59%] [G loss: 1.018818]\n",
      "epoch:29 step:23249 [D loss: 0.047035, acc.: 100.00%] [G loss: 2.348454]\n",
      "epoch:29 step:23250 [D loss: 0.028489, acc.: 100.00%] [G loss: 2.502761]\n",
      "epoch:29 step:23251 [D loss: 0.139564, acc.: 96.09%] [G loss: 3.943191]\n",
      "epoch:29 step:23252 [D loss: 0.027379, acc.: 100.00%] [G loss: 4.695457]\n",
      "epoch:29 step:23253 [D loss: 1.140723, acc.: 51.56%] [G loss: 1.606749]\n",
      "epoch:29 step:23254 [D loss: 0.139520, acc.: 96.09%] [G loss: 3.269373]\n",
      "epoch:29 step:23255 [D loss: 0.019367, acc.: 100.00%] [G loss: 3.941253]\n",
      "epoch:29 step:23256 [D loss: 0.035978, acc.: 98.44%] [G loss: 4.304975]\n",
      "epoch:29 step:23257 [D loss: 0.014708, acc.: 100.00%] [G loss: 3.577358]\n",
      "epoch:29 step:23258 [D loss: 0.024012, acc.: 100.00%] [G loss: 3.454397]\n",
      "epoch:29 step:23259 [D loss: 0.033237, acc.: 100.00%] [G loss: 3.747283]\n",
      "epoch:29 step:23260 [D loss: 0.018082, acc.: 100.00%] [G loss: 3.528328]\n",
      "epoch:29 step:23261 [D loss: 0.015128, acc.: 100.00%] [G loss: 3.626786]\n",
      "epoch:29 step:23262 [D loss: 0.194272, acc.: 94.53%] [G loss: 2.320461]\n",
      "epoch:29 step:23263 [D loss: 0.052758, acc.: 100.00%] [G loss: 3.291198]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:29 step:23264 [D loss: 0.020756, acc.: 100.00%] [G loss: 3.666680]\n",
      "epoch:29 step:23265 [D loss: 0.021198, acc.: 100.00%] [G loss: 3.675018]\n",
      "epoch:29 step:23266 [D loss: 0.020551, acc.: 100.00%] [G loss: 3.916047]\n",
      "epoch:29 step:23267 [D loss: 0.093053, acc.: 100.00%] [G loss: 3.819264]\n",
      "epoch:29 step:23268 [D loss: 0.040994, acc.: 100.00%] [G loss: 3.956296]\n",
      "epoch:29 step:23269 [D loss: 0.072020, acc.: 99.22%] [G loss: 3.995481]\n",
      "epoch:29 step:23270 [D loss: 0.011026, acc.: 100.00%] [G loss: 4.103286]\n",
      "epoch:29 step:23271 [D loss: 0.029664, acc.: 100.00%] [G loss: 4.319135]\n",
      "epoch:29 step:23272 [D loss: 0.055030, acc.: 100.00%] [G loss: 3.875535]\n",
      "epoch:29 step:23273 [D loss: 0.019446, acc.: 100.00%] [G loss: 3.680763]\n",
      "epoch:29 step:23274 [D loss: 0.085562, acc.: 98.44%] [G loss: 3.934639]\n",
      "epoch:29 step:23275 [D loss: 0.019512, acc.: 100.00%] [G loss: 4.137036]\n",
      "epoch:29 step:23276 [D loss: 0.159878, acc.: 94.53%] [G loss: 3.135156]\n",
      "epoch:29 step:23277 [D loss: 0.057380, acc.: 100.00%] [G loss: 4.027805]\n",
      "epoch:29 step:23278 [D loss: 0.022933, acc.: 100.00%] [G loss: 4.563528]\n",
      "epoch:29 step:23279 [D loss: 0.013785, acc.: 100.00%] [G loss: 4.268052]\n",
      "epoch:29 step:23280 [D loss: 0.022316, acc.: 100.00%] [G loss: 3.999835]\n",
      "epoch:29 step:23281 [D loss: 0.013634, acc.: 100.00%] [G loss: 3.953307]\n",
      "epoch:29 step:23282 [D loss: 0.025081, acc.: 99.22%] [G loss: 3.197096]\n",
      "epoch:29 step:23283 [D loss: 0.047757, acc.: 99.22%] [G loss: 3.763791]\n",
      "epoch:29 step:23284 [D loss: 0.036671, acc.: 99.22%] [G loss: 3.737733]\n",
      "epoch:29 step:23285 [D loss: 0.023224, acc.: 100.00%] [G loss: 4.285076]\n",
      "epoch:29 step:23286 [D loss: 0.406135, acc.: 82.03%] [G loss: 6.099718]\n",
      "epoch:29 step:23287 [D loss: 0.252909, acc.: 87.50%] [G loss: 5.860129]\n",
      "epoch:29 step:23288 [D loss: 0.066822, acc.: 96.09%] [G loss: 4.315636]\n",
      "epoch:29 step:23289 [D loss: 0.018147, acc.: 100.00%] [G loss: 4.137925]\n",
      "epoch:29 step:23290 [D loss: 0.010657, acc.: 100.00%] [G loss: 4.356762]\n",
      "epoch:29 step:23291 [D loss: 0.028371, acc.: 100.00%] [G loss: 4.043910]\n",
      "epoch:29 step:23292 [D loss: 0.015113, acc.: 100.00%] [G loss: 4.761525]\n",
      "epoch:29 step:23293 [D loss: 0.016135, acc.: 100.00%] [G loss: 4.644135]\n",
      "epoch:29 step:23294 [D loss: 0.019702, acc.: 100.00%] [G loss: 4.651145]\n",
      "epoch:29 step:23295 [D loss: 0.107064, acc.: 96.88%] [G loss: 3.329792]\n",
      "epoch:29 step:23296 [D loss: 0.050474, acc.: 99.22%] [G loss: 3.918939]\n",
      "epoch:29 step:23297 [D loss: 0.017756, acc.: 100.00%] [G loss: 4.708920]\n",
      "epoch:29 step:23298 [D loss: 0.039847, acc.: 100.00%] [G loss: 4.452713]\n",
      "epoch:29 step:23299 [D loss: 0.045663, acc.: 99.22%] [G loss: 4.374157]\n",
      "epoch:29 step:23300 [D loss: 0.004108, acc.: 100.00%] [G loss: 4.720929]\n",
      "epoch:29 step:23301 [D loss: 0.087876, acc.: 96.88%] [G loss: 3.679585]\n",
      "epoch:29 step:23302 [D loss: 0.025014, acc.: 100.00%] [G loss: 3.494261]\n",
      "epoch:29 step:23303 [D loss: 0.043211, acc.: 100.00%] [G loss: 4.007236]\n",
      "epoch:29 step:23304 [D loss: 0.007568, acc.: 100.00%] [G loss: 4.268155]\n",
      "epoch:29 step:23305 [D loss: 0.030325, acc.: 100.00%] [G loss: 4.483378]\n",
      "epoch:29 step:23306 [D loss: 0.008616, acc.: 100.00%] [G loss: 4.502976]\n",
      "epoch:29 step:23307 [D loss: 0.166356, acc.: 94.53%] [G loss: 3.352213]\n",
      "epoch:29 step:23308 [D loss: 0.020896, acc.: 100.00%] [G loss: 5.565619]\n",
      "epoch:29 step:23309 [D loss: 0.010811, acc.: 100.00%] [G loss: 3.883529]\n",
      "epoch:29 step:23310 [D loss: 0.016661, acc.: 100.00%] [G loss: 3.556496]\n",
      "epoch:29 step:23311 [D loss: 0.092931, acc.: 99.22%] [G loss: 5.571785]\n",
      "epoch:29 step:23312 [D loss: 0.045988, acc.: 99.22%] [G loss: 4.997621]\n",
      "epoch:29 step:23313 [D loss: 0.012269, acc.: 100.00%] [G loss: 4.767843]\n",
      "epoch:29 step:23314 [D loss: 0.029423, acc.: 100.00%] [G loss: 5.331269]\n",
      "epoch:29 step:23315 [D loss: 0.005146, acc.: 100.00%] [G loss: 5.424718]\n",
      "epoch:29 step:23316 [D loss: 0.031976, acc.: 99.22%] [G loss: 4.765267]\n",
      "epoch:29 step:23317 [D loss: 0.018624, acc.: 100.00%] [G loss: 5.013286]\n",
      "epoch:29 step:23318 [D loss: 0.020990, acc.: 100.00%] [G loss: 5.558974]\n",
      "epoch:29 step:23319 [D loss: 0.043378, acc.: 100.00%] [G loss: 3.909877]\n",
      "epoch:29 step:23320 [D loss: 0.028929, acc.: 100.00%] [G loss: 4.495196]\n",
      "epoch:29 step:23321 [D loss: 0.003790, acc.: 100.00%] [G loss: 4.607783]\n",
      "epoch:29 step:23322 [D loss: 0.071781, acc.: 99.22%] [G loss: 4.909085]\n",
      "epoch:29 step:23323 [D loss: 0.012400, acc.: 100.00%] [G loss: 5.086577]\n",
      "epoch:29 step:23324 [D loss: 0.003272, acc.: 100.00%] [G loss: 5.163557]\n",
      "epoch:29 step:23325 [D loss: 0.041889, acc.: 99.22%] [G loss: 4.618560]\n",
      "epoch:29 step:23326 [D loss: 0.029495, acc.: 100.00%] [G loss: 4.950611]\n",
      "epoch:29 step:23327 [D loss: 0.051726, acc.: 100.00%] [G loss: 4.046389]\n",
      "epoch:29 step:23328 [D loss: 0.015673, acc.: 99.22%] [G loss: 4.807210]\n",
      "epoch:29 step:23329 [D loss: 0.009249, acc.: 100.00%] [G loss: 5.087156]\n",
      "epoch:29 step:23330 [D loss: 0.018133, acc.: 100.00%] [G loss: 5.948856]\n",
      "epoch:29 step:23331 [D loss: 0.080862, acc.: 98.44%] [G loss: 3.131445]\n",
      "epoch:29 step:23332 [D loss: 0.047713, acc.: 99.22%] [G loss: 4.800443]\n",
      "epoch:29 step:23333 [D loss: 0.005816, acc.: 100.00%] [G loss: 5.808611]\n",
      "epoch:29 step:23334 [D loss: 1.722772, acc.: 30.47%] [G loss: 8.068497]\n",
      "epoch:29 step:23335 [D loss: 1.515722, acc.: 53.12%] [G loss: 4.936656]\n",
      "epoch:29 step:23336 [D loss: 0.133142, acc.: 96.09%] [G loss: 3.457817]\n",
      "epoch:29 step:23337 [D loss: 0.068272, acc.: 98.44%] [G loss: 3.355122]\n",
      "epoch:29 step:23338 [D loss: 0.189472, acc.: 93.75%] [G loss: 3.968811]\n",
      "epoch:29 step:23339 [D loss: 0.133057, acc.: 96.88%] [G loss: 2.713115]\n",
      "epoch:29 step:23340 [D loss: 0.030302, acc.: 100.00%] [G loss: 2.105433]\n",
      "epoch:29 step:23341 [D loss: 0.098874, acc.: 97.66%] [G loss: 0.845941]\n",
      "epoch:29 step:23342 [D loss: 0.025163, acc.: 100.00%] [G loss: 0.429437]\n",
      "epoch:29 step:23343 [D loss: 0.026233, acc.: 100.00%] [G loss: 1.393264]\n",
      "epoch:29 step:23344 [D loss: 0.019304, acc.: 100.00%] [G loss: 0.823561]\n",
      "epoch:29 step:23345 [D loss: 0.104257, acc.: 97.66%] [G loss: 0.777262]\n",
      "epoch:29 step:23346 [D loss: 0.057854, acc.: 98.44%] [G loss: 2.500249]\n",
      "epoch:29 step:23347 [D loss: 0.426429, acc.: 81.25%] [G loss: 3.048722]\n",
      "epoch:29 step:23348 [D loss: 0.040647, acc.: 99.22%] [G loss: 4.831826]\n",
      "epoch:29 step:23349 [D loss: 0.891132, acc.: 57.81%] [G loss: 1.900099]\n",
      "epoch:29 step:23350 [D loss: 0.099631, acc.: 96.88%] [G loss: 2.477980]\n",
      "epoch:29 step:23351 [D loss: 0.043166, acc.: 100.00%] [G loss: 2.893094]\n",
      "epoch:29 step:23352 [D loss: 2.692065, acc.: 25.00%] [G loss: 7.002097]\n",
      "epoch:29 step:23353 [D loss: 1.492878, acc.: 50.78%] [G loss: 5.520277]\n",
      "epoch:29 step:23354 [D loss: 0.451876, acc.: 80.47%] [G loss: 3.296780]\n",
      "epoch:29 step:23355 [D loss: 0.285097, acc.: 86.72%] [G loss: 4.276759]\n",
      "epoch:29 step:23356 [D loss: 0.044374, acc.: 100.00%] [G loss: 4.836761]\n",
      "epoch:29 step:23357 [D loss: 0.090536, acc.: 98.44%] [G loss: 3.894398]\n",
      "epoch:29 step:23358 [D loss: 0.195298, acc.: 94.53%] [G loss: 3.811948]\n",
      "epoch:29 step:23359 [D loss: 0.032414, acc.: 99.22%] [G loss: 4.100918]\n",
      "epoch:29 step:23360 [D loss: 0.057089, acc.: 99.22%] [G loss: 3.121980]\n",
      "epoch:29 step:23361 [D loss: 0.062714, acc.: 99.22%] [G loss: 3.371903]\n",
      "epoch:29 step:23362 [D loss: 0.132424, acc.: 97.66%] [G loss: 2.716686]\n",
      "epoch:29 step:23363 [D loss: 0.074843, acc.: 99.22%] [G loss: 3.806963]\n",
      "epoch:29 step:23364 [D loss: 0.037690, acc.: 100.00%] [G loss: 3.032735]\n",
      "epoch:29 step:23365 [D loss: 0.090219, acc.: 99.22%] [G loss: 3.840073]\n",
      "epoch:29 step:23366 [D loss: 0.026385, acc.: 100.00%] [G loss: 3.582063]\n",
      "epoch:29 step:23367 [D loss: 0.038293, acc.: 100.00%] [G loss: 4.245294]\n",
      "epoch:29 step:23368 [D loss: 0.045107, acc.: 100.00%] [G loss: 2.901242]\n",
      "epoch:29 step:23369 [D loss: 0.041595, acc.: 100.00%] [G loss: 2.665633]\n",
      "epoch:29 step:23370 [D loss: 0.029941, acc.: 100.00%] [G loss: 2.532528]\n",
      "epoch:29 step:23371 [D loss: 0.066558, acc.: 99.22%] [G loss: 3.046383]\n",
      "epoch:29 step:23372 [D loss: 0.063179, acc.: 99.22%] [G loss: 3.002774]\n",
      "epoch:29 step:23373 [D loss: 0.121563, acc.: 98.44%] [G loss: 2.294041]\n",
      "epoch:29 step:23374 [D loss: 0.672617, acc.: 65.62%] [G loss: 6.212654]\n",
      "epoch:29 step:23375 [D loss: 0.242246, acc.: 85.94%] [G loss: 5.887689]\n",
      "epoch:29 step:23376 [D loss: 0.355156, acc.: 82.81%] [G loss: 3.417739]\n",
      "epoch:29 step:23377 [D loss: 0.144034, acc.: 94.53%] [G loss: 4.282013]\n",
      "epoch:29 step:23378 [D loss: 0.012775, acc.: 100.00%] [G loss: 5.245193]\n",
      "epoch:29 step:23379 [D loss: 0.022494, acc.: 100.00%] [G loss: 4.677024]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:29 step:23380 [D loss: 0.013703, acc.: 100.00%] [G loss: 4.623972]\n",
      "epoch:29 step:23381 [D loss: 0.020058, acc.: 100.00%] [G loss: 4.430449]\n",
      "epoch:29 step:23382 [D loss: 0.053281, acc.: 99.22%] [G loss: 3.378431]\n",
      "epoch:29 step:23383 [D loss: 0.018824, acc.: 100.00%] [G loss: 3.820644]\n",
      "epoch:29 step:23384 [D loss: 0.084070, acc.: 100.00%] [G loss: 2.822454]\n",
      "epoch:29 step:23385 [D loss: 0.049185, acc.: 99.22%] [G loss: 3.558382]\n",
      "epoch:29 step:23386 [D loss: 0.018115, acc.: 100.00%] [G loss: 4.669540]\n",
      "epoch:29 step:23387 [D loss: 0.032049, acc.: 100.00%] [G loss: 4.332942]\n",
      "epoch:29 step:23388 [D loss: 0.147766, acc.: 95.31%] [G loss: 2.864796]\n",
      "epoch:29 step:23389 [D loss: 0.053772, acc.: 99.22%] [G loss: 3.484950]\n",
      "epoch:29 step:23390 [D loss: 0.035889, acc.: 99.22%] [G loss: 3.818281]\n",
      "epoch:29 step:23391 [D loss: 0.022450, acc.: 100.00%] [G loss: 4.094626]\n",
      "epoch:29 step:23392 [D loss: 0.058316, acc.: 99.22%] [G loss: 3.549775]\n",
      "epoch:29 step:23393 [D loss: 0.018189, acc.: 100.00%] [G loss: 4.120639]\n",
      "epoch:29 step:23394 [D loss: 0.018058, acc.: 100.00%] [G loss: 3.894225]\n",
      "epoch:29 step:23395 [D loss: 0.024824, acc.: 100.00%] [G loss: 3.354555]\n",
      "epoch:29 step:23396 [D loss: 0.051296, acc.: 99.22%] [G loss: 2.900478]\n",
      "epoch:29 step:23397 [D loss: 0.031609, acc.: 100.00%] [G loss: 3.881098]\n",
      "epoch:29 step:23398 [D loss: 0.030943, acc.: 100.00%] [G loss: 2.964743]\n",
      "epoch:29 step:23399 [D loss: 0.044316, acc.: 99.22%] [G loss: 3.252378]\n",
      "epoch:29 step:23400 [D loss: 0.029188, acc.: 99.22%] [G loss: 5.062926]\n",
      "##############\n",
      "[2.1265777  0.99915618 1.03086567 0.95170662 2.10825672 1.09599437\n",
      " 1.07138208 2.10248903 2.1169043  1.08356493]\n",
      "##########\n",
      "epoch:29 step:23401 [D loss: 0.033190, acc.: 99.22%] [G loss: 4.207793]\n",
      "epoch:29 step:23402 [D loss: 0.065451, acc.: 100.00%] [G loss: 3.546090]\n",
      "epoch:29 step:23403 [D loss: 0.085199, acc.: 98.44%] [G loss: 2.835597]\n",
      "epoch:29 step:23404 [D loss: 0.007893, acc.: 100.00%] [G loss: 4.451360]\n",
      "epoch:29 step:23405 [D loss: 0.018959, acc.: 100.00%] [G loss: 4.527902]\n",
      "epoch:29 step:23406 [D loss: 0.171216, acc.: 96.09%] [G loss: 5.228407]\n",
      "epoch:29 step:23407 [D loss: 0.012121, acc.: 100.00%] [G loss: 6.566093]\n",
      "epoch:29 step:23408 [D loss: 0.277921, acc.: 86.72%] [G loss: 3.314368]\n",
      "epoch:29 step:23409 [D loss: 0.254235, acc.: 86.72%] [G loss: 6.007479]\n",
      "epoch:29 step:23410 [D loss: 0.014916, acc.: 100.00%] [G loss: 7.122527]\n",
      "epoch:29 step:23411 [D loss: 0.871437, acc.: 62.50%] [G loss: 1.369035]\n",
      "epoch:29 step:23412 [D loss: 0.639217, acc.: 72.66%] [G loss: 5.966624]\n",
      "epoch:29 step:23413 [D loss: 0.011519, acc.: 100.00%] [G loss: 7.074820]\n",
      "epoch:29 step:23414 [D loss: 0.862036, acc.: 65.62%] [G loss: 3.304081]\n",
      "epoch:29 step:23415 [D loss: 0.334880, acc.: 77.34%] [G loss: 5.082611]\n",
      "epoch:29 step:23416 [D loss: 0.001981, acc.: 100.00%] [G loss: 6.044664]\n",
      "epoch:29 step:23417 [D loss: 0.081428, acc.: 96.88%] [G loss: 4.837532]\n",
      "epoch:29 step:23418 [D loss: 0.010433, acc.: 100.00%] [G loss: 4.705874]\n",
      "epoch:29 step:23419 [D loss: 0.015136, acc.: 100.00%] [G loss: 4.391456]\n",
      "epoch:29 step:23420 [D loss: 0.042658, acc.: 100.00%] [G loss: 3.888017]\n",
      "epoch:29 step:23421 [D loss: 0.021101, acc.: 100.00%] [G loss: 3.884004]\n",
      "epoch:29 step:23422 [D loss: 0.024356, acc.: 100.00%] [G loss: 3.502296]\n",
      "epoch:29 step:23423 [D loss: 0.036830, acc.: 100.00%] [G loss: 3.842979]\n",
      "epoch:29 step:23424 [D loss: 0.021753, acc.: 100.00%] [G loss: 4.047739]\n",
      "epoch:29 step:23425 [D loss: 0.092140, acc.: 98.44%] [G loss: 3.170955]\n",
      "epoch:29 step:23426 [D loss: 0.021617, acc.: 100.00%] [G loss: 2.902518]\n",
      "epoch:29 step:23427 [D loss: 0.043345, acc.: 99.22%] [G loss: 3.496443]\n",
      "epoch:29 step:23428 [D loss: 0.048105, acc.: 99.22%] [G loss: 4.345821]\n",
      "epoch:29 step:23429 [D loss: 0.025338, acc.: 100.00%] [G loss: 4.903369]\n",
      "epoch:29 step:23430 [D loss: 0.029541, acc.: 100.00%] [G loss: 4.168403]\n",
      "epoch:30 step:23431 [D loss: 0.011506, acc.: 100.00%] [G loss: 4.418653]\n",
      "epoch:30 step:23432 [D loss: 0.024269, acc.: 100.00%] [G loss: 4.532084]\n",
      "epoch:30 step:23433 [D loss: 0.041545, acc.: 100.00%] [G loss: 4.258720]\n",
      "epoch:30 step:23434 [D loss: 0.018173, acc.: 100.00%] [G loss: 3.749440]\n",
      "epoch:30 step:23435 [D loss: 0.035060, acc.: 100.00%] [G loss: 4.161078]\n",
      "epoch:30 step:23436 [D loss: 0.047996, acc.: 99.22%] [G loss: 3.500720]\n",
      "epoch:30 step:23437 [D loss: 0.035646, acc.: 100.00%] [G loss: 2.474903]\n",
      "epoch:30 step:23438 [D loss: 0.003395, acc.: 100.00%] [G loss: 2.786603]\n",
      "epoch:30 step:23439 [D loss: 0.085835, acc.: 99.22%] [G loss: 5.017717]\n",
      "epoch:30 step:23440 [D loss: 0.027246, acc.: 100.00%] [G loss: 5.290929]\n",
      "epoch:30 step:23441 [D loss: 0.014288, acc.: 100.00%] [G loss: 5.247312]\n",
      "epoch:30 step:23442 [D loss: 0.217427, acc.: 92.19%] [G loss: 5.008327]\n",
      "epoch:30 step:23443 [D loss: 0.002956, acc.: 100.00%] [G loss: 5.772989]\n",
      "epoch:30 step:23444 [D loss: 0.361005, acc.: 87.50%] [G loss: 3.620677]\n",
      "epoch:30 step:23445 [D loss: 0.049331, acc.: 98.44%] [G loss: 3.896662]\n",
      "epoch:30 step:23446 [D loss: 0.011607, acc.: 100.00%] [G loss: 4.414599]\n",
      "epoch:30 step:23447 [D loss: 0.011416, acc.: 100.00%] [G loss: 5.042717]\n",
      "epoch:30 step:23448 [D loss: 0.005462, acc.: 100.00%] [G loss: 4.769384]\n",
      "epoch:30 step:23449 [D loss: 0.010030, acc.: 100.00%] [G loss: 4.106166]\n",
      "epoch:30 step:23450 [D loss: 0.016338, acc.: 100.00%] [G loss: 3.773018]\n",
      "epoch:30 step:23451 [D loss: 0.013423, acc.: 100.00%] [G loss: 3.630433]\n",
      "epoch:30 step:23452 [D loss: 0.013878, acc.: 100.00%] [G loss: 4.152270]\n",
      "epoch:30 step:23453 [D loss: 0.117518, acc.: 95.31%] [G loss: 0.476927]\n",
      "epoch:30 step:23454 [D loss: 0.237530, acc.: 89.06%] [G loss: 7.480922]\n",
      "epoch:30 step:23455 [D loss: 0.785801, acc.: 64.06%] [G loss: 0.957341]\n",
      "epoch:30 step:23456 [D loss: 1.885433, acc.: 53.91%] [G loss: 8.797828]\n",
      "epoch:30 step:23457 [D loss: 2.233399, acc.: 50.00%] [G loss: 7.244719]\n",
      "epoch:30 step:23458 [D loss: 1.673826, acc.: 50.78%] [G loss: 4.133288]\n",
      "epoch:30 step:23459 [D loss: 0.133720, acc.: 95.31%] [G loss: 3.039290]\n",
      "epoch:30 step:23460 [D loss: 0.083131, acc.: 98.44%] [G loss: 3.926370]\n",
      "epoch:30 step:23461 [D loss: 0.088934, acc.: 99.22%] [G loss: 3.393130]\n",
      "epoch:30 step:23462 [D loss: 0.029480, acc.: 100.00%] [G loss: 3.492274]\n",
      "epoch:30 step:23463 [D loss: 0.051747, acc.: 99.22%] [G loss: 3.232484]\n",
      "epoch:30 step:23464 [D loss: 0.089471, acc.: 98.44%] [G loss: 4.078619]\n",
      "epoch:30 step:23465 [D loss: 0.045343, acc.: 100.00%] [G loss: 3.657694]\n",
      "epoch:30 step:23466 [D loss: 0.106833, acc.: 96.88%] [G loss: 3.950303]\n",
      "epoch:30 step:23467 [D loss: 0.035473, acc.: 100.00%] [G loss: 3.394202]\n",
      "epoch:30 step:23468 [D loss: 0.077628, acc.: 98.44%] [G loss: 3.520385]\n",
      "epoch:30 step:23469 [D loss: 0.045487, acc.: 100.00%] [G loss: 3.787170]\n",
      "epoch:30 step:23470 [D loss: 0.044488, acc.: 98.44%] [G loss: 3.491671]\n",
      "epoch:30 step:23471 [D loss: 0.066954, acc.: 100.00%] [G loss: 2.971438]\n",
      "epoch:30 step:23472 [D loss: 0.120421, acc.: 96.88%] [G loss: 3.850727]\n",
      "epoch:30 step:23473 [D loss: 0.091321, acc.: 96.88%] [G loss: 3.896397]\n",
      "epoch:30 step:23474 [D loss: 0.307985, acc.: 87.50%] [G loss: 2.687058]\n",
      "epoch:30 step:23475 [D loss: 0.110557, acc.: 98.44%] [G loss: 3.267648]\n",
      "epoch:30 step:23476 [D loss: 0.022327, acc.: 99.22%] [G loss: 3.451536]\n",
      "epoch:30 step:23477 [D loss: 0.035919, acc.: 100.00%] [G loss: 3.215361]\n",
      "epoch:30 step:23478 [D loss: 0.049403, acc.: 100.00%] [G loss: 3.420389]\n",
      "epoch:30 step:23479 [D loss: 0.049962, acc.: 100.00%] [G loss: 2.375068]\n",
      "epoch:30 step:23480 [D loss: 0.048027, acc.: 98.44%] [G loss: 2.652344]\n",
      "epoch:30 step:23481 [D loss: 0.075116, acc.: 99.22%] [G loss: 4.295991]\n",
      "epoch:30 step:23482 [D loss: 0.020774, acc.: 100.00%] [G loss: 3.833993]\n",
      "epoch:30 step:23483 [D loss: 0.025272, acc.: 100.00%] [G loss: 4.129241]\n",
      "epoch:30 step:23484 [D loss: 0.056920, acc.: 99.22%] [G loss: 3.054012]\n",
      "epoch:30 step:23485 [D loss: 0.063483, acc.: 98.44%] [G loss: 2.687062]\n",
      "epoch:30 step:23486 [D loss: 0.048561, acc.: 100.00%] [G loss: 3.487243]\n",
      "epoch:30 step:23487 [D loss: 0.064013, acc.: 99.22%] [G loss: 3.742175]\n",
      "epoch:30 step:23488 [D loss: 0.059483, acc.: 100.00%] [G loss: 3.826835]\n",
      "epoch:30 step:23489 [D loss: 0.018748, acc.: 100.00%] [G loss: 3.943622]\n",
      "epoch:30 step:23490 [D loss: 0.022335, acc.: 99.22%] [G loss: 3.521253]\n",
      "epoch:30 step:23491 [D loss: 0.116925, acc.: 97.66%] [G loss: 2.976067]\n",
      "epoch:30 step:23492 [D loss: 0.032399, acc.: 100.00%] [G loss: 3.655342]\n",
      "epoch:30 step:23493 [D loss: 0.046104, acc.: 100.00%] [G loss: 4.119276]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:30 step:23494 [D loss: 0.021935, acc.: 100.00%] [G loss: 3.658348]\n",
      "epoch:30 step:23495 [D loss: 0.108881, acc.: 100.00%] [G loss: 3.770670]\n",
      "epoch:30 step:23496 [D loss: 0.016812, acc.: 100.00%] [G loss: 3.689813]\n",
      "epoch:30 step:23497 [D loss: 0.018470, acc.: 100.00%] [G loss: 4.189728]\n",
      "epoch:30 step:23498 [D loss: 0.016194, acc.: 100.00%] [G loss: 4.192532]\n",
      "epoch:30 step:23499 [D loss: 0.017268, acc.: 100.00%] [G loss: 4.493159]\n",
      "epoch:30 step:23500 [D loss: 0.008894, acc.: 100.00%] [G loss: 4.365620]\n",
      "epoch:30 step:23501 [D loss: 0.012977, acc.: 100.00%] [G loss: 4.170265]\n",
      "epoch:30 step:23502 [D loss: 0.023892, acc.: 100.00%] [G loss: 4.386322]\n",
      "epoch:30 step:23503 [D loss: 0.058146, acc.: 98.44%] [G loss: 3.537046]\n",
      "epoch:30 step:23504 [D loss: 0.031812, acc.: 99.22%] [G loss: 3.882275]\n",
      "epoch:30 step:23505 [D loss: 0.018813, acc.: 100.00%] [G loss: 4.119667]\n",
      "epoch:30 step:23506 [D loss: 0.020986, acc.: 100.00%] [G loss: 4.017838]\n",
      "epoch:30 step:23507 [D loss: 0.059728, acc.: 99.22%] [G loss: 3.443706]\n",
      "epoch:30 step:23508 [D loss: 0.024932, acc.: 100.00%] [G loss: 3.212606]\n",
      "epoch:30 step:23509 [D loss: 0.056168, acc.: 99.22%] [G loss: 4.313897]\n",
      "epoch:30 step:23510 [D loss: 0.017788, acc.: 100.00%] [G loss: 5.190317]\n",
      "epoch:30 step:23511 [D loss: 0.017564, acc.: 100.00%] [G loss: 4.625770]\n",
      "epoch:30 step:23512 [D loss: 0.016346, acc.: 100.00%] [G loss: 4.700202]\n",
      "epoch:30 step:23513 [D loss: 0.016735, acc.: 100.00%] [G loss: 4.495603]\n",
      "epoch:30 step:23514 [D loss: 0.026041, acc.: 99.22%] [G loss: 4.214679]\n",
      "epoch:30 step:23515 [D loss: 0.074157, acc.: 100.00%] [G loss: 4.032305]\n",
      "epoch:30 step:23516 [D loss: 0.024768, acc.: 100.00%] [G loss: 3.845041]\n",
      "epoch:30 step:23517 [D loss: 0.012937, acc.: 100.00%] [G loss: 3.688495]\n",
      "epoch:30 step:23518 [D loss: 0.170385, acc.: 97.66%] [G loss: 6.185393]\n",
      "epoch:30 step:23519 [D loss: 0.011503, acc.: 100.00%] [G loss: 7.124397]\n",
      "epoch:30 step:23520 [D loss: 2.995136, acc.: 10.94%] [G loss: 6.809244]\n",
      "epoch:30 step:23521 [D loss: 1.428959, acc.: 51.56%] [G loss: 5.556384]\n",
      "epoch:30 step:23522 [D loss: 0.179019, acc.: 91.41%] [G loss: 4.353786]\n",
      "epoch:30 step:23523 [D loss: 0.034331, acc.: 100.00%] [G loss: 4.169753]\n",
      "epoch:30 step:23524 [D loss: 0.030400, acc.: 100.00%] [G loss: 3.825898]\n",
      "epoch:30 step:23525 [D loss: 0.023256, acc.: 100.00%] [G loss: 2.971559]\n",
      "epoch:30 step:23526 [D loss: 0.047098, acc.: 99.22%] [G loss: 3.881915]\n",
      "epoch:30 step:23527 [D loss: 0.033628, acc.: 100.00%] [G loss: 3.375556]\n",
      "epoch:30 step:23528 [D loss: 0.059709, acc.: 100.00%] [G loss: 3.460757]\n",
      "epoch:30 step:23529 [D loss: 0.138728, acc.: 97.66%] [G loss: 4.295102]\n",
      "epoch:30 step:23530 [D loss: 0.080850, acc.: 98.44%] [G loss: 4.166581]\n",
      "epoch:30 step:23531 [D loss: 0.135075, acc.: 96.09%] [G loss: 2.961287]\n",
      "epoch:30 step:23532 [D loss: 0.069281, acc.: 99.22%] [G loss: 3.936754]\n",
      "epoch:30 step:23533 [D loss: 0.128807, acc.: 94.53%] [G loss: 2.867693]\n",
      "epoch:30 step:23534 [D loss: 0.076837, acc.: 99.22%] [G loss: 1.686224]\n",
      "epoch:30 step:23535 [D loss: 0.155541, acc.: 96.09%] [G loss: 4.340899]\n",
      "epoch:30 step:23536 [D loss: 0.019446, acc.: 100.00%] [G loss: 4.518390]\n",
      "epoch:30 step:23537 [D loss: 0.127145, acc.: 95.31%] [G loss: 2.358896]\n",
      "epoch:30 step:23538 [D loss: 0.169711, acc.: 92.19%] [G loss: 3.658104]\n",
      "epoch:30 step:23539 [D loss: 0.012147, acc.: 100.00%] [G loss: 5.007063]\n",
      "epoch:30 step:23540 [D loss: 0.605791, acc.: 71.09%] [G loss: 2.355494]\n",
      "epoch:30 step:23541 [D loss: 0.069324, acc.: 99.22%] [G loss: 2.912868]\n",
      "epoch:30 step:23542 [D loss: 0.014363, acc.: 100.00%] [G loss: 4.556813]\n",
      "epoch:30 step:23543 [D loss: 0.015980, acc.: 100.00%] [G loss: 4.019152]\n",
      "epoch:30 step:23544 [D loss: 0.019797, acc.: 100.00%] [G loss: 4.509810]\n",
      "epoch:30 step:23545 [D loss: 0.094765, acc.: 97.66%] [G loss: 4.592034]\n",
      "epoch:30 step:23546 [D loss: 0.374291, acc.: 85.16%] [G loss: 3.286765]\n",
      "epoch:30 step:23547 [D loss: 0.043157, acc.: 100.00%] [G loss: 4.967534]\n",
      "epoch:30 step:23548 [D loss: 0.012186, acc.: 100.00%] [G loss: 5.136887]\n",
      "epoch:30 step:23549 [D loss: 0.037171, acc.: 100.00%] [G loss: 4.783270]\n",
      "epoch:30 step:23550 [D loss: 0.027820, acc.: 99.22%] [G loss: 4.278594]\n",
      "epoch:30 step:23551 [D loss: 0.035090, acc.: 100.00%] [G loss: 3.675094]\n",
      "epoch:30 step:23552 [D loss: 0.055370, acc.: 99.22%] [G loss: 3.646825]\n",
      "epoch:30 step:23553 [D loss: 0.112963, acc.: 97.66%] [G loss: 4.646375]\n",
      "epoch:30 step:23554 [D loss: 0.012789, acc.: 100.00%] [G loss: 4.658050]\n",
      "epoch:30 step:23555 [D loss: 0.020847, acc.: 100.00%] [G loss: 4.360935]\n",
      "epoch:30 step:23556 [D loss: 0.029658, acc.: 99.22%] [G loss: 4.651928]\n",
      "epoch:30 step:23557 [D loss: 0.031716, acc.: 100.00%] [G loss: 4.028469]\n",
      "epoch:30 step:23558 [D loss: 0.043572, acc.: 99.22%] [G loss: 3.656437]\n",
      "epoch:30 step:23559 [D loss: 0.022649, acc.: 100.00%] [G loss: 3.904344]\n",
      "epoch:30 step:23560 [D loss: 0.061044, acc.: 98.44%] [G loss: 4.347831]\n",
      "epoch:30 step:23561 [D loss: 0.018374, acc.: 100.00%] [G loss: 4.925874]\n",
      "epoch:30 step:23562 [D loss: 0.244318, acc.: 89.06%] [G loss: 0.893769]\n",
      "epoch:30 step:23563 [D loss: 0.621696, acc.: 72.66%] [G loss: 6.575237]\n",
      "epoch:30 step:23564 [D loss: 1.163013, acc.: 57.81%] [G loss: 4.822457]\n",
      "epoch:30 step:23565 [D loss: 0.058343, acc.: 99.22%] [G loss: 3.191074]\n",
      "epoch:30 step:23566 [D loss: 0.048042, acc.: 100.00%] [G loss: 3.520039]\n",
      "epoch:30 step:23567 [D loss: 0.030356, acc.: 100.00%] [G loss: 3.182322]\n",
      "epoch:30 step:23568 [D loss: 0.029413, acc.: 100.00%] [G loss: 3.657148]\n",
      "epoch:30 step:23569 [D loss: 0.019914, acc.: 100.00%] [G loss: 4.445152]\n",
      "epoch:30 step:23570 [D loss: 0.088094, acc.: 98.44%] [G loss: 3.512129]\n",
      "epoch:30 step:23571 [D loss: 0.040418, acc.: 99.22%] [G loss: 3.473765]\n",
      "epoch:30 step:23572 [D loss: 0.022475, acc.: 100.00%] [G loss: 3.956946]\n",
      "epoch:30 step:23573 [D loss: 0.044263, acc.: 100.00%] [G loss: 4.179191]\n",
      "epoch:30 step:23574 [D loss: 0.042897, acc.: 100.00%] [G loss: 4.190182]\n",
      "epoch:30 step:23575 [D loss: 0.063819, acc.: 98.44%] [G loss: 3.450864]\n",
      "epoch:30 step:23576 [D loss: 0.019073, acc.: 100.00%] [G loss: 3.660222]\n",
      "epoch:30 step:23577 [D loss: 0.018441, acc.: 100.00%] [G loss: 3.229172]\n",
      "epoch:30 step:23578 [D loss: 0.040626, acc.: 100.00%] [G loss: 3.778609]\n",
      "epoch:30 step:23579 [D loss: 0.019357, acc.: 100.00%] [G loss: 4.534458]\n",
      "epoch:30 step:23580 [D loss: 0.053259, acc.: 99.22%] [G loss: 4.077038]\n",
      "epoch:30 step:23581 [D loss: 0.016823, acc.: 100.00%] [G loss: 4.533033]\n",
      "epoch:30 step:23582 [D loss: 0.021103, acc.: 100.00%] [G loss: 3.378844]\n",
      "epoch:30 step:23583 [D loss: 0.137432, acc.: 97.66%] [G loss: 4.928805]\n",
      "epoch:30 step:23584 [D loss: 0.011992, acc.: 100.00%] [G loss: 5.560079]\n",
      "epoch:30 step:23585 [D loss: 0.049650, acc.: 98.44%] [G loss: 4.832695]\n",
      "epoch:30 step:23586 [D loss: 0.038132, acc.: 100.00%] [G loss: 4.581144]\n",
      "epoch:30 step:23587 [D loss: 0.035249, acc.: 100.00%] [G loss: 4.003377]\n",
      "epoch:30 step:23588 [D loss: 0.057153, acc.: 99.22%] [G loss: 4.588522]\n",
      "epoch:30 step:23589 [D loss: 0.009054, acc.: 100.00%] [G loss: 4.522503]\n",
      "epoch:30 step:23590 [D loss: 0.013397, acc.: 100.00%] [G loss: 4.806929]\n",
      "epoch:30 step:23591 [D loss: 0.211919, acc.: 91.41%] [G loss: 2.545414]\n",
      "epoch:30 step:23592 [D loss: 0.075290, acc.: 97.66%] [G loss: 3.699846]\n",
      "epoch:30 step:23593 [D loss: 0.019743, acc.: 100.00%] [G loss: 4.851627]\n",
      "epoch:30 step:23594 [D loss: 0.012642, acc.: 100.00%] [G loss: 4.485264]\n",
      "epoch:30 step:23595 [D loss: 0.011664, acc.: 100.00%] [G loss: 4.216034]\n",
      "epoch:30 step:23596 [D loss: 0.030103, acc.: 99.22%] [G loss: 4.079801]\n",
      "epoch:30 step:23597 [D loss: 0.010451, acc.: 100.00%] [G loss: 3.136582]\n",
      "epoch:30 step:23598 [D loss: 0.037260, acc.: 98.44%] [G loss: 4.102435]\n",
      "epoch:30 step:23599 [D loss: 0.006431, acc.: 100.00%] [G loss: 4.767263]\n",
      "epoch:30 step:23600 [D loss: 0.023827, acc.: 99.22%] [G loss: 4.206645]\n",
      "##############\n",
      "[0.96843436 1.0603325  0.90762894 0.98426866 1.10579914 2.10257069\n",
      " 2.11058213 2.11654461 2.10066775 2.10628721]\n",
      "##########\n",
      "epoch:30 step:23601 [D loss: 0.007962, acc.: 100.00%] [G loss: 5.065056]\n",
      "epoch:30 step:23602 [D loss: 0.007398, acc.: 100.00%] [G loss: 4.851211]\n",
      "epoch:30 step:23603 [D loss: 0.008946, acc.: 100.00%] [G loss: 3.971947]\n",
      "epoch:30 step:23604 [D loss: 0.023892, acc.: 100.00%] [G loss: 3.729292]\n",
      "epoch:30 step:23605 [D loss: 0.011755, acc.: 100.00%] [G loss: 2.867182]\n",
      "epoch:30 step:23606 [D loss: 0.293493, acc.: 85.94%] [G loss: 4.146136]\n",
      "epoch:30 step:23607 [D loss: 0.021186, acc.: 100.00%] [G loss: 5.839919]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:30 step:23608 [D loss: 0.026355, acc.: 100.00%] [G loss: 5.483380]\n",
      "epoch:30 step:23609 [D loss: 0.300681, acc.: 85.94%] [G loss: 5.748619]\n",
      "epoch:30 step:23610 [D loss: 0.004627, acc.: 100.00%] [G loss: 5.533759]\n",
      "epoch:30 step:23611 [D loss: 0.051898, acc.: 99.22%] [G loss: 4.643723]\n",
      "epoch:30 step:23612 [D loss: 0.012635, acc.: 100.00%] [G loss: 4.177495]\n",
      "epoch:30 step:23613 [D loss: 0.041475, acc.: 100.00%] [G loss: 5.719456]\n",
      "epoch:30 step:23614 [D loss: 0.004280, acc.: 100.00%] [G loss: 5.049778]\n",
      "epoch:30 step:23615 [D loss: 0.006109, acc.: 100.00%] [G loss: 5.632955]\n",
      "epoch:30 step:23616 [D loss: 0.004231, acc.: 100.00%] [G loss: 5.212674]\n",
      "epoch:30 step:23617 [D loss: 0.944042, acc.: 50.00%] [G loss: 6.499008]\n",
      "epoch:30 step:23618 [D loss: 0.684777, acc.: 71.88%] [G loss: 3.065120]\n",
      "epoch:30 step:23619 [D loss: 0.366888, acc.: 81.25%] [G loss: 6.329741]\n",
      "epoch:30 step:23620 [D loss: 0.259371, acc.: 87.50%] [G loss: 4.468038]\n",
      "epoch:30 step:23621 [D loss: 0.029654, acc.: 99.22%] [G loss: 3.361028]\n",
      "epoch:30 step:23622 [D loss: 0.066643, acc.: 98.44%] [G loss: 4.282742]\n",
      "epoch:30 step:23623 [D loss: 0.090458, acc.: 98.44%] [G loss: 3.419792]\n",
      "epoch:30 step:23624 [D loss: 0.035537, acc.: 99.22%] [G loss: 4.225050]\n",
      "epoch:30 step:23625 [D loss: 0.059255, acc.: 100.00%] [G loss: 4.133944]\n",
      "epoch:30 step:23626 [D loss: 0.025515, acc.: 100.00%] [G loss: 5.276875]\n",
      "epoch:30 step:23627 [D loss: 0.522421, acc.: 71.09%] [G loss: 5.487877]\n",
      "epoch:30 step:23628 [D loss: 0.059240, acc.: 97.66%] [G loss: 6.959024]\n",
      "epoch:30 step:23629 [D loss: 0.024249, acc.: 100.00%] [G loss: 5.772993]\n",
      "epoch:30 step:23630 [D loss: 0.010586, acc.: 100.00%] [G loss: 5.433560]\n",
      "epoch:30 step:23631 [D loss: 0.021755, acc.: 99.22%] [G loss: 5.606871]\n",
      "epoch:30 step:23632 [D loss: 0.006061, acc.: 100.00%] [G loss: 5.741194]\n",
      "epoch:30 step:23633 [D loss: 0.005218, acc.: 100.00%] [G loss: 5.079258]\n",
      "epoch:30 step:23634 [D loss: 0.020533, acc.: 100.00%] [G loss: 5.264216]\n",
      "epoch:30 step:23635 [D loss: 0.176710, acc.: 92.97%] [G loss: 3.489515]\n",
      "epoch:30 step:23636 [D loss: 0.104733, acc.: 96.09%] [G loss: 5.333675]\n",
      "epoch:30 step:23637 [D loss: 0.001398, acc.: 100.00%] [G loss: 6.912081]\n",
      "epoch:30 step:23638 [D loss: 0.046514, acc.: 98.44%] [G loss: 5.597692]\n",
      "epoch:30 step:23639 [D loss: 0.011385, acc.: 100.00%] [G loss: 5.263909]\n",
      "epoch:30 step:23640 [D loss: 0.028341, acc.: 100.00%] [G loss: 4.791076]\n",
      "epoch:30 step:23641 [D loss: 0.030749, acc.: 100.00%] [G loss: 4.819807]\n",
      "epoch:30 step:23642 [D loss: 0.008742, acc.: 100.00%] [G loss: 5.309379]\n",
      "epoch:30 step:23643 [D loss: 0.015794, acc.: 100.00%] [G loss: 5.184994]\n",
      "epoch:30 step:23644 [D loss: 1.333063, acc.: 47.66%] [G loss: 6.584143]\n",
      "epoch:30 step:23645 [D loss: 0.154288, acc.: 93.75%] [G loss: 7.181670]\n",
      "epoch:30 step:23646 [D loss: 0.216861, acc.: 86.72%] [G loss: 4.795620]\n",
      "epoch:30 step:23647 [D loss: 0.021703, acc.: 100.00%] [G loss: 3.776453]\n",
      "epoch:30 step:23648 [D loss: 0.031527, acc.: 100.00%] [G loss: 2.711632]\n",
      "epoch:30 step:23649 [D loss: 0.063647, acc.: 99.22%] [G loss: 5.164858]\n",
      "epoch:30 step:23650 [D loss: 0.013156, acc.: 100.00%] [G loss: 4.672915]\n",
      "epoch:30 step:23651 [D loss: 0.007178, acc.: 100.00%] [G loss: 4.087811]\n",
      "epoch:30 step:23652 [D loss: 0.007693, acc.: 100.00%] [G loss: 3.737855]\n",
      "epoch:30 step:23653 [D loss: 0.012536, acc.: 100.00%] [G loss: 3.141443]\n",
      "epoch:30 step:23654 [D loss: 0.033893, acc.: 100.00%] [G loss: 3.544636]\n",
      "epoch:30 step:23655 [D loss: 0.063823, acc.: 99.22%] [G loss: 5.729660]\n",
      "epoch:30 step:23656 [D loss: 0.149949, acc.: 92.97%] [G loss: 3.231157]\n",
      "epoch:30 step:23657 [D loss: 0.017369, acc.: 100.00%] [G loss: 4.084242]\n",
      "epoch:30 step:23658 [D loss: 0.041807, acc.: 99.22%] [G loss: 4.578281]\n",
      "epoch:30 step:23659 [D loss: 0.005356, acc.: 100.00%] [G loss: 5.319885]\n",
      "epoch:30 step:23660 [D loss: 0.007847, acc.: 100.00%] [G loss: 5.759439]\n",
      "epoch:30 step:23661 [D loss: 0.005951, acc.: 100.00%] [G loss: 3.950170]\n",
      "epoch:30 step:23662 [D loss: 0.991490, acc.: 56.25%] [G loss: 8.584742]\n",
      "epoch:30 step:23663 [D loss: 2.791122, acc.: 50.00%] [G loss: 6.356607]\n",
      "epoch:30 step:23664 [D loss: 1.931865, acc.: 50.00%] [G loss: 2.081522]\n",
      "epoch:30 step:23665 [D loss: 0.538942, acc.: 77.34%] [G loss: 3.636039]\n",
      "epoch:30 step:23666 [D loss: 0.079288, acc.: 98.44%] [G loss: 4.491163]\n",
      "epoch:30 step:23667 [D loss: 0.304335, acc.: 82.81%] [G loss: 2.439459]\n",
      "epoch:30 step:23668 [D loss: 0.156857, acc.: 94.53%] [G loss: 3.479078]\n",
      "epoch:30 step:23669 [D loss: 0.042680, acc.: 99.22%] [G loss: 4.067673]\n",
      "epoch:30 step:23670 [D loss: 0.369174, acc.: 84.38%] [G loss: 2.686856]\n",
      "epoch:30 step:23671 [D loss: 0.046641, acc.: 100.00%] [G loss: 3.148898]\n",
      "epoch:30 step:23672 [D loss: 0.034160, acc.: 100.00%] [G loss: 2.924959]\n",
      "epoch:30 step:23673 [D loss: 0.051768, acc.: 99.22%] [G loss: 3.582585]\n",
      "epoch:30 step:23674 [D loss: 0.028892, acc.: 100.00%] [G loss: 2.815417]\n",
      "epoch:30 step:23675 [D loss: 0.085651, acc.: 98.44%] [G loss: 3.714926]\n",
      "epoch:30 step:23676 [D loss: 0.021124, acc.: 100.00%] [G loss: 3.576701]\n",
      "epoch:30 step:23677 [D loss: 0.120911, acc.: 97.66%] [G loss: 1.254651]\n",
      "epoch:30 step:23678 [D loss: 0.092358, acc.: 99.22%] [G loss: 2.567984]\n",
      "epoch:30 step:23679 [D loss: 0.053022, acc.: 99.22%] [G loss: 3.015182]\n",
      "epoch:30 step:23680 [D loss: 0.082211, acc.: 98.44%] [G loss: 2.515976]\n",
      "epoch:30 step:23681 [D loss: 0.027783, acc.: 100.00%] [G loss: 1.493743]\n",
      "epoch:30 step:23682 [D loss: 0.037264, acc.: 100.00%] [G loss: 3.052314]\n",
      "epoch:30 step:23683 [D loss: 0.025705, acc.: 100.00%] [G loss: 1.424714]\n",
      "epoch:30 step:23684 [D loss: 0.021364, acc.: 100.00%] [G loss: 1.142023]\n",
      "epoch:30 step:23685 [D loss: 0.101735, acc.: 99.22%] [G loss: 2.256614]\n",
      "epoch:30 step:23686 [D loss: 0.089565, acc.: 95.31%] [G loss: 2.900974]\n",
      "epoch:30 step:23687 [D loss: 0.023933, acc.: 100.00%] [G loss: 2.344526]\n",
      "epoch:30 step:23688 [D loss: 0.129441, acc.: 98.44%] [G loss: 4.137467]\n",
      "epoch:30 step:23689 [D loss: 0.265342, acc.: 90.62%] [G loss: 2.824232]\n",
      "epoch:30 step:23690 [D loss: 0.067049, acc.: 99.22%] [G loss: 2.848704]\n",
      "epoch:30 step:23691 [D loss: 0.176749, acc.: 92.97%] [G loss: 2.393635]\n",
      "epoch:30 step:23692 [D loss: 0.011017, acc.: 100.00%] [G loss: 3.088506]\n",
      "epoch:30 step:23693 [D loss: 0.022056, acc.: 100.00%] [G loss: 2.379034]\n",
      "epoch:30 step:23694 [D loss: 0.035637, acc.: 100.00%] [G loss: 2.972537]\n",
      "epoch:30 step:23695 [D loss: 0.232791, acc.: 89.84%] [G loss: 5.531768]\n",
      "epoch:30 step:23696 [D loss: 0.297863, acc.: 88.28%] [G loss: 3.546635]\n",
      "epoch:30 step:23697 [D loss: 0.024269, acc.: 100.00%] [G loss: 2.841313]\n",
      "epoch:30 step:23698 [D loss: 0.031656, acc.: 99.22%] [G loss: 3.054563]\n",
      "epoch:30 step:23699 [D loss: 0.035417, acc.: 100.00%] [G loss: 3.904087]\n",
      "epoch:30 step:23700 [D loss: 0.131112, acc.: 93.75%] [G loss: 4.217014]\n",
      "epoch:30 step:23701 [D loss: 0.014777, acc.: 100.00%] [G loss: 4.498700]\n",
      "epoch:30 step:23702 [D loss: 0.010130, acc.: 100.00%] [G loss: 4.627898]\n",
      "epoch:30 step:23703 [D loss: 0.090913, acc.: 98.44%] [G loss: 3.524780]\n",
      "epoch:30 step:23704 [D loss: 0.051991, acc.: 98.44%] [G loss: 3.599999]\n",
      "epoch:30 step:23705 [D loss: 0.020894, acc.: 100.00%] [G loss: 4.153346]\n",
      "epoch:30 step:23706 [D loss: 0.023110, acc.: 100.00%] [G loss: 4.674353]\n",
      "epoch:30 step:23707 [D loss: 0.040712, acc.: 100.00%] [G loss: 4.945474]\n",
      "epoch:30 step:23708 [D loss: 0.023467, acc.: 99.22%] [G loss: 4.193956]\n",
      "epoch:30 step:23709 [D loss: 0.123108, acc.: 96.88%] [G loss: 4.114986]\n",
      "epoch:30 step:23710 [D loss: 0.015114, acc.: 100.00%] [G loss: 4.584729]\n",
      "epoch:30 step:23711 [D loss: 0.016299, acc.: 100.00%] [G loss: 4.943017]\n",
      "epoch:30 step:23712 [D loss: 0.021459, acc.: 100.00%] [G loss: 4.536797]\n",
      "epoch:30 step:23713 [D loss: 0.012805, acc.: 100.00%] [G loss: 4.317727]\n",
      "epoch:30 step:23714 [D loss: 0.008492, acc.: 100.00%] [G loss: 4.513266]\n",
      "epoch:30 step:23715 [D loss: 0.018441, acc.: 100.00%] [G loss: 4.268828]\n",
      "epoch:30 step:23716 [D loss: 0.595865, acc.: 71.88%] [G loss: 6.764414]\n",
      "epoch:30 step:23717 [D loss: 0.438429, acc.: 78.91%] [G loss: 5.190496]\n",
      "epoch:30 step:23718 [D loss: 0.042490, acc.: 97.66%] [G loss: 4.607916]\n",
      "epoch:30 step:23719 [D loss: 0.028919, acc.: 100.00%] [G loss: 4.731371]\n",
      "epoch:30 step:23720 [D loss: 0.021644, acc.: 100.00%] [G loss: 3.715976]\n",
      "epoch:30 step:23721 [D loss: 0.065751, acc.: 98.44%] [G loss: 4.732970]\n",
      "epoch:30 step:23722 [D loss: 0.012180, acc.: 100.00%] [G loss: 4.669411]\n",
      "epoch:30 step:23723 [D loss: 0.015594, acc.: 99.22%] [G loss: 5.811591]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:30 step:23724 [D loss: 0.019919, acc.: 100.00%] [G loss: 4.599661]\n",
      "epoch:30 step:23725 [D loss: 0.057479, acc.: 99.22%] [G loss: 4.995896]\n",
      "epoch:30 step:23726 [D loss: 0.104061, acc.: 99.22%] [G loss: 3.213846]\n",
      "epoch:30 step:23727 [D loss: 0.025169, acc.: 100.00%] [G loss: 3.829726]\n",
      "epoch:30 step:23728 [D loss: 0.025083, acc.: 100.00%] [G loss: 3.602076]\n",
      "epoch:30 step:23729 [D loss: 0.017686, acc.: 100.00%] [G loss: 4.870740]\n",
      "epoch:30 step:23730 [D loss: 0.014795, acc.: 100.00%] [G loss: 3.991147]\n",
      "epoch:30 step:23731 [D loss: 0.030052, acc.: 100.00%] [G loss: 5.557354]\n",
      "epoch:30 step:23732 [D loss: 0.205684, acc.: 92.19%] [G loss: 3.341754]\n",
      "epoch:30 step:23733 [D loss: 0.006639, acc.: 100.00%] [G loss: 4.071492]\n",
      "epoch:30 step:23734 [D loss: 0.006086, acc.: 100.00%] [G loss: 3.112387]\n",
      "epoch:30 step:23735 [D loss: 0.020065, acc.: 100.00%] [G loss: 3.290394]\n",
      "epoch:30 step:23736 [D loss: 0.055221, acc.: 99.22%] [G loss: 4.747474]\n",
      "epoch:30 step:23737 [D loss: 0.020351, acc.: 100.00%] [G loss: 4.536957]\n",
      "epoch:30 step:23738 [D loss: 0.004758, acc.: 100.00%] [G loss: 3.627898]\n",
      "epoch:30 step:23739 [D loss: 0.407022, acc.: 84.38%] [G loss: 7.135446]\n",
      "epoch:30 step:23740 [D loss: 0.501135, acc.: 75.00%] [G loss: 2.377954]\n",
      "epoch:30 step:23741 [D loss: 0.335497, acc.: 79.69%] [G loss: 7.024281]\n",
      "epoch:30 step:23742 [D loss: 0.003431, acc.: 100.00%] [G loss: 7.969553]\n",
      "epoch:30 step:23743 [D loss: 0.166456, acc.: 92.19%] [G loss: 6.856355]\n",
      "epoch:30 step:23744 [D loss: 0.007546, acc.: 100.00%] [G loss: 5.573215]\n",
      "epoch:30 step:23745 [D loss: 0.007331, acc.: 100.00%] [G loss: 4.138540]\n",
      "epoch:30 step:23746 [D loss: 0.003516, acc.: 100.00%] [G loss: 2.293118]\n",
      "epoch:30 step:23747 [D loss: 0.011166, acc.: 100.00%] [G loss: 2.609628]\n",
      "epoch:30 step:23748 [D loss: 0.143918, acc.: 93.75%] [G loss: 5.273790]\n",
      "epoch:30 step:23749 [D loss: 0.001406, acc.: 100.00%] [G loss: 6.534642]\n",
      "epoch:30 step:23750 [D loss: 2.007310, acc.: 26.56%] [G loss: 7.526990]\n",
      "epoch:30 step:23751 [D loss: 0.475194, acc.: 72.66%] [G loss: 6.755320]\n",
      "epoch:30 step:23752 [D loss: 0.009897, acc.: 100.00%] [G loss: 6.275457]\n",
      "epoch:30 step:23753 [D loss: 0.009001, acc.: 100.00%] [G loss: 5.723977]\n",
      "epoch:30 step:23754 [D loss: 0.050461, acc.: 97.66%] [G loss: 5.542969]\n",
      "epoch:30 step:23755 [D loss: 0.020403, acc.: 99.22%] [G loss: 4.298545]\n",
      "epoch:30 step:23756 [D loss: 0.027479, acc.: 99.22%] [G loss: 4.348279]\n",
      "epoch:30 step:23757 [D loss: 0.020852, acc.: 100.00%] [G loss: 4.011809]\n",
      "epoch:30 step:23758 [D loss: 0.043214, acc.: 98.44%] [G loss: 4.441471]\n",
      "epoch:30 step:23759 [D loss: 0.037895, acc.: 100.00%] [G loss: 4.808336]\n",
      "epoch:30 step:23760 [D loss: 0.008474, acc.: 100.00%] [G loss: 4.898460]\n",
      "epoch:30 step:23761 [D loss: 0.026727, acc.: 100.00%] [G loss: 4.405917]\n",
      "epoch:30 step:23762 [D loss: 0.086868, acc.: 98.44%] [G loss: 3.742076]\n",
      "epoch:30 step:23763 [D loss: 0.018165, acc.: 100.00%] [G loss: 6.252565]\n",
      "epoch:30 step:23764 [D loss: 0.046726, acc.: 99.22%] [G loss: 4.151230]\n",
      "epoch:30 step:23765 [D loss: 0.014065, acc.: 100.00%] [G loss: 3.969820]\n",
      "epoch:30 step:23766 [D loss: 0.199601, acc.: 92.19%] [G loss: 4.231298]\n",
      "epoch:30 step:23767 [D loss: 0.012927, acc.: 100.00%] [G loss: 6.106300]\n",
      "epoch:30 step:23768 [D loss: 1.072023, acc.: 45.31%] [G loss: 6.813930]\n",
      "epoch:30 step:23769 [D loss: 0.044319, acc.: 98.44%] [G loss: 7.365381]\n",
      "epoch:30 step:23770 [D loss: 0.855135, acc.: 64.84%] [G loss: 2.695808]\n",
      "epoch:30 step:23771 [D loss: 0.086005, acc.: 96.88%] [G loss: 4.086747]\n",
      "epoch:30 step:23772 [D loss: 0.006415, acc.: 100.00%] [G loss: 4.593829]\n",
      "epoch:30 step:23773 [D loss: 0.042302, acc.: 100.00%] [G loss: 4.707827]\n",
      "epoch:30 step:23774 [D loss: 0.024046, acc.: 100.00%] [G loss: 3.722573]\n",
      "epoch:30 step:23775 [D loss: 0.020097, acc.: 100.00%] [G loss: 3.302392]\n",
      "epoch:30 step:23776 [D loss: 0.079050, acc.: 98.44%] [G loss: 2.914073]\n",
      "epoch:30 step:23777 [D loss: 0.051623, acc.: 99.22%] [G loss: 3.793742]\n",
      "epoch:30 step:23778 [D loss: 0.044427, acc.: 100.00%] [G loss: 4.438509]\n",
      "epoch:30 step:23779 [D loss: 0.016555, acc.: 100.00%] [G loss: 4.439132]\n",
      "epoch:30 step:23780 [D loss: 0.110759, acc.: 97.66%] [G loss: 1.534561]\n",
      "epoch:30 step:23781 [D loss: 0.188093, acc.: 92.97%] [G loss: 4.335919]\n",
      "epoch:30 step:23782 [D loss: 0.016082, acc.: 99.22%] [G loss: 5.948570]\n",
      "epoch:30 step:23783 [D loss: 0.251042, acc.: 88.28%] [G loss: 2.912143]\n",
      "epoch:30 step:23784 [D loss: 0.441048, acc.: 74.22%] [G loss: 7.477459]\n",
      "epoch:30 step:23785 [D loss: 0.855848, acc.: 65.62%] [G loss: 3.559488]\n",
      "epoch:30 step:23786 [D loss: 0.037357, acc.: 99.22%] [G loss: 1.966641]\n",
      "epoch:30 step:23787 [D loss: 0.362027, acc.: 84.38%] [G loss: 6.717268]\n",
      "epoch:30 step:23788 [D loss: 0.261995, acc.: 86.72%] [G loss: 6.635011]\n",
      "epoch:30 step:23789 [D loss: 0.003703, acc.: 100.00%] [G loss: 6.647317]\n",
      "epoch:30 step:23790 [D loss: 0.006540, acc.: 100.00%] [G loss: 6.737932]\n",
      "epoch:30 step:23791 [D loss: 0.051299, acc.: 98.44%] [G loss: 5.526867]\n",
      "epoch:30 step:23792 [D loss: 0.032354, acc.: 99.22%] [G loss: 5.359186]\n",
      "epoch:30 step:23793 [D loss: 0.004789, acc.: 100.00%] [G loss: 5.435646]\n",
      "epoch:30 step:23794 [D loss: 0.005441, acc.: 100.00%] [G loss: 4.666643]\n",
      "epoch:30 step:23795 [D loss: 0.006159, acc.: 100.00%] [G loss: 4.808107]\n",
      "epoch:30 step:23796 [D loss: 0.058381, acc.: 99.22%] [G loss: 5.364763]\n",
      "epoch:30 step:23797 [D loss: 0.008941, acc.: 100.00%] [G loss: 5.358710]\n",
      "epoch:30 step:23798 [D loss: 0.011832, acc.: 100.00%] [G loss: 4.088961]\n",
      "epoch:30 step:23799 [D loss: 0.043527, acc.: 99.22%] [G loss: 3.666395]\n",
      "epoch:30 step:23800 [D loss: 0.377550, acc.: 84.38%] [G loss: 5.401545]\n",
      "##############\n",
      "[1.08295848 1.11345074 1.11237729 0.91783749 0.96525956 2.09932049\n",
      " 2.11154265 0.89269148 1.00698032 2.11438254]\n",
      "##########\n",
      "epoch:30 step:23801 [D loss: 0.022389, acc.: 100.00%] [G loss: 6.377970]\n",
      "epoch:30 step:23802 [D loss: 0.130223, acc.: 95.31%] [G loss: 5.681579]\n",
      "epoch:30 step:23803 [D loss: 0.018750, acc.: 100.00%] [G loss: 4.291420]\n",
      "epoch:30 step:23804 [D loss: 0.179203, acc.: 92.19%] [G loss: 4.518792]\n",
      "epoch:30 step:23805 [D loss: 0.018936, acc.: 100.00%] [G loss: 4.785233]\n",
      "epoch:30 step:23806 [D loss: 0.065760, acc.: 97.66%] [G loss: 3.054097]\n",
      "epoch:30 step:23807 [D loss: 0.029471, acc.: 100.00%] [G loss: 3.015684]\n",
      "epoch:30 step:23808 [D loss: 0.111536, acc.: 94.53%] [G loss: 5.312392]\n",
      "epoch:30 step:23809 [D loss: 0.009972, acc.: 100.00%] [G loss: 6.401765]\n",
      "epoch:30 step:23810 [D loss: 0.034135, acc.: 99.22%] [G loss: 5.332597]\n",
      "epoch:30 step:23811 [D loss: 0.004680, acc.: 100.00%] [G loss: 5.827108]\n",
      "epoch:30 step:23812 [D loss: 0.045954, acc.: 99.22%] [G loss: 3.771431]\n",
      "epoch:30 step:23813 [D loss: 0.014699, acc.: 100.00%] [G loss: 3.942931]\n",
      "epoch:30 step:23814 [D loss: 0.027371, acc.: 100.00%] [G loss: 4.128115]\n",
      "epoch:30 step:23815 [D loss: 0.045421, acc.: 99.22%] [G loss: 4.337754]\n",
      "epoch:30 step:23816 [D loss: 0.084375, acc.: 96.88%] [G loss: 4.110053]\n",
      "epoch:30 step:23817 [D loss: 0.009135, acc.: 100.00%] [G loss: 3.303765]\n",
      "epoch:30 step:23818 [D loss: 0.164866, acc.: 92.97%] [G loss: 6.744826]\n",
      "epoch:30 step:23819 [D loss: 0.011174, acc.: 100.00%] [G loss: 6.964233]\n",
      "epoch:30 step:23820 [D loss: 0.719486, acc.: 67.19%] [G loss: 1.379873]\n",
      "epoch:30 step:23821 [D loss: 1.118054, acc.: 56.25%] [G loss: 7.676284]\n",
      "epoch:30 step:23822 [D loss: 1.517750, acc.: 55.47%] [G loss: 5.762403]\n",
      "epoch:30 step:23823 [D loss: 0.191227, acc.: 90.62%] [G loss: 2.933030]\n",
      "epoch:30 step:23824 [D loss: 0.195509, acc.: 89.84%] [G loss: 3.682060]\n",
      "epoch:30 step:23825 [D loss: 0.077382, acc.: 95.31%] [G loss: 4.505135]\n",
      "epoch:30 step:23826 [D loss: 0.435569, acc.: 78.12%] [G loss: 3.941669]\n",
      "epoch:30 step:23827 [D loss: 0.306267, acc.: 85.94%] [G loss: 2.130514]\n",
      "epoch:30 step:23828 [D loss: 0.105317, acc.: 95.31%] [G loss: 4.296632]\n",
      "epoch:30 step:23829 [D loss: 0.118331, acc.: 96.88%] [G loss: 2.670091]\n",
      "epoch:30 step:23830 [D loss: 0.715525, acc.: 66.41%] [G loss: 6.169344]\n",
      "epoch:30 step:23831 [D loss: 0.449824, acc.: 77.34%] [G loss: 5.802386]\n",
      "epoch:30 step:23832 [D loss: 0.253561, acc.: 87.50%] [G loss: 3.071151]\n",
      "epoch:30 step:23833 [D loss: 0.077828, acc.: 98.44%] [G loss: 2.888422]\n",
      "epoch:30 step:23834 [D loss: 0.042664, acc.: 99.22%] [G loss: 4.562765]\n",
      "epoch:30 step:23835 [D loss: 0.034827, acc.: 100.00%] [G loss: 2.920426]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:30 step:23836 [D loss: 0.072723, acc.: 99.22%] [G loss: 1.747062]\n",
      "epoch:30 step:23837 [D loss: 0.199048, acc.: 92.19%] [G loss: 4.234146]\n",
      "epoch:30 step:23838 [D loss: 0.098400, acc.: 97.66%] [G loss: 4.140147]\n",
      "epoch:30 step:23839 [D loss: 0.737047, acc.: 61.72%] [G loss: 1.880599]\n",
      "epoch:30 step:23840 [D loss: 0.063029, acc.: 100.00%] [G loss: 2.579496]\n",
      "epoch:30 step:23841 [D loss: 0.089302, acc.: 97.66%] [G loss: 3.965952]\n",
      "epoch:30 step:23842 [D loss: 0.095445, acc.: 98.44%] [G loss: 3.326714]\n",
      "epoch:30 step:23843 [D loss: 0.113639, acc.: 97.66%] [G loss: 1.753742]\n",
      "epoch:30 step:23844 [D loss: 0.336649, acc.: 82.81%] [G loss: 4.274089]\n",
      "epoch:30 step:23845 [D loss: 0.256036, acc.: 91.41%] [G loss: 3.944342]\n",
      "epoch:30 step:23846 [D loss: 0.060220, acc.: 99.22%] [G loss: 3.546100]\n",
      "epoch:30 step:23847 [D loss: 0.059798, acc.: 100.00%] [G loss: 2.947927]\n",
      "epoch:30 step:23848 [D loss: 0.137858, acc.: 96.09%] [G loss: 3.180479]\n",
      "epoch:30 step:23849 [D loss: 0.123570, acc.: 96.88%] [G loss: 4.645777]\n",
      "epoch:30 step:23850 [D loss: 0.381791, acc.: 82.81%] [G loss: 3.513212]\n",
      "epoch:30 step:23851 [D loss: 0.403409, acc.: 84.38%] [G loss: 3.267974]\n",
      "epoch:30 step:23852 [D loss: 0.031037, acc.: 100.00%] [G loss: 4.178673]\n",
      "epoch:30 step:23853 [D loss: 0.032670, acc.: 99.22%] [G loss: 3.762316]\n",
      "epoch:30 step:23854 [D loss: 0.093740, acc.: 99.22%] [G loss: 3.186896]\n",
      "epoch:30 step:23855 [D loss: 0.033022, acc.: 100.00%] [G loss: 4.561805]\n",
      "epoch:30 step:23856 [D loss: 0.052724, acc.: 99.22%] [G loss: 3.117789]\n",
      "epoch:30 step:23857 [D loss: 0.148796, acc.: 93.75%] [G loss: 5.219007]\n",
      "epoch:30 step:23858 [D loss: 0.168845, acc.: 92.97%] [G loss: 3.495077]\n",
      "epoch:30 step:23859 [D loss: 0.072646, acc.: 99.22%] [G loss: 3.432915]\n",
      "epoch:30 step:23860 [D loss: 0.082596, acc.: 98.44%] [G loss: 3.851373]\n",
      "epoch:30 step:23861 [D loss: 0.242965, acc.: 89.84%] [G loss: 3.430905]\n",
      "epoch:30 step:23862 [D loss: 0.018525, acc.: 100.00%] [G loss: 4.136671]\n",
      "epoch:30 step:23863 [D loss: 0.011534, acc.: 100.00%] [G loss: 4.448835]\n",
      "epoch:30 step:23864 [D loss: 0.055633, acc.: 98.44%] [G loss: 3.788294]\n",
      "epoch:30 step:23865 [D loss: 0.015094, acc.: 100.00%] [G loss: 4.538997]\n",
      "epoch:30 step:23866 [D loss: 0.015469, acc.: 100.00%] [G loss: 3.419601]\n",
      "epoch:30 step:23867 [D loss: 0.536161, acc.: 77.34%] [G loss: 6.495507]\n",
      "epoch:30 step:23868 [D loss: 0.309962, acc.: 86.72%] [G loss: 4.881364]\n",
      "epoch:30 step:23869 [D loss: 0.088399, acc.: 96.88%] [G loss: 4.121212]\n",
      "epoch:30 step:23870 [D loss: 0.027328, acc.: 100.00%] [G loss: 4.196798]\n",
      "epoch:30 step:23871 [D loss: 0.033332, acc.: 99.22%] [G loss: 4.317556]\n",
      "epoch:30 step:23872 [D loss: 0.019052, acc.: 100.00%] [G loss: 4.000882]\n",
      "epoch:30 step:23873 [D loss: 0.046486, acc.: 100.00%] [G loss: 4.416122]\n",
      "epoch:30 step:23874 [D loss: 0.042804, acc.: 99.22%] [G loss: 4.530228]\n",
      "epoch:30 step:23875 [D loss: 0.023127, acc.: 100.00%] [G loss: 3.457803]\n",
      "epoch:30 step:23876 [D loss: 0.017981, acc.: 100.00%] [G loss: 3.890395]\n",
      "epoch:30 step:23877 [D loss: 0.087545, acc.: 98.44%] [G loss: 3.621844]\n",
      "epoch:30 step:23878 [D loss: 0.145281, acc.: 95.31%] [G loss: 4.612300]\n",
      "epoch:30 step:23879 [D loss: 0.018817, acc.: 100.00%] [G loss: 5.205016]\n",
      "epoch:30 step:23880 [D loss: 0.041805, acc.: 99.22%] [G loss: 3.972205]\n",
      "epoch:30 step:23881 [D loss: 0.069241, acc.: 99.22%] [G loss: 3.953032]\n",
      "epoch:30 step:23882 [D loss: 0.034345, acc.: 100.00%] [G loss: 3.787479]\n",
      "epoch:30 step:23883 [D loss: 0.151316, acc.: 96.09%] [G loss: 5.548080]\n",
      "epoch:30 step:23884 [D loss: 0.027026, acc.: 100.00%] [G loss: 6.748188]\n",
      "epoch:30 step:23885 [D loss: 0.050371, acc.: 99.22%] [G loss: 5.490411]\n",
      "epoch:30 step:23886 [D loss: 0.011501, acc.: 100.00%] [G loss: 5.191273]\n",
      "epoch:30 step:23887 [D loss: 0.012683, acc.: 100.00%] [G loss: 4.691238]\n",
      "epoch:30 step:23888 [D loss: 0.010571, acc.: 100.00%] [G loss: 4.252905]\n",
      "epoch:30 step:23889 [D loss: 0.064794, acc.: 100.00%] [G loss: 3.815738]\n",
      "epoch:30 step:23890 [D loss: 0.041975, acc.: 99.22%] [G loss: 3.801749]\n",
      "epoch:30 step:23891 [D loss: 0.035815, acc.: 100.00%] [G loss: 4.239000]\n",
      "epoch:30 step:23892 [D loss: 0.012619, acc.: 100.00%] [G loss: 5.030503]\n",
      "epoch:30 step:23893 [D loss: 0.028475, acc.: 99.22%] [G loss: 4.813533]\n",
      "epoch:30 step:23894 [D loss: 0.026833, acc.: 100.00%] [G loss: 4.690678]\n",
      "epoch:30 step:23895 [D loss: 0.009476, acc.: 100.00%] [G loss: 4.832192]\n",
      "epoch:30 step:23896 [D loss: 0.068122, acc.: 99.22%] [G loss: 3.757030]\n",
      "epoch:30 step:23897 [D loss: 0.014790, acc.: 100.00%] [G loss: 4.178554]\n",
      "epoch:30 step:23898 [D loss: 0.035284, acc.: 100.00%] [G loss: 4.234353]\n",
      "epoch:30 step:23899 [D loss: 0.039652, acc.: 99.22%] [G loss: 5.113595]\n",
      "epoch:30 step:23900 [D loss: 0.004473, acc.: 100.00%] [G loss: 5.656144]\n",
      "epoch:30 step:23901 [D loss: 0.061496, acc.: 97.66%] [G loss: 4.258310]\n",
      "epoch:30 step:23902 [D loss: 0.014547, acc.: 100.00%] [G loss: 3.306309]\n",
      "epoch:30 step:23903 [D loss: 0.085112, acc.: 98.44%] [G loss: 5.133601]\n",
      "epoch:30 step:23904 [D loss: 0.021711, acc.: 100.00%] [G loss: 5.766616]\n",
      "epoch:30 step:23905 [D loss: 0.042872, acc.: 100.00%] [G loss: 4.615554]\n",
      "epoch:30 step:23906 [D loss: 0.006213, acc.: 100.00%] [G loss: 4.815042]\n",
      "epoch:30 step:23907 [D loss: 0.093269, acc.: 99.22%] [G loss: 6.900790]\n",
      "epoch:30 step:23908 [D loss: 0.029652, acc.: 100.00%] [G loss: 7.295083]\n",
      "epoch:30 step:23909 [D loss: 0.041012, acc.: 100.00%] [G loss: 6.915275]\n",
      "epoch:30 step:23910 [D loss: 0.003699, acc.: 100.00%] [G loss: 5.902635]\n",
      "epoch:30 step:23911 [D loss: 0.009545, acc.: 100.00%] [G loss: 6.238292]\n",
      "epoch:30 step:23912 [D loss: 0.017816, acc.: 100.00%] [G loss: 5.146392]\n",
      "epoch:30 step:23913 [D loss: 0.004739, acc.: 100.00%] [G loss: 5.543637]\n",
      "epoch:30 step:23914 [D loss: 0.005654, acc.: 100.00%] [G loss: 5.876455]\n",
      "epoch:30 step:23915 [D loss: 0.016639, acc.: 100.00%] [G loss: 5.208342]\n",
      "epoch:30 step:23916 [D loss: 0.010428, acc.: 100.00%] [G loss: 5.730755]\n",
      "epoch:30 step:23917 [D loss: 0.007152, acc.: 100.00%] [G loss: 5.422953]\n",
      "epoch:30 step:23918 [D loss: 0.006156, acc.: 100.00%] [G loss: 5.151716]\n",
      "epoch:30 step:23919 [D loss: 0.008051, acc.: 100.00%] [G loss: 5.280887]\n",
      "epoch:30 step:23920 [D loss: 0.006273, acc.: 100.00%] [G loss: 4.574964]\n",
      "epoch:30 step:23921 [D loss: 0.010534, acc.: 100.00%] [G loss: 4.512555]\n",
      "epoch:30 step:23922 [D loss: 0.315008, acc.: 86.72%] [G loss: 7.957975]\n",
      "epoch:30 step:23923 [D loss: 0.098053, acc.: 96.09%] [G loss: 8.291404]\n",
      "epoch:30 step:23924 [D loss: 0.010157, acc.: 100.00%] [G loss: 8.271597]\n",
      "epoch:30 step:23925 [D loss: 0.076676, acc.: 98.44%] [G loss: 6.842395]\n",
      "epoch:30 step:23926 [D loss: 0.003701, acc.: 100.00%] [G loss: 5.861975]\n",
      "epoch:30 step:23927 [D loss: 0.011809, acc.: 100.00%] [G loss: 6.119431]\n",
      "epoch:30 step:23928 [D loss: 0.006242, acc.: 100.00%] [G loss: 5.701719]\n",
      "epoch:30 step:23929 [D loss: 0.008556, acc.: 100.00%] [G loss: 5.874388]\n",
      "epoch:30 step:23930 [D loss: 0.009057, acc.: 100.00%] [G loss: 6.118855]\n",
      "epoch:30 step:23931 [D loss: 0.465441, acc.: 76.56%] [G loss: 7.062112]\n",
      "epoch:30 step:23932 [D loss: 0.001785, acc.: 100.00%] [G loss: 8.912836]\n",
      "epoch:30 step:23933 [D loss: 0.004946, acc.: 100.00%] [G loss: 8.711583]\n",
      "epoch:30 step:23934 [D loss: 0.012094, acc.: 100.00%] [G loss: 8.533749]\n",
      "epoch:30 step:23935 [D loss: 0.036751, acc.: 99.22%] [G loss: 7.893010]\n",
      "epoch:30 step:23936 [D loss: 0.001411, acc.: 100.00%] [G loss: 7.686687]\n",
      "epoch:30 step:23937 [D loss: 0.005399, acc.: 100.00%] [G loss: 7.400882]\n",
      "epoch:30 step:23938 [D loss: 0.012252, acc.: 100.00%] [G loss: 6.245900]\n",
      "epoch:30 step:23939 [D loss: 0.013337, acc.: 99.22%] [G loss: 6.314184]\n",
      "epoch:30 step:23940 [D loss: 0.005892, acc.: 100.00%] [G loss: 6.234640]\n",
      "epoch:30 step:23941 [D loss: 0.020866, acc.: 100.00%] [G loss: 5.827398]\n",
      "epoch:30 step:23942 [D loss: 0.020944, acc.: 100.00%] [G loss: 4.739654]\n",
      "epoch:30 step:23943 [D loss: 0.025114, acc.: 100.00%] [G loss: 5.404917]\n",
      "epoch:30 step:23944 [D loss: 0.002933, acc.: 100.00%] [G loss: 5.104336]\n",
      "epoch:30 step:23945 [D loss: 0.010956, acc.: 100.00%] [G loss: 4.254189]\n",
      "epoch:30 step:23946 [D loss: 0.016062, acc.: 100.00%] [G loss: 5.414584]\n",
      "epoch:30 step:23947 [D loss: 0.007589, acc.: 100.00%] [G loss: 4.845563]\n",
      "epoch:30 step:23948 [D loss: 0.014460, acc.: 100.00%] [G loss: 4.873650]\n",
      "epoch:30 step:23949 [D loss: 0.006066, acc.: 100.00%] [G loss: 5.044543]\n",
      "epoch:30 step:23950 [D loss: 0.034897, acc.: 100.00%] [G loss: 4.195282]\n",
      "epoch:30 step:23951 [D loss: 0.053375, acc.: 99.22%] [G loss: 3.809458]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:30 step:23952 [D loss: 0.057236, acc.: 99.22%] [G loss: 6.567955]\n",
      "epoch:30 step:23953 [D loss: 2.455955, acc.: 38.28%] [G loss: 11.292074]\n",
      "epoch:30 step:23954 [D loss: 4.939574, acc.: 50.00%] [G loss: 7.881988]\n",
      "epoch:30 step:23955 [D loss: 3.456953, acc.: 50.00%] [G loss: 5.393363]\n",
      "epoch:30 step:23956 [D loss: 2.309097, acc.: 50.00%] [G loss: 3.940620]\n",
      "epoch:30 step:23957 [D loss: 1.496534, acc.: 50.78%] [G loss: 2.636250]\n",
      "epoch:30 step:23958 [D loss: 0.927157, acc.: 52.34%] [G loss: 1.628039]\n",
      "epoch:30 step:23959 [D loss: 0.536568, acc.: 75.00%] [G loss: 1.070737]\n",
      "epoch:30 step:23960 [D loss: 0.356695, acc.: 86.72%] [G loss: 1.266432]\n",
      "epoch:30 step:23961 [D loss: 0.267261, acc.: 93.75%] [G loss: 1.912578]\n",
      "epoch:30 step:23962 [D loss: 0.379438, acc.: 85.16%] [G loss: 1.708277]\n",
      "epoch:30 step:23963 [D loss: 0.520767, acc.: 75.00%] [G loss: 1.787835]\n",
      "epoch:30 step:23964 [D loss: 0.207453, acc.: 96.09%] [G loss: 1.765482]\n",
      "epoch:30 step:23965 [D loss: 0.160370, acc.: 97.66%] [G loss: 2.053535]\n",
      "epoch:30 step:23966 [D loss: 0.316260, acc.: 88.28%] [G loss: 1.873306]\n",
      "epoch:30 step:23967 [D loss: 0.403438, acc.: 83.59%] [G loss: 1.823004]\n",
      "epoch:30 step:23968 [D loss: 0.207758, acc.: 94.53%] [G loss: 2.230623]\n",
      "epoch:30 step:23969 [D loss: 0.166508, acc.: 96.88%] [G loss: 2.322834]\n",
      "epoch:30 step:23970 [D loss: 0.144771, acc.: 98.44%] [G loss: 2.196602]\n",
      "epoch:30 step:23971 [D loss: 0.112008, acc.: 100.00%] [G loss: 1.889034]\n",
      "epoch:30 step:23972 [D loss: 0.157102, acc.: 98.44%] [G loss: 1.847305]\n",
      "epoch:30 step:23973 [D loss: 0.131592, acc.: 99.22%] [G loss: 1.838506]\n",
      "epoch:30 step:23974 [D loss: 0.121909, acc.: 99.22%] [G loss: 2.179153]\n",
      "epoch:30 step:23975 [D loss: 0.245211, acc.: 92.97%] [G loss: 2.681162]\n",
      "epoch:30 step:23976 [D loss: 0.267964, acc.: 89.06%] [G loss: 2.081280]\n",
      "epoch:30 step:23977 [D loss: 0.194485, acc.: 95.31%] [G loss: 2.439172]\n",
      "epoch:30 step:23978 [D loss: 0.106035, acc.: 100.00%] [G loss: 2.620421]\n",
      "epoch:30 step:23979 [D loss: 0.075511, acc.: 100.00%] [G loss: 3.033421]\n",
      "epoch:30 step:23980 [D loss: 0.499840, acc.: 75.78%] [G loss: 2.402258]\n",
      "epoch:30 step:23981 [D loss: 0.098280, acc.: 97.66%] [G loss: 2.859733]\n",
      "epoch:30 step:23982 [D loss: 0.080334, acc.: 100.00%] [G loss: 2.370973]\n",
      "epoch:30 step:23983 [D loss: 0.162805, acc.: 96.09%] [G loss: 2.740088]\n",
      "epoch:30 step:23984 [D loss: 0.101250, acc.: 97.66%] [G loss: 2.836251]\n",
      "epoch:30 step:23985 [D loss: 0.045405, acc.: 100.00%] [G loss: 3.128924]\n",
      "epoch:30 step:23986 [D loss: 0.064471, acc.: 99.22%] [G loss: 3.152043]\n",
      "epoch:30 step:23987 [D loss: 0.087491, acc.: 99.22%] [G loss: 3.153343]\n",
      "epoch:30 step:23988 [D loss: 0.245605, acc.: 96.09%] [G loss: 3.500292]\n",
      "epoch:30 step:23989 [D loss: 0.059184, acc.: 100.00%] [G loss: 3.571595]\n",
      "epoch:30 step:23990 [D loss: 0.109080, acc.: 98.44%] [G loss: 3.307676]\n",
      "epoch:30 step:23991 [D loss: 0.043761, acc.: 99.22%] [G loss: 2.682871]\n",
      "epoch:30 step:23992 [D loss: 0.098187, acc.: 96.88%] [G loss: 1.356573]\n",
      "epoch:30 step:23993 [D loss: 0.220486, acc.: 90.62%] [G loss: 4.010977]\n",
      "epoch:30 step:23994 [D loss: 0.132159, acc.: 92.97%] [G loss: 4.195822]\n",
      "epoch:30 step:23995 [D loss: 0.371795, acc.: 85.16%] [G loss: 1.674003]\n",
      "epoch:30 step:23996 [D loss: 0.251238, acc.: 86.72%] [G loss: 3.308511]\n",
      "epoch:30 step:23997 [D loss: 0.019182, acc.: 100.00%] [G loss: 4.213202]\n",
      "epoch:30 step:23998 [D loss: 0.188073, acc.: 93.75%] [G loss: 3.108620]\n",
      "epoch:30 step:23999 [D loss: 0.093594, acc.: 98.44%] [G loss: 3.581462]\n",
      "epoch:30 step:24000 [D loss: 0.031285, acc.: 99.22%] [G loss: 3.903967]\n",
      "##############\n",
      "[0.9933645  1.11184335 0.96585161 1.00144297 2.10753154 2.11642916\n",
      " 2.11122571 0.8196469  1.10898786 0.9019871 ]\n",
      "##########\n",
      "epoch:30 step:24001 [D loss: 0.068634, acc.: 99.22%] [G loss: 3.081657]\n",
      "epoch:30 step:24002 [D loss: 0.064861, acc.: 99.22%] [G loss: 3.159766]\n",
      "epoch:30 step:24003 [D loss: 0.058092, acc.: 100.00%] [G loss: 3.651367]\n",
      "epoch:30 step:24004 [D loss: 0.092116, acc.: 96.88%] [G loss: 2.676739]\n",
      "epoch:30 step:24005 [D loss: 0.283261, acc.: 89.84%] [G loss: 4.204228]\n",
      "epoch:30 step:24006 [D loss: 0.136379, acc.: 94.53%] [G loss: 3.548363]\n",
      "epoch:30 step:24007 [D loss: 0.283795, acc.: 87.50%] [G loss: 3.262842]\n",
      "epoch:30 step:24008 [D loss: 0.039969, acc.: 100.00%] [G loss: 3.049857]\n",
      "epoch:30 step:24009 [D loss: 0.024061, acc.: 100.00%] [G loss: 3.446432]\n",
      "epoch:30 step:24010 [D loss: 0.021499, acc.: 100.00%] [G loss: 3.435935]\n",
      "epoch:30 step:24011 [D loss: 0.024969, acc.: 100.00%] [G loss: 3.311462]\n",
      "epoch:30 step:24012 [D loss: 0.051537, acc.: 99.22%] [G loss: 3.590016]\n",
      "epoch:30 step:24013 [D loss: 0.044053, acc.: 100.00%] [G loss: 3.409141]\n",
      "epoch:30 step:24014 [D loss: 0.062817, acc.: 99.22%] [G loss: 3.314961]\n",
      "epoch:30 step:24015 [D loss: 0.069094, acc.: 99.22%] [G loss: 3.672482]\n",
      "epoch:30 step:24016 [D loss: 0.030813, acc.: 100.00%] [G loss: 3.899175]\n",
      "epoch:30 step:24017 [D loss: 0.038549, acc.: 100.00%] [G loss: 3.393997]\n",
      "epoch:30 step:24018 [D loss: 0.053757, acc.: 100.00%] [G loss: 3.059161]\n",
      "epoch:30 step:24019 [D loss: 0.022709, acc.: 100.00%] [G loss: 2.735418]\n",
      "epoch:30 step:24020 [D loss: 0.233943, acc.: 91.41%] [G loss: 1.584154]\n",
      "epoch:30 step:24021 [D loss: 0.206118, acc.: 93.75%] [G loss: 4.298306]\n",
      "epoch:30 step:24022 [D loss: 0.092176, acc.: 96.88%] [G loss: 4.663893]\n",
      "epoch:30 step:24023 [D loss: 0.143468, acc.: 96.09%] [G loss: 1.798090]\n",
      "epoch:30 step:24024 [D loss: 0.224304, acc.: 87.50%] [G loss: 4.671706]\n",
      "epoch:30 step:24025 [D loss: 0.031468, acc.: 100.00%] [G loss: 5.492044]\n",
      "epoch:30 step:24026 [D loss: 0.562587, acc.: 72.66%] [G loss: 1.666577]\n",
      "epoch:30 step:24027 [D loss: 0.863896, acc.: 64.84%] [G loss: 6.041352]\n",
      "epoch:30 step:24028 [D loss: 1.345939, acc.: 52.34%] [G loss: 4.886195]\n",
      "epoch:30 step:24029 [D loss: 0.915198, acc.: 60.16%] [G loss: 3.491727]\n",
      "epoch:30 step:24030 [D loss: 0.083450, acc.: 99.22%] [G loss: 2.906170]\n",
      "epoch:30 step:24031 [D loss: 0.086026, acc.: 98.44%] [G loss: 3.056465]\n",
      "epoch:30 step:24032 [D loss: 0.070630, acc.: 99.22%] [G loss: 3.128859]\n",
      "epoch:30 step:24033 [D loss: 0.027909, acc.: 100.00%] [G loss: 3.593644]\n",
      "epoch:30 step:24034 [D loss: 0.038508, acc.: 100.00%] [G loss: 3.386679]\n",
      "epoch:30 step:24035 [D loss: 0.031813, acc.: 100.00%] [G loss: 3.455649]\n",
      "epoch:30 step:24036 [D loss: 0.049033, acc.: 99.22%] [G loss: 3.031923]\n",
      "epoch:30 step:24037 [D loss: 0.038252, acc.: 100.00%] [G loss: 2.691329]\n",
      "epoch:30 step:24038 [D loss: 0.048684, acc.: 100.00%] [G loss: 3.032194]\n",
      "epoch:30 step:24039 [D loss: 0.036846, acc.: 100.00%] [G loss: 3.103797]\n",
      "epoch:30 step:24040 [D loss: 0.066986, acc.: 100.00%] [G loss: 3.158455]\n",
      "epoch:30 step:24041 [D loss: 0.051499, acc.: 100.00%] [G loss: 3.277724]\n",
      "epoch:30 step:24042 [D loss: 0.044916, acc.: 99.22%] [G loss: 3.147372]\n",
      "epoch:30 step:24043 [D loss: 0.093028, acc.: 97.66%] [G loss: 3.211242]\n",
      "epoch:30 step:24044 [D loss: 0.035346, acc.: 100.00%] [G loss: 2.901968]\n",
      "epoch:30 step:24045 [D loss: 0.100570, acc.: 96.09%] [G loss: 3.757919]\n",
      "epoch:30 step:24046 [D loss: 0.293234, acc.: 85.94%] [G loss: 2.002564]\n",
      "epoch:30 step:24047 [D loss: 0.102083, acc.: 96.88%] [G loss: 3.461463]\n",
      "epoch:30 step:24048 [D loss: 0.043556, acc.: 100.00%] [G loss: 3.682254]\n",
      "epoch:30 step:24049 [D loss: 0.079689, acc.: 99.22%] [G loss: 3.408087]\n",
      "epoch:30 step:24050 [D loss: 0.066233, acc.: 100.00%] [G loss: 3.151172]\n",
      "epoch:30 step:24051 [D loss: 0.026984, acc.: 100.00%] [G loss: 2.925185]\n",
      "epoch:30 step:24052 [D loss: 0.033779, acc.: 100.00%] [G loss: 3.524616]\n",
      "epoch:30 step:24053 [D loss: 0.048082, acc.: 99.22%] [G loss: 2.899818]\n",
      "epoch:30 step:24054 [D loss: 0.043961, acc.: 100.00%] [G loss: 2.383927]\n",
      "epoch:30 step:24055 [D loss: 0.026232, acc.: 100.00%] [G loss: 3.455395]\n",
      "epoch:30 step:24056 [D loss: 0.023193, acc.: 100.00%] [G loss: 2.746238]\n",
      "epoch:30 step:24057 [D loss: 0.334760, acc.: 85.94%] [G loss: 2.536065]\n",
      "epoch:30 step:24058 [D loss: 0.057443, acc.: 97.66%] [G loss: 3.754557]\n",
      "epoch:30 step:24059 [D loss: 0.047822, acc.: 100.00%] [G loss: 3.206996]\n",
      "epoch:30 step:24060 [D loss: 0.042118, acc.: 100.00%] [G loss: 3.266013]\n",
      "epoch:30 step:24061 [D loss: 0.067500, acc.: 99.22%] [G loss: 2.843582]\n",
      "epoch:30 step:24062 [D loss: 0.073773, acc.: 100.00%] [G loss: 3.237725]\n",
      "epoch:30 step:24063 [D loss: 0.013231, acc.: 100.00%] [G loss: 3.894277]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:30 step:24064 [D loss: 0.072128, acc.: 99.22%] [G loss: 3.640390]\n",
      "epoch:30 step:24065 [D loss: 0.015019, acc.: 100.00%] [G loss: 2.930073]\n",
      "epoch:30 step:24066 [D loss: 0.080044, acc.: 99.22%] [G loss: 4.014287]\n",
      "epoch:30 step:24067 [D loss: 0.021746, acc.: 100.00%] [G loss: 4.124874]\n",
      "epoch:30 step:24068 [D loss: 0.008887, acc.: 100.00%] [G loss: 3.645070]\n",
      "epoch:30 step:24069 [D loss: 0.139247, acc.: 98.44%] [G loss: 3.741771]\n",
      "epoch:30 step:24070 [D loss: 0.021500, acc.: 100.00%] [G loss: 3.906882]\n",
      "epoch:30 step:24071 [D loss: 0.017401, acc.: 100.00%] [G loss: 4.230547]\n",
      "epoch:30 step:24072 [D loss: 0.097073, acc.: 97.66%] [G loss: 3.112259]\n",
      "epoch:30 step:24073 [D loss: 0.024694, acc.: 100.00%] [G loss: 3.809856]\n",
      "epoch:30 step:24074 [D loss: 0.046254, acc.: 99.22%] [G loss: 3.694138]\n",
      "epoch:30 step:24075 [D loss: 0.040933, acc.: 99.22%] [G loss: 3.781539]\n",
      "epoch:30 step:24076 [D loss: 0.158547, acc.: 96.88%] [G loss: 4.159365]\n",
      "epoch:30 step:24077 [D loss: 0.008553, acc.: 100.00%] [G loss: 4.818253]\n",
      "epoch:30 step:24078 [D loss: 0.085137, acc.: 96.09%] [G loss: 3.772857]\n",
      "epoch:30 step:24079 [D loss: 0.105361, acc.: 96.88%] [G loss: 3.346113]\n",
      "epoch:30 step:24080 [D loss: 0.015938, acc.: 100.00%] [G loss: 4.286280]\n",
      "epoch:30 step:24081 [D loss: 0.026327, acc.: 100.00%] [G loss: 4.273764]\n",
      "epoch:30 step:24082 [D loss: 0.055867, acc.: 99.22%] [G loss: 3.577450]\n",
      "epoch:30 step:24083 [D loss: 0.030848, acc.: 99.22%] [G loss: 4.056547]\n",
      "epoch:30 step:24084 [D loss: 0.054908, acc.: 99.22%] [G loss: 3.099235]\n",
      "epoch:30 step:24085 [D loss: 0.031663, acc.: 100.00%] [G loss: 3.461755]\n",
      "epoch:30 step:24086 [D loss: 0.026684, acc.: 100.00%] [G loss: 4.392523]\n",
      "epoch:30 step:24087 [D loss: 0.017649, acc.: 100.00%] [G loss: 4.649470]\n",
      "epoch:30 step:24088 [D loss: 0.085113, acc.: 99.22%] [G loss: 3.254750]\n",
      "epoch:30 step:24089 [D loss: 0.010922, acc.: 100.00%] [G loss: 3.776856]\n",
      "epoch:30 step:24090 [D loss: 0.030157, acc.: 100.00%] [G loss: 4.075374]\n",
      "epoch:30 step:24091 [D loss: 0.026793, acc.: 99.22%] [G loss: 3.466408]\n",
      "epoch:30 step:24092 [D loss: 0.103255, acc.: 96.88%] [G loss: 5.277531]\n",
      "epoch:30 step:24093 [D loss: 0.015876, acc.: 100.00%] [G loss: 5.680685]\n",
      "epoch:30 step:24094 [D loss: 0.043851, acc.: 98.44%] [G loss: 4.818688]\n",
      "epoch:30 step:24095 [D loss: 0.281100, acc.: 86.72%] [G loss: 5.826613]\n",
      "epoch:30 step:24096 [D loss: 0.003516, acc.: 100.00%] [G loss: 6.320699]\n",
      "epoch:30 step:24097 [D loss: 0.077308, acc.: 98.44%] [G loss: 5.330990]\n",
      "epoch:30 step:24098 [D loss: 0.016570, acc.: 100.00%] [G loss: 4.218534]\n",
      "epoch:30 step:24099 [D loss: 0.028570, acc.: 99.22%] [G loss: 4.425250]\n",
      "epoch:30 step:24100 [D loss: 0.020372, acc.: 100.00%] [G loss: 4.375454]\n",
      "epoch:30 step:24101 [D loss: 0.020107, acc.: 100.00%] [G loss: 5.594595]\n",
      "epoch:30 step:24102 [D loss: 0.004905, acc.: 100.00%] [G loss: 5.103249]\n",
      "epoch:30 step:24103 [D loss: 0.010680, acc.: 100.00%] [G loss: 4.414753]\n",
      "epoch:30 step:24104 [D loss: 0.024454, acc.: 100.00%] [G loss: 4.762781]\n",
      "epoch:30 step:24105 [D loss: 0.025214, acc.: 100.00%] [G loss: 4.446618]\n",
      "epoch:30 step:24106 [D loss: 0.009088, acc.: 100.00%] [G loss: 5.264185]\n",
      "epoch:30 step:24107 [D loss: 0.015552, acc.: 100.00%] [G loss: 4.722190]\n",
      "epoch:30 step:24108 [D loss: 0.024615, acc.: 100.00%] [G loss: 3.565631]\n",
      "epoch:30 step:24109 [D loss: 0.024732, acc.: 100.00%] [G loss: 4.708681]\n",
      "epoch:30 step:24110 [D loss: 0.023370, acc.: 100.00%] [G loss: 5.141747]\n",
      "epoch:30 step:24111 [D loss: 0.007557, acc.: 100.00%] [G loss: 4.800663]\n",
      "epoch:30 step:24112 [D loss: 0.069703, acc.: 99.22%] [G loss: 2.914708]\n",
      "epoch:30 step:24113 [D loss: 0.228415, acc.: 91.41%] [G loss: 7.046023]\n",
      "epoch:30 step:24114 [D loss: 0.601770, acc.: 72.66%] [G loss: 0.458751]\n",
      "epoch:30 step:24115 [D loss: 1.485991, acc.: 56.25%] [G loss: 8.204877]\n",
      "epoch:30 step:24116 [D loss: 2.436990, acc.: 50.00%] [G loss: 6.647908]\n",
      "epoch:30 step:24117 [D loss: 2.041398, acc.: 50.00%] [G loss: 4.360499]\n",
      "epoch:30 step:24118 [D loss: 0.351111, acc.: 81.25%] [G loss: 2.932862]\n",
      "epoch:30 step:24119 [D loss: 0.182739, acc.: 91.41%] [G loss: 3.402079]\n",
      "epoch:30 step:24120 [D loss: 0.037971, acc.: 100.00%] [G loss: 3.519528]\n",
      "epoch:30 step:24121 [D loss: 0.036604, acc.: 100.00%] [G loss: 3.002270]\n",
      "epoch:30 step:24122 [D loss: 0.069335, acc.: 98.44%] [G loss: 3.629857]\n",
      "epoch:30 step:24123 [D loss: 0.033122, acc.: 100.00%] [G loss: 2.910895]\n",
      "epoch:30 step:24124 [D loss: 0.030283, acc.: 100.00%] [G loss: 3.225731]\n",
      "epoch:30 step:24125 [D loss: 0.034984, acc.: 100.00%] [G loss: 2.703409]\n",
      "epoch:30 step:24126 [D loss: 0.064158, acc.: 99.22%] [G loss: 2.571688]\n",
      "epoch:30 step:24127 [D loss: 0.035053, acc.: 100.00%] [G loss: 3.194824]\n",
      "epoch:30 step:24128 [D loss: 0.027212, acc.: 100.00%] [G loss: 3.609729]\n",
      "epoch:30 step:24129 [D loss: 0.019296, acc.: 100.00%] [G loss: 3.125735]\n",
      "epoch:30 step:24130 [D loss: 0.078425, acc.: 100.00%] [G loss: 2.425604]\n",
      "epoch:30 step:24131 [D loss: 0.032221, acc.: 100.00%] [G loss: 3.289677]\n",
      "epoch:30 step:24132 [D loss: 0.024918, acc.: 100.00%] [G loss: 3.681331]\n",
      "epoch:30 step:24133 [D loss: 0.047267, acc.: 100.00%] [G loss: 2.880049]\n",
      "epoch:30 step:24134 [D loss: 0.115038, acc.: 98.44%] [G loss: 2.737027]\n",
      "epoch:30 step:24135 [D loss: 0.031122, acc.: 100.00%] [G loss: 3.237370]\n",
      "epoch:30 step:24136 [D loss: 0.022767, acc.: 100.00%] [G loss: 3.384394]\n",
      "epoch:30 step:24137 [D loss: 0.043019, acc.: 100.00%] [G loss: 3.506044]\n",
      "epoch:30 step:24138 [D loss: 0.034369, acc.: 99.22%] [G loss: 3.617208]\n",
      "epoch:30 step:24139 [D loss: 0.025905, acc.: 100.00%] [G loss: 2.965511]\n",
      "epoch:30 step:24140 [D loss: 0.049709, acc.: 99.22%] [G loss: 3.354757]\n",
      "epoch:30 step:24141 [D loss: 0.188082, acc.: 96.09%] [G loss: 4.122851]\n",
      "epoch:30 step:24142 [D loss: 0.017476, acc.: 100.00%] [G loss: 4.376466]\n",
      "epoch:30 step:24143 [D loss: 0.956200, acc.: 47.66%] [G loss: 3.995575]\n",
      "epoch:30 step:24144 [D loss: 0.065147, acc.: 98.44%] [G loss: 4.169852]\n",
      "epoch:30 step:24145 [D loss: 0.097691, acc.: 97.66%] [G loss: 3.280678]\n",
      "epoch:30 step:24146 [D loss: 0.055094, acc.: 98.44%] [G loss: 3.426069]\n",
      "epoch:30 step:24147 [D loss: 0.019708, acc.: 100.00%] [G loss: 3.818654]\n",
      "epoch:30 step:24148 [D loss: 0.019478, acc.: 100.00%] [G loss: 3.567376]\n",
      "epoch:30 step:24149 [D loss: 0.022260, acc.: 100.00%] [G loss: 3.881740]\n",
      "epoch:30 step:24150 [D loss: 0.044011, acc.: 100.00%] [G loss: 3.486187]\n",
      "epoch:30 step:24151 [D loss: 0.048962, acc.: 99.22%] [G loss: 3.729290]\n",
      "epoch:30 step:24152 [D loss: 0.029936, acc.: 100.00%] [G loss: 3.773088]\n",
      "epoch:30 step:24153 [D loss: 0.042185, acc.: 100.00%] [G loss: 3.449070]\n",
      "epoch:30 step:24154 [D loss: 0.038665, acc.: 100.00%] [G loss: 3.965397]\n",
      "epoch:30 step:24155 [D loss: 0.357794, acc.: 84.38%] [G loss: 4.656246]\n",
      "epoch:30 step:24156 [D loss: 0.007566, acc.: 100.00%] [G loss: 5.199847]\n",
      "epoch:30 step:24157 [D loss: 0.197395, acc.: 92.19%] [G loss: 3.180008]\n",
      "epoch:30 step:24158 [D loss: 0.045968, acc.: 100.00%] [G loss: 2.862502]\n",
      "epoch:30 step:24159 [D loss: 0.012952, acc.: 100.00%] [G loss: 3.213085]\n",
      "epoch:30 step:24160 [D loss: 0.012747, acc.: 100.00%] [G loss: 3.198871]\n",
      "epoch:30 step:24161 [D loss: 0.024087, acc.: 100.00%] [G loss: 3.140161]\n",
      "epoch:30 step:24162 [D loss: 0.010989, acc.: 100.00%] [G loss: 3.577700]\n",
      "epoch:30 step:24163 [D loss: 0.010962, acc.: 100.00%] [G loss: 3.564985]\n",
      "epoch:30 step:24164 [D loss: 0.046139, acc.: 100.00%] [G loss: 3.485638]\n",
      "epoch:30 step:24165 [D loss: 0.056367, acc.: 99.22%] [G loss: 2.914457]\n",
      "epoch:30 step:24166 [D loss: 0.051219, acc.: 100.00%] [G loss: 3.816979]\n",
      "epoch:30 step:24167 [D loss: 0.009826, acc.: 100.00%] [G loss: 4.168092]\n",
      "epoch:30 step:24168 [D loss: 0.022048, acc.: 100.00%] [G loss: 4.493240]\n",
      "epoch:30 step:24169 [D loss: 0.071930, acc.: 100.00%] [G loss: 3.727040]\n",
      "epoch:30 step:24170 [D loss: 0.018774, acc.: 100.00%] [G loss: 3.132095]\n",
      "epoch:30 step:24171 [D loss: 0.029758, acc.: 100.00%] [G loss: 3.570694]\n",
      "epoch:30 step:24172 [D loss: 0.023247, acc.: 100.00%] [G loss: 3.978229]\n",
      "epoch:30 step:24173 [D loss: 0.023960, acc.: 100.00%] [G loss: 3.622535]\n",
      "epoch:30 step:24174 [D loss: 0.020525, acc.: 100.00%] [G loss: 3.784289]\n",
      "epoch:30 step:24175 [D loss: 0.029394, acc.: 100.00%] [G loss: 4.062408]\n",
      "epoch:30 step:24176 [D loss: 0.032132, acc.: 100.00%] [G loss: 4.196347]\n",
      "epoch:30 step:24177 [D loss: 0.031640, acc.: 100.00%] [G loss: 3.344391]\n",
      "epoch:30 step:24178 [D loss: 0.040002, acc.: 100.00%] [G loss: 4.082266]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:30 step:24179 [D loss: 0.017132, acc.: 100.00%] [G loss: 4.133641]\n",
      "epoch:30 step:24180 [D loss: 0.013061, acc.: 100.00%] [G loss: 3.873650]\n",
      "epoch:30 step:24181 [D loss: 0.011811, acc.: 100.00%] [G loss: 4.317990]\n",
      "epoch:30 step:24182 [D loss: 0.027781, acc.: 100.00%] [G loss: 3.958190]\n",
      "epoch:30 step:24183 [D loss: 0.166095, acc.: 94.53%] [G loss: 2.332561]\n",
      "epoch:30 step:24184 [D loss: 0.048267, acc.: 100.00%] [G loss: 3.627817]\n",
      "epoch:30 step:24185 [D loss: 0.015095, acc.: 100.00%] [G loss: 3.951105]\n",
      "epoch:30 step:24186 [D loss: 0.010582, acc.: 100.00%] [G loss: 5.128945]\n",
      "epoch:30 step:24187 [D loss: 0.027427, acc.: 99.22%] [G loss: 4.884300]\n",
      "epoch:30 step:24188 [D loss: 0.010889, acc.: 100.00%] [G loss: 4.179532]\n",
      "epoch:30 step:24189 [D loss: 0.011101, acc.: 100.00%] [G loss: 4.393078]\n",
      "epoch:30 step:24190 [D loss: 0.022145, acc.: 100.00%] [G loss: 3.571945]\n",
      "epoch:30 step:24191 [D loss: 0.018256, acc.: 100.00%] [G loss: 3.963904]\n",
      "epoch:30 step:24192 [D loss: 0.193136, acc.: 96.09%] [G loss: 4.173376]\n",
      "epoch:30 step:24193 [D loss: 0.003863, acc.: 100.00%] [G loss: 5.038131]\n",
      "epoch:30 step:24194 [D loss: 0.007466, acc.: 100.00%] [G loss: 5.289479]\n",
      "epoch:30 step:24195 [D loss: 0.007282, acc.: 100.00%] [G loss: 4.522381]\n",
      "epoch:30 step:24196 [D loss: 0.044486, acc.: 99.22%] [G loss: 3.972931]\n",
      "epoch:30 step:24197 [D loss: 0.018351, acc.: 100.00%] [G loss: 3.792047]\n",
      "epoch:30 step:24198 [D loss: 0.027938, acc.: 100.00%] [G loss: 3.924552]\n",
      "epoch:30 step:24199 [D loss: 0.019396, acc.: 100.00%] [G loss: 4.097370]\n",
      "epoch:30 step:24200 [D loss: 0.018680, acc.: 100.00%] [G loss: 4.491568]\n",
      "##############\n",
      "[0.77404169 0.89264169 2.11173129 0.92614911 2.11259429 0.97638211\n",
      " 2.12024654 0.67806838 1.12265246 0.92097675]\n",
      "##########\n",
      "epoch:30 step:24201 [D loss: 0.056354, acc.: 99.22%] [G loss: 3.964362]\n",
      "epoch:30 step:24202 [D loss: 0.024691, acc.: 100.00%] [G loss: 3.502990]\n",
      "epoch:30 step:24203 [D loss: 0.009181, acc.: 100.00%] [G loss: 3.541704]\n",
      "epoch:30 step:24204 [D loss: 0.010527, acc.: 100.00%] [G loss: 3.236731]\n",
      "epoch:30 step:24205 [D loss: 0.021495, acc.: 100.00%] [G loss: 4.438968]\n",
      "epoch:30 step:24206 [D loss: 0.048778, acc.: 100.00%] [G loss: 5.395314]\n",
      "epoch:30 step:24207 [D loss: 0.014454, acc.: 100.00%] [G loss: 5.018733]\n",
      "epoch:30 step:24208 [D loss: 0.008272, acc.: 100.00%] [G loss: 5.528484]\n",
      "epoch:30 step:24209 [D loss: 0.026877, acc.: 100.00%] [G loss: 4.873219]\n",
      "epoch:30 step:24210 [D loss: 0.007047, acc.: 100.00%] [G loss: 4.478885]\n",
      "epoch:30 step:24211 [D loss: 0.013661, acc.: 100.00%] [G loss: 5.693835]\n",
      "epoch:31 step:24212 [D loss: 0.032965, acc.: 100.00%] [G loss: 4.692788]\n",
      "epoch:31 step:24213 [D loss: 0.023497, acc.: 100.00%] [G loss: 5.201357]\n",
      "epoch:31 step:24214 [D loss: 0.017182, acc.: 100.00%] [G loss: 5.359127]\n",
      "epoch:31 step:24215 [D loss: 0.006604, acc.: 100.00%] [G loss: 3.981381]\n",
      "epoch:31 step:24216 [D loss: 0.012725, acc.: 100.00%] [G loss: 5.189146]\n",
      "epoch:31 step:24217 [D loss: 0.012377, acc.: 100.00%] [G loss: 4.643114]\n",
      "epoch:31 step:24218 [D loss: 0.017226, acc.: 100.00%] [G loss: 5.534784]\n",
      "epoch:31 step:24219 [D loss: 0.005711, acc.: 100.00%] [G loss: 5.779420]\n",
      "epoch:31 step:24220 [D loss: 0.029006, acc.: 100.00%] [G loss: 4.794021]\n",
      "epoch:31 step:24221 [D loss: 0.011081, acc.: 100.00%] [G loss: 4.446335]\n",
      "epoch:31 step:24222 [D loss: 0.016128, acc.: 100.00%] [G loss: 5.048806]\n",
      "epoch:31 step:24223 [D loss: 0.021599, acc.: 100.00%] [G loss: 4.973444]\n",
      "epoch:31 step:24224 [D loss: 0.007506, acc.: 100.00%] [G loss: 4.767347]\n",
      "epoch:31 step:24225 [D loss: 0.029092, acc.: 99.22%] [G loss: 5.717510]\n",
      "epoch:31 step:24226 [D loss: 0.026379, acc.: 100.00%] [G loss: 5.411496]\n",
      "epoch:31 step:24227 [D loss: 0.020770, acc.: 100.00%] [G loss: 4.009111]\n",
      "epoch:31 step:24228 [D loss: 0.012043, acc.: 100.00%] [G loss: 5.102180]\n",
      "epoch:31 step:24229 [D loss: 0.005516, acc.: 100.00%] [G loss: 5.239974]\n",
      "epoch:31 step:24230 [D loss: 0.003402, acc.: 100.00%] [G loss: 5.477235]\n",
      "epoch:31 step:24231 [D loss: 0.006233, acc.: 100.00%] [G loss: 3.961546]\n",
      "epoch:31 step:24232 [D loss: 0.019425, acc.: 99.22%] [G loss: 5.266261]\n",
      "epoch:31 step:24233 [D loss: 0.024336, acc.: 100.00%] [G loss: 4.771746]\n",
      "epoch:31 step:24234 [D loss: 0.314172, acc.: 85.16%] [G loss: 10.447393]\n",
      "epoch:31 step:24235 [D loss: 0.000608, acc.: 100.00%] [G loss: 11.962508]\n",
      "epoch:31 step:24236 [D loss: 1.317419, acc.: 54.69%] [G loss: 1.614504]\n",
      "epoch:31 step:24237 [D loss: 2.187021, acc.: 51.56%] [G loss: 10.078957]\n",
      "epoch:31 step:24238 [D loss: 0.039360, acc.: 97.66%] [G loss: 11.472914]\n",
      "epoch:31 step:24239 [D loss: 3.213639, acc.: 50.00%] [G loss: 7.449696]\n",
      "epoch:31 step:24240 [D loss: 0.004820, acc.: 100.00%] [G loss: 6.320504]\n",
      "epoch:31 step:24241 [D loss: 0.003259, acc.: 100.00%] [G loss: 6.313070]\n",
      "epoch:31 step:24242 [D loss: 0.002629, acc.: 100.00%] [G loss: 5.090655]\n",
      "epoch:31 step:24243 [D loss: 0.002756, acc.: 100.00%] [G loss: 5.540182]\n",
      "epoch:31 step:24244 [D loss: 0.003161, acc.: 100.00%] [G loss: 5.257605]\n",
      "epoch:31 step:24245 [D loss: 0.004519, acc.: 100.00%] [G loss: 5.225591]\n",
      "epoch:31 step:24246 [D loss: 0.004535, acc.: 100.00%] [G loss: 5.294747]\n",
      "epoch:31 step:24247 [D loss: 0.045896, acc.: 98.44%] [G loss: 5.052659]\n",
      "epoch:31 step:24248 [D loss: 0.003364, acc.: 100.00%] [G loss: 4.740205]\n",
      "epoch:31 step:24249 [D loss: 0.008610, acc.: 100.00%] [G loss: 4.494053]\n",
      "epoch:31 step:24250 [D loss: 0.006658, acc.: 100.00%] [G loss: 5.272781]\n",
      "epoch:31 step:24251 [D loss: 0.008178, acc.: 100.00%] [G loss: 4.605691]\n",
      "epoch:31 step:24252 [D loss: 0.009501, acc.: 100.00%] [G loss: 4.370470]\n",
      "epoch:31 step:24253 [D loss: 0.005525, acc.: 100.00%] [G loss: 4.097959]\n",
      "epoch:31 step:24254 [D loss: 0.022280, acc.: 100.00%] [G loss: 4.219474]\n",
      "epoch:31 step:24255 [D loss: 0.018305, acc.: 100.00%] [G loss: 3.154177]\n",
      "epoch:31 step:24256 [D loss: 0.003621, acc.: 100.00%] [G loss: 2.897610]\n",
      "epoch:31 step:24257 [D loss: 0.011371, acc.: 100.00%] [G loss: 1.415386]\n",
      "epoch:31 step:24258 [D loss: 0.018768, acc.: 100.00%] [G loss: 3.615122]\n",
      "epoch:31 step:24259 [D loss: 1.761215, acc.: 51.56%] [G loss: 7.377617]\n",
      "epoch:31 step:24260 [D loss: 1.416601, acc.: 52.34%] [G loss: 7.106851]\n",
      "epoch:31 step:24261 [D loss: 0.104579, acc.: 95.31%] [G loss: 6.606972]\n",
      "epoch:31 step:24262 [D loss: 0.002420, acc.: 100.00%] [G loss: 5.924887]\n",
      "epoch:31 step:24263 [D loss: 0.111957, acc.: 96.88%] [G loss: 5.249828]\n",
      "epoch:31 step:24264 [D loss: 0.006486, acc.: 100.00%] [G loss: 4.982992]\n",
      "epoch:31 step:24265 [D loss: 0.002829, acc.: 100.00%] [G loss: 4.018787]\n",
      "epoch:31 step:24266 [D loss: 0.007533, acc.: 100.00%] [G loss: 4.530295]\n",
      "epoch:31 step:24267 [D loss: 0.017914, acc.: 100.00%] [G loss: 3.537060]\n",
      "epoch:31 step:24268 [D loss: 0.070137, acc.: 99.22%] [G loss: 4.230144]\n",
      "epoch:31 step:24269 [D loss: 0.045711, acc.: 98.44%] [G loss: 3.667711]\n",
      "epoch:31 step:24270 [D loss: 0.057729, acc.: 100.00%] [G loss: 2.726923]\n",
      "epoch:31 step:24271 [D loss: 0.022409, acc.: 100.00%] [G loss: 4.098439]\n",
      "epoch:31 step:24272 [D loss: 0.007069, acc.: 100.00%] [G loss: 2.812759]\n",
      "epoch:31 step:24273 [D loss: 0.026765, acc.: 100.00%] [G loss: 1.353852]\n",
      "epoch:31 step:24274 [D loss: 0.325984, acc.: 83.59%] [G loss: 5.727822]\n",
      "epoch:31 step:24275 [D loss: 0.491152, acc.: 73.44%] [G loss: 4.335124]\n",
      "epoch:31 step:24276 [D loss: 0.161588, acc.: 94.53%] [G loss: 3.223389]\n",
      "epoch:31 step:24277 [D loss: 0.065717, acc.: 98.44%] [G loss: 3.572253]\n",
      "epoch:31 step:24278 [D loss: 0.013775, acc.: 100.00%] [G loss: 3.961359]\n",
      "epoch:31 step:24279 [D loss: 0.017645, acc.: 100.00%] [G loss: 4.384905]\n",
      "epoch:31 step:24280 [D loss: 0.088436, acc.: 98.44%] [G loss: 4.729608]\n",
      "epoch:31 step:24281 [D loss: 0.013263, acc.: 100.00%] [G loss: 5.235973]\n",
      "epoch:31 step:24282 [D loss: 0.444965, acc.: 79.69%] [G loss: 2.204895]\n",
      "epoch:31 step:24283 [D loss: 0.254587, acc.: 87.50%] [G loss: 6.027546]\n",
      "epoch:31 step:24284 [D loss: 0.094052, acc.: 96.88%] [G loss: 7.024563]\n",
      "epoch:31 step:24285 [D loss: 0.172170, acc.: 90.62%] [G loss: 5.397724]\n",
      "epoch:31 step:24286 [D loss: 0.015020, acc.: 100.00%] [G loss: 4.854462]\n",
      "epoch:31 step:24287 [D loss: 0.013488, acc.: 100.00%] [G loss: 4.573792]\n",
      "epoch:31 step:24288 [D loss: 0.287912, acc.: 83.59%] [G loss: 4.555717]\n",
      "epoch:31 step:24289 [D loss: 0.021046, acc.: 100.00%] [G loss: 6.276645]\n",
      "epoch:31 step:24290 [D loss: 0.036067, acc.: 99.22%] [G loss: 5.244610]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:31 step:24291 [D loss: 0.007025, acc.: 100.00%] [G loss: 4.161699]\n",
      "epoch:31 step:24292 [D loss: 0.016162, acc.: 100.00%] [G loss: 4.515982]\n",
      "epoch:31 step:24293 [D loss: 0.025152, acc.: 100.00%] [G loss: 3.472298]\n",
      "epoch:31 step:24294 [D loss: 0.027298, acc.: 100.00%] [G loss: 3.002467]\n",
      "epoch:31 step:24295 [D loss: 0.065201, acc.: 99.22%] [G loss: 2.587900]\n",
      "epoch:31 step:24296 [D loss: 0.629760, acc.: 68.75%] [G loss: 6.188298]\n",
      "epoch:31 step:24297 [D loss: 0.681591, acc.: 66.41%] [G loss: 1.221526]\n",
      "epoch:31 step:24298 [D loss: 0.116433, acc.: 96.88%] [G loss: 1.330133]\n",
      "epoch:31 step:24299 [D loss: 0.011068, acc.: 100.00%] [G loss: 3.468883]\n",
      "epoch:31 step:24300 [D loss: 0.011657, acc.: 100.00%] [G loss: 2.938141]\n",
      "epoch:31 step:24301 [D loss: 0.079329, acc.: 99.22%] [G loss: 2.170608]\n",
      "epoch:31 step:24302 [D loss: 0.085597, acc.: 97.66%] [G loss: 3.498594]\n",
      "epoch:31 step:24303 [D loss: 0.025012, acc.: 100.00%] [G loss: 3.061281]\n",
      "epoch:31 step:24304 [D loss: 0.016249, acc.: 100.00%] [G loss: 2.638043]\n",
      "epoch:31 step:24305 [D loss: 0.177864, acc.: 94.53%] [G loss: 2.517165]\n",
      "epoch:31 step:24306 [D loss: 0.012264, acc.: 100.00%] [G loss: 3.929009]\n",
      "epoch:31 step:24307 [D loss: 0.040962, acc.: 99.22%] [G loss: 2.112237]\n",
      "epoch:31 step:24308 [D loss: 0.219873, acc.: 89.84%] [G loss: 6.650000]\n",
      "epoch:31 step:24309 [D loss: 0.392554, acc.: 84.38%] [G loss: 1.573450]\n",
      "epoch:31 step:24310 [D loss: 0.050477, acc.: 100.00%] [G loss: 2.396260]\n",
      "epoch:31 step:24311 [D loss: 0.009204, acc.: 100.00%] [G loss: 0.986764]\n",
      "epoch:31 step:24312 [D loss: 0.066066, acc.: 99.22%] [G loss: 3.302011]\n",
      "epoch:31 step:24313 [D loss: 0.021699, acc.: 100.00%] [G loss: 4.754777]\n",
      "epoch:31 step:24314 [D loss: 0.099471, acc.: 97.66%] [G loss: 3.772478]\n",
      "epoch:31 step:24315 [D loss: 0.872430, acc.: 54.69%] [G loss: 8.347204]\n",
      "epoch:31 step:24316 [D loss: 2.353456, acc.: 50.00%] [G loss: 6.349672]\n",
      "epoch:31 step:24317 [D loss: 0.313867, acc.: 84.38%] [G loss: 3.755251]\n",
      "epoch:31 step:24318 [D loss: 0.200078, acc.: 90.62%] [G loss: 4.626334]\n",
      "epoch:31 step:24319 [D loss: 0.031166, acc.: 99.22%] [G loss: 4.982260]\n",
      "epoch:31 step:24320 [D loss: 0.006169, acc.: 100.00%] [G loss: 5.129622]\n",
      "epoch:31 step:24321 [D loss: 0.075842, acc.: 98.44%] [G loss: 4.225390]\n",
      "epoch:31 step:24322 [D loss: 0.009706, acc.: 100.00%] [G loss: 3.716543]\n",
      "epoch:31 step:24323 [D loss: 0.024816, acc.: 100.00%] [G loss: 3.836239]\n",
      "epoch:31 step:24324 [D loss: 0.017196, acc.: 100.00%] [G loss: 3.739633]\n",
      "epoch:31 step:24325 [D loss: 0.042996, acc.: 100.00%] [G loss: 4.377175]\n",
      "epoch:31 step:24326 [D loss: 0.013244, acc.: 100.00%] [G loss: 4.376789]\n",
      "epoch:31 step:24327 [D loss: 0.169062, acc.: 94.53%] [G loss: 2.587859]\n",
      "epoch:31 step:24328 [D loss: 0.124220, acc.: 96.09%] [G loss: 4.107004]\n",
      "epoch:31 step:24329 [D loss: 0.011384, acc.: 100.00%] [G loss: 5.198852]\n",
      "epoch:31 step:24330 [D loss: 0.051526, acc.: 99.22%] [G loss: 4.714637]\n",
      "epoch:31 step:24331 [D loss: 0.026927, acc.: 100.00%] [G loss: 3.979239]\n",
      "epoch:31 step:24332 [D loss: 0.045746, acc.: 100.00%] [G loss: 4.042110]\n",
      "epoch:31 step:24333 [D loss: 0.007223, acc.: 100.00%] [G loss: 3.016209]\n",
      "epoch:31 step:24334 [D loss: 0.015658, acc.: 100.00%] [G loss: 2.657437]\n",
      "epoch:31 step:24335 [D loss: 0.070819, acc.: 98.44%] [G loss: 2.668694]\n",
      "epoch:31 step:24336 [D loss: 0.089383, acc.: 98.44%] [G loss: 3.457294]\n",
      "epoch:31 step:24337 [D loss: 0.040760, acc.: 99.22%] [G loss: 3.447186]\n",
      "epoch:31 step:24338 [D loss: 0.065203, acc.: 99.22%] [G loss: 4.278003]\n",
      "epoch:31 step:24339 [D loss: 0.121290, acc.: 94.53%] [G loss: 2.402317]\n",
      "epoch:31 step:24340 [D loss: 0.081224, acc.: 99.22%] [G loss: 3.083587]\n",
      "epoch:31 step:24341 [D loss: 0.021188, acc.: 100.00%] [G loss: 3.658057]\n",
      "epoch:31 step:24342 [D loss: 0.033027, acc.: 98.44%] [G loss: 3.724878]\n",
      "epoch:31 step:24343 [D loss: 0.188572, acc.: 91.41%] [G loss: 0.069245]\n",
      "epoch:31 step:24344 [D loss: 2.748670, acc.: 50.78%] [G loss: 7.173473]\n",
      "epoch:31 step:24345 [D loss: 2.706369, acc.: 50.00%] [G loss: 6.466975]\n",
      "epoch:31 step:24346 [D loss: 2.414187, acc.: 50.00%] [G loss: 4.680879]\n",
      "epoch:31 step:24347 [D loss: 1.122838, acc.: 52.34%] [G loss: 2.983091]\n",
      "epoch:31 step:24348 [D loss: 0.530736, acc.: 75.00%] [G loss: 1.811830]\n",
      "epoch:31 step:24349 [D loss: 0.285791, acc.: 89.84%] [G loss: 2.313691]\n",
      "epoch:31 step:24350 [D loss: 0.125866, acc.: 99.22%] [G loss: 2.180002]\n",
      "epoch:31 step:24351 [D loss: 0.210460, acc.: 93.75%] [G loss: 2.290209]\n",
      "epoch:31 step:24352 [D loss: 0.196976, acc.: 97.66%] [G loss: 2.625459]\n",
      "epoch:31 step:24353 [D loss: 0.187800, acc.: 99.22%] [G loss: 2.382238]\n",
      "epoch:31 step:24354 [D loss: 0.149155, acc.: 97.66%] [G loss: 2.217149]\n",
      "epoch:31 step:24355 [D loss: 0.188667, acc.: 93.75%] [G loss: 2.383883]\n",
      "epoch:31 step:24356 [D loss: 0.127866, acc.: 98.44%] [G loss: 2.394093]\n",
      "epoch:31 step:24357 [D loss: 0.134494, acc.: 97.66%] [G loss: 2.377625]\n",
      "epoch:31 step:24358 [D loss: 0.264465, acc.: 90.62%] [G loss: 1.867178]\n",
      "epoch:31 step:24359 [D loss: 0.194299, acc.: 93.75%] [G loss: 2.595540]\n",
      "epoch:31 step:24360 [D loss: 0.062773, acc.: 100.00%] [G loss: 2.811390]\n",
      "epoch:31 step:24361 [D loss: 0.271769, acc.: 90.62%] [G loss: 1.800111]\n",
      "epoch:31 step:24362 [D loss: 0.390764, acc.: 80.47%] [G loss: 3.577387]\n",
      "epoch:31 step:24363 [D loss: 0.264952, acc.: 85.94%] [G loss: 3.307595]\n",
      "epoch:31 step:24364 [D loss: 0.186341, acc.: 92.97%] [G loss: 2.040058]\n",
      "epoch:31 step:24365 [D loss: 0.234159, acc.: 92.97%] [G loss: 2.467935]\n",
      "epoch:31 step:24366 [D loss: 0.133043, acc.: 97.66%] [G loss: 3.277124]\n",
      "epoch:31 step:24367 [D loss: 0.245577, acc.: 89.06%] [G loss: 2.330771]\n",
      "epoch:31 step:24368 [D loss: 0.173867, acc.: 94.53%] [G loss: 2.482132]\n",
      "epoch:31 step:24369 [D loss: 0.049729, acc.: 100.00%] [G loss: 2.409102]\n",
      "epoch:31 step:24370 [D loss: 0.080469, acc.: 98.44%] [G loss: 3.175890]\n",
      "epoch:31 step:24371 [D loss: 0.263932, acc.: 89.06%] [G loss: 2.672371]\n",
      "epoch:31 step:24372 [D loss: 0.341910, acc.: 85.16%] [G loss: 3.180369]\n",
      "epoch:31 step:24373 [D loss: 0.041720, acc.: 100.00%] [G loss: 4.196712]\n",
      "epoch:31 step:24374 [D loss: 0.163643, acc.: 95.31%] [G loss: 2.921814]\n",
      "epoch:31 step:24375 [D loss: 0.218992, acc.: 91.41%] [G loss: 2.866639]\n",
      "epoch:31 step:24376 [D loss: 0.154306, acc.: 95.31%] [G loss: 2.608219]\n",
      "epoch:31 step:24377 [D loss: 0.124517, acc.: 96.09%] [G loss: 2.799786]\n",
      "epoch:31 step:24378 [D loss: 0.083890, acc.: 100.00%] [G loss: 3.376210]\n",
      "epoch:31 step:24379 [D loss: 0.128223, acc.: 96.09%] [G loss: 3.598017]\n",
      "epoch:31 step:24380 [D loss: 0.073550, acc.: 98.44%] [G loss: 3.723966]\n",
      "epoch:31 step:24381 [D loss: 0.066763, acc.: 99.22%] [G loss: 3.759196]\n",
      "epoch:31 step:24382 [D loss: 0.105397, acc.: 98.44%] [G loss: 2.818970]\n",
      "epoch:31 step:24383 [D loss: 0.194965, acc.: 95.31%] [G loss: 3.281228]\n",
      "epoch:31 step:24384 [D loss: 0.051090, acc.: 100.00%] [G loss: 3.596015]\n",
      "epoch:31 step:24385 [D loss: 0.310967, acc.: 88.28%] [G loss: 3.656342]\n",
      "epoch:31 step:24386 [D loss: 0.044778, acc.: 99.22%] [G loss: 3.826312]\n",
      "epoch:31 step:24387 [D loss: 0.167752, acc.: 94.53%] [G loss: 2.593472]\n",
      "epoch:31 step:24388 [D loss: 0.117622, acc.: 97.66%] [G loss: 3.476875]\n",
      "epoch:31 step:24389 [D loss: 0.070753, acc.: 98.44%] [G loss: 3.235301]\n",
      "epoch:31 step:24390 [D loss: 0.067462, acc.: 98.44%] [G loss: 2.962890]\n",
      "epoch:31 step:24391 [D loss: 0.041565, acc.: 99.22%] [G loss: 2.918193]\n",
      "epoch:31 step:24392 [D loss: 0.557288, acc.: 76.56%] [G loss: 4.378255]\n",
      "epoch:31 step:24393 [D loss: 0.345384, acc.: 82.03%] [G loss: 3.347855]\n",
      "epoch:31 step:24394 [D loss: 0.086649, acc.: 98.44%] [G loss: 2.154433]\n",
      "epoch:31 step:24395 [D loss: 0.132575, acc.: 94.53%] [G loss: 3.278504]\n",
      "epoch:31 step:24396 [D loss: 0.145612, acc.: 97.66%] [G loss: 3.492127]\n",
      "epoch:31 step:24397 [D loss: 0.135570, acc.: 96.09%] [G loss: 2.398834]\n",
      "epoch:31 step:24398 [D loss: 0.050773, acc.: 99.22%] [G loss: 2.417008]\n",
      "epoch:31 step:24399 [D loss: 0.036460, acc.: 100.00%] [G loss: 2.217430]\n",
      "epoch:31 step:24400 [D loss: 0.076327, acc.: 100.00%] [G loss: 2.292293]\n",
      "##############\n",
      "[0.89308985 1.03654423 2.12094545 0.91870262 2.10864405 0.96868847\n",
      " 2.10375727 1.1170804  2.10545768 2.11131595]\n",
      "##########\n",
      "epoch:31 step:24401 [D loss: 0.118364, acc.: 98.44%] [G loss: 2.677193]\n",
      "epoch:31 step:24402 [D loss: 0.131558, acc.: 96.88%] [G loss: 2.235371]\n",
      "epoch:31 step:24403 [D loss: 0.051682, acc.: 99.22%] [G loss: 2.347514]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:31 step:24404 [D loss: 0.148092, acc.: 96.88%] [G loss: 2.167615]\n",
      "epoch:31 step:24405 [D loss: 0.186088, acc.: 94.53%] [G loss: 3.708549]\n",
      "epoch:31 step:24406 [D loss: 0.086166, acc.: 96.09%] [G loss: 3.866760]\n",
      "epoch:31 step:24407 [D loss: 0.533681, acc.: 70.31%] [G loss: 3.676859]\n",
      "epoch:31 step:24408 [D loss: 0.088427, acc.: 97.66%] [G loss: 4.411755]\n",
      "epoch:31 step:24409 [D loss: 0.100735, acc.: 96.88%] [G loss: 4.242784]\n",
      "epoch:31 step:24410 [D loss: 0.161191, acc.: 92.19%] [G loss: 3.974183]\n",
      "epoch:31 step:24411 [D loss: 0.133655, acc.: 95.31%] [G loss: 3.501211]\n",
      "epoch:31 step:24412 [D loss: 0.040221, acc.: 99.22%] [G loss: 3.990648]\n",
      "epoch:31 step:24413 [D loss: 0.517175, acc.: 74.22%] [G loss: 4.828348]\n",
      "epoch:31 step:24414 [D loss: 0.040367, acc.: 99.22%] [G loss: 5.311507]\n",
      "epoch:31 step:24415 [D loss: 0.133984, acc.: 96.09%] [G loss: 3.806279]\n",
      "epoch:31 step:24416 [D loss: 0.046546, acc.: 99.22%] [G loss: 3.360307]\n",
      "epoch:31 step:24417 [D loss: 0.088291, acc.: 97.66%] [G loss: 4.025797]\n",
      "epoch:31 step:24418 [D loss: 0.020432, acc.: 100.00%] [G loss: 4.889620]\n",
      "epoch:31 step:24419 [D loss: 0.021858, acc.: 100.00%] [G loss: 4.406394]\n",
      "epoch:31 step:24420 [D loss: 0.054195, acc.: 99.22%] [G loss: 4.508533]\n",
      "epoch:31 step:24421 [D loss: 0.042790, acc.: 98.44%] [G loss: 4.130662]\n",
      "epoch:31 step:24422 [D loss: 0.043267, acc.: 99.22%] [G loss: 3.997287]\n",
      "epoch:31 step:24423 [D loss: 0.148159, acc.: 96.09%] [G loss: 2.626477]\n",
      "epoch:31 step:24424 [D loss: 0.196898, acc.: 92.19%] [G loss: 5.040978]\n",
      "epoch:31 step:24425 [D loss: 0.022839, acc.: 100.00%] [G loss: 6.167363]\n",
      "epoch:31 step:24426 [D loss: 0.362035, acc.: 83.59%] [G loss: 2.614197]\n",
      "epoch:31 step:24427 [D loss: 0.052435, acc.: 99.22%] [G loss: 3.260806]\n",
      "epoch:31 step:24428 [D loss: 0.050899, acc.: 100.00%] [G loss: 4.128192]\n",
      "epoch:31 step:24429 [D loss: 0.211636, acc.: 94.53%] [G loss: 3.840767]\n",
      "epoch:31 step:24430 [D loss: 0.017904, acc.: 100.00%] [G loss: 3.803666]\n",
      "epoch:31 step:24431 [D loss: 0.060904, acc.: 98.44%] [G loss: 3.385103]\n",
      "epoch:31 step:24432 [D loss: 0.108890, acc.: 96.88%] [G loss: 3.906684]\n",
      "epoch:31 step:24433 [D loss: 0.024556, acc.: 100.00%] [G loss: 4.990603]\n",
      "epoch:31 step:24434 [D loss: 0.093817, acc.: 98.44%] [G loss: 3.346840]\n",
      "epoch:31 step:24435 [D loss: 0.054495, acc.: 100.00%] [G loss: 3.265073]\n",
      "epoch:31 step:24436 [D loss: 0.025027, acc.: 100.00%] [G loss: 3.860413]\n",
      "epoch:31 step:24437 [D loss: 0.054069, acc.: 99.22%] [G loss: 2.569111]\n",
      "epoch:31 step:24438 [D loss: 0.227017, acc.: 91.41%] [G loss: 6.045601]\n",
      "epoch:31 step:24439 [D loss: 0.757749, acc.: 62.50%] [G loss: 5.242504]\n",
      "epoch:31 step:24440 [D loss: 0.129047, acc.: 95.31%] [G loss: 4.638188]\n",
      "epoch:31 step:24441 [D loss: 0.028450, acc.: 99.22%] [G loss: 4.850204]\n",
      "epoch:31 step:24442 [D loss: 0.077248, acc.: 100.00%] [G loss: 6.174309]\n",
      "epoch:31 step:24443 [D loss: 0.193269, acc.: 91.41%] [G loss: 5.311022]\n",
      "epoch:31 step:24444 [D loss: 0.005959, acc.: 100.00%] [G loss: 5.107879]\n",
      "epoch:31 step:24445 [D loss: 0.049528, acc.: 98.44%] [G loss: 4.581897]\n",
      "epoch:31 step:24446 [D loss: 0.036459, acc.: 100.00%] [G loss: 4.541418]\n",
      "epoch:31 step:24447 [D loss: 0.006321, acc.: 100.00%] [G loss: 4.963646]\n",
      "epoch:31 step:24448 [D loss: 0.020205, acc.: 100.00%] [G loss: 4.454871]\n",
      "epoch:31 step:24449 [D loss: 0.030908, acc.: 99.22%] [G loss: 4.238325]\n",
      "epoch:31 step:24450 [D loss: 0.020137, acc.: 100.00%] [G loss: 4.010556]\n",
      "epoch:31 step:24451 [D loss: 4.205046, acc.: 26.56%] [G loss: 7.565105]\n",
      "epoch:31 step:24452 [D loss: 2.323274, acc.: 50.00%] [G loss: 5.978222]\n",
      "epoch:31 step:24453 [D loss: 1.780438, acc.: 50.00%] [G loss: 3.968811]\n",
      "epoch:31 step:24454 [D loss: 0.493300, acc.: 77.34%] [G loss: 2.492502]\n",
      "epoch:31 step:24455 [D loss: 0.136288, acc.: 96.88%] [G loss: 2.770876]\n",
      "epoch:31 step:24456 [D loss: 0.052845, acc.: 100.00%] [G loss: 2.666374]\n",
      "epoch:31 step:24457 [D loss: 0.316080, acc.: 88.28%] [G loss: 2.538486]\n",
      "epoch:31 step:24458 [D loss: 0.069968, acc.: 99.22%] [G loss: 2.663518]\n",
      "epoch:31 step:24459 [D loss: 0.044157, acc.: 100.00%] [G loss: 2.753582]\n",
      "epoch:31 step:24460 [D loss: 0.043823, acc.: 99.22%] [G loss: 2.472274]\n",
      "epoch:31 step:24461 [D loss: 0.441642, acc.: 79.69%] [G loss: 2.394645]\n",
      "epoch:31 step:24462 [D loss: 0.062436, acc.: 99.22%] [G loss: 2.611435]\n",
      "epoch:31 step:24463 [D loss: 0.090827, acc.: 99.22%] [G loss: 2.569947]\n",
      "epoch:31 step:24464 [D loss: 0.069536, acc.: 100.00%] [G loss: 2.848701]\n",
      "epoch:31 step:24465 [D loss: 0.034186, acc.: 100.00%] [G loss: 2.857051]\n",
      "epoch:31 step:24466 [D loss: 0.042731, acc.: 99.22%] [G loss: 3.095949]\n",
      "epoch:31 step:24467 [D loss: 0.106809, acc.: 98.44%] [G loss: 2.886828]\n",
      "epoch:31 step:24468 [D loss: 0.030426, acc.: 100.00%] [G loss: 2.962104]\n",
      "epoch:31 step:24469 [D loss: 0.151213, acc.: 97.66%] [G loss: 2.425615]\n",
      "epoch:31 step:24470 [D loss: 0.050838, acc.: 100.00%] [G loss: 2.281532]\n",
      "epoch:31 step:24471 [D loss: 0.083097, acc.: 99.22%] [G loss: 2.618767]\n",
      "epoch:31 step:24472 [D loss: 0.442500, acc.: 75.00%] [G loss: 4.233564]\n",
      "epoch:31 step:24473 [D loss: 0.215094, acc.: 88.28%] [G loss: 4.148593]\n",
      "epoch:31 step:24474 [D loss: 0.031899, acc.: 99.22%] [G loss: 4.098357]\n",
      "epoch:31 step:24475 [D loss: 0.132843, acc.: 96.88%] [G loss: 2.806127]\n",
      "epoch:31 step:24476 [D loss: 0.057810, acc.: 100.00%] [G loss: 3.063213]\n",
      "epoch:31 step:24477 [D loss: 0.065629, acc.: 100.00%] [G loss: 2.987279]\n",
      "epoch:31 step:24478 [D loss: 0.027420, acc.: 100.00%] [G loss: 2.924361]\n",
      "epoch:31 step:24479 [D loss: 0.035044, acc.: 100.00%] [G loss: 3.642400]\n",
      "epoch:31 step:24480 [D loss: 0.029611, acc.: 100.00%] [G loss: 3.821325]\n",
      "epoch:31 step:24481 [D loss: 0.199520, acc.: 96.88%] [G loss: 3.413765]\n",
      "epoch:31 step:24482 [D loss: 0.034597, acc.: 100.00%] [G loss: 3.567855]\n",
      "epoch:31 step:24483 [D loss: 0.018517, acc.: 100.00%] [G loss: 3.937899]\n",
      "epoch:31 step:24484 [D loss: 0.080964, acc.: 98.44%] [G loss: 3.226711]\n",
      "epoch:31 step:24485 [D loss: 0.023876, acc.: 100.00%] [G loss: 2.689259]\n",
      "epoch:31 step:24486 [D loss: 0.024337, acc.: 100.00%] [G loss: 2.988575]\n",
      "epoch:31 step:24487 [D loss: 0.058535, acc.: 99.22%] [G loss: 3.540137]\n",
      "epoch:31 step:24488 [D loss: 0.038784, acc.: 100.00%] [G loss: 3.683485]\n",
      "epoch:31 step:24489 [D loss: 0.048467, acc.: 100.00%] [G loss: 3.295386]\n",
      "epoch:31 step:24490 [D loss: 0.023847, acc.: 100.00%] [G loss: 4.048911]\n",
      "epoch:31 step:24491 [D loss: 0.026212, acc.: 100.00%] [G loss: 2.972730]\n",
      "epoch:31 step:24492 [D loss: 0.030225, acc.: 100.00%] [G loss: 3.445389]\n",
      "epoch:31 step:24493 [D loss: 0.026872, acc.: 100.00%] [G loss: 3.465404]\n",
      "epoch:31 step:24494 [D loss: 0.029645, acc.: 100.00%] [G loss: 3.814382]\n",
      "epoch:31 step:24495 [D loss: 0.023896, acc.: 100.00%] [G loss: 3.665195]\n",
      "epoch:31 step:24496 [D loss: 0.042759, acc.: 100.00%] [G loss: 2.771538]\n",
      "epoch:31 step:24497 [D loss: 1.109627, acc.: 48.44%] [G loss: 4.149455]\n",
      "epoch:31 step:24498 [D loss: 0.016777, acc.: 100.00%] [G loss: 4.801836]\n",
      "epoch:31 step:24499 [D loss: 0.302103, acc.: 84.38%] [G loss: 3.331774]\n",
      "epoch:31 step:24500 [D loss: 0.028070, acc.: 100.00%] [G loss: 2.952358]\n",
      "epoch:31 step:24501 [D loss: 0.017450, acc.: 100.00%] [G loss: 3.149534]\n",
      "epoch:31 step:24502 [D loss: 0.033494, acc.: 100.00%] [G loss: 3.016331]\n",
      "epoch:31 step:24503 [D loss: 0.039611, acc.: 100.00%] [G loss: 3.603589]\n",
      "epoch:31 step:24504 [D loss: 0.044121, acc.: 100.00%] [G loss: 3.506686]\n",
      "epoch:31 step:24505 [D loss: 0.123249, acc.: 96.88%] [G loss: 3.790594]\n",
      "epoch:31 step:24506 [D loss: 0.031124, acc.: 100.00%] [G loss: 3.795288]\n",
      "epoch:31 step:24507 [D loss: 0.027766, acc.: 100.00%] [G loss: 2.922136]\n",
      "epoch:31 step:24508 [D loss: 0.041125, acc.: 100.00%] [G loss: 3.328743]\n",
      "epoch:31 step:24509 [D loss: 0.020940, acc.: 100.00%] [G loss: 3.592341]\n",
      "epoch:31 step:24510 [D loss: 0.054745, acc.: 100.00%] [G loss: 2.673333]\n",
      "epoch:31 step:24511 [D loss: 0.036459, acc.: 100.00%] [G loss: 3.197348]\n",
      "epoch:31 step:24512 [D loss: 0.008606, acc.: 100.00%] [G loss: 3.839044]\n",
      "epoch:31 step:24513 [D loss: 0.055735, acc.: 100.00%] [G loss: 2.776034]\n",
      "epoch:31 step:24514 [D loss: 0.090094, acc.: 99.22%] [G loss: 2.853421]\n",
      "epoch:31 step:24515 [D loss: 0.052197, acc.: 100.00%] [G loss: 3.584135]\n",
      "epoch:31 step:24516 [D loss: 0.078894, acc.: 98.44%] [G loss: 3.942756]\n",
      "epoch:31 step:24517 [D loss: 0.016416, acc.: 100.00%] [G loss: 4.059247]\n",
      "epoch:31 step:24518 [D loss: 0.013398, acc.: 100.00%] [G loss: 4.551567]\n",
      "epoch:31 step:24519 [D loss: 0.042574, acc.: 99.22%] [G loss: 4.041507]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:31 step:24520 [D loss: 0.023671, acc.: 100.00%] [G loss: 3.754931]\n",
      "epoch:31 step:24521 [D loss: 0.040014, acc.: 100.00%] [G loss: 3.228244]\n",
      "epoch:31 step:24522 [D loss: 0.027288, acc.: 100.00%] [G loss: 3.766458]\n",
      "epoch:31 step:24523 [D loss: 0.137475, acc.: 96.88%] [G loss: 4.511462]\n",
      "epoch:31 step:24524 [D loss: 0.019195, acc.: 99.22%] [G loss: 4.025664]\n",
      "epoch:31 step:24525 [D loss: 0.007850, acc.: 100.00%] [G loss: 4.215789]\n",
      "epoch:31 step:24526 [D loss: 0.121955, acc.: 96.88%] [G loss: 3.719783]\n",
      "epoch:31 step:24527 [D loss: 0.009119, acc.: 100.00%] [G loss: 4.612764]\n",
      "epoch:31 step:24528 [D loss: 0.039143, acc.: 99.22%] [G loss: 4.843140]\n",
      "epoch:31 step:24529 [D loss: 0.007908, acc.: 100.00%] [G loss: 5.093028]\n",
      "epoch:31 step:24530 [D loss: 0.007635, acc.: 100.00%] [G loss: 5.094068]\n",
      "epoch:31 step:24531 [D loss: 0.045028, acc.: 98.44%] [G loss: 4.637000]\n",
      "epoch:31 step:24532 [D loss: 0.028672, acc.: 100.00%] [G loss: 4.154998]\n",
      "epoch:31 step:24533 [D loss: 0.006608, acc.: 100.00%] [G loss: 4.870308]\n",
      "epoch:31 step:24534 [D loss: 0.013326, acc.: 100.00%] [G loss: 4.816563]\n",
      "epoch:31 step:24535 [D loss: 0.015316, acc.: 100.00%] [G loss: 4.615698]\n",
      "epoch:31 step:24536 [D loss: 0.040891, acc.: 100.00%] [G loss: 3.758217]\n",
      "epoch:31 step:24537 [D loss: 0.013473, acc.: 100.00%] [G loss: 4.651415]\n",
      "epoch:31 step:24538 [D loss: 0.006556, acc.: 100.00%] [G loss: 4.214777]\n",
      "epoch:31 step:24539 [D loss: 0.012792, acc.: 100.00%] [G loss: 4.459234]\n",
      "epoch:31 step:24540 [D loss: 0.012910, acc.: 100.00%] [G loss: 4.144617]\n",
      "epoch:31 step:24541 [D loss: 0.015015, acc.: 100.00%] [G loss: 3.069739]\n",
      "epoch:31 step:24542 [D loss: 0.040172, acc.: 100.00%] [G loss: 3.958379]\n",
      "epoch:31 step:24543 [D loss: 0.010743, acc.: 100.00%] [G loss: 5.434605]\n",
      "epoch:31 step:24544 [D loss: 0.016962, acc.: 100.00%] [G loss: 4.440042]\n",
      "epoch:31 step:24545 [D loss: 0.052489, acc.: 99.22%] [G loss: 3.540612]\n",
      "epoch:31 step:24546 [D loss: 0.034681, acc.: 100.00%] [G loss: 3.358738]\n",
      "epoch:31 step:24547 [D loss: 2.661826, acc.: 39.84%] [G loss: 7.534772]\n",
      "epoch:31 step:24548 [D loss: 2.242488, acc.: 50.00%] [G loss: 5.118804]\n",
      "epoch:31 step:24549 [D loss: 0.976978, acc.: 57.03%] [G loss: 2.866723]\n",
      "epoch:31 step:24550 [D loss: 0.306924, acc.: 85.16%] [G loss: 2.783029]\n",
      "epoch:31 step:24551 [D loss: 0.075993, acc.: 99.22%] [G loss: 3.018873]\n",
      "epoch:31 step:24552 [D loss: 0.057329, acc.: 100.00%] [G loss: 3.113949]\n",
      "epoch:31 step:24553 [D loss: 0.039862, acc.: 100.00%] [G loss: 3.517225]\n",
      "epoch:31 step:24554 [D loss: 0.215986, acc.: 92.19%] [G loss: 2.472768]\n",
      "epoch:31 step:24555 [D loss: 0.075694, acc.: 100.00%] [G loss: 2.398239]\n",
      "epoch:31 step:24556 [D loss: 0.106642, acc.: 98.44%] [G loss: 2.645158]\n",
      "epoch:31 step:24557 [D loss: 0.199321, acc.: 95.31%] [G loss: 3.703224]\n",
      "epoch:31 step:24558 [D loss: 0.068062, acc.: 99.22%] [G loss: 2.967948]\n",
      "epoch:31 step:24559 [D loss: 0.051574, acc.: 100.00%] [G loss: 3.090277]\n",
      "epoch:31 step:24560 [D loss: 0.020991, acc.: 100.00%] [G loss: 3.455928]\n",
      "epoch:31 step:24561 [D loss: 0.037609, acc.: 100.00%] [G loss: 3.870552]\n",
      "epoch:31 step:24562 [D loss: 0.246073, acc.: 90.62%] [G loss: 3.105737]\n",
      "epoch:31 step:24563 [D loss: 0.039080, acc.: 99.22%] [G loss: 2.904924]\n",
      "epoch:31 step:24564 [D loss: 0.040617, acc.: 100.00%] [G loss: 3.381036]\n",
      "epoch:31 step:24565 [D loss: 0.042690, acc.: 100.00%] [G loss: 3.151619]\n",
      "epoch:31 step:24566 [D loss: 0.162859, acc.: 96.09%] [G loss: 3.579764]\n",
      "epoch:31 step:24567 [D loss: 0.028708, acc.: 99.22%] [G loss: 3.859803]\n",
      "epoch:31 step:24568 [D loss: 0.040530, acc.: 100.00%] [G loss: 3.687284]\n",
      "epoch:31 step:24569 [D loss: 0.385163, acc.: 83.59%] [G loss: 3.509552]\n",
      "epoch:31 step:24570 [D loss: 0.015824, acc.: 100.00%] [G loss: 5.001280]\n",
      "epoch:31 step:24571 [D loss: 0.063294, acc.: 98.44%] [G loss: 4.342996]\n",
      "epoch:31 step:24572 [D loss: 0.082043, acc.: 98.44%] [G loss: 3.066532]\n",
      "epoch:31 step:24573 [D loss: 0.036236, acc.: 100.00%] [G loss: 2.796212]\n",
      "epoch:31 step:24574 [D loss: 0.012708, acc.: 100.00%] [G loss: 3.451848]\n",
      "epoch:31 step:24575 [D loss: 0.116075, acc.: 100.00%] [G loss: 2.926296]\n",
      "epoch:31 step:24576 [D loss: 0.014847, acc.: 100.00%] [G loss: 3.324370]\n",
      "epoch:31 step:24577 [D loss: 0.042988, acc.: 100.00%] [G loss: 3.408264]\n",
      "epoch:31 step:24578 [D loss: 0.020915, acc.: 100.00%] [G loss: 4.045578]\n",
      "epoch:31 step:24579 [D loss: 0.050201, acc.: 99.22%] [G loss: 4.376081]\n",
      "epoch:31 step:24580 [D loss: 0.019080, acc.: 100.00%] [G loss: 4.718903]\n",
      "epoch:31 step:24581 [D loss: 0.536258, acc.: 73.44%] [G loss: 5.096938]\n",
      "epoch:31 step:24582 [D loss: 0.027529, acc.: 99.22%] [G loss: 6.272772]\n",
      "epoch:31 step:24583 [D loss: 0.282482, acc.: 85.16%] [G loss: 2.664669]\n",
      "epoch:31 step:24584 [D loss: 0.173945, acc.: 92.19%] [G loss: 4.546692]\n",
      "epoch:31 step:24585 [D loss: 0.035681, acc.: 99.22%] [G loss: 5.159749]\n",
      "epoch:31 step:24586 [D loss: 0.105089, acc.: 96.09%] [G loss: 3.295645]\n",
      "epoch:31 step:24587 [D loss: 0.046220, acc.: 99.22%] [G loss: 3.020918]\n",
      "epoch:31 step:24588 [D loss: 0.037184, acc.: 100.00%] [G loss: 3.907688]\n",
      "epoch:31 step:24589 [D loss: 0.011925, acc.: 100.00%] [G loss: 4.103909]\n",
      "epoch:31 step:24590 [D loss: 0.021353, acc.: 100.00%] [G loss: 4.362472]\n",
      "epoch:31 step:24591 [D loss: 0.009111, acc.: 100.00%] [G loss: 4.157348]\n",
      "epoch:31 step:24592 [D loss: 0.011128, acc.: 100.00%] [G loss: 4.196007]\n",
      "epoch:31 step:24593 [D loss: 0.013711, acc.: 100.00%] [G loss: 4.344950]\n",
      "epoch:31 step:24594 [D loss: 0.010429, acc.: 100.00%] [G loss: 3.668932]\n",
      "epoch:31 step:24595 [D loss: 0.007985, acc.: 100.00%] [G loss: 4.252428]\n",
      "epoch:31 step:24596 [D loss: 0.035375, acc.: 99.22%] [G loss: 3.138373]\n",
      "epoch:31 step:24597 [D loss: 0.055969, acc.: 100.00%] [G loss: 4.183468]\n",
      "epoch:31 step:24598 [D loss: 0.020994, acc.: 100.00%] [G loss: 4.414717]\n",
      "epoch:31 step:24599 [D loss: 0.021386, acc.: 100.00%] [G loss: 4.578825]\n",
      "epoch:31 step:24600 [D loss: 0.014361, acc.: 100.00%] [G loss: 3.745847]\n",
      "##############\n",
      "[0.8759088  0.90541849 0.90265937 0.95638473 0.88431489 1.10710458\n",
      " 2.11722655 2.10887639 2.1129091  1.01380154]\n",
      "##########\n",
      "epoch:31 step:24601 [D loss: 0.020175, acc.: 100.00%] [G loss: 4.018874]\n",
      "epoch:31 step:24602 [D loss: 0.011505, acc.: 100.00%] [G loss: 3.663568]\n",
      "epoch:31 step:24603 [D loss: 0.020427, acc.: 100.00%] [G loss: 3.385151]\n",
      "epoch:31 step:24604 [D loss: 0.056884, acc.: 99.22%] [G loss: 2.963242]\n",
      "epoch:31 step:24605 [D loss: 0.045191, acc.: 100.00%] [G loss: 3.542243]\n",
      "epoch:31 step:24606 [D loss: 0.042189, acc.: 100.00%] [G loss: 3.483151]\n",
      "epoch:31 step:24607 [D loss: 0.046627, acc.: 99.22%] [G loss: 2.858073]\n",
      "epoch:31 step:24608 [D loss: 0.026222, acc.: 100.00%] [G loss: 2.747949]\n",
      "epoch:31 step:24609 [D loss: 0.024639, acc.: 100.00%] [G loss: 3.971384]\n",
      "epoch:31 step:24610 [D loss: 0.011232, acc.: 100.00%] [G loss: 3.818928]\n",
      "epoch:31 step:24611 [D loss: 0.014638, acc.: 100.00%] [G loss: 4.484685]\n",
      "epoch:31 step:24612 [D loss: 0.760845, acc.: 55.47%] [G loss: 6.115650]\n",
      "epoch:31 step:24613 [D loss: 1.546630, acc.: 53.12%] [G loss: 3.529592]\n",
      "epoch:31 step:24614 [D loss: 0.234591, acc.: 90.62%] [G loss: 2.973407]\n",
      "epoch:31 step:24615 [D loss: 0.067680, acc.: 97.66%] [G loss: 3.267958]\n",
      "epoch:31 step:24616 [D loss: 0.079609, acc.: 97.66%] [G loss: 2.789223]\n",
      "epoch:31 step:24617 [D loss: 0.038812, acc.: 99.22%] [G loss: 3.794109]\n",
      "epoch:31 step:24618 [D loss: 0.059169, acc.: 98.44%] [G loss: 3.799280]\n",
      "epoch:31 step:24619 [D loss: 0.045114, acc.: 99.22%] [G loss: 4.052909]\n",
      "epoch:31 step:24620 [D loss: 0.081937, acc.: 99.22%] [G loss: 3.270949]\n",
      "epoch:31 step:24621 [D loss: 0.016871, acc.: 100.00%] [G loss: 3.317618]\n",
      "epoch:31 step:24622 [D loss: 0.052088, acc.: 100.00%] [G loss: 3.211655]\n",
      "epoch:31 step:24623 [D loss: 0.020028, acc.: 100.00%] [G loss: 4.101576]\n",
      "epoch:31 step:24624 [D loss: 0.031828, acc.: 100.00%] [G loss: 3.473582]\n",
      "epoch:31 step:24625 [D loss: 0.026017, acc.: 100.00%] [G loss: 3.480881]\n",
      "epoch:31 step:24626 [D loss: 0.013466, acc.: 100.00%] [G loss: 3.238206]\n",
      "epoch:31 step:24627 [D loss: 0.050545, acc.: 99.22%] [G loss: 3.512943]\n",
      "epoch:31 step:24628 [D loss: 0.049534, acc.: 100.00%] [G loss: 3.356184]\n",
      "epoch:31 step:24629 [D loss: 0.020933, acc.: 100.00%] [G loss: 3.217423]\n",
      "epoch:31 step:24630 [D loss: 0.028958, acc.: 100.00%] [G loss: 3.437814]\n",
      "epoch:31 step:24631 [D loss: 0.165047, acc.: 97.66%] [G loss: 3.288589]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:31 step:24632 [D loss: 0.030233, acc.: 100.00%] [G loss: 4.105133]\n",
      "epoch:31 step:24633 [D loss: 0.039744, acc.: 100.00%] [G loss: 4.030577]\n",
      "epoch:31 step:24634 [D loss: 0.016603, acc.: 100.00%] [G loss: 4.030709]\n",
      "epoch:31 step:24635 [D loss: 0.269358, acc.: 89.84%] [G loss: 3.725072]\n",
      "epoch:31 step:24636 [D loss: 0.009964, acc.: 100.00%] [G loss: 5.646160]\n",
      "epoch:31 step:24637 [D loss: 0.012914, acc.: 100.00%] [G loss: 4.570612]\n",
      "epoch:31 step:24638 [D loss: 0.006524, acc.: 100.00%] [G loss: 4.910575]\n",
      "epoch:31 step:24639 [D loss: 0.034928, acc.: 100.00%] [G loss: 3.951076]\n",
      "epoch:31 step:24640 [D loss: 0.046448, acc.: 100.00%] [G loss: 4.050048]\n",
      "epoch:31 step:24641 [D loss: 0.020320, acc.: 100.00%] [G loss: 3.389090]\n",
      "epoch:31 step:24642 [D loss: 0.787269, acc.: 62.50%] [G loss: 6.242105]\n",
      "epoch:31 step:24643 [D loss: 0.092506, acc.: 96.09%] [G loss: 6.631154]\n",
      "epoch:31 step:24644 [D loss: 0.779284, acc.: 68.75%] [G loss: 2.457730]\n",
      "epoch:31 step:24645 [D loss: 0.257879, acc.: 87.50%] [G loss: 4.276164]\n",
      "epoch:31 step:24646 [D loss: 0.004938, acc.: 100.00%] [G loss: 5.221053]\n",
      "epoch:31 step:24647 [D loss: 0.055986, acc.: 98.44%] [G loss: 4.731903]\n",
      "epoch:31 step:24648 [D loss: 0.029410, acc.: 100.00%] [G loss: 3.532237]\n",
      "epoch:31 step:24649 [D loss: 0.022768, acc.: 100.00%] [G loss: 4.137739]\n",
      "epoch:31 step:24650 [D loss: 0.016468, acc.: 100.00%] [G loss: 3.903711]\n",
      "epoch:31 step:24651 [D loss: 0.013356, acc.: 100.00%] [G loss: 3.572597]\n",
      "epoch:31 step:24652 [D loss: 0.045749, acc.: 99.22%] [G loss: 3.775731]\n",
      "epoch:31 step:24653 [D loss: 0.012886, acc.: 100.00%] [G loss: 4.334177]\n",
      "epoch:31 step:24654 [D loss: 0.016025, acc.: 100.00%] [G loss: 3.588046]\n",
      "epoch:31 step:24655 [D loss: 0.015752, acc.: 100.00%] [G loss: 3.485567]\n",
      "epoch:31 step:24656 [D loss: 0.013837, acc.: 100.00%] [G loss: 2.843305]\n",
      "epoch:31 step:24657 [D loss: 0.113124, acc.: 96.88%] [G loss: 4.975873]\n",
      "epoch:31 step:24658 [D loss: 0.069666, acc.: 98.44%] [G loss: 4.605371]\n",
      "epoch:31 step:24659 [D loss: 0.092892, acc.: 99.22%] [G loss: 4.206113]\n",
      "epoch:31 step:24660 [D loss: 0.011962, acc.: 100.00%] [G loss: 4.943415]\n",
      "epoch:31 step:24661 [D loss: 0.010151, acc.: 100.00%] [G loss: 4.369960]\n",
      "epoch:31 step:24662 [D loss: 0.056758, acc.: 100.00%] [G loss: 2.929930]\n",
      "epoch:31 step:24663 [D loss: 0.021691, acc.: 100.00%] [G loss: 3.307945]\n",
      "epoch:31 step:24664 [D loss: 0.064244, acc.: 99.22%] [G loss: 4.224351]\n",
      "epoch:31 step:24665 [D loss: 0.034271, acc.: 100.00%] [G loss: 4.363288]\n",
      "epoch:31 step:24666 [D loss: 0.014789, acc.: 100.00%] [G loss: 4.693276]\n",
      "epoch:31 step:24667 [D loss: 0.034695, acc.: 99.22%] [G loss: 4.273747]\n",
      "epoch:31 step:24668 [D loss: 0.013821, acc.: 100.00%] [G loss: 4.319304]\n",
      "epoch:31 step:24669 [D loss: 0.016488, acc.: 100.00%] [G loss: 5.311664]\n",
      "epoch:31 step:24670 [D loss: 0.376253, acc.: 82.81%] [G loss: 5.708904]\n",
      "epoch:31 step:24671 [D loss: 0.815937, acc.: 61.72%] [G loss: 2.368191]\n",
      "epoch:31 step:24672 [D loss: 0.109451, acc.: 94.53%] [G loss: 3.960649]\n",
      "epoch:31 step:24673 [D loss: 0.028591, acc.: 99.22%] [G loss: 5.246808]\n",
      "epoch:31 step:24674 [D loss: 0.028059, acc.: 100.00%] [G loss: 5.192799]\n",
      "epoch:31 step:24675 [D loss: 0.024546, acc.: 99.22%] [G loss: 4.368987]\n",
      "epoch:31 step:24676 [D loss: 0.010762, acc.: 100.00%] [G loss: 4.554577]\n",
      "epoch:31 step:24677 [D loss: 0.012878, acc.: 100.00%] [G loss: 4.405025]\n",
      "epoch:31 step:24678 [D loss: 0.030478, acc.: 100.00%] [G loss: 4.154614]\n",
      "epoch:31 step:24679 [D loss: 0.026596, acc.: 100.00%] [G loss: 3.268975]\n",
      "epoch:31 step:24680 [D loss: 0.008578, acc.: 100.00%] [G loss: 4.392348]\n",
      "epoch:31 step:24681 [D loss: 0.009093, acc.: 100.00%] [G loss: 4.430128]\n",
      "epoch:31 step:24682 [D loss: 0.045933, acc.: 100.00%] [G loss: 5.341272]\n",
      "epoch:31 step:24683 [D loss: 0.025609, acc.: 100.00%] [G loss: 4.571294]\n",
      "epoch:31 step:24684 [D loss: 0.011251, acc.: 100.00%] [G loss: 4.337234]\n",
      "epoch:31 step:24685 [D loss: 0.437281, acc.: 78.12%] [G loss: 5.551997]\n",
      "epoch:31 step:24686 [D loss: 0.017664, acc.: 100.00%] [G loss: 6.538500]\n",
      "epoch:31 step:24687 [D loss: 0.062107, acc.: 98.44%] [G loss: 5.974608]\n",
      "epoch:31 step:24688 [D loss: 0.003307, acc.: 100.00%] [G loss: 5.431144]\n",
      "epoch:31 step:24689 [D loss: 0.007340, acc.: 100.00%] [G loss: 5.226292]\n",
      "epoch:31 step:24690 [D loss: 0.011714, acc.: 100.00%] [G loss: 4.999450]\n",
      "epoch:31 step:24691 [D loss: 0.005485, acc.: 100.00%] [G loss: 4.665487]\n",
      "epoch:31 step:24692 [D loss: 0.026378, acc.: 100.00%] [G loss: 5.031206]\n",
      "epoch:31 step:24693 [D loss: 0.025765, acc.: 99.22%] [G loss: 4.478446]\n",
      "epoch:31 step:24694 [D loss: 0.011801, acc.: 100.00%] [G loss: 4.230702]\n",
      "epoch:31 step:24695 [D loss: 0.006907, acc.: 100.00%] [G loss: 5.505700]\n",
      "epoch:31 step:24696 [D loss: 0.043573, acc.: 100.00%] [G loss: 4.439668]\n",
      "epoch:31 step:24697 [D loss: 0.016584, acc.: 100.00%] [G loss: 3.701215]\n",
      "epoch:31 step:24698 [D loss: 0.017003, acc.: 100.00%] [G loss: 4.481467]\n",
      "epoch:31 step:24699 [D loss: 0.009647, acc.: 100.00%] [G loss: 4.094032]\n",
      "epoch:31 step:24700 [D loss: 0.023604, acc.: 100.00%] [G loss: 4.750282]\n",
      "epoch:31 step:24701 [D loss: 0.023351, acc.: 99.22%] [G loss: 4.409949]\n",
      "epoch:31 step:24702 [D loss: 0.095413, acc.: 98.44%] [G loss: 4.305354]\n",
      "epoch:31 step:24703 [D loss: 0.127276, acc.: 96.88%] [G loss: 4.636669]\n",
      "epoch:31 step:24704 [D loss: 0.005951, acc.: 100.00%] [G loss: 6.113408]\n",
      "epoch:31 step:24705 [D loss: 0.001865, acc.: 100.00%] [G loss: 5.883489]\n",
      "epoch:31 step:24706 [D loss: 0.016082, acc.: 100.00%] [G loss: 5.637585]\n",
      "epoch:31 step:24707 [D loss: 0.004790, acc.: 100.00%] [G loss: 5.476420]\n",
      "epoch:31 step:24708 [D loss: 0.017835, acc.: 100.00%] [G loss: 4.722029]\n",
      "epoch:31 step:24709 [D loss: 0.023747, acc.: 100.00%] [G loss: 5.763678]\n",
      "epoch:31 step:24710 [D loss: 0.015769, acc.: 100.00%] [G loss: 4.818872]\n",
      "epoch:31 step:24711 [D loss: 0.022456, acc.: 100.00%] [G loss: 5.746987]\n",
      "epoch:31 step:24712 [D loss: 1.295917, acc.: 37.50%] [G loss: 6.492163]\n",
      "epoch:31 step:24713 [D loss: 0.017210, acc.: 99.22%] [G loss: 7.769226]\n",
      "epoch:31 step:24714 [D loss: 0.828811, acc.: 65.62%] [G loss: 3.851707]\n",
      "epoch:31 step:24715 [D loss: 0.130045, acc.: 95.31%] [G loss: 4.684633]\n",
      "epoch:31 step:24716 [D loss: 0.010601, acc.: 100.00%] [G loss: 4.819006]\n",
      "epoch:31 step:24717 [D loss: 0.100377, acc.: 97.66%] [G loss: 3.980832]\n",
      "epoch:31 step:24718 [D loss: 0.125697, acc.: 96.88%] [G loss: 4.180590]\n",
      "epoch:31 step:24719 [D loss: 0.062362, acc.: 98.44%] [G loss: 4.764410]\n",
      "epoch:31 step:24720 [D loss: 0.126302, acc.: 96.88%] [G loss: 1.264563]\n",
      "epoch:31 step:24721 [D loss: 0.085625, acc.: 99.22%] [G loss: 2.567420]\n",
      "epoch:31 step:24722 [D loss: 0.063120, acc.: 98.44%] [G loss: 1.647176]\n",
      "epoch:31 step:24723 [D loss: 0.093483, acc.: 97.66%] [G loss: 2.664409]\n",
      "epoch:31 step:24724 [D loss: 0.574676, acc.: 75.78%] [G loss: 6.594961]\n",
      "epoch:31 step:24725 [D loss: 0.194021, acc.: 87.50%] [G loss: 5.316707]\n",
      "epoch:31 step:24726 [D loss: 0.218296, acc.: 89.84%] [G loss: 2.230166]\n",
      "epoch:31 step:24727 [D loss: 0.339476, acc.: 84.38%] [G loss: 5.683067]\n",
      "epoch:31 step:24728 [D loss: 0.002903, acc.: 100.00%] [G loss: 6.767294]\n",
      "epoch:31 step:24729 [D loss: 0.237116, acc.: 89.84%] [G loss: 4.448812]\n",
      "epoch:31 step:24730 [D loss: 0.038636, acc.: 98.44%] [G loss: 4.069191]\n",
      "epoch:31 step:24731 [D loss: 0.008638, acc.: 100.00%] [G loss: 4.197842]\n",
      "epoch:31 step:24732 [D loss: 0.008760, acc.: 100.00%] [G loss: 4.010205]\n",
      "epoch:31 step:24733 [D loss: 0.094994, acc.: 96.88%] [G loss: 2.684655]\n",
      "epoch:31 step:24734 [D loss: 0.030597, acc.: 100.00%] [G loss: 3.918479]\n",
      "epoch:31 step:24735 [D loss: 0.032287, acc.: 99.22%] [G loss: 2.560479]\n",
      "epoch:31 step:24736 [D loss: 0.009565, acc.: 100.00%] [G loss: 1.800838]\n",
      "epoch:31 step:24737 [D loss: 0.024326, acc.: 100.00%] [G loss: 1.729846]\n",
      "epoch:31 step:24738 [D loss: 0.031454, acc.: 100.00%] [G loss: 2.220861]\n",
      "epoch:31 step:24739 [D loss: 0.025440, acc.: 100.00%] [G loss: 2.783390]\n",
      "epoch:31 step:24740 [D loss: 0.074837, acc.: 100.00%] [G loss: 3.813441]\n",
      "epoch:31 step:24741 [D loss: 0.050787, acc.: 99.22%] [G loss: 4.889005]\n",
      "epoch:31 step:24742 [D loss: 0.010554, acc.: 100.00%] [G loss: 4.221751]\n",
      "epoch:31 step:24743 [D loss: 0.339340, acc.: 85.94%] [G loss: 4.883436]\n",
      "epoch:31 step:24744 [D loss: 0.464018, acc.: 75.78%] [G loss: 4.641230]\n",
      "epoch:31 step:24745 [D loss: 0.033935, acc.: 99.22%] [G loss: 5.796415]\n",
      "epoch:31 step:24746 [D loss: 0.018478, acc.: 99.22%] [G loss: 5.573612]\n",
      "epoch:31 step:24747 [D loss: 0.042111, acc.: 99.22%] [G loss: 4.437616]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:31 step:24748 [D loss: 0.050466, acc.: 99.22%] [G loss: 4.533298]\n",
      "epoch:31 step:24749 [D loss: 0.016805, acc.: 100.00%] [G loss: 3.704555]\n",
      "epoch:31 step:24750 [D loss: 0.008369, acc.: 100.00%] [G loss: 3.654270]\n",
      "epoch:31 step:24751 [D loss: 0.052141, acc.: 100.00%] [G loss: 3.809434]\n",
      "epoch:31 step:24752 [D loss: 0.006208, acc.: 100.00%] [G loss: 4.177078]\n",
      "epoch:31 step:24753 [D loss: 0.077242, acc.: 96.88%] [G loss: 2.323724]\n",
      "epoch:31 step:24754 [D loss: 0.005672, acc.: 100.00%] [G loss: 1.583120]\n",
      "epoch:31 step:24755 [D loss: 0.240160, acc.: 87.50%] [G loss: 7.678398]\n",
      "epoch:31 step:24756 [D loss: 0.309402, acc.: 85.94%] [G loss: 5.028098]\n",
      "epoch:31 step:24757 [D loss: 0.001360, acc.: 100.00%] [G loss: 2.497494]\n",
      "epoch:31 step:24758 [D loss: 0.006619, acc.: 100.00%] [G loss: 2.806702]\n",
      "epoch:31 step:24759 [D loss: 0.075935, acc.: 96.09%] [G loss: 4.534340]\n",
      "epoch:31 step:24760 [D loss: 0.002625, acc.: 100.00%] [G loss: 4.738778]\n",
      "epoch:31 step:24761 [D loss: 0.335643, acc.: 85.94%] [G loss: 0.331536]\n",
      "epoch:31 step:24762 [D loss: 0.220735, acc.: 90.62%] [G loss: 6.294387]\n",
      "epoch:31 step:24763 [D loss: 0.419677, acc.: 80.47%] [G loss: 0.642727]\n",
      "epoch:31 step:24764 [D loss: 0.631425, acc.: 72.66%] [G loss: 7.632775]\n",
      "epoch:31 step:24765 [D loss: 0.384998, acc.: 81.25%] [G loss: 6.517917]\n",
      "epoch:31 step:24766 [D loss: 0.051868, acc.: 99.22%] [G loss: 5.042164]\n",
      "epoch:31 step:24767 [D loss: 0.050492, acc.: 98.44%] [G loss: 2.962136]\n",
      "epoch:31 step:24768 [D loss: 0.631228, acc.: 65.62%] [G loss: 5.901286]\n",
      "epoch:31 step:24769 [D loss: 0.352922, acc.: 82.03%] [G loss: 5.741347]\n",
      "epoch:31 step:24770 [D loss: 0.030957, acc.: 100.00%] [G loss: 4.325794]\n",
      "epoch:31 step:24771 [D loss: 0.355577, acc.: 83.59%] [G loss: 2.784125]\n",
      "epoch:31 step:24772 [D loss: 0.603469, acc.: 67.19%] [G loss: 7.318394]\n",
      "epoch:31 step:24773 [D loss: 0.700602, acc.: 67.19%] [G loss: 5.675656]\n",
      "epoch:31 step:24774 [D loss: 0.014227, acc.: 100.00%] [G loss: 4.846040]\n",
      "epoch:31 step:24775 [D loss: 0.009727, acc.: 100.00%] [G loss: 3.345000]\n",
      "epoch:31 step:24776 [D loss: 0.013309, acc.: 100.00%] [G loss: 4.384385]\n",
      "epoch:31 step:24777 [D loss: 0.089447, acc.: 96.88%] [G loss: 4.041698]\n",
      "epoch:31 step:24778 [D loss: 0.032925, acc.: 98.44%] [G loss: 4.488341]\n",
      "epoch:31 step:24779 [D loss: 0.029087, acc.: 100.00%] [G loss: 4.234079]\n",
      "epoch:31 step:24780 [D loss: 0.066670, acc.: 98.44%] [G loss: 4.417296]\n",
      "epoch:31 step:24781 [D loss: 0.023752, acc.: 100.00%] [G loss: 3.902290]\n",
      "epoch:31 step:24782 [D loss: 0.143995, acc.: 95.31%] [G loss: 2.082680]\n",
      "epoch:31 step:24783 [D loss: 0.129364, acc.: 93.75%] [G loss: 3.861938]\n",
      "epoch:31 step:24784 [D loss: 0.145556, acc.: 94.53%] [G loss: 3.339296]\n",
      "epoch:31 step:24785 [D loss: 0.039512, acc.: 100.00%] [G loss: 5.013896]\n",
      "epoch:31 step:24786 [D loss: 0.064892, acc.: 97.66%] [G loss: 4.016713]\n",
      "epoch:31 step:24787 [D loss: 0.035923, acc.: 100.00%] [G loss: 3.850451]\n",
      "epoch:31 step:24788 [D loss: 0.032472, acc.: 99.22%] [G loss: 3.788617]\n",
      "epoch:31 step:24789 [D loss: 0.021428, acc.: 100.00%] [G loss: 3.690866]\n",
      "epoch:31 step:24790 [D loss: 0.055453, acc.: 99.22%] [G loss: 4.400737]\n",
      "epoch:31 step:24791 [D loss: 0.217004, acc.: 90.62%] [G loss: 1.786276]\n",
      "epoch:31 step:24792 [D loss: 0.023291, acc.: 100.00%] [G loss: 1.947427]\n",
      "epoch:31 step:24793 [D loss: 0.068915, acc.: 99.22%] [G loss: 4.820973]\n",
      "epoch:31 step:24794 [D loss: 0.002808, acc.: 100.00%] [G loss: 6.723426]\n",
      "epoch:31 step:24795 [D loss: 0.048900, acc.: 97.66%] [G loss: 5.033054]\n",
      "epoch:31 step:24796 [D loss: 0.028810, acc.: 99.22%] [G loss: 4.706211]\n",
      "epoch:31 step:24797 [D loss: 0.003456, acc.: 100.00%] [G loss: 5.484574]\n",
      "epoch:31 step:24798 [D loss: 0.006703, acc.: 100.00%] [G loss: 5.214966]\n",
      "epoch:31 step:24799 [D loss: 0.012582, acc.: 100.00%] [G loss: 3.921703]\n",
      "epoch:31 step:24800 [D loss: 0.023191, acc.: 100.00%] [G loss: 4.393100]\n",
      "##############\n",
      "[1.02299273 1.03482642 1.11980847 0.85266748 0.97740524 0.76771894\n",
      " 2.11098772 2.11333066 1.11102908 0.94992611]\n",
      "##########\n",
      "epoch:31 step:24801 [D loss: 0.161872, acc.: 96.09%] [G loss: 3.457569]\n",
      "epoch:31 step:24802 [D loss: 0.019859, acc.: 100.00%] [G loss: 4.767032]\n",
      "epoch:31 step:24803 [D loss: 0.014628, acc.: 100.00%] [G loss: 5.111960]\n",
      "epoch:31 step:24804 [D loss: 0.013544, acc.: 99.22%] [G loss: 4.598512]\n",
      "epoch:31 step:24805 [D loss: 0.022350, acc.: 100.00%] [G loss: 5.167277]\n",
      "epoch:31 step:24806 [D loss: 0.007814, acc.: 100.00%] [G loss: 3.704553]\n",
      "epoch:31 step:24807 [D loss: 0.031796, acc.: 100.00%] [G loss: 4.281410]\n",
      "epoch:31 step:24808 [D loss: 0.010967, acc.: 100.00%] [G loss: 4.403404]\n",
      "epoch:31 step:24809 [D loss: 0.003301, acc.: 100.00%] [G loss: 3.993337]\n",
      "epoch:31 step:24810 [D loss: 0.110886, acc.: 96.88%] [G loss: 4.031533]\n",
      "epoch:31 step:24811 [D loss: 0.019092, acc.: 100.00%] [G loss: 5.268937]\n",
      "epoch:31 step:24812 [D loss: 0.021160, acc.: 100.00%] [G loss: 4.101978]\n",
      "epoch:31 step:24813 [D loss: 0.034215, acc.: 100.00%] [G loss: 4.393464]\n",
      "epoch:31 step:24814 [D loss: 0.010105, acc.: 100.00%] [G loss: 2.715950]\n",
      "epoch:31 step:24815 [D loss: 4.094781, acc.: 31.25%] [G loss: 9.250836]\n",
      "epoch:31 step:24816 [D loss: 2.524752, acc.: 50.00%] [G loss: 7.203126]\n",
      "epoch:31 step:24817 [D loss: 1.778748, acc.: 51.56%] [G loss: 4.107942]\n",
      "epoch:31 step:24818 [D loss: 0.184065, acc.: 92.97%] [G loss: 3.434528]\n",
      "epoch:31 step:24819 [D loss: 0.031190, acc.: 99.22%] [G loss: 3.751119]\n",
      "epoch:31 step:24820 [D loss: 0.079538, acc.: 98.44%] [G loss: 3.233195]\n",
      "epoch:31 step:24821 [D loss: 0.055466, acc.: 99.22%] [G loss: 3.581648]\n",
      "epoch:31 step:24822 [D loss: 0.041311, acc.: 100.00%] [G loss: 3.891695]\n",
      "epoch:31 step:24823 [D loss: 0.057813, acc.: 99.22%] [G loss: 3.329895]\n",
      "epoch:31 step:24824 [D loss: 0.030188, acc.: 100.00%] [G loss: 3.947499]\n",
      "epoch:31 step:24825 [D loss: 0.037637, acc.: 100.00%] [G loss: 3.511584]\n",
      "epoch:31 step:24826 [D loss: 0.032015, acc.: 99.22%] [G loss: 3.217621]\n",
      "epoch:31 step:24827 [D loss: 0.046063, acc.: 100.00%] [G loss: 3.051903]\n",
      "epoch:31 step:24828 [D loss: 0.141367, acc.: 96.88%] [G loss: 2.776137]\n",
      "epoch:31 step:24829 [D loss: 0.042540, acc.: 100.00%] [G loss: 2.998799]\n",
      "epoch:31 step:24830 [D loss: 0.104182, acc.: 96.88%] [G loss: 2.943014]\n",
      "epoch:31 step:24831 [D loss: 0.036087, acc.: 100.00%] [G loss: 3.047688]\n",
      "epoch:31 step:24832 [D loss: 0.030386, acc.: 100.00%] [G loss: 3.594018]\n",
      "epoch:31 step:24833 [D loss: 0.031387, acc.: 100.00%] [G loss: 3.291902]\n",
      "epoch:31 step:24834 [D loss: 0.154256, acc.: 96.88%] [G loss: 2.309835]\n",
      "epoch:31 step:24835 [D loss: 0.112170, acc.: 99.22%] [G loss: 2.703910]\n",
      "epoch:31 step:24836 [D loss: 0.015265, acc.: 100.00%] [G loss: 4.309657]\n",
      "epoch:31 step:24837 [D loss: 0.044633, acc.: 99.22%] [G loss: 3.389933]\n",
      "epoch:31 step:24838 [D loss: 0.095550, acc.: 98.44%] [G loss: 3.401396]\n",
      "epoch:31 step:24839 [D loss: 0.091235, acc.: 98.44%] [G loss: 3.878145]\n",
      "epoch:31 step:24840 [D loss: 0.042297, acc.: 98.44%] [G loss: 4.153469]\n",
      "epoch:31 step:24841 [D loss: 0.068859, acc.: 98.44%] [G loss: 3.898119]\n",
      "epoch:31 step:24842 [D loss: 0.134565, acc.: 96.09%] [G loss: 2.702177]\n",
      "epoch:31 step:24843 [D loss: 0.068779, acc.: 99.22%] [G loss: 3.002707]\n",
      "epoch:31 step:24844 [D loss: 0.025571, acc.: 100.00%] [G loss: 3.965904]\n",
      "epoch:31 step:24845 [D loss: 0.090297, acc.: 97.66%] [G loss: 3.465821]\n",
      "epoch:31 step:24846 [D loss: 0.062215, acc.: 99.22%] [G loss: 3.482191]\n",
      "epoch:31 step:24847 [D loss: 0.037914, acc.: 100.00%] [G loss: 3.239674]\n",
      "epoch:31 step:24848 [D loss: 0.040267, acc.: 100.00%] [G loss: 3.523156]\n",
      "epoch:31 step:24849 [D loss: 0.019833, acc.: 100.00%] [G loss: 3.873762]\n",
      "epoch:31 step:24850 [D loss: 0.079975, acc.: 100.00%] [G loss: 3.334911]\n",
      "epoch:31 step:24851 [D loss: 0.019855, acc.: 100.00%] [G loss: 4.202303]\n",
      "epoch:31 step:24852 [D loss: 0.052976, acc.: 99.22%] [G loss: 3.745945]\n",
      "epoch:31 step:24853 [D loss: 0.345436, acc.: 83.59%] [G loss: 4.925499]\n",
      "epoch:31 step:24854 [D loss: 0.058232, acc.: 99.22%] [G loss: 5.622257]\n",
      "epoch:31 step:24855 [D loss: 0.089068, acc.: 96.88%] [G loss: 3.777154]\n",
      "epoch:31 step:24856 [D loss: 0.014015, acc.: 100.00%] [G loss: 4.634830]\n",
      "epoch:31 step:24857 [D loss: 1.120711, acc.: 53.91%] [G loss: 7.042014]\n",
      "epoch:31 step:24858 [D loss: 1.493386, acc.: 51.56%] [G loss: 4.892758]\n",
      "epoch:31 step:24859 [D loss: 0.255496, acc.: 89.84%] [G loss: 3.673429]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:31 step:24860 [D loss: 0.063715, acc.: 99.22%] [G loss: 3.325238]\n",
      "epoch:31 step:24861 [D loss: 0.068867, acc.: 98.44%] [G loss: 3.105125]\n",
      "epoch:31 step:24862 [D loss: 0.028489, acc.: 100.00%] [G loss: 4.090222]\n",
      "epoch:31 step:24863 [D loss: 0.076119, acc.: 98.44%] [G loss: 3.390595]\n",
      "epoch:31 step:24864 [D loss: 0.040652, acc.: 100.00%] [G loss: 3.511390]\n",
      "epoch:31 step:24865 [D loss: 0.075797, acc.: 99.22%] [G loss: 3.643403]\n",
      "epoch:31 step:24866 [D loss: 0.046349, acc.: 99.22%] [G loss: 4.152800]\n",
      "epoch:31 step:24867 [D loss: 0.037597, acc.: 100.00%] [G loss: 4.601293]\n",
      "epoch:31 step:24868 [D loss: 0.032805, acc.: 100.00%] [G loss: 4.071997]\n",
      "epoch:31 step:24869 [D loss: 0.041325, acc.: 100.00%] [G loss: 4.003940]\n",
      "epoch:31 step:24870 [D loss: 0.038092, acc.: 100.00%] [G loss: 3.455231]\n",
      "epoch:31 step:24871 [D loss: 0.062001, acc.: 99.22%] [G loss: 3.481483]\n",
      "epoch:31 step:24872 [D loss: 0.153688, acc.: 97.66%] [G loss: 3.710782]\n",
      "epoch:31 step:24873 [D loss: 0.015322, acc.: 100.00%] [G loss: 4.170676]\n",
      "epoch:31 step:24874 [D loss: 0.020964, acc.: 100.00%] [G loss: 4.209656]\n",
      "epoch:31 step:24875 [D loss: 0.026001, acc.: 100.00%] [G loss: 4.481722]\n",
      "epoch:31 step:24876 [D loss: 0.058918, acc.: 99.22%] [G loss: 3.295841]\n",
      "epoch:31 step:24877 [D loss: 0.027796, acc.: 100.00%] [G loss: 3.855742]\n",
      "epoch:31 step:24878 [D loss: 0.016754, acc.: 100.00%] [G loss: 4.297804]\n",
      "epoch:31 step:24879 [D loss: 0.043156, acc.: 99.22%] [G loss: 3.448214]\n",
      "epoch:31 step:24880 [D loss: 0.035436, acc.: 100.00%] [G loss: 4.142536]\n",
      "epoch:31 step:24881 [D loss: 0.040129, acc.: 100.00%] [G loss: 4.520829]\n",
      "epoch:31 step:24882 [D loss: 0.013031, acc.: 100.00%] [G loss: 4.822691]\n",
      "epoch:31 step:24883 [D loss: 0.011684, acc.: 100.00%] [G loss: 4.826124]\n",
      "epoch:31 step:24884 [D loss: 0.085746, acc.: 98.44%] [G loss: 3.817208]\n",
      "epoch:31 step:24885 [D loss: 0.032986, acc.: 100.00%] [G loss: 4.200612]\n",
      "epoch:31 step:24886 [D loss: 0.011117, acc.: 100.00%] [G loss: 4.718758]\n",
      "epoch:31 step:24887 [D loss: 0.019751, acc.: 100.00%] [G loss: 4.999537]\n",
      "epoch:31 step:24888 [D loss: 0.027774, acc.: 100.00%] [G loss: 4.163052]\n",
      "epoch:31 step:24889 [D loss: 0.018416, acc.: 100.00%] [G loss: 2.991610]\n",
      "epoch:31 step:24890 [D loss: 0.014152, acc.: 100.00%] [G loss: 4.226234]\n",
      "epoch:31 step:24891 [D loss: 0.042905, acc.: 100.00%] [G loss: 3.976233]\n",
      "epoch:31 step:24892 [D loss: 0.012108, acc.: 100.00%] [G loss: 4.814484]\n",
      "epoch:31 step:24893 [D loss: 0.035024, acc.: 100.00%] [G loss: 3.957862]\n",
      "epoch:31 step:24894 [D loss: 0.075344, acc.: 98.44%] [G loss: 3.741988]\n",
      "epoch:31 step:24895 [D loss: 0.123038, acc.: 96.88%] [G loss: 5.777414]\n",
      "epoch:31 step:24896 [D loss: 1.278006, acc.: 42.97%] [G loss: 3.453279]\n",
      "epoch:31 step:24897 [D loss: 0.030590, acc.: 100.00%] [G loss: 4.834477]\n",
      "epoch:31 step:24898 [D loss: 0.064715, acc.: 97.66%] [G loss: 4.318593]\n",
      "epoch:31 step:24899 [D loss: 0.113557, acc.: 96.09%] [G loss: 4.008534]\n",
      "epoch:31 step:24900 [D loss: 0.028314, acc.: 100.00%] [G loss: 4.142189]\n",
      "epoch:31 step:24901 [D loss: 0.017868, acc.: 100.00%] [G loss: 4.382624]\n",
      "epoch:31 step:24902 [D loss: 0.092038, acc.: 96.88%] [G loss: 4.097160]\n",
      "epoch:31 step:24903 [D loss: 0.016163, acc.: 100.00%] [G loss: 3.595969]\n",
      "epoch:31 step:24904 [D loss: 0.046605, acc.: 99.22%] [G loss: 3.885582]\n",
      "epoch:31 step:24905 [D loss: 0.070919, acc.: 97.66%] [G loss: 3.722869]\n",
      "epoch:31 step:24906 [D loss: 0.009594, acc.: 100.00%] [G loss: 3.419053]\n",
      "epoch:31 step:24907 [D loss: 0.027693, acc.: 100.00%] [G loss: 3.454297]\n",
      "epoch:31 step:24908 [D loss: 0.033492, acc.: 100.00%] [G loss: 3.123674]\n",
      "epoch:31 step:24909 [D loss: 0.021127, acc.: 100.00%] [G loss: 4.087887]\n",
      "epoch:31 step:24910 [D loss: 0.035010, acc.: 100.00%] [G loss: 3.290770]\n",
      "epoch:31 step:24911 [D loss: 0.037754, acc.: 100.00%] [G loss: 4.101238]\n",
      "epoch:31 step:24912 [D loss: 0.031568, acc.: 100.00%] [G loss: 3.230509]\n",
      "epoch:31 step:24913 [D loss: 0.027957, acc.: 99.22%] [G loss: 3.883057]\n",
      "epoch:31 step:24914 [D loss: 0.044975, acc.: 99.22%] [G loss: 3.949943]\n",
      "epoch:31 step:24915 [D loss: 0.023959, acc.: 99.22%] [G loss: 3.600087]\n",
      "epoch:31 step:24916 [D loss: 0.023164, acc.: 100.00%] [G loss: 3.526257]\n",
      "epoch:31 step:24917 [D loss: 0.007065, acc.: 100.00%] [G loss: 3.453023]\n",
      "epoch:31 step:24918 [D loss: 0.017469, acc.: 100.00%] [G loss: 3.984288]\n",
      "epoch:31 step:24919 [D loss: 0.034158, acc.: 100.00%] [G loss: 4.598751]\n",
      "epoch:31 step:24920 [D loss: 0.012992, acc.: 100.00%] [G loss: 4.849283]\n",
      "epoch:31 step:24921 [D loss: 0.061380, acc.: 98.44%] [G loss: 4.291930]\n",
      "epoch:31 step:24922 [D loss: 0.023457, acc.: 100.00%] [G loss: 4.816224]\n",
      "epoch:31 step:24923 [D loss: 0.005278, acc.: 100.00%] [G loss: 4.054828]\n",
      "epoch:31 step:24924 [D loss: 2.179372, acc.: 14.06%] [G loss: 7.689028]\n",
      "epoch:31 step:24925 [D loss: 0.017091, acc.: 100.00%] [G loss: 8.878063]\n",
      "epoch:31 step:24926 [D loss: 0.639996, acc.: 68.75%] [G loss: 6.773212]\n",
      "epoch:31 step:24927 [D loss: 0.053841, acc.: 95.31%] [G loss: 5.291378]\n",
      "epoch:31 step:24928 [D loss: 0.922514, acc.: 66.41%] [G loss: 7.957396]\n",
      "epoch:31 step:24929 [D loss: 0.091807, acc.: 96.09%] [G loss: 8.129182]\n",
      "epoch:31 step:24930 [D loss: 0.403948, acc.: 82.03%] [G loss: 6.979915]\n",
      "epoch:31 step:24931 [D loss: 0.006955, acc.: 100.00%] [G loss: 5.987022]\n",
      "epoch:31 step:24932 [D loss: 0.002547, acc.: 100.00%] [G loss: 5.476519]\n",
      "epoch:31 step:24933 [D loss: 0.017329, acc.: 100.00%] [G loss: 4.680420]\n",
      "epoch:31 step:24934 [D loss: 0.024906, acc.: 100.00%] [G loss: 4.287834]\n",
      "epoch:31 step:24935 [D loss: 0.096336, acc.: 96.88%] [G loss: 4.668078]\n",
      "epoch:31 step:24936 [D loss: 0.621648, acc.: 67.19%] [G loss: 3.672537]\n",
      "epoch:31 step:24937 [D loss: 0.017199, acc.: 100.00%] [G loss: 4.488317]\n",
      "epoch:31 step:24938 [D loss: 0.023957, acc.: 100.00%] [G loss: 4.251909]\n",
      "epoch:31 step:24939 [D loss: 0.047643, acc.: 99.22%] [G loss: 4.707557]\n",
      "epoch:31 step:24940 [D loss: 0.057691, acc.: 100.00%] [G loss: 4.051035]\n",
      "epoch:31 step:24941 [D loss: 0.032731, acc.: 99.22%] [G loss: 4.279642]\n",
      "epoch:31 step:24942 [D loss: 0.023141, acc.: 100.00%] [G loss: 3.544289]\n",
      "epoch:31 step:24943 [D loss: 0.030932, acc.: 100.00%] [G loss: 3.447234]\n",
      "epoch:31 step:24944 [D loss: 0.035610, acc.: 100.00%] [G loss: 3.915531]\n",
      "epoch:31 step:24945 [D loss: 0.081676, acc.: 99.22%] [G loss: 3.437005]\n",
      "epoch:31 step:24946 [D loss: 0.155438, acc.: 96.88%] [G loss: 3.215237]\n",
      "epoch:31 step:24947 [D loss: 0.459481, acc.: 78.12%] [G loss: 2.479797]\n",
      "epoch:31 step:24948 [D loss: 0.011866, acc.: 100.00%] [G loss: 2.845946]\n",
      "epoch:31 step:24949 [D loss: 0.030270, acc.: 100.00%] [G loss: 2.439654]\n",
      "epoch:31 step:24950 [D loss: 0.255399, acc.: 87.50%] [G loss: 6.142131]\n",
      "epoch:31 step:24951 [D loss: 0.527860, acc.: 77.34%] [G loss: 1.125773]\n",
      "epoch:31 step:24952 [D loss: 0.032800, acc.: 100.00%] [G loss: 0.764257]\n",
      "epoch:31 step:24953 [D loss: 0.103890, acc.: 97.66%] [G loss: 3.063597]\n",
      "epoch:31 step:24954 [D loss: 0.058293, acc.: 100.00%] [G loss: 2.872463]\n",
      "epoch:31 step:24955 [D loss: 0.039816, acc.: 100.00%] [G loss: 3.046737]\n",
      "epoch:31 step:24956 [D loss: 0.054069, acc.: 100.00%] [G loss: 1.521509]\n",
      "epoch:31 step:24957 [D loss: 0.029037, acc.: 100.00%] [G loss: 0.588446]\n",
      "epoch:31 step:24958 [D loss: 0.083588, acc.: 99.22%] [G loss: 1.559860]\n",
      "epoch:31 step:24959 [D loss: 0.030942, acc.: 99.22%] [G loss: 2.311369]\n",
      "epoch:31 step:24960 [D loss: 0.029064, acc.: 100.00%] [G loss: 1.989945]\n",
      "epoch:31 step:24961 [D loss: 0.020198, acc.: 100.00%] [G loss: 0.705139]\n",
      "epoch:31 step:24962 [D loss: 0.108474, acc.: 97.66%] [G loss: 3.443609]\n",
      "epoch:31 step:24963 [D loss: 0.170136, acc.: 92.19%] [G loss: 1.642505]\n",
      "epoch:31 step:24964 [D loss: 0.119500, acc.: 97.66%] [G loss: 2.088440]\n",
      "epoch:31 step:24965 [D loss: 0.582617, acc.: 65.62%] [G loss: 6.240168]\n",
      "epoch:31 step:24966 [D loss: 0.197730, acc.: 92.19%] [G loss: 6.009602]\n",
      "epoch:31 step:24967 [D loss: 0.065191, acc.: 97.66%] [G loss: 4.466412]\n",
      "epoch:31 step:24968 [D loss: 0.019781, acc.: 100.00%] [G loss: 3.701882]\n",
      "epoch:31 step:24969 [D loss: 0.056759, acc.: 98.44%] [G loss: 4.395399]\n",
      "epoch:31 step:24970 [D loss: 0.007293, acc.: 100.00%] [G loss: 4.518346]\n",
      "epoch:31 step:24971 [D loss: 0.010879, acc.: 100.00%] [G loss: 4.640278]\n",
      "epoch:31 step:24972 [D loss: 0.021248, acc.: 100.00%] [G loss: 4.365899]\n",
      "epoch:31 step:24973 [D loss: 0.154166, acc.: 96.09%] [G loss: 3.739925]\n",
      "epoch:31 step:24974 [D loss: 0.009419, acc.: 100.00%] [G loss: 4.374180]\n",
      "epoch:31 step:24975 [D loss: 0.012540, acc.: 100.00%] [G loss: 4.431179]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:31 step:24976 [D loss: 0.012019, acc.: 100.00%] [G loss: 3.587109]\n",
      "epoch:31 step:24977 [D loss: 0.010981, acc.: 100.00%] [G loss: 4.374200]\n",
      "epoch:31 step:24978 [D loss: 0.022384, acc.: 100.00%] [G loss: 4.420602]\n",
      "epoch:31 step:24979 [D loss: 0.057146, acc.: 97.66%] [G loss: 3.963243]\n",
      "epoch:31 step:24980 [D loss: 0.012761, acc.: 100.00%] [G loss: 4.142572]\n",
      "epoch:31 step:24981 [D loss: 0.031131, acc.: 100.00%] [G loss: 4.162042]\n",
      "epoch:31 step:24982 [D loss: 0.051403, acc.: 100.00%] [G loss: 4.616550]\n",
      "epoch:31 step:24983 [D loss: 0.004944, acc.: 100.00%] [G loss: 5.101608]\n",
      "epoch:31 step:24984 [D loss: 0.009270, acc.: 100.00%] [G loss: 4.876275]\n",
      "epoch:31 step:24985 [D loss: 0.005819, acc.: 100.00%] [G loss: 4.252176]\n",
      "epoch:31 step:24986 [D loss: 0.016771, acc.: 100.00%] [G loss: 4.852612]\n",
      "epoch:31 step:24987 [D loss: 0.016872, acc.: 100.00%] [G loss: 4.435710]\n",
      "epoch:31 step:24988 [D loss: 0.179823, acc.: 94.53%] [G loss: 4.986829]\n",
      "epoch:31 step:24989 [D loss: 0.037181, acc.: 98.44%] [G loss: 5.917608]\n",
      "epoch:31 step:24990 [D loss: 0.167406, acc.: 95.31%] [G loss: 2.810494]\n",
      "epoch:31 step:24991 [D loss: 0.022129, acc.: 100.00%] [G loss: 2.826486]\n",
      "epoch:31 step:24992 [D loss: 0.021363, acc.: 100.00%] [G loss: 3.329102]\n",
      "epoch:32 step:24993 [D loss: 0.020182, acc.: 100.00%] [G loss: 5.016053]\n",
      "epoch:32 step:24994 [D loss: 0.013948, acc.: 100.00%] [G loss: 5.041895]\n",
      "epoch:32 step:24995 [D loss: 0.021291, acc.: 100.00%] [G loss: 5.188715]\n",
      "epoch:32 step:24996 [D loss: 0.037743, acc.: 98.44%] [G loss: 3.838803]\n",
      "epoch:32 step:24997 [D loss: 0.009225, acc.: 100.00%] [G loss: 4.440361]\n",
      "epoch:32 step:24998 [D loss: 0.013014, acc.: 100.00%] [G loss: 4.340409]\n",
      "epoch:32 step:24999 [D loss: 0.034289, acc.: 99.22%] [G loss: 1.737301]\n",
      "epoch:32 step:25000 [D loss: 0.039900, acc.: 100.00%] [G loss: 4.474108]\n",
      "##############\n",
      "[1.05887042 0.94547129 1.10042238 0.92136976 2.11736478 0.86076887\n",
      " 2.12551966 1.10467873 2.11517988 1.12134075]\n",
      "##########\n",
      "epoch:32 step:25001 [D loss: 0.020624, acc.: 100.00%] [G loss: 3.709559]\n",
      "epoch:32 step:25002 [D loss: 0.024356, acc.: 100.00%] [G loss: 2.985816]\n",
      "epoch:32 step:25003 [D loss: 0.039422, acc.: 99.22%] [G loss: 5.645606]\n",
      "epoch:32 step:25004 [D loss: 0.055808, acc.: 98.44%] [G loss: 4.287499]\n",
      "epoch:32 step:25005 [D loss: 0.003683, acc.: 100.00%] [G loss: 4.262715]\n",
      "epoch:32 step:25006 [D loss: 0.169127, acc.: 96.09%] [G loss: 5.650536]\n",
      "epoch:32 step:25007 [D loss: 0.048474, acc.: 100.00%] [G loss: 4.880271]\n",
      "epoch:32 step:25008 [D loss: 0.008889, acc.: 100.00%] [G loss: 4.851745]\n",
      "epoch:32 step:25009 [D loss: 0.576979, acc.: 70.31%] [G loss: 9.749121]\n",
      "epoch:32 step:25010 [D loss: 2.501863, acc.: 50.00%] [G loss: 6.141510]\n",
      "epoch:32 step:25011 [D loss: 0.167786, acc.: 93.75%] [G loss: 4.364130]\n",
      "epoch:32 step:25012 [D loss: 0.028408, acc.: 100.00%] [G loss: 4.837234]\n",
      "epoch:32 step:25013 [D loss: 0.075222, acc.: 98.44%] [G loss: 3.825104]\n",
      "epoch:32 step:25014 [D loss: 0.023407, acc.: 100.00%] [G loss: 4.296824]\n",
      "epoch:32 step:25015 [D loss: 0.080251, acc.: 98.44%] [G loss: 3.372357]\n",
      "epoch:32 step:25016 [D loss: 0.167466, acc.: 95.31%] [G loss: 3.701126]\n",
      "epoch:32 step:25017 [D loss: 0.025530, acc.: 99.22%] [G loss: 5.291378]\n",
      "epoch:32 step:25018 [D loss: 0.189140, acc.: 92.19%] [G loss: 2.594867]\n",
      "epoch:32 step:25019 [D loss: 0.205982, acc.: 91.41%] [G loss: 4.946177]\n",
      "epoch:32 step:25020 [D loss: 0.171277, acc.: 92.19%] [G loss: 4.859854]\n",
      "epoch:32 step:25021 [D loss: 0.063910, acc.: 97.66%] [G loss: 3.092893]\n",
      "epoch:32 step:25022 [D loss: 0.142561, acc.: 96.09%] [G loss: 4.896593]\n",
      "epoch:32 step:25023 [D loss: 0.020276, acc.: 100.00%] [G loss: 4.948634]\n",
      "epoch:32 step:25024 [D loss: 0.239498, acc.: 92.19%] [G loss: 4.285046]\n",
      "epoch:32 step:25025 [D loss: 0.072335, acc.: 99.22%] [G loss: 3.993452]\n",
      "epoch:32 step:25026 [D loss: 0.025001, acc.: 100.00%] [G loss: 5.388408]\n",
      "epoch:32 step:25027 [D loss: 0.028222, acc.: 100.00%] [G loss: 4.645487]\n",
      "epoch:32 step:25028 [D loss: 0.213003, acc.: 92.19%] [G loss: 3.893723]\n",
      "epoch:32 step:25029 [D loss: 0.045217, acc.: 100.00%] [G loss: 4.916278]\n",
      "epoch:32 step:25030 [D loss: 0.019632, acc.: 100.00%] [G loss: 5.031577]\n",
      "epoch:32 step:25031 [D loss: 0.016175, acc.: 100.00%] [G loss: 5.229225]\n",
      "epoch:32 step:25032 [D loss: 0.053796, acc.: 98.44%] [G loss: 3.778289]\n",
      "epoch:32 step:25033 [D loss: 0.011637, acc.: 100.00%] [G loss: 3.243861]\n",
      "epoch:32 step:25034 [D loss: 0.017908, acc.: 100.00%] [G loss: 4.388547]\n",
      "epoch:32 step:25035 [D loss: 0.012885, acc.: 100.00%] [G loss: 4.663853]\n",
      "epoch:32 step:25036 [D loss: 0.022874, acc.: 100.00%] [G loss: 3.586655]\n",
      "epoch:32 step:25037 [D loss: 0.575400, acc.: 72.66%] [G loss: 7.430903]\n",
      "epoch:32 step:25038 [D loss: 0.409920, acc.: 76.56%] [G loss: 5.495204]\n",
      "epoch:32 step:25039 [D loss: 0.006968, acc.: 100.00%] [G loss: 4.147328]\n",
      "epoch:32 step:25040 [D loss: 0.099264, acc.: 94.53%] [G loss: 4.724538]\n",
      "epoch:32 step:25041 [D loss: 0.005568, acc.: 100.00%] [G loss: 6.492021]\n",
      "epoch:32 step:25042 [D loss: 0.115854, acc.: 94.53%] [G loss: 4.491147]\n",
      "epoch:32 step:25043 [D loss: 0.094338, acc.: 96.88%] [G loss: 6.190379]\n",
      "epoch:32 step:25044 [D loss: 0.038729, acc.: 100.00%] [G loss: 6.324290]\n",
      "epoch:32 step:25045 [D loss: 0.002556, acc.: 100.00%] [G loss: 6.034367]\n",
      "epoch:32 step:25046 [D loss: 0.034774, acc.: 99.22%] [G loss: 4.506621]\n",
      "epoch:32 step:25047 [D loss: 0.010628, acc.: 100.00%] [G loss: 4.659128]\n",
      "epoch:32 step:25048 [D loss: 0.004925, acc.: 100.00%] [G loss: 4.410388]\n",
      "epoch:32 step:25049 [D loss: 0.030335, acc.: 100.00%] [G loss: 5.133671]\n",
      "epoch:32 step:25050 [D loss: 0.007223, acc.: 100.00%] [G loss: 5.542786]\n",
      "epoch:32 step:25051 [D loss: 0.003049, acc.: 100.00%] [G loss: 5.383110]\n",
      "epoch:32 step:25052 [D loss: 0.009952, acc.: 99.22%] [G loss: 4.826087]\n",
      "epoch:32 step:25053 [D loss: 0.208428, acc.: 94.53%] [G loss: 5.914439]\n",
      "epoch:32 step:25054 [D loss: 0.019883, acc.: 100.00%] [G loss: 7.064664]\n",
      "epoch:32 step:25055 [D loss: 0.382625, acc.: 84.38%] [G loss: 2.451378]\n",
      "epoch:32 step:25056 [D loss: 0.012455, acc.: 100.00%] [G loss: 2.373430]\n",
      "epoch:32 step:25057 [D loss: 0.115984, acc.: 96.09%] [G loss: 5.375350]\n",
      "epoch:32 step:25058 [D loss: 0.002583, acc.: 100.00%] [G loss: 5.783305]\n",
      "epoch:32 step:25059 [D loss: 0.004570, acc.: 100.00%] [G loss: 5.431767]\n",
      "epoch:32 step:25060 [D loss: 0.016044, acc.: 100.00%] [G loss: 4.852867]\n",
      "epoch:32 step:25061 [D loss: 0.031400, acc.: 98.44%] [G loss: 3.399165]\n",
      "epoch:32 step:25062 [D loss: 0.003590, acc.: 100.00%] [G loss: 3.640769]\n",
      "epoch:32 step:25063 [D loss: 0.036531, acc.: 99.22%] [G loss: 3.314242]\n",
      "epoch:32 step:25064 [D loss: 0.031927, acc.: 99.22%] [G loss: 4.431767]\n",
      "epoch:32 step:25065 [D loss: 0.010841, acc.: 100.00%] [G loss: 4.522402]\n",
      "epoch:32 step:25066 [D loss: 0.005548, acc.: 100.00%] [G loss: 4.986814]\n",
      "epoch:32 step:25067 [D loss: 0.011309, acc.: 100.00%] [G loss: 4.607656]\n",
      "epoch:32 step:25068 [D loss: 0.015322, acc.: 100.00%] [G loss: 3.703020]\n",
      "epoch:32 step:25069 [D loss: 4.620074, acc.: 20.31%] [G loss: 7.608274]\n",
      "epoch:32 step:25070 [D loss: 1.825491, acc.: 50.78%] [G loss: 4.941914]\n",
      "epoch:32 step:25071 [D loss: 1.120008, acc.: 53.12%] [G loss: 3.385850]\n",
      "epoch:32 step:25072 [D loss: 0.163117, acc.: 92.97%] [G loss: 3.811118]\n",
      "epoch:32 step:25073 [D loss: 0.159586, acc.: 96.88%] [G loss: 2.357370]\n",
      "epoch:32 step:25074 [D loss: 0.230119, acc.: 92.19%] [G loss: 3.014007]\n",
      "epoch:32 step:25075 [D loss: 0.089755, acc.: 97.66%] [G loss: 2.790783]\n",
      "epoch:32 step:25076 [D loss: 0.560231, acc.: 72.66%] [G loss: 2.962764]\n",
      "epoch:32 step:25077 [D loss: 0.019482, acc.: 100.00%] [G loss: 3.802267]\n",
      "epoch:32 step:25078 [D loss: 0.116016, acc.: 96.88%] [G loss: 4.189301]\n",
      "epoch:32 step:25079 [D loss: 0.089942, acc.: 98.44%] [G loss: 2.789976]\n",
      "epoch:32 step:25080 [D loss: 0.399289, acc.: 82.03%] [G loss: 3.375925]\n",
      "epoch:32 step:25081 [D loss: 0.049734, acc.: 99.22%] [G loss: 3.798368]\n",
      "epoch:32 step:25082 [D loss: 0.536931, acc.: 73.44%] [G loss: 2.059808]\n",
      "epoch:32 step:25083 [D loss: 0.078607, acc.: 99.22%] [G loss: 2.779873]\n",
      "epoch:32 step:25084 [D loss: 0.115564, acc.: 97.66%] [G loss: 2.312613]\n",
      "epoch:32 step:25085 [D loss: 0.034267, acc.: 100.00%] [G loss: 2.042485]\n",
      "epoch:32 step:25086 [D loss: 0.296860, acc.: 90.62%] [G loss: 2.026420]\n",
      "epoch:32 step:25087 [D loss: 0.088000, acc.: 98.44%] [G loss: 3.826550]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:32 step:25088 [D loss: 0.257401, acc.: 92.97%] [G loss: 2.930161]\n",
      "epoch:32 step:25089 [D loss: 0.125631, acc.: 96.88%] [G loss: 2.798038]\n",
      "epoch:32 step:25090 [D loss: 0.102514, acc.: 100.00%] [G loss: 3.340481]\n",
      "epoch:32 step:25091 [D loss: 0.489822, acc.: 78.12%] [G loss: 4.263500]\n",
      "epoch:32 step:25092 [D loss: 0.131292, acc.: 95.31%] [G loss: 3.804380]\n",
      "epoch:32 step:25093 [D loss: 0.068162, acc.: 98.44%] [G loss: 3.397784]\n",
      "epoch:32 step:25094 [D loss: 0.111388, acc.: 96.88%] [G loss: 2.804111]\n",
      "epoch:32 step:25095 [D loss: 0.145769, acc.: 96.09%] [G loss: 3.623736]\n",
      "epoch:32 step:25096 [D loss: 0.114271, acc.: 97.66%] [G loss: 2.778343]\n",
      "epoch:32 step:25097 [D loss: 0.092707, acc.: 97.66%] [G loss: 3.021998]\n",
      "epoch:32 step:25098 [D loss: 0.078769, acc.: 99.22%] [G loss: 3.872408]\n",
      "epoch:32 step:25099 [D loss: 0.161248, acc.: 93.75%] [G loss: 2.761584]\n",
      "epoch:32 step:25100 [D loss: 0.435276, acc.: 80.47%] [G loss: 5.364587]\n",
      "epoch:32 step:25101 [D loss: 0.208721, acc.: 90.62%] [G loss: 5.046214]\n",
      "epoch:32 step:25102 [D loss: 0.253365, acc.: 86.72%] [G loss: 2.370935]\n",
      "epoch:32 step:25103 [D loss: 0.233135, acc.: 88.28%] [G loss: 4.276453]\n",
      "epoch:32 step:25104 [D loss: 0.056842, acc.: 98.44%] [G loss: 5.119481]\n",
      "epoch:32 step:25105 [D loss: 0.189637, acc.: 92.19%] [G loss: 2.983731]\n",
      "epoch:32 step:25106 [D loss: 0.048493, acc.: 99.22%] [G loss: 2.380949]\n",
      "epoch:32 step:25107 [D loss: 0.085993, acc.: 97.66%] [G loss: 3.688228]\n",
      "epoch:32 step:25108 [D loss: 0.038712, acc.: 100.00%] [G loss: 3.166612]\n",
      "epoch:32 step:25109 [D loss: 0.063197, acc.: 99.22%] [G loss: 3.720924]\n",
      "epoch:32 step:25110 [D loss: 0.036541, acc.: 100.00%] [G loss: 2.993838]\n",
      "epoch:32 step:25111 [D loss: 0.304111, acc.: 86.72%] [G loss: 5.050767]\n",
      "epoch:32 step:25112 [D loss: 0.266119, acc.: 86.72%] [G loss: 3.117711]\n",
      "epoch:32 step:25113 [D loss: 0.184563, acc.: 89.84%] [G loss: 5.526617]\n",
      "epoch:32 step:25114 [D loss: 0.186627, acc.: 93.75%] [G loss: 4.462142]\n",
      "epoch:32 step:25115 [D loss: 0.153414, acc.: 93.75%] [G loss: 4.153546]\n",
      "epoch:32 step:25116 [D loss: 0.040993, acc.: 100.00%] [G loss: 3.966381]\n",
      "epoch:32 step:25117 [D loss: 0.025192, acc.: 100.00%] [G loss: 4.254720]\n",
      "epoch:32 step:25118 [D loss: 0.055043, acc.: 98.44%] [G loss: 4.195459]\n",
      "epoch:32 step:25119 [D loss: 0.128076, acc.: 96.88%] [G loss: 3.386171]\n",
      "epoch:32 step:25120 [D loss: 0.027364, acc.: 99.22%] [G loss: 3.752537]\n",
      "epoch:32 step:25121 [D loss: 0.104466, acc.: 98.44%] [G loss: 2.950927]\n",
      "epoch:32 step:25122 [D loss: 0.238246, acc.: 89.84%] [G loss: 5.470094]\n",
      "epoch:32 step:25123 [D loss: 0.563604, acc.: 73.44%] [G loss: 1.757059]\n",
      "epoch:32 step:25124 [D loss: 0.083675, acc.: 98.44%] [G loss: 2.373594]\n",
      "epoch:32 step:25125 [D loss: 0.042323, acc.: 100.00%] [G loss: 4.635244]\n",
      "epoch:32 step:25126 [D loss: 0.038797, acc.: 100.00%] [G loss: 4.703733]\n",
      "epoch:32 step:25127 [D loss: 0.054382, acc.: 100.00%] [G loss: 3.920287]\n",
      "epoch:32 step:25128 [D loss: 0.108977, acc.: 97.66%] [G loss: 3.634499]\n",
      "epoch:32 step:25129 [D loss: 0.134284, acc.: 96.88%] [G loss: 3.823320]\n",
      "epoch:32 step:25130 [D loss: 0.051898, acc.: 97.66%] [G loss: 4.116480]\n",
      "epoch:32 step:25131 [D loss: 0.093345, acc.: 97.66%] [G loss: 3.929140]\n",
      "epoch:32 step:25132 [D loss: 0.137891, acc.: 97.66%] [G loss: 2.683275]\n",
      "epoch:32 step:25133 [D loss: 0.134356, acc.: 96.09%] [G loss: 4.049222]\n",
      "epoch:32 step:25134 [D loss: 0.099840, acc.: 96.88%] [G loss: 4.214264]\n",
      "epoch:32 step:25135 [D loss: 0.132637, acc.: 97.66%] [G loss: 2.924345]\n",
      "epoch:32 step:25136 [D loss: 0.032675, acc.: 100.00%] [G loss: 4.288344]\n",
      "epoch:32 step:25137 [D loss: 0.115154, acc.: 97.66%] [G loss: 4.732099]\n",
      "epoch:32 step:25138 [D loss: 0.021654, acc.: 100.00%] [G loss: 4.381804]\n",
      "epoch:32 step:25139 [D loss: 0.360688, acc.: 82.03%] [G loss: 5.708480]\n",
      "epoch:32 step:25140 [D loss: 0.203262, acc.: 89.84%] [G loss: 4.665660]\n",
      "epoch:32 step:25141 [D loss: 0.023211, acc.: 100.00%] [G loss: 3.336185]\n",
      "epoch:32 step:25142 [D loss: 0.033428, acc.: 100.00%] [G loss: 3.954072]\n",
      "epoch:32 step:25143 [D loss: 0.029475, acc.: 99.22%] [G loss: 4.190310]\n",
      "epoch:32 step:25144 [D loss: 0.146030, acc.: 96.88%] [G loss: 3.979126]\n",
      "epoch:32 step:25145 [D loss: 0.065802, acc.: 99.22%] [G loss: 4.114310]\n",
      "epoch:32 step:25146 [D loss: 0.033513, acc.: 99.22%] [G loss: 4.484896]\n",
      "epoch:32 step:25147 [D loss: 0.428090, acc.: 81.25%] [G loss: 5.909181]\n",
      "epoch:32 step:25148 [D loss: 0.153185, acc.: 92.97%] [G loss: 5.692291]\n",
      "epoch:32 step:25149 [D loss: 0.046362, acc.: 100.00%] [G loss: 4.511116]\n",
      "epoch:32 step:25150 [D loss: 0.043664, acc.: 99.22%] [G loss: 4.001463]\n",
      "epoch:32 step:25151 [D loss: 0.014392, acc.: 100.00%] [G loss: 4.388834]\n",
      "epoch:32 step:25152 [D loss: 0.053822, acc.: 100.00%] [G loss: 4.154925]\n",
      "epoch:32 step:25153 [D loss: 0.091560, acc.: 96.88%] [G loss: 3.681316]\n",
      "epoch:32 step:25154 [D loss: 0.060287, acc.: 98.44%] [G loss: 4.171333]\n",
      "epoch:32 step:25155 [D loss: 0.052810, acc.: 99.22%] [G loss: 4.671093]\n",
      "epoch:32 step:25156 [D loss: 0.120236, acc.: 95.31%] [G loss: 2.584253]\n",
      "epoch:32 step:25157 [D loss: 0.082366, acc.: 100.00%] [G loss: 2.996879]\n",
      "epoch:32 step:25158 [D loss: 0.015802, acc.: 100.00%] [G loss: 4.506157]\n",
      "epoch:32 step:25159 [D loss: 0.037898, acc.: 99.22%] [G loss: 4.102140]\n",
      "epoch:32 step:25160 [D loss: 0.087751, acc.: 100.00%] [G loss: 4.601441]\n",
      "epoch:32 step:25161 [D loss: 0.014327, acc.: 100.00%] [G loss: 5.463686]\n",
      "epoch:32 step:25162 [D loss: 0.159193, acc.: 93.75%] [G loss: 2.283779]\n",
      "epoch:32 step:25163 [D loss: 0.252360, acc.: 86.72%] [G loss: 7.028557]\n",
      "epoch:32 step:25164 [D loss: 0.376535, acc.: 81.25%] [G loss: 4.224621]\n",
      "epoch:32 step:25165 [D loss: 0.167454, acc.: 92.97%] [G loss: 6.331994]\n",
      "epoch:32 step:25166 [D loss: 0.647414, acc.: 65.62%] [G loss: 6.321717]\n",
      "epoch:32 step:25167 [D loss: 0.003551, acc.: 100.00%] [G loss: 7.107025]\n",
      "epoch:32 step:25168 [D loss: 0.264967, acc.: 85.94%] [G loss: 3.018984]\n",
      "epoch:32 step:25169 [D loss: 0.145603, acc.: 92.97%] [G loss: 5.166295]\n",
      "epoch:32 step:25170 [D loss: 0.030199, acc.: 100.00%] [G loss: 5.537343]\n",
      "epoch:32 step:25171 [D loss: 0.029874, acc.: 99.22%] [G loss: 5.557942]\n",
      "epoch:32 step:25172 [D loss: 0.019016, acc.: 100.00%] [G loss: 4.649281]\n",
      "epoch:32 step:25173 [D loss: 0.052243, acc.: 100.00%] [G loss: 5.778725]\n",
      "epoch:32 step:25174 [D loss: 0.026907, acc.: 99.22%] [G loss: 4.640620]\n",
      "epoch:32 step:25175 [D loss: 0.033823, acc.: 100.00%] [G loss: 4.078628]\n",
      "epoch:32 step:25176 [D loss: 0.049931, acc.: 99.22%] [G loss: 3.474143]\n",
      "epoch:32 step:25177 [D loss: 0.087548, acc.: 96.09%] [G loss: 5.062635]\n",
      "epoch:32 step:25178 [D loss: 0.008715, acc.: 100.00%] [G loss: 6.388778]\n",
      "epoch:32 step:25179 [D loss: 1.113241, acc.: 53.91%] [G loss: 7.781553]\n",
      "epoch:32 step:25180 [D loss: 0.757544, acc.: 69.53%] [G loss: 4.311671]\n",
      "epoch:32 step:25181 [D loss: 0.099139, acc.: 96.09%] [G loss: 5.550280]\n",
      "epoch:32 step:25182 [D loss: 0.028863, acc.: 99.22%] [G loss: 5.576768]\n",
      "epoch:32 step:25183 [D loss: 0.133294, acc.: 93.75%] [G loss: 3.600939]\n",
      "epoch:32 step:25184 [D loss: 0.052895, acc.: 99.22%] [G loss: 3.840644]\n",
      "epoch:32 step:25185 [D loss: 0.029818, acc.: 100.00%] [G loss: 5.452130]\n",
      "epoch:32 step:25186 [D loss: 0.005952, acc.: 100.00%] [G loss: 4.671506]\n",
      "epoch:32 step:25187 [D loss: 0.018664, acc.: 100.00%] [G loss: 5.306730]\n",
      "epoch:32 step:25188 [D loss: 0.040276, acc.: 99.22%] [G loss: 5.500980]\n",
      "epoch:32 step:25189 [D loss: 0.014625, acc.: 100.00%] [G loss: 5.316208]\n",
      "epoch:32 step:25190 [D loss: 0.025287, acc.: 100.00%] [G loss: 4.377718]\n",
      "epoch:32 step:25191 [D loss: 0.023360, acc.: 100.00%] [G loss: 3.926317]\n",
      "epoch:32 step:25192 [D loss: 0.034947, acc.: 99.22%] [G loss: 4.833856]\n",
      "epoch:32 step:25193 [D loss: 0.045020, acc.: 99.22%] [G loss: 3.739177]\n",
      "epoch:32 step:25194 [D loss: 0.041039, acc.: 100.00%] [G loss: 5.049012]\n",
      "epoch:32 step:25195 [D loss: 0.072609, acc.: 97.66%] [G loss: 4.658336]\n",
      "epoch:32 step:25196 [D loss: 0.062055, acc.: 98.44%] [G loss: 4.820795]\n",
      "epoch:32 step:25197 [D loss: 0.028896, acc.: 100.00%] [G loss: 5.019372]\n",
      "epoch:32 step:25198 [D loss: 0.035980, acc.: 100.00%] [G loss: 4.565042]\n",
      "epoch:32 step:25199 [D loss: 0.017097, acc.: 100.00%] [G loss: 4.628483]\n",
      "epoch:32 step:25200 [D loss: 0.038986, acc.: 100.00%] [G loss: 3.918376]\n",
      "##############\n",
      "[0.92924145 1.01393187 0.93064628 1.03784814 1.05695515 0.99399677\n",
      " 0.89074985 0.96340138 2.10681426 0.91412073]\n",
      "##########\n",
      "epoch:32 step:25201 [D loss: 0.112831, acc.: 98.44%] [G loss: 5.350205]\n",
      "epoch:32 step:25202 [D loss: 0.108604, acc.: 94.53%] [G loss: 3.965924]\n",
      "epoch:32 step:25203 [D loss: 0.034365, acc.: 99.22%] [G loss: 4.352158]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:32 step:25204 [D loss: 0.017564, acc.: 100.00%] [G loss: 4.169100]\n",
      "epoch:32 step:25205 [D loss: 0.073455, acc.: 100.00%] [G loss: 5.330488]\n",
      "epoch:32 step:25206 [D loss: 0.146375, acc.: 92.97%] [G loss: 3.767452]\n",
      "epoch:32 step:25207 [D loss: 0.081823, acc.: 99.22%] [G loss: 5.351321]\n",
      "epoch:32 step:25208 [D loss: 0.008561, acc.: 100.00%] [G loss: 5.842095]\n",
      "epoch:32 step:25209 [D loss: 0.019454, acc.: 100.00%] [G loss: 5.884812]\n",
      "epoch:32 step:25210 [D loss: 0.176225, acc.: 93.75%] [G loss: 2.909285]\n",
      "epoch:32 step:25211 [D loss: 0.046955, acc.: 100.00%] [G loss: 4.552876]\n",
      "epoch:32 step:25212 [D loss: 0.069776, acc.: 96.09%] [G loss: 5.054331]\n",
      "epoch:32 step:25213 [D loss: 0.027256, acc.: 99.22%] [G loss: 4.999639]\n",
      "epoch:32 step:25214 [D loss: 0.017336, acc.: 100.00%] [G loss: 5.009318]\n",
      "epoch:32 step:25215 [D loss: 0.012858, acc.: 100.00%] [G loss: 4.744717]\n",
      "epoch:32 step:25216 [D loss: 0.010562, acc.: 100.00%] [G loss: 4.712934]\n",
      "epoch:32 step:25217 [D loss: 0.008128, acc.: 100.00%] [G loss: 4.344510]\n",
      "epoch:32 step:25218 [D loss: 0.008197, acc.: 100.00%] [G loss: 4.176786]\n",
      "epoch:32 step:25219 [D loss: 0.016552, acc.: 100.00%] [G loss: 3.561805]\n",
      "epoch:32 step:25220 [D loss: 0.047611, acc.: 99.22%] [G loss: 5.271351]\n",
      "epoch:32 step:25221 [D loss: 0.230352, acc.: 91.41%] [G loss: 4.371408]\n",
      "epoch:32 step:25222 [D loss: 0.012247, acc.: 100.00%] [G loss: 6.117145]\n",
      "epoch:32 step:25223 [D loss: 0.019036, acc.: 100.00%] [G loss: 6.097225]\n",
      "epoch:32 step:25224 [D loss: 0.364322, acc.: 82.03%] [G loss: 7.328827]\n",
      "epoch:32 step:25225 [D loss: 0.050893, acc.: 99.22%] [G loss: 7.744159]\n",
      "epoch:32 step:25226 [D loss: 1.593763, acc.: 38.28%] [G loss: 7.826344]\n",
      "epoch:32 step:25227 [D loss: 0.359288, acc.: 78.91%] [G loss: 7.674071]\n",
      "epoch:32 step:25228 [D loss: 0.002271, acc.: 100.00%] [G loss: 6.040647]\n",
      "epoch:32 step:25229 [D loss: 0.006864, acc.: 100.00%] [G loss: 6.834101]\n",
      "epoch:32 step:25230 [D loss: 0.004603, acc.: 100.00%] [G loss: 6.548636]\n",
      "epoch:32 step:25231 [D loss: 0.003578, acc.: 100.00%] [G loss: 5.716928]\n",
      "epoch:32 step:25232 [D loss: 0.015477, acc.: 100.00%] [G loss: 5.462046]\n",
      "epoch:32 step:25233 [D loss: 0.105621, acc.: 97.66%] [G loss: 5.821635]\n",
      "epoch:32 step:25234 [D loss: 0.003365, acc.: 100.00%] [G loss: 6.346392]\n",
      "epoch:32 step:25235 [D loss: 0.011931, acc.: 99.22%] [G loss: 5.889869]\n",
      "epoch:32 step:25236 [D loss: 0.002740, acc.: 100.00%] [G loss: 6.052579]\n",
      "epoch:32 step:25237 [D loss: 0.004447, acc.: 100.00%] [G loss: 6.213773]\n",
      "epoch:32 step:25238 [D loss: 0.052557, acc.: 98.44%] [G loss: 5.061197]\n",
      "epoch:32 step:25239 [D loss: 0.164389, acc.: 92.97%] [G loss: 5.859964]\n",
      "epoch:32 step:25240 [D loss: 0.022036, acc.: 99.22%] [G loss: 6.873693]\n",
      "epoch:32 step:25241 [D loss: 0.054743, acc.: 99.22%] [G loss: 5.500425]\n",
      "epoch:32 step:25242 [D loss: 0.175303, acc.: 92.97%] [G loss: 5.366826]\n",
      "epoch:32 step:25243 [D loss: 0.007111, acc.: 100.00%] [G loss: 5.302060]\n",
      "epoch:32 step:25244 [D loss: 0.018628, acc.: 100.00%] [G loss: 4.747797]\n",
      "epoch:32 step:25245 [D loss: 0.016722, acc.: 100.00%] [G loss: 5.058520]\n",
      "epoch:32 step:25246 [D loss: 0.004092, acc.: 100.00%] [G loss: 4.735664]\n",
      "epoch:32 step:25247 [D loss: 0.005906, acc.: 100.00%] [G loss: 4.140270]\n",
      "epoch:32 step:25248 [D loss: 0.012663, acc.: 100.00%] [G loss: 3.588183]\n",
      "epoch:32 step:25249 [D loss: 0.012935, acc.: 100.00%] [G loss: 4.480482]\n",
      "epoch:32 step:25250 [D loss: 0.059049, acc.: 99.22%] [G loss: 4.361692]\n",
      "epoch:32 step:25251 [D loss: 0.004327, acc.: 100.00%] [G loss: 5.834428]\n",
      "epoch:32 step:25252 [D loss: 0.144137, acc.: 98.44%] [G loss: 5.324716]\n",
      "epoch:32 step:25253 [D loss: 0.155854, acc.: 95.31%] [G loss: 4.140347]\n",
      "epoch:32 step:25254 [D loss: 0.007078, acc.: 100.00%] [G loss: 4.923663]\n",
      "epoch:32 step:25255 [D loss: 0.003309, acc.: 100.00%] [G loss: 4.732864]\n",
      "epoch:32 step:25256 [D loss: 0.066974, acc.: 98.44%] [G loss: 4.769681]\n",
      "epoch:32 step:25257 [D loss: 0.013331, acc.: 100.00%] [G loss: 4.227179]\n",
      "epoch:32 step:25258 [D loss: 0.023495, acc.: 100.00%] [G loss: 4.811182]\n",
      "epoch:32 step:25259 [D loss: 0.014349, acc.: 100.00%] [G loss: 5.165496]\n",
      "epoch:32 step:25260 [D loss: 0.005564, acc.: 100.00%] [G loss: 5.818203]\n",
      "epoch:32 step:25261 [D loss: 0.004428, acc.: 100.00%] [G loss: 4.923315]\n",
      "epoch:32 step:25262 [D loss: 0.035056, acc.: 99.22%] [G loss: 4.030733]\n",
      "epoch:32 step:25263 [D loss: 0.014871, acc.: 100.00%] [G loss: 5.153710]\n",
      "epoch:32 step:25264 [D loss: 0.006946, acc.: 100.00%] [G loss: 4.456829]\n",
      "epoch:32 step:25265 [D loss: 0.037385, acc.: 100.00%] [G loss: 4.725110]\n",
      "epoch:32 step:25266 [D loss: 0.022419, acc.: 100.00%] [G loss: 5.490989]\n",
      "epoch:32 step:25267 [D loss: 0.006991, acc.: 100.00%] [G loss: 6.395585]\n",
      "epoch:32 step:25268 [D loss: 0.062908, acc.: 97.66%] [G loss: 3.888076]\n",
      "epoch:32 step:25269 [D loss: 0.056147, acc.: 98.44%] [G loss: 5.928364]\n",
      "epoch:32 step:25270 [D loss: 0.007372, acc.: 100.00%] [G loss: 6.158687]\n",
      "epoch:32 step:25271 [D loss: 0.078706, acc.: 97.66%] [G loss: 4.552745]\n",
      "epoch:32 step:25272 [D loss: 0.027531, acc.: 100.00%] [G loss: 4.685851]\n",
      "epoch:32 step:25273 [D loss: 0.005643, acc.: 100.00%] [G loss: 5.227158]\n",
      "epoch:32 step:25274 [D loss: 0.011554, acc.: 100.00%] [G loss: 5.428859]\n",
      "epoch:32 step:25275 [D loss: 0.005607, acc.: 100.00%] [G loss: 4.924118]\n",
      "epoch:32 step:25276 [D loss: 0.008966, acc.: 100.00%] [G loss: 5.221131]\n",
      "epoch:32 step:25277 [D loss: 0.005418, acc.: 100.00%] [G loss: 5.145371]\n",
      "epoch:32 step:25278 [D loss: 0.269023, acc.: 88.28%] [G loss: 5.662733]\n",
      "epoch:32 step:25279 [D loss: 0.014594, acc.: 100.00%] [G loss: 6.890464]\n",
      "epoch:32 step:25280 [D loss: 0.003362, acc.: 100.00%] [G loss: 6.596457]\n",
      "epoch:32 step:25281 [D loss: 0.032705, acc.: 99.22%] [G loss: 6.168445]\n",
      "epoch:32 step:25282 [D loss: 0.004302, acc.: 100.00%] [G loss: 5.725827]\n",
      "epoch:32 step:25283 [D loss: 0.012514, acc.: 100.00%] [G loss: 5.646531]\n",
      "epoch:32 step:25284 [D loss: 0.009545, acc.: 100.00%] [G loss: 5.916468]\n",
      "epoch:32 step:25285 [D loss: 0.007181, acc.: 100.00%] [G loss: 5.766229]\n",
      "epoch:32 step:25286 [D loss: 0.008658, acc.: 100.00%] [G loss: 5.825639]\n",
      "epoch:32 step:25287 [D loss: 0.049030, acc.: 98.44%] [G loss: 3.730324]\n",
      "epoch:32 step:25288 [D loss: 0.023713, acc.: 99.22%] [G loss: 4.155924]\n",
      "epoch:32 step:25289 [D loss: 0.010345, acc.: 100.00%] [G loss: 4.180806]\n",
      "epoch:32 step:25290 [D loss: 0.024308, acc.: 99.22%] [G loss: 4.504572]\n",
      "epoch:32 step:25291 [D loss: 0.029934, acc.: 100.00%] [G loss: 5.254457]\n",
      "epoch:32 step:25292 [D loss: 0.007390, acc.: 100.00%] [G loss: 4.996117]\n",
      "epoch:32 step:25293 [D loss: 0.005498, acc.: 100.00%] [G loss: 4.903995]\n",
      "epoch:32 step:25294 [D loss: 0.016276, acc.: 100.00%] [G loss: 5.381547]\n",
      "epoch:32 step:25295 [D loss: 0.072602, acc.: 98.44%] [G loss: 6.136199]\n",
      "epoch:32 step:25296 [D loss: 0.002748, acc.: 100.00%] [G loss: 7.290915]\n",
      "epoch:32 step:25297 [D loss: 0.607472, acc.: 68.75%] [G loss: 9.178005]\n",
      "epoch:32 step:25298 [D loss: 3.100774, acc.: 50.00%] [G loss: 4.270413]\n",
      "epoch:32 step:25299 [D loss: 0.115735, acc.: 95.31%] [G loss: 4.048414]\n",
      "epoch:32 step:25300 [D loss: 0.017209, acc.: 100.00%] [G loss: 4.736467]\n",
      "epoch:32 step:25301 [D loss: 0.079477, acc.: 97.66%] [G loss: 3.336009]\n",
      "epoch:32 step:25302 [D loss: 0.519546, acc.: 75.78%] [G loss: 7.101768]\n",
      "epoch:32 step:25303 [D loss: 1.473122, acc.: 50.78%] [G loss: 4.757669]\n",
      "epoch:32 step:25304 [D loss: 0.069022, acc.: 97.66%] [G loss: 3.907534]\n",
      "epoch:32 step:25305 [D loss: 0.167458, acc.: 91.41%] [G loss: 4.855150]\n",
      "epoch:32 step:25306 [D loss: 0.007496, acc.: 100.00%] [G loss: 4.272191]\n",
      "epoch:32 step:25307 [D loss: 0.068364, acc.: 98.44%] [G loss: 3.822458]\n",
      "epoch:32 step:25308 [D loss: 0.095064, acc.: 97.66%] [G loss: 4.358002]\n",
      "epoch:32 step:25309 [D loss: 0.034302, acc.: 99.22%] [G loss: 5.018863]\n",
      "epoch:32 step:25310 [D loss: 0.007942, acc.: 100.00%] [G loss: 4.504094]\n",
      "epoch:32 step:25311 [D loss: 0.010894, acc.: 100.00%] [G loss: 4.966932]\n",
      "epoch:32 step:25312 [D loss: 0.029093, acc.: 100.00%] [G loss: 3.374985]\n",
      "epoch:32 step:25313 [D loss: 0.022927, acc.: 100.00%] [G loss: 4.064888]\n",
      "epoch:32 step:25314 [D loss: 0.026986, acc.: 100.00%] [G loss: 4.118662]\n",
      "epoch:32 step:25315 [D loss: 0.010345, acc.: 100.00%] [G loss: 4.523530]\n",
      "epoch:32 step:25316 [D loss: 0.054150, acc.: 99.22%] [G loss: 4.127913]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:32 step:25317 [D loss: 0.014253, acc.: 100.00%] [G loss: 2.949066]\n",
      "epoch:32 step:25318 [D loss: 0.017583, acc.: 100.00%] [G loss: 3.622982]\n",
      "epoch:32 step:25319 [D loss: 0.096644, acc.: 98.44%] [G loss: 4.066698]\n",
      "epoch:32 step:25320 [D loss: 0.017668, acc.: 100.00%] [G loss: 5.075854]\n",
      "epoch:32 step:25321 [D loss: 0.016556, acc.: 100.00%] [G loss: 5.035228]\n",
      "epoch:32 step:25322 [D loss: 0.011009, acc.: 100.00%] [G loss: 4.971764]\n",
      "epoch:32 step:25323 [D loss: 0.026124, acc.: 100.00%] [G loss: 4.327706]\n",
      "epoch:32 step:25324 [D loss: 0.028910, acc.: 100.00%] [G loss: 3.365778]\n",
      "epoch:32 step:25325 [D loss: 0.013476, acc.: 100.00%] [G loss: 4.615100]\n",
      "epoch:32 step:25326 [D loss: 0.026483, acc.: 100.00%] [G loss: 4.220361]\n",
      "epoch:32 step:25327 [D loss: 0.031520, acc.: 100.00%] [G loss: 4.555433]\n",
      "epoch:32 step:25328 [D loss: 1.584501, acc.: 30.47%] [G loss: 6.701355]\n",
      "epoch:32 step:25329 [D loss: 2.360381, acc.: 50.00%] [G loss: 5.419037]\n",
      "epoch:32 step:25330 [D loss: 1.249203, acc.: 50.78%] [G loss: 3.391959]\n",
      "epoch:32 step:25331 [D loss: 0.171197, acc.: 92.19%] [G loss: 3.371959]\n",
      "epoch:32 step:25332 [D loss: 0.082150, acc.: 98.44%] [G loss: 4.257324]\n",
      "epoch:32 step:25333 [D loss: 0.014382, acc.: 100.00%] [G loss: 4.601002]\n",
      "epoch:32 step:25334 [D loss: 0.025726, acc.: 99.22%] [G loss: 3.764223]\n",
      "epoch:32 step:25335 [D loss: 0.106566, acc.: 96.88%] [G loss: 3.965846]\n",
      "epoch:32 step:25336 [D loss: 0.018715, acc.: 100.00%] [G loss: 4.666050]\n",
      "epoch:32 step:25337 [D loss: 0.027289, acc.: 100.00%] [G loss: 3.959948]\n",
      "epoch:32 step:25338 [D loss: 0.157405, acc.: 93.75%] [G loss: 3.392114]\n",
      "epoch:32 step:25339 [D loss: 0.038106, acc.: 99.22%] [G loss: 3.587809]\n",
      "epoch:32 step:25340 [D loss: 0.047738, acc.: 99.22%] [G loss: 3.045998]\n",
      "epoch:32 step:25341 [D loss: 0.020499, acc.: 100.00%] [G loss: 3.185848]\n",
      "epoch:32 step:25342 [D loss: 0.021900, acc.: 100.00%] [G loss: 3.847336]\n",
      "epoch:32 step:25343 [D loss: 0.058722, acc.: 99.22%] [G loss: 1.930614]\n",
      "epoch:32 step:25344 [D loss: 0.076157, acc.: 99.22%] [G loss: 3.304929]\n",
      "epoch:32 step:25345 [D loss: 0.022934, acc.: 100.00%] [G loss: 3.568924]\n",
      "epoch:32 step:25346 [D loss: 0.101841, acc.: 97.66%] [G loss: 1.347769]\n",
      "epoch:32 step:25347 [D loss: 1.138152, acc.: 45.31%] [G loss: 5.966875]\n",
      "epoch:32 step:25348 [D loss: 0.445843, acc.: 74.22%] [G loss: 4.971544]\n",
      "epoch:32 step:25349 [D loss: 0.052455, acc.: 98.44%] [G loss: 3.931946]\n",
      "epoch:32 step:25350 [D loss: 0.020933, acc.: 100.00%] [G loss: 3.667959]\n",
      "epoch:32 step:25351 [D loss: 0.146326, acc.: 95.31%] [G loss: 3.687095]\n",
      "epoch:32 step:25352 [D loss: 0.028052, acc.: 100.00%] [G loss: 3.543339]\n",
      "epoch:32 step:25353 [D loss: 0.026876, acc.: 100.00%] [G loss: 4.523394]\n",
      "epoch:32 step:25354 [D loss: 0.097660, acc.: 98.44%] [G loss: 2.464000]\n",
      "epoch:32 step:25355 [D loss: 0.028002, acc.: 100.00%] [G loss: 3.347152]\n",
      "epoch:32 step:25356 [D loss: 0.093341, acc.: 100.00%] [G loss: 4.010943]\n",
      "epoch:32 step:25357 [D loss: 0.033822, acc.: 100.00%] [G loss: 4.267063]\n",
      "epoch:32 step:25358 [D loss: 0.015735, acc.: 100.00%] [G loss: 3.606291]\n",
      "epoch:32 step:25359 [D loss: 0.023814, acc.: 100.00%] [G loss: 3.431730]\n",
      "epoch:32 step:25360 [D loss: 0.073062, acc.: 99.22%] [G loss: 2.378820]\n",
      "epoch:32 step:25361 [D loss: 0.023441, acc.: 100.00%] [G loss: 2.792078]\n",
      "epoch:32 step:25362 [D loss: 0.991256, acc.: 56.25%] [G loss: 6.732678]\n",
      "epoch:32 step:25363 [D loss: 0.862254, acc.: 69.53%] [G loss: 5.589953]\n",
      "epoch:32 step:25364 [D loss: 0.391206, acc.: 85.16%] [G loss: 3.402129]\n",
      "epoch:32 step:25365 [D loss: 0.144227, acc.: 94.53%] [G loss: 3.160162]\n",
      "epoch:32 step:25366 [D loss: 0.126941, acc.: 96.88%] [G loss: 3.963502]\n",
      "epoch:32 step:25367 [D loss: 0.011854, acc.: 100.00%] [G loss: 4.471787]\n",
      "epoch:32 step:25368 [D loss: 0.018136, acc.: 100.00%] [G loss: 4.405901]\n",
      "epoch:32 step:25369 [D loss: 0.026629, acc.: 99.22%] [G loss: 4.150748]\n",
      "epoch:32 step:25370 [D loss: 0.051971, acc.: 100.00%] [G loss: 4.462245]\n",
      "epoch:32 step:25371 [D loss: 0.013065, acc.: 100.00%] [G loss: 4.563770]\n",
      "epoch:32 step:25372 [D loss: 0.012084, acc.: 100.00%] [G loss: 4.223849]\n",
      "epoch:32 step:25373 [D loss: 0.009637, acc.: 100.00%] [G loss: 4.424768]\n",
      "epoch:32 step:25374 [D loss: 0.037262, acc.: 99.22%] [G loss: 4.063106]\n",
      "epoch:32 step:25375 [D loss: 0.042426, acc.: 100.00%] [G loss: 4.136494]\n",
      "epoch:32 step:25376 [D loss: 0.051068, acc.: 99.22%] [G loss: 3.944196]\n",
      "epoch:32 step:25377 [D loss: 0.069186, acc.: 97.66%] [G loss: 4.177523]\n",
      "epoch:32 step:25378 [D loss: 0.044980, acc.: 99.22%] [G loss: 4.268650]\n",
      "epoch:32 step:25379 [D loss: 0.051590, acc.: 99.22%] [G loss: 3.052190]\n",
      "epoch:32 step:25380 [D loss: 0.021744, acc.: 100.00%] [G loss: 3.435802]\n",
      "epoch:32 step:25381 [D loss: 0.007136, acc.: 100.00%] [G loss: 3.827282]\n",
      "epoch:32 step:25382 [D loss: 0.031320, acc.: 100.00%] [G loss: 4.705462]\n",
      "epoch:32 step:25383 [D loss: 0.020299, acc.: 100.00%] [G loss: 3.581090]\n",
      "epoch:32 step:25384 [D loss: 0.012862, acc.: 100.00%] [G loss: 3.509570]\n",
      "epoch:32 step:25385 [D loss: 0.182982, acc.: 91.41%] [G loss: 3.138670]\n",
      "epoch:32 step:25386 [D loss: 0.116617, acc.: 97.66%] [G loss: 4.263985]\n",
      "epoch:32 step:25387 [D loss: 0.058583, acc.: 98.44%] [G loss: 4.728456]\n",
      "epoch:32 step:25388 [D loss: 0.017932, acc.: 100.00%] [G loss: 4.323384]\n",
      "epoch:32 step:25389 [D loss: 0.008764, acc.: 100.00%] [G loss: 4.179722]\n",
      "epoch:32 step:25390 [D loss: 0.010043, acc.: 100.00%] [G loss: 4.355866]\n",
      "epoch:32 step:25391 [D loss: 0.008983, acc.: 100.00%] [G loss: 3.857279]\n",
      "epoch:32 step:25392 [D loss: 0.014067, acc.: 100.00%] [G loss: 3.538842]\n",
      "epoch:32 step:25393 [D loss: 0.115596, acc.: 96.88%] [G loss: 3.521872]\n",
      "epoch:32 step:25394 [D loss: 0.036366, acc.: 100.00%] [G loss: 4.050649]\n",
      "epoch:32 step:25395 [D loss: 0.050589, acc.: 98.44%] [G loss: 4.340584]\n",
      "epoch:32 step:25396 [D loss: 0.018280, acc.: 100.00%] [G loss: 3.990931]\n",
      "epoch:32 step:25397 [D loss: 0.045546, acc.: 100.00%] [G loss: 3.737281]\n",
      "epoch:32 step:25398 [D loss: 0.013115, acc.: 100.00%] [G loss: 4.424312]\n",
      "epoch:32 step:25399 [D loss: 0.017416, acc.: 100.00%] [G loss: 4.945747]\n",
      "epoch:32 step:25400 [D loss: 0.008186, acc.: 100.00%] [G loss: 4.708798]\n",
      "##############\n",
      "[0.98598571 0.98875942 1.07074821 0.99440644 2.09673265 2.11734628\n",
      " 1.04295444 2.12236149 2.12565572 1.07451113]\n",
      "##########\n",
      "epoch:32 step:25401 [D loss: 0.009068, acc.: 100.00%] [G loss: 3.954435]\n",
      "epoch:32 step:25402 [D loss: 0.010563, acc.: 100.00%] [G loss: 3.972886]\n",
      "epoch:32 step:25403 [D loss: 0.067078, acc.: 100.00%] [G loss: 4.779711]\n",
      "epoch:32 step:25404 [D loss: 0.009895, acc.: 100.00%] [G loss: 3.763958]\n",
      "epoch:32 step:25405 [D loss: 0.035161, acc.: 100.00%] [G loss: 3.915984]\n",
      "epoch:32 step:25406 [D loss: 0.019058, acc.: 100.00%] [G loss: 3.668453]\n",
      "epoch:32 step:25407 [D loss: 0.024614, acc.: 100.00%] [G loss: 4.739711]\n",
      "epoch:32 step:25408 [D loss: 0.029235, acc.: 100.00%] [G loss: 3.934232]\n",
      "epoch:32 step:25409 [D loss: 0.078471, acc.: 98.44%] [G loss: 4.011901]\n",
      "epoch:32 step:25410 [D loss: 0.015667, acc.: 100.00%] [G loss: 4.261061]\n",
      "epoch:32 step:25411 [D loss: 0.014154, acc.: 100.00%] [G loss: 4.997556]\n",
      "epoch:32 step:25412 [D loss: 0.986274, acc.: 53.12%] [G loss: 6.983293]\n",
      "epoch:32 step:25413 [D loss: 2.018307, acc.: 50.00%] [G loss: 5.200562]\n",
      "epoch:32 step:25414 [D loss: 0.230500, acc.: 89.84%] [G loss: 3.962554]\n",
      "epoch:32 step:25415 [D loss: 0.024379, acc.: 100.00%] [G loss: 4.083926]\n",
      "epoch:32 step:25416 [D loss: 0.065911, acc.: 97.66%] [G loss: 2.915143]\n",
      "epoch:32 step:25417 [D loss: 0.018897, acc.: 100.00%] [G loss: 4.363330]\n",
      "epoch:32 step:25418 [D loss: 0.009613, acc.: 100.00%] [G loss: 3.934680]\n",
      "epoch:32 step:25419 [D loss: 0.057165, acc.: 99.22%] [G loss: 4.171350]\n",
      "epoch:32 step:25420 [D loss: 0.014991, acc.: 100.00%] [G loss: 4.159102]\n",
      "epoch:32 step:25421 [D loss: 0.030855, acc.: 100.00%] [G loss: 3.623567]\n",
      "epoch:32 step:25422 [D loss: 0.029541, acc.: 99.22%] [G loss: 3.198639]\n",
      "epoch:32 step:25423 [D loss: 0.142446, acc.: 95.31%] [G loss: 3.189811]\n",
      "epoch:32 step:25424 [D loss: 0.030970, acc.: 99.22%] [G loss: 3.475785]\n",
      "epoch:32 step:25425 [D loss: 0.035502, acc.: 100.00%] [G loss: 3.401914]\n",
      "epoch:32 step:25426 [D loss: 0.019455, acc.: 100.00%] [G loss: 3.744076]\n",
      "epoch:32 step:25427 [D loss: 0.017192, acc.: 100.00%] [G loss: 3.516775]\n",
      "epoch:32 step:25428 [D loss: 0.066188, acc.: 100.00%] [G loss: 4.201684]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:32 step:25429 [D loss: 0.020100, acc.: 99.22%] [G loss: 4.303841]\n",
      "epoch:32 step:25430 [D loss: 0.045510, acc.: 99.22%] [G loss: 3.549443]\n",
      "epoch:32 step:25431 [D loss: 0.024015, acc.: 100.00%] [G loss: 3.761293]\n",
      "epoch:32 step:25432 [D loss: 0.027714, acc.: 100.00%] [G loss: 3.384361]\n",
      "epoch:32 step:25433 [D loss: 0.051357, acc.: 100.00%] [G loss: 4.293838]\n",
      "epoch:32 step:25434 [D loss: 0.025521, acc.: 100.00%] [G loss: 3.725298]\n",
      "epoch:32 step:25435 [D loss: 0.028957, acc.: 100.00%] [G loss: 2.887934]\n",
      "epoch:32 step:25436 [D loss: 0.087032, acc.: 99.22%] [G loss: 4.045163]\n",
      "epoch:32 step:25437 [D loss: 0.101833, acc.: 97.66%] [G loss: 2.938661]\n",
      "epoch:32 step:25438 [D loss: 0.028660, acc.: 100.00%] [G loss: 2.581413]\n",
      "epoch:32 step:25439 [D loss: 0.032117, acc.: 100.00%] [G loss: 3.325608]\n",
      "epoch:32 step:25440 [D loss: 0.020560, acc.: 100.00%] [G loss: 3.194022]\n",
      "epoch:32 step:25441 [D loss: 0.099884, acc.: 96.88%] [G loss: 5.801289]\n",
      "epoch:32 step:25442 [D loss: 0.064239, acc.: 97.66%] [G loss: 5.338829]\n",
      "epoch:32 step:25443 [D loss: 0.046918, acc.: 99.22%] [G loss: 4.696148]\n",
      "epoch:32 step:25444 [D loss: 0.025539, acc.: 100.00%] [G loss: 4.362908]\n",
      "epoch:32 step:25445 [D loss: 0.050587, acc.: 99.22%] [G loss: 3.551721]\n",
      "epoch:32 step:25446 [D loss: 0.022433, acc.: 100.00%] [G loss: 3.634169]\n",
      "epoch:32 step:25447 [D loss: 0.011588, acc.: 100.00%] [G loss: 3.676588]\n",
      "epoch:32 step:25448 [D loss: 0.008194, acc.: 100.00%] [G loss: 3.929414]\n",
      "epoch:32 step:25449 [D loss: 0.007904, acc.: 100.00%] [G loss: 3.679753]\n",
      "epoch:32 step:25450 [D loss: 0.035340, acc.: 100.00%] [G loss: 4.160422]\n",
      "epoch:32 step:25451 [D loss: 0.390805, acc.: 82.81%] [G loss: 6.385299]\n",
      "epoch:32 step:25452 [D loss: 0.653697, acc.: 68.75%] [G loss: 3.122527]\n",
      "epoch:32 step:25453 [D loss: 0.474113, acc.: 77.34%] [G loss: 6.310195]\n",
      "epoch:32 step:25454 [D loss: 0.014586, acc.: 100.00%] [G loss: 6.875827]\n",
      "epoch:32 step:25455 [D loss: 0.221249, acc.: 89.84%] [G loss: 6.122024]\n",
      "epoch:32 step:25456 [D loss: 0.010935, acc.: 100.00%] [G loss: 5.029093]\n",
      "epoch:32 step:25457 [D loss: 0.003309, acc.: 100.00%] [G loss: 4.758784]\n",
      "epoch:32 step:25458 [D loss: 0.007220, acc.: 100.00%] [G loss: 4.136387]\n",
      "epoch:32 step:25459 [D loss: 0.007591, acc.: 100.00%] [G loss: 3.893700]\n",
      "epoch:32 step:25460 [D loss: 0.022727, acc.: 100.00%] [G loss: 3.404499]\n",
      "epoch:32 step:25461 [D loss: 0.022587, acc.: 100.00%] [G loss: 4.319655]\n",
      "epoch:32 step:25462 [D loss: 0.013025, acc.: 100.00%] [G loss: 4.079773]\n",
      "epoch:32 step:25463 [D loss: 0.008325, acc.: 100.00%] [G loss: 4.416270]\n",
      "epoch:32 step:25464 [D loss: 0.036624, acc.: 100.00%] [G loss: 4.115030]\n",
      "epoch:32 step:25465 [D loss: 0.010157, acc.: 100.00%] [G loss: 4.924282]\n",
      "epoch:32 step:25466 [D loss: 0.109945, acc.: 98.44%] [G loss: 2.390045]\n",
      "epoch:32 step:25467 [D loss: 0.053816, acc.: 99.22%] [G loss: 3.433249]\n",
      "epoch:32 step:25468 [D loss: 0.014095, acc.: 100.00%] [G loss: 3.559811]\n",
      "epoch:32 step:25469 [D loss: 0.013167, acc.: 100.00%] [G loss: 3.736238]\n",
      "epoch:32 step:25470 [D loss: 0.029809, acc.: 100.00%] [G loss: 4.186513]\n",
      "epoch:32 step:25471 [D loss: 0.027197, acc.: 100.00%] [G loss: 3.905488]\n",
      "epoch:32 step:25472 [D loss: 0.072166, acc.: 99.22%] [G loss: 5.174186]\n",
      "epoch:32 step:25473 [D loss: 0.012350, acc.: 100.00%] [G loss: 5.535447]\n",
      "epoch:32 step:25474 [D loss: 0.261426, acc.: 89.84%] [G loss: 2.654300]\n",
      "epoch:32 step:25475 [D loss: 0.109245, acc.: 97.66%] [G loss: 5.383066]\n",
      "epoch:32 step:25476 [D loss: 0.002566, acc.: 100.00%] [G loss: 5.881061]\n",
      "epoch:32 step:25477 [D loss: 0.195582, acc.: 91.41%] [G loss: 2.070970]\n",
      "epoch:32 step:25478 [D loss: 0.121310, acc.: 96.09%] [G loss: 4.493601]\n",
      "epoch:32 step:25479 [D loss: 0.002582, acc.: 100.00%] [G loss: 5.741180]\n",
      "epoch:32 step:25480 [D loss: 0.012990, acc.: 100.00%] [G loss: 4.517949]\n",
      "epoch:32 step:25481 [D loss: 0.017610, acc.: 100.00%] [G loss: 4.986563]\n",
      "epoch:32 step:25482 [D loss: 0.005110, acc.: 100.00%] [G loss: 3.174120]\n",
      "epoch:32 step:25483 [D loss: 0.012273, acc.: 100.00%] [G loss: 4.226141]\n",
      "epoch:32 step:25484 [D loss: 0.034761, acc.: 100.00%] [G loss: 2.663814]\n",
      "epoch:32 step:25485 [D loss: 0.069238, acc.: 100.00%] [G loss: 4.556287]\n",
      "epoch:32 step:25486 [D loss: 0.006262, acc.: 100.00%] [G loss: 5.354683]\n",
      "epoch:32 step:25487 [D loss: 0.050433, acc.: 98.44%] [G loss: 4.940988]\n",
      "epoch:32 step:25488 [D loss: 0.011555, acc.: 100.00%] [G loss: 3.374722]\n",
      "epoch:32 step:25489 [D loss: 0.014181, acc.: 100.00%] [G loss: 3.457624]\n",
      "epoch:32 step:25490 [D loss: 0.024933, acc.: 100.00%] [G loss: 4.731322]\n",
      "epoch:32 step:25491 [D loss: 0.025940, acc.: 100.00%] [G loss: 4.516981]\n",
      "epoch:32 step:25492 [D loss: 0.011586, acc.: 100.00%] [G loss: 5.322167]\n",
      "epoch:32 step:25493 [D loss: 0.032393, acc.: 100.00%] [G loss: 5.284122]\n",
      "epoch:32 step:25494 [D loss: 0.005908, acc.: 100.00%] [G loss: 4.409776]\n",
      "epoch:32 step:25495 [D loss: 0.004531, acc.: 100.00%] [G loss: 4.091357]\n",
      "epoch:32 step:25496 [D loss: 0.007134, acc.: 100.00%] [G loss: 3.819645]\n",
      "epoch:32 step:25497 [D loss: 0.012811, acc.: 100.00%] [G loss: 3.924915]\n",
      "epoch:32 step:25498 [D loss: 0.020504, acc.: 100.00%] [G loss: 3.542662]\n",
      "epoch:32 step:25499 [D loss: 0.008840, acc.: 100.00%] [G loss: 4.271040]\n",
      "epoch:32 step:25500 [D loss: 0.144613, acc.: 96.09%] [G loss: 2.102030]\n",
      "epoch:32 step:25501 [D loss: 0.177204, acc.: 93.75%] [G loss: 5.739535]\n",
      "epoch:32 step:25502 [D loss: 0.008144, acc.: 100.00%] [G loss: 6.687335]\n",
      "epoch:32 step:25503 [D loss: 0.806594, acc.: 60.94%] [G loss: 2.850475]\n",
      "epoch:32 step:25504 [D loss: 0.059708, acc.: 99.22%] [G loss: 4.680084]\n",
      "epoch:32 step:25505 [D loss: 0.013508, acc.: 100.00%] [G loss: 4.671093]\n",
      "epoch:32 step:25506 [D loss: 0.005951, acc.: 100.00%] [G loss: 4.572884]\n",
      "epoch:32 step:25507 [D loss: 0.003076, acc.: 100.00%] [G loss: 4.378776]\n",
      "epoch:32 step:25508 [D loss: 0.010492, acc.: 100.00%] [G loss: 2.361048]\n",
      "epoch:32 step:25509 [D loss: 0.015832, acc.: 100.00%] [G loss: 1.794227]\n",
      "epoch:32 step:25510 [D loss: 0.023343, acc.: 100.00%] [G loss: 0.872500]\n",
      "epoch:32 step:25511 [D loss: 0.157528, acc.: 94.53%] [G loss: 4.364208]\n",
      "epoch:32 step:25512 [D loss: 0.047152, acc.: 97.66%] [G loss: 5.525299]\n",
      "epoch:32 step:25513 [D loss: 0.109331, acc.: 97.66%] [G loss: 0.302386]\n",
      "epoch:32 step:25514 [D loss: 0.940904, acc.: 62.50%] [G loss: 8.860690]\n",
      "epoch:32 step:25515 [D loss: 2.988381, acc.: 50.00%] [G loss: 7.087206]\n",
      "epoch:32 step:25516 [D loss: 2.546360, acc.: 50.00%] [G loss: 4.759420]\n",
      "epoch:32 step:25517 [D loss: 1.720196, acc.: 50.78%] [G loss: 3.107692]\n",
      "epoch:32 step:25518 [D loss: 0.240289, acc.: 89.06%] [G loss: 2.290507]\n",
      "epoch:32 step:25519 [D loss: 0.389606, acc.: 82.81%] [G loss: 2.709898]\n",
      "epoch:32 step:25520 [D loss: 0.186877, acc.: 96.09%] [G loss: 2.671389]\n",
      "epoch:32 step:25521 [D loss: 0.042667, acc.: 100.00%] [G loss: 3.301099]\n",
      "epoch:32 step:25522 [D loss: 0.084901, acc.: 97.66%] [G loss: 3.089901]\n",
      "epoch:32 step:25523 [D loss: 0.061093, acc.: 100.00%] [G loss: 2.772043]\n",
      "epoch:32 step:25524 [D loss: 0.069719, acc.: 99.22%] [G loss: 2.805223]\n",
      "epoch:32 step:25525 [D loss: 0.185741, acc.: 95.31%] [G loss: 2.650920]\n",
      "epoch:32 step:25526 [D loss: 0.041159, acc.: 100.00%] [G loss: 2.902055]\n",
      "epoch:32 step:25527 [D loss: 0.096251, acc.: 98.44%] [G loss: 1.951839]\n",
      "epoch:32 step:25528 [D loss: 0.046667, acc.: 100.00%] [G loss: 1.941602]\n",
      "epoch:32 step:25529 [D loss: 0.083368, acc.: 99.22%] [G loss: 3.487602]\n",
      "epoch:32 step:25530 [D loss: 0.116190, acc.: 96.88%] [G loss: 3.732315]\n",
      "epoch:32 step:25531 [D loss: 0.040415, acc.: 99.22%] [G loss: 3.338121]\n",
      "epoch:32 step:25532 [D loss: 0.064614, acc.: 100.00%] [G loss: 3.052509]\n",
      "epoch:32 step:25533 [D loss: 0.047238, acc.: 99.22%] [G loss: 3.803401]\n",
      "epoch:32 step:25534 [D loss: 0.316903, acc.: 88.28%] [G loss: 2.509923]\n",
      "epoch:32 step:25535 [D loss: 0.049530, acc.: 100.00%] [G loss: 2.097229]\n",
      "epoch:32 step:25536 [D loss: 0.116493, acc.: 98.44%] [G loss: 3.644613]\n",
      "epoch:32 step:25537 [D loss: 0.039835, acc.: 100.00%] [G loss: 3.540631]\n",
      "epoch:32 step:25538 [D loss: 0.104216, acc.: 96.88%] [G loss: 2.891480]\n",
      "epoch:32 step:25539 [D loss: 0.108998, acc.: 98.44%] [G loss: 4.444419]\n",
      "epoch:32 step:25540 [D loss: 0.048405, acc.: 98.44%] [G loss: 3.687187]\n",
      "epoch:32 step:25541 [D loss: 0.030114, acc.: 99.22%] [G loss: 3.940455]\n",
      "epoch:32 step:25542 [D loss: 0.035354, acc.: 100.00%] [G loss: 3.825450]\n",
      "epoch:32 step:25543 [D loss: 0.022242, acc.: 100.00%] [G loss: 3.938698]\n",
      "epoch:32 step:25544 [D loss: 0.019551, acc.: 100.00%] [G loss: 3.985115]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:32 step:25545 [D loss: 0.170920, acc.: 95.31%] [G loss: 4.078752]\n",
      "epoch:32 step:25546 [D loss: 0.030089, acc.: 99.22%] [G loss: 4.091259]\n",
      "epoch:32 step:25547 [D loss: 0.050794, acc.: 100.00%] [G loss: 3.381492]\n",
      "epoch:32 step:25548 [D loss: 0.069202, acc.: 99.22%] [G loss: 3.173132]\n",
      "epoch:32 step:25549 [D loss: 0.116808, acc.: 96.88%] [G loss: 3.882227]\n",
      "epoch:32 step:25550 [D loss: 0.011000, acc.: 100.00%] [G loss: 4.015800]\n",
      "epoch:32 step:25551 [D loss: 0.017363, acc.: 100.00%] [G loss: 4.703586]\n",
      "epoch:32 step:25552 [D loss: 0.371478, acc.: 85.16%] [G loss: 3.759774]\n",
      "epoch:32 step:25553 [D loss: 0.009085, acc.: 100.00%] [G loss: 4.852310]\n",
      "epoch:32 step:25554 [D loss: 0.045152, acc.: 97.66%] [G loss: 4.419445]\n",
      "epoch:32 step:25555 [D loss: 0.017195, acc.: 100.00%] [G loss: 4.840150]\n",
      "epoch:32 step:25556 [D loss: 0.037356, acc.: 100.00%] [G loss: 4.021724]\n",
      "epoch:32 step:25557 [D loss: 0.029877, acc.: 100.00%] [G loss: 3.924173]\n",
      "epoch:32 step:25558 [D loss: 0.042495, acc.: 100.00%] [G loss: 4.459499]\n",
      "epoch:32 step:25559 [D loss: 0.009643, acc.: 100.00%] [G loss: 4.146225]\n",
      "epoch:32 step:25560 [D loss: 0.020149, acc.: 100.00%] [G loss: 4.219222]\n",
      "epoch:32 step:25561 [D loss: 0.110230, acc.: 96.88%] [G loss: 3.146874]\n",
      "epoch:32 step:25562 [D loss: 0.044970, acc.: 99.22%] [G loss: 3.274323]\n",
      "epoch:32 step:25563 [D loss: 0.030008, acc.: 100.00%] [G loss: 3.746985]\n",
      "epoch:32 step:25564 [D loss: 0.026084, acc.: 100.00%] [G loss: 3.625384]\n",
      "epoch:32 step:25565 [D loss: 0.204668, acc.: 92.19%] [G loss: 3.801341]\n",
      "epoch:32 step:25566 [D loss: 0.034602, acc.: 98.44%] [G loss: 4.070626]\n",
      "epoch:32 step:25567 [D loss: 0.149405, acc.: 96.09%] [G loss: 4.050089]\n",
      "epoch:32 step:25568 [D loss: 0.038290, acc.: 100.00%] [G loss: 3.888286]\n",
      "epoch:32 step:25569 [D loss: 0.070481, acc.: 97.66%] [G loss: 4.364112]\n",
      "epoch:32 step:25570 [D loss: 0.011062, acc.: 100.00%] [G loss: 4.726863]\n",
      "epoch:32 step:25571 [D loss: 0.021820, acc.: 100.00%] [G loss: 3.852439]\n",
      "epoch:32 step:25572 [D loss: 0.069469, acc.: 99.22%] [G loss: 2.708661]\n",
      "epoch:32 step:25573 [D loss: 0.078220, acc.: 98.44%] [G loss: 4.374879]\n",
      "epoch:32 step:25574 [D loss: 0.113670, acc.: 97.66%] [G loss: 4.102970]\n",
      "epoch:32 step:25575 [D loss: 0.072160, acc.: 98.44%] [G loss: 4.222877]\n",
      "epoch:32 step:25576 [D loss: 0.014091, acc.: 100.00%] [G loss: 4.240715]\n",
      "epoch:32 step:25577 [D loss: 0.062498, acc.: 98.44%] [G loss: 3.815110]\n",
      "epoch:32 step:25578 [D loss: 0.011803, acc.: 100.00%] [G loss: 4.790017]\n",
      "epoch:32 step:25579 [D loss: 0.142346, acc.: 95.31%] [G loss: 3.766054]\n",
      "epoch:32 step:25580 [D loss: 0.045863, acc.: 100.00%] [G loss: 3.935668]\n",
      "epoch:32 step:25581 [D loss: 0.009494, acc.: 100.00%] [G loss: 4.509549]\n",
      "epoch:32 step:25582 [D loss: 0.033742, acc.: 99.22%] [G loss: 4.324764]\n",
      "epoch:32 step:25583 [D loss: 0.049187, acc.: 99.22%] [G loss: 4.943351]\n",
      "epoch:32 step:25584 [D loss: 0.009633, acc.: 100.00%] [G loss: 4.381773]\n",
      "epoch:32 step:25585 [D loss: 0.044094, acc.: 100.00%] [G loss: 3.464518]\n",
      "epoch:32 step:25586 [D loss: 0.036338, acc.: 100.00%] [G loss: 3.806145]\n",
      "epoch:32 step:25587 [D loss: 0.031203, acc.: 100.00%] [G loss: 4.454031]\n",
      "epoch:32 step:25588 [D loss: 0.829151, acc.: 57.81%] [G loss: 7.924415]\n",
      "epoch:32 step:25589 [D loss: 1.320659, acc.: 58.59%] [G loss: 6.203188]\n",
      "epoch:32 step:25590 [D loss: 0.004642, acc.: 100.00%] [G loss: 4.637105]\n",
      "epoch:32 step:25591 [D loss: 0.103987, acc.: 96.88%] [G loss: 3.034230]\n",
      "epoch:32 step:25592 [D loss: 0.017463, acc.: 100.00%] [G loss: 2.906625]\n",
      "epoch:32 step:25593 [D loss: 0.158377, acc.: 94.53%] [G loss: 5.022416]\n",
      "epoch:32 step:25594 [D loss: 0.004806, acc.: 100.00%] [G loss: 5.941382]\n",
      "epoch:32 step:25595 [D loss: 0.059573, acc.: 97.66%] [G loss: 4.641848]\n",
      "epoch:32 step:25596 [D loss: 0.198905, acc.: 94.53%] [G loss: 3.479974]\n",
      "epoch:32 step:25597 [D loss: 0.067837, acc.: 99.22%] [G loss: 4.453780]\n",
      "epoch:32 step:25598 [D loss: 0.001229, acc.: 100.00%] [G loss: 5.385853]\n",
      "epoch:32 step:25599 [D loss: 0.030958, acc.: 99.22%] [G loss: 4.822727]\n",
      "epoch:32 step:25600 [D loss: 0.008667, acc.: 100.00%] [G loss: 4.482482]\n",
      "##############\n",
      "[0.85381108 0.98982842 1.00578655 0.93245124 0.93673632 1.07229223\n",
      " 2.11378956 2.1267187  0.81550598 0.97950185]\n",
      "##########\n",
      "epoch:32 step:25601 [D loss: 0.008267, acc.: 100.00%] [G loss: 4.090972]\n",
      "epoch:32 step:25602 [D loss: 0.007059, acc.: 100.00%] [G loss: 4.206084]\n",
      "epoch:32 step:25603 [D loss: 0.015933, acc.: 100.00%] [G loss: 4.841866]\n",
      "epoch:32 step:25604 [D loss: 0.005563, acc.: 100.00%] [G loss: 4.598062]\n",
      "epoch:32 step:25605 [D loss: 0.014863, acc.: 100.00%] [G loss: 3.552849]\n",
      "epoch:32 step:25606 [D loss: 0.007178, acc.: 100.00%] [G loss: 4.950115]\n",
      "epoch:32 step:25607 [D loss: 0.022683, acc.: 100.00%] [G loss: 4.568320]\n",
      "epoch:32 step:25608 [D loss: 0.013265, acc.: 100.00%] [G loss: 4.333885]\n",
      "epoch:32 step:25609 [D loss: 0.032335, acc.: 100.00%] [G loss: 4.722082]\n",
      "epoch:32 step:25610 [D loss: 0.020605, acc.: 100.00%] [G loss: 4.303129]\n",
      "epoch:32 step:25611 [D loss: 0.066086, acc.: 98.44%] [G loss: 2.998219]\n",
      "epoch:32 step:25612 [D loss: 0.119268, acc.: 96.88%] [G loss: 5.486330]\n",
      "epoch:32 step:25613 [D loss: 0.004212, acc.: 100.00%] [G loss: 6.019756]\n",
      "epoch:32 step:25614 [D loss: 0.233971, acc.: 89.06%] [G loss: 3.435842]\n",
      "epoch:32 step:25615 [D loss: 0.104313, acc.: 97.66%] [G loss: 4.972397]\n",
      "epoch:32 step:25616 [D loss: 0.001828, acc.: 100.00%] [G loss: 6.152415]\n",
      "epoch:32 step:25617 [D loss: 0.015616, acc.: 100.00%] [G loss: 5.411657]\n",
      "epoch:32 step:25618 [D loss: 0.009300, acc.: 100.00%] [G loss: 5.785844]\n",
      "epoch:32 step:25619 [D loss: 0.026985, acc.: 99.22%] [G loss: 4.942459]\n",
      "epoch:32 step:25620 [D loss: 0.003924, acc.: 100.00%] [G loss: 4.895259]\n",
      "epoch:32 step:25621 [D loss: 0.010964, acc.: 100.00%] [G loss: 4.761624]\n",
      "epoch:32 step:25622 [D loss: 0.010411, acc.: 100.00%] [G loss: 5.227637]\n",
      "epoch:32 step:25623 [D loss: 0.017970, acc.: 100.00%] [G loss: 4.313779]\n",
      "epoch:32 step:25624 [D loss: 0.010800, acc.: 100.00%] [G loss: 4.202613]\n",
      "epoch:32 step:25625 [D loss: 0.006194, acc.: 100.00%] [G loss: 4.713765]\n",
      "epoch:32 step:25626 [D loss: 0.008956, acc.: 100.00%] [G loss: 5.304781]\n",
      "epoch:32 step:25627 [D loss: 0.009255, acc.: 100.00%] [G loss: 4.048711]\n",
      "epoch:32 step:25628 [D loss: 0.022066, acc.: 100.00%] [G loss: 3.735793]\n",
      "epoch:32 step:25629 [D loss: 0.116271, acc.: 96.09%] [G loss: 4.660766]\n",
      "epoch:32 step:25630 [D loss: 0.002205, acc.: 100.00%] [G loss: 5.405754]\n",
      "epoch:32 step:25631 [D loss: 0.014459, acc.: 100.00%] [G loss: 5.009362]\n",
      "epoch:32 step:25632 [D loss: 0.003020, acc.: 100.00%] [G loss: 4.972487]\n",
      "epoch:32 step:25633 [D loss: 0.010400, acc.: 100.00%] [G loss: 4.593506]\n",
      "epoch:32 step:25634 [D loss: 0.052681, acc.: 98.44%] [G loss: 3.334006]\n",
      "epoch:32 step:25635 [D loss: 0.041294, acc.: 100.00%] [G loss: 4.075624]\n",
      "epoch:32 step:25636 [D loss: 0.003592, acc.: 100.00%] [G loss: 4.298282]\n",
      "epoch:32 step:25637 [D loss: 0.021853, acc.: 100.00%] [G loss: 3.421983]\n",
      "epoch:32 step:25638 [D loss: 2.965507, acc.: 34.38%] [G loss: 8.034822]\n",
      "epoch:32 step:25639 [D loss: 2.956880, acc.: 50.00%] [G loss: 6.260374]\n",
      "epoch:32 step:25640 [D loss: 2.582290, acc.: 50.00%] [G loss: 4.440873]\n",
      "epoch:32 step:25641 [D loss: 1.777777, acc.: 50.00%] [G loss: 3.161066]\n",
      "epoch:32 step:25642 [D loss: 1.254375, acc.: 50.00%] [G loss: 2.233491]\n",
      "epoch:32 step:25643 [D loss: 0.777579, acc.: 60.16%] [G loss: 1.668477]\n",
      "epoch:32 step:25644 [D loss: 0.570955, acc.: 69.53%] [G loss: 1.388122]\n",
      "epoch:32 step:25645 [D loss: 0.350125, acc.: 90.62%] [G loss: 1.556675]\n",
      "epoch:32 step:25646 [D loss: 0.335220, acc.: 90.62%] [G loss: 1.443649]\n",
      "epoch:32 step:25647 [D loss: 0.141043, acc.: 100.00%] [G loss: 1.686345]\n",
      "epoch:32 step:25648 [D loss: 0.341877, acc.: 89.06%] [G loss: 1.595799]\n",
      "epoch:32 step:25649 [D loss: 0.149769, acc.: 100.00%] [G loss: 1.808204]\n",
      "epoch:32 step:25650 [D loss: 0.134874, acc.: 99.22%] [G loss: 1.645630]\n",
      "epoch:32 step:25651 [D loss: 0.124596, acc.: 99.22%] [G loss: 2.010381]\n",
      "epoch:32 step:25652 [D loss: 0.136678, acc.: 98.44%] [G loss: 2.172527]\n",
      "epoch:32 step:25653 [D loss: 0.105324, acc.: 98.44%] [G loss: 2.362731]\n",
      "epoch:32 step:25654 [D loss: 0.148448, acc.: 97.66%] [G loss: 2.349856]\n",
      "epoch:32 step:25655 [D loss: 0.083844, acc.: 100.00%] [G loss: 2.169302]\n",
      "epoch:32 step:25656 [D loss: 0.068775, acc.: 100.00%] [G loss: 2.368388]\n",
      "epoch:32 step:25657 [D loss: 0.121852, acc.: 99.22%] [G loss: 1.839746]\n",
      "epoch:32 step:25658 [D loss: 0.128401, acc.: 97.66%] [G loss: 2.498630]\n",
      "epoch:32 step:25659 [D loss: 0.196886, acc.: 92.97%] [G loss: 2.187211]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:32 step:25660 [D loss: 0.202816, acc.: 92.19%] [G loss: 2.941657]\n",
      "epoch:32 step:25661 [D loss: 0.197371, acc.: 93.75%] [G loss: 2.538124]\n",
      "epoch:32 step:25662 [D loss: 0.233476, acc.: 94.53%] [G loss: 3.137868]\n",
      "epoch:32 step:25663 [D loss: 0.269894, acc.: 89.84%] [G loss: 2.779326]\n",
      "epoch:32 step:25664 [D loss: 0.105920, acc.: 98.44%] [G loss: 3.025985]\n",
      "epoch:32 step:25665 [D loss: 0.036743, acc.: 99.22%] [G loss: 3.428014]\n",
      "epoch:32 step:25666 [D loss: 0.306245, acc.: 86.72%] [G loss: 2.852016]\n",
      "epoch:32 step:25667 [D loss: 0.051382, acc.: 99.22%] [G loss: 2.305771]\n",
      "epoch:32 step:25668 [D loss: 0.349660, acc.: 84.38%] [G loss: 3.602827]\n",
      "epoch:32 step:25669 [D loss: 0.147564, acc.: 92.97%] [G loss: 3.967624]\n",
      "epoch:32 step:25670 [D loss: 0.446104, acc.: 77.34%] [G loss: 1.487492]\n",
      "epoch:32 step:25671 [D loss: 0.535814, acc.: 67.97%] [G loss: 4.295003]\n",
      "epoch:32 step:25672 [D loss: 0.408846, acc.: 77.34%] [G loss: 4.368906]\n",
      "epoch:32 step:25673 [D loss: 0.571827, acc.: 72.66%] [G loss: 3.464601]\n",
      "epoch:32 step:25674 [D loss: 0.118019, acc.: 99.22%] [G loss: 3.036768]\n",
      "epoch:32 step:25675 [D loss: 0.043539, acc.: 100.00%] [G loss: 2.639777]\n",
      "epoch:32 step:25676 [D loss: 0.051889, acc.: 99.22%] [G loss: 2.585806]\n",
      "epoch:32 step:25677 [D loss: 0.070871, acc.: 98.44%] [G loss: 2.940150]\n",
      "epoch:32 step:25678 [D loss: 0.044221, acc.: 100.00%] [G loss: 2.899348]\n",
      "epoch:32 step:25679 [D loss: 0.073220, acc.: 100.00%] [G loss: 3.159607]\n",
      "epoch:32 step:25680 [D loss: 0.063922, acc.: 99.22%] [G loss: 3.064496]\n",
      "epoch:32 step:25681 [D loss: 0.158463, acc.: 96.09%] [G loss: 2.521309]\n",
      "epoch:32 step:25682 [D loss: 0.118926, acc.: 98.44%] [G loss: 2.839546]\n",
      "epoch:32 step:25683 [D loss: 0.073998, acc.: 99.22%] [G loss: 3.166046]\n",
      "epoch:32 step:25684 [D loss: 0.081315, acc.: 99.22%] [G loss: 3.564696]\n",
      "epoch:32 step:25685 [D loss: 0.066744, acc.: 99.22%] [G loss: 3.288692]\n",
      "epoch:32 step:25686 [D loss: 0.027976, acc.: 100.00%] [G loss: 3.424868]\n",
      "epoch:32 step:25687 [D loss: 0.059058, acc.: 100.00%] [G loss: 2.765284]\n",
      "epoch:32 step:25688 [D loss: 0.128256, acc.: 99.22%] [G loss: 2.695309]\n",
      "epoch:32 step:25689 [D loss: 0.117845, acc.: 97.66%] [G loss: 2.559391]\n",
      "epoch:32 step:25690 [D loss: 0.188934, acc.: 92.19%] [G loss: 2.859012]\n",
      "epoch:32 step:25691 [D loss: 0.032934, acc.: 100.00%] [G loss: 2.773152]\n",
      "epoch:32 step:25692 [D loss: 0.108472, acc.: 100.00%] [G loss: 3.681460]\n",
      "epoch:32 step:25693 [D loss: 0.402069, acc.: 85.94%] [G loss: 2.148730]\n",
      "epoch:32 step:25694 [D loss: 0.106917, acc.: 97.66%] [G loss: 3.154047]\n",
      "epoch:32 step:25695 [D loss: 0.137753, acc.: 95.31%] [G loss: 2.166708]\n",
      "epoch:32 step:25696 [D loss: 0.076954, acc.: 99.22%] [G loss: 3.366520]\n",
      "epoch:32 step:25697 [D loss: 0.062458, acc.: 99.22%] [G loss: 3.193676]\n",
      "epoch:32 step:25698 [D loss: 0.260409, acc.: 91.41%] [G loss: 2.710333]\n",
      "epoch:32 step:25699 [D loss: 0.087349, acc.: 96.88%] [G loss: 3.024907]\n",
      "epoch:32 step:25700 [D loss: 0.056693, acc.: 100.00%] [G loss: 3.714732]\n",
      "epoch:32 step:25701 [D loss: 0.035867, acc.: 99.22%] [G loss: 3.903507]\n",
      "epoch:32 step:25702 [D loss: 0.181883, acc.: 92.97%] [G loss: 2.184926]\n",
      "epoch:32 step:25703 [D loss: 0.223305, acc.: 89.84%] [G loss: 4.514126]\n",
      "epoch:32 step:25704 [D loss: 0.277229, acc.: 86.72%] [G loss: 3.429596]\n",
      "epoch:32 step:25705 [D loss: 0.053413, acc.: 99.22%] [G loss: 3.480261]\n",
      "epoch:32 step:25706 [D loss: 0.097295, acc.: 98.44%] [G loss: 3.471303]\n",
      "epoch:32 step:25707 [D loss: 0.116996, acc.: 98.44%] [G loss: 3.258818]\n",
      "epoch:32 step:25708 [D loss: 0.034416, acc.: 100.00%] [G loss: 3.921993]\n",
      "epoch:32 step:25709 [D loss: 0.026523, acc.: 100.00%] [G loss: 4.206649]\n",
      "epoch:32 step:25710 [D loss: 0.026753, acc.: 100.00%] [G loss: 3.923561]\n",
      "epoch:32 step:25711 [D loss: 0.063400, acc.: 100.00%] [G loss: 3.762853]\n",
      "epoch:32 step:25712 [D loss: 0.145594, acc.: 96.88%] [G loss: 2.984790]\n",
      "epoch:32 step:25713 [D loss: 0.092069, acc.: 97.66%] [G loss: 4.067865]\n",
      "epoch:32 step:25714 [D loss: 0.104676, acc.: 97.66%] [G loss: 3.383287]\n",
      "epoch:32 step:25715 [D loss: 0.073753, acc.: 98.44%] [G loss: 3.230016]\n",
      "epoch:32 step:25716 [D loss: 0.084372, acc.: 97.66%] [G loss: 4.433456]\n",
      "epoch:32 step:25717 [D loss: 0.050091, acc.: 99.22%] [G loss: 4.161358]\n",
      "epoch:32 step:25718 [D loss: 0.024969, acc.: 99.22%] [G loss: 3.611911]\n",
      "epoch:32 step:25719 [D loss: 0.100930, acc.: 98.44%] [G loss: 3.753120]\n",
      "epoch:32 step:25720 [D loss: 0.020378, acc.: 100.00%] [G loss: 4.174363]\n",
      "epoch:32 step:25721 [D loss: 0.049359, acc.: 100.00%] [G loss: 3.874447]\n",
      "epoch:32 step:25722 [D loss: 0.039288, acc.: 99.22%] [G loss: 3.764730]\n",
      "epoch:32 step:25723 [D loss: 0.091692, acc.: 97.66%] [G loss: 3.759546]\n",
      "epoch:32 step:25724 [D loss: 0.090736, acc.: 98.44%] [G loss: 4.122529]\n",
      "epoch:32 step:25725 [D loss: 0.013747, acc.: 100.00%] [G loss: 4.396372]\n",
      "epoch:32 step:25726 [D loss: 0.142595, acc.: 97.66%] [G loss: 5.083668]\n",
      "epoch:32 step:25727 [D loss: 0.033439, acc.: 97.66%] [G loss: 5.018539]\n",
      "epoch:32 step:25728 [D loss: 0.016733, acc.: 100.00%] [G loss: 4.603736]\n",
      "epoch:32 step:25729 [D loss: 0.097735, acc.: 96.88%] [G loss: 4.041061]\n",
      "epoch:32 step:25730 [D loss: 0.023967, acc.: 100.00%] [G loss: 4.380271]\n",
      "epoch:32 step:25731 [D loss: 0.018840, acc.: 100.00%] [G loss: 3.700163]\n",
      "epoch:32 step:25732 [D loss: 0.299410, acc.: 89.06%] [G loss: 5.624892]\n",
      "epoch:32 step:25733 [D loss: 0.026146, acc.: 99.22%] [G loss: 6.461223]\n",
      "epoch:32 step:25734 [D loss: 0.043012, acc.: 99.22%] [G loss: 5.592731]\n",
      "epoch:32 step:25735 [D loss: 0.422587, acc.: 81.25%] [G loss: 3.680022]\n",
      "epoch:32 step:25736 [D loss: 0.044368, acc.: 98.44%] [G loss: 4.817685]\n",
      "epoch:32 step:25737 [D loss: 0.010406, acc.: 100.00%] [G loss: 4.920747]\n",
      "epoch:32 step:25738 [D loss: 0.105796, acc.: 96.88%] [G loss: 5.797622]\n",
      "epoch:32 step:25739 [D loss: 0.007998, acc.: 100.00%] [G loss: 5.305245]\n",
      "epoch:32 step:25740 [D loss: 0.050186, acc.: 97.66%] [G loss: 4.875579]\n",
      "epoch:32 step:25741 [D loss: 0.038588, acc.: 99.22%] [G loss: 4.534717]\n",
      "epoch:32 step:25742 [D loss: 0.039675, acc.: 100.00%] [G loss: 5.273192]\n",
      "epoch:32 step:25743 [D loss: 0.009732, acc.: 100.00%] [G loss: 4.717298]\n",
      "epoch:32 step:25744 [D loss: 0.012037, acc.: 100.00%] [G loss: 4.829056]\n",
      "epoch:32 step:25745 [D loss: 0.021367, acc.: 99.22%] [G loss: 5.049347]\n",
      "epoch:32 step:25746 [D loss: 0.189708, acc.: 92.19%] [G loss: 5.261765]\n",
      "epoch:32 step:25747 [D loss: 0.017873, acc.: 99.22%] [G loss: 5.923098]\n",
      "epoch:32 step:25748 [D loss: 0.062140, acc.: 97.66%] [G loss: 5.341295]\n",
      "epoch:32 step:25749 [D loss: 0.062337, acc.: 98.44%] [G loss: 5.105490]\n",
      "epoch:32 step:25750 [D loss: 0.001623, acc.: 100.00%] [G loss: 6.127673]\n",
      "epoch:32 step:25751 [D loss: 0.009765, acc.: 100.00%] [G loss: 5.264991]\n",
      "epoch:32 step:25752 [D loss: 0.047924, acc.: 99.22%] [G loss: 3.726794]\n",
      "epoch:32 step:25753 [D loss: 0.011465, acc.: 100.00%] [G loss: 4.554478]\n",
      "epoch:32 step:25754 [D loss: 0.075192, acc.: 99.22%] [G loss: 5.763220]\n",
      "epoch:32 step:25755 [D loss: 0.058324, acc.: 96.88%] [G loss: 5.059805]\n",
      "epoch:32 step:25756 [D loss: 0.011144, acc.: 100.00%] [G loss: 4.469591]\n",
      "epoch:32 step:25757 [D loss: 0.010165, acc.: 100.00%] [G loss: 4.100666]\n",
      "epoch:32 step:25758 [D loss: 0.968789, acc.: 60.16%] [G loss: 10.193867]\n",
      "epoch:32 step:25759 [D loss: 3.835641, acc.: 50.00%] [G loss: 5.471575]\n",
      "epoch:32 step:25760 [D loss: 1.761311, acc.: 53.12%] [G loss: 2.541267]\n",
      "epoch:32 step:25761 [D loss: 0.409819, acc.: 78.12%] [G loss: 2.658937]\n",
      "epoch:32 step:25762 [D loss: 0.098357, acc.: 99.22%] [G loss: 3.501751]\n",
      "epoch:32 step:25763 [D loss: 0.210776, acc.: 91.41%] [G loss: 3.249104]\n",
      "epoch:32 step:25764 [D loss: 0.048880, acc.: 100.00%] [G loss: 3.462884]\n",
      "epoch:32 step:25765 [D loss: 0.042840, acc.: 99.22%] [G loss: 3.336385]\n",
      "epoch:32 step:25766 [D loss: 0.016656, acc.: 100.00%] [G loss: 3.312165]\n",
      "epoch:32 step:25767 [D loss: 0.035386, acc.: 100.00%] [G loss: 2.882084]\n",
      "epoch:32 step:25768 [D loss: 0.032501, acc.: 100.00%] [G loss: 2.740173]\n",
      "epoch:32 step:25769 [D loss: 0.141763, acc.: 96.88%] [G loss: 3.226508]\n",
      "epoch:32 step:25770 [D loss: 0.033773, acc.: 100.00%] [G loss: 3.273429]\n",
      "epoch:32 step:25771 [D loss: 0.042140, acc.: 100.00%] [G loss: 3.476220]\n",
      "epoch:32 step:25772 [D loss: 0.043844, acc.: 100.00%] [G loss: 3.406264]\n",
      "epoch:32 step:25773 [D loss: 0.028002, acc.: 100.00%] [G loss: 3.471702]\n",
      "epoch:33 step:25774 [D loss: 0.025504, acc.: 100.00%] [G loss: 3.361073]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:33 step:25775 [D loss: 0.078605, acc.: 99.22%] [G loss: 3.426770]\n",
      "epoch:33 step:25776 [D loss: 0.099143, acc.: 97.66%] [G loss: 3.414953]\n",
      "epoch:33 step:25777 [D loss: 0.022079, acc.: 100.00%] [G loss: 3.700258]\n",
      "epoch:33 step:25778 [D loss: 0.017220, acc.: 100.00%] [G loss: 3.660009]\n",
      "epoch:33 step:25779 [D loss: 0.013012, acc.: 100.00%] [G loss: 3.524387]\n",
      "epoch:33 step:25780 [D loss: 0.020357, acc.: 100.00%] [G loss: 3.769926]\n",
      "epoch:33 step:25781 [D loss: 0.021901, acc.: 100.00%] [G loss: 3.075690]\n",
      "epoch:33 step:25782 [D loss: 0.030684, acc.: 100.00%] [G loss: 2.988089]\n",
      "epoch:33 step:25783 [D loss: 0.068969, acc.: 99.22%] [G loss: 3.043307]\n",
      "epoch:33 step:25784 [D loss: 0.033921, acc.: 100.00%] [G loss: 3.495442]\n",
      "epoch:33 step:25785 [D loss: 0.077240, acc.: 98.44%] [G loss: 2.838794]\n",
      "epoch:33 step:25786 [D loss: 0.034975, acc.: 100.00%] [G loss: 3.222858]\n",
      "epoch:33 step:25787 [D loss: 0.023308, acc.: 100.00%] [G loss: 3.263604]\n",
      "epoch:33 step:25788 [D loss: 0.071021, acc.: 99.22%] [G loss: 3.462069]\n",
      "epoch:33 step:25789 [D loss: 0.016219, acc.: 100.00%] [G loss: 3.888987]\n",
      "epoch:33 step:25790 [D loss: 0.100494, acc.: 98.44%] [G loss: 2.456051]\n",
      "epoch:33 step:25791 [D loss: 0.015845, acc.: 100.00%] [G loss: 2.137760]\n",
      "epoch:33 step:25792 [D loss: 0.049971, acc.: 100.00%] [G loss: 3.212950]\n",
      "epoch:33 step:25793 [D loss: 0.006921, acc.: 100.00%] [G loss: 3.545153]\n",
      "epoch:33 step:25794 [D loss: 0.012668, acc.: 100.00%] [G loss: 3.503252]\n",
      "epoch:33 step:25795 [D loss: 0.038796, acc.: 100.00%] [G loss: 3.812899]\n",
      "epoch:33 step:25796 [D loss: 0.228519, acc.: 92.97%] [G loss: 2.156065]\n",
      "epoch:33 step:25797 [D loss: 0.133644, acc.: 95.31%] [G loss: 5.341442]\n",
      "epoch:33 step:25798 [D loss: 0.208123, acc.: 91.41%] [G loss: 4.562545]\n",
      "epoch:33 step:25799 [D loss: 0.006745, acc.: 100.00%] [G loss: 3.692344]\n",
      "epoch:33 step:25800 [D loss: 0.027557, acc.: 100.00%] [G loss: 3.877783]\n",
      "##############\n",
      "[1.11619191 1.05453397 2.11282349 0.95854001 2.10985466 0.9444107\n",
      " 2.12400267 2.11194572 2.10838909 0.94841465]\n",
      "##########\n",
      "epoch:33 step:25801 [D loss: 0.024416, acc.: 100.00%] [G loss: 4.093643]\n",
      "epoch:33 step:25802 [D loss: 0.010993, acc.: 100.00%] [G loss: 4.689248]\n",
      "epoch:33 step:25803 [D loss: 0.018933, acc.: 100.00%] [G loss: 4.076202]\n",
      "epoch:33 step:25804 [D loss: 0.008688, acc.: 100.00%] [G loss: 4.198317]\n",
      "epoch:33 step:25805 [D loss: 0.013684, acc.: 100.00%] [G loss: 3.812682]\n",
      "epoch:33 step:25806 [D loss: 0.053461, acc.: 99.22%] [G loss: 3.756787]\n",
      "epoch:33 step:25807 [D loss: 0.015222, acc.: 100.00%] [G loss: 3.859257]\n",
      "epoch:33 step:25808 [D loss: 0.016888, acc.: 100.00%] [G loss: 3.943031]\n",
      "epoch:33 step:25809 [D loss: 0.083485, acc.: 98.44%] [G loss: 4.072964]\n",
      "epoch:33 step:25810 [D loss: 0.010308, acc.: 100.00%] [G loss: 3.596359]\n",
      "epoch:33 step:25811 [D loss: 0.019332, acc.: 100.00%] [G loss: 4.150400]\n",
      "epoch:33 step:25812 [D loss: 0.014999, acc.: 100.00%] [G loss: 3.803555]\n",
      "epoch:33 step:25813 [D loss: 0.017242, acc.: 100.00%] [G loss: 4.588468]\n",
      "epoch:33 step:25814 [D loss: 0.019572, acc.: 100.00%] [G loss: 4.213414]\n",
      "epoch:33 step:25815 [D loss: 0.014772, acc.: 100.00%] [G loss: 4.171022]\n",
      "epoch:33 step:25816 [D loss: 0.007797, acc.: 100.00%] [G loss: 4.343063]\n",
      "epoch:33 step:25817 [D loss: 0.799304, acc.: 59.38%] [G loss: 6.224290]\n",
      "epoch:33 step:25818 [D loss: 1.497711, acc.: 53.91%] [G loss: 2.610969]\n",
      "epoch:33 step:25819 [D loss: 0.290027, acc.: 88.28%] [G loss: 4.681283]\n",
      "epoch:33 step:25820 [D loss: 0.012812, acc.: 100.00%] [G loss: 5.096686]\n",
      "epoch:33 step:25821 [D loss: 0.299461, acc.: 81.25%] [G loss: 2.872458]\n",
      "epoch:33 step:25822 [D loss: 0.106073, acc.: 96.88%] [G loss: 3.559509]\n",
      "epoch:33 step:25823 [D loss: 0.019461, acc.: 100.00%] [G loss: 3.195696]\n",
      "epoch:33 step:25824 [D loss: 0.015231, acc.: 100.00%] [G loss: 3.291197]\n",
      "epoch:33 step:25825 [D loss: 0.021860, acc.: 100.00%] [G loss: 3.154570]\n",
      "epoch:33 step:25826 [D loss: 0.011058, acc.: 100.00%] [G loss: 2.846398]\n",
      "epoch:33 step:25827 [D loss: 0.036090, acc.: 100.00%] [G loss: 2.441833]\n",
      "epoch:33 step:25828 [D loss: 0.028759, acc.: 100.00%] [G loss: 2.269293]\n",
      "epoch:33 step:25829 [D loss: 0.084256, acc.: 99.22%] [G loss: 4.335692]\n",
      "epoch:33 step:25830 [D loss: 0.320996, acc.: 82.03%] [G loss: 0.855804]\n",
      "epoch:33 step:25831 [D loss: 0.225450, acc.: 90.62%] [G loss: 5.846220]\n",
      "epoch:33 step:25832 [D loss: 0.141283, acc.: 92.19%] [G loss: 5.431620]\n",
      "epoch:33 step:25833 [D loss: 0.006123, acc.: 100.00%] [G loss: 5.302159]\n",
      "epoch:33 step:25834 [D loss: 0.036485, acc.: 100.00%] [G loss: 4.880353]\n",
      "epoch:33 step:25835 [D loss: 0.008884, acc.: 100.00%] [G loss: 4.226171]\n",
      "epoch:33 step:25836 [D loss: 0.011915, acc.: 100.00%] [G loss: 4.404499]\n",
      "epoch:33 step:25837 [D loss: 0.013147, acc.: 100.00%] [G loss: 4.538170]\n",
      "epoch:33 step:25838 [D loss: 0.047707, acc.: 98.44%] [G loss: 3.701886]\n",
      "epoch:33 step:25839 [D loss: 0.030475, acc.: 100.00%] [G loss: 3.889852]\n",
      "epoch:33 step:25840 [D loss: 0.008956, acc.: 100.00%] [G loss: 4.088037]\n",
      "epoch:33 step:25841 [D loss: 0.020585, acc.: 100.00%] [G loss: 4.218452]\n",
      "epoch:33 step:25842 [D loss: 0.009672, acc.: 100.00%] [G loss: 4.448201]\n",
      "epoch:33 step:25843 [D loss: 0.021551, acc.: 100.00%] [G loss: 4.898221]\n",
      "epoch:33 step:25844 [D loss: 0.005774, acc.: 100.00%] [G loss: 4.873883]\n",
      "epoch:33 step:25845 [D loss: 0.006315, acc.: 100.00%] [G loss: 4.653589]\n",
      "epoch:33 step:25846 [D loss: 0.017105, acc.: 100.00%] [G loss: 4.381504]\n",
      "epoch:33 step:25847 [D loss: 0.026825, acc.: 100.00%] [G loss: 4.809752]\n",
      "epoch:33 step:25848 [D loss: 0.017257, acc.: 100.00%] [G loss: 4.991645]\n",
      "epoch:33 step:25849 [D loss: 0.010464, acc.: 100.00%] [G loss: 4.214503]\n",
      "epoch:33 step:25850 [D loss: 0.183982, acc.: 93.75%] [G loss: 4.435443]\n",
      "epoch:33 step:25851 [D loss: 0.008001, acc.: 100.00%] [G loss: 5.209227]\n",
      "epoch:33 step:25852 [D loss: 0.034813, acc.: 99.22%] [G loss: 4.189871]\n",
      "epoch:33 step:25853 [D loss: 0.019848, acc.: 100.00%] [G loss: 4.352237]\n",
      "epoch:33 step:25854 [D loss: 0.010538, acc.: 100.00%] [G loss: 4.239382]\n",
      "epoch:33 step:25855 [D loss: 0.010730, acc.: 100.00%] [G loss: 5.021973]\n",
      "epoch:33 step:25856 [D loss: 0.007272, acc.: 100.00%] [G loss: 4.922168]\n",
      "epoch:33 step:25857 [D loss: 0.014065, acc.: 100.00%] [G loss: 4.474902]\n",
      "epoch:33 step:25858 [D loss: 0.024913, acc.: 99.22%] [G loss: 4.517741]\n",
      "epoch:33 step:25859 [D loss: 0.010012, acc.: 100.00%] [G loss: 4.784828]\n",
      "epoch:33 step:25860 [D loss: 0.004478, acc.: 100.00%] [G loss: 4.166265]\n",
      "epoch:33 step:25861 [D loss: 0.046081, acc.: 99.22%] [G loss: 3.793248]\n",
      "epoch:33 step:25862 [D loss: 0.013692, acc.: 100.00%] [G loss: 3.748472]\n",
      "epoch:33 step:25863 [D loss: 0.417863, acc.: 79.69%] [G loss: 7.282201]\n",
      "epoch:33 step:25864 [D loss: 1.031194, acc.: 57.81%] [G loss: 3.680809]\n",
      "epoch:33 step:25865 [D loss: 0.287017, acc.: 83.59%] [G loss: 6.130911]\n",
      "epoch:33 step:25866 [D loss: 0.009848, acc.: 100.00%] [G loss: 7.014234]\n",
      "epoch:33 step:25867 [D loss: 0.372891, acc.: 82.03%] [G loss: 3.371418]\n",
      "epoch:33 step:25868 [D loss: 0.153144, acc.: 93.75%] [G loss: 3.613164]\n",
      "epoch:33 step:25869 [D loss: 0.006587, acc.: 100.00%] [G loss: 4.281126]\n",
      "epoch:33 step:25870 [D loss: 0.014957, acc.: 100.00%] [G loss: 4.201710]\n",
      "epoch:33 step:25871 [D loss: 0.005554, acc.: 100.00%] [G loss: 3.885589]\n",
      "epoch:33 step:25872 [D loss: 0.058186, acc.: 98.44%] [G loss: 3.767641]\n",
      "epoch:33 step:25873 [D loss: 0.021674, acc.: 100.00%] [G loss: 4.256484]\n",
      "epoch:33 step:25874 [D loss: 0.044784, acc.: 98.44%] [G loss: 3.285304]\n",
      "epoch:33 step:25875 [D loss: 0.030033, acc.: 100.00%] [G loss: 3.486408]\n",
      "epoch:33 step:25876 [D loss: 0.027042, acc.: 99.22%] [G loss: 2.505383]\n",
      "epoch:33 step:25877 [D loss: 0.065944, acc.: 99.22%] [G loss: 4.163271]\n",
      "epoch:33 step:25878 [D loss: 0.013905, acc.: 100.00%] [G loss: 4.965160]\n",
      "epoch:33 step:25879 [D loss: 0.019370, acc.: 100.00%] [G loss: 4.505840]\n",
      "epoch:33 step:25880 [D loss: 0.030502, acc.: 99.22%] [G loss: 3.173722]\n",
      "epoch:33 step:25881 [D loss: 0.021929, acc.: 100.00%] [G loss: 3.814610]\n",
      "epoch:33 step:25882 [D loss: 0.019202, acc.: 100.00%] [G loss: 4.330648]\n",
      "epoch:33 step:25883 [D loss: 0.016379, acc.: 100.00%] [G loss: 4.332234]\n",
      "epoch:33 step:25884 [D loss: 0.016897, acc.: 100.00%] [G loss: 3.935705]\n",
      "epoch:33 step:25885 [D loss: 0.006214, acc.: 100.00%] [G loss: 3.430144]\n",
      "epoch:33 step:25886 [D loss: 0.030454, acc.: 100.00%] [G loss: 4.996335]\n",
      "epoch:33 step:25887 [D loss: 0.012468, acc.: 100.00%] [G loss: 4.780133]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:33 step:25888 [D loss: 0.010599, acc.: 100.00%] [G loss: 4.904768]\n",
      "epoch:33 step:25889 [D loss: 0.154654, acc.: 93.75%] [G loss: 4.139396]\n",
      "epoch:33 step:25890 [D loss: 0.026534, acc.: 100.00%] [G loss: 4.616419]\n",
      "epoch:33 step:25891 [D loss: 0.007323, acc.: 100.00%] [G loss: 4.962711]\n",
      "epoch:33 step:25892 [D loss: 0.004295, acc.: 100.00%] [G loss: 5.209288]\n",
      "epoch:33 step:25893 [D loss: 0.004557, acc.: 100.00%] [G loss: 5.058286]\n",
      "epoch:33 step:25894 [D loss: 0.005578, acc.: 100.00%] [G loss: 4.130836]\n",
      "epoch:33 step:25895 [D loss: 0.007301, acc.: 100.00%] [G loss: 4.323443]\n",
      "epoch:33 step:25896 [D loss: 0.008556, acc.: 100.00%] [G loss: 3.952737]\n",
      "epoch:33 step:25897 [D loss: 0.017287, acc.: 100.00%] [G loss: 4.177477]\n",
      "epoch:33 step:25898 [D loss: 0.004142, acc.: 100.00%] [G loss: 4.074611]\n",
      "epoch:33 step:25899 [D loss: 0.013440, acc.: 100.00%] [G loss: 4.237305]\n",
      "epoch:33 step:25900 [D loss: 0.018754, acc.: 100.00%] [G loss: 3.701284]\n",
      "epoch:33 step:25901 [D loss: 0.005384, acc.: 100.00%] [G loss: 3.761895]\n",
      "epoch:33 step:25902 [D loss: 0.004231, acc.: 100.00%] [G loss: 3.381011]\n",
      "epoch:33 step:25903 [D loss: 0.007368, acc.: 100.00%] [G loss: 2.987986]\n",
      "epoch:33 step:25904 [D loss: 0.011636, acc.: 100.00%] [G loss: 2.731757]\n",
      "epoch:33 step:25905 [D loss: 0.029543, acc.: 99.22%] [G loss: 2.631918]\n",
      "epoch:33 step:25906 [D loss: 0.005666, acc.: 100.00%] [G loss: 1.765545]\n",
      "epoch:33 step:25907 [D loss: 0.006360, acc.: 100.00%] [G loss: 1.028098]\n",
      "epoch:33 step:25908 [D loss: 0.006827, acc.: 100.00%] [G loss: 0.418802]\n",
      "epoch:33 step:25909 [D loss: 0.168483, acc.: 93.75%] [G loss: 6.469163]\n",
      "epoch:33 step:25910 [D loss: 0.245788, acc.: 89.06%] [G loss: 1.541004]\n",
      "epoch:33 step:25911 [D loss: 0.032992, acc.: 99.22%] [G loss: 0.977481]\n",
      "epoch:33 step:25912 [D loss: 0.017568, acc.: 100.00%] [G loss: 0.541388]\n",
      "epoch:33 step:25913 [D loss: 0.001283, acc.: 100.00%] [G loss: 0.510884]\n",
      "epoch:33 step:25914 [D loss: 0.035020, acc.: 100.00%] [G loss: 1.589181]\n",
      "epoch:33 step:25915 [D loss: 0.001133, acc.: 100.00%] [G loss: 2.071782]\n",
      "epoch:33 step:25916 [D loss: 0.001662, acc.: 100.00%] [G loss: 2.032676]\n",
      "epoch:33 step:25917 [D loss: 0.001467, acc.: 100.00%] [G loss: 0.685914]\n",
      "epoch:33 step:25918 [D loss: 0.019338, acc.: 100.00%] [G loss: 1.010744]\n",
      "epoch:33 step:25919 [D loss: 0.025607, acc.: 100.00%] [G loss: 1.906154]\n",
      "epoch:33 step:25920 [D loss: 0.002997, acc.: 100.00%] [G loss: 1.442956]\n",
      "epoch:33 step:25921 [D loss: 0.001094, acc.: 100.00%] [G loss: 2.238631]\n",
      "epoch:33 step:25922 [D loss: 0.053386, acc.: 99.22%] [G loss: 3.223205]\n",
      "epoch:33 step:25923 [D loss: 0.021233, acc.: 100.00%] [G loss: 3.861302]\n",
      "epoch:33 step:25924 [D loss: 0.069845, acc.: 99.22%] [G loss: 4.125382]\n",
      "epoch:33 step:25925 [D loss: 0.114018, acc.: 97.66%] [G loss: 3.773259]\n",
      "epoch:33 step:25926 [D loss: 0.058444, acc.: 100.00%] [G loss: 8.381453]\n",
      "epoch:33 step:25927 [D loss: 0.011925, acc.: 100.00%] [G loss: 9.019083]\n",
      "epoch:33 step:25928 [D loss: 0.049392, acc.: 98.44%] [G loss: 5.786129]\n",
      "epoch:33 step:25929 [D loss: 0.014081, acc.: 100.00%] [G loss: 4.558717]\n",
      "epoch:33 step:25930 [D loss: 0.011940, acc.: 100.00%] [G loss: 5.685328]\n",
      "epoch:33 step:25931 [D loss: 0.004564, acc.: 100.00%] [G loss: 4.748278]\n",
      "epoch:33 step:25932 [D loss: 0.006010, acc.: 100.00%] [G loss: 4.987866]\n",
      "epoch:33 step:25933 [D loss: 0.016618, acc.: 100.00%] [G loss: 6.595367]\n",
      "epoch:33 step:25934 [D loss: 0.136031, acc.: 96.88%] [G loss: 4.628288]\n",
      "epoch:33 step:25935 [D loss: 0.040456, acc.: 98.44%] [G loss: 6.864557]\n",
      "epoch:33 step:25936 [D loss: 0.012967, acc.: 100.00%] [G loss: 7.721082]\n",
      "epoch:33 step:25937 [D loss: 0.033012, acc.: 99.22%] [G loss: 6.968793]\n",
      "epoch:33 step:25938 [D loss: 0.023118, acc.: 100.00%] [G loss: 3.341740]\n",
      "epoch:33 step:25939 [D loss: 0.014594, acc.: 100.00%] [G loss: 4.006319]\n",
      "epoch:33 step:25940 [D loss: 0.003972, acc.: 100.00%] [G loss: 2.917108]\n",
      "epoch:33 step:25941 [D loss: 0.000662, acc.: 100.00%] [G loss: 4.531100]\n",
      "epoch:33 step:25942 [D loss: 0.011947, acc.: 100.00%] [G loss: 2.218830]\n",
      "epoch:33 step:25943 [D loss: 0.005107, acc.: 100.00%] [G loss: 1.385386]\n",
      "epoch:33 step:25944 [D loss: 0.011470, acc.: 100.00%] [G loss: 3.975173]\n",
      "epoch:33 step:25945 [D loss: 0.001758, acc.: 100.00%] [G loss: 3.446335]\n",
      "epoch:33 step:25946 [D loss: 0.001628, acc.: 100.00%] [G loss: 1.864466]\n",
      "epoch:33 step:25947 [D loss: 0.045353, acc.: 100.00%] [G loss: 0.928612]\n",
      "epoch:33 step:25948 [D loss: 0.025576, acc.: 100.00%] [G loss: 4.151267]\n",
      "epoch:33 step:25949 [D loss: 0.059728, acc.: 98.44%] [G loss: 1.377790]\n",
      "epoch:33 step:25950 [D loss: 0.023476, acc.: 100.00%] [G loss: 1.481674]\n",
      "epoch:33 step:25951 [D loss: 0.093767, acc.: 96.88%] [G loss: 0.192201]\n",
      "epoch:33 step:25952 [D loss: 0.853251, acc.: 63.28%] [G loss: 11.589731]\n",
      "epoch:33 step:25953 [D loss: 3.161979, acc.: 50.00%] [G loss: 6.321703]\n",
      "epoch:33 step:25954 [D loss: 0.085843, acc.: 96.09%] [G loss: 5.100541]\n",
      "epoch:33 step:25955 [D loss: 0.994655, acc.: 63.28%] [G loss: 6.486326]\n",
      "epoch:33 step:25956 [D loss: 1.036731, acc.: 59.38%] [G loss: 4.632710]\n",
      "epoch:33 step:25957 [D loss: 0.148373, acc.: 92.97%] [G loss: 4.335712]\n",
      "epoch:33 step:25958 [D loss: 0.023505, acc.: 100.00%] [G loss: 3.364802]\n",
      "epoch:33 step:25959 [D loss: 0.034800, acc.: 100.00%] [G loss: 2.603270]\n",
      "epoch:33 step:25960 [D loss: 0.207322, acc.: 89.84%] [G loss: 2.584869]\n",
      "epoch:33 step:25961 [D loss: 0.089887, acc.: 98.44%] [G loss: 3.801917]\n",
      "epoch:33 step:25962 [D loss: 0.071933, acc.: 100.00%] [G loss: 3.982663]\n",
      "epoch:33 step:25963 [D loss: 0.458930, acc.: 75.78%] [G loss: 4.754817]\n",
      "epoch:33 step:25964 [D loss: 0.628817, acc.: 64.06%] [G loss: 2.233387]\n",
      "epoch:33 step:25965 [D loss: 0.327649, acc.: 88.28%] [G loss: 4.711084]\n",
      "epoch:33 step:25966 [D loss: 0.197785, acc.: 91.41%] [G loss: 4.510968]\n",
      "epoch:33 step:25967 [D loss: 0.100038, acc.: 96.88%] [G loss: 3.166587]\n",
      "epoch:33 step:25968 [D loss: 0.042306, acc.: 99.22%] [G loss: 3.003505]\n",
      "epoch:33 step:25969 [D loss: 0.072267, acc.: 99.22%] [G loss: 3.278930]\n",
      "epoch:33 step:25970 [D loss: 0.089363, acc.: 98.44%] [G loss: 4.076261]\n",
      "epoch:33 step:25971 [D loss: 0.034048, acc.: 100.00%] [G loss: 3.343804]\n",
      "epoch:33 step:25972 [D loss: 0.124173, acc.: 97.66%] [G loss: 3.786540]\n",
      "epoch:33 step:25973 [D loss: 0.521075, acc.: 75.00%] [G loss: 3.902333]\n",
      "epoch:33 step:25974 [D loss: 0.010898, acc.: 100.00%] [G loss: 4.864776]\n",
      "epoch:33 step:25975 [D loss: 0.184172, acc.: 91.41%] [G loss: 3.222032]\n",
      "epoch:33 step:25976 [D loss: 0.110451, acc.: 97.66%] [G loss: 3.484816]\n",
      "epoch:33 step:25977 [D loss: 0.043742, acc.: 99.22%] [G loss: 3.311890]\n",
      "epoch:33 step:25978 [D loss: 0.060015, acc.: 98.44%] [G loss: 3.560185]\n",
      "epoch:33 step:25979 [D loss: 0.087093, acc.: 97.66%] [G loss: 4.064610]\n",
      "epoch:33 step:25980 [D loss: 0.031372, acc.: 99.22%] [G loss: 4.694707]\n",
      "epoch:33 step:25981 [D loss: 0.137017, acc.: 95.31%] [G loss: 3.107319]\n",
      "epoch:33 step:25982 [D loss: 0.058169, acc.: 99.22%] [G loss: 2.622335]\n",
      "epoch:33 step:25983 [D loss: 2.616679, acc.: 29.69%] [G loss: 8.844661]\n",
      "epoch:33 step:25984 [D loss: 3.317195, acc.: 50.00%] [G loss: 6.770778]\n",
      "epoch:33 step:25985 [D loss: 2.748871, acc.: 50.00%] [G loss: 4.929715]\n",
      "epoch:33 step:25986 [D loss: 1.925726, acc.: 50.00%] [G loss: 3.570793]\n",
      "epoch:33 step:25987 [D loss: 1.397972, acc.: 50.00%] [G loss: 2.513315]\n",
      "epoch:33 step:25988 [D loss: 0.746634, acc.: 56.25%] [G loss: 1.957680]\n",
      "epoch:33 step:25989 [D loss: 0.570581, acc.: 70.31%] [G loss: 1.472330]\n",
      "epoch:33 step:25990 [D loss: 0.355123, acc.: 92.19%] [G loss: 1.631650]\n",
      "epoch:33 step:25991 [D loss: 0.276784, acc.: 94.53%] [G loss: 1.820516]\n",
      "epoch:33 step:25992 [D loss: 0.117856, acc.: 100.00%] [G loss: 1.740829]\n",
      "epoch:33 step:25993 [D loss: 0.167724, acc.: 97.66%] [G loss: 1.896281]\n",
      "epoch:33 step:25994 [D loss: 0.115846, acc.: 100.00%] [G loss: 2.311082]\n",
      "epoch:33 step:25995 [D loss: 0.100288, acc.: 98.44%] [G loss: 2.114094]\n",
      "epoch:33 step:25996 [D loss: 0.063649, acc.: 100.00%] [G loss: 2.005648]\n",
      "epoch:33 step:25997 [D loss: 0.078078, acc.: 99.22%] [G loss: 2.428527]\n",
      "epoch:33 step:25998 [D loss: 0.066254, acc.: 100.00%] [G loss: 2.407492]\n",
      "epoch:33 step:25999 [D loss: 0.082880, acc.: 99.22%] [G loss: 2.408661]\n",
      "epoch:33 step:26000 [D loss: 0.070460, acc.: 100.00%] [G loss: 2.114844]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##############\n",
      "[0.92103881 0.9599109  1.01103172 0.94502302 0.85531779 1.06820614\n",
      " 2.11381561 2.10631186 2.12350994 1.08799309]\n",
      "##########\n",
      "epoch:33 step:26001 [D loss: 0.054020, acc.: 100.00%] [G loss: 2.384690]\n",
      "epoch:33 step:26002 [D loss: 0.065501, acc.: 100.00%] [G loss: 2.533265]\n",
      "epoch:33 step:26003 [D loss: 0.104078, acc.: 99.22%] [G loss: 2.832132]\n",
      "epoch:33 step:26004 [D loss: 0.069904, acc.: 99.22%] [G loss: 2.547934]\n",
      "epoch:33 step:26005 [D loss: 0.105466, acc.: 99.22%] [G loss: 2.289898]\n",
      "epoch:33 step:26006 [D loss: 0.106654, acc.: 100.00%] [G loss: 2.305476]\n",
      "epoch:33 step:26007 [D loss: 0.076979, acc.: 99.22%] [G loss: 1.870897]\n",
      "epoch:33 step:26008 [D loss: 0.151572, acc.: 96.09%] [G loss: 2.945483]\n",
      "epoch:33 step:26009 [D loss: 0.040412, acc.: 100.00%] [G loss: 3.408783]\n",
      "epoch:33 step:26010 [D loss: 0.068642, acc.: 99.22%] [G loss: 2.793435]\n",
      "epoch:33 step:26011 [D loss: 0.090956, acc.: 100.00%] [G loss: 2.278019]\n",
      "epoch:33 step:26012 [D loss: 0.056140, acc.: 100.00%] [G loss: 2.664985]\n",
      "epoch:33 step:26013 [D loss: 0.815182, acc.: 56.25%] [G loss: 4.443629]\n",
      "epoch:33 step:26014 [D loss: 0.588109, acc.: 67.97%] [G loss: 4.159368]\n",
      "epoch:33 step:26015 [D loss: 0.256589, acc.: 85.94%] [G loss: 3.320723]\n",
      "epoch:33 step:26016 [D loss: 0.043517, acc.: 99.22%] [G loss: 3.085536]\n",
      "epoch:33 step:26017 [D loss: 0.028386, acc.: 100.00%] [G loss: 3.054323]\n",
      "epoch:33 step:26018 [D loss: 0.035688, acc.: 100.00%] [G loss: 2.947148]\n",
      "epoch:33 step:26019 [D loss: 0.041068, acc.: 100.00%] [G loss: 3.027461]\n",
      "epoch:33 step:26020 [D loss: 0.059712, acc.: 98.44%] [G loss: 2.738214]\n",
      "epoch:33 step:26021 [D loss: 0.041252, acc.: 100.00%] [G loss: 2.435298]\n",
      "epoch:33 step:26022 [D loss: 0.037930, acc.: 100.00%] [G loss: 2.984296]\n",
      "epoch:33 step:26023 [D loss: 0.088432, acc.: 98.44%] [G loss: 2.893826]\n",
      "epoch:33 step:26024 [D loss: 0.044588, acc.: 100.00%] [G loss: 3.076241]\n",
      "epoch:33 step:26025 [D loss: 0.050756, acc.: 100.00%] [G loss: 3.045910]\n",
      "epoch:33 step:26026 [D loss: 0.090158, acc.: 98.44%] [G loss: 2.886484]\n",
      "epoch:33 step:26027 [D loss: 0.046977, acc.: 100.00%] [G loss: 2.792317]\n",
      "epoch:33 step:26028 [D loss: 0.012526, acc.: 100.00%] [G loss: 3.436357]\n",
      "epoch:33 step:26029 [D loss: 0.017377, acc.: 100.00%] [G loss: 2.178383]\n",
      "epoch:33 step:26030 [D loss: 0.036771, acc.: 100.00%] [G loss: 3.167336]\n",
      "epoch:33 step:26031 [D loss: 0.332057, acc.: 84.38%] [G loss: 6.500047]\n",
      "epoch:33 step:26032 [D loss: 0.709676, acc.: 62.50%] [G loss: 2.665766]\n",
      "epoch:33 step:26033 [D loss: 0.368365, acc.: 86.72%] [G loss: 5.243795]\n",
      "epoch:33 step:26034 [D loss: 0.228578, acc.: 86.72%] [G loss: 5.249434]\n",
      "epoch:33 step:26035 [D loss: 0.007337, acc.: 100.00%] [G loss: 5.147650]\n",
      "epoch:33 step:26036 [D loss: 0.009354, acc.: 100.00%] [G loss: 4.978574]\n",
      "epoch:33 step:26037 [D loss: 0.037504, acc.: 99.22%] [G loss: 4.636156]\n",
      "epoch:33 step:26038 [D loss: 0.011707, acc.: 100.00%] [G loss: 4.378860]\n",
      "epoch:33 step:26039 [D loss: 0.012595, acc.: 100.00%] [G loss: 4.175794]\n",
      "epoch:33 step:26040 [D loss: 0.017737, acc.: 100.00%] [G loss: 4.054176]\n",
      "epoch:33 step:26041 [D loss: 0.026722, acc.: 100.00%] [G loss: 4.284574]\n",
      "epoch:33 step:26042 [D loss: 0.015208, acc.: 100.00%] [G loss: 4.123363]\n",
      "epoch:33 step:26043 [D loss: 0.021902, acc.: 99.22%] [G loss: 4.096794]\n",
      "epoch:33 step:26044 [D loss: 0.032528, acc.: 99.22%] [G loss: 3.663151]\n",
      "epoch:33 step:26045 [D loss: 0.031906, acc.: 100.00%] [G loss: 3.697345]\n",
      "epoch:33 step:26046 [D loss: 0.095252, acc.: 98.44%] [G loss: 4.851554]\n",
      "epoch:33 step:26047 [D loss: 0.014327, acc.: 100.00%] [G loss: 4.793376]\n",
      "epoch:33 step:26048 [D loss: 0.015471, acc.: 100.00%] [G loss: 4.541364]\n",
      "epoch:33 step:26049 [D loss: 0.012423, acc.: 100.00%] [G loss: 4.732147]\n",
      "epoch:33 step:26050 [D loss: 0.011927, acc.: 100.00%] [G loss: 4.356484]\n",
      "epoch:33 step:26051 [D loss: 0.013944, acc.: 100.00%] [G loss: 4.442701]\n",
      "epoch:33 step:26052 [D loss: 0.014317, acc.: 100.00%] [G loss: 4.130035]\n",
      "epoch:33 step:26053 [D loss: 0.028819, acc.: 99.22%] [G loss: 3.615233]\n",
      "epoch:33 step:26054 [D loss: 0.019521, acc.: 100.00%] [G loss: 3.902834]\n",
      "epoch:33 step:26055 [D loss: 0.016174, acc.: 100.00%] [G loss: 3.947837]\n",
      "epoch:33 step:26056 [D loss: 0.020183, acc.: 99.22%] [G loss: 3.961215]\n",
      "epoch:33 step:26057 [D loss: 0.031168, acc.: 99.22%] [G loss: 4.255284]\n",
      "epoch:33 step:26058 [D loss: 0.029564, acc.: 100.00%] [G loss: 3.952751]\n",
      "epoch:33 step:26059 [D loss: 0.260910, acc.: 91.41%] [G loss: 2.177274]\n",
      "epoch:33 step:26060 [D loss: 0.012097, acc.: 100.00%] [G loss: 3.255728]\n",
      "epoch:33 step:26061 [D loss: 0.034260, acc.: 99.22%] [G loss: 4.296159]\n",
      "epoch:33 step:26062 [D loss: 0.027049, acc.: 100.00%] [G loss: 4.786443]\n",
      "epoch:33 step:26063 [D loss: 0.013311, acc.: 100.00%] [G loss: 4.743462]\n",
      "epoch:33 step:26064 [D loss: 0.091721, acc.: 97.66%] [G loss: 2.086797]\n",
      "epoch:33 step:26065 [D loss: 0.095284, acc.: 96.88%] [G loss: 5.206192]\n",
      "epoch:33 step:26066 [D loss: 0.011257, acc.: 100.00%] [G loss: 5.551015]\n",
      "epoch:33 step:26067 [D loss: 0.095528, acc.: 97.66%] [G loss: 4.199531]\n",
      "epoch:33 step:26068 [D loss: 0.054987, acc.: 99.22%] [G loss: 3.750852]\n",
      "epoch:33 step:26069 [D loss: 0.017897, acc.: 100.00%] [G loss: 5.150361]\n",
      "epoch:33 step:26070 [D loss: 0.019629, acc.: 100.00%] [G loss: 5.282222]\n",
      "epoch:33 step:26071 [D loss: 0.015668, acc.: 100.00%] [G loss: 4.387277]\n",
      "epoch:33 step:26072 [D loss: 0.025655, acc.: 100.00%] [G loss: 4.551987]\n",
      "epoch:33 step:26073 [D loss: 0.011560, acc.: 100.00%] [G loss: 4.844373]\n",
      "epoch:33 step:26074 [D loss: 0.012201, acc.: 100.00%] [G loss: 5.112232]\n",
      "epoch:33 step:26075 [D loss: 0.008523, acc.: 100.00%] [G loss: 4.383607]\n",
      "epoch:33 step:26076 [D loss: 0.029488, acc.: 100.00%] [G loss: 3.879929]\n",
      "epoch:33 step:26077 [D loss: 0.043286, acc.: 99.22%] [G loss: 4.440415]\n",
      "epoch:33 step:26078 [D loss: 0.078124, acc.: 97.66%] [G loss: 3.524526]\n",
      "epoch:33 step:26079 [D loss: 0.024272, acc.: 100.00%] [G loss: 4.450981]\n",
      "epoch:33 step:26080 [D loss: 0.022382, acc.: 100.00%] [G loss: 4.725313]\n",
      "epoch:33 step:26081 [D loss: 0.009075, acc.: 100.00%] [G loss: 4.561144]\n",
      "epoch:33 step:26082 [D loss: 0.042083, acc.: 100.00%] [G loss: 3.524520]\n",
      "epoch:33 step:26083 [D loss: 0.032837, acc.: 100.00%] [G loss: 4.136781]\n",
      "epoch:33 step:26084 [D loss: 0.005655, acc.: 100.00%] [G loss: 4.931454]\n",
      "epoch:33 step:26085 [D loss: 0.890428, acc.: 59.38%] [G loss: 7.540764]\n",
      "epoch:33 step:26086 [D loss: 2.704845, acc.: 50.00%] [G loss: 5.470578]\n",
      "epoch:33 step:26087 [D loss: 0.657471, acc.: 75.00%] [G loss: 3.658276]\n",
      "epoch:33 step:26088 [D loss: 0.039248, acc.: 100.00%] [G loss: 3.306531]\n",
      "epoch:33 step:26089 [D loss: 0.062424, acc.: 99.22%] [G loss: 2.535577]\n",
      "epoch:33 step:26090 [D loss: 0.024047, acc.: 100.00%] [G loss: 2.599767]\n",
      "epoch:33 step:26091 [D loss: 0.030046, acc.: 100.00%] [G loss: 2.844198]\n",
      "epoch:33 step:26092 [D loss: 0.049588, acc.: 100.00%] [G loss: 3.172954]\n",
      "epoch:33 step:26093 [D loss: 0.068597, acc.: 99.22%] [G loss: 4.319241]\n",
      "epoch:33 step:26094 [D loss: 0.006584, acc.: 100.00%] [G loss: 4.238893]\n",
      "epoch:33 step:26095 [D loss: 0.014614, acc.: 100.00%] [G loss: 4.781076]\n",
      "epoch:33 step:26096 [D loss: 0.009722, acc.: 100.00%] [G loss: 4.200542]\n",
      "epoch:33 step:26097 [D loss: 0.121742, acc.: 94.53%] [G loss: 2.455483]\n",
      "epoch:33 step:26098 [D loss: 0.195230, acc.: 93.75%] [G loss: 4.777842]\n",
      "epoch:33 step:26099 [D loss: 0.005869, acc.: 100.00%] [G loss: 5.698283]\n",
      "epoch:33 step:26100 [D loss: 0.099376, acc.: 96.88%] [G loss: 5.200990]\n",
      "epoch:33 step:26101 [D loss: 0.004529, acc.: 100.00%] [G loss: 4.598954]\n",
      "epoch:33 step:26102 [D loss: 0.006295, acc.: 100.00%] [G loss: 4.142517]\n",
      "epoch:33 step:26103 [D loss: 0.025623, acc.: 100.00%] [G loss: 4.416782]\n",
      "epoch:33 step:26104 [D loss: 0.014962, acc.: 100.00%] [G loss: 4.720017]\n",
      "epoch:33 step:26105 [D loss: 0.008328, acc.: 100.00%] [G loss: 4.697307]\n",
      "epoch:33 step:26106 [D loss: 0.012912, acc.: 100.00%] [G loss: 4.832182]\n",
      "epoch:33 step:26107 [D loss: 0.017545, acc.: 100.00%] [G loss: 4.183445]\n",
      "epoch:33 step:26108 [D loss: 0.012076, acc.: 100.00%] [G loss: 4.024753]\n",
      "epoch:33 step:26109 [D loss: 0.031194, acc.: 100.00%] [G loss: 3.813971]\n",
      "epoch:33 step:26110 [D loss: 0.032854, acc.: 100.00%] [G loss: 4.397292]\n",
      "epoch:33 step:26111 [D loss: 0.030879, acc.: 99.22%] [G loss: 4.574913]\n",
      "epoch:33 step:26112 [D loss: 0.012823, acc.: 100.00%] [G loss: 3.849997]\n",
      "epoch:33 step:26113 [D loss: 0.107937, acc.: 96.88%] [G loss: 3.985475]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:33 step:26114 [D loss: 0.014065, acc.: 100.00%] [G loss: 4.091989]\n",
      "epoch:33 step:26115 [D loss: 0.005780, acc.: 100.00%] [G loss: 4.133763]\n",
      "epoch:33 step:26116 [D loss: 0.022972, acc.: 99.22%] [G loss: 4.241447]\n",
      "epoch:33 step:26117 [D loss: 0.015362, acc.: 100.00%] [G loss: 3.455156]\n",
      "epoch:33 step:26118 [D loss: 0.027761, acc.: 99.22%] [G loss: 3.470132]\n",
      "epoch:33 step:26119 [D loss: 0.028335, acc.: 100.00%] [G loss: 4.002288]\n",
      "epoch:33 step:26120 [D loss: 0.016609, acc.: 100.00%] [G loss: 4.573056]\n",
      "epoch:33 step:26121 [D loss: 0.008485, acc.: 100.00%] [G loss: 4.843750]\n",
      "epoch:33 step:26122 [D loss: 0.016555, acc.: 100.00%] [G loss: 4.557571]\n",
      "epoch:33 step:26123 [D loss: 0.120004, acc.: 95.31%] [G loss: 3.199631]\n",
      "epoch:33 step:26124 [D loss: 0.083915, acc.: 97.66%] [G loss: 4.103235]\n",
      "epoch:33 step:26125 [D loss: 0.005707, acc.: 100.00%] [G loss: 4.648236]\n",
      "epoch:33 step:26126 [D loss: 0.009588, acc.: 100.00%] [G loss: 4.671295]\n",
      "epoch:33 step:26127 [D loss: 0.006413, acc.: 100.00%] [G loss: 4.740201]\n",
      "epoch:33 step:26128 [D loss: 0.015528, acc.: 100.00%] [G loss: 4.384980]\n",
      "epoch:33 step:26129 [D loss: 0.014250, acc.: 100.00%] [G loss: 4.339890]\n",
      "epoch:33 step:26130 [D loss: 0.018333, acc.: 100.00%] [G loss: 4.036692]\n",
      "epoch:33 step:26131 [D loss: 0.031839, acc.: 100.00%] [G loss: 4.729343]\n",
      "epoch:33 step:26132 [D loss: 0.012722, acc.: 100.00%] [G loss: 5.668387]\n",
      "epoch:33 step:26133 [D loss: 0.006251, acc.: 100.00%] [G loss: 4.730921]\n",
      "epoch:33 step:26134 [D loss: 0.055633, acc.: 99.22%] [G loss: 4.387334]\n",
      "epoch:33 step:26135 [D loss: 0.015732, acc.: 100.00%] [G loss: 2.807940]\n",
      "epoch:33 step:26136 [D loss: 0.024049, acc.: 100.00%] [G loss: 4.061599]\n",
      "epoch:33 step:26137 [D loss: 0.025170, acc.: 100.00%] [G loss: 4.879239]\n",
      "epoch:33 step:26138 [D loss: 0.012003, acc.: 100.00%] [G loss: 5.954833]\n",
      "epoch:33 step:26139 [D loss: 0.021931, acc.: 100.00%] [G loss: 4.077138]\n",
      "epoch:33 step:26140 [D loss: 0.012357, acc.: 100.00%] [G loss: 4.499852]\n",
      "epoch:33 step:26141 [D loss: 0.005040, acc.: 100.00%] [G loss: 4.696777]\n",
      "epoch:33 step:26142 [D loss: 0.010104, acc.: 100.00%] [G loss: 4.247900]\n",
      "epoch:33 step:26143 [D loss: 0.031802, acc.: 100.00%] [G loss: 3.868264]\n",
      "epoch:33 step:26144 [D loss: 0.039611, acc.: 100.00%] [G loss: 5.059477]\n",
      "epoch:33 step:26145 [D loss: 0.003833, acc.: 100.00%] [G loss: 5.477692]\n",
      "epoch:33 step:26146 [D loss: 0.011566, acc.: 100.00%] [G loss: 5.745825]\n",
      "epoch:33 step:26147 [D loss: 0.110925, acc.: 96.09%] [G loss: 3.499795]\n",
      "epoch:33 step:26148 [D loss: 0.116376, acc.: 97.66%] [G loss: 6.674950]\n",
      "epoch:33 step:26149 [D loss: 1.122074, acc.: 37.50%] [G loss: 7.627111]\n",
      "epoch:33 step:26150 [D loss: 1.510239, acc.: 52.34%] [G loss: 2.452947]\n",
      "epoch:33 step:26151 [D loss: 0.419789, acc.: 83.59%] [G loss: 4.744422]\n",
      "epoch:33 step:26152 [D loss: 0.014804, acc.: 100.00%] [G loss: 6.386209]\n",
      "epoch:33 step:26153 [D loss: 0.099001, acc.: 96.88%] [G loss: 4.714196]\n",
      "epoch:33 step:26154 [D loss: 0.031347, acc.: 99.22%] [G loss: 4.686660]\n",
      "epoch:33 step:26155 [D loss: 0.042836, acc.: 100.00%] [G loss: 3.713920]\n",
      "epoch:33 step:26156 [D loss: 0.083363, acc.: 97.66%] [G loss: 4.166423]\n",
      "epoch:33 step:26157 [D loss: 0.040554, acc.: 99.22%] [G loss: 4.148358]\n",
      "epoch:33 step:26158 [D loss: 0.139826, acc.: 95.31%] [G loss: 3.321877]\n",
      "epoch:33 step:26159 [D loss: 0.052816, acc.: 98.44%] [G loss: 3.654909]\n",
      "epoch:33 step:26160 [D loss: 0.065890, acc.: 98.44%] [G loss: 3.308130]\n",
      "epoch:33 step:26161 [D loss: 0.255549, acc.: 90.62%] [G loss: 5.336499]\n",
      "epoch:33 step:26162 [D loss: 0.209624, acc.: 89.06%] [G loss: 2.750791]\n",
      "epoch:33 step:26163 [D loss: 0.027620, acc.: 99.22%] [G loss: 2.903521]\n",
      "epoch:33 step:26164 [D loss: 0.049523, acc.: 99.22%] [G loss: 3.465118]\n",
      "epoch:33 step:26165 [D loss: 0.044520, acc.: 99.22%] [G loss: 3.485392]\n",
      "epoch:33 step:26166 [D loss: 1.384875, acc.: 41.41%] [G loss: 8.130758]\n",
      "epoch:33 step:26167 [D loss: 0.909168, acc.: 60.94%] [G loss: 6.639644]\n",
      "epoch:33 step:26168 [D loss: 0.012726, acc.: 99.22%] [G loss: 5.670141]\n",
      "epoch:33 step:26169 [D loss: 0.009183, acc.: 100.00%] [G loss: 4.079565]\n",
      "epoch:33 step:26170 [D loss: 0.017754, acc.: 100.00%] [G loss: 4.492885]\n",
      "epoch:33 step:26171 [D loss: 0.061113, acc.: 98.44%] [G loss: 4.773019]\n",
      "epoch:33 step:26172 [D loss: 0.004230, acc.: 100.00%] [G loss: 4.979011]\n",
      "epoch:33 step:26173 [D loss: 0.004645, acc.: 100.00%] [G loss: 5.227477]\n",
      "epoch:33 step:26174 [D loss: 0.005023, acc.: 100.00%] [G loss: 3.854647]\n",
      "epoch:33 step:26175 [D loss: 0.012158, acc.: 100.00%] [G loss: 4.621066]\n",
      "epoch:33 step:26176 [D loss: 0.111316, acc.: 98.44%] [G loss: 4.612617]\n",
      "epoch:33 step:26177 [D loss: 0.029866, acc.: 100.00%] [G loss: 5.030578]\n",
      "epoch:33 step:26178 [D loss: 0.471598, acc.: 76.56%] [G loss: 5.428503]\n",
      "epoch:33 step:26179 [D loss: 0.004698, acc.: 100.00%] [G loss: 6.822718]\n",
      "epoch:33 step:26180 [D loss: 0.008650, acc.: 100.00%] [G loss: 6.531124]\n",
      "epoch:33 step:26181 [D loss: 0.014594, acc.: 100.00%] [G loss: 6.222925]\n",
      "epoch:33 step:26182 [D loss: 0.005917, acc.: 100.00%] [G loss: 5.587210]\n",
      "epoch:33 step:26183 [D loss: 0.003048, acc.: 100.00%] [G loss: 5.256979]\n",
      "epoch:33 step:26184 [D loss: 0.013257, acc.: 100.00%] [G loss: 5.077726]\n",
      "epoch:33 step:26185 [D loss: 0.005209, acc.: 100.00%] [G loss: 5.416934]\n",
      "epoch:33 step:26186 [D loss: 0.035184, acc.: 100.00%] [G loss: 4.389984]\n",
      "epoch:33 step:26187 [D loss: 0.006482, acc.: 100.00%] [G loss: 4.556340]\n",
      "epoch:33 step:26188 [D loss: 0.027883, acc.: 100.00%] [G loss: 4.835648]\n",
      "epoch:33 step:26189 [D loss: 0.014575, acc.: 100.00%] [G loss: 5.052366]\n",
      "epoch:33 step:26190 [D loss: 0.054449, acc.: 99.22%] [G loss: 4.243297]\n",
      "epoch:33 step:26191 [D loss: 0.004745, acc.: 100.00%] [G loss: 4.412072]\n",
      "epoch:33 step:26192 [D loss: 0.062896, acc.: 99.22%] [G loss: 5.178010]\n",
      "epoch:33 step:26193 [D loss: 0.037623, acc.: 99.22%] [G loss: 5.134420]\n",
      "epoch:33 step:26194 [D loss: 0.013831, acc.: 100.00%] [G loss: 4.834729]\n",
      "epoch:33 step:26195 [D loss: 0.022452, acc.: 100.00%] [G loss: 4.521253]\n",
      "epoch:33 step:26196 [D loss: 0.012235, acc.: 100.00%] [G loss: 3.478732]\n",
      "epoch:33 step:26197 [D loss: 0.061776, acc.: 100.00%] [G loss: 4.068278]\n",
      "epoch:33 step:26198 [D loss: 0.007783, acc.: 100.00%] [G loss: 3.916354]\n",
      "epoch:33 step:26199 [D loss: 0.010673, acc.: 100.00%] [G loss: 4.214899]\n",
      "epoch:33 step:26200 [D loss: 0.012633, acc.: 100.00%] [G loss: 3.314337]\n",
      "##############\n",
      "[0.988785   1.04003739 0.96726069 0.98222949 0.89535428 1.11486286\n",
      " 2.11333678 2.10409078 2.10761404 2.1064683 ]\n",
      "##########\n",
      "epoch:33 step:26201 [D loss: 1.175189, acc.: 46.09%] [G loss: 7.785371]\n",
      "epoch:33 step:26202 [D loss: 2.434729, acc.: 50.00%] [G loss: 5.431509]\n",
      "epoch:33 step:26203 [D loss: 0.270597, acc.: 89.06%] [G loss: 3.157531]\n",
      "epoch:33 step:26204 [D loss: 0.194338, acc.: 93.75%] [G loss: 2.921377]\n",
      "epoch:33 step:26205 [D loss: 0.148341, acc.: 92.97%] [G loss: 3.542444]\n",
      "epoch:33 step:26206 [D loss: 0.037026, acc.: 99.22%] [G loss: 3.953480]\n",
      "epoch:33 step:26207 [D loss: 0.144961, acc.: 95.31%] [G loss: 3.996508]\n",
      "epoch:33 step:26208 [D loss: 0.164370, acc.: 96.09%] [G loss: 2.838425]\n",
      "epoch:33 step:26209 [D loss: 0.281352, acc.: 85.94%] [G loss: 4.046857]\n",
      "epoch:33 step:26210 [D loss: 0.041680, acc.: 100.00%] [G loss: 4.143672]\n",
      "epoch:33 step:26211 [D loss: 0.643659, acc.: 69.53%] [G loss: 1.797833]\n",
      "epoch:33 step:26212 [D loss: 0.033067, acc.: 100.00%] [G loss: 3.283842]\n",
      "epoch:33 step:26213 [D loss: 0.547494, acc.: 74.22%] [G loss: 6.290709]\n",
      "epoch:33 step:26214 [D loss: 1.492318, acc.: 51.56%] [G loss: 5.226186]\n",
      "epoch:33 step:26215 [D loss: 0.104018, acc.: 95.31%] [G loss: 4.419934]\n",
      "epoch:33 step:26216 [D loss: 0.030192, acc.: 100.00%] [G loss: 4.147318]\n",
      "epoch:33 step:26217 [D loss: 0.027255, acc.: 100.00%] [G loss: 3.754935]\n",
      "epoch:33 step:26218 [D loss: 0.016004, acc.: 100.00%] [G loss: 2.957788]\n",
      "epoch:33 step:26219 [D loss: 0.042873, acc.: 100.00%] [G loss: 3.285518]\n",
      "epoch:33 step:26220 [D loss: 0.035498, acc.: 100.00%] [G loss: 3.417605]\n",
      "epoch:33 step:26221 [D loss: 0.103907, acc.: 98.44%] [G loss: 3.610344]\n",
      "epoch:33 step:26222 [D loss: 0.039299, acc.: 100.00%] [G loss: 3.742454]\n",
      "epoch:33 step:26223 [D loss: 0.076400, acc.: 98.44%] [G loss: 3.429039]\n",
      "epoch:33 step:26224 [D loss: 0.111023, acc.: 97.66%] [G loss: 3.049997]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:33 step:26225 [D loss: 0.022215, acc.: 100.00%] [G loss: 2.847175]\n",
      "epoch:33 step:26226 [D loss: 0.056935, acc.: 99.22%] [G loss: 2.870570]\n",
      "epoch:33 step:26227 [D loss: 0.034987, acc.: 100.00%] [G loss: 3.419724]\n",
      "epoch:33 step:26228 [D loss: 0.048061, acc.: 100.00%] [G loss: 3.781997]\n",
      "epoch:33 step:26229 [D loss: 0.051956, acc.: 99.22%] [G loss: 3.174438]\n",
      "epoch:33 step:26230 [D loss: 0.021424, acc.: 100.00%] [G loss: 3.713324]\n",
      "epoch:33 step:26231 [D loss: 0.026342, acc.: 100.00%] [G loss: 2.740937]\n",
      "epoch:33 step:26232 [D loss: 0.197402, acc.: 96.88%] [G loss: 2.688258]\n",
      "epoch:33 step:26233 [D loss: 0.186334, acc.: 92.19%] [G loss: 2.735701]\n",
      "epoch:33 step:26234 [D loss: 0.122089, acc.: 95.31%] [G loss: 5.052152]\n",
      "epoch:33 step:26235 [D loss: 0.108553, acc.: 94.53%] [G loss: 4.753075]\n",
      "epoch:33 step:26236 [D loss: 0.008010, acc.: 100.00%] [G loss: 4.434388]\n",
      "epoch:33 step:26237 [D loss: 0.042818, acc.: 99.22%] [G loss: 4.080545]\n",
      "epoch:33 step:26238 [D loss: 0.025370, acc.: 100.00%] [G loss: 3.896776]\n",
      "epoch:33 step:26239 [D loss: 0.036063, acc.: 100.00%] [G loss: 4.032916]\n",
      "epoch:33 step:26240 [D loss: 0.027867, acc.: 100.00%] [G loss: 3.569305]\n",
      "epoch:33 step:26241 [D loss: 0.040885, acc.: 100.00%] [G loss: 3.272553]\n",
      "epoch:33 step:26242 [D loss: 0.020474, acc.: 100.00%] [G loss: 3.200141]\n",
      "epoch:33 step:26243 [D loss: 0.009746, acc.: 100.00%] [G loss: 3.239282]\n",
      "epoch:33 step:26244 [D loss: 0.058218, acc.: 100.00%] [G loss: 4.406547]\n",
      "epoch:33 step:26245 [D loss: 0.025730, acc.: 100.00%] [G loss: 4.982399]\n",
      "epoch:33 step:26246 [D loss: 0.068761, acc.: 97.66%] [G loss: 3.492834]\n",
      "epoch:33 step:26247 [D loss: 0.157206, acc.: 96.09%] [G loss: 4.770932]\n",
      "epoch:33 step:26248 [D loss: 0.040927, acc.: 99.22%] [G loss: 4.581611]\n",
      "epoch:33 step:26249 [D loss: 0.004672, acc.: 100.00%] [G loss: 5.193287]\n",
      "epoch:33 step:26250 [D loss: 0.011926, acc.: 100.00%] [G loss: 4.922640]\n",
      "epoch:33 step:26251 [D loss: 0.018273, acc.: 100.00%] [G loss: 4.558383]\n",
      "epoch:33 step:26252 [D loss: 0.009748, acc.: 100.00%] [G loss: 5.044035]\n",
      "epoch:33 step:26253 [D loss: 0.011215, acc.: 100.00%] [G loss: 4.706963]\n",
      "epoch:33 step:26254 [D loss: 0.009433, acc.: 100.00%] [G loss: 4.780711]\n",
      "epoch:33 step:26255 [D loss: 0.035012, acc.: 100.00%] [G loss: 4.648234]\n",
      "epoch:33 step:26256 [D loss: 0.025869, acc.: 100.00%] [G loss: 3.829088]\n",
      "epoch:33 step:26257 [D loss: 0.036804, acc.: 100.00%] [G loss: 4.644700]\n",
      "epoch:33 step:26258 [D loss: 0.100912, acc.: 96.88%] [G loss: 4.014302]\n",
      "epoch:33 step:26259 [D loss: 0.039107, acc.: 99.22%] [G loss: 4.876937]\n",
      "epoch:33 step:26260 [D loss: 0.005093, acc.: 100.00%] [G loss: 5.377645]\n",
      "epoch:33 step:26261 [D loss: 0.010879, acc.: 100.00%] [G loss: 5.049548]\n",
      "epoch:33 step:26262 [D loss: 0.012248, acc.: 100.00%] [G loss: 4.577378]\n",
      "epoch:33 step:26263 [D loss: 0.005478, acc.: 100.00%] [G loss: 4.691629]\n",
      "epoch:33 step:26264 [D loss: 0.035800, acc.: 99.22%] [G loss: 4.211663]\n",
      "epoch:33 step:26265 [D loss: 0.043695, acc.: 99.22%] [G loss: 3.128013]\n",
      "epoch:33 step:26266 [D loss: 0.029746, acc.: 99.22%] [G loss: 4.560102]\n",
      "epoch:33 step:26267 [D loss: 0.005082, acc.: 100.00%] [G loss: 5.047413]\n",
      "epoch:33 step:26268 [D loss: 0.034822, acc.: 99.22%] [G loss: 4.283334]\n",
      "epoch:33 step:26269 [D loss: 0.015916, acc.: 100.00%] [G loss: 4.940258]\n",
      "epoch:33 step:26270 [D loss: 0.029653, acc.: 100.00%] [G loss: 3.886323]\n",
      "epoch:33 step:26271 [D loss: 0.016885, acc.: 100.00%] [G loss: 3.566264]\n",
      "epoch:33 step:26272 [D loss: 0.095107, acc.: 99.22%] [G loss: 5.547472]\n",
      "epoch:33 step:26273 [D loss: 0.006980, acc.: 100.00%] [G loss: 6.263790]\n",
      "epoch:33 step:26274 [D loss: 0.872167, acc.: 55.47%] [G loss: 6.156079]\n",
      "epoch:33 step:26275 [D loss: 0.011518, acc.: 100.00%] [G loss: 7.023317]\n",
      "epoch:33 step:26276 [D loss: 0.095819, acc.: 96.88%] [G loss: 6.155118]\n",
      "epoch:33 step:26277 [D loss: 0.003434, acc.: 100.00%] [G loss: 5.831549]\n",
      "epoch:33 step:26278 [D loss: 0.006546, acc.: 100.00%] [G loss: 5.624728]\n",
      "epoch:33 step:26279 [D loss: 0.002321, acc.: 100.00%] [G loss: 6.124890]\n",
      "epoch:33 step:26280 [D loss: 0.004831, acc.: 100.00%] [G loss: 5.850368]\n",
      "epoch:33 step:26281 [D loss: 0.013712, acc.: 100.00%] [G loss: 4.864533]\n",
      "epoch:33 step:26282 [D loss: 0.033258, acc.: 100.00%] [G loss: 4.036137]\n",
      "epoch:33 step:26283 [D loss: 0.055377, acc.: 99.22%] [G loss: 4.263022]\n",
      "epoch:33 step:26284 [D loss: 0.022748, acc.: 100.00%] [G loss: 3.983411]\n",
      "epoch:33 step:26285 [D loss: 0.011759, acc.: 100.00%] [G loss: 3.782563]\n",
      "epoch:33 step:26286 [D loss: 0.047657, acc.: 99.22%] [G loss: 4.655392]\n",
      "epoch:33 step:26287 [D loss: 0.031928, acc.: 99.22%] [G loss: 3.915486]\n",
      "epoch:33 step:26288 [D loss: 0.023116, acc.: 99.22%] [G loss: 4.370522]\n",
      "epoch:33 step:26289 [D loss: 0.038950, acc.: 100.00%] [G loss: 4.198327]\n",
      "epoch:33 step:26290 [D loss: 0.005820, acc.: 100.00%] [G loss: 4.897883]\n",
      "epoch:33 step:26291 [D loss: 0.020882, acc.: 100.00%] [G loss: 4.036742]\n",
      "epoch:33 step:26292 [D loss: 0.008550, acc.: 100.00%] [G loss: 5.332501]\n",
      "epoch:33 step:26293 [D loss: 0.019200, acc.: 100.00%] [G loss: 4.458031]\n",
      "epoch:33 step:26294 [D loss: 0.014125, acc.: 100.00%] [G loss: 4.648390]\n",
      "epoch:33 step:26295 [D loss: 0.012945, acc.: 100.00%] [G loss: 5.006221]\n",
      "epoch:33 step:26296 [D loss: 0.130968, acc.: 98.44%] [G loss: 5.802155]\n",
      "epoch:33 step:26297 [D loss: 0.185170, acc.: 92.97%] [G loss: 3.887320]\n",
      "epoch:33 step:26298 [D loss: 0.013143, acc.: 100.00%] [G loss: 4.437964]\n",
      "epoch:33 step:26299 [D loss: 0.043680, acc.: 100.00%] [G loss: 4.173886]\n",
      "epoch:33 step:26300 [D loss: 0.004958, acc.: 100.00%] [G loss: 5.175480]\n",
      "epoch:33 step:26301 [D loss: 0.004790, acc.: 100.00%] [G loss: 4.963882]\n",
      "epoch:33 step:26302 [D loss: 0.017230, acc.: 100.00%] [G loss: 5.274753]\n",
      "epoch:33 step:26303 [D loss: 0.017595, acc.: 100.00%] [G loss: 4.727419]\n",
      "epoch:33 step:26304 [D loss: 0.028175, acc.: 100.00%] [G loss: 5.099957]\n",
      "epoch:33 step:26305 [D loss: 0.053398, acc.: 99.22%] [G loss: 3.069765]\n",
      "epoch:33 step:26306 [D loss: 0.456441, acc.: 73.44%] [G loss: 8.091202]\n",
      "epoch:33 step:26307 [D loss: 1.853942, acc.: 47.66%] [G loss: 2.544418]\n",
      "epoch:33 step:26308 [D loss: 1.300865, acc.: 57.03%] [G loss: 7.407677]\n",
      "epoch:33 step:26309 [D loss: 0.968906, acc.: 64.84%] [G loss: 6.464805]\n",
      "epoch:33 step:26310 [D loss: 0.338697, acc.: 83.59%] [G loss: 4.239526]\n",
      "epoch:33 step:26311 [D loss: 0.066912, acc.: 97.66%] [G loss: 4.025414]\n",
      "epoch:33 step:26312 [D loss: 0.022681, acc.: 100.00%] [G loss: 4.459626]\n",
      "epoch:33 step:26313 [D loss: 0.025264, acc.: 100.00%] [G loss: 3.863197]\n",
      "epoch:33 step:26314 [D loss: 0.030529, acc.: 98.44%] [G loss: 3.824399]\n",
      "epoch:33 step:26315 [D loss: 0.082833, acc.: 97.66%] [G loss: 3.085890]\n",
      "epoch:33 step:26316 [D loss: 0.068404, acc.: 100.00%] [G loss: 4.429646]\n",
      "epoch:33 step:26317 [D loss: 0.137352, acc.: 96.88%] [G loss: 3.559783]\n",
      "epoch:33 step:26318 [D loss: 0.065288, acc.: 98.44%] [G loss: 3.716103]\n",
      "epoch:33 step:26319 [D loss: 0.048060, acc.: 97.66%] [G loss: 3.106861]\n",
      "epoch:33 step:26320 [D loss: 0.186963, acc.: 92.19%] [G loss: 4.565305]\n",
      "epoch:33 step:26321 [D loss: 0.328355, acc.: 84.38%] [G loss: 2.792497]\n",
      "epoch:33 step:26322 [D loss: 0.116950, acc.: 95.31%] [G loss: 3.799144]\n",
      "epoch:33 step:26323 [D loss: 0.039691, acc.: 98.44%] [G loss: 4.166861]\n",
      "epoch:33 step:26324 [D loss: 0.030428, acc.: 99.22%] [G loss: 3.735729]\n",
      "epoch:33 step:26325 [D loss: 0.038174, acc.: 100.00%] [G loss: 3.480822]\n",
      "epoch:33 step:26326 [D loss: 0.136865, acc.: 96.09%] [G loss: 3.844843]\n",
      "epoch:33 step:26327 [D loss: 0.076605, acc.: 98.44%] [G loss: 3.337507]\n",
      "epoch:33 step:26328 [D loss: 0.063412, acc.: 100.00%] [G loss: 3.786493]\n",
      "epoch:33 step:26329 [D loss: 0.041796, acc.: 100.00%] [G loss: 3.740720]\n",
      "epoch:33 step:26330 [D loss: 0.048617, acc.: 99.22%] [G loss: 3.096562]\n",
      "epoch:33 step:26331 [D loss: 0.120616, acc.: 96.09%] [G loss: 2.086217]\n",
      "epoch:33 step:26332 [D loss: 0.295430, acc.: 85.94%] [G loss: 6.771627]\n",
      "epoch:33 step:26333 [D loss: 1.121220, acc.: 54.69%] [G loss: 1.833293]\n",
      "epoch:33 step:26334 [D loss: 0.464010, acc.: 78.91%] [G loss: 5.637101]\n",
      "epoch:33 step:26335 [D loss: 0.085349, acc.: 95.31%] [G loss: 6.711930]\n",
      "epoch:33 step:26336 [D loss: 0.213101, acc.: 90.62%] [G loss: 4.672774]\n",
      "epoch:33 step:26337 [D loss: 0.030250, acc.: 99.22%] [G loss: 2.862972]\n",
      "epoch:33 step:26338 [D loss: 0.348901, acc.: 81.25%] [G loss: 6.464513]\n",
      "epoch:33 step:26339 [D loss: 0.077728, acc.: 96.88%] [G loss: 7.100381]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:33 step:26340 [D loss: 0.281631, acc.: 85.94%] [G loss: 4.781509]\n",
      "epoch:33 step:26341 [D loss: 0.106245, acc.: 96.88%] [G loss: 4.413299]\n",
      "epoch:33 step:26342 [D loss: 0.052716, acc.: 98.44%] [G loss: 4.956267]\n",
      "epoch:33 step:26343 [D loss: 0.012847, acc.: 100.00%] [G loss: 5.627187]\n",
      "epoch:33 step:26344 [D loss: 0.014102, acc.: 100.00%] [G loss: 4.887954]\n",
      "epoch:33 step:26345 [D loss: 0.026459, acc.: 100.00%] [G loss: 4.928378]\n",
      "epoch:33 step:26346 [D loss: 0.077853, acc.: 98.44%] [G loss: 4.258012]\n",
      "epoch:33 step:26347 [D loss: 0.011705, acc.: 100.00%] [G loss: 4.395678]\n",
      "epoch:33 step:26348 [D loss: 0.053811, acc.: 99.22%] [G loss: 2.992466]\n",
      "epoch:33 step:26349 [D loss: 0.219979, acc.: 89.84%] [G loss: 6.041367]\n",
      "epoch:33 step:26350 [D loss: 0.447458, acc.: 78.12%] [G loss: 4.150544]\n",
      "epoch:33 step:26351 [D loss: 0.061604, acc.: 99.22%] [G loss: 3.183819]\n",
      "epoch:33 step:26352 [D loss: 0.005959, acc.: 100.00%] [G loss: 4.146556]\n",
      "epoch:33 step:26353 [D loss: 0.063851, acc.: 98.44%] [G loss: 4.165169]\n",
      "epoch:33 step:26354 [D loss: 0.016895, acc.: 100.00%] [G loss: 4.562023]\n",
      "epoch:33 step:26355 [D loss: 0.041253, acc.: 99.22%] [G loss: 4.298128]\n",
      "epoch:33 step:26356 [D loss: 0.021046, acc.: 100.00%] [G loss: 4.047158]\n",
      "epoch:33 step:26357 [D loss: 0.032515, acc.: 100.00%] [G loss: 3.891488]\n",
      "epoch:33 step:26358 [D loss: 0.010158, acc.: 100.00%] [G loss: 3.494056]\n",
      "epoch:33 step:26359 [D loss: 0.028609, acc.: 100.00%] [G loss: 3.696006]\n",
      "epoch:33 step:26360 [D loss: 0.020948, acc.: 100.00%] [G loss: 3.522743]\n",
      "epoch:33 step:26361 [D loss: 0.039614, acc.: 100.00%] [G loss: 3.990097]\n",
      "epoch:33 step:26362 [D loss: 0.032841, acc.: 100.00%] [G loss: 4.205559]\n",
      "epoch:33 step:26363 [D loss: 0.019962, acc.: 100.00%] [G loss: 4.425841]\n",
      "epoch:33 step:26364 [D loss: 0.034880, acc.: 100.00%] [G loss: 3.595501]\n",
      "epoch:33 step:26365 [D loss: 0.039322, acc.: 99.22%] [G loss: 5.107047]\n",
      "epoch:33 step:26366 [D loss: 0.060069, acc.: 97.66%] [G loss: 4.022017]\n",
      "epoch:33 step:26367 [D loss: 0.022810, acc.: 100.00%] [G loss: 3.765115]\n",
      "epoch:33 step:26368 [D loss: 0.018494, acc.: 100.00%] [G loss: 4.294724]\n",
      "epoch:33 step:26369 [D loss: 0.022093, acc.: 100.00%] [G loss: 4.109434]\n",
      "epoch:33 step:26370 [D loss: 0.013472, acc.: 100.00%] [G loss: 4.089833]\n",
      "epoch:33 step:26371 [D loss: 0.022393, acc.: 99.22%] [G loss: 4.655327]\n",
      "epoch:33 step:26372 [D loss: 0.034521, acc.: 99.22%] [G loss: 4.030477]\n",
      "epoch:33 step:26373 [D loss: 0.025088, acc.: 100.00%] [G loss: 3.900752]\n",
      "epoch:33 step:26374 [D loss: 0.022654, acc.: 100.00%] [G loss: 3.316410]\n",
      "epoch:33 step:26375 [D loss: 0.030129, acc.: 100.00%] [G loss: 3.733313]\n",
      "epoch:33 step:26376 [D loss: 0.017644, acc.: 100.00%] [G loss: 3.267570]\n",
      "epoch:33 step:26377 [D loss: 0.953192, acc.: 53.91%] [G loss: 7.849339]\n",
      "epoch:33 step:26378 [D loss: 1.524699, acc.: 54.69%] [G loss: 5.430265]\n",
      "epoch:33 step:26379 [D loss: 0.027444, acc.: 100.00%] [G loss: 4.283658]\n",
      "epoch:33 step:26380 [D loss: 0.026828, acc.: 100.00%] [G loss: 3.767435]\n",
      "epoch:33 step:26381 [D loss: 0.025768, acc.: 100.00%] [G loss: 4.166305]\n",
      "epoch:33 step:26382 [D loss: 0.031017, acc.: 100.00%] [G loss: 3.496864]\n",
      "epoch:33 step:26383 [D loss: 0.112495, acc.: 96.88%] [G loss: 4.715797]\n",
      "epoch:33 step:26384 [D loss: 0.021737, acc.: 99.22%] [G loss: 6.257061]\n",
      "epoch:33 step:26385 [D loss: 0.041441, acc.: 98.44%] [G loss: 4.865932]\n",
      "epoch:33 step:26386 [D loss: 0.078468, acc.: 96.88%] [G loss: 2.317417]\n",
      "epoch:33 step:26387 [D loss: 0.346760, acc.: 81.25%] [G loss: 6.666543]\n",
      "epoch:33 step:26388 [D loss: 0.068926, acc.: 97.66%] [G loss: 7.540597]\n",
      "epoch:33 step:26389 [D loss: 0.375947, acc.: 80.47%] [G loss: 2.712946]\n",
      "epoch:33 step:26390 [D loss: 0.422256, acc.: 78.12%] [G loss: 7.170000]\n",
      "epoch:33 step:26391 [D loss: 0.013665, acc.: 99.22%] [G loss: 7.746750]\n",
      "epoch:33 step:26392 [D loss: 1.388126, acc.: 53.91%] [G loss: 3.007108]\n",
      "epoch:33 step:26393 [D loss: 0.019163, acc.: 100.00%] [G loss: 2.977993]\n",
      "epoch:33 step:26394 [D loss: 0.403109, acc.: 79.69%] [G loss: 6.001545]\n",
      "epoch:33 step:26395 [D loss: 0.023773, acc.: 98.44%] [G loss: 6.725872]\n",
      "epoch:33 step:26396 [D loss: 0.685805, acc.: 67.19%] [G loss: 3.940621]\n",
      "epoch:33 step:26397 [D loss: 0.107813, acc.: 96.09%] [G loss: 3.121089]\n",
      "epoch:33 step:26398 [D loss: 0.020774, acc.: 100.00%] [G loss: 3.505220]\n",
      "epoch:33 step:26399 [D loss: 0.011961, acc.: 100.00%] [G loss: 2.746772]\n",
      "epoch:33 step:26400 [D loss: 0.030268, acc.: 100.00%] [G loss: 2.845445]\n",
      "##############\n",
      "[0.93256903 0.92050736 0.81467124 1.0587108  0.96304452 0.95230438\n",
      " 1.0976914  1.10795008 2.11108675 1.02527485]\n",
      "##########\n",
      "epoch:33 step:26401 [D loss: 0.019697, acc.: 100.00%] [G loss: 2.279401]\n",
      "epoch:33 step:26402 [D loss: 0.022030, acc.: 100.00%] [G loss: 2.154956]\n",
      "epoch:33 step:26403 [D loss: 0.024701, acc.: 100.00%] [G loss: 3.006860]\n",
      "epoch:33 step:26404 [D loss: 0.096976, acc.: 100.00%] [G loss: 1.777836]\n",
      "epoch:33 step:26405 [D loss: 0.009989, acc.: 100.00%] [G loss: 1.397283]\n",
      "epoch:33 step:26406 [D loss: 0.123567, acc.: 96.88%] [G loss: 2.674570]\n",
      "epoch:33 step:26407 [D loss: 0.018369, acc.: 100.00%] [G loss: 3.217932]\n",
      "epoch:33 step:26408 [D loss: 0.065267, acc.: 98.44%] [G loss: 1.787592]\n",
      "epoch:33 step:26409 [D loss: 0.031953, acc.: 100.00%] [G loss: 2.180569]\n",
      "epoch:33 step:26410 [D loss: 0.033177, acc.: 99.22%] [G loss: 1.890387]\n",
      "epoch:33 step:26411 [D loss: 0.014879, acc.: 100.00%] [G loss: 0.956497]\n",
      "epoch:33 step:26412 [D loss: 0.052932, acc.: 99.22%] [G loss: 0.418080]\n",
      "epoch:33 step:26413 [D loss: 0.934207, acc.: 59.38%] [G loss: 7.789393]\n",
      "epoch:33 step:26414 [D loss: 3.032442, acc.: 50.00%] [G loss: 6.575778]\n",
      "epoch:33 step:26415 [D loss: 2.319675, acc.: 50.00%] [G loss: 4.747902]\n",
      "epoch:33 step:26416 [D loss: 1.593299, acc.: 50.78%] [G loss: 3.189813]\n",
      "epoch:33 step:26417 [D loss: 0.839625, acc.: 60.94%] [G loss: 2.200691]\n",
      "epoch:33 step:26418 [D loss: 0.343924, acc.: 84.38%] [G loss: 2.010046]\n",
      "epoch:33 step:26419 [D loss: 0.259875, acc.: 89.84%] [G loss: 1.690856]\n",
      "epoch:33 step:26420 [D loss: 0.212126, acc.: 96.09%] [G loss: 2.157498]\n",
      "epoch:33 step:26421 [D loss: 0.099732, acc.: 99.22%] [G loss: 2.359349]\n",
      "epoch:33 step:26422 [D loss: 0.168454, acc.: 94.53%] [G loss: 2.292631]\n",
      "epoch:33 step:26423 [D loss: 0.113584, acc.: 99.22%] [G loss: 2.365013]\n",
      "epoch:33 step:26424 [D loss: 0.104685, acc.: 100.00%] [G loss: 2.469189]\n",
      "epoch:33 step:26425 [D loss: 0.091542, acc.: 99.22%] [G loss: 2.014578]\n",
      "epoch:33 step:26426 [D loss: 0.138895, acc.: 97.66%] [G loss: 2.796900]\n",
      "epoch:33 step:26427 [D loss: 0.085567, acc.: 99.22%] [G loss: 3.088824]\n",
      "epoch:33 step:26428 [D loss: 0.079104, acc.: 100.00%] [G loss: 2.637334]\n",
      "epoch:33 step:26429 [D loss: 0.148119, acc.: 96.88%] [G loss: 2.465124]\n",
      "epoch:33 step:26430 [D loss: 0.058183, acc.: 99.22%] [G loss: 2.510706]\n",
      "epoch:33 step:26431 [D loss: 0.060716, acc.: 100.00%] [G loss: 3.226276]\n",
      "epoch:33 step:26432 [D loss: 0.092124, acc.: 97.66%] [G loss: 3.122647]\n",
      "epoch:33 step:26433 [D loss: 0.066584, acc.: 98.44%] [G loss: 3.057988]\n",
      "epoch:33 step:26434 [D loss: 0.045950, acc.: 100.00%] [G loss: 3.133698]\n",
      "epoch:33 step:26435 [D loss: 0.104261, acc.: 96.88%] [G loss: 2.408844]\n",
      "epoch:33 step:26436 [D loss: 0.100099, acc.: 99.22%] [G loss: 2.350807]\n",
      "epoch:33 step:26437 [D loss: 0.135594, acc.: 99.22%] [G loss: 3.588437]\n",
      "epoch:33 step:26438 [D loss: 0.034198, acc.: 100.00%] [G loss: 3.990742]\n",
      "epoch:33 step:26439 [D loss: 0.025932, acc.: 100.00%] [G loss: 2.817606]\n",
      "epoch:33 step:26440 [D loss: 0.047632, acc.: 100.00%] [G loss: 3.042960]\n",
      "epoch:33 step:26441 [D loss: 0.064273, acc.: 98.44%] [G loss: 2.336885]\n",
      "epoch:33 step:26442 [D loss: 0.057459, acc.: 98.44%] [G loss: 2.383172]\n",
      "epoch:33 step:26443 [D loss: 0.023286, acc.: 100.00%] [G loss: 1.867525]\n",
      "epoch:33 step:26444 [D loss: 0.057365, acc.: 99.22%] [G loss: 1.642692]\n",
      "epoch:33 step:26445 [D loss: 0.107075, acc.: 96.88%] [G loss: 2.687984]\n",
      "epoch:33 step:26446 [D loss: 0.192581, acc.: 96.88%] [G loss: 3.730287]\n",
      "epoch:33 step:26447 [D loss: 0.108944, acc.: 96.88%] [G loss: 3.694429]\n",
      "epoch:33 step:26448 [D loss: 0.053930, acc.: 99.22%] [G loss: 2.838533]\n",
      "epoch:33 step:26449 [D loss: 0.025866, acc.: 100.00%] [G loss: 3.798380]\n",
      "epoch:33 step:26450 [D loss: 0.015663, acc.: 100.00%] [G loss: 3.665746]\n",
      "epoch:33 step:26451 [D loss: 0.352954, acc.: 84.38%] [G loss: 4.878235]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:33 step:26452 [D loss: 0.178922, acc.: 89.84%] [G loss: 4.962502]\n",
      "epoch:33 step:26453 [D loss: 0.030257, acc.: 100.00%] [G loss: 4.814508]\n",
      "epoch:33 step:26454 [D loss: 0.035046, acc.: 99.22%] [G loss: 4.439533]\n",
      "epoch:33 step:26455 [D loss: 0.014062, acc.: 100.00%] [G loss: 3.644909]\n",
      "epoch:33 step:26456 [D loss: 0.013086, acc.: 100.00%] [G loss: 4.225655]\n",
      "epoch:33 step:26457 [D loss: 0.011883, acc.: 100.00%] [G loss: 3.774682]\n",
      "epoch:33 step:26458 [D loss: 0.058086, acc.: 99.22%] [G loss: 3.333998]\n",
      "epoch:33 step:26459 [D loss: 0.059668, acc.: 97.66%] [G loss: 4.144948]\n",
      "epoch:33 step:26460 [D loss: 0.016395, acc.: 100.00%] [G loss: 4.141471]\n",
      "epoch:33 step:26461 [D loss: 0.025410, acc.: 100.00%] [G loss: 4.211882]\n",
      "epoch:33 step:26462 [D loss: 0.055883, acc.: 97.66%] [G loss: 3.405118]\n",
      "epoch:33 step:26463 [D loss: 0.062329, acc.: 99.22%] [G loss: 4.360399]\n",
      "epoch:33 step:26464 [D loss: 0.021231, acc.: 100.00%] [G loss: 3.894264]\n",
      "epoch:33 step:26465 [D loss: 0.040410, acc.: 100.00%] [G loss: 4.069706]\n",
      "epoch:33 step:26466 [D loss: 0.023451, acc.: 100.00%] [G loss: 4.473284]\n",
      "epoch:33 step:26467 [D loss: 0.019714, acc.: 99.22%] [G loss: 4.220604]\n",
      "epoch:33 step:26468 [D loss: 0.018870, acc.: 100.00%] [G loss: 4.373065]\n",
      "epoch:33 step:26469 [D loss: 0.077480, acc.: 98.44%] [G loss: 3.665977]\n",
      "epoch:33 step:26470 [D loss: 0.027529, acc.: 100.00%] [G loss: 3.729436]\n",
      "epoch:33 step:26471 [D loss: 0.028015, acc.: 100.00%] [G loss: 3.942621]\n",
      "epoch:33 step:26472 [D loss: 0.041546, acc.: 98.44%] [G loss: 4.732588]\n",
      "epoch:33 step:26473 [D loss: 0.051962, acc.: 98.44%] [G loss: 4.032345]\n",
      "epoch:33 step:26474 [D loss: 0.052982, acc.: 100.00%] [G loss: 4.452626]\n",
      "epoch:33 step:26475 [D loss: 0.027340, acc.: 100.00%] [G loss: 4.891839]\n",
      "epoch:33 step:26476 [D loss: 0.077767, acc.: 98.44%] [G loss: 4.092546]\n",
      "epoch:33 step:26477 [D loss: 0.039534, acc.: 99.22%] [G loss: 4.627901]\n",
      "epoch:33 step:26478 [D loss: 0.024427, acc.: 100.00%] [G loss: 5.027442]\n",
      "epoch:33 step:26479 [D loss: 0.007920, acc.: 100.00%] [G loss: 4.617549]\n",
      "epoch:33 step:26480 [D loss: 0.017624, acc.: 100.00%] [G loss: 4.615286]\n",
      "epoch:33 step:26481 [D loss: 0.026025, acc.: 100.00%] [G loss: 4.494023]\n",
      "epoch:33 step:26482 [D loss: 0.015550, acc.: 100.00%] [G loss: 5.015717]\n",
      "epoch:33 step:26483 [D loss: 0.053643, acc.: 99.22%] [G loss: 3.181012]\n",
      "epoch:33 step:26484 [D loss: 0.233978, acc.: 89.84%] [G loss: 7.076991]\n",
      "epoch:33 step:26485 [D loss: 0.221788, acc.: 89.84%] [G loss: 5.344693]\n",
      "epoch:33 step:26486 [D loss: 0.117149, acc.: 96.88%] [G loss: 5.246465]\n",
      "epoch:33 step:26487 [D loss: 0.018686, acc.: 100.00%] [G loss: 5.925790]\n",
      "epoch:33 step:26488 [D loss: 0.064429, acc.: 96.88%] [G loss: 3.694201]\n",
      "epoch:33 step:26489 [D loss: 0.028799, acc.: 99.22%] [G loss: 4.056132]\n",
      "epoch:33 step:26490 [D loss: 0.017298, acc.: 100.00%] [G loss: 4.429951]\n",
      "epoch:33 step:26491 [D loss: 0.007858, acc.: 100.00%] [G loss: 4.626348]\n",
      "epoch:33 step:26492 [D loss: 0.014043, acc.: 100.00%] [G loss: 5.191268]\n",
      "epoch:33 step:26493 [D loss: 0.011582, acc.: 100.00%] [G loss: 4.661681]\n",
      "epoch:33 step:26494 [D loss: 0.011845, acc.: 100.00%] [G loss: 5.182781]\n",
      "epoch:33 step:26495 [D loss: 0.011295, acc.: 100.00%] [G loss: 4.333015]\n",
      "epoch:33 step:26496 [D loss: 0.006016, acc.: 100.00%] [G loss: 4.524981]\n",
      "epoch:33 step:26497 [D loss: 0.038888, acc.: 100.00%] [G loss: 5.099539]\n",
      "epoch:33 step:26498 [D loss: 0.231089, acc.: 89.06%] [G loss: 5.886662]\n",
      "epoch:33 step:26499 [D loss: 0.007715, acc.: 100.00%] [G loss: 5.103707]\n",
      "epoch:33 step:26500 [D loss: 0.061026, acc.: 97.66%] [G loss: 3.958889]\n",
      "epoch:33 step:26501 [D loss: 0.042206, acc.: 99.22%] [G loss: 4.544691]\n",
      "epoch:33 step:26502 [D loss: 0.006521, acc.: 100.00%] [G loss: 5.837696]\n",
      "epoch:33 step:26503 [D loss: 0.007924, acc.: 100.00%] [G loss: 5.194291]\n",
      "epoch:33 step:26504 [D loss: 0.030288, acc.: 100.00%] [G loss: 5.261977]\n",
      "epoch:33 step:26505 [D loss: 0.014251, acc.: 100.00%] [G loss: 4.934402]\n",
      "epoch:33 step:26506 [D loss: 0.014394, acc.: 100.00%] [G loss: 4.446139]\n",
      "epoch:33 step:26507 [D loss: 0.005613, acc.: 100.00%] [G loss: 4.463215]\n",
      "epoch:33 step:26508 [D loss: 0.060851, acc.: 98.44%] [G loss: 4.285164]\n",
      "epoch:33 step:26509 [D loss: 0.075152, acc.: 98.44%] [G loss: 4.293750]\n",
      "epoch:33 step:26510 [D loss: 0.023902, acc.: 100.00%] [G loss: 4.081310]\n",
      "epoch:33 step:26511 [D loss: 0.009716, acc.: 100.00%] [G loss: 3.802510]\n",
      "epoch:33 step:26512 [D loss: 0.024782, acc.: 100.00%] [G loss: 3.170577]\n",
      "epoch:33 step:26513 [D loss: 0.375899, acc.: 82.03%] [G loss: 8.584553]\n",
      "epoch:33 step:26514 [D loss: 2.252557, acc.: 50.78%] [G loss: 3.395965]\n",
      "epoch:33 step:26515 [D loss: 0.396076, acc.: 85.16%] [G loss: 5.653742]\n",
      "epoch:33 step:26516 [D loss: 0.023898, acc.: 100.00%] [G loss: 6.186134]\n",
      "epoch:33 step:26517 [D loss: 0.184336, acc.: 92.97%] [G loss: 4.153570]\n",
      "epoch:33 step:26518 [D loss: 0.026319, acc.: 100.00%] [G loss: 4.376066]\n",
      "epoch:33 step:26519 [D loss: 0.061093, acc.: 99.22%] [G loss: 4.676269]\n",
      "epoch:33 step:26520 [D loss: 0.037153, acc.: 98.44%] [G loss: 4.832211]\n",
      "epoch:33 step:26521 [D loss: 0.025872, acc.: 100.00%] [G loss: 4.545994]\n",
      "epoch:33 step:26522 [D loss: 0.008813, acc.: 100.00%] [G loss: 4.285694]\n",
      "epoch:33 step:26523 [D loss: 0.013329, acc.: 100.00%] [G loss: 5.240379]\n",
      "epoch:33 step:26524 [D loss: 0.007923, acc.: 100.00%] [G loss: 4.466176]\n",
      "epoch:33 step:26525 [D loss: 0.014647, acc.: 100.00%] [G loss: 4.622124]\n",
      "epoch:33 step:26526 [D loss: 0.050978, acc.: 100.00%] [G loss: 3.282912]\n",
      "epoch:33 step:26527 [D loss: 0.050387, acc.: 100.00%] [G loss: 4.132820]\n",
      "epoch:33 step:26528 [D loss: 0.023142, acc.: 100.00%] [G loss: 4.154487]\n",
      "epoch:33 step:26529 [D loss: 0.011872, acc.: 100.00%] [G loss: 4.528160]\n",
      "epoch:33 step:26530 [D loss: 0.043791, acc.: 98.44%] [G loss: 3.834842]\n",
      "epoch:33 step:26531 [D loss: 0.029458, acc.: 100.00%] [G loss: 4.424184]\n",
      "epoch:33 step:26532 [D loss: 0.025685, acc.: 100.00%] [G loss: 4.991212]\n",
      "epoch:33 step:26533 [D loss: 0.006049, acc.: 100.00%] [G loss: 4.375657]\n",
      "epoch:33 step:26534 [D loss: 0.021898, acc.: 100.00%] [G loss: 4.255552]\n",
      "epoch:33 step:26535 [D loss: 0.168892, acc.: 95.31%] [G loss: 4.072687]\n",
      "epoch:33 step:26536 [D loss: 0.006151, acc.: 100.00%] [G loss: 4.620180]\n",
      "epoch:33 step:26537 [D loss: 0.003706, acc.: 100.00%] [G loss: 4.760713]\n",
      "epoch:33 step:26538 [D loss: 0.014825, acc.: 100.00%] [G loss: 3.882164]\n",
      "epoch:33 step:26539 [D loss: 0.029627, acc.: 100.00%] [G loss: 4.137923]\n",
      "epoch:33 step:26540 [D loss: 0.017947, acc.: 100.00%] [G loss: 4.576630]\n",
      "epoch:33 step:26541 [D loss: 0.071275, acc.: 99.22%] [G loss: 4.740223]\n",
      "epoch:33 step:26542 [D loss: 0.002773, acc.: 100.00%] [G loss: 4.772127]\n",
      "epoch:33 step:26543 [D loss: 0.055462, acc.: 98.44%] [G loss: 4.407216]\n",
      "epoch:33 step:26544 [D loss: 0.029439, acc.: 100.00%] [G loss: 3.393741]\n",
      "epoch:33 step:26545 [D loss: 0.019093, acc.: 100.00%] [G loss: 4.693767]\n",
      "epoch:33 step:26546 [D loss: 0.029207, acc.: 99.22%] [G loss: 5.186720]\n",
      "epoch:33 step:26547 [D loss: 0.004064, acc.: 100.00%] [G loss: 6.066550]\n",
      "epoch:33 step:26548 [D loss: 0.005386, acc.: 100.00%] [G loss: 6.173897]\n",
      "epoch:33 step:26549 [D loss: 0.026380, acc.: 99.22%] [G loss: 5.336711]\n",
      "epoch:33 step:26550 [D loss: 0.022416, acc.: 100.00%] [G loss: 4.176287]\n",
      "epoch:33 step:26551 [D loss: 0.012326, acc.: 100.00%] [G loss: 4.628832]\n",
      "epoch:33 step:26552 [D loss: 0.034555, acc.: 100.00%] [G loss: 5.715178]\n",
      "epoch:33 step:26553 [D loss: 0.005393, acc.: 100.00%] [G loss: 5.149679]\n",
      "epoch:33 step:26554 [D loss: 0.007105, acc.: 100.00%] [G loss: 6.109640]\n",
      "epoch:34 step:26555 [D loss: 0.007694, acc.: 100.00%] [G loss: 5.279236]\n",
      "epoch:34 step:26556 [D loss: 0.020496, acc.: 100.00%] [G loss: 3.937584]\n",
      "epoch:34 step:26557 [D loss: 0.008150, acc.: 100.00%] [G loss: 4.755110]\n",
      "epoch:34 step:26558 [D loss: 0.010404, acc.: 100.00%] [G loss: 4.063604]\n",
      "epoch:34 step:26559 [D loss: 0.017366, acc.: 100.00%] [G loss: 4.131035]\n",
      "epoch:34 step:26560 [D loss: 0.003591, acc.: 100.00%] [G loss: 3.994311]\n",
      "epoch:34 step:26561 [D loss: 0.016646, acc.: 100.00%] [G loss: 3.865262]\n",
      "epoch:34 step:26562 [D loss: 0.018584, acc.: 100.00%] [G loss: 4.607795]\n",
      "epoch:34 step:26563 [D loss: 0.061233, acc.: 99.22%] [G loss: 3.670688]\n",
      "epoch:34 step:26564 [D loss: 0.012694, acc.: 100.00%] [G loss: 4.814287]\n",
      "epoch:34 step:26565 [D loss: 0.003866, acc.: 100.00%] [G loss: 4.833755]\n",
      "epoch:34 step:26566 [D loss: 0.040119, acc.: 99.22%] [G loss: 3.203962]\n",
      "epoch:34 step:26567 [D loss: 0.043110, acc.: 100.00%] [G loss: 4.099655]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:34 step:26568 [D loss: 0.005827, acc.: 100.00%] [G loss: 5.249350]\n",
      "epoch:34 step:26569 [D loss: 0.060410, acc.: 99.22%] [G loss: 3.543046]\n",
      "epoch:34 step:26570 [D loss: 0.102667, acc.: 95.31%] [G loss: 6.986938]\n",
      "epoch:34 step:26571 [D loss: 0.006716, acc.: 100.00%] [G loss: 7.718228]\n",
      "epoch:34 step:26572 [D loss: 0.005737, acc.: 100.00%] [G loss: 7.174283]\n",
      "epoch:34 step:26573 [D loss: 0.023040, acc.: 98.44%] [G loss: 6.055023]\n",
      "epoch:34 step:26574 [D loss: 0.002883, acc.: 100.00%] [G loss: 5.475587]\n",
      "epoch:34 step:26575 [D loss: 0.010594, acc.: 99.22%] [G loss: 4.728195]\n",
      "epoch:34 step:26576 [D loss: 0.003210, acc.: 100.00%] [G loss: 5.345604]\n",
      "epoch:34 step:26577 [D loss: 0.015677, acc.: 100.00%] [G loss: 5.957712]\n",
      "epoch:34 step:26578 [D loss: 0.016689, acc.: 100.00%] [G loss: 5.560454]\n",
      "epoch:34 step:26579 [D loss: 0.031275, acc.: 100.00%] [G loss: 5.409291]\n",
      "epoch:34 step:26580 [D loss: 0.006512, acc.: 100.00%] [G loss: 4.703716]\n",
      "epoch:34 step:26581 [D loss: 0.017563, acc.: 100.00%] [G loss: 4.500541]\n",
      "epoch:34 step:26582 [D loss: 0.020415, acc.: 100.00%] [G loss: 5.700035]\n",
      "epoch:34 step:26583 [D loss: 0.005769, acc.: 100.00%] [G loss: 6.074904]\n",
      "epoch:34 step:26584 [D loss: 0.017765, acc.: 100.00%] [G loss: 5.846219]\n",
      "epoch:34 step:26585 [D loss: 0.003054, acc.: 100.00%] [G loss: 5.211648]\n",
      "epoch:34 step:26586 [D loss: 0.005122, acc.: 100.00%] [G loss: 5.934159]\n",
      "epoch:34 step:26587 [D loss: 0.018388, acc.: 100.00%] [G loss: 4.693631]\n",
      "epoch:34 step:26588 [D loss: 0.030641, acc.: 100.00%] [G loss: 6.273384]\n",
      "epoch:34 step:26589 [D loss: 0.004599, acc.: 100.00%] [G loss: 5.918633]\n",
      "epoch:34 step:26590 [D loss: 1.329214, acc.: 39.84%] [G loss: 9.750259]\n",
      "epoch:34 step:26591 [D loss: 0.000421, acc.: 100.00%] [G loss: 11.634840]\n",
      "epoch:34 step:26592 [D loss: 1.679985, acc.: 54.69%] [G loss: 8.497374]\n",
      "epoch:34 step:26593 [D loss: 0.000546, acc.: 100.00%] [G loss: 6.921204]\n",
      "epoch:34 step:26594 [D loss: 0.007153, acc.: 100.00%] [G loss: 6.766203]\n",
      "epoch:34 step:26595 [D loss: 0.022342, acc.: 100.00%] [G loss: 6.148863]\n",
      "epoch:34 step:26596 [D loss: 0.003830, acc.: 100.00%] [G loss: 6.603749]\n",
      "epoch:34 step:26597 [D loss: 0.007635, acc.: 100.00%] [G loss: 6.419937]\n",
      "epoch:34 step:26598 [D loss: 0.005664, acc.: 100.00%] [G loss: 4.951245]\n",
      "epoch:34 step:26599 [D loss: 0.015887, acc.: 99.22%] [G loss: 4.916513]\n",
      "epoch:34 step:26600 [D loss: 0.057395, acc.: 99.22%] [G loss: 5.997758]\n",
      "##############\n",
      "[1.02602382 0.87703115 0.98139259 1.08868831 1.08088655 0.99931927\n",
      " 1.09489734 2.1052552  2.11825215 1.02698712]\n",
      "##########\n",
      "epoch:34 step:26601 [D loss: 0.001227, acc.: 100.00%] [G loss: 6.669713]\n",
      "epoch:34 step:26602 [D loss: 0.000929, acc.: 100.00%] [G loss: 6.259969]\n",
      "epoch:34 step:26603 [D loss: 0.047311, acc.: 98.44%] [G loss: 3.885578]\n",
      "epoch:34 step:26604 [D loss: 0.163348, acc.: 94.53%] [G loss: 8.595382]\n",
      "epoch:34 step:26605 [D loss: 0.141644, acc.: 93.75%] [G loss: 7.337288]\n",
      "epoch:34 step:26606 [D loss: 0.001906, acc.: 100.00%] [G loss: 5.354321]\n",
      "epoch:34 step:26607 [D loss: 0.004468, acc.: 100.00%] [G loss: 4.321098]\n",
      "epoch:34 step:26608 [D loss: 0.313525, acc.: 85.16%] [G loss: 10.236995]\n",
      "epoch:34 step:26609 [D loss: 1.268373, acc.: 48.44%] [G loss: 6.646473]\n",
      "epoch:34 step:26610 [D loss: 0.006074, acc.: 100.00%] [G loss: 6.960540]\n",
      "epoch:34 step:26611 [D loss: 0.002661, acc.: 100.00%] [G loss: 6.723999]\n",
      "epoch:34 step:26612 [D loss: 0.037214, acc.: 98.44%] [G loss: 4.988827]\n",
      "epoch:34 step:26613 [D loss: 0.005073, acc.: 100.00%] [G loss: 4.624018]\n",
      "epoch:34 step:26614 [D loss: 0.022918, acc.: 100.00%] [G loss: 3.139173]\n",
      "epoch:34 step:26615 [D loss: 0.307994, acc.: 84.38%] [G loss: 7.613941]\n",
      "epoch:34 step:26616 [D loss: 0.266789, acc.: 87.50%] [G loss: 6.414469]\n",
      "epoch:34 step:26617 [D loss: 0.043657, acc.: 99.22%] [G loss: 4.868026]\n",
      "epoch:34 step:26618 [D loss: 0.096303, acc.: 96.88%] [G loss: 5.855347]\n",
      "epoch:34 step:26619 [D loss: 0.059929, acc.: 98.44%] [G loss: 4.285274]\n",
      "epoch:34 step:26620 [D loss: 0.027910, acc.: 100.00%] [G loss: 4.929700]\n",
      "epoch:34 step:26621 [D loss: 0.007481, acc.: 100.00%] [G loss: 4.229414]\n",
      "epoch:34 step:26622 [D loss: 0.003818, acc.: 100.00%] [G loss: 3.959865]\n",
      "epoch:34 step:26623 [D loss: 0.021026, acc.: 100.00%] [G loss: 3.421581]\n",
      "epoch:34 step:26624 [D loss: 0.081378, acc.: 100.00%] [G loss: 5.601456]\n",
      "epoch:34 step:26625 [D loss: 0.271698, acc.: 88.28%] [G loss: 3.447841]\n",
      "epoch:34 step:26626 [D loss: 0.004140, acc.: 100.00%] [G loss: 3.267469]\n",
      "epoch:34 step:26627 [D loss: 0.062693, acc.: 99.22%] [G loss: 4.573294]\n",
      "epoch:34 step:26628 [D loss: 0.005551, acc.: 100.00%] [G loss: 5.007935]\n",
      "epoch:34 step:26629 [D loss: 0.005367, acc.: 100.00%] [G loss: 3.917025]\n",
      "epoch:34 step:26630 [D loss: 0.037986, acc.: 99.22%] [G loss: 4.370712]\n",
      "epoch:34 step:26631 [D loss: 0.174551, acc.: 92.19%] [G loss: 3.427276]\n",
      "epoch:34 step:26632 [D loss: 0.008933, acc.: 100.00%] [G loss: 4.555392]\n",
      "epoch:34 step:26633 [D loss: 0.021482, acc.: 100.00%] [G loss: 4.627851]\n",
      "epoch:34 step:26634 [D loss: 0.026863, acc.: 100.00%] [G loss: 4.288268]\n",
      "epoch:34 step:26635 [D loss: 0.027358, acc.: 100.00%] [G loss: 4.896487]\n",
      "epoch:34 step:26636 [D loss: 0.010633, acc.: 100.00%] [G loss: 5.251600]\n",
      "epoch:34 step:26637 [D loss: 0.015372, acc.: 100.00%] [G loss: 3.679124]\n",
      "epoch:34 step:26638 [D loss: 0.020828, acc.: 100.00%] [G loss: 3.863912]\n",
      "epoch:34 step:26639 [D loss: 0.418890, acc.: 77.34%] [G loss: 7.529721]\n",
      "epoch:34 step:26640 [D loss: 0.262674, acc.: 89.84%] [G loss: 5.976887]\n",
      "epoch:34 step:26641 [D loss: 0.002466, acc.: 100.00%] [G loss: 5.453700]\n",
      "epoch:34 step:26642 [D loss: 0.014597, acc.: 100.00%] [G loss: 6.432554]\n",
      "epoch:34 step:26643 [D loss: 0.003251, acc.: 100.00%] [G loss: 4.313138]\n",
      "epoch:34 step:26644 [D loss: 1.698948, acc.: 46.09%] [G loss: 9.737563]\n",
      "epoch:34 step:26645 [D loss: 2.095751, acc.: 53.12%] [G loss: 7.981622]\n",
      "epoch:34 step:26646 [D loss: 0.956138, acc.: 71.09%] [G loss: 5.741203]\n",
      "epoch:34 step:26647 [D loss: 0.095876, acc.: 96.88%] [G loss: 3.850214]\n",
      "epoch:34 step:26648 [D loss: 0.038580, acc.: 98.44%] [G loss: 4.913840]\n",
      "epoch:34 step:26649 [D loss: 0.028936, acc.: 98.44%] [G loss: 5.317102]\n",
      "epoch:34 step:26650 [D loss: 0.095121, acc.: 96.09%] [G loss: 4.922074]\n",
      "epoch:34 step:26651 [D loss: 0.011386, acc.: 100.00%] [G loss: 4.202168]\n",
      "epoch:34 step:26652 [D loss: 0.133230, acc.: 94.53%] [G loss: 4.084783]\n",
      "epoch:34 step:26653 [D loss: 0.017205, acc.: 100.00%] [G loss: 3.105951]\n",
      "epoch:34 step:26654 [D loss: 0.057270, acc.: 99.22%] [G loss: 4.269267]\n",
      "epoch:34 step:26655 [D loss: 0.007130, acc.: 100.00%] [G loss: 3.585818]\n",
      "epoch:34 step:26656 [D loss: 0.059388, acc.: 99.22%] [G loss: 3.966511]\n",
      "epoch:34 step:26657 [D loss: 0.133784, acc.: 95.31%] [G loss: 2.854821]\n",
      "epoch:34 step:26658 [D loss: 0.060063, acc.: 99.22%] [G loss: 5.208825]\n",
      "epoch:34 step:26659 [D loss: 0.010386, acc.: 100.00%] [G loss: 5.083293]\n",
      "epoch:34 step:26660 [D loss: 0.060474, acc.: 98.44%] [G loss: 5.288651]\n",
      "epoch:34 step:26661 [D loss: 0.192752, acc.: 90.62%] [G loss: 4.878834]\n",
      "epoch:34 step:26662 [D loss: 0.011036, acc.: 100.00%] [G loss: 5.026091]\n",
      "epoch:34 step:26663 [D loss: 0.004187, acc.: 100.00%] [G loss: 5.682441]\n",
      "epoch:34 step:26664 [D loss: 0.062772, acc.: 98.44%] [G loss: 4.542162]\n",
      "epoch:34 step:26665 [D loss: 0.017335, acc.: 100.00%] [G loss: 3.788206]\n",
      "epoch:34 step:26666 [D loss: 0.013316, acc.: 100.00%] [G loss: 4.352018]\n",
      "epoch:34 step:26667 [D loss: 0.012339, acc.: 100.00%] [G loss: 4.233168]\n",
      "epoch:34 step:26668 [D loss: 0.013640, acc.: 100.00%] [G loss: 4.899259]\n",
      "epoch:34 step:26669 [D loss: 0.019828, acc.: 100.00%] [G loss: 4.014565]\n",
      "epoch:34 step:26670 [D loss: 0.157385, acc.: 97.66%] [G loss: 4.457082]\n",
      "epoch:34 step:26671 [D loss: 0.006467, acc.: 100.00%] [G loss: 5.344468]\n",
      "epoch:34 step:26672 [D loss: 0.005231, acc.: 100.00%] [G loss: 5.504250]\n",
      "epoch:34 step:26673 [D loss: 0.007415, acc.: 100.00%] [G loss: 5.518373]\n",
      "epoch:34 step:26674 [D loss: 0.042039, acc.: 97.66%] [G loss: 5.439268]\n",
      "epoch:34 step:26675 [D loss: 0.018321, acc.: 100.00%] [G loss: 5.349673]\n",
      "epoch:34 step:26676 [D loss: 0.010789, acc.: 99.22%] [G loss: 5.082298]\n",
      "epoch:34 step:26677 [D loss: 0.024629, acc.: 100.00%] [G loss: 5.773905]\n",
      "epoch:34 step:26678 [D loss: 0.008091, acc.: 100.00%] [G loss: 4.924465]\n",
      "epoch:34 step:26679 [D loss: 0.017534, acc.: 100.00%] [G loss: 5.632164]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:34 step:26680 [D loss: 0.004822, acc.: 100.00%] [G loss: 5.115869]\n",
      "epoch:34 step:26681 [D loss: 0.018153, acc.: 100.00%] [G loss: 4.634904]\n",
      "epoch:34 step:26682 [D loss: 0.065261, acc.: 98.44%] [G loss: 3.885437]\n",
      "epoch:34 step:26683 [D loss: 0.010960, acc.: 100.00%] [G loss: 4.011142]\n",
      "epoch:34 step:26684 [D loss: 0.008170, acc.: 100.00%] [G loss: 2.872166]\n",
      "epoch:34 step:26685 [D loss: 0.018084, acc.: 100.00%] [G loss: 4.108504]\n",
      "epoch:34 step:26686 [D loss: 0.901225, acc.: 58.59%] [G loss: 8.674270]\n",
      "epoch:34 step:26687 [D loss: 1.750766, acc.: 51.56%] [G loss: 4.546705]\n",
      "epoch:34 step:26688 [D loss: 0.249494, acc.: 88.28%] [G loss: 4.908998]\n",
      "epoch:34 step:26689 [D loss: 0.008744, acc.: 100.00%] [G loss: 6.173039]\n",
      "epoch:34 step:26690 [D loss: 0.285078, acc.: 87.50%] [G loss: 3.752333]\n",
      "epoch:34 step:26691 [D loss: 0.523430, acc.: 80.47%] [G loss: 7.481153]\n",
      "epoch:34 step:26692 [D loss: 0.086724, acc.: 96.88%] [G loss: 7.916468]\n",
      "epoch:34 step:26693 [D loss: 0.324483, acc.: 85.94%] [G loss: 6.776507]\n",
      "epoch:34 step:26694 [D loss: 0.018038, acc.: 100.00%] [G loss: 5.753700]\n",
      "epoch:34 step:26695 [D loss: 0.003934, acc.: 100.00%] [G loss: 4.519626]\n",
      "epoch:34 step:26696 [D loss: 0.029933, acc.: 100.00%] [G loss: 4.967938]\n",
      "epoch:34 step:26697 [D loss: 0.005657, acc.: 100.00%] [G loss: 4.837721]\n",
      "epoch:34 step:26698 [D loss: 0.015024, acc.: 100.00%] [G loss: 3.756205]\n",
      "epoch:34 step:26699 [D loss: 0.041468, acc.: 99.22%] [G loss: 4.073459]\n",
      "epoch:34 step:26700 [D loss: 0.035774, acc.: 99.22%] [G loss: 4.412447]\n",
      "epoch:34 step:26701 [D loss: 0.013541, acc.: 100.00%] [G loss: 5.362341]\n",
      "epoch:34 step:26702 [D loss: 0.009608, acc.: 100.00%] [G loss: 5.111422]\n",
      "epoch:34 step:26703 [D loss: 0.090719, acc.: 96.88%] [G loss: 3.860285]\n",
      "epoch:34 step:26704 [D loss: 0.036176, acc.: 99.22%] [G loss: 3.046013]\n",
      "epoch:34 step:26705 [D loss: 0.119466, acc.: 96.09%] [G loss: 5.892451]\n",
      "epoch:34 step:26706 [D loss: 0.589549, acc.: 75.00%] [G loss: 0.724193]\n",
      "epoch:34 step:26707 [D loss: 0.167283, acc.: 92.97%] [G loss: 2.909958]\n",
      "epoch:34 step:26708 [D loss: 0.002330, acc.: 100.00%] [G loss: 4.805824]\n",
      "epoch:34 step:26709 [D loss: 0.008722, acc.: 100.00%] [G loss: 4.636411]\n",
      "epoch:34 step:26710 [D loss: 0.056426, acc.: 97.66%] [G loss: 3.756352]\n",
      "epoch:34 step:26711 [D loss: 0.039981, acc.: 98.44%] [G loss: 2.314806]\n",
      "epoch:34 step:26712 [D loss: 0.211308, acc.: 89.84%] [G loss: 5.375059]\n",
      "epoch:34 step:26713 [D loss: 0.061808, acc.: 98.44%] [G loss: 6.115185]\n",
      "epoch:34 step:26714 [D loss: 0.720111, acc.: 69.53%] [G loss: 3.113664]\n",
      "epoch:34 step:26715 [D loss: 0.045752, acc.: 99.22%] [G loss: 5.384834]\n",
      "epoch:34 step:26716 [D loss: 0.005767, acc.: 100.00%] [G loss: 5.714912]\n",
      "epoch:34 step:26717 [D loss: 0.042046, acc.: 100.00%] [G loss: 6.585626]\n",
      "epoch:34 step:26718 [D loss: 0.029351, acc.: 98.44%] [G loss: 5.654416]\n",
      "epoch:34 step:26719 [D loss: 0.021625, acc.: 100.00%] [G loss: 4.534190]\n",
      "epoch:34 step:26720 [D loss: 0.039595, acc.: 98.44%] [G loss: 3.384038]\n",
      "epoch:34 step:26721 [D loss: 0.014836, acc.: 100.00%] [G loss: 3.384903]\n",
      "epoch:34 step:26722 [D loss: 0.059533, acc.: 98.44%] [G loss: 5.502752]\n",
      "epoch:34 step:26723 [D loss: 0.013630, acc.: 99.22%] [G loss: 6.421208]\n",
      "epoch:34 step:26724 [D loss: 0.063985, acc.: 99.22%] [G loss: 4.239266]\n",
      "epoch:34 step:26725 [D loss: 0.008740, acc.: 100.00%] [G loss: 3.980444]\n",
      "epoch:34 step:26726 [D loss: 0.058031, acc.: 99.22%] [G loss: 5.006639]\n",
      "epoch:34 step:26727 [D loss: 0.004459, acc.: 100.00%] [G loss: 5.759933]\n",
      "epoch:34 step:26728 [D loss: 0.021693, acc.: 100.00%] [G loss: 5.054718]\n",
      "epoch:34 step:26729 [D loss: 0.003441, acc.: 100.00%] [G loss: 5.235560]\n",
      "epoch:34 step:26730 [D loss: 0.240152, acc.: 91.41%] [G loss: 5.285956]\n",
      "epoch:34 step:26731 [D loss: 0.009225, acc.: 100.00%] [G loss: 6.062554]\n",
      "epoch:34 step:26732 [D loss: 0.257695, acc.: 88.28%] [G loss: 2.303337]\n",
      "epoch:34 step:26733 [D loss: 0.810700, acc.: 60.94%] [G loss: 8.552903]\n",
      "epoch:34 step:26734 [D loss: 1.758977, acc.: 50.78%] [G loss: 5.691881]\n",
      "epoch:34 step:26735 [D loss: 0.565947, acc.: 71.09%] [G loss: 3.112162]\n",
      "epoch:34 step:26736 [D loss: 0.185940, acc.: 89.06%] [G loss: 4.545976]\n",
      "epoch:34 step:26737 [D loss: 0.013579, acc.: 100.00%] [G loss: 5.481019]\n",
      "epoch:34 step:26738 [D loss: 0.073847, acc.: 98.44%] [G loss: 4.700495]\n",
      "epoch:34 step:26739 [D loss: 0.058348, acc.: 98.44%] [G loss: 4.391990]\n",
      "epoch:34 step:26740 [D loss: 0.037518, acc.: 99.22%] [G loss: 3.559769]\n",
      "epoch:34 step:26741 [D loss: 0.116794, acc.: 96.88%] [G loss: 3.585318]\n",
      "epoch:34 step:26742 [D loss: 0.038213, acc.: 99.22%] [G loss: 4.073308]\n",
      "epoch:34 step:26743 [D loss: 0.079321, acc.: 96.88%] [G loss: 4.102064]\n",
      "epoch:34 step:26744 [D loss: 0.049182, acc.: 100.00%] [G loss: 3.995560]\n",
      "epoch:34 step:26745 [D loss: 0.038207, acc.: 100.00%] [G loss: 3.937531]\n",
      "epoch:34 step:26746 [D loss: 0.021988, acc.: 100.00%] [G loss: 3.772545]\n",
      "epoch:34 step:26747 [D loss: 0.245557, acc.: 91.41%] [G loss: 3.517672]\n",
      "epoch:34 step:26748 [D loss: 0.025059, acc.: 100.00%] [G loss: 4.259658]\n",
      "epoch:34 step:26749 [D loss: 0.030800, acc.: 100.00%] [G loss: 3.845403]\n",
      "epoch:34 step:26750 [D loss: 0.018548, acc.: 100.00%] [G loss: 4.270821]\n",
      "epoch:34 step:26751 [D loss: 0.065436, acc.: 98.44%] [G loss: 3.556513]\n",
      "epoch:34 step:26752 [D loss: 0.042909, acc.: 100.00%] [G loss: 3.975606]\n",
      "epoch:34 step:26753 [D loss: 0.010830, acc.: 100.00%] [G loss: 4.145426]\n",
      "epoch:34 step:26754 [D loss: 0.014978, acc.: 100.00%] [G loss: 4.283716]\n",
      "epoch:34 step:26755 [D loss: 0.014003, acc.: 100.00%] [G loss: 4.568294]\n",
      "epoch:34 step:26756 [D loss: 0.035340, acc.: 100.00%] [G loss: 4.235964]\n",
      "epoch:34 step:26757 [D loss: 0.014026, acc.: 100.00%] [G loss: 4.203682]\n",
      "epoch:34 step:26758 [D loss: 0.026506, acc.: 100.00%] [G loss: 4.455747]\n",
      "epoch:34 step:26759 [D loss: 0.046217, acc.: 99.22%] [G loss: 4.640059]\n",
      "epoch:34 step:26760 [D loss: 0.015786, acc.: 100.00%] [G loss: 4.381440]\n",
      "epoch:34 step:26761 [D loss: 0.010012, acc.: 100.00%] [G loss: 4.562861]\n",
      "epoch:34 step:26762 [D loss: 0.027854, acc.: 100.00%] [G loss: 4.192523]\n",
      "epoch:34 step:26763 [D loss: 0.033497, acc.: 100.00%] [G loss: 4.595494]\n",
      "epoch:34 step:26764 [D loss: 0.016802, acc.: 100.00%] [G loss: 4.347834]\n",
      "epoch:34 step:26765 [D loss: 0.010736, acc.: 100.00%] [G loss: 3.844360]\n",
      "epoch:34 step:26766 [D loss: 0.063078, acc.: 100.00%] [G loss: 5.151981]\n",
      "epoch:34 step:26767 [D loss: 0.112390, acc.: 96.09%] [G loss: 2.492823]\n",
      "epoch:34 step:26768 [D loss: 2.989423, acc.: 34.38%] [G loss: 8.152693]\n",
      "epoch:34 step:26769 [D loss: 1.766460, acc.: 50.78%] [G loss: 7.969210]\n",
      "epoch:34 step:26770 [D loss: 0.391218, acc.: 82.81%] [G loss: 5.694752]\n",
      "epoch:34 step:26771 [D loss: 0.006841, acc.: 100.00%] [G loss: 4.603365]\n",
      "epoch:34 step:26772 [D loss: 0.079694, acc.: 96.09%] [G loss: 4.640873]\n",
      "epoch:34 step:26773 [D loss: 0.006811, acc.: 100.00%] [G loss: 4.521187]\n",
      "epoch:34 step:26774 [D loss: 0.032588, acc.: 100.00%] [G loss: 3.711576]\n",
      "epoch:34 step:26775 [D loss: 0.016099, acc.: 100.00%] [G loss: 3.781800]\n",
      "epoch:34 step:26776 [D loss: 0.082226, acc.: 100.00%] [G loss: 4.652037]\n",
      "epoch:34 step:26777 [D loss: 0.059734, acc.: 98.44%] [G loss: 3.655689]\n",
      "epoch:34 step:26778 [D loss: 0.044075, acc.: 100.00%] [G loss: 3.921635]\n",
      "epoch:34 step:26779 [D loss: 0.036182, acc.: 100.00%] [G loss: 2.490210]\n",
      "epoch:34 step:26780 [D loss: 0.034073, acc.: 100.00%] [G loss: 2.620391]\n",
      "epoch:34 step:26781 [D loss: 0.106068, acc.: 97.66%] [G loss: 4.230954]\n",
      "epoch:34 step:26782 [D loss: 0.031628, acc.: 100.00%] [G loss: 3.903512]\n",
      "epoch:34 step:26783 [D loss: 0.065139, acc.: 99.22%] [G loss: 3.898788]\n",
      "epoch:34 step:26784 [D loss: 0.053377, acc.: 100.00%] [G loss: 3.203046]\n",
      "epoch:34 step:26785 [D loss: 0.084288, acc.: 97.66%] [G loss: 4.558392]\n",
      "epoch:34 step:26786 [D loss: 0.075724, acc.: 98.44%] [G loss: 2.685853]\n",
      "epoch:34 step:26787 [D loss: 0.031367, acc.: 100.00%] [G loss: 2.553287]\n",
      "epoch:34 step:26788 [D loss: 0.065889, acc.: 97.66%] [G loss: 1.092680]\n",
      "epoch:34 step:26789 [D loss: 0.363448, acc.: 81.25%] [G loss: 7.282951]\n",
      "epoch:34 step:26790 [D loss: 0.972432, acc.: 57.81%] [G loss: 3.388455]\n",
      "epoch:34 step:26791 [D loss: 0.144105, acc.: 93.75%] [G loss: 4.121359]\n",
      "epoch:34 step:26792 [D loss: 0.016032, acc.: 100.00%] [G loss: 5.454185]\n",
      "epoch:34 step:26793 [D loss: 0.015845, acc.: 100.00%] [G loss: 5.496293]\n",
      "epoch:34 step:26794 [D loss: 0.128401, acc.: 96.09%] [G loss: 3.322501]\n",
      "epoch:34 step:26795 [D loss: 0.132705, acc.: 95.31%] [G loss: 5.051563]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:34 step:26796 [D loss: 0.005485, acc.: 100.00%] [G loss: 6.120508]\n",
      "epoch:34 step:26797 [D loss: 0.080117, acc.: 97.66%] [G loss: 4.796950]\n",
      "epoch:34 step:26798 [D loss: 0.015627, acc.: 100.00%] [G loss: 4.055870]\n",
      "epoch:34 step:26799 [D loss: 0.039775, acc.: 100.00%] [G loss: 4.391125]\n",
      "epoch:34 step:26800 [D loss: 0.040417, acc.: 99.22%] [G loss: 4.596329]\n",
      "##############\n",
      "[2.11763379 1.00561074 0.97838249 1.05765613 0.77845241 1.05048869\n",
      " 2.10592329 2.10244937 0.92902954 1.07146098]\n",
      "##########\n",
      "epoch:34 step:26801 [D loss: 0.065354, acc.: 99.22%] [G loss: 3.071777]\n",
      "epoch:34 step:26802 [D loss: 0.065754, acc.: 100.00%] [G loss: 4.421151]\n",
      "epoch:34 step:26803 [D loss: 0.024010, acc.: 99.22%] [G loss: 5.235463]\n",
      "epoch:34 step:26804 [D loss: 0.147428, acc.: 96.09%] [G loss: 3.904904]\n",
      "epoch:34 step:26805 [D loss: 0.046053, acc.: 99.22%] [G loss: 3.910602]\n",
      "epoch:34 step:26806 [D loss: 0.019822, acc.: 100.00%] [G loss: 4.613806]\n",
      "epoch:34 step:26807 [D loss: 0.010555, acc.: 100.00%] [G loss: 4.650699]\n",
      "epoch:34 step:26808 [D loss: 0.009939, acc.: 100.00%] [G loss: 4.496390]\n",
      "epoch:34 step:26809 [D loss: 0.029207, acc.: 100.00%] [G loss: 4.656497]\n",
      "epoch:34 step:26810 [D loss: 0.045440, acc.: 98.44%] [G loss: 3.621039]\n",
      "epoch:34 step:26811 [D loss: 0.020959, acc.: 100.00%] [G loss: 4.297726]\n",
      "epoch:34 step:26812 [D loss: 0.032228, acc.: 100.00%] [G loss: 4.265530]\n",
      "epoch:34 step:26813 [D loss: 0.009438, acc.: 100.00%] [G loss: 3.893654]\n",
      "epoch:34 step:26814 [D loss: 0.066686, acc.: 98.44%] [G loss: 4.418520]\n",
      "epoch:34 step:26815 [D loss: 0.068076, acc.: 99.22%] [G loss: 2.729349]\n",
      "epoch:34 step:26816 [D loss: 0.059756, acc.: 99.22%] [G loss: 3.820322]\n",
      "epoch:34 step:26817 [D loss: 0.006950, acc.: 100.00%] [G loss: 5.531846]\n",
      "epoch:34 step:26818 [D loss: 0.140913, acc.: 94.53%] [G loss: 3.037894]\n",
      "epoch:34 step:26819 [D loss: 0.098716, acc.: 96.88%] [G loss: 4.754091]\n",
      "epoch:34 step:26820 [D loss: 0.007106, acc.: 100.00%] [G loss: 5.802938]\n",
      "epoch:34 step:26821 [D loss: 0.069795, acc.: 98.44%] [G loss: 4.063811]\n",
      "epoch:34 step:26822 [D loss: 0.016356, acc.: 100.00%] [G loss: 3.864912]\n",
      "epoch:34 step:26823 [D loss: 0.023749, acc.: 99.22%] [G loss: 4.618119]\n",
      "epoch:34 step:26824 [D loss: 0.022149, acc.: 100.00%] [G loss: 4.429953]\n",
      "epoch:34 step:26825 [D loss: 0.020286, acc.: 100.00%] [G loss: 4.591174]\n",
      "epoch:34 step:26826 [D loss: 0.011883, acc.: 100.00%] [G loss: 4.570926]\n",
      "epoch:34 step:26827 [D loss: 0.021648, acc.: 100.00%] [G loss: 3.587981]\n",
      "epoch:34 step:26828 [D loss: 0.031091, acc.: 100.00%] [G loss: 4.356963]\n",
      "epoch:34 step:26829 [D loss: 0.009736, acc.: 100.00%] [G loss: 4.743843]\n",
      "epoch:34 step:26830 [D loss: 0.017742, acc.: 100.00%] [G loss: 3.876919]\n",
      "epoch:34 step:26831 [D loss: 0.058747, acc.: 99.22%] [G loss: 4.934319]\n",
      "epoch:34 step:26832 [D loss: 0.010448, acc.: 100.00%] [G loss: 5.072363]\n",
      "epoch:34 step:26833 [D loss: 0.056208, acc.: 99.22%] [G loss: 4.398518]\n",
      "epoch:34 step:26834 [D loss: 0.023537, acc.: 100.00%] [G loss: 4.957114]\n",
      "epoch:34 step:26835 [D loss: 0.027324, acc.: 100.00%] [G loss: 5.329825]\n",
      "epoch:34 step:26836 [D loss: 0.018925, acc.: 100.00%] [G loss: 5.176549]\n",
      "epoch:34 step:26837 [D loss: 0.016067, acc.: 100.00%] [G loss: 4.377528]\n",
      "epoch:34 step:26838 [D loss: 0.016757, acc.: 100.00%] [G loss: 4.428777]\n",
      "epoch:34 step:26839 [D loss: 0.010996, acc.: 100.00%] [G loss: 4.865707]\n",
      "epoch:34 step:26840 [D loss: 0.124509, acc.: 96.09%] [G loss: 4.350972]\n",
      "epoch:34 step:26841 [D loss: 0.008483, acc.: 100.00%] [G loss: 5.424674]\n",
      "epoch:34 step:26842 [D loss: 0.009758, acc.: 100.00%] [G loss: 4.832532]\n",
      "epoch:34 step:26843 [D loss: 0.058944, acc.: 100.00%] [G loss: 3.517691]\n",
      "epoch:34 step:26844 [D loss: 0.080547, acc.: 97.66%] [G loss: 5.518636]\n",
      "epoch:34 step:26845 [D loss: 0.009330, acc.: 100.00%] [G loss: 5.707068]\n",
      "epoch:34 step:26846 [D loss: 0.095683, acc.: 96.88%] [G loss: 4.757342]\n",
      "epoch:34 step:26847 [D loss: 0.015668, acc.: 100.00%] [G loss: 3.600255]\n",
      "epoch:34 step:26848 [D loss: 0.029097, acc.: 100.00%] [G loss: 4.434373]\n",
      "epoch:34 step:26849 [D loss: 0.029633, acc.: 100.00%] [G loss: 4.844582]\n",
      "epoch:34 step:26850 [D loss: 0.008857, acc.: 100.00%] [G loss: 4.868276]\n",
      "epoch:34 step:26851 [D loss: 0.007570, acc.: 100.00%] [G loss: 4.793998]\n",
      "epoch:34 step:26852 [D loss: 0.015887, acc.: 100.00%] [G loss: 3.850858]\n",
      "epoch:34 step:26853 [D loss: 0.026837, acc.: 100.00%] [G loss: 4.930648]\n",
      "epoch:34 step:26854 [D loss: 0.021413, acc.: 100.00%] [G loss: 4.704995]\n",
      "epoch:34 step:26855 [D loss: 0.011162, acc.: 100.00%] [G loss: 5.196136]\n",
      "epoch:34 step:26856 [D loss: 0.016254, acc.: 100.00%] [G loss: 4.813725]\n",
      "epoch:34 step:26857 [D loss: 0.014579, acc.: 100.00%] [G loss: 4.758720]\n",
      "epoch:34 step:26858 [D loss: 0.017643, acc.: 100.00%] [G loss: 4.805927]\n",
      "epoch:34 step:26859 [D loss: 0.021915, acc.: 100.00%] [G loss: 4.293468]\n",
      "epoch:34 step:26860 [D loss: 0.008154, acc.: 100.00%] [G loss: 3.812346]\n",
      "epoch:34 step:26861 [D loss: 0.016807, acc.: 100.00%] [G loss: 4.689234]\n",
      "epoch:34 step:26862 [D loss: 0.011071, acc.: 100.00%] [G loss: 4.711017]\n",
      "epoch:34 step:26863 [D loss: 0.080410, acc.: 98.44%] [G loss: 3.514928]\n",
      "epoch:34 step:26864 [D loss: 0.081608, acc.: 97.66%] [G loss: 5.571923]\n",
      "epoch:34 step:26865 [D loss: 0.005993, acc.: 100.00%] [G loss: 5.761529]\n",
      "epoch:34 step:26866 [D loss: 0.093441, acc.: 97.66%] [G loss: 4.756633]\n",
      "epoch:34 step:26867 [D loss: 0.041813, acc.: 99.22%] [G loss: 5.372888]\n",
      "epoch:34 step:26868 [D loss: 0.005069, acc.: 100.00%] [G loss: 5.204005]\n",
      "epoch:34 step:26869 [D loss: 0.007777, acc.: 100.00%] [G loss: 4.821863]\n",
      "epoch:34 step:26870 [D loss: 0.008189, acc.: 100.00%] [G loss: 5.918598]\n",
      "epoch:34 step:26871 [D loss: 0.009726, acc.: 100.00%] [G loss: 5.124037]\n",
      "epoch:34 step:26872 [D loss: 0.003365, acc.: 100.00%] [G loss: 5.395146]\n",
      "epoch:34 step:26873 [D loss: 0.007031, acc.: 100.00%] [G loss: 5.620592]\n",
      "epoch:34 step:26874 [D loss: 0.035134, acc.: 98.44%] [G loss: 3.625751]\n",
      "epoch:34 step:26875 [D loss: 0.048293, acc.: 100.00%] [G loss: 4.694010]\n",
      "epoch:34 step:26876 [D loss: 0.020961, acc.: 99.22%] [G loss: 6.118777]\n",
      "epoch:34 step:26877 [D loss: 0.002885, acc.: 100.00%] [G loss: 6.363277]\n",
      "epoch:34 step:26878 [D loss: 0.060328, acc.: 99.22%] [G loss: 4.750760]\n",
      "epoch:34 step:26879 [D loss: 0.039356, acc.: 100.00%] [G loss: 4.372611]\n",
      "epoch:34 step:26880 [D loss: 0.007645, acc.: 100.00%] [G loss: 3.242546]\n",
      "epoch:34 step:26881 [D loss: 0.005340, acc.: 100.00%] [G loss: 3.872421]\n",
      "epoch:34 step:26882 [D loss: 0.014842, acc.: 100.00%] [G loss: 4.547239]\n",
      "epoch:34 step:26883 [D loss: 0.008088, acc.: 100.00%] [G loss: 4.039687]\n",
      "epoch:34 step:26884 [D loss: 0.009519, acc.: 100.00%] [G loss: 3.933727]\n",
      "epoch:34 step:26885 [D loss: 0.057239, acc.: 99.22%] [G loss: 3.048287]\n",
      "epoch:34 step:26886 [D loss: 0.011643, acc.: 100.00%] [G loss: 3.561954]\n",
      "epoch:34 step:26887 [D loss: 0.009169, acc.: 100.00%] [G loss: 3.954653]\n",
      "epoch:34 step:26888 [D loss: 0.046207, acc.: 99.22%] [G loss: 4.802144]\n",
      "epoch:34 step:26889 [D loss: 0.060458, acc.: 98.44%] [G loss: 2.993682]\n",
      "epoch:34 step:26890 [D loss: 1.256577, acc.: 49.22%] [G loss: 10.155703]\n",
      "epoch:34 step:26891 [D loss: 4.426023, acc.: 50.00%] [G loss: 7.248264]\n",
      "epoch:34 step:26892 [D loss: 2.913068, acc.: 50.00%] [G loss: 3.744222]\n",
      "epoch:34 step:26893 [D loss: 2.156448, acc.: 25.78%] [G loss: 2.566970]\n",
      "epoch:34 step:26894 [D loss: 0.751595, acc.: 64.06%] [G loss: 2.472120]\n",
      "epoch:34 step:26895 [D loss: 0.398648, acc.: 79.69%] [G loss: 1.910864]\n",
      "epoch:34 step:26896 [D loss: 0.311021, acc.: 88.28%] [G loss: 1.299088]\n",
      "epoch:34 step:26897 [D loss: 0.328853, acc.: 82.03%] [G loss: 2.636240]\n",
      "epoch:34 step:26898 [D loss: 0.166794, acc.: 94.53%] [G loss: 2.991784]\n",
      "epoch:34 step:26899 [D loss: 0.141152, acc.: 97.66%] [G loss: 2.448398]\n",
      "epoch:34 step:26900 [D loss: 0.337119, acc.: 89.06%] [G loss: 3.061904]\n",
      "epoch:34 step:26901 [D loss: 0.082391, acc.: 97.66%] [G loss: 3.250890]\n",
      "epoch:34 step:26902 [D loss: 0.094025, acc.: 99.22%] [G loss: 2.351551]\n",
      "epoch:34 step:26903 [D loss: 0.355239, acc.: 83.59%] [G loss: 1.492346]\n",
      "epoch:34 step:26904 [D loss: 0.396216, acc.: 85.16%] [G loss: 3.027353]\n",
      "epoch:34 step:26905 [D loss: 0.201941, acc.: 92.19%] [G loss: 3.621184]\n",
      "epoch:34 step:26906 [D loss: 0.067124, acc.: 99.22%] [G loss: 2.944863]\n",
      "epoch:34 step:26907 [D loss: 0.294224, acc.: 89.06%] [G loss: 2.352345]\n",
      "epoch:34 step:26908 [D loss: 0.083053, acc.: 99.22%] [G loss: 3.012589]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:34 step:26909 [D loss: 0.119813, acc.: 96.88%] [G loss: 2.977488]\n",
      "epoch:34 step:26910 [D loss: 0.038682, acc.: 100.00%] [G loss: 3.072763]\n",
      "epoch:34 step:26911 [D loss: 0.046560, acc.: 99.22%] [G loss: 3.201695]\n",
      "epoch:34 step:26912 [D loss: 0.068414, acc.: 100.00%] [G loss: 2.840228]\n",
      "epoch:34 step:26913 [D loss: 0.178001, acc.: 95.31%] [G loss: 2.676458]\n",
      "epoch:34 step:26914 [D loss: 0.121235, acc.: 97.66%] [G loss: 3.126042]\n",
      "epoch:34 step:26915 [D loss: 0.175812, acc.: 92.97%] [G loss: 3.300072]\n",
      "epoch:34 step:26916 [D loss: 0.083412, acc.: 98.44%] [G loss: 3.351038]\n",
      "epoch:34 step:26917 [D loss: 0.160413, acc.: 95.31%] [G loss: 4.005526]\n",
      "epoch:34 step:26918 [D loss: 0.689920, acc.: 64.06%] [G loss: 3.970052]\n",
      "epoch:34 step:26919 [D loss: 0.065822, acc.: 96.88%] [G loss: 4.709741]\n",
      "epoch:34 step:26920 [D loss: 0.068879, acc.: 97.66%] [G loss: 4.052190]\n",
      "epoch:34 step:26921 [D loss: 0.079231, acc.: 97.66%] [G loss: 3.133597]\n",
      "epoch:34 step:26922 [D loss: 0.135701, acc.: 96.88%] [G loss: 3.339830]\n",
      "epoch:34 step:26923 [D loss: 0.079576, acc.: 97.66%] [G loss: 4.022822]\n",
      "epoch:34 step:26924 [D loss: 0.109442, acc.: 96.09%] [G loss: 3.265200]\n",
      "epoch:34 step:26925 [D loss: 0.071317, acc.: 99.22%] [G loss: 3.072443]\n",
      "epoch:34 step:26926 [D loss: 0.089646, acc.: 98.44%] [G loss: 3.811217]\n",
      "epoch:34 step:26927 [D loss: 0.187580, acc.: 95.31%] [G loss: 3.539998]\n",
      "epoch:34 step:26928 [D loss: 0.080449, acc.: 99.22%] [G loss: 3.116686]\n",
      "epoch:34 step:26929 [D loss: 0.079969, acc.: 99.22%] [G loss: 4.067517]\n",
      "epoch:34 step:26930 [D loss: 0.031004, acc.: 100.00%] [G loss: 3.921856]\n",
      "epoch:34 step:26931 [D loss: 0.149987, acc.: 95.31%] [G loss: 2.158494]\n",
      "epoch:34 step:26932 [D loss: 0.286883, acc.: 85.94%] [G loss: 5.150280]\n",
      "epoch:34 step:26933 [D loss: 0.256827, acc.: 86.72%] [G loss: 4.565051]\n",
      "epoch:34 step:26934 [D loss: 0.191617, acc.: 93.75%] [G loss: 3.072134]\n",
      "epoch:34 step:26935 [D loss: 0.130608, acc.: 95.31%] [G loss: 4.256278]\n",
      "epoch:34 step:26936 [D loss: 0.052844, acc.: 98.44%] [G loss: 3.972610]\n",
      "epoch:34 step:26937 [D loss: 0.074719, acc.: 98.44%] [G loss: 3.617497]\n",
      "epoch:34 step:26938 [D loss: 0.060555, acc.: 98.44%] [G loss: 3.763277]\n",
      "epoch:34 step:26939 [D loss: 0.099929, acc.: 98.44%] [G loss: 3.611625]\n",
      "epoch:34 step:26940 [D loss: 0.076721, acc.: 100.00%] [G loss: 3.960108]\n",
      "epoch:34 step:26941 [D loss: 0.038941, acc.: 99.22%] [G loss: 3.515575]\n",
      "epoch:34 step:26942 [D loss: 0.449278, acc.: 79.69%] [G loss: 6.171124]\n",
      "epoch:34 step:26943 [D loss: 0.943789, acc.: 60.94%] [G loss: 2.804929]\n",
      "epoch:34 step:26944 [D loss: 0.167407, acc.: 93.75%] [G loss: 2.983462]\n",
      "epoch:34 step:26945 [D loss: 0.027838, acc.: 100.00%] [G loss: 3.975917]\n",
      "epoch:34 step:26946 [D loss: 0.037174, acc.: 100.00%] [G loss: 3.395196]\n",
      "epoch:34 step:26947 [D loss: 0.091246, acc.: 98.44%] [G loss: 3.631767]\n",
      "epoch:34 step:26948 [D loss: 0.019342, acc.: 100.00%] [G loss: 3.101949]\n",
      "epoch:34 step:26949 [D loss: 0.061147, acc.: 99.22%] [G loss: 3.272129]\n",
      "epoch:34 step:26950 [D loss: 0.249488, acc.: 91.41%] [G loss: 4.554037]\n",
      "epoch:34 step:26951 [D loss: 0.448263, acc.: 78.91%] [G loss: 3.167555]\n",
      "epoch:34 step:26952 [D loss: 0.071904, acc.: 98.44%] [G loss: 4.312583]\n",
      "epoch:34 step:26953 [D loss: 0.047475, acc.: 99.22%] [G loss: 3.549177]\n",
      "epoch:34 step:26954 [D loss: 0.027651, acc.: 100.00%] [G loss: 3.515488]\n",
      "epoch:34 step:26955 [D loss: 0.048741, acc.: 100.00%] [G loss: 3.041262]\n",
      "epoch:34 step:26956 [D loss: 0.122148, acc.: 97.66%] [G loss: 4.533284]\n",
      "epoch:34 step:26957 [D loss: 0.321001, acc.: 89.06%] [G loss: 2.497166]\n",
      "epoch:34 step:26958 [D loss: 0.152770, acc.: 91.41%] [G loss: 4.659091]\n",
      "epoch:34 step:26959 [D loss: 0.075735, acc.: 96.88%] [G loss: 4.780578]\n",
      "epoch:34 step:26960 [D loss: 0.123610, acc.: 96.09%] [G loss: 3.988472]\n",
      "epoch:34 step:26961 [D loss: 0.061461, acc.: 99.22%] [G loss: 3.728410]\n",
      "epoch:34 step:26962 [D loss: 0.046049, acc.: 99.22%] [G loss: 4.643806]\n",
      "epoch:34 step:26963 [D loss: 0.223251, acc.: 92.97%] [G loss: 4.207302]\n",
      "epoch:34 step:26964 [D loss: 0.018294, acc.: 100.00%] [G loss: 5.175763]\n",
      "epoch:34 step:26965 [D loss: 0.023330, acc.: 100.00%] [G loss: 4.477827]\n",
      "epoch:34 step:26966 [D loss: 0.042403, acc.: 99.22%] [G loss: 4.975037]\n",
      "epoch:34 step:26967 [D loss: 0.092104, acc.: 98.44%] [G loss: 3.001576]\n",
      "epoch:34 step:26968 [D loss: 0.183603, acc.: 91.41%] [G loss: 5.871200]\n",
      "epoch:34 step:26969 [D loss: 0.180000, acc.: 91.41%] [G loss: 4.548117]\n",
      "epoch:34 step:26970 [D loss: 0.017545, acc.: 100.00%] [G loss: 4.812996]\n",
      "epoch:34 step:26971 [D loss: 0.065224, acc.: 98.44%] [G loss: 4.654206]\n",
      "epoch:34 step:26972 [D loss: 0.062868, acc.: 98.44%] [G loss: 4.003329]\n",
      "epoch:34 step:26973 [D loss: 0.057036, acc.: 98.44%] [G loss: 4.747590]\n",
      "epoch:34 step:26974 [D loss: 0.324461, acc.: 86.72%] [G loss: 4.847380]\n",
      "epoch:34 step:26975 [D loss: 0.319030, acc.: 84.38%] [G loss: 3.231888]\n",
      "epoch:34 step:26976 [D loss: 0.064480, acc.: 98.44%] [G loss: 3.720427]\n",
      "epoch:34 step:26977 [D loss: 0.021846, acc.: 100.00%] [G loss: 4.456320]\n",
      "epoch:34 step:26978 [D loss: 0.033516, acc.: 100.00%] [G loss: 3.608871]\n",
      "epoch:34 step:26979 [D loss: 0.017755, acc.: 100.00%] [G loss: 3.590126]\n",
      "epoch:34 step:26980 [D loss: 0.028452, acc.: 99.22%] [G loss: 3.795622]\n",
      "epoch:34 step:26981 [D loss: 0.029090, acc.: 100.00%] [G loss: 3.720907]\n",
      "epoch:34 step:26982 [D loss: 0.121553, acc.: 96.88%] [G loss: 3.997922]\n",
      "epoch:34 step:26983 [D loss: 0.081106, acc.: 98.44%] [G loss: 2.660412]\n",
      "epoch:34 step:26984 [D loss: 0.256630, acc.: 88.28%] [G loss: 5.503568]\n",
      "epoch:34 step:26985 [D loss: 0.816170, acc.: 58.59%] [G loss: 1.732401]\n",
      "epoch:34 step:26986 [D loss: 0.112523, acc.: 97.66%] [G loss: 3.674481]\n",
      "epoch:34 step:26987 [D loss: 0.064406, acc.: 98.44%] [G loss: 4.636545]\n",
      "epoch:34 step:26988 [D loss: 0.348003, acc.: 83.59%] [G loss: 2.400165]\n",
      "epoch:34 step:26989 [D loss: 0.104545, acc.: 96.88%] [G loss: 3.858172]\n",
      "epoch:34 step:26990 [D loss: 0.087170, acc.: 98.44%] [G loss: 5.314070]\n",
      "epoch:34 step:26991 [D loss: 0.436772, acc.: 80.47%] [G loss: 1.904726]\n",
      "epoch:34 step:26992 [D loss: 0.598116, acc.: 68.75%] [G loss: 7.166390]\n",
      "epoch:34 step:26993 [D loss: 1.219347, acc.: 60.16%] [G loss: 5.939937]\n",
      "epoch:34 step:26994 [D loss: 0.156747, acc.: 92.97%] [G loss: 3.891918]\n",
      "epoch:34 step:26995 [D loss: 0.072189, acc.: 99.22%] [G loss: 3.476842]\n",
      "epoch:34 step:26996 [D loss: 0.041519, acc.: 100.00%] [G loss: 3.433679]\n",
      "epoch:34 step:26997 [D loss: 0.054396, acc.: 98.44%] [G loss: 4.256611]\n",
      "epoch:34 step:26998 [D loss: 0.028572, acc.: 100.00%] [G loss: 3.857196]\n",
      "epoch:34 step:26999 [D loss: 0.041539, acc.: 99.22%] [G loss: 4.241591]\n",
      "epoch:34 step:27000 [D loss: 0.153985, acc.: 95.31%] [G loss: 2.556894]\n",
      "##############\n",
      "[0.97419652 1.08008661 0.96684382 0.93557168 2.09601635 2.1219425\n",
      " 2.11156896 1.1116751  0.73958158 2.1149968 ]\n",
      "##########\n",
      "epoch:34 step:27001 [D loss: 0.070558, acc.: 98.44%] [G loss: 2.267210]\n",
      "epoch:34 step:27002 [D loss: 0.034221, acc.: 100.00%] [G loss: 3.290296]\n",
      "epoch:34 step:27003 [D loss: 0.159433, acc.: 92.97%] [G loss: 4.175500]\n",
      "epoch:34 step:27004 [D loss: 0.078840, acc.: 96.88%] [G loss: 3.847980]\n",
      "epoch:34 step:27005 [D loss: 0.149031, acc.: 95.31%] [G loss: 1.734083]\n",
      "epoch:34 step:27006 [D loss: 0.133366, acc.: 97.66%] [G loss: 3.735111]\n",
      "epoch:34 step:27007 [D loss: 0.055662, acc.: 98.44%] [G loss: 3.170821]\n",
      "epoch:34 step:27008 [D loss: 0.035625, acc.: 99.22%] [G loss: 2.862602]\n",
      "epoch:34 step:27009 [D loss: 0.041266, acc.: 100.00%] [G loss: 2.753683]\n",
      "epoch:34 step:27010 [D loss: 0.081111, acc.: 99.22%] [G loss: 3.816400]\n",
      "epoch:34 step:27011 [D loss: 0.053703, acc.: 98.44%] [G loss: 2.832672]\n",
      "epoch:34 step:27012 [D loss: 0.114108, acc.: 96.88%] [G loss: 3.817237]\n",
      "epoch:34 step:27013 [D loss: 0.380083, acc.: 85.16%] [G loss: 4.792877]\n",
      "epoch:34 step:27014 [D loss: 0.137974, acc.: 92.19%] [G loss: 4.150588]\n",
      "epoch:34 step:27015 [D loss: 0.054758, acc.: 100.00%] [G loss: 3.824889]\n",
      "epoch:34 step:27016 [D loss: 0.017579, acc.: 100.00%] [G loss: 3.998100]\n",
      "epoch:34 step:27017 [D loss: 0.054513, acc.: 100.00%] [G loss: 3.887462]\n",
      "epoch:34 step:27018 [D loss: 0.030755, acc.: 100.00%] [G loss: 3.303373]\n",
      "epoch:34 step:27019 [D loss: 0.035555, acc.: 100.00%] [G loss: 3.457031]\n",
      "epoch:34 step:27020 [D loss: 0.074298, acc.: 98.44%] [G loss: 3.267044]\n",
      "epoch:34 step:27021 [D loss: 0.139700, acc.: 95.31%] [G loss: 4.696023]\n",
      "epoch:34 step:27022 [D loss: 1.776845, acc.: 31.25%] [G loss: 7.146538]\n",
      "epoch:34 step:27023 [D loss: 1.348142, acc.: 53.12%] [G loss: 6.146003]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:34 step:27024 [D loss: 0.865372, acc.: 64.84%] [G loss: 2.762764]\n",
      "epoch:34 step:27025 [D loss: 0.226682, acc.: 89.84%] [G loss: 2.865310]\n",
      "epoch:34 step:27026 [D loss: 0.045748, acc.: 100.00%] [G loss: 3.590850]\n",
      "epoch:34 step:27027 [D loss: 0.037803, acc.: 100.00%] [G loss: 3.969546]\n",
      "epoch:34 step:27028 [D loss: 0.066868, acc.: 99.22%] [G loss: 3.516970]\n",
      "epoch:34 step:27029 [D loss: 0.078224, acc.: 99.22%] [G loss: 3.910072]\n",
      "epoch:34 step:27030 [D loss: 0.049494, acc.: 99.22%] [G loss: 3.395397]\n",
      "epoch:34 step:27031 [D loss: 0.041146, acc.: 100.00%] [G loss: 3.697868]\n",
      "epoch:34 step:27032 [D loss: 0.068892, acc.: 100.00%] [G loss: 3.310017]\n",
      "epoch:34 step:27033 [D loss: 0.064320, acc.: 98.44%] [G loss: 3.664577]\n",
      "epoch:34 step:27034 [D loss: 0.059099, acc.: 99.22%] [G loss: 3.348821]\n",
      "epoch:34 step:27035 [D loss: 0.096986, acc.: 98.44%] [G loss: 3.940940]\n",
      "epoch:34 step:27036 [D loss: 0.085037, acc.: 97.66%] [G loss: 4.046671]\n",
      "epoch:34 step:27037 [D loss: 0.217740, acc.: 95.31%] [G loss: 3.565780]\n",
      "epoch:34 step:27038 [D loss: 0.060092, acc.: 100.00%] [G loss: 4.264687]\n",
      "epoch:34 step:27039 [D loss: 0.045200, acc.: 100.00%] [G loss: 4.219872]\n",
      "epoch:34 step:27040 [D loss: 0.062404, acc.: 100.00%] [G loss: 3.422396]\n",
      "epoch:34 step:27041 [D loss: 0.101094, acc.: 99.22%] [G loss: 4.437252]\n",
      "epoch:34 step:27042 [D loss: 0.174381, acc.: 93.75%] [G loss: 3.288420]\n",
      "epoch:34 step:27043 [D loss: 0.066363, acc.: 100.00%] [G loss: 3.462012]\n",
      "epoch:34 step:27044 [D loss: 0.031066, acc.: 99.22%] [G loss: 3.582755]\n",
      "epoch:34 step:27045 [D loss: 0.048364, acc.: 99.22%] [G loss: 3.146544]\n",
      "epoch:34 step:27046 [D loss: 0.132824, acc.: 96.09%] [G loss: 3.536006]\n",
      "epoch:34 step:27047 [D loss: 0.030700, acc.: 100.00%] [G loss: 3.924213]\n",
      "epoch:34 step:27048 [D loss: 0.091851, acc.: 97.66%] [G loss: 3.816195]\n",
      "epoch:34 step:27049 [D loss: 0.051753, acc.: 99.22%] [G loss: 4.015027]\n",
      "epoch:34 step:27050 [D loss: 0.035811, acc.: 99.22%] [G loss: 4.082211]\n",
      "epoch:34 step:27051 [D loss: 0.097866, acc.: 97.66%] [G loss: 4.056993]\n",
      "epoch:34 step:27052 [D loss: 0.138111, acc.: 95.31%] [G loss: 2.630449]\n",
      "epoch:34 step:27053 [D loss: 0.052015, acc.: 98.44%] [G loss: 3.666677]\n",
      "epoch:34 step:27054 [D loss: 0.064201, acc.: 98.44%] [G loss: 3.029169]\n",
      "epoch:34 step:27055 [D loss: 0.027790, acc.: 99.22%] [G loss: 4.012762]\n",
      "epoch:34 step:27056 [D loss: 0.115781, acc.: 95.31%] [G loss: 5.123409]\n",
      "epoch:34 step:27057 [D loss: 0.050225, acc.: 98.44%] [G loss: 5.490843]\n",
      "epoch:34 step:27058 [D loss: 0.077756, acc.: 96.09%] [G loss: 4.555185]\n",
      "epoch:34 step:27059 [D loss: 0.044777, acc.: 99.22%] [G loss: 3.900116]\n",
      "epoch:34 step:27060 [D loss: 0.082148, acc.: 98.44%] [G loss: 5.153120]\n",
      "epoch:34 step:27061 [D loss: 0.036368, acc.: 98.44%] [G loss: 4.839053]\n",
      "epoch:34 step:27062 [D loss: 0.046025, acc.: 99.22%] [G loss: 3.985213]\n",
      "epoch:34 step:27063 [D loss: 1.423641, acc.: 35.16%] [G loss: 6.855034]\n",
      "epoch:34 step:27064 [D loss: 0.111900, acc.: 96.09%] [G loss: 7.511439]\n",
      "epoch:34 step:27065 [D loss: 0.870502, acc.: 65.62%] [G loss: 4.477719]\n",
      "epoch:34 step:27066 [D loss: 0.046915, acc.: 97.66%] [G loss: 3.955965]\n",
      "epoch:34 step:27067 [D loss: 0.047916, acc.: 99.22%] [G loss: 4.142561]\n",
      "epoch:34 step:27068 [D loss: 0.037367, acc.: 99.22%] [G loss: 3.778034]\n",
      "epoch:34 step:27069 [D loss: 0.049530, acc.: 97.66%] [G loss: 3.605304]\n",
      "epoch:34 step:27070 [D loss: 0.013340, acc.: 100.00%] [G loss: 3.272877]\n",
      "epoch:34 step:27071 [D loss: 0.058130, acc.: 98.44%] [G loss: 4.192117]\n",
      "epoch:34 step:27072 [D loss: 0.054997, acc.: 99.22%] [G loss: 4.777040]\n",
      "epoch:34 step:27073 [D loss: 0.636473, acc.: 62.50%] [G loss: 4.585590]\n",
      "epoch:34 step:27074 [D loss: 0.091713, acc.: 96.09%] [G loss: 4.987032]\n",
      "epoch:34 step:27075 [D loss: 0.048052, acc.: 98.44%] [G loss: 4.272933]\n",
      "epoch:34 step:27076 [D loss: 0.027639, acc.: 100.00%] [G loss: 3.908088]\n",
      "epoch:34 step:27077 [D loss: 0.030347, acc.: 100.00%] [G loss: 4.364918]\n",
      "epoch:34 step:27078 [D loss: 0.061008, acc.: 98.44%] [G loss: 4.075773]\n",
      "epoch:34 step:27079 [D loss: 0.047648, acc.: 100.00%] [G loss: 4.399131]\n",
      "epoch:34 step:27080 [D loss: 0.115824, acc.: 95.31%] [G loss: 3.540143]\n",
      "epoch:34 step:27081 [D loss: 0.033121, acc.: 100.00%] [G loss: 2.965622]\n",
      "epoch:34 step:27082 [D loss: 0.056852, acc.: 99.22%] [G loss: 4.241846]\n",
      "epoch:34 step:27083 [D loss: 0.024352, acc.: 100.00%] [G loss: 3.261834]\n",
      "epoch:34 step:27084 [D loss: 0.009726, acc.: 100.00%] [G loss: 3.968032]\n",
      "epoch:34 step:27085 [D loss: 0.007522, acc.: 100.00%] [G loss: 3.340934]\n",
      "epoch:34 step:27086 [D loss: 0.029460, acc.: 99.22%] [G loss: 2.539229]\n",
      "epoch:34 step:27087 [D loss: 0.150209, acc.: 96.09%] [G loss: 2.479964]\n",
      "epoch:34 step:27088 [D loss: 0.009016, acc.: 100.00%] [G loss: 2.970402]\n",
      "epoch:34 step:27089 [D loss: 0.086770, acc.: 96.88%] [G loss: 1.318235]\n",
      "epoch:34 step:27090 [D loss: 0.263577, acc.: 89.84%] [G loss: 4.159301]\n",
      "epoch:34 step:27091 [D loss: 0.070802, acc.: 98.44%] [G loss: 5.689863]\n",
      "epoch:34 step:27092 [D loss: 0.070983, acc.: 98.44%] [G loss: 4.184578]\n",
      "epoch:34 step:27093 [D loss: 0.009311, acc.: 100.00%] [G loss: 2.675522]\n",
      "epoch:34 step:27094 [D loss: 0.029528, acc.: 100.00%] [G loss: 3.693813]\n",
      "epoch:34 step:27095 [D loss: 0.052025, acc.: 99.22%] [G loss: 4.148014]\n",
      "epoch:34 step:27096 [D loss: 0.072701, acc.: 97.66%] [G loss: 2.845983]\n",
      "epoch:34 step:27097 [D loss: 0.058761, acc.: 100.00%] [G loss: 2.606672]\n",
      "epoch:34 step:27098 [D loss: 0.050462, acc.: 99.22%] [G loss: 1.081750]\n",
      "epoch:34 step:27099 [D loss: 0.007977, acc.: 100.00%] [G loss: 1.151780]\n",
      "epoch:34 step:27100 [D loss: 0.128314, acc.: 94.53%] [G loss: 3.738456]\n",
      "epoch:34 step:27101 [D loss: 0.017767, acc.: 100.00%] [G loss: 5.694042]\n",
      "epoch:34 step:27102 [D loss: 0.710867, acc.: 64.84%] [G loss: 3.931436]\n",
      "epoch:34 step:27103 [D loss: 0.008001, acc.: 100.00%] [G loss: 6.323888]\n",
      "epoch:34 step:27104 [D loss: 0.063426, acc.: 98.44%] [G loss: 5.515627]\n",
      "epoch:34 step:27105 [D loss: 0.010867, acc.: 100.00%] [G loss: 4.590483]\n",
      "epoch:34 step:27106 [D loss: 0.095257, acc.: 97.66%] [G loss: 5.571794]\n",
      "epoch:34 step:27107 [D loss: 0.104575, acc.: 95.31%] [G loss: 4.688776]\n",
      "epoch:34 step:27108 [D loss: 0.034228, acc.: 98.44%] [G loss: 5.248739]\n",
      "epoch:34 step:27109 [D loss: 0.016511, acc.: 100.00%] [G loss: 5.221630]\n",
      "epoch:34 step:27110 [D loss: 0.022738, acc.: 100.00%] [G loss: 4.642828]\n",
      "epoch:34 step:27111 [D loss: 0.031338, acc.: 100.00%] [G loss: 4.047086]\n",
      "epoch:34 step:27112 [D loss: 0.024006, acc.: 99.22%] [G loss: 4.175534]\n",
      "epoch:34 step:27113 [D loss: 0.021397, acc.: 99.22%] [G loss: 4.615251]\n",
      "epoch:34 step:27114 [D loss: 0.027475, acc.: 100.00%] [G loss: 4.359430]\n",
      "epoch:34 step:27115 [D loss: 0.052934, acc.: 99.22%] [G loss: 4.053193]\n",
      "epoch:34 step:27116 [D loss: 0.061364, acc.: 98.44%] [G loss: 4.526525]\n",
      "epoch:34 step:27117 [D loss: 0.008703, acc.: 100.00%] [G loss: 4.450276]\n",
      "epoch:34 step:27118 [D loss: 0.064733, acc.: 97.66%] [G loss: 3.307866]\n",
      "epoch:34 step:27119 [D loss: 0.147329, acc.: 96.09%] [G loss: 5.361474]\n",
      "epoch:34 step:27120 [D loss: 0.075601, acc.: 96.88%] [G loss: 4.347019]\n",
      "epoch:34 step:27121 [D loss: 0.010486, acc.: 100.00%] [G loss: 4.265102]\n",
      "epoch:34 step:27122 [D loss: 0.015171, acc.: 100.00%] [G loss: 3.550953]\n",
      "epoch:34 step:27123 [D loss: 0.133141, acc.: 96.09%] [G loss: 5.008849]\n",
      "epoch:34 step:27124 [D loss: 0.147657, acc.: 93.75%] [G loss: 1.867242]\n",
      "epoch:34 step:27125 [D loss: 0.440221, acc.: 75.78%] [G loss: 8.731309]\n",
      "epoch:34 step:27126 [D loss: 1.672128, acc.: 53.91%] [G loss: 5.585025]\n",
      "epoch:34 step:27127 [D loss: 0.080664, acc.: 95.31%] [G loss: 5.017080]\n",
      "epoch:34 step:27128 [D loss: 0.016289, acc.: 100.00%] [G loss: 4.846210]\n",
      "epoch:34 step:27129 [D loss: 0.208644, acc.: 92.19%] [G loss: 3.642753]\n",
      "epoch:34 step:27130 [D loss: 0.034094, acc.: 99.22%] [G loss: 4.526806]\n",
      "epoch:34 step:27131 [D loss: 0.067429, acc.: 98.44%] [G loss: 4.344370]\n",
      "epoch:34 step:27132 [D loss: 0.017302, acc.: 99.22%] [G loss: 2.969081]\n",
      "epoch:34 step:27133 [D loss: 0.013657, acc.: 100.00%] [G loss: 3.967063]\n",
      "epoch:34 step:27134 [D loss: 0.091768, acc.: 97.66%] [G loss: 4.447659]\n",
      "epoch:34 step:27135 [D loss: 0.020967, acc.: 100.00%] [G loss: 4.621479]\n",
      "epoch:34 step:27136 [D loss: 0.065470, acc.: 98.44%] [G loss: 3.851899]\n",
      "epoch:34 step:27137 [D loss: 0.147089, acc.: 95.31%] [G loss: 4.555593]\n",
      "epoch:34 step:27138 [D loss: 0.021820, acc.: 100.00%] [G loss: 5.075032]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:34 step:27139 [D loss: 0.022876, acc.: 99.22%] [G loss: 4.093530]\n",
      "epoch:34 step:27140 [D loss: 0.042424, acc.: 100.00%] [G loss: 3.707545]\n",
      "epoch:34 step:27141 [D loss: 0.020693, acc.: 100.00%] [G loss: 3.696506]\n",
      "epoch:34 step:27142 [D loss: 0.066617, acc.: 98.44%] [G loss: 3.249411]\n",
      "epoch:34 step:27143 [D loss: 0.119424, acc.: 96.88%] [G loss: 4.988085]\n",
      "epoch:34 step:27144 [D loss: 0.035255, acc.: 100.00%] [G loss: 5.574757]\n",
      "epoch:34 step:27145 [D loss: 0.761802, acc.: 64.84%] [G loss: 6.340392]\n",
      "epoch:34 step:27146 [D loss: 0.342134, acc.: 87.50%] [G loss: 4.898481]\n",
      "epoch:34 step:27147 [D loss: 0.009859, acc.: 100.00%] [G loss: 4.758002]\n",
      "epoch:34 step:27148 [D loss: 0.047311, acc.: 98.44%] [G loss: 5.126957]\n",
      "epoch:34 step:27149 [D loss: 0.006768, acc.: 100.00%] [G loss: 5.087132]\n",
      "epoch:34 step:27150 [D loss: 0.006027, acc.: 100.00%] [G loss: 4.695186]\n",
      "epoch:34 step:27151 [D loss: 0.015424, acc.: 100.00%] [G loss: 5.107611]\n",
      "epoch:34 step:27152 [D loss: 0.012753, acc.: 100.00%] [G loss: 5.015304]\n",
      "epoch:34 step:27153 [D loss: 0.014599, acc.: 100.00%] [G loss: 4.912033]\n",
      "epoch:34 step:27154 [D loss: 0.020843, acc.: 100.00%] [G loss: 4.451437]\n",
      "epoch:34 step:27155 [D loss: 0.044121, acc.: 99.22%] [G loss: 3.990836]\n",
      "epoch:34 step:27156 [D loss: 0.022211, acc.: 100.00%] [G loss: 4.214308]\n",
      "epoch:34 step:27157 [D loss: 0.022360, acc.: 100.00%] [G loss: 4.394471]\n",
      "epoch:34 step:27158 [D loss: 0.059250, acc.: 100.00%] [G loss: 3.547619]\n",
      "epoch:34 step:27159 [D loss: 0.035488, acc.: 100.00%] [G loss: 3.898588]\n",
      "epoch:34 step:27160 [D loss: 0.040551, acc.: 100.00%] [G loss: 3.932280]\n",
      "epoch:34 step:27161 [D loss: 0.014152, acc.: 100.00%] [G loss: 4.001840]\n",
      "epoch:34 step:27162 [D loss: 0.005854, acc.: 100.00%] [G loss: 3.801491]\n",
      "epoch:34 step:27163 [D loss: 0.042874, acc.: 100.00%] [G loss: 4.035363]\n",
      "epoch:34 step:27164 [D loss: 0.029667, acc.: 100.00%] [G loss: 4.392779]\n",
      "epoch:34 step:27165 [D loss: 0.006968, acc.: 100.00%] [G loss: 4.345807]\n",
      "epoch:34 step:27166 [D loss: 0.026753, acc.: 100.00%] [G loss: 3.212330]\n",
      "epoch:34 step:27167 [D loss: 0.171875, acc.: 93.75%] [G loss: 4.302304]\n",
      "epoch:34 step:27168 [D loss: 0.004283, acc.: 100.00%] [G loss: 5.443550]\n",
      "epoch:34 step:27169 [D loss: 0.013946, acc.: 100.00%] [G loss: 4.627794]\n",
      "epoch:34 step:27170 [D loss: 0.062313, acc.: 98.44%] [G loss: 3.247819]\n",
      "epoch:34 step:27171 [D loss: 0.123794, acc.: 98.44%] [G loss: 6.185456]\n",
      "epoch:34 step:27172 [D loss: 0.055260, acc.: 97.66%] [G loss: 5.599472]\n",
      "epoch:34 step:27173 [D loss: 0.073242, acc.: 99.22%] [G loss: 4.036638]\n",
      "epoch:34 step:27174 [D loss: 0.021513, acc.: 100.00%] [G loss: 3.775185]\n",
      "epoch:34 step:27175 [D loss: 0.011528, acc.: 100.00%] [G loss: 3.784646]\n",
      "epoch:34 step:27176 [D loss: 0.023268, acc.: 100.00%] [G loss: 5.378361]\n",
      "epoch:34 step:27177 [D loss: 0.191059, acc.: 93.75%] [G loss: 4.269506]\n",
      "epoch:34 step:27178 [D loss: 0.004947, acc.: 100.00%] [G loss: 6.254107]\n",
      "epoch:34 step:27179 [D loss: 0.006474, acc.: 100.00%] [G loss: 5.819086]\n",
      "epoch:34 step:27180 [D loss: 0.022707, acc.: 100.00%] [G loss: 4.797394]\n",
      "epoch:34 step:27181 [D loss: 0.283026, acc.: 88.28%] [G loss: 6.553177]\n",
      "epoch:34 step:27182 [D loss: 0.003846, acc.: 100.00%] [G loss: 8.090531]\n",
      "epoch:34 step:27183 [D loss: 2.858445, acc.: 21.88%] [G loss: 8.775430]\n",
      "epoch:34 step:27184 [D loss: 2.754599, acc.: 50.00%] [G loss: 6.298149]\n",
      "epoch:34 step:27185 [D loss: 0.377670, acc.: 83.59%] [G loss: 3.326315]\n",
      "epoch:34 step:27186 [D loss: 0.557180, acc.: 80.47%] [G loss: 5.513192]\n",
      "epoch:34 step:27187 [D loss: 0.392854, acc.: 78.12%] [G loss: 4.045153]\n",
      "epoch:34 step:27188 [D loss: 0.102722, acc.: 96.88%] [G loss: 3.561182]\n",
      "epoch:34 step:27189 [D loss: 0.208464, acc.: 89.06%] [G loss: 4.824819]\n",
      "epoch:34 step:27190 [D loss: 0.166706, acc.: 92.19%] [G loss: 3.526460]\n",
      "epoch:34 step:27191 [D loss: 0.062751, acc.: 99.22%] [G loss: 3.542794]\n",
      "epoch:34 step:27192 [D loss: 0.034698, acc.: 100.00%] [G loss: 3.134879]\n",
      "epoch:34 step:27193 [D loss: 0.044332, acc.: 100.00%] [G loss: 2.205453]\n",
      "epoch:34 step:27194 [D loss: 0.291491, acc.: 91.41%] [G loss: 4.631069]\n",
      "epoch:34 step:27195 [D loss: 0.316869, acc.: 83.59%] [G loss: 2.032734]\n",
      "epoch:34 step:27196 [D loss: 0.173048, acc.: 92.97%] [G loss: 3.728907]\n",
      "epoch:34 step:27197 [D loss: 0.145635, acc.: 94.53%] [G loss: 3.619328]\n",
      "epoch:34 step:27198 [D loss: 0.096442, acc.: 96.09%] [G loss: 2.787500]\n",
      "epoch:34 step:27199 [D loss: 0.580799, acc.: 69.53%] [G loss: 6.405114]\n",
      "epoch:34 step:27200 [D loss: 1.239603, acc.: 53.12%] [G loss: 4.126463]\n",
      "##############\n",
      "[0.88017522 0.92420874 1.12624252 0.9154837  0.78628576 2.11833662\n",
      " 1.11691428 1.10844225 0.84809428 1.01694286]\n",
      "##########\n",
      "epoch:34 step:27201 [D loss: 0.284191, acc.: 86.72%] [G loss: 2.685816]\n",
      "epoch:34 step:27202 [D loss: 0.073488, acc.: 98.44%] [G loss: 4.048509]\n",
      "epoch:34 step:27203 [D loss: 0.067457, acc.: 98.44%] [G loss: 4.067608]\n",
      "epoch:34 step:27204 [D loss: 0.059551, acc.: 99.22%] [G loss: 3.351361]\n",
      "epoch:34 step:27205 [D loss: 0.039447, acc.: 100.00%] [G loss: 3.541473]\n",
      "epoch:34 step:27206 [D loss: 0.203961, acc.: 92.19%] [G loss: 4.776182]\n",
      "epoch:34 step:27207 [D loss: 0.028023, acc.: 99.22%] [G loss: 4.930211]\n",
      "epoch:34 step:27208 [D loss: 0.389217, acc.: 80.47%] [G loss: 2.476823]\n",
      "epoch:34 step:27209 [D loss: 0.178536, acc.: 92.97%] [G loss: 3.979743]\n",
      "epoch:34 step:27210 [D loss: 0.023025, acc.: 100.00%] [G loss: 4.875113]\n",
      "epoch:34 step:27211 [D loss: 0.079007, acc.: 97.66%] [G loss: 3.960309]\n",
      "epoch:34 step:27212 [D loss: 0.080735, acc.: 96.88%] [G loss: 3.860357]\n",
      "epoch:34 step:27213 [D loss: 0.039382, acc.: 100.00%] [G loss: 4.273529]\n",
      "epoch:34 step:27214 [D loss: 0.009472, acc.: 100.00%] [G loss: 3.581800]\n",
      "epoch:34 step:27215 [D loss: 0.059141, acc.: 98.44%] [G loss: 3.581226]\n",
      "epoch:34 step:27216 [D loss: 0.044364, acc.: 99.22%] [G loss: 3.692848]\n",
      "epoch:34 step:27217 [D loss: 0.019208, acc.: 100.00%] [G loss: 3.974652]\n",
      "epoch:34 step:27218 [D loss: 0.022754, acc.: 100.00%] [G loss: 3.470764]\n",
      "epoch:34 step:27219 [D loss: 0.061260, acc.: 99.22%] [G loss: 2.682818]\n",
      "epoch:34 step:27220 [D loss: 0.102365, acc.: 98.44%] [G loss: 4.139150]\n",
      "epoch:34 step:27221 [D loss: 0.014383, acc.: 100.00%] [G loss: 4.875098]\n",
      "epoch:34 step:27222 [D loss: 0.180613, acc.: 93.75%] [G loss: 2.554357]\n",
      "epoch:34 step:27223 [D loss: 0.047836, acc.: 100.00%] [G loss: 2.745077]\n",
      "epoch:34 step:27224 [D loss: 0.086918, acc.: 97.66%] [G loss: 4.103690]\n",
      "epoch:34 step:27225 [D loss: 0.012973, acc.: 100.00%] [G loss: 4.679624]\n",
      "epoch:34 step:27226 [D loss: 0.023788, acc.: 100.00%] [G loss: 4.488312]\n",
      "epoch:34 step:27227 [D loss: 0.024582, acc.: 100.00%] [G loss: 4.898556]\n",
      "epoch:34 step:27228 [D loss: 0.028584, acc.: 100.00%] [G loss: 3.739395]\n",
      "epoch:34 step:27229 [D loss: 0.166824, acc.: 92.19%] [G loss: 5.355830]\n",
      "epoch:34 step:27230 [D loss: 0.094127, acc.: 96.88%] [G loss: 5.008224]\n",
      "epoch:34 step:27231 [D loss: 0.010743, acc.: 100.00%] [G loss: 4.503428]\n",
      "epoch:34 step:27232 [D loss: 0.014164, acc.: 100.00%] [G loss: 4.346852]\n",
      "epoch:34 step:27233 [D loss: 0.014363, acc.: 100.00%] [G loss: 4.026816]\n",
      "epoch:34 step:27234 [D loss: 0.058093, acc.: 100.00%] [G loss: 4.285021]\n",
      "epoch:34 step:27235 [D loss: 0.014562, acc.: 100.00%] [G loss: 5.324646]\n",
      "epoch:34 step:27236 [D loss: 0.025899, acc.: 99.22%] [G loss: 3.552478]\n",
      "epoch:34 step:27237 [D loss: 0.031120, acc.: 99.22%] [G loss: 4.115063]\n",
      "epoch:34 step:27238 [D loss: 0.029426, acc.: 100.00%] [G loss: 3.906001]\n",
      "epoch:34 step:27239 [D loss: 0.545874, acc.: 70.31%] [G loss: 5.164316]\n",
      "epoch:34 step:27240 [D loss: 0.008738, acc.: 100.00%] [G loss: 7.026888]\n",
      "epoch:34 step:27241 [D loss: 0.555600, acc.: 73.44%] [G loss: 2.423481]\n",
      "epoch:34 step:27242 [D loss: 0.405752, acc.: 79.69%] [G loss: 6.195678]\n",
      "epoch:34 step:27243 [D loss: 0.001475, acc.: 100.00%] [G loss: 7.133691]\n",
      "epoch:34 step:27244 [D loss: 0.128918, acc.: 95.31%] [G loss: 6.647326]\n",
      "epoch:34 step:27245 [D loss: 0.008956, acc.: 100.00%] [G loss: 6.228276]\n",
      "epoch:34 step:27246 [D loss: 0.006819, acc.: 100.00%] [G loss: 5.584344]\n",
      "epoch:34 step:27247 [D loss: 0.004028, acc.: 100.00%] [G loss: 5.177495]\n",
      "epoch:34 step:27248 [D loss: 0.005499, acc.: 100.00%] [G loss: 5.214218]\n",
      "epoch:34 step:27249 [D loss: 0.003895, acc.: 100.00%] [G loss: 4.472254]\n",
      "epoch:34 step:27250 [D loss: 0.017585, acc.: 100.00%] [G loss: 3.368721]\n",
      "epoch:34 step:27251 [D loss: 0.014279, acc.: 100.00%] [G loss: 3.268940]\n",
      "epoch:34 step:27252 [D loss: 0.014151, acc.: 100.00%] [G loss: 3.931559]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:34 step:27253 [D loss: 0.017521, acc.: 100.00%] [G loss: 4.074147]\n",
      "epoch:34 step:27254 [D loss: 0.011146, acc.: 100.00%] [G loss: 3.906952]\n",
      "epoch:34 step:27255 [D loss: 0.013628, acc.: 100.00%] [G loss: 2.925712]\n",
      "epoch:34 step:27256 [D loss: 0.011723, acc.: 100.00%] [G loss: 1.942301]\n",
      "epoch:34 step:27257 [D loss: 0.018047, acc.: 100.00%] [G loss: 3.724196]\n",
      "epoch:34 step:27258 [D loss: 0.047980, acc.: 99.22%] [G loss: 2.575933]\n",
      "epoch:34 step:27259 [D loss: 0.298247, acc.: 90.62%] [G loss: 4.195987]\n",
      "epoch:34 step:27260 [D loss: 0.003706, acc.: 100.00%] [G loss: 5.980026]\n",
      "epoch:34 step:27261 [D loss: 0.044247, acc.: 97.66%] [G loss: 5.134735]\n",
      "epoch:34 step:27262 [D loss: 0.013157, acc.: 100.00%] [G loss: 3.866322]\n",
      "epoch:34 step:27263 [D loss: 0.011578, acc.: 100.00%] [G loss: 3.673982]\n",
      "epoch:34 step:27264 [D loss: 0.034220, acc.: 100.00%] [G loss: 3.738451]\n",
      "epoch:34 step:27265 [D loss: 0.083963, acc.: 97.66%] [G loss: 4.898633]\n",
      "epoch:34 step:27266 [D loss: 0.004407, acc.: 100.00%] [G loss: 5.001035]\n",
      "epoch:34 step:27267 [D loss: 0.369924, acc.: 86.72%] [G loss: 2.079410]\n",
      "epoch:34 step:27268 [D loss: 0.262500, acc.: 90.62%] [G loss: 6.393208]\n",
      "epoch:34 step:27269 [D loss: 0.351749, acc.: 80.47%] [G loss: 4.600396]\n",
      "epoch:34 step:27270 [D loss: 0.031156, acc.: 100.00%] [G loss: 4.624168]\n",
      "epoch:34 step:27271 [D loss: 0.027168, acc.: 100.00%] [G loss: 4.718893]\n",
      "epoch:34 step:27272 [D loss: 0.018606, acc.: 100.00%] [G loss: 4.943659]\n",
      "epoch:34 step:27273 [D loss: 0.042761, acc.: 99.22%] [G loss: 5.679303]\n",
      "epoch:34 step:27274 [D loss: 0.037042, acc.: 99.22%] [G loss: 4.361583]\n",
      "epoch:34 step:27275 [D loss: 0.021450, acc.: 100.00%] [G loss: 4.304431]\n",
      "epoch:34 step:27276 [D loss: 0.020832, acc.: 100.00%] [G loss: 4.525485]\n",
      "epoch:34 step:27277 [D loss: 0.051622, acc.: 100.00%] [G loss: 4.120517]\n",
      "epoch:34 step:27278 [D loss: 0.048161, acc.: 100.00%] [G loss: 4.279374]\n",
      "epoch:34 step:27279 [D loss: 0.089286, acc.: 97.66%] [G loss: 2.774955]\n",
      "epoch:34 step:27280 [D loss: 0.011590, acc.: 100.00%] [G loss: 3.196127]\n",
      "epoch:34 step:27281 [D loss: 0.107956, acc.: 97.66%] [G loss: 5.046932]\n",
      "epoch:34 step:27282 [D loss: 0.018975, acc.: 99.22%] [G loss: 5.782220]\n",
      "epoch:34 step:27283 [D loss: 0.034919, acc.: 99.22%] [G loss: 5.327115]\n",
      "epoch:34 step:27284 [D loss: 0.012696, acc.: 100.00%] [G loss: 4.987274]\n",
      "epoch:34 step:27285 [D loss: 0.018252, acc.: 100.00%] [G loss: 4.797556]\n",
      "epoch:34 step:27286 [D loss: 0.025156, acc.: 100.00%] [G loss: 3.971575]\n",
      "epoch:34 step:27287 [D loss: 0.008109, acc.: 100.00%] [G loss: 3.788912]\n",
      "epoch:34 step:27288 [D loss: 0.034277, acc.: 99.22%] [G loss: 4.744648]\n",
      "epoch:34 step:27289 [D loss: 0.089260, acc.: 98.44%] [G loss: 4.249592]\n",
      "epoch:34 step:27290 [D loss: 0.016373, acc.: 100.00%] [G loss: 3.467631]\n",
      "epoch:34 step:27291 [D loss: 0.023824, acc.: 100.00%] [G loss: 3.819873]\n",
      "epoch:34 step:27292 [D loss: 0.007429, acc.: 100.00%] [G loss: 4.816140]\n",
      "epoch:34 step:27293 [D loss: 0.069795, acc.: 98.44%] [G loss: 3.353510]\n",
      "epoch:34 step:27294 [D loss: 0.107503, acc.: 97.66%] [G loss: 4.918525]\n",
      "epoch:34 step:27295 [D loss: 0.034024, acc.: 98.44%] [G loss: 5.923260]\n",
      "epoch:34 step:27296 [D loss: 0.021433, acc.: 100.00%] [G loss: 3.846731]\n",
      "epoch:34 step:27297 [D loss: 0.029887, acc.: 99.22%] [G loss: 4.906263]\n",
      "epoch:34 step:27298 [D loss: 0.027754, acc.: 100.00%] [G loss: 4.065467]\n",
      "epoch:34 step:27299 [D loss: 0.006233, acc.: 100.00%] [G loss: 3.928451]\n",
      "epoch:34 step:27300 [D loss: 0.014855, acc.: 100.00%] [G loss: 4.974904]\n",
      "epoch:34 step:27301 [D loss: 0.025071, acc.: 100.00%] [G loss: 3.639943]\n",
      "epoch:34 step:27302 [D loss: 0.027532, acc.: 100.00%] [G loss: 3.174805]\n",
      "epoch:34 step:27303 [D loss: 0.027615, acc.: 100.00%] [G loss: 1.944472]\n",
      "epoch:34 step:27304 [D loss: 0.024259, acc.: 100.00%] [G loss: 2.831912]\n",
      "epoch:34 step:27305 [D loss: 0.028073, acc.: 100.00%] [G loss: 3.668502]\n",
      "epoch:34 step:27306 [D loss: 0.007784, acc.: 100.00%] [G loss: 6.107996]\n",
      "epoch:34 step:27307 [D loss: 0.354976, acc.: 85.94%] [G loss: 6.726667]\n",
      "epoch:34 step:27308 [D loss: 0.358557, acc.: 81.25%] [G loss: 3.112728]\n",
      "epoch:34 step:27309 [D loss: 0.176835, acc.: 90.62%] [G loss: 6.532228]\n",
      "epoch:34 step:27310 [D loss: 0.009458, acc.: 100.00%] [G loss: 6.423532]\n",
      "epoch:34 step:27311 [D loss: 0.038466, acc.: 98.44%] [G loss: 6.597102]\n",
      "epoch:34 step:27312 [D loss: 0.008827, acc.: 100.00%] [G loss: 6.591673]\n",
      "epoch:34 step:27313 [D loss: 0.006240, acc.: 100.00%] [G loss: 5.937098]\n",
      "epoch:34 step:27314 [D loss: 0.027923, acc.: 100.00%] [G loss: 4.443959]\n",
      "epoch:34 step:27315 [D loss: 0.002358, acc.: 100.00%] [G loss: 4.489188]\n",
      "epoch:34 step:27316 [D loss: 0.013793, acc.: 100.00%] [G loss: 4.027739]\n",
      "epoch:34 step:27317 [D loss: 0.028297, acc.: 100.00%] [G loss: 4.377399]\n",
      "epoch:34 step:27318 [D loss: 0.008635, acc.: 100.00%] [G loss: 5.385050]\n",
      "epoch:34 step:27319 [D loss: 0.013648, acc.: 100.00%] [G loss: 4.515621]\n",
      "epoch:34 step:27320 [D loss: 0.032141, acc.: 99.22%] [G loss: 2.761799]\n",
      "epoch:34 step:27321 [D loss: 0.010523, acc.: 100.00%] [G loss: 3.244723]\n",
      "epoch:34 step:27322 [D loss: 0.077126, acc.: 97.66%] [G loss: 6.254069]\n",
      "epoch:34 step:27323 [D loss: 0.033436, acc.: 99.22%] [G loss: 7.649542]\n",
      "epoch:34 step:27324 [D loss: 0.390920, acc.: 82.81%] [G loss: 7.680368]\n",
      "epoch:34 step:27325 [D loss: 0.166797, acc.: 91.41%] [G loss: 4.780913]\n",
      "epoch:34 step:27326 [D loss: 0.008053, acc.: 100.00%] [G loss: 3.481343]\n",
      "epoch:34 step:27327 [D loss: 0.029639, acc.: 100.00%] [G loss: 4.198346]\n",
      "epoch:34 step:27328 [D loss: 0.016566, acc.: 100.00%] [G loss: 5.233019]\n",
      "epoch:34 step:27329 [D loss: 0.002086, acc.: 100.00%] [G loss: 6.191203]\n",
      "epoch:34 step:27330 [D loss: 0.032818, acc.: 99.22%] [G loss: 5.965105]\n",
      "epoch:34 step:27331 [D loss: 0.004898, acc.: 100.00%] [G loss: 5.750070]\n",
      "epoch:34 step:27332 [D loss: 0.004087, acc.: 100.00%] [G loss: 4.832770]\n",
      "epoch:34 step:27333 [D loss: 0.130371, acc.: 96.88%] [G loss: 3.086400]\n",
      "epoch:34 step:27334 [D loss: 0.021205, acc.: 100.00%] [G loss: 6.065001]\n",
      "epoch:34 step:27335 [D loss: 0.001483, acc.: 100.00%] [G loss: 6.932473]\n",
      "epoch:35 step:27336 [D loss: 0.034055, acc.: 100.00%] [G loss: 4.854656]\n",
      "epoch:35 step:27337 [D loss: 0.022889, acc.: 100.00%] [G loss: 3.998749]\n",
      "epoch:35 step:27338 [D loss: 0.013429, acc.: 100.00%] [G loss: 6.703918]\n",
      "epoch:35 step:27339 [D loss: 0.005165, acc.: 100.00%] [G loss: 6.187446]\n",
      "epoch:35 step:27340 [D loss: 0.010122, acc.: 100.00%] [G loss: 6.013367]\n",
      "epoch:35 step:27341 [D loss: 0.011965, acc.: 100.00%] [G loss: 6.094000]\n",
      "epoch:35 step:27342 [D loss: 0.027823, acc.: 100.00%] [G loss: 3.020272]\n",
      "epoch:35 step:27343 [D loss: 0.004151, acc.: 100.00%] [G loss: 3.939539]\n",
      "epoch:35 step:27344 [D loss: 0.027853, acc.: 99.22%] [G loss: 6.169089]\n",
      "epoch:35 step:27345 [D loss: 0.015157, acc.: 100.00%] [G loss: 5.268627]\n",
      "epoch:35 step:27346 [D loss: 0.006098, acc.: 100.00%] [G loss: 5.473036]\n",
      "epoch:35 step:27347 [D loss: 0.022987, acc.: 99.22%] [G loss: 5.052908]\n",
      "epoch:35 step:27348 [D loss: 0.014043, acc.: 100.00%] [G loss: 6.287425]\n",
      "epoch:35 step:27349 [D loss: 1.065000, acc.: 57.81%] [G loss: 10.330070]\n",
      "epoch:35 step:27350 [D loss: 3.369110, acc.: 50.00%] [G loss: 7.319365]\n",
      "epoch:35 step:27351 [D loss: 1.276313, acc.: 58.59%] [G loss: 0.715815]\n",
      "epoch:35 step:27352 [D loss: 1.975359, acc.: 54.69%] [G loss: 5.768576]\n",
      "epoch:35 step:27353 [D loss: 0.996194, acc.: 58.59%] [G loss: 5.620480]\n",
      "epoch:35 step:27354 [D loss: 0.572313, acc.: 73.44%] [G loss: 4.270270]\n",
      "epoch:35 step:27355 [D loss: 0.146778, acc.: 93.75%] [G loss: 3.237219]\n",
      "epoch:35 step:27356 [D loss: 0.018575, acc.: 100.00%] [G loss: 2.388011]\n",
      "epoch:35 step:27357 [D loss: 0.094665, acc.: 99.22%] [G loss: 2.920904]\n",
      "epoch:35 step:27358 [D loss: 0.265582, acc.: 89.06%] [G loss: 3.138862]\n",
      "epoch:35 step:27359 [D loss: 0.117142, acc.: 98.44%] [G loss: 3.292231]\n",
      "epoch:35 step:27360 [D loss: 0.106294, acc.: 96.88%] [G loss: 2.914076]\n",
      "epoch:35 step:27361 [D loss: 0.240635, acc.: 91.41%] [G loss: 3.612512]\n",
      "epoch:35 step:27362 [D loss: 0.047274, acc.: 99.22%] [G loss: 3.772867]\n",
      "epoch:35 step:27363 [D loss: 0.175380, acc.: 92.97%] [G loss: 1.722885]\n",
      "epoch:35 step:27364 [D loss: 0.123095, acc.: 97.66%] [G loss: 3.089973]\n",
      "epoch:35 step:27365 [D loss: 0.090980, acc.: 98.44%] [G loss: 2.711340]\n",
      "epoch:35 step:27366 [D loss: 0.020679, acc.: 100.00%] [G loss: 2.779721]\n",
      "epoch:35 step:27367 [D loss: 1.257793, acc.: 38.28%] [G loss: 3.291936]\n",
      "epoch:35 step:27368 [D loss: 0.147272, acc.: 93.75%] [G loss: 3.423808]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:35 step:27369 [D loss: 0.051985, acc.: 100.00%] [G loss: 3.226624]\n",
      "epoch:35 step:27370 [D loss: 0.165138, acc.: 95.31%] [G loss: 1.445375]\n",
      "epoch:35 step:27371 [D loss: 0.572892, acc.: 77.34%] [G loss: 5.201976]\n",
      "epoch:35 step:27372 [D loss: 0.337174, acc.: 82.03%] [G loss: 5.173241]\n",
      "epoch:35 step:27373 [D loss: 0.087164, acc.: 97.66%] [G loss: 4.230768]\n",
      "epoch:35 step:27374 [D loss: 0.025670, acc.: 100.00%] [G loss: 3.363226]\n",
      "epoch:35 step:27375 [D loss: 0.085251, acc.: 100.00%] [G loss: 2.810119]\n",
      "epoch:35 step:27376 [D loss: 0.067584, acc.: 99.22%] [G loss: 3.128822]\n",
      "epoch:35 step:27377 [D loss: 0.147573, acc.: 95.31%] [G loss: 3.716594]\n",
      "epoch:35 step:27378 [D loss: 0.021996, acc.: 99.22%] [G loss: 3.296470]\n",
      "epoch:35 step:27379 [D loss: 0.023933, acc.: 100.00%] [G loss: 2.763763]\n",
      "epoch:35 step:27380 [D loss: 0.134353, acc.: 96.09%] [G loss: 2.180356]\n",
      "epoch:35 step:27381 [D loss: 0.169445, acc.: 92.97%] [G loss: 4.758587]\n",
      "epoch:35 step:27382 [D loss: 0.078566, acc.: 97.66%] [G loss: 4.784123]\n",
      "epoch:35 step:27383 [D loss: 0.034751, acc.: 99.22%] [G loss: 4.412552]\n",
      "epoch:35 step:27384 [D loss: 0.121584, acc.: 96.88%] [G loss: 2.765811]\n",
      "epoch:35 step:27385 [D loss: 0.171073, acc.: 94.53%] [G loss: 4.486600]\n",
      "epoch:35 step:27386 [D loss: 0.016101, acc.: 100.00%] [G loss: 5.183164]\n",
      "epoch:35 step:27387 [D loss: 0.103316, acc.: 96.09%] [G loss: 3.425450]\n",
      "epoch:35 step:27388 [D loss: 0.020437, acc.: 100.00%] [G loss: 3.540682]\n",
      "epoch:35 step:27389 [D loss: 0.154111, acc.: 96.09%] [G loss: 5.022201]\n",
      "epoch:35 step:27390 [D loss: 0.010870, acc.: 100.00%] [G loss: 5.206761]\n",
      "epoch:35 step:27391 [D loss: 0.036437, acc.: 99.22%] [G loss: 4.456866]\n",
      "epoch:35 step:27392 [D loss: 0.029396, acc.: 100.00%] [G loss: 4.227737]\n",
      "epoch:35 step:27393 [D loss: 0.034228, acc.: 100.00%] [G loss: 4.218073]\n",
      "epoch:35 step:27394 [D loss: 0.021544, acc.: 100.00%] [G loss: 4.668576]\n",
      "epoch:35 step:27395 [D loss: 0.007728, acc.: 100.00%] [G loss: 4.804383]\n",
      "epoch:35 step:27396 [D loss: 0.021141, acc.: 100.00%] [G loss: 4.099568]\n",
      "epoch:35 step:27397 [D loss: 0.038367, acc.: 100.00%] [G loss: 3.419377]\n",
      "epoch:35 step:27398 [D loss: 0.148612, acc.: 97.66%] [G loss: 4.252816]\n",
      "epoch:35 step:27399 [D loss: 0.049829, acc.: 98.44%] [G loss: 4.150539]\n",
      "epoch:35 step:27400 [D loss: 0.251655, acc.: 90.62%] [G loss: 5.004561]\n",
      "##############\n",
      "[0.94324362 0.92041281 0.81437284 0.91658851 2.11399687 2.11817272\n",
      " 2.10108778 0.85820085 2.10340478 1.0648924 ]\n",
      "##########\n",
      "epoch:35 step:27401 [D loss: 0.106792, acc.: 95.31%] [G loss: 4.420424]\n",
      "epoch:35 step:27402 [D loss: 0.047239, acc.: 99.22%] [G loss: 2.952325]\n",
      "epoch:35 step:27403 [D loss: 0.006060, acc.: 100.00%] [G loss: 3.772461]\n",
      "epoch:35 step:27404 [D loss: 0.024155, acc.: 100.00%] [G loss: 3.490673]\n",
      "epoch:35 step:27405 [D loss: 0.016313, acc.: 100.00%] [G loss: 4.563338]\n",
      "epoch:35 step:27406 [D loss: 0.005105, acc.: 100.00%] [G loss: 3.641081]\n",
      "epoch:35 step:27407 [D loss: 0.007731, acc.: 100.00%] [G loss: 2.053385]\n",
      "epoch:35 step:27408 [D loss: 0.019097, acc.: 99.22%] [G loss: 2.511971]\n",
      "epoch:35 step:27409 [D loss: 0.022807, acc.: 100.00%] [G loss: 3.290998]\n",
      "epoch:35 step:27410 [D loss: 0.123454, acc.: 97.66%] [G loss: 2.443232]\n",
      "epoch:35 step:27411 [D loss: 0.017624, acc.: 100.00%] [G loss: 3.298430]\n",
      "epoch:35 step:27412 [D loss: 0.079198, acc.: 98.44%] [G loss: 0.612904]\n",
      "epoch:35 step:27413 [D loss: 0.043617, acc.: 100.00%] [G loss: 1.625881]\n",
      "epoch:35 step:27414 [D loss: 0.013327, acc.: 100.00%] [G loss: 2.341826]\n",
      "epoch:35 step:27415 [D loss: 0.279179, acc.: 89.06%] [G loss: 6.965106]\n",
      "epoch:35 step:27416 [D loss: 1.278731, acc.: 50.78%] [G loss: 1.481281]\n",
      "epoch:35 step:27417 [D loss: 0.417260, acc.: 78.91%] [G loss: 6.628707]\n",
      "epoch:35 step:27418 [D loss: 0.930364, acc.: 59.38%] [G loss: 4.536375]\n",
      "epoch:35 step:27419 [D loss: 0.342014, acc.: 80.47%] [G loss: 6.016348]\n",
      "epoch:35 step:27420 [D loss: 0.010598, acc.: 100.00%] [G loss: 6.422346]\n",
      "epoch:35 step:27421 [D loss: 0.026273, acc.: 99.22%] [G loss: 6.726715]\n",
      "epoch:35 step:27422 [D loss: 0.094885, acc.: 97.66%] [G loss: 4.618314]\n",
      "epoch:35 step:27423 [D loss: 0.042807, acc.: 99.22%] [G loss: 3.837218]\n",
      "epoch:35 step:27424 [D loss: 0.036918, acc.: 99.22%] [G loss: 4.008322]\n",
      "epoch:35 step:27425 [D loss: 0.151981, acc.: 93.75%] [G loss: 2.221303]\n",
      "epoch:35 step:27426 [D loss: 0.358936, acc.: 85.16%] [G loss: 5.781033]\n",
      "epoch:35 step:27427 [D loss: 0.002065, acc.: 100.00%] [G loss: 6.953830]\n",
      "epoch:35 step:27428 [D loss: 0.581951, acc.: 70.31%] [G loss: 3.342135]\n",
      "epoch:35 step:27429 [D loss: 0.072073, acc.: 98.44%] [G loss: 3.653645]\n",
      "epoch:35 step:27430 [D loss: 0.169931, acc.: 93.75%] [G loss: 4.752682]\n",
      "epoch:35 step:27431 [D loss: 0.005035, acc.: 100.00%] [G loss: 5.279243]\n",
      "epoch:35 step:27432 [D loss: 0.031275, acc.: 99.22%] [G loss: 5.241328]\n",
      "epoch:35 step:27433 [D loss: 0.013365, acc.: 100.00%] [G loss: 4.650612]\n",
      "epoch:35 step:27434 [D loss: 0.070640, acc.: 98.44%] [G loss: 4.197231]\n",
      "epoch:35 step:27435 [D loss: 0.021720, acc.: 100.00%] [G loss: 3.729568]\n",
      "epoch:35 step:27436 [D loss: 0.007974, acc.: 100.00%] [G loss: 3.662151]\n",
      "epoch:35 step:27437 [D loss: 0.045415, acc.: 99.22%] [G loss: 4.234893]\n",
      "epoch:35 step:27438 [D loss: 0.007649, acc.: 100.00%] [G loss: 3.620563]\n",
      "epoch:35 step:27439 [D loss: 0.184789, acc.: 94.53%] [G loss: 4.560244]\n",
      "epoch:35 step:27440 [D loss: 0.079082, acc.: 98.44%] [G loss: 3.848781]\n",
      "epoch:35 step:27441 [D loss: 0.003766, acc.: 100.00%] [G loss: 3.438215]\n",
      "epoch:35 step:27442 [D loss: 0.053102, acc.: 99.22%] [G loss: 3.195738]\n",
      "epoch:35 step:27443 [D loss: 0.006038, acc.: 100.00%] [G loss: 4.666191]\n",
      "epoch:35 step:27444 [D loss: 0.017290, acc.: 100.00%] [G loss: 4.785861]\n",
      "epoch:35 step:27445 [D loss: 0.035792, acc.: 100.00%] [G loss: 4.655139]\n",
      "epoch:35 step:27446 [D loss: 0.026490, acc.: 100.00%] [G loss: 3.117498]\n",
      "epoch:35 step:27447 [D loss: 0.030369, acc.: 100.00%] [G loss: 4.005007]\n",
      "epoch:35 step:27448 [D loss: 0.010357, acc.: 100.00%] [G loss: 5.218895]\n",
      "epoch:35 step:27449 [D loss: 0.033418, acc.: 99.22%] [G loss: 4.779713]\n",
      "epoch:35 step:27450 [D loss: 0.011521, acc.: 100.00%] [G loss: 4.488172]\n",
      "epoch:35 step:27451 [D loss: 1.259007, acc.: 50.78%] [G loss: 8.120302]\n",
      "epoch:35 step:27452 [D loss: 1.692437, acc.: 50.78%] [G loss: 5.951977]\n",
      "epoch:35 step:27453 [D loss: 0.010436, acc.: 100.00%] [G loss: 5.017186]\n",
      "epoch:35 step:27454 [D loss: 0.016453, acc.: 99.22%] [G loss: 4.007050]\n",
      "epoch:35 step:27455 [D loss: 0.025795, acc.: 99.22%] [G loss: 4.334682]\n",
      "epoch:35 step:27456 [D loss: 0.051533, acc.: 99.22%] [G loss: 4.581054]\n",
      "epoch:35 step:27457 [D loss: 0.036932, acc.: 99.22%] [G loss: 4.442848]\n",
      "epoch:35 step:27458 [D loss: 0.014532, acc.: 100.00%] [G loss: 4.811110]\n",
      "epoch:35 step:27459 [D loss: 0.014539, acc.: 100.00%] [G loss: 5.102063]\n",
      "epoch:35 step:27460 [D loss: 0.026967, acc.: 99.22%] [G loss: 4.278846]\n",
      "epoch:35 step:27461 [D loss: 0.040183, acc.: 99.22%] [G loss: 4.872848]\n",
      "epoch:35 step:27462 [D loss: 0.007650, acc.: 100.00%] [G loss: 4.609601]\n",
      "epoch:35 step:27463 [D loss: 0.106032, acc.: 96.09%] [G loss: 3.601557]\n",
      "epoch:35 step:27464 [D loss: 0.038062, acc.: 100.00%] [G loss: 3.755698]\n",
      "epoch:35 step:27465 [D loss: 0.013118, acc.: 100.00%] [G loss: 4.116016]\n",
      "epoch:35 step:27466 [D loss: 0.011761, acc.: 100.00%] [G loss: 4.865639]\n",
      "epoch:35 step:27467 [D loss: 0.154836, acc.: 96.88%] [G loss: 3.568914]\n",
      "epoch:35 step:27468 [D loss: 0.019478, acc.: 100.00%] [G loss: 4.307719]\n",
      "epoch:35 step:27469 [D loss: 0.006909, acc.: 100.00%] [G loss: 4.381684]\n",
      "epoch:35 step:27470 [D loss: 0.008497, acc.: 100.00%] [G loss: 3.925055]\n",
      "epoch:35 step:27471 [D loss: 0.078866, acc.: 99.22%] [G loss: 3.646655]\n",
      "epoch:35 step:27472 [D loss: 0.019648, acc.: 100.00%] [G loss: 4.246366]\n",
      "epoch:35 step:27473 [D loss: 0.015276, acc.: 100.00%] [G loss: 4.382543]\n",
      "epoch:35 step:27474 [D loss: 0.006465, acc.: 100.00%] [G loss: 4.921791]\n",
      "epoch:35 step:27475 [D loss: 0.151713, acc.: 97.66%] [G loss: 2.757191]\n",
      "epoch:35 step:27476 [D loss: 0.051459, acc.: 98.44%] [G loss: 5.198349]\n",
      "epoch:35 step:27477 [D loss: 0.002844, acc.: 100.00%] [G loss: 5.995384]\n",
      "epoch:35 step:27478 [D loss: 0.052440, acc.: 99.22%] [G loss: 4.165141]\n",
      "epoch:35 step:27479 [D loss: 0.010091, acc.: 100.00%] [G loss: 3.990058]\n",
      "epoch:35 step:27480 [D loss: 0.022800, acc.: 100.00%] [G loss: 4.167766]\n",
      "epoch:35 step:27481 [D loss: 0.007248, acc.: 100.00%] [G loss: 4.449230]\n",
      "epoch:35 step:27482 [D loss: 0.022576, acc.: 100.00%] [G loss: 4.742114]\n",
      "epoch:35 step:27483 [D loss: 0.003806, acc.: 100.00%] [G loss: 4.726864]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:35 step:27484 [D loss: 0.006486, acc.: 100.00%] [G loss: 4.835348]\n",
      "epoch:35 step:27485 [D loss: 0.008400, acc.: 100.00%] [G loss: 4.172161]\n",
      "epoch:35 step:27486 [D loss: 0.015015, acc.: 100.00%] [G loss: 4.289119]\n",
      "epoch:35 step:27487 [D loss: 0.035126, acc.: 100.00%] [G loss: 3.469420]\n",
      "epoch:35 step:27488 [D loss: 1.002881, acc.: 53.12%] [G loss: 7.469814]\n",
      "epoch:35 step:27489 [D loss: 0.742916, acc.: 66.41%] [G loss: 6.059978]\n",
      "epoch:35 step:27490 [D loss: 0.062141, acc.: 96.88%] [G loss: 4.516112]\n",
      "epoch:35 step:27491 [D loss: 0.119811, acc.: 97.66%] [G loss: 3.570919]\n",
      "epoch:35 step:27492 [D loss: 0.158062, acc.: 95.31%] [G loss: 4.637465]\n",
      "epoch:35 step:27493 [D loss: 0.035784, acc.: 99.22%] [G loss: 5.448669]\n",
      "epoch:35 step:27494 [D loss: 0.008723, acc.: 100.00%] [G loss: 5.536690]\n",
      "epoch:35 step:27495 [D loss: 0.043389, acc.: 99.22%] [G loss: 5.019252]\n",
      "epoch:35 step:27496 [D loss: 0.169935, acc.: 93.75%] [G loss: 2.543945]\n",
      "epoch:35 step:27497 [D loss: 0.159044, acc.: 95.31%] [G loss: 4.183798]\n",
      "epoch:35 step:27498 [D loss: 0.015758, acc.: 99.22%] [G loss: 5.174396]\n",
      "epoch:35 step:27499 [D loss: 0.007996, acc.: 100.00%] [G loss: 4.983673]\n",
      "epoch:35 step:27500 [D loss: 0.116155, acc.: 96.09%] [G loss: 4.259884]\n",
      "epoch:35 step:27501 [D loss: 0.064907, acc.: 99.22%] [G loss: 3.950550]\n",
      "epoch:35 step:27502 [D loss: 0.017521, acc.: 100.00%] [G loss: 4.889620]\n",
      "epoch:35 step:27503 [D loss: 0.010906, acc.: 100.00%] [G loss: 4.377267]\n",
      "epoch:35 step:27504 [D loss: 0.026030, acc.: 100.00%] [G loss: 5.023239]\n",
      "epoch:35 step:27505 [D loss: 0.023682, acc.: 100.00%] [G loss: 4.159586]\n",
      "epoch:35 step:27506 [D loss: 0.023204, acc.: 100.00%] [G loss: 5.084751]\n",
      "epoch:35 step:27507 [D loss: 0.010477, acc.: 100.00%] [G loss: 3.883403]\n",
      "epoch:35 step:27508 [D loss: 0.011543, acc.: 100.00%] [G loss: 3.376303]\n",
      "epoch:35 step:27509 [D loss: 0.029787, acc.: 100.00%] [G loss: 3.096016]\n",
      "epoch:35 step:27510 [D loss: 0.036832, acc.: 99.22%] [G loss: 3.521121]\n",
      "epoch:35 step:27511 [D loss: 0.586177, acc.: 68.75%] [G loss: 7.404469]\n",
      "epoch:35 step:27512 [D loss: 0.786291, acc.: 66.41%] [G loss: 3.129965]\n",
      "epoch:35 step:27513 [D loss: 0.121658, acc.: 95.31%] [G loss: 4.638195]\n",
      "epoch:35 step:27514 [D loss: 0.041236, acc.: 100.00%] [G loss: 4.946136]\n",
      "epoch:35 step:27515 [D loss: 0.010042, acc.: 100.00%] [G loss: 5.632857]\n",
      "epoch:35 step:27516 [D loss: 0.007517, acc.: 100.00%] [G loss: 5.235080]\n",
      "epoch:35 step:27517 [D loss: 0.049447, acc.: 100.00%] [G loss: 3.995376]\n",
      "epoch:35 step:27518 [D loss: 0.009566, acc.: 100.00%] [G loss: 3.960412]\n",
      "epoch:35 step:27519 [D loss: 0.010463, acc.: 100.00%] [G loss: 3.116756]\n",
      "epoch:35 step:27520 [D loss: 0.045850, acc.: 100.00%] [G loss: 4.798591]\n",
      "epoch:35 step:27521 [D loss: 0.009267, acc.: 100.00%] [G loss: 4.043337]\n",
      "epoch:35 step:27522 [D loss: 1.178294, acc.: 50.78%] [G loss: 6.820728]\n",
      "epoch:35 step:27523 [D loss: 0.236096, acc.: 89.06%] [G loss: 7.460955]\n",
      "epoch:35 step:27524 [D loss: 0.107496, acc.: 95.31%] [G loss: 6.288523]\n",
      "epoch:35 step:27525 [D loss: 0.013535, acc.: 100.00%] [G loss: 5.308819]\n",
      "epoch:35 step:27526 [D loss: 0.024752, acc.: 99.22%] [G loss: 4.392832]\n",
      "epoch:35 step:27527 [D loss: 0.007414, acc.: 100.00%] [G loss: 4.143894]\n",
      "epoch:35 step:27528 [D loss: 0.055027, acc.: 97.66%] [G loss: 4.494620]\n",
      "epoch:35 step:27529 [D loss: 0.005615, acc.: 100.00%] [G loss: 5.284399]\n",
      "epoch:35 step:27530 [D loss: 0.006441, acc.: 100.00%] [G loss: 4.964165]\n",
      "epoch:35 step:27531 [D loss: 0.010720, acc.: 100.00%] [G loss: 5.134818]\n",
      "epoch:35 step:27532 [D loss: 0.020946, acc.: 99.22%] [G loss: 4.499856]\n",
      "epoch:35 step:27533 [D loss: 0.009809, acc.: 100.00%] [G loss: 5.023507]\n",
      "epoch:35 step:27534 [D loss: 0.007638, acc.: 100.00%] [G loss: 4.656505]\n",
      "epoch:35 step:27535 [D loss: 0.039682, acc.: 100.00%] [G loss: 5.091627]\n",
      "epoch:35 step:27536 [D loss: 0.028838, acc.: 100.00%] [G loss: 5.019368]\n",
      "epoch:35 step:27537 [D loss: 0.007065, acc.: 100.00%] [G loss: 4.854089]\n",
      "epoch:35 step:27538 [D loss: 0.007666, acc.: 100.00%] [G loss: 5.013156]\n",
      "epoch:35 step:27539 [D loss: 0.008264, acc.: 100.00%] [G loss: 4.948220]\n",
      "epoch:35 step:27540 [D loss: 0.023865, acc.: 99.22%] [G loss: 4.543036]\n",
      "epoch:35 step:27541 [D loss: 0.010407, acc.: 100.00%] [G loss: 4.490213]\n",
      "epoch:35 step:27542 [D loss: 0.008296, acc.: 100.00%] [G loss: 3.929058]\n",
      "epoch:35 step:27543 [D loss: 0.031089, acc.: 100.00%] [G loss: 4.353843]\n",
      "epoch:35 step:27544 [D loss: 0.018381, acc.: 100.00%] [G loss: 4.426203]\n",
      "epoch:35 step:27545 [D loss: 0.024822, acc.: 100.00%] [G loss: 3.478506]\n",
      "epoch:35 step:27546 [D loss: 0.034955, acc.: 99.22%] [G loss: 4.638087]\n",
      "epoch:35 step:27547 [D loss: 0.076807, acc.: 98.44%] [G loss: 4.368537]\n",
      "epoch:35 step:27548 [D loss: 0.010917, acc.: 100.00%] [G loss: 4.043839]\n",
      "epoch:35 step:27549 [D loss: 0.373818, acc.: 81.25%] [G loss: 5.561733]\n",
      "epoch:35 step:27550 [D loss: 0.004156, acc.: 100.00%] [G loss: 6.278277]\n",
      "epoch:35 step:27551 [D loss: 0.243196, acc.: 88.28%] [G loss: 3.522532]\n",
      "epoch:35 step:27552 [D loss: 0.048617, acc.: 98.44%] [G loss: 3.801346]\n",
      "epoch:35 step:27553 [D loss: 0.027014, acc.: 100.00%] [G loss: 3.159544]\n",
      "epoch:35 step:27554 [D loss: 0.016408, acc.: 100.00%] [G loss: 4.541525]\n",
      "epoch:35 step:27555 [D loss: 0.029739, acc.: 99.22%] [G loss: 3.478575]\n",
      "epoch:35 step:27556 [D loss: 0.026026, acc.: 100.00%] [G loss: 4.362495]\n",
      "epoch:35 step:27557 [D loss: 0.024717, acc.: 100.00%] [G loss: 4.947280]\n",
      "epoch:35 step:27558 [D loss: 0.006755, acc.: 100.00%] [G loss: 5.474885]\n",
      "epoch:35 step:27559 [D loss: 0.013353, acc.: 99.22%] [G loss: 5.277868]\n",
      "epoch:35 step:27560 [D loss: 0.012037, acc.: 100.00%] [G loss: 4.055609]\n",
      "epoch:35 step:27561 [D loss: 0.009007, acc.: 100.00%] [G loss: 4.130618]\n",
      "epoch:35 step:27562 [D loss: 0.009619, acc.: 100.00%] [G loss: 4.370280]\n",
      "epoch:35 step:27563 [D loss: 0.046499, acc.: 100.00%] [G loss: 4.398842]\n",
      "epoch:35 step:27564 [D loss: 0.056372, acc.: 99.22%] [G loss: 4.554627]\n",
      "epoch:35 step:27565 [D loss: 0.007916, acc.: 100.00%] [G loss: 4.654729]\n",
      "epoch:35 step:27566 [D loss: 0.021554, acc.: 100.00%] [G loss: 5.446669]\n",
      "epoch:35 step:27567 [D loss: 0.066890, acc.: 97.66%] [G loss: 3.698792]\n",
      "epoch:35 step:27568 [D loss: 0.018323, acc.: 100.00%] [G loss: 3.019119]\n",
      "epoch:35 step:27569 [D loss: 0.041026, acc.: 100.00%] [G loss: 4.131429]\n",
      "epoch:35 step:27570 [D loss: 0.008751, acc.: 100.00%] [G loss: 4.445674]\n",
      "epoch:35 step:27571 [D loss: 0.014308, acc.: 100.00%] [G loss: 4.432310]\n",
      "epoch:35 step:27572 [D loss: 0.031352, acc.: 100.00%] [G loss: 4.437930]\n",
      "epoch:35 step:27573 [D loss: 0.003881, acc.: 100.00%] [G loss: 4.428778]\n",
      "epoch:35 step:27574 [D loss: 0.008772, acc.: 100.00%] [G loss: 4.847551]\n",
      "epoch:35 step:27575 [D loss: 0.523658, acc.: 72.66%] [G loss: 7.929737]\n",
      "epoch:35 step:27576 [D loss: 2.232096, acc.: 50.78%] [G loss: 5.454161]\n",
      "epoch:35 step:27577 [D loss: 0.019818, acc.: 100.00%] [G loss: 4.340971]\n",
      "epoch:35 step:27578 [D loss: 0.035420, acc.: 100.00%] [G loss: 4.286306]\n",
      "epoch:35 step:27579 [D loss: 0.009646, acc.: 100.00%] [G loss: 3.649944]\n",
      "epoch:35 step:27580 [D loss: 0.012735, acc.: 100.00%] [G loss: 4.441058]\n",
      "epoch:35 step:27581 [D loss: 0.031991, acc.: 99.22%] [G loss: 3.535388]\n",
      "epoch:35 step:27582 [D loss: 0.079609, acc.: 98.44%] [G loss: 3.714443]\n",
      "epoch:35 step:27583 [D loss: 0.031358, acc.: 99.22%] [G loss: 3.761328]\n",
      "epoch:35 step:27584 [D loss: 0.014039, acc.: 100.00%] [G loss: 3.767988]\n",
      "epoch:35 step:27585 [D loss: 0.208421, acc.: 92.97%] [G loss: 4.858218]\n",
      "epoch:35 step:27586 [D loss: 0.008497, acc.: 100.00%] [G loss: 5.463583]\n",
      "epoch:35 step:27587 [D loss: 0.471786, acc.: 74.22%] [G loss: 3.012977]\n",
      "epoch:35 step:27588 [D loss: 0.024722, acc.: 100.00%] [G loss: 4.285949]\n",
      "epoch:35 step:27589 [D loss: 0.010024, acc.: 100.00%] [G loss: 4.229645]\n",
      "epoch:35 step:27590 [D loss: 0.011006, acc.: 100.00%] [G loss: 3.996494]\n",
      "epoch:35 step:27591 [D loss: 0.014074, acc.: 100.00%] [G loss: 4.167346]\n",
      "epoch:35 step:27592 [D loss: 0.009921, acc.: 100.00%] [G loss: 4.857810]\n",
      "epoch:35 step:27593 [D loss: 0.021249, acc.: 99.22%] [G loss: 4.153460]\n",
      "epoch:35 step:27594 [D loss: 0.006237, acc.: 100.00%] [G loss: 3.698257]\n",
      "epoch:35 step:27595 [D loss: 0.036540, acc.: 100.00%] [G loss: 4.007969]\n",
      "epoch:35 step:27596 [D loss: 0.058591, acc.: 99.22%] [G loss: 5.220413]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:35 step:27597 [D loss: 0.014503, acc.: 100.00%] [G loss: 5.561243]\n",
      "epoch:35 step:27598 [D loss: 0.007389, acc.: 100.00%] [G loss: 4.992919]\n",
      "epoch:35 step:27599 [D loss: 0.318663, acc.: 84.38%] [G loss: 4.008755]\n",
      "epoch:35 step:27600 [D loss: 0.018749, acc.: 100.00%] [G loss: 4.370405]\n",
      "##############\n",
      "[1.02665576 0.90919175 0.99206592 0.98899747 2.11012856 1.11364566\n",
      " 0.89175398 2.11848434 0.96092175 2.12770109]\n",
      "##########\n",
      "epoch:35 step:27601 [D loss: 0.008069, acc.: 100.00%] [G loss: 5.039789]\n",
      "epoch:35 step:27602 [D loss: 0.007815, acc.: 100.00%] [G loss: 5.335084]\n",
      "epoch:35 step:27603 [D loss: 0.008514, acc.: 100.00%] [G loss: 4.691647]\n",
      "epoch:35 step:27604 [D loss: 0.008188, acc.: 100.00%] [G loss: 4.312848]\n",
      "epoch:35 step:27605 [D loss: 0.028535, acc.: 99.22%] [G loss: 4.799430]\n",
      "epoch:35 step:27606 [D loss: 0.010668, acc.: 100.00%] [G loss: 4.089778]\n",
      "epoch:35 step:27607 [D loss: 0.007535, acc.: 100.00%] [G loss: 3.820845]\n",
      "epoch:35 step:27608 [D loss: 0.076101, acc.: 97.66%] [G loss: 5.558840]\n",
      "epoch:35 step:27609 [D loss: 0.024040, acc.: 99.22%] [G loss: 5.867018]\n",
      "epoch:35 step:27610 [D loss: 0.034266, acc.: 99.22%] [G loss: 4.641714]\n",
      "epoch:35 step:27611 [D loss: 0.012454, acc.: 100.00%] [G loss: 4.055217]\n",
      "epoch:35 step:27612 [D loss: 0.007259, acc.: 100.00%] [G loss: 4.668499]\n",
      "epoch:35 step:27613 [D loss: 0.011702, acc.: 100.00%] [G loss: 3.710521]\n",
      "epoch:35 step:27614 [D loss: 0.007284, acc.: 100.00%] [G loss: 3.952045]\n",
      "epoch:35 step:27615 [D loss: 0.009812, acc.: 100.00%] [G loss: 4.522071]\n",
      "epoch:35 step:27616 [D loss: 0.007374, acc.: 100.00%] [G loss: 3.743598]\n",
      "epoch:35 step:27617 [D loss: 0.013971, acc.: 100.00%] [G loss: 4.218000]\n",
      "epoch:35 step:27618 [D loss: 0.013465, acc.: 100.00%] [G loss: 4.089738]\n",
      "epoch:35 step:27619 [D loss: 0.003860, acc.: 100.00%] [G loss: 4.464652]\n",
      "epoch:35 step:27620 [D loss: 0.010083, acc.: 100.00%] [G loss: 4.295658]\n",
      "epoch:35 step:27621 [D loss: 0.883611, acc.: 57.03%] [G loss: 7.414402]\n",
      "epoch:35 step:27622 [D loss: 1.530022, acc.: 51.56%] [G loss: 3.167846]\n",
      "epoch:35 step:27623 [D loss: 0.846357, acc.: 72.66%] [G loss: 5.819374]\n",
      "epoch:35 step:27624 [D loss: 0.893697, acc.: 60.94%] [G loss: 3.540386]\n",
      "epoch:35 step:27625 [D loss: 0.197699, acc.: 92.19%] [G loss: 3.585186]\n",
      "epoch:35 step:27626 [D loss: 0.047884, acc.: 100.00%] [G loss: 3.332516]\n",
      "epoch:35 step:27627 [D loss: 0.062790, acc.: 100.00%] [G loss: 3.527839]\n",
      "epoch:35 step:27628 [D loss: 0.064605, acc.: 98.44%] [G loss: 3.081139]\n",
      "epoch:35 step:27629 [D loss: 0.144642, acc.: 97.66%] [G loss: 4.547668]\n",
      "epoch:35 step:27630 [D loss: 0.027919, acc.: 100.00%] [G loss: 4.724933]\n",
      "epoch:35 step:27631 [D loss: 0.173050, acc.: 92.19%] [G loss: 3.353319]\n",
      "epoch:35 step:27632 [D loss: 0.183152, acc.: 93.75%] [G loss: 3.218379]\n",
      "epoch:35 step:27633 [D loss: 0.136591, acc.: 96.88%] [G loss: 3.190347]\n",
      "epoch:35 step:27634 [D loss: 0.087729, acc.: 98.44%] [G loss: 3.230792]\n",
      "epoch:35 step:27635 [D loss: 0.115222, acc.: 97.66%] [G loss: 3.404065]\n",
      "epoch:35 step:27636 [D loss: 0.042209, acc.: 100.00%] [G loss: 3.092663]\n",
      "epoch:35 step:27637 [D loss: 0.117607, acc.: 96.88%] [G loss: 1.102093]\n",
      "epoch:35 step:27638 [D loss: 0.121632, acc.: 98.44%] [G loss: 4.592672]\n",
      "epoch:35 step:27639 [D loss: 0.093540, acc.: 96.09%] [G loss: 4.731048]\n",
      "epoch:35 step:27640 [D loss: 0.686190, acc.: 66.41%] [G loss: 6.005970]\n",
      "epoch:35 step:27641 [D loss: 0.498238, acc.: 70.31%] [G loss: 4.265336]\n",
      "epoch:35 step:27642 [D loss: 0.034466, acc.: 100.00%] [G loss: 3.179417]\n",
      "epoch:35 step:27643 [D loss: 0.055532, acc.: 99.22%] [G loss: 3.571079]\n",
      "epoch:35 step:27644 [D loss: 0.026746, acc.: 100.00%] [G loss: 4.165777]\n",
      "epoch:35 step:27645 [D loss: 0.026899, acc.: 100.00%] [G loss: 4.429759]\n",
      "epoch:35 step:27646 [D loss: 0.035170, acc.: 100.00%] [G loss: 4.143781]\n",
      "epoch:35 step:27647 [D loss: 0.302360, acc.: 86.72%] [G loss: 5.344437]\n",
      "epoch:35 step:27648 [D loss: 0.083842, acc.: 96.88%] [G loss: 4.716651]\n",
      "epoch:35 step:27649 [D loss: 0.009858, acc.: 100.00%] [G loss: 4.913910]\n",
      "epoch:35 step:27650 [D loss: 0.139586, acc.: 94.53%] [G loss: 4.704263]\n",
      "epoch:35 step:27651 [D loss: 0.014130, acc.: 100.00%] [G loss: 5.427469]\n",
      "epoch:35 step:27652 [D loss: 0.053625, acc.: 99.22%] [G loss: 4.530598]\n",
      "epoch:35 step:27653 [D loss: 0.014330, acc.: 100.00%] [G loss: 5.010106]\n",
      "epoch:35 step:27654 [D loss: 0.143711, acc.: 96.09%] [G loss: 3.593163]\n",
      "epoch:35 step:27655 [D loss: 1.492036, acc.: 40.62%] [G loss: 7.063435]\n",
      "epoch:35 step:27656 [D loss: 0.358589, acc.: 82.03%] [G loss: 7.008910]\n",
      "epoch:35 step:27657 [D loss: 0.042605, acc.: 98.44%] [G loss: 5.587358]\n",
      "epoch:35 step:27658 [D loss: 0.013570, acc.: 100.00%] [G loss: 4.495037]\n",
      "epoch:35 step:27659 [D loss: 0.030258, acc.: 99.22%] [G loss: 3.777977]\n",
      "epoch:35 step:27660 [D loss: 0.033263, acc.: 100.00%] [G loss: 4.038136]\n",
      "epoch:35 step:27661 [D loss: 0.026046, acc.: 100.00%] [G loss: 3.575346]\n",
      "epoch:35 step:27662 [D loss: 0.027045, acc.: 100.00%] [G loss: 4.614757]\n",
      "epoch:35 step:27663 [D loss: 0.058518, acc.: 98.44%] [G loss: 4.911686]\n",
      "epoch:35 step:27664 [D loss: 0.079765, acc.: 98.44%] [G loss: 4.294962]\n",
      "epoch:35 step:27665 [D loss: 0.065212, acc.: 96.88%] [G loss: 3.107145]\n",
      "epoch:35 step:27666 [D loss: 0.072692, acc.: 99.22%] [G loss: 3.003758]\n",
      "epoch:35 step:27667 [D loss: 0.050197, acc.: 99.22%] [G loss: 2.716308]\n",
      "epoch:35 step:27668 [D loss: 0.034378, acc.: 100.00%] [G loss: 2.607297]\n",
      "epoch:35 step:27669 [D loss: 0.312251, acc.: 91.41%] [G loss: 5.641654]\n",
      "epoch:35 step:27670 [D loss: 0.068261, acc.: 97.66%] [G loss: 6.247894]\n",
      "epoch:35 step:27671 [D loss: 0.465201, acc.: 79.69%] [G loss: 2.569459]\n",
      "epoch:35 step:27672 [D loss: 0.106358, acc.: 99.22%] [G loss: 3.985954]\n",
      "epoch:35 step:27673 [D loss: 0.021433, acc.: 99.22%] [G loss: 4.846995]\n",
      "epoch:35 step:27674 [D loss: 0.020989, acc.: 100.00%] [G loss: 4.653605]\n",
      "epoch:35 step:27675 [D loss: 0.092293, acc.: 98.44%] [G loss: 2.689871]\n",
      "epoch:35 step:27676 [D loss: 0.122284, acc.: 96.88%] [G loss: 4.628801]\n",
      "epoch:35 step:27677 [D loss: 0.012193, acc.: 100.00%] [G loss: 5.095306]\n",
      "epoch:35 step:27678 [D loss: 0.059667, acc.: 98.44%] [G loss: 4.401883]\n",
      "epoch:35 step:27679 [D loss: 0.028851, acc.: 100.00%] [G loss: 4.571758]\n",
      "epoch:35 step:27680 [D loss: 0.034921, acc.: 100.00%] [G loss: 3.509540]\n",
      "epoch:35 step:27681 [D loss: 0.030962, acc.: 100.00%] [G loss: 3.810970]\n",
      "epoch:35 step:27682 [D loss: 0.022218, acc.: 100.00%] [G loss: 4.189578]\n",
      "epoch:35 step:27683 [D loss: 0.021927, acc.: 100.00%] [G loss: 4.187358]\n",
      "epoch:35 step:27684 [D loss: 0.069007, acc.: 97.66%] [G loss: 3.386827]\n",
      "epoch:35 step:27685 [D loss: 0.097962, acc.: 99.22%] [G loss: 4.263610]\n",
      "epoch:35 step:27686 [D loss: 0.054932, acc.: 98.44%] [G loss: 3.790095]\n",
      "epoch:35 step:27687 [D loss: 0.070369, acc.: 98.44%] [G loss: 3.946527]\n",
      "epoch:35 step:27688 [D loss: 0.070715, acc.: 99.22%] [G loss: 4.472660]\n",
      "epoch:35 step:27689 [D loss: 0.032494, acc.: 99.22%] [G loss: 4.604286]\n",
      "epoch:35 step:27690 [D loss: 0.016025, acc.: 100.00%] [G loss: 3.969684]\n",
      "epoch:35 step:27691 [D loss: 0.011192, acc.: 100.00%] [G loss: 4.683853]\n",
      "epoch:35 step:27692 [D loss: 0.005439, acc.: 100.00%] [G loss: 4.346572]\n",
      "epoch:35 step:27693 [D loss: 0.039957, acc.: 100.00%] [G loss: 4.370397]\n",
      "epoch:35 step:27694 [D loss: 0.041199, acc.: 99.22%] [G loss: 4.745873]\n",
      "epoch:35 step:27695 [D loss: 0.020238, acc.: 100.00%] [G loss: 5.800963]\n",
      "epoch:35 step:27696 [D loss: 0.082634, acc.: 96.88%] [G loss: 3.104310]\n",
      "epoch:35 step:27697 [D loss: 0.029635, acc.: 100.00%] [G loss: 2.953490]\n",
      "epoch:35 step:27698 [D loss: 0.025453, acc.: 100.00%] [G loss: 4.960812]\n",
      "epoch:35 step:27699 [D loss: 0.621165, acc.: 70.31%] [G loss: 8.554168]\n",
      "epoch:35 step:27700 [D loss: 0.442863, acc.: 80.47%] [G loss: 6.084890]\n",
      "epoch:35 step:27701 [D loss: 0.024496, acc.: 100.00%] [G loss: 5.391842]\n",
      "epoch:35 step:27702 [D loss: 0.038737, acc.: 100.00%] [G loss: 4.158674]\n",
      "epoch:35 step:27703 [D loss: 0.028349, acc.: 99.22%] [G loss: 4.751637]\n",
      "epoch:35 step:27704 [D loss: 0.019974, acc.: 100.00%] [G loss: 5.089883]\n",
      "epoch:35 step:27705 [D loss: 0.248789, acc.: 89.84%] [G loss: 6.497793]\n",
      "epoch:35 step:27706 [D loss: 0.005070, acc.: 100.00%] [G loss: 7.471261]\n",
      "epoch:35 step:27707 [D loss: 0.006311, acc.: 100.00%] [G loss: 6.456555]\n",
      "epoch:35 step:27708 [D loss: 0.017799, acc.: 99.22%] [G loss: 5.768269]\n",
      "epoch:35 step:27709 [D loss: 0.007703, acc.: 100.00%] [G loss: 4.783577]\n",
      "epoch:35 step:27710 [D loss: 0.008873, acc.: 100.00%] [G loss: 5.215222]\n",
      "epoch:35 step:27711 [D loss: 0.006088, acc.: 100.00%] [G loss: 4.974244]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:35 step:27712 [D loss: 0.021009, acc.: 99.22%] [G loss: 5.215298]\n",
      "epoch:35 step:27713 [D loss: 0.022158, acc.: 100.00%] [G loss: 4.896725]\n",
      "epoch:35 step:27714 [D loss: 0.011156, acc.: 100.00%] [G loss: 4.737316]\n",
      "epoch:35 step:27715 [D loss: 0.029524, acc.: 98.44%] [G loss: 2.957159]\n",
      "epoch:35 step:27716 [D loss: 0.071971, acc.: 98.44%] [G loss: 5.516874]\n",
      "epoch:35 step:27717 [D loss: 0.085140, acc.: 97.66%] [G loss: 5.413943]\n",
      "epoch:35 step:27718 [D loss: 0.018225, acc.: 100.00%] [G loss: 5.049826]\n",
      "epoch:35 step:27719 [D loss: 0.002648, acc.: 100.00%] [G loss: 5.062406]\n",
      "epoch:35 step:27720 [D loss: 0.388636, acc.: 84.38%] [G loss: 7.222985]\n",
      "epoch:35 step:27721 [D loss: 0.596423, acc.: 74.22%] [G loss: 1.968558]\n",
      "epoch:35 step:27722 [D loss: 0.047939, acc.: 100.00%] [G loss: 4.309054]\n",
      "epoch:35 step:27723 [D loss: 0.040327, acc.: 99.22%] [G loss: 5.258244]\n",
      "epoch:35 step:27724 [D loss: 0.009857, acc.: 100.00%] [G loss: 5.967019]\n",
      "epoch:35 step:27725 [D loss: 0.004942, acc.: 100.00%] [G loss: 5.902507]\n",
      "epoch:35 step:27726 [D loss: 0.006151, acc.: 100.00%] [G loss: 5.109849]\n",
      "epoch:35 step:27727 [D loss: 0.053764, acc.: 98.44%] [G loss: 4.531655]\n",
      "epoch:35 step:27728 [D loss: 0.064789, acc.: 97.66%] [G loss: 0.554047]\n",
      "epoch:35 step:27729 [D loss: 0.386884, acc.: 84.38%] [G loss: 9.046418]\n",
      "epoch:35 step:27730 [D loss: 1.422131, acc.: 50.00%] [G loss: 2.096561]\n",
      "epoch:35 step:27731 [D loss: 0.744115, acc.: 69.53%] [G loss: 7.831458]\n",
      "epoch:35 step:27732 [D loss: 0.000920, acc.: 100.00%] [G loss: 9.444495]\n",
      "epoch:35 step:27733 [D loss: 0.852171, acc.: 67.97%] [G loss: 3.062069]\n",
      "epoch:35 step:27734 [D loss: 3.115894, acc.: 50.78%] [G loss: 8.575377]\n",
      "epoch:35 step:27735 [D loss: 0.359506, acc.: 80.47%] [G loss: 8.539387]\n",
      "epoch:35 step:27736 [D loss: 1.803515, acc.: 50.78%] [G loss: 4.589925]\n",
      "epoch:35 step:27737 [D loss: 0.035652, acc.: 100.00%] [G loss: 2.799148]\n",
      "epoch:35 step:27738 [D loss: 0.435514, acc.: 81.25%] [G loss: 4.369468]\n",
      "epoch:35 step:27739 [D loss: 0.045344, acc.: 97.66%] [G loss: 5.633719]\n",
      "epoch:35 step:27740 [D loss: 0.464523, acc.: 77.34%] [G loss: 3.766373]\n",
      "epoch:35 step:27741 [D loss: 0.057092, acc.: 98.44%] [G loss: 3.053942]\n",
      "epoch:35 step:27742 [D loss: 0.056987, acc.: 99.22%] [G loss: 2.726804]\n",
      "epoch:35 step:27743 [D loss: 0.133100, acc.: 95.31%] [G loss: 3.813185]\n",
      "epoch:35 step:27744 [D loss: 0.043654, acc.: 99.22%] [G loss: 3.733787]\n",
      "epoch:35 step:27745 [D loss: 0.055511, acc.: 99.22%] [G loss: 3.528132]\n",
      "epoch:35 step:27746 [D loss: 0.114013, acc.: 98.44%] [G loss: 3.016101]\n",
      "epoch:35 step:27747 [D loss: 0.040476, acc.: 100.00%] [G loss: 3.016038]\n",
      "epoch:35 step:27748 [D loss: 0.295474, acc.: 88.28%] [G loss: 3.706815]\n",
      "epoch:35 step:27749 [D loss: 0.141355, acc.: 94.53%] [G loss: 3.528038]\n",
      "epoch:35 step:27750 [D loss: 0.124195, acc.: 98.44%] [G loss: 3.976501]\n",
      "epoch:35 step:27751 [D loss: 0.056816, acc.: 99.22%] [G loss: 4.309992]\n",
      "epoch:35 step:27752 [D loss: 0.201373, acc.: 90.62%] [G loss: 2.551324]\n",
      "epoch:35 step:27753 [D loss: 0.111621, acc.: 98.44%] [G loss: 3.378942]\n",
      "epoch:35 step:27754 [D loss: 0.061067, acc.: 99.22%] [G loss: 3.819786]\n",
      "epoch:35 step:27755 [D loss: 0.803601, acc.: 59.38%] [G loss: 2.289856]\n",
      "epoch:35 step:27756 [D loss: 0.053751, acc.: 99.22%] [G loss: 2.758229]\n",
      "epoch:35 step:27757 [D loss: 0.027685, acc.: 100.00%] [G loss: 4.087551]\n",
      "epoch:35 step:27758 [D loss: 0.013478, acc.: 100.00%] [G loss: 3.449012]\n",
      "epoch:35 step:27759 [D loss: 0.031341, acc.: 100.00%] [G loss: 3.184023]\n",
      "epoch:35 step:27760 [D loss: 0.024143, acc.: 100.00%] [G loss: 2.787137]\n",
      "epoch:35 step:27761 [D loss: 0.026681, acc.: 99.22%] [G loss: 2.097838]\n",
      "epoch:35 step:27762 [D loss: 0.137455, acc.: 95.31%] [G loss: 4.792725]\n",
      "epoch:35 step:27763 [D loss: 0.102108, acc.: 96.09%] [G loss: 4.343095]\n",
      "epoch:35 step:27764 [D loss: 0.082030, acc.: 97.66%] [G loss: 3.812659]\n",
      "epoch:35 step:27765 [D loss: 0.083420, acc.: 99.22%] [G loss: 3.503816]\n",
      "epoch:35 step:27766 [D loss: 0.171308, acc.: 93.75%] [G loss: 3.944137]\n",
      "epoch:35 step:27767 [D loss: 0.021459, acc.: 100.00%] [G loss: 3.557252]\n",
      "epoch:35 step:27768 [D loss: 0.059801, acc.: 100.00%] [G loss: 4.035336]\n",
      "epoch:35 step:27769 [D loss: 0.008677, acc.: 100.00%] [G loss: 4.422795]\n",
      "epoch:35 step:27770 [D loss: 0.016060, acc.: 100.00%] [G loss: 4.804584]\n",
      "epoch:35 step:27771 [D loss: 0.038793, acc.: 100.00%] [G loss: 3.314558]\n",
      "epoch:35 step:27772 [D loss: 0.028300, acc.: 100.00%] [G loss: 3.993036]\n",
      "epoch:35 step:27773 [D loss: 0.097284, acc.: 97.66%] [G loss: 4.252182]\n",
      "epoch:35 step:27774 [D loss: 0.010658, acc.: 100.00%] [G loss: 5.103811]\n",
      "epoch:35 step:27775 [D loss: 0.024495, acc.: 99.22%] [G loss: 4.760264]\n",
      "epoch:35 step:27776 [D loss: 0.035518, acc.: 100.00%] [G loss: 4.115074]\n",
      "epoch:35 step:27777 [D loss: 0.033393, acc.: 100.00%] [G loss: 4.098671]\n",
      "epoch:35 step:27778 [D loss: 0.068523, acc.: 98.44%] [G loss: 4.557905]\n",
      "epoch:35 step:27779 [D loss: 0.014011, acc.: 100.00%] [G loss: 3.849361]\n",
      "epoch:35 step:27780 [D loss: 0.044327, acc.: 99.22%] [G loss: 4.046321]\n",
      "epoch:35 step:27781 [D loss: 0.012302, acc.: 100.00%] [G loss: 3.913385]\n",
      "epoch:35 step:27782 [D loss: 0.038208, acc.: 99.22%] [G loss: 3.156691]\n",
      "epoch:35 step:27783 [D loss: 0.680333, acc.: 62.50%] [G loss: 5.996113]\n",
      "epoch:35 step:27784 [D loss: 0.035159, acc.: 99.22%] [G loss: 7.087235]\n",
      "epoch:35 step:27785 [D loss: 0.255299, acc.: 85.16%] [G loss: 3.695631]\n",
      "epoch:35 step:27786 [D loss: 0.011486, acc.: 100.00%] [G loss: 1.746608]\n",
      "epoch:35 step:27787 [D loss: 0.038749, acc.: 99.22%] [G loss: 1.855998]\n",
      "epoch:35 step:27788 [D loss: 0.037090, acc.: 100.00%] [G loss: 2.799630]\n",
      "epoch:35 step:27789 [D loss: 0.019298, acc.: 100.00%] [G loss: 3.827609]\n",
      "epoch:35 step:27790 [D loss: 0.084744, acc.: 99.22%] [G loss: 4.937446]\n",
      "epoch:35 step:27791 [D loss: 0.049998, acc.: 96.88%] [G loss: 5.409028]\n",
      "epoch:35 step:27792 [D loss: 0.004521, acc.: 100.00%] [G loss: 5.607671]\n",
      "epoch:35 step:27793 [D loss: 0.014655, acc.: 100.00%] [G loss: 4.124419]\n",
      "epoch:35 step:27794 [D loss: 0.114150, acc.: 95.31%] [G loss: 1.813492]\n",
      "epoch:35 step:27795 [D loss: 0.659415, acc.: 70.31%] [G loss: 7.603815]\n",
      "epoch:35 step:27796 [D loss: 1.250712, acc.: 53.12%] [G loss: 5.330494]\n",
      "epoch:35 step:27797 [D loss: 0.047898, acc.: 99.22%] [G loss: 4.528217]\n",
      "epoch:35 step:27798 [D loss: 0.079665, acc.: 100.00%] [G loss: 4.041472]\n",
      "epoch:35 step:27799 [D loss: 0.027486, acc.: 100.00%] [G loss: 3.903323]\n",
      "epoch:35 step:27800 [D loss: 0.016421, acc.: 100.00%] [G loss: 3.902214]\n",
      "##############\n",
      "[0.96628573 0.95117672 0.94191004 0.98161364 2.11250761 2.10824871\n",
      " 1.09644657 2.11763647 2.10267144 2.10430619]\n",
      "##########\n",
      "epoch:35 step:27801 [D loss: 0.016809, acc.: 100.00%] [G loss: 3.781935]\n",
      "epoch:35 step:27802 [D loss: 0.065686, acc.: 100.00%] [G loss: 2.791474]\n",
      "epoch:35 step:27803 [D loss: 0.041454, acc.: 100.00%] [G loss: 2.515445]\n",
      "epoch:35 step:27804 [D loss: 0.015333, acc.: 100.00%] [G loss: 3.359047]\n",
      "epoch:35 step:27805 [D loss: 0.079709, acc.: 99.22%] [G loss: 4.102799]\n",
      "epoch:35 step:27806 [D loss: 0.041901, acc.: 98.44%] [G loss: 3.997502]\n",
      "epoch:35 step:27807 [D loss: 0.017218, acc.: 100.00%] [G loss: 3.688483]\n",
      "epoch:35 step:27808 [D loss: 0.031552, acc.: 99.22%] [G loss: 3.827825]\n",
      "epoch:35 step:27809 [D loss: 0.016838, acc.: 100.00%] [G loss: 2.513779]\n",
      "epoch:35 step:27810 [D loss: 0.046739, acc.: 100.00%] [G loss: 1.767702]\n",
      "epoch:35 step:27811 [D loss: 0.050022, acc.: 99.22%] [G loss: 3.532810]\n",
      "epoch:35 step:27812 [D loss: 0.024699, acc.: 99.22%] [G loss: 3.915386]\n",
      "epoch:35 step:27813 [D loss: 0.056058, acc.: 99.22%] [G loss: 3.181370]\n",
      "epoch:35 step:27814 [D loss: 0.075537, acc.: 98.44%] [G loss: 3.773855]\n",
      "epoch:35 step:27815 [D loss: 0.026037, acc.: 100.00%] [G loss: 3.964071]\n",
      "epoch:35 step:27816 [D loss: 0.045057, acc.: 99.22%] [G loss: 2.814617]\n",
      "epoch:35 step:27817 [D loss: 0.065004, acc.: 99.22%] [G loss: 2.459626]\n",
      "epoch:35 step:27818 [D loss: 0.204228, acc.: 92.19%] [G loss: 6.058210]\n",
      "epoch:35 step:27819 [D loss: 0.057930, acc.: 97.66%] [G loss: 7.311288]\n",
      "epoch:35 step:27820 [D loss: 1.067206, acc.: 47.66%] [G loss: 5.571268]\n",
      "epoch:35 step:27821 [D loss: 0.002167, acc.: 100.00%] [G loss: 6.821484]\n",
      "epoch:35 step:27822 [D loss: 0.030373, acc.: 99.22%] [G loss: 6.945522]\n",
      "epoch:35 step:27823 [D loss: 0.087036, acc.: 96.88%] [G loss: 5.789233]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:35 step:27824 [D loss: 0.009179, acc.: 100.00%] [G loss: 5.187861]\n",
      "epoch:35 step:27825 [D loss: 0.009239, acc.: 100.00%] [G loss: 5.142748]\n",
      "epoch:35 step:27826 [D loss: 0.060631, acc.: 97.66%] [G loss: 4.513460]\n",
      "epoch:35 step:27827 [D loss: 0.027968, acc.: 98.44%] [G loss: 5.652076]\n",
      "epoch:35 step:27828 [D loss: 0.007454, acc.: 100.00%] [G loss: 4.600120]\n",
      "epoch:35 step:27829 [D loss: 0.003363, acc.: 100.00%] [G loss: 4.033442]\n",
      "epoch:35 step:27830 [D loss: 0.006102, acc.: 100.00%] [G loss: 4.342168]\n",
      "epoch:35 step:27831 [D loss: 0.022514, acc.: 100.00%] [G loss: 3.087972]\n",
      "epoch:35 step:27832 [D loss: 0.025334, acc.: 100.00%] [G loss: 4.701649]\n",
      "epoch:35 step:27833 [D loss: 0.040910, acc.: 100.00%] [G loss: 5.095918]\n",
      "epoch:35 step:27834 [D loss: 0.082301, acc.: 96.88%] [G loss: 3.320424]\n",
      "epoch:35 step:27835 [D loss: 0.012554, acc.: 100.00%] [G loss: 3.508479]\n",
      "epoch:35 step:27836 [D loss: 0.049116, acc.: 100.00%] [G loss: 4.217239]\n",
      "epoch:35 step:27837 [D loss: 0.007888, acc.: 100.00%] [G loss: 5.126441]\n",
      "epoch:35 step:27838 [D loss: 0.003955, acc.: 100.00%] [G loss: 4.123538]\n",
      "epoch:35 step:27839 [D loss: 0.010412, acc.: 100.00%] [G loss: 4.137687]\n",
      "epoch:35 step:27840 [D loss: 0.111590, acc.: 95.31%] [G loss: 4.218487]\n",
      "epoch:35 step:27841 [D loss: 0.009228, acc.: 100.00%] [G loss: 4.600702]\n",
      "epoch:35 step:27842 [D loss: 0.010760, acc.: 100.00%] [G loss: 4.453293]\n",
      "epoch:35 step:27843 [D loss: 0.057448, acc.: 97.66%] [G loss: 3.039217]\n",
      "epoch:35 step:27844 [D loss: 0.360637, acc.: 84.38%] [G loss: 7.306314]\n",
      "epoch:35 step:27845 [D loss: 1.207084, acc.: 58.59%] [G loss: 2.097877]\n",
      "epoch:35 step:27846 [D loss: 0.428905, acc.: 79.69%] [G loss: 5.890158]\n",
      "epoch:35 step:27847 [D loss: 0.031802, acc.: 100.00%] [G loss: 6.648126]\n",
      "epoch:35 step:27848 [D loss: 0.565455, acc.: 70.31%] [G loss: 2.608283]\n",
      "epoch:35 step:27849 [D loss: 0.399471, acc.: 78.12%] [G loss: 6.008222]\n",
      "epoch:35 step:27850 [D loss: 0.033720, acc.: 99.22%] [G loss: 6.761567]\n",
      "epoch:35 step:27851 [D loss: 0.176797, acc.: 92.19%] [G loss: 5.305268]\n",
      "epoch:35 step:27852 [D loss: 0.004788, acc.: 100.00%] [G loss: 4.811030]\n",
      "epoch:35 step:27853 [D loss: 0.010293, acc.: 100.00%] [G loss: 4.769042]\n",
      "epoch:35 step:27854 [D loss: 0.010149, acc.: 100.00%] [G loss: 4.900541]\n",
      "epoch:35 step:27855 [D loss: 0.020782, acc.: 100.00%] [G loss: 3.652562]\n",
      "epoch:35 step:27856 [D loss: 0.008284, acc.: 100.00%] [G loss: 3.475095]\n",
      "epoch:35 step:27857 [D loss: 0.015970, acc.: 100.00%] [G loss: 4.454115]\n",
      "epoch:35 step:27858 [D loss: 0.028826, acc.: 100.00%] [G loss: 4.378801]\n",
      "epoch:35 step:27859 [D loss: 0.058975, acc.: 99.22%] [G loss: 3.830523]\n",
      "epoch:35 step:27860 [D loss: 0.011514, acc.: 100.00%] [G loss: 4.348997]\n",
      "epoch:35 step:27861 [D loss: 0.023024, acc.: 100.00%] [G loss: 4.061600]\n",
      "epoch:35 step:27862 [D loss: 0.014410, acc.: 100.00%] [G loss: 4.026827]\n",
      "epoch:35 step:27863 [D loss: 0.006572, acc.: 100.00%] [G loss: 4.265616]\n",
      "epoch:35 step:27864 [D loss: 0.011900, acc.: 100.00%] [G loss: 4.472758]\n",
      "epoch:35 step:27865 [D loss: 0.006062, acc.: 100.00%] [G loss: 3.541053]\n",
      "epoch:35 step:27866 [D loss: 0.024374, acc.: 100.00%] [G loss: 4.323339]\n",
      "epoch:35 step:27867 [D loss: 0.059990, acc.: 98.44%] [G loss: 4.192451]\n",
      "epoch:35 step:27868 [D loss: 0.708814, acc.: 61.72%] [G loss: 6.407481]\n",
      "epoch:35 step:27869 [D loss: 0.117395, acc.: 96.88%] [G loss: 7.886463]\n",
      "epoch:35 step:27870 [D loss: 0.175898, acc.: 90.62%] [G loss: 4.927427]\n",
      "epoch:35 step:27871 [D loss: 0.037273, acc.: 99.22%] [G loss: 4.496265]\n",
      "epoch:35 step:27872 [D loss: 0.018305, acc.: 100.00%] [G loss: 4.375394]\n",
      "epoch:35 step:27873 [D loss: 0.007910, acc.: 100.00%] [G loss: 4.233291]\n",
      "epoch:35 step:27874 [D loss: 0.010096, acc.: 100.00%] [G loss: 4.280600]\n",
      "epoch:35 step:27875 [D loss: 0.006974, acc.: 100.00%] [G loss: 4.531931]\n",
      "epoch:35 step:27876 [D loss: 0.007218, acc.: 100.00%] [G loss: 4.209711]\n",
      "epoch:35 step:27877 [D loss: 0.015535, acc.: 100.00%] [G loss: 4.192080]\n",
      "epoch:35 step:27878 [D loss: 0.031990, acc.: 99.22%] [G loss: 2.764870]\n",
      "epoch:35 step:27879 [D loss: 0.042955, acc.: 99.22%] [G loss: 4.409049]\n",
      "epoch:35 step:27880 [D loss: 0.005743, acc.: 100.00%] [G loss: 5.324798]\n",
      "epoch:35 step:27881 [D loss: 0.004686, acc.: 100.00%] [G loss: 5.682504]\n",
      "epoch:35 step:27882 [D loss: 0.337715, acc.: 88.28%] [G loss: 6.364902]\n",
      "epoch:35 step:27883 [D loss: 0.016075, acc.: 99.22%] [G loss: 6.988692]\n",
      "epoch:35 step:27884 [D loss: 0.062385, acc.: 96.09%] [G loss: 6.405923]\n",
      "epoch:35 step:27885 [D loss: 0.073900, acc.: 98.44%] [G loss: 4.282554]\n",
      "epoch:35 step:27886 [D loss: 0.008219, acc.: 100.00%] [G loss: 4.424947]\n",
      "epoch:35 step:27887 [D loss: 0.023514, acc.: 100.00%] [G loss: 4.060151]\n",
      "epoch:35 step:27888 [D loss: 0.015929, acc.: 100.00%] [G loss: 4.189353]\n",
      "epoch:35 step:27889 [D loss: 0.007342, acc.: 100.00%] [G loss: 4.045011]\n",
      "epoch:35 step:27890 [D loss: 0.011845, acc.: 100.00%] [G loss: 4.599517]\n",
      "epoch:35 step:27891 [D loss: 0.012005, acc.: 100.00%] [G loss: 4.490161]\n",
      "epoch:35 step:27892 [D loss: 0.008498, acc.: 100.00%] [G loss: 4.814047]\n",
      "epoch:35 step:27893 [D loss: 0.082918, acc.: 97.66%] [G loss: 4.254651]\n",
      "epoch:35 step:27894 [D loss: 0.018145, acc.: 100.00%] [G loss: 3.914995]\n",
      "epoch:35 step:27895 [D loss: 0.007550, acc.: 100.00%] [G loss: 5.103306]\n",
      "epoch:35 step:27896 [D loss: 0.014778, acc.: 100.00%] [G loss: 4.531951]\n",
      "epoch:35 step:27897 [D loss: 0.036049, acc.: 100.00%] [G loss: 3.350792]\n",
      "epoch:35 step:27898 [D loss: 0.068161, acc.: 100.00%] [G loss: 5.276958]\n",
      "epoch:35 step:27899 [D loss: 0.023576, acc.: 98.44%] [G loss: 5.135672]\n",
      "epoch:35 step:27900 [D loss: 0.124897, acc.: 95.31%] [G loss: 1.901504]\n",
      "epoch:35 step:27901 [D loss: 0.432447, acc.: 76.56%] [G loss: 8.023014]\n",
      "epoch:35 step:27902 [D loss: 0.448737, acc.: 75.78%] [G loss: 5.515772]\n",
      "epoch:35 step:27903 [D loss: 0.232467, acc.: 90.62%] [G loss: 7.507126]\n",
      "epoch:35 step:27904 [D loss: 0.140340, acc.: 95.31%] [G loss: 6.841189]\n",
      "epoch:35 step:27905 [D loss: 0.001618, acc.: 100.00%] [G loss: 5.895552]\n",
      "epoch:35 step:27906 [D loss: 0.003766, acc.: 100.00%] [G loss: 5.652180]\n",
      "epoch:35 step:27907 [D loss: 0.002934, acc.: 100.00%] [G loss: 5.927968]\n",
      "epoch:35 step:27908 [D loss: 0.004322, acc.: 100.00%] [G loss: 5.513880]\n",
      "epoch:35 step:27909 [D loss: 0.002472, acc.: 100.00%] [G loss: 4.898112]\n",
      "epoch:35 step:27910 [D loss: 0.347090, acc.: 84.38%] [G loss: 6.493884]\n",
      "epoch:35 step:27911 [D loss: 0.002555, acc.: 100.00%] [G loss: 7.683594]\n",
      "epoch:35 step:27912 [D loss: 0.736558, acc.: 66.41%] [G loss: 0.912827]\n",
      "epoch:35 step:27913 [D loss: 1.457961, acc.: 53.12%] [G loss: 7.889752]\n",
      "epoch:35 step:27914 [D loss: 1.115409, acc.: 57.03%] [G loss: 4.783096]\n",
      "epoch:35 step:27915 [D loss: 0.259259, acc.: 89.84%] [G loss: 3.180718]\n",
      "epoch:35 step:27916 [D loss: 0.180973, acc.: 94.53%] [G loss: 4.759872]\n",
      "epoch:35 step:27917 [D loss: 0.200659, acc.: 91.41%] [G loss: 3.783181]\n",
      "epoch:35 step:27918 [D loss: 0.623523, acc.: 63.28%] [G loss: 2.868756]\n",
      "epoch:35 step:27919 [D loss: 0.071371, acc.: 98.44%] [G loss: 5.150382]\n",
      "epoch:35 step:27920 [D loss: 0.133019, acc.: 96.09%] [G loss: 3.663420]\n",
      "epoch:35 step:27921 [D loss: 0.085254, acc.: 98.44%] [G loss: 4.054358]\n",
      "epoch:35 step:27922 [D loss: 0.103952, acc.: 97.66%] [G loss: 3.934426]\n",
      "epoch:35 step:27923 [D loss: 0.091058, acc.: 98.44%] [G loss: 2.956001]\n",
      "epoch:35 step:27924 [D loss: 0.293422, acc.: 86.72%] [G loss: 6.003300]\n",
      "epoch:35 step:27925 [D loss: 0.513270, acc.: 73.44%] [G loss: 4.245781]\n",
      "epoch:35 step:27926 [D loss: 0.069152, acc.: 97.66%] [G loss: 2.466864]\n",
      "epoch:35 step:27927 [D loss: 0.108288, acc.: 96.09%] [G loss: 3.749015]\n",
      "epoch:35 step:27928 [D loss: 0.039473, acc.: 99.22%] [G loss: 3.909557]\n",
      "epoch:35 step:27929 [D loss: 0.074264, acc.: 99.22%] [G loss: 3.453095]\n",
      "epoch:35 step:27930 [D loss: 0.067256, acc.: 98.44%] [G loss: 3.648134]\n",
      "epoch:35 step:27931 [D loss: 0.104953, acc.: 98.44%] [G loss: 2.667017]\n",
      "epoch:35 step:27932 [D loss: 0.045428, acc.: 100.00%] [G loss: 2.253491]\n",
      "epoch:35 step:27933 [D loss: 0.057799, acc.: 100.00%] [G loss: 2.351821]\n",
      "epoch:35 step:27934 [D loss: 0.038756, acc.: 99.22%] [G loss: 2.747817]\n",
      "epoch:35 step:27935 [D loss: 0.394852, acc.: 78.91%] [G loss: 5.745195]\n",
      "epoch:35 step:27936 [D loss: 1.313615, acc.: 53.12%] [G loss: 0.890193]\n",
      "epoch:35 step:27937 [D loss: 0.628614, acc.: 74.22%] [G loss: 5.932531]\n",
      "epoch:35 step:27938 [D loss: 0.156263, acc.: 91.41%] [G loss: 6.400481]\n",
      "epoch:35 step:27939 [D loss: 0.649267, acc.: 67.97%] [G loss: 2.592996]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:35 step:27940 [D loss: 0.399054, acc.: 76.56%] [G loss: 4.843120]\n",
      "epoch:35 step:27941 [D loss: 0.158822, acc.: 92.19%] [G loss: 5.320761]\n",
      "epoch:35 step:27942 [D loss: 0.129271, acc.: 95.31%] [G loss: 3.844229]\n",
      "epoch:35 step:27943 [D loss: 0.032304, acc.: 100.00%] [G loss: 3.493665]\n",
      "epoch:35 step:27944 [D loss: 0.050068, acc.: 99.22%] [G loss: 3.675624]\n",
      "epoch:35 step:27945 [D loss: 0.054571, acc.: 100.00%] [G loss: 3.470856]\n",
      "epoch:35 step:27946 [D loss: 0.037655, acc.: 99.22%] [G loss: 3.779458]\n",
      "epoch:35 step:27947 [D loss: 0.045306, acc.: 99.22%] [G loss: 3.533938]\n",
      "epoch:35 step:27948 [D loss: 0.088124, acc.: 98.44%] [G loss: 3.568312]\n",
      "epoch:35 step:27949 [D loss: 0.035480, acc.: 99.22%] [G loss: 3.538300]\n",
      "epoch:35 step:27950 [D loss: 0.094789, acc.: 97.66%] [G loss: 3.790047]\n",
      "epoch:35 step:27951 [D loss: 0.059380, acc.: 99.22%] [G loss: 3.645783]\n",
      "epoch:35 step:27952 [D loss: 0.125886, acc.: 96.88%] [G loss: 2.954116]\n",
      "epoch:35 step:27953 [D loss: 0.127358, acc.: 96.88%] [G loss: 5.063937]\n",
      "epoch:35 step:27954 [D loss: 0.195836, acc.: 92.97%] [G loss: 3.846605]\n",
      "epoch:35 step:27955 [D loss: 0.030749, acc.: 100.00%] [G loss: 3.244801]\n",
      "epoch:35 step:27956 [D loss: 0.027675, acc.: 99.22%] [G loss: 2.515148]\n",
      "epoch:35 step:27957 [D loss: 0.052802, acc.: 98.44%] [G loss: 3.004782]\n",
      "epoch:35 step:27958 [D loss: 0.027912, acc.: 100.00%] [G loss: 2.828704]\n",
      "epoch:35 step:27959 [D loss: 0.063860, acc.: 98.44%] [G loss: 2.301803]\n",
      "epoch:35 step:27960 [D loss: 0.025058, acc.: 100.00%] [G loss: 2.884966]\n",
      "epoch:35 step:27961 [D loss: 0.019785, acc.: 100.00%] [G loss: 2.450661]\n",
      "epoch:35 step:27962 [D loss: 0.051689, acc.: 99.22%] [G loss: 2.926693]\n",
      "epoch:35 step:27963 [D loss: 0.012163, acc.: 100.00%] [G loss: 2.790193]\n",
      "epoch:35 step:27964 [D loss: 0.168564, acc.: 98.44%] [G loss: 3.654075]\n",
      "epoch:35 step:27965 [D loss: 0.030895, acc.: 99.22%] [G loss: 4.095006]\n",
      "epoch:35 step:27966 [D loss: 0.084498, acc.: 96.88%] [G loss: 2.152085]\n",
      "epoch:35 step:27967 [D loss: 0.412849, acc.: 78.12%] [G loss: 6.612037]\n",
      "epoch:35 step:27968 [D loss: 0.568161, acc.: 73.44%] [G loss: 2.427013]\n",
      "epoch:35 step:27969 [D loss: 0.238866, acc.: 88.28%] [G loss: 4.899093]\n",
      "epoch:35 step:27970 [D loss: 0.060050, acc.: 97.66%] [G loss: 5.325007]\n",
      "epoch:35 step:27971 [D loss: 0.219502, acc.: 89.84%] [G loss: 1.519635]\n",
      "epoch:35 step:27972 [D loss: 0.180875, acc.: 90.62%] [G loss: 5.884962]\n",
      "epoch:35 step:27973 [D loss: 0.018972, acc.: 100.00%] [G loss: 6.655951]\n",
      "epoch:35 step:27974 [D loss: 0.227241, acc.: 89.84%] [G loss: 4.551124]\n",
      "epoch:35 step:27975 [D loss: 0.078752, acc.: 96.88%] [G loss: 4.662181]\n",
      "epoch:35 step:27976 [D loss: 0.002707, acc.: 100.00%] [G loss: 5.106273]\n",
      "epoch:35 step:27977 [D loss: 0.010308, acc.: 100.00%] [G loss: 5.182496]\n",
      "epoch:35 step:27978 [D loss: 0.017196, acc.: 100.00%] [G loss: 5.153281]\n",
      "epoch:35 step:27979 [D loss: 0.009724, acc.: 100.00%] [G loss: 4.788671]\n",
      "epoch:35 step:27980 [D loss: 0.021091, acc.: 100.00%] [G loss: 3.834863]\n",
      "epoch:35 step:27981 [D loss: 0.063819, acc.: 99.22%] [G loss: 4.876369]\n",
      "epoch:35 step:27982 [D loss: 0.017319, acc.: 100.00%] [G loss: 4.851106]\n",
      "epoch:35 step:27983 [D loss: 0.024112, acc.: 100.00%] [G loss: 4.185631]\n",
      "epoch:35 step:27984 [D loss: 0.033956, acc.: 100.00%] [G loss: 3.881102]\n",
      "epoch:35 step:27985 [D loss: 0.028772, acc.: 100.00%] [G loss: 4.542968]\n",
      "epoch:35 step:27986 [D loss: 0.012002, acc.: 100.00%] [G loss: 3.504092]\n",
      "epoch:35 step:27987 [D loss: 0.060082, acc.: 100.00%] [G loss: 3.967800]\n",
      "epoch:35 step:27988 [D loss: 0.025783, acc.: 99.22%] [G loss: 4.489315]\n",
      "epoch:35 step:27989 [D loss: 0.171342, acc.: 94.53%] [G loss: 4.109918]\n",
      "epoch:35 step:27990 [D loss: 0.045502, acc.: 100.00%] [G loss: 3.777798]\n",
      "epoch:35 step:27991 [D loss: 0.030060, acc.: 100.00%] [G loss: 4.507571]\n",
      "epoch:35 step:27992 [D loss: 0.015437, acc.: 100.00%] [G loss: 4.848991]\n",
      "epoch:35 step:27993 [D loss: 0.050710, acc.: 98.44%] [G loss: 4.150242]\n",
      "epoch:35 step:27994 [D loss: 0.044869, acc.: 100.00%] [G loss: 4.221833]\n",
      "epoch:35 step:27995 [D loss: 0.023885, acc.: 100.00%] [G loss: 4.533343]\n",
      "epoch:35 step:27996 [D loss: 0.036997, acc.: 100.00%] [G loss: 4.245843]\n",
      "epoch:35 step:27997 [D loss: 0.091012, acc.: 96.88%] [G loss: 3.752116]\n",
      "epoch:35 step:27998 [D loss: 0.121085, acc.: 97.66%] [G loss: 5.772282]\n",
      "epoch:35 step:27999 [D loss: 0.109055, acc.: 97.66%] [G loss: 4.541343]\n",
      "epoch:35 step:28000 [D loss: 0.033576, acc.: 100.00%] [G loss: 4.116530]\n",
      "##############\n",
      "[0.98825088 1.00345165 0.97672686 0.92945024 2.10496904 0.78972052\n",
      " 2.10777459 2.10940618 2.11293629 0.85217751]\n",
      "##########\n",
      "epoch:35 step:28001 [D loss: 0.004300, acc.: 100.00%] [G loss: 4.979912]\n",
      "epoch:35 step:28002 [D loss: 0.026005, acc.: 100.00%] [G loss: 5.196367]\n",
      "epoch:35 step:28003 [D loss: 0.027931, acc.: 99.22%] [G loss: 5.259223]\n",
      "epoch:35 step:28004 [D loss: 0.011310, acc.: 100.00%] [G loss: 4.621250]\n",
      "epoch:35 step:28005 [D loss: 0.070146, acc.: 99.22%] [G loss: 4.891616]\n",
      "epoch:35 step:28006 [D loss: 0.007349, acc.: 100.00%] [G loss: 5.913045]\n",
      "epoch:35 step:28007 [D loss: 0.030417, acc.: 100.00%] [G loss: 4.975234]\n",
      "epoch:35 step:28008 [D loss: 0.057405, acc.: 98.44%] [G loss: 5.613656]\n",
      "epoch:35 step:28009 [D loss: 0.028061, acc.: 100.00%] [G loss: 5.290501]\n",
      "epoch:35 step:28010 [D loss: 0.006516, acc.: 100.00%] [G loss: 6.117155]\n",
      "epoch:35 step:28011 [D loss: 0.021616, acc.: 100.00%] [G loss: 4.508815]\n",
      "epoch:35 step:28012 [D loss: 0.013061, acc.: 100.00%] [G loss: 4.105920]\n",
      "epoch:35 step:28013 [D loss: 0.096123, acc.: 97.66%] [G loss: 5.488087]\n",
      "epoch:35 step:28014 [D loss: 0.015427, acc.: 99.22%] [G loss: 6.301618]\n",
      "epoch:35 step:28015 [D loss: 0.079066, acc.: 97.66%] [G loss: 4.332734]\n",
      "epoch:35 step:28016 [D loss: 0.038659, acc.: 100.00%] [G loss: 4.776978]\n",
      "epoch:35 step:28017 [D loss: 0.020674, acc.: 100.00%] [G loss: 4.921243]\n",
      "epoch:35 step:28018 [D loss: 0.008811, acc.: 100.00%] [G loss: 4.424949]\n",
      "epoch:35 step:28019 [D loss: 0.039428, acc.: 100.00%] [G loss: 5.243567]\n",
      "epoch:35 step:28020 [D loss: 0.758759, acc.: 64.06%] [G loss: 8.006076]\n",
      "epoch:35 step:28021 [D loss: 0.655491, acc.: 74.22%] [G loss: 3.512936]\n",
      "epoch:35 step:28022 [D loss: 0.291424, acc.: 86.72%] [G loss: 6.005791]\n",
      "epoch:35 step:28023 [D loss: 0.150466, acc.: 92.97%] [G loss: 5.763311]\n",
      "epoch:35 step:28024 [D loss: 0.043976, acc.: 99.22%] [G loss: 5.262323]\n",
      "epoch:35 step:28025 [D loss: 0.101395, acc.: 96.88%] [G loss: 5.142526]\n",
      "epoch:35 step:28026 [D loss: 0.005685, acc.: 100.00%] [G loss: 5.134665]\n",
      "epoch:35 step:28027 [D loss: 0.031401, acc.: 99.22%] [G loss: 5.053102]\n",
      "epoch:35 step:28028 [D loss: 0.016573, acc.: 100.00%] [G loss: 4.964035]\n",
      "epoch:35 step:28029 [D loss: 0.018676, acc.: 100.00%] [G loss: 4.446169]\n",
      "epoch:35 step:28030 [D loss: 0.006141, acc.: 100.00%] [G loss: 4.616397]\n",
      "epoch:35 step:28031 [D loss: 0.091786, acc.: 96.88%] [G loss: 4.996227]\n",
      "epoch:35 step:28032 [D loss: 0.006924, acc.: 100.00%] [G loss: 5.685154]\n",
      "epoch:35 step:28033 [D loss: 0.194274, acc.: 92.19%] [G loss: 1.484286]\n",
      "epoch:35 step:28034 [D loss: 0.537578, acc.: 71.88%] [G loss: 8.708963]\n",
      "epoch:35 step:28035 [D loss: 1.696331, acc.: 54.69%] [G loss: 4.942138]\n",
      "epoch:35 step:28036 [D loss: 0.162868, acc.: 94.53%] [G loss: 4.463631]\n",
      "epoch:35 step:28037 [D loss: 0.016625, acc.: 100.00%] [G loss: 5.054791]\n",
      "epoch:35 step:28038 [D loss: 0.046332, acc.: 99.22%] [G loss: 4.710331]\n",
      "epoch:35 step:28039 [D loss: 0.150977, acc.: 93.75%] [G loss: 4.327277]\n",
      "epoch:35 step:28040 [D loss: 0.022926, acc.: 100.00%] [G loss: 4.647235]\n",
      "epoch:35 step:28041 [D loss: 0.008746, acc.: 100.00%] [G loss: 4.504679]\n",
      "epoch:35 step:28042 [D loss: 0.031410, acc.: 99.22%] [G loss: 4.340006]\n",
      "epoch:35 step:28043 [D loss: 0.047112, acc.: 98.44%] [G loss: 4.178823]\n",
      "epoch:35 step:28044 [D loss: 0.021295, acc.: 100.00%] [G loss: 4.156096]\n",
      "epoch:35 step:28045 [D loss: 0.027219, acc.: 100.00%] [G loss: 4.594343]\n",
      "epoch:35 step:28046 [D loss: 0.112269, acc.: 98.44%] [G loss: 2.752830]\n",
      "epoch:35 step:28047 [D loss: 0.019822, acc.: 100.00%] [G loss: 4.589934]\n",
      "epoch:35 step:28048 [D loss: 0.133031, acc.: 96.88%] [G loss: 4.067275]\n",
      "epoch:35 step:28049 [D loss: 0.090051, acc.: 97.66%] [G loss: 4.865439]\n",
      "epoch:35 step:28050 [D loss: 0.090450, acc.: 98.44%] [G loss: 4.539484]\n",
      "epoch:35 step:28051 [D loss: 0.015041, acc.: 100.00%] [G loss: 5.169895]\n",
      "epoch:35 step:28052 [D loss: 0.023252, acc.: 100.00%] [G loss: 4.763592]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:35 step:28053 [D loss: 0.033094, acc.: 100.00%] [G loss: 4.617993]\n",
      "epoch:35 step:28054 [D loss: 0.024358, acc.: 100.00%] [G loss: 5.014092]\n",
      "epoch:35 step:28055 [D loss: 0.009087, acc.: 100.00%] [G loss: 4.086593]\n",
      "epoch:35 step:28056 [D loss: 0.036688, acc.: 100.00%] [G loss: 2.846051]\n",
      "epoch:35 step:28057 [D loss: 0.017712, acc.: 100.00%] [G loss: 4.511647]\n",
      "epoch:35 step:28058 [D loss: 0.027265, acc.: 100.00%] [G loss: 3.798935]\n",
      "epoch:35 step:28059 [D loss: 0.211646, acc.: 90.62%] [G loss: 7.148826]\n",
      "epoch:35 step:28060 [D loss: 1.510254, acc.: 33.59%] [G loss: 5.578612]\n",
      "epoch:35 step:28061 [D loss: 0.002118, acc.: 100.00%] [G loss: 6.213244]\n",
      "epoch:35 step:28062 [D loss: 0.052556, acc.: 99.22%] [G loss: 5.947316]\n",
      "epoch:35 step:28063 [D loss: 0.010430, acc.: 100.00%] [G loss: 4.614898]\n",
      "epoch:35 step:28064 [D loss: 0.003942, acc.: 100.00%] [G loss: 4.916092]\n",
      "epoch:35 step:28065 [D loss: 0.020502, acc.: 100.00%] [G loss: 4.680157]\n",
      "epoch:35 step:28066 [D loss: 0.008326, acc.: 100.00%] [G loss: 3.767531]\n",
      "epoch:35 step:28067 [D loss: 0.026580, acc.: 100.00%] [G loss: 4.376568]\n",
      "epoch:35 step:28068 [D loss: 0.040572, acc.: 100.00%] [G loss: 3.755562]\n",
      "epoch:35 step:28069 [D loss: 0.021714, acc.: 100.00%] [G loss: 4.735449]\n",
      "epoch:35 step:28070 [D loss: 0.072567, acc.: 99.22%] [G loss: 3.302292]\n",
      "epoch:35 step:28071 [D loss: 0.035319, acc.: 100.00%] [G loss: 3.604153]\n",
      "epoch:35 step:28072 [D loss: 0.018608, acc.: 100.00%] [G loss: 4.187628]\n",
      "epoch:35 step:28073 [D loss: 0.020422, acc.: 100.00%] [G loss: 4.252390]\n",
      "epoch:35 step:28074 [D loss: 0.057324, acc.: 99.22%] [G loss: 4.501775]\n",
      "epoch:35 step:28075 [D loss: 0.136982, acc.: 96.88%] [G loss: 3.614995]\n",
      "epoch:35 step:28076 [D loss: 0.016384, acc.: 100.00%] [G loss: 4.623884]\n",
      "epoch:35 step:28077 [D loss: 0.010992, acc.: 100.00%] [G loss: 4.305675]\n",
      "epoch:35 step:28078 [D loss: 0.066398, acc.: 98.44%] [G loss: 3.333849]\n",
      "epoch:35 step:28079 [D loss: 0.048965, acc.: 99.22%] [G loss: 3.883098]\n",
      "epoch:35 step:28080 [D loss: 0.017657, acc.: 100.00%] [G loss: 4.473786]\n",
      "epoch:35 step:28081 [D loss: 0.018467, acc.: 100.00%] [G loss: 4.410142]\n",
      "epoch:35 step:28082 [D loss: 0.030806, acc.: 100.00%] [G loss: 4.085310]\n",
      "epoch:35 step:28083 [D loss: 0.020578, acc.: 99.22%] [G loss: 5.513907]\n",
      "epoch:35 step:28084 [D loss: 0.011789, acc.: 100.00%] [G loss: 4.661638]\n",
      "epoch:35 step:28085 [D loss: 0.053812, acc.: 97.66%] [G loss: 3.482704]\n",
      "epoch:35 step:28086 [D loss: 0.019455, acc.: 100.00%] [G loss: 3.951092]\n",
      "epoch:35 step:28087 [D loss: 0.034662, acc.: 100.00%] [G loss: 4.507645]\n",
      "epoch:35 step:28088 [D loss: 0.045024, acc.: 99.22%] [G loss: 5.357412]\n",
      "epoch:35 step:28089 [D loss: 0.072952, acc.: 99.22%] [G loss: 4.780688]\n",
      "epoch:35 step:28090 [D loss: 0.005277, acc.: 100.00%] [G loss: 5.628228]\n",
      "epoch:35 step:28091 [D loss: 0.027939, acc.: 99.22%] [G loss: 4.227628]\n",
      "epoch:35 step:28092 [D loss: 0.014859, acc.: 100.00%] [G loss: 4.687567]\n",
      "epoch:35 step:28093 [D loss: 0.058416, acc.: 98.44%] [G loss: 5.755684]\n",
      "epoch:35 step:28094 [D loss: 0.019842, acc.: 100.00%] [G loss: 5.912354]\n",
      "epoch:35 step:28095 [D loss: 0.043971, acc.: 99.22%] [G loss: 4.365731]\n",
      "epoch:35 step:28096 [D loss: 0.050885, acc.: 100.00%] [G loss: 5.606452]\n",
      "epoch:35 step:28097 [D loss: 0.756626, acc.: 61.72%] [G loss: 7.915091]\n",
      "epoch:35 step:28098 [D loss: 2.143940, acc.: 50.00%] [G loss: 4.863227]\n",
      "epoch:35 step:28099 [D loss: 0.074800, acc.: 97.66%] [G loss: 4.569072]\n",
      "epoch:35 step:28100 [D loss: 0.009108, acc.: 100.00%] [G loss: 4.493965]\n",
      "epoch:35 step:28101 [D loss: 0.031570, acc.: 100.00%] [G loss: 4.451332]\n",
      "epoch:35 step:28102 [D loss: 0.033133, acc.: 99.22%] [G loss: 4.050421]\n",
      "epoch:35 step:28103 [D loss: 0.031585, acc.: 99.22%] [G loss: 4.115410]\n",
      "epoch:35 step:28104 [D loss: 0.022760, acc.: 100.00%] [G loss: 4.169585]\n",
      "epoch:35 step:28105 [D loss: 0.023591, acc.: 100.00%] [G loss: 3.555677]\n",
      "epoch:35 step:28106 [D loss: 0.109149, acc.: 96.09%] [G loss: 4.224035]\n",
      "epoch:35 step:28107 [D loss: 0.013946, acc.: 100.00%] [G loss: 4.105609]\n",
      "epoch:35 step:28108 [D loss: 0.011979, acc.: 100.00%] [G loss: 4.106371]\n",
      "epoch:35 step:28109 [D loss: 0.042023, acc.: 100.00%] [G loss: 4.306804]\n",
      "epoch:35 step:28110 [D loss: 0.063163, acc.: 97.66%] [G loss: 3.926848]\n",
      "epoch:35 step:28111 [D loss: 0.125861, acc.: 96.88%] [G loss: 4.431785]\n",
      "epoch:35 step:28112 [D loss: 0.110572, acc.: 95.31%] [G loss: 3.724436]\n",
      "epoch:35 step:28113 [D loss: 0.036146, acc.: 100.00%] [G loss: 4.392881]\n",
      "epoch:35 step:28114 [D loss: 0.041600, acc.: 99.22%] [G loss: 4.477828]\n",
      "epoch:35 step:28115 [D loss: 0.043143, acc.: 100.00%] [G loss: 4.489355]\n",
      "epoch:35 step:28116 [D loss: 0.012990, acc.: 100.00%] [G loss: 4.318370]\n",
      "epoch:36 step:28117 [D loss: 0.015208, acc.: 100.00%] [G loss: 3.999857]\n",
      "epoch:36 step:28118 [D loss: 0.059748, acc.: 99.22%] [G loss: 3.649241]\n",
      "epoch:36 step:28119 [D loss: 0.171951, acc.: 91.41%] [G loss: 6.431157]\n",
      "epoch:36 step:28120 [D loss: 0.096905, acc.: 94.53%] [G loss: 6.250612]\n",
      "epoch:36 step:28121 [D loss: 0.004935, acc.: 100.00%] [G loss: 6.011953]\n",
      "epoch:36 step:28122 [D loss: 0.010342, acc.: 100.00%] [G loss: 5.715804]\n",
      "epoch:36 step:28123 [D loss: 0.003551, acc.: 100.00%] [G loss: 5.204597]\n",
      "epoch:36 step:28124 [D loss: 0.015359, acc.: 100.00%] [G loss: 5.422982]\n",
      "epoch:36 step:28125 [D loss: 0.008503, acc.: 100.00%] [G loss: 5.339405]\n",
      "epoch:36 step:28126 [D loss: 0.007970, acc.: 100.00%] [G loss: 5.170286]\n",
      "epoch:36 step:28127 [D loss: 0.003142, acc.: 100.00%] [G loss: 4.872790]\n",
      "epoch:36 step:28128 [D loss: 0.014112, acc.: 100.00%] [G loss: 4.700105]\n",
      "epoch:36 step:28129 [D loss: 0.020033, acc.: 100.00%] [G loss: 5.032727]\n",
      "epoch:36 step:28130 [D loss: 0.029870, acc.: 99.22%] [G loss: 4.728782]\n",
      "epoch:36 step:28131 [D loss: 0.007095, acc.: 100.00%] [G loss: 4.899212]\n",
      "epoch:36 step:28132 [D loss: 0.009198, acc.: 100.00%] [G loss: 4.998960]\n",
      "epoch:36 step:28133 [D loss: 0.012011, acc.: 100.00%] [G loss: 4.640140]\n",
      "epoch:36 step:28134 [D loss: 0.014031, acc.: 100.00%] [G loss: 4.199861]\n",
      "epoch:36 step:28135 [D loss: 0.010421, acc.: 100.00%] [G loss: 4.617172]\n",
      "epoch:36 step:28136 [D loss: 0.008274, acc.: 100.00%] [G loss: 4.495162]\n",
      "epoch:36 step:28137 [D loss: 0.013971, acc.: 100.00%] [G loss: 4.473235]\n",
      "epoch:36 step:28138 [D loss: 0.024041, acc.: 100.00%] [G loss: 4.466477]\n",
      "epoch:36 step:28139 [D loss: 0.489067, acc.: 73.44%] [G loss: 5.953184]\n",
      "epoch:36 step:28140 [D loss: 0.003494, acc.: 100.00%] [G loss: 7.142579]\n",
      "epoch:36 step:28141 [D loss: 0.019680, acc.: 99.22%] [G loss: 6.821302]\n",
      "epoch:36 step:28142 [D loss: 0.016034, acc.: 99.22%] [G loss: 6.614401]\n",
      "epoch:36 step:28143 [D loss: 0.007070, acc.: 100.00%] [G loss: 5.800900]\n",
      "epoch:36 step:28144 [D loss: 0.011279, acc.: 100.00%] [G loss: 5.725056]\n",
      "epoch:36 step:28145 [D loss: 0.007263, acc.: 100.00%] [G loss: 4.539538]\n",
      "epoch:36 step:28146 [D loss: 0.013319, acc.: 100.00%] [G loss: 5.119905]\n",
      "epoch:36 step:28147 [D loss: 0.034728, acc.: 99.22%] [G loss: 5.122977]\n",
      "epoch:36 step:28148 [D loss: 0.003566, acc.: 100.00%] [G loss: 5.532392]\n",
      "epoch:36 step:28149 [D loss: 0.012843, acc.: 100.00%] [G loss: 5.657154]\n",
      "epoch:36 step:28150 [D loss: 0.010507, acc.: 100.00%] [G loss: 4.789067]\n",
      "epoch:36 step:28151 [D loss: 0.008787, acc.: 100.00%] [G loss: 5.006161]\n",
      "epoch:36 step:28152 [D loss: 1.181509, acc.: 46.09%] [G loss: 8.371755]\n",
      "epoch:36 step:28153 [D loss: 1.898498, acc.: 52.34%] [G loss: 6.039007]\n",
      "epoch:36 step:28154 [D loss: 0.147366, acc.: 92.97%] [G loss: 3.111071]\n",
      "epoch:36 step:28155 [D loss: 0.183537, acc.: 94.53%] [G loss: 5.465504]\n",
      "epoch:36 step:28156 [D loss: 0.004899, acc.: 100.00%] [G loss: 5.912344]\n",
      "epoch:36 step:28157 [D loss: 0.229565, acc.: 92.97%] [G loss: 3.903198]\n",
      "epoch:36 step:28158 [D loss: 0.047893, acc.: 99.22%] [G loss: 3.214853]\n",
      "epoch:36 step:28159 [D loss: 0.030531, acc.: 100.00%] [G loss: 3.405247]\n",
      "epoch:36 step:28160 [D loss: 0.033907, acc.: 99.22%] [G loss: 4.269098]\n",
      "epoch:36 step:28161 [D loss: 0.027459, acc.: 100.00%] [G loss: 4.111574]\n",
      "epoch:36 step:28162 [D loss: 0.023870, acc.: 100.00%] [G loss: 3.639765]\n",
      "epoch:36 step:28163 [D loss: 0.010740, acc.: 100.00%] [G loss: 4.002316]\n",
      "epoch:36 step:28164 [D loss: 0.011168, acc.: 100.00%] [G loss: 4.334533]\n",
      "epoch:36 step:28165 [D loss: 0.020464, acc.: 100.00%] [G loss: 4.120056]\n",
      "epoch:36 step:28166 [D loss: 0.025571, acc.: 100.00%] [G loss: 3.861001]\n",
      "epoch:36 step:28167 [D loss: 0.024150, acc.: 100.00%] [G loss: 4.513669]\n",
      "epoch:36 step:28168 [D loss: 0.047364, acc.: 99.22%] [G loss: 4.152378]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:36 step:28169 [D loss: 0.007537, acc.: 100.00%] [G loss: 4.201604]\n",
      "epoch:36 step:28170 [D loss: 0.121720, acc.: 98.44%] [G loss: 3.100174]\n",
      "epoch:36 step:28171 [D loss: 0.017298, acc.: 100.00%] [G loss: 2.896716]\n",
      "epoch:36 step:28172 [D loss: 0.008428, acc.: 100.00%] [G loss: 3.495959]\n",
      "epoch:36 step:28173 [D loss: 0.008487, acc.: 100.00%] [G loss: 3.510556]\n",
      "epoch:36 step:28174 [D loss: 0.013459, acc.: 100.00%] [G loss: 3.837275]\n",
      "epoch:36 step:28175 [D loss: 0.022378, acc.: 100.00%] [G loss: 3.726091]\n",
      "epoch:36 step:28176 [D loss: 0.007066, acc.: 100.00%] [G loss: 3.701179]\n",
      "epoch:36 step:28177 [D loss: 0.024462, acc.: 100.00%] [G loss: 3.192872]\n",
      "epoch:36 step:28178 [D loss: 0.072800, acc.: 99.22%] [G loss: 4.682502]\n",
      "epoch:36 step:28179 [D loss: 0.945685, acc.: 46.88%] [G loss: 7.933528]\n",
      "epoch:36 step:28180 [D loss: 1.651581, acc.: 53.91%] [G loss: 5.501729]\n",
      "epoch:36 step:28181 [D loss: 0.237296, acc.: 90.62%] [G loss: 2.161622]\n",
      "epoch:36 step:28182 [D loss: 0.260590, acc.: 84.38%] [G loss: 4.450108]\n",
      "epoch:36 step:28183 [D loss: 0.009768, acc.: 100.00%] [G loss: 5.594130]\n",
      "epoch:36 step:28184 [D loss: 0.008284, acc.: 100.00%] [G loss: 5.395684]\n",
      "epoch:36 step:28185 [D loss: 0.058706, acc.: 97.66%] [G loss: 4.665298]\n",
      "epoch:36 step:28186 [D loss: 0.014188, acc.: 100.00%] [G loss: 4.466250]\n",
      "epoch:36 step:28187 [D loss: 0.013065, acc.: 100.00%] [G loss: 4.481856]\n",
      "epoch:36 step:28188 [D loss: 0.013602, acc.: 100.00%] [G loss: 4.205524]\n",
      "epoch:36 step:28189 [D loss: 0.013812, acc.: 100.00%] [G loss: 3.819101]\n",
      "epoch:36 step:28190 [D loss: 0.027111, acc.: 100.00%] [G loss: 4.728280]\n",
      "epoch:36 step:28191 [D loss: 0.022564, acc.: 99.22%] [G loss: 4.217791]\n",
      "epoch:36 step:28192 [D loss: 0.014706, acc.: 100.00%] [G loss: 4.152385]\n",
      "epoch:36 step:28193 [D loss: 0.573900, acc.: 71.88%] [G loss: 6.735632]\n",
      "epoch:36 step:28194 [D loss: 0.185079, acc.: 90.62%] [G loss: 6.523305]\n",
      "epoch:36 step:28195 [D loss: 0.264605, acc.: 87.50%] [G loss: 5.108855]\n",
      "epoch:36 step:28196 [D loss: 0.133422, acc.: 95.31%] [G loss: 4.337585]\n",
      "epoch:36 step:28197 [D loss: 0.004906, acc.: 100.00%] [G loss: 5.159287]\n",
      "epoch:36 step:28198 [D loss: 0.003208, acc.: 100.00%] [G loss: 5.591504]\n",
      "epoch:36 step:28199 [D loss: 0.004788, acc.: 100.00%] [G loss: 5.262966]\n",
      "epoch:36 step:28200 [D loss: 0.005712, acc.: 100.00%] [G loss: 5.027090]\n",
      "##############\n",
      "[0.89436355 0.97443142 0.84362628 0.93751512 2.10439085 1.11759534\n",
      " 1.12124595 2.11666909 2.11013184 2.11159987]\n",
      "##########\n",
      "epoch:36 step:28201 [D loss: 0.006616, acc.: 100.00%] [G loss: 4.557817]\n",
      "epoch:36 step:28202 [D loss: 0.013764, acc.: 100.00%] [G loss: 4.762502]\n",
      "epoch:36 step:28203 [D loss: 0.012046, acc.: 100.00%] [G loss: 4.454508]\n",
      "epoch:36 step:28204 [D loss: 0.006026, acc.: 100.00%] [G loss: 4.895017]\n",
      "epoch:36 step:28205 [D loss: 0.006298, acc.: 100.00%] [G loss: 4.089182]\n",
      "epoch:36 step:28206 [D loss: 1.003877, acc.: 53.12%] [G loss: 7.512480]\n",
      "epoch:36 step:28207 [D loss: 0.351246, acc.: 83.59%] [G loss: 6.160164]\n",
      "epoch:36 step:28208 [D loss: 0.002698, acc.: 100.00%] [G loss: 6.106086]\n",
      "epoch:36 step:28209 [D loss: 0.004385, acc.: 100.00%] [G loss: 5.886954]\n",
      "epoch:36 step:28210 [D loss: 0.004026, acc.: 100.00%] [G loss: 5.734438]\n",
      "epoch:36 step:28211 [D loss: 0.007368, acc.: 100.00%] [G loss: 5.439562]\n",
      "epoch:36 step:28212 [D loss: 0.009379, acc.: 100.00%] [G loss: 4.720594]\n",
      "epoch:36 step:28213 [D loss: 0.043682, acc.: 99.22%] [G loss: 4.858508]\n",
      "epoch:36 step:28214 [D loss: 0.057288, acc.: 99.22%] [G loss: 4.834921]\n",
      "epoch:36 step:28215 [D loss: 0.357459, acc.: 82.03%] [G loss: 6.257859]\n",
      "epoch:36 step:28216 [D loss: 0.006643, acc.: 100.00%] [G loss: 7.183876]\n",
      "epoch:36 step:28217 [D loss: 0.163700, acc.: 89.84%] [G loss: 5.025075]\n",
      "epoch:36 step:28218 [D loss: 0.017107, acc.: 100.00%] [G loss: 4.538740]\n",
      "epoch:36 step:28219 [D loss: 0.020591, acc.: 100.00%] [G loss: 5.215133]\n",
      "epoch:36 step:28220 [D loss: 0.004316, acc.: 100.00%] [G loss: 5.291304]\n",
      "epoch:36 step:28221 [D loss: 0.002944, acc.: 100.00%] [G loss: 5.402748]\n",
      "epoch:36 step:28222 [D loss: 0.006041, acc.: 100.00%] [G loss: 4.515565]\n",
      "epoch:36 step:28223 [D loss: 0.021364, acc.: 100.00%] [G loss: 4.819458]\n",
      "epoch:36 step:28224 [D loss: 0.007401, acc.: 100.00%] [G loss: 5.575689]\n",
      "epoch:36 step:28225 [D loss: 0.003274, acc.: 100.00%] [G loss: 5.337227]\n",
      "epoch:36 step:28226 [D loss: 0.046334, acc.: 100.00%] [G loss: 4.646131]\n",
      "epoch:36 step:28227 [D loss: 0.017619, acc.: 100.00%] [G loss: 4.159913]\n",
      "epoch:36 step:28228 [D loss: 0.015929, acc.: 100.00%] [G loss: 5.216902]\n",
      "epoch:36 step:28229 [D loss: 0.001268, acc.: 100.00%] [G loss: 6.366456]\n",
      "epoch:36 step:28230 [D loss: 0.012181, acc.: 100.00%] [G loss: 5.511069]\n",
      "epoch:36 step:28231 [D loss: 0.005144, acc.: 100.00%] [G loss: 4.766766]\n",
      "epoch:36 step:28232 [D loss: 0.056018, acc.: 98.44%] [G loss: 5.195932]\n",
      "epoch:36 step:28233 [D loss: 0.033103, acc.: 100.00%] [G loss: 5.003782]\n",
      "epoch:36 step:28234 [D loss: 0.003353, acc.: 100.00%] [G loss: 5.358451]\n",
      "epoch:36 step:28235 [D loss: 0.010694, acc.: 100.00%] [G loss: 4.677914]\n",
      "epoch:36 step:28236 [D loss: 0.004044, acc.: 100.00%] [G loss: 5.355998]\n",
      "epoch:36 step:28237 [D loss: 0.014378, acc.: 100.00%] [G loss: 5.148447]\n",
      "epoch:36 step:28238 [D loss: 0.012950, acc.: 100.00%] [G loss: 4.346961]\n",
      "epoch:36 step:28239 [D loss: 0.005924, acc.: 100.00%] [G loss: 4.310697]\n",
      "epoch:36 step:28240 [D loss: 0.033639, acc.: 100.00%] [G loss: 4.334291]\n",
      "epoch:36 step:28241 [D loss: 0.014038, acc.: 100.00%] [G loss: 4.682742]\n",
      "epoch:36 step:28242 [D loss: 0.013861, acc.: 100.00%] [G loss: 5.256108]\n",
      "epoch:36 step:28243 [D loss: 0.004806, acc.: 100.00%] [G loss: 4.748253]\n",
      "epoch:36 step:28244 [D loss: 0.027303, acc.: 99.22%] [G loss: 4.108140]\n",
      "epoch:36 step:28245 [D loss: 0.065892, acc.: 98.44%] [G loss: 5.026877]\n",
      "epoch:36 step:28246 [D loss: 0.017610, acc.: 100.00%] [G loss: 5.118178]\n",
      "epoch:36 step:28247 [D loss: 0.011425, acc.: 100.00%] [G loss: 4.721467]\n",
      "epoch:36 step:28248 [D loss: 0.075740, acc.: 98.44%] [G loss: 3.440848]\n",
      "epoch:36 step:28249 [D loss: 0.031234, acc.: 99.22%] [G loss: 4.329172]\n",
      "epoch:36 step:28250 [D loss: 0.011577, acc.: 100.00%] [G loss: 4.856924]\n",
      "epoch:36 step:28251 [D loss: 0.011707, acc.: 100.00%] [G loss: 4.300288]\n",
      "epoch:36 step:28252 [D loss: 0.024068, acc.: 100.00%] [G loss: 3.781708]\n",
      "epoch:36 step:28253 [D loss: 0.010714, acc.: 100.00%] [G loss: 4.322505]\n",
      "epoch:36 step:28254 [D loss: 0.006477, acc.: 100.00%] [G loss: 3.281354]\n",
      "epoch:36 step:28255 [D loss: 0.044315, acc.: 100.00%] [G loss: 5.182049]\n",
      "epoch:36 step:28256 [D loss: 0.107895, acc.: 97.66%] [G loss: 3.006173]\n",
      "epoch:36 step:28257 [D loss: 0.040798, acc.: 100.00%] [G loss: 4.966144]\n",
      "epoch:36 step:28258 [D loss: 0.010443, acc.: 100.00%] [G loss: 5.483015]\n",
      "epoch:36 step:28259 [D loss: 0.023965, acc.: 100.00%] [G loss: 4.669195]\n",
      "epoch:36 step:28260 [D loss: 0.007523, acc.: 100.00%] [G loss: 4.385759]\n",
      "epoch:36 step:28261 [D loss: 0.061493, acc.: 97.66%] [G loss: 3.275934]\n",
      "epoch:36 step:28262 [D loss: 0.125373, acc.: 95.31%] [G loss: 6.899202]\n",
      "epoch:36 step:28263 [D loss: 0.016032, acc.: 100.00%] [G loss: 7.566185]\n",
      "epoch:36 step:28264 [D loss: 0.017711, acc.: 100.00%] [G loss: 7.075534]\n",
      "epoch:36 step:28265 [D loss: 0.007840, acc.: 100.00%] [G loss: 6.679917]\n",
      "epoch:36 step:28266 [D loss: 0.012005, acc.: 100.00%] [G loss: 6.381433]\n",
      "epoch:36 step:28267 [D loss: 0.004124, acc.: 100.00%] [G loss: 6.448842]\n",
      "epoch:36 step:28268 [D loss: 0.005278, acc.: 100.00%] [G loss: 5.340013]\n",
      "epoch:36 step:28269 [D loss: 0.027365, acc.: 99.22%] [G loss: 4.936300]\n",
      "epoch:36 step:28270 [D loss: 0.015168, acc.: 100.00%] [G loss: 5.623591]\n",
      "epoch:36 step:28271 [D loss: 0.019464, acc.: 100.00%] [G loss: 5.688257]\n",
      "epoch:36 step:28272 [D loss: 0.007782, acc.: 100.00%] [G loss: 5.419667]\n",
      "epoch:36 step:28273 [D loss: 0.016834, acc.: 100.00%] [G loss: 4.982136]\n",
      "epoch:36 step:28274 [D loss: 0.030817, acc.: 100.00%] [G loss: 4.815243]\n",
      "epoch:36 step:28275 [D loss: 0.013469, acc.: 100.00%] [G loss: 5.095493]\n",
      "epoch:36 step:28276 [D loss: 0.010700, acc.: 100.00%] [G loss: 4.009084]\n",
      "epoch:36 step:28277 [D loss: 0.010550, acc.: 100.00%] [G loss: 4.730134]\n",
      "epoch:36 step:28278 [D loss: 0.007298, acc.: 100.00%] [G loss: 5.088765]\n",
      "epoch:36 step:28279 [D loss: 0.003246, acc.: 100.00%] [G loss: 4.886266]\n",
      "epoch:36 step:28280 [D loss: 0.007508, acc.: 100.00%] [G loss: 4.540123]\n",
      "epoch:36 step:28281 [D loss: 0.006847, acc.: 100.00%] [G loss: 4.152631]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:36 step:28282 [D loss: 0.020925, acc.: 100.00%] [G loss: 5.102294]\n",
      "epoch:36 step:28283 [D loss: 0.003289, acc.: 100.00%] [G loss: 4.284272]\n",
      "epoch:36 step:28284 [D loss: 0.003197, acc.: 100.00%] [G loss: 4.583117]\n",
      "epoch:36 step:28285 [D loss: 0.036592, acc.: 100.00%] [G loss: 5.416221]\n",
      "epoch:36 step:28286 [D loss: 0.011889, acc.: 100.00%] [G loss: 5.836442]\n",
      "epoch:36 step:28287 [D loss: 0.003401, acc.: 100.00%] [G loss: 6.059081]\n",
      "epoch:36 step:28288 [D loss: 0.014350, acc.: 100.00%] [G loss: 5.502045]\n",
      "epoch:36 step:28289 [D loss: 0.012188, acc.: 100.00%] [G loss: 5.328343]\n",
      "epoch:36 step:28290 [D loss: 0.029045, acc.: 100.00%] [G loss: 5.629598]\n",
      "epoch:36 step:28291 [D loss: 0.009550, acc.: 100.00%] [G loss: 4.942606]\n",
      "epoch:36 step:28292 [D loss: 0.543375, acc.: 73.44%] [G loss: 8.804886]\n",
      "epoch:36 step:28293 [D loss: 1.571397, acc.: 53.91%] [G loss: 3.065454]\n",
      "epoch:36 step:28294 [D loss: 0.709835, acc.: 75.00%] [G loss: 8.081327]\n",
      "epoch:36 step:28295 [D loss: 0.042461, acc.: 99.22%] [G loss: 8.167150]\n",
      "epoch:36 step:28296 [D loss: 0.498006, acc.: 79.69%] [G loss: 4.174891]\n",
      "epoch:36 step:28297 [D loss: 0.550038, acc.: 78.12%] [G loss: 7.613900]\n",
      "epoch:36 step:28298 [D loss: 0.094295, acc.: 96.88%] [G loss: 7.218970]\n",
      "epoch:36 step:28299 [D loss: 0.252413, acc.: 87.50%] [G loss: 5.063360]\n",
      "epoch:36 step:28300 [D loss: 0.011049, acc.: 100.00%] [G loss: 3.667633]\n",
      "epoch:36 step:28301 [D loss: 0.057346, acc.: 98.44%] [G loss: 3.778662]\n",
      "epoch:36 step:28302 [D loss: 0.044988, acc.: 99.22%] [G loss: 5.033303]\n",
      "epoch:36 step:28303 [D loss: 0.005520, acc.: 100.00%] [G loss: 4.282292]\n",
      "epoch:36 step:28304 [D loss: 0.008813, acc.: 100.00%] [G loss: 4.019732]\n",
      "epoch:36 step:28305 [D loss: 0.029327, acc.: 100.00%] [G loss: 3.317179]\n",
      "epoch:36 step:28306 [D loss: 0.009609, acc.: 100.00%] [G loss: 4.753251]\n",
      "epoch:36 step:28307 [D loss: 0.108297, acc.: 97.66%] [G loss: 4.934815]\n",
      "epoch:36 step:28308 [D loss: 0.009013, acc.: 99.22%] [G loss: 5.584002]\n",
      "epoch:36 step:28309 [D loss: 0.824620, acc.: 54.69%] [G loss: 5.500965]\n",
      "epoch:36 step:28310 [D loss: 0.003069, acc.: 100.00%] [G loss: 6.256833]\n",
      "epoch:36 step:28311 [D loss: 0.004222, acc.: 100.00%] [G loss: 5.639544]\n",
      "epoch:36 step:28312 [D loss: 0.003024, acc.: 100.00%] [G loss: 5.556893]\n",
      "epoch:36 step:28313 [D loss: 0.003524, acc.: 100.00%] [G loss: 5.123075]\n",
      "epoch:36 step:28314 [D loss: 0.008445, acc.: 100.00%] [G loss: 4.911776]\n",
      "epoch:36 step:28315 [D loss: 0.020056, acc.: 100.00%] [G loss: 5.255496]\n",
      "epoch:36 step:28316 [D loss: 0.015964, acc.: 99.22%] [G loss: 4.744357]\n",
      "epoch:36 step:28317 [D loss: 0.013393, acc.: 100.00%] [G loss: 4.242062]\n",
      "epoch:36 step:28318 [D loss: 0.015242, acc.: 100.00%] [G loss: 4.055561]\n",
      "epoch:36 step:28319 [D loss: 0.022543, acc.: 100.00%] [G loss: 3.266177]\n",
      "epoch:36 step:28320 [D loss: 0.038891, acc.: 100.00%] [G loss: 4.157578]\n",
      "epoch:36 step:28321 [D loss: 0.080944, acc.: 97.66%] [G loss: 4.711215]\n",
      "epoch:36 step:28322 [D loss: 0.045120, acc.: 99.22%] [G loss: 3.578998]\n",
      "epoch:36 step:28323 [D loss: 0.003311, acc.: 100.00%] [G loss: 3.779991]\n",
      "epoch:36 step:28324 [D loss: 0.022806, acc.: 100.00%] [G loss: 4.605936]\n",
      "epoch:36 step:28325 [D loss: 0.073615, acc.: 96.88%] [G loss: 2.846283]\n",
      "epoch:36 step:28326 [D loss: 0.045664, acc.: 100.00%] [G loss: 3.826556]\n",
      "epoch:36 step:28327 [D loss: 0.013643, acc.: 100.00%] [G loss: 5.630930]\n",
      "epoch:36 step:28328 [D loss: 0.088306, acc.: 97.66%] [G loss: 3.867304]\n",
      "epoch:36 step:28329 [D loss: 0.067886, acc.: 99.22%] [G loss: 5.181124]\n",
      "epoch:36 step:28330 [D loss: 2.049175, acc.: 25.00%] [G loss: 7.943814]\n",
      "epoch:36 step:28331 [D loss: 1.169809, acc.: 54.69%] [G loss: 5.127416]\n",
      "epoch:36 step:28332 [D loss: 0.038614, acc.: 99.22%] [G loss: 3.892554]\n",
      "epoch:36 step:28333 [D loss: 0.012051, acc.: 100.00%] [G loss: 3.232355]\n",
      "epoch:36 step:28334 [D loss: 0.096811, acc.: 96.88%] [G loss: 3.806003]\n",
      "epoch:36 step:28335 [D loss: 0.033786, acc.: 100.00%] [G loss: 4.213324]\n",
      "epoch:36 step:28336 [D loss: 0.083042, acc.: 97.66%] [G loss: 3.593916]\n",
      "epoch:36 step:28337 [D loss: 0.032753, acc.: 99.22%] [G loss: 3.713330]\n",
      "epoch:36 step:28338 [D loss: 0.011331, acc.: 100.00%] [G loss: 3.157597]\n",
      "epoch:36 step:28339 [D loss: 0.076240, acc.: 100.00%] [G loss: 2.685436]\n",
      "epoch:36 step:28340 [D loss: 0.044650, acc.: 100.00%] [G loss: 2.948497]\n",
      "epoch:36 step:28341 [D loss: 0.010102, acc.: 100.00%] [G loss: 3.282520]\n",
      "epoch:36 step:28342 [D loss: 0.105306, acc.: 97.66%] [G loss: 4.705748]\n",
      "epoch:36 step:28343 [D loss: 0.148693, acc.: 96.09%] [G loss: 2.654871]\n",
      "epoch:36 step:28344 [D loss: 0.099044, acc.: 96.88%] [G loss: 4.319324]\n",
      "epoch:36 step:28345 [D loss: 0.073510, acc.: 98.44%] [G loss: 4.670647]\n",
      "epoch:36 step:28346 [D loss: 0.024180, acc.: 100.00%] [G loss: 3.873165]\n",
      "epoch:36 step:28347 [D loss: 0.129189, acc.: 94.53%] [G loss: 1.939464]\n",
      "epoch:36 step:28348 [D loss: 0.024729, acc.: 100.00%] [G loss: 1.956026]\n",
      "epoch:36 step:28349 [D loss: 0.017231, acc.: 100.00%] [G loss: 1.434467]\n",
      "epoch:36 step:28350 [D loss: 0.894285, acc.: 60.94%] [G loss: 9.315918]\n",
      "epoch:36 step:28351 [D loss: 1.718551, acc.: 50.78%] [G loss: 7.758486]\n",
      "epoch:36 step:28352 [D loss: 0.016300, acc.: 99.22%] [G loss: 6.497818]\n",
      "epoch:36 step:28353 [D loss: 0.006032, acc.: 100.00%] [G loss: 5.462256]\n",
      "epoch:36 step:28354 [D loss: 0.003248, acc.: 100.00%] [G loss: 4.966434]\n",
      "epoch:36 step:28355 [D loss: 0.008695, acc.: 100.00%] [G loss: 5.969414]\n",
      "epoch:36 step:28356 [D loss: 0.417416, acc.: 78.91%] [G loss: 5.626002]\n",
      "epoch:36 step:28357 [D loss: 0.001788, acc.: 100.00%] [G loss: 5.868634]\n",
      "epoch:36 step:28358 [D loss: 0.035161, acc.: 98.44%] [G loss: 6.355055]\n",
      "epoch:36 step:28359 [D loss: 0.004116, acc.: 100.00%] [G loss: 6.244462]\n",
      "epoch:36 step:28360 [D loss: 0.010338, acc.: 100.00%] [G loss: 5.690571]\n",
      "epoch:36 step:28361 [D loss: 0.038890, acc.: 97.66%] [G loss: 5.719720]\n",
      "epoch:36 step:28362 [D loss: 0.014614, acc.: 100.00%] [G loss: 4.946392]\n",
      "epoch:36 step:28363 [D loss: 0.010454, acc.: 100.00%] [G loss: 4.450682]\n",
      "epoch:36 step:28364 [D loss: 0.009097, acc.: 100.00%] [G loss: 5.249319]\n",
      "epoch:36 step:28365 [D loss: 0.007128, acc.: 100.00%] [G loss: 4.539064]\n",
      "epoch:36 step:28366 [D loss: 0.069491, acc.: 98.44%] [G loss: 3.377620]\n",
      "epoch:36 step:28367 [D loss: 0.079789, acc.: 97.66%] [G loss: 4.607584]\n",
      "epoch:36 step:28368 [D loss: 0.005855, acc.: 100.00%] [G loss: 5.311971]\n",
      "epoch:36 step:28369 [D loss: 0.006925, acc.: 100.00%] [G loss: 5.497440]\n",
      "epoch:36 step:28370 [D loss: 0.008765, acc.: 100.00%] [G loss: 4.969718]\n",
      "epoch:36 step:28371 [D loss: 0.012784, acc.: 100.00%] [G loss: 5.086809]\n",
      "epoch:36 step:28372 [D loss: 0.022523, acc.: 99.22%] [G loss: 3.931405]\n",
      "epoch:36 step:28373 [D loss: 0.012339, acc.: 100.00%] [G loss: 4.791468]\n",
      "epoch:36 step:28374 [D loss: 0.034735, acc.: 100.00%] [G loss: 4.385375]\n",
      "epoch:36 step:28375 [D loss: 0.006564, acc.: 100.00%] [G loss: 4.060328]\n",
      "epoch:36 step:28376 [D loss: 0.049131, acc.: 99.22%] [G loss: 3.461817]\n",
      "epoch:36 step:28377 [D loss: 0.036453, acc.: 100.00%] [G loss: 3.126438]\n",
      "epoch:36 step:28378 [D loss: 0.068944, acc.: 99.22%] [G loss: 4.682039]\n",
      "epoch:36 step:28379 [D loss: 0.003926, acc.: 100.00%] [G loss: 5.368583]\n",
      "epoch:36 step:28380 [D loss: 0.051967, acc.: 100.00%] [G loss: 4.279441]\n",
      "epoch:36 step:28381 [D loss: 0.040676, acc.: 100.00%] [G loss: 3.687726]\n",
      "epoch:36 step:28382 [D loss: 0.012088, acc.: 100.00%] [G loss: 3.658567]\n",
      "epoch:36 step:28383 [D loss: 0.058414, acc.: 100.00%] [G loss: 4.694842]\n",
      "epoch:36 step:28384 [D loss: 0.005200, acc.: 100.00%] [G loss: 4.716250]\n",
      "epoch:36 step:28385 [D loss: 0.018189, acc.: 100.00%] [G loss: 5.001103]\n",
      "epoch:36 step:28386 [D loss: 0.044371, acc.: 100.00%] [G loss: 4.929135]\n",
      "epoch:36 step:28387 [D loss: 0.037076, acc.: 100.00%] [G loss: 4.879602]\n",
      "epoch:36 step:28388 [D loss: 0.008082, acc.: 100.00%] [G loss: 4.821982]\n",
      "epoch:36 step:28389 [D loss: 0.130215, acc.: 97.66%] [G loss: 3.658037]\n",
      "epoch:36 step:28390 [D loss: 0.002598, acc.: 100.00%] [G loss: 3.936766]\n",
      "epoch:36 step:28391 [D loss: 0.008803, acc.: 100.00%] [G loss: 4.023472]\n",
      "epoch:36 step:28392 [D loss: 0.016162, acc.: 99.22%] [G loss: 3.898895]\n",
      "epoch:36 step:28393 [D loss: 0.015487, acc.: 100.00%] [G loss: 4.777997]\n",
      "epoch:36 step:28394 [D loss: 0.025498, acc.: 100.00%] [G loss: 4.407346]\n",
      "epoch:36 step:28395 [D loss: 0.064684, acc.: 100.00%] [G loss: 4.350701]\n",
      "epoch:36 step:28396 [D loss: 0.006629, acc.: 100.00%] [G loss: 5.089131]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:36 step:28397 [D loss: 0.023899, acc.: 100.00%] [G loss: 4.132045]\n",
      "epoch:36 step:28398 [D loss: 0.017024, acc.: 100.00%] [G loss: 4.271850]\n",
      "epoch:36 step:28399 [D loss: 0.009338, acc.: 100.00%] [G loss: 5.614043]\n",
      "epoch:36 step:28400 [D loss: 0.006505, acc.: 100.00%] [G loss: 4.442513]\n",
      "##############\n",
      "[1.04531729 1.11668047 0.84837835 0.92054682 0.81508014 0.95211642\n",
      " 2.11355532 0.76106763 1.08132593 2.11144989]\n",
      "##########\n",
      "epoch:36 step:28401 [D loss: 0.018077, acc.: 100.00%] [G loss: 4.775981]\n",
      "epoch:36 step:28402 [D loss: 0.342328, acc.: 84.38%] [G loss: 5.970930]\n",
      "epoch:36 step:28403 [D loss: 0.009682, acc.: 100.00%] [G loss: 7.614575]\n",
      "epoch:36 step:28404 [D loss: 0.027052, acc.: 100.00%] [G loss: 6.833502]\n",
      "epoch:36 step:28405 [D loss: 0.071916, acc.: 98.44%] [G loss: 4.743601]\n",
      "epoch:36 step:28406 [D loss: 0.023088, acc.: 100.00%] [G loss: 4.480628]\n",
      "epoch:36 step:28407 [D loss: 0.010797, acc.: 100.00%] [G loss: 4.898127]\n",
      "epoch:36 step:28408 [D loss: 0.017644, acc.: 100.00%] [G loss: 4.895094]\n",
      "epoch:36 step:28409 [D loss: 0.007158, acc.: 100.00%] [G loss: 4.173620]\n",
      "epoch:36 step:28410 [D loss: 0.012841, acc.: 100.00%] [G loss: 4.285276]\n",
      "epoch:36 step:28411 [D loss: 0.034506, acc.: 99.22%] [G loss: 3.526771]\n",
      "epoch:36 step:28412 [D loss: 0.034838, acc.: 100.00%] [G loss: 4.567751]\n",
      "epoch:36 step:28413 [D loss: 0.034017, acc.: 99.22%] [G loss: 5.045259]\n",
      "epoch:36 step:28414 [D loss: 0.022284, acc.: 99.22%] [G loss: 4.127429]\n",
      "epoch:36 step:28415 [D loss: 0.017524, acc.: 100.00%] [G loss: 4.991189]\n",
      "epoch:36 step:28416 [D loss: 0.017705, acc.: 100.00%] [G loss: 4.245980]\n",
      "epoch:36 step:28417 [D loss: 0.009111, acc.: 100.00%] [G loss: 5.108284]\n",
      "epoch:36 step:28418 [D loss: 0.020612, acc.: 100.00%] [G loss: 4.741150]\n",
      "epoch:36 step:28419 [D loss: 0.019400, acc.: 100.00%] [G loss: 4.516349]\n",
      "epoch:36 step:28420 [D loss: 0.009267, acc.: 100.00%] [G loss: 5.924334]\n",
      "epoch:36 step:28421 [D loss: 0.045374, acc.: 99.22%] [G loss: 3.140213]\n",
      "epoch:36 step:28422 [D loss: 0.052025, acc.: 98.44%] [G loss: 5.273469]\n",
      "epoch:36 step:28423 [D loss: 0.037760, acc.: 99.22%] [G loss: 5.788911]\n",
      "epoch:36 step:28424 [D loss: 0.007985, acc.: 100.00%] [G loss: 5.740013]\n",
      "epoch:36 step:28425 [D loss: 1.425403, acc.: 43.75%] [G loss: 8.132192]\n",
      "epoch:36 step:28426 [D loss: 2.534245, acc.: 50.00%] [G loss: 6.105900]\n",
      "epoch:36 step:28427 [D loss: 2.024383, acc.: 42.19%] [G loss: 2.279108]\n",
      "epoch:36 step:28428 [D loss: 0.222585, acc.: 92.19%] [G loss: 2.387439]\n",
      "epoch:36 step:28429 [D loss: 0.270411, acc.: 85.94%] [G loss: 3.748948]\n",
      "epoch:36 step:28430 [D loss: 0.064266, acc.: 99.22%] [G loss: 3.855285]\n",
      "epoch:36 step:28431 [D loss: 0.537841, acc.: 67.97%] [G loss: 3.012777]\n",
      "epoch:36 step:28432 [D loss: 0.042169, acc.: 100.00%] [G loss: 3.007018]\n",
      "epoch:36 step:28433 [D loss: 0.177632, acc.: 94.53%] [G loss: 3.357368]\n",
      "epoch:36 step:28434 [D loss: 0.036695, acc.: 99.22%] [G loss: 2.925111]\n",
      "epoch:36 step:28435 [D loss: 0.981698, acc.: 48.44%] [G loss: 3.621415]\n",
      "epoch:36 step:28436 [D loss: 0.832975, acc.: 57.81%] [G loss: 2.151891]\n",
      "epoch:36 step:28437 [D loss: 0.159159, acc.: 94.53%] [G loss: 1.348329]\n",
      "epoch:36 step:28438 [D loss: 0.135122, acc.: 97.66%] [G loss: 1.444915]\n",
      "epoch:36 step:28439 [D loss: 0.220348, acc.: 91.41%] [G loss: 1.600255]\n",
      "epoch:36 step:28440 [D loss: 0.082377, acc.: 98.44%] [G loss: 1.995884]\n",
      "epoch:36 step:28441 [D loss: 0.158990, acc.: 94.53%] [G loss: 0.916580]\n",
      "epoch:36 step:28442 [D loss: 0.224205, acc.: 93.75%] [G loss: 1.006494]\n",
      "epoch:36 step:28443 [D loss: 0.060705, acc.: 99.22%] [G loss: 0.839889]\n",
      "epoch:36 step:28444 [D loss: 0.384955, acc.: 81.25%] [G loss: 4.093064]\n",
      "epoch:36 step:28445 [D loss: 0.287791, acc.: 85.94%] [G loss: 3.980742]\n",
      "epoch:36 step:28446 [D loss: 0.350732, acc.: 85.16%] [G loss: 1.980190]\n",
      "epoch:36 step:28447 [D loss: 0.205357, acc.: 92.97%] [G loss: 2.444753]\n",
      "epoch:36 step:28448 [D loss: 0.194802, acc.: 94.53%] [G loss: 3.099814]\n",
      "epoch:36 step:28449 [D loss: 0.101231, acc.: 96.09%] [G loss: 3.322150]\n",
      "epoch:36 step:28450 [D loss: 0.231340, acc.: 92.19%] [G loss: 3.256921]\n",
      "epoch:36 step:28451 [D loss: 0.053034, acc.: 100.00%] [G loss: 2.851187]\n",
      "epoch:36 step:28452 [D loss: 0.066543, acc.: 100.00%] [G loss: 2.919394]\n",
      "epoch:36 step:28453 [D loss: 0.172825, acc.: 96.09%] [G loss: 4.098642]\n",
      "epoch:36 step:28454 [D loss: 0.116047, acc.: 97.66%] [G loss: 4.377811]\n",
      "epoch:36 step:28455 [D loss: 0.221998, acc.: 92.97%] [G loss: 3.737728]\n",
      "epoch:36 step:28456 [D loss: 0.114106, acc.: 96.88%] [G loss: 3.830573]\n",
      "epoch:36 step:28457 [D loss: 0.077017, acc.: 98.44%] [G loss: 3.811216]\n",
      "epoch:36 step:28458 [D loss: 0.106899, acc.: 98.44%] [G loss: 3.446005]\n",
      "epoch:36 step:28459 [D loss: 0.039119, acc.: 100.00%] [G loss: 2.726897]\n",
      "epoch:36 step:28460 [D loss: 0.089168, acc.: 99.22%] [G loss: 4.203263]\n",
      "epoch:36 step:28461 [D loss: 0.049437, acc.: 100.00%] [G loss: 3.993352]\n",
      "epoch:36 step:28462 [D loss: 0.143756, acc.: 96.09%] [G loss: 3.115795]\n",
      "epoch:36 step:28463 [D loss: 0.040199, acc.: 100.00%] [G loss: 2.383127]\n",
      "epoch:36 step:28464 [D loss: 0.089126, acc.: 96.09%] [G loss: 3.257623]\n",
      "epoch:36 step:28465 [D loss: 0.135466, acc.: 94.53%] [G loss: 2.492982]\n",
      "epoch:36 step:28466 [D loss: 1.085189, acc.: 57.03%] [G loss: 7.089200]\n",
      "epoch:36 step:28467 [D loss: 2.106311, acc.: 50.00%] [G loss: 6.011821]\n",
      "epoch:36 step:28468 [D loss: 0.891427, acc.: 61.72%] [G loss: 3.220677]\n",
      "epoch:36 step:28469 [D loss: 0.153720, acc.: 94.53%] [G loss: 2.299620]\n",
      "epoch:36 step:28470 [D loss: 0.333853, acc.: 85.94%] [G loss: 3.652091]\n",
      "epoch:36 step:28471 [D loss: 0.119307, acc.: 95.31%] [G loss: 3.790157]\n",
      "epoch:36 step:28472 [D loss: 0.038963, acc.: 100.00%] [G loss: 3.508865]\n",
      "epoch:36 step:28473 [D loss: 0.076791, acc.: 98.44%] [G loss: 2.548279]\n",
      "epoch:36 step:28474 [D loss: 0.051929, acc.: 100.00%] [G loss: 2.667404]\n",
      "epoch:36 step:28475 [D loss: 0.301663, acc.: 89.06%] [G loss: 3.957065]\n",
      "epoch:36 step:28476 [D loss: 0.110304, acc.: 96.09%] [G loss: 3.370679]\n",
      "epoch:36 step:28477 [D loss: 0.164116, acc.: 95.31%] [G loss: 2.229575]\n",
      "epoch:36 step:28478 [D loss: 0.085885, acc.: 99.22%] [G loss: 1.740243]\n",
      "epoch:36 step:28479 [D loss: 0.147925, acc.: 96.88%] [G loss: 2.141201]\n",
      "epoch:36 step:28480 [D loss: 0.304785, acc.: 85.94%] [G loss: 3.322287]\n",
      "epoch:36 step:28481 [D loss: 0.012770, acc.: 100.00%] [G loss: 3.361997]\n",
      "epoch:36 step:28482 [D loss: 0.068993, acc.: 100.00%] [G loss: 3.379441]\n",
      "epoch:36 step:28483 [D loss: 0.098342, acc.: 100.00%] [G loss: 2.947882]\n",
      "epoch:36 step:28484 [D loss: 0.055013, acc.: 99.22%] [G loss: 2.673513]\n",
      "epoch:36 step:28485 [D loss: 0.065457, acc.: 97.66%] [G loss: 2.359478]\n",
      "epoch:36 step:28486 [D loss: 0.206039, acc.: 94.53%] [G loss: 2.907243]\n",
      "epoch:36 step:28487 [D loss: 0.064381, acc.: 99.22%] [G loss: 3.274445]\n",
      "epoch:36 step:28488 [D loss: 0.031564, acc.: 100.00%] [G loss: 3.443945]\n",
      "epoch:36 step:28489 [D loss: 0.369183, acc.: 80.47%] [G loss: 0.957628]\n",
      "epoch:36 step:28490 [D loss: 0.694262, acc.: 71.09%] [G loss: 4.960035]\n",
      "epoch:36 step:28491 [D loss: 0.059054, acc.: 96.09%] [G loss: 6.073744]\n",
      "epoch:36 step:28492 [D loss: 0.532144, acc.: 72.66%] [G loss: 4.161951]\n",
      "epoch:36 step:28493 [D loss: 0.031749, acc.: 100.00%] [G loss: 3.909863]\n",
      "epoch:36 step:28494 [D loss: 0.047691, acc.: 99.22%] [G loss: 3.410107]\n",
      "epoch:36 step:28495 [D loss: 0.031901, acc.: 100.00%] [G loss: 3.115196]\n",
      "epoch:36 step:28496 [D loss: 0.085876, acc.: 97.66%] [G loss: 3.662400]\n",
      "epoch:36 step:28497 [D loss: 0.046536, acc.: 100.00%] [G loss: 4.225241]\n",
      "epoch:36 step:28498 [D loss: 0.086795, acc.: 98.44%] [G loss: 3.834135]\n",
      "epoch:36 step:28499 [D loss: 0.056791, acc.: 99.22%] [G loss: 3.134073]\n",
      "epoch:36 step:28500 [D loss: 0.044542, acc.: 100.00%] [G loss: 3.251677]\n",
      "epoch:36 step:28501 [D loss: 0.066744, acc.: 99.22%] [G loss: 3.693833]\n",
      "epoch:36 step:28502 [D loss: 0.076655, acc.: 99.22%] [G loss: 3.141262]\n",
      "epoch:36 step:28503 [D loss: 0.036582, acc.: 100.00%] [G loss: 3.644598]\n",
      "epoch:36 step:28504 [D loss: 0.046589, acc.: 100.00%] [G loss: 3.847570]\n",
      "epoch:36 step:28505 [D loss: 0.196717, acc.: 94.53%] [G loss: 2.812487]\n",
      "epoch:36 step:28506 [D loss: 0.034209, acc.: 100.00%] [G loss: 3.004825]\n",
      "epoch:36 step:28507 [D loss: 0.254150, acc.: 89.84%] [G loss: 4.231225]\n",
      "epoch:36 step:28508 [D loss: 0.078038, acc.: 97.66%] [G loss: 4.235579]\n",
      "epoch:36 step:28509 [D loss: 0.126897, acc.: 96.09%] [G loss: 3.367244]\n",
      "epoch:36 step:28510 [D loss: 0.026629, acc.: 100.00%] [G loss: 3.885671]\n",
      "epoch:36 step:28511 [D loss: 0.062906, acc.: 98.44%] [G loss: 3.615210]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:36 step:28512 [D loss: 0.103283, acc.: 99.22%] [G loss: 2.810323]\n",
      "epoch:36 step:28513 [D loss: 0.134886, acc.: 96.09%] [G loss: 4.471158]\n",
      "epoch:36 step:28514 [D loss: 0.040604, acc.: 100.00%] [G loss: 4.753864]\n",
      "epoch:36 step:28515 [D loss: 0.121283, acc.: 96.09%] [G loss: 2.938446]\n",
      "epoch:36 step:28516 [D loss: 0.090349, acc.: 97.66%] [G loss: 3.875190]\n",
      "epoch:36 step:28517 [D loss: 0.013372, acc.: 100.00%] [G loss: 4.266454]\n",
      "epoch:36 step:28518 [D loss: 0.046627, acc.: 98.44%] [G loss: 3.274153]\n",
      "epoch:36 step:28519 [D loss: 0.091990, acc.: 97.66%] [G loss: 2.955168]\n",
      "epoch:36 step:28520 [D loss: 0.085421, acc.: 97.66%] [G loss: 4.262333]\n",
      "epoch:36 step:28521 [D loss: 0.084145, acc.: 96.88%] [G loss: 4.035886]\n",
      "epoch:36 step:28522 [D loss: 0.290492, acc.: 88.28%] [G loss: 4.361193]\n",
      "epoch:36 step:28523 [D loss: 0.012772, acc.: 100.00%] [G loss: 5.783824]\n",
      "epoch:36 step:28524 [D loss: 0.174621, acc.: 91.41%] [G loss: 3.006190]\n",
      "epoch:36 step:28525 [D loss: 0.170588, acc.: 94.53%] [G loss: 5.022013]\n",
      "epoch:36 step:28526 [D loss: 0.006386, acc.: 100.00%] [G loss: 5.820152]\n",
      "epoch:36 step:28527 [D loss: 0.504102, acc.: 77.34%] [G loss: 2.146227]\n",
      "epoch:36 step:28528 [D loss: 0.112241, acc.: 96.09%] [G loss: 3.438128]\n",
      "epoch:36 step:28529 [D loss: 0.024261, acc.: 100.00%] [G loss: 4.233334]\n",
      "epoch:36 step:28530 [D loss: 0.012478, acc.: 100.00%] [G loss: 3.908586]\n",
      "epoch:36 step:28531 [D loss: 0.014437, acc.: 100.00%] [G loss: 4.299839]\n",
      "epoch:36 step:28532 [D loss: 0.030995, acc.: 100.00%] [G loss: 4.431807]\n",
      "epoch:36 step:28533 [D loss: 0.068505, acc.: 98.44%] [G loss: 4.370263]\n",
      "epoch:36 step:28534 [D loss: 0.054645, acc.: 99.22%] [G loss: 4.122732]\n",
      "epoch:36 step:28535 [D loss: 0.040379, acc.: 100.00%] [G loss: 3.925119]\n",
      "epoch:36 step:28536 [D loss: 0.071827, acc.: 99.22%] [G loss: 3.389377]\n",
      "epoch:36 step:28537 [D loss: 1.031056, acc.: 53.91%] [G loss: 6.885569]\n",
      "epoch:36 step:28538 [D loss: 1.328561, acc.: 56.25%] [G loss: 5.639147]\n",
      "epoch:36 step:28539 [D loss: 0.218549, acc.: 92.19%] [G loss: 3.829826]\n",
      "epoch:36 step:28540 [D loss: 0.030471, acc.: 100.00%] [G loss: 3.905015]\n",
      "epoch:36 step:28541 [D loss: 0.073946, acc.: 99.22%] [G loss: 3.880109]\n",
      "epoch:36 step:28542 [D loss: 0.029356, acc.: 99.22%] [G loss: 4.824755]\n",
      "epoch:36 step:28543 [D loss: 0.029465, acc.: 99.22%] [G loss: 4.496019]\n",
      "epoch:36 step:28544 [D loss: 0.022093, acc.: 100.00%] [G loss: 4.596210]\n",
      "epoch:36 step:28545 [D loss: 0.045206, acc.: 100.00%] [G loss: 4.025105]\n",
      "epoch:36 step:28546 [D loss: 0.024749, acc.: 100.00%] [G loss: 3.638687]\n",
      "epoch:36 step:28547 [D loss: 0.048961, acc.: 99.22%] [G loss: 3.622064]\n",
      "epoch:36 step:28548 [D loss: 0.122676, acc.: 96.88%] [G loss: 3.662397]\n",
      "epoch:36 step:28549 [D loss: 0.073983, acc.: 96.88%] [G loss: 4.270904]\n",
      "epoch:36 step:28550 [D loss: 0.025190, acc.: 100.00%] [G loss: 4.531805]\n",
      "epoch:36 step:28551 [D loss: 0.027287, acc.: 100.00%] [G loss: 4.004456]\n",
      "epoch:36 step:28552 [D loss: 0.037867, acc.: 100.00%] [G loss: 3.447814]\n",
      "epoch:36 step:28553 [D loss: 0.057151, acc.: 99.22%] [G loss: 3.356574]\n",
      "epoch:36 step:28554 [D loss: 0.111306, acc.: 96.88%] [G loss: 3.916048]\n",
      "epoch:36 step:28555 [D loss: 0.047112, acc.: 100.00%] [G loss: 3.935352]\n",
      "epoch:36 step:28556 [D loss: 0.088319, acc.: 97.66%] [G loss: 4.084836]\n",
      "epoch:36 step:28557 [D loss: 0.092543, acc.: 98.44%] [G loss: 3.511734]\n",
      "epoch:36 step:28558 [D loss: 0.135248, acc.: 94.53%] [G loss: 3.720498]\n",
      "epoch:36 step:28559 [D loss: 0.049068, acc.: 100.00%] [G loss: 5.014823]\n",
      "epoch:36 step:28560 [D loss: 0.035945, acc.: 98.44%] [G loss: 4.631493]\n",
      "epoch:36 step:28561 [D loss: 0.031117, acc.: 100.00%] [G loss: 3.606081]\n",
      "epoch:36 step:28562 [D loss: 0.146192, acc.: 96.09%] [G loss: 3.415598]\n",
      "epoch:36 step:28563 [D loss: 0.100091, acc.: 99.22%] [G loss: 4.452506]\n",
      "epoch:36 step:28564 [D loss: 0.014593, acc.: 100.00%] [G loss: 5.384868]\n",
      "epoch:36 step:28565 [D loss: 0.271325, acc.: 88.28%] [G loss: 3.337501]\n",
      "epoch:36 step:28566 [D loss: 0.084375, acc.: 98.44%] [G loss: 4.540134]\n",
      "epoch:36 step:28567 [D loss: 0.042437, acc.: 98.44%] [G loss: 4.745592]\n",
      "epoch:36 step:28568 [D loss: 0.079763, acc.: 98.44%] [G loss: 3.950601]\n",
      "epoch:36 step:28569 [D loss: 0.053986, acc.: 100.00%] [G loss: 4.735645]\n",
      "epoch:36 step:28570 [D loss: 0.012785, acc.: 100.00%] [G loss: 4.752512]\n",
      "epoch:36 step:28571 [D loss: 0.018979, acc.: 100.00%] [G loss: 4.829372]\n",
      "epoch:36 step:28572 [D loss: 0.059925, acc.: 99.22%] [G loss: 4.813306]\n",
      "epoch:36 step:28573 [D loss: 0.008680, acc.: 100.00%] [G loss: 4.881041]\n",
      "epoch:36 step:28574 [D loss: 0.029789, acc.: 100.00%] [G loss: 4.549539]\n",
      "epoch:36 step:28575 [D loss: 0.041046, acc.: 100.00%] [G loss: 3.173925]\n",
      "epoch:36 step:28576 [D loss: 0.037820, acc.: 100.00%] [G loss: 3.774834]\n",
      "epoch:36 step:28577 [D loss: 0.034782, acc.: 99.22%] [G loss: 4.156191]\n",
      "epoch:36 step:28578 [D loss: 0.014744, acc.: 100.00%] [G loss: 5.056131]\n",
      "epoch:36 step:28579 [D loss: 0.023225, acc.: 100.00%] [G loss: 4.544991]\n",
      "epoch:36 step:28580 [D loss: 0.026238, acc.: 100.00%] [G loss: 4.887030]\n",
      "epoch:36 step:28581 [D loss: 0.020055, acc.: 100.00%] [G loss: 5.537212]\n",
      "epoch:36 step:28582 [D loss: 0.107196, acc.: 98.44%] [G loss: 4.441868]\n",
      "epoch:36 step:28583 [D loss: 0.112705, acc.: 96.88%] [G loss: 5.697207]\n",
      "epoch:36 step:28584 [D loss: 0.164108, acc.: 91.41%] [G loss: 5.008116]\n",
      "epoch:36 step:28585 [D loss: 0.025608, acc.: 99.22%] [G loss: 5.830772]\n",
      "epoch:36 step:28586 [D loss: 0.046130, acc.: 99.22%] [G loss: 5.608092]\n",
      "epoch:36 step:28587 [D loss: 0.020362, acc.: 99.22%] [G loss: 4.583644]\n",
      "epoch:36 step:28588 [D loss: 0.023110, acc.: 100.00%] [G loss: 5.177636]\n",
      "epoch:36 step:28589 [D loss: 0.018025, acc.: 100.00%] [G loss: 4.333060]\n",
      "epoch:36 step:28590 [D loss: 0.008822, acc.: 100.00%] [G loss: 5.494514]\n",
      "epoch:36 step:28591 [D loss: 1.510383, acc.: 53.91%] [G loss: 10.117338]\n",
      "epoch:36 step:28592 [D loss: 3.095167, acc.: 50.00%] [G loss: 7.418085]\n",
      "epoch:36 step:28593 [D loss: 1.887855, acc.: 54.69%] [G loss: 4.453027]\n",
      "epoch:36 step:28594 [D loss: 0.252451, acc.: 88.28%] [G loss: 3.130779]\n",
      "epoch:36 step:28595 [D loss: 0.066153, acc.: 100.00%] [G loss: 3.926519]\n",
      "epoch:36 step:28596 [D loss: 0.036442, acc.: 100.00%] [G loss: 3.465610]\n",
      "epoch:36 step:28597 [D loss: 0.020606, acc.: 100.00%] [G loss: 4.026170]\n",
      "epoch:36 step:28598 [D loss: 0.054940, acc.: 99.22%] [G loss: 3.541329]\n",
      "epoch:36 step:28599 [D loss: 0.028041, acc.: 100.00%] [G loss: 4.114829]\n",
      "epoch:36 step:28600 [D loss: 0.018806, acc.: 100.00%] [G loss: 3.736528]\n",
      "##############\n",
      "[2.11865405 1.02087653 1.0162411  0.91798164 1.08249666 0.84640701\n",
      " 2.11303566 1.10405762 2.10319343 2.10637402]\n",
      "##########\n",
      "epoch:36 step:28601 [D loss: 0.034111, acc.: 100.00%] [G loss: 3.711051]\n",
      "epoch:36 step:28602 [D loss: 0.044750, acc.: 99.22%] [G loss: 3.261999]\n",
      "epoch:36 step:28603 [D loss: 0.020927, acc.: 100.00%] [G loss: 3.508558]\n",
      "epoch:36 step:28604 [D loss: 0.036054, acc.: 100.00%] [G loss: 4.004739]\n",
      "epoch:36 step:28605 [D loss: 0.029998, acc.: 99.22%] [G loss: 3.593393]\n",
      "epoch:36 step:28606 [D loss: 0.025864, acc.: 100.00%] [G loss: 3.588279]\n",
      "epoch:36 step:28607 [D loss: 0.015619, acc.: 100.00%] [G loss: 2.964534]\n",
      "epoch:36 step:28608 [D loss: 0.160552, acc.: 92.97%] [G loss: 2.700520]\n",
      "epoch:36 step:28609 [D loss: 0.050060, acc.: 100.00%] [G loss: 3.378092]\n",
      "epoch:36 step:28610 [D loss: 0.020106, acc.: 100.00%] [G loss: 3.534828]\n",
      "epoch:36 step:28611 [D loss: 0.033680, acc.: 100.00%] [G loss: 2.778789]\n",
      "epoch:36 step:28612 [D loss: 0.036681, acc.: 100.00%] [G loss: 3.718865]\n",
      "epoch:36 step:28613 [D loss: 0.033508, acc.: 99.22%] [G loss: 3.015047]\n",
      "epoch:36 step:28614 [D loss: 0.069760, acc.: 99.22%] [G loss: 2.789454]\n",
      "epoch:36 step:28615 [D loss: 0.034796, acc.: 100.00%] [G loss: 3.332849]\n",
      "epoch:36 step:28616 [D loss: 0.019945, acc.: 100.00%] [G loss: 2.807258]\n",
      "epoch:36 step:28617 [D loss: 0.121100, acc.: 95.31%] [G loss: 2.625113]\n",
      "epoch:36 step:28618 [D loss: 0.031146, acc.: 99.22%] [G loss: 3.288682]\n",
      "epoch:36 step:28619 [D loss: 0.008791, acc.: 100.00%] [G loss: 3.310118]\n",
      "epoch:36 step:28620 [D loss: 0.021772, acc.: 100.00%] [G loss: 3.177253]\n",
      "epoch:36 step:28621 [D loss: 0.028964, acc.: 100.00%] [G loss: 3.518593]\n",
      "epoch:36 step:28622 [D loss: 0.017691, acc.: 100.00%] [G loss: 3.950900]\n",
      "epoch:36 step:28623 [D loss: 0.022990, acc.: 99.22%] [G loss: 4.210122]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:36 step:28624 [D loss: 0.027753, acc.: 99.22%] [G loss: 3.651818]\n",
      "epoch:36 step:28625 [D loss: 0.322569, acc.: 85.94%] [G loss: 3.966741]\n",
      "epoch:36 step:28626 [D loss: 0.008306, acc.: 100.00%] [G loss: 5.250442]\n",
      "epoch:36 step:28627 [D loss: 0.361660, acc.: 86.72%] [G loss: 1.184702]\n",
      "epoch:36 step:28628 [D loss: 0.175979, acc.: 93.75%] [G loss: 3.451217]\n",
      "epoch:36 step:28629 [D loss: 0.003796, acc.: 100.00%] [G loss: 4.220371]\n",
      "epoch:36 step:28630 [D loss: 0.006701, acc.: 100.00%] [G loss: 3.869274]\n",
      "epoch:36 step:28631 [D loss: 0.084078, acc.: 96.09%] [G loss: 2.358619]\n",
      "epoch:36 step:28632 [D loss: 0.029995, acc.: 100.00%] [G loss: 1.782261]\n",
      "epoch:36 step:28633 [D loss: 0.026341, acc.: 100.00%] [G loss: 2.047771]\n",
      "epoch:36 step:28634 [D loss: 0.012243, acc.: 100.00%] [G loss: 2.435286]\n",
      "epoch:36 step:28635 [D loss: 0.049181, acc.: 100.00%] [G loss: 3.615779]\n",
      "epoch:36 step:28636 [D loss: 0.007099, acc.: 100.00%] [G loss: 2.925484]\n",
      "epoch:36 step:28637 [D loss: 0.026821, acc.: 100.00%] [G loss: 2.320202]\n",
      "epoch:36 step:28638 [D loss: 0.028754, acc.: 100.00%] [G loss: 2.734787]\n",
      "epoch:36 step:28639 [D loss: 0.088035, acc.: 99.22%] [G loss: 2.701263]\n",
      "epoch:36 step:28640 [D loss: 0.047502, acc.: 99.22%] [G loss: 3.745626]\n",
      "epoch:36 step:28641 [D loss: 0.011265, acc.: 100.00%] [G loss: 3.825363]\n",
      "epoch:36 step:28642 [D loss: 0.106275, acc.: 98.44%] [G loss: 2.175049]\n",
      "epoch:36 step:28643 [D loss: 0.028929, acc.: 100.00%] [G loss: 2.239325]\n",
      "epoch:36 step:28644 [D loss: 0.026166, acc.: 100.00%] [G loss: 4.031573]\n",
      "epoch:36 step:28645 [D loss: 0.037842, acc.: 100.00%] [G loss: 5.693044]\n",
      "epoch:36 step:28646 [D loss: 0.007123, acc.: 100.00%] [G loss: 5.455701]\n",
      "epoch:36 step:28647 [D loss: 0.004662, acc.: 100.00%] [G loss: 5.418155]\n",
      "epoch:36 step:28648 [D loss: 0.227543, acc.: 90.62%] [G loss: 3.353573]\n",
      "epoch:36 step:28649 [D loss: 0.038039, acc.: 100.00%] [G loss: 5.253960]\n",
      "epoch:36 step:28650 [D loss: 0.005504, acc.: 100.00%] [G loss: 5.054209]\n",
      "epoch:36 step:28651 [D loss: 0.004845, acc.: 100.00%] [G loss: 5.074314]\n",
      "epoch:36 step:28652 [D loss: 0.031935, acc.: 99.22%] [G loss: 4.596181]\n",
      "epoch:36 step:28653 [D loss: 0.025768, acc.: 100.00%] [G loss: 4.507593]\n",
      "epoch:36 step:28654 [D loss: 0.030758, acc.: 100.00%] [G loss: 4.879136]\n",
      "epoch:36 step:28655 [D loss: 0.004155, acc.: 100.00%] [G loss: 5.374152]\n",
      "epoch:36 step:28656 [D loss: 0.006877, acc.: 100.00%] [G loss: 4.617961]\n",
      "epoch:36 step:28657 [D loss: 0.004248, acc.: 100.00%] [G loss: 5.178871]\n",
      "epoch:36 step:28658 [D loss: 0.008664, acc.: 100.00%] [G loss: 4.383050]\n",
      "epoch:36 step:28659 [D loss: 0.022291, acc.: 100.00%] [G loss: 4.797430]\n",
      "epoch:36 step:28660 [D loss: 0.003391, acc.: 100.00%] [G loss: 4.532887]\n",
      "epoch:36 step:28661 [D loss: 0.006743, acc.: 100.00%] [G loss: 4.638245]\n",
      "epoch:36 step:28662 [D loss: 0.014094, acc.: 100.00%] [G loss: 4.584302]\n",
      "epoch:36 step:28663 [D loss: 0.729082, acc.: 63.28%] [G loss: 7.234936]\n",
      "epoch:36 step:28664 [D loss: 0.092746, acc.: 96.88%] [G loss: 7.806714]\n",
      "epoch:36 step:28665 [D loss: 0.046788, acc.: 98.44%] [G loss: 7.429678]\n",
      "epoch:36 step:28666 [D loss: 0.968220, acc.: 62.50%] [G loss: 1.941797]\n",
      "epoch:36 step:28667 [D loss: 0.160648, acc.: 92.97%] [G loss: 1.136760]\n",
      "epoch:36 step:28668 [D loss: 0.844307, acc.: 72.66%] [G loss: 7.532034]\n",
      "epoch:36 step:28669 [D loss: 1.247501, acc.: 54.69%] [G loss: 6.542215]\n",
      "epoch:36 step:28670 [D loss: 0.024782, acc.: 99.22%] [G loss: 6.006882]\n",
      "epoch:36 step:28671 [D loss: 0.006985, acc.: 100.00%] [G loss: 4.980863]\n",
      "epoch:36 step:28672 [D loss: 0.007791, acc.: 100.00%] [G loss: 5.167119]\n",
      "epoch:36 step:28673 [D loss: 0.010769, acc.: 100.00%] [G loss: 4.475018]\n",
      "epoch:36 step:28674 [D loss: 0.016838, acc.: 100.00%] [G loss: 3.815578]\n",
      "epoch:36 step:28675 [D loss: 0.060170, acc.: 98.44%] [G loss: 3.334329]\n",
      "epoch:36 step:28676 [D loss: 0.052487, acc.: 98.44%] [G loss: 3.696286]\n",
      "epoch:36 step:28677 [D loss: 0.014205, acc.: 100.00%] [G loss: 4.228696]\n",
      "epoch:36 step:28678 [D loss: 0.116445, acc.: 97.66%] [G loss: 3.202751]\n",
      "epoch:36 step:28679 [D loss: 0.011586, acc.: 100.00%] [G loss: 2.656661]\n",
      "epoch:36 step:28680 [D loss: 0.031094, acc.: 100.00%] [G loss: 3.274231]\n",
      "epoch:36 step:28681 [D loss: 0.030701, acc.: 99.22%] [G loss: 1.837995]\n",
      "epoch:36 step:28682 [D loss: 0.406411, acc.: 82.03%] [G loss: 6.808704]\n",
      "epoch:36 step:28683 [D loss: 0.009733, acc.: 100.00%] [G loss: 7.646018]\n",
      "epoch:36 step:28684 [D loss: 0.704686, acc.: 73.44%] [G loss: 4.550591]\n",
      "epoch:36 step:28685 [D loss: 0.028928, acc.: 100.00%] [G loss: 3.848049]\n",
      "epoch:36 step:28686 [D loss: 0.058905, acc.: 98.44%] [G loss: 4.081508]\n",
      "epoch:36 step:28687 [D loss: 0.007581, acc.: 100.00%] [G loss: 4.780773]\n",
      "epoch:36 step:28688 [D loss: 0.010355, acc.: 100.00%] [G loss: 4.504732]\n",
      "epoch:36 step:28689 [D loss: 0.008253, acc.: 100.00%] [G loss: 4.381793]\n",
      "epoch:36 step:28690 [D loss: 0.015570, acc.: 100.00%] [G loss: 4.212248]\n",
      "epoch:36 step:28691 [D loss: 0.012656, acc.: 100.00%] [G loss: 3.717487]\n",
      "epoch:36 step:28692 [D loss: 0.036996, acc.: 99.22%] [G loss: 4.478139]\n",
      "epoch:36 step:28693 [D loss: 0.016552, acc.: 100.00%] [G loss: 3.923586]\n",
      "epoch:36 step:28694 [D loss: 0.010234, acc.: 100.00%] [G loss: 3.769312]\n",
      "epoch:36 step:28695 [D loss: 0.020228, acc.: 100.00%] [G loss: 4.281434]\n",
      "epoch:36 step:28696 [D loss: 0.016780, acc.: 100.00%] [G loss: 3.817079]\n",
      "epoch:36 step:28697 [D loss: 0.021898, acc.: 100.00%] [G loss: 3.942579]\n",
      "epoch:36 step:28698 [D loss: 0.015016, acc.: 100.00%] [G loss: 4.097764]\n",
      "epoch:36 step:28699 [D loss: 0.014380, acc.: 100.00%] [G loss: 3.977174]\n",
      "epoch:36 step:28700 [D loss: 0.014139, acc.: 100.00%] [G loss: 3.903169]\n",
      "epoch:36 step:28701 [D loss: 0.027507, acc.: 99.22%] [G loss: 3.656285]\n",
      "epoch:36 step:28702 [D loss: 0.043415, acc.: 98.44%] [G loss: 3.803342]\n",
      "epoch:36 step:28703 [D loss: 0.024803, acc.: 100.00%] [G loss: 4.559947]\n",
      "epoch:36 step:28704 [D loss: 0.019776, acc.: 100.00%] [G loss: 4.746712]\n",
      "epoch:36 step:28705 [D loss: 0.061229, acc.: 99.22%] [G loss: 3.612916]\n",
      "epoch:36 step:28706 [D loss: 0.004210, acc.: 100.00%] [G loss: 4.960917]\n",
      "epoch:36 step:28707 [D loss: 0.010169, acc.: 100.00%] [G loss: 3.805482]\n",
      "epoch:36 step:28708 [D loss: 0.038586, acc.: 100.00%] [G loss: 4.317350]\n",
      "epoch:36 step:28709 [D loss: 0.011501, acc.: 100.00%] [G loss: 4.560891]\n",
      "epoch:36 step:28710 [D loss: 0.005879, acc.: 100.00%] [G loss: 4.765912]\n",
      "epoch:36 step:28711 [D loss: 0.013299, acc.: 100.00%] [G loss: 5.113286]\n",
      "epoch:36 step:28712 [D loss: 0.035595, acc.: 99.22%] [G loss: 3.926169]\n",
      "epoch:36 step:28713 [D loss: 0.056856, acc.: 100.00%] [G loss: 4.981072]\n",
      "epoch:36 step:28714 [D loss: 0.004136, acc.: 100.00%] [G loss: 6.002506]\n",
      "epoch:36 step:28715 [D loss: 0.272114, acc.: 89.84%] [G loss: 2.838859]\n",
      "epoch:36 step:28716 [D loss: 0.149143, acc.: 96.09%] [G loss: 5.833706]\n",
      "epoch:36 step:28717 [D loss: 0.004637, acc.: 100.00%] [G loss: 6.384556]\n",
      "epoch:36 step:28718 [D loss: 0.506528, acc.: 78.12%] [G loss: 2.842706]\n",
      "epoch:36 step:28719 [D loss: 0.348800, acc.: 80.47%] [G loss: 7.252537]\n",
      "epoch:36 step:28720 [D loss: 0.141154, acc.: 93.75%] [G loss: 7.106730]\n",
      "epoch:36 step:28721 [D loss: 0.003403, acc.: 100.00%] [G loss: 7.372603]\n",
      "epoch:36 step:28722 [D loss: 0.061696, acc.: 97.66%] [G loss: 6.919016]\n",
      "epoch:36 step:28723 [D loss: 0.002896, acc.: 100.00%] [G loss: 6.139006]\n",
      "epoch:36 step:28724 [D loss: 0.002290, acc.: 100.00%] [G loss: 5.969586]\n",
      "epoch:36 step:28725 [D loss: 0.006828, acc.: 100.00%] [G loss: 5.883014]\n",
      "epoch:36 step:28726 [D loss: 0.003007, acc.: 100.00%] [G loss: 5.499988]\n",
      "epoch:36 step:28727 [D loss: 0.005540, acc.: 100.00%] [G loss: 5.678782]\n",
      "epoch:36 step:28728 [D loss: 0.006376, acc.: 100.00%] [G loss: 5.789397]\n",
      "epoch:36 step:28729 [D loss: 0.004828, acc.: 100.00%] [G loss: 5.361214]\n",
      "epoch:36 step:28730 [D loss: 0.007616, acc.: 100.00%] [G loss: 4.725665]\n",
      "epoch:36 step:28731 [D loss: 0.007626, acc.: 100.00%] [G loss: 4.364085]\n",
      "epoch:36 step:28732 [D loss: 0.023637, acc.: 100.00%] [G loss: 4.331697]\n",
      "epoch:36 step:28733 [D loss: 0.005770, acc.: 100.00%] [G loss: 4.358097]\n",
      "epoch:36 step:28734 [D loss: 0.008503, acc.: 100.00%] [G loss: 3.810297]\n",
      "epoch:36 step:28735 [D loss: 0.005075, acc.: 100.00%] [G loss: 3.804352]\n",
      "epoch:36 step:28736 [D loss: 0.011115, acc.: 100.00%] [G loss: 3.564390]\n",
      "epoch:36 step:28737 [D loss: 0.007335, acc.: 100.00%] [G loss: 3.539412]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:36 step:28738 [D loss: 0.005295, acc.: 100.00%] [G loss: 3.169650]\n",
      "epoch:36 step:28739 [D loss: 0.042481, acc.: 100.00%] [G loss: 3.346271]\n",
      "epoch:36 step:28740 [D loss: 0.044792, acc.: 99.22%] [G loss: 5.954062]\n",
      "epoch:36 step:28741 [D loss: 0.001392, acc.: 100.00%] [G loss: 6.189005]\n",
      "epoch:36 step:28742 [D loss: 0.005406, acc.: 100.00%] [G loss: 6.240922]\n",
      "epoch:36 step:28743 [D loss: 0.052256, acc.: 97.66%] [G loss: 5.392908]\n",
      "epoch:36 step:28744 [D loss: 0.006192, acc.: 100.00%] [G loss: 4.788347]\n",
      "epoch:36 step:28745 [D loss: 0.071395, acc.: 97.66%] [G loss: 5.646041]\n",
      "epoch:36 step:28746 [D loss: 0.001571, acc.: 100.00%] [G loss: 6.724824]\n",
      "epoch:36 step:28747 [D loss: 0.119717, acc.: 96.09%] [G loss: 3.932780]\n",
      "epoch:36 step:28748 [D loss: 0.043505, acc.: 100.00%] [G loss: 4.371870]\n",
      "epoch:36 step:28749 [D loss: 0.002216, acc.: 100.00%] [G loss: 5.243028]\n",
      "epoch:36 step:28750 [D loss: 0.005374, acc.: 100.00%] [G loss: 4.979501]\n",
      "epoch:36 step:28751 [D loss: 0.002181, acc.: 100.00%] [G loss: 4.583443]\n",
      "epoch:36 step:28752 [D loss: 0.018307, acc.: 100.00%] [G loss: 3.541969]\n",
      "epoch:36 step:28753 [D loss: 0.024011, acc.: 100.00%] [G loss: 3.067141]\n",
      "epoch:36 step:28754 [D loss: 0.021845, acc.: 100.00%] [G loss: 3.631887]\n",
      "epoch:36 step:28755 [D loss: 0.003756, acc.: 100.00%] [G loss: 5.235828]\n",
      "epoch:36 step:28756 [D loss: 0.032001, acc.: 99.22%] [G loss: 2.667398]\n",
      "epoch:36 step:28757 [D loss: 0.014338, acc.: 100.00%] [G loss: 3.118773]\n",
      "epoch:36 step:28758 [D loss: 1.215673, acc.: 54.69%] [G loss: 9.219228]\n",
      "epoch:36 step:28759 [D loss: 3.769818, acc.: 50.00%] [G loss: 6.552599]\n",
      "epoch:36 step:28760 [D loss: 2.586779, acc.: 50.00%] [G loss: 4.417088]\n",
      "epoch:36 step:28761 [D loss: 1.577418, acc.: 51.56%] [G loss: 2.432894]\n",
      "epoch:36 step:28762 [D loss: 0.500048, acc.: 75.00%] [G loss: 2.177619]\n",
      "epoch:36 step:28763 [D loss: 0.262724, acc.: 91.41%] [G loss: 2.430631]\n",
      "epoch:36 step:28764 [D loss: 0.175138, acc.: 96.09%] [G loss: 3.051171]\n",
      "epoch:36 step:28765 [D loss: 0.290435, acc.: 92.19%] [G loss: 2.153568]\n",
      "epoch:36 step:28766 [D loss: 0.187879, acc.: 94.53%] [G loss: 2.275001]\n",
      "epoch:36 step:28767 [D loss: 0.149452, acc.: 98.44%] [G loss: 2.705743]\n",
      "epoch:36 step:28768 [D loss: 0.098331, acc.: 100.00%] [G loss: 3.172475]\n",
      "epoch:36 step:28769 [D loss: 0.087969, acc.: 100.00%] [G loss: 2.772787]\n",
      "epoch:36 step:28770 [D loss: 0.417828, acc.: 79.69%] [G loss: 2.902188]\n",
      "epoch:36 step:28771 [D loss: 0.051496, acc.: 99.22%] [G loss: 3.510941]\n",
      "epoch:36 step:28772 [D loss: 0.307406, acc.: 86.72%] [G loss: 2.482462]\n",
      "epoch:36 step:28773 [D loss: 0.049488, acc.: 99.22%] [G loss: 1.797127]\n",
      "epoch:36 step:28774 [D loss: 0.129684, acc.: 97.66%] [G loss: 2.146180]\n",
      "epoch:36 step:28775 [D loss: 0.130275, acc.: 97.66%] [G loss: 1.955600]\n",
      "epoch:36 step:28776 [D loss: 0.045430, acc.: 100.00%] [G loss: 1.874266]\n",
      "epoch:36 step:28777 [D loss: 0.106243, acc.: 99.22%] [G loss: 0.927124]\n",
      "epoch:36 step:28778 [D loss: 0.135388, acc.: 96.09%] [G loss: 0.406716]\n",
      "epoch:36 step:28779 [D loss: 0.348000, acc.: 81.25%] [G loss: 4.109583]\n",
      "epoch:36 step:28780 [D loss: 0.411792, acc.: 76.56%] [G loss: 2.763129]\n",
      "epoch:36 step:28781 [D loss: 1.221483, acc.: 45.31%] [G loss: 4.418717]\n",
      "epoch:36 step:28782 [D loss: 0.254602, acc.: 87.50%] [G loss: 4.464412]\n",
      "epoch:36 step:28783 [D loss: 0.431206, acc.: 79.69%] [G loss: 3.320037]\n",
      "epoch:36 step:28784 [D loss: 0.077500, acc.: 98.44%] [G loss: 3.522069]\n",
      "epoch:36 step:28785 [D loss: 0.080524, acc.: 98.44%] [G loss: 3.079537]\n",
      "epoch:36 step:28786 [D loss: 0.083295, acc.: 100.00%] [G loss: 3.285791]\n",
      "epoch:36 step:28787 [D loss: 0.048186, acc.: 100.00%] [G loss: 3.174352]\n",
      "epoch:36 step:28788 [D loss: 0.027877, acc.: 100.00%] [G loss: 2.889192]\n",
      "epoch:36 step:28789 [D loss: 0.035970, acc.: 100.00%] [G loss: 3.067698]\n",
      "epoch:36 step:28790 [D loss: 0.053026, acc.: 100.00%] [G loss: 2.764994]\n",
      "epoch:36 step:28791 [D loss: 0.055571, acc.: 100.00%] [G loss: 3.161912]\n",
      "epoch:36 step:28792 [D loss: 0.102594, acc.: 98.44%] [G loss: 3.200686]\n",
      "epoch:36 step:28793 [D loss: 0.062346, acc.: 100.00%] [G loss: 3.585912]\n",
      "epoch:36 step:28794 [D loss: 0.096911, acc.: 97.66%] [G loss: 3.166135]\n",
      "epoch:36 step:28795 [D loss: 0.058330, acc.: 99.22%] [G loss: 3.666638]\n",
      "epoch:36 step:28796 [D loss: 0.057374, acc.: 99.22%] [G loss: 3.481115]\n",
      "epoch:36 step:28797 [D loss: 0.065378, acc.: 98.44%] [G loss: 3.439221]\n",
      "epoch:36 step:28798 [D loss: 0.310028, acc.: 88.28%] [G loss: 4.430308]\n",
      "epoch:36 step:28799 [D loss: 0.104616, acc.: 95.31%] [G loss: 3.922800]\n",
      "epoch:36 step:28800 [D loss: 0.021404, acc.: 100.00%] [G loss: 4.250462]\n",
      "##############\n",
      "[0.96192033 0.94276564 0.86701485 0.96404887 1.04460508 2.12190707\n",
      " 2.11471119 1.03206311 2.10782884 2.1129139 ]\n",
      "##########\n",
      "epoch:36 step:28801 [D loss: 0.040528, acc.: 100.00%] [G loss: 3.914617]\n",
      "epoch:36 step:28802 [D loss: 0.052326, acc.: 99.22%] [G loss: 3.030452]\n",
      "epoch:36 step:28803 [D loss: 0.066859, acc.: 99.22%] [G loss: 3.895244]\n",
      "epoch:36 step:28804 [D loss: 0.035100, acc.: 100.00%] [G loss: 3.834706]\n",
      "epoch:36 step:28805 [D loss: 0.090928, acc.: 99.22%] [G loss: 2.733084]\n",
      "epoch:36 step:28806 [D loss: 0.063394, acc.: 99.22%] [G loss: 3.216114]\n",
      "epoch:36 step:28807 [D loss: 0.035918, acc.: 100.00%] [G loss: 3.092646]\n",
      "epoch:36 step:28808 [D loss: 0.166727, acc.: 97.66%] [G loss: 3.863777]\n",
      "epoch:36 step:28809 [D loss: 0.035029, acc.: 100.00%] [G loss: 4.622923]\n",
      "epoch:36 step:28810 [D loss: 0.326544, acc.: 86.72%] [G loss: 3.252500]\n",
      "epoch:36 step:28811 [D loss: 0.011641, acc.: 100.00%] [G loss: 4.865953]\n",
      "epoch:36 step:28812 [D loss: 0.239666, acc.: 89.84%] [G loss: 2.505795]\n",
      "epoch:36 step:28813 [D loss: 0.107947, acc.: 97.66%] [G loss: 3.661297]\n",
      "epoch:36 step:28814 [D loss: 0.060706, acc.: 98.44%] [G loss: 3.673585]\n",
      "epoch:36 step:28815 [D loss: 0.029251, acc.: 100.00%] [G loss: 3.651033]\n",
      "epoch:36 step:28816 [D loss: 0.059143, acc.: 100.00%] [G loss: 4.140908]\n",
      "epoch:36 step:28817 [D loss: 0.061671, acc.: 99.22%] [G loss: 3.442512]\n",
      "epoch:36 step:28818 [D loss: 0.028377, acc.: 100.00%] [G loss: 3.476069]\n",
      "epoch:36 step:28819 [D loss: 0.140515, acc.: 96.88%] [G loss: 2.759508]\n",
      "epoch:36 step:28820 [D loss: 0.039171, acc.: 100.00%] [G loss: 4.341967]\n",
      "epoch:36 step:28821 [D loss: 0.067037, acc.: 99.22%] [G loss: 3.420732]\n",
      "epoch:36 step:28822 [D loss: 0.096800, acc.: 96.88%] [G loss: 4.560762]\n",
      "epoch:36 step:28823 [D loss: 0.157437, acc.: 95.31%] [G loss: 3.100347]\n",
      "epoch:36 step:28824 [D loss: 0.029194, acc.: 100.00%] [G loss: 3.767117]\n",
      "epoch:36 step:28825 [D loss: 0.019503, acc.: 100.00%] [G loss: 4.026704]\n",
      "epoch:36 step:28826 [D loss: 0.067747, acc.: 99.22%] [G loss: 3.648450]\n",
      "epoch:36 step:28827 [D loss: 0.040187, acc.: 100.00%] [G loss: 3.932573]\n",
      "epoch:36 step:28828 [D loss: 0.052033, acc.: 99.22%] [G loss: 4.373458]\n",
      "epoch:36 step:28829 [D loss: 0.262221, acc.: 88.28%] [G loss: 3.393676]\n",
      "epoch:36 step:28830 [D loss: 0.053634, acc.: 99.22%] [G loss: 4.184091]\n",
      "epoch:36 step:28831 [D loss: 0.271933, acc.: 91.41%] [G loss: 5.365607]\n",
      "epoch:36 step:28832 [D loss: 0.067590, acc.: 97.66%] [G loss: 4.951607]\n",
      "epoch:36 step:28833 [D loss: 0.012616, acc.: 100.00%] [G loss: 5.388089]\n",
      "epoch:36 step:28834 [D loss: 0.032835, acc.: 99.22%] [G loss: 3.998817]\n",
      "epoch:36 step:28835 [D loss: 0.015169, acc.: 100.00%] [G loss: 4.947750]\n",
      "epoch:36 step:28836 [D loss: 0.041219, acc.: 100.00%] [G loss: 4.345632]\n",
      "epoch:36 step:28837 [D loss: 0.022848, acc.: 100.00%] [G loss: 4.534932]\n",
      "epoch:36 step:28838 [D loss: 0.036299, acc.: 100.00%] [G loss: 3.468819]\n",
      "epoch:36 step:28839 [D loss: 0.057227, acc.: 99.22%] [G loss: 3.426528]\n",
      "epoch:36 step:28840 [D loss: 0.036806, acc.: 98.44%] [G loss: 3.995682]\n",
      "epoch:36 step:28841 [D loss: 0.109088, acc.: 97.66%] [G loss: 3.673075]\n",
      "epoch:36 step:28842 [D loss: 0.009895, acc.: 100.00%] [G loss: 4.993261]\n",
      "epoch:36 step:28843 [D loss: 0.194774, acc.: 94.53%] [G loss: 3.296849]\n",
      "epoch:36 step:28844 [D loss: 0.019481, acc.: 99.22%] [G loss: 4.076431]\n",
      "epoch:36 step:28845 [D loss: 0.027030, acc.: 99.22%] [G loss: 4.040486]\n",
      "epoch:36 step:28846 [D loss: 0.011568, acc.: 100.00%] [G loss: 4.360642]\n",
      "epoch:36 step:28847 [D loss: 0.022331, acc.: 100.00%] [G loss: 4.361168]\n",
      "epoch:36 step:28848 [D loss: 0.046418, acc.: 100.00%] [G loss: 3.710458]\n",
      "epoch:36 step:28849 [D loss: 0.008451, acc.: 100.00%] [G loss: 4.180305]\n",
      "epoch:36 step:28850 [D loss: 0.087813, acc.: 97.66%] [G loss: 2.030636]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:36 step:28851 [D loss: 0.140344, acc.: 96.09%] [G loss: 5.810142]\n",
      "epoch:36 step:28852 [D loss: 0.004109, acc.: 100.00%] [G loss: 7.209893]\n",
      "epoch:36 step:28853 [D loss: 1.808994, acc.: 28.91%] [G loss: 7.594317]\n",
      "epoch:36 step:28854 [D loss: 0.175302, acc.: 95.31%] [G loss: 7.776458]\n",
      "epoch:36 step:28855 [D loss: 0.341067, acc.: 83.59%] [G loss: 6.484253]\n",
      "epoch:36 step:28856 [D loss: 0.031565, acc.: 99.22%] [G loss: 5.066195]\n",
      "epoch:36 step:28857 [D loss: 0.011281, acc.: 100.00%] [G loss: 4.572622]\n",
      "epoch:36 step:28858 [D loss: 0.032140, acc.: 99.22%] [G loss: 4.378822]\n",
      "epoch:36 step:28859 [D loss: 0.022733, acc.: 100.00%] [G loss: 4.475601]\n",
      "epoch:36 step:28860 [D loss: 0.036162, acc.: 100.00%] [G loss: 4.357835]\n",
      "epoch:36 step:28861 [D loss: 0.013087, acc.: 100.00%] [G loss: 4.256770]\n",
      "epoch:36 step:28862 [D loss: 0.045231, acc.: 100.00%] [G loss: 3.794794]\n",
      "epoch:36 step:28863 [D loss: 0.007963, acc.: 100.00%] [G loss: 4.272982]\n",
      "epoch:36 step:28864 [D loss: 0.015297, acc.: 100.00%] [G loss: 4.796161]\n",
      "epoch:36 step:28865 [D loss: 0.031929, acc.: 100.00%] [G loss: 3.954153]\n",
      "epoch:36 step:28866 [D loss: 0.050919, acc.: 99.22%] [G loss: 3.130338]\n",
      "epoch:36 step:28867 [D loss: 0.013019, acc.: 100.00%] [G loss: 3.644702]\n",
      "epoch:36 step:28868 [D loss: 0.080281, acc.: 98.44%] [G loss: 4.961986]\n",
      "epoch:36 step:28869 [D loss: 0.018484, acc.: 100.00%] [G loss: 5.459853]\n",
      "epoch:36 step:28870 [D loss: 0.235871, acc.: 89.06%] [G loss: 4.019297]\n",
      "epoch:36 step:28871 [D loss: 0.026590, acc.: 100.00%] [G loss: 3.748112]\n",
      "epoch:36 step:28872 [D loss: 0.035499, acc.: 100.00%] [G loss: 4.423809]\n",
      "epoch:36 step:28873 [D loss: 0.019100, acc.: 100.00%] [G loss: 3.400107]\n",
      "epoch:36 step:28874 [D loss: 0.010711, acc.: 100.00%] [G loss: 4.934324]\n",
      "epoch:36 step:28875 [D loss: 0.014855, acc.: 100.00%] [G loss: 4.057318]\n",
      "epoch:36 step:28876 [D loss: 0.173851, acc.: 92.19%] [G loss: 5.166089]\n",
      "epoch:36 step:28877 [D loss: 0.013496, acc.: 100.00%] [G loss: 5.421127]\n",
      "epoch:36 step:28878 [D loss: 0.262766, acc.: 85.94%] [G loss: 1.310520]\n",
      "epoch:36 step:28879 [D loss: 0.050446, acc.: 99.22%] [G loss: 2.460062]\n",
      "epoch:36 step:28880 [D loss: 0.045217, acc.: 100.00%] [G loss: 5.194894]\n",
      "epoch:36 step:28881 [D loss: 0.005420, acc.: 100.00%] [G loss: 4.951653]\n",
      "epoch:36 step:28882 [D loss: 0.022166, acc.: 100.00%] [G loss: 4.881681]\n",
      "epoch:36 step:28883 [D loss: 0.004823, acc.: 100.00%] [G loss: 4.601521]\n",
      "epoch:36 step:28884 [D loss: 0.012304, acc.: 100.00%] [G loss: 4.370415]\n",
      "epoch:36 step:28885 [D loss: 0.020250, acc.: 100.00%] [G loss: 5.369717]\n",
      "epoch:36 step:28886 [D loss: 0.013775, acc.: 100.00%] [G loss: 4.447697]\n",
      "epoch:36 step:28887 [D loss: 0.019921, acc.: 100.00%] [G loss: 3.706508]\n",
      "epoch:36 step:28888 [D loss: 0.005613, acc.: 100.00%] [G loss: 4.016836]\n",
      "epoch:36 step:28889 [D loss: 0.018976, acc.: 100.00%] [G loss: 3.344528]\n",
      "epoch:36 step:28890 [D loss: 0.024969, acc.: 100.00%] [G loss: 4.207495]\n",
      "epoch:36 step:28891 [D loss: 0.010289, acc.: 100.00%] [G loss: 4.734897]\n",
      "epoch:36 step:28892 [D loss: 0.025125, acc.: 100.00%] [G loss: 4.987907]\n",
      "epoch:36 step:28893 [D loss: 0.046392, acc.: 98.44%] [G loss: 4.242726]\n",
      "epoch:36 step:28894 [D loss: 0.088487, acc.: 98.44%] [G loss: 5.442654]\n",
      "epoch:36 step:28895 [D loss: 0.032795, acc.: 100.00%] [G loss: 5.026128]\n",
      "epoch:36 step:28896 [D loss: 0.007061, acc.: 100.00%] [G loss: 4.255790]\n",
      "epoch:36 step:28897 [D loss: 0.028742, acc.: 100.00%] [G loss: 4.993563]\n",
      "epoch:37 step:28898 [D loss: 0.003992, acc.: 100.00%] [G loss: 5.464428]\n",
      "epoch:37 step:28899 [D loss: 0.010457, acc.: 100.00%] [G loss: 5.159328]\n",
      "epoch:37 step:28900 [D loss: 0.022480, acc.: 100.00%] [G loss: 4.502999]\n",
      "epoch:37 step:28901 [D loss: 0.024613, acc.: 100.00%] [G loss: 4.872899]\n",
      "epoch:37 step:28902 [D loss: 0.014626, acc.: 100.00%] [G loss: 4.729959]\n",
      "epoch:37 step:28903 [D loss: 0.015806, acc.: 100.00%] [G loss: 4.748071]\n",
      "epoch:37 step:28904 [D loss: 0.093552, acc.: 96.88%] [G loss: 4.783063]\n",
      "epoch:37 step:28905 [D loss: 0.004773, acc.: 100.00%] [G loss: 4.652575]\n",
      "epoch:37 step:28906 [D loss: 0.022563, acc.: 99.22%] [G loss: 3.666286]\n",
      "epoch:37 step:28907 [D loss: 0.019008, acc.: 100.00%] [G loss: 4.561920]\n",
      "epoch:37 step:28908 [D loss: 0.022305, acc.: 99.22%] [G loss: 5.264925]\n",
      "epoch:37 step:28909 [D loss: 0.009975, acc.: 100.00%] [G loss: 5.520480]\n",
      "epoch:37 step:28910 [D loss: 0.002042, acc.: 100.00%] [G loss: 5.676505]\n",
      "epoch:37 step:28911 [D loss: 0.103635, acc.: 96.09%] [G loss: 2.438920]\n",
      "epoch:37 step:28912 [D loss: 0.062298, acc.: 98.44%] [G loss: 5.445936]\n",
      "epoch:37 step:28913 [D loss: 0.001618, acc.: 100.00%] [G loss: 6.667775]\n",
      "epoch:37 step:28914 [D loss: 0.048459, acc.: 99.22%] [G loss: 4.824415]\n",
      "epoch:37 step:28915 [D loss: 0.003862, acc.: 100.00%] [G loss: 3.796669]\n",
      "epoch:37 step:28916 [D loss: 0.021133, acc.: 100.00%] [G loss: 4.730348]\n",
      "epoch:37 step:28917 [D loss: 0.002960, acc.: 100.00%] [G loss: 5.453429]\n",
      "epoch:37 step:28918 [D loss: 0.002805, acc.: 100.00%] [G loss: 5.093964]\n",
      "epoch:37 step:28919 [D loss: 0.012097, acc.: 100.00%] [G loss: 4.961451]\n",
      "epoch:37 step:28920 [D loss: 0.037391, acc.: 99.22%] [G loss: 4.861952]\n",
      "epoch:37 step:28921 [D loss: 0.005663, acc.: 100.00%] [G loss: 5.367677]\n",
      "epoch:37 step:28922 [D loss: 0.046797, acc.: 99.22%] [G loss: 4.810683]\n",
      "epoch:37 step:28923 [D loss: 0.047698, acc.: 100.00%] [G loss: 6.741313]\n",
      "epoch:37 step:28924 [D loss: 0.022307, acc.: 100.00%] [G loss: 6.530116]\n",
      "epoch:37 step:28925 [D loss: 0.192165, acc.: 93.75%] [G loss: 6.013629]\n",
      "epoch:37 step:28926 [D loss: 0.001238, acc.: 100.00%] [G loss: 7.515919]\n",
      "epoch:37 step:28927 [D loss: 0.094121, acc.: 96.88%] [G loss: 4.646873]\n",
      "epoch:37 step:28928 [D loss: 0.028591, acc.: 100.00%] [G loss: 3.626805]\n",
      "epoch:37 step:28929 [D loss: 0.004713, acc.: 100.00%] [G loss: 4.809782]\n",
      "epoch:37 step:28930 [D loss: 0.002674, acc.: 100.00%] [G loss: 5.063972]\n",
      "epoch:37 step:28931 [D loss: 0.006182, acc.: 100.00%] [G loss: 3.944087]\n",
      "epoch:37 step:28932 [D loss: 0.007249, acc.: 100.00%] [G loss: 5.223473]\n",
      "epoch:37 step:28933 [D loss: 0.159835, acc.: 94.53%] [G loss: 8.067850]\n",
      "epoch:37 step:28934 [D loss: 0.025271, acc.: 99.22%] [G loss: 9.040811]\n",
      "epoch:37 step:28935 [D loss: 0.780755, acc.: 66.41%] [G loss: 0.638914]\n",
      "epoch:37 step:28936 [D loss: 2.254288, acc.: 56.25%] [G loss: 9.401526]\n",
      "epoch:37 step:28937 [D loss: 3.628888, acc.: 50.00%] [G loss: 6.920425]\n",
      "epoch:37 step:28938 [D loss: 2.336438, acc.: 49.22%] [G loss: 3.461380]\n",
      "epoch:37 step:28939 [D loss: 0.872212, acc.: 57.03%] [G loss: 3.517650]\n",
      "epoch:37 step:28940 [D loss: 0.163520, acc.: 96.09%] [G loss: 4.072932]\n",
      "epoch:37 step:28941 [D loss: 0.706104, acc.: 64.84%] [G loss: 2.364519]\n",
      "epoch:37 step:28942 [D loss: 0.209393, acc.: 94.53%] [G loss: 3.077658]\n",
      "epoch:37 step:28943 [D loss: 0.037084, acc.: 100.00%] [G loss: 3.456777]\n",
      "epoch:37 step:28944 [D loss: 0.124485, acc.: 95.31%] [G loss: 2.849479]\n",
      "epoch:37 step:28945 [D loss: 0.130653, acc.: 98.44%] [G loss: 3.066734]\n",
      "epoch:37 step:28946 [D loss: 0.106452, acc.: 98.44%] [G loss: 2.630545]\n",
      "epoch:37 step:28947 [D loss: 0.179028, acc.: 92.97%] [G loss: 3.632250]\n",
      "epoch:37 step:28948 [D loss: 0.108209, acc.: 98.44%] [G loss: 3.630869]\n",
      "epoch:37 step:28949 [D loss: 0.126040, acc.: 96.88%] [G loss: 2.677869]\n",
      "epoch:37 step:28950 [D loss: 0.052624, acc.: 99.22%] [G loss: 2.971292]\n",
      "epoch:37 step:28951 [D loss: 0.164933, acc.: 95.31%] [G loss: 2.859684]\n",
      "epoch:37 step:28952 [D loss: 0.083425, acc.: 98.44%] [G loss: 3.285140]\n",
      "epoch:37 step:28953 [D loss: 0.322655, acc.: 90.62%] [G loss: 4.010225]\n",
      "epoch:37 step:28954 [D loss: 0.493775, acc.: 77.34%] [G loss: 3.064728]\n",
      "epoch:37 step:28955 [D loss: 0.091673, acc.: 98.44%] [G loss: 2.459033]\n",
      "epoch:37 step:28956 [D loss: 0.316199, acc.: 80.47%] [G loss: 4.699279]\n",
      "epoch:37 step:28957 [D loss: 0.172685, acc.: 91.41%] [G loss: 4.257530]\n",
      "epoch:37 step:28958 [D loss: 0.193264, acc.: 90.62%] [G loss: 2.113155]\n",
      "epoch:37 step:28959 [D loss: 0.290532, acc.: 87.50%] [G loss: 4.495922]\n",
      "epoch:37 step:28960 [D loss: 0.531653, acc.: 73.44%] [G loss: 3.940817]\n",
      "epoch:37 step:28961 [D loss: 0.108826, acc.: 99.22%] [G loss: 4.054565]\n",
      "epoch:37 step:28962 [D loss: 0.167679, acc.: 96.09%] [G loss: 3.257522]\n",
      "epoch:37 step:28963 [D loss: 0.032877, acc.: 100.00%] [G loss: 3.116555]\n",
      "epoch:37 step:28964 [D loss: 0.100885, acc.: 98.44%] [G loss: 3.101974]\n",
      "epoch:37 step:28965 [D loss: 0.041257, acc.: 99.22%] [G loss: 3.750228]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:37 step:28966 [D loss: 0.077574, acc.: 99.22%] [G loss: 3.294665]\n",
      "epoch:37 step:28967 [D loss: 0.074585, acc.: 98.44%] [G loss: 3.484924]\n",
      "epoch:37 step:28968 [D loss: 0.053477, acc.: 100.00%] [G loss: 3.381869]\n",
      "epoch:37 step:28969 [D loss: 0.109890, acc.: 97.66%] [G loss: 3.114339]\n",
      "epoch:37 step:28970 [D loss: 0.153074, acc.: 96.09%] [G loss: 4.006588]\n",
      "epoch:37 step:28971 [D loss: 0.020289, acc.: 100.00%] [G loss: 4.541917]\n",
      "epoch:37 step:28972 [D loss: 0.077069, acc.: 96.88%] [G loss: 3.836304]\n",
      "epoch:37 step:28973 [D loss: 0.126147, acc.: 95.31%] [G loss: 2.581770]\n",
      "epoch:37 step:28974 [D loss: 0.459934, acc.: 75.00%] [G loss: 5.394876]\n",
      "epoch:37 step:28975 [D loss: 0.331623, acc.: 83.59%] [G loss: 3.254359]\n",
      "epoch:37 step:28976 [D loss: 0.352629, acc.: 85.94%] [G loss: 5.059435]\n",
      "epoch:37 step:28977 [D loss: 0.271363, acc.: 87.50%] [G loss: 4.604717]\n",
      "epoch:37 step:28978 [D loss: 0.061381, acc.: 99.22%] [G loss: 4.065670]\n",
      "epoch:37 step:28979 [D loss: 0.014987, acc.: 100.00%] [G loss: 2.522963]\n",
      "epoch:37 step:28980 [D loss: 0.018805, acc.: 100.00%] [G loss: 2.681196]\n",
      "epoch:37 step:28981 [D loss: 0.439374, acc.: 81.25%] [G loss: 6.106426]\n",
      "epoch:37 step:28982 [D loss: 0.625031, acc.: 68.75%] [G loss: 4.961926]\n",
      "epoch:37 step:28983 [D loss: 0.025357, acc.: 100.00%] [G loss: 3.799651]\n",
      "epoch:37 step:28984 [D loss: 0.020178, acc.: 100.00%] [G loss: 3.743104]\n",
      "epoch:37 step:28985 [D loss: 0.137892, acc.: 96.09%] [G loss: 4.408915]\n",
      "epoch:37 step:28986 [D loss: 0.028368, acc.: 99.22%] [G loss: 4.973229]\n",
      "epoch:37 step:28987 [D loss: 0.904631, acc.: 51.56%] [G loss: 5.559340]\n",
      "epoch:37 step:28988 [D loss: 0.015665, acc.: 100.00%] [G loss: 5.543472]\n",
      "epoch:37 step:28989 [D loss: 0.191580, acc.: 90.62%] [G loss: 4.756028]\n",
      "epoch:37 step:28990 [D loss: 0.010865, acc.: 100.00%] [G loss: 4.147396]\n",
      "epoch:37 step:28991 [D loss: 0.010312, acc.: 100.00%] [G loss: 3.963572]\n",
      "epoch:37 step:28992 [D loss: 0.029981, acc.: 100.00%] [G loss: 3.939548]\n",
      "epoch:37 step:28993 [D loss: 0.014221, acc.: 100.00%] [G loss: 3.649253]\n",
      "epoch:37 step:28994 [D loss: 0.014127, acc.: 100.00%] [G loss: 3.877822]\n",
      "epoch:37 step:28995 [D loss: 0.022412, acc.: 100.00%] [G loss: 4.013535]\n",
      "epoch:37 step:28996 [D loss: 0.034033, acc.: 99.22%] [G loss: 3.081350]\n",
      "epoch:37 step:28997 [D loss: 0.054873, acc.: 97.66%] [G loss: 3.571245]\n",
      "epoch:37 step:28998 [D loss: 0.014778, acc.: 100.00%] [G loss: 3.319976]\n",
      "epoch:37 step:28999 [D loss: 0.036500, acc.: 100.00%] [G loss: 3.576636]\n",
      "epoch:37 step:29000 [D loss: 0.090158, acc.: 99.22%] [G loss: 2.641613]\n",
      "##############\n",
      "[1.02972124 0.93770501 1.01163302 0.81368007 1.10986354 1.00039747\n",
      " 0.75978951 2.11765215 1.1082823  1.05392106]\n",
      "##########\n",
      "epoch:37 step:29001 [D loss: 0.261581, acc.: 88.28%] [G loss: 3.809887]\n",
      "epoch:37 step:29002 [D loss: 0.084648, acc.: 97.66%] [G loss: 3.997097]\n",
      "epoch:37 step:29003 [D loss: 0.035212, acc.: 99.22%] [G loss: 3.848513]\n",
      "epoch:37 step:29004 [D loss: 0.065915, acc.: 99.22%] [G loss: 3.352565]\n",
      "epoch:37 step:29005 [D loss: 0.066222, acc.: 99.22%] [G loss: 2.780107]\n",
      "epoch:37 step:29006 [D loss: 0.018567, acc.: 100.00%] [G loss: 4.156234]\n",
      "epoch:37 step:29007 [D loss: 0.490250, acc.: 78.12%] [G loss: 5.582295]\n",
      "epoch:37 step:29008 [D loss: 0.099587, acc.: 96.88%] [G loss: 5.489295]\n",
      "epoch:37 step:29009 [D loss: 0.019902, acc.: 100.00%] [G loss: 5.615250]\n",
      "epoch:37 step:29010 [D loss: 0.010103, acc.: 100.00%] [G loss: 4.952600]\n",
      "epoch:37 step:29011 [D loss: 0.011915, acc.: 100.00%] [G loss: 4.432853]\n",
      "epoch:37 step:29012 [D loss: 0.020914, acc.: 100.00%] [G loss: 4.743281]\n",
      "epoch:37 step:29013 [D loss: 0.227739, acc.: 90.62%] [G loss: 2.572935]\n",
      "epoch:37 step:29014 [D loss: 0.089643, acc.: 100.00%] [G loss: 4.765414]\n",
      "epoch:37 step:29015 [D loss: 0.008767, acc.: 100.00%] [G loss: 5.255851]\n",
      "epoch:37 step:29016 [D loss: 0.101933, acc.: 99.22%] [G loss: 3.785245]\n",
      "epoch:37 step:29017 [D loss: 0.055666, acc.: 100.00%] [G loss: 3.865533]\n",
      "epoch:37 step:29018 [D loss: 0.009516, acc.: 100.00%] [G loss: 4.997299]\n",
      "epoch:37 step:29019 [D loss: 0.016400, acc.: 100.00%] [G loss: 4.287087]\n",
      "epoch:37 step:29020 [D loss: 0.028616, acc.: 99.22%] [G loss: 3.696078]\n",
      "epoch:37 step:29021 [D loss: 0.016066, acc.: 100.00%] [G loss: 3.323768]\n",
      "epoch:37 step:29022 [D loss: 0.008469, acc.: 100.00%] [G loss: 3.856812]\n",
      "epoch:37 step:29023 [D loss: 0.030903, acc.: 100.00%] [G loss: 3.331165]\n",
      "epoch:37 step:29024 [D loss: 0.036506, acc.: 100.00%] [G loss: 3.693875]\n",
      "epoch:37 step:29025 [D loss: 0.087573, acc.: 99.22%] [G loss: 2.527148]\n",
      "epoch:37 step:29026 [D loss: 0.257105, acc.: 85.16%] [G loss: 6.414392]\n",
      "epoch:37 step:29027 [D loss: 2.164681, acc.: 50.00%] [G loss: 4.476242]\n",
      "epoch:37 step:29028 [D loss: 0.042193, acc.: 100.00%] [G loss: 1.884348]\n",
      "epoch:37 step:29029 [D loss: 0.097459, acc.: 97.66%] [G loss: 2.515131]\n",
      "epoch:37 step:29030 [D loss: 0.089646, acc.: 96.88%] [G loss: 3.879241]\n",
      "epoch:37 step:29031 [D loss: 0.346957, acc.: 86.72%] [G loss: 3.361012]\n",
      "epoch:37 step:29032 [D loss: 0.078417, acc.: 99.22%] [G loss: 3.977300]\n",
      "epoch:37 step:29033 [D loss: 0.063511, acc.: 97.66%] [G loss: 3.472147]\n",
      "epoch:37 step:29034 [D loss: 0.023024, acc.: 100.00%] [G loss: 4.101948]\n",
      "epoch:37 step:29035 [D loss: 0.037420, acc.: 100.00%] [G loss: 3.948687]\n",
      "epoch:37 step:29036 [D loss: 0.017016, acc.: 100.00%] [G loss: 4.002434]\n",
      "epoch:37 step:29037 [D loss: 0.050493, acc.: 99.22%] [G loss: 3.135870]\n",
      "epoch:37 step:29038 [D loss: 0.025344, acc.: 100.00%] [G loss: 4.254245]\n",
      "epoch:37 step:29039 [D loss: 0.062625, acc.: 99.22%] [G loss: 4.415772]\n",
      "epoch:37 step:29040 [D loss: 0.111282, acc.: 97.66%] [G loss: 4.723607]\n",
      "epoch:37 step:29041 [D loss: 0.049061, acc.: 99.22%] [G loss: 4.864429]\n",
      "epoch:37 step:29042 [D loss: 0.088030, acc.: 97.66%] [G loss: 3.640590]\n",
      "epoch:37 step:29043 [D loss: 0.066001, acc.: 98.44%] [G loss: 4.739535]\n",
      "epoch:37 step:29044 [D loss: 0.035460, acc.: 99.22%] [G loss: 4.792956]\n",
      "epoch:37 step:29045 [D loss: 0.008916, acc.: 100.00%] [G loss: 4.328349]\n",
      "epoch:37 step:29046 [D loss: 0.029466, acc.: 100.00%] [G loss: 4.697233]\n",
      "epoch:37 step:29047 [D loss: 0.073058, acc.: 99.22%] [G loss: 4.095688]\n",
      "epoch:37 step:29048 [D loss: 0.037272, acc.: 100.00%] [G loss: 4.224727]\n",
      "epoch:37 step:29049 [D loss: 0.012311, acc.: 100.00%] [G loss: 4.830563]\n",
      "epoch:37 step:29050 [D loss: 0.007763, acc.: 100.00%] [G loss: 4.283082]\n",
      "epoch:37 step:29051 [D loss: 0.013413, acc.: 100.00%] [G loss: 5.041101]\n",
      "epoch:37 step:29052 [D loss: 0.013942, acc.: 100.00%] [G loss: 4.407432]\n",
      "epoch:37 step:29053 [D loss: 0.083383, acc.: 99.22%] [G loss: 4.286074]\n",
      "epoch:37 step:29054 [D loss: 0.016696, acc.: 100.00%] [G loss: 3.708031]\n",
      "epoch:37 step:29055 [D loss: 0.085122, acc.: 98.44%] [G loss: 3.988312]\n",
      "epoch:37 step:29056 [D loss: 0.021374, acc.: 100.00%] [G loss: 4.584831]\n",
      "epoch:37 step:29057 [D loss: 0.006620, acc.: 100.00%] [G loss: 4.873023]\n",
      "epoch:37 step:29058 [D loss: 0.673702, acc.: 68.75%] [G loss: 6.711685]\n",
      "epoch:37 step:29059 [D loss: 0.302657, acc.: 82.03%] [G loss: 6.225200]\n",
      "epoch:37 step:29060 [D loss: 0.035015, acc.: 100.00%] [G loss: 4.982809]\n",
      "epoch:37 step:29061 [D loss: 0.047475, acc.: 99.22%] [G loss: 4.650778]\n",
      "epoch:37 step:29062 [D loss: 0.007805, acc.: 100.00%] [G loss: 4.918188]\n",
      "epoch:37 step:29063 [D loss: 0.012408, acc.: 100.00%] [G loss: 3.995241]\n",
      "epoch:37 step:29064 [D loss: 0.019621, acc.: 100.00%] [G loss: 4.128183]\n",
      "epoch:37 step:29065 [D loss: 0.030987, acc.: 100.00%] [G loss: 3.563823]\n",
      "epoch:37 step:29066 [D loss: 0.005086, acc.: 100.00%] [G loss: 3.915660]\n",
      "epoch:37 step:29067 [D loss: 0.014046, acc.: 100.00%] [G loss: 2.725671]\n",
      "epoch:37 step:29068 [D loss: 0.017612, acc.: 100.00%] [G loss: 3.638517]\n",
      "epoch:37 step:29069 [D loss: 0.014560, acc.: 100.00%] [G loss: 3.745983]\n",
      "epoch:37 step:29070 [D loss: 0.045401, acc.: 98.44%] [G loss: 3.171680]\n",
      "epoch:37 step:29071 [D loss: 0.046546, acc.: 100.00%] [G loss: 4.513032]\n",
      "epoch:37 step:29072 [D loss: 0.013202, acc.: 100.00%] [G loss: 4.016817]\n",
      "epoch:37 step:29073 [D loss: 1.798949, acc.: 32.81%] [G loss: 7.140208]\n",
      "epoch:37 step:29074 [D loss: 1.367332, acc.: 51.56%] [G loss: 4.412353]\n",
      "epoch:37 step:29075 [D loss: 0.025382, acc.: 100.00%] [G loss: 3.118305]\n",
      "epoch:37 step:29076 [D loss: 0.062644, acc.: 99.22%] [G loss: 3.333157]\n",
      "epoch:37 step:29077 [D loss: 0.044228, acc.: 100.00%] [G loss: 2.986683]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:37 step:29078 [D loss: 0.391997, acc.: 84.38%] [G loss: 3.530946]\n",
      "epoch:37 step:29079 [D loss: 0.113082, acc.: 97.66%] [G loss: 3.678305]\n",
      "epoch:37 step:29080 [D loss: 0.210606, acc.: 91.41%] [G loss: 3.592722]\n",
      "epoch:37 step:29081 [D loss: 0.179743, acc.: 93.75%] [G loss: 5.224161]\n",
      "epoch:37 step:29082 [D loss: 0.241953, acc.: 87.50%] [G loss: 3.074998]\n",
      "epoch:37 step:29083 [D loss: 0.334579, acc.: 85.16%] [G loss: 4.555917]\n",
      "epoch:37 step:29084 [D loss: 0.012309, acc.: 100.00%] [G loss: 4.767419]\n",
      "epoch:37 step:29085 [D loss: 0.175989, acc.: 92.19%] [G loss: 3.261494]\n",
      "epoch:37 step:29086 [D loss: 0.138408, acc.: 94.53%] [G loss: 4.587380]\n",
      "epoch:37 step:29087 [D loss: 0.013221, acc.: 100.00%] [G loss: 5.138544]\n",
      "epoch:37 step:29088 [D loss: 0.017121, acc.: 100.00%] [G loss: 4.930096]\n",
      "epoch:37 step:29089 [D loss: 0.045562, acc.: 98.44%] [G loss: 4.223336]\n",
      "epoch:37 step:29090 [D loss: 0.076169, acc.: 100.00%] [G loss: 2.987842]\n",
      "epoch:37 step:29091 [D loss: 0.038631, acc.: 100.00%] [G loss: 2.522249]\n",
      "epoch:37 step:29092 [D loss: 0.115103, acc.: 98.44%] [G loss: 4.062223]\n",
      "epoch:37 step:29093 [D loss: 0.413857, acc.: 83.59%] [G loss: 5.484747]\n",
      "epoch:37 step:29094 [D loss: 0.249516, acc.: 85.94%] [G loss: 4.283963]\n",
      "epoch:37 step:29095 [D loss: 0.033212, acc.: 99.22%] [G loss: 5.154300]\n",
      "epoch:37 step:29096 [D loss: 0.378363, acc.: 79.69%] [G loss: 6.441844]\n",
      "epoch:37 step:29097 [D loss: 0.123277, acc.: 92.97%] [G loss: 5.624793]\n",
      "epoch:37 step:29098 [D loss: 0.020380, acc.: 100.00%] [G loss: 5.928679]\n",
      "epoch:37 step:29099 [D loss: 0.630159, acc.: 71.09%] [G loss: 0.401913]\n",
      "epoch:37 step:29100 [D loss: 1.607379, acc.: 54.69%] [G loss: 7.160943]\n",
      "epoch:37 step:29101 [D loss: 1.010777, acc.: 57.03%] [G loss: 6.245249]\n",
      "epoch:37 step:29102 [D loss: 0.534172, acc.: 75.00%] [G loss: 3.974909]\n",
      "epoch:37 step:29103 [D loss: 0.090547, acc.: 96.88%] [G loss: 3.528967]\n",
      "epoch:37 step:29104 [D loss: 0.068002, acc.: 97.66%] [G loss: 3.360246]\n",
      "epoch:37 step:29105 [D loss: 0.025541, acc.: 100.00%] [G loss: 3.468339]\n",
      "epoch:37 step:29106 [D loss: 0.150383, acc.: 94.53%] [G loss: 3.495537]\n",
      "epoch:37 step:29107 [D loss: 0.025117, acc.: 100.00%] [G loss: 3.594870]\n",
      "epoch:37 step:29108 [D loss: 0.038260, acc.: 100.00%] [G loss: 3.982880]\n",
      "epoch:37 step:29109 [D loss: 0.062767, acc.: 98.44%] [G loss: 3.686628]\n",
      "epoch:37 step:29110 [D loss: 0.042001, acc.: 99.22%] [G loss: 3.601218]\n",
      "epoch:37 step:29111 [D loss: 0.046964, acc.: 100.00%] [G loss: 3.667741]\n",
      "epoch:37 step:29112 [D loss: 0.032765, acc.: 100.00%] [G loss: 4.057594]\n",
      "epoch:37 step:29113 [D loss: 0.109205, acc.: 97.66%] [G loss: 2.851830]\n",
      "epoch:37 step:29114 [D loss: 0.072668, acc.: 99.22%] [G loss: 3.484010]\n",
      "epoch:37 step:29115 [D loss: 0.187661, acc.: 92.97%] [G loss: 2.640655]\n",
      "epoch:37 step:29116 [D loss: 0.091626, acc.: 98.44%] [G loss: 3.606664]\n",
      "epoch:37 step:29117 [D loss: 0.036640, acc.: 100.00%] [G loss: 3.895629]\n",
      "epoch:37 step:29118 [D loss: 0.053589, acc.: 99.22%] [G loss: 3.506418]\n",
      "epoch:37 step:29119 [D loss: 0.079627, acc.: 100.00%] [G loss: 4.028776]\n",
      "epoch:37 step:29120 [D loss: 0.068993, acc.: 98.44%] [G loss: 3.506855]\n",
      "epoch:37 step:29121 [D loss: 0.030617, acc.: 100.00%] [G loss: 3.783219]\n",
      "epoch:37 step:29122 [D loss: 0.027989, acc.: 100.00%] [G loss: 3.416819]\n",
      "epoch:37 step:29123 [D loss: 0.036285, acc.: 100.00%] [G loss: 3.361887]\n",
      "epoch:37 step:29124 [D loss: 0.047233, acc.: 100.00%] [G loss: 3.444116]\n",
      "epoch:37 step:29125 [D loss: 0.017761, acc.: 100.00%] [G loss: 3.916596]\n",
      "epoch:37 step:29126 [D loss: 0.215658, acc.: 95.31%] [G loss: 4.688712]\n",
      "epoch:37 step:29127 [D loss: 0.017503, acc.: 100.00%] [G loss: 5.313823]\n",
      "epoch:37 step:29128 [D loss: 0.028444, acc.: 99.22%] [G loss: 5.321492]\n",
      "epoch:37 step:29129 [D loss: 0.201136, acc.: 91.41%] [G loss: 2.191440]\n",
      "epoch:37 step:29130 [D loss: 0.333550, acc.: 85.16%] [G loss: 5.732471]\n",
      "epoch:37 step:29131 [D loss: 0.005153, acc.: 100.00%] [G loss: 6.727564]\n",
      "epoch:37 step:29132 [D loss: 0.004104, acc.: 100.00%] [G loss: 6.706536]\n",
      "epoch:37 step:29133 [D loss: 0.071568, acc.: 96.88%] [G loss: 6.183081]\n",
      "epoch:37 step:29134 [D loss: 0.002324, acc.: 100.00%] [G loss: 5.950892]\n",
      "epoch:37 step:29135 [D loss: 0.003000, acc.: 100.00%] [G loss: 5.924487]\n",
      "epoch:37 step:29136 [D loss: 0.003817, acc.: 100.00%] [G loss: 5.873808]\n",
      "epoch:37 step:29137 [D loss: 0.101480, acc.: 96.09%] [G loss: 4.658636]\n",
      "epoch:37 step:29138 [D loss: 0.034249, acc.: 99.22%] [G loss: 4.445883]\n",
      "epoch:37 step:29139 [D loss: 0.009048, acc.: 100.00%] [G loss: 4.355225]\n",
      "epoch:37 step:29140 [D loss: 0.011311, acc.: 100.00%] [G loss: 5.182239]\n",
      "epoch:37 step:29141 [D loss: 0.008872, acc.: 100.00%] [G loss: 4.100951]\n",
      "epoch:37 step:29142 [D loss: 0.021012, acc.: 99.22%] [G loss: 4.608465]\n",
      "epoch:37 step:29143 [D loss: 0.011229, acc.: 100.00%] [G loss: 5.145605]\n",
      "epoch:37 step:29144 [D loss: 0.006648, acc.: 100.00%] [G loss: 4.949288]\n",
      "epoch:37 step:29145 [D loss: 0.009436, acc.: 100.00%] [G loss: 4.822859]\n",
      "epoch:37 step:29146 [D loss: 0.013043, acc.: 100.00%] [G loss: 4.427817]\n",
      "epoch:37 step:29147 [D loss: 0.063909, acc.: 99.22%] [G loss: 4.564030]\n",
      "epoch:37 step:29148 [D loss: 0.009784, acc.: 100.00%] [G loss: 4.383453]\n",
      "epoch:37 step:29149 [D loss: 0.027540, acc.: 100.00%] [G loss: 4.694392]\n",
      "epoch:37 step:29150 [D loss: 0.010081, acc.: 100.00%] [G loss: 4.985291]\n",
      "epoch:37 step:29151 [D loss: 0.026474, acc.: 100.00%] [G loss: 4.698523]\n",
      "epoch:37 step:29152 [D loss: 0.006964, acc.: 100.00%] [G loss: 4.781720]\n",
      "epoch:37 step:29153 [D loss: 0.009204, acc.: 100.00%] [G loss: 4.712625]\n",
      "epoch:37 step:29154 [D loss: 0.011812, acc.: 100.00%] [G loss: 5.136565]\n",
      "epoch:37 step:29155 [D loss: 0.024165, acc.: 100.00%] [G loss: 4.064878]\n",
      "epoch:37 step:29156 [D loss: 0.013358, acc.: 100.00%] [G loss: 4.075540]\n",
      "epoch:37 step:29157 [D loss: 0.016725, acc.: 100.00%] [G loss: 3.581277]\n",
      "epoch:37 step:29158 [D loss: 0.162015, acc.: 93.75%] [G loss: 4.669862]\n",
      "epoch:37 step:29159 [D loss: 0.003731, acc.: 100.00%] [G loss: 5.016212]\n",
      "epoch:37 step:29160 [D loss: 0.008816, acc.: 100.00%] [G loss: 5.569894]\n",
      "epoch:37 step:29161 [D loss: 0.084095, acc.: 96.88%] [G loss: 3.923033]\n",
      "epoch:37 step:29162 [D loss: 0.017923, acc.: 100.00%] [G loss: 3.568305]\n",
      "epoch:37 step:29163 [D loss: 0.060994, acc.: 99.22%] [G loss: 3.598876]\n",
      "epoch:37 step:29164 [D loss: 0.004987, acc.: 100.00%] [G loss: 4.287605]\n",
      "epoch:37 step:29165 [D loss: 0.007645, acc.: 100.00%] [G loss: 4.595810]\n",
      "epoch:37 step:29166 [D loss: 0.007486, acc.: 100.00%] [G loss: 4.981212]\n",
      "epoch:37 step:29167 [D loss: 0.036432, acc.: 98.44%] [G loss: 4.025431]\n",
      "epoch:37 step:29168 [D loss: 0.024766, acc.: 100.00%] [G loss: 4.196263]\n",
      "epoch:37 step:29169 [D loss: 0.009852, acc.: 100.00%] [G loss: 4.756669]\n",
      "epoch:37 step:29170 [D loss: 0.017222, acc.: 100.00%] [G loss: 4.416820]\n",
      "epoch:37 step:29171 [D loss: 0.018711, acc.: 100.00%] [G loss: 4.545561]\n",
      "epoch:37 step:29172 [D loss: 0.009062, acc.: 100.00%] [G loss: 4.574944]\n",
      "epoch:37 step:29173 [D loss: 0.020916, acc.: 100.00%] [G loss: 4.818326]\n",
      "epoch:37 step:29174 [D loss: 0.009804, acc.: 100.00%] [G loss: 4.194225]\n",
      "epoch:37 step:29175 [D loss: 0.029242, acc.: 100.00%] [G loss: 5.317152]\n",
      "epoch:37 step:29176 [D loss: 0.174932, acc.: 93.75%] [G loss: 3.932091]\n",
      "epoch:37 step:29177 [D loss: 0.015458, acc.: 100.00%] [G loss: 4.359879]\n",
      "epoch:37 step:29178 [D loss: 0.021016, acc.: 100.00%] [G loss: 4.989825]\n",
      "epoch:37 step:29179 [D loss: 0.008431, acc.: 100.00%] [G loss: 5.516208]\n",
      "epoch:37 step:29180 [D loss: 0.009109, acc.: 100.00%] [G loss: 6.120328]\n",
      "epoch:37 step:29181 [D loss: 0.005420, acc.: 100.00%] [G loss: 5.452801]\n",
      "epoch:37 step:29182 [D loss: 0.004242, acc.: 100.00%] [G loss: 5.300821]\n",
      "epoch:37 step:29183 [D loss: 0.690914, acc.: 69.53%] [G loss: 6.019127]\n",
      "epoch:37 step:29184 [D loss: 0.001482, acc.: 100.00%] [G loss: 7.142309]\n",
      "epoch:37 step:29185 [D loss: 0.007102, acc.: 100.00%] [G loss: 7.263186]\n",
      "epoch:37 step:29186 [D loss: 0.400573, acc.: 84.38%] [G loss: 4.808723]\n",
      "epoch:37 step:29187 [D loss: 0.904189, acc.: 70.31%] [G loss: 7.854590]\n",
      "epoch:37 step:29188 [D loss: 0.685879, acc.: 64.06%] [G loss: 6.517322]\n",
      "epoch:37 step:29189 [D loss: 0.062319, acc.: 98.44%] [G loss: 5.422123]\n",
      "epoch:37 step:29190 [D loss: 0.054553, acc.: 99.22%] [G loss: 5.307099]\n",
      "epoch:37 step:29191 [D loss: 0.064977, acc.: 98.44%] [G loss: 5.105077]\n",
      "epoch:37 step:29192 [D loss: 0.031256, acc.: 100.00%] [G loss: 4.842124]\n",
      "epoch:37 step:29193 [D loss: 0.019344, acc.: 100.00%] [G loss: 4.359662]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:37 step:29194 [D loss: 0.071826, acc.: 98.44%] [G loss: 4.740740]\n",
      "epoch:37 step:29195 [D loss: 0.046733, acc.: 99.22%] [G loss: 3.877460]\n",
      "epoch:37 step:29196 [D loss: 0.061562, acc.: 98.44%] [G loss: 4.397973]\n",
      "epoch:37 step:29197 [D loss: 0.019356, acc.: 100.00%] [G loss: 4.256059]\n",
      "epoch:37 step:29198 [D loss: 0.030398, acc.: 99.22%] [G loss: 3.461016]\n",
      "epoch:37 step:29199 [D loss: 0.091369, acc.: 97.66%] [G loss: 4.129737]\n",
      "epoch:37 step:29200 [D loss: 0.058433, acc.: 99.22%] [G loss: 4.653806]\n",
      "##############\n",
      "[0.96758409 0.83407126 2.11133575 0.89320258 1.03107857 2.10281017\n",
      " 2.1238851  0.99905869 2.10777422 2.10872956]\n",
      "##########\n",
      "epoch:37 step:29201 [D loss: 0.020188, acc.: 100.00%] [G loss: 4.551109]\n",
      "epoch:37 step:29202 [D loss: 0.086938, acc.: 96.88%] [G loss: 3.890522]\n",
      "epoch:37 step:29203 [D loss: 0.048028, acc.: 99.22%] [G loss: 3.442571]\n",
      "epoch:37 step:29204 [D loss: 0.025439, acc.: 100.00%] [G loss: 4.704973]\n",
      "epoch:37 step:29205 [D loss: 0.038227, acc.: 100.00%] [G loss: 4.582936]\n",
      "epoch:37 step:29206 [D loss: 0.206985, acc.: 91.41%] [G loss: 4.482981]\n",
      "epoch:37 step:29207 [D loss: 0.017253, acc.: 100.00%] [G loss: 4.237757]\n",
      "epoch:37 step:29208 [D loss: 0.016618, acc.: 100.00%] [G loss: 4.638456]\n",
      "epoch:37 step:29209 [D loss: 0.092161, acc.: 97.66%] [G loss: 4.454497]\n",
      "epoch:37 step:29210 [D loss: 0.032409, acc.: 100.00%] [G loss: 3.931707]\n",
      "epoch:37 step:29211 [D loss: 0.016773, acc.: 100.00%] [G loss: 3.834551]\n",
      "epoch:37 step:29212 [D loss: 0.042346, acc.: 99.22%] [G loss: 4.262529]\n",
      "epoch:37 step:29213 [D loss: 0.040207, acc.: 99.22%] [G loss: 3.998789]\n",
      "epoch:37 step:29214 [D loss: 0.008578, acc.: 100.00%] [G loss: 2.489026]\n",
      "epoch:37 step:29215 [D loss: 0.026148, acc.: 99.22%] [G loss: 3.298928]\n",
      "epoch:37 step:29216 [D loss: 0.097733, acc.: 96.88%] [G loss: 3.251333]\n",
      "epoch:37 step:29217 [D loss: 2.116448, acc.: 27.34%] [G loss: 8.402769]\n",
      "epoch:37 step:29218 [D loss: 2.633094, acc.: 50.00%] [G loss: 6.605684]\n",
      "epoch:37 step:29219 [D loss: 1.220395, acc.: 53.91%] [G loss: 2.598064]\n",
      "epoch:37 step:29220 [D loss: 0.094662, acc.: 96.88%] [G loss: 2.355097]\n",
      "epoch:37 step:29221 [D loss: 0.169079, acc.: 90.62%] [G loss: 3.155690]\n",
      "epoch:37 step:29222 [D loss: 0.009601, acc.: 100.00%] [G loss: 3.377605]\n",
      "epoch:37 step:29223 [D loss: 0.054336, acc.: 99.22%] [G loss: 3.852790]\n",
      "epoch:37 step:29224 [D loss: 0.008702, acc.: 100.00%] [G loss: 4.339750]\n",
      "epoch:37 step:29225 [D loss: 0.016341, acc.: 99.22%] [G loss: 3.199499]\n",
      "epoch:37 step:29226 [D loss: 0.024271, acc.: 100.00%] [G loss: 2.873016]\n",
      "epoch:37 step:29227 [D loss: 0.035407, acc.: 100.00%] [G loss: 2.705410]\n",
      "epoch:37 step:29228 [D loss: 0.035858, acc.: 100.00%] [G loss: 2.733001]\n",
      "epoch:37 step:29229 [D loss: 0.031380, acc.: 100.00%] [G loss: 1.935967]\n",
      "epoch:37 step:29230 [D loss: 0.034398, acc.: 100.00%] [G loss: 1.923440]\n",
      "epoch:37 step:29231 [D loss: 0.143345, acc.: 96.88%] [G loss: 4.795034]\n",
      "epoch:37 step:29232 [D loss: 0.381884, acc.: 82.03%] [G loss: 2.025218]\n",
      "epoch:37 step:29233 [D loss: 0.203186, acc.: 92.19%] [G loss: 3.071012]\n",
      "epoch:37 step:29234 [D loss: 0.008920, acc.: 100.00%] [G loss: 4.473670]\n",
      "epoch:37 step:29235 [D loss: 0.030249, acc.: 100.00%] [G loss: 3.324049]\n",
      "epoch:37 step:29236 [D loss: 0.145122, acc.: 96.88%] [G loss: 2.925604]\n",
      "epoch:37 step:29237 [D loss: 0.234619, acc.: 89.84%] [G loss: 4.802068]\n",
      "epoch:37 step:29238 [D loss: 0.009419, acc.: 100.00%] [G loss: 5.965478]\n",
      "epoch:37 step:29239 [D loss: 0.549037, acc.: 68.75%] [G loss: 0.959347]\n",
      "epoch:37 step:29240 [D loss: 1.060913, acc.: 53.12%] [G loss: 6.675422]\n",
      "epoch:37 step:29241 [D loss: 0.465753, acc.: 77.34%] [G loss: 6.615442]\n",
      "epoch:37 step:29242 [D loss: 0.670722, acc.: 64.84%] [G loss: 4.391154]\n",
      "epoch:37 step:29243 [D loss: 0.023026, acc.: 99.22%] [G loss: 3.678559]\n",
      "epoch:37 step:29244 [D loss: 0.082729, acc.: 96.88%] [G loss: 4.092161]\n",
      "epoch:37 step:29245 [D loss: 0.012588, acc.: 100.00%] [G loss: 3.584050]\n",
      "epoch:37 step:29246 [D loss: 0.023156, acc.: 100.00%] [G loss: 3.706180]\n",
      "epoch:37 step:29247 [D loss: 0.060783, acc.: 99.22%] [G loss: 2.247823]\n",
      "epoch:37 step:29248 [D loss: 0.068850, acc.: 98.44%] [G loss: 3.646937]\n",
      "epoch:37 step:29249 [D loss: 0.059075, acc.: 98.44%] [G loss: 4.304014]\n",
      "epoch:37 step:29250 [D loss: 0.019569, acc.: 100.00%] [G loss: 3.778725]\n",
      "epoch:37 step:29251 [D loss: 0.061835, acc.: 98.44%] [G loss: 3.244923]\n",
      "epoch:37 step:29252 [D loss: 0.011900, acc.: 100.00%] [G loss: 2.811588]\n",
      "epoch:37 step:29253 [D loss: 0.043123, acc.: 100.00%] [G loss: 2.455417]\n",
      "epoch:37 step:29254 [D loss: 0.022628, acc.: 100.00%] [G loss: 3.981075]\n",
      "epoch:37 step:29255 [D loss: 0.019006, acc.: 100.00%] [G loss: 2.547310]\n",
      "epoch:37 step:29256 [D loss: 0.055200, acc.: 99.22%] [G loss: 2.577914]\n",
      "epoch:37 step:29257 [D loss: 0.014006, acc.: 100.00%] [G loss: 3.319609]\n",
      "epoch:37 step:29258 [D loss: 0.064277, acc.: 100.00%] [G loss: 1.266455]\n",
      "epoch:37 step:29259 [D loss: 0.160231, acc.: 91.41%] [G loss: 3.890927]\n",
      "epoch:37 step:29260 [D loss: 0.113912, acc.: 98.44%] [G loss: 4.351542]\n",
      "epoch:37 step:29261 [D loss: 0.871377, acc.: 59.38%] [G loss: 5.398216]\n",
      "epoch:37 step:29262 [D loss: 0.002676, acc.: 100.00%] [G loss: 6.377594]\n",
      "epoch:37 step:29263 [D loss: 0.326004, acc.: 85.16%] [G loss: 4.071875]\n",
      "epoch:37 step:29264 [D loss: 0.015905, acc.: 100.00%] [G loss: 3.647168]\n",
      "epoch:37 step:29265 [D loss: 0.031925, acc.: 100.00%] [G loss: 3.222840]\n",
      "epoch:37 step:29266 [D loss: 0.017805, acc.: 100.00%] [G loss: 3.595986]\n",
      "epoch:37 step:29267 [D loss: 0.038451, acc.: 100.00%] [G loss: 3.867749]\n",
      "epoch:37 step:29268 [D loss: 0.018194, acc.: 100.00%] [G loss: 4.610803]\n",
      "epoch:37 step:29269 [D loss: 0.007661, acc.: 100.00%] [G loss: 4.495195]\n",
      "epoch:37 step:29270 [D loss: 0.015619, acc.: 100.00%] [G loss: 3.813941]\n",
      "epoch:37 step:29271 [D loss: 0.061819, acc.: 99.22%] [G loss: 3.726915]\n",
      "epoch:37 step:29272 [D loss: 0.047323, acc.: 100.00%] [G loss: 3.890523]\n",
      "epoch:37 step:29273 [D loss: 0.026526, acc.: 100.00%] [G loss: 4.079611]\n",
      "epoch:37 step:29274 [D loss: 0.007567, acc.: 100.00%] [G loss: 3.296786]\n",
      "epoch:37 step:29275 [D loss: 0.019599, acc.: 100.00%] [G loss: 3.195981]\n",
      "epoch:37 step:29276 [D loss: 0.028686, acc.: 100.00%] [G loss: 3.422008]\n",
      "epoch:37 step:29277 [D loss: 0.010786, acc.: 100.00%] [G loss: 3.949171]\n",
      "epoch:37 step:29278 [D loss: 0.013114, acc.: 100.00%] [G loss: 3.437561]\n",
      "epoch:37 step:29279 [D loss: 0.098090, acc.: 99.22%] [G loss: 3.506882]\n",
      "epoch:37 step:29280 [D loss: 0.039929, acc.: 100.00%] [G loss: 3.863326]\n",
      "epoch:37 step:29281 [D loss: 0.032233, acc.: 99.22%] [G loss: 4.358027]\n",
      "epoch:37 step:29282 [D loss: 0.677212, acc.: 64.06%] [G loss: 6.071843]\n",
      "epoch:37 step:29283 [D loss: 1.290513, acc.: 55.47%] [G loss: 3.059006]\n",
      "epoch:37 step:29284 [D loss: 0.163845, acc.: 94.53%] [G loss: 2.037273]\n",
      "epoch:37 step:29285 [D loss: 0.183260, acc.: 92.19%] [G loss: 4.132688]\n",
      "epoch:37 step:29286 [D loss: 0.229282, acc.: 91.41%] [G loss: 4.151463]\n",
      "epoch:37 step:29287 [D loss: 0.250656, acc.: 89.06%] [G loss: 3.701763]\n",
      "epoch:37 step:29288 [D loss: 0.047366, acc.: 99.22%] [G loss: 3.843793]\n",
      "epoch:37 step:29289 [D loss: 0.026670, acc.: 100.00%] [G loss: 3.712886]\n",
      "epoch:37 step:29290 [D loss: 0.085735, acc.: 96.88%] [G loss: 3.281489]\n",
      "epoch:37 step:29291 [D loss: 0.024682, acc.: 100.00%] [G loss: 3.257262]\n",
      "epoch:37 step:29292 [D loss: 0.047202, acc.: 100.00%] [G loss: 3.672886]\n",
      "epoch:37 step:29293 [D loss: 0.184165, acc.: 93.75%] [G loss: 3.012619]\n",
      "epoch:37 step:29294 [D loss: 0.074042, acc.: 98.44%] [G loss: 4.370748]\n",
      "epoch:37 step:29295 [D loss: 0.008947, acc.: 100.00%] [G loss: 5.008034]\n",
      "epoch:37 step:29296 [D loss: 0.055992, acc.: 99.22%] [G loss: 4.000381]\n",
      "epoch:37 step:29297 [D loss: 0.053374, acc.: 99.22%] [G loss: 3.118332]\n",
      "epoch:37 step:29298 [D loss: 0.032691, acc.: 100.00%] [G loss: 4.011930]\n",
      "epoch:37 step:29299 [D loss: 0.025187, acc.: 100.00%] [G loss: 3.793518]\n",
      "epoch:37 step:29300 [D loss: 0.105130, acc.: 97.66%] [G loss: 2.307273]\n",
      "epoch:37 step:29301 [D loss: 0.062943, acc.: 100.00%] [G loss: 4.941838]\n",
      "epoch:37 step:29302 [D loss: 0.087239, acc.: 98.44%] [G loss: 3.670716]\n",
      "epoch:37 step:29303 [D loss: 0.024443, acc.: 100.00%] [G loss: 4.093915]\n",
      "epoch:37 step:29304 [D loss: 0.019148, acc.: 100.00%] [G loss: 4.443500]\n",
      "epoch:37 step:29305 [D loss: 0.021673, acc.: 100.00%] [G loss: 3.910687]\n",
      "epoch:37 step:29306 [D loss: 0.112323, acc.: 96.88%] [G loss: 4.727936]\n",
      "epoch:37 step:29307 [D loss: 0.009499, acc.: 100.00%] [G loss: 5.664875]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:37 step:29308 [D loss: 0.281465, acc.: 87.50%] [G loss: 3.997989]\n",
      "epoch:37 step:29309 [D loss: 0.004279, acc.: 100.00%] [G loss: 3.913941]\n",
      "epoch:37 step:29310 [D loss: 0.016933, acc.: 100.00%] [G loss: 3.574258]\n",
      "epoch:37 step:29311 [D loss: 0.031030, acc.: 99.22%] [G loss: 4.884050]\n",
      "epoch:37 step:29312 [D loss: 0.012422, acc.: 100.00%] [G loss: 5.534365]\n",
      "epoch:37 step:29313 [D loss: 0.015998, acc.: 100.00%] [G loss: 4.540413]\n",
      "epoch:37 step:29314 [D loss: 0.092842, acc.: 97.66%] [G loss: 5.094401]\n",
      "epoch:37 step:29315 [D loss: 0.009236, acc.: 100.00%] [G loss: 5.068260]\n",
      "epoch:37 step:29316 [D loss: 0.007966, acc.: 100.00%] [G loss: 5.092659]\n",
      "epoch:37 step:29317 [D loss: 0.225971, acc.: 92.97%] [G loss: 4.463898]\n",
      "epoch:37 step:29318 [D loss: 0.004808, acc.: 100.00%] [G loss: 5.560465]\n",
      "epoch:37 step:29319 [D loss: 0.013251, acc.: 100.00%] [G loss: 5.141276]\n",
      "epoch:37 step:29320 [D loss: 0.009746, acc.: 100.00%] [G loss: 4.354164]\n",
      "epoch:37 step:29321 [D loss: 0.048120, acc.: 99.22%] [G loss: 4.661789]\n",
      "epoch:37 step:29322 [D loss: 0.016401, acc.: 100.00%] [G loss: 4.747842]\n",
      "epoch:37 step:29323 [D loss: 0.011201, acc.: 100.00%] [G loss: 4.755395]\n",
      "epoch:37 step:29324 [D loss: 0.017561, acc.: 100.00%] [G loss: 5.186895]\n",
      "epoch:37 step:29325 [D loss: 0.031249, acc.: 99.22%] [G loss: 4.426851]\n",
      "epoch:37 step:29326 [D loss: 0.023778, acc.: 100.00%] [G loss: 4.292645]\n",
      "epoch:37 step:29327 [D loss: 0.029587, acc.: 99.22%] [G loss: 4.313560]\n",
      "epoch:37 step:29328 [D loss: 0.490175, acc.: 75.78%] [G loss: 7.227878]\n",
      "epoch:37 step:29329 [D loss: 1.237513, acc.: 57.03%] [G loss: 2.661410]\n",
      "epoch:37 step:29330 [D loss: 0.911959, acc.: 63.28%] [G loss: 6.841382]\n",
      "epoch:37 step:29331 [D loss: 0.715222, acc.: 70.31%] [G loss: 5.548415]\n",
      "epoch:37 step:29332 [D loss: 0.022279, acc.: 100.00%] [G loss: 4.672514]\n",
      "epoch:37 step:29333 [D loss: 0.078622, acc.: 96.88%] [G loss: 4.025450]\n",
      "epoch:37 step:29334 [D loss: 0.019175, acc.: 100.00%] [G loss: 3.383951]\n",
      "epoch:37 step:29335 [D loss: 0.045455, acc.: 100.00%] [G loss: 4.540059]\n",
      "epoch:37 step:29336 [D loss: 0.059095, acc.: 100.00%] [G loss: 4.576436]\n",
      "epoch:37 step:29337 [D loss: 0.018619, acc.: 100.00%] [G loss: 4.560021]\n",
      "epoch:37 step:29338 [D loss: 0.110153, acc.: 97.66%] [G loss: 3.302031]\n",
      "epoch:37 step:29339 [D loss: 0.050337, acc.: 100.00%] [G loss: 4.195120]\n",
      "epoch:37 step:29340 [D loss: 0.039479, acc.: 100.00%] [G loss: 4.478029]\n",
      "epoch:37 step:29341 [D loss: 0.036672, acc.: 99.22%] [G loss: 4.742528]\n",
      "epoch:37 step:29342 [D loss: 0.054316, acc.: 98.44%] [G loss: 3.822269]\n",
      "epoch:37 step:29343 [D loss: 0.021258, acc.: 100.00%] [G loss: 3.655833]\n",
      "epoch:37 step:29344 [D loss: 0.040853, acc.: 99.22%] [G loss: 3.114950]\n",
      "epoch:37 step:29345 [D loss: 0.296174, acc.: 85.94%] [G loss: 5.566688]\n",
      "epoch:37 step:29346 [D loss: 0.079043, acc.: 97.66%] [G loss: 5.594544]\n",
      "epoch:37 step:29347 [D loss: 0.015801, acc.: 100.00%] [G loss: 5.679227]\n",
      "epoch:37 step:29348 [D loss: 0.055235, acc.: 98.44%] [G loss: 4.369729]\n",
      "epoch:37 step:29349 [D loss: 0.040565, acc.: 99.22%] [G loss: 4.527954]\n",
      "epoch:37 step:29350 [D loss: 0.099492, acc.: 96.09%] [G loss: 3.802035]\n",
      "epoch:37 step:29351 [D loss: 0.020930, acc.: 100.00%] [G loss: 4.326376]\n",
      "epoch:37 step:29352 [D loss: 0.013310, acc.: 100.00%] [G loss: 4.599218]\n",
      "epoch:37 step:29353 [D loss: 0.016719, acc.: 100.00%] [G loss: 4.149833]\n",
      "epoch:37 step:29354 [D loss: 0.015731, acc.: 99.22%] [G loss: 4.232478]\n",
      "epoch:37 step:29355 [D loss: 0.023445, acc.: 100.00%] [G loss: 4.215531]\n",
      "epoch:37 step:29356 [D loss: 0.117512, acc.: 96.88%] [G loss: 4.270034]\n",
      "epoch:37 step:29357 [D loss: 0.193430, acc.: 95.31%] [G loss: 3.973489]\n",
      "epoch:37 step:29358 [D loss: 0.014791, acc.: 100.00%] [G loss: 4.706403]\n",
      "epoch:37 step:29359 [D loss: 0.017627, acc.: 100.00%] [G loss: 4.391590]\n",
      "epoch:37 step:29360 [D loss: 0.018273, acc.: 100.00%] [G loss: 4.395565]\n",
      "epoch:37 step:29361 [D loss: 0.060156, acc.: 98.44%] [G loss: 4.070426]\n",
      "epoch:37 step:29362 [D loss: 0.016892, acc.: 100.00%] [G loss: 5.326426]\n",
      "epoch:37 step:29363 [D loss: 0.006383, acc.: 100.00%] [G loss: 4.949145]\n",
      "epoch:37 step:29364 [D loss: 0.020815, acc.: 100.00%] [G loss: 4.633797]\n",
      "epoch:37 step:29365 [D loss: 0.094923, acc.: 97.66%] [G loss: 4.851110]\n",
      "epoch:37 step:29366 [D loss: 0.006995, acc.: 100.00%] [G loss: 5.511994]\n",
      "epoch:37 step:29367 [D loss: 0.007870, acc.: 100.00%] [G loss: 5.487688]\n",
      "epoch:37 step:29368 [D loss: 0.045325, acc.: 99.22%] [G loss: 4.081722]\n",
      "epoch:37 step:29369 [D loss: 0.012448, acc.: 100.00%] [G loss: 4.101146]\n",
      "epoch:37 step:29370 [D loss: 0.024054, acc.: 100.00%] [G loss: 4.629330]\n",
      "epoch:37 step:29371 [D loss: 0.020992, acc.: 100.00%] [G loss: 4.442193]\n",
      "epoch:37 step:29372 [D loss: 0.021909, acc.: 99.22%] [G loss: 3.604729]\n",
      "epoch:37 step:29373 [D loss: 0.023158, acc.: 100.00%] [G loss: 4.447329]\n",
      "epoch:37 step:29374 [D loss: 0.015580, acc.: 100.00%] [G loss: 4.404572]\n",
      "epoch:37 step:29375 [D loss: 0.022254, acc.: 100.00%] [G loss: 3.256037]\n",
      "epoch:37 step:29376 [D loss: 0.013946, acc.: 100.00%] [G loss: 3.155567]\n",
      "epoch:37 step:29377 [D loss: 0.073611, acc.: 98.44%] [G loss: 3.953481]\n",
      "epoch:37 step:29378 [D loss: 0.007345, acc.: 100.00%] [G loss: 6.119550]\n",
      "epoch:37 step:29379 [D loss: 0.474504, acc.: 76.56%] [G loss: 5.879749]\n",
      "epoch:37 step:29380 [D loss: 0.004290, acc.: 100.00%] [G loss: 6.786868]\n",
      "epoch:37 step:29381 [D loss: 0.016077, acc.: 100.00%] [G loss: 6.623869]\n",
      "epoch:37 step:29382 [D loss: 0.486228, acc.: 78.12%] [G loss: 3.620557]\n",
      "epoch:37 step:29383 [D loss: 0.045259, acc.: 99.22%] [G loss: 5.229375]\n",
      "epoch:37 step:29384 [D loss: 0.008938, acc.: 100.00%] [G loss: 6.328451]\n",
      "epoch:37 step:29385 [D loss: 0.017527, acc.: 99.22%] [G loss: 5.653831]\n",
      "epoch:37 step:29386 [D loss: 0.037356, acc.: 99.22%] [G loss: 5.790318]\n",
      "epoch:37 step:29387 [D loss: 0.009229, acc.: 100.00%] [G loss: 5.175796]\n",
      "epoch:37 step:29388 [D loss: 0.028649, acc.: 100.00%] [G loss: 3.875031]\n",
      "epoch:37 step:29389 [D loss: 1.624297, acc.: 51.56%] [G loss: 9.260056]\n",
      "epoch:37 step:29390 [D loss: 3.718327, acc.: 50.00%] [G loss: 7.194187]\n",
      "epoch:37 step:29391 [D loss: 2.952274, acc.: 50.00%] [G loss: 5.382818]\n",
      "epoch:37 step:29392 [D loss: 1.994979, acc.: 50.00%] [G loss: 3.756371]\n",
      "epoch:37 step:29393 [D loss: 1.119583, acc.: 53.91%] [G loss: 2.220213]\n",
      "epoch:37 step:29394 [D loss: 0.389921, acc.: 85.94%] [G loss: 1.775031]\n",
      "epoch:37 step:29395 [D loss: 0.256697, acc.: 92.97%] [G loss: 1.660656]\n",
      "epoch:37 step:29396 [D loss: 0.187789, acc.: 96.09%] [G loss: 2.293374]\n",
      "epoch:37 step:29397 [D loss: 0.192907, acc.: 96.88%] [G loss: 2.260437]\n",
      "epoch:37 step:29398 [D loss: 0.052683, acc.: 100.00%] [G loss: 2.664675]\n",
      "epoch:37 step:29399 [D loss: 0.113816, acc.: 99.22%] [G loss: 2.348401]\n",
      "epoch:37 step:29400 [D loss: 0.124996, acc.: 99.22%] [G loss: 1.966297]\n",
      "##############\n",
      "[0.91944208 0.93176224 0.93258077 0.80246662 2.10889412 0.97763855\n",
      " 2.10982253 0.81881866 1.11287662 0.99281269]\n",
      "##########\n",
      "epoch:37 step:29401 [D loss: 0.232398, acc.: 91.41%] [G loss: 2.696556]\n",
      "epoch:37 step:29402 [D loss: 0.044789, acc.: 100.00%] [G loss: 3.322593]\n",
      "epoch:37 step:29403 [D loss: 0.389461, acc.: 80.47%] [G loss: 2.145753]\n",
      "epoch:37 step:29404 [D loss: 0.146712, acc.: 98.44%] [G loss: 2.340246]\n",
      "epoch:37 step:29405 [D loss: 0.054921, acc.: 99.22%] [G loss: 2.994765]\n",
      "epoch:37 step:29406 [D loss: 0.726063, acc.: 64.84%] [G loss: 2.061440]\n",
      "epoch:37 step:29407 [D loss: 0.187468, acc.: 96.09%] [G loss: 3.086228]\n",
      "epoch:37 step:29408 [D loss: 0.170259, acc.: 92.97%] [G loss: 2.812062]\n",
      "epoch:37 step:29409 [D loss: 0.201513, acc.: 96.88%] [G loss: 2.886656]\n",
      "epoch:37 step:29410 [D loss: 0.110855, acc.: 98.44%] [G loss: 2.670479]\n",
      "epoch:37 step:29411 [D loss: 0.189442, acc.: 96.88%] [G loss: 2.804073]\n",
      "epoch:37 step:29412 [D loss: 0.087707, acc.: 100.00%] [G loss: 2.573804]\n",
      "epoch:37 step:29413 [D loss: 0.088243, acc.: 99.22%] [G loss: 3.070955]\n",
      "epoch:37 step:29414 [D loss: 0.067210, acc.: 99.22%] [G loss: 3.062454]\n",
      "epoch:37 step:29415 [D loss: 0.043086, acc.: 100.00%] [G loss: 3.252614]\n",
      "epoch:37 step:29416 [D loss: 0.255453, acc.: 90.62%] [G loss: 1.833874]\n",
      "epoch:37 step:29417 [D loss: 0.334802, acc.: 82.03%] [G loss: 4.021724]\n",
      "epoch:37 step:29418 [D loss: 0.615872, acc.: 67.97%] [G loss: 3.594067]\n",
      "epoch:37 step:29419 [D loss: 0.172793, acc.: 92.19%] [G loss: 2.055502]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:37 step:29420 [D loss: 0.280268, acc.: 83.59%] [G loss: 3.677089]\n",
      "epoch:37 step:29421 [D loss: 0.183082, acc.: 91.41%] [G loss: 3.622995]\n",
      "epoch:37 step:29422 [D loss: 0.161796, acc.: 96.09%] [G loss: 2.929966]\n",
      "epoch:37 step:29423 [D loss: 0.222146, acc.: 92.97%] [G loss: 3.536104]\n",
      "epoch:37 step:29424 [D loss: 0.122108, acc.: 97.66%] [G loss: 3.053780]\n",
      "epoch:37 step:29425 [D loss: 0.214755, acc.: 93.75%] [G loss: 2.601914]\n",
      "epoch:37 step:29426 [D loss: 0.054026, acc.: 100.00%] [G loss: 3.362792]\n",
      "epoch:37 step:29427 [D loss: 0.145810, acc.: 96.88%] [G loss: 2.851936]\n",
      "epoch:37 step:29428 [D loss: 0.034482, acc.: 100.00%] [G loss: 2.426908]\n",
      "epoch:37 step:29429 [D loss: 0.158557, acc.: 96.88%] [G loss: 3.503507]\n",
      "epoch:37 step:29430 [D loss: 0.354786, acc.: 85.94%] [G loss: 2.304595]\n",
      "epoch:37 step:29431 [D loss: 0.382685, acc.: 83.59%] [G loss: 4.060410]\n",
      "epoch:37 step:29432 [D loss: 0.145913, acc.: 93.75%] [G loss: 4.368815]\n",
      "epoch:37 step:29433 [D loss: 0.124643, acc.: 94.53%] [G loss: 3.902027]\n",
      "epoch:37 step:29434 [D loss: 0.114662, acc.: 96.88%] [G loss: 3.065638]\n",
      "epoch:37 step:29435 [D loss: 0.044514, acc.: 100.00%] [G loss: 2.681210]\n",
      "epoch:37 step:29436 [D loss: 0.043904, acc.: 100.00%] [G loss: 3.198436]\n",
      "epoch:37 step:29437 [D loss: 0.071932, acc.: 99.22%] [G loss: 3.116676]\n",
      "epoch:37 step:29438 [D loss: 0.067778, acc.: 100.00%] [G loss: 3.223682]\n",
      "epoch:37 step:29439 [D loss: 0.113891, acc.: 99.22%] [G loss: 3.234104]\n",
      "epoch:37 step:29440 [D loss: 0.142193, acc.: 96.88%] [G loss: 3.226599]\n",
      "epoch:37 step:29441 [D loss: 0.205267, acc.: 96.09%] [G loss: 3.377024]\n",
      "epoch:37 step:29442 [D loss: 0.076774, acc.: 99.22%] [G loss: 4.118313]\n",
      "epoch:37 step:29443 [D loss: 0.223522, acc.: 91.41%] [G loss: 2.825438]\n",
      "epoch:37 step:29444 [D loss: 0.142875, acc.: 92.19%] [G loss: 4.134139]\n",
      "epoch:37 step:29445 [D loss: 0.061729, acc.: 97.66%] [G loss: 4.080820]\n",
      "epoch:37 step:29446 [D loss: 0.042397, acc.: 100.00%] [G loss: 3.600074]\n",
      "epoch:37 step:29447 [D loss: 0.048126, acc.: 99.22%] [G loss: 2.655382]\n",
      "epoch:37 step:29448 [D loss: 0.078168, acc.: 97.66%] [G loss: 2.602956]\n",
      "epoch:37 step:29449 [D loss: 0.180982, acc.: 92.97%] [G loss: 2.991216]\n",
      "epoch:37 step:29450 [D loss: 0.113026, acc.: 96.09%] [G loss: 3.463513]\n",
      "epoch:37 step:29451 [D loss: 0.131719, acc.: 96.88%] [G loss: 3.808528]\n",
      "epoch:37 step:29452 [D loss: 0.081613, acc.: 97.66%] [G loss: 3.129218]\n",
      "epoch:37 step:29453 [D loss: 0.078372, acc.: 100.00%] [G loss: 2.705793]\n",
      "epoch:37 step:29454 [D loss: 0.205213, acc.: 93.75%] [G loss: 4.413966]\n",
      "epoch:37 step:29455 [D loss: 0.061946, acc.: 97.66%] [G loss: 4.381894]\n",
      "epoch:37 step:29456 [D loss: 0.056075, acc.: 99.22%] [G loss: 3.580502]\n",
      "epoch:37 step:29457 [D loss: 0.231470, acc.: 92.19%] [G loss: 3.914592]\n",
      "epoch:37 step:29458 [D loss: 0.083975, acc.: 96.88%] [G loss: 3.429616]\n",
      "epoch:37 step:29459 [D loss: 0.147270, acc.: 96.88%] [G loss: 2.473188]\n",
      "epoch:37 step:29460 [D loss: 0.056185, acc.: 99.22%] [G loss: 3.102318]\n",
      "epoch:37 step:29461 [D loss: 0.027727, acc.: 100.00%] [G loss: 3.253202]\n",
      "epoch:37 step:29462 [D loss: 0.195571, acc.: 93.75%] [G loss: 3.395340]\n",
      "epoch:37 step:29463 [D loss: 0.023871, acc.: 100.00%] [G loss: 3.770114]\n",
      "epoch:37 step:29464 [D loss: 0.008411, acc.: 100.00%] [G loss: 2.881486]\n",
      "epoch:37 step:29465 [D loss: 0.050997, acc.: 100.00%] [G loss: 3.036758]\n",
      "epoch:37 step:29466 [D loss: 0.191443, acc.: 93.75%] [G loss: 2.071743]\n",
      "epoch:37 step:29467 [D loss: 0.175421, acc.: 96.09%] [G loss: 3.268235]\n",
      "epoch:37 step:29468 [D loss: 0.138598, acc.: 95.31%] [G loss: 1.577366]\n",
      "epoch:37 step:29469 [D loss: 0.124898, acc.: 96.88%] [G loss: 3.308734]\n",
      "epoch:37 step:29470 [D loss: 0.182825, acc.: 94.53%] [G loss: 3.122069]\n",
      "epoch:37 step:29471 [D loss: 0.147819, acc.: 96.09%] [G loss: 1.539527]\n",
      "epoch:37 step:29472 [D loss: 0.177105, acc.: 93.75%] [G loss: 3.139039]\n",
      "epoch:37 step:29473 [D loss: 0.025275, acc.: 100.00%] [G loss: 4.325769]\n",
      "epoch:37 step:29474 [D loss: 0.372855, acc.: 82.03%] [G loss: 2.651897]\n",
      "epoch:37 step:29475 [D loss: 0.174064, acc.: 93.75%] [G loss: 4.916727]\n",
      "epoch:37 step:29476 [D loss: 0.024400, acc.: 99.22%] [G loss: 6.079028]\n",
      "epoch:37 step:29477 [D loss: 0.328391, acc.: 85.16%] [G loss: 1.920516]\n",
      "epoch:37 step:29478 [D loss: 0.345008, acc.: 87.50%] [G loss: 5.731433]\n",
      "epoch:37 step:29479 [D loss: 0.022283, acc.: 100.00%] [G loss: 6.583346]\n",
      "epoch:37 step:29480 [D loss: 1.112713, acc.: 57.03%] [G loss: 2.494003]\n",
      "epoch:37 step:29481 [D loss: 0.185325, acc.: 90.62%] [G loss: 3.899549]\n",
      "epoch:37 step:29482 [D loss: 0.036795, acc.: 98.44%] [G loss: 4.707578]\n",
      "epoch:37 step:29483 [D loss: 0.020173, acc.: 100.00%] [G loss: 4.318026]\n",
      "epoch:37 step:29484 [D loss: 0.140221, acc.: 96.09%] [G loss: 3.602385]\n",
      "epoch:37 step:29485 [D loss: 0.053836, acc.: 99.22%] [G loss: 3.819415]\n",
      "epoch:37 step:29486 [D loss: 0.048236, acc.: 98.44%] [G loss: 3.809364]\n",
      "epoch:37 step:29487 [D loss: 0.030698, acc.: 100.00%] [G loss: 4.032544]\n",
      "epoch:37 step:29488 [D loss: 0.054330, acc.: 98.44%] [G loss: 3.662474]\n",
      "epoch:37 step:29489 [D loss: 0.056918, acc.: 99.22%] [G loss: 3.702378]\n",
      "epoch:37 step:29490 [D loss: 0.017722, acc.: 100.00%] [G loss: 4.227470]\n",
      "epoch:37 step:29491 [D loss: 0.061689, acc.: 100.00%] [G loss: 4.430044]\n",
      "epoch:37 step:29492 [D loss: 0.054630, acc.: 98.44%] [G loss: 4.294003]\n",
      "epoch:37 step:29493 [D loss: 0.255418, acc.: 89.06%] [G loss: 2.813597]\n",
      "epoch:37 step:29494 [D loss: 0.090410, acc.: 98.44%] [G loss: 3.965854]\n",
      "epoch:37 step:29495 [D loss: 0.019855, acc.: 100.00%] [G loss: 3.823761]\n",
      "epoch:37 step:29496 [D loss: 0.019951, acc.: 100.00%] [G loss: 4.537342]\n",
      "epoch:37 step:29497 [D loss: 0.224825, acc.: 92.97%] [G loss: 4.065258]\n",
      "epoch:37 step:29498 [D loss: 0.034301, acc.: 100.00%] [G loss: 4.402174]\n",
      "epoch:37 step:29499 [D loss: 0.044079, acc.: 99.22%] [G loss: 4.116549]\n",
      "epoch:37 step:29500 [D loss: 0.033389, acc.: 100.00%] [G loss: 3.691231]\n",
      "epoch:37 step:29501 [D loss: 0.024836, acc.: 100.00%] [G loss: 3.501885]\n",
      "epoch:37 step:29502 [D loss: 0.061691, acc.: 99.22%] [G loss: 4.259233]\n",
      "epoch:37 step:29503 [D loss: 0.176267, acc.: 92.97%] [G loss: 3.043705]\n",
      "epoch:37 step:29504 [D loss: 0.076418, acc.: 98.44%] [G loss: 3.648024]\n",
      "epoch:37 step:29505 [D loss: 0.028728, acc.: 99.22%] [G loss: 4.566061]\n",
      "epoch:37 step:29506 [D loss: 0.154060, acc.: 95.31%] [G loss: 4.270941]\n",
      "epoch:37 step:29507 [D loss: 0.169120, acc.: 95.31%] [G loss: 3.800011]\n",
      "epoch:37 step:29508 [D loss: 0.024865, acc.: 100.00%] [G loss: 4.469163]\n",
      "epoch:37 step:29509 [D loss: 0.113184, acc.: 98.44%] [G loss: 3.316057]\n",
      "epoch:37 step:29510 [D loss: 0.032995, acc.: 100.00%] [G loss: 3.448938]\n",
      "epoch:37 step:29511 [D loss: 0.026106, acc.: 100.00%] [G loss: 3.347847]\n",
      "epoch:37 step:29512 [D loss: 0.036435, acc.: 100.00%] [G loss: 4.272201]\n",
      "epoch:37 step:29513 [D loss: 0.140649, acc.: 96.88%] [G loss: 4.077407]\n",
      "epoch:37 step:29514 [D loss: 0.123604, acc.: 95.31%] [G loss: 4.978986]\n",
      "epoch:37 step:29515 [D loss: 0.142840, acc.: 92.97%] [G loss: 4.037178]\n",
      "epoch:37 step:29516 [D loss: 0.093292, acc.: 98.44%] [G loss: 5.531723]\n",
      "epoch:37 step:29517 [D loss: 0.042983, acc.: 97.66%] [G loss: 5.099833]\n",
      "epoch:37 step:29518 [D loss: 0.049646, acc.: 98.44%] [G loss: 4.315882]\n",
      "epoch:37 step:29519 [D loss: 0.085935, acc.: 97.66%] [G loss: 4.271938]\n",
      "epoch:37 step:29520 [D loss: 0.016455, acc.: 100.00%] [G loss: 5.413654]\n",
      "epoch:37 step:29521 [D loss: 0.031277, acc.: 99.22%] [G loss: 4.712085]\n",
      "epoch:37 step:29522 [D loss: 0.021155, acc.: 100.00%] [G loss: 4.298591]\n",
      "epoch:37 step:29523 [D loss: 0.094137, acc.: 98.44%] [G loss: 4.759834]\n",
      "epoch:37 step:29524 [D loss: 0.181362, acc.: 93.75%] [G loss: 4.079264]\n",
      "epoch:37 step:29525 [D loss: 0.011743, acc.: 100.00%] [G loss: 5.202385]\n",
      "epoch:37 step:29526 [D loss: 0.183142, acc.: 93.75%] [G loss: 3.376400]\n",
      "epoch:37 step:29527 [D loss: 0.139015, acc.: 95.31%] [G loss: 5.003521]\n",
      "epoch:37 step:29528 [D loss: 0.018039, acc.: 99.22%] [G loss: 5.996673]\n",
      "epoch:37 step:29529 [D loss: 0.109372, acc.: 94.53%] [G loss: 3.617170]\n",
      "epoch:37 step:29530 [D loss: 0.125214, acc.: 93.75%] [G loss: 4.755517]\n",
      "epoch:37 step:29531 [D loss: 0.045281, acc.: 98.44%] [G loss: 4.951589]\n",
      "epoch:37 step:29532 [D loss: 0.056274, acc.: 96.88%] [G loss: 3.658740]\n",
      "epoch:37 step:29533 [D loss: 0.012988, acc.: 100.00%] [G loss: 3.389963]\n",
      "epoch:37 step:29534 [D loss: 0.060227, acc.: 100.00%] [G loss: 4.318543]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:37 step:29535 [D loss: 0.007168, acc.: 100.00%] [G loss: 5.486116]\n",
      "epoch:37 step:29536 [D loss: 0.132551, acc.: 95.31%] [G loss: 2.399603]\n",
      "epoch:37 step:29537 [D loss: 0.675550, acc.: 67.97%] [G loss: 8.202072]\n",
      "epoch:37 step:29538 [D loss: 2.120401, acc.: 50.78%] [G loss: 5.992535]\n",
      "epoch:37 step:29539 [D loss: 1.275173, acc.: 56.25%] [G loss: 2.906714]\n",
      "epoch:37 step:29540 [D loss: 0.480245, acc.: 80.47%] [G loss: 4.173981]\n",
      "epoch:37 step:29541 [D loss: 0.042919, acc.: 99.22%] [G loss: 4.520243]\n",
      "epoch:37 step:29542 [D loss: 0.272365, acc.: 88.28%] [G loss: 3.511833]\n",
      "epoch:37 step:29543 [D loss: 0.040124, acc.: 100.00%] [G loss: 3.010339]\n",
      "epoch:37 step:29544 [D loss: 0.078745, acc.: 100.00%] [G loss: 3.213889]\n",
      "epoch:37 step:29545 [D loss: 0.015326, acc.: 100.00%] [G loss: 3.880921]\n",
      "epoch:37 step:29546 [D loss: 0.088673, acc.: 97.66%] [G loss: 3.383634]\n",
      "epoch:37 step:29547 [D loss: 0.040374, acc.: 100.00%] [G loss: 3.405730]\n",
      "epoch:37 step:29548 [D loss: 0.048712, acc.: 99.22%] [G loss: 3.081035]\n",
      "epoch:37 step:29549 [D loss: 0.058409, acc.: 99.22%] [G loss: 3.191408]\n",
      "epoch:37 step:29550 [D loss: 0.033511, acc.: 100.00%] [G loss: 3.856305]\n",
      "epoch:37 step:29551 [D loss: 0.085759, acc.: 98.44%] [G loss: 3.310973]\n",
      "epoch:37 step:29552 [D loss: 0.045215, acc.: 100.00%] [G loss: 3.770699]\n",
      "epoch:37 step:29553 [D loss: 0.097119, acc.: 97.66%] [G loss: 3.021775]\n",
      "epoch:37 step:29554 [D loss: 0.046712, acc.: 100.00%] [G loss: 3.573958]\n",
      "epoch:37 step:29555 [D loss: 0.048608, acc.: 100.00%] [G loss: 3.773620]\n",
      "epoch:37 step:29556 [D loss: 0.035763, acc.: 98.44%] [G loss: 3.636804]\n",
      "epoch:37 step:29557 [D loss: 0.056790, acc.: 98.44%] [G loss: 3.383900]\n",
      "epoch:37 step:29558 [D loss: 0.108136, acc.: 96.88%] [G loss: 4.031376]\n",
      "epoch:37 step:29559 [D loss: 0.078842, acc.: 99.22%] [G loss: 3.936025]\n",
      "epoch:37 step:29560 [D loss: 0.022934, acc.: 100.00%] [G loss: 3.515322]\n",
      "epoch:37 step:29561 [D loss: 0.021038, acc.: 100.00%] [G loss: 4.050554]\n",
      "epoch:37 step:29562 [D loss: 0.044984, acc.: 99.22%] [G loss: 3.339893]\n",
      "epoch:37 step:29563 [D loss: 0.019176, acc.: 100.00%] [G loss: 3.499876]\n",
      "epoch:37 step:29564 [D loss: 0.203091, acc.: 93.75%] [G loss: 4.646192]\n",
      "epoch:37 step:29565 [D loss: 0.072806, acc.: 97.66%] [G loss: 4.700526]\n",
      "epoch:37 step:29566 [D loss: 0.022289, acc.: 100.00%] [G loss: 3.680552]\n",
      "epoch:37 step:29567 [D loss: 0.026159, acc.: 100.00%] [G loss: 3.686115]\n",
      "epoch:37 step:29568 [D loss: 0.060578, acc.: 100.00%] [G loss: 4.199460]\n",
      "epoch:37 step:29569 [D loss: 0.019693, acc.: 100.00%] [G loss: 4.158155]\n",
      "epoch:37 step:29570 [D loss: 0.011505, acc.: 100.00%] [G loss: 4.485151]\n",
      "epoch:37 step:29571 [D loss: 0.021267, acc.: 100.00%] [G loss: 4.523470]\n",
      "epoch:37 step:29572 [D loss: 0.010259, acc.: 100.00%] [G loss: 3.546713]\n",
      "epoch:37 step:29573 [D loss: 0.098559, acc.: 96.09%] [G loss: 4.190554]\n",
      "epoch:37 step:29574 [D loss: 0.036304, acc.: 99.22%] [G loss: 4.486639]\n",
      "epoch:37 step:29575 [D loss: 0.271748, acc.: 86.72%] [G loss: 2.881168]\n",
      "epoch:37 step:29576 [D loss: 0.096987, acc.: 95.31%] [G loss: 5.038067]\n",
      "epoch:37 step:29577 [D loss: 0.009169, acc.: 100.00%] [G loss: 6.055634]\n",
      "epoch:37 step:29578 [D loss: 0.069571, acc.: 98.44%] [G loss: 5.375596]\n",
      "epoch:37 step:29579 [D loss: 0.007378, acc.: 100.00%] [G loss: 4.379739]\n",
      "epoch:37 step:29580 [D loss: 0.009154, acc.: 100.00%] [G loss: 4.491639]\n",
      "epoch:37 step:29581 [D loss: 0.019592, acc.: 100.00%] [G loss: 4.617048]\n",
      "epoch:37 step:29582 [D loss: 0.083187, acc.: 98.44%] [G loss: 3.936346]\n",
      "epoch:37 step:29583 [D loss: 0.007974, acc.: 100.00%] [G loss: 4.020689]\n",
      "epoch:37 step:29584 [D loss: 0.008940, acc.: 100.00%] [G loss: 4.029649]\n",
      "epoch:37 step:29585 [D loss: 0.019917, acc.: 100.00%] [G loss: 3.957833]\n",
      "epoch:37 step:29586 [D loss: 0.034033, acc.: 100.00%] [G loss: 4.663605]\n",
      "epoch:37 step:29587 [D loss: 0.046389, acc.: 98.44%] [G loss: 2.787901]\n",
      "epoch:37 step:29588 [D loss: 0.040313, acc.: 100.00%] [G loss: 3.485315]\n",
      "epoch:37 step:29589 [D loss: 0.016907, acc.: 100.00%] [G loss: 3.830011]\n",
      "epoch:37 step:29590 [D loss: 0.010911, acc.: 100.00%] [G loss: 4.142596]\n",
      "epoch:37 step:29591 [D loss: 0.056593, acc.: 99.22%] [G loss: 2.412074]\n",
      "epoch:37 step:29592 [D loss: 0.017864, acc.: 100.00%] [G loss: 2.771064]\n",
      "epoch:37 step:29593 [D loss: 0.009972, acc.: 100.00%] [G loss: 2.637914]\n",
      "epoch:37 step:29594 [D loss: 0.031846, acc.: 100.00%] [G loss: 3.743932]\n",
      "epoch:37 step:29595 [D loss: 0.170149, acc.: 94.53%] [G loss: 5.332093]\n",
      "epoch:37 step:29596 [D loss: 0.034558, acc.: 100.00%] [G loss: 5.932113]\n",
      "epoch:37 step:29597 [D loss: 0.170422, acc.: 96.09%] [G loss: 4.844264]\n",
      "epoch:37 step:29598 [D loss: 0.018630, acc.: 100.00%] [G loss: 5.386615]\n",
      "epoch:37 step:29599 [D loss: 0.002061, acc.: 100.00%] [G loss: 5.929568]\n",
      "epoch:37 step:29600 [D loss: 0.041430, acc.: 98.44%] [G loss: 4.687331]\n",
      "##############\n",
      "[0.97363151 0.89460002 1.02715441 0.78416085 1.0163467  2.11088945\n",
      " 2.11482085 0.98923958 1.06818883 1.03749412]\n",
      "##########\n",
      "epoch:37 step:29601 [D loss: 0.014041, acc.: 100.00%] [G loss: 4.169384]\n",
      "epoch:37 step:29602 [D loss: 0.012306, acc.: 100.00%] [G loss: 3.879110]\n",
      "epoch:37 step:29603 [D loss: 0.015174, acc.: 100.00%] [G loss: 4.842533]\n",
      "epoch:37 step:29604 [D loss: 0.044154, acc.: 100.00%] [G loss: 5.179840]\n",
      "epoch:37 step:29605 [D loss: 0.006032, acc.: 100.00%] [G loss: 5.149502]\n",
      "epoch:37 step:29606 [D loss: 0.021077, acc.: 100.00%] [G loss: 4.846176]\n",
      "epoch:37 step:29607 [D loss: 0.705823, acc.: 67.97%] [G loss: 9.683042]\n",
      "epoch:37 step:29608 [D loss: 3.791310, acc.: 50.00%] [G loss: 7.249928]\n",
      "epoch:37 step:29609 [D loss: 2.412648, acc.: 50.00%] [G loss: 4.774522]\n",
      "epoch:37 step:29610 [D loss: 1.276725, acc.: 53.91%] [G loss: 1.296646]\n",
      "epoch:37 step:29611 [D loss: 0.434329, acc.: 85.16%] [G loss: 2.599060]\n",
      "epoch:37 step:29612 [D loss: 0.110280, acc.: 99.22%] [G loss: 3.060731]\n",
      "epoch:37 step:29613 [D loss: 0.123486, acc.: 96.09%] [G loss: 3.325218]\n",
      "epoch:37 step:29614 [D loss: 0.050904, acc.: 100.00%] [G loss: 2.584994]\n",
      "epoch:37 step:29615 [D loss: 0.052268, acc.: 99.22%] [G loss: 2.224563]\n",
      "epoch:37 step:29616 [D loss: 0.046789, acc.: 99.22%] [G loss: 2.693569]\n",
      "epoch:37 step:29617 [D loss: 0.087936, acc.: 98.44%] [G loss: 2.615768]\n",
      "epoch:37 step:29618 [D loss: 0.048516, acc.: 100.00%] [G loss: 3.602322]\n",
      "epoch:37 step:29619 [D loss: 0.169890, acc.: 94.53%] [G loss: 1.857780]\n",
      "epoch:37 step:29620 [D loss: 0.071625, acc.: 99.22%] [G loss: 2.453020]\n",
      "epoch:37 step:29621 [D loss: 0.067305, acc.: 100.00%] [G loss: 2.347220]\n",
      "epoch:37 step:29622 [D loss: 0.487422, acc.: 76.56%] [G loss: 3.074666]\n",
      "epoch:37 step:29623 [D loss: 0.022236, acc.: 100.00%] [G loss: 3.940827]\n",
      "epoch:37 step:29624 [D loss: 0.088778, acc.: 98.44%] [G loss: 2.898619]\n",
      "epoch:37 step:29625 [D loss: 0.112161, acc.: 96.88%] [G loss: 3.938595]\n",
      "epoch:37 step:29626 [D loss: 0.025140, acc.: 100.00%] [G loss: 4.269877]\n",
      "epoch:37 step:29627 [D loss: 0.046800, acc.: 100.00%] [G loss: 3.833027]\n",
      "epoch:37 step:29628 [D loss: 0.015061, acc.: 100.00%] [G loss: 3.941556]\n",
      "epoch:37 step:29629 [D loss: 0.024450, acc.: 100.00%] [G loss: 3.369553]\n",
      "epoch:37 step:29630 [D loss: 0.026110, acc.: 100.00%] [G loss: 3.300939]\n",
      "epoch:37 step:29631 [D loss: 0.034193, acc.: 100.00%] [G loss: 3.291365]\n",
      "epoch:37 step:29632 [D loss: 0.094418, acc.: 100.00%] [G loss: 3.154834]\n",
      "epoch:37 step:29633 [D loss: 0.047694, acc.: 100.00%] [G loss: 3.301877]\n",
      "epoch:37 step:29634 [D loss: 0.062709, acc.: 100.00%] [G loss: 3.437607]\n",
      "epoch:37 step:29635 [D loss: 0.017671, acc.: 100.00%] [G loss: 4.151977]\n",
      "epoch:37 step:29636 [D loss: 0.203921, acc.: 95.31%] [G loss: 2.989518]\n",
      "epoch:37 step:29637 [D loss: 0.050538, acc.: 100.00%] [G loss: 3.800543]\n",
      "epoch:37 step:29638 [D loss: 0.119814, acc.: 97.66%] [G loss: 3.098115]\n",
      "epoch:37 step:29639 [D loss: 0.130382, acc.: 97.66%] [G loss: 4.571745]\n",
      "epoch:37 step:29640 [D loss: 0.048655, acc.: 98.44%] [G loss: 4.738008]\n",
      "epoch:37 step:29641 [D loss: 0.026690, acc.: 99.22%] [G loss: 3.946849]\n",
      "epoch:37 step:29642 [D loss: 0.029368, acc.: 100.00%] [G loss: 3.943588]\n",
      "epoch:37 step:29643 [D loss: 0.026535, acc.: 100.00%] [G loss: 3.569368]\n",
      "epoch:37 step:29644 [D loss: 0.024719, acc.: 100.00%] [G loss: 3.836891]\n",
      "epoch:37 step:29645 [D loss: 0.063267, acc.: 98.44%] [G loss: 3.225360]\n",
      "epoch:37 step:29646 [D loss: 0.041857, acc.: 100.00%] [G loss: 3.020108]\n",
      "epoch:37 step:29647 [D loss: 0.026177, acc.: 100.00%] [G loss: 2.935116]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:37 step:29648 [D loss: 0.016640, acc.: 100.00%] [G loss: 3.315143]\n",
      "epoch:37 step:29649 [D loss: 0.046245, acc.: 100.00%] [G loss: 2.072532]\n",
      "epoch:37 step:29650 [D loss: 0.299362, acc.: 85.94%] [G loss: 5.749612]\n",
      "epoch:37 step:29651 [D loss: 1.138267, acc.: 56.25%] [G loss: 2.306935]\n",
      "epoch:37 step:29652 [D loss: 0.200362, acc.: 90.62%] [G loss: 3.698594]\n",
      "epoch:37 step:29653 [D loss: 0.012083, acc.: 100.00%] [G loss: 4.654488]\n",
      "epoch:37 step:29654 [D loss: 0.131525, acc.: 92.97%] [G loss: 3.685608]\n",
      "epoch:37 step:29655 [D loss: 0.016849, acc.: 100.00%] [G loss: 3.230461]\n",
      "epoch:37 step:29656 [D loss: 0.027389, acc.: 100.00%] [G loss: 3.628025]\n",
      "epoch:37 step:29657 [D loss: 0.018237, acc.: 100.00%] [G loss: 3.265584]\n",
      "epoch:37 step:29658 [D loss: 0.019388, acc.: 100.00%] [G loss: 3.025192]\n",
      "epoch:37 step:29659 [D loss: 0.023085, acc.: 100.00%] [G loss: 3.343637]\n",
      "epoch:37 step:29660 [D loss: 0.038315, acc.: 100.00%] [G loss: 3.581250]\n",
      "epoch:37 step:29661 [D loss: 0.020014, acc.: 100.00%] [G loss: 3.723504]\n",
      "epoch:37 step:29662 [D loss: 0.018414, acc.: 99.22%] [G loss: 3.704094]\n",
      "epoch:37 step:29663 [D loss: 0.096126, acc.: 98.44%] [G loss: 3.186105]\n",
      "epoch:37 step:29664 [D loss: 0.020245, acc.: 100.00%] [G loss: 3.915437]\n",
      "epoch:37 step:29665 [D loss: 0.055320, acc.: 100.00%] [G loss: 4.070164]\n",
      "epoch:37 step:29666 [D loss: 0.016108, acc.: 100.00%] [G loss: 3.961397]\n",
      "epoch:37 step:29667 [D loss: 0.044278, acc.: 99.22%] [G loss: 3.739434]\n",
      "epoch:37 step:29668 [D loss: 0.097860, acc.: 98.44%] [G loss: 3.544520]\n",
      "epoch:37 step:29669 [D loss: 0.020006, acc.: 100.00%] [G loss: 3.888714]\n",
      "epoch:37 step:29670 [D loss: 0.013823, acc.: 100.00%] [G loss: 3.827411]\n",
      "epoch:37 step:29671 [D loss: 0.014872, acc.: 100.00%] [G loss: 4.077005]\n",
      "epoch:37 step:29672 [D loss: 0.014143, acc.: 100.00%] [G loss: 4.161059]\n",
      "epoch:37 step:29673 [D loss: 0.022840, acc.: 100.00%] [G loss: 4.153213]\n",
      "epoch:37 step:29674 [D loss: 0.018182, acc.: 100.00%] [G loss: 3.577655]\n",
      "epoch:37 step:29675 [D loss: 0.009865, acc.: 100.00%] [G loss: 4.026038]\n",
      "epoch:37 step:29676 [D loss: 0.015510, acc.: 100.00%] [G loss: 3.429360]\n",
      "epoch:37 step:29677 [D loss: 0.037112, acc.: 100.00%] [G loss: 4.252703]\n",
      "epoch:37 step:29678 [D loss: 0.012569, acc.: 99.22%] [G loss: 3.927678]\n",
      "epoch:38 step:29679 [D loss: 0.021714, acc.: 100.00%] [G loss: 4.817342]\n",
      "epoch:38 step:29680 [D loss: 0.019413, acc.: 100.00%] [G loss: 3.373449]\n",
      "epoch:38 step:29681 [D loss: 0.014363, acc.: 100.00%] [G loss: 3.331334]\n",
      "epoch:38 step:29682 [D loss: 0.072153, acc.: 99.22%] [G loss: 4.980344]\n",
      "epoch:38 step:29683 [D loss: 0.002784, acc.: 100.00%] [G loss: 5.544679]\n",
      "epoch:38 step:29684 [D loss: 0.212798, acc.: 92.19%] [G loss: 4.820653]\n",
      "epoch:38 step:29685 [D loss: 0.007098, acc.: 100.00%] [G loss: 5.085601]\n",
      "epoch:38 step:29686 [D loss: 0.014066, acc.: 100.00%] [G loss: 5.642242]\n",
      "epoch:38 step:29687 [D loss: 0.004701, acc.: 100.00%] [G loss: 5.406637]\n",
      "epoch:38 step:29688 [D loss: 0.002292, acc.: 100.00%] [G loss: 5.619779]\n",
      "epoch:38 step:29689 [D loss: 0.011179, acc.: 100.00%] [G loss: 5.243786]\n",
      "epoch:38 step:29690 [D loss: 0.005926, acc.: 100.00%] [G loss: 4.748151]\n",
      "epoch:38 step:29691 [D loss: 0.005191, acc.: 100.00%] [G loss: 5.411427]\n",
      "epoch:38 step:29692 [D loss: 0.017427, acc.: 99.22%] [G loss: 3.991815]\n",
      "epoch:38 step:29693 [D loss: 0.002816, acc.: 100.00%] [G loss: 4.889408]\n",
      "epoch:38 step:29694 [D loss: 0.003928, acc.: 100.00%] [G loss: 4.406095]\n",
      "epoch:38 step:29695 [D loss: 0.011097, acc.: 100.00%] [G loss: 5.244012]\n",
      "epoch:38 step:29696 [D loss: 0.017582, acc.: 100.00%] [G loss: 5.115786]\n",
      "epoch:38 step:29697 [D loss: 0.004678, acc.: 100.00%] [G loss: 5.379488]\n",
      "epoch:38 step:29698 [D loss: 0.010941, acc.: 100.00%] [G loss: 5.954371]\n",
      "epoch:38 step:29699 [D loss: 0.004109, acc.: 100.00%] [G loss: 5.834076]\n",
      "epoch:38 step:29700 [D loss: 0.019043, acc.: 99.22%] [G loss: 5.565536]\n",
      "epoch:38 step:29701 [D loss: 0.038311, acc.: 100.00%] [G loss: 4.041228]\n",
      "epoch:38 step:29702 [D loss: 0.005574, acc.: 100.00%] [G loss: 3.814768]\n",
      "epoch:38 step:29703 [D loss: 0.017250, acc.: 100.00%] [G loss: 3.747941]\n",
      "epoch:38 step:29704 [D loss: 0.025490, acc.: 100.00%] [G loss: 5.119907]\n",
      "epoch:38 step:29705 [D loss: 0.003823, acc.: 100.00%] [G loss: 5.002718]\n",
      "epoch:38 step:29706 [D loss: 0.040181, acc.: 100.00%] [G loss: 4.133338]\n",
      "epoch:38 step:29707 [D loss: 0.051558, acc.: 97.66%] [G loss: 6.138058]\n",
      "epoch:38 step:29708 [D loss: 0.293665, acc.: 88.28%] [G loss: 6.958386]\n",
      "epoch:38 step:29709 [D loss: 0.000505, acc.: 100.00%] [G loss: 8.301482]\n",
      "epoch:38 step:29710 [D loss: 0.005794, acc.: 100.00%] [G loss: 8.462034]\n",
      "epoch:38 step:29711 [D loss: 0.601000, acc.: 68.75%] [G loss: 7.625341]\n",
      "epoch:38 step:29712 [D loss: 0.000386, acc.: 100.00%] [G loss: 8.439528]\n",
      "epoch:38 step:29713 [D loss: 3.355871, acc.: 16.41%] [G loss: 8.158373]\n",
      "epoch:38 step:29714 [D loss: 0.984149, acc.: 64.84%] [G loss: 4.264599]\n",
      "epoch:38 step:29715 [D loss: 0.362829, acc.: 85.94%] [G loss: 5.158882]\n",
      "epoch:38 step:29716 [D loss: 0.044986, acc.: 98.44%] [G loss: 5.407995]\n",
      "epoch:38 step:29717 [D loss: 0.062410, acc.: 97.66%] [G loss: 5.038092]\n",
      "epoch:38 step:29718 [D loss: 0.019172, acc.: 100.00%] [G loss: 3.981333]\n",
      "epoch:38 step:29719 [D loss: 0.334758, acc.: 86.72%] [G loss: 4.058957]\n",
      "epoch:38 step:29720 [D loss: 0.166347, acc.: 94.53%] [G loss: 4.691853]\n",
      "epoch:38 step:29721 [D loss: 0.074924, acc.: 97.66%] [G loss: 3.269110]\n",
      "epoch:38 step:29722 [D loss: 0.089860, acc.: 97.66%] [G loss: 4.330366]\n",
      "epoch:38 step:29723 [D loss: 0.061508, acc.: 98.44%] [G loss: 5.211383]\n",
      "epoch:38 step:29724 [D loss: 0.063995, acc.: 99.22%] [G loss: 4.071588]\n",
      "epoch:38 step:29725 [D loss: 0.111617, acc.: 96.09%] [G loss: 4.057504]\n",
      "epoch:38 step:29726 [D loss: 0.184632, acc.: 95.31%] [G loss: 4.650940]\n",
      "epoch:38 step:29727 [D loss: 1.134558, acc.: 44.53%] [G loss: 7.256403]\n",
      "epoch:38 step:29728 [D loss: 1.496018, acc.: 52.34%] [G loss: 5.726496]\n",
      "epoch:38 step:29729 [D loss: 0.038352, acc.: 98.44%] [G loss: 4.444573]\n",
      "epoch:38 step:29730 [D loss: 0.059888, acc.: 98.44%] [G loss: 4.303959]\n",
      "epoch:38 step:29731 [D loss: 0.014362, acc.: 100.00%] [G loss: 4.157304]\n",
      "epoch:38 step:29732 [D loss: 0.072798, acc.: 97.66%] [G loss: 4.198655]\n",
      "epoch:38 step:29733 [D loss: 0.031160, acc.: 100.00%] [G loss: 4.726177]\n",
      "epoch:38 step:29734 [D loss: 0.110146, acc.: 96.09%] [G loss: 4.766220]\n",
      "epoch:38 step:29735 [D loss: 0.067460, acc.: 97.66%] [G loss: 4.067796]\n",
      "epoch:38 step:29736 [D loss: 0.188166, acc.: 93.75%] [G loss: 2.991157]\n",
      "epoch:38 step:29737 [D loss: 0.090908, acc.: 96.88%] [G loss: 4.404515]\n",
      "epoch:38 step:29738 [D loss: 0.022328, acc.: 100.00%] [G loss: 4.137681]\n",
      "epoch:38 step:29739 [D loss: 0.021261, acc.: 100.00%] [G loss: 4.612491]\n",
      "epoch:38 step:29740 [D loss: 0.082944, acc.: 99.22%] [G loss: 3.138994]\n",
      "epoch:38 step:29741 [D loss: 0.491212, acc.: 78.12%] [G loss: 6.112220]\n",
      "epoch:38 step:29742 [D loss: 0.267903, acc.: 89.84%] [G loss: 6.076857]\n",
      "epoch:38 step:29743 [D loss: 0.113260, acc.: 96.09%] [G loss: 4.448060]\n",
      "epoch:38 step:29744 [D loss: 0.051351, acc.: 100.00%] [G loss: 3.713846]\n",
      "epoch:38 step:29745 [D loss: 0.092873, acc.: 97.66%] [G loss: 4.209042]\n",
      "epoch:38 step:29746 [D loss: 0.146064, acc.: 95.31%] [G loss: 3.687910]\n",
      "epoch:38 step:29747 [D loss: 0.010077, acc.: 100.00%] [G loss: 3.617351]\n",
      "epoch:38 step:29748 [D loss: 0.049606, acc.: 99.22%] [G loss: 3.950522]\n",
      "epoch:38 step:29749 [D loss: 0.062593, acc.: 99.22%] [G loss: 4.063219]\n",
      "epoch:38 step:29750 [D loss: 0.070020, acc.: 99.22%] [G loss: 4.949373]\n",
      "epoch:38 step:29751 [D loss: 0.059137, acc.: 98.44%] [G loss: 3.424283]\n",
      "epoch:38 step:29752 [D loss: 0.039445, acc.: 100.00%] [G loss: 2.830675]\n",
      "epoch:38 step:29753 [D loss: 0.043800, acc.: 100.00%] [G loss: 3.895410]\n",
      "epoch:38 step:29754 [D loss: 0.083233, acc.: 98.44%] [G loss: 2.967775]\n",
      "epoch:38 step:29755 [D loss: 0.027857, acc.: 100.00%] [G loss: 3.292042]\n",
      "epoch:38 step:29756 [D loss: 0.025431, acc.: 100.00%] [G loss: 2.787496]\n",
      "epoch:38 step:29757 [D loss: 0.115654, acc.: 97.66%] [G loss: 4.466107]\n",
      "epoch:38 step:29758 [D loss: 0.179038, acc.: 92.19%] [G loss: 2.260196]\n",
      "epoch:38 step:29759 [D loss: 0.316672, acc.: 82.03%] [G loss: 6.791431]\n",
      "epoch:38 step:29760 [D loss: 0.621537, acc.: 70.31%] [G loss: 4.612439]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:38 step:29761 [D loss: 0.152818, acc.: 95.31%] [G loss: 4.847224]\n",
      "epoch:38 step:29762 [D loss: 0.082322, acc.: 96.88%] [G loss: 4.487669]\n",
      "epoch:38 step:29763 [D loss: 0.012272, acc.: 100.00%] [G loss: 4.465176]\n",
      "epoch:38 step:29764 [D loss: 0.015786, acc.: 100.00%] [G loss: 4.752277]\n",
      "epoch:38 step:29765 [D loss: 0.018620, acc.: 100.00%] [G loss: 4.319990]\n",
      "epoch:38 step:29766 [D loss: 0.122950, acc.: 99.22%] [G loss: 2.201420]\n",
      "epoch:38 step:29767 [D loss: 0.031845, acc.: 100.00%] [G loss: 3.679322]\n",
      "epoch:38 step:29768 [D loss: 0.425991, acc.: 78.12%] [G loss: 6.713857]\n",
      "epoch:38 step:29769 [D loss: 0.271308, acc.: 85.94%] [G loss: 5.929619]\n",
      "epoch:38 step:29770 [D loss: 0.018755, acc.: 100.00%] [G loss: 5.032992]\n",
      "epoch:38 step:29771 [D loss: 0.017120, acc.: 100.00%] [G loss: 5.379081]\n",
      "epoch:38 step:29772 [D loss: 0.020151, acc.: 100.00%] [G loss: 4.551356]\n",
      "epoch:38 step:29773 [D loss: 0.016384, acc.: 100.00%] [G loss: 2.684749]\n",
      "epoch:38 step:29774 [D loss: 0.058943, acc.: 100.00%] [G loss: 5.253446]\n",
      "epoch:38 step:29775 [D loss: 0.056967, acc.: 98.44%] [G loss: 3.848587]\n",
      "epoch:38 step:29776 [D loss: 0.040751, acc.: 100.00%] [G loss: 4.316187]\n",
      "epoch:38 step:29777 [D loss: 0.184830, acc.: 94.53%] [G loss: 5.285150]\n",
      "epoch:38 step:29778 [D loss: 0.012651, acc.: 100.00%] [G loss: 4.697298]\n",
      "epoch:38 step:29779 [D loss: 0.012345, acc.: 100.00%] [G loss: 5.093026]\n",
      "epoch:38 step:29780 [D loss: 0.019038, acc.: 99.22%] [G loss: 4.847719]\n",
      "epoch:38 step:29781 [D loss: 0.020030, acc.: 99.22%] [G loss: 4.620722]\n",
      "epoch:38 step:29782 [D loss: 0.064703, acc.: 99.22%] [G loss: 4.620520]\n",
      "epoch:38 step:29783 [D loss: 0.021792, acc.: 100.00%] [G loss: 3.847744]\n",
      "epoch:38 step:29784 [D loss: 0.014275, acc.: 100.00%] [G loss: 4.761518]\n",
      "epoch:38 step:29785 [D loss: 0.056383, acc.: 98.44%] [G loss: 4.108986]\n",
      "epoch:38 step:29786 [D loss: 0.048865, acc.: 100.00%] [G loss: 3.582973]\n",
      "epoch:38 step:29787 [D loss: 0.013661, acc.: 100.00%] [G loss: 4.666626]\n",
      "epoch:38 step:29788 [D loss: 0.057462, acc.: 99.22%] [G loss: 3.472381]\n",
      "epoch:38 step:29789 [D loss: 0.147511, acc.: 95.31%] [G loss: 5.735907]\n",
      "epoch:38 step:29790 [D loss: 0.174439, acc.: 92.19%] [G loss: 4.766362]\n",
      "epoch:38 step:29791 [D loss: 0.031134, acc.: 100.00%] [G loss: 5.085392]\n",
      "epoch:38 step:29792 [D loss: 0.025856, acc.: 99.22%] [G loss: 4.685317]\n",
      "epoch:38 step:29793 [D loss: 0.023395, acc.: 100.00%] [G loss: 4.428052]\n",
      "epoch:38 step:29794 [D loss: 0.087767, acc.: 97.66%] [G loss: 4.903321]\n",
      "epoch:38 step:29795 [D loss: 0.008199, acc.: 100.00%] [G loss: 5.139527]\n",
      "epoch:38 step:29796 [D loss: 0.004051, acc.: 100.00%] [G loss: 5.021090]\n",
      "epoch:38 step:29797 [D loss: 0.126650, acc.: 96.88%] [G loss: 3.919784]\n",
      "epoch:38 step:29798 [D loss: 0.013867, acc.: 100.00%] [G loss: 4.254473]\n",
      "epoch:38 step:29799 [D loss: 0.035244, acc.: 99.22%] [G loss: 4.090687]\n",
      "epoch:38 step:29800 [D loss: 0.030268, acc.: 98.44%] [G loss: 4.293711]\n",
      "##############\n",
      "[1.05206277 1.01897784 2.10168421 0.96731746 0.84115989 0.99692986\n",
      " 0.97528544 2.11043516 1.07691672 2.11505665]\n",
      "##########\n",
      "epoch:38 step:29801 [D loss: 0.052871, acc.: 99.22%] [G loss: 6.189707]\n",
      "epoch:38 step:29802 [D loss: 0.007706, acc.: 100.00%] [G loss: 6.256454]\n",
      "epoch:38 step:29803 [D loss: 0.259412, acc.: 88.28%] [G loss: 5.170334]\n",
      "epoch:38 step:29804 [D loss: 0.010565, acc.: 100.00%] [G loss: 5.933569]\n",
      "epoch:38 step:29805 [D loss: 0.006519, acc.: 100.00%] [G loss: 6.123161]\n",
      "epoch:38 step:29806 [D loss: 0.028611, acc.: 100.00%] [G loss: 2.764535]\n",
      "epoch:38 step:29807 [D loss: 0.031876, acc.: 100.00%] [G loss: 4.657764]\n",
      "epoch:38 step:29808 [D loss: 0.014834, acc.: 100.00%] [G loss: 4.322177]\n",
      "epoch:38 step:29809 [D loss: 0.011806, acc.: 100.00%] [G loss: 5.139446]\n",
      "epoch:38 step:29810 [D loss: 0.332747, acc.: 85.16%] [G loss: 8.113995]\n",
      "epoch:38 step:29811 [D loss: 0.185045, acc.: 91.41%] [G loss: 6.711553]\n",
      "epoch:38 step:29812 [D loss: 0.011515, acc.: 100.00%] [G loss: 5.840171]\n",
      "epoch:38 step:29813 [D loss: 0.010283, acc.: 100.00%] [G loss: 6.181535]\n",
      "epoch:38 step:29814 [D loss: 0.037488, acc.: 100.00%] [G loss: 6.255030]\n",
      "epoch:38 step:29815 [D loss: 0.002219, acc.: 100.00%] [G loss: 6.598200]\n",
      "epoch:38 step:29816 [D loss: 0.002105, acc.: 100.00%] [G loss: 5.818256]\n",
      "epoch:38 step:29817 [D loss: 0.002360, acc.: 100.00%] [G loss: 4.828883]\n",
      "epoch:38 step:29818 [D loss: 0.075532, acc.: 98.44%] [G loss: 3.726117]\n",
      "epoch:38 step:29819 [D loss: 0.023405, acc.: 99.22%] [G loss: 3.775945]\n",
      "epoch:38 step:29820 [D loss: 0.009321, acc.: 100.00%] [G loss: 5.871502]\n",
      "epoch:38 step:29821 [D loss: 0.011453, acc.: 100.00%] [G loss: 5.025456]\n",
      "epoch:38 step:29822 [D loss: 0.011644, acc.: 100.00%] [G loss: 5.920737]\n",
      "epoch:38 step:29823 [D loss: 0.658324, acc.: 66.41%] [G loss: 8.591700]\n",
      "epoch:38 step:29824 [D loss: 2.584715, acc.: 50.00%] [G loss: 6.330632]\n",
      "epoch:38 step:29825 [D loss: 0.276791, acc.: 85.16%] [G loss: 2.335616]\n",
      "epoch:38 step:29826 [D loss: 1.427588, acc.: 59.38%] [G loss: 7.207996]\n",
      "epoch:38 step:29827 [D loss: 0.568480, acc.: 73.44%] [G loss: 6.824892]\n",
      "epoch:38 step:29828 [D loss: 0.692451, acc.: 70.31%] [G loss: 4.178630]\n",
      "epoch:38 step:29829 [D loss: 0.060003, acc.: 97.66%] [G loss: 3.460560]\n",
      "epoch:38 step:29830 [D loss: 0.012039, acc.: 100.00%] [G loss: 2.794148]\n",
      "epoch:38 step:29831 [D loss: 0.054280, acc.: 99.22%] [G loss: 2.937815]\n",
      "epoch:38 step:29832 [D loss: 0.005704, acc.: 100.00%] [G loss: 3.326535]\n",
      "epoch:38 step:29833 [D loss: 0.004424, acc.: 100.00%] [G loss: 2.928825]\n",
      "epoch:38 step:29834 [D loss: 0.008210, acc.: 100.00%] [G loss: 2.338581]\n",
      "epoch:38 step:29835 [D loss: 0.016952, acc.: 100.00%] [G loss: 2.852282]\n",
      "epoch:38 step:29836 [D loss: 0.035671, acc.: 99.22%] [G loss: 2.098101]\n",
      "epoch:38 step:29837 [D loss: 0.015301, acc.: 100.00%] [G loss: 2.256588]\n",
      "epoch:38 step:29838 [D loss: 0.028028, acc.: 100.00%] [G loss: 1.223862]\n",
      "epoch:38 step:29839 [D loss: 0.050036, acc.: 98.44%] [G loss: 1.472648]\n",
      "epoch:38 step:29840 [D loss: 0.568840, acc.: 70.31%] [G loss: 6.562664]\n",
      "epoch:38 step:29841 [D loss: 1.696239, acc.: 51.56%] [G loss: 5.000008]\n",
      "epoch:38 step:29842 [D loss: 0.057034, acc.: 98.44%] [G loss: 3.988533]\n",
      "epoch:38 step:29843 [D loss: 0.064161, acc.: 97.66%] [G loss: 3.701596]\n",
      "epoch:38 step:29844 [D loss: 0.026692, acc.: 99.22%] [G loss: 2.622495]\n",
      "epoch:38 step:29845 [D loss: 0.023422, acc.: 99.22%] [G loss: 2.889310]\n",
      "epoch:38 step:29846 [D loss: 0.021210, acc.: 100.00%] [G loss: 2.951558]\n",
      "epoch:38 step:29847 [D loss: 0.016259, acc.: 100.00%] [G loss: 3.546407]\n",
      "epoch:38 step:29848 [D loss: 0.023426, acc.: 100.00%] [G loss: 2.758826]\n",
      "epoch:38 step:29849 [D loss: 0.047522, acc.: 99.22%] [G loss: 3.847003]\n",
      "epoch:38 step:29850 [D loss: 0.034358, acc.: 100.00%] [G loss: 4.008244]\n",
      "epoch:38 step:29851 [D loss: 0.033200, acc.: 100.00%] [G loss: 4.548485]\n",
      "epoch:38 step:29852 [D loss: 0.141267, acc.: 96.09%] [G loss: 3.481290]\n",
      "epoch:38 step:29853 [D loss: 0.039863, acc.: 100.00%] [G loss: 3.879298]\n",
      "epoch:38 step:29854 [D loss: 0.030678, acc.: 100.00%] [G loss: 4.661487]\n",
      "epoch:38 step:29855 [D loss: 0.015365, acc.: 100.00%] [G loss: 4.466791]\n",
      "epoch:38 step:29856 [D loss: 0.006464, acc.: 100.00%] [G loss: 4.850282]\n",
      "epoch:38 step:29857 [D loss: 0.163743, acc.: 95.31%] [G loss: 3.794648]\n",
      "epoch:38 step:29858 [D loss: 0.015071, acc.: 100.00%] [G loss: 3.521193]\n",
      "epoch:38 step:29859 [D loss: 0.014490, acc.: 100.00%] [G loss: 4.401148]\n",
      "epoch:38 step:29860 [D loss: 0.010329, acc.: 100.00%] [G loss: 4.219830]\n",
      "epoch:38 step:29861 [D loss: 0.019027, acc.: 100.00%] [G loss: 3.936587]\n",
      "epoch:38 step:29862 [D loss: 0.025611, acc.: 100.00%] [G loss: 4.105421]\n",
      "epoch:38 step:29863 [D loss: 0.011542, acc.: 100.00%] [G loss: 4.316758]\n",
      "epoch:38 step:29864 [D loss: 0.007010, acc.: 100.00%] [G loss: 3.978675]\n",
      "epoch:38 step:29865 [D loss: 0.110783, acc.: 98.44%] [G loss: 2.588917]\n",
      "epoch:38 step:29866 [D loss: 0.016317, acc.: 100.00%] [G loss: 3.751843]\n",
      "epoch:38 step:29867 [D loss: 0.010234, acc.: 100.00%] [G loss: 2.595086]\n",
      "epoch:38 step:29868 [D loss: 0.017273, acc.: 100.00%] [G loss: 2.715070]\n",
      "epoch:38 step:29869 [D loss: 0.023502, acc.: 100.00%] [G loss: 3.214600]\n",
      "epoch:38 step:29870 [D loss: 0.023054, acc.: 100.00%] [G loss: 2.830742]\n",
      "epoch:38 step:29871 [D loss: 0.036338, acc.: 99.22%] [G loss: 2.796547]\n",
      "epoch:38 step:29872 [D loss: 0.069695, acc.: 99.22%] [G loss: 4.545450]\n",
      "epoch:38 step:29873 [D loss: 0.023882, acc.: 99.22%] [G loss: 4.167519]\n",
      "epoch:38 step:29874 [D loss: 0.061457, acc.: 99.22%] [G loss: 1.625317]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:38 step:29875 [D loss: 0.201047, acc.: 89.84%] [G loss: 6.588065]\n",
      "epoch:38 step:29876 [D loss: 0.696373, acc.: 67.19%] [G loss: 4.071032]\n",
      "epoch:38 step:29877 [D loss: 0.010947, acc.: 100.00%] [G loss: 4.800244]\n",
      "epoch:38 step:29878 [D loss: 0.075634, acc.: 98.44%] [G loss: 5.040561]\n",
      "epoch:38 step:29879 [D loss: 0.013142, acc.: 100.00%] [G loss: 4.958283]\n",
      "epoch:38 step:29880 [D loss: 0.912186, acc.: 53.91%] [G loss: 7.111015]\n",
      "epoch:38 step:29881 [D loss: 1.138035, acc.: 57.81%] [G loss: 5.761865]\n",
      "epoch:38 step:29882 [D loss: 0.013419, acc.: 100.00%] [G loss: 5.318715]\n",
      "epoch:38 step:29883 [D loss: 0.009392, acc.: 100.00%] [G loss: 4.251018]\n",
      "epoch:38 step:29884 [D loss: 0.010501, acc.: 100.00%] [G loss: 3.852141]\n",
      "epoch:38 step:29885 [D loss: 0.016343, acc.: 100.00%] [G loss: 4.000795]\n",
      "epoch:38 step:29886 [D loss: 0.016563, acc.: 100.00%] [G loss: 3.111003]\n",
      "epoch:38 step:29887 [D loss: 0.019835, acc.: 100.00%] [G loss: 3.292622]\n",
      "epoch:38 step:29888 [D loss: 0.275435, acc.: 87.50%] [G loss: 4.340215]\n",
      "epoch:38 step:29889 [D loss: 0.005253, acc.: 100.00%] [G loss: 5.936875]\n",
      "epoch:38 step:29890 [D loss: 0.106807, acc.: 96.09%] [G loss: 4.934729]\n",
      "epoch:38 step:29891 [D loss: 0.008980, acc.: 100.00%] [G loss: 3.867486]\n",
      "epoch:38 step:29892 [D loss: 0.162296, acc.: 96.88%] [G loss: 4.168065]\n",
      "epoch:38 step:29893 [D loss: 0.003931, acc.: 100.00%] [G loss: 4.844996]\n",
      "epoch:38 step:29894 [D loss: 0.006264, acc.: 100.00%] [G loss: 4.376153]\n",
      "epoch:38 step:29895 [D loss: 0.005582, acc.: 100.00%] [G loss: 4.345057]\n",
      "epoch:38 step:29896 [D loss: 0.059051, acc.: 98.44%] [G loss: 3.650390]\n",
      "epoch:38 step:29897 [D loss: 0.052529, acc.: 99.22%] [G loss: 3.493690]\n",
      "epoch:38 step:29898 [D loss: 0.018733, acc.: 100.00%] [G loss: 4.171315]\n",
      "epoch:38 step:29899 [D loss: 0.015701, acc.: 100.00%] [G loss: 4.020054]\n",
      "epoch:38 step:29900 [D loss: 0.022815, acc.: 100.00%] [G loss: 4.361976]\n",
      "epoch:38 step:29901 [D loss: 0.016792, acc.: 100.00%] [G loss: 4.183352]\n",
      "epoch:38 step:29902 [D loss: 0.029878, acc.: 99.22%] [G loss: 3.587766]\n",
      "epoch:38 step:29903 [D loss: 0.016418, acc.: 100.00%] [G loss: 3.573628]\n",
      "epoch:38 step:29904 [D loss: 0.022034, acc.: 100.00%] [G loss: 3.268799]\n",
      "epoch:38 step:29905 [D loss: 0.025997, acc.: 100.00%] [G loss: 3.687753]\n",
      "epoch:38 step:29906 [D loss: 0.025073, acc.: 100.00%] [G loss: 3.326184]\n",
      "epoch:38 step:29907 [D loss: 0.166520, acc.: 96.88%] [G loss: 4.819025]\n",
      "epoch:38 step:29908 [D loss: 0.014838, acc.: 100.00%] [G loss: 5.618716]\n",
      "epoch:38 step:29909 [D loss: 0.013207, acc.: 100.00%] [G loss: 5.126607]\n",
      "epoch:38 step:29910 [D loss: 0.080833, acc.: 97.66%] [G loss: 2.934194]\n",
      "epoch:38 step:29911 [D loss: 0.129818, acc.: 96.09%] [G loss: 5.933134]\n",
      "epoch:38 step:29912 [D loss: 0.090295, acc.: 97.66%] [G loss: 5.762631]\n",
      "epoch:38 step:29913 [D loss: 0.003449, acc.: 100.00%] [G loss: 5.118401]\n",
      "epoch:38 step:29914 [D loss: 0.050594, acc.: 99.22%] [G loss: 4.822798]\n",
      "epoch:38 step:29915 [D loss: 0.014716, acc.: 100.00%] [G loss: 4.688159]\n",
      "epoch:38 step:29916 [D loss: 0.015497, acc.: 100.00%] [G loss: 4.661447]\n",
      "epoch:38 step:29917 [D loss: 0.004955, acc.: 100.00%] [G loss: 4.589181]\n",
      "epoch:38 step:29918 [D loss: 0.089506, acc.: 97.66%] [G loss: 3.537175]\n",
      "epoch:38 step:29919 [D loss: 0.034823, acc.: 100.00%] [G loss: 4.299459]\n",
      "epoch:38 step:29920 [D loss: 0.007778, acc.: 100.00%] [G loss: 4.654816]\n",
      "epoch:38 step:29921 [D loss: 0.008005, acc.: 100.00%] [G loss: 4.690121]\n",
      "epoch:38 step:29922 [D loss: 0.008577, acc.: 100.00%] [G loss: 4.604163]\n",
      "epoch:38 step:29923 [D loss: 0.004617, acc.: 100.00%] [G loss: 4.857782]\n",
      "epoch:38 step:29924 [D loss: 0.017805, acc.: 99.22%] [G loss: 4.153711]\n",
      "epoch:38 step:29925 [D loss: 0.057883, acc.: 99.22%] [G loss: 2.253846]\n",
      "epoch:38 step:29926 [D loss: 0.082496, acc.: 99.22%] [G loss: 5.188140]\n",
      "epoch:38 step:29927 [D loss: 0.002520, acc.: 100.00%] [G loss: 5.718668]\n",
      "epoch:38 step:29928 [D loss: 0.333836, acc.: 86.72%] [G loss: 4.046333]\n",
      "epoch:38 step:29929 [D loss: 0.006193, acc.: 100.00%] [G loss: 4.286807]\n",
      "epoch:38 step:29930 [D loss: 0.025603, acc.: 100.00%] [G loss: 4.708697]\n",
      "epoch:38 step:29931 [D loss: 0.006356, acc.: 100.00%] [G loss: 4.192180]\n",
      "epoch:38 step:29932 [D loss: 0.028503, acc.: 100.00%] [G loss: 5.076514]\n",
      "epoch:38 step:29933 [D loss: 0.010222, acc.: 99.22%] [G loss: 5.017162]\n",
      "epoch:38 step:29934 [D loss: 0.023077, acc.: 99.22%] [G loss: 4.623360]\n",
      "epoch:38 step:29935 [D loss: 0.019920, acc.: 100.00%] [G loss: 4.248044]\n",
      "epoch:38 step:29936 [D loss: 0.016043, acc.: 100.00%] [G loss: 5.083368]\n",
      "epoch:38 step:29937 [D loss: 0.004405, acc.: 100.00%] [G loss: 4.644931]\n",
      "epoch:38 step:29938 [D loss: 0.027479, acc.: 98.44%] [G loss: 3.018799]\n",
      "epoch:38 step:29939 [D loss: 0.017600, acc.: 100.00%] [G loss: 2.936379]\n",
      "epoch:38 step:29940 [D loss: 0.003982, acc.: 100.00%] [G loss: 2.383743]\n",
      "epoch:38 step:29941 [D loss: 0.006724, acc.: 100.00%] [G loss: 2.993202]\n",
      "epoch:38 step:29942 [D loss: 0.123623, acc.: 96.88%] [G loss: 4.590814]\n",
      "epoch:38 step:29943 [D loss: 0.014080, acc.: 100.00%] [G loss: 5.793936]\n",
      "epoch:38 step:29944 [D loss: 0.022627, acc.: 99.22%] [G loss: 5.371782]\n",
      "epoch:38 step:29945 [D loss: 0.034726, acc.: 98.44%] [G loss: 3.825801]\n",
      "epoch:38 step:29946 [D loss: 0.244853, acc.: 90.62%] [G loss: 6.936447]\n",
      "epoch:38 step:29947 [D loss: 1.521005, acc.: 53.12%] [G loss: 2.420787]\n",
      "epoch:38 step:29948 [D loss: 0.553378, acc.: 73.44%] [G loss: 6.334445]\n",
      "epoch:38 step:29949 [D loss: 0.330020, acc.: 84.38%] [G loss: 5.058751]\n",
      "epoch:38 step:29950 [D loss: 0.027227, acc.: 98.44%] [G loss: 4.328093]\n",
      "epoch:38 step:29951 [D loss: 0.028850, acc.: 100.00%] [G loss: 4.291130]\n",
      "epoch:38 step:29952 [D loss: 0.017096, acc.: 100.00%] [G loss: 5.224279]\n",
      "epoch:38 step:29953 [D loss: 0.003624, acc.: 100.00%] [G loss: 5.299506]\n",
      "epoch:38 step:29954 [D loss: 0.006235, acc.: 100.00%] [G loss: 5.080674]\n",
      "epoch:38 step:29955 [D loss: 0.013769, acc.: 100.00%] [G loss: 4.817369]\n",
      "epoch:38 step:29956 [D loss: 0.010401, acc.: 100.00%] [G loss: 4.198335]\n",
      "epoch:38 step:29957 [D loss: 0.021860, acc.: 100.00%] [G loss: 4.279472]\n",
      "epoch:38 step:29958 [D loss: 0.006072, acc.: 100.00%] [G loss: 4.760392]\n",
      "epoch:38 step:29959 [D loss: 0.002989, acc.: 100.00%] [G loss: 4.362647]\n",
      "epoch:38 step:29960 [D loss: 0.005180, acc.: 100.00%] [G loss: 4.149779]\n",
      "epoch:38 step:29961 [D loss: 0.008852, acc.: 100.00%] [G loss: 3.918196]\n",
      "epoch:38 step:29962 [D loss: 0.029052, acc.: 99.22%] [G loss: 4.529475]\n",
      "epoch:38 step:29963 [D loss: 0.019815, acc.: 100.00%] [G loss: 4.134761]\n",
      "epoch:38 step:29964 [D loss: 0.134285, acc.: 96.88%] [G loss: 3.595397]\n",
      "epoch:38 step:29965 [D loss: 0.006872, acc.: 100.00%] [G loss: 4.215451]\n",
      "epoch:38 step:29966 [D loss: 0.023922, acc.: 100.00%] [G loss: 3.958980]\n",
      "epoch:38 step:29967 [D loss: 0.055577, acc.: 98.44%] [G loss: 4.470397]\n",
      "epoch:38 step:29968 [D loss: 0.005241, acc.: 100.00%] [G loss: 4.597549]\n",
      "epoch:38 step:29969 [D loss: 0.008559, acc.: 100.00%] [G loss: 4.637263]\n",
      "epoch:38 step:29970 [D loss: 0.031092, acc.: 99.22%] [G loss: 3.789180]\n",
      "epoch:38 step:29971 [D loss: 0.009070, acc.: 100.00%] [G loss: 2.601671]\n",
      "epoch:38 step:29972 [D loss: 0.025116, acc.: 100.00%] [G loss: 2.939571]\n",
      "epoch:38 step:29973 [D loss: 0.036310, acc.: 99.22%] [G loss: 3.024566]\n",
      "epoch:38 step:29974 [D loss: 0.036675, acc.: 99.22%] [G loss: 3.136484]\n",
      "epoch:38 step:29975 [D loss: 0.015238, acc.: 100.00%] [G loss: 4.132707]\n",
      "epoch:38 step:29976 [D loss: 0.014030, acc.: 100.00%] [G loss: 4.928391]\n",
      "epoch:38 step:29977 [D loss: 0.040510, acc.: 97.66%] [G loss: 4.314687]\n",
      "epoch:38 step:29978 [D loss: 0.040266, acc.: 98.44%] [G loss: 4.253865]\n",
      "epoch:38 step:29979 [D loss: 0.048919, acc.: 98.44%] [G loss: 5.001489]\n",
      "epoch:38 step:29980 [D loss: 0.005471, acc.: 100.00%] [G loss: 5.540754]\n",
      "epoch:38 step:29981 [D loss: 0.223454, acc.: 92.97%] [G loss: 3.795472]\n",
      "epoch:38 step:29982 [D loss: 0.004318, acc.: 100.00%] [G loss: 4.724819]\n",
      "epoch:38 step:29983 [D loss: 0.006485, acc.: 100.00%] [G loss: 4.802748]\n",
      "epoch:38 step:29984 [D loss: 0.012561, acc.: 100.00%] [G loss: 4.796289]\n",
      "epoch:38 step:29985 [D loss: 0.005823, acc.: 100.00%] [G loss: 4.581087]\n",
      "epoch:38 step:29986 [D loss: 0.012320, acc.: 100.00%] [G loss: 4.804962]\n",
      "epoch:38 step:29987 [D loss: 0.019386, acc.: 100.00%] [G loss: 4.205685]\n",
      "epoch:38 step:29988 [D loss: 0.036657, acc.: 99.22%] [G loss: 3.833663]\n",
      "epoch:38 step:29989 [D loss: 0.012268, acc.: 100.00%] [G loss: 4.225802]\n",
      "epoch:38 step:29990 [D loss: 0.217106, acc.: 93.75%] [G loss: 6.140203]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:38 step:29991 [D loss: 0.024131, acc.: 99.22%] [G loss: 6.998812]\n",
      "epoch:38 step:29992 [D loss: 0.011258, acc.: 100.00%] [G loss: 7.152670]\n",
      "epoch:38 step:29993 [D loss: 0.123115, acc.: 94.53%] [G loss: 3.247063]\n",
      "epoch:38 step:29994 [D loss: 0.015331, acc.: 100.00%] [G loss: 1.446484]\n",
      "epoch:38 step:29995 [D loss: 0.250669, acc.: 84.38%] [G loss: 7.505544]\n",
      "epoch:38 step:29996 [D loss: 0.047341, acc.: 99.22%] [G loss: 8.030711]\n",
      "epoch:38 step:29997 [D loss: 1.909015, acc.: 48.44%] [G loss: 1.294224]\n",
      "epoch:38 step:29998 [D loss: 0.684473, acc.: 67.19%] [G loss: 7.319175]\n",
      "epoch:38 step:29999 [D loss: 1.951681, acc.: 50.78%] [G loss: 5.690702]\n",
      "epoch:38 step:30000 [D loss: 0.958364, acc.: 53.91%] [G loss: 2.129130]\n",
      "##############\n",
      "[1.00948799 0.91106731 0.98310766 0.97655531 1.03527264 1.00583636\n",
      " 2.10524549 0.81791052 2.10040189 0.90462428]\n",
      "##########\n",
      "epoch:38 step:30001 [D loss: 0.381921, acc.: 81.25%] [G loss: 3.446939]\n",
      "epoch:38 step:30002 [D loss: 0.015460, acc.: 100.00%] [G loss: 4.489135]\n",
      "epoch:38 step:30003 [D loss: 0.063892, acc.: 97.66%] [G loss: 4.125568]\n",
      "epoch:38 step:30004 [D loss: 0.048156, acc.: 99.22%] [G loss: 4.295424]\n",
      "epoch:38 step:30005 [D loss: 0.098597, acc.: 96.09%] [G loss: 3.108995]\n",
      "epoch:38 step:30006 [D loss: 0.107336, acc.: 97.66%] [G loss: 3.561374]\n",
      "epoch:38 step:30007 [D loss: 0.066316, acc.: 99.22%] [G loss: 4.274124]\n",
      "epoch:38 step:30008 [D loss: 0.077064, acc.: 97.66%] [G loss: 3.602164]\n",
      "epoch:38 step:30009 [D loss: 0.254493, acc.: 88.28%] [G loss: 4.728796]\n",
      "epoch:38 step:30010 [D loss: 0.390497, acc.: 81.25%] [G loss: 3.621763]\n",
      "epoch:38 step:30011 [D loss: 0.046430, acc.: 99.22%] [G loss: 2.393948]\n",
      "epoch:38 step:30012 [D loss: 0.120041, acc.: 96.88%] [G loss: 4.464448]\n",
      "epoch:38 step:30013 [D loss: 0.013986, acc.: 100.00%] [G loss: 4.071171]\n",
      "epoch:38 step:30014 [D loss: 0.215992, acc.: 92.19%] [G loss: 3.289279]\n",
      "epoch:38 step:30015 [D loss: 0.074114, acc.: 98.44%] [G loss: 2.927330]\n",
      "epoch:38 step:30016 [D loss: 0.097735, acc.: 98.44%] [G loss: 3.787271]\n",
      "epoch:38 step:30017 [D loss: 0.025719, acc.: 100.00%] [G loss: 3.833862]\n",
      "epoch:38 step:30018 [D loss: 0.571067, acc.: 73.44%] [G loss: 4.884469]\n",
      "epoch:38 step:30019 [D loss: 0.024960, acc.: 100.00%] [G loss: 5.623253]\n",
      "epoch:38 step:30020 [D loss: 0.066158, acc.: 98.44%] [G loss: 5.208886]\n",
      "epoch:38 step:30021 [D loss: 0.087677, acc.: 96.09%] [G loss: 3.853287]\n",
      "epoch:38 step:30022 [D loss: 0.042852, acc.: 99.22%] [G loss: 3.020409]\n",
      "epoch:38 step:30023 [D loss: 0.017301, acc.: 100.00%] [G loss: 4.185184]\n",
      "epoch:38 step:30024 [D loss: 0.029184, acc.: 100.00%] [G loss: 3.991539]\n",
      "epoch:38 step:30025 [D loss: 0.006928, acc.: 100.00%] [G loss: 4.046971]\n",
      "epoch:38 step:30026 [D loss: 0.007692, acc.: 100.00%] [G loss: 4.153754]\n",
      "epoch:38 step:30027 [D loss: 0.010980, acc.: 100.00%] [G loss: 2.891415]\n",
      "epoch:38 step:30028 [D loss: 0.128768, acc.: 97.66%] [G loss: 5.202254]\n",
      "epoch:38 step:30029 [D loss: 0.146079, acc.: 91.41%] [G loss: 3.340615]\n",
      "epoch:38 step:30030 [D loss: 0.020344, acc.: 100.00%] [G loss: 3.083817]\n",
      "epoch:38 step:30031 [D loss: 0.105288, acc.: 99.22%] [G loss: 4.616623]\n",
      "epoch:38 step:30032 [D loss: 0.025320, acc.: 99.22%] [G loss: 5.511126]\n",
      "epoch:38 step:30033 [D loss: 0.056177, acc.: 98.44%] [G loss: 3.948767]\n",
      "epoch:38 step:30034 [D loss: 0.027099, acc.: 100.00%] [G loss: 3.861210]\n",
      "epoch:38 step:30035 [D loss: 0.010765, acc.: 100.00%] [G loss: 4.997309]\n",
      "epoch:38 step:30036 [D loss: 0.016195, acc.: 100.00%] [G loss: 5.756201]\n",
      "epoch:38 step:30037 [D loss: 0.007613, acc.: 100.00%] [G loss: 4.576907]\n",
      "epoch:38 step:30038 [D loss: 0.017897, acc.: 100.00%] [G loss: 4.886177]\n",
      "epoch:38 step:30039 [D loss: 0.012610, acc.: 100.00%] [G loss: 4.880219]\n",
      "epoch:38 step:30040 [D loss: 0.006677, acc.: 100.00%] [G loss: 4.903354]\n",
      "epoch:38 step:30041 [D loss: 0.025687, acc.: 100.00%] [G loss: 4.625477]\n",
      "epoch:38 step:30042 [D loss: 0.340009, acc.: 89.06%] [G loss: 6.580736]\n",
      "epoch:38 step:30043 [D loss: 0.063035, acc.: 98.44%] [G loss: 6.727335]\n",
      "epoch:38 step:30044 [D loss: 0.152030, acc.: 94.53%] [G loss: 5.390658]\n",
      "epoch:38 step:30045 [D loss: 0.016060, acc.: 100.00%] [G loss: 4.578629]\n",
      "epoch:38 step:30046 [D loss: 0.005922, acc.: 100.00%] [G loss: 4.721690]\n",
      "epoch:38 step:30047 [D loss: 0.009413, acc.: 100.00%] [G loss: 3.593340]\n",
      "epoch:38 step:30048 [D loss: 0.047587, acc.: 99.22%] [G loss: 4.340066]\n",
      "epoch:38 step:30049 [D loss: 0.002341, acc.: 100.00%] [G loss: 5.161645]\n",
      "epoch:38 step:30050 [D loss: 0.003078, acc.: 100.00%] [G loss: 4.980485]\n",
      "epoch:38 step:30051 [D loss: 0.003132, acc.: 100.00%] [G loss: 4.749166]\n",
      "epoch:38 step:30052 [D loss: 0.008902, acc.: 100.00%] [G loss: 4.434322]\n",
      "epoch:38 step:30053 [D loss: 0.014388, acc.: 100.00%] [G loss: 3.402596]\n",
      "epoch:38 step:30054 [D loss: 0.019786, acc.: 100.00%] [G loss: 3.221985]\n",
      "epoch:38 step:30055 [D loss: 0.025158, acc.: 100.00%] [G loss: 3.366180]\n",
      "epoch:38 step:30056 [D loss: 0.020671, acc.: 100.00%] [G loss: 4.270585]\n",
      "epoch:38 step:30057 [D loss: 0.006932, acc.: 100.00%] [G loss: 4.627568]\n",
      "epoch:38 step:30058 [D loss: 0.026590, acc.: 100.00%] [G loss: 3.329261]\n",
      "epoch:38 step:30059 [D loss: 0.009804, acc.: 100.00%] [G loss: 3.472989]\n",
      "epoch:38 step:30060 [D loss: 0.028862, acc.: 100.00%] [G loss: 3.680931]\n",
      "epoch:38 step:30061 [D loss: 0.017119, acc.: 99.22%] [G loss: 3.535720]\n",
      "epoch:38 step:30062 [D loss: 0.021540, acc.: 100.00%] [G loss: 3.797001]\n",
      "epoch:38 step:30063 [D loss: 0.870610, acc.: 60.16%] [G loss: 7.744347]\n",
      "epoch:38 step:30064 [D loss: 2.290259, acc.: 50.00%] [G loss: 6.054166]\n",
      "epoch:38 step:30065 [D loss: 0.238076, acc.: 85.94%] [G loss: 4.115046]\n",
      "epoch:38 step:30066 [D loss: 0.040065, acc.: 99.22%] [G loss: 2.919391]\n",
      "epoch:38 step:30067 [D loss: 0.201671, acc.: 90.62%] [G loss: 5.185690]\n",
      "epoch:38 step:30068 [D loss: 0.002348, acc.: 100.00%] [G loss: 5.769348]\n",
      "epoch:38 step:30069 [D loss: 0.130386, acc.: 96.09%] [G loss: 4.353878]\n",
      "epoch:38 step:30070 [D loss: 0.007000, acc.: 100.00%] [G loss: 4.095019]\n",
      "epoch:38 step:30071 [D loss: 0.006201, acc.: 100.00%] [G loss: 3.675426]\n",
      "epoch:38 step:30072 [D loss: 0.020053, acc.: 100.00%] [G loss: 4.286474]\n",
      "epoch:38 step:30073 [D loss: 0.006806, acc.: 100.00%] [G loss: 4.133195]\n",
      "epoch:38 step:30074 [D loss: 0.021604, acc.: 100.00%] [G loss: 4.293883]\n",
      "epoch:38 step:30075 [D loss: 0.011486, acc.: 100.00%] [G loss: 4.383194]\n",
      "epoch:38 step:30076 [D loss: 0.013668, acc.: 100.00%] [G loss: 4.486269]\n",
      "epoch:38 step:30077 [D loss: 0.014168, acc.: 100.00%] [G loss: 4.388180]\n",
      "epoch:38 step:30078 [D loss: 0.010630, acc.: 100.00%] [G loss: 4.250453]\n",
      "epoch:38 step:30079 [D loss: 0.019544, acc.: 100.00%] [G loss: 4.153466]\n",
      "epoch:38 step:30080 [D loss: 0.015723, acc.: 100.00%] [G loss: 4.040268]\n",
      "epoch:38 step:30081 [D loss: 0.014635, acc.: 100.00%] [G loss: 3.670704]\n",
      "epoch:38 step:30082 [D loss: 0.011373, acc.: 100.00%] [G loss: 4.204901]\n",
      "epoch:38 step:30083 [D loss: 0.037194, acc.: 99.22%] [G loss: 3.547083]\n",
      "epoch:38 step:30084 [D loss: 0.015113, acc.: 100.00%] [G loss: 3.947453]\n",
      "epoch:38 step:30085 [D loss: 0.020900, acc.: 100.00%] [G loss: 3.831934]\n",
      "epoch:38 step:30086 [D loss: 0.008916, acc.: 100.00%] [G loss: 4.066264]\n",
      "epoch:38 step:30087 [D loss: 0.015369, acc.: 100.00%] [G loss: 4.959553]\n",
      "epoch:38 step:30088 [D loss: 0.014036, acc.: 100.00%] [G loss: 4.192057]\n",
      "epoch:38 step:30089 [D loss: 0.011002, acc.: 100.00%] [G loss: 3.470706]\n",
      "epoch:38 step:30090 [D loss: 0.018552, acc.: 100.00%] [G loss: 4.280340]\n",
      "epoch:38 step:30091 [D loss: 0.058633, acc.: 100.00%] [G loss: 2.969473]\n",
      "epoch:38 step:30092 [D loss: 0.138915, acc.: 95.31%] [G loss: 5.399667]\n",
      "epoch:38 step:30093 [D loss: 0.088988, acc.: 98.44%] [G loss: 5.414436]\n",
      "epoch:38 step:30094 [D loss: 0.003225, acc.: 100.00%] [G loss: 6.047065]\n",
      "epoch:38 step:30095 [D loss: 0.017088, acc.: 100.00%] [G loss: 4.551982]\n",
      "epoch:38 step:30096 [D loss: 0.004321, acc.: 100.00%] [G loss: 4.851929]\n",
      "epoch:38 step:30097 [D loss: 0.015217, acc.: 100.00%] [G loss: 4.883450]\n",
      "epoch:38 step:30098 [D loss: 0.065759, acc.: 97.66%] [G loss: 2.945168]\n",
      "epoch:38 step:30099 [D loss: 0.063665, acc.: 100.00%] [G loss: 3.030776]\n",
      "epoch:38 step:30100 [D loss: 0.003725, acc.: 100.00%] [G loss: 3.799297]\n",
      "epoch:38 step:30101 [D loss: 0.006720, acc.: 100.00%] [G loss: 4.322364]\n",
      "epoch:38 step:30102 [D loss: 0.017561, acc.: 100.00%] [G loss: 2.682015]\n",
      "epoch:38 step:30103 [D loss: 0.027715, acc.: 100.00%] [G loss: 3.566876]\n",
      "epoch:38 step:30104 [D loss: 0.003013, acc.: 100.00%] [G loss: 2.620116]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:38 step:30105 [D loss: 0.004622, acc.: 100.00%] [G loss: 2.210177]\n",
      "epoch:38 step:30106 [D loss: 0.047458, acc.: 100.00%] [G loss: 5.538024]\n",
      "epoch:38 step:30107 [D loss: 0.011169, acc.: 100.00%] [G loss: 4.831430]\n",
      "epoch:38 step:30108 [D loss: 0.089192, acc.: 99.22%] [G loss: 4.109964]\n",
      "epoch:38 step:30109 [D loss: 1.647999, acc.: 45.31%] [G loss: 8.172701]\n",
      "epoch:38 step:30110 [D loss: 3.221319, acc.: 50.00%] [G loss: 5.747600]\n",
      "epoch:38 step:30111 [D loss: 2.280979, acc.: 50.00%] [G loss: 4.117307]\n",
      "epoch:38 step:30112 [D loss: 1.520211, acc.: 50.00%] [G loss: 2.709293]\n",
      "epoch:38 step:30113 [D loss: 0.599580, acc.: 69.53%] [G loss: 1.659928]\n",
      "epoch:38 step:30114 [D loss: 0.220352, acc.: 94.53%] [G loss: 2.139646]\n",
      "epoch:38 step:30115 [D loss: 0.169610, acc.: 96.09%] [G loss: 2.203816]\n",
      "epoch:38 step:30116 [D loss: 0.266710, acc.: 93.75%] [G loss: 2.339955]\n",
      "epoch:38 step:30117 [D loss: 0.128002, acc.: 96.09%] [G loss: 2.249283]\n",
      "epoch:38 step:30118 [D loss: 0.170028, acc.: 97.66%] [G loss: 2.111804]\n",
      "epoch:38 step:30119 [D loss: 0.397794, acc.: 82.81%] [G loss: 2.808561]\n",
      "epoch:38 step:30120 [D loss: 0.425181, acc.: 78.12%] [G loss: 2.190197]\n",
      "epoch:38 step:30121 [D loss: 0.170008, acc.: 98.44%] [G loss: 2.391500]\n",
      "epoch:38 step:30122 [D loss: 0.136867, acc.: 96.88%] [G loss: 2.721714]\n",
      "epoch:38 step:30123 [D loss: 0.406288, acc.: 78.91%] [G loss: 3.377155]\n",
      "epoch:38 step:30124 [D loss: 0.311141, acc.: 88.28%] [G loss: 2.532345]\n",
      "epoch:38 step:30125 [D loss: 0.162967, acc.: 96.09%] [G loss: 2.862903]\n",
      "epoch:38 step:30126 [D loss: 0.105454, acc.: 98.44%] [G loss: 3.372261]\n",
      "epoch:38 step:30127 [D loss: 0.080444, acc.: 100.00%] [G loss: 3.104237]\n",
      "epoch:38 step:30128 [D loss: 0.132859, acc.: 96.88%] [G loss: 2.527364]\n",
      "epoch:38 step:30129 [D loss: 0.084061, acc.: 99.22%] [G loss: 3.039855]\n",
      "epoch:38 step:30130 [D loss: 0.078389, acc.: 99.22%] [G loss: 2.837596]\n",
      "epoch:38 step:30131 [D loss: 0.130168, acc.: 97.66%] [G loss: 3.262913]\n",
      "epoch:38 step:30132 [D loss: 0.187422, acc.: 94.53%] [G loss: 3.493528]\n",
      "epoch:38 step:30133 [D loss: 0.034708, acc.: 99.22%] [G loss: 3.911698]\n",
      "epoch:38 step:30134 [D loss: 0.085666, acc.: 97.66%] [G loss: 2.609390]\n",
      "epoch:38 step:30135 [D loss: 0.073536, acc.: 100.00%] [G loss: 3.119882]\n",
      "epoch:38 step:30136 [D loss: 0.036415, acc.: 100.00%] [G loss: 2.939416]\n",
      "epoch:38 step:30137 [D loss: 0.133259, acc.: 95.31%] [G loss: 1.963532]\n",
      "epoch:38 step:30138 [D loss: 0.240265, acc.: 88.28%] [G loss: 4.194668]\n",
      "epoch:38 step:30139 [D loss: 0.017852, acc.: 100.00%] [G loss: 5.106337]\n",
      "epoch:38 step:30140 [D loss: 0.194617, acc.: 90.62%] [G loss: 3.890322]\n",
      "epoch:38 step:30141 [D loss: 0.132516, acc.: 96.88%] [G loss: 2.373940]\n",
      "epoch:38 step:30142 [D loss: 0.060310, acc.: 99.22%] [G loss: 0.904230]\n",
      "epoch:38 step:30143 [D loss: 0.197744, acc.: 96.88%] [G loss: 3.777104]\n",
      "epoch:38 step:30144 [D loss: 0.082370, acc.: 97.66%] [G loss: 3.534634]\n",
      "epoch:38 step:30145 [D loss: 0.433743, acc.: 82.03%] [G loss: 4.588577]\n",
      "epoch:38 step:30146 [D loss: 0.170866, acc.: 92.19%] [G loss: 3.115732]\n",
      "epoch:38 step:30147 [D loss: 0.053684, acc.: 98.44%] [G loss: 1.914849]\n",
      "epoch:38 step:30148 [D loss: 0.126536, acc.: 92.97%] [G loss: 4.175598]\n",
      "epoch:38 step:30149 [D loss: 0.048580, acc.: 98.44%] [G loss: 3.947590]\n",
      "epoch:38 step:30150 [D loss: 0.044407, acc.: 99.22%] [G loss: 2.618204]\n",
      "epoch:38 step:30151 [D loss: 0.045781, acc.: 100.00%] [G loss: 2.242154]\n",
      "epoch:38 step:30152 [D loss: 0.084289, acc.: 99.22%] [G loss: 3.536929]\n",
      "epoch:38 step:30153 [D loss: 0.421341, acc.: 87.50%] [G loss: 3.314456]\n",
      "epoch:38 step:30154 [D loss: 0.037992, acc.: 100.00%] [G loss: 4.126405]\n",
      "epoch:38 step:30155 [D loss: 0.024336, acc.: 100.00%] [G loss: 3.611765]\n",
      "epoch:38 step:30156 [D loss: 0.045209, acc.: 99.22%] [G loss: 3.443716]\n",
      "epoch:38 step:30157 [D loss: 0.028657, acc.: 100.00%] [G loss: 3.087887]\n",
      "epoch:38 step:30158 [D loss: 0.081859, acc.: 99.22%] [G loss: 3.901801]\n",
      "epoch:38 step:30159 [D loss: 0.075179, acc.: 98.44%] [G loss: 3.783848]\n",
      "epoch:38 step:30160 [D loss: 0.160740, acc.: 96.88%] [G loss: 3.268373]\n",
      "epoch:38 step:30161 [D loss: 0.038265, acc.: 100.00%] [G loss: 3.875757]\n",
      "epoch:38 step:30162 [D loss: 0.041785, acc.: 99.22%] [G loss: 3.436040]\n",
      "epoch:38 step:30163 [D loss: 0.062054, acc.: 99.22%] [G loss: 2.764453]\n",
      "epoch:38 step:30164 [D loss: 0.134126, acc.: 97.66%] [G loss: 4.756567]\n",
      "epoch:38 step:30165 [D loss: 0.040991, acc.: 99.22%] [G loss: 5.075363]\n",
      "epoch:38 step:30166 [D loss: 1.906149, acc.: 29.69%] [G loss: 6.353573]\n",
      "epoch:38 step:30167 [D loss: 0.879411, acc.: 67.19%] [G loss: 5.898378]\n",
      "epoch:38 step:30168 [D loss: 0.149297, acc.: 92.97%] [G loss: 5.040398]\n",
      "epoch:38 step:30169 [D loss: 0.042123, acc.: 99.22%] [G loss: 3.977029]\n",
      "epoch:38 step:30170 [D loss: 0.033198, acc.: 100.00%] [G loss: 4.232218]\n",
      "epoch:38 step:30171 [D loss: 0.025991, acc.: 100.00%] [G loss: 4.051156]\n",
      "epoch:38 step:30172 [D loss: 0.049189, acc.: 100.00%] [G loss: 3.831440]\n",
      "epoch:38 step:30173 [D loss: 0.044166, acc.: 100.00%] [G loss: 3.840052]\n",
      "epoch:38 step:30174 [D loss: 0.013058, acc.: 100.00%] [G loss: 4.209033]\n",
      "epoch:38 step:30175 [D loss: 0.053947, acc.: 99.22%] [G loss: 3.881304]\n",
      "epoch:38 step:30176 [D loss: 0.038863, acc.: 100.00%] [G loss: 3.529408]\n",
      "epoch:38 step:30177 [D loss: 0.035119, acc.: 99.22%] [G loss: 3.319008]\n",
      "epoch:38 step:30178 [D loss: 0.055789, acc.: 100.00%] [G loss: 3.493425]\n",
      "epoch:38 step:30179 [D loss: 0.047632, acc.: 98.44%] [G loss: 2.905470]\n",
      "epoch:38 step:30180 [D loss: 0.016347, acc.: 100.00%] [G loss: 3.127169]\n",
      "epoch:38 step:30181 [D loss: 0.012206, acc.: 100.00%] [G loss: 2.791979]\n",
      "epoch:38 step:30182 [D loss: 0.060179, acc.: 100.00%] [G loss: 2.988698]\n",
      "epoch:38 step:30183 [D loss: 0.015158, acc.: 100.00%] [G loss: 3.054557]\n",
      "epoch:38 step:30184 [D loss: 0.047622, acc.: 100.00%] [G loss: 3.906678]\n",
      "epoch:38 step:30185 [D loss: 0.051074, acc.: 99.22%] [G loss: 3.926387]\n",
      "epoch:38 step:30186 [D loss: 0.114722, acc.: 96.88%] [G loss: 1.458336]\n",
      "epoch:38 step:30187 [D loss: 0.503625, acc.: 73.44%] [G loss: 5.947787]\n",
      "epoch:38 step:30188 [D loss: 0.956275, acc.: 57.81%] [G loss: 4.363406]\n",
      "epoch:38 step:30189 [D loss: 0.049318, acc.: 98.44%] [G loss: 2.832419]\n",
      "epoch:38 step:30190 [D loss: 0.016229, acc.: 100.00%] [G loss: 2.235631]\n",
      "epoch:38 step:30191 [D loss: 0.123939, acc.: 96.88%] [G loss: 4.349996]\n",
      "epoch:38 step:30192 [D loss: 0.023022, acc.: 100.00%] [G loss: 3.932912]\n",
      "epoch:38 step:30193 [D loss: 0.034855, acc.: 99.22%] [G loss: 3.395500]\n",
      "epoch:38 step:30194 [D loss: 0.049113, acc.: 99.22%] [G loss: 3.194165]\n",
      "epoch:38 step:30195 [D loss: 0.037893, acc.: 100.00%] [G loss: 3.437805]\n",
      "epoch:38 step:30196 [D loss: 0.042109, acc.: 100.00%] [G loss: 4.096250]\n",
      "epoch:38 step:30197 [D loss: 0.082531, acc.: 100.00%] [G loss: 3.107177]\n",
      "epoch:38 step:30198 [D loss: 0.040536, acc.: 100.00%] [G loss: 2.866340]\n",
      "epoch:38 step:30199 [D loss: 0.035749, acc.: 100.00%] [G loss: 2.085313]\n",
      "epoch:38 step:30200 [D loss: 0.072043, acc.: 100.00%] [G loss: 2.317016]\n",
      "##############\n",
      "[0.96236144 0.97016706 1.08443006 1.03480453 0.81277894 0.85523651\n",
      " 1.1128096  1.10698305 2.10680307 2.11078611]\n",
      "##########\n",
      "epoch:38 step:30201 [D loss: 0.125363, acc.: 98.44%] [G loss: 3.011685]\n",
      "epoch:38 step:30202 [D loss: 0.048356, acc.: 99.22%] [G loss: 3.334327]\n",
      "epoch:38 step:30203 [D loss: 0.039699, acc.: 100.00%] [G loss: 3.080543]\n",
      "epoch:38 step:30204 [D loss: 0.070957, acc.: 96.88%] [G loss: 1.416368]\n",
      "epoch:38 step:30205 [D loss: 0.111256, acc.: 99.22%] [G loss: 1.932106]\n",
      "epoch:38 step:30206 [D loss: 0.032182, acc.: 100.00%] [G loss: 1.915204]\n",
      "epoch:38 step:30207 [D loss: 0.017755, acc.: 100.00%] [G loss: 1.649533]\n",
      "epoch:38 step:30208 [D loss: 0.057456, acc.: 99.22%] [G loss: 1.184211]\n",
      "epoch:38 step:30209 [D loss: 0.023311, acc.: 99.22%] [G loss: 0.799514]\n",
      "epoch:38 step:30210 [D loss: 0.018888, acc.: 100.00%] [G loss: 0.426871]\n",
      "epoch:38 step:30211 [D loss: 0.439823, acc.: 79.69%] [G loss: 6.485714]\n",
      "epoch:38 step:30212 [D loss: 0.696664, acc.: 66.41%] [G loss: 4.083267]\n",
      "epoch:38 step:30213 [D loss: 0.030301, acc.: 99.22%] [G loss: 2.647645]\n",
      "epoch:38 step:30214 [D loss: 0.050245, acc.: 100.00%] [G loss: 3.307817]\n",
      "epoch:38 step:30215 [D loss: 0.079092, acc.: 97.66%] [G loss: 4.118655]\n",
      "epoch:38 step:30216 [D loss: 0.027993, acc.: 100.00%] [G loss: 4.457813]\n",
      "epoch:38 step:30217 [D loss: 0.028452, acc.: 100.00%] [G loss: 4.924443]\n",
      "epoch:38 step:30218 [D loss: 0.026057, acc.: 100.00%] [G loss: 4.572491]\n",
      "epoch:38 step:30219 [D loss: 0.021867, acc.: 100.00%] [G loss: 4.923740]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:38 step:30220 [D loss: 0.049315, acc.: 98.44%] [G loss: 3.120673]\n",
      "epoch:38 step:30221 [D loss: 0.056029, acc.: 100.00%] [G loss: 3.709836]\n",
      "epoch:38 step:30222 [D loss: 0.026182, acc.: 100.00%] [G loss: 4.482940]\n",
      "epoch:38 step:30223 [D loss: 0.016234, acc.: 100.00%] [G loss: 3.493274]\n",
      "epoch:38 step:30224 [D loss: 0.060880, acc.: 99.22%] [G loss: 4.227908]\n",
      "epoch:38 step:30225 [D loss: 0.123577, acc.: 97.66%] [G loss: 4.549422]\n",
      "epoch:38 step:30226 [D loss: 0.020159, acc.: 100.00%] [G loss: 5.049610]\n",
      "epoch:38 step:30227 [D loss: 0.011555, acc.: 100.00%] [G loss: 4.333869]\n",
      "epoch:38 step:30228 [D loss: 0.125147, acc.: 96.88%] [G loss: 4.177107]\n",
      "epoch:38 step:30229 [D loss: 0.027656, acc.: 100.00%] [G loss: 3.983351]\n",
      "epoch:38 step:30230 [D loss: 0.012337, acc.: 100.00%] [G loss: 4.759301]\n",
      "epoch:38 step:30231 [D loss: 0.056605, acc.: 100.00%] [G loss: 3.862827]\n",
      "epoch:38 step:30232 [D loss: 0.058104, acc.: 100.00%] [G loss: 5.105873]\n",
      "epoch:38 step:30233 [D loss: 0.006494, acc.: 100.00%] [G loss: 4.887865]\n",
      "epoch:38 step:30234 [D loss: 0.031926, acc.: 99.22%] [G loss: 3.654473]\n",
      "epoch:38 step:30235 [D loss: 0.075212, acc.: 99.22%] [G loss: 3.704664]\n",
      "epoch:38 step:30236 [D loss: 0.023022, acc.: 100.00%] [G loss: 3.608873]\n",
      "epoch:38 step:30237 [D loss: 0.059536, acc.: 100.00%] [G loss: 5.696222]\n",
      "epoch:38 step:30238 [D loss: 0.020198, acc.: 100.00%] [G loss: 5.352267]\n",
      "epoch:38 step:30239 [D loss: 0.011850, acc.: 100.00%] [G loss: 5.196377]\n",
      "epoch:38 step:30240 [D loss: 0.049291, acc.: 99.22%] [G loss: 3.779350]\n",
      "epoch:38 step:30241 [D loss: 0.017993, acc.: 100.00%] [G loss: 3.385935]\n",
      "epoch:38 step:30242 [D loss: 0.004240, acc.: 100.00%] [G loss: 3.265203]\n",
      "epoch:38 step:30243 [D loss: 0.034664, acc.: 100.00%] [G loss: 3.838369]\n",
      "epoch:38 step:30244 [D loss: 0.084097, acc.: 98.44%] [G loss: 3.929259]\n",
      "epoch:38 step:30245 [D loss: 0.010453, acc.: 100.00%] [G loss: 4.514671]\n",
      "epoch:38 step:30246 [D loss: 0.029213, acc.: 99.22%] [G loss: 3.503782]\n",
      "epoch:38 step:30247 [D loss: 0.006367, acc.: 100.00%] [G loss: 3.154030]\n",
      "epoch:38 step:30248 [D loss: 0.079160, acc.: 96.88%] [G loss: 5.138639]\n",
      "epoch:38 step:30249 [D loss: 0.383393, acc.: 82.03%] [G loss: 5.864632]\n",
      "epoch:38 step:30250 [D loss: 0.001534, acc.: 100.00%] [G loss: 7.856380]\n",
      "epoch:38 step:30251 [D loss: 0.322493, acc.: 87.50%] [G loss: 2.238335]\n",
      "epoch:38 step:30252 [D loss: 0.765150, acc.: 66.41%] [G loss: 8.412530]\n",
      "epoch:38 step:30253 [D loss: 2.705243, acc.: 50.00%] [G loss: 6.586575]\n",
      "epoch:38 step:30254 [D loss: 1.903012, acc.: 50.00%] [G loss: 4.345797]\n",
      "epoch:38 step:30255 [D loss: 0.591051, acc.: 71.09%] [G loss: 2.281606]\n",
      "epoch:38 step:30256 [D loss: 0.213146, acc.: 89.84%] [G loss: 2.630668]\n",
      "epoch:38 step:30257 [D loss: 0.057730, acc.: 100.00%] [G loss: 3.392362]\n",
      "epoch:38 step:30258 [D loss: 0.056307, acc.: 99.22%] [G loss: 3.412167]\n",
      "epoch:38 step:30259 [D loss: 0.037650, acc.: 100.00%] [G loss: 3.076778]\n",
      "epoch:38 step:30260 [D loss: 0.066039, acc.: 100.00%] [G loss: 3.095218]\n",
      "epoch:38 step:30261 [D loss: 0.024823, acc.: 100.00%] [G loss: 3.293396]\n",
      "epoch:38 step:30262 [D loss: 0.050757, acc.: 98.44%] [G loss: 3.052552]\n",
      "epoch:38 step:30263 [D loss: 0.047825, acc.: 100.00%] [G loss: 3.307393]\n",
      "epoch:38 step:30264 [D loss: 0.050467, acc.: 100.00%] [G loss: 3.368532]\n",
      "epoch:38 step:30265 [D loss: 0.021470, acc.: 100.00%] [G loss: 3.279110]\n",
      "epoch:38 step:30266 [D loss: 0.056129, acc.: 100.00%] [G loss: 2.979385]\n",
      "epoch:38 step:30267 [D loss: 0.032918, acc.: 100.00%] [G loss: 4.037396]\n",
      "epoch:38 step:30268 [D loss: 0.046939, acc.: 100.00%] [G loss: 3.565945]\n",
      "epoch:38 step:30269 [D loss: 0.054283, acc.: 100.00%] [G loss: 3.198524]\n",
      "epoch:38 step:30270 [D loss: 0.048960, acc.: 100.00%] [G loss: 3.820809]\n",
      "epoch:38 step:30271 [D loss: 0.034030, acc.: 100.00%] [G loss: 3.401622]\n",
      "epoch:38 step:30272 [D loss: 0.039047, acc.: 99.22%] [G loss: 3.512768]\n",
      "epoch:38 step:30273 [D loss: 0.044836, acc.: 100.00%] [G loss: 3.450457]\n",
      "epoch:38 step:30274 [D loss: 0.032609, acc.: 100.00%] [G loss: 3.288347]\n",
      "epoch:38 step:30275 [D loss: 0.010487, acc.: 100.00%] [G loss: 3.385705]\n",
      "epoch:38 step:30276 [D loss: 0.036629, acc.: 100.00%] [G loss: 3.535286]\n",
      "epoch:38 step:30277 [D loss: 0.037843, acc.: 100.00%] [G loss: 3.857215]\n",
      "epoch:38 step:30278 [D loss: 0.015786, acc.: 100.00%] [G loss: 3.036082]\n",
      "epoch:38 step:30279 [D loss: 0.045076, acc.: 100.00%] [G loss: 3.246266]\n",
      "epoch:38 step:30280 [D loss: 0.030322, acc.: 100.00%] [G loss: 3.378983]\n",
      "epoch:38 step:30281 [D loss: 0.012902, acc.: 100.00%] [G loss: 3.750052]\n",
      "epoch:38 step:30282 [D loss: 0.285651, acc.: 90.62%] [G loss: 5.314307]\n",
      "epoch:38 step:30283 [D loss: 0.004583, acc.: 100.00%] [G loss: 5.904215]\n",
      "epoch:38 step:30284 [D loss: 0.629443, acc.: 67.97%] [G loss: 3.631751]\n",
      "epoch:38 step:30285 [D loss: 0.124135, acc.: 96.88%] [G loss: 3.709167]\n",
      "epoch:38 step:30286 [D loss: 0.031174, acc.: 100.00%] [G loss: 4.652634]\n",
      "epoch:38 step:30287 [D loss: 0.013041, acc.: 100.00%] [G loss: 4.600930]\n",
      "epoch:38 step:30288 [D loss: 0.033019, acc.: 100.00%] [G loss: 3.727813]\n",
      "epoch:38 step:30289 [D loss: 0.016194, acc.: 100.00%] [G loss: 3.922532]\n",
      "epoch:38 step:30290 [D loss: 0.013332, acc.: 100.00%] [G loss: 3.751203]\n",
      "epoch:38 step:30291 [D loss: 0.029147, acc.: 100.00%] [G loss: 3.199281]\n",
      "epoch:38 step:30292 [D loss: 0.027842, acc.: 100.00%] [G loss: 3.727518]\n",
      "epoch:38 step:30293 [D loss: 0.013659, acc.: 100.00%] [G loss: 3.877209]\n",
      "epoch:38 step:30294 [D loss: 0.008926, acc.: 100.00%] [G loss: 4.461618]\n",
      "epoch:38 step:30295 [D loss: 0.013143, acc.: 100.00%] [G loss: 3.862545]\n",
      "epoch:38 step:30296 [D loss: 0.026868, acc.: 100.00%] [G loss: 2.977550]\n",
      "epoch:38 step:30297 [D loss: 0.013901, acc.: 100.00%] [G loss: 3.702562]\n",
      "epoch:38 step:30298 [D loss: 0.042505, acc.: 99.22%] [G loss: 4.041417]\n",
      "epoch:38 step:30299 [D loss: 0.029362, acc.: 100.00%] [G loss: 3.641492]\n",
      "epoch:38 step:30300 [D loss: 0.009420, acc.: 100.00%] [G loss: 4.164698]\n",
      "epoch:38 step:30301 [D loss: 0.045320, acc.: 100.00%] [G loss: 4.087803]\n",
      "epoch:38 step:30302 [D loss: 0.010944, acc.: 100.00%] [G loss: 3.497785]\n",
      "epoch:38 step:30303 [D loss: 0.028844, acc.: 100.00%] [G loss: 3.929289]\n",
      "epoch:38 step:30304 [D loss: 0.023624, acc.: 100.00%] [G loss: 3.656879]\n",
      "epoch:38 step:30305 [D loss: 0.064239, acc.: 100.00%] [G loss: 3.265446]\n",
      "epoch:38 step:30306 [D loss: 0.043504, acc.: 99.22%] [G loss: 3.413587]\n",
      "epoch:38 step:30307 [D loss: 0.013918, acc.: 100.00%] [G loss: 3.922627]\n",
      "epoch:38 step:30308 [D loss: 0.032515, acc.: 100.00%] [G loss: 3.893476]\n",
      "epoch:38 step:30309 [D loss: 0.016872, acc.: 100.00%] [G loss: 4.041941]\n",
      "epoch:38 step:30310 [D loss: 0.008955, acc.: 100.00%] [G loss: 4.440647]\n",
      "epoch:38 step:30311 [D loss: 0.013349, acc.: 100.00%] [G loss: 3.812611]\n",
      "epoch:38 step:30312 [D loss: 0.021496, acc.: 100.00%] [G loss: 4.681707]\n",
      "epoch:38 step:30313 [D loss: 0.014033, acc.: 100.00%] [G loss: 4.421084]\n",
      "epoch:38 step:30314 [D loss: 0.028721, acc.: 100.00%] [G loss: 3.951927]\n",
      "epoch:38 step:30315 [D loss: 0.049233, acc.: 99.22%] [G loss: 3.264664]\n",
      "epoch:38 step:30316 [D loss: 0.022005, acc.: 100.00%] [G loss: 4.175207]\n",
      "epoch:38 step:30317 [D loss: 0.013115, acc.: 100.00%] [G loss: 4.692286]\n",
      "epoch:38 step:30318 [D loss: 0.011797, acc.: 100.00%] [G loss: 4.325876]\n",
      "epoch:38 step:30319 [D loss: 0.008979, acc.: 100.00%] [G loss: 4.613383]\n",
      "epoch:38 step:30320 [D loss: 0.107403, acc.: 96.09%] [G loss: 3.303398]\n",
      "epoch:38 step:30321 [D loss: 0.057584, acc.: 100.00%] [G loss: 4.236255]\n",
      "epoch:38 step:30322 [D loss: 0.008265, acc.: 100.00%] [G loss: 5.161781]\n",
      "epoch:38 step:30323 [D loss: 0.005957, acc.: 100.00%] [G loss: 4.762005]\n",
      "epoch:38 step:30324 [D loss: 0.450640, acc.: 79.69%] [G loss: 5.600037]\n",
      "epoch:38 step:30325 [D loss: 0.001417, acc.: 100.00%] [G loss: 6.484141]\n",
      "epoch:38 step:30326 [D loss: 0.039027, acc.: 97.66%] [G loss: 6.236886]\n",
      "epoch:38 step:30327 [D loss: 0.003122, acc.: 100.00%] [G loss: 6.021304]\n",
      "epoch:38 step:30328 [D loss: 0.008861, acc.: 100.00%] [G loss: 6.282310]\n",
      "epoch:38 step:30329 [D loss: 0.002100, acc.: 100.00%] [G loss: 5.563808]\n",
      "epoch:38 step:30330 [D loss: 0.007464, acc.: 100.00%] [G loss: 5.329723]\n",
      "epoch:38 step:30331 [D loss: 0.004304, acc.: 100.00%] [G loss: 5.485507]\n",
      "epoch:38 step:30332 [D loss: 0.014780, acc.: 100.00%] [G loss: 5.590413]\n",
      "epoch:38 step:30333 [D loss: 0.006766, acc.: 100.00%] [G loss: 5.504749]\n",
      "epoch:38 step:30334 [D loss: 0.006860, acc.: 100.00%] [G loss: 5.456069]\n",
      "epoch:38 step:30335 [D loss: 0.005510, acc.: 100.00%] [G loss: 5.297509]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:38 step:30336 [D loss: 0.008851, acc.: 100.00%] [G loss: 4.912627]\n",
      "epoch:38 step:30337 [D loss: 0.006369, acc.: 100.00%] [G loss: 5.569351]\n",
      "epoch:38 step:30338 [D loss: 0.006480, acc.: 100.00%] [G loss: 5.163990]\n",
      "epoch:38 step:30339 [D loss: 0.004461, acc.: 100.00%] [G loss: 5.653509]\n",
      "epoch:38 step:30340 [D loss: 0.005152, acc.: 100.00%] [G loss: 4.662452]\n",
      "epoch:38 step:30341 [D loss: 0.008152, acc.: 100.00%] [G loss: 5.689460]\n",
      "epoch:38 step:30342 [D loss: 0.010034, acc.: 100.00%] [G loss: 5.310538]\n",
      "epoch:38 step:30343 [D loss: 0.015687, acc.: 100.00%] [G loss: 5.000862]\n",
      "epoch:38 step:30344 [D loss: 0.010179, acc.: 100.00%] [G loss: 5.031066]\n",
      "epoch:38 step:30345 [D loss: 0.012579, acc.: 100.00%] [G loss: 5.235566]\n",
      "epoch:38 step:30346 [D loss: 0.009824, acc.: 100.00%] [G loss: 4.798160]\n",
      "epoch:38 step:30347 [D loss: 0.008451, acc.: 100.00%] [G loss: 4.719520]\n",
      "epoch:38 step:30348 [D loss: 0.007856, acc.: 100.00%] [G loss: 4.135582]\n",
      "epoch:38 step:30349 [D loss: 0.009066, acc.: 100.00%] [G loss: 4.045730]\n",
      "epoch:38 step:30350 [D loss: 0.021405, acc.: 100.00%] [G loss: 4.487707]\n",
      "epoch:38 step:30351 [D loss: 0.017509, acc.: 100.00%] [G loss: 4.877499]\n",
      "epoch:38 step:30352 [D loss: 0.007097, acc.: 100.00%] [G loss: 3.140801]\n",
      "epoch:38 step:30353 [D loss: 0.018314, acc.: 100.00%] [G loss: 3.839679]\n",
      "epoch:38 step:30354 [D loss: 0.043943, acc.: 98.44%] [G loss: 5.953226]\n",
      "epoch:38 step:30355 [D loss: 0.009330, acc.: 99.22%] [G loss: 6.549332]\n",
      "epoch:38 step:30356 [D loss: 0.036195, acc.: 100.00%] [G loss: 4.972889]\n",
      "epoch:38 step:30357 [D loss: 0.012914, acc.: 100.00%] [G loss: 3.687254]\n",
      "epoch:38 step:30358 [D loss: 0.088396, acc.: 98.44%] [G loss: 6.733483]\n",
      "epoch:38 step:30359 [D loss: 0.212265, acc.: 92.19%] [G loss: 6.011868]\n",
      "epoch:38 step:30360 [D loss: 0.003890, acc.: 100.00%] [G loss: 6.045644]\n",
      "epoch:38 step:30361 [D loss: 0.002357, acc.: 100.00%] [G loss: 5.202758]\n",
      "epoch:38 step:30362 [D loss: 0.003175, acc.: 100.00%] [G loss: 6.073178]\n",
      "epoch:38 step:30363 [D loss: 0.070694, acc.: 97.66%] [G loss: 4.872927]\n",
      "epoch:38 step:30364 [D loss: 0.001781, acc.: 100.00%] [G loss: 5.941990]\n",
      "epoch:38 step:30365 [D loss: 0.010079, acc.: 100.00%] [G loss: 6.647724]\n",
      "epoch:38 step:30366 [D loss: 0.019819, acc.: 100.00%] [G loss: 5.610761]\n",
      "epoch:38 step:30367 [D loss: 0.008151, acc.: 100.00%] [G loss: 5.081697]\n",
      "epoch:38 step:30368 [D loss: 0.010696, acc.: 100.00%] [G loss: 4.628596]\n",
      "epoch:38 step:30369 [D loss: 0.013138, acc.: 100.00%] [G loss: 5.836146]\n",
      "epoch:38 step:30370 [D loss: 0.002869, acc.: 100.00%] [G loss: 4.015594]\n",
      "epoch:38 step:30371 [D loss: 0.001296, acc.: 100.00%] [G loss: 3.727014]\n",
      "epoch:38 step:30372 [D loss: 0.006862, acc.: 100.00%] [G loss: 3.266654]\n",
      "epoch:38 step:30373 [D loss: 0.428955, acc.: 75.78%] [G loss: 11.784809]\n",
      "epoch:38 step:30374 [D loss: 2.051243, acc.: 50.78%] [G loss: 5.132330]\n",
      "epoch:38 step:30375 [D loss: 1.148649, acc.: 76.56%] [G loss: 8.978411]\n",
      "epoch:38 step:30376 [D loss: 2.237316, acc.: 50.78%] [G loss: 7.158227]\n",
      "epoch:38 step:30377 [D loss: 0.036659, acc.: 99.22%] [G loss: 6.059327]\n",
      "epoch:38 step:30378 [D loss: 0.002090, acc.: 100.00%] [G loss: 6.213703]\n",
      "epoch:38 step:30379 [D loss: 0.002100, acc.: 100.00%] [G loss: 6.025830]\n",
      "epoch:38 step:30380 [D loss: 0.002326, acc.: 100.00%] [G loss: 5.318082]\n",
      "epoch:38 step:30381 [D loss: 0.002589, acc.: 100.00%] [G loss: 5.342265]\n",
      "epoch:38 step:30382 [D loss: 0.003225, acc.: 100.00%] [G loss: 4.949929]\n",
      "epoch:38 step:30383 [D loss: 0.005871, acc.: 100.00%] [G loss: 5.609684]\n",
      "epoch:38 step:30384 [D loss: 0.033197, acc.: 99.22%] [G loss: 4.749547]\n",
      "epoch:38 step:30385 [D loss: 0.007628, acc.: 100.00%] [G loss: 4.922474]\n",
      "epoch:38 step:30386 [D loss: 0.008931, acc.: 100.00%] [G loss: 4.670620]\n",
      "epoch:38 step:30387 [D loss: 0.007477, acc.: 100.00%] [G loss: 4.133947]\n",
      "epoch:38 step:30388 [D loss: 0.042170, acc.: 99.22%] [G loss: 4.112741]\n",
      "epoch:38 step:30389 [D loss: 0.028070, acc.: 100.00%] [G loss: 4.050524]\n",
      "epoch:38 step:30390 [D loss: 0.034239, acc.: 100.00%] [G loss: 3.817752]\n",
      "epoch:38 step:30391 [D loss: 0.135987, acc.: 96.88%] [G loss: 4.685856]\n",
      "epoch:38 step:30392 [D loss: 0.028015, acc.: 100.00%] [G loss: 3.633028]\n",
      "epoch:38 step:30393 [D loss: 0.022438, acc.: 100.00%] [G loss: 3.830367]\n",
      "epoch:38 step:30394 [D loss: 0.060837, acc.: 99.22%] [G loss: 4.223651]\n",
      "epoch:38 step:30395 [D loss: 0.013003, acc.: 100.00%] [G loss: 4.756507]\n",
      "epoch:38 step:30396 [D loss: 0.014895, acc.: 100.00%] [G loss: 4.612571]\n",
      "epoch:38 step:30397 [D loss: 0.012254, acc.: 100.00%] [G loss: 4.426006]\n",
      "epoch:38 step:30398 [D loss: 0.014186, acc.: 100.00%] [G loss: 4.100488]\n",
      "epoch:38 step:30399 [D loss: 0.051100, acc.: 99.22%] [G loss: 4.515995]\n",
      "epoch:38 step:30400 [D loss: 0.012700, acc.: 100.00%] [G loss: 4.922209]\n",
      "##############\n",
      "[1.05259727 1.0065478  0.86810282 1.08846623 1.10981187 1.11125989\n",
      " 1.11715772 2.11047062 2.10772931 2.11330409]\n",
      "##########\n",
      "epoch:38 step:30401 [D loss: 0.026166, acc.: 99.22%] [G loss: 4.297826]\n",
      "epoch:38 step:30402 [D loss: 0.051222, acc.: 98.44%] [G loss: 3.199828]\n",
      "epoch:38 step:30403 [D loss: 1.478829, acc.: 51.56%] [G loss: 8.719466]\n",
      "epoch:38 step:30404 [D loss: 2.935207, acc.: 50.00%] [G loss: 7.022625]\n",
      "epoch:38 step:30405 [D loss: 2.305714, acc.: 50.78%] [G loss: 5.122289]\n",
      "epoch:38 step:30406 [D loss: 1.354714, acc.: 55.47%] [G loss: 3.604903]\n",
      "epoch:38 step:30407 [D loss: 0.434080, acc.: 78.12%] [G loss: 2.893668]\n",
      "epoch:38 step:30408 [D loss: 0.095235, acc.: 97.66%] [G loss: 2.737579]\n",
      "epoch:38 step:30409 [D loss: 0.060592, acc.: 99.22%] [G loss: 2.891630]\n",
      "epoch:38 step:30410 [D loss: 0.041313, acc.: 100.00%] [G loss: 2.979863]\n",
      "epoch:38 step:30411 [D loss: 0.058859, acc.: 100.00%] [G loss: 2.894919]\n",
      "epoch:38 step:30412 [D loss: 0.037280, acc.: 100.00%] [G loss: 2.782069]\n",
      "epoch:38 step:30413 [D loss: 0.037328, acc.: 100.00%] [G loss: 2.861705]\n",
      "epoch:38 step:30414 [D loss: 0.033433, acc.: 100.00%] [G loss: 2.457317]\n",
      "epoch:38 step:30415 [D loss: 0.058610, acc.: 99.22%] [G loss: 2.412518]\n",
      "epoch:38 step:30416 [D loss: 0.097431, acc.: 98.44%] [G loss: 2.832997]\n",
      "epoch:38 step:30417 [D loss: 0.025794, acc.: 100.00%] [G loss: 2.867694]\n",
      "epoch:38 step:30418 [D loss: 0.237683, acc.: 89.84%] [G loss: 3.217758]\n",
      "epoch:38 step:30419 [D loss: 0.227595, acc.: 92.19%] [G loss: 3.090680]\n",
      "epoch:38 step:30420 [D loss: 0.055080, acc.: 100.00%] [G loss: 3.505250]\n",
      "epoch:38 step:30421 [D loss: 0.172463, acc.: 95.31%] [G loss: 2.944401]\n",
      "epoch:38 step:30422 [D loss: 0.094324, acc.: 97.66%] [G loss: 3.557578]\n",
      "epoch:38 step:30423 [D loss: 0.097895, acc.: 97.66%] [G loss: 3.694993]\n",
      "epoch:38 step:30424 [D loss: 0.585362, acc.: 70.31%] [G loss: 3.397200]\n",
      "epoch:38 step:30425 [D loss: 0.041015, acc.: 100.00%] [G loss: 3.807178]\n",
      "epoch:38 step:30426 [D loss: 0.039307, acc.: 100.00%] [G loss: 3.971826]\n",
      "epoch:38 step:30427 [D loss: 0.148399, acc.: 93.75%] [G loss: 3.233948]\n",
      "epoch:38 step:30428 [D loss: 0.066571, acc.: 99.22%] [G loss: 2.820428]\n",
      "epoch:38 step:30429 [D loss: 0.086899, acc.: 98.44%] [G loss: 3.405064]\n",
      "epoch:38 step:30430 [D loss: 0.035762, acc.: 100.00%] [G loss: 2.757904]\n",
      "epoch:38 step:30431 [D loss: 0.047856, acc.: 100.00%] [G loss: 3.091793]\n",
      "epoch:38 step:30432 [D loss: 0.033309, acc.: 100.00%] [G loss: 2.763622]\n",
      "epoch:38 step:30433 [D loss: 0.036870, acc.: 100.00%] [G loss: 3.131984]\n",
      "epoch:38 step:30434 [D loss: 0.311288, acc.: 87.50%] [G loss: 4.435724]\n",
      "epoch:38 step:30435 [D loss: 0.472471, acc.: 77.34%] [G loss: 3.784101]\n",
      "epoch:38 step:30436 [D loss: 0.036957, acc.: 99.22%] [G loss: 3.199352]\n",
      "epoch:38 step:30437 [D loss: 0.105396, acc.: 96.88%] [G loss: 3.486105]\n",
      "epoch:38 step:30438 [D loss: 0.154279, acc.: 93.75%] [G loss: 3.472781]\n",
      "epoch:38 step:30439 [D loss: 0.045446, acc.: 100.00%] [G loss: 3.286178]\n",
      "epoch:38 step:30440 [D loss: 0.052526, acc.: 100.00%] [G loss: 3.396859]\n",
      "epoch:38 step:30441 [D loss: 0.061539, acc.: 100.00%] [G loss: 3.495495]\n",
      "epoch:38 step:30442 [D loss: 0.064269, acc.: 100.00%] [G loss: 3.546049]\n",
      "epoch:38 step:30443 [D loss: 0.053891, acc.: 100.00%] [G loss: 4.167526]\n",
      "epoch:38 step:30444 [D loss: 0.406813, acc.: 84.38%] [G loss: 3.917491]\n",
      "epoch:38 step:30445 [D loss: 0.192355, acc.: 92.19%] [G loss: 4.031952]\n",
      "epoch:38 step:30446 [D loss: 0.083770, acc.: 99.22%] [G loss: 3.915935]\n",
      "epoch:38 step:30447 [D loss: 0.023799, acc.: 100.00%] [G loss: 4.548130]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:38 step:30448 [D loss: 0.027137, acc.: 99.22%] [G loss: 4.585351]\n",
      "epoch:38 step:30449 [D loss: 0.039497, acc.: 99.22%] [G loss: 3.262432]\n",
      "epoch:38 step:30450 [D loss: 0.012630, acc.: 100.00%] [G loss: 3.705886]\n",
      "epoch:38 step:30451 [D loss: 0.015071, acc.: 100.00%] [G loss: 2.433918]\n",
      "epoch:38 step:30452 [D loss: 0.023102, acc.: 100.00%] [G loss: 3.469286]\n",
      "epoch:38 step:30453 [D loss: 0.104348, acc.: 96.88%] [G loss: 4.521052]\n",
      "epoch:38 step:30454 [D loss: 0.092042, acc.: 97.66%] [G loss: 3.991698]\n",
      "epoch:38 step:30455 [D loss: 0.231708, acc.: 93.75%] [G loss: 3.698842]\n",
      "epoch:38 step:30456 [D loss: 0.034817, acc.: 100.00%] [G loss: 3.887597]\n",
      "epoch:38 step:30457 [D loss: 0.017910, acc.: 100.00%] [G loss: 4.166634]\n",
      "epoch:38 step:30458 [D loss: 0.044024, acc.: 100.00%] [G loss: 4.105026]\n",
      "epoch:38 step:30459 [D loss: 0.017090, acc.: 100.00%] [G loss: 4.747967]\n",
      "epoch:39 step:30460 [D loss: 0.289723, acc.: 89.06%] [G loss: 4.418929]\n",
      "epoch:39 step:30461 [D loss: 0.018318, acc.: 100.00%] [G loss: 5.282900]\n",
      "epoch:39 step:30462 [D loss: 0.074313, acc.: 98.44%] [G loss: 3.912715]\n",
      "epoch:39 step:30463 [D loss: 0.038534, acc.: 99.22%] [G loss: 2.691045]\n",
      "epoch:39 step:30464 [D loss: 0.028872, acc.: 100.00%] [G loss: 3.157743]\n",
      "epoch:39 step:30465 [D loss: 0.071162, acc.: 98.44%] [G loss: 4.739824]\n",
      "epoch:39 step:30466 [D loss: 0.051144, acc.: 99.22%] [G loss: 4.283464]\n",
      "epoch:39 step:30467 [D loss: 0.008302, acc.: 100.00%] [G loss: 3.894494]\n",
      "epoch:39 step:30468 [D loss: 0.037324, acc.: 100.00%] [G loss: 1.782879]\n",
      "epoch:39 step:30469 [D loss: 0.042237, acc.: 100.00%] [G loss: 3.657361]\n",
      "epoch:39 step:30470 [D loss: 0.076340, acc.: 99.22%] [G loss: 4.314727]\n",
      "epoch:39 step:30471 [D loss: 0.011620, acc.: 100.00%] [G loss: 3.796568]\n",
      "epoch:39 step:30472 [D loss: 0.050832, acc.: 99.22%] [G loss: 4.611566]\n",
      "epoch:39 step:30473 [D loss: 0.707667, acc.: 60.16%] [G loss: 7.233020]\n",
      "epoch:39 step:30474 [D loss: 1.975057, acc.: 50.00%] [G loss: 5.861954]\n",
      "epoch:39 step:30475 [D loss: 0.381128, acc.: 82.03%] [G loss: 3.641101]\n",
      "epoch:39 step:30476 [D loss: 0.399968, acc.: 83.59%] [G loss: 4.779695]\n",
      "epoch:39 step:30477 [D loss: 0.020687, acc.: 100.00%] [G loss: 5.724111]\n",
      "epoch:39 step:30478 [D loss: 0.341505, acc.: 89.06%] [G loss: 4.615843]\n",
      "epoch:39 step:30479 [D loss: 0.016371, acc.: 100.00%] [G loss: 4.797253]\n",
      "epoch:39 step:30480 [D loss: 0.029941, acc.: 100.00%] [G loss: 4.436632]\n",
      "epoch:39 step:30481 [D loss: 0.022618, acc.: 100.00%] [G loss: 4.436899]\n",
      "epoch:39 step:30482 [D loss: 0.058692, acc.: 98.44%] [G loss: 4.348179]\n",
      "epoch:39 step:30483 [D loss: 0.013593, acc.: 100.00%] [G loss: 4.247680]\n",
      "epoch:39 step:30484 [D loss: 0.080808, acc.: 96.88%] [G loss: 3.673005]\n",
      "epoch:39 step:30485 [D loss: 0.025034, acc.: 100.00%] [G loss: 3.490411]\n",
      "epoch:39 step:30486 [D loss: 0.019864, acc.: 100.00%] [G loss: 3.115185]\n",
      "epoch:39 step:30487 [D loss: 0.027478, acc.: 100.00%] [G loss: 3.247394]\n",
      "epoch:39 step:30488 [D loss: 0.025278, acc.: 100.00%] [G loss: 3.407384]\n",
      "epoch:39 step:30489 [D loss: 0.023713, acc.: 100.00%] [G loss: 2.794536]\n",
      "epoch:39 step:30490 [D loss: 0.022102, acc.: 100.00%] [G loss: 3.434468]\n",
      "epoch:39 step:30491 [D loss: 0.039929, acc.: 100.00%] [G loss: 3.152514]\n",
      "epoch:39 step:30492 [D loss: 0.048924, acc.: 99.22%] [G loss: 2.185051]\n",
      "epoch:39 step:30493 [D loss: 0.027393, acc.: 100.00%] [G loss: 2.290680]\n",
      "epoch:39 step:30494 [D loss: 0.043665, acc.: 99.22%] [G loss: 2.392785]\n",
      "epoch:39 step:30495 [D loss: 0.097497, acc.: 99.22%] [G loss: 2.440191]\n",
      "epoch:39 step:30496 [D loss: 0.014468, acc.: 100.00%] [G loss: 3.021346]\n",
      "epoch:39 step:30497 [D loss: 0.027202, acc.: 100.00%] [G loss: 3.319412]\n",
      "epoch:39 step:30498 [D loss: 0.156357, acc.: 96.09%] [G loss: 2.203474]\n",
      "epoch:39 step:30499 [D loss: 0.093993, acc.: 100.00%] [G loss: 3.847798]\n",
      "epoch:39 step:30500 [D loss: 0.102876, acc.: 97.66%] [G loss: 3.744637]\n",
      "epoch:39 step:30501 [D loss: 0.011312, acc.: 100.00%] [G loss: 2.670118]\n",
      "epoch:39 step:30502 [D loss: 0.031108, acc.: 100.00%] [G loss: 3.807535]\n",
      "epoch:39 step:30503 [D loss: 0.339667, acc.: 87.50%] [G loss: 5.311379]\n",
      "epoch:39 step:30504 [D loss: 0.696892, acc.: 67.97%] [G loss: 3.351139]\n",
      "epoch:39 step:30505 [D loss: 0.034477, acc.: 100.00%] [G loss: 4.106344]\n",
      "epoch:39 step:30506 [D loss: 0.011516, acc.: 100.00%] [G loss: 4.282156]\n",
      "epoch:39 step:30507 [D loss: 0.017242, acc.: 100.00%] [G loss: 4.809321]\n",
      "epoch:39 step:30508 [D loss: 0.019702, acc.: 100.00%] [G loss: 3.418121]\n",
      "epoch:39 step:30509 [D loss: 0.037574, acc.: 100.00%] [G loss: 4.757142]\n",
      "epoch:39 step:30510 [D loss: 0.024953, acc.: 99.22%] [G loss: 4.612811]\n",
      "epoch:39 step:30511 [D loss: 0.038345, acc.: 98.44%] [G loss: 4.136087]\n",
      "epoch:39 step:30512 [D loss: 0.037512, acc.: 99.22%] [G loss: 4.263719]\n",
      "epoch:39 step:30513 [D loss: 0.082439, acc.: 98.44%] [G loss: 3.944712]\n",
      "epoch:39 step:30514 [D loss: 0.010798, acc.: 100.00%] [G loss: 3.800887]\n",
      "epoch:39 step:30515 [D loss: 0.032167, acc.: 100.00%] [G loss: 4.883253]\n",
      "epoch:39 step:30516 [D loss: 0.014069, acc.: 100.00%] [G loss: 4.814555]\n",
      "epoch:39 step:30517 [D loss: 0.045803, acc.: 99.22%] [G loss: 3.350736]\n",
      "epoch:39 step:30518 [D loss: 0.022293, acc.: 100.00%] [G loss: 4.186866]\n",
      "epoch:39 step:30519 [D loss: 0.017773, acc.: 100.00%] [G loss: 4.529093]\n",
      "epoch:39 step:30520 [D loss: 0.019743, acc.: 99.22%] [G loss: 4.663740]\n",
      "epoch:39 step:30521 [D loss: 0.042058, acc.: 99.22%] [G loss: 3.933980]\n",
      "epoch:39 step:30522 [D loss: 0.225243, acc.: 92.19%] [G loss: 6.226049]\n",
      "epoch:39 step:30523 [D loss: 0.428735, acc.: 79.69%] [G loss: 3.611062]\n",
      "epoch:39 step:30524 [D loss: 0.205451, acc.: 92.19%] [G loss: 6.420840]\n",
      "epoch:39 step:30525 [D loss: 0.039603, acc.: 98.44%] [G loss: 6.549467]\n",
      "epoch:39 step:30526 [D loss: 0.847184, acc.: 64.84%] [G loss: 1.689257]\n",
      "epoch:39 step:30527 [D loss: 1.464365, acc.: 50.78%] [G loss: 7.079477]\n",
      "epoch:39 step:30528 [D loss: 2.349026, acc.: 50.00%] [G loss: 5.553890]\n",
      "epoch:39 step:30529 [D loss: 2.037159, acc.: 50.00%] [G loss: 3.710254]\n",
      "epoch:39 step:30530 [D loss: 1.484581, acc.: 50.00%] [G loss: 2.528361]\n",
      "epoch:39 step:30531 [D loss: 0.747183, acc.: 60.94%] [G loss: 1.867145]\n",
      "epoch:39 step:30532 [D loss: 0.504185, acc.: 71.88%] [G loss: 1.347800]\n",
      "epoch:39 step:30533 [D loss: 0.404180, acc.: 83.59%] [G loss: 1.391122]\n",
      "epoch:39 step:30534 [D loss: 0.433446, acc.: 82.81%] [G loss: 1.676867]\n",
      "epoch:39 step:30535 [D loss: 0.203897, acc.: 96.09%] [G loss: 1.906385]\n",
      "epoch:39 step:30536 [D loss: 0.264861, acc.: 91.41%] [G loss: 1.874056]\n",
      "epoch:39 step:30537 [D loss: 0.220546, acc.: 96.88%] [G loss: 1.853852]\n",
      "epoch:39 step:30538 [D loss: 0.192365, acc.: 98.44%] [G loss: 2.098393]\n",
      "epoch:39 step:30539 [D loss: 0.311242, acc.: 91.41%] [G loss: 1.842541]\n",
      "epoch:39 step:30540 [D loss: 0.184903, acc.: 96.88%] [G loss: 2.080205]\n",
      "epoch:39 step:30541 [D loss: 0.124372, acc.: 99.22%] [G loss: 1.979222]\n",
      "epoch:39 step:30542 [D loss: 0.210058, acc.: 94.53%] [G loss: 1.792165]\n",
      "epoch:39 step:30543 [D loss: 0.229476, acc.: 93.75%] [G loss: 2.120337]\n",
      "epoch:39 step:30544 [D loss: 0.206473, acc.: 96.88%] [G loss: 2.437744]\n",
      "epoch:39 step:30545 [D loss: 0.284572, acc.: 87.50%] [G loss: 2.137532]\n",
      "epoch:39 step:30546 [D loss: 0.248160, acc.: 92.19%] [G loss: 1.843359]\n",
      "epoch:39 step:30547 [D loss: 0.175418, acc.: 96.09%] [G loss: 2.364251]\n",
      "epoch:39 step:30548 [D loss: 0.231388, acc.: 94.53%] [G loss: 2.074403]\n",
      "epoch:39 step:30549 [D loss: 0.585309, acc.: 70.31%] [G loss: 3.950897]\n",
      "epoch:39 step:30550 [D loss: 0.260771, acc.: 89.06%] [G loss: 3.691818]\n",
      "epoch:39 step:30551 [D loss: 0.315432, acc.: 88.28%] [G loss: 3.211631]\n",
      "epoch:39 step:30552 [D loss: 0.127857, acc.: 98.44%] [G loss: 2.755417]\n",
      "epoch:39 step:30553 [D loss: 0.106068, acc.: 99.22%] [G loss: 2.463985]\n",
      "epoch:39 step:30554 [D loss: 0.122999, acc.: 97.66%] [G loss: 2.476407]\n",
      "epoch:39 step:30555 [D loss: 0.190465, acc.: 94.53%] [G loss: 3.114015]\n",
      "epoch:39 step:30556 [D loss: 0.092100, acc.: 97.66%] [G loss: 3.141079]\n",
      "epoch:39 step:30557 [D loss: 0.204371, acc.: 94.53%] [G loss: 2.706852]\n",
      "epoch:39 step:30558 [D loss: 0.117566, acc.: 99.22%] [G loss: 3.578149]\n",
      "epoch:39 step:30559 [D loss: 0.067870, acc.: 99.22%] [G loss: 3.202391]\n",
      "epoch:39 step:30560 [D loss: 0.111030, acc.: 98.44%] [G loss: 3.418661]\n",
      "epoch:39 step:30561 [D loss: 0.144844, acc.: 95.31%] [G loss: 3.377879]\n",
      "epoch:39 step:30562 [D loss: 0.104771, acc.: 97.66%] [G loss: 3.068848]\n",
      "epoch:39 step:30563 [D loss: 0.169396, acc.: 96.09%] [G loss: 3.195231]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:39 step:30564 [D loss: 0.152317, acc.: 96.09%] [G loss: 3.390771]\n",
      "epoch:39 step:30565 [D loss: 0.066825, acc.: 98.44%] [G loss: 3.719914]\n",
      "epoch:39 step:30566 [D loss: 0.139927, acc.: 96.88%] [G loss: 3.158575]\n",
      "epoch:39 step:30567 [D loss: 0.062436, acc.: 100.00%] [G loss: 2.601140]\n",
      "epoch:39 step:30568 [D loss: 0.082352, acc.: 99.22%] [G loss: 2.440675]\n",
      "epoch:39 step:30569 [D loss: 0.222236, acc.: 91.41%] [G loss: 4.293216]\n",
      "epoch:39 step:30570 [D loss: 0.256644, acc.: 86.72%] [G loss: 3.929265]\n",
      "epoch:39 step:30571 [D loss: 0.111395, acc.: 98.44%] [G loss: 3.411166]\n",
      "epoch:39 step:30572 [D loss: 0.036864, acc.: 100.00%] [G loss: 3.002622]\n",
      "epoch:39 step:30573 [D loss: 0.082708, acc.: 98.44%] [G loss: 1.680851]\n",
      "epoch:39 step:30574 [D loss: 0.131850, acc.: 96.09%] [G loss: 2.861100]\n",
      "epoch:39 step:30575 [D loss: 0.118474, acc.: 96.88%] [G loss: 2.442116]\n",
      "epoch:39 step:30576 [D loss: 0.120328, acc.: 96.09%] [G loss: 1.094119]\n",
      "epoch:39 step:30577 [D loss: 0.070925, acc.: 98.44%] [G loss: 0.794155]\n",
      "epoch:39 step:30578 [D loss: 0.133741, acc.: 96.88%] [G loss: 2.639319]\n",
      "epoch:39 step:30579 [D loss: 0.089390, acc.: 96.09%] [G loss: 2.109640]\n",
      "epoch:39 step:30580 [D loss: 0.064664, acc.: 99.22%] [G loss: 1.890836]\n",
      "epoch:39 step:30581 [D loss: 0.156379, acc.: 96.09%] [G loss: 2.481605]\n",
      "epoch:39 step:30582 [D loss: 0.106239, acc.: 96.88%] [G loss: 3.551853]\n",
      "epoch:39 step:30583 [D loss: 0.168189, acc.: 97.66%] [G loss: 2.718059]\n",
      "epoch:39 step:30584 [D loss: 0.074201, acc.: 97.66%] [G loss: 3.977905]\n",
      "epoch:39 step:30585 [D loss: 0.288798, acc.: 88.28%] [G loss: 3.119924]\n",
      "epoch:39 step:30586 [D loss: 0.133909, acc.: 97.66%] [G loss: 3.667180]\n",
      "epoch:39 step:30587 [D loss: 0.088421, acc.: 98.44%] [G loss: 4.319642]\n",
      "epoch:39 step:30588 [D loss: 0.132260, acc.: 94.53%] [G loss: 4.087683]\n",
      "epoch:39 step:30589 [D loss: 0.079675, acc.: 98.44%] [G loss: 3.986727]\n",
      "epoch:39 step:30590 [D loss: 0.016656, acc.: 100.00%] [G loss: 4.467517]\n",
      "epoch:39 step:30591 [D loss: 0.125705, acc.: 96.09%] [G loss: 3.386336]\n",
      "epoch:39 step:30592 [D loss: 0.043183, acc.: 100.00%] [G loss: 4.436310]\n",
      "epoch:39 step:30593 [D loss: 0.034215, acc.: 100.00%] [G loss: 4.106216]\n",
      "epoch:39 step:30594 [D loss: 0.101142, acc.: 96.09%] [G loss: 3.321517]\n",
      "epoch:39 step:30595 [D loss: 0.082679, acc.: 98.44%] [G loss: 3.411000]\n",
      "epoch:39 step:30596 [D loss: 0.020451, acc.: 100.00%] [G loss: 4.339958]\n",
      "epoch:39 step:30597 [D loss: 0.103590, acc.: 95.31%] [G loss: 5.445374]\n",
      "epoch:39 step:30598 [D loss: 0.030091, acc.: 98.44%] [G loss: 5.298714]\n",
      "epoch:39 step:30599 [D loss: 0.537414, acc.: 75.00%] [G loss: 4.682522]\n",
      "epoch:39 step:30600 [D loss: 0.018611, acc.: 99.22%] [G loss: 5.977737]\n",
      "##############\n",
      "[0.99316116 0.80222433 1.10520959 0.89145146 1.00032635 1.1025619\n",
      " 1.10537135 2.10466266 2.11559373 2.12080711]\n",
      "##########\n",
      "epoch:39 step:30601 [D loss: 0.114476, acc.: 95.31%] [G loss: 3.618159]\n",
      "epoch:39 step:30602 [D loss: 0.148795, acc.: 93.75%] [G loss: 3.382293]\n",
      "epoch:39 step:30603 [D loss: 0.023504, acc.: 100.00%] [G loss: 4.294352]\n",
      "epoch:39 step:30604 [D loss: 0.037495, acc.: 99.22%] [G loss: 3.407485]\n",
      "epoch:39 step:30605 [D loss: 0.034465, acc.: 100.00%] [G loss: 3.292989]\n",
      "epoch:39 step:30606 [D loss: 0.173219, acc.: 93.75%] [G loss: 4.967441]\n",
      "epoch:39 step:30607 [D loss: 0.019420, acc.: 100.00%] [G loss: 6.083049]\n",
      "epoch:39 step:30608 [D loss: 0.124308, acc.: 94.53%] [G loss: 2.961878]\n",
      "epoch:39 step:30609 [D loss: 0.061096, acc.: 98.44%] [G loss: 3.359931]\n",
      "epoch:39 step:30610 [D loss: 0.074475, acc.: 99.22%] [G loss: 4.784727]\n",
      "epoch:39 step:30611 [D loss: 0.048205, acc.: 98.44%] [G loss: 5.081257]\n",
      "epoch:39 step:30612 [D loss: 1.095796, acc.: 47.66%] [G loss: 7.091146]\n",
      "epoch:39 step:30613 [D loss: 0.182814, acc.: 89.06%] [G loss: 8.108383]\n",
      "epoch:39 step:30614 [D loss: 0.060494, acc.: 99.22%] [G loss: 7.548390]\n",
      "epoch:39 step:30615 [D loss: 0.063776, acc.: 97.66%] [G loss: 6.159716]\n",
      "epoch:39 step:30616 [D loss: 0.004901, acc.: 100.00%] [G loss: 4.742040]\n",
      "epoch:39 step:30617 [D loss: 0.074149, acc.: 98.44%] [G loss: 3.450860]\n",
      "epoch:39 step:30618 [D loss: 0.097787, acc.: 95.31%] [G loss: 4.424227]\n",
      "epoch:39 step:30619 [D loss: 0.002143, acc.: 100.00%] [G loss: 5.378970]\n",
      "epoch:39 step:30620 [D loss: 0.163641, acc.: 92.97%] [G loss: 3.421558]\n",
      "epoch:39 step:30621 [D loss: 0.286504, acc.: 86.72%] [G loss: 6.341835]\n",
      "epoch:39 step:30622 [D loss: 0.053792, acc.: 100.00%] [G loss: 7.190486]\n",
      "epoch:39 step:30623 [D loss: 0.853666, acc.: 65.62%] [G loss: 2.135983]\n",
      "epoch:39 step:30624 [D loss: 0.500229, acc.: 78.91%] [G loss: 5.617492]\n",
      "epoch:39 step:30625 [D loss: 0.032684, acc.: 98.44%] [G loss: 6.155098]\n",
      "epoch:39 step:30626 [D loss: 0.477986, acc.: 82.81%] [G loss: 4.598310]\n",
      "epoch:39 step:30627 [D loss: 0.016406, acc.: 99.22%] [G loss: 4.178226]\n",
      "epoch:39 step:30628 [D loss: 0.012166, acc.: 100.00%] [G loss: 3.546576]\n",
      "epoch:39 step:30629 [D loss: 0.011846, acc.: 100.00%] [G loss: 3.500761]\n",
      "epoch:39 step:30630 [D loss: 0.012979, acc.: 100.00%] [G loss: 3.100532]\n",
      "epoch:39 step:30631 [D loss: 0.020360, acc.: 100.00%] [G loss: 2.960993]\n",
      "epoch:39 step:30632 [D loss: 0.047308, acc.: 99.22%] [G loss: 2.245848]\n",
      "epoch:39 step:30633 [D loss: 0.020635, acc.: 100.00%] [G loss: 2.100510]\n",
      "epoch:39 step:30634 [D loss: 0.052049, acc.: 98.44%] [G loss: 2.683367]\n",
      "epoch:39 step:30635 [D loss: 0.105057, acc.: 97.66%] [G loss: 2.466490]\n",
      "epoch:39 step:30636 [D loss: 0.014407, acc.: 100.00%] [G loss: 2.678883]\n",
      "epoch:39 step:30637 [D loss: 0.043050, acc.: 100.00%] [G loss: 1.977853]\n",
      "epoch:39 step:30638 [D loss: 0.215331, acc.: 91.41%] [G loss: 1.412842]\n",
      "epoch:39 step:30639 [D loss: 0.089781, acc.: 96.88%] [G loss: 3.650552]\n",
      "epoch:39 step:30640 [D loss: 0.007389, acc.: 100.00%] [G loss: 4.537859]\n",
      "epoch:39 step:30641 [D loss: 0.004443, acc.: 100.00%] [G loss: 4.139070]\n",
      "epoch:39 step:30642 [D loss: 0.016525, acc.: 100.00%] [G loss: 3.745632]\n",
      "epoch:39 step:30643 [D loss: 0.019750, acc.: 100.00%] [G loss: 3.663558]\n",
      "epoch:39 step:30644 [D loss: 0.020904, acc.: 99.22%] [G loss: 3.209974]\n",
      "epoch:39 step:30645 [D loss: 0.053384, acc.: 98.44%] [G loss: 4.167489]\n",
      "epoch:39 step:30646 [D loss: 0.379844, acc.: 81.25%] [G loss: 5.169572]\n",
      "epoch:39 step:30647 [D loss: 0.009284, acc.: 100.00%] [G loss: 5.951056]\n",
      "epoch:39 step:30648 [D loss: 0.004398, acc.: 100.00%] [G loss: 6.375463]\n",
      "epoch:39 step:30649 [D loss: 0.003811, acc.: 100.00%] [G loss: 6.747157]\n",
      "epoch:39 step:30650 [D loss: 0.041118, acc.: 100.00%] [G loss: 5.791157]\n",
      "epoch:39 step:30651 [D loss: 0.033310, acc.: 99.22%] [G loss: 5.896688]\n",
      "epoch:39 step:30652 [D loss: 0.010875, acc.: 100.00%] [G loss: 5.581673]\n",
      "epoch:39 step:30653 [D loss: 0.005312, acc.: 100.00%] [G loss: 5.103566]\n",
      "epoch:39 step:30654 [D loss: 0.005658, acc.: 100.00%] [G loss: 5.323577]\n",
      "epoch:39 step:30655 [D loss: 0.008368, acc.: 100.00%] [G loss: 4.150389]\n",
      "epoch:39 step:30656 [D loss: 0.009658, acc.: 100.00%] [G loss: 4.446422]\n",
      "epoch:39 step:30657 [D loss: 0.044975, acc.: 97.66%] [G loss: 4.999434]\n",
      "epoch:39 step:30658 [D loss: 0.004062, acc.: 100.00%] [G loss: 5.984299]\n",
      "epoch:39 step:30659 [D loss: 0.002523, acc.: 100.00%] [G loss: 6.515642]\n",
      "epoch:39 step:30660 [D loss: 0.002234, acc.: 100.00%] [G loss: 6.069167]\n",
      "epoch:39 step:30661 [D loss: 0.010185, acc.: 100.00%] [G loss: 5.202090]\n",
      "epoch:39 step:30662 [D loss: 0.004360, acc.: 100.00%] [G loss: 5.125586]\n",
      "epoch:39 step:30663 [D loss: 0.009418, acc.: 100.00%] [G loss: 5.040933]\n",
      "epoch:39 step:30664 [D loss: 0.023738, acc.: 99.22%] [G loss: 4.738509]\n",
      "epoch:39 step:30665 [D loss: 0.009864, acc.: 100.00%] [G loss: 5.011627]\n",
      "epoch:39 step:30666 [D loss: 0.030079, acc.: 100.00%] [G loss: 4.515452]\n",
      "epoch:39 step:30667 [D loss: 0.005974, acc.: 100.00%] [G loss: 5.152090]\n",
      "epoch:39 step:30668 [D loss: 0.007362, acc.: 100.00%] [G loss: 4.780954]\n",
      "epoch:39 step:30669 [D loss: 0.019298, acc.: 100.00%] [G loss: 4.711995]\n",
      "epoch:39 step:30670 [D loss: 0.005458, acc.: 100.00%] [G loss: 4.462494]\n",
      "epoch:39 step:30671 [D loss: 0.051611, acc.: 99.22%] [G loss: 6.325401]\n",
      "epoch:39 step:30672 [D loss: 0.005209, acc.: 100.00%] [G loss: 7.366005]\n",
      "epoch:39 step:30673 [D loss: 1.727582, acc.: 43.75%] [G loss: 4.548008]\n",
      "epoch:39 step:30674 [D loss: 0.011084, acc.: 100.00%] [G loss: 5.618122]\n",
      "epoch:39 step:30675 [D loss: 0.009511, acc.: 100.00%] [G loss: 5.885506]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:39 step:30676 [D loss: 0.004445, acc.: 100.00%] [G loss: 5.555014]\n",
      "epoch:39 step:30677 [D loss: 0.007743, acc.: 100.00%] [G loss: 4.954812]\n",
      "epoch:39 step:30678 [D loss: 0.005941, acc.: 100.00%] [G loss: 5.146118]\n",
      "epoch:39 step:30679 [D loss: 0.015636, acc.: 100.00%] [G loss: 4.498562]\n",
      "epoch:39 step:30680 [D loss: 0.009360, acc.: 100.00%] [G loss: 2.915528]\n",
      "epoch:39 step:30681 [D loss: 0.008479, acc.: 100.00%] [G loss: 1.295234]\n",
      "epoch:39 step:30682 [D loss: 0.037322, acc.: 100.00%] [G loss: 1.015228]\n",
      "epoch:39 step:30683 [D loss: 0.005125, acc.: 100.00%] [G loss: 0.421747]\n",
      "epoch:39 step:30684 [D loss: 0.017986, acc.: 100.00%] [G loss: 2.211814]\n",
      "epoch:39 step:30685 [D loss: 0.066069, acc.: 99.22%] [G loss: 2.550075]\n",
      "epoch:39 step:30686 [D loss: 0.005871, acc.: 100.00%] [G loss: 1.821151]\n",
      "epoch:39 step:30687 [D loss: 0.001180, acc.: 100.00%] [G loss: 2.217963]\n",
      "epoch:39 step:30688 [D loss: 0.004643, acc.: 100.00%] [G loss: 0.864814]\n",
      "epoch:39 step:30689 [D loss: 0.003446, acc.: 100.00%] [G loss: 0.948640]\n",
      "epoch:39 step:30690 [D loss: 0.046553, acc.: 100.00%] [G loss: 1.397282]\n",
      "epoch:39 step:30691 [D loss: 0.098353, acc.: 97.66%] [G loss: 0.392971]\n",
      "epoch:39 step:30692 [D loss: 0.003424, acc.: 100.00%] [G loss: 1.016991]\n",
      "epoch:39 step:30693 [D loss: 0.284093, acc.: 85.94%] [G loss: 7.507664]\n",
      "epoch:39 step:30694 [D loss: 1.517016, acc.: 53.12%] [G loss: 4.197166]\n",
      "epoch:39 step:30695 [D loss: 0.223594, acc.: 90.62%] [G loss: 5.226989]\n",
      "epoch:39 step:30696 [D loss: 0.063750, acc.: 99.22%] [G loss: 5.671233]\n",
      "epoch:39 step:30697 [D loss: 0.036403, acc.: 98.44%] [G loss: 5.305003]\n",
      "epoch:39 step:30698 [D loss: 0.040370, acc.: 98.44%] [G loss: 4.318446]\n",
      "epoch:39 step:30699 [D loss: 0.052764, acc.: 100.00%] [G loss: 3.481442]\n",
      "epoch:39 step:30700 [D loss: 0.072987, acc.: 98.44%] [G loss: 4.415052]\n",
      "epoch:39 step:30701 [D loss: 0.009140, acc.: 100.00%] [G loss: 5.501499]\n",
      "epoch:39 step:30702 [D loss: 0.009342, acc.: 100.00%] [G loss: 4.762496]\n",
      "epoch:39 step:30703 [D loss: 0.004397, acc.: 100.00%] [G loss: 3.873396]\n",
      "epoch:39 step:30704 [D loss: 0.009846, acc.: 100.00%] [G loss: 3.407753]\n",
      "epoch:39 step:30705 [D loss: 0.026898, acc.: 100.00%] [G loss: 3.615324]\n",
      "epoch:39 step:30706 [D loss: 0.037584, acc.: 99.22%] [G loss: 2.235246]\n",
      "epoch:39 step:30707 [D loss: 0.042915, acc.: 100.00%] [G loss: 2.558853]\n",
      "epoch:39 step:30708 [D loss: 0.030952, acc.: 100.00%] [G loss: 3.221169]\n",
      "epoch:39 step:30709 [D loss: 0.075084, acc.: 96.88%] [G loss: 2.650420]\n",
      "epoch:39 step:30710 [D loss: 0.031928, acc.: 99.22%] [G loss: 2.515293]\n",
      "epoch:39 step:30711 [D loss: 0.041197, acc.: 100.00%] [G loss: 3.328488]\n",
      "epoch:39 step:30712 [D loss: 0.046910, acc.: 98.44%] [G loss: 4.081059]\n",
      "epoch:39 step:30713 [D loss: 0.014241, acc.: 100.00%] [G loss: 3.726667]\n",
      "epoch:39 step:30714 [D loss: 0.043328, acc.: 99.22%] [G loss: 2.975074]\n",
      "epoch:39 step:30715 [D loss: 0.007359, acc.: 100.00%] [G loss: 2.892086]\n",
      "epoch:39 step:30716 [D loss: 0.081617, acc.: 99.22%] [G loss: 4.207837]\n",
      "epoch:39 step:30717 [D loss: 0.046546, acc.: 99.22%] [G loss: 3.782512]\n",
      "epoch:39 step:30718 [D loss: 0.006060, acc.: 100.00%] [G loss: 4.099496]\n",
      "epoch:39 step:30719 [D loss: 0.092238, acc.: 96.88%] [G loss: 3.207503]\n",
      "epoch:39 step:30720 [D loss: 0.101721, acc.: 97.66%] [G loss: 4.857724]\n",
      "epoch:39 step:30721 [D loss: 0.001337, acc.: 100.00%] [G loss: 5.717505]\n",
      "epoch:39 step:30722 [D loss: 0.002495, acc.: 100.00%] [G loss: 5.007209]\n",
      "epoch:39 step:30723 [D loss: 0.210390, acc.: 92.19%] [G loss: 2.616462]\n",
      "epoch:39 step:30724 [D loss: 0.125205, acc.: 99.22%] [G loss: 5.430120]\n",
      "epoch:39 step:30725 [D loss: 0.003521, acc.: 100.00%] [G loss: 7.417395]\n",
      "epoch:39 step:30726 [D loss: 0.318283, acc.: 88.28%] [G loss: 4.376719]\n",
      "epoch:39 step:30727 [D loss: 0.041801, acc.: 99.22%] [G loss: 5.581014]\n",
      "epoch:39 step:30728 [D loss: 0.038986, acc.: 99.22%] [G loss: 3.756082]\n",
      "epoch:39 step:30729 [D loss: 0.132204, acc.: 96.88%] [G loss: 2.938412]\n",
      "epoch:39 step:30730 [D loss: 0.025594, acc.: 100.00%] [G loss: 4.032564]\n",
      "epoch:39 step:30731 [D loss: 0.096740, acc.: 96.09%] [G loss: 6.665067]\n",
      "epoch:39 step:30732 [D loss: 0.344407, acc.: 86.72%] [G loss: 1.470988]\n",
      "epoch:39 step:30733 [D loss: 0.073622, acc.: 99.22%] [G loss: 2.459353]\n",
      "epoch:39 step:30734 [D loss: 0.025042, acc.: 100.00%] [G loss: 5.014473]\n",
      "epoch:39 step:30735 [D loss: 0.004928, acc.: 100.00%] [G loss: 5.203286]\n",
      "epoch:39 step:30736 [D loss: 0.010281, acc.: 100.00%] [G loss: 5.368939]\n",
      "epoch:39 step:30737 [D loss: 0.009260, acc.: 100.00%] [G loss: 5.065343]\n",
      "epoch:39 step:30738 [D loss: 0.007481, acc.: 100.00%] [G loss: 4.723874]\n",
      "epoch:39 step:30739 [D loss: 0.007414, acc.: 100.00%] [G loss: 4.157307]\n",
      "epoch:39 step:30740 [D loss: 0.009659, acc.: 100.00%] [G loss: 3.264876]\n",
      "epoch:39 step:30741 [D loss: 0.006394, acc.: 100.00%] [G loss: 3.798628]\n",
      "epoch:39 step:30742 [D loss: 0.003206, acc.: 100.00%] [G loss: 4.640812]\n",
      "epoch:39 step:30743 [D loss: 0.014797, acc.: 100.00%] [G loss: 3.241726]\n",
      "epoch:39 step:30744 [D loss: 0.017631, acc.: 100.00%] [G loss: 4.421392]\n",
      "epoch:39 step:30745 [D loss: 0.597895, acc.: 71.09%] [G loss: 6.894186]\n",
      "epoch:39 step:30746 [D loss: 0.153479, acc.: 94.53%] [G loss: 6.499773]\n",
      "epoch:39 step:30747 [D loss: 0.001504, acc.: 100.00%] [G loss: 5.526665]\n",
      "epoch:39 step:30748 [D loss: 0.004026, acc.: 100.00%] [G loss: 5.329850]\n",
      "epoch:39 step:30749 [D loss: 0.009126, acc.: 100.00%] [G loss: 4.483773]\n",
      "epoch:39 step:30750 [D loss: 0.007058, acc.: 100.00%] [G loss: 3.665768]\n",
      "epoch:39 step:30751 [D loss: 0.039745, acc.: 99.22%] [G loss: 4.578782]\n",
      "epoch:39 step:30752 [D loss: 0.003677, acc.: 100.00%] [G loss: 5.524020]\n",
      "epoch:39 step:30753 [D loss: 0.116707, acc.: 96.88%] [G loss: 4.531193]\n",
      "epoch:39 step:30754 [D loss: 0.003622, acc.: 100.00%] [G loss: 4.970084]\n",
      "epoch:39 step:30755 [D loss: 0.016376, acc.: 99.22%] [G loss: 4.357889]\n",
      "epoch:39 step:30756 [D loss: 0.069095, acc.: 98.44%] [G loss: 5.514686]\n",
      "epoch:39 step:30757 [D loss: 0.063043, acc.: 96.88%] [G loss: 5.147964]\n",
      "epoch:39 step:30758 [D loss: 0.017650, acc.: 99.22%] [G loss: 4.096780]\n",
      "epoch:39 step:30759 [D loss: 0.100627, acc.: 100.00%] [G loss: 6.519577]\n",
      "epoch:39 step:30760 [D loss: 0.060917, acc.: 98.44%] [G loss: 6.873545]\n",
      "epoch:39 step:30761 [D loss: 0.010639, acc.: 100.00%] [G loss: 6.102676]\n",
      "epoch:39 step:30762 [D loss: 0.037385, acc.: 100.00%] [G loss: 4.522180]\n",
      "epoch:39 step:30763 [D loss: 0.012994, acc.: 100.00%] [G loss: 3.507240]\n",
      "epoch:39 step:30764 [D loss: 0.018017, acc.: 100.00%] [G loss: 4.589841]\n",
      "epoch:39 step:30765 [D loss: 0.010535, acc.: 100.00%] [G loss: 5.093577]\n",
      "epoch:39 step:30766 [D loss: 0.004157, acc.: 100.00%] [G loss: 5.422771]\n",
      "epoch:39 step:30767 [D loss: 0.036054, acc.: 100.00%] [G loss: 5.658329]\n",
      "epoch:39 step:30768 [D loss: 0.026870, acc.: 99.22%] [G loss: 5.072789]\n",
      "epoch:39 step:30769 [D loss: 0.061617, acc.: 99.22%] [G loss: 5.436056]\n",
      "epoch:39 step:30770 [D loss: 0.002316, acc.: 100.00%] [G loss: 5.815014]\n",
      "epoch:39 step:30771 [D loss: 3.002979, acc.: 42.19%] [G loss: 10.926052]\n",
      "epoch:39 step:30772 [D loss: 4.707438, acc.: 50.00%] [G loss: 7.001760]\n",
      "epoch:39 step:30773 [D loss: 2.879791, acc.: 50.00%] [G loss: 3.926873]\n",
      "epoch:39 step:30774 [D loss: 1.784730, acc.: 50.00%] [G loss: 2.571754]\n",
      "epoch:39 step:30775 [D loss: 0.937598, acc.: 54.69%] [G loss: 1.515728]\n",
      "epoch:39 step:30776 [D loss: 0.374446, acc.: 85.94%] [G loss: 1.293943]\n",
      "epoch:39 step:30777 [D loss: 0.422936, acc.: 78.12%] [G loss: 1.612078]\n",
      "epoch:39 step:30778 [D loss: 0.424171, acc.: 80.47%] [G loss: 1.812316]\n",
      "epoch:39 step:30779 [D loss: 0.486616, acc.: 78.12%] [G loss: 1.646317]\n",
      "epoch:39 step:30780 [D loss: 0.282735, acc.: 93.75%] [G loss: 1.806097]\n",
      "epoch:39 step:30781 [D loss: 0.197793, acc.: 96.09%] [G loss: 1.685756]\n",
      "epoch:39 step:30782 [D loss: 0.308367, acc.: 90.62%] [G loss: 1.461460]\n",
      "epoch:39 step:30783 [D loss: 0.184958, acc.: 93.75%] [G loss: 1.960262]\n",
      "epoch:39 step:30784 [D loss: 0.145849, acc.: 97.66%] [G loss: 2.090761]\n",
      "epoch:39 step:30785 [D loss: 0.160651, acc.: 94.53%] [G loss: 1.493140]\n",
      "epoch:39 step:30786 [D loss: 0.107040, acc.: 99.22%] [G loss: 1.122555]\n",
      "epoch:39 step:30787 [D loss: 0.329092, acc.: 88.28%] [G loss: 2.813009]\n",
      "epoch:39 step:30788 [D loss: 0.161664, acc.: 96.88%] [G loss: 2.971514]\n",
      "epoch:39 step:30789 [D loss: 0.314041, acc.: 86.72%] [G loss: 1.980217]\n",
      "epoch:39 step:30790 [D loss: 0.173448, acc.: 93.75%] [G loss: 1.349108]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:39 step:30791 [D loss: 0.468456, acc.: 73.44%] [G loss: 3.158731]\n",
      "epoch:39 step:30792 [D loss: 0.367902, acc.: 80.47%] [G loss: 2.862188]\n",
      "epoch:39 step:30793 [D loss: 0.397293, acc.: 82.03%] [G loss: 2.375353]\n",
      "epoch:39 step:30794 [D loss: 0.127613, acc.: 97.66%] [G loss: 2.137759]\n",
      "epoch:39 step:30795 [D loss: 0.113438, acc.: 99.22%] [G loss: 2.736183]\n",
      "epoch:39 step:30796 [D loss: 0.076683, acc.: 99.22%] [G loss: 2.736246]\n",
      "epoch:39 step:30797 [D loss: 0.070589, acc.: 100.00%] [G loss: 2.563997]\n",
      "epoch:39 step:30798 [D loss: 0.098843, acc.: 99.22%] [G loss: 2.622652]\n",
      "epoch:39 step:30799 [D loss: 0.071212, acc.: 100.00%] [G loss: 2.556498]\n",
      "epoch:39 step:30800 [D loss: 0.100761, acc.: 98.44%] [G loss: 2.906947]\n",
      "##############\n",
      "[0.84877927 2.11521971 1.10983229 0.91760517 1.10429347 1.11794603\n",
      " 2.10633498 2.10783198 0.85370186 1.05548139]\n",
      "##########\n",
      "epoch:39 step:30801 [D loss: 0.117013, acc.: 98.44%] [G loss: 3.196512]\n",
      "epoch:39 step:30802 [D loss: 0.133629, acc.: 97.66%] [G loss: 2.594968]\n",
      "epoch:39 step:30803 [D loss: 0.140762, acc.: 96.88%] [G loss: 2.228628]\n",
      "epoch:39 step:30804 [D loss: 0.048572, acc.: 100.00%] [G loss: 3.338114]\n",
      "epoch:39 step:30805 [D loss: 0.122731, acc.: 96.88%] [G loss: 2.753078]\n",
      "epoch:39 step:30806 [D loss: 0.071575, acc.: 100.00%] [G loss: 3.162084]\n",
      "epoch:39 step:30807 [D loss: 0.073342, acc.: 98.44%] [G loss: 2.658077]\n",
      "epoch:39 step:30808 [D loss: 0.252302, acc.: 93.75%] [G loss: 3.511731]\n",
      "epoch:39 step:30809 [D loss: 0.236276, acc.: 92.97%] [G loss: 2.886561]\n",
      "epoch:39 step:30810 [D loss: 0.203744, acc.: 94.53%] [G loss: 3.466108]\n",
      "epoch:39 step:30811 [D loss: 0.117165, acc.: 97.66%] [G loss: 3.114079]\n",
      "epoch:39 step:30812 [D loss: 0.182228, acc.: 95.31%] [G loss: 1.910180]\n",
      "epoch:39 step:30813 [D loss: 0.050808, acc.: 100.00%] [G loss: 2.858845]\n",
      "epoch:39 step:30814 [D loss: 0.082673, acc.: 99.22%] [G loss: 2.487099]\n",
      "epoch:39 step:30815 [D loss: 0.039509, acc.: 100.00%] [G loss: 2.426614]\n",
      "epoch:39 step:30816 [D loss: 0.132052, acc.: 96.09%] [G loss: 2.797418]\n",
      "epoch:39 step:30817 [D loss: 0.047475, acc.: 100.00%] [G loss: 2.569314]\n",
      "epoch:39 step:30818 [D loss: 0.306488, acc.: 89.84%] [G loss: 2.504990]\n",
      "epoch:39 step:30819 [D loss: 0.034662, acc.: 99.22%] [G loss: 2.134387]\n",
      "epoch:39 step:30820 [D loss: 0.071784, acc.: 99.22%] [G loss: 2.005970]\n",
      "epoch:39 step:30821 [D loss: 0.120468, acc.: 98.44%] [G loss: 2.539703]\n",
      "epoch:39 step:30822 [D loss: 0.171216, acc.: 96.09%] [G loss: 2.250576]\n",
      "epoch:39 step:30823 [D loss: 0.172855, acc.: 96.09%] [G loss: 1.865093]\n",
      "epoch:39 step:30824 [D loss: 0.155929, acc.: 95.31%] [G loss: 4.186675]\n",
      "epoch:39 step:30825 [D loss: 0.440645, acc.: 75.00%] [G loss: 1.491022]\n",
      "epoch:39 step:30826 [D loss: 0.289644, acc.: 87.50%] [G loss: 4.659712]\n",
      "epoch:39 step:30827 [D loss: 0.109619, acc.: 96.09%] [G loss: 4.451197]\n",
      "epoch:39 step:30828 [D loss: 0.246370, acc.: 89.06%] [G loss: 3.088293]\n",
      "epoch:39 step:30829 [D loss: 0.162371, acc.: 95.31%] [G loss: 1.624882]\n",
      "epoch:39 step:30830 [D loss: 0.270511, acc.: 87.50%] [G loss: 4.216815]\n",
      "epoch:39 step:30831 [D loss: 0.032965, acc.: 99.22%] [G loss: 4.894183]\n",
      "epoch:39 step:30832 [D loss: 1.079990, acc.: 57.03%] [G loss: 1.828974]\n",
      "epoch:39 step:30833 [D loss: 0.200762, acc.: 92.19%] [G loss: 2.976529]\n",
      "epoch:39 step:30834 [D loss: 0.027451, acc.: 100.00%] [G loss: 3.489672]\n",
      "epoch:39 step:30835 [D loss: 0.050472, acc.: 99.22%] [G loss: 3.512655]\n",
      "epoch:39 step:30836 [D loss: 0.097943, acc.: 97.66%] [G loss: 2.896509]\n",
      "epoch:39 step:30837 [D loss: 0.086629, acc.: 98.44%] [G loss: 2.570276]\n",
      "epoch:39 step:30838 [D loss: 0.048182, acc.: 100.00%] [G loss: 3.376764]\n",
      "epoch:39 step:30839 [D loss: 0.085046, acc.: 99.22%] [G loss: 2.431273]\n",
      "epoch:39 step:30840 [D loss: 0.037866, acc.: 100.00%] [G loss: 3.145452]\n",
      "epoch:39 step:30841 [D loss: 0.104754, acc.: 96.09%] [G loss: 2.593341]\n",
      "epoch:39 step:30842 [D loss: 0.121040, acc.: 95.31%] [G loss: 3.018329]\n",
      "epoch:39 step:30843 [D loss: 0.078522, acc.: 97.66%] [G loss: 2.815350]\n",
      "epoch:39 step:30844 [D loss: 0.185552, acc.: 93.75%] [G loss: 3.739949]\n",
      "epoch:39 step:30845 [D loss: 0.082554, acc.: 97.66%] [G loss: 3.659611]\n",
      "epoch:39 step:30846 [D loss: 0.043914, acc.: 100.00%] [G loss: 3.760951]\n",
      "epoch:39 step:30847 [D loss: 0.065924, acc.: 99.22%] [G loss: 3.277023]\n",
      "epoch:39 step:30848 [D loss: 0.380473, acc.: 81.25%] [G loss: 4.970430]\n",
      "epoch:39 step:30849 [D loss: 0.419827, acc.: 79.69%] [G loss: 3.710342]\n",
      "epoch:39 step:30850 [D loss: 0.199372, acc.: 89.84%] [G loss: 3.644580]\n",
      "epoch:39 step:30851 [D loss: 0.016449, acc.: 100.00%] [G loss: 3.873982]\n",
      "epoch:39 step:30852 [D loss: 0.057573, acc.: 99.22%] [G loss: 3.418161]\n",
      "epoch:39 step:30853 [D loss: 0.013584, acc.: 100.00%] [G loss: 3.532691]\n",
      "epoch:39 step:30854 [D loss: 0.044936, acc.: 100.00%] [G loss: 3.384210]\n",
      "epoch:39 step:30855 [D loss: 0.054812, acc.: 99.22%] [G loss: 3.164486]\n",
      "epoch:39 step:30856 [D loss: 0.097882, acc.: 96.88%] [G loss: 1.870644]\n",
      "epoch:39 step:30857 [D loss: 0.028940, acc.: 100.00%] [G loss: 1.225966]\n",
      "epoch:39 step:30858 [D loss: 0.097083, acc.: 98.44%] [G loss: 2.494760]\n",
      "epoch:39 step:30859 [D loss: 0.140266, acc.: 96.88%] [G loss: 3.172871]\n",
      "epoch:39 step:30860 [D loss: 0.009075, acc.: 100.00%] [G loss: 3.777968]\n",
      "epoch:39 step:30861 [D loss: 0.055053, acc.: 99.22%] [G loss: 2.334704]\n",
      "epoch:39 step:30862 [D loss: 0.331966, acc.: 85.16%] [G loss: 5.167809]\n",
      "epoch:39 step:30863 [D loss: 1.064564, acc.: 57.81%] [G loss: 3.546137]\n",
      "epoch:39 step:30864 [D loss: 0.144786, acc.: 96.09%] [G loss: 1.695857]\n",
      "epoch:39 step:30865 [D loss: 0.182321, acc.: 93.75%] [G loss: 3.661938]\n",
      "epoch:39 step:30866 [D loss: 0.029101, acc.: 99.22%] [G loss: 4.468578]\n",
      "epoch:39 step:30867 [D loss: 0.089664, acc.: 96.88%] [G loss: 3.749552]\n",
      "epoch:39 step:30868 [D loss: 0.617755, acc.: 73.44%] [G loss: 4.180747]\n",
      "epoch:39 step:30869 [D loss: 0.032533, acc.: 99.22%] [G loss: 5.050156]\n",
      "epoch:39 step:30870 [D loss: 0.686677, acc.: 70.31%] [G loss: 1.947201]\n",
      "epoch:39 step:30871 [D loss: 0.606823, acc.: 67.97%] [G loss: 4.507961]\n",
      "epoch:39 step:30872 [D loss: 0.468998, acc.: 75.00%] [G loss: 4.980196]\n",
      "epoch:39 step:30873 [D loss: 0.379411, acc.: 79.69%] [G loss: 3.700972]\n",
      "epoch:39 step:30874 [D loss: 0.041188, acc.: 100.00%] [G loss: 3.163285]\n",
      "epoch:39 step:30875 [D loss: 0.032449, acc.: 100.00%] [G loss: 3.073238]\n",
      "epoch:39 step:30876 [D loss: 0.064152, acc.: 100.00%] [G loss: 3.206031]\n",
      "epoch:39 step:30877 [D loss: 0.036447, acc.: 99.22%] [G loss: 3.440525]\n",
      "epoch:39 step:30878 [D loss: 0.034306, acc.: 100.00%] [G loss: 3.773328]\n",
      "epoch:39 step:30879 [D loss: 0.040497, acc.: 100.00%] [G loss: 3.429716]\n",
      "epoch:39 step:30880 [D loss: 0.066396, acc.: 100.00%] [G loss: 2.959605]\n",
      "epoch:39 step:30881 [D loss: 0.046630, acc.: 100.00%] [G loss: 2.940564]\n",
      "epoch:39 step:30882 [D loss: 0.077197, acc.: 98.44%] [G loss: 3.542415]\n",
      "epoch:39 step:30883 [D loss: 0.027325, acc.: 100.00%] [G loss: 3.027274]\n",
      "epoch:39 step:30884 [D loss: 0.044403, acc.: 100.00%] [G loss: 3.180686]\n",
      "epoch:39 step:30885 [D loss: 0.040097, acc.: 100.00%] [G loss: 2.481032]\n",
      "epoch:39 step:30886 [D loss: 0.042898, acc.: 100.00%] [G loss: 3.482543]\n",
      "epoch:39 step:30887 [D loss: 0.084637, acc.: 97.66%] [G loss: 3.847355]\n",
      "epoch:39 step:30888 [D loss: 0.040302, acc.: 100.00%] [G loss: 3.805132]\n",
      "epoch:39 step:30889 [D loss: 0.038156, acc.: 99.22%] [G loss: 3.880104]\n",
      "epoch:39 step:30890 [D loss: 0.087634, acc.: 98.44%] [G loss: 2.940698]\n",
      "epoch:39 step:30891 [D loss: 0.070009, acc.: 99.22%] [G loss: 3.062161]\n",
      "epoch:39 step:30892 [D loss: 0.028432, acc.: 100.00%] [G loss: 3.619179]\n",
      "epoch:39 step:30893 [D loss: 0.162721, acc.: 92.19%] [G loss: 4.223833]\n",
      "epoch:39 step:30894 [D loss: 0.055298, acc.: 97.66%] [G loss: 3.146984]\n",
      "epoch:39 step:30895 [D loss: 0.032563, acc.: 100.00%] [G loss: 3.639586]\n",
      "epoch:39 step:30896 [D loss: 0.040046, acc.: 100.00%] [G loss: 2.537935]\n",
      "epoch:39 step:30897 [D loss: 0.024166, acc.: 99.22%] [G loss: 2.633532]\n",
      "epoch:39 step:30898 [D loss: 0.090987, acc.: 98.44%] [G loss: 3.873380]\n",
      "epoch:39 step:30899 [D loss: 0.111010, acc.: 96.09%] [G loss: 3.136035]\n",
      "epoch:39 step:30900 [D loss: 0.480565, acc.: 76.56%] [G loss: 5.844513]\n",
      "epoch:39 step:30901 [D loss: 1.105783, acc.: 56.25%] [G loss: 4.830196]\n",
      "epoch:39 step:30902 [D loss: 0.030843, acc.: 100.00%] [G loss: 3.856160]\n",
      "epoch:39 step:30903 [D loss: 0.098611, acc.: 97.66%] [G loss: 4.401972]\n",
      "epoch:39 step:30904 [D loss: 0.030685, acc.: 99.22%] [G loss: 4.175097]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:39 step:30905 [D loss: 0.083492, acc.: 99.22%] [G loss: 4.004788]\n",
      "epoch:39 step:30906 [D loss: 0.021346, acc.: 100.00%] [G loss: 3.310218]\n",
      "epoch:39 step:30907 [D loss: 0.024550, acc.: 100.00%] [G loss: 3.223021]\n",
      "epoch:39 step:30908 [D loss: 0.058882, acc.: 99.22%] [G loss: 3.089848]\n",
      "epoch:39 step:30909 [D loss: 0.028415, acc.: 100.00%] [G loss: 3.742369]\n",
      "epoch:39 step:30910 [D loss: 0.020864, acc.: 100.00%] [G loss: 3.902812]\n",
      "epoch:39 step:30911 [D loss: 0.219275, acc.: 89.84%] [G loss: 1.993374]\n",
      "epoch:39 step:30912 [D loss: 0.333034, acc.: 83.59%] [G loss: 5.129419]\n",
      "epoch:39 step:30913 [D loss: 0.072262, acc.: 97.66%] [G loss: 5.281579]\n",
      "epoch:39 step:30914 [D loss: 0.399003, acc.: 80.47%] [G loss: 3.190499]\n",
      "epoch:39 step:30915 [D loss: 0.124675, acc.: 96.88%] [G loss: 3.249765]\n",
      "epoch:39 step:30916 [D loss: 0.057436, acc.: 99.22%] [G loss: 3.361392]\n",
      "epoch:39 step:30917 [D loss: 0.023506, acc.: 100.00%] [G loss: 4.412026]\n",
      "epoch:39 step:30918 [D loss: 0.107954, acc.: 98.44%] [G loss: 3.892347]\n",
      "epoch:39 step:30919 [D loss: 0.102592, acc.: 98.44%] [G loss: 3.998537]\n",
      "epoch:39 step:30920 [D loss: 0.043292, acc.: 99.22%] [G loss: 3.801699]\n",
      "epoch:39 step:30921 [D loss: 0.011575, acc.: 100.00%] [G loss: 4.254784]\n",
      "epoch:39 step:30922 [D loss: 0.040398, acc.: 100.00%] [G loss: 3.770705]\n",
      "epoch:39 step:30923 [D loss: 0.020350, acc.: 100.00%] [G loss: 3.455812]\n",
      "epoch:39 step:30924 [D loss: 0.060054, acc.: 100.00%] [G loss: 3.583010]\n",
      "epoch:39 step:30925 [D loss: 0.070744, acc.: 98.44%] [G loss: 2.561742]\n",
      "epoch:39 step:30926 [D loss: 0.373214, acc.: 82.03%] [G loss: 4.202712]\n",
      "epoch:39 step:30927 [D loss: 1.370608, acc.: 41.41%] [G loss: 6.134749]\n",
      "epoch:39 step:30928 [D loss: 0.808257, acc.: 64.06%] [G loss: 5.481653]\n",
      "epoch:39 step:30929 [D loss: 0.199289, acc.: 93.75%] [G loss: 4.213351]\n",
      "epoch:39 step:30930 [D loss: 0.056696, acc.: 98.44%] [G loss: 3.752953]\n",
      "epoch:39 step:30931 [D loss: 0.030641, acc.: 100.00%] [G loss: 4.168916]\n",
      "epoch:39 step:30932 [D loss: 0.015661, acc.: 100.00%] [G loss: 4.339809]\n",
      "epoch:39 step:30933 [D loss: 0.018422, acc.: 100.00%] [G loss: 4.011398]\n",
      "epoch:39 step:30934 [D loss: 0.101707, acc.: 96.09%] [G loss: 3.781363]\n",
      "epoch:39 step:30935 [D loss: 0.041803, acc.: 98.44%] [G loss: 4.004130]\n",
      "epoch:39 step:30936 [D loss: 0.058633, acc.: 100.00%] [G loss: 4.284738]\n",
      "epoch:39 step:30937 [D loss: 0.057158, acc.: 99.22%] [G loss: 3.586299]\n",
      "epoch:39 step:30938 [D loss: 0.050993, acc.: 100.00%] [G loss: 3.924113]\n",
      "epoch:39 step:30939 [D loss: 0.055097, acc.: 98.44%] [G loss: 3.758084]\n",
      "epoch:39 step:30940 [D loss: 0.042382, acc.: 99.22%] [G loss: 4.492322]\n",
      "epoch:39 step:30941 [D loss: 0.061855, acc.: 98.44%] [G loss: 4.094651]\n",
      "epoch:39 step:30942 [D loss: 0.237869, acc.: 91.41%] [G loss: 2.483539]\n",
      "epoch:39 step:30943 [D loss: 0.144911, acc.: 92.97%] [G loss: 4.322499]\n",
      "epoch:39 step:30944 [D loss: 0.041360, acc.: 99.22%] [G loss: 4.850514]\n",
      "epoch:39 step:30945 [D loss: 0.039281, acc.: 99.22%] [G loss: 4.556017]\n",
      "epoch:39 step:30946 [D loss: 0.025063, acc.: 100.00%] [G loss: 4.494747]\n",
      "epoch:39 step:30947 [D loss: 0.026524, acc.: 100.00%] [G loss: 4.012599]\n",
      "epoch:39 step:30948 [D loss: 0.015356, acc.: 100.00%] [G loss: 3.325052]\n",
      "epoch:39 step:30949 [D loss: 0.028722, acc.: 100.00%] [G loss: 3.230405]\n",
      "epoch:39 step:30950 [D loss: 0.044557, acc.: 99.22%] [G loss: 3.240131]\n",
      "epoch:39 step:30951 [D loss: 0.084860, acc.: 98.44%] [G loss: 2.553899]\n",
      "epoch:39 step:30952 [D loss: 0.060885, acc.: 97.66%] [G loss: 2.870410]\n",
      "epoch:39 step:30953 [D loss: 0.036483, acc.: 98.44%] [G loss: 2.876817]\n",
      "epoch:39 step:30954 [D loss: 0.050211, acc.: 100.00%] [G loss: 2.588834]\n",
      "epoch:39 step:30955 [D loss: 0.070063, acc.: 99.22%] [G loss: 3.800410]\n",
      "epoch:39 step:30956 [D loss: 1.058100, acc.: 50.00%] [G loss: 5.969433]\n",
      "epoch:39 step:30957 [D loss: 1.249219, acc.: 57.81%] [G loss: 4.880034]\n",
      "epoch:39 step:30958 [D loss: 0.175231, acc.: 92.97%] [G loss: 3.356567]\n",
      "epoch:39 step:30959 [D loss: 0.072080, acc.: 100.00%] [G loss: 3.088842]\n",
      "epoch:39 step:30960 [D loss: 0.090418, acc.: 98.44%] [G loss: 3.809696]\n",
      "epoch:39 step:30961 [D loss: 0.039908, acc.: 100.00%] [G loss: 3.826210]\n",
      "epoch:39 step:30962 [D loss: 0.093755, acc.: 96.88%] [G loss: 3.806542]\n",
      "epoch:39 step:30963 [D loss: 0.095515, acc.: 97.66%] [G loss: 4.162886]\n",
      "epoch:39 step:30964 [D loss: 0.014915, acc.: 100.00%] [G loss: 4.306118]\n",
      "epoch:39 step:30965 [D loss: 0.046175, acc.: 99.22%] [G loss: 4.316868]\n",
      "epoch:39 step:30966 [D loss: 0.038757, acc.: 99.22%] [G loss: 3.193624]\n",
      "epoch:39 step:30967 [D loss: 0.058718, acc.: 100.00%] [G loss: 3.487725]\n",
      "epoch:39 step:30968 [D loss: 2.033730, acc.: 28.12%] [G loss: 5.693390]\n",
      "epoch:39 step:30969 [D loss: 0.814826, acc.: 65.62%] [G loss: 4.936222]\n",
      "epoch:39 step:30970 [D loss: 0.755360, acc.: 69.53%] [G loss: 2.050020]\n",
      "epoch:39 step:30971 [D loss: 0.392427, acc.: 83.59%] [G loss: 3.864192]\n",
      "epoch:39 step:30972 [D loss: 0.096080, acc.: 95.31%] [G loss: 4.459190]\n",
      "epoch:39 step:30973 [D loss: 0.220356, acc.: 91.41%] [G loss: 3.578490]\n",
      "epoch:39 step:30974 [D loss: 0.114533, acc.: 96.88%] [G loss: 3.615693]\n",
      "epoch:39 step:30975 [D loss: 0.045216, acc.: 99.22%] [G loss: 3.440281]\n",
      "epoch:39 step:30976 [D loss: 0.035305, acc.: 100.00%] [G loss: 2.976919]\n",
      "epoch:39 step:30977 [D loss: 0.048308, acc.: 99.22%] [G loss: 3.491605]\n",
      "epoch:39 step:30978 [D loss: 0.138396, acc.: 95.31%] [G loss: 3.128367]\n",
      "epoch:39 step:30979 [D loss: 0.124155, acc.: 96.88%] [G loss: 3.553756]\n",
      "epoch:39 step:30980 [D loss: 0.046663, acc.: 100.00%] [G loss: 3.442054]\n",
      "epoch:39 step:30981 [D loss: 0.050858, acc.: 100.00%] [G loss: 3.367073]\n",
      "epoch:39 step:30982 [D loss: 0.104677, acc.: 97.66%] [G loss: 3.552000]\n",
      "epoch:39 step:30983 [D loss: 0.088431, acc.: 97.66%] [G loss: 2.705061]\n",
      "epoch:39 step:30984 [D loss: 0.054281, acc.: 99.22%] [G loss: 3.023999]\n",
      "epoch:39 step:30985 [D loss: 0.177013, acc.: 92.97%] [G loss: 3.730798]\n",
      "epoch:39 step:30986 [D loss: 0.141811, acc.: 93.75%] [G loss: 3.437180]\n",
      "epoch:39 step:30987 [D loss: 0.179686, acc.: 92.19%] [G loss: 2.949067]\n",
      "epoch:39 step:30988 [D loss: 0.034696, acc.: 100.00%] [G loss: 3.679490]\n",
      "epoch:39 step:30989 [D loss: 0.061347, acc.: 99.22%] [G loss: 3.666348]\n",
      "epoch:39 step:30990 [D loss: 0.023734, acc.: 100.00%] [G loss: 3.535600]\n",
      "epoch:39 step:30991 [D loss: 0.088229, acc.: 98.44%] [G loss: 2.198130]\n",
      "epoch:39 step:30992 [D loss: 0.159281, acc.: 95.31%] [G loss: 3.201503]\n",
      "epoch:39 step:30993 [D loss: 0.040052, acc.: 99.22%] [G loss: 2.838732]\n",
      "epoch:39 step:30994 [D loss: 0.263897, acc.: 93.75%] [G loss: 1.279930]\n",
      "epoch:39 step:30995 [D loss: 0.047850, acc.: 100.00%] [G loss: 2.484526]\n",
      "epoch:39 step:30996 [D loss: 0.071502, acc.: 98.44%] [G loss: 1.290216]\n",
      "epoch:39 step:30997 [D loss: 0.166182, acc.: 93.75%] [G loss: 4.858117]\n",
      "epoch:39 step:30998 [D loss: 0.122044, acc.: 95.31%] [G loss: 4.565904]\n",
      "epoch:39 step:30999 [D loss: 0.060083, acc.: 99.22%] [G loss: 2.744988]\n",
      "epoch:39 step:31000 [D loss: 0.014499, acc.: 100.00%] [G loss: 1.962072]\n",
      "##############\n",
      "[2.10227019 0.98521423 0.91982989 0.9657148  1.11337588 0.99669117\n",
      " 0.87247685 0.89182856 0.81696321 1.00900669]\n",
      "##########\n",
      "epoch:39 step:31001 [D loss: 0.093849, acc.: 97.66%] [G loss: 3.563131]\n",
      "epoch:39 step:31002 [D loss: 0.092546, acc.: 99.22%] [G loss: 3.684594]\n",
      "epoch:39 step:31003 [D loss: 0.145593, acc.: 96.09%] [G loss: 3.804418]\n",
      "epoch:39 step:31004 [D loss: 0.031564, acc.: 99.22%] [G loss: 3.395324]\n",
      "epoch:39 step:31005 [D loss: 0.049046, acc.: 97.66%] [G loss: 2.596423]\n",
      "epoch:39 step:31006 [D loss: 0.141161, acc.: 93.75%] [G loss: 5.520197]\n",
      "epoch:39 step:31007 [D loss: 0.176242, acc.: 92.19%] [G loss: 4.163221]\n",
      "epoch:39 step:31008 [D loss: 0.039503, acc.: 98.44%] [G loss: 4.273823]\n",
      "epoch:39 step:31009 [D loss: 0.111478, acc.: 96.09%] [G loss: 5.174873]\n",
      "epoch:39 step:31010 [D loss: 0.234001, acc.: 91.41%] [G loss: 3.005971]\n",
      "epoch:39 step:31011 [D loss: 0.041191, acc.: 100.00%] [G loss: 3.485333]\n",
      "epoch:39 step:31012 [D loss: 0.085270, acc.: 97.66%] [G loss: 4.542215]\n",
      "epoch:39 step:31013 [D loss: 0.058611, acc.: 99.22%] [G loss: 4.834625]\n",
      "epoch:39 step:31014 [D loss: 0.054083, acc.: 99.22%] [G loss: 4.587216]\n",
      "epoch:39 step:31015 [D loss: 0.080118, acc.: 99.22%] [G loss: 3.329085]\n",
      "epoch:39 step:31016 [D loss: 0.032783, acc.: 100.00%] [G loss: 3.045908]\n",
      "epoch:39 step:31017 [D loss: 0.052200, acc.: 100.00%] [G loss: 3.948601]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:39 step:31018 [D loss: 0.031590, acc.: 99.22%] [G loss: 4.454671]\n",
      "epoch:39 step:31019 [D loss: 0.109877, acc.: 98.44%] [G loss: 3.530381]\n",
      "epoch:39 step:31020 [D loss: 0.107096, acc.: 96.09%] [G loss: 3.738601]\n",
      "epoch:39 step:31021 [D loss: 0.161668, acc.: 94.53%] [G loss: 5.153275]\n",
      "epoch:39 step:31022 [D loss: 0.013238, acc.: 100.00%] [G loss: 5.292478]\n",
      "epoch:39 step:31023 [D loss: 0.365992, acc.: 85.16%] [G loss: 4.578615]\n",
      "epoch:39 step:31024 [D loss: 0.021031, acc.: 100.00%] [G loss: 4.783252]\n",
      "epoch:39 step:31025 [D loss: 0.085334, acc.: 98.44%] [G loss: 4.715816]\n",
      "epoch:39 step:31026 [D loss: 0.021073, acc.: 100.00%] [G loss: 4.412724]\n",
      "epoch:39 step:31027 [D loss: 0.068356, acc.: 99.22%] [G loss: 4.342145]\n",
      "epoch:39 step:31028 [D loss: 0.186202, acc.: 92.97%] [G loss: 4.036856]\n",
      "epoch:39 step:31029 [D loss: 0.047192, acc.: 99.22%] [G loss: 4.664110]\n",
      "epoch:39 step:31030 [D loss: 0.115577, acc.: 95.31%] [G loss: 3.011735]\n",
      "epoch:39 step:31031 [D loss: 0.018998, acc.: 100.00%] [G loss: 3.877217]\n",
      "epoch:39 step:31032 [D loss: 0.157829, acc.: 93.75%] [G loss: 5.274606]\n",
      "epoch:39 step:31033 [D loss: 0.243072, acc.: 89.06%] [G loss: 3.641963]\n",
      "epoch:39 step:31034 [D loss: 0.258546, acc.: 91.41%] [G loss: 4.901381]\n",
      "epoch:39 step:31035 [D loss: 0.014352, acc.: 100.00%] [G loss: 6.612605]\n",
      "epoch:39 step:31036 [D loss: 0.249820, acc.: 90.62%] [G loss: 2.512795]\n",
      "epoch:39 step:31037 [D loss: 0.244075, acc.: 90.62%] [G loss: 6.686849]\n",
      "epoch:39 step:31038 [D loss: 0.008479, acc.: 100.00%] [G loss: 7.287061]\n",
      "epoch:39 step:31039 [D loss: 0.392374, acc.: 82.81%] [G loss: 4.253874]\n",
      "epoch:39 step:31040 [D loss: 0.324326, acc.: 86.72%] [G loss: 7.708940]\n",
      "epoch:39 step:31041 [D loss: 0.195185, acc.: 90.62%] [G loss: 6.916503]\n",
      "epoch:39 step:31042 [D loss: 0.055217, acc.: 98.44%] [G loss: 6.119847]\n",
      "epoch:39 step:31043 [D loss: 0.018748, acc.: 100.00%] [G loss: 5.948823]\n",
      "epoch:39 step:31044 [D loss: 0.024749, acc.: 99.22%] [G loss: 4.837150]\n",
      "epoch:39 step:31045 [D loss: 0.022434, acc.: 100.00%] [G loss: 4.269684]\n",
      "epoch:39 step:31046 [D loss: 0.009196, acc.: 100.00%] [G loss: 5.041337]\n",
      "epoch:39 step:31047 [D loss: 0.033978, acc.: 99.22%] [G loss: 4.343585]\n",
      "epoch:39 step:31048 [D loss: 0.029727, acc.: 100.00%] [G loss: 3.757305]\n",
      "epoch:39 step:31049 [D loss: 0.036651, acc.: 99.22%] [G loss: 4.015458]\n",
      "epoch:39 step:31050 [D loss: 0.021182, acc.: 100.00%] [G loss: 4.063044]\n",
      "epoch:39 step:31051 [D loss: 0.119441, acc.: 98.44%] [G loss: 5.581550]\n",
      "epoch:39 step:31052 [D loss: 0.045222, acc.: 97.66%] [G loss: 5.656128]\n",
      "epoch:39 step:31053 [D loss: 0.109631, acc.: 96.09%] [G loss: 3.205624]\n",
      "epoch:39 step:31054 [D loss: 0.040252, acc.: 99.22%] [G loss: 3.005719]\n",
      "epoch:39 step:31055 [D loss: 0.064126, acc.: 99.22%] [G loss: 2.790968]\n",
      "epoch:39 step:31056 [D loss: 0.040916, acc.: 99.22%] [G loss: 3.742588]\n",
      "epoch:39 step:31057 [D loss: 0.024555, acc.: 100.00%] [G loss: 4.063001]\n",
      "epoch:39 step:31058 [D loss: 0.031719, acc.: 100.00%] [G loss: 4.798315]\n",
      "epoch:39 step:31059 [D loss: 0.321902, acc.: 87.50%] [G loss: 2.054267]\n",
      "epoch:39 step:31060 [D loss: 0.176323, acc.: 92.19%] [G loss: 6.536400]\n",
      "epoch:39 step:31061 [D loss: 0.333401, acc.: 82.03%] [G loss: 3.341786]\n",
      "epoch:39 step:31062 [D loss: 0.354925, acc.: 86.72%] [G loss: 7.110791]\n",
      "epoch:39 step:31063 [D loss: 0.826966, acc.: 67.19%] [G loss: 4.628316]\n",
      "epoch:39 step:31064 [D loss: 0.111941, acc.: 95.31%] [G loss: 3.765976]\n",
      "epoch:39 step:31065 [D loss: 0.004381, acc.: 100.00%] [G loss: 5.440833]\n",
      "epoch:39 step:31066 [D loss: 0.006266, acc.: 100.00%] [G loss: 4.272413]\n",
      "epoch:39 step:31067 [D loss: 0.007094, acc.: 100.00%] [G loss: 3.374673]\n",
      "epoch:39 step:31068 [D loss: 0.125508, acc.: 94.53%] [G loss: 4.482470]\n",
      "epoch:39 step:31069 [D loss: 0.034427, acc.: 99.22%] [G loss: 4.727743]\n",
      "epoch:39 step:31070 [D loss: 0.021386, acc.: 100.00%] [G loss: 4.745377]\n",
      "epoch:39 step:31071 [D loss: 0.033204, acc.: 100.00%] [G loss: 4.611861]\n",
      "epoch:39 step:31072 [D loss: 0.301539, acc.: 85.94%] [G loss: 5.762910]\n",
      "epoch:39 step:31073 [D loss: 0.008056, acc.: 100.00%] [G loss: 6.910951]\n",
      "epoch:39 step:31074 [D loss: 0.094468, acc.: 94.53%] [G loss: 5.860482]\n",
      "epoch:39 step:31075 [D loss: 0.233539, acc.: 90.62%] [G loss: 4.131907]\n",
      "epoch:39 step:31076 [D loss: 0.014581, acc.: 100.00%] [G loss: 4.384769]\n",
      "epoch:39 step:31077 [D loss: 0.009097, acc.: 100.00%] [G loss: 4.892103]\n",
      "epoch:39 step:31078 [D loss: 0.014078, acc.: 100.00%] [G loss: 3.615050]\n",
      "epoch:39 step:31079 [D loss: 0.067708, acc.: 99.22%] [G loss: 5.287796]\n",
      "epoch:39 step:31080 [D loss: 0.048988, acc.: 97.66%] [G loss: 4.656851]\n",
      "epoch:39 step:31081 [D loss: 0.080248, acc.: 96.88%] [G loss: 2.996780]\n",
      "epoch:39 step:31082 [D loss: 0.088160, acc.: 98.44%] [G loss: 4.179799]\n",
      "epoch:39 step:31083 [D loss: 0.006622, acc.: 100.00%] [G loss: 5.169703]\n",
      "epoch:39 step:31084 [D loss: 0.064145, acc.: 98.44%] [G loss: 2.397837]\n",
      "epoch:39 step:31085 [D loss: 0.128542, acc.: 95.31%] [G loss: 5.576478]\n",
      "epoch:39 step:31086 [D loss: 0.131912, acc.: 93.75%] [G loss: 4.393873]\n",
      "epoch:39 step:31087 [D loss: 0.041556, acc.: 100.00%] [G loss: 4.073709]\n",
      "epoch:39 step:31088 [D loss: 0.014311, acc.: 100.00%] [G loss: 4.004295]\n",
      "epoch:39 step:31089 [D loss: 0.046367, acc.: 99.22%] [G loss: 3.710122]\n",
      "epoch:39 step:31090 [D loss: 0.006264, acc.: 100.00%] [G loss: 4.752392]\n",
      "epoch:39 step:31091 [D loss: 0.042115, acc.: 98.44%] [G loss: 2.841893]\n",
      "epoch:39 step:31092 [D loss: 0.219953, acc.: 91.41%] [G loss: 6.697540]\n",
      "epoch:39 step:31093 [D loss: 0.580968, acc.: 75.78%] [G loss: 1.485053]\n",
      "epoch:39 step:31094 [D loss: 1.263603, acc.: 65.62%] [G loss: 7.977411]\n",
      "epoch:39 step:31095 [D loss: 2.279608, acc.: 50.78%] [G loss: 6.968386]\n",
      "epoch:39 step:31096 [D loss: 1.505965, acc.: 52.34%] [G loss: 4.588705]\n",
      "epoch:39 step:31097 [D loss: 0.332942, acc.: 85.94%] [G loss: 3.608023]\n",
      "epoch:39 step:31098 [D loss: 0.091694, acc.: 98.44%] [G loss: 3.471416]\n",
      "epoch:39 step:31099 [D loss: 0.047180, acc.: 99.22%] [G loss: 3.169733]\n",
      "epoch:39 step:31100 [D loss: 0.049111, acc.: 99.22%] [G loss: 3.292667]\n",
      "epoch:39 step:31101 [D loss: 0.100774, acc.: 95.31%] [G loss: 2.803421]\n",
      "epoch:39 step:31102 [D loss: 0.101871, acc.: 96.88%] [G loss: 3.534527]\n",
      "epoch:39 step:31103 [D loss: 0.038130, acc.: 100.00%] [G loss: 3.400535]\n",
      "epoch:39 step:31104 [D loss: 0.062230, acc.: 100.00%] [G loss: 3.685060]\n",
      "epoch:39 step:31105 [D loss: 0.167579, acc.: 94.53%] [G loss: 2.735472]\n",
      "epoch:39 step:31106 [D loss: 0.165935, acc.: 92.97%] [G loss: 3.545965]\n",
      "epoch:39 step:31107 [D loss: 0.073155, acc.: 97.66%] [G loss: 4.596686]\n",
      "epoch:39 step:31108 [D loss: 0.118829, acc.: 96.09%] [G loss: 2.905951]\n",
      "epoch:39 step:31109 [D loss: 0.047061, acc.: 100.00%] [G loss: 3.359066]\n",
      "epoch:39 step:31110 [D loss: 0.024890, acc.: 100.00%] [G loss: 2.863862]\n",
      "epoch:39 step:31111 [D loss: 0.078539, acc.: 98.44%] [G loss: 3.307759]\n",
      "epoch:39 step:31112 [D loss: 0.130795, acc.: 96.88%] [G loss: 4.348932]\n",
      "epoch:39 step:31113 [D loss: 0.147003, acc.: 92.19%] [G loss: 3.905527]\n",
      "epoch:39 step:31114 [D loss: 0.068888, acc.: 97.66%] [G loss: 3.018964]\n",
      "epoch:39 step:31115 [D loss: 0.015234, acc.: 100.00%] [G loss: 3.126227]\n",
      "epoch:39 step:31116 [D loss: 0.030605, acc.: 100.00%] [G loss: 3.101063]\n",
      "epoch:39 step:31117 [D loss: 0.059468, acc.: 99.22%] [G loss: 2.178118]\n",
      "epoch:39 step:31118 [D loss: 0.062785, acc.: 100.00%] [G loss: 3.582689]\n",
      "epoch:39 step:31119 [D loss: 0.020150, acc.: 99.22%] [G loss: 3.370967]\n",
      "epoch:39 step:31120 [D loss: 0.170476, acc.: 93.75%] [G loss: 1.459183]\n",
      "epoch:39 step:31121 [D loss: 0.408973, acc.: 76.56%] [G loss: 6.972545]\n",
      "epoch:39 step:31122 [D loss: 0.130476, acc.: 95.31%] [G loss: 7.337663]\n",
      "epoch:39 step:31123 [D loss: 0.268523, acc.: 85.94%] [G loss: 6.582712]\n",
      "epoch:39 step:31124 [D loss: 0.003354, acc.: 100.00%] [G loss: 5.749527]\n",
      "epoch:39 step:31125 [D loss: 0.001867, acc.: 100.00%] [G loss: 5.686300]\n",
      "epoch:39 step:31126 [D loss: 0.016286, acc.: 100.00%] [G loss: 5.265852]\n",
      "epoch:39 step:31127 [D loss: 0.006248, acc.: 100.00%] [G loss: 4.993125]\n",
      "epoch:39 step:31128 [D loss: 0.006458, acc.: 100.00%] [G loss: 4.564240]\n",
      "epoch:39 step:31129 [D loss: 0.006805, acc.: 100.00%] [G loss: 3.730145]\n",
      "epoch:39 step:31130 [D loss: 0.010947, acc.: 100.00%] [G loss: 4.063441]\n",
      "epoch:39 step:31131 [D loss: 0.041153, acc.: 100.00%] [G loss: 3.567958]\n",
      "epoch:39 step:31132 [D loss: 0.009291, acc.: 100.00%] [G loss: 4.060312]\n",
      "epoch:39 step:31133 [D loss: 0.015738, acc.: 100.00%] [G loss: 3.475397]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:39 step:31134 [D loss: 0.016307, acc.: 100.00%] [G loss: 2.901975]\n",
      "epoch:39 step:31135 [D loss: 0.008295, acc.: 100.00%] [G loss: 3.407716]\n",
      "epoch:39 step:31136 [D loss: 0.003269, acc.: 100.00%] [G loss: 2.833314]\n",
      "epoch:39 step:31137 [D loss: 0.025093, acc.: 100.00%] [G loss: 3.057386]\n",
      "epoch:39 step:31138 [D loss: 0.018778, acc.: 100.00%] [G loss: 4.026331]\n",
      "epoch:39 step:31139 [D loss: 0.008552, acc.: 100.00%] [G loss: 3.004219]\n",
      "epoch:39 step:31140 [D loss: 0.057453, acc.: 100.00%] [G loss: 3.611228]\n",
      "epoch:39 step:31141 [D loss: 0.032080, acc.: 100.00%] [G loss: 3.826375]\n",
      "epoch:39 step:31142 [D loss: 0.034121, acc.: 99.22%] [G loss: 3.783036]\n",
      "epoch:39 step:31143 [D loss: 0.188450, acc.: 92.19%] [G loss: 5.837753]\n",
      "epoch:39 step:31144 [D loss: 1.709631, acc.: 51.56%] [G loss: 1.802032]\n",
      "epoch:39 step:31145 [D loss: 0.247575, acc.: 89.06%] [G loss: 3.926484]\n",
      "epoch:39 step:31146 [D loss: 0.083917, acc.: 95.31%] [G loss: 4.635771]\n",
      "epoch:39 step:31147 [D loss: 0.025795, acc.: 100.00%] [G loss: 4.479487]\n",
      "epoch:39 step:31148 [D loss: 0.012836, acc.: 100.00%] [G loss: 3.868653]\n",
      "epoch:39 step:31149 [D loss: 0.025785, acc.: 100.00%] [G loss: 4.055766]\n",
      "epoch:39 step:31150 [D loss: 0.023821, acc.: 99.22%] [G loss: 4.163028]\n",
      "epoch:39 step:31151 [D loss: 0.015753, acc.: 100.00%] [G loss: 3.990665]\n",
      "epoch:39 step:31152 [D loss: 0.049047, acc.: 99.22%] [G loss: 4.531313]\n",
      "epoch:39 step:31153 [D loss: 0.011148, acc.: 100.00%] [G loss: 4.254801]\n",
      "epoch:39 step:31154 [D loss: 0.009222, acc.: 100.00%] [G loss: 4.267755]\n",
      "epoch:39 step:31155 [D loss: 0.019788, acc.: 100.00%] [G loss: 3.796981]\n",
      "epoch:39 step:31156 [D loss: 0.015941, acc.: 100.00%] [G loss: 4.096359]\n",
      "epoch:39 step:31157 [D loss: 0.018001, acc.: 100.00%] [G loss: 3.408335]\n",
      "epoch:39 step:31158 [D loss: 0.025193, acc.: 100.00%] [G loss: 3.790489]\n",
      "epoch:39 step:31159 [D loss: 0.035060, acc.: 100.00%] [G loss: 4.032355]\n",
      "epoch:39 step:31160 [D loss: 0.028456, acc.: 99.22%] [G loss: 3.160469]\n",
      "epoch:39 step:31161 [D loss: 0.017844, acc.: 100.00%] [G loss: 4.240366]\n",
      "epoch:39 step:31162 [D loss: 0.062105, acc.: 98.44%] [G loss: 2.245857]\n",
      "epoch:39 step:31163 [D loss: 0.123147, acc.: 95.31%] [G loss: 4.448201]\n",
      "epoch:39 step:31164 [D loss: 0.180367, acc.: 94.53%] [G loss: 3.468529]\n",
      "epoch:39 step:31165 [D loss: 0.028177, acc.: 98.44%] [G loss: 3.929341]\n",
      "epoch:39 step:31166 [D loss: 0.005293, acc.: 100.00%] [G loss: 4.789275]\n",
      "epoch:39 step:31167 [D loss: 0.016203, acc.: 100.00%] [G loss: 4.354609]\n",
      "epoch:39 step:31168 [D loss: 0.004411, acc.: 100.00%] [G loss: 4.989519]\n",
      "epoch:39 step:31169 [D loss: 0.005930, acc.: 100.00%] [G loss: 4.209070]\n",
      "epoch:39 step:31170 [D loss: 0.076114, acc.: 98.44%] [G loss: 3.707908]\n",
      "epoch:39 step:31171 [D loss: 0.022866, acc.: 100.00%] [G loss: 4.978056]\n",
      "epoch:39 step:31172 [D loss: 3.637912, acc.: 24.22%] [G loss: 7.221942]\n",
      "epoch:39 step:31173 [D loss: 2.480905, acc.: 50.00%] [G loss: 6.074808]\n",
      "epoch:39 step:31174 [D loss: 2.329926, acc.: 50.00%] [G loss: 4.480541]\n",
      "epoch:39 step:31175 [D loss: 1.461328, acc.: 50.00%] [G loss: 2.994945]\n",
      "epoch:39 step:31176 [D loss: 0.756689, acc.: 63.28%] [G loss: 2.031406]\n",
      "epoch:39 step:31177 [D loss: 0.219339, acc.: 93.75%] [G loss: 2.019593]\n",
      "epoch:39 step:31178 [D loss: 0.173611, acc.: 94.53%] [G loss: 2.369361]\n",
      "epoch:39 step:31179 [D loss: 0.199502, acc.: 91.41%] [G loss: 1.840039]\n",
      "epoch:39 step:31180 [D loss: 0.484097, acc.: 75.78%] [G loss: 2.981493]\n",
      "epoch:39 step:31181 [D loss: 0.193218, acc.: 92.19%] [G loss: 3.127548]\n",
      "epoch:39 step:31182 [D loss: 0.326610, acc.: 83.59%] [G loss: 2.659185]\n",
      "epoch:39 step:31183 [D loss: 0.103598, acc.: 98.44%] [G loss: 2.080630]\n",
      "epoch:39 step:31184 [D loss: 0.160541, acc.: 95.31%] [G loss: 2.700902]\n",
      "epoch:39 step:31185 [D loss: 0.070655, acc.: 100.00%] [G loss: 2.869277]\n",
      "epoch:39 step:31186 [D loss: 0.085452, acc.: 100.00%] [G loss: 2.761929]\n",
      "epoch:39 step:31187 [D loss: 0.054593, acc.: 100.00%] [G loss: 3.063213]\n",
      "epoch:39 step:31188 [D loss: 0.131360, acc.: 97.66%] [G loss: 1.678167]\n",
      "epoch:39 step:31189 [D loss: 0.141179, acc.: 94.53%] [G loss: 2.403915]\n",
      "epoch:39 step:31190 [D loss: 0.116196, acc.: 97.66%] [G loss: 2.730439]\n",
      "epoch:39 step:31191 [D loss: 0.123219, acc.: 96.88%] [G loss: 2.442607]\n",
      "epoch:39 step:31192 [D loss: 0.153113, acc.: 96.09%] [G loss: 2.854224]\n",
      "epoch:39 step:31193 [D loss: 0.158409, acc.: 93.75%] [G loss: 2.029027]\n",
      "epoch:39 step:31194 [D loss: 0.096789, acc.: 99.22%] [G loss: 2.870265]\n",
      "epoch:39 step:31195 [D loss: 0.051800, acc.: 99.22%] [G loss: 2.297511]\n",
      "epoch:39 step:31196 [D loss: 0.071157, acc.: 98.44%] [G loss: 2.192929]\n",
      "epoch:39 step:31197 [D loss: 0.082362, acc.: 99.22%] [G loss: 2.265362]\n",
      "epoch:39 step:31198 [D loss: 0.106489, acc.: 97.66%] [G loss: 1.810711]\n",
      "epoch:39 step:31199 [D loss: 0.164661, acc.: 95.31%] [G loss: 2.476637]\n",
      "epoch:39 step:31200 [D loss: 0.051236, acc.: 100.00%] [G loss: 3.388739]\n",
      "##############\n",
      "[0.97872994 2.10936134 0.97253875 1.0176565  1.11795429 0.96288614\n",
      " 2.11940956 0.77675249 2.10150652 0.99337449]\n",
      "##########\n",
      "epoch:39 step:31201 [D loss: 0.450033, acc.: 82.81%] [G loss: 2.927307]\n",
      "epoch:39 step:31202 [D loss: 0.271733, acc.: 87.50%] [G loss: 1.901551]\n",
      "epoch:39 step:31203 [D loss: 0.178844, acc.: 92.97%] [G loss: 3.908443]\n",
      "epoch:39 step:31204 [D loss: 0.032824, acc.: 100.00%] [G loss: 3.912912]\n",
      "epoch:39 step:31205 [D loss: 0.323944, acc.: 87.50%] [G loss: 3.156005]\n",
      "epoch:39 step:31206 [D loss: 0.062944, acc.: 100.00%] [G loss: 3.504962]\n",
      "epoch:39 step:31207 [D loss: 0.026172, acc.: 100.00%] [G loss: 3.025444]\n",
      "epoch:39 step:31208 [D loss: 0.062914, acc.: 100.00%] [G loss: 2.841864]\n",
      "epoch:39 step:31209 [D loss: 0.111146, acc.: 95.31%] [G loss: 2.494514]\n",
      "epoch:39 step:31210 [D loss: 0.053935, acc.: 100.00%] [G loss: 2.180910]\n",
      "epoch:39 step:31211 [D loss: 0.129338, acc.: 97.66%] [G loss: 1.935037]\n",
      "epoch:39 step:31212 [D loss: 0.055190, acc.: 100.00%] [G loss: 1.412178]\n",
      "epoch:39 step:31213 [D loss: 0.063047, acc.: 100.00%] [G loss: 1.678822]\n",
      "epoch:39 step:31214 [D loss: 0.077353, acc.: 99.22%] [G loss: 2.091759]\n",
      "epoch:39 step:31215 [D loss: 0.073325, acc.: 99.22%] [G loss: 2.134579]\n",
      "epoch:39 step:31216 [D loss: 0.042513, acc.: 100.00%] [G loss: 2.318759]\n",
      "epoch:39 step:31217 [D loss: 0.026184, acc.: 100.00%] [G loss: 1.606099]\n",
      "epoch:39 step:31218 [D loss: 0.083601, acc.: 99.22%] [G loss: 1.903751]\n",
      "epoch:39 step:31219 [D loss: 0.398424, acc.: 85.16%] [G loss: 3.347528]\n",
      "epoch:39 step:31220 [D loss: 0.131607, acc.: 96.09%] [G loss: 2.310030]\n",
      "epoch:39 step:31221 [D loss: 0.137462, acc.: 96.88%] [G loss: 2.016270]\n",
      "epoch:39 step:31222 [D loss: 0.220772, acc.: 92.19%] [G loss: 4.745481]\n",
      "epoch:39 step:31223 [D loss: 0.245405, acc.: 87.50%] [G loss: 3.812521]\n",
      "epoch:39 step:31224 [D loss: 0.042616, acc.: 98.44%] [G loss: 3.565675]\n",
      "epoch:39 step:31225 [D loss: 0.045168, acc.: 100.00%] [G loss: 3.094433]\n",
      "epoch:39 step:31226 [D loss: 0.084771, acc.: 97.66%] [G loss: 2.668538]\n",
      "epoch:39 step:31227 [D loss: 0.045244, acc.: 100.00%] [G loss: 3.375351]\n",
      "epoch:39 step:31228 [D loss: 0.146721, acc.: 96.88%] [G loss: 2.403811]\n",
      "epoch:39 step:31229 [D loss: 0.026078, acc.: 100.00%] [G loss: 3.106294]\n",
      "epoch:39 step:31230 [D loss: 0.072171, acc.: 99.22%] [G loss: 2.758538]\n",
      "epoch:39 step:31231 [D loss: 0.113281, acc.: 97.66%] [G loss: 4.616140]\n",
      "epoch:39 step:31232 [D loss: 0.028617, acc.: 100.00%] [G loss: 4.725606]\n",
      "epoch:39 step:31233 [D loss: 0.279749, acc.: 86.72%] [G loss: 1.457958]\n",
      "epoch:39 step:31234 [D loss: 0.410046, acc.: 78.91%] [G loss: 4.946777]\n",
      "epoch:39 step:31235 [D loss: 0.321303, acc.: 82.81%] [G loss: 4.732845]\n",
      "epoch:39 step:31236 [D loss: 0.122964, acc.: 94.53%] [G loss: 4.036365]\n",
      "epoch:39 step:31237 [D loss: 0.030044, acc.: 100.00%] [G loss: 3.719973]\n",
      "epoch:39 step:31238 [D loss: 0.073741, acc.: 99.22%] [G loss: 3.841147]\n",
      "epoch:39 step:31239 [D loss: 0.065562, acc.: 100.00%] [G loss: 4.109936]\n",
      "epoch:39 step:31240 [D loss: 0.013299, acc.: 100.00%] [G loss: 4.375996]\n",
      "epoch:40 step:31241 [D loss: 0.024268, acc.: 100.00%] [G loss: 4.160123]\n",
      "epoch:40 step:31242 [D loss: 0.013201, acc.: 100.00%] [G loss: 4.111165]\n",
      "epoch:40 step:31243 [D loss: 0.017802, acc.: 100.00%] [G loss: 3.749459]\n",
      "epoch:40 step:31244 [D loss: 0.038041, acc.: 100.00%] [G loss: 3.643224]\n",
      "epoch:40 step:31245 [D loss: 0.102097, acc.: 97.66%] [G loss: 3.610040]\n",
      "epoch:40 step:31246 [D loss: 0.048353, acc.: 99.22%] [G loss: 3.778686]\n",
      "epoch:40 step:31247 [D loss: 0.022629, acc.: 100.00%] [G loss: 3.327261]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:40 step:31248 [D loss: 0.024107, acc.: 100.00%] [G loss: 3.634788]\n",
      "epoch:40 step:31249 [D loss: 0.241932, acc.: 90.62%] [G loss: 2.118503]\n",
      "epoch:40 step:31250 [D loss: 0.145276, acc.: 95.31%] [G loss: 4.190543]\n",
      "epoch:40 step:31251 [D loss: 0.042121, acc.: 98.44%] [G loss: 5.207178]\n",
      "epoch:40 step:31252 [D loss: 0.292189, acc.: 89.06%] [G loss: 2.162105]\n",
      "epoch:40 step:31253 [D loss: 0.341531, acc.: 83.59%] [G loss: 5.592334]\n",
      "epoch:40 step:31254 [D loss: 0.082597, acc.: 97.66%] [G loss: 5.850993]\n",
      "epoch:40 step:31255 [D loss: 0.140792, acc.: 93.75%] [G loss: 4.759619]\n",
      "epoch:40 step:31256 [D loss: 0.023078, acc.: 100.00%] [G loss: 4.070847]\n",
      "epoch:40 step:31257 [D loss: 0.019316, acc.: 100.00%] [G loss: 3.947189]\n",
      "epoch:40 step:31258 [D loss: 0.009888, acc.: 100.00%] [G loss: 3.313063]\n",
      "epoch:40 step:31259 [D loss: 0.022989, acc.: 100.00%] [G loss: 3.864201]\n",
      "epoch:40 step:31260 [D loss: 0.021752, acc.: 100.00%] [G loss: 3.034478]\n",
      "epoch:40 step:31261 [D loss: 0.018581, acc.: 100.00%] [G loss: 3.256878]\n",
      "epoch:40 step:31262 [D loss: 0.032530, acc.: 100.00%] [G loss: 3.115652]\n",
      "epoch:40 step:31263 [D loss: 0.040525, acc.: 99.22%] [G loss: 2.598189]\n",
      "epoch:40 step:31264 [D loss: 0.062862, acc.: 99.22%] [G loss: 3.699531]\n",
      "epoch:40 step:31265 [D loss: 0.024665, acc.: 100.00%] [G loss: 3.927252]\n",
      "epoch:40 step:31266 [D loss: 0.039492, acc.: 98.44%] [G loss: 3.407367]\n",
      "epoch:40 step:31267 [D loss: 0.019495, acc.: 100.00%] [G loss: 3.640403]\n",
      "epoch:40 step:31268 [D loss: 0.024693, acc.: 99.22%] [G loss: 2.551731]\n",
      "epoch:40 step:31269 [D loss: 0.062442, acc.: 99.22%] [G loss: 2.686603]\n",
      "epoch:40 step:31270 [D loss: 0.064213, acc.: 99.22%] [G loss: 3.140190]\n",
      "epoch:40 step:31271 [D loss: 0.035573, acc.: 99.22%] [G loss: 4.537102]\n",
      "epoch:40 step:31272 [D loss: 0.515791, acc.: 78.12%] [G loss: 3.258276]\n",
      "epoch:40 step:31273 [D loss: 0.050067, acc.: 98.44%] [G loss: 5.915465]\n",
      "epoch:40 step:31274 [D loss: 0.021269, acc.: 100.00%] [G loss: 5.287001]\n",
      "epoch:40 step:31275 [D loss: 0.040867, acc.: 100.00%] [G loss: 6.299790]\n",
      "epoch:40 step:31276 [D loss: 0.023376, acc.: 100.00%] [G loss: 5.178729]\n",
      "epoch:40 step:31277 [D loss: 0.013889, acc.: 100.00%] [G loss: 4.922532]\n",
      "epoch:40 step:31278 [D loss: 0.019687, acc.: 100.00%] [G loss: 4.740944]\n",
      "epoch:40 step:31279 [D loss: 0.010873, acc.: 100.00%] [G loss: 3.045882]\n",
      "epoch:40 step:31280 [D loss: 0.040280, acc.: 100.00%] [G loss: 3.203742]\n",
      "epoch:40 step:31281 [D loss: 0.183961, acc.: 93.75%] [G loss: 5.546822]\n",
      "epoch:40 step:31282 [D loss: 0.309479, acc.: 88.28%] [G loss: 2.919305]\n",
      "epoch:40 step:31283 [D loss: 0.178137, acc.: 92.97%] [G loss: 6.926818]\n",
      "epoch:40 step:31284 [D loss: 0.018519, acc.: 99.22%] [G loss: 7.578455]\n",
      "epoch:40 step:31285 [D loss: 0.413895, acc.: 82.03%] [G loss: 3.688303]\n",
      "epoch:40 step:31286 [D loss: 0.327539, acc.: 85.94%] [G loss: 7.057859]\n",
      "epoch:40 step:31287 [D loss: 0.001504, acc.: 100.00%] [G loss: 7.379639]\n",
      "epoch:40 step:31288 [D loss: 0.091409, acc.: 94.53%] [G loss: 7.205192]\n",
      "epoch:40 step:31289 [D loss: 0.052877, acc.: 98.44%] [G loss: 6.117724]\n",
      "epoch:40 step:31290 [D loss: 0.004547, acc.: 100.00%] [G loss: 5.262630]\n",
      "epoch:40 step:31291 [D loss: 0.010125, acc.: 100.00%] [G loss: 4.370964]\n",
      "epoch:40 step:31292 [D loss: 0.015052, acc.: 100.00%] [G loss: 4.009990]\n",
      "epoch:40 step:31293 [D loss: 0.057820, acc.: 98.44%] [G loss: 5.424105]\n",
      "epoch:40 step:31294 [D loss: 0.021562, acc.: 99.22%] [G loss: 4.548063]\n",
      "epoch:40 step:31295 [D loss: 0.002699, acc.: 100.00%] [G loss: 4.491195]\n",
      "epoch:40 step:31296 [D loss: 0.005031, acc.: 100.00%] [G loss: 3.427319]\n",
      "epoch:40 step:31297 [D loss: 0.006230, acc.: 100.00%] [G loss: 3.193417]\n",
      "epoch:40 step:31298 [D loss: 0.022146, acc.: 100.00%] [G loss: 2.751573]\n",
      "epoch:40 step:31299 [D loss: 0.012703, acc.: 100.00%] [G loss: 2.154175]\n",
      "epoch:40 step:31300 [D loss: 0.016758, acc.: 100.00%] [G loss: 0.770400]\n",
      "epoch:40 step:31301 [D loss: 0.063118, acc.: 99.22%] [G loss: 3.227871]\n",
      "epoch:40 step:31302 [D loss: 0.011398, acc.: 100.00%] [G loss: 4.007040]\n",
      "epoch:40 step:31303 [D loss: 0.375247, acc.: 84.38%] [G loss: 0.850793]\n",
      "epoch:40 step:31304 [D loss: 0.062080, acc.: 100.00%] [G loss: 2.935377]\n",
      "epoch:40 step:31305 [D loss: 0.096246, acc.: 97.66%] [G loss: 1.380327]\n",
      "epoch:40 step:31306 [D loss: 0.153735, acc.: 94.53%] [G loss: 4.841318]\n",
      "epoch:40 step:31307 [D loss: 0.089404, acc.: 96.09%] [G loss: 4.027629]\n",
      "epoch:40 step:31308 [D loss: 0.104748, acc.: 97.66%] [G loss: 3.666297]\n",
      "epoch:40 step:31309 [D loss: 0.024798, acc.: 99.22%] [G loss: 4.259521]\n",
      "epoch:40 step:31310 [D loss: 0.035020, acc.: 99.22%] [G loss: 4.631909]\n",
      "epoch:40 step:31311 [D loss: 0.113270, acc.: 97.66%] [G loss: 4.774359]\n",
      "epoch:40 step:31312 [D loss: 0.011870, acc.: 100.00%] [G loss: 5.379756]\n",
      "epoch:40 step:31313 [D loss: 0.004229, acc.: 100.00%] [G loss: 4.917544]\n",
      "epoch:40 step:31314 [D loss: 0.019346, acc.: 99.22%] [G loss: 3.982798]\n",
      "epoch:40 step:31315 [D loss: 0.069252, acc.: 97.66%] [G loss: 5.415672]\n",
      "epoch:40 step:31316 [D loss: 0.083047, acc.: 98.44%] [G loss: 3.513562]\n",
      "epoch:40 step:31317 [D loss: 0.270501, acc.: 89.06%] [G loss: 6.668733]\n",
      "epoch:40 step:31318 [D loss: 0.014887, acc.: 100.00%] [G loss: 7.777733]\n",
      "epoch:40 step:31319 [D loss: 0.702708, acc.: 70.31%] [G loss: 0.491194]\n",
      "epoch:40 step:31320 [D loss: 2.719337, acc.: 53.91%] [G loss: 8.277660]\n",
      "epoch:40 step:31321 [D loss: 2.613981, acc.: 50.00%] [G loss: 6.389357]\n",
      "epoch:40 step:31322 [D loss: 2.227982, acc.: 50.00%] [G loss: 4.156046]\n",
      "epoch:40 step:31323 [D loss: 1.127992, acc.: 56.25%] [G loss: 2.683033]\n",
      "epoch:40 step:31324 [D loss: 0.588745, acc.: 66.41%] [G loss: 1.892547]\n",
      "epoch:40 step:31325 [D loss: 0.189673, acc.: 93.75%] [G loss: 1.893943]\n",
      "epoch:40 step:31326 [D loss: 0.073249, acc.: 99.22%] [G loss: 2.069122]\n",
      "epoch:40 step:31327 [D loss: 0.083427, acc.: 99.22%] [G loss: 1.860170]\n",
      "epoch:40 step:31328 [D loss: 0.091691, acc.: 100.00%] [G loss: 1.873976]\n",
      "epoch:40 step:31329 [D loss: 0.150765, acc.: 98.44%] [G loss: 1.363390]\n",
      "epoch:40 step:31330 [D loss: 0.128487, acc.: 97.66%] [G loss: 1.884513]\n",
      "epoch:40 step:31331 [D loss: 0.083717, acc.: 100.00%] [G loss: 2.266956]\n",
      "epoch:40 step:31332 [D loss: 0.145155, acc.: 97.66%] [G loss: 1.923006]\n",
      "epoch:40 step:31333 [D loss: 0.239394, acc.: 92.19%] [G loss: 2.461082]\n",
      "epoch:40 step:31334 [D loss: 0.285734, acc.: 88.28%] [G loss: 2.185812]\n",
      "epoch:40 step:31335 [D loss: 0.269864, acc.: 92.19%] [G loss: 2.822639]\n",
      "epoch:40 step:31336 [D loss: 0.154589, acc.: 96.09%] [G loss: 2.778784]\n",
      "epoch:40 step:31337 [D loss: 0.213503, acc.: 92.19%] [G loss: 2.655507]\n",
      "epoch:40 step:31338 [D loss: 0.312021, acc.: 88.28%] [G loss: 4.162537]\n",
      "epoch:40 step:31339 [D loss: 0.772138, acc.: 57.81%] [G loss: 3.469009]\n",
      "epoch:40 step:31340 [D loss: 0.226597, acc.: 92.97%] [G loss: 3.221778]\n",
      "epoch:40 step:31341 [D loss: 0.110105, acc.: 96.09%] [G loss: 2.886217]\n",
      "epoch:40 step:31342 [D loss: 0.058349, acc.: 100.00%] [G loss: 3.035532]\n",
      "epoch:40 step:31343 [D loss: 0.107005, acc.: 96.88%] [G loss: 2.933225]\n",
      "epoch:40 step:31344 [D loss: 0.344673, acc.: 83.59%] [G loss: 3.763204]\n",
      "epoch:40 step:31345 [D loss: 0.177372, acc.: 92.97%] [G loss: 4.243705]\n",
      "epoch:40 step:31346 [D loss: 0.116374, acc.: 96.09%] [G loss: 3.045802]\n",
      "epoch:40 step:31347 [D loss: 0.084038, acc.: 100.00%] [G loss: 2.590890]\n",
      "epoch:40 step:31348 [D loss: 0.109247, acc.: 100.00%] [G loss: 3.411962]\n",
      "epoch:40 step:31349 [D loss: 0.038425, acc.: 100.00%] [G loss: 3.403628]\n",
      "epoch:40 step:31350 [D loss: 0.088954, acc.: 98.44%] [G loss: 2.823616]\n",
      "epoch:40 step:31351 [D loss: 0.068727, acc.: 99.22%] [G loss: 3.283044]\n",
      "epoch:40 step:31352 [D loss: 0.038849, acc.: 99.22%] [G loss: 3.237378]\n",
      "epoch:40 step:31353 [D loss: 0.021924, acc.: 100.00%] [G loss: 2.841427]\n",
      "epoch:40 step:31354 [D loss: 0.097943, acc.: 99.22%] [G loss: 2.838049]\n",
      "epoch:40 step:31355 [D loss: 0.104362, acc.: 99.22%] [G loss: 3.652066]\n",
      "epoch:40 step:31356 [D loss: 0.088383, acc.: 99.22%] [G loss: 3.011635]\n",
      "epoch:40 step:31357 [D loss: 0.082042, acc.: 97.66%] [G loss: 2.412850]\n",
      "epoch:40 step:31358 [D loss: 0.074480, acc.: 100.00%] [G loss: 3.302357]\n",
      "epoch:40 step:31359 [D loss: 0.063502, acc.: 99.22%] [G loss: 3.614178]\n",
      "epoch:40 step:31360 [D loss: 0.018936, acc.: 100.00%] [G loss: 3.140246]\n",
      "epoch:40 step:31361 [D loss: 0.067056, acc.: 100.00%] [G loss: 3.393502]\n",
      "epoch:40 step:31362 [D loss: 0.045876, acc.: 100.00%] [G loss: 3.273032]\n",
      "epoch:40 step:31363 [D loss: 0.083532, acc.: 99.22%] [G loss: 3.417714]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:40 step:31364 [D loss: 0.449387, acc.: 75.00%] [G loss: 4.414882]\n",
      "epoch:40 step:31365 [D loss: 0.013245, acc.: 100.00%] [G loss: 5.585787]\n",
      "epoch:40 step:31366 [D loss: 0.083099, acc.: 96.09%] [G loss: 4.395757]\n",
      "epoch:40 step:31367 [D loss: 0.009332, acc.: 100.00%] [G loss: 4.152371]\n",
      "epoch:40 step:31368 [D loss: 0.025483, acc.: 100.00%] [G loss: 4.227958]\n",
      "epoch:40 step:31369 [D loss: 0.005422, acc.: 100.00%] [G loss: 3.446522]\n",
      "epoch:40 step:31370 [D loss: 0.030162, acc.: 100.00%] [G loss: 3.868972]\n",
      "epoch:40 step:31371 [D loss: 0.027245, acc.: 100.00%] [G loss: 4.321357]\n",
      "epoch:40 step:31372 [D loss: 0.199453, acc.: 94.53%] [G loss: 3.076658]\n",
      "epoch:40 step:31373 [D loss: 0.040888, acc.: 99.22%] [G loss: 3.353578]\n",
      "epoch:40 step:31374 [D loss: 0.012492, acc.: 100.00%] [G loss: 4.079949]\n",
      "epoch:40 step:31375 [D loss: 0.017063, acc.: 100.00%] [G loss: 4.042796]\n",
      "epoch:40 step:31376 [D loss: 0.029090, acc.: 99.22%] [G loss: 4.325068]\n",
      "epoch:40 step:31377 [D loss: 0.020917, acc.: 100.00%] [G loss: 4.374846]\n",
      "epoch:40 step:31378 [D loss: 0.019095, acc.: 99.22%] [G loss: 3.741832]\n",
      "epoch:40 step:31379 [D loss: 0.053157, acc.: 100.00%] [G loss: 3.155304]\n",
      "epoch:40 step:31380 [D loss: 0.674720, acc.: 63.28%] [G loss: 4.610514]\n",
      "epoch:40 step:31381 [D loss: 0.007925, acc.: 100.00%] [G loss: 5.763452]\n",
      "epoch:40 step:31382 [D loss: 0.085465, acc.: 96.09%] [G loss: 4.524890]\n",
      "epoch:40 step:31383 [D loss: 0.031487, acc.: 100.00%] [G loss: 5.354474]\n",
      "epoch:40 step:31384 [D loss: 0.018616, acc.: 100.00%] [G loss: 4.744824]\n",
      "epoch:40 step:31385 [D loss: 0.019830, acc.: 100.00%] [G loss: 3.939400]\n",
      "epoch:40 step:31386 [D loss: 0.015223, acc.: 100.00%] [G loss: 4.616668]\n",
      "epoch:40 step:31387 [D loss: 0.014020, acc.: 100.00%] [G loss: 4.490679]\n",
      "epoch:40 step:31388 [D loss: 0.015441, acc.: 100.00%] [G loss: 3.885414]\n",
      "epoch:40 step:31389 [D loss: 0.015672, acc.: 100.00%] [G loss: 4.175460]\n",
      "epoch:40 step:31390 [D loss: 0.013454, acc.: 100.00%] [G loss: 3.640088]\n",
      "epoch:40 step:31391 [D loss: 0.032396, acc.: 100.00%] [G loss: 3.929928]\n",
      "epoch:40 step:31392 [D loss: 0.013413, acc.: 100.00%] [G loss: 3.732588]\n",
      "epoch:40 step:31393 [D loss: 0.359959, acc.: 86.72%] [G loss: 4.377107]\n",
      "epoch:40 step:31394 [D loss: 0.005371, acc.: 100.00%] [G loss: 5.280545]\n",
      "epoch:40 step:31395 [D loss: 0.008552, acc.: 100.00%] [G loss: 5.773438]\n",
      "epoch:40 step:31396 [D loss: 0.179026, acc.: 93.75%] [G loss: 3.310220]\n",
      "epoch:40 step:31397 [D loss: 0.058017, acc.: 99.22%] [G loss: 3.191059]\n",
      "epoch:40 step:31398 [D loss: 0.020107, acc.: 100.00%] [G loss: 3.806029]\n",
      "epoch:40 step:31399 [D loss: 0.030405, acc.: 100.00%] [G loss: 3.841060]\n",
      "epoch:40 step:31400 [D loss: 0.008049, acc.: 100.00%] [G loss: 4.221077]\n",
      "##############\n",
      "[0.88543334 1.00099214 0.74527265 0.89405988 0.70807637 2.11425794\n",
      " 1.0836822  0.85936072 2.11203256 2.11191228]\n",
      "##########\n",
      "epoch:40 step:31401 [D loss: 0.021515, acc.: 100.00%] [G loss: 4.619649]\n",
      "epoch:40 step:31402 [D loss: 0.013489, acc.: 100.00%] [G loss: 4.303861]\n",
      "epoch:40 step:31403 [D loss: 0.018857, acc.: 100.00%] [G loss: 4.764102]\n",
      "epoch:40 step:31404 [D loss: 0.006936, acc.: 100.00%] [G loss: 4.310143]\n",
      "epoch:40 step:31405 [D loss: 0.028391, acc.: 100.00%] [G loss: 3.837095]\n",
      "epoch:40 step:31406 [D loss: 0.104322, acc.: 98.44%] [G loss: 3.948159]\n",
      "epoch:40 step:31407 [D loss: 0.015021, acc.: 100.00%] [G loss: 3.614831]\n",
      "epoch:40 step:31408 [D loss: 0.016190, acc.: 100.00%] [G loss: 4.363730]\n",
      "epoch:40 step:31409 [D loss: 0.007545, acc.: 100.00%] [G loss: 4.740359]\n",
      "epoch:40 step:31410 [D loss: 0.023244, acc.: 100.00%] [G loss: 4.031071]\n",
      "epoch:40 step:31411 [D loss: 0.019948, acc.: 100.00%] [G loss: 4.049708]\n",
      "epoch:40 step:31412 [D loss: 0.016454, acc.: 100.00%] [G loss: 5.093942]\n",
      "epoch:40 step:31413 [D loss: 0.019353, acc.: 100.00%] [G loss: 4.072546]\n",
      "epoch:40 step:31414 [D loss: 0.056889, acc.: 96.88%] [G loss: 3.298538]\n",
      "epoch:40 step:31415 [D loss: 0.104719, acc.: 97.66%] [G loss: 4.814996]\n",
      "epoch:40 step:31416 [D loss: 5.978868, acc.: 12.50%] [G loss: 6.191540]\n",
      "epoch:40 step:31417 [D loss: 2.474410, acc.: 50.00%] [G loss: 4.615331]\n",
      "epoch:40 step:31418 [D loss: 1.940436, acc.: 50.00%] [G loss: 3.344386]\n",
      "epoch:40 step:31419 [D loss: 1.214888, acc.: 50.78%] [G loss: 2.350582]\n",
      "epoch:40 step:31420 [D loss: 0.610218, acc.: 66.41%] [G loss: 1.704595]\n",
      "epoch:40 step:31421 [D loss: 0.253134, acc.: 92.97%] [G loss: 1.734113]\n",
      "epoch:40 step:31422 [D loss: 0.255767, acc.: 89.84%] [G loss: 2.237613]\n",
      "epoch:40 step:31423 [D loss: 0.155616, acc.: 95.31%] [G loss: 2.142392]\n",
      "epoch:40 step:31424 [D loss: 0.101304, acc.: 99.22%] [G loss: 2.248444]\n",
      "epoch:40 step:31425 [D loss: 0.156996, acc.: 97.66%] [G loss: 1.640688]\n",
      "epoch:40 step:31426 [D loss: 0.134883, acc.: 97.66%] [G loss: 2.167553]\n",
      "epoch:40 step:31427 [D loss: 0.143452, acc.: 97.66%] [G loss: 2.517281]\n",
      "epoch:40 step:31428 [D loss: 0.088624, acc.: 99.22%] [G loss: 2.365691]\n",
      "epoch:40 step:31429 [D loss: 0.071459, acc.: 100.00%] [G loss: 2.741160]\n",
      "epoch:40 step:31430 [D loss: 0.183195, acc.: 95.31%] [G loss: 2.209636]\n",
      "epoch:40 step:31431 [D loss: 0.105060, acc.: 99.22%] [G loss: 1.886707]\n",
      "epoch:40 step:31432 [D loss: 0.168223, acc.: 97.66%] [G loss: 2.879815]\n",
      "epoch:40 step:31433 [D loss: 0.065998, acc.: 100.00%] [G loss: 2.693972]\n",
      "epoch:40 step:31434 [D loss: 0.396405, acc.: 85.94%] [G loss: 2.206560]\n",
      "epoch:40 step:31435 [D loss: 0.243646, acc.: 92.97%] [G loss: 1.843288]\n",
      "epoch:40 step:31436 [D loss: 0.710433, acc.: 62.50%] [G loss: 3.532625]\n",
      "epoch:40 step:31437 [D loss: 0.334678, acc.: 83.59%] [G loss: 3.512821]\n",
      "epoch:40 step:31438 [D loss: 0.139808, acc.: 96.09%] [G loss: 3.338047]\n",
      "epoch:40 step:31439 [D loss: 0.058337, acc.: 100.00%] [G loss: 2.900377]\n",
      "epoch:40 step:31440 [D loss: 0.038773, acc.: 100.00%] [G loss: 2.587765]\n",
      "epoch:40 step:31441 [D loss: 0.041337, acc.: 100.00%] [G loss: 2.467332]\n",
      "epoch:40 step:31442 [D loss: 0.208111, acc.: 94.53%] [G loss: 2.048454]\n",
      "epoch:40 step:31443 [D loss: 0.209965, acc.: 92.97%] [G loss: 3.315314]\n",
      "epoch:40 step:31444 [D loss: 0.396930, acc.: 81.25%] [G loss: 2.255193]\n",
      "epoch:40 step:31445 [D loss: 0.223273, acc.: 93.75%] [G loss: 3.576106]\n",
      "epoch:40 step:31446 [D loss: 0.127000, acc.: 96.09%] [G loss: 3.037889]\n",
      "epoch:40 step:31447 [D loss: 0.060790, acc.: 99.22%] [G loss: 3.018095]\n",
      "epoch:40 step:31448 [D loss: 0.072135, acc.: 98.44%] [G loss: 3.500108]\n",
      "epoch:40 step:31449 [D loss: 0.120711, acc.: 96.88%] [G loss: 3.484018]\n",
      "epoch:40 step:31450 [D loss: 0.060521, acc.: 99.22%] [G loss: 3.643816]\n",
      "epoch:40 step:31451 [D loss: 0.062234, acc.: 99.22%] [G loss: 3.854346]\n",
      "epoch:40 step:31452 [D loss: 0.069789, acc.: 99.22%] [G loss: 2.745372]\n",
      "epoch:40 step:31453 [D loss: 0.564393, acc.: 67.97%] [G loss: 3.765979]\n",
      "epoch:40 step:31454 [D loss: 0.025401, acc.: 100.00%] [G loss: 4.549568]\n",
      "epoch:40 step:31455 [D loss: 0.171659, acc.: 92.97%] [G loss: 3.650956]\n",
      "epoch:40 step:31456 [D loss: 0.055777, acc.: 98.44%] [G loss: 2.819454]\n",
      "epoch:40 step:31457 [D loss: 0.034426, acc.: 100.00%] [G loss: 2.483226]\n",
      "epoch:40 step:31458 [D loss: 0.103515, acc.: 97.66%] [G loss: 1.965715]\n",
      "epoch:40 step:31459 [D loss: 0.053745, acc.: 100.00%] [G loss: 3.164796]\n",
      "epoch:40 step:31460 [D loss: 0.140163, acc.: 96.88%] [G loss: 2.891739]\n",
      "epoch:40 step:31461 [D loss: 0.017510, acc.: 100.00%] [G loss: 2.942885]\n",
      "epoch:40 step:31462 [D loss: 0.034713, acc.: 100.00%] [G loss: 2.814194]\n",
      "epoch:40 step:31463 [D loss: 0.071295, acc.: 100.00%] [G loss: 2.347978]\n",
      "epoch:40 step:31464 [D loss: 0.409682, acc.: 83.59%] [G loss: 4.417167]\n",
      "epoch:40 step:31465 [D loss: 0.916232, acc.: 60.94%] [G loss: 3.569523]\n",
      "epoch:40 step:31466 [D loss: 0.071643, acc.: 97.66%] [G loss: 2.103646]\n",
      "epoch:40 step:31467 [D loss: 0.105584, acc.: 98.44%] [G loss: 3.143551]\n",
      "epoch:40 step:31468 [D loss: 0.043132, acc.: 100.00%] [G loss: 3.226946]\n",
      "epoch:40 step:31469 [D loss: 0.079447, acc.: 100.00%] [G loss: 3.094941]\n",
      "epoch:40 step:31470 [D loss: 0.091670, acc.: 98.44%] [G loss: 3.191733]\n",
      "epoch:40 step:31471 [D loss: 0.046870, acc.: 100.00%] [G loss: 2.873009]\n",
      "epoch:40 step:31472 [D loss: 0.058316, acc.: 100.00%] [G loss: 2.821634]\n",
      "epoch:40 step:31473 [D loss: 0.023278, acc.: 100.00%] [G loss: 2.790562]\n",
      "epoch:40 step:31474 [D loss: 0.055719, acc.: 100.00%] [G loss: 2.940478]\n",
      "epoch:40 step:31475 [D loss: 0.032413, acc.: 100.00%] [G loss: 3.121001]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:40 step:31476 [D loss: 0.062634, acc.: 99.22%] [G loss: 3.016512]\n",
      "epoch:40 step:31477 [D loss: 0.022038, acc.: 100.00%] [G loss: 3.450626]\n",
      "epoch:40 step:31478 [D loss: 0.196812, acc.: 93.75%] [G loss: 2.944857]\n",
      "epoch:40 step:31479 [D loss: 0.042673, acc.: 99.22%] [G loss: 3.194376]\n",
      "epoch:40 step:31480 [D loss: 0.603926, acc.: 66.41%] [G loss: 4.405817]\n",
      "epoch:40 step:31481 [D loss: 0.399769, acc.: 76.56%] [G loss: 3.851548]\n",
      "epoch:40 step:31482 [D loss: 0.051592, acc.: 99.22%] [G loss: 3.478405]\n",
      "epoch:40 step:31483 [D loss: 0.024526, acc.: 100.00%] [G loss: 3.379248]\n",
      "epoch:40 step:31484 [D loss: 0.024539, acc.: 100.00%] [G loss: 3.266567]\n",
      "epoch:40 step:31485 [D loss: 0.024804, acc.: 100.00%] [G loss: 3.347943]\n",
      "epoch:40 step:31486 [D loss: 0.071121, acc.: 99.22%] [G loss: 3.139162]\n",
      "epoch:40 step:31487 [D loss: 0.054684, acc.: 100.00%] [G loss: 2.944076]\n",
      "epoch:40 step:31488 [D loss: 0.096043, acc.: 100.00%] [G loss: 3.266288]\n",
      "epoch:40 step:31489 [D loss: 0.010032, acc.: 100.00%] [G loss: 3.875749]\n",
      "epoch:40 step:31490 [D loss: 0.160714, acc.: 92.97%] [G loss: 2.721438]\n",
      "epoch:40 step:31491 [D loss: 0.073080, acc.: 100.00%] [G loss: 3.244405]\n",
      "epoch:40 step:31492 [D loss: 0.022250, acc.: 100.00%] [G loss: 3.181517]\n",
      "epoch:40 step:31493 [D loss: 0.025325, acc.: 100.00%] [G loss: 3.161720]\n",
      "epoch:40 step:31494 [D loss: 0.023130, acc.: 100.00%] [G loss: 2.729220]\n",
      "epoch:40 step:31495 [D loss: 0.043467, acc.: 99.22%] [G loss: 3.378062]\n",
      "epoch:40 step:31496 [D loss: 0.054623, acc.: 100.00%] [G loss: 3.053298]\n",
      "epoch:40 step:31497 [D loss: 0.048747, acc.: 100.00%] [G loss: 3.376953]\n",
      "epoch:40 step:31498 [D loss: 0.109244, acc.: 97.66%] [G loss: 2.830216]\n",
      "epoch:40 step:31499 [D loss: 0.015732, acc.: 100.00%] [G loss: 2.935267]\n",
      "epoch:40 step:31500 [D loss: 0.047699, acc.: 100.00%] [G loss: 2.696922]\n",
      "epoch:40 step:31501 [D loss: 0.375438, acc.: 86.72%] [G loss: 5.077209]\n",
      "epoch:40 step:31502 [D loss: 0.205627, acc.: 89.84%] [G loss: 4.627989]\n",
      "epoch:40 step:31503 [D loss: 0.060703, acc.: 96.88%] [G loss: 3.988503]\n",
      "epoch:40 step:31504 [D loss: 0.029522, acc.: 100.00%] [G loss: 4.070702]\n",
      "epoch:40 step:31505 [D loss: 0.015745, acc.: 100.00%] [G loss: 4.026629]\n",
      "epoch:40 step:31506 [D loss: 0.025196, acc.: 100.00%] [G loss: 4.167067]\n",
      "epoch:40 step:31507 [D loss: 0.785206, acc.: 57.81%] [G loss: 6.177782]\n",
      "epoch:40 step:31508 [D loss: 0.006156, acc.: 100.00%] [G loss: 6.735451]\n",
      "epoch:40 step:31509 [D loss: 0.447233, acc.: 78.12%] [G loss: 3.807970]\n",
      "epoch:40 step:31510 [D loss: 0.033079, acc.: 98.44%] [G loss: 4.059921]\n",
      "epoch:40 step:31511 [D loss: 0.021393, acc.: 100.00%] [G loss: 4.492689]\n",
      "epoch:40 step:31512 [D loss: 0.070557, acc.: 97.66%] [G loss: 4.435378]\n",
      "epoch:40 step:31513 [D loss: 0.008280, acc.: 100.00%] [G loss: 4.348960]\n",
      "epoch:40 step:31514 [D loss: 0.012054, acc.: 100.00%] [G loss: 4.587902]\n",
      "epoch:40 step:31515 [D loss: 0.024136, acc.: 100.00%] [G loss: 4.041441]\n",
      "epoch:40 step:31516 [D loss: 0.024373, acc.: 100.00%] [G loss: 4.543724]\n",
      "epoch:40 step:31517 [D loss: 0.054587, acc.: 99.22%] [G loss: 4.488666]\n",
      "epoch:40 step:31518 [D loss: 0.039016, acc.: 99.22%] [G loss: 4.279839]\n",
      "epoch:40 step:31519 [D loss: 0.035111, acc.: 99.22%] [G loss: 4.336290]\n",
      "epoch:40 step:31520 [D loss: 0.028530, acc.: 100.00%] [G loss: 4.334729]\n",
      "epoch:40 step:31521 [D loss: 0.017565, acc.: 100.00%] [G loss: 4.018809]\n",
      "epoch:40 step:31522 [D loss: 0.024019, acc.: 100.00%] [G loss: 4.270950]\n",
      "epoch:40 step:31523 [D loss: 0.024291, acc.: 100.00%] [G loss: 4.454417]\n",
      "epoch:40 step:31524 [D loss: 0.021715, acc.: 100.00%] [G loss: 4.232909]\n",
      "epoch:40 step:31525 [D loss: 0.045200, acc.: 97.66%] [G loss: 3.970331]\n",
      "epoch:40 step:31526 [D loss: 0.056801, acc.: 98.44%] [G loss: 4.061806]\n",
      "epoch:40 step:31527 [D loss: 0.015228, acc.: 100.00%] [G loss: 4.404756]\n",
      "epoch:40 step:31528 [D loss: 0.012770, acc.: 100.00%] [G loss: 4.115808]\n",
      "epoch:40 step:31529 [D loss: 0.219578, acc.: 91.41%] [G loss: 4.311550]\n",
      "epoch:40 step:31530 [D loss: 0.018755, acc.: 100.00%] [G loss: 5.089970]\n",
      "epoch:40 step:31531 [D loss: 0.026080, acc.: 99.22%] [G loss: 4.241140]\n",
      "epoch:40 step:31532 [D loss: 0.034430, acc.: 98.44%] [G loss: 4.396511]\n",
      "epoch:40 step:31533 [D loss: 0.017293, acc.: 100.00%] [G loss: 4.302971]\n",
      "epoch:40 step:31534 [D loss: 0.017369, acc.: 100.00%] [G loss: 4.206573]\n",
      "epoch:40 step:31535 [D loss: 0.038856, acc.: 99.22%] [G loss: 4.205277]\n",
      "epoch:40 step:31536 [D loss: 0.032539, acc.: 100.00%] [G loss: 4.230529]\n",
      "epoch:40 step:31537 [D loss: 0.062005, acc.: 99.22%] [G loss: 3.425272]\n",
      "epoch:40 step:31538 [D loss: 0.092653, acc.: 98.44%] [G loss: 5.511919]\n",
      "epoch:40 step:31539 [D loss: 0.040837, acc.: 99.22%] [G loss: 5.260570]\n",
      "epoch:40 step:31540 [D loss: 0.023247, acc.: 99.22%] [G loss: 4.242145]\n",
      "epoch:40 step:31541 [D loss: 0.015757, acc.: 100.00%] [G loss: 4.359745]\n",
      "epoch:40 step:31542 [D loss: 0.014436, acc.: 100.00%] [G loss: 3.879092]\n",
      "epoch:40 step:31543 [D loss: 0.026394, acc.: 100.00%] [G loss: 4.294434]\n",
      "epoch:40 step:31544 [D loss: 0.033497, acc.: 99.22%] [G loss: 4.026716]\n",
      "epoch:40 step:31545 [D loss: 0.043603, acc.: 100.00%] [G loss: 3.607519]\n",
      "epoch:40 step:31546 [D loss: 0.015656, acc.: 100.00%] [G loss: 4.369230]\n",
      "epoch:40 step:31547 [D loss: 0.016274, acc.: 100.00%] [G loss: 3.548524]\n",
      "epoch:40 step:31548 [D loss: 0.048321, acc.: 100.00%] [G loss: 3.977141]\n",
      "epoch:40 step:31549 [D loss: 0.120029, acc.: 97.66%] [G loss: 5.434925]\n",
      "epoch:40 step:31550 [D loss: 0.236051, acc.: 91.41%] [G loss: 4.673934]\n",
      "epoch:40 step:31551 [D loss: 0.003185, acc.: 100.00%] [G loss: 6.421348]\n",
      "epoch:40 step:31552 [D loss: 0.365195, acc.: 84.38%] [G loss: 7.127989]\n",
      "epoch:40 step:31553 [D loss: 0.198074, acc.: 89.06%] [G loss: 5.719928]\n",
      "epoch:40 step:31554 [D loss: 0.005515, acc.: 100.00%] [G loss: 4.737561]\n",
      "epoch:40 step:31555 [D loss: 0.024168, acc.: 98.44%] [G loss: 3.998779]\n",
      "epoch:40 step:31556 [D loss: 0.044758, acc.: 99.22%] [G loss: 6.054091]\n",
      "epoch:40 step:31557 [D loss: 0.002210, acc.: 100.00%] [G loss: 5.952836]\n",
      "epoch:40 step:31558 [D loss: 0.003531, acc.: 100.00%] [G loss: 5.806896]\n",
      "epoch:40 step:31559 [D loss: 0.004861, acc.: 100.00%] [G loss: 5.576450]\n",
      "epoch:40 step:31560 [D loss: 0.202937, acc.: 91.41%] [G loss: 7.180833]\n",
      "epoch:40 step:31561 [D loss: 0.010595, acc.: 100.00%] [G loss: 7.687139]\n",
      "epoch:40 step:31562 [D loss: 0.062787, acc.: 98.44%] [G loss: 6.611444]\n",
      "epoch:40 step:31563 [D loss: 0.010211, acc.: 100.00%] [G loss: 6.072620]\n",
      "epoch:40 step:31564 [D loss: 0.062187, acc.: 96.88%] [G loss: 4.675026]\n",
      "epoch:40 step:31565 [D loss: 0.009814, acc.: 100.00%] [G loss: 4.193874]\n",
      "epoch:40 step:31566 [D loss: 0.017362, acc.: 100.00%] [G loss: 4.868774]\n",
      "epoch:40 step:31567 [D loss: 0.007219, acc.: 100.00%] [G loss: 5.327277]\n",
      "epoch:40 step:31568 [D loss: 0.011172, acc.: 100.00%] [G loss: 4.597298]\n",
      "epoch:40 step:31569 [D loss: 0.008338, acc.: 100.00%] [G loss: 5.083520]\n",
      "epoch:40 step:31570 [D loss: 0.004915, acc.: 100.00%] [G loss: 4.854523]\n",
      "epoch:40 step:31571 [D loss: 0.082494, acc.: 97.66%] [G loss: 5.065926]\n",
      "epoch:40 step:31572 [D loss: 0.025878, acc.: 99.22%] [G loss: 5.042381]\n",
      "epoch:40 step:31573 [D loss: 0.014471, acc.: 100.00%] [G loss: 4.842505]\n",
      "epoch:40 step:31574 [D loss: 0.553007, acc.: 72.66%] [G loss: 9.492281]\n",
      "epoch:40 step:31575 [D loss: 2.347589, acc.: 50.78%] [G loss: 0.533420]\n",
      "epoch:40 step:31576 [D loss: 0.625505, acc.: 75.00%] [G loss: 6.609655]\n",
      "epoch:40 step:31577 [D loss: 0.603429, acc.: 68.75%] [G loss: 4.124444]\n",
      "epoch:40 step:31578 [D loss: 0.154322, acc.: 94.53%] [G loss: 4.626191]\n",
      "epoch:40 step:31579 [D loss: 0.127567, acc.: 94.53%] [G loss: 3.448726]\n",
      "epoch:40 step:31580 [D loss: 0.059770, acc.: 97.66%] [G loss: 2.162813]\n",
      "epoch:40 step:31581 [D loss: 0.032470, acc.: 99.22%] [G loss: 3.755951]\n",
      "epoch:40 step:31582 [D loss: 0.015648, acc.: 100.00%] [G loss: 3.091229]\n",
      "epoch:40 step:31583 [D loss: 3.220135, acc.: 31.25%] [G loss: 7.711844]\n",
      "epoch:40 step:31584 [D loss: 2.139816, acc.: 50.78%] [G loss: 6.463674]\n",
      "epoch:40 step:31585 [D loss: 0.156536, acc.: 92.19%] [G loss: 4.823449]\n",
      "epoch:40 step:31586 [D loss: 0.043579, acc.: 99.22%] [G loss: 3.357128]\n",
      "epoch:40 step:31587 [D loss: 0.017127, acc.: 100.00%] [G loss: 3.088809]\n",
      "epoch:40 step:31588 [D loss: 0.245751, acc.: 85.94%] [G loss: 4.785464]\n",
      "epoch:40 step:31589 [D loss: 0.058310, acc.: 98.44%] [G loss: 4.983408]\n",
      "epoch:40 step:31590 [D loss: 0.719426, acc.: 61.72%] [G loss: 2.559642]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:40 step:31591 [D loss: 0.257729, acc.: 89.84%] [G loss: 3.439149]\n",
      "epoch:40 step:31592 [D loss: 0.048558, acc.: 99.22%] [G loss: 4.714265]\n",
      "epoch:40 step:31593 [D loss: 0.104053, acc.: 97.66%] [G loss: 3.441853]\n",
      "epoch:40 step:31594 [D loss: 0.085730, acc.: 97.66%] [G loss: 3.391448]\n",
      "epoch:40 step:31595 [D loss: 0.096207, acc.: 97.66%] [G loss: 3.354650]\n",
      "epoch:40 step:31596 [D loss: 0.038347, acc.: 100.00%] [G loss: 3.801832]\n",
      "epoch:40 step:31597 [D loss: 0.011614, acc.: 100.00%] [G loss: 3.366672]\n",
      "epoch:40 step:31598 [D loss: 0.045181, acc.: 99.22%] [G loss: 3.170003]\n",
      "epoch:40 step:31599 [D loss: 0.049064, acc.: 99.22%] [G loss: 2.868997]\n",
      "epoch:40 step:31600 [D loss: 0.084716, acc.: 100.00%] [G loss: 3.052013]\n",
      "##############\n",
      "[0.98073706 1.02258641 0.71872879 0.93193368 2.11234543 0.99337037\n",
      " 2.1112668  2.12017485 2.10465482 2.10363231]\n",
      "##########\n",
      "epoch:40 step:31601 [D loss: 0.103597, acc.: 99.22%] [G loss: 3.519048]\n",
      "epoch:40 step:31602 [D loss: 0.124484, acc.: 98.44%] [G loss: 2.702858]\n",
      "epoch:40 step:31603 [D loss: 0.125091, acc.: 96.09%] [G loss: 3.079432]\n",
      "epoch:40 step:31604 [D loss: 0.346389, acc.: 85.16%] [G loss: 2.493371]\n",
      "epoch:40 step:31605 [D loss: 0.076299, acc.: 99.22%] [G loss: 3.752959]\n",
      "epoch:40 step:31606 [D loss: 0.018609, acc.: 100.00%] [G loss: 4.431038]\n",
      "epoch:40 step:31607 [D loss: 0.019437, acc.: 100.00%] [G loss: 4.230383]\n",
      "epoch:40 step:31608 [D loss: 0.035961, acc.: 99.22%] [G loss: 3.956189]\n",
      "epoch:40 step:31609 [D loss: 0.016153, acc.: 100.00%] [G loss: 4.068596]\n",
      "epoch:40 step:31610 [D loss: 0.226488, acc.: 93.75%] [G loss: 3.168887]\n",
      "epoch:40 step:31611 [D loss: 0.023891, acc.: 100.00%] [G loss: 4.222404]\n",
      "epoch:40 step:31612 [D loss: 0.009621, acc.: 100.00%] [G loss: 4.111547]\n",
      "epoch:40 step:31613 [D loss: 0.056216, acc.: 99.22%] [G loss: 2.966860]\n",
      "epoch:40 step:31614 [D loss: 0.066638, acc.: 99.22%] [G loss: 2.521050]\n",
      "epoch:40 step:31615 [D loss: 0.055284, acc.: 99.22%] [G loss: 2.793305]\n",
      "epoch:40 step:31616 [D loss: 0.048884, acc.: 100.00%] [G loss: 3.654840]\n",
      "epoch:40 step:31617 [D loss: 0.015618, acc.: 100.00%] [G loss: 4.038155]\n",
      "epoch:40 step:31618 [D loss: 0.026903, acc.: 100.00%] [G loss: 4.022787]\n",
      "epoch:40 step:31619 [D loss: 0.025281, acc.: 100.00%] [G loss: 3.476303]\n",
      "epoch:40 step:31620 [D loss: 0.089869, acc.: 98.44%] [G loss: 3.744504]\n",
      "epoch:40 step:31621 [D loss: 0.023890, acc.: 100.00%] [G loss: 3.890443]\n",
      "epoch:40 step:31622 [D loss: 0.042207, acc.: 100.00%] [G loss: 3.699381]\n",
      "epoch:40 step:31623 [D loss: 0.015206, acc.: 100.00%] [G loss: 3.781285]\n",
      "epoch:40 step:31624 [D loss: 0.022840, acc.: 100.00%] [G loss: 3.334746]\n",
      "epoch:40 step:31625 [D loss: 0.308903, acc.: 85.16%] [G loss: 5.220730]\n",
      "epoch:40 step:31626 [D loss: 0.105647, acc.: 96.88%] [G loss: 5.457269]\n",
      "epoch:40 step:31627 [D loss: 0.074717, acc.: 99.22%] [G loss: 4.415098]\n",
      "epoch:40 step:31628 [D loss: 0.029068, acc.: 100.00%] [G loss: 4.128007]\n",
      "epoch:40 step:31629 [D loss: 0.011610, acc.: 100.00%] [G loss: 4.536932]\n",
      "epoch:40 step:31630 [D loss: 0.028384, acc.: 100.00%] [G loss: 4.407566]\n",
      "epoch:40 step:31631 [D loss: 0.010879, acc.: 100.00%] [G loss: 4.763044]\n",
      "epoch:40 step:31632 [D loss: 0.022475, acc.: 100.00%] [G loss: 4.314926]\n",
      "epoch:40 step:31633 [D loss: 0.083030, acc.: 97.66%] [G loss: 2.814556]\n",
      "epoch:40 step:31634 [D loss: 0.071700, acc.: 99.22%] [G loss: 3.518407]\n",
      "epoch:40 step:31635 [D loss: 0.023600, acc.: 99.22%] [G loss: 4.043336]\n",
      "epoch:40 step:31636 [D loss: 0.089377, acc.: 98.44%] [G loss: 5.120275]\n",
      "epoch:40 step:31637 [D loss: 0.052681, acc.: 98.44%] [G loss: 4.761552]\n",
      "epoch:40 step:31638 [D loss: 0.005875, acc.: 100.00%] [G loss: 4.289649]\n",
      "epoch:40 step:31639 [D loss: 0.006484, acc.: 100.00%] [G loss: 4.013237]\n",
      "epoch:40 step:31640 [D loss: 0.027478, acc.: 100.00%] [G loss: 3.146291]\n",
      "epoch:40 step:31641 [D loss: 0.094416, acc.: 97.66%] [G loss: 2.762577]\n",
      "epoch:40 step:31642 [D loss: 0.048646, acc.: 98.44%] [G loss: 3.997824]\n",
      "epoch:40 step:31643 [D loss: 0.124145, acc.: 96.09%] [G loss: 3.084435]\n",
      "epoch:40 step:31644 [D loss: 0.141491, acc.: 94.53%] [G loss: 5.044625]\n",
      "epoch:40 step:31645 [D loss: 0.652134, acc.: 67.19%] [G loss: 5.425321]\n",
      "epoch:40 step:31646 [D loss: 0.014749, acc.: 100.00%] [G loss: 6.325920]\n",
      "epoch:40 step:31647 [D loss: 0.046271, acc.: 98.44%] [G loss: 5.438520]\n",
      "epoch:40 step:31648 [D loss: 0.003045, acc.: 100.00%] [G loss: 5.703873]\n",
      "epoch:40 step:31649 [D loss: 0.012473, acc.: 99.22%] [G loss: 4.744343]\n",
      "epoch:40 step:31650 [D loss: 0.006491, acc.: 100.00%] [G loss: 4.852878]\n",
      "epoch:40 step:31651 [D loss: 0.010478, acc.: 100.00%] [G loss: 5.000522]\n",
      "epoch:40 step:31652 [D loss: 0.006242, acc.: 100.00%] [G loss: 3.767880]\n",
      "epoch:40 step:31653 [D loss: 0.086500, acc.: 98.44%] [G loss: 3.140166]\n",
      "epoch:40 step:31654 [D loss: 0.009265, acc.: 100.00%] [G loss: 4.624100]\n",
      "epoch:40 step:31655 [D loss: 0.007523, acc.: 100.00%] [G loss: 4.773892]\n",
      "epoch:40 step:31656 [D loss: 0.018006, acc.: 100.00%] [G loss: 4.352990]\n",
      "epoch:40 step:31657 [D loss: 0.039924, acc.: 100.00%] [G loss: 4.101196]\n",
      "epoch:40 step:31658 [D loss: 0.004497, acc.: 100.00%] [G loss: 4.825727]\n",
      "epoch:40 step:31659 [D loss: 0.015618, acc.: 100.00%] [G loss: 3.750232]\n",
      "epoch:40 step:31660 [D loss: 0.616516, acc.: 69.53%] [G loss: 6.078861]\n",
      "epoch:40 step:31661 [D loss: 0.897667, acc.: 57.81%] [G loss: 2.425211]\n",
      "epoch:40 step:31662 [D loss: 0.423993, acc.: 78.12%] [G loss: 6.568247]\n",
      "epoch:40 step:31663 [D loss: 0.971746, acc.: 59.38%] [G loss: 3.519935]\n",
      "epoch:40 step:31664 [D loss: 0.307888, acc.: 89.84%] [G loss: 4.355548]\n",
      "epoch:40 step:31665 [D loss: 0.012463, acc.: 100.00%] [G loss: 4.822989]\n",
      "epoch:40 step:31666 [D loss: 0.021688, acc.: 99.22%] [G loss: 4.838352]\n",
      "epoch:40 step:31667 [D loss: 0.052743, acc.: 98.44%] [G loss: 4.024693]\n",
      "epoch:40 step:31668 [D loss: 0.029500, acc.: 100.00%] [G loss: 3.593766]\n",
      "epoch:40 step:31669 [D loss: 0.066790, acc.: 98.44%] [G loss: 2.924437]\n",
      "epoch:40 step:31670 [D loss: 0.027766, acc.: 100.00%] [G loss: 2.790371]\n",
      "epoch:40 step:31671 [D loss: 0.134722, acc.: 98.44%] [G loss: 3.933086]\n",
      "epoch:40 step:31672 [D loss: 0.023646, acc.: 100.00%] [G loss: 4.336652]\n",
      "epoch:40 step:31673 [D loss: 0.038292, acc.: 99.22%] [G loss: 4.429940]\n",
      "epoch:40 step:31674 [D loss: 0.045516, acc.: 100.00%] [G loss: 3.740070]\n",
      "epoch:40 step:31675 [D loss: 0.030737, acc.: 100.00%] [G loss: 3.819032]\n",
      "epoch:40 step:31676 [D loss: 0.034497, acc.: 100.00%] [G loss: 3.642023]\n",
      "epoch:40 step:31677 [D loss: 0.017528, acc.: 100.00%] [G loss: 2.999123]\n",
      "epoch:40 step:31678 [D loss: 0.043023, acc.: 99.22%] [G loss: 3.639518]\n",
      "epoch:40 step:31679 [D loss: 0.035293, acc.: 100.00%] [G loss: 5.179758]\n",
      "epoch:40 step:31680 [D loss: 0.018496, acc.: 99.22%] [G loss: 4.622218]\n",
      "epoch:40 step:31681 [D loss: 0.178929, acc.: 95.31%] [G loss: 5.341216]\n",
      "epoch:40 step:31682 [D loss: 0.003544, acc.: 100.00%] [G loss: 6.007008]\n",
      "epoch:40 step:31683 [D loss: 0.055202, acc.: 100.00%] [G loss: 5.941686]\n",
      "epoch:40 step:31684 [D loss: 0.009915, acc.: 100.00%] [G loss: 4.814902]\n",
      "epoch:40 step:31685 [D loss: 0.006665, acc.: 100.00%] [G loss: 3.978414]\n",
      "epoch:40 step:31686 [D loss: 0.027870, acc.: 99.22%] [G loss: 4.551075]\n",
      "epoch:40 step:31687 [D loss: 0.009083, acc.: 100.00%] [G loss: 4.986110]\n",
      "epoch:40 step:31688 [D loss: 0.090013, acc.: 97.66%] [G loss: 3.945689]\n",
      "epoch:40 step:31689 [D loss: 0.042864, acc.: 100.00%] [G loss: 4.527339]\n",
      "epoch:40 step:31690 [D loss: 0.008469, acc.: 100.00%] [G loss: 5.400860]\n",
      "epoch:40 step:31691 [D loss: 0.050992, acc.: 98.44%] [G loss: 3.906291]\n",
      "epoch:40 step:31692 [D loss: 0.030102, acc.: 100.00%] [G loss: 4.332656]\n",
      "epoch:40 step:31693 [D loss: 0.090869, acc.: 96.88%] [G loss: 3.792563]\n",
      "epoch:40 step:31694 [D loss: 0.018422, acc.: 100.00%] [G loss: 4.889557]\n",
      "epoch:40 step:31695 [D loss: 0.028042, acc.: 99.22%] [G loss: 4.899169]\n",
      "epoch:40 step:31696 [D loss: 0.007411, acc.: 100.00%] [G loss: 4.464964]\n",
      "epoch:40 step:31697 [D loss: 0.010123, acc.: 100.00%] [G loss: 4.426914]\n",
      "epoch:40 step:31698 [D loss: 0.007823, acc.: 100.00%] [G loss: 3.791978]\n",
      "epoch:40 step:31699 [D loss: 0.847969, acc.: 61.72%] [G loss: 7.878338]\n",
      "epoch:40 step:31700 [D loss: 2.697207, acc.: 50.78%] [G loss: 5.730487]\n",
      "epoch:40 step:31701 [D loss: 0.327781, acc.: 82.81%] [G loss: 3.414613]\n",
      "epoch:40 step:31702 [D loss: 0.036496, acc.: 100.00%] [G loss: 2.193117]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:40 step:31703 [D loss: 0.102948, acc.: 98.44%] [G loss: 3.948856]\n",
      "epoch:40 step:31704 [D loss: 0.006706, acc.: 100.00%] [G loss: 4.009965]\n",
      "epoch:40 step:31705 [D loss: 0.005138, acc.: 100.00%] [G loss: 4.183813]\n",
      "epoch:40 step:31706 [D loss: 0.008578, acc.: 100.00%] [G loss: 3.271055]\n",
      "epoch:40 step:31707 [D loss: 0.019844, acc.: 100.00%] [G loss: 3.561411]\n",
      "epoch:40 step:31708 [D loss: 0.037410, acc.: 99.22%] [G loss: 2.295733]\n",
      "epoch:40 step:31709 [D loss: 0.091490, acc.: 98.44%] [G loss: 3.438337]\n",
      "epoch:40 step:31710 [D loss: 0.009774, acc.: 100.00%] [G loss: 4.559776]\n",
      "epoch:40 step:31711 [D loss: 0.010355, acc.: 100.00%] [G loss: 4.488941]\n",
      "epoch:40 step:31712 [D loss: 0.011121, acc.: 100.00%] [G loss: 4.535105]\n",
      "epoch:40 step:31713 [D loss: 0.013769, acc.: 100.00%] [G loss: 3.976164]\n",
      "epoch:40 step:31714 [D loss: 0.035048, acc.: 100.00%] [G loss: 3.288370]\n",
      "epoch:40 step:31715 [D loss: 0.058806, acc.: 100.00%] [G loss: 3.737711]\n",
      "epoch:40 step:31716 [D loss: 0.006706, acc.: 100.00%] [G loss: 4.030216]\n",
      "epoch:40 step:31717 [D loss: 0.023145, acc.: 100.00%] [G loss: 3.701849]\n",
      "epoch:40 step:31718 [D loss: 0.016918, acc.: 100.00%] [G loss: 4.125709]\n",
      "epoch:40 step:31719 [D loss: 0.026602, acc.: 100.00%] [G loss: 3.431585]\n",
      "epoch:40 step:31720 [D loss: 0.021216, acc.: 100.00%] [G loss: 4.261760]\n",
      "epoch:40 step:31721 [D loss: 0.028744, acc.: 100.00%] [G loss: 4.198433]\n",
      "epoch:40 step:31722 [D loss: 0.030227, acc.: 99.22%] [G loss: 3.958609]\n",
      "epoch:40 step:31723 [D loss: 0.026902, acc.: 100.00%] [G loss: 3.962172]\n",
      "epoch:40 step:31724 [D loss: 0.027034, acc.: 100.00%] [G loss: 4.352785]\n",
      "epoch:40 step:31725 [D loss: 0.101838, acc.: 98.44%] [G loss: 3.022537]\n",
      "epoch:40 step:31726 [D loss: 0.037255, acc.: 100.00%] [G loss: 3.838840]\n",
      "epoch:40 step:31727 [D loss: 0.013094, acc.: 100.00%] [G loss: 4.520744]\n",
      "epoch:40 step:31728 [D loss: 0.014690, acc.: 100.00%] [G loss: 4.909198]\n",
      "epoch:40 step:31729 [D loss: 0.014351, acc.: 100.00%] [G loss: 4.531806]\n",
      "epoch:40 step:31730 [D loss: 0.003773, acc.: 100.00%] [G loss: 5.283216]\n",
      "epoch:40 step:31731 [D loss: 0.037597, acc.: 100.00%] [G loss: 4.040661]\n",
      "epoch:40 step:31732 [D loss: 0.174549, acc.: 95.31%] [G loss: 3.129454]\n",
      "epoch:40 step:31733 [D loss: 0.014595, acc.: 100.00%] [G loss: 4.040458]\n",
      "epoch:40 step:31734 [D loss: 0.010755, acc.: 100.00%] [G loss: 4.859331]\n",
      "epoch:40 step:31735 [D loss: 0.013618, acc.: 100.00%] [G loss: 3.887748]\n",
      "epoch:40 step:31736 [D loss: 0.013096, acc.: 100.00%] [G loss: 4.089110]\n",
      "epoch:40 step:31737 [D loss: 0.018284, acc.: 100.00%] [G loss: 3.848388]\n",
      "epoch:40 step:31738 [D loss: 0.068360, acc.: 100.00%] [G loss: 3.532292]\n",
      "epoch:40 step:31739 [D loss: 0.057725, acc.: 100.00%] [G loss: 4.531738]\n",
      "epoch:40 step:31740 [D loss: 0.009807, acc.: 100.00%] [G loss: 5.393158]\n",
      "epoch:40 step:31741 [D loss: 0.013716, acc.: 100.00%] [G loss: 4.372765]\n",
      "epoch:40 step:31742 [D loss: 0.005579, acc.: 100.00%] [G loss: 4.721562]\n",
      "epoch:40 step:31743 [D loss: 0.006008, acc.: 100.00%] [G loss: 3.006384]\n",
      "epoch:40 step:31744 [D loss: 0.005811, acc.: 100.00%] [G loss: 3.514059]\n",
      "epoch:40 step:31745 [D loss: 0.036229, acc.: 99.22%] [G loss: 2.281059]\n",
      "epoch:40 step:31746 [D loss: 0.063030, acc.: 100.00%] [G loss: 5.291583]\n",
      "epoch:40 step:31747 [D loss: 0.044807, acc.: 98.44%] [G loss: 4.480453]\n",
      "epoch:40 step:31748 [D loss: 0.172615, acc.: 95.31%] [G loss: 1.731282]\n",
      "epoch:40 step:31749 [D loss: 0.639253, acc.: 68.75%] [G loss: 8.285861]\n",
      "epoch:40 step:31750 [D loss: 2.143364, acc.: 50.78%] [G loss: 5.346469]\n",
      "epoch:40 step:31751 [D loss: 0.163076, acc.: 92.19%] [G loss: 3.837536]\n",
      "epoch:40 step:31752 [D loss: 0.384462, acc.: 82.03%] [G loss: 5.559045]\n",
      "epoch:40 step:31753 [D loss: 0.225390, acc.: 89.84%] [G loss: 5.170944]\n",
      "epoch:40 step:31754 [D loss: 0.029945, acc.: 99.22%] [G loss: 4.923028]\n",
      "epoch:40 step:31755 [D loss: 0.034690, acc.: 99.22%] [G loss: 4.885174]\n",
      "epoch:40 step:31756 [D loss: 0.061080, acc.: 99.22%] [G loss: 4.330925]\n",
      "epoch:40 step:31757 [D loss: 0.021071, acc.: 100.00%] [G loss: 4.721168]\n",
      "epoch:40 step:31758 [D loss: 0.012247, acc.: 100.00%] [G loss: 4.211204]\n",
      "epoch:40 step:31759 [D loss: 0.015524, acc.: 100.00%] [G loss: 4.026898]\n",
      "epoch:40 step:31760 [D loss: 0.016371, acc.: 100.00%] [G loss: 4.668093]\n",
      "epoch:40 step:31761 [D loss: 0.035804, acc.: 99.22%] [G loss: 3.836324]\n",
      "epoch:40 step:31762 [D loss: 0.058832, acc.: 99.22%] [G loss: 3.658522]\n",
      "epoch:40 step:31763 [D loss: 0.021706, acc.: 100.00%] [G loss: 3.801465]\n",
      "epoch:40 step:31764 [D loss: 0.050353, acc.: 99.22%] [G loss: 3.412044]\n",
      "epoch:40 step:31765 [D loss: 0.015531, acc.: 100.00%] [G loss: 3.586490]\n",
      "epoch:40 step:31766 [D loss: 0.061564, acc.: 99.22%] [G loss: 4.207359]\n",
      "epoch:40 step:31767 [D loss: 0.019329, acc.: 100.00%] [G loss: 4.522570]\n",
      "epoch:40 step:31768 [D loss: 0.019003, acc.: 100.00%] [G loss: 3.991745]\n",
      "epoch:40 step:31769 [D loss: 0.046314, acc.: 99.22%] [G loss: 3.889073]\n",
      "epoch:40 step:31770 [D loss: 0.026426, acc.: 100.00%] [G loss: 4.901543]\n",
      "epoch:40 step:31771 [D loss: 0.011905, acc.: 100.00%] [G loss: 4.269932]\n",
      "epoch:40 step:31772 [D loss: 1.186006, acc.: 49.22%] [G loss: 7.028281]\n",
      "epoch:40 step:31773 [D loss: 2.495833, acc.: 50.00%] [G loss: 5.042094]\n",
      "epoch:40 step:31774 [D loss: 0.829737, acc.: 64.06%] [G loss: 2.479493]\n",
      "epoch:40 step:31775 [D loss: 0.435954, acc.: 82.03%] [G loss: 4.469231]\n",
      "epoch:40 step:31776 [D loss: 0.250167, acc.: 83.59%] [G loss: 4.289540]\n",
      "epoch:40 step:31777 [D loss: 0.082338, acc.: 98.44%] [G loss: 3.899656]\n",
      "epoch:40 step:31778 [D loss: 0.078500, acc.: 98.44%] [G loss: 3.689975]\n",
      "epoch:40 step:31779 [D loss: 0.019578, acc.: 100.00%] [G loss: 3.265956]\n",
      "epoch:40 step:31780 [D loss: 0.069940, acc.: 99.22%] [G loss: 3.396794]\n",
      "epoch:40 step:31781 [D loss: 0.038604, acc.: 100.00%] [G loss: 3.435350]\n",
      "epoch:40 step:31782 [D loss: 0.042679, acc.: 100.00%] [G loss: 3.100562]\n",
      "epoch:40 step:31783 [D loss: 0.055475, acc.: 99.22%] [G loss: 3.689134]\n",
      "epoch:40 step:31784 [D loss: 0.027516, acc.: 100.00%] [G loss: 3.693398]\n",
      "epoch:40 step:31785 [D loss: 0.019644, acc.: 100.00%] [G loss: 3.561383]\n",
      "epoch:40 step:31786 [D loss: 0.021050, acc.: 100.00%] [G loss: 3.388135]\n",
      "epoch:40 step:31787 [D loss: 0.055451, acc.: 100.00%] [G loss: 3.464329]\n",
      "epoch:40 step:31788 [D loss: 0.030250, acc.: 100.00%] [G loss: 3.944471]\n",
      "epoch:40 step:31789 [D loss: 0.013630, acc.: 100.00%] [G loss: 3.369368]\n",
      "epoch:40 step:31790 [D loss: 0.037507, acc.: 99.22%] [G loss: 2.907162]\n",
      "epoch:40 step:31791 [D loss: 0.225143, acc.: 90.62%] [G loss: 5.125892]\n",
      "epoch:40 step:31792 [D loss: 0.484377, acc.: 76.56%] [G loss: 4.045743]\n",
      "epoch:40 step:31793 [D loss: 0.025151, acc.: 100.00%] [G loss: 3.423344]\n",
      "epoch:40 step:31794 [D loss: 0.035359, acc.: 100.00%] [G loss: 3.691093]\n",
      "epoch:40 step:31795 [D loss: 0.008128, acc.: 100.00%] [G loss: 3.034470]\n",
      "epoch:40 step:31796 [D loss: 0.032423, acc.: 100.00%] [G loss: 3.440268]\n",
      "epoch:40 step:31797 [D loss: 0.025383, acc.: 99.22%] [G loss: 3.335609]\n",
      "epoch:40 step:31798 [D loss: 0.034656, acc.: 100.00%] [G loss: 3.826159]\n",
      "epoch:40 step:31799 [D loss: 0.026237, acc.: 100.00%] [G loss: 4.116339]\n",
      "epoch:40 step:31800 [D loss: 0.021212, acc.: 100.00%] [G loss: 3.768609]\n",
      "##############\n",
      "[0.97787646 1.08932399 0.92722552 0.91613921 2.12442825 0.99589964\n",
      " 2.10779339 2.11349174 0.96129234 2.10852105]\n",
      "##########\n",
      "epoch:40 step:31801 [D loss: 0.039686, acc.: 100.00%] [G loss: 3.557081]\n",
      "epoch:40 step:31802 [D loss: 0.185312, acc.: 95.31%] [G loss: 2.699862]\n",
      "epoch:40 step:31803 [D loss: 0.019013, acc.: 100.00%] [G loss: 4.481386]\n",
      "epoch:40 step:31804 [D loss: 0.006739, acc.: 100.00%] [G loss: 3.745491]\n",
      "epoch:40 step:31805 [D loss: 0.039203, acc.: 99.22%] [G loss: 3.976208]\n",
      "epoch:40 step:31806 [D loss: 0.034527, acc.: 100.00%] [G loss: 3.463809]\n",
      "epoch:40 step:31807 [D loss: 0.016519, acc.: 100.00%] [G loss: 3.921182]\n",
      "epoch:40 step:31808 [D loss: 0.017707, acc.: 100.00%] [G loss: 4.082535]\n",
      "epoch:40 step:31809 [D loss: 0.011056, acc.: 100.00%] [G loss: 4.126139]\n",
      "epoch:40 step:31810 [D loss: 0.015169, acc.: 100.00%] [G loss: 4.146398]\n",
      "epoch:40 step:31811 [D loss: 0.021708, acc.: 100.00%] [G loss: 3.576269]\n",
      "epoch:40 step:31812 [D loss: 0.014664, acc.: 100.00%] [G loss: 4.311293]\n",
      "epoch:40 step:31813 [D loss: 0.036787, acc.: 100.00%] [G loss: 3.540478]\n",
      "epoch:40 step:31814 [D loss: 0.025717, acc.: 100.00%] [G loss: 4.336689]\n",
      "epoch:40 step:31815 [D loss: 0.163360, acc.: 97.66%] [G loss: 3.370903]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:40 step:31816 [D loss: 0.019480, acc.: 100.00%] [G loss: 4.128328]\n",
      "epoch:40 step:31817 [D loss: 0.030445, acc.: 100.00%] [G loss: 4.220955]\n",
      "epoch:40 step:31818 [D loss: 0.017498, acc.: 100.00%] [G loss: 4.333169]\n",
      "epoch:40 step:31819 [D loss: 0.022435, acc.: 100.00%] [G loss: 4.654978]\n",
      "epoch:40 step:31820 [D loss: 0.040854, acc.: 99.22%] [G loss: 4.099384]\n",
      "epoch:40 step:31821 [D loss: 0.015594, acc.: 100.00%] [G loss: 3.043352]\n",
      "epoch:40 step:31822 [D loss: 0.006288, acc.: 100.00%] [G loss: 3.801144]\n",
      "epoch:40 step:31823 [D loss: 0.014812, acc.: 100.00%] [G loss: 4.142886]\n",
      "epoch:40 step:31824 [D loss: 0.011900, acc.: 100.00%] [G loss: 3.328897]\n",
      "epoch:40 step:31825 [D loss: 0.018205, acc.: 100.00%] [G loss: 4.746399]\n",
      "epoch:40 step:31826 [D loss: 0.009854, acc.: 100.00%] [G loss: 3.843493]\n",
      "epoch:40 step:31827 [D loss: 0.012694, acc.: 100.00%] [G loss: 4.699936]\n",
      "epoch:40 step:31828 [D loss: 0.035673, acc.: 100.00%] [G loss: 4.298708]\n",
      "epoch:40 step:31829 [D loss: 0.013997, acc.: 100.00%] [G loss: 4.842557]\n",
      "epoch:40 step:31830 [D loss: 0.045473, acc.: 99.22%] [G loss: 3.754322]\n",
      "epoch:40 step:31831 [D loss: 0.020443, acc.: 100.00%] [G loss: 3.028843]\n",
      "epoch:40 step:31832 [D loss: 0.010520, acc.: 100.00%] [G loss: 4.187765]\n",
      "epoch:40 step:31833 [D loss: 0.007945, acc.: 100.00%] [G loss: 3.827139]\n",
      "epoch:40 step:31834 [D loss: 0.022965, acc.: 100.00%] [G loss: 4.382175]\n",
      "epoch:40 step:31835 [D loss: 0.009651, acc.: 100.00%] [G loss: 5.211042]\n",
      "epoch:40 step:31836 [D loss: 0.134976, acc.: 95.31%] [G loss: 3.876196]\n",
      "epoch:40 step:31837 [D loss: 0.013187, acc.: 100.00%] [G loss: 4.643049]\n",
      "epoch:40 step:31838 [D loss: 0.006113, acc.: 100.00%] [G loss: 4.845754]\n",
      "epoch:40 step:31839 [D loss: 0.051677, acc.: 98.44%] [G loss: 3.905408]\n",
      "epoch:40 step:31840 [D loss: 0.006529, acc.: 100.00%] [G loss: 4.376392]\n",
      "epoch:40 step:31841 [D loss: 0.010771, acc.: 100.00%] [G loss: 3.839503]\n",
      "epoch:40 step:31842 [D loss: 0.026356, acc.: 100.00%] [G loss: 4.167970]\n",
      "epoch:40 step:31843 [D loss: 0.006154, acc.: 100.00%] [G loss: 4.820075]\n",
      "epoch:40 step:31844 [D loss: 1.846011, acc.: 29.69%] [G loss: 7.126612]\n",
      "epoch:40 step:31845 [D loss: 1.137697, acc.: 55.47%] [G loss: 5.560985]\n",
      "epoch:40 step:31846 [D loss: 1.429931, acc.: 40.62%] [G loss: 3.320098]\n",
      "epoch:40 step:31847 [D loss: 0.007865, acc.: 100.00%] [G loss: 4.128101]\n",
      "epoch:40 step:31848 [D loss: 0.054399, acc.: 98.44%] [G loss: 4.960945]\n",
      "epoch:40 step:31849 [D loss: 0.098397, acc.: 99.22%] [G loss: 3.281394]\n",
      "epoch:40 step:31850 [D loss: 0.039511, acc.: 100.00%] [G loss: 2.541805]\n",
      "epoch:40 step:31851 [D loss: 0.045187, acc.: 100.00%] [G loss: 2.988907]\n",
      "epoch:40 step:31852 [D loss: 0.098897, acc.: 96.09%] [G loss: 3.513021]\n",
      "epoch:40 step:31853 [D loss: 0.136153, acc.: 95.31%] [G loss: 3.118583]\n",
      "epoch:40 step:31854 [D loss: 0.098863, acc.: 98.44%] [G loss: 4.399973]\n",
      "epoch:40 step:31855 [D loss: 0.029634, acc.: 100.00%] [G loss: 4.478018]\n",
      "epoch:40 step:31856 [D loss: 0.111484, acc.: 97.66%] [G loss: 3.667993]\n",
      "epoch:40 step:31857 [D loss: 0.024428, acc.: 100.00%] [G loss: 2.557510]\n",
      "epoch:40 step:31858 [D loss: 0.017096, acc.: 100.00%] [G loss: 2.750601]\n",
      "epoch:40 step:31859 [D loss: 0.154203, acc.: 96.88%] [G loss: 3.921127]\n",
      "epoch:40 step:31860 [D loss: 0.015308, acc.: 100.00%] [G loss: 5.059209]\n",
      "epoch:40 step:31861 [D loss: 0.086383, acc.: 96.88%] [G loss: 3.378406]\n",
      "epoch:40 step:31862 [D loss: 0.061564, acc.: 99.22%] [G loss: 3.142205]\n",
      "epoch:40 step:31863 [D loss: 0.839096, acc.: 55.47%] [G loss: 6.873362]\n",
      "epoch:40 step:31864 [D loss: 1.065393, acc.: 57.81%] [G loss: 5.106979]\n",
      "epoch:40 step:31865 [D loss: 0.065363, acc.: 96.88%] [G loss: 4.214826]\n",
      "epoch:40 step:31866 [D loss: 0.022033, acc.: 100.00%] [G loss: 3.487777]\n",
      "epoch:40 step:31867 [D loss: 0.021247, acc.: 100.00%] [G loss: 3.266533]\n",
      "epoch:40 step:31868 [D loss: 0.032833, acc.: 100.00%] [G loss: 3.408541]\n",
      "epoch:40 step:31869 [D loss: 0.043660, acc.: 99.22%] [G loss: 3.541066]\n",
      "epoch:40 step:31870 [D loss: 0.022735, acc.: 100.00%] [G loss: 3.838993]\n",
      "epoch:40 step:31871 [D loss: 0.054967, acc.: 99.22%] [G loss: 3.384125]\n",
      "epoch:40 step:31872 [D loss: 0.015241, acc.: 100.00%] [G loss: 3.680998]\n",
      "epoch:40 step:31873 [D loss: 0.063460, acc.: 99.22%] [G loss: 3.791075]\n",
      "epoch:40 step:31874 [D loss: 0.032967, acc.: 100.00%] [G loss: 3.428486]\n",
      "epoch:40 step:31875 [D loss: 0.035399, acc.: 100.00%] [G loss: 3.351130]\n",
      "epoch:40 step:31876 [D loss: 0.067056, acc.: 99.22%] [G loss: 3.731885]\n",
      "epoch:40 step:31877 [D loss: 0.044257, acc.: 100.00%] [G loss: 3.189844]\n",
      "epoch:40 step:31878 [D loss: 0.016563, acc.: 100.00%] [G loss: 3.151271]\n",
      "epoch:40 step:31879 [D loss: 0.055926, acc.: 100.00%] [G loss: 4.147791]\n",
      "epoch:40 step:31880 [D loss: 0.021815, acc.: 100.00%] [G loss: 4.430734]\n",
      "epoch:40 step:31881 [D loss: 0.086511, acc.: 97.66%] [G loss: 2.926385]\n",
      "epoch:40 step:31882 [D loss: 0.126331, acc.: 95.31%] [G loss: 3.774701]\n",
      "epoch:40 step:31883 [D loss: 0.021642, acc.: 100.00%] [G loss: 4.631350]\n",
      "epoch:40 step:31884 [D loss: 0.011083, acc.: 100.00%] [G loss: 4.626380]\n",
      "epoch:40 step:31885 [D loss: 0.014261, acc.: 100.00%] [G loss: 4.329286]\n",
      "epoch:40 step:31886 [D loss: 1.126400, acc.: 40.62%] [G loss: 5.323745]\n",
      "epoch:40 step:31887 [D loss: 0.111459, acc.: 96.09%] [G loss: 5.596361]\n",
      "epoch:40 step:31888 [D loss: 0.202501, acc.: 89.06%] [G loss: 4.539147]\n",
      "epoch:40 step:31889 [D loss: 0.019102, acc.: 100.00%] [G loss: 4.227657]\n",
      "epoch:40 step:31890 [D loss: 0.036375, acc.: 100.00%] [G loss: 3.697267]\n",
      "epoch:40 step:31891 [D loss: 0.006954, acc.: 100.00%] [G loss: 4.807838]\n",
      "epoch:40 step:31892 [D loss: 0.005015, acc.: 100.00%] [G loss: 3.891631]\n",
      "epoch:40 step:31893 [D loss: 0.016306, acc.: 100.00%] [G loss: 3.660886]\n",
      "epoch:40 step:31894 [D loss: 0.023196, acc.: 100.00%] [G loss: 4.307968]\n",
      "epoch:40 step:31895 [D loss: 0.007281, acc.: 100.00%] [G loss: 4.063681]\n",
      "epoch:40 step:31896 [D loss: 0.012278, acc.: 100.00%] [G loss: 4.193769]\n",
      "epoch:40 step:31897 [D loss: 0.014063, acc.: 100.00%] [G loss: 4.450531]\n",
      "epoch:40 step:31898 [D loss: 0.027627, acc.: 100.00%] [G loss: 3.941153]\n",
      "epoch:40 step:31899 [D loss: 0.015732, acc.: 100.00%] [G loss: 4.407835]\n",
      "epoch:40 step:31900 [D loss: 0.009971, acc.: 100.00%] [G loss: 4.968841]\n",
      "epoch:40 step:31901 [D loss: 0.011176, acc.: 100.00%] [G loss: 4.831704]\n",
      "epoch:40 step:31902 [D loss: 0.081920, acc.: 98.44%] [G loss: 4.060649]\n",
      "epoch:40 step:31903 [D loss: 0.021762, acc.: 100.00%] [G loss: 3.677973]\n",
      "epoch:40 step:31904 [D loss: 0.030954, acc.: 100.00%] [G loss: 4.055873]\n",
      "epoch:40 step:31905 [D loss: 0.024569, acc.: 99.22%] [G loss: 4.488430]\n",
      "epoch:40 step:31906 [D loss: 0.009507, acc.: 100.00%] [G loss: 4.369941]\n",
      "epoch:40 step:31907 [D loss: 0.014344, acc.: 100.00%] [G loss: 4.581287]\n",
      "epoch:40 step:31908 [D loss: 0.023145, acc.: 100.00%] [G loss: 4.734053]\n",
      "epoch:40 step:31909 [D loss: 0.015891, acc.: 100.00%] [G loss: 3.720272]\n",
      "epoch:40 step:31910 [D loss: 0.029895, acc.: 100.00%] [G loss: 4.017993]\n",
      "epoch:40 step:31911 [D loss: 0.015919, acc.: 100.00%] [G loss: 4.118401]\n",
      "epoch:40 step:31912 [D loss: 0.026640, acc.: 100.00%] [G loss: 4.388394]\n",
      "epoch:40 step:31913 [D loss: 0.074481, acc.: 97.66%] [G loss: 4.275587]\n",
      "epoch:40 step:31914 [D loss: 0.016562, acc.: 100.00%] [G loss: 4.196654]\n",
      "epoch:40 step:31915 [D loss: 0.004196, acc.: 100.00%] [G loss: 4.430295]\n",
      "epoch:40 step:31916 [D loss: 0.031620, acc.: 100.00%] [G loss: 4.300031]\n",
      "epoch:40 step:31917 [D loss: 0.027340, acc.: 100.00%] [G loss: 4.538234]\n",
      "epoch:40 step:31918 [D loss: 0.028703, acc.: 100.00%] [G loss: 4.161180]\n",
      "epoch:40 step:31919 [D loss: 0.018506, acc.: 100.00%] [G loss: 4.099804]\n",
      "epoch:40 step:31920 [D loss: 0.011619, acc.: 100.00%] [G loss: 4.502528]\n",
      "epoch:40 step:31921 [D loss: 0.037824, acc.: 100.00%] [G loss: 4.923527]\n",
      "epoch:40 step:31922 [D loss: 0.017853, acc.: 100.00%] [G loss: 3.301072]\n",
      "epoch:40 step:31923 [D loss: 0.023470, acc.: 100.00%] [G loss: 4.702358]\n",
      "epoch:40 step:31924 [D loss: 0.088435, acc.: 99.22%] [G loss: 4.448762]\n",
      "epoch:40 step:31925 [D loss: 0.544361, acc.: 71.88%] [G loss: 5.769537]\n",
      "epoch:40 step:31926 [D loss: 0.027990, acc.: 100.00%] [G loss: 6.055494]\n",
      "epoch:40 step:31927 [D loss: 0.170753, acc.: 91.41%] [G loss: 3.658374]\n",
      "epoch:40 step:31928 [D loss: 0.178722, acc.: 95.31%] [G loss: 6.037418]\n",
      "epoch:40 step:31929 [D loss: 0.050116, acc.: 98.44%] [G loss: 6.404090]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:40 step:31930 [D loss: 0.437399, acc.: 79.69%] [G loss: 3.176176]\n",
      "epoch:40 step:31931 [D loss: 0.998023, acc.: 66.41%] [G loss: 7.539248]\n",
      "epoch:40 step:31932 [D loss: 1.963076, acc.: 50.00%] [G loss: 5.239252]\n",
      "epoch:40 step:31933 [D loss: 0.027090, acc.: 99.22%] [G loss: 4.916258]\n",
      "epoch:40 step:31934 [D loss: 0.107309, acc.: 94.53%] [G loss: 5.243643]\n",
      "epoch:40 step:31935 [D loss: 0.017230, acc.: 99.22%] [G loss: 5.630469]\n",
      "epoch:40 step:31936 [D loss: 0.093213, acc.: 96.88%] [G loss: 4.279036]\n",
      "epoch:40 step:31937 [D loss: 0.043394, acc.: 100.00%] [G loss: 2.948823]\n",
      "epoch:40 step:31938 [D loss: 0.064392, acc.: 100.00%] [G loss: 3.577425]\n",
      "epoch:40 step:31939 [D loss: 0.018147, acc.: 100.00%] [G loss: 3.912992]\n",
      "epoch:40 step:31940 [D loss: 0.168876, acc.: 94.53%] [G loss: 2.469678]\n",
      "epoch:40 step:31941 [D loss: 0.203766, acc.: 93.75%] [G loss: 5.037915]\n",
      "epoch:40 step:31942 [D loss: 0.065763, acc.: 98.44%] [G loss: 5.250048]\n",
      "epoch:40 step:31943 [D loss: 0.590809, acc.: 72.66%] [G loss: 0.886510]\n",
      "epoch:40 step:31944 [D loss: 0.967744, acc.: 56.25%] [G loss: 6.107094]\n",
      "epoch:40 step:31945 [D loss: 0.433818, acc.: 75.00%] [G loss: 6.078417]\n",
      "epoch:40 step:31946 [D loss: 0.067258, acc.: 96.88%] [G loss: 5.810280]\n",
      "epoch:40 step:31947 [D loss: 0.007521, acc.: 100.00%] [G loss: 5.231727]\n",
      "epoch:40 step:31948 [D loss: 0.008545, acc.: 100.00%] [G loss: 5.111217]\n",
      "epoch:40 step:31949 [D loss: 0.018124, acc.: 100.00%] [G loss: 4.829982]\n",
      "epoch:40 step:31950 [D loss: 0.013003, acc.: 100.00%] [G loss: 4.500404]\n",
      "epoch:40 step:31951 [D loss: 0.012310, acc.: 100.00%] [G loss: 4.178092]\n",
      "epoch:40 step:31952 [D loss: 0.021556, acc.: 100.00%] [G loss: 4.082840]\n",
      "epoch:40 step:31953 [D loss: 0.615932, acc.: 72.66%] [G loss: 2.766515]\n",
      "epoch:40 step:31954 [D loss: 0.038733, acc.: 100.00%] [G loss: 3.842056]\n",
      "epoch:40 step:31955 [D loss: 0.030393, acc.: 100.00%] [G loss: 3.757236]\n",
      "epoch:40 step:31956 [D loss: 0.038761, acc.: 100.00%] [G loss: 4.160583]\n",
      "epoch:40 step:31957 [D loss: 0.005787, acc.: 100.00%] [G loss: 4.636084]\n",
      "epoch:40 step:31958 [D loss: 0.010359, acc.: 100.00%] [G loss: 4.307111]\n",
      "epoch:40 step:31959 [D loss: 0.012015, acc.: 100.00%] [G loss: 4.205740]\n",
      "epoch:40 step:31960 [D loss: 0.009040, acc.: 100.00%] [G loss: 4.177150]\n",
      "epoch:40 step:31961 [D loss: 0.011057, acc.: 100.00%] [G loss: 3.871520]\n",
      "epoch:40 step:31962 [D loss: 0.055639, acc.: 99.22%] [G loss: 3.604931]\n",
      "epoch:40 step:31963 [D loss: 0.020586, acc.: 100.00%] [G loss: 3.654886]\n",
      "epoch:40 step:31964 [D loss: 0.049063, acc.: 99.22%] [G loss: 3.671169]\n",
      "epoch:40 step:31965 [D loss: 0.307169, acc.: 89.06%] [G loss: 3.994462]\n",
      "epoch:40 step:31966 [D loss: 0.010079, acc.: 100.00%] [G loss: 4.487859]\n",
      "epoch:40 step:31967 [D loss: 0.012445, acc.: 100.00%] [G loss: 4.713127]\n",
      "epoch:40 step:31968 [D loss: 0.025188, acc.: 100.00%] [G loss: 4.266149]\n",
      "epoch:40 step:31969 [D loss: 0.012173, acc.: 100.00%] [G loss: 4.130369]\n",
      "epoch:40 step:31970 [D loss: 0.015077, acc.: 100.00%] [G loss: 3.651305]\n",
      "epoch:40 step:31971 [D loss: 0.029477, acc.: 100.00%] [G loss: 3.981801]\n",
      "epoch:40 step:31972 [D loss: 0.011786, acc.: 100.00%] [G loss: 4.544436]\n",
      "epoch:40 step:31973 [D loss: 0.025931, acc.: 100.00%] [G loss: 4.107420]\n",
      "epoch:40 step:31974 [D loss: 0.010833, acc.: 100.00%] [G loss: 4.075269]\n",
      "epoch:40 step:31975 [D loss: 0.175289, acc.: 96.09%] [G loss: 3.701422]\n",
      "epoch:40 step:31976 [D loss: 0.041787, acc.: 100.00%] [G loss: 3.742617]\n",
      "epoch:40 step:31977 [D loss: 0.018045, acc.: 100.00%] [G loss: 4.715133]\n",
      "epoch:40 step:31978 [D loss: 0.016581, acc.: 100.00%] [G loss: 4.831636]\n",
      "epoch:40 step:31979 [D loss: 0.083481, acc.: 96.88%] [G loss: 3.424053]\n",
      "epoch:40 step:31980 [D loss: 0.029738, acc.: 99.22%] [G loss: 4.295350]\n",
      "epoch:40 step:31981 [D loss: 0.033432, acc.: 100.00%] [G loss: 4.084556]\n",
      "epoch:40 step:31982 [D loss: 0.026563, acc.: 100.00%] [G loss: 4.250243]\n",
      "epoch:40 step:31983 [D loss: 0.040801, acc.: 99.22%] [G loss: 4.341985]\n",
      "epoch:40 step:31984 [D loss: 0.018052, acc.: 100.00%] [G loss: 4.307967]\n",
      "epoch:40 step:31985 [D loss: 0.021341, acc.: 100.00%] [G loss: 4.776828]\n",
      "epoch:40 step:31986 [D loss: 0.018532, acc.: 100.00%] [G loss: 4.346866]\n",
      "epoch:40 step:31987 [D loss: 0.095290, acc.: 98.44%] [G loss: 4.324233]\n",
      "epoch:40 step:31988 [D loss: 0.035629, acc.: 99.22%] [G loss: 4.909204]\n",
      "epoch:40 step:31989 [D loss: 0.012087, acc.: 100.00%] [G loss: 4.480978]\n",
      "epoch:40 step:31990 [D loss: 0.005130, acc.: 100.00%] [G loss: 4.602108]\n",
      "epoch:40 step:31991 [D loss: 0.004910, acc.: 100.00%] [G loss: 4.883041]\n",
      "epoch:40 step:31992 [D loss: 0.012052, acc.: 100.00%] [G loss: 4.185298]\n",
      "epoch:40 step:31993 [D loss: 0.173874, acc.: 96.09%] [G loss: 4.624794]\n",
      "epoch:40 step:31994 [D loss: 0.010294, acc.: 100.00%] [G loss: 5.362064]\n",
      "epoch:40 step:31995 [D loss: 0.009127, acc.: 100.00%] [G loss: 5.288794]\n",
      "epoch:40 step:31996 [D loss: 0.053840, acc.: 97.66%] [G loss: 4.121109]\n",
      "epoch:40 step:31997 [D loss: 0.042189, acc.: 100.00%] [G loss: 4.307121]\n",
      "epoch:40 step:31998 [D loss: 0.017127, acc.: 100.00%] [G loss: 4.727347]\n",
      "epoch:40 step:31999 [D loss: 0.008411, acc.: 100.00%] [G loss: 4.674658]\n",
      "epoch:40 step:32000 [D loss: 0.011606, acc.: 100.00%] [G loss: 4.546744]\n",
      "##############\n",
      "[0.94476378 0.84700401 0.85183307 1.02508065 1.04293895 1.00373737\n",
      " 1.09185131 0.88292262 1.11172531 0.90964878]\n",
      "##########\n",
      "epoch:40 step:32001 [D loss: 0.013433, acc.: 100.00%] [G loss: 4.018468]\n",
      "epoch:40 step:32002 [D loss: 0.071974, acc.: 97.66%] [G loss: 2.815188]\n",
      "epoch:40 step:32003 [D loss: 0.054279, acc.: 99.22%] [G loss: 4.706689]\n",
      "epoch:40 step:32004 [D loss: 0.005399, acc.: 100.00%] [G loss: 4.813551]\n",
      "epoch:40 step:32005 [D loss: 0.006799, acc.: 100.00%] [G loss: 5.338649]\n",
      "epoch:40 step:32006 [D loss: 0.039841, acc.: 99.22%] [G loss: 4.720057]\n",
      "epoch:40 step:32007 [D loss: 0.055387, acc.: 99.22%] [G loss: 4.856046]\n",
      "epoch:40 step:32008 [D loss: 0.094001, acc.: 97.66%] [G loss: 3.842313]\n",
      "epoch:40 step:32009 [D loss: 0.029166, acc.: 100.00%] [G loss: 4.260645]\n",
      "epoch:40 step:32010 [D loss: 0.006360, acc.: 100.00%] [G loss: 3.533000]\n",
      "epoch:40 step:32011 [D loss: 0.046948, acc.: 99.22%] [G loss: 4.954200]\n",
      "epoch:40 step:32012 [D loss: 0.004230, acc.: 100.00%] [G loss: 6.229408]\n",
      "epoch:40 step:32013 [D loss: 0.003350, acc.: 100.00%] [G loss: 5.539947]\n",
      "epoch:40 step:32014 [D loss: 0.002844, acc.: 100.00%] [G loss: 5.660503]\n",
      "epoch:40 step:32015 [D loss: 0.007491, acc.: 100.00%] [G loss: 5.117809]\n",
      "epoch:40 step:32016 [D loss: 0.021409, acc.: 100.00%] [G loss: 4.072041]\n",
      "epoch:40 step:32017 [D loss: 0.025968, acc.: 99.22%] [G loss: 4.565987]\n",
      "epoch:40 step:32018 [D loss: 0.018413, acc.: 100.00%] [G loss: 4.716464]\n",
      "epoch:40 step:32019 [D loss: 0.151891, acc.: 96.88%] [G loss: 5.753294]\n",
      "epoch:40 step:32020 [D loss: 0.002648, acc.: 100.00%] [G loss: 6.140500]\n",
      "epoch:40 step:32021 [D loss: 0.002032, acc.: 100.00%] [G loss: 6.503912]\n",
      "epoch:41 step:32022 [D loss: 0.020930, acc.: 100.00%] [G loss: 5.974296]\n",
      "epoch:41 step:32023 [D loss: 0.006168, acc.: 100.00%] [G loss: 5.393643]\n",
      "epoch:41 step:32024 [D loss: 0.004532, acc.: 100.00%] [G loss: 5.081711]\n",
      "epoch:41 step:32025 [D loss: 0.008055, acc.: 100.00%] [G loss: 4.841056]\n",
      "epoch:41 step:32026 [D loss: 0.008890, acc.: 100.00%] [G loss: 5.182986]\n",
      "epoch:41 step:32027 [D loss: 0.007932, acc.: 100.00%] [G loss: 5.444747]\n",
      "epoch:41 step:32028 [D loss: 0.005715, acc.: 100.00%] [G loss: 5.956905]\n",
      "epoch:41 step:32029 [D loss: 0.007051, acc.: 100.00%] [G loss: 5.332248]\n",
      "epoch:41 step:32030 [D loss: 0.023134, acc.: 100.00%] [G loss: 4.906829]\n",
      "epoch:41 step:32031 [D loss: 0.007174, acc.: 100.00%] [G loss: 4.931254]\n",
      "epoch:41 step:32032 [D loss: 0.014820, acc.: 100.00%] [G loss: 4.904840]\n",
      "epoch:41 step:32033 [D loss: 0.017244, acc.: 99.22%] [G loss: 4.739476]\n",
      "epoch:41 step:32034 [D loss: 0.007755, acc.: 100.00%] [G loss: 4.891082]\n",
      "epoch:41 step:32035 [D loss: 0.017275, acc.: 100.00%] [G loss: 4.859996]\n",
      "epoch:41 step:32036 [D loss: 0.020015, acc.: 100.00%] [G loss: 4.709373]\n",
      "epoch:41 step:32037 [D loss: 0.007995, acc.: 100.00%] [G loss: 5.062762]\n",
      "epoch:41 step:32038 [D loss: 0.015011, acc.: 100.00%] [G loss: 5.225589]\n",
      "epoch:41 step:32039 [D loss: 0.007834, acc.: 100.00%] [G loss: 4.777461]\n",
      "epoch:41 step:32040 [D loss: 0.007806, acc.: 100.00%] [G loss: 4.312253]\n",
      "epoch:41 step:32041 [D loss: 0.005369, acc.: 100.00%] [G loss: 5.006932]\n",
      "epoch:41 step:32042 [D loss: 0.006531, acc.: 100.00%] [G loss: 4.527456]\n",
      "epoch:41 step:32043 [D loss: 0.041638, acc.: 100.00%] [G loss: 5.135059]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:41 step:32044 [D loss: 0.048063, acc.: 100.00%] [G loss: 3.508657]\n",
      "epoch:41 step:32045 [D loss: 0.015902, acc.: 100.00%] [G loss: 3.899581]\n",
      "epoch:41 step:32046 [D loss: 0.018103, acc.: 100.00%] [G loss: 4.865751]\n",
      "epoch:41 step:32047 [D loss: 0.031546, acc.: 100.00%] [G loss: 5.559479]\n",
      "epoch:41 step:32048 [D loss: 0.029627, acc.: 100.00%] [G loss: 4.006009]\n",
      "epoch:41 step:32049 [D loss: 0.035234, acc.: 100.00%] [G loss: 5.291221]\n",
      "epoch:41 step:32050 [D loss: 0.009012, acc.: 100.00%] [G loss: 5.096019]\n",
      "epoch:41 step:32051 [D loss: 0.141084, acc.: 96.09%] [G loss: 2.764910]\n",
      "epoch:41 step:32052 [D loss: 0.053246, acc.: 100.00%] [G loss: 5.880917]\n",
      "epoch:41 step:32053 [D loss: 0.001463, acc.: 100.00%] [G loss: 7.070127]\n",
      "epoch:41 step:32054 [D loss: 0.361488, acc.: 87.50%] [G loss: 5.016150]\n",
      "epoch:41 step:32055 [D loss: 0.000746, acc.: 100.00%] [G loss: 7.173094]\n",
      "epoch:41 step:32056 [D loss: 2.357649, acc.: 41.41%] [G loss: 10.024265]\n",
      "epoch:41 step:32057 [D loss: 4.687903, acc.: 50.00%] [G loss: 6.883157]\n",
      "epoch:41 step:32058 [D loss: 3.381828, acc.: 50.00%] [G loss: 4.748065]\n",
      "epoch:41 step:32059 [D loss: 2.207536, acc.: 50.00%] [G loss: 3.214586]\n",
      "epoch:41 step:32060 [D loss: 1.480612, acc.: 50.00%] [G loss: 2.292164]\n",
      "epoch:41 step:32061 [D loss: 1.104246, acc.: 50.78%] [G loss: 1.627437]\n",
      "epoch:41 step:32062 [D loss: 0.973204, acc.: 49.22%] [G loss: 1.289013]\n",
      "epoch:41 step:32063 [D loss: 0.860504, acc.: 50.00%] [G loss: 1.171148]\n",
      "epoch:41 step:32064 [D loss: 0.749196, acc.: 47.66%] [G loss: 1.083470]\n",
      "epoch:41 step:32065 [D loss: 0.709907, acc.: 51.56%] [G loss: 1.091458]\n",
      "epoch:41 step:32066 [D loss: 0.667295, acc.: 55.47%] [G loss: 1.044322]\n",
      "epoch:41 step:32067 [D loss: 0.631798, acc.: 64.06%] [G loss: 1.177374]\n",
      "epoch:41 step:32068 [D loss: 0.666707, acc.: 60.16%] [G loss: 1.103238]\n",
      "epoch:41 step:32069 [D loss: 0.600241, acc.: 64.06%] [G loss: 1.194620]\n",
      "epoch:41 step:32070 [D loss: 0.621132, acc.: 61.72%] [G loss: 1.155726]\n",
      "epoch:41 step:32071 [D loss: 0.548781, acc.: 71.88%] [G loss: 1.142100]\n",
      "epoch:41 step:32072 [D loss: 0.532439, acc.: 75.78%] [G loss: 1.340154]\n",
      "epoch:41 step:32073 [D loss: 0.518775, acc.: 80.47%] [G loss: 1.262384]\n",
      "epoch:41 step:32074 [D loss: 0.509714, acc.: 79.69%] [G loss: 1.176975]\n",
      "epoch:41 step:32075 [D loss: 0.511486, acc.: 82.03%] [G loss: 1.297505]\n",
      "epoch:41 step:32076 [D loss: 0.431301, acc.: 85.16%] [G loss: 1.479890]\n",
      "epoch:41 step:32077 [D loss: 0.475862, acc.: 81.25%] [G loss: 1.505457]\n",
      "epoch:41 step:32078 [D loss: 0.462604, acc.: 82.81%] [G loss: 1.282523]\n",
      "epoch:41 step:32079 [D loss: 0.387806, acc.: 91.41%] [G loss: 1.616361]\n",
      "epoch:41 step:32080 [D loss: 0.387540, acc.: 89.84%] [G loss: 1.802718]\n",
      "epoch:41 step:32081 [D loss: 0.383091, acc.: 87.50%] [G loss: 1.828756]\n",
      "epoch:41 step:32082 [D loss: 0.329767, acc.: 91.41%] [G loss: 1.753980]\n",
      "epoch:41 step:32083 [D loss: 0.371331, acc.: 94.53%] [G loss: 1.618473]\n",
      "epoch:41 step:32084 [D loss: 0.335519, acc.: 92.19%] [G loss: 1.599709]\n",
      "epoch:41 step:32085 [D loss: 0.286007, acc.: 95.31%] [G loss: 1.832020]\n",
      "epoch:41 step:32086 [D loss: 0.322350, acc.: 93.75%] [G loss: 1.981043]\n",
      "epoch:41 step:32087 [D loss: 0.361546, acc.: 89.84%] [G loss: 1.672948]\n",
      "epoch:41 step:32088 [D loss: 0.363641, acc.: 90.62%] [G loss: 1.517993]\n",
      "epoch:41 step:32089 [D loss: 0.233152, acc.: 98.44%] [G loss: 1.719918]\n",
      "epoch:41 step:32090 [D loss: 0.244069, acc.: 93.75%] [G loss: 1.901075]\n",
      "epoch:41 step:32091 [D loss: 0.336763, acc.: 95.31%] [G loss: 1.936885]\n",
      "epoch:41 step:32092 [D loss: 0.272428, acc.: 94.53%] [G loss: 1.556970]\n",
      "epoch:41 step:32093 [D loss: 0.260055, acc.: 94.53%] [G loss: 1.691781]\n",
      "epoch:41 step:32094 [D loss: 0.250017, acc.: 96.09%] [G loss: 1.713507]\n",
      "epoch:41 step:32095 [D loss: 0.267637, acc.: 92.97%] [G loss: 1.765757]\n",
      "epoch:41 step:32096 [D loss: 0.356559, acc.: 92.19%] [G loss: 1.454982]\n",
      "epoch:41 step:32097 [D loss: 0.257784, acc.: 94.53%] [G loss: 1.917702]\n",
      "epoch:41 step:32098 [D loss: 0.213895, acc.: 98.44%] [G loss: 1.977677]\n",
      "epoch:41 step:32099 [D loss: 0.169910, acc.: 98.44%] [G loss: 1.838937]\n",
      "epoch:41 step:32100 [D loss: 0.220981, acc.: 98.44%] [G loss: 1.690668]\n",
      "epoch:41 step:32101 [D loss: 0.206991, acc.: 94.53%] [G loss: 1.543881]\n",
      "epoch:41 step:32102 [D loss: 0.332825, acc.: 82.03%] [G loss: 2.410544]\n",
      "epoch:41 step:32103 [D loss: 0.283435, acc.: 90.62%] [G loss: 2.289906]\n",
      "epoch:41 step:32104 [D loss: 0.431575, acc.: 80.47%] [G loss: 2.304134]\n",
      "epoch:41 step:32105 [D loss: 0.447721, acc.: 82.81%] [G loss: 1.661938]\n",
      "epoch:41 step:32106 [D loss: 0.175812, acc.: 99.22%] [G loss: 2.128695]\n",
      "epoch:41 step:32107 [D loss: 0.133426, acc.: 100.00%] [G loss: 2.743027]\n",
      "epoch:41 step:32108 [D loss: 0.224927, acc.: 93.75%] [G loss: 2.284773]\n",
      "epoch:41 step:32109 [D loss: 0.221853, acc.: 96.09%] [G loss: 1.915229]\n",
      "epoch:41 step:32110 [D loss: 0.148370, acc.: 98.44%] [G loss: 3.069417]\n",
      "epoch:41 step:32111 [D loss: 0.219227, acc.: 94.53%] [G loss: 2.482188]\n",
      "epoch:41 step:32112 [D loss: 0.210809, acc.: 94.53%] [G loss: 2.167950]\n",
      "epoch:41 step:32113 [D loss: 0.078664, acc.: 100.00%] [G loss: 2.441192]\n",
      "epoch:41 step:32114 [D loss: 0.211394, acc.: 94.53%] [G loss: 3.121155]\n",
      "epoch:41 step:32115 [D loss: 0.145136, acc.: 96.88%] [G loss: 2.954679]\n",
      "epoch:41 step:32116 [D loss: 0.040159, acc.: 100.00%] [G loss: 3.109087]\n",
      "epoch:41 step:32117 [D loss: 0.232808, acc.: 95.31%] [G loss: 3.212768]\n",
      "epoch:41 step:32118 [D loss: 0.159684, acc.: 98.44%] [G loss: 1.769759]\n",
      "epoch:41 step:32119 [D loss: 0.063922, acc.: 100.00%] [G loss: 1.777576]\n",
      "epoch:41 step:32120 [D loss: 0.157593, acc.: 95.31%] [G loss: 0.825823]\n",
      "epoch:41 step:32121 [D loss: 0.087718, acc.: 99.22%] [G loss: 1.949185]\n",
      "epoch:41 step:32122 [D loss: 0.043825, acc.: 100.00%] [G loss: 1.111549]\n",
      "epoch:41 step:32123 [D loss: 0.150271, acc.: 97.66%] [G loss: 1.457614]\n",
      "epoch:41 step:32124 [D loss: 0.372858, acc.: 83.59%] [G loss: 0.462632]\n",
      "epoch:41 step:32125 [D loss: 0.110474, acc.: 98.44%] [G loss: 0.978213]\n",
      "epoch:41 step:32126 [D loss: 0.032588, acc.: 100.00%] [G loss: 1.334879]\n",
      "epoch:41 step:32127 [D loss: 0.013533, acc.: 100.00%] [G loss: 0.851160]\n",
      "epoch:41 step:32128 [D loss: 0.083776, acc.: 100.00%] [G loss: 1.048829]\n",
      "epoch:41 step:32129 [D loss: 0.402423, acc.: 83.59%] [G loss: 1.644198]\n",
      "epoch:41 step:32130 [D loss: 0.079139, acc.: 100.00%] [G loss: 2.486605]\n",
      "epoch:41 step:32131 [D loss: 0.125152, acc.: 97.66%] [G loss: 2.432175]\n",
      "epoch:41 step:32132 [D loss: 0.355846, acc.: 82.03%] [G loss: 2.535115]\n",
      "epoch:41 step:32133 [D loss: 0.041689, acc.: 100.00%] [G loss: 3.945442]\n",
      "epoch:41 step:32134 [D loss: 0.109051, acc.: 96.09%] [G loss: 2.506902]\n",
      "epoch:41 step:32135 [D loss: 0.182270, acc.: 92.97%] [G loss: 1.622595]\n",
      "epoch:41 step:32136 [D loss: 0.132090, acc.: 97.66%] [G loss: 1.877554]\n",
      "epoch:41 step:32137 [D loss: 0.125640, acc.: 97.66%] [G loss: 3.701606]\n",
      "epoch:41 step:32138 [D loss: 0.059400, acc.: 100.00%] [G loss: 3.767897]\n",
      "epoch:41 step:32139 [D loss: 0.067594, acc.: 99.22%] [G loss: 2.895670]\n",
      "epoch:41 step:32140 [D loss: 0.203143, acc.: 92.97%] [G loss: 3.012652]\n",
      "epoch:41 step:32141 [D loss: 0.141105, acc.: 98.44%] [G loss: 3.797307]\n",
      "epoch:41 step:32142 [D loss: 0.084687, acc.: 97.66%] [G loss: 4.393656]\n",
      "epoch:41 step:32143 [D loss: 0.074714, acc.: 98.44%] [G loss: 3.873947]\n",
      "epoch:41 step:32144 [D loss: 0.079280, acc.: 97.66%] [G loss: 1.681404]\n",
      "epoch:41 step:32145 [D loss: 0.066415, acc.: 100.00%] [G loss: 2.413429]\n",
      "epoch:41 step:32146 [D loss: 0.086113, acc.: 97.66%] [G loss: 4.673896]\n",
      "epoch:41 step:32147 [D loss: 0.151452, acc.: 96.09%] [G loss: 2.945866]\n",
      "epoch:41 step:32148 [D loss: 0.153003, acc.: 96.88%] [G loss: 3.426765]\n",
      "epoch:41 step:32149 [D loss: 0.012033, acc.: 100.00%] [G loss: 4.839307]\n",
      "epoch:41 step:32150 [D loss: 0.108569, acc.: 98.44%] [G loss: 2.023572]\n",
      "epoch:41 step:32151 [D loss: 0.020985, acc.: 100.00%] [G loss: 2.520438]\n",
      "epoch:41 step:32152 [D loss: 0.196072, acc.: 92.19%] [G loss: 5.773140]\n",
      "epoch:41 step:32153 [D loss: 0.329876, acc.: 87.50%] [G loss: 3.112431]\n",
      "epoch:41 step:32154 [D loss: 0.082729, acc.: 98.44%] [G loss: 3.174857]\n",
      "epoch:41 step:32155 [D loss: 0.037262, acc.: 99.22%] [G loss: 3.680911]\n",
      "epoch:41 step:32156 [D loss: 0.014803, acc.: 100.00%] [G loss: 2.528801]\n",
      "epoch:41 step:32157 [D loss: 0.069980, acc.: 97.66%] [G loss: 2.422066]\n",
      "epoch:41 step:32158 [D loss: 0.144133, acc.: 96.88%] [G loss: 1.295245]\n",
      "epoch:41 step:32159 [D loss: 0.077440, acc.: 97.66%] [G loss: 3.119061]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:41 step:32160 [D loss: 0.035207, acc.: 98.44%] [G loss: 4.469400]\n",
      "epoch:41 step:32161 [D loss: 0.109186, acc.: 97.66%] [G loss: 2.867895]\n",
      "epoch:41 step:32162 [D loss: 0.297479, acc.: 85.94%] [G loss: 6.950657]\n",
      "epoch:41 step:32163 [D loss: 0.695098, acc.: 70.31%] [G loss: 3.182605]\n",
      "epoch:41 step:32164 [D loss: 0.298168, acc.: 83.59%] [G loss: 7.154222]\n",
      "epoch:41 step:32165 [D loss: 0.124610, acc.: 93.75%] [G loss: 7.050845]\n",
      "epoch:41 step:32166 [D loss: 0.740559, acc.: 64.06%] [G loss: 2.528936]\n",
      "epoch:41 step:32167 [D loss: 0.904557, acc.: 66.41%] [G loss: 7.288476]\n",
      "epoch:41 step:32168 [D loss: 0.149582, acc.: 92.97%] [G loss: 7.901684]\n",
      "epoch:41 step:32169 [D loss: 0.529215, acc.: 72.66%] [G loss: 5.060883]\n",
      "epoch:41 step:32170 [D loss: 0.085861, acc.: 96.09%] [G loss: 4.485572]\n",
      "epoch:41 step:32171 [D loss: 0.033691, acc.: 100.00%] [G loss: 5.395787]\n",
      "epoch:41 step:32172 [D loss: 0.014431, acc.: 100.00%] [G loss: 4.508908]\n",
      "epoch:41 step:32173 [D loss: 0.023051, acc.: 100.00%] [G loss: 4.565229]\n",
      "epoch:41 step:32174 [D loss: 0.007099, acc.: 100.00%] [G loss: 4.344824]\n",
      "epoch:41 step:32175 [D loss: 0.014822, acc.: 100.00%] [G loss: 4.100715]\n",
      "epoch:41 step:32176 [D loss: 0.043501, acc.: 99.22%] [G loss: 4.443460]\n",
      "epoch:41 step:32177 [D loss: 0.082550, acc.: 97.66%] [G loss: 4.100837]\n",
      "epoch:41 step:32178 [D loss: 0.025639, acc.: 100.00%] [G loss: 4.193525]\n",
      "epoch:41 step:32179 [D loss: 0.024128, acc.: 99.22%] [G loss: 3.907493]\n",
      "epoch:41 step:32180 [D loss: 0.020436, acc.: 100.00%] [G loss: 4.322319]\n",
      "epoch:41 step:32181 [D loss: 0.065962, acc.: 97.66%] [G loss: 3.130425]\n",
      "epoch:41 step:32182 [D loss: 0.134168, acc.: 96.88%] [G loss: 2.881390]\n",
      "epoch:41 step:32183 [D loss: 0.021660, acc.: 100.00%] [G loss: 4.602275]\n",
      "epoch:41 step:32184 [D loss: 0.013491, acc.: 100.00%] [G loss: 4.808741]\n",
      "epoch:41 step:32185 [D loss: 0.038342, acc.: 100.00%] [G loss: 4.709656]\n",
      "epoch:41 step:32186 [D loss: 0.653973, acc.: 71.09%] [G loss: 6.605279]\n",
      "epoch:41 step:32187 [D loss: 0.123035, acc.: 93.75%] [G loss: 6.968022]\n",
      "epoch:41 step:32188 [D loss: 0.267156, acc.: 89.06%] [G loss: 4.469023]\n",
      "epoch:41 step:32189 [D loss: 0.014043, acc.: 100.00%] [G loss: 4.280791]\n",
      "epoch:41 step:32190 [D loss: 0.033086, acc.: 100.00%] [G loss: 3.760967]\n",
      "epoch:41 step:32191 [D loss: 0.031789, acc.: 99.22%] [G loss: 4.261966]\n",
      "epoch:41 step:32192 [D loss: 0.020554, acc.: 100.00%] [G loss: 4.369597]\n",
      "epoch:41 step:32193 [D loss: 0.018105, acc.: 100.00%] [G loss: 4.748014]\n",
      "epoch:41 step:32194 [D loss: 0.008215, acc.: 100.00%] [G loss: 4.201180]\n",
      "epoch:41 step:32195 [D loss: 0.019800, acc.: 100.00%] [G loss: 4.403425]\n",
      "epoch:41 step:32196 [D loss: 0.022933, acc.: 100.00%] [G loss: 4.349176]\n",
      "epoch:41 step:32197 [D loss: 0.015781, acc.: 100.00%] [G loss: 4.371640]\n",
      "epoch:41 step:32198 [D loss: 0.008806, acc.: 100.00%] [G loss: 3.957278]\n",
      "epoch:41 step:32199 [D loss: 0.018801, acc.: 100.00%] [G loss: 4.091928]\n",
      "epoch:41 step:32200 [D loss: 0.056348, acc.: 100.00%] [G loss: 4.216385]\n",
      "##############\n",
      "[0.75672471 1.10187482 0.86264243 0.96692856 2.10544013 1.10773012\n",
      " 2.10285884 2.11642447 1.02411497 0.84152762]\n",
      "##########\n",
      "epoch:41 step:32201 [D loss: 0.014032, acc.: 100.00%] [G loss: 4.484821]\n",
      "epoch:41 step:32202 [D loss: 0.055048, acc.: 97.66%] [G loss: 3.974033]\n",
      "epoch:41 step:32203 [D loss: 0.063311, acc.: 99.22%] [G loss: 3.678803]\n",
      "epoch:41 step:32204 [D loss: 0.049204, acc.: 99.22%] [G loss: 4.689787]\n",
      "epoch:41 step:32205 [D loss: 0.064567, acc.: 98.44%] [G loss: 4.909118]\n",
      "epoch:41 step:32206 [D loss: 0.028733, acc.: 100.00%] [G loss: 4.256969]\n",
      "epoch:41 step:32207 [D loss: 0.027471, acc.: 100.00%] [G loss: 5.287569]\n",
      "epoch:41 step:32208 [D loss: 0.562491, acc.: 71.88%] [G loss: 7.920797]\n",
      "epoch:41 step:32209 [D loss: 0.108207, acc.: 95.31%] [G loss: 8.337261]\n",
      "epoch:41 step:32210 [D loss: 0.090316, acc.: 96.88%] [G loss: 7.587731]\n",
      "epoch:41 step:32211 [D loss: 0.006527, acc.: 100.00%] [G loss: 6.781543]\n",
      "epoch:41 step:32212 [D loss: 0.005491, acc.: 100.00%] [G loss: 6.113679]\n",
      "epoch:41 step:32213 [D loss: 0.013782, acc.: 100.00%] [G loss: 5.444016]\n",
      "epoch:41 step:32214 [D loss: 0.014889, acc.: 100.00%] [G loss: 5.095960]\n",
      "epoch:41 step:32215 [D loss: 0.015787, acc.: 100.00%] [G loss: 5.705441]\n",
      "epoch:41 step:32216 [D loss: 0.007134, acc.: 100.00%] [G loss: 3.990543]\n",
      "epoch:41 step:32217 [D loss: 0.013435, acc.: 100.00%] [G loss: 4.663765]\n",
      "epoch:41 step:32218 [D loss: 0.015480, acc.: 100.00%] [G loss: 3.151263]\n",
      "epoch:41 step:32219 [D loss: 0.002889, acc.: 100.00%] [G loss: 3.240595]\n",
      "epoch:41 step:32220 [D loss: 0.097319, acc.: 97.66%] [G loss: 2.349478]\n",
      "epoch:41 step:32221 [D loss: 0.097319, acc.: 96.88%] [G loss: 3.208733]\n",
      "epoch:41 step:32222 [D loss: 0.017081, acc.: 100.00%] [G loss: 4.956014]\n",
      "epoch:41 step:32223 [D loss: 0.102071, acc.: 96.09%] [G loss: 6.679608]\n",
      "epoch:41 step:32224 [D loss: 0.030205, acc.: 98.44%] [G loss: 6.737696]\n",
      "epoch:41 step:32225 [D loss: 0.174956, acc.: 92.97%] [G loss: 2.190127]\n",
      "epoch:41 step:32226 [D loss: 0.229049, acc.: 89.06%] [G loss: 8.362735]\n",
      "epoch:41 step:32227 [D loss: 0.398148, acc.: 79.69%] [G loss: 4.336203]\n",
      "epoch:41 step:32228 [D loss: 0.033367, acc.: 100.00%] [G loss: 2.728071]\n",
      "epoch:41 step:32229 [D loss: 0.053165, acc.: 100.00%] [G loss: 3.870541]\n",
      "epoch:41 step:32230 [D loss: 0.075172, acc.: 97.66%] [G loss: 5.564597]\n",
      "epoch:41 step:32231 [D loss: 0.169018, acc.: 94.53%] [G loss: 4.602918]\n",
      "epoch:41 step:32232 [D loss: 0.016572, acc.: 100.00%] [G loss: 4.855868]\n",
      "epoch:41 step:32233 [D loss: 0.014227, acc.: 100.00%] [G loss: 5.019300]\n",
      "epoch:41 step:32234 [D loss: 0.013614, acc.: 100.00%] [G loss: 3.604197]\n",
      "epoch:41 step:32235 [D loss: 0.034969, acc.: 99.22%] [G loss: 4.833263]\n",
      "epoch:41 step:32236 [D loss: 0.015057, acc.: 100.00%] [G loss: 5.420128]\n",
      "epoch:41 step:32237 [D loss: 0.018272, acc.: 100.00%] [G loss: 6.008096]\n",
      "epoch:41 step:32238 [D loss: 0.012918, acc.: 100.00%] [G loss: 5.856084]\n",
      "epoch:41 step:32239 [D loss: 0.565269, acc.: 78.12%] [G loss: 7.732448]\n",
      "epoch:41 step:32240 [D loss: 0.230734, acc.: 88.28%] [G loss: 6.836763]\n",
      "epoch:41 step:32241 [D loss: 0.090286, acc.: 96.09%] [G loss: 5.731277]\n",
      "epoch:41 step:32242 [D loss: 0.007920, acc.: 100.00%] [G loss: 5.562923]\n",
      "epoch:41 step:32243 [D loss: 0.004258, acc.: 100.00%] [G loss: 5.160233]\n",
      "epoch:41 step:32244 [D loss: 0.010541, acc.: 100.00%] [G loss: 4.647524]\n",
      "epoch:41 step:32245 [D loss: 0.059070, acc.: 97.66%] [G loss: 6.649886]\n",
      "epoch:41 step:32246 [D loss: 0.002307, acc.: 100.00%] [G loss: 5.923743]\n",
      "epoch:41 step:32247 [D loss: 0.008064, acc.: 100.00%] [G loss: 6.448699]\n",
      "epoch:41 step:32248 [D loss: 0.031406, acc.: 100.00%] [G loss: 4.822977]\n",
      "epoch:41 step:32249 [D loss: 0.021021, acc.: 100.00%] [G loss: 4.853508]\n",
      "epoch:41 step:32250 [D loss: 0.011678, acc.: 100.00%] [G loss: 4.171505]\n",
      "epoch:41 step:32251 [D loss: 0.019743, acc.: 100.00%] [G loss: 5.248524]\n",
      "epoch:41 step:32252 [D loss: 0.011614, acc.: 100.00%] [G loss: 5.173542]\n",
      "epoch:41 step:32253 [D loss: 0.146011, acc.: 93.75%] [G loss: 3.728904]\n",
      "epoch:41 step:32254 [D loss: 0.033040, acc.: 100.00%] [G loss: 5.820662]\n",
      "epoch:41 step:32255 [D loss: 0.059866, acc.: 98.44%] [G loss: 4.466188]\n",
      "epoch:41 step:32256 [D loss: 0.023586, acc.: 100.00%] [G loss: 3.560231]\n",
      "epoch:41 step:32257 [D loss: 0.045074, acc.: 99.22%] [G loss: 6.290836]\n",
      "epoch:41 step:32258 [D loss: 0.005367, acc.: 100.00%] [G loss: 6.334761]\n",
      "epoch:41 step:32259 [D loss: 0.002464, acc.: 100.00%] [G loss: 6.249349]\n",
      "epoch:41 step:32260 [D loss: 0.002188, acc.: 100.00%] [G loss: 6.378656]\n",
      "epoch:41 step:32261 [D loss: 1.216121, acc.: 50.00%] [G loss: 8.857681]\n",
      "epoch:41 step:32262 [D loss: 0.583977, acc.: 74.22%] [G loss: 7.017185]\n",
      "epoch:41 step:32263 [D loss: 0.006412, acc.: 100.00%] [G loss: 6.871746]\n",
      "epoch:41 step:32264 [D loss: 0.005649, acc.: 100.00%] [G loss: 6.201896]\n",
      "epoch:41 step:32265 [D loss: 0.006224, acc.: 100.00%] [G loss: 6.079703]\n",
      "epoch:41 step:32266 [D loss: 0.010359, acc.: 100.00%] [G loss: 4.148104]\n",
      "epoch:41 step:32267 [D loss: 0.009316, acc.: 100.00%] [G loss: 4.586516]\n",
      "epoch:41 step:32268 [D loss: 0.033312, acc.: 100.00%] [G loss: 5.092965]\n",
      "epoch:41 step:32269 [D loss: 0.008339, acc.: 100.00%] [G loss: 5.427274]\n",
      "epoch:41 step:32270 [D loss: 0.006697, acc.: 100.00%] [G loss: 5.080295]\n",
      "epoch:41 step:32271 [D loss: 0.013405, acc.: 100.00%] [G loss: 4.218825]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:41 step:32272 [D loss: 0.059181, acc.: 100.00%] [G loss: 5.586308]\n",
      "epoch:41 step:32273 [D loss: 0.237412, acc.: 92.19%] [G loss: 4.640227]\n",
      "epoch:41 step:32274 [D loss: 0.006911, acc.: 100.00%] [G loss: 5.496190]\n",
      "epoch:41 step:32275 [D loss: 0.003145, acc.: 100.00%] [G loss: 5.656801]\n",
      "epoch:41 step:32276 [D loss: 0.010208, acc.: 100.00%] [G loss: 4.226021]\n",
      "epoch:41 step:32277 [D loss: 0.018070, acc.: 100.00%] [G loss: 4.427706]\n",
      "epoch:41 step:32278 [D loss: 0.028030, acc.: 100.00%] [G loss: 5.208907]\n",
      "epoch:41 step:32279 [D loss: 0.018136, acc.: 99.22%] [G loss: 5.328421]\n",
      "epoch:41 step:32280 [D loss: 0.002669, acc.: 100.00%] [G loss: 4.827668]\n",
      "epoch:41 step:32281 [D loss: 0.035779, acc.: 100.00%] [G loss: 4.192369]\n",
      "epoch:41 step:32282 [D loss: 0.115517, acc.: 97.66%] [G loss: 6.171912]\n",
      "epoch:41 step:32283 [D loss: 0.005488, acc.: 100.00%] [G loss: 8.096457]\n",
      "epoch:41 step:32284 [D loss: 0.019530, acc.: 100.00%] [G loss: 6.590555]\n",
      "epoch:41 step:32285 [D loss: 0.450585, acc.: 81.25%] [G loss: 6.435328]\n",
      "epoch:41 step:32286 [D loss: 0.001015, acc.: 100.00%] [G loss: 8.258873]\n",
      "epoch:41 step:32287 [D loss: 0.008663, acc.: 100.00%] [G loss: 7.678160]\n",
      "epoch:41 step:32288 [D loss: 0.009930, acc.: 100.00%] [G loss: 6.732730]\n",
      "epoch:41 step:32289 [D loss: 0.002005, acc.: 100.00%] [G loss: 6.775718]\n",
      "epoch:41 step:32290 [D loss: 0.001418, acc.: 100.00%] [G loss: 6.351370]\n",
      "epoch:41 step:32291 [D loss: 0.007325, acc.: 100.00%] [G loss: 5.477791]\n",
      "epoch:41 step:32292 [D loss: 0.004605, acc.: 100.00%] [G loss: 6.036919]\n",
      "epoch:41 step:32293 [D loss: 0.004917, acc.: 100.00%] [G loss: 5.783242]\n",
      "epoch:41 step:32294 [D loss: 0.028135, acc.: 100.00%] [G loss: 4.518217]\n",
      "epoch:41 step:32295 [D loss: 0.013908, acc.: 100.00%] [G loss: 4.750824]\n",
      "epoch:41 step:32296 [D loss: 0.002623, acc.: 100.00%] [G loss: 4.491081]\n",
      "epoch:41 step:32297 [D loss: 0.014412, acc.: 100.00%] [G loss: 4.734472]\n",
      "epoch:41 step:32298 [D loss: 0.053071, acc.: 99.22%] [G loss: 6.156466]\n",
      "epoch:41 step:32299 [D loss: 0.114756, acc.: 96.88%] [G loss: 4.930972]\n",
      "epoch:41 step:32300 [D loss: 0.005630, acc.: 100.00%] [G loss: 6.588356]\n",
      "epoch:41 step:32301 [D loss: 0.005469, acc.: 100.00%] [G loss: 5.691786]\n",
      "epoch:41 step:32302 [D loss: 0.004649, acc.: 100.00%] [G loss: 6.651354]\n",
      "epoch:41 step:32303 [D loss: 0.005946, acc.: 100.00%] [G loss: 5.684712]\n",
      "epoch:41 step:32304 [D loss: 0.019272, acc.: 100.00%] [G loss: 5.712578]\n",
      "epoch:41 step:32305 [D loss: 0.009780, acc.: 100.00%] [G loss: 5.701431]\n",
      "epoch:41 step:32306 [D loss: 0.024219, acc.: 100.00%] [G loss: 6.061967]\n",
      "epoch:41 step:32307 [D loss: 0.277643, acc.: 89.84%] [G loss: 2.018226]\n",
      "epoch:41 step:32308 [D loss: 0.186740, acc.: 91.41%] [G loss: 7.853883]\n",
      "epoch:41 step:32309 [D loss: 0.063484, acc.: 97.66%] [G loss: 8.013035]\n",
      "epoch:41 step:32310 [D loss: 2.113850, acc.: 45.31%] [G loss: 12.416042]\n",
      "epoch:41 step:32311 [D loss: 3.174382, acc.: 50.00%] [G loss: 7.063357]\n",
      "epoch:41 step:32312 [D loss: 0.133876, acc.: 94.53%] [G loss: 4.152473]\n",
      "epoch:41 step:32313 [D loss: 0.016423, acc.: 100.00%] [G loss: 2.623716]\n",
      "epoch:41 step:32314 [D loss: 0.103651, acc.: 96.09%] [G loss: 4.137926]\n",
      "epoch:41 step:32315 [D loss: 0.051626, acc.: 98.44%] [G loss: 3.829117]\n",
      "epoch:41 step:32316 [D loss: 0.385555, acc.: 83.59%] [G loss: 4.965069]\n",
      "epoch:41 step:32317 [D loss: 0.093641, acc.: 96.88%] [G loss: 5.270827]\n",
      "epoch:41 step:32318 [D loss: 0.099261, acc.: 96.88%] [G loss: 3.548680]\n",
      "epoch:41 step:32319 [D loss: 0.051889, acc.: 98.44%] [G loss: 3.045589]\n",
      "epoch:41 step:32320 [D loss: 0.129131, acc.: 95.31%] [G loss: 5.010284]\n",
      "epoch:41 step:32321 [D loss: 0.050046, acc.: 98.44%] [G loss: 4.516569]\n",
      "epoch:41 step:32322 [D loss: 0.012594, acc.: 100.00%] [G loss: 4.419852]\n",
      "epoch:41 step:32323 [D loss: 0.008054, acc.: 100.00%] [G loss: 4.679139]\n",
      "epoch:41 step:32324 [D loss: 0.301057, acc.: 89.84%] [G loss: 3.665740]\n",
      "epoch:41 step:32325 [D loss: 0.010171, acc.: 100.00%] [G loss: 4.422052]\n",
      "epoch:41 step:32326 [D loss: 0.052441, acc.: 98.44%] [G loss: 3.161832]\n",
      "epoch:41 step:32327 [D loss: 0.030202, acc.: 100.00%] [G loss: 4.197465]\n",
      "epoch:41 step:32328 [D loss: 0.030067, acc.: 100.00%] [G loss: 4.102894]\n",
      "epoch:41 step:32329 [D loss: 0.033239, acc.: 100.00%] [G loss: 3.845464]\n",
      "epoch:41 step:32330 [D loss: 0.101597, acc.: 97.66%] [G loss: 2.318518]\n",
      "epoch:41 step:32331 [D loss: 0.026228, acc.: 100.00%] [G loss: 3.260984]\n",
      "epoch:41 step:32332 [D loss: 0.011370, acc.: 100.00%] [G loss: 2.414798]\n",
      "epoch:41 step:32333 [D loss: 1.537625, acc.: 49.22%] [G loss: 9.769672]\n",
      "epoch:41 step:32334 [D loss: 2.579499, acc.: 50.78%] [G loss: 6.861492]\n",
      "epoch:41 step:32335 [D loss: 1.325009, acc.: 53.12%] [G loss: 3.213286]\n",
      "epoch:41 step:32336 [D loss: 0.175594, acc.: 94.53%] [G loss: 2.477287]\n",
      "epoch:41 step:32337 [D loss: 0.047693, acc.: 100.00%] [G loss: 2.389126]\n",
      "epoch:41 step:32338 [D loss: 0.093402, acc.: 99.22%] [G loss: 2.737058]\n",
      "epoch:41 step:32339 [D loss: 0.054298, acc.: 100.00%] [G loss: 3.113139]\n",
      "epoch:41 step:32340 [D loss: 0.063479, acc.: 100.00%] [G loss: 3.026760]\n",
      "epoch:41 step:32341 [D loss: 0.210653, acc.: 94.53%] [G loss: 1.443678]\n",
      "epoch:41 step:32342 [D loss: 0.049358, acc.: 100.00%] [G loss: 1.981085]\n",
      "epoch:41 step:32343 [D loss: 0.037358, acc.: 100.00%] [G loss: 2.377625]\n",
      "epoch:41 step:32344 [D loss: 0.042143, acc.: 100.00%] [G loss: 2.267547]\n",
      "epoch:41 step:32345 [D loss: 0.601731, acc.: 67.97%] [G loss: 3.649613]\n",
      "epoch:41 step:32346 [D loss: 0.105557, acc.: 96.09%] [G loss: 3.549521]\n",
      "epoch:41 step:32347 [D loss: 0.076288, acc.: 99.22%] [G loss: 2.812556]\n",
      "epoch:41 step:32348 [D loss: 0.026749, acc.: 100.00%] [G loss: 2.748055]\n",
      "epoch:41 step:32349 [D loss: 0.030158, acc.: 100.00%] [G loss: 2.382759]\n",
      "epoch:41 step:32350 [D loss: 0.031411, acc.: 100.00%] [G loss: 3.266513]\n",
      "epoch:41 step:32351 [D loss: 0.043678, acc.: 100.00%] [G loss: 3.210894]\n",
      "epoch:41 step:32352 [D loss: 0.090269, acc.: 99.22%] [G loss: 2.777853]\n",
      "epoch:41 step:32353 [D loss: 0.025707, acc.: 100.00%] [G loss: 2.825049]\n",
      "epoch:41 step:32354 [D loss: 0.051771, acc.: 99.22%] [G loss: 3.088329]\n",
      "epoch:41 step:32355 [D loss: 0.047766, acc.: 99.22%] [G loss: 3.075385]\n",
      "epoch:41 step:32356 [D loss: 0.038499, acc.: 100.00%] [G loss: 3.200197]\n",
      "epoch:41 step:32357 [D loss: 0.117900, acc.: 96.09%] [G loss: 2.741487]\n",
      "epoch:41 step:32358 [D loss: 0.022217, acc.: 100.00%] [G loss: 2.721025]\n",
      "epoch:41 step:32359 [D loss: 0.055789, acc.: 98.44%] [G loss: 3.230949]\n",
      "epoch:41 step:32360 [D loss: 0.098001, acc.: 99.22%] [G loss: 4.502414]\n",
      "epoch:41 step:32361 [D loss: 0.199523, acc.: 95.31%] [G loss: 2.767243]\n",
      "epoch:41 step:32362 [D loss: 0.080369, acc.: 97.66%] [G loss: 4.003748]\n",
      "epoch:41 step:32363 [D loss: 0.009223, acc.: 100.00%] [G loss: 4.340141]\n",
      "epoch:41 step:32364 [D loss: 0.038661, acc.: 100.00%] [G loss: 4.280647]\n",
      "epoch:41 step:32365 [D loss: 0.008624, acc.: 100.00%] [G loss: 4.433470]\n",
      "epoch:41 step:32366 [D loss: 0.018770, acc.: 100.00%] [G loss: 4.395005]\n",
      "epoch:41 step:32367 [D loss: 0.012033, acc.: 100.00%] [G loss: 4.382771]\n",
      "epoch:41 step:32368 [D loss: 0.011008, acc.: 100.00%] [G loss: 4.420664]\n",
      "epoch:41 step:32369 [D loss: 0.009279, acc.: 100.00%] [G loss: 4.966868]\n",
      "epoch:41 step:32370 [D loss: 0.008633, acc.: 100.00%] [G loss: 4.675689]\n",
      "epoch:41 step:32371 [D loss: 0.103303, acc.: 97.66%] [G loss: 2.028302]\n",
      "epoch:41 step:32372 [D loss: 0.110907, acc.: 96.88%] [G loss: 5.839366]\n",
      "epoch:41 step:32373 [D loss: 0.012956, acc.: 99.22%] [G loss: 5.911541]\n",
      "epoch:41 step:32374 [D loss: 0.166125, acc.: 93.75%] [G loss: 4.797523]\n",
      "epoch:41 step:32375 [D loss: 0.010057, acc.: 100.00%] [G loss: 3.770082]\n",
      "epoch:41 step:32376 [D loss: 0.008596, acc.: 100.00%] [G loss: 3.514358]\n",
      "epoch:41 step:32377 [D loss: 0.009125, acc.: 100.00%] [G loss: 3.165893]\n",
      "epoch:41 step:32378 [D loss: 0.017828, acc.: 100.00%] [G loss: 4.095685]\n",
      "epoch:41 step:32379 [D loss: 0.011983, acc.: 100.00%] [G loss: 3.533663]\n",
      "epoch:41 step:32380 [D loss: 0.012002, acc.: 100.00%] [G loss: 2.854321]\n",
      "epoch:41 step:32381 [D loss: 0.036402, acc.: 100.00%] [G loss: 3.716622]\n",
      "epoch:41 step:32382 [D loss: 0.010577, acc.: 100.00%] [G loss: 3.771625]\n",
      "epoch:41 step:32383 [D loss: 0.014826, acc.: 100.00%] [G loss: 3.306917]\n",
      "epoch:41 step:32384 [D loss: 0.007028, acc.: 100.00%] [G loss: 2.920034]\n",
      "epoch:41 step:32385 [D loss: 0.045402, acc.: 99.22%] [G loss: 2.364677]\n",
      "epoch:41 step:32386 [D loss: 0.036894, acc.: 100.00%] [G loss: 3.158875]\n",
      "epoch:41 step:32387 [D loss: 0.011541, acc.: 100.00%] [G loss: 3.959289]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:41 step:32388 [D loss: 0.022332, acc.: 100.00%] [G loss: 3.562999]\n",
      "epoch:41 step:32389 [D loss: 0.008275, acc.: 100.00%] [G loss: 4.354715]\n",
      "epoch:41 step:32390 [D loss: 0.040391, acc.: 99.22%] [G loss: 2.866175]\n",
      "epoch:41 step:32391 [D loss: 0.059399, acc.: 100.00%] [G loss: 4.131342]\n",
      "epoch:41 step:32392 [D loss: 0.002576, acc.: 100.00%] [G loss: 4.799507]\n",
      "epoch:41 step:32393 [D loss: 0.003480, acc.: 100.00%] [G loss: 5.005527]\n",
      "epoch:41 step:32394 [D loss: 0.023286, acc.: 99.22%] [G loss: 4.279515]\n",
      "epoch:41 step:32395 [D loss: 0.365140, acc.: 82.81%] [G loss: 6.382529]\n",
      "epoch:41 step:32396 [D loss: 0.287579, acc.: 85.94%] [G loss: 4.319015]\n",
      "epoch:41 step:32397 [D loss: 0.045047, acc.: 100.00%] [G loss: 4.425489]\n",
      "epoch:41 step:32398 [D loss: 0.021149, acc.: 100.00%] [G loss: 4.620515]\n",
      "epoch:41 step:32399 [D loss: 0.005787, acc.: 100.00%] [G loss: 4.910888]\n",
      "epoch:41 step:32400 [D loss: 0.004737, acc.: 100.00%] [G loss: 4.798071]\n",
      "##############\n",
      "[1.01263185 0.91568935 1.10230624 0.93407519 1.0680755  2.10374828\n",
      " 1.10945753 0.95518384 2.11475544 2.12650319]\n",
      "##########\n",
      "epoch:41 step:32401 [D loss: 0.008063, acc.: 100.00%] [G loss: 4.595054]\n",
      "epoch:41 step:32402 [D loss: 0.007877, acc.: 100.00%] [G loss: 4.975117]\n",
      "epoch:41 step:32403 [D loss: 0.009726, acc.: 100.00%] [G loss: 5.468819]\n",
      "epoch:41 step:32404 [D loss: 0.007170, acc.: 100.00%] [G loss: 5.092691]\n",
      "epoch:41 step:32405 [D loss: 0.008242, acc.: 100.00%] [G loss: 5.224707]\n",
      "epoch:41 step:32406 [D loss: 0.530574, acc.: 75.00%] [G loss: 7.133781]\n",
      "epoch:41 step:32407 [D loss: 0.210312, acc.: 87.50%] [G loss: 6.278287]\n",
      "epoch:41 step:32408 [D loss: 0.004506, acc.: 100.00%] [G loss: 5.749874]\n",
      "epoch:41 step:32409 [D loss: 0.001793, acc.: 100.00%] [G loss: 5.488407]\n",
      "epoch:41 step:32410 [D loss: 0.003151, acc.: 100.00%] [G loss: 5.521460]\n",
      "epoch:41 step:32411 [D loss: 0.010062, acc.: 100.00%] [G loss: 5.220559]\n",
      "epoch:41 step:32412 [D loss: 0.001984, acc.: 100.00%] [G loss: 5.036420]\n",
      "epoch:41 step:32413 [D loss: 0.003386, acc.: 100.00%] [G loss: 4.815695]\n",
      "epoch:41 step:32414 [D loss: 0.038265, acc.: 98.44%] [G loss: 3.730811]\n",
      "epoch:41 step:32415 [D loss: 0.012657, acc.: 100.00%] [G loss: 3.949830]\n",
      "epoch:41 step:32416 [D loss: 0.052521, acc.: 99.22%] [G loss: 4.929451]\n",
      "epoch:41 step:32417 [D loss: 0.008753, acc.: 100.00%] [G loss: 5.510125]\n",
      "epoch:41 step:32418 [D loss: 0.002841, acc.: 100.00%] [G loss: 5.434686]\n",
      "epoch:41 step:32419 [D loss: 0.002277, acc.: 100.00%] [G loss: 5.100657]\n",
      "epoch:41 step:32420 [D loss: 0.010508, acc.: 100.00%] [G loss: 4.940248]\n",
      "epoch:41 step:32421 [D loss: 0.005530, acc.: 100.00%] [G loss: 5.030783]\n",
      "epoch:41 step:32422 [D loss: 0.124593, acc.: 96.88%] [G loss: 3.023419]\n",
      "epoch:41 step:32423 [D loss: 0.073439, acc.: 99.22%] [G loss: 5.093150]\n",
      "epoch:41 step:32424 [D loss: 0.007319, acc.: 100.00%] [G loss: 5.707901]\n",
      "epoch:41 step:32425 [D loss: 0.004966, acc.: 100.00%] [G loss: 5.744446]\n",
      "epoch:41 step:32426 [D loss: 0.006205, acc.: 100.00%] [G loss: 5.665579]\n",
      "epoch:41 step:32427 [D loss: 0.006191, acc.: 100.00%] [G loss: 5.378248]\n",
      "epoch:41 step:32428 [D loss: 0.003426, acc.: 100.00%] [G loss: 5.238787]\n",
      "epoch:41 step:32429 [D loss: 0.004773, acc.: 100.00%] [G loss: 4.899754]\n",
      "epoch:41 step:32430 [D loss: 0.028794, acc.: 99.22%] [G loss: 4.406909]\n",
      "epoch:41 step:32431 [D loss: 0.010451, acc.: 100.00%] [G loss: 4.301868]\n",
      "epoch:41 step:32432 [D loss: 0.011677, acc.: 100.00%] [G loss: 4.559144]\n",
      "epoch:41 step:32433 [D loss: 0.005369, acc.: 100.00%] [G loss: 4.472383]\n",
      "epoch:41 step:32434 [D loss: 0.032985, acc.: 100.00%] [G loss: 3.904349]\n",
      "epoch:41 step:32435 [D loss: 0.011711, acc.: 100.00%] [G loss: 3.985718]\n",
      "epoch:41 step:32436 [D loss: 0.049352, acc.: 99.22%] [G loss: 5.163312]\n",
      "epoch:41 step:32437 [D loss: 0.003992, acc.: 100.00%] [G loss: 5.519407]\n",
      "epoch:41 step:32438 [D loss: 0.212797, acc.: 91.41%] [G loss: 4.906837]\n",
      "epoch:41 step:32439 [D loss: 0.014984, acc.: 100.00%] [G loss: 3.982067]\n",
      "epoch:41 step:32440 [D loss: 0.005663, acc.: 100.00%] [G loss: 5.187202]\n",
      "epoch:41 step:32441 [D loss: 0.107051, acc.: 96.09%] [G loss: 6.037227]\n",
      "epoch:41 step:32442 [D loss: 0.002620, acc.: 100.00%] [G loss: 6.827504]\n",
      "epoch:41 step:32443 [D loss: 0.088331, acc.: 96.09%] [G loss: 5.733874]\n",
      "epoch:41 step:32444 [D loss: 0.026071, acc.: 100.00%] [G loss: 5.141199]\n",
      "epoch:41 step:32445 [D loss: 0.013497, acc.: 100.00%] [G loss: 5.477780]\n",
      "epoch:41 step:32446 [D loss: 0.040448, acc.: 99.22%] [G loss: 6.864851]\n",
      "epoch:41 step:32447 [D loss: 0.004976, acc.: 100.00%] [G loss: 7.282474]\n",
      "epoch:41 step:32448 [D loss: 0.001418, acc.: 100.00%] [G loss: 6.509039]\n",
      "epoch:41 step:32449 [D loss: 0.060036, acc.: 98.44%] [G loss: 4.242105]\n",
      "epoch:41 step:32450 [D loss: 0.037065, acc.: 99.22%] [G loss: 4.758680]\n",
      "epoch:41 step:32451 [D loss: 0.005368, acc.: 100.00%] [G loss: 5.181188]\n",
      "epoch:41 step:32452 [D loss: 0.730263, acc.: 67.19%] [G loss: 10.318936]\n",
      "epoch:41 step:32453 [D loss: 2.233809, acc.: 53.12%] [G loss: 2.537283]\n",
      "epoch:41 step:32454 [D loss: 4.860035, acc.: 50.00%] [G loss: 8.849403]\n",
      "epoch:41 step:32455 [D loss: 2.355865, acc.: 50.78%] [G loss: 7.552653]\n",
      "epoch:41 step:32456 [D loss: 1.275970, acc.: 53.91%] [G loss: 4.855637]\n",
      "epoch:41 step:32457 [D loss: 0.069059, acc.: 96.88%] [G loss: 3.256932]\n",
      "epoch:41 step:32458 [D loss: 0.079062, acc.: 98.44%] [G loss: 3.912889]\n",
      "epoch:41 step:32459 [D loss: 0.090428, acc.: 97.66%] [G loss: 3.538213]\n",
      "epoch:41 step:32460 [D loss: 0.020172, acc.: 100.00%] [G loss: 3.814363]\n",
      "epoch:41 step:32461 [D loss: 0.018948, acc.: 100.00%] [G loss: 3.505992]\n",
      "epoch:41 step:32462 [D loss: 0.250940, acc.: 86.72%] [G loss: 2.771820]\n",
      "epoch:41 step:32463 [D loss: 0.067273, acc.: 99.22%] [G loss: 3.726880]\n",
      "epoch:41 step:32464 [D loss: 0.013920, acc.: 100.00%] [G loss: 3.239123]\n",
      "epoch:41 step:32465 [D loss: 0.048484, acc.: 100.00%] [G loss: 3.535188]\n",
      "epoch:41 step:32466 [D loss: 0.026976, acc.: 100.00%] [G loss: 3.038672]\n",
      "epoch:41 step:32467 [D loss: 0.186417, acc.: 94.53%] [G loss: 1.427976]\n",
      "epoch:41 step:32468 [D loss: 0.064110, acc.: 100.00%] [G loss: 2.139716]\n",
      "epoch:41 step:32469 [D loss: 0.207009, acc.: 92.97%] [G loss: 3.781513]\n",
      "epoch:41 step:32470 [D loss: 0.241251, acc.: 90.62%] [G loss: 3.456482]\n",
      "epoch:41 step:32471 [D loss: 0.083991, acc.: 99.22%] [G loss: 3.847750]\n",
      "epoch:41 step:32472 [D loss: 0.086343, acc.: 97.66%] [G loss: 3.048932]\n",
      "epoch:41 step:32473 [D loss: 0.106184, acc.: 98.44%] [G loss: 3.414563]\n",
      "epoch:41 step:32474 [D loss: 0.092622, acc.: 97.66%] [G loss: 3.068974]\n",
      "epoch:41 step:32475 [D loss: 0.054249, acc.: 99.22%] [G loss: 2.667358]\n",
      "epoch:41 step:32476 [D loss: 0.029691, acc.: 100.00%] [G loss: 3.134977]\n",
      "epoch:41 step:32477 [D loss: 0.134450, acc.: 96.09%] [G loss: 2.652178]\n",
      "epoch:41 step:32478 [D loss: 0.082470, acc.: 98.44%] [G loss: 1.916284]\n",
      "epoch:41 step:32479 [D loss: 0.056643, acc.: 100.00%] [G loss: 3.100581]\n",
      "epoch:41 step:32480 [D loss: 0.464070, acc.: 73.44%] [G loss: 5.125269]\n",
      "epoch:41 step:32481 [D loss: 0.630746, acc.: 70.31%] [G loss: 1.756559]\n",
      "epoch:41 step:32482 [D loss: 0.100709, acc.: 97.66%] [G loss: 2.923383]\n",
      "epoch:41 step:32483 [D loss: 0.016241, acc.: 100.00%] [G loss: 3.250639]\n",
      "epoch:41 step:32484 [D loss: 0.014315, acc.: 100.00%] [G loss: 3.114644]\n",
      "epoch:41 step:32485 [D loss: 0.039933, acc.: 100.00%] [G loss: 2.734710]\n",
      "epoch:41 step:32486 [D loss: 0.054715, acc.: 100.00%] [G loss: 3.983684]\n",
      "epoch:41 step:32487 [D loss: 0.020952, acc.: 100.00%] [G loss: 3.501439]\n",
      "epoch:41 step:32488 [D loss: 0.051613, acc.: 99.22%] [G loss: 3.595723]\n",
      "epoch:41 step:32489 [D loss: 0.102433, acc.: 99.22%] [G loss: 3.460320]\n",
      "epoch:41 step:32490 [D loss: 0.011856, acc.: 100.00%] [G loss: 4.094687]\n",
      "epoch:41 step:32491 [D loss: 0.059824, acc.: 97.66%] [G loss: 3.897261]\n",
      "epoch:41 step:32492 [D loss: 0.057387, acc.: 99.22%] [G loss: 4.134450]\n",
      "epoch:41 step:32493 [D loss: 0.041801, acc.: 100.00%] [G loss: 3.042689]\n",
      "epoch:41 step:32494 [D loss: 0.023385, acc.: 100.00%] [G loss: 4.004354]\n",
      "epoch:41 step:32495 [D loss: 0.026183, acc.: 100.00%] [G loss: 4.106812]\n",
      "epoch:41 step:32496 [D loss: 0.344366, acc.: 83.59%] [G loss: 5.621597]\n",
      "epoch:41 step:32497 [D loss: 0.018176, acc.: 100.00%] [G loss: 6.294700]\n",
      "epoch:41 step:32498 [D loss: 0.035537, acc.: 99.22%] [G loss: 6.418706]\n",
      "epoch:41 step:32499 [D loss: 0.010444, acc.: 100.00%] [G loss: 5.923704]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:41 step:32500 [D loss: 0.033094, acc.: 99.22%] [G loss: 5.028423]\n",
      "epoch:41 step:32501 [D loss: 0.009318, acc.: 100.00%] [G loss: 4.607094]\n",
      "epoch:41 step:32502 [D loss: 0.016440, acc.: 100.00%] [G loss: 5.305007]\n",
      "epoch:41 step:32503 [D loss: 0.015508, acc.: 100.00%] [G loss: 4.858846]\n",
      "epoch:41 step:32504 [D loss: 0.011084, acc.: 100.00%] [G loss: 4.755613]\n",
      "epoch:41 step:32505 [D loss: 0.008037, acc.: 100.00%] [G loss: 4.590576]\n",
      "epoch:41 step:32506 [D loss: 0.026458, acc.: 99.22%] [G loss: 4.288524]\n",
      "epoch:41 step:32507 [D loss: 0.030521, acc.: 100.00%] [G loss: 3.222089]\n",
      "epoch:41 step:32508 [D loss: 0.015493, acc.: 100.00%] [G loss: 4.260065]\n",
      "epoch:41 step:32509 [D loss: 0.010027, acc.: 100.00%] [G loss: 4.292230]\n",
      "epoch:41 step:32510 [D loss: 0.017429, acc.: 100.00%] [G loss: 4.041059]\n",
      "epoch:41 step:32511 [D loss: 0.007187, acc.: 100.00%] [G loss: 3.783847]\n",
      "epoch:41 step:32512 [D loss: 0.042651, acc.: 100.00%] [G loss: 4.094952]\n",
      "epoch:41 step:32513 [D loss: 2.933814, acc.: 29.69%] [G loss: 7.910673]\n",
      "epoch:41 step:32514 [D loss: 1.271776, acc.: 56.25%] [G loss: 6.692308]\n",
      "epoch:41 step:32515 [D loss: 0.319387, acc.: 89.06%] [G loss: 4.583065]\n",
      "epoch:41 step:32516 [D loss: 0.113954, acc.: 97.66%] [G loss: 3.877015]\n",
      "epoch:41 step:32517 [D loss: 0.065801, acc.: 97.66%] [G loss: 4.229869]\n",
      "epoch:41 step:32518 [D loss: 0.115192, acc.: 96.88%] [G loss: 4.215109]\n",
      "epoch:41 step:32519 [D loss: 0.388697, acc.: 84.38%] [G loss: 4.982609]\n",
      "epoch:41 step:32520 [D loss: 0.071807, acc.: 96.88%] [G loss: 4.742325]\n",
      "epoch:41 step:32521 [D loss: 0.458675, acc.: 82.03%] [G loss: 3.654149]\n",
      "epoch:41 step:32522 [D loss: 0.053923, acc.: 98.44%] [G loss: 3.433810]\n",
      "epoch:41 step:32523 [D loss: 0.037611, acc.: 99.22%] [G loss: 3.920451]\n",
      "epoch:41 step:32524 [D loss: 0.095516, acc.: 98.44%] [G loss: 3.760834]\n",
      "epoch:41 step:32525 [D loss: 0.061369, acc.: 99.22%] [G loss: 3.278778]\n",
      "epoch:41 step:32526 [D loss: 0.045701, acc.: 99.22%] [G loss: 3.284785]\n",
      "epoch:41 step:32527 [D loss: 0.290558, acc.: 89.06%] [G loss: 3.621902]\n",
      "epoch:41 step:32528 [D loss: 0.074721, acc.: 98.44%] [G loss: 3.957644]\n",
      "epoch:41 step:32529 [D loss: 0.453973, acc.: 76.56%] [G loss: 4.509097]\n",
      "epoch:41 step:32530 [D loss: 1.185700, acc.: 40.62%] [G loss: 4.587009]\n",
      "epoch:41 step:32531 [D loss: 0.045931, acc.: 99.22%] [G loss: 4.814356]\n",
      "epoch:41 step:32532 [D loss: 0.359587, acc.: 82.03%] [G loss: 3.043655]\n",
      "epoch:41 step:32533 [D loss: 0.275507, acc.: 91.41%] [G loss: 3.614148]\n",
      "epoch:41 step:32534 [D loss: 0.045643, acc.: 100.00%] [G loss: 4.340059]\n",
      "epoch:41 step:32535 [D loss: 0.164458, acc.: 95.31%] [G loss: 2.771551]\n",
      "epoch:41 step:32536 [D loss: 0.185693, acc.: 93.75%] [G loss: 3.540676]\n",
      "epoch:41 step:32537 [D loss: 0.145669, acc.: 96.09%] [G loss: 2.946113]\n",
      "epoch:41 step:32538 [D loss: 0.192071, acc.: 92.97%] [G loss: 3.754889]\n",
      "epoch:41 step:32539 [D loss: 0.105849, acc.: 96.09%] [G loss: 3.284865]\n",
      "epoch:41 step:32540 [D loss: 0.847248, acc.: 60.94%] [G loss: 4.655638]\n",
      "epoch:41 step:32541 [D loss: 0.059931, acc.: 96.88%] [G loss: 5.428266]\n",
      "epoch:41 step:32542 [D loss: 0.337177, acc.: 82.03%] [G loss: 3.205367]\n",
      "epoch:41 step:32543 [D loss: 0.141794, acc.: 92.19%] [G loss: 3.798729]\n",
      "epoch:41 step:32544 [D loss: 0.110215, acc.: 96.09%] [G loss: 4.118937]\n",
      "epoch:41 step:32545 [D loss: 0.150064, acc.: 93.75%] [G loss: 3.616689]\n",
      "epoch:41 step:32546 [D loss: 0.094608, acc.: 98.44%] [G loss: 3.332428]\n",
      "epoch:41 step:32547 [D loss: 0.094495, acc.: 97.66%] [G loss: 3.839566]\n",
      "epoch:41 step:32548 [D loss: 0.127854, acc.: 95.31%] [G loss: 3.130941]\n",
      "epoch:41 step:32549 [D loss: 0.142720, acc.: 96.88%] [G loss: 3.195718]\n",
      "epoch:41 step:32550 [D loss: 0.077596, acc.: 100.00%] [G loss: 3.999844]\n",
      "epoch:41 step:32551 [D loss: 0.080836, acc.: 98.44%] [G loss: 4.298250]\n",
      "epoch:41 step:32552 [D loss: 0.056360, acc.: 98.44%] [G loss: 3.334158]\n",
      "epoch:41 step:32553 [D loss: 0.066586, acc.: 99.22%] [G loss: 3.678217]\n",
      "epoch:41 step:32554 [D loss: 0.402487, acc.: 79.69%] [G loss: 4.051981]\n",
      "epoch:41 step:32555 [D loss: 0.061732, acc.: 97.66%] [G loss: 5.325952]\n",
      "epoch:41 step:32556 [D loss: 0.190481, acc.: 90.62%] [G loss: 3.518277]\n",
      "epoch:41 step:32557 [D loss: 0.042821, acc.: 100.00%] [G loss: 3.099437]\n",
      "epoch:41 step:32558 [D loss: 0.089024, acc.: 99.22%] [G loss: 3.857271]\n",
      "epoch:41 step:32559 [D loss: 0.018997, acc.: 100.00%] [G loss: 4.411689]\n",
      "epoch:41 step:32560 [D loss: 0.023199, acc.: 100.00%] [G loss: 3.992662]\n",
      "epoch:41 step:32561 [D loss: 0.057120, acc.: 98.44%] [G loss: 3.154541]\n",
      "epoch:41 step:32562 [D loss: 0.087409, acc.: 98.44%] [G loss: 4.389218]\n",
      "epoch:41 step:32563 [D loss: 0.134322, acc.: 94.53%] [G loss: 3.447601]\n",
      "epoch:41 step:32564 [D loss: 0.124838, acc.: 96.88%] [G loss: 2.978984]\n",
      "epoch:41 step:32565 [D loss: 0.048554, acc.: 100.00%] [G loss: 3.621837]\n",
      "epoch:41 step:32566 [D loss: 0.060415, acc.: 100.00%] [G loss: 4.537567]\n",
      "epoch:41 step:32567 [D loss: 0.086150, acc.: 96.09%] [G loss: 3.182239]\n",
      "epoch:41 step:32568 [D loss: 0.035387, acc.: 100.00%] [G loss: 3.452077]\n",
      "epoch:41 step:32569 [D loss: 0.138619, acc.: 95.31%] [G loss: 4.667623]\n",
      "epoch:41 step:32570 [D loss: 0.015666, acc.: 100.00%] [G loss: 5.295850]\n",
      "epoch:41 step:32571 [D loss: 0.072428, acc.: 96.88%] [G loss: 2.608042]\n",
      "epoch:41 step:32572 [D loss: 0.113545, acc.: 96.88%] [G loss: 5.352877]\n",
      "epoch:41 step:32573 [D loss: 0.194524, acc.: 90.62%] [G loss: 2.921512]\n",
      "epoch:41 step:32574 [D loss: 0.061955, acc.: 99.22%] [G loss: 4.330137]\n",
      "epoch:41 step:32575 [D loss: 0.026009, acc.: 99.22%] [G loss: 4.567870]\n",
      "epoch:41 step:32576 [D loss: 0.016357, acc.: 100.00%] [G loss: 4.563893]\n",
      "epoch:41 step:32577 [D loss: 0.025650, acc.: 100.00%] [G loss: 4.377666]\n",
      "epoch:41 step:32578 [D loss: 0.046323, acc.: 99.22%] [G loss: 3.118456]\n",
      "epoch:41 step:32579 [D loss: 0.052495, acc.: 100.00%] [G loss: 4.758493]\n",
      "epoch:41 step:32580 [D loss: 0.048541, acc.: 98.44%] [G loss: 5.379930]\n",
      "epoch:41 step:32581 [D loss: 0.052072, acc.: 98.44%] [G loss: 3.857121]\n",
      "epoch:41 step:32582 [D loss: 0.012514, acc.: 100.00%] [G loss: 3.295043]\n",
      "epoch:41 step:32583 [D loss: 0.106258, acc.: 96.09%] [G loss: 4.419680]\n",
      "epoch:41 step:32584 [D loss: 0.012296, acc.: 100.00%] [G loss: 5.015963]\n",
      "epoch:41 step:32585 [D loss: 0.021714, acc.: 100.00%] [G loss: 5.083200]\n",
      "epoch:41 step:32586 [D loss: 0.512790, acc.: 75.78%] [G loss: 6.593333]\n",
      "epoch:41 step:32587 [D loss: 0.306085, acc.: 87.50%] [G loss: 4.352370]\n",
      "epoch:41 step:32588 [D loss: 0.185987, acc.: 92.97%] [G loss: 5.935790]\n",
      "epoch:41 step:32589 [D loss: 0.001508, acc.: 100.00%] [G loss: 6.792852]\n",
      "epoch:41 step:32590 [D loss: 0.036154, acc.: 99.22%] [G loss: 7.001595]\n",
      "epoch:41 step:32591 [D loss: 0.032026, acc.: 98.44%] [G loss: 6.135919]\n",
      "epoch:41 step:32592 [D loss: 0.012809, acc.: 100.00%] [G loss: 5.679452]\n",
      "epoch:41 step:32593 [D loss: 0.001974, acc.: 100.00%] [G loss: 5.765594]\n",
      "epoch:41 step:32594 [D loss: 0.011304, acc.: 100.00%] [G loss: 5.537786]\n",
      "epoch:41 step:32595 [D loss: 0.016260, acc.: 100.00%] [G loss: 5.180984]\n",
      "epoch:41 step:32596 [D loss: 0.064060, acc.: 98.44%] [G loss: 3.973838]\n",
      "epoch:41 step:32597 [D loss: 0.017847, acc.: 100.00%] [G loss: 4.136212]\n",
      "epoch:41 step:32598 [D loss: 0.073255, acc.: 98.44%] [G loss: 4.734927]\n",
      "epoch:41 step:32599 [D loss: 0.004289, acc.: 100.00%] [G loss: 6.010424]\n",
      "epoch:41 step:32600 [D loss: 0.012389, acc.: 100.00%] [G loss: 5.607107]\n",
      "##############\n",
      "[1.09895336 2.11669898 1.10951507 0.94271035 2.09478738 1.07145082\n",
      " 2.11119551 2.11272192 1.00617881 0.92279543]\n",
      "##########\n",
      "epoch:41 step:32601 [D loss: 0.010021, acc.: 100.00%] [G loss: 4.732948]\n",
      "epoch:41 step:32602 [D loss: 0.009341, acc.: 100.00%] [G loss: 5.002392]\n",
      "epoch:41 step:32603 [D loss: 0.003556, acc.: 100.00%] [G loss: 4.681561]\n",
      "epoch:41 step:32604 [D loss: 0.011079, acc.: 100.00%] [G loss: 4.733134]\n",
      "epoch:41 step:32605 [D loss: 0.021248, acc.: 100.00%] [G loss: 4.904267]\n",
      "epoch:41 step:32606 [D loss: 0.004952, acc.: 100.00%] [G loss: 4.394513]\n",
      "epoch:41 step:32607 [D loss: 0.009899, acc.: 100.00%] [G loss: 3.995524]\n",
      "epoch:41 step:32608 [D loss: 0.086077, acc.: 98.44%] [G loss: 5.087974]\n",
      "epoch:41 step:32609 [D loss: 0.086958, acc.: 97.66%] [G loss: 5.263155]\n",
      "epoch:41 step:32610 [D loss: 0.011786, acc.: 100.00%] [G loss: 4.961092]\n",
      "epoch:41 step:32611 [D loss: 0.012539, acc.: 100.00%] [G loss: 5.112766]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:41 step:32612 [D loss: 0.029642, acc.: 99.22%] [G loss: 5.351293]\n",
      "epoch:41 step:32613 [D loss: 0.006971, acc.: 100.00%] [G loss: 5.737058]\n",
      "epoch:41 step:32614 [D loss: 0.019514, acc.: 99.22%] [G loss: 4.697290]\n",
      "epoch:41 step:32615 [D loss: 0.015113, acc.: 100.00%] [G loss: 5.327827]\n",
      "epoch:41 step:32616 [D loss: 0.017821, acc.: 100.00%] [G loss: 5.549352]\n",
      "epoch:41 step:32617 [D loss: 0.016724, acc.: 100.00%] [G loss: 4.840340]\n",
      "epoch:41 step:32618 [D loss: 0.046786, acc.: 100.00%] [G loss: 5.219174]\n",
      "epoch:41 step:32619 [D loss: 0.001905, acc.: 100.00%] [G loss: 6.157224]\n",
      "epoch:41 step:32620 [D loss: 2.161057, acc.: 29.69%] [G loss: 9.525066]\n",
      "epoch:41 step:32621 [D loss: 0.951665, acc.: 58.59%] [G loss: 8.255695]\n",
      "epoch:41 step:32622 [D loss: 0.002245, acc.: 100.00%] [G loss: 6.390731]\n",
      "epoch:41 step:32623 [D loss: 0.007069, acc.: 100.00%] [G loss: 3.564832]\n",
      "epoch:41 step:32624 [D loss: 0.003525, acc.: 100.00%] [G loss: 4.862054]\n",
      "epoch:41 step:32625 [D loss: 0.009156, acc.: 100.00%] [G loss: 3.912502]\n",
      "epoch:41 step:32626 [D loss: 0.020535, acc.: 99.22%] [G loss: 3.830064]\n",
      "epoch:41 step:32627 [D loss: 0.088250, acc.: 96.88%] [G loss: 5.397955]\n",
      "epoch:41 step:32628 [D loss: 0.016681, acc.: 100.00%] [G loss: 6.037597]\n",
      "epoch:41 step:32629 [D loss: 0.009441, acc.: 99.22%] [G loss: 5.223475]\n",
      "epoch:41 step:32630 [D loss: 0.113920, acc.: 97.66%] [G loss: 4.046277]\n",
      "epoch:41 step:32631 [D loss: 0.017233, acc.: 100.00%] [G loss: 5.353590]\n",
      "epoch:41 step:32632 [D loss: 0.015609, acc.: 99.22%] [G loss: 4.996165]\n",
      "epoch:41 step:32633 [D loss: 0.036875, acc.: 99.22%] [G loss: 4.172809]\n",
      "epoch:41 step:32634 [D loss: 0.021852, acc.: 100.00%] [G loss: 5.083936]\n",
      "epoch:41 step:32635 [D loss: 0.069621, acc.: 99.22%] [G loss: 6.198867]\n",
      "epoch:41 step:32636 [D loss: 0.013239, acc.: 100.00%] [G loss: 6.404504]\n",
      "epoch:41 step:32637 [D loss: 0.079100, acc.: 97.66%] [G loss: 4.620857]\n",
      "epoch:41 step:32638 [D loss: 0.206385, acc.: 91.41%] [G loss: 6.269147]\n",
      "epoch:41 step:32639 [D loss: 0.094819, acc.: 93.75%] [G loss: 6.806385]\n",
      "epoch:41 step:32640 [D loss: 0.049815, acc.: 97.66%] [G loss: 5.873671]\n",
      "epoch:41 step:32641 [D loss: 0.006383, acc.: 100.00%] [G loss: 4.895819]\n",
      "epoch:41 step:32642 [D loss: 0.008824, acc.: 100.00%] [G loss: 4.526322]\n",
      "epoch:41 step:32643 [D loss: 0.009137, acc.: 100.00%] [G loss: 3.927296]\n",
      "epoch:41 step:32644 [D loss: 0.010333, acc.: 100.00%] [G loss: 3.523078]\n",
      "epoch:41 step:32645 [D loss: 0.015974, acc.: 100.00%] [G loss: 3.117401]\n",
      "epoch:41 step:32646 [D loss: 0.012791, acc.: 100.00%] [G loss: 3.523773]\n",
      "epoch:41 step:32647 [D loss: 0.047643, acc.: 98.44%] [G loss: 2.891916]\n",
      "epoch:41 step:32648 [D loss: 0.097345, acc.: 98.44%] [G loss: 4.758554]\n",
      "epoch:41 step:32649 [D loss: 0.008122, acc.: 100.00%] [G loss: 6.379470]\n",
      "epoch:41 step:32650 [D loss: 0.174276, acc.: 91.41%] [G loss: 2.957711]\n",
      "epoch:41 step:32651 [D loss: 0.249704, acc.: 88.28%] [G loss: 7.620914]\n",
      "epoch:41 step:32652 [D loss: 0.176965, acc.: 90.62%] [G loss: 7.340863]\n",
      "epoch:41 step:32653 [D loss: 0.002298, acc.: 100.00%] [G loss: 6.355270]\n",
      "epoch:41 step:32654 [D loss: 0.002170, acc.: 100.00%] [G loss: 6.675440]\n",
      "epoch:41 step:32655 [D loss: 0.010729, acc.: 100.00%] [G loss: 6.265386]\n",
      "epoch:41 step:32656 [D loss: 0.003474, acc.: 100.00%] [G loss: 5.892830]\n",
      "epoch:41 step:32657 [D loss: 0.003763, acc.: 100.00%] [G loss: 5.563466]\n",
      "epoch:41 step:32658 [D loss: 0.015042, acc.: 100.00%] [G loss: 4.249932]\n",
      "epoch:41 step:32659 [D loss: 0.021759, acc.: 100.00%] [G loss: 4.686688]\n",
      "epoch:41 step:32660 [D loss: 0.013802, acc.: 100.00%] [G loss: 5.075968]\n",
      "epoch:41 step:32661 [D loss: 0.011737, acc.: 100.00%] [G loss: 5.030196]\n",
      "epoch:41 step:32662 [D loss: 0.010969, acc.: 100.00%] [G loss: 4.023393]\n",
      "epoch:41 step:32663 [D loss: 0.036302, acc.: 100.00%] [G loss: 4.653084]\n",
      "epoch:41 step:32664 [D loss: 0.006435, acc.: 100.00%] [G loss: 4.292726]\n",
      "epoch:41 step:32665 [D loss: 0.049009, acc.: 99.22%] [G loss: 5.463944]\n",
      "epoch:41 step:32666 [D loss: 0.005219, acc.: 100.00%] [G loss: 7.208491]\n",
      "epoch:41 step:32667 [D loss: 1.103734, acc.: 55.47%] [G loss: 6.252907]\n",
      "epoch:41 step:32668 [D loss: 0.139248, acc.: 95.31%] [G loss: 7.202207]\n",
      "epoch:41 step:32669 [D loss: 0.025818, acc.: 99.22%] [G loss: 5.279365]\n",
      "epoch:41 step:32670 [D loss: 0.199840, acc.: 92.97%] [G loss: 3.699889]\n",
      "epoch:41 step:32671 [D loss: 0.015040, acc.: 100.00%] [G loss: 3.712122]\n",
      "epoch:41 step:32672 [D loss: 0.050586, acc.: 98.44%] [G loss: 4.481418]\n",
      "epoch:41 step:32673 [D loss: 0.011686, acc.: 100.00%] [G loss: 5.072558]\n",
      "epoch:41 step:32674 [D loss: 0.011996, acc.: 100.00%] [G loss: 4.598212]\n",
      "epoch:41 step:32675 [D loss: 0.022764, acc.: 100.00%] [G loss: 4.644640]\n",
      "epoch:41 step:32676 [D loss: 0.013268, acc.: 100.00%] [G loss: 4.696894]\n",
      "epoch:41 step:32677 [D loss: 0.041241, acc.: 99.22%] [G loss: 3.966763]\n",
      "epoch:41 step:32678 [D loss: 0.015159, acc.: 100.00%] [G loss: 5.660025]\n",
      "epoch:41 step:32679 [D loss: 0.024446, acc.: 100.00%] [G loss: 6.111704]\n",
      "epoch:41 step:32680 [D loss: 0.002591, acc.: 100.00%] [G loss: 4.782529]\n",
      "epoch:41 step:32681 [D loss: 0.030139, acc.: 100.00%] [G loss: 4.380219]\n",
      "epoch:41 step:32682 [D loss: 0.017056, acc.: 100.00%] [G loss: 4.026510]\n",
      "epoch:41 step:32683 [D loss: 0.027707, acc.: 100.00%] [G loss: 4.808131]\n",
      "epoch:41 step:32684 [D loss: 0.006618, acc.: 100.00%] [G loss: 5.543326]\n",
      "epoch:41 step:32685 [D loss: 0.005437, acc.: 100.00%] [G loss: 4.847163]\n",
      "epoch:41 step:32686 [D loss: 0.037209, acc.: 100.00%] [G loss: 3.778572]\n",
      "epoch:41 step:32687 [D loss: 0.002898, acc.: 100.00%] [G loss: 5.049077]\n",
      "epoch:41 step:32688 [D loss: 0.006766, acc.: 100.00%] [G loss: 4.572365]\n",
      "epoch:41 step:32689 [D loss: 0.004007, acc.: 100.00%] [G loss: 4.605548]\n",
      "epoch:41 step:32690 [D loss: 0.004042, acc.: 100.00%] [G loss: 5.014645]\n",
      "epoch:41 step:32691 [D loss: 0.028245, acc.: 99.22%] [G loss: 4.135722]\n",
      "epoch:41 step:32692 [D loss: 0.042868, acc.: 100.00%] [G loss: 5.146551]\n",
      "epoch:41 step:32693 [D loss: 0.016794, acc.: 100.00%] [G loss: 6.059297]\n",
      "epoch:41 step:32694 [D loss: 0.045416, acc.: 99.22%] [G loss: 3.669643]\n",
      "epoch:41 step:32695 [D loss: 0.009709, acc.: 100.00%] [G loss: 4.244076]\n",
      "epoch:41 step:32696 [D loss: 0.007979, acc.: 100.00%] [G loss: 3.662927]\n",
      "epoch:41 step:32697 [D loss: 0.014679, acc.: 100.00%] [G loss: 4.886037]\n",
      "epoch:41 step:32698 [D loss: 0.005372, acc.: 100.00%] [G loss: 4.909478]\n",
      "epoch:41 step:32699 [D loss: 0.004951, acc.: 100.00%] [G loss: 5.537729]\n",
      "epoch:41 step:32700 [D loss: 0.020165, acc.: 100.00%] [G loss: 3.992165]\n",
      "epoch:41 step:32701 [D loss: 0.041160, acc.: 100.00%] [G loss: 6.323498]\n",
      "epoch:41 step:32702 [D loss: 0.012555, acc.: 100.00%] [G loss: 7.119970]\n",
      "epoch:41 step:32703 [D loss: 0.055933, acc.: 98.44%] [G loss: 4.350461]\n",
      "epoch:41 step:32704 [D loss: 0.021518, acc.: 99.22%] [G loss: 3.167576]\n",
      "epoch:41 step:32705 [D loss: 0.072562, acc.: 99.22%] [G loss: 6.837201]\n",
      "epoch:41 step:32706 [D loss: 0.505778, acc.: 78.12%] [G loss: 4.924260]\n",
      "epoch:41 step:32707 [D loss: 0.005464, acc.: 100.00%] [G loss: 4.927264]\n",
      "epoch:41 step:32708 [D loss: 0.021310, acc.: 100.00%] [G loss: 5.525439]\n",
      "epoch:41 step:32709 [D loss: 0.030863, acc.: 99.22%] [G loss: 5.839611]\n",
      "epoch:41 step:32710 [D loss: 0.003894, acc.: 100.00%] [G loss: 6.549022]\n",
      "epoch:41 step:32711 [D loss: 0.064167, acc.: 99.22%] [G loss: 6.983807]\n",
      "epoch:41 step:32712 [D loss: 0.010285, acc.: 100.00%] [G loss: 5.551070]\n",
      "epoch:41 step:32713 [D loss: 0.170939, acc.: 92.19%] [G loss: 1.544750]\n",
      "epoch:41 step:32714 [D loss: 1.574798, acc.: 51.56%] [G loss: 10.961016]\n",
      "epoch:41 step:32715 [D loss: 3.269443, acc.: 50.00%] [G loss: 7.099429]\n",
      "epoch:41 step:32716 [D loss: 1.107715, acc.: 57.03%] [G loss: 4.845292]\n",
      "epoch:41 step:32717 [D loss: 0.030855, acc.: 100.00%] [G loss: 5.264197]\n",
      "epoch:41 step:32718 [D loss: 0.203697, acc.: 92.19%] [G loss: 4.272008]\n",
      "epoch:41 step:32719 [D loss: 0.020171, acc.: 100.00%] [G loss: 2.934125]\n",
      "epoch:41 step:32720 [D loss: 0.031612, acc.: 99.22%] [G loss: 3.040915]\n",
      "epoch:41 step:32721 [D loss: 0.062157, acc.: 97.66%] [G loss: 2.706336]\n",
      "epoch:41 step:32722 [D loss: 0.168475, acc.: 95.31%] [G loss: 3.919863]\n",
      "epoch:41 step:32723 [D loss: 0.199378, acc.: 94.53%] [G loss: 5.207900]\n",
      "epoch:41 step:32724 [D loss: 0.737451, acc.: 64.84%] [G loss: 0.859904]\n",
      "epoch:41 step:32725 [D loss: 1.269130, acc.: 60.94%] [G loss: 8.382496]\n",
      "epoch:41 step:32726 [D loss: 2.134723, acc.: 50.00%] [G loss: 7.112885]\n",
      "epoch:41 step:32727 [D loss: 0.909584, acc.: 57.81%] [G loss: 4.233541]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:41 step:32728 [D loss: 0.017257, acc.: 100.00%] [G loss: 3.707776]\n",
      "epoch:41 step:32729 [D loss: 0.033830, acc.: 100.00%] [G loss: 3.751242]\n",
      "epoch:41 step:32730 [D loss: 0.029138, acc.: 100.00%] [G loss: 3.162357]\n",
      "epoch:41 step:32731 [D loss: 0.040369, acc.: 100.00%] [G loss: 2.948309]\n",
      "epoch:41 step:32732 [D loss: 0.084897, acc.: 98.44%] [G loss: 3.110053]\n",
      "epoch:41 step:32733 [D loss: 0.018018, acc.: 100.00%] [G loss: 3.611578]\n",
      "epoch:41 step:32734 [D loss: 0.107462, acc.: 96.88%] [G loss: 3.458362]\n",
      "epoch:41 step:32735 [D loss: 0.017513, acc.: 100.00%] [G loss: 3.186214]\n",
      "epoch:41 step:32736 [D loss: 0.116007, acc.: 96.88%] [G loss: 2.338172]\n",
      "epoch:41 step:32737 [D loss: 0.056132, acc.: 99.22%] [G loss: 3.131753]\n",
      "epoch:41 step:32738 [D loss: 0.039001, acc.: 99.22%] [G loss: 3.385912]\n",
      "epoch:41 step:32739 [D loss: 0.039744, acc.: 100.00%] [G loss: 4.170366]\n",
      "epoch:41 step:32740 [D loss: 0.014009, acc.: 100.00%] [G loss: 3.858351]\n",
      "epoch:41 step:32741 [D loss: 0.038503, acc.: 100.00%] [G loss: 3.555238]\n",
      "epoch:41 step:32742 [D loss: 0.009386, acc.: 100.00%] [G loss: 3.552738]\n",
      "epoch:41 step:32743 [D loss: 0.065573, acc.: 100.00%] [G loss: 3.234415]\n",
      "epoch:41 step:32744 [D loss: 0.050846, acc.: 100.00%] [G loss: 3.573671]\n",
      "epoch:41 step:32745 [D loss: 0.042009, acc.: 100.00%] [G loss: 3.308500]\n",
      "epoch:41 step:32746 [D loss: 0.662999, acc.: 63.28%] [G loss: 4.858650]\n",
      "epoch:41 step:32747 [D loss: 0.027559, acc.: 100.00%] [G loss: 5.552372]\n",
      "epoch:41 step:32748 [D loss: 0.386631, acc.: 78.91%] [G loss: 3.958543]\n",
      "epoch:41 step:32749 [D loss: 0.021363, acc.: 100.00%] [G loss: 3.875510]\n",
      "epoch:41 step:32750 [D loss: 0.023411, acc.: 100.00%] [G loss: 3.389361]\n",
      "epoch:41 step:32751 [D loss: 0.025541, acc.: 100.00%] [G loss: 3.006037]\n",
      "epoch:41 step:32752 [D loss: 0.067021, acc.: 98.44%] [G loss: 4.188427]\n",
      "epoch:41 step:32753 [D loss: 0.014559, acc.: 100.00%] [G loss: 4.269448]\n",
      "epoch:41 step:32754 [D loss: 0.013013, acc.: 100.00%] [G loss: 4.644943]\n",
      "epoch:41 step:32755 [D loss: 0.013993, acc.: 100.00%] [G loss: 4.281501]\n",
      "epoch:41 step:32756 [D loss: 0.027160, acc.: 100.00%] [G loss: 4.037019]\n",
      "epoch:41 step:32757 [D loss: 0.017014, acc.: 100.00%] [G loss: 3.747397]\n",
      "epoch:41 step:32758 [D loss: 0.040667, acc.: 100.00%] [G loss: 3.836264]\n",
      "epoch:41 step:32759 [D loss: 0.012914, acc.: 100.00%] [G loss: 4.042332]\n",
      "epoch:41 step:32760 [D loss: 0.122816, acc.: 96.09%] [G loss: 3.324981]\n",
      "epoch:41 step:32761 [D loss: 0.051582, acc.: 100.00%] [G loss: 3.948444]\n",
      "epoch:41 step:32762 [D loss: 0.015082, acc.: 100.00%] [G loss: 4.490576]\n",
      "epoch:41 step:32763 [D loss: 0.018953, acc.: 100.00%] [G loss: 3.671168]\n",
      "epoch:41 step:32764 [D loss: 0.020758, acc.: 100.00%] [G loss: 4.195107]\n",
      "epoch:41 step:32765 [D loss: 0.020009, acc.: 100.00%] [G loss: 3.913710]\n",
      "epoch:41 step:32766 [D loss: 0.014326, acc.: 100.00%] [G loss: 4.178586]\n",
      "epoch:41 step:32767 [D loss: 0.028519, acc.: 99.22%] [G loss: 4.225033]\n",
      "epoch:41 step:32768 [D loss: 0.040454, acc.: 99.22%] [G loss: 3.010558]\n",
      "epoch:41 step:32769 [D loss: 0.041558, acc.: 100.00%] [G loss: 3.568533]\n",
      "epoch:41 step:32770 [D loss: 0.029996, acc.: 100.00%] [G loss: 4.451614]\n",
      "epoch:41 step:32771 [D loss: 0.034160, acc.: 100.00%] [G loss: 4.481275]\n",
      "epoch:41 step:32772 [D loss: 0.011362, acc.: 100.00%] [G loss: 4.480001]\n",
      "epoch:41 step:32773 [D loss: 0.024860, acc.: 100.00%] [G loss: 4.728964]\n",
      "epoch:41 step:32774 [D loss: 0.146838, acc.: 96.09%] [G loss: 1.448970]\n",
      "epoch:41 step:32775 [D loss: 0.188300, acc.: 95.31%] [G loss: 5.554017]\n",
      "epoch:41 step:32776 [D loss: 0.012136, acc.: 99.22%] [G loss: 6.823344]\n",
      "epoch:41 step:32777 [D loss: 0.611388, acc.: 69.53%] [G loss: 1.391783]\n",
      "epoch:41 step:32778 [D loss: 0.316906, acc.: 84.38%] [G loss: 5.488264]\n",
      "epoch:41 step:32779 [D loss: 0.007454, acc.: 100.00%] [G loss: 6.708836]\n",
      "epoch:41 step:32780 [D loss: 0.373805, acc.: 82.03%] [G loss: 4.469186]\n",
      "epoch:41 step:32781 [D loss: 0.020216, acc.: 100.00%] [G loss: 3.640530]\n",
      "epoch:41 step:32782 [D loss: 0.021300, acc.: 100.00%] [G loss: 4.018106]\n",
      "epoch:41 step:32783 [D loss: 0.007889, acc.: 100.00%] [G loss: 3.405210]\n",
      "epoch:41 step:32784 [D loss: 0.037947, acc.: 100.00%] [G loss: 4.189463]\n",
      "epoch:41 step:32785 [D loss: 0.009680, acc.: 100.00%] [G loss: 4.726757]\n",
      "epoch:41 step:32786 [D loss: 0.008890, acc.: 100.00%] [G loss: 4.897837]\n",
      "epoch:41 step:32787 [D loss: 0.005816, acc.: 100.00%] [G loss: 4.917684]\n",
      "epoch:41 step:32788 [D loss: 0.012063, acc.: 100.00%] [G loss: 4.367355]\n",
      "epoch:41 step:32789 [D loss: 0.012917, acc.: 100.00%] [G loss: 4.367818]\n",
      "epoch:41 step:32790 [D loss: 0.009211, acc.: 100.00%] [G loss: 4.297483]\n",
      "epoch:41 step:32791 [D loss: 0.020903, acc.: 100.00%] [G loss: 4.056959]\n",
      "epoch:41 step:32792 [D loss: 0.021886, acc.: 99.22%] [G loss: 4.416120]\n",
      "epoch:41 step:32793 [D loss: 0.010404, acc.: 100.00%] [G loss: 3.616585]\n",
      "epoch:41 step:32794 [D loss: 0.022691, acc.: 100.00%] [G loss: 4.169487]\n",
      "epoch:41 step:32795 [D loss: 0.005732, acc.: 100.00%] [G loss: 4.184062]\n",
      "epoch:41 step:32796 [D loss: 0.016043, acc.: 100.00%] [G loss: 3.664765]\n",
      "epoch:41 step:32797 [D loss: 0.008241, acc.: 100.00%] [G loss: 3.498402]\n",
      "epoch:41 step:32798 [D loss: 0.024164, acc.: 100.00%] [G loss: 4.737904]\n",
      "epoch:41 step:32799 [D loss: 0.024247, acc.: 100.00%] [G loss: 2.761218]\n",
      "epoch:41 step:32800 [D loss: 0.041400, acc.: 99.22%] [G loss: 3.755800]\n",
      "##############\n",
      "[0.85713373 0.99483751 0.94870753 0.9435784  2.10708926 1.0928235\n",
      " 0.96620778 1.11257499 2.12331999 0.96005954]\n",
      "##########\n",
      "epoch:41 step:32801 [D loss: 0.038208, acc.: 100.00%] [G loss: 3.672137]\n",
      "epoch:41 step:32802 [D loss: 0.030776, acc.: 100.00%] [G loss: 3.937130]\n",
      "epoch:42 step:32803 [D loss: 0.014655, acc.: 100.00%] [G loss: 3.002230]\n",
      "epoch:42 step:32804 [D loss: 0.040448, acc.: 100.00%] [G loss: 4.022533]\n",
      "epoch:42 step:32805 [D loss: 0.048306, acc.: 97.66%] [G loss: 5.068242]\n",
      "epoch:42 step:32806 [D loss: 0.043464, acc.: 99.22%] [G loss: 3.697053]\n",
      "epoch:42 step:32807 [D loss: 0.017164, acc.: 100.00%] [G loss: 4.197429]\n",
      "epoch:42 step:32808 [D loss: 0.025067, acc.: 100.00%] [G loss: 5.132985]\n",
      "epoch:42 step:32809 [D loss: 0.011734, acc.: 100.00%] [G loss: 5.333246]\n",
      "epoch:42 step:32810 [D loss: 0.005721, acc.: 100.00%] [G loss: 4.708139]\n",
      "epoch:42 step:32811 [D loss: 0.015281, acc.: 100.00%] [G loss: 4.693576]\n",
      "epoch:42 step:32812 [D loss: 0.035006, acc.: 100.00%] [G loss: 5.482842]\n",
      "epoch:42 step:32813 [D loss: 0.009908, acc.: 100.00%] [G loss: 4.260179]\n",
      "epoch:42 step:32814 [D loss: 0.046819, acc.: 99.22%] [G loss: 3.263802]\n",
      "epoch:42 step:32815 [D loss: 0.030882, acc.: 100.00%] [G loss: 4.752827]\n",
      "epoch:42 step:32816 [D loss: 0.013247, acc.: 100.00%] [G loss: 4.510128]\n",
      "epoch:42 step:32817 [D loss: 0.036651, acc.: 100.00%] [G loss: 3.882613]\n",
      "epoch:42 step:32818 [D loss: 0.010949, acc.: 100.00%] [G loss: 4.433339]\n",
      "epoch:42 step:32819 [D loss: 0.008043, acc.: 100.00%] [G loss: 3.364286]\n",
      "epoch:42 step:32820 [D loss: 0.002625, acc.: 100.00%] [G loss: 3.630660]\n",
      "epoch:42 step:32821 [D loss: 0.010679, acc.: 100.00%] [G loss: 3.041358]\n",
      "epoch:42 step:32822 [D loss: 0.005960, acc.: 100.00%] [G loss: 3.126276]\n",
      "epoch:42 step:32823 [D loss: 0.068440, acc.: 100.00%] [G loss: 5.576332]\n",
      "epoch:42 step:32824 [D loss: 0.022539, acc.: 99.22%] [G loss: 6.093113]\n",
      "epoch:42 step:32825 [D loss: 3.081422, acc.: 17.97%] [G loss: 9.007707]\n",
      "epoch:42 step:32826 [D loss: 1.633046, acc.: 51.56%] [G loss: 6.319375]\n",
      "epoch:42 step:32827 [D loss: 0.104610, acc.: 96.09%] [G loss: 5.019739]\n",
      "epoch:42 step:32828 [D loss: 0.052255, acc.: 99.22%] [G loss: 4.097278]\n",
      "epoch:42 step:32829 [D loss: 0.153883, acc.: 93.75%] [G loss: 4.255913]\n",
      "epoch:42 step:32830 [D loss: 0.019689, acc.: 100.00%] [G loss: 5.416323]\n",
      "epoch:42 step:32831 [D loss: 0.069462, acc.: 99.22%] [G loss: 4.187797]\n",
      "epoch:42 step:32832 [D loss: 0.048842, acc.: 99.22%] [G loss: 3.582421]\n",
      "epoch:42 step:32833 [D loss: 0.053998, acc.: 99.22%] [G loss: 3.741919]\n",
      "epoch:42 step:32834 [D loss: 0.164037, acc.: 96.88%] [G loss: 2.895507]\n",
      "epoch:42 step:32835 [D loss: 0.056413, acc.: 100.00%] [G loss: 3.242415]\n",
      "epoch:42 step:32836 [D loss: 0.054119, acc.: 100.00%] [G loss: 3.857294]\n",
      "epoch:42 step:32837 [D loss: 0.106314, acc.: 98.44%] [G loss: 2.829028]\n",
      "epoch:42 step:32838 [D loss: 0.185465, acc.: 92.97%] [G loss: 5.162743]\n",
      "epoch:42 step:32839 [D loss: 0.069401, acc.: 97.66%] [G loss: 5.198556]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:42 step:32840 [D loss: 0.236383, acc.: 92.19%] [G loss: 2.784748]\n",
      "epoch:42 step:32841 [D loss: 0.125601, acc.: 96.88%] [G loss: 3.867754]\n",
      "epoch:42 step:32842 [D loss: 0.031842, acc.: 100.00%] [G loss: 4.893660]\n",
      "epoch:42 step:32843 [D loss: 0.234184, acc.: 90.62%] [G loss: 3.385152]\n",
      "epoch:42 step:32844 [D loss: 0.074838, acc.: 98.44%] [G loss: 2.991390]\n",
      "epoch:42 step:32845 [D loss: 0.024936, acc.: 100.00%] [G loss: 2.711627]\n",
      "epoch:42 step:32846 [D loss: 0.116994, acc.: 95.31%] [G loss: 4.172171]\n",
      "epoch:42 step:32847 [D loss: 0.024223, acc.: 100.00%] [G loss: 5.397973]\n",
      "epoch:42 step:32848 [D loss: 0.162552, acc.: 93.75%] [G loss: 2.766752]\n",
      "epoch:42 step:32849 [D loss: 0.054945, acc.: 100.00%] [G loss: 3.076171]\n",
      "epoch:42 step:32850 [D loss: 0.039259, acc.: 100.00%] [G loss: 4.058294]\n",
      "epoch:42 step:32851 [D loss: 0.355712, acc.: 86.72%] [G loss: 4.343338]\n",
      "epoch:42 step:32852 [D loss: 0.104525, acc.: 95.31%] [G loss: 4.551536]\n",
      "epoch:42 step:32853 [D loss: 0.042927, acc.: 99.22%] [G loss: 4.503918]\n",
      "epoch:42 step:32854 [D loss: 0.020973, acc.: 100.00%] [G loss: 5.351106]\n",
      "epoch:42 step:32855 [D loss: 0.078747, acc.: 98.44%] [G loss: 3.607868]\n",
      "epoch:42 step:32856 [D loss: 0.036485, acc.: 100.00%] [G loss: 3.948025]\n",
      "epoch:42 step:32857 [D loss: 0.026241, acc.: 100.00%] [G loss: 3.921202]\n",
      "epoch:42 step:32858 [D loss: 0.078955, acc.: 97.66%] [G loss: 2.758536]\n",
      "epoch:42 step:32859 [D loss: 0.124586, acc.: 96.88%] [G loss: 5.818942]\n",
      "epoch:42 step:32860 [D loss: 0.096558, acc.: 96.88%] [G loss: 5.167784]\n",
      "epoch:42 step:32861 [D loss: 0.024859, acc.: 100.00%] [G loss: 4.576018]\n",
      "epoch:42 step:32862 [D loss: 0.089465, acc.: 98.44%] [G loss: 3.508275]\n",
      "epoch:42 step:32863 [D loss: 0.019148, acc.: 100.00%] [G loss: 4.059994]\n",
      "epoch:42 step:32864 [D loss: 0.033642, acc.: 100.00%] [G loss: 4.662917]\n",
      "epoch:42 step:32865 [D loss: 0.071665, acc.: 98.44%] [G loss: 3.845045]\n",
      "epoch:42 step:32866 [D loss: 0.059209, acc.: 100.00%] [G loss: 4.425170]\n",
      "epoch:42 step:32867 [D loss: 0.017424, acc.: 100.00%] [G loss: 4.585217]\n",
      "epoch:42 step:32868 [D loss: 0.066501, acc.: 97.66%] [G loss: 2.945790]\n",
      "epoch:42 step:32869 [D loss: 0.133165, acc.: 96.09%] [G loss: 5.954221]\n",
      "epoch:42 step:32870 [D loss: 0.393317, acc.: 82.03%] [G loss: 2.115731]\n",
      "epoch:42 step:32871 [D loss: 0.055080, acc.: 98.44%] [G loss: 3.177697]\n",
      "epoch:42 step:32872 [D loss: 0.023444, acc.: 100.00%] [G loss: 4.378951]\n",
      "epoch:42 step:32873 [D loss: 0.062330, acc.: 98.44%] [G loss: 3.491732]\n",
      "epoch:42 step:32874 [D loss: 0.056038, acc.: 100.00%] [G loss: 4.781618]\n",
      "epoch:42 step:32875 [D loss: 0.071929, acc.: 97.66%] [G loss: 3.883865]\n",
      "epoch:42 step:32876 [D loss: 0.069143, acc.: 99.22%] [G loss: 5.676144]\n",
      "epoch:42 step:32877 [D loss: 0.091979, acc.: 97.66%] [G loss: 4.438756]\n",
      "epoch:42 step:32878 [D loss: 0.020879, acc.: 100.00%] [G loss: 4.431652]\n",
      "epoch:42 step:32879 [D loss: 0.015193, acc.: 100.00%] [G loss: 4.059612]\n",
      "epoch:42 step:32880 [D loss: 0.016089, acc.: 100.00%] [G loss: 4.163635]\n",
      "epoch:42 step:32881 [D loss: 0.021022, acc.: 100.00%] [G loss: 3.748900]\n",
      "epoch:42 step:32882 [D loss: 0.019420, acc.: 100.00%] [G loss: 4.505697]\n",
      "epoch:42 step:32883 [D loss: 0.022016, acc.: 100.00%] [G loss: 4.789120]\n",
      "epoch:42 step:32884 [D loss: 0.008596, acc.: 100.00%] [G loss: 4.728875]\n",
      "epoch:42 step:32885 [D loss: 0.104719, acc.: 98.44%] [G loss: 3.912773]\n",
      "epoch:42 step:32886 [D loss: 0.027052, acc.: 99.22%] [G loss: 5.565722]\n",
      "epoch:42 step:32887 [D loss: 0.012837, acc.: 100.00%] [G loss: 4.847954]\n",
      "epoch:42 step:32888 [D loss: 0.012592, acc.: 100.00%] [G loss: 4.646132]\n",
      "epoch:42 step:32889 [D loss: 0.011945, acc.: 100.00%] [G loss: 5.355023]\n",
      "epoch:42 step:32890 [D loss: 0.376736, acc.: 83.59%] [G loss: 7.833552]\n",
      "epoch:42 step:32891 [D loss: 0.553349, acc.: 75.00%] [G loss: 1.507234]\n",
      "epoch:42 step:32892 [D loss: 0.509771, acc.: 70.31%] [G loss: 8.725725]\n",
      "epoch:42 step:32893 [D loss: 1.507274, acc.: 55.47%] [G loss: 4.519173]\n",
      "epoch:42 step:32894 [D loss: 0.112266, acc.: 95.31%] [G loss: 4.592309]\n",
      "epoch:42 step:32895 [D loss: 0.018521, acc.: 100.00%] [G loss: 4.512519]\n",
      "epoch:42 step:32896 [D loss: 0.026456, acc.: 99.22%] [G loss: 5.201965]\n",
      "epoch:42 step:32897 [D loss: 0.018492, acc.: 99.22%] [G loss: 4.583479]\n",
      "epoch:42 step:32898 [D loss: 0.086699, acc.: 97.66%] [G loss: 4.212166]\n",
      "epoch:42 step:32899 [D loss: 0.029351, acc.: 100.00%] [G loss: 3.939738]\n",
      "epoch:42 step:32900 [D loss: 0.053185, acc.: 99.22%] [G loss: 3.660506]\n",
      "epoch:42 step:32901 [D loss: 0.194091, acc.: 94.53%] [G loss: 5.516900]\n",
      "epoch:42 step:32902 [D loss: 0.043999, acc.: 97.66%] [G loss: 5.839584]\n",
      "epoch:42 step:32903 [D loss: 0.032955, acc.: 98.44%] [G loss: 5.775075]\n",
      "epoch:42 step:32904 [D loss: 0.012622, acc.: 100.00%] [G loss: 4.817469]\n",
      "epoch:42 step:32905 [D loss: 0.037469, acc.: 100.00%] [G loss: 5.272320]\n",
      "epoch:42 step:32906 [D loss: 0.007400, acc.: 100.00%] [G loss: 3.959352]\n",
      "epoch:42 step:32907 [D loss: 0.024600, acc.: 100.00%] [G loss: 4.259375]\n",
      "epoch:42 step:32908 [D loss: 0.005471, acc.: 100.00%] [G loss: 4.469356]\n",
      "epoch:42 step:32909 [D loss: 0.102237, acc.: 96.88%] [G loss: 3.445335]\n",
      "epoch:42 step:32910 [D loss: 0.028437, acc.: 99.22%] [G loss: 4.356188]\n",
      "epoch:42 step:32911 [D loss: 0.006627, acc.: 100.00%] [G loss: 4.240156]\n",
      "epoch:42 step:32912 [D loss: 0.015702, acc.: 100.00%] [G loss: 4.434460]\n",
      "epoch:42 step:32913 [D loss: 0.014435, acc.: 100.00%] [G loss: 3.916891]\n",
      "epoch:42 step:32914 [D loss: 0.016323, acc.: 100.00%] [G loss: 4.036014]\n",
      "epoch:42 step:32915 [D loss: 0.011394, acc.: 100.00%] [G loss: 4.318052]\n",
      "epoch:42 step:32916 [D loss: 0.030704, acc.: 99.22%] [G loss: 4.994860]\n",
      "epoch:42 step:32917 [D loss: 0.026551, acc.: 100.00%] [G loss: 5.169933]\n",
      "epoch:42 step:32918 [D loss: 1.195502, acc.: 47.66%] [G loss: 8.159879]\n",
      "epoch:42 step:32919 [D loss: 1.208770, acc.: 55.47%] [G loss: 6.776405]\n",
      "epoch:42 step:32920 [D loss: 0.004484, acc.: 100.00%] [G loss: 6.382903]\n",
      "epoch:42 step:32921 [D loss: 0.029621, acc.: 99.22%] [G loss: 5.658082]\n",
      "epoch:42 step:32922 [D loss: 0.007856, acc.: 100.00%] [G loss: 4.931087]\n",
      "epoch:42 step:32923 [D loss: 0.029463, acc.: 99.22%] [G loss: 5.369706]\n",
      "epoch:42 step:32924 [D loss: 0.052490, acc.: 97.66%] [G loss: 4.489712]\n",
      "epoch:42 step:32925 [D loss: 0.021853, acc.: 100.00%] [G loss: 3.299476]\n",
      "epoch:42 step:32926 [D loss: 0.077011, acc.: 97.66%] [G loss: 4.546074]\n",
      "epoch:42 step:32927 [D loss: 0.031727, acc.: 99.22%] [G loss: 4.874053]\n",
      "epoch:42 step:32928 [D loss: 0.025943, acc.: 100.00%] [G loss: 4.444699]\n",
      "epoch:42 step:32929 [D loss: 0.082375, acc.: 96.88%] [G loss: 1.798294]\n",
      "epoch:42 step:32930 [D loss: 0.034389, acc.: 100.00%] [G loss: 2.211524]\n",
      "epoch:42 step:32931 [D loss: 0.046681, acc.: 100.00%] [G loss: 3.993359]\n",
      "epoch:42 step:32932 [D loss: 0.015259, acc.: 100.00%] [G loss: 3.464417]\n",
      "epoch:42 step:32933 [D loss: 0.032102, acc.: 100.00%] [G loss: 3.927046]\n",
      "epoch:42 step:32934 [D loss: 0.480186, acc.: 78.91%] [G loss: 5.273046]\n",
      "epoch:42 step:32935 [D loss: 0.009685, acc.: 100.00%] [G loss: 6.293046]\n",
      "epoch:42 step:32936 [D loss: 0.323097, acc.: 88.28%] [G loss: 4.980644]\n",
      "epoch:42 step:32937 [D loss: 0.141472, acc.: 93.75%] [G loss: 6.026705]\n",
      "epoch:42 step:32938 [D loss: 0.048361, acc.: 99.22%] [G loss: 6.382219]\n",
      "epoch:42 step:32939 [D loss: 0.024334, acc.: 99.22%] [G loss: 5.917151]\n",
      "epoch:42 step:32940 [D loss: 0.004944, acc.: 100.00%] [G loss: 5.454098]\n",
      "epoch:42 step:32941 [D loss: 0.006728, acc.: 100.00%] [G loss: 5.465246]\n",
      "epoch:42 step:32942 [D loss: 0.026443, acc.: 99.22%] [G loss: 4.257289]\n",
      "epoch:42 step:32943 [D loss: 0.011286, acc.: 100.00%] [G loss: 4.863939]\n",
      "epoch:42 step:32944 [D loss: 0.004735, acc.: 100.00%] [G loss: 4.519057]\n",
      "epoch:42 step:32945 [D loss: 0.022884, acc.: 99.22%] [G loss: 3.613632]\n",
      "epoch:42 step:32946 [D loss: 0.023803, acc.: 100.00%] [G loss: 4.048854]\n",
      "epoch:42 step:32947 [D loss: 0.011587, acc.: 100.00%] [G loss: 4.349323]\n",
      "epoch:42 step:32948 [D loss: 0.009690, acc.: 100.00%] [G loss: 4.198166]\n",
      "epoch:42 step:32949 [D loss: 0.021875, acc.: 100.00%] [G loss: 4.376965]\n",
      "epoch:42 step:32950 [D loss: 0.012191, acc.: 100.00%] [G loss: 4.512190]\n",
      "epoch:42 step:32951 [D loss: 0.013194, acc.: 100.00%] [G loss: 4.808658]\n",
      "epoch:42 step:32952 [D loss: 0.025651, acc.: 100.00%] [G loss: 5.064832]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:42 step:32953 [D loss: 0.010157, acc.: 100.00%] [G loss: 4.388496]\n",
      "epoch:42 step:32954 [D loss: 0.050850, acc.: 99.22%] [G loss: 2.991150]\n",
      "epoch:42 step:32955 [D loss: 0.117516, acc.: 96.09%] [G loss: 1.188313]\n",
      "epoch:42 step:32956 [D loss: 0.326573, acc.: 84.38%] [G loss: 6.914105]\n",
      "epoch:42 step:32957 [D loss: 0.333204, acc.: 84.38%] [G loss: 6.400181]\n",
      "epoch:42 step:32958 [D loss: 0.008728, acc.: 100.00%] [G loss: 5.926564]\n",
      "epoch:42 step:32959 [D loss: 0.060600, acc.: 98.44%] [G loss: 4.524057]\n",
      "epoch:42 step:32960 [D loss: 0.008255, acc.: 100.00%] [G loss: 4.114955]\n",
      "epoch:42 step:32961 [D loss: 0.025128, acc.: 100.00%] [G loss: 4.929028]\n",
      "epoch:42 step:32962 [D loss: 0.012027, acc.: 100.00%] [G loss: 4.856386]\n",
      "epoch:42 step:32963 [D loss: 0.052226, acc.: 98.44%] [G loss: 3.185656]\n",
      "epoch:42 step:32964 [D loss: 0.037264, acc.: 100.00%] [G loss: 4.075550]\n",
      "epoch:42 step:32965 [D loss: 0.005780, acc.: 100.00%] [G loss: 5.520340]\n",
      "epoch:42 step:32966 [D loss: 0.004259, acc.: 100.00%] [G loss: 5.373755]\n",
      "epoch:42 step:32967 [D loss: 0.013408, acc.: 100.00%] [G loss: 3.950246]\n",
      "epoch:42 step:32968 [D loss: 0.050058, acc.: 99.22%] [G loss: 3.880214]\n",
      "epoch:42 step:32969 [D loss: 0.010032, acc.: 100.00%] [G loss: 4.188000]\n",
      "epoch:42 step:32970 [D loss: 0.005652, acc.: 100.00%] [G loss: 4.872530]\n",
      "epoch:42 step:32971 [D loss: 0.069392, acc.: 100.00%] [G loss: 6.012428]\n",
      "epoch:42 step:32972 [D loss: 0.150376, acc.: 92.97%] [G loss: 3.121521]\n",
      "epoch:42 step:32973 [D loss: 0.111715, acc.: 99.22%] [G loss: 5.793370]\n",
      "epoch:42 step:32974 [D loss: 0.001128, acc.: 100.00%] [G loss: 7.069929]\n",
      "epoch:42 step:32975 [D loss: 0.104293, acc.: 94.53%] [G loss: 4.962858]\n",
      "epoch:42 step:32976 [D loss: 0.014079, acc.: 100.00%] [G loss: 4.002440]\n",
      "epoch:42 step:32977 [D loss: 0.025228, acc.: 100.00%] [G loss: 4.809040]\n",
      "epoch:42 step:32978 [D loss: 0.055607, acc.: 100.00%] [G loss: 4.936255]\n",
      "epoch:42 step:32979 [D loss: 0.011882, acc.: 100.00%] [G loss: 4.509475]\n",
      "epoch:42 step:32980 [D loss: 0.006392, acc.: 100.00%] [G loss: 5.148258]\n",
      "epoch:42 step:32981 [D loss: 0.039301, acc.: 99.22%] [G loss: 3.860525]\n",
      "epoch:42 step:32982 [D loss: 0.011184, acc.: 100.00%] [G loss: 4.222886]\n",
      "epoch:42 step:32983 [D loss: 0.009857, acc.: 100.00%] [G loss: 4.506883]\n",
      "epoch:42 step:32984 [D loss: 0.034777, acc.: 100.00%] [G loss: 4.500211]\n",
      "epoch:42 step:32985 [D loss: 0.007498, acc.: 100.00%] [G loss: 5.042844]\n",
      "epoch:42 step:32986 [D loss: 0.009522, acc.: 100.00%] [G loss: 4.670101]\n",
      "epoch:42 step:32987 [D loss: 0.012181, acc.: 100.00%] [G loss: 4.909729]\n",
      "epoch:42 step:32988 [D loss: 0.005527, acc.: 100.00%] [G loss: 4.731315]\n",
      "epoch:42 step:32989 [D loss: 0.149665, acc.: 96.09%] [G loss: 2.426099]\n",
      "epoch:42 step:32990 [D loss: 0.138688, acc.: 95.31%] [G loss: 6.717334]\n",
      "epoch:42 step:32991 [D loss: 0.076462, acc.: 96.88%] [G loss: 7.096746]\n",
      "epoch:42 step:32992 [D loss: 0.053940, acc.: 97.66%] [G loss: 4.789536]\n",
      "epoch:42 step:32993 [D loss: 0.008057, acc.: 100.00%] [G loss: 4.658804]\n",
      "epoch:42 step:32994 [D loss: 0.008522, acc.: 100.00%] [G loss: 3.809831]\n",
      "epoch:42 step:32995 [D loss: 0.033201, acc.: 100.00%] [G loss: 5.068499]\n",
      "epoch:42 step:32996 [D loss: 0.007414, acc.: 100.00%] [G loss: 5.413919]\n",
      "epoch:42 step:32997 [D loss: 0.003236, acc.: 100.00%] [G loss: 5.937131]\n",
      "epoch:42 step:32998 [D loss: 0.019658, acc.: 100.00%] [G loss: 4.237533]\n",
      "epoch:42 step:32999 [D loss: 0.007194, acc.: 100.00%] [G loss: 4.501463]\n",
      "epoch:42 step:33000 [D loss: 0.005359, acc.: 100.00%] [G loss: 4.668823]\n",
      "##############\n",
      "[1.00583577 1.08034588 2.10871601 1.01881317 2.10991776 0.91581104\n",
      " 2.10843723 2.11586828 1.1012746  1.11183142]\n",
      "##########\n",
      "epoch:42 step:33001 [D loss: 0.011382, acc.: 100.00%] [G loss: 6.021450]\n",
      "epoch:42 step:33002 [D loss: 0.017191, acc.: 100.00%] [G loss: 4.708693]\n",
      "epoch:42 step:33003 [D loss: 0.002956, acc.: 100.00%] [G loss: 4.942796]\n",
      "epoch:42 step:33004 [D loss: 0.029962, acc.: 100.00%] [G loss: 5.187593]\n",
      "epoch:42 step:33005 [D loss: 0.006686, acc.: 100.00%] [G loss: 5.288168]\n",
      "epoch:42 step:33006 [D loss: 0.018141, acc.: 100.00%] [G loss: 4.445732]\n",
      "epoch:42 step:33007 [D loss: 0.041899, acc.: 99.22%] [G loss: 4.381577]\n",
      "epoch:42 step:33008 [D loss: 0.009303, acc.: 100.00%] [G loss: 5.419832]\n",
      "epoch:42 step:33009 [D loss: 0.005508, acc.: 100.00%] [G loss: 4.587983]\n",
      "epoch:42 step:33010 [D loss: 0.006321, acc.: 100.00%] [G loss: 5.248396]\n",
      "epoch:42 step:33011 [D loss: 0.027053, acc.: 100.00%] [G loss: 4.009139]\n",
      "epoch:42 step:33012 [D loss: 0.017163, acc.: 100.00%] [G loss: 3.726569]\n",
      "epoch:42 step:33013 [D loss: 0.010353, acc.: 100.00%] [G loss: 4.008569]\n",
      "epoch:42 step:33014 [D loss: 0.034430, acc.: 100.00%] [G loss: 4.456799]\n",
      "epoch:42 step:33015 [D loss: 0.297191, acc.: 87.50%] [G loss: 7.607176]\n",
      "epoch:42 step:33016 [D loss: 2.085003, acc.: 32.81%] [G loss: 5.019055]\n",
      "epoch:42 step:33017 [D loss: 0.141695, acc.: 92.97%] [G loss: 7.433941]\n",
      "epoch:42 step:33018 [D loss: 0.250656, acc.: 89.84%] [G loss: 5.558444]\n",
      "epoch:42 step:33019 [D loss: 0.039559, acc.: 99.22%] [G loss: 4.812887]\n",
      "epoch:42 step:33020 [D loss: 0.105558, acc.: 97.66%] [G loss: 5.399454]\n",
      "epoch:42 step:33021 [D loss: 0.009867, acc.: 100.00%] [G loss: 4.060332]\n",
      "epoch:42 step:33022 [D loss: 0.005346, acc.: 100.00%] [G loss: 4.937686]\n",
      "epoch:42 step:33023 [D loss: 0.007341, acc.: 100.00%] [G loss: 3.804801]\n",
      "epoch:42 step:33024 [D loss: 0.095470, acc.: 99.22%] [G loss: 5.282540]\n",
      "epoch:42 step:33025 [D loss: 0.020518, acc.: 100.00%] [G loss: 6.266308]\n",
      "epoch:42 step:33026 [D loss: 0.079883, acc.: 96.88%] [G loss: 4.921016]\n",
      "epoch:42 step:33027 [D loss: 0.026438, acc.: 99.22%] [G loss: 4.384877]\n",
      "epoch:42 step:33028 [D loss: 0.019476, acc.: 100.00%] [G loss: 3.925036]\n",
      "epoch:42 step:33029 [D loss: 0.017777, acc.: 100.00%] [G loss: 5.033208]\n",
      "epoch:42 step:33030 [D loss: 0.015669, acc.: 100.00%] [G loss: 5.443124]\n",
      "epoch:42 step:33031 [D loss: 0.550979, acc.: 75.78%] [G loss: 7.844150]\n",
      "epoch:42 step:33032 [D loss: 0.020621, acc.: 100.00%] [G loss: 9.146423]\n",
      "epoch:42 step:33033 [D loss: 1.582533, acc.: 50.00%] [G loss: 1.003665]\n",
      "epoch:42 step:33034 [D loss: 0.574244, acc.: 69.53%] [G loss: 7.892451]\n",
      "epoch:42 step:33035 [D loss: 0.187199, acc.: 89.84%] [G loss: 8.447668]\n",
      "epoch:42 step:33036 [D loss: 0.288969, acc.: 85.94%] [G loss: 6.003101]\n",
      "epoch:42 step:33037 [D loss: 0.001507, acc.: 100.00%] [G loss: 5.386302]\n",
      "epoch:42 step:33038 [D loss: 0.009517, acc.: 100.00%] [G loss: 4.782869]\n",
      "epoch:42 step:33039 [D loss: 0.101897, acc.: 96.09%] [G loss: 4.858429]\n",
      "epoch:42 step:33040 [D loss: 0.004837, acc.: 100.00%] [G loss: 5.450487]\n",
      "epoch:42 step:33041 [D loss: 0.011903, acc.: 100.00%] [G loss: 4.920501]\n",
      "epoch:42 step:33042 [D loss: 0.017634, acc.: 100.00%] [G loss: 3.904122]\n",
      "epoch:42 step:33043 [D loss: 0.070949, acc.: 99.22%] [G loss: 5.511476]\n",
      "epoch:42 step:33044 [D loss: 0.017108, acc.: 100.00%] [G loss: 5.533980]\n",
      "epoch:42 step:33045 [D loss: 0.060356, acc.: 98.44%] [G loss: 5.976119]\n",
      "epoch:42 step:33046 [D loss: 0.008049, acc.: 100.00%] [G loss: 5.805915]\n",
      "epoch:42 step:33047 [D loss: 0.061696, acc.: 99.22%] [G loss: 3.683473]\n",
      "epoch:42 step:33048 [D loss: 0.075556, acc.: 98.44%] [G loss: 5.370172]\n",
      "epoch:42 step:33049 [D loss: 0.284540, acc.: 91.41%] [G loss: 5.827311]\n",
      "epoch:42 step:33050 [D loss: 0.004234, acc.: 100.00%] [G loss: 7.150332]\n",
      "epoch:42 step:33051 [D loss: 0.025495, acc.: 99.22%] [G loss: 5.911370]\n",
      "epoch:42 step:33052 [D loss: 0.080957, acc.: 96.88%] [G loss: 3.712277]\n",
      "epoch:42 step:33053 [D loss: 0.039050, acc.: 99.22%] [G loss: 2.608904]\n",
      "epoch:42 step:33054 [D loss: 0.039959, acc.: 99.22%] [G loss: 5.390012]\n",
      "epoch:42 step:33055 [D loss: 0.007831, acc.: 100.00%] [G loss: 6.046347]\n",
      "epoch:42 step:33056 [D loss: 0.007501, acc.: 100.00%] [G loss: 5.905098]\n",
      "epoch:42 step:33057 [D loss: 0.010808, acc.: 100.00%] [G loss: 5.704914]\n",
      "epoch:42 step:33058 [D loss: 0.039147, acc.: 98.44%] [G loss: 3.586483]\n",
      "epoch:42 step:33059 [D loss: 0.007339, acc.: 100.00%] [G loss: 3.579029]\n",
      "epoch:42 step:33060 [D loss: 0.079437, acc.: 98.44%] [G loss: 5.730110]\n",
      "epoch:42 step:33061 [D loss: 0.013134, acc.: 100.00%] [G loss: 6.773872]\n",
      "epoch:42 step:33062 [D loss: 1.108726, acc.: 47.66%] [G loss: 9.034324]\n",
      "epoch:42 step:33063 [D loss: 0.193513, acc.: 89.06%] [G loss: 7.720516]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:42 step:33064 [D loss: 0.070819, acc.: 96.09%] [G loss: 7.178909]\n",
      "epoch:42 step:33065 [D loss: 0.006505, acc.: 100.00%] [G loss: 7.142757]\n",
      "epoch:42 step:33066 [D loss: 0.112065, acc.: 96.88%] [G loss: 5.298057]\n",
      "epoch:42 step:33067 [D loss: 0.023962, acc.: 100.00%] [G loss: 3.415492]\n",
      "epoch:42 step:33068 [D loss: 0.029513, acc.: 100.00%] [G loss: 3.597394]\n",
      "epoch:42 step:33069 [D loss: 0.065730, acc.: 98.44%] [G loss: 2.580161]\n",
      "epoch:42 step:33070 [D loss: 0.221097, acc.: 91.41%] [G loss: 6.491308]\n",
      "epoch:42 step:33071 [D loss: 0.485041, acc.: 79.69%] [G loss: 4.069663]\n",
      "epoch:42 step:33072 [D loss: 0.044290, acc.: 100.00%] [G loss: 3.922413]\n",
      "epoch:42 step:33073 [D loss: 0.029482, acc.: 100.00%] [G loss: 2.920152]\n",
      "epoch:42 step:33074 [D loss: 0.008675, acc.: 100.00%] [G loss: 5.520885]\n",
      "epoch:42 step:33075 [D loss: 0.039246, acc.: 99.22%] [G loss: 5.695004]\n",
      "epoch:42 step:33076 [D loss: 0.087734, acc.: 96.88%] [G loss: 5.985256]\n",
      "epoch:42 step:33077 [D loss: 0.019431, acc.: 99.22%] [G loss: 6.957542]\n",
      "epoch:42 step:33078 [D loss: 0.031319, acc.: 98.44%] [G loss: 5.496718]\n",
      "epoch:42 step:33079 [D loss: 0.028197, acc.: 100.00%] [G loss: 3.853537]\n",
      "epoch:42 step:33080 [D loss: 0.173895, acc.: 93.75%] [G loss: 5.024381]\n",
      "epoch:42 step:33081 [D loss: 0.007121, acc.: 100.00%] [G loss: 6.764344]\n",
      "epoch:42 step:33082 [D loss: 0.342102, acc.: 88.28%] [G loss: 1.331284]\n",
      "epoch:42 step:33083 [D loss: 0.102592, acc.: 98.44%] [G loss: 2.538361]\n",
      "epoch:42 step:33084 [D loss: 0.012696, acc.: 100.00%] [G loss: 4.693934]\n",
      "epoch:42 step:33085 [D loss: 0.008342, acc.: 100.00%] [G loss: 3.266135]\n",
      "epoch:42 step:33086 [D loss: 0.004848, acc.: 100.00%] [G loss: 2.942836]\n",
      "epoch:42 step:33087 [D loss: 0.007289, acc.: 100.00%] [G loss: 3.463942]\n",
      "epoch:42 step:33088 [D loss: 0.069263, acc.: 98.44%] [G loss: 3.386495]\n",
      "epoch:42 step:33089 [D loss: 0.054355, acc.: 97.66%] [G loss: 4.327232]\n",
      "epoch:42 step:33090 [D loss: 0.007479, acc.: 100.00%] [G loss: 6.044716]\n",
      "epoch:42 step:33091 [D loss: 0.084705, acc.: 96.88%] [G loss: 2.920520]\n",
      "epoch:42 step:33092 [D loss: 0.051879, acc.: 100.00%] [G loss: 4.299718]\n",
      "epoch:42 step:33093 [D loss: 0.030834, acc.: 100.00%] [G loss: 3.855672]\n",
      "epoch:42 step:33094 [D loss: 0.026514, acc.: 99.22%] [G loss: 3.141007]\n",
      "epoch:42 step:33095 [D loss: 0.029387, acc.: 99.22%] [G loss: 3.067066]\n",
      "epoch:42 step:33096 [D loss: 0.160973, acc.: 95.31%] [G loss: 6.156532]\n",
      "epoch:42 step:33097 [D loss: 0.128586, acc.: 95.31%] [G loss: 3.919271]\n",
      "epoch:42 step:33098 [D loss: 0.058320, acc.: 99.22%] [G loss: 4.770423]\n",
      "epoch:42 step:33099 [D loss: 0.224987, acc.: 87.50%] [G loss: 5.556558]\n",
      "epoch:42 step:33100 [D loss: 0.169160, acc.: 94.53%] [G loss: 2.957975]\n",
      "epoch:42 step:33101 [D loss: 0.054559, acc.: 99.22%] [G loss: 3.556768]\n",
      "epoch:42 step:33102 [D loss: 0.018313, acc.: 100.00%] [G loss: 6.865726]\n",
      "epoch:42 step:33103 [D loss: 0.002124, acc.: 100.00%] [G loss: 6.238120]\n",
      "epoch:42 step:33104 [D loss: 0.011495, acc.: 100.00%] [G loss: 6.405112]\n",
      "epoch:42 step:33105 [D loss: 0.041251, acc.: 98.44%] [G loss: 5.497427]\n",
      "epoch:42 step:33106 [D loss: 0.005591, acc.: 100.00%] [G loss: 3.638664]\n",
      "epoch:42 step:33107 [D loss: 0.034299, acc.: 99.22%] [G loss: 4.899094]\n",
      "epoch:42 step:33108 [D loss: 0.010789, acc.: 100.00%] [G loss: 6.318506]\n",
      "epoch:42 step:33109 [D loss: 0.012162, acc.: 100.00%] [G loss: 5.177699]\n",
      "epoch:42 step:33110 [D loss: 0.002627, acc.: 100.00%] [G loss: 5.740897]\n",
      "epoch:42 step:33111 [D loss: 0.023323, acc.: 99.22%] [G loss: 5.357535]\n",
      "epoch:42 step:33112 [D loss: 0.046979, acc.: 99.22%] [G loss: 3.870008]\n",
      "epoch:42 step:33113 [D loss: 0.034662, acc.: 100.00%] [G loss: 5.532611]\n",
      "epoch:42 step:33114 [D loss: 0.062646, acc.: 99.22%] [G loss: 5.077070]\n",
      "epoch:42 step:33115 [D loss: 0.002637, acc.: 100.00%] [G loss: 5.910811]\n",
      "epoch:42 step:33116 [D loss: 0.021026, acc.: 100.00%] [G loss: 5.079632]\n",
      "epoch:42 step:33117 [D loss: 0.017840, acc.: 100.00%] [G loss: 4.691332]\n",
      "epoch:42 step:33118 [D loss: 0.039865, acc.: 98.44%] [G loss: 5.429753]\n",
      "epoch:42 step:33119 [D loss: 0.001775, acc.: 100.00%] [G loss: 5.753082]\n",
      "epoch:42 step:33120 [D loss: 0.012520, acc.: 100.00%] [G loss: 5.681045]\n",
      "epoch:42 step:33121 [D loss: 0.006102, acc.: 100.00%] [G loss: 5.843964]\n",
      "epoch:42 step:33122 [D loss: 0.204559, acc.: 92.19%] [G loss: 6.129035]\n",
      "epoch:42 step:33123 [D loss: 0.004032, acc.: 100.00%] [G loss: 7.628370]\n",
      "epoch:42 step:33124 [D loss: 0.000555, acc.: 100.00%] [G loss: 7.482314]\n",
      "epoch:42 step:33125 [D loss: 0.004664, acc.: 100.00%] [G loss: 7.894632]\n",
      "epoch:42 step:33126 [D loss: 0.099428, acc.: 96.09%] [G loss: 3.819886]\n",
      "epoch:42 step:33127 [D loss: 0.047071, acc.: 100.00%] [G loss: 4.954960]\n",
      "epoch:42 step:33128 [D loss: 0.008016, acc.: 100.00%] [G loss: 6.842213]\n",
      "epoch:42 step:33129 [D loss: 0.004595, acc.: 100.00%] [G loss: 6.445114]\n",
      "epoch:42 step:33130 [D loss: 0.002219, acc.: 100.00%] [G loss: 6.062960]\n",
      "epoch:42 step:33131 [D loss: 0.002289, acc.: 100.00%] [G loss: 5.447530]\n",
      "epoch:42 step:33132 [D loss: 0.046611, acc.: 98.44%] [G loss: 6.370241]\n",
      "epoch:42 step:33133 [D loss: 0.008232, acc.: 100.00%] [G loss: 6.407006]\n",
      "epoch:42 step:33134 [D loss: 0.005597, acc.: 100.00%] [G loss: 6.645877]\n",
      "epoch:42 step:33135 [D loss: 0.000789, acc.: 100.00%] [G loss: 6.561090]\n",
      "epoch:42 step:33136 [D loss: 0.014590, acc.: 99.22%] [G loss: 4.412865]\n",
      "epoch:42 step:33137 [D loss: 0.001036, acc.: 100.00%] [G loss: 5.388102]\n",
      "epoch:42 step:33138 [D loss: 0.046458, acc.: 98.44%] [G loss: 3.307913]\n",
      "epoch:42 step:33139 [D loss: 0.120278, acc.: 95.31%] [G loss: 8.124516]\n",
      "epoch:42 step:33140 [D loss: 0.031975, acc.: 99.22%] [G loss: 7.685340]\n",
      "epoch:42 step:33141 [D loss: 0.000964, acc.: 100.00%] [G loss: 7.084382]\n",
      "epoch:42 step:33142 [D loss: 0.043705, acc.: 98.44%] [G loss: 7.016986]\n",
      "epoch:42 step:33143 [D loss: 0.001399, acc.: 100.00%] [G loss: 7.189629]\n",
      "epoch:42 step:33144 [D loss: 0.001036, acc.: 100.00%] [G loss: 5.754029]\n",
      "epoch:42 step:33145 [D loss: 0.010933, acc.: 100.00%] [G loss: 5.747061]\n",
      "epoch:42 step:33146 [D loss: 0.002886, acc.: 100.00%] [G loss: 4.922956]\n",
      "epoch:42 step:33147 [D loss: 0.003720, acc.: 100.00%] [G loss: 5.097355]\n",
      "epoch:42 step:33148 [D loss: 0.013256, acc.: 100.00%] [G loss: 4.405082]\n",
      "epoch:42 step:33149 [D loss: 0.003073, acc.: 100.00%] [G loss: 4.468764]\n",
      "epoch:42 step:33150 [D loss: 0.001347, acc.: 100.00%] [G loss: 4.373403]\n",
      "epoch:42 step:33151 [D loss: 0.011471, acc.: 100.00%] [G loss: 4.619655]\n",
      "epoch:42 step:33152 [D loss: 0.108753, acc.: 96.88%] [G loss: 5.609287]\n",
      "epoch:42 step:33153 [D loss: 0.000865, acc.: 100.00%] [G loss: 8.206926]\n",
      "epoch:42 step:33154 [D loss: 0.002059, acc.: 100.00%] [G loss: 7.940948]\n",
      "epoch:42 step:33155 [D loss: 0.014818, acc.: 100.00%] [G loss: 7.269677]\n",
      "epoch:42 step:33156 [D loss: 0.001876, acc.: 100.00%] [G loss: 6.852425]\n",
      "epoch:42 step:33157 [D loss: 0.014108, acc.: 99.22%] [G loss: 6.471619]\n",
      "epoch:42 step:33158 [D loss: 0.000705, acc.: 100.00%] [G loss: 5.810827]\n",
      "epoch:42 step:33159 [D loss: 0.017297, acc.: 100.00%] [G loss: 7.392243]\n",
      "epoch:42 step:33160 [D loss: 0.005827, acc.: 100.00%] [G loss: 6.846782]\n",
      "epoch:42 step:33161 [D loss: 0.002426, acc.: 100.00%] [G loss: 5.358416]\n",
      "epoch:42 step:33162 [D loss: 0.009829, acc.: 100.00%] [G loss: 7.453951]\n",
      "epoch:42 step:33163 [D loss: 0.004372, acc.: 100.00%] [G loss: 6.137674]\n",
      "epoch:42 step:33164 [D loss: 0.003803, acc.: 100.00%] [G loss: 5.381989]\n",
      "epoch:42 step:33165 [D loss: 0.002778, acc.: 100.00%] [G loss: 5.831936]\n",
      "epoch:42 step:33166 [D loss: 0.023332, acc.: 100.00%] [G loss: 5.566540]\n",
      "epoch:42 step:33167 [D loss: 0.006869, acc.: 100.00%] [G loss: 4.412513]\n",
      "epoch:42 step:33168 [D loss: 0.022348, acc.: 100.00%] [G loss: 6.034466]\n",
      "epoch:42 step:33169 [D loss: 0.000833, acc.: 100.00%] [G loss: 6.277339]\n",
      "epoch:42 step:33170 [D loss: 0.005220, acc.: 100.00%] [G loss: 6.265308]\n",
      "epoch:42 step:33171 [D loss: 0.003243, acc.: 100.00%] [G loss: 5.997772]\n",
      "epoch:42 step:33172 [D loss: 0.005554, acc.: 100.00%] [G loss: 5.399096]\n",
      "epoch:42 step:33173 [D loss: 0.030728, acc.: 99.22%] [G loss: 5.311015]\n",
      "epoch:42 step:33174 [D loss: 0.005323, acc.: 100.00%] [G loss: 6.893075]\n",
      "epoch:42 step:33175 [D loss: 0.009555, acc.: 100.00%] [G loss: 6.346624]\n",
      "epoch:42 step:33176 [D loss: 0.047567, acc.: 99.22%] [G loss: 6.074471]\n",
      "epoch:42 step:33177 [D loss: 0.014975, acc.: 99.22%] [G loss: 5.779882]\n",
      "epoch:42 step:33178 [D loss: 0.012288, acc.: 100.00%] [G loss: 4.872486]\n",
      "epoch:42 step:33179 [D loss: 0.001221, acc.: 100.00%] [G loss: 5.706682]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:42 step:33180 [D loss: 0.001215, acc.: 100.00%] [G loss: 4.679162]\n",
      "epoch:42 step:33181 [D loss: 0.011162, acc.: 100.00%] [G loss: 4.496834]\n",
      "epoch:42 step:33182 [D loss: 0.001242, acc.: 100.00%] [G loss: 5.585739]\n",
      "epoch:42 step:33183 [D loss: 0.020455, acc.: 100.00%] [G loss: 8.477680]\n",
      "epoch:42 step:33184 [D loss: 0.008221, acc.: 100.00%] [G loss: 7.093045]\n",
      "epoch:42 step:33185 [D loss: 0.001338, acc.: 100.00%] [G loss: 5.917503]\n",
      "epoch:42 step:33186 [D loss: 0.004943, acc.: 100.00%] [G loss: 5.312174]\n",
      "epoch:42 step:33187 [D loss: 0.061178, acc.: 98.44%] [G loss: 8.854064]\n",
      "epoch:42 step:33188 [D loss: 0.024952, acc.: 99.22%] [G loss: 10.532490]\n",
      "epoch:42 step:33189 [D loss: 0.000418, acc.: 100.00%] [G loss: 9.791506]\n",
      "epoch:42 step:33190 [D loss: 0.000150, acc.: 100.00%] [G loss: 9.354296]\n",
      "epoch:42 step:33191 [D loss: 0.000527, acc.: 100.00%] [G loss: 8.475101]\n",
      "epoch:42 step:33192 [D loss: 0.002175, acc.: 100.00%] [G loss: 8.330249]\n",
      "epoch:42 step:33193 [D loss: 0.001787, acc.: 100.00%] [G loss: 7.417998]\n",
      "epoch:42 step:33194 [D loss: 0.003155, acc.: 100.00%] [G loss: 8.116357]\n",
      "epoch:42 step:33195 [D loss: 0.001896, acc.: 100.00%] [G loss: 9.160662]\n",
      "epoch:42 step:33196 [D loss: 0.003035, acc.: 100.00%] [G loss: 6.551127]\n",
      "epoch:42 step:33197 [D loss: 0.002676, acc.: 100.00%] [G loss: 6.808881]\n",
      "epoch:42 step:33198 [D loss: 0.005194, acc.: 100.00%] [G loss: 4.202440]\n",
      "epoch:42 step:33199 [D loss: 0.021506, acc.: 100.00%] [G loss: 7.128215]\n",
      "epoch:42 step:33200 [D loss: 0.000108, acc.: 100.00%] [G loss: 8.067814]\n",
      "##############\n",
      "[1.07549828 1.04437422 1.10649157 0.98851071 1.11859692 1.01845652\n",
      " 1.05492823 2.10010994 1.07848389 1.0494499 ]\n",
      "##########\n",
      "epoch:42 step:33201 [D loss: 0.001763, acc.: 100.00%] [G loss: 8.455055]\n",
      "epoch:42 step:33202 [D loss: 0.000244, acc.: 100.00%] [G loss: 7.892914]\n",
      "epoch:42 step:33203 [D loss: 1.893782, acc.: 37.50%] [G loss: 14.257539]\n",
      "epoch:42 step:33204 [D loss: 0.000265, acc.: 100.00%] [G loss: 15.729467]\n",
      "epoch:42 step:33205 [D loss: 0.406592, acc.: 86.72%] [G loss: 14.204988]\n",
      "epoch:42 step:33206 [D loss: 0.000016, acc.: 100.00%] [G loss: 12.957750]\n",
      "epoch:42 step:33207 [D loss: 0.000029, acc.: 100.00%] [G loss: 11.786233]\n",
      "epoch:42 step:33208 [D loss: 0.000999, acc.: 100.00%] [G loss: 10.196602]\n",
      "epoch:42 step:33209 [D loss: 0.002626, acc.: 100.00%] [G loss: 8.348810]\n",
      "epoch:42 step:33210 [D loss: 0.008394, acc.: 100.00%] [G loss: 7.050117]\n",
      "epoch:42 step:33211 [D loss: 0.032727, acc.: 98.44%] [G loss: 6.327441]\n",
      "epoch:42 step:33212 [D loss: 0.008365, acc.: 100.00%] [G loss: 4.361889]\n",
      "epoch:42 step:33213 [D loss: 0.018292, acc.: 100.00%] [G loss: 5.663771]\n",
      "epoch:42 step:33214 [D loss: 0.052852, acc.: 96.88%] [G loss: 3.940684]\n",
      "epoch:42 step:33215 [D loss: 0.003662, acc.: 100.00%] [G loss: 2.827764]\n",
      "epoch:42 step:33216 [D loss: 0.001044, acc.: 100.00%] [G loss: 1.832047]\n",
      "epoch:42 step:33217 [D loss: 0.032431, acc.: 98.44%] [G loss: 2.224706]\n",
      "epoch:42 step:33218 [D loss: 0.005539, acc.: 100.00%] [G loss: 0.851072]\n",
      "epoch:42 step:33219 [D loss: 0.006744, acc.: 100.00%] [G loss: 1.057484]\n",
      "epoch:42 step:33220 [D loss: 0.001186, acc.: 100.00%] [G loss: 1.106583]\n",
      "epoch:42 step:33221 [D loss: 0.087845, acc.: 97.66%] [G loss: 5.934043]\n",
      "epoch:42 step:33222 [D loss: 1.099052, acc.: 53.91%] [G loss: 10.987158]\n",
      "epoch:42 step:33223 [D loss: 0.868209, acc.: 65.62%] [G loss: 4.162534]\n",
      "epoch:42 step:33224 [D loss: 0.010487, acc.: 100.00%] [G loss: 1.454977]\n",
      "epoch:42 step:33225 [D loss: 0.332019, acc.: 88.28%] [G loss: 7.775145]\n",
      "epoch:42 step:33226 [D loss: 0.003773, acc.: 100.00%] [G loss: 9.476092]\n",
      "epoch:42 step:33227 [D loss: 0.041032, acc.: 98.44%] [G loss: 7.336064]\n",
      "epoch:42 step:33228 [D loss: 0.004488, acc.: 100.00%] [G loss: 5.946912]\n",
      "epoch:42 step:33229 [D loss: 0.005684, acc.: 100.00%] [G loss: 5.597979]\n",
      "epoch:42 step:33230 [D loss: 0.024376, acc.: 100.00%] [G loss: 3.351739]\n",
      "epoch:42 step:33231 [D loss: 0.018457, acc.: 100.00%] [G loss: 2.227605]\n",
      "epoch:42 step:33232 [D loss: 0.079348, acc.: 97.66%] [G loss: 5.336051]\n",
      "epoch:42 step:33233 [D loss: 2.876498, acc.: 21.09%] [G loss: 9.012433]\n",
      "epoch:42 step:33234 [D loss: 0.347246, acc.: 92.19%] [G loss: 9.972517]\n",
      "epoch:42 step:33235 [D loss: 0.654464, acc.: 76.56%] [G loss: 4.057618]\n",
      "epoch:42 step:33236 [D loss: 0.014589, acc.: 100.00%] [G loss: 0.993274]\n",
      "epoch:42 step:33237 [D loss: 0.131027, acc.: 92.97%] [G loss: 4.492038]\n",
      "epoch:42 step:33238 [D loss: 0.009932, acc.: 100.00%] [G loss: 4.711598]\n",
      "epoch:42 step:33239 [D loss: 0.002968, acc.: 100.00%] [G loss: 4.894320]\n",
      "epoch:42 step:33240 [D loss: 0.446153, acc.: 82.03%] [G loss: 4.626528]\n",
      "epoch:42 step:33241 [D loss: 0.009956, acc.: 100.00%] [G loss: 6.051810]\n",
      "epoch:42 step:33242 [D loss: 0.391121, acc.: 82.81%] [G loss: 1.841806]\n",
      "epoch:42 step:33243 [D loss: 0.025838, acc.: 99.22%] [G loss: 1.129809]\n",
      "epoch:42 step:33244 [D loss: 0.142284, acc.: 90.62%] [G loss: 4.091752]\n",
      "epoch:42 step:33245 [D loss: 0.007130, acc.: 100.00%] [G loss: 5.748414]\n",
      "epoch:42 step:33246 [D loss: 0.154347, acc.: 91.41%] [G loss: 1.333952]\n",
      "epoch:42 step:33247 [D loss: 0.127760, acc.: 97.66%] [G loss: 4.173274]\n",
      "epoch:42 step:33248 [D loss: 0.015189, acc.: 100.00%] [G loss: 4.912622]\n",
      "epoch:42 step:33249 [D loss: 0.005011, acc.: 100.00%] [G loss: 5.514092]\n",
      "epoch:42 step:33250 [D loss: 0.182326, acc.: 92.19%] [G loss: 1.548080]\n",
      "epoch:42 step:33251 [D loss: 0.855915, acc.: 60.16%] [G loss: 9.694382]\n",
      "epoch:42 step:33252 [D loss: 0.577783, acc.: 71.09%] [G loss: 8.953129]\n",
      "epoch:42 step:33253 [D loss: 0.019164, acc.: 100.00%] [G loss: 7.878274]\n",
      "epoch:42 step:33254 [D loss: 0.011561, acc.: 100.00%] [G loss: 7.655175]\n",
      "epoch:42 step:33255 [D loss: 0.019628, acc.: 99.22%] [G loss: 6.783087]\n",
      "epoch:42 step:33256 [D loss: 0.011134, acc.: 100.00%] [G loss: 5.955703]\n",
      "epoch:42 step:33257 [D loss: 0.011349, acc.: 100.00%] [G loss: 5.377029]\n",
      "epoch:42 step:33258 [D loss: 0.005504, acc.: 100.00%] [G loss: 5.713866]\n",
      "epoch:42 step:33259 [D loss: 0.010531, acc.: 100.00%] [G loss: 4.895444]\n",
      "epoch:42 step:33260 [D loss: 0.049376, acc.: 99.22%] [G loss: 5.195626]\n",
      "epoch:42 step:33261 [D loss: 0.065792, acc.: 97.66%] [G loss: 3.544048]\n",
      "epoch:42 step:33262 [D loss: 0.022865, acc.: 100.00%] [G loss: 2.152292]\n",
      "epoch:42 step:33263 [D loss: 0.051425, acc.: 99.22%] [G loss: 3.154967]\n",
      "epoch:42 step:33264 [D loss: 0.009649, acc.: 100.00%] [G loss: 3.829680]\n",
      "epoch:42 step:33265 [D loss: 0.014717, acc.: 100.00%] [G loss: 4.996491]\n",
      "epoch:42 step:33266 [D loss: 0.064363, acc.: 97.66%] [G loss: 1.850971]\n",
      "epoch:42 step:33267 [D loss: 0.042591, acc.: 99.22%] [G loss: 3.468762]\n",
      "epoch:42 step:33268 [D loss: 0.031812, acc.: 99.22%] [G loss: 3.181530]\n",
      "epoch:42 step:33269 [D loss: 0.079417, acc.: 97.66%] [G loss: 0.532361]\n",
      "epoch:42 step:33270 [D loss: 0.204838, acc.: 92.97%] [G loss: 5.227984]\n",
      "epoch:42 step:33271 [D loss: 0.003864, acc.: 100.00%] [G loss: 7.665057]\n",
      "epoch:42 step:33272 [D loss: 0.491625, acc.: 80.47%] [G loss: 0.583345]\n",
      "epoch:42 step:33273 [D loss: 0.663870, acc.: 69.53%] [G loss: 9.661961]\n",
      "epoch:42 step:33274 [D loss: 0.240333, acc.: 89.06%] [G loss: 10.660107]\n",
      "epoch:42 step:33275 [D loss: 0.232383, acc.: 89.84%] [G loss: 8.583021]\n",
      "epoch:42 step:33276 [D loss: 0.013733, acc.: 100.00%] [G loss: 6.754591]\n",
      "epoch:42 step:33277 [D loss: 0.001021, acc.: 100.00%] [G loss: 5.316057]\n",
      "epoch:42 step:33278 [D loss: 0.005084, acc.: 100.00%] [G loss: 4.422175]\n",
      "epoch:42 step:33279 [D loss: 0.022243, acc.: 99.22%] [G loss: 5.595159]\n",
      "epoch:42 step:33280 [D loss: 0.023176, acc.: 99.22%] [G loss: 6.536803]\n",
      "epoch:42 step:33281 [D loss: 0.011368, acc.: 100.00%] [G loss: 5.677663]\n",
      "epoch:42 step:33282 [D loss: 0.003970, acc.: 100.00%] [G loss: 5.377955]\n",
      "epoch:42 step:33283 [D loss: 0.015910, acc.: 100.00%] [G loss: 4.891132]\n",
      "epoch:42 step:33284 [D loss: 0.005136, acc.: 100.00%] [G loss: 6.204275]\n",
      "epoch:42 step:33285 [D loss: 0.007664, acc.: 100.00%] [G loss: 5.003308]\n",
      "epoch:42 step:33286 [D loss: 0.027174, acc.: 100.00%] [G loss: 5.369649]\n",
      "epoch:42 step:33287 [D loss: 0.037058, acc.: 99.22%] [G loss: 5.446620]\n",
      "epoch:42 step:33288 [D loss: 0.039402, acc.: 100.00%] [G loss: 5.249311]\n",
      "epoch:42 step:33289 [D loss: 0.026484, acc.: 99.22%] [G loss: 4.634896]\n",
      "epoch:42 step:33290 [D loss: 0.024087, acc.: 100.00%] [G loss: 5.001990]\n",
      "epoch:42 step:33291 [D loss: 0.008507, acc.: 100.00%] [G loss: 4.638973]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:42 step:33292 [D loss: 0.017898, acc.: 100.00%] [G loss: 4.042521]\n",
      "epoch:42 step:33293 [D loss: 0.082378, acc.: 97.66%] [G loss: 3.774512]\n",
      "epoch:42 step:33294 [D loss: 0.089329, acc.: 99.22%] [G loss: 4.310742]\n",
      "epoch:42 step:33295 [D loss: 0.007277, acc.: 100.00%] [G loss: 3.926960]\n",
      "epoch:42 step:33296 [D loss: 0.018326, acc.: 100.00%] [G loss: 6.112427]\n",
      "epoch:42 step:33297 [D loss: 0.065964, acc.: 98.44%] [G loss: 4.354009]\n",
      "epoch:42 step:33298 [D loss: 0.006864, acc.: 100.00%] [G loss: 3.957690]\n",
      "epoch:42 step:33299 [D loss: 0.064336, acc.: 98.44%] [G loss: 5.454288]\n",
      "epoch:42 step:33300 [D loss: 0.086656, acc.: 96.88%] [G loss: 3.527959]\n",
      "epoch:42 step:33301 [D loss: 0.144297, acc.: 94.53%] [G loss: 6.232723]\n",
      "epoch:42 step:33302 [D loss: 0.015457, acc.: 99.22%] [G loss: 7.302167]\n",
      "epoch:42 step:33303 [D loss: 0.477693, acc.: 81.25%] [G loss: 0.125942]\n",
      "epoch:42 step:33304 [D loss: 2.831418, acc.: 50.78%] [G loss: 11.506372]\n",
      "epoch:42 step:33305 [D loss: 2.708931, acc.: 50.00%] [G loss: 7.705815]\n",
      "epoch:42 step:33306 [D loss: 0.351322, acc.: 81.25%] [G loss: 3.979441]\n",
      "epoch:42 step:33307 [D loss: 0.236878, acc.: 93.75%] [G loss: 4.828985]\n",
      "epoch:42 step:33308 [D loss: 0.025238, acc.: 99.22%] [G loss: 5.148997]\n",
      "epoch:42 step:33309 [D loss: 0.079940, acc.: 98.44%] [G loss: 5.244899]\n",
      "epoch:42 step:33310 [D loss: 0.093680, acc.: 96.88%] [G loss: 3.495380]\n",
      "epoch:42 step:33311 [D loss: 0.044357, acc.: 100.00%] [G loss: 3.764534]\n",
      "epoch:42 step:33312 [D loss: 0.030467, acc.: 100.00%] [G loss: 4.237239]\n",
      "epoch:42 step:33313 [D loss: 0.045152, acc.: 99.22%] [G loss: 3.600420]\n",
      "epoch:42 step:33314 [D loss: 0.164514, acc.: 92.97%] [G loss: 5.033004]\n",
      "epoch:42 step:33315 [D loss: 0.061820, acc.: 99.22%] [G loss: 6.198627]\n",
      "epoch:42 step:33316 [D loss: 0.048143, acc.: 98.44%] [G loss: 4.345343]\n",
      "epoch:42 step:33317 [D loss: 0.036779, acc.: 100.00%] [G loss: 4.906281]\n",
      "epoch:42 step:33318 [D loss: 0.073729, acc.: 98.44%] [G loss: 4.124658]\n",
      "epoch:42 step:33319 [D loss: 0.036500, acc.: 100.00%] [G loss: 4.887199]\n",
      "epoch:42 step:33320 [D loss: 0.055302, acc.: 99.22%] [G loss: 4.223402]\n",
      "epoch:42 step:33321 [D loss: 0.021233, acc.: 100.00%] [G loss: 4.251971]\n",
      "epoch:42 step:33322 [D loss: 0.017192, acc.: 100.00%] [G loss: 3.924344]\n",
      "epoch:42 step:33323 [D loss: 0.028477, acc.: 100.00%] [G loss: 3.215605]\n",
      "epoch:42 step:33324 [D loss: 0.114023, acc.: 96.88%] [G loss: 5.157297]\n",
      "epoch:42 step:33325 [D loss: 0.135507, acc.: 96.09%] [G loss: 4.049762]\n",
      "epoch:42 step:33326 [D loss: 0.077610, acc.: 99.22%] [G loss: 2.635191]\n",
      "epoch:42 step:33327 [D loss: 0.027981, acc.: 100.00%] [G loss: 5.631094]\n",
      "epoch:42 step:33328 [D loss: 0.050558, acc.: 99.22%] [G loss: 4.940786]\n",
      "epoch:42 step:33329 [D loss: 0.024452, acc.: 100.00%] [G loss: 4.538428]\n",
      "epoch:42 step:33330 [D loss: 0.014181, acc.: 100.00%] [G loss: 5.010405]\n",
      "epoch:42 step:33331 [D loss: 0.021859, acc.: 100.00%] [G loss: 5.182082]\n",
      "epoch:42 step:33332 [D loss: 0.009887, acc.: 100.00%] [G loss: 4.547103]\n",
      "epoch:42 step:33333 [D loss: 0.021669, acc.: 100.00%] [G loss: 4.306965]\n",
      "epoch:42 step:33334 [D loss: 0.100142, acc.: 98.44%] [G loss: 2.099456]\n",
      "epoch:42 step:33335 [D loss: 0.835481, acc.: 59.38%] [G loss: 10.846493]\n",
      "epoch:42 step:33336 [D loss: 2.419064, acc.: 50.00%] [G loss: 7.256525]\n",
      "epoch:42 step:33337 [D loss: 0.115837, acc.: 96.88%] [G loss: 4.881439]\n",
      "epoch:42 step:33338 [D loss: 0.085148, acc.: 96.88%] [G loss: 3.369605]\n",
      "epoch:42 step:33339 [D loss: 0.041760, acc.: 100.00%] [G loss: 4.009730]\n",
      "epoch:42 step:33340 [D loss: 0.025886, acc.: 100.00%] [G loss: 3.826034]\n",
      "epoch:42 step:33341 [D loss: 0.019300, acc.: 100.00%] [G loss: 3.810753]\n",
      "epoch:42 step:33342 [D loss: 0.017526, acc.: 100.00%] [G loss: 3.867170]\n",
      "epoch:42 step:33343 [D loss: 0.008999, acc.: 100.00%] [G loss: 4.226548]\n",
      "epoch:42 step:33344 [D loss: 0.048043, acc.: 99.22%] [G loss: 3.651148]\n",
      "epoch:42 step:33345 [D loss: 0.012483, acc.: 100.00%] [G loss: 3.815831]\n",
      "epoch:42 step:33346 [D loss: 0.010245, acc.: 100.00%] [G loss: 3.662164]\n",
      "epoch:42 step:33347 [D loss: 0.019730, acc.: 100.00%] [G loss: 4.430387]\n",
      "epoch:42 step:33348 [D loss: 0.028440, acc.: 99.22%] [G loss: 4.436435]\n",
      "epoch:42 step:33349 [D loss: 0.187626, acc.: 94.53%] [G loss: 4.142961]\n",
      "epoch:42 step:33350 [D loss: 0.013013, acc.: 100.00%] [G loss: 4.665781]\n",
      "epoch:42 step:33351 [D loss: 0.016083, acc.: 99.22%] [G loss: 4.926096]\n",
      "epoch:42 step:33352 [D loss: 0.184419, acc.: 92.97%] [G loss: 3.260788]\n",
      "epoch:42 step:33353 [D loss: 0.012999, acc.: 100.00%] [G loss: 3.506837]\n",
      "epoch:42 step:33354 [D loss: 0.012491, acc.: 100.00%] [G loss: 4.161786]\n",
      "epoch:42 step:33355 [D loss: 0.011154, acc.: 100.00%] [G loss: 3.941310]\n",
      "epoch:42 step:33356 [D loss: 0.043786, acc.: 97.66%] [G loss: 3.583049]\n",
      "epoch:42 step:33357 [D loss: 0.016273, acc.: 100.00%] [G loss: 3.210986]\n",
      "epoch:42 step:33358 [D loss: 0.015956, acc.: 100.00%] [G loss: 4.343371]\n",
      "epoch:42 step:33359 [D loss: 0.028357, acc.: 100.00%] [G loss: 5.128683]\n",
      "epoch:42 step:33360 [D loss: 0.020093, acc.: 100.00%] [G loss: 3.186510]\n",
      "epoch:42 step:33361 [D loss: 0.043966, acc.: 100.00%] [G loss: 3.751599]\n",
      "epoch:42 step:33362 [D loss: 0.027221, acc.: 100.00%] [G loss: 3.663170]\n",
      "epoch:42 step:33363 [D loss: 0.028970, acc.: 99.22%] [G loss: 4.650055]\n",
      "epoch:42 step:33364 [D loss: 0.086251, acc.: 98.44%] [G loss: 3.664403]\n",
      "epoch:42 step:33365 [D loss: 0.017549, acc.: 100.00%] [G loss: 5.037982]\n",
      "epoch:42 step:33366 [D loss: 0.008646, acc.: 100.00%] [G loss: 5.282664]\n",
      "epoch:42 step:33367 [D loss: 0.068697, acc.: 98.44%] [G loss: 3.434984]\n",
      "epoch:42 step:33368 [D loss: 0.026535, acc.: 100.00%] [G loss: 4.027233]\n",
      "epoch:42 step:33369 [D loss: 0.021302, acc.: 100.00%] [G loss: 3.999256]\n",
      "epoch:42 step:33370 [D loss: 0.011147, acc.: 100.00%] [G loss: 4.876997]\n",
      "epoch:42 step:33371 [D loss: 0.027449, acc.: 100.00%] [G loss: 4.438852]\n",
      "epoch:42 step:33372 [D loss: 0.020375, acc.: 100.00%] [G loss: 4.723622]\n",
      "epoch:42 step:33373 [D loss: 0.019948, acc.: 100.00%] [G loss: 4.237978]\n",
      "epoch:42 step:33374 [D loss: 0.026970, acc.: 100.00%] [G loss: 4.655622]\n",
      "epoch:42 step:33375 [D loss: 0.019997, acc.: 100.00%] [G loss: 5.115363]\n",
      "epoch:42 step:33376 [D loss: 0.004633, acc.: 100.00%] [G loss: 5.855530]\n",
      "epoch:42 step:33377 [D loss: 0.867768, acc.: 53.12%] [G loss: 8.591682]\n",
      "epoch:42 step:33378 [D loss: 1.501009, acc.: 56.25%] [G loss: 5.718085]\n",
      "epoch:42 step:33379 [D loss: 0.047937, acc.: 98.44%] [G loss: 4.854719]\n",
      "epoch:42 step:33380 [D loss: 0.042168, acc.: 100.00%] [G loss: 3.687639]\n",
      "epoch:42 step:33381 [D loss: 0.039128, acc.: 100.00%] [G loss: 4.032983]\n",
      "epoch:42 step:33382 [D loss: 0.052680, acc.: 99.22%] [G loss: 4.096689]\n",
      "epoch:42 step:33383 [D loss: 0.024384, acc.: 100.00%] [G loss: 3.645914]\n",
      "epoch:42 step:33384 [D loss: 0.031121, acc.: 99.22%] [G loss: 3.321133]\n",
      "epoch:42 step:33385 [D loss: 0.031765, acc.: 100.00%] [G loss: 3.888228]\n",
      "epoch:42 step:33386 [D loss: 0.026921, acc.: 99.22%] [G loss: 3.992234]\n",
      "epoch:42 step:33387 [D loss: 0.040515, acc.: 100.00%] [G loss: 3.317457]\n",
      "epoch:42 step:33388 [D loss: 0.038040, acc.: 99.22%] [G loss: 3.920180]\n",
      "epoch:42 step:33389 [D loss: 0.043596, acc.: 99.22%] [G loss: 4.402385]\n",
      "epoch:42 step:33390 [D loss: 0.048637, acc.: 100.00%] [G loss: 4.260820]\n",
      "epoch:42 step:33391 [D loss: 0.071708, acc.: 98.44%] [G loss: 2.851252]\n",
      "epoch:42 step:33392 [D loss: 0.033370, acc.: 99.22%] [G loss: 3.858918]\n",
      "epoch:42 step:33393 [D loss: 0.044399, acc.: 100.00%] [G loss: 3.592566]\n",
      "epoch:42 step:33394 [D loss: 0.026313, acc.: 100.00%] [G loss: 4.409059]\n",
      "epoch:42 step:33395 [D loss: 0.054922, acc.: 98.44%] [G loss: 4.061889]\n",
      "epoch:42 step:33396 [D loss: 0.060953, acc.: 99.22%] [G loss: 3.147664]\n",
      "epoch:42 step:33397 [D loss: 0.050078, acc.: 98.44%] [G loss: 3.205152]\n",
      "epoch:42 step:33398 [D loss: 0.112767, acc.: 98.44%] [G loss: 4.002583]\n",
      "epoch:42 step:33399 [D loss: 0.015699, acc.: 100.00%] [G loss: 5.157503]\n",
      "epoch:42 step:33400 [D loss: 0.050754, acc.: 99.22%] [G loss: 3.587830]\n",
      "##############\n",
      "[1.02123505 1.11002191 0.94794595 0.85245866 2.10467836 0.98384877\n",
      " 2.10608818 2.12036348 0.9795818  1.11570476]\n",
      "##########\n",
      "epoch:42 step:33401 [D loss: 0.025938, acc.: 100.00%] [G loss: 2.293615]\n",
      "epoch:42 step:33402 [D loss: 0.006250, acc.: 100.00%] [G loss: 2.161769]\n",
      "epoch:42 step:33403 [D loss: 0.085773, acc.: 97.66%] [G loss: 5.765041]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:42 step:33404 [D loss: 0.074679, acc.: 96.88%] [G loss: 5.958243]\n",
      "epoch:42 step:33405 [D loss: 0.008562, acc.: 100.00%] [G loss: 4.821517]\n",
      "epoch:42 step:33406 [D loss: 0.028398, acc.: 99.22%] [G loss: 4.569585]\n",
      "epoch:42 step:33407 [D loss: 0.004675, acc.: 100.00%] [G loss: 4.453510]\n",
      "epoch:42 step:33408 [D loss: 0.063185, acc.: 98.44%] [G loss: 3.251342]\n",
      "epoch:42 step:33409 [D loss: 0.050829, acc.: 100.00%] [G loss: 4.345924]\n",
      "epoch:42 step:33410 [D loss: 0.002016, acc.: 100.00%] [G loss: 5.363897]\n",
      "epoch:42 step:33411 [D loss: 0.004166, acc.: 100.00%] [G loss: 4.987235]\n",
      "epoch:42 step:33412 [D loss: 0.004262, acc.: 100.00%] [G loss: 3.903346]\n",
      "epoch:42 step:33413 [D loss: 0.007906, acc.: 100.00%] [G loss: 3.801807]\n",
      "epoch:42 step:33414 [D loss: 0.020562, acc.: 99.22%] [G loss: 3.583940]\n",
      "epoch:42 step:33415 [D loss: 0.049356, acc.: 99.22%] [G loss: 2.939641]\n",
      "epoch:42 step:33416 [D loss: 0.111109, acc.: 96.09%] [G loss: 5.744637]\n",
      "epoch:42 step:33417 [D loss: 0.002575, acc.: 100.00%] [G loss: 7.043082]\n",
      "epoch:42 step:33418 [D loss: 0.103815, acc.: 96.09%] [G loss: 5.133016]\n",
      "epoch:42 step:33419 [D loss: 0.011325, acc.: 100.00%] [G loss: 4.390744]\n",
      "epoch:42 step:33420 [D loss: 0.009483, acc.: 100.00%] [G loss: 4.414539]\n",
      "epoch:42 step:33421 [D loss: 0.017396, acc.: 100.00%] [G loss: 4.619774]\n",
      "epoch:42 step:33422 [D loss: 0.013680, acc.: 100.00%] [G loss: 4.096739]\n",
      "epoch:42 step:33423 [D loss: 0.003811, acc.: 100.00%] [G loss: 4.679274]\n",
      "epoch:42 step:33424 [D loss: 0.013837, acc.: 100.00%] [G loss: 5.273264]\n",
      "epoch:42 step:33425 [D loss: 0.045606, acc.: 99.22%] [G loss: 3.914691]\n",
      "epoch:42 step:33426 [D loss: 0.055858, acc.: 100.00%] [G loss: 5.045501]\n",
      "epoch:42 step:33427 [D loss: 0.003874, acc.: 100.00%] [G loss: 5.644737]\n",
      "epoch:42 step:33428 [D loss: 0.028184, acc.: 99.22%] [G loss: 4.539073]\n",
      "epoch:42 step:33429 [D loss: 0.159607, acc.: 96.09%] [G loss: 5.357381]\n",
      "epoch:42 step:33430 [D loss: 0.004020, acc.: 100.00%] [G loss: 6.227311]\n",
      "epoch:42 step:33431 [D loss: 0.017286, acc.: 99.22%] [G loss: 5.753202]\n",
      "epoch:42 step:33432 [D loss: 0.003613, acc.: 100.00%] [G loss: 5.296048]\n",
      "epoch:42 step:33433 [D loss: 0.047556, acc.: 98.44%] [G loss: 3.533767]\n",
      "epoch:42 step:33434 [D loss: 0.038211, acc.: 100.00%] [G loss: 5.648966]\n",
      "epoch:42 step:33435 [D loss: 0.001458, acc.: 100.00%] [G loss: 5.645685]\n",
      "epoch:42 step:33436 [D loss: 0.005756, acc.: 100.00%] [G loss: 4.823453]\n",
      "epoch:42 step:33437 [D loss: 0.004939, acc.: 100.00%] [G loss: 4.754178]\n",
      "epoch:42 step:33438 [D loss: 0.011133, acc.: 100.00%] [G loss: 4.466123]\n",
      "epoch:42 step:33439 [D loss: 0.026279, acc.: 100.00%] [G loss: 4.781158]\n",
      "epoch:42 step:33440 [D loss: 0.018319, acc.: 100.00%] [G loss: 4.557618]\n",
      "epoch:42 step:33441 [D loss: 0.013936, acc.: 100.00%] [G loss: 5.844436]\n",
      "epoch:42 step:33442 [D loss: 0.032662, acc.: 100.00%] [G loss: 5.453876]\n",
      "epoch:42 step:33443 [D loss: 0.006715, acc.: 100.00%] [G loss: 4.598932]\n",
      "epoch:42 step:33444 [D loss: 0.208825, acc.: 95.31%] [G loss: 5.284286]\n",
      "epoch:42 step:33445 [D loss: 0.001291, acc.: 100.00%] [G loss: 7.295866]\n",
      "epoch:42 step:33446 [D loss: 0.023686, acc.: 99.22%] [G loss: 7.031201]\n",
      "epoch:42 step:33447 [D loss: 0.009936, acc.: 100.00%] [G loss: 7.020967]\n",
      "epoch:42 step:33448 [D loss: 0.080614, acc.: 96.09%] [G loss: 2.661271]\n",
      "epoch:42 step:33449 [D loss: 0.004673, acc.: 100.00%] [G loss: 1.953581]\n",
      "epoch:42 step:33450 [D loss: 0.004237, acc.: 100.00%] [G loss: 1.718331]\n",
      "epoch:42 step:33451 [D loss: 0.080498, acc.: 97.66%] [G loss: 7.122906]\n",
      "epoch:42 step:33452 [D loss: 0.017943, acc.: 99.22%] [G loss: 8.348938]\n",
      "epoch:42 step:33453 [D loss: 0.017880, acc.: 99.22%] [G loss: 7.613034]\n",
      "epoch:42 step:33454 [D loss: 0.068838, acc.: 98.44%] [G loss: 3.950145]\n",
      "epoch:42 step:33455 [D loss: 0.026189, acc.: 98.44%] [G loss: 3.459004]\n",
      "epoch:42 step:33456 [D loss: 0.020119, acc.: 100.00%] [G loss: 5.506712]\n",
      "epoch:42 step:33457 [D loss: 0.016213, acc.: 100.00%] [G loss: 6.374125]\n",
      "epoch:42 step:33458 [D loss: 0.004979, acc.: 100.00%] [G loss: 5.917333]\n",
      "epoch:42 step:33459 [D loss: 0.002804, acc.: 100.00%] [G loss: 6.980894]\n",
      "epoch:42 step:33460 [D loss: 0.137905, acc.: 96.09%] [G loss: 7.917537]\n",
      "epoch:42 step:33461 [D loss: 0.064394, acc.: 97.66%] [G loss: 6.863092]\n",
      "epoch:42 step:33462 [D loss: 0.005909, acc.: 100.00%] [G loss: 6.234122]\n",
      "epoch:42 step:33463 [D loss: 0.006735, acc.: 100.00%] [G loss: 6.018021]\n",
      "epoch:42 step:33464 [D loss: 0.009017, acc.: 100.00%] [G loss: 5.429054]\n",
      "epoch:42 step:33465 [D loss: 0.034507, acc.: 100.00%] [G loss: 6.127215]\n",
      "epoch:42 step:33466 [D loss: 0.010339, acc.: 100.00%] [G loss: 7.460014]\n",
      "epoch:42 step:33467 [D loss: 0.482799, acc.: 78.91%] [G loss: 9.826344]\n",
      "epoch:42 step:33468 [D loss: 0.522241, acc.: 75.78%] [G loss: 3.950073]\n",
      "epoch:42 step:33469 [D loss: 0.158763, acc.: 91.41%] [G loss: 7.313244]\n",
      "epoch:42 step:33470 [D loss: 0.076923, acc.: 96.88%] [G loss: 6.613544]\n",
      "epoch:42 step:33471 [D loss: 0.007262, acc.: 100.00%] [G loss: 5.407897]\n",
      "epoch:42 step:33472 [D loss: 0.069341, acc.: 96.88%] [G loss: 7.675899]\n",
      "epoch:42 step:33473 [D loss: 0.002137, acc.: 100.00%] [G loss: 8.169018]\n",
      "epoch:42 step:33474 [D loss: 0.129172, acc.: 94.53%] [G loss: 3.720346]\n",
      "epoch:42 step:33475 [D loss: 0.017885, acc.: 100.00%] [G loss: 4.473714]\n",
      "epoch:42 step:33476 [D loss: 0.035341, acc.: 99.22%] [G loss: 4.801459]\n",
      "epoch:42 step:33477 [D loss: 0.001168, acc.: 100.00%] [G loss: 6.097654]\n",
      "epoch:42 step:33478 [D loss: 0.003892, acc.: 100.00%] [G loss: 5.703359]\n",
      "epoch:42 step:33479 [D loss: 0.013431, acc.: 100.00%] [G loss: 6.279174]\n",
      "epoch:42 step:33480 [D loss: 0.022894, acc.: 99.22%] [G loss: 4.292336]\n",
      "epoch:42 step:33481 [D loss: 0.037678, acc.: 98.44%] [G loss: 6.770918]\n",
      "epoch:42 step:33482 [D loss: 0.001828, acc.: 100.00%] [G loss: 7.064226]\n",
      "epoch:42 step:33483 [D loss: 0.131273, acc.: 96.88%] [G loss: 6.007318]\n",
      "epoch:42 step:33484 [D loss: 0.003561, acc.: 100.00%] [G loss: 5.049091]\n",
      "epoch:42 step:33485 [D loss: 0.013505, acc.: 100.00%] [G loss: 6.643349]\n",
      "epoch:42 step:33486 [D loss: 0.012035, acc.: 100.00%] [G loss: 6.696430]\n",
      "epoch:42 step:33487 [D loss: 1.986350, acc.: 43.75%] [G loss: 12.542328]\n",
      "epoch:42 step:33488 [D loss: 4.053792, acc.: 50.00%] [G loss: 9.305353]\n",
      "epoch:42 step:33489 [D loss: 2.706826, acc.: 49.22%] [G loss: 4.153277]\n",
      "epoch:42 step:33490 [D loss: 0.261737, acc.: 89.06%] [G loss: 3.777104]\n",
      "epoch:42 step:33491 [D loss: 0.475070, acc.: 80.47%] [G loss: 4.267127]\n",
      "epoch:42 step:33492 [D loss: 0.108711, acc.: 96.88%] [G loss: 3.842956]\n",
      "epoch:42 step:33493 [D loss: 0.093777, acc.: 96.88%] [G loss: 3.823532]\n",
      "epoch:42 step:33494 [D loss: 0.166763, acc.: 95.31%] [G loss: 3.409970]\n",
      "epoch:42 step:33495 [D loss: 0.021699, acc.: 100.00%] [G loss: 3.175852]\n",
      "epoch:42 step:33496 [D loss: 0.079972, acc.: 97.66%] [G loss: 3.523648]\n",
      "epoch:42 step:33497 [D loss: 0.040886, acc.: 100.00%] [G loss: 3.590698]\n",
      "epoch:42 step:33498 [D loss: 0.132836, acc.: 96.88%] [G loss: 3.856360]\n",
      "epoch:42 step:33499 [D loss: 0.048014, acc.: 100.00%] [G loss: 3.869390]\n",
      "epoch:42 step:33500 [D loss: 0.462332, acc.: 77.34%] [G loss: 3.265282]\n",
      "epoch:42 step:33501 [D loss: 0.025302, acc.: 100.00%] [G loss: 4.138083]\n",
      "epoch:42 step:33502 [D loss: 0.061230, acc.: 99.22%] [G loss: 2.493709]\n",
      "epoch:42 step:33503 [D loss: 0.582346, acc.: 73.44%] [G loss: 5.273261]\n",
      "epoch:42 step:33504 [D loss: 0.185484, acc.: 92.97%] [G loss: 5.741160]\n",
      "epoch:42 step:33505 [D loss: 1.144547, acc.: 55.47%] [G loss: 1.321480]\n",
      "epoch:42 step:33506 [D loss: 0.586408, acc.: 71.09%] [G loss: 4.811669]\n",
      "epoch:42 step:33507 [D loss: 0.195823, acc.: 90.62%] [G loss: 5.327066]\n",
      "epoch:42 step:33508 [D loss: 0.491110, acc.: 71.88%] [G loss: 3.512808]\n",
      "epoch:42 step:33509 [D loss: 0.054984, acc.: 99.22%] [G loss: 3.345813]\n",
      "epoch:42 step:33510 [D loss: 0.098028, acc.: 96.88%] [G loss: 3.344009]\n",
      "epoch:42 step:33511 [D loss: 0.016495, acc.: 100.00%] [G loss: 3.633356]\n",
      "epoch:42 step:33512 [D loss: 0.079695, acc.: 98.44%] [G loss: 3.809620]\n",
      "epoch:42 step:33513 [D loss: 0.063894, acc.: 98.44%] [G loss: 3.500080]\n",
      "epoch:42 step:33514 [D loss: 0.039740, acc.: 100.00%] [G loss: 3.508876]\n",
      "epoch:42 step:33515 [D loss: 0.106111, acc.: 97.66%] [G loss: 3.110728]\n",
      "epoch:42 step:33516 [D loss: 0.045814, acc.: 99.22%] [G loss: 3.075445]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:42 step:33517 [D loss: 0.100237, acc.: 99.22%] [G loss: 2.175090]\n",
      "epoch:42 step:33518 [D loss: 0.077663, acc.: 100.00%] [G loss: 2.848580]\n",
      "epoch:42 step:33519 [D loss: 0.025120, acc.: 100.00%] [G loss: 3.386106]\n",
      "epoch:42 step:33520 [D loss: 0.050212, acc.: 100.00%] [G loss: 3.750056]\n",
      "epoch:42 step:33521 [D loss: 0.047057, acc.: 100.00%] [G loss: 3.366619]\n",
      "epoch:42 step:33522 [D loss: 0.349070, acc.: 84.38%] [G loss: 2.682012]\n",
      "epoch:42 step:33523 [D loss: 0.104874, acc.: 95.31%] [G loss: 4.332965]\n",
      "epoch:42 step:33524 [D loss: 0.021094, acc.: 100.00%] [G loss: 4.855760]\n",
      "epoch:42 step:33525 [D loss: 0.108466, acc.: 96.88%] [G loss: 3.171199]\n",
      "epoch:42 step:33526 [D loss: 0.069646, acc.: 97.66%] [G loss: 3.293977]\n",
      "epoch:42 step:33527 [D loss: 0.070063, acc.: 98.44%] [G loss: 4.000722]\n",
      "epoch:42 step:33528 [D loss: 0.034090, acc.: 100.00%] [G loss: 4.605913]\n",
      "epoch:42 step:33529 [D loss: 0.085363, acc.: 100.00%] [G loss: 3.124128]\n",
      "epoch:42 step:33530 [D loss: 0.027619, acc.: 100.00%] [G loss: 3.327433]\n",
      "epoch:42 step:33531 [D loss: 0.050742, acc.: 99.22%] [G loss: 3.572454]\n",
      "epoch:42 step:33532 [D loss: 0.051127, acc.: 100.00%] [G loss: 3.507101]\n",
      "epoch:42 step:33533 [D loss: 0.076937, acc.: 97.66%] [G loss: 3.979765]\n",
      "epoch:42 step:33534 [D loss: 0.039989, acc.: 100.00%] [G loss: 3.147093]\n",
      "epoch:42 step:33535 [D loss: 0.070049, acc.: 98.44%] [G loss: 2.764775]\n",
      "epoch:42 step:33536 [D loss: 0.082053, acc.: 98.44%] [G loss: 2.464524]\n",
      "epoch:42 step:33537 [D loss: 0.034809, acc.: 100.00%] [G loss: 2.725320]\n",
      "epoch:42 step:33538 [D loss: 0.017200, acc.: 100.00%] [G loss: 2.998078]\n",
      "epoch:42 step:33539 [D loss: 0.249334, acc.: 91.41%] [G loss: 5.882792]\n",
      "epoch:42 step:33540 [D loss: 0.176677, acc.: 90.62%] [G loss: 5.738395]\n",
      "epoch:42 step:33541 [D loss: 0.189832, acc.: 93.75%] [G loss: 3.181841]\n",
      "epoch:42 step:33542 [D loss: 0.117577, acc.: 96.88%] [G loss: 4.028546]\n",
      "epoch:42 step:33543 [D loss: 0.017687, acc.: 99.22%] [G loss: 5.093163]\n",
      "epoch:42 step:33544 [D loss: 0.009239, acc.: 100.00%] [G loss: 4.911453]\n",
      "epoch:42 step:33545 [D loss: 0.383354, acc.: 85.94%] [G loss: 1.450258]\n",
      "epoch:42 step:33546 [D loss: 0.450201, acc.: 78.91%] [G loss: 5.846457]\n",
      "epoch:42 step:33547 [D loss: 0.079694, acc.: 97.66%] [G loss: 6.789880]\n",
      "epoch:42 step:33548 [D loss: 1.303836, acc.: 54.69%] [G loss: 4.759949]\n",
      "epoch:42 step:33549 [D loss: 0.043056, acc.: 100.00%] [G loss: 2.610970]\n",
      "epoch:42 step:33550 [D loss: 0.152765, acc.: 92.97%] [G loss: 4.009718]\n",
      "epoch:42 step:33551 [D loss: 0.017342, acc.: 100.00%] [G loss: 4.334117]\n",
      "epoch:42 step:33552 [D loss: 0.110688, acc.: 96.88%] [G loss: 3.755895]\n",
      "epoch:42 step:33553 [D loss: 0.037605, acc.: 100.00%] [G loss: 4.145837]\n",
      "epoch:42 step:33554 [D loss: 0.022537, acc.: 100.00%] [G loss: 3.911629]\n",
      "epoch:42 step:33555 [D loss: 0.040206, acc.: 100.00%] [G loss: 3.444449]\n",
      "epoch:42 step:33556 [D loss: 0.014276, acc.: 100.00%] [G loss: 3.696493]\n",
      "epoch:42 step:33557 [D loss: 0.030875, acc.: 99.22%] [G loss: 3.864361]\n",
      "epoch:42 step:33558 [D loss: 0.047461, acc.: 99.22%] [G loss: 2.668623]\n",
      "epoch:42 step:33559 [D loss: 0.039873, acc.: 100.00%] [G loss: 3.113234]\n",
      "epoch:42 step:33560 [D loss: 0.057962, acc.: 98.44%] [G loss: 3.658008]\n",
      "epoch:42 step:33561 [D loss: 0.011981, acc.: 100.00%] [G loss: 4.453610]\n",
      "epoch:42 step:33562 [D loss: 0.158935, acc.: 93.75%] [G loss: 2.902123]\n",
      "epoch:42 step:33563 [D loss: 0.044959, acc.: 99.22%] [G loss: 3.648929]\n",
      "epoch:42 step:33564 [D loss: 0.032788, acc.: 100.00%] [G loss: 3.483503]\n",
      "epoch:42 step:33565 [D loss: 0.062811, acc.: 98.44%] [G loss: 3.796404]\n",
      "epoch:42 step:33566 [D loss: 0.007773, acc.: 100.00%] [G loss: 3.823860]\n",
      "epoch:42 step:33567 [D loss: 0.051050, acc.: 100.00%] [G loss: 4.158962]\n",
      "epoch:42 step:33568 [D loss: 0.067719, acc.: 99.22%] [G loss: 2.899662]\n",
      "epoch:42 step:33569 [D loss: 0.127220, acc.: 95.31%] [G loss: 3.111388]\n",
      "epoch:42 step:33570 [D loss: 0.056860, acc.: 97.66%] [G loss: 4.230292]\n",
      "epoch:42 step:33571 [D loss: 0.127894, acc.: 95.31%] [G loss: 3.146545]\n",
      "epoch:42 step:33572 [D loss: 0.041119, acc.: 99.22%] [G loss: 3.844481]\n",
      "epoch:42 step:33573 [D loss: 0.082802, acc.: 96.88%] [G loss: 3.215831]\n",
      "epoch:42 step:33574 [D loss: 0.032485, acc.: 100.00%] [G loss: 3.810354]\n",
      "epoch:42 step:33575 [D loss: 0.014716, acc.: 100.00%] [G loss: 3.820424]\n",
      "epoch:42 step:33576 [D loss: 0.028428, acc.: 99.22%] [G loss: 3.504210]\n",
      "epoch:42 step:33577 [D loss: 0.035492, acc.: 100.00%] [G loss: 4.438326]\n",
      "epoch:42 step:33578 [D loss: 0.045577, acc.: 100.00%] [G loss: 3.779916]\n",
      "epoch:42 step:33579 [D loss: 0.013587, acc.: 100.00%] [G loss: 4.555065]\n",
      "epoch:42 step:33580 [D loss: 0.059558, acc.: 100.00%] [G loss: 3.872256]\n",
      "epoch:42 step:33581 [D loss: 0.212428, acc.: 92.19%] [G loss: 4.828021]\n",
      "epoch:42 step:33582 [D loss: 0.169938, acc.: 92.97%] [G loss: 3.738386]\n",
      "epoch:42 step:33583 [D loss: 0.017940, acc.: 100.00%] [G loss: 3.795374]\n",
      "epoch:43 step:33584 [D loss: 0.065691, acc.: 98.44%] [G loss: 3.902199]\n",
      "epoch:43 step:33585 [D loss: 0.021517, acc.: 100.00%] [G loss: 4.109078]\n",
      "epoch:43 step:33586 [D loss: 0.016287, acc.: 100.00%] [G loss: 3.906565]\n",
      "epoch:43 step:33587 [D loss: 0.354407, acc.: 85.16%] [G loss: 5.858879]\n",
      "epoch:43 step:33588 [D loss: 0.183708, acc.: 93.75%] [G loss: 4.902868]\n",
      "epoch:43 step:33589 [D loss: 0.036253, acc.: 100.00%] [G loss: 4.932907]\n",
      "epoch:43 step:33590 [D loss: 0.012234, acc.: 100.00%] [G loss: 5.399639]\n",
      "epoch:43 step:33591 [D loss: 0.007554, acc.: 100.00%] [G loss: 5.351555]\n",
      "epoch:43 step:33592 [D loss: 0.013657, acc.: 100.00%] [G loss: 4.824663]\n",
      "epoch:43 step:33593 [D loss: 0.011791, acc.: 100.00%] [G loss: 4.623439]\n",
      "epoch:43 step:33594 [D loss: 0.032511, acc.: 100.00%] [G loss: 4.713445]\n",
      "epoch:43 step:33595 [D loss: 0.054717, acc.: 98.44%] [G loss: 4.276525]\n",
      "epoch:43 step:33596 [D loss: 0.007706, acc.: 100.00%] [G loss: 4.568965]\n",
      "epoch:43 step:33597 [D loss: 0.030846, acc.: 100.00%] [G loss: 5.162467]\n",
      "epoch:43 step:33598 [D loss: 0.049326, acc.: 98.44%] [G loss: 4.119917]\n",
      "epoch:43 step:33599 [D loss: 0.018108, acc.: 100.00%] [G loss: 4.012168]\n",
      "epoch:43 step:33600 [D loss: 0.058682, acc.: 99.22%] [G loss: 5.109288]\n",
      "##############\n",
      "[0.90048426 0.90749594 0.96451554 1.0968828  2.11161369 0.95579297\n",
      " 2.10287349 2.10140585 0.9639418  0.91623628]\n",
      "##########\n",
      "epoch:43 step:33601 [D loss: 0.014782, acc.: 100.00%] [G loss: 4.741886]\n",
      "epoch:43 step:33602 [D loss: 0.039623, acc.: 98.44%] [G loss: 3.719602]\n",
      "epoch:43 step:33603 [D loss: 0.045735, acc.: 99.22%] [G loss: 4.723810]\n",
      "epoch:43 step:33604 [D loss: 0.042652, acc.: 100.00%] [G loss: 4.608962]\n",
      "epoch:43 step:33605 [D loss: 0.009017, acc.: 100.00%] [G loss: 4.744658]\n",
      "epoch:43 step:33606 [D loss: 0.023787, acc.: 99.22%] [G loss: 4.196947]\n",
      "epoch:43 step:33607 [D loss: 0.022632, acc.: 100.00%] [G loss: 5.018566]\n",
      "epoch:43 step:33608 [D loss: 0.042499, acc.: 99.22%] [G loss: 3.705445]\n",
      "epoch:43 step:33609 [D loss: 0.016843, acc.: 100.00%] [G loss: 4.254371]\n",
      "epoch:43 step:33610 [D loss: 0.011519, acc.: 100.00%] [G loss: 4.591151]\n",
      "epoch:43 step:33611 [D loss: 0.046331, acc.: 99.22%] [G loss: 3.532290]\n",
      "epoch:43 step:33612 [D loss: 0.211692, acc.: 92.19%] [G loss: 6.194894]\n",
      "epoch:43 step:33613 [D loss: 0.127522, acc.: 96.09%] [G loss: 4.394983]\n",
      "epoch:43 step:33614 [D loss: 0.024604, acc.: 100.00%] [G loss: 3.625240]\n",
      "epoch:43 step:33615 [D loss: 0.030690, acc.: 100.00%] [G loss: 5.362622]\n",
      "epoch:43 step:33616 [D loss: 0.005429, acc.: 100.00%] [G loss: 6.043125]\n",
      "epoch:43 step:33617 [D loss: 0.007141, acc.: 100.00%] [G loss: 5.692292]\n",
      "epoch:43 step:33618 [D loss: 0.034494, acc.: 99.22%] [G loss: 4.503910]\n",
      "epoch:43 step:33619 [D loss: 0.017060, acc.: 100.00%] [G loss: 4.644510]\n",
      "epoch:43 step:33620 [D loss: 0.012828, acc.: 100.00%] [G loss: 5.725389]\n",
      "epoch:43 step:33621 [D loss: 0.013669, acc.: 99.22%] [G loss: 5.911875]\n",
      "epoch:43 step:33622 [D loss: 0.017335, acc.: 100.00%] [G loss: 5.344944]\n",
      "epoch:43 step:33623 [D loss: 0.010218, acc.: 100.00%] [G loss: 5.135632]\n",
      "epoch:43 step:33624 [D loss: 0.012220, acc.: 100.00%] [G loss: 5.227415]\n",
      "epoch:43 step:33625 [D loss: 0.011722, acc.: 100.00%] [G loss: 5.372760]\n",
      "epoch:43 step:33626 [D loss: 0.009143, acc.: 100.00%] [G loss: 4.699435]\n",
      "epoch:43 step:33627 [D loss: 2.740544, acc.: 39.06%] [G loss: 9.767570]\n",
      "epoch:43 step:33628 [D loss: 3.513477, acc.: 50.00%] [G loss: 6.699123]\n",
      "epoch:43 step:33629 [D loss: 2.304643, acc.: 50.78%] [G loss: 4.240712]\n",
      "epoch:43 step:33630 [D loss: 0.802182, acc.: 63.28%] [G loss: 2.992693]\n",
      "epoch:43 step:33631 [D loss: 0.216410, acc.: 95.31%] [G loss: 2.471312]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:43 step:33632 [D loss: 0.162827, acc.: 96.09%] [G loss: 2.630663]\n",
      "epoch:43 step:33633 [D loss: 0.148283, acc.: 97.66%] [G loss: 2.654530]\n",
      "epoch:43 step:33634 [D loss: 0.040366, acc.: 100.00%] [G loss: 2.986352]\n",
      "epoch:43 step:33635 [D loss: 0.080562, acc.: 98.44%] [G loss: 4.274621]\n",
      "epoch:43 step:33636 [D loss: 0.018629, acc.: 100.00%] [G loss: 3.332951]\n",
      "epoch:43 step:33637 [D loss: 0.040947, acc.: 100.00%] [G loss: 2.178023]\n",
      "epoch:43 step:33638 [D loss: 0.036163, acc.: 100.00%] [G loss: 3.173488]\n",
      "epoch:43 step:33639 [D loss: 0.096625, acc.: 99.22%] [G loss: 3.435047]\n",
      "epoch:43 step:33640 [D loss: 0.034501, acc.: 100.00%] [G loss: 3.041324]\n",
      "epoch:43 step:33641 [D loss: 0.110190, acc.: 99.22%] [G loss: 3.386683]\n",
      "epoch:43 step:33642 [D loss: 0.028338, acc.: 100.00%] [G loss: 3.754429]\n",
      "epoch:43 step:33643 [D loss: 0.042896, acc.: 100.00%] [G loss: 3.663375]\n",
      "epoch:43 step:33644 [D loss: 0.026702, acc.: 100.00%] [G loss: 3.348181]\n",
      "epoch:43 step:33645 [D loss: 0.047717, acc.: 99.22%] [G loss: 3.746452]\n",
      "epoch:43 step:33646 [D loss: 0.123981, acc.: 97.66%] [G loss: 2.426119]\n",
      "epoch:43 step:33647 [D loss: 0.072004, acc.: 100.00%] [G loss: 3.356550]\n",
      "epoch:43 step:33648 [D loss: 0.020263, acc.: 100.00%] [G loss: 3.728454]\n",
      "epoch:43 step:33649 [D loss: 0.037001, acc.: 100.00%] [G loss: 3.704397]\n",
      "epoch:43 step:33650 [D loss: 0.071625, acc.: 98.44%] [G loss: 3.147307]\n",
      "epoch:43 step:33651 [D loss: 0.046009, acc.: 100.00%] [G loss: 3.055431]\n",
      "epoch:43 step:33652 [D loss: 0.027894, acc.: 100.00%] [G loss: 3.624890]\n",
      "epoch:43 step:33653 [D loss: 0.027339, acc.: 99.22%] [G loss: 2.746397]\n",
      "epoch:43 step:33654 [D loss: 0.018229, acc.: 100.00%] [G loss: 3.348229]\n",
      "epoch:43 step:33655 [D loss: 0.033828, acc.: 100.00%] [G loss: 3.300428]\n",
      "epoch:43 step:33656 [D loss: 0.105674, acc.: 97.66%] [G loss: 3.122894]\n",
      "epoch:43 step:33657 [D loss: 0.029089, acc.: 100.00%] [G loss: 3.829271]\n",
      "epoch:43 step:33658 [D loss: 0.048744, acc.: 100.00%] [G loss: 3.949352]\n",
      "epoch:43 step:33659 [D loss: 0.025410, acc.: 100.00%] [G loss: 3.636613]\n",
      "epoch:43 step:33660 [D loss: 0.014881, acc.: 100.00%] [G loss: 4.226774]\n",
      "epoch:43 step:33661 [D loss: 0.028466, acc.: 100.00%] [G loss: 3.604435]\n",
      "epoch:43 step:33662 [D loss: 0.033794, acc.: 100.00%] [G loss: 3.885779]\n",
      "epoch:43 step:33663 [D loss: 0.052523, acc.: 99.22%] [G loss: 3.781138]\n",
      "epoch:43 step:33664 [D loss: 0.017729, acc.: 100.00%] [G loss: 3.901658]\n",
      "epoch:43 step:33665 [D loss: 0.021654, acc.: 100.00%] [G loss: 3.776214]\n",
      "epoch:43 step:33666 [D loss: 0.053644, acc.: 99.22%] [G loss: 3.002917]\n",
      "epoch:43 step:33667 [D loss: 0.061828, acc.: 99.22%] [G loss: 2.910849]\n",
      "epoch:43 step:33668 [D loss: 0.020927, acc.: 100.00%] [G loss: 3.780878]\n",
      "epoch:43 step:33669 [D loss: 0.031825, acc.: 100.00%] [G loss: 4.209397]\n",
      "epoch:43 step:33670 [D loss: 0.016443, acc.: 100.00%] [G loss: 3.435165]\n",
      "epoch:43 step:33671 [D loss: 0.135754, acc.: 95.31%] [G loss: 1.397353]\n",
      "epoch:43 step:33672 [D loss: 0.198964, acc.: 91.41%] [G loss: 6.024302]\n",
      "epoch:43 step:33673 [D loss: 0.537665, acc.: 75.00%] [G loss: 3.239044]\n",
      "epoch:43 step:33674 [D loss: 0.047534, acc.: 100.00%] [G loss: 3.227015]\n",
      "epoch:43 step:33675 [D loss: 0.016982, acc.: 100.00%] [G loss: 3.678934]\n",
      "epoch:43 step:33676 [D loss: 0.034693, acc.: 100.00%] [G loss: 4.721549]\n",
      "epoch:43 step:33677 [D loss: 0.075293, acc.: 99.22%] [G loss: 3.506138]\n",
      "epoch:43 step:33678 [D loss: 0.037145, acc.: 100.00%] [G loss: 4.012429]\n",
      "epoch:43 step:33679 [D loss: 0.021029, acc.: 100.00%] [G loss: 4.202460]\n",
      "epoch:43 step:33680 [D loss: 0.014552, acc.: 100.00%] [G loss: 4.277284]\n",
      "epoch:43 step:33681 [D loss: 0.013760, acc.: 100.00%] [G loss: 4.406188]\n",
      "epoch:43 step:33682 [D loss: 0.017678, acc.: 99.22%] [G loss: 4.392143]\n",
      "epoch:43 step:33683 [D loss: 0.016716, acc.: 100.00%] [G loss: 3.992436]\n",
      "epoch:43 step:33684 [D loss: 0.009971, acc.: 100.00%] [G loss: 4.132127]\n",
      "epoch:43 step:33685 [D loss: 0.035980, acc.: 99.22%] [G loss: 3.956573]\n",
      "epoch:43 step:33686 [D loss: 0.054702, acc.: 100.00%] [G loss: 4.115763]\n",
      "epoch:43 step:33687 [D loss: 0.017400, acc.: 100.00%] [G loss: 4.171281]\n",
      "epoch:43 step:33688 [D loss: 0.019056, acc.: 100.00%] [G loss: 4.142552]\n",
      "epoch:43 step:33689 [D loss: 0.015452, acc.: 100.00%] [G loss: 4.328998]\n",
      "epoch:43 step:33690 [D loss: 0.122304, acc.: 96.88%] [G loss: 3.807460]\n",
      "epoch:43 step:33691 [D loss: 0.057318, acc.: 99.22%] [G loss: 4.478779]\n",
      "epoch:43 step:33692 [D loss: 0.027269, acc.: 99.22%] [G loss: 5.546136]\n",
      "epoch:43 step:33693 [D loss: 0.048794, acc.: 99.22%] [G loss: 4.105242]\n",
      "epoch:43 step:33694 [D loss: 0.008430, acc.: 100.00%] [G loss: 3.362365]\n",
      "epoch:43 step:33695 [D loss: 0.023683, acc.: 100.00%] [G loss: 4.029110]\n",
      "epoch:43 step:33696 [D loss: 0.019262, acc.: 100.00%] [G loss: 4.033518]\n",
      "epoch:43 step:33697 [D loss: 0.048663, acc.: 99.22%] [G loss: 3.997465]\n",
      "epoch:43 step:33698 [D loss: 0.031623, acc.: 100.00%] [G loss: 4.424199]\n",
      "epoch:43 step:33699 [D loss: 0.170492, acc.: 95.31%] [G loss: 4.536629]\n",
      "epoch:43 step:33700 [D loss: 0.009063, acc.: 100.00%] [G loss: 5.060318]\n",
      "epoch:43 step:33701 [D loss: 0.004517, acc.: 100.00%] [G loss: 4.615473]\n",
      "epoch:43 step:33702 [D loss: 0.032994, acc.: 100.00%] [G loss: 4.793711]\n",
      "epoch:43 step:33703 [D loss: 0.019172, acc.: 100.00%] [G loss: 4.053059]\n",
      "epoch:43 step:33704 [D loss: 0.034454, acc.: 100.00%] [G loss: 4.179913]\n",
      "epoch:43 step:33705 [D loss: 0.010100, acc.: 100.00%] [G loss: 4.900916]\n",
      "epoch:43 step:33706 [D loss: 0.018716, acc.: 100.00%] [G loss: 4.008229]\n",
      "epoch:43 step:33707 [D loss: 0.052829, acc.: 100.00%] [G loss: 4.508866]\n",
      "epoch:43 step:33708 [D loss: 0.061467, acc.: 99.22%] [G loss: 4.112333]\n",
      "epoch:43 step:33709 [D loss: 0.009274, acc.: 100.00%] [G loss: 4.510338]\n",
      "epoch:43 step:33710 [D loss: 0.008243, acc.: 100.00%] [G loss: 4.707512]\n",
      "epoch:43 step:33711 [D loss: 0.035896, acc.: 100.00%] [G loss: 4.351741]\n",
      "epoch:43 step:33712 [D loss: 0.029231, acc.: 99.22%] [G loss: 4.201282]\n",
      "epoch:43 step:33713 [D loss: 0.067569, acc.: 99.22%] [G loss: 6.127407]\n",
      "epoch:43 step:33714 [D loss: 0.057696, acc.: 98.44%] [G loss: 5.586023]\n",
      "epoch:43 step:33715 [D loss: 0.320898, acc.: 85.94%] [G loss: 6.603518]\n",
      "epoch:43 step:33716 [D loss: 0.028829, acc.: 99.22%] [G loss: 7.738289]\n",
      "epoch:43 step:33717 [D loss: 1.300517, acc.: 54.69%] [G loss: 0.068065]\n",
      "epoch:43 step:33718 [D loss: 1.716706, acc.: 56.25%] [G loss: 7.913764]\n",
      "epoch:43 step:33719 [D loss: 1.200886, acc.: 64.06%] [G loss: 7.252268]\n",
      "epoch:43 step:33720 [D loss: 0.849560, acc.: 65.62%] [G loss: 3.872761]\n",
      "epoch:43 step:33721 [D loss: 0.096896, acc.: 98.44%] [G loss: 4.005558]\n",
      "epoch:43 step:33722 [D loss: 0.024003, acc.: 100.00%] [G loss: 4.099135]\n",
      "epoch:43 step:33723 [D loss: 0.018658, acc.: 100.00%] [G loss: 4.261721]\n",
      "epoch:43 step:33724 [D loss: 0.011870, acc.: 100.00%] [G loss: 3.691529]\n",
      "epoch:43 step:33725 [D loss: 0.033057, acc.: 100.00%] [G loss: 3.996977]\n",
      "epoch:43 step:33726 [D loss: 0.021235, acc.: 99.22%] [G loss: 3.484693]\n",
      "epoch:43 step:33727 [D loss: 0.017732, acc.: 100.00%] [G loss: 4.066466]\n",
      "epoch:43 step:33728 [D loss: 0.024734, acc.: 100.00%] [G loss: 3.441624]\n",
      "epoch:43 step:33729 [D loss: 0.019359, acc.: 100.00%] [G loss: 3.446825]\n",
      "epoch:43 step:33730 [D loss: 0.041306, acc.: 100.00%] [G loss: 4.081717]\n",
      "epoch:43 step:33731 [D loss: 0.047449, acc.: 100.00%] [G loss: 4.089439]\n",
      "epoch:43 step:33732 [D loss: 0.034424, acc.: 100.00%] [G loss: 3.900459]\n",
      "epoch:43 step:33733 [D loss: 0.020382, acc.: 100.00%] [G loss: 3.681381]\n",
      "epoch:43 step:33734 [D loss: 0.023469, acc.: 100.00%] [G loss: 3.946527]\n",
      "epoch:43 step:33735 [D loss: 0.040554, acc.: 99.22%] [G loss: 3.205411]\n",
      "epoch:43 step:33736 [D loss: 0.215277, acc.: 92.19%] [G loss: 4.042775]\n",
      "epoch:43 step:33737 [D loss: 0.011109, acc.: 100.00%] [G loss: 4.778350]\n",
      "epoch:43 step:33738 [D loss: 0.007372, acc.: 100.00%] [G loss: 4.626903]\n",
      "epoch:43 step:33739 [D loss: 0.020054, acc.: 100.00%] [G loss: 4.261068]\n",
      "epoch:43 step:33740 [D loss: 0.022939, acc.: 100.00%] [G loss: 3.957723]\n",
      "epoch:43 step:33741 [D loss: 0.035764, acc.: 100.00%] [G loss: 3.887254]\n",
      "epoch:43 step:33742 [D loss: 0.060845, acc.: 99.22%] [G loss: 4.097098]\n",
      "epoch:43 step:33743 [D loss: 0.025601, acc.: 100.00%] [G loss: 4.528158]\n",
      "epoch:43 step:33744 [D loss: 0.015334, acc.: 100.00%] [G loss: 4.263464]\n",
      "epoch:43 step:33745 [D loss: 0.018273, acc.: 100.00%] [G loss: 4.228969]\n",
      "epoch:43 step:33746 [D loss: 0.025321, acc.: 100.00%] [G loss: 4.194024]\n",
      "epoch:43 step:33747 [D loss: 0.015792, acc.: 100.00%] [G loss: 4.140445]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:43 step:33748 [D loss: 0.019734, acc.: 100.00%] [G loss: 4.431445]\n",
      "epoch:43 step:33749 [D loss: 0.034803, acc.: 100.00%] [G loss: 3.643218]\n",
      "epoch:43 step:33750 [D loss: 0.032939, acc.: 100.00%] [G loss: 3.851032]\n",
      "epoch:43 step:33751 [D loss: 0.013648, acc.: 100.00%] [G loss: 3.707726]\n",
      "epoch:43 step:33752 [D loss: 0.017364, acc.: 100.00%] [G loss: 4.151289]\n",
      "epoch:43 step:33753 [D loss: 0.029458, acc.: 100.00%] [G loss: 3.952474]\n",
      "epoch:43 step:33754 [D loss: 0.038063, acc.: 100.00%] [G loss: 4.211228]\n",
      "epoch:43 step:33755 [D loss: 0.019604, acc.: 100.00%] [G loss: 4.631129]\n",
      "epoch:43 step:33756 [D loss: 0.011014, acc.: 100.00%] [G loss: 4.368006]\n",
      "epoch:43 step:33757 [D loss: 0.031747, acc.: 100.00%] [G loss: 4.401237]\n",
      "epoch:43 step:33758 [D loss: 0.014260, acc.: 100.00%] [G loss: 4.740446]\n",
      "epoch:43 step:33759 [D loss: 1.027717, acc.: 44.53%] [G loss: 6.374683]\n",
      "epoch:43 step:33760 [D loss: 0.456451, acc.: 81.25%] [G loss: 6.524583]\n",
      "epoch:43 step:33761 [D loss: 0.067504, acc.: 99.22%] [G loss: 6.586275]\n",
      "epoch:43 step:33762 [D loss: 0.020478, acc.: 100.00%] [G loss: 6.021141]\n",
      "epoch:43 step:33763 [D loss: 0.004098, acc.: 100.00%] [G loss: 5.737266]\n",
      "epoch:43 step:33764 [D loss: 0.011180, acc.: 100.00%] [G loss: 5.626252]\n",
      "epoch:43 step:33765 [D loss: 0.004369, acc.: 100.00%] [G loss: 5.577991]\n",
      "epoch:43 step:33766 [D loss: 0.005476, acc.: 100.00%] [G loss: 5.095254]\n",
      "epoch:43 step:33767 [D loss: 0.006486, acc.: 100.00%] [G loss: 5.016606]\n",
      "epoch:43 step:33768 [D loss: 0.011301, acc.: 100.00%] [G loss: 4.671244]\n",
      "epoch:43 step:33769 [D loss: 0.013005, acc.: 100.00%] [G loss: 4.901641]\n",
      "epoch:43 step:33770 [D loss: 0.008359, acc.: 100.00%] [G loss: 5.299286]\n",
      "epoch:43 step:33771 [D loss: 0.004833, acc.: 100.00%] [G loss: 4.872915]\n",
      "epoch:43 step:33772 [D loss: 0.009975, acc.: 100.00%] [G loss: 4.578890]\n",
      "epoch:43 step:33773 [D loss: 0.009176, acc.: 100.00%] [G loss: 4.629675]\n",
      "epoch:43 step:33774 [D loss: 0.018111, acc.: 100.00%] [G loss: 4.467462]\n",
      "epoch:43 step:33775 [D loss: 0.013089, acc.: 100.00%] [G loss: 4.731499]\n",
      "epoch:43 step:33776 [D loss: 0.019031, acc.: 100.00%] [G loss: 5.181795]\n",
      "epoch:43 step:33777 [D loss: 0.027963, acc.: 100.00%] [G loss: 3.772403]\n",
      "epoch:43 step:33778 [D loss: 0.007417, acc.: 100.00%] [G loss: 4.118073]\n",
      "epoch:43 step:33779 [D loss: 0.009881, acc.: 100.00%] [G loss: 3.959019]\n",
      "epoch:43 step:33780 [D loss: 0.011551, acc.: 100.00%] [G loss: 4.479207]\n",
      "epoch:43 step:33781 [D loss: 0.010851, acc.: 100.00%] [G loss: 3.606995]\n",
      "epoch:43 step:33782 [D loss: 0.016573, acc.: 100.00%] [G loss: 3.365831]\n",
      "epoch:43 step:33783 [D loss: 0.012601, acc.: 100.00%] [G loss: 4.212097]\n",
      "epoch:43 step:33784 [D loss: 0.009905, acc.: 100.00%] [G loss: 4.026952]\n",
      "epoch:43 step:33785 [D loss: 0.027494, acc.: 100.00%] [G loss: 4.647491]\n",
      "epoch:43 step:33786 [D loss: 0.027039, acc.: 100.00%] [G loss: 4.895241]\n",
      "epoch:43 step:33787 [D loss: 0.009634, acc.: 100.00%] [G loss: 4.508912]\n",
      "epoch:43 step:33788 [D loss: 0.046516, acc.: 100.00%] [G loss: 4.454517]\n",
      "epoch:43 step:33789 [D loss: 0.007386, acc.: 100.00%] [G loss: 4.230789]\n",
      "epoch:43 step:33790 [D loss: 0.011518, acc.: 100.00%] [G loss: 3.832633]\n",
      "epoch:43 step:33791 [D loss: 0.024347, acc.: 100.00%] [G loss: 4.806455]\n",
      "epoch:43 step:33792 [D loss: 0.014661, acc.: 100.00%] [G loss: 4.713602]\n",
      "epoch:43 step:33793 [D loss: 0.018617, acc.: 100.00%] [G loss: 4.760384]\n",
      "epoch:43 step:33794 [D loss: 0.006133, acc.: 100.00%] [G loss: 4.424765]\n",
      "epoch:43 step:33795 [D loss: 0.010232, acc.: 100.00%] [G loss: 4.389598]\n",
      "epoch:43 step:33796 [D loss: 0.011656, acc.: 100.00%] [G loss: 4.407924]\n",
      "epoch:43 step:33797 [D loss: 0.294570, acc.: 92.19%] [G loss: 2.441820]\n",
      "epoch:43 step:33798 [D loss: 0.075473, acc.: 98.44%] [G loss: 4.950369]\n",
      "epoch:43 step:33799 [D loss: 0.004018, acc.: 100.00%] [G loss: 5.293941]\n",
      "epoch:43 step:33800 [D loss: 0.005350, acc.: 100.00%] [G loss: 5.426666]\n",
      "##############\n",
      "[0.86566484 0.84112479 1.04910731 0.91911024 2.11204612 1.13281211\n",
      " 2.10583995 2.11654538 1.10143025 2.11415253]\n",
      "##########\n",
      "epoch:43 step:33801 [D loss: 0.053890, acc.: 98.44%] [G loss: 4.547679]\n",
      "epoch:43 step:33802 [D loss: 0.016627, acc.: 100.00%] [G loss: 4.516303]\n",
      "epoch:43 step:33803 [D loss: 0.010398, acc.: 100.00%] [G loss: 4.892885]\n",
      "epoch:43 step:33804 [D loss: 0.014804, acc.: 100.00%] [G loss: 4.874328]\n",
      "epoch:43 step:33805 [D loss: 0.003221, acc.: 100.00%] [G loss: 4.587297]\n",
      "epoch:43 step:33806 [D loss: 0.018670, acc.: 100.00%] [G loss: 4.694251]\n",
      "epoch:43 step:33807 [D loss: 0.003067, acc.: 100.00%] [G loss: 5.790640]\n",
      "epoch:43 step:33808 [D loss: 0.006224, acc.: 100.00%] [G loss: 6.401006]\n",
      "epoch:43 step:33809 [D loss: 0.024046, acc.: 99.22%] [G loss: 5.070470]\n",
      "epoch:43 step:33810 [D loss: 0.006820, acc.: 100.00%] [G loss: 5.571816]\n",
      "epoch:43 step:33811 [D loss: 0.011941, acc.: 100.00%] [G loss: 5.041274]\n",
      "epoch:43 step:33812 [D loss: 0.022170, acc.: 100.00%] [G loss: 4.196339]\n",
      "epoch:43 step:33813 [D loss: 0.011767, acc.: 100.00%] [G loss: 3.673722]\n",
      "epoch:43 step:33814 [D loss: 0.044255, acc.: 99.22%] [G loss: 5.804780]\n",
      "epoch:43 step:33815 [D loss: 0.094766, acc.: 97.66%] [G loss: 6.299179]\n",
      "epoch:43 step:33816 [D loss: 0.006079, acc.: 100.00%] [G loss: 5.040843]\n",
      "epoch:43 step:33817 [D loss: 0.024428, acc.: 100.00%] [G loss: 4.994867]\n",
      "epoch:43 step:33818 [D loss: 0.007487, acc.: 100.00%] [G loss: 5.383254]\n",
      "epoch:43 step:33819 [D loss: 0.013364, acc.: 100.00%] [G loss: 5.135816]\n",
      "epoch:43 step:33820 [D loss: 0.013342, acc.: 100.00%] [G loss: 6.236000]\n",
      "epoch:43 step:33821 [D loss: 0.002672, acc.: 100.00%] [G loss: 5.695038]\n",
      "epoch:43 step:33822 [D loss: 0.001923, acc.: 100.00%] [G loss: 6.686252]\n",
      "epoch:43 step:33823 [D loss: 6.279724, acc.: 10.94%] [G loss: 8.906807]\n",
      "epoch:43 step:33824 [D loss: 2.735738, acc.: 50.78%] [G loss: 3.780013]\n",
      "epoch:43 step:33825 [D loss: 1.100256, acc.: 53.91%] [G loss: 4.111942]\n",
      "epoch:43 step:33826 [D loss: 0.110229, acc.: 94.53%] [G loss: 5.490242]\n",
      "epoch:43 step:33827 [D loss: 0.296268, acc.: 89.06%] [G loss: 3.380654]\n",
      "epoch:43 step:33828 [D loss: 0.338306, acc.: 82.81%] [G loss: 3.562645]\n",
      "epoch:43 step:33829 [D loss: 0.159410, acc.: 94.53%] [G loss: 4.206942]\n",
      "epoch:43 step:33830 [D loss: 0.181116, acc.: 93.75%] [G loss: 2.217422]\n",
      "epoch:43 step:33831 [D loss: 0.073204, acc.: 98.44%] [G loss: 3.120152]\n",
      "epoch:43 step:33832 [D loss: 0.049296, acc.: 99.22%] [G loss: 2.337268]\n",
      "epoch:43 step:33833 [D loss: 0.353770, acc.: 81.25%] [G loss: 6.241565]\n",
      "epoch:43 step:33834 [D loss: 1.047202, acc.: 57.03%] [G loss: 2.661141]\n",
      "epoch:43 step:33835 [D loss: 0.897869, acc.: 60.16%] [G loss: 6.033617]\n",
      "epoch:43 step:33836 [D loss: 0.306722, acc.: 85.16%] [G loss: 6.454477]\n",
      "epoch:43 step:33837 [D loss: 0.374053, acc.: 83.59%] [G loss: 3.716361]\n",
      "epoch:43 step:33838 [D loss: 0.129408, acc.: 96.88%] [G loss: 2.716813]\n",
      "epoch:43 step:33839 [D loss: 0.531128, acc.: 74.22%] [G loss: 5.125333]\n",
      "epoch:43 step:33840 [D loss: 0.134058, acc.: 92.97%] [G loss: 5.900088]\n",
      "epoch:43 step:33841 [D loss: 0.086669, acc.: 97.66%] [G loss: 5.080468]\n",
      "epoch:43 step:33842 [D loss: 0.236197, acc.: 91.41%] [G loss: 2.818965]\n",
      "epoch:43 step:33843 [D loss: 0.125000, acc.: 96.09%] [G loss: 3.750527]\n",
      "epoch:43 step:33844 [D loss: 0.039659, acc.: 100.00%] [G loss: 4.225368]\n",
      "epoch:43 step:33845 [D loss: 0.043663, acc.: 100.00%] [G loss: 4.052932]\n",
      "epoch:43 step:33846 [D loss: 0.126981, acc.: 96.09%] [G loss: 1.898376]\n",
      "epoch:43 step:33847 [D loss: 0.289936, acc.: 88.28%] [G loss: 2.426811]\n",
      "epoch:43 step:33848 [D loss: 0.070856, acc.: 97.66%] [G loss: 4.328032]\n",
      "epoch:43 step:33849 [D loss: 0.270637, acc.: 92.19%] [G loss: 2.682851]\n",
      "epoch:43 step:33850 [D loss: 2.346657, acc.: 14.06%] [G loss: 5.198121]\n",
      "epoch:43 step:33851 [D loss: 0.217525, acc.: 90.62%] [G loss: 6.326810]\n",
      "epoch:43 step:33852 [D loss: 1.122276, acc.: 56.25%] [G loss: 4.183261]\n",
      "epoch:43 step:33853 [D loss: 0.053789, acc.: 100.00%] [G loss: 3.580387]\n",
      "epoch:43 step:33854 [D loss: 0.060006, acc.: 99.22%] [G loss: 3.422215]\n",
      "epoch:43 step:33855 [D loss: 0.084862, acc.: 97.66%] [G loss: 4.358383]\n",
      "epoch:43 step:33856 [D loss: 0.069806, acc.: 99.22%] [G loss: 4.036407]\n",
      "epoch:43 step:33857 [D loss: 0.171330, acc.: 94.53%] [G loss: 3.145202]\n",
      "epoch:43 step:33858 [D loss: 0.022878, acc.: 100.00%] [G loss: 2.880471]\n",
      "epoch:43 step:33859 [D loss: 0.073218, acc.: 97.66%] [G loss: 3.054130]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:43 step:33860 [D loss: 0.144176, acc.: 96.88%] [G loss: 3.195093]\n",
      "epoch:43 step:33861 [D loss: 0.056484, acc.: 99.22%] [G loss: 3.821628]\n",
      "epoch:43 step:33862 [D loss: 0.036096, acc.: 99.22%] [G loss: 3.453165]\n",
      "epoch:43 step:33863 [D loss: 0.133922, acc.: 97.66%] [G loss: 2.432690]\n",
      "epoch:43 step:33864 [D loss: 0.117527, acc.: 98.44%] [G loss: 1.803829]\n",
      "epoch:43 step:33865 [D loss: 0.036477, acc.: 100.00%] [G loss: 3.150916]\n",
      "epoch:43 step:33866 [D loss: 0.144787, acc.: 96.09%] [G loss: 2.627511]\n",
      "epoch:43 step:33867 [D loss: 0.055343, acc.: 100.00%] [G loss: 1.920118]\n",
      "epoch:43 step:33868 [D loss: 0.123526, acc.: 98.44%] [G loss: 2.895910]\n",
      "epoch:43 step:33869 [D loss: 0.045732, acc.: 100.00%] [G loss: 3.312973]\n",
      "epoch:43 step:33870 [D loss: 0.131248, acc.: 95.31%] [G loss: 1.731155]\n",
      "epoch:43 step:33871 [D loss: 0.263911, acc.: 89.06%] [G loss: 4.776286]\n",
      "epoch:43 step:33872 [D loss: 0.249839, acc.: 85.16%] [G loss: 3.873493]\n",
      "epoch:43 step:33873 [D loss: 0.102132, acc.: 96.09%] [G loss: 3.190525]\n",
      "epoch:43 step:33874 [D loss: 0.046295, acc.: 100.00%] [G loss: 3.164468]\n",
      "epoch:43 step:33875 [D loss: 0.059200, acc.: 98.44%] [G loss: 3.272178]\n",
      "epoch:43 step:33876 [D loss: 0.066528, acc.: 99.22%] [G loss: 2.654965]\n",
      "epoch:43 step:33877 [D loss: 0.034542, acc.: 99.22%] [G loss: 2.922554]\n",
      "epoch:43 step:33878 [D loss: 0.092071, acc.: 99.22%] [G loss: 2.319467]\n",
      "epoch:43 step:33879 [D loss: 0.050748, acc.: 100.00%] [G loss: 3.378612]\n",
      "epoch:43 step:33880 [D loss: 0.191588, acc.: 92.19%] [G loss: 2.748827]\n",
      "epoch:43 step:33881 [D loss: 0.065288, acc.: 99.22%] [G loss: 3.164235]\n",
      "epoch:43 step:33882 [D loss: 0.121040, acc.: 97.66%] [G loss: 3.601888]\n",
      "epoch:43 step:33883 [D loss: 0.089265, acc.: 99.22%] [G loss: 3.474181]\n",
      "epoch:43 step:33884 [D loss: 0.080406, acc.: 99.22%] [G loss: 3.612678]\n",
      "epoch:43 step:33885 [D loss: 0.182037, acc.: 92.97%] [G loss: 3.636325]\n",
      "epoch:43 step:33886 [D loss: 0.028173, acc.: 100.00%] [G loss: 4.945927]\n",
      "epoch:43 step:33887 [D loss: 0.151396, acc.: 94.53%] [G loss: 2.697827]\n",
      "epoch:43 step:33888 [D loss: 0.050206, acc.: 100.00%] [G loss: 2.916030]\n",
      "epoch:43 step:33889 [D loss: 0.085813, acc.: 98.44%] [G loss: 3.626515]\n",
      "epoch:43 step:33890 [D loss: 0.029768, acc.: 100.00%] [G loss: 4.591524]\n",
      "epoch:43 step:33891 [D loss: 0.086271, acc.: 96.88%] [G loss: 4.091648]\n",
      "epoch:43 step:33892 [D loss: 0.092071, acc.: 97.66%] [G loss: 4.063414]\n",
      "epoch:43 step:33893 [D loss: 0.164873, acc.: 95.31%] [G loss: 3.629550]\n",
      "epoch:43 step:33894 [D loss: 0.082784, acc.: 100.00%] [G loss: 4.309954]\n",
      "epoch:43 step:33895 [D loss: 0.131745, acc.: 94.53%] [G loss: 3.520608]\n",
      "epoch:43 step:33896 [D loss: 0.057038, acc.: 98.44%] [G loss: 3.463202]\n",
      "epoch:43 step:33897 [D loss: 0.023275, acc.: 100.00%] [G loss: 4.143392]\n",
      "epoch:43 step:33898 [D loss: 0.228320, acc.: 90.62%] [G loss: 3.551185]\n",
      "epoch:43 step:33899 [D loss: 0.016710, acc.: 100.00%] [G loss: 4.121994]\n",
      "epoch:43 step:33900 [D loss: 0.037801, acc.: 99.22%] [G loss: 3.622219]\n",
      "epoch:43 step:33901 [D loss: 0.050929, acc.: 100.00%] [G loss: 3.723924]\n",
      "epoch:43 step:33902 [D loss: 0.248633, acc.: 92.19%] [G loss: 2.986859]\n",
      "epoch:43 step:33903 [D loss: 0.414246, acc.: 75.78%] [G loss: 5.745323]\n",
      "epoch:43 step:33904 [D loss: 0.078496, acc.: 98.44%] [G loss: 6.309895]\n",
      "epoch:43 step:33905 [D loss: 0.100253, acc.: 96.09%] [G loss: 5.399333]\n",
      "epoch:43 step:33906 [D loss: 0.080055, acc.: 98.44%] [G loss: 4.012287]\n",
      "epoch:43 step:33907 [D loss: 0.015897, acc.: 100.00%] [G loss: 3.382875]\n",
      "epoch:43 step:33908 [D loss: 0.077524, acc.: 97.66%] [G loss: 3.706186]\n",
      "epoch:43 step:33909 [D loss: 0.019554, acc.: 100.00%] [G loss: 4.183535]\n",
      "epoch:43 step:33910 [D loss: 0.009157, acc.: 100.00%] [G loss: 4.364533]\n",
      "epoch:43 step:33911 [D loss: 0.037638, acc.: 98.44%] [G loss: 3.599379]\n",
      "epoch:43 step:33912 [D loss: 0.015547, acc.: 100.00%] [G loss: 3.924180]\n",
      "epoch:43 step:33913 [D loss: 0.046679, acc.: 100.00%] [G loss: 3.490598]\n",
      "epoch:43 step:33914 [D loss: 0.038661, acc.: 100.00%] [G loss: 3.993953]\n",
      "epoch:43 step:33915 [D loss: 0.295244, acc.: 87.50%] [G loss: 3.680880]\n",
      "epoch:43 step:33916 [D loss: 0.242082, acc.: 91.41%] [G loss: 3.257688]\n",
      "epoch:43 step:33917 [D loss: 0.252372, acc.: 91.41%] [G loss: 3.032950]\n",
      "epoch:43 step:33918 [D loss: 0.041947, acc.: 100.00%] [G loss: 4.193952]\n",
      "epoch:43 step:33919 [D loss: 0.030316, acc.: 100.00%] [G loss: 3.196424]\n",
      "epoch:43 step:33920 [D loss: 0.125331, acc.: 96.09%] [G loss: 2.914877]\n",
      "epoch:43 step:33921 [D loss: 0.052267, acc.: 100.00%] [G loss: 3.896328]\n",
      "epoch:43 step:33922 [D loss: 0.276753, acc.: 89.84%] [G loss: 4.556695]\n",
      "epoch:43 step:33923 [D loss: 0.025299, acc.: 100.00%] [G loss: 4.886164]\n",
      "epoch:43 step:33924 [D loss: 0.022700, acc.: 100.00%] [G loss: 4.576887]\n",
      "epoch:43 step:33925 [D loss: 0.026560, acc.: 100.00%] [G loss: 4.641576]\n",
      "epoch:43 step:33926 [D loss: 0.084042, acc.: 96.88%] [G loss: 3.060093]\n",
      "epoch:43 step:33927 [D loss: 0.030411, acc.: 100.00%] [G loss: 3.044641]\n",
      "epoch:43 step:33928 [D loss: 0.030452, acc.: 99.22%] [G loss: 4.170238]\n",
      "epoch:43 step:33929 [D loss: 0.052151, acc.: 99.22%] [G loss: 3.981185]\n",
      "epoch:43 step:33930 [D loss: 0.042785, acc.: 99.22%] [G loss: 3.701130]\n",
      "epoch:43 step:33931 [D loss: 0.029700, acc.: 100.00%] [G loss: 4.358239]\n",
      "epoch:43 step:33932 [D loss: 0.037792, acc.: 100.00%] [G loss: 3.631473]\n",
      "epoch:43 step:33933 [D loss: 0.627303, acc.: 71.88%] [G loss: 4.034510]\n",
      "epoch:43 step:33934 [D loss: 0.196049, acc.: 92.19%] [G loss: 4.731870]\n",
      "epoch:43 step:33935 [D loss: 0.022709, acc.: 100.00%] [G loss: 4.579876]\n",
      "epoch:43 step:33936 [D loss: 0.036762, acc.: 100.00%] [G loss: 3.993386]\n",
      "epoch:43 step:33937 [D loss: 0.028401, acc.: 99.22%] [G loss: 4.279839]\n",
      "epoch:43 step:33938 [D loss: 0.086687, acc.: 98.44%] [G loss: 4.553425]\n",
      "epoch:43 step:33939 [D loss: 0.008366, acc.: 100.00%] [G loss: 4.376118]\n",
      "epoch:43 step:33940 [D loss: 0.007734, acc.: 100.00%] [G loss: 3.810751]\n",
      "epoch:43 step:33941 [D loss: 0.003848, acc.: 100.00%] [G loss: 3.217187]\n",
      "epoch:43 step:33942 [D loss: 0.054128, acc.: 98.44%] [G loss: 2.821722]\n",
      "epoch:43 step:33943 [D loss: 0.026170, acc.: 100.00%] [G loss: 2.299818]\n",
      "epoch:43 step:33944 [D loss: 0.031310, acc.: 100.00%] [G loss: 1.671804]\n",
      "epoch:43 step:33945 [D loss: 0.018039, acc.: 100.00%] [G loss: 2.769795]\n",
      "epoch:43 step:33946 [D loss: 0.048963, acc.: 100.00%] [G loss: 1.930507]\n",
      "epoch:43 step:33947 [D loss: 0.907186, acc.: 54.69%] [G loss: 5.265109]\n",
      "epoch:43 step:33948 [D loss: 0.079733, acc.: 98.44%] [G loss: 6.955966]\n",
      "epoch:43 step:33949 [D loss: 0.311836, acc.: 84.38%] [G loss: 3.769048]\n",
      "epoch:43 step:33950 [D loss: 0.231936, acc.: 86.72%] [G loss: 6.081114]\n",
      "epoch:43 step:33951 [D loss: 0.004924, acc.: 100.00%] [G loss: 6.447145]\n",
      "epoch:43 step:33952 [D loss: 0.110871, acc.: 96.09%] [G loss: 5.422072]\n",
      "epoch:43 step:33953 [D loss: 0.036847, acc.: 99.22%] [G loss: 5.357566]\n",
      "epoch:43 step:33954 [D loss: 0.033443, acc.: 100.00%] [G loss: 4.143229]\n",
      "epoch:43 step:33955 [D loss: 0.010074, acc.: 100.00%] [G loss: 5.146192]\n",
      "epoch:43 step:33956 [D loss: 0.012001, acc.: 100.00%] [G loss: 4.602032]\n",
      "epoch:43 step:33957 [D loss: 0.050982, acc.: 99.22%] [G loss: 3.107982]\n",
      "epoch:43 step:33958 [D loss: 0.035218, acc.: 100.00%] [G loss: 3.210745]\n",
      "epoch:43 step:33959 [D loss: 0.045233, acc.: 100.00%] [G loss: 4.789709]\n",
      "epoch:43 step:33960 [D loss: 0.020532, acc.: 100.00%] [G loss: 4.716015]\n",
      "epoch:43 step:33961 [D loss: 0.020026, acc.: 99.22%] [G loss: 4.534119]\n",
      "epoch:43 step:33962 [D loss: 0.012064, acc.: 100.00%] [G loss: 4.409044]\n",
      "epoch:43 step:33963 [D loss: 0.019326, acc.: 100.00%] [G loss: 4.802354]\n",
      "epoch:43 step:33964 [D loss: 0.037058, acc.: 100.00%] [G loss: 4.802892]\n",
      "epoch:43 step:33965 [D loss: 0.072656, acc.: 99.22%] [G loss: 4.552796]\n",
      "epoch:43 step:33966 [D loss: 0.042214, acc.: 100.00%] [G loss: 4.671114]\n",
      "epoch:43 step:33967 [D loss: 0.010329, acc.: 100.00%] [G loss: 5.103493]\n",
      "epoch:43 step:33968 [D loss: 0.116184, acc.: 96.09%] [G loss: 2.606735]\n",
      "epoch:43 step:33969 [D loss: 0.116158, acc.: 96.88%] [G loss: 5.257563]\n",
      "epoch:43 step:33970 [D loss: 0.012883, acc.: 100.00%] [G loss: 6.173888]\n",
      "epoch:43 step:33971 [D loss: 0.015773, acc.: 100.00%] [G loss: 6.176421]\n",
      "epoch:43 step:33972 [D loss: 0.004959, acc.: 100.00%] [G loss: 6.076768]\n",
      "epoch:43 step:33973 [D loss: 0.045656, acc.: 100.00%] [G loss: 4.605541]\n",
      "epoch:43 step:33974 [D loss: 0.027964, acc.: 99.22%] [G loss: 4.748579]\n",
      "epoch:43 step:33975 [D loss: 0.007485, acc.: 100.00%] [G loss: 4.671017]\n",
      "epoch:43 step:33976 [D loss: 0.353156, acc.: 82.03%] [G loss: 5.728088]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:43 step:33977 [D loss: 0.067909, acc.: 99.22%] [G loss: 6.982986]\n",
      "epoch:43 step:33978 [D loss: 0.039400, acc.: 99.22%] [G loss: 6.039231]\n",
      "epoch:43 step:33979 [D loss: 0.016215, acc.: 100.00%] [G loss: 5.141508]\n",
      "epoch:43 step:33980 [D loss: 0.013132, acc.: 100.00%] [G loss: 4.891778]\n",
      "epoch:43 step:33981 [D loss: 0.012611, acc.: 100.00%] [G loss: 4.669841]\n",
      "epoch:43 step:33982 [D loss: 0.010983, acc.: 100.00%] [G loss: 5.920631]\n",
      "epoch:43 step:33983 [D loss: 0.005987, acc.: 100.00%] [G loss: 5.365674]\n",
      "epoch:43 step:33984 [D loss: 0.095226, acc.: 97.66%] [G loss: 4.530417]\n",
      "epoch:43 step:33985 [D loss: 0.010558, acc.: 100.00%] [G loss: 3.924538]\n",
      "epoch:43 step:33986 [D loss: 0.030898, acc.: 100.00%] [G loss: 4.084136]\n",
      "epoch:43 step:33987 [D loss: 0.097419, acc.: 98.44%] [G loss: 6.499228]\n",
      "epoch:43 step:33988 [D loss: 0.267420, acc.: 89.06%] [G loss: 2.464028]\n",
      "epoch:43 step:33989 [D loss: 0.149497, acc.: 93.75%] [G loss: 6.524273]\n",
      "epoch:43 step:33990 [D loss: 0.002757, acc.: 100.00%] [G loss: 7.416039]\n",
      "epoch:43 step:33991 [D loss: 0.173767, acc.: 89.84%] [G loss: 4.077145]\n",
      "epoch:43 step:33992 [D loss: 0.049075, acc.: 99.22%] [G loss: 5.010525]\n",
      "epoch:43 step:33993 [D loss: 0.007094, acc.: 100.00%] [G loss: 5.670542]\n",
      "epoch:43 step:33994 [D loss: 0.058061, acc.: 99.22%] [G loss: 5.183918]\n",
      "epoch:43 step:33995 [D loss: 0.009181, acc.: 100.00%] [G loss: 5.928713]\n",
      "epoch:43 step:33996 [D loss: 0.053708, acc.: 99.22%] [G loss: 3.939244]\n",
      "epoch:43 step:33997 [D loss: 0.031410, acc.: 99.22%] [G loss: 4.565670]\n",
      "epoch:43 step:33998 [D loss: 0.002078, acc.: 100.00%] [G loss: 5.839637]\n",
      "epoch:43 step:33999 [D loss: 0.006563, acc.: 100.00%] [G loss: 4.606739]\n",
      "epoch:43 step:34000 [D loss: 0.040311, acc.: 100.00%] [G loss: 2.690601]\n",
      "##############\n",
      "[0.86447584 0.70331718 0.86876484 0.90473216 0.81578477 1.01291755\n",
      " 2.11964236 2.11402612 2.09303855 2.11637187]\n",
      "##########\n",
      "epoch:43 step:34001 [D loss: 0.009770, acc.: 100.00%] [G loss: 3.504542]\n",
      "epoch:43 step:34002 [D loss: 0.059332, acc.: 100.00%] [G loss: 5.008199]\n",
      "epoch:43 step:34003 [D loss: 2.132183, acc.: 30.47%] [G loss: 10.706824]\n",
      "epoch:43 step:34004 [D loss: 3.281968, acc.: 50.00%] [G loss: 8.038960]\n",
      "epoch:43 step:34005 [D loss: 2.149416, acc.: 50.78%] [G loss: 4.180358]\n",
      "epoch:43 step:34006 [D loss: 0.195904, acc.: 91.41%] [G loss: 3.140706]\n",
      "epoch:43 step:34007 [D loss: 0.180427, acc.: 93.75%] [G loss: 3.270523]\n",
      "epoch:43 step:34008 [D loss: 0.062565, acc.: 99.22%] [G loss: 3.415644]\n",
      "epoch:43 step:34009 [D loss: 0.050612, acc.: 100.00%] [G loss: 3.028009]\n",
      "epoch:43 step:34010 [D loss: 0.023612, acc.: 100.00%] [G loss: 3.850090]\n",
      "epoch:43 step:34011 [D loss: 0.024352, acc.: 100.00%] [G loss: 3.216524]\n",
      "epoch:43 step:34012 [D loss: 0.026246, acc.: 100.00%] [G loss: 3.713720]\n",
      "epoch:43 step:34013 [D loss: 0.088356, acc.: 98.44%] [G loss: 3.548883]\n",
      "epoch:43 step:34014 [D loss: 0.078005, acc.: 98.44%] [G loss: 3.478064]\n",
      "epoch:43 step:34015 [D loss: 0.025693, acc.: 100.00%] [G loss: 3.612383]\n",
      "epoch:43 step:34016 [D loss: 0.017980, acc.: 100.00%] [G loss: 3.656801]\n",
      "epoch:43 step:34017 [D loss: 0.011791, acc.: 100.00%] [G loss: 4.072582]\n",
      "epoch:43 step:34018 [D loss: 0.028909, acc.: 100.00%] [G loss: 3.313959]\n",
      "epoch:43 step:34019 [D loss: 0.056243, acc.: 100.00%] [G loss: 3.593765]\n",
      "epoch:43 step:34020 [D loss: 0.059031, acc.: 100.00%] [G loss: 3.077750]\n",
      "epoch:43 step:34021 [D loss: 0.050674, acc.: 100.00%] [G loss: 3.122865]\n",
      "epoch:43 step:34022 [D loss: 0.082309, acc.: 99.22%] [G loss: 4.093612]\n",
      "epoch:43 step:34023 [D loss: 0.024671, acc.: 100.00%] [G loss: 4.328783]\n",
      "epoch:43 step:34024 [D loss: 0.274448, acc.: 87.50%] [G loss: 2.516994]\n",
      "epoch:43 step:34025 [D loss: 0.092906, acc.: 100.00%] [G loss: 3.393950]\n",
      "epoch:43 step:34026 [D loss: 0.053165, acc.: 100.00%] [G loss: 4.227012]\n",
      "epoch:43 step:34027 [D loss: 0.032465, acc.: 100.00%] [G loss: 4.229577]\n",
      "epoch:43 step:34028 [D loss: 0.027591, acc.: 99.22%] [G loss: 3.670269]\n",
      "epoch:43 step:34029 [D loss: 0.017062, acc.: 100.00%] [G loss: 3.597343]\n",
      "epoch:43 step:34030 [D loss: 0.048261, acc.: 100.00%] [G loss: 3.698533]\n",
      "epoch:43 step:34031 [D loss: 0.332920, acc.: 87.50%] [G loss: 4.083459]\n",
      "epoch:43 step:34032 [D loss: 0.007752, acc.: 100.00%] [G loss: 5.555326]\n",
      "epoch:43 step:34033 [D loss: 0.021532, acc.: 100.00%] [G loss: 5.403436]\n",
      "epoch:43 step:34034 [D loss: 0.071538, acc.: 99.22%] [G loss: 3.874154]\n",
      "epoch:43 step:34035 [D loss: 0.023329, acc.: 100.00%] [G loss: 3.645023]\n",
      "epoch:43 step:34036 [D loss: 0.030795, acc.: 100.00%] [G loss: 3.737020]\n",
      "epoch:43 step:34037 [D loss: 0.013821, acc.: 100.00%] [G loss: 3.614238]\n",
      "epoch:43 step:34038 [D loss: 0.017183, acc.: 100.00%] [G loss: 3.787812]\n",
      "epoch:43 step:34039 [D loss: 0.029016, acc.: 100.00%] [G loss: 4.330091]\n",
      "epoch:43 step:34040 [D loss: 0.022055, acc.: 100.00%] [G loss: 4.440794]\n",
      "epoch:43 step:34041 [D loss: 0.020154, acc.: 100.00%] [G loss: 4.580874]\n",
      "epoch:43 step:34042 [D loss: 0.085414, acc.: 98.44%] [G loss: 3.152088]\n",
      "epoch:43 step:34043 [D loss: 0.051389, acc.: 99.22%] [G loss: 3.167595]\n",
      "epoch:43 step:34044 [D loss: 0.022902, acc.: 100.00%] [G loss: 3.924361]\n",
      "epoch:43 step:34045 [D loss: 0.015483, acc.: 100.00%] [G loss: 3.904690]\n",
      "epoch:43 step:34046 [D loss: 0.009798, acc.: 100.00%] [G loss: 4.576669]\n",
      "epoch:43 step:34047 [D loss: 0.044329, acc.: 100.00%] [G loss: 3.761556]\n",
      "epoch:43 step:34048 [D loss: 0.019623, acc.: 100.00%] [G loss: 3.438415]\n",
      "epoch:43 step:34049 [D loss: 0.013814, acc.: 100.00%] [G loss: 4.492366]\n",
      "epoch:43 step:34050 [D loss: 0.019822, acc.: 100.00%] [G loss: 3.707830]\n",
      "epoch:43 step:34051 [D loss: 0.100723, acc.: 96.88%] [G loss: 3.258677]\n",
      "epoch:43 step:34052 [D loss: 0.041680, acc.: 100.00%] [G loss: 5.097122]\n",
      "epoch:43 step:34053 [D loss: 0.006577, acc.: 100.00%] [G loss: 5.828325]\n",
      "epoch:43 step:34054 [D loss: 0.570335, acc.: 76.56%] [G loss: 6.607891]\n",
      "epoch:43 step:34055 [D loss: 0.014278, acc.: 100.00%] [G loss: 6.855760]\n",
      "epoch:43 step:34056 [D loss: 0.510511, acc.: 73.44%] [G loss: 2.333432]\n",
      "epoch:43 step:34057 [D loss: 0.349737, acc.: 78.91%] [G loss: 5.051101]\n",
      "epoch:43 step:34058 [D loss: 0.035173, acc.: 100.00%] [G loss: 6.239971]\n",
      "epoch:43 step:34059 [D loss: 0.096449, acc.: 96.09%] [G loss: 5.282820]\n",
      "epoch:43 step:34060 [D loss: 0.010567, acc.: 100.00%] [G loss: 4.247861]\n",
      "epoch:43 step:34061 [D loss: 0.018472, acc.: 100.00%] [G loss: 4.544725]\n",
      "epoch:43 step:34062 [D loss: 0.009276, acc.: 100.00%] [G loss: 4.611046]\n",
      "epoch:43 step:34063 [D loss: 0.015193, acc.: 100.00%] [G loss: 4.676089]\n",
      "epoch:43 step:34064 [D loss: 0.024319, acc.: 100.00%] [G loss: 4.634824]\n",
      "epoch:43 step:34065 [D loss: 0.037329, acc.: 100.00%] [G loss: 4.180493]\n",
      "epoch:43 step:34066 [D loss: 0.037520, acc.: 98.44%] [G loss: 3.311299]\n",
      "epoch:43 step:34067 [D loss: 0.031370, acc.: 99.22%] [G loss: 3.988131]\n",
      "epoch:43 step:34068 [D loss: 0.276022, acc.: 92.97%] [G loss: 4.100088]\n",
      "epoch:43 step:34069 [D loss: 0.006111, acc.: 100.00%] [G loss: 5.377254]\n",
      "epoch:43 step:34070 [D loss: 0.009179, acc.: 100.00%] [G loss: 5.660613]\n",
      "epoch:43 step:34071 [D loss: 0.013581, acc.: 100.00%] [G loss: 5.428635]\n",
      "epoch:43 step:34072 [D loss: 0.012739, acc.: 100.00%] [G loss: 5.324148]\n",
      "epoch:43 step:34073 [D loss: 0.008820, acc.: 100.00%] [G loss: 4.696036]\n",
      "epoch:43 step:34074 [D loss: 0.037316, acc.: 100.00%] [G loss: 4.322928]\n",
      "epoch:43 step:34075 [D loss: 0.047980, acc.: 100.00%] [G loss: 4.723571]\n",
      "epoch:43 step:34076 [D loss: 0.011992, acc.: 100.00%] [G loss: 4.625909]\n",
      "epoch:43 step:34077 [D loss: 0.006778, acc.: 100.00%] [G loss: 4.379539]\n",
      "epoch:43 step:34078 [D loss: 0.031620, acc.: 100.00%] [G loss: 4.665887]\n",
      "epoch:43 step:34079 [D loss: 0.003038, acc.: 100.00%] [G loss: 4.838572]\n",
      "epoch:43 step:34080 [D loss: 0.022887, acc.: 99.22%] [G loss: 4.544687]\n",
      "epoch:43 step:34081 [D loss: 0.028885, acc.: 99.22%] [G loss: 4.846687]\n",
      "epoch:43 step:34082 [D loss: 0.012408, acc.: 100.00%] [G loss: 4.574342]\n",
      "epoch:43 step:34083 [D loss: 0.015903, acc.: 100.00%] [G loss: 5.040554]\n",
      "epoch:43 step:34084 [D loss: 0.100323, acc.: 98.44%] [G loss: 2.599944]\n",
      "epoch:43 step:34085 [D loss: 0.043220, acc.: 100.00%] [G loss: 4.771525]\n",
      "epoch:43 step:34086 [D loss: 0.002202, acc.: 100.00%] [G loss: 4.914665]\n",
      "epoch:43 step:34087 [D loss: 0.011101, acc.: 100.00%] [G loss: 5.207469]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:43 step:34088 [D loss: 0.015413, acc.: 100.00%] [G loss: 4.880096]\n",
      "epoch:43 step:34089 [D loss: 0.010025, acc.: 100.00%] [G loss: 4.711970]\n",
      "epoch:43 step:34090 [D loss: 0.012384, acc.: 100.00%] [G loss: 4.455608]\n",
      "epoch:43 step:34091 [D loss: 0.086545, acc.: 97.66%] [G loss: 3.642892]\n",
      "epoch:43 step:34092 [D loss: 0.284477, acc.: 89.84%] [G loss: 5.287641]\n",
      "epoch:43 step:34093 [D loss: 0.004299, acc.: 100.00%] [G loss: 6.864369]\n",
      "epoch:43 step:34094 [D loss: 0.895042, acc.: 59.38%] [G loss: 6.562427]\n",
      "epoch:43 step:34095 [D loss: 0.006697, acc.: 100.00%] [G loss: 6.568537]\n",
      "epoch:43 step:34096 [D loss: 0.072262, acc.: 97.66%] [G loss: 6.221149]\n",
      "epoch:43 step:34097 [D loss: 0.002515, acc.: 100.00%] [G loss: 5.663819]\n",
      "epoch:43 step:34098 [D loss: 0.009379, acc.: 100.00%] [G loss: 5.431347]\n",
      "epoch:43 step:34099 [D loss: 0.008676, acc.: 100.00%] [G loss: 5.701845]\n",
      "epoch:43 step:34100 [D loss: 0.004647, acc.: 100.00%] [G loss: 4.530182]\n",
      "epoch:43 step:34101 [D loss: 0.004805, acc.: 100.00%] [G loss: 4.412644]\n",
      "epoch:43 step:34102 [D loss: 0.023341, acc.: 99.22%] [G loss: 3.222529]\n",
      "epoch:43 step:34103 [D loss: 0.009624, acc.: 100.00%] [G loss: 2.766016]\n",
      "epoch:43 step:34104 [D loss: 0.011986, acc.: 100.00%] [G loss: 2.526175]\n",
      "epoch:43 step:34105 [D loss: 0.011624, acc.: 100.00%] [G loss: 1.810750]\n",
      "epoch:43 step:34106 [D loss: 0.023129, acc.: 100.00%] [G loss: 1.338066]\n",
      "epoch:43 step:34107 [D loss: 0.088021, acc.: 98.44%] [G loss: 4.480842]\n",
      "epoch:43 step:34108 [D loss: 0.024081, acc.: 100.00%] [G loss: 5.420247]\n",
      "epoch:43 step:34109 [D loss: 0.079118, acc.: 97.66%] [G loss: 1.759432]\n",
      "epoch:43 step:34110 [D loss: 0.014728, acc.: 100.00%] [G loss: 3.221140]\n",
      "epoch:43 step:34111 [D loss: 0.027853, acc.: 100.00%] [G loss: 3.126314]\n",
      "epoch:43 step:34112 [D loss: 0.003680, acc.: 100.00%] [G loss: 3.815354]\n",
      "epoch:43 step:34113 [D loss: 0.017272, acc.: 100.00%] [G loss: 4.181861]\n",
      "epoch:43 step:34114 [D loss: 0.034723, acc.: 100.00%] [G loss: 4.194396]\n",
      "epoch:43 step:34115 [D loss: 0.024138, acc.: 100.00%] [G loss: 4.469229]\n",
      "epoch:43 step:34116 [D loss: 1.675059, acc.: 43.75%] [G loss: 8.304708]\n",
      "epoch:43 step:34117 [D loss: 1.597221, acc.: 53.12%] [G loss: 5.774956]\n",
      "epoch:43 step:34118 [D loss: 0.444728, acc.: 82.81%] [G loss: 3.732937]\n",
      "epoch:43 step:34119 [D loss: 0.048467, acc.: 99.22%] [G loss: 4.156119]\n",
      "epoch:43 step:34120 [D loss: 0.046411, acc.: 100.00%] [G loss: 4.269199]\n",
      "epoch:43 step:34121 [D loss: 0.022103, acc.: 100.00%] [G loss: 4.282809]\n",
      "epoch:43 step:34122 [D loss: 0.034905, acc.: 99.22%] [G loss: 4.507856]\n",
      "epoch:43 step:34123 [D loss: 0.077016, acc.: 96.88%] [G loss: 3.390453]\n",
      "epoch:43 step:34124 [D loss: 0.075289, acc.: 96.88%] [G loss: 3.531320]\n",
      "epoch:43 step:34125 [D loss: 0.174562, acc.: 95.31%] [G loss: 3.767542]\n",
      "epoch:43 step:34126 [D loss: 0.137689, acc.: 95.31%] [G loss: 2.317466]\n",
      "epoch:43 step:34127 [D loss: 0.467866, acc.: 74.22%] [G loss: 6.074597]\n",
      "epoch:43 step:34128 [D loss: 0.264967, acc.: 84.38%] [G loss: 5.787194]\n",
      "epoch:43 step:34129 [D loss: 0.303485, acc.: 89.84%] [G loss: 2.405220]\n",
      "epoch:43 step:34130 [D loss: 0.174590, acc.: 92.97%] [G loss: 3.101180]\n",
      "epoch:43 step:34131 [D loss: 0.015694, acc.: 100.00%] [G loss: 4.381164]\n",
      "epoch:43 step:34132 [D loss: 0.017375, acc.: 100.00%] [G loss: 3.451867]\n",
      "epoch:43 step:34133 [D loss: 0.019850, acc.: 100.00%] [G loss: 3.119358]\n",
      "epoch:43 step:34134 [D loss: 0.038858, acc.: 99.22%] [G loss: 1.792082]\n",
      "epoch:43 step:34135 [D loss: 0.040133, acc.: 100.00%] [G loss: 1.851082]\n",
      "epoch:43 step:34136 [D loss: 0.042233, acc.: 100.00%] [G loss: 1.535863]\n",
      "epoch:43 step:34137 [D loss: 0.047641, acc.: 100.00%] [G loss: 2.739709]\n",
      "epoch:43 step:34138 [D loss: 0.062455, acc.: 96.88%] [G loss: 1.799654]\n",
      "epoch:43 step:34139 [D loss: 0.055469, acc.: 100.00%] [G loss: 1.751716]\n",
      "epoch:43 step:34140 [D loss: 0.177923, acc.: 92.97%] [G loss: 4.967615]\n",
      "epoch:43 step:34141 [D loss: 0.200837, acc.: 92.19%] [G loss: 2.871373]\n",
      "epoch:43 step:34142 [D loss: 0.013896, acc.: 100.00%] [G loss: 1.798108]\n",
      "epoch:43 step:34143 [D loss: 0.060048, acc.: 99.22%] [G loss: 1.900525]\n",
      "epoch:43 step:34144 [D loss: 0.043273, acc.: 100.00%] [G loss: 4.929658]\n",
      "epoch:43 step:34145 [D loss: 0.154126, acc.: 92.97%] [G loss: 2.045347]\n",
      "epoch:43 step:34146 [D loss: 0.166948, acc.: 91.41%] [G loss: 5.218022]\n",
      "epoch:43 step:34147 [D loss: 0.068138, acc.: 98.44%] [G loss: 5.750821]\n",
      "epoch:43 step:34148 [D loss: 0.316175, acc.: 89.06%] [G loss: 3.106559]\n",
      "epoch:43 step:34149 [D loss: 0.006469, acc.: 100.00%] [G loss: 2.799143]\n",
      "epoch:43 step:34150 [D loss: 0.290986, acc.: 84.38%] [G loss: 6.737616]\n",
      "epoch:43 step:34151 [D loss: 0.047784, acc.: 99.22%] [G loss: 7.575920]\n",
      "epoch:43 step:34152 [D loss: 0.422313, acc.: 82.03%] [G loss: 4.642172]\n",
      "epoch:43 step:34153 [D loss: 0.044540, acc.: 98.44%] [G loss: 4.042605]\n",
      "epoch:43 step:34154 [D loss: 0.068063, acc.: 98.44%] [G loss: 3.825783]\n",
      "epoch:43 step:34155 [D loss: 0.011102, acc.: 100.00%] [G loss: 4.893697]\n",
      "epoch:43 step:34156 [D loss: 0.139232, acc.: 94.53%] [G loss: 3.512368]\n",
      "epoch:43 step:34157 [D loss: 0.070960, acc.: 98.44%] [G loss: 4.067751]\n",
      "epoch:43 step:34158 [D loss: 0.008399, acc.: 100.00%] [G loss: 5.049299]\n",
      "epoch:43 step:34159 [D loss: 0.026295, acc.: 99.22%] [G loss: 4.900661]\n",
      "epoch:43 step:34160 [D loss: 0.022852, acc.: 100.00%] [G loss: 4.139762]\n",
      "epoch:43 step:34161 [D loss: 0.018575, acc.: 100.00%] [G loss: 4.300298]\n",
      "epoch:43 step:34162 [D loss: 0.017153, acc.: 100.00%] [G loss: 4.561577]\n",
      "epoch:43 step:34163 [D loss: 0.034781, acc.: 100.00%] [G loss: 4.363255]\n",
      "epoch:43 step:34164 [D loss: 0.053879, acc.: 98.44%] [G loss: 3.332382]\n",
      "epoch:43 step:34165 [D loss: 0.037520, acc.: 100.00%] [G loss: 4.292719]\n",
      "epoch:43 step:34166 [D loss: 0.049081, acc.: 99.22%] [G loss: 3.665635]\n",
      "epoch:43 step:34167 [D loss: 0.034362, acc.: 100.00%] [G loss: 4.384128]\n",
      "epoch:43 step:34168 [D loss: 0.089278, acc.: 98.44%] [G loss: 4.856258]\n",
      "epoch:43 step:34169 [D loss: 0.019332, acc.: 100.00%] [G loss: 3.783607]\n",
      "epoch:43 step:34170 [D loss: 0.086205, acc.: 96.88%] [G loss: 2.337853]\n",
      "epoch:43 step:34171 [D loss: 0.103864, acc.: 96.09%] [G loss: 4.386650]\n",
      "epoch:43 step:34172 [D loss: 0.030094, acc.: 99.22%] [G loss: 5.097393]\n",
      "epoch:43 step:34173 [D loss: 0.038317, acc.: 97.66%] [G loss: 3.739963]\n",
      "epoch:43 step:34174 [D loss: 0.057304, acc.: 100.00%] [G loss: 4.261796]\n",
      "epoch:43 step:34175 [D loss: 0.008623, acc.: 100.00%] [G loss: 4.299205]\n",
      "epoch:43 step:34176 [D loss: 0.033438, acc.: 99.22%] [G loss: 3.268503]\n",
      "epoch:43 step:34177 [D loss: 0.022043, acc.: 100.00%] [G loss: 2.125471]\n",
      "epoch:43 step:34178 [D loss: 0.034324, acc.: 99.22%] [G loss: 2.665436]\n",
      "epoch:43 step:34179 [D loss: 0.135637, acc.: 95.31%] [G loss: 3.114124]\n",
      "epoch:43 step:34180 [D loss: 0.018234, acc.: 100.00%] [G loss: 2.763409]\n",
      "epoch:43 step:34181 [D loss: 0.040039, acc.: 100.00%] [G loss: 3.733044]\n",
      "epoch:43 step:34182 [D loss: 0.007437, acc.: 100.00%] [G loss: 3.585884]\n",
      "epoch:43 step:34183 [D loss: 0.266104, acc.: 89.84%] [G loss: 1.455116]\n",
      "epoch:43 step:34184 [D loss: 0.024265, acc.: 100.00%] [G loss: 2.304974]\n",
      "epoch:43 step:34185 [D loss: 0.017533, acc.: 100.00%] [G loss: 2.822539]\n",
      "epoch:43 step:34186 [D loss: 0.081116, acc.: 98.44%] [G loss: 4.000191]\n",
      "epoch:43 step:34187 [D loss: 0.022045, acc.: 100.00%] [G loss: 4.561735]\n",
      "epoch:43 step:34188 [D loss: 0.020246, acc.: 99.22%] [G loss: 3.152926]\n",
      "epoch:43 step:34189 [D loss: 0.599775, acc.: 69.53%] [G loss: 8.332823]\n",
      "epoch:43 step:34190 [D loss: 1.666682, acc.: 52.34%] [G loss: 5.166092]\n",
      "epoch:43 step:34191 [D loss: 0.047327, acc.: 97.66%] [G loss: 3.713622]\n",
      "epoch:43 step:34192 [D loss: 0.075171, acc.: 99.22%] [G loss: 4.977168]\n",
      "epoch:43 step:34193 [D loss: 0.034433, acc.: 99.22%] [G loss: 4.485096]\n",
      "epoch:43 step:34194 [D loss: 0.078863, acc.: 97.66%] [G loss: 4.621307]\n",
      "epoch:43 step:34195 [D loss: 0.040062, acc.: 98.44%] [G loss: 4.406895]\n",
      "epoch:43 step:34196 [D loss: 0.020374, acc.: 100.00%] [G loss: 4.362757]\n",
      "epoch:43 step:34197 [D loss: 0.212865, acc.: 92.97%] [G loss: 4.007674]\n",
      "epoch:43 step:34198 [D loss: 0.037673, acc.: 100.00%] [G loss: 4.796213]\n",
      "epoch:43 step:34199 [D loss: 0.028424, acc.: 100.00%] [G loss: 5.164180]\n",
      "epoch:43 step:34200 [D loss: 0.045032, acc.: 99.22%] [G loss: 4.644094]\n",
      "##############\n",
      "[0.92826954 0.96965122 0.94848607 0.88786154 0.82926505 0.92017263\n",
      " 0.96250367 1.11116914 1.12139208 1.04789126]\n",
      "##########\n",
      "epoch:43 step:34201 [D loss: 0.014803, acc.: 100.00%] [G loss: 4.862837]\n",
      "epoch:43 step:34202 [D loss: 0.013170, acc.: 100.00%] [G loss: 3.960617]\n",
      "epoch:43 step:34203 [D loss: 0.023445, acc.: 100.00%] [G loss: 4.123250]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:43 step:34204 [D loss: 0.056430, acc.: 100.00%] [G loss: 3.768861]\n",
      "epoch:43 step:34205 [D loss: 0.034660, acc.: 99.22%] [G loss: 4.861225]\n",
      "epoch:43 step:34206 [D loss: 0.036020, acc.: 100.00%] [G loss: 3.981116]\n",
      "epoch:43 step:34207 [D loss: 0.024860, acc.: 100.00%] [G loss: 4.457864]\n",
      "epoch:43 step:34208 [D loss: 0.029809, acc.: 100.00%] [G loss: 4.642909]\n",
      "epoch:43 step:34209 [D loss: 0.017180, acc.: 100.00%] [G loss: 4.823193]\n",
      "epoch:43 step:34210 [D loss: 0.024204, acc.: 100.00%] [G loss: 3.797395]\n",
      "epoch:43 step:34211 [D loss: 0.020757, acc.: 100.00%] [G loss: 4.804541]\n",
      "epoch:43 step:34212 [D loss: 0.045245, acc.: 99.22%] [G loss: 5.048271]\n",
      "epoch:43 step:34213 [D loss: 0.007654, acc.: 100.00%] [G loss: 5.551812]\n",
      "epoch:43 step:34214 [D loss: 0.011361, acc.: 100.00%] [G loss: 4.200306]\n",
      "epoch:43 step:34215 [D loss: 0.027535, acc.: 99.22%] [G loss: 4.690655]\n",
      "epoch:43 step:34216 [D loss: 0.043620, acc.: 99.22%] [G loss: 5.410413]\n",
      "epoch:43 step:34217 [D loss: 0.003210, acc.: 100.00%] [G loss: 5.630268]\n",
      "epoch:43 step:34218 [D loss: 0.023241, acc.: 100.00%] [G loss: 5.336835]\n",
      "epoch:43 step:34219 [D loss: 0.145349, acc.: 96.88%] [G loss: 4.576652]\n",
      "epoch:43 step:34220 [D loss: 0.024608, acc.: 100.00%] [G loss: 4.029749]\n",
      "epoch:43 step:34221 [D loss: 0.007569, acc.: 100.00%] [G loss: 5.195025]\n",
      "epoch:43 step:34222 [D loss: 0.012046, acc.: 100.00%] [G loss: 4.191051]\n",
      "epoch:43 step:34223 [D loss: 0.028690, acc.: 99.22%] [G loss: 4.304597]\n",
      "epoch:43 step:34224 [D loss: 0.051654, acc.: 99.22%] [G loss: 5.729442]\n",
      "epoch:43 step:34225 [D loss: 0.128896, acc.: 96.09%] [G loss: 2.968171]\n",
      "epoch:43 step:34226 [D loss: 0.034877, acc.: 100.00%] [G loss: 5.394898]\n",
      "epoch:43 step:34227 [D loss: 0.003040, acc.: 100.00%] [G loss: 5.803555]\n",
      "epoch:43 step:34228 [D loss: 0.003910, acc.: 100.00%] [G loss: 5.368731]\n",
      "epoch:43 step:34229 [D loss: 0.032525, acc.: 100.00%] [G loss: 4.911261]\n",
      "epoch:43 step:34230 [D loss: 0.018418, acc.: 100.00%] [G loss: 5.287072]\n",
      "epoch:43 step:34231 [D loss: 0.008572, acc.: 100.00%] [G loss: 4.229768]\n",
      "epoch:43 step:34232 [D loss: 0.020581, acc.: 100.00%] [G loss: 3.852726]\n",
      "epoch:43 step:34233 [D loss: 0.015507, acc.: 100.00%] [G loss: 4.572826]\n",
      "epoch:43 step:34234 [D loss: 0.005890, acc.: 100.00%] [G loss: 3.781963]\n",
      "epoch:43 step:34235 [D loss: 0.007029, acc.: 100.00%] [G loss: 4.653142]\n",
      "epoch:43 step:34236 [D loss: 0.013097, acc.: 100.00%] [G loss: 5.031720]\n",
      "epoch:43 step:34237 [D loss: 0.007779, acc.: 100.00%] [G loss: 4.744108]\n",
      "epoch:43 step:34238 [D loss: 0.032691, acc.: 99.22%] [G loss: 4.092370]\n",
      "epoch:43 step:34239 [D loss: 0.009720, acc.: 100.00%] [G loss: 4.830771]\n",
      "epoch:43 step:34240 [D loss: 0.008011, acc.: 100.00%] [G loss: 4.692404]\n",
      "epoch:43 step:34241 [D loss: 0.052873, acc.: 98.44%] [G loss: 3.838587]\n",
      "epoch:43 step:34242 [D loss: 0.084316, acc.: 98.44%] [G loss: 5.993493]\n",
      "epoch:43 step:34243 [D loss: 0.013234, acc.: 99.22%] [G loss: 7.604422]\n",
      "epoch:43 step:34244 [D loss: 0.439428, acc.: 77.34%] [G loss: 7.708049]\n",
      "epoch:43 step:34245 [D loss: 0.837461, acc.: 60.94%] [G loss: 6.282487]\n",
      "epoch:43 step:34246 [D loss: 0.004245, acc.: 100.00%] [G loss: 6.690211]\n",
      "epoch:43 step:34247 [D loss: 0.011252, acc.: 100.00%] [G loss: 6.352013]\n",
      "epoch:43 step:34248 [D loss: 0.069047, acc.: 96.88%] [G loss: 5.316178]\n",
      "epoch:43 step:34249 [D loss: 0.021772, acc.: 99.22%] [G loss: 5.178154]\n",
      "epoch:43 step:34250 [D loss: 0.007407, acc.: 100.00%] [G loss: 5.317077]\n",
      "epoch:43 step:34251 [D loss: 0.010156, acc.: 100.00%] [G loss: 5.081576]\n",
      "epoch:43 step:34252 [D loss: 0.010796, acc.: 100.00%] [G loss: 5.014547]\n",
      "epoch:43 step:34253 [D loss: 0.011018, acc.: 100.00%] [G loss: 4.641708]\n",
      "epoch:43 step:34254 [D loss: 0.007772, acc.: 100.00%] [G loss: 4.336218]\n",
      "epoch:43 step:34255 [D loss: 0.011293, acc.: 100.00%] [G loss: 4.474948]\n",
      "epoch:43 step:34256 [D loss: 0.024176, acc.: 100.00%] [G loss: 4.796773]\n",
      "epoch:43 step:34257 [D loss: 0.028104, acc.: 100.00%] [G loss: 5.252168]\n",
      "epoch:43 step:34258 [D loss: 0.008717, acc.: 100.00%] [G loss: 5.616673]\n",
      "epoch:43 step:34259 [D loss: 0.024258, acc.: 100.00%] [G loss: 5.319468]\n",
      "epoch:43 step:34260 [D loss: 0.008330, acc.: 100.00%] [G loss: 4.456574]\n",
      "epoch:43 step:34261 [D loss: 0.019708, acc.: 100.00%] [G loss: 4.947775]\n",
      "epoch:43 step:34262 [D loss: 0.014227, acc.: 100.00%] [G loss: 3.753806]\n",
      "epoch:43 step:34263 [D loss: 0.024194, acc.: 100.00%] [G loss: 4.592312]\n",
      "epoch:43 step:34264 [D loss: 0.025904, acc.: 100.00%] [G loss: 5.511106]\n",
      "epoch:43 step:34265 [D loss: 0.020342, acc.: 99.22%] [G loss: 4.142525]\n",
      "epoch:43 step:34266 [D loss: 0.013281, acc.: 100.00%] [G loss: 4.134475]\n",
      "epoch:43 step:34267 [D loss: 0.014698, acc.: 99.22%] [G loss: 4.322855]\n",
      "epoch:43 step:34268 [D loss: 0.354875, acc.: 83.59%] [G loss: 5.261793]\n",
      "epoch:43 step:34269 [D loss: 0.005913, acc.: 100.00%] [G loss: 6.841521]\n",
      "epoch:43 step:34270 [D loss: 0.211989, acc.: 89.84%] [G loss: 4.332332]\n",
      "epoch:43 step:34271 [D loss: 0.007378, acc.: 100.00%] [G loss: 3.027714]\n",
      "epoch:43 step:34272 [D loss: 0.027755, acc.: 100.00%] [G loss: 2.985949]\n",
      "epoch:43 step:34273 [D loss: 0.138665, acc.: 96.09%] [G loss: 3.724804]\n",
      "epoch:43 step:34274 [D loss: 0.018886, acc.: 99.22%] [G loss: 4.322398]\n",
      "epoch:43 step:34275 [D loss: 0.044366, acc.: 99.22%] [G loss: 5.507071]\n",
      "epoch:43 step:34276 [D loss: 0.003747, acc.: 100.00%] [G loss: 2.898231]\n",
      "epoch:43 step:34277 [D loss: 0.021682, acc.: 100.00%] [G loss: 4.199376]\n",
      "epoch:43 step:34278 [D loss: 0.031062, acc.: 100.00%] [G loss: 5.746157]\n",
      "epoch:43 step:34279 [D loss: 0.028282, acc.: 99.22%] [G loss: 5.526771]\n",
      "epoch:43 step:34280 [D loss: 0.004941, acc.: 100.00%] [G loss: 5.741531]\n",
      "epoch:43 step:34281 [D loss: 0.005849, acc.: 100.00%] [G loss: 5.321608]\n",
      "epoch:43 step:34282 [D loss: 0.006066, acc.: 100.00%] [G loss: 4.469570]\n",
      "epoch:43 step:34283 [D loss: 0.048488, acc.: 100.00%] [G loss: 5.578446]\n",
      "epoch:43 step:34284 [D loss: 0.003293, acc.: 100.00%] [G loss: 6.173976]\n",
      "epoch:43 step:34285 [D loss: 0.002366, acc.: 100.00%] [G loss: 5.258326]\n",
      "epoch:43 step:34286 [D loss: 0.029297, acc.: 99.22%] [G loss: 5.087917]\n",
      "epoch:43 step:34287 [D loss: 0.017642, acc.: 100.00%] [G loss: 5.468714]\n",
      "epoch:43 step:34288 [D loss: 0.002977, acc.: 100.00%] [G loss: 4.703919]\n",
      "epoch:43 step:34289 [D loss: 0.004480, acc.: 100.00%] [G loss: 4.957804]\n",
      "epoch:43 step:34290 [D loss: 0.038244, acc.: 99.22%] [G loss: 4.597926]\n",
      "epoch:43 step:34291 [D loss: 0.018330, acc.: 99.22%] [G loss: 5.926755]\n",
      "epoch:43 step:34292 [D loss: 0.004979, acc.: 100.00%] [G loss: 3.888939]\n",
      "epoch:43 step:34293 [D loss: 0.018179, acc.: 100.00%] [G loss: 5.165200]\n",
      "epoch:43 step:34294 [D loss: 0.038394, acc.: 99.22%] [G loss: 2.795712]\n",
      "epoch:43 step:34295 [D loss: 0.528435, acc.: 77.34%] [G loss: 10.324402]\n",
      "epoch:43 step:34296 [D loss: 3.621864, acc.: 50.00%] [G loss: 6.988008]\n",
      "epoch:43 step:34297 [D loss: 0.718151, acc.: 67.97%] [G loss: 2.602368]\n",
      "epoch:43 step:34298 [D loss: 0.310160, acc.: 86.72%] [G loss: 6.177106]\n",
      "epoch:43 step:34299 [D loss: 0.011872, acc.: 100.00%] [G loss: 7.406835]\n",
      "epoch:43 step:34300 [D loss: 0.089267, acc.: 96.88%] [G loss: 5.182802]\n",
      "epoch:43 step:34301 [D loss: 0.016782, acc.: 100.00%] [G loss: 4.652259]\n",
      "epoch:43 step:34302 [D loss: 0.009098, acc.: 100.00%] [G loss: 4.302313]\n",
      "epoch:43 step:34303 [D loss: 0.063792, acc.: 99.22%] [G loss: 3.746860]\n",
      "epoch:43 step:34304 [D loss: 0.057894, acc.: 99.22%] [G loss: 4.526711]\n",
      "epoch:43 step:34305 [D loss: 0.055539, acc.: 97.66%] [G loss: 4.789234]\n",
      "epoch:43 step:34306 [D loss: 0.049289, acc.: 98.44%] [G loss: 3.132687]\n",
      "epoch:43 step:34307 [D loss: 0.081062, acc.: 99.22%] [G loss: 4.578235]\n",
      "epoch:43 step:34308 [D loss: 0.219968, acc.: 92.19%] [G loss: 2.702105]\n",
      "epoch:43 step:34309 [D loss: 0.017657, acc.: 100.00%] [G loss: 3.472705]\n",
      "epoch:43 step:34310 [D loss: 0.015073, acc.: 100.00%] [G loss: 2.779800]\n",
      "epoch:43 step:34311 [D loss: 0.014976, acc.: 100.00%] [G loss: 3.427765]\n",
      "epoch:43 step:34312 [D loss: 0.019254, acc.: 100.00%] [G loss: 3.643988]\n",
      "epoch:43 step:34313 [D loss: 0.015032, acc.: 100.00%] [G loss: 2.964950]\n",
      "epoch:43 step:34314 [D loss: 0.058952, acc.: 99.22%] [G loss: 2.861409]\n",
      "epoch:43 step:34315 [D loss: 0.033358, acc.: 99.22%] [G loss: 4.174690]\n",
      "epoch:43 step:34316 [D loss: 0.159634, acc.: 93.75%] [G loss: 1.617128]\n",
      "epoch:43 step:34317 [D loss: 0.080512, acc.: 100.00%] [G loss: 5.307760]\n",
      "epoch:43 step:34318 [D loss: 0.031757, acc.: 99.22%] [G loss: 5.420571]\n",
      "epoch:43 step:34319 [D loss: 0.047165, acc.: 98.44%] [G loss: 4.342579]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:43 step:34320 [D loss: 0.037123, acc.: 100.00%] [G loss: 4.171949]\n",
      "epoch:43 step:34321 [D loss: 0.007210, acc.: 100.00%] [G loss: 4.920431]\n",
      "epoch:43 step:34322 [D loss: 0.003499, acc.: 100.00%] [G loss: 4.802153]\n",
      "epoch:43 step:34323 [D loss: 0.363972, acc.: 82.03%] [G loss: 6.367769]\n",
      "epoch:43 step:34324 [D loss: 0.113595, acc.: 97.66%] [G loss: 6.153354]\n",
      "epoch:43 step:34325 [D loss: 0.004033, acc.: 100.00%] [G loss: 6.251923]\n",
      "epoch:43 step:34326 [D loss: 0.022598, acc.: 99.22%] [G loss: 5.226855]\n",
      "epoch:43 step:34327 [D loss: 0.007825, acc.: 100.00%] [G loss: 5.334239]\n",
      "epoch:43 step:34328 [D loss: 0.005988, acc.: 100.00%] [G loss: 4.974708]\n",
      "epoch:43 step:34329 [D loss: 0.009592, acc.: 100.00%] [G loss: 4.690434]\n",
      "epoch:43 step:34330 [D loss: 0.013905, acc.: 100.00%] [G loss: 4.491940]\n",
      "epoch:43 step:34331 [D loss: 0.018777, acc.: 100.00%] [G loss: 4.255785]\n",
      "epoch:43 step:34332 [D loss: 0.007650, acc.: 100.00%] [G loss: 4.134543]\n",
      "epoch:43 step:34333 [D loss: 0.021070, acc.: 100.00%] [G loss: 3.419675]\n",
      "epoch:43 step:34334 [D loss: 0.015666, acc.: 100.00%] [G loss: 4.114466]\n",
      "epoch:43 step:34335 [D loss: 0.040746, acc.: 99.22%] [G loss: 4.798443]\n",
      "epoch:43 step:34336 [D loss: 0.008187, acc.: 100.00%] [G loss: 5.888853]\n",
      "epoch:43 step:34337 [D loss: 0.031923, acc.: 99.22%] [G loss: 4.921733]\n",
      "epoch:43 step:34338 [D loss: 0.006538, acc.: 100.00%] [G loss: 4.717094]\n",
      "epoch:43 step:34339 [D loss: 0.012063, acc.: 100.00%] [G loss: 4.218848]\n",
      "epoch:43 step:34340 [D loss: 0.012284, acc.: 100.00%] [G loss: 4.649727]\n",
      "epoch:43 step:34341 [D loss: 0.006431, acc.: 100.00%] [G loss: 4.867877]\n",
      "epoch:43 step:34342 [D loss: 0.014060, acc.: 100.00%] [G loss: 5.020470]\n",
      "epoch:43 step:34343 [D loss: 0.170017, acc.: 96.09%] [G loss: 5.971632]\n",
      "epoch:43 step:34344 [D loss: 0.027238, acc.: 99.22%] [G loss: 6.224149]\n",
      "epoch:43 step:34345 [D loss: 0.059049, acc.: 98.44%] [G loss: 4.401221]\n",
      "epoch:43 step:34346 [D loss: 0.052944, acc.: 99.22%] [G loss: 4.615138]\n",
      "epoch:43 step:34347 [D loss: 0.003013, acc.: 100.00%] [G loss: 5.791492]\n",
      "epoch:43 step:34348 [D loss: 0.008545, acc.: 100.00%] [G loss: 5.929351]\n",
      "epoch:43 step:34349 [D loss: 0.006872, acc.: 100.00%] [G loss: 6.222215]\n",
      "epoch:43 step:34350 [D loss: 0.011964, acc.: 100.00%] [G loss: 4.077006]\n",
      "epoch:43 step:34351 [D loss: 0.047059, acc.: 99.22%] [G loss: 4.102780]\n",
      "epoch:43 step:34352 [D loss: 0.032640, acc.: 98.44%] [G loss: 4.090918]\n",
      "epoch:43 step:34353 [D loss: 0.051598, acc.: 98.44%] [G loss: 5.826682]\n",
      "epoch:43 step:34354 [D loss: 0.054931, acc.: 99.22%] [G loss: 4.494841]\n",
      "epoch:43 step:34355 [D loss: 0.009937, acc.: 100.00%] [G loss: 5.393109]\n",
      "epoch:43 step:34356 [D loss: 0.004713, acc.: 100.00%] [G loss: 5.138864]\n",
      "epoch:43 step:34357 [D loss: 0.005822, acc.: 100.00%] [G loss: 6.021056]\n",
      "epoch:43 step:34358 [D loss: 0.006994, acc.: 100.00%] [G loss: 6.652297]\n",
      "epoch:43 step:34359 [D loss: 0.018878, acc.: 100.00%] [G loss: 6.038550]\n",
      "epoch:43 step:34360 [D loss: 0.025436, acc.: 100.00%] [G loss: 5.524460]\n",
      "epoch:43 step:34361 [D loss: 0.006685, acc.: 100.00%] [G loss: 5.271203]\n",
      "epoch:43 step:34362 [D loss: 0.007964, acc.: 100.00%] [G loss: 5.026386]\n",
      "epoch:43 step:34363 [D loss: 0.013124, acc.: 100.00%] [G loss: 4.851590]\n",
      "epoch:43 step:34364 [D loss: 0.015522, acc.: 100.00%] [G loss: 5.176421]\n",
      "epoch:44 step:34365 [D loss: 0.029137, acc.: 100.00%] [G loss: 4.329438]\n",
      "epoch:44 step:34366 [D loss: 0.030935, acc.: 100.00%] [G loss: 6.073772]\n",
      "epoch:44 step:34367 [D loss: 0.009580, acc.: 100.00%] [G loss: 5.887171]\n",
      "epoch:44 step:34368 [D loss: 0.079025, acc.: 98.44%] [G loss: 6.007744]\n",
      "epoch:44 step:34369 [D loss: 0.002262, acc.: 100.00%] [G loss: 6.051175]\n",
      "epoch:44 step:34370 [D loss: 0.018105, acc.: 100.00%] [G loss: 5.909308]\n",
      "epoch:44 step:34371 [D loss: 0.002999, acc.: 100.00%] [G loss: 4.066135]\n",
      "epoch:44 step:34372 [D loss: 0.008487, acc.: 100.00%] [G loss: 4.882879]\n",
      "epoch:44 step:34373 [D loss: 0.023330, acc.: 100.00%] [G loss: 5.221609]\n",
      "epoch:44 step:34374 [D loss: 0.007702, acc.: 100.00%] [G loss: 4.214852]\n",
      "epoch:44 step:34375 [D loss: 0.005832, acc.: 100.00%] [G loss: 3.995724]\n",
      "epoch:44 step:34376 [D loss: 0.077661, acc.: 99.22%] [G loss: 6.719142]\n",
      "epoch:44 step:34377 [D loss: 0.002208, acc.: 100.00%] [G loss: 8.038639]\n",
      "epoch:44 step:34378 [D loss: 1.650749, acc.: 34.38%] [G loss: 9.591635]\n",
      "epoch:44 step:34379 [D loss: 2.408545, acc.: 50.00%] [G loss: 7.198491]\n",
      "epoch:44 step:34380 [D loss: 0.120698, acc.: 92.97%] [G loss: 6.552778]\n",
      "epoch:44 step:34381 [D loss: 0.015956, acc.: 99.22%] [G loss: 5.850859]\n",
      "epoch:44 step:34382 [D loss: 0.011249, acc.: 100.00%] [G loss: 5.196043]\n",
      "epoch:44 step:34383 [D loss: 0.157167, acc.: 89.84%] [G loss: 5.465664]\n",
      "epoch:44 step:34384 [D loss: 0.011043, acc.: 100.00%] [G loss: 5.190195]\n",
      "epoch:44 step:34385 [D loss: 0.015992, acc.: 100.00%] [G loss: 4.863378]\n",
      "epoch:44 step:34386 [D loss: 0.019437, acc.: 100.00%] [G loss: 3.906348]\n",
      "epoch:44 step:34387 [D loss: 0.463805, acc.: 78.91%] [G loss: 4.957128]\n",
      "epoch:44 step:34388 [D loss: 0.088307, acc.: 96.09%] [G loss: 4.856059]\n",
      "epoch:44 step:34389 [D loss: 0.049840, acc.: 98.44%] [G loss: 4.238441]\n",
      "epoch:44 step:34390 [D loss: 0.021531, acc.: 100.00%] [G loss: 4.008866]\n",
      "epoch:44 step:34391 [D loss: 0.036993, acc.: 100.00%] [G loss: 3.032760]\n",
      "epoch:44 step:34392 [D loss: 0.059606, acc.: 99.22%] [G loss: 2.873019]\n",
      "epoch:44 step:34393 [D loss: 0.072783, acc.: 99.22%] [G loss: 3.622015]\n",
      "epoch:44 step:34394 [D loss: 0.238774, acc.: 92.19%] [G loss: 2.709195]\n",
      "epoch:44 step:34395 [D loss: 0.015371, acc.: 100.00%] [G loss: 3.702116]\n",
      "epoch:44 step:34396 [D loss: 2.095666, acc.: 31.25%] [G loss: 8.095259]\n",
      "epoch:44 step:34397 [D loss: 1.334346, acc.: 56.25%] [G loss: 7.403883]\n",
      "epoch:44 step:34398 [D loss: 0.027224, acc.: 99.22%] [G loss: 7.754007]\n",
      "epoch:44 step:34399 [D loss: 0.009468, acc.: 100.00%] [G loss: 7.499200]\n",
      "epoch:44 step:34400 [D loss: 0.051244, acc.: 97.66%] [G loss: 6.736135]\n",
      "##############\n",
      "[0.95416305 0.93590109 1.08574038 0.99706353 0.93364099 0.97702887\n",
      " 2.09621287 0.9520393  1.10364554 1.11511753]\n",
      "##########\n",
      "epoch:44 step:34401 [D loss: 0.006249, acc.: 100.00%] [G loss: 6.413524]\n",
      "epoch:44 step:34402 [D loss: 0.005918, acc.: 100.00%] [G loss: 6.929014]\n",
      "epoch:44 step:34403 [D loss: 0.008887, acc.: 100.00%] [G loss: 5.559808]\n",
      "epoch:44 step:34404 [D loss: 0.019135, acc.: 100.00%] [G loss: 5.277659]\n",
      "epoch:44 step:34405 [D loss: 0.005993, acc.: 100.00%] [G loss: 5.321956]\n",
      "epoch:44 step:34406 [D loss: 0.010502, acc.: 100.00%] [G loss: 4.803696]\n",
      "epoch:44 step:34407 [D loss: 0.006788, acc.: 100.00%] [G loss: 4.213335]\n",
      "epoch:44 step:34408 [D loss: 0.012503, acc.: 100.00%] [G loss: 4.016973]\n",
      "epoch:44 step:34409 [D loss: 0.039756, acc.: 100.00%] [G loss: 3.527061]\n",
      "epoch:44 step:34410 [D loss: 0.081276, acc.: 99.22%] [G loss: 4.938835]\n",
      "epoch:44 step:34411 [D loss: 0.006324, acc.: 100.00%] [G loss: 5.505666]\n",
      "epoch:44 step:34412 [D loss: 0.024346, acc.: 100.00%] [G loss: 5.594606]\n",
      "epoch:44 step:34413 [D loss: 0.044232, acc.: 98.44%] [G loss: 5.308578]\n",
      "epoch:44 step:34414 [D loss: 0.108290, acc.: 96.09%] [G loss: 4.531280]\n",
      "epoch:44 step:34415 [D loss: 0.016315, acc.: 100.00%] [G loss: 5.670136]\n",
      "epoch:44 step:34416 [D loss: 0.184760, acc.: 93.75%] [G loss: 3.994574]\n",
      "epoch:44 step:34417 [D loss: 0.019053, acc.: 100.00%] [G loss: 4.400628]\n",
      "epoch:44 step:34418 [D loss: 0.081709, acc.: 99.22%] [G loss: 2.156617]\n",
      "epoch:44 step:34419 [D loss: 0.015869, acc.: 100.00%] [G loss: 4.859539]\n",
      "epoch:44 step:34420 [D loss: 0.020755, acc.: 100.00%] [G loss: 3.215755]\n",
      "epoch:44 step:34421 [D loss: 0.018644, acc.: 100.00%] [G loss: 3.609726]\n",
      "epoch:44 step:34422 [D loss: 0.041221, acc.: 100.00%] [G loss: 4.685990]\n",
      "epoch:44 step:34423 [D loss: 0.019619, acc.: 100.00%] [G loss: 4.373529]\n",
      "epoch:44 step:34424 [D loss: 0.020501, acc.: 100.00%] [G loss: 4.031231]\n",
      "epoch:44 step:34425 [D loss: 0.039941, acc.: 100.00%] [G loss: 5.342776]\n",
      "epoch:44 step:34426 [D loss: 0.160429, acc.: 96.09%] [G loss: 3.402086]\n",
      "epoch:44 step:34427 [D loss: 0.263501, acc.: 89.84%] [G loss: 6.484546]\n",
      "epoch:44 step:34428 [D loss: 0.375640, acc.: 79.69%] [G loss: 5.767006]\n",
      "epoch:44 step:34429 [D loss: 0.034766, acc.: 99.22%] [G loss: 3.567248]\n",
      "epoch:44 step:34430 [D loss: 0.018583, acc.: 100.00%] [G loss: 2.987804]\n",
      "epoch:44 step:34431 [D loss: 0.004760, acc.: 100.00%] [G loss: 4.412807]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:44 step:34432 [D loss: 0.017615, acc.: 100.00%] [G loss: 5.422644]\n",
      "epoch:44 step:34433 [D loss: 0.009716, acc.: 100.00%] [G loss: 3.550109]\n",
      "epoch:44 step:34434 [D loss: 0.023179, acc.: 100.00%] [G loss: 3.827686]\n",
      "epoch:44 step:34435 [D loss: 0.041168, acc.: 99.22%] [G loss: 5.125078]\n",
      "epoch:44 step:34436 [D loss: 0.026030, acc.: 100.00%] [G loss: 4.971726]\n",
      "epoch:44 step:34437 [D loss: 0.120595, acc.: 96.09%] [G loss: 3.465015]\n",
      "epoch:44 step:34438 [D loss: 0.018566, acc.: 100.00%] [G loss: 4.254587]\n",
      "epoch:44 step:34439 [D loss: 0.021433, acc.: 100.00%] [G loss: 3.866947]\n",
      "epoch:44 step:34440 [D loss: 0.024713, acc.: 100.00%] [G loss: 4.364104]\n",
      "epoch:44 step:34441 [D loss: 0.074179, acc.: 99.22%] [G loss: 3.366278]\n",
      "epoch:44 step:34442 [D loss: 0.051599, acc.: 100.00%] [G loss: 4.614405]\n",
      "epoch:44 step:34443 [D loss: 0.005649, acc.: 100.00%] [G loss: 5.300968]\n",
      "epoch:44 step:34444 [D loss: 0.011494, acc.: 100.00%] [G loss: 5.801883]\n",
      "epoch:44 step:34445 [D loss: 0.008613, acc.: 100.00%] [G loss: 5.015873]\n",
      "epoch:44 step:34446 [D loss: 0.004326, acc.: 100.00%] [G loss: 5.208015]\n",
      "epoch:44 step:34447 [D loss: 0.043926, acc.: 100.00%] [G loss: 3.553958]\n",
      "epoch:44 step:34448 [D loss: 0.018079, acc.: 100.00%] [G loss: 4.503456]\n",
      "epoch:44 step:34449 [D loss: 0.036434, acc.: 100.00%] [G loss: 3.622003]\n",
      "epoch:44 step:34450 [D loss: 0.013851, acc.: 100.00%] [G loss: 4.376600]\n",
      "epoch:44 step:34451 [D loss: 0.005358, acc.: 100.00%] [G loss: 4.115707]\n",
      "epoch:44 step:34452 [D loss: 0.065980, acc.: 99.22%] [G loss: 3.572384]\n",
      "epoch:44 step:34453 [D loss: 0.009934, acc.: 100.00%] [G loss: 3.323871]\n",
      "epoch:44 step:34454 [D loss: 2.139505, acc.: 38.28%] [G loss: 9.088160]\n",
      "epoch:44 step:34455 [D loss: 2.848344, acc.: 50.00%] [G loss: 7.008175]\n",
      "epoch:44 step:34456 [D loss: 1.512766, acc.: 54.69%] [G loss: 2.998383]\n",
      "epoch:44 step:34457 [D loss: 0.302213, acc.: 84.38%] [G loss: 4.078394]\n",
      "epoch:44 step:34458 [D loss: 0.037253, acc.: 99.22%] [G loss: 5.074868]\n",
      "epoch:44 step:34459 [D loss: 0.050212, acc.: 97.66%] [G loss: 4.398849]\n",
      "epoch:44 step:34460 [D loss: 0.128815, acc.: 96.09%] [G loss: 3.481637]\n",
      "epoch:44 step:34461 [D loss: 0.078380, acc.: 98.44%] [G loss: 3.448059]\n",
      "epoch:44 step:34462 [D loss: 0.032322, acc.: 100.00%] [G loss: 3.540813]\n",
      "epoch:44 step:34463 [D loss: 0.028659, acc.: 99.22%] [G loss: 4.459464]\n",
      "epoch:44 step:34464 [D loss: 0.010362, acc.: 100.00%] [G loss: 4.403896]\n",
      "epoch:44 step:34465 [D loss: 0.009388, acc.: 100.00%] [G loss: 3.656207]\n",
      "epoch:44 step:34466 [D loss: 0.034282, acc.: 99.22%] [G loss: 3.757739]\n",
      "epoch:44 step:34467 [D loss: 0.049134, acc.: 100.00%] [G loss: 3.374497]\n",
      "epoch:44 step:34468 [D loss: 0.035422, acc.: 100.00%] [G loss: 3.466330]\n",
      "epoch:44 step:34469 [D loss: 0.016563, acc.: 100.00%] [G loss: 3.298540]\n",
      "epoch:44 step:34470 [D loss: 0.028315, acc.: 100.00%] [G loss: 3.859451]\n",
      "epoch:44 step:34471 [D loss: 0.102122, acc.: 96.88%] [G loss: 2.175169]\n",
      "epoch:44 step:34472 [D loss: 0.084611, acc.: 99.22%] [G loss: 4.099534]\n",
      "epoch:44 step:34473 [D loss: 0.026920, acc.: 99.22%] [G loss: 3.929331]\n",
      "epoch:44 step:34474 [D loss: 0.113307, acc.: 98.44%] [G loss: 3.514292]\n",
      "epoch:44 step:34475 [D loss: 0.116935, acc.: 97.66%] [G loss: 2.931602]\n",
      "epoch:44 step:34476 [D loss: 0.018440, acc.: 100.00%] [G loss: 4.108441]\n",
      "epoch:44 step:34477 [D loss: 0.011774, acc.: 100.00%] [G loss: 3.960240]\n",
      "epoch:44 step:34478 [D loss: 0.064406, acc.: 98.44%] [G loss: 2.582281]\n",
      "epoch:44 step:34479 [D loss: 0.081074, acc.: 98.44%] [G loss: 3.434202]\n",
      "epoch:44 step:34480 [D loss: 0.297599, acc.: 92.19%] [G loss: 3.625026]\n",
      "epoch:44 step:34481 [D loss: 0.022936, acc.: 100.00%] [G loss: 4.860980]\n",
      "epoch:44 step:34482 [D loss: 0.014176, acc.: 100.00%] [G loss: 4.686550]\n",
      "epoch:44 step:34483 [D loss: 0.024391, acc.: 100.00%] [G loss: 3.957260]\n",
      "epoch:44 step:34484 [D loss: 0.014315, acc.: 100.00%] [G loss: 3.741483]\n",
      "epoch:44 step:34485 [D loss: 0.022614, acc.: 100.00%] [G loss: 3.615745]\n",
      "epoch:44 step:34486 [D loss: 0.009897, acc.: 100.00%] [G loss: 3.313610]\n",
      "epoch:44 step:34487 [D loss: 0.021861, acc.: 100.00%] [G loss: 4.068512]\n",
      "epoch:44 step:34488 [D loss: 0.031962, acc.: 100.00%] [G loss: 2.781942]\n",
      "epoch:44 step:34489 [D loss: 0.073434, acc.: 99.22%] [G loss: 4.852349]\n",
      "epoch:44 step:34490 [D loss: 0.046349, acc.: 98.44%] [G loss: 4.176433]\n",
      "epoch:44 step:34491 [D loss: 0.015466, acc.: 100.00%] [G loss: 4.543637]\n",
      "epoch:44 step:34492 [D loss: 0.015778, acc.: 100.00%] [G loss: 4.213592]\n",
      "epoch:44 step:34493 [D loss: 0.028465, acc.: 100.00%] [G loss: 3.856787]\n",
      "epoch:44 step:34494 [D loss: 0.016983, acc.: 100.00%] [G loss: 3.432345]\n",
      "epoch:44 step:34495 [D loss: 0.019058, acc.: 100.00%] [G loss: 3.955667]\n",
      "epoch:44 step:34496 [D loss: 0.056911, acc.: 99.22%] [G loss: 3.190551]\n",
      "epoch:44 step:34497 [D loss: 0.037889, acc.: 100.00%] [G loss: 3.684043]\n",
      "epoch:44 step:34498 [D loss: 0.029336, acc.: 99.22%] [G loss: 3.740938]\n",
      "epoch:44 step:34499 [D loss: 0.024006, acc.: 100.00%] [G loss: 3.913590]\n",
      "epoch:44 step:34500 [D loss: 0.050330, acc.: 100.00%] [G loss: 3.689289]\n",
      "epoch:44 step:34501 [D loss: 0.009258, acc.: 100.00%] [G loss: 4.173997]\n",
      "epoch:44 step:34502 [D loss: 0.026091, acc.: 99.22%] [G loss: 3.874728]\n",
      "epoch:44 step:34503 [D loss: 0.042809, acc.: 100.00%] [G loss: 4.052155]\n",
      "epoch:44 step:34504 [D loss: 0.037251, acc.: 100.00%] [G loss: 3.558391]\n",
      "epoch:44 step:34505 [D loss: 0.009472, acc.: 100.00%] [G loss: 4.164644]\n",
      "epoch:44 step:34506 [D loss: 0.006341, acc.: 100.00%] [G loss: 4.264755]\n",
      "epoch:44 step:34507 [D loss: 0.132642, acc.: 98.44%] [G loss: 5.333332]\n",
      "epoch:44 step:34508 [D loss: 0.021964, acc.: 99.22%] [G loss: 5.565055]\n",
      "epoch:44 step:34509 [D loss: 1.596459, acc.: 31.25%] [G loss: 7.109367]\n",
      "epoch:44 step:34510 [D loss: 1.299314, acc.: 54.69%] [G loss: 5.000887]\n",
      "epoch:44 step:34511 [D loss: 0.061344, acc.: 98.44%] [G loss: 4.319216]\n",
      "epoch:44 step:34512 [D loss: 0.068879, acc.: 96.88%] [G loss: 4.540882]\n",
      "epoch:44 step:34513 [D loss: 0.042461, acc.: 99.22%] [G loss: 4.303487]\n",
      "epoch:44 step:34514 [D loss: 0.019570, acc.: 100.00%] [G loss: 4.287846]\n",
      "epoch:44 step:34515 [D loss: 0.032604, acc.: 100.00%] [G loss: 4.297399]\n",
      "epoch:44 step:34516 [D loss: 0.078048, acc.: 97.66%] [G loss: 3.950317]\n",
      "epoch:44 step:34517 [D loss: 0.054412, acc.: 98.44%] [G loss: 4.177382]\n",
      "epoch:44 step:34518 [D loss: 0.027473, acc.: 100.00%] [G loss: 4.382846]\n",
      "epoch:44 step:34519 [D loss: 0.013538, acc.: 100.00%] [G loss: 4.310887]\n",
      "epoch:44 step:34520 [D loss: 0.049249, acc.: 100.00%] [G loss: 3.256776]\n",
      "epoch:44 step:34521 [D loss: 0.100199, acc.: 98.44%] [G loss: 4.382771]\n",
      "epoch:44 step:34522 [D loss: 0.282623, acc.: 90.62%] [G loss: 4.084126]\n",
      "epoch:44 step:34523 [D loss: 0.022097, acc.: 100.00%] [G loss: 4.280133]\n",
      "epoch:44 step:34524 [D loss: 0.006506, acc.: 100.00%] [G loss: 4.036234]\n",
      "epoch:44 step:34525 [D loss: 0.024869, acc.: 99.22%] [G loss: 4.409588]\n",
      "epoch:44 step:34526 [D loss: 0.020881, acc.: 100.00%] [G loss: 3.981260]\n",
      "epoch:44 step:34527 [D loss: 0.043983, acc.: 99.22%] [G loss: 4.187713]\n",
      "epoch:44 step:34528 [D loss: 0.020761, acc.: 100.00%] [G loss: 3.518635]\n",
      "epoch:44 step:34529 [D loss: 0.017400, acc.: 100.00%] [G loss: 2.928103]\n",
      "epoch:44 step:34530 [D loss: 0.063322, acc.: 99.22%] [G loss: 2.230547]\n",
      "epoch:44 step:34531 [D loss: 0.074796, acc.: 99.22%] [G loss: 5.397781]\n",
      "epoch:44 step:34532 [D loss: 0.013004, acc.: 100.00%] [G loss: 5.830526]\n",
      "epoch:44 step:34533 [D loss: 0.026010, acc.: 100.00%] [G loss: 5.182305]\n",
      "epoch:44 step:34534 [D loss: 0.011650, acc.: 100.00%] [G loss: 4.632550]\n",
      "epoch:44 step:34535 [D loss: 0.012962, acc.: 100.00%] [G loss: 4.664747]\n",
      "epoch:44 step:34536 [D loss: 0.006675, acc.: 100.00%] [G loss: 4.607435]\n",
      "epoch:44 step:34537 [D loss: 0.008497, acc.: 100.00%] [G loss: 4.608388]\n",
      "epoch:44 step:34538 [D loss: 0.019212, acc.: 100.00%] [G loss: 3.733266]\n",
      "epoch:44 step:34539 [D loss: 0.007522, acc.: 100.00%] [G loss: 4.088867]\n",
      "epoch:44 step:34540 [D loss: 0.110621, acc.: 98.44%] [G loss: 3.634795]\n",
      "epoch:44 step:34541 [D loss: 0.006676, acc.: 100.00%] [G loss: 4.204563]\n",
      "epoch:44 step:34542 [D loss: 0.008656, acc.: 100.00%] [G loss: 4.555584]\n",
      "epoch:44 step:34543 [D loss: 0.021501, acc.: 100.00%] [G loss: 3.370862]\n",
      "epoch:44 step:34544 [D loss: 0.015880, acc.: 100.00%] [G loss: 3.077807]\n",
      "epoch:44 step:34545 [D loss: 0.012111, acc.: 100.00%] [G loss: 3.863508]\n",
      "epoch:44 step:34546 [D loss: 0.008595, acc.: 100.00%] [G loss: 4.195457]\n",
      "epoch:44 step:34547 [D loss: 0.005726, acc.: 100.00%] [G loss: 3.859088]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:44 step:34548 [D loss: 0.011462, acc.: 100.00%] [G loss: 3.711485]\n",
      "epoch:44 step:34549 [D loss: 0.019764, acc.: 100.00%] [G loss: 3.673668]\n",
      "epoch:44 step:34550 [D loss: 0.008080, acc.: 100.00%] [G loss: 3.570978]\n",
      "epoch:44 step:34551 [D loss: 0.033364, acc.: 100.00%] [G loss: 4.389995]\n",
      "epoch:44 step:34552 [D loss: 0.049023, acc.: 100.00%] [G loss: 4.481421]\n",
      "epoch:44 step:34553 [D loss: 0.040460, acc.: 98.44%] [G loss: 4.514317]\n",
      "epoch:44 step:34554 [D loss: 0.021052, acc.: 100.00%] [G loss: 3.779062]\n",
      "epoch:44 step:34555 [D loss: 0.029887, acc.: 100.00%] [G loss: 3.447089]\n",
      "epoch:44 step:34556 [D loss: 0.025906, acc.: 100.00%] [G loss: 3.765299]\n",
      "epoch:44 step:34557 [D loss: 0.016812, acc.: 100.00%] [G loss: 4.111258]\n",
      "epoch:44 step:34558 [D loss: 0.020360, acc.: 100.00%] [G loss: 4.043434]\n",
      "epoch:44 step:34559 [D loss: 0.006982, acc.: 100.00%] [G loss: 3.408146]\n",
      "epoch:44 step:34560 [D loss: 0.020486, acc.: 100.00%] [G loss: 3.566381]\n",
      "epoch:44 step:34561 [D loss: 0.035894, acc.: 100.00%] [G loss: 4.052512]\n",
      "epoch:44 step:34562 [D loss: 0.017621, acc.: 100.00%] [G loss: 3.997360]\n",
      "epoch:44 step:34563 [D loss: 0.015539, acc.: 100.00%] [G loss: 4.570479]\n",
      "epoch:44 step:34564 [D loss: 0.016260, acc.: 100.00%] [G loss: 4.211078]\n",
      "epoch:44 step:34565 [D loss: 0.008643, acc.: 100.00%] [G loss: 4.349218]\n",
      "epoch:44 step:34566 [D loss: 0.030212, acc.: 100.00%] [G loss: 4.301792]\n",
      "epoch:44 step:34567 [D loss: 0.028004, acc.: 100.00%] [G loss: 5.089302]\n",
      "epoch:44 step:34568 [D loss: 0.019654, acc.: 99.22%] [G loss: 3.971991]\n",
      "epoch:44 step:34569 [D loss: 0.014362, acc.: 100.00%] [G loss: 3.129046]\n",
      "epoch:44 step:34570 [D loss: 0.053151, acc.: 100.00%] [G loss: 4.150845]\n",
      "epoch:44 step:34571 [D loss: 0.002232, acc.: 100.00%] [G loss: 4.458949]\n",
      "epoch:44 step:34572 [D loss: 0.004850, acc.: 100.00%] [G loss: 4.011023]\n",
      "epoch:44 step:34573 [D loss: 0.015899, acc.: 100.00%] [G loss: 3.683555]\n",
      "epoch:44 step:34574 [D loss: 0.015031, acc.: 100.00%] [G loss: 3.497873]\n",
      "epoch:44 step:34575 [D loss: 0.008067, acc.: 100.00%] [G loss: 2.868757]\n",
      "epoch:44 step:34576 [D loss: 0.020708, acc.: 100.00%] [G loss: 4.094791]\n",
      "epoch:44 step:34577 [D loss: 0.360485, acc.: 84.38%] [G loss: 5.356792]\n",
      "epoch:44 step:34578 [D loss: 0.023111, acc.: 100.00%] [G loss: 5.654272]\n",
      "epoch:44 step:34579 [D loss: 0.005251, acc.: 100.00%] [G loss: 6.285038]\n",
      "epoch:44 step:34580 [D loss: 0.003149, acc.: 100.00%] [G loss: 5.319597]\n",
      "epoch:44 step:34581 [D loss: 0.003199, acc.: 100.00%] [G loss: 5.625420]\n",
      "epoch:44 step:34582 [D loss: 0.004995, acc.: 100.00%] [G loss: 5.770731]\n",
      "epoch:44 step:34583 [D loss: 0.003226, acc.: 100.00%] [G loss: 5.313227]\n",
      "epoch:44 step:34584 [D loss: 0.001631, acc.: 100.00%] [G loss: 4.640119]\n",
      "epoch:44 step:34585 [D loss: 0.008162, acc.: 100.00%] [G loss: 4.655783]\n",
      "epoch:44 step:34586 [D loss: 0.009999, acc.: 100.00%] [G loss: 4.440282]\n",
      "epoch:44 step:34587 [D loss: 0.017865, acc.: 100.00%] [G loss: 3.872818]\n",
      "epoch:44 step:34588 [D loss: 0.017291, acc.: 100.00%] [G loss: 4.466518]\n",
      "epoch:44 step:34589 [D loss: 0.016859, acc.: 100.00%] [G loss: 3.990057]\n",
      "epoch:44 step:34590 [D loss: 0.007911, acc.: 100.00%] [G loss: 4.212584]\n",
      "epoch:44 step:34591 [D loss: 0.008458, acc.: 100.00%] [G loss: 3.923134]\n",
      "epoch:44 step:34592 [D loss: 0.005548, acc.: 100.00%] [G loss: 3.575062]\n",
      "epoch:44 step:34593 [D loss: 0.016361, acc.: 100.00%] [G loss: 3.816725]\n",
      "epoch:44 step:34594 [D loss: 0.013467, acc.: 100.00%] [G loss: 3.923111]\n",
      "epoch:44 step:34595 [D loss: 0.035559, acc.: 100.00%] [G loss: 3.870680]\n",
      "epoch:44 step:34596 [D loss: 0.996385, acc.: 49.22%] [G loss: 9.335880]\n",
      "epoch:44 step:34597 [D loss: 0.003379, acc.: 100.00%] [G loss: 11.043934]\n",
      "epoch:44 step:34598 [D loss: 0.760987, acc.: 66.41%] [G loss: 8.829538]\n",
      "epoch:44 step:34599 [D loss: 0.000720, acc.: 100.00%] [G loss: 8.242694]\n",
      "epoch:44 step:34600 [D loss: 0.000674, acc.: 100.00%] [G loss: 7.794071]\n",
      "##############\n",
      "[1.10404826 2.11517854 0.92464828 1.01518553 1.05956512 0.97110435\n",
      " 1.06523603 0.72672689 2.09969931 1.0240577 ]\n",
      "##########\n",
      "epoch:44 step:34601 [D loss: 0.001714, acc.: 100.00%] [G loss: 7.294542]\n",
      "epoch:44 step:34602 [D loss: 0.001485, acc.: 100.00%] [G loss: 7.634341]\n",
      "epoch:44 step:34603 [D loss: 0.002218, acc.: 100.00%] [G loss: 6.456751]\n",
      "epoch:44 step:34604 [D loss: 0.001833, acc.: 100.00%] [G loss: 7.312706]\n",
      "epoch:44 step:34605 [D loss: 0.002886, acc.: 100.00%] [G loss: 6.318045]\n",
      "epoch:44 step:34606 [D loss: 0.014243, acc.: 99.22%] [G loss: 6.343597]\n",
      "epoch:44 step:34607 [D loss: 0.002313, acc.: 100.00%] [G loss: 6.138419]\n",
      "epoch:44 step:34608 [D loss: 0.002654, acc.: 100.00%] [G loss: 7.271001]\n",
      "epoch:44 step:34609 [D loss: 0.002826, acc.: 100.00%] [G loss: 6.475740]\n",
      "epoch:44 step:34610 [D loss: 0.004122, acc.: 100.00%] [G loss: 6.140798]\n",
      "epoch:44 step:34611 [D loss: 0.006247, acc.: 100.00%] [G loss: 5.906239]\n",
      "epoch:44 step:34612 [D loss: 0.004669, acc.: 100.00%] [G loss: 6.278952]\n",
      "epoch:44 step:34613 [D loss: 0.004199, acc.: 100.00%] [G loss: 5.675757]\n",
      "epoch:44 step:34614 [D loss: 0.004652, acc.: 100.00%] [G loss: 5.470711]\n",
      "epoch:44 step:34615 [D loss: 0.013625, acc.: 100.00%] [G loss: 5.437443]\n",
      "epoch:44 step:34616 [D loss: 0.001600, acc.: 100.00%] [G loss: 4.968150]\n",
      "epoch:44 step:34617 [D loss: 0.005245, acc.: 100.00%] [G loss: 4.528671]\n",
      "epoch:44 step:34618 [D loss: 0.002045, acc.: 100.00%] [G loss: 4.268358]\n",
      "epoch:44 step:34619 [D loss: 0.103834, acc.: 96.09%] [G loss: 6.204068]\n",
      "epoch:44 step:34620 [D loss: 0.001215, acc.: 100.00%] [G loss: 6.592247]\n",
      "epoch:44 step:34621 [D loss: 0.001506, acc.: 100.00%] [G loss: 6.902846]\n",
      "epoch:44 step:34622 [D loss: 0.002341, acc.: 100.00%] [G loss: 6.499280]\n",
      "epoch:44 step:34623 [D loss: 0.001335, acc.: 100.00%] [G loss: 6.327561]\n",
      "epoch:44 step:34624 [D loss: 0.001240, acc.: 100.00%] [G loss: 6.368161]\n",
      "epoch:44 step:34625 [D loss: 0.000881, acc.: 100.00%] [G loss: 5.485375]\n",
      "epoch:44 step:34626 [D loss: 0.003273, acc.: 100.00%] [G loss: 5.479040]\n",
      "epoch:44 step:34627 [D loss: 0.004635, acc.: 100.00%] [G loss: 5.991217]\n",
      "epoch:44 step:34628 [D loss: 0.010964, acc.: 100.00%] [G loss: 4.848315]\n",
      "epoch:44 step:34629 [D loss: 0.013711, acc.: 100.00%] [G loss: 6.803206]\n",
      "epoch:44 step:34630 [D loss: 0.008789, acc.: 100.00%] [G loss: 5.953001]\n",
      "epoch:44 step:34631 [D loss: 0.007182, acc.: 100.00%] [G loss: 5.604252]\n",
      "epoch:44 step:34632 [D loss: 0.002367, acc.: 100.00%] [G loss: 5.444145]\n",
      "epoch:44 step:34633 [D loss: 0.014120, acc.: 100.00%] [G loss: 5.139355]\n",
      "epoch:44 step:34634 [D loss: 0.004326, acc.: 100.00%] [G loss: 5.706906]\n",
      "epoch:44 step:34635 [D loss: 0.008464, acc.: 100.00%] [G loss: 5.071815]\n",
      "epoch:44 step:34636 [D loss: 0.007535, acc.: 100.00%] [G loss: 5.135696]\n",
      "epoch:44 step:34637 [D loss: 0.002789, acc.: 100.00%] [G loss: 5.294535]\n",
      "epoch:44 step:34638 [D loss: 0.016810, acc.: 100.00%] [G loss: 5.000313]\n",
      "epoch:44 step:34639 [D loss: 0.003648, acc.: 100.00%] [G loss: 5.542789]\n",
      "epoch:44 step:34640 [D loss: 0.001680, acc.: 100.00%] [G loss: 4.224692]\n",
      "epoch:44 step:34641 [D loss: 0.047594, acc.: 100.00%] [G loss: 5.966902]\n",
      "epoch:44 step:34642 [D loss: 0.005912, acc.: 100.00%] [G loss: 7.034358]\n",
      "epoch:44 step:34643 [D loss: 0.002143, acc.: 100.00%] [G loss: 6.784745]\n",
      "epoch:44 step:34644 [D loss: 0.011686, acc.: 100.00%] [G loss: 6.995001]\n",
      "epoch:44 step:34645 [D loss: 0.003646, acc.: 100.00%] [G loss: 5.987710]\n",
      "epoch:44 step:34646 [D loss: 0.005229, acc.: 100.00%] [G loss: 5.625093]\n",
      "epoch:44 step:34647 [D loss: 0.009932, acc.: 100.00%] [G loss: 5.127828]\n",
      "epoch:44 step:34648 [D loss: 0.006205, acc.: 100.00%] [G loss: 5.344462]\n",
      "epoch:44 step:34649 [D loss: 0.017103, acc.: 100.00%] [G loss: 5.754470]\n",
      "epoch:44 step:34650 [D loss: 0.004980, acc.: 100.00%] [G loss: 6.525141]\n",
      "epoch:44 step:34651 [D loss: 0.003081, acc.: 100.00%] [G loss: 5.269822]\n",
      "epoch:44 step:34652 [D loss: 0.005840, acc.: 100.00%] [G loss: 6.076988]\n",
      "epoch:44 step:34653 [D loss: 0.008286, acc.: 100.00%] [G loss: 7.115546]\n",
      "epoch:44 step:34654 [D loss: 0.003676, acc.: 100.00%] [G loss: 7.117367]\n",
      "epoch:44 step:34655 [D loss: 0.003850, acc.: 100.00%] [G loss: 5.203878]\n",
      "epoch:44 step:34656 [D loss: 0.013585, acc.: 100.00%] [G loss: 5.566393]\n",
      "epoch:44 step:34657 [D loss: 0.011128, acc.: 100.00%] [G loss: 6.753747]\n",
      "epoch:44 step:34658 [D loss: 0.004828, acc.: 100.00%] [G loss: 5.076154]\n",
      "epoch:44 step:34659 [D loss: 0.006976, acc.: 100.00%] [G loss: 3.607530]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:44 step:34660 [D loss: 0.076514, acc.: 98.44%] [G loss: 8.844223]\n",
      "epoch:44 step:34661 [D loss: 0.007478, acc.: 99.22%] [G loss: 8.124430]\n",
      "epoch:44 step:34662 [D loss: 0.003637, acc.: 100.00%] [G loss: 7.352276]\n",
      "epoch:44 step:34663 [D loss: 0.028727, acc.: 98.44%] [G loss: 7.350152]\n",
      "epoch:44 step:34664 [D loss: 0.002332, acc.: 100.00%] [G loss: 6.131611]\n",
      "epoch:44 step:34665 [D loss: 0.003569, acc.: 100.00%] [G loss: 5.521477]\n",
      "epoch:44 step:34666 [D loss: 0.014447, acc.: 99.22%] [G loss: 7.684566]\n",
      "epoch:44 step:34667 [D loss: 0.001799, acc.: 100.00%] [G loss: 5.817886]\n",
      "epoch:44 step:34668 [D loss: 0.000572, acc.: 100.00%] [G loss: 4.987252]\n",
      "epoch:44 step:34669 [D loss: 0.056558, acc.: 98.44%] [G loss: 8.334545]\n",
      "epoch:44 step:34670 [D loss: 0.000528, acc.: 100.00%] [G loss: 8.594615]\n",
      "epoch:44 step:34671 [D loss: 0.001617, acc.: 100.00%] [G loss: 9.349527]\n",
      "epoch:44 step:34672 [D loss: 0.000697, acc.: 100.00%] [G loss: 7.664029]\n",
      "epoch:44 step:34673 [D loss: 0.010380, acc.: 100.00%] [G loss: 7.177552]\n",
      "epoch:44 step:34674 [D loss: 0.010186, acc.: 100.00%] [G loss: 7.547964]\n",
      "epoch:44 step:34675 [D loss: 0.005461, acc.: 100.00%] [G loss: 6.922892]\n",
      "epoch:44 step:34676 [D loss: 0.011266, acc.: 100.00%] [G loss: 6.161783]\n",
      "epoch:44 step:34677 [D loss: 0.000199, acc.: 100.00%] [G loss: 10.059147]\n",
      "epoch:44 step:34678 [D loss: 0.007431, acc.: 100.00%] [G loss: 6.188507]\n",
      "epoch:44 step:34679 [D loss: 0.003178, acc.: 100.00%] [G loss: 7.209283]\n",
      "epoch:44 step:34680 [D loss: 0.014052, acc.: 100.00%] [G loss: 4.700142]\n",
      "epoch:44 step:34681 [D loss: 0.002849, acc.: 100.00%] [G loss: 4.810052]\n",
      "epoch:44 step:34682 [D loss: 0.095657, acc.: 95.31%] [G loss: 11.302378]\n",
      "epoch:44 step:34683 [D loss: 0.005293, acc.: 100.00%] [G loss: 11.867321]\n",
      "epoch:44 step:34684 [D loss: 0.464942, acc.: 79.69%] [G loss: 6.904597]\n",
      "epoch:44 step:34685 [D loss: 0.024664, acc.: 99.22%] [G loss: 5.312220]\n",
      "epoch:44 step:34686 [D loss: 0.125574, acc.: 93.75%] [G loss: 10.937992]\n",
      "epoch:44 step:34687 [D loss: 0.000020, acc.: 100.00%] [G loss: 11.925451]\n",
      "epoch:44 step:34688 [D loss: 0.000023, acc.: 100.00%] [G loss: 12.322515]\n",
      "epoch:44 step:34689 [D loss: 0.000060, acc.: 100.00%] [G loss: 12.140257]\n",
      "epoch:44 step:34690 [D loss: 0.000057, acc.: 100.00%] [G loss: 11.351562]\n",
      "epoch:44 step:34691 [D loss: 0.000021, acc.: 100.00%] [G loss: 10.329046]\n",
      "epoch:44 step:34692 [D loss: 0.000065, acc.: 100.00%] [G loss: 10.830609]\n",
      "epoch:44 step:34693 [D loss: 0.000159, acc.: 100.00%] [G loss: 9.992521]\n",
      "epoch:44 step:34694 [D loss: 0.000198, acc.: 100.00%] [G loss: 9.689260]\n",
      "epoch:44 step:34695 [D loss: 0.000074, acc.: 100.00%] [G loss: 9.558205]\n",
      "epoch:44 step:34696 [D loss: 0.000918, acc.: 100.00%] [G loss: 8.519854]\n",
      "epoch:44 step:34697 [D loss: 0.000302, acc.: 100.00%] [G loss: 8.134420]\n",
      "epoch:44 step:34698 [D loss: 0.000919, acc.: 100.00%] [G loss: 7.503827]\n",
      "epoch:44 step:34699 [D loss: 0.003652, acc.: 100.00%] [G loss: 5.671401]\n",
      "epoch:44 step:34700 [D loss: 0.046189, acc.: 98.44%] [G loss: 6.745999]\n",
      "epoch:44 step:34701 [D loss: 0.003542, acc.: 100.00%] [G loss: 9.983676]\n",
      "epoch:44 step:34702 [D loss: 0.002503, acc.: 100.00%] [G loss: 7.695253]\n",
      "epoch:44 step:34703 [D loss: 0.002650, acc.: 100.00%] [G loss: 6.507591]\n",
      "epoch:44 step:34704 [D loss: 0.004514, acc.: 100.00%] [G loss: 5.713892]\n",
      "epoch:44 step:34705 [D loss: 0.019259, acc.: 100.00%] [G loss: 6.536714]\n",
      "epoch:44 step:34706 [D loss: 0.000272, acc.: 100.00%] [G loss: 5.373358]\n",
      "epoch:44 step:34707 [D loss: 0.276199, acc.: 87.50%] [G loss: 12.878311]\n",
      "epoch:44 step:34708 [D loss: 0.003350, acc.: 100.00%] [G loss: 15.059011]\n",
      "epoch:44 step:34709 [D loss: 0.035200, acc.: 98.44%] [G loss: 14.788684]\n",
      "epoch:44 step:34710 [D loss: 0.004981, acc.: 100.00%] [G loss: 14.687644]\n",
      "epoch:44 step:34711 [D loss: 0.000887, acc.: 100.00%] [G loss: 15.333387]\n",
      "epoch:44 step:34712 [D loss: 0.000457, acc.: 100.00%] [G loss: 14.284944]\n",
      "epoch:44 step:34713 [D loss: 0.000050, acc.: 100.00%] [G loss: 14.758931]\n",
      "epoch:44 step:34714 [D loss: 0.001174, acc.: 100.00%] [G loss: 13.760513]\n",
      "epoch:44 step:34715 [D loss: 0.000283, acc.: 100.00%] [G loss: 13.422579]\n",
      "epoch:44 step:34716 [D loss: 0.000324, acc.: 100.00%] [G loss: 13.852976]\n",
      "epoch:44 step:34717 [D loss: 0.000835, acc.: 100.00%] [G loss: 12.999160]\n",
      "epoch:44 step:34718 [D loss: 0.000846, acc.: 100.00%] [G loss: 13.054095]\n",
      "epoch:44 step:34719 [D loss: 0.002867, acc.: 100.00%] [G loss: 13.218801]\n",
      "epoch:44 step:34720 [D loss: 0.000020, acc.: 100.00%] [G loss: 13.575361]\n",
      "epoch:44 step:34721 [D loss: 0.000011, acc.: 100.00%] [G loss: 12.958826]\n",
      "epoch:44 step:34722 [D loss: 0.000202, acc.: 100.00%] [G loss: 12.815424]\n",
      "epoch:44 step:34723 [D loss: 0.001494, acc.: 100.00%] [G loss: 12.161523]\n",
      "epoch:44 step:34724 [D loss: 0.000128, acc.: 100.00%] [G loss: 12.192829]\n",
      "epoch:44 step:34725 [D loss: 0.000024, acc.: 100.00%] [G loss: 12.320122]\n",
      "epoch:44 step:34726 [D loss: 0.000087, acc.: 100.00%] [G loss: 11.654936]\n",
      "epoch:44 step:34727 [D loss: 0.000042, acc.: 100.00%] [G loss: 11.900015]\n",
      "epoch:44 step:34728 [D loss: 0.000439, acc.: 100.00%] [G loss: 11.498496]\n",
      "epoch:44 step:34729 [D loss: 0.000028, acc.: 100.00%] [G loss: 11.500393]\n",
      "epoch:44 step:34730 [D loss: 0.000159, acc.: 100.00%] [G loss: 11.701996]\n",
      "epoch:44 step:34731 [D loss: 0.000017, acc.: 100.00%] [G loss: 11.344633]\n",
      "epoch:44 step:34732 [D loss: 0.000016, acc.: 100.00%] [G loss: 10.748486]\n",
      "epoch:44 step:34733 [D loss: 0.000047, acc.: 100.00%] [G loss: 10.894119]\n",
      "epoch:44 step:34734 [D loss: 0.002328, acc.: 100.00%] [G loss: 11.402394]\n",
      "epoch:44 step:34735 [D loss: 0.000039, acc.: 100.00%] [G loss: 11.513224]\n",
      "epoch:44 step:34736 [D loss: 0.000051, acc.: 100.00%] [G loss: 10.432108]\n",
      "epoch:44 step:34737 [D loss: 0.000263, acc.: 100.00%] [G loss: 10.907493]\n",
      "epoch:44 step:34738 [D loss: 0.000144, acc.: 100.00%] [G loss: 10.518873]\n",
      "epoch:44 step:34739 [D loss: 0.000057, acc.: 100.00%] [G loss: 10.423409]\n",
      "epoch:44 step:34740 [D loss: 0.000261, acc.: 100.00%] [G loss: 10.155420]\n",
      "epoch:44 step:34741 [D loss: 0.000094, acc.: 100.00%] [G loss: 10.193660]\n",
      "epoch:44 step:34742 [D loss: 0.000035, acc.: 100.00%] [G loss: 10.178493]\n",
      "epoch:44 step:34743 [D loss: 0.000169, acc.: 100.00%] [G loss: 9.734818]\n",
      "epoch:44 step:34744 [D loss: 0.000078, acc.: 100.00%] [G loss: 9.643957]\n",
      "epoch:44 step:34745 [D loss: 0.000074, acc.: 100.00%] [G loss: 9.691273]\n",
      "epoch:44 step:34746 [D loss: 0.000259, acc.: 100.00%] [G loss: 9.510953]\n",
      "epoch:44 step:34747 [D loss: 0.000132, acc.: 100.00%] [G loss: 9.623756]\n",
      "epoch:44 step:34748 [D loss: 0.000211, acc.: 100.00%] [G loss: 9.082900]\n",
      "epoch:44 step:34749 [D loss: 0.001455, acc.: 100.00%] [G loss: 9.359912]\n",
      "epoch:44 step:34750 [D loss: 0.000290, acc.: 100.00%] [G loss: 9.419625]\n",
      "epoch:44 step:34751 [D loss: 0.000135, acc.: 100.00%] [G loss: 8.790522]\n",
      "epoch:44 step:34752 [D loss: 0.000131, acc.: 100.00%] [G loss: 8.779425]\n",
      "epoch:44 step:34753 [D loss: 0.000244, acc.: 100.00%] [G loss: 8.998096]\n",
      "epoch:44 step:34754 [D loss: 0.000538, acc.: 100.00%] [G loss: 8.205036]\n",
      "epoch:44 step:34755 [D loss: 0.000318, acc.: 100.00%] [G loss: 8.478729]\n",
      "epoch:44 step:34756 [D loss: 0.000713, acc.: 100.00%] [G loss: 8.568567]\n",
      "epoch:44 step:34757 [D loss: 0.002993, acc.: 100.00%] [G loss: 7.727253]\n",
      "epoch:44 step:34758 [D loss: 0.000341, acc.: 100.00%] [G loss: 6.939211]\n",
      "epoch:44 step:34759 [D loss: 0.000967, acc.: 100.00%] [G loss: 6.452184]\n",
      "epoch:44 step:34760 [D loss: 0.005182, acc.: 100.00%] [G loss: 6.218489]\n",
      "epoch:44 step:34761 [D loss: 0.003591, acc.: 100.00%] [G loss: 6.001585]\n",
      "epoch:44 step:34762 [D loss: 0.002514, acc.: 100.00%] [G loss: 5.606584]\n",
      "epoch:44 step:34763 [D loss: 0.006036, acc.: 100.00%] [G loss: 5.039292]\n",
      "epoch:44 step:34764 [D loss: 0.022346, acc.: 100.00%] [G loss: 6.287189]\n",
      "epoch:44 step:34765 [D loss: 0.002018, acc.: 100.00%] [G loss: 6.081706]\n",
      "epoch:44 step:34766 [D loss: 0.004731, acc.: 100.00%] [G loss: 5.837197]\n",
      "epoch:44 step:34767 [D loss: 0.006190, acc.: 100.00%] [G loss: 6.225300]\n",
      "epoch:44 step:34768 [D loss: 0.000986, acc.: 100.00%] [G loss: 6.746024]\n",
      "epoch:44 step:34769 [D loss: 0.008966, acc.: 100.00%] [G loss: 5.244332]\n",
      "epoch:44 step:34770 [D loss: 0.001961, acc.: 100.00%] [G loss: 6.833874]\n",
      "epoch:44 step:34771 [D loss: 0.002513, acc.: 100.00%] [G loss: 4.505663]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:44 step:34772 [D loss: 0.001279, acc.: 100.00%] [G loss: 6.084183]\n",
      "epoch:44 step:34773 [D loss: 0.001174, acc.: 100.00%] [G loss: 7.569874]\n",
      "epoch:44 step:34774 [D loss: 0.014514, acc.: 100.00%] [G loss: 6.619850]\n",
      "epoch:44 step:34775 [D loss: 0.000584, acc.: 100.00%] [G loss: 7.693341]\n",
      "epoch:44 step:34776 [D loss: 0.002157, acc.: 100.00%] [G loss: 8.340364]\n",
      "epoch:44 step:34777 [D loss: 0.009438, acc.: 100.00%] [G loss: 7.082720]\n",
      "epoch:44 step:34778 [D loss: 0.001782, acc.: 100.00%] [G loss: 6.520523]\n",
      "epoch:44 step:34779 [D loss: 0.047189, acc.: 100.00%] [G loss: 13.728571]\n",
      "epoch:44 step:34780 [D loss: 0.003144, acc.: 100.00%] [G loss: 15.489229]\n",
      "epoch:44 step:34781 [D loss: 0.002435, acc.: 100.00%] [G loss: 14.998878]\n",
      "epoch:44 step:34782 [D loss: 0.000065, acc.: 100.00%] [G loss: 14.686269]\n",
      "epoch:44 step:34783 [D loss: 0.000982, acc.: 100.00%] [G loss: 14.423357]\n",
      "epoch:44 step:34784 [D loss: 0.047175, acc.: 97.66%] [G loss: 14.321606]\n",
      "epoch:44 step:34785 [D loss: 0.000154, acc.: 100.00%] [G loss: 13.142973]\n",
      "epoch:44 step:34786 [D loss: 0.000402, acc.: 100.00%] [G loss: 13.674730]\n",
      "epoch:44 step:34787 [D loss: 0.000902, acc.: 100.00%] [G loss: 14.269651]\n",
      "epoch:44 step:34788 [D loss: 0.000407, acc.: 100.00%] [G loss: 13.181005]\n",
      "epoch:44 step:34789 [D loss: 0.000090, acc.: 100.00%] [G loss: 12.231330]\n",
      "epoch:44 step:34790 [D loss: 0.000589, acc.: 100.00%] [G loss: 12.248189]\n",
      "epoch:44 step:34791 [D loss: 0.001167, acc.: 100.00%] [G loss: 11.216602]\n",
      "epoch:44 step:34792 [D loss: 0.000111, acc.: 100.00%] [G loss: 12.081642]\n",
      "epoch:44 step:34793 [D loss: 0.000445, acc.: 100.00%] [G loss: 10.948709]\n",
      "epoch:44 step:34794 [D loss: 0.014310, acc.: 99.22%] [G loss: 11.197731]\n",
      "epoch:44 step:34795 [D loss: 0.002302, acc.: 100.00%] [G loss: 12.899769]\n",
      "epoch:44 step:34796 [D loss: 0.000062, acc.: 100.00%] [G loss: 12.443432]\n",
      "epoch:44 step:34797 [D loss: 0.000128, acc.: 100.00%] [G loss: 10.144861]\n",
      "epoch:44 step:34798 [D loss: 0.000921, acc.: 100.00%] [G loss: 9.511458]\n",
      "epoch:44 step:34799 [D loss: 0.000406, acc.: 100.00%] [G loss: 11.352269]\n",
      "epoch:44 step:34800 [D loss: 0.012556, acc.: 100.00%] [G loss: 9.786735]\n",
      "##############\n",
      "[1.10310788 0.90861349 2.10122859 1.1083875  2.1048065  0.96216585\n",
      " 2.11276993 2.11409397 2.10420345 1.05300804]\n",
      "##########\n",
      "epoch:44 step:34801 [D loss: 0.002228, acc.: 100.00%] [G loss: 11.932090]\n",
      "epoch:44 step:34802 [D loss: 0.000338, acc.: 100.00%] [G loss: 11.860624]\n",
      "epoch:44 step:34803 [D loss: 0.000213, acc.: 100.00%] [G loss: 10.724301]\n",
      "epoch:44 step:34804 [D loss: 0.001182, acc.: 100.00%] [G loss: 10.509888]\n",
      "epoch:44 step:34805 [D loss: 0.005137, acc.: 100.00%] [G loss: 7.550045]\n",
      "epoch:44 step:34806 [D loss: 0.002954, acc.: 100.00%] [G loss: 6.592407]\n",
      "epoch:44 step:34807 [D loss: 0.039497, acc.: 99.22%] [G loss: 11.463797]\n",
      "epoch:44 step:34808 [D loss: 0.000191, acc.: 100.00%] [G loss: 12.542825]\n",
      "epoch:44 step:34809 [D loss: 0.004765, acc.: 100.00%] [G loss: 12.198650]\n",
      "epoch:44 step:34810 [D loss: 0.006970, acc.: 100.00%] [G loss: 11.777517]\n",
      "epoch:44 step:34811 [D loss: 0.000598, acc.: 100.00%] [G loss: 13.117815]\n",
      "epoch:44 step:34812 [D loss: 0.004090, acc.: 100.00%] [G loss: 9.718098]\n",
      "epoch:44 step:34813 [D loss: 0.001611, acc.: 100.00%] [G loss: 9.126211]\n",
      "epoch:44 step:34814 [D loss: 0.000886, acc.: 100.00%] [G loss: 6.624845]\n",
      "epoch:44 step:34815 [D loss: 0.029103, acc.: 100.00%] [G loss: 10.459474]\n",
      "epoch:44 step:34816 [D loss: 0.001076, acc.: 100.00%] [G loss: 10.132767]\n",
      "epoch:44 step:34817 [D loss: 0.026961, acc.: 99.22%] [G loss: 8.095660]\n",
      "epoch:44 step:34818 [D loss: 0.001300, acc.: 100.00%] [G loss: 8.995497]\n",
      "epoch:44 step:34819 [D loss: 0.000073, acc.: 100.00%] [G loss: 8.453350]\n",
      "epoch:44 step:34820 [D loss: 0.000179, acc.: 100.00%] [G loss: 6.944650]\n",
      "epoch:44 step:34821 [D loss: 0.005554, acc.: 100.00%] [G loss: 5.636461]\n",
      "epoch:44 step:34822 [D loss: 0.027674, acc.: 100.00%] [G loss: 7.899130]\n",
      "epoch:44 step:34823 [D loss: 0.010082, acc.: 100.00%] [G loss: 12.793695]\n",
      "epoch:44 step:34824 [D loss: 0.000618, acc.: 100.00%] [G loss: 12.917592]\n",
      "epoch:44 step:34825 [D loss: 0.000073, acc.: 100.00%] [G loss: 8.119602]\n",
      "epoch:44 step:34826 [D loss: 0.001250, acc.: 100.00%] [G loss: 8.246641]\n",
      "epoch:44 step:34827 [D loss: 0.000077, acc.: 100.00%] [G loss: 5.892304]\n",
      "epoch:44 step:34828 [D loss: 0.010363, acc.: 100.00%] [G loss: 9.651440]\n",
      "epoch:44 step:34829 [D loss: 0.000459, acc.: 100.00%] [G loss: 8.892148]\n",
      "epoch:44 step:34830 [D loss: 0.000292, acc.: 100.00%] [G loss: 7.582500]\n",
      "epoch:44 step:34831 [D loss: 0.002327, acc.: 100.00%] [G loss: 12.631342]\n",
      "epoch:44 step:34832 [D loss: 0.142966, acc.: 91.41%] [G loss: 16.057392]\n",
      "epoch:44 step:34833 [D loss: 0.000801, acc.: 100.00%] [G loss: 16.055782]\n",
      "epoch:44 step:34834 [D loss: 0.107247, acc.: 97.66%] [G loss: 16.108232]\n",
      "epoch:44 step:34835 [D loss: 0.000069, acc.: 100.00%] [G loss: 16.044271]\n",
      "epoch:44 step:34836 [D loss: 0.000072, acc.: 100.00%] [G loss: 15.827923]\n",
      "epoch:44 step:34837 [D loss: 0.000055, acc.: 100.00%] [G loss: 15.898081]\n",
      "epoch:44 step:34838 [D loss: 0.013999, acc.: 99.22%] [G loss: 15.851561]\n",
      "epoch:44 step:34839 [D loss: 0.000143, acc.: 100.00%] [G loss: 15.881149]\n",
      "epoch:44 step:34840 [D loss: 0.000088, acc.: 100.00%] [G loss: 15.671847]\n",
      "epoch:44 step:34841 [D loss: 0.000069, acc.: 100.00%] [G loss: 15.924004]\n",
      "epoch:44 step:34842 [D loss: 0.001440, acc.: 100.00%] [G loss: 15.660091]\n",
      "epoch:44 step:34843 [D loss: 0.005824, acc.: 100.00%] [G loss: 15.601902]\n",
      "epoch:44 step:34844 [D loss: 0.000137, acc.: 100.00%] [G loss: 15.893848]\n",
      "epoch:44 step:34845 [D loss: 0.000291, acc.: 100.00%] [G loss: 15.526773]\n",
      "epoch:44 step:34846 [D loss: 0.000768, acc.: 100.00%] [G loss: 15.661598]\n",
      "epoch:44 step:34847 [D loss: 0.000137, acc.: 100.00%] [G loss: 15.672908]\n",
      "epoch:44 step:34848 [D loss: 0.001590, acc.: 100.00%] [G loss: 16.037693]\n",
      "epoch:44 step:34849 [D loss: 0.002926, acc.: 100.00%] [G loss: 15.938785]\n",
      "epoch:44 step:34850 [D loss: 0.000033, acc.: 100.00%] [G loss: 15.632737]\n",
      "epoch:44 step:34851 [D loss: 0.000005, acc.: 100.00%] [G loss: 15.948006]\n",
      "epoch:44 step:34852 [D loss: 0.000092, acc.: 100.00%] [G loss: 15.900507]\n",
      "epoch:44 step:34853 [D loss: 0.000002, acc.: 100.00%] [G loss: 15.891689]\n",
      "epoch:44 step:34854 [D loss: 0.000009, acc.: 100.00%] [G loss: 15.668080]\n",
      "epoch:44 step:34855 [D loss: 0.000040, acc.: 100.00%] [G loss: 15.912236]\n",
      "epoch:44 step:34856 [D loss: 0.001226, acc.: 100.00%] [G loss: 15.871696]\n",
      "epoch:44 step:34857 [D loss: 0.000007, acc.: 100.00%] [G loss: 15.854248]\n",
      "epoch:44 step:34858 [D loss: 0.000005, acc.: 100.00%] [G loss: 15.972872]\n",
      "epoch:44 step:34859 [D loss: 0.000064, acc.: 100.00%] [G loss: 15.825182]\n",
      "epoch:44 step:34860 [D loss: 0.000002, acc.: 100.00%] [G loss: 15.850356]\n",
      "epoch:44 step:34861 [D loss: 0.000026, acc.: 100.00%] [G loss: 15.911909]\n",
      "epoch:44 step:34862 [D loss: 0.000181, acc.: 100.00%] [G loss: 15.930223]\n",
      "epoch:44 step:34863 [D loss: 0.000122, acc.: 100.00%] [G loss: 15.685911]\n",
      "epoch:44 step:34864 [D loss: 0.000168, acc.: 100.00%] [G loss: 15.503652]\n",
      "epoch:44 step:34865 [D loss: 0.000147, acc.: 100.00%] [G loss: 15.645840]\n",
      "epoch:44 step:34866 [D loss: 0.000202, acc.: 100.00%] [G loss: 15.691351]\n",
      "epoch:44 step:34867 [D loss: 0.000004, acc.: 100.00%] [G loss: 15.872621]\n",
      "epoch:44 step:34868 [D loss: 0.000002, acc.: 100.00%] [G loss: 15.889937]\n",
      "epoch:44 step:34869 [D loss: 0.000069, acc.: 100.00%] [G loss: 15.288183]\n",
      "epoch:44 step:34870 [D loss: 0.000024, acc.: 100.00%] [G loss: 15.386950]\n",
      "epoch:44 step:34871 [D loss: 0.000038, acc.: 100.00%] [G loss: 15.605575]\n",
      "epoch:44 step:34872 [D loss: 0.000170, acc.: 100.00%] [G loss: 15.686350]\n",
      "epoch:44 step:34873 [D loss: 0.000037, acc.: 100.00%] [G loss: 15.523529]\n",
      "epoch:44 step:34874 [D loss: 0.000062, acc.: 100.00%] [G loss: 15.194666]\n",
      "epoch:44 step:34875 [D loss: 0.000032, acc.: 100.00%] [G loss: 15.861866]\n",
      "epoch:44 step:34876 [D loss: 0.000022, acc.: 100.00%] [G loss: 15.801769]\n",
      "epoch:44 step:34877 [D loss: 0.000222, acc.: 100.00%] [G loss: 15.235548]\n",
      "epoch:44 step:34878 [D loss: 0.000002, acc.: 100.00%] [G loss: 15.331350]\n",
      "epoch:44 step:34879 [D loss: 0.000175, acc.: 100.00%] [G loss: 15.384714]\n",
      "epoch:44 step:34880 [D loss: 0.000110, acc.: 100.00%] [G loss: 15.117548]\n",
      "epoch:44 step:34881 [D loss: 0.000020, acc.: 100.00%] [G loss: 15.377380]\n",
      "epoch:44 step:34882 [D loss: 0.000031, acc.: 100.00%] [G loss: 15.140082]\n",
      "epoch:44 step:34883 [D loss: 0.000162, acc.: 100.00%] [G loss: 15.752251]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:44 step:34884 [D loss: 0.000007, acc.: 100.00%] [G loss: 14.790803]\n",
      "epoch:44 step:34885 [D loss: 0.000278, acc.: 100.00%] [G loss: 15.062111]\n",
      "epoch:44 step:34886 [D loss: 0.000105, acc.: 100.00%] [G loss: 14.832540]\n",
      "epoch:44 step:34887 [D loss: 0.000099, acc.: 100.00%] [G loss: 15.297187]\n",
      "epoch:44 step:34888 [D loss: 0.000462, acc.: 100.00%] [G loss: 15.163218]\n",
      "epoch:44 step:34889 [D loss: 0.000081, acc.: 100.00%] [G loss: 15.409472]\n",
      "epoch:44 step:34890 [D loss: 0.000266, acc.: 100.00%] [G loss: 15.368828]\n",
      "epoch:44 step:34891 [D loss: 0.000018, acc.: 100.00%] [G loss: 15.041772]\n",
      "epoch:44 step:34892 [D loss: 0.000012, acc.: 100.00%] [G loss: 14.701779]\n",
      "epoch:44 step:34893 [D loss: 0.000306, acc.: 100.00%] [G loss: 14.149647]\n",
      "epoch:44 step:34894 [D loss: 0.000007, acc.: 100.00%] [G loss: 14.882437]\n",
      "epoch:44 step:34895 [D loss: 0.000023, acc.: 100.00%] [G loss: 14.661089]\n",
      "epoch:44 step:34896 [D loss: 0.000007, acc.: 100.00%] [G loss: 15.978844]\n",
      "epoch:44 step:34897 [D loss: 0.000230, acc.: 100.00%] [G loss: 14.297075]\n",
      "epoch:44 step:34898 [D loss: 0.000260, acc.: 100.00%] [G loss: 14.232015]\n",
      "epoch:44 step:34899 [D loss: 0.001312, acc.: 100.00%] [G loss: 14.770372]\n",
      "epoch:44 step:34900 [D loss: 0.000108, acc.: 100.00%] [G loss: 14.115977]\n",
      "epoch:44 step:34901 [D loss: 0.000030, acc.: 100.00%] [G loss: 14.455530]\n",
      "epoch:44 step:34902 [D loss: 0.000524, acc.: 100.00%] [G loss: 14.458262]\n",
      "epoch:44 step:34903 [D loss: 0.000043, acc.: 100.00%] [G loss: 14.381637]\n",
      "epoch:44 step:34904 [D loss: 0.000010, acc.: 100.00%] [G loss: 14.323225]\n",
      "epoch:44 step:34905 [D loss: 0.000133, acc.: 100.00%] [G loss: 15.824273]\n",
      "epoch:44 step:34906 [D loss: 0.000466, acc.: 100.00%] [G loss: 14.313841]\n",
      "epoch:44 step:34907 [D loss: 0.000282, acc.: 100.00%] [G loss: 13.741089]\n",
      "epoch:44 step:34908 [D loss: 0.000006, acc.: 100.00%] [G loss: 14.293692]\n",
      "epoch:44 step:34909 [D loss: 0.000030, acc.: 100.00%] [G loss: 14.061832]\n",
      "epoch:44 step:34910 [D loss: 0.000070, acc.: 100.00%] [G loss: 13.677785]\n",
      "epoch:44 step:34911 [D loss: 0.000288, acc.: 100.00%] [G loss: 14.012960]\n",
      "epoch:44 step:34912 [D loss: 0.000039, acc.: 100.00%] [G loss: 14.098763]\n",
      "epoch:44 step:34913 [D loss: 0.000038, acc.: 100.00%] [G loss: 13.930918]\n",
      "epoch:44 step:34914 [D loss: 0.000181, acc.: 100.00%] [G loss: 13.776418]\n",
      "epoch:44 step:34915 [D loss: 0.000650, acc.: 100.00%] [G loss: 13.554380]\n",
      "epoch:44 step:34916 [D loss: 0.001131, acc.: 100.00%] [G loss: 13.636877]\n",
      "epoch:44 step:34917 [D loss: 0.000699, acc.: 100.00%] [G loss: 13.664125]\n",
      "epoch:44 step:34918 [D loss: 0.000288, acc.: 100.00%] [G loss: 13.898271]\n",
      "epoch:44 step:34919 [D loss: 0.000809, acc.: 100.00%] [G loss: 15.611980]\n",
      "epoch:44 step:34920 [D loss: 0.000116, acc.: 100.00%] [G loss: 13.380823]\n",
      "epoch:44 step:34921 [D loss: 0.000074, acc.: 100.00%] [G loss: 13.998702]\n",
      "epoch:44 step:34922 [D loss: 0.000340, acc.: 100.00%] [G loss: 14.188991]\n",
      "epoch:44 step:34923 [D loss: 0.001252, acc.: 100.00%] [G loss: 14.097980]\n",
      "epoch:44 step:34924 [D loss: 0.000777, acc.: 100.00%] [G loss: 13.951337]\n",
      "epoch:44 step:34925 [D loss: 0.000278, acc.: 100.00%] [G loss: 13.825788]\n",
      "epoch:44 step:34926 [D loss: 0.000302, acc.: 100.00%] [G loss: 13.128936]\n",
      "epoch:44 step:34927 [D loss: 0.000098, acc.: 100.00%] [G loss: 13.828383]\n",
      "epoch:44 step:34928 [D loss: 0.000124, acc.: 100.00%] [G loss: 12.790878]\n",
      "epoch:44 step:34929 [D loss: 0.000710, acc.: 100.00%] [G loss: 13.087475]\n",
      "epoch:44 step:34930 [D loss: 0.001118, acc.: 100.00%] [G loss: 12.189754]\n",
      "epoch:44 step:34931 [D loss: 0.000622, acc.: 100.00%] [G loss: 11.716235]\n",
      "epoch:44 step:34932 [D loss: 0.000104, acc.: 100.00%] [G loss: 12.150425]\n",
      "epoch:44 step:34933 [D loss: 0.001067, acc.: 100.00%] [G loss: 10.197216]\n",
      "epoch:44 step:34934 [D loss: 0.003479, acc.: 100.00%] [G loss: 12.279318]\n",
      "epoch:44 step:34935 [D loss: 0.001297, acc.: 100.00%] [G loss: 11.885396]\n",
      "epoch:44 step:34936 [D loss: 0.001033, acc.: 100.00%] [G loss: 11.268764]\n",
      "epoch:44 step:34937 [D loss: 0.001574, acc.: 100.00%] [G loss: 8.921402]\n",
      "epoch:44 step:34938 [D loss: 0.000955, acc.: 100.00%] [G loss: 8.418681]\n",
      "epoch:44 step:34939 [D loss: 0.001620, acc.: 100.00%] [G loss: 9.260460]\n",
      "epoch:44 step:34940 [D loss: 0.002904, acc.: 100.00%] [G loss: 8.024787]\n",
      "epoch:44 step:34941 [D loss: 0.009492, acc.: 100.00%] [G loss: 7.431335]\n",
      "epoch:44 step:34942 [D loss: 0.000327, acc.: 100.00%] [G loss: 7.270055]\n",
      "epoch:44 step:34943 [D loss: 0.007022, acc.: 100.00%] [G loss: 7.551171]\n",
      "epoch:44 step:34944 [D loss: 0.000203, acc.: 100.00%] [G loss: 7.351983]\n",
      "epoch:44 step:34945 [D loss: 0.001616, acc.: 100.00%] [G loss: 6.572939]\n",
      "epoch:44 step:34946 [D loss: 0.001492, acc.: 100.00%] [G loss: 6.652352]\n",
      "epoch:44 step:34947 [D loss: 0.025299, acc.: 99.22%] [G loss: 10.574676]\n",
      "epoch:44 step:34948 [D loss: 0.000344, acc.: 100.00%] [G loss: 10.257353]\n",
      "epoch:44 step:34949 [D loss: 0.010053, acc.: 99.22%] [G loss: 11.563196]\n",
      "epoch:44 step:34950 [D loss: 0.001225, acc.: 100.00%] [G loss: 11.601312]\n",
      "epoch:44 step:34951 [D loss: 0.022433, acc.: 98.44%] [G loss: 10.616995]\n",
      "epoch:44 step:34952 [D loss: 0.001242, acc.: 100.00%] [G loss: 8.425980]\n",
      "epoch:44 step:34953 [D loss: 0.000774, acc.: 100.00%] [G loss: 7.716082]\n",
      "epoch:44 step:34954 [D loss: 0.004472, acc.: 100.00%] [G loss: 6.212456]\n",
      "epoch:44 step:34955 [D loss: 0.001158, acc.: 100.00%] [G loss: 14.624248]\n",
      "epoch:44 step:34956 [D loss: 0.008867, acc.: 99.22%] [G loss: 9.380897]\n",
      "epoch:44 step:34957 [D loss: 0.000486, acc.: 100.00%] [G loss: 7.852294]\n",
      "epoch:44 step:34958 [D loss: 0.000190, acc.: 100.00%] [G loss: 7.606906]\n",
      "epoch:44 step:34959 [D loss: 0.000866, acc.: 100.00%] [G loss: 13.198540]\n",
      "epoch:44 step:34960 [D loss: 0.002215, acc.: 100.00%] [G loss: 8.291073]\n",
      "epoch:44 step:34961 [D loss: 0.001727, acc.: 100.00%] [G loss: 6.757225]\n",
      "epoch:44 step:34962 [D loss: 0.000442, acc.: 100.00%] [G loss: 7.853981]\n",
      "epoch:44 step:34963 [D loss: 0.000108, acc.: 100.00%] [G loss: 8.401014]\n",
      "epoch:44 step:34964 [D loss: 0.029213, acc.: 99.22%] [G loss: 7.996212]\n",
      "epoch:44 step:34965 [D loss: 0.000432, acc.: 100.00%] [G loss: 9.193759]\n",
      "epoch:44 step:34966 [D loss: 0.000949, acc.: 100.00%] [G loss: 8.998261]\n",
      "epoch:44 step:34967 [D loss: 0.002465, acc.: 100.00%] [G loss: 7.618866]\n",
      "epoch:44 step:34968 [D loss: 0.048864, acc.: 97.66%] [G loss: 7.518691]\n",
      "epoch:44 step:34969 [D loss: 0.003891, acc.: 100.00%] [G loss: 4.554932]\n",
      "epoch:44 step:34970 [D loss: 0.025274, acc.: 100.00%] [G loss: 11.680400]\n",
      "epoch:44 step:34971 [D loss: 0.000151, acc.: 100.00%] [G loss: 11.932324]\n",
      "epoch:44 step:34972 [D loss: 0.000022, acc.: 100.00%] [G loss: 12.469097]\n",
      "epoch:44 step:34973 [D loss: 0.003563, acc.: 100.00%] [G loss: 12.480268]\n",
      "epoch:44 step:34974 [D loss: 0.000392, acc.: 100.00%] [G loss: 14.458788]\n",
      "epoch:44 step:34975 [D loss: 0.000413, acc.: 100.00%] [G loss: 9.412393]\n",
      "epoch:44 step:34976 [D loss: 0.002374, acc.: 100.00%] [G loss: 10.251259]\n",
      "epoch:44 step:34977 [D loss: 0.000097, acc.: 100.00%] [G loss: 10.047709]\n",
      "epoch:44 step:34978 [D loss: 0.003111, acc.: 100.00%] [G loss: 10.758747]\n",
      "epoch:44 step:34979 [D loss: 0.000420, acc.: 100.00%] [G loss: 8.157608]\n",
      "epoch:44 step:34980 [D loss: 0.003737, acc.: 100.00%] [G loss: 10.471616]\n",
      "epoch:44 step:34981 [D loss: 0.003888, acc.: 100.00%] [G loss: 13.980943]\n",
      "epoch:44 step:34982 [D loss: 0.001057, acc.: 100.00%] [G loss: 8.189528]\n",
      "epoch:44 step:34983 [D loss: 0.000474, acc.: 100.00%] [G loss: 7.852041]\n",
      "epoch:44 step:34984 [D loss: 0.007015, acc.: 100.00%] [G loss: 9.371969]\n",
      "epoch:44 step:34985 [D loss: 0.000128, acc.: 100.00%] [G loss: 7.414653]\n",
      "epoch:44 step:34986 [D loss: 0.003272, acc.: 100.00%] [G loss: 4.688365]\n",
      "epoch:44 step:34987 [D loss: 0.003024, acc.: 100.00%] [G loss: 4.836082]\n",
      "epoch:44 step:34988 [D loss: 0.126138, acc.: 96.09%] [G loss: 16.046684]\n",
      "epoch:44 step:34989 [D loss: 0.007418, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:44 step:34990 [D loss: 0.322908, acc.: 89.84%] [G loss: 16.087425]\n",
      "epoch:44 step:34991 [D loss: 0.079639, acc.: 98.44%] [G loss: 16.024979]\n",
      "epoch:44 step:34992 [D loss: 0.000002, acc.: 100.00%] [G loss: 16.030556]\n",
      "epoch:44 step:34993 [D loss: 0.000088, acc.: 100.00%] [G loss: 15.772808]\n",
      "epoch:44 step:34994 [D loss: 0.000517, acc.: 100.00%] [G loss: 15.828236]\n",
      "epoch:44 step:34995 [D loss: 0.000005, acc.: 100.00%] [G loss: 15.949614]\n",
      "epoch:44 step:34996 [D loss: 0.000002, acc.: 100.00%] [G loss: 15.822039]\n",
      "epoch:44 step:34997 [D loss: 0.000038, acc.: 100.00%] [G loss: 15.559981]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:44 step:34998 [D loss: 0.000235, acc.: 100.00%] [G loss: 15.744688]\n",
      "epoch:44 step:34999 [D loss: 0.000003, acc.: 100.00%] [G loss: 16.098120]\n",
      "epoch:44 step:35000 [D loss: 0.000003, acc.: 100.00%] [G loss: 15.921163]\n",
      "##############\n",
      "[0.81774863 1.0083978  0.7536882  0.79945348 0.84048949 1.051896\n",
      " 0.7628522  1.11930249 1.03603408 0.98784732]\n",
      "##########\n",
      "epoch:44 step:35001 [D loss: 0.000006, acc.: 100.00%] [G loss: 15.619505]\n",
      "epoch:44 step:35002 [D loss: 0.000178, acc.: 100.00%] [G loss: 15.729527]\n",
      "epoch:44 step:35003 [D loss: 0.000000, acc.: 100.00%] [G loss: 15.202398]\n",
      "epoch:44 step:35004 [D loss: 0.020372, acc.: 99.22%] [G loss: 15.198051]\n",
      "epoch:44 step:35005 [D loss: 0.000002, acc.: 100.00%] [G loss: 15.767266]\n",
      "epoch:44 step:35006 [D loss: 0.000002, acc.: 100.00%] [G loss: 15.582273]\n",
      "epoch:44 step:35007 [D loss: 0.000001, acc.: 100.00%] [G loss: 15.850653]\n",
      "epoch:44 step:35008 [D loss: 0.000003, acc.: 100.00%] [G loss: 16.050943]\n",
      "epoch:44 step:35009 [D loss: 0.000066, acc.: 100.00%] [G loss: 15.665422]\n",
      "epoch:44 step:35010 [D loss: 0.000014, acc.: 100.00%] [G loss: 15.734207]\n",
      "epoch:44 step:35011 [D loss: 0.000890, acc.: 100.00%] [G loss: 15.470882]\n",
      "epoch:44 step:35012 [D loss: 0.000065, acc.: 100.00%] [G loss: 15.668198]\n",
      "epoch:44 step:35013 [D loss: 0.000003, acc.: 100.00%] [G loss: 15.625782]\n",
      "epoch:44 step:35014 [D loss: 0.000015, acc.: 100.00%] [G loss: 15.499277]\n",
      "epoch:44 step:35015 [D loss: 0.000017, acc.: 100.00%] [G loss: 15.475855]\n",
      "epoch:44 step:35016 [D loss: 0.000020, acc.: 100.00%] [G loss: 15.600401]\n",
      "epoch:44 step:35017 [D loss: 0.001385, acc.: 100.00%] [G loss: 15.426439]\n",
      "epoch:44 step:35018 [D loss: 0.001413, acc.: 100.00%] [G loss: 15.507923]\n",
      "epoch:44 step:35019 [D loss: 0.000102, acc.: 100.00%] [G loss: 14.658047]\n",
      "epoch:44 step:35020 [D loss: 0.000201, acc.: 100.00%] [G loss: 15.565166]\n",
      "epoch:44 step:35021 [D loss: 0.000075, acc.: 100.00%] [G loss: 15.842363]\n",
      "epoch:44 step:35022 [D loss: 0.001213, acc.: 100.00%] [G loss: 14.514421]\n",
      "epoch:44 step:35023 [D loss: 0.001673, acc.: 100.00%] [G loss: 14.862112]\n",
      "epoch:44 step:35024 [D loss: 0.000004, acc.: 100.00%] [G loss: 14.169841]\n",
      "epoch:44 step:35025 [D loss: 0.000233, acc.: 100.00%] [G loss: 15.548061]\n",
      "epoch:44 step:35026 [D loss: 0.000900, acc.: 100.00%] [G loss: 13.503428]\n",
      "epoch:44 step:35027 [D loss: 0.015564, acc.: 99.22%] [G loss: 14.528791]\n",
      "epoch:44 step:35028 [D loss: 0.000398, acc.: 100.00%] [G loss: 15.385696]\n",
      "epoch:44 step:35029 [D loss: 0.000005, acc.: 100.00%] [G loss: 14.572733]\n",
      "epoch:44 step:35030 [D loss: 0.000003, acc.: 100.00%] [G loss: 15.412263]\n",
      "epoch:44 step:35031 [D loss: 0.000201, acc.: 100.00%] [G loss: 14.961791]\n",
      "epoch:44 step:35032 [D loss: 0.000017, acc.: 100.00%] [G loss: 14.964630]\n",
      "epoch:44 step:35033 [D loss: 0.042302, acc.: 98.44%] [G loss: 16.118095]\n",
      "epoch:44 step:35034 [D loss: 0.000009, acc.: 100.00%] [G loss: 15.899045]\n",
      "epoch:44 step:35035 [D loss: 0.000001, acc.: 100.00%] [G loss: 15.768251]\n",
      "epoch:44 step:35036 [D loss: 0.000009, acc.: 100.00%] [G loss: 15.981483]\n",
      "epoch:44 step:35037 [D loss: 0.000042, acc.: 100.00%] [G loss: 15.665359]\n",
      "epoch:44 step:35038 [D loss: 0.000002, acc.: 100.00%] [G loss: 15.525495]\n",
      "epoch:44 step:35039 [D loss: 0.000000, acc.: 100.00%] [G loss: 15.541498]\n",
      "epoch:44 step:35040 [D loss: 0.000072, acc.: 100.00%] [G loss: 14.860996]\n",
      "epoch:44 step:35041 [D loss: 0.000013, acc.: 100.00%] [G loss: 15.391167]\n",
      "epoch:44 step:35042 [D loss: 0.000182, acc.: 100.00%] [G loss: 14.995132]\n",
      "epoch:44 step:35043 [D loss: 0.000685, acc.: 100.00%] [G loss: 15.219753]\n",
      "epoch:44 step:35044 [D loss: 0.001462, acc.: 100.00%] [G loss: 16.014847]\n",
      "epoch:44 step:35045 [D loss: 0.000145, acc.: 100.00%] [G loss: 14.826615]\n",
      "epoch:44 step:35046 [D loss: 0.000004, acc.: 100.00%] [G loss: 14.477224]\n",
      "epoch:44 step:35047 [D loss: 0.000029, acc.: 100.00%] [G loss: 14.242592]\n",
      "epoch:44 step:35048 [D loss: 0.000348, acc.: 100.00%] [G loss: 14.302563]\n",
      "epoch:44 step:35049 [D loss: 0.051838, acc.: 99.22%] [G loss: 13.874206]\n",
      "epoch:44 step:35050 [D loss: 0.000603, acc.: 100.00%] [G loss: 13.499906]\n",
      "epoch:44 step:35051 [D loss: 0.000238, acc.: 100.00%] [G loss: 13.318148]\n",
      "epoch:44 step:35052 [D loss: 0.000320, acc.: 100.00%] [G loss: 13.306150]\n",
      "epoch:44 step:35053 [D loss: 0.000032, acc.: 100.00%] [G loss: 12.261278]\n",
      "epoch:44 step:35054 [D loss: 0.000988, acc.: 100.00%] [G loss: 13.165815]\n",
      "epoch:44 step:35055 [D loss: 0.000318, acc.: 100.00%] [G loss: 13.874767]\n",
      "epoch:44 step:35056 [D loss: 0.004399, acc.: 100.00%] [G loss: 13.835171]\n",
      "epoch:44 step:35057 [D loss: 0.000013, acc.: 100.00%] [G loss: 12.883760]\n",
      "epoch:44 step:35058 [D loss: 0.000746, acc.: 100.00%] [G loss: 12.926939]\n",
      "epoch:44 step:35059 [D loss: 0.000030, acc.: 100.00%] [G loss: 12.796046]\n",
      "epoch:44 step:35060 [D loss: 0.000155, acc.: 100.00%] [G loss: 13.186251]\n",
      "epoch:44 step:35061 [D loss: 0.003529, acc.: 100.00%] [G loss: 13.296827]\n",
      "epoch:44 step:35062 [D loss: 0.000097, acc.: 100.00%] [G loss: 14.222349]\n",
      "epoch:44 step:35063 [D loss: 0.002004, acc.: 100.00%] [G loss: 16.071825]\n",
      "epoch:44 step:35064 [D loss: 0.000098, acc.: 100.00%] [G loss: 11.078592]\n",
      "epoch:44 step:35065 [D loss: 0.000093, acc.: 100.00%] [G loss: 11.123470]\n",
      "epoch:44 step:35066 [D loss: 0.001307, acc.: 100.00%] [G loss: 11.959198]\n",
      "epoch:44 step:35067 [D loss: 0.011276, acc.: 100.00%] [G loss: 12.977202]\n",
      "epoch:44 step:35068 [D loss: 0.000004, acc.: 100.00%] [G loss: 14.590836]\n",
      "epoch:44 step:35069 [D loss: 0.000268, acc.: 100.00%] [G loss: 11.980341]\n",
      "epoch:44 step:35070 [D loss: 0.000401, acc.: 100.00%] [G loss: 10.946272]\n",
      "epoch:44 step:35071 [D loss: 0.000148, acc.: 100.00%] [G loss: 11.118786]\n",
      "epoch:44 step:35072 [D loss: 0.008363, acc.: 100.00%] [G loss: 12.510249]\n",
      "epoch:44 step:35073 [D loss: 0.001906, acc.: 100.00%] [G loss: 13.400146]\n",
      "epoch:44 step:35074 [D loss: 0.000138, acc.: 100.00%] [G loss: 9.363747]\n",
      "epoch:44 step:35075 [D loss: 0.020684, acc.: 98.44%] [G loss: 11.732800]\n",
      "epoch:44 step:35076 [D loss: 0.000009, acc.: 100.00%] [G loss: 14.176163]\n",
      "epoch:44 step:35077 [D loss: 0.257929, acc.: 93.75%] [G loss: 9.181742]\n",
      "epoch:44 step:35078 [D loss: 0.242535, acc.: 89.06%] [G loss: 16.118095]\n",
      "epoch:44 step:35079 [D loss: 0.000035, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:44 step:35080 [D loss: 0.000009, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:44 step:35081 [D loss: 0.000257, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:44 step:35082 [D loss: 0.000623, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:44 step:35083 [D loss: 0.003877, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:44 step:35084 [D loss: 0.004045, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:44 step:35085 [D loss: 0.000003, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:44 step:35086 [D loss: 0.001552, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:44 step:35087 [D loss: 0.000014, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:44 step:35088 [D loss: 0.004727, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:44 step:35089 [D loss: 0.036767, acc.: 97.66%] [G loss: 16.118095]\n",
      "epoch:44 step:35090 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:44 step:35091 [D loss: 0.000034, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:44 step:35092 [D loss: 0.000002, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:44 step:35093 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:44 step:35094 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:44 step:35095 [D loss: 0.000007, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:44 step:35096 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:44 step:35097 [D loss: 0.000589, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:44 step:35098 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:44 step:35099 [D loss: 0.000359, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:44 step:35100 [D loss: 0.004959, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:44 step:35101 [D loss: 0.000002, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:44 step:35102 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:44 step:35103 [D loss: 0.000005, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:44 step:35104 [D loss: 0.000028, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:44 step:35105 [D loss: 0.000005, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:44 step:35106 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:44 step:35107 [D loss: 0.000013, acc.: 100.00%] [G loss: 16.118095]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:44 step:35108 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:44 step:35109 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:44 step:35110 [D loss: 0.000003, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:44 step:35111 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:44 step:35112 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:44 step:35113 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:44 step:35114 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:44 step:35115 [D loss: 0.000005, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:44 step:35116 [D loss: 0.000003, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:44 step:35117 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:44 step:35118 [D loss: 0.000002, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:44 step:35119 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:44 step:35120 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:44 step:35121 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:44 step:35122 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:44 step:35123 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:44 step:35124 [D loss: 0.000030, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:44 step:35125 [D loss: 0.000012, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:44 step:35126 [D loss: 0.000026, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:44 step:35127 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:44 step:35128 [D loss: 0.000002, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:44 step:35129 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:44 step:35130 [D loss: 0.000002, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:44 step:35131 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:44 step:35132 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:44 step:35133 [D loss: 0.000003, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:44 step:35134 [D loss: 0.000032, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:44 step:35135 [D loss: 0.000022, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:44 step:35136 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:44 step:35137 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:44 step:35138 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:44 step:35139 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:44 step:35140 [D loss: 0.000005, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:44 step:35141 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:44 step:35142 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:44 step:35143 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:44 step:35144 [D loss: 0.000021, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:44 step:35145 [D loss: 0.000139, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35146 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35147 [D loss: 0.000004, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35148 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35149 [D loss: 0.000014, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35150 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35151 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35152 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35153 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35154 [D loss: 0.000002, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35155 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35156 [D loss: 0.000002, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35157 [D loss: 0.000021, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35158 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35159 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35160 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35161 [D loss: 0.000017, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35162 [D loss: 0.000007, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35163 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35164 [D loss: 0.000002, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35165 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35166 [D loss: 0.000003, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35167 [D loss: 0.000005, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35168 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35169 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35170 [D loss: 0.000006, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35171 [D loss: 0.000008, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35172 [D loss: 0.000002, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35173 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35174 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35175 [D loss: 0.000101, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35176 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35177 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35178 [D loss: 0.000006, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35179 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35180 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35181 [D loss: 0.000481, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35182 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35183 [D loss: 0.000002, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35184 [D loss: 0.000021, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35185 [D loss: 0.000014, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35186 [D loss: 0.000008, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35187 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35188 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35189 [D loss: 0.000007, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35190 [D loss: 0.000010, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35191 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35192 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35193 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35194 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35195 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35196 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35197 [D loss: 0.000012, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35198 [D loss: 0.000008, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35199 [D loss: 0.000049, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35200 [D loss: 0.000004, acc.: 100.00%] [G loss: 16.118095]\n",
      "##############\n",
      "[2.12070966 0.96115472 1.01228061 0.9900574  0.88587287 0.85677044\n",
      " 2.12051123 0.92521331 2.11344758 1.05260008]\n",
      "##########\n",
      "epoch:45 step:35201 [D loss: 0.000032, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35202 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35203 [D loss: 0.000092, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35204 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35205 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35206 [D loss: 0.000021, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35207 [D loss: 0.000006, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35208 [D loss: 0.000144, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35209 [D loss: 0.000004, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35210 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35211 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35212 [D loss: 0.000002, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35213 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35214 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35215 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35216 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35217 [D loss: 0.000004, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35218 [D loss: 0.000008, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35219 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:45 step:35220 [D loss: 0.000002, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35221 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35222 [D loss: 0.000002, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35223 [D loss: 0.000005, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35224 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35225 [D loss: 0.000002, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35226 [D loss: 0.000003, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35227 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35228 [D loss: 0.000022, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35229 [D loss: 0.000006, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35230 [D loss: 0.000032, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35231 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35232 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35233 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35234 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35235 [D loss: 0.000006, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35236 [D loss: 0.000006, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35237 [D loss: 0.000009, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35238 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35239 [D loss: 0.000023, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35240 [D loss: 0.000007, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35241 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35242 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35243 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35244 [D loss: 0.000049, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35245 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35246 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35247 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35248 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35249 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35250 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35251 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35252 [D loss: 0.000006, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35253 [D loss: 0.000006, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35254 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35255 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35256 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35257 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35258 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35259 [D loss: 0.000003, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35260 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35261 [D loss: 0.000020, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35262 [D loss: 0.000019, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35263 [D loss: 0.000010, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35264 [D loss: 0.000030, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35265 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35266 [D loss: 0.000002, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35267 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35268 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35269 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35270 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35271 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35272 [D loss: 0.000005, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35273 [D loss: 0.000006, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35274 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35275 [D loss: 0.000028, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35276 [D loss: 0.000002, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35277 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35278 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35279 [D loss: 0.000011, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35280 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35281 [D loss: 0.000040, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35282 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35283 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35284 [D loss: 0.000003, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35285 [D loss: 0.000201, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35286 [D loss: 0.000003, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35287 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35288 [D loss: 0.000005, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35289 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35290 [D loss: 0.000007, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35291 [D loss: 0.000005, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35292 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35293 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35294 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35295 [D loss: 0.000014, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35296 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35297 [D loss: 0.000004, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35298 [D loss: 0.000102, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35299 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35300 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35301 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35302 [D loss: 0.000003, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35303 [D loss: 0.000419, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35304 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35305 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35306 [D loss: 0.002883, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35307 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35308 [D loss: 0.000002, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35309 [D loss: 0.000008, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35310 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35311 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35312 [D loss: 0.000008, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35313 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35314 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35315 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35316 [D loss: 0.000045, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35317 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35318 [D loss: 0.000002, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35319 [D loss: 0.000049, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35320 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35321 [D loss: 0.000312, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35322 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35323 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35324 [D loss: 0.000340, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35325 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35326 [D loss: 0.000117, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35327 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35328 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35329 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35330 [D loss: 0.000006, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35331 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:45 step:35332 [D loss: 0.000018, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35333 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35334 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35335 [D loss: 0.000007, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35336 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35337 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35338 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35339 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35340 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35341 [D loss: 0.000006, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35342 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35343 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35344 [D loss: 0.000003, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35345 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35346 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35347 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35348 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35349 [D loss: 0.000050, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35350 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35351 [D loss: 0.000160, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35352 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35353 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35354 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35355 [D loss: 0.000007, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35356 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35357 [D loss: 0.000005, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35358 [D loss: 0.000006, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35359 [D loss: 0.002256, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35360 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35361 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35362 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35363 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35364 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35365 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35366 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35367 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35368 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35369 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35370 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35371 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35372 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35373 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35374 [D loss: 0.000075, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35375 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35376 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35377 [D loss: 0.000004, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35378 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35379 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35380 [D loss: 0.000016, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35381 [D loss: 0.000002, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35382 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35383 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35384 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35385 [D loss: 0.000007, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35386 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35387 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35388 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35389 [D loss: 0.000019, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35390 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35391 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35392 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35393 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35394 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35395 [D loss: 0.000010, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35396 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35397 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35398 [D loss: 0.000002, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35399 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35400 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "##############\n",
      "[2.11908827 0.89593487 0.98811585 0.99379203 0.89161976 2.10310621\n",
      " 2.11580224 0.80986996 2.12749714 1.08336683]\n",
      "##########\n",
      "epoch:45 step:35401 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35402 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35403 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35404 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35405 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35406 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35407 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35408 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35409 [D loss: 0.000406, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35410 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35411 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35412 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35413 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35414 [D loss: 0.000002, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35415 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35416 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35417 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35418 [D loss: 0.000013, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35419 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35420 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35421 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35422 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35423 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35424 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35425 [D loss: 0.000004, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35426 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35427 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35428 [D loss: 0.000018, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35429 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35430 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35431 [D loss: 0.000004, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35432 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35433 [D loss: 0.000003, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35434 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35435 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35436 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35437 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35438 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35439 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35440 [D loss: 0.000003, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35441 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35442 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35443 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:45 step:35444 [D loss: 0.000002, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35445 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35446 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35447 [D loss: 0.000004, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35448 [D loss: 0.000063, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35449 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35450 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35451 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35452 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35453 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35454 [D loss: 0.000014, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35455 [D loss: 0.000104, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35456 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35457 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35458 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35459 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35460 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35461 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35462 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35463 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35464 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35465 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35466 [D loss: 0.000006, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35467 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35468 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35469 [D loss: 0.000034, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35470 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35471 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35472 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35473 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35474 [D loss: 0.000010, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35475 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35476 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35477 [D loss: 0.000074, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35478 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35479 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35480 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35481 [D loss: 0.000013, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35482 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35483 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35484 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35485 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35486 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35487 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35488 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35489 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35490 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35491 [D loss: 0.000020, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35492 [D loss: 0.000002, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35493 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35494 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35495 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35496 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35497 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35498 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35499 [D loss: 0.000004, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35500 [D loss: 0.000017, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35501 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35502 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35503 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35504 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35505 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35506 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35507 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35508 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35509 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35510 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35511 [D loss: 0.000004, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35512 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35513 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35514 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35515 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35516 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35517 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35518 [D loss: 0.000005, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35519 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35520 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35521 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35522 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35523 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35524 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35525 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35526 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35527 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35528 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35529 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35530 [D loss: 0.000023, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35531 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35532 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35533 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35534 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35535 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35536 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35537 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35538 [D loss: 0.000009, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35539 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35540 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35541 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35542 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35543 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35544 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35545 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35546 [D loss: 0.000009, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35547 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35548 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35549 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35550 [D loss: 0.000004, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35551 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35552 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35553 [D loss: 0.000014, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35554 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35555 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:45 step:35556 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35557 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35558 [D loss: 0.000049, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35559 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35560 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35561 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35562 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35563 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35564 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35565 [D loss: 0.000002, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35566 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35567 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35568 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35569 [D loss: 0.000003, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35570 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35571 [D loss: 0.000005, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35572 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35573 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35574 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35575 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35576 [D loss: 0.000249, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35577 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35578 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35579 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35580 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35581 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35582 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35583 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35584 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35585 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35586 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35587 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35588 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35589 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35590 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35591 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35592 [D loss: 0.000003, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35593 [D loss: 0.000002, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35594 [D loss: 0.000013, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35595 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35596 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35597 [D loss: 0.000002, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35598 [D loss: 0.000006, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35599 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35600 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "##############\n",
      "[1.11278719 0.88946162 0.99444513 0.99848416 0.90418461 0.84087661\n",
      " 0.9780957  0.97108366 2.10747514 1.06978213]\n",
      "##########\n",
      "epoch:45 step:35601 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35602 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35603 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35604 [D loss: 0.000009, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35605 [D loss: 0.000002, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35606 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35607 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35608 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35609 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35610 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35611 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35612 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35613 [D loss: 0.000003, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35614 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35615 [D loss: 0.000004, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35616 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35617 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35618 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35619 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35620 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35621 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35622 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35623 [D loss: 0.000002, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35624 [D loss: 0.000005, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35625 [D loss: 0.000002, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35626 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35627 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35628 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35629 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35630 [D loss: 0.000079, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35631 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35632 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35633 [D loss: 0.000005, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35634 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35635 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35636 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35637 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35638 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35639 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35640 [D loss: 0.000002, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35641 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35642 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35643 [D loss: 0.000002, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35644 [D loss: 0.000007, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35645 [D loss: 0.000002, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35646 [D loss: 0.000015, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35647 [D loss: 0.000003, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35648 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35649 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35650 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35651 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35652 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35653 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35654 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35655 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35656 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35657 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35658 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35659 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35660 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35661 [D loss: 0.000002, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35662 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35663 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35664 [D loss: 0.000002, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35665 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35666 [D loss: 0.000002, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35667 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:45 step:35668 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35669 [D loss: 0.000002, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35670 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35671 [D loss: 0.000017, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35672 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35673 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35674 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35675 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35676 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35677 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35678 [D loss: 0.000006, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35679 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35680 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35681 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35682 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35683 [D loss: 0.000006, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35684 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35685 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35686 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35687 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35688 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35689 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35690 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35691 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35692 [D loss: 0.000047, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35693 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35694 [D loss: 0.000002, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35695 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35696 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35697 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35698 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35699 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35700 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35701 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35702 [D loss: 0.000002, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35703 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35704 [D loss: 0.000015, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35705 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35706 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35707 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35708 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35709 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35710 [D loss: 0.000033, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35711 [D loss: 0.000008, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35712 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35713 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35714 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35715 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35716 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35717 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35718 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35719 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35720 [D loss: 0.000002, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35721 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35722 [D loss: 0.000005, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35723 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35724 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35725 [D loss: 0.000007, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35726 [D loss: 0.000003, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35727 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35728 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35729 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35730 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35731 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35732 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35733 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35734 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35735 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35736 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35737 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35738 [D loss: 0.000004, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35739 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35740 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35741 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35742 [D loss: 0.000002, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35743 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35744 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35745 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35746 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35747 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35748 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35749 [D loss: 0.000003, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35750 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35751 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35752 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35753 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35754 [D loss: 0.000024, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35755 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35756 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35757 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35758 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35759 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35760 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35761 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35762 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35763 [D loss: 0.000002, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35764 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35765 [D loss: 0.000002, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35766 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35767 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35768 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35769 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35770 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35771 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35772 [D loss: 0.000008, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35773 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35774 [D loss: 0.000003, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35775 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35776 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35777 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35778 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35779 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:45 step:35780 [D loss: 0.000002, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35781 [D loss: 0.000005, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35782 [D loss: 0.000002, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35783 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35784 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35785 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35786 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35787 [D loss: 0.000002, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35788 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35789 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35790 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35791 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35792 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35793 [D loss: 0.000007, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35794 [D loss: 0.000014, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35795 [D loss: 0.000002, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35796 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35797 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35798 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35799 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35800 [D loss: 0.000002, acc.: 100.00%] [G loss: 16.118095]\n",
      "##############\n",
      "[1.09321513 0.97460808 1.01000228 1.00292465 0.89782964 2.11085218\n",
      " 2.11551914 0.79984929 2.10830408 1.11165511]\n",
      "##########\n",
      "epoch:45 step:35801 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35802 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35803 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35804 [D loss: 0.000002, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35805 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35806 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35807 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35808 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35809 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35810 [D loss: 0.000019, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35811 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35812 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35813 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35814 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35815 [D loss: 0.000005, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35816 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35817 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35818 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35819 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35820 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35821 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35822 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35823 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35824 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35825 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35826 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35827 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35828 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35829 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35830 [D loss: 0.000027, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35831 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35832 [D loss: 0.000072, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35833 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35834 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35835 [D loss: 0.000002, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35836 [D loss: 0.000002, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35837 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35838 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35839 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35840 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35841 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35842 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35843 [D loss: 0.000003, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35844 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35845 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35846 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35847 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35848 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35849 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35850 [D loss: 0.000013, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35851 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35852 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35853 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35854 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35855 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35856 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35857 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35858 [D loss: 0.000156, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35859 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35860 [D loss: 0.000007, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35861 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35862 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35863 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35864 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35865 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35866 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35867 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35868 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35869 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35870 [D loss: 0.000004, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35871 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35872 [D loss: 0.000002, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35873 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35874 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35875 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35876 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35877 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35878 [D loss: 0.000004, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35879 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35880 [D loss: 0.000015, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35881 [D loss: 0.000002, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35882 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35883 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35884 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35885 [D loss: 0.000002, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35886 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35887 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35888 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35889 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35890 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35891 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:45 step:35892 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35893 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35894 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35895 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35896 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35897 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35898 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35899 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35900 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35901 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35902 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35903 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35904 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35905 [D loss: 0.000003, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35906 [D loss: 0.000002, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35907 [D loss: 0.000004, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35908 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35909 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35910 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35911 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35912 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35913 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35914 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35915 [D loss: 0.000002, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35916 [D loss: 0.000006, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35917 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35918 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35919 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35920 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35921 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35922 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35923 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35924 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35925 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:45 step:35926 [D loss: 0.000012, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:35927 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:35928 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:35929 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:35930 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:35931 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:35932 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:35933 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:35934 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:35935 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:35936 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:35937 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:35938 [D loss: 0.000004, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:35939 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:35940 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:35941 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:35942 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:35943 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:35944 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:35945 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:35946 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:35947 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:35948 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:35949 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:35950 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:35951 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:35952 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:35953 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:35954 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:35955 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:35956 [D loss: 0.000014, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:35957 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:35958 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:35959 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:35960 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:35961 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:35962 [D loss: 0.000031, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:35963 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:35964 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:35965 [D loss: 0.000002, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:35966 [D loss: 0.000003, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:35967 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:35968 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:35969 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:35970 [D loss: 0.000002, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:35971 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:35972 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:35973 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:35974 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:35975 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:35976 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:35977 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:35978 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:35979 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:35980 [D loss: 0.000006, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:35981 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:35982 [D loss: 0.000002, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:35983 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:35984 [D loss: 0.000016, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:35985 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:35986 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:35987 [D loss: 0.000002, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:35988 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:35989 [D loss: 0.000012, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:35990 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:35991 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:35992 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:35993 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:35994 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:35995 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:35996 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:35997 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:35998 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:35999 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36000 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "##############\n",
      "[1.10358576 0.84362024 0.99789854 1.01998068 0.83427323 0.87385062\n",
      " 2.11199063 0.91320811 2.10469419 1.00929666]\n",
      "##########\n",
      "epoch:46 step:36001 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36002 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36003 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:46 step:36004 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36005 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36006 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36007 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36008 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36009 [D loss: 0.000004, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36010 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36011 [D loss: 0.000010, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36012 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36013 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36014 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36015 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36016 [D loss: 0.000002, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36017 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36018 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36019 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36020 [D loss: 0.000002, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36021 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36022 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36023 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36024 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36025 [D loss: 0.000003, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36026 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36027 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36028 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36029 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36030 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36031 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36032 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36033 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36034 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36035 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36036 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36037 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36038 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36039 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36040 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36041 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36042 [D loss: 0.000004, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36043 [D loss: 0.000003, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36044 [D loss: 0.000002, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36045 [D loss: 0.000002, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36046 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36047 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36048 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36049 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36050 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36051 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36052 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36053 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36054 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36055 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36056 [D loss: 0.000005, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36057 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36058 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36059 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36060 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36061 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36062 [D loss: 0.000006, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36063 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36064 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36065 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36066 [D loss: 0.000013, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36067 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36068 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36069 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36070 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36071 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36072 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36073 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36074 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36075 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36076 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36077 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36078 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36079 [D loss: 0.000013, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36080 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36081 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36082 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36083 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36084 [D loss: 0.000031, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36085 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36086 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36087 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36088 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36089 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36090 [D loss: 0.000002, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36091 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36092 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36093 [D loss: 0.000002, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36094 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36095 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36096 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36097 [D loss: 0.000017, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36098 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36099 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36100 [D loss: 0.000007, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36101 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36102 [D loss: 0.000056, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36103 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36104 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36105 [D loss: 0.000061, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36106 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36107 [D loss: 0.000031, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36108 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36109 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36110 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36111 [D loss: 0.000002, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36112 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36113 [D loss: 0.000005, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36114 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36115 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:46 step:36116 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36117 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36118 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36119 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36120 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36121 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36122 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36123 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36124 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36125 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36126 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36127 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36128 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36129 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36130 [D loss: 0.000009, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36131 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36132 [D loss: 0.000037, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36133 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36134 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36135 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36136 [D loss: 0.000002, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36137 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36138 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36139 [D loss: 0.000002, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36140 [D loss: 0.000109, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36141 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36142 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36143 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36144 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36145 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36146 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36147 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36148 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36149 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36150 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36151 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36152 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36153 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36154 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36155 [D loss: 0.000024, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36156 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36157 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36158 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36159 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36160 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36161 [D loss: 0.000008, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36162 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36163 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36164 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36165 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36166 [D loss: 0.000003, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36167 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36168 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36169 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36170 [D loss: 0.000007, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36171 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36172 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36173 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36174 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36175 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36176 [D loss: 0.000004, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36177 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36178 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36179 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36180 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36181 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36182 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36183 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36184 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36185 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36186 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36187 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36188 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36189 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36190 [D loss: 0.000102, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36191 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36192 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36193 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36194 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36195 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36196 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36197 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36198 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36199 [D loss: 0.000007, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36200 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "##############\n",
      "[1.09360126 0.85908043 1.01544589 1.01988527 0.88184019 0.89114708\n",
      " 2.12933062 2.11430339 2.12768093 1.11172502]\n",
      "##########\n",
      "epoch:46 step:36201 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36202 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36203 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36204 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36205 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36206 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36207 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36208 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36209 [D loss: 0.000007, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36210 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36211 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36212 [D loss: 0.000002, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36213 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36214 [D loss: 0.000002, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36215 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36216 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36217 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36218 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36219 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36220 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36221 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36222 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36223 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36224 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36225 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36226 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36227 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:46 step:36228 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36229 [D loss: 0.000030, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36230 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36231 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36232 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36233 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36234 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36235 [D loss: 0.000006, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36236 [D loss: 0.000054, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36237 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36238 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36239 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36240 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36241 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36242 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36243 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36244 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36245 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36246 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36247 [D loss: 0.000002, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36248 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36249 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36250 [D loss: 0.000012, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36251 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36252 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36253 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36254 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36255 [D loss: 0.000004, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36256 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36257 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36258 [D loss: 0.000026, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36259 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36260 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36261 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36262 [D loss: 0.000006, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36263 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36264 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36265 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36266 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36267 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36268 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36269 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36270 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36271 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36272 [D loss: 0.000009, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36273 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36274 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36275 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36276 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36277 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36278 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36279 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36280 [D loss: 0.000002, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36281 [D loss: 0.000006, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36282 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36283 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36284 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36285 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36286 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36287 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36288 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36289 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36290 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36291 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36292 [D loss: 0.000002, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36293 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36294 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36295 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36296 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36297 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36298 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36299 [D loss: 0.000003, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36300 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36301 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36302 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36303 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36304 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36305 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36306 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36307 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36308 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36309 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36310 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36311 [D loss: 0.000009, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36312 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36313 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36314 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36315 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36316 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36317 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36318 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36319 [D loss: 0.000004, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36320 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36321 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36322 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36323 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36324 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36325 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36326 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36327 [D loss: 0.000004, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36328 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36329 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36330 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36331 [D loss: 0.000002, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36332 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36333 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36334 [D loss: 0.000005, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36335 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36336 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36337 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36338 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36339 [D loss: 0.000015, acc.: 100.00%] [G loss: 16.118095]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:46 step:36340 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36341 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36342 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36343 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36344 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36345 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36346 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36347 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36348 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36349 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36350 [D loss: 0.000002, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36351 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36352 [D loss: 0.000003, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36353 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36354 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36355 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36356 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36357 [D loss: 0.000080, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36358 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36359 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36360 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36361 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36362 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36363 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36364 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36365 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36366 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36367 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36368 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36369 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36370 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36371 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36372 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36373 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36374 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36375 [D loss: 0.000006, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36376 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36377 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36378 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36379 [D loss: 0.000003, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36380 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36381 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36382 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36383 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36384 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36385 [D loss: 0.000004, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36386 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36387 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36388 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36389 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36390 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36391 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36392 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36393 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36394 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36395 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36396 [D loss: 0.000002, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36397 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36398 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36399 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36400 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "##############\n",
      "[1.11560626 0.93072357 0.99012798 1.02595429 0.83525181 2.1076628\n",
      " 2.12059679 2.1140228  2.11238256 0.86719943]\n",
      "##########\n",
      "epoch:46 step:36401 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36402 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36403 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36404 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36405 [D loss: 0.000002, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36406 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36407 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36408 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36409 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36410 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36411 [D loss: 0.000030, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36412 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36413 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36414 [D loss: 0.000002, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36415 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36416 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36417 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36418 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36419 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36420 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36421 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36422 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36423 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36424 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36425 [D loss: 0.000003, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36426 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36427 [D loss: 0.000007, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36428 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36429 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36430 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36431 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36432 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36433 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36434 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36435 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36436 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36437 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36438 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36439 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36440 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36441 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36442 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36443 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36444 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36445 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36446 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36447 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36448 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36449 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36450 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36451 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:46 step:36452 [D loss: 0.000007, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36453 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36454 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36455 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36456 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36457 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36458 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36459 [D loss: 0.000003, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36460 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36461 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36462 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36463 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36464 [D loss: 0.000003, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36465 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36466 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36467 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36468 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36469 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36470 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36471 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36472 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36473 [D loss: 0.000017, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36474 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36475 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36476 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36477 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36478 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36479 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36480 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36481 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36482 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36483 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36484 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36485 [D loss: 0.000006, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36486 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36487 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36488 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36489 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36490 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36491 [D loss: 0.000014, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36492 [D loss: 0.000004, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36493 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36494 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36495 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36496 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36497 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36498 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36499 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36500 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36501 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36502 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36503 [D loss: 0.000003, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36504 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36505 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36506 [D loss: 0.000003, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36507 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36508 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36509 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36510 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36511 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36512 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36513 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36514 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36515 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36516 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36517 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36518 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36519 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36520 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36521 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36522 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36523 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36524 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36525 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36526 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36527 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36528 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36529 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36530 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36531 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36532 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36533 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36534 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36535 [D loss: 0.000009, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36536 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36537 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36538 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36539 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36540 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36541 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36542 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36543 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36544 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36545 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36546 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36547 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36548 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36549 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36550 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36551 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36552 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36553 [D loss: 0.000003, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36554 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36555 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36556 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36557 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36558 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36559 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36560 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36561 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36562 [D loss: 0.000002, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36563 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:46 step:36564 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36565 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36566 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36567 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36568 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36569 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36570 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36571 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36572 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36573 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36574 [D loss: 0.000003, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36575 [D loss: 0.000009, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36576 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36577 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36578 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36579 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36580 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36581 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36582 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36583 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36584 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36585 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36586 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36587 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36588 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36589 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36590 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36591 [D loss: 0.000009, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36592 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36593 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36594 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36595 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36596 [D loss: 0.000003, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36597 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36598 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36599 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36600 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "##############\n",
      "[2.10562871 0.8562553  1.00242681 1.03567719 0.89878795 1.11069104\n",
      " 2.10555205 2.11524679 2.11754033 0.84001322]\n",
      "##########\n",
      "epoch:46 step:36601 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36602 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36603 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36604 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36605 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36606 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36607 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36608 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36609 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36610 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36611 [D loss: 0.000011, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36612 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36613 [D loss: 0.000027, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36614 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36615 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36616 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36617 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36618 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36619 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36620 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36621 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36622 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36623 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36624 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36625 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36626 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36627 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36628 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36629 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36630 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36631 [D loss: 0.000006, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36632 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36633 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36634 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36635 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36636 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36637 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36638 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36639 [D loss: 0.000053, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36640 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36641 [D loss: 0.000003, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36642 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36643 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36644 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36645 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36646 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36647 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36648 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36649 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36650 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36651 [D loss: 0.000002, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36652 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36653 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36654 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36655 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36656 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36657 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36658 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36659 [D loss: 0.000002, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36660 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36661 [D loss: 0.000007, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36662 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36663 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36664 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36665 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36666 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36667 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36668 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36669 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36670 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36671 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36672 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36673 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36674 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36675 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:46 step:36676 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36677 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36678 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36679 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36680 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36681 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36682 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36683 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36684 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36685 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36686 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36687 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36688 [D loss: 0.000002, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36689 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36690 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36691 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36692 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36693 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36694 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36695 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36696 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36697 [D loss: 0.000004, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36698 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36699 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36700 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36701 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36702 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36703 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36704 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36705 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36706 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:46 step:36707 [D loss: 0.000005, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:36708 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:36709 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:36710 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:36711 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:36712 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:36713 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:36714 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:36715 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:36716 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:36717 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:36718 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:36719 [D loss: 0.000002, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:36720 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:36721 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:36722 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:36723 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:36724 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:36725 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:36726 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:36727 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:36728 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:36729 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:36730 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:36731 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:36732 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:36733 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:36734 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:36735 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:36736 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:36737 [D loss: 0.000007, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:36738 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:36739 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:36740 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:36741 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:36742 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:36743 [D loss: 0.000015, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:36744 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:36745 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:36746 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:36747 [D loss: 0.000002, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:36748 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:36749 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:36750 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:36751 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:36752 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:36753 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:36754 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:36755 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:36756 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:36757 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:36758 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:36759 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:36760 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:36761 [D loss: 0.000004, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:36762 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:36763 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:36764 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:36765 [D loss: 0.000008, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:36766 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:36767 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:36768 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:36769 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:36770 [D loss: 0.000006, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:36771 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:36772 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:36773 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:36774 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:36775 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:36776 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:36777 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:36778 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:36779 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:36780 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:36781 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:36782 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:36783 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:36784 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:36785 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:36786 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:36787 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:47 step:36788 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:36789 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:36790 [D loss: 0.000002, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:36791 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:36792 [D loss: 0.000006, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:36793 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:36794 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:36795 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:36796 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:36797 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:36798 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:36799 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:36800 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "##############\n",
      "[2.11928563 0.88928848 0.97916093 0.98967644 0.85829771 0.88105467\n",
      " 2.09456352 0.99310949 2.1005932  0.95205135]\n",
      "##########\n",
      "epoch:47 step:36801 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:36802 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:36803 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:36804 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:36805 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:36806 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:36807 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:36808 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:36809 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:36810 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:36811 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:36812 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:36813 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:36814 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:36815 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:36816 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:36817 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:36818 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:36819 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:36820 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:36821 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:36822 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:36823 [D loss: 0.000003, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:36824 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:36825 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:36826 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:36827 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:36828 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:36829 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:36830 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:36831 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:36832 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:36833 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:36834 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:36835 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:36836 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:36837 [D loss: 0.000003, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:36838 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:36839 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:36840 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:36841 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:36842 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:36843 [D loss: 0.000003, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:36844 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:36845 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:36846 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:36847 [D loss: 0.000006, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:36848 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:36849 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:36850 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:36851 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:36852 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:36853 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:36854 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:36855 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:36856 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:36857 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:36858 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:36859 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:36860 [D loss: 0.000006, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:36861 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:36862 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:36863 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:36864 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:36865 [D loss: 0.000012, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:36866 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:36867 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:36868 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:36869 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:36870 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:36871 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:36872 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:36873 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:36874 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:36875 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:36876 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:36877 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:36878 [D loss: 0.000009, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:36879 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:36880 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:36881 [D loss: 0.000003, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:36882 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:36883 [D loss: 0.000024, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:36884 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:36885 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:36886 [D loss: 0.000025, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:36887 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:36888 [D loss: 0.000014, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:36889 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:36890 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:36891 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:36892 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:36893 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:36894 [D loss: 0.000002, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:36895 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:36896 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:36897 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:36898 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:36899 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:36900 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:47 step:36901 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:36902 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:36903 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:36904 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:36905 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:36906 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:36907 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:36908 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:36909 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:36910 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:36911 [D loss: 0.000003, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:36912 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:36913 [D loss: 0.000015, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:36914 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:36915 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:36916 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:36917 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:36918 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:36919 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:36920 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:36921 [D loss: 0.000044, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:36922 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:36923 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:36924 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:36925 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:36926 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:36927 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:36928 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:36929 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:36930 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:36931 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:36932 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:36933 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:36934 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:36935 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:36936 [D loss: 0.000010, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:36937 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:36938 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:36939 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:36940 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:36941 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:36942 [D loss: 0.000004, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:36943 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:36944 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:36945 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:36946 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:36947 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:36948 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:36949 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:36950 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:36951 [D loss: 0.000003, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:36952 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:36953 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:36954 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:36955 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:36956 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:36957 [D loss: 0.000002, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:36958 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:36959 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:36960 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:36961 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:36962 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:36963 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:36964 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:36965 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:36966 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:36967 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:36968 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:36969 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:36970 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:36971 [D loss: 0.000040, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:36972 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:36973 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:36974 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:36975 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:36976 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:36977 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:36978 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:36979 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:36980 [D loss: 0.000004, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:36981 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:36982 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:36983 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:36984 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:36985 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:36986 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:36987 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:36988 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:36989 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:36990 [D loss: 0.000003, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:36991 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:36992 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:36993 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:36994 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:36995 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:36996 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:36997 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:36998 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:36999 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37000 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "##############\n",
      "[1.10919326 0.91027064 0.96790088 0.99507481 0.89176995 1.106356\n",
      " 2.11420189 1.06578339 2.11452071 1.10546371]\n",
      "##########\n",
      "epoch:47 step:37001 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37002 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37003 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37004 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37005 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37006 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37007 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37008 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37009 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37010 [D loss: 0.000016, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37011 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:47 step:37012 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37013 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37014 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37015 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37016 [D loss: 0.000003, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37017 [D loss: 0.000029, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37018 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37019 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37020 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37021 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37022 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37023 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37024 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37025 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37026 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37027 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37028 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37029 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37030 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37031 [D loss: 0.000005, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37032 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37033 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37034 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37035 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37036 [D loss: 0.000002, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37037 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37038 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37039 [D loss: 0.000011, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37040 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37041 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37042 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37043 [D loss: 0.000003, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37044 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37045 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37046 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37047 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37048 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37049 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37050 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37051 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37052 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37053 [D loss: 0.000004, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37054 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37055 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37056 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37057 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37058 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37059 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37060 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37061 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37062 [D loss: 0.000002, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37063 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37064 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37065 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37066 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37067 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37068 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37069 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37070 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37071 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37072 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37073 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37074 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37075 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37076 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37077 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37078 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37079 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37080 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37081 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37082 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37083 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37084 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37085 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37086 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37087 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37088 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37089 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37090 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37091 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37092 [D loss: 0.000004, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37093 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37094 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37095 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37096 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37097 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37098 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37099 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37100 [D loss: 0.000002, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37101 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37102 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37103 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37104 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37105 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37106 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37107 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37108 [D loss: 0.000002, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37109 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37110 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37111 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37112 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37113 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37114 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37115 [D loss: 0.000002, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37116 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37117 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37118 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37119 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37120 [D loss: 0.000005, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37121 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37122 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37123 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:47 step:37124 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37125 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37126 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37127 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37128 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37129 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37130 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37131 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37132 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37133 [D loss: 0.000002, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37134 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37135 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37136 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37137 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37138 [D loss: 0.000030, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37139 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37140 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37141 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37142 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37143 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37144 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37145 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37146 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37147 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37148 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37149 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37150 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37151 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37152 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37153 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37154 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37155 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37156 [D loss: 0.000003, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37157 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37158 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37159 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37160 [D loss: 0.000002, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37161 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37162 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37163 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37164 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37165 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37166 [D loss: 0.000002, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37167 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37168 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37169 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37170 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37171 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37172 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37173 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37174 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37175 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37176 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37177 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37178 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37179 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37180 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37181 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37182 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37183 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37184 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37185 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37186 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37187 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37188 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37189 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37190 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37191 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37192 [D loss: 0.000013, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37193 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37194 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37195 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37196 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37197 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37198 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37199 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37200 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "##############\n",
      "[2.10977208 0.87853267 1.02000655 0.99768745 0.78312773 0.77698572\n",
      " 2.12530008 1.10782444 2.11151684 1.11691738]\n",
      "##########\n",
      "epoch:47 step:37201 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37202 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37203 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37204 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37205 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37206 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37207 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37208 [D loss: 0.000003, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37209 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37210 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37211 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37212 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37213 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37214 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37215 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37216 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37217 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37218 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37219 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37220 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37221 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37222 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37223 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37224 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37225 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37226 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37227 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37228 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37229 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37230 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37231 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37232 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37233 [D loss: 0.000004, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37234 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37235 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:47 step:37236 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37237 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37238 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37239 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37240 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37241 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37242 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37243 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37244 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37245 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37246 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37247 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37248 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37249 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37250 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37251 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37252 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37253 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37254 [D loss: 0.000007, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37255 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37256 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37257 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37258 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37259 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37260 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37261 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37262 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37263 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37264 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37265 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37266 [D loss: 0.000003, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37267 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37268 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37269 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37270 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37271 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37272 [D loss: 0.000007, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37273 [D loss: 0.000002, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37274 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37275 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37276 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37277 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37278 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37279 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37280 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37281 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37282 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37283 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37284 [D loss: 0.000002, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37285 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37286 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37287 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37288 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37289 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37290 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37291 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37292 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37293 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37294 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37295 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37296 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37297 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37298 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37299 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37300 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37301 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37302 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37303 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37304 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37305 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37306 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37307 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37308 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37309 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37310 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37311 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37312 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37313 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37314 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37315 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37316 [D loss: 0.000004, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37317 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37318 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37319 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37320 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37321 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37322 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37323 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37324 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37325 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37326 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37327 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37328 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37329 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37330 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37331 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37332 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37333 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37334 [D loss: 0.000002, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37335 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37336 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37337 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37338 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37339 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37340 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37341 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37342 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37343 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37344 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37345 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37346 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37347 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:47 step:37348 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37349 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37350 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37351 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37352 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37353 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37354 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37355 [D loss: 0.000002, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37356 [D loss: 0.000006, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37357 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37358 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37359 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37360 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37361 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37362 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37363 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37364 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37365 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37366 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37367 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37368 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37369 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37370 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37371 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37372 [D loss: 0.000005, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37373 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37374 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37375 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37376 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37377 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37378 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37379 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37380 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37381 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37382 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37383 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37384 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37385 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37386 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37387 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37388 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37389 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37390 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37391 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37392 [D loss: 0.000005, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37393 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37394 [D loss: 0.000011, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37395 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37396 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37397 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37398 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37399 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37400 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "##############\n",
      "[2.09558588 0.93975409 0.98716507 1.01930793 0.74417016 1.11329437\n",
      " 2.12520208 1.05145613 2.10888635 1.10956924]\n",
      "##########\n",
      "epoch:47 step:37401 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37402 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37403 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37404 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37405 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37406 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37407 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37408 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37409 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37410 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37411 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37412 [D loss: 0.000003, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37413 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37414 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37415 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37416 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37417 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37418 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37419 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37420 [D loss: 0.000023, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37421 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37422 [D loss: 0.000002, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37423 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37424 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37425 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37426 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37427 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37428 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37429 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37430 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37431 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37432 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37433 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37434 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37435 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37436 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37437 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37438 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37439 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37440 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37441 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37442 [D loss: 0.000004, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37443 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37444 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37445 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37446 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37447 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37448 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37449 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37450 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37451 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37452 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37453 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37454 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37455 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37456 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:47 step:37457 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37458 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37459 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37460 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37461 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37462 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37463 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37464 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37465 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37466 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37467 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37468 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37469 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37470 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37471 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37472 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37473 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37474 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37475 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37476 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37477 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37478 [D loss: 0.000002, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37479 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37480 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37481 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37482 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37483 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37484 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37485 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37486 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37487 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:47 step:37488 [D loss: 0.000002, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37489 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37490 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37491 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37492 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37493 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37494 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37495 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37496 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37497 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37498 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37499 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37500 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37501 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37502 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37503 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37504 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37505 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37506 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37507 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37508 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37509 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37510 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37511 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37512 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37513 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37514 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37515 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37516 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37517 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37518 [D loss: 0.000004, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37519 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37520 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37521 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37522 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37523 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37524 [D loss: 0.000008, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37525 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37526 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37527 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37528 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37529 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37530 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37531 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37532 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37533 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37534 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37535 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37536 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37537 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37538 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37539 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37540 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37541 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37542 [D loss: 0.000002, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37543 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37544 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37545 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37546 [D loss: 0.000004, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37547 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37548 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37549 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37550 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37551 [D loss: 0.000003, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37552 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37553 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37554 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37555 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37556 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37557 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37558 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37559 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37560 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37561 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37562 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37563 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37564 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37565 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37566 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37567 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37568 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:48 step:37569 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37570 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37571 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37572 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37573 [D loss: 0.000003, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37574 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37575 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37576 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37577 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37578 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37579 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37580 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37581 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37582 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37583 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37584 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37585 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37586 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37587 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37588 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37589 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37590 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37591 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37592 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37593 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37594 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37595 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37596 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37597 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37598 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37599 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37600 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "##############\n",
      "[1.1109502  0.89094747 0.96960331 1.01053363 0.92408225 0.77642326\n",
      " 2.11005365 2.11554127 2.11768612 2.11733085]\n",
      "##########\n",
      "epoch:48 step:37601 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37602 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37603 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37604 [D loss: 0.000002, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37605 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37606 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37607 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37608 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37609 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37610 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37611 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37612 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37613 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37614 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37615 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37616 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37617 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37618 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37619 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37620 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37621 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37622 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37623 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37624 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37625 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37626 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37627 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37628 [D loss: 0.000003, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37629 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37630 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37631 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37632 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37633 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37634 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37635 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37636 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37637 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37638 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37639 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37640 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37641 [D loss: 0.000003, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37642 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37643 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37644 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37645 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37646 [D loss: 0.000005, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37647 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37648 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37649 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37650 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37651 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37652 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37653 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37654 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37655 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37656 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37657 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37658 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37659 [D loss: 0.000005, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37660 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37661 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37662 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37663 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37664 [D loss: 0.000011, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37665 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37666 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37667 [D loss: 0.000011, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37668 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37669 [D loss: 0.000007, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37670 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37671 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37672 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37673 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37674 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37675 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37676 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37677 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37678 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37679 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37680 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:48 step:37681 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37682 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37683 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37684 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37685 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37686 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37687 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37688 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37689 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37690 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37691 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37692 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37693 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37694 [D loss: 0.000007, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37695 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37696 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37697 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37698 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37699 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37700 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37701 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37702 [D loss: 0.000019, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37703 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37704 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37705 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37706 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37707 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37708 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37709 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37710 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37711 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37712 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37713 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37714 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37715 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37716 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37717 [D loss: 0.000004, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37718 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37719 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37720 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37721 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37722 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37723 [D loss: 0.000002, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37724 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37725 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37726 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37727 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37728 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37729 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37730 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37731 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37732 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37733 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37734 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37735 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37736 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37737 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37738 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37739 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37740 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37741 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37742 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37743 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37744 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37745 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37746 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37747 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37748 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37749 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37750 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37751 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37752 [D loss: 0.000017, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37753 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37754 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37755 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37756 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37757 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37758 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37759 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37760 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37761 [D loss: 0.000002, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37762 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37763 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37764 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37765 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37766 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37767 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37768 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37769 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37770 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37771 [D loss: 0.000002, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37772 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37773 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37774 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37775 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37776 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37777 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37778 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37779 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37780 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37781 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37782 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37783 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37784 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37785 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37786 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37787 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37788 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37789 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37790 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37791 [D loss: 0.000008, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37792 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:48 step:37793 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37794 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37795 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37796 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37797 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37798 [D loss: 0.000014, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37799 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37800 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "##############\n",
      "[1.11977784 0.90943856 1.01313215 1.00555922 0.85024831 2.11397096\n",
      " 2.10359589 1.10403081 2.1172484  2.11738999]\n",
      "##########\n",
      "epoch:48 step:37801 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37802 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37803 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37804 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37805 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37806 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37807 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37808 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37809 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37810 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37811 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37812 [D loss: 0.000002, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37813 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37814 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37815 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37816 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37817 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37818 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37819 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37820 [D loss: 0.000005, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37821 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37822 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37823 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37824 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37825 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37826 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37827 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37828 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37829 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37830 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37831 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37832 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37833 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37834 [D loss: 0.000002, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37835 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37836 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37837 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37838 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37839 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37840 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37841 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37842 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37843 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37844 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37845 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37846 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37847 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37848 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37849 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37850 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37851 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37852 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37853 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37854 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37855 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37856 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37857 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37858 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37859 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37860 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37861 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37862 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37863 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37864 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37865 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37866 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37867 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37868 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37869 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37870 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37871 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37872 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37873 [D loss: 0.000002, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37874 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37875 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37876 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37877 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37878 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37879 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37880 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37881 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37882 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37883 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37884 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37885 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37886 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37887 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37888 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37889 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37890 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37891 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37892 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37893 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37894 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37895 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37896 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37897 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37898 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37899 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37900 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37901 [D loss: 0.000002, acc.: 100.00%] [G loss: 16.118095]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:48 step:37902 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37903 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37904 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37905 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37906 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37907 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37908 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37909 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37910 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37911 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37912 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37913 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37914 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37915 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37916 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37917 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37918 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37919 [D loss: 0.000012, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37920 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37921 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37922 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37923 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37924 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37925 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37926 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37927 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37928 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37929 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37930 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37931 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37932 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37933 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37934 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37935 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37936 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37937 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37938 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37939 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37940 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37941 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37942 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37943 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37944 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37945 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37946 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37947 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37948 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37949 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37950 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37951 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37952 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37953 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37954 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37955 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37956 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37957 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37958 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37959 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37960 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37961 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37962 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37963 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37964 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37965 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37966 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37967 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37968 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37969 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37970 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37971 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37972 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37973 [D loss: 0.000005, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37974 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37975 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37976 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37977 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37978 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37979 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37980 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37981 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37982 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37983 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37984 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37985 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37986 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37987 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37988 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37989 [D loss: 0.000002, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37990 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37991 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37992 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37993 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37994 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37995 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37996 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37997 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37998 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:37999 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:38000 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "##############\n",
      "[1.10823401 0.80364217 1.00970386 1.044056   0.78917483 0.74160679\n",
      " 2.10539326 0.69878472 2.10849353 1.02407425]\n",
      "##########\n",
      "epoch:48 step:38001 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:38002 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:38003 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:38004 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:38005 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:38006 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:38007 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:38008 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:38009 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:38010 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:38011 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:48 step:38012 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:38013 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:38014 [D loss: 0.000002, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:38015 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:38016 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:38017 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:38018 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:38019 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:38020 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:38021 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:38022 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:38023 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:38024 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:38025 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:38026 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:38027 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:38028 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:38029 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:38030 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:38031 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:38032 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:38033 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:38034 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:38035 [D loss: 0.000003, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:38036 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:38037 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:38038 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:38039 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:38040 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:38041 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:38042 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:38043 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:38044 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:38045 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:38046 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:38047 [D loss: 0.000002, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:38048 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:38049 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:38050 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:38051 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:38052 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:38053 [D loss: 0.000003, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:38054 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:38055 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:38056 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:38057 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:38058 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:38059 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:38060 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:38061 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:38062 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:38063 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:38064 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:38065 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:38066 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:38067 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:38068 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:38069 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:38070 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:38071 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:38072 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:38073 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:38074 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:38075 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:38076 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:38077 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:38078 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:38079 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:38080 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:38081 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:38082 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:38083 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:38084 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:38085 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:38086 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:38087 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:38088 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:38089 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:38090 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:38091 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:38092 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:38093 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:38094 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:38095 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:38096 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:38097 [D loss: 0.000002, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:38098 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:38099 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:38100 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:38101 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:38102 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:38103 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:38104 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:38105 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:38106 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:38107 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:38108 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:38109 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:38110 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:38111 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:38112 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:38113 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:38114 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:38115 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:38116 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:38117 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:38118 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:38119 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:38120 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:38121 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:38122 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:38123 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:48 step:38124 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:38125 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:38126 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:38127 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:38128 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:38129 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:38130 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:38131 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:38132 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:38133 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:38134 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:38135 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:38136 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:38137 [D loss: 0.000004, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:38138 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:38139 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:38140 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:38141 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:38142 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:38143 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:38144 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:38145 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:38146 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:38147 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:38148 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:38149 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:38150 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:38151 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:38152 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:38153 [D loss: 0.000002, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:38154 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:38155 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:38156 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:38157 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:38158 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:38159 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:38160 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:38161 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:38162 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:38163 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:38164 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:38165 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:38166 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:38167 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:38168 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:38169 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:38170 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:38171 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:38172 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:38173 [D loss: 0.000002, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:38174 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:38175 [D loss: 0.000005, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:38176 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:38177 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:38178 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:38179 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:38180 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:38181 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:38182 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:38183 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:38184 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:38185 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:38186 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:38187 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:38188 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:38189 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:38190 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:38191 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:38192 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:38193 [D loss: 0.000002, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:38194 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:38195 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:38196 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:38197 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:38198 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:38199 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:38200 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "##############\n",
      "[1.1173224  0.91022517 1.02478325 0.98540428 0.80588331 0.70993531\n",
      " 2.09906447 0.82433829 2.12066735 1.08928968]\n",
      "##########\n",
      "epoch:48 step:38201 [D loss: 0.000010, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:38202 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:38203 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:38204 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:38205 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:38206 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:38207 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:38208 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:38209 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:38210 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:38211 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:38212 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:38213 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:38214 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:38215 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:38216 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:38217 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:38218 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:38219 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:38220 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:38221 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:38222 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:38223 [D loss: 0.000002, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:38224 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:38225 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:38226 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:38227 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:38228 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:38229 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:38230 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:38231 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:38232 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:38233 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:38234 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:38235 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:48 step:38236 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:38237 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:38238 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:38239 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:38240 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:38241 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:38242 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:38243 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:38244 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:38245 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:38246 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:38247 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:38248 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:38249 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:38250 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:38251 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:38252 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:38253 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:38254 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:38255 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:38256 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:38257 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:38258 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:38259 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:38260 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:38261 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:38262 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:38263 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:38264 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:38265 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:38266 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:38267 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:38268 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:48 step:38269 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38270 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38271 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38272 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38273 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38274 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38275 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38276 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38277 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38278 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38279 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38280 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38281 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38282 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38283 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38284 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38285 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38286 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38287 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38288 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38289 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38290 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38291 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38292 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38293 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38294 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38295 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38296 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38297 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38298 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38299 [D loss: 0.000002, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38300 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38301 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38302 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38303 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38304 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38305 [D loss: 0.000004, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38306 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38307 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38308 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38309 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38310 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38311 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38312 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38313 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38314 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38315 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38316 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38317 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38318 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38319 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38320 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38321 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38322 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38323 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38324 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38325 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38326 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38327 [D loss: 0.000002, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38328 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38329 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38330 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38331 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38332 [D loss: 0.000002, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38333 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38334 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38335 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38336 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38337 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38338 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38339 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38340 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38341 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38342 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38343 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38344 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38345 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38346 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38347 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:49 step:38348 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38349 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38350 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38351 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38352 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38353 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38354 [D loss: 0.000002, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38355 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38356 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38357 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38358 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38359 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38360 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38361 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38362 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38363 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38364 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38365 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38366 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38367 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38368 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38369 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38370 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38371 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38372 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38373 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38374 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38375 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38376 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38377 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38378 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38379 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38380 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38381 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38382 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38383 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38384 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38385 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38386 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38387 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38388 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38389 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38390 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38391 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38392 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38393 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38394 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38395 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38396 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38397 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38398 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38399 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38400 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "##############\n",
      "[0.8569736  0.90022891 1.03636186 1.00512812 0.78640417 0.82948981\n",
      " 2.10021774 1.10863088 2.10515664 1.09576973]\n",
      "##########\n",
      "epoch:49 step:38401 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38402 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38403 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38404 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38405 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38406 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38407 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38408 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38409 [D loss: 0.000002, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38410 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38411 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38412 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38413 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38414 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38415 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38416 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38417 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38418 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38419 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38420 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38421 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38422 [D loss: 0.000002, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38423 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38424 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38425 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38426 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38427 [D loss: 0.000002, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38428 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38429 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38430 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38431 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38432 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38433 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38434 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38435 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38436 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38437 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38438 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38439 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38440 [D loss: 0.000002, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38441 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38442 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38443 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38444 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38445 [D loss: 0.000005, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38446 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38447 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38448 [D loss: 0.000005, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38449 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38450 [D loss: 0.000003, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38451 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38452 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38453 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38454 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38455 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38456 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38457 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38458 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38459 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:49 step:38460 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38461 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38462 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38463 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38464 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38465 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38466 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38467 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38468 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38469 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38470 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38471 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38472 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38473 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38474 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38475 [D loss: 0.000003, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38476 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38477 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38478 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38479 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38480 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38481 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38482 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38483 [D loss: 0.000009, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38484 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38485 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38486 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38487 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38488 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38489 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38490 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38491 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38492 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38493 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38494 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38495 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38496 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38497 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38498 [D loss: 0.000002, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38499 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38500 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38501 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38502 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38503 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38504 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38505 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38506 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38507 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38508 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38509 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38510 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38511 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38512 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38513 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38514 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38515 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38516 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38517 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38518 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38519 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38520 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38521 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38522 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38523 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38524 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38525 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38526 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38527 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38528 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38529 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38530 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38531 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38532 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38533 [D loss: 0.000007, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38534 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38535 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38536 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38537 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38538 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38539 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38540 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38541 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38542 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38543 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38544 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38545 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38546 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38547 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38548 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38549 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38550 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38551 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38552 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38553 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38554 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38555 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38556 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38557 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38558 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38559 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38560 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38561 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38562 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38563 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38564 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38565 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38566 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38567 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38568 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38569 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38570 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38571 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:49 step:38572 [D loss: 0.000004, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38573 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38574 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38575 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38576 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38577 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38578 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38579 [D loss: 0.000007, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38580 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38581 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38582 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38583 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38584 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38585 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38586 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38587 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38588 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38589 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38590 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38591 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38592 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38593 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38594 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38595 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38596 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38597 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38598 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38599 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38600 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "##############\n",
      "[1.10220195 0.95019193 1.02710431 1.00859851 0.78491418 1.11166484\n",
      " 2.11458778 0.95169435 2.12123806 1.12440506]\n",
      "##########\n",
      "epoch:49 step:38601 [D loss: 0.000002, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38602 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38603 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38604 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38605 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38606 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38607 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38608 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38609 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38610 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38611 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38612 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38613 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38614 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38615 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38616 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38617 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38618 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38619 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38620 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38621 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38622 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38623 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38624 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38625 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38626 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38627 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38628 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38629 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38630 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38631 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38632 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38633 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38634 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38635 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38636 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38637 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38638 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38639 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38640 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38641 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38642 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38643 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38644 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38645 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38646 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38647 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38648 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38649 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38650 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38651 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38652 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38653 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38654 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38655 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38656 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38657 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38658 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38659 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38660 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38661 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38662 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38663 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38664 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38665 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38666 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38667 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38668 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38669 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38670 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38671 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38672 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38673 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38674 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38675 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38676 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38677 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38678 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38679 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38680 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38681 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38682 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38683 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:49 step:38684 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38685 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38686 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38687 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38688 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38689 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38690 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38691 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38692 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38693 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38694 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38695 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38696 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38697 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38698 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38699 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38700 [D loss: 0.000005, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38701 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38702 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38703 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38704 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38705 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38706 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38707 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38708 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38709 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38710 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38711 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38712 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38713 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38714 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38715 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38716 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38717 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38718 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38719 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38720 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38721 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38722 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38723 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38724 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38725 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38726 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38727 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38728 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38729 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38730 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38731 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38732 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38733 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38734 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38735 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38736 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38737 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38738 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38739 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38740 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38741 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38742 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38743 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38744 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38745 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38746 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38747 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38748 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38749 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38750 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38751 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38752 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38753 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38754 [D loss: 0.000002, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38755 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38756 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38757 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38758 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38759 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38760 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38761 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38762 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38763 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38764 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38765 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38766 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38767 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38768 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38769 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38770 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38771 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38772 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38773 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38774 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38775 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38776 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38777 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38778 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38779 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38780 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38781 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38782 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38783 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38784 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38785 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38786 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38787 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38788 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38789 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38790 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38791 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38792 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38793 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38794 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38795 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:49 step:38796 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38797 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38798 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38799 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38800 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "##############\n",
      "[2.1099617  0.91477292 1.05289716 1.02195946 0.82833592 0.96019002\n",
      " 1.11056359 0.89457065 2.12307529 1.05704471]\n",
      "##########\n",
      "epoch:49 step:38801 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38802 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38803 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38804 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38805 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38806 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38807 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38808 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38809 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38810 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38811 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38812 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38813 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38814 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38815 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38816 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38817 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38818 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38819 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38820 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38821 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38822 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38823 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38824 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38825 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38826 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38827 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38828 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38829 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38830 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38831 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38832 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38833 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38834 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38835 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38836 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38837 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38838 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38839 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38840 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38841 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38842 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38843 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38844 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38845 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38846 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38847 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38848 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38849 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38850 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38851 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38852 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38853 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38854 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38855 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38856 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38857 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38858 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38859 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38860 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38861 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38862 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38863 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38864 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38865 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38866 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38867 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38868 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38869 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38870 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38871 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38872 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38873 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38874 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38875 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38876 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38877 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38878 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38879 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38880 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38881 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38882 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38883 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38884 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38885 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38886 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38887 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38888 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38889 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38890 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38891 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38892 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38893 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38894 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38895 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38896 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38897 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38898 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38899 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38900 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38901 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38902 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38903 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38904 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:49 step:38905 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38906 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38907 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38908 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38909 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38910 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38911 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38912 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38913 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38914 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38915 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38916 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38917 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38918 [D loss: 0.000002, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38919 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38920 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38921 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38922 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38923 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38924 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38925 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38926 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38927 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38928 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38929 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38930 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38931 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38932 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38933 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38934 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38935 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38936 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38937 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38938 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38939 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38940 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38941 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38942 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38943 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38944 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38945 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38946 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38947 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38948 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38949 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38950 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38951 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38952 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38953 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38954 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38955 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38956 [D loss: 0.000002, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38957 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38958 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38959 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38960 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38961 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38962 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38963 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38964 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38965 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38966 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38967 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38968 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38969 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38970 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38971 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38972 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38973 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38974 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38975 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38976 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38977 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38978 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38979 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38980 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38981 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38982 [D loss: 0.000005, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38983 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38984 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38985 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38986 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38987 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38988 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38989 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38990 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38991 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38992 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38993 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38994 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38995 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38996 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38997 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38998 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:38999 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:39000 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "##############\n",
      "[2.11816498 0.91200031 1.02631098 1.00997262 0.76606981 0.79235492\n",
      " 2.11088402 0.91036329 2.11213405 1.09828852]\n",
      "##########\n",
      "epoch:49 step:39001 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:39002 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:39003 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:39004 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:39005 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:39006 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:39007 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:39008 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:39009 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:39010 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:39011 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:39012 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:39013 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:39014 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:39015 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:49 step:39016 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:39017 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:39018 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:39019 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:39020 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:39021 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:39022 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:39023 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:39024 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:39025 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:39026 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:39027 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:39028 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:39029 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:39030 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:39031 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:39032 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:39033 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:39034 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:39035 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:39036 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:39037 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:39038 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:39039 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:39040 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:39041 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:39042 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:39043 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:39044 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:39045 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:39046 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:39047 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:39048 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:39049 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "epoch:49 step:39050 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.datasets import cifar10\n",
    "from torch.utils.data import Dataset\n",
    "import torch.utils.data as Data\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, imgs, transform=None):\n",
    "        # super().__init__()\n",
    "        self.imgs = imgs\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.imgs)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img = self.imgs[index]\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "        else:\n",
    "            img = torch.from_numpy(img)\n",
    "        img=img.reshape([3,32,32])\n",
    "        return img\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import model\n",
    "import torch.nn.functional as F\n",
    "model = model.cifar10(128)\n",
    "model.load_state_dict(torch.load('./log/default/best-85.pth'))\n",
    "model.cuda()\n",
    "def EuclideanDistances(A, B):\n",
    "    BT = B.transpose()\n",
    "    # vecProd = A * BT\n",
    "    vecProd = np.dot(A, BT)\n",
    "    # print(vecProd)\n",
    "    SqA = A ** 2\n",
    "    # print(SqA)\n",
    "    sumSqA = np.matrix(np.sum(SqA, axis=1))\n",
    "    sumSqAEx = np.tile(sumSqA.transpose(), (1, vecProd.shape[1]))\n",
    "    # print(sumSqAEx)\n",
    "\n",
    "    SqB = B ** 2\n",
    "    sumSqB = np.sum(SqB, axis=1)\n",
    "    sumSqBEx = np.tile(sumSqB, (vecProd.shape[0], 1))\n",
    "    SqED = sumSqBEx + sumSqAEx - 2 * vecProd\n",
    "    SqED[SqED < 0] = 0.0\n",
    "    ED = np.sqrt(SqED)\n",
    "    return np.divide(ED.sum(), ED.shape[0] * ED.shape[1])\n",
    "\n",
    "\n",
    "def cal_distance_image_real(images, labels):\n",
    "    x_dataset = MyDataset(images)\n",
    "    # print(x_dataset[0].shape)\n",
    "    x_real_loader = Data.DataLoader(dataset=x_dataset, batch_size=200, shuffle=True)\n",
    "    y_logits = []\n",
    "    for i, data in enumerate(x_real_loader):\n",
    "        # indx_target = target.clone()\n",
    "        data = data.cuda()\n",
    "        data = Variable(data, volatile=True)\n",
    "        output = model(data)\n",
    "        pred = F.softmax(output).cpu().detach().numpy()\n",
    "        y_logits += [i for i in pred]\n",
    "    dict = {}\n",
    "    all_dis = []\n",
    "    for i in range(10):\n",
    "        dict[i] = []\n",
    "    for i in range(len(labels)):\n",
    "        dict[labels[i]].append(y_logits[i])\n",
    "    for i in range(10):\n",
    "        dict[i] = np.array(dict[i])\n",
    "        if len(dict[i]):\n",
    "            dis = EuclideanDistances(dict[i], dict[i])  # 生成图片的紧度\n",
    "        else:\n",
    "            dis = -1\n",
    "        all_dis.append(dis)\n",
    "    return np.array(all_dis)\n",
    "\n",
    "\n",
    "def cal_distance_image_fake(images):\n",
    "    x_dataset = MyDataset(images)\n",
    "    # print(x_dataset[0].shape)\n",
    "    x_real_loader = Data.DataLoader(dataset=x_dataset, batch_size=200, shuffle=True)\n",
    "    y_logits = []\n",
    "    labels=[]\n",
    "    for i, data in enumerate(x_real_loader):\n",
    "        # indx_target = target.clone()\n",
    "        data = data.cuda()\n",
    "        data = Variable(data, volatile=True)\n",
    "        output = model(data)\n",
    "        pred = output.data.max(1)[1]\n",
    "        labels += [i for i in pred.cpu().numpy()]\n",
    "        pred = F.softmax(output).cpu().detach().numpy()\n",
    "        y_logits += [i for i in pred]\n",
    "\n",
    "    dict = {}\n",
    "    all_dis = []\n",
    "    for i in range(10):\n",
    "        dict[i] = []\n",
    "    for i in range(len(labels)):\n",
    "        dict[labels[i]].append(y_logits[i])\n",
    "    for i in range(10):\n",
    "        dict[i] = np.array(dict[i])\n",
    "        if len(dict[i]):\n",
    "            dis = EuclideanDistances(dict[i], dict[i])  # 生成图片的紧度\n",
    "        else:\n",
    "            dis = -1\n",
    "        all_dis.append(dis)\n",
    "    return np.array(all_dis)\n",
    "\n",
    "import os\n",
    "if not os.path.isdir('saved_models_{}'.format('cgan')):\n",
    "    os.mkdir('saved_models_{}'.format('cgan'))\n",
    "f = open('saved_models_{}/log_collapse1.txt'.format('cgan'), mode='w')\n",
    "import torch.utils.data as Data\n",
    "import cv2\n",
    "\n",
    "\n",
    "from keras.datasets import fashion_mnist,cifar10\n",
    "from keras.layers import Input, Dense, Reshape, Flatten, Dropout, multiply, Conv2DTranspose\n",
    "from keras.layers import BatchNormalization, Activation, Embedding, ZeroPadding2D\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.layers.convolutional import UpSampling2D, Conv2D\n",
    "from keras.layers import Concatenate, GaussianNoise,Activation\n",
    "from keras.models import Sequential, Model\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import os\n",
    "class CGAN():\n",
    "    def __init__(self):\n",
    "        # Input shape\n",
    "        self.img_rows = 32\n",
    "        self.img_cols = 32\n",
    "        self.channels = 3\n",
    "        self.img_shape = (self.img_rows, self.img_cols, self.channels)\n",
    "        self.num_classes = 10\n",
    "        self.latent_dim = 100\n",
    "        self.x = []\n",
    "        self.y = np.zeros((31, 1), dtype=np.int)\n",
    "        self.y = list(self.y)\n",
    "        for i in range(31):\n",
    "            self.y[i] = []\n",
    "\n",
    "        optimizer = Adam(0.0002, 0.5)\n",
    "\n",
    "        # Build and compile the discriminator\n",
    "        self.discriminator = self.build_discriminator()\n",
    "        self.discriminator.compile(loss=['binary_crossentropy'],\n",
    "            optimizer=optimizer,\n",
    "            metrics=['accuracy'])\n",
    "\n",
    "        # Build the generator\n",
    "        self.generator = self.build_generator()\n",
    "\n",
    "        # The generator takes noise and the target label as input\n",
    "        # and generates the corresponding digit of that label\n",
    "        noise = Input(shape=(self.latent_dim,))\n",
    "        label = Input(shape=(1,))\n",
    "        img = self.generator([noise, label])\n",
    "\n",
    "        # For the combined model we will only train the generator\n",
    "        self.discriminator.trainable = False\n",
    "\n",
    "        # The discriminator takes generated image as input and determines validity\n",
    "        # and the label of that image\n",
    "        valid = self.discriminator([img, label])\n",
    "\n",
    "        # The combined model  (stacked generator and discriminator)\n",
    "        # Trains generator to fool discriminator\n",
    "        self.combined = Model([noise, label], valid)\n",
    "        self.combined.compile(loss=['binary_crossentropy'],\n",
    "            optimizer=optimizer)\n",
    "\n",
    "    def build_generator(self):\n",
    "        model = Sequential()\n",
    "\n",
    "        model.add(Dense(2 * 2 * 512, activation='relu',input_dim=self.latent_dim))\n",
    "        model.add(BatchNormalization(momentum=0.9))\n",
    "        model.add(LeakyReLU(alpha=0.1))\n",
    "        model.add(Reshape((2, 2, 512)))\n",
    "\n",
    "        model.add(Conv2DTranspose(256, kernel_size=5, strides=2, padding='same'))\n",
    "        model.add(BatchNormalization(momentum=0.9))\n",
    "        model.add(LeakyReLU(alpha=0.1))\n",
    "\n",
    "\n",
    "        model.add(Conv2DTranspose(128, kernel_size=5, strides=2, padding='same'))\n",
    "        model.add(BatchNormalization(momentum=0.9))\n",
    "        model.add(LeakyReLU(alpha=0.1))\n",
    "\n",
    "        model.add(Conv2DTranspose(64, kernel_size=5, strides=2, padding='same'))\n",
    "        model.add( BatchNormalization(momentum=0.9))\n",
    "        model.add(LeakyReLU(alpha=0.1))\n",
    "\n",
    "\n",
    "        model.add(Conv2DTranspose(3, kernel_size=5, strides=2, padding='same', activation='tanh'))\n",
    "        model.add(Activation(\"tanh\"))\n",
    "\n",
    "        model.summary()\n",
    "\n",
    "        noise = Input(shape=(self.latent_dim,))\n",
    "        label = Input(shape=(1,), dtype='int32')\n",
    "        label_embedding = Flatten()(Embedding(self.num_classes, self.latent_dim)(label))\n",
    "\n",
    "        model_input = multiply([noise, label_embedding])\n",
    "        img = model(model_input)\n",
    "\n",
    "        return Model([noise, label], img)\n",
    "    def build_discriminator(self):\n",
    "        model = Sequential()\n",
    "\n",
    "        # Conv 1: 16x16x64\n",
    "        model.add(Conv2D(64, kernel_size=5, strides=2, padding='same' ,input_shape=self.img_shape))\n",
    "        model.add(BatchNormalization(momentum=0.9))\n",
    "        model.add(LeakyReLU(alpha=0.1))\n",
    "\n",
    "        # Conv 2:\n",
    "        model.add(Conv2D(128, kernel_size=5, strides=2, padding='same'))\n",
    "        model.add(BatchNormalization(momentum=0.9))\n",
    "        model.add(LeakyReLU(alpha=0.1))\n",
    "\n",
    "        # Conv 3:\n",
    "        model.add(Conv2D(256, kernel_size=5, strides=2, padding='same'))\n",
    "        model.add( BatchNormalization(momentum=0.9))\n",
    "        model.add( LeakyReLU(alpha=0.1))\n",
    "\n",
    "        # Conv 4:\n",
    "        model.add(Conv2D(512, kernel_size=5, strides=2, padding='same'))\n",
    "        model.add(BatchNormalization(momentum=0.9))\n",
    "        model.add( LeakyReLU(alpha=0.1))\n",
    "        model.summary()\n",
    "\n",
    "        # FC\n",
    "        model.add(Flatten())\n",
    "        img = Input(shape=self.img_shape)\n",
    "        label = Input(shape=(1,), dtype='int32')\n",
    "        labels = Flatten()(Embedding(self.num_classes, self.latent_dim)(label))\n",
    "        discriminator =model(img)\n",
    "\n",
    "        # Concatenate\n",
    "        merged_layer = Concatenate()([discriminator, labels])\n",
    "        discriminator = Dense(512, activation='relu')(merged_layer)\n",
    "\n",
    "        # Output\n",
    "        discriminator = Dense(1, activation='sigmoid')(discriminator)\n",
    "\n",
    "        return Model([img, label], discriminator)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def train(self, epochs, batch_size=128, sample_interval=50):\n",
    "\n",
    "        # Load the dataset\n",
    "        (X_train, y_train), (X_test, y_test) = cifar10.load_data()\n",
    "\n",
    "        X_train = (X_train.astype(np.float32) - 127.5) / 127.5\n",
    "        X_test = (X_test.astype(np.float32) - 127.5) / 127.5\n",
    "        # X_train = np.expand_dims(X_train, axis=3)\n",
    "        y_train = y_train.reshape(-1, 1)\n",
    "\n",
    "        # Adversarial ground truths\n",
    "        valid = np.ones((batch_size, 1))\n",
    "        fake = np.zeros((batch_size, 1))\n",
    "\n",
    "        nb_batches = int(X_train.shape[0] / batch_size)\n",
    "        global_step = 0\n",
    "        steps = []\n",
    "        values = []\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "\n",
    "            for index in range(nb_batches):\n",
    "                global_step += 1\n",
    "                imgs = X_train[index * batch_size:(index + 1) * batch_size]\n",
    "                labels = y_train[index * batch_size:(index + 1) * batch_size]\n",
    "\n",
    "                # Sample noise as generator input\n",
    "                noise = np.random.normal(0, 1, (batch_size, 100))\n",
    "\n",
    "                # Generate a half batch of new images\n",
    "                gen_imgs = self.generator.predict([noise, labels])\n",
    "\n",
    "                # Train the discriminator\n",
    "                d_loss_real = self.discriminator.train_on_batch([imgs, labels], valid)\n",
    "                d_loss_fake = self.discriminator.train_on_batch([gen_imgs, labels], fake)\n",
    "                d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "\n",
    "                # ---------------------\n",
    "                #  Train Generator\n",
    "                # ---------------------\n",
    "\n",
    "                # Condition on labels\n",
    "                sampled_labels = np.random.randint(0, 10, batch_size).reshape(-1, 1)\n",
    "\n",
    "                # Train the generator\n",
    "                g_loss = self.combined.train_on_batch([noise, sampled_labels], valid)\n",
    "\n",
    "                # Plot the progress\n",
    "                print(\"epoch:%d step:%d [D loss: %f, acc.: %.2f%%] [G loss: %f]\" % (epoch,global_step, d_loss[0], 100 * d_loss[1], g_loss))\n",
    "\n",
    "                sample_num=5000\n",
    "                if global_step % sample_interval == 0:\n",
    "                    self.mode_drop(X_test,y_test,sample_num, global_step)\n",
    "\n",
    "\n",
    "    def mode_drop(self, x_test,y_test,sample_num, global_step):\n",
    "        r, c = 10, 1000\n",
    "        noise = np.random.normal(0, 1, (r * c, 100))\n",
    "        sampled_labels = np.array([num for _ in range(r) for num in range(c)])\n",
    "        gen_imgs = self.generator.predict([noise,sampled_labels])\n",
    "        # Rescale images 0 - 1\n",
    "        gen_imgs = 0.5 * gen_imgs + 0.5\n",
    "        labels = np.squeeze(y_test[:sample_num])\n",
    "        labels = np.squeeze(labels)\n",
    "        dis_real = cal_distance_image_real(x_test[:sample_num], labels)\n",
    "        dis_fake = cal_distance_image_fake(gen_imgs)\n",
    "        dis_cha = dis_real - dis_fake\n",
    "        print('##############')\n",
    "        # print(dis_real)\n",
    "        # print(dis_fake)\n",
    "        print(dis_cha)\n",
    "        print('##########')\n",
    "        f.writelines('\\n')\n",
    "        f.writelines('step:' + str(global_step))\n",
    "        f.writelines('\\n')\n",
    "        f.writelines('紧度')\n",
    "        f.writelines('\\n')\n",
    "        f.writelines(' %.8f ' % (i) for i in dis_cha)\n",
    "        f.writelines('\\n')\n",
    "if __name__ == '__main__':\n",
    "    cgan = CGAN()\n",
    "    cgan.train(epochs=50, batch_size=64, sample_interval=200)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pppppppp [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
