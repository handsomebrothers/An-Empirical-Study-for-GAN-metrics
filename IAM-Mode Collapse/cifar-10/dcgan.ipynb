{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.client.session.Session at 0x7f65104c90f0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "# #指定使用那块GUP训练\n",
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "config = tf.ConfigProto()\n",
    "# 设置最大占有GPU不超过显存的70%\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.7 \n",
    "# # 重点：设置动态分配GPU\n",
    "config.gpu_options.allow_growth = True\n",
    "# 创建session时\n",
    "tf.Session(config=config) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "Sequential(\n",
      "  (0): Conv2d(3, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "  (2): ReLU()\n",
      "  (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "  (5): ReLU()\n",
      "  (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (7): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (8): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "  (9): ReLU()\n",
      "  (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (11): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "  (12): ReLU()\n",
      "  (13): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (14): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (15): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "  (16): ReLU()\n",
      "  (17): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (18): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "  (19): ReLU()\n",
      "  (20): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (21): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (22): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "  (23): ReLU()\n",
      "  (24): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      ")\n",
      "Sequential(\n",
      "  (0): Linear(in_features=1024, out_features=10, bias=True)\n",
      ")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 16, 16, 32)        896       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)    (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 8, 8, 64)          18496     \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 8, 8, 64)          256       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)    (None, 8, 8, 64)          0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 8, 8, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 4, 4, 128)         73856     \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 4, 4, 128)         512       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)    (None, 4, 4, 128)         0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 4, 4, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 4, 4, 256)         295168    \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 4, 4, 256)         1024      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)    (None, 4, 4, 256)         0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 4, 4, 256)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 4097      \n",
      "=================================================================\n",
      "Total params: 394,305\n",
      "Trainable params: 393,409\n",
      "Non-trainable params: 896\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_2 (Dense)              (None, 8192)              827392    \n",
      "_________________________________________________________________\n",
      "reshape_1 (Reshape)          (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "up_sampling2d_1 (UpSampling2 (None, 16, 16, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 16, 16, 128)       147584    \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 16, 16, 128)       512       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 16, 16, 128)       0         \n",
      "_________________________________________________________________\n",
      "up_sampling2d_2 (UpSampling2 (None, 32, 32, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 32, 32, 64)        73792     \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 32, 32, 64)        256       \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 32, 32, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 32, 32, 3)         1731      \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 32, 32, 3)         0         \n",
      "=================================================================\n",
      "Total params: 1,051,267\n",
      "Trainable params: 1,050,883\n",
      "Non-trainable params: 384\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/imi432_006/anaconda3/envs/tf/lib/python3.5/site-packages/keras/engine/training.py:490: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0 step:200 [D loss: 0.550247, acc.: 72.66%] [G loss: 1.106247]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/imi432_006/anaconda3/envs/tf/lib/python3.5/site-packages/ipykernel_launcher.py:57: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "/home/imi432_006/anaconda3/envs/tf/lib/python3.5/site-packages/ipykernel_launcher.py:59: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "/home/imi432_006/anaconda3/envs/tf/lib/python3.5/site-packages/ipykernel_launcher.py:86: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "/home/imi432_006/anaconda3/envs/tf/lib/python3.5/site-packages/ipykernel_launcher.py:90: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##############\n",
      "[0.88015035 0.84705945 0.81680094 0.80974238 0.77868681 0.7906159\n",
      " 0.89798133 0.83300413 0.82417315 0.84928828]\n",
      "##########\n",
      "epoch:0 step:400 [D loss: 0.611269, acc.: 68.75%] [G loss: 1.181673]\n",
      "##############\n",
      "[0.84113403 0.84839224 0.80893177 0.83358759 0.83729473 0.80621141\n",
      " 0.87326223 0.81462029 0.83362887 0.82429233]\n",
      "##########\n",
      "epoch:0 step:600 [D loss: 0.882431, acc.: 39.84%] [G loss: 0.924161]\n",
      "##############\n",
      "[0.84875438 0.88471881 0.81275305 0.81246224 0.79102452 0.82090392\n",
      " 0.89020711 0.81522005 0.80745293 0.81522996]\n",
      "##########\n",
      "epoch:1 step:800 [D loss: 0.795008, acc.: 49.22%] [G loss: 0.888867]\n",
      "##############\n",
      "[0.86333341 0.87337321 0.81619267 0.82257824 0.7959238  0.8196529\n",
      " 0.9060951  0.8095662  0.82671472 0.83322938]\n",
      "##########\n",
      "epoch:1 step:1000 [D loss: 0.739402, acc.: 50.78%] [G loss: 0.819836]\n",
      "##############\n",
      "[0.87647884 0.83945851 0.79611566 0.81833346 0.80842657 0.83386183\n",
      " 0.89858219 0.81600744 0.82955747 0.84114839]\n",
      "##########\n",
      "epoch:1 step:1200 [D loss: 0.734953, acc.: 53.12%] [G loss: 0.848454]\n",
      "##############\n",
      "[0.88773589 0.85523898 0.81308877 0.81426317 0.77513342 0.84075478\n",
      " 0.89608725 0.81067225 0.81260863 0.83880453]\n",
      "##########\n",
      "epoch:1 step:1400 [D loss: 0.753351, acc.: 50.00%] [G loss: 0.866096]\n",
      "##############\n",
      "[0.84232974 0.85962752 0.79348113 0.81261818 0.79328739 0.82075269\n",
      " 0.88318659 0.83120641 0.8101044  0.83421236]\n",
      "##########\n",
      "epoch:2 step:1600 [D loss: 0.775221, acc.: 45.31%] [G loss: 0.841569]\n",
      "##############\n",
      "[0.82798664 0.85375713 0.80483104 0.81497893 0.77685217 0.83366938\n",
      " 0.89338698 0.82349443 0.79903659 0.83402148]\n",
      "##########\n",
      "epoch:2 step:1800 [D loss: 0.700605, acc.: 53.12%] [G loss: 0.861735]\n",
      "##############\n",
      "[0.86447903 0.84966862 0.79542107 0.80375307 0.78391646 0.81612036\n",
      " 0.8879479  0.81537856 0.8002456  0.83548156]\n",
      "##########\n",
      "epoch:2 step:2000 [D loss: 0.762230, acc.: 45.31%] [G loss: 0.834777]\n",
      "##############\n",
      "[0.84966287 0.8771775  0.78338786 0.83147869 0.78445293 0.82069967\n",
      " 0.90445961 0.81823586 0.8166661  0.8169793 ]\n",
      "##########\n",
      "epoch:2 step:2200 [D loss: 0.682377, acc.: 60.16%] [G loss: 0.910759]\n",
      "##############\n",
      "[0.85517642 0.8389081  0.80704204 0.80270039 0.7694828  0.7956625\n",
      " 0.89169446 0.8133215  0.79936351 0.8473634 ]\n",
      "##########\n",
      "epoch:3 step:2400 [D loss: 0.704085, acc.: 53.91%] [G loss: 0.782548]\n",
      "##############\n",
      "[0.84200814 0.85802899 0.80967506 0.8156231  0.79536133 0.779632\n",
      " 0.89013744 0.82389222 0.82851467 0.81754849]\n",
      "##########\n",
      "epoch:3 step:2600 [D loss: 0.640419, acc.: 63.28%] [G loss: 0.987827]\n",
      "##############\n",
      "[0.8550316  0.85977188 0.78140239 0.79329949 0.7717673  0.80425906\n",
      " 0.88743455 0.81346783 0.80575304 0.85558675]\n",
      "##########\n",
      "epoch:3 step:2800 [D loss: 0.638439, acc.: 67.97%] [G loss: 0.944897]\n",
      "##############\n",
      "[0.85554855 0.85157817 0.78958547 0.83193788 0.80075906 0.83711173\n",
      " 0.86601797 0.83714534 0.83037369 0.84110401]\n",
      "##########\n",
      "epoch:3 step:3000 [D loss: 0.828044, acc.: 34.38%] [G loss: 0.675945]\n",
      "##############\n",
      "[0.86394283 0.86065175 0.79214729 0.83027991 0.80213282 0.8157941\n",
      " 0.87782073 0.80364887 0.80054846 0.83985504]\n",
      "##########\n",
      "epoch:4 step:3200 [D loss: 0.734873, acc.: 47.66%] [G loss: 0.850598]\n",
      "##############\n",
      "[0.86010213 0.84249549 0.80658305 0.83586517 0.81219326 0.83626631\n",
      " 0.90033083 0.82694176 0.81721768 0.83227242]\n",
      "##########\n",
      "epoch:4 step:3400 [D loss: 0.672253, acc.: 60.94%] [G loss: 0.869397]\n",
      "##############\n",
      "[0.84107731 0.86801246 0.7861544  0.78581749 0.79772767 0.82722578\n",
      " 0.89532945 0.80398049 0.84286649 0.82674058]\n",
      "##########\n",
      "epoch:4 step:3600 [D loss: 0.635606, acc.: 60.94%] [G loss: 0.890935]\n",
      "##############\n",
      "[0.85026219 0.86128409 0.80351989 0.80886533 0.79948631 0.8208\n",
      " 0.87308323 0.78525711 0.80882975 0.8275347 ]\n",
      "##########\n",
      "epoch:4 step:3800 [D loss: 0.681066, acc.: 59.38%] [G loss: 0.891121]\n",
      "##############\n",
      "[0.862646   0.85484412 0.79401685 0.82075456 0.79873286 0.80421305\n",
      " 0.88666285 0.82345565 0.81593392 0.8445517 ]\n",
      "##########\n",
      "epoch:5 step:4000 [D loss: 0.606064, acc.: 71.88%] [G loss: 0.843459]\n",
      "##############\n",
      "[0.86172125 0.8666421  0.78493457 0.79529239 0.76906357 0.82228972\n",
      " 0.88459471 0.82100601 0.81771579 0.83357778]\n",
      "##########\n",
      "epoch:5 step:4200 [D loss: 0.706981, acc.: 53.12%] [G loss: 0.705038]\n",
      "##############\n",
      "[0.87767316 0.85260981 0.78726969 0.79865822 0.76337807 0.83889706\n",
      " 0.86163529 0.8226528  0.82608655 0.83608256]\n",
      "##########\n",
      "epoch:5 step:4400 [D loss: 0.613954, acc.: 64.84%] [G loss: 0.932721]\n",
      "##############\n",
      "[0.86928966 0.85385636 0.79759667 0.78665336 0.7810663  0.84991516\n",
      " 0.88838083 0.81429478 0.82978123 0.84132006]\n",
      "##########\n",
      "epoch:5 step:4600 [D loss: 0.676296, acc.: 57.03%] [G loss: 0.695965]\n",
      "##############\n",
      "[0.87731462 0.86782776 0.7980813  0.80307187 0.79907006 0.81629007\n",
      " 0.89477039 0.80542187 0.83434663 0.82499723]\n",
      "##########\n",
      "epoch:6 step:4800 [D loss: 0.572590, acc.: 70.31%] [G loss: 0.855068]\n",
      "##############\n",
      "[0.84452322 0.85536872 0.80717557 0.82085116 0.77813329 0.80354237\n",
      " 0.89787866 0.84203158 0.82853711 0.82224012]\n",
      "##########\n",
      "epoch:6 step:5000 [D loss: 0.769257, acc.: 42.97%] [G loss: 0.773788]\n",
      "##############\n",
      "[0.85479108 0.85809717 0.78847698 0.8135985  0.81026104 0.82472091\n",
      " 0.91126641 0.83460696 0.82154634 0.83962635]\n",
      "##########\n",
      "epoch:6 step:5200 [D loss: 0.712834, acc.: 50.78%] [G loss: 0.753341]\n",
      "##############\n",
      "[0.85976293 0.87199785 0.80162588 0.82730255 0.78178731 0.79913298\n",
      " 0.88651922 0.83256245 0.82386386 0.84866509]\n",
      "##########\n",
      "epoch:6 step:5400 [D loss: 0.615998, acc.: 64.06%] [G loss: 0.924267]\n",
      "##############\n",
      "[0.86930524 0.8433117  0.81432223 0.82439131 0.78735746 0.82160386\n",
      " 0.87718224 0.80051376 0.83098522 0.82734497]\n",
      "##########\n",
      "epoch:7 step:5600 [D loss: 0.756852, acc.: 42.19%] [G loss: 0.828273]\n",
      "##############\n",
      "[0.84978195 0.8608709  0.80911508 0.82467391 0.80173168 0.81412796\n",
      " 0.86982726 0.82610762 0.81393258 0.85411771]\n",
      "##########\n",
      "epoch:7 step:5800 [D loss: 0.677911, acc.: 64.06%] [G loss: 0.790133]\n",
      "##############\n",
      "[0.85721494 0.87401749 0.82053053 0.81886941 0.80054822 0.81690414\n",
      " 0.89397566 0.82927608 0.79796373 0.84687308]\n",
      "##########\n",
      "epoch:7 step:6000 [D loss: 0.684761, acc.: 53.91%] [G loss: 0.833979]\n",
      "##############\n",
      "[0.87454948 0.8690084  0.80408584 0.81209985 0.77772303 0.81665469\n",
      " 0.89774602 0.80910287 0.82381919 0.83820273]\n",
      "##########\n",
      "epoch:7 step:6200 [D loss: 0.662962, acc.: 66.41%] [G loss: 0.928740]\n",
      "##############\n",
      "[0.85787203 0.85809246 0.80771462 0.78613646 0.78487704 0.81574733\n",
      " 0.8925223  0.79858032 0.82209207 0.83593861]\n",
      "##########\n",
      "epoch:8 step:6400 [D loss: 0.703715, acc.: 52.34%] [G loss: 0.927136]\n",
      "##############\n",
      "[0.86035794 0.8465475  0.79422001 0.81649324 0.80036052 0.81017881\n",
      " 0.89408472 0.8162484  0.80465639 0.83435732]\n",
      "##########\n",
      "epoch:8 step:6600 [D loss: 0.633204, acc.: 64.84%] [G loss: 0.820366]\n",
      "##############\n",
      "[0.86991308 0.84278339 0.78837484 0.79600054 0.77682164 0.82535636\n",
      " 0.89102342 0.81268306 0.79999341 0.83950557]\n",
      "##########\n",
      "epoch:8 step:6800 [D loss: 0.765979, acc.: 43.75%] [G loss: 0.766102]\n",
      "##############\n",
      "[0.8555542  0.84049864 0.82006068 0.79858799 0.79114607 0.79865361\n",
      " 0.89352458 0.81819616 0.82496418 0.82329181]\n",
      "##########\n",
      "epoch:8 step:7000 [D loss: 0.691722, acc.: 53.12%] [G loss: 0.910525]\n",
      "##############\n",
      "[0.84786774 0.83777304 0.80925828 0.82232108 0.77580674 0.81858405\n",
      " 0.88854569 0.81150454 0.82186664 0.83693959]\n",
      "##########\n",
      "epoch:9 step:7200 [D loss: 0.703007, acc.: 55.47%] [G loss: 0.830785]\n",
      "##############\n",
      "[0.85267049 0.86115208 0.80012743 0.80033721 0.7843907  0.82756608\n",
      " 0.90039745 0.82016973 0.7944783  0.84456119]\n",
      "##########\n",
      "epoch:9 step:7400 [D loss: 0.699800, acc.: 53.12%] [G loss: 0.676294]\n",
      "##############\n",
      "[0.86281896 0.83405068 0.80817011 0.82305593 0.77420552 0.80786945\n",
      " 0.89608571 0.81650013 0.82221498 0.82985907]\n",
      "##########\n",
      "epoch:9 step:7600 [D loss: 0.664030, acc.: 61.72%] [G loss: 0.888311]\n",
      "##############\n",
      "[0.8530964  0.85161107 0.8115508  0.81392117 0.77935122 0.82748115\n",
      " 0.88607649 0.83001193 0.81885216 0.82687071]\n",
      "##########\n",
      "epoch:9 step:7800 [D loss: 0.654186, acc.: 59.38%] [G loss: 0.899216]\n",
      "##############\n",
      "[0.86117516 0.85173643 0.788248   0.80183138 0.79088268 0.82386674\n",
      " 0.89075163 0.8352226  0.78685626 0.85062525]\n",
      "##########\n",
      "epoch:10 step:8000 [D loss: 0.582016, acc.: 68.75%] [G loss: 0.647297]\n",
      "##############\n",
      "[0.86549779 0.84504557 0.79401505 0.79194343 0.81792303 0.82584839\n",
      " 0.88288179 0.81679215 0.81237839 0.83352442]\n",
      "##########\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:10 step:8200 [D loss: 0.739428, acc.: 42.19%] [G loss: 0.800787]\n",
      "##############\n",
      "[0.82954098 0.83614388 0.78581478 0.81874512 0.80226168 0.82585682\n",
      " 0.8816478  0.82258364 0.81624767 0.83034763]\n",
      "##########\n",
      "epoch:10 step:8400 [D loss: 0.745901, acc.: 41.41%] [G loss: 0.803642]\n",
      "##############\n",
      "[0.83459707 0.84994118 0.79016554 0.80312147 0.8147039  0.79748811\n",
      " 0.8529069  0.8024495  0.83518506 0.84671573]\n",
      "##########\n",
      "epoch:11 step:8600 [D loss: 0.697498, acc.: 52.34%] [G loss: 0.824998]\n",
      "##############\n",
      "[0.8439825  0.85471287 0.80210931 0.78924979 0.78198268 0.82464851\n",
      " 0.8703156  0.83986628 0.84187066 0.84071625]\n",
      "##########\n",
      "epoch:11 step:8800 [D loss: 0.671702, acc.: 60.94%] [G loss: 0.886734]\n",
      "##############\n",
      "[0.86273235 0.85736771 0.79499343 0.80173454 0.81717208 0.83465194\n",
      " 0.86340756 0.83476769 0.83556675 0.83467372]\n",
      "##########\n",
      "epoch:11 step:9000 [D loss: 0.647830, acc.: 65.62%] [G loss: 0.826627]\n",
      "##############\n",
      "[0.84512237 0.8608667  0.79471946 0.80819371 0.80192523 0.81551638\n",
      " 0.87006829 0.80904278 0.82320996 0.82869866]\n",
      "##########\n",
      "epoch:11 step:9200 [D loss: 0.650957, acc.: 60.16%] [G loss: 0.773060]\n",
      "##############\n",
      "[0.85609384 0.85439111 0.79898898 0.81729122 0.79213301 0.82961428\n",
      " 0.88013148 0.82891806 0.82209258 0.85253279]\n",
      "##########\n",
      "epoch:12 step:9400 [D loss: 0.777217, acc.: 40.62%] [G loss: 0.747966]\n",
      "##############\n",
      "[0.85861241 0.84314727 0.78601035 0.8287216  0.80328255 0.84257326\n",
      " 0.88685796 0.81628046 0.81159215 0.83094633]\n",
      "##########\n",
      "epoch:12 step:9600 [D loss: 0.705517, acc.: 53.12%] [G loss: 0.792709]\n",
      "##############\n",
      "[0.84577548 0.84330417 0.81382682 0.810269   0.79390843 0.82561263\n",
      " 0.89981316 0.81375442 0.80513986 0.84261828]\n",
      "##########\n",
      "epoch:12 step:9800 [D loss: 0.661495, acc.: 57.81%] [G loss: 0.886735]\n",
      "##############\n",
      "[0.87119087 0.83717498 0.80389505 0.8111639  0.77367834 0.82959572\n",
      " 0.8853681  0.7953311  0.78241842 0.84450658]\n",
      "##########\n",
      "epoch:12 step:10000 [D loss: 0.615899, acc.: 64.06%] [G loss: 0.984871]\n",
      "##############\n",
      "[0.85353958 0.85085732 0.79661026 0.82907871 0.79685844 0.83854223\n",
      " 0.8623835  0.83576294 0.82103833 0.85195042]\n",
      "##########\n",
      "epoch:13 step:10200 [D loss: 0.719731, acc.: 53.12%] [G loss: 0.788732]\n",
      "##############\n",
      "[0.86003712 0.877716   0.79554197 0.79826649 0.7927697  0.83355216\n",
      " 0.886617   0.8255568  0.83604127 0.85784888]\n",
      "##########\n",
      "epoch:13 step:10400 [D loss: 0.705643, acc.: 49.22%] [G loss: 0.841617]\n",
      "##############\n",
      "[0.85472145 0.8444058  0.80322091 0.80823507 0.78804896 0.81610275\n",
      " 0.87483174 0.8384904  0.82328507 0.84370797]\n",
      "##########\n",
      "epoch:13 step:10600 [D loss: 0.710627, acc.: 48.44%] [G loss: 0.841401]\n",
      "##############\n",
      "[0.85763891 0.85127225 0.79828438 0.81351064 0.78729164 0.81036778\n",
      " 0.89626848 0.81893432 0.81605574 0.83711201]\n",
      "##########\n",
      "epoch:13 step:10800 [D loss: 0.767706, acc.: 42.19%] [G loss: 0.751091]\n",
      "##############\n",
      "[0.85247532 0.86038489 0.81093774 0.83222862 0.78099063 0.80867725\n",
      " 0.89693786 0.83063338 0.80864077 0.83984056]\n",
      "##########\n",
      "epoch:14 step:11000 [D loss: 0.767919, acc.: 35.16%] [G loss: 0.793506]\n",
      "##############\n",
      "[0.84673236 0.85250485 0.78462477 0.81937953 0.7673455  0.82057913\n",
      " 0.87400816 0.82384088 0.83396214 0.82217077]\n",
      "##########\n",
      "epoch:14 step:11200 [D loss: 0.666019, acc.: 64.06%] [G loss: 0.763728]\n",
      "##############\n",
      "[0.84921257 0.8477012  0.79728281 0.79701727 0.80168199 0.83029309\n",
      " 0.86316323 0.81962917 0.82194524 0.82901588]\n",
      "##########\n",
      "epoch:14 step:11400 [D loss: 0.670267, acc.: 56.25%] [G loss: 0.718178]\n",
      "##############\n",
      "[0.85240722 0.84499263 0.80324187 0.80159535 0.7901887  0.80139339\n",
      " 0.88048119 0.82911924 0.80970161 0.84564361]\n",
      "##########\n",
      "epoch:14 step:11600 [D loss: 0.709835, acc.: 44.53%] [G loss: 0.733448]\n",
      "##############\n",
      "[0.86315718 0.84989193 0.77286303 0.80790406 0.79794955 0.85200144\n",
      " 0.87319335 0.82603119 0.80704325 0.83754581]\n",
      "##########\n",
      "epoch:15 step:11800 [D loss: 0.721915, acc.: 50.00%] [G loss: 0.769636]\n",
      "##############\n",
      "[0.86500797 0.87582502 0.80536351 0.81693395 0.77416449 0.83251321\n",
      " 0.86710265 0.82180312 0.81758685 0.84339364]\n",
      "##########\n",
      "epoch:15 step:12000 [D loss: 0.627016, acc.: 67.19%] [G loss: 0.671423]\n",
      "##############\n",
      "[0.85090979 0.87126987 0.81282574 0.80097891 0.77180688 0.82037882\n",
      " 0.88457475 0.80386848 0.80202959 0.8438671 ]\n",
      "##########\n",
      "epoch:15 step:12200 [D loss: 0.677857, acc.: 58.59%] [G loss: 0.696432]\n",
      "##############\n",
      "[0.85996656 0.85271802 0.81667709 0.81561596 0.80459519 0.8355032\n",
      " 0.87092675 0.83356403 0.79681817 0.82112658]\n",
      "##########\n",
      "epoch:15 step:12400 [D loss: 0.668876, acc.: 61.72%] [G loss: 0.952999]\n",
      "##############\n",
      "[0.84390737 0.86384539 0.79353778 0.81762962 0.78336006 0.82375681\n",
      " 0.87995783 0.81698411 0.81599082 0.8423392 ]\n",
      "##########\n",
      "epoch:16 step:12600 [D loss: 0.730202, acc.: 53.91%] [G loss: 0.764706]\n",
      "##############\n",
      "[0.86985711 0.83629901 0.80396339 0.8052161  0.80047922 0.79548592\n",
      " 0.89824032 0.83035354 0.80690778 0.84066702]\n",
      "##########\n",
      "epoch:16 step:12800 [D loss: 0.652122, acc.: 60.94%] [G loss: 0.842673]\n",
      "##############\n",
      "[0.84480707 0.84177596 0.80363181 0.80583965 0.79515968 0.82880672\n",
      " 0.87742854 0.84126979 0.82574828 0.8259449 ]\n",
      "##########\n",
      "epoch:16 step:13000 [D loss: 0.710277, acc.: 43.75%] [G loss: 0.787838]\n",
      "##############\n",
      "[0.84989461 0.84080158 0.79312003 0.8048305  0.7887332  0.82041346\n",
      " 0.88112552 0.80625602 0.82488378 0.83555615]\n",
      "##########\n",
      "epoch:16 step:13200 [D loss: 0.684140, acc.: 57.03%] [G loss: 0.847290]\n",
      "##############\n",
      "[0.86398042 0.83585181 0.79416431 0.78632928 0.80324387 0.81348044\n",
      " 0.91786046 0.81584483 0.82910531 0.82486467]\n",
      "##########\n",
      "epoch:17 step:13400 [D loss: 0.762757, acc.: 49.22%] [G loss: 0.555663]\n",
      "##############\n",
      "[0.85489359 0.8478245  0.79553604 0.80779048 0.79085359 0.82148408\n",
      " 0.85624872 0.80672022 0.82695215 0.84861581]\n",
      "##########\n",
      "epoch:17 step:13600 [D loss: 0.736012, acc.: 39.84%] [G loss: 0.686989]\n",
      "##############\n",
      "[0.83709323 0.86067217 0.81962497 0.79278965 0.79383721 0.82344884\n",
      " 0.89265331 0.83641556 0.82513344 0.84229179]\n",
      "##########\n",
      "epoch:17 step:13800 [D loss: 0.640407, acc.: 65.62%] [G loss: 0.791025]\n",
      "##############\n",
      "[0.83889809 0.85283824 0.79181918 0.81192041 0.78946568 0.81590925\n",
      " 0.88177247 0.80053754 0.82300718 0.82258888]\n",
      "##########\n",
      "epoch:17 step:14000 [D loss: 0.719056, acc.: 51.56%] [G loss: 0.797919]\n",
      "##############\n",
      "[0.85323265 0.84717044 0.79902819 0.79040427 0.79069404 0.82919216\n",
      " 0.88848515 0.82264519 0.81211358 0.85262326]\n",
      "##########\n",
      "epoch:18 step:14200 [D loss: 0.808046, acc.: 35.94%] [G loss: 0.725569]\n",
      "##############\n",
      "[0.8695507  0.8356919  0.80397111 0.80086312 0.80081506 0.81544222\n",
      " 0.88895395 0.79980991 0.83435027 0.82973943]\n",
      "##########\n",
      "epoch:18 step:14400 [D loss: 0.675129, acc.: 53.91%] [G loss: 0.751656]\n",
      "##############\n",
      "[0.85767661 0.87106578 0.79790797 0.79250695 0.79977209 0.82927363\n",
      " 0.90077632 0.81909406 0.8073911  0.82820535]\n",
      "##########\n",
      "epoch:18 step:14600 [D loss: 0.709423, acc.: 53.12%] [G loss: 0.800743]\n",
      "##############\n",
      "[0.86674705 0.83360046 0.78963969 0.81336025 0.77772442 0.80760857\n",
      " 0.87097402 0.83996899 0.80814595 0.82252715]\n",
      "##########\n",
      "epoch:18 step:14800 [D loss: 0.699810, acc.: 57.81%] [G loss: 0.669217]\n",
      "##############\n",
      "[0.83675858 0.85220308 0.79711068 0.8142942  0.79794369 0.82190546\n",
      " 0.88749052 0.8278265  0.82959667 0.83405522]\n",
      "##########\n",
      "epoch:19 step:15000 [D loss: 0.711682, acc.: 53.91%] [G loss: 0.726719]\n",
      "##############\n",
      "[0.85624972 0.85095428 0.78985585 0.81001395 0.79036432 0.82563674\n",
      " 0.86414764 0.82863338 0.82114814 0.80980563]\n",
      "##########\n",
      "epoch:19 step:15200 [D loss: 0.752028, acc.: 40.62%] [G loss: 0.772851]\n",
      "##############\n",
      "[0.85598339 0.8539626  0.79801626 0.80932402 0.80387919 0.81623748\n",
      " 0.88452449 0.80411582 0.82976616 0.83548139]\n",
      "##########\n",
      "epoch:19 step:15400 [D loss: 0.640684, acc.: 60.94%] [G loss: 0.969441]\n",
      "##############\n",
      "[0.87173786 0.84796074 0.80018521 0.78027969 0.82052105 0.83820014\n",
      " 0.89497637 0.81623957 0.80230644 0.80416819]\n",
      "##########\n",
      "epoch:19 step:15600 [D loss: 0.690054, acc.: 54.69%] [G loss: 0.897022]\n",
      "##############\n",
      "[0.85358849 0.85084532 0.80516933 0.81297106 0.78839031 0.81786468\n",
      " 0.89267269 0.83669824 0.82463127 0.80512223]\n",
      "##########\n",
      "epoch:20 step:15800 [D loss: 0.696769, acc.: 54.69%] [G loss: 0.816377]\n",
      "##############\n",
      "[0.86909476 0.84583956 0.79463398 0.80910706 0.78641435 0.81885745\n",
      " 0.91005736 0.82881464 0.83154003 0.81795908]\n",
      "##########\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:20 step:16000 [D loss: 0.666832, acc.: 64.06%] [G loss: 0.775941]\n",
      "##############\n",
      "[0.82311891 0.8513523  0.8016375  0.83083967 0.80556541 0.82719117\n",
      " 0.89165449 0.81507229 0.82304898 0.83243708]\n",
      "##########\n",
      "epoch:20 step:16200 [D loss: 0.708075, acc.: 50.78%] [G loss: 0.798870]\n",
      "##############\n",
      "[0.87518188 0.8336801  0.80005443 0.81533768 0.80039243 0.79992027\n",
      " 0.88112113 0.82862177 0.81722317 0.83163686]\n",
      "##########\n",
      "epoch:20 step:16400 [D loss: 0.660394, acc.: 60.94%] [G loss: 0.880207]\n",
      "##############\n",
      "[0.83807833 0.82861815 0.79340227 0.79495897 0.79413235 0.82149328\n",
      " 0.89998439 0.81586292 0.82945748 0.82088937]\n",
      "##########\n",
      "epoch:21 step:16600 [D loss: 0.651824, acc.: 59.38%] [G loss: 0.826308]\n",
      "##############\n",
      "[0.87350892 0.86791599 0.78433691 0.81730249 0.80817263 0.82058567\n",
      " 0.88279835 0.81651012 0.82667927 0.8222022 ]\n",
      "##########\n",
      "epoch:21 step:16800 [D loss: 0.682261, acc.: 59.38%] [G loss: 0.846219]\n",
      "##############\n",
      "[0.869607   0.86280561 0.8031876  0.80153062 0.82797122 0.80712309\n",
      " 0.87327145 0.83987746 0.81041237 0.81903308]\n",
      "##########\n",
      "epoch:21 step:17000 [D loss: 0.679775, acc.: 58.59%] [G loss: 0.727647]\n",
      "##############\n",
      "[0.85101686 0.84117724 0.79710333 0.82351106 0.80941984 0.80850705\n",
      " 0.89573545 0.83544989 0.8339542  0.82047825]\n",
      "##########\n",
      "epoch:22 step:17200 [D loss: 0.799909, acc.: 37.50%] [G loss: 0.701057]\n",
      "##############\n",
      "[0.85302016 0.86517915 0.81659358 0.80439117 0.78856019 0.83063962\n",
      " 0.88418914 0.81427504 0.84013462 0.84352298]\n",
      "##########\n",
      "epoch:22 step:17400 [D loss: 0.596823, acc.: 74.22%] [G loss: 0.708020]\n",
      "##############\n",
      "[0.86041105 0.86944152 0.81391472 0.78934339 0.78410753 0.82646839\n",
      " 0.87686595 0.82080888 0.82219349 0.83836466]\n",
      "##########\n",
      "epoch:22 step:17600 [D loss: 0.674068, acc.: 60.16%] [G loss: 0.662142]\n",
      "##############\n",
      "[0.85320843 0.84765585 0.78819822 0.80940304 0.79540841 0.80597424\n",
      " 0.87582205 0.83496646 0.8097811  0.84681659]\n",
      "##########\n",
      "epoch:22 step:17800 [D loss: 0.629361, acc.: 66.41%] [G loss: 0.665613]\n",
      "##############\n",
      "[0.86570964 0.8473619  0.82520863 0.80884568 0.80058967 0.8021036\n",
      " 0.88623948 0.84323296 0.80410705 0.82391674]\n",
      "##########\n",
      "epoch:23 step:18000 [D loss: 0.652804, acc.: 67.19%] [G loss: 0.679936]\n",
      "##############\n",
      "[0.84914502 0.85528524 0.79233358 0.82500249 0.78620366 0.8329845\n",
      " 0.8863531  0.80958431 0.82943656 0.83067289]\n",
      "##########\n",
      "epoch:23 step:18200 [D loss: 0.754712, acc.: 46.09%] [G loss: 0.721641]\n",
      "##############\n",
      "[0.86098604 0.85468742 0.81592561 0.80909962 0.77685911 0.81704624\n",
      " 0.88223132 0.82582951 0.8035782  0.83709578]\n",
      "##########\n",
      "epoch:23 step:18400 [D loss: 0.744871, acc.: 44.53%] [G loss: 0.765463]\n",
      "##############\n",
      "[0.85043925 0.84474562 0.80026548 0.80711383 0.8085237  0.82816886\n",
      " 0.8630584  0.81735573 0.79490695 0.83154108]\n",
      "##########\n",
      "epoch:23 step:18600 [D loss: 0.699373, acc.: 50.00%] [G loss: 0.722537]\n",
      "##############\n",
      "[0.83932331 0.83239295 0.80515555 0.81378654 0.79218666 0.80705311\n",
      " 0.85747674 0.82336853 0.82242592 0.8638529 ]\n",
      "##########\n",
      "epoch:24 step:18800 [D loss: 0.714801, acc.: 53.12%] [G loss: 0.768975]\n",
      "##############\n",
      "[0.85464147 0.85832115 0.79267505 0.80017112 0.80717165 0.82795298\n",
      " 0.87703501 0.83302097 0.8072715  0.81300946]\n",
      "##########\n",
      "epoch:24 step:19000 [D loss: 0.627181, acc.: 66.41%] [G loss: 0.694039]\n",
      "##############\n",
      "[0.85447357 0.86589617 0.80566543 0.7976717  0.79020102 0.81433098\n",
      " 0.89202936 0.8426941  0.78002139 0.81331847]\n",
      "##########\n",
      "epoch:24 step:19200 [D loss: 0.692822, acc.: 54.69%] [G loss: 0.759582]\n",
      "##############\n",
      "[0.87042117 0.85620337 0.79798854 0.80915977 0.7918677  0.82301391\n",
      " 0.89107893 0.81038223 0.82527147 0.82822374]\n",
      "##########\n",
      "epoch:24 step:19400 [D loss: 0.835255, acc.: 32.81%] [G loss: 0.717150]\n",
      "##############\n",
      "[0.84553689 0.85970075 0.81937521 0.79383332 0.79196286 0.82659534\n",
      " 0.87119829 0.83457105 0.84617801 0.83192915]\n",
      "##########\n",
      "epoch:25 step:19600 [D loss: 0.670599, acc.: 57.81%] [G loss: 0.859366]\n",
      "##############\n",
      "[0.87261561 0.8616007  0.80108353 0.80582537 0.78930753 0.8234892\n",
      " 0.87849715 0.80081566 0.83502666 0.86455215]\n",
      "##########\n",
      "epoch:25 step:19800 [D loss: 0.730804, acc.: 47.66%] [G loss: 0.708569]\n",
      "##############\n",
      "[0.86374231 0.85135941 0.79939099 0.79019425 0.80662144 0.81130128\n",
      " 0.89439811 0.80970056 0.80845109 0.8086203 ]\n",
      "##########\n",
      "epoch:25 step:20000 [D loss: 0.728006, acc.: 42.19%] [G loss: 0.837910]\n",
      "##############\n",
      "[0.84302646 0.85533043 0.80571831 0.80838748 0.79843357 0.80900718\n",
      " 0.89740755 0.8153724  0.8007427  0.81324808]\n",
      "##########\n",
      "epoch:25 step:20200 [D loss: 0.616095, acc.: 71.09%] [G loss: 0.803411]\n",
      "##############\n",
      "[0.8571823  0.84502368 0.795782   0.78677644 0.79309673 0.82886182\n",
      " 0.89088085 0.81668733 0.81404571 0.85116542]\n",
      "##########\n",
      "epoch:26 step:20400 [D loss: 0.624830, acc.: 71.09%] [G loss: 0.799221]\n",
      "##############\n",
      "[0.86845768 0.84477957 0.79001338 0.83138184 0.78925732 0.81902271\n",
      " 0.89066225 0.8009909  0.80614501 0.83331831]\n",
      "##########\n",
      "epoch:26 step:20600 [D loss: 0.692658, acc.: 53.91%] [G loss: 0.711203]\n",
      "##############\n",
      "[0.83180818 0.84321819 0.80940345 0.81681295 0.8014065  0.8099489\n",
      " 0.88812866 0.83589583 0.80133635 0.81883685]\n",
      "##########\n",
      "epoch:26 step:20800 [D loss: 0.756810, acc.: 39.84%] [G loss: 0.730484]\n",
      "##############\n",
      "[0.86695739 0.84503184 0.78882824 0.80910169 0.79959289 0.83394198\n",
      " 0.86991505 0.80913294 0.80629195 0.82854839]\n",
      "##########\n",
      "epoch:26 step:21000 [D loss: 0.757665, acc.: 46.88%] [G loss: 0.805565]\n",
      "##############\n",
      "[0.86441364 0.85278322 0.79333014 0.81187267 0.81760037 0.81307762\n",
      " 0.90005632 0.80884268 0.80633996 0.82979656]\n",
      "##########\n",
      "epoch:27 step:21200 [D loss: 0.759552, acc.: 37.50%] [G loss: 0.718407]\n",
      "##############\n",
      "[0.84148653 0.86917662 0.7988432  0.822612   0.78746751 0.82665492\n",
      " 0.89047116 0.81485628 0.8023611  0.81791892]\n",
      "##########\n",
      "epoch:27 step:21400 [D loss: 0.615307, acc.: 71.09%] [G loss: 0.708603]\n",
      "##############\n",
      "[0.84927439 0.83736438 0.81660782 0.82055417 0.79108878 0.80993201\n",
      " 0.87740283 0.82292608 0.83891533 0.84139243]\n",
      "##########\n",
      "epoch:27 step:21600 [D loss: 0.775035, acc.: 39.06%] [G loss: 0.685795]\n",
      "##############\n",
      "[0.86925405 0.88195139 0.80236696 0.81971184 0.78092849 0.81707733\n",
      " 0.86236288 0.81156346 0.82619199 0.82721486]\n",
      "##########\n",
      "epoch:27 step:21800 [D loss: 0.535166, acc.: 83.59%] [G loss: 0.709901]\n",
      "##############\n",
      "[0.85677797 0.85750023 0.79952842 0.80274282 0.78028993 0.82551295\n",
      " 0.91404048 0.80387418 0.82348799 0.84550788]\n",
      "##########\n",
      "epoch:28 step:22000 [D loss: 0.713026, acc.: 50.00%] [G loss: 0.859443]\n",
      "##############\n",
      "[0.86902295 0.83974029 0.79999864 0.82114161 0.80047329 0.77883544\n",
      " 0.86011031 0.83133638 0.81656135 0.85109866]\n",
      "##########\n",
      "epoch:28 step:22200 [D loss: 0.694281, acc.: 56.25%] [G loss: 0.810559]\n",
      "##############\n",
      "[0.86288804 0.85662046 0.79345739 0.81157171 0.77860416 0.8095911\n",
      " 0.88599732 0.82120853 0.80604229 0.83285215]\n",
      "##########\n",
      "epoch:28 step:22400 [D loss: 0.711360, acc.: 47.66%] [G loss: 0.777737]\n",
      "##############\n",
      "[0.86094218 0.84722129 0.81280421 0.81815029 0.77988643 0.82706076\n",
      " 0.90317684 0.79062689 0.80774554 0.83823599]\n",
      "##########\n",
      "epoch:28 step:22600 [D loss: 0.641094, acc.: 64.84%] [G loss: 0.709936]\n",
      "##############\n",
      "[0.84500986 0.85737253 0.81154436 0.81312108 0.78570935 0.83860909\n",
      " 0.87578699 0.81424541 0.81487267 0.84087102]\n",
      "##########\n",
      "epoch:29 step:22800 [D loss: 0.655202, acc.: 55.47%] [G loss: 0.867358]\n",
      "##############\n",
      "[0.85263839 0.84096211 0.80050263 0.80503772 0.81200151 0.82380535\n",
      " 0.86460677 0.81778101 0.84137708 0.81917914]\n",
      "##########\n",
      "epoch:29 step:23000 [D loss: 0.732676, acc.: 44.53%] [G loss: 0.855684]\n",
      "##############\n",
      "[0.84483565 0.86917323 0.78527581 0.79784092 0.79249165 0.81346818\n",
      " 0.86146969 0.81564248 0.83563887 0.82238622]\n",
      "##########\n",
      "epoch:29 step:23200 [D loss: 0.641682, acc.: 59.38%] [G loss: 0.857768]\n",
      "##############\n",
      "[0.85398466 0.85571966 0.79086234 0.79711439 0.77335304 0.81323651\n",
      " 0.8713309  0.81518684 0.82784607 0.82063873]\n",
      "##########\n",
      "epoch:29 step:23400 [D loss: 0.691913, acc.: 50.00%] [G loss: 0.949234]\n",
      "##############\n",
      "[0.86179726 0.85213498 0.79936895 0.81884516 0.76439914 0.83905118\n",
      " 0.88236045 0.82309714 0.82907152 0.82581328]\n",
      "##########\n",
      "epoch:30 step:23600 [D loss: 0.661441, acc.: 62.50%] [G loss: 0.938570]\n",
      "##############\n",
      "[0.86065197 0.83024863 0.76958285 0.81709703 0.7930395  0.82025253\n",
      " 0.87227055 0.80554475 0.83095787 0.84243883]\n",
      "##########\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:30 step:23800 [D loss: 0.658500, acc.: 60.94%] [G loss: 0.926948]\n",
      "##############\n",
      "[0.85240279 0.82823651 0.81235057 0.81196806 0.79006136 0.82097823\n",
      " 0.8682491  0.83857077 0.81548991 0.81675871]\n",
      "##########\n",
      "epoch:30 step:24000 [D loss: 0.694968, acc.: 51.56%] [G loss: 0.821235]\n",
      "##############\n",
      "[0.85364902 0.84839908 0.81604474 0.8007826  0.78773399 0.82606474\n",
      " 0.87172554 0.8205119  0.83432576 0.83314401]\n",
      "##########\n",
      "epoch:30 step:24200 [D loss: 0.643545, acc.: 60.94%] [G loss: 0.928423]\n",
      "##############\n",
      "[0.86264773 0.83391901 0.78821326 0.83238137 0.81825731 0.81416399\n",
      " 0.89050822 0.80214722 0.83933656 0.84195561]\n",
      "##########\n",
      "epoch:31 step:24400 [D loss: 0.641738, acc.: 63.28%] [G loss: 0.794345]\n",
      "##############\n",
      "[0.85065886 0.83089206 0.79785968 0.80198679 0.78315123 0.81196021\n",
      " 0.87323297 0.82381301 0.8286566  0.83867947]\n",
      "##########\n",
      "epoch:31 step:24600 [D loss: 0.666880, acc.: 59.38%] [G loss: 0.859963]\n",
      "##############\n",
      "[0.85515092 0.86422832 0.79193927 0.82581396 0.78338796 0.82619831\n",
      " 0.90396666 0.81294988 0.80033149 0.83702377]\n",
      "##########\n",
      "epoch:31 step:24800 [D loss: 0.723655, acc.: 51.56%] [G loss: 0.813194]\n",
      "##############\n",
      "[0.84438513 0.84601201 0.79349018 0.81511315 0.78266264 0.82330441\n",
      " 0.91263562 0.82548001 0.80929306 0.84819017]\n",
      "##########\n",
      "epoch:32 step:25000 [D loss: 0.690899, acc.: 53.12%] [G loss: 0.780310]\n",
      "##############\n",
      "[0.85443723 0.83646772 0.79427593 0.79348789 0.81863742 0.83437065\n",
      " 0.86717363 0.83079169 0.824127   0.84006479]\n",
      "##########\n",
      "epoch:32 step:25200 [D loss: 0.753286, acc.: 40.62%] [G loss: 0.666498]\n",
      "##############\n",
      "[0.84023526 0.83554133 0.80229817 0.80137578 0.77662271 0.82975982\n",
      " 0.88636258 0.81430783 0.82599715 0.83823124]\n",
      "##########\n",
      "epoch:32 step:25400 [D loss: 0.676727, acc.: 57.81%] [G loss: 0.675627]\n",
      "##############\n",
      "[0.85181249 0.8449609  0.77487683 0.82855752 0.79934984 0.81284399\n",
      " 0.88436381 0.8251159  0.79219278 0.83372671]\n",
      "##########\n",
      "epoch:32 step:25600 [D loss: 0.675189, acc.: 53.91%] [G loss: 0.727603]\n",
      "##############\n",
      "[0.84312056 0.86072832 0.79507577 0.80882725 0.79958597 0.83634967\n",
      " 0.87704787 0.79352456 0.82700675 0.84615287]\n",
      "##########\n",
      "epoch:33 step:25800 [D loss: 0.616084, acc.: 74.22%] [G loss: 0.693519]\n",
      "##############\n",
      "[0.84921658 0.85404037 0.78603097 0.80226275 0.79173357 0.84529492\n",
      " 0.88059295 0.80731778 0.80395279 0.83811199]\n",
      "##########\n",
      "epoch:33 step:26000 [D loss: 0.615369, acc.: 73.44%] [G loss: 0.876484]\n",
      "##############\n",
      "[0.8512084  0.8590446  0.80606159 0.81312964 0.79926003 0.83543369\n",
      " 0.88225608 0.79633454 0.79939362 0.84027472]\n",
      "##########\n",
      "epoch:33 step:26200 [D loss: 0.745801, acc.: 38.28%] [G loss: 0.873827]\n",
      "##############\n",
      "[0.86314875 0.85754536 0.80888056 0.80323919 0.79237875 0.83492071\n",
      " 0.90054462 0.82741387 0.81550127 0.82443194]\n",
      "##########\n",
      "epoch:33 step:26400 [D loss: 0.787185, acc.: 39.84%] [G loss: 0.842088]\n",
      "##############\n",
      "[0.85080629 0.84196692 0.78755945 0.81649602 0.80849609 0.82363361\n",
      " 0.8837077  0.82724354 0.82125585 0.83153881]\n",
      "##########\n",
      "epoch:34 step:26600 [D loss: 0.739166, acc.: 45.31%] [G loss: 0.900609]\n",
      "##############\n",
      "[0.8611424  0.85106395 0.79209575 0.80053604 0.80340295 0.81841551\n",
      " 0.8842862  0.8049193  0.80218352 0.84912894]\n",
      "##########\n",
      "epoch:34 step:26800 [D loss: 0.754914, acc.: 43.75%] [G loss: 0.824244]\n",
      "##############\n",
      "[0.85996719 0.86217765 0.7948643  0.79932235 0.78659927 0.82449308\n",
      " 0.90769253 0.82726079 0.82613667 0.8394208 ]\n",
      "##########\n",
      "epoch:34 step:27000 [D loss: 0.710822, acc.: 52.34%] [G loss: 0.739800]\n",
      "##############\n",
      "[0.84558378 0.86077249 0.79024206 0.81393012 0.81069393 0.83172325\n",
      " 0.87481683 0.81954407 0.82576229 0.83097452]\n",
      "##########\n",
      "epoch:34 step:27200 [D loss: 0.726013, acc.: 49.22%] [G loss: 0.838254]\n",
      "##############\n",
      "[0.8511479  0.8539555  0.80264236 0.81930137 0.79259478 0.82246651\n",
      " 0.88188786 0.79201131 0.80809005 0.82360317]\n",
      "##########\n",
      "epoch:35 step:27400 [D loss: 0.710062, acc.: 48.44%] [G loss: 0.810830]\n",
      "##############\n",
      "[0.86690626 0.86618718 0.78229051 0.80608691 0.79262155 0.81161435\n",
      " 0.90285063 0.83098876 0.81300816 0.83988463]\n",
      "##########\n",
      "epoch:35 step:27600 [D loss: 0.717498, acc.: 55.47%] [G loss: 0.822513]\n",
      "##############\n",
      "[0.85494936 0.86135238 0.79902752 0.8130438  0.79314832 0.81217779\n",
      " 0.87996234 0.8094084  0.80967543 0.83380779]\n",
      "##########\n",
      "epoch:35 step:27800 [D loss: 0.671378, acc.: 57.81%] [G loss: 0.828265]\n",
      "##############\n",
      "[0.85802251 0.85650346 0.81318183 0.81291936 0.79966954 0.81924899\n",
      " 0.87812167 0.80543836 0.83017762 0.82376494]\n",
      "##########\n",
      "epoch:35 step:28000 [D loss: 0.694978, acc.: 50.78%] [G loss: 0.906965]\n",
      "##############\n",
      "[0.85533225 0.85341225 0.80902509 0.79859293 0.77565543 0.83071529\n",
      " 0.9079741  0.80118638 0.83867649 0.83740825]\n",
      "##########\n",
      "epoch:36 step:28200 [D loss: 0.772344, acc.: 32.03%] [G loss: 0.822525]\n",
      "##############\n",
      "[0.83666671 0.84627083 0.79675975 0.81306752 0.81565332 0.83241476\n",
      " 0.87201269 0.82307188 0.80656806 0.84047817]\n",
      "##########\n",
      "epoch:36 step:28400 [D loss: 0.745021, acc.: 46.09%] [G loss: 0.744069]\n",
      "##############\n",
      "[0.85115444 0.85406805 0.78975519 0.79624122 0.80488521 0.81499265\n",
      " 0.89528105 0.81858177 0.78845042 0.81019203]\n",
      "##########\n",
      "epoch:36 step:28600 [D loss: 0.668271, acc.: 58.59%] [G loss: 0.765358]\n",
      "##############\n",
      "[0.84739042 0.83382887 0.80632814 0.80123007 0.79674017 0.80947338\n",
      " 0.88279549 0.8058959  0.81451323 0.83078425]\n",
      "##########\n",
      "epoch:36 step:28800 [D loss: 0.728228, acc.: 44.53%] [G loss: 0.772978]\n",
      "##############\n",
      "[0.84162149 0.87177801 0.80831556 0.80883281 0.79611558 0.82115724\n",
      " 0.90554791 0.81602868 0.82971374 0.85143303]\n",
      "##########\n",
      "epoch:37 step:29000 [D loss: 0.747756, acc.: 39.06%] [G loss: 0.789485]\n",
      "##############\n",
      "[0.85733097 0.86668557 0.79409388 0.80745913 0.81012695 0.78977761\n",
      " 0.89968665 0.82143326 0.82024459 0.81699318]\n",
      "##########\n",
      "epoch:37 step:29200 [D loss: 0.700167, acc.: 51.56%] [G loss: 0.856733]\n",
      "##############\n",
      "[0.87221479 0.86784808 0.79823064 0.80616351 0.8069549  0.82906271\n",
      " 0.88127079 0.83605185 0.82335991 0.82916756]\n",
      "##########\n",
      "epoch:37 step:29400 [D loss: 0.653765, acc.: 65.62%] [G loss: 0.841798]\n",
      "##############\n",
      "[0.85199983 0.85618413 0.78148781 0.80282062 0.80554883 0.80954303\n",
      " 0.8903381  0.83174399 0.81459782 0.83148347]\n",
      "##########\n",
      "epoch:37 step:29600 [D loss: 0.745266, acc.: 38.28%] [G loss: 0.731187]\n",
      "##############\n",
      "[0.85148418 0.83357159 0.7753888  0.80655132 0.77654198 0.80717132\n",
      " 0.87682032 0.8027553  0.83038548 0.83318257]\n",
      "##########\n",
      "epoch:38 step:29800 [D loss: 0.649618, acc.: 60.94%] [G loss: 0.768657]\n",
      "##############\n",
      "[0.84697231 0.84470851 0.79867273 0.79839058 0.79664743 0.78989489\n",
      " 0.87625136 0.79477487 0.81469115 0.81848669]\n",
      "##########\n",
      "epoch:38 step:30000 [D loss: 0.765083, acc.: 42.19%] [G loss: 0.744078]\n",
      "##############\n",
      "[0.86174185 0.85412902 0.80007475 0.80612396 0.79474265 0.81616328\n",
      " 0.8887937  0.82802865 0.80316105 0.81807207]\n",
      "##########\n",
      "epoch:38 step:30200 [D loss: 0.703183, acc.: 51.56%] [G loss: 0.740809]\n",
      "##############\n",
      "[0.84962729 0.86862311 0.78852274 0.8096927  0.78860191 0.81222951\n",
      " 0.89628732 0.81141163 0.80129713 0.85515474]\n",
      "##########\n",
      "epoch:38 step:30400 [D loss: 0.762541, acc.: 46.09%] [G loss: 0.738957]\n",
      "##############\n",
      "[0.85876632 0.82279798 0.81569324 0.79585505 0.76959867 0.80153315\n",
      " 0.90741879 0.81879163 0.81803865 0.8309864 ]\n",
      "##########\n",
      "epoch:39 step:30600 [D loss: 0.656231, acc.: 62.50%] [G loss: 0.697659]\n",
      "##############\n",
      "[0.86944589 0.86320873 0.7968999  0.82088938 0.78049407 0.80770348\n",
      " 0.88919324 0.82275131 0.83677167 0.82121161]\n",
      "##########\n",
      "epoch:39 step:30800 [D loss: 0.704082, acc.: 50.78%] [G loss: 0.857922]\n",
      "##############\n",
      "[0.82858617 0.87049508 0.80077312 0.7881242  0.8137032  0.82560118\n",
      " 0.89996467 0.8227406  0.81227499 0.82217087]\n",
      "##########\n",
      "epoch:39 step:31000 [D loss: 0.702360, acc.: 51.56%] [G loss: 0.797123]\n",
      "##############\n",
      "[0.85765682 0.86723769 0.80859905 0.80314877 0.78846278 0.81748093\n",
      " 0.89053378 0.82826169 0.81661759 0.82737358]\n",
      "##########\n",
      "epoch:39 step:31200 [D loss: 0.705161, acc.: 50.78%] [G loss: 0.819364]\n",
      "##############\n",
      "[0.85634506 0.83650349 0.77707272 0.81866063 0.80835494 0.81307228\n",
      " 0.88352636 0.82056553 0.83653964 0.82873051]\n",
      "##########\n",
      "epoch:40 step:31400 [D loss: 0.624615, acc.: 71.09%] [G loss: 0.871838]\n",
      "##############\n",
      "[0.86344873 0.84573205 0.81217524 0.82559488 0.80951414 0.81483684\n",
      " 0.8674869  0.82904368 0.82504199 0.82579566]\n",
      "##########\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:40 step:31600 [D loss: 0.728725, acc.: 47.66%] [G loss: 0.787801]\n",
      "##############\n",
      "[0.86400665 0.87009165 0.79778304 0.79553781 0.78949553 0.80673314\n",
      " 0.89560433 0.82603701 0.82431516 0.83703843]\n",
      "##########\n",
      "epoch:40 step:31800 [D loss: 0.662511, acc.: 60.16%] [G loss: 0.963825]\n",
      "##############\n",
      "[0.85023593 0.84610265 0.79824176 0.78549935 0.80833658 0.80504804\n",
      " 0.84993232 0.80907511 0.80126017 0.83399101]\n",
      "##########\n",
      "epoch:40 step:32000 [D loss: 0.659641, acc.: 68.75%] [G loss: 0.846484]\n",
      "##############\n",
      "[0.84605979 0.83713613 0.80937511 0.81483961 0.78427408 0.81107224\n",
      " 0.90684482 0.81305397 0.81754204 0.8412964 ]\n",
      "##########\n",
      "epoch:41 step:32200 [D loss: 0.625453, acc.: 67.97%] [G loss: 0.823433]\n",
      "##############\n",
      "[0.85309038 0.85098424 0.80639951 0.79847235 0.79433719 0.810828\n",
      " 0.88372539 0.82138863 0.81153799 0.83324594]\n",
      "##########\n",
      "epoch:41 step:32400 [D loss: 0.708280, acc.: 51.56%] [G loss: 0.807787]\n",
      "##############\n",
      "[0.84800138 0.85350544 0.78657738 0.81236574 0.78016227 0.810899\n",
      " 0.89215688 0.78996    0.81962718 0.82397326]\n",
      "##########\n",
      "epoch:41 step:32600 [D loss: 0.663135, acc.: 57.81%] [G loss: 0.868236]\n",
      "##############\n",
      "[0.8630598  0.83539869 0.80931313 0.81640487 0.80566007 0.82149715\n",
      " 0.88958498 0.83071387 0.81643817 0.83695803]\n",
      "##########\n",
      "epoch:41 step:32800 [D loss: 0.661040, acc.: 60.94%] [G loss: 0.747415]\n",
      "##############\n",
      "[0.84910169 0.85042947 0.79665729 0.79816812 0.79864478 0.80925041\n",
      " 0.89171011 0.85358022 0.82531389 0.84671652]\n",
      "##########\n",
      "epoch:42 step:33000 [D loss: 0.693072, acc.: 51.56%] [G loss: 0.794093]\n",
      "##############\n",
      "[0.85658963 0.84006244 0.80313834 0.80485914 0.81321034 0.80773097\n",
      " 0.89715263 0.79517662 0.80913368 0.8519989 ]\n",
      "##########\n",
      "epoch:42 step:33200 [D loss: 0.736048, acc.: 46.88%] [G loss: 0.777010]\n",
      "##############\n",
      "[0.8460081  0.84602369 0.79799514 0.8179052  0.80892686 0.81814548\n",
      " 0.90249833 0.81347985 0.82514695 0.81393957]\n",
      "##########\n",
      "epoch:42 step:33400 [D loss: 0.703231, acc.: 51.56%] [G loss: 0.806923]\n",
      "##############\n",
      "[0.85288732 0.85920259 0.81057862 0.82080116 0.78810265 0.83524063\n",
      " 0.87273956 0.80897199 0.79959654 0.82879694]\n",
      "##########\n",
      "epoch:43 step:33600 [D loss: 0.620884, acc.: 71.09%] [G loss: 0.801513]\n",
      "##############\n",
      "[0.84387936 0.85414594 0.80887377 0.79318859 0.79915698 0.83900906\n",
      " 0.87416529 0.81690816 0.81231071 0.82834711]\n",
      "##########\n",
      "epoch:43 step:33800 [D loss: 0.655081, acc.: 60.16%] [G loss: 0.738420]\n",
      "##############\n",
      "[0.84707789 0.85034039 0.79592762 0.80326335 0.8217395  0.823389\n",
      " 0.87166152 0.8154893  0.82607025 0.83698598]\n",
      "##########\n",
      "epoch:43 step:34000 [D loss: 0.676128, acc.: 53.91%] [G loss: 0.795354]\n",
      "##############\n",
      "[0.83390351 0.85703078 0.78984369 0.81542897 0.77463642 0.8182926\n",
      " 0.9083109  0.82291064 0.83909027 0.82395209]\n",
      "##########\n",
      "epoch:43 step:34200 [D loss: 0.753975, acc.: 39.84%] [G loss: 0.722387]\n",
      "##############\n",
      "[0.8385163  0.82549145 0.80924704 0.79435801 0.7876417  0.82699443\n",
      " 0.91294917 0.83755632 0.82283608 0.8406724 ]\n",
      "##########\n",
      "epoch:44 step:34400 [D loss: 0.623464, acc.: 67.97%] [G loss: 0.760321]\n",
      "##############\n",
      "[0.84384625 0.84871782 0.79408148 0.81240226 0.78383407 0.84082184\n",
      " 0.89755017 0.81750312 0.83948426 0.83741294]\n",
      "##########\n",
      "epoch:44 step:34600 [D loss: 0.749708, acc.: 42.97%] [G loss: 0.797099]\n",
      "##############\n",
      "[0.85661185 0.86460198 0.80230004 0.82426601 0.80817361 0.82592294\n",
      " 0.89888791 0.81345171 0.79856093 0.81818861]\n",
      "##########\n",
      "epoch:44 step:34800 [D loss: 0.768093, acc.: 39.84%] [G loss: 0.876023]\n",
      "##############\n",
      "[0.84943861 0.85821307 0.80219539 0.80814508 0.8040301  0.79047231\n",
      " 0.88472007 0.82511636 0.82282122 0.81990489]\n",
      "##########\n",
      "epoch:44 step:35000 [D loss: 0.711488, acc.: 48.44%] [G loss: 0.818027]\n",
      "##############\n",
      "[0.86691236 0.86412711 0.78591089 0.82476861 0.80033958 0.79643541\n",
      " 0.87698538 0.84785141 0.82492378 0.82061246]\n",
      "##########\n",
      "epoch:45 step:35200 [D loss: 0.726380, acc.: 47.66%] [G loss: 0.765863]\n",
      "##############\n",
      "[0.85392253 0.84456891 0.8167447  0.8048417  0.79623672 0.816524\n",
      " 0.86907513 0.82826298 0.81571031 0.82575113]\n",
      "##########\n",
      "epoch:45 step:35400 [D loss: 0.684427, acc.: 54.69%] [G loss: 0.777184]\n",
      "##############\n",
      "[0.84894079 0.85792618 0.78596898 0.80494997 0.793197   0.82400556\n",
      " 0.87404068 0.81915708 0.82242881 0.82124361]\n",
      "##########\n",
      "epoch:45 step:35600 [D loss: 0.685930, acc.: 56.25%] [G loss: 0.839007]\n",
      "##############\n",
      "[0.84898581 0.85093607 0.78435062 0.80559817 0.79936892 0.82635672\n",
      " 0.88089113 0.8203033  0.8342482  0.84596843]\n",
      "##########\n",
      "epoch:45 step:35800 [D loss: 0.624164, acc.: 76.56%] [G loss: 0.733377]\n",
      "##############\n",
      "[0.85963399 0.83635527 0.80704565 0.79360349 0.80331108 0.82257368\n",
      " 0.8655496  0.81970033 0.84628477 0.8238226 ]\n",
      "##########\n",
      "epoch:46 step:36000 [D loss: 0.617505, acc.: 69.53%] [G loss: 0.800120]\n",
      "##############\n",
      "[0.85497659 0.8568387  0.78361242 0.79403377 0.7849605  0.81592552\n",
      " 0.86547509 0.82571249 0.80894043 0.83617278]\n",
      "##########\n",
      "epoch:46 step:36200 [D loss: 0.671268, acc.: 56.25%] [G loss: 0.826365]\n",
      "##############\n",
      "[0.84570142 0.85187854 0.81679309 0.80767487 0.78965698 0.81642923\n",
      " 0.8833794  0.79054125 0.85328525 0.83841541]\n",
      "##########\n",
      "epoch:46 step:36400 [D loss: 0.722874, acc.: 50.00%] [G loss: 0.830914]\n",
      "##############\n",
      "[0.84436305 0.85480202 0.79879428 0.80422871 0.7888235  0.84350733\n",
      " 0.87684077 0.80981016 0.79780712 0.81923243]\n",
      "##########\n",
      "epoch:46 step:36600 [D loss: 0.638885, acc.: 64.06%] [G loss: 0.857932]\n",
      "##############\n",
      "[0.83813236 0.8332729  0.81282269 0.81289601 0.79642434 0.81813441\n",
      " 0.88950319 0.82411652 0.81159203 0.83179278]\n",
      "##########\n",
      "epoch:47 step:36800 [D loss: 0.656595, acc.: 64.06%] [G loss: 0.837739]\n",
      "##############\n",
      "[0.86969753 0.84297831 0.80953591 0.79032569 0.81693181 0.82209748\n",
      " 0.89100702 0.83578811 0.81141681 0.82692019]\n",
      "##########\n",
      "epoch:47 step:37000 [D loss: 0.729385, acc.: 48.44%] [G loss: 0.824837]\n",
      "##############\n",
      "[0.86414474 0.85471783 0.80472335 0.81541431 0.77185021 0.8038426\n",
      " 0.88732666 0.8058236  0.81216473 0.8245812 ]\n",
      "##########\n",
      "epoch:47 step:37200 [D loss: 0.662452, acc.: 63.28%] [G loss: 0.773137]\n",
      "##############\n",
      "[0.85417153 0.85358735 0.78250074 0.80260797 0.7823336  0.83286452\n",
      " 0.86787245 0.82113387 0.80958329 0.81753595]\n",
      "##########\n",
      "epoch:47 step:37400 [D loss: 0.655027, acc.: 62.50%] [G loss: 0.776726]\n",
      "##############\n",
      "[0.87613906 0.85149878 0.80092817 0.79662233 0.79220595 0.83984947\n",
      " 0.89698423 0.81853734 0.8104035  0.82187443]\n",
      "##########\n",
      "epoch:48 step:37600 [D loss: 0.626194, acc.: 68.75%] [G loss: 0.774947]\n",
      "##############\n",
      "[0.85752055 0.84831363 0.78959006 0.80587129 0.80131309 0.8196423\n",
      " 0.87885614 0.82902082 0.78617034 0.81096366]\n",
      "##########\n",
      "epoch:48 step:37800 [D loss: 0.730625, acc.: 42.19%] [G loss: 0.841338]\n",
      "##############\n",
      "[0.84182138 0.84537354 0.78791324 0.81629049 0.7791532  0.83215318\n",
      " 0.90499355 0.81630509 0.82323889 0.82674575]\n",
      "##########\n",
      "epoch:48 step:38000 [D loss: 0.723630, acc.: 46.88%] [G loss: 0.755288]\n",
      "##############\n",
      "[0.84321532 0.86568389 0.81616346 0.82045275 0.78925107 0.81142701\n",
      " 0.88457146 0.81866052 0.8394913  0.82772426]\n",
      "##########\n",
      "epoch:48 step:38200 [D loss: 0.667142, acc.: 57.81%] [G loss: 0.842206]\n",
      "##############\n",
      "[0.84447897 0.83957578 0.8091631  0.79183335 0.787877   0.80429726\n",
      " 0.88842088 0.8036602  0.80742128 0.82756611]\n",
      "##########\n",
      "epoch:49 step:38400 [D loss: 0.699148, acc.: 46.88%] [G loss: 0.818102]\n",
      "##############\n",
      "[0.84180893 0.84557043 0.79783548 0.81510257 0.79943083 0.80174112\n",
      " 0.87516614 0.81914023 0.81575241 0.84081633]\n",
      "##########\n",
      "epoch:49 step:38600 [D loss: 0.686208, acc.: 60.16%] [G loss: 0.885838]\n",
      "##############\n",
      "[0.86236509 0.85641367 0.79859416 0.80782245 0.79544696 0.82857998\n",
      " 0.87721625 0.81426874 0.82724531 0.82078958]\n",
      "##########\n",
      "epoch:49 step:38800 [D loss: 0.696841, acc.: 52.34%] [G loss: 0.805327]\n",
      "##############\n",
      "[0.85036748 0.8517637  0.79622713 0.80156673 0.75322393 0.81159657\n",
      " 0.88218737 0.79810131 0.80606996 0.82187974]\n",
      "##########\n",
      "epoch:49 step:39000 [D loss: 0.665262, acc.: 60.16%] [G loss: 0.827600]\n",
      "##############\n",
      "[0.82806208 0.84125638 0.8070929  0.80316671 0.78368766 0.81810078\n",
      " 0.90963878 0.82247182 0.79979046 0.82716953]\n",
      "##########\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.datasets import cifar10\n",
    "from torch.utils.data import Dataset\n",
    "import torch.utils.data as Data\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, imgs, transform=None):\n",
    "        # super().__init__()\n",
    "        self.imgs = imgs\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.imgs)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img = self.imgs[index]\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "        else:\n",
    "            img = torch.from_numpy(img)\n",
    "        img=img.reshape([3,32,32])\n",
    "        return img\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import model\n",
    "import torch.nn.functional as F\n",
    "model = model.cifar10(128)\n",
    "model.load_state_dict(torch.load('./log/default/best-85.pth'))\n",
    "model.cuda()\n",
    "def EuclideanDistances(A, B):\n",
    "    BT = B.transpose()\n",
    "    # vecProd = A * BT\n",
    "    vecProd = np.dot(A, BT)\n",
    "    # print(vecProd)\n",
    "    SqA = A ** 2\n",
    "    # print(SqA)\n",
    "    sumSqA = np.matrix(np.sum(SqA, axis=1))\n",
    "    sumSqAEx = np.tile(sumSqA.transpose(), (1, vecProd.shape[1]))\n",
    "    # print(sumSqAEx)\n",
    "\n",
    "    SqB = B ** 2\n",
    "    sumSqB = np.sum(SqB, axis=1)\n",
    "    sumSqBEx = np.tile(sumSqB, (vecProd.shape[0], 1))\n",
    "    SqED = sumSqBEx + sumSqAEx - 2 * vecProd\n",
    "    SqED[SqED < 0] = 0.0\n",
    "    ED = np.sqrt(SqED)\n",
    "    return np.divide(ED.sum(), ED.shape[0] * ED.shape[1])\n",
    "\n",
    "\n",
    "def cal_distance_image_real(images, labels):\n",
    "    x_dataset = MyDataset(images)\n",
    "    # print(x_dataset[0].shape)\n",
    "    x_real_loader = Data.DataLoader(dataset=x_dataset, batch_size=200, shuffle=True)\n",
    "    y_logits = []\n",
    "    for i, data in enumerate(x_real_loader):\n",
    "        # indx_target = target.clone()\n",
    "        data = data.cuda()\n",
    "        data = Variable(data, volatile=True)\n",
    "        output = model(data)\n",
    "        pred = F.softmax(output).cpu().detach().numpy()\n",
    "        y_logits += [i for i in pred]\n",
    "    dict = {}\n",
    "    all_dis = []\n",
    "    for i in range(10):\n",
    "        dict[i] = []\n",
    "    for i in range(len(labels)):\n",
    "        dict[labels[i]].append(y_logits[i])\n",
    "    for i in range(10):\n",
    "        dict[i] = np.array(dict[i])\n",
    "        if len(dict[i]):\n",
    "            dis = EuclideanDistances(dict[i], dict[i])  # 生成图片的紧度\n",
    "        else:\n",
    "            dis = -1\n",
    "        all_dis.append(dis)\n",
    "    return np.array(all_dis)\n",
    "\n",
    "\n",
    "def cal_distance_image_fake(images):\n",
    "    x_dataset = MyDataset(images)\n",
    "    # print(x_dataset[0].shape)\n",
    "    x_real_loader = Data.DataLoader(dataset=x_dataset, batch_size=200, shuffle=True)\n",
    "    y_logits = []\n",
    "    labels=[]\n",
    "    for i, data in enumerate(x_real_loader):\n",
    "        # indx_target = target.clone()\n",
    "        data = data.cuda()\n",
    "        data = Variable(data, volatile=True)\n",
    "        output = model(data)\n",
    "        pred = output.data.max(1)[1]\n",
    "        labels += [i for i in pred.cpu().numpy()]\n",
    "        pred = F.softmax(output).cpu().detach().numpy()\n",
    "        y_logits += [i for i in pred]\n",
    "\n",
    "    dict = {}\n",
    "    all_dis = []\n",
    "    for i in range(10):\n",
    "        dict[i] = []\n",
    "    for i in range(len(labels)):\n",
    "        dict[labels[i]].append(y_logits[i])\n",
    "    for i in range(10):\n",
    "        dict[i] = np.array(dict[i])\n",
    "        if len(dict[i]):\n",
    "            dis = EuclideanDistances(dict[i], dict[i])  # 生成图片的紧度\n",
    "        else:\n",
    "            dis = -1\n",
    "        all_dis.append(dis)\n",
    "    return np.array(all_dis)\n",
    "\n",
    "import os\n",
    "if not os.path.isdir('saved_models_{}'.format('dcgan')):\n",
    "    os.mkdir('saved_models_{}'.format('dcgan'))\n",
    "f = open('saved_models_{}/log_collapse1.txt'.format('dcgan'), mode='w')\n",
    "import torch.utils.data as Data\n",
    "import cv2\n",
    "\n",
    "from keras.datasets import cifar10\n",
    "from keras.layers import Input, Dense, Reshape, Flatten, Dropout\n",
    "from keras.layers import BatchNormalization, Activation, ZeroPadding2D\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.layers.convolutional import UpSampling2D, Conv2D\n",
    "from keras.models import Sequential, Model\n",
    "from keras.optimizers import Adam\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "class DCGAN():\n",
    "    def __init__(self):\n",
    "        # Input shape\n",
    "        self.img_rows = 32\n",
    "        self.img_cols = 32\n",
    "        self.channels = 3\n",
    "        self.img_shape = (self.img_rows, self.img_cols, self.channels)\n",
    "        self.latent_dim = 100\n",
    "        self.x = []\n",
    "        self.y = np.zeros((31, 1), dtype=np.int)\n",
    "        self.y = list(self.y)\n",
    "        for i in range(31):\n",
    "            self.y[i] = []\n",
    "\n",
    "        optimizer = Adam(0.0002, 0.5)\n",
    "\n",
    "        # Build and compile the discriminator\n",
    "        self.discriminator = self.build_discriminator()\n",
    "        self.discriminator.compile(loss='binary_crossentropy',\n",
    "            optimizer=optimizer,\n",
    "            metrics=['accuracy'])\n",
    "\n",
    "        # Build the generator\n",
    "        self.generator = self.build_generator()\n",
    "\n",
    "        # The generator takes noise as input and generates imgs\n",
    "        z = Input(shape=(self.latent_dim,))\n",
    "        img = self.generator(z)\n",
    "\n",
    "        # For the combined model we will only train the generator\n",
    "        self.discriminator.trainable = False\n",
    "\n",
    "        # The discriminator takes generated images as input and determines validity\n",
    "        valid = self.discriminator(img)\n",
    "\n",
    "        # The combined model  (stacked generator and discriminator)\n",
    "        # Trains the generator to fool the discriminator\n",
    "        self.combined = Model(z, valid)\n",
    "        self.combined.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
    "\n",
    "    def build_generator(self):\n",
    "\n",
    "        model = Sequential()\n",
    "\n",
    "        model.add(Dense(128 * 8 * 8, activation=\"relu\", input_dim=self.latent_dim))\n",
    "        model.add(Reshape((8, 8, 128)))\n",
    "        model.add(UpSampling2D())\n",
    "        model.add(Conv2D(128, kernel_size=3, padding=\"same\"))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(Activation(\"relu\"))\n",
    "        model.add(UpSampling2D())\n",
    "        model.add(Conv2D(64, kernel_size=3, padding=\"same\"))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(Activation(\"relu\"))\n",
    "        model.add(Conv2D(self.channels, kernel_size=3, padding=\"same\"))\n",
    "        model.add(Activation(\"tanh\"))\n",
    "\n",
    "        model.summary()\n",
    "\n",
    "        noise = Input(shape=(self.latent_dim,))\n",
    "        img = model(noise)\n",
    "\n",
    "        return Model(noise, img)\n",
    "\n",
    "    def build_discriminator(self):\n",
    "\n",
    "        model = Sequential()\n",
    "\n",
    "        model.add(Conv2D(32, kernel_size=3, strides=2, input_shape=self.img_shape, padding=\"same\"))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dropout(0.25))\n",
    "        model.add(Conv2D(64, kernel_size=3, strides=2, padding=\"same\"))\n",
    "        # model.add(ZeroPadding2D(padding=((0,1),(0,1))))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dropout(0.25))\n",
    "        model.add(Conv2D(128, kernel_size=3, strides=2, padding=\"same\"))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dropout(0.25))\n",
    "        model.add(Conv2D(256, kernel_size=3, strides=1, padding=\"same\"))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dropout(0.25))\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "        model.summary()\n",
    "\n",
    "        img = Input(shape=self.img_shape)\n",
    "        validity = model(img)\n",
    "\n",
    "        return Model(img, validity)\n",
    "\n",
    "    def train(self, epochs, batch_size=128, save_interval=50):\n",
    "\n",
    "        # Load the dataset\n",
    "        (X_train, _), (X_test, y_test) = cifar10.load_data()\n",
    "\n",
    "        # Rescale -1 to 1\n",
    "        X_train = (X_train.astype(np.float32) - 127.5) / 127.5\n",
    "        X_test = (X_test.astype(np.float32) - 127.5) / 127.5\n",
    "\n",
    "        # Adversarial ground truths\n",
    "        valid = np.ones((batch_size, 1))\n",
    "        fake = np.zeros((batch_size, 1))\n",
    "\n",
    "        nb_batches = int(X_train.shape[0] / batch_size)\n",
    "        global_step = 0\n",
    "        steps = []\n",
    "        values = []\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "\n",
    "            for index in range(nb_batches):\n",
    "                global_step += 1\n",
    "                imgs = X_train[index * batch_size:(index + 1) * batch_size]\n",
    "                # labels = y_train[index * batch_size:(index + 1) * batch_size]\n",
    "                # Sample noise and generate a batch of new images\n",
    "                noise = np.random.normal(0, 1, (batch_size, self.latent_dim))\n",
    "                gen_imgs = self.generator.predict(noise)\n",
    "\n",
    "                # Train the discriminator (real classified as ones and generated as zeros)\n",
    "                d_loss_real = self.discriminator.train_on_batch(imgs, valid)\n",
    "                d_loss_fake = self.discriminator.train_on_batch(gen_imgs, fake)\n",
    "                d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "\n",
    "                # ---------------------\n",
    "                #  Train Generator\n",
    "                # ---------------------\n",
    "\n",
    "                # Train the generator (wants discriminator to mistake images as real)\n",
    "                g_loss = self.combined.train_on_batch(noise, valid)\n",
    "                sample_num=5000\n",
    "\n",
    "                if global_step % save_interval == 0:\n",
    "                    print(\"epoch:%d step:%d [D loss: %f, acc.: %.2f%%] [G loss: %f]\" % (epoch,global_step, d_loss[0], 100 * d_loss[1], g_loss))\n",
    "\n",
    "                    self.mode_drop(epoch,X_test,y_test,sample_num, global_step)\n",
    "\n",
    "\n",
    "    def mode_drop(self, epoch,x_test,y_test,sample_num, global_step):\n",
    "        r, c = 10, 1000\n",
    "        noise = np.random.normal(0, 1, (r * c, 100))\n",
    "        # sampled_labels = np.array([num for _ in range(r) for num in range(c)])\n",
    "        gen_imgs = self.generator.predict([noise])\n",
    "        labels = np.squeeze(y_test[:sample_num])\n",
    "        labels = np.squeeze(labels)\n",
    "        dis_real = cal_distance_image_real(x_test[:sample_num], labels)\n",
    "        dis_fake = cal_distance_image_fake(gen_imgs)\n",
    "        dis_cha = dis_real - dis_fake\n",
    "        \n",
    "        print('##############')\n",
    "        # print(dis_real)\n",
    "        # print(dis_fake)\n",
    "        print(dis_cha)\n",
    "        print('##########')\n",
    "        f.writelines('\\n')\n",
    "        f.writelines('step:' + str(global_step))\n",
    "        f.writelines('\\n')\n",
    "        f.writelines('紧度')\n",
    "        f.writelines('\\n')\n",
    "        f.writelines(' %.8f ' % (i) for i in dis_cha)\n",
    "        f.writelines('\\n')\n",
    "if __name__ == '__main__':\n",
    "    dcgan = DCGAN()\n",
    "    dcgan.train(epochs=50, batch_size=64, save_interval=200)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pppppppp [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
