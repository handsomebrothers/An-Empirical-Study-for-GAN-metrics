{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-*-coding:utf-8-*-\n",
    "import util\n",
    "import tensorflow as tf\n",
    "MNIST_CLASSIFIER_FROZEN_GRAPH = './classify_mnist_graph_def.pb'\n",
    "INPUT_TENSOR = 'inputs:0'\n",
    "OUTPUT_TENSOR = 'logits:0'\n",
    "def EuclideanDistances(A, B):\n",
    "\n",
    "    BT = B.transpose()\n",
    "    # vecProd = A * BT\n",
    "    vecProd = np.dot(A, BT)\n",
    "    # print(vecProd)\n",
    "    SqA = A ** 2\n",
    "    # print(SqA)\n",
    "    sumSqA = np.matrix(np.sum(SqA, axis=1))\n",
    "    sumSqAEx = np.tile(sumSqA.transpose(), (1, vecProd.shape[1]))\n",
    "    # print(sumSqAEx)\n",
    "\n",
    "    SqB = B ** 2\n",
    "    sumSqB = np.sum(SqB, axis=1)\n",
    "    sumSqBEx = np.tile(sumSqB, (vecProd.shape[0], 1))\n",
    "    SqED = sumSqBEx + sumSqAEx - 2 * vecProd\n",
    "    SqED[SqED < 0] = 0.0\n",
    "    ED = np.sqrt(SqED)\n",
    "    return np.divide(ED.sum(),ED.shape[0]*ED.shape[1])\n",
    "def cal_distance_image_real(images,labels):\n",
    "    eval_images=tf.convert_to_tensor(images)\n",
    "    y_logits=util.mnist_logits(eval_images, MNIST_CLASSIFIER_FROZEN_GRAPH, INPUT_TENSOR, OUTPUT_TENSOR)\n",
    "    y_logits=tf.Session().run(y_logits)\n",
    "    dict={}\n",
    "    all_dis=[]\n",
    "    for i in range(10):\n",
    "        dict[i]=[]\n",
    "    for i in range(len(labels)):\n",
    "        dict[labels[i]].append(y_logits[i])\n",
    "    for i in range(10):\n",
    "        dict[i]=np.array(dict[i])\n",
    "        if len(dict[i]):\n",
    "            dis = EuclideanDistances(dict[i], dict[i])  # 生成图片的紧度\n",
    "        else:\n",
    "            dis = -1\n",
    "        all_dis.append(dis)\n",
    "    return np.array(all_dis)\n",
    "def cal_distance_image_fake(images):\n",
    "    eval_images=tf.convert_to_tensor(images)\n",
    "    y_logits=util.mnist_logits(eval_images, MNIST_CLASSIFIER_FROZEN_GRAPH, INPUT_TENSOR, OUTPUT_TENSOR)\n",
    "    y_logits=tf.Session().run(y_logits)\n",
    "    labels = tf.Session().run(tf.argmax(y_logits, 1))\n",
    "    dict={}\n",
    "    all_dis=[]\n",
    "    for i in range(10):\n",
    "        dict[i]=[]\n",
    "    for i in range(len(labels)):\n",
    "        dict[labels[i]].append(y_logits[i])\n",
    "    for i in range(10):\n",
    "        dict[i]=np.array(dict[i])\n",
    "        if len(dict[i]):\n",
    "            dis = EuclideanDistances(dict[i], dict[i])  # 生成图片的紧度\n",
    "        else:\n",
    "            dis = -1\n",
    "        all_dis.append(dis)\n",
    "    return np.array(all_dis)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_5 (Dense)              (None, 512)               51712     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 784)               402192    \n",
      "_________________________________________________________________\n",
      "reshape_1 (Reshape)          (None, 28, 28, 1)         0         \n",
      "=================================================================\n",
      "Total params: 720,656\n",
      "Trainable params: 718,608\n",
      "Non-trainable params: 2,048\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_2 (Flatten)          (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 512)               401920    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_6 (LeakyReLU)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_7 (LeakyReLU)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 100)               51300     \n",
      "=================================================================\n",
      "Total params: 719,972\n",
      "Trainable params: 717,924\n",
      "Non-trainable params: 2,048\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/imi432_006/anaconda3/envs/tf/lib/python3.5/site-packages/keras/engine/training.py:490: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0 step:1 [D loss: 0.867266, acc: 41.41%] [G loss: 4.966393]\n",
      "epoch:0 step:2 [D loss: 0.424162, acc: 74.22%] [G loss: 5.270675]\n",
      "epoch:0 step:3 [D loss: 0.170252, acc: 98.44%] [G loss: 5.608031]\n",
      "epoch:0 step:4 [D loss: 0.131281, acc: 98.44%] [G loss: 7.154961]\n",
      "epoch:0 step:5 [D loss: 0.072370, acc: 99.22%] [G loss: 9.180442]\n",
      "epoch:0 step:6 [D loss: 0.042065, acc: 100.00%] [G loss: 9.376972]\n",
      "epoch:0 step:7 [D loss: 0.035580, acc: 100.00%] [G loss: 10.704073]\n",
      "epoch:0 step:8 [D loss: 0.019201, acc: 100.00%] [G loss: 10.631642]\n",
      "epoch:0 step:9 [D loss: 0.018400, acc: 100.00%] [G loss: 11.233311]\n",
      "epoch:0 step:10 [D loss: 0.014424, acc: 100.00%] [G loss: 11.991962]\n",
      "epoch:0 step:11 [D loss: 0.014868, acc: 100.00%] [G loss: 12.082867]\n",
      "epoch:0 step:12 [D loss: 0.008434, acc: 100.00%] [G loss: 12.335401]\n",
      "epoch:0 step:13 [D loss: 0.011807, acc: 100.00%] [G loss: 13.163174]\n",
      "epoch:0 step:14 [D loss: 0.012628, acc: 100.00%] [G loss: 12.675383]\n",
      "epoch:0 step:15 [D loss: 0.007499, acc: 100.00%] [G loss: 13.458523]\n",
      "epoch:0 step:16 [D loss: 0.008286, acc: 100.00%] [G loss: 13.449549]\n",
      "epoch:0 step:17 [D loss: 0.006086, acc: 100.00%] [G loss: 14.454098]\n",
      "epoch:0 step:18 [D loss: 0.005915, acc: 100.00%] [G loss: 13.546961]\n",
      "epoch:0 step:19 [D loss: 0.018642, acc: 100.00%] [G loss: 14.929277]\n",
      "epoch:0 step:20 [D loss: 0.007968, acc: 100.00%] [G loss: 16.051256]\n",
      "epoch:0 step:21 [D loss: 0.010224, acc: 100.00%] [G loss: 16.066166]\n",
      "epoch:0 step:22 [D loss: 0.004960, acc: 100.00%] [G loss: 15.272480]\n",
      "epoch:0 step:23 [D loss: 0.018333, acc: 99.22%] [G loss: 17.402536]\n",
      "epoch:0 step:24 [D loss: 0.016319, acc: 99.22%] [G loss: 18.579857]\n",
      "epoch:0 step:25 [D loss: 0.008964, acc: 100.00%] [G loss: 18.399918]\n",
      "epoch:0 step:26 [D loss: 0.008476, acc: 100.00%] [G loss: 18.461826]\n",
      "epoch:0 step:27 [D loss: 0.017060, acc: 99.22%] [G loss: 19.257568]\n",
      "epoch:0 step:28 [D loss: 0.003921, acc: 100.00%] [G loss: 19.289459]\n",
      "epoch:0 step:29 [D loss: 0.048633, acc: 99.22%] [G loss: 20.095686]\n",
      "epoch:0 step:30 [D loss: 0.031113, acc: 99.22%] [G loss: 20.476719]\n",
      "epoch:0 step:31 [D loss: 0.012318, acc: 100.00%] [G loss: 21.053726]\n",
      "epoch:0 step:32 [D loss: 0.007631, acc: 100.00%] [G loss: 20.744099]\n",
      "epoch:0 step:33 [D loss: 0.005062, acc: 100.00%] [G loss: 21.205807]\n",
      "epoch:0 step:34 [D loss: 0.005463, acc: 100.00%] [G loss: 20.591606]\n",
      "epoch:0 step:35 [D loss: 0.007130, acc: 100.00%] [G loss: 20.918793]\n",
      "epoch:0 step:36 [D loss: 0.008397, acc: 100.00%] [G loss: 20.993137]\n",
      "epoch:0 step:37 [D loss: 0.004156, acc: 100.00%] [G loss: 20.517714]\n",
      "epoch:0 step:38 [D loss: 0.004017, acc: 100.00%] [G loss: 20.501064]\n",
      "epoch:0 step:39 [D loss: 0.017654, acc: 99.22%] [G loss: 21.601753]\n",
      "epoch:0 step:40 [D loss: 0.059923, acc: 99.22%] [G loss: 22.100918]\n",
      "epoch:0 step:41 [D loss: 0.008735, acc: 100.00%] [G loss: 21.717598]\n",
      "epoch:0 step:42 [D loss: 0.009584, acc: 100.00%] [G loss: 21.779524]\n",
      "epoch:0 step:43 [D loss: 0.007743, acc: 100.00%] [G loss: 22.236752]\n",
      "epoch:0 step:44 [D loss: 0.009852, acc: 100.00%] [G loss: 21.860983]\n",
      "epoch:0 step:45 [D loss: 0.004125, acc: 100.00%] [G loss: 21.461588]\n",
      "epoch:0 step:46 [D loss: 0.148293, acc: 98.44%] [G loss: 21.994547]\n",
      "epoch:0 step:47 [D loss: 0.009415, acc: 100.00%] [G loss: 22.251551]\n",
      "epoch:0 step:48 [D loss: 0.018841, acc: 99.22%] [G loss: 22.765171]\n",
      "epoch:0 step:49 [D loss: 0.055350, acc: 98.44%] [G loss: 21.865730]\n",
      "epoch:0 step:50 [D loss: 0.057538, acc: 97.66%] [G loss: 22.067724]\n",
      "epoch:0 step:51 [D loss: 0.021316, acc: 99.22%] [G loss: 22.729134]\n",
      "epoch:0 step:52 [D loss: 0.018799, acc: 99.22%] [G loss: 23.728172]\n",
      "epoch:0 step:53 [D loss: 0.026201, acc: 98.44%] [G loss: 23.144474]\n",
      "epoch:0 step:54 [D loss: 0.048057, acc: 97.66%] [G loss: 23.946178]\n",
      "epoch:0 step:55 [D loss: 0.004855, acc: 100.00%] [G loss: 23.789135]\n",
      "epoch:0 step:56 [D loss: 0.009670, acc: 99.22%] [G loss: 24.007381]\n",
      "epoch:0 step:57 [D loss: 0.055020, acc: 99.22%] [G loss: 23.440250]\n",
      "epoch:0 step:58 [D loss: 0.014386, acc: 100.00%] [G loss: 23.285391]\n",
      "epoch:0 step:59 [D loss: 0.038521, acc: 99.22%] [G loss: 23.463751]\n",
      "epoch:0 step:60 [D loss: 0.025228, acc: 99.22%] [G loss: 23.586813]\n",
      "epoch:0 step:61 [D loss: 0.039200, acc: 98.44%] [G loss: 23.913975]\n",
      "epoch:0 step:62 [D loss: 0.041441, acc: 98.44%] [G loss: 23.906067]\n",
      "epoch:0 step:63 [D loss: 0.036059, acc: 98.44%] [G loss: 24.080315]\n",
      "epoch:0 step:64 [D loss: 0.718673, acc: 88.28%] [G loss: 22.491285]\n",
      "epoch:0 step:65 [D loss: 0.626042, acc: 81.25%] [G loss: 23.362869]\n",
      "epoch:0 step:66 [D loss: 0.222713, acc: 92.97%] [G loss: 24.034292]\n",
      "epoch:0 step:67 [D loss: 0.168545, acc: 93.75%] [G loss: 25.315014]\n",
      "epoch:0 step:68 [D loss: 0.167267, acc: 93.75%] [G loss: 19.396915]\n",
      "epoch:0 step:69 [D loss: 0.576633, acc: 85.94%] [G loss: 24.374857]\n",
      "epoch:0 step:70 [D loss: 0.475416, acc: 88.28%] [G loss: 23.492224]\n",
      "epoch:0 step:71 [D loss: 0.318995, acc: 89.84%] [G loss: 21.947025]\n",
      "epoch:0 step:72 [D loss: 0.511475, acc: 87.50%] [G loss: 19.506731]\n",
      "epoch:0 step:73 [D loss: 0.469546, acc: 91.41%] [G loss: 21.456459]\n",
      "epoch:0 step:74 [D loss: 0.568736, acc: 86.72%] [G loss: 20.025461]\n",
      "epoch:0 step:75 [D loss: 0.196345, acc: 91.41%] [G loss: 18.296997]\n",
      "epoch:0 step:76 [D loss: 0.367076, acc: 89.84%] [G loss: 17.213894]\n",
      "epoch:0 step:77 [D loss: 0.396521, acc: 86.72%] [G loss: 18.384129]\n",
      "epoch:0 step:78 [D loss: 0.403287, acc: 85.16%] [G loss: 19.742020]\n",
      "epoch:0 step:79 [D loss: 0.199201, acc: 92.19%] [G loss: 13.006529]\n",
      "epoch:0 step:80 [D loss: 1.883012, acc: 58.59%] [G loss: 23.430490]\n",
      "epoch:0 step:81 [D loss: 1.632737, acc: 77.34%] [G loss: 22.956520]\n",
      "epoch:0 step:82 [D loss: 0.930502, acc: 79.69%] [G loss: 18.117617]\n",
      "epoch:0 step:83 [D loss: 0.754910, acc: 78.91%] [G loss: 17.764240]\n",
      "epoch:0 step:84 [D loss: 0.211697, acc: 88.28%] [G loss: 13.604068]\n",
      "epoch:0 step:85 [D loss: 0.436071, acc: 85.94%] [G loss: 15.384774]\n",
      "epoch:0 step:86 [D loss: 0.063251, acc: 97.66%] [G loss: 14.804661]\n",
      "epoch:0 step:87 [D loss: 0.210722, acc: 88.28%] [G loss: 15.043455]\n",
      "epoch:0 step:88 [D loss: 0.165834, acc: 92.97%] [G loss: 13.248699]\n",
      "epoch:0 step:89 [D loss: 0.333534, acc: 83.59%] [G loss: 12.267948]\n",
      "epoch:0 step:90 [D loss: 0.190146, acc: 90.62%] [G loss: 12.704855]\n",
      "epoch:0 step:91 [D loss: 0.483191, acc: 75.78%] [G loss: 13.612396]\n",
      "epoch:0 step:92 [D loss: 0.259220, acc: 87.50%] [G loss: 10.952082]\n",
      "epoch:0 step:93 [D loss: 0.409447, acc: 81.25%] [G loss: 14.824749]\n",
      "epoch:0 step:94 [D loss: 0.468656, acc: 78.91%] [G loss: 11.621058]\n",
      "epoch:0 step:95 [D loss: 0.279564, acc: 85.94%] [G loss: 13.215433]\n",
      "epoch:0 step:96 [D loss: 0.311327, acc: 83.59%] [G loss: 10.296854]\n",
      "epoch:0 step:97 [D loss: 0.429055, acc: 81.25%] [G loss: 10.895844]\n",
      "epoch:0 step:98 [D loss: 0.126940, acc: 96.09%] [G loss: 11.022320]\n",
      "epoch:0 step:99 [D loss: 0.256848, acc: 85.16%] [G loss: 10.300731]\n",
      "epoch:0 step:100 [D loss: 0.241098, acc: 87.50%] [G loss: 10.440868]\n",
      "epoch:0 step:101 [D loss: 0.236150, acc: 88.28%] [G loss: 10.755913]\n",
      "epoch:0 step:102 [D loss: 0.316204, acc: 82.03%] [G loss: 10.419169]\n",
      "epoch:0 step:103 [D loss: 0.252383, acc: 87.50%] [G loss: 10.407714]\n",
      "epoch:0 step:104 [D loss: 0.361658, acc: 80.47%] [G loss: 10.427528]\n",
      "epoch:0 step:105 [D loss: 0.421898, acc: 78.91%] [G loss: 9.703854]\n",
      "epoch:0 step:106 [D loss: 0.379821, acc: 81.25%] [G loss: 9.873935]\n",
      "epoch:0 step:107 [D loss: 0.236447, acc: 88.28%] [G loss: 8.972054]\n",
      "epoch:0 step:108 [D loss: 0.587945, acc: 75.78%] [G loss: 12.497721]\n",
      "epoch:0 step:109 [D loss: 0.496880, acc: 75.00%] [G loss: 9.554220]\n",
      "epoch:0 step:110 [D loss: 0.244424, acc: 88.28%] [G loss: 8.916498]\n",
      "epoch:0 step:111 [D loss: 0.475363, acc: 76.56%] [G loss: 10.500317]\n",
      "epoch:0 step:112 [D loss: 0.238601, acc: 86.72%] [G loss: 9.015745]\n",
      "epoch:0 step:113 [D loss: 0.247354, acc: 84.38%] [G loss: 9.665363]\n",
      "epoch:0 step:114 [D loss: 0.271568, acc: 85.94%] [G loss: 9.691507]\n",
      "epoch:0 step:115 [D loss: 0.145068, acc: 93.75%] [G loss: 10.333932]\n",
      "epoch:0 step:116 [D loss: 0.243079, acc: 87.50%] [G loss: 9.844389]\n",
      "epoch:0 step:117 [D loss: 0.233969, acc: 89.06%] [G loss: 8.965940]\n",
      "epoch:0 step:118 [D loss: 0.326606, acc: 82.03%] [G loss: 10.732052]\n",
      "epoch:0 step:119 [D loss: 0.261299, acc: 87.50%] [G loss: 10.188917]\n",
      "epoch:0 step:120 [D loss: 0.204878, acc: 90.62%] [G loss: 9.556033]\n",
      "epoch:0 step:121 [D loss: 0.255408, acc: 85.94%] [G loss: 8.815687]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0 step:122 [D loss: 0.380156, acc: 79.69%] [G loss: 9.781399]\n",
      "epoch:0 step:123 [D loss: 0.238576, acc: 87.50%] [G loss: 9.530716]\n",
      "epoch:0 step:124 [D loss: 0.251878, acc: 89.06%] [G loss: 9.489276]\n",
      "epoch:0 step:125 [D loss: 0.226107, acc: 87.50%] [G loss: 8.816999]\n",
      "epoch:0 step:126 [D loss: 0.263031, acc: 89.84%] [G loss: 9.181766]\n",
      "epoch:0 step:127 [D loss: 0.247813, acc: 88.28%] [G loss: 9.568509]\n",
      "epoch:0 step:128 [D loss: 0.237147, acc: 86.72%] [G loss: 9.461643]\n",
      "epoch:0 step:129 [D loss: 0.168317, acc: 94.53%] [G loss: 9.462431]\n",
      "epoch:0 step:130 [D loss: 0.291471, acc: 84.38%] [G loss: 9.225388]\n",
      "epoch:0 step:131 [D loss: 0.186655, acc: 92.97%] [G loss: 8.702338]\n",
      "epoch:0 step:132 [D loss: 0.196270, acc: 89.06%] [G loss: 8.918526]\n",
      "epoch:0 step:133 [D loss: 0.317965, acc: 87.50%] [G loss: 10.272030]\n",
      "epoch:0 step:134 [D loss: 0.142425, acc: 92.97%] [G loss: 9.430574]\n",
      "epoch:0 step:135 [D loss: 0.272644, acc: 86.72%] [G loss: 9.087436]\n",
      "epoch:0 step:136 [D loss: 0.287575, acc: 86.72%] [G loss: 9.612010]\n",
      "epoch:0 step:137 [D loss: 0.196284, acc: 91.41%] [G loss: 8.546944]\n",
      "epoch:0 step:138 [D loss: 0.306103, acc: 85.16%] [G loss: 9.752945]\n",
      "epoch:0 step:139 [D loss: 0.187408, acc: 91.41%] [G loss: 8.808638]\n",
      "epoch:0 step:140 [D loss: 0.264288, acc: 88.28%] [G loss: 8.584115]\n",
      "epoch:0 step:141 [D loss: 0.143978, acc: 92.97%] [G loss: 8.437173]\n",
      "epoch:0 step:142 [D loss: 0.256314, acc: 91.41%] [G loss: 10.047991]\n",
      "epoch:0 step:143 [D loss: 0.184705, acc: 89.84%] [G loss: 9.024469]\n",
      "epoch:0 step:144 [D loss: 0.189298, acc: 92.19%] [G loss: 8.668122]\n",
      "epoch:0 step:145 [D loss: 0.274167, acc: 85.94%] [G loss: 9.817575]\n",
      "epoch:0 step:146 [D loss: 0.219515, acc: 89.84%] [G loss: 9.214956]\n",
      "epoch:0 step:147 [D loss: 0.129406, acc: 96.09%] [G loss: 8.983812]\n",
      "epoch:0 step:148 [D loss: 0.195241, acc: 90.62%] [G loss: 9.170039]\n",
      "epoch:0 step:149 [D loss: 0.192344, acc: 90.62%] [G loss: 7.877212]\n",
      "epoch:0 step:150 [D loss: 0.222004, acc: 92.19%] [G loss: 8.255558]\n",
      "epoch:0 step:151 [D loss: 0.118456, acc: 96.09%] [G loss: 8.425337]\n",
      "epoch:0 step:152 [D loss: 0.316484, acc: 85.16%] [G loss: 9.568974]\n",
      "epoch:0 step:153 [D loss: 0.106457, acc: 97.66%] [G loss: 8.890685]\n",
      "epoch:0 step:154 [D loss: 0.204439, acc: 92.97%] [G loss: 8.781439]\n",
      "epoch:0 step:155 [D loss: 0.107432, acc: 95.31%] [G loss: 8.007669]\n",
      "epoch:0 step:156 [D loss: 0.213191, acc: 90.62%] [G loss: 9.677600]\n",
      "epoch:0 step:157 [D loss: 0.167052, acc: 90.62%] [G loss: 9.326181]\n",
      "epoch:0 step:158 [D loss: 0.095178, acc: 96.09%] [G loss: 8.527966]\n",
      "epoch:0 step:159 [D loss: 0.248230, acc: 90.62%] [G loss: 9.957971]\n",
      "epoch:0 step:160 [D loss: 0.146999, acc: 91.41%] [G loss: 9.788690]\n",
      "epoch:0 step:161 [D loss: 0.195425, acc: 90.62%] [G loss: 8.811301]\n",
      "epoch:0 step:162 [D loss: 0.310886, acc: 86.72%] [G loss: 10.146279]\n",
      "epoch:0 step:163 [D loss: 0.194117, acc: 90.62%] [G loss: 8.607420]\n",
      "epoch:0 step:164 [D loss: 0.118601, acc: 95.31%] [G loss: 7.798624]\n",
      "epoch:0 step:165 [D loss: 0.286129, acc: 88.28%] [G loss: 9.046158]\n",
      "epoch:0 step:166 [D loss: 0.212947, acc: 89.06%] [G loss: 8.045664]\n",
      "epoch:0 step:167 [D loss: 0.261398, acc: 89.84%] [G loss: 8.888899]\n",
      "epoch:0 step:168 [D loss: 0.277674, acc: 85.94%] [G loss: 8.576299]\n",
      "epoch:0 step:169 [D loss: 0.350452, acc: 86.72%] [G loss: 8.795213]\n",
      "epoch:0 step:170 [D loss: 0.216100, acc: 88.28%] [G loss: 8.550414]\n",
      "epoch:0 step:171 [D loss: 0.209966, acc: 86.72%] [G loss: 8.092589]\n",
      "epoch:0 step:172 [D loss: 0.217026, acc: 93.75%] [G loss: 7.649205]\n",
      "epoch:0 step:173 [D loss: 0.286896, acc: 87.50%] [G loss: 7.334490]\n",
      "epoch:0 step:174 [D loss: 0.157659, acc: 94.53%] [G loss: 6.772449]\n",
      "epoch:0 step:175 [D loss: 0.198349, acc: 93.75%] [G loss: 7.385920]\n",
      "epoch:0 step:176 [D loss: 0.159757, acc: 94.53%] [G loss: 7.432185]\n",
      "epoch:0 step:177 [D loss: 0.249134, acc: 87.50%] [G loss: 6.954144]\n",
      "epoch:0 step:178 [D loss: 0.160266, acc: 95.31%] [G loss: 6.976573]\n",
      "epoch:0 step:179 [D loss: 0.225666, acc: 91.41%] [G loss: 7.987776]\n",
      "epoch:0 step:180 [D loss: 0.162745, acc: 93.75%] [G loss: 7.260077]\n",
      "epoch:0 step:181 [D loss: 0.163564, acc: 94.53%] [G loss: 6.955860]\n",
      "epoch:0 step:182 [D loss: 0.221199, acc: 93.75%] [G loss: 7.795672]\n",
      "epoch:0 step:183 [D loss: 0.151319, acc: 92.19%] [G loss: 7.548760]\n",
      "epoch:0 step:184 [D loss: 0.249772, acc: 88.28%] [G loss: 7.659413]\n",
      "epoch:0 step:185 [D loss: 0.224470, acc: 92.97%] [G loss: 7.494238]\n",
      "epoch:0 step:186 [D loss: 0.175254, acc: 91.41%] [G loss: 6.912924]\n",
      "epoch:0 step:187 [D loss: 0.372782, acc: 82.81%] [G loss: 8.560715]\n",
      "epoch:0 step:188 [D loss: 0.188891, acc: 92.19%] [G loss: 6.573292]\n",
      "epoch:0 step:189 [D loss: 0.463924, acc: 78.91%] [G loss: 8.727347]\n",
      "epoch:0 step:190 [D loss: 0.145104, acc: 92.97%] [G loss: 6.919041]\n",
      "epoch:0 step:191 [D loss: 0.247671, acc: 89.06%] [G loss: 6.529652]\n",
      "epoch:0 step:192 [D loss: 0.271059, acc: 85.94%] [G loss: 5.984040]\n",
      "epoch:0 step:193 [D loss: 0.206921, acc: 92.97%] [G loss: 6.925513]\n",
      "epoch:0 step:194 [D loss: 0.183666, acc: 91.41%] [G loss: 6.388848]\n",
      "epoch:0 step:195 [D loss: 0.309846, acc: 88.28%] [G loss: 6.732497]\n",
      "epoch:0 step:196 [D loss: 0.192601, acc: 92.19%] [G loss: 5.165817]\n",
      "epoch:0 step:197 [D loss: 0.365322, acc: 83.59%] [G loss: 6.015604]\n",
      "epoch:0 step:198 [D loss: 0.188748, acc: 89.06%] [G loss: 4.789147]\n",
      "epoch:0 step:199 [D loss: 0.380217, acc: 80.47%] [G loss: 6.125499]\n",
      "epoch:0 step:200 [D loss: 0.224817, acc: 87.50%] [G loss: 4.680439]\n",
      "WARNING:tensorflow:From /home/imi432_006/anaconda3/envs/tf/lib/python3.5/site-packages/tensorflow/contrib/gan/python/eval/python/classifier_metrics_impl.py:185: FastGFile.__init__ (from tensorflow.python.platform.gfile) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.gfile.GFile.\n",
      "##############\n",
      "[ 7.37170007  7.59974359 12.25367035 11.80824     8.81954761 11.98333983\n",
      " 11.27914699  9.97091446 10.02284026 10.29102348]\n",
      "##########\n",
      "epoch:0 step:201 [D loss: 0.330212, acc: 83.59%] [G loss: 5.268862]\n",
      "epoch:0 step:202 [D loss: 0.293489, acc: 88.28%] [G loss: 4.739145]\n",
      "epoch:0 step:203 [D loss: 0.333477, acc: 85.16%] [G loss: 5.127398]\n",
      "epoch:0 step:204 [D loss: 0.244655, acc: 89.06%] [G loss: 4.939608]\n",
      "epoch:0 step:205 [D loss: 0.373797, acc: 84.38%] [G loss: 5.465028]\n",
      "epoch:0 step:206 [D loss: 0.181306, acc: 91.41%] [G loss: 5.262498]\n",
      "epoch:0 step:207 [D loss: 0.476345, acc: 77.34%] [G loss: 5.146076]\n",
      "epoch:0 step:208 [D loss: 0.169001, acc: 95.31%] [G loss: 5.488743]\n",
      "epoch:0 step:209 [D loss: 0.273719, acc: 87.50%] [G loss: 4.717699]\n",
      "epoch:0 step:210 [D loss: 0.277579, acc: 86.72%] [G loss: 4.168157]\n",
      "epoch:0 step:211 [D loss: 0.721772, acc: 64.84%] [G loss: 5.718460]\n",
      "epoch:0 step:212 [D loss: 0.198564, acc: 94.53%] [G loss: 4.471354]\n",
      "epoch:0 step:213 [D loss: 0.620397, acc: 70.31%] [G loss: 5.662191]\n",
      "epoch:0 step:214 [D loss: 0.169983, acc: 92.97%] [G loss: 3.988397]\n",
      "epoch:0 step:215 [D loss: 0.243371, acc: 92.97%] [G loss: 3.977818]\n",
      "epoch:0 step:216 [D loss: 0.560661, acc: 71.88%] [G loss: 5.950664]\n",
      "epoch:0 step:217 [D loss: 0.271514, acc: 87.50%] [G loss: 3.924007]\n",
      "epoch:0 step:218 [D loss: 0.449496, acc: 75.00%] [G loss: 5.265867]\n",
      "epoch:0 step:219 [D loss: 0.198944, acc: 89.84%] [G loss: 4.443459]\n",
      "epoch:0 step:220 [D loss: 0.698620, acc: 66.41%] [G loss: 5.568320]\n",
      "epoch:0 step:221 [D loss: 0.176665, acc: 93.75%] [G loss: 4.075991]\n",
      "epoch:0 step:222 [D loss: 0.333852, acc: 85.16%] [G loss: 4.537920]\n",
      "epoch:0 step:223 [D loss: 0.184227, acc: 92.97%] [G loss: 4.455116]\n",
      "epoch:0 step:224 [D loss: 0.235093, acc: 93.75%] [G loss: 4.709061]\n",
      "epoch:0 step:225 [D loss: 0.208387, acc: 93.75%] [G loss: 4.465461]\n",
      "epoch:0 step:226 [D loss: 0.215317, acc: 93.75%] [G loss: 4.831619]\n",
      "epoch:0 step:227 [D loss: 0.238896, acc: 89.06%] [G loss: 4.162458]\n",
      "epoch:0 step:228 [D loss: 0.221016, acc: 93.75%] [G loss: 4.630116]\n",
      "epoch:0 step:229 [D loss: 0.568699, acc: 70.31%] [G loss: 5.590607]\n",
      "epoch:0 step:230 [D loss: 0.239670, acc: 92.19%] [G loss: 4.426665]\n",
      "epoch:0 step:231 [D loss: 0.390188, acc: 82.03%] [G loss: 5.922621]\n",
      "epoch:0 step:232 [D loss: 0.142653, acc: 96.09%] [G loss: 4.626872]\n",
      "epoch:0 step:233 [D loss: 0.540854, acc: 74.22%] [G loss: 5.616895]\n",
      "epoch:0 step:234 [D loss: 0.151049, acc: 95.31%] [G loss: 4.175900]\n",
      "epoch:0 step:235 [D loss: 0.302590, acc: 88.28%] [G loss: 4.130238]\n",
      "epoch:0 step:236 [D loss: 0.195453, acc: 94.53%] [G loss: 4.163089]\n",
      "epoch:0 step:237 [D loss: 0.711262, acc: 64.06%] [G loss: 5.798352]\n",
      "epoch:0 step:238 [D loss: 0.266161, acc: 89.06%] [G loss: 4.178252]\n",
      "epoch:0 step:239 [D loss: 0.582625, acc: 71.09%] [G loss: 5.046126]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0 step:240 [D loss: 0.297212, acc: 85.16%] [G loss: 3.869474]\n",
      "epoch:0 step:241 [D loss: 0.249147, acc: 94.53%] [G loss: 4.626138]\n",
      "epoch:0 step:242 [D loss: 0.296232, acc: 86.72%] [G loss: 4.274498]\n",
      "epoch:0 step:243 [D loss: 0.460588, acc: 79.69%] [G loss: 5.275514]\n",
      "epoch:0 step:244 [D loss: 0.232823, acc: 90.62%] [G loss: 4.034443]\n",
      "epoch:0 step:245 [D loss: 0.290870, acc: 90.62%] [G loss: 4.789290]\n",
      "epoch:0 step:246 [D loss: 0.337011, acc: 85.16%] [G loss: 4.229318]\n",
      "epoch:0 step:247 [D loss: 0.337223, acc: 87.50%] [G loss: 4.178967]\n",
      "epoch:0 step:248 [D loss: 0.397671, acc: 82.03%] [G loss: 5.079225]\n",
      "epoch:0 step:249 [D loss: 0.275848, acc: 92.97%] [G loss: 4.109962]\n",
      "epoch:0 step:250 [D loss: 0.390296, acc: 80.47%] [G loss: 4.807235]\n",
      "epoch:0 step:251 [D loss: 0.292798, acc: 86.72%] [G loss: 4.292518]\n",
      "epoch:0 step:252 [D loss: 0.297483, acc: 89.06%] [G loss: 4.183035]\n",
      "epoch:0 step:253 [D loss: 0.376731, acc: 82.81%] [G loss: 4.124509]\n",
      "epoch:0 step:254 [D loss: 0.403476, acc: 82.81%] [G loss: 4.781252]\n",
      "epoch:0 step:255 [D loss: 0.292996, acc: 88.28%] [G loss: 4.357707]\n",
      "epoch:0 step:256 [D loss: 0.540267, acc: 74.22%] [G loss: 5.358921]\n",
      "epoch:0 step:257 [D loss: 0.243664, acc: 89.84%] [G loss: 4.391862]\n",
      "epoch:0 step:258 [D loss: 0.255639, acc: 93.75%] [G loss: 5.265810]\n",
      "epoch:0 step:259 [D loss: 0.184850, acc: 94.53%] [G loss: 4.540215]\n",
      "epoch:0 step:260 [D loss: 0.509762, acc: 73.44%] [G loss: 5.102658]\n",
      "epoch:0 step:261 [D loss: 0.193071, acc: 95.31%] [G loss: 4.078478]\n",
      "epoch:0 step:262 [D loss: 0.380399, acc: 82.03%] [G loss: 4.898667]\n",
      "epoch:0 step:263 [D loss: 0.222081, acc: 95.31%] [G loss: 4.281078]\n",
      "epoch:0 step:264 [D loss: 0.476845, acc: 77.34%] [G loss: 4.757663]\n",
      "epoch:0 step:265 [D loss: 0.448997, acc: 78.12%] [G loss: 4.753756]\n",
      "epoch:0 step:266 [D loss: 0.317220, acc: 85.16%] [G loss: 5.017507]\n",
      "epoch:0 step:267 [D loss: 0.482203, acc: 78.12%] [G loss: 5.807508]\n",
      "epoch:0 step:268 [D loss: 0.303286, acc: 88.28%] [G loss: 4.610446]\n",
      "epoch:0 step:269 [D loss: 0.291525, acc: 92.19%] [G loss: 3.504945]\n",
      "epoch:0 step:270 [D loss: 0.328134, acc: 83.59%] [G loss: 4.113824]\n",
      "epoch:0 step:271 [D loss: 0.409465, acc: 81.25%] [G loss: 5.099473]\n",
      "epoch:0 step:272 [D loss: 0.203530, acc: 96.09%] [G loss: 4.840827]\n",
      "epoch:0 step:273 [D loss: 0.503494, acc: 74.22%] [G loss: 5.440645]\n",
      "epoch:0 step:274 [D loss: 0.329320, acc: 87.50%] [G loss: 4.388586]\n",
      "epoch:0 step:275 [D loss: 0.415626, acc: 77.34%] [G loss: 4.537110]\n",
      "epoch:0 step:276 [D loss: 0.334960, acc: 85.16%] [G loss: 4.533812]\n",
      "epoch:0 step:277 [D loss: 0.463497, acc: 76.56%] [G loss: 4.453544]\n",
      "epoch:0 step:278 [D loss: 0.384079, acc: 80.47%] [G loss: 4.257526]\n",
      "epoch:0 step:279 [D loss: 0.582202, acc: 73.44%] [G loss: 5.279509]\n",
      "epoch:0 step:280 [D loss: 0.280762, acc: 85.16%] [G loss: 4.288098]\n",
      "epoch:0 step:281 [D loss: 0.710975, acc: 60.16%] [G loss: 5.386001]\n",
      "epoch:0 step:282 [D loss: 0.185747, acc: 95.31%] [G loss: 3.884005]\n",
      "epoch:0 step:283 [D loss: 0.406013, acc: 83.59%] [G loss: 5.301620]\n",
      "epoch:0 step:284 [D loss: 0.221373, acc: 91.41%] [G loss: 4.020612]\n",
      "epoch:0 step:285 [D loss: 0.397621, acc: 80.47%] [G loss: 5.061337]\n",
      "epoch:0 step:286 [D loss: 0.264514, acc: 90.62%] [G loss: 4.522615]\n",
      "epoch:0 step:287 [D loss: 0.291152, acc: 91.41%] [G loss: 4.635618]\n",
      "epoch:0 step:288 [D loss: 0.362218, acc: 87.50%] [G loss: 4.068128]\n",
      "epoch:0 step:289 [D loss: 0.293712, acc: 92.97%] [G loss: 4.446952]\n",
      "epoch:0 step:290 [D loss: 0.365934, acc: 84.38%] [G loss: 4.098619]\n",
      "epoch:0 step:291 [D loss: 0.426030, acc: 80.47%] [G loss: 4.277225]\n",
      "epoch:0 step:292 [D loss: 0.267988, acc: 92.97%] [G loss: 4.363490]\n",
      "epoch:0 step:293 [D loss: 0.327922, acc: 88.28%] [G loss: 3.519267]\n",
      "epoch:0 step:294 [D loss: 0.418340, acc: 82.81%] [G loss: 4.516062]\n",
      "epoch:0 step:295 [D loss: 0.228765, acc: 95.31%] [G loss: 4.273967]\n",
      "epoch:0 step:296 [D loss: 0.449526, acc: 76.56%] [G loss: 5.207945]\n",
      "epoch:0 step:297 [D loss: 0.285261, acc: 88.28%] [G loss: 3.969696]\n",
      "epoch:0 step:298 [D loss: 0.309473, acc: 89.84%] [G loss: 4.236811]\n",
      "epoch:0 step:299 [D loss: 0.365197, acc: 82.81%] [G loss: 4.617160]\n",
      "epoch:0 step:300 [D loss: 0.242533, acc: 93.75%] [G loss: 4.188439]\n",
      "epoch:0 step:301 [D loss: 0.522127, acc: 75.78%] [G loss: 5.342319]\n",
      "epoch:0 step:302 [D loss: 0.359149, acc: 85.16%] [G loss: 4.744049]\n",
      "epoch:0 step:303 [D loss: 0.266069, acc: 92.19%] [G loss: 4.666905]\n",
      "epoch:0 step:304 [D loss: 0.297929, acc: 91.41%] [G loss: 4.739291]\n",
      "epoch:0 step:305 [D loss: 0.291812, acc: 89.06%] [G loss: 5.104303]\n",
      "epoch:0 step:306 [D loss: 0.319815, acc: 87.50%] [G loss: 6.091314]\n",
      "epoch:0 step:307 [D loss: 0.346569, acc: 86.72%] [G loss: 5.182965]\n",
      "epoch:0 step:308 [D loss: 0.268487, acc: 92.97%] [G loss: 4.536106]\n",
      "epoch:0 step:309 [D loss: 0.174572, acc: 96.09%] [G loss: 4.229935]\n",
      "epoch:0 step:310 [D loss: 0.270246, acc: 92.97%] [G loss: 5.122485]\n",
      "epoch:0 step:311 [D loss: 0.208368, acc: 95.31%] [G loss: 4.500489]\n",
      "epoch:0 step:312 [D loss: 0.403634, acc: 84.38%] [G loss: 5.552794]\n",
      "epoch:0 step:313 [D loss: 0.161228, acc: 96.09%] [G loss: 4.917473]\n",
      "epoch:0 step:314 [D loss: 0.204995, acc: 92.19%] [G loss: 4.627276]\n",
      "epoch:0 step:315 [D loss: 0.300582, acc: 89.84%] [G loss: 5.328335]\n",
      "epoch:0 step:316 [D loss: 0.399831, acc: 82.03%] [G loss: 4.121512]\n",
      "epoch:0 step:317 [D loss: 0.736169, acc: 54.69%] [G loss: 4.663808]\n",
      "epoch:0 step:318 [D loss: 0.296015, acc: 89.06%] [G loss: 3.910530]\n",
      "epoch:0 step:319 [D loss: 0.687514, acc: 65.62%] [G loss: 6.246298]\n",
      "epoch:0 step:320 [D loss: 0.232328, acc: 91.41%] [G loss: 3.820436]\n",
      "epoch:0 step:321 [D loss: 0.481635, acc: 75.78%] [G loss: 5.306183]\n",
      "epoch:0 step:322 [D loss: 0.261326, acc: 91.41%] [G loss: 4.057576]\n",
      "epoch:0 step:323 [D loss: 0.374599, acc: 83.59%] [G loss: 4.844804]\n",
      "epoch:0 step:324 [D loss: 0.235744, acc: 93.75%] [G loss: 4.319820]\n",
      "epoch:0 step:325 [D loss: 0.360835, acc: 83.59%] [G loss: 4.852227]\n",
      "epoch:0 step:326 [D loss: 0.382857, acc: 85.16%] [G loss: 4.843821]\n",
      "epoch:0 step:327 [D loss: 0.295944, acc: 89.84%] [G loss: 4.859851]\n",
      "epoch:0 step:328 [D loss: 0.416495, acc: 79.69%] [G loss: 4.448353]\n",
      "epoch:0 step:329 [D loss: 0.208622, acc: 98.44%] [G loss: 4.348184]\n",
      "epoch:0 step:330 [D loss: 0.391309, acc: 84.38%] [G loss: 4.935814]\n",
      "epoch:0 step:331 [D loss: 0.395691, acc: 82.81%] [G loss: 4.350170]\n",
      "epoch:0 step:332 [D loss: 0.359415, acc: 84.38%] [G loss: 4.569272]\n",
      "epoch:0 step:333 [D loss: 0.511723, acc: 73.44%] [G loss: 4.459413]\n",
      "epoch:0 step:334 [D loss: 0.384284, acc: 85.94%] [G loss: 4.145440]\n",
      "epoch:0 step:335 [D loss: 0.357818, acc: 85.16%] [G loss: 4.969430]\n",
      "epoch:0 step:336 [D loss: 0.421244, acc: 82.03%] [G loss: 4.435279]\n",
      "epoch:0 step:337 [D loss: 0.273310, acc: 91.41%] [G loss: 4.716276]\n",
      "epoch:0 step:338 [D loss: 0.491937, acc: 75.78%] [G loss: 4.452899]\n",
      "epoch:0 step:339 [D loss: 0.311612, acc: 90.62%] [G loss: 4.363802]\n",
      "epoch:0 step:340 [D loss: 0.278797, acc: 94.53%] [G loss: 4.643696]\n",
      "epoch:0 step:341 [D loss: 0.718350, acc: 64.84%] [G loss: 5.197889]\n",
      "epoch:0 step:342 [D loss: 0.242822, acc: 89.84%] [G loss: 3.883814]\n",
      "epoch:0 step:343 [D loss: 0.547831, acc: 73.44%] [G loss: 5.520817]\n",
      "epoch:0 step:344 [D loss: 0.218346, acc: 94.53%] [G loss: 4.156775]\n",
      "epoch:0 step:345 [D loss: 0.465318, acc: 82.03%] [G loss: 5.034804]\n",
      "epoch:0 step:346 [D loss: 0.254347, acc: 92.19%] [G loss: 4.430283]\n",
      "epoch:0 step:347 [D loss: 0.340139, acc: 87.50%] [G loss: 5.006214]\n",
      "epoch:0 step:348 [D loss: 0.483500, acc: 76.56%] [G loss: 4.786135]\n",
      "epoch:0 step:349 [D loss: 0.353181, acc: 84.38%] [G loss: 4.128529]\n",
      "epoch:0 step:350 [D loss: 0.355371, acc: 82.81%] [G loss: 4.710220]\n",
      "epoch:0 step:351 [D loss: 0.405632, acc: 78.91%] [G loss: 4.061322]\n",
      "epoch:0 step:352 [D loss: 0.317657, acc: 87.50%] [G loss: 4.221690]\n",
      "epoch:0 step:353 [D loss: 0.362294, acc: 85.16%] [G loss: 3.877152]\n",
      "epoch:0 step:354 [D loss: 0.492296, acc: 79.69%] [G loss: 4.743247]\n",
      "epoch:0 step:355 [D loss: 0.342739, acc: 89.06%] [G loss: 4.592793]\n",
      "epoch:0 step:356 [D loss: 0.540188, acc: 72.66%] [G loss: 4.487466]\n",
      "epoch:0 step:357 [D loss: 0.336743, acc: 87.50%] [G loss: 4.173626]\n",
      "epoch:0 step:358 [D loss: 0.262009, acc: 93.75%] [G loss: 4.244846]\n",
      "epoch:0 step:359 [D loss: 0.355843, acc: 84.38%] [G loss: 4.705482]\n",
      "epoch:0 step:360 [D loss: 0.268104, acc: 88.28%] [G loss: 4.899397]\n",
      "epoch:0 step:361 [D loss: 0.284261, acc: 88.28%] [G loss: 3.844854]\n",
      "epoch:0 step:362 [D loss: 0.442517, acc: 79.69%] [G loss: 4.175997]\n",
      "epoch:0 step:363 [D loss: 0.350621, acc: 86.72%] [G loss: 4.582386]\n",
      "epoch:0 step:364 [D loss: 0.474894, acc: 78.12%] [G loss: 4.953100]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0 step:365 [D loss: 0.337765, acc: 89.06%] [G loss: 4.613484]\n",
      "epoch:0 step:366 [D loss: 0.287693, acc: 93.75%] [G loss: 4.194414]\n",
      "epoch:0 step:367 [D loss: 0.466283, acc: 75.00%] [G loss: 5.223933]\n",
      "epoch:0 step:368 [D loss: 0.280048, acc: 88.28%] [G loss: 4.631252]\n",
      "epoch:0 step:369 [D loss: 0.395309, acc: 84.38%] [G loss: 4.767400]\n",
      "epoch:0 step:370 [D loss: 0.338964, acc: 87.50%] [G loss: 4.459675]\n",
      "epoch:0 step:371 [D loss: 0.279777, acc: 92.97%] [G loss: 4.437447]\n",
      "epoch:0 step:372 [D loss: 0.454017, acc: 78.12%] [G loss: 4.716449]\n",
      "epoch:0 step:373 [D loss: 0.263794, acc: 92.97%] [G loss: 4.202140]\n",
      "epoch:0 step:374 [D loss: 0.418704, acc: 82.81%] [G loss: 4.368792]\n",
      "epoch:0 step:375 [D loss: 0.342503, acc: 86.72%] [G loss: 4.640287]\n",
      "epoch:0 step:376 [D loss: 0.682094, acc: 63.28%] [G loss: 4.902408]\n",
      "epoch:0 step:377 [D loss: 0.245800, acc: 92.97%] [G loss: 4.363643]\n",
      "epoch:0 step:378 [D loss: 0.384411, acc: 83.59%] [G loss: 4.792859]\n",
      "epoch:0 step:379 [D loss: 0.484146, acc: 77.34%] [G loss: 4.369658]\n",
      "epoch:0 step:380 [D loss: 0.239586, acc: 94.53%] [G loss: 4.385088]\n",
      "epoch:0 step:381 [D loss: 0.343303, acc: 89.84%] [G loss: 4.623904]\n",
      "epoch:0 step:382 [D loss: 0.385200, acc: 83.59%] [G loss: 4.347184]\n",
      "epoch:0 step:383 [D loss: 0.598665, acc: 69.53%] [G loss: 4.648019]\n",
      "epoch:0 step:384 [D loss: 0.260620, acc: 92.19%] [G loss: 3.808412]\n",
      "epoch:0 step:385 [D loss: 0.566250, acc: 67.97%] [G loss: 4.099065]\n",
      "epoch:0 step:386 [D loss: 0.260625, acc: 92.97%] [G loss: 4.736434]\n",
      "epoch:0 step:387 [D loss: 0.279434, acc: 89.06%] [G loss: 4.788087]\n",
      "epoch:0 step:388 [D loss: 0.324076, acc: 90.62%] [G loss: 5.178519]\n",
      "epoch:0 step:389 [D loss: 0.350371, acc: 85.94%] [G loss: 4.894542]\n",
      "epoch:0 step:390 [D loss: 0.451770, acc: 75.78%] [G loss: 4.552494]\n",
      "epoch:0 step:391 [D loss: 0.440991, acc: 82.03%] [G loss: 4.160964]\n",
      "epoch:0 step:392 [D loss: 0.314238, acc: 88.28%] [G loss: 4.313823]\n",
      "epoch:0 step:393 [D loss: 0.354540, acc: 88.28%] [G loss: 4.088086]\n",
      "epoch:0 step:394 [D loss: 0.387230, acc: 86.72%] [G loss: 4.446939]\n",
      "epoch:0 step:395 [D loss: 0.400774, acc: 84.38%] [G loss: 4.434929]\n",
      "epoch:0 step:396 [D loss: 0.627738, acc: 73.44%] [G loss: 4.801857]\n",
      "epoch:0 step:397 [D loss: 0.314908, acc: 88.28%] [G loss: 4.709390]\n",
      "epoch:0 step:398 [D loss: 0.320468, acc: 88.28%] [G loss: 4.599051]\n",
      "epoch:0 step:399 [D loss: 0.357805, acc: 87.50%] [G loss: 4.689340]\n",
      "epoch:0 step:400 [D loss: 0.336325, acc: 90.62%] [G loss: 4.352463]\n",
      "##############\n",
      "[ 6.65893914  5.92736964 10.92307235  9.14099941  8.30284882 10.98333983\n",
      " 11.27914699  9.15376519  9.50247365  8.59499853]\n",
      "##########\n",
      "epoch:0 step:401 [D loss: 0.336277, acc: 85.16%] [G loss: 4.540729]\n",
      "epoch:0 step:402 [D loss: 0.335001, acc: 88.28%] [G loss: 3.970923]\n",
      "epoch:0 step:403 [D loss: 0.410487, acc: 78.12%] [G loss: 4.071671]\n",
      "epoch:0 step:404 [D loss: 0.352644, acc: 84.38%] [G loss: 4.605586]\n",
      "epoch:0 step:405 [D loss: 0.265332, acc: 92.19%] [G loss: 4.643296]\n",
      "epoch:0 step:406 [D loss: 0.584720, acc: 73.44%] [G loss: 4.542397]\n",
      "epoch:0 step:407 [D loss: 0.291884, acc: 87.50%] [G loss: 4.399013]\n",
      "epoch:0 step:408 [D loss: 0.532411, acc: 74.22%] [G loss: 4.932091]\n",
      "epoch:0 step:409 [D loss: 0.276737, acc: 91.41%] [G loss: 3.798150]\n",
      "epoch:0 step:410 [D loss: 0.533274, acc: 74.22%] [G loss: 4.619731]\n",
      "epoch:0 step:411 [D loss: 0.322635, acc: 88.28%] [G loss: 4.037565]\n",
      "epoch:0 step:412 [D loss: 0.448003, acc: 79.69%] [G loss: 3.881628]\n",
      "epoch:0 step:413 [D loss: 0.379863, acc: 85.94%] [G loss: 4.134634]\n",
      "epoch:0 step:414 [D loss: 0.330558, acc: 85.94%] [G loss: 4.181935]\n",
      "epoch:0 step:415 [D loss: 0.421106, acc: 81.25%] [G loss: 4.230729]\n",
      "epoch:0 step:416 [D loss: 0.338997, acc: 88.28%] [G loss: 4.399259]\n",
      "epoch:0 step:417 [D loss: 0.352349, acc: 89.84%] [G loss: 4.392441]\n",
      "epoch:0 step:418 [D loss: 0.523422, acc: 73.44%] [G loss: 4.361288]\n",
      "epoch:0 step:419 [D loss: 0.403588, acc: 82.03%] [G loss: 4.645877]\n",
      "epoch:0 step:420 [D loss: 0.329995, acc: 85.94%] [G loss: 4.522426]\n",
      "epoch:0 step:421 [D loss: 0.341988, acc: 85.94%] [G loss: 4.302841]\n",
      "epoch:0 step:422 [D loss: 0.344372, acc: 82.81%] [G loss: 4.168100]\n",
      "epoch:0 step:423 [D loss: 0.385936, acc: 82.03%] [G loss: 4.272351]\n",
      "epoch:0 step:424 [D loss: 0.403501, acc: 83.59%] [G loss: 4.063758]\n",
      "epoch:0 step:425 [D loss: 0.307779, acc: 90.62%] [G loss: 4.916204]\n",
      "epoch:0 step:426 [D loss: 0.389364, acc: 83.59%] [G loss: 4.472265]\n",
      "epoch:0 step:427 [D loss: 0.456711, acc: 76.56%] [G loss: 4.423593]\n",
      "epoch:0 step:428 [D loss: 0.369272, acc: 83.59%] [G loss: 4.401430]\n",
      "epoch:0 step:429 [D loss: 0.288504, acc: 89.84%] [G loss: 4.504565]\n",
      "epoch:0 step:430 [D loss: 0.367112, acc: 83.59%] [G loss: 4.109479]\n",
      "epoch:0 step:431 [D loss: 0.289488, acc: 89.84%] [G loss: 3.990518]\n",
      "epoch:0 step:432 [D loss: 0.355431, acc: 90.62%] [G loss: 3.997254]\n",
      "epoch:0 step:433 [D loss: 0.577206, acc: 71.09%] [G loss: 4.105080]\n",
      "epoch:0 step:434 [D loss: 0.587701, acc: 71.09%] [G loss: 4.451831]\n",
      "epoch:0 step:435 [D loss: 0.392402, acc: 84.38%] [G loss: 4.125138]\n",
      "epoch:0 step:436 [D loss: 0.486362, acc: 77.34%] [G loss: 4.215860]\n",
      "epoch:0 step:437 [D loss: 0.533772, acc: 77.34%] [G loss: 4.077930]\n",
      "epoch:0 step:438 [D loss: 0.332508, acc: 86.72%] [G loss: 3.997625]\n",
      "epoch:0 step:439 [D loss: 0.468983, acc: 73.44%] [G loss: 4.581310]\n",
      "epoch:0 step:440 [D loss: 0.392859, acc: 82.03%] [G loss: 4.246392]\n",
      "epoch:0 step:441 [D loss: 0.440623, acc: 78.12%] [G loss: 3.989436]\n",
      "epoch:0 step:442 [D loss: 0.344899, acc: 85.94%] [G loss: 4.575162]\n",
      "epoch:0 step:443 [D loss: 0.343229, acc: 87.50%] [G loss: 4.162964]\n",
      "epoch:0 step:444 [D loss: 0.464033, acc: 75.78%] [G loss: 4.547019]\n",
      "epoch:0 step:445 [D loss: 0.447043, acc: 77.34%] [G loss: 4.058114]\n",
      "epoch:0 step:446 [D loss: 0.387753, acc: 85.16%] [G loss: 3.975147]\n",
      "epoch:0 step:447 [D loss: 0.405318, acc: 82.81%] [G loss: 4.511144]\n",
      "epoch:0 step:448 [D loss: 0.438981, acc: 79.69%] [G loss: 4.178363]\n",
      "epoch:0 step:449 [D loss: 0.454006, acc: 79.69%] [G loss: 4.167577]\n",
      "epoch:0 step:450 [D loss: 0.381587, acc: 84.38%] [G loss: 4.242683]\n",
      "epoch:0 step:451 [D loss: 0.476739, acc: 73.44%] [G loss: 3.630415]\n",
      "epoch:0 step:452 [D loss: 0.314500, acc: 87.50%] [G loss: 3.771705]\n",
      "epoch:0 step:453 [D loss: 0.521422, acc: 76.56%] [G loss: 4.524041]\n",
      "epoch:0 step:454 [D loss: 0.371298, acc: 85.16%] [G loss: 4.545203]\n",
      "epoch:0 step:455 [D loss: 0.459251, acc: 75.00%] [G loss: 4.047557]\n",
      "epoch:0 step:456 [D loss: 0.434437, acc: 81.25%] [G loss: 4.119282]\n",
      "epoch:0 step:457 [D loss: 0.539654, acc: 71.88%] [G loss: 4.173193]\n",
      "epoch:0 step:458 [D loss: 0.507333, acc: 74.22%] [G loss: 4.385667]\n",
      "epoch:0 step:459 [D loss: 0.484859, acc: 77.34%] [G loss: 4.144527]\n",
      "epoch:0 step:460 [D loss: 0.326854, acc: 89.84%] [G loss: 4.201711]\n",
      "epoch:0 step:461 [D loss: 0.418614, acc: 82.03%] [G loss: 3.983022]\n",
      "epoch:0 step:462 [D loss: 0.438570, acc: 82.81%] [G loss: 4.138445]\n",
      "epoch:0 step:463 [D loss: 0.413074, acc: 80.47%] [G loss: 3.510587]\n",
      "epoch:0 step:464 [D loss: 0.514985, acc: 77.34%] [G loss: 3.900114]\n",
      "epoch:0 step:465 [D loss: 0.418842, acc: 82.81%] [G loss: 4.067501]\n",
      "epoch:0 step:466 [D loss: 0.346720, acc: 85.94%] [G loss: 4.102578]\n",
      "epoch:0 step:467 [D loss: 0.350640, acc: 85.16%] [G loss: 4.132206]\n",
      "epoch:0 step:468 [D loss: 0.467385, acc: 75.00%] [G loss: 4.217669]\n",
      "epoch:0 step:469 [D loss: 0.419449, acc: 81.25%] [G loss: 4.719686]\n",
      "epoch:0 step:470 [D loss: 0.446769, acc: 76.56%] [G loss: 4.136967]\n",
      "epoch:0 step:471 [D loss: 0.480784, acc: 73.44%] [G loss: 4.158440]\n",
      "epoch:0 step:472 [D loss: 0.430103, acc: 80.47%] [G loss: 4.330489]\n",
      "epoch:0 step:473 [D loss: 0.419675, acc: 78.12%] [G loss: 3.697714]\n",
      "epoch:0 step:474 [D loss: 0.377868, acc: 82.81%] [G loss: 3.672348]\n",
      "epoch:0 step:475 [D loss: 0.550577, acc: 67.97%] [G loss: 3.673746]\n",
      "epoch:0 step:476 [D loss: 0.585016, acc: 68.75%] [G loss: 3.760725]\n",
      "epoch:0 step:477 [D loss: 0.511039, acc: 75.78%] [G loss: 4.020684]\n",
      "epoch:0 step:478 [D loss: 0.570339, acc: 76.56%] [G loss: 3.523987]\n",
      "epoch:0 step:479 [D loss: 0.433055, acc: 77.34%] [G loss: 4.141923]\n",
      "epoch:0 step:480 [D loss: 0.305939, acc: 89.06%] [G loss: 4.227091]\n",
      "epoch:0 step:481 [D loss: 0.535065, acc: 72.66%] [G loss: 3.878617]\n",
      "epoch:0 step:482 [D loss: 0.462071, acc: 78.91%] [G loss: 3.781403]\n",
      "epoch:0 step:483 [D loss: 0.418877, acc: 80.47%] [G loss: 3.488176]\n",
      "epoch:0 step:484 [D loss: 0.342269, acc: 89.06%] [G loss: 3.678373]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0 step:485 [D loss: 0.457572, acc: 78.12%] [G loss: 4.055634]\n",
      "epoch:0 step:486 [D loss: 0.641482, acc: 69.53%] [G loss: 3.417547]\n",
      "epoch:0 step:487 [D loss: 0.440316, acc: 78.91%] [G loss: 3.621647]\n",
      "epoch:0 step:488 [D loss: 0.492276, acc: 76.56%] [G loss: 4.080889]\n",
      "epoch:0 step:489 [D loss: 0.579707, acc: 66.41%] [G loss: 3.579936]\n",
      "epoch:0 step:490 [D loss: 0.417509, acc: 82.81%] [G loss: 3.569694]\n",
      "epoch:0 step:491 [D loss: 0.458343, acc: 78.12%] [G loss: 4.309608]\n",
      "epoch:0 step:492 [D loss: 0.459910, acc: 79.69%] [G loss: 3.826642]\n",
      "epoch:0 step:493 [D loss: 0.460774, acc: 77.34%] [G loss: 3.729230]\n",
      "epoch:0 step:494 [D loss: 0.437751, acc: 82.81%] [G loss: 4.030076]\n",
      "epoch:0 step:495 [D loss: 0.455563, acc: 79.69%] [G loss: 4.093102]\n",
      "epoch:0 step:496 [D loss: 0.351630, acc: 84.38%] [G loss: 3.368777]\n",
      "epoch:0 step:497 [D loss: 0.567485, acc: 70.31%] [G loss: 3.946540]\n",
      "epoch:0 step:498 [D loss: 0.398439, acc: 79.69%] [G loss: 3.730987]\n",
      "epoch:0 step:499 [D loss: 0.348576, acc: 89.84%] [G loss: 4.018207]\n",
      "epoch:0 step:500 [D loss: 0.694364, acc: 63.28%] [G loss: 3.666268]\n",
      "epoch:0 step:501 [D loss: 0.457536, acc: 75.00%] [G loss: 3.431047]\n",
      "epoch:0 step:502 [D loss: 0.469390, acc: 78.91%] [G loss: 3.568030]\n",
      "epoch:0 step:503 [D loss: 0.513293, acc: 78.12%] [G loss: 3.905743]\n",
      "epoch:0 step:504 [D loss: 0.376793, acc: 84.38%] [G loss: 4.087326]\n",
      "epoch:0 step:505 [D loss: 0.494624, acc: 77.34%] [G loss: 3.502252]\n",
      "epoch:0 step:506 [D loss: 0.446033, acc: 78.91%] [G loss: 4.021962]\n",
      "epoch:0 step:507 [D loss: 0.454036, acc: 80.47%] [G loss: 3.920823]\n",
      "epoch:0 step:508 [D loss: 0.441743, acc: 79.69%] [G loss: 4.119412]\n",
      "epoch:0 step:509 [D loss: 0.434766, acc: 85.16%] [G loss: 3.567036]\n",
      "epoch:0 step:510 [D loss: 0.448680, acc: 81.25%] [G loss: 3.466473]\n",
      "epoch:0 step:511 [D loss: 0.376561, acc: 85.16%] [G loss: 3.221624]\n",
      "epoch:0 step:512 [D loss: 0.454514, acc: 75.78%] [G loss: 3.466712]\n",
      "epoch:0 step:513 [D loss: 0.515799, acc: 72.66%] [G loss: 3.534263]\n",
      "epoch:0 step:514 [D loss: 0.445502, acc: 78.91%] [G loss: 3.621227]\n",
      "epoch:0 step:515 [D loss: 0.574832, acc: 64.84%] [G loss: 3.940104]\n",
      "epoch:0 step:516 [D loss: 0.445355, acc: 79.69%] [G loss: 3.483409]\n",
      "epoch:0 step:517 [D loss: 0.626899, acc: 61.72%] [G loss: 3.833728]\n",
      "epoch:0 step:518 [D loss: 0.402968, acc: 82.03%] [G loss: 3.598736]\n",
      "epoch:0 step:519 [D loss: 0.491327, acc: 77.34%] [G loss: 3.965274]\n",
      "epoch:0 step:520 [D loss: 0.478795, acc: 79.69%] [G loss: 3.730652]\n",
      "epoch:0 step:521 [D loss: 0.526810, acc: 72.66%] [G loss: 3.539770]\n",
      "epoch:0 step:522 [D loss: 0.468478, acc: 81.25%] [G loss: 3.672591]\n",
      "epoch:0 step:523 [D loss: 0.517074, acc: 76.56%] [G loss: 3.790260]\n",
      "epoch:0 step:524 [D loss: 0.446325, acc: 79.69%] [G loss: 3.262948]\n",
      "epoch:0 step:525 [D loss: 0.499996, acc: 78.91%] [G loss: 3.092752]\n",
      "epoch:0 step:526 [D loss: 0.521389, acc: 75.00%] [G loss: 3.620513]\n",
      "epoch:0 step:527 [D loss: 0.499683, acc: 78.91%] [G loss: 3.439771]\n",
      "epoch:0 step:528 [D loss: 0.465937, acc: 77.34%] [G loss: 3.679993]\n",
      "epoch:0 step:529 [D loss: 0.503170, acc: 69.53%] [G loss: 3.972206]\n",
      "epoch:0 step:530 [D loss: 0.350120, acc: 86.72%] [G loss: 3.775236]\n",
      "epoch:0 step:531 [D loss: 0.513236, acc: 77.34%] [G loss: 3.526354]\n",
      "epoch:0 step:532 [D loss: 0.488904, acc: 80.47%] [G loss: 3.321878]\n",
      "epoch:0 step:533 [D loss: 0.592879, acc: 71.88%] [G loss: 4.073068]\n",
      "epoch:0 step:534 [D loss: 0.348675, acc: 85.16%] [G loss: 3.330627]\n",
      "epoch:0 step:535 [D loss: 0.545449, acc: 75.00%] [G loss: 3.287431]\n",
      "epoch:0 step:536 [D loss: 0.459744, acc: 76.56%] [G loss: 3.643806]\n",
      "epoch:0 step:537 [D loss: 0.561510, acc: 67.97%] [G loss: 3.447465]\n",
      "epoch:0 step:538 [D loss: 0.599097, acc: 72.66%] [G loss: 3.579359]\n",
      "epoch:0 step:539 [D loss: 0.481030, acc: 76.56%] [G loss: 3.369846]\n",
      "epoch:0 step:540 [D loss: 0.568725, acc: 67.97%] [G loss: 3.312931]\n",
      "epoch:0 step:541 [D loss: 0.473371, acc: 75.00%] [G loss: 3.370967]\n",
      "epoch:0 step:542 [D loss: 0.478254, acc: 75.00%] [G loss: 3.533212]\n",
      "epoch:0 step:543 [D loss: 0.539125, acc: 75.00%] [G loss: 3.476031]\n",
      "epoch:0 step:544 [D loss: 0.497945, acc: 75.00%] [G loss: 3.526558]\n",
      "epoch:0 step:545 [D loss: 0.516984, acc: 71.88%] [G loss: 3.404306]\n",
      "epoch:0 step:546 [D loss: 0.442636, acc: 75.78%] [G loss: 3.575055]\n",
      "epoch:0 step:547 [D loss: 0.428794, acc: 85.94%] [G loss: 3.367770]\n",
      "epoch:0 step:548 [D loss: 0.649209, acc: 62.50%] [G loss: 3.425245]\n",
      "epoch:0 step:549 [D loss: 0.411133, acc: 82.81%] [G loss: 3.121554]\n",
      "epoch:0 step:550 [D loss: 0.416033, acc: 83.59%] [G loss: 3.530478]\n",
      "epoch:0 step:551 [D loss: 0.525371, acc: 72.66%] [G loss: 3.422254]\n",
      "epoch:0 step:552 [D loss: 0.480946, acc: 81.25%] [G loss: 3.457642]\n",
      "epoch:0 step:553 [D loss: 0.499779, acc: 76.56%] [G loss: 3.292089]\n",
      "epoch:0 step:554 [D loss: 0.379742, acc: 83.59%] [G loss: 3.923106]\n",
      "epoch:0 step:555 [D loss: 0.591608, acc: 69.53%] [G loss: 3.842424]\n",
      "epoch:0 step:556 [D loss: 0.488848, acc: 75.78%] [G loss: 3.467431]\n",
      "epoch:0 step:557 [D loss: 0.495258, acc: 76.56%] [G loss: 3.400335]\n",
      "epoch:0 step:558 [D loss: 0.487638, acc: 79.69%] [G loss: 3.489458]\n",
      "epoch:0 step:559 [D loss: 0.568694, acc: 73.44%] [G loss: 3.372552]\n",
      "epoch:0 step:560 [D loss: 0.519729, acc: 71.09%] [G loss: 3.068977]\n",
      "epoch:0 step:561 [D loss: 0.549007, acc: 71.88%] [G loss: 2.908232]\n",
      "epoch:0 step:562 [D loss: 0.710302, acc: 57.81%] [G loss: 3.268186]\n",
      "epoch:0 step:563 [D loss: 0.530714, acc: 76.56%] [G loss: 3.316026]\n",
      "epoch:0 step:564 [D loss: 0.472632, acc: 75.78%] [G loss: 3.281688]\n",
      "epoch:0 step:565 [D loss: 0.584270, acc: 64.84%] [G loss: 3.403828]\n",
      "epoch:0 step:566 [D loss: 0.506084, acc: 75.00%] [G loss: 3.437563]\n",
      "epoch:0 step:567 [D loss: 0.432687, acc: 82.03%] [G loss: 3.553848]\n",
      "epoch:0 step:568 [D loss: 0.476825, acc: 78.12%] [G loss: 3.128952]\n",
      "epoch:0 step:569 [D loss: 0.585639, acc: 65.62%] [G loss: 3.211557]\n",
      "epoch:0 step:570 [D loss: 0.429037, acc: 81.25%] [G loss: 3.192502]\n",
      "epoch:0 step:571 [D loss: 0.507848, acc: 73.44%] [G loss: 3.256022]\n",
      "epoch:0 step:572 [D loss: 0.467157, acc: 77.34%] [G loss: 3.641549]\n",
      "epoch:0 step:573 [D loss: 0.533094, acc: 69.53%] [G loss: 3.594208]\n",
      "epoch:0 step:574 [D loss: 0.498708, acc: 77.34%] [G loss: 3.224830]\n",
      "epoch:0 step:575 [D loss: 0.415767, acc: 78.91%] [G loss: 3.674830]\n",
      "epoch:0 step:576 [D loss: 0.580668, acc: 68.75%] [G loss: 3.158957]\n",
      "epoch:0 step:577 [D loss: 0.491515, acc: 78.12%] [G loss: 3.102144]\n",
      "epoch:0 step:578 [D loss: 0.572989, acc: 72.66%] [G loss: 3.288732]\n",
      "epoch:0 step:579 [D loss: 0.528392, acc: 78.91%] [G loss: 2.853343]\n",
      "epoch:0 step:580 [D loss: 0.544641, acc: 71.88%] [G loss: 3.527156]\n",
      "epoch:0 step:581 [D loss: 0.469530, acc: 78.12%] [G loss: 3.353918]\n",
      "epoch:0 step:582 [D loss: 0.478011, acc: 78.91%] [G loss: 3.864529]\n",
      "epoch:0 step:583 [D loss: 0.604962, acc: 70.31%] [G loss: 3.593122]\n",
      "epoch:0 step:584 [D loss: 0.521900, acc: 76.56%] [G loss: 3.097207]\n",
      "epoch:0 step:585 [D loss: 0.451904, acc: 78.12%] [G loss: 3.244656]\n",
      "epoch:0 step:586 [D loss: 0.527696, acc: 77.34%] [G loss: 3.065781]\n",
      "epoch:0 step:587 [D loss: 0.616145, acc: 71.09%] [G loss: 3.093876]\n",
      "epoch:0 step:588 [D loss: 0.487274, acc: 76.56%] [G loss: 3.128804]\n",
      "epoch:0 step:589 [D loss: 0.592760, acc: 67.19%] [G loss: 2.967658]\n",
      "epoch:0 step:590 [D loss: 0.535106, acc: 75.00%] [G loss: 3.735309]\n",
      "epoch:0 step:591 [D loss: 0.506646, acc: 77.34%] [G loss: 3.354588]\n",
      "epoch:0 step:592 [D loss: 0.493666, acc: 76.56%] [G loss: 3.349970]\n",
      "epoch:0 step:593 [D loss: 0.602084, acc: 70.31%] [G loss: 3.241222]\n",
      "epoch:0 step:594 [D loss: 0.594101, acc: 64.06%] [G loss: 3.090967]\n",
      "epoch:0 step:595 [D loss: 0.527715, acc: 72.66%] [G loss: 3.278289]\n",
      "epoch:0 step:596 [D loss: 0.595116, acc: 64.06%] [G loss: 3.301344]\n",
      "epoch:0 step:597 [D loss: 0.524597, acc: 74.22%] [G loss: 3.095963]\n",
      "epoch:0 step:598 [D loss: 0.494335, acc: 76.56%] [G loss: 3.248011]\n",
      "epoch:0 step:599 [D loss: 0.527087, acc: 71.09%] [G loss: 3.042822]\n",
      "epoch:0 step:600 [D loss: 0.447155, acc: 80.47%] [G loss: 3.156797]\n",
      "##############\n",
      "[5.39964189 3.69073285 9.45598375 7.54329415 6.88976686 8.34013437\n",
      " 7.88509427 7.68439143 8.03469954 6.14365167]\n",
      "##########\n",
      "epoch:0 step:601 [D loss: 0.471184, acc: 78.12%] [G loss: 3.110772]\n",
      "epoch:0 step:602 [D loss: 0.493927, acc: 75.00%] [G loss: 3.162968]\n",
      "epoch:0 step:603 [D loss: 0.606547, acc: 71.09%] [G loss: 3.299971]\n",
      "epoch:0 step:604 [D loss: 0.588112, acc: 68.75%] [G loss: 2.842668]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0 step:605 [D loss: 0.500378, acc: 77.34%] [G loss: 3.005129]\n",
      "epoch:0 step:606 [D loss: 0.539629, acc: 67.97%] [G loss: 3.126914]\n",
      "epoch:0 step:607 [D loss: 0.488290, acc: 80.47%] [G loss: 2.905988]\n",
      "epoch:0 step:608 [D loss: 0.476712, acc: 78.12%] [G loss: 3.169239]\n",
      "epoch:0 step:609 [D loss: 0.504317, acc: 79.69%] [G loss: 3.060659]\n",
      "epoch:0 step:610 [D loss: 0.547451, acc: 75.00%] [G loss: 2.962158]\n",
      "epoch:0 step:611 [D loss: 0.451120, acc: 78.91%] [G loss: 3.182455]\n",
      "epoch:0 step:612 [D loss: 0.531337, acc: 73.44%] [G loss: 3.295504]\n",
      "epoch:0 step:613 [D loss: 0.504807, acc: 75.78%] [G loss: 3.333185]\n",
      "epoch:0 step:614 [D loss: 0.424853, acc: 83.59%] [G loss: 3.215624]\n",
      "epoch:0 step:615 [D loss: 0.508888, acc: 76.56%] [G loss: 2.972919]\n",
      "epoch:0 step:616 [D loss: 0.587056, acc: 69.53%] [G loss: 3.228372]\n",
      "epoch:0 step:617 [D loss: 0.598920, acc: 67.19%] [G loss: 3.332433]\n",
      "epoch:0 step:618 [D loss: 0.502926, acc: 74.22%] [G loss: 3.209733]\n",
      "epoch:0 step:619 [D loss: 0.410319, acc: 82.81%] [G loss: 3.435360]\n",
      "epoch:0 step:620 [D loss: 0.501888, acc: 75.00%] [G loss: 3.459768]\n",
      "epoch:0 step:621 [D loss: 0.557930, acc: 66.41%] [G loss: 3.081566]\n",
      "epoch:0 step:622 [D loss: 0.698617, acc: 62.50%] [G loss: 2.945674]\n",
      "epoch:0 step:623 [D loss: 0.530971, acc: 68.75%] [G loss: 3.156797]\n",
      "epoch:0 step:624 [D loss: 0.569147, acc: 72.66%] [G loss: 2.878431]\n",
      "epoch:0 step:625 [D loss: 0.522817, acc: 75.00%] [G loss: 2.996793]\n",
      "epoch:0 step:626 [D loss: 0.545893, acc: 75.78%] [G loss: 2.910898]\n",
      "epoch:0 step:627 [D loss: 0.534781, acc: 76.56%] [G loss: 2.861550]\n",
      "epoch:0 step:628 [D loss: 0.495260, acc: 80.47%] [G loss: 3.095399]\n",
      "epoch:0 step:629 [D loss: 0.638592, acc: 67.19%] [G loss: 2.974187]\n",
      "epoch:0 step:630 [D loss: 0.492423, acc: 75.78%] [G loss: 3.233040]\n",
      "epoch:0 step:631 [D loss: 0.533191, acc: 74.22%] [G loss: 3.031943]\n",
      "epoch:0 step:632 [D loss: 0.461029, acc: 76.56%] [G loss: 3.038113]\n",
      "epoch:0 step:633 [D loss: 0.466045, acc: 80.47%] [G loss: 3.116008]\n",
      "epoch:0 step:634 [D loss: 0.460314, acc: 77.34%] [G loss: 3.284331]\n",
      "epoch:0 step:635 [D loss: 0.452260, acc: 78.91%] [G loss: 3.342205]\n",
      "epoch:0 step:636 [D loss: 0.617288, acc: 65.62%] [G loss: 2.834473]\n",
      "epoch:0 step:637 [D loss: 0.422316, acc: 82.03%] [G loss: 3.144421]\n",
      "epoch:0 step:638 [D loss: 0.549259, acc: 71.09%] [G loss: 3.046039]\n",
      "epoch:0 step:639 [D loss: 0.518664, acc: 71.88%] [G loss: 3.088213]\n",
      "epoch:0 step:640 [D loss: 0.611882, acc: 67.19%] [G loss: 3.368594]\n",
      "epoch:0 step:641 [D loss: 0.415895, acc: 78.12%] [G loss: 3.075490]\n",
      "epoch:0 step:642 [D loss: 0.472816, acc: 78.91%] [G loss: 3.211024]\n",
      "epoch:0 step:643 [D loss: 0.609036, acc: 62.50%] [G loss: 2.957074]\n",
      "epoch:0 step:644 [D loss: 0.500605, acc: 75.78%] [G loss: 3.129367]\n",
      "epoch:0 step:645 [D loss: 0.549808, acc: 69.53%] [G loss: 3.059335]\n",
      "epoch:0 step:646 [D loss: 0.425517, acc: 82.81%] [G loss: 3.299497]\n",
      "epoch:0 step:647 [D loss: 0.552203, acc: 75.78%] [G loss: 3.083726]\n",
      "epoch:0 step:648 [D loss: 0.458432, acc: 80.47%] [G loss: 3.381906]\n",
      "epoch:0 step:649 [D loss: 0.440130, acc: 81.25%] [G loss: 3.275630]\n",
      "epoch:0 step:650 [D loss: 0.589954, acc: 71.88%] [G loss: 3.389637]\n",
      "epoch:0 step:651 [D loss: 0.482863, acc: 76.56%] [G loss: 3.315077]\n",
      "epoch:0 step:652 [D loss: 0.648776, acc: 60.16%] [G loss: 3.007318]\n",
      "epoch:0 step:653 [D loss: 0.488176, acc: 80.47%] [G loss: 2.947839]\n",
      "epoch:0 step:654 [D loss: 0.503630, acc: 79.69%] [G loss: 3.278685]\n",
      "epoch:0 step:655 [D loss: 0.680949, acc: 60.16%] [G loss: 3.102255]\n",
      "epoch:0 step:656 [D loss: 0.506235, acc: 77.34%] [G loss: 3.177611]\n",
      "epoch:0 step:657 [D loss: 0.537138, acc: 72.66%] [G loss: 3.215025]\n",
      "epoch:0 step:658 [D loss: 0.617841, acc: 66.41%] [G loss: 3.329186]\n",
      "epoch:0 step:659 [D loss: 0.438806, acc: 79.69%] [G loss: 3.450377]\n",
      "epoch:0 step:660 [D loss: 0.413580, acc: 83.59%] [G loss: 3.562077]\n",
      "epoch:0 step:661 [D loss: 0.553399, acc: 69.53%] [G loss: 3.351606]\n",
      "epoch:0 step:662 [D loss: 0.616569, acc: 67.97%] [G loss: 2.857730]\n",
      "epoch:0 step:663 [D loss: 0.385886, acc: 85.16%] [G loss: 3.344360]\n",
      "epoch:0 step:664 [D loss: 0.484061, acc: 75.00%] [G loss: 3.023826]\n",
      "epoch:0 step:665 [D loss: 0.499698, acc: 76.56%] [G loss: 3.285973]\n",
      "epoch:0 step:666 [D loss: 0.590343, acc: 68.75%] [G loss: 3.215282]\n",
      "epoch:0 step:667 [D loss: 0.504363, acc: 71.09%] [G loss: 3.152159]\n",
      "epoch:0 step:668 [D loss: 0.486899, acc: 76.56%] [G loss: 3.115718]\n",
      "epoch:0 step:669 [D loss: 0.590110, acc: 71.09%] [G loss: 3.123963]\n",
      "epoch:0 step:670 [D loss: 0.547780, acc: 73.44%] [G loss: 3.013061]\n",
      "epoch:0 step:671 [D loss: 0.551704, acc: 70.31%] [G loss: 3.078433]\n",
      "epoch:0 step:672 [D loss: 0.599497, acc: 70.31%] [G loss: 2.946847]\n",
      "epoch:0 step:673 [D loss: 0.445885, acc: 78.91%] [G loss: 2.954858]\n",
      "epoch:0 step:674 [D loss: 0.536510, acc: 74.22%] [G loss: 2.982680]\n",
      "epoch:0 step:675 [D loss: 0.615252, acc: 65.62%] [G loss: 3.193684]\n",
      "epoch:0 step:676 [D loss: 0.547270, acc: 69.53%] [G loss: 3.469867]\n",
      "epoch:0 step:677 [D loss: 0.538903, acc: 72.66%] [G loss: 3.183619]\n",
      "epoch:0 step:678 [D loss: 0.465055, acc: 75.00%] [G loss: 3.145807]\n",
      "epoch:0 step:679 [D loss: 0.598643, acc: 65.62%] [G loss: 2.981652]\n",
      "epoch:0 step:680 [D loss: 0.427004, acc: 82.03%] [G loss: 2.901746]\n",
      "epoch:0 step:681 [D loss: 0.586354, acc: 66.41%] [G loss: 2.847572]\n",
      "epoch:0 step:682 [D loss: 0.654016, acc: 63.28%] [G loss: 2.933208]\n",
      "epoch:0 step:683 [D loss: 0.522700, acc: 75.00%] [G loss: 3.058534]\n",
      "epoch:0 step:684 [D loss: 0.520146, acc: 72.66%] [G loss: 2.811849]\n",
      "epoch:0 step:685 [D loss: 0.597140, acc: 66.41%] [G loss: 3.022908]\n",
      "epoch:0 step:686 [D loss: 0.617208, acc: 66.41%] [G loss: 3.022316]\n",
      "epoch:0 step:687 [D loss: 0.486928, acc: 78.91%] [G loss: 2.954928]\n",
      "epoch:0 step:688 [D loss: 0.547796, acc: 68.75%] [G loss: 2.810219]\n",
      "epoch:0 step:689 [D loss: 0.565437, acc: 67.97%] [G loss: 3.216407]\n",
      "epoch:0 step:690 [D loss: 0.545258, acc: 71.88%] [G loss: 2.990696]\n",
      "epoch:0 step:691 [D loss: 0.466748, acc: 76.56%] [G loss: 3.149479]\n",
      "epoch:0 step:692 [D loss: 0.541914, acc: 72.66%] [G loss: 3.144382]\n",
      "epoch:0 step:693 [D loss: 0.573567, acc: 73.44%] [G loss: 3.267057]\n",
      "epoch:0 step:694 [D loss: 0.571090, acc: 70.31%] [G loss: 3.187929]\n",
      "epoch:0 step:695 [D loss: 0.478227, acc: 75.78%] [G loss: 2.948261]\n",
      "epoch:0 step:696 [D loss: 0.497021, acc: 76.56%] [G loss: 3.172247]\n",
      "epoch:0 step:697 [D loss: 0.536211, acc: 71.88%] [G loss: 3.032741]\n",
      "epoch:0 step:698 [D loss: 0.626407, acc: 68.75%] [G loss: 2.958959]\n",
      "epoch:0 step:699 [D loss: 0.505474, acc: 75.78%] [G loss: 2.768433]\n",
      "epoch:0 step:700 [D loss: 0.455830, acc: 78.12%] [G loss: 2.779887]\n",
      "epoch:0 step:701 [D loss: 0.634203, acc: 65.62%] [G loss: 3.197649]\n",
      "epoch:0 step:702 [D loss: 0.601256, acc: 68.75%] [G loss: 2.924951]\n",
      "epoch:0 step:703 [D loss: 0.623123, acc: 66.41%] [G loss: 2.780913]\n",
      "epoch:0 step:704 [D loss: 0.552113, acc: 71.09%] [G loss: 3.175520]\n",
      "epoch:0 step:705 [D loss: 0.558757, acc: 69.53%] [G loss: 3.131564]\n",
      "epoch:0 step:706 [D loss: 0.472234, acc: 78.91%] [G loss: 3.348458]\n",
      "epoch:0 step:707 [D loss: 0.512279, acc: 75.00%] [G loss: 3.279673]\n",
      "epoch:0 step:708 [D loss: 0.451016, acc: 79.69%] [G loss: 3.144523]\n",
      "epoch:0 step:709 [D loss: 0.494561, acc: 77.34%] [G loss: 3.064815]\n",
      "epoch:0 step:710 [D loss: 0.727286, acc: 57.03%] [G loss: 2.850470]\n",
      "epoch:0 step:711 [D loss: 0.548059, acc: 74.22%] [G loss: 2.770493]\n",
      "epoch:0 step:712 [D loss: 0.592087, acc: 68.75%] [G loss: 3.217384]\n",
      "epoch:0 step:713 [D loss: 0.585104, acc: 67.97%] [G loss: 3.147807]\n",
      "epoch:0 step:714 [D loss: 0.629781, acc: 64.84%] [G loss: 2.917104]\n",
      "epoch:0 step:715 [D loss: 0.524107, acc: 72.66%] [G loss: 3.114981]\n",
      "epoch:0 step:716 [D loss: 0.592699, acc: 67.19%] [G loss: 2.807258]\n",
      "epoch:0 step:717 [D loss: 0.568608, acc: 69.53%] [G loss: 2.741859]\n",
      "epoch:0 step:718 [D loss: 0.559510, acc: 71.09%] [G loss: 2.870820]\n",
      "epoch:0 step:719 [D loss: 0.608080, acc: 66.41%] [G loss: 2.941518]\n",
      "epoch:0 step:720 [D loss: 0.528973, acc: 78.12%] [G loss: 3.109054]\n",
      "epoch:0 step:721 [D loss: 0.638555, acc: 66.41%] [G loss: 3.003753]\n",
      "epoch:0 step:722 [D loss: 0.556825, acc: 64.84%] [G loss: 3.164704]\n",
      "epoch:0 step:723 [D loss: 0.694328, acc: 60.16%] [G loss: 2.815416]\n",
      "epoch:0 step:724 [D loss: 0.512303, acc: 71.88%] [G loss: 2.650583]\n",
      "epoch:0 step:725 [D loss: 0.566397, acc: 71.88%] [G loss: 2.729785]\n",
      "epoch:0 step:726 [D loss: 0.525600, acc: 77.34%] [G loss: 2.857868]\n",
      "epoch:0 step:727 [D loss: 0.601127, acc: 60.16%] [G loss: 2.980334]\n",
      "epoch:0 step:728 [D loss: 0.599419, acc: 63.28%] [G loss: 2.797238]\n",
      "epoch:0 step:729 [D loss: 0.611461, acc: 68.75%] [G loss: 2.927584]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0 step:730 [D loss: 0.554968, acc: 71.88%] [G loss: 2.901856]\n",
      "epoch:0 step:731 [D loss: 0.509692, acc: 74.22%] [G loss: 2.876524]\n",
      "epoch:0 step:732 [D loss: 0.516450, acc: 73.44%] [G loss: 3.086832]\n",
      "epoch:0 step:733 [D loss: 0.487161, acc: 76.56%] [G loss: 3.134032]\n",
      "epoch:0 step:734 [D loss: 0.607901, acc: 63.28%] [G loss: 2.925665]\n",
      "epoch:0 step:735 [D loss: 0.650242, acc: 59.38%] [G loss: 2.666201]\n",
      "epoch:0 step:736 [D loss: 0.656073, acc: 67.97%] [G loss: 2.658832]\n",
      "epoch:0 step:737 [D loss: 0.586566, acc: 67.97%] [G loss: 2.571631]\n",
      "epoch:0 step:738 [D loss: 0.592669, acc: 71.09%] [G loss: 2.694236]\n",
      "epoch:0 step:739 [D loss: 0.601083, acc: 63.28%] [G loss: 2.717565]\n",
      "epoch:0 step:740 [D loss: 0.583334, acc: 66.41%] [G loss: 2.794650]\n",
      "epoch:0 step:741 [D loss: 0.665937, acc: 62.50%] [G loss: 2.739636]\n",
      "epoch:0 step:742 [D loss: 0.574616, acc: 70.31%] [G loss: 2.526485]\n",
      "epoch:0 step:743 [D loss: 0.597175, acc: 67.97%] [G loss: 2.835011]\n",
      "epoch:0 step:744 [D loss: 0.643083, acc: 65.62%] [G loss: 2.882369]\n",
      "epoch:0 step:745 [D loss: 0.562369, acc: 74.22%] [G loss: 3.001410]\n",
      "epoch:0 step:746 [D loss: 0.492792, acc: 80.47%] [G loss: 2.929429]\n",
      "epoch:0 step:747 [D loss: 0.528744, acc: 67.97%] [G loss: 3.029779]\n",
      "epoch:0 step:748 [D loss: 0.630616, acc: 64.06%] [G loss: 2.784870]\n",
      "epoch:0 step:749 [D loss: 0.627109, acc: 63.28%] [G loss: 2.878154]\n",
      "epoch:0 step:750 [D loss: 0.615453, acc: 67.19%] [G loss: 2.649958]\n",
      "epoch:0 step:751 [D loss: 0.549853, acc: 74.22%] [G loss: 2.899815]\n",
      "epoch:0 step:752 [D loss: 0.511798, acc: 72.66%] [G loss: 2.832492]\n",
      "epoch:0 step:753 [D loss: 0.512437, acc: 74.22%] [G loss: 2.909727]\n",
      "epoch:0 step:754 [D loss: 0.503120, acc: 75.00%] [G loss: 3.024215]\n",
      "epoch:0 step:755 [D loss: 0.507325, acc: 74.22%] [G loss: 2.846201]\n",
      "epoch:0 step:756 [D loss: 0.525884, acc: 72.66%] [G loss: 2.890691]\n",
      "epoch:0 step:757 [D loss: 0.495434, acc: 69.53%] [G loss: 2.989228]\n",
      "epoch:0 step:758 [D loss: 0.670747, acc: 64.84%] [G loss: 2.907213]\n",
      "epoch:0 step:759 [D loss: 0.612563, acc: 67.19%] [G loss: 2.873464]\n",
      "epoch:0 step:760 [D loss: 0.570075, acc: 69.53%] [G loss: 2.835906]\n",
      "epoch:0 step:761 [D loss: 0.564189, acc: 72.66%] [G loss: 3.070112]\n",
      "epoch:0 step:762 [D loss: 0.548630, acc: 70.31%] [G loss: 2.879039]\n",
      "epoch:0 step:763 [D loss: 0.514790, acc: 77.34%] [G loss: 3.000982]\n",
      "epoch:0 step:764 [D loss: 0.552659, acc: 72.66%] [G loss: 2.918613]\n",
      "epoch:0 step:765 [D loss: 0.586966, acc: 69.53%] [G loss: 2.614690]\n",
      "epoch:0 step:766 [D loss: 0.595026, acc: 73.44%] [G loss: 2.855298]\n",
      "epoch:0 step:767 [D loss: 0.597545, acc: 67.19%] [G loss: 2.787270]\n",
      "epoch:0 step:768 [D loss: 0.579502, acc: 71.09%] [G loss: 2.792054]\n",
      "epoch:0 step:769 [D loss: 0.598567, acc: 66.41%] [G loss: 2.911561]\n",
      "epoch:0 step:770 [D loss: 0.650346, acc: 63.28%] [G loss: 2.710599]\n",
      "epoch:0 step:771 [D loss: 0.546553, acc: 72.66%] [G loss: 2.739277]\n",
      "epoch:0 step:772 [D loss: 0.626133, acc: 64.06%] [G loss: 2.851649]\n",
      "epoch:0 step:773 [D loss: 0.608338, acc: 64.06%] [G loss: 2.727132]\n",
      "epoch:0 step:774 [D loss: 0.617750, acc: 63.28%] [G loss: 2.470541]\n",
      "epoch:0 step:775 [D loss: 0.516615, acc: 76.56%] [G loss: 2.818682]\n",
      "epoch:0 step:776 [D loss: 0.627665, acc: 64.06%] [G loss: 2.585219]\n",
      "epoch:0 step:777 [D loss: 0.517977, acc: 75.00%] [G loss: 2.713666]\n",
      "epoch:0 step:778 [D loss: 0.594748, acc: 67.19%] [G loss: 2.579889]\n",
      "epoch:0 step:779 [D loss: 0.608514, acc: 64.06%] [G loss: 2.669011]\n",
      "epoch:0 step:780 [D loss: 0.571494, acc: 67.19%] [G loss: 2.506471]\n",
      "epoch:0 step:781 [D loss: 0.542211, acc: 72.66%] [G loss: 2.806557]\n",
      "epoch:0 step:782 [D loss: 0.543355, acc: 72.66%] [G loss: 2.591032]\n",
      "epoch:0 step:783 [D loss: 0.497907, acc: 75.00%] [G loss: 2.739846]\n",
      "epoch:0 step:784 [D loss: 0.644632, acc: 60.16%] [G loss: 2.718740]\n",
      "epoch:0 step:785 [D loss: 0.513022, acc: 78.91%] [G loss: 2.618448]\n",
      "epoch:0 step:786 [D loss: 0.471687, acc: 77.34%] [G loss: 2.842546]\n",
      "epoch:0 step:787 [D loss: 0.570269, acc: 69.53%] [G loss: 2.869120]\n",
      "epoch:0 step:788 [D loss: 0.637601, acc: 62.50%] [G loss: 2.618751]\n",
      "epoch:0 step:789 [D loss: 0.566157, acc: 71.09%] [G loss: 2.675975]\n",
      "epoch:0 step:790 [D loss: 0.636889, acc: 63.28%] [G loss: 2.973527]\n",
      "epoch:0 step:791 [D loss: 0.654448, acc: 67.19%] [G loss: 2.939883]\n",
      "epoch:0 step:792 [D loss: 0.492854, acc: 72.66%] [G loss: 3.180811]\n",
      "epoch:0 step:793 [D loss: 0.574953, acc: 69.53%] [G loss: 2.852965]\n",
      "epoch:0 step:794 [D loss: 0.660468, acc: 65.62%] [G loss: 2.721885]\n",
      "epoch:0 step:795 [D loss: 0.588791, acc: 68.75%] [G loss: 2.932043]\n",
      "epoch:0 step:796 [D loss: 0.544718, acc: 73.44%] [G loss: 2.788140]\n",
      "epoch:0 step:797 [D loss: 0.547719, acc: 71.09%] [G loss: 2.858756]\n",
      "epoch:0 step:798 [D loss: 0.493369, acc: 78.91%] [G loss: 2.968963]\n",
      "epoch:0 step:799 [D loss: 0.585944, acc: 67.19%] [G loss: 2.787825]\n",
      "epoch:0 step:800 [D loss: 0.637648, acc: 69.53%] [G loss: 2.734296]\n",
      "##############\n",
      "[4.55263526 3.20073179 8.6211654  6.96544119 5.94061976 7.90872623\n",
      " 6.92895126 6.86024795 7.09481729 5.49164616]\n",
      "##########\n",
      "epoch:0 step:801 [D loss: 0.531312, acc: 74.22%] [G loss: 2.962995]\n",
      "epoch:0 step:802 [D loss: 0.568227, acc: 71.09%] [G loss: 2.855452]\n",
      "epoch:0 step:803 [D loss: 0.591215, acc: 66.41%] [G loss: 2.932699]\n",
      "epoch:0 step:804 [D loss: 0.468239, acc: 81.25%] [G loss: 2.931352]\n",
      "epoch:0 step:805 [D loss: 0.467962, acc: 82.03%] [G loss: 3.022841]\n",
      "epoch:0 step:806 [D loss: 0.418697, acc: 82.81%] [G loss: 3.225672]\n",
      "epoch:0 step:807 [D loss: 0.498956, acc: 78.12%] [G loss: 3.084933]\n",
      "epoch:0 step:808 [D loss: 0.638890, acc: 69.53%] [G loss: 2.857650]\n",
      "epoch:0 step:809 [D loss: 0.602717, acc: 67.97%] [G loss: 2.590098]\n",
      "epoch:0 step:810 [D loss: 0.581751, acc: 72.66%] [G loss: 2.990411]\n",
      "epoch:0 step:811 [D loss: 0.579131, acc: 64.84%] [G loss: 2.775549]\n",
      "epoch:0 step:812 [D loss: 0.624303, acc: 62.50%] [G loss: 2.869884]\n",
      "epoch:0 step:813 [D loss: 0.609773, acc: 68.75%] [G loss: 2.802886]\n",
      "epoch:0 step:814 [D loss: 0.597918, acc: 70.31%] [G loss: 2.910457]\n",
      "epoch:0 step:815 [D loss: 0.586279, acc: 70.31%] [G loss: 2.698437]\n",
      "epoch:0 step:816 [D loss: 0.511763, acc: 76.56%] [G loss: 2.907460]\n",
      "epoch:0 step:817 [D loss: 0.571694, acc: 70.31%] [G loss: 3.111196]\n",
      "epoch:0 step:818 [D loss: 0.617018, acc: 63.28%] [G loss: 2.883428]\n",
      "epoch:0 step:819 [D loss: 0.486580, acc: 76.56%] [G loss: 2.727951]\n",
      "epoch:0 step:820 [D loss: 0.613312, acc: 64.84%] [G loss: 2.780093]\n",
      "epoch:0 step:821 [D loss: 0.540288, acc: 71.88%] [G loss: 2.751175]\n",
      "epoch:0 step:822 [D loss: 0.511575, acc: 78.12%] [G loss: 2.897112]\n",
      "epoch:0 step:823 [D loss: 0.541586, acc: 73.44%] [G loss: 2.755641]\n",
      "epoch:0 step:824 [D loss: 0.543743, acc: 73.44%] [G loss: 2.698916]\n",
      "epoch:0 step:825 [D loss: 0.483744, acc: 76.56%] [G loss: 2.867379]\n",
      "epoch:0 step:826 [D loss: 0.515838, acc: 74.22%] [G loss: 2.994438]\n",
      "epoch:0 step:827 [D loss: 0.543172, acc: 71.88%] [G loss: 2.753508]\n",
      "epoch:0 step:828 [D loss: 0.576912, acc: 67.19%] [G loss: 3.013173]\n",
      "epoch:0 step:829 [D loss: 0.480945, acc: 79.69%] [G loss: 2.985635]\n",
      "epoch:0 step:830 [D loss: 0.443744, acc: 81.25%] [G loss: 2.944316]\n",
      "epoch:0 step:831 [D loss: 0.554331, acc: 72.66%] [G loss: 2.855453]\n",
      "epoch:0 step:832 [D loss: 0.468489, acc: 79.69%] [G loss: 3.091587]\n",
      "epoch:0 step:833 [D loss: 0.531405, acc: 74.22%] [G loss: 2.960559]\n",
      "epoch:0 step:834 [D loss: 0.570512, acc: 71.88%] [G loss: 2.926995]\n",
      "epoch:0 step:835 [D loss: 0.487378, acc: 78.12%] [G loss: 3.082538]\n",
      "epoch:0 step:836 [D loss: 0.613066, acc: 71.09%] [G loss: 3.017525]\n",
      "epoch:0 step:837 [D loss: 0.567288, acc: 71.09%] [G loss: 3.091990]\n",
      "epoch:0 step:838 [D loss: 0.541329, acc: 71.88%] [G loss: 3.165428]\n",
      "epoch:0 step:839 [D loss: 0.534133, acc: 70.31%] [G loss: 2.811491]\n",
      "epoch:0 step:840 [D loss: 0.548204, acc: 70.31%] [G loss: 2.909373]\n",
      "epoch:0 step:841 [D loss: 0.592927, acc: 66.41%] [G loss: 3.056847]\n",
      "epoch:0 step:842 [D loss: 0.508326, acc: 71.09%] [G loss: 2.976165]\n",
      "epoch:0 step:843 [D loss: 0.600282, acc: 71.09%] [G loss: 2.950284]\n",
      "epoch:0 step:844 [D loss: 0.568750, acc: 71.88%] [G loss: 3.243648]\n",
      "epoch:0 step:845 [D loss: 0.572985, acc: 74.22%] [G loss: 2.922764]\n",
      "epoch:0 step:846 [D loss: 0.619764, acc: 63.28%] [G loss: 2.707289]\n",
      "epoch:0 step:847 [D loss: 0.496123, acc: 75.78%] [G loss: 3.032593]\n",
      "epoch:0 step:848 [D loss: 0.597652, acc: 66.41%] [G loss: 2.827786]\n",
      "epoch:0 step:849 [D loss: 0.575278, acc: 70.31%] [G loss: 2.742242]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0 step:850 [D loss: 0.587875, acc: 64.84%] [G loss: 3.009340]\n",
      "epoch:0 step:851 [D loss: 0.486137, acc: 76.56%] [G loss: 2.922434]\n",
      "epoch:0 step:852 [D loss: 0.508826, acc: 72.66%] [G loss: 2.975181]\n",
      "epoch:0 step:853 [D loss: 0.466226, acc: 79.69%] [G loss: 3.377934]\n",
      "epoch:0 step:854 [D loss: 0.567178, acc: 67.19%] [G loss: 2.987964]\n",
      "epoch:0 step:855 [D loss: 0.592385, acc: 71.88%] [G loss: 3.280328]\n",
      "epoch:0 step:856 [D loss: 0.613714, acc: 68.75%] [G loss: 2.982290]\n",
      "epoch:0 step:857 [D loss: 0.640895, acc: 66.41%] [G loss: 3.176605]\n",
      "epoch:0 step:858 [D loss: 0.710615, acc: 56.25%] [G loss: 2.862476]\n",
      "epoch:0 step:859 [D loss: 0.562040, acc: 69.53%] [G loss: 2.731364]\n",
      "epoch:0 step:860 [D loss: 0.508407, acc: 76.56%] [G loss: 3.236397]\n",
      "epoch:0 step:861 [D loss: 0.584160, acc: 71.88%] [G loss: 2.444352]\n",
      "epoch:0 step:862 [D loss: 0.540813, acc: 71.88%] [G loss: 2.751980]\n",
      "epoch:0 step:863 [D loss: 0.545788, acc: 72.66%] [G loss: 2.818773]\n",
      "epoch:0 step:864 [D loss: 0.515979, acc: 76.56%] [G loss: 2.860704]\n",
      "epoch:0 step:865 [D loss: 0.590269, acc: 71.88%] [G loss: 2.785822]\n",
      "epoch:0 step:866 [D loss: 0.536473, acc: 81.25%] [G loss: 2.784031]\n",
      "epoch:0 step:867 [D loss: 0.557717, acc: 71.09%] [G loss: 3.032912]\n",
      "epoch:0 step:868 [D loss: 0.618937, acc: 64.06%] [G loss: 2.795759]\n",
      "epoch:0 step:869 [D loss: 0.472900, acc: 78.91%] [G loss: 2.830894]\n",
      "epoch:0 step:870 [D loss: 0.552462, acc: 71.09%] [G loss: 3.031435]\n",
      "epoch:0 step:871 [D loss: 0.548796, acc: 69.53%] [G loss: 3.119071]\n",
      "epoch:0 step:872 [D loss: 0.546275, acc: 76.56%] [G loss: 3.000935]\n",
      "epoch:0 step:873 [D loss: 0.561506, acc: 68.75%] [G loss: 2.902123]\n",
      "epoch:0 step:874 [D loss: 0.590501, acc: 68.75%] [G loss: 2.706568]\n",
      "epoch:0 step:875 [D loss: 0.441492, acc: 82.81%] [G loss: 2.943983]\n",
      "epoch:0 step:876 [D loss: 0.604012, acc: 64.84%] [G loss: 2.954293]\n",
      "epoch:0 step:877 [D loss: 0.541769, acc: 73.44%] [G loss: 2.811499]\n",
      "epoch:0 step:878 [D loss: 0.562020, acc: 68.75%] [G loss: 3.150257]\n",
      "epoch:0 step:879 [D loss: 0.591461, acc: 71.88%] [G loss: 2.710672]\n",
      "epoch:0 step:880 [D loss: 0.506249, acc: 76.56%] [G loss: 2.742045]\n",
      "epoch:0 step:881 [D loss: 0.601016, acc: 66.41%] [G loss: 2.831903]\n",
      "epoch:0 step:882 [D loss: 0.651643, acc: 64.84%] [G loss: 2.769848]\n",
      "epoch:0 step:883 [D loss: 0.544882, acc: 75.00%] [G loss: 2.963223]\n",
      "epoch:0 step:884 [D loss: 0.575126, acc: 68.75%] [G loss: 2.979123]\n",
      "epoch:0 step:885 [D loss: 0.477383, acc: 77.34%] [G loss: 2.901244]\n",
      "epoch:0 step:886 [D loss: 0.447352, acc: 79.69%] [G loss: 3.515793]\n",
      "epoch:0 step:887 [D loss: 0.561066, acc: 69.53%] [G loss: 3.161217]\n",
      "epoch:0 step:888 [D loss: 0.567223, acc: 73.44%] [G loss: 3.276577]\n",
      "epoch:0 step:889 [D loss: 0.450552, acc: 79.69%] [G loss: 3.159734]\n",
      "epoch:0 step:890 [D loss: 0.510703, acc: 80.47%] [G loss: 3.117794]\n",
      "epoch:0 step:891 [D loss: 0.621788, acc: 68.75%] [G loss: 2.689527]\n",
      "epoch:0 step:892 [D loss: 0.588694, acc: 72.66%] [G loss: 2.843038]\n",
      "epoch:0 step:893 [D loss: 0.532275, acc: 75.00%] [G loss: 3.071325]\n",
      "epoch:0 step:894 [D loss: 0.487215, acc: 75.78%] [G loss: 3.105140]\n",
      "epoch:0 step:895 [D loss: 0.656088, acc: 65.62%] [G loss: 2.948415]\n",
      "epoch:0 step:896 [D loss: 0.628130, acc: 61.72%] [G loss: 2.846114]\n",
      "epoch:0 step:897 [D loss: 0.495025, acc: 78.12%] [G loss: 3.043466]\n",
      "epoch:0 step:898 [D loss: 0.586434, acc: 68.75%] [G loss: 2.853352]\n",
      "epoch:0 step:899 [D loss: 0.483622, acc: 73.44%] [G loss: 2.960480]\n",
      "epoch:0 step:900 [D loss: 0.516146, acc: 74.22%] [G loss: 2.867640]\n",
      "epoch:0 step:901 [D loss: 0.446261, acc: 80.47%] [G loss: 3.013669]\n",
      "epoch:0 step:902 [D loss: 0.627426, acc: 67.97%] [G loss: 2.847228]\n",
      "epoch:0 step:903 [D loss: 0.592709, acc: 65.62%] [G loss: 2.898458]\n",
      "epoch:0 step:904 [D loss: 0.602715, acc: 69.53%] [G loss: 2.891808]\n",
      "epoch:0 step:905 [D loss: 0.528220, acc: 77.34%] [G loss: 2.811129]\n",
      "epoch:0 step:906 [D loss: 0.531780, acc: 73.44%] [G loss: 3.029133]\n",
      "epoch:0 step:907 [D loss: 0.733699, acc: 54.69%] [G loss: 2.834103]\n",
      "epoch:0 step:908 [D loss: 0.539959, acc: 77.34%] [G loss: 2.821468]\n",
      "epoch:0 step:909 [D loss: 0.549524, acc: 73.44%] [G loss: 2.780159]\n",
      "epoch:0 step:910 [D loss: 0.488030, acc: 75.78%] [G loss: 3.092575]\n",
      "epoch:0 step:911 [D loss: 0.630211, acc: 58.59%] [G loss: 2.837000]\n",
      "epoch:0 step:912 [D loss: 0.472632, acc: 78.91%] [G loss: 2.959497]\n",
      "epoch:0 step:913 [D loss: 0.610643, acc: 67.19%] [G loss: 3.116390]\n",
      "epoch:0 step:914 [D loss: 0.470547, acc: 78.91%] [G loss: 3.495569]\n",
      "epoch:0 step:915 [D loss: 0.603415, acc: 63.28%] [G loss: 2.956970]\n",
      "epoch:0 step:916 [D loss: 0.507741, acc: 75.78%] [G loss: 2.719109]\n",
      "epoch:0 step:917 [D loss: 0.579600, acc: 69.53%] [G loss: 2.871726]\n",
      "epoch:0 step:918 [D loss: 0.466852, acc: 75.78%] [G loss: 3.123436]\n",
      "epoch:0 step:919 [D loss: 0.669275, acc: 67.19%] [G loss: 2.897406]\n",
      "epoch:0 step:920 [D loss: 0.685712, acc: 64.06%] [G loss: 3.070272]\n",
      "epoch:0 step:921 [D loss: 0.508906, acc: 71.09%] [G loss: 3.260050]\n",
      "epoch:0 step:922 [D loss: 0.630308, acc: 64.84%] [G loss: 3.059877]\n",
      "epoch:0 step:923 [D loss: 0.514306, acc: 75.78%] [G loss: 3.144739]\n",
      "epoch:0 step:924 [D loss: 0.469269, acc: 78.12%] [G loss: 2.979960]\n",
      "epoch:0 step:925 [D loss: 0.485698, acc: 78.12%] [G loss: 3.231650]\n",
      "epoch:0 step:926 [D loss: 0.484454, acc: 79.69%] [G loss: 3.217892]\n",
      "epoch:0 step:927 [D loss: 0.524481, acc: 74.22%] [G loss: 3.162323]\n",
      "epoch:0 step:928 [D loss: 0.678701, acc: 66.41%] [G loss: 3.636784]\n",
      "epoch:0 step:929 [D loss: 0.418811, acc: 83.59%] [G loss: 3.876189]\n",
      "epoch:0 step:930 [D loss: 0.512880, acc: 75.78%] [G loss: 2.992094]\n",
      "epoch:0 step:931 [D loss: 0.669496, acc: 63.28%] [G loss: 2.754618]\n",
      "epoch:0 step:932 [D loss: 0.562536, acc: 71.09%] [G loss: 2.810960]\n",
      "epoch:0 step:933 [D loss: 0.484653, acc: 78.91%] [G loss: 3.106543]\n",
      "epoch:0 step:934 [D loss: 0.529831, acc: 70.31%] [G loss: 3.102102]\n",
      "epoch:0 step:935 [D loss: 0.502937, acc: 75.78%] [G loss: 3.505387]\n",
      "epoch:0 step:936 [D loss: 0.371918, acc: 85.94%] [G loss: 3.500534]\n",
      "epoch:0 step:937 [D loss: 0.676679, acc: 66.41%] [G loss: 3.737636]\n",
      "epoch:1 step:938 [D loss: 0.627755, acc: 64.06%] [G loss: 3.381667]\n",
      "epoch:1 step:939 [D loss: 0.580796, acc: 72.66%] [G loss: 3.092587]\n",
      "epoch:1 step:940 [D loss: 0.561838, acc: 70.31%] [G loss: 2.882477]\n",
      "epoch:1 step:941 [D loss: 0.546206, acc: 71.88%] [G loss: 2.973700]\n",
      "epoch:1 step:942 [D loss: 0.529894, acc: 79.69%] [G loss: 3.178540]\n",
      "epoch:1 step:943 [D loss: 0.590525, acc: 68.75%] [G loss: 3.222670]\n",
      "epoch:1 step:944 [D loss: 0.536905, acc: 71.09%] [G loss: 3.038891]\n",
      "epoch:1 step:945 [D loss: 0.492091, acc: 77.34%] [G loss: 2.722079]\n",
      "epoch:1 step:946 [D loss: 0.492748, acc: 76.56%] [G loss: 2.981400]\n",
      "epoch:1 step:947 [D loss: 0.630937, acc: 67.19%] [G loss: 2.900634]\n",
      "epoch:1 step:948 [D loss: 0.515696, acc: 68.75%] [G loss: 3.174642]\n",
      "epoch:1 step:949 [D loss: 0.502927, acc: 74.22%] [G loss: 2.834183]\n",
      "epoch:1 step:950 [D loss: 0.579759, acc: 65.62%] [G loss: 2.902864]\n",
      "epoch:1 step:951 [D loss: 0.586893, acc: 65.62%] [G loss: 3.022590]\n",
      "epoch:1 step:952 [D loss: 0.492281, acc: 77.34%] [G loss: 2.891226]\n",
      "epoch:1 step:953 [D loss: 0.510298, acc: 76.56%] [G loss: 2.893485]\n",
      "epoch:1 step:954 [D loss: 0.599237, acc: 69.53%] [G loss: 2.730673]\n",
      "epoch:1 step:955 [D loss: 0.582603, acc: 69.53%] [G loss: 2.594957]\n",
      "epoch:1 step:956 [D loss: 0.544470, acc: 74.22%] [G loss: 3.042088]\n",
      "epoch:1 step:957 [D loss: 0.626435, acc: 63.28%] [G loss: 2.752071]\n",
      "epoch:1 step:958 [D loss: 0.575696, acc: 70.31%] [G loss: 2.753617]\n",
      "epoch:1 step:959 [D loss: 0.502362, acc: 71.09%] [G loss: 3.277007]\n",
      "epoch:1 step:960 [D loss: 0.528434, acc: 71.09%] [G loss: 2.806754]\n",
      "epoch:1 step:961 [D loss: 0.631844, acc: 61.72%] [G loss: 2.655555]\n",
      "epoch:1 step:962 [D loss: 0.592204, acc: 69.53%] [G loss: 2.909448]\n",
      "epoch:1 step:963 [D loss: 0.561224, acc: 69.53%] [G loss: 2.803997]\n",
      "epoch:1 step:964 [D loss: 0.575525, acc: 71.09%] [G loss: 3.069273]\n",
      "epoch:1 step:965 [D loss: 0.568148, acc: 71.09%] [G loss: 2.644814]\n",
      "epoch:1 step:966 [D loss: 0.521276, acc: 78.91%] [G loss: 2.774724]\n",
      "epoch:1 step:967 [D loss: 0.644770, acc: 61.72%] [G loss: 2.772127]\n",
      "epoch:1 step:968 [D loss: 0.583334, acc: 70.31%] [G loss: 2.878677]\n",
      "epoch:1 step:969 [D loss: 0.504558, acc: 77.34%] [G loss: 2.996843]\n",
      "epoch:1 step:970 [D loss: 0.489760, acc: 75.00%] [G loss: 2.661980]\n",
      "epoch:1 step:971 [D loss: 0.565746, acc: 72.66%] [G loss: 2.834595]\n",
      "epoch:1 step:972 [D loss: 0.586109, acc: 67.97%] [G loss: 2.986626]\n",
      "epoch:1 step:973 [D loss: 0.539882, acc: 75.00%] [G loss: 2.894378]\n",
      "epoch:1 step:974 [D loss: 0.622113, acc: 64.06%] [G loss: 3.085770]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:1 step:975 [D loss: 0.569010, acc: 69.53%] [G loss: 3.079136]\n",
      "epoch:1 step:976 [D loss: 0.567576, acc: 75.00%] [G loss: 2.826886]\n",
      "epoch:1 step:977 [D loss: 0.538797, acc: 77.34%] [G loss: 3.276625]\n",
      "epoch:1 step:978 [D loss: 0.526398, acc: 75.00%] [G loss: 2.755561]\n",
      "epoch:1 step:979 [D loss: 0.633290, acc: 64.84%] [G loss: 2.784681]\n",
      "epoch:1 step:980 [D loss: 0.553025, acc: 71.09%] [G loss: 3.016616]\n",
      "epoch:1 step:981 [D loss: 0.605493, acc: 64.84%] [G loss: 2.743377]\n",
      "epoch:1 step:982 [D loss: 0.547511, acc: 67.97%] [G loss: 3.095552]\n",
      "epoch:1 step:983 [D loss: 0.558283, acc: 72.66%] [G loss: 2.894179]\n",
      "epoch:1 step:984 [D loss: 0.646109, acc: 67.19%] [G loss: 2.745719]\n",
      "epoch:1 step:985 [D loss: 0.567413, acc: 72.66%] [G loss: 2.501318]\n",
      "epoch:1 step:986 [D loss: 0.618937, acc: 63.28%] [G loss: 2.818234]\n",
      "epoch:1 step:987 [D loss: 0.517036, acc: 73.44%] [G loss: 2.692818]\n",
      "epoch:1 step:988 [D loss: 0.572953, acc: 66.41%] [G loss: 2.886644]\n",
      "epoch:1 step:989 [D loss: 0.638519, acc: 65.62%] [G loss: 2.605529]\n",
      "epoch:1 step:990 [D loss: 0.550433, acc: 71.09%] [G loss: 3.126615]\n",
      "epoch:1 step:991 [D loss: 0.540067, acc: 75.78%] [G loss: 2.758914]\n",
      "epoch:1 step:992 [D loss: 0.528040, acc: 70.31%] [G loss: 2.778639]\n",
      "epoch:1 step:993 [D loss: 0.583558, acc: 69.53%] [G loss: 2.738236]\n",
      "epoch:1 step:994 [D loss: 0.542291, acc: 73.44%] [G loss: 3.098376]\n",
      "epoch:1 step:995 [D loss: 0.643531, acc: 66.41%] [G loss: 2.887114]\n",
      "epoch:1 step:996 [D loss: 0.447920, acc: 77.34%] [G loss: 2.869339]\n",
      "epoch:1 step:997 [D loss: 0.535339, acc: 75.00%] [G loss: 3.024099]\n",
      "epoch:1 step:998 [D loss: 0.612698, acc: 67.19%] [G loss: 2.841761]\n",
      "epoch:1 step:999 [D loss: 0.678266, acc: 57.03%] [G loss: 2.674972]\n",
      "epoch:1 step:1000 [D loss: 0.472022, acc: 75.00%] [G loss: 2.949088]\n",
      "##############\n",
      "[4.12878258 2.52145166 8.0174008  6.41241786 5.41699107 7.39065746\n",
      " 6.5344363  6.28862861 6.57753722 4.84959304]\n",
      "##########\n",
      "epoch:1 step:1001 [D loss: 0.546383, acc: 67.97%] [G loss: 3.082653]\n",
      "epoch:1 step:1002 [D loss: 0.573854, acc: 71.09%] [G loss: 2.675037]\n",
      "epoch:1 step:1003 [D loss: 0.554027, acc: 73.44%] [G loss: 2.872269]\n",
      "epoch:1 step:1004 [D loss: 0.551987, acc: 75.00%] [G loss: 3.108550]\n",
      "epoch:1 step:1005 [D loss: 0.579863, acc: 73.44%] [G loss: 2.835979]\n",
      "epoch:1 step:1006 [D loss: 0.591278, acc: 64.84%] [G loss: 2.735006]\n",
      "epoch:1 step:1007 [D loss: 0.576865, acc: 67.19%] [G loss: 2.963211]\n",
      "epoch:1 step:1008 [D loss: 0.474889, acc: 80.47%] [G loss: 2.881102]\n",
      "epoch:1 step:1009 [D loss: 0.593813, acc: 70.31%] [G loss: 2.977941]\n",
      "epoch:1 step:1010 [D loss: 0.526455, acc: 76.56%] [G loss: 2.974388]\n",
      "epoch:1 step:1011 [D loss: 0.521464, acc: 77.34%] [G loss: 2.977553]\n",
      "epoch:1 step:1012 [D loss: 0.537425, acc: 70.31%] [G loss: 3.348561]\n",
      "epoch:1 step:1013 [D loss: 0.502138, acc: 76.56%] [G loss: 3.266565]\n",
      "epoch:1 step:1014 [D loss: 0.505063, acc: 72.66%] [G loss: 3.296499]\n",
      "epoch:1 step:1015 [D loss: 0.549224, acc: 77.34%] [G loss: 3.025743]\n",
      "epoch:1 step:1016 [D loss: 0.619000, acc: 63.28%] [G loss: 2.703084]\n",
      "epoch:1 step:1017 [D loss: 0.613251, acc: 67.19%] [G loss: 2.882247]\n",
      "epoch:1 step:1018 [D loss: 0.550565, acc: 71.09%] [G loss: 2.695254]\n",
      "epoch:1 step:1019 [D loss: 0.531437, acc: 71.09%] [G loss: 2.796055]\n",
      "epoch:1 step:1020 [D loss: 0.504105, acc: 75.00%] [G loss: 2.836897]\n",
      "epoch:1 step:1021 [D loss: 0.550860, acc: 71.88%] [G loss: 2.969090]\n",
      "epoch:1 step:1022 [D loss: 0.524477, acc: 73.44%] [G loss: 3.049481]\n",
      "epoch:1 step:1023 [D loss: 0.564933, acc: 67.97%] [G loss: 2.888478]\n",
      "epoch:1 step:1024 [D loss: 0.543753, acc: 75.00%] [G loss: 2.952404]\n",
      "epoch:1 step:1025 [D loss: 0.568119, acc: 69.53%] [G loss: 3.134196]\n",
      "epoch:1 step:1026 [D loss: 0.509345, acc: 75.00%] [G loss: 3.030566]\n",
      "epoch:1 step:1027 [D loss: 0.602254, acc: 67.97%] [G loss: 2.844781]\n",
      "epoch:1 step:1028 [D loss: 0.521435, acc: 75.78%] [G loss: 2.881510]\n",
      "epoch:1 step:1029 [D loss: 0.491296, acc: 74.22%] [G loss: 2.947184]\n",
      "epoch:1 step:1030 [D loss: 0.491861, acc: 74.22%] [G loss: 2.768733]\n",
      "epoch:1 step:1031 [D loss: 0.552855, acc: 72.66%] [G loss: 3.103403]\n",
      "epoch:1 step:1032 [D loss: 0.485061, acc: 73.44%] [G loss: 2.900326]\n",
      "epoch:1 step:1033 [D loss: 0.512295, acc: 77.34%] [G loss: 3.018759]\n",
      "epoch:1 step:1034 [D loss: 0.542042, acc: 69.53%] [G loss: 3.058345]\n",
      "epoch:1 step:1035 [D loss: 0.627232, acc: 66.41%] [G loss: 3.044961]\n",
      "epoch:1 step:1036 [D loss: 0.517999, acc: 75.00%] [G loss: 3.043894]\n",
      "epoch:1 step:1037 [D loss: 0.534221, acc: 73.44%] [G loss: 2.953198]\n",
      "epoch:1 step:1038 [D loss: 0.647530, acc: 64.84%] [G loss: 2.871873]\n",
      "epoch:1 step:1039 [D loss: 0.534056, acc: 72.66%] [G loss: 3.118359]\n",
      "epoch:1 step:1040 [D loss: 0.469582, acc: 78.12%] [G loss: 3.089900]\n",
      "epoch:1 step:1041 [D loss: 0.542899, acc: 75.00%] [G loss: 2.822123]\n",
      "epoch:1 step:1042 [D loss: 0.545008, acc: 70.31%] [G loss: 3.001802]\n",
      "epoch:1 step:1043 [D loss: 0.602075, acc: 71.09%] [G loss: 2.769742]\n",
      "epoch:1 step:1044 [D loss: 0.659042, acc: 58.59%] [G loss: 2.839525]\n",
      "epoch:1 step:1045 [D loss: 0.518054, acc: 75.00%] [G loss: 2.981812]\n",
      "epoch:1 step:1046 [D loss: 0.569206, acc: 70.31%] [G loss: 2.959914]\n",
      "epoch:1 step:1047 [D loss: 0.468167, acc: 78.91%] [G loss: 3.157793]\n",
      "epoch:1 step:1048 [D loss: 0.623141, acc: 64.06%] [G loss: 2.938890]\n",
      "epoch:1 step:1049 [D loss: 0.502543, acc: 78.12%] [G loss: 2.979588]\n",
      "epoch:1 step:1050 [D loss: 0.503078, acc: 76.56%] [G loss: 2.641053]\n",
      "epoch:1 step:1051 [D loss: 0.602488, acc: 67.97%] [G loss: 3.016269]\n",
      "epoch:1 step:1052 [D loss: 0.633409, acc: 68.75%] [G loss: 2.927000]\n",
      "epoch:1 step:1053 [D loss: 0.526575, acc: 74.22%] [G loss: 2.956676]\n",
      "epoch:1 step:1054 [D loss: 0.717982, acc: 60.16%] [G loss: 2.686407]\n",
      "epoch:1 step:1055 [D loss: 0.612133, acc: 61.72%] [G loss: 2.893758]\n",
      "epoch:1 step:1056 [D loss: 0.506492, acc: 75.00%] [G loss: 2.970983]\n",
      "epoch:1 step:1057 [D loss: 0.644208, acc: 65.62%] [G loss: 3.013010]\n",
      "epoch:1 step:1058 [D loss: 0.582974, acc: 67.97%] [G loss: 2.996959]\n",
      "epoch:1 step:1059 [D loss: 0.565003, acc: 68.75%] [G loss: 3.016557]\n",
      "epoch:1 step:1060 [D loss: 0.548693, acc: 71.88%] [G loss: 2.811057]\n",
      "epoch:1 step:1061 [D loss: 0.635489, acc: 68.75%] [G loss: 2.880607]\n",
      "epoch:1 step:1062 [D loss: 0.513894, acc: 74.22%] [G loss: 3.076417]\n",
      "epoch:1 step:1063 [D loss: 0.521201, acc: 71.09%] [G loss: 2.868747]\n",
      "epoch:1 step:1064 [D loss: 0.548139, acc: 71.09%] [G loss: 2.824675]\n",
      "epoch:1 step:1065 [D loss: 0.552451, acc: 69.53%] [G loss: 2.761023]\n",
      "epoch:1 step:1066 [D loss: 0.668385, acc: 61.72%] [G loss: 2.848746]\n",
      "epoch:1 step:1067 [D loss: 0.494523, acc: 78.12%] [G loss: 2.759449]\n",
      "epoch:1 step:1068 [D loss: 0.549572, acc: 74.22%] [G loss: 2.976681]\n",
      "epoch:1 step:1069 [D loss: 0.550874, acc: 71.88%] [G loss: 3.057847]\n",
      "epoch:1 step:1070 [D loss: 0.541177, acc: 72.66%] [G loss: 2.959225]\n",
      "epoch:1 step:1071 [D loss: 0.502746, acc: 74.22%] [G loss: 3.055285]\n",
      "epoch:1 step:1072 [D loss: 0.458662, acc: 78.12%] [G loss: 3.199752]\n",
      "epoch:1 step:1073 [D loss: 0.611796, acc: 66.41%] [G loss: 3.055927]\n",
      "epoch:1 step:1074 [D loss: 0.571918, acc: 71.09%] [G loss: 3.055084]\n",
      "epoch:1 step:1075 [D loss: 0.513436, acc: 74.22%] [G loss: 2.880604]\n",
      "epoch:1 step:1076 [D loss: 0.490864, acc: 73.44%] [G loss: 2.775706]\n",
      "epoch:1 step:1077 [D loss: 0.585482, acc: 67.97%] [G loss: 2.920742]\n",
      "epoch:1 step:1078 [D loss: 0.499138, acc: 79.69%] [G loss: 3.372323]\n",
      "epoch:1 step:1079 [D loss: 0.511694, acc: 72.66%] [G loss: 2.742831]\n",
      "epoch:1 step:1080 [D loss: 0.588857, acc: 70.31%] [G loss: 2.852288]\n",
      "epoch:1 step:1081 [D loss: 0.587875, acc: 66.41%] [G loss: 2.787057]\n",
      "epoch:1 step:1082 [D loss: 0.661086, acc: 67.97%] [G loss: 2.665931]\n",
      "epoch:1 step:1083 [D loss: 0.666360, acc: 60.16%] [G loss: 2.882515]\n",
      "epoch:1 step:1084 [D loss: 0.455643, acc: 82.03%] [G loss: 2.943972]\n",
      "epoch:1 step:1085 [D loss: 0.573688, acc: 74.22%] [G loss: 2.569509]\n",
      "epoch:1 step:1086 [D loss: 0.528354, acc: 75.00%] [G loss: 3.076548]\n",
      "epoch:1 step:1087 [D loss: 0.681266, acc: 64.84%] [G loss: 3.023215]\n",
      "epoch:1 step:1088 [D loss: 0.481983, acc: 74.22%] [G loss: 3.434176]\n",
      "epoch:1 step:1089 [D loss: 0.567637, acc: 68.75%] [G loss: 2.960268]\n",
      "epoch:1 step:1090 [D loss: 0.559831, acc: 66.41%] [G loss: 2.921810]\n",
      "epoch:1 step:1091 [D loss: 0.554251, acc: 69.53%] [G loss: 3.154954]\n",
      "epoch:1 step:1092 [D loss: 0.491634, acc: 73.44%] [G loss: 2.821953]\n",
      "epoch:1 step:1093 [D loss: 0.545625, acc: 72.66%] [G loss: 2.667863]\n",
      "epoch:1 step:1094 [D loss: 0.514037, acc: 75.78%] [G loss: 2.895527]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:1 step:1095 [D loss: 0.606443, acc: 70.31%] [G loss: 2.770231]\n",
      "epoch:1 step:1096 [D loss: 0.538054, acc: 72.66%] [G loss: 2.768984]\n",
      "epoch:1 step:1097 [D loss: 0.632342, acc: 62.50%] [G loss: 2.887352]\n",
      "epoch:1 step:1098 [D loss: 0.573956, acc: 71.88%] [G loss: 2.987914]\n",
      "epoch:1 step:1099 [D loss: 0.519728, acc: 72.66%] [G loss: 2.889844]\n",
      "epoch:1 step:1100 [D loss: 0.474582, acc: 78.12%] [G loss: 2.966688]\n",
      "epoch:1 step:1101 [D loss: 0.462427, acc: 78.91%] [G loss: 3.425448]\n",
      "epoch:1 step:1102 [D loss: 0.594843, acc: 64.84%] [G loss: 3.091453]\n",
      "epoch:1 step:1103 [D loss: 0.557197, acc: 68.75%] [G loss: 2.777283]\n",
      "epoch:1 step:1104 [D loss: 0.484409, acc: 75.00%] [G loss: 3.034931]\n",
      "epoch:1 step:1105 [D loss: 0.547510, acc: 71.88%] [G loss: 2.829013]\n",
      "epoch:1 step:1106 [D loss: 0.536767, acc: 70.31%] [G loss: 2.569515]\n",
      "epoch:1 step:1107 [D loss: 0.496735, acc: 73.44%] [G loss: 3.005699]\n",
      "epoch:1 step:1108 [D loss: 0.525038, acc: 74.22%] [G loss: 3.272138]\n",
      "epoch:1 step:1109 [D loss: 0.546899, acc: 68.75%] [G loss: 3.087300]\n",
      "epoch:1 step:1110 [D loss: 0.560865, acc: 67.97%] [G loss: 3.440155]\n",
      "epoch:1 step:1111 [D loss: 0.500043, acc: 75.00%] [G loss: 2.970845]\n",
      "epoch:1 step:1112 [D loss: 0.530533, acc: 72.66%] [G loss: 2.974624]\n",
      "epoch:1 step:1113 [D loss: 0.631518, acc: 59.38%] [G loss: 3.144073]\n",
      "epoch:1 step:1114 [D loss: 0.576203, acc: 66.41%] [G loss: 2.944973]\n",
      "epoch:1 step:1115 [D loss: 0.470583, acc: 77.34%] [G loss: 3.429128]\n",
      "epoch:1 step:1116 [D loss: 0.574062, acc: 71.09%] [G loss: 3.033120]\n",
      "epoch:1 step:1117 [D loss: 0.547843, acc: 67.97%] [G loss: 2.906882]\n",
      "epoch:1 step:1118 [D loss: 0.606586, acc: 66.41%] [G loss: 2.981620]\n",
      "epoch:1 step:1119 [D loss: 0.572284, acc: 64.84%] [G loss: 3.027183]\n",
      "epoch:1 step:1120 [D loss: 0.577739, acc: 67.97%] [G loss: 3.104220]\n",
      "epoch:1 step:1121 [D loss: 0.569238, acc: 75.00%] [G loss: 3.408631]\n",
      "epoch:1 step:1122 [D loss: 0.595557, acc: 68.75%] [G loss: 3.050583]\n",
      "epoch:1 step:1123 [D loss: 0.474757, acc: 79.69%] [G loss: 3.175743]\n",
      "epoch:1 step:1124 [D loss: 0.619711, acc: 69.53%] [G loss: 3.217524]\n",
      "epoch:1 step:1125 [D loss: 0.498537, acc: 73.44%] [G loss: 3.078733]\n",
      "epoch:1 step:1126 [D loss: 0.617704, acc: 62.50%] [G loss: 3.098670]\n",
      "epoch:1 step:1127 [D loss: 0.633458, acc: 64.84%] [G loss: 3.033926]\n",
      "epoch:1 step:1128 [D loss: 0.595483, acc: 64.84%] [G loss: 3.062485]\n",
      "epoch:1 step:1129 [D loss: 0.600748, acc: 67.97%] [G loss: 3.267167]\n",
      "epoch:1 step:1130 [D loss: 0.541720, acc: 71.88%] [G loss: 2.724599]\n",
      "epoch:1 step:1131 [D loss: 0.522850, acc: 74.22%] [G loss: 3.163225]\n",
      "epoch:1 step:1132 [D loss: 0.635438, acc: 67.19%] [G loss: 2.947255]\n",
      "epoch:1 step:1133 [D loss: 0.612172, acc: 66.41%] [G loss: 2.776241]\n",
      "epoch:1 step:1134 [D loss: 0.610150, acc: 69.53%] [G loss: 2.893127]\n",
      "epoch:1 step:1135 [D loss: 0.462602, acc: 78.91%] [G loss: 2.985137]\n",
      "epoch:1 step:1136 [D loss: 0.606623, acc: 67.19%] [G loss: 2.995137]\n",
      "epoch:1 step:1137 [D loss: 0.633185, acc: 64.84%] [G loss: 2.797118]\n",
      "epoch:1 step:1138 [D loss: 0.589463, acc: 68.75%] [G loss: 2.802412]\n",
      "epoch:1 step:1139 [D loss: 0.570356, acc: 67.97%] [G loss: 2.985675]\n",
      "epoch:1 step:1140 [D loss: 0.647217, acc: 65.62%] [G loss: 2.743503]\n",
      "epoch:1 step:1141 [D loss: 0.569305, acc: 71.09%] [G loss: 2.972987]\n",
      "epoch:1 step:1142 [D loss: 0.518238, acc: 67.97%] [G loss: 3.026645]\n",
      "epoch:1 step:1143 [D loss: 0.593144, acc: 73.44%] [G loss: 2.865396]\n",
      "epoch:1 step:1144 [D loss: 0.517262, acc: 75.78%] [G loss: 3.168328]\n",
      "epoch:1 step:1145 [D loss: 0.481566, acc: 79.69%] [G loss: 2.997135]\n",
      "epoch:1 step:1146 [D loss: 0.538702, acc: 68.75%] [G loss: 3.048880]\n",
      "epoch:1 step:1147 [D loss: 0.619298, acc: 65.62%] [G loss: 2.739633]\n",
      "epoch:1 step:1148 [D loss: 0.665266, acc: 64.06%] [G loss: 2.657885]\n",
      "epoch:1 step:1149 [D loss: 0.585161, acc: 68.75%] [G loss: 2.851915]\n",
      "epoch:1 step:1150 [D loss: 0.606526, acc: 67.19%] [G loss: 3.008838]\n",
      "epoch:1 step:1151 [D loss: 0.649550, acc: 62.50%] [G loss: 2.740499]\n",
      "epoch:1 step:1152 [D loss: 0.681774, acc: 58.59%] [G loss: 2.641891]\n",
      "epoch:1 step:1153 [D loss: 0.564351, acc: 73.44%] [G loss: 2.590110]\n",
      "epoch:1 step:1154 [D loss: 0.623406, acc: 67.19%] [G loss: 2.937715]\n",
      "epoch:1 step:1155 [D loss: 0.591177, acc: 67.97%] [G loss: 3.077719]\n",
      "epoch:1 step:1156 [D loss: 0.643334, acc: 66.41%] [G loss: 2.540764]\n",
      "epoch:1 step:1157 [D loss: 0.754797, acc: 60.94%] [G loss: 2.960877]\n",
      "epoch:1 step:1158 [D loss: 0.520917, acc: 75.00%] [G loss: 3.108066]\n",
      "epoch:1 step:1159 [D loss: 0.576334, acc: 65.62%] [G loss: 3.163734]\n",
      "epoch:1 step:1160 [D loss: 0.564577, acc: 67.97%] [G loss: 3.075810]\n",
      "epoch:1 step:1161 [D loss: 0.625538, acc: 64.84%] [G loss: 2.668567]\n",
      "epoch:1 step:1162 [D loss: 0.576027, acc: 68.75%] [G loss: 2.568255]\n",
      "epoch:1 step:1163 [D loss: 0.626481, acc: 66.41%] [G loss: 2.786349]\n",
      "epoch:1 step:1164 [D loss: 0.576984, acc: 64.84%] [G loss: 2.366542]\n",
      "epoch:1 step:1165 [D loss: 0.626370, acc: 60.94%] [G loss: 2.746296]\n",
      "epoch:1 step:1166 [D loss: 0.647697, acc: 63.28%] [G loss: 2.562029]\n",
      "epoch:1 step:1167 [D loss: 0.479747, acc: 75.78%] [G loss: 3.010192]\n",
      "epoch:1 step:1168 [D loss: 0.504452, acc: 69.53%] [G loss: 3.414257]\n",
      "epoch:1 step:1169 [D loss: 0.443291, acc: 76.56%] [G loss: 3.581919]\n",
      "epoch:1 step:1170 [D loss: 0.771483, acc: 62.50%] [G loss: 2.845961]\n",
      "epoch:1 step:1171 [D loss: 0.579076, acc: 68.75%] [G loss: 2.877780]\n",
      "epoch:1 step:1172 [D loss: 0.620833, acc: 64.84%] [G loss: 2.519348]\n",
      "epoch:1 step:1173 [D loss: 0.589197, acc: 71.88%] [G loss: 2.663481]\n",
      "epoch:1 step:1174 [D loss: 0.649184, acc: 62.50%] [G loss: 2.629007]\n",
      "epoch:1 step:1175 [D loss: 0.625073, acc: 68.75%] [G loss: 2.528626]\n",
      "epoch:1 step:1176 [D loss: 0.602909, acc: 64.84%] [G loss: 2.789654]\n",
      "epoch:1 step:1177 [D loss: 0.543500, acc: 71.09%] [G loss: 2.625423]\n",
      "epoch:1 step:1178 [D loss: 0.719374, acc: 57.03%] [G loss: 2.535354]\n",
      "epoch:1 step:1179 [D loss: 0.695049, acc: 60.16%] [G loss: 2.566254]\n",
      "epoch:1 step:1180 [D loss: 0.591230, acc: 67.19%] [G loss: 2.691354]\n",
      "epoch:1 step:1181 [D loss: 0.738310, acc: 54.69%] [G loss: 2.889087]\n",
      "epoch:1 step:1182 [D loss: 0.596396, acc: 71.09%] [G loss: 2.792115]\n",
      "epoch:1 step:1183 [D loss: 0.603450, acc: 67.19%] [G loss: 2.519127]\n",
      "epoch:1 step:1184 [D loss: 0.554542, acc: 69.53%] [G loss: 2.537642]\n",
      "epoch:1 step:1185 [D loss: 0.544027, acc: 71.88%] [G loss: 2.799849]\n",
      "epoch:1 step:1186 [D loss: 0.645226, acc: 61.72%] [G loss: 2.399503]\n",
      "epoch:1 step:1187 [D loss: 0.623661, acc: 62.50%] [G loss: 2.781352]\n",
      "epoch:1 step:1188 [D loss: 0.626502, acc: 64.84%] [G loss: 2.588327]\n",
      "epoch:1 step:1189 [D loss: 0.671978, acc: 64.84%] [G loss: 2.273157]\n",
      "epoch:1 step:1190 [D loss: 0.562568, acc: 67.97%] [G loss: 2.826148]\n",
      "epoch:1 step:1191 [D loss: 0.563338, acc: 74.22%] [G loss: 2.764045]\n",
      "epoch:1 step:1192 [D loss: 0.625549, acc: 62.50%] [G loss: 2.880770]\n",
      "epoch:1 step:1193 [D loss: 0.558513, acc: 70.31%] [G loss: 2.960323]\n",
      "epoch:1 step:1194 [D loss: 0.527773, acc: 74.22%] [G loss: 2.715486]\n",
      "epoch:1 step:1195 [D loss: 0.549288, acc: 71.88%] [G loss: 2.570113]\n",
      "epoch:1 step:1196 [D loss: 0.517982, acc: 77.34%] [G loss: 2.910746]\n",
      "epoch:1 step:1197 [D loss: 0.579010, acc: 69.53%] [G loss: 2.842923]\n",
      "epoch:1 step:1198 [D loss: 0.550385, acc: 67.19%] [G loss: 2.910324]\n",
      "epoch:1 step:1199 [D loss: 0.570535, acc: 72.66%] [G loss: 2.676878]\n",
      "epoch:1 step:1200 [D loss: 0.840231, acc: 42.97%] [G loss: 2.336111]\n",
      "##############\n",
      "[3.93474642 2.19477042 7.9382671  6.15318877 5.33683683 6.84934272\n",
      " 6.06961669 6.15259107 6.29730129 4.66250441]\n",
      "##########\n",
      "epoch:1 step:1201 [D loss: 0.557690, acc: 69.53%] [G loss: 2.817428]\n",
      "epoch:1 step:1202 [D loss: 0.529391, acc: 74.22%] [G loss: 2.883276]\n",
      "epoch:1 step:1203 [D loss: 0.565091, acc: 72.66%] [G loss: 2.592149]\n",
      "epoch:1 step:1204 [D loss: 0.599691, acc: 66.41%] [G loss: 2.768081]\n",
      "epoch:1 step:1205 [D loss: 0.592267, acc: 66.41%] [G loss: 2.584239]\n",
      "epoch:1 step:1206 [D loss: 0.686795, acc: 61.72%] [G loss: 2.362788]\n",
      "epoch:1 step:1207 [D loss: 0.586482, acc: 68.75%] [G loss: 2.575422]\n",
      "epoch:1 step:1208 [D loss: 0.563766, acc: 68.75%] [G loss: 2.663012]\n",
      "epoch:1 step:1209 [D loss: 0.712923, acc: 57.03%] [G loss: 2.363822]\n",
      "epoch:1 step:1210 [D loss: 0.655232, acc: 61.72%] [G loss: 2.798175]\n",
      "epoch:1 step:1211 [D loss: 0.684674, acc: 58.59%] [G loss: 2.497945]\n",
      "epoch:1 step:1212 [D loss: 0.590020, acc: 64.84%] [G loss: 2.590189]\n",
      "epoch:1 step:1213 [D loss: 0.603317, acc: 67.97%] [G loss: 2.408880]\n",
      "epoch:1 step:1214 [D loss: 0.720500, acc: 57.81%] [G loss: 2.189800]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:1 step:1215 [D loss: 0.630101, acc: 66.41%] [G loss: 2.548404]\n",
      "epoch:1 step:1216 [D loss: 0.557999, acc: 67.19%] [G loss: 2.678904]\n",
      "epoch:1 step:1217 [D loss: 0.588449, acc: 66.41%] [G loss: 2.554348]\n",
      "epoch:1 step:1218 [D loss: 0.628517, acc: 64.06%] [G loss: 2.676003]\n",
      "epoch:1 step:1219 [D loss: 0.716735, acc: 52.34%] [G loss: 2.531455]\n",
      "epoch:1 step:1220 [D loss: 0.584406, acc: 66.41%] [G loss: 2.555397]\n",
      "epoch:1 step:1221 [D loss: 0.477653, acc: 80.47%] [G loss: 2.837319]\n",
      "epoch:1 step:1222 [D loss: 0.536315, acc: 74.22%] [G loss: 2.673779]\n",
      "epoch:1 step:1223 [D loss: 0.603270, acc: 65.62%] [G loss: 2.577467]\n",
      "epoch:1 step:1224 [D loss: 0.624953, acc: 64.84%] [G loss: 2.620009]\n",
      "epoch:1 step:1225 [D loss: 0.609974, acc: 62.50%] [G loss: 2.613353]\n",
      "epoch:1 step:1226 [D loss: 0.637471, acc: 71.88%] [G loss: 2.625942]\n",
      "epoch:1 step:1227 [D loss: 0.611993, acc: 66.41%] [G loss: 2.511620]\n",
      "epoch:1 step:1228 [D loss: 0.616112, acc: 66.41%] [G loss: 2.679070]\n",
      "epoch:1 step:1229 [D loss: 0.616315, acc: 66.41%] [G loss: 2.452538]\n",
      "epoch:1 step:1230 [D loss: 0.662034, acc: 59.38%] [G loss: 2.528088]\n",
      "epoch:1 step:1231 [D loss: 0.645972, acc: 63.28%] [G loss: 2.667413]\n",
      "epoch:1 step:1232 [D loss: 0.597456, acc: 66.41%] [G loss: 2.556312]\n",
      "epoch:1 step:1233 [D loss: 0.551069, acc: 69.53%] [G loss: 2.965506]\n",
      "epoch:1 step:1234 [D loss: 0.607859, acc: 61.72%] [G loss: 2.596271]\n",
      "epoch:1 step:1235 [D loss: 0.515190, acc: 75.78%] [G loss: 2.667338]\n",
      "epoch:1 step:1236 [D loss: 0.613962, acc: 70.31%] [G loss: 2.655529]\n",
      "epoch:1 step:1237 [D loss: 0.557937, acc: 71.88%] [G loss: 2.763663]\n",
      "epoch:1 step:1238 [D loss: 0.651778, acc: 66.41%] [G loss: 2.553684]\n",
      "epoch:1 step:1239 [D loss: 0.601394, acc: 66.41%] [G loss: 2.680508]\n",
      "epoch:1 step:1240 [D loss: 0.574664, acc: 67.97%] [G loss: 2.488865]\n",
      "epoch:1 step:1241 [D loss: 0.492177, acc: 75.00%] [G loss: 2.677381]\n",
      "epoch:1 step:1242 [D loss: 0.592672, acc: 70.31%] [G loss: 2.843107]\n",
      "epoch:1 step:1243 [D loss: 0.546213, acc: 69.53%] [G loss: 2.916695]\n",
      "epoch:1 step:1244 [D loss: 0.547944, acc: 72.66%] [G loss: 2.585233]\n",
      "epoch:1 step:1245 [D loss: 0.546995, acc: 75.78%] [G loss: 2.642335]\n",
      "epoch:1 step:1246 [D loss: 0.538485, acc: 78.12%] [G loss: 3.061318]\n",
      "epoch:1 step:1247 [D loss: 0.502020, acc: 78.91%] [G loss: 3.031366]\n",
      "epoch:1 step:1248 [D loss: 0.548909, acc: 71.88%] [G loss: 3.269236]\n",
      "epoch:1 step:1249 [D loss: 0.461343, acc: 81.25%] [G loss: 3.106191]\n",
      "epoch:1 step:1250 [D loss: 0.508412, acc: 75.78%] [G loss: 3.396634]\n",
      "epoch:1 step:1251 [D loss: 0.492327, acc: 78.12%] [G loss: 3.233672]\n",
      "epoch:1 step:1252 [D loss: 0.583464, acc: 67.97%] [G loss: 3.086597]\n",
      "epoch:1 step:1253 [D loss: 0.733250, acc: 57.03%] [G loss: 2.519993]\n",
      "epoch:1 step:1254 [D loss: 0.578758, acc: 67.97%] [G loss: 2.638865]\n",
      "epoch:1 step:1255 [D loss: 0.696459, acc: 64.84%] [G loss: 2.684088]\n",
      "epoch:1 step:1256 [D loss: 0.574992, acc: 70.31%] [G loss: 2.640876]\n",
      "epoch:1 step:1257 [D loss: 0.523845, acc: 75.00%] [G loss: 2.493082]\n",
      "epoch:1 step:1258 [D loss: 0.614378, acc: 69.53%] [G loss: 2.623843]\n",
      "epoch:1 step:1259 [D loss: 0.622362, acc: 67.19%] [G loss: 2.564247]\n",
      "epoch:1 step:1260 [D loss: 0.681316, acc: 60.94%] [G loss: 2.775544]\n",
      "epoch:1 step:1261 [D loss: 0.534138, acc: 72.66%] [G loss: 2.847269]\n",
      "epoch:1 step:1262 [D loss: 0.590548, acc: 69.53%] [G loss: 2.653506]\n",
      "epoch:1 step:1263 [D loss: 0.563281, acc: 66.41%] [G loss: 2.637748]\n",
      "epoch:1 step:1264 [D loss: 0.635955, acc: 68.75%] [G loss: 2.873138]\n",
      "epoch:1 step:1265 [D loss: 0.653026, acc: 65.62%] [G loss: 3.090540]\n",
      "epoch:1 step:1266 [D loss: 0.623772, acc: 67.19%] [G loss: 2.448581]\n",
      "epoch:1 step:1267 [D loss: 0.595657, acc: 71.09%] [G loss: 2.511765]\n",
      "epoch:1 step:1268 [D loss: 0.569612, acc: 73.44%] [G loss: 2.662323]\n",
      "epoch:1 step:1269 [D loss: 0.582183, acc: 72.66%] [G loss: 2.809285]\n",
      "epoch:1 step:1270 [D loss: 0.556316, acc: 75.00%] [G loss: 2.880151]\n",
      "epoch:1 step:1271 [D loss: 0.638480, acc: 65.62%] [G loss: 3.136799]\n",
      "epoch:1 step:1272 [D loss: 0.519865, acc: 75.78%] [G loss: 3.187298]\n",
      "epoch:1 step:1273 [D loss: 0.506758, acc: 74.22%] [G loss: 2.976229]\n",
      "epoch:1 step:1274 [D loss: 0.578911, acc: 71.88%] [G loss: 3.084874]\n",
      "epoch:1 step:1275 [D loss: 0.591165, acc: 62.50%] [G loss: 2.521831]\n",
      "epoch:1 step:1276 [D loss: 0.645855, acc: 60.94%] [G loss: 2.457869]\n",
      "epoch:1 step:1277 [D loss: 0.573939, acc: 64.06%] [G loss: 2.611613]\n",
      "epoch:1 step:1278 [D loss: 0.555087, acc: 70.31%] [G loss: 3.070987]\n",
      "epoch:1 step:1279 [D loss: 0.507104, acc: 75.78%] [G loss: 2.811303]\n",
      "epoch:1 step:1280 [D loss: 0.439492, acc: 79.69%] [G loss: 3.088410]\n",
      "epoch:1 step:1281 [D loss: 0.461950, acc: 81.25%] [G loss: 2.981419]\n",
      "epoch:1 step:1282 [D loss: 0.571183, acc: 71.88%] [G loss: 2.832326]\n",
      "epoch:1 step:1283 [D loss: 0.541874, acc: 75.00%] [G loss: 3.107304]\n",
      "epoch:1 step:1284 [D loss: 0.454787, acc: 82.03%] [G loss: 3.047388]\n",
      "epoch:1 step:1285 [D loss: 0.634855, acc: 66.41%] [G loss: 2.592373]\n",
      "epoch:1 step:1286 [D loss: 0.744934, acc: 54.69%] [G loss: 2.344252]\n",
      "epoch:1 step:1287 [D loss: 0.571504, acc: 71.09%] [G loss: 2.849698]\n",
      "epoch:1 step:1288 [D loss: 0.567967, acc: 71.88%] [G loss: 2.700087]\n",
      "epoch:1 step:1289 [D loss: 0.625682, acc: 64.84%] [G loss: 2.932193]\n",
      "epoch:1 step:1290 [D loss: 0.551998, acc: 66.41%] [G loss: 2.526627]\n",
      "epoch:1 step:1291 [D loss: 0.517876, acc: 73.44%] [G loss: 2.892850]\n",
      "epoch:1 step:1292 [D loss: 0.594352, acc: 67.19%] [G loss: 2.875870]\n",
      "epoch:1 step:1293 [D loss: 0.560954, acc: 69.53%] [G loss: 2.709726]\n",
      "epoch:1 step:1294 [D loss: 0.622438, acc: 71.09%] [G loss: 2.859872]\n",
      "epoch:1 step:1295 [D loss: 0.468032, acc: 78.91%] [G loss: 2.939959]\n",
      "epoch:1 step:1296 [D loss: 0.513129, acc: 76.56%] [G loss: 2.696144]\n",
      "epoch:1 step:1297 [D loss: 0.592490, acc: 71.09%] [G loss: 2.629475]\n",
      "epoch:1 step:1298 [D loss: 0.557515, acc: 70.31%] [G loss: 2.603327]\n",
      "epoch:1 step:1299 [D loss: 0.559557, acc: 69.53%] [G loss: 2.720298]\n",
      "epoch:1 step:1300 [D loss: 0.546507, acc: 69.53%] [G loss: 2.759388]\n",
      "epoch:1 step:1301 [D loss: 0.502472, acc: 75.78%] [G loss: 2.785970]\n",
      "epoch:1 step:1302 [D loss: 0.590950, acc: 64.84%] [G loss: 2.661808]\n",
      "epoch:1 step:1303 [D loss: 0.529211, acc: 70.31%] [G loss: 3.045735]\n",
      "epoch:1 step:1304 [D loss: 0.627977, acc: 67.19%] [G loss: 2.638241]\n",
      "epoch:1 step:1305 [D loss: 0.513959, acc: 75.78%] [G loss: 2.893264]\n",
      "epoch:1 step:1306 [D loss: 0.571046, acc: 68.75%] [G loss: 2.538277]\n",
      "epoch:1 step:1307 [D loss: 0.728099, acc: 60.94%] [G loss: 2.567359]\n",
      "epoch:1 step:1308 [D loss: 0.585561, acc: 67.97%] [G loss: 2.606237]\n",
      "epoch:1 step:1309 [D loss: 0.615607, acc: 60.94%] [G loss: 2.500272]\n",
      "epoch:1 step:1310 [D loss: 0.619145, acc: 64.06%] [G loss: 2.694148]\n",
      "epoch:1 step:1311 [D loss: 0.589143, acc: 71.09%] [G loss: 2.703181]\n",
      "epoch:1 step:1312 [D loss: 0.615118, acc: 62.50%] [G loss: 2.824433]\n",
      "epoch:1 step:1313 [D loss: 0.615068, acc: 66.41%] [G loss: 2.765393]\n",
      "epoch:1 step:1314 [D loss: 0.567987, acc: 68.75%] [G loss: 2.596808]\n",
      "epoch:1 step:1315 [D loss: 0.553138, acc: 66.41%] [G loss: 2.657615]\n",
      "epoch:1 step:1316 [D loss: 0.558342, acc: 67.97%] [G loss: 2.527423]\n",
      "epoch:1 step:1317 [D loss: 0.555009, acc: 74.22%] [G loss: 2.692145]\n",
      "epoch:1 step:1318 [D loss: 0.443363, acc: 82.81%] [G loss: 3.069770]\n",
      "epoch:1 step:1319 [D loss: 0.507459, acc: 75.78%] [G loss: 2.866412]\n",
      "epoch:1 step:1320 [D loss: 0.541518, acc: 69.53%] [G loss: 3.062961]\n",
      "epoch:1 step:1321 [D loss: 0.593773, acc: 69.53%] [G loss: 2.900816]\n",
      "epoch:1 step:1322 [D loss: 0.586162, acc: 68.75%] [G loss: 2.579995]\n",
      "epoch:1 step:1323 [D loss: 0.552643, acc: 71.09%] [G loss: 2.742936]\n",
      "epoch:1 step:1324 [D loss: 0.675659, acc: 67.19%] [G loss: 2.844050]\n",
      "epoch:1 step:1325 [D loss: 0.624767, acc: 68.75%] [G loss: 2.857070]\n",
      "epoch:1 step:1326 [D loss: 0.607412, acc: 67.97%] [G loss: 2.427556]\n",
      "epoch:1 step:1327 [D loss: 0.567606, acc: 71.88%] [G loss: 2.707903]\n",
      "epoch:1 step:1328 [D loss: 0.579613, acc: 69.53%] [G loss: 2.917894]\n",
      "epoch:1 step:1329 [D loss: 0.481851, acc: 79.69%] [G loss: 2.746439]\n",
      "epoch:1 step:1330 [D loss: 0.708216, acc: 60.16%] [G loss: 2.545542]\n",
      "epoch:1 step:1331 [D loss: 0.596321, acc: 61.72%] [G loss: 2.794893]\n",
      "epoch:1 step:1332 [D loss: 0.537536, acc: 72.66%] [G loss: 2.892810]\n",
      "epoch:1 step:1333 [D loss: 0.563396, acc: 71.88%] [G loss: 2.934599]\n",
      "epoch:1 step:1334 [D loss: 0.447743, acc: 83.59%] [G loss: 3.027874]\n",
      "epoch:1 step:1335 [D loss: 0.501041, acc: 78.12%] [G loss: 3.110688]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:1 step:1336 [D loss: 0.538614, acc: 74.22%] [G loss: 3.013354]\n",
      "epoch:1 step:1337 [D loss: 0.601262, acc: 64.84%] [G loss: 2.949452]\n",
      "epoch:1 step:1338 [D loss: 0.680166, acc: 62.50%] [G loss: 2.757989]\n",
      "epoch:1 step:1339 [D loss: 0.449552, acc: 78.91%] [G loss: 3.062455]\n",
      "epoch:1 step:1340 [D loss: 0.593740, acc: 64.84%] [G loss: 2.931190]\n",
      "epoch:1 step:1341 [D loss: 0.600750, acc: 65.62%] [G loss: 2.503037]\n",
      "epoch:1 step:1342 [D loss: 0.449084, acc: 80.47%] [G loss: 3.029076]\n",
      "epoch:1 step:1343 [D loss: 0.618510, acc: 67.19%] [G loss: 2.882108]\n",
      "epoch:1 step:1344 [D loss: 0.575021, acc: 69.53%] [G loss: 3.336764]\n",
      "epoch:1 step:1345 [D loss: 0.614126, acc: 64.06%] [G loss: 3.072487]\n",
      "epoch:1 step:1346 [D loss: 0.562404, acc: 73.44%] [G loss: 2.628407]\n",
      "epoch:1 step:1347 [D loss: 0.544191, acc: 72.66%] [G loss: 2.920736]\n",
      "epoch:1 step:1348 [D loss: 0.659733, acc: 67.19%] [G loss: 2.627753]\n",
      "epoch:1 step:1349 [D loss: 0.571292, acc: 70.31%] [G loss: 2.892101]\n",
      "epoch:1 step:1350 [D loss: 0.531295, acc: 74.22%] [G loss: 2.914526]\n",
      "epoch:1 step:1351 [D loss: 0.491493, acc: 76.56%] [G loss: 2.965372]\n",
      "epoch:1 step:1352 [D loss: 0.610371, acc: 67.97%] [G loss: 2.622122]\n",
      "epoch:1 step:1353 [D loss: 0.622493, acc: 60.94%] [G loss: 2.740567]\n",
      "epoch:1 step:1354 [D loss: 0.613712, acc: 68.75%] [G loss: 2.531114]\n",
      "epoch:1 step:1355 [D loss: 0.568109, acc: 72.66%] [G loss: 2.788584]\n",
      "epoch:1 step:1356 [D loss: 0.536223, acc: 75.78%] [G loss: 2.932795]\n",
      "epoch:1 step:1357 [D loss: 0.509441, acc: 75.78%] [G loss: 3.044590]\n",
      "epoch:1 step:1358 [D loss: 0.571641, acc: 71.88%] [G loss: 3.098244]\n",
      "epoch:1 step:1359 [D loss: 0.624852, acc: 67.19%] [G loss: 2.686710]\n",
      "epoch:1 step:1360 [D loss: 0.572558, acc: 70.31%] [G loss: 2.954255]\n",
      "epoch:1 step:1361 [D loss: 0.564122, acc: 67.19%] [G loss: 2.830336]\n",
      "epoch:1 step:1362 [D loss: 0.591125, acc: 67.97%] [G loss: 2.635135]\n",
      "epoch:1 step:1363 [D loss: 0.502524, acc: 79.69%] [G loss: 3.152253]\n",
      "epoch:1 step:1364 [D loss: 0.604586, acc: 67.97%] [G loss: 3.201836]\n",
      "epoch:1 step:1365 [D loss: 0.580984, acc: 74.22%] [G loss: 2.864738]\n",
      "epoch:1 step:1366 [D loss: 0.433172, acc: 76.56%] [G loss: 3.184053]\n",
      "epoch:1 step:1367 [D loss: 0.576279, acc: 67.19%] [G loss: 3.059037]\n",
      "epoch:1 step:1368 [D loss: 0.497173, acc: 75.78%] [G loss: 2.966552]\n",
      "epoch:1 step:1369 [D loss: 0.556195, acc: 66.41%] [G loss: 2.665750]\n",
      "epoch:1 step:1370 [D loss: 0.578257, acc: 71.88%] [G loss: 2.753936]\n",
      "epoch:1 step:1371 [D loss: 0.461648, acc: 78.12%] [G loss: 2.950147]\n",
      "epoch:1 step:1372 [D loss: 0.556970, acc: 69.53%] [G loss: 2.856052]\n",
      "epoch:1 step:1373 [D loss: 0.560887, acc: 69.53%] [G loss: 3.112493]\n",
      "epoch:1 step:1374 [D loss: 0.658494, acc: 66.41%] [G loss: 2.714769]\n",
      "epoch:1 step:1375 [D loss: 0.539122, acc: 71.09%] [G loss: 2.954505]\n",
      "epoch:1 step:1376 [D loss: 0.482290, acc: 80.47%] [G loss: 3.100162]\n",
      "epoch:1 step:1377 [D loss: 0.517110, acc: 75.78%] [G loss: 2.977905]\n",
      "epoch:1 step:1378 [D loss: 0.625607, acc: 64.06%] [G loss: 2.831568]\n",
      "epoch:1 step:1379 [D loss: 0.521668, acc: 75.00%] [G loss: 2.880637]\n",
      "epoch:1 step:1380 [D loss: 0.543473, acc: 73.44%] [G loss: 2.629971]\n",
      "epoch:1 step:1381 [D loss: 0.535556, acc: 71.09%] [G loss: 3.039246]\n",
      "epoch:1 step:1382 [D loss: 0.513640, acc: 77.34%] [G loss: 2.873364]\n",
      "epoch:1 step:1383 [D loss: 0.555842, acc: 70.31%] [G loss: 3.004416]\n",
      "epoch:1 step:1384 [D loss: 0.582647, acc: 72.66%] [G loss: 2.957333]\n",
      "epoch:1 step:1385 [D loss: 0.650360, acc: 67.97%] [G loss: 2.906693]\n",
      "epoch:1 step:1386 [D loss: 0.649823, acc: 62.50%] [G loss: 2.715208]\n",
      "epoch:1 step:1387 [D loss: 0.520182, acc: 75.78%] [G loss: 2.789832]\n",
      "epoch:1 step:1388 [D loss: 0.507417, acc: 75.00%] [G loss: 2.920810]\n",
      "epoch:1 step:1389 [D loss: 0.603523, acc: 70.31%] [G loss: 3.173398]\n",
      "epoch:1 step:1390 [D loss: 0.615244, acc: 66.41%] [G loss: 2.791777]\n",
      "epoch:1 step:1391 [D loss: 0.609794, acc: 65.62%] [G loss: 2.872806]\n",
      "epoch:1 step:1392 [D loss: 0.538207, acc: 68.75%] [G loss: 2.647925]\n",
      "epoch:1 step:1393 [D loss: 0.583951, acc: 64.84%] [G loss: 2.482919]\n",
      "epoch:1 step:1394 [D loss: 0.555667, acc: 71.88%] [G loss: 2.798994]\n",
      "epoch:1 step:1395 [D loss: 0.529278, acc: 71.88%] [G loss: 3.452476]\n",
      "epoch:1 step:1396 [D loss: 0.593466, acc: 69.53%] [G loss: 2.973548]\n",
      "epoch:1 step:1397 [D loss: 0.526919, acc: 76.56%] [G loss: 3.431944]\n",
      "epoch:1 step:1398 [D loss: 0.468230, acc: 80.47%] [G loss: 3.234396]\n",
      "epoch:1 step:1399 [D loss: 0.548261, acc: 70.31%] [G loss: 3.100004]\n",
      "epoch:1 step:1400 [D loss: 0.596898, acc: 67.19%] [G loss: 2.835974]\n",
      "##############\n",
      "[3.87246456 2.23235604 7.76230159 5.97412642 5.16892189 6.78348557\n",
      " 6.09825844 5.86681561 6.14782761 4.38869687]\n",
      "##########\n",
      "epoch:1 step:1401 [D loss: 0.538328, acc: 72.66%] [G loss: 2.762516]\n",
      "epoch:1 step:1402 [D loss: 0.575902, acc: 68.75%] [G loss: 2.593394]\n",
      "epoch:1 step:1403 [D loss: 0.687550, acc: 57.81%] [G loss: 2.742091]\n",
      "epoch:1 step:1404 [D loss: 0.559937, acc: 71.88%] [G loss: 3.050777]\n",
      "epoch:1 step:1405 [D loss: 0.661775, acc: 70.31%] [G loss: 2.724606]\n",
      "epoch:1 step:1406 [D loss: 0.550740, acc: 68.75%] [G loss: 2.750831]\n",
      "epoch:1 step:1407 [D loss: 0.682433, acc: 64.84%] [G loss: 2.804019]\n",
      "epoch:1 step:1408 [D loss: 0.530453, acc: 71.88%] [G loss: 2.945177]\n",
      "epoch:1 step:1409 [D loss: 0.495119, acc: 76.56%] [G loss: 2.906249]\n",
      "epoch:1 step:1410 [D loss: 0.650743, acc: 64.84%] [G loss: 2.939957]\n",
      "epoch:1 step:1411 [D loss: 0.580153, acc: 67.97%] [G loss: 3.034865]\n",
      "epoch:1 step:1412 [D loss: 0.493098, acc: 77.34%] [G loss: 3.217318]\n",
      "epoch:1 step:1413 [D loss: 0.561867, acc: 71.88%] [G loss: 3.022290]\n",
      "epoch:1 step:1414 [D loss: 0.694461, acc: 57.03%] [G loss: 2.515255]\n",
      "epoch:1 step:1415 [D loss: 0.635846, acc: 64.84%] [G loss: 2.597072]\n",
      "epoch:1 step:1416 [D loss: 0.643247, acc: 66.41%] [G loss: 2.594222]\n",
      "epoch:1 step:1417 [D loss: 0.598535, acc: 63.28%] [G loss: 2.532456]\n",
      "epoch:1 step:1418 [D loss: 0.584830, acc: 67.97%] [G loss: 2.686762]\n",
      "epoch:1 step:1419 [D loss: 0.620693, acc: 64.84%] [G loss: 2.691390]\n",
      "epoch:1 step:1420 [D loss: 0.574331, acc: 71.09%] [G loss: 2.559807]\n",
      "epoch:1 step:1421 [D loss: 0.576192, acc: 65.62%] [G loss: 2.901860]\n",
      "epoch:1 step:1422 [D loss: 0.561232, acc: 71.09%] [G loss: 2.833146]\n",
      "epoch:1 step:1423 [D loss: 0.586139, acc: 67.19%] [G loss: 2.898077]\n",
      "epoch:1 step:1424 [D loss: 0.462351, acc: 79.69%] [G loss: 2.772118]\n",
      "epoch:1 step:1425 [D loss: 0.553052, acc: 70.31%] [G loss: 2.946927]\n",
      "epoch:1 step:1426 [D loss: 0.534283, acc: 75.78%] [G loss: 2.547209]\n",
      "epoch:1 step:1427 [D loss: 0.600313, acc: 63.28%] [G loss: 2.678156]\n",
      "epoch:1 step:1428 [D loss: 0.574830, acc: 69.53%] [G loss: 2.942044]\n",
      "epoch:1 step:1429 [D loss: 0.577718, acc: 66.41%] [G loss: 2.648626]\n",
      "epoch:1 step:1430 [D loss: 0.566293, acc: 69.53%] [G loss: 2.919390]\n",
      "epoch:1 step:1431 [D loss: 0.614448, acc: 66.41%] [G loss: 2.969163]\n",
      "epoch:1 step:1432 [D loss: 0.565412, acc: 69.53%] [G loss: 2.727762]\n",
      "epoch:1 step:1433 [D loss: 0.593360, acc: 68.75%] [G loss: 2.470492]\n",
      "epoch:1 step:1434 [D loss: 0.584542, acc: 64.06%] [G loss: 2.830881]\n",
      "epoch:1 step:1435 [D loss: 0.605297, acc: 68.75%] [G loss: 3.222012]\n",
      "epoch:1 step:1436 [D loss: 0.478144, acc: 75.00%] [G loss: 3.253449]\n",
      "epoch:1 step:1437 [D loss: 0.737840, acc: 63.28%] [G loss: 2.902436]\n",
      "epoch:1 step:1438 [D loss: 0.599244, acc: 65.62%] [G loss: 2.571411]\n",
      "epoch:1 step:1439 [D loss: 0.622014, acc: 64.84%] [G loss: 2.714549]\n",
      "epoch:1 step:1440 [D loss: 0.581749, acc: 69.53%] [G loss: 2.569296]\n",
      "epoch:1 step:1441 [D loss: 0.549270, acc: 67.97%] [G loss: 2.989544]\n",
      "epoch:1 step:1442 [D loss: 0.550121, acc: 75.00%] [G loss: 2.907367]\n",
      "epoch:1 step:1443 [D loss: 0.507886, acc: 75.78%] [G loss: 3.040764]\n",
      "epoch:1 step:1444 [D loss: 0.483792, acc: 78.12%] [G loss: 3.252937]\n",
      "epoch:1 step:1445 [D loss: 0.559071, acc: 71.09%] [G loss: 3.286293]\n",
      "epoch:1 step:1446 [D loss: 0.525499, acc: 75.00%] [G loss: 2.963813]\n",
      "epoch:1 step:1447 [D loss: 0.608837, acc: 67.97%] [G loss: 2.885649]\n",
      "epoch:1 step:1448 [D loss: 0.666615, acc: 59.38%] [G loss: 2.641038]\n",
      "epoch:1 step:1449 [D loss: 0.528699, acc: 72.66%] [G loss: 2.878739]\n",
      "epoch:1 step:1450 [D loss: 0.535430, acc: 75.00%] [G loss: 2.853725]\n",
      "epoch:1 step:1451 [D loss: 0.525381, acc: 71.88%] [G loss: 2.961460]\n",
      "epoch:1 step:1452 [D loss: 0.550364, acc: 71.88%] [G loss: 3.130227]\n",
      "epoch:1 step:1453 [D loss: 0.631237, acc: 68.75%] [G loss: 2.895864]\n",
      "epoch:1 step:1454 [D loss: 0.581887, acc: 66.41%] [G loss: 2.667155]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:1 step:1455 [D loss: 0.589901, acc: 71.88%] [G loss: 2.907110]\n",
      "epoch:1 step:1456 [D loss: 0.552445, acc: 71.88%] [G loss: 2.767198]\n",
      "epoch:1 step:1457 [D loss: 0.547835, acc: 69.53%] [G loss: 2.932747]\n",
      "epoch:1 step:1458 [D loss: 0.491963, acc: 75.78%] [G loss: 3.161850]\n",
      "epoch:1 step:1459 [D loss: 0.608423, acc: 65.62%] [G loss: 2.838904]\n",
      "epoch:1 step:1460 [D loss: 0.485785, acc: 75.78%] [G loss: 2.991935]\n",
      "epoch:1 step:1461 [D loss: 0.589758, acc: 70.31%] [G loss: 2.802855]\n",
      "epoch:1 step:1462 [D loss: 0.577536, acc: 67.97%] [G loss: 2.748942]\n",
      "epoch:1 step:1463 [D loss: 0.587410, acc: 69.53%] [G loss: 3.071958]\n",
      "epoch:1 step:1464 [D loss: 0.567758, acc: 74.22%] [G loss: 2.916834]\n",
      "epoch:1 step:1465 [D loss: 0.556666, acc: 71.88%] [G loss: 2.873969]\n",
      "epoch:1 step:1466 [D loss: 0.545729, acc: 77.34%] [G loss: 2.586088]\n",
      "epoch:1 step:1467 [D loss: 0.508786, acc: 76.56%] [G loss: 3.040631]\n",
      "epoch:1 step:1468 [D loss: 0.614755, acc: 70.31%] [G loss: 2.789538]\n",
      "epoch:1 step:1469 [D loss: 0.586520, acc: 71.88%] [G loss: 2.714297]\n",
      "epoch:1 step:1470 [D loss: 0.606068, acc: 65.62%] [G loss: 2.652721]\n",
      "epoch:1 step:1471 [D loss: 0.504680, acc: 71.88%] [G loss: 3.045888]\n",
      "epoch:1 step:1472 [D loss: 0.517257, acc: 73.44%] [G loss: 2.850750]\n",
      "epoch:1 step:1473 [D loss: 0.532355, acc: 72.66%] [G loss: 2.881927]\n",
      "epoch:1 step:1474 [D loss: 0.517815, acc: 73.44%] [G loss: 2.867776]\n",
      "epoch:1 step:1475 [D loss: 0.578768, acc: 69.53%] [G loss: 2.741519]\n",
      "epoch:1 step:1476 [D loss: 0.600120, acc: 64.84%] [G loss: 2.913963]\n",
      "epoch:1 step:1477 [D loss: 0.455036, acc: 78.91%] [G loss: 3.179481]\n",
      "epoch:1 step:1478 [D loss: 0.483681, acc: 78.12%] [G loss: 3.138364]\n",
      "epoch:1 step:1479 [D loss: 0.592359, acc: 64.84%] [G loss: 2.833994]\n",
      "epoch:1 step:1480 [D loss: 0.542658, acc: 72.66%] [G loss: 2.880561]\n",
      "epoch:1 step:1481 [D loss: 0.591807, acc: 66.41%] [G loss: 3.140136]\n",
      "epoch:1 step:1482 [D loss: 0.554450, acc: 71.09%] [G loss: 2.846176]\n",
      "epoch:1 step:1483 [D loss: 0.537896, acc: 72.66%] [G loss: 3.184296]\n",
      "epoch:1 step:1484 [D loss: 0.458086, acc: 76.56%] [G loss: 3.072577]\n",
      "epoch:1 step:1485 [D loss: 0.635246, acc: 62.50%] [G loss: 2.964803]\n",
      "epoch:1 step:1486 [D loss: 0.591644, acc: 68.75%] [G loss: 2.813483]\n",
      "epoch:1 step:1487 [D loss: 0.517712, acc: 75.00%] [G loss: 3.022226]\n",
      "epoch:1 step:1488 [D loss: 0.571537, acc: 72.66%] [G loss: 2.921931]\n",
      "epoch:1 step:1489 [D loss: 0.489106, acc: 77.34%] [G loss: 2.940012]\n",
      "epoch:1 step:1490 [D loss: 0.597809, acc: 69.53%] [G loss: 3.060421]\n",
      "epoch:1 step:1491 [D loss: 0.520475, acc: 74.22%] [G loss: 3.156327]\n",
      "epoch:1 step:1492 [D loss: 0.494544, acc: 74.22%] [G loss: 3.069408]\n",
      "epoch:1 step:1493 [D loss: 0.570377, acc: 67.97%] [G loss: 2.972917]\n",
      "epoch:1 step:1494 [D loss: 0.592809, acc: 68.75%] [G loss: 2.974948]\n",
      "epoch:1 step:1495 [D loss: 0.551729, acc: 73.44%] [G loss: 3.341978]\n",
      "epoch:1 step:1496 [D loss: 0.543830, acc: 67.19%] [G loss: 2.712746]\n",
      "epoch:1 step:1497 [D loss: 0.607265, acc: 64.84%] [G loss: 2.840086]\n",
      "epoch:1 step:1498 [D loss: 0.521752, acc: 75.78%] [G loss: 2.966335]\n",
      "epoch:1 step:1499 [D loss: 0.585227, acc: 68.75%] [G loss: 2.516414]\n",
      "epoch:1 step:1500 [D loss: 0.559422, acc: 73.44%] [G loss: 2.814189]\n",
      "epoch:1 step:1501 [D loss: 0.558672, acc: 68.75%] [G loss: 3.174470]\n",
      "epoch:1 step:1502 [D loss: 0.542951, acc: 70.31%] [G loss: 3.129514]\n",
      "epoch:1 step:1503 [D loss: 0.576469, acc: 70.31%] [G loss: 3.158941]\n",
      "epoch:1 step:1504 [D loss: 0.552087, acc: 71.88%] [G loss: 3.210955]\n",
      "epoch:1 step:1505 [D loss: 0.493258, acc: 73.44%] [G loss: 3.217813]\n",
      "epoch:1 step:1506 [D loss: 0.674475, acc: 64.06%] [G loss: 2.773446]\n",
      "epoch:1 step:1507 [D loss: 0.522480, acc: 71.88%] [G loss: 3.082384]\n",
      "epoch:1 step:1508 [D loss: 0.547369, acc: 74.22%] [G loss: 3.052276]\n",
      "epoch:1 step:1509 [D loss: 0.551958, acc: 73.44%] [G loss: 2.611050]\n",
      "epoch:1 step:1510 [D loss: 0.615840, acc: 68.75%] [G loss: 2.870302]\n",
      "epoch:1 step:1511 [D loss: 0.487027, acc: 74.22%] [G loss: 3.028570]\n",
      "epoch:1 step:1512 [D loss: 0.663681, acc: 64.06%] [G loss: 3.089515]\n",
      "epoch:1 step:1513 [D loss: 0.528558, acc: 71.88%] [G loss: 2.757744]\n",
      "epoch:1 step:1514 [D loss: 0.563616, acc: 72.66%] [G loss: 2.814461]\n",
      "epoch:1 step:1515 [D loss: 0.517733, acc: 72.66%] [G loss: 2.870204]\n",
      "epoch:1 step:1516 [D loss: 0.462939, acc: 79.69%] [G loss: 2.819399]\n",
      "epoch:1 step:1517 [D loss: 0.554724, acc: 72.66%] [G loss: 3.030035]\n",
      "epoch:1 step:1518 [D loss: 0.531132, acc: 69.53%] [G loss: 2.863295]\n",
      "epoch:1 step:1519 [D loss: 0.496008, acc: 79.69%] [G loss: 3.264640]\n",
      "epoch:1 step:1520 [D loss: 0.544792, acc: 73.44%] [G loss: 3.217799]\n",
      "epoch:1 step:1521 [D loss: 0.639928, acc: 67.19%] [G loss: 2.673487]\n",
      "epoch:1 step:1522 [D loss: 0.540521, acc: 73.44%] [G loss: 3.192450]\n",
      "epoch:1 step:1523 [D loss: 0.699377, acc: 64.06%] [G loss: 2.840499]\n",
      "epoch:1 step:1524 [D loss: 0.619745, acc: 64.84%] [G loss: 2.962846]\n",
      "epoch:1 step:1525 [D loss: 0.581511, acc: 69.53%] [G loss: 2.953740]\n",
      "epoch:1 step:1526 [D loss: 0.455125, acc: 80.47%] [G loss: 3.150599]\n",
      "epoch:1 step:1527 [D loss: 0.597593, acc: 70.31%] [G loss: 3.315712]\n",
      "epoch:1 step:1528 [D loss: 0.606558, acc: 64.84%] [G loss: 2.636596]\n",
      "epoch:1 step:1529 [D loss: 0.594883, acc: 67.97%] [G loss: 3.284148]\n",
      "epoch:1 step:1530 [D loss: 0.634956, acc: 63.28%] [G loss: 2.921380]\n",
      "epoch:1 step:1531 [D loss: 0.502568, acc: 77.34%] [G loss: 2.926329]\n",
      "epoch:1 step:1532 [D loss: 0.579899, acc: 69.53%] [G loss: 2.783600]\n",
      "epoch:1 step:1533 [D loss: 0.522448, acc: 76.56%] [G loss: 2.853404]\n",
      "epoch:1 step:1534 [D loss: 0.597993, acc: 64.06%] [G loss: 2.798001]\n",
      "epoch:1 step:1535 [D loss: 0.545903, acc: 71.88%] [G loss: 2.832928]\n",
      "epoch:1 step:1536 [D loss: 0.605258, acc: 64.84%] [G loss: 2.472562]\n",
      "epoch:1 step:1537 [D loss: 0.562533, acc: 71.09%] [G loss: 2.993052]\n",
      "epoch:1 step:1538 [D loss: 0.547433, acc: 74.22%] [G loss: 2.853861]\n",
      "epoch:1 step:1539 [D loss: 0.521882, acc: 71.09%] [G loss: 2.966865]\n",
      "epoch:1 step:1540 [D loss: 0.544893, acc: 72.66%] [G loss: 2.723600]\n",
      "epoch:1 step:1541 [D loss: 0.610272, acc: 64.06%] [G loss: 2.996289]\n",
      "epoch:1 step:1542 [D loss: 0.462944, acc: 78.91%] [G loss: 2.949725]\n",
      "epoch:1 step:1543 [D loss: 0.513673, acc: 75.78%] [G loss: 3.275716]\n",
      "epoch:1 step:1544 [D loss: 0.577475, acc: 66.41%] [G loss: 3.115038]\n",
      "epoch:1 step:1545 [D loss: 0.539694, acc: 67.97%] [G loss: 3.118205]\n",
      "epoch:1 step:1546 [D loss: 0.557079, acc: 72.66%] [G loss: 2.850982]\n",
      "epoch:1 step:1547 [D loss: 0.513893, acc: 74.22%] [G loss: 2.875940]\n",
      "epoch:1 step:1548 [D loss: 0.558262, acc: 74.22%] [G loss: 2.823622]\n",
      "epoch:1 step:1549 [D loss: 0.567070, acc: 72.66%] [G loss: 2.980586]\n",
      "epoch:1 step:1550 [D loss: 0.567968, acc: 67.97%] [G loss: 3.148819]\n",
      "epoch:1 step:1551 [D loss: 0.615351, acc: 71.09%] [G loss: 3.008665]\n",
      "epoch:1 step:1552 [D loss: 0.600611, acc: 66.41%] [G loss: 3.076561]\n",
      "epoch:1 step:1553 [D loss: 0.565756, acc: 76.56%] [G loss: 3.225205]\n",
      "epoch:1 step:1554 [D loss: 0.590474, acc: 67.19%] [G loss: 2.772485]\n",
      "epoch:1 step:1555 [D loss: 0.589741, acc: 68.75%] [G loss: 2.909640]\n",
      "epoch:1 step:1556 [D loss: 0.513732, acc: 76.56%] [G loss: 2.847347]\n",
      "epoch:1 step:1557 [D loss: 0.502825, acc: 75.78%] [G loss: 3.181229]\n",
      "epoch:1 step:1558 [D loss: 0.625873, acc: 67.19%] [G loss: 2.914227]\n",
      "epoch:1 step:1559 [D loss: 0.672812, acc: 61.72%] [G loss: 2.511615]\n",
      "epoch:1 step:1560 [D loss: 0.502770, acc: 76.56%] [G loss: 2.842465]\n",
      "epoch:1 step:1561 [D loss: 0.595532, acc: 71.09%] [G loss: 2.732002]\n",
      "epoch:1 step:1562 [D loss: 0.526042, acc: 71.88%] [G loss: 2.640212]\n",
      "epoch:1 step:1563 [D loss: 0.570888, acc: 74.22%] [G loss: 2.739644]\n",
      "epoch:1 step:1564 [D loss: 0.626240, acc: 62.50%] [G loss: 2.871899]\n",
      "epoch:1 step:1565 [D loss: 0.549904, acc: 71.88%] [G loss: 2.910540]\n",
      "epoch:1 step:1566 [D loss: 0.528576, acc: 77.34%] [G loss: 2.964448]\n",
      "epoch:1 step:1567 [D loss: 0.588374, acc: 70.31%] [G loss: 2.834880]\n",
      "epoch:1 step:1568 [D loss: 0.607668, acc: 69.53%] [G loss: 2.806729]\n",
      "epoch:1 step:1569 [D loss: 0.618759, acc: 71.88%] [G loss: 2.818280]\n",
      "epoch:1 step:1570 [D loss: 0.498726, acc: 72.66%] [G loss: 3.231617]\n",
      "epoch:1 step:1571 [D loss: 0.475170, acc: 78.12%] [G loss: 3.196168]\n",
      "epoch:1 step:1572 [D loss: 0.484739, acc: 76.56%] [G loss: 2.996609]\n",
      "epoch:1 step:1573 [D loss: 0.650676, acc: 62.50%] [G loss: 2.617710]\n",
      "epoch:1 step:1574 [D loss: 0.556522, acc: 75.00%] [G loss: 2.851612]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:1 step:1575 [D loss: 0.606683, acc: 66.41%] [G loss: 2.865809]\n",
      "epoch:1 step:1576 [D loss: 0.534542, acc: 72.66%] [G loss: 3.016500]\n",
      "epoch:1 step:1577 [D loss: 0.560714, acc: 66.41%] [G loss: 3.039629]\n",
      "epoch:1 step:1578 [D loss: 0.536473, acc: 74.22%] [G loss: 3.158916]\n",
      "epoch:1 step:1579 [D loss: 0.457170, acc: 78.12%] [G loss: 3.539300]\n",
      "epoch:1 step:1580 [D loss: 0.695151, acc: 61.72%] [G loss: 2.794937]\n",
      "epoch:1 step:1581 [D loss: 0.514392, acc: 75.00%] [G loss: 2.842180]\n",
      "epoch:1 step:1582 [D loss: 0.668130, acc: 59.38%] [G loss: 2.543961]\n",
      "epoch:1 step:1583 [D loss: 0.569724, acc: 70.31%] [G loss: 2.842042]\n",
      "epoch:1 step:1584 [D loss: 0.640552, acc: 62.50%] [G loss: 2.849673]\n",
      "epoch:1 step:1585 [D loss: 0.484752, acc: 78.12%] [G loss: 3.041245]\n",
      "epoch:1 step:1586 [D loss: 0.640134, acc: 71.09%] [G loss: 2.928669]\n",
      "epoch:1 step:1587 [D loss: 0.681755, acc: 62.50%] [G loss: 2.868798]\n",
      "epoch:1 step:1588 [D loss: 0.488276, acc: 73.44%] [G loss: 2.962624]\n",
      "epoch:1 step:1589 [D loss: 0.653985, acc: 61.72%] [G loss: 2.643773]\n",
      "epoch:1 step:1590 [D loss: 0.634592, acc: 64.06%] [G loss: 2.629642]\n",
      "epoch:1 step:1591 [D loss: 0.539877, acc: 68.75%] [G loss: 2.902570]\n",
      "epoch:1 step:1592 [D loss: 0.651372, acc: 67.97%] [G loss: 2.735962]\n",
      "epoch:1 step:1593 [D loss: 0.530853, acc: 82.81%] [G loss: 2.576469]\n",
      "epoch:1 step:1594 [D loss: 0.478297, acc: 78.12%] [G loss: 2.875115]\n",
      "epoch:1 step:1595 [D loss: 0.584954, acc: 68.75%] [G loss: 3.158856]\n",
      "epoch:1 step:1596 [D loss: 0.578512, acc: 69.53%] [G loss: 2.867879]\n",
      "epoch:1 step:1597 [D loss: 0.601524, acc: 69.53%] [G loss: 3.050499]\n",
      "epoch:1 step:1598 [D loss: 0.558955, acc: 71.88%] [G loss: 3.020984]\n",
      "epoch:1 step:1599 [D loss: 0.582314, acc: 73.44%] [G loss: 2.986778]\n",
      "epoch:1 step:1600 [D loss: 0.508056, acc: 74.22%] [G loss: 3.350745]\n",
      "##############\n",
      "[3.53170886 2.23632754 7.55408412 5.9181322  4.974111   6.46359671\n",
      " 5.72022638 5.86165387 6.19693597 4.43876624]\n",
      "##########\n",
      "epoch:1 step:1601 [D loss: 0.588819, acc: 65.62%] [G loss: 2.851790]\n",
      "epoch:1 step:1602 [D loss: 0.580564, acc: 67.19%] [G loss: 2.842859]\n",
      "epoch:1 step:1603 [D loss: 0.549567, acc: 75.78%] [G loss: 2.950936]\n",
      "epoch:1 step:1604 [D loss: 0.517529, acc: 71.09%] [G loss: 2.848493]\n",
      "epoch:1 step:1605 [D loss: 0.537589, acc: 70.31%] [G loss: 3.021615]\n",
      "epoch:1 step:1606 [D loss: 0.533857, acc: 71.09%] [G loss: 2.976875]\n",
      "epoch:1 step:1607 [D loss: 0.544153, acc: 74.22%] [G loss: 2.767591]\n",
      "epoch:1 step:1608 [D loss: 0.516300, acc: 73.44%] [G loss: 3.094527]\n",
      "epoch:1 step:1609 [D loss: 0.710440, acc: 60.94%] [G loss: 2.858660]\n",
      "epoch:1 step:1610 [D loss: 0.539078, acc: 71.09%] [G loss: 3.011356]\n",
      "epoch:1 step:1611 [D loss: 0.599587, acc: 68.75%] [G loss: 3.028449]\n",
      "epoch:1 step:1612 [D loss: 0.593586, acc: 68.75%] [G loss: 3.231926]\n",
      "epoch:1 step:1613 [D loss: 0.628079, acc: 65.62%] [G loss: 2.800990]\n",
      "epoch:1 step:1614 [D loss: 0.491396, acc: 80.47%] [G loss: 2.927071]\n",
      "epoch:1 step:1615 [D loss: 0.560570, acc: 68.75%] [G loss: 2.809810]\n",
      "epoch:1 step:1616 [D loss: 0.534482, acc: 70.31%] [G loss: 2.846358]\n",
      "epoch:1 step:1617 [D loss: 0.500708, acc: 73.44%] [G loss: 3.143897]\n",
      "epoch:1 step:1618 [D loss: 0.579306, acc: 69.53%] [G loss: 2.759034]\n",
      "epoch:1 step:1619 [D loss: 0.625644, acc: 65.62%] [G loss: 2.847118]\n",
      "epoch:1 step:1620 [D loss: 0.615968, acc: 65.62%] [G loss: 2.862447]\n",
      "epoch:1 step:1621 [D loss: 0.578091, acc: 69.53%] [G loss: 3.000655]\n",
      "epoch:1 step:1622 [D loss: 0.593921, acc: 67.19%] [G loss: 2.894281]\n",
      "epoch:1 step:1623 [D loss: 0.486376, acc: 78.12%] [G loss: 2.890714]\n",
      "epoch:1 step:1624 [D loss: 0.572637, acc: 67.19%] [G loss: 2.793681]\n",
      "epoch:1 step:1625 [D loss: 0.693551, acc: 61.72%] [G loss: 2.492668]\n",
      "epoch:1 step:1626 [D loss: 0.586760, acc: 69.53%] [G loss: 2.858341]\n",
      "epoch:1 step:1627 [D loss: 0.617161, acc: 70.31%] [G loss: 2.643232]\n",
      "epoch:1 step:1628 [D loss: 0.613742, acc: 65.62%] [G loss: 2.629802]\n",
      "epoch:1 step:1629 [D loss: 0.488906, acc: 77.34%] [G loss: 2.755088]\n",
      "epoch:1 step:1630 [D loss: 0.528353, acc: 74.22%] [G loss: 2.878566]\n",
      "epoch:1 step:1631 [D loss: 0.544153, acc: 73.44%] [G loss: 2.994594]\n",
      "epoch:1 step:1632 [D loss: 0.532799, acc: 72.66%] [G loss: 3.151105]\n",
      "epoch:1 step:1633 [D loss: 0.633499, acc: 61.72%] [G loss: 2.849166]\n",
      "epoch:1 step:1634 [D loss: 0.446001, acc: 75.00%] [G loss: 3.128436]\n",
      "epoch:1 step:1635 [D loss: 0.545274, acc: 73.44%] [G loss: 3.159390]\n",
      "epoch:1 step:1636 [D loss: 0.551622, acc: 69.53%] [G loss: 2.996650]\n",
      "epoch:1 step:1637 [D loss: 0.507010, acc: 75.78%] [G loss: 2.955164]\n",
      "epoch:1 step:1638 [D loss: 0.626783, acc: 68.75%] [G loss: 2.923246]\n",
      "epoch:1 step:1639 [D loss: 0.512424, acc: 76.56%] [G loss: 3.057369]\n",
      "epoch:1 step:1640 [D loss: 0.621630, acc: 64.84%] [G loss: 3.134399]\n",
      "epoch:1 step:1641 [D loss: 0.508529, acc: 75.00%] [G loss: 3.113038]\n",
      "epoch:1 step:1642 [D loss: 0.643325, acc: 62.50%] [G loss: 2.852671]\n",
      "epoch:1 step:1643 [D loss: 0.547939, acc: 73.44%] [G loss: 3.012809]\n",
      "epoch:1 step:1644 [D loss: 0.520296, acc: 68.75%] [G loss: 3.264623]\n",
      "epoch:1 step:1645 [D loss: 0.486807, acc: 75.78%] [G loss: 3.326684]\n",
      "epoch:1 step:1646 [D loss: 0.518064, acc: 75.78%] [G loss: 3.647942]\n",
      "epoch:1 step:1647 [D loss: 0.583050, acc: 67.19%] [G loss: 2.935935]\n",
      "epoch:1 step:1648 [D loss: 0.653709, acc: 64.06%] [G loss: 2.774310]\n",
      "epoch:1 step:1649 [D loss: 0.501022, acc: 77.34%] [G loss: 2.988125]\n",
      "epoch:1 step:1650 [D loss: 0.537835, acc: 75.78%] [G loss: 3.103287]\n",
      "epoch:1 step:1651 [D loss: 0.555251, acc: 67.19%] [G loss: 3.073064]\n",
      "epoch:1 step:1652 [D loss: 0.628886, acc: 68.75%] [G loss: 2.630556]\n",
      "epoch:1 step:1653 [D loss: 0.583625, acc: 71.09%] [G loss: 2.840789]\n",
      "epoch:1 step:1654 [D loss: 0.572117, acc: 70.31%] [G loss: 2.707479]\n",
      "epoch:1 step:1655 [D loss: 0.641061, acc: 62.50%] [G loss: 2.791145]\n",
      "epoch:1 step:1656 [D loss: 0.634923, acc: 64.84%] [G loss: 2.824094]\n",
      "epoch:1 step:1657 [D loss: 0.648068, acc: 65.62%] [G loss: 3.104042]\n",
      "epoch:1 step:1658 [D loss: 0.613571, acc: 61.72%] [G loss: 2.938877]\n",
      "epoch:1 step:1659 [D loss: 0.679189, acc: 63.28%] [G loss: 2.820591]\n",
      "epoch:1 step:1660 [D loss: 0.544485, acc: 71.88%] [G loss: 2.872503]\n",
      "epoch:1 step:1661 [D loss: 0.529669, acc: 75.78%] [G loss: 2.740547]\n",
      "epoch:1 step:1662 [D loss: 0.556975, acc: 74.22%] [G loss: 2.634817]\n",
      "epoch:1 step:1663 [D loss: 0.554530, acc: 74.22%] [G loss: 2.881061]\n",
      "epoch:1 step:1664 [D loss: 0.615087, acc: 71.88%] [G loss: 2.691176]\n",
      "epoch:1 step:1665 [D loss: 0.465460, acc: 79.69%] [G loss: 2.895335]\n",
      "epoch:1 step:1666 [D loss: 0.566970, acc: 70.31%] [G loss: 3.040657]\n",
      "epoch:1 step:1667 [D loss: 0.592614, acc: 69.53%] [G loss: 3.110363]\n",
      "epoch:1 step:1668 [D loss: 0.505756, acc: 76.56%] [G loss: 3.153407]\n",
      "epoch:1 step:1669 [D loss: 0.460629, acc: 77.34%] [G loss: 3.098089]\n",
      "epoch:1 step:1670 [D loss: 0.549284, acc: 75.00%] [G loss: 3.282696]\n",
      "epoch:1 step:1671 [D loss: 0.520991, acc: 78.91%] [G loss: 3.279020]\n",
      "epoch:1 step:1672 [D loss: 0.566755, acc: 71.09%] [G loss: 2.920980]\n",
      "epoch:1 step:1673 [D loss: 0.662611, acc: 62.50%] [G loss: 3.080420]\n",
      "epoch:1 step:1674 [D loss: 0.489427, acc: 78.12%] [G loss: 3.119004]\n",
      "epoch:1 step:1675 [D loss: 0.542412, acc: 68.75%] [G loss: 3.035702]\n",
      "epoch:1 step:1676 [D loss: 0.559506, acc: 71.09%] [G loss: 3.109569]\n",
      "epoch:1 step:1677 [D loss: 0.577422, acc: 69.53%] [G loss: 2.902539]\n",
      "epoch:1 step:1678 [D loss: 0.531910, acc: 74.22%] [G loss: 3.188984]\n",
      "epoch:1 step:1679 [D loss: 0.548898, acc: 75.00%] [G loss: 2.782952]\n",
      "epoch:1 step:1680 [D loss: 0.563361, acc: 65.62%] [G loss: 3.086013]\n",
      "epoch:1 step:1681 [D loss: 0.672424, acc: 66.41%] [G loss: 3.429837]\n",
      "epoch:1 step:1682 [D loss: 0.564137, acc: 71.88%] [G loss: 3.194430]\n",
      "epoch:1 step:1683 [D loss: 0.523517, acc: 73.44%] [G loss: 3.504025]\n",
      "epoch:1 step:1684 [D loss: 0.512311, acc: 76.56%] [G loss: 4.011643]\n",
      "epoch:1 step:1685 [D loss: 0.583123, acc: 67.97%] [G loss: 2.912420]\n",
      "epoch:1 step:1686 [D loss: 0.485018, acc: 79.69%] [G loss: 3.441344]\n",
      "epoch:1 step:1687 [D loss: 0.589561, acc: 69.53%] [G loss: 2.738436]\n",
      "epoch:1 step:1688 [D loss: 0.488960, acc: 78.91%] [G loss: 3.135481]\n",
      "epoch:1 step:1689 [D loss: 0.500666, acc: 75.00%] [G loss: 3.274842]\n",
      "epoch:1 step:1690 [D loss: 0.504114, acc: 77.34%] [G loss: 3.380789]\n",
      "epoch:1 step:1691 [D loss: 0.503423, acc: 75.78%] [G loss: 3.435568]\n",
      "epoch:1 step:1692 [D loss: 0.549520, acc: 73.44%] [G loss: 3.085946]\n",
      "epoch:1 step:1693 [D loss: 0.518793, acc: 75.78%] [G loss: 3.298850]\n",
      "epoch:1 step:1694 [D loss: 0.502234, acc: 74.22%] [G loss: 3.371161]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:1 step:1695 [D loss: 0.636225, acc: 67.19%] [G loss: 3.040453]\n",
      "epoch:1 step:1696 [D loss: 0.519600, acc: 71.09%] [G loss: 3.186049]\n",
      "epoch:1 step:1697 [D loss: 0.583095, acc: 75.00%] [G loss: 3.055162]\n",
      "epoch:1 step:1698 [D loss: 0.514815, acc: 72.66%] [G loss: 3.152141]\n",
      "epoch:1 step:1699 [D loss: 0.532005, acc: 72.66%] [G loss: 3.298655]\n",
      "epoch:1 step:1700 [D loss: 0.586348, acc: 75.00%] [G loss: 3.025271]\n",
      "epoch:1 step:1701 [D loss: 0.523182, acc: 71.88%] [G loss: 3.106913]\n",
      "epoch:1 step:1702 [D loss: 0.601340, acc: 69.53%] [G loss: 2.768036]\n",
      "epoch:1 step:1703 [D loss: 0.611678, acc: 65.62%] [G loss: 2.724120]\n",
      "epoch:1 step:1704 [D loss: 0.622402, acc: 67.19%] [G loss: 2.882889]\n",
      "epoch:1 step:1705 [D loss: 0.636265, acc: 64.06%] [G loss: 2.820570]\n",
      "epoch:1 step:1706 [D loss: 0.507994, acc: 74.22%] [G loss: 2.967849]\n",
      "epoch:1 step:1707 [D loss: 0.512190, acc: 79.69%] [G loss: 2.826443]\n",
      "epoch:1 step:1708 [D loss: 0.631281, acc: 62.50%] [G loss: 2.703103]\n",
      "epoch:1 step:1709 [D loss: 0.645801, acc: 68.75%] [G loss: 2.810383]\n",
      "epoch:1 step:1710 [D loss: 0.593378, acc: 73.44%] [G loss: 2.754277]\n",
      "epoch:1 step:1711 [D loss: 0.640863, acc: 61.72%] [G loss: 2.833256]\n",
      "epoch:1 step:1712 [D loss: 0.494751, acc: 78.12%] [G loss: 3.135157]\n",
      "epoch:1 step:1713 [D loss: 0.543806, acc: 75.00%] [G loss: 2.752899]\n",
      "epoch:1 step:1714 [D loss: 0.578353, acc: 69.53%] [G loss: 2.734883]\n",
      "epoch:1 step:1715 [D loss: 0.637971, acc: 64.06%] [G loss: 3.066439]\n",
      "epoch:1 step:1716 [D loss: 0.547042, acc: 67.97%] [G loss: 2.740798]\n",
      "epoch:1 step:1717 [D loss: 0.584214, acc: 71.09%] [G loss: 3.234600]\n",
      "epoch:1 step:1718 [D loss: 0.464497, acc: 82.03%] [G loss: 3.059217]\n",
      "epoch:1 step:1719 [D loss: 0.544474, acc: 71.88%] [G loss: 3.094605]\n",
      "epoch:1 step:1720 [D loss: 0.603138, acc: 67.19%] [G loss: 3.129319]\n",
      "epoch:1 step:1721 [D loss: 0.509373, acc: 73.44%] [G loss: 2.813125]\n",
      "epoch:1 step:1722 [D loss: 0.480218, acc: 75.00%] [G loss: 2.921830]\n",
      "epoch:1 step:1723 [D loss: 0.546705, acc: 71.88%] [G loss: 2.913350]\n",
      "epoch:1 step:1724 [D loss: 0.682977, acc: 60.94%] [G loss: 2.859262]\n",
      "epoch:1 step:1725 [D loss: 0.533991, acc: 75.78%] [G loss: 2.762505]\n",
      "epoch:1 step:1726 [D loss: 0.600336, acc: 72.66%] [G loss: 2.994299]\n",
      "epoch:1 step:1727 [D loss: 0.449073, acc: 78.12%] [G loss: 3.323188]\n",
      "epoch:1 step:1728 [D loss: 0.587207, acc: 67.97%] [G loss: 3.040770]\n",
      "epoch:1 step:1729 [D loss: 0.444622, acc: 78.12%] [G loss: 3.569140]\n",
      "epoch:1 step:1730 [D loss: 0.633673, acc: 66.41%] [G loss: 2.861305]\n",
      "epoch:1 step:1731 [D loss: 0.509865, acc: 71.88%] [G loss: 2.948012]\n",
      "epoch:1 step:1732 [D loss: 0.537388, acc: 75.00%] [G loss: 3.308289]\n",
      "epoch:1 step:1733 [D loss: 0.500552, acc: 78.91%] [G loss: 3.111230]\n",
      "epoch:1 step:1734 [D loss: 0.538006, acc: 75.78%] [G loss: 2.853161]\n",
      "epoch:1 step:1735 [D loss: 0.521664, acc: 74.22%] [G loss: 3.306015]\n",
      "epoch:1 step:1736 [D loss: 0.543719, acc: 70.31%] [G loss: 2.915373]\n",
      "epoch:1 step:1737 [D loss: 0.629841, acc: 66.41%] [G loss: 2.728163]\n",
      "epoch:1 step:1738 [D loss: 0.599358, acc: 70.31%] [G loss: 2.745075]\n",
      "epoch:1 step:1739 [D loss: 0.550911, acc: 76.56%] [G loss: 3.373573]\n",
      "epoch:1 step:1740 [D loss: 0.502237, acc: 75.00%] [G loss: 3.087966]\n",
      "epoch:1 step:1741 [D loss: 0.528946, acc: 75.78%] [G loss: 2.947171]\n",
      "epoch:1 step:1742 [D loss: 0.529222, acc: 71.88%] [G loss: 3.158839]\n",
      "epoch:1 step:1743 [D loss: 0.530722, acc: 75.78%] [G loss: 3.287270]\n",
      "epoch:1 step:1744 [D loss: 0.568194, acc: 69.53%] [G loss: 2.875045]\n",
      "epoch:1 step:1745 [D loss: 0.577300, acc: 66.41%] [G loss: 2.740042]\n",
      "epoch:1 step:1746 [D loss: 0.617462, acc: 69.53%] [G loss: 2.751723]\n",
      "epoch:1 step:1747 [D loss: 0.500747, acc: 78.91%] [G loss: 2.792285]\n",
      "epoch:1 step:1748 [D loss: 0.486825, acc: 78.12%] [G loss: 2.826565]\n",
      "epoch:1 step:1749 [D loss: 0.522212, acc: 68.75%] [G loss: 2.993095]\n",
      "epoch:1 step:1750 [D loss: 0.533731, acc: 76.56%] [G loss: 2.992311]\n",
      "epoch:1 step:1751 [D loss: 0.541229, acc: 71.88%] [G loss: 2.830490]\n",
      "epoch:1 step:1752 [D loss: 0.530699, acc: 72.66%] [G loss: 3.104592]\n",
      "epoch:1 step:1753 [D loss: 0.534022, acc: 74.22%] [G loss: 3.128575]\n",
      "epoch:1 step:1754 [D loss: 0.592579, acc: 67.19%] [G loss: 2.653323]\n",
      "epoch:1 step:1755 [D loss: 0.597963, acc: 67.97%] [G loss: 3.017682]\n",
      "epoch:1 step:1756 [D loss: 0.564944, acc: 71.09%] [G loss: 3.049602]\n",
      "epoch:1 step:1757 [D loss: 0.581170, acc: 71.88%] [G loss: 2.878224]\n",
      "epoch:1 step:1758 [D loss: 0.493248, acc: 74.22%] [G loss: 3.135805]\n",
      "epoch:1 step:1759 [D loss: 0.488756, acc: 80.47%] [G loss: 3.086221]\n",
      "epoch:1 step:1760 [D loss: 0.523736, acc: 73.44%] [G loss: 2.796550]\n",
      "epoch:1 step:1761 [D loss: 0.605411, acc: 69.53%] [G loss: 2.660290]\n",
      "epoch:1 step:1762 [D loss: 0.527061, acc: 80.47%] [G loss: 2.878725]\n",
      "epoch:1 step:1763 [D loss: 0.611764, acc: 64.06%] [G loss: 3.000027]\n",
      "epoch:1 step:1764 [D loss: 0.592534, acc: 68.75%] [G loss: 2.903002]\n",
      "epoch:1 step:1765 [D loss: 0.677437, acc: 64.06%] [G loss: 3.024867]\n",
      "epoch:1 step:1766 [D loss: 0.566222, acc: 68.75%] [G loss: 3.061860]\n",
      "epoch:1 step:1767 [D loss: 0.474806, acc: 80.47%] [G loss: 2.998827]\n",
      "epoch:1 step:1768 [D loss: 0.543569, acc: 70.31%] [G loss: 3.256903]\n",
      "epoch:1 step:1769 [D loss: 0.560322, acc: 74.22%] [G loss: 2.976103]\n",
      "epoch:1 step:1770 [D loss: 0.507935, acc: 72.66%] [G loss: 3.318314]\n",
      "epoch:1 step:1771 [D loss: 0.551903, acc: 73.44%] [G loss: 2.696169]\n",
      "epoch:1 step:1772 [D loss: 0.538405, acc: 72.66%] [G loss: 3.064145]\n",
      "epoch:1 step:1773 [D loss: 0.554612, acc: 68.75%] [G loss: 2.859066]\n",
      "epoch:1 step:1774 [D loss: 0.477113, acc: 74.22%] [G loss: 2.984016]\n",
      "epoch:1 step:1775 [D loss: 0.492519, acc: 75.78%] [G loss: 2.889217]\n",
      "epoch:1 step:1776 [D loss: 0.480414, acc: 77.34%] [G loss: 3.054592]\n",
      "epoch:1 step:1777 [D loss: 0.671424, acc: 58.59%] [G loss: 2.760118]\n",
      "epoch:1 step:1778 [D loss: 0.522112, acc: 75.00%] [G loss: 3.045142]\n",
      "epoch:1 step:1779 [D loss: 0.546572, acc: 69.53%] [G loss: 3.285936]\n",
      "epoch:1 step:1780 [D loss: 0.529318, acc: 73.44%] [G loss: 3.073836]\n",
      "epoch:1 step:1781 [D loss: 0.745184, acc: 53.91%] [G loss: 2.881613]\n",
      "epoch:1 step:1782 [D loss: 0.569638, acc: 71.88%] [G loss: 2.765005]\n",
      "epoch:1 step:1783 [D loss: 0.508030, acc: 73.44%] [G loss: 3.242590]\n",
      "epoch:1 step:1784 [D loss: 0.583792, acc: 71.09%] [G loss: 3.151909]\n",
      "epoch:1 step:1785 [D loss: 0.481034, acc: 81.25%] [G loss: 3.231674]\n",
      "epoch:1 step:1786 [D loss: 0.622221, acc: 67.19%] [G loss: 2.855569]\n",
      "epoch:1 step:1787 [D loss: 0.480540, acc: 78.12%] [G loss: 2.865519]\n",
      "epoch:1 step:1788 [D loss: 0.598018, acc: 70.31%] [G loss: 3.142440]\n",
      "epoch:1 step:1789 [D loss: 0.575443, acc: 68.75%] [G loss: 3.186684]\n",
      "epoch:1 step:1790 [D loss: 0.586466, acc: 69.53%] [G loss: 3.050619]\n",
      "epoch:1 step:1791 [D loss: 0.484552, acc: 77.34%] [G loss: 3.249253]\n",
      "epoch:1 step:1792 [D loss: 0.527570, acc: 71.09%] [G loss: 3.099355]\n",
      "epoch:1 step:1793 [D loss: 0.598987, acc: 69.53%] [G loss: 2.798113]\n",
      "epoch:1 step:1794 [D loss: 0.433320, acc: 78.91%] [G loss: 3.180309]\n",
      "epoch:1 step:1795 [D loss: 0.712146, acc: 60.94%] [G loss: 2.972985]\n",
      "epoch:1 step:1796 [D loss: 0.675750, acc: 60.94%] [G loss: 2.721052]\n",
      "epoch:1 step:1797 [D loss: 0.564873, acc: 66.41%] [G loss: 2.934813]\n",
      "epoch:1 step:1798 [D loss: 0.585035, acc: 68.75%] [G loss: 2.702800]\n",
      "epoch:1 step:1799 [D loss: 0.508844, acc: 75.78%] [G loss: 2.985766]\n",
      "epoch:1 step:1800 [D loss: 0.588365, acc: 69.53%] [G loss: 3.367805]\n",
      "##############\n",
      "[3.59285994 2.07934805 7.61226758 5.81278657 4.86537454 6.38991873\n",
      " 5.81866556 5.68709661 5.90520761 4.31939032]\n",
      "##########\n",
      "epoch:1 step:1801 [D loss: 0.553951, acc: 75.78%] [G loss: 3.061213]\n",
      "epoch:1 step:1802 [D loss: 0.577149, acc: 71.88%] [G loss: 3.032831]\n",
      "epoch:1 step:1803 [D loss: 0.587475, acc: 67.97%] [G loss: 3.051202]\n",
      "epoch:1 step:1804 [D loss: 0.500609, acc: 76.56%] [G loss: 2.872791]\n",
      "epoch:1 step:1805 [D loss: 0.635059, acc: 66.41%] [G loss: 2.584676]\n",
      "epoch:1 step:1806 [D loss: 0.565366, acc: 71.09%] [G loss: 3.126891]\n",
      "epoch:1 step:1807 [D loss: 0.499180, acc: 75.78%] [G loss: 2.844569]\n",
      "epoch:1 step:1808 [D loss: 0.502751, acc: 71.88%] [G loss: 2.858742]\n",
      "epoch:1 step:1809 [D loss: 0.521048, acc: 75.78%] [G loss: 3.223595]\n",
      "epoch:1 step:1810 [D loss: 0.537706, acc: 71.88%] [G loss: 3.348020]\n",
      "epoch:1 step:1811 [D loss: 0.520538, acc: 75.00%] [G loss: 3.147926]\n",
      "epoch:1 step:1812 [D loss: 0.557471, acc: 71.09%] [G loss: 3.143013]\n",
      "epoch:1 step:1813 [D loss: 0.498542, acc: 76.56%] [G loss: 2.797320]\n",
      "epoch:1 step:1814 [D loss: 0.541426, acc: 71.88%] [G loss: 3.122770]\n",
      "epoch:1 step:1815 [D loss: 0.560964, acc: 70.31%] [G loss: 2.780282]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:1 step:1816 [D loss: 0.518848, acc: 71.09%] [G loss: 2.946248]\n",
      "epoch:1 step:1817 [D loss: 0.546649, acc: 74.22%] [G loss: 2.558737]\n",
      "epoch:1 step:1818 [D loss: 0.576013, acc: 72.66%] [G loss: 2.816915]\n",
      "epoch:1 step:1819 [D loss: 0.576959, acc: 67.19%] [G loss: 2.934948]\n",
      "epoch:1 step:1820 [D loss: 0.613682, acc: 67.19%] [G loss: 2.809554]\n",
      "epoch:1 step:1821 [D loss: 0.514047, acc: 77.34%] [G loss: 2.925009]\n",
      "epoch:1 step:1822 [D loss: 0.530544, acc: 72.66%] [G loss: 3.307886]\n",
      "epoch:1 step:1823 [D loss: 0.557562, acc: 73.44%] [G loss: 3.307523]\n",
      "epoch:1 step:1824 [D loss: 0.543016, acc: 73.44%] [G loss: 2.964014]\n",
      "epoch:1 step:1825 [D loss: 0.548660, acc: 67.19%] [G loss: 3.359244]\n",
      "epoch:1 step:1826 [D loss: 0.467999, acc: 75.00%] [G loss: 3.069688]\n",
      "epoch:1 step:1827 [D loss: 0.467571, acc: 79.69%] [G loss: 3.254295]\n",
      "epoch:1 step:1828 [D loss: 0.640737, acc: 64.84%] [G loss: 2.808629]\n",
      "epoch:1 step:1829 [D loss: 0.650684, acc: 65.62%] [G loss: 2.801464]\n",
      "epoch:1 step:1830 [D loss: 0.581403, acc: 67.97%] [G loss: 2.738237]\n",
      "epoch:1 step:1831 [D loss: 0.544026, acc: 69.53%] [G loss: 2.838583]\n",
      "epoch:1 step:1832 [D loss: 0.539349, acc: 75.78%] [G loss: 2.803279]\n",
      "epoch:1 step:1833 [D loss: 0.557917, acc: 75.00%] [G loss: 2.966003]\n",
      "epoch:1 step:1834 [D loss: 0.653565, acc: 64.84%] [G loss: 3.010272]\n",
      "epoch:1 step:1835 [D loss: 0.484530, acc: 76.56%] [G loss: 3.242898]\n",
      "epoch:1 step:1836 [D loss: 0.558496, acc: 71.88%] [G loss: 3.205480]\n",
      "epoch:1 step:1837 [D loss: 0.579279, acc: 70.31%] [G loss: 3.356030]\n",
      "epoch:1 step:1838 [D loss: 0.482588, acc: 75.78%] [G loss: 3.023642]\n",
      "epoch:1 step:1839 [D loss: 0.649489, acc: 64.06%] [G loss: 2.998905]\n",
      "epoch:1 step:1840 [D loss: 0.480504, acc: 77.34%] [G loss: 2.729307]\n",
      "epoch:1 step:1841 [D loss: 0.537471, acc: 72.66%] [G loss: 2.994260]\n",
      "epoch:1 step:1842 [D loss: 0.505994, acc: 75.00%] [G loss: 3.054495]\n",
      "epoch:1 step:1843 [D loss: 0.521386, acc: 76.56%] [G loss: 3.039113]\n",
      "epoch:1 step:1844 [D loss: 0.559047, acc: 68.75%] [G loss: 3.142390]\n",
      "epoch:1 step:1845 [D loss: 0.564687, acc: 68.75%] [G loss: 2.737160]\n",
      "epoch:1 step:1846 [D loss: 0.553295, acc: 71.09%] [G loss: 2.857368]\n",
      "epoch:1 step:1847 [D loss: 0.435616, acc: 78.12%] [G loss: 3.331748]\n",
      "epoch:1 step:1848 [D loss: 0.596315, acc: 68.75%] [G loss: 2.972078]\n",
      "epoch:1 step:1849 [D loss: 0.558088, acc: 72.66%] [G loss: 3.281440]\n",
      "epoch:1 step:1850 [D loss: 0.596009, acc: 67.19%] [G loss: 2.992531]\n",
      "epoch:1 step:1851 [D loss: 0.459397, acc: 79.69%] [G loss: 3.378931]\n",
      "epoch:1 step:1852 [D loss: 0.647462, acc: 59.38%] [G loss: 2.999656]\n",
      "epoch:1 step:1853 [D loss: 0.566833, acc: 72.66%] [G loss: 3.201929]\n",
      "epoch:1 step:1854 [D loss: 0.554193, acc: 74.22%] [G loss: 2.830560]\n",
      "epoch:1 step:1855 [D loss: 0.565493, acc: 71.88%] [G loss: 3.154141]\n",
      "epoch:1 step:1856 [D loss: 0.577771, acc: 72.66%] [G loss: 3.135618]\n",
      "epoch:1 step:1857 [D loss: 0.729984, acc: 55.47%] [G loss: 3.222992]\n",
      "epoch:1 step:1858 [D loss: 0.458277, acc: 80.47%] [G loss: 3.714703]\n",
      "epoch:1 step:1859 [D loss: 0.521507, acc: 70.31%] [G loss: 3.152390]\n",
      "epoch:1 step:1860 [D loss: 0.496459, acc: 78.91%] [G loss: 3.544651]\n",
      "epoch:1 step:1861 [D loss: 0.427494, acc: 84.38%] [G loss: 3.426312]\n",
      "epoch:1 step:1862 [D loss: 0.445890, acc: 83.59%] [G loss: 3.803802]\n",
      "epoch:1 step:1863 [D loss: 0.538262, acc: 74.22%] [G loss: 3.796804]\n",
      "epoch:1 step:1864 [D loss: 0.471505, acc: 78.91%] [G loss: 3.255783]\n",
      "epoch:1 step:1865 [D loss: 0.856107, acc: 55.47%] [G loss: 3.261857]\n",
      "epoch:1 step:1866 [D loss: 0.510776, acc: 75.00%] [G loss: 3.838439]\n",
      "epoch:1 step:1867 [D loss: 0.440198, acc: 81.25%] [G loss: 3.761889]\n",
      "epoch:1 step:1868 [D loss: 0.725818, acc: 63.28%] [G loss: 3.474114]\n",
      "epoch:1 step:1869 [D loss: 0.584135, acc: 68.75%] [G loss: 2.947721]\n",
      "epoch:1 step:1870 [D loss: 0.480251, acc: 75.78%] [G loss: 3.314861]\n",
      "epoch:1 step:1871 [D loss: 0.564302, acc: 71.88%] [G loss: 3.115158]\n",
      "epoch:1 step:1872 [D loss: 0.456762, acc: 77.34%] [G loss: 3.260352]\n",
      "epoch:1 step:1873 [D loss: 0.383619, acc: 85.16%] [G loss: 3.947124]\n",
      "epoch:1 step:1874 [D loss: 0.729061, acc: 54.69%] [G loss: 2.896876]\n",
      "epoch:2 step:1875 [D loss: 0.482932, acc: 78.12%] [G loss: 3.315959]\n",
      "epoch:2 step:1876 [D loss: 0.480379, acc: 75.00%] [G loss: 3.618760]\n",
      "epoch:2 step:1877 [D loss: 0.623583, acc: 67.97%] [G loss: 2.718249]\n",
      "epoch:2 step:1878 [D loss: 0.561028, acc: 72.66%] [G loss: 3.118240]\n",
      "epoch:2 step:1879 [D loss: 0.506581, acc: 75.78%] [G loss: 3.196320]\n",
      "epoch:2 step:1880 [D loss: 0.499595, acc: 75.78%] [G loss: 3.364106]\n",
      "epoch:2 step:1881 [D loss: 0.535205, acc: 75.78%] [G loss: 2.714487]\n",
      "epoch:2 step:1882 [D loss: 0.612843, acc: 65.62%] [G loss: 2.939380]\n",
      "epoch:2 step:1883 [D loss: 0.623778, acc: 67.19%] [G loss: 3.249147]\n",
      "epoch:2 step:1884 [D loss: 0.571496, acc: 68.75%] [G loss: 3.203838]\n",
      "epoch:2 step:1885 [D loss: 0.583287, acc: 63.28%] [G loss: 3.290660]\n",
      "epoch:2 step:1886 [D loss: 0.565777, acc: 68.75%] [G loss: 2.934409]\n",
      "epoch:2 step:1887 [D loss: 0.515966, acc: 72.66%] [G loss: 2.933117]\n",
      "epoch:2 step:1888 [D loss: 0.515357, acc: 78.91%] [G loss: 3.165715]\n",
      "epoch:2 step:1889 [D loss: 0.537587, acc: 74.22%] [G loss: 3.041472]\n",
      "epoch:2 step:1890 [D loss: 0.556217, acc: 75.78%] [G loss: 2.837076]\n",
      "epoch:2 step:1891 [D loss: 0.671258, acc: 61.72%] [G loss: 2.745833]\n",
      "epoch:2 step:1892 [D loss: 0.623937, acc: 64.06%] [G loss: 2.567130]\n",
      "epoch:2 step:1893 [D loss: 0.544624, acc: 71.09%] [G loss: 2.777777]\n",
      "epoch:2 step:1894 [D loss: 0.579279, acc: 70.31%] [G loss: 2.620384]\n",
      "epoch:2 step:1895 [D loss: 0.700866, acc: 59.38%] [G loss: 2.875996]\n",
      "epoch:2 step:1896 [D loss: 0.462638, acc: 82.81%] [G loss: 3.276516]\n",
      "epoch:2 step:1897 [D loss: 0.498317, acc: 73.44%] [G loss: 3.201640]\n",
      "epoch:2 step:1898 [D loss: 0.457069, acc: 78.91%] [G loss: 3.160538]\n",
      "epoch:2 step:1899 [D loss: 0.520577, acc: 76.56%] [G loss: 3.130728]\n",
      "epoch:2 step:1900 [D loss: 0.586793, acc: 66.41%] [G loss: 2.999248]\n",
      "epoch:2 step:1901 [D loss: 0.580999, acc: 67.97%] [G loss: 3.051629]\n",
      "epoch:2 step:1902 [D loss: 0.566953, acc: 70.31%] [G loss: 2.974828]\n",
      "epoch:2 step:1903 [D loss: 0.565706, acc: 71.88%] [G loss: 3.053226]\n",
      "epoch:2 step:1904 [D loss: 0.630552, acc: 64.84%] [G loss: 2.873941]\n",
      "epoch:2 step:1905 [D loss: 0.588937, acc: 67.19%] [G loss: 3.065961]\n",
      "epoch:2 step:1906 [D loss: 0.521389, acc: 70.31%] [G loss: 3.016456]\n",
      "epoch:2 step:1907 [D loss: 0.627098, acc: 63.28%] [G loss: 3.304174]\n",
      "epoch:2 step:1908 [D loss: 0.561077, acc: 71.88%] [G loss: 2.889370]\n",
      "epoch:2 step:1909 [D loss: 0.567060, acc: 69.53%] [G loss: 3.071747]\n",
      "epoch:2 step:1910 [D loss: 0.523166, acc: 67.97%] [G loss: 3.201068]\n",
      "epoch:2 step:1911 [D loss: 0.459779, acc: 78.91%] [G loss: 2.670685]\n",
      "epoch:2 step:1912 [D loss: 0.566496, acc: 72.66%] [G loss: 2.886716]\n",
      "epoch:2 step:1913 [D loss: 0.546078, acc: 65.62%] [G loss: 3.249476]\n",
      "epoch:2 step:1914 [D loss: 0.453585, acc: 78.12%] [G loss: 3.067231]\n",
      "epoch:2 step:1915 [D loss: 0.550734, acc: 71.09%] [G loss: 3.075114]\n",
      "epoch:2 step:1916 [D loss: 0.514850, acc: 72.66%] [G loss: 3.187795]\n",
      "epoch:2 step:1917 [D loss: 0.597197, acc: 71.09%] [G loss: 2.901901]\n",
      "epoch:2 step:1918 [D loss: 0.554110, acc: 71.09%] [G loss: 3.073022]\n",
      "epoch:2 step:1919 [D loss: 0.558712, acc: 67.19%] [G loss: 2.874428]\n",
      "epoch:2 step:1920 [D loss: 0.627699, acc: 71.09%] [G loss: 3.042823]\n",
      "epoch:2 step:1921 [D loss: 0.588996, acc: 68.75%] [G loss: 2.916031]\n",
      "epoch:2 step:1922 [D loss: 0.574085, acc: 71.09%] [G loss: 2.902413]\n",
      "epoch:2 step:1923 [D loss: 0.492724, acc: 71.88%] [G loss: 3.152410]\n",
      "epoch:2 step:1924 [D loss: 0.646369, acc: 65.62%] [G loss: 3.056177]\n",
      "epoch:2 step:1925 [D loss: 0.579778, acc: 67.97%] [G loss: 3.063921]\n",
      "epoch:2 step:1926 [D loss: 0.579387, acc: 67.97%] [G loss: 2.843631]\n",
      "epoch:2 step:1927 [D loss: 0.484770, acc: 75.00%] [G loss: 3.262464]\n",
      "epoch:2 step:1928 [D loss: 0.555241, acc: 67.19%] [G loss: 3.252525]\n",
      "epoch:2 step:1929 [D loss: 0.533730, acc: 71.09%] [G loss: 3.044539]\n",
      "epoch:2 step:1930 [D loss: 0.611695, acc: 67.97%] [G loss: 3.413425]\n",
      "epoch:2 step:1931 [D loss: 0.561929, acc: 75.00%] [G loss: 3.272784]\n",
      "epoch:2 step:1932 [D loss: 0.517352, acc: 75.00%] [G loss: 3.414432]\n",
      "epoch:2 step:1933 [D loss: 0.549797, acc: 71.09%] [G loss: 3.129428]\n",
      "epoch:2 step:1934 [D loss: 0.562344, acc: 69.53%] [G loss: 3.096858]\n",
      "epoch:2 step:1935 [D loss: 0.571153, acc: 72.66%] [G loss: 3.365019]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:2 step:1936 [D loss: 0.693278, acc: 59.38%] [G loss: 2.885549]\n",
      "epoch:2 step:1937 [D loss: 0.471929, acc: 75.00%] [G loss: 3.066935]\n",
      "epoch:2 step:1938 [D loss: 0.636056, acc: 64.84%] [G loss: 2.704598]\n",
      "epoch:2 step:1939 [D loss: 0.598080, acc: 67.97%] [G loss: 3.141924]\n",
      "epoch:2 step:1940 [D loss: 0.517167, acc: 75.00%] [G loss: 3.229446]\n",
      "epoch:2 step:1941 [D loss: 0.449061, acc: 81.25%] [G loss: 3.082859]\n",
      "epoch:2 step:1942 [D loss: 0.539482, acc: 71.88%] [G loss: 2.738329]\n",
      "epoch:2 step:1943 [D loss: 0.703045, acc: 57.81%] [G loss: 2.632278]\n",
      "epoch:2 step:1944 [D loss: 0.575387, acc: 74.22%] [G loss: 2.663235]\n",
      "epoch:2 step:1945 [D loss: 0.560540, acc: 68.75%] [G loss: 2.855762]\n",
      "epoch:2 step:1946 [D loss: 0.434796, acc: 82.03%] [G loss: 3.151547]\n",
      "epoch:2 step:1947 [D loss: 0.595658, acc: 67.19%] [G loss: 3.066222]\n",
      "epoch:2 step:1948 [D loss: 0.560480, acc: 70.31%] [G loss: 3.104333]\n",
      "epoch:2 step:1949 [D loss: 0.557207, acc: 70.31%] [G loss: 3.569329]\n",
      "epoch:2 step:1950 [D loss: 0.538478, acc: 71.88%] [G loss: 3.164251]\n",
      "epoch:2 step:1951 [D loss: 0.491919, acc: 77.34%] [G loss: 3.750000]\n",
      "epoch:2 step:1952 [D loss: 0.563172, acc: 70.31%] [G loss: 3.176510]\n",
      "epoch:2 step:1953 [D loss: 0.593971, acc: 65.62%] [G loss: 2.856199]\n",
      "epoch:2 step:1954 [D loss: 0.554806, acc: 72.66%] [G loss: 2.963755]\n",
      "epoch:2 step:1955 [D loss: 0.561113, acc: 69.53%] [G loss: 2.983694]\n",
      "epoch:2 step:1956 [D loss: 0.424219, acc: 84.38%] [G loss: 3.023290]\n",
      "epoch:2 step:1957 [D loss: 0.523999, acc: 71.88%] [G loss: 2.843256]\n",
      "epoch:2 step:1958 [D loss: 0.528677, acc: 73.44%] [G loss: 2.997852]\n",
      "epoch:2 step:1959 [D loss: 0.542609, acc: 73.44%] [G loss: 3.008579]\n",
      "epoch:2 step:1960 [D loss: 0.518543, acc: 72.66%] [G loss: 3.331533]\n",
      "epoch:2 step:1961 [D loss: 0.478495, acc: 77.34%] [G loss: 3.298346]\n",
      "epoch:2 step:1962 [D loss: 0.486388, acc: 75.78%] [G loss: 3.341447]\n",
      "epoch:2 step:1963 [D loss: 0.395616, acc: 85.94%] [G loss: 3.343156]\n",
      "epoch:2 step:1964 [D loss: 0.566579, acc: 68.75%] [G loss: 2.893663]\n",
      "epoch:2 step:1965 [D loss: 0.560072, acc: 72.66%] [G loss: 3.124661]\n",
      "epoch:2 step:1966 [D loss: 0.527504, acc: 76.56%] [G loss: 3.043375]\n",
      "epoch:2 step:1967 [D loss: 0.479370, acc: 78.12%] [G loss: 3.308433]\n",
      "epoch:2 step:1968 [D loss: 0.645276, acc: 68.75%] [G loss: 2.974440]\n",
      "epoch:2 step:1969 [D loss: 0.485656, acc: 76.56%] [G loss: 3.127054]\n",
      "epoch:2 step:1970 [D loss: 0.499800, acc: 73.44%] [G loss: 3.699362]\n",
      "epoch:2 step:1971 [D loss: 0.582942, acc: 64.06%] [G loss: 3.150753]\n",
      "epoch:2 step:1972 [D loss: 0.512150, acc: 75.78%] [G loss: 3.062842]\n",
      "epoch:2 step:1973 [D loss: 0.628454, acc: 67.97%] [G loss: 3.081408]\n",
      "epoch:2 step:1974 [D loss: 0.545177, acc: 78.12%] [G loss: 3.297090]\n",
      "epoch:2 step:1975 [D loss: 0.597760, acc: 66.41%] [G loss: 2.898867]\n",
      "epoch:2 step:1976 [D loss: 0.559334, acc: 72.66%] [G loss: 3.295476]\n",
      "epoch:2 step:1977 [D loss: 0.395193, acc: 82.81%] [G loss: 3.388114]\n",
      "epoch:2 step:1978 [D loss: 0.545284, acc: 68.75%] [G loss: 3.250168]\n",
      "epoch:2 step:1979 [D loss: 0.564470, acc: 69.53%] [G loss: 2.974235]\n",
      "epoch:2 step:1980 [D loss: 0.603026, acc: 71.09%] [G loss: 2.501950]\n",
      "epoch:2 step:1981 [D loss: 0.696467, acc: 60.94%] [G loss: 2.522932]\n",
      "epoch:2 step:1982 [D loss: 0.621320, acc: 72.66%] [G loss: 2.720835]\n",
      "epoch:2 step:1983 [D loss: 0.567227, acc: 68.75%] [G loss: 2.872584]\n",
      "epoch:2 step:1984 [D loss: 0.506115, acc: 79.69%] [G loss: 3.246010]\n",
      "epoch:2 step:1985 [D loss: 0.552244, acc: 71.09%] [G loss: 3.046439]\n",
      "epoch:2 step:1986 [D loss: 0.625471, acc: 64.06%] [G loss: 3.174181]\n",
      "epoch:2 step:1987 [D loss: 0.610882, acc: 66.41%] [G loss: 2.618399]\n",
      "epoch:2 step:1988 [D loss: 0.564356, acc: 69.53%] [G loss: 2.881729]\n",
      "epoch:2 step:1989 [D loss: 0.651520, acc: 64.84%] [G loss: 2.895604]\n",
      "epoch:2 step:1990 [D loss: 0.633127, acc: 60.16%] [G loss: 2.870344]\n",
      "epoch:2 step:1991 [D loss: 0.617649, acc: 66.41%] [G loss: 2.971420]\n",
      "epoch:2 step:1992 [D loss: 0.474363, acc: 81.25%] [G loss: 3.212406]\n",
      "epoch:2 step:1993 [D loss: 0.581051, acc: 71.09%] [G loss: 3.789357]\n",
      "epoch:2 step:1994 [D loss: 0.619357, acc: 63.28%] [G loss: 2.797428]\n",
      "epoch:2 step:1995 [D loss: 0.595156, acc: 69.53%] [G loss: 2.575100]\n",
      "epoch:2 step:1996 [D loss: 0.656084, acc: 60.16%] [G loss: 2.896092]\n",
      "epoch:2 step:1997 [D loss: 0.570972, acc: 71.09%] [G loss: 2.833192]\n",
      "epoch:2 step:1998 [D loss: 0.602832, acc: 64.84%] [G loss: 2.964215]\n",
      "epoch:2 step:1999 [D loss: 0.514737, acc: 75.00%] [G loss: 3.099315]\n",
      "epoch:2 step:2000 [D loss: 0.579478, acc: 68.75%] [G loss: 3.210369]\n",
      "##############\n",
      "[3.31201233 2.08725948 7.26559552 5.72817949 4.76563652 6.33318165\n",
      " 5.64104292 5.65657352 5.8212543  4.2224129 ]\n",
      "##########\n",
      "epoch:2 step:2001 [D loss: 0.587911, acc: 70.31%] [G loss: 2.898423]\n",
      "epoch:2 step:2002 [D loss: 0.526796, acc: 74.22%] [G loss: 2.856052]\n",
      "epoch:2 step:2003 [D loss: 0.558105, acc: 69.53%] [G loss: 2.753315]\n",
      "epoch:2 step:2004 [D loss: 0.524625, acc: 75.78%] [G loss: 2.909836]\n",
      "epoch:2 step:2005 [D loss: 0.543517, acc: 71.88%] [G loss: 3.148766]\n",
      "epoch:2 step:2006 [D loss: 0.503530, acc: 74.22%] [G loss: 2.829223]\n",
      "epoch:2 step:2007 [D loss: 0.658544, acc: 64.06%] [G loss: 3.025290]\n",
      "epoch:2 step:2008 [D loss: 0.537325, acc: 71.09%] [G loss: 3.082195]\n",
      "epoch:2 step:2009 [D loss: 0.477897, acc: 75.00%] [G loss: 3.232545]\n",
      "epoch:2 step:2010 [D loss: 0.517134, acc: 74.22%] [G loss: 2.927350]\n",
      "epoch:2 step:2011 [D loss: 0.696963, acc: 59.38%] [G loss: 2.860980]\n",
      "epoch:2 step:2012 [D loss: 0.549744, acc: 72.66%] [G loss: 2.706801]\n",
      "epoch:2 step:2013 [D loss: 0.613087, acc: 67.19%] [G loss: 2.661462]\n",
      "epoch:2 step:2014 [D loss: 0.505822, acc: 75.78%] [G loss: 3.099717]\n",
      "epoch:2 step:2015 [D loss: 0.540749, acc: 72.66%] [G loss: 2.982884]\n",
      "epoch:2 step:2016 [D loss: 0.492420, acc: 77.34%] [G loss: 2.775609]\n",
      "epoch:2 step:2017 [D loss: 0.626979, acc: 62.50%] [G loss: 2.588777]\n",
      "epoch:2 step:2018 [D loss: 0.477530, acc: 75.00%] [G loss: 3.079076]\n",
      "epoch:2 step:2019 [D loss: 0.608636, acc: 64.84%] [G loss: 2.850521]\n",
      "epoch:2 step:2020 [D loss: 0.525056, acc: 70.31%] [G loss: 3.150573]\n",
      "epoch:2 step:2021 [D loss: 0.507554, acc: 71.88%] [G loss: 3.343473]\n",
      "epoch:2 step:2022 [D loss: 0.527540, acc: 74.22%] [G loss: 3.146935]\n",
      "epoch:2 step:2023 [D loss: 0.571718, acc: 72.66%] [G loss: 3.129023]\n",
      "epoch:2 step:2024 [D loss: 0.573424, acc: 73.44%] [G loss: 2.943433]\n",
      "epoch:2 step:2025 [D loss: 0.529283, acc: 76.56%] [G loss: 3.252546]\n",
      "epoch:2 step:2026 [D loss: 0.541038, acc: 69.53%] [G loss: 3.655570]\n",
      "epoch:2 step:2027 [D loss: 0.596185, acc: 71.09%] [G loss: 3.013662]\n",
      "epoch:2 step:2028 [D loss: 0.629495, acc: 66.41%] [G loss: 3.002184]\n",
      "epoch:2 step:2029 [D loss: 0.505685, acc: 73.44%] [G loss: 3.235730]\n",
      "epoch:2 step:2030 [D loss: 0.638876, acc: 60.16%] [G loss: 2.717344]\n",
      "epoch:2 step:2031 [D loss: 0.528197, acc: 77.34%] [G loss: 2.901076]\n",
      "epoch:2 step:2032 [D loss: 0.526851, acc: 73.44%] [G loss: 2.913899]\n",
      "epoch:2 step:2033 [D loss: 0.569489, acc: 70.31%] [G loss: 3.523299]\n",
      "epoch:2 step:2034 [D loss: 0.528331, acc: 75.78%] [G loss: 2.853144]\n",
      "epoch:2 step:2035 [D loss: 0.578598, acc: 68.75%] [G loss: 2.843926]\n",
      "epoch:2 step:2036 [D loss: 0.431049, acc: 81.25%] [G loss: 3.361204]\n",
      "epoch:2 step:2037 [D loss: 0.477283, acc: 78.91%] [G loss: 3.332793]\n",
      "epoch:2 step:2038 [D loss: 0.571609, acc: 71.09%] [G loss: 3.018338]\n",
      "epoch:2 step:2039 [D loss: 0.486209, acc: 80.47%] [G loss: 3.188808]\n",
      "epoch:2 step:2040 [D loss: 0.682088, acc: 65.62%] [G loss: 2.948978]\n",
      "epoch:2 step:2041 [D loss: 0.618211, acc: 67.19%] [G loss: 2.840158]\n",
      "epoch:2 step:2042 [D loss: 0.569612, acc: 69.53%] [G loss: 3.003019]\n",
      "epoch:2 step:2043 [D loss: 0.508341, acc: 75.00%] [G loss: 2.843263]\n",
      "epoch:2 step:2044 [D loss: 0.463085, acc: 78.91%] [G loss: 3.240909]\n",
      "epoch:2 step:2045 [D loss: 0.522652, acc: 74.22%] [G loss: 2.937160]\n",
      "epoch:2 step:2046 [D loss: 0.565010, acc: 75.00%] [G loss: 3.032857]\n",
      "epoch:2 step:2047 [D loss: 0.508433, acc: 84.38%] [G loss: 3.170809]\n",
      "epoch:2 step:2048 [D loss: 0.533492, acc: 75.00%] [G loss: 3.108044]\n",
      "epoch:2 step:2049 [D loss: 0.514963, acc: 73.44%] [G loss: 3.074864]\n",
      "epoch:2 step:2050 [D loss: 0.465334, acc: 80.47%] [G loss: 3.075482]\n",
      "epoch:2 step:2051 [D loss: 0.564039, acc: 71.09%] [G loss: 3.356536]\n",
      "epoch:2 step:2052 [D loss: 0.515320, acc: 76.56%] [G loss: 3.213075]\n",
      "epoch:2 step:2053 [D loss: 0.546444, acc: 71.88%] [G loss: 2.908892]\n",
      "epoch:2 step:2054 [D loss: 0.498627, acc: 75.00%] [G loss: 2.889660]\n",
      "epoch:2 step:2055 [D loss: 0.529065, acc: 71.09%] [G loss: 2.844759]\n",
      "epoch:2 step:2056 [D loss: 0.684989, acc: 59.38%] [G loss: 2.897975]\n",
      "epoch:2 step:2057 [D loss: 0.553308, acc: 70.31%] [G loss: 3.042059]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:2 step:2058 [D loss: 0.565150, acc: 64.06%] [G loss: 3.078515]\n",
      "epoch:2 step:2059 [D loss: 0.625713, acc: 64.06%] [G loss: 2.895703]\n",
      "epoch:2 step:2060 [D loss: 0.651198, acc: 67.19%] [G loss: 3.008531]\n",
      "epoch:2 step:2061 [D loss: 0.515938, acc: 71.09%] [G loss: 2.610477]\n",
      "epoch:2 step:2062 [D loss: 0.489610, acc: 76.56%] [G loss: 3.057268]\n",
      "epoch:2 step:2063 [D loss: 0.518017, acc: 71.88%] [G loss: 2.920001]\n",
      "epoch:2 step:2064 [D loss: 0.549213, acc: 69.53%] [G loss: 3.376575]\n",
      "epoch:2 step:2065 [D loss: 0.552846, acc: 71.09%] [G loss: 2.951838]\n",
      "epoch:2 step:2066 [D loss: 0.540075, acc: 79.69%] [G loss: 3.017269]\n",
      "epoch:2 step:2067 [D loss: 0.524895, acc: 72.66%] [G loss: 2.927492]\n",
      "epoch:2 step:2068 [D loss: 0.487553, acc: 76.56%] [G loss: 3.127500]\n",
      "epoch:2 step:2069 [D loss: 0.506416, acc: 73.44%] [G loss: 3.108798]\n",
      "epoch:2 step:2070 [D loss: 0.596355, acc: 70.31%] [G loss: 2.980415]\n",
      "epoch:2 step:2071 [D loss: 0.547047, acc: 67.19%] [G loss: 3.006930]\n",
      "epoch:2 step:2072 [D loss: 0.492165, acc: 78.12%] [G loss: 3.033811]\n",
      "epoch:2 step:2073 [D loss: 0.491565, acc: 75.00%] [G loss: 3.246933]\n",
      "epoch:2 step:2074 [D loss: 0.657187, acc: 62.50%] [G loss: 3.070347]\n",
      "epoch:2 step:2075 [D loss: 0.637443, acc: 63.28%] [G loss: 3.353677]\n",
      "epoch:2 step:2076 [D loss: 0.583835, acc: 71.09%] [G loss: 2.944316]\n",
      "epoch:2 step:2077 [D loss: 0.647389, acc: 65.62%] [G loss: 2.947717]\n",
      "epoch:2 step:2078 [D loss: 0.503400, acc: 73.44%] [G loss: 3.090254]\n",
      "epoch:2 step:2079 [D loss: 0.649707, acc: 71.09%] [G loss: 3.130051]\n",
      "epoch:2 step:2080 [D loss: 0.498662, acc: 76.56%] [G loss: 2.883424]\n",
      "epoch:2 step:2081 [D loss: 0.492213, acc: 73.44%] [G loss: 3.161614]\n",
      "epoch:2 step:2082 [D loss: 0.470034, acc: 81.25%] [G loss: 3.239068]\n",
      "epoch:2 step:2083 [D loss: 0.533376, acc: 75.00%] [G loss: 3.177928]\n",
      "epoch:2 step:2084 [D loss: 0.521356, acc: 73.44%] [G loss: 3.090282]\n",
      "epoch:2 step:2085 [D loss: 0.479749, acc: 76.56%] [G loss: 3.013306]\n",
      "epoch:2 step:2086 [D loss: 0.532623, acc: 76.56%] [G loss: 3.206423]\n",
      "epoch:2 step:2087 [D loss: 0.430584, acc: 80.47%] [G loss: 3.497625]\n",
      "epoch:2 step:2088 [D loss: 0.631301, acc: 61.72%] [G loss: 2.689087]\n",
      "epoch:2 step:2089 [D loss: 0.595928, acc: 68.75%] [G loss: 2.783324]\n",
      "epoch:2 step:2090 [D loss: 0.515045, acc: 76.56%] [G loss: 2.770399]\n",
      "epoch:2 step:2091 [D loss: 0.486577, acc: 75.78%] [G loss: 3.237823]\n",
      "epoch:2 step:2092 [D loss: 0.530660, acc: 72.66%] [G loss: 3.360803]\n",
      "epoch:2 step:2093 [D loss: 0.598877, acc: 71.09%] [G loss: 2.911453]\n",
      "epoch:2 step:2094 [D loss: 0.569160, acc: 73.44%] [G loss: 3.282677]\n",
      "epoch:2 step:2095 [D loss: 0.613550, acc: 67.19%] [G loss: 3.250016]\n",
      "epoch:2 step:2096 [D loss: 0.578819, acc: 75.78%] [G loss: 3.280004]\n",
      "epoch:2 step:2097 [D loss: 0.496891, acc: 79.69%] [G loss: 3.484504]\n",
      "epoch:2 step:2098 [D loss: 0.643428, acc: 71.09%] [G loss: 3.133287]\n",
      "epoch:2 step:2099 [D loss: 0.458882, acc: 82.03%] [G loss: 3.038448]\n",
      "epoch:2 step:2100 [D loss: 0.609117, acc: 64.06%] [G loss: 2.919484]\n",
      "epoch:2 step:2101 [D loss: 0.636526, acc: 67.19%] [G loss: 2.789538]\n",
      "epoch:2 step:2102 [D loss: 0.611106, acc: 68.75%] [G loss: 2.747347]\n",
      "epoch:2 step:2103 [D loss: 0.537935, acc: 70.31%] [G loss: 2.680707]\n",
      "epoch:2 step:2104 [D loss: 0.461725, acc: 78.91%] [G loss: 3.669620]\n",
      "epoch:2 step:2105 [D loss: 0.506725, acc: 72.66%] [G loss: 3.415823]\n",
      "epoch:2 step:2106 [D loss: 0.388257, acc: 82.03%] [G loss: 3.710394]\n",
      "epoch:2 step:2107 [D loss: 0.594531, acc: 71.09%] [G loss: 3.675569]\n",
      "epoch:2 step:2108 [D loss: 0.593718, acc: 67.97%] [G loss: 3.352034]\n",
      "epoch:2 step:2109 [D loss: 0.530889, acc: 73.44%] [G loss: 3.339495]\n",
      "epoch:2 step:2110 [D loss: 0.490884, acc: 71.88%] [G loss: 2.956886]\n",
      "epoch:2 step:2111 [D loss: 0.544324, acc: 72.66%] [G loss: 2.761916]\n",
      "epoch:2 step:2112 [D loss: 0.584972, acc: 69.53%] [G loss: 3.039748]\n",
      "epoch:2 step:2113 [D loss: 0.736407, acc: 57.81%] [G loss: 2.993282]\n",
      "epoch:2 step:2114 [D loss: 0.498290, acc: 76.56%] [G loss: 3.148901]\n",
      "epoch:2 step:2115 [D loss: 0.491083, acc: 78.12%] [G loss: 3.097496]\n",
      "epoch:2 step:2116 [D loss: 0.619289, acc: 64.84%] [G loss: 2.959873]\n",
      "epoch:2 step:2117 [D loss: 0.524451, acc: 74.22%] [G loss: 3.093372]\n",
      "epoch:2 step:2118 [D loss: 0.620504, acc: 64.84%] [G loss: 3.198886]\n",
      "epoch:2 step:2119 [D loss: 0.494043, acc: 74.22%] [G loss: 3.112296]\n",
      "epoch:2 step:2120 [D loss: 0.664909, acc: 64.06%] [G loss: 3.077673]\n",
      "epoch:2 step:2121 [D loss: 0.541016, acc: 72.66%] [G loss: 2.933376]\n",
      "epoch:2 step:2122 [D loss: 0.705686, acc: 57.03%] [G loss: 2.674863]\n",
      "epoch:2 step:2123 [D loss: 0.551239, acc: 69.53%] [G loss: 2.991458]\n",
      "epoch:2 step:2124 [D loss: 0.586160, acc: 70.31%] [G loss: 2.915310]\n",
      "epoch:2 step:2125 [D loss: 0.635313, acc: 66.41%] [G loss: 2.852771]\n",
      "epoch:2 step:2126 [D loss: 0.602006, acc: 67.19%] [G loss: 2.567377]\n",
      "epoch:2 step:2127 [D loss: 0.515852, acc: 73.44%] [G loss: 2.656216]\n",
      "epoch:2 step:2128 [D loss: 0.513200, acc: 73.44%] [G loss: 3.018238]\n",
      "epoch:2 step:2129 [D loss: 0.465638, acc: 78.12%] [G loss: 3.275894]\n",
      "epoch:2 step:2130 [D loss: 0.488694, acc: 77.34%] [G loss: 3.071442]\n",
      "epoch:2 step:2131 [D loss: 0.558662, acc: 75.00%] [G loss: 3.126964]\n",
      "epoch:2 step:2132 [D loss: 0.531096, acc: 73.44%] [G loss: 3.060553]\n",
      "epoch:2 step:2133 [D loss: 0.517235, acc: 71.09%] [G loss: 3.179019]\n",
      "epoch:2 step:2134 [D loss: 0.571426, acc: 70.31%] [G loss: 2.873741]\n",
      "epoch:2 step:2135 [D loss: 0.496606, acc: 68.75%] [G loss: 3.492747]\n",
      "epoch:2 step:2136 [D loss: 0.571057, acc: 71.88%] [G loss: 3.290128]\n",
      "epoch:2 step:2137 [D loss: 0.619997, acc: 64.06%] [G loss: 2.590978]\n",
      "epoch:2 step:2138 [D loss: 0.675778, acc: 57.81%] [G loss: 3.011174]\n",
      "epoch:2 step:2139 [D loss: 0.603439, acc: 69.53%] [G loss: 3.032546]\n",
      "epoch:2 step:2140 [D loss: 0.535728, acc: 78.12%] [G loss: 2.750294]\n",
      "epoch:2 step:2141 [D loss: 0.644519, acc: 64.06%] [G loss: 3.149678]\n",
      "epoch:2 step:2142 [D loss: 0.566524, acc: 71.09%] [G loss: 3.046239]\n",
      "epoch:2 step:2143 [D loss: 0.700524, acc: 59.38%] [G loss: 2.538565]\n",
      "epoch:2 step:2144 [D loss: 0.564466, acc: 73.44%] [G loss: 2.425502]\n",
      "epoch:2 step:2145 [D loss: 0.457611, acc: 79.69%] [G loss: 2.947604]\n",
      "epoch:2 step:2146 [D loss: 0.621032, acc: 66.41%] [G loss: 2.763311]\n",
      "epoch:2 step:2147 [D loss: 0.547454, acc: 71.88%] [G loss: 2.926256]\n",
      "epoch:2 step:2148 [D loss: 0.573566, acc: 74.22%] [G loss: 2.681721]\n",
      "epoch:2 step:2149 [D loss: 0.570953, acc: 75.78%] [G loss: 2.853793]\n",
      "epoch:2 step:2150 [D loss: 0.527871, acc: 68.75%] [G loss: 2.663518]\n",
      "epoch:2 step:2151 [D loss: 0.703778, acc: 60.16%] [G loss: 2.928449]\n",
      "epoch:2 step:2152 [D loss: 0.608136, acc: 67.97%] [G loss: 3.106027]\n",
      "epoch:2 step:2153 [D loss: 0.541754, acc: 70.31%] [G loss: 3.200705]\n",
      "epoch:2 step:2154 [D loss: 0.466977, acc: 72.66%] [G loss: 2.846515]\n",
      "epoch:2 step:2155 [D loss: 0.550326, acc: 74.22%] [G loss: 3.102461]\n",
      "epoch:2 step:2156 [D loss: 0.557892, acc: 67.97%] [G loss: 3.138323]\n",
      "epoch:2 step:2157 [D loss: 0.633285, acc: 62.50%] [G loss: 3.148101]\n",
      "epoch:2 step:2158 [D loss: 0.493039, acc: 77.34%] [G loss: 3.021666]\n",
      "epoch:2 step:2159 [D loss: 0.565534, acc: 74.22%] [G loss: 2.818487]\n",
      "epoch:2 step:2160 [D loss: 0.445049, acc: 82.81%] [G loss: 3.066242]\n",
      "epoch:2 step:2161 [D loss: 0.690942, acc: 64.06%] [G loss: 2.867081]\n",
      "epoch:2 step:2162 [D loss: 0.590751, acc: 67.19%] [G loss: 3.000880]\n",
      "epoch:2 step:2163 [D loss: 0.562391, acc: 72.66%] [G loss: 2.981534]\n",
      "epoch:2 step:2164 [D loss: 0.540188, acc: 71.09%] [G loss: 3.135955]\n",
      "epoch:2 step:2165 [D loss: 0.567529, acc: 74.22%] [G loss: 2.769782]\n",
      "epoch:2 step:2166 [D loss: 0.633000, acc: 66.41%] [G loss: 3.051274]\n",
      "epoch:2 step:2167 [D loss: 0.499981, acc: 79.69%] [G loss: 3.243029]\n",
      "epoch:2 step:2168 [D loss: 0.511964, acc: 76.56%] [G loss: 3.129118]\n",
      "epoch:2 step:2169 [D loss: 0.593358, acc: 65.62%] [G loss: 3.009966]\n",
      "epoch:2 step:2170 [D loss: 0.535190, acc: 71.88%] [G loss: 3.210712]\n",
      "epoch:2 step:2171 [D loss: 0.516719, acc: 72.66%] [G loss: 2.711707]\n",
      "epoch:2 step:2172 [D loss: 0.542062, acc: 67.19%] [G loss: 3.206304]\n",
      "epoch:2 step:2173 [D loss: 0.594605, acc: 67.97%] [G loss: 2.943704]\n",
      "epoch:2 step:2174 [D loss: 0.531318, acc: 69.53%] [G loss: 2.785982]\n",
      "epoch:2 step:2175 [D loss: 0.661798, acc: 64.06%] [G loss: 2.777855]\n",
      "epoch:2 step:2176 [D loss: 0.573544, acc: 72.66%] [G loss: 3.038200]\n",
      "epoch:2 step:2177 [D loss: 0.616811, acc: 67.97%] [G loss: 2.895433]\n",
      "epoch:2 step:2178 [D loss: 0.471502, acc: 78.12%] [G loss: 3.317363]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:2 step:2179 [D loss: 0.501063, acc: 76.56%] [G loss: 3.020270]\n",
      "epoch:2 step:2180 [D loss: 0.591865, acc: 67.19%] [G loss: 2.875754]\n",
      "epoch:2 step:2181 [D loss: 0.459594, acc: 81.25%] [G loss: 3.059218]\n",
      "epoch:2 step:2182 [D loss: 0.451195, acc: 76.56%] [G loss: 3.190209]\n",
      "epoch:2 step:2183 [D loss: 0.429781, acc: 82.03%] [G loss: 3.408734]\n",
      "epoch:2 step:2184 [D loss: 0.515680, acc: 75.00%] [G loss: 3.496838]\n",
      "epoch:2 step:2185 [D loss: 0.474747, acc: 77.34%] [G loss: 3.433270]\n",
      "epoch:2 step:2186 [D loss: 0.490152, acc: 78.12%] [G loss: 3.549177]\n",
      "epoch:2 step:2187 [D loss: 0.507679, acc: 73.44%] [G loss: 3.573973]\n",
      "epoch:2 step:2188 [D loss: 0.429461, acc: 81.25%] [G loss: 3.781268]\n",
      "epoch:2 step:2189 [D loss: 0.433413, acc: 79.69%] [G loss: 3.620116]\n",
      "epoch:2 step:2190 [D loss: 0.783134, acc: 56.25%] [G loss: 2.706784]\n",
      "epoch:2 step:2191 [D loss: 0.540859, acc: 74.22%] [G loss: 2.878500]\n",
      "epoch:2 step:2192 [D loss: 0.492345, acc: 76.56%] [G loss: 2.940969]\n",
      "epoch:2 step:2193 [D loss: 0.631688, acc: 64.84%] [G loss: 2.887967]\n",
      "epoch:2 step:2194 [D loss: 0.494851, acc: 74.22%] [G loss: 3.068394]\n",
      "epoch:2 step:2195 [D loss: 0.500094, acc: 78.91%] [G loss: 3.207497]\n",
      "epoch:2 step:2196 [D loss: 0.599878, acc: 64.84%] [G loss: 2.831873]\n",
      "epoch:2 step:2197 [D loss: 0.558408, acc: 65.62%] [G loss: 3.215084]\n",
      "epoch:2 step:2198 [D loss: 0.537781, acc: 70.31%] [G loss: 3.017003]\n",
      "epoch:2 step:2199 [D loss: 0.612371, acc: 69.53%] [G loss: 2.883110]\n",
      "epoch:2 step:2200 [D loss: 0.581446, acc: 67.19%] [G loss: 2.693959]\n",
      "##############\n",
      "[3.22992651 1.8121111  7.57776161 5.8580167  4.85785558 6.54216388\n",
      " 5.58328424 5.67156018 6.00323386 4.29314092]\n",
      "##########\n",
      "epoch:2 step:2201 [D loss: 0.644348, acc: 64.84%] [G loss: 2.674767]\n",
      "epoch:2 step:2202 [D loss: 0.588814, acc: 69.53%] [G loss: 2.821259]\n",
      "epoch:2 step:2203 [D loss: 0.517128, acc: 72.66%] [G loss: 3.113925]\n",
      "epoch:2 step:2204 [D loss: 0.496106, acc: 77.34%] [G loss: 3.282629]\n",
      "epoch:2 step:2205 [D loss: 0.499540, acc: 73.44%] [G loss: 3.036178]\n",
      "epoch:2 step:2206 [D loss: 0.504644, acc: 76.56%] [G loss: 3.230924]\n",
      "epoch:2 step:2207 [D loss: 0.506516, acc: 75.00%] [G loss: 3.057407]\n",
      "epoch:2 step:2208 [D loss: 0.603082, acc: 64.06%] [G loss: 3.215529]\n",
      "epoch:2 step:2209 [D loss: 0.559735, acc: 71.88%] [G loss: 3.514328]\n",
      "epoch:2 step:2210 [D loss: 0.548477, acc: 73.44%] [G loss: 2.965832]\n",
      "epoch:2 step:2211 [D loss: 0.530703, acc: 73.44%] [G loss: 3.001268]\n",
      "epoch:2 step:2212 [D loss: 0.631617, acc: 66.41%] [G loss: 3.266808]\n",
      "epoch:2 step:2213 [D loss: 0.541712, acc: 72.66%] [G loss: 2.765948]\n",
      "epoch:2 step:2214 [D loss: 0.593091, acc: 70.31%] [G loss: 2.826476]\n",
      "epoch:2 step:2215 [D loss: 0.544295, acc: 71.88%] [G loss: 3.196845]\n",
      "epoch:2 step:2216 [D loss: 0.552985, acc: 70.31%] [G loss: 2.975852]\n",
      "epoch:2 step:2217 [D loss: 0.530415, acc: 75.78%] [G loss: 3.400171]\n",
      "epoch:2 step:2218 [D loss: 0.443789, acc: 82.03%] [G loss: 3.495906]\n",
      "epoch:2 step:2219 [D loss: 0.531909, acc: 73.44%] [G loss: 3.852954]\n",
      "epoch:2 step:2220 [D loss: 0.472434, acc: 78.12%] [G loss: 3.677486]\n",
      "epoch:2 step:2221 [D loss: 0.510001, acc: 76.56%] [G loss: 3.819431]\n",
      "epoch:2 step:2222 [D loss: 0.591513, acc: 64.06%] [G loss: 3.205806]\n",
      "epoch:2 step:2223 [D loss: 0.669704, acc: 62.50%] [G loss: 2.606738]\n",
      "epoch:2 step:2224 [D loss: 0.490336, acc: 77.34%] [G loss: 3.233605]\n",
      "epoch:2 step:2225 [D loss: 0.540357, acc: 77.34%] [G loss: 3.072642]\n",
      "epoch:2 step:2226 [D loss: 0.640727, acc: 64.06%] [G loss: 2.882306]\n",
      "epoch:2 step:2227 [D loss: 0.640938, acc: 61.72%] [G loss: 2.677750]\n",
      "epoch:2 step:2228 [D loss: 0.524694, acc: 73.44%] [G loss: 2.874674]\n",
      "epoch:2 step:2229 [D loss: 0.629065, acc: 67.19%] [G loss: 3.011595]\n",
      "epoch:2 step:2230 [D loss: 0.465316, acc: 73.44%] [G loss: 3.168692]\n",
      "epoch:2 step:2231 [D loss: 0.574966, acc: 70.31%] [G loss: 3.133481]\n",
      "epoch:2 step:2232 [D loss: 0.496862, acc: 81.25%] [G loss: 2.911637]\n",
      "epoch:2 step:2233 [D loss: 0.547937, acc: 73.44%] [G loss: 3.013600]\n",
      "epoch:2 step:2234 [D loss: 0.568985, acc: 71.88%] [G loss: 2.902392]\n",
      "epoch:2 step:2235 [D loss: 0.583021, acc: 66.41%] [G loss: 3.004200]\n",
      "epoch:2 step:2236 [D loss: 0.630415, acc: 64.06%] [G loss: 2.727438]\n",
      "epoch:2 step:2237 [D loss: 0.484809, acc: 77.34%] [G loss: 2.918653]\n",
      "epoch:2 step:2238 [D loss: 0.536576, acc: 70.31%] [G loss: 2.806362]\n",
      "epoch:2 step:2239 [D loss: 0.590643, acc: 67.19%] [G loss: 2.992615]\n",
      "epoch:2 step:2240 [D loss: 0.417967, acc: 85.16%] [G loss: 3.142973]\n",
      "epoch:2 step:2241 [D loss: 0.619874, acc: 71.09%] [G loss: 2.849171]\n",
      "epoch:2 step:2242 [D loss: 0.546012, acc: 68.75%] [G loss: 3.281601]\n",
      "epoch:2 step:2243 [D loss: 0.548854, acc: 71.88%] [G loss: 2.722066]\n",
      "epoch:2 step:2244 [D loss: 0.577403, acc: 67.97%] [G loss: 2.828200]\n",
      "epoch:2 step:2245 [D loss: 0.551974, acc: 73.44%] [G loss: 3.097322]\n",
      "epoch:2 step:2246 [D loss: 0.578017, acc: 69.53%] [G loss: 2.983075]\n",
      "epoch:2 step:2247 [D loss: 0.579062, acc: 65.62%] [G loss: 2.716166]\n",
      "epoch:2 step:2248 [D loss: 0.485540, acc: 78.91%] [G loss: 3.436177]\n",
      "epoch:2 step:2249 [D loss: 0.528950, acc: 73.44%] [G loss: 2.946103]\n",
      "epoch:2 step:2250 [D loss: 0.608521, acc: 72.66%] [G loss: 2.636581]\n",
      "epoch:2 step:2251 [D loss: 0.607380, acc: 68.75%] [G loss: 2.842449]\n",
      "epoch:2 step:2252 [D loss: 0.523015, acc: 78.12%] [G loss: 3.009700]\n",
      "epoch:2 step:2253 [D loss: 0.632390, acc: 64.06%] [G loss: 2.845448]\n",
      "epoch:2 step:2254 [D loss: 0.571135, acc: 71.09%] [G loss: 2.859535]\n",
      "epoch:2 step:2255 [D loss: 0.476308, acc: 75.78%] [G loss: 3.201208]\n",
      "epoch:2 step:2256 [D loss: 0.624356, acc: 71.09%] [G loss: 3.060351]\n",
      "epoch:2 step:2257 [D loss: 0.585908, acc: 66.41%] [G loss: 3.064814]\n",
      "epoch:2 step:2258 [D loss: 0.594687, acc: 68.75%] [G loss: 2.727541]\n",
      "epoch:2 step:2259 [D loss: 0.538220, acc: 71.88%] [G loss: 3.075647]\n",
      "epoch:2 step:2260 [D loss: 0.568073, acc: 68.75%] [G loss: 2.689770]\n",
      "epoch:2 step:2261 [D loss: 0.602859, acc: 69.53%] [G loss: 2.867510]\n",
      "epoch:2 step:2262 [D loss: 0.610104, acc: 67.97%] [G loss: 3.057272]\n",
      "epoch:2 step:2263 [D loss: 0.567562, acc: 68.75%] [G loss: 2.902425]\n",
      "epoch:2 step:2264 [D loss: 0.527128, acc: 78.91%] [G loss: 2.884012]\n",
      "epoch:2 step:2265 [D loss: 0.592835, acc: 71.09%] [G loss: 3.137449]\n",
      "epoch:2 step:2266 [D loss: 0.582694, acc: 66.41%] [G loss: 3.310559]\n",
      "epoch:2 step:2267 [D loss: 0.539256, acc: 69.53%] [G loss: 2.985230]\n",
      "epoch:2 step:2268 [D loss: 0.542457, acc: 67.19%] [G loss: 2.980849]\n",
      "epoch:2 step:2269 [D loss: 0.553940, acc: 70.31%] [G loss: 2.906952]\n",
      "epoch:2 step:2270 [D loss: 0.594609, acc: 69.53%] [G loss: 3.261577]\n",
      "epoch:2 step:2271 [D loss: 0.444472, acc: 77.34%] [G loss: 3.549328]\n",
      "epoch:2 step:2272 [D loss: 0.339201, acc: 90.62%] [G loss: 3.740556]\n",
      "epoch:2 step:2273 [D loss: 0.406564, acc: 85.16%] [G loss: 3.972981]\n",
      "epoch:2 step:2274 [D loss: 0.694699, acc: 67.97%] [G loss: 3.341172]\n",
      "epoch:2 step:2275 [D loss: 0.554447, acc: 73.44%] [G loss: 3.278890]\n",
      "epoch:2 step:2276 [D loss: 0.471818, acc: 78.12%] [G loss: 3.425801]\n",
      "epoch:2 step:2277 [D loss: 0.468940, acc: 77.34%] [G loss: 3.420806]\n",
      "epoch:2 step:2278 [D loss: 0.606799, acc: 62.50%] [G loss: 3.132004]\n",
      "epoch:2 step:2279 [D loss: 0.531731, acc: 71.88%] [G loss: 3.404427]\n",
      "epoch:2 step:2280 [D loss: 0.673942, acc: 66.41%] [G loss: 3.072173]\n",
      "epoch:2 step:2281 [D loss: 0.533691, acc: 71.88%] [G loss: 2.915990]\n",
      "epoch:2 step:2282 [D loss: 0.508241, acc: 75.00%] [G loss: 3.144499]\n",
      "epoch:2 step:2283 [D loss: 0.456633, acc: 79.69%] [G loss: 3.257302]\n",
      "epoch:2 step:2284 [D loss: 0.597623, acc: 68.75%] [G loss: 3.011647]\n",
      "epoch:2 step:2285 [D loss: 0.577874, acc: 71.88%] [G loss: 2.924764]\n",
      "epoch:2 step:2286 [D loss: 0.546941, acc: 76.56%] [G loss: 2.569194]\n",
      "epoch:2 step:2287 [D loss: 0.636319, acc: 63.28%] [G loss: 3.214893]\n",
      "epoch:2 step:2288 [D loss: 0.629126, acc: 69.53%] [G loss: 3.172871]\n",
      "epoch:2 step:2289 [D loss: 0.557669, acc: 69.53%] [G loss: 2.930939]\n",
      "epoch:2 step:2290 [D loss: 0.501633, acc: 76.56%] [G loss: 3.290563]\n",
      "epoch:2 step:2291 [D loss: 0.636233, acc: 66.41%] [G loss: 2.989841]\n",
      "epoch:2 step:2292 [D loss: 0.599509, acc: 67.19%] [G loss: 2.994184]\n",
      "epoch:2 step:2293 [D loss: 0.664846, acc: 60.94%] [G loss: 3.409702]\n",
      "epoch:2 step:2294 [D loss: 0.517923, acc: 69.53%] [G loss: 3.125916]\n",
      "epoch:2 step:2295 [D loss: 0.594614, acc: 72.66%] [G loss: 2.725262]\n",
      "epoch:2 step:2296 [D loss: 0.499594, acc: 75.00%] [G loss: 3.101018]\n",
      "epoch:2 step:2297 [D loss: 0.456073, acc: 82.03%] [G loss: 3.355367]\n",
      "epoch:2 step:2298 [D loss: 0.569288, acc: 71.88%] [G loss: 2.690129]\n",
      "epoch:2 step:2299 [D loss: 0.553803, acc: 76.56%] [G loss: 3.241839]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:2 step:2300 [D loss: 0.552975, acc: 67.97%] [G loss: 3.035257]\n",
      "epoch:2 step:2301 [D loss: 0.535719, acc: 70.31%] [G loss: 3.105049]\n",
      "epoch:2 step:2302 [D loss: 0.523339, acc: 72.66%] [G loss: 3.386919]\n",
      "epoch:2 step:2303 [D loss: 0.572976, acc: 73.44%] [G loss: 3.264356]\n",
      "epoch:2 step:2304 [D loss: 0.575996, acc: 71.09%] [G loss: 3.404643]\n",
      "epoch:2 step:2305 [D loss: 0.549533, acc: 75.00%] [G loss: 3.312408]\n",
      "epoch:2 step:2306 [D loss: 0.545057, acc: 75.00%] [G loss: 2.914646]\n",
      "epoch:2 step:2307 [D loss: 0.568612, acc: 74.22%] [G loss: 3.068131]\n",
      "epoch:2 step:2308 [D loss: 0.563080, acc: 71.09%] [G loss: 2.813959]\n",
      "epoch:2 step:2309 [D loss: 0.624902, acc: 65.62%] [G loss: 2.622744]\n",
      "epoch:2 step:2310 [D loss: 0.590032, acc: 71.88%] [G loss: 2.939720]\n",
      "epoch:2 step:2311 [D loss: 0.745973, acc: 61.72%] [G loss: 2.784507]\n",
      "epoch:2 step:2312 [D loss: 0.581776, acc: 68.75%] [G loss: 2.639860]\n",
      "epoch:2 step:2313 [D loss: 0.553531, acc: 71.88%] [G loss: 3.199471]\n",
      "epoch:2 step:2314 [D loss: 0.514512, acc: 70.31%] [G loss: 3.327785]\n",
      "epoch:2 step:2315 [D loss: 0.624274, acc: 64.06%] [G loss: 2.888420]\n",
      "epoch:2 step:2316 [D loss: 0.573855, acc: 71.88%] [G loss: 2.838890]\n",
      "epoch:2 step:2317 [D loss: 0.607185, acc: 68.75%] [G loss: 2.681331]\n",
      "epoch:2 step:2318 [D loss: 0.727380, acc: 63.28%] [G loss: 2.807418]\n",
      "epoch:2 step:2319 [D loss: 0.606320, acc: 66.41%] [G loss: 2.836205]\n",
      "epoch:2 step:2320 [D loss: 0.516370, acc: 73.44%] [G loss: 3.155968]\n",
      "epoch:2 step:2321 [D loss: 0.516014, acc: 76.56%] [G loss: 2.728376]\n",
      "epoch:2 step:2322 [D loss: 0.585222, acc: 68.75%] [G loss: 2.665495]\n",
      "epoch:2 step:2323 [D loss: 0.584100, acc: 68.75%] [G loss: 3.039009]\n",
      "epoch:2 step:2324 [D loss: 0.521264, acc: 72.66%] [G loss: 2.961977]\n",
      "epoch:2 step:2325 [D loss: 0.524690, acc: 72.66%] [G loss: 3.279597]\n",
      "epoch:2 step:2326 [D loss: 0.581998, acc: 72.66%] [G loss: 3.000074]\n",
      "epoch:2 step:2327 [D loss: 0.602855, acc: 68.75%] [G loss: 3.109692]\n",
      "epoch:2 step:2328 [D loss: 0.522941, acc: 73.44%] [G loss: 2.686732]\n",
      "epoch:2 step:2329 [D loss: 0.627865, acc: 67.19%] [G loss: 2.754961]\n",
      "epoch:2 step:2330 [D loss: 0.622687, acc: 67.97%] [G loss: 2.573598]\n",
      "epoch:2 step:2331 [D loss: 0.585899, acc: 65.62%] [G loss: 2.723452]\n",
      "epoch:2 step:2332 [D loss: 0.512775, acc: 75.00%] [G loss: 2.856751]\n",
      "epoch:2 step:2333 [D loss: 0.535541, acc: 73.44%] [G loss: 2.876031]\n",
      "epoch:2 step:2334 [D loss: 0.556424, acc: 69.53%] [G loss: 3.276958]\n",
      "epoch:2 step:2335 [D loss: 0.492609, acc: 78.91%] [G loss: 3.005590]\n",
      "epoch:2 step:2336 [D loss: 0.560452, acc: 71.88%] [G loss: 2.963658]\n",
      "epoch:2 step:2337 [D loss: 0.618919, acc: 70.31%] [G loss: 2.975121]\n",
      "epoch:2 step:2338 [D loss: 0.532551, acc: 75.78%] [G loss: 3.187645]\n",
      "epoch:2 step:2339 [D loss: 0.625856, acc: 67.97%] [G loss: 2.703997]\n",
      "epoch:2 step:2340 [D loss: 0.610080, acc: 68.75%] [G loss: 2.744904]\n",
      "epoch:2 step:2341 [D loss: 0.577924, acc: 70.31%] [G loss: 2.569574]\n",
      "epoch:2 step:2342 [D loss: 0.618961, acc: 66.41%] [G loss: 2.805968]\n",
      "epoch:2 step:2343 [D loss: 0.583295, acc: 62.50%] [G loss: 2.718122]\n",
      "epoch:2 step:2344 [D loss: 0.557303, acc: 75.78%] [G loss: 2.957328]\n",
      "epoch:2 step:2345 [D loss: 0.511445, acc: 76.56%] [G loss: 3.466997]\n",
      "epoch:2 step:2346 [D loss: 0.537172, acc: 74.22%] [G loss: 3.320994]\n",
      "epoch:2 step:2347 [D loss: 0.611347, acc: 67.97%] [G loss: 2.924742]\n",
      "epoch:2 step:2348 [D loss: 0.502907, acc: 73.44%] [G loss: 2.990935]\n",
      "epoch:2 step:2349 [D loss: 0.578470, acc: 70.31%] [G loss: 3.047738]\n",
      "epoch:2 step:2350 [D loss: 0.594685, acc: 65.62%] [G loss: 2.918528]\n",
      "epoch:2 step:2351 [D loss: 0.638473, acc: 64.84%] [G loss: 2.645192]\n",
      "epoch:2 step:2352 [D loss: 0.575971, acc: 71.88%] [G loss: 2.729385]\n",
      "epoch:2 step:2353 [D loss: 0.558699, acc: 74.22%] [G loss: 2.862807]\n",
      "epoch:2 step:2354 [D loss: 0.559858, acc: 67.19%] [G loss: 2.831390]\n",
      "epoch:2 step:2355 [D loss: 0.556455, acc: 69.53%] [G loss: 2.679322]\n",
      "epoch:2 step:2356 [D loss: 0.538271, acc: 75.78%] [G loss: 2.812943]\n",
      "epoch:2 step:2357 [D loss: 0.619422, acc: 67.97%] [G loss: 2.474942]\n",
      "epoch:2 step:2358 [D loss: 0.620382, acc: 64.06%] [G loss: 2.853063]\n",
      "epoch:2 step:2359 [D loss: 0.633830, acc: 67.97%] [G loss: 2.672098]\n",
      "epoch:2 step:2360 [D loss: 0.583330, acc: 62.50%] [G loss: 2.947209]\n",
      "epoch:2 step:2361 [D loss: 0.574232, acc: 71.88%] [G loss: 2.907001]\n",
      "epoch:2 step:2362 [D loss: 0.555606, acc: 71.09%] [G loss: 3.013394]\n",
      "epoch:2 step:2363 [D loss: 0.621156, acc: 66.41%] [G loss: 3.034294]\n",
      "epoch:2 step:2364 [D loss: 0.597949, acc: 65.62%] [G loss: 2.869137]\n",
      "epoch:2 step:2365 [D loss: 0.553276, acc: 75.00%] [G loss: 2.995749]\n",
      "epoch:2 step:2366 [D loss: 0.629688, acc: 68.75%] [G loss: 2.731296]\n",
      "epoch:2 step:2367 [D loss: 0.587155, acc: 65.62%] [G loss: 2.681216]\n",
      "epoch:2 step:2368 [D loss: 0.517925, acc: 79.69%] [G loss: 3.065834]\n",
      "epoch:2 step:2369 [D loss: 0.553636, acc: 71.09%] [G loss: 2.961193]\n",
      "epoch:2 step:2370 [D loss: 0.573749, acc: 71.09%] [G loss: 2.843731]\n",
      "epoch:2 step:2371 [D loss: 0.583995, acc: 67.19%] [G loss: 3.040794]\n",
      "epoch:2 step:2372 [D loss: 0.507494, acc: 71.88%] [G loss: 3.544147]\n",
      "epoch:2 step:2373 [D loss: 0.513895, acc: 76.56%] [G loss: 3.353345]\n",
      "epoch:2 step:2374 [D loss: 0.626911, acc: 65.62%] [G loss: 2.749620]\n",
      "epoch:2 step:2375 [D loss: 0.694209, acc: 60.94%] [G loss: 2.746863]\n",
      "epoch:2 step:2376 [D loss: 0.580845, acc: 66.41%] [G loss: 2.623414]\n",
      "epoch:2 step:2377 [D loss: 0.579701, acc: 71.09%] [G loss: 3.271725]\n",
      "epoch:2 step:2378 [D loss: 0.434716, acc: 85.16%] [G loss: 3.167334]\n",
      "epoch:2 step:2379 [D loss: 0.624443, acc: 66.41%] [G loss: 2.523853]\n",
      "epoch:2 step:2380 [D loss: 0.606736, acc: 67.19%] [G loss: 3.005315]\n",
      "epoch:2 step:2381 [D loss: 0.486277, acc: 71.88%] [G loss: 2.976035]\n",
      "epoch:2 step:2382 [D loss: 0.409125, acc: 86.72%] [G loss: 3.499927]\n",
      "epoch:2 step:2383 [D loss: 0.571475, acc: 68.75%] [G loss: 3.017884]\n",
      "epoch:2 step:2384 [D loss: 0.599020, acc: 71.88%] [G loss: 2.796090]\n",
      "epoch:2 step:2385 [D loss: 0.606046, acc: 60.94%] [G loss: 3.012118]\n",
      "epoch:2 step:2386 [D loss: 0.517478, acc: 74.22%] [G loss: 3.065645]\n",
      "epoch:2 step:2387 [D loss: 0.582039, acc: 71.88%] [G loss: 3.031791]\n",
      "epoch:2 step:2388 [D loss: 0.476546, acc: 83.59%] [G loss: 3.400546]\n",
      "epoch:2 step:2389 [D loss: 0.526950, acc: 71.88%] [G loss: 3.098693]\n",
      "epoch:2 step:2390 [D loss: 0.468187, acc: 78.91%] [G loss: 3.008518]\n",
      "epoch:2 step:2391 [D loss: 0.593312, acc: 65.62%] [G loss: 2.811100]\n",
      "epoch:2 step:2392 [D loss: 0.516247, acc: 75.00%] [G loss: 3.320298]\n",
      "epoch:2 step:2393 [D loss: 0.588648, acc: 70.31%] [G loss: 2.775410]\n",
      "epoch:2 step:2394 [D loss: 0.596453, acc: 71.09%] [G loss: 2.740738]\n",
      "epoch:2 step:2395 [D loss: 0.477417, acc: 78.12%] [G loss: 3.056775]\n",
      "epoch:2 step:2396 [D loss: 0.586136, acc: 66.41%] [G loss: 3.024787]\n",
      "epoch:2 step:2397 [D loss: 0.496392, acc: 78.91%] [G loss: 3.356182]\n",
      "epoch:2 step:2398 [D loss: 0.479697, acc: 81.25%] [G loss: 2.996846]\n",
      "epoch:2 step:2399 [D loss: 0.542511, acc: 74.22%] [G loss: 2.892519]\n",
      "epoch:2 step:2400 [D loss: 0.543451, acc: 71.88%] [G loss: 3.051878]\n",
      "##############\n",
      "[3.14912955 2.29477969 7.26962181 5.60702043 4.71076762 6.32149758\n",
      " 5.48282577 5.57022904 5.76264105 4.18233334]\n",
      "##########\n",
      "epoch:2 step:2401 [D loss: 0.556202, acc: 69.53%] [G loss: 2.855643]\n",
      "epoch:2 step:2402 [D loss: 0.636203, acc: 71.88%] [G loss: 2.777146]\n",
      "epoch:2 step:2403 [D loss: 0.631738, acc: 57.81%] [G loss: 2.979272]\n",
      "epoch:2 step:2404 [D loss: 0.519128, acc: 72.66%] [G loss: 2.995473]\n",
      "epoch:2 step:2405 [D loss: 0.568250, acc: 74.22%] [G loss: 2.919098]\n",
      "epoch:2 step:2406 [D loss: 0.521256, acc: 74.22%] [G loss: 2.821887]\n",
      "epoch:2 step:2407 [D loss: 0.628057, acc: 72.66%] [G loss: 3.023815]\n",
      "epoch:2 step:2408 [D loss: 0.591462, acc: 71.88%] [G loss: 2.933327]\n",
      "epoch:2 step:2409 [D loss: 0.608733, acc: 68.75%] [G loss: 2.787535]\n",
      "epoch:2 step:2410 [D loss: 0.629180, acc: 64.84%] [G loss: 3.105444]\n",
      "epoch:2 step:2411 [D loss: 0.554533, acc: 67.97%] [G loss: 2.817397]\n",
      "epoch:2 step:2412 [D loss: 0.554284, acc: 71.88%] [G loss: 2.576543]\n",
      "epoch:2 step:2413 [D loss: 0.572012, acc: 72.66%] [G loss: 2.972220]\n",
      "epoch:2 step:2414 [D loss: 0.554332, acc: 71.09%] [G loss: 2.618879]\n",
      "epoch:2 step:2415 [D loss: 0.613382, acc: 71.88%] [G loss: 3.078652]\n",
      "epoch:2 step:2416 [D loss: 0.585170, acc: 64.84%] [G loss: 3.229487]\n",
      "epoch:2 step:2417 [D loss: 0.607430, acc: 66.41%] [G loss: 2.900329]\n",
      "epoch:2 step:2418 [D loss: 0.532345, acc: 71.09%] [G loss: 3.177628]\n",
      "epoch:2 step:2419 [D loss: 0.620626, acc: 66.41%] [G loss: 3.081020]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:2 step:2420 [D loss: 0.448519, acc: 81.25%] [G loss: 3.390028]\n",
      "epoch:2 step:2421 [D loss: 0.533276, acc: 74.22%] [G loss: 2.987189]\n",
      "epoch:2 step:2422 [D loss: 0.556989, acc: 75.00%] [G loss: 3.196685]\n",
      "epoch:2 step:2423 [D loss: 0.635647, acc: 62.50%] [G loss: 2.847569]\n",
      "epoch:2 step:2424 [D loss: 0.650265, acc: 66.41%] [G loss: 2.618640]\n",
      "epoch:2 step:2425 [D loss: 0.541205, acc: 74.22%] [G loss: 2.672654]\n",
      "epoch:2 step:2426 [D loss: 0.532608, acc: 74.22%] [G loss: 2.812045]\n",
      "epoch:2 step:2427 [D loss: 0.589745, acc: 68.75%] [G loss: 3.290504]\n",
      "epoch:2 step:2428 [D loss: 0.496824, acc: 77.34%] [G loss: 3.220682]\n",
      "epoch:2 step:2429 [D loss: 0.562800, acc: 72.66%] [G loss: 3.216474]\n",
      "epoch:2 step:2430 [D loss: 0.516667, acc: 78.12%] [G loss: 3.086965]\n",
      "epoch:2 step:2431 [D loss: 0.534910, acc: 76.56%] [G loss: 3.268276]\n",
      "epoch:2 step:2432 [D loss: 0.474580, acc: 78.12%] [G loss: 3.248164]\n",
      "epoch:2 step:2433 [D loss: 0.678301, acc: 57.81%] [G loss: 2.788247]\n",
      "epoch:2 step:2434 [D loss: 0.580220, acc: 69.53%] [G loss: 2.920162]\n",
      "epoch:2 step:2435 [D loss: 0.554393, acc: 75.78%] [G loss: 2.883933]\n",
      "epoch:2 step:2436 [D loss: 0.660343, acc: 64.84%] [G loss: 2.718435]\n",
      "epoch:2 step:2437 [D loss: 0.551619, acc: 75.00%] [G loss: 2.743222]\n",
      "epoch:2 step:2438 [D loss: 0.626464, acc: 65.62%] [G loss: 2.918061]\n",
      "epoch:2 step:2439 [D loss: 0.561218, acc: 71.09%] [G loss: 2.801185]\n",
      "epoch:2 step:2440 [D loss: 0.643543, acc: 64.84%] [G loss: 2.874943]\n",
      "epoch:2 step:2441 [D loss: 0.509771, acc: 71.09%] [G loss: 3.123731]\n",
      "epoch:2 step:2442 [D loss: 0.493620, acc: 77.34%] [G loss: 3.165799]\n",
      "epoch:2 step:2443 [D loss: 0.563786, acc: 72.66%] [G loss: 3.187596]\n",
      "epoch:2 step:2444 [D loss: 0.608524, acc: 67.97%] [G loss: 3.067410]\n",
      "epoch:2 step:2445 [D loss: 0.530348, acc: 75.78%] [G loss: 3.082703]\n",
      "epoch:2 step:2446 [D loss: 0.549458, acc: 73.44%] [G loss: 2.883945]\n",
      "epoch:2 step:2447 [D loss: 0.455695, acc: 79.69%] [G loss: 2.852576]\n",
      "epoch:2 step:2448 [D loss: 0.587072, acc: 72.66%] [G loss: 2.904689]\n",
      "epoch:2 step:2449 [D loss: 0.437217, acc: 76.56%] [G loss: 3.618773]\n",
      "epoch:2 step:2450 [D loss: 0.669928, acc: 62.50%] [G loss: 2.803056]\n",
      "epoch:2 step:2451 [D loss: 0.488854, acc: 80.47%] [G loss: 2.888142]\n",
      "epoch:2 step:2452 [D loss: 0.583564, acc: 73.44%] [G loss: 3.337673]\n",
      "epoch:2 step:2453 [D loss: 0.572994, acc: 71.88%] [G loss: 3.292237]\n",
      "epoch:2 step:2454 [D loss: 0.630852, acc: 64.84%] [G loss: 2.784607]\n",
      "epoch:2 step:2455 [D loss: 0.508413, acc: 74.22%] [G loss: 3.043145]\n",
      "epoch:2 step:2456 [D loss: 0.484893, acc: 74.22%] [G loss: 3.329143]\n",
      "epoch:2 step:2457 [D loss: 0.536675, acc: 73.44%] [G loss: 2.907732]\n",
      "epoch:2 step:2458 [D loss: 0.568392, acc: 68.75%] [G loss: 3.091124]\n",
      "epoch:2 step:2459 [D loss: 0.654555, acc: 65.62%] [G loss: 3.232663]\n",
      "epoch:2 step:2460 [D loss: 0.556046, acc: 68.75%] [G loss: 2.964492]\n",
      "epoch:2 step:2461 [D loss: 0.570969, acc: 73.44%] [G loss: 3.002693]\n",
      "epoch:2 step:2462 [D loss: 0.611545, acc: 70.31%] [G loss: 3.031554]\n",
      "epoch:2 step:2463 [D loss: 0.535999, acc: 72.66%] [G loss: 3.026365]\n",
      "epoch:2 step:2464 [D loss: 0.636833, acc: 64.84%] [G loss: 2.732323]\n",
      "epoch:2 step:2465 [D loss: 0.549982, acc: 76.56%] [G loss: 2.624827]\n",
      "epoch:2 step:2466 [D loss: 0.551053, acc: 77.34%] [G loss: 2.912372]\n",
      "epoch:2 step:2467 [D loss: 0.510875, acc: 78.91%] [G loss: 2.818224]\n",
      "epoch:2 step:2468 [D loss: 0.552673, acc: 70.31%] [G loss: 2.794393]\n",
      "epoch:2 step:2469 [D loss: 0.650433, acc: 67.19%] [G loss: 2.941997]\n",
      "epoch:2 step:2470 [D loss: 0.617471, acc: 67.19%] [G loss: 2.905755]\n",
      "epoch:2 step:2471 [D loss: 0.506663, acc: 75.00%] [G loss: 3.177905]\n",
      "epoch:2 step:2472 [D loss: 0.565250, acc: 70.31%] [G loss: 3.006762]\n",
      "epoch:2 step:2473 [D loss: 0.575281, acc: 74.22%] [G loss: 2.969646]\n",
      "epoch:2 step:2474 [D loss: 0.662622, acc: 63.28%] [G loss: 2.647761]\n",
      "epoch:2 step:2475 [D loss: 0.545932, acc: 71.09%] [G loss: 2.913753]\n",
      "epoch:2 step:2476 [D loss: 0.502934, acc: 74.22%] [G loss: 2.847539]\n",
      "epoch:2 step:2477 [D loss: 0.454433, acc: 80.47%] [G loss: 3.208423]\n",
      "epoch:2 step:2478 [D loss: 0.559781, acc: 71.88%] [G loss: 3.231095]\n",
      "epoch:2 step:2479 [D loss: 0.528597, acc: 75.00%] [G loss: 3.281487]\n",
      "epoch:2 step:2480 [D loss: 0.547757, acc: 72.66%] [G loss: 3.441674]\n",
      "epoch:2 step:2481 [D loss: 0.538591, acc: 72.66%] [G loss: 3.185209]\n",
      "epoch:2 step:2482 [D loss: 0.530458, acc: 75.00%] [G loss: 2.960476]\n",
      "epoch:2 step:2483 [D loss: 0.467997, acc: 82.81%] [G loss: 3.121169]\n",
      "epoch:2 step:2484 [D loss: 0.622157, acc: 65.62%] [G loss: 3.218498]\n",
      "epoch:2 step:2485 [D loss: 0.442102, acc: 80.47%] [G loss: 3.392773]\n",
      "epoch:2 step:2486 [D loss: 0.579283, acc: 73.44%] [G loss: 3.271070]\n",
      "epoch:2 step:2487 [D loss: 0.495065, acc: 73.44%] [G loss: 3.392229]\n",
      "epoch:2 step:2488 [D loss: 0.591888, acc: 71.09%] [G loss: 3.016938]\n",
      "epoch:2 step:2489 [D loss: 0.577789, acc: 69.53%] [G loss: 2.690995]\n",
      "epoch:2 step:2490 [D loss: 0.519488, acc: 74.22%] [G loss: 3.354846]\n",
      "epoch:2 step:2491 [D loss: 0.581403, acc: 71.09%] [G loss: 2.599084]\n",
      "epoch:2 step:2492 [D loss: 0.606121, acc: 71.88%] [G loss: 3.076309]\n",
      "epoch:2 step:2493 [D loss: 0.467095, acc: 76.56%] [G loss: 3.178681]\n",
      "epoch:2 step:2494 [D loss: 0.511050, acc: 78.12%] [G loss: 2.851013]\n",
      "epoch:2 step:2495 [D loss: 0.573860, acc: 65.62%] [G loss: 2.839890]\n",
      "epoch:2 step:2496 [D loss: 0.724357, acc: 58.59%] [G loss: 2.577445]\n",
      "epoch:2 step:2497 [D loss: 0.602808, acc: 71.88%] [G loss: 2.567270]\n",
      "epoch:2 step:2498 [D loss: 0.579108, acc: 65.62%] [G loss: 2.853906]\n",
      "epoch:2 step:2499 [D loss: 0.581421, acc: 69.53%] [G loss: 2.896417]\n",
      "epoch:2 step:2500 [D loss: 0.500258, acc: 78.12%] [G loss: 2.845692]\n",
      "epoch:2 step:2501 [D loss: 0.566827, acc: 69.53%] [G loss: 2.895734]\n",
      "epoch:2 step:2502 [D loss: 0.545646, acc: 77.34%] [G loss: 2.996195]\n",
      "epoch:2 step:2503 [D loss: 0.545248, acc: 74.22%] [G loss: 2.985757]\n",
      "epoch:2 step:2504 [D loss: 0.476023, acc: 79.69%] [G loss: 3.028530]\n",
      "epoch:2 step:2505 [D loss: 0.567065, acc: 68.75%] [G loss: 3.079181]\n",
      "epoch:2 step:2506 [D loss: 0.530461, acc: 78.91%] [G loss: 3.097746]\n",
      "epoch:2 step:2507 [D loss: 0.456300, acc: 79.69%] [G loss: 2.954464]\n",
      "epoch:2 step:2508 [D loss: 0.597247, acc: 68.75%] [G loss: 3.265688]\n",
      "epoch:2 step:2509 [D loss: 0.512325, acc: 69.53%] [G loss: 3.219267]\n",
      "epoch:2 step:2510 [D loss: 0.595580, acc: 66.41%] [G loss: 2.979572]\n",
      "epoch:2 step:2511 [D loss: 0.557656, acc: 70.31%] [G loss: 3.011290]\n",
      "epoch:2 step:2512 [D loss: 0.563420, acc: 73.44%] [G loss: 2.991803]\n",
      "epoch:2 step:2513 [D loss: 0.503384, acc: 75.00%] [G loss: 3.080560]\n",
      "epoch:2 step:2514 [D loss: 0.478102, acc: 82.03%] [G loss: 3.086479]\n",
      "epoch:2 step:2515 [D loss: 0.477839, acc: 82.03%] [G loss: 3.492058]\n",
      "epoch:2 step:2516 [D loss: 0.528548, acc: 71.09%] [G loss: 3.179421]\n",
      "epoch:2 step:2517 [D loss: 0.556533, acc: 71.88%] [G loss: 2.748274]\n",
      "epoch:2 step:2518 [D loss: 0.551539, acc: 70.31%] [G loss: 2.988844]\n",
      "epoch:2 step:2519 [D loss: 0.618750, acc: 61.72%] [G loss: 2.949439]\n",
      "epoch:2 step:2520 [D loss: 0.512811, acc: 76.56%] [G loss: 3.164105]\n",
      "epoch:2 step:2521 [D loss: 0.660593, acc: 64.06%] [G loss: 2.969516]\n",
      "epoch:2 step:2522 [D loss: 0.507505, acc: 73.44%] [G loss: 3.222396]\n",
      "epoch:2 step:2523 [D loss: 0.594569, acc: 67.97%] [G loss: 3.338989]\n",
      "epoch:2 step:2524 [D loss: 0.509675, acc: 74.22%] [G loss: 3.303507]\n",
      "epoch:2 step:2525 [D loss: 0.552322, acc: 72.66%] [G loss: 2.966813]\n",
      "epoch:2 step:2526 [D loss: 0.641620, acc: 67.97%] [G loss: 2.590319]\n",
      "epoch:2 step:2527 [D loss: 0.596519, acc: 65.62%] [G loss: 3.016694]\n",
      "epoch:2 step:2528 [D loss: 0.509604, acc: 76.56%] [G loss: 3.202981]\n",
      "epoch:2 step:2529 [D loss: 0.696446, acc: 60.94%] [G loss: 3.025410]\n",
      "epoch:2 step:2530 [D loss: 0.637414, acc: 66.41%] [G loss: 2.945035]\n",
      "epoch:2 step:2531 [D loss: 0.559165, acc: 66.41%] [G loss: 2.913131]\n",
      "epoch:2 step:2532 [D loss: 0.574780, acc: 70.31%] [G loss: 2.808013]\n",
      "epoch:2 step:2533 [D loss: 0.561817, acc: 67.19%] [G loss: 3.076328]\n",
      "epoch:2 step:2534 [D loss: 0.581718, acc: 67.19%] [G loss: 3.042768]\n",
      "epoch:2 step:2535 [D loss: 0.462192, acc: 77.34%] [G loss: 3.040757]\n",
      "epoch:2 step:2536 [D loss: 0.630254, acc: 66.41%] [G loss: 2.964408]\n",
      "epoch:2 step:2537 [D loss: 0.634605, acc: 63.28%] [G loss: 3.338661]\n",
      "epoch:2 step:2538 [D loss: 0.668347, acc: 62.50%] [G loss: 3.026648]\n",
      "epoch:2 step:2539 [D loss: 0.618146, acc: 64.06%] [G loss: 2.789460]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:2 step:2540 [D loss: 0.553516, acc: 71.88%] [G loss: 2.954520]\n",
      "epoch:2 step:2541 [D loss: 0.603398, acc: 68.75%] [G loss: 2.798361]\n",
      "epoch:2 step:2542 [D loss: 0.595834, acc: 69.53%] [G loss: 2.771474]\n",
      "epoch:2 step:2543 [D loss: 0.538603, acc: 78.91%] [G loss: 2.685681]\n",
      "epoch:2 step:2544 [D loss: 0.609653, acc: 67.97%] [G loss: 2.776816]\n",
      "epoch:2 step:2545 [D loss: 0.546186, acc: 70.31%] [G loss: 2.770306]\n",
      "epoch:2 step:2546 [D loss: 0.601133, acc: 69.53%] [G loss: 2.948087]\n",
      "epoch:2 step:2547 [D loss: 0.595379, acc: 67.19%] [G loss: 2.736230]\n",
      "epoch:2 step:2548 [D loss: 0.629301, acc: 71.88%] [G loss: 2.698895]\n",
      "epoch:2 step:2549 [D loss: 0.570712, acc: 69.53%] [G loss: 3.181666]\n",
      "epoch:2 step:2550 [D loss: 0.552826, acc: 71.09%] [G loss: 2.834457]\n",
      "epoch:2 step:2551 [D loss: 0.504147, acc: 75.00%] [G loss: 3.120653]\n",
      "epoch:2 step:2552 [D loss: 0.518625, acc: 74.22%] [G loss: 3.064497]\n",
      "epoch:2 step:2553 [D loss: 0.555379, acc: 69.53%] [G loss: 2.846324]\n",
      "epoch:2 step:2554 [D loss: 0.552157, acc: 67.97%] [G loss: 3.253076]\n",
      "epoch:2 step:2555 [D loss: 0.524260, acc: 75.00%] [G loss: 3.165626]\n",
      "epoch:2 step:2556 [D loss: 0.572302, acc: 73.44%] [G loss: 2.987933]\n",
      "epoch:2 step:2557 [D loss: 0.634961, acc: 62.50%] [G loss: 3.225059]\n",
      "epoch:2 step:2558 [D loss: 0.553487, acc: 71.09%] [G loss: 2.705883]\n",
      "epoch:2 step:2559 [D loss: 0.545337, acc: 68.75%] [G loss: 2.985076]\n",
      "epoch:2 step:2560 [D loss: 0.523648, acc: 71.88%] [G loss: 2.989562]\n",
      "epoch:2 step:2561 [D loss: 0.624191, acc: 61.72%] [G loss: 3.293436]\n",
      "epoch:2 step:2562 [D loss: 0.658810, acc: 62.50%] [G loss: 2.684543]\n",
      "epoch:2 step:2563 [D loss: 0.561868, acc: 70.31%] [G loss: 2.897280]\n",
      "epoch:2 step:2564 [D loss: 0.539256, acc: 71.88%] [G loss: 2.992873]\n",
      "epoch:2 step:2565 [D loss: 0.532024, acc: 73.44%] [G loss: 3.097059]\n",
      "epoch:2 step:2566 [D loss: 0.542912, acc: 71.88%] [G loss: 2.791654]\n",
      "epoch:2 step:2567 [D loss: 0.650940, acc: 65.62%] [G loss: 2.842003]\n",
      "epoch:2 step:2568 [D loss: 0.477967, acc: 75.78%] [G loss: 2.873011]\n",
      "epoch:2 step:2569 [D loss: 0.575605, acc: 74.22%] [G loss: 3.075017]\n",
      "epoch:2 step:2570 [D loss: 0.659355, acc: 66.41%] [G loss: 2.868863]\n",
      "epoch:2 step:2571 [D loss: 0.537279, acc: 71.88%] [G loss: 3.045784]\n",
      "epoch:2 step:2572 [D loss: 0.553959, acc: 67.97%] [G loss: 3.071775]\n",
      "epoch:2 step:2573 [D loss: 0.590029, acc: 67.19%] [G loss: 3.090703]\n",
      "epoch:2 step:2574 [D loss: 0.515931, acc: 72.66%] [G loss: 2.868598]\n",
      "epoch:2 step:2575 [D loss: 0.526726, acc: 76.56%] [G loss: 3.039017]\n",
      "epoch:2 step:2576 [D loss: 0.569473, acc: 66.41%] [G loss: 2.644500]\n",
      "epoch:2 step:2577 [D loss: 0.560068, acc: 70.31%] [G loss: 3.035174]\n",
      "epoch:2 step:2578 [D loss: 0.544091, acc: 71.09%] [G loss: 3.015808]\n",
      "epoch:2 step:2579 [D loss: 0.607620, acc: 72.66%] [G loss: 3.087824]\n",
      "epoch:2 step:2580 [D loss: 0.488735, acc: 78.91%] [G loss: 2.966486]\n",
      "epoch:2 step:2581 [D loss: 0.441562, acc: 78.91%] [G loss: 3.406849]\n",
      "epoch:2 step:2582 [D loss: 0.503511, acc: 76.56%] [G loss: 3.226820]\n",
      "epoch:2 step:2583 [D loss: 0.558703, acc: 69.53%] [G loss: 3.457608]\n",
      "epoch:2 step:2584 [D loss: 0.725891, acc: 60.16%] [G loss: 2.693686]\n",
      "epoch:2 step:2585 [D loss: 0.656480, acc: 67.19%] [G loss: 2.842981]\n",
      "epoch:2 step:2586 [D loss: 0.562896, acc: 67.97%] [G loss: 3.077662]\n",
      "epoch:2 step:2587 [D loss: 0.594758, acc: 69.53%] [G loss: 3.211646]\n",
      "epoch:2 step:2588 [D loss: 0.534564, acc: 74.22%] [G loss: 3.357608]\n",
      "epoch:2 step:2589 [D loss: 0.630681, acc: 62.50%] [G loss: 3.138012]\n",
      "epoch:2 step:2590 [D loss: 0.562675, acc: 76.56%] [G loss: 2.932844]\n",
      "epoch:2 step:2591 [D loss: 0.606016, acc: 67.97%] [G loss: 2.678094]\n",
      "epoch:2 step:2592 [D loss: 0.527262, acc: 70.31%] [G loss: 3.033599]\n",
      "epoch:2 step:2593 [D loss: 0.607674, acc: 71.88%] [G loss: 2.678849]\n",
      "epoch:2 step:2594 [D loss: 0.651414, acc: 67.19%] [G loss: 2.529515]\n",
      "epoch:2 step:2595 [D loss: 0.503713, acc: 77.34%] [G loss: 3.144904]\n",
      "epoch:2 step:2596 [D loss: 0.655141, acc: 67.19%] [G loss: 3.107653]\n",
      "epoch:2 step:2597 [D loss: 0.535763, acc: 74.22%] [G loss: 2.886790]\n",
      "epoch:2 step:2598 [D loss: 0.570012, acc: 64.84%] [G loss: 2.889560]\n",
      "epoch:2 step:2599 [D loss: 0.650537, acc: 60.94%] [G loss: 3.083173]\n",
      "epoch:2 step:2600 [D loss: 0.628265, acc: 63.28%] [G loss: 2.672742]\n",
      "##############\n",
      "[3.13028981 1.84878695 7.33865111 5.52263886 4.48978446 6.17273891\n",
      " 5.52706439 5.52865757 5.69332615 3.8889044 ]\n",
      "##########\n",
      "epoch:2 step:2601 [D loss: 0.536313, acc: 71.09%] [G loss: 2.570575]\n",
      "epoch:2 step:2602 [D loss: 0.569022, acc: 70.31%] [G loss: 3.087795]\n",
      "epoch:2 step:2603 [D loss: 0.541534, acc: 67.19%] [G loss: 2.876161]\n",
      "epoch:2 step:2604 [D loss: 0.572653, acc: 67.97%] [G loss: 2.827109]\n",
      "epoch:2 step:2605 [D loss: 0.570273, acc: 71.88%] [G loss: 2.852589]\n",
      "epoch:2 step:2606 [D loss: 0.498034, acc: 77.34%] [G loss: 3.437680]\n",
      "epoch:2 step:2607 [D loss: 0.397742, acc: 83.59%] [G loss: 3.273137]\n",
      "epoch:2 step:2608 [D loss: 0.544965, acc: 75.00%] [G loss: 3.261594]\n",
      "epoch:2 step:2609 [D loss: 0.599473, acc: 67.19%] [G loss: 2.861745]\n",
      "epoch:2 step:2610 [D loss: 0.415551, acc: 82.03%] [G loss: 3.195860]\n",
      "epoch:2 step:2611 [D loss: 0.589954, acc: 66.41%] [G loss: 2.735351]\n",
      "epoch:2 step:2612 [D loss: 0.573136, acc: 71.88%] [G loss: 2.861535]\n",
      "epoch:2 step:2613 [D loss: 0.571000, acc: 69.53%] [G loss: 2.813403]\n",
      "epoch:2 step:2614 [D loss: 0.546792, acc: 72.66%] [G loss: 2.999209]\n",
      "epoch:2 step:2615 [D loss: 0.493987, acc: 76.56%] [G loss: 3.475394]\n",
      "epoch:2 step:2616 [D loss: 0.622817, acc: 70.31%] [G loss: 2.860682]\n",
      "epoch:2 step:2617 [D loss: 0.505139, acc: 73.44%] [G loss: 2.840711]\n",
      "epoch:2 step:2618 [D loss: 0.580586, acc: 72.66%] [G loss: 2.967994]\n",
      "epoch:2 step:2619 [D loss: 0.650897, acc: 64.84%] [G loss: 3.170592]\n",
      "epoch:2 step:2620 [D loss: 0.596925, acc: 70.31%] [G loss: 2.961658]\n",
      "epoch:2 step:2621 [D loss: 0.440346, acc: 82.81%] [G loss: 3.686432]\n",
      "epoch:2 step:2622 [D loss: 0.515178, acc: 71.09%] [G loss: 3.070866]\n",
      "epoch:2 step:2623 [D loss: 0.588915, acc: 71.88%] [G loss: 2.858197]\n",
      "epoch:2 step:2624 [D loss: 0.640299, acc: 67.19%] [G loss: 3.062754]\n",
      "epoch:2 step:2625 [D loss: 0.530417, acc: 74.22%] [G loss: 3.092273]\n",
      "epoch:2 step:2626 [D loss: 0.434395, acc: 85.16%] [G loss: 3.335096]\n",
      "epoch:2 step:2627 [D loss: 0.579120, acc: 70.31%] [G loss: 3.347835]\n",
      "epoch:2 step:2628 [D loss: 0.420007, acc: 82.81%] [G loss: 3.462034]\n",
      "epoch:2 step:2629 [D loss: 0.532878, acc: 76.56%] [G loss: 2.988122]\n",
      "epoch:2 step:2630 [D loss: 0.529792, acc: 77.34%] [G loss: 2.786008]\n",
      "epoch:2 step:2631 [D loss: 0.505421, acc: 72.66%] [G loss: 3.392527]\n",
      "epoch:2 step:2632 [D loss: 0.570390, acc: 71.09%] [G loss: 3.080415]\n",
      "epoch:2 step:2633 [D loss: 0.496174, acc: 77.34%] [G loss: 3.242856]\n",
      "epoch:2 step:2634 [D loss: 0.537889, acc: 71.88%] [G loss: 3.373815]\n",
      "epoch:2 step:2635 [D loss: 0.518770, acc: 72.66%] [G loss: 3.477505]\n",
      "epoch:2 step:2636 [D loss: 0.595085, acc: 73.44%] [G loss: 3.328731]\n",
      "epoch:2 step:2637 [D loss: 0.580788, acc: 69.53%] [G loss: 3.096683]\n",
      "epoch:2 step:2638 [D loss: 0.475781, acc: 75.00%] [G loss: 3.310722]\n",
      "epoch:2 step:2639 [D loss: 0.770724, acc: 53.91%] [G loss: 2.751336]\n",
      "epoch:2 step:2640 [D loss: 0.561448, acc: 67.19%] [G loss: 2.642355]\n",
      "epoch:2 step:2641 [D loss: 0.586193, acc: 67.19%] [G loss: 2.680653]\n",
      "epoch:2 step:2642 [D loss: 0.507926, acc: 75.78%] [G loss: 2.775499]\n",
      "epoch:2 step:2643 [D loss: 0.543545, acc: 72.66%] [G loss: 2.874748]\n",
      "epoch:2 step:2644 [D loss: 0.456326, acc: 82.03%] [G loss: 3.131913]\n",
      "epoch:2 step:2645 [D loss: 0.551857, acc: 75.00%] [G loss: 3.009578]\n",
      "epoch:2 step:2646 [D loss: 0.465195, acc: 80.47%] [G loss: 3.180712]\n",
      "epoch:2 step:2647 [D loss: 0.548793, acc: 74.22%] [G loss: 3.203437]\n",
      "epoch:2 step:2648 [D loss: 0.548797, acc: 69.53%] [G loss: 3.261888]\n",
      "epoch:2 step:2649 [D loss: 0.533352, acc: 75.00%] [G loss: 3.376238]\n",
      "epoch:2 step:2650 [D loss: 0.594335, acc: 67.97%] [G loss: 2.896706]\n",
      "epoch:2 step:2651 [D loss: 0.534284, acc: 71.09%] [G loss: 2.770795]\n",
      "epoch:2 step:2652 [D loss: 0.569404, acc: 73.44%] [G loss: 2.813399]\n",
      "epoch:2 step:2653 [D loss: 0.553304, acc: 72.66%] [G loss: 3.343026]\n",
      "epoch:2 step:2654 [D loss: 0.508068, acc: 74.22%] [G loss: 3.020598]\n",
      "epoch:2 step:2655 [D loss: 0.495556, acc: 75.78%] [G loss: 3.318054]\n",
      "epoch:2 step:2656 [D loss: 0.503696, acc: 78.12%] [G loss: 2.891006]\n",
      "epoch:2 step:2657 [D loss: 0.533858, acc: 76.56%] [G loss: 3.380121]\n",
      "epoch:2 step:2658 [D loss: 0.511066, acc: 75.00%] [G loss: 3.223181]\n",
      "epoch:2 step:2659 [D loss: 0.622185, acc: 66.41%] [G loss: 3.118921]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:2 step:2660 [D loss: 0.551030, acc: 75.78%] [G loss: 3.417874]\n",
      "epoch:2 step:2661 [D loss: 0.568099, acc: 69.53%] [G loss: 3.191149]\n",
      "epoch:2 step:2662 [D loss: 0.630445, acc: 69.53%] [G loss: 2.597960]\n",
      "epoch:2 step:2663 [D loss: 0.575910, acc: 66.41%] [G loss: 2.950830]\n",
      "epoch:2 step:2664 [D loss: 0.524999, acc: 75.00%] [G loss: 3.302146]\n",
      "epoch:2 step:2665 [D loss: 0.614183, acc: 64.84%] [G loss: 3.727708]\n",
      "epoch:2 step:2666 [D loss: 0.454082, acc: 82.81%] [G loss: 3.821401]\n",
      "epoch:2 step:2667 [D loss: 0.519366, acc: 74.22%] [G loss: 3.129436]\n",
      "epoch:2 step:2668 [D loss: 0.580000, acc: 67.19%] [G loss: 2.963891]\n",
      "epoch:2 step:2669 [D loss: 0.519878, acc: 73.44%] [G loss: 3.135067]\n",
      "epoch:2 step:2670 [D loss: 0.549997, acc: 74.22%] [G loss: 2.959967]\n",
      "epoch:2 step:2671 [D loss: 0.513776, acc: 71.09%] [G loss: 2.991150]\n",
      "epoch:2 step:2672 [D loss: 0.500259, acc: 73.44%] [G loss: 3.318870]\n",
      "epoch:2 step:2673 [D loss: 0.498599, acc: 79.69%] [G loss: 3.207060]\n",
      "epoch:2 step:2674 [D loss: 0.542740, acc: 74.22%] [G loss: 2.926128]\n",
      "epoch:2 step:2675 [D loss: 0.621777, acc: 71.09%] [G loss: 2.870497]\n",
      "epoch:2 step:2676 [D loss: 0.473749, acc: 77.34%] [G loss: 2.834276]\n",
      "epoch:2 step:2677 [D loss: 0.546143, acc: 75.00%] [G loss: 3.310291]\n",
      "epoch:2 step:2678 [D loss: 0.592773, acc: 71.09%] [G loss: 3.595921]\n",
      "epoch:2 step:2679 [D loss: 0.489970, acc: 78.91%] [G loss: 3.311100]\n",
      "epoch:2 step:2680 [D loss: 0.541226, acc: 77.34%] [G loss: 3.729281]\n",
      "epoch:2 step:2681 [D loss: 0.589558, acc: 73.44%] [G loss: 3.074300]\n",
      "epoch:2 step:2682 [D loss: 0.651569, acc: 64.06%] [G loss: 2.945442]\n",
      "epoch:2 step:2683 [D loss: 0.599524, acc: 67.19%] [G loss: 3.141064]\n",
      "epoch:2 step:2684 [D loss: 0.583294, acc: 75.00%] [G loss: 2.747393]\n",
      "epoch:2 step:2685 [D loss: 0.623351, acc: 62.50%] [G loss: 2.897330]\n",
      "epoch:2 step:2686 [D loss: 0.608523, acc: 64.84%] [G loss: 2.868229]\n",
      "epoch:2 step:2687 [D loss: 0.563170, acc: 72.66%] [G loss: 3.102988]\n",
      "epoch:2 step:2688 [D loss: 0.541832, acc: 70.31%] [G loss: 2.978105]\n",
      "epoch:2 step:2689 [D loss: 0.689470, acc: 59.38%] [G loss: 2.751013]\n",
      "epoch:2 step:2690 [D loss: 0.517643, acc: 79.69%] [G loss: 2.984721]\n",
      "epoch:2 step:2691 [D loss: 0.604439, acc: 62.50%] [G loss: 2.879725]\n",
      "epoch:2 step:2692 [D loss: 0.649640, acc: 67.19%] [G loss: 3.077153]\n",
      "epoch:2 step:2693 [D loss: 0.456447, acc: 79.69%] [G loss: 3.155115]\n",
      "epoch:2 step:2694 [D loss: 0.589189, acc: 67.19%] [G loss: 3.196303]\n",
      "epoch:2 step:2695 [D loss: 0.515571, acc: 78.12%] [G loss: 2.967196]\n",
      "epoch:2 step:2696 [D loss: 0.493539, acc: 73.44%] [G loss: 3.398010]\n",
      "epoch:2 step:2697 [D loss: 0.498714, acc: 78.91%] [G loss: 3.240789]\n",
      "epoch:2 step:2698 [D loss: 0.634370, acc: 62.50%] [G loss: 2.794107]\n",
      "epoch:2 step:2699 [D loss: 0.501080, acc: 73.44%] [G loss: 2.866799]\n",
      "epoch:2 step:2700 [D loss: 0.663611, acc: 60.16%] [G loss: 2.700544]\n",
      "epoch:2 step:2701 [D loss: 0.661052, acc: 66.41%] [G loss: 2.496665]\n",
      "epoch:2 step:2702 [D loss: 0.580108, acc: 71.88%] [G loss: 2.526639]\n",
      "epoch:2 step:2703 [D loss: 0.528971, acc: 69.53%] [G loss: 3.005392]\n",
      "epoch:2 step:2704 [D loss: 0.508728, acc: 73.44%] [G loss: 2.853797]\n",
      "epoch:2 step:2705 [D loss: 0.515190, acc: 78.91%] [G loss: 3.251137]\n",
      "epoch:2 step:2706 [D loss: 0.482033, acc: 77.34%] [G loss: 3.131530]\n",
      "epoch:2 step:2707 [D loss: 0.568624, acc: 73.44%] [G loss: 3.095437]\n",
      "epoch:2 step:2708 [D loss: 0.464582, acc: 78.91%] [G loss: 3.328083]\n",
      "epoch:2 step:2709 [D loss: 0.549248, acc: 71.09%] [G loss: 3.124749]\n",
      "epoch:2 step:2710 [D loss: 0.539989, acc: 70.31%] [G loss: 3.289021]\n",
      "epoch:2 step:2711 [D loss: 0.476259, acc: 79.69%] [G loss: 3.059103]\n",
      "epoch:2 step:2712 [D loss: 0.510319, acc: 74.22%] [G loss: 3.290843]\n",
      "epoch:2 step:2713 [D loss: 0.545238, acc: 74.22%] [G loss: 2.813171]\n",
      "epoch:2 step:2714 [D loss: 0.583373, acc: 64.84%] [G loss: 2.747693]\n",
      "epoch:2 step:2715 [D loss: 0.551601, acc: 71.09%] [G loss: 3.317657]\n",
      "epoch:2 step:2716 [D loss: 0.558153, acc: 74.22%] [G loss: 3.169761]\n",
      "epoch:2 step:2717 [D loss: 0.655695, acc: 60.16%] [G loss: 3.270908]\n",
      "epoch:2 step:2718 [D loss: 0.553638, acc: 68.75%] [G loss: 2.772652]\n",
      "epoch:2 step:2719 [D loss: 0.513038, acc: 75.00%] [G loss: 3.019972]\n",
      "epoch:2 step:2720 [D loss: 0.536696, acc: 70.31%] [G loss: 2.694318]\n",
      "epoch:2 step:2721 [D loss: 0.430555, acc: 82.03%] [G loss: 3.310976]\n",
      "epoch:2 step:2722 [D loss: 0.456515, acc: 79.69%] [G loss: 2.945166]\n",
      "epoch:2 step:2723 [D loss: 0.762663, acc: 58.59%] [G loss: 2.645391]\n",
      "epoch:2 step:2724 [D loss: 0.620321, acc: 70.31%] [G loss: 2.887008]\n",
      "epoch:2 step:2725 [D loss: 0.591320, acc: 67.97%] [G loss: 2.864073]\n",
      "epoch:2 step:2726 [D loss: 0.598349, acc: 66.41%] [G loss: 2.948230]\n",
      "epoch:2 step:2727 [D loss: 0.573841, acc: 72.66%] [G loss: 3.122045]\n",
      "epoch:2 step:2728 [D loss: 0.429880, acc: 79.69%] [G loss: 3.092872]\n",
      "epoch:2 step:2729 [D loss: 0.528356, acc: 74.22%] [G loss: 3.155484]\n",
      "epoch:2 step:2730 [D loss: 0.557011, acc: 71.88%] [G loss: 2.849936]\n",
      "epoch:2 step:2731 [D loss: 0.483714, acc: 75.78%] [G loss: 2.856159]\n",
      "epoch:2 step:2732 [D loss: 0.679624, acc: 64.84%] [G loss: 2.848656]\n",
      "epoch:2 step:2733 [D loss: 0.707568, acc: 55.47%] [G loss: 2.644520]\n",
      "epoch:2 step:2734 [D loss: 0.484692, acc: 75.00%] [G loss: 3.082018]\n",
      "epoch:2 step:2735 [D loss: 0.573862, acc: 66.41%] [G loss: 2.872297]\n",
      "epoch:2 step:2736 [D loss: 0.555803, acc: 70.31%] [G loss: 3.155109]\n",
      "epoch:2 step:2737 [D loss: 0.621010, acc: 67.97%] [G loss: 3.167554]\n",
      "epoch:2 step:2738 [D loss: 0.632082, acc: 67.19%] [G loss: 3.029943]\n",
      "epoch:2 step:2739 [D loss: 0.549079, acc: 75.78%] [G loss: 2.802520]\n",
      "epoch:2 step:2740 [D loss: 0.507789, acc: 75.00%] [G loss: 3.105230]\n",
      "epoch:2 step:2741 [D loss: 0.621300, acc: 71.09%] [G loss: 2.906325]\n",
      "epoch:2 step:2742 [D loss: 0.561010, acc: 71.88%] [G loss: 2.890405]\n",
      "epoch:2 step:2743 [D loss: 0.559448, acc: 70.31%] [G loss: 3.105929]\n",
      "epoch:2 step:2744 [D loss: 0.613211, acc: 64.84%] [G loss: 2.823951]\n",
      "epoch:2 step:2745 [D loss: 0.584310, acc: 70.31%] [G loss: 2.820619]\n",
      "epoch:2 step:2746 [D loss: 0.587618, acc: 72.66%] [G loss: 3.048083]\n",
      "epoch:2 step:2747 [D loss: 0.458327, acc: 76.56%] [G loss: 3.170799]\n",
      "epoch:2 step:2748 [D loss: 0.513788, acc: 75.00%] [G loss: 3.170759]\n",
      "epoch:2 step:2749 [D loss: 0.528846, acc: 75.78%] [G loss: 2.986586]\n",
      "epoch:2 step:2750 [D loss: 0.514907, acc: 75.00%] [G loss: 2.968623]\n",
      "epoch:2 step:2751 [D loss: 0.448666, acc: 84.38%] [G loss: 3.110756]\n",
      "epoch:2 step:2752 [D loss: 0.551451, acc: 73.44%] [G loss: 2.684864]\n",
      "epoch:2 step:2753 [D loss: 0.623327, acc: 69.53%] [G loss: 2.815075]\n",
      "epoch:2 step:2754 [D loss: 0.653535, acc: 60.94%] [G loss: 2.642932]\n",
      "epoch:2 step:2755 [D loss: 0.655488, acc: 69.53%] [G loss: 2.502014]\n",
      "epoch:2 step:2756 [D loss: 0.625802, acc: 65.62%] [G loss: 2.813720]\n",
      "epoch:2 step:2757 [D loss: 0.604128, acc: 66.41%] [G loss: 2.844473]\n",
      "epoch:2 step:2758 [D loss: 0.436422, acc: 79.69%] [G loss: 3.325209]\n",
      "epoch:2 step:2759 [D loss: 0.512483, acc: 73.44%] [G loss: 3.339277]\n",
      "epoch:2 step:2760 [D loss: 0.545620, acc: 71.88%] [G loss: 3.662004]\n",
      "epoch:2 step:2761 [D loss: 0.470693, acc: 77.34%] [G loss: 3.471743]\n",
      "epoch:2 step:2762 [D loss: 0.500839, acc: 74.22%] [G loss: 3.651456]\n",
      "epoch:2 step:2763 [D loss: 0.523752, acc: 70.31%] [G loss: 3.581159]\n",
      "epoch:2 step:2764 [D loss: 0.497858, acc: 76.56%] [G loss: 3.589630]\n",
      "epoch:2 step:2765 [D loss: 0.602188, acc: 66.41%] [G loss: 3.072681]\n",
      "epoch:2 step:2766 [D loss: 0.650463, acc: 67.19%] [G loss: 2.702955]\n",
      "epoch:2 step:2767 [D loss: 0.655001, acc: 67.19%] [G loss: 3.132396]\n",
      "epoch:2 step:2768 [D loss: 0.603051, acc: 66.41%] [G loss: 3.202793]\n",
      "epoch:2 step:2769 [D loss: 0.636947, acc: 66.41%] [G loss: 3.023336]\n",
      "epoch:2 step:2770 [D loss: 0.654005, acc: 61.72%] [G loss: 2.572539]\n",
      "epoch:2 step:2771 [D loss: 0.525479, acc: 75.78%] [G loss: 3.219911]\n",
      "epoch:2 step:2772 [D loss: 0.538541, acc: 71.88%] [G loss: 3.401838]\n",
      "epoch:2 step:2773 [D loss: 0.528650, acc: 73.44%] [G loss: 2.805772]\n",
      "epoch:2 step:2774 [D loss: 0.545474, acc: 68.75%] [G loss: 2.808411]\n",
      "epoch:2 step:2775 [D loss: 0.533497, acc: 73.44%] [G loss: 2.764702]\n",
      "epoch:2 step:2776 [D loss: 0.640530, acc: 64.84%] [G loss: 2.882625]\n",
      "epoch:2 step:2777 [D loss: 0.612918, acc: 67.97%] [G loss: 3.351322]\n",
      "epoch:2 step:2778 [D loss: 0.504065, acc: 71.88%] [G loss: 3.498219]\n",
      "epoch:2 step:2779 [D loss: 0.599747, acc: 71.09%] [G loss: 3.166015]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:2 step:2780 [D loss: 0.456008, acc: 78.12%] [G loss: 3.324142]\n",
      "epoch:2 step:2781 [D loss: 0.516903, acc: 75.00%] [G loss: 3.185842]\n",
      "epoch:2 step:2782 [D loss: 0.615861, acc: 70.31%] [G loss: 3.163228]\n",
      "epoch:2 step:2783 [D loss: 0.570416, acc: 74.22%] [G loss: 2.778353]\n",
      "epoch:2 step:2784 [D loss: 0.553433, acc: 73.44%] [G loss: 3.398358]\n",
      "epoch:2 step:2785 [D loss: 0.488555, acc: 75.78%] [G loss: 3.282730]\n",
      "epoch:2 step:2786 [D loss: 0.478613, acc: 77.34%] [G loss: 3.101922]\n",
      "epoch:2 step:2787 [D loss: 0.546077, acc: 71.09%] [G loss: 3.208357]\n",
      "epoch:2 step:2788 [D loss: 0.471605, acc: 76.56%] [G loss: 3.173763]\n",
      "epoch:2 step:2789 [D loss: 0.509395, acc: 76.56%] [G loss: 3.368427]\n",
      "epoch:2 step:2790 [D loss: 0.591192, acc: 64.84%] [G loss: 3.386493]\n",
      "epoch:2 step:2791 [D loss: 0.567069, acc: 71.88%] [G loss: 2.903877]\n",
      "epoch:2 step:2792 [D loss: 0.508350, acc: 75.00%] [G loss: 3.379570]\n",
      "epoch:2 step:2793 [D loss: 0.518633, acc: 72.66%] [G loss: 3.135329]\n",
      "epoch:2 step:2794 [D loss: 0.677731, acc: 64.84%] [G loss: 3.668067]\n",
      "epoch:2 step:2795 [D loss: 0.450912, acc: 76.56%] [G loss: 3.619753]\n",
      "epoch:2 step:2796 [D loss: 0.576620, acc: 72.66%] [G loss: 3.034236]\n",
      "epoch:2 step:2797 [D loss: 0.507124, acc: 74.22%] [G loss: 3.612887]\n",
      "epoch:2 step:2798 [D loss: 0.485628, acc: 78.12%] [G loss: 3.785557]\n",
      "epoch:2 step:2799 [D loss: 0.444564, acc: 78.91%] [G loss: 4.413391]\n",
      "epoch:2 step:2800 [D loss: 0.495586, acc: 78.12%] [G loss: 3.864637]\n",
      "##############\n",
      "[3.14993947 2.27460443 7.17119327 5.61402272 4.48692028 6.28110097\n",
      " 5.49018476 5.58255824 5.78623118 3.98317669]\n",
      "##########\n",
      "epoch:2 step:2801 [D loss: 0.523011, acc: 74.22%] [G loss: 3.580919]\n",
      "epoch:2 step:2802 [D loss: 0.674150, acc: 70.31%] [G loss: 3.307545]\n",
      "epoch:2 step:2803 [D loss: 0.500540, acc: 75.00%] [G loss: 3.769894]\n",
      "epoch:2 step:2804 [D loss: 0.505948, acc: 74.22%] [G loss: 3.871255]\n",
      "epoch:2 step:2805 [D loss: 0.482186, acc: 78.12%] [G loss: 3.767467]\n",
      "epoch:2 step:2806 [D loss: 0.623874, acc: 74.22%] [G loss: 2.917915]\n",
      "epoch:2 step:2807 [D loss: 0.535801, acc: 75.78%] [G loss: 3.202908]\n",
      "epoch:2 step:2808 [D loss: 0.501015, acc: 72.66%] [G loss: 3.279186]\n",
      "epoch:2 step:2809 [D loss: 0.525504, acc: 75.78%] [G loss: 3.302865]\n",
      "epoch:2 step:2810 [D loss: 0.430540, acc: 82.03%] [G loss: 3.730115]\n",
      "epoch:2 step:2811 [D loss: 0.523395, acc: 78.12%] [G loss: 3.631964]\n",
      "epoch:3 step:2812 [D loss: 0.624538, acc: 72.66%] [G loss: 3.020952]\n",
      "epoch:3 step:2813 [D loss: 0.497549, acc: 76.56%] [G loss: 3.287708]\n",
      "epoch:3 step:2814 [D loss: 0.527367, acc: 73.44%] [G loss: 2.867390]\n",
      "epoch:3 step:2815 [D loss: 0.497085, acc: 74.22%] [G loss: 3.634776]\n",
      "epoch:3 step:2816 [D loss: 0.563969, acc: 70.31%] [G loss: 3.311083]\n",
      "epoch:3 step:2817 [D loss: 0.526648, acc: 67.97%] [G loss: 3.424958]\n",
      "epoch:3 step:2818 [D loss: 0.592550, acc: 69.53%] [G loss: 3.306230]\n",
      "epoch:3 step:2819 [D loss: 0.493360, acc: 75.78%] [G loss: 3.098759]\n",
      "epoch:3 step:2820 [D loss: 0.599066, acc: 73.44%] [G loss: 3.451382]\n",
      "epoch:3 step:2821 [D loss: 0.595034, acc: 68.75%] [G loss: 3.316706]\n",
      "epoch:3 step:2822 [D loss: 0.526659, acc: 78.12%] [G loss: 3.397526]\n",
      "epoch:3 step:2823 [D loss: 0.516808, acc: 75.00%] [G loss: 3.194021]\n",
      "epoch:3 step:2824 [D loss: 0.546893, acc: 69.53%] [G loss: 3.047247]\n",
      "epoch:3 step:2825 [D loss: 0.566168, acc: 71.09%] [G loss: 3.110768]\n",
      "epoch:3 step:2826 [D loss: 0.449911, acc: 80.47%] [G loss: 3.453485]\n",
      "epoch:3 step:2827 [D loss: 0.469252, acc: 78.91%] [G loss: 3.264113]\n",
      "epoch:3 step:2828 [D loss: 0.542896, acc: 73.44%] [G loss: 2.770573]\n",
      "epoch:3 step:2829 [D loss: 0.709567, acc: 60.16%] [G loss: 2.603611]\n",
      "epoch:3 step:2830 [D loss: 0.586377, acc: 66.41%] [G loss: 2.800220]\n",
      "epoch:3 step:2831 [D loss: 0.687974, acc: 63.28%] [G loss: 2.674150]\n",
      "epoch:3 step:2832 [D loss: 0.577355, acc: 71.88%] [G loss: 3.049720]\n",
      "epoch:3 step:2833 [D loss: 0.606177, acc: 72.66%] [G loss: 3.090571]\n",
      "epoch:3 step:2834 [D loss: 0.529399, acc: 72.66%] [G loss: 3.411361]\n",
      "epoch:3 step:2835 [D loss: 0.542511, acc: 72.66%] [G loss: 2.892945]\n",
      "epoch:3 step:2836 [D loss: 0.522833, acc: 76.56%] [G loss: 3.258188]\n",
      "epoch:3 step:2837 [D loss: 0.545707, acc: 73.44%] [G loss: 3.143477]\n",
      "epoch:3 step:2838 [D loss: 0.524736, acc: 73.44%] [G loss: 3.377181]\n",
      "epoch:3 step:2839 [D loss: 0.622447, acc: 66.41%] [G loss: 2.726401]\n",
      "epoch:3 step:2840 [D loss: 0.565836, acc: 68.75%] [G loss: 3.220721]\n",
      "epoch:3 step:2841 [D loss: 0.611264, acc: 71.88%] [G loss: 2.480731]\n",
      "epoch:3 step:2842 [D loss: 0.564181, acc: 67.97%] [G loss: 3.203266]\n",
      "epoch:3 step:2843 [D loss: 0.599604, acc: 71.88%] [G loss: 2.973679]\n",
      "epoch:3 step:2844 [D loss: 0.541164, acc: 74.22%] [G loss: 3.569999]\n",
      "epoch:3 step:2845 [D loss: 0.472925, acc: 75.78%] [G loss: 3.137151]\n",
      "epoch:3 step:2846 [D loss: 0.451836, acc: 79.69%] [G loss: 3.338508]\n",
      "epoch:3 step:2847 [D loss: 0.487097, acc: 75.78%] [G loss: 3.191708]\n",
      "epoch:3 step:2848 [D loss: 0.548601, acc: 70.31%] [G loss: 3.834303]\n",
      "epoch:3 step:2849 [D loss: 0.615297, acc: 66.41%] [G loss: 3.125118]\n",
      "epoch:3 step:2850 [D loss: 0.470268, acc: 73.44%] [G loss: 3.451973]\n",
      "epoch:3 step:2851 [D loss: 0.454552, acc: 79.69%] [G loss: 3.511666]\n",
      "epoch:3 step:2852 [D loss: 0.573686, acc: 72.66%] [G loss: 2.987877]\n",
      "epoch:3 step:2853 [D loss: 0.584260, acc: 69.53%] [G loss: 3.207634]\n",
      "epoch:3 step:2854 [D loss: 0.577807, acc: 69.53%] [G loss: 3.690872]\n",
      "epoch:3 step:2855 [D loss: 0.567493, acc: 70.31%] [G loss: 3.089888]\n",
      "epoch:3 step:2856 [D loss: 0.612609, acc: 71.88%] [G loss: 2.746633]\n",
      "epoch:3 step:2857 [D loss: 0.507311, acc: 73.44%] [G loss: 2.944871]\n",
      "epoch:3 step:2858 [D loss: 0.488740, acc: 78.12%] [G loss: 3.007040]\n",
      "epoch:3 step:2859 [D loss: 0.533127, acc: 75.78%] [G loss: 3.026366]\n",
      "epoch:3 step:2860 [D loss: 0.549493, acc: 71.88%] [G loss: 2.774801]\n",
      "epoch:3 step:2861 [D loss: 0.519206, acc: 70.31%] [G loss: 3.112018]\n",
      "epoch:3 step:2862 [D loss: 0.511948, acc: 73.44%] [G loss: 3.348692]\n",
      "epoch:3 step:2863 [D loss: 0.572386, acc: 71.88%] [G loss: 3.149652]\n",
      "epoch:3 step:2864 [D loss: 0.565344, acc: 75.78%] [G loss: 3.464571]\n",
      "epoch:3 step:2865 [D loss: 0.627656, acc: 66.41%] [G loss: 3.003045]\n",
      "epoch:3 step:2866 [D loss: 0.497976, acc: 72.66%] [G loss: 3.330878]\n",
      "epoch:3 step:2867 [D loss: 0.589746, acc: 69.53%] [G loss: 2.734889]\n",
      "epoch:3 step:2868 [D loss: 0.490471, acc: 75.78%] [G loss: 3.058274]\n",
      "epoch:3 step:2869 [D loss: 0.552166, acc: 70.31%] [G loss: 2.828513]\n",
      "epoch:3 step:2870 [D loss: 0.493650, acc: 73.44%] [G loss: 3.083166]\n",
      "epoch:3 step:2871 [D loss: 0.530066, acc: 71.09%] [G loss: 3.011160]\n",
      "epoch:3 step:2872 [D loss: 0.455278, acc: 78.91%] [G loss: 3.399126]\n",
      "epoch:3 step:2873 [D loss: 0.619415, acc: 68.75%] [G loss: 3.164514]\n",
      "epoch:3 step:2874 [D loss: 0.492104, acc: 74.22%] [G loss: 2.958678]\n",
      "epoch:3 step:2875 [D loss: 0.550685, acc: 73.44%] [G loss: 3.138045]\n",
      "epoch:3 step:2876 [D loss: 0.530667, acc: 72.66%] [G loss: 2.884089]\n",
      "epoch:3 step:2877 [D loss: 0.582646, acc: 69.53%] [G loss: 2.956734]\n",
      "epoch:3 step:2878 [D loss: 0.545341, acc: 71.09%] [G loss: 3.271360]\n",
      "epoch:3 step:2879 [D loss: 0.563320, acc: 75.00%] [G loss: 3.366362]\n",
      "epoch:3 step:2880 [D loss: 0.574538, acc: 71.09%] [G loss: 2.689976]\n",
      "epoch:3 step:2881 [D loss: 0.590997, acc: 68.75%] [G loss: 3.026742]\n",
      "epoch:3 step:2882 [D loss: 0.481163, acc: 75.00%] [G loss: 3.346948]\n",
      "epoch:3 step:2883 [D loss: 0.528428, acc: 72.66%] [G loss: 3.307868]\n",
      "epoch:3 step:2884 [D loss: 0.595447, acc: 70.31%] [G loss: 3.317759]\n",
      "epoch:3 step:2885 [D loss: 0.551152, acc: 70.31%] [G loss: 2.993064]\n",
      "epoch:3 step:2886 [D loss: 0.489557, acc: 77.34%] [G loss: 3.846244]\n",
      "epoch:3 step:2887 [D loss: 0.544756, acc: 75.00%] [G loss: 3.315839]\n",
      "epoch:3 step:2888 [D loss: 0.531581, acc: 75.78%] [G loss: 3.802765]\n",
      "epoch:3 step:2889 [D loss: 0.597500, acc: 65.62%] [G loss: 3.215121]\n",
      "epoch:3 step:2890 [D loss: 0.476644, acc: 78.91%] [G loss: 3.048554]\n",
      "epoch:3 step:2891 [D loss: 0.530017, acc: 69.53%] [G loss: 3.091971]\n",
      "epoch:3 step:2892 [D loss: 0.596344, acc: 68.75%] [G loss: 2.795060]\n",
      "epoch:3 step:2893 [D loss: 0.473263, acc: 73.44%] [G loss: 3.408950]\n",
      "epoch:3 step:2894 [D loss: 0.552526, acc: 72.66%] [G loss: 3.208775]\n",
      "epoch:3 step:2895 [D loss: 0.585699, acc: 67.97%] [G loss: 2.778597]\n",
      "epoch:3 step:2896 [D loss: 0.586322, acc: 66.41%] [G loss: 3.033280]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:3 step:2897 [D loss: 0.555554, acc: 71.09%] [G loss: 3.072174]\n",
      "epoch:3 step:2898 [D loss: 0.498963, acc: 75.78%] [G loss: 3.632527]\n",
      "epoch:3 step:2899 [D loss: 0.566850, acc: 65.62%] [G loss: 3.206548]\n",
      "epoch:3 step:2900 [D loss: 0.533118, acc: 75.78%] [G loss: 3.055626]\n",
      "epoch:3 step:2901 [D loss: 0.547861, acc: 75.00%] [G loss: 2.974113]\n",
      "epoch:3 step:2902 [D loss: 0.529991, acc: 74.22%] [G loss: 3.233254]\n",
      "epoch:3 step:2903 [D loss: 0.521136, acc: 75.78%] [G loss: 3.126778]\n",
      "epoch:3 step:2904 [D loss: 0.579053, acc: 69.53%] [G loss: 2.993903]\n",
      "epoch:3 step:2905 [D loss: 0.761708, acc: 57.03%] [G loss: 2.702741]\n",
      "epoch:3 step:2906 [D loss: 0.568098, acc: 70.31%] [G loss: 2.973681]\n",
      "epoch:3 step:2907 [D loss: 0.441284, acc: 78.12%] [G loss: 3.276073]\n",
      "epoch:3 step:2908 [D loss: 0.582623, acc: 67.97%] [G loss: 2.978779]\n",
      "epoch:3 step:2909 [D loss: 0.580298, acc: 72.66%] [G loss: 3.170975]\n",
      "epoch:3 step:2910 [D loss: 0.551396, acc: 71.09%] [G loss: 2.836452]\n",
      "epoch:3 step:2911 [D loss: 0.469920, acc: 77.34%] [G loss: 3.103482]\n",
      "epoch:3 step:2912 [D loss: 0.442472, acc: 81.25%] [G loss: 3.122692]\n",
      "epoch:3 step:2913 [D loss: 0.635850, acc: 66.41%] [G loss: 3.196837]\n",
      "epoch:3 step:2914 [D loss: 0.524120, acc: 69.53%] [G loss: 3.943135]\n",
      "epoch:3 step:2915 [D loss: 0.649693, acc: 62.50%] [G loss: 3.023122]\n",
      "epoch:3 step:2916 [D loss: 0.548933, acc: 70.31%] [G loss: 3.486594]\n",
      "epoch:3 step:2917 [D loss: 0.509035, acc: 74.22%] [G loss: 2.745534]\n",
      "epoch:3 step:2918 [D loss: 0.622423, acc: 61.72%] [G loss: 2.615685]\n",
      "epoch:3 step:2919 [D loss: 0.692688, acc: 60.94%] [G loss: 2.913382]\n",
      "epoch:3 step:2920 [D loss: 0.596149, acc: 64.84%] [G loss: 2.795338]\n",
      "epoch:3 step:2921 [D loss: 0.517516, acc: 76.56%] [G loss: 2.758631]\n",
      "epoch:3 step:2922 [D loss: 0.538528, acc: 74.22%] [G loss: 3.026288]\n",
      "epoch:3 step:2923 [D loss: 0.557770, acc: 74.22%] [G loss: 3.034129]\n",
      "epoch:3 step:2924 [D loss: 0.600985, acc: 66.41%] [G loss: 3.266006]\n",
      "epoch:3 step:2925 [D loss: 0.504242, acc: 76.56%] [G loss: 2.807848]\n",
      "epoch:3 step:2926 [D loss: 0.558794, acc: 69.53%] [G loss: 2.974102]\n",
      "epoch:3 step:2927 [D loss: 0.633737, acc: 63.28%] [G loss: 3.262070]\n",
      "epoch:3 step:2928 [D loss: 0.517670, acc: 76.56%] [G loss: 3.538749]\n",
      "epoch:3 step:2929 [D loss: 0.533049, acc: 73.44%] [G loss: 3.514101]\n",
      "epoch:3 step:2930 [D loss: 0.485177, acc: 75.78%] [G loss: 3.625264]\n",
      "epoch:3 step:2931 [D loss: 0.566399, acc: 70.31%] [G loss: 3.367421]\n",
      "epoch:3 step:2932 [D loss: 0.591232, acc: 69.53%] [G loss: 3.237682]\n",
      "epoch:3 step:2933 [D loss: 0.557043, acc: 70.31%] [G loss: 3.178837]\n",
      "epoch:3 step:2934 [D loss: 0.559215, acc: 68.75%] [G loss: 3.215093]\n",
      "epoch:3 step:2935 [D loss: 0.628173, acc: 67.97%] [G loss: 2.883038]\n",
      "epoch:3 step:2936 [D loss: 0.551219, acc: 73.44%] [G loss: 2.791148]\n",
      "epoch:3 step:2937 [D loss: 0.491499, acc: 76.56%] [G loss: 3.057966]\n",
      "epoch:3 step:2938 [D loss: 0.527009, acc: 73.44%] [G loss: 3.105300]\n",
      "epoch:3 step:2939 [D loss: 0.592876, acc: 69.53%] [G loss: 3.159314]\n",
      "epoch:3 step:2940 [D loss: 0.620302, acc: 66.41%] [G loss: 2.924850]\n",
      "epoch:3 step:2941 [D loss: 0.476503, acc: 82.03%] [G loss: 3.252253]\n",
      "epoch:3 step:2942 [D loss: 0.450576, acc: 78.91%] [G loss: 3.511767]\n",
      "epoch:3 step:2943 [D loss: 0.589743, acc: 69.53%] [G loss: 3.305946]\n",
      "epoch:3 step:2944 [D loss: 0.666105, acc: 62.50%] [G loss: 2.609447]\n",
      "epoch:3 step:2945 [D loss: 0.598551, acc: 70.31%] [G loss: 3.091424]\n",
      "epoch:3 step:2946 [D loss: 0.475264, acc: 77.34%] [G loss: 2.773214]\n",
      "epoch:3 step:2947 [D loss: 0.596446, acc: 71.09%] [G loss: 2.731249]\n",
      "epoch:3 step:2948 [D loss: 0.546837, acc: 75.00%] [G loss: 2.689850]\n",
      "epoch:3 step:2949 [D loss: 0.573842, acc: 68.75%] [G loss: 3.074590]\n",
      "epoch:3 step:2950 [D loss: 0.566496, acc: 71.09%] [G loss: 2.701527]\n",
      "epoch:3 step:2951 [D loss: 0.578182, acc: 71.09%] [G loss: 2.808758]\n",
      "epoch:3 step:2952 [D loss: 0.490328, acc: 75.00%] [G loss: 3.454808]\n",
      "epoch:3 step:2953 [D loss: 0.560216, acc: 75.00%] [G loss: 3.368956]\n",
      "epoch:3 step:2954 [D loss: 0.612422, acc: 67.19%] [G loss: 3.124205]\n",
      "epoch:3 step:2955 [D loss: 0.649282, acc: 63.28%] [G loss: 2.893806]\n",
      "epoch:3 step:2956 [D loss: 0.483986, acc: 76.56%] [G loss: 3.326700]\n",
      "epoch:3 step:2957 [D loss: 0.620521, acc: 67.19%] [G loss: 3.045577]\n",
      "epoch:3 step:2958 [D loss: 0.621481, acc: 70.31%] [G loss: 2.968652]\n",
      "epoch:3 step:2959 [D loss: 0.625503, acc: 66.41%] [G loss: 2.880377]\n",
      "epoch:3 step:2960 [D loss: 0.532097, acc: 72.66%] [G loss: 3.395056]\n",
      "epoch:3 step:2961 [D loss: 0.583641, acc: 72.66%] [G loss: 2.838863]\n",
      "epoch:3 step:2962 [D loss: 0.660730, acc: 62.50%] [G loss: 3.252282]\n",
      "epoch:3 step:2963 [D loss: 0.516065, acc: 76.56%] [G loss: 3.293134]\n",
      "epoch:3 step:2964 [D loss: 0.626947, acc: 70.31%] [G loss: 2.645730]\n",
      "epoch:3 step:2965 [D loss: 0.527978, acc: 75.78%] [G loss: 3.114901]\n",
      "epoch:3 step:2966 [D loss: 0.510581, acc: 75.00%] [G loss: 2.980869]\n",
      "epoch:3 step:2967 [D loss: 0.587602, acc: 75.00%] [G loss: 3.032752]\n",
      "epoch:3 step:2968 [D loss: 0.596103, acc: 67.19%] [G loss: 2.912090]\n",
      "epoch:3 step:2969 [D loss: 0.585234, acc: 65.62%] [G loss: 3.107921]\n",
      "epoch:3 step:2970 [D loss: 0.523778, acc: 67.97%] [G loss: 3.306567]\n",
      "epoch:3 step:2971 [D loss: 0.673844, acc: 62.50%] [G loss: 2.487002]\n",
      "epoch:3 step:2972 [D loss: 0.582697, acc: 68.75%] [G loss: 2.625928]\n",
      "epoch:3 step:2973 [D loss: 0.548148, acc: 75.78%] [G loss: 3.268926]\n",
      "epoch:3 step:2974 [D loss: 0.476590, acc: 76.56%] [G loss: 3.157892]\n",
      "epoch:3 step:2975 [D loss: 0.527181, acc: 72.66%] [G loss: 3.287874]\n",
      "epoch:3 step:2976 [D loss: 0.565803, acc: 76.56%] [G loss: 3.127961]\n",
      "epoch:3 step:2977 [D loss: 0.554749, acc: 72.66%] [G loss: 2.521472]\n",
      "epoch:3 step:2978 [D loss: 0.543144, acc: 71.88%] [G loss: 2.859064]\n",
      "epoch:3 step:2979 [D loss: 0.554855, acc: 70.31%] [G loss: 2.850513]\n",
      "epoch:3 step:2980 [D loss: 0.551896, acc: 73.44%] [G loss: 2.683033]\n",
      "epoch:3 step:2981 [D loss: 0.515771, acc: 73.44%] [G loss: 3.074255]\n",
      "epoch:3 step:2982 [D loss: 0.554599, acc: 77.34%] [G loss: 2.840838]\n",
      "epoch:3 step:2983 [D loss: 0.497913, acc: 81.25%] [G loss: 3.155581]\n",
      "epoch:3 step:2984 [D loss: 0.559770, acc: 69.53%] [G loss: 3.267322]\n",
      "epoch:3 step:2985 [D loss: 0.508520, acc: 71.88%] [G loss: 3.492514]\n",
      "epoch:3 step:2986 [D loss: 0.603502, acc: 69.53%] [G loss: 3.072486]\n",
      "epoch:3 step:2987 [D loss: 0.613591, acc: 65.62%] [G loss: 3.258351]\n",
      "epoch:3 step:2988 [D loss: 0.495579, acc: 76.56%] [G loss: 3.534975]\n",
      "epoch:3 step:2989 [D loss: 0.523795, acc: 72.66%] [G loss: 3.219226]\n",
      "epoch:3 step:2990 [D loss: 0.532906, acc: 72.66%] [G loss: 3.566179]\n",
      "epoch:3 step:2991 [D loss: 0.583261, acc: 69.53%] [G loss: 3.009922]\n",
      "epoch:3 step:2992 [D loss: 0.569243, acc: 66.41%] [G loss: 2.896689]\n",
      "epoch:3 step:2993 [D loss: 0.562524, acc: 72.66%] [G loss: 3.114038]\n",
      "epoch:3 step:2994 [D loss: 0.611712, acc: 64.06%] [G loss: 2.860407]\n",
      "epoch:3 step:2995 [D loss: 0.512937, acc: 77.34%] [G loss: 3.021124]\n",
      "epoch:3 step:2996 [D loss: 0.576051, acc: 71.88%] [G loss: 2.907279]\n",
      "epoch:3 step:2997 [D loss: 0.617317, acc: 67.97%] [G loss: 3.104752]\n",
      "epoch:3 step:2998 [D loss: 0.653562, acc: 63.28%] [G loss: 2.915419]\n",
      "epoch:3 step:2999 [D loss: 0.581151, acc: 70.31%] [G loss: 3.189100]\n",
      "epoch:3 step:3000 [D loss: 0.561147, acc: 71.09%] [G loss: 3.007668]\n",
      "##############\n",
      "[3.01761145 1.87183068 7.10582323 5.46613429 4.47591284 6.12282184\n",
      " 5.69012067 5.44838839 5.69223575 4.04956918]\n",
      "##########\n",
      "epoch:3 step:3001 [D loss: 0.475844, acc: 75.78%] [G loss: 3.117389]\n",
      "epoch:3 step:3002 [D loss: 0.547387, acc: 75.00%] [G loss: 3.204070]\n",
      "epoch:3 step:3003 [D loss: 0.525274, acc: 73.44%] [G loss: 2.761926]\n",
      "epoch:3 step:3004 [D loss: 0.498962, acc: 74.22%] [G loss: 2.885070]\n",
      "epoch:3 step:3005 [D loss: 0.477724, acc: 82.03%] [G loss: 3.658484]\n",
      "epoch:3 step:3006 [D loss: 0.533897, acc: 71.09%] [G loss: 3.389807]\n",
      "epoch:3 step:3007 [D loss: 0.543903, acc: 72.66%] [G loss: 2.863131]\n",
      "epoch:3 step:3008 [D loss: 0.453912, acc: 82.81%] [G loss: 2.953436]\n",
      "epoch:3 step:3009 [D loss: 0.473555, acc: 77.34%] [G loss: 2.865091]\n",
      "epoch:3 step:3010 [D loss: 0.580171, acc: 67.97%] [G loss: 2.971815]\n",
      "epoch:3 step:3011 [D loss: 0.642213, acc: 64.06%] [G loss: 2.891825]\n",
      "epoch:3 step:3012 [D loss: 0.482853, acc: 78.91%] [G loss: 3.243539]\n",
      "epoch:3 step:3013 [D loss: 0.604519, acc: 71.09%] [G loss: 2.886034]\n",
      "epoch:3 step:3014 [D loss: 0.595544, acc: 69.53%] [G loss: 3.116179]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:3 step:3015 [D loss: 0.532453, acc: 73.44%] [G loss: 3.178631]\n",
      "epoch:3 step:3016 [D loss: 0.480666, acc: 77.34%] [G loss: 3.201871]\n",
      "epoch:3 step:3017 [D loss: 0.645689, acc: 69.53%] [G loss: 3.207975]\n",
      "epoch:3 step:3018 [D loss: 0.508760, acc: 78.12%] [G loss: 3.193007]\n",
      "epoch:3 step:3019 [D loss: 0.606963, acc: 67.97%] [G loss: 3.110071]\n",
      "epoch:3 step:3020 [D loss: 0.479126, acc: 78.91%] [G loss: 2.989294]\n",
      "epoch:3 step:3021 [D loss: 0.538961, acc: 78.12%] [G loss: 3.275818]\n",
      "epoch:3 step:3022 [D loss: 0.597460, acc: 67.97%] [G loss: 2.934004]\n",
      "epoch:3 step:3023 [D loss: 0.547320, acc: 72.66%] [G loss: 3.330061]\n",
      "epoch:3 step:3024 [D loss: 0.514590, acc: 74.22%] [G loss: 3.157495]\n",
      "epoch:3 step:3025 [D loss: 0.667330, acc: 64.06%] [G loss: 3.204637]\n",
      "epoch:3 step:3026 [D loss: 0.568030, acc: 72.66%] [G loss: 3.053463]\n",
      "epoch:3 step:3027 [D loss: 0.552295, acc: 71.09%] [G loss: 2.677353]\n",
      "epoch:3 step:3028 [D loss: 0.535177, acc: 74.22%] [G loss: 2.883538]\n",
      "epoch:3 step:3029 [D loss: 0.666328, acc: 66.41%] [G loss: 3.046504]\n",
      "epoch:3 step:3030 [D loss: 0.625447, acc: 64.84%] [G loss: 3.062345]\n",
      "epoch:3 step:3031 [D loss: 0.591949, acc: 67.97%] [G loss: 3.165940]\n",
      "epoch:3 step:3032 [D loss: 0.474961, acc: 75.78%] [G loss: 2.881811]\n",
      "epoch:3 step:3033 [D loss: 0.460289, acc: 78.12%] [G loss: 3.085955]\n",
      "epoch:3 step:3034 [D loss: 0.543691, acc: 67.97%] [G loss: 3.479338]\n",
      "epoch:3 step:3035 [D loss: 0.575783, acc: 67.97%] [G loss: 3.002727]\n",
      "epoch:3 step:3036 [D loss: 0.512796, acc: 74.22%] [G loss: 2.995808]\n",
      "epoch:3 step:3037 [D loss: 0.540414, acc: 72.66%] [G loss: 2.845483]\n",
      "epoch:3 step:3038 [D loss: 0.539350, acc: 71.09%] [G loss: 2.687193]\n",
      "epoch:3 step:3039 [D loss: 0.578487, acc: 74.22%] [G loss: 3.087788]\n",
      "epoch:3 step:3040 [D loss: 0.601266, acc: 72.66%] [G loss: 3.284683]\n",
      "epoch:3 step:3041 [D loss: 0.444425, acc: 78.12%] [G loss: 3.498372]\n",
      "epoch:3 step:3042 [D loss: 0.516092, acc: 73.44%] [G loss: 3.396986]\n",
      "epoch:3 step:3043 [D loss: 0.503288, acc: 76.56%] [G loss: 3.718489]\n",
      "epoch:3 step:3044 [D loss: 0.569271, acc: 71.09%] [G loss: 3.691097]\n",
      "epoch:3 step:3045 [D loss: 0.522558, acc: 75.00%] [G loss: 3.503308]\n",
      "epoch:3 step:3046 [D loss: 0.489897, acc: 76.56%] [G loss: 3.162934]\n",
      "epoch:3 step:3047 [D loss: 0.523113, acc: 74.22%] [G loss: 3.002619]\n",
      "epoch:3 step:3048 [D loss: 0.539522, acc: 75.00%] [G loss: 2.835299]\n",
      "epoch:3 step:3049 [D loss: 0.553541, acc: 71.09%] [G loss: 3.072251]\n",
      "epoch:3 step:3050 [D loss: 0.535745, acc: 71.88%] [G loss: 2.878885]\n",
      "epoch:3 step:3051 [D loss: 0.463657, acc: 77.34%] [G loss: 3.149378]\n",
      "epoch:3 step:3052 [D loss: 0.597074, acc: 65.62%] [G loss: 2.937661]\n",
      "epoch:3 step:3053 [D loss: 0.545295, acc: 70.31%] [G loss: 2.881111]\n",
      "epoch:3 step:3054 [D loss: 0.532149, acc: 70.31%] [G loss: 2.996034]\n",
      "epoch:3 step:3055 [D loss: 0.600794, acc: 70.31%] [G loss: 3.080612]\n",
      "epoch:3 step:3056 [D loss: 0.546592, acc: 69.53%] [G loss: 2.701332]\n",
      "epoch:3 step:3057 [D loss: 0.497268, acc: 75.78%] [G loss: 3.360913]\n",
      "epoch:3 step:3058 [D loss: 0.695711, acc: 60.16%] [G loss: 2.500246]\n",
      "epoch:3 step:3059 [D loss: 0.510090, acc: 75.00%] [G loss: 2.881374]\n",
      "epoch:3 step:3060 [D loss: 0.613839, acc: 68.75%] [G loss: 3.162191]\n",
      "epoch:3 step:3061 [D loss: 0.525994, acc: 78.12%] [G loss: 3.130927]\n",
      "epoch:3 step:3062 [D loss: 0.608881, acc: 69.53%] [G loss: 2.890437]\n",
      "epoch:3 step:3063 [D loss: 0.589156, acc: 66.41%] [G loss: 2.799414]\n",
      "epoch:3 step:3064 [D loss: 0.521215, acc: 78.12%] [G loss: 2.992800]\n",
      "epoch:3 step:3065 [D loss: 0.507327, acc: 75.78%] [G loss: 3.186768]\n",
      "epoch:3 step:3066 [D loss: 0.480573, acc: 75.78%] [G loss: 3.333643]\n",
      "epoch:3 step:3067 [D loss: 0.532068, acc: 76.56%] [G loss: 2.892350]\n",
      "epoch:3 step:3068 [D loss: 0.526332, acc: 72.66%] [G loss: 3.005254]\n",
      "epoch:3 step:3069 [D loss: 0.461891, acc: 80.47%] [G loss: 3.160824]\n",
      "epoch:3 step:3070 [D loss: 0.560011, acc: 71.88%] [G loss: 3.369709]\n",
      "epoch:3 step:3071 [D loss: 0.532215, acc: 74.22%] [G loss: 3.164091]\n",
      "epoch:3 step:3072 [D loss: 0.570079, acc: 74.22%] [G loss: 3.760334]\n",
      "epoch:3 step:3073 [D loss: 0.552666, acc: 71.09%] [G loss: 3.294545]\n",
      "epoch:3 step:3074 [D loss: 0.635564, acc: 66.41%] [G loss: 2.823268]\n",
      "epoch:3 step:3075 [D loss: 0.545897, acc: 72.66%] [G loss: 3.353469]\n",
      "epoch:3 step:3076 [D loss: 0.563765, acc: 67.97%] [G loss: 3.114313]\n",
      "epoch:3 step:3077 [D loss: 0.558981, acc: 71.88%] [G loss: 3.387892]\n",
      "epoch:3 step:3078 [D loss: 0.575435, acc: 71.88%] [G loss: 3.227087]\n",
      "epoch:3 step:3079 [D loss: 0.515218, acc: 72.66%] [G loss: 2.845400]\n",
      "epoch:3 step:3080 [D loss: 0.555516, acc: 72.66%] [G loss: 2.928986]\n",
      "epoch:3 step:3081 [D loss: 0.521990, acc: 75.00%] [G loss: 2.680465]\n",
      "epoch:3 step:3082 [D loss: 0.560230, acc: 73.44%] [G loss: 3.208104]\n",
      "epoch:3 step:3083 [D loss: 0.545761, acc: 74.22%] [G loss: 3.266132]\n",
      "epoch:3 step:3084 [D loss: 0.593702, acc: 67.19%] [G loss: 3.368452]\n",
      "epoch:3 step:3085 [D loss: 0.666196, acc: 63.28%] [G loss: 2.666783]\n",
      "epoch:3 step:3086 [D loss: 0.620866, acc: 67.97%] [G loss: 2.787361]\n",
      "epoch:3 step:3087 [D loss: 0.651580, acc: 62.50%] [G loss: 2.942822]\n",
      "epoch:3 step:3088 [D loss: 0.709940, acc: 60.94%] [G loss: 2.721546]\n",
      "epoch:3 step:3089 [D loss: 0.567889, acc: 68.75%] [G loss: 2.949691]\n",
      "epoch:3 step:3090 [D loss: 0.517658, acc: 75.00%] [G loss: 3.384210]\n",
      "epoch:3 step:3091 [D loss: 0.628934, acc: 70.31%] [G loss: 3.019158]\n",
      "epoch:3 step:3092 [D loss: 0.725242, acc: 64.84%] [G loss: 3.151024]\n",
      "epoch:3 step:3093 [D loss: 0.548579, acc: 70.31%] [G loss: 3.162150]\n",
      "epoch:3 step:3094 [D loss: 0.449587, acc: 78.12%] [G loss: 3.020247]\n",
      "epoch:3 step:3095 [D loss: 0.530471, acc: 71.88%] [G loss: 3.222810]\n",
      "epoch:3 step:3096 [D loss: 0.557399, acc: 68.75%] [G loss: 2.898941]\n",
      "epoch:3 step:3097 [D loss: 0.464845, acc: 81.25%] [G loss: 3.385228]\n",
      "epoch:3 step:3098 [D loss: 0.553607, acc: 71.88%] [G loss: 3.568733]\n",
      "epoch:3 step:3099 [D loss: 0.608577, acc: 68.75%] [G loss: 2.910706]\n",
      "epoch:3 step:3100 [D loss: 0.591547, acc: 67.19%] [G loss: 3.314271]\n",
      "epoch:3 step:3101 [D loss: 0.538762, acc: 72.66%] [G loss: 2.790999]\n",
      "epoch:3 step:3102 [D loss: 0.638309, acc: 62.50%] [G loss: 2.739156]\n",
      "epoch:3 step:3103 [D loss: 0.627909, acc: 62.50%] [G loss: 2.735118]\n",
      "epoch:3 step:3104 [D loss: 0.566962, acc: 73.44%] [G loss: 2.971783]\n",
      "epoch:3 step:3105 [D loss: 0.506571, acc: 74.22%] [G loss: 3.099495]\n",
      "epoch:3 step:3106 [D loss: 0.502522, acc: 75.78%] [G loss: 2.991587]\n",
      "epoch:3 step:3107 [D loss: 0.456675, acc: 75.00%] [G loss: 3.299873]\n",
      "epoch:3 step:3108 [D loss: 0.561922, acc: 74.22%] [G loss: 3.011065]\n",
      "epoch:3 step:3109 [D loss: 0.531531, acc: 74.22%] [G loss: 3.244085]\n",
      "epoch:3 step:3110 [D loss: 0.576078, acc: 69.53%] [G loss: 2.914544]\n",
      "epoch:3 step:3111 [D loss: 0.492686, acc: 76.56%] [G loss: 3.288181]\n",
      "epoch:3 step:3112 [D loss: 0.665113, acc: 66.41%] [G loss: 2.613852]\n",
      "epoch:3 step:3113 [D loss: 0.518850, acc: 74.22%] [G loss: 3.181593]\n",
      "epoch:3 step:3114 [D loss: 0.569566, acc: 68.75%] [G loss: 3.201275]\n",
      "epoch:3 step:3115 [D loss: 0.426662, acc: 80.47%] [G loss: 3.331595]\n",
      "epoch:3 step:3116 [D loss: 0.506045, acc: 71.09%] [G loss: 3.086613]\n",
      "epoch:3 step:3117 [D loss: 0.603378, acc: 67.97%] [G loss: 2.936224]\n",
      "epoch:3 step:3118 [D loss: 0.610926, acc: 64.84%] [G loss: 3.079837]\n",
      "epoch:3 step:3119 [D loss: 0.553327, acc: 74.22%] [G loss: 2.963391]\n",
      "epoch:3 step:3120 [D loss: 0.493041, acc: 74.22%] [G loss: 3.200066]\n",
      "epoch:3 step:3121 [D loss: 0.536931, acc: 70.31%] [G loss: 3.045763]\n",
      "epoch:3 step:3122 [D loss: 0.498919, acc: 74.22%] [G loss: 3.285714]\n",
      "epoch:3 step:3123 [D loss: 0.524984, acc: 79.69%] [G loss: 3.905016]\n",
      "epoch:3 step:3124 [D loss: 0.590600, acc: 69.53%] [G loss: 3.413314]\n",
      "epoch:3 step:3125 [D loss: 0.520906, acc: 75.00%] [G loss: 3.566118]\n",
      "epoch:3 step:3126 [D loss: 0.470940, acc: 78.12%] [G loss: 3.645216]\n",
      "epoch:3 step:3127 [D loss: 0.676847, acc: 65.62%] [G loss: 2.767794]\n",
      "epoch:3 step:3128 [D loss: 0.591589, acc: 62.50%] [G loss: 2.580551]\n",
      "epoch:3 step:3129 [D loss: 0.640301, acc: 63.28%] [G loss: 2.646691]\n",
      "epoch:3 step:3130 [D loss: 0.579530, acc: 68.75%] [G loss: 2.716988]\n",
      "epoch:3 step:3131 [D loss: 0.510057, acc: 74.22%] [G loss: 2.981929]\n",
      "epoch:3 step:3132 [D loss: 0.479487, acc: 78.12%] [G loss: 3.336165]\n",
      "epoch:3 step:3133 [D loss: 0.502856, acc: 72.66%] [G loss: 2.779973]\n",
      "epoch:3 step:3134 [D loss: 0.590355, acc: 67.97%] [G loss: 2.936779]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:3 step:3135 [D loss: 0.570504, acc: 70.31%] [G loss: 3.220269]\n",
      "epoch:3 step:3136 [D loss: 0.541858, acc: 67.97%] [G loss: 3.466218]\n",
      "epoch:3 step:3137 [D loss: 0.577455, acc: 67.97%] [G loss: 3.088965]\n",
      "epoch:3 step:3138 [D loss: 0.550418, acc: 72.66%] [G loss: 3.230641]\n",
      "epoch:3 step:3139 [D loss: 0.592799, acc: 70.31%] [G loss: 2.659208]\n",
      "epoch:3 step:3140 [D loss: 0.531643, acc: 73.44%] [G loss: 3.046314]\n",
      "epoch:3 step:3141 [D loss: 0.536845, acc: 76.56%] [G loss: 2.995676]\n",
      "epoch:3 step:3142 [D loss: 0.550793, acc: 73.44%] [G loss: 3.041466]\n",
      "epoch:3 step:3143 [D loss: 0.526001, acc: 71.88%] [G loss: 2.853320]\n",
      "epoch:3 step:3144 [D loss: 0.471376, acc: 77.34%] [G loss: 3.176882]\n",
      "epoch:3 step:3145 [D loss: 0.595201, acc: 67.19%] [G loss: 3.035568]\n",
      "epoch:3 step:3146 [D loss: 0.562523, acc: 69.53%] [G loss: 3.309831]\n",
      "epoch:3 step:3147 [D loss: 0.469806, acc: 78.91%] [G loss: 3.130512]\n",
      "epoch:3 step:3148 [D loss: 0.569636, acc: 70.31%] [G loss: 3.068467]\n",
      "epoch:3 step:3149 [D loss: 0.536355, acc: 75.00%] [G loss: 3.504867]\n",
      "epoch:3 step:3150 [D loss: 0.577635, acc: 67.97%] [G loss: 2.947227]\n",
      "epoch:3 step:3151 [D loss: 0.595586, acc: 72.66%] [G loss: 3.225727]\n",
      "epoch:3 step:3152 [D loss: 0.616300, acc: 68.75%] [G loss: 2.867182]\n",
      "epoch:3 step:3153 [D loss: 0.551819, acc: 67.19%] [G loss: 2.945010]\n",
      "epoch:3 step:3154 [D loss: 0.467940, acc: 78.12%] [G loss: 3.038418]\n",
      "epoch:3 step:3155 [D loss: 0.503271, acc: 76.56%] [G loss: 3.407455]\n",
      "epoch:3 step:3156 [D loss: 0.462416, acc: 80.47%] [G loss: 3.522817]\n",
      "epoch:3 step:3157 [D loss: 0.477714, acc: 78.12%] [G loss: 3.872992]\n",
      "epoch:3 step:3158 [D loss: 0.446295, acc: 76.56%] [G loss: 3.522270]\n",
      "epoch:3 step:3159 [D loss: 0.560945, acc: 74.22%] [G loss: 3.035084]\n",
      "epoch:3 step:3160 [D loss: 0.632846, acc: 65.62%] [G loss: 2.619139]\n",
      "epoch:3 step:3161 [D loss: 0.657493, acc: 62.50%] [G loss: 2.948713]\n",
      "epoch:3 step:3162 [D loss: 0.536948, acc: 74.22%] [G loss: 3.228306]\n",
      "epoch:3 step:3163 [D loss: 0.641405, acc: 63.28%] [G loss: 3.064845]\n",
      "epoch:3 step:3164 [D loss: 0.588363, acc: 68.75%] [G loss: 3.200042]\n",
      "epoch:3 step:3165 [D loss: 0.554987, acc: 67.19%] [G loss: 3.290430]\n",
      "epoch:3 step:3166 [D loss: 0.577838, acc: 71.09%] [G loss: 2.974048]\n",
      "epoch:3 step:3167 [D loss: 0.515751, acc: 74.22%] [G loss: 2.718325]\n",
      "epoch:3 step:3168 [D loss: 0.466300, acc: 75.00%] [G loss: 3.056642]\n",
      "epoch:3 step:3169 [D loss: 0.465793, acc: 82.81%] [G loss: 3.191996]\n",
      "epoch:3 step:3170 [D loss: 0.525291, acc: 77.34%] [G loss: 3.637507]\n",
      "epoch:3 step:3171 [D loss: 0.487721, acc: 74.22%] [G loss: 3.098441]\n",
      "epoch:3 step:3172 [D loss: 0.522026, acc: 75.78%] [G loss: 3.035978]\n",
      "epoch:3 step:3173 [D loss: 0.593836, acc: 64.84%] [G loss: 3.013773]\n",
      "epoch:3 step:3174 [D loss: 0.498622, acc: 73.44%] [G loss: 3.146946]\n",
      "epoch:3 step:3175 [D loss: 0.594511, acc: 67.19%] [G loss: 3.214317]\n",
      "epoch:3 step:3176 [D loss: 0.507608, acc: 76.56%] [G loss: 3.379067]\n",
      "epoch:3 step:3177 [D loss: 0.617247, acc: 66.41%] [G loss: 3.457168]\n",
      "epoch:3 step:3178 [D loss: 0.632677, acc: 66.41%] [G loss: 2.913635]\n",
      "epoch:3 step:3179 [D loss: 0.549568, acc: 72.66%] [G loss: 3.538275]\n",
      "epoch:3 step:3180 [D loss: 0.583075, acc: 66.41%] [G loss: 3.051671]\n",
      "epoch:3 step:3181 [D loss: 0.616992, acc: 68.75%] [G loss: 2.765278]\n",
      "epoch:3 step:3182 [D loss: 0.551068, acc: 75.00%] [G loss: 3.106096]\n",
      "epoch:3 step:3183 [D loss: 0.490203, acc: 73.44%] [G loss: 2.906852]\n",
      "epoch:3 step:3184 [D loss: 0.657479, acc: 67.19%] [G loss: 2.999179]\n",
      "epoch:3 step:3185 [D loss: 0.557185, acc: 71.09%] [G loss: 2.987515]\n",
      "epoch:3 step:3186 [D loss: 0.621337, acc: 71.09%] [G loss: 2.824134]\n",
      "epoch:3 step:3187 [D loss: 0.629681, acc: 67.19%] [G loss: 2.738922]\n",
      "epoch:3 step:3188 [D loss: 0.629829, acc: 67.19%] [G loss: 2.845901]\n",
      "epoch:3 step:3189 [D loss: 0.518416, acc: 71.88%] [G loss: 2.918583]\n",
      "epoch:3 step:3190 [D loss: 0.587000, acc: 67.19%] [G loss: 3.163378]\n",
      "epoch:3 step:3191 [D loss: 0.539834, acc: 70.31%] [G loss: 2.979915]\n",
      "epoch:3 step:3192 [D loss: 0.454961, acc: 78.91%] [G loss: 3.238802]\n",
      "epoch:3 step:3193 [D loss: 0.601099, acc: 67.19%] [G loss: 3.043716]\n",
      "epoch:3 step:3194 [D loss: 0.502683, acc: 75.00%] [G loss: 3.179185]\n",
      "epoch:3 step:3195 [D loss: 0.613121, acc: 69.53%] [G loss: 3.009877]\n",
      "epoch:3 step:3196 [D loss: 0.527298, acc: 75.78%] [G loss: 3.257596]\n",
      "epoch:3 step:3197 [D loss: 0.618609, acc: 64.84%] [G loss: 2.799037]\n",
      "epoch:3 step:3198 [D loss: 0.623966, acc: 65.62%] [G loss: 2.919865]\n",
      "epoch:3 step:3199 [D loss: 0.495559, acc: 73.44%] [G loss: 2.811435]\n",
      "epoch:3 step:3200 [D loss: 0.607528, acc: 72.66%] [G loss: 3.225511]\n",
      "##############\n",
      "[3.10243065 1.8731585  7.25639137 5.55782189 4.49899845 6.00602658\n",
      " 5.45554213 5.2854461  5.60021921 4.01533243]\n",
      "##########\n",
      "epoch:3 step:3201 [D loss: 0.563383, acc: 69.53%] [G loss: 3.197299]\n",
      "epoch:3 step:3202 [D loss: 0.659970, acc: 66.41%] [G loss: 2.747551]\n",
      "epoch:3 step:3203 [D loss: 0.444064, acc: 79.69%] [G loss: 3.285273]\n",
      "epoch:3 step:3204 [D loss: 0.503813, acc: 71.88%] [G loss: 3.107164]\n",
      "epoch:3 step:3205 [D loss: 0.499048, acc: 79.69%] [G loss: 3.078759]\n",
      "epoch:3 step:3206 [D loss: 0.658419, acc: 67.19%] [G loss: 3.137674]\n",
      "epoch:3 step:3207 [D loss: 0.634994, acc: 68.75%] [G loss: 2.907765]\n",
      "epoch:3 step:3208 [D loss: 0.517575, acc: 75.00%] [G loss: 3.327643]\n",
      "epoch:3 step:3209 [D loss: 0.506296, acc: 71.88%] [G loss: 3.517901]\n",
      "epoch:3 step:3210 [D loss: 0.549995, acc: 72.66%] [G loss: 3.502023]\n",
      "epoch:3 step:3211 [D loss: 0.499109, acc: 75.00%] [G loss: 3.346334]\n",
      "epoch:3 step:3212 [D loss: 0.619090, acc: 67.97%] [G loss: 3.554438]\n",
      "epoch:3 step:3213 [D loss: 0.464927, acc: 78.12%] [G loss: 2.910534]\n",
      "epoch:3 step:3214 [D loss: 0.700623, acc: 64.84%] [G loss: 3.194027]\n",
      "epoch:3 step:3215 [D loss: 0.600584, acc: 67.19%] [G loss: 3.054086]\n",
      "epoch:3 step:3216 [D loss: 0.611361, acc: 67.19%] [G loss: 3.128291]\n",
      "epoch:3 step:3217 [D loss: 0.488541, acc: 71.88%] [G loss: 3.334250]\n",
      "epoch:3 step:3218 [D loss: 0.508288, acc: 72.66%] [G loss: 3.077372]\n",
      "epoch:3 step:3219 [D loss: 0.538169, acc: 68.75%] [G loss: 3.204626]\n",
      "epoch:3 step:3220 [D loss: 0.516245, acc: 74.22%] [G loss: 3.487074]\n",
      "epoch:3 step:3221 [D loss: 0.667909, acc: 63.28%] [G loss: 2.858767]\n",
      "epoch:3 step:3222 [D loss: 0.591240, acc: 67.97%] [G loss: 2.805606]\n",
      "epoch:3 step:3223 [D loss: 0.542813, acc: 73.44%] [G loss: 2.924155]\n",
      "epoch:3 step:3224 [D loss: 0.647106, acc: 62.50%] [G loss: 3.293470]\n",
      "epoch:3 step:3225 [D loss: 0.588070, acc: 67.97%] [G loss: 3.250029]\n",
      "epoch:3 step:3226 [D loss: 0.589048, acc: 71.09%] [G loss: 2.731200]\n",
      "epoch:3 step:3227 [D loss: 0.588988, acc: 67.97%] [G loss: 2.714270]\n",
      "epoch:3 step:3228 [D loss: 0.649216, acc: 67.19%] [G loss: 2.570564]\n",
      "epoch:3 step:3229 [D loss: 0.585000, acc: 67.19%] [G loss: 3.325556]\n",
      "epoch:3 step:3230 [D loss: 0.529416, acc: 69.53%] [G loss: 2.989396]\n",
      "epoch:3 step:3231 [D loss: 0.522840, acc: 74.22%] [G loss: 2.820805]\n",
      "epoch:3 step:3232 [D loss: 0.484362, acc: 78.12%] [G loss: 3.267202]\n",
      "epoch:3 step:3233 [D loss: 0.431736, acc: 79.69%] [G loss: 3.279945]\n",
      "epoch:3 step:3234 [D loss: 0.491698, acc: 76.56%] [G loss: 3.243542]\n",
      "epoch:3 step:3235 [D loss: 0.523076, acc: 75.00%] [G loss: 3.052121]\n",
      "epoch:3 step:3236 [D loss: 0.560007, acc: 70.31%] [G loss: 3.122667]\n",
      "epoch:3 step:3237 [D loss: 0.472061, acc: 78.12%] [G loss: 3.373200]\n",
      "epoch:3 step:3238 [D loss: 0.527061, acc: 75.00%] [G loss: 3.129840]\n",
      "epoch:3 step:3239 [D loss: 0.467252, acc: 74.22%] [G loss: 3.415601]\n",
      "epoch:3 step:3240 [D loss: 0.527557, acc: 72.66%] [G loss: 3.655523]\n",
      "epoch:3 step:3241 [D loss: 0.566742, acc: 70.31%] [G loss: 3.393965]\n",
      "epoch:3 step:3242 [D loss: 0.596763, acc: 71.09%] [G loss: 3.301529]\n",
      "epoch:3 step:3243 [D loss: 0.625497, acc: 64.84%] [G loss: 2.811866]\n",
      "epoch:3 step:3244 [D loss: 0.570497, acc: 69.53%] [G loss: 3.166948]\n",
      "epoch:3 step:3245 [D loss: 0.596596, acc: 67.97%] [G loss: 3.104047]\n",
      "epoch:3 step:3246 [D loss: 0.606971, acc: 69.53%] [G loss: 2.900574]\n",
      "epoch:3 step:3247 [D loss: 0.560108, acc: 71.09%] [G loss: 3.045720]\n",
      "epoch:3 step:3248 [D loss: 0.673041, acc: 65.62%] [G loss: 2.898125]\n",
      "epoch:3 step:3249 [D loss: 0.576214, acc: 70.31%] [G loss: 3.103142]\n",
      "epoch:3 step:3250 [D loss: 0.544386, acc: 73.44%] [G loss: 3.231181]\n",
      "epoch:3 step:3251 [D loss: 0.527099, acc: 72.66%] [G loss: 3.422589]\n",
      "epoch:3 step:3252 [D loss: 0.548081, acc: 71.88%] [G loss: 3.031408]\n",
      "epoch:3 step:3253 [D loss: 0.614709, acc: 66.41%] [G loss: 2.929209]\n",
      "epoch:3 step:3254 [D loss: 0.571477, acc: 71.88%] [G loss: 3.240535]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:3 step:3255 [D loss: 0.653679, acc: 63.28%] [G loss: 2.934776]\n",
      "epoch:3 step:3256 [D loss: 0.577841, acc: 70.31%] [G loss: 2.766958]\n",
      "epoch:3 step:3257 [D loss: 0.557143, acc: 68.75%] [G loss: 3.127638]\n",
      "epoch:3 step:3258 [D loss: 0.574825, acc: 73.44%] [G loss: 3.000009]\n",
      "epoch:3 step:3259 [D loss: 0.594716, acc: 67.19%] [G loss: 3.001068]\n",
      "epoch:3 step:3260 [D loss: 0.557941, acc: 71.88%] [G loss: 2.960627]\n",
      "epoch:3 step:3261 [D loss: 0.580403, acc: 70.31%] [G loss: 3.275430]\n",
      "epoch:3 step:3262 [D loss: 0.468340, acc: 78.91%] [G loss: 3.302976]\n",
      "epoch:3 step:3263 [D loss: 0.538559, acc: 75.78%] [G loss: 2.916867]\n",
      "epoch:3 step:3264 [D loss: 0.500536, acc: 76.56%] [G loss: 3.141686]\n",
      "epoch:3 step:3265 [D loss: 0.638951, acc: 67.97%] [G loss: 2.826785]\n",
      "epoch:3 step:3266 [D loss: 0.529293, acc: 72.66%] [G loss: 3.164203]\n",
      "epoch:3 step:3267 [D loss: 0.625854, acc: 62.50%] [G loss: 2.533361]\n",
      "epoch:3 step:3268 [D loss: 0.576838, acc: 69.53%] [G loss: 2.983976]\n",
      "epoch:3 step:3269 [D loss: 0.617530, acc: 65.62%] [G loss: 3.014508]\n",
      "epoch:3 step:3270 [D loss: 0.591698, acc: 67.19%] [G loss: 2.706783]\n",
      "epoch:3 step:3271 [D loss: 0.549919, acc: 71.09%] [G loss: 3.237414]\n",
      "epoch:3 step:3272 [D loss: 0.514892, acc: 71.88%] [G loss: 2.807570]\n",
      "epoch:3 step:3273 [D loss: 0.518865, acc: 76.56%] [G loss: 3.285436]\n",
      "epoch:3 step:3274 [D loss: 0.515641, acc: 76.56%] [G loss: 2.777357]\n",
      "epoch:3 step:3275 [D loss: 0.529863, acc: 71.09%] [G loss: 3.227960]\n",
      "epoch:3 step:3276 [D loss: 0.645944, acc: 62.50%] [G loss: 2.758986]\n",
      "epoch:3 step:3277 [D loss: 0.557170, acc: 69.53%] [G loss: 3.045147]\n",
      "epoch:3 step:3278 [D loss: 0.596864, acc: 67.97%] [G loss: 3.085833]\n",
      "epoch:3 step:3279 [D loss: 0.518190, acc: 75.00%] [G loss: 3.188256]\n",
      "epoch:3 step:3280 [D loss: 0.608898, acc: 73.44%] [G loss: 2.841656]\n",
      "epoch:3 step:3281 [D loss: 0.511779, acc: 75.00%] [G loss: 2.850921]\n",
      "epoch:3 step:3282 [D loss: 0.478437, acc: 77.34%] [G loss: 3.671359]\n",
      "epoch:3 step:3283 [D loss: 0.595638, acc: 67.19%] [G loss: 3.650554]\n",
      "epoch:3 step:3284 [D loss: 0.552756, acc: 74.22%] [G loss: 3.146918]\n",
      "epoch:3 step:3285 [D loss: 0.579543, acc: 69.53%] [G loss: 3.252915]\n",
      "epoch:3 step:3286 [D loss: 0.511824, acc: 73.44%] [G loss: 3.155047]\n",
      "epoch:3 step:3287 [D loss: 0.552810, acc: 71.09%] [G loss: 3.000547]\n",
      "epoch:3 step:3288 [D loss: 0.718284, acc: 57.81%] [G loss: 2.350014]\n",
      "epoch:3 step:3289 [D loss: 0.532301, acc: 75.00%] [G loss: 2.757821]\n",
      "epoch:3 step:3290 [D loss: 0.560770, acc: 71.88%] [G loss: 2.668175]\n",
      "epoch:3 step:3291 [D loss: 0.585322, acc: 68.75%] [G loss: 2.666687]\n",
      "epoch:3 step:3292 [D loss: 0.610386, acc: 61.72%] [G loss: 2.774392]\n",
      "epoch:3 step:3293 [D loss: 0.628931, acc: 66.41%] [G loss: 3.054056]\n",
      "epoch:3 step:3294 [D loss: 0.619996, acc: 64.06%] [G loss: 2.950042]\n",
      "epoch:3 step:3295 [D loss: 0.570950, acc: 71.88%] [G loss: 2.996473]\n",
      "epoch:3 step:3296 [D loss: 0.607747, acc: 68.75%] [G loss: 3.070464]\n",
      "epoch:3 step:3297 [D loss: 0.683890, acc: 64.06%] [G loss: 2.573212]\n",
      "epoch:3 step:3298 [D loss: 0.508349, acc: 78.12%] [G loss: 2.884286]\n",
      "epoch:3 step:3299 [D loss: 0.584195, acc: 73.44%] [G loss: 2.941086]\n",
      "epoch:3 step:3300 [D loss: 0.567636, acc: 68.75%] [G loss: 3.020188]\n",
      "epoch:3 step:3301 [D loss: 0.586586, acc: 66.41%] [G loss: 2.903294]\n",
      "epoch:3 step:3302 [D loss: 0.593980, acc: 65.62%] [G loss: 2.722652]\n",
      "epoch:3 step:3303 [D loss: 0.537013, acc: 71.88%] [G loss: 2.730122]\n",
      "epoch:3 step:3304 [D loss: 0.603088, acc: 69.53%] [G loss: 2.940447]\n",
      "epoch:3 step:3305 [D loss: 0.530688, acc: 73.44%] [G loss: 3.014051]\n",
      "epoch:3 step:3306 [D loss: 0.514921, acc: 74.22%] [G loss: 2.924733]\n",
      "epoch:3 step:3307 [D loss: 0.695048, acc: 60.94%] [G loss: 2.662595]\n",
      "epoch:3 step:3308 [D loss: 0.471875, acc: 82.03%] [G loss: 3.602541]\n",
      "epoch:3 step:3309 [D loss: 0.540345, acc: 73.44%] [G loss: 3.502281]\n",
      "epoch:3 step:3310 [D loss: 0.467554, acc: 77.34%] [G loss: 3.755270]\n",
      "epoch:3 step:3311 [D loss: 0.714709, acc: 60.16%] [G loss: 2.331972]\n",
      "epoch:3 step:3312 [D loss: 0.728297, acc: 54.69%] [G loss: 2.673834]\n",
      "epoch:3 step:3313 [D loss: 0.579089, acc: 69.53%] [G loss: 2.624939]\n",
      "epoch:3 step:3314 [D loss: 0.546007, acc: 72.66%] [G loss: 3.118098]\n",
      "epoch:3 step:3315 [D loss: 0.443461, acc: 80.47%] [G loss: 3.128016]\n",
      "epoch:3 step:3316 [D loss: 0.683923, acc: 63.28%] [G loss: 2.562201]\n",
      "epoch:3 step:3317 [D loss: 0.545364, acc: 73.44%] [G loss: 3.017804]\n",
      "epoch:3 step:3318 [D loss: 0.666727, acc: 61.72%] [G loss: 3.154746]\n",
      "epoch:3 step:3319 [D loss: 0.522503, acc: 73.44%] [G loss: 3.350460]\n",
      "epoch:3 step:3320 [D loss: 0.484141, acc: 70.31%] [G loss: 2.932035]\n",
      "epoch:3 step:3321 [D loss: 0.621030, acc: 67.97%] [G loss: 3.109536]\n",
      "epoch:3 step:3322 [D loss: 0.541370, acc: 78.91%] [G loss: 2.975447]\n",
      "epoch:3 step:3323 [D loss: 0.557769, acc: 71.09%] [G loss: 3.127130]\n",
      "epoch:3 step:3324 [D loss: 0.549369, acc: 73.44%] [G loss: 3.288189]\n",
      "epoch:3 step:3325 [D loss: 0.558566, acc: 74.22%] [G loss: 3.343636]\n",
      "epoch:3 step:3326 [D loss: 0.561325, acc: 72.66%] [G loss: 2.933527]\n",
      "epoch:3 step:3327 [D loss: 0.547975, acc: 74.22%] [G loss: 2.669029]\n",
      "epoch:3 step:3328 [D loss: 0.565076, acc: 69.53%] [G loss: 3.078846]\n",
      "epoch:3 step:3329 [D loss: 0.584654, acc: 68.75%] [G loss: 3.165565]\n",
      "epoch:3 step:3330 [D loss: 0.473899, acc: 78.91%] [G loss: 3.328377]\n",
      "epoch:3 step:3331 [D loss: 0.562404, acc: 67.97%] [G loss: 3.037512]\n",
      "epoch:3 step:3332 [D loss: 0.542245, acc: 75.78%] [G loss: 2.784696]\n",
      "epoch:3 step:3333 [D loss: 0.540710, acc: 75.78%] [G loss: 3.457998]\n",
      "epoch:3 step:3334 [D loss: 0.539656, acc: 75.78%] [G loss: 3.364330]\n",
      "epoch:3 step:3335 [D loss: 0.511507, acc: 73.44%] [G loss: 2.921350]\n",
      "epoch:3 step:3336 [D loss: 0.630757, acc: 69.53%] [G loss: 2.979596]\n",
      "epoch:3 step:3337 [D loss: 0.529670, acc: 73.44%] [G loss: 2.973261]\n",
      "epoch:3 step:3338 [D loss: 0.531725, acc: 74.22%] [G loss: 2.789530]\n",
      "epoch:3 step:3339 [D loss: 0.620434, acc: 64.06%] [G loss: 2.953661]\n",
      "epoch:3 step:3340 [D loss: 0.556318, acc: 72.66%] [G loss: 3.054601]\n",
      "epoch:3 step:3341 [D loss: 0.521996, acc: 71.88%] [G loss: 3.136089]\n",
      "epoch:3 step:3342 [D loss: 0.600363, acc: 67.19%] [G loss: 3.189266]\n",
      "epoch:3 step:3343 [D loss: 0.533015, acc: 75.00%] [G loss: 3.023959]\n",
      "epoch:3 step:3344 [D loss: 0.545249, acc: 71.88%] [G loss: 3.056798]\n",
      "epoch:3 step:3345 [D loss: 0.486465, acc: 82.03%] [G loss: 3.427337]\n",
      "epoch:3 step:3346 [D loss: 0.611216, acc: 71.88%] [G loss: 2.913512]\n",
      "epoch:3 step:3347 [D loss: 0.532000, acc: 71.88%] [G loss: 3.054567]\n",
      "epoch:3 step:3348 [D loss: 0.587539, acc: 66.41%] [G loss: 3.005520]\n",
      "epoch:3 step:3349 [D loss: 0.572947, acc: 71.09%] [G loss: 2.753716]\n",
      "epoch:3 step:3350 [D loss: 0.540547, acc: 71.09%] [G loss: 2.913403]\n",
      "epoch:3 step:3351 [D loss: 0.598871, acc: 66.41%] [G loss: 3.171184]\n",
      "epoch:3 step:3352 [D loss: 0.591388, acc: 69.53%] [G loss: 3.393797]\n",
      "epoch:3 step:3353 [D loss: 0.699663, acc: 63.28%] [G loss: 2.858531]\n",
      "epoch:3 step:3354 [D loss: 0.598242, acc: 67.97%] [G loss: 2.620363]\n",
      "epoch:3 step:3355 [D loss: 0.555727, acc: 71.88%] [G loss: 3.118094]\n",
      "epoch:3 step:3356 [D loss: 0.559844, acc: 69.53%] [G loss: 2.838617]\n",
      "epoch:3 step:3357 [D loss: 0.454400, acc: 78.12%] [G loss: 3.189776]\n",
      "epoch:3 step:3358 [D loss: 0.532105, acc: 73.44%] [G loss: 3.377715]\n",
      "epoch:3 step:3359 [D loss: 0.555787, acc: 72.66%] [G loss: 3.193203]\n",
      "epoch:3 step:3360 [D loss: 0.561246, acc: 71.09%] [G loss: 3.269208]\n",
      "epoch:3 step:3361 [D loss: 0.580919, acc: 71.09%] [G loss: 3.008488]\n",
      "epoch:3 step:3362 [D loss: 0.538950, acc: 70.31%] [G loss: 3.052289]\n",
      "epoch:3 step:3363 [D loss: 0.490177, acc: 75.00%] [G loss: 3.114027]\n",
      "epoch:3 step:3364 [D loss: 0.556647, acc: 71.88%] [G loss: 3.461547]\n",
      "epoch:3 step:3365 [D loss: 0.496798, acc: 75.00%] [G loss: 3.471966]\n",
      "epoch:3 step:3366 [D loss: 0.587371, acc: 71.09%] [G loss: 3.154850]\n",
      "epoch:3 step:3367 [D loss: 0.447009, acc: 82.03%] [G loss: 3.412486]\n",
      "epoch:3 step:3368 [D loss: 0.540360, acc: 78.12%] [G loss: 3.197439]\n",
      "epoch:3 step:3369 [D loss: 0.467329, acc: 74.22%] [G loss: 3.478366]\n",
      "epoch:3 step:3370 [D loss: 0.662253, acc: 58.59%] [G loss: 3.014921]\n",
      "epoch:3 step:3371 [D loss: 0.567411, acc: 69.53%] [G loss: 3.142472]\n",
      "epoch:3 step:3372 [D loss: 0.551172, acc: 76.56%] [G loss: 2.905487]\n",
      "epoch:3 step:3373 [D loss: 0.575169, acc: 69.53%] [G loss: 2.811192]\n",
      "epoch:3 step:3374 [D loss: 0.559157, acc: 69.53%] [G loss: 2.697756]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:3 step:3375 [D loss: 0.700088, acc: 62.50%] [G loss: 2.875850]\n",
      "epoch:3 step:3376 [D loss: 0.519360, acc: 74.22%] [G loss: 2.858952]\n",
      "epoch:3 step:3377 [D loss: 0.574159, acc: 71.09%] [G loss: 2.736717]\n",
      "epoch:3 step:3378 [D loss: 0.619053, acc: 65.62%] [G loss: 3.181034]\n",
      "epoch:3 step:3379 [D loss: 0.603780, acc: 69.53%] [G loss: 3.061589]\n",
      "epoch:3 step:3380 [D loss: 0.543533, acc: 73.44%] [G loss: 2.891998]\n",
      "epoch:3 step:3381 [D loss: 0.609251, acc: 71.88%] [G loss: 3.179248]\n",
      "epoch:3 step:3382 [D loss: 0.598509, acc: 67.97%] [G loss: 2.743443]\n",
      "epoch:3 step:3383 [D loss: 0.618722, acc: 68.75%] [G loss: 2.687590]\n",
      "epoch:3 step:3384 [D loss: 0.516044, acc: 78.12%] [G loss: 2.711437]\n",
      "epoch:3 step:3385 [D loss: 0.485260, acc: 75.78%] [G loss: 2.757705]\n",
      "epoch:3 step:3386 [D loss: 0.521421, acc: 75.00%] [G loss: 2.632280]\n",
      "epoch:3 step:3387 [D loss: 0.631904, acc: 67.97%] [G loss: 2.667473]\n",
      "epoch:3 step:3388 [D loss: 0.483886, acc: 78.12%] [G loss: 2.989548]\n",
      "epoch:3 step:3389 [D loss: 0.558132, acc: 70.31%] [G loss: 2.839612]\n",
      "epoch:3 step:3390 [D loss: 0.536505, acc: 74.22%] [G loss: 2.865973]\n",
      "epoch:3 step:3391 [D loss: 0.622904, acc: 65.62%] [G loss: 2.754650]\n",
      "epoch:3 step:3392 [D loss: 0.647009, acc: 64.06%] [G loss: 2.530190]\n",
      "epoch:3 step:3393 [D loss: 0.589532, acc: 68.75%] [G loss: 3.275106]\n",
      "epoch:3 step:3394 [D loss: 0.603011, acc: 64.06%] [G loss: 2.642978]\n",
      "epoch:3 step:3395 [D loss: 0.563187, acc: 71.09%] [G loss: 2.755205]\n",
      "epoch:3 step:3396 [D loss: 0.726539, acc: 57.03%] [G loss: 2.564627]\n",
      "epoch:3 step:3397 [D loss: 0.570924, acc: 70.31%] [G loss: 3.037226]\n",
      "epoch:3 step:3398 [D loss: 0.555991, acc: 68.75%] [G loss: 3.188161]\n",
      "epoch:3 step:3399 [D loss: 0.574329, acc: 70.31%] [G loss: 2.892133]\n",
      "epoch:3 step:3400 [D loss: 0.498990, acc: 72.66%] [G loss: 3.456389]\n",
      "##############\n",
      "[2.92476395 1.91608902 7.00561237 5.48877813 4.27961903 6.28090235\n",
      " 5.54048365 5.19684659 5.7356357  3.87658928]\n",
      "##########\n",
      "epoch:3 step:3401 [D loss: 0.542317, acc: 71.88%] [G loss: 2.764288]\n",
      "epoch:3 step:3402 [D loss: 0.611517, acc: 73.44%] [G loss: 2.776627]\n",
      "epoch:3 step:3403 [D loss: 0.571550, acc: 73.44%] [G loss: 2.989679]\n",
      "epoch:3 step:3404 [D loss: 0.609433, acc: 66.41%] [G loss: 2.740471]\n",
      "epoch:3 step:3405 [D loss: 0.603125, acc: 72.66%] [G loss: 2.946615]\n",
      "epoch:3 step:3406 [D loss: 0.503806, acc: 75.78%] [G loss: 2.725081]\n",
      "epoch:3 step:3407 [D loss: 0.661194, acc: 64.06%] [G loss: 2.975442]\n",
      "epoch:3 step:3408 [D loss: 0.548597, acc: 74.22%] [G loss: 2.702046]\n",
      "epoch:3 step:3409 [D loss: 0.635135, acc: 66.41%] [G loss: 2.574207]\n",
      "epoch:3 step:3410 [D loss: 0.628344, acc: 64.84%] [G loss: 2.985141]\n",
      "epoch:3 step:3411 [D loss: 0.625573, acc: 71.88%] [G loss: 3.150420]\n",
      "epoch:3 step:3412 [D loss: 0.600652, acc: 68.75%] [G loss: 2.810449]\n",
      "epoch:3 step:3413 [D loss: 0.622730, acc: 64.06%] [G loss: 3.031888]\n",
      "epoch:3 step:3414 [D loss: 0.510870, acc: 73.44%] [G loss: 3.161887]\n",
      "epoch:3 step:3415 [D loss: 0.606204, acc: 69.53%] [G loss: 2.509929]\n",
      "epoch:3 step:3416 [D loss: 0.527242, acc: 70.31%] [G loss: 2.973211]\n",
      "epoch:3 step:3417 [D loss: 0.501423, acc: 79.69%] [G loss: 3.402750]\n",
      "epoch:3 step:3418 [D loss: 0.633713, acc: 67.19%] [G loss: 2.958456]\n",
      "epoch:3 step:3419 [D loss: 0.443581, acc: 81.25%] [G loss: 2.913459]\n",
      "epoch:3 step:3420 [D loss: 0.489400, acc: 77.34%] [G loss: 3.122629]\n",
      "epoch:3 step:3421 [D loss: 0.621107, acc: 65.62%] [G loss: 2.635878]\n",
      "epoch:3 step:3422 [D loss: 0.546762, acc: 74.22%] [G loss: 3.060828]\n",
      "epoch:3 step:3423 [D loss: 0.500339, acc: 75.78%] [G loss: 3.196907]\n",
      "epoch:3 step:3424 [D loss: 0.604270, acc: 69.53%] [G loss: 2.848300]\n",
      "epoch:3 step:3425 [D loss: 0.537666, acc: 73.44%] [G loss: 3.007699]\n",
      "epoch:3 step:3426 [D loss: 0.600694, acc: 64.84%] [G loss: 2.842275]\n",
      "epoch:3 step:3427 [D loss: 0.592361, acc: 70.31%] [G loss: 2.913393]\n",
      "epoch:3 step:3428 [D loss: 0.673405, acc: 67.19%] [G loss: 2.627483]\n",
      "epoch:3 step:3429 [D loss: 0.631022, acc: 62.50%] [G loss: 2.670051]\n",
      "epoch:3 step:3430 [D loss: 0.544572, acc: 73.44%] [G loss: 2.754099]\n",
      "epoch:3 step:3431 [D loss: 0.549141, acc: 67.19%] [G loss: 2.637238]\n",
      "epoch:3 step:3432 [D loss: 0.581205, acc: 69.53%] [G loss: 2.913577]\n",
      "epoch:3 step:3433 [D loss: 0.535620, acc: 72.66%] [G loss: 2.644288]\n",
      "epoch:3 step:3434 [D loss: 0.627869, acc: 65.62%] [G loss: 3.044551]\n",
      "epoch:3 step:3435 [D loss: 0.634391, acc: 68.75%] [G loss: 2.618932]\n",
      "epoch:3 step:3436 [D loss: 0.572640, acc: 67.19%] [G loss: 2.934935]\n",
      "epoch:3 step:3437 [D loss: 0.567138, acc: 69.53%] [G loss: 2.810977]\n",
      "epoch:3 step:3438 [D loss: 0.560137, acc: 70.31%] [G loss: 2.626389]\n",
      "epoch:3 step:3439 [D loss: 0.535511, acc: 75.78%] [G loss: 3.130421]\n",
      "epoch:3 step:3440 [D loss: 0.682100, acc: 59.38%] [G loss: 3.203176]\n",
      "epoch:3 step:3441 [D loss: 0.537742, acc: 79.69%] [G loss: 3.115201]\n",
      "epoch:3 step:3442 [D loss: 0.531595, acc: 75.78%] [G loss: 3.188926]\n",
      "epoch:3 step:3443 [D loss: 0.552211, acc: 72.66%] [G loss: 2.995620]\n",
      "epoch:3 step:3444 [D loss: 0.608939, acc: 67.97%] [G loss: 3.022177]\n",
      "epoch:3 step:3445 [D loss: 0.438461, acc: 78.12%] [G loss: 3.111726]\n",
      "epoch:3 step:3446 [D loss: 0.499603, acc: 72.66%] [G loss: 2.955420]\n",
      "epoch:3 step:3447 [D loss: 0.608973, acc: 66.41%] [G loss: 3.030458]\n",
      "epoch:3 step:3448 [D loss: 0.570258, acc: 72.66%] [G loss: 3.153241]\n",
      "epoch:3 step:3449 [D loss: 0.498609, acc: 78.91%] [G loss: 2.969213]\n",
      "epoch:3 step:3450 [D loss: 0.512288, acc: 74.22%] [G loss: 3.400739]\n",
      "epoch:3 step:3451 [D loss: 0.568859, acc: 72.66%] [G loss: 3.298714]\n",
      "epoch:3 step:3452 [D loss: 0.581895, acc: 70.31%] [G loss: 3.361953]\n",
      "epoch:3 step:3453 [D loss: 0.453775, acc: 80.47%] [G loss: 3.363165]\n",
      "epoch:3 step:3454 [D loss: 0.504902, acc: 74.22%] [G loss: 3.398470]\n",
      "epoch:3 step:3455 [D loss: 0.654732, acc: 67.97%] [G loss: 2.960302]\n",
      "epoch:3 step:3456 [D loss: 0.577513, acc: 69.53%] [G loss: 2.907696]\n",
      "epoch:3 step:3457 [D loss: 0.547801, acc: 75.00%] [G loss: 2.747483]\n",
      "epoch:3 step:3458 [D loss: 0.557842, acc: 67.97%] [G loss: 3.189303]\n",
      "epoch:3 step:3459 [D loss: 0.509031, acc: 75.00%] [G loss: 3.457754]\n",
      "epoch:3 step:3460 [D loss: 0.525004, acc: 75.00%] [G loss: 3.336297]\n",
      "epoch:3 step:3461 [D loss: 0.555455, acc: 73.44%] [G loss: 3.398024]\n",
      "epoch:3 step:3462 [D loss: 0.539106, acc: 73.44%] [G loss: 2.946520]\n",
      "epoch:3 step:3463 [D loss: 0.606988, acc: 67.19%] [G loss: 2.868878]\n",
      "epoch:3 step:3464 [D loss: 0.568049, acc: 69.53%] [G loss: 2.978464]\n",
      "epoch:3 step:3465 [D loss: 0.521717, acc: 75.78%] [G loss: 3.335121]\n",
      "epoch:3 step:3466 [D loss: 0.597294, acc: 69.53%] [G loss: 3.069836]\n",
      "epoch:3 step:3467 [D loss: 0.524334, acc: 70.31%] [G loss: 2.847153]\n",
      "epoch:3 step:3468 [D loss: 0.538855, acc: 71.88%] [G loss: 2.989373]\n",
      "epoch:3 step:3469 [D loss: 0.552277, acc: 74.22%] [G loss: 2.904782]\n",
      "epoch:3 step:3470 [D loss: 0.520200, acc: 76.56%] [G loss: 2.970290]\n",
      "epoch:3 step:3471 [D loss: 0.461943, acc: 79.69%] [G loss: 3.188723]\n",
      "epoch:3 step:3472 [D loss: 0.514403, acc: 75.78%] [G loss: 3.243650]\n",
      "epoch:3 step:3473 [D loss: 0.576400, acc: 68.75%] [G loss: 3.042925]\n",
      "epoch:3 step:3474 [D loss: 0.515718, acc: 73.44%] [G loss: 3.290202]\n",
      "epoch:3 step:3475 [D loss: 0.512342, acc: 73.44%] [G loss: 3.404306]\n",
      "epoch:3 step:3476 [D loss: 0.552323, acc: 73.44%] [G loss: 3.394215]\n",
      "epoch:3 step:3477 [D loss: 0.634366, acc: 66.41%] [G loss: 3.073992]\n",
      "epoch:3 step:3478 [D loss: 0.586282, acc: 71.88%] [G loss: 2.672222]\n",
      "epoch:3 step:3479 [D loss: 0.539347, acc: 70.31%] [G loss: 3.072558]\n",
      "epoch:3 step:3480 [D loss: 0.570947, acc: 72.66%] [G loss: 2.707710]\n",
      "epoch:3 step:3481 [D loss: 0.546317, acc: 72.66%] [G loss: 3.081444]\n",
      "epoch:3 step:3482 [D loss: 0.570749, acc: 68.75%] [G loss: 3.201794]\n",
      "epoch:3 step:3483 [D loss: 0.664234, acc: 65.62%] [G loss: 3.037685]\n",
      "epoch:3 step:3484 [D loss: 0.560484, acc: 67.97%] [G loss: 3.044615]\n",
      "epoch:3 step:3485 [D loss: 0.569739, acc: 69.53%] [G loss: 3.027990]\n",
      "epoch:3 step:3486 [D loss: 0.682531, acc: 67.19%] [G loss: 2.868084]\n",
      "epoch:3 step:3487 [D loss: 0.545723, acc: 73.44%] [G loss: 2.780885]\n",
      "epoch:3 step:3488 [D loss: 0.494450, acc: 77.34%] [G loss: 3.015286]\n",
      "epoch:3 step:3489 [D loss: 0.483493, acc: 78.91%] [G loss: 3.234506]\n",
      "epoch:3 step:3490 [D loss: 0.539921, acc: 71.88%] [G loss: 3.028205]\n",
      "epoch:3 step:3491 [D loss: 0.479588, acc: 80.47%] [G loss: 3.259227]\n",
      "epoch:3 step:3492 [D loss: 0.468368, acc: 80.47%] [G loss: 3.168754]\n",
      "epoch:3 step:3493 [D loss: 0.529486, acc: 75.78%] [G loss: 2.964818]\n",
      "epoch:3 step:3494 [D loss: 0.604635, acc: 67.19%] [G loss: 3.515363]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:3 step:3495 [D loss: 0.532984, acc: 71.09%] [G loss: 2.783023]\n",
      "epoch:3 step:3496 [D loss: 0.607893, acc: 70.31%] [G loss: 2.696846]\n",
      "epoch:3 step:3497 [D loss: 0.547360, acc: 71.88%] [G loss: 2.691609]\n",
      "epoch:3 step:3498 [D loss: 0.520895, acc: 75.00%] [G loss: 3.094645]\n",
      "epoch:3 step:3499 [D loss: 0.610041, acc: 67.97%] [G loss: 2.928720]\n",
      "epoch:3 step:3500 [D loss: 0.520161, acc: 78.12%] [G loss: 2.913549]\n",
      "epoch:3 step:3501 [D loss: 0.628188, acc: 60.94%] [G loss: 3.054562]\n",
      "epoch:3 step:3502 [D loss: 0.650441, acc: 65.62%] [G loss: 3.009149]\n",
      "epoch:3 step:3503 [D loss: 0.515859, acc: 75.78%] [G loss: 2.696691]\n",
      "epoch:3 step:3504 [D loss: 0.526325, acc: 75.78%] [G loss: 3.370940]\n",
      "epoch:3 step:3505 [D loss: 0.515668, acc: 77.34%] [G loss: 3.298156]\n",
      "epoch:3 step:3506 [D loss: 0.486254, acc: 79.69%] [G loss: 3.000237]\n",
      "epoch:3 step:3507 [D loss: 0.586958, acc: 71.88%] [G loss: 3.022225]\n",
      "epoch:3 step:3508 [D loss: 0.612701, acc: 64.06%] [G loss: 2.958313]\n",
      "epoch:3 step:3509 [D loss: 0.587008, acc: 71.09%] [G loss: 3.008409]\n",
      "epoch:3 step:3510 [D loss: 0.478425, acc: 76.56%] [G loss: 3.141456]\n",
      "epoch:3 step:3511 [D loss: 0.596963, acc: 70.31%] [G loss: 2.863195]\n",
      "epoch:3 step:3512 [D loss: 0.594918, acc: 66.41%] [G loss: 2.904232]\n",
      "epoch:3 step:3513 [D loss: 0.538293, acc: 72.66%] [G loss: 3.147182]\n",
      "epoch:3 step:3514 [D loss: 0.583452, acc: 65.62%] [G loss: 2.860517]\n",
      "epoch:3 step:3515 [D loss: 0.565700, acc: 71.09%] [G loss: 2.888324]\n",
      "epoch:3 step:3516 [D loss: 0.622759, acc: 71.09%] [G loss: 3.001214]\n",
      "epoch:3 step:3517 [D loss: 0.541064, acc: 75.78%] [G loss: 3.075898]\n",
      "epoch:3 step:3518 [D loss: 0.452284, acc: 81.25%] [G loss: 3.691159]\n",
      "epoch:3 step:3519 [D loss: 0.552637, acc: 74.22%] [G loss: 3.340822]\n",
      "epoch:3 step:3520 [D loss: 0.504908, acc: 79.69%] [G loss: 3.304778]\n",
      "epoch:3 step:3521 [D loss: 0.641944, acc: 63.28%] [G loss: 2.763582]\n",
      "epoch:3 step:3522 [D loss: 0.580621, acc: 69.53%] [G loss: 2.808566]\n",
      "epoch:3 step:3523 [D loss: 0.566232, acc: 70.31%] [G loss: 2.821185]\n",
      "epoch:3 step:3524 [D loss: 0.615858, acc: 64.84%] [G loss: 2.811730]\n",
      "epoch:3 step:3525 [D loss: 0.618151, acc: 70.31%] [G loss: 2.852264]\n",
      "epoch:3 step:3526 [D loss: 0.502310, acc: 75.78%] [G loss: 3.084274]\n",
      "epoch:3 step:3527 [D loss: 0.652462, acc: 66.41%] [G loss: 2.813874]\n",
      "epoch:3 step:3528 [D loss: 0.623676, acc: 71.88%] [G loss: 2.697695]\n",
      "epoch:3 step:3529 [D loss: 0.550057, acc: 71.09%] [G loss: 2.958970]\n",
      "epoch:3 step:3530 [D loss: 0.568678, acc: 64.84%] [G loss: 2.770577]\n",
      "epoch:3 step:3531 [D loss: 0.589967, acc: 64.06%] [G loss: 2.502034]\n",
      "epoch:3 step:3532 [D loss: 0.551506, acc: 71.88%] [G loss: 2.993235]\n",
      "epoch:3 step:3533 [D loss: 0.573093, acc: 72.66%] [G loss: 3.262901]\n",
      "epoch:3 step:3534 [D loss: 0.563193, acc: 70.31%] [G loss: 3.000569]\n",
      "epoch:3 step:3535 [D loss: 0.510919, acc: 75.78%] [G loss: 3.166122]\n",
      "epoch:3 step:3536 [D loss: 0.533358, acc: 75.00%] [G loss: 2.914210]\n",
      "epoch:3 step:3537 [D loss: 0.570069, acc: 67.19%] [G loss: 2.915195]\n",
      "epoch:3 step:3538 [D loss: 0.520234, acc: 73.44%] [G loss: 2.759910]\n",
      "epoch:3 step:3539 [D loss: 0.537434, acc: 78.12%] [G loss: 2.907777]\n",
      "epoch:3 step:3540 [D loss: 0.588397, acc: 67.19%] [G loss: 2.649708]\n",
      "epoch:3 step:3541 [D loss: 0.545819, acc: 73.44%] [G loss: 3.191250]\n",
      "epoch:3 step:3542 [D loss: 0.611127, acc: 69.53%] [G loss: 2.930644]\n",
      "epoch:3 step:3543 [D loss: 0.508453, acc: 73.44%] [G loss: 3.420124]\n",
      "epoch:3 step:3544 [D loss: 0.535428, acc: 75.00%] [G loss: 3.349042]\n",
      "epoch:3 step:3545 [D loss: 0.562663, acc: 70.31%] [G loss: 3.386527]\n",
      "epoch:3 step:3546 [D loss: 0.660788, acc: 65.62%] [G loss: 2.710420]\n",
      "epoch:3 step:3547 [D loss: 0.544942, acc: 74.22%] [G loss: 3.392481]\n",
      "epoch:3 step:3548 [D loss: 0.519347, acc: 76.56%] [G loss: 3.294208]\n",
      "epoch:3 step:3549 [D loss: 0.588025, acc: 69.53%] [G loss: 2.914610]\n",
      "epoch:3 step:3550 [D loss: 0.511734, acc: 74.22%] [G loss: 2.905800]\n",
      "epoch:3 step:3551 [D loss: 0.585064, acc: 67.97%] [G loss: 2.701132]\n",
      "epoch:3 step:3552 [D loss: 0.508126, acc: 77.34%] [G loss: 3.167634]\n",
      "epoch:3 step:3553 [D loss: 0.647797, acc: 64.84%] [G loss: 3.004359]\n",
      "epoch:3 step:3554 [D loss: 0.616111, acc: 66.41%] [G loss: 3.095988]\n",
      "epoch:3 step:3555 [D loss: 0.556257, acc: 68.75%] [G loss: 2.987042]\n",
      "epoch:3 step:3556 [D loss: 0.623925, acc: 68.75%] [G loss: 2.986881]\n",
      "epoch:3 step:3557 [D loss: 0.521469, acc: 72.66%] [G loss: 3.399698]\n",
      "epoch:3 step:3558 [D loss: 0.519937, acc: 78.12%] [G loss: 2.992874]\n",
      "epoch:3 step:3559 [D loss: 0.514590, acc: 73.44%] [G loss: 2.887576]\n",
      "epoch:3 step:3560 [D loss: 0.570409, acc: 68.75%] [G loss: 3.090038]\n",
      "epoch:3 step:3561 [D loss: 0.530562, acc: 78.12%] [G loss: 3.284140]\n",
      "epoch:3 step:3562 [D loss: 0.545581, acc: 68.75%] [G loss: 3.055519]\n",
      "epoch:3 step:3563 [D loss: 0.490999, acc: 78.91%] [G loss: 3.082456]\n",
      "epoch:3 step:3564 [D loss: 0.572396, acc: 73.44%] [G loss: 3.151418]\n",
      "epoch:3 step:3565 [D loss: 0.532311, acc: 74.22%] [G loss: 3.376384]\n",
      "epoch:3 step:3566 [D loss: 0.575269, acc: 75.00%] [G loss: 3.041729]\n",
      "epoch:3 step:3567 [D loss: 0.559677, acc: 72.66%] [G loss: 3.024170]\n",
      "epoch:3 step:3568 [D loss: 0.549572, acc: 67.97%] [G loss: 3.211115]\n",
      "epoch:3 step:3569 [D loss: 0.540545, acc: 67.97%] [G loss: 3.149764]\n",
      "epoch:3 step:3570 [D loss: 0.533123, acc: 73.44%] [G loss: 2.805616]\n",
      "epoch:3 step:3571 [D loss: 0.572308, acc: 69.53%] [G loss: 2.626038]\n",
      "epoch:3 step:3572 [D loss: 0.523450, acc: 69.53%] [G loss: 2.946379]\n",
      "epoch:3 step:3573 [D loss: 0.612479, acc: 64.84%] [G loss: 2.663124]\n",
      "epoch:3 step:3574 [D loss: 0.593916, acc: 71.88%] [G loss: 2.897394]\n",
      "epoch:3 step:3575 [D loss: 0.597135, acc: 67.97%] [G loss: 2.996418]\n",
      "epoch:3 step:3576 [D loss: 0.596629, acc: 67.97%] [G loss: 2.772477]\n",
      "epoch:3 step:3577 [D loss: 0.638160, acc: 63.28%] [G loss: 2.648861]\n",
      "epoch:3 step:3578 [D loss: 0.592499, acc: 73.44%] [G loss: 3.029729]\n",
      "epoch:3 step:3579 [D loss: 0.642080, acc: 67.19%] [G loss: 2.904881]\n",
      "epoch:3 step:3580 [D loss: 0.624002, acc: 65.62%] [G loss: 3.027969]\n",
      "epoch:3 step:3581 [D loss: 0.546542, acc: 69.53%] [G loss: 2.581803]\n",
      "epoch:3 step:3582 [D loss: 0.581847, acc: 70.31%] [G loss: 2.940564]\n",
      "epoch:3 step:3583 [D loss: 0.562437, acc: 71.88%] [G loss: 2.745837]\n",
      "epoch:3 step:3584 [D loss: 0.526578, acc: 73.44%] [G loss: 2.867074]\n",
      "epoch:3 step:3585 [D loss: 0.523874, acc: 73.44%] [G loss: 3.404341]\n",
      "epoch:3 step:3586 [D loss: 0.519242, acc: 74.22%] [G loss: 3.368816]\n",
      "epoch:3 step:3587 [D loss: 0.508644, acc: 70.31%] [G loss: 2.697365]\n",
      "epoch:3 step:3588 [D loss: 0.509371, acc: 69.53%] [G loss: 3.249861]\n",
      "epoch:3 step:3589 [D loss: 0.674963, acc: 67.19%] [G loss: 2.926088]\n",
      "epoch:3 step:3590 [D loss: 0.554934, acc: 73.44%] [G loss: 3.077211]\n",
      "epoch:3 step:3591 [D loss: 0.526251, acc: 75.00%] [G loss: 3.436250]\n",
      "epoch:3 step:3592 [D loss: 0.550081, acc: 70.31%] [G loss: 3.299975]\n",
      "epoch:3 step:3593 [D loss: 0.590971, acc: 67.19%] [G loss: 3.078956]\n",
      "epoch:3 step:3594 [D loss: 0.552466, acc: 75.00%] [G loss: 3.112726]\n",
      "epoch:3 step:3595 [D loss: 0.587701, acc: 63.28%] [G loss: 2.968985]\n",
      "epoch:3 step:3596 [D loss: 0.699826, acc: 61.72%] [G loss: 2.642112]\n",
      "epoch:3 step:3597 [D loss: 0.557226, acc: 72.66%] [G loss: 3.064680]\n",
      "epoch:3 step:3598 [D loss: 0.581087, acc: 68.75%] [G loss: 2.964499]\n",
      "epoch:3 step:3599 [D loss: 0.607225, acc: 67.19%] [G loss: 2.836879]\n",
      "epoch:3 step:3600 [D loss: 0.674069, acc: 61.72%] [G loss: 2.779562]\n",
      "##############\n",
      "[2.97905254 1.74011398 7.03844231 5.41753114 4.31884587 6.27292276\n",
      " 5.30440849 5.29677991 5.58982487 3.75110751]\n",
      "##########\n",
      "epoch:3 step:3601 [D loss: 0.500390, acc: 78.91%] [G loss: 2.619278]\n",
      "epoch:3 step:3602 [D loss: 0.543170, acc: 72.66%] [G loss: 3.035758]\n",
      "epoch:3 step:3603 [D loss: 0.481216, acc: 78.12%] [G loss: 3.249743]\n",
      "epoch:3 step:3604 [D loss: 0.511541, acc: 75.00%] [G loss: 2.522729]\n",
      "epoch:3 step:3605 [D loss: 0.576617, acc: 67.19%] [G loss: 2.582665]\n",
      "epoch:3 step:3606 [D loss: 0.668200, acc: 70.31%] [G loss: 3.210638]\n",
      "epoch:3 step:3607 [D loss: 0.543415, acc: 73.44%] [G loss: 3.240404]\n",
      "epoch:3 step:3608 [D loss: 0.580101, acc: 69.53%] [G loss: 2.731556]\n",
      "epoch:3 step:3609 [D loss: 0.473884, acc: 75.78%] [G loss: 3.185205]\n",
      "epoch:3 step:3610 [D loss: 0.620116, acc: 70.31%] [G loss: 2.584524]\n",
      "epoch:3 step:3611 [D loss: 0.565915, acc: 67.19%] [G loss: 2.771828]\n",
      "epoch:3 step:3612 [D loss: 0.549946, acc: 71.09%] [G loss: 3.010953]\n",
      "epoch:3 step:3613 [D loss: 0.534923, acc: 74.22%] [G loss: 3.004599]\n",
      "epoch:3 step:3614 [D loss: 0.481029, acc: 78.12%] [G loss: 3.174546]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:3 step:3615 [D loss: 0.515719, acc: 77.34%] [G loss: 3.259447]\n",
      "epoch:3 step:3616 [D loss: 0.540559, acc: 69.53%] [G loss: 3.151195]\n",
      "epoch:3 step:3617 [D loss: 0.584015, acc: 67.97%] [G loss: 3.110368]\n",
      "epoch:3 step:3618 [D loss: 0.579405, acc: 69.53%] [G loss: 2.871876]\n",
      "epoch:3 step:3619 [D loss: 0.558641, acc: 75.00%] [G loss: 2.737724]\n",
      "epoch:3 step:3620 [D loss: 0.575829, acc: 70.31%] [G loss: 2.979455]\n",
      "epoch:3 step:3621 [D loss: 0.512977, acc: 69.53%] [G loss: 3.100957]\n",
      "epoch:3 step:3622 [D loss: 0.553583, acc: 75.78%] [G loss: 2.831270]\n",
      "epoch:3 step:3623 [D loss: 0.548792, acc: 76.56%] [G loss: 2.680427]\n",
      "epoch:3 step:3624 [D loss: 0.563127, acc: 75.78%] [G loss: 2.817904]\n",
      "epoch:3 step:3625 [D loss: 0.626742, acc: 71.09%] [G loss: 3.385087]\n",
      "epoch:3 step:3626 [D loss: 0.647214, acc: 67.19%] [G loss: 3.096735]\n",
      "epoch:3 step:3627 [D loss: 0.519102, acc: 76.56%] [G loss: 3.023619]\n",
      "epoch:3 step:3628 [D loss: 0.602862, acc: 66.41%] [G loss: 3.168233]\n",
      "epoch:3 step:3629 [D loss: 0.565455, acc: 73.44%] [G loss: 2.811398]\n",
      "epoch:3 step:3630 [D loss: 0.608036, acc: 76.56%] [G loss: 2.785866]\n",
      "epoch:3 step:3631 [D loss: 0.532260, acc: 75.78%] [G loss: 2.522139]\n",
      "epoch:3 step:3632 [D loss: 0.503491, acc: 76.56%] [G loss: 3.249557]\n",
      "epoch:3 step:3633 [D loss: 0.469182, acc: 75.00%] [G loss: 2.926149]\n",
      "epoch:3 step:3634 [D loss: 0.549865, acc: 75.00%] [G loss: 3.096450]\n",
      "epoch:3 step:3635 [D loss: 0.594346, acc: 70.31%] [G loss: 2.905585]\n",
      "epoch:3 step:3636 [D loss: 0.636700, acc: 73.44%] [G loss: 2.786417]\n",
      "epoch:3 step:3637 [D loss: 0.583762, acc: 73.44%] [G loss: 2.659974]\n",
      "epoch:3 step:3638 [D loss: 0.709669, acc: 60.16%] [G loss: 2.705358]\n",
      "epoch:3 step:3639 [D loss: 0.660436, acc: 60.94%] [G loss: 2.446404]\n",
      "epoch:3 step:3640 [D loss: 0.481844, acc: 80.47%] [G loss: 2.766558]\n",
      "epoch:3 step:3641 [D loss: 0.453062, acc: 80.47%] [G loss: 3.014750]\n",
      "epoch:3 step:3642 [D loss: 0.538363, acc: 76.56%] [G loss: 2.761872]\n",
      "epoch:3 step:3643 [D loss: 0.611135, acc: 67.19%] [G loss: 2.729654]\n",
      "epoch:3 step:3644 [D loss: 0.541668, acc: 75.78%] [G loss: 2.941085]\n",
      "epoch:3 step:3645 [D loss: 0.491415, acc: 77.34%] [G loss: 3.356414]\n",
      "epoch:3 step:3646 [D loss: 0.526167, acc: 75.00%] [G loss: 2.873492]\n",
      "epoch:3 step:3647 [D loss: 0.575897, acc: 78.12%] [G loss: 2.755568]\n",
      "epoch:3 step:3648 [D loss: 0.512778, acc: 76.56%] [G loss: 2.938544]\n",
      "epoch:3 step:3649 [D loss: 0.505943, acc: 74.22%] [G loss: 2.865556]\n",
      "epoch:3 step:3650 [D loss: 0.557883, acc: 72.66%] [G loss: 2.865475]\n",
      "epoch:3 step:3651 [D loss: 0.553176, acc: 67.97%] [G loss: 2.994433]\n",
      "epoch:3 step:3652 [D loss: 0.589199, acc: 71.88%] [G loss: 2.932185]\n",
      "epoch:3 step:3653 [D loss: 0.582642, acc: 66.41%] [G loss: 3.204598]\n",
      "epoch:3 step:3654 [D loss: 0.533563, acc: 75.78%] [G loss: 2.730358]\n",
      "epoch:3 step:3655 [D loss: 0.547811, acc: 70.31%] [G loss: 2.486347]\n",
      "epoch:3 step:3656 [D loss: 0.593388, acc: 75.78%] [G loss: 2.746609]\n",
      "epoch:3 step:3657 [D loss: 0.529438, acc: 75.78%] [G loss: 2.539476]\n",
      "epoch:3 step:3658 [D loss: 0.547121, acc: 70.31%] [G loss: 3.173855]\n",
      "epoch:3 step:3659 [D loss: 0.492426, acc: 78.91%] [G loss: 3.135071]\n",
      "epoch:3 step:3660 [D loss: 0.589877, acc: 67.19%] [G loss: 2.735086]\n",
      "epoch:3 step:3661 [D loss: 0.513753, acc: 71.88%] [G loss: 3.083163]\n",
      "epoch:3 step:3662 [D loss: 0.574772, acc: 69.53%] [G loss: 2.603055]\n",
      "epoch:3 step:3663 [D loss: 0.537222, acc: 74.22%] [G loss: 2.817338]\n",
      "epoch:3 step:3664 [D loss: 0.598611, acc: 63.28%] [G loss: 2.933659]\n",
      "epoch:3 step:3665 [D loss: 0.537618, acc: 75.78%] [G loss: 3.156637]\n",
      "epoch:3 step:3666 [D loss: 0.517061, acc: 69.53%] [G loss: 2.708013]\n",
      "epoch:3 step:3667 [D loss: 0.566258, acc: 71.88%] [G loss: 2.922469]\n",
      "epoch:3 step:3668 [D loss: 0.488483, acc: 78.91%] [G loss: 2.970897]\n",
      "epoch:3 step:3669 [D loss: 0.678768, acc: 60.94%] [G loss: 2.732146]\n",
      "epoch:3 step:3670 [D loss: 0.635290, acc: 65.62%] [G loss: 2.633383]\n",
      "epoch:3 step:3671 [D loss: 0.506131, acc: 73.44%] [G loss: 2.924957]\n",
      "epoch:3 step:3672 [D loss: 0.563103, acc: 71.09%] [G loss: 2.745005]\n",
      "epoch:3 step:3673 [D loss: 0.673734, acc: 63.28%] [G loss: 2.623645]\n",
      "epoch:3 step:3674 [D loss: 0.554344, acc: 68.75%] [G loss: 2.543908]\n",
      "epoch:3 step:3675 [D loss: 0.565115, acc: 75.00%] [G loss: 2.794284]\n",
      "epoch:3 step:3676 [D loss: 0.529928, acc: 74.22%] [G loss: 2.852214]\n",
      "epoch:3 step:3677 [D loss: 0.527349, acc: 75.78%] [G loss: 3.035380]\n",
      "epoch:3 step:3678 [D loss: 0.573212, acc: 71.09%] [G loss: 2.700291]\n",
      "epoch:3 step:3679 [D loss: 0.607077, acc: 65.62%] [G loss: 2.659256]\n",
      "epoch:3 step:3680 [D loss: 0.499401, acc: 72.66%] [G loss: 2.641138]\n",
      "epoch:3 step:3681 [D loss: 0.569579, acc: 70.31%] [G loss: 2.962373]\n",
      "epoch:3 step:3682 [D loss: 0.549935, acc: 73.44%] [G loss: 3.243662]\n",
      "epoch:3 step:3683 [D loss: 0.563702, acc: 73.44%] [G loss: 3.009821]\n",
      "epoch:3 step:3684 [D loss: 0.521799, acc: 72.66%] [G loss: 2.814886]\n",
      "epoch:3 step:3685 [D loss: 0.559327, acc: 71.09%] [G loss: 2.921383]\n",
      "epoch:3 step:3686 [D loss: 0.457184, acc: 79.69%] [G loss: 3.354926]\n",
      "epoch:3 step:3687 [D loss: 0.556648, acc: 67.19%] [G loss: 3.036067]\n",
      "epoch:3 step:3688 [D loss: 0.558023, acc: 68.75%] [G loss: 2.955690]\n",
      "epoch:3 step:3689 [D loss: 0.528786, acc: 73.44%] [G loss: 2.854534]\n",
      "epoch:3 step:3690 [D loss: 0.619799, acc: 64.06%] [G loss: 2.673901]\n",
      "epoch:3 step:3691 [D loss: 0.532078, acc: 71.09%] [G loss: 2.792871]\n",
      "epoch:3 step:3692 [D loss: 0.605844, acc: 69.53%] [G loss: 2.833227]\n",
      "epoch:3 step:3693 [D loss: 0.612484, acc: 68.75%] [G loss: 2.803273]\n",
      "epoch:3 step:3694 [D loss: 0.614634, acc: 64.84%] [G loss: 2.954827]\n",
      "epoch:3 step:3695 [D loss: 0.512059, acc: 73.44%] [G loss: 3.447872]\n",
      "epoch:3 step:3696 [D loss: 0.546527, acc: 72.66%] [G loss: 3.010416]\n",
      "epoch:3 step:3697 [D loss: 0.491713, acc: 77.34%] [G loss: 3.789423]\n",
      "epoch:3 step:3698 [D loss: 0.544014, acc: 74.22%] [G loss: 3.533423]\n",
      "epoch:3 step:3699 [D loss: 0.585064, acc: 74.22%] [G loss: 3.139673]\n",
      "epoch:3 step:3700 [D loss: 0.504558, acc: 78.12%] [G loss: 3.542656]\n",
      "epoch:3 step:3701 [D loss: 0.441215, acc: 79.69%] [G loss: 3.201777]\n",
      "epoch:3 step:3702 [D loss: 0.558831, acc: 66.41%] [G loss: 3.275854]\n",
      "epoch:3 step:3703 [D loss: 0.548328, acc: 70.31%] [G loss: 3.151757]\n",
      "epoch:3 step:3704 [D loss: 0.562439, acc: 69.53%] [G loss: 3.035266]\n",
      "epoch:3 step:3705 [D loss: 0.594019, acc: 68.75%] [G loss: 2.855094]\n",
      "epoch:3 step:3706 [D loss: 0.633198, acc: 63.28%] [G loss: 3.002057]\n",
      "epoch:3 step:3707 [D loss: 0.531237, acc: 76.56%] [G loss: 3.129291]\n",
      "epoch:3 step:3708 [D loss: 0.472657, acc: 72.66%] [G loss: 2.769978]\n",
      "epoch:3 step:3709 [D loss: 0.476471, acc: 75.78%] [G loss: 3.294279]\n",
      "epoch:3 step:3710 [D loss: 0.560027, acc: 71.88%] [G loss: 2.994504]\n",
      "epoch:3 step:3711 [D loss: 0.506867, acc: 76.56%] [G loss: 3.183775]\n",
      "epoch:3 step:3712 [D loss: 0.630824, acc: 67.97%] [G loss: 3.119777]\n",
      "epoch:3 step:3713 [D loss: 0.658561, acc: 64.06%] [G loss: 2.991772]\n",
      "epoch:3 step:3714 [D loss: 0.589781, acc: 66.41%] [G loss: 2.857580]\n",
      "epoch:3 step:3715 [D loss: 0.490346, acc: 80.47%] [G loss: 2.917477]\n",
      "epoch:3 step:3716 [D loss: 0.610613, acc: 68.75%] [G loss: 3.079329]\n",
      "epoch:3 step:3717 [D loss: 0.568908, acc: 70.31%] [G loss: 2.685342]\n",
      "epoch:3 step:3718 [D loss: 0.605820, acc: 74.22%] [G loss: 3.024058]\n",
      "epoch:3 step:3719 [D loss: 0.680437, acc: 62.50%] [G loss: 2.865211]\n",
      "epoch:3 step:3720 [D loss: 0.467133, acc: 82.03%] [G loss: 2.793188]\n",
      "epoch:3 step:3721 [D loss: 0.523690, acc: 77.34%] [G loss: 3.408466]\n",
      "epoch:3 step:3722 [D loss: 0.471220, acc: 77.34%] [G loss: 3.397285]\n",
      "epoch:3 step:3723 [D loss: 0.555137, acc: 67.19%] [G loss: 3.227176]\n",
      "epoch:3 step:3724 [D loss: 0.577845, acc: 69.53%] [G loss: 3.030598]\n",
      "epoch:3 step:3725 [D loss: 0.499372, acc: 74.22%] [G loss: 3.262321]\n",
      "epoch:3 step:3726 [D loss: 0.568973, acc: 69.53%] [G loss: 2.961858]\n",
      "epoch:3 step:3727 [D loss: 0.575724, acc: 71.88%] [G loss: 2.891707]\n",
      "epoch:3 step:3728 [D loss: 0.668494, acc: 63.28%] [G loss: 2.713607]\n",
      "epoch:3 step:3729 [D loss: 0.488645, acc: 80.47%] [G loss: 3.147901]\n",
      "epoch:3 step:3730 [D loss: 0.599627, acc: 66.41%] [G loss: 2.966808]\n",
      "epoch:3 step:3731 [D loss: 0.615921, acc: 69.53%] [G loss: 3.411917]\n",
      "epoch:3 step:3732 [D loss: 0.399632, acc: 79.69%] [G loss: 3.678123]\n",
      "epoch:3 step:3733 [D loss: 0.582941, acc: 66.41%] [G loss: 2.972086]\n",
      "epoch:3 step:3734 [D loss: 0.493110, acc: 78.91%] [G loss: 3.703686]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:3 step:3735 [D loss: 0.429617, acc: 79.69%] [G loss: 3.624320]\n",
      "epoch:3 step:3736 [D loss: 0.496670, acc: 78.12%] [G loss: 4.036829]\n",
      "epoch:3 step:3737 [D loss: 0.484263, acc: 79.69%] [G loss: 3.621361]\n",
      "epoch:3 step:3738 [D loss: 0.510977, acc: 75.78%] [G loss: 3.055675]\n",
      "epoch:3 step:3739 [D loss: 0.707523, acc: 67.19%] [G loss: 3.040438]\n",
      "epoch:3 step:3740 [D loss: 0.584248, acc: 71.88%] [G loss: 3.353728]\n",
      "epoch:3 step:3741 [D loss: 0.509976, acc: 75.00%] [G loss: 3.287139]\n",
      "epoch:3 step:3742 [D loss: 0.540937, acc: 69.53%] [G loss: 3.557421]\n",
      "epoch:3 step:3743 [D loss: 0.659698, acc: 68.75%] [G loss: 2.815902]\n",
      "epoch:3 step:3744 [D loss: 0.588366, acc: 71.09%] [G loss: 3.322179]\n",
      "epoch:3 step:3745 [D loss: 0.509103, acc: 75.00%] [G loss: 3.036385]\n",
      "epoch:3 step:3746 [D loss: 0.497531, acc: 71.09%] [G loss: 3.735749]\n",
      "epoch:3 step:3747 [D loss: 0.584559, acc: 70.31%] [G loss: 3.371398]\n",
      "epoch:3 step:3748 [D loss: 0.540123, acc: 71.88%] [G loss: 3.588007]\n",
      "epoch:4 step:3749 [D loss: 0.557221, acc: 75.00%] [G loss: 3.065475]\n",
      "epoch:4 step:3750 [D loss: 0.580186, acc: 71.88%] [G loss: 3.330944]\n",
      "epoch:4 step:3751 [D loss: 0.546606, acc: 70.31%] [G loss: 2.952069]\n",
      "epoch:4 step:3752 [D loss: 0.511511, acc: 71.09%] [G loss: 3.099089]\n",
      "epoch:4 step:3753 [D loss: 0.529381, acc: 72.66%] [G loss: 3.078962]\n",
      "epoch:4 step:3754 [D loss: 0.540899, acc: 71.88%] [G loss: 3.138339]\n",
      "epoch:4 step:3755 [D loss: 0.471221, acc: 81.25%] [G loss: 3.405073]\n",
      "epoch:4 step:3756 [D loss: 0.684962, acc: 67.19%] [G loss: 3.114471]\n",
      "epoch:4 step:3757 [D loss: 0.522905, acc: 75.78%] [G loss: 2.747692]\n",
      "epoch:4 step:3758 [D loss: 0.521628, acc: 76.56%] [G loss: 2.787664]\n",
      "epoch:4 step:3759 [D loss: 0.512909, acc: 77.34%] [G loss: 3.032431]\n",
      "epoch:4 step:3760 [D loss: 0.545531, acc: 71.09%] [G loss: 3.100779]\n",
      "epoch:4 step:3761 [D loss: 0.493560, acc: 72.66%] [G loss: 3.306974]\n",
      "epoch:4 step:3762 [D loss: 0.553703, acc: 74.22%] [G loss: 3.069974]\n",
      "epoch:4 step:3763 [D loss: 0.574608, acc: 69.53%] [G loss: 3.404870]\n",
      "epoch:4 step:3764 [D loss: 0.452219, acc: 79.69%] [G loss: 2.813251]\n",
      "epoch:4 step:3765 [D loss: 0.622337, acc: 74.22%] [G loss: 2.784066]\n",
      "epoch:4 step:3766 [D loss: 0.674454, acc: 64.06%] [G loss: 2.853251]\n",
      "epoch:4 step:3767 [D loss: 0.536325, acc: 69.53%] [G loss: 2.867167]\n",
      "epoch:4 step:3768 [D loss: 0.670634, acc: 64.84%] [G loss: 2.388582]\n",
      "epoch:4 step:3769 [D loss: 0.634574, acc: 64.06%] [G loss: 2.852012]\n",
      "epoch:4 step:3770 [D loss: 0.597356, acc: 74.22%] [G loss: 2.643185]\n",
      "epoch:4 step:3771 [D loss: 0.524156, acc: 72.66%] [G loss: 3.075912]\n",
      "epoch:4 step:3772 [D loss: 0.487146, acc: 76.56%] [G loss: 3.074261]\n",
      "epoch:4 step:3773 [D loss: 0.537882, acc: 72.66%] [G loss: 3.343721]\n",
      "epoch:4 step:3774 [D loss: 0.572833, acc: 71.09%] [G loss: 2.745435]\n",
      "epoch:4 step:3775 [D loss: 0.533125, acc: 70.31%] [G loss: 3.183890]\n",
      "epoch:4 step:3776 [D loss: 0.556218, acc: 73.44%] [G loss: 3.082931]\n",
      "epoch:4 step:3777 [D loss: 0.463203, acc: 76.56%] [G loss: 3.076156]\n",
      "epoch:4 step:3778 [D loss: 0.641436, acc: 61.72%] [G loss: 2.755090]\n",
      "epoch:4 step:3779 [D loss: 0.679760, acc: 60.94%] [G loss: 2.690227]\n",
      "epoch:4 step:3780 [D loss: 0.614591, acc: 65.62%] [G loss: 2.999062]\n",
      "epoch:4 step:3781 [D loss: 0.482735, acc: 73.44%] [G loss: 3.238539]\n",
      "epoch:4 step:3782 [D loss: 0.490990, acc: 74.22%] [G loss: 3.052553]\n",
      "epoch:4 step:3783 [D loss: 0.623309, acc: 63.28%] [G loss: 2.818166]\n",
      "epoch:4 step:3784 [D loss: 0.449182, acc: 77.34%] [G loss: 3.170665]\n",
      "epoch:4 step:3785 [D loss: 0.575499, acc: 71.88%] [G loss: 3.623575]\n",
      "epoch:4 step:3786 [D loss: 0.480234, acc: 80.47%] [G loss: 3.088480]\n",
      "epoch:4 step:3787 [D loss: 0.579971, acc: 67.97%] [G loss: 3.399041]\n",
      "epoch:4 step:3788 [D loss: 0.480643, acc: 79.69%] [G loss: 3.241671]\n",
      "epoch:4 step:3789 [D loss: 0.560143, acc: 71.09%] [G loss: 3.087879]\n",
      "epoch:4 step:3790 [D loss: 0.470464, acc: 75.00%] [G loss: 3.574051]\n",
      "epoch:4 step:3791 [D loss: 0.574068, acc: 70.31%] [G loss: 3.441663]\n",
      "epoch:4 step:3792 [D loss: 0.580511, acc: 67.97%] [G loss: 2.875543]\n",
      "epoch:4 step:3793 [D loss: 0.507408, acc: 72.66%] [G loss: 3.243057]\n",
      "epoch:4 step:3794 [D loss: 0.545120, acc: 72.66%] [G loss: 3.498595]\n",
      "epoch:4 step:3795 [D loss: 0.560526, acc: 73.44%] [G loss: 2.883091]\n",
      "epoch:4 step:3796 [D loss: 0.614824, acc: 70.31%] [G loss: 3.181445]\n",
      "epoch:4 step:3797 [D loss: 0.643055, acc: 67.97%] [G loss: 2.935039]\n",
      "epoch:4 step:3798 [D loss: 0.472953, acc: 71.88%] [G loss: 2.836014]\n",
      "epoch:4 step:3799 [D loss: 0.593453, acc: 68.75%] [G loss: 2.698370]\n",
      "epoch:4 step:3800 [D loss: 0.564202, acc: 72.66%] [G loss: 2.788336]\n",
      "##############\n",
      "[2.96544851 1.92845162 6.93755049 5.36958492 4.22360428 6.20109785\n",
      " 5.19181576 5.23432581 5.63710534 3.87757786]\n",
      "##########\n",
      "epoch:4 step:3801 [D loss: 0.632570, acc: 64.06%] [G loss: 3.472226]\n",
      "epoch:4 step:3802 [D loss: 0.528329, acc: 74.22%] [G loss: 3.155019]\n",
      "epoch:4 step:3803 [D loss: 0.541223, acc: 74.22%] [G loss: 3.416452]\n",
      "epoch:4 step:3804 [D loss: 0.518422, acc: 76.56%] [G loss: 2.920963]\n",
      "epoch:4 step:3805 [D loss: 0.629927, acc: 67.19%] [G loss: 2.960561]\n",
      "epoch:4 step:3806 [D loss: 0.522703, acc: 73.44%] [G loss: 3.127498]\n",
      "epoch:4 step:3807 [D loss: 0.593331, acc: 71.88%] [G loss: 3.432848]\n",
      "epoch:4 step:3808 [D loss: 0.604335, acc: 68.75%] [G loss: 2.987451]\n",
      "epoch:4 step:3809 [D loss: 0.590959, acc: 74.22%] [G loss: 3.377368]\n",
      "epoch:4 step:3810 [D loss: 0.602206, acc: 67.97%] [G loss: 2.924475]\n",
      "epoch:4 step:3811 [D loss: 0.557881, acc: 74.22%] [G loss: 2.654050]\n",
      "epoch:4 step:3812 [D loss: 0.573137, acc: 68.75%] [G loss: 2.783988]\n",
      "epoch:4 step:3813 [D loss: 0.557527, acc: 70.31%] [G loss: 2.882549]\n",
      "epoch:4 step:3814 [D loss: 0.535768, acc: 75.78%] [G loss: 2.989168]\n",
      "epoch:4 step:3815 [D loss: 0.555008, acc: 71.09%] [G loss: 2.877024]\n",
      "epoch:4 step:3816 [D loss: 0.596432, acc: 65.62%] [G loss: 2.661911]\n",
      "epoch:4 step:3817 [D loss: 0.627401, acc: 64.84%] [G loss: 2.635962]\n",
      "epoch:4 step:3818 [D loss: 0.523372, acc: 72.66%] [G loss: 3.383893]\n",
      "epoch:4 step:3819 [D loss: 0.558124, acc: 74.22%] [G loss: 2.772305]\n",
      "epoch:4 step:3820 [D loss: 0.569443, acc: 71.09%] [G loss: 3.243646]\n",
      "epoch:4 step:3821 [D loss: 0.607548, acc: 71.09%] [G loss: 2.867220]\n",
      "epoch:4 step:3822 [D loss: 0.623609, acc: 64.06%] [G loss: 2.771562]\n",
      "epoch:4 step:3823 [D loss: 0.592735, acc: 68.75%] [G loss: 2.988491]\n",
      "epoch:4 step:3824 [D loss: 0.558561, acc: 72.66%] [G loss: 3.427064]\n",
      "epoch:4 step:3825 [D loss: 0.495888, acc: 77.34%] [G loss: 3.674715]\n",
      "epoch:4 step:3826 [D loss: 0.535256, acc: 73.44%] [G loss: 3.112622]\n",
      "epoch:4 step:3827 [D loss: 0.591452, acc: 70.31%] [G loss: 2.960330]\n",
      "epoch:4 step:3828 [D loss: 0.587527, acc: 67.97%] [G loss: 2.801457]\n",
      "epoch:4 step:3829 [D loss: 0.578222, acc: 71.09%] [G loss: 2.556283]\n",
      "epoch:4 step:3830 [D loss: 0.591159, acc: 67.97%] [G loss: 2.694512]\n",
      "epoch:4 step:3831 [D loss: 0.632380, acc: 64.84%] [G loss: 3.079742]\n",
      "epoch:4 step:3832 [D loss: 0.593036, acc: 68.75%] [G loss: 2.610146]\n",
      "epoch:4 step:3833 [D loss: 0.589243, acc: 71.88%] [G loss: 2.816291]\n",
      "epoch:4 step:3834 [D loss: 0.568581, acc: 73.44%] [G loss: 2.661167]\n",
      "epoch:4 step:3835 [D loss: 0.528739, acc: 71.09%] [G loss: 2.902306]\n",
      "epoch:4 step:3836 [D loss: 0.537330, acc: 71.88%] [G loss: 2.888327]\n",
      "epoch:4 step:3837 [D loss: 0.531550, acc: 78.12%] [G loss: 3.007856]\n",
      "epoch:4 step:3838 [D loss: 0.594470, acc: 71.09%] [G loss: 2.791847]\n",
      "epoch:4 step:3839 [D loss: 0.621050, acc: 61.72%] [G loss: 2.738351]\n",
      "epoch:4 step:3840 [D loss: 0.547625, acc: 72.66%] [G loss: 2.862526]\n",
      "epoch:4 step:3841 [D loss: 0.556848, acc: 73.44%] [G loss: 2.683843]\n",
      "epoch:4 step:3842 [D loss: 0.601346, acc: 64.84%] [G loss: 2.821583]\n",
      "epoch:4 step:3843 [D loss: 0.548611, acc: 73.44%] [G loss: 2.773656]\n",
      "epoch:4 step:3844 [D loss: 0.505170, acc: 72.66%] [G loss: 3.041191]\n",
      "epoch:4 step:3845 [D loss: 0.537901, acc: 68.75%] [G loss: 2.758344]\n",
      "epoch:4 step:3846 [D loss: 0.544237, acc: 70.31%] [G loss: 2.939703]\n",
      "epoch:4 step:3847 [D loss: 0.558596, acc: 74.22%] [G loss: 3.032261]\n",
      "epoch:4 step:3848 [D loss: 0.525393, acc: 76.56%] [G loss: 3.004675]\n",
      "epoch:4 step:3849 [D loss: 0.560592, acc: 73.44%] [G loss: 2.832231]\n",
      "epoch:4 step:3850 [D loss: 0.550807, acc: 69.53%] [G loss: 2.624588]\n",
      "epoch:4 step:3851 [D loss: 0.512450, acc: 72.66%] [G loss: 2.962194]\n",
      "epoch:4 step:3852 [D loss: 0.528142, acc: 74.22%] [G loss: 3.010979]\n",
      "epoch:4 step:3853 [D loss: 0.668984, acc: 64.84%] [G loss: 2.971914]\n",
      "epoch:4 step:3854 [D loss: 0.648066, acc: 60.16%] [G loss: 2.951197]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:4 step:3855 [D loss: 0.573891, acc: 68.75%] [G loss: 2.737596]\n",
      "epoch:4 step:3856 [D loss: 0.554355, acc: 67.19%] [G loss: 2.765436]\n",
      "epoch:4 step:3857 [D loss: 0.628437, acc: 64.84%] [G loss: 2.724872]\n",
      "epoch:4 step:3858 [D loss: 0.585235, acc: 67.97%] [G loss: 3.173292]\n",
      "epoch:4 step:3859 [D loss: 0.516586, acc: 70.31%] [G loss: 3.048514]\n",
      "epoch:4 step:3860 [D loss: 0.556202, acc: 66.41%] [G loss: 2.751390]\n",
      "epoch:4 step:3861 [D loss: 0.671635, acc: 67.97%] [G loss: 2.541668]\n",
      "epoch:4 step:3862 [D loss: 0.584971, acc: 63.28%] [G loss: 2.634802]\n",
      "epoch:4 step:3863 [D loss: 0.593354, acc: 65.62%] [G loss: 2.518243]\n",
      "epoch:4 step:3864 [D loss: 0.595183, acc: 70.31%] [G loss: 3.356421]\n",
      "epoch:4 step:3865 [D loss: 0.524073, acc: 75.78%] [G loss: 3.139010]\n",
      "epoch:4 step:3866 [D loss: 0.533251, acc: 73.44%] [G loss: 3.238424]\n",
      "epoch:4 step:3867 [D loss: 0.441333, acc: 83.59%] [G loss: 3.317276]\n",
      "epoch:4 step:3868 [D loss: 0.607305, acc: 67.97%] [G loss: 3.086824]\n",
      "epoch:4 step:3869 [D loss: 0.531016, acc: 76.56%] [G loss: 2.810146]\n",
      "epoch:4 step:3870 [D loss: 0.517458, acc: 76.56%] [G loss: 3.198565]\n",
      "epoch:4 step:3871 [D loss: 0.647692, acc: 67.19%] [G loss: 2.774768]\n",
      "epoch:4 step:3872 [D loss: 0.724476, acc: 60.94%] [G loss: 2.903964]\n",
      "epoch:4 step:3873 [D loss: 0.610453, acc: 66.41%] [G loss: 2.741060]\n",
      "epoch:4 step:3874 [D loss: 0.526695, acc: 76.56%] [G loss: 3.085318]\n",
      "epoch:4 step:3875 [D loss: 0.547402, acc: 76.56%] [G loss: 2.836604]\n",
      "epoch:4 step:3876 [D loss: 0.527359, acc: 75.78%] [G loss: 3.160123]\n",
      "epoch:4 step:3877 [D loss: 0.581610, acc: 70.31%] [G loss: 2.625401]\n",
      "epoch:4 step:3878 [D loss: 0.516995, acc: 78.91%] [G loss: 3.043566]\n",
      "epoch:4 step:3879 [D loss: 0.508382, acc: 70.31%] [G loss: 2.896480]\n",
      "epoch:4 step:3880 [D loss: 0.567347, acc: 71.09%] [G loss: 2.991043]\n",
      "epoch:4 step:3881 [D loss: 0.655594, acc: 63.28%] [G loss: 3.001989]\n",
      "epoch:4 step:3882 [D loss: 0.618099, acc: 67.19%] [G loss: 2.760406]\n",
      "epoch:4 step:3883 [D loss: 0.491865, acc: 77.34%] [G loss: 2.986895]\n",
      "epoch:4 step:3884 [D loss: 0.463897, acc: 76.56%] [G loss: 2.890960]\n",
      "epoch:4 step:3885 [D loss: 0.565282, acc: 69.53%] [G loss: 2.748720]\n",
      "epoch:4 step:3886 [D loss: 0.604882, acc: 69.53%] [G loss: 3.169004]\n",
      "epoch:4 step:3887 [D loss: 0.539270, acc: 73.44%] [G loss: 2.826372]\n",
      "epoch:4 step:3888 [D loss: 0.646955, acc: 67.97%] [G loss: 2.780804]\n",
      "epoch:4 step:3889 [D loss: 0.509909, acc: 78.12%] [G loss: 3.051111]\n",
      "epoch:4 step:3890 [D loss: 0.610693, acc: 65.62%] [G loss: 2.737751]\n",
      "epoch:4 step:3891 [D loss: 0.576432, acc: 68.75%] [G loss: 2.954665]\n",
      "epoch:4 step:3892 [D loss: 0.476560, acc: 73.44%] [G loss: 2.925406]\n",
      "epoch:4 step:3893 [D loss: 0.502181, acc: 75.00%] [G loss: 3.310237]\n",
      "epoch:4 step:3894 [D loss: 0.632063, acc: 68.75%] [G loss: 2.759865]\n",
      "epoch:4 step:3895 [D loss: 0.627672, acc: 67.97%] [G loss: 2.857602]\n",
      "epoch:4 step:3896 [D loss: 0.549486, acc: 69.53%] [G loss: 2.884948]\n",
      "epoch:4 step:3897 [D loss: 0.637555, acc: 67.97%] [G loss: 3.132116]\n",
      "epoch:4 step:3898 [D loss: 0.595447, acc: 67.19%] [G loss: 3.107314]\n",
      "epoch:4 step:3899 [D loss: 0.628610, acc: 64.06%] [G loss: 3.173556]\n",
      "epoch:4 step:3900 [D loss: 0.496980, acc: 79.69%] [G loss: 3.389414]\n",
      "epoch:4 step:3901 [D loss: 0.620937, acc: 69.53%] [G loss: 2.637242]\n",
      "epoch:4 step:3902 [D loss: 0.631592, acc: 62.50%] [G loss: 2.966982]\n",
      "epoch:4 step:3903 [D loss: 0.552149, acc: 70.31%] [G loss: 2.799006]\n",
      "epoch:4 step:3904 [D loss: 0.583846, acc: 70.31%] [G loss: 2.671607]\n",
      "epoch:4 step:3905 [D loss: 0.578257, acc: 64.84%] [G loss: 2.803659]\n",
      "epoch:4 step:3906 [D loss: 0.658589, acc: 65.62%] [G loss: 2.796580]\n",
      "epoch:4 step:3907 [D loss: 0.545606, acc: 71.88%] [G loss: 3.224234]\n",
      "epoch:4 step:3908 [D loss: 0.662880, acc: 62.50%] [G loss: 2.302671]\n",
      "epoch:4 step:3909 [D loss: 0.648309, acc: 63.28%] [G loss: 2.384755]\n",
      "epoch:4 step:3910 [D loss: 0.550434, acc: 72.66%] [G loss: 3.113112]\n",
      "epoch:4 step:3911 [D loss: 0.547262, acc: 75.00%] [G loss: 2.925265]\n",
      "epoch:4 step:3912 [D loss: 0.556935, acc: 70.31%] [G loss: 2.715232]\n",
      "epoch:4 step:3913 [D loss: 0.628718, acc: 66.41%] [G loss: 2.945654]\n",
      "epoch:4 step:3914 [D loss: 0.567252, acc: 71.09%] [G loss: 2.891814]\n",
      "epoch:4 step:3915 [D loss: 0.593830, acc: 68.75%] [G loss: 2.685741]\n",
      "epoch:4 step:3916 [D loss: 0.562614, acc: 70.31%] [G loss: 3.332429]\n",
      "epoch:4 step:3917 [D loss: 0.620628, acc: 70.31%] [G loss: 2.663619]\n",
      "epoch:4 step:3918 [D loss: 0.561462, acc: 67.97%] [G loss: 2.850762]\n",
      "epoch:4 step:3919 [D loss: 0.595025, acc: 66.41%] [G loss: 2.907468]\n",
      "epoch:4 step:3920 [D loss: 0.555616, acc: 67.97%] [G loss: 2.759985]\n",
      "epoch:4 step:3921 [D loss: 0.534848, acc: 75.78%] [G loss: 2.855468]\n",
      "epoch:4 step:3922 [D loss: 0.592699, acc: 68.75%] [G loss: 2.693908]\n",
      "epoch:4 step:3923 [D loss: 0.660753, acc: 64.84%] [G loss: 2.577324]\n",
      "epoch:4 step:3924 [D loss: 0.616529, acc: 64.06%] [G loss: 2.426935]\n",
      "epoch:4 step:3925 [D loss: 0.600423, acc: 68.75%] [G loss: 2.634335]\n",
      "epoch:4 step:3926 [D loss: 0.563110, acc: 72.66%] [G loss: 2.841865]\n",
      "epoch:4 step:3927 [D loss: 0.589570, acc: 67.97%] [G loss: 2.695207]\n",
      "epoch:4 step:3928 [D loss: 0.547078, acc: 71.09%] [G loss: 2.650693]\n",
      "epoch:4 step:3929 [D loss: 0.602434, acc: 71.09%] [G loss: 2.805601]\n",
      "epoch:4 step:3930 [D loss: 0.674190, acc: 63.28%] [G loss: 2.642724]\n",
      "epoch:4 step:3931 [D loss: 0.539890, acc: 72.66%] [G loss: 2.902579]\n",
      "epoch:4 step:3932 [D loss: 0.575084, acc: 70.31%] [G loss: 2.981099]\n",
      "epoch:4 step:3933 [D loss: 0.643418, acc: 67.19%] [G loss: 2.785667]\n",
      "epoch:4 step:3934 [D loss: 0.560081, acc: 67.97%] [G loss: 2.742730]\n",
      "epoch:4 step:3935 [D loss: 0.566764, acc: 67.19%] [G loss: 2.523621]\n",
      "epoch:4 step:3936 [D loss: 0.537354, acc: 74.22%] [G loss: 2.756731]\n",
      "epoch:4 step:3937 [D loss: 0.573503, acc: 71.09%] [G loss: 2.832901]\n",
      "epoch:4 step:3938 [D loss: 0.617406, acc: 69.53%] [G loss: 2.996012]\n",
      "epoch:4 step:3939 [D loss: 0.502288, acc: 75.00%] [G loss: 2.717281]\n",
      "epoch:4 step:3940 [D loss: 0.535717, acc: 71.88%] [G loss: 2.942187]\n",
      "epoch:4 step:3941 [D loss: 0.569692, acc: 74.22%] [G loss: 2.813170]\n",
      "epoch:4 step:3942 [D loss: 0.540243, acc: 76.56%] [G loss: 3.311403]\n",
      "epoch:4 step:3943 [D loss: 0.606989, acc: 68.75%] [G loss: 2.792223]\n",
      "epoch:4 step:3944 [D loss: 0.620246, acc: 67.19%] [G loss: 2.600444]\n",
      "epoch:4 step:3945 [D loss: 0.586845, acc: 66.41%] [G loss: 2.603689]\n",
      "epoch:4 step:3946 [D loss: 0.536614, acc: 77.34%] [G loss: 2.717408]\n",
      "epoch:4 step:3947 [D loss: 0.643439, acc: 65.62%] [G loss: 2.472229]\n",
      "epoch:4 step:3948 [D loss: 0.697095, acc: 66.41%] [G loss: 2.394343]\n",
      "epoch:4 step:3949 [D loss: 0.469568, acc: 79.69%] [G loss: 2.703956]\n",
      "epoch:4 step:3950 [D loss: 0.744733, acc: 60.16%] [G loss: 2.757543]\n",
      "epoch:4 step:3951 [D loss: 0.578757, acc: 71.09%] [G loss: 2.758441]\n",
      "epoch:4 step:3952 [D loss: 0.581604, acc: 73.44%] [G loss: 2.728000]\n",
      "epoch:4 step:3953 [D loss: 0.579750, acc: 71.09%] [G loss: 2.509277]\n",
      "epoch:4 step:3954 [D loss: 0.555686, acc: 67.97%] [G loss: 2.786123]\n",
      "epoch:4 step:3955 [D loss: 0.539369, acc: 77.34%] [G loss: 2.988078]\n",
      "epoch:4 step:3956 [D loss: 0.542038, acc: 74.22%] [G loss: 3.212269]\n",
      "epoch:4 step:3957 [D loss: 0.583894, acc: 70.31%] [G loss: 2.890692]\n",
      "epoch:4 step:3958 [D loss: 0.594754, acc: 64.84%] [G loss: 2.946803]\n",
      "epoch:4 step:3959 [D loss: 0.622718, acc: 64.84%] [G loss: 2.770529]\n",
      "epoch:4 step:3960 [D loss: 0.548368, acc: 72.66%] [G loss: 2.917002]\n",
      "epoch:4 step:3961 [D loss: 0.547468, acc: 72.66%] [G loss: 2.747015]\n",
      "epoch:4 step:3962 [D loss: 0.588888, acc: 69.53%] [G loss: 2.579410]\n",
      "epoch:4 step:3963 [D loss: 0.570664, acc: 72.66%] [G loss: 2.880062]\n",
      "epoch:4 step:3964 [D loss: 0.582153, acc: 69.53%] [G loss: 2.805486]\n",
      "epoch:4 step:3965 [D loss: 0.469438, acc: 75.00%] [G loss: 2.989673]\n",
      "epoch:4 step:3966 [D loss: 0.515135, acc: 76.56%] [G loss: 2.961949]\n",
      "epoch:4 step:3967 [D loss: 0.572270, acc: 70.31%] [G loss: 2.732445]\n",
      "epoch:4 step:3968 [D loss: 0.572410, acc: 71.88%] [G loss: 2.918966]\n",
      "epoch:4 step:3969 [D loss: 0.657316, acc: 61.72%] [G loss: 2.908691]\n",
      "epoch:4 step:3970 [D loss: 0.543432, acc: 69.53%] [G loss: 3.088973]\n",
      "epoch:4 step:3971 [D loss: 0.510772, acc: 72.66%] [G loss: 2.581303]\n",
      "epoch:4 step:3972 [D loss: 0.596448, acc: 69.53%] [G loss: 2.904177]\n",
      "epoch:4 step:3973 [D loss: 0.659264, acc: 60.94%] [G loss: 2.840245]\n",
      "epoch:4 step:3974 [D loss: 0.625180, acc: 73.44%] [G loss: 2.477360]\n",
      "epoch:4 step:3975 [D loss: 0.593783, acc: 66.41%] [G loss: 2.672378]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:4 step:3976 [D loss: 0.557703, acc: 69.53%] [G loss: 2.662187]\n",
      "epoch:4 step:3977 [D loss: 0.623563, acc: 64.84%] [G loss: 3.008790]\n",
      "epoch:4 step:3978 [D loss: 0.575160, acc: 68.75%] [G loss: 3.212889]\n",
      "epoch:4 step:3979 [D loss: 0.555660, acc: 71.88%] [G loss: 3.192001]\n",
      "epoch:4 step:3980 [D loss: 0.439902, acc: 78.91%] [G loss: 3.145676]\n",
      "epoch:4 step:3981 [D loss: 0.636153, acc: 67.19%] [G loss: 3.007939]\n",
      "epoch:4 step:3982 [D loss: 0.604763, acc: 67.97%] [G loss: 2.705711]\n",
      "epoch:4 step:3983 [D loss: 0.558156, acc: 71.88%] [G loss: 2.664624]\n",
      "epoch:4 step:3984 [D loss: 0.452967, acc: 78.91%] [G loss: 3.141840]\n",
      "epoch:4 step:3985 [D loss: 0.508505, acc: 74.22%] [G loss: 2.618808]\n",
      "epoch:4 step:3986 [D loss: 0.518389, acc: 77.34%] [G loss: 2.939660]\n",
      "epoch:4 step:3987 [D loss: 0.528486, acc: 75.78%] [G loss: 3.015676]\n",
      "epoch:4 step:3988 [D loss: 0.574340, acc: 71.09%] [G loss: 2.793646]\n",
      "epoch:4 step:3989 [D loss: 0.598895, acc: 65.62%] [G loss: 2.726800]\n",
      "epoch:4 step:3990 [D loss: 0.573830, acc: 69.53%] [G loss: 3.207224]\n",
      "epoch:4 step:3991 [D loss: 0.633760, acc: 67.97%] [G loss: 2.990543]\n",
      "epoch:4 step:3992 [D loss: 0.506924, acc: 73.44%] [G loss: 2.790958]\n",
      "epoch:4 step:3993 [D loss: 0.506903, acc: 76.56%] [G loss: 2.724630]\n",
      "epoch:4 step:3994 [D loss: 0.592461, acc: 67.97%] [G loss: 2.788032]\n",
      "epoch:4 step:3995 [D loss: 0.530498, acc: 72.66%] [G loss: 2.576407]\n",
      "epoch:4 step:3996 [D loss: 0.601391, acc: 69.53%] [G loss: 2.478025]\n",
      "epoch:4 step:3997 [D loss: 0.640038, acc: 62.50%] [G loss: 2.889303]\n",
      "epoch:4 step:3998 [D loss: 0.591375, acc: 68.75%] [G loss: 2.716088]\n",
      "epoch:4 step:3999 [D loss: 0.662191, acc: 69.53%] [G loss: 2.328074]\n",
      "epoch:4 step:4000 [D loss: 0.580473, acc: 66.41%] [G loss: 2.394584]\n",
      "##############\n",
      "[2.86910097 1.61969609 7.13030583 5.41657799 4.2919793  6.14499353\n",
      " 5.3245478  5.21153369 5.51609575 3.91238024]\n",
      "##########\n",
      "epoch:4 step:4001 [D loss: 0.573373, acc: 68.75%] [G loss: 2.680358]\n",
      "epoch:4 step:4002 [D loss: 0.510534, acc: 71.88%] [G loss: 2.982942]\n",
      "epoch:4 step:4003 [D loss: 0.596893, acc: 67.19%] [G loss: 2.876381]\n",
      "epoch:4 step:4004 [D loss: 0.609215, acc: 67.19%] [G loss: 2.771394]\n",
      "epoch:4 step:4005 [D loss: 0.615444, acc: 65.62%] [G loss: 2.807306]\n",
      "epoch:4 step:4006 [D loss: 0.567235, acc: 71.88%] [G loss: 2.802916]\n",
      "epoch:4 step:4007 [D loss: 0.556133, acc: 77.34%] [G loss: 3.100097]\n",
      "epoch:4 step:4008 [D loss: 0.560079, acc: 75.00%] [G loss: 2.905619]\n",
      "epoch:4 step:4009 [D loss: 0.515702, acc: 74.22%] [G loss: 2.996550]\n",
      "epoch:4 step:4010 [D loss: 0.617034, acc: 64.84%] [G loss: 3.010951]\n",
      "epoch:4 step:4011 [D loss: 0.658416, acc: 65.62%] [G loss: 2.467397]\n",
      "epoch:4 step:4012 [D loss: 0.535094, acc: 72.66%] [G loss: 2.839742]\n",
      "epoch:4 step:4013 [D loss: 0.562011, acc: 70.31%] [G loss: 2.781168]\n",
      "epoch:4 step:4014 [D loss: 0.636468, acc: 68.75%] [G loss: 2.947542]\n",
      "epoch:4 step:4015 [D loss: 0.685767, acc: 57.03%] [G loss: 2.727069]\n",
      "epoch:4 step:4016 [D loss: 0.548712, acc: 69.53%] [G loss: 2.857834]\n",
      "epoch:4 step:4017 [D loss: 0.611190, acc: 67.97%] [G loss: 2.471416]\n",
      "epoch:4 step:4018 [D loss: 0.569194, acc: 71.09%] [G loss: 2.423461]\n",
      "epoch:4 step:4019 [D loss: 0.619311, acc: 67.97%] [G loss: 2.566099]\n",
      "epoch:4 step:4020 [D loss: 0.599164, acc: 68.75%] [G loss: 2.675653]\n",
      "epoch:4 step:4021 [D loss: 0.536455, acc: 71.88%] [G loss: 2.777853]\n",
      "epoch:4 step:4022 [D loss: 0.623969, acc: 68.75%] [G loss: 2.702361]\n",
      "epoch:4 step:4023 [D loss: 0.634482, acc: 61.72%] [G loss: 2.532582]\n",
      "epoch:4 step:4024 [D loss: 0.540179, acc: 71.88%] [G loss: 2.634202]\n",
      "epoch:4 step:4025 [D loss: 0.736063, acc: 55.47%] [G loss: 2.171303]\n",
      "epoch:4 step:4026 [D loss: 0.640767, acc: 64.06%] [G loss: 2.327360]\n",
      "epoch:4 step:4027 [D loss: 0.579563, acc: 72.66%] [G loss: 2.758174]\n",
      "epoch:4 step:4028 [D loss: 0.655654, acc: 64.06%] [G loss: 2.751976]\n",
      "epoch:4 step:4029 [D loss: 0.588679, acc: 71.09%] [G loss: 2.381437]\n",
      "epoch:4 step:4030 [D loss: 0.594747, acc: 70.31%] [G loss: 2.942197]\n",
      "epoch:4 step:4031 [D loss: 0.537118, acc: 75.78%] [G loss: 2.542571]\n",
      "epoch:4 step:4032 [D loss: 0.552770, acc: 71.88%] [G loss: 2.804827]\n",
      "epoch:4 step:4033 [D loss: 0.654532, acc: 55.47%] [G loss: 2.863114]\n",
      "epoch:4 step:4034 [D loss: 0.520807, acc: 74.22%] [G loss: 3.055650]\n",
      "epoch:4 step:4035 [D loss: 0.588120, acc: 66.41%] [G loss: 2.641224]\n",
      "epoch:4 step:4036 [D loss: 0.531205, acc: 68.75%] [G loss: 2.893960]\n",
      "epoch:4 step:4037 [D loss: 0.641211, acc: 65.62%] [G loss: 2.816701]\n",
      "epoch:4 step:4038 [D loss: 0.555580, acc: 70.31%] [G loss: 2.629223]\n",
      "epoch:4 step:4039 [D loss: 0.604389, acc: 66.41%] [G loss: 2.778509]\n",
      "epoch:4 step:4040 [D loss: 0.577542, acc: 72.66%] [G loss: 2.670155]\n",
      "epoch:4 step:4041 [D loss: 0.621778, acc: 67.97%] [G loss: 2.754864]\n",
      "epoch:4 step:4042 [D loss: 0.586000, acc: 71.88%] [G loss: 2.718372]\n",
      "epoch:4 step:4043 [D loss: 0.540452, acc: 74.22%] [G loss: 2.733792]\n",
      "epoch:4 step:4044 [D loss: 0.546436, acc: 75.78%] [G loss: 2.805203]\n",
      "epoch:4 step:4045 [D loss: 0.525997, acc: 74.22%] [G loss: 2.928663]\n",
      "epoch:4 step:4046 [D loss: 0.538029, acc: 76.56%] [G loss: 3.327575]\n",
      "epoch:4 step:4047 [D loss: 0.622100, acc: 71.88%] [G loss: 2.945707]\n",
      "epoch:4 step:4048 [D loss: 0.608454, acc: 69.53%] [G loss: 2.901106]\n",
      "epoch:4 step:4049 [D loss: 0.625631, acc: 72.66%] [G loss: 2.407489]\n",
      "epoch:4 step:4050 [D loss: 0.582739, acc: 71.88%] [G loss: 2.632539]\n",
      "epoch:4 step:4051 [D loss: 0.557704, acc: 71.88%] [G loss: 2.933438]\n",
      "epoch:4 step:4052 [D loss: 0.552444, acc: 75.00%] [G loss: 2.926713]\n",
      "epoch:4 step:4053 [D loss: 0.587383, acc: 68.75%] [G loss: 2.859805]\n",
      "epoch:4 step:4054 [D loss: 0.639758, acc: 67.97%] [G loss: 2.925229]\n",
      "epoch:4 step:4055 [D loss: 0.553397, acc: 73.44%] [G loss: 3.033950]\n",
      "epoch:4 step:4056 [D loss: 0.600372, acc: 71.88%] [G loss: 2.699013]\n",
      "epoch:4 step:4057 [D loss: 0.545761, acc: 69.53%] [G loss: 2.788176]\n",
      "epoch:4 step:4058 [D loss: 0.525307, acc: 74.22%] [G loss: 2.839930]\n",
      "epoch:4 step:4059 [D loss: 0.587984, acc: 67.19%] [G loss: 2.836792]\n",
      "epoch:4 step:4060 [D loss: 0.502138, acc: 74.22%] [G loss: 3.382131]\n",
      "epoch:4 step:4061 [D loss: 0.543448, acc: 72.66%] [G loss: 3.250921]\n",
      "epoch:4 step:4062 [D loss: 0.429373, acc: 82.81%] [G loss: 3.683376]\n",
      "epoch:4 step:4063 [D loss: 0.454803, acc: 79.69%] [G loss: 3.788323]\n",
      "epoch:4 step:4064 [D loss: 0.733807, acc: 53.91%] [G loss: 2.278138]\n",
      "epoch:4 step:4065 [D loss: 0.628231, acc: 67.97%] [G loss: 2.496803]\n",
      "epoch:4 step:4066 [D loss: 0.534314, acc: 75.78%] [G loss: 2.969867]\n",
      "epoch:4 step:4067 [D loss: 0.662902, acc: 63.28%] [G loss: 2.488766]\n",
      "epoch:4 step:4068 [D loss: 0.560169, acc: 67.19%] [G loss: 2.756943]\n",
      "epoch:4 step:4069 [D loss: 0.513342, acc: 74.22%] [G loss: 2.787930]\n",
      "epoch:4 step:4070 [D loss: 0.553353, acc: 70.31%] [G loss: 2.908263]\n",
      "epoch:4 step:4071 [D loss: 0.655308, acc: 60.94%] [G loss: 2.581750]\n",
      "epoch:4 step:4072 [D loss: 0.583250, acc: 67.97%] [G loss: 2.593315]\n",
      "epoch:4 step:4073 [D loss: 0.585057, acc: 70.31%] [G loss: 3.273787]\n",
      "epoch:4 step:4074 [D loss: 0.617842, acc: 67.97%] [G loss: 2.719342]\n",
      "epoch:4 step:4075 [D loss: 0.568239, acc: 62.50%] [G loss: 2.810357]\n",
      "epoch:4 step:4076 [D loss: 0.522178, acc: 78.12%] [G loss: 2.938750]\n",
      "epoch:4 step:4077 [D loss: 0.591449, acc: 68.75%] [G loss: 2.638550]\n",
      "epoch:4 step:4078 [D loss: 0.558249, acc: 71.88%] [G loss: 3.196278]\n",
      "epoch:4 step:4079 [D loss: 0.447781, acc: 81.25%] [G loss: 2.771791]\n",
      "epoch:4 step:4080 [D loss: 0.571959, acc: 70.31%] [G loss: 2.773963]\n",
      "epoch:4 step:4081 [D loss: 0.544333, acc: 72.66%] [G loss: 3.244579]\n",
      "epoch:4 step:4082 [D loss: 0.580775, acc: 72.66%] [G loss: 2.914564]\n",
      "epoch:4 step:4083 [D loss: 0.562396, acc: 69.53%] [G loss: 3.043959]\n",
      "epoch:4 step:4084 [D loss: 0.535628, acc: 71.09%] [G loss: 3.129069]\n",
      "epoch:4 step:4085 [D loss: 0.553863, acc: 71.88%] [G loss: 3.118166]\n",
      "epoch:4 step:4086 [D loss: 0.567740, acc: 72.66%] [G loss: 2.904732]\n",
      "epoch:4 step:4087 [D loss: 0.557914, acc: 71.88%] [G loss: 3.092315]\n",
      "epoch:4 step:4088 [D loss: 0.579110, acc: 65.62%] [G loss: 2.961439]\n",
      "epoch:4 step:4089 [D loss: 0.562789, acc: 72.66%] [G loss: 3.229554]\n",
      "epoch:4 step:4090 [D loss: 0.615058, acc: 68.75%] [G loss: 2.982114]\n",
      "epoch:4 step:4091 [D loss: 0.550672, acc: 75.00%] [G loss: 3.045658]\n",
      "epoch:4 step:4092 [D loss: 0.521938, acc: 73.44%] [G loss: 3.214203]\n",
      "epoch:4 step:4093 [D loss: 0.528015, acc: 73.44%] [G loss: 2.927962]\n",
      "epoch:4 step:4094 [D loss: 0.615355, acc: 70.31%] [G loss: 3.324129]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:4 step:4095 [D loss: 0.609470, acc: 71.88%] [G loss: 3.371395]\n",
      "epoch:4 step:4096 [D loss: 0.624650, acc: 66.41%] [G loss: 2.831152]\n",
      "epoch:4 step:4097 [D loss: 0.719081, acc: 57.03%] [G loss: 2.361934]\n",
      "epoch:4 step:4098 [D loss: 0.565459, acc: 72.66%] [G loss: 2.674406]\n",
      "epoch:4 step:4099 [D loss: 0.603432, acc: 73.44%] [G loss: 2.904051]\n",
      "epoch:4 step:4100 [D loss: 0.545513, acc: 71.09%] [G loss: 2.849051]\n",
      "epoch:4 step:4101 [D loss: 0.585785, acc: 68.75%] [G loss: 2.823610]\n",
      "epoch:4 step:4102 [D loss: 0.653428, acc: 61.72%] [G loss: 2.713275]\n",
      "epoch:4 step:4103 [D loss: 0.537693, acc: 69.53%] [G loss: 2.730503]\n",
      "epoch:4 step:4104 [D loss: 0.683153, acc: 63.28%] [G loss: 2.456906]\n",
      "epoch:4 step:4105 [D loss: 0.575186, acc: 72.66%] [G loss: 2.662965]\n",
      "epoch:4 step:4106 [D loss: 0.478196, acc: 78.12%] [G loss: 3.022629]\n",
      "epoch:4 step:4107 [D loss: 0.514512, acc: 68.75%] [G loss: 3.173355]\n",
      "epoch:4 step:4108 [D loss: 0.591265, acc: 66.41%] [G loss: 3.044512]\n",
      "epoch:4 step:4109 [D loss: 0.529472, acc: 74.22%] [G loss: 2.783121]\n",
      "epoch:4 step:4110 [D loss: 0.537837, acc: 71.09%] [G loss: 2.874996]\n",
      "epoch:4 step:4111 [D loss: 0.521453, acc: 75.78%] [G loss: 2.991279]\n",
      "epoch:4 step:4112 [D loss: 0.550619, acc: 78.12%] [G loss: 2.612997]\n",
      "epoch:4 step:4113 [D loss: 0.575944, acc: 69.53%] [G loss: 2.802281]\n",
      "epoch:4 step:4114 [D loss: 0.585059, acc: 71.88%] [G loss: 3.086804]\n",
      "epoch:4 step:4115 [D loss: 0.588555, acc: 71.88%] [G loss: 2.798459]\n",
      "epoch:4 step:4116 [D loss: 0.483820, acc: 76.56%] [G loss: 2.819461]\n",
      "epoch:4 step:4117 [D loss: 0.635716, acc: 65.62%] [G loss: 2.756959]\n",
      "epoch:4 step:4118 [D loss: 0.514621, acc: 69.53%] [G loss: 2.840804]\n",
      "epoch:4 step:4119 [D loss: 0.505705, acc: 75.78%] [G loss: 3.038839]\n",
      "epoch:4 step:4120 [D loss: 0.567874, acc: 70.31%] [G loss: 2.906776]\n",
      "epoch:4 step:4121 [D loss: 0.572915, acc: 71.09%] [G loss: 2.870640]\n",
      "epoch:4 step:4122 [D loss: 0.551907, acc: 69.53%] [G loss: 2.852060]\n",
      "epoch:4 step:4123 [D loss: 0.622011, acc: 61.72%] [G loss: 2.906675]\n",
      "epoch:4 step:4124 [D loss: 0.608786, acc: 65.62%] [G loss: 2.308256]\n",
      "epoch:4 step:4125 [D loss: 0.657713, acc: 63.28%] [G loss: 2.546269]\n",
      "epoch:4 step:4126 [D loss: 0.571269, acc: 74.22%] [G loss: 3.156024]\n",
      "epoch:4 step:4127 [D loss: 0.534728, acc: 71.88%] [G loss: 2.766171]\n",
      "epoch:4 step:4128 [D loss: 0.556834, acc: 73.44%] [G loss: 3.126928]\n",
      "epoch:4 step:4129 [D loss: 0.480404, acc: 73.44%] [G loss: 3.244597]\n",
      "epoch:4 step:4130 [D loss: 0.585448, acc: 67.97%] [G loss: 2.913953]\n",
      "epoch:4 step:4131 [D loss: 0.599031, acc: 67.19%] [G loss: 2.727762]\n",
      "epoch:4 step:4132 [D loss: 0.644840, acc: 71.09%] [G loss: 2.690788]\n",
      "epoch:4 step:4133 [D loss: 0.628217, acc: 66.41%] [G loss: 2.709324]\n",
      "epoch:4 step:4134 [D loss: 0.651195, acc: 63.28%] [G loss: 2.419423]\n",
      "epoch:4 step:4135 [D loss: 0.687901, acc: 61.72%] [G loss: 2.236055]\n",
      "epoch:4 step:4136 [D loss: 0.570552, acc: 67.97%] [G loss: 2.404025]\n",
      "epoch:4 step:4137 [D loss: 0.552250, acc: 74.22%] [G loss: 2.650440]\n",
      "epoch:4 step:4138 [D loss: 0.564114, acc: 66.41%] [G loss: 2.713701]\n",
      "epoch:4 step:4139 [D loss: 0.597559, acc: 64.84%] [G loss: 2.564397]\n",
      "epoch:4 step:4140 [D loss: 0.598921, acc: 69.53%] [G loss: 2.664432]\n",
      "epoch:4 step:4141 [D loss: 0.599350, acc: 66.41%] [G loss: 2.621408]\n",
      "epoch:4 step:4142 [D loss: 0.603108, acc: 70.31%] [G loss: 2.687263]\n",
      "epoch:4 step:4143 [D loss: 0.555472, acc: 71.88%] [G loss: 3.103884]\n",
      "epoch:4 step:4144 [D loss: 0.643341, acc: 65.62%] [G loss: 2.495003]\n",
      "epoch:4 step:4145 [D loss: 0.491564, acc: 80.47%] [G loss: 2.819399]\n",
      "epoch:4 step:4146 [D loss: 0.567167, acc: 74.22%] [G loss: 3.057061]\n",
      "epoch:4 step:4147 [D loss: 0.479229, acc: 78.91%] [G loss: 3.098087]\n",
      "epoch:4 step:4148 [D loss: 0.591718, acc: 67.97%] [G loss: 3.046930]\n",
      "epoch:4 step:4149 [D loss: 0.581604, acc: 67.97%] [G loss: 2.549884]\n",
      "epoch:4 step:4150 [D loss: 0.444129, acc: 83.59%] [G loss: 2.988420]\n",
      "epoch:4 step:4151 [D loss: 0.481772, acc: 78.91%] [G loss: 3.106109]\n",
      "epoch:4 step:4152 [D loss: 0.616487, acc: 64.84%] [G loss: 2.858645]\n",
      "epoch:4 step:4153 [D loss: 0.551086, acc: 73.44%] [G loss: 2.643914]\n",
      "epoch:4 step:4154 [D loss: 0.554719, acc: 75.00%] [G loss: 2.811506]\n",
      "epoch:4 step:4155 [D loss: 0.571082, acc: 73.44%] [G loss: 3.038318]\n",
      "epoch:4 step:4156 [D loss: 0.630844, acc: 66.41%] [G loss: 2.806592]\n",
      "epoch:4 step:4157 [D loss: 0.505094, acc: 71.09%] [G loss: 3.210636]\n",
      "epoch:4 step:4158 [D loss: 0.554488, acc: 63.28%] [G loss: 3.005654]\n",
      "epoch:4 step:4159 [D loss: 0.551300, acc: 71.88%] [G loss: 2.933703]\n",
      "epoch:4 step:4160 [D loss: 0.554764, acc: 73.44%] [G loss: 2.720621]\n",
      "epoch:4 step:4161 [D loss: 0.682615, acc: 62.50%] [G loss: 2.900213]\n",
      "epoch:4 step:4162 [D loss: 0.550472, acc: 74.22%] [G loss: 2.747540]\n",
      "epoch:4 step:4163 [D loss: 0.673498, acc: 60.16%] [G loss: 2.910158]\n",
      "epoch:4 step:4164 [D loss: 0.600472, acc: 66.41%] [G loss: 2.721035]\n",
      "epoch:4 step:4165 [D loss: 0.561150, acc: 75.00%] [G loss: 2.745162]\n",
      "epoch:4 step:4166 [D loss: 0.600472, acc: 65.62%] [G loss: 2.768477]\n",
      "epoch:4 step:4167 [D loss: 0.541348, acc: 74.22%] [G loss: 2.855401]\n",
      "epoch:4 step:4168 [D loss: 0.639270, acc: 64.84%] [G loss: 2.381213]\n",
      "epoch:4 step:4169 [D loss: 0.570262, acc: 70.31%] [G loss: 2.897270]\n",
      "epoch:4 step:4170 [D loss: 0.495259, acc: 75.78%] [G loss: 2.995582]\n",
      "epoch:4 step:4171 [D loss: 0.601942, acc: 62.50%] [G loss: 3.229356]\n",
      "epoch:4 step:4172 [D loss: 0.566044, acc: 76.56%] [G loss: 2.541998]\n",
      "epoch:4 step:4173 [D loss: 0.544123, acc: 69.53%] [G loss: 2.855718]\n",
      "epoch:4 step:4174 [D loss: 0.550679, acc: 69.53%] [G loss: 2.898707]\n",
      "epoch:4 step:4175 [D loss: 0.535184, acc: 72.66%] [G loss: 3.047127]\n",
      "epoch:4 step:4176 [D loss: 0.473808, acc: 77.34%] [G loss: 3.213933]\n",
      "epoch:4 step:4177 [D loss: 0.538420, acc: 71.88%] [G loss: 3.346191]\n",
      "epoch:4 step:4178 [D loss: 0.465303, acc: 79.69%] [G loss: 3.506961]\n",
      "epoch:4 step:4179 [D loss: 0.567584, acc: 70.31%] [G loss: 3.069262]\n",
      "epoch:4 step:4180 [D loss: 0.612658, acc: 64.06%] [G loss: 2.817303]\n",
      "epoch:4 step:4181 [D loss: 0.556762, acc: 71.09%] [G loss: 3.159684]\n",
      "epoch:4 step:4182 [D loss: 0.601176, acc: 62.50%] [G loss: 2.760780]\n",
      "epoch:4 step:4183 [D loss: 0.533246, acc: 68.75%] [G loss: 2.883850]\n",
      "epoch:4 step:4184 [D loss: 0.572849, acc: 70.31%] [G loss: 2.804072]\n",
      "epoch:4 step:4185 [D loss: 0.617605, acc: 64.84%] [G loss: 2.808715]\n",
      "epoch:4 step:4186 [D loss: 0.592124, acc: 70.31%] [G loss: 2.838403]\n",
      "epoch:4 step:4187 [D loss: 0.610971, acc: 71.88%] [G loss: 2.957204]\n",
      "epoch:4 step:4188 [D loss: 0.549793, acc: 75.78%] [G loss: 2.692611]\n",
      "epoch:4 step:4189 [D loss: 0.607476, acc: 61.72%] [G loss: 2.759413]\n",
      "epoch:4 step:4190 [D loss: 0.624141, acc: 67.19%] [G loss: 2.689660]\n",
      "epoch:4 step:4191 [D loss: 0.563519, acc: 69.53%] [G loss: 2.849112]\n",
      "epoch:4 step:4192 [D loss: 0.582045, acc: 67.19%] [G loss: 2.599940]\n",
      "epoch:4 step:4193 [D loss: 0.524028, acc: 74.22%] [G loss: 2.440146]\n",
      "epoch:4 step:4194 [D loss: 0.561468, acc: 70.31%] [G loss: 3.023050]\n",
      "epoch:4 step:4195 [D loss: 0.634797, acc: 66.41%] [G loss: 2.860935]\n",
      "epoch:4 step:4196 [D loss: 0.554681, acc: 70.31%] [G loss: 2.638853]\n",
      "epoch:4 step:4197 [D loss: 0.521826, acc: 71.88%] [G loss: 3.226153]\n",
      "epoch:4 step:4198 [D loss: 0.592685, acc: 67.19%] [G loss: 2.791862]\n",
      "epoch:4 step:4199 [D loss: 0.549745, acc: 71.09%] [G loss: 2.789042]\n",
      "epoch:4 step:4200 [D loss: 0.515216, acc: 75.78%] [G loss: 2.621214]\n",
      "##############\n",
      "[2.99695668 1.63861885 7.06080152 5.31375058 4.20066797 6.12276186\n",
      " 5.26222773 5.00952901 5.52588855 3.62891282]\n",
      "##########\n",
      "epoch:4 step:4201 [D loss: 0.588836, acc: 68.75%] [G loss: 2.422366]\n",
      "epoch:4 step:4202 [D loss: 0.551297, acc: 71.88%] [G loss: 2.492093]\n",
      "epoch:4 step:4203 [D loss: 0.577691, acc: 66.41%] [G loss: 2.892655]\n",
      "epoch:4 step:4204 [D loss: 0.553618, acc: 75.00%] [G loss: 2.728074]\n",
      "epoch:4 step:4205 [D loss: 0.548113, acc: 74.22%] [G loss: 2.831409]\n",
      "epoch:4 step:4206 [D loss: 0.660707, acc: 65.62%] [G loss: 2.597063]\n",
      "epoch:4 step:4207 [D loss: 0.638166, acc: 61.72%] [G loss: 2.631069]\n",
      "epoch:4 step:4208 [D loss: 0.545818, acc: 71.09%] [G loss: 2.786972]\n",
      "epoch:4 step:4209 [D loss: 0.654034, acc: 64.06%] [G loss: 2.678098]\n",
      "epoch:4 step:4210 [D loss: 0.649705, acc: 60.16%] [G loss: 2.775204]\n",
      "epoch:4 step:4211 [D loss: 0.578104, acc: 66.41%] [G loss: 2.758630]\n",
      "epoch:4 step:4212 [D loss: 0.513780, acc: 77.34%] [G loss: 2.820521]\n",
      "epoch:4 step:4213 [D loss: 0.690612, acc: 59.38%] [G loss: 2.279326]\n",
      "epoch:4 step:4214 [D loss: 0.592706, acc: 69.53%] [G loss: 2.741486]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:4 step:4215 [D loss: 0.548286, acc: 71.88%] [G loss: 2.674300]\n",
      "epoch:4 step:4216 [D loss: 0.567199, acc: 67.19%] [G loss: 2.914997]\n",
      "epoch:4 step:4217 [D loss: 0.589826, acc: 67.19%] [G loss: 2.834208]\n",
      "epoch:4 step:4218 [D loss: 0.549034, acc: 70.31%] [G loss: 3.394890]\n",
      "epoch:4 step:4219 [D loss: 0.568630, acc: 71.88%] [G loss: 2.967633]\n",
      "epoch:4 step:4220 [D loss: 0.555060, acc: 67.19%] [G loss: 3.274648]\n",
      "epoch:4 step:4221 [D loss: 0.619732, acc: 67.19%] [G loss: 3.034901]\n",
      "epoch:4 step:4222 [D loss: 0.535722, acc: 74.22%] [G loss: 3.164064]\n",
      "epoch:4 step:4223 [D loss: 0.588709, acc: 69.53%] [G loss: 2.993323]\n",
      "epoch:4 step:4224 [D loss: 0.590117, acc: 65.62%] [G loss: 2.647323]\n",
      "epoch:4 step:4225 [D loss: 0.662386, acc: 63.28%] [G loss: 2.462011]\n",
      "epoch:4 step:4226 [D loss: 0.548759, acc: 72.66%] [G loss: 2.450983]\n",
      "epoch:4 step:4227 [D loss: 0.578401, acc: 69.53%] [G loss: 2.617802]\n",
      "epoch:4 step:4228 [D loss: 0.547066, acc: 79.69%] [G loss: 2.602812]\n",
      "epoch:4 step:4229 [D loss: 0.622307, acc: 68.75%] [G loss: 2.490165]\n",
      "epoch:4 step:4230 [D loss: 0.582616, acc: 70.31%] [G loss: 2.436046]\n",
      "epoch:4 step:4231 [D loss: 0.633706, acc: 62.50%] [G loss: 2.720292]\n",
      "epoch:4 step:4232 [D loss: 0.559913, acc: 71.09%] [G loss: 3.194185]\n",
      "epoch:4 step:4233 [D loss: 0.585269, acc: 65.62%] [G loss: 2.582995]\n",
      "epoch:4 step:4234 [D loss: 0.606914, acc: 67.19%] [G loss: 2.786701]\n",
      "epoch:4 step:4235 [D loss: 0.606816, acc: 69.53%] [G loss: 2.806266]\n",
      "epoch:4 step:4236 [D loss: 0.536346, acc: 73.44%] [G loss: 2.720136]\n",
      "epoch:4 step:4237 [D loss: 0.595129, acc: 64.84%] [G loss: 2.735249]\n",
      "epoch:4 step:4238 [D loss: 0.583811, acc: 67.97%] [G loss: 2.713644]\n",
      "epoch:4 step:4239 [D loss: 0.612401, acc: 69.53%] [G loss: 2.615512]\n",
      "epoch:4 step:4240 [D loss: 0.599303, acc: 67.97%] [G loss: 2.564358]\n",
      "epoch:4 step:4241 [D loss: 0.554780, acc: 67.19%] [G loss: 2.715486]\n",
      "epoch:4 step:4242 [D loss: 0.509852, acc: 75.00%] [G loss: 3.308363]\n",
      "epoch:4 step:4243 [D loss: 0.574398, acc: 67.19%] [G loss: 3.040655]\n",
      "epoch:4 step:4244 [D loss: 0.675938, acc: 66.41%] [G loss: 2.691279]\n",
      "epoch:4 step:4245 [D loss: 0.522519, acc: 71.88%] [G loss: 3.379950]\n",
      "epoch:4 step:4246 [D loss: 0.480245, acc: 77.34%] [G loss: 3.189435]\n",
      "epoch:4 step:4247 [D loss: 0.457819, acc: 78.12%] [G loss: 3.775248]\n",
      "epoch:4 step:4248 [D loss: 0.744212, acc: 57.81%] [G loss: 2.648159]\n",
      "epoch:4 step:4249 [D loss: 0.663950, acc: 64.06%] [G loss: 2.476974]\n",
      "epoch:4 step:4250 [D loss: 0.593149, acc: 64.06%] [G loss: 2.646708]\n",
      "epoch:4 step:4251 [D loss: 0.534857, acc: 72.66%] [G loss: 2.966827]\n",
      "epoch:4 step:4252 [D loss: 0.574378, acc: 70.31%] [G loss: 3.179638]\n",
      "epoch:4 step:4253 [D loss: 0.652303, acc: 66.41%] [G loss: 2.433311]\n",
      "epoch:4 step:4254 [D loss: 0.557411, acc: 74.22%] [G loss: 2.459834]\n",
      "epoch:4 step:4255 [D loss: 0.660440, acc: 65.62%] [G loss: 2.714475]\n",
      "epoch:4 step:4256 [D loss: 0.510302, acc: 71.09%] [G loss: 3.258029]\n",
      "epoch:4 step:4257 [D loss: 0.499312, acc: 78.12%] [G loss: 2.804728]\n",
      "epoch:4 step:4258 [D loss: 0.599849, acc: 65.62%] [G loss: 2.577354]\n",
      "epoch:4 step:4259 [D loss: 0.556827, acc: 71.88%] [G loss: 2.735764]\n",
      "epoch:4 step:4260 [D loss: 0.622724, acc: 67.97%] [G loss: 3.031137]\n",
      "epoch:4 step:4261 [D loss: 0.578754, acc: 70.31%] [G loss: 2.893839]\n",
      "epoch:4 step:4262 [D loss: 0.559417, acc: 70.31%] [G loss: 2.773596]\n",
      "epoch:4 step:4263 [D loss: 0.587073, acc: 74.22%] [G loss: 2.786577]\n",
      "epoch:4 step:4264 [D loss: 0.602723, acc: 68.75%] [G loss: 2.844113]\n",
      "epoch:4 step:4265 [D loss: 0.700452, acc: 60.16%] [G loss: 2.429159]\n",
      "epoch:4 step:4266 [D loss: 0.516861, acc: 69.53%] [G loss: 2.884104]\n",
      "epoch:4 step:4267 [D loss: 0.606704, acc: 68.75%] [G loss: 2.929025]\n",
      "epoch:4 step:4268 [D loss: 0.515759, acc: 76.56%] [G loss: 2.955906]\n",
      "epoch:4 step:4269 [D loss: 0.621572, acc: 69.53%] [G loss: 2.442470]\n",
      "epoch:4 step:4270 [D loss: 0.508093, acc: 74.22%] [G loss: 2.913314]\n",
      "epoch:4 step:4271 [D loss: 0.533082, acc: 71.88%] [G loss: 3.023333]\n",
      "epoch:4 step:4272 [D loss: 0.607162, acc: 67.97%] [G loss: 2.766016]\n",
      "epoch:4 step:4273 [D loss: 0.659151, acc: 60.16%] [G loss: 2.418804]\n",
      "epoch:4 step:4274 [D loss: 0.596426, acc: 72.66%] [G loss: 2.988040]\n",
      "epoch:4 step:4275 [D loss: 0.618276, acc: 63.28%] [G loss: 2.545652]\n",
      "epoch:4 step:4276 [D loss: 0.583836, acc: 64.06%] [G loss: 2.502245]\n",
      "epoch:4 step:4277 [D loss: 0.594584, acc: 65.62%] [G loss: 2.442900]\n",
      "epoch:4 step:4278 [D loss: 0.606454, acc: 67.19%] [G loss: 2.803861]\n",
      "epoch:4 step:4279 [D loss: 0.605504, acc: 64.84%] [G loss: 2.624301]\n",
      "epoch:4 step:4280 [D loss: 0.539620, acc: 69.53%] [G loss: 2.728318]\n",
      "epoch:4 step:4281 [D loss: 0.633559, acc: 68.75%] [G loss: 2.971552]\n",
      "epoch:4 step:4282 [D loss: 0.521069, acc: 76.56%] [G loss: 2.756924]\n",
      "epoch:4 step:4283 [D loss: 0.554177, acc: 71.09%] [G loss: 2.601646]\n",
      "epoch:4 step:4284 [D loss: 0.535198, acc: 75.00%] [G loss: 2.760291]\n",
      "epoch:4 step:4285 [D loss: 0.661032, acc: 60.94%] [G loss: 2.509009]\n",
      "epoch:4 step:4286 [D loss: 0.639173, acc: 64.06%] [G loss: 2.534503]\n",
      "epoch:4 step:4287 [D loss: 0.537173, acc: 71.09%] [G loss: 2.680224]\n",
      "epoch:4 step:4288 [D loss: 0.556663, acc: 70.31%] [G loss: 2.636037]\n",
      "epoch:4 step:4289 [D loss: 0.571946, acc: 74.22%] [G loss: 2.719170]\n",
      "epoch:4 step:4290 [D loss: 0.570102, acc: 69.53%] [G loss: 2.659664]\n",
      "epoch:4 step:4291 [D loss: 0.638309, acc: 67.19%] [G loss: 2.655166]\n",
      "epoch:4 step:4292 [D loss: 0.608251, acc: 65.62%] [G loss: 2.757329]\n",
      "epoch:4 step:4293 [D loss: 0.549879, acc: 70.31%] [G loss: 2.696616]\n",
      "epoch:4 step:4294 [D loss: 0.566554, acc: 74.22%] [G loss: 2.620686]\n",
      "epoch:4 step:4295 [D loss: 0.555749, acc: 67.97%] [G loss: 2.893412]\n",
      "epoch:4 step:4296 [D loss: 0.551501, acc: 74.22%] [G loss: 2.943784]\n",
      "epoch:4 step:4297 [D loss: 0.561932, acc: 70.31%] [G loss: 2.644807]\n",
      "epoch:4 step:4298 [D loss: 0.546245, acc: 71.09%] [G loss: 2.715659]\n",
      "epoch:4 step:4299 [D loss: 0.551325, acc: 75.78%] [G loss: 2.929135]\n",
      "epoch:4 step:4300 [D loss: 0.576189, acc: 72.66%] [G loss: 3.085623]\n",
      "epoch:4 step:4301 [D loss: 0.643417, acc: 64.84%] [G loss: 2.886234]\n",
      "epoch:4 step:4302 [D loss: 0.551003, acc: 72.66%] [G loss: 2.887335]\n",
      "epoch:4 step:4303 [D loss: 0.547142, acc: 74.22%] [G loss: 2.645851]\n",
      "epoch:4 step:4304 [D loss: 0.473869, acc: 76.56%] [G loss: 2.900593]\n",
      "epoch:4 step:4305 [D loss: 0.594657, acc: 67.97%] [G loss: 2.951360]\n",
      "epoch:4 step:4306 [D loss: 0.479244, acc: 79.69%] [G loss: 3.163458]\n",
      "epoch:4 step:4307 [D loss: 0.683204, acc: 57.81%] [G loss: 2.770726]\n",
      "epoch:4 step:4308 [D loss: 0.681856, acc: 57.03%] [G loss: 2.441347]\n",
      "epoch:4 step:4309 [D loss: 0.600028, acc: 68.75%] [G loss: 2.646412]\n",
      "epoch:4 step:4310 [D loss: 0.658869, acc: 65.62%] [G loss: 2.391033]\n",
      "epoch:4 step:4311 [D loss: 0.663820, acc: 62.50%] [G loss: 2.305813]\n",
      "epoch:4 step:4312 [D loss: 0.622432, acc: 64.84%] [G loss: 2.443748]\n",
      "epoch:4 step:4313 [D loss: 0.520302, acc: 74.22%] [G loss: 2.647071]\n",
      "epoch:4 step:4314 [D loss: 0.628155, acc: 66.41%] [G loss: 2.475001]\n",
      "epoch:4 step:4315 [D loss: 0.648800, acc: 63.28%] [G loss: 2.806048]\n",
      "epoch:4 step:4316 [D loss: 0.512563, acc: 74.22%] [G loss: 2.885223]\n",
      "epoch:4 step:4317 [D loss: 0.559349, acc: 75.78%] [G loss: 2.794094]\n",
      "epoch:4 step:4318 [D loss: 0.568888, acc: 66.41%] [G loss: 2.852980]\n",
      "epoch:4 step:4319 [D loss: 0.545793, acc: 71.09%] [G loss: 2.715758]\n",
      "epoch:4 step:4320 [D loss: 0.594357, acc: 67.19%] [G loss: 2.512407]\n",
      "epoch:4 step:4321 [D loss: 0.608692, acc: 64.06%] [G loss: 2.354073]\n",
      "epoch:4 step:4322 [D loss: 0.558796, acc: 71.88%] [G loss: 2.491590]\n",
      "epoch:4 step:4323 [D loss: 0.562348, acc: 72.66%] [G loss: 2.718945]\n",
      "epoch:4 step:4324 [D loss: 0.589116, acc: 69.53%] [G loss: 2.454748]\n",
      "epoch:4 step:4325 [D loss: 0.536206, acc: 70.31%] [G loss: 2.669794]\n",
      "epoch:4 step:4326 [D loss: 0.577394, acc: 70.31%] [G loss: 2.576055]\n",
      "epoch:4 step:4327 [D loss: 0.547035, acc: 69.53%] [G loss: 3.034846]\n",
      "epoch:4 step:4328 [D loss: 0.649564, acc: 64.06%] [G loss: 2.525846]\n",
      "epoch:4 step:4329 [D loss: 0.588070, acc: 71.09%] [G loss: 2.733598]\n",
      "epoch:4 step:4330 [D loss: 0.497248, acc: 78.91%] [G loss: 3.171917]\n",
      "epoch:4 step:4331 [D loss: 0.625089, acc: 64.06%] [G loss: 2.727804]\n",
      "epoch:4 step:4332 [D loss: 0.613650, acc: 67.19%] [G loss: 2.669990]\n",
      "epoch:4 step:4333 [D loss: 0.519845, acc: 78.91%] [G loss: 2.604476]\n",
      "epoch:4 step:4334 [D loss: 0.510540, acc: 75.78%] [G loss: 2.657736]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:4 step:4335 [D loss: 0.518980, acc: 76.56%] [G loss: 3.273670]\n",
      "epoch:4 step:4336 [D loss: 0.561743, acc: 75.78%] [G loss: 3.021413]\n",
      "epoch:4 step:4337 [D loss: 0.522192, acc: 75.78%] [G loss: 3.195221]\n",
      "epoch:4 step:4338 [D loss: 0.586118, acc: 63.28%] [G loss: 2.822155]\n",
      "epoch:4 step:4339 [D loss: 0.598122, acc: 69.53%] [G loss: 2.450019]\n",
      "epoch:4 step:4340 [D loss: 0.509775, acc: 75.00%] [G loss: 3.045562]\n",
      "epoch:4 step:4341 [D loss: 0.533314, acc: 75.78%] [G loss: 2.683189]\n",
      "epoch:4 step:4342 [D loss: 0.632631, acc: 64.06%] [G loss: 2.738504]\n",
      "epoch:4 step:4343 [D loss: 0.534812, acc: 70.31%] [G loss: 2.616234]\n",
      "epoch:4 step:4344 [D loss: 0.577453, acc: 67.97%] [G loss: 2.470522]\n",
      "epoch:4 step:4345 [D loss: 0.646960, acc: 67.19%] [G loss: 2.559649]\n",
      "epoch:4 step:4346 [D loss: 0.477748, acc: 78.12%] [G loss: 2.849921]\n",
      "epoch:4 step:4347 [D loss: 0.638868, acc: 65.62%] [G loss: 2.739459]\n",
      "epoch:4 step:4348 [D loss: 0.590216, acc: 66.41%] [G loss: 2.862244]\n",
      "epoch:4 step:4349 [D loss: 0.612452, acc: 69.53%] [G loss: 2.786694]\n",
      "epoch:4 step:4350 [D loss: 0.578974, acc: 72.66%] [G loss: 2.808962]\n",
      "epoch:4 step:4351 [D loss: 0.509014, acc: 71.09%] [G loss: 2.916533]\n",
      "epoch:4 step:4352 [D loss: 0.648590, acc: 65.62%] [G loss: 2.576591]\n",
      "epoch:4 step:4353 [D loss: 0.559086, acc: 69.53%] [G loss: 2.736694]\n",
      "epoch:4 step:4354 [D loss: 0.599082, acc: 67.19%] [G loss: 2.717287]\n",
      "epoch:4 step:4355 [D loss: 0.588952, acc: 65.62%] [G loss: 2.628773]\n",
      "epoch:4 step:4356 [D loss: 0.470813, acc: 79.69%] [G loss: 2.665783]\n",
      "epoch:4 step:4357 [D loss: 0.498802, acc: 74.22%] [G loss: 2.884910]\n",
      "epoch:4 step:4358 [D loss: 0.603410, acc: 73.44%] [G loss: 2.977266]\n",
      "epoch:4 step:4359 [D loss: 0.603941, acc: 71.88%] [G loss: 2.830417]\n",
      "epoch:4 step:4360 [D loss: 0.555813, acc: 69.53%] [G loss: 2.632770]\n",
      "epoch:4 step:4361 [D loss: 0.544932, acc: 70.31%] [G loss: 2.853114]\n",
      "epoch:4 step:4362 [D loss: 0.594643, acc: 68.75%] [G loss: 2.745683]\n",
      "epoch:4 step:4363 [D loss: 0.678127, acc: 64.84%] [G loss: 2.466045]\n",
      "epoch:4 step:4364 [D loss: 0.647778, acc: 64.06%] [G loss: 2.628366]\n",
      "epoch:4 step:4365 [D loss: 0.607929, acc: 64.06%] [G loss: 2.725082]\n",
      "epoch:4 step:4366 [D loss: 0.520478, acc: 75.78%] [G loss: 2.620689]\n",
      "epoch:4 step:4367 [D loss: 0.695976, acc: 64.84%] [G loss: 2.779552]\n",
      "epoch:4 step:4368 [D loss: 0.724880, acc: 57.03%] [G loss: 2.544179]\n",
      "epoch:4 step:4369 [D loss: 0.623730, acc: 68.75%] [G loss: 2.333694]\n",
      "epoch:4 step:4370 [D loss: 0.669519, acc: 59.38%] [G loss: 2.307480]\n",
      "epoch:4 step:4371 [D loss: 0.593998, acc: 69.53%] [G loss: 2.571892]\n",
      "epoch:4 step:4372 [D loss: 0.549043, acc: 74.22%] [G loss: 2.645361]\n",
      "epoch:4 step:4373 [D loss: 0.603043, acc: 69.53%] [G loss: 2.372890]\n",
      "epoch:4 step:4374 [D loss: 0.614494, acc: 66.41%] [G loss: 2.574424]\n",
      "epoch:4 step:4375 [D loss: 0.655631, acc: 61.72%] [G loss: 2.829045]\n",
      "epoch:4 step:4376 [D loss: 0.507560, acc: 79.69%] [G loss: 2.848263]\n",
      "epoch:4 step:4377 [D loss: 0.669414, acc: 60.94%] [G loss: 2.670783]\n",
      "epoch:4 step:4378 [D loss: 0.520199, acc: 75.00%] [G loss: 2.848627]\n",
      "epoch:4 step:4379 [D loss: 0.500354, acc: 75.78%] [G loss: 2.715347]\n",
      "epoch:4 step:4380 [D loss: 0.539808, acc: 76.56%] [G loss: 2.738388]\n",
      "epoch:4 step:4381 [D loss: 0.580789, acc: 66.41%] [G loss: 3.030915]\n",
      "epoch:4 step:4382 [D loss: 0.614008, acc: 63.28%] [G loss: 2.707559]\n",
      "epoch:4 step:4383 [D loss: 0.482302, acc: 79.69%] [G loss: 2.872243]\n",
      "epoch:4 step:4384 [D loss: 0.612977, acc: 67.97%] [G loss: 2.791483]\n",
      "epoch:4 step:4385 [D loss: 0.555291, acc: 70.31%] [G loss: 2.796655]\n",
      "epoch:4 step:4386 [D loss: 0.583059, acc: 65.62%] [G loss: 2.769423]\n",
      "epoch:4 step:4387 [D loss: 0.542553, acc: 74.22%] [G loss: 3.178504]\n",
      "epoch:4 step:4388 [D loss: 0.565520, acc: 75.00%] [G loss: 3.026036]\n",
      "epoch:4 step:4389 [D loss: 0.489614, acc: 74.22%] [G loss: 3.255817]\n",
      "epoch:4 step:4390 [D loss: 0.506713, acc: 70.31%] [G loss: 3.212016]\n",
      "epoch:4 step:4391 [D loss: 0.544436, acc: 73.44%] [G loss: 3.207798]\n",
      "epoch:4 step:4392 [D loss: 0.592001, acc: 67.97%] [G loss: 2.917388]\n",
      "epoch:4 step:4393 [D loss: 0.584972, acc: 67.97%] [G loss: 2.873927]\n",
      "epoch:4 step:4394 [D loss: 0.544877, acc: 71.09%] [G loss: 2.725398]\n",
      "epoch:4 step:4395 [D loss: 0.587934, acc: 71.88%] [G loss: 2.812696]\n",
      "epoch:4 step:4396 [D loss: 0.480650, acc: 81.25%] [G loss: 3.429073]\n",
      "epoch:4 step:4397 [D loss: 0.512318, acc: 75.78%] [G loss: 3.424313]\n",
      "epoch:4 step:4398 [D loss: 0.547143, acc: 73.44%] [G loss: 3.290101]\n",
      "epoch:4 step:4399 [D loss: 0.556620, acc: 69.53%] [G loss: 2.825131]\n",
      "epoch:4 step:4400 [D loss: 0.622700, acc: 66.41%] [G loss: 2.698336]\n",
      "##############\n",
      "[3.04566015 1.69082757 6.80857342 5.42045382 4.21016283 6.05445168\n",
      " 5.18227839 4.93315469 5.4872807  3.84572845]\n",
      "##########\n",
      "epoch:4 step:4401 [D loss: 0.575729, acc: 71.09%] [G loss: 2.916392]\n",
      "epoch:4 step:4402 [D loss: 0.565800, acc: 69.53%] [G loss: 3.037654]\n",
      "epoch:4 step:4403 [D loss: 0.678459, acc: 65.62%] [G loss: 2.941032]\n",
      "epoch:4 step:4404 [D loss: 0.603375, acc: 71.09%] [G loss: 2.752195]\n",
      "epoch:4 step:4405 [D loss: 0.544132, acc: 71.88%] [G loss: 2.694924]\n",
      "epoch:4 step:4406 [D loss: 0.647834, acc: 60.94%] [G loss: 2.311813]\n",
      "epoch:4 step:4407 [D loss: 0.553367, acc: 70.31%] [G loss: 2.604101]\n",
      "epoch:4 step:4408 [D loss: 0.498890, acc: 78.91%] [G loss: 3.200843]\n",
      "epoch:4 step:4409 [D loss: 0.613438, acc: 70.31%] [G loss: 2.694989]\n",
      "epoch:4 step:4410 [D loss: 0.485405, acc: 75.78%] [G loss: 3.141724]\n",
      "epoch:4 step:4411 [D loss: 0.537938, acc: 75.00%] [G loss: 3.570096]\n",
      "epoch:4 step:4412 [D loss: 0.507631, acc: 74.22%] [G loss: 2.917414]\n",
      "epoch:4 step:4413 [D loss: 0.531836, acc: 72.66%] [G loss: 3.082941]\n",
      "epoch:4 step:4414 [D loss: 0.600945, acc: 70.31%] [G loss: 2.823814]\n",
      "epoch:4 step:4415 [D loss: 0.616565, acc: 71.09%] [G loss: 2.491637]\n",
      "epoch:4 step:4416 [D loss: 0.545919, acc: 70.31%] [G loss: 2.858141]\n",
      "epoch:4 step:4417 [D loss: 0.655926, acc: 69.53%] [G loss: 2.563017]\n",
      "epoch:4 step:4418 [D loss: 0.557271, acc: 73.44%] [G loss: 2.797818]\n",
      "epoch:4 step:4419 [D loss: 0.615989, acc: 64.06%] [G loss: 2.833693]\n",
      "epoch:4 step:4420 [D loss: 0.615748, acc: 64.84%] [G loss: 2.599546]\n",
      "epoch:4 step:4421 [D loss: 0.639018, acc: 67.97%] [G loss: 2.818438]\n",
      "epoch:4 step:4422 [D loss: 0.659147, acc: 58.59%] [G loss: 2.772763]\n",
      "epoch:4 step:4423 [D loss: 0.571573, acc: 71.88%] [G loss: 2.613003]\n",
      "epoch:4 step:4424 [D loss: 0.526190, acc: 68.75%] [G loss: 2.481433]\n",
      "epoch:4 step:4425 [D loss: 0.503738, acc: 75.00%] [G loss: 3.242800]\n",
      "epoch:4 step:4426 [D loss: 0.559064, acc: 67.19%] [G loss: 2.853312]\n",
      "epoch:4 step:4427 [D loss: 0.557293, acc: 69.53%] [G loss: 2.672042]\n",
      "epoch:4 step:4428 [D loss: 0.520054, acc: 73.44%] [G loss: 2.992066]\n",
      "epoch:4 step:4429 [D loss: 0.567542, acc: 71.88%] [G loss: 2.921313]\n",
      "epoch:4 step:4430 [D loss: 0.570001, acc: 70.31%] [G loss: 2.689425]\n",
      "epoch:4 step:4431 [D loss: 0.544161, acc: 71.88%] [G loss: 2.666747]\n",
      "epoch:4 step:4432 [D loss: 0.509869, acc: 73.44%] [G loss: 2.666674]\n",
      "epoch:4 step:4433 [D loss: 0.585130, acc: 73.44%] [G loss: 2.809054]\n",
      "epoch:4 step:4434 [D loss: 0.579736, acc: 71.88%] [G loss: 2.740000]\n",
      "epoch:4 step:4435 [D loss: 0.570062, acc: 69.53%] [G loss: 3.102085]\n",
      "epoch:4 step:4436 [D loss: 0.624466, acc: 66.41%] [G loss: 2.511992]\n",
      "epoch:4 step:4437 [D loss: 0.625850, acc: 71.88%] [G loss: 2.750982]\n",
      "epoch:4 step:4438 [D loss: 0.558601, acc: 69.53%] [G loss: 2.631689]\n",
      "epoch:4 step:4439 [D loss: 0.594730, acc: 67.97%] [G loss: 2.706234]\n",
      "epoch:4 step:4440 [D loss: 0.587600, acc: 73.44%] [G loss: 2.479094]\n",
      "epoch:4 step:4441 [D loss: 0.543031, acc: 70.31%] [G loss: 2.958697]\n",
      "epoch:4 step:4442 [D loss: 0.528120, acc: 77.34%] [G loss: 3.008153]\n",
      "epoch:4 step:4443 [D loss: 0.602742, acc: 67.19%] [G loss: 2.940481]\n",
      "epoch:4 step:4444 [D loss: 0.631049, acc: 65.62%] [G loss: 2.715677]\n",
      "epoch:4 step:4445 [D loss: 0.503582, acc: 75.78%] [G loss: 2.887924]\n",
      "epoch:4 step:4446 [D loss: 0.549161, acc: 71.09%] [G loss: 2.661289]\n",
      "epoch:4 step:4447 [D loss: 0.637773, acc: 65.62%] [G loss: 3.118670]\n",
      "epoch:4 step:4448 [D loss: 0.542917, acc: 72.66%] [G loss: 3.079528]\n",
      "epoch:4 step:4449 [D loss: 0.460411, acc: 79.69%] [G loss: 2.928033]\n",
      "epoch:4 step:4450 [D loss: 0.718300, acc: 61.72%] [G loss: 2.746584]\n",
      "epoch:4 step:4451 [D loss: 0.574057, acc: 64.84%] [G loss: 2.906978]\n",
      "epoch:4 step:4452 [D loss: 0.643047, acc: 63.28%] [G loss: 2.484343]\n",
      "epoch:4 step:4453 [D loss: 0.584936, acc: 69.53%] [G loss: 2.696535]\n",
      "epoch:4 step:4454 [D loss: 0.574053, acc: 66.41%] [G loss: 2.813771]\n",
      "epoch:4 step:4455 [D loss: 0.439735, acc: 83.59%] [G loss: 3.550114]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:4 step:4456 [D loss: 0.498106, acc: 76.56%] [G loss: 3.164136]\n",
      "epoch:4 step:4457 [D loss: 0.513448, acc: 76.56%] [G loss: 3.194577]\n",
      "epoch:4 step:4458 [D loss: 0.680468, acc: 60.16%] [G loss: 2.526225]\n",
      "epoch:4 step:4459 [D loss: 0.627533, acc: 69.53%] [G loss: 2.627615]\n",
      "epoch:4 step:4460 [D loss: 0.525039, acc: 75.78%] [G loss: 2.790797]\n",
      "epoch:4 step:4461 [D loss: 0.589676, acc: 70.31%] [G loss: 2.899123]\n",
      "epoch:4 step:4462 [D loss: 0.506240, acc: 71.88%] [G loss: 3.303530]\n",
      "epoch:4 step:4463 [D loss: 0.624227, acc: 63.28%] [G loss: 2.665657]\n",
      "epoch:4 step:4464 [D loss: 0.559695, acc: 71.09%] [G loss: 2.468582]\n",
      "epoch:4 step:4465 [D loss: 0.604323, acc: 66.41%] [G loss: 2.777220]\n",
      "epoch:4 step:4466 [D loss: 0.652990, acc: 61.72%] [G loss: 2.507850]\n",
      "epoch:4 step:4467 [D loss: 0.551569, acc: 75.00%] [G loss: 2.578303]\n",
      "epoch:4 step:4468 [D loss: 0.535567, acc: 72.66%] [G loss: 2.563513]\n",
      "epoch:4 step:4469 [D loss: 0.663926, acc: 63.28%] [G loss: 2.655905]\n",
      "epoch:4 step:4470 [D loss: 0.482052, acc: 78.12%] [G loss: 2.728544]\n",
      "epoch:4 step:4471 [D loss: 0.552790, acc: 68.75%] [G loss: 2.615551]\n",
      "epoch:4 step:4472 [D loss: 0.506648, acc: 71.88%] [G loss: 2.956118]\n",
      "epoch:4 step:4473 [D loss: 0.597921, acc: 71.09%] [G loss: 2.983508]\n",
      "epoch:4 step:4474 [D loss: 0.633645, acc: 67.19%] [G loss: 2.739615]\n",
      "epoch:4 step:4475 [D loss: 0.619695, acc: 70.31%] [G loss: 2.611029]\n",
      "epoch:4 step:4476 [D loss: 0.560515, acc: 65.62%] [G loss: 2.690663]\n",
      "epoch:4 step:4477 [D loss: 0.637911, acc: 64.06%] [G loss: 2.553982]\n",
      "epoch:4 step:4478 [D loss: 0.644626, acc: 58.59%] [G loss: 2.668772]\n",
      "epoch:4 step:4479 [D loss: 0.539020, acc: 71.09%] [G loss: 2.508214]\n",
      "epoch:4 step:4480 [D loss: 0.544479, acc: 77.34%] [G loss: 2.487525]\n",
      "epoch:4 step:4481 [D loss: 0.549102, acc: 73.44%] [G loss: 2.635267]\n",
      "epoch:4 step:4482 [D loss: 0.517075, acc: 74.22%] [G loss: 2.885961]\n",
      "epoch:4 step:4483 [D loss: 0.577659, acc: 71.88%] [G loss: 2.543222]\n",
      "epoch:4 step:4484 [D loss: 0.518373, acc: 75.00%] [G loss: 2.882958]\n",
      "epoch:4 step:4485 [D loss: 0.531870, acc: 78.12%] [G loss: 2.842980]\n",
      "epoch:4 step:4486 [D loss: 0.638455, acc: 66.41%] [G loss: 2.650760]\n",
      "epoch:4 step:4487 [D loss: 0.539447, acc: 75.00%] [G loss: 2.740551]\n",
      "epoch:4 step:4488 [D loss: 0.601949, acc: 64.06%] [G loss: 2.607348]\n",
      "epoch:4 step:4489 [D loss: 0.544307, acc: 72.66%] [G loss: 2.821625]\n",
      "epoch:4 step:4490 [D loss: 0.645184, acc: 64.06%] [G loss: 2.650600]\n",
      "epoch:4 step:4491 [D loss: 0.573238, acc: 66.41%] [G loss: 2.527841]\n",
      "epoch:4 step:4492 [D loss: 0.562518, acc: 76.56%] [G loss: 2.667423]\n",
      "epoch:4 step:4493 [D loss: 0.632613, acc: 60.16%] [G loss: 2.888348]\n",
      "epoch:4 step:4494 [D loss: 0.533243, acc: 73.44%] [G loss: 2.720873]\n",
      "epoch:4 step:4495 [D loss: 0.549827, acc: 69.53%] [G loss: 2.852909]\n",
      "epoch:4 step:4496 [D loss: 0.564889, acc: 68.75%] [G loss: 2.665658]\n",
      "epoch:4 step:4497 [D loss: 0.612414, acc: 70.31%] [G loss: 2.469688]\n",
      "epoch:4 step:4498 [D loss: 0.590143, acc: 69.53%] [G loss: 2.569537]\n",
      "epoch:4 step:4499 [D loss: 0.549475, acc: 69.53%] [G loss: 2.535667]\n",
      "epoch:4 step:4500 [D loss: 0.589798, acc: 68.75%] [G loss: 2.596764]\n",
      "epoch:4 step:4501 [D loss: 0.538072, acc: 71.09%] [G loss: 2.656412]\n",
      "epoch:4 step:4502 [D loss: 0.490028, acc: 78.91%] [G loss: 2.750400]\n",
      "epoch:4 step:4503 [D loss: 0.555494, acc: 69.53%] [G loss: 2.524716]\n",
      "epoch:4 step:4504 [D loss: 0.540529, acc: 71.88%] [G loss: 2.808900]\n",
      "epoch:4 step:4505 [D loss: 0.580957, acc: 71.09%] [G loss: 2.514795]\n",
      "epoch:4 step:4506 [D loss: 0.630999, acc: 66.41%] [G loss: 2.634037]\n",
      "epoch:4 step:4507 [D loss: 0.530269, acc: 77.34%] [G loss: 2.627921]\n",
      "epoch:4 step:4508 [D loss: 0.573699, acc: 65.62%] [G loss: 2.848733]\n",
      "epoch:4 step:4509 [D loss: 0.601592, acc: 66.41%] [G loss: 2.855309]\n",
      "epoch:4 step:4510 [D loss: 0.552753, acc: 71.09%] [G loss: 2.505509]\n",
      "epoch:4 step:4511 [D loss: 0.553833, acc: 69.53%] [G loss: 2.487801]\n",
      "epoch:4 step:4512 [D loss: 0.632055, acc: 65.62%] [G loss: 2.750403]\n",
      "epoch:4 step:4513 [D loss: 0.596541, acc: 70.31%] [G loss: 2.639040]\n",
      "epoch:4 step:4514 [D loss: 0.673747, acc: 62.50%] [G loss: 2.412697]\n",
      "epoch:4 step:4515 [D loss: 0.655234, acc: 65.62%] [G loss: 2.486172]\n",
      "epoch:4 step:4516 [D loss: 0.689125, acc: 60.16%] [G loss: 2.332037]\n",
      "epoch:4 step:4517 [D loss: 0.488804, acc: 76.56%] [G loss: 2.596377]\n",
      "epoch:4 step:4518 [D loss: 0.658132, acc: 57.03%] [G loss: 2.593174]\n",
      "epoch:4 step:4519 [D loss: 0.627059, acc: 64.84%] [G loss: 2.486875]\n",
      "epoch:4 step:4520 [D loss: 0.548903, acc: 69.53%] [G loss: 2.740614]\n",
      "epoch:4 step:4521 [D loss: 0.564601, acc: 69.53%] [G loss: 2.856754]\n",
      "epoch:4 step:4522 [D loss: 0.539156, acc: 74.22%] [G loss: 2.864142]\n",
      "epoch:4 step:4523 [D loss: 0.542967, acc: 76.56%] [G loss: 3.101304]\n",
      "epoch:4 step:4524 [D loss: 0.579019, acc: 67.97%] [G loss: 2.899065]\n",
      "epoch:4 step:4525 [D loss: 0.546833, acc: 73.44%] [G loss: 2.554290]\n",
      "epoch:4 step:4526 [D loss: 0.553625, acc: 73.44%] [G loss: 2.587013]\n",
      "epoch:4 step:4527 [D loss: 0.570116, acc: 67.97%] [G loss: 2.718074]\n",
      "epoch:4 step:4528 [D loss: 0.620987, acc: 70.31%] [G loss: 3.089822]\n",
      "epoch:4 step:4529 [D loss: 0.542575, acc: 76.56%] [G loss: 3.069239]\n",
      "epoch:4 step:4530 [D loss: 0.623076, acc: 67.19%] [G loss: 3.150055]\n",
      "epoch:4 step:4531 [D loss: 0.539173, acc: 71.09%] [G loss: 2.818770]\n",
      "epoch:4 step:4532 [D loss: 0.583774, acc: 67.19%] [G loss: 2.625181]\n",
      "epoch:4 step:4533 [D loss: 0.544013, acc: 73.44%] [G loss: 2.789302]\n",
      "epoch:4 step:4534 [D loss: 0.570955, acc: 66.41%] [G loss: 2.982274]\n",
      "epoch:4 step:4535 [D loss: 0.603319, acc: 64.06%] [G loss: 2.855880]\n",
      "epoch:4 step:4536 [D loss: 0.654293, acc: 64.84%] [G loss: 2.581555]\n",
      "epoch:4 step:4537 [D loss: 0.608502, acc: 67.97%] [G loss: 2.683476]\n",
      "epoch:4 step:4538 [D loss: 0.559088, acc: 70.31%] [G loss: 2.778129]\n",
      "epoch:4 step:4539 [D loss: 0.525711, acc: 73.44%] [G loss: 2.814501]\n",
      "epoch:4 step:4540 [D loss: 0.543587, acc: 73.44%] [G loss: 2.975598]\n",
      "epoch:4 step:4541 [D loss: 0.586089, acc: 68.75%] [G loss: 2.621245]\n",
      "epoch:4 step:4542 [D loss: 0.663964, acc: 60.94%] [G loss: 2.526490]\n",
      "epoch:4 step:4543 [D loss: 0.574122, acc: 72.66%] [G loss: 2.801176]\n",
      "epoch:4 step:4544 [D loss: 0.561265, acc: 71.09%] [G loss: 2.587453]\n",
      "epoch:4 step:4545 [D loss: 0.620175, acc: 63.28%] [G loss: 2.657984]\n",
      "epoch:4 step:4546 [D loss: 0.626907, acc: 66.41%] [G loss: 2.982290]\n",
      "epoch:4 step:4547 [D loss: 0.668795, acc: 60.16%] [G loss: 2.559056]\n",
      "epoch:4 step:4548 [D loss: 0.666818, acc: 62.50%] [G loss: 2.632256]\n",
      "epoch:4 step:4549 [D loss: 0.650475, acc: 67.19%] [G loss: 2.491436]\n",
      "epoch:4 step:4550 [D loss: 0.624243, acc: 67.19%] [G loss: 2.558390]\n",
      "epoch:4 step:4551 [D loss: 0.506055, acc: 72.66%] [G loss: 2.936424]\n",
      "epoch:4 step:4552 [D loss: 0.537628, acc: 74.22%] [G loss: 2.934037]\n",
      "epoch:4 step:4553 [D loss: 0.592374, acc: 70.31%] [G loss: 2.859048]\n",
      "epoch:4 step:4554 [D loss: 0.535186, acc: 73.44%] [G loss: 3.344837]\n",
      "epoch:4 step:4555 [D loss: 0.472653, acc: 81.25%] [G loss: 2.901692]\n",
      "epoch:4 step:4556 [D loss: 0.600139, acc: 67.97%] [G loss: 2.562461]\n",
      "epoch:4 step:4557 [D loss: 0.531563, acc: 75.78%] [G loss: 3.042202]\n",
      "epoch:4 step:4558 [D loss: 0.576466, acc: 72.66%] [G loss: 2.357568]\n",
      "epoch:4 step:4559 [D loss: 0.544405, acc: 73.44%] [G loss: 2.732261]\n",
      "epoch:4 step:4560 [D loss: 0.550172, acc: 71.09%] [G loss: 2.725988]\n",
      "epoch:4 step:4561 [D loss: 0.618905, acc: 67.97%] [G loss: 2.577501]\n",
      "epoch:4 step:4562 [D loss: 0.551726, acc: 70.31%] [G loss: 2.859873]\n",
      "epoch:4 step:4563 [D loss: 0.621043, acc: 64.06%] [G loss: 2.895885]\n",
      "epoch:4 step:4564 [D loss: 0.555686, acc: 75.00%] [G loss: 2.889371]\n",
      "epoch:4 step:4565 [D loss: 0.680689, acc: 65.62%] [G loss: 2.719824]\n",
      "epoch:4 step:4566 [D loss: 0.643641, acc: 64.84%] [G loss: 2.601992]\n",
      "epoch:4 step:4567 [D loss: 0.569309, acc: 68.75%] [G loss: 2.571982]\n",
      "epoch:4 step:4568 [D loss: 0.592471, acc: 67.97%] [G loss: 2.446367]\n",
      "epoch:4 step:4569 [D loss: 0.583286, acc: 70.31%] [G loss: 2.863006]\n",
      "epoch:4 step:4570 [D loss: 0.575879, acc: 71.09%] [G loss: 2.878422]\n",
      "epoch:4 step:4571 [D loss: 0.554132, acc: 74.22%] [G loss: 2.844897]\n",
      "epoch:4 step:4572 [D loss: 0.594754, acc: 64.84%] [G loss: 2.433392]\n",
      "epoch:4 step:4573 [D loss: 0.553362, acc: 77.34%] [G loss: 2.921320]\n",
      "epoch:4 step:4574 [D loss: 0.636515, acc: 63.28%] [G loss: 2.545544]\n",
      "epoch:4 step:4575 [D loss: 0.640866, acc: 67.19%] [G loss: 2.524839]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:4 step:4576 [D loss: 0.646713, acc: 67.97%] [G loss: 2.230067]\n",
      "epoch:4 step:4577 [D loss: 0.593741, acc: 73.44%] [G loss: 2.633440]\n",
      "epoch:4 step:4578 [D loss: 0.596982, acc: 61.72%] [G loss: 2.718800]\n",
      "epoch:4 step:4579 [D loss: 0.538118, acc: 72.66%] [G loss: 3.166827]\n",
      "epoch:4 step:4580 [D loss: 0.551791, acc: 72.66%] [G loss: 2.895092]\n",
      "epoch:4 step:4581 [D loss: 0.604272, acc: 64.06%] [G loss: 2.875206]\n",
      "epoch:4 step:4582 [D loss: 0.466728, acc: 78.91%] [G loss: 3.010511]\n",
      "epoch:4 step:4583 [D loss: 0.536723, acc: 73.44%] [G loss: 2.638163]\n",
      "epoch:4 step:4584 [D loss: 0.558541, acc: 71.09%] [G loss: 2.992230]\n",
      "epoch:4 step:4585 [D loss: 0.587005, acc: 67.19%] [G loss: 2.862877]\n",
      "epoch:4 step:4586 [D loss: 0.535105, acc: 80.47%] [G loss: 2.807092]\n",
      "epoch:4 step:4587 [D loss: 0.633359, acc: 67.97%] [G loss: 2.829536]\n",
      "epoch:4 step:4588 [D loss: 0.616123, acc: 67.97%] [G loss: 2.470611]\n",
      "epoch:4 step:4589 [D loss: 0.591656, acc: 67.97%] [G loss: 2.645647]\n",
      "epoch:4 step:4590 [D loss: 0.631265, acc: 61.72%] [G loss: 2.717870]\n",
      "epoch:4 step:4591 [D loss: 0.497626, acc: 77.34%] [G loss: 2.756212]\n",
      "epoch:4 step:4592 [D loss: 0.675015, acc: 59.38%] [G loss: 2.421569]\n",
      "epoch:4 step:4593 [D loss: 0.532777, acc: 69.53%] [G loss: 2.692207]\n",
      "epoch:4 step:4594 [D loss: 0.643473, acc: 60.16%] [G loss: 2.572992]\n",
      "epoch:4 step:4595 [D loss: 0.575611, acc: 74.22%] [G loss: 2.844934]\n",
      "epoch:4 step:4596 [D loss: 0.505585, acc: 73.44%] [G loss: 3.013318]\n",
      "epoch:4 step:4597 [D loss: 0.576907, acc: 69.53%] [G loss: 2.757439]\n",
      "epoch:4 step:4598 [D loss: 0.569706, acc: 64.84%] [G loss: 2.931509]\n",
      "epoch:4 step:4599 [D loss: 0.652154, acc: 65.62%] [G loss: 2.366551]\n",
      "epoch:4 step:4600 [D loss: 0.561834, acc: 70.31%] [G loss: 2.560327]\n",
      "##############\n",
      "[2.92639217 1.61269973 6.86915472 5.24197227 4.16318666 5.90499208\n",
      " 5.11342873 5.04798052 5.34835691 3.70845739]\n",
      "##########\n",
      "epoch:4 step:4601 [D loss: 0.626096, acc: 66.41%] [G loss: 2.952057]\n",
      "epoch:4 step:4602 [D loss: 0.541748, acc: 69.53%] [G loss: 2.680759]\n",
      "epoch:4 step:4603 [D loss: 0.567105, acc: 66.41%] [G loss: 2.567925]\n",
      "epoch:4 step:4604 [D loss: 0.600262, acc: 67.19%] [G loss: 2.697585]\n",
      "epoch:4 step:4605 [D loss: 0.575194, acc: 67.97%] [G loss: 2.798502]\n",
      "epoch:4 step:4606 [D loss: 0.659667, acc: 63.28%] [G loss: 2.284210]\n",
      "epoch:4 step:4607 [D loss: 0.577566, acc: 67.19%] [G loss: 2.729796]\n",
      "epoch:4 step:4608 [D loss: 0.549745, acc: 72.66%] [G loss: 2.837019]\n",
      "epoch:4 step:4609 [D loss: 0.624035, acc: 60.94%] [G loss: 2.240755]\n",
      "epoch:4 step:4610 [D loss: 0.681834, acc: 66.41%] [G loss: 2.491632]\n",
      "epoch:4 step:4611 [D loss: 0.535282, acc: 70.31%] [G loss: 2.805245]\n",
      "epoch:4 step:4612 [D loss: 0.595331, acc: 74.22%] [G loss: 2.718762]\n",
      "epoch:4 step:4613 [D loss: 0.514808, acc: 78.91%] [G loss: 2.915638]\n",
      "epoch:4 step:4614 [D loss: 0.636656, acc: 67.97%] [G loss: 2.605727]\n",
      "epoch:4 step:4615 [D loss: 0.572276, acc: 71.09%] [G loss: 2.878935]\n",
      "epoch:4 step:4616 [D loss: 0.578113, acc: 69.53%] [G loss: 2.692255]\n",
      "epoch:4 step:4617 [D loss: 0.610163, acc: 64.84%] [G loss: 2.711995]\n",
      "epoch:4 step:4618 [D loss: 0.615775, acc: 69.53%] [G loss: 2.661309]\n",
      "epoch:4 step:4619 [D loss: 0.546261, acc: 73.44%] [G loss: 2.748993]\n",
      "epoch:4 step:4620 [D loss: 0.608490, acc: 64.06%] [G loss: 2.716660]\n",
      "epoch:4 step:4621 [D loss: 0.520196, acc: 75.00%] [G loss: 2.571017]\n",
      "epoch:4 step:4622 [D loss: 0.650293, acc: 67.97%] [G loss: 2.604442]\n",
      "epoch:4 step:4623 [D loss: 0.503179, acc: 74.22%] [G loss: 2.679032]\n",
      "epoch:4 step:4624 [D loss: 0.602828, acc: 70.31%] [G loss: 2.982684]\n",
      "epoch:4 step:4625 [D loss: 0.452530, acc: 82.03%] [G loss: 3.054982]\n",
      "epoch:4 step:4626 [D loss: 0.659385, acc: 59.38%] [G loss: 2.564130]\n",
      "epoch:4 step:4627 [D loss: 0.558003, acc: 71.88%] [G loss: 2.689866]\n",
      "epoch:4 step:4628 [D loss: 0.587546, acc: 72.66%] [G loss: 2.585963]\n",
      "epoch:4 step:4629 [D loss: 0.607775, acc: 69.53%] [G loss: 2.763799]\n",
      "epoch:4 step:4630 [D loss: 0.605965, acc: 71.88%] [G loss: 2.589896]\n",
      "epoch:4 step:4631 [D loss: 0.601887, acc: 64.84%] [G loss: 2.899092]\n",
      "epoch:4 step:4632 [D loss: 0.581658, acc: 67.19%] [G loss: 3.177958]\n",
      "epoch:4 step:4633 [D loss: 0.525900, acc: 73.44%] [G loss: 2.926756]\n",
      "epoch:4 step:4634 [D loss: 0.601727, acc: 67.19%] [G loss: 3.204623]\n",
      "epoch:4 step:4635 [D loss: 0.602675, acc: 65.62%] [G loss: 2.630094]\n",
      "epoch:4 step:4636 [D loss: 0.630251, acc: 66.41%] [G loss: 2.673555]\n",
      "epoch:4 step:4637 [D loss: 0.555844, acc: 70.31%] [G loss: 2.951928]\n",
      "epoch:4 step:4638 [D loss: 0.466041, acc: 78.91%] [G loss: 2.963998]\n",
      "epoch:4 step:4639 [D loss: 0.620565, acc: 66.41%] [G loss: 2.652515]\n",
      "epoch:4 step:4640 [D loss: 0.648755, acc: 63.28%] [G loss: 2.365897]\n",
      "epoch:4 step:4641 [D loss: 0.568814, acc: 70.31%] [G loss: 2.623837]\n",
      "epoch:4 step:4642 [D loss: 0.602159, acc: 71.09%] [G loss: 2.912926]\n",
      "epoch:4 step:4643 [D loss: 0.595189, acc: 68.75%] [G loss: 2.895669]\n",
      "epoch:4 step:4644 [D loss: 0.604574, acc: 66.41%] [G loss: 2.792997]\n",
      "epoch:4 step:4645 [D loss: 0.539660, acc: 77.34%] [G loss: 2.713473]\n",
      "epoch:4 step:4646 [D loss: 0.618867, acc: 67.97%] [G loss: 2.757947]\n",
      "epoch:4 step:4647 [D loss: 0.603755, acc: 66.41%] [G loss: 2.709610]\n",
      "epoch:4 step:4648 [D loss: 0.558884, acc: 70.31%] [G loss: 2.959839]\n",
      "epoch:4 step:4649 [D loss: 0.527518, acc: 71.88%] [G loss: 2.817773]\n",
      "epoch:4 step:4650 [D loss: 0.628003, acc: 69.53%] [G loss: 2.290527]\n",
      "epoch:4 step:4651 [D loss: 0.518213, acc: 75.78%] [G loss: 2.919803]\n",
      "epoch:4 step:4652 [D loss: 0.570441, acc: 67.19%] [G loss: 2.896292]\n",
      "epoch:4 step:4653 [D loss: 0.614947, acc: 67.19%] [G loss: 2.713187]\n",
      "epoch:4 step:4654 [D loss: 0.516132, acc: 74.22%] [G loss: 2.937549]\n",
      "epoch:4 step:4655 [D loss: 0.575924, acc: 70.31%] [G loss: 2.837235]\n",
      "epoch:4 step:4656 [D loss: 0.517734, acc: 75.78%] [G loss: 2.667348]\n",
      "epoch:4 step:4657 [D loss: 0.476843, acc: 81.25%] [G loss: 3.127599]\n",
      "epoch:4 step:4658 [D loss: 0.556959, acc: 67.19%] [G loss: 2.921107]\n",
      "epoch:4 step:4659 [D loss: 0.525485, acc: 71.09%] [G loss: 3.164274]\n",
      "epoch:4 step:4660 [D loss: 0.514609, acc: 71.09%] [G loss: 3.278924]\n",
      "epoch:4 step:4661 [D loss: 0.627943, acc: 64.06%] [G loss: 2.522284]\n",
      "epoch:4 step:4662 [D loss: 0.662006, acc: 59.38%] [G loss: 2.848394]\n",
      "epoch:4 step:4663 [D loss: 0.520356, acc: 73.44%] [G loss: 2.904958]\n",
      "epoch:4 step:4664 [D loss: 0.597049, acc: 67.97%] [G loss: 2.914356]\n",
      "epoch:4 step:4665 [D loss: 0.621503, acc: 71.09%] [G loss: 2.796353]\n",
      "epoch:4 step:4666 [D loss: 0.623136, acc: 67.19%] [G loss: 2.902508]\n",
      "epoch:4 step:4667 [D loss: 0.589739, acc: 70.31%] [G loss: 2.740543]\n",
      "epoch:4 step:4668 [D loss: 0.643022, acc: 65.62%] [G loss: 3.050699]\n",
      "epoch:4 step:4669 [D loss: 0.496101, acc: 78.91%] [G loss: 3.316914]\n",
      "epoch:4 step:4670 [D loss: 0.563720, acc: 70.31%] [G loss: 2.723962]\n",
      "epoch:4 step:4671 [D loss: 0.567727, acc: 71.88%] [G loss: 3.268016]\n",
      "epoch:4 step:4672 [D loss: 0.460789, acc: 81.25%] [G loss: 3.314839]\n",
      "epoch:4 step:4673 [D loss: 0.437742, acc: 78.91%] [G loss: 3.374646]\n",
      "epoch:4 step:4674 [D loss: 0.603437, acc: 67.19%] [G loss: 3.195343]\n",
      "epoch:4 step:4675 [D loss: 0.553808, acc: 72.66%] [G loss: 3.122118]\n",
      "epoch:4 step:4676 [D loss: 0.766579, acc: 57.81%] [G loss: 2.730266]\n",
      "epoch:4 step:4677 [D loss: 0.643075, acc: 67.97%] [G loss: 3.164371]\n",
      "epoch:4 step:4678 [D loss: 0.510291, acc: 77.34%] [G loss: 2.745903]\n",
      "epoch:4 step:4679 [D loss: 0.503946, acc: 75.78%] [G loss: 2.979822]\n",
      "epoch:4 step:4680 [D loss: 0.568997, acc: 75.00%] [G loss: 2.715451]\n",
      "epoch:4 step:4681 [D loss: 0.524016, acc: 78.91%] [G loss: 3.212544]\n",
      "epoch:4 step:4682 [D loss: 0.505457, acc: 78.91%] [G loss: 3.150916]\n",
      "epoch:4 step:4683 [D loss: 0.622434, acc: 67.19%] [G loss: 2.922255]\n",
      "epoch:4 step:4684 [D loss: 0.541681, acc: 72.66%] [G loss: 2.897793]\n",
      "epoch:4 step:4685 [D loss: 0.486383, acc: 78.12%] [G loss: 3.492200]\n",
      "epoch:5 step:4686 [D loss: 0.534622, acc: 70.31%] [G loss: 3.044069]\n",
      "epoch:5 step:4687 [D loss: 0.553109, acc: 71.09%] [G loss: 3.206911]\n",
      "epoch:5 step:4688 [D loss: 0.671526, acc: 66.41%] [G loss: 2.736786]\n",
      "epoch:5 step:4689 [D loss: 0.574416, acc: 67.19%] [G loss: 2.645902]\n",
      "epoch:5 step:4690 [D loss: 0.598566, acc: 67.97%] [G loss: 2.715700]\n",
      "epoch:5 step:4691 [D loss: 0.563988, acc: 71.09%] [G loss: 3.057985]\n",
      "epoch:5 step:4692 [D loss: 0.511537, acc: 78.12%] [G loss: 2.934389]\n",
      "epoch:5 step:4693 [D loss: 0.548687, acc: 73.44%] [G loss: 3.162159]\n",
      "epoch:5 step:4694 [D loss: 0.527010, acc: 73.44%] [G loss: 3.125321]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:5 step:4695 [D loss: 0.610400, acc: 65.62%] [G loss: 3.278968]\n",
      "epoch:5 step:4696 [D loss: 0.648131, acc: 67.19%] [G loss: 2.689445]\n",
      "epoch:5 step:4697 [D loss: 0.605238, acc: 71.88%] [G loss: 2.873324]\n",
      "epoch:5 step:4698 [D loss: 0.537850, acc: 75.78%] [G loss: 2.845229]\n",
      "epoch:5 step:4699 [D loss: 0.629208, acc: 64.84%] [G loss: 2.602592]\n",
      "epoch:5 step:4700 [D loss: 0.561460, acc: 75.00%] [G loss: 3.040957]\n",
      "epoch:5 step:4701 [D loss: 0.575826, acc: 68.75%] [G loss: 2.749985]\n",
      "epoch:5 step:4702 [D loss: 0.659354, acc: 64.06%] [G loss: 2.561027]\n",
      "epoch:5 step:4703 [D loss: 0.709766, acc: 59.38%] [G loss: 2.642084]\n",
      "epoch:5 step:4704 [D loss: 0.744220, acc: 57.81%] [G loss: 2.323195]\n",
      "epoch:5 step:4705 [D loss: 0.711421, acc: 60.94%] [G loss: 2.075775]\n",
      "epoch:5 step:4706 [D loss: 0.629789, acc: 60.94%] [G loss: 2.205360]\n",
      "epoch:5 step:4707 [D loss: 0.590959, acc: 71.09%] [G loss: 2.671771]\n",
      "epoch:5 step:4708 [D loss: 0.523747, acc: 72.66%] [G loss: 2.459235]\n",
      "epoch:5 step:4709 [D loss: 0.548740, acc: 73.44%] [G loss: 2.855539]\n",
      "epoch:5 step:4710 [D loss: 0.617766, acc: 67.97%] [G loss: 2.859678]\n",
      "epoch:5 step:4711 [D loss: 0.596910, acc: 71.09%] [G loss: 2.527864]\n",
      "epoch:5 step:4712 [D loss: 0.572688, acc: 66.41%] [G loss: 2.805194]\n",
      "epoch:5 step:4713 [D loss: 0.594131, acc: 66.41%] [G loss: 2.356195]\n",
      "epoch:5 step:4714 [D loss: 0.617346, acc: 65.62%] [G loss: 2.486721]\n",
      "epoch:5 step:4715 [D loss: 0.601742, acc: 65.62%] [G loss: 2.402109]\n",
      "epoch:5 step:4716 [D loss: 0.533883, acc: 71.88%] [G loss: 2.589277]\n",
      "epoch:5 step:4717 [D loss: 0.630879, acc: 64.84%] [G loss: 2.386078]\n",
      "epoch:5 step:4718 [D loss: 0.657621, acc: 63.28%] [G loss: 2.899649]\n",
      "epoch:5 step:4719 [D loss: 0.602541, acc: 67.97%] [G loss: 2.536251]\n",
      "epoch:5 step:4720 [D loss: 0.564177, acc: 69.53%] [G loss: 2.981390]\n",
      "epoch:5 step:4721 [D loss: 0.504576, acc: 77.34%] [G loss: 2.960157]\n",
      "epoch:5 step:4722 [D loss: 0.537531, acc: 76.56%] [G loss: 3.082714]\n",
      "epoch:5 step:4723 [D loss: 0.699351, acc: 60.16%] [G loss: 2.533062]\n",
      "epoch:5 step:4724 [D loss: 0.590729, acc: 69.53%] [G loss: 3.012987]\n",
      "epoch:5 step:4725 [D loss: 0.514707, acc: 71.88%] [G loss: 2.872657]\n",
      "epoch:5 step:4726 [D loss: 0.597940, acc: 67.19%] [G loss: 2.728308]\n",
      "epoch:5 step:4727 [D loss: 0.563409, acc: 71.09%] [G loss: 2.871902]\n",
      "epoch:5 step:4728 [D loss: 0.522861, acc: 77.34%] [G loss: 2.821310]\n",
      "epoch:5 step:4729 [D loss: 0.612523, acc: 64.84%] [G loss: 2.609620]\n",
      "epoch:5 step:4730 [D loss: 0.620815, acc: 72.66%] [G loss: 2.462578]\n",
      "epoch:5 step:4731 [D loss: 0.569473, acc: 67.97%] [G loss: 2.481152]\n",
      "epoch:5 step:4732 [D loss: 0.534512, acc: 70.31%] [G loss: 3.122203]\n",
      "epoch:5 step:4733 [D loss: 0.585205, acc: 64.84%] [G loss: 2.555362]\n",
      "epoch:5 step:4734 [D loss: 0.565846, acc: 66.41%] [G loss: 2.803248]\n",
      "epoch:5 step:4735 [D loss: 0.656242, acc: 64.84%] [G loss: 2.451654]\n",
      "epoch:5 step:4736 [D loss: 0.529005, acc: 71.09%] [G loss: 2.697067]\n",
      "epoch:5 step:4737 [D loss: 0.594395, acc: 69.53%] [G loss: 2.549696]\n",
      "epoch:5 step:4738 [D loss: 0.587705, acc: 67.97%] [G loss: 2.692155]\n",
      "epoch:5 step:4739 [D loss: 0.515529, acc: 71.09%] [G loss: 2.845936]\n",
      "epoch:5 step:4740 [D loss: 0.507867, acc: 73.44%] [G loss: 3.289124]\n",
      "epoch:5 step:4741 [D loss: 0.598851, acc: 71.88%] [G loss: 2.687793]\n",
      "epoch:5 step:4742 [D loss: 0.662186, acc: 66.41%] [G loss: 2.704520]\n",
      "epoch:5 step:4743 [D loss: 0.636651, acc: 71.88%] [G loss: 2.629671]\n",
      "epoch:5 step:4744 [D loss: 0.515422, acc: 72.66%] [G loss: 2.604132]\n",
      "epoch:5 step:4745 [D loss: 0.493049, acc: 76.56%] [G loss: 2.893036]\n",
      "epoch:5 step:4746 [D loss: 0.583207, acc: 71.88%] [G loss: 2.716688]\n",
      "epoch:5 step:4747 [D loss: 0.585101, acc: 67.97%] [G loss: 2.772604]\n",
      "epoch:5 step:4748 [D loss: 0.616473, acc: 71.09%] [G loss: 2.629251]\n",
      "epoch:5 step:4749 [D loss: 0.562347, acc: 70.31%] [G loss: 2.836444]\n",
      "epoch:5 step:4750 [D loss: 0.647538, acc: 64.84%] [G loss: 2.740367]\n",
      "epoch:5 step:4751 [D loss: 0.631019, acc: 65.62%] [G loss: 2.517113]\n",
      "epoch:5 step:4752 [D loss: 0.566933, acc: 68.75%] [G loss: 2.456013]\n",
      "epoch:5 step:4753 [D loss: 0.542540, acc: 71.88%] [G loss: 2.847258]\n",
      "epoch:5 step:4754 [D loss: 0.545603, acc: 71.88%] [G loss: 2.758471]\n",
      "epoch:5 step:4755 [D loss: 0.497755, acc: 77.34%] [G loss: 3.003987]\n",
      "epoch:5 step:4756 [D loss: 0.513015, acc: 76.56%] [G loss: 2.958185]\n",
      "epoch:5 step:4757 [D loss: 0.568330, acc: 68.75%] [G loss: 2.918393]\n",
      "epoch:5 step:4758 [D loss: 0.597271, acc: 71.09%] [G loss: 2.686073]\n",
      "epoch:5 step:4759 [D loss: 0.534542, acc: 73.44%] [G loss: 2.839972]\n",
      "epoch:5 step:4760 [D loss: 0.476374, acc: 79.69%] [G loss: 3.746673]\n",
      "epoch:5 step:4761 [D loss: 0.567106, acc: 69.53%] [G loss: 3.125944]\n",
      "epoch:5 step:4762 [D loss: 0.418894, acc: 78.91%] [G loss: 3.707896]\n",
      "epoch:5 step:4763 [D loss: 0.682745, acc: 63.28%] [G loss: 2.731083]\n",
      "epoch:5 step:4764 [D loss: 0.611945, acc: 67.19%] [G loss: 2.615056]\n",
      "epoch:5 step:4765 [D loss: 0.619664, acc: 68.75%] [G loss: 2.444583]\n",
      "epoch:5 step:4766 [D loss: 0.695269, acc: 55.47%] [G loss: 2.495080]\n",
      "epoch:5 step:4767 [D loss: 0.452975, acc: 81.25%] [G loss: 2.833940]\n",
      "epoch:5 step:4768 [D loss: 0.565027, acc: 70.31%] [G loss: 2.525124]\n",
      "epoch:5 step:4769 [D loss: 0.649890, acc: 63.28%] [G loss: 2.189255]\n",
      "epoch:5 step:4770 [D loss: 0.530480, acc: 73.44%] [G loss: 2.750554]\n",
      "epoch:5 step:4771 [D loss: 0.568242, acc: 75.78%] [G loss: 2.688694]\n",
      "epoch:5 step:4772 [D loss: 0.529975, acc: 73.44%] [G loss: 2.402534]\n",
      "epoch:5 step:4773 [D loss: 0.604096, acc: 68.75%] [G loss: 2.526551]\n",
      "epoch:5 step:4774 [D loss: 0.545619, acc: 74.22%] [G loss: 2.866767]\n",
      "epoch:5 step:4775 [D loss: 0.593659, acc: 67.97%] [G loss: 2.792048]\n",
      "epoch:5 step:4776 [D loss: 0.524465, acc: 74.22%] [G loss: 2.865353]\n",
      "epoch:5 step:4777 [D loss: 0.630189, acc: 65.62%] [G loss: 2.904296]\n",
      "epoch:5 step:4778 [D loss: 0.551085, acc: 69.53%] [G loss: 2.907424]\n",
      "epoch:5 step:4779 [D loss: 0.611043, acc: 64.84%] [G loss: 2.470678]\n",
      "epoch:5 step:4780 [D loss: 0.624361, acc: 66.41%] [G loss: 2.527694]\n",
      "epoch:5 step:4781 [D loss: 0.550349, acc: 73.44%] [G loss: 2.605443]\n",
      "epoch:5 step:4782 [D loss: 0.476058, acc: 80.47%] [G loss: 2.672845]\n",
      "epoch:5 step:4783 [D loss: 0.558624, acc: 71.88%] [G loss: 2.828710]\n",
      "epoch:5 step:4784 [D loss: 0.572598, acc: 69.53%] [G loss: 2.412492]\n",
      "epoch:5 step:4785 [D loss: 0.644420, acc: 67.97%] [G loss: 2.692147]\n",
      "epoch:5 step:4786 [D loss: 0.592422, acc: 68.75%] [G loss: 2.730259]\n",
      "epoch:5 step:4787 [D loss: 0.671802, acc: 59.38%] [G loss: 2.656892]\n",
      "epoch:5 step:4788 [D loss: 0.462282, acc: 78.91%] [G loss: 2.816311]\n",
      "epoch:5 step:4789 [D loss: 0.646806, acc: 67.97%] [G loss: 2.391635]\n",
      "epoch:5 step:4790 [D loss: 0.566980, acc: 70.31%] [G loss: 2.942277]\n",
      "epoch:5 step:4791 [D loss: 0.620622, acc: 64.06%] [G loss: 2.509917]\n",
      "epoch:5 step:4792 [D loss: 0.607108, acc: 64.06%] [G loss: 2.651651]\n",
      "epoch:5 step:4793 [D loss: 0.662141, acc: 62.50%] [G loss: 2.673145]\n",
      "epoch:5 step:4794 [D loss: 0.714152, acc: 60.16%] [G loss: 2.458427]\n",
      "epoch:5 step:4795 [D loss: 0.652613, acc: 61.72%] [G loss: 2.612111]\n",
      "epoch:5 step:4796 [D loss: 0.574814, acc: 71.09%] [G loss: 2.705841]\n",
      "epoch:5 step:4797 [D loss: 0.583633, acc: 67.19%] [G loss: 2.674486]\n",
      "epoch:5 step:4798 [D loss: 0.634348, acc: 63.28%] [G loss: 2.582255]\n",
      "epoch:5 step:4799 [D loss: 0.636588, acc: 59.38%] [G loss: 2.612889]\n",
      "epoch:5 step:4800 [D loss: 0.646818, acc: 65.62%] [G loss: 2.711205]\n",
      "##############\n",
      "[2.86642248 1.67614321 6.82626664 5.16616556 4.04943793 5.9113717\n",
      " 5.08718441 4.94813027 5.31234041 3.54847672]\n",
      "##########\n",
      "epoch:5 step:4801 [D loss: 0.524876, acc: 75.78%] [G loss: 2.691048]\n",
      "epoch:5 step:4802 [D loss: 0.560558, acc: 67.19%] [G loss: 3.228122]\n",
      "epoch:5 step:4803 [D loss: 0.549271, acc: 68.75%] [G loss: 2.821579]\n",
      "epoch:5 step:4804 [D loss: 0.487786, acc: 78.12%] [G loss: 3.262665]\n",
      "epoch:5 step:4805 [D loss: 0.545110, acc: 71.88%] [G loss: 3.095445]\n",
      "epoch:5 step:4806 [D loss: 0.663450, acc: 60.94%] [G loss: 2.543503]\n",
      "epoch:5 step:4807 [D loss: 0.583536, acc: 67.19%] [G loss: 3.144936]\n",
      "epoch:5 step:4808 [D loss: 0.631883, acc: 60.94%] [G loss: 2.772761]\n",
      "epoch:5 step:4809 [D loss: 0.694259, acc: 54.69%] [G loss: 2.563371]\n",
      "epoch:5 step:4810 [D loss: 0.614467, acc: 65.62%] [G loss: 2.339155]\n",
      "epoch:5 step:4811 [D loss: 0.718690, acc: 64.84%] [G loss: 2.696521]\n",
      "epoch:5 step:4812 [D loss: 0.657994, acc: 64.06%] [G loss: 2.320054]\n",
      "epoch:5 step:4813 [D loss: 0.604554, acc: 66.41%] [G loss: 2.711534]\n",
      "epoch:5 step:4814 [D loss: 0.573655, acc: 71.88%] [G loss: 2.487359]\n",
      "epoch:5 step:4815 [D loss: 0.538187, acc: 72.66%] [G loss: 2.718877]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:5 step:4816 [D loss: 0.596497, acc: 68.75%] [G loss: 2.414291]\n",
      "epoch:5 step:4817 [D loss: 0.528677, acc: 73.44%] [G loss: 2.478510]\n",
      "epoch:5 step:4818 [D loss: 0.724165, acc: 55.47%] [G loss: 2.404484]\n",
      "epoch:5 step:4819 [D loss: 0.656679, acc: 64.06%] [G loss: 2.442338]\n",
      "epoch:5 step:4820 [D loss: 0.534722, acc: 71.88%] [G loss: 2.522038]\n",
      "epoch:5 step:4821 [D loss: 0.578643, acc: 70.31%] [G loss: 2.357887]\n",
      "epoch:5 step:4822 [D loss: 0.643553, acc: 65.62%] [G loss: 2.154614]\n",
      "epoch:5 step:4823 [D loss: 0.563555, acc: 71.09%] [G loss: 2.541425]\n",
      "epoch:5 step:4824 [D loss: 0.599085, acc: 69.53%] [G loss: 2.399538]\n",
      "epoch:5 step:4825 [D loss: 0.625930, acc: 65.62%] [G loss: 2.437549]\n",
      "epoch:5 step:4826 [D loss: 0.611387, acc: 66.41%] [G loss: 2.489931]\n",
      "epoch:5 step:4827 [D loss: 0.591007, acc: 66.41%] [G loss: 2.427749]\n",
      "epoch:5 step:4828 [D loss: 0.612215, acc: 67.19%] [G loss: 2.750864]\n",
      "epoch:5 step:4829 [D loss: 0.561510, acc: 70.31%] [G loss: 2.857555]\n",
      "epoch:5 step:4830 [D loss: 0.615708, acc: 68.75%] [G loss: 2.766284]\n",
      "epoch:5 step:4831 [D loss: 0.616710, acc: 71.09%] [G loss: 2.550824]\n",
      "epoch:5 step:4832 [D loss: 0.559905, acc: 67.97%] [G loss: 2.313366]\n",
      "epoch:5 step:4833 [D loss: 0.590854, acc: 67.97%] [G loss: 2.440507]\n",
      "epoch:5 step:4834 [D loss: 0.539542, acc: 75.00%] [G loss: 2.663401]\n",
      "epoch:5 step:4835 [D loss: 0.626555, acc: 69.53%] [G loss: 2.828321]\n",
      "epoch:5 step:4836 [D loss: 0.549774, acc: 70.31%] [G loss: 2.844568]\n",
      "epoch:5 step:4837 [D loss: 0.619248, acc: 69.53%] [G loss: 2.768000]\n",
      "epoch:5 step:4838 [D loss: 0.633502, acc: 65.62%] [G loss: 2.645908]\n",
      "epoch:5 step:4839 [D loss: 0.520113, acc: 78.12%] [G loss: 2.753956]\n",
      "epoch:5 step:4840 [D loss: 0.564242, acc: 67.19%] [G loss: 2.469488]\n",
      "epoch:5 step:4841 [D loss: 0.601730, acc: 67.97%] [G loss: 2.418351]\n",
      "epoch:5 step:4842 [D loss: 0.546712, acc: 73.44%] [G loss: 2.443834]\n",
      "epoch:5 step:4843 [D loss: 0.561904, acc: 71.09%] [G loss: 2.744584]\n",
      "epoch:5 step:4844 [D loss: 0.551323, acc: 72.66%] [G loss: 2.525424]\n",
      "epoch:5 step:4845 [D loss: 0.639564, acc: 65.62%] [G loss: 2.182909]\n",
      "epoch:5 step:4846 [D loss: 0.671653, acc: 63.28%] [G loss: 2.661831]\n",
      "epoch:5 step:4847 [D loss: 0.548324, acc: 71.09%] [G loss: 2.663824]\n",
      "epoch:5 step:4848 [D loss: 0.647533, acc: 64.06%] [G loss: 2.487243]\n",
      "epoch:5 step:4849 [D loss: 0.571037, acc: 70.31%] [G loss: 2.238793]\n",
      "epoch:5 step:4850 [D loss: 0.538202, acc: 73.44%] [G loss: 2.442100]\n",
      "epoch:5 step:4851 [D loss: 0.621004, acc: 64.84%] [G loss: 2.599168]\n",
      "epoch:5 step:4852 [D loss: 0.595309, acc: 67.19%] [G loss: 2.583962]\n",
      "epoch:5 step:4853 [D loss: 0.576675, acc: 71.88%] [G loss: 2.634074]\n",
      "epoch:5 step:4854 [D loss: 0.619464, acc: 64.06%] [G loss: 2.516436]\n",
      "epoch:5 step:4855 [D loss: 0.579231, acc: 62.50%] [G loss: 2.453431]\n",
      "epoch:5 step:4856 [D loss: 0.488528, acc: 75.78%] [G loss: 2.680954]\n",
      "epoch:5 step:4857 [D loss: 0.594041, acc: 72.66%] [G loss: 2.539011]\n",
      "epoch:5 step:4858 [D loss: 0.547216, acc: 75.78%] [G loss: 2.669167]\n",
      "epoch:5 step:4859 [D loss: 0.600063, acc: 64.84%] [G loss: 2.968260]\n",
      "epoch:5 step:4860 [D loss: 0.586621, acc: 68.75%] [G loss: 2.716256]\n",
      "epoch:5 step:4861 [D loss: 0.522951, acc: 71.88%] [G loss: 2.813122]\n",
      "epoch:5 step:4862 [D loss: 0.624940, acc: 64.06%] [G loss: 2.666603]\n",
      "epoch:5 step:4863 [D loss: 0.584059, acc: 71.88%] [G loss: 2.794253]\n",
      "epoch:5 step:4864 [D loss: 0.615812, acc: 64.84%] [G loss: 2.586740]\n",
      "epoch:5 step:4865 [D loss: 0.576104, acc: 68.75%] [G loss: 2.556817]\n",
      "epoch:5 step:4866 [D loss: 0.535905, acc: 75.00%] [G loss: 2.676126]\n",
      "epoch:5 step:4867 [D loss: 0.599121, acc: 65.62%] [G loss: 2.507328]\n",
      "epoch:5 step:4868 [D loss: 0.583339, acc: 71.88%] [G loss: 2.868168]\n",
      "epoch:5 step:4869 [D loss: 0.602626, acc: 60.16%] [G loss: 2.709548]\n",
      "epoch:5 step:4870 [D loss: 0.593420, acc: 71.09%] [G loss: 2.522398]\n",
      "epoch:5 step:4871 [D loss: 0.598285, acc: 70.31%] [G loss: 2.669608]\n",
      "epoch:5 step:4872 [D loss: 0.625716, acc: 63.28%] [G loss: 2.595788]\n",
      "epoch:5 step:4873 [D loss: 0.573976, acc: 71.88%] [G loss: 2.470258]\n",
      "epoch:5 step:4874 [D loss: 0.593554, acc: 68.75%] [G loss: 2.628150]\n",
      "epoch:5 step:4875 [D loss: 0.520423, acc: 71.88%] [G loss: 2.607908]\n",
      "epoch:5 step:4876 [D loss: 0.584711, acc: 70.31%] [G loss: 2.613204]\n",
      "epoch:5 step:4877 [D loss: 0.554301, acc: 75.00%] [G loss: 2.857297]\n",
      "epoch:5 step:4878 [D loss: 0.571942, acc: 69.53%] [G loss: 3.031318]\n",
      "epoch:5 step:4879 [D loss: 0.482436, acc: 77.34%] [G loss: 3.609584]\n",
      "epoch:5 step:4880 [D loss: 0.551296, acc: 73.44%] [G loss: 2.709927]\n",
      "epoch:5 step:4881 [D loss: 0.634226, acc: 65.62%] [G loss: 2.749151]\n",
      "epoch:5 step:4882 [D loss: 0.542687, acc: 73.44%] [G loss: 2.779373]\n",
      "epoch:5 step:4883 [D loss: 0.484081, acc: 75.78%] [G loss: 2.938018]\n",
      "epoch:5 step:4884 [D loss: 0.717611, acc: 57.03%] [G loss: 2.785923]\n",
      "epoch:5 step:4885 [D loss: 0.600086, acc: 69.53%] [G loss: 2.589769]\n",
      "epoch:5 step:4886 [D loss: 0.582453, acc: 71.88%] [G loss: 2.495591]\n",
      "epoch:5 step:4887 [D loss: 0.544855, acc: 72.66%] [G loss: 2.716977]\n",
      "epoch:5 step:4888 [D loss: 0.682283, acc: 62.50%] [G loss: 2.312953]\n",
      "epoch:5 step:4889 [D loss: 0.575129, acc: 72.66%] [G loss: 2.466121]\n",
      "epoch:5 step:4890 [D loss: 0.581309, acc: 70.31%] [G loss: 2.474250]\n",
      "epoch:5 step:4891 [D loss: 0.566738, acc: 70.31%] [G loss: 2.744883]\n",
      "epoch:5 step:4892 [D loss: 0.543256, acc: 71.88%] [G loss: 3.048097]\n",
      "epoch:5 step:4893 [D loss: 0.528972, acc: 74.22%] [G loss: 3.075410]\n",
      "epoch:5 step:4894 [D loss: 0.466273, acc: 76.56%] [G loss: 3.295717]\n",
      "epoch:5 step:4895 [D loss: 0.602006, acc: 67.97%] [G loss: 2.687581]\n",
      "epoch:5 step:4896 [D loss: 0.554179, acc: 71.88%] [G loss: 2.554424]\n",
      "epoch:5 step:4897 [D loss: 0.595077, acc: 70.31%] [G loss: 2.847567]\n",
      "epoch:5 step:4898 [D loss: 0.601059, acc: 65.62%] [G loss: 2.620623]\n",
      "epoch:5 step:4899 [D loss: 0.612335, acc: 67.97%] [G loss: 2.339010]\n",
      "epoch:5 step:4900 [D loss: 0.602389, acc: 71.09%] [G loss: 2.749549]\n",
      "epoch:5 step:4901 [D loss: 0.678261, acc: 61.72%] [G loss: 2.424994]\n",
      "epoch:5 step:4902 [D loss: 0.561006, acc: 70.31%] [G loss: 2.787410]\n",
      "epoch:5 step:4903 [D loss: 0.525229, acc: 69.53%] [G loss: 3.131194]\n",
      "epoch:5 step:4904 [D loss: 0.630973, acc: 65.62%] [G loss: 2.670174]\n",
      "epoch:5 step:4905 [D loss: 0.633624, acc: 65.62%] [G loss: 2.479405]\n",
      "epoch:5 step:4906 [D loss: 0.562659, acc: 69.53%] [G loss: 2.760493]\n",
      "epoch:5 step:4907 [D loss: 0.527603, acc: 68.75%] [G loss: 2.903675]\n",
      "epoch:5 step:4908 [D loss: 0.598109, acc: 65.62%] [G loss: 2.876740]\n",
      "epoch:5 step:4909 [D loss: 0.582090, acc: 67.19%] [G loss: 2.879239]\n",
      "epoch:5 step:4910 [D loss: 0.596506, acc: 67.97%] [G loss: 2.527078]\n",
      "epoch:5 step:4911 [D loss: 0.669362, acc: 57.81%] [G loss: 2.613034]\n",
      "epoch:5 step:4912 [D loss: 0.539129, acc: 74.22%] [G loss: 2.310112]\n",
      "epoch:5 step:4913 [D loss: 0.630449, acc: 61.72%] [G loss: 2.385284]\n",
      "epoch:5 step:4914 [D loss: 0.622680, acc: 62.50%] [G loss: 2.747480]\n",
      "epoch:5 step:4915 [D loss: 0.504203, acc: 75.78%] [G loss: 3.122313]\n",
      "epoch:5 step:4916 [D loss: 0.677482, acc: 65.62%] [G loss: 3.307946]\n",
      "epoch:5 step:4917 [D loss: 0.497920, acc: 76.56%] [G loss: 3.121549]\n",
      "epoch:5 step:4918 [D loss: 0.597756, acc: 70.31%] [G loss: 2.887001]\n",
      "epoch:5 step:4919 [D loss: 0.539701, acc: 69.53%] [G loss: 2.726186]\n",
      "epoch:5 step:4920 [D loss: 0.585039, acc: 71.09%] [G loss: 2.732143]\n",
      "epoch:5 step:4921 [D loss: 0.613979, acc: 68.75%] [G loss: 2.894657]\n",
      "epoch:5 step:4922 [D loss: 0.524460, acc: 75.00%] [G loss: 2.656194]\n",
      "epoch:5 step:4923 [D loss: 0.626769, acc: 65.62%] [G loss: 2.567336]\n",
      "epoch:5 step:4924 [D loss: 0.569323, acc: 68.75%] [G loss: 2.769562]\n",
      "epoch:5 step:4925 [D loss: 0.620500, acc: 71.09%] [G loss: 2.744749]\n",
      "epoch:5 step:4926 [D loss: 0.550232, acc: 75.00%] [G loss: 2.666612]\n",
      "epoch:5 step:4927 [D loss: 0.584640, acc: 69.53%] [G loss: 2.768429]\n",
      "epoch:5 step:4928 [D loss: 0.548767, acc: 71.88%] [G loss: 2.709794]\n",
      "epoch:5 step:4929 [D loss: 0.600816, acc: 65.62%] [G loss: 2.568786]\n",
      "epoch:5 step:4930 [D loss: 0.555538, acc: 71.09%] [G loss: 2.664167]\n",
      "epoch:5 step:4931 [D loss: 0.619329, acc: 65.62%] [G loss: 2.522380]\n",
      "epoch:5 step:4932 [D loss: 0.666718, acc: 60.94%] [G loss: 2.607199]\n",
      "epoch:5 step:4933 [D loss: 0.642657, acc: 71.09%] [G loss: 2.530690]\n",
      "epoch:5 step:4934 [D loss: 0.600240, acc: 66.41%] [G loss: 2.603770]\n",
      "epoch:5 step:4935 [D loss: 0.595531, acc: 67.97%] [G loss: 2.586717]\n",
      "epoch:5 step:4936 [D loss: 0.623899, acc: 66.41%] [G loss: 2.236473]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:5 step:4937 [D loss: 0.667559, acc: 61.72%] [G loss: 2.370035]\n",
      "epoch:5 step:4938 [D loss: 0.583590, acc: 70.31%] [G loss: 2.456111]\n",
      "epoch:5 step:4939 [D loss: 0.507827, acc: 77.34%] [G loss: 2.684967]\n",
      "epoch:5 step:4940 [D loss: 0.579862, acc: 72.66%] [G loss: 2.708393]\n",
      "epoch:5 step:4941 [D loss: 0.619314, acc: 62.50%] [G loss: 2.679909]\n",
      "epoch:5 step:4942 [D loss: 0.532501, acc: 80.47%] [G loss: 2.746403]\n",
      "epoch:5 step:4943 [D loss: 0.570766, acc: 71.09%] [G loss: 2.794128]\n",
      "epoch:5 step:4944 [D loss: 0.483223, acc: 78.12%] [G loss: 2.673200]\n",
      "epoch:5 step:4945 [D loss: 0.624124, acc: 67.97%] [G loss: 2.825243]\n",
      "epoch:5 step:4946 [D loss: 0.519407, acc: 71.88%] [G loss: 2.921855]\n",
      "epoch:5 step:4947 [D loss: 0.501707, acc: 73.44%] [G loss: 3.215060]\n",
      "epoch:5 step:4948 [D loss: 0.607608, acc: 67.19%] [G loss: 2.526679]\n",
      "epoch:5 step:4949 [D loss: 0.545710, acc: 73.44%] [G loss: 3.011327]\n",
      "epoch:5 step:4950 [D loss: 0.566260, acc: 78.91%] [G loss: 2.631462]\n",
      "epoch:5 step:4951 [D loss: 0.693507, acc: 59.38%] [G loss: 2.762308]\n",
      "epoch:5 step:4952 [D loss: 0.577973, acc: 75.78%] [G loss: 2.485475]\n",
      "epoch:5 step:4953 [D loss: 0.631418, acc: 66.41%] [G loss: 2.538424]\n",
      "epoch:5 step:4954 [D loss: 0.605813, acc: 67.97%] [G loss: 2.720948]\n",
      "epoch:5 step:4955 [D loss: 0.627649, acc: 69.53%] [G loss: 2.733469]\n",
      "epoch:5 step:4956 [D loss: 0.542560, acc: 71.88%] [G loss: 2.705723]\n",
      "epoch:5 step:4957 [D loss: 0.551685, acc: 69.53%] [G loss: 2.436874]\n",
      "epoch:5 step:4958 [D loss: 0.590165, acc: 65.62%] [G loss: 2.575398]\n",
      "epoch:5 step:4959 [D loss: 0.645325, acc: 63.28%] [G loss: 2.528507]\n",
      "epoch:5 step:4960 [D loss: 0.633218, acc: 64.06%] [G loss: 2.419185]\n",
      "epoch:5 step:4961 [D loss: 0.547045, acc: 74.22%] [G loss: 2.686203]\n",
      "epoch:5 step:4962 [D loss: 0.688608, acc: 62.50%] [G loss: 2.551230]\n",
      "epoch:5 step:4963 [D loss: 0.716547, acc: 59.38%] [G loss: 2.749499]\n",
      "epoch:5 step:4964 [D loss: 0.485216, acc: 77.34%] [G loss: 2.939771]\n",
      "epoch:5 step:4965 [D loss: 0.586981, acc: 71.88%] [G loss: 2.833928]\n",
      "epoch:5 step:4966 [D loss: 0.620538, acc: 64.06%] [G loss: 2.365209]\n",
      "epoch:5 step:4967 [D loss: 0.593983, acc: 70.31%] [G loss: 2.564820]\n",
      "epoch:5 step:4968 [D loss: 0.522770, acc: 76.56%] [G loss: 2.682199]\n",
      "epoch:5 step:4969 [D loss: 0.546777, acc: 75.00%] [G loss: 2.898047]\n",
      "epoch:5 step:4970 [D loss: 0.536976, acc: 71.09%] [G loss: 2.828223]\n",
      "epoch:5 step:4971 [D loss: 0.525399, acc: 74.22%] [G loss: 2.796009]\n",
      "epoch:5 step:4972 [D loss: 0.561723, acc: 71.88%] [G loss: 2.728245]\n",
      "epoch:5 step:4973 [D loss: 0.629121, acc: 63.28%] [G loss: 2.667563]\n",
      "epoch:5 step:4974 [D loss: 0.516955, acc: 77.34%] [G loss: 2.969167]\n",
      "epoch:5 step:4975 [D loss: 0.575908, acc: 72.66%] [G loss: 2.991252]\n",
      "epoch:5 step:4976 [D loss: 0.552599, acc: 72.66%] [G loss: 2.588889]\n",
      "epoch:5 step:4977 [D loss: 0.588146, acc: 66.41%] [G loss: 2.859236]\n",
      "epoch:5 step:4978 [D loss: 0.595929, acc: 65.62%] [G loss: 2.694169]\n",
      "epoch:5 step:4979 [D loss: 0.581408, acc: 67.97%] [G loss: 2.690009]\n",
      "epoch:5 step:4980 [D loss: 0.537962, acc: 73.44%] [G loss: 2.836405]\n",
      "epoch:5 step:4981 [D loss: 0.540112, acc: 75.00%] [G loss: 2.894949]\n",
      "epoch:5 step:4982 [D loss: 0.652794, acc: 72.66%] [G loss: 2.625870]\n",
      "epoch:5 step:4983 [D loss: 0.551518, acc: 71.88%] [G loss: 2.637909]\n",
      "epoch:5 step:4984 [D loss: 0.601754, acc: 66.41%] [G loss: 2.848787]\n",
      "epoch:5 step:4985 [D loss: 0.533439, acc: 72.66%] [G loss: 3.191214]\n",
      "epoch:5 step:4986 [D loss: 0.673140, acc: 64.06%] [G loss: 2.221084]\n",
      "epoch:5 step:4987 [D loss: 0.643235, acc: 64.84%] [G loss: 2.465944]\n",
      "epoch:5 step:4988 [D loss: 0.598526, acc: 64.84%] [G loss: 2.562901]\n",
      "epoch:5 step:4989 [D loss: 0.590271, acc: 71.88%] [G loss: 2.853972]\n",
      "epoch:5 step:4990 [D loss: 0.535053, acc: 67.19%] [G loss: 3.004494]\n",
      "epoch:5 step:4991 [D loss: 0.665915, acc: 61.72%] [G loss: 2.710072]\n",
      "epoch:5 step:4992 [D loss: 0.518820, acc: 73.44%] [G loss: 2.892607]\n",
      "epoch:5 step:4993 [D loss: 0.610384, acc: 67.19%] [G loss: 2.676957]\n",
      "epoch:5 step:4994 [D loss: 0.590927, acc: 67.97%] [G loss: 3.056331]\n",
      "epoch:5 step:4995 [D loss: 0.537803, acc: 73.44%] [G loss: 3.130241]\n",
      "epoch:5 step:4996 [D loss: 0.524379, acc: 72.66%] [G loss: 3.017909]\n",
      "epoch:5 step:4997 [D loss: 0.505180, acc: 78.12%] [G loss: 3.452363]\n",
      "epoch:5 step:4998 [D loss: 0.497004, acc: 76.56%] [G loss: 3.339276]\n",
      "epoch:5 step:4999 [D loss: 0.539224, acc: 74.22%] [G loss: 3.191164]\n",
      "epoch:5 step:5000 [D loss: 0.585364, acc: 73.44%] [G loss: 3.104293]\n",
      "##############\n",
      "[2.84318483 1.57660495 6.9686994  5.36628333 4.28647714 5.93204271\n",
      " 5.12854072 4.9477485  5.26964848 3.75928119]\n",
      "##########\n",
      "epoch:5 step:5001 [D loss: 0.702225, acc: 66.41%] [G loss: 2.274306]\n",
      "epoch:5 step:5002 [D loss: 0.601634, acc: 71.09%] [G loss: 2.409099]\n",
      "epoch:5 step:5003 [D loss: 0.595755, acc: 70.31%] [G loss: 2.690254]\n",
      "epoch:5 step:5004 [D loss: 0.640448, acc: 69.53%] [G loss: 2.455150]\n",
      "epoch:5 step:5005 [D loss: 0.503417, acc: 79.69%] [G loss: 2.961214]\n",
      "epoch:5 step:5006 [D loss: 0.558878, acc: 73.44%] [G loss: 2.846139]\n",
      "epoch:5 step:5007 [D loss: 0.569495, acc: 70.31%] [G loss: 2.574333]\n",
      "epoch:5 step:5008 [D loss: 0.571255, acc: 75.78%] [G loss: 2.272320]\n",
      "epoch:5 step:5009 [D loss: 0.619278, acc: 62.50%] [G loss: 2.388133]\n",
      "epoch:5 step:5010 [D loss: 0.517523, acc: 75.78%] [G loss: 2.913051]\n",
      "epoch:5 step:5011 [D loss: 0.558653, acc: 70.31%] [G loss: 2.582335]\n",
      "epoch:5 step:5012 [D loss: 0.583615, acc: 68.75%] [G loss: 2.692234]\n",
      "epoch:5 step:5013 [D loss: 0.651017, acc: 61.72%] [G loss: 2.361415]\n",
      "epoch:5 step:5014 [D loss: 0.645633, acc: 66.41%] [G loss: 2.428774]\n",
      "epoch:5 step:5015 [D loss: 0.610261, acc: 74.22%] [G loss: 2.772160]\n",
      "epoch:5 step:5016 [D loss: 0.522901, acc: 75.00%] [G loss: 2.567575]\n",
      "epoch:5 step:5017 [D loss: 0.544346, acc: 71.88%] [G loss: 2.541423]\n",
      "epoch:5 step:5018 [D loss: 0.556685, acc: 72.66%] [G loss: 2.809384]\n",
      "epoch:5 step:5019 [D loss: 0.535154, acc: 73.44%] [G loss: 2.686736]\n",
      "epoch:5 step:5020 [D loss: 0.534971, acc: 72.66%] [G loss: 3.177336]\n",
      "epoch:5 step:5021 [D loss: 0.511694, acc: 74.22%] [G loss: 2.720133]\n",
      "epoch:5 step:5022 [D loss: 0.590182, acc: 75.00%] [G loss: 2.839936]\n",
      "epoch:5 step:5023 [D loss: 0.586168, acc: 67.19%] [G loss: 2.717789]\n",
      "epoch:5 step:5024 [D loss: 0.526991, acc: 73.44%] [G loss: 2.939640]\n",
      "epoch:5 step:5025 [D loss: 0.603056, acc: 71.88%] [G loss: 3.184884]\n",
      "epoch:5 step:5026 [D loss: 0.695574, acc: 57.81%] [G loss: 2.494258]\n",
      "epoch:5 step:5027 [D loss: 0.605977, acc: 68.75%] [G loss: 2.531475]\n",
      "epoch:5 step:5028 [D loss: 0.558502, acc: 75.78%] [G loss: 3.007299]\n",
      "epoch:5 step:5029 [D loss: 0.743052, acc: 65.62%] [G loss: 2.811054]\n",
      "epoch:5 step:5030 [D loss: 0.522047, acc: 74.22%] [G loss: 3.070704]\n",
      "epoch:5 step:5031 [D loss: 0.474021, acc: 78.12%] [G loss: 3.136727]\n",
      "epoch:5 step:5032 [D loss: 0.544467, acc: 69.53%] [G loss: 3.002215]\n",
      "epoch:5 step:5033 [D loss: 0.689870, acc: 62.50%] [G loss: 2.362401]\n",
      "epoch:5 step:5034 [D loss: 0.710417, acc: 61.72%] [G loss: 2.216146]\n",
      "epoch:5 step:5035 [D loss: 0.552109, acc: 71.09%] [G loss: 2.399183]\n",
      "epoch:5 step:5036 [D loss: 0.571127, acc: 74.22%] [G loss: 2.659468]\n",
      "epoch:5 step:5037 [D loss: 0.699784, acc: 59.38%] [G loss: 2.311359]\n",
      "epoch:5 step:5038 [D loss: 0.611850, acc: 64.84%] [G loss: 2.895677]\n",
      "epoch:5 step:5039 [D loss: 0.551845, acc: 71.88%] [G loss: 2.924708]\n",
      "epoch:5 step:5040 [D loss: 0.564011, acc: 72.66%] [G loss: 2.530756]\n",
      "epoch:5 step:5041 [D loss: 0.596168, acc: 69.53%] [G loss: 2.433376]\n",
      "epoch:5 step:5042 [D loss: 0.581028, acc: 70.31%] [G loss: 2.513806]\n",
      "epoch:5 step:5043 [D loss: 0.552203, acc: 76.56%] [G loss: 2.671238]\n",
      "epoch:5 step:5044 [D loss: 0.481399, acc: 78.12%] [G loss: 2.719859]\n",
      "epoch:5 step:5045 [D loss: 0.549649, acc: 70.31%] [G loss: 2.895145]\n",
      "epoch:5 step:5046 [D loss: 0.547843, acc: 72.66%] [G loss: 2.694045]\n",
      "epoch:5 step:5047 [D loss: 0.691442, acc: 58.59%] [G loss: 2.563852]\n",
      "epoch:5 step:5048 [D loss: 0.530883, acc: 73.44%] [G loss: 2.783578]\n",
      "epoch:5 step:5049 [D loss: 0.565674, acc: 71.88%] [G loss: 2.667762]\n",
      "epoch:5 step:5050 [D loss: 0.575916, acc: 76.56%] [G loss: 2.726460]\n",
      "epoch:5 step:5051 [D loss: 0.588527, acc: 69.53%] [G loss: 2.991366]\n",
      "epoch:5 step:5052 [D loss: 0.539049, acc: 72.66%] [G loss: 2.894578]\n",
      "epoch:5 step:5053 [D loss: 0.596123, acc: 67.19%] [G loss: 2.632427]\n",
      "epoch:5 step:5054 [D loss: 0.555502, acc: 73.44%] [G loss: 2.863818]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:5 step:5055 [D loss: 0.577753, acc: 71.09%] [G loss: 2.910573]\n",
      "epoch:5 step:5056 [D loss: 0.576300, acc: 73.44%] [G loss: 2.738312]\n",
      "epoch:5 step:5057 [D loss: 0.533664, acc: 72.66%] [G loss: 2.553397]\n",
      "epoch:5 step:5058 [D loss: 0.623802, acc: 64.84%] [G loss: 2.451687]\n",
      "epoch:5 step:5059 [D loss: 0.673938, acc: 65.62%] [G loss: 2.856009]\n",
      "epoch:5 step:5060 [D loss: 0.644213, acc: 61.72%] [G loss: 2.444218]\n",
      "epoch:5 step:5061 [D loss: 0.662763, acc: 59.38%] [G loss: 2.434920]\n",
      "epoch:5 step:5062 [D loss: 0.703213, acc: 57.03%] [G loss: 2.186127]\n",
      "epoch:5 step:5063 [D loss: 0.565431, acc: 68.75%] [G loss: 2.718153]\n",
      "epoch:5 step:5064 [D loss: 0.606836, acc: 72.66%] [G loss: 2.459401]\n",
      "epoch:5 step:5065 [D loss: 0.656950, acc: 63.28%] [G loss: 2.540508]\n",
      "epoch:5 step:5066 [D loss: 0.508407, acc: 74.22%] [G loss: 2.756287]\n",
      "epoch:5 step:5067 [D loss: 0.582962, acc: 70.31%] [G loss: 2.472627]\n",
      "epoch:5 step:5068 [D loss: 0.593491, acc: 71.88%] [G loss: 2.261090]\n",
      "epoch:5 step:5069 [D loss: 0.470408, acc: 80.47%] [G loss: 2.730935]\n",
      "epoch:5 step:5070 [D loss: 0.655396, acc: 65.62%] [G loss: 2.909089]\n",
      "epoch:5 step:5071 [D loss: 0.610296, acc: 71.88%] [G loss: 2.240467]\n",
      "epoch:5 step:5072 [D loss: 0.602549, acc: 68.75%] [G loss: 2.341666]\n",
      "epoch:5 step:5073 [D loss: 0.660229, acc: 61.72%] [G loss: 2.351375]\n",
      "epoch:5 step:5074 [D loss: 0.605625, acc: 66.41%] [G loss: 2.330955]\n",
      "epoch:5 step:5075 [D loss: 0.536230, acc: 69.53%] [G loss: 2.582503]\n",
      "epoch:5 step:5076 [D loss: 0.621007, acc: 67.19%] [G loss: 2.562021]\n",
      "epoch:5 step:5077 [D loss: 0.515436, acc: 82.03%] [G loss: 2.953093]\n",
      "epoch:5 step:5078 [D loss: 0.582098, acc: 73.44%] [G loss: 2.703662]\n",
      "epoch:5 step:5079 [D loss: 0.652168, acc: 66.41%] [G loss: 2.640159]\n",
      "epoch:5 step:5080 [D loss: 0.609708, acc: 68.75%] [G loss: 2.887803]\n",
      "epoch:5 step:5081 [D loss: 0.724111, acc: 57.03%] [G loss: 2.130648]\n",
      "epoch:5 step:5082 [D loss: 0.552092, acc: 73.44%] [G loss: 2.356917]\n",
      "epoch:5 step:5083 [D loss: 0.502440, acc: 77.34%] [G loss: 2.704250]\n",
      "epoch:5 step:5084 [D loss: 0.543968, acc: 71.88%] [G loss: 2.902638]\n",
      "epoch:5 step:5085 [D loss: 0.587593, acc: 70.31%] [G loss: 2.465871]\n",
      "epoch:5 step:5086 [D loss: 0.574337, acc: 73.44%] [G loss: 2.707267]\n",
      "epoch:5 step:5087 [D loss: 0.495593, acc: 75.78%] [G loss: 2.627254]\n",
      "epoch:5 step:5088 [D loss: 0.613874, acc: 70.31%] [G loss: 2.703330]\n",
      "epoch:5 step:5089 [D loss: 0.611483, acc: 64.84%] [G loss: 2.826261]\n",
      "epoch:5 step:5090 [D loss: 0.499083, acc: 78.12%] [G loss: 2.756442]\n",
      "epoch:5 step:5091 [D loss: 0.557017, acc: 74.22%] [G loss: 2.805058]\n",
      "epoch:5 step:5092 [D loss: 0.672544, acc: 63.28%] [G loss: 2.584331]\n",
      "epoch:5 step:5093 [D loss: 0.610644, acc: 64.06%] [G loss: 2.779282]\n",
      "epoch:5 step:5094 [D loss: 0.667054, acc: 64.84%] [G loss: 2.576237]\n",
      "epoch:5 step:5095 [D loss: 0.598012, acc: 73.44%] [G loss: 2.706533]\n",
      "epoch:5 step:5096 [D loss: 0.643128, acc: 67.97%] [G loss: 2.395294]\n",
      "epoch:5 step:5097 [D loss: 0.584116, acc: 68.75%] [G loss: 2.471275]\n",
      "epoch:5 step:5098 [D loss: 0.666760, acc: 67.19%] [G loss: 2.456762]\n",
      "epoch:5 step:5099 [D loss: 0.669195, acc: 64.84%] [G loss: 2.336874]\n",
      "epoch:5 step:5100 [D loss: 0.571909, acc: 73.44%] [G loss: 2.814726]\n",
      "epoch:5 step:5101 [D loss: 0.567224, acc: 69.53%] [G loss: 2.890876]\n",
      "epoch:5 step:5102 [D loss: 0.652313, acc: 62.50%] [G loss: 2.341126]\n",
      "epoch:5 step:5103 [D loss: 0.647365, acc: 60.16%] [G loss: 2.441471]\n",
      "epoch:5 step:5104 [D loss: 0.625987, acc: 71.88%] [G loss: 2.309413]\n",
      "epoch:5 step:5105 [D loss: 0.647601, acc: 61.72%] [G loss: 2.321525]\n",
      "epoch:5 step:5106 [D loss: 0.645635, acc: 64.84%] [G loss: 2.316096]\n",
      "epoch:5 step:5107 [D loss: 0.623525, acc: 64.84%] [G loss: 2.348758]\n",
      "epoch:5 step:5108 [D loss: 0.660703, acc: 64.84%] [G loss: 2.392510]\n",
      "epoch:5 step:5109 [D loss: 0.607086, acc: 70.31%] [G loss: 2.499883]\n",
      "epoch:5 step:5110 [D loss: 0.582721, acc: 72.66%] [G loss: 2.273727]\n",
      "epoch:5 step:5111 [D loss: 0.592117, acc: 64.84%] [G loss: 2.691706]\n",
      "epoch:5 step:5112 [D loss: 0.538893, acc: 71.88%] [G loss: 2.546502]\n",
      "epoch:5 step:5113 [D loss: 0.506086, acc: 77.34%] [G loss: 3.255020]\n",
      "epoch:5 step:5114 [D loss: 0.506727, acc: 74.22%] [G loss: 2.956604]\n",
      "epoch:5 step:5115 [D loss: 0.505007, acc: 75.78%] [G loss: 3.268747]\n",
      "epoch:5 step:5116 [D loss: 0.724856, acc: 62.50%] [G loss: 3.060029]\n",
      "epoch:5 step:5117 [D loss: 0.609347, acc: 66.41%] [G loss: 2.502515]\n",
      "epoch:5 step:5118 [D loss: 0.516246, acc: 78.12%] [G loss: 2.753606]\n",
      "epoch:5 step:5119 [D loss: 0.579834, acc: 67.97%] [G loss: 2.683512]\n",
      "epoch:5 step:5120 [D loss: 0.644852, acc: 64.06%] [G loss: 2.452837]\n",
      "epoch:5 step:5121 [D loss: 0.643290, acc: 65.62%] [G loss: 2.502701]\n",
      "epoch:5 step:5122 [D loss: 0.653008, acc: 66.41%] [G loss: 2.348797]\n",
      "epoch:5 step:5123 [D loss: 0.731657, acc: 57.03%] [G loss: 2.420552]\n",
      "epoch:5 step:5124 [D loss: 0.685227, acc: 57.81%] [G loss: 2.339577]\n",
      "epoch:5 step:5125 [D loss: 0.631552, acc: 60.94%] [G loss: 2.548719]\n",
      "epoch:5 step:5126 [D loss: 0.635342, acc: 71.09%] [G loss: 2.392390]\n",
      "epoch:5 step:5127 [D loss: 0.579740, acc: 71.88%] [G loss: 2.405209]\n",
      "epoch:5 step:5128 [D loss: 0.556043, acc: 71.09%] [G loss: 2.508991]\n",
      "epoch:5 step:5129 [D loss: 0.662481, acc: 67.19%] [G loss: 2.361763]\n",
      "epoch:5 step:5130 [D loss: 0.625572, acc: 63.28%] [G loss: 2.325517]\n",
      "epoch:5 step:5131 [D loss: 0.597326, acc: 72.66%] [G loss: 2.597084]\n",
      "epoch:5 step:5132 [D loss: 0.578293, acc: 67.19%] [G loss: 2.533086]\n",
      "epoch:5 step:5133 [D loss: 0.618091, acc: 71.09%] [G loss: 2.434992]\n",
      "epoch:5 step:5134 [D loss: 0.618720, acc: 67.19%] [G loss: 2.489656]\n",
      "epoch:5 step:5135 [D loss: 0.567626, acc: 72.66%] [G loss: 2.297009]\n",
      "epoch:5 step:5136 [D loss: 0.565136, acc: 71.09%] [G loss: 2.937697]\n",
      "epoch:5 step:5137 [D loss: 0.521934, acc: 76.56%] [G loss: 2.858393]\n",
      "epoch:5 step:5138 [D loss: 0.527420, acc: 71.09%] [G loss: 2.948402]\n",
      "epoch:5 step:5139 [D loss: 0.585860, acc: 68.75%] [G loss: 2.788891]\n",
      "epoch:5 step:5140 [D loss: 0.604532, acc: 68.75%] [G loss: 2.802313]\n",
      "epoch:5 step:5141 [D loss: 0.576159, acc: 71.88%] [G loss: 2.532347]\n",
      "epoch:5 step:5142 [D loss: 0.590726, acc: 71.09%] [G loss: 2.732325]\n",
      "epoch:5 step:5143 [D loss: 0.717288, acc: 59.38%] [G loss: 2.511541]\n",
      "epoch:5 step:5144 [D loss: 0.578567, acc: 67.19%] [G loss: 2.429883]\n",
      "epoch:5 step:5145 [D loss: 0.589970, acc: 66.41%] [G loss: 2.368998]\n",
      "epoch:5 step:5146 [D loss: 0.600716, acc: 71.09%] [G loss: 2.575479]\n",
      "epoch:5 step:5147 [D loss: 0.540525, acc: 75.00%] [G loss: 2.897104]\n",
      "epoch:5 step:5148 [D loss: 0.624143, acc: 65.62%] [G loss: 2.601858]\n",
      "epoch:5 step:5149 [D loss: 0.483290, acc: 77.34%] [G loss: 2.502673]\n",
      "epoch:5 step:5150 [D loss: 0.517162, acc: 75.78%] [G loss: 2.691559]\n",
      "epoch:5 step:5151 [D loss: 0.539762, acc: 72.66%] [G loss: 3.012100]\n",
      "epoch:5 step:5152 [D loss: 0.563664, acc: 68.75%] [G loss: 2.511775]\n",
      "epoch:5 step:5153 [D loss: 0.569966, acc: 69.53%] [G loss: 2.808967]\n",
      "epoch:5 step:5154 [D loss: 0.582178, acc: 69.53%] [G loss: 2.490644]\n",
      "epoch:5 step:5155 [D loss: 0.509748, acc: 75.00%] [G loss: 2.665101]\n",
      "epoch:5 step:5156 [D loss: 0.500031, acc: 71.88%] [G loss: 3.245520]\n",
      "epoch:5 step:5157 [D loss: 0.527887, acc: 70.31%] [G loss: 3.253057]\n",
      "epoch:5 step:5158 [D loss: 0.612197, acc: 66.41%] [G loss: 2.828361]\n",
      "epoch:5 step:5159 [D loss: 0.507347, acc: 80.47%] [G loss: 3.109750]\n",
      "epoch:5 step:5160 [D loss: 0.676741, acc: 60.94%] [G loss: 2.964345]\n",
      "epoch:5 step:5161 [D loss: 0.629001, acc: 67.19%] [G loss: 2.859268]\n",
      "epoch:5 step:5162 [D loss: 0.680683, acc: 63.28%] [G loss: 2.096868]\n",
      "epoch:5 step:5163 [D loss: 0.663470, acc: 53.91%] [G loss: 2.393253]\n",
      "epoch:5 step:5164 [D loss: 0.559800, acc: 71.88%] [G loss: 2.739612]\n",
      "epoch:5 step:5165 [D loss: 0.555949, acc: 74.22%] [G loss: 2.466993]\n",
      "epoch:5 step:5166 [D loss: 0.546482, acc: 72.66%] [G loss: 2.827798]\n",
      "epoch:5 step:5167 [D loss: 0.544961, acc: 66.41%] [G loss: 2.361299]\n",
      "epoch:5 step:5168 [D loss: 0.596477, acc: 66.41%] [G loss: 2.885138]\n",
      "epoch:5 step:5169 [D loss: 0.526940, acc: 69.53%] [G loss: 2.854422]\n",
      "epoch:5 step:5170 [D loss: 0.602705, acc: 67.19%] [G loss: 2.633831]\n",
      "epoch:5 step:5171 [D loss: 0.552040, acc: 74.22%] [G loss: 2.797540]\n",
      "epoch:5 step:5172 [D loss: 0.470679, acc: 80.47%] [G loss: 2.797463]\n",
      "epoch:5 step:5173 [D loss: 0.602079, acc: 66.41%] [G loss: 2.917647]\n",
      "epoch:5 step:5174 [D loss: 0.632677, acc: 65.62%] [G loss: 2.869074]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:5 step:5175 [D loss: 0.601416, acc: 67.19%] [G loss: 2.888106]\n",
      "epoch:5 step:5176 [D loss: 0.677858, acc: 64.06%] [G loss: 2.750840]\n",
      "epoch:5 step:5177 [D loss: 0.680849, acc: 56.25%] [G loss: 2.561964]\n",
      "epoch:5 step:5178 [D loss: 0.601827, acc: 60.94%] [G loss: 2.436406]\n",
      "epoch:5 step:5179 [D loss: 0.584303, acc: 69.53%] [G loss: 2.644758]\n",
      "epoch:5 step:5180 [D loss: 0.507760, acc: 71.09%] [G loss: 2.959376]\n",
      "epoch:5 step:5181 [D loss: 0.572487, acc: 71.09%] [G loss: 2.690019]\n",
      "epoch:5 step:5182 [D loss: 0.476585, acc: 75.00%] [G loss: 3.094314]\n",
      "epoch:5 step:5183 [D loss: 0.519947, acc: 72.66%] [G loss: 3.584664]\n",
      "epoch:5 step:5184 [D loss: 0.509875, acc: 74.22%] [G loss: 3.219536]\n",
      "epoch:5 step:5185 [D loss: 0.656587, acc: 66.41%] [G loss: 2.084125]\n",
      "epoch:5 step:5186 [D loss: 0.682669, acc: 60.94%] [G loss: 2.413819]\n",
      "epoch:5 step:5187 [D loss: 0.663733, acc: 61.72%] [G loss: 2.282064]\n",
      "epoch:5 step:5188 [D loss: 0.551445, acc: 80.47%] [G loss: 2.720907]\n",
      "epoch:5 step:5189 [D loss: 0.618954, acc: 67.97%] [G loss: 2.926415]\n",
      "epoch:5 step:5190 [D loss: 0.594558, acc: 65.62%] [G loss: 2.557498]\n",
      "epoch:5 step:5191 [D loss: 0.653164, acc: 60.94%] [G loss: 2.305371]\n",
      "epoch:5 step:5192 [D loss: 0.668931, acc: 60.16%] [G loss: 2.400631]\n",
      "epoch:5 step:5193 [D loss: 0.509470, acc: 76.56%] [G loss: 2.838706]\n",
      "epoch:5 step:5194 [D loss: 0.617251, acc: 65.62%] [G loss: 2.917066]\n",
      "epoch:5 step:5195 [D loss: 0.610519, acc: 68.75%] [G loss: 2.535194]\n",
      "epoch:5 step:5196 [D loss: 0.687435, acc: 60.16%] [G loss: 2.314523]\n",
      "epoch:5 step:5197 [D loss: 0.610767, acc: 70.31%] [G loss: 2.339770]\n",
      "epoch:5 step:5198 [D loss: 0.550872, acc: 70.31%] [G loss: 2.338000]\n",
      "epoch:5 step:5199 [D loss: 0.530855, acc: 72.66%] [G loss: 2.680696]\n",
      "epoch:5 step:5200 [D loss: 0.565711, acc: 71.09%] [G loss: 2.651456]\n",
      "##############\n",
      "[3.01102211 1.58198244 6.81734121 5.31146676 4.28129642 6.07385778\n",
      " 5.09923003 5.04357441 5.36928894 3.60527926]\n",
      "##########\n",
      "epoch:5 step:5201 [D loss: 0.532494, acc: 78.12%] [G loss: 2.622315]\n",
      "epoch:5 step:5202 [D loss: 0.551706, acc: 73.44%] [G loss: 2.586504]\n",
      "epoch:5 step:5203 [D loss: 0.561659, acc: 71.09%] [G loss: 2.567387]\n",
      "epoch:5 step:5204 [D loss: 0.567844, acc: 68.75%] [G loss: 2.691031]\n",
      "epoch:5 step:5205 [D loss: 0.556233, acc: 76.56%] [G loss: 2.768010]\n",
      "epoch:5 step:5206 [D loss: 0.562062, acc: 73.44%] [G loss: 2.808904]\n",
      "epoch:5 step:5207 [D loss: 0.564637, acc: 71.09%] [G loss: 2.624722]\n",
      "epoch:5 step:5208 [D loss: 0.600033, acc: 68.75%] [G loss: 2.983323]\n",
      "epoch:5 step:5209 [D loss: 0.611761, acc: 64.84%] [G loss: 2.756910]\n",
      "epoch:5 step:5210 [D loss: 0.624473, acc: 69.53%] [G loss: 2.579118]\n",
      "epoch:5 step:5211 [D loss: 0.541422, acc: 71.88%] [G loss: 2.416596]\n",
      "epoch:5 step:5212 [D loss: 0.605361, acc: 63.28%] [G loss: 2.536074]\n",
      "epoch:5 step:5213 [D loss: 0.640768, acc: 62.50%] [G loss: 2.500981]\n",
      "epoch:5 step:5214 [D loss: 0.680125, acc: 67.19%] [G loss: 2.379175]\n",
      "epoch:5 step:5215 [D loss: 0.541900, acc: 72.66%] [G loss: 2.572757]\n",
      "epoch:5 step:5216 [D loss: 0.779359, acc: 52.34%] [G loss: 2.182097]\n",
      "epoch:5 step:5217 [D loss: 0.590961, acc: 68.75%] [G loss: 2.330045]\n",
      "epoch:5 step:5218 [D loss: 0.598003, acc: 64.06%] [G loss: 2.367308]\n",
      "epoch:5 step:5219 [D loss: 0.596536, acc: 68.75%] [G loss: 2.767910]\n",
      "epoch:5 step:5220 [D loss: 0.592643, acc: 67.97%] [G loss: 2.245269]\n",
      "epoch:5 step:5221 [D loss: 0.561665, acc: 73.44%] [G loss: 2.659570]\n",
      "epoch:5 step:5222 [D loss: 0.639875, acc: 61.72%] [G loss: 2.394774]\n",
      "epoch:5 step:5223 [D loss: 0.646040, acc: 60.94%] [G loss: 2.432932]\n",
      "epoch:5 step:5224 [D loss: 0.628895, acc: 67.19%] [G loss: 2.419654]\n",
      "epoch:5 step:5225 [D loss: 0.615076, acc: 64.06%] [G loss: 2.165801]\n",
      "epoch:5 step:5226 [D loss: 0.642164, acc: 63.28%] [G loss: 2.563653]\n",
      "epoch:5 step:5227 [D loss: 0.680101, acc: 57.03%] [G loss: 2.315630]\n",
      "epoch:5 step:5228 [D loss: 0.623794, acc: 64.84%] [G loss: 2.224324]\n",
      "epoch:5 step:5229 [D loss: 0.595007, acc: 71.88%] [G loss: 2.258063]\n",
      "epoch:5 step:5230 [D loss: 0.567485, acc: 71.88%] [G loss: 2.253906]\n",
      "epoch:5 step:5231 [D loss: 0.506308, acc: 76.56%] [G loss: 2.811496]\n",
      "epoch:5 step:5232 [D loss: 0.572369, acc: 71.88%] [G loss: 2.809558]\n",
      "epoch:5 step:5233 [D loss: 0.569953, acc: 69.53%] [G loss: 2.784898]\n",
      "epoch:5 step:5234 [D loss: 0.599117, acc: 69.53%] [G loss: 2.542300]\n",
      "epoch:5 step:5235 [D loss: 0.619591, acc: 64.06%] [G loss: 2.875722]\n",
      "epoch:5 step:5236 [D loss: 0.576697, acc: 69.53%] [G loss: 2.685488]\n",
      "epoch:5 step:5237 [D loss: 0.529736, acc: 73.44%] [G loss: 2.714996]\n",
      "epoch:5 step:5238 [D loss: 0.608393, acc: 62.50%] [G loss: 2.574198]\n",
      "epoch:5 step:5239 [D loss: 0.518378, acc: 75.00%] [G loss: 3.158988]\n",
      "epoch:5 step:5240 [D loss: 0.527016, acc: 73.44%] [G loss: 2.771921]\n",
      "epoch:5 step:5241 [D loss: 0.553965, acc: 74.22%] [G loss: 3.018847]\n",
      "epoch:5 step:5242 [D loss: 0.520584, acc: 77.34%] [G loss: 2.909218]\n",
      "epoch:5 step:5243 [D loss: 0.552804, acc: 71.88%] [G loss: 2.945405]\n",
      "epoch:5 step:5244 [D loss: 0.686953, acc: 60.16%] [G loss: 2.610272]\n",
      "epoch:5 step:5245 [D loss: 0.617034, acc: 67.19%] [G loss: 2.564790]\n",
      "epoch:5 step:5246 [D loss: 0.610840, acc: 70.31%] [G loss: 2.731434]\n",
      "epoch:5 step:5247 [D loss: 0.587633, acc: 66.41%] [G loss: 2.652412]\n",
      "epoch:5 step:5248 [D loss: 0.643206, acc: 62.50%] [G loss: 2.364071]\n",
      "epoch:5 step:5249 [D loss: 0.630054, acc: 68.75%] [G loss: 2.656509]\n",
      "epoch:5 step:5250 [D loss: 0.606555, acc: 64.84%] [G loss: 2.510142]\n",
      "epoch:5 step:5251 [D loss: 0.645558, acc: 64.06%] [G loss: 2.510421]\n",
      "epoch:5 step:5252 [D loss: 0.574355, acc: 67.97%] [G loss: 2.683961]\n",
      "epoch:5 step:5253 [D loss: 0.627068, acc: 69.53%] [G loss: 2.577508]\n",
      "epoch:5 step:5254 [D loss: 0.627802, acc: 62.50%] [G loss: 2.477098]\n",
      "epoch:5 step:5255 [D loss: 0.493261, acc: 78.12%] [G loss: 2.591859]\n",
      "epoch:5 step:5256 [D loss: 0.587785, acc: 71.09%] [G loss: 2.404600]\n",
      "epoch:5 step:5257 [D loss: 0.621877, acc: 71.88%] [G loss: 2.454922]\n",
      "epoch:5 step:5258 [D loss: 0.628955, acc: 65.62%] [G loss: 2.428330]\n",
      "epoch:5 step:5259 [D loss: 0.524872, acc: 76.56%] [G loss: 2.550616]\n",
      "epoch:5 step:5260 [D loss: 0.562774, acc: 71.88%] [G loss: 2.339357]\n",
      "epoch:5 step:5261 [D loss: 0.673552, acc: 60.16%] [G loss: 2.205659]\n",
      "epoch:5 step:5262 [D loss: 0.631916, acc: 67.97%] [G loss: 2.508482]\n",
      "epoch:5 step:5263 [D loss: 0.663098, acc: 63.28%] [G loss: 2.365301]\n",
      "epoch:5 step:5264 [D loss: 0.578311, acc: 67.19%] [G loss: 2.507215]\n",
      "epoch:5 step:5265 [D loss: 0.622570, acc: 65.62%] [G loss: 2.305202]\n",
      "epoch:5 step:5266 [D loss: 0.639779, acc: 68.75%] [G loss: 2.330003]\n",
      "epoch:5 step:5267 [D loss: 0.531957, acc: 71.88%] [G loss: 2.692529]\n",
      "epoch:5 step:5268 [D loss: 0.654732, acc: 60.94%] [G loss: 2.385016]\n",
      "epoch:5 step:5269 [D loss: 0.631014, acc: 69.53%] [G loss: 2.605578]\n",
      "epoch:5 step:5270 [D loss: 0.603146, acc: 71.88%] [G loss: 2.440920]\n",
      "epoch:5 step:5271 [D loss: 0.590539, acc: 68.75%] [G loss: 2.651427]\n",
      "epoch:5 step:5272 [D loss: 0.606834, acc: 68.75%] [G loss: 2.694995]\n",
      "epoch:5 step:5273 [D loss: 0.616557, acc: 69.53%] [G loss: 2.472684]\n",
      "epoch:5 step:5274 [D loss: 0.510546, acc: 72.66%] [G loss: 2.573618]\n",
      "epoch:5 step:5275 [D loss: 0.626601, acc: 67.97%] [G loss: 2.368251]\n",
      "epoch:5 step:5276 [D loss: 0.588172, acc: 69.53%] [G loss: 2.394382]\n",
      "epoch:5 step:5277 [D loss: 0.498888, acc: 75.00%] [G loss: 2.609587]\n",
      "epoch:5 step:5278 [D loss: 0.701694, acc: 60.94%] [G loss: 2.295024]\n",
      "epoch:5 step:5279 [D loss: 0.605129, acc: 67.19%] [G loss: 2.584893]\n",
      "epoch:5 step:5280 [D loss: 0.605896, acc: 64.06%] [G loss: 2.500380]\n",
      "epoch:5 step:5281 [D loss: 0.629726, acc: 61.72%] [G loss: 2.437991]\n",
      "epoch:5 step:5282 [D loss: 0.602802, acc: 68.75%] [G loss: 2.748064]\n",
      "epoch:5 step:5283 [D loss: 0.611611, acc: 66.41%] [G loss: 2.578393]\n",
      "epoch:5 step:5284 [D loss: 0.582641, acc: 75.00%] [G loss: 2.579747]\n",
      "epoch:5 step:5285 [D loss: 0.581703, acc: 71.09%] [G loss: 2.708747]\n",
      "epoch:5 step:5286 [D loss: 0.595606, acc: 71.09%] [G loss: 2.597153]\n",
      "epoch:5 step:5287 [D loss: 0.625046, acc: 71.09%] [G loss: 2.444721]\n",
      "epoch:5 step:5288 [D loss: 0.547193, acc: 72.66%] [G loss: 2.720757]\n",
      "epoch:5 step:5289 [D loss: 0.605126, acc: 64.06%] [G loss: 2.540422]\n",
      "epoch:5 step:5290 [D loss: 0.572511, acc: 72.66%] [G loss: 2.650865]\n",
      "epoch:5 step:5291 [D loss: 0.639354, acc: 64.06%] [G loss: 2.437291]\n",
      "epoch:5 step:5292 [D loss: 0.607197, acc: 67.19%] [G loss: 2.570556]\n",
      "epoch:5 step:5293 [D loss: 0.558123, acc: 70.31%] [G loss: 2.309289]\n",
      "epoch:5 step:5294 [D loss: 0.514142, acc: 78.12%] [G loss: 2.908664]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:5 step:5295 [D loss: 0.590156, acc: 73.44%] [G loss: 2.611804]\n",
      "epoch:5 step:5296 [D loss: 0.557537, acc: 76.56%] [G loss: 2.518078]\n",
      "epoch:5 step:5297 [D loss: 0.600372, acc: 67.97%] [G loss: 2.572267]\n",
      "epoch:5 step:5298 [D loss: 0.508697, acc: 75.78%] [G loss: 2.991212]\n",
      "epoch:5 step:5299 [D loss: 0.576915, acc: 70.31%] [G loss: 2.427606]\n",
      "epoch:5 step:5300 [D loss: 0.699158, acc: 56.25%] [G loss: 2.328999]\n",
      "epoch:5 step:5301 [D loss: 0.516156, acc: 77.34%] [G loss: 2.871404]\n",
      "epoch:5 step:5302 [D loss: 0.604183, acc: 65.62%] [G loss: 2.552357]\n",
      "epoch:5 step:5303 [D loss: 0.679452, acc: 64.84%] [G loss: 2.284784]\n",
      "epoch:5 step:5304 [D loss: 0.698053, acc: 63.28%] [G loss: 2.285305]\n",
      "epoch:5 step:5305 [D loss: 0.622227, acc: 64.06%] [G loss: 2.284908]\n",
      "epoch:5 step:5306 [D loss: 0.552699, acc: 71.88%] [G loss: 2.281418]\n",
      "epoch:5 step:5307 [D loss: 0.701561, acc: 60.94%] [G loss: 2.373576]\n",
      "epoch:5 step:5308 [D loss: 0.588181, acc: 64.84%] [G loss: 2.501951]\n",
      "epoch:5 step:5309 [D loss: 0.640389, acc: 67.19%] [G loss: 2.560771]\n",
      "epoch:5 step:5310 [D loss: 0.593224, acc: 72.66%] [G loss: 2.218744]\n",
      "epoch:5 step:5311 [D loss: 0.564388, acc: 71.88%] [G loss: 2.619429]\n",
      "epoch:5 step:5312 [D loss: 0.612517, acc: 65.62%] [G loss: 2.457712]\n",
      "epoch:5 step:5313 [D loss: 0.583344, acc: 68.75%] [G loss: 2.369797]\n",
      "epoch:5 step:5314 [D loss: 0.655310, acc: 63.28%] [G loss: 2.620789]\n",
      "epoch:5 step:5315 [D loss: 0.476925, acc: 79.69%] [G loss: 2.758399]\n",
      "epoch:5 step:5316 [D loss: 0.563558, acc: 73.44%] [G loss: 2.739762]\n",
      "epoch:5 step:5317 [D loss: 0.529384, acc: 75.00%] [G loss: 2.627514]\n",
      "epoch:5 step:5318 [D loss: 0.564342, acc: 75.78%] [G loss: 2.773719]\n",
      "epoch:5 step:5319 [D loss: 0.615661, acc: 67.19%] [G loss: 2.807369]\n",
      "epoch:5 step:5320 [D loss: 0.514378, acc: 74.22%] [G loss: 2.873634]\n",
      "epoch:5 step:5321 [D loss: 0.632441, acc: 61.72%] [G loss: 2.541580]\n",
      "epoch:5 step:5322 [D loss: 0.581264, acc: 67.97%] [G loss: 2.829823]\n",
      "epoch:5 step:5323 [D loss: 0.657604, acc: 57.03%] [G loss: 2.538264]\n",
      "epoch:5 step:5324 [D loss: 0.577620, acc: 66.41%] [G loss: 2.648909]\n",
      "epoch:5 step:5325 [D loss: 0.606773, acc: 64.06%] [G loss: 2.416937]\n",
      "epoch:5 step:5326 [D loss: 0.584234, acc: 68.75%] [G loss: 2.745146]\n",
      "epoch:5 step:5327 [D loss: 0.577538, acc: 71.88%] [G loss: 2.610273]\n",
      "epoch:5 step:5328 [D loss: 0.647486, acc: 68.75%] [G loss: 2.457237]\n",
      "epoch:5 step:5329 [D loss: 0.534365, acc: 71.09%] [G loss: 2.787724]\n",
      "epoch:5 step:5330 [D loss: 0.586001, acc: 77.34%] [G loss: 2.599461]\n",
      "epoch:5 step:5331 [D loss: 0.538125, acc: 71.88%] [G loss: 2.706294]\n",
      "epoch:5 step:5332 [D loss: 0.562877, acc: 74.22%] [G loss: 2.804206]\n",
      "epoch:5 step:5333 [D loss: 0.536760, acc: 77.34%] [G loss: 3.492736]\n",
      "epoch:5 step:5334 [D loss: 0.620395, acc: 68.75%] [G loss: 2.827404]\n",
      "epoch:5 step:5335 [D loss: 0.625482, acc: 65.62%] [G loss: 2.822112]\n",
      "epoch:5 step:5336 [D loss: 0.628846, acc: 66.41%] [G loss: 2.452947]\n",
      "epoch:5 step:5337 [D loss: 0.618488, acc: 62.50%] [G loss: 2.494039]\n",
      "epoch:5 step:5338 [D loss: 0.562766, acc: 69.53%] [G loss: 2.607432]\n",
      "epoch:5 step:5339 [D loss: 0.489327, acc: 77.34%] [G loss: 3.042564]\n",
      "epoch:5 step:5340 [D loss: 0.594724, acc: 67.19%] [G loss: 2.592227]\n",
      "epoch:5 step:5341 [D loss: 0.519493, acc: 73.44%] [G loss: 2.822390]\n",
      "epoch:5 step:5342 [D loss: 0.601150, acc: 67.19%] [G loss: 2.532220]\n",
      "epoch:5 step:5343 [D loss: 0.612366, acc: 60.94%] [G loss: 2.607049]\n",
      "epoch:5 step:5344 [D loss: 0.650280, acc: 64.06%] [G loss: 2.631813]\n",
      "epoch:5 step:5345 [D loss: 0.560606, acc: 71.88%] [G loss: 2.653213]\n",
      "epoch:5 step:5346 [D loss: 0.577594, acc: 73.44%] [G loss: 2.505432]\n",
      "epoch:5 step:5347 [D loss: 0.598053, acc: 66.41%] [G loss: 2.658892]\n",
      "epoch:5 step:5348 [D loss: 0.620892, acc: 64.06%] [G loss: 2.722126]\n",
      "epoch:5 step:5349 [D loss: 0.589030, acc: 71.09%] [G loss: 2.681851]\n",
      "epoch:5 step:5350 [D loss: 0.647110, acc: 60.94%] [G loss: 2.450018]\n",
      "epoch:5 step:5351 [D loss: 0.561967, acc: 71.09%] [G loss: 2.534344]\n",
      "epoch:5 step:5352 [D loss: 0.723665, acc: 54.69%] [G loss: 2.300325]\n",
      "epoch:5 step:5353 [D loss: 0.552923, acc: 74.22%] [G loss: 2.653326]\n",
      "epoch:5 step:5354 [D loss: 0.637712, acc: 66.41%] [G loss: 2.232111]\n",
      "epoch:5 step:5355 [D loss: 0.626133, acc: 64.06%] [G loss: 2.335062]\n",
      "epoch:5 step:5356 [D loss: 0.577272, acc: 67.97%] [G loss: 2.288934]\n",
      "epoch:5 step:5357 [D loss: 0.592454, acc: 67.19%] [G loss: 2.695032]\n",
      "epoch:5 step:5358 [D loss: 0.600522, acc: 69.53%] [G loss: 2.560565]\n",
      "epoch:5 step:5359 [D loss: 0.601978, acc: 67.97%] [G loss: 2.478382]\n",
      "epoch:5 step:5360 [D loss: 0.582769, acc: 70.31%] [G loss: 2.412899]\n",
      "epoch:5 step:5361 [D loss: 0.572752, acc: 70.31%] [G loss: 2.677685]\n",
      "epoch:5 step:5362 [D loss: 0.530461, acc: 75.78%] [G loss: 2.594681]\n",
      "epoch:5 step:5363 [D loss: 0.566032, acc: 71.88%] [G loss: 2.367353]\n",
      "epoch:5 step:5364 [D loss: 0.526218, acc: 73.44%] [G loss: 2.823950]\n",
      "epoch:5 step:5365 [D loss: 0.545222, acc: 73.44%] [G loss: 2.592654]\n",
      "epoch:5 step:5366 [D loss: 0.562071, acc: 66.41%] [G loss: 2.752493]\n",
      "epoch:5 step:5367 [D loss: 0.570155, acc: 72.66%] [G loss: 2.455652]\n",
      "epoch:5 step:5368 [D loss: 0.546269, acc: 68.75%] [G loss: 2.633565]\n",
      "epoch:5 step:5369 [D loss: 0.585782, acc: 68.75%] [G loss: 2.482924]\n",
      "epoch:5 step:5370 [D loss: 0.559356, acc: 64.84%] [G loss: 2.584051]\n",
      "epoch:5 step:5371 [D loss: 0.599682, acc: 72.66%] [G loss: 2.593408]\n",
      "epoch:5 step:5372 [D loss: 0.652041, acc: 58.59%] [G loss: 2.326803]\n",
      "epoch:5 step:5373 [D loss: 0.575753, acc: 64.06%] [G loss: 2.362335]\n",
      "epoch:5 step:5374 [D loss: 0.638473, acc: 65.62%] [G loss: 2.539197]\n",
      "epoch:5 step:5375 [D loss: 0.585085, acc: 69.53%] [G loss: 2.747159]\n",
      "epoch:5 step:5376 [D loss: 0.517281, acc: 75.78%] [G loss: 2.888287]\n",
      "epoch:5 step:5377 [D loss: 0.514492, acc: 75.78%] [G loss: 2.682134]\n",
      "epoch:5 step:5378 [D loss: 0.604750, acc: 70.31%] [G loss: 2.720560]\n",
      "epoch:5 step:5379 [D loss: 0.511561, acc: 70.31%] [G loss: 3.145958]\n",
      "epoch:5 step:5380 [D loss: 0.612295, acc: 71.09%] [G loss: 2.793812]\n",
      "epoch:5 step:5381 [D loss: 0.565840, acc: 71.88%] [G loss: 2.380361]\n",
      "epoch:5 step:5382 [D loss: 0.563374, acc: 71.09%] [G loss: 2.634640]\n",
      "epoch:5 step:5383 [D loss: 0.625624, acc: 61.72%] [G loss: 2.591558]\n",
      "epoch:5 step:5384 [D loss: 0.560916, acc: 67.97%] [G loss: 2.688299]\n",
      "epoch:5 step:5385 [D loss: 0.611232, acc: 65.62%] [G loss: 2.831151]\n",
      "epoch:5 step:5386 [D loss: 0.627471, acc: 67.97%] [G loss: 2.660098]\n",
      "epoch:5 step:5387 [D loss: 0.668146, acc: 57.03%] [G loss: 2.346698]\n",
      "epoch:5 step:5388 [D loss: 0.611875, acc: 64.06%] [G loss: 2.327486]\n",
      "epoch:5 step:5389 [D loss: 0.645470, acc: 64.84%] [G loss: 2.291961]\n",
      "epoch:5 step:5390 [D loss: 0.551587, acc: 76.56%] [G loss: 2.662997]\n",
      "epoch:5 step:5391 [D loss: 0.589623, acc: 68.75%] [G loss: 2.642314]\n",
      "epoch:5 step:5392 [D loss: 0.539697, acc: 74.22%] [G loss: 3.116453]\n",
      "epoch:5 step:5393 [D loss: 0.570948, acc: 68.75%] [G loss: 2.751539]\n",
      "epoch:5 step:5394 [D loss: 0.590505, acc: 68.75%] [G loss: 2.757289]\n",
      "epoch:5 step:5395 [D loss: 0.700903, acc: 60.94%] [G loss: 2.476724]\n",
      "epoch:5 step:5396 [D loss: 0.662693, acc: 60.16%] [G loss: 2.488757]\n",
      "epoch:5 step:5397 [D loss: 0.539772, acc: 71.88%] [G loss: 2.602841]\n",
      "epoch:5 step:5398 [D loss: 0.629891, acc: 64.84%] [G loss: 2.371113]\n",
      "epoch:5 step:5399 [D loss: 0.588778, acc: 64.84%] [G loss: 2.506404]\n",
      "epoch:5 step:5400 [D loss: 0.579972, acc: 70.31%] [G loss: 2.522611]\n",
      "##############\n",
      "[2.79190957 1.88548365 6.93468705 5.33542141 4.1098817  5.9959396\n",
      " 5.11412011 4.81475629 5.27215191 3.56561938]\n",
      "##########\n",
      "epoch:5 step:5401 [D loss: 0.596525, acc: 68.75%] [G loss: 2.470485]\n",
      "epoch:5 step:5402 [D loss: 0.576693, acc: 68.75%] [G loss: 2.548070]\n",
      "epoch:5 step:5403 [D loss: 0.637686, acc: 62.50%] [G loss: 2.624464]\n",
      "epoch:5 step:5404 [D loss: 0.584330, acc: 69.53%] [G loss: 2.611396]\n",
      "epoch:5 step:5405 [D loss: 0.527878, acc: 75.00%] [G loss: 2.496452]\n",
      "epoch:5 step:5406 [D loss: 0.624588, acc: 61.72%] [G loss: 2.517323]\n",
      "epoch:5 step:5407 [D loss: 0.597210, acc: 71.09%] [G loss: 2.637608]\n",
      "epoch:5 step:5408 [D loss: 0.614622, acc: 64.06%] [G loss: 2.189275]\n",
      "epoch:5 step:5409 [D loss: 0.519764, acc: 78.91%] [G loss: 2.802484]\n",
      "epoch:5 step:5410 [D loss: 0.495126, acc: 75.78%] [G loss: 2.633619]\n",
      "epoch:5 step:5411 [D loss: 0.753604, acc: 61.72%] [G loss: 2.541736]\n",
      "epoch:5 step:5412 [D loss: 0.592339, acc: 65.62%] [G loss: 2.398127]\n",
      "epoch:5 step:5413 [D loss: 0.641362, acc: 67.19%] [G loss: 2.457314]\n",
      "epoch:5 step:5414 [D loss: 0.620490, acc: 64.84%] [G loss: 2.391169]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:5 step:5415 [D loss: 0.645043, acc: 65.62%] [G loss: 2.432374]\n",
      "epoch:5 step:5416 [D loss: 0.604565, acc: 65.62%] [G loss: 2.241511]\n",
      "epoch:5 step:5417 [D loss: 0.589651, acc: 68.75%] [G loss: 2.720696]\n",
      "epoch:5 step:5418 [D loss: 0.627862, acc: 65.62%] [G loss: 2.457967]\n",
      "epoch:5 step:5419 [D loss: 0.568207, acc: 72.66%] [G loss: 2.575618]\n",
      "epoch:5 step:5420 [D loss: 0.654078, acc: 67.19%] [G loss: 2.277854]\n",
      "epoch:5 step:5421 [D loss: 0.534073, acc: 70.31%] [G loss: 2.828313]\n",
      "epoch:5 step:5422 [D loss: 0.520666, acc: 74.22%] [G loss: 2.487625]\n",
      "epoch:5 step:5423 [D loss: 0.602282, acc: 66.41%] [G loss: 2.723373]\n",
      "epoch:5 step:5424 [D loss: 0.621210, acc: 64.06%] [G loss: 2.540507]\n",
      "epoch:5 step:5425 [D loss: 0.658766, acc: 64.84%] [G loss: 2.539157]\n",
      "epoch:5 step:5426 [D loss: 0.664567, acc: 65.62%] [G loss: 2.467057]\n",
      "epoch:5 step:5427 [D loss: 0.623778, acc: 66.41%] [G loss: 2.201250]\n",
      "epoch:5 step:5428 [D loss: 0.559877, acc: 64.84%] [G loss: 2.358880]\n",
      "epoch:5 step:5429 [D loss: 0.528845, acc: 71.09%] [G loss: 2.657275]\n",
      "epoch:5 step:5430 [D loss: 0.664012, acc: 62.50%] [G loss: 2.455378]\n",
      "epoch:5 step:5431 [D loss: 0.560245, acc: 70.31%] [G loss: 2.588074]\n",
      "epoch:5 step:5432 [D loss: 0.559423, acc: 67.19%] [G loss: 2.688290]\n",
      "epoch:5 step:5433 [D loss: 0.535161, acc: 71.09%] [G loss: 2.803504]\n",
      "epoch:5 step:5434 [D loss: 0.580903, acc: 66.41%] [G loss: 2.416097]\n",
      "epoch:5 step:5435 [D loss: 0.546059, acc: 76.56%] [G loss: 2.710248]\n",
      "epoch:5 step:5436 [D loss: 0.547573, acc: 71.88%] [G loss: 2.360846]\n",
      "epoch:5 step:5437 [D loss: 0.528883, acc: 73.44%] [G loss: 2.628125]\n",
      "epoch:5 step:5438 [D loss: 0.584320, acc: 67.19%] [G loss: 2.659281]\n",
      "epoch:5 step:5439 [D loss: 0.587020, acc: 71.09%] [G loss: 2.772829]\n",
      "epoch:5 step:5440 [D loss: 0.583960, acc: 67.97%] [G loss: 2.614945]\n",
      "epoch:5 step:5441 [D loss: 0.615950, acc: 65.62%] [G loss: 2.578598]\n",
      "epoch:5 step:5442 [D loss: 0.595724, acc: 68.75%] [G loss: 2.728371]\n",
      "epoch:5 step:5443 [D loss: 0.607340, acc: 65.62%] [G loss: 2.374228]\n",
      "epoch:5 step:5444 [D loss: 0.660068, acc: 62.50%] [G loss: 2.512316]\n",
      "epoch:5 step:5445 [D loss: 0.596871, acc: 62.50%] [G loss: 2.524181]\n",
      "epoch:5 step:5446 [D loss: 0.631694, acc: 70.31%] [G loss: 2.594762]\n",
      "epoch:5 step:5447 [D loss: 0.615143, acc: 65.62%] [G loss: 2.418087]\n",
      "epoch:5 step:5448 [D loss: 0.590595, acc: 67.97%] [G loss: 2.367420]\n",
      "epoch:5 step:5449 [D loss: 0.582400, acc: 67.19%] [G loss: 2.570058]\n",
      "epoch:5 step:5450 [D loss: 0.640539, acc: 60.94%] [G loss: 2.373289]\n",
      "epoch:5 step:5451 [D loss: 0.613777, acc: 66.41%] [G loss: 2.386564]\n",
      "epoch:5 step:5452 [D loss: 0.543546, acc: 75.00%] [G loss: 2.305732]\n",
      "epoch:5 step:5453 [D loss: 0.610510, acc: 64.84%] [G loss: 2.647743]\n",
      "epoch:5 step:5454 [D loss: 0.555621, acc: 73.44%] [G loss: 2.789690]\n",
      "epoch:5 step:5455 [D loss: 0.657433, acc: 61.72%] [G loss: 2.260490]\n",
      "epoch:5 step:5456 [D loss: 0.613971, acc: 62.50%] [G loss: 2.310468]\n",
      "epoch:5 step:5457 [D loss: 0.525535, acc: 77.34%] [G loss: 2.544685]\n",
      "epoch:5 step:5458 [D loss: 0.614869, acc: 67.19%] [G loss: 2.513218]\n",
      "epoch:5 step:5459 [D loss: 0.661984, acc: 64.84%] [G loss: 2.829652]\n",
      "epoch:5 step:5460 [D loss: 0.628740, acc: 67.97%] [G loss: 2.820552]\n",
      "epoch:5 step:5461 [D loss: 0.610813, acc: 67.97%] [G loss: 2.472283]\n",
      "epoch:5 step:5462 [D loss: 0.693502, acc: 60.16%] [G loss: 2.473225]\n",
      "epoch:5 step:5463 [D loss: 0.679882, acc: 59.38%] [G loss: 2.341048]\n",
      "epoch:5 step:5464 [D loss: 0.610216, acc: 67.97%] [G loss: 2.275764]\n",
      "epoch:5 step:5465 [D loss: 0.586389, acc: 69.53%] [G loss: 2.386717]\n",
      "epoch:5 step:5466 [D loss: 0.572058, acc: 71.88%] [G loss: 2.512774]\n",
      "epoch:5 step:5467 [D loss: 0.606305, acc: 70.31%] [G loss: 2.465516]\n",
      "epoch:5 step:5468 [D loss: 0.583174, acc: 68.75%] [G loss: 2.563464]\n",
      "epoch:5 step:5469 [D loss: 0.650257, acc: 64.84%] [G loss: 2.577581]\n",
      "epoch:5 step:5470 [D loss: 0.635179, acc: 58.59%] [G loss: 2.297383]\n",
      "epoch:5 step:5471 [D loss: 0.635552, acc: 62.50%] [G loss: 2.469640]\n",
      "epoch:5 step:5472 [D loss: 0.577308, acc: 65.62%] [G loss: 2.475735]\n",
      "epoch:5 step:5473 [D loss: 0.640494, acc: 65.62%] [G loss: 2.311007]\n",
      "epoch:5 step:5474 [D loss: 0.669734, acc: 63.28%] [G loss: 2.199294]\n",
      "epoch:5 step:5475 [D loss: 0.579949, acc: 67.97%] [G loss: 2.699150]\n",
      "epoch:5 step:5476 [D loss: 0.541408, acc: 71.09%] [G loss: 2.933993]\n",
      "epoch:5 step:5477 [D loss: 0.481982, acc: 75.78%] [G loss: 2.822070]\n",
      "epoch:5 step:5478 [D loss: 0.609786, acc: 64.06%] [G loss: 2.704738]\n",
      "epoch:5 step:5479 [D loss: 0.709046, acc: 61.72%] [G loss: 2.338667]\n",
      "epoch:5 step:5480 [D loss: 0.580303, acc: 67.19%] [G loss: 2.724791]\n",
      "epoch:5 step:5481 [D loss: 0.737816, acc: 62.50%] [G loss: 2.421400]\n",
      "epoch:5 step:5482 [D loss: 0.614032, acc: 69.53%] [G loss: 2.069921]\n",
      "epoch:5 step:5483 [D loss: 0.524622, acc: 72.66%] [G loss: 2.535347]\n",
      "epoch:5 step:5484 [D loss: 0.670843, acc: 60.16%] [G loss: 2.293925]\n",
      "epoch:5 step:5485 [D loss: 0.592505, acc: 71.09%] [G loss: 2.365784]\n",
      "epoch:5 step:5486 [D loss: 0.656401, acc: 59.38%] [G loss: 2.331015]\n",
      "epoch:5 step:5487 [D loss: 0.540494, acc: 71.09%] [G loss: 2.699588]\n",
      "epoch:5 step:5488 [D loss: 0.529831, acc: 71.09%] [G loss: 2.467886]\n",
      "epoch:5 step:5489 [D loss: 0.631473, acc: 65.62%] [G loss: 2.688246]\n",
      "epoch:5 step:5490 [D loss: 0.607106, acc: 64.84%] [G loss: 2.817508]\n",
      "epoch:5 step:5491 [D loss: 0.576975, acc: 66.41%] [G loss: 2.802023]\n",
      "epoch:5 step:5492 [D loss: 0.554243, acc: 71.09%] [G loss: 2.514903]\n",
      "epoch:5 step:5493 [D loss: 0.589805, acc: 67.97%] [G loss: 2.668801]\n",
      "epoch:5 step:5494 [D loss: 0.534620, acc: 74.22%] [G loss: 2.704802]\n",
      "epoch:5 step:5495 [D loss: 0.609204, acc: 67.19%] [G loss: 2.666049]\n",
      "epoch:5 step:5496 [D loss: 0.574190, acc: 72.66%] [G loss: 2.521396]\n",
      "epoch:5 step:5497 [D loss: 0.614301, acc: 65.62%] [G loss: 2.315742]\n",
      "epoch:5 step:5498 [D loss: 0.608857, acc: 64.06%] [G loss: 2.492634]\n",
      "epoch:5 step:5499 [D loss: 0.608179, acc: 69.53%] [G loss: 2.493733]\n",
      "epoch:5 step:5500 [D loss: 0.556312, acc: 68.75%] [G loss: 2.938513]\n",
      "epoch:5 step:5501 [D loss: 0.623220, acc: 66.41%] [G loss: 2.645793]\n",
      "epoch:5 step:5502 [D loss: 0.575358, acc: 68.75%] [G loss: 2.716236]\n",
      "epoch:5 step:5503 [D loss: 0.600381, acc: 66.41%] [G loss: 2.332753]\n",
      "epoch:5 step:5504 [D loss: 0.522732, acc: 79.69%] [G loss: 2.406026]\n",
      "epoch:5 step:5505 [D loss: 0.607964, acc: 67.97%] [G loss: 2.286459]\n",
      "epoch:5 step:5506 [D loss: 0.629920, acc: 67.97%] [G loss: 2.530231]\n",
      "epoch:5 step:5507 [D loss: 0.571287, acc: 69.53%] [G loss: 2.772583]\n",
      "epoch:5 step:5508 [D loss: 0.618159, acc: 65.62%] [G loss: 2.314609]\n",
      "epoch:5 step:5509 [D loss: 0.609925, acc: 70.31%] [G loss: 2.418143]\n",
      "epoch:5 step:5510 [D loss: 0.587680, acc: 68.75%] [G loss: 2.346839]\n",
      "epoch:5 step:5511 [D loss: 0.578281, acc: 67.19%] [G loss: 2.730522]\n",
      "epoch:5 step:5512 [D loss: 0.613610, acc: 63.28%] [G loss: 2.305231]\n",
      "epoch:5 step:5513 [D loss: 0.655483, acc: 67.97%] [G loss: 2.193745]\n",
      "epoch:5 step:5514 [D loss: 0.569516, acc: 72.66%] [G loss: 2.268753]\n",
      "epoch:5 step:5515 [D loss: 0.640256, acc: 68.75%] [G loss: 2.503733]\n",
      "epoch:5 step:5516 [D loss: 0.651764, acc: 59.38%] [G loss: 2.452573]\n",
      "epoch:5 step:5517 [D loss: 0.654857, acc: 60.94%] [G loss: 2.438781]\n",
      "epoch:5 step:5518 [D loss: 0.556630, acc: 72.66%] [G loss: 2.520670]\n",
      "epoch:5 step:5519 [D loss: 0.626288, acc: 64.06%] [G loss: 2.620961]\n",
      "epoch:5 step:5520 [D loss: 0.608620, acc: 63.28%] [G loss: 2.518609]\n",
      "epoch:5 step:5521 [D loss: 0.522511, acc: 72.66%] [G loss: 2.366083]\n",
      "epoch:5 step:5522 [D loss: 0.641486, acc: 60.16%] [G loss: 2.584539]\n",
      "epoch:5 step:5523 [D loss: 0.524311, acc: 74.22%] [G loss: 2.517868]\n",
      "epoch:5 step:5524 [D loss: 0.545856, acc: 73.44%] [G loss: 2.362390]\n",
      "epoch:5 step:5525 [D loss: 0.573087, acc: 68.75%] [G loss: 2.903980]\n",
      "epoch:5 step:5526 [D loss: 0.581569, acc: 68.75%] [G loss: 2.598030]\n",
      "epoch:5 step:5527 [D loss: 0.567037, acc: 75.78%] [G loss: 2.767176]\n",
      "epoch:5 step:5528 [D loss: 0.650477, acc: 67.19%] [G loss: 2.381483]\n",
      "epoch:5 step:5529 [D loss: 0.568407, acc: 72.66%] [G loss: 2.527892]\n",
      "epoch:5 step:5530 [D loss: 0.602840, acc: 69.53%] [G loss: 2.488701]\n",
      "epoch:5 step:5531 [D loss: 0.616659, acc: 67.97%] [G loss: 2.332983]\n",
      "epoch:5 step:5532 [D loss: 0.546527, acc: 73.44%] [G loss: 2.492280]\n",
      "epoch:5 step:5533 [D loss: 0.505838, acc: 77.34%] [G loss: 2.573872]\n",
      "epoch:5 step:5534 [D loss: 0.631719, acc: 67.19%] [G loss: 2.507497]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:5 step:5535 [D loss: 0.606766, acc: 64.06%] [G loss: 2.700197]\n",
      "epoch:5 step:5536 [D loss: 0.594648, acc: 63.28%] [G loss: 2.728184]\n",
      "epoch:5 step:5537 [D loss: 0.554058, acc: 70.31%] [G loss: 2.610091]\n",
      "epoch:5 step:5538 [D loss: 0.607161, acc: 65.62%] [G loss: 2.405282]\n",
      "epoch:5 step:5539 [D loss: 0.565179, acc: 70.31%] [G loss: 2.353776]\n",
      "epoch:5 step:5540 [D loss: 0.696470, acc: 62.50%] [G loss: 2.316849]\n",
      "epoch:5 step:5541 [D loss: 0.640934, acc: 60.16%] [G loss: 2.495595]\n",
      "epoch:5 step:5542 [D loss: 0.569174, acc: 72.66%] [G loss: 2.669065]\n",
      "epoch:5 step:5543 [D loss: 0.684810, acc: 62.50%] [G loss: 2.335322]\n",
      "epoch:5 step:5544 [D loss: 0.612773, acc: 67.19%] [G loss: 2.367079]\n",
      "epoch:5 step:5545 [D loss: 0.576278, acc: 73.44%] [G loss: 2.580936]\n",
      "epoch:5 step:5546 [D loss: 0.584027, acc: 68.75%] [G loss: 2.245623]\n",
      "epoch:5 step:5547 [D loss: 0.613525, acc: 65.62%] [G loss: 2.349459]\n",
      "epoch:5 step:5548 [D loss: 0.562126, acc: 70.31%] [G loss: 2.398719]\n",
      "epoch:5 step:5549 [D loss: 0.615892, acc: 67.97%] [G loss: 2.670698]\n",
      "epoch:5 step:5550 [D loss: 0.535351, acc: 67.97%] [G loss: 2.583807]\n",
      "epoch:5 step:5551 [D loss: 0.574334, acc: 70.31%] [G loss: 2.750277]\n",
      "epoch:5 step:5552 [D loss: 0.627190, acc: 64.84%] [G loss: 2.254042]\n",
      "epoch:5 step:5553 [D loss: 0.529074, acc: 75.00%] [G loss: 2.581984]\n",
      "epoch:5 step:5554 [D loss: 0.594173, acc: 70.31%] [G loss: 2.462785]\n",
      "epoch:5 step:5555 [D loss: 0.646293, acc: 61.72%] [G loss: 2.434047]\n",
      "epoch:5 step:5556 [D loss: 0.534447, acc: 72.66%] [G loss: 2.569255]\n",
      "epoch:5 step:5557 [D loss: 0.591078, acc: 71.09%] [G loss: 2.437326]\n",
      "epoch:5 step:5558 [D loss: 0.554786, acc: 73.44%] [G loss: 2.620775]\n",
      "epoch:5 step:5559 [D loss: 0.600157, acc: 65.62%] [G loss: 2.574552]\n",
      "epoch:5 step:5560 [D loss: 0.600375, acc: 71.09%] [G loss: 2.581987]\n",
      "epoch:5 step:5561 [D loss: 0.560272, acc: 71.88%] [G loss: 2.664080]\n",
      "epoch:5 step:5562 [D loss: 0.563075, acc: 73.44%] [G loss: 2.547302]\n",
      "epoch:5 step:5563 [D loss: 0.613751, acc: 69.53%] [G loss: 2.495729]\n",
      "epoch:5 step:5564 [D loss: 0.585917, acc: 68.75%] [G loss: 2.586674]\n",
      "epoch:5 step:5565 [D loss: 0.615061, acc: 67.97%] [G loss: 2.342085]\n",
      "epoch:5 step:5566 [D loss: 0.568048, acc: 71.88%] [G loss: 2.567339]\n",
      "epoch:5 step:5567 [D loss: 0.555039, acc: 70.31%] [G loss: 2.286173]\n",
      "epoch:5 step:5568 [D loss: 0.726237, acc: 60.94%] [G loss: 2.508141]\n",
      "epoch:5 step:5569 [D loss: 0.599038, acc: 65.62%] [G loss: 2.891181]\n",
      "epoch:5 step:5570 [D loss: 0.597113, acc: 68.75%] [G loss: 2.635190]\n",
      "epoch:5 step:5571 [D loss: 0.511201, acc: 74.22%] [G loss: 3.112931]\n",
      "epoch:5 step:5572 [D loss: 0.566925, acc: 71.88%] [G loss: 2.697551]\n",
      "epoch:5 step:5573 [D loss: 0.565357, acc: 70.31%] [G loss: 2.488358]\n",
      "epoch:5 step:5574 [D loss: 0.585833, acc: 69.53%] [G loss: 2.758442]\n",
      "epoch:5 step:5575 [D loss: 0.571070, acc: 74.22%] [G loss: 2.957866]\n",
      "epoch:5 step:5576 [D loss: 0.617319, acc: 67.19%] [G loss: 2.561512]\n",
      "epoch:5 step:5577 [D loss: 0.645424, acc: 67.19%] [G loss: 2.304342]\n",
      "epoch:5 step:5578 [D loss: 0.630399, acc: 65.62%] [G loss: 2.417391]\n",
      "epoch:5 step:5579 [D loss: 0.569669, acc: 72.66%] [G loss: 2.509148]\n",
      "epoch:5 step:5580 [D loss: 0.590555, acc: 66.41%] [G loss: 2.552469]\n",
      "epoch:5 step:5581 [D loss: 0.610420, acc: 68.75%] [G loss: 2.320109]\n",
      "epoch:5 step:5582 [D loss: 0.742614, acc: 58.59%] [G loss: 2.236736]\n",
      "epoch:5 step:5583 [D loss: 0.558839, acc: 69.53%] [G loss: 2.519640]\n",
      "epoch:5 step:5584 [D loss: 0.640303, acc: 62.50%] [G loss: 2.582943]\n",
      "epoch:5 step:5585 [D loss: 0.614559, acc: 67.19%] [G loss: 2.434057]\n",
      "epoch:5 step:5586 [D loss: 0.642319, acc: 62.50%] [G loss: 2.643775]\n",
      "epoch:5 step:5587 [D loss: 0.647087, acc: 60.16%] [G loss: 2.218922]\n",
      "epoch:5 step:5588 [D loss: 0.539134, acc: 71.09%] [G loss: 2.603365]\n",
      "epoch:5 step:5589 [D loss: 0.602052, acc: 64.84%] [G loss: 2.741051]\n",
      "epoch:5 step:5590 [D loss: 0.540180, acc: 72.66%] [G loss: 2.719778]\n",
      "epoch:5 step:5591 [D loss: 0.547939, acc: 71.88%] [G loss: 2.818333]\n",
      "epoch:5 step:5592 [D loss: 0.629186, acc: 65.62%] [G loss: 2.550817]\n",
      "epoch:5 step:5593 [D loss: 0.551324, acc: 69.53%] [G loss: 2.421889]\n",
      "epoch:5 step:5594 [D loss: 0.577989, acc: 71.09%] [G loss: 3.027039]\n",
      "epoch:5 step:5595 [D loss: 0.511666, acc: 77.34%] [G loss: 2.401232]\n",
      "epoch:5 step:5596 [D loss: 0.556887, acc: 69.53%] [G loss: 2.765403]\n",
      "epoch:5 step:5597 [D loss: 0.483317, acc: 78.91%] [G loss: 2.861384]\n",
      "epoch:5 step:5598 [D loss: 0.637312, acc: 65.62%] [G loss: 2.759904]\n",
      "epoch:5 step:5599 [D loss: 0.620418, acc: 64.06%] [G loss: 2.504710]\n",
      "epoch:5 step:5600 [D loss: 0.604041, acc: 68.75%] [G loss: 2.726393]\n",
      "##############\n",
      "[2.775976   1.61922335 6.70770785 5.14087618 4.27436045 5.87039412\n",
      " 5.1797966  5.29315397 5.1937507  3.72780196]\n",
      "##########\n",
      "epoch:5 step:5601 [D loss: 0.559956, acc: 68.75%] [G loss: 2.552688]\n",
      "epoch:5 step:5602 [D loss: 0.607369, acc: 67.97%] [G loss: 2.614580]\n",
      "epoch:5 step:5603 [D loss: 0.502441, acc: 74.22%] [G loss: 2.731392]\n",
      "epoch:5 step:5604 [D loss: 0.498047, acc: 74.22%] [G loss: 3.181000]\n",
      "epoch:5 step:5605 [D loss: 0.714519, acc: 57.03%] [G loss: 2.567715]\n",
      "epoch:5 step:5606 [D loss: 0.576863, acc: 71.88%] [G loss: 3.055374]\n",
      "epoch:5 step:5607 [D loss: 0.589073, acc: 70.31%] [G loss: 2.806382]\n",
      "epoch:5 step:5608 [D loss: 0.579902, acc: 71.88%] [G loss: 2.673895]\n",
      "epoch:5 step:5609 [D loss: 0.420831, acc: 79.69%] [G loss: 3.240474]\n",
      "epoch:5 step:5610 [D loss: 0.567533, acc: 73.44%] [G loss: 2.934655]\n",
      "epoch:5 step:5611 [D loss: 0.608075, acc: 63.28%] [G loss: 2.976360]\n",
      "epoch:5 step:5612 [D loss: 0.611695, acc: 70.31%] [G loss: 3.255633]\n",
      "epoch:5 step:5613 [D loss: 0.845012, acc: 53.12%] [G loss: 2.634209]\n",
      "epoch:5 step:5614 [D loss: 0.662932, acc: 70.31%] [G loss: 2.802112]\n",
      "epoch:5 step:5615 [D loss: 0.557767, acc: 70.31%] [G loss: 2.425191]\n",
      "epoch:5 step:5616 [D loss: 0.542673, acc: 70.31%] [G loss: 2.556425]\n",
      "epoch:5 step:5617 [D loss: 0.583450, acc: 70.31%] [G loss: 2.427722]\n",
      "epoch:5 step:5618 [D loss: 0.528578, acc: 76.56%] [G loss: 2.712036]\n",
      "epoch:5 step:5619 [D loss: 0.577309, acc: 71.88%] [G loss: 2.757734]\n",
      "epoch:5 step:5620 [D loss: 0.573612, acc: 69.53%] [G loss: 2.597371]\n",
      "epoch:5 step:5621 [D loss: 0.498601, acc: 75.00%] [G loss: 2.941294]\n",
      "epoch:5 step:5622 [D loss: 0.490719, acc: 80.47%] [G loss: 3.670375]\n",
      "epoch:6 step:5623 [D loss: 0.574297, acc: 67.97%] [G loss: 2.632881]\n",
      "epoch:6 step:5624 [D loss: 0.529742, acc: 75.00%] [G loss: 2.525747]\n",
      "epoch:6 step:5625 [D loss: 0.611198, acc: 66.41%] [G loss: 2.811685]\n",
      "epoch:6 step:5626 [D loss: 0.618132, acc: 66.41%] [G loss: 2.640205]\n",
      "epoch:6 step:5627 [D loss: 0.570449, acc: 71.09%] [G loss: 2.540802]\n",
      "epoch:6 step:5628 [D loss: 0.639884, acc: 64.84%] [G loss: 2.805517]\n",
      "epoch:6 step:5629 [D loss: 0.558298, acc: 68.75%] [G loss: 2.507368]\n",
      "epoch:6 step:5630 [D loss: 0.585493, acc: 68.75%] [G loss: 2.671281]\n",
      "epoch:6 step:5631 [D loss: 0.588229, acc: 67.19%] [G loss: 2.928539]\n",
      "epoch:6 step:5632 [D loss: 0.541062, acc: 74.22%] [G loss: 3.147835]\n",
      "epoch:6 step:5633 [D loss: 0.575836, acc: 72.66%] [G loss: 2.699203]\n",
      "epoch:6 step:5634 [D loss: 0.631642, acc: 64.06%] [G loss: 2.891896]\n",
      "epoch:6 step:5635 [D loss: 0.545774, acc: 72.66%] [G loss: 2.771120]\n",
      "epoch:6 step:5636 [D loss: 0.635208, acc: 64.06%] [G loss: 2.574692]\n",
      "epoch:6 step:5637 [D loss: 0.504531, acc: 72.66%] [G loss: 2.761944]\n",
      "epoch:6 step:5638 [D loss: 0.567436, acc: 73.44%] [G loss: 2.623501]\n",
      "epoch:6 step:5639 [D loss: 0.601757, acc: 62.50%] [G loss: 2.454065]\n",
      "epoch:6 step:5640 [D loss: 0.637647, acc: 60.16%] [G loss: 2.367234]\n",
      "epoch:6 step:5641 [D loss: 0.661140, acc: 61.72%] [G loss: 2.450866]\n",
      "epoch:6 step:5642 [D loss: 0.735029, acc: 54.69%] [G loss: 2.088803]\n",
      "epoch:6 step:5643 [D loss: 0.588906, acc: 68.75%] [G loss: 2.486020]\n",
      "epoch:6 step:5644 [D loss: 0.555136, acc: 71.88%] [G loss: 2.562367]\n",
      "epoch:6 step:5645 [D loss: 0.580727, acc: 73.44%] [G loss: 2.593787]\n",
      "epoch:6 step:5646 [D loss: 0.595658, acc: 68.75%] [G loss: 2.709866]\n",
      "epoch:6 step:5647 [D loss: 0.586219, acc: 72.66%] [G loss: 2.805155]\n",
      "epoch:6 step:5648 [D loss: 0.633870, acc: 64.06%] [G loss: 2.394504]\n",
      "epoch:6 step:5649 [D loss: 0.613967, acc: 71.09%] [G loss: 2.588084]\n",
      "epoch:6 step:5650 [D loss: 0.595664, acc: 69.53%] [G loss: 2.750350]\n",
      "epoch:6 step:5651 [D loss: 0.617063, acc: 65.62%] [G loss: 2.642742]\n",
      "epoch:6 step:5652 [D loss: 0.629063, acc: 64.06%] [G loss: 2.344197]\n",
      "epoch:6 step:5653 [D loss: 0.643972, acc: 67.97%] [G loss: 2.131192]\n",
      "epoch:6 step:5654 [D loss: 0.619318, acc: 71.09%] [G loss: 2.296750]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:6 step:5655 [D loss: 0.569049, acc: 69.53%] [G loss: 2.711640]\n",
      "epoch:6 step:5656 [D loss: 0.574823, acc: 68.75%] [G loss: 2.644345]\n",
      "epoch:6 step:5657 [D loss: 0.576363, acc: 69.53%] [G loss: 2.653897]\n",
      "epoch:6 step:5658 [D loss: 0.522369, acc: 76.56%] [G loss: 2.745721]\n",
      "epoch:6 step:5659 [D loss: 0.556120, acc: 75.00%] [G loss: 2.823540]\n",
      "epoch:6 step:5660 [D loss: 0.547392, acc: 71.09%] [G loss: 2.679868]\n",
      "epoch:6 step:5661 [D loss: 0.550013, acc: 71.88%] [G loss: 2.535269]\n",
      "epoch:6 step:5662 [D loss: 0.537760, acc: 75.00%] [G loss: 3.071931]\n",
      "epoch:6 step:5663 [D loss: 0.564750, acc: 70.31%] [G loss: 2.313873]\n",
      "epoch:6 step:5664 [D loss: 0.520533, acc: 75.00%] [G loss: 2.760885]\n",
      "epoch:6 step:5665 [D loss: 0.497645, acc: 75.78%] [G loss: 2.609413]\n",
      "epoch:6 step:5666 [D loss: 0.639380, acc: 60.16%] [G loss: 2.430745]\n",
      "epoch:6 step:5667 [D loss: 0.595304, acc: 65.62%] [G loss: 2.526488]\n",
      "epoch:6 step:5668 [D loss: 0.570560, acc: 75.00%] [G loss: 2.571152]\n",
      "epoch:6 step:5669 [D loss: 0.571738, acc: 68.75%] [G loss: 2.779888]\n",
      "epoch:6 step:5670 [D loss: 0.546065, acc: 73.44%] [G loss: 2.647938]\n",
      "epoch:6 step:5671 [D loss: 0.625068, acc: 62.50%] [G loss: 2.814873]\n",
      "epoch:6 step:5672 [D loss: 0.655633, acc: 64.06%] [G loss: 2.426262]\n",
      "epoch:6 step:5673 [D loss: 0.536851, acc: 72.66%] [G loss: 2.588579]\n",
      "epoch:6 step:5674 [D loss: 0.551764, acc: 71.09%] [G loss: 2.646333]\n",
      "epoch:6 step:5675 [D loss: 0.580748, acc: 67.19%] [G loss: 2.707042]\n",
      "epoch:6 step:5676 [D loss: 0.548461, acc: 73.44%] [G loss: 2.735072]\n",
      "epoch:6 step:5677 [D loss: 0.625228, acc: 64.84%] [G loss: 2.807122]\n",
      "epoch:6 step:5678 [D loss: 0.547017, acc: 73.44%] [G loss: 2.585778]\n",
      "epoch:6 step:5679 [D loss: 0.562636, acc: 67.19%] [G loss: 2.728779]\n",
      "epoch:6 step:5680 [D loss: 0.571499, acc: 68.75%] [G loss: 2.520529]\n",
      "epoch:6 step:5681 [D loss: 0.591082, acc: 67.19%] [G loss: 2.552234]\n",
      "epoch:6 step:5682 [D loss: 0.604307, acc: 64.84%] [G loss: 2.625902]\n",
      "epoch:6 step:5683 [D loss: 0.592717, acc: 65.62%] [G loss: 2.632885]\n",
      "epoch:6 step:5684 [D loss: 0.616870, acc: 66.41%] [G loss: 2.534657]\n",
      "epoch:6 step:5685 [D loss: 0.583473, acc: 71.09%] [G loss: 2.474505]\n",
      "epoch:6 step:5686 [D loss: 0.637745, acc: 72.66%] [G loss: 2.513628]\n",
      "epoch:6 step:5687 [D loss: 0.725431, acc: 61.72%] [G loss: 2.311664]\n",
      "epoch:6 step:5688 [D loss: 0.600369, acc: 66.41%] [G loss: 2.231030]\n",
      "epoch:6 step:5689 [D loss: 0.626745, acc: 66.41%] [G loss: 2.483610]\n",
      "epoch:6 step:5690 [D loss: 0.557826, acc: 71.09%] [G loss: 2.410278]\n",
      "epoch:6 step:5691 [D loss: 0.625454, acc: 65.62%] [G loss: 2.568403]\n",
      "epoch:6 step:5692 [D loss: 0.593765, acc: 67.97%] [G loss: 2.705648]\n",
      "epoch:6 step:5693 [D loss: 0.642667, acc: 64.84%] [G loss: 2.693490]\n",
      "epoch:6 step:5694 [D loss: 0.560079, acc: 67.97%] [G loss: 2.473369]\n",
      "epoch:6 step:5695 [D loss: 0.601718, acc: 68.75%] [G loss: 2.471003]\n",
      "epoch:6 step:5696 [D loss: 0.589258, acc: 65.62%] [G loss: 2.427360]\n",
      "epoch:6 step:5697 [D loss: 0.502387, acc: 75.78%] [G loss: 3.112350]\n",
      "epoch:6 step:5698 [D loss: 0.544189, acc: 73.44%] [G loss: 2.926838]\n",
      "epoch:6 step:5699 [D loss: 0.576414, acc: 71.09%] [G loss: 2.978228]\n",
      "epoch:6 step:5700 [D loss: 0.672331, acc: 59.38%] [G loss: 2.604081]\n",
      "epoch:6 step:5701 [D loss: 0.682113, acc: 62.50%] [G loss: 2.447896]\n",
      "epoch:6 step:5702 [D loss: 0.581991, acc: 69.53%] [G loss: 2.369281]\n",
      "epoch:6 step:5703 [D loss: 0.605117, acc: 71.09%] [G loss: 2.394062]\n",
      "epoch:6 step:5704 [D loss: 0.559081, acc: 66.41%] [G loss: 2.567161]\n",
      "epoch:6 step:5705 [D loss: 0.651815, acc: 66.41%] [G loss: 2.553365]\n",
      "epoch:6 step:5706 [D loss: 0.632968, acc: 62.50%] [G loss: 2.222219]\n",
      "epoch:6 step:5707 [D loss: 0.666950, acc: 56.25%] [G loss: 2.521390]\n",
      "epoch:6 step:5708 [D loss: 0.597597, acc: 64.84%] [G loss: 2.378240]\n",
      "epoch:6 step:5709 [D loss: 0.543486, acc: 74.22%] [G loss: 2.439132]\n",
      "epoch:6 step:5710 [D loss: 0.571347, acc: 71.09%] [G loss: 2.265524]\n",
      "epoch:6 step:5711 [D loss: 0.573422, acc: 71.88%] [G loss: 2.708396]\n",
      "epoch:6 step:5712 [D loss: 0.616380, acc: 68.75%] [G loss: 2.436821]\n",
      "epoch:6 step:5713 [D loss: 0.569154, acc: 75.00%] [G loss: 2.803888]\n",
      "epoch:6 step:5714 [D loss: 0.550346, acc: 73.44%] [G loss: 2.922695]\n",
      "epoch:6 step:5715 [D loss: 0.526653, acc: 77.34%] [G loss: 2.734783]\n",
      "epoch:6 step:5716 [D loss: 0.588037, acc: 69.53%] [G loss: 2.768700]\n",
      "epoch:6 step:5717 [D loss: 0.590020, acc: 70.31%] [G loss: 2.236753]\n",
      "epoch:6 step:5718 [D loss: 0.525919, acc: 71.88%] [G loss: 2.824069]\n",
      "epoch:6 step:5719 [D loss: 0.584085, acc: 67.97%] [G loss: 2.488184]\n",
      "epoch:6 step:5720 [D loss: 0.656447, acc: 71.88%] [G loss: 2.350970]\n",
      "epoch:6 step:5721 [D loss: 0.632753, acc: 67.97%] [G loss: 2.490796]\n",
      "epoch:6 step:5722 [D loss: 0.595986, acc: 68.75%] [G loss: 2.726643]\n",
      "epoch:6 step:5723 [D loss: 0.617773, acc: 70.31%] [G loss: 2.403789]\n",
      "epoch:6 step:5724 [D loss: 0.618567, acc: 71.09%] [G loss: 2.448303]\n",
      "epoch:6 step:5725 [D loss: 0.559164, acc: 73.44%] [G loss: 2.694647]\n",
      "epoch:6 step:5726 [D loss: 0.561990, acc: 70.31%] [G loss: 2.403713]\n",
      "epoch:6 step:5727 [D loss: 0.558302, acc: 68.75%] [G loss: 2.594237]\n",
      "epoch:6 step:5728 [D loss: 0.647852, acc: 62.50%] [G loss: 2.453684]\n",
      "epoch:6 step:5729 [D loss: 0.690498, acc: 59.38%] [G loss: 2.515117]\n",
      "epoch:6 step:5730 [D loss: 0.694261, acc: 61.72%] [G loss: 2.325327]\n",
      "epoch:6 step:5731 [D loss: 0.630181, acc: 61.72%] [G loss: 2.356197]\n",
      "epoch:6 step:5732 [D loss: 0.658359, acc: 64.06%] [G loss: 2.266840]\n",
      "epoch:6 step:5733 [D loss: 0.542380, acc: 75.78%] [G loss: 2.529636]\n",
      "epoch:6 step:5734 [D loss: 0.525775, acc: 75.00%] [G loss: 2.624794]\n",
      "epoch:6 step:5735 [D loss: 0.621252, acc: 64.06%] [G loss: 2.314281]\n",
      "epoch:6 step:5736 [D loss: 0.638571, acc: 58.59%] [G loss: 2.636134]\n",
      "epoch:6 step:5737 [D loss: 0.600199, acc: 67.19%] [G loss: 2.653170]\n",
      "epoch:6 step:5738 [D loss: 0.663705, acc: 65.62%] [G loss: 2.624602]\n",
      "epoch:6 step:5739 [D loss: 0.522077, acc: 79.69%] [G loss: 2.697413]\n",
      "epoch:6 step:5740 [D loss: 0.526099, acc: 75.00%] [G loss: 2.366397]\n",
      "epoch:6 step:5741 [D loss: 0.526500, acc: 72.66%] [G loss: 2.874978]\n",
      "epoch:6 step:5742 [D loss: 0.602626, acc: 68.75%] [G loss: 2.643021]\n",
      "epoch:6 step:5743 [D loss: 0.591348, acc: 67.19%] [G loss: 2.392801]\n",
      "epoch:6 step:5744 [D loss: 0.604380, acc: 71.09%] [G loss: 2.778878]\n",
      "epoch:6 step:5745 [D loss: 0.627200, acc: 64.84%] [G loss: 2.557720]\n",
      "epoch:6 step:5746 [D loss: 0.688665, acc: 62.50%] [G loss: 2.343032]\n",
      "epoch:6 step:5747 [D loss: 0.565939, acc: 69.53%] [G loss: 2.166775]\n",
      "epoch:6 step:5748 [D loss: 0.648427, acc: 64.06%] [G loss: 2.612910]\n",
      "epoch:6 step:5749 [D loss: 0.608552, acc: 67.19%] [G loss: 2.217621]\n",
      "epoch:6 step:5750 [D loss: 0.643549, acc: 59.38%] [G loss: 2.623433]\n",
      "epoch:6 step:5751 [D loss: 0.614041, acc: 67.97%] [G loss: 2.429765]\n",
      "epoch:6 step:5752 [D loss: 0.581966, acc: 68.75%] [G loss: 2.543529]\n",
      "epoch:6 step:5753 [D loss: 0.577302, acc: 71.09%] [G loss: 2.603376]\n",
      "epoch:6 step:5754 [D loss: 0.570981, acc: 71.88%] [G loss: 2.517808]\n",
      "epoch:6 step:5755 [D loss: 0.693590, acc: 60.16%] [G loss: 2.202429]\n",
      "epoch:6 step:5756 [D loss: 0.695653, acc: 61.72%] [G loss: 2.366739]\n",
      "epoch:6 step:5757 [D loss: 0.568754, acc: 71.09%] [G loss: 2.351187]\n",
      "epoch:6 step:5758 [D loss: 0.584922, acc: 69.53%] [G loss: 2.443534]\n",
      "epoch:6 step:5759 [D loss: 0.584054, acc: 74.22%] [G loss: 2.350218]\n",
      "epoch:6 step:5760 [D loss: 0.625674, acc: 62.50%] [G loss: 2.220744]\n",
      "epoch:6 step:5761 [D loss: 0.619680, acc: 66.41%] [G loss: 2.264256]\n",
      "epoch:6 step:5762 [D loss: 0.655190, acc: 57.81%] [G loss: 2.321163]\n",
      "epoch:6 step:5763 [D loss: 0.586298, acc: 71.09%] [G loss: 2.465276]\n",
      "epoch:6 step:5764 [D loss: 0.591283, acc: 71.88%] [G loss: 2.163503]\n",
      "epoch:6 step:5765 [D loss: 0.668470, acc: 62.50%] [G loss: 2.531914]\n",
      "epoch:6 step:5766 [D loss: 0.663407, acc: 59.38%] [G loss: 2.609802]\n",
      "epoch:6 step:5767 [D loss: 0.621569, acc: 64.84%] [G loss: 2.365440]\n",
      "epoch:6 step:5768 [D loss: 0.664100, acc: 64.06%] [G loss: 2.360753]\n",
      "epoch:6 step:5769 [D loss: 0.726985, acc: 57.03%] [G loss: 2.201029]\n",
      "epoch:6 step:5770 [D loss: 0.647999, acc: 64.06%] [G loss: 2.213324]\n",
      "epoch:6 step:5771 [D loss: 0.506189, acc: 77.34%] [G loss: 2.502255]\n",
      "epoch:6 step:5772 [D loss: 0.620893, acc: 64.84%] [G loss: 2.639681]\n",
      "epoch:6 step:5773 [D loss: 0.508676, acc: 71.09%] [G loss: 3.020685]\n",
      "epoch:6 step:5774 [D loss: 0.547630, acc: 75.78%] [G loss: 2.725831]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:6 step:5775 [D loss: 0.679323, acc: 57.81%] [G loss: 2.445085]\n",
      "epoch:6 step:5776 [D loss: 0.638905, acc: 64.06%] [G loss: 2.538697]\n",
      "epoch:6 step:5777 [D loss: 0.568285, acc: 71.88%] [G loss: 2.455608]\n",
      "epoch:6 step:5778 [D loss: 0.580511, acc: 73.44%] [G loss: 2.494514]\n",
      "epoch:6 step:5779 [D loss: 0.614578, acc: 67.97%] [G loss: 2.290204]\n",
      "epoch:6 step:5780 [D loss: 0.602122, acc: 64.84%] [G loss: 2.322587]\n",
      "epoch:6 step:5781 [D loss: 0.559090, acc: 71.88%] [G loss: 2.667695]\n",
      "epoch:6 step:5782 [D loss: 0.745598, acc: 57.81%] [G loss: 2.199402]\n",
      "epoch:6 step:5783 [D loss: 0.643424, acc: 63.28%] [G loss: 2.423270]\n",
      "epoch:6 step:5784 [D loss: 0.587222, acc: 67.97%] [G loss: 2.490277]\n",
      "epoch:6 step:5785 [D loss: 0.585698, acc: 67.97%] [G loss: 2.401663]\n",
      "epoch:6 step:5786 [D loss: 0.631372, acc: 66.41%] [G loss: 2.117032]\n",
      "epoch:6 step:5787 [D loss: 0.634101, acc: 71.09%] [G loss: 2.389620]\n",
      "epoch:6 step:5788 [D loss: 0.629364, acc: 66.41%] [G loss: 2.189614]\n",
      "epoch:6 step:5789 [D loss: 0.605379, acc: 67.19%] [G loss: 2.558018]\n",
      "epoch:6 step:5790 [D loss: 0.562797, acc: 72.66%] [G loss: 2.723608]\n",
      "epoch:6 step:5791 [D loss: 0.580492, acc: 64.84%] [G loss: 2.416533]\n",
      "epoch:6 step:5792 [D loss: 0.584914, acc: 67.97%] [G loss: 2.282489]\n",
      "epoch:6 step:5793 [D loss: 0.585562, acc: 68.75%] [G loss: 2.367992]\n",
      "epoch:6 step:5794 [D loss: 0.651838, acc: 55.47%] [G loss: 2.216435]\n",
      "epoch:6 step:5795 [D loss: 0.608482, acc: 67.19%] [G loss: 2.501096]\n",
      "epoch:6 step:5796 [D loss: 0.598381, acc: 66.41%] [G loss: 2.444964]\n",
      "epoch:6 step:5797 [D loss: 0.649962, acc: 63.28%] [G loss: 2.232911]\n",
      "epoch:6 step:5798 [D loss: 0.555457, acc: 73.44%] [G loss: 2.590350]\n",
      "epoch:6 step:5799 [D loss: 0.546991, acc: 74.22%] [G loss: 2.685411]\n",
      "epoch:6 step:5800 [D loss: 0.662368, acc: 59.38%] [G loss: 2.435094]\n",
      "##############\n",
      "[2.84888821 1.55777944 6.76466303 5.2147847  4.20569638 5.85012795\n",
      " 5.17539788 4.80232823 5.22366179 3.59076605]\n",
      "##########\n",
      "epoch:6 step:5801 [D loss: 0.575952, acc: 73.44%] [G loss: 2.310052]\n",
      "epoch:6 step:5802 [D loss: 0.591930, acc: 70.31%] [G loss: 2.485103]\n",
      "epoch:6 step:5803 [D loss: 0.568110, acc: 72.66%] [G loss: 2.417292]\n",
      "epoch:6 step:5804 [D loss: 0.568466, acc: 70.31%] [G loss: 2.316977]\n",
      "epoch:6 step:5805 [D loss: 0.647219, acc: 64.06%] [G loss: 2.468338]\n",
      "epoch:6 step:5806 [D loss: 0.579366, acc: 72.66%] [G loss: 2.615058]\n",
      "epoch:6 step:5807 [D loss: 0.614662, acc: 67.97%] [G loss: 2.487569]\n",
      "epoch:6 step:5808 [D loss: 0.602987, acc: 67.97%] [G loss: 2.493679]\n",
      "epoch:6 step:5809 [D loss: 0.627096, acc: 63.28%] [G loss: 2.578358]\n",
      "epoch:6 step:5810 [D loss: 0.578353, acc: 67.97%] [G loss: 2.444963]\n",
      "epoch:6 step:5811 [D loss: 0.622174, acc: 67.97%] [G loss: 2.737529]\n",
      "epoch:6 step:5812 [D loss: 0.605900, acc: 72.66%] [G loss: 2.761849]\n",
      "epoch:6 step:5813 [D loss: 0.498862, acc: 81.25%] [G loss: 2.578133]\n",
      "epoch:6 step:5814 [D loss: 0.572873, acc: 68.75%] [G loss: 2.546032]\n",
      "epoch:6 step:5815 [D loss: 0.614318, acc: 66.41%] [G loss: 2.721170]\n",
      "epoch:6 step:5816 [D loss: 0.528204, acc: 71.09%] [G loss: 3.057714]\n",
      "epoch:6 step:5817 [D loss: 0.594860, acc: 67.97%] [G loss: 2.575831]\n",
      "epoch:6 step:5818 [D loss: 0.524000, acc: 78.91%] [G loss: 2.846250]\n",
      "epoch:6 step:5819 [D loss: 0.569580, acc: 70.31%] [G loss: 2.751538]\n",
      "epoch:6 step:5820 [D loss: 0.592811, acc: 64.84%] [G loss: 2.625148]\n",
      "epoch:6 step:5821 [D loss: 0.710834, acc: 56.25%] [G loss: 2.480359]\n",
      "epoch:6 step:5822 [D loss: 0.683931, acc: 63.28%] [G loss: 2.138156]\n",
      "epoch:6 step:5823 [D loss: 0.627174, acc: 66.41%] [G loss: 2.369151]\n",
      "epoch:6 step:5824 [D loss: 0.608613, acc: 67.19%] [G loss: 2.422245]\n",
      "epoch:6 step:5825 [D loss: 0.537966, acc: 68.75%] [G loss: 2.486712]\n",
      "epoch:6 step:5826 [D loss: 0.631650, acc: 63.28%] [G loss: 2.396413]\n",
      "epoch:6 step:5827 [D loss: 0.622263, acc: 65.62%] [G loss: 2.258739]\n",
      "epoch:6 step:5828 [D loss: 0.589530, acc: 67.19%] [G loss: 2.480053]\n",
      "epoch:6 step:5829 [D loss: 0.568615, acc: 68.75%] [G loss: 3.034883]\n",
      "epoch:6 step:5830 [D loss: 0.578546, acc: 68.75%] [G loss: 2.920169]\n",
      "epoch:6 step:5831 [D loss: 0.544284, acc: 72.66%] [G loss: 2.705491]\n",
      "epoch:6 step:5832 [D loss: 0.650946, acc: 59.38%] [G loss: 2.471016]\n",
      "epoch:6 step:5833 [D loss: 0.713881, acc: 55.47%] [G loss: 2.065823]\n",
      "epoch:6 step:5834 [D loss: 0.637888, acc: 66.41%] [G loss: 2.327369]\n",
      "epoch:6 step:5835 [D loss: 0.595911, acc: 67.97%] [G loss: 2.289711]\n",
      "epoch:6 step:5836 [D loss: 0.612875, acc: 66.41%] [G loss: 2.182004]\n",
      "epoch:6 step:5837 [D loss: 0.618354, acc: 65.62%] [G loss: 2.206325]\n",
      "epoch:6 step:5838 [D loss: 0.671851, acc: 61.72%] [G loss: 2.315365]\n",
      "epoch:6 step:5839 [D loss: 0.524538, acc: 71.88%] [G loss: 2.920415]\n",
      "epoch:6 step:5840 [D loss: 0.627375, acc: 67.19%] [G loss: 2.747363]\n",
      "epoch:6 step:5841 [D loss: 0.607282, acc: 61.72%] [G loss: 2.791090]\n",
      "epoch:6 step:5842 [D loss: 0.628985, acc: 64.06%] [G loss: 2.281210]\n",
      "epoch:6 step:5843 [D loss: 0.614081, acc: 63.28%] [G loss: 2.553320]\n",
      "epoch:6 step:5844 [D loss: 0.565884, acc: 69.53%] [G loss: 2.787469]\n",
      "epoch:6 step:5845 [D loss: 0.625260, acc: 66.41%] [G loss: 2.548033]\n",
      "epoch:6 step:5846 [D loss: 0.608918, acc: 69.53%] [G loss: 2.266309]\n",
      "epoch:6 step:5847 [D loss: 0.615865, acc: 64.06%] [G loss: 2.246485]\n",
      "epoch:6 step:5848 [D loss: 0.572185, acc: 62.50%] [G loss: 2.210187]\n",
      "epoch:6 step:5849 [D loss: 0.571391, acc: 72.66%] [G loss: 2.140808]\n",
      "epoch:6 step:5850 [D loss: 0.715689, acc: 59.38%] [G loss: 1.956449]\n",
      "epoch:6 step:5851 [D loss: 0.630989, acc: 66.41%] [G loss: 2.318357]\n",
      "epoch:6 step:5852 [D loss: 0.519708, acc: 76.56%] [G loss: 2.654304]\n",
      "epoch:6 step:5853 [D loss: 0.580317, acc: 71.09%] [G loss: 2.557891]\n",
      "epoch:6 step:5854 [D loss: 0.550802, acc: 73.44%] [G loss: 2.920683]\n",
      "epoch:6 step:5855 [D loss: 0.612310, acc: 68.75%] [G loss: 2.600605]\n",
      "epoch:6 step:5856 [D loss: 0.632704, acc: 61.72%] [G loss: 2.417279]\n",
      "epoch:6 step:5857 [D loss: 0.606554, acc: 67.19%] [G loss: 2.326792]\n",
      "epoch:6 step:5858 [D loss: 0.540607, acc: 69.53%] [G loss: 2.417001]\n",
      "epoch:6 step:5859 [D loss: 0.674282, acc: 60.94%] [G loss: 2.237292]\n",
      "epoch:6 step:5860 [D loss: 0.644526, acc: 70.31%] [G loss: 2.279604]\n",
      "epoch:6 step:5861 [D loss: 0.558751, acc: 68.75%] [G loss: 2.426236]\n",
      "epoch:6 step:5862 [D loss: 0.580145, acc: 63.28%] [G loss: 2.537909]\n",
      "epoch:6 step:5863 [D loss: 0.574927, acc: 67.19%] [G loss: 2.556084]\n",
      "epoch:6 step:5864 [D loss: 0.632519, acc: 64.06%] [G loss: 2.516217]\n",
      "epoch:6 step:5865 [D loss: 0.579472, acc: 71.09%] [G loss: 2.327970]\n",
      "epoch:6 step:5866 [D loss: 0.597510, acc: 71.09%] [G loss: 2.609038]\n",
      "epoch:6 step:5867 [D loss: 0.551095, acc: 73.44%] [G loss: 2.505322]\n",
      "epoch:6 step:5868 [D loss: 0.701739, acc: 57.03%] [G loss: 2.346173]\n",
      "epoch:6 step:5869 [D loss: 0.604706, acc: 67.97%] [G loss: 2.435774]\n",
      "epoch:6 step:5870 [D loss: 0.620156, acc: 68.75%] [G loss: 2.402329]\n",
      "epoch:6 step:5871 [D loss: 0.624527, acc: 67.97%] [G loss: 2.128455]\n",
      "epoch:6 step:5872 [D loss: 0.670967, acc: 60.16%] [G loss: 2.406844]\n",
      "epoch:6 step:5873 [D loss: 0.668621, acc: 57.03%] [G loss: 2.373408]\n",
      "epoch:6 step:5874 [D loss: 0.584059, acc: 69.53%] [G loss: 2.228510]\n",
      "epoch:6 step:5875 [D loss: 0.542682, acc: 70.31%] [G loss: 2.331737]\n",
      "epoch:6 step:5876 [D loss: 0.560449, acc: 74.22%] [G loss: 2.401331]\n",
      "epoch:6 step:5877 [D loss: 0.625374, acc: 63.28%] [G loss: 2.522186]\n",
      "epoch:6 step:5878 [D loss: 0.677602, acc: 61.72%] [G loss: 2.289051]\n",
      "epoch:6 step:5879 [D loss: 0.573910, acc: 71.88%] [G loss: 2.392653]\n",
      "epoch:6 step:5880 [D loss: 0.605048, acc: 71.09%] [G loss: 2.581429]\n",
      "epoch:6 step:5881 [D loss: 0.618923, acc: 70.31%] [G loss: 2.390482]\n",
      "epoch:6 step:5882 [D loss: 0.595768, acc: 67.19%] [G loss: 2.445829]\n",
      "epoch:6 step:5883 [D loss: 0.581452, acc: 64.84%] [G loss: 2.477061]\n",
      "epoch:6 step:5884 [D loss: 0.590739, acc: 67.19%] [G loss: 2.509549]\n",
      "epoch:6 step:5885 [D loss: 0.614581, acc: 65.62%] [G loss: 2.517675]\n",
      "epoch:6 step:5886 [D loss: 0.531098, acc: 72.66%] [G loss: 2.533139]\n",
      "epoch:6 step:5887 [D loss: 0.659392, acc: 64.84%] [G loss: 2.279702]\n",
      "epoch:6 step:5888 [D loss: 0.636491, acc: 64.84%] [G loss: 2.522501]\n",
      "epoch:6 step:5889 [D loss: 0.552834, acc: 71.09%] [G loss: 2.396720]\n",
      "epoch:6 step:5890 [D loss: 0.690144, acc: 60.16%] [G loss: 2.514886]\n",
      "epoch:6 step:5891 [D loss: 0.577851, acc: 68.75%] [G loss: 2.567065]\n",
      "epoch:6 step:5892 [D loss: 0.602985, acc: 65.62%] [G loss: 2.323437]\n",
      "epoch:6 step:5893 [D loss: 0.509301, acc: 77.34%] [G loss: 2.481712]\n",
      "epoch:6 step:5894 [D loss: 0.586618, acc: 69.53%] [G loss: 2.504538]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:6 step:5895 [D loss: 0.587494, acc: 66.41%] [G loss: 2.535527]\n",
      "epoch:6 step:5896 [D loss: 0.559630, acc: 69.53%] [G loss: 2.555850]\n",
      "epoch:6 step:5897 [D loss: 0.579644, acc: 64.84%] [G loss: 2.449511]\n",
      "epoch:6 step:5898 [D loss: 0.622521, acc: 63.28%] [G loss: 2.538597]\n",
      "epoch:6 step:5899 [D loss: 0.678272, acc: 60.94%] [G loss: 2.140265]\n",
      "epoch:6 step:5900 [D loss: 0.582747, acc: 70.31%] [G loss: 2.245515]\n",
      "epoch:6 step:5901 [D loss: 0.602545, acc: 64.84%] [G loss: 2.539819]\n",
      "epoch:6 step:5902 [D loss: 0.636620, acc: 60.94%] [G loss: 2.508108]\n",
      "epoch:6 step:5903 [D loss: 0.681716, acc: 61.72%] [G loss: 2.271107]\n",
      "epoch:6 step:5904 [D loss: 0.532720, acc: 75.78%] [G loss: 2.619018]\n",
      "epoch:6 step:5905 [D loss: 0.641297, acc: 64.84%] [G loss: 2.597512]\n",
      "epoch:6 step:5906 [D loss: 0.612451, acc: 65.62%] [G loss: 2.645466]\n",
      "epoch:6 step:5907 [D loss: 0.537297, acc: 71.09%] [G loss: 2.522894]\n",
      "epoch:6 step:5908 [D loss: 0.570024, acc: 65.62%] [G loss: 2.589657]\n",
      "epoch:6 step:5909 [D loss: 0.549252, acc: 69.53%] [G loss: 2.579069]\n",
      "epoch:6 step:5910 [D loss: 0.612342, acc: 64.84%] [G loss: 2.431415]\n",
      "epoch:6 step:5911 [D loss: 0.579089, acc: 72.66%] [G loss: 2.362265]\n",
      "epoch:6 step:5912 [D loss: 0.627788, acc: 63.28%] [G loss: 2.552250]\n",
      "epoch:6 step:5913 [D loss: 0.577706, acc: 68.75%] [G loss: 2.563765]\n",
      "epoch:6 step:5914 [D loss: 0.669149, acc: 65.62%] [G loss: 2.314705]\n",
      "epoch:6 step:5915 [D loss: 0.554984, acc: 75.00%] [G loss: 2.422770]\n",
      "epoch:6 step:5916 [D loss: 0.580004, acc: 71.09%] [G loss: 2.535035]\n",
      "epoch:6 step:5917 [D loss: 0.610671, acc: 66.41%] [G loss: 2.552757]\n",
      "epoch:6 step:5918 [D loss: 0.581068, acc: 67.97%] [G loss: 2.675034]\n",
      "epoch:6 step:5919 [D loss: 0.716906, acc: 58.59%] [G loss: 2.321717]\n",
      "epoch:6 step:5920 [D loss: 0.527083, acc: 75.78%] [G loss: 2.677120]\n",
      "epoch:6 step:5921 [D loss: 0.569310, acc: 67.97%] [G loss: 2.487577]\n",
      "epoch:6 step:5922 [D loss: 0.599338, acc: 67.97%] [G loss: 2.423513]\n",
      "epoch:6 step:5923 [D loss: 0.634090, acc: 64.06%] [G loss: 2.231725]\n",
      "epoch:6 step:5924 [D loss: 0.507617, acc: 75.78%] [G loss: 2.530048]\n",
      "epoch:6 step:5925 [D loss: 0.650814, acc: 61.72%] [G loss: 2.710559]\n",
      "epoch:6 step:5926 [D loss: 0.564676, acc: 70.31%] [G loss: 2.574092]\n",
      "epoch:6 step:5927 [D loss: 0.522292, acc: 71.88%] [G loss: 2.633720]\n",
      "epoch:6 step:5928 [D loss: 0.723090, acc: 57.03%] [G loss: 2.280340]\n",
      "epoch:6 step:5929 [D loss: 0.534447, acc: 73.44%] [G loss: 2.360648]\n",
      "epoch:6 step:5930 [D loss: 0.691639, acc: 57.81%] [G loss: 2.169784]\n",
      "epoch:6 step:5931 [D loss: 0.585932, acc: 67.19%] [G loss: 2.632816]\n",
      "epoch:6 step:5932 [D loss: 0.570132, acc: 72.66%] [G loss: 2.399432]\n",
      "epoch:6 step:5933 [D loss: 0.578702, acc: 68.75%] [G loss: 2.530007]\n",
      "epoch:6 step:5934 [D loss: 0.521954, acc: 70.31%] [G loss: 3.038307]\n",
      "epoch:6 step:5935 [D loss: 0.465889, acc: 77.34%] [G loss: 3.286685]\n",
      "epoch:6 step:5936 [D loss: 0.495349, acc: 71.88%] [G loss: 2.964565]\n",
      "epoch:6 step:5937 [D loss: 0.511687, acc: 72.66%] [G loss: 3.350039]\n",
      "epoch:6 step:5938 [D loss: 0.628689, acc: 67.19%] [G loss: 2.561279]\n",
      "epoch:6 step:5939 [D loss: 0.662817, acc: 59.38%] [G loss: 2.469483]\n",
      "epoch:6 step:5940 [D loss: 0.528769, acc: 74.22%] [G loss: 2.453555]\n",
      "epoch:6 step:5941 [D loss: 0.558593, acc: 68.75%] [G loss: 2.503685]\n",
      "epoch:6 step:5942 [D loss: 0.627293, acc: 69.53%] [G loss: 2.466241]\n",
      "epoch:6 step:5943 [D loss: 0.536270, acc: 72.66%] [G loss: 2.783249]\n",
      "epoch:6 step:5944 [D loss: 0.684558, acc: 65.62%] [G loss: 2.389899]\n",
      "epoch:6 step:5945 [D loss: 0.729219, acc: 58.59%] [G loss: 2.198644]\n",
      "epoch:6 step:5946 [D loss: 0.650737, acc: 62.50%] [G loss: 2.245242]\n",
      "epoch:6 step:5947 [D loss: 0.620867, acc: 67.19%] [G loss: 2.599838]\n",
      "epoch:6 step:5948 [D loss: 0.573834, acc: 69.53%] [G loss: 2.240515]\n",
      "epoch:6 step:5949 [D loss: 0.622901, acc: 67.97%] [G loss: 2.431726]\n",
      "epoch:6 step:5950 [D loss: 0.591043, acc: 70.31%] [G loss: 2.441208]\n",
      "epoch:6 step:5951 [D loss: 0.636082, acc: 65.62%] [G loss: 2.333459]\n",
      "epoch:6 step:5952 [D loss: 0.634787, acc: 64.06%] [G loss: 2.409744]\n",
      "epoch:6 step:5953 [D loss: 0.533185, acc: 75.78%] [G loss: 2.586730]\n",
      "epoch:6 step:5954 [D loss: 0.597275, acc: 71.88%] [G loss: 2.564776]\n",
      "epoch:6 step:5955 [D loss: 0.638163, acc: 67.19%] [G loss: 2.459932]\n",
      "epoch:6 step:5956 [D loss: 0.672197, acc: 57.03%] [G loss: 2.266924]\n",
      "epoch:6 step:5957 [D loss: 0.623459, acc: 64.06%] [G loss: 2.520031]\n",
      "epoch:6 step:5958 [D loss: 0.675150, acc: 63.28%] [G loss: 2.540073]\n",
      "epoch:6 step:5959 [D loss: 0.567834, acc: 70.31%] [G loss: 2.383527]\n",
      "epoch:6 step:5960 [D loss: 0.573860, acc: 72.66%] [G loss: 2.387031]\n",
      "epoch:6 step:5961 [D loss: 0.666807, acc: 67.19%] [G loss: 2.316454]\n",
      "epoch:6 step:5962 [D loss: 0.672839, acc: 65.62%] [G loss: 2.355109]\n",
      "epoch:6 step:5963 [D loss: 0.648408, acc: 65.62%] [G loss: 2.332777]\n",
      "epoch:6 step:5964 [D loss: 0.683698, acc: 56.25%] [G loss: 2.100266]\n",
      "epoch:6 step:5965 [D loss: 0.551368, acc: 69.53%] [G loss: 2.452150]\n",
      "epoch:6 step:5966 [D loss: 0.555637, acc: 68.75%] [G loss: 2.672730]\n",
      "epoch:6 step:5967 [D loss: 0.664652, acc: 61.72%] [G loss: 2.723296]\n",
      "epoch:6 step:5968 [D loss: 0.579771, acc: 66.41%] [G loss: 2.780190]\n",
      "epoch:6 step:5969 [D loss: 0.501864, acc: 71.88%] [G loss: 2.744398]\n",
      "epoch:6 step:5970 [D loss: 0.724062, acc: 59.38%] [G loss: 2.148976]\n",
      "epoch:6 step:5971 [D loss: 0.666453, acc: 61.72%] [G loss: 2.081820]\n",
      "epoch:6 step:5972 [D loss: 0.590232, acc: 65.62%] [G loss: 2.497552]\n",
      "epoch:6 step:5973 [D loss: 0.603534, acc: 66.41%] [G loss: 2.287427]\n",
      "epoch:6 step:5974 [D loss: 0.620926, acc: 67.97%] [G loss: 2.543896]\n",
      "epoch:6 step:5975 [D loss: 0.560599, acc: 71.88%] [G loss: 2.632346]\n",
      "epoch:6 step:5976 [D loss: 0.575048, acc: 69.53%] [G loss: 2.766817]\n",
      "epoch:6 step:5977 [D loss: 0.628085, acc: 64.06%] [G loss: 2.538099]\n",
      "epoch:6 step:5978 [D loss: 0.664942, acc: 64.84%] [G loss: 2.301589]\n",
      "epoch:6 step:5979 [D loss: 0.590661, acc: 71.09%] [G loss: 2.593056]\n",
      "epoch:6 step:5980 [D loss: 0.609948, acc: 72.66%] [G loss: 2.405115]\n",
      "epoch:6 step:5981 [D loss: 0.563747, acc: 71.88%] [G loss: 2.698515]\n",
      "epoch:6 step:5982 [D loss: 0.563391, acc: 67.97%] [G loss: 2.492159]\n",
      "epoch:6 step:5983 [D loss: 0.599726, acc: 67.19%] [G loss: 2.098798]\n",
      "epoch:6 step:5984 [D loss: 0.644080, acc: 61.72%] [G loss: 2.516494]\n",
      "epoch:6 step:5985 [D loss: 0.637145, acc: 60.94%] [G loss: 2.203590]\n",
      "epoch:6 step:5986 [D loss: 0.537651, acc: 73.44%] [G loss: 2.528372]\n",
      "epoch:6 step:5987 [D loss: 0.574999, acc: 70.31%] [G loss: 2.647533]\n",
      "epoch:6 step:5988 [D loss: 0.558771, acc: 70.31%] [G loss: 2.644435]\n",
      "epoch:6 step:5989 [D loss: 0.561855, acc: 72.66%] [G loss: 2.376520]\n",
      "epoch:6 step:5990 [D loss: 0.587061, acc: 68.75%] [G loss: 2.567080]\n",
      "epoch:6 step:5991 [D loss: 0.587305, acc: 68.75%] [G loss: 2.524731]\n",
      "epoch:6 step:5992 [D loss: 0.578829, acc: 71.09%] [G loss: 2.436105]\n",
      "epoch:6 step:5993 [D loss: 0.578316, acc: 70.31%] [G loss: 2.986061]\n",
      "epoch:6 step:5994 [D loss: 0.559691, acc: 70.31%] [G loss: 2.717813]\n",
      "epoch:6 step:5995 [D loss: 0.664192, acc: 60.16%] [G loss: 2.309271]\n",
      "epoch:6 step:5996 [D loss: 0.526678, acc: 72.66%] [G loss: 2.897491]\n",
      "epoch:6 step:5997 [D loss: 0.758192, acc: 53.91%] [G loss: 2.358534]\n",
      "epoch:6 step:5998 [D loss: 0.670935, acc: 64.06%] [G loss: 2.351024]\n",
      "epoch:6 step:5999 [D loss: 0.695080, acc: 57.81%] [G loss: 2.177249]\n",
      "epoch:6 step:6000 [D loss: 0.589678, acc: 66.41%] [G loss: 2.550223]\n",
      "##############\n",
      "[2.85254695 1.60316604 6.7868118  5.13834304 4.26038569 5.82500425\n",
      " 5.04386057 5.04768031 5.27334827 3.77407567]\n",
      "##########\n",
      "epoch:6 step:6001 [D loss: 0.621998, acc: 67.97%] [G loss: 2.552284]\n",
      "epoch:6 step:6002 [D loss: 0.580465, acc: 68.75%] [G loss: 2.456029]\n",
      "epoch:6 step:6003 [D loss: 0.552547, acc: 77.34%] [G loss: 2.639430]\n",
      "epoch:6 step:6004 [D loss: 0.604616, acc: 66.41%] [G loss: 2.686410]\n",
      "epoch:6 step:6005 [D loss: 0.606006, acc: 72.66%] [G loss: 2.347875]\n",
      "epoch:6 step:6006 [D loss: 0.578988, acc: 71.88%] [G loss: 2.420221]\n",
      "epoch:6 step:6007 [D loss: 0.590409, acc: 68.75%] [G loss: 2.506810]\n",
      "epoch:6 step:6008 [D loss: 0.656442, acc: 61.72%] [G loss: 2.145426]\n",
      "epoch:6 step:6009 [D loss: 0.670768, acc: 62.50%] [G loss: 2.177236]\n",
      "epoch:6 step:6010 [D loss: 0.556930, acc: 67.97%] [G loss: 2.367120]\n",
      "epoch:6 step:6011 [D loss: 0.630237, acc: 68.75%] [G loss: 2.429600]\n",
      "epoch:6 step:6012 [D loss: 0.577793, acc: 68.75%] [G loss: 2.539405]\n",
      "epoch:6 step:6013 [D loss: 0.577896, acc: 71.88%] [G loss: 2.442435]\n",
      "epoch:6 step:6014 [D loss: 0.603373, acc: 71.09%] [G loss: 2.757162]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:6 step:6015 [D loss: 0.511884, acc: 73.44%] [G loss: 2.539963]\n",
      "epoch:6 step:6016 [D loss: 0.550613, acc: 75.00%] [G loss: 2.311987]\n",
      "epoch:6 step:6017 [D loss: 0.544047, acc: 71.88%] [G loss: 2.412668]\n",
      "epoch:6 step:6018 [D loss: 0.661178, acc: 64.84%] [G loss: 2.237807]\n",
      "epoch:6 step:6019 [D loss: 0.623589, acc: 64.84%] [G loss: 2.284875]\n",
      "epoch:6 step:6020 [D loss: 0.634145, acc: 62.50%] [G loss: 2.637979]\n",
      "epoch:6 step:6021 [D loss: 0.541187, acc: 74.22%] [G loss: 2.606896]\n",
      "epoch:6 step:6022 [D loss: 0.642053, acc: 60.16%] [G loss: 2.517174]\n",
      "epoch:6 step:6023 [D loss: 0.584175, acc: 71.09%] [G loss: 2.379073]\n",
      "epoch:6 step:6024 [D loss: 0.538552, acc: 69.53%] [G loss: 3.000142]\n",
      "epoch:6 step:6025 [D loss: 0.584634, acc: 66.41%] [G loss: 2.865813]\n",
      "epoch:6 step:6026 [D loss: 0.690156, acc: 62.50%] [G loss: 2.348145]\n",
      "epoch:6 step:6027 [D loss: 0.598094, acc: 67.19%] [G loss: 2.440815]\n",
      "epoch:6 step:6028 [D loss: 0.567457, acc: 67.97%] [G loss: 2.573196]\n",
      "epoch:6 step:6029 [D loss: 0.639097, acc: 64.84%] [G loss: 2.255828]\n",
      "epoch:6 step:6030 [D loss: 0.656982, acc: 69.53%] [G loss: 2.388094]\n",
      "epoch:6 step:6031 [D loss: 0.606613, acc: 65.62%] [G loss: 2.537024]\n",
      "epoch:6 step:6032 [D loss: 0.624337, acc: 64.84%] [G loss: 2.427332]\n",
      "epoch:6 step:6033 [D loss: 0.658287, acc: 63.28%] [G loss: 2.249197]\n",
      "epoch:6 step:6034 [D loss: 0.621753, acc: 64.06%] [G loss: 2.178648]\n",
      "epoch:6 step:6035 [D loss: 0.626075, acc: 61.72%] [G loss: 2.389711]\n",
      "epoch:6 step:6036 [D loss: 0.640246, acc: 64.06%] [G loss: 2.558357]\n",
      "epoch:6 step:6037 [D loss: 0.567146, acc: 68.75%] [G loss: 2.295298]\n",
      "epoch:6 step:6038 [D loss: 0.523263, acc: 76.56%] [G loss: 2.611597]\n",
      "epoch:6 step:6039 [D loss: 0.764378, acc: 59.38%] [G loss: 2.329424]\n",
      "epoch:6 step:6040 [D loss: 0.673572, acc: 66.41%] [G loss: 2.259762]\n",
      "epoch:6 step:6041 [D loss: 0.674476, acc: 63.28%] [G loss: 2.402764]\n",
      "epoch:6 step:6042 [D loss: 0.642368, acc: 67.19%] [G loss: 2.325364]\n",
      "epoch:6 step:6043 [D loss: 0.632665, acc: 67.97%] [G loss: 2.296957]\n",
      "epoch:6 step:6044 [D loss: 0.614337, acc: 67.97%] [G loss: 2.396928]\n",
      "epoch:6 step:6045 [D loss: 0.660761, acc: 64.84%] [G loss: 2.126610]\n",
      "epoch:6 step:6046 [D loss: 0.673899, acc: 62.50%] [G loss: 2.365044]\n",
      "epoch:6 step:6047 [D loss: 0.632797, acc: 69.53%] [G loss: 2.401438]\n",
      "epoch:6 step:6048 [D loss: 0.579812, acc: 70.31%] [G loss: 2.477956]\n",
      "epoch:6 step:6049 [D loss: 0.572261, acc: 73.44%] [G loss: 2.518387]\n",
      "epoch:6 step:6050 [D loss: 0.518156, acc: 71.88%] [G loss: 2.670362]\n",
      "epoch:6 step:6051 [D loss: 0.553535, acc: 71.09%] [G loss: 2.848687]\n",
      "epoch:6 step:6052 [D loss: 0.561180, acc: 71.09%] [G loss: 2.878159]\n",
      "epoch:6 step:6053 [D loss: 0.644793, acc: 66.41%] [G loss: 2.537774]\n",
      "epoch:6 step:6054 [D loss: 0.688587, acc: 64.06%] [G loss: 2.084085]\n",
      "epoch:6 step:6055 [D loss: 0.639293, acc: 67.19%] [G loss: 2.190055]\n",
      "epoch:6 step:6056 [D loss: 0.634540, acc: 64.06%] [G loss: 2.304495]\n",
      "epoch:6 step:6057 [D loss: 0.573480, acc: 72.66%] [G loss: 2.591281]\n",
      "epoch:6 step:6058 [D loss: 0.630579, acc: 68.75%] [G loss: 2.287287]\n",
      "epoch:6 step:6059 [D loss: 0.684539, acc: 57.03%] [G loss: 2.165511]\n",
      "epoch:6 step:6060 [D loss: 0.657565, acc: 62.50%] [G loss: 2.218858]\n",
      "epoch:6 step:6061 [D loss: 0.602842, acc: 69.53%] [G loss: 2.066982]\n",
      "epoch:6 step:6062 [D loss: 0.633335, acc: 67.19%] [G loss: 2.254569]\n",
      "epoch:6 step:6063 [D loss: 0.666129, acc: 63.28%] [G loss: 2.037193]\n",
      "epoch:6 step:6064 [D loss: 0.609111, acc: 66.41%] [G loss: 2.375952]\n",
      "epoch:6 step:6065 [D loss: 0.535789, acc: 78.91%] [G loss: 2.415064]\n",
      "epoch:6 step:6066 [D loss: 0.657772, acc: 64.06%] [G loss: 2.238080]\n",
      "epoch:6 step:6067 [D loss: 0.641152, acc: 61.72%] [G loss: 2.169183]\n",
      "epoch:6 step:6068 [D loss: 0.605022, acc: 63.28%] [G loss: 2.169620]\n",
      "epoch:6 step:6069 [D loss: 0.553641, acc: 74.22%] [G loss: 2.360661]\n",
      "epoch:6 step:6070 [D loss: 0.656573, acc: 64.06%] [G loss: 2.299755]\n",
      "epoch:6 step:6071 [D loss: 0.593090, acc: 68.75%] [G loss: 2.815995]\n",
      "epoch:6 step:6072 [D loss: 0.574376, acc: 67.19%] [G loss: 2.569647]\n",
      "epoch:6 step:6073 [D loss: 0.592054, acc: 71.09%] [G loss: 2.494591]\n",
      "epoch:6 step:6074 [D loss: 0.574699, acc: 71.09%] [G loss: 2.579989]\n",
      "epoch:6 step:6075 [D loss: 0.593408, acc: 68.75%] [G loss: 2.624043]\n",
      "epoch:6 step:6076 [D loss: 0.572195, acc: 67.97%] [G loss: 2.538267]\n",
      "epoch:6 step:6077 [D loss: 0.656960, acc: 58.59%] [G loss: 2.570580]\n",
      "epoch:6 step:6078 [D loss: 0.609048, acc: 62.50%] [G loss: 2.466807]\n",
      "epoch:6 step:6079 [D loss: 0.550781, acc: 70.31%] [G loss: 2.323485]\n",
      "epoch:6 step:6080 [D loss: 0.587990, acc: 68.75%] [G loss: 2.227931]\n",
      "epoch:6 step:6081 [D loss: 0.716523, acc: 57.03%] [G loss: 2.306226]\n",
      "epoch:6 step:6082 [D loss: 0.677039, acc: 60.94%] [G loss: 2.010722]\n",
      "epoch:6 step:6083 [D loss: 0.618366, acc: 65.62%] [G loss: 2.034621]\n",
      "epoch:6 step:6084 [D loss: 0.656564, acc: 59.38%] [G loss: 2.161711]\n",
      "epoch:6 step:6085 [D loss: 0.600422, acc: 66.41%] [G loss: 2.290397]\n",
      "epoch:6 step:6086 [D loss: 0.546655, acc: 75.00%] [G loss: 2.508836]\n",
      "epoch:6 step:6087 [D loss: 0.593337, acc: 69.53%] [G loss: 2.265152]\n",
      "epoch:6 step:6088 [D loss: 0.569286, acc: 78.91%] [G loss: 2.569838]\n",
      "epoch:6 step:6089 [D loss: 0.638859, acc: 64.84%] [G loss: 2.433541]\n",
      "epoch:6 step:6090 [D loss: 0.582752, acc: 71.09%] [G loss: 2.605024]\n",
      "epoch:6 step:6091 [D loss: 0.575129, acc: 68.75%] [G loss: 2.547805]\n",
      "epoch:6 step:6092 [D loss: 0.645607, acc: 64.84%] [G loss: 2.825449]\n",
      "epoch:6 step:6093 [D loss: 0.557547, acc: 73.44%] [G loss: 2.958751]\n",
      "epoch:6 step:6094 [D loss: 0.607223, acc: 70.31%] [G loss: 2.790925]\n",
      "epoch:6 step:6095 [D loss: 0.635063, acc: 66.41%] [G loss: 2.421329]\n",
      "epoch:6 step:6096 [D loss: 0.625373, acc: 63.28%] [G loss: 2.431781]\n",
      "epoch:6 step:6097 [D loss: 0.653653, acc: 67.97%] [G loss: 2.346273]\n",
      "epoch:6 step:6098 [D loss: 0.574853, acc: 70.31%] [G loss: 2.289637]\n",
      "epoch:6 step:6099 [D loss: 0.723625, acc: 57.81%] [G loss: 1.995056]\n",
      "epoch:6 step:6100 [D loss: 0.635655, acc: 64.06%] [G loss: 2.226424]\n",
      "epoch:6 step:6101 [D loss: 0.533143, acc: 72.66%] [G loss: 2.697957]\n",
      "epoch:6 step:6102 [D loss: 0.575801, acc: 69.53%] [G loss: 2.763804]\n",
      "epoch:6 step:6103 [D loss: 0.535941, acc: 68.75%] [G loss: 2.457099]\n",
      "epoch:6 step:6104 [D loss: 0.664014, acc: 65.62%] [G loss: 2.259830]\n",
      "epoch:6 step:6105 [D loss: 0.709275, acc: 57.81%] [G loss: 2.099553]\n",
      "epoch:6 step:6106 [D loss: 0.572835, acc: 72.66%] [G loss: 2.518214]\n",
      "epoch:6 step:6107 [D loss: 0.574310, acc: 67.19%] [G loss: 2.195780]\n",
      "epoch:6 step:6108 [D loss: 0.607618, acc: 63.28%] [G loss: 2.208433]\n",
      "epoch:6 step:6109 [D loss: 0.582283, acc: 66.41%] [G loss: 2.406533]\n",
      "epoch:6 step:6110 [D loss: 0.678858, acc: 62.50%] [G loss: 2.321648]\n",
      "epoch:6 step:6111 [D loss: 0.639506, acc: 60.94%] [G loss: 2.403732]\n",
      "epoch:6 step:6112 [D loss: 0.652832, acc: 66.41%] [G loss: 2.364305]\n",
      "epoch:6 step:6113 [D loss: 0.618907, acc: 63.28%] [G loss: 2.546460]\n",
      "epoch:6 step:6114 [D loss: 0.590485, acc: 67.19%] [G loss: 2.327744]\n",
      "epoch:6 step:6115 [D loss: 0.716700, acc: 57.03%] [G loss: 2.058580]\n",
      "epoch:6 step:6116 [D loss: 0.512978, acc: 78.91%] [G loss: 2.542428]\n",
      "epoch:6 step:6117 [D loss: 0.589194, acc: 68.75%] [G loss: 2.372347]\n",
      "epoch:6 step:6118 [D loss: 0.670559, acc: 68.75%] [G loss: 2.173709]\n",
      "epoch:6 step:6119 [D loss: 0.602942, acc: 67.97%] [G loss: 2.610340]\n",
      "epoch:6 step:6120 [D loss: 0.525029, acc: 71.09%] [G loss: 2.743083]\n",
      "epoch:6 step:6121 [D loss: 0.570128, acc: 75.78%] [G loss: 2.880406]\n",
      "epoch:6 step:6122 [D loss: 0.738151, acc: 60.16%] [G loss: 2.047759]\n",
      "epoch:6 step:6123 [D loss: 0.653095, acc: 61.72%] [G loss: 2.212565]\n",
      "epoch:6 step:6124 [D loss: 0.630173, acc: 64.84%] [G loss: 2.137566]\n",
      "epoch:6 step:6125 [D loss: 0.531934, acc: 75.78%] [G loss: 2.535768]\n",
      "epoch:6 step:6126 [D loss: 0.589278, acc: 66.41%] [G loss: 2.762992]\n",
      "epoch:6 step:6127 [D loss: 0.578405, acc: 70.31%] [G loss: 2.093406]\n",
      "epoch:6 step:6128 [D loss: 0.574922, acc: 69.53%] [G loss: 2.166754]\n",
      "epoch:6 step:6129 [D loss: 0.632540, acc: 65.62%] [G loss: 2.282508]\n",
      "epoch:6 step:6130 [D loss: 0.581862, acc: 75.00%] [G loss: 2.543669]\n",
      "epoch:6 step:6131 [D loss: 0.616437, acc: 64.84%] [G loss: 2.283229]\n",
      "epoch:6 step:6132 [D loss: 0.633324, acc: 63.28%] [G loss: 2.189632]\n",
      "epoch:6 step:6133 [D loss: 0.571191, acc: 71.88%] [G loss: 2.012818]\n",
      "epoch:6 step:6134 [D loss: 0.692086, acc: 65.62%] [G loss: 2.186187]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:6 step:6135 [D loss: 0.623246, acc: 67.97%] [G loss: 2.326291]\n",
      "epoch:6 step:6136 [D loss: 0.632354, acc: 70.31%] [G loss: 2.248099]\n",
      "epoch:6 step:6137 [D loss: 0.607102, acc: 71.09%] [G loss: 2.163338]\n",
      "epoch:6 step:6138 [D loss: 0.601417, acc: 67.97%] [G loss: 2.322255]\n",
      "epoch:6 step:6139 [D loss: 0.597196, acc: 68.75%] [G loss: 2.230410]\n",
      "epoch:6 step:6140 [D loss: 0.609328, acc: 71.09%] [G loss: 2.515351]\n",
      "epoch:6 step:6141 [D loss: 0.590686, acc: 67.97%] [G loss: 2.470773]\n",
      "epoch:6 step:6142 [D loss: 0.524235, acc: 75.78%] [G loss: 2.802876]\n",
      "epoch:6 step:6143 [D loss: 0.606605, acc: 67.97%] [G loss: 2.451219]\n",
      "epoch:6 step:6144 [D loss: 0.626395, acc: 68.75%] [G loss: 2.767538]\n",
      "epoch:6 step:6145 [D loss: 0.579148, acc: 73.44%] [G loss: 2.647035]\n",
      "epoch:6 step:6146 [D loss: 0.593139, acc: 66.41%] [G loss: 2.426837]\n",
      "epoch:6 step:6147 [D loss: 0.655991, acc: 60.94%] [G loss: 2.263230]\n",
      "epoch:6 step:6148 [D loss: 0.546028, acc: 71.09%] [G loss: 2.449700]\n",
      "epoch:6 step:6149 [D loss: 0.625078, acc: 64.06%] [G loss: 2.210325]\n",
      "epoch:6 step:6150 [D loss: 0.629332, acc: 70.31%] [G loss: 2.134183]\n",
      "epoch:6 step:6151 [D loss: 0.731473, acc: 55.47%] [G loss: 2.020883]\n",
      "epoch:6 step:6152 [D loss: 0.634098, acc: 63.28%] [G loss: 2.381344]\n",
      "epoch:6 step:6153 [D loss: 0.713940, acc: 59.38%] [G loss: 2.154806]\n",
      "epoch:6 step:6154 [D loss: 0.570209, acc: 70.31%] [G loss: 2.277686]\n",
      "epoch:6 step:6155 [D loss: 0.614170, acc: 67.19%] [G loss: 2.335841]\n",
      "epoch:6 step:6156 [D loss: 0.572282, acc: 67.19%] [G loss: 2.566306]\n",
      "epoch:6 step:6157 [D loss: 0.670924, acc: 60.94%] [G loss: 2.398234]\n",
      "epoch:6 step:6158 [D loss: 0.538789, acc: 77.34%] [G loss: 2.683315]\n",
      "epoch:6 step:6159 [D loss: 0.548386, acc: 75.00%] [G loss: 2.299924]\n",
      "epoch:6 step:6160 [D loss: 0.682937, acc: 60.16%] [G loss: 2.150403]\n",
      "epoch:6 step:6161 [D loss: 0.615005, acc: 65.62%] [G loss: 2.301629]\n",
      "epoch:6 step:6162 [D loss: 0.630331, acc: 64.06%] [G loss: 2.180903]\n",
      "epoch:6 step:6163 [D loss: 0.597560, acc: 68.75%] [G loss: 2.328477]\n",
      "epoch:6 step:6164 [D loss: 0.586373, acc: 68.75%] [G loss: 2.238631]\n",
      "epoch:6 step:6165 [D loss: 0.696427, acc: 55.47%] [G loss: 2.288926]\n",
      "epoch:6 step:6166 [D loss: 0.595476, acc: 72.66%] [G loss: 2.329684]\n",
      "epoch:6 step:6167 [D loss: 0.604363, acc: 66.41%] [G loss: 2.236639]\n",
      "epoch:6 step:6168 [D loss: 0.557138, acc: 71.88%] [G loss: 2.569154]\n",
      "epoch:6 step:6169 [D loss: 0.568212, acc: 68.75%] [G loss: 2.552289]\n",
      "epoch:6 step:6170 [D loss: 0.632385, acc: 65.62%] [G loss: 2.469628]\n",
      "epoch:6 step:6171 [D loss: 0.579143, acc: 71.09%] [G loss: 2.626302]\n",
      "epoch:6 step:6172 [D loss: 0.570828, acc: 74.22%] [G loss: 2.403670]\n",
      "epoch:6 step:6173 [D loss: 0.581596, acc: 65.62%] [G loss: 2.859243]\n",
      "epoch:6 step:6174 [D loss: 0.547589, acc: 72.66%] [G loss: 2.741549]\n",
      "epoch:6 step:6175 [D loss: 0.548493, acc: 73.44%] [G loss: 2.213532]\n",
      "epoch:6 step:6176 [D loss: 0.521444, acc: 73.44%] [G loss: 2.542318]\n",
      "epoch:6 step:6177 [D loss: 0.609695, acc: 66.41%] [G loss: 2.432269]\n",
      "epoch:6 step:6178 [D loss: 0.554908, acc: 71.09%] [G loss: 2.654619]\n",
      "epoch:6 step:6179 [D loss: 0.557732, acc: 69.53%] [G loss: 2.660943]\n",
      "epoch:6 step:6180 [D loss: 0.612566, acc: 68.75%] [G loss: 2.747871]\n",
      "epoch:6 step:6181 [D loss: 0.596300, acc: 69.53%] [G loss: 2.311761]\n",
      "epoch:6 step:6182 [D loss: 0.700400, acc: 59.38%] [G loss: 2.247011]\n",
      "epoch:6 step:6183 [D loss: 0.559564, acc: 71.88%] [G loss: 2.498757]\n",
      "epoch:6 step:6184 [D loss: 0.624750, acc: 64.84%] [G loss: 2.262682]\n",
      "epoch:6 step:6185 [D loss: 0.626148, acc: 64.06%] [G loss: 2.347338]\n",
      "epoch:6 step:6186 [D loss: 0.545664, acc: 71.88%] [G loss: 2.468653]\n",
      "epoch:6 step:6187 [D loss: 0.612315, acc: 69.53%] [G loss: 2.586345]\n",
      "epoch:6 step:6188 [D loss: 0.643389, acc: 62.50%] [G loss: 2.204084]\n",
      "epoch:6 step:6189 [D loss: 0.621270, acc: 68.75%] [G loss: 2.437126]\n",
      "epoch:6 step:6190 [D loss: 0.575049, acc: 73.44%] [G loss: 2.376712]\n",
      "epoch:6 step:6191 [D loss: 0.614899, acc: 65.62%] [G loss: 2.374681]\n",
      "epoch:6 step:6192 [D loss: 0.541651, acc: 72.66%] [G loss: 2.473001]\n",
      "epoch:6 step:6193 [D loss: 0.604793, acc: 67.19%] [G loss: 2.429955]\n",
      "epoch:6 step:6194 [D loss: 0.610948, acc: 64.84%] [G loss: 2.177363]\n",
      "epoch:6 step:6195 [D loss: 0.653319, acc: 64.06%] [G loss: 2.285698]\n",
      "epoch:6 step:6196 [D loss: 0.523662, acc: 71.88%] [G loss: 2.556258]\n",
      "epoch:6 step:6197 [D loss: 0.618735, acc: 64.06%] [G loss: 2.332040]\n",
      "epoch:6 step:6198 [D loss: 0.668954, acc: 60.16%] [G loss: 2.126918]\n",
      "epoch:6 step:6199 [D loss: 0.644110, acc: 64.06%] [G loss: 2.535700]\n",
      "epoch:6 step:6200 [D loss: 0.623250, acc: 60.94%] [G loss: 2.314403]\n",
      "##############\n",
      "[2.59639811 1.64984799 6.57319322 5.03544311 4.07014889 5.95706774\n",
      " 4.83446632 4.83069117 5.17491785 3.43219104]\n",
      "##########\n",
      "epoch:6 step:6201 [D loss: 0.589892, acc: 69.53%] [G loss: 2.117388]\n",
      "epoch:6 step:6202 [D loss: 0.681090, acc: 62.50%] [G loss: 2.010541]\n",
      "epoch:6 step:6203 [D loss: 0.574040, acc: 71.09%] [G loss: 2.446622]\n",
      "epoch:6 step:6204 [D loss: 0.575464, acc: 75.00%] [G loss: 2.396910]\n",
      "epoch:6 step:6205 [D loss: 0.684319, acc: 59.38%] [G loss: 2.398044]\n",
      "epoch:6 step:6206 [D loss: 0.622143, acc: 64.84%] [G loss: 2.095745]\n",
      "epoch:6 step:6207 [D loss: 0.520315, acc: 76.56%] [G loss: 2.504820]\n",
      "epoch:6 step:6208 [D loss: 0.673799, acc: 59.38%] [G loss: 2.302664]\n",
      "epoch:6 step:6209 [D loss: 0.647122, acc: 64.06%] [G loss: 2.392336]\n",
      "epoch:6 step:6210 [D loss: 0.657368, acc: 57.81%] [G loss: 2.317640]\n",
      "epoch:6 step:6211 [D loss: 0.610310, acc: 70.31%] [G loss: 2.233579]\n",
      "epoch:6 step:6212 [D loss: 0.625079, acc: 64.84%] [G loss: 2.214727]\n",
      "epoch:6 step:6213 [D loss: 0.623068, acc: 69.53%] [G loss: 2.360223]\n",
      "epoch:6 step:6214 [D loss: 0.529606, acc: 72.66%] [G loss: 2.559216]\n",
      "epoch:6 step:6215 [D loss: 0.660404, acc: 58.59%] [G loss: 2.140547]\n",
      "epoch:6 step:6216 [D loss: 0.601365, acc: 71.88%] [G loss: 2.103259]\n",
      "epoch:6 step:6217 [D loss: 0.521022, acc: 72.66%] [G loss: 2.261466]\n",
      "epoch:6 step:6218 [D loss: 0.680736, acc: 57.81%] [G loss: 2.160716]\n",
      "epoch:6 step:6219 [D loss: 0.613182, acc: 70.31%] [G loss: 2.327754]\n",
      "epoch:6 step:6220 [D loss: 0.669399, acc: 66.41%] [G loss: 2.416291]\n",
      "epoch:6 step:6221 [D loss: 0.598345, acc: 67.19%] [G loss: 2.392628]\n",
      "epoch:6 step:6222 [D loss: 0.674109, acc: 57.03%] [G loss: 2.262163]\n",
      "epoch:6 step:6223 [D loss: 0.617167, acc: 72.66%] [G loss: 2.468691]\n",
      "epoch:6 step:6224 [D loss: 0.564040, acc: 67.97%] [G loss: 2.172745]\n",
      "epoch:6 step:6225 [D loss: 0.515866, acc: 76.56%] [G loss: 2.593680]\n",
      "epoch:6 step:6226 [D loss: 0.611297, acc: 65.62%] [G loss: 2.304842]\n",
      "epoch:6 step:6227 [D loss: 0.543972, acc: 68.75%] [G loss: 2.573926]\n",
      "epoch:6 step:6228 [D loss: 0.701179, acc: 64.84%] [G loss: 2.215009]\n",
      "epoch:6 step:6229 [D loss: 0.645805, acc: 64.84%] [G loss: 2.347758]\n",
      "epoch:6 step:6230 [D loss: 0.624120, acc: 63.28%] [G loss: 2.225913]\n",
      "epoch:6 step:6231 [D loss: 0.584205, acc: 69.53%] [G loss: 2.162196]\n",
      "epoch:6 step:6232 [D loss: 0.538026, acc: 73.44%] [G loss: 2.481148]\n",
      "epoch:6 step:6233 [D loss: 0.620864, acc: 64.84%] [G loss: 2.428380]\n",
      "epoch:6 step:6234 [D loss: 0.556314, acc: 74.22%] [G loss: 2.405892]\n",
      "epoch:6 step:6235 [D loss: 0.591612, acc: 66.41%] [G loss: 2.763049]\n",
      "epoch:6 step:6236 [D loss: 0.611961, acc: 64.06%] [G loss: 2.457908]\n",
      "epoch:6 step:6237 [D loss: 0.687000, acc: 57.81%] [G loss: 2.219428]\n",
      "epoch:6 step:6238 [D loss: 0.618546, acc: 70.31%] [G loss: 2.333730]\n",
      "epoch:6 step:6239 [D loss: 0.670483, acc: 57.81%] [G loss: 2.333411]\n",
      "epoch:6 step:6240 [D loss: 0.583507, acc: 73.44%] [G loss: 2.245808]\n",
      "epoch:6 step:6241 [D loss: 0.606383, acc: 66.41%] [G loss: 2.417480]\n",
      "epoch:6 step:6242 [D loss: 0.619985, acc: 66.41%] [G loss: 2.265990]\n",
      "epoch:6 step:6243 [D loss: 0.615451, acc: 66.41%] [G loss: 2.206450]\n",
      "epoch:6 step:6244 [D loss: 0.681994, acc: 61.72%] [G loss: 2.288851]\n",
      "epoch:6 step:6245 [D loss: 0.606438, acc: 74.22%] [G loss: 2.339637]\n",
      "epoch:6 step:6246 [D loss: 0.561811, acc: 71.88%] [G loss: 2.436992]\n",
      "epoch:6 step:6247 [D loss: 0.686366, acc: 62.50%] [G loss: 2.280121]\n",
      "epoch:6 step:6248 [D loss: 0.614864, acc: 68.75%] [G loss: 2.322306]\n",
      "epoch:6 step:6249 [D loss: 0.645043, acc: 65.62%] [G loss: 2.224626]\n",
      "epoch:6 step:6250 [D loss: 0.668303, acc: 62.50%] [G loss: 2.361972]\n",
      "epoch:6 step:6251 [D loss: 0.572071, acc: 69.53%] [G loss: 2.352334]\n",
      "epoch:6 step:6252 [D loss: 0.573311, acc: 72.66%] [G loss: 2.545237]\n",
      "epoch:6 step:6253 [D loss: 0.649547, acc: 60.94%] [G loss: 2.331842]\n",
      "epoch:6 step:6254 [D loss: 0.539362, acc: 71.88%] [G loss: 2.451777]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:6 step:6255 [D loss: 0.551876, acc: 71.88%] [G loss: 2.686141]\n",
      "epoch:6 step:6256 [D loss: 0.585244, acc: 69.53%] [G loss: 2.580743]\n",
      "epoch:6 step:6257 [D loss: 0.614769, acc: 69.53%] [G loss: 2.585957]\n",
      "epoch:6 step:6258 [D loss: 0.607941, acc: 65.62%] [G loss: 2.405990]\n",
      "epoch:6 step:6259 [D loss: 0.569809, acc: 71.09%] [G loss: 2.462708]\n",
      "epoch:6 step:6260 [D loss: 0.575463, acc: 66.41%] [G loss: 2.252011]\n",
      "epoch:6 step:6261 [D loss: 0.594499, acc: 65.62%] [G loss: 2.621083]\n",
      "epoch:6 step:6262 [D loss: 0.640479, acc: 68.75%] [G loss: 2.415074]\n",
      "epoch:6 step:6263 [D loss: 0.498842, acc: 76.56%] [G loss: 2.767053]\n",
      "epoch:6 step:6264 [D loss: 0.540474, acc: 71.88%] [G loss: 2.976977]\n",
      "epoch:6 step:6265 [D loss: 0.594579, acc: 65.62%] [G loss: 2.301308]\n",
      "epoch:6 step:6266 [D loss: 0.587804, acc: 64.84%] [G loss: 2.540257]\n",
      "epoch:6 step:6267 [D loss: 0.573502, acc: 74.22%] [G loss: 2.397338]\n",
      "epoch:6 step:6268 [D loss: 0.651930, acc: 63.28%] [G loss: 2.694821]\n",
      "epoch:6 step:6269 [D loss: 0.539411, acc: 75.78%] [G loss: 2.970416]\n",
      "epoch:6 step:6270 [D loss: 0.505933, acc: 75.00%] [G loss: 3.094914]\n",
      "epoch:6 step:6271 [D loss: 0.533273, acc: 71.09%] [G loss: 2.791037]\n",
      "epoch:6 step:6272 [D loss: 0.607575, acc: 68.75%] [G loss: 2.530942]\n",
      "epoch:6 step:6273 [D loss: 0.564816, acc: 70.31%] [G loss: 2.490337]\n",
      "epoch:6 step:6274 [D loss: 0.540640, acc: 74.22%] [G loss: 2.408670]\n",
      "epoch:6 step:6275 [D loss: 0.598047, acc: 65.62%] [G loss: 2.403235]\n",
      "epoch:6 step:6276 [D loss: 0.517554, acc: 78.12%] [G loss: 2.773313]\n",
      "epoch:6 step:6277 [D loss: 0.650628, acc: 62.50%] [G loss: 2.418165]\n",
      "epoch:6 step:6278 [D loss: 0.660138, acc: 71.09%] [G loss: 2.398761]\n",
      "epoch:6 step:6279 [D loss: 0.568162, acc: 67.19%] [G loss: 2.417950]\n",
      "epoch:6 step:6280 [D loss: 0.657132, acc: 60.94%] [G loss: 2.120491]\n",
      "epoch:6 step:6281 [D loss: 0.593494, acc: 71.09%] [G loss: 2.261640]\n",
      "epoch:6 step:6282 [D loss: 0.592743, acc: 69.53%] [G loss: 2.499929]\n",
      "epoch:6 step:6283 [D loss: 0.617097, acc: 68.75%] [G loss: 2.499572]\n",
      "epoch:6 step:6284 [D loss: 0.565614, acc: 67.97%] [G loss: 2.622305]\n",
      "epoch:6 step:6285 [D loss: 0.582202, acc: 71.88%] [G loss: 2.690873]\n",
      "epoch:6 step:6286 [D loss: 0.683354, acc: 67.19%] [G loss: 2.429776]\n",
      "epoch:6 step:6287 [D loss: 0.622298, acc: 66.41%] [G loss: 2.468013]\n",
      "epoch:6 step:6288 [D loss: 0.613514, acc: 63.28%] [G loss: 2.373117]\n",
      "epoch:6 step:6289 [D loss: 0.589798, acc: 68.75%] [G loss: 2.241850]\n",
      "epoch:6 step:6290 [D loss: 0.566122, acc: 71.88%] [G loss: 2.536238]\n",
      "epoch:6 step:6291 [D loss: 0.637904, acc: 63.28%] [G loss: 2.095495]\n",
      "epoch:6 step:6292 [D loss: 0.599564, acc: 69.53%] [G loss: 2.339106]\n",
      "epoch:6 step:6293 [D loss: 0.572173, acc: 70.31%] [G loss: 2.526511]\n",
      "epoch:6 step:6294 [D loss: 0.631790, acc: 65.62%] [G loss: 2.472939]\n",
      "epoch:6 step:6295 [D loss: 0.538078, acc: 79.69%] [G loss: 2.478469]\n",
      "epoch:6 step:6296 [D loss: 0.624335, acc: 68.75%] [G loss: 2.343798]\n",
      "epoch:6 step:6297 [D loss: 0.624894, acc: 64.84%] [G loss: 2.345433]\n",
      "epoch:6 step:6298 [D loss: 0.585706, acc: 71.09%] [G loss: 2.505006]\n",
      "epoch:6 step:6299 [D loss: 0.517030, acc: 75.00%] [G loss: 2.415543]\n",
      "epoch:6 step:6300 [D loss: 0.589735, acc: 67.19%] [G loss: 2.498107]\n",
      "epoch:6 step:6301 [D loss: 0.479234, acc: 76.56%] [G loss: 2.601794]\n",
      "epoch:6 step:6302 [D loss: 0.644794, acc: 62.50%] [G loss: 2.461224]\n",
      "epoch:6 step:6303 [D loss: 0.610021, acc: 66.41%] [G loss: 2.696926]\n",
      "epoch:6 step:6304 [D loss: 0.676524, acc: 58.59%] [G loss: 2.382855]\n",
      "epoch:6 step:6305 [D loss: 0.564370, acc: 72.66%] [G loss: 2.464366]\n",
      "epoch:6 step:6306 [D loss: 0.598209, acc: 64.84%] [G loss: 2.345908]\n",
      "epoch:6 step:6307 [D loss: 0.622529, acc: 67.19%] [G loss: 2.309618]\n",
      "epoch:6 step:6308 [D loss: 0.554070, acc: 75.00%] [G loss: 2.395481]\n",
      "epoch:6 step:6309 [D loss: 0.586512, acc: 66.41%] [G loss: 2.369278]\n",
      "epoch:6 step:6310 [D loss: 0.624995, acc: 71.09%] [G loss: 2.477190]\n",
      "epoch:6 step:6311 [D loss: 0.565079, acc: 71.88%] [G loss: 2.443236]\n",
      "epoch:6 step:6312 [D loss: 0.535717, acc: 76.56%] [G loss: 2.857576]\n",
      "epoch:6 step:6313 [D loss: 0.554615, acc: 70.31%] [G loss: 2.690341]\n",
      "epoch:6 step:6314 [D loss: 0.538385, acc: 72.66%] [G loss: 2.817194]\n",
      "epoch:6 step:6315 [D loss: 0.517761, acc: 78.91%] [G loss: 2.843101]\n",
      "epoch:6 step:6316 [D loss: 0.561446, acc: 71.09%] [G loss: 3.034518]\n",
      "epoch:6 step:6317 [D loss: 0.570095, acc: 70.31%] [G loss: 2.522352]\n",
      "epoch:6 step:6318 [D loss: 0.577446, acc: 71.09%] [G loss: 2.252723]\n",
      "epoch:6 step:6319 [D loss: 0.596143, acc: 64.06%] [G loss: 2.585298]\n",
      "epoch:6 step:6320 [D loss: 0.632369, acc: 60.94%] [G loss: 2.259789]\n",
      "epoch:6 step:6321 [D loss: 0.591452, acc: 74.22%] [G loss: 2.553523]\n",
      "epoch:6 step:6322 [D loss: 0.634351, acc: 65.62%] [G loss: 2.496910]\n",
      "epoch:6 step:6323 [D loss: 0.614249, acc: 64.06%] [G loss: 2.234567]\n",
      "epoch:6 step:6324 [D loss: 0.602358, acc: 66.41%] [G loss: 2.267847]\n",
      "epoch:6 step:6325 [D loss: 0.635578, acc: 64.84%] [G loss: 2.014309]\n",
      "epoch:6 step:6326 [D loss: 0.698304, acc: 58.59%] [G loss: 2.209815]\n",
      "epoch:6 step:6327 [D loss: 0.647504, acc: 63.28%] [G loss: 2.281113]\n",
      "epoch:6 step:6328 [D loss: 0.611449, acc: 64.06%] [G loss: 2.452498]\n",
      "epoch:6 step:6329 [D loss: 0.517339, acc: 74.22%] [G loss: 2.599817]\n",
      "epoch:6 step:6330 [D loss: 0.569156, acc: 71.09%] [G loss: 2.757853]\n",
      "epoch:6 step:6331 [D loss: 0.601391, acc: 70.31%] [G loss: 2.504523]\n",
      "epoch:6 step:6332 [D loss: 0.672367, acc: 66.41%] [G loss: 2.103907]\n",
      "epoch:6 step:6333 [D loss: 0.626042, acc: 67.19%] [G loss: 2.580325]\n",
      "epoch:6 step:6334 [D loss: 0.628339, acc: 65.62%] [G loss: 2.393518]\n",
      "epoch:6 step:6335 [D loss: 0.662486, acc: 60.94%] [G loss: 2.082953]\n",
      "epoch:6 step:6336 [D loss: 0.684845, acc: 61.72%] [G loss: 2.300207]\n",
      "epoch:6 step:6337 [D loss: 0.664185, acc: 60.16%] [G loss: 2.141118]\n",
      "epoch:6 step:6338 [D loss: 0.679217, acc: 60.16%] [G loss: 2.133443]\n",
      "epoch:6 step:6339 [D loss: 0.600394, acc: 70.31%] [G loss: 2.193973]\n",
      "epoch:6 step:6340 [D loss: 0.666230, acc: 62.50%] [G loss: 2.199774]\n",
      "epoch:6 step:6341 [D loss: 0.529312, acc: 78.91%] [G loss: 2.444122]\n",
      "epoch:6 step:6342 [D loss: 0.548386, acc: 75.78%] [G loss: 2.240267]\n",
      "epoch:6 step:6343 [D loss: 0.584587, acc: 67.19%] [G loss: 2.493159]\n",
      "epoch:6 step:6344 [D loss: 0.584328, acc: 67.97%] [G loss: 2.432607]\n",
      "epoch:6 step:6345 [D loss: 0.638760, acc: 61.72%] [G loss: 2.242197]\n",
      "epoch:6 step:6346 [D loss: 0.562421, acc: 73.44%] [G loss: 2.676431]\n",
      "epoch:6 step:6347 [D loss: 0.564501, acc: 71.88%] [G loss: 2.404463]\n",
      "epoch:6 step:6348 [D loss: 0.653431, acc: 66.41%] [G loss: 2.418194]\n",
      "epoch:6 step:6349 [D loss: 0.605191, acc: 69.53%] [G loss: 2.389293]\n",
      "epoch:6 step:6350 [D loss: 0.571563, acc: 71.09%] [G loss: 2.409693]\n",
      "epoch:6 step:6351 [D loss: 0.613453, acc: 69.53%] [G loss: 2.418542]\n",
      "epoch:6 step:6352 [D loss: 0.685145, acc: 57.81%] [G loss: 2.103879]\n",
      "epoch:6 step:6353 [D loss: 0.579754, acc: 71.88%] [G loss: 2.250347]\n",
      "epoch:6 step:6354 [D loss: 0.569223, acc: 70.31%] [G loss: 2.411474]\n",
      "epoch:6 step:6355 [D loss: 0.642008, acc: 63.28%] [G loss: 2.291203]\n",
      "epoch:6 step:6356 [D loss: 0.690612, acc: 57.81%] [G loss: 2.152268]\n",
      "epoch:6 step:6357 [D loss: 0.595234, acc: 67.97%] [G loss: 2.193056]\n",
      "epoch:6 step:6358 [D loss: 0.627181, acc: 67.19%] [G loss: 2.319782]\n",
      "epoch:6 step:6359 [D loss: 0.664937, acc: 60.16%] [G loss: 2.086934]\n",
      "epoch:6 step:6360 [D loss: 0.574068, acc: 67.97%] [G loss: 2.149436]\n",
      "epoch:6 step:6361 [D loss: 0.625143, acc: 64.84%] [G loss: 2.200793]\n",
      "epoch:6 step:6362 [D loss: 0.574794, acc: 69.53%] [G loss: 2.382416]\n",
      "epoch:6 step:6363 [D loss: 0.662930, acc: 65.62%] [G loss: 2.267866]\n",
      "epoch:6 step:6364 [D loss: 0.657242, acc: 60.16%] [G loss: 2.312865]\n",
      "epoch:6 step:6365 [D loss: 0.648103, acc: 69.53%] [G loss: 2.249977]\n",
      "epoch:6 step:6366 [D loss: 0.632852, acc: 66.41%] [G loss: 2.322798]\n",
      "epoch:6 step:6367 [D loss: 0.577231, acc: 65.62%] [G loss: 2.118459]\n",
      "epoch:6 step:6368 [D loss: 0.661619, acc: 64.06%] [G loss: 2.285033]\n",
      "epoch:6 step:6369 [D loss: 0.587113, acc: 67.97%] [G loss: 2.370585]\n",
      "epoch:6 step:6370 [D loss: 0.523942, acc: 75.78%] [G loss: 2.209850]\n",
      "epoch:6 step:6371 [D loss: 0.536064, acc: 74.22%] [G loss: 2.541472]\n",
      "epoch:6 step:6372 [D loss: 0.587702, acc: 68.75%] [G loss: 2.551462]\n",
      "epoch:6 step:6373 [D loss: 0.648826, acc: 66.41%] [G loss: 2.205846]\n",
      "epoch:6 step:6374 [D loss: 0.610098, acc: 68.75%] [G loss: 2.463153]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:6 step:6375 [D loss: 0.720248, acc: 57.03%] [G loss: 2.366795]\n",
      "epoch:6 step:6376 [D loss: 0.605982, acc: 66.41%] [G loss: 2.478854]\n",
      "epoch:6 step:6377 [D loss: 0.624187, acc: 61.72%] [G loss: 2.202310]\n",
      "epoch:6 step:6378 [D loss: 0.630150, acc: 67.97%] [G loss: 2.148419]\n",
      "epoch:6 step:6379 [D loss: 0.569250, acc: 67.19%] [G loss: 2.337694]\n",
      "epoch:6 step:6380 [D loss: 0.661420, acc: 65.62%] [G loss: 2.319772]\n",
      "epoch:6 step:6381 [D loss: 0.576409, acc: 71.09%] [G loss: 2.264945]\n",
      "epoch:6 step:6382 [D loss: 0.643801, acc: 65.62%] [G loss: 2.039869]\n",
      "epoch:6 step:6383 [D loss: 0.654527, acc: 60.94%] [G loss: 2.412029]\n",
      "epoch:6 step:6384 [D loss: 0.630357, acc: 61.72%] [G loss: 2.139304]\n",
      "epoch:6 step:6385 [D loss: 0.587749, acc: 69.53%] [G loss: 2.412556]\n",
      "epoch:6 step:6386 [D loss: 0.647190, acc: 63.28%] [G loss: 2.275169]\n",
      "epoch:6 step:6387 [D loss: 0.692364, acc: 60.16%] [G loss: 2.134082]\n",
      "epoch:6 step:6388 [D loss: 0.634906, acc: 62.50%] [G loss: 2.040011]\n",
      "epoch:6 step:6389 [D loss: 0.644752, acc: 66.41%] [G loss: 2.325977]\n",
      "epoch:6 step:6390 [D loss: 0.679552, acc: 57.03%] [G loss: 2.149134]\n",
      "epoch:6 step:6391 [D loss: 0.534944, acc: 74.22%] [G loss: 2.451053]\n",
      "epoch:6 step:6392 [D loss: 0.647558, acc: 63.28%] [G loss: 1.943566]\n",
      "epoch:6 step:6393 [D loss: 0.625511, acc: 64.06%] [G loss: 2.264871]\n",
      "epoch:6 step:6394 [D loss: 0.649007, acc: 61.72%] [G loss: 2.382733]\n",
      "epoch:6 step:6395 [D loss: 0.594970, acc: 70.31%] [G loss: 2.441809]\n",
      "epoch:6 step:6396 [D loss: 0.606506, acc: 64.06%] [G loss: 2.424400]\n",
      "epoch:6 step:6397 [D loss: 0.576532, acc: 68.75%] [G loss: 2.833836]\n",
      "epoch:6 step:6398 [D loss: 0.645929, acc: 60.16%] [G loss: 2.267441]\n",
      "epoch:6 step:6399 [D loss: 0.538054, acc: 75.78%] [G loss: 2.525508]\n",
      "epoch:6 step:6400 [D loss: 0.612994, acc: 64.84%] [G loss: 2.375135]\n",
      "##############\n",
      "[2.70577136 1.42732866 6.83196742 5.11649852 4.00926679 5.91949291\n",
      " 4.75632232 5.06481378 5.09708324 3.6656158 ]\n",
      "##########\n",
      "epoch:6 step:6401 [D loss: 0.644057, acc: 58.59%] [G loss: 2.240815]\n",
      "epoch:6 step:6402 [D loss: 0.626376, acc: 68.75%] [G loss: 2.378433]\n",
      "epoch:6 step:6403 [D loss: 0.555120, acc: 71.09%] [G loss: 2.453762]\n",
      "epoch:6 step:6404 [D loss: 0.550802, acc: 73.44%] [G loss: 2.307183]\n",
      "epoch:6 step:6405 [D loss: 0.724781, acc: 57.81%] [G loss: 2.335698]\n",
      "epoch:6 step:6406 [D loss: 0.672612, acc: 60.16%] [G loss: 2.317404]\n",
      "epoch:6 step:6407 [D loss: 0.628149, acc: 60.94%] [G loss: 2.173397]\n",
      "epoch:6 step:6408 [D loss: 0.595065, acc: 64.06%] [G loss: 2.450746]\n",
      "epoch:6 step:6409 [D loss: 0.609243, acc: 65.62%] [G loss: 2.363050]\n",
      "epoch:6 step:6410 [D loss: 0.650545, acc: 61.72%] [G loss: 2.006600]\n",
      "epoch:6 step:6411 [D loss: 0.653842, acc: 61.72%] [G loss: 2.097429]\n",
      "epoch:6 step:6412 [D loss: 0.617979, acc: 64.84%] [G loss: 2.197145]\n",
      "epoch:6 step:6413 [D loss: 0.668712, acc: 64.06%] [G loss: 2.329477]\n",
      "epoch:6 step:6414 [D loss: 0.580910, acc: 68.75%] [G loss: 2.766034]\n",
      "epoch:6 step:6415 [D loss: 0.617861, acc: 60.16%] [G loss: 2.288576]\n",
      "epoch:6 step:6416 [D loss: 0.715999, acc: 60.16%] [G loss: 2.363825]\n",
      "epoch:6 step:6417 [D loss: 0.645577, acc: 64.06%] [G loss: 2.489141]\n",
      "epoch:6 step:6418 [D loss: 0.670283, acc: 61.72%] [G loss: 2.328319]\n",
      "epoch:6 step:6419 [D loss: 0.646706, acc: 60.16%] [G loss: 2.093846]\n",
      "epoch:6 step:6420 [D loss: 0.578305, acc: 68.75%] [G loss: 2.432873]\n",
      "epoch:6 step:6421 [D loss: 0.613225, acc: 67.97%] [G loss: 2.148426]\n",
      "epoch:6 step:6422 [D loss: 0.621567, acc: 61.72%] [G loss: 2.349761]\n",
      "epoch:6 step:6423 [D loss: 0.607782, acc: 66.41%] [G loss: 2.309498]\n",
      "epoch:6 step:6424 [D loss: 0.618312, acc: 67.19%] [G loss: 2.243565]\n",
      "epoch:6 step:6425 [D loss: 0.634224, acc: 64.06%] [G loss: 2.493557]\n",
      "epoch:6 step:6426 [D loss: 0.640771, acc: 59.38%] [G loss: 2.167197]\n",
      "epoch:6 step:6427 [D loss: 0.603093, acc: 64.84%] [G loss: 2.440791]\n",
      "epoch:6 step:6428 [D loss: 0.572947, acc: 70.31%] [G loss: 2.515648]\n",
      "epoch:6 step:6429 [D loss: 0.546072, acc: 70.31%] [G loss: 2.617983]\n",
      "epoch:6 step:6430 [D loss: 0.571290, acc: 70.31%] [G loss: 2.488365]\n",
      "epoch:6 step:6431 [D loss: 0.552015, acc: 70.31%] [G loss: 2.721245]\n",
      "epoch:6 step:6432 [D loss: 0.575967, acc: 66.41%] [G loss: 2.524623]\n",
      "epoch:6 step:6433 [D loss: 0.668392, acc: 65.62%] [G loss: 2.371884]\n",
      "epoch:6 step:6434 [D loss: 0.582871, acc: 70.31%] [G loss: 2.267827]\n",
      "epoch:6 step:6435 [D loss: 0.577941, acc: 71.09%] [G loss: 2.381480]\n",
      "epoch:6 step:6436 [D loss: 0.608559, acc: 67.19%] [G loss: 2.431478]\n",
      "epoch:6 step:6437 [D loss: 0.589895, acc: 70.31%] [G loss: 2.997868]\n",
      "epoch:6 step:6438 [D loss: 0.616751, acc: 68.75%] [G loss: 2.927706]\n",
      "epoch:6 step:6439 [D loss: 0.627600, acc: 62.50%] [G loss: 2.342015]\n",
      "epoch:6 step:6440 [D loss: 0.621145, acc: 65.62%] [G loss: 2.237309]\n",
      "epoch:6 step:6441 [D loss: 0.690322, acc: 56.25%] [G loss: 2.371435]\n",
      "epoch:6 step:6442 [D loss: 0.688132, acc: 57.03%] [G loss: 1.858182]\n",
      "epoch:6 step:6443 [D loss: 0.550293, acc: 74.22%] [G loss: 2.427220]\n",
      "epoch:6 step:6444 [D loss: 0.584065, acc: 67.97%] [G loss: 2.608470]\n",
      "epoch:6 step:6445 [D loss: 0.567625, acc: 75.78%] [G loss: 2.331355]\n",
      "epoch:6 step:6446 [D loss: 0.592709, acc: 67.19%] [G loss: 2.206299]\n",
      "epoch:6 step:6447 [D loss: 0.581578, acc: 69.53%] [G loss: 2.561347]\n",
      "epoch:6 step:6448 [D loss: 0.659609, acc: 59.38%] [G loss: 2.177568]\n",
      "epoch:6 step:6449 [D loss: 0.626909, acc: 59.38%] [G loss: 2.006652]\n",
      "epoch:6 step:6450 [D loss: 0.649074, acc: 62.50%] [G loss: 2.087281]\n",
      "epoch:6 step:6451 [D loss: 0.568861, acc: 70.31%] [G loss: 2.324048]\n",
      "epoch:6 step:6452 [D loss: 0.629291, acc: 66.41%] [G loss: 2.488495]\n",
      "epoch:6 step:6453 [D loss: 0.606465, acc: 66.41%] [G loss: 2.268558]\n",
      "epoch:6 step:6454 [D loss: 0.580701, acc: 67.19%] [G loss: 2.267009]\n",
      "epoch:6 step:6455 [D loss: 0.543142, acc: 72.66%] [G loss: 2.396649]\n",
      "epoch:6 step:6456 [D loss: 0.536085, acc: 66.41%] [G loss: 2.447770]\n",
      "epoch:6 step:6457 [D loss: 0.638489, acc: 65.62%] [G loss: 2.200164]\n",
      "epoch:6 step:6458 [D loss: 0.629658, acc: 67.19%] [G loss: 2.187627]\n",
      "epoch:6 step:6459 [D loss: 0.580996, acc: 67.97%] [G loss: 2.345534]\n",
      "epoch:6 step:6460 [D loss: 0.562683, acc: 73.44%] [G loss: 2.325212]\n",
      "epoch:6 step:6461 [D loss: 0.553548, acc: 70.31%] [G loss: 2.368417]\n",
      "epoch:6 step:6462 [D loss: 0.583242, acc: 68.75%] [G loss: 2.348228]\n",
      "epoch:6 step:6463 [D loss: 0.587758, acc: 65.62%] [G loss: 2.427374]\n",
      "epoch:6 step:6464 [D loss: 0.521737, acc: 76.56%] [G loss: 2.326786]\n",
      "epoch:6 step:6465 [D loss: 0.660962, acc: 62.50%] [G loss: 2.389817]\n",
      "epoch:6 step:6466 [D loss: 0.639649, acc: 67.97%] [G loss: 2.401363]\n",
      "epoch:6 step:6467 [D loss: 0.618086, acc: 67.19%] [G loss: 2.535513]\n",
      "epoch:6 step:6468 [D loss: 0.636063, acc: 64.84%] [G loss: 2.470337]\n",
      "epoch:6 step:6469 [D loss: 0.607395, acc: 64.84%] [G loss: 2.449493]\n",
      "epoch:6 step:6470 [D loss: 0.589734, acc: 66.41%] [G loss: 2.478686]\n",
      "epoch:6 step:6471 [D loss: 0.605625, acc: 67.97%] [G loss: 2.242668]\n",
      "epoch:6 step:6472 [D loss: 0.675182, acc: 64.84%] [G loss: 2.229578]\n",
      "epoch:6 step:6473 [D loss: 0.606319, acc: 67.19%] [G loss: 2.141092]\n",
      "epoch:6 step:6474 [D loss: 0.587395, acc: 70.31%] [G loss: 2.638253]\n",
      "epoch:6 step:6475 [D loss: 0.606641, acc: 71.88%] [G loss: 2.370589]\n",
      "epoch:6 step:6476 [D loss: 0.576241, acc: 69.53%] [G loss: 2.341897]\n",
      "epoch:6 step:6477 [D loss: 0.591676, acc: 67.19%] [G loss: 2.362481]\n",
      "epoch:6 step:6478 [D loss: 0.643861, acc: 63.28%] [G loss: 2.218133]\n",
      "epoch:6 step:6479 [D loss: 0.583202, acc: 74.22%] [G loss: 2.646258]\n",
      "epoch:6 step:6480 [D loss: 0.745819, acc: 56.25%] [G loss: 2.402065]\n",
      "epoch:6 step:6481 [D loss: 0.593019, acc: 64.06%] [G loss: 2.404702]\n",
      "epoch:6 step:6482 [D loss: 0.578200, acc: 72.66%] [G loss: 2.477104]\n",
      "epoch:6 step:6483 [D loss: 0.616933, acc: 67.97%] [G loss: 2.001388]\n",
      "epoch:6 step:6484 [D loss: 0.611929, acc: 69.53%] [G loss: 2.203861]\n",
      "epoch:6 step:6485 [D loss: 0.555861, acc: 75.00%] [G loss: 2.343960]\n",
      "epoch:6 step:6486 [D loss: 0.547298, acc: 71.88%] [G loss: 2.510037]\n",
      "epoch:6 step:6487 [D loss: 0.584619, acc: 64.06%] [G loss: 2.419177]\n",
      "epoch:6 step:6488 [D loss: 0.549698, acc: 75.78%] [G loss: 2.494003]\n",
      "epoch:6 step:6489 [D loss: 0.622158, acc: 65.62%] [G loss: 2.193253]\n",
      "epoch:6 step:6490 [D loss: 0.629275, acc: 65.62%] [G loss: 2.498861]\n",
      "epoch:6 step:6491 [D loss: 0.639109, acc: 66.41%] [G loss: 2.387950]\n",
      "epoch:6 step:6492 [D loss: 0.629920, acc: 61.72%] [G loss: 2.238902]\n",
      "epoch:6 step:6493 [D loss: 0.592029, acc: 71.88%] [G loss: 2.510893]\n",
      "epoch:6 step:6494 [D loss: 0.597391, acc: 65.62%] [G loss: 2.342342]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:6 step:6495 [D loss: 0.603790, acc: 69.53%] [G loss: 2.405437]\n",
      "epoch:6 step:6496 [D loss: 0.689753, acc: 56.25%] [G loss: 2.165975]\n",
      "epoch:6 step:6497 [D loss: 0.520642, acc: 77.34%] [G loss: 2.549425]\n",
      "epoch:6 step:6498 [D loss: 0.570005, acc: 69.53%] [G loss: 2.463823]\n",
      "epoch:6 step:6499 [D loss: 0.559898, acc: 71.88%] [G loss: 2.619875]\n",
      "epoch:6 step:6500 [D loss: 0.679665, acc: 61.72%] [G loss: 2.406001]\n",
      "epoch:6 step:6501 [D loss: 0.676142, acc: 57.03%] [G loss: 2.156114]\n",
      "epoch:6 step:6502 [D loss: 0.770114, acc: 56.25%] [G loss: 2.272078]\n",
      "epoch:6 step:6503 [D loss: 0.720551, acc: 57.03%] [G loss: 2.325898]\n",
      "epoch:6 step:6504 [D loss: 0.661695, acc: 64.06%] [G loss: 2.160599]\n",
      "epoch:6 step:6505 [D loss: 0.609725, acc: 67.97%] [G loss: 2.369002]\n",
      "epoch:6 step:6506 [D loss: 0.617085, acc: 70.31%] [G loss: 2.428622]\n",
      "epoch:6 step:6507 [D loss: 0.561570, acc: 72.66%] [G loss: 2.266254]\n",
      "epoch:6 step:6508 [D loss: 0.518468, acc: 78.91%] [G loss: 2.849837]\n",
      "epoch:6 step:6509 [D loss: 0.531154, acc: 68.75%] [G loss: 2.579797]\n",
      "epoch:6 step:6510 [D loss: 0.667364, acc: 65.62%] [G loss: 2.462428]\n",
      "epoch:6 step:6511 [D loss: 0.586728, acc: 68.75%] [G loss: 2.321371]\n",
      "epoch:6 step:6512 [D loss: 0.564708, acc: 75.00%] [G loss: 2.585249]\n",
      "epoch:6 step:6513 [D loss: 0.633496, acc: 65.62%] [G loss: 2.099588]\n",
      "epoch:6 step:6514 [D loss: 0.659999, acc: 66.41%] [G loss: 2.497415]\n",
      "epoch:6 step:6515 [D loss: 0.584643, acc: 72.66%] [G loss: 2.562049]\n",
      "epoch:6 step:6516 [D loss: 0.541433, acc: 70.31%] [G loss: 2.381849]\n",
      "epoch:6 step:6517 [D loss: 0.637572, acc: 65.62%] [G loss: 2.197787]\n",
      "epoch:6 step:6518 [D loss: 0.678801, acc: 61.72%] [G loss: 2.203193]\n",
      "epoch:6 step:6519 [D loss: 0.612186, acc: 64.84%] [G loss: 2.531348]\n",
      "epoch:6 step:6520 [D loss: 0.641954, acc: 65.62%] [G loss: 2.327163]\n",
      "epoch:6 step:6521 [D loss: 0.552408, acc: 70.31%] [G loss: 2.443585]\n",
      "epoch:6 step:6522 [D loss: 0.607343, acc: 68.75%] [G loss: 2.236751]\n",
      "epoch:6 step:6523 [D loss: 0.681810, acc: 63.28%] [G loss: 2.334949]\n",
      "epoch:6 step:6524 [D loss: 0.725178, acc: 53.91%] [G loss: 2.081369]\n",
      "epoch:6 step:6525 [D loss: 0.610936, acc: 68.75%] [G loss: 2.305391]\n",
      "epoch:6 step:6526 [D loss: 0.615248, acc: 64.84%] [G loss: 2.295535]\n",
      "epoch:6 step:6527 [D loss: 0.627783, acc: 64.06%] [G loss: 2.429717]\n",
      "epoch:6 step:6528 [D loss: 0.615131, acc: 68.75%] [G loss: 2.305366]\n",
      "epoch:6 step:6529 [D loss: 0.550238, acc: 71.88%] [G loss: 2.189552]\n",
      "epoch:6 step:6530 [D loss: 0.624238, acc: 64.84%] [G loss: 2.248620]\n",
      "epoch:6 step:6531 [D loss: 0.638234, acc: 64.06%] [G loss: 2.469513]\n",
      "epoch:6 step:6532 [D loss: 0.535210, acc: 74.22%] [G loss: 2.517600]\n",
      "epoch:6 step:6533 [D loss: 0.564549, acc: 69.53%] [G loss: 2.601207]\n",
      "epoch:6 step:6534 [D loss: 0.550062, acc: 67.97%] [G loss: 2.991687]\n",
      "epoch:6 step:6535 [D loss: 0.558310, acc: 71.88%] [G loss: 2.470675]\n",
      "epoch:6 step:6536 [D loss: 0.661447, acc: 64.06%] [G loss: 2.370649]\n",
      "epoch:6 step:6537 [D loss: 0.607217, acc: 67.97%] [G loss: 2.279111]\n",
      "epoch:6 step:6538 [D loss: 0.659687, acc: 60.16%] [G loss: 2.268620]\n",
      "epoch:6 step:6539 [D loss: 0.635557, acc: 61.72%] [G loss: 2.310015]\n",
      "epoch:6 step:6540 [D loss: 0.573549, acc: 70.31%] [G loss: 2.469285]\n",
      "epoch:6 step:6541 [D loss: 0.603973, acc: 67.19%] [G loss: 2.635167]\n",
      "epoch:6 step:6542 [D loss: 0.743172, acc: 56.25%] [G loss: 2.401117]\n",
      "epoch:6 step:6543 [D loss: 0.600789, acc: 66.41%] [G loss: 2.531785]\n",
      "epoch:6 step:6544 [D loss: 0.644886, acc: 73.44%] [G loss: 2.300422]\n",
      "epoch:6 step:6545 [D loss: 0.514910, acc: 77.34%] [G loss: 2.718112]\n",
      "epoch:6 step:6546 [D loss: 0.539586, acc: 72.66%] [G loss: 2.899227]\n",
      "epoch:6 step:6547 [D loss: 0.526589, acc: 73.44%] [G loss: 2.943027]\n",
      "epoch:6 step:6548 [D loss: 0.535526, acc: 75.00%] [G loss: 2.716070]\n",
      "epoch:6 step:6549 [D loss: 0.589850, acc: 70.31%] [G loss: 2.865484]\n",
      "epoch:6 step:6550 [D loss: 0.901262, acc: 53.91%] [G loss: 2.070982]\n",
      "epoch:6 step:6551 [D loss: 0.692184, acc: 57.81%] [G loss: 2.442159]\n",
      "epoch:6 step:6552 [D loss: 0.580354, acc: 69.53%] [G loss: 2.268926]\n",
      "epoch:6 step:6553 [D loss: 0.602067, acc: 69.53%] [G loss: 2.576178]\n",
      "epoch:6 step:6554 [D loss: 0.637488, acc: 64.06%] [G loss: 2.293098]\n",
      "epoch:6 step:6555 [D loss: 0.584295, acc: 67.97%] [G loss: 2.454176]\n",
      "epoch:6 step:6556 [D loss: 0.570352, acc: 71.88%] [G loss: 2.681137]\n",
      "epoch:6 step:6557 [D loss: 0.575989, acc: 77.34%] [G loss: 2.432597]\n",
      "epoch:6 step:6558 [D loss: 0.544722, acc: 68.75%] [G loss: 2.843594]\n",
      "epoch:6 step:6559 [D loss: 0.530995, acc: 78.91%] [G loss: 3.225152]\n",
      "epoch:7 step:6560 [D loss: 0.636170, acc: 62.50%] [G loss: 2.414987]\n",
      "epoch:7 step:6561 [D loss: 0.554342, acc: 72.66%] [G loss: 2.557700]\n",
      "epoch:7 step:6562 [D loss: 0.639932, acc: 61.72%] [G loss: 2.282663]\n",
      "epoch:7 step:6563 [D loss: 0.548954, acc: 72.66%] [G loss: 2.611932]\n",
      "epoch:7 step:6564 [D loss: 0.554885, acc: 68.75%] [G loss: 2.543232]\n",
      "epoch:7 step:6565 [D loss: 0.593826, acc: 71.09%] [G loss: 2.632050]\n",
      "epoch:7 step:6566 [D loss: 0.621238, acc: 66.41%] [G loss: 2.624203]\n",
      "epoch:7 step:6567 [D loss: 0.631953, acc: 64.06%] [G loss: 2.567798]\n",
      "epoch:7 step:6568 [D loss: 0.694382, acc: 63.28%] [G loss: 2.606927]\n",
      "epoch:7 step:6569 [D loss: 0.590371, acc: 65.62%] [G loss: 2.556345]\n",
      "epoch:7 step:6570 [D loss: 0.632639, acc: 67.97%] [G loss: 2.192779]\n",
      "epoch:7 step:6571 [D loss: 0.598784, acc: 64.06%] [G loss: 2.345580]\n",
      "epoch:7 step:6572 [D loss: 0.570447, acc: 70.31%] [G loss: 2.444103]\n",
      "epoch:7 step:6573 [D loss: 0.565149, acc: 71.09%] [G loss: 2.430571]\n",
      "epoch:7 step:6574 [D loss: 0.563082, acc: 69.53%] [G loss: 2.520829]\n",
      "epoch:7 step:6575 [D loss: 0.593092, acc: 75.78%] [G loss: 2.435669]\n",
      "epoch:7 step:6576 [D loss: 0.619146, acc: 67.97%] [G loss: 2.022747]\n",
      "epoch:7 step:6577 [D loss: 0.688650, acc: 54.69%] [G loss: 2.297168]\n",
      "epoch:7 step:6578 [D loss: 0.572044, acc: 73.44%] [G loss: 2.264047]\n",
      "epoch:7 step:6579 [D loss: 0.639056, acc: 62.50%] [G loss: 1.953737]\n",
      "epoch:7 step:6580 [D loss: 0.640861, acc: 66.41%] [G loss: 2.292029]\n",
      "epoch:7 step:6581 [D loss: 0.605455, acc: 67.97%] [G loss: 2.496874]\n",
      "epoch:7 step:6582 [D loss: 0.576160, acc: 69.53%] [G loss: 2.538617]\n",
      "epoch:7 step:6583 [D loss: 0.595994, acc: 67.97%] [G loss: 2.633500]\n",
      "epoch:7 step:6584 [D loss: 0.551975, acc: 73.44%] [G loss: 2.739449]\n",
      "epoch:7 step:6585 [D loss: 0.641166, acc: 67.97%] [G loss: 2.338230]\n",
      "epoch:7 step:6586 [D loss: 0.595048, acc: 71.09%] [G loss: 2.409635]\n",
      "epoch:7 step:6587 [D loss: 0.560271, acc: 71.88%] [G loss: 2.165864]\n",
      "epoch:7 step:6588 [D loss: 0.643655, acc: 65.62%] [G loss: 2.459489]\n",
      "epoch:7 step:6589 [D loss: 0.641391, acc: 65.62%] [G loss: 2.260205]\n",
      "epoch:7 step:6590 [D loss: 0.663753, acc: 63.28%] [G loss: 2.182517]\n",
      "epoch:7 step:6591 [D loss: 0.637667, acc: 61.72%] [G loss: 2.320397]\n",
      "epoch:7 step:6592 [D loss: 0.604954, acc: 66.41%] [G loss: 2.269445]\n",
      "epoch:7 step:6593 [D loss: 0.562036, acc: 70.31%] [G loss: 2.479689]\n",
      "epoch:7 step:6594 [D loss: 0.654412, acc: 62.50%] [G loss: 2.271853]\n",
      "epoch:7 step:6595 [D loss: 0.585372, acc: 66.41%] [G loss: 2.508942]\n",
      "epoch:7 step:6596 [D loss: 0.591489, acc: 67.97%] [G loss: 2.584536]\n",
      "epoch:7 step:6597 [D loss: 0.608768, acc: 67.19%] [G loss: 2.368183]\n",
      "epoch:7 step:6598 [D loss: 0.602802, acc: 67.19%] [G loss: 2.501876]\n",
      "epoch:7 step:6599 [D loss: 0.554433, acc: 70.31%] [G loss: 2.801724]\n",
      "epoch:7 step:6600 [D loss: 0.568742, acc: 68.75%] [G loss: 2.285751]\n",
      "##############\n",
      "[2.51471244 1.41770338 6.7501031  4.99730682 4.01353734 5.81732666\n",
      " 4.78299315 4.8756295  5.0964353  3.62373673]\n",
      "##########\n",
      "epoch:7 step:6601 [D loss: 0.592373, acc: 65.62%] [G loss: 2.852786]\n",
      "epoch:7 step:6602 [D loss: 0.599431, acc: 66.41%] [G loss: 2.383374]\n",
      "epoch:7 step:6603 [D loss: 0.640870, acc: 60.16%] [G loss: 2.111177]\n",
      "epoch:7 step:6604 [D loss: 0.661563, acc: 59.38%] [G loss: 2.193568]\n",
      "epoch:7 step:6605 [D loss: 0.616077, acc: 64.84%] [G loss: 2.208990]\n",
      "epoch:7 step:6606 [D loss: 0.636946, acc: 67.97%] [G loss: 2.167055]\n",
      "epoch:7 step:6607 [D loss: 0.593264, acc: 67.19%] [G loss: 2.603988]\n",
      "epoch:7 step:6608 [D loss: 0.642393, acc: 63.28%] [G loss: 2.271704]\n",
      "epoch:7 step:6609 [D loss: 0.630592, acc: 61.72%] [G loss: 2.434767]\n",
      "epoch:7 step:6610 [D loss: 0.634215, acc: 64.06%] [G loss: 2.266482]\n",
      "epoch:7 step:6611 [D loss: 0.573608, acc: 72.66%] [G loss: 2.452472]\n",
      "epoch:7 step:6612 [D loss: 0.643593, acc: 64.84%] [G loss: 2.603925]\n",
      "epoch:7 step:6613 [D loss: 0.590409, acc: 67.97%] [G loss: 2.579395]\n",
      "epoch:7 step:6614 [D loss: 0.601168, acc: 65.62%] [G loss: 2.507578]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:7 step:6615 [D loss: 0.603852, acc: 65.62%] [G loss: 2.313869]\n",
      "epoch:7 step:6616 [D loss: 0.645764, acc: 64.84%] [G loss: 2.531722]\n",
      "epoch:7 step:6617 [D loss: 0.564712, acc: 68.75%] [G loss: 2.184786]\n",
      "epoch:7 step:6618 [D loss: 0.592723, acc: 69.53%] [G loss: 2.240333]\n",
      "epoch:7 step:6619 [D loss: 0.636969, acc: 61.72%] [G loss: 2.322393]\n",
      "epoch:7 step:6620 [D loss: 0.639346, acc: 62.50%] [G loss: 2.610289]\n",
      "epoch:7 step:6621 [D loss: 0.615952, acc: 66.41%] [G loss: 2.382977]\n",
      "epoch:7 step:6622 [D loss: 0.601045, acc: 64.06%] [G loss: 2.165149]\n",
      "epoch:7 step:6623 [D loss: 0.587012, acc: 67.19%] [G loss: 2.556897]\n",
      "epoch:7 step:6624 [D loss: 0.654112, acc: 64.84%] [G loss: 2.674594]\n",
      "epoch:7 step:6625 [D loss: 0.595731, acc: 69.53%] [G loss: 2.267493]\n",
      "epoch:7 step:6626 [D loss: 0.556557, acc: 67.97%] [G loss: 2.262388]\n",
      "epoch:7 step:6627 [D loss: 0.601682, acc: 71.09%] [G loss: 2.551931]\n",
      "epoch:7 step:6628 [D loss: 0.593173, acc: 69.53%] [G loss: 2.593217]\n",
      "epoch:7 step:6629 [D loss: 0.557182, acc: 71.09%] [G loss: 2.534724]\n",
      "epoch:7 step:6630 [D loss: 0.581388, acc: 72.66%] [G loss: 2.457919]\n",
      "epoch:7 step:6631 [D loss: 0.586375, acc: 69.53%] [G loss: 2.345973]\n",
      "epoch:7 step:6632 [D loss: 0.650896, acc: 61.72%] [G loss: 2.197717]\n",
      "epoch:7 step:6633 [D loss: 0.589441, acc: 67.97%] [G loss: 2.390402]\n",
      "epoch:7 step:6634 [D loss: 0.547320, acc: 71.88%] [G loss: 2.486197]\n",
      "epoch:7 step:6635 [D loss: 0.553464, acc: 71.88%] [G loss: 2.653866]\n",
      "epoch:7 step:6636 [D loss: 0.525165, acc: 79.69%] [G loss: 2.915979]\n",
      "epoch:7 step:6637 [D loss: 0.625665, acc: 70.31%] [G loss: 2.285150]\n",
      "epoch:7 step:6638 [D loss: 0.603734, acc: 65.62%] [G loss: 2.202710]\n",
      "epoch:7 step:6639 [D loss: 0.760058, acc: 56.25%] [G loss: 2.159324]\n",
      "epoch:7 step:6640 [D loss: 0.620700, acc: 60.16%] [G loss: 2.212663]\n",
      "epoch:7 step:6641 [D loss: 0.571806, acc: 71.88%] [G loss: 2.459550]\n",
      "epoch:7 step:6642 [D loss: 0.585130, acc: 71.09%] [G loss: 2.502870]\n",
      "epoch:7 step:6643 [D loss: 0.598926, acc: 70.31%] [G loss: 2.375899]\n",
      "epoch:7 step:6644 [D loss: 0.554837, acc: 71.09%] [G loss: 2.375505]\n",
      "epoch:7 step:6645 [D loss: 0.593962, acc: 70.31%] [G loss: 2.092183]\n",
      "epoch:7 step:6646 [D loss: 0.581967, acc: 67.97%] [G loss: 2.362005]\n",
      "epoch:7 step:6647 [D loss: 0.639738, acc: 61.72%] [G loss: 2.355306]\n",
      "epoch:7 step:6648 [D loss: 0.641184, acc: 63.28%] [G loss: 2.588312]\n",
      "epoch:7 step:6649 [D loss: 0.602521, acc: 66.41%] [G loss: 2.370677]\n",
      "epoch:7 step:6650 [D loss: 0.634627, acc: 65.62%] [G loss: 2.423200]\n",
      "epoch:7 step:6651 [D loss: 0.632229, acc: 66.41%] [G loss: 2.433949]\n",
      "epoch:7 step:6652 [D loss: 0.575881, acc: 71.09%] [G loss: 2.533869]\n",
      "epoch:7 step:6653 [D loss: 0.605014, acc: 65.62%] [G loss: 2.474145]\n",
      "epoch:7 step:6654 [D loss: 0.688159, acc: 58.59%] [G loss: 2.266537]\n",
      "epoch:7 step:6655 [D loss: 0.597754, acc: 72.66%] [G loss: 2.297843]\n",
      "epoch:7 step:6656 [D loss: 0.561409, acc: 75.00%] [G loss: 2.439209]\n",
      "epoch:7 step:6657 [D loss: 0.648174, acc: 64.06%] [G loss: 2.167825]\n",
      "epoch:7 step:6658 [D loss: 0.666305, acc: 57.81%] [G loss: 2.249987]\n",
      "epoch:7 step:6659 [D loss: 0.563706, acc: 74.22%] [G loss: 2.345454]\n",
      "epoch:7 step:6660 [D loss: 0.703838, acc: 56.25%] [G loss: 2.427098]\n",
      "epoch:7 step:6661 [D loss: 0.658928, acc: 61.72%] [G loss: 2.177070]\n",
      "epoch:7 step:6662 [D loss: 0.533779, acc: 73.44%] [G loss: 2.284394]\n",
      "epoch:7 step:6663 [D loss: 0.613859, acc: 69.53%] [G loss: 2.342070]\n",
      "epoch:7 step:6664 [D loss: 0.645089, acc: 60.16%] [G loss: 2.456751]\n",
      "epoch:7 step:6665 [D loss: 0.600732, acc: 65.62%] [G loss: 2.169190]\n",
      "epoch:7 step:6666 [D loss: 0.612256, acc: 67.97%] [G loss: 2.236030]\n",
      "epoch:7 step:6667 [D loss: 0.750110, acc: 55.47%] [G loss: 1.999261]\n",
      "epoch:7 step:6668 [D loss: 0.645967, acc: 64.06%] [G loss: 2.019121]\n",
      "epoch:7 step:6669 [D loss: 0.620870, acc: 69.53%] [G loss: 2.218018]\n",
      "epoch:7 step:6670 [D loss: 0.590889, acc: 71.09%] [G loss: 2.253341]\n",
      "epoch:7 step:6671 [D loss: 0.595348, acc: 67.97%] [G loss: 2.245858]\n",
      "epoch:7 step:6672 [D loss: 0.645065, acc: 64.06%] [G loss: 2.189707]\n",
      "epoch:7 step:6673 [D loss: 0.606598, acc: 67.97%] [G loss: 2.290781]\n",
      "epoch:7 step:6674 [D loss: 0.620685, acc: 64.84%] [G loss: 2.304244]\n",
      "epoch:7 step:6675 [D loss: 0.593208, acc: 65.62%] [G loss: 2.432815]\n",
      "epoch:7 step:6676 [D loss: 0.645058, acc: 68.75%] [G loss: 2.429058]\n",
      "epoch:7 step:6677 [D loss: 0.685122, acc: 66.41%] [G loss: 2.350816]\n",
      "epoch:7 step:6678 [D loss: 0.550858, acc: 68.75%] [G loss: 2.725382]\n",
      "epoch:7 step:6679 [D loss: 0.656426, acc: 65.62%] [G loss: 2.358246]\n",
      "epoch:7 step:6680 [D loss: 0.610811, acc: 64.06%] [G loss: 2.378772]\n",
      "epoch:7 step:6681 [D loss: 0.543730, acc: 70.31%] [G loss: 2.561536]\n",
      "epoch:7 step:6682 [D loss: 0.637811, acc: 61.72%] [G loss: 2.328386]\n",
      "epoch:7 step:6683 [D loss: 0.695411, acc: 60.16%] [G loss: 2.271781]\n",
      "epoch:7 step:6684 [D loss: 0.696393, acc: 57.81%] [G loss: 2.302152]\n",
      "epoch:7 step:6685 [D loss: 0.588226, acc: 70.31%] [G loss: 2.455637]\n",
      "epoch:7 step:6686 [D loss: 0.619662, acc: 65.62%] [G loss: 2.204746]\n",
      "epoch:7 step:6687 [D loss: 0.578021, acc: 69.53%] [G loss: 2.373780]\n",
      "epoch:7 step:6688 [D loss: 0.638539, acc: 61.72%] [G loss: 2.359864]\n",
      "epoch:7 step:6689 [D loss: 0.607958, acc: 70.31%] [G loss: 2.431759]\n",
      "epoch:7 step:6690 [D loss: 0.652655, acc: 59.38%] [G loss: 2.275167]\n",
      "epoch:7 step:6691 [D loss: 0.613079, acc: 70.31%] [G loss: 2.164261]\n",
      "epoch:7 step:6692 [D loss: 0.655837, acc: 55.47%] [G loss: 2.313598]\n",
      "epoch:7 step:6693 [D loss: 0.723764, acc: 50.78%] [G loss: 2.055982]\n",
      "epoch:7 step:6694 [D loss: 0.610216, acc: 67.97%] [G loss: 2.304009]\n",
      "epoch:7 step:6695 [D loss: 0.635939, acc: 63.28%] [G loss: 2.180531]\n",
      "epoch:7 step:6696 [D loss: 0.675379, acc: 60.94%] [G loss: 2.163030]\n",
      "epoch:7 step:6697 [D loss: 0.574368, acc: 68.75%] [G loss: 2.140936]\n",
      "epoch:7 step:6698 [D loss: 0.661035, acc: 64.06%] [G loss: 2.615919]\n",
      "epoch:7 step:6699 [D loss: 0.613264, acc: 65.62%] [G loss: 2.382120]\n",
      "epoch:7 step:6700 [D loss: 0.625196, acc: 66.41%] [G loss: 2.354827]\n",
      "epoch:7 step:6701 [D loss: 0.609142, acc: 64.84%] [G loss: 2.114939]\n",
      "epoch:7 step:6702 [D loss: 0.573781, acc: 67.97%] [G loss: 2.153627]\n",
      "epoch:7 step:6703 [D loss: 0.587924, acc: 69.53%] [G loss: 2.402145]\n",
      "epoch:7 step:6704 [D loss: 0.556974, acc: 73.44%] [G loss: 2.373005]\n",
      "epoch:7 step:6705 [D loss: 0.619281, acc: 70.31%] [G loss: 2.365102]\n",
      "epoch:7 step:6706 [D loss: 0.673873, acc: 64.84%] [G loss: 2.159104]\n",
      "epoch:7 step:6707 [D loss: 0.686157, acc: 58.59%] [G loss: 2.222651]\n",
      "epoch:7 step:6708 [D loss: 0.589343, acc: 65.62%] [G loss: 2.680445]\n",
      "epoch:7 step:6709 [D loss: 0.632291, acc: 64.06%] [G loss: 2.208583]\n",
      "epoch:7 step:6710 [D loss: 0.593354, acc: 66.41%] [G loss: 2.594235]\n",
      "epoch:7 step:6711 [D loss: 0.586291, acc: 71.09%] [G loss: 2.580498]\n",
      "epoch:7 step:6712 [D loss: 0.618073, acc: 66.41%] [G loss: 2.264356]\n",
      "epoch:7 step:6713 [D loss: 0.588765, acc: 66.41%] [G loss: 2.618615]\n",
      "epoch:7 step:6714 [D loss: 0.652857, acc: 67.97%] [G loss: 2.211007]\n",
      "epoch:7 step:6715 [D loss: 0.647724, acc: 63.28%] [G loss: 2.462537]\n",
      "epoch:7 step:6716 [D loss: 0.641647, acc: 64.06%] [G loss: 2.185322]\n",
      "epoch:7 step:6717 [D loss: 0.642592, acc: 63.28%] [G loss: 2.278278]\n",
      "epoch:7 step:6718 [D loss: 0.620443, acc: 64.06%] [G loss: 2.531336]\n",
      "epoch:7 step:6719 [D loss: 0.598478, acc: 67.97%] [G loss: 2.101554]\n",
      "epoch:7 step:6720 [D loss: 0.685988, acc: 61.72%] [G loss: 2.230665]\n",
      "epoch:7 step:6721 [D loss: 0.624448, acc: 67.97%] [G loss: 2.080088]\n",
      "epoch:7 step:6722 [D loss: 0.634754, acc: 61.72%] [G loss: 2.469922]\n",
      "epoch:7 step:6723 [D loss: 0.601755, acc: 70.31%] [G loss: 2.049260]\n",
      "epoch:7 step:6724 [D loss: 0.608223, acc: 65.62%] [G loss: 2.228368]\n",
      "epoch:7 step:6725 [D loss: 0.603963, acc: 68.75%] [G loss: 2.324465]\n",
      "epoch:7 step:6726 [D loss: 0.593033, acc: 67.97%] [G loss: 2.429588]\n",
      "epoch:7 step:6727 [D loss: 0.591284, acc: 69.53%] [G loss: 2.294166]\n",
      "epoch:7 step:6728 [D loss: 0.567718, acc: 73.44%] [G loss: 2.302091]\n",
      "epoch:7 step:6729 [D loss: 0.603760, acc: 62.50%] [G loss: 2.294434]\n",
      "epoch:7 step:6730 [D loss: 0.574589, acc: 64.84%] [G loss: 2.542753]\n",
      "epoch:7 step:6731 [D loss: 0.693623, acc: 57.03%] [G loss: 2.282247]\n",
      "epoch:7 step:6732 [D loss: 0.597781, acc: 66.41%] [G loss: 2.208646]\n",
      "epoch:7 step:6733 [D loss: 0.601402, acc: 68.75%] [G loss: 2.385380]\n",
      "epoch:7 step:6734 [D loss: 0.579866, acc: 69.53%] [G loss: 2.266266]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:7 step:6735 [D loss: 0.611595, acc: 67.19%] [G loss: 2.362296]\n",
      "epoch:7 step:6736 [D loss: 0.668144, acc: 55.47%] [G loss: 2.117266]\n",
      "epoch:7 step:6737 [D loss: 0.620848, acc: 62.50%] [G loss: 2.293865]\n",
      "epoch:7 step:6738 [D loss: 0.628770, acc: 66.41%] [G loss: 2.130141]\n",
      "epoch:7 step:6739 [D loss: 0.651928, acc: 65.62%] [G loss: 2.107240]\n",
      "epoch:7 step:6740 [D loss: 0.674283, acc: 58.59%] [G loss: 2.333747]\n",
      "epoch:7 step:6741 [D loss: 0.642975, acc: 65.62%] [G loss: 2.238830]\n",
      "epoch:7 step:6742 [D loss: 0.652108, acc: 64.84%] [G loss: 2.222026]\n",
      "epoch:7 step:6743 [D loss: 0.686163, acc: 57.03%] [G loss: 2.244233]\n",
      "epoch:7 step:6744 [D loss: 0.629742, acc: 62.50%] [G loss: 2.122841]\n",
      "epoch:7 step:6745 [D loss: 0.634569, acc: 62.50%] [G loss: 2.279596]\n",
      "epoch:7 step:6746 [D loss: 0.543112, acc: 72.66%] [G loss: 2.265507]\n",
      "epoch:7 step:6747 [D loss: 0.690164, acc: 60.94%] [G loss: 2.069675]\n",
      "epoch:7 step:6748 [D loss: 0.640225, acc: 64.84%] [G loss: 2.239564]\n",
      "epoch:7 step:6749 [D loss: 0.581275, acc: 71.88%] [G loss: 2.355141]\n",
      "epoch:7 step:6750 [D loss: 0.601716, acc: 70.31%] [G loss: 2.279868]\n",
      "epoch:7 step:6751 [D loss: 0.621691, acc: 65.62%] [G loss: 2.281849]\n",
      "epoch:7 step:6752 [D loss: 0.645152, acc: 64.84%] [G loss: 2.075216]\n",
      "epoch:7 step:6753 [D loss: 0.548510, acc: 74.22%] [G loss: 2.774792]\n",
      "epoch:7 step:6754 [D loss: 0.701772, acc: 60.94%] [G loss: 2.260911]\n",
      "epoch:7 step:6755 [D loss: 0.660839, acc: 63.28%] [G loss: 2.124283]\n",
      "epoch:7 step:6756 [D loss: 0.601548, acc: 67.19%] [G loss: 2.133040]\n",
      "epoch:7 step:6757 [D loss: 0.605712, acc: 65.62%] [G loss: 2.387122]\n",
      "epoch:7 step:6758 [D loss: 0.611632, acc: 65.62%] [G loss: 2.246761]\n",
      "epoch:7 step:6759 [D loss: 0.656151, acc: 64.84%] [G loss: 2.175436]\n",
      "epoch:7 step:6760 [D loss: 0.563997, acc: 71.09%] [G loss: 2.339685]\n",
      "epoch:7 step:6761 [D loss: 0.572389, acc: 71.09%] [G loss: 2.189911]\n",
      "epoch:7 step:6762 [D loss: 0.672116, acc: 57.81%] [G loss: 2.027772]\n",
      "epoch:7 step:6763 [D loss: 0.667836, acc: 60.94%] [G loss: 2.085140]\n",
      "epoch:7 step:6764 [D loss: 0.622360, acc: 63.28%] [G loss: 2.146920]\n",
      "epoch:7 step:6765 [D loss: 0.601763, acc: 67.19%] [G loss: 2.361236]\n",
      "epoch:7 step:6766 [D loss: 0.556656, acc: 67.97%] [G loss: 2.664572]\n",
      "epoch:7 step:6767 [D loss: 0.568144, acc: 70.31%] [G loss: 2.719042]\n",
      "epoch:7 step:6768 [D loss: 0.518719, acc: 71.88%] [G loss: 2.761936]\n",
      "epoch:7 step:6769 [D loss: 0.663916, acc: 59.38%] [G loss: 2.295106]\n",
      "epoch:7 step:6770 [D loss: 0.691339, acc: 61.72%] [G loss: 2.096708]\n",
      "epoch:7 step:6771 [D loss: 0.668849, acc: 63.28%] [G loss: 2.309376]\n",
      "epoch:7 step:6772 [D loss: 0.661684, acc: 58.59%] [G loss: 2.241786]\n",
      "epoch:7 step:6773 [D loss: 0.592121, acc: 68.75%] [G loss: 2.208505]\n",
      "epoch:7 step:6774 [D loss: 0.649907, acc: 64.84%] [G loss: 2.165746]\n",
      "epoch:7 step:6775 [D loss: 0.549993, acc: 71.09%] [G loss: 2.316744]\n",
      "epoch:7 step:6776 [D loss: 0.511064, acc: 75.78%] [G loss: 2.525552]\n",
      "epoch:7 step:6777 [D loss: 0.568145, acc: 68.75%] [G loss: 2.487703]\n",
      "epoch:7 step:6778 [D loss: 0.537041, acc: 71.88%] [G loss: 2.424684]\n",
      "epoch:7 step:6779 [D loss: 0.663928, acc: 64.84%] [G loss: 2.204234]\n",
      "epoch:7 step:6780 [D loss: 0.704324, acc: 57.03%] [G loss: 2.270496]\n",
      "epoch:7 step:6781 [D loss: 0.600624, acc: 71.09%] [G loss: 2.480806]\n",
      "epoch:7 step:6782 [D loss: 0.613988, acc: 70.31%] [G loss: 2.347966]\n",
      "epoch:7 step:6783 [D loss: 0.592832, acc: 65.62%] [G loss: 2.199251]\n",
      "epoch:7 step:6784 [D loss: 0.628245, acc: 66.41%] [G loss: 2.215129]\n",
      "epoch:7 step:6785 [D loss: 0.566279, acc: 70.31%] [G loss: 2.482631]\n",
      "epoch:7 step:6786 [D loss: 0.684738, acc: 57.03%] [G loss: 2.163457]\n",
      "epoch:7 step:6787 [D loss: 0.637905, acc: 63.28%] [G loss: 2.152813]\n",
      "epoch:7 step:6788 [D loss: 0.618161, acc: 68.75%] [G loss: 2.431301]\n",
      "epoch:7 step:6789 [D loss: 0.549900, acc: 74.22%] [G loss: 2.760617]\n",
      "epoch:7 step:6790 [D loss: 0.571626, acc: 65.62%] [G loss: 2.700986]\n",
      "epoch:7 step:6791 [D loss: 0.563121, acc: 71.88%] [G loss: 2.859350]\n",
      "epoch:7 step:6792 [D loss: 0.693711, acc: 61.72%] [G loss: 2.216259]\n",
      "epoch:7 step:6793 [D loss: 0.673142, acc: 65.62%] [G loss: 2.386165]\n",
      "epoch:7 step:6794 [D loss: 0.580148, acc: 68.75%] [G loss: 2.380218]\n",
      "epoch:7 step:6795 [D loss: 0.604914, acc: 69.53%] [G loss: 2.399899]\n",
      "epoch:7 step:6796 [D loss: 0.613802, acc: 66.41%] [G loss: 2.298496]\n",
      "epoch:7 step:6797 [D loss: 0.610934, acc: 64.06%] [G loss: 2.298510]\n",
      "epoch:7 step:6798 [D loss: 0.626627, acc: 60.94%] [G loss: 2.099929]\n",
      "epoch:7 step:6799 [D loss: 0.552666, acc: 71.09%] [G loss: 2.274377]\n",
      "epoch:7 step:6800 [D loss: 0.587192, acc: 64.84%] [G loss: 2.222316]\n",
      "##############\n",
      "[2.65633541 1.61681038 6.772489   5.13032545 3.92625605 5.89406918\n",
      " 4.90129678 4.98674464 5.14232529 3.64926895]\n",
      "##########\n",
      "epoch:7 step:6801 [D loss: 0.598751, acc: 68.75%] [G loss: 2.439490]\n",
      "epoch:7 step:6802 [D loss: 0.573102, acc: 71.09%] [G loss: 2.089183]\n",
      "epoch:7 step:6803 [D loss: 0.548779, acc: 73.44%] [G loss: 2.308385]\n",
      "epoch:7 step:6804 [D loss: 0.614099, acc: 61.72%] [G loss: 2.325755]\n",
      "epoch:7 step:6805 [D loss: 0.659733, acc: 60.94%] [G loss: 2.291150]\n",
      "epoch:7 step:6806 [D loss: 0.601552, acc: 66.41%] [G loss: 2.264850]\n",
      "epoch:7 step:6807 [D loss: 0.615339, acc: 67.19%] [G loss: 2.234163]\n",
      "epoch:7 step:6808 [D loss: 0.648058, acc: 67.97%] [G loss: 2.402220]\n",
      "epoch:7 step:6809 [D loss: 0.687585, acc: 56.25%] [G loss: 2.249473]\n",
      "epoch:7 step:6810 [D loss: 0.646658, acc: 60.16%] [G loss: 2.156953]\n",
      "epoch:7 step:6811 [D loss: 0.672026, acc: 63.28%] [G loss: 2.092842]\n",
      "epoch:7 step:6812 [D loss: 0.606916, acc: 64.84%] [G loss: 2.232031]\n",
      "epoch:7 step:6813 [D loss: 0.593310, acc: 67.97%] [G loss: 2.146429]\n",
      "epoch:7 step:6814 [D loss: 0.641536, acc: 67.97%] [G loss: 2.301610]\n",
      "epoch:7 step:6815 [D loss: 0.603114, acc: 66.41%] [G loss: 2.044543]\n",
      "epoch:7 step:6816 [D loss: 0.686827, acc: 60.16%] [G loss: 2.040354]\n",
      "epoch:7 step:6817 [D loss: 0.652653, acc: 66.41%] [G loss: 2.012346]\n",
      "epoch:7 step:6818 [D loss: 0.587947, acc: 70.31%] [G loss: 2.167285]\n",
      "epoch:7 step:6819 [D loss: 0.560258, acc: 69.53%] [G loss: 2.172312]\n",
      "epoch:7 step:6820 [D loss: 0.680486, acc: 59.38%] [G loss: 2.216026]\n",
      "epoch:7 step:6821 [D loss: 0.552003, acc: 71.09%] [G loss: 2.525890]\n",
      "epoch:7 step:6822 [D loss: 0.711233, acc: 60.16%] [G loss: 2.185827]\n",
      "epoch:7 step:6823 [D loss: 0.582818, acc: 68.75%] [G loss: 2.645648]\n",
      "epoch:7 step:6824 [D loss: 0.709571, acc: 54.69%] [G loss: 2.005975]\n",
      "epoch:7 step:6825 [D loss: 0.676108, acc: 64.06%] [G loss: 2.084906]\n",
      "epoch:7 step:6826 [D loss: 0.614774, acc: 64.06%] [G loss: 2.215728]\n",
      "epoch:7 step:6827 [D loss: 0.609676, acc: 64.06%] [G loss: 2.064897]\n",
      "epoch:7 step:6828 [D loss: 0.624849, acc: 63.28%] [G loss: 2.213466]\n",
      "epoch:7 step:6829 [D loss: 0.652686, acc: 64.06%] [G loss: 2.341760]\n",
      "epoch:7 step:6830 [D loss: 0.587670, acc: 67.97%] [G loss: 2.251112]\n",
      "epoch:7 step:6831 [D loss: 0.583736, acc: 71.88%] [G loss: 2.350252]\n",
      "epoch:7 step:6832 [D loss: 0.580503, acc: 64.84%] [G loss: 2.145451]\n",
      "epoch:7 step:6833 [D loss: 0.586235, acc: 71.88%] [G loss: 2.263132]\n",
      "epoch:7 step:6834 [D loss: 0.632135, acc: 63.28%] [G loss: 2.295151]\n",
      "epoch:7 step:6835 [D loss: 0.561581, acc: 73.44%] [G loss: 2.324064]\n",
      "epoch:7 step:6836 [D loss: 0.631029, acc: 64.84%] [G loss: 1.956407]\n",
      "epoch:7 step:6837 [D loss: 0.648999, acc: 60.94%] [G loss: 2.256463]\n",
      "epoch:7 step:6838 [D loss: 0.646341, acc: 64.84%] [G loss: 2.301040]\n",
      "epoch:7 step:6839 [D loss: 0.601001, acc: 64.84%] [G loss: 2.249792]\n",
      "epoch:7 step:6840 [D loss: 0.700913, acc: 59.38%] [G loss: 2.176070]\n",
      "epoch:7 step:6841 [D loss: 0.591086, acc: 71.09%] [G loss: 2.278389]\n",
      "epoch:7 step:6842 [D loss: 0.622702, acc: 67.19%] [G loss: 2.238096]\n",
      "epoch:7 step:6843 [D loss: 0.588292, acc: 69.53%] [G loss: 2.265838]\n",
      "epoch:7 step:6844 [D loss: 0.609262, acc: 64.06%] [G loss: 2.159652]\n",
      "epoch:7 step:6845 [D loss: 0.565192, acc: 67.97%] [G loss: 2.366287]\n",
      "epoch:7 step:6846 [D loss: 0.598908, acc: 71.09%] [G loss: 2.210017]\n",
      "epoch:7 step:6847 [D loss: 0.605198, acc: 63.28%] [G loss: 2.065742]\n",
      "epoch:7 step:6848 [D loss: 0.699187, acc: 57.81%] [G loss: 2.030736]\n",
      "epoch:7 step:6849 [D loss: 0.561020, acc: 72.66%] [G loss: 2.405167]\n",
      "epoch:7 step:6850 [D loss: 0.652412, acc: 64.84%] [G loss: 2.309882]\n",
      "epoch:7 step:6851 [D loss: 0.641891, acc: 60.94%] [G loss: 2.152929]\n",
      "epoch:7 step:6852 [D loss: 0.540973, acc: 73.44%] [G loss: 2.163387]\n",
      "epoch:7 step:6853 [D loss: 0.666150, acc: 59.38%] [G loss: 2.143674]\n",
      "epoch:7 step:6854 [D loss: 0.624547, acc: 61.72%] [G loss: 2.197357]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:7 step:6855 [D loss: 0.556108, acc: 71.88%] [G loss: 2.221615]\n",
      "epoch:7 step:6856 [D loss: 0.618977, acc: 59.38%] [G loss: 2.225953]\n",
      "epoch:7 step:6857 [D loss: 0.558466, acc: 67.19%] [G loss: 2.559355]\n",
      "epoch:7 step:6858 [D loss: 0.579502, acc: 68.75%] [G loss: 2.538532]\n",
      "epoch:7 step:6859 [D loss: 0.635132, acc: 66.41%] [G loss: 2.396977]\n",
      "epoch:7 step:6860 [D loss: 0.633521, acc: 60.16%] [G loss: 2.096776]\n",
      "epoch:7 step:6861 [D loss: 0.577729, acc: 71.88%] [G loss: 2.270878]\n",
      "epoch:7 step:6862 [D loss: 0.555752, acc: 73.44%] [G loss: 2.319221]\n",
      "epoch:7 step:6863 [D loss: 0.603398, acc: 71.88%] [G loss: 2.216424]\n",
      "epoch:7 step:6864 [D loss: 0.635702, acc: 62.50%] [G loss: 2.140457]\n",
      "epoch:7 step:6865 [D loss: 0.655571, acc: 64.06%] [G loss: 2.198184]\n",
      "epoch:7 step:6866 [D loss: 0.577864, acc: 75.00%] [G loss: 2.389148]\n",
      "epoch:7 step:6867 [D loss: 0.648527, acc: 66.41%] [G loss: 2.291275]\n",
      "epoch:7 step:6868 [D loss: 0.558891, acc: 69.53%] [G loss: 2.475059]\n",
      "epoch:7 step:6869 [D loss: 0.619509, acc: 68.75%] [G loss: 2.289191]\n",
      "epoch:7 step:6870 [D loss: 0.617689, acc: 65.62%] [G loss: 2.346826]\n",
      "epoch:7 step:6871 [D loss: 0.489038, acc: 80.47%] [G loss: 2.717991]\n",
      "epoch:7 step:6872 [D loss: 0.514999, acc: 75.00%] [G loss: 2.798819]\n",
      "epoch:7 step:6873 [D loss: 0.496611, acc: 78.91%] [G loss: 2.717465]\n",
      "epoch:7 step:6874 [D loss: 0.483442, acc: 80.47%] [G loss: 2.944017]\n",
      "epoch:7 step:6875 [D loss: 0.664798, acc: 60.16%] [G loss: 2.151824]\n",
      "epoch:7 step:6876 [D loss: 0.645866, acc: 64.06%] [G loss: 2.376130]\n",
      "epoch:7 step:6877 [D loss: 0.603841, acc: 65.62%] [G loss: 2.395816]\n",
      "epoch:7 step:6878 [D loss: 0.601968, acc: 68.75%] [G loss: 2.296830]\n",
      "epoch:7 step:6879 [D loss: 0.568874, acc: 75.78%] [G loss: 2.289571]\n",
      "epoch:7 step:6880 [D loss: 0.542886, acc: 71.88%] [G loss: 2.456112]\n",
      "epoch:7 step:6881 [D loss: 0.591607, acc: 65.62%] [G loss: 2.432956]\n",
      "epoch:7 step:6882 [D loss: 0.674115, acc: 64.06%] [G loss: 2.345175]\n",
      "epoch:7 step:6883 [D loss: 0.671035, acc: 58.59%] [G loss: 2.150420]\n",
      "epoch:7 step:6884 [D loss: 0.612414, acc: 70.31%] [G loss: 2.236121]\n",
      "epoch:7 step:6885 [D loss: 0.629014, acc: 61.72%] [G loss: 2.070524]\n",
      "epoch:7 step:6886 [D loss: 0.587094, acc: 68.75%] [G loss: 2.457952]\n",
      "epoch:7 step:6887 [D loss: 0.626355, acc: 63.28%] [G loss: 2.158546]\n",
      "epoch:7 step:6888 [D loss: 0.625996, acc: 66.41%] [G loss: 2.258184]\n",
      "epoch:7 step:6889 [D loss: 0.647536, acc: 63.28%] [G loss: 2.215810]\n",
      "epoch:7 step:6890 [D loss: 0.551797, acc: 70.31%] [G loss: 2.475616]\n",
      "epoch:7 step:6891 [D loss: 0.543087, acc: 69.53%] [G loss: 2.516037]\n",
      "epoch:7 step:6892 [D loss: 0.601549, acc: 71.09%] [G loss: 2.627121]\n",
      "epoch:7 step:6893 [D loss: 0.666347, acc: 62.50%] [G loss: 2.452797]\n",
      "epoch:7 step:6894 [D loss: 0.646566, acc: 67.19%] [G loss: 2.485607]\n",
      "epoch:7 step:6895 [D loss: 0.578162, acc: 72.66%] [G loss: 2.339477]\n",
      "epoch:7 step:6896 [D loss: 0.679580, acc: 66.41%] [G loss: 2.617469]\n",
      "epoch:7 step:6897 [D loss: 0.592025, acc: 67.19%] [G loss: 2.353665]\n",
      "epoch:7 step:6898 [D loss: 0.645718, acc: 64.06%] [G loss: 2.414068]\n",
      "epoch:7 step:6899 [D loss: 0.545990, acc: 73.44%] [G loss: 2.430282]\n",
      "epoch:7 step:6900 [D loss: 0.628855, acc: 69.53%] [G loss: 2.039605]\n",
      "epoch:7 step:6901 [D loss: 0.693965, acc: 54.69%] [G loss: 2.023753]\n",
      "epoch:7 step:6902 [D loss: 0.639064, acc: 66.41%] [G loss: 2.324322]\n",
      "epoch:7 step:6903 [D loss: 0.636128, acc: 63.28%] [G loss: 2.301599]\n",
      "epoch:7 step:6904 [D loss: 0.509384, acc: 72.66%] [G loss: 2.589338]\n",
      "epoch:7 step:6905 [D loss: 0.500574, acc: 75.78%] [G loss: 2.849128]\n",
      "epoch:7 step:6906 [D loss: 0.482258, acc: 82.03%] [G loss: 2.990837]\n",
      "epoch:7 step:6907 [D loss: 0.710991, acc: 58.59%] [G loss: 2.359085]\n",
      "epoch:7 step:6908 [D loss: 0.694424, acc: 61.72%] [G loss: 2.433469]\n",
      "epoch:7 step:6909 [D loss: 0.617591, acc: 71.09%] [G loss: 2.402651]\n",
      "epoch:7 step:6910 [D loss: 0.547496, acc: 72.66%] [G loss: 2.523733]\n",
      "epoch:7 step:6911 [D loss: 0.614284, acc: 69.53%] [G loss: 2.251318]\n",
      "epoch:7 step:6912 [D loss: 0.542617, acc: 73.44%] [G loss: 2.495819]\n",
      "epoch:7 step:6913 [D loss: 0.663702, acc: 65.62%] [G loss: 2.544723]\n",
      "epoch:7 step:6914 [D loss: 0.633487, acc: 65.62%] [G loss: 2.274744]\n",
      "epoch:7 step:6915 [D loss: 0.633054, acc: 64.06%] [G loss: 2.142724]\n",
      "epoch:7 step:6916 [D loss: 0.525189, acc: 74.22%] [G loss: 2.405242]\n",
      "epoch:7 step:6917 [D loss: 0.578676, acc: 72.66%] [G loss: 2.758843]\n",
      "epoch:7 step:6918 [D loss: 0.562213, acc: 68.75%] [G loss: 2.496812]\n",
      "epoch:7 step:6919 [D loss: 0.571494, acc: 72.66%] [G loss: 2.486243]\n",
      "epoch:7 step:6920 [D loss: 0.598063, acc: 67.97%] [G loss: 2.187317]\n",
      "epoch:7 step:6921 [D loss: 0.658526, acc: 67.19%] [G loss: 2.097505]\n",
      "epoch:7 step:6922 [D loss: 0.546254, acc: 71.09%] [G loss: 2.606763]\n",
      "epoch:7 step:6923 [D loss: 0.600944, acc: 66.41%] [G loss: 2.280992]\n",
      "epoch:7 step:6924 [D loss: 0.578866, acc: 71.09%] [G loss: 2.418784]\n",
      "epoch:7 step:6925 [D loss: 0.569476, acc: 69.53%] [G loss: 2.403073]\n",
      "epoch:7 step:6926 [D loss: 0.533370, acc: 67.19%] [G loss: 2.548687]\n",
      "epoch:7 step:6927 [D loss: 0.624488, acc: 66.41%] [G loss: 2.442829]\n",
      "epoch:7 step:6928 [D loss: 0.681969, acc: 57.81%] [G loss: 2.650548]\n",
      "epoch:7 step:6929 [D loss: 0.599264, acc: 65.62%] [G loss: 2.469720]\n",
      "epoch:7 step:6930 [D loss: 0.573952, acc: 67.97%] [G loss: 2.681314]\n",
      "epoch:7 step:6931 [D loss: 0.584257, acc: 64.84%] [G loss: 2.389699]\n",
      "epoch:7 step:6932 [D loss: 0.735139, acc: 57.03%] [G loss: 2.147544]\n",
      "epoch:7 step:6933 [D loss: 0.497885, acc: 75.78%] [G loss: 2.676742]\n",
      "epoch:7 step:6934 [D loss: 0.610676, acc: 66.41%] [G loss: 2.411114]\n",
      "epoch:7 step:6935 [D loss: 0.639288, acc: 67.19%] [G loss: 2.447260]\n",
      "epoch:7 step:6936 [D loss: 0.643962, acc: 60.94%] [G loss: 2.122016]\n",
      "epoch:7 step:6937 [D loss: 0.599744, acc: 69.53%] [G loss: 2.343579]\n",
      "epoch:7 step:6938 [D loss: 0.601632, acc: 64.06%] [G loss: 2.445853]\n",
      "epoch:7 step:6939 [D loss: 0.611270, acc: 67.19%] [G loss: 2.502486]\n",
      "epoch:7 step:6940 [D loss: 0.593220, acc: 69.53%] [G loss: 2.598193]\n",
      "epoch:7 step:6941 [D loss: 0.574148, acc: 67.19%] [G loss: 2.471607]\n",
      "epoch:7 step:6942 [D loss: 0.633976, acc: 62.50%] [G loss: 2.215494]\n",
      "epoch:7 step:6943 [D loss: 0.589648, acc: 71.88%] [G loss: 2.355853]\n",
      "epoch:7 step:6944 [D loss: 0.619029, acc: 60.16%] [G loss: 2.461831]\n",
      "epoch:7 step:6945 [D loss: 0.672414, acc: 59.38%] [G loss: 2.179981]\n",
      "epoch:7 step:6946 [D loss: 0.713066, acc: 57.03%] [G loss: 2.106480]\n",
      "epoch:7 step:6947 [D loss: 0.627267, acc: 69.53%] [G loss: 2.209582]\n",
      "epoch:7 step:6948 [D loss: 0.569277, acc: 71.09%] [G loss: 2.159666]\n",
      "epoch:7 step:6949 [D loss: 0.639466, acc: 64.84%] [G loss: 2.289609]\n",
      "epoch:7 step:6950 [D loss: 0.666696, acc: 64.06%] [G loss: 2.214199]\n",
      "epoch:7 step:6951 [D loss: 0.588507, acc: 66.41%] [G loss: 2.381868]\n",
      "epoch:7 step:6952 [D loss: 0.603624, acc: 65.62%] [G loss: 2.523639]\n",
      "epoch:7 step:6953 [D loss: 0.624307, acc: 64.84%] [G loss: 2.408138]\n",
      "epoch:7 step:6954 [D loss: 0.592748, acc: 64.06%] [G loss: 2.196789]\n",
      "epoch:7 step:6955 [D loss: 0.669528, acc: 59.38%] [G loss: 2.241204]\n",
      "epoch:7 step:6956 [D loss: 0.653926, acc: 64.84%] [G loss: 2.217049]\n",
      "epoch:7 step:6957 [D loss: 0.624123, acc: 61.72%] [G loss: 2.460473]\n",
      "epoch:7 step:6958 [D loss: 0.598863, acc: 71.09%] [G loss: 2.303729]\n",
      "epoch:7 step:6959 [D loss: 0.584393, acc: 70.31%] [G loss: 2.212134]\n",
      "epoch:7 step:6960 [D loss: 0.631949, acc: 64.06%] [G loss: 2.389702]\n",
      "epoch:7 step:6961 [D loss: 0.556973, acc: 74.22%] [G loss: 2.381975]\n",
      "epoch:7 step:6962 [D loss: 0.634726, acc: 64.06%] [G loss: 2.440174]\n",
      "epoch:7 step:6963 [D loss: 0.586311, acc: 67.97%] [G loss: 2.428231]\n",
      "epoch:7 step:6964 [D loss: 0.585129, acc: 70.31%] [G loss: 2.624770]\n",
      "epoch:7 step:6965 [D loss: 0.596832, acc: 75.00%] [G loss: 2.619621]\n",
      "epoch:7 step:6966 [D loss: 0.602335, acc: 65.62%] [G loss: 2.405766]\n",
      "epoch:7 step:6967 [D loss: 0.693936, acc: 58.59%] [G loss: 2.221688]\n",
      "epoch:7 step:6968 [D loss: 0.597719, acc: 70.31%] [G loss: 2.337799]\n",
      "epoch:7 step:6969 [D loss: 0.668353, acc: 63.28%] [G loss: 2.141873]\n",
      "epoch:7 step:6970 [D loss: 0.593836, acc: 65.62%] [G loss: 2.465069]\n",
      "epoch:7 step:6971 [D loss: 0.613050, acc: 64.84%] [G loss: 2.255410]\n",
      "epoch:7 step:6972 [D loss: 0.659745, acc: 65.62%] [G loss: 2.367369]\n",
      "epoch:7 step:6973 [D loss: 0.647248, acc: 59.38%] [G loss: 2.312904]\n",
      "epoch:7 step:6974 [D loss: 0.656050, acc: 58.59%] [G loss: 2.429085]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:7 step:6975 [D loss: 0.572039, acc: 68.75%] [G loss: 2.494054]\n",
      "epoch:7 step:6976 [D loss: 0.696791, acc: 61.72%] [G loss: 2.276700]\n",
      "epoch:7 step:6977 [D loss: 0.643561, acc: 64.06%] [G loss: 2.180119]\n",
      "epoch:7 step:6978 [D loss: 0.637955, acc: 69.53%] [G loss: 2.253923]\n",
      "epoch:7 step:6979 [D loss: 0.647467, acc: 59.38%] [G loss: 2.098020]\n",
      "epoch:7 step:6980 [D loss: 0.674598, acc: 62.50%] [G loss: 2.123863]\n",
      "epoch:7 step:6981 [D loss: 0.599193, acc: 69.53%] [G loss: 2.147905]\n",
      "epoch:7 step:6982 [D loss: 0.618864, acc: 63.28%] [G loss: 2.124881]\n",
      "epoch:7 step:6983 [D loss: 0.627771, acc: 65.62%] [G loss: 2.198745]\n",
      "epoch:7 step:6984 [D loss: 0.602166, acc: 67.97%] [G loss: 2.199621]\n",
      "epoch:7 step:6985 [D loss: 0.644703, acc: 64.06%] [G loss: 2.110747]\n",
      "epoch:7 step:6986 [D loss: 0.570799, acc: 67.97%] [G loss: 2.485376]\n",
      "epoch:7 step:6987 [D loss: 0.524528, acc: 76.56%] [G loss: 2.762783]\n",
      "epoch:7 step:6988 [D loss: 0.583499, acc: 74.22%] [G loss: 2.606309]\n",
      "epoch:7 step:6989 [D loss: 0.553730, acc: 71.09%] [G loss: 2.720417]\n",
      "epoch:7 step:6990 [D loss: 0.635348, acc: 59.38%] [G loss: 2.359126]\n",
      "epoch:7 step:6991 [D loss: 0.699662, acc: 60.94%] [G loss: 2.053393]\n",
      "epoch:7 step:6992 [D loss: 0.615123, acc: 63.28%] [G loss: 2.586626]\n",
      "epoch:7 step:6993 [D loss: 0.597778, acc: 71.09%] [G loss: 2.648635]\n",
      "epoch:7 step:6994 [D loss: 0.566819, acc: 67.97%] [G loss: 2.426633]\n",
      "epoch:7 step:6995 [D loss: 0.615203, acc: 72.66%] [G loss: 2.340858]\n",
      "epoch:7 step:6996 [D loss: 0.767744, acc: 55.47%] [G loss: 2.013861]\n",
      "epoch:7 step:6997 [D loss: 0.670452, acc: 59.38%] [G loss: 2.087346]\n",
      "epoch:7 step:6998 [D loss: 0.580599, acc: 68.75%] [G loss: 2.121916]\n",
      "epoch:7 step:6999 [D loss: 0.708088, acc: 58.59%] [G loss: 2.228443]\n",
      "epoch:7 step:7000 [D loss: 0.672524, acc: 65.62%] [G loss: 2.076414]\n",
      "##############\n",
      "[2.53702635 1.34041816 6.4834699  4.98336898 3.95139289 5.78521817\n",
      " 4.69897963 4.81818936 5.15158472 3.56897879]\n",
      "##########\n",
      "epoch:7 step:7001 [D loss: 0.608357, acc: 66.41%] [G loss: 2.127204]\n",
      "epoch:7 step:7002 [D loss: 0.601110, acc: 70.31%] [G loss: 2.096303]\n",
      "epoch:7 step:7003 [D loss: 0.605571, acc: 67.97%] [G loss: 1.993866]\n",
      "epoch:7 step:7004 [D loss: 0.599056, acc: 70.31%] [G loss: 2.282214]\n",
      "epoch:7 step:7005 [D loss: 0.588734, acc: 67.19%] [G loss: 2.090587]\n",
      "epoch:7 step:7006 [D loss: 0.612755, acc: 64.84%] [G loss: 2.204818]\n",
      "epoch:7 step:7007 [D loss: 0.607418, acc: 61.72%] [G loss: 2.137089]\n",
      "epoch:7 step:7008 [D loss: 0.641694, acc: 66.41%] [G loss: 2.370745]\n",
      "epoch:7 step:7009 [D loss: 0.617969, acc: 64.06%] [G loss: 2.482972]\n",
      "epoch:7 step:7010 [D loss: 0.575890, acc: 72.66%] [G loss: 2.430702]\n",
      "epoch:7 step:7011 [D loss: 0.605599, acc: 69.53%] [G loss: 2.345517]\n",
      "epoch:7 step:7012 [D loss: 0.565709, acc: 69.53%] [G loss: 2.220172]\n",
      "epoch:7 step:7013 [D loss: 0.547195, acc: 77.34%] [G loss: 2.515066]\n",
      "epoch:7 step:7014 [D loss: 0.560017, acc: 71.09%] [G loss: 2.377422]\n",
      "epoch:7 step:7015 [D loss: 0.639662, acc: 61.72%] [G loss: 2.449512]\n",
      "epoch:7 step:7016 [D loss: 0.570391, acc: 73.44%] [G loss: 2.326886]\n",
      "epoch:7 step:7017 [D loss: 0.670705, acc: 61.72%] [G loss: 2.346349]\n",
      "epoch:7 step:7018 [D loss: 0.686188, acc: 55.47%] [G loss: 2.120892]\n",
      "epoch:7 step:7019 [D loss: 0.627895, acc: 65.62%] [G loss: 2.190317]\n",
      "epoch:7 step:7020 [D loss: 0.631820, acc: 66.41%] [G loss: 2.032183]\n",
      "epoch:7 step:7021 [D loss: 0.603450, acc: 66.41%] [G loss: 2.347671]\n",
      "epoch:7 step:7022 [D loss: 0.609049, acc: 66.41%] [G loss: 2.314122]\n",
      "epoch:7 step:7023 [D loss: 0.562409, acc: 71.88%] [G loss: 2.275836]\n",
      "epoch:7 step:7024 [D loss: 0.610436, acc: 63.28%] [G loss: 2.273254]\n",
      "epoch:7 step:7025 [D loss: 0.658794, acc: 64.06%] [G loss: 2.343350]\n",
      "epoch:7 step:7026 [D loss: 0.662384, acc: 67.19%] [G loss: 2.248050]\n",
      "epoch:7 step:7027 [D loss: 0.568373, acc: 68.75%] [G loss: 2.391294]\n",
      "epoch:7 step:7028 [D loss: 0.615561, acc: 71.09%] [G loss: 2.357021]\n",
      "epoch:7 step:7029 [D loss: 0.595814, acc: 68.75%] [G loss: 2.312600]\n",
      "epoch:7 step:7030 [D loss: 0.530438, acc: 75.00%] [G loss: 2.961029]\n",
      "epoch:7 step:7031 [D loss: 0.550841, acc: 71.09%] [G loss: 2.598069]\n",
      "epoch:7 step:7032 [D loss: 0.590669, acc: 67.97%] [G loss: 2.188144]\n",
      "epoch:7 step:7033 [D loss: 0.606871, acc: 64.84%] [G loss: 2.438538]\n",
      "epoch:7 step:7034 [D loss: 0.648076, acc: 60.94%] [G loss: 2.481701]\n",
      "epoch:7 step:7035 [D loss: 0.649229, acc: 63.28%] [G loss: 2.229914]\n",
      "epoch:7 step:7036 [D loss: 0.660116, acc: 62.50%] [G loss: 2.013286]\n",
      "epoch:7 step:7037 [D loss: 0.665898, acc: 54.69%] [G loss: 2.258594]\n",
      "epoch:7 step:7038 [D loss: 0.602494, acc: 66.41%] [G loss: 2.349523]\n",
      "epoch:7 step:7039 [D loss: 0.593674, acc: 71.09%] [G loss: 2.343907]\n",
      "epoch:7 step:7040 [D loss: 0.630697, acc: 68.75%] [G loss: 2.683879]\n",
      "epoch:7 step:7041 [D loss: 0.731917, acc: 58.59%] [G loss: 2.321991]\n",
      "epoch:7 step:7042 [D loss: 0.617681, acc: 66.41%] [G loss: 2.403190]\n",
      "epoch:7 step:7043 [D loss: 0.744497, acc: 57.03%] [G loss: 2.126895]\n",
      "epoch:7 step:7044 [D loss: 0.671245, acc: 66.41%] [G loss: 1.999169]\n",
      "epoch:7 step:7045 [D loss: 0.589111, acc: 70.31%] [G loss: 2.209656]\n",
      "epoch:7 step:7046 [D loss: 0.677993, acc: 59.38%] [G loss: 2.038019]\n",
      "epoch:7 step:7047 [D loss: 0.614792, acc: 67.19%] [G loss: 2.248219]\n",
      "epoch:7 step:7048 [D loss: 0.597677, acc: 71.09%] [G loss: 2.111309]\n",
      "epoch:7 step:7049 [D loss: 0.628408, acc: 64.06%] [G loss: 2.335198]\n",
      "epoch:7 step:7050 [D loss: 0.628750, acc: 69.53%] [G loss: 2.196814]\n",
      "epoch:7 step:7051 [D loss: 0.612779, acc: 66.41%] [G loss: 2.175588]\n",
      "epoch:7 step:7052 [D loss: 0.599134, acc: 69.53%] [G loss: 2.218550]\n",
      "epoch:7 step:7053 [D loss: 0.567361, acc: 71.88%] [G loss: 2.378099]\n",
      "epoch:7 step:7054 [D loss: 0.631952, acc: 61.72%] [G loss: 2.259398]\n",
      "epoch:7 step:7055 [D loss: 0.585674, acc: 70.31%] [G loss: 2.337003]\n",
      "epoch:7 step:7056 [D loss: 0.610975, acc: 66.41%] [G loss: 2.417876]\n",
      "epoch:7 step:7057 [D loss: 0.646884, acc: 68.75%] [G loss: 2.488583]\n",
      "epoch:7 step:7058 [D loss: 0.536351, acc: 71.88%] [G loss: 2.580507]\n",
      "epoch:7 step:7059 [D loss: 0.669431, acc: 57.81%] [G loss: 2.059592]\n",
      "epoch:7 step:7060 [D loss: 0.673506, acc: 63.28%] [G loss: 2.089023]\n",
      "epoch:7 step:7061 [D loss: 0.661667, acc: 60.94%] [G loss: 2.022786]\n",
      "epoch:7 step:7062 [D loss: 0.587403, acc: 71.09%] [G loss: 2.466954]\n",
      "epoch:7 step:7063 [D loss: 0.559146, acc: 66.41%] [G loss: 2.661879]\n",
      "epoch:7 step:7064 [D loss: 0.627613, acc: 67.97%] [G loss: 2.294039]\n",
      "epoch:7 step:7065 [D loss: 0.612548, acc: 64.84%] [G loss: 2.255099]\n",
      "epoch:7 step:7066 [D loss: 0.618439, acc: 64.84%] [G loss: 2.306106]\n",
      "epoch:7 step:7067 [D loss: 0.591442, acc: 71.88%] [G loss: 2.413174]\n",
      "epoch:7 step:7068 [D loss: 0.580326, acc: 68.75%] [G loss: 2.268266]\n",
      "epoch:7 step:7069 [D loss: 0.665017, acc: 60.16%] [G loss: 2.247284]\n",
      "epoch:7 step:7070 [D loss: 0.609637, acc: 69.53%] [G loss: 2.101664]\n",
      "epoch:7 step:7071 [D loss: 0.588665, acc: 67.19%] [G loss: 2.154012]\n",
      "epoch:7 step:7072 [D loss: 0.614194, acc: 67.97%] [G loss: 2.326665]\n",
      "epoch:7 step:7073 [D loss: 0.635558, acc: 59.38%] [G loss: 2.195139]\n",
      "epoch:7 step:7074 [D loss: 0.620303, acc: 64.84%] [G loss: 2.410914]\n",
      "epoch:7 step:7075 [D loss: 0.577913, acc: 68.75%] [G loss: 2.272768]\n",
      "epoch:7 step:7076 [D loss: 0.593227, acc: 67.97%] [G loss: 2.450293]\n",
      "epoch:7 step:7077 [D loss: 0.604758, acc: 72.66%] [G loss: 2.278829]\n",
      "epoch:7 step:7078 [D loss: 0.641574, acc: 66.41%] [G loss: 2.307564]\n",
      "epoch:7 step:7079 [D loss: 0.592601, acc: 65.62%] [G loss: 2.426808]\n",
      "epoch:7 step:7080 [D loss: 0.558674, acc: 71.88%] [G loss: 2.417620]\n",
      "epoch:7 step:7081 [D loss: 0.648516, acc: 64.06%] [G loss: 2.522967]\n",
      "epoch:7 step:7082 [D loss: 0.603392, acc: 71.09%] [G loss: 2.391189]\n",
      "epoch:7 step:7083 [D loss: 0.592208, acc: 67.97%] [G loss: 2.343707]\n",
      "epoch:7 step:7084 [D loss: 0.621364, acc: 64.06%] [G loss: 2.267135]\n",
      "epoch:7 step:7085 [D loss: 0.596348, acc: 71.09%] [G loss: 2.157821]\n",
      "epoch:7 step:7086 [D loss: 0.601415, acc: 68.75%] [G loss: 2.396506]\n",
      "epoch:7 step:7087 [D loss: 0.600057, acc: 64.84%] [G loss: 2.126631]\n",
      "epoch:7 step:7088 [D loss: 0.668336, acc: 62.50%] [G loss: 2.130558]\n",
      "epoch:7 step:7089 [D loss: 0.567487, acc: 71.88%] [G loss: 2.253693]\n",
      "epoch:7 step:7090 [D loss: 0.639296, acc: 60.94%] [G loss: 2.246192]\n",
      "epoch:7 step:7091 [D loss: 0.637352, acc: 64.84%] [G loss: 2.460293]\n",
      "epoch:7 step:7092 [D loss: 0.688539, acc: 60.16%] [G loss: 2.304245]\n",
      "epoch:7 step:7093 [D loss: 0.526246, acc: 73.44%] [G loss: 2.552835]\n",
      "epoch:7 step:7094 [D loss: 0.631887, acc: 64.84%] [G loss: 2.187909]\n",
      "epoch:7 step:7095 [D loss: 0.549422, acc: 71.88%] [G loss: 2.276271]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:7 step:7096 [D loss: 0.680947, acc: 64.06%] [G loss: 2.201691]\n",
      "epoch:7 step:7097 [D loss: 0.658308, acc: 60.16%] [G loss: 2.167765]\n",
      "epoch:7 step:7098 [D loss: 0.688067, acc: 62.50%] [G loss: 2.010469]\n",
      "epoch:7 step:7099 [D loss: 0.614511, acc: 67.97%] [G loss: 2.360063]\n",
      "epoch:7 step:7100 [D loss: 0.630791, acc: 70.31%] [G loss: 2.283828]\n",
      "epoch:7 step:7101 [D loss: 0.716672, acc: 52.34%] [G loss: 2.218084]\n",
      "epoch:7 step:7102 [D loss: 0.596042, acc: 71.09%] [G loss: 2.179047]\n",
      "epoch:7 step:7103 [D loss: 0.616478, acc: 64.84%] [G loss: 2.097496]\n",
      "epoch:7 step:7104 [D loss: 0.706050, acc: 58.59%] [G loss: 1.990523]\n",
      "epoch:7 step:7105 [D loss: 0.615189, acc: 68.75%] [G loss: 2.181121]\n",
      "epoch:7 step:7106 [D loss: 0.596365, acc: 76.56%] [G loss: 2.127601]\n",
      "epoch:7 step:7107 [D loss: 0.555863, acc: 73.44%] [G loss: 2.283299]\n",
      "epoch:7 step:7108 [D loss: 0.555799, acc: 70.31%] [G loss: 2.419319]\n",
      "epoch:7 step:7109 [D loss: 0.622010, acc: 64.06%] [G loss: 2.315265]\n",
      "epoch:7 step:7110 [D loss: 0.584407, acc: 75.78%] [G loss: 2.522220]\n",
      "epoch:7 step:7111 [D loss: 0.587346, acc: 67.97%] [G loss: 2.415758]\n",
      "epoch:7 step:7112 [D loss: 0.624168, acc: 63.28%] [G loss: 2.095552]\n",
      "epoch:7 step:7113 [D loss: 0.559081, acc: 70.31%] [G loss: 2.674345]\n",
      "epoch:7 step:7114 [D loss: 0.632125, acc: 63.28%] [G loss: 2.302221]\n",
      "epoch:7 step:7115 [D loss: 0.647950, acc: 61.72%] [G loss: 2.585396]\n",
      "epoch:7 step:7116 [D loss: 0.571248, acc: 75.00%] [G loss: 2.394228]\n",
      "epoch:7 step:7117 [D loss: 0.538665, acc: 72.66%] [G loss: 2.548056]\n",
      "epoch:7 step:7118 [D loss: 0.681366, acc: 60.16%] [G loss: 2.103130]\n",
      "epoch:7 step:7119 [D loss: 0.629556, acc: 63.28%] [G loss: 2.170572]\n",
      "epoch:7 step:7120 [D loss: 0.575710, acc: 72.66%] [G loss: 2.275447]\n",
      "epoch:7 step:7121 [D loss: 0.662037, acc: 62.50%] [G loss: 2.198808]\n",
      "epoch:7 step:7122 [D loss: 0.602054, acc: 67.19%] [G loss: 2.195885]\n",
      "epoch:7 step:7123 [D loss: 0.591166, acc: 73.44%] [G loss: 2.728880]\n",
      "epoch:7 step:7124 [D loss: 0.667988, acc: 62.50%] [G loss: 2.209002]\n",
      "epoch:7 step:7125 [D loss: 0.760405, acc: 52.34%] [G loss: 2.112833]\n",
      "epoch:7 step:7126 [D loss: 0.644755, acc: 60.94%] [G loss: 2.247072]\n",
      "epoch:7 step:7127 [D loss: 0.628193, acc: 67.97%] [G loss: 2.504122]\n",
      "epoch:7 step:7128 [D loss: 0.647947, acc: 64.84%] [G loss: 2.199508]\n",
      "epoch:7 step:7129 [D loss: 0.649414, acc: 63.28%] [G loss: 2.160470]\n",
      "epoch:7 step:7130 [D loss: 0.527921, acc: 79.69%] [G loss: 2.225081]\n",
      "epoch:7 step:7131 [D loss: 0.631488, acc: 59.38%] [G loss: 1.985839]\n",
      "epoch:7 step:7132 [D loss: 0.680146, acc: 56.25%] [G loss: 2.118977]\n",
      "epoch:7 step:7133 [D loss: 0.585750, acc: 73.44%] [G loss: 2.243606]\n",
      "epoch:7 step:7134 [D loss: 0.614330, acc: 63.28%] [G loss: 2.322011]\n",
      "epoch:7 step:7135 [D loss: 0.617744, acc: 67.19%] [G loss: 1.954661]\n",
      "epoch:7 step:7136 [D loss: 0.595205, acc: 70.31%] [G loss: 2.092979]\n",
      "epoch:7 step:7137 [D loss: 0.643277, acc: 65.62%] [G loss: 2.031038]\n",
      "epoch:7 step:7138 [D loss: 0.648177, acc: 61.72%] [G loss: 2.149177]\n",
      "epoch:7 step:7139 [D loss: 0.641989, acc: 67.97%] [G loss: 2.066854]\n",
      "epoch:7 step:7140 [D loss: 0.588017, acc: 67.19%] [G loss: 2.155299]\n",
      "epoch:7 step:7141 [D loss: 0.570890, acc: 71.88%] [G loss: 2.309088]\n",
      "epoch:7 step:7142 [D loss: 0.642813, acc: 65.62%] [G loss: 2.205610]\n",
      "epoch:7 step:7143 [D loss: 0.610036, acc: 66.41%] [G loss: 2.262451]\n",
      "epoch:7 step:7144 [D loss: 0.654906, acc: 64.06%] [G loss: 2.062864]\n",
      "epoch:7 step:7145 [D loss: 0.646758, acc: 64.84%] [G loss: 2.192649]\n",
      "epoch:7 step:7146 [D loss: 0.576831, acc: 71.09%] [G loss: 2.193927]\n",
      "epoch:7 step:7147 [D loss: 0.592628, acc: 71.09%] [G loss: 2.162578]\n",
      "epoch:7 step:7148 [D loss: 0.624506, acc: 67.97%] [G loss: 2.456182]\n",
      "epoch:7 step:7149 [D loss: 0.646780, acc: 63.28%] [G loss: 2.257181]\n",
      "epoch:7 step:7150 [D loss: 0.625564, acc: 64.84%] [G loss: 2.030836]\n",
      "epoch:7 step:7151 [D loss: 0.543557, acc: 76.56%] [G loss: 2.379822]\n",
      "epoch:7 step:7152 [D loss: 0.746370, acc: 53.12%] [G loss: 2.232092]\n",
      "epoch:7 step:7153 [D loss: 0.655263, acc: 62.50%] [G loss: 2.080026]\n",
      "epoch:7 step:7154 [D loss: 0.526876, acc: 80.47%] [G loss: 2.206874]\n",
      "epoch:7 step:7155 [D loss: 0.583264, acc: 70.31%] [G loss: 2.094810]\n",
      "epoch:7 step:7156 [D loss: 0.605791, acc: 64.84%] [G loss: 2.105600]\n",
      "epoch:7 step:7157 [D loss: 0.580169, acc: 67.19%] [G loss: 2.287617]\n",
      "epoch:7 step:7158 [D loss: 0.626036, acc: 67.19%] [G loss: 2.199585]\n",
      "epoch:7 step:7159 [D loss: 0.562678, acc: 68.75%] [G loss: 2.272407]\n",
      "epoch:7 step:7160 [D loss: 0.647828, acc: 64.06%] [G loss: 2.261065]\n",
      "epoch:7 step:7161 [D loss: 0.608280, acc: 66.41%] [G loss: 2.320145]\n",
      "epoch:7 step:7162 [D loss: 0.612074, acc: 64.06%] [G loss: 2.372594]\n",
      "epoch:7 step:7163 [D loss: 0.619873, acc: 67.19%] [G loss: 2.050877]\n",
      "epoch:7 step:7164 [D loss: 0.558315, acc: 71.09%] [G loss: 2.562399]\n",
      "epoch:7 step:7165 [D loss: 0.630585, acc: 58.59%] [G loss: 2.318057]\n",
      "epoch:7 step:7166 [D loss: 0.662212, acc: 53.91%] [G loss: 2.313300]\n",
      "epoch:7 step:7167 [D loss: 0.708748, acc: 57.81%] [G loss: 2.413639]\n",
      "epoch:7 step:7168 [D loss: 0.579100, acc: 65.62%] [G loss: 2.579027]\n",
      "epoch:7 step:7169 [D loss: 0.643892, acc: 61.72%] [G loss: 2.087286]\n",
      "epoch:7 step:7170 [D loss: 0.681562, acc: 61.72%] [G loss: 2.299771]\n",
      "epoch:7 step:7171 [D loss: 0.645371, acc: 59.38%] [G loss: 2.431423]\n",
      "epoch:7 step:7172 [D loss: 0.539357, acc: 70.31%] [G loss: 2.400775]\n",
      "epoch:7 step:7173 [D loss: 0.569374, acc: 71.88%] [G loss: 2.187299]\n",
      "epoch:7 step:7174 [D loss: 0.658769, acc: 63.28%] [G loss: 2.181108]\n",
      "epoch:7 step:7175 [D loss: 0.566383, acc: 70.31%] [G loss: 2.348584]\n",
      "epoch:7 step:7176 [D loss: 0.590304, acc: 70.31%] [G loss: 2.140322]\n",
      "epoch:7 step:7177 [D loss: 0.648659, acc: 65.62%] [G loss: 2.101508]\n",
      "epoch:7 step:7178 [D loss: 0.637553, acc: 59.38%] [G loss: 2.236425]\n",
      "epoch:7 step:7179 [D loss: 0.599217, acc: 67.97%] [G loss: 2.318158]\n",
      "epoch:7 step:7180 [D loss: 0.671120, acc: 63.28%] [G loss: 2.088026]\n",
      "epoch:7 step:7181 [D loss: 0.638618, acc: 64.84%] [G loss: 2.128169]\n",
      "epoch:7 step:7182 [D loss: 0.569494, acc: 71.09%] [G loss: 2.253360]\n",
      "epoch:7 step:7183 [D loss: 0.595434, acc: 62.50%] [G loss: 2.317311]\n",
      "epoch:7 step:7184 [D loss: 0.638952, acc: 67.97%] [G loss: 2.198102]\n",
      "epoch:7 step:7185 [D loss: 0.563750, acc: 74.22%] [G loss: 2.425917]\n",
      "epoch:7 step:7186 [D loss: 0.641292, acc: 67.97%] [G loss: 2.117152]\n",
      "epoch:7 step:7187 [D loss: 0.632797, acc: 67.19%] [G loss: 2.202918]\n",
      "epoch:7 step:7188 [D loss: 0.587439, acc: 67.19%] [G loss: 2.437636]\n",
      "epoch:7 step:7189 [D loss: 0.631575, acc: 63.28%] [G loss: 2.165191]\n",
      "epoch:7 step:7190 [D loss: 0.508326, acc: 77.34%] [G loss: 2.477071]\n",
      "epoch:7 step:7191 [D loss: 0.580341, acc: 71.09%] [G loss: 2.119801]\n",
      "epoch:7 step:7192 [D loss: 0.596326, acc: 69.53%] [G loss: 2.266505]\n",
      "epoch:7 step:7193 [D loss: 0.576052, acc: 71.88%] [G loss: 2.602396]\n",
      "epoch:7 step:7194 [D loss: 0.546108, acc: 72.66%] [G loss: 2.284854]\n",
      "epoch:7 step:7195 [D loss: 0.615854, acc: 70.31%] [G loss: 2.430784]\n",
      "epoch:7 step:7196 [D loss: 0.632681, acc: 65.62%] [G loss: 2.515133]\n",
      "epoch:7 step:7197 [D loss: 0.649851, acc: 64.06%] [G loss: 2.135384]\n",
      "epoch:7 step:7198 [D loss: 0.582896, acc: 71.88%] [G loss: 2.494477]\n",
      "epoch:7 step:7199 [D loss: 0.650726, acc: 60.16%] [G loss: 2.168089]\n",
      "epoch:7 step:7200 [D loss: 0.661014, acc: 57.81%] [G loss: 2.385748]\n",
      "##############\n",
      "[2.6178812  1.24083145 6.49822148 5.15057605 4.07362664 5.59399162\n",
      " 4.69862287 4.92678213 4.98143202 3.76870337]\n",
      "##########\n",
      "epoch:7 step:7201 [D loss: 0.531353, acc: 72.66%] [G loss: 2.514072]\n",
      "epoch:7 step:7202 [D loss: 0.577918, acc: 70.31%] [G loss: 2.514240]\n",
      "epoch:7 step:7203 [D loss: 0.622940, acc: 66.41%] [G loss: 2.202447]\n",
      "epoch:7 step:7204 [D loss: 0.553288, acc: 75.00%] [G loss: 2.340581]\n",
      "epoch:7 step:7205 [D loss: 0.622677, acc: 67.97%] [G loss: 2.431818]\n",
      "epoch:7 step:7206 [D loss: 0.523263, acc: 71.09%] [G loss: 2.755118]\n",
      "epoch:7 step:7207 [D loss: 0.509204, acc: 73.44%] [G loss: 2.853389]\n",
      "epoch:7 step:7208 [D loss: 0.602933, acc: 67.19%] [G loss: 2.831781]\n",
      "epoch:7 step:7209 [D loss: 0.675590, acc: 61.72%] [G loss: 2.610295]\n",
      "epoch:7 step:7210 [D loss: 0.571861, acc: 67.19%] [G loss: 2.225994]\n",
      "epoch:7 step:7211 [D loss: 0.633070, acc: 67.19%] [G loss: 2.359766]\n",
      "epoch:7 step:7212 [D loss: 0.737287, acc: 53.91%] [G loss: 2.132962]\n",
      "epoch:7 step:7213 [D loss: 0.590970, acc: 68.75%] [G loss: 2.466209]\n",
      "epoch:7 step:7214 [D loss: 0.622682, acc: 63.28%] [G loss: 2.517350]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:7 step:7215 [D loss: 0.683824, acc: 62.50%] [G loss: 2.182797]\n",
      "epoch:7 step:7216 [D loss: 0.606654, acc: 66.41%] [G loss: 2.115808]\n",
      "epoch:7 step:7217 [D loss: 0.694271, acc: 55.47%] [G loss: 2.233636]\n",
      "epoch:7 step:7218 [D loss: 0.567601, acc: 67.97%] [G loss: 2.351991]\n",
      "epoch:7 step:7219 [D loss: 0.615733, acc: 65.62%] [G loss: 2.197601]\n",
      "epoch:7 step:7220 [D loss: 0.633913, acc: 67.19%] [G loss: 2.130152]\n",
      "epoch:7 step:7221 [D loss: 0.597918, acc: 64.84%] [G loss: 2.225460]\n",
      "epoch:7 step:7222 [D loss: 0.599888, acc: 65.62%] [G loss: 2.102412]\n",
      "epoch:7 step:7223 [D loss: 0.632646, acc: 67.19%] [G loss: 2.235090]\n",
      "epoch:7 step:7224 [D loss: 0.656581, acc: 64.06%] [G loss: 2.129666]\n",
      "epoch:7 step:7225 [D loss: 0.624818, acc: 60.94%] [G loss: 2.113624]\n",
      "epoch:7 step:7226 [D loss: 0.608760, acc: 67.19%] [G loss: 1.997725]\n",
      "epoch:7 step:7227 [D loss: 0.694863, acc: 53.12%] [G loss: 2.351830]\n",
      "epoch:7 step:7228 [D loss: 0.604525, acc: 66.41%] [G loss: 2.102740]\n",
      "epoch:7 step:7229 [D loss: 0.696544, acc: 53.12%] [G loss: 2.330099]\n",
      "epoch:7 step:7230 [D loss: 0.620951, acc: 62.50%] [G loss: 2.175483]\n",
      "epoch:7 step:7231 [D loss: 0.647923, acc: 60.16%] [G loss: 2.194245]\n",
      "epoch:7 step:7232 [D loss: 0.634063, acc: 65.62%] [G loss: 2.001381]\n",
      "epoch:7 step:7233 [D loss: 0.599379, acc: 63.28%] [G loss: 2.186527]\n",
      "epoch:7 step:7234 [D loss: 0.622150, acc: 66.41%] [G loss: 2.281301]\n",
      "epoch:7 step:7235 [D loss: 0.624255, acc: 64.06%] [G loss: 2.284079]\n",
      "epoch:7 step:7236 [D loss: 0.603842, acc: 67.97%] [G loss: 2.345204]\n",
      "epoch:7 step:7237 [D loss: 0.611117, acc: 66.41%] [G loss: 2.333297]\n",
      "epoch:7 step:7238 [D loss: 0.568847, acc: 71.09%] [G loss: 2.258317]\n",
      "epoch:7 step:7239 [D loss: 0.559996, acc: 71.09%] [G loss: 2.248712]\n",
      "epoch:7 step:7240 [D loss: 0.621394, acc: 64.06%] [G loss: 2.258176]\n",
      "epoch:7 step:7241 [D loss: 0.660870, acc: 60.94%] [G loss: 2.252333]\n",
      "epoch:7 step:7242 [D loss: 0.548774, acc: 73.44%] [G loss: 2.300560]\n",
      "epoch:7 step:7243 [D loss: 0.603152, acc: 65.62%] [G loss: 2.300506]\n",
      "epoch:7 step:7244 [D loss: 0.615486, acc: 70.31%] [G loss: 2.211513]\n",
      "epoch:7 step:7245 [D loss: 0.578694, acc: 67.19%] [G loss: 2.205318]\n",
      "epoch:7 step:7246 [D loss: 0.519223, acc: 71.88%] [G loss: 2.494599]\n",
      "epoch:7 step:7247 [D loss: 0.662411, acc: 64.84%] [G loss: 2.348867]\n",
      "epoch:7 step:7248 [D loss: 0.658306, acc: 64.84%] [G loss: 2.329804]\n",
      "epoch:7 step:7249 [D loss: 0.660122, acc: 61.72%] [G loss: 2.495934]\n",
      "epoch:7 step:7250 [D loss: 0.599368, acc: 67.19%] [G loss: 2.737751]\n",
      "epoch:7 step:7251 [D loss: 0.559434, acc: 70.31%] [G loss: 2.655612]\n",
      "epoch:7 step:7252 [D loss: 0.559679, acc: 75.78%] [G loss: 2.648616]\n",
      "epoch:7 step:7253 [D loss: 0.553457, acc: 71.88%] [G loss: 2.631358]\n",
      "epoch:7 step:7254 [D loss: 0.607117, acc: 67.97%] [G loss: 2.314736]\n",
      "epoch:7 step:7255 [D loss: 0.670698, acc: 59.38%] [G loss: 2.342587]\n",
      "epoch:7 step:7256 [D loss: 0.703624, acc: 60.16%] [G loss: 2.068296]\n",
      "epoch:7 step:7257 [D loss: 0.606011, acc: 71.09%] [G loss: 2.314905]\n",
      "epoch:7 step:7258 [D loss: 0.619980, acc: 71.09%] [G loss: 2.366617]\n",
      "epoch:7 step:7259 [D loss: 0.622751, acc: 66.41%] [G loss: 2.264321]\n",
      "epoch:7 step:7260 [D loss: 0.631646, acc: 64.84%] [G loss: 2.101936]\n",
      "epoch:7 step:7261 [D loss: 0.617533, acc: 64.06%] [G loss: 1.917124]\n",
      "epoch:7 step:7262 [D loss: 0.673826, acc: 60.16%] [G loss: 2.169518]\n",
      "epoch:7 step:7263 [D loss: 0.689070, acc: 59.38%] [G loss: 1.917166]\n",
      "epoch:7 step:7264 [D loss: 0.628659, acc: 64.84%] [G loss: 2.102265]\n",
      "epoch:7 step:7265 [D loss: 0.583081, acc: 74.22%] [G loss: 2.452905]\n",
      "epoch:7 step:7266 [D loss: 0.577634, acc: 73.44%] [G loss: 2.331989]\n",
      "epoch:7 step:7267 [D loss: 0.543180, acc: 75.78%] [G loss: 2.499396]\n",
      "epoch:7 step:7268 [D loss: 0.563017, acc: 74.22%] [G loss: 2.446633]\n",
      "epoch:7 step:7269 [D loss: 0.637680, acc: 60.16%] [G loss: 2.246851]\n",
      "epoch:7 step:7270 [D loss: 0.657263, acc: 61.72%] [G loss: 2.154706]\n",
      "epoch:7 step:7271 [D loss: 0.586978, acc: 65.62%] [G loss: 2.280011]\n",
      "epoch:7 step:7272 [D loss: 0.625382, acc: 61.72%] [G loss: 2.261842]\n",
      "epoch:7 step:7273 [D loss: 0.597480, acc: 69.53%] [G loss: 2.426727]\n",
      "epoch:7 step:7274 [D loss: 0.641098, acc: 59.38%] [G loss: 2.144044]\n",
      "epoch:7 step:7275 [D loss: 0.644209, acc: 62.50%] [G loss: 2.253510]\n",
      "epoch:7 step:7276 [D loss: 0.630669, acc: 64.84%] [G loss: 2.075999]\n",
      "epoch:7 step:7277 [D loss: 0.630738, acc: 67.97%] [G loss: 2.519011]\n",
      "epoch:7 step:7278 [D loss: 0.584072, acc: 72.66%] [G loss: 2.297316]\n",
      "epoch:7 step:7279 [D loss: 0.631798, acc: 66.41%] [G loss: 2.120885]\n",
      "epoch:7 step:7280 [D loss: 0.639664, acc: 66.41%] [G loss: 2.104968]\n",
      "epoch:7 step:7281 [D loss: 0.663835, acc: 54.69%] [G loss: 2.278386]\n",
      "epoch:7 step:7282 [D loss: 0.669274, acc: 62.50%] [G loss: 2.162121]\n",
      "epoch:7 step:7283 [D loss: 0.571334, acc: 73.44%] [G loss: 2.309040]\n",
      "epoch:7 step:7284 [D loss: 0.578852, acc: 67.97%] [G loss: 2.334704]\n",
      "epoch:7 step:7285 [D loss: 0.553484, acc: 71.88%] [G loss: 2.272748]\n",
      "epoch:7 step:7286 [D loss: 0.670534, acc: 63.28%] [G loss: 2.006291]\n",
      "epoch:7 step:7287 [D loss: 0.664142, acc: 57.81%] [G loss: 2.393664]\n",
      "epoch:7 step:7288 [D loss: 0.632937, acc: 67.19%] [G loss: 2.155345]\n",
      "epoch:7 step:7289 [D loss: 0.586133, acc: 67.19%] [G loss: 2.038222]\n",
      "epoch:7 step:7290 [D loss: 0.651021, acc: 60.16%] [G loss: 2.055570]\n",
      "epoch:7 step:7291 [D loss: 0.526950, acc: 76.56%] [G loss: 2.247955]\n",
      "epoch:7 step:7292 [D loss: 0.638198, acc: 67.97%] [G loss: 2.334508]\n",
      "epoch:7 step:7293 [D loss: 0.612836, acc: 68.75%] [G loss: 2.196945]\n",
      "epoch:7 step:7294 [D loss: 0.647955, acc: 62.50%] [G loss: 2.382058]\n",
      "epoch:7 step:7295 [D loss: 0.602764, acc: 67.97%] [G loss: 2.252616]\n",
      "epoch:7 step:7296 [D loss: 0.653016, acc: 61.72%] [G loss: 2.127467]\n",
      "epoch:7 step:7297 [D loss: 0.656043, acc: 64.84%] [G loss: 2.163192]\n",
      "epoch:7 step:7298 [D loss: 0.697370, acc: 57.81%] [G loss: 2.002796]\n",
      "epoch:7 step:7299 [D loss: 0.592896, acc: 68.75%] [G loss: 2.344839]\n",
      "epoch:7 step:7300 [D loss: 0.642851, acc: 62.50%] [G loss: 2.029880]\n",
      "epoch:7 step:7301 [D loss: 0.665636, acc: 65.62%] [G loss: 1.937766]\n",
      "epoch:7 step:7302 [D loss: 0.641062, acc: 69.53%] [G loss: 2.269773]\n",
      "epoch:7 step:7303 [D loss: 0.698710, acc: 64.06%] [G loss: 2.347316]\n",
      "epoch:7 step:7304 [D loss: 0.629447, acc: 64.06%] [G loss: 2.093817]\n",
      "epoch:7 step:7305 [D loss: 0.663802, acc: 63.28%] [G loss: 1.934963]\n",
      "epoch:7 step:7306 [D loss: 0.631964, acc: 67.97%] [G loss: 2.263648]\n",
      "epoch:7 step:7307 [D loss: 0.652202, acc: 63.28%] [G loss: 2.191870]\n",
      "epoch:7 step:7308 [D loss: 0.649629, acc: 64.84%] [G loss: 2.025993]\n",
      "epoch:7 step:7309 [D loss: 0.598777, acc: 67.97%] [G loss: 2.043874]\n",
      "epoch:7 step:7310 [D loss: 0.607665, acc: 63.28%] [G loss: 2.068976]\n",
      "epoch:7 step:7311 [D loss: 0.610024, acc: 67.19%] [G loss: 2.255864]\n",
      "epoch:7 step:7312 [D loss: 0.649283, acc: 60.94%] [G loss: 2.117553]\n",
      "epoch:7 step:7313 [D loss: 0.593834, acc: 68.75%] [G loss: 2.325019]\n",
      "epoch:7 step:7314 [D loss: 0.660516, acc: 60.16%] [G loss: 2.131305]\n",
      "epoch:7 step:7315 [D loss: 0.625178, acc: 57.81%] [G loss: 2.121722]\n",
      "epoch:7 step:7316 [D loss: 0.582083, acc: 66.41%] [G loss: 2.294564]\n",
      "epoch:7 step:7317 [D loss: 0.647469, acc: 65.62%] [G loss: 2.033154]\n",
      "epoch:7 step:7318 [D loss: 0.600612, acc: 69.53%] [G loss: 2.141340]\n",
      "epoch:7 step:7319 [D loss: 0.658173, acc: 65.62%] [G loss: 2.129707]\n",
      "epoch:7 step:7320 [D loss: 0.657347, acc: 62.50%] [G loss: 2.223422]\n",
      "epoch:7 step:7321 [D loss: 0.625055, acc: 60.94%] [G loss: 1.906106]\n",
      "epoch:7 step:7322 [D loss: 0.620348, acc: 68.75%] [G loss: 1.952236]\n",
      "epoch:7 step:7323 [D loss: 0.581731, acc: 72.66%] [G loss: 2.189577]\n",
      "epoch:7 step:7324 [D loss: 0.651606, acc: 64.84%] [G loss: 2.035007]\n",
      "epoch:7 step:7325 [D loss: 0.716780, acc: 58.59%] [G loss: 2.201534]\n",
      "epoch:7 step:7326 [D loss: 0.578181, acc: 71.09%] [G loss: 2.173196]\n",
      "epoch:7 step:7327 [D loss: 0.685342, acc: 57.03%] [G loss: 2.090861]\n",
      "epoch:7 step:7328 [D loss: 0.558219, acc: 70.31%] [G loss: 2.284639]\n",
      "epoch:7 step:7329 [D loss: 0.617081, acc: 63.28%] [G loss: 2.100042]\n",
      "epoch:7 step:7330 [D loss: 0.695087, acc: 60.94%] [G loss: 2.110656]\n",
      "epoch:7 step:7331 [D loss: 0.605380, acc: 65.62%] [G loss: 2.185040]\n",
      "epoch:7 step:7332 [D loss: 0.604322, acc: 67.97%] [G loss: 2.051553]\n",
      "epoch:7 step:7333 [D loss: 0.645400, acc: 64.84%] [G loss: 2.394241]\n",
      "epoch:7 step:7334 [D loss: 0.604384, acc: 69.53%] [G loss: 2.418344]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:7 step:7335 [D loss: 0.579449, acc: 64.06%] [G loss: 2.281977]\n",
      "epoch:7 step:7336 [D loss: 0.626480, acc: 61.72%] [G loss: 2.205416]\n",
      "epoch:7 step:7337 [D loss: 0.655395, acc: 62.50%] [G loss: 2.039761]\n",
      "epoch:7 step:7338 [D loss: 0.600195, acc: 67.19%] [G loss: 2.255058]\n",
      "epoch:7 step:7339 [D loss: 0.580104, acc: 67.19%] [G loss: 2.263062]\n",
      "epoch:7 step:7340 [D loss: 0.587321, acc: 64.84%] [G loss: 2.636679]\n",
      "epoch:7 step:7341 [D loss: 0.630843, acc: 58.59%] [G loss: 2.351438]\n",
      "epoch:7 step:7342 [D loss: 0.715336, acc: 57.03%] [G loss: 2.328791]\n",
      "epoch:7 step:7343 [D loss: 0.654928, acc: 62.50%] [G loss: 2.119453]\n",
      "epoch:7 step:7344 [D loss: 0.659644, acc: 60.94%] [G loss: 2.087853]\n",
      "epoch:7 step:7345 [D loss: 0.555761, acc: 72.66%] [G loss: 2.430150]\n",
      "epoch:7 step:7346 [D loss: 0.626420, acc: 68.75%] [G loss: 2.095423]\n",
      "epoch:7 step:7347 [D loss: 0.668131, acc: 57.81%] [G loss: 2.130735]\n",
      "epoch:7 step:7348 [D loss: 0.626968, acc: 67.97%] [G loss: 2.113309]\n",
      "epoch:7 step:7349 [D loss: 0.590240, acc: 71.09%] [G loss: 2.121926]\n",
      "epoch:7 step:7350 [D loss: 0.605463, acc: 66.41%] [G loss: 2.427731]\n",
      "epoch:7 step:7351 [D loss: 0.559625, acc: 69.53%] [G loss: 2.675724]\n",
      "epoch:7 step:7352 [D loss: 0.543525, acc: 73.44%] [G loss: 2.180272]\n",
      "epoch:7 step:7353 [D loss: 0.674738, acc: 57.81%] [G loss: 1.887442]\n",
      "epoch:7 step:7354 [D loss: 0.630546, acc: 62.50%] [G loss: 2.238340]\n",
      "epoch:7 step:7355 [D loss: 0.634030, acc: 62.50%] [G loss: 2.310305]\n",
      "epoch:7 step:7356 [D loss: 0.605305, acc: 69.53%] [G loss: 2.061687]\n",
      "epoch:7 step:7357 [D loss: 0.576935, acc: 71.09%] [G loss: 2.300001]\n",
      "epoch:7 step:7358 [D loss: 0.610177, acc: 63.28%] [G loss: 2.207453]\n",
      "epoch:7 step:7359 [D loss: 0.623065, acc: 69.53%] [G loss: 2.046392]\n",
      "epoch:7 step:7360 [D loss: 0.640267, acc: 59.38%] [G loss: 2.094953]\n",
      "epoch:7 step:7361 [D loss: 0.605361, acc: 71.88%] [G loss: 2.204047]\n",
      "epoch:7 step:7362 [D loss: 0.543038, acc: 73.44%] [G loss: 2.396209]\n",
      "epoch:7 step:7363 [D loss: 0.631804, acc: 67.97%] [G loss: 2.147712]\n",
      "epoch:7 step:7364 [D loss: 0.598315, acc: 67.19%] [G loss: 2.449038]\n",
      "epoch:7 step:7365 [D loss: 0.576581, acc: 69.53%] [G loss: 2.560536]\n",
      "epoch:7 step:7366 [D loss: 0.587335, acc: 71.09%] [G loss: 2.290467]\n",
      "epoch:7 step:7367 [D loss: 0.598954, acc: 64.06%] [G loss: 2.332982]\n",
      "epoch:7 step:7368 [D loss: 0.605180, acc: 64.06%] [G loss: 2.402437]\n",
      "epoch:7 step:7369 [D loss: 0.531063, acc: 76.56%] [G loss: 2.348410]\n",
      "epoch:7 step:7370 [D loss: 0.666372, acc: 65.62%] [G loss: 2.293601]\n",
      "epoch:7 step:7371 [D loss: 0.626864, acc: 65.62%] [G loss: 2.094515]\n",
      "epoch:7 step:7372 [D loss: 0.656645, acc: 60.94%] [G loss: 2.297755]\n",
      "epoch:7 step:7373 [D loss: 0.629170, acc: 64.06%] [G loss: 2.459496]\n",
      "epoch:7 step:7374 [D loss: 0.514488, acc: 73.44%] [G loss: 2.792460]\n",
      "epoch:7 step:7375 [D loss: 0.609402, acc: 66.41%] [G loss: 2.571709]\n",
      "epoch:7 step:7376 [D loss: 0.649165, acc: 60.16%] [G loss: 2.261250]\n",
      "epoch:7 step:7377 [D loss: 0.704025, acc: 60.16%] [G loss: 2.085233]\n",
      "epoch:7 step:7378 [D loss: 0.626178, acc: 66.41%] [G loss: 2.061466]\n",
      "epoch:7 step:7379 [D loss: 0.624998, acc: 60.16%] [G loss: 2.152939]\n",
      "epoch:7 step:7380 [D loss: 0.585343, acc: 64.06%] [G loss: 2.196588]\n",
      "epoch:7 step:7381 [D loss: 0.590607, acc: 64.06%] [G loss: 2.327284]\n",
      "epoch:7 step:7382 [D loss: 0.618063, acc: 68.75%] [G loss: 2.426783]\n",
      "epoch:7 step:7383 [D loss: 0.608056, acc: 66.41%] [G loss: 2.255237]\n",
      "epoch:7 step:7384 [D loss: 0.596787, acc: 68.75%] [G loss: 2.333203]\n",
      "epoch:7 step:7385 [D loss: 0.601771, acc: 67.97%] [G loss: 2.150886]\n",
      "epoch:7 step:7386 [D loss: 0.793437, acc: 56.25%] [G loss: 1.935060]\n",
      "epoch:7 step:7387 [D loss: 0.686914, acc: 55.47%] [G loss: 2.094946]\n",
      "epoch:7 step:7388 [D loss: 0.628488, acc: 58.59%] [G loss: 1.932860]\n",
      "epoch:7 step:7389 [D loss: 0.565718, acc: 74.22%] [G loss: 2.292722]\n",
      "epoch:7 step:7390 [D loss: 0.581027, acc: 68.75%] [G loss: 2.414803]\n",
      "epoch:7 step:7391 [D loss: 0.662727, acc: 63.28%] [G loss: 2.205157]\n",
      "epoch:7 step:7392 [D loss: 0.583998, acc: 64.06%] [G loss: 2.246707]\n",
      "epoch:7 step:7393 [D loss: 0.592689, acc: 66.41%] [G loss: 2.176807]\n",
      "epoch:7 step:7394 [D loss: 0.589730, acc: 68.75%] [G loss: 2.274808]\n",
      "epoch:7 step:7395 [D loss: 0.604624, acc: 67.97%] [G loss: 2.349123]\n",
      "epoch:7 step:7396 [D loss: 0.555176, acc: 77.34%] [G loss: 2.400175]\n",
      "epoch:7 step:7397 [D loss: 0.590409, acc: 67.19%] [G loss: 2.258780]\n",
      "epoch:7 step:7398 [D loss: 0.589483, acc: 71.88%] [G loss: 2.425470]\n",
      "epoch:7 step:7399 [D loss: 0.614694, acc: 66.41%] [G loss: 2.160751]\n",
      "epoch:7 step:7400 [D loss: 0.562537, acc: 71.88%] [G loss: 2.201937]\n",
      "##############\n",
      "[2.59385939 1.36823675 6.47703359 4.94516479 3.74827741 5.92073948\n",
      " 4.6913142  4.80610834 4.88054319 3.45181269]\n",
      "##########\n",
      "epoch:7 step:7401 [D loss: 0.594361, acc: 63.28%] [G loss: 2.548829]\n",
      "epoch:7 step:7402 [D loss: 0.623029, acc: 64.84%] [G loss: 2.394537]\n",
      "epoch:7 step:7403 [D loss: 0.676455, acc: 60.94%] [G loss: 2.273438]\n",
      "epoch:7 step:7404 [D loss: 0.671869, acc: 62.50%] [G loss: 2.390197]\n",
      "epoch:7 step:7405 [D loss: 0.629095, acc: 67.19%] [G loss: 2.104453]\n",
      "epoch:7 step:7406 [D loss: 0.559668, acc: 71.88%] [G loss: 2.264764]\n",
      "epoch:7 step:7407 [D loss: 0.623773, acc: 54.69%] [G loss: 2.289976]\n",
      "epoch:7 step:7408 [D loss: 0.670526, acc: 60.94%] [G loss: 2.219659]\n",
      "epoch:7 step:7409 [D loss: 0.634680, acc: 63.28%] [G loss: 2.214028]\n",
      "epoch:7 step:7410 [D loss: 0.622077, acc: 66.41%] [G loss: 2.066747]\n",
      "epoch:7 step:7411 [D loss: 0.603143, acc: 70.31%] [G loss: 2.438757]\n",
      "epoch:7 step:7412 [D loss: 0.635839, acc: 64.06%] [G loss: 2.369164]\n",
      "epoch:7 step:7413 [D loss: 0.556677, acc: 69.53%] [G loss: 2.274174]\n",
      "epoch:7 step:7414 [D loss: 0.604811, acc: 67.97%] [G loss: 2.165934]\n",
      "epoch:7 step:7415 [D loss: 0.644788, acc: 59.38%] [G loss: 2.257699]\n",
      "epoch:7 step:7416 [D loss: 0.663168, acc: 60.16%] [G loss: 2.267795]\n",
      "epoch:7 step:7417 [D loss: 0.720193, acc: 53.91%] [G loss: 2.209552]\n",
      "epoch:7 step:7418 [D loss: 0.607817, acc: 69.53%] [G loss: 2.039125]\n",
      "epoch:7 step:7419 [D loss: 0.564321, acc: 71.88%] [G loss: 2.206967]\n",
      "epoch:7 step:7420 [D loss: 0.611828, acc: 67.97%] [G loss: 2.052113]\n",
      "epoch:7 step:7421 [D loss: 0.601085, acc: 64.84%] [G loss: 2.235402]\n",
      "epoch:7 step:7422 [D loss: 0.610253, acc: 64.84%] [G loss: 2.200589]\n",
      "epoch:7 step:7423 [D loss: 0.625064, acc: 61.72%] [G loss: 2.214799]\n",
      "epoch:7 step:7424 [D loss: 0.641062, acc: 61.72%] [G loss: 2.161231]\n",
      "epoch:7 step:7425 [D loss: 0.657649, acc: 61.72%] [G loss: 2.180733]\n",
      "epoch:7 step:7426 [D loss: 0.599295, acc: 66.41%] [G loss: 2.157883]\n",
      "epoch:7 step:7427 [D loss: 0.680376, acc: 56.25%] [G loss: 2.194146]\n",
      "epoch:7 step:7428 [D loss: 0.658363, acc: 58.59%] [G loss: 1.940847]\n",
      "epoch:7 step:7429 [D loss: 0.649958, acc: 69.53%] [G loss: 2.198954]\n",
      "epoch:7 step:7430 [D loss: 0.668699, acc: 60.16%] [G loss: 2.165196]\n",
      "epoch:7 step:7431 [D loss: 0.581406, acc: 63.28%] [G loss: 2.129611]\n",
      "epoch:7 step:7432 [D loss: 0.568125, acc: 72.66%] [G loss: 2.023881]\n",
      "epoch:7 step:7433 [D loss: 0.644824, acc: 66.41%] [G loss: 2.136735]\n",
      "epoch:7 step:7434 [D loss: 0.581242, acc: 66.41%] [G loss: 2.373995]\n",
      "epoch:7 step:7435 [D loss: 0.578504, acc: 66.41%] [G loss: 2.275794]\n",
      "epoch:7 step:7436 [D loss: 0.639574, acc: 62.50%] [G loss: 2.537477]\n",
      "epoch:7 step:7437 [D loss: 0.658852, acc: 60.16%] [G loss: 2.091952]\n",
      "epoch:7 step:7438 [D loss: 0.595175, acc: 72.66%] [G loss: 2.124221]\n",
      "epoch:7 step:7439 [D loss: 0.615663, acc: 67.19%] [G loss: 2.257108]\n",
      "epoch:7 step:7440 [D loss: 0.628700, acc: 64.06%] [G loss: 2.203172]\n",
      "epoch:7 step:7441 [D loss: 0.663864, acc: 55.47%] [G loss: 1.966207]\n",
      "epoch:7 step:7442 [D loss: 0.613903, acc: 69.53%] [G loss: 2.192002]\n",
      "epoch:7 step:7443 [D loss: 0.536883, acc: 72.66%] [G loss: 2.245563]\n",
      "epoch:7 step:7444 [D loss: 0.597777, acc: 67.97%] [G loss: 2.411298]\n",
      "epoch:7 step:7445 [D loss: 0.563363, acc: 71.88%] [G loss: 2.752636]\n",
      "epoch:7 step:7446 [D loss: 0.676120, acc: 57.81%] [G loss: 2.465104]\n",
      "epoch:7 step:7447 [D loss: 0.617203, acc: 70.31%] [G loss: 2.212930]\n",
      "epoch:7 step:7448 [D loss: 0.673012, acc: 58.59%] [G loss: 2.161608]\n",
      "epoch:7 step:7449 [D loss: 0.577477, acc: 69.53%] [G loss: 2.367879]\n",
      "epoch:7 step:7450 [D loss: 0.640022, acc: 66.41%] [G loss: 2.210729]\n",
      "epoch:7 step:7451 [D loss: 0.610710, acc: 67.97%] [G loss: 2.020264]\n",
      "epoch:7 step:7452 [D loss: 0.596485, acc: 66.41%] [G loss: 2.373572]\n",
      "epoch:7 step:7453 [D loss: 0.608259, acc: 64.06%] [G loss: 2.306432]\n",
      "epoch:7 step:7454 [D loss: 0.587555, acc: 68.75%] [G loss: 2.208951]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:7 step:7455 [D loss: 0.639267, acc: 64.84%] [G loss: 2.286460]\n",
      "epoch:7 step:7456 [D loss: 0.600762, acc: 66.41%] [G loss: 2.239509]\n",
      "epoch:7 step:7457 [D loss: 0.539751, acc: 75.78%] [G loss: 2.428246]\n",
      "epoch:7 step:7458 [D loss: 0.593321, acc: 66.41%] [G loss: 2.348008]\n",
      "epoch:7 step:7459 [D loss: 0.688275, acc: 58.59%] [G loss: 2.394578]\n",
      "epoch:7 step:7460 [D loss: 0.689319, acc: 61.72%] [G loss: 2.029339]\n",
      "epoch:7 step:7461 [D loss: 0.611784, acc: 65.62%] [G loss: 2.135918]\n",
      "epoch:7 step:7462 [D loss: 0.668071, acc: 60.94%] [G loss: 2.226598]\n",
      "epoch:7 step:7463 [D loss: 0.654423, acc: 60.94%] [G loss: 2.189838]\n",
      "epoch:7 step:7464 [D loss: 0.612851, acc: 64.84%] [G loss: 2.144781]\n",
      "epoch:7 step:7465 [D loss: 0.609650, acc: 66.41%] [G loss: 2.425324]\n",
      "epoch:7 step:7466 [D loss: 0.698520, acc: 61.72%] [G loss: 2.240362]\n",
      "epoch:7 step:7467 [D loss: 0.540631, acc: 75.78%] [G loss: 2.388031]\n",
      "epoch:7 step:7468 [D loss: 0.598839, acc: 65.62%] [G loss: 2.427158]\n",
      "epoch:7 step:7469 [D loss: 0.633504, acc: 64.06%] [G loss: 2.388295]\n",
      "epoch:7 step:7470 [D loss: 0.574798, acc: 70.31%] [G loss: 2.469836]\n",
      "epoch:7 step:7471 [D loss: 0.614712, acc: 67.97%] [G loss: 2.863903]\n",
      "epoch:7 step:7472 [D loss: 0.666683, acc: 57.03%] [G loss: 2.276328]\n",
      "epoch:7 step:7473 [D loss: 0.661264, acc: 57.81%] [G loss: 2.357853]\n",
      "epoch:7 step:7474 [D loss: 0.571644, acc: 71.09%] [G loss: 2.325953]\n",
      "epoch:7 step:7475 [D loss: 0.606706, acc: 70.31%] [G loss: 2.307659]\n",
      "epoch:7 step:7476 [D loss: 0.617745, acc: 68.75%] [G loss: 2.386772]\n",
      "epoch:7 step:7477 [D loss: 0.521358, acc: 76.56%] [G loss: 2.574774]\n",
      "epoch:7 step:7478 [D loss: 0.634285, acc: 60.16%] [G loss: 2.446710]\n",
      "epoch:7 step:7479 [D loss: 0.681601, acc: 60.16%] [G loss: 2.233860]\n",
      "epoch:7 step:7480 [D loss: 0.601198, acc: 61.72%] [G loss: 2.185321]\n",
      "epoch:7 step:7481 [D loss: 0.579785, acc: 65.62%] [G loss: 2.193041]\n",
      "epoch:7 step:7482 [D loss: 0.618376, acc: 67.19%] [G loss: 2.474607]\n",
      "epoch:7 step:7483 [D loss: 0.565165, acc: 71.09%] [G loss: 2.491574]\n",
      "epoch:7 step:7484 [D loss: 0.562776, acc: 71.09%] [G loss: 2.864618]\n",
      "epoch:7 step:7485 [D loss: 0.566147, acc: 67.19%] [G loss: 2.707733]\n",
      "epoch:7 step:7486 [D loss: 0.599376, acc: 75.00%] [G loss: 2.634683]\n",
      "epoch:7 step:7487 [D loss: 0.755409, acc: 54.69%] [G loss: 2.051421]\n",
      "epoch:7 step:7488 [D loss: 0.678680, acc: 54.69%] [G loss: 2.346181]\n",
      "epoch:7 step:7489 [D loss: 0.663886, acc: 63.28%] [G loss: 2.335881]\n",
      "epoch:7 step:7490 [D loss: 0.659036, acc: 66.41%] [G loss: 2.255311]\n",
      "epoch:7 step:7491 [D loss: 0.614422, acc: 65.62%] [G loss: 2.116714]\n",
      "epoch:7 step:7492 [D loss: 0.601152, acc: 67.97%] [G loss: 2.185936]\n",
      "epoch:7 step:7493 [D loss: 0.589664, acc: 68.75%] [G loss: 2.463598]\n",
      "epoch:7 step:7494 [D loss: 0.575711, acc: 70.31%] [G loss: 2.201246]\n",
      "epoch:7 step:7495 [D loss: 0.544630, acc: 72.66%] [G loss: 2.736053]\n",
      "epoch:7 step:7496 [D loss: 0.509612, acc: 76.56%] [G loss: 3.307821]\n",
      "epoch:8 step:7497 [D loss: 0.601880, acc: 66.41%] [G loss: 2.362416]\n",
      "epoch:8 step:7498 [D loss: 0.580782, acc: 67.19%] [G loss: 2.418464]\n",
      "epoch:8 step:7499 [D loss: 0.648056, acc: 63.28%] [G loss: 2.397387]\n",
      "epoch:8 step:7500 [D loss: 0.618804, acc: 65.62%] [G loss: 2.163823]\n",
      "epoch:8 step:7501 [D loss: 0.649275, acc: 64.06%] [G loss: 2.314936]\n",
      "epoch:8 step:7502 [D loss: 0.611627, acc: 65.62%] [G loss: 2.482500]\n",
      "epoch:8 step:7503 [D loss: 0.594384, acc: 69.53%] [G loss: 2.361581]\n",
      "epoch:8 step:7504 [D loss: 0.594613, acc: 71.88%] [G loss: 2.577000]\n",
      "epoch:8 step:7505 [D loss: 0.603878, acc: 68.75%] [G loss: 2.538804]\n",
      "epoch:8 step:7506 [D loss: 0.554855, acc: 72.66%] [G loss: 2.475059]\n",
      "epoch:8 step:7507 [D loss: 0.659629, acc: 64.84%] [G loss: 2.192410]\n",
      "epoch:8 step:7508 [D loss: 0.621513, acc: 62.50%] [G loss: 2.300723]\n",
      "epoch:8 step:7509 [D loss: 0.580182, acc: 71.09%] [G loss: 2.508015]\n",
      "epoch:8 step:7510 [D loss: 0.642912, acc: 64.84%] [G loss: 2.233711]\n",
      "epoch:8 step:7511 [D loss: 0.612278, acc: 66.41%] [G loss: 2.470997]\n",
      "epoch:8 step:7512 [D loss: 0.531155, acc: 72.66%] [G loss: 2.518308]\n",
      "epoch:8 step:7513 [D loss: 0.655647, acc: 62.50%] [G loss: 2.289996]\n",
      "epoch:8 step:7514 [D loss: 0.716797, acc: 55.47%] [G loss: 2.084949]\n",
      "epoch:8 step:7515 [D loss: 0.680703, acc: 61.72%] [G loss: 2.281022]\n",
      "epoch:8 step:7516 [D loss: 0.606313, acc: 69.53%] [G loss: 2.126597]\n",
      "epoch:8 step:7517 [D loss: 0.653869, acc: 61.72%] [G loss: 2.116858]\n",
      "epoch:8 step:7518 [D loss: 0.623988, acc: 65.62%] [G loss: 2.086890]\n",
      "epoch:8 step:7519 [D loss: 0.538873, acc: 74.22%] [G loss: 2.277431]\n",
      "epoch:8 step:7520 [D loss: 0.607429, acc: 67.97%] [G loss: 2.174303]\n",
      "epoch:8 step:7521 [D loss: 0.584483, acc: 69.53%] [G loss: 2.493915]\n",
      "epoch:8 step:7522 [D loss: 0.605368, acc: 65.62%] [G loss: 2.177311]\n",
      "epoch:8 step:7523 [D loss: 0.668282, acc: 62.50%] [G loss: 2.260545]\n",
      "epoch:8 step:7524 [D loss: 0.546195, acc: 73.44%] [G loss: 2.179199]\n",
      "epoch:8 step:7525 [D loss: 0.634935, acc: 65.62%] [G loss: 2.247587]\n",
      "epoch:8 step:7526 [D loss: 0.609149, acc: 68.75%] [G loss: 2.326488]\n",
      "epoch:8 step:7527 [D loss: 0.626779, acc: 71.88%] [G loss: 2.230887]\n",
      "epoch:8 step:7528 [D loss: 0.609371, acc: 64.06%] [G loss: 2.071579]\n",
      "epoch:8 step:7529 [D loss: 0.598989, acc: 69.53%] [G loss: 2.225332]\n",
      "epoch:8 step:7530 [D loss: 0.604874, acc: 65.62%] [G loss: 2.314480]\n",
      "epoch:8 step:7531 [D loss: 0.666782, acc: 54.69%] [G loss: 2.062760]\n",
      "epoch:8 step:7532 [D loss: 0.610132, acc: 73.44%] [G loss: 2.414102]\n",
      "epoch:8 step:7533 [D loss: 0.604673, acc: 70.31%] [G loss: 2.386173]\n",
      "epoch:8 step:7534 [D loss: 0.638121, acc: 66.41%] [G loss: 2.486761]\n",
      "epoch:8 step:7535 [D loss: 0.587994, acc: 70.31%] [G loss: 2.532786]\n",
      "epoch:8 step:7536 [D loss: 0.532259, acc: 76.56%] [G loss: 2.344921]\n",
      "epoch:8 step:7537 [D loss: 0.640278, acc: 64.84%] [G loss: 2.302699]\n",
      "epoch:8 step:7538 [D loss: 0.600798, acc: 68.75%] [G loss: 2.499776]\n",
      "epoch:8 step:7539 [D loss: 0.586604, acc: 66.41%] [G loss: 2.285196]\n",
      "epoch:8 step:7540 [D loss: 0.613820, acc: 64.84%] [G loss: 2.239995]\n",
      "epoch:8 step:7541 [D loss: 0.649936, acc: 64.06%] [G loss: 2.099110]\n",
      "epoch:8 step:7542 [D loss: 0.607029, acc: 69.53%] [G loss: 2.230310]\n",
      "epoch:8 step:7543 [D loss: 0.608686, acc: 62.50%] [G loss: 2.207350]\n",
      "epoch:8 step:7544 [D loss: 0.593083, acc: 66.41%] [G loss: 2.284921]\n",
      "epoch:8 step:7545 [D loss: 0.553540, acc: 65.62%] [G loss: 2.509032]\n",
      "epoch:8 step:7546 [D loss: 0.523870, acc: 76.56%] [G loss: 2.473928]\n",
      "epoch:8 step:7547 [D loss: 0.626780, acc: 65.62%] [G loss: 2.340943]\n",
      "epoch:8 step:7548 [D loss: 0.583183, acc: 62.50%] [G loss: 2.386155]\n",
      "epoch:8 step:7549 [D loss: 0.567385, acc: 71.09%] [G loss: 2.391878]\n",
      "epoch:8 step:7550 [D loss: 0.644611, acc: 60.16%] [G loss: 2.127473]\n",
      "epoch:8 step:7551 [D loss: 0.612766, acc: 67.97%] [G loss: 2.350273]\n",
      "epoch:8 step:7552 [D loss: 0.652953, acc: 65.62%] [G loss: 2.405123]\n",
      "epoch:8 step:7553 [D loss: 0.637476, acc: 65.62%] [G loss: 2.280250]\n",
      "epoch:8 step:7554 [D loss: 0.650313, acc: 60.94%] [G loss: 2.245314]\n",
      "epoch:8 step:7555 [D loss: 0.631696, acc: 60.94%] [G loss: 2.089893]\n",
      "epoch:8 step:7556 [D loss: 0.692568, acc: 63.28%] [G loss: 2.247438]\n",
      "epoch:8 step:7557 [D loss: 0.623888, acc: 65.62%] [G loss: 2.334981]\n",
      "epoch:8 step:7558 [D loss: 0.580735, acc: 70.31%] [G loss: 2.365559]\n",
      "epoch:8 step:7559 [D loss: 0.659201, acc: 61.72%] [G loss: 1.988192]\n",
      "epoch:8 step:7560 [D loss: 0.633891, acc: 64.84%] [G loss: 2.132301]\n",
      "epoch:8 step:7561 [D loss: 0.583321, acc: 69.53%] [G loss: 2.138597]\n",
      "epoch:8 step:7562 [D loss: 0.625772, acc: 63.28%] [G loss: 2.028946]\n",
      "epoch:8 step:7563 [D loss: 0.594703, acc: 71.88%] [G loss: 2.247809]\n",
      "epoch:8 step:7564 [D loss: 0.615529, acc: 65.62%] [G loss: 2.164105]\n",
      "epoch:8 step:7565 [D loss: 0.635908, acc: 67.19%] [G loss: 2.128128]\n",
      "epoch:8 step:7566 [D loss: 0.589566, acc: 66.41%] [G loss: 2.174624]\n",
      "epoch:8 step:7567 [D loss: 0.618415, acc: 62.50%] [G loss: 2.242208]\n",
      "epoch:8 step:7568 [D loss: 0.625780, acc: 64.06%] [G loss: 2.174549]\n",
      "epoch:8 step:7569 [D loss: 0.655161, acc: 64.06%] [G loss: 2.241374]\n",
      "epoch:8 step:7570 [D loss: 0.582989, acc: 70.31%] [G loss: 2.340918]\n",
      "epoch:8 step:7571 [D loss: 0.598593, acc: 67.19%] [G loss: 2.601458]\n",
      "epoch:8 step:7572 [D loss: 0.643256, acc: 64.06%] [G loss: 2.612809]\n",
      "epoch:8 step:7573 [D loss: 0.614934, acc: 67.97%] [G loss: 2.360301]\n",
      "epoch:8 step:7574 [D loss: 0.622085, acc: 71.09%] [G loss: 2.122774]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:8 step:7575 [D loss: 0.677209, acc: 64.06%] [G loss: 2.109612]\n",
      "epoch:8 step:7576 [D loss: 0.672521, acc: 60.16%] [G loss: 2.187441]\n",
      "epoch:8 step:7577 [D loss: 0.638369, acc: 64.84%] [G loss: 2.070981]\n",
      "epoch:8 step:7578 [D loss: 0.593164, acc: 64.84%] [G loss: 2.262291]\n",
      "epoch:8 step:7579 [D loss: 0.614577, acc: 67.19%] [G loss: 2.314941]\n",
      "epoch:8 step:7580 [D loss: 0.544561, acc: 69.53%] [G loss: 2.354628]\n",
      "epoch:8 step:7581 [D loss: 0.664502, acc: 61.72%] [G loss: 2.193848]\n",
      "epoch:8 step:7582 [D loss: 0.649485, acc: 60.16%] [G loss: 2.058413]\n",
      "epoch:8 step:7583 [D loss: 0.625074, acc: 65.62%] [G loss: 1.998696]\n",
      "epoch:8 step:7584 [D loss: 0.594935, acc: 68.75%] [G loss: 2.074545]\n",
      "epoch:8 step:7585 [D loss: 0.584022, acc: 66.41%] [G loss: 2.350424]\n",
      "epoch:8 step:7586 [D loss: 0.598418, acc: 66.41%] [G loss: 2.245793]\n",
      "epoch:8 step:7587 [D loss: 0.659616, acc: 57.81%] [G loss: 2.179830]\n",
      "epoch:8 step:7588 [D loss: 0.604424, acc: 65.62%] [G loss: 2.361366]\n",
      "epoch:8 step:7589 [D loss: 0.618725, acc: 65.62%] [G loss: 2.363970]\n",
      "epoch:8 step:7590 [D loss: 0.626956, acc: 64.84%] [G loss: 2.192599]\n",
      "epoch:8 step:7591 [D loss: 0.614406, acc: 62.50%] [G loss: 2.140034]\n",
      "epoch:8 step:7592 [D loss: 0.588609, acc: 71.88%] [G loss: 2.381384]\n",
      "epoch:8 step:7593 [D loss: 0.569430, acc: 70.31%] [G loss: 2.467597]\n",
      "epoch:8 step:7594 [D loss: 0.665104, acc: 64.06%] [G loss: 2.131088]\n",
      "epoch:8 step:7595 [D loss: 0.586323, acc: 70.31%] [G loss: 2.297754]\n",
      "epoch:8 step:7596 [D loss: 0.656478, acc: 62.50%] [G loss: 2.283087]\n",
      "epoch:8 step:7597 [D loss: 0.544182, acc: 71.88%] [G loss: 2.341235]\n",
      "epoch:8 step:7598 [D loss: 0.657396, acc: 59.38%] [G loss: 2.156343]\n",
      "epoch:8 step:7599 [D loss: 0.631517, acc: 65.62%] [G loss: 2.480814]\n",
      "epoch:8 step:7600 [D loss: 0.645423, acc: 64.84%] [G loss: 2.140749]\n",
      "##############\n",
      "[2.70349124 1.15921028 6.52692317 5.04479396 3.84763287 5.70299337\n",
      " 4.5314754  4.877897   5.03803447 3.66630733]\n",
      "##########\n",
      "epoch:8 step:7601 [D loss: 0.693635, acc: 61.72%] [G loss: 2.073842]\n",
      "epoch:8 step:7602 [D loss: 0.580279, acc: 73.44%] [G loss: 2.233742]\n",
      "epoch:8 step:7603 [D loss: 0.617164, acc: 60.16%] [G loss: 2.222471]\n",
      "epoch:8 step:7604 [D loss: 0.676052, acc: 62.50%] [G loss: 2.283897]\n",
      "epoch:8 step:7605 [D loss: 0.675699, acc: 60.94%] [G loss: 2.225174]\n",
      "epoch:8 step:7606 [D loss: 0.634288, acc: 62.50%] [G loss: 1.872219]\n",
      "epoch:8 step:7607 [D loss: 0.627273, acc: 67.19%] [G loss: 2.433651]\n",
      "epoch:8 step:7608 [D loss: 0.622550, acc: 64.06%] [G loss: 2.226161]\n",
      "epoch:8 step:7609 [D loss: 0.695152, acc: 60.94%] [G loss: 2.045218]\n",
      "epoch:8 step:7610 [D loss: 0.630946, acc: 61.72%] [G loss: 1.913995]\n",
      "epoch:8 step:7611 [D loss: 0.661700, acc: 57.03%] [G loss: 2.250765]\n",
      "epoch:8 step:7612 [D loss: 0.569656, acc: 70.31%] [G loss: 2.288066]\n",
      "epoch:8 step:7613 [D loss: 0.585441, acc: 71.09%] [G loss: 2.387850]\n",
      "epoch:8 step:7614 [D loss: 0.653937, acc: 60.16%] [G loss: 2.326834]\n",
      "epoch:8 step:7615 [D loss: 0.621019, acc: 64.84%] [G loss: 2.663604]\n",
      "epoch:8 step:7616 [D loss: 0.615060, acc: 65.62%] [G loss: 2.225319]\n",
      "epoch:8 step:7617 [D loss: 0.621376, acc: 60.94%] [G loss: 2.091069]\n",
      "epoch:8 step:7618 [D loss: 0.592074, acc: 68.75%] [G loss: 2.495905]\n",
      "epoch:8 step:7619 [D loss: 0.617852, acc: 67.97%] [G loss: 2.048243]\n",
      "epoch:8 step:7620 [D loss: 0.679020, acc: 58.59%] [G loss: 2.060879]\n",
      "epoch:8 step:7621 [D loss: 0.651459, acc: 59.38%] [G loss: 2.191182]\n",
      "epoch:8 step:7622 [D loss: 0.623605, acc: 66.41%] [G loss: 2.359058]\n",
      "epoch:8 step:7623 [D loss: 0.601571, acc: 67.97%] [G loss: 2.239327]\n",
      "epoch:8 step:7624 [D loss: 0.615585, acc: 71.88%] [G loss: 2.100246]\n",
      "epoch:8 step:7625 [D loss: 0.616497, acc: 64.06%] [G loss: 2.109211]\n",
      "epoch:8 step:7626 [D loss: 0.533995, acc: 76.56%] [G loss: 2.437567]\n",
      "epoch:8 step:7627 [D loss: 0.576993, acc: 67.19%] [G loss: 2.360374]\n",
      "epoch:8 step:7628 [D loss: 0.622281, acc: 66.41%] [G loss: 2.382763]\n",
      "epoch:8 step:7629 [D loss: 0.697363, acc: 57.81%] [G loss: 2.017968]\n",
      "epoch:8 step:7630 [D loss: 0.611757, acc: 61.72%] [G loss: 2.187220]\n",
      "epoch:8 step:7631 [D loss: 0.673836, acc: 54.69%] [G loss: 2.047179]\n",
      "epoch:8 step:7632 [D loss: 0.604945, acc: 69.53%] [G loss: 2.160642]\n",
      "epoch:8 step:7633 [D loss: 0.650890, acc: 55.47%] [G loss: 2.119324]\n",
      "epoch:8 step:7634 [D loss: 0.592074, acc: 70.31%] [G loss: 2.148606]\n",
      "epoch:8 step:7635 [D loss: 0.560116, acc: 72.66%] [G loss: 2.160144]\n",
      "epoch:8 step:7636 [D loss: 0.650685, acc: 60.94%] [G loss: 1.963454]\n",
      "epoch:8 step:7637 [D loss: 0.583111, acc: 70.31%] [G loss: 2.258316]\n",
      "epoch:8 step:7638 [D loss: 0.635221, acc: 60.94%] [G loss: 2.228944]\n",
      "epoch:8 step:7639 [D loss: 0.587914, acc: 68.75%] [G loss: 2.169710]\n",
      "epoch:8 step:7640 [D loss: 0.609333, acc: 71.09%] [G loss: 2.327452]\n",
      "epoch:8 step:7641 [D loss: 0.649295, acc: 60.94%] [G loss: 2.300192]\n",
      "epoch:8 step:7642 [D loss: 0.575082, acc: 71.09%] [G loss: 2.017298]\n",
      "epoch:8 step:7643 [D loss: 0.624749, acc: 65.62%] [G loss: 2.268827]\n",
      "epoch:8 step:7644 [D loss: 0.588226, acc: 67.97%] [G loss: 2.186542]\n",
      "epoch:8 step:7645 [D loss: 0.599398, acc: 66.41%] [G loss: 2.399305]\n",
      "epoch:8 step:7646 [D loss: 0.671661, acc: 63.28%] [G loss: 2.209870]\n",
      "epoch:8 step:7647 [D loss: 0.630693, acc: 69.53%] [G loss: 2.746519]\n",
      "epoch:8 step:7648 [D loss: 0.588412, acc: 71.09%] [G loss: 2.449729]\n",
      "epoch:8 step:7649 [D loss: 0.644345, acc: 58.59%] [G loss: 1.997337]\n",
      "epoch:8 step:7650 [D loss: 0.619610, acc: 64.84%] [G loss: 2.209895]\n",
      "epoch:8 step:7651 [D loss: 0.648757, acc: 60.94%] [G loss: 2.169242]\n",
      "epoch:8 step:7652 [D loss: 0.612377, acc: 66.41%] [G loss: 2.269371]\n",
      "epoch:8 step:7653 [D loss: 0.643944, acc: 60.94%] [G loss: 2.103351]\n",
      "epoch:8 step:7654 [D loss: 0.625054, acc: 64.84%] [G loss: 2.178639]\n",
      "epoch:8 step:7655 [D loss: 0.576293, acc: 73.44%] [G loss: 2.338771]\n",
      "epoch:8 step:7656 [D loss: 0.638361, acc: 60.94%] [G loss: 2.119575]\n",
      "epoch:8 step:7657 [D loss: 0.644076, acc: 66.41%] [G loss: 2.261629]\n",
      "epoch:8 step:7658 [D loss: 0.599815, acc: 62.50%] [G loss: 2.302527]\n",
      "epoch:8 step:7659 [D loss: 0.651074, acc: 63.28%] [G loss: 2.048800]\n",
      "epoch:8 step:7660 [D loss: 0.663170, acc: 64.06%] [G loss: 1.886881]\n",
      "epoch:8 step:7661 [D loss: 0.638756, acc: 62.50%] [G loss: 2.157888]\n",
      "epoch:8 step:7662 [D loss: 0.646332, acc: 66.41%] [G loss: 2.139838]\n",
      "epoch:8 step:7663 [D loss: 0.611474, acc: 69.53%] [G loss: 2.071605]\n",
      "epoch:8 step:7664 [D loss: 0.568391, acc: 68.75%] [G loss: 2.199601]\n",
      "epoch:8 step:7665 [D loss: 0.643720, acc: 64.84%] [G loss: 2.275712]\n",
      "epoch:8 step:7666 [D loss: 0.606001, acc: 68.75%] [G loss: 2.103860]\n",
      "epoch:8 step:7667 [D loss: 0.604399, acc: 67.19%] [G loss: 2.297003]\n",
      "epoch:8 step:7668 [D loss: 0.612665, acc: 67.19%] [G loss: 2.285057]\n",
      "epoch:8 step:7669 [D loss: 0.606306, acc: 69.53%] [G loss: 1.994009]\n",
      "epoch:8 step:7670 [D loss: 0.632911, acc: 61.72%] [G loss: 2.026843]\n",
      "epoch:8 step:7671 [D loss: 0.655824, acc: 61.72%] [G loss: 2.315761]\n",
      "epoch:8 step:7672 [D loss: 0.636534, acc: 65.62%] [G loss: 2.232260]\n",
      "epoch:8 step:7673 [D loss: 0.626188, acc: 71.88%] [G loss: 2.279262]\n",
      "epoch:8 step:7674 [D loss: 0.626315, acc: 64.84%] [G loss: 2.225167]\n",
      "epoch:8 step:7675 [D loss: 0.585190, acc: 69.53%] [G loss: 2.040851]\n",
      "epoch:8 step:7676 [D loss: 0.658762, acc: 63.28%] [G loss: 2.205332]\n",
      "epoch:8 step:7677 [D loss: 0.684346, acc: 60.94%] [G loss: 2.304070]\n",
      "epoch:8 step:7678 [D loss: 0.675897, acc: 59.38%] [G loss: 1.974221]\n",
      "epoch:8 step:7679 [D loss: 0.636249, acc: 63.28%] [G loss: 2.022689]\n",
      "epoch:8 step:7680 [D loss: 0.696915, acc: 61.72%] [G loss: 1.976791]\n",
      "epoch:8 step:7681 [D loss: 0.606573, acc: 67.97%] [G loss: 2.171926]\n",
      "epoch:8 step:7682 [D loss: 0.619615, acc: 66.41%] [G loss: 1.986976]\n",
      "epoch:8 step:7683 [D loss: 0.600199, acc: 66.41%] [G loss: 2.203405]\n",
      "epoch:8 step:7684 [D loss: 0.670370, acc: 58.59%] [G loss: 2.129833]\n",
      "epoch:8 step:7685 [D loss: 0.621483, acc: 68.75%] [G loss: 1.984476]\n",
      "epoch:8 step:7686 [D loss: 0.638800, acc: 65.62%] [G loss: 2.246821]\n",
      "epoch:8 step:7687 [D loss: 0.605812, acc: 60.94%] [G loss: 2.201383]\n",
      "epoch:8 step:7688 [D loss: 0.586503, acc: 67.97%] [G loss: 2.405851]\n",
      "epoch:8 step:7689 [D loss: 0.578035, acc: 65.62%] [G loss: 2.194077]\n",
      "epoch:8 step:7690 [D loss: 0.578109, acc: 67.19%] [G loss: 2.362710]\n",
      "epoch:8 step:7691 [D loss: 0.631370, acc: 66.41%] [G loss: 2.154938]\n",
      "epoch:8 step:7692 [D loss: 0.681311, acc: 56.25%] [G loss: 2.131079]\n",
      "epoch:8 step:7693 [D loss: 0.571371, acc: 72.66%] [G loss: 2.428329]\n",
      "epoch:8 step:7694 [D loss: 0.655911, acc: 59.38%] [G loss: 2.186341]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:8 step:7695 [D loss: 0.699800, acc: 58.59%] [G loss: 2.192472]\n",
      "epoch:8 step:7696 [D loss: 0.638597, acc: 63.28%] [G loss: 2.036993]\n",
      "epoch:8 step:7697 [D loss: 0.621864, acc: 60.16%] [G loss: 2.335052]\n",
      "epoch:8 step:7698 [D loss: 0.607355, acc: 70.31%] [G loss: 2.266315]\n",
      "epoch:8 step:7699 [D loss: 0.589817, acc: 71.09%] [G loss: 2.050215]\n",
      "epoch:8 step:7700 [D loss: 0.647467, acc: 59.38%] [G loss: 2.039963]\n",
      "epoch:8 step:7701 [D loss: 0.611469, acc: 64.84%] [G loss: 2.194418]\n",
      "epoch:8 step:7702 [D loss: 0.644630, acc: 60.16%] [G loss: 2.208488]\n",
      "epoch:8 step:7703 [D loss: 0.552339, acc: 75.78%] [G loss: 2.763108]\n",
      "epoch:8 step:7704 [D loss: 0.539396, acc: 72.66%] [G loss: 2.449738]\n",
      "epoch:8 step:7705 [D loss: 0.571977, acc: 75.78%] [G loss: 2.575485]\n",
      "epoch:8 step:7706 [D loss: 0.658736, acc: 60.94%] [G loss: 2.197118]\n",
      "epoch:8 step:7707 [D loss: 0.719524, acc: 53.91%] [G loss: 2.110145]\n",
      "epoch:8 step:7708 [D loss: 0.637121, acc: 64.06%] [G loss: 2.186409]\n",
      "epoch:8 step:7709 [D loss: 0.677500, acc: 61.72%] [G loss: 1.945730]\n",
      "epoch:8 step:7710 [D loss: 0.616167, acc: 63.28%] [G loss: 2.174943]\n",
      "epoch:8 step:7711 [D loss: 0.620515, acc: 62.50%] [G loss: 2.315187]\n",
      "epoch:8 step:7712 [D loss: 0.599571, acc: 66.41%] [G loss: 2.094798]\n",
      "epoch:8 step:7713 [D loss: 0.629343, acc: 63.28%] [G loss: 2.212685]\n",
      "epoch:8 step:7714 [D loss: 0.594576, acc: 67.97%] [G loss: 2.562999]\n",
      "epoch:8 step:7715 [D loss: 0.534507, acc: 74.22%] [G loss: 2.407105]\n",
      "epoch:8 step:7716 [D loss: 0.783858, acc: 52.34%] [G loss: 1.984491]\n",
      "epoch:8 step:7717 [D loss: 0.679834, acc: 56.25%] [G loss: 2.143472]\n",
      "epoch:8 step:7718 [D loss: 0.620442, acc: 69.53%] [G loss: 2.172238]\n",
      "epoch:8 step:7719 [D loss: 0.595110, acc: 69.53%] [G loss: 2.172227]\n",
      "epoch:8 step:7720 [D loss: 0.689133, acc: 58.59%] [G loss: 2.084104]\n",
      "epoch:8 step:7721 [D loss: 0.661922, acc: 56.25%] [G loss: 2.051688]\n",
      "epoch:8 step:7722 [D loss: 0.605988, acc: 64.84%] [G loss: 2.169981]\n",
      "epoch:8 step:7723 [D loss: 0.676161, acc: 60.94%] [G loss: 1.921066]\n",
      "epoch:8 step:7724 [D loss: 0.662272, acc: 59.38%] [G loss: 1.961609]\n",
      "epoch:8 step:7725 [D loss: 0.565163, acc: 76.56%] [G loss: 2.346230]\n",
      "epoch:8 step:7726 [D loss: 0.566144, acc: 73.44%] [G loss: 2.468348]\n",
      "epoch:8 step:7727 [D loss: 0.575039, acc: 69.53%] [G loss: 2.547752]\n",
      "epoch:8 step:7728 [D loss: 0.521042, acc: 75.00%] [G loss: 2.910352]\n",
      "epoch:8 step:7729 [D loss: 0.684527, acc: 62.50%] [G loss: 2.393865]\n",
      "epoch:8 step:7730 [D loss: 0.626765, acc: 62.50%] [G loss: 2.096815]\n",
      "epoch:8 step:7731 [D loss: 0.587631, acc: 71.09%] [G loss: 2.122592]\n",
      "epoch:8 step:7732 [D loss: 0.615518, acc: 64.84%] [G loss: 2.295201]\n",
      "epoch:8 step:7733 [D loss: 0.654652, acc: 61.72%] [G loss: 2.037049]\n",
      "epoch:8 step:7734 [D loss: 0.611355, acc: 65.62%] [G loss: 2.238395]\n",
      "epoch:8 step:7735 [D loss: 0.622807, acc: 67.19%] [G loss: 2.033219]\n",
      "epoch:8 step:7736 [D loss: 0.652637, acc: 57.81%] [G loss: 2.114142]\n",
      "epoch:8 step:7737 [D loss: 0.649583, acc: 61.72%] [G loss: 2.219812]\n",
      "epoch:8 step:7738 [D loss: 0.565769, acc: 71.88%] [G loss: 2.554413]\n",
      "epoch:8 step:7739 [D loss: 0.618077, acc: 67.97%] [G loss: 2.440419]\n",
      "epoch:8 step:7740 [D loss: 0.603978, acc: 65.62%] [G loss: 2.367325]\n",
      "epoch:8 step:7741 [D loss: 0.599240, acc: 66.41%] [G loss: 2.372334]\n",
      "epoch:8 step:7742 [D loss: 0.636584, acc: 60.94%] [G loss: 2.328484]\n",
      "epoch:8 step:7743 [D loss: 0.705301, acc: 61.72%] [G loss: 2.181296]\n",
      "epoch:8 step:7744 [D loss: 0.583095, acc: 71.09%] [G loss: 2.197060]\n",
      "epoch:8 step:7745 [D loss: 0.721443, acc: 50.00%] [G loss: 2.117971]\n",
      "epoch:8 step:7746 [D loss: 0.675975, acc: 57.81%] [G loss: 1.953950]\n",
      "epoch:8 step:7747 [D loss: 0.717170, acc: 56.25%] [G loss: 1.963011]\n",
      "epoch:8 step:7748 [D loss: 0.663498, acc: 64.84%] [G loss: 1.824942]\n",
      "epoch:8 step:7749 [D loss: 0.618130, acc: 65.62%] [G loss: 2.135638]\n",
      "epoch:8 step:7750 [D loss: 0.597710, acc: 68.75%] [G loss: 1.901317]\n",
      "epoch:8 step:7751 [D loss: 0.578980, acc: 68.75%] [G loss: 2.214020]\n",
      "epoch:8 step:7752 [D loss: 0.647001, acc: 65.62%] [G loss: 2.078338]\n",
      "epoch:8 step:7753 [D loss: 0.605604, acc: 69.53%] [G loss: 2.145995]\n",
      "epoch:8 step:7754 [D loss: 0.569780, acc: 71.88%] [G loss: 2.109213]\n",
      "epoch:8 step:7755 [D loss: 0.586079, acc: 64.84%] [G loss: 2.351195]\n",
      "epoch:8 step:7756 [D loss: 0.570160, acc: 74.22%] [G loss: 2.231211]\n",
      "epoch:8 step:7757 [D loss: 0.645872, acc: 64.06%] [G loss: 2.512950]\n",
      "epoch:8 step:7758 [D loss: 0.596994, acc: 63.28%] [G loss: 2.353509]\n",
      "epoch:8 step:7759 [D loss: 0.675992, acc: 59.38%] [G loss: 2.069369]\n",
      "epoch:8 step:7760 [D loss: 0.576785, acc: 67.97%] [G loss: 2.445603]\n",
      "epoch:8 step:7761 [D loss: 0.675120, acc: 65.62%] [G loss: 1.962575]\n",
      "epoch:8 step:7762 [D loss: 0.604418, acc: 69.53%] [G loss: 2.220642]\n",
      "epoch:8 step:7763 [D loss: 0.646633, acc: 65.62%] [G loss: 2.073398]\n",
      "epoch:8 step:7764 [D loss: 0.645921, acc: 67.19%] [G loss: 2.088880]\n",
      "epoch:8 step:7765 [D loss: 0.578077, acc: 72.66%] [G loss: 2.327287]\n",
      "epoch:8 step:7766 [D loss: 0.570581, acc: 71.88%] [G loss: 2.299886]\n",
      "epoch:8 step:7767 [D loss: 0.573797, acc: 70.31%] [G loss: 2.283916]\n",
      "epoch:8 step:7768 [D loss: 0.621256, acc: 64.06%] [G loss: 2.208886]\n",
      "epoch:8 step:7769 [D loss: 0.618523, acc: 64.06%] [G loss: 2.165681]\n",
      "epoch:8 step:7770 [D loss: 0.653258, acc: 66.41%] [G loss: 2.324214]\n",
      "epoch:8 step:7771 [D loss: 0.651709, acc: 61.72%] [G loss: 2.136640]\n",
      "epoch:8 step:7772 [D loss: 0.652433, acc: 60.94%] [G loss: 2.245740]\n",
      "epoch:8 step:7773 [D loss: 0.675998, acc: 61.72%] [G loss: 2.064110]\n",
      "epoch:8 step:7774 [D loss: 0.684933, acc: 55.47%] [G loss: 2.030113]\n",
      "epoch:8 step:7775 [D loss: 0.693954, acc: 60.94%] [G loss: 2.056890]\n",
      "epoch:8 step:7776 [D loss: 0.604084, acc: 64.84%] [G loss: 2.185488]\n",
      "epoch:8 step:7777 [D loss: 0.648528, acc: 62.50%] [G loss: 2.040222]\n",
      "epoch:8 step:7778 [D loss: 0.656812, acc: 59.38%] [G loss: 2.097183]\n",
      "epoch:8 step:7779 [D loss: 0.626386, acc: 66.41%] [G loss: 2.134067]\n",
      "epoch:8 step:7780 [D loss: 0.612669, acc: 66.41%] [G loss: 2.128965]\n",
      "epoch:8 step:7781 [D loss: 0.649721, acc: 62.50%] [G loss: 2.199552]\n",
      "epoch:8 step:7782 [D loss: 0.619619, acc: 66.41%] [G loss: 2.260623]\n",
      "epoch:8 step:7783 [D loss: 0.661799, acc: 62.50%] [G loss: 1.935485]\n",
      "epoch:8 step:7784 [D loss: 0.646963, acc: 63.28%] [G loss: 1.914702]\n",
      "epoch:8 step:7785 [D loss: 0.645508, acc: 61.72%] [G loss: 2.040255]\n",
      "epoch:8 step:7786 [D loss: 0.638076, acc: 60.94%] [G loss: 1.932556]\n",
      "epoch:8 step:7787 [D loss: 0.701966, acc: 60.94%] [G loss: 2.018274]\n",
      "epoch:8 step:7788 [D loss: 0.603631, acc: 61.72%] [G loss: 2.090428]\n",
      "epoch:8 step:7789 [D loss: 0.612064, acc: 67.97%] [G loss: 2.072478]\n",
      "epoch:8 step:7790 [D loss: 0.597226, acc: 68.75%] [G loss: 2.173123]\n",
      "epoch:8 step:7791 [D loss: 0.598478, acc: 65.62%] [G loss: 2.214006]\n",
      "epoch:8 step:7792 [D loss: 0.606750, acc: 67.97%] [G loss: 2.252057]\n",
      "epoch:8 step:7793 [D loss: 0.592419, acc: 69.53%] [G loss: 2.115543]\n",
      "epoch:8 step:7794 [D loss: 0.556536, acc: 76.56%] [G loss: 2.432220]\n",
      "epoch:8 step:7795 [D loss: 0.626104, acc: 68.75%] [G loss: 2.162251]\n",
      "epoch:8 step:7796 [D loss: 0.597218, acc: 67.19%] [G loss: 2.443351]\n",
      "epoch:8 step:7797 [D loss: 0.694854, acc: 53.91%] [G loss: 1.967964]\n",
      "epoch:8 step:7798 [D loss: 0.622820, acc: 65.62%] [G loss: 2.029660]\n",
      "epoch:8 step:7799 [D loss: 0.582380, acc: 67.97%] [G loss: 2.116895]\n",
      "epoch:8 step:7800 [D loss: 0.738637, acc: 51.56%] [G loss: 1.990245]\n",
      "##############\n",
      "[2.58157351 1.19997015 6.59427582 4.98618211 3.95103464 5.81097404\n",
      " 4.63211625 4.97557592 5.03057404 3.71600224]\n",
      "##########\n",
      "epoch:8 step:7801 [D loss: 0.608030, acc: 62.50%] [G loss: 2.308756]\n",
      "epoch:8 step:7802 [D loss: 0.664236, acc: 55.47%] [G loss: 2.005265]\n",
      "epoch:8 step:7803 [D loss: 0.652657, acc: 59.38%] [G loss: 2.093372]\n",
      "epoch:8 step:7804 [D loss: 0.605676, acc: 67.97%] [G loss: 2.147791]\n",
      "epoch:8 step:7805 [D loss: 0.566161, acc: 74.22%] [G loss: 2.197809]\n",
      "epoch:8 step:7806 [D loss: 0.560748, acc: 71.88%] [G loss: 2.151615]\n",
      "epoch:8 step:7807 [D loss: 0.687869, acc: 59.38%] [G loss: 2.235016]\n",
      "epoch:8 step:7808 [D loss: 0.641649, acc: 64.06%] [G loss: 2.719749]\n",
      "epoch:8 step:7809 [D loss: 0.565188, acc: 71.88%] [G loss: 2.338930]\n",
      "epoch:8 step:7810 [D loss: 0.539909, acc: 74.22%] [G loss: 2.584646]\n",
      "epoch:8 step:7811 [D loss: 0.576619, acc: 70.31%] [G loss: 2.460907]\n",
      "epoch:8 step:7812 [D loss: 0.690542, acc: 59.38%] [G loss: 1.881550]\n",
      "epoch:8 step:7813 [D loss: 0.611376, acc: 67.97%] [G loss: 2.171117]\n",
      "epoch:8 step:7814 [D loss: 0.595708, acc: 64.84%] [G loss: 2.296609]\n",
      "epoch:8 step:7815 [D loss: 0.714874, acc: 55.47%] [G loss: 2.090162]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:8 step:7816 [D loss: 0.652303, acc: 60.94%] [G loss: 2.131916]\n",
      "epoch:8 step:7817 [D loss: 0.547693, acc: 75.00%] [G loss: 2.282902]\n",
      "epoch:8 step:7818 [D loss: 0.644690, acc: 60.16%] [G loss: 2.138443]\n",
      "epoch:8 step:7819 [D loss: 0.654474, acc: 61.72%] [G loss: 2.044742]\n",
      "epoch:8 step:7820 [D loss: 0.603382, acc: 66.41%] [G loss: 2.124905]\n",
      "epoch:8 step:7821 [D loss: 0.623466, acc: 67.19%] [G loss: 2.192268]\n",
      "epoch:8 step:7822 [D loss: 0.689926, acc: 60.94%] [G loss: 1.983706]\n",
      "epoch:8 step:7823 [D loss: 0.654245, acc: 60.16%] [G loss: 2.012822]\n",
      "epoch:8 step:7824 [D loss: 0.636429, acc: 62.50%] [G loss: 2.197005]\n",
      "epoch:8 step:7825 [D loss: 0.570592, acc: 73.44%] [G loss: 2.242199]\n",
      "epoch:8 step:7826 [D loss: 0.566180, acc: 71.88%] [G loss: 2.278866]\n",
      "epoch:8 step:7827 [D loss: 0.555797, acc: 71.88%] [G loss: 2.404636]\n",
      "epoch:8 step:7828 [D loss: 0.582204, acc: 67.97%] [G loss: 2.229593]\n",
      "epoch:8 step:7829 [D loss: 0.605186, acc: 71.09%] [G loss: 2.298005]\n",
      "epoch:8 step:7830 [D loss: 0.638725, acc: 60.16%] [G loss: 2.138786]\n",
      "epoch:8 step:7831 [D loss: 0.609452, acc: 65.62%] [G loss: 2.365105]\n",
      "epoch:8 step:7832 [D loss: 0.580319, acc: 71.88%] [G loss: 2.455306]\n",
      "epoch:8 step:7833 [D loss: 0.658342, acc: 64.06%] [G loss: 2.166968]\n",
      "epoch:8 step:7834 [D loss: 0.629110, acc: 65.62%] [G loss: 2.408694]\n",
      "epoch:8 step:7835 [D loss: 0.599946, acc: 69.53%] [G loss: 2.160237]\n",
      "epoch:8 step:7836 [D loss: 0.606432, acc: 67.19%] [G loss: 1.892596]\n",
      "epoch:8 step:7837 [D loss: 0.705572, acc: 58.59%] [G loss: 1.945515]\n",
      "epoch:8 step:7838 [D loss: 0.662334, acc: 59.38%] [G loss: 1.958876]\n",
      "epoch:8 step:7839 [D loss: 0.611323, acc: 65.62%] [G loss: 2.281998]\n",
      "epoch:8 step:7840 [D loss: 0.620371, acc: 70.31%] [G loss: 2.161637]\n",
      "epoch:8 step:7841 [D loss: 0.546576, acc: 72.66%] [G loss: 2.356715]\n",
      "epoch:8 step:7842 [D loss: 0.544683, acc: 75.00%] [G loss: 2.491093]\n",
      "epoch:8 step:7843 [D loss: 0.539726, acc: 70.31%] [G loss: 2.548531]\n",
      "epoch:8 step:7844 [D loss: 0.744657, acc: 61.72%] [G loss: 2.047020]\n",
      "epoch:8 step:7845 [D loss: 0.672329, acc: 64.84%] [G loss: 2.044270]\n",
      "epoch:8 step:7846 [D loss: 0.606997, acc: 65.62%] [G loss: 1.990069]\n",
      "epoch:8 step:7847 [D loss: 0.619752, acc: 63.28%] [G loss: 2.153244]\n",
      "epoch:8 step:7848 [D loss: 0.682515, acc: 64.06%] [G loss: 2.027901]\n",
      "epoch:8 step:7849 [D loss: 0.580955, acc: 74.22%] [G loss: 2.298664]\n",
      "epoch:8 step:7850 [D loss: 0.577664, acc: 65.62%] [G loss: 2.153444]\n",
      "epoch:8 step:7851 [D loss: 0.646518, acc: 62.50%] [G loss: 2.016173]\n",
      "epoch:8 step:7852 [D loss: 0.631959, acc: 66.41%] [G loss: 2.092449]\n",
      "epoch:8 step:7853 [D loss: 0.584075, acc: 71.88%] [G loss: 2.294397]\n",
      "epoch:8 step:7854 [D loss: 0.627332, acc: 62.50%] [G loss: 2.305722]\n",
      "epoch:8 step:7855 [D loss: 0.566076, acc: 73.44%] [G loss: 2.288965]\n",
      "epoch:8 step:7856 [D loss: 0.657685, acc: 55.47%] [G loss: 2.244524]\n",
      "epoch:8 step:7857 [D loss: 0.570640, acc: 70.31%] [G loss: 2.193837]\n",
      "epoch:8 step:7858 [D loss: 0.643117, acc: 57.03%] [G loss: 2.263354]\n",
      "epoch:8 step:7859 [D loss: 0.641191, acc: 60.16%] [G loss: 2.256457]\n",
      "epoch:8 step:7860 [D loss: 0.649869, acc: 60.16%] [G loss: 2.034901]\n",
      "epoch:8 step:7861 [D loss: 0.566213, acc: 70.31%] [G loss: 2.240893]\n",
      "epoch:8 step:7862 [D loss: 0.638306, acc: 62.50%] [G loss: 2.145891]\n",
      "epoch:8 step:7863 [D loss: 0.642230, acc: 65.62%] [G loss: 2.072024]\n",
      "epoch:8 step:7864 [D loss: 0.589918, acc: 66.41%] [G loss: 2.161268]\n",
      "epoch:8 step:7865 [D loss: 0.589079, acc: 68.75%] [G loss: 2.356744]\n",
      "epoch:8 step:7866 [D loss: 0.680120, acc: 62.50%] [G loss: 2.486997]\n",
      "epoch:8 step:7867 [D loss: 0.614199, acc: 67.19%] [G loss: 2.292825]\n",
      "epoch:8 step:7868 [D loss: 0.573655, acc: 71.09%] [G loss: 2.232131]\n",
      "epoch:8 step:7869 [D loss: 0.673305, acc: 60.16%] [G loss: 1.864419]\n",
      "epoch:8 step:7870 [D loss: 0.577872, acc: 69.53%] [G loss: 2.572235]\n",
      "epoch:8 step:7871 [D loss: 0.707799, acc: 53.12%] [G loss: 2.240251]\n",
      "epoch:8 step:7872 [D loss: 0.641371, acc: 61.72%] [G loss: 2.050312]\n",
      "epoch:8 step:7873 [D loss: 0.647762, acc: 61.72%] [G loss: 2.073751]\n",
      "epoch:8 step:7874 [D loss: 0.617913, acc: 64.84%] [G loss: 2.337331]\n",
      "epoch:8 step:7875 [D loss: 0.590176, acc: 69.53%] [G loss: 2.195246]\n",
      "epoch:8 step:7876 [D loss: 0.614853, acc: 66.41%] [G loss: 2.313293]\n",
      "epoch:8 step:7877 [D loss: 0.610106, acc: 69.53%] [G loss: 2.510252]\n",
      "epoch:8 step:7878 [D loss: 0.616989, acc: 72.66%] [G loss: 2.276719]\n",
      "epoch:8 step:7879 [D loss: 0.649853, acc: 60.94%] [G loss: 2.225681]\n",
      "epoch:8 step:7880 [D loss: 0.559491, acc: 71.09%] [G loss: 2.281962]\n",
      "epoch:8 step:7881 [D loss: 0.588560, acc: 64.06%] [G loss: 2.230431]\n",
      "epoch:8 step:7882 [D loss: 0.620797, acc: 65.62%] [G loss: 1.818197]\n",
      "epoch:8 step:7883 [D loss: 0.665507, acc: 59.38%] [G loss: 2.185135]\n",
      "epoch:8 step:7884 [D loss: 0.654029, acc: 64.06%] [G loss: 2.252354]\n",
      "epoch:8 step:7885 [D loss: 0.589099, acc: 63.28%] [G loss: 2.230596]\n",
      "epoch:8 step:7886 [D loss: 0.660121, acc: 61.72%] [G loss: 2.357670]\n",
      "epoch:8 step:7887 [D loss: 0.649552, acc: 60.94%] [G loss: 2.234641]\n",
      "epoch:8 step:7888 [D loss: 0.600956, acc: 63.28%] [G loss: 2.278050]\n",
      "epoch:8 step:7889 [D loss: 0.587199, acc: 64.84%] [G loss: 2.387539]\n",
      "epoch:8 step:7890 [D loss: 0.592014, acc: 66.41%] [G loss: 2.439250]\n",
      "epoch:8 step:7891 [D loss: 0.648451, acc: 61.72%] [G loss: 2.218064]\n",
      "epoch:8 step:7892 [D loss: 0.659532, acc: 58.59%] [G loss: 2.271323]\n",
      "epoch:8 step:7893 [D loss: 0.618451, acc: 67.19%] [G loss: 2.172406]\n",
      "epoch:8 step:7894 [D loss: 0.637491, acc: 60.94%] [G loss: 2.202459]\n",
      "epoch:8 step:7895 [D loss: 0.596146, acc: 64.06%] [G loss: 2.266108]\n",
      "epoch:8 step:7896 [D loss: 0.599092, acc: 67.19%] [G loss: 2.071101]\n",
      "epoch:8 step:7897 [D loss: 0.615092, acc: 64.06%] [G loss: 2.192335]\n",
      "epoch:8 step:7898 [D loss: 0.685157, acc: 59.38%] [G loss: 2.375048]\n",
      "epoch:8 step:7899 [D loss: 0.657773, acc: 61.72%] [G loss: 2.098925]\n",
      "epoch:8 step:7900 [D loss: 0.625445, acc: 64.06%] [G loss: 2.206979]\n",
      "epoch:8 step:7901 [D loss: 0.593040, acc: 65.62%] [G loss: 2.352381]\n",
      "epoch:8 step:7902 [D loss: 0.635801, acc: 67.97%] [G loss: 2.106095]\n",
      "epoch:8 step:7903 [D loss: 0.627462, acc: 60.94%] [G loss: 2.001658]\n",
      "epoch:8 step:7904 [D loss: 0.692623, acc: 61.72%] [G loss: 1.975859]\n",
      "epoch:8 step:7905 [D loss: 0.592791, acc: 67.97%] [G loss: 2.080697]\n",
      "epoch:8 step:7906 [D loss: 0.579204, acc: 65.62%] [G loss: 1.973333]\n",
      "epoch:8 step:7907 [D loss: 0.659383, acc: 60.16%] [G loss: 2.219692]\n",
      "epoch:8 step:7908 [D loss: 0.569924, acc: 69.53%] [G loss: 2.141380]\n",
      "epoch:8 step:7909 [D loss: 0.649890, acc: 66.41%] [G loss: 2.212607]\n",
      "epoch:8 step:7910 [D loss: 0.631726, acc: 67.19%] [G loss: 2.202701]\n",
      "epoch:8 step:7911 [D loss: 0.580809, acc: 71.09%] [G loss: 2.282086]\n",
      "epoch:8 step:7912 [D loss: 0.567398, acc: 71.88%] [G loss: 2.188698]\n",
      "epoch:8 step:7913 [D loss: 0.647342, acc: 60.94%] [G loss: 2.126390]\n",
      "epoch:8 step:7914 [D loss: 0.670244, acc: 62.50%] [G loss: 1.952653]\n",
      "epoch:8 step:7915 [D loss: 0.580113, acc: 67.19%] [G loss: 2.201756]\n",
      "epoch:8 step:7916 [D loss: 0.641020, acc: 63.28%] [G loss: 1.876530]\n",
      "epoch:8 step:7917 [D loss: 0.602575, acc: 67.97%] [G loss: 2.038743]\n",
      "epoch:8 step:7918 [D loss: 0.544989, acc: 69.53%] [G loss: 2.184287]\n",
      "epoch:8 step:7919 [D loss: 0.601724, acc: 69.53%] [G loss: 1.926522]\n",
      "epoch:8 step:7920 [D loss: 0.645756, acc: 63.28%] [G loss: 2.182450]\n",
      "epoch:8 step:7921 [D loss: 0.599250, acc: 67.97%] [G loss: 2.167032]\n",
      "epoch:8 step:7922 [D loss: 0.629096, acc: 68.75%] [G loss: 2.255723]\n",
      "epoch:8 step:7923 [D loss: 0.536480, acc: 74.22%] [G loss: 2.346268]\n",
      "epoch:8 step:7924 [D loss: 0.510467, acc: 75.78%] [G loss: 2.805033]\n",
      "epoch:8 step:7925 [D loss: 0.583845, acc: 66.41%] [G loss: 2.655400]\n",
      "epoch:8 step:7926 [D loss: 0.575157, acc: 67.19%] [G loss: 2.736066]\n",
      "epoch:8 step:7927 [D loss: 0.583925, acc: 67.97%] [G loss: 2.338673]\n",
      "epoch:8 step:7928 [D loss: 0.721062, acc: 58.59%] [G loss: 1.945124]\n",
      "epoch:8 step:7929 [D loss: 0.633400, acc: 65.62%] [G loss: 2.138309]\n",
      "epoch:8 step:7930 [D loss: 0.644553, acc: 63.28%] [G loss: 2.171931]\n",
      "epoch:8 step:7931 [D loss: 0.638137, acc: 67.19%] [G loss: 2.274662]\n",
      "epoch:8 step:7932 [D loss: 0.628526, acc: 63.28%] [G loss: 2.477511]\n",
      "epoch:8 step:7933 [D loss: 0.708422, acc: 51.56%] [G loss: 1.928419]\n",
      "epoch:8 step:7934 [D loss: 0.686451, acc: 58.59%] [G loss: 2.031187]\n",
      "epoch:8 step:7935 [D loss: 0.644310, acc: 63.28%] [G loss: 2.220395]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:8 step:7936 [D loss: 0.602928, acc: 67.19%] [G loss: 2.030810]\n",
      "epoch:8 step:7937 [D loss: 0.633631, acc: 67.19%] [G loss: 2.228523]\n",
      "epoch:8 step:7938 [D loss: 0.691926, acc: 60.94%] [G loss: 1.973062]\n",
      "epoch:8 step:7939 [D loss: 0.618903, acc: 64.06%] [G loss: 2.073206]\n",
      "epoch:8 step:7940 [D loss: 0.668730, acc: 63.28%] [G loss: 1.912389]\n",
      "epoch:8 step:7941 [D loss: 0.623699, acc: 68.75%] [G loss: 2.010692]\n",
      "epoch:8 step:7942 [D loss: 0.633472, acc: 60.94%] [G loss: 2.102561]\n",
      "epoch:8 step:7943 [D loss: 0.617174, acc: 64.84%] [G loss: 2.112513]\n",
      "epoch:8 step:7944 [D loss: 0.584930, acc: 69.53%] [G loss: 2.126112]\n",
      "epoch:8 step:7945 [D loss: 0.647222, acc: 60.94%] [G loss: 2.192706]\n",
      "epoch:8 step:7946 [D loss: 0.627821, acc: 58.59%] [G loss: 2.198841]\n",
      "epoch:8 step:7947 [D loss: 0.612781, acc: 66.41%] [G loss: 2.276181]\n",
      "epoch:8 step:7948 [D loss: 0.624510, acc: 63.28%] [G loss: 2.197126]\n",
      "epoch:8 step:7949 [D loss: 0.645271, acc: 60.16%] [G loss: 2.206554]\n",
      "epoch:8 step:7950 [D loss: 0.669305, acc: 62.50%] [G loss: 2.320300]\n",
      "epoch:8 step:7951 [D loss: 0.568817, acc: 68.75%] [G loss: 2.170095]\n",
      "epoch:8 step:7952 [D loss: 0.594938, acc: 68.75%] [G loss: 2.221698]\n",
      "epoch:8 step:7953 [D loss: 0.665520, acc: 59.38%] [G loss: 2.162977]\n",
      "epoch:8 step:7954 [D loss: 0.678762, acc: 62.50%] [G loss: 2.019186]\n",
      "epoch:8 step:7955 [D loss: 0.629011, acc: 63.28%] [G loss: 1.887307]\n",
      "epoch:8 step:7956 [D loss: 0.679542, acc: 60.16%] [G loss: 2.078373]\n",
      "epoch:8 step:7957 [D loss: 0.648162, acc: 61.72%] [G loss: 2.051969]\n",
      "epoch:8 step:7958 [D loss: 0.706414, acc: 60.16%] [G loss: 2.101956]\n",
      "epoch:8 step:7959 [D loss: 0.651649, acc: 67.19%] [G loss: 1.981933]\n",
      "epoch:8 step:7960 [D loss: 0.638146, acc: 60.16%] [G loss: 2.111758]\n",
      "epoch:8 step:7961 [D loss: 0.694628, acc: 57.81%] [G loss: 2.038281]\n",
      "epoch:8 step:7962 [D loss: 0.627232, acc: 65.62%] [G loss: 2.248336]\n",
      "epoch:8 step:7963 [D loss: 0.686109, acc: 60.94%] [G loss: 2.113369]\n",
      "epoch:8 step:7964 [D loss: 0.562742, acc: 73.44%] [G loss: 2.287117]\n",
      "epoch:8 step:7965 [D loss: 0.594192, acc: 66.41%] [G loss: 2.356273]\n",
      "epoch:8 step:7966 [D loss: 0.631109, acc: 67.97%] [G loss: 2.598771]\n",
      "epoch:8 step:7967 [D loss: 0.630333, acc: 63.28%] [G loss: 2.534566]\n",
      "epoch:8 step:7968 [D loss: 0.594143, acc: 68.75%] [G loss: 2.520803]\n",
      "epoch:8 step:7969 [D loss: 0.654840, acc: 64.84%] [G loss: 2.047062]\n",
      "epoch:8 step:7970 [D loss: 0.592922, acc: 68.75%] [G loss: 2.265025]\n",
      "epoch:8 step:7971 [D loss: 0.646460, acc: 65.62%] [G loss: 2.395221]\n",
      "epoch:8 step:7972 [D loss: 0.629120, acc: 62.50%] [G loss: 2.286734]\n",
      "epoch:8 step:7973 [D loss: 0.645923, acc: 62.50%] [G loss: 1.943457]\n",
      "epoch:8 step:7974 [D loss: 0.686113, acc: 54.69%] [G loss: 1.986452]\n",
      "epoch:8 step:7975 [D loss: 0.660599, acc: 67.19%] [G loss: 2.286161]\n",
      "epoch:8 step:7976 [D loss: 0.601670, acc: 66.41%] [G loss: 2.166061]\n",
      "epoch:8 step:7977 [D loss: 0.492461, acc: 78.12%] [G loss: 2.445599]\n",
      "epoch:8 step:7978 [D loss: 0.642878, acc: 64.84%] [G loss: 1.964893]\n",
      "epoch:8 step:7979 [D loss: 0.688640, acc: 64.84%] [G loss: 2.179905]\n",
      "epoch:8 step:7980 [D loss: 0.624947, acc: 66.41%] [G loss: 2.184236]\n",
      "epoch:8 step:7981 [D loss: 0.629803, acc: 62.50%] [G loss: 2.065719]\n",
      "epoch:8 step:7982 [D loss: 0.656647, acc: 64.84%] [G loss: 2.018604]\n",
      "epoch:8 step:7983 [D loss: 0.660422, acc: 58.59%] [G loss: 2.095152]\n",
      "epoch:8 step:7984 [D loss: 0.557246, acc: 69.53%] [G loss: 2.203480]\n",
      "epoch:8 step:7985 [D loss: 0.619048, acc: 62.50%] [G loss: 2.133479]\n",
      "epoch:8 step:7986 [D loss: 0.597639, acc: 65.62%] [G loss: 2.223773]\n",
      "epoch:8 step:7987 [D loss: 0.600055, acc: 70.31%] [G loss: 2.185715]\n",
      "epoch:8 step:7988 [D loss: 0.643385, acc: 64.06%] [G loss: 2.172079]\n",
      "epoch:8 step:7989 [D loss: 0.643263, acc: 63.28%] [G loss: 1.905617]\n",
      "epoch:8 step:7990 [D loss: 0.586645, acc: 64.06%] [G loss: 2.312987]\n",
      "epoch:8 step:7991 [D loss: 0.623694, acc: 72.66%] [G loss: 2.422765]\n",
      "epoch:8 step:7992 [D loss: 0.622407, acc: 64.84%] [G loss: 2.109839]\n",
      "epoch:8 step:7993 [D loss: 0.576461, acc: 70.31%] [G loss: 2.266554]\n",
      "epoch:8 step:7994 [D loss: 0.568838, acc: 71.09%] [G loss: 2.416460]\n",
      "epoch:8 step:7995 [D loss: 0.554217, acc: 70.31%] [G loss: 2.267622]\n",
      "epoch:8 step:7996 [D loss: 0.676221, acc: 59.38%] [G loss: 2.039448]\n",
      "epoch:8 step:7997 [D loss: 0.700084, acc: 53.12%] [G loss: 1.980864]\n",
      "epoch:8 step:7998 [D loss: 0.654137, acc: 60.94%] [G loss: 2.095020]\n",
      "epoch:8 step:7999 [D loss: 0.614178, acc: 67.19%] [G loss: 2.202959]\n",
      "epoch:8 step:8000 [D loss: 0.563142, acc: 71.09%] [G loss: 2.308156]\n",
      "##############\n",
      "[2.70709974 1.55784299 6.7334603  5.08087111 3.82226946 5.70200015\n",
      " 4.45170212 4.6591293  5.10466356 3.51069869]\n",
      "##########\n",
      "epoch:8 step:8001 [D loss: 0.703629, acc: 53.12%] [G loss: 2.137881]\n",
      "epoch:8 step:8002 [D loss: 0.691268, acc: 53.91%] [G loss: 2.006402]\n",
      "epoch:8 step:8003 [D loss: 0.682402, acc: 60.16%] [G loss: 1.921865]\n",
      "epoch:8 step:8004 [D loss: 0.646181, acc: 60.16%] [G loss: 2.295759]\n",
      "epoch:8 step:8005 [D loss: 0.705291, acc: 61.72%] [G loss: 2.058868]\n",
      "epoch:8 step:8006 [D loss: 0.670631, acc: 59.38%] [G loss: 1.945093]\n",
      "epoch:8 step:8007 [D loss: 0.634613, acc: 60.16%] [G loss: 2.151760]\n",
      "epoch:8 step:8008 [D loss: 0.655109, acc: 66.41%] [G loss: 1.884329]\n",
      "epoch:8 step:8009 [D loss: 0.578288, acc: 67.97%] [G loss: 2.056887]\n",
      "epoch:8 step:8010 [D loss: 0.622586, acc: 63.28%] [G loss: 2.069736]\n",
      "epoch:8 step:8011 [D loss: 0.627761, acc: 63.28%] [G loss: 2.095753]\n",
      "epoch:8 step:8012 [D loss: 0.583634, acc: 69.53%] [G loss: 2.138104]\n",
      "epoch:8 step:8013 [D loss: 0.613459, acc: 64.84%] [G loss: 2.078161]\n",
      "epoch:8 step:8014 [D loss: 0.657368, acc: 64.06%] [G loss: 2.084168]\n",
      "epoch:8 step:8015 [D loss: 0.668695, acc: 57.03%] [G loss: 2.325218]\n",
      "epoch:8 step:8016 [D loss: 0.605899, acc: 67.19%] [G loss: 2.247189]\n",
      "epoch:8 step:8017 [D loss: 0.563848, acc: 66.41%] [G loss: 2.237546]\n",
      "epoch:8 step:8018 [D loss: 0.531096, acc: 75.00%] [G loss: 2.494416]\n",
      "epoch:8 step:8019 [D loss: 0.578634, acc: 68.75%] [G loss: 2.254041]\n",
      "epoch:8 step:8020 [D loss: 0.641552, acc: 65.62%] [G loss: 2.204457]\n",
      "epoch:8 step:8021 [D loss: 0.656916, acc: 67.19%] [G loss: 2.100298]\n",
      "epoch:8 step:8022 [D loss: 0.628784, acc: 62.50%] [G loss: 2.034626]\n",
      "epoch:8 step:8023 [D loss: 0.605047, acc: 63.28%] [G loss: 2.303615]\n",
      "epoch:8 step:8024 [D loss: 0.688712, acc: 56.25%] [G loss: 1.913892]\n",
      "epoch:8 step:8025 [D loss: 0.647513, acc: 59.38%] [G loss: 1.895714]\n",
      "epoch:8 step:8026 [D loss: 0.588068, acc: 72.66%] [G loss: 2.096433]\n",
      "epoch:8 step:8027 [D loss: 0.643388, acc: 65.62%] [G loss: 1.897807]\n",
      "epoch:8 step:8028 [D loss: 0.534868, acc: 73.44%] [G loss: 2.377606]\n",
      "epoch:8 step:8029 [D loss: 0.641756, acc: 59.38%] [G loss: 2.207565]\n",
      "epoch:8 step:8030 [D loss: 0.585979, acc: 69.53%] [G loss: 2.290538]\n",
      "epoch:8 step:8031 [D loss: 0.664234, acc: 61.72%] [G loss: 1.924488]\n",
      "epoch:8 step:8032 [D loss: 0.599523, acc: 67.97%] [G loss: 2.092956]\n",
      "epoch:8 step:8033 [D loss: 0.706013, acc: 58.59%] [G loss: 2.049873]\n",
      "epoch:8 step:8034 [D loss: 0.671118, acc: 60.16%] [G loss: 1.871712]\n",
      "epoch:8 step:8035 [D loss: 0.683090, acc: 57.03%] [G loss: 2.005753]\n",
      "epoch:8 step:8036 [D loss: 0.670907, acc: 61.72%] [G loss: 2.160664]\n",
      "epoch:8 step:8037 [D loss: 0.620915, acc: 63.28%] [G loss: 2.041583]\n",
      "epoch:8 step:8038 [D loss: 0.630489, acc: 62.50%] [G loss: 1.985878]\n",
      "epoch:8 step:8039 [D loss: 0.745388, acc: 58.59%] [G loss: 2.067619]\n",
      "epoch:8 step:8040 [D loss: 0.647581, acc: 61.72%] [G loss: 2.200068]\n",
      "epoch:8 step:8041 [D loss: 0.603650, acc: 64.06%] [G loss: 2.084577]\n",
      "epoch:8 step:8042 [D loss: 0.575701, acc: 75.00%] [G loss: 2.058815]\n",
      "epoch:8 step:8043 [D loss: 0.568757, acc: 70.31%] [G loss: 2.103341]\n",
      "epoch:8 step:8044 [D loss: 0.565081, acc: 69.53%] [G loss: 2.174431]\n",
      "epoch:8 step:8045 [D loss: 0.586072, acc: 69.53%] [G loss: 2.245081]\n",
      "epoch:8 step:8046 [D loss: 0.624052, acc: 65.62%] [G loss: 2.466221]\n",
      "epoch:8 step:8047 [D loss: 0.606950, acc: 67.19%] [G loss: 2.786667]\n",
      "epoch:8 step:8048 [D loss: 0.585299, acc: 75.00%] [G loss: 2.284762]\n",
      "epoch:8 step:8049 [D loss: 0.639114, acc: 60.94%] [G loss: 2.107062]\n",
      "epoch:8 step:8050 [D loss: 0.568235, acc: 70.31%] [G loss: 2.490610]\n",
      "epoch:8 step:8051 [D loss: 0.557516, acc: 72.66%] [G loss: 2.395933]\n",
      "epoch:8 step:8052 [D loss: 0.586135, acc: 67.97%] [G loss: 2.404837]\n",
      "epoch:8 step:8053 [D loss: 0.588914, acc: 66.41%] [G loss: 2.325122]\n",
      "epoch:8 step:8054 [D loss: 0.631561, acc: 64.84%] [G loss: 2.209264]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:8 step:8055 [D loss: 0.639156, acc: 66.41%] [G loss: 1.897402]\n",
      "epoch:8 step:8056 [D loss: 0.656790, acc: 60.16%] [G loss: 2.053948]\n",
      "epoch:8 step:8057 [D loss: 0.599685, acc: 68.75%] [G loss: 1.934410]\n",
      "epoch:8 step:8058 [D loss: 0.713305, acc: 55.47%] [G loss: 2.014368]\n",
      "epoch:8 step:8059 [D loss: 0.642370, acc: 65.62%] [G loss: 2.060385]\n",
      "epoch:8 step:8060 [D loss: 0.541269, acc: 79.69%] [G loss: 2.441305]\n",
      "epoch:8 step:8061 [D loss: 0.624420, acc: 65.62%] [G loss: 2.153436]\n",
      "epoch:8 step:8062 [D loss: 0.715016, acc: 57.03%] [G loss: 2.076018]\n",
      "epoch:8 step:8063 [D loss: 0.602226, acc: 66.41%] [G loss: 2.166244]\n",
      "epoch:8 step:8064 [D loss: 0.660959, acc: 63.28%] [G loss: 2.060498]\n",
      "epoch:8 step:8065 [D loss: 0.637417, acc: 65.62%] [G loss: 1.930400]\n",
      "epoch:8 step:8066 [D loss: 0.631143, acc: 62.50%] [G loss: 2.016274]\n",
      "epoch:8 step:8067 [D loss: 0.669390, acc: 60.94%] [G loss: 2.046947]\n",
      "epoch:8 step:8068 [D loss: 0.663932, acc: 62.50%] [G loss: 1.988980]\n",
      "epoch:8 step:8069 [D loss: 0.576799, acc: 75.00%] [G loss: 1.926042]\n",
      "epoch:8 step:8070 [D loss: 0.551128, acc: 75.00%] [G loss: 2.332375]\n",
      "epoch:8 step:8071 [D loss: 0.673130, acc: 67.19%] [G loss: 2.074992]\n",
      "epoch:8 step:8072 [D loss: 0.580885, acc: 72.66%] [G loss: 2.046664]\n",
      "epoch:8 step:8073 [D loss: 0.657796, acc: 59.38%] [G loss: 2.054865]\n",
      "epoch:8 step:8074 [D loss: 0.675833, acc: 56.25%] [G loss: 2.039768]\n",
      "epoch:8 step:8075 [D loss: 0.685709, acc: 61.72%] [G loss: 2.059462]\n",
      "epoch:8 step:8076 [D loss: 0.643552, acc: 66.41%] [G loss: 1.953886]\n",
      "epoch:8 step:8077 [D loss: 0.617936, acc: 63.28%] [G loss: 2.056836]\n",
      "epoch:8 step:8078 [D loss: 0.586117, acc: 73.44%] [G loss: 2.067675]\n",
      "epoch:8 step:8079 [D loss: 0.674547, acc: 60.16%] [G loss: 2.191776]\n",
      "epoch:8 step:8080 [D loss: 0.639113, acc: 65.62%] [G loss: 2.009532]\n",
      "epoch:8 step:8081 [D loss: 0.609907, acc: 61.72%] [G loss: 2.164031]\n",
      "epoch:8 step:8082 [D loss: 0.651265, acc: 64.06%] [G loss: 2.187715]\n",
      "epoch:8 step:8083 [D loss: 0.640363, acc: 65.62%] [G loss: 2.067364]\n",
      "epoch:8 step:8084 [D loss: 0.681398, acc: 57.81%] [G loss: 2.009973]\n",
      "epoch:8 step:8085 [D loss: 0.635460, acc: 64.84%] [G loss: 2.279758]\n",
      "epoch:8 step:8086 [D loss: 0.672445, acc: 53.12%] [G loss: 2.102450]\n",
      "epoch:8 step:8087 [D loss: 0.671540, acc: 60.16%] [G loss: 2.045778]\n",
      "epoch:8 step:8088 [D loss: 0.550899, acc: 76.56%] [G loss: 2.360019]\n",
      "epoch:8 step:8089 [D loss: 0.604464, acc: 67.97%] [G loss: 2.046990]\n",
      "epoch:8 step:8090 [D loss: 0.640261, acc: 59.38%] [G loss: 1.958376]\n",
      "epoch:8 step:8091 [D loss: 0.651217, acc: 63.28%] [G loss: 2.017032]\n",
      "epoch:8 step:8092 [D loss: 0.596925, acc: 72.66%] [G loss: 1.989094]\n",
      "epoch:8 step:8093 [D loss: 0.676372, acc: 57.03%] [G loss: 1.955422]\n",
      "epoch:8 step:8094 [D loss: 0.591893, acc: 70.31%] [G loss: 2.246974]\n",
      "epoch:8 step:8095 [D loss: 0.611467, acc: 67.97%] [G loss: 1.991352]\n",
      "epoch:8 step:8096 [D loss: 0.681355, acc: 58.59%] [G loss: 2.099565]\n",
      "epoch:8 step:8097 [D loss: 0.667599, acc: 65.62%] [G loss: 2.229306]\n",
      "epoch:8 step:8098 [D loss: 0.651315, acc: 67.19%] [G loss: 2.303033]\n",
      "epoch:8 step:8099 [D loss: 0.593830, acc: 70.31%] [G loss: 2.197962]\n",
      "epoch:8 step:8100 [D loss: 0.621333, acc: 61.72%] [G loss: 2.249368]\n",
      "epoch:8 step:8101 [D loss: 0.582634, acc: 73.44%] [G loss: 2.189453]\n",
      "epoch:8 step:8102 [D loss: 0.669433, acc: 65.62%] [G loss: 2.182982]\n",
      "epoch:8 step:8103 [D loss: 0.631170, acc: 60.16%] [G loss: 1.949113]\n",
      "epoch:8 step:8104 [D loss: 0.643509, acc: 62.50%] [G loss: 1.894320]\n",
      "epoch:8 step:8105 [D loss: 0.581642, acc: 70.31%] [G loss: 2.237280]\n",
      "epoch:8 step:8106 [D loss: 0.606752, acc: 67.19%] [G loss: 2.199286]\n",
      "epoch:8 step:8107 [D loss: 0.649231, acc: 61.72%] [G loss: 1.950966]\n",
      "epoch:8 step:8108 [D loss: 0.589540, acc: 70.31%] [G loss: 2.030082]\n",
      "epoch:8 step:8109 [D loss: 0.555402, acc: 71.09%] [G loss: 2.215941]\n",
      "epoch:8 step:8110 [D loss: 0.566995, acc: 70.31%] [G loss: 2.050057]\n",
      "epoch:8 step:8111 [D loss: 0.652976, acc: 62.50%] [G loss: 2.184682]\n",
      "epoch:8 step:8112 [D loss: 0.648659, acc: 64.84%] [G loss: 2.072514]\n",
      "epoch:8 step:8113 [D loss: 0.676115, acc: 61.72%] [G loss: 1.967455]\n",
      "epoch:8 step:8114 [D loss: 0.648644, acc: 62.50%] [G loss: 2.173108]\n",
      "epoch:8 step:8115 [D loss: 0.621278, acc: 65.62%] [G loss: 2.147849]\n",
      "epoch:8 step:8116 [D loss: 0.648674, acc: 67.19%] [G loss: 2.058061]\n",
      "epoch:8 step:8117 [D loss: 0.710019, acc: 57.81%] [G loss: 1.974173]\n",
      "epoch:8 step:8118 [D loss: 0.682478, acc: 62.50%] [G loss: 2.027237]\n",
      "epoch:8 step:8119 [D loss: 0.616684, acc: 67.97%] [G loss: 2.027616]\n",
      "epoch:8 step:8120 [D loss: 0.552440, acc: 71.88%] [G loss: 2.189037]\n",
      "epoch:8 step:8121 [D loss: 0.633562, acc: 64.84%] [G loss: 2.208512]\n",
      "epoch:8 step:8122 [D loss: 0.584371, acc: 67.19%] [G loss: 2.219648]\n",
      "epoch:8 step:8123 [D loss: 0.594628, acc: 68.75%] [G loss: 2.186756]\n",
      "epoch:8 step:8124 [D loss: 0.670808, acc: 66.41%] [G loss: 2.062126]\n",
      "epoch:8 step:8125 [D loss: 0.649431, acc: 64.06%] [G loss: 2.027508]\n",
      "epoch:8 step:8126 [D loss: 0.629988, acc: 68.75%] [G loss: 2.443938]\n",
      "epoch:8 step:8127 [D loss: 0.602479, acc: 67.97%] [G loss: 2.209526]\n",
      "epoch:8 step:8128 [D loss: 0.573378, acc: 71.88%] [G loss: 2.197820]\n",
      "epoch:8 step:8129 [D loss: 0.559480, acc: 71.09%] [G loss: 2.250288]\n",
      "epoch:8 step:8130 [D loss: 0.599396, acc: 69.53%] [G loss: 2.449269]\n",
      "epoch:8 step:8131 [D loss: 0.594731, acc: 69.53%] [G loss: 2.330795]\n",
      "epoch:8 step:8132 [D loss: 0.652921, acc: 64.06%] [G loss: 2.315103]\n",
      "epoch:8 step:8133 [D loss: 0.607100, acc: 64.84%] [G loss: 2.485991]\n",
      "epoch:8 step:8134 [D loss: 0.571107, acc: 71.88%] [G loss: 2.296278]\n",
      "epoch:8 step:8135 [D loss: 0.645307, acc: 61.72%] [G loss: 2.193191]\n",
      "epoch:8 step:8136 [D loss: 0.611468, acc: 68.75%] [G loss: 2.415319]\n",
      "epoch:8 step:8137 [D loss: 0.587298, acc: 68.75%] [G loss: 2.521040]\n",
      "epoch:8 step:8138 [D loss: 0.591727, acc: 63.28%] [G loss: 2.640045]\n",
      "epoch:8 step:8139 [D loss: 0.666207, acc: 63.28%] [G loss: 2.130392]\n",
      "epoch:8 step:8140 [D loss: 0.639396, acc: 61.72%] [G loss: 2.258748]\n",
      "epoch:8 step:8141 [D loss: 0.595295, acc: 68.75%] [G loss: 2.503850]\n",
      "epoch:8 step:8142 [D loss: 0.609372, acc: 67.97%] [G loss: 2.187420]\n",
      "epoch:8 step:8143 [D loss: 0.650298, acc: 68.75%] [G loss: 2.578302]\n",
      "epoch:8 step:8144 [D loss: 0.555609, acc: 72.66%] [G loss: 2.582355]\n",
      "epoch:8 step:8145 [D loss: 0.503638, acc: 78.91%] [G loss: 2.555961]\n",
      "epoch:8 step:8146 [D loss: 0.648587, acc: 65.62%] [G loss: 2.299600]\n",
      "epoch:8 step:8147 [D loss: 0.587529, acc: 67.97%] [G loss: 2.152932]\n",
      "epoch:8 step:8148 [D loss: 0.607954, acc: 64.06%] [G loss: 2.113216]\n",
      "epoch:8 step:8149 [D loss: 0.554176, acc: 73.44%] [G loss: 2.376713]\n",
      "epoch:8 step:8150 [D loss: 0.641813, acc: 67.97%] [G loss: 2.299331]\n",
      "epoch:8 step:8151 [D loss: 0.656014, acc: 60.94%] [G loss: 2.355772]\n",
      "epoch:8 step:8152 [D loss: 0.591279, acc: 70.31%] [G loss: 2.245942]\n",
      "epoch:8 step:8153 [D loss: 0.658061, acc: 60.16%] [G loss: 2.049238]\n",
      "epoch:8 step:8154 [D loss: 0.644362, acc: 60.94%] [G loss: 2.098681]\n",
      "epoch:8 step:8155 [D loss: 0.625575, acc: 70.31%] [G loss: 2.182155]\n",
      "epoch:8 step:8156 [D loss: 0.571063, acc: 71.88%] [G loss: 2.143614]\n",
      "epoch:8 step:8157 [D loss: 0.617304, acc: 65.62%] [G loss: 2.047112]\n",
      "epoch:8 step:8158 [D loss: 0.626230, acc: 66.41%] [G loss: 2.274634]\n",
      "epoch:8 step:8159 [D loss: 0.668778, acc: 63.28%] [G loss: 2.161832]\n",
      "epoch:8 step:8160 [D loss: 0.645545, acc: 64.06%] [G loss: 2.016821]\n",
      "epoch:8 step:8161 [D loss: 0.572548, acc: 68.75%] [G loss: 2.133414]\n",
      "epoch:8 step:8162 [D loss: 0.635920, acc: 64.06%] [G loss: 2.125050]\n",
      "epoch:8 step:8163 [D loss: 0.653956, acc: 59.38%] [G loss: 2.108685]\n",
      "epoch:8 step:8164 [D loss: 0.618535, acc: 66.41%] [G loss: 2.200820]\n",
      "epoch:8 step:8165 [D loss: 0.614914, acc: 59.38%] [G loss: 1.985853]\n",
      "epoch:8 step:8166 [D loss: 0.621511, acc: 67.97%] [G loss: 2.050012]\n",
      "epoch:8 step:8167 [D loss: 0.704940, acc: 61.72%] [G loss: 2.055895]\n",
      "epoch:8 step:8168 [D loss: 0.632494, acc: 60.16%] [G loss: 2.038303]\n",
      "epoch:8 step:8169 [D loss: 0.671969, acc: 60.16%] [G loss: 2.168933]\n",
      "epoch:8 step:8170 [D loss: 0.619652, acc: 62.50%] [G loss: 2.107797]\n",
      "epoch:8 step:8171 [D loss: 0.585629, acc: 71.09%] [G loss: 2.127359]\n",
      "epoch:8 step:8172 [D loss: 0.675132, acc: 63.28%] [G loss: 2.098805]\n",
      "epoch:8 step:8173 [D loss: 0.563695, acc: 74.22%] [G loss: 2.231180]\n",
      "epoch:8 step:8174 [D loss: 0.576280, acc: 71.09%] [G loss: 2.271021]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:8 step:8175 [D loss: 0.622048, acc: 62.50%] [G loss: 2.200748]\n",
      "epoch:8 step:8176 [D loss: 0.561861, acc: 69.53%] [G loss: 2.198268]\n",
      "epoch:8 step:8177 [D loss: 0.526266, acc: 78.91%] [G loss: 2.283306]\n",
      "epoch:8 step:8178 [D loss: 0.651316, acc: 64.06%] [G loss: 2.100780]\n",
      "epoch:8 step:8179 [D loss: 0.645474, acc: 66.41%] [G loss: 1.933734]\n",
      "epoch:8 step:8180 [D loss: 0.619392, acc: 67.19%] [G loss: 2.065728]\n",
      "epoch:8 step:8181 [D loss: 0.566583, acc: 69.53%] [G loss: 2.286762]\n",
      "epoch:8 step:8182 [D loss: 0.649257, acc: 61.72%] [G loss: 2.228580]\n",
      "epoch:8 step:8183 [D loss: 0.615233, acc: 66.41%] [G loss: 2.126895]\n",
      "epoch:8 step:8184 [D loss: 0.639471, acc: 66.41%] [G loss: 2.170683]\n",
      "epoch:8 step:8185 [D loss: 0.601852, acc: 65.62%] [G loss: 2.140445]\n",
      "epoch:8 step:8186 [D loss: 0.589044, acc: 71.88%] [G loss: 2.654099]\n",
      "epoch:8 step:8187 [D loss: 0.606229, acc: 66.41%] [G loss: 2.292070]\n",
      "epoch:8 step:8188 [D loss: 0.591547, acc: 61.72%] [G loss: 2.239235]\n",
      "epoch:8 step:8189 [D loss: 0.597837, acc: 69.53%] [G loss: 2.324944]\n",
      "epoch:8 step:8190 [D loss: 0.566244, acc: 71.09%] [G loss: 2.234186]\n",
      "epoch:8 step:8191 [D loss: 0.649172, acc: 64.06%] [G loss: 2.566047]\n",
      "epoch:8 step:8192 [D loss: 0.659093, acc: 65.62%] [G loss: 2.119101]\n",
      "epoch:8 step:8193 [D loss: 0.673438, acc: 62.50%] [G loss: 2.155522]\n",
      "epoch:8 step:8194 [D loss: 0.661030, acc: 61.72%] [G loss: 2.137153]\n",
      "epoch:8 step:8195 [D loss: 0.570457, acc: 71.88%] [G loss: 2.224920]\n",
      "epoch:8 step:8196 [D loss: 0.597133, acc: 69.53%] [G loss: 2.113821]\n",
      "epoch:8 step:8197 [D loss: 0.655431, acc: 60.94%] [G loss: 1.890423]\n",
      "epoch:8 step:8198 [D loss: 0.726298, acc: 54.69%] [G loss: 1.956608]\n",
      "epoch:8 step:8199 [D loss: 0.673455, acc: 62.50%] [G loss: 1.938546]\n",
      "epoch:8 step:8200 [D loss: 0.627550, acc: 60.16%] [G loss: 2.117099]\n",
      "##############\n",
      "[2.50894368 1.2243871  6.55746914 4.77762436 4.09119763 5.72410087\n",
      " 4.57853358 4.89852868 4.89145726 3.73161081]\n",
      "##########\n",
      "epoch:8 step:8201 [D loss: 0.602296, acc: 69.53%] [G loss: 2.066504]\n",
      "epoch:8 step:8202 [D loss: 0.639413, acc: 66.41%] [G loss: 2.186999]\n",
      "epoch:8 step:8203 [D loss: 0.568864, acc: 73.44%] [G loss: 2.298689]\n",
      "epoch:8 step:8204 [D loss: 0.581885, acc: 68.75%] [G loss: 2.318932]\n",
      "epoch:8 step:8205 [D loss: 0.605944, acc: 66.41%] [G loss: 2.266656]\n",
      "epoch:8 step:8206 [D loss: 0.596593, acc: 71.09%] [G loss: 2.244657]\n",
      "epoch:8 step:8207 [D loss: 0.698239, acc: 60.16%] [G loss: 2.205255]\n",
      "epoch:8 step:8208 [D loss: 0.589291, acc: 71.88%] [G loss: 2.246864]\n",
      "epoch:8 step:8209 [D loss: 0.617079, acc: 70.31%] [G loss: 2.208857]\n",
      "epoch:8 step:8210 [D loss: 0.654270, acc: 58.59%] [G loss: 1.982473]\n",
      "epoch:8 step:8211 [D loss: 0.649604, acc: 65.62%] [G loss: 2.220406]\n",
      "epoch:8 step:8212 [D loss: 0.617825, acc: 65.62%] [G loss: 1.971466]\n",
      "epoch:8 step:8213 [D loss: 0.625008, acc: 74.22%] [G loss: 2.099465]\n",
      "epoch:8 step:8214 [D loss: 0.633245, acc: 62.50%] [G loss: 2.019416]\n",
      "epoch:8 step:8215 [D loss: 0.588337, acc: 69.53%] [G loss: 2.187004]\n",
      "epoch:8 step:8216 [D loss: 0.625614, acc: 64.84%] [G loss: 2.316488]\n",
      "epoch:8 step:8217 [D loss: 0.600236, acc: 70.31%] [G loss: 2.247094]\n",
      "epoch:8 step:8218 [D loss: 0.606846, acc: 66.41%] [G loss: 2.076221]\n",
      "epoch:8 step:8219 [D loss: 0.653253, acc: 60.16%] [G loss: 2.014850]\n",
      "epoch:8 step:8220 [D loss: 0.666869, acc: 61.72%] [G loss: 2.115796]\n",
      "epoch:8 step:8221 [D loss: 0.608739, acc: 71.88%] [G loss: 2.156334]\n",
      "epoch:8 step:8222 [D loss: 0.647538, acc: 65.62%] [G loss: 2.391602]\n",
      "epoch:8 step:8223 [D loss: 0.635504, acc: 60.94%] [G loss: 2.048546]\n",
      "epoch:8 step:8224 [D loss: 0.606416, acc: 67.19%] [G loss: 2.099962]\n",
      "epoch:8 step:8225 [D loss: 0.599865, acc: 69.53%] [G loss: 2.203761]\n",
      "epoch:8 step:8226 [D loss: 0.682613, acc: 60.94%] [G loss: 2.136125]\n",
      "epoch:8 step:8227 [D loss: 0.669957, acc: 60.94%] [G loss: 2.209126]\n",
      "epoch:8 step:8228 [D loss: 0.636267, acc: 65.62%] [G loss: 2.157188]\n",
      "epoch:8 step:8229 [D loss: 0.623866, acc: 66.41%] [G loss: 2.266716]\n",
      "epoch:8 step:8230 [D loss: 0.674684, acc: 60.94%] [G loss: 2.025498]\n",
      "epoch:8 step:8231 [D loss: 0.609523, acc: 70.31%] [G loss: 2.122449]\n",
      "epoch:8 step:8232 [D loss: 0.631391, acc: 65.62%] [G loss: 2.051956]\n",
      "epoch:8 step:8233 [D loss: 0.593242, acc: 66.41%] [G loss: 2.079084]\n",
      "epoch:8 step:8234 [D loss: 0.658212, acc: 61.72%] [G loss: 1.838579]\n",
      "epoch:8 step:8235 [D loss: 0.621081, acc: 64.84%] [G loss: 1.877841]\n",
      "epoch:8 step:8236 [D loss: 0.681363, acc: 60.16%] [G loss: 1.985422]\n",
      "epoch:8 step:8237 [D loss: 0.606395, acc: 66.41%] [G loss: 2.181283]\n",
      "epoch:8 step:8238 [D loss: 0.667955, acc: 59.38%] [G loss: 2.063345]\n",
      "epoch:8 step:8239 [D loss: 0.627291, acc: 60.94%] [G loss: 1.994141]\n",
      "epoch:8 step:8240 [D loss: 0.633818, acc: 60.94%] [G loss: 2.006034]\n",
      "epoch:8 step:8241 [D loss: 0.617775, acc: 63.28%] [G loss: 2.075366]\n",
      "epoch:8 step:8242 [D loss: 0.541656, acc: 77.34%] [G loss: 2.269920]\n",
      "epoch:8 step:8243 [D loss: 0.598385, acc: 70.31%] [G loss: 2.158417]\n",
      "epoch:8 step:8244 [D loss: 0.688766, acc: 59.38%] [G loss: 2.005512]\n",
      "epoch:8 step:8245 [D loss: 0.661767, acc: 60.94%] [G loss: 1.949698]\n",
      "epoch:8 step:8246 [D loss: 0.677510, acc: 58.59%] [G loss: 1.880695]\n",
      "epoch:8 step:8247 [D loss: 0.709001, acc: 63.28%] [G loss: 1.950665]\n",
      "epoch:8 step:8248 [D loss: 0.626300, acc: 63.28%] [G loss: 1.856820]\n",
      "epoch:8 step:8249 [D loss: 0.645819, acc: 70.31%] [G loss: 2.098116]\n",
      "epoch:8 step:8250 [D loss: 0.557748, acc: 74.22%] [G loss: 2.109655]\n",
      "epoch:8 step:8251 [D loss: 0.583230, acc: 69.53%] [G loss: 2.148305]\n",
      "epoch:8 step:8252 [D loss: 0.582069, acc: 68.75%] [G loss: 2.229785]\n",
      "epoch:8 step:8253 [D loss: 0.620693, acc: 64.84%] [G loss: 2.168403]\n",
      "epoch:8 step:8254 [D loss: 0.609970, acc: 68.75%] [G loss: 1.934247]\n",
      "epoch:8 step:8255 [D loss: 0.656590, acc: 62.50%] [G loss: 2.078016]\n",
      "epoch:8 step:8256 [D loss: 0.607933, acc: 67.97%] [G loss: 2.111221]\n",
      "epoch:8 step:8257 [D loss: 0.666437, acc: 57.81%] [G loss: 2.196293]\n",
      "epoch:8 step:8258 [D loss: 0.645487, acc: 60.94%] [G loss: 1.982099]\n",
      "epoch:8 step:8259 [D loss: 0.637889, acc: 63.28%] [G loss: 2.122905]\n",
      "epoch:8 step:8260 [D loss: 0.594368, acc: 68.75%] [G loss: 1.987114]\n",
      "epoch:8 step:8261 [D loss: 0.686357, acc: 57.81%] [G loss: 1.921916]\n",
      "epoch:8 step:8262 [D loss: 0.669586, acc: 59.38%] [G loss: 2.091152]\n",
      "epoch:8 step:8263 [D loss: 0.615815, acc: 66.41%] [G loss: 2.172147]\n",
      "epoch:8 step:8264 [D loss: 0.635474, acc: 63.28%] [G loss: 1.880351]\n",
      "epoch:8 step:8265 [D loss: 0.567599, acc: 65.62%] [G loss: 2.282949]\n",
      "epoch:8 step:8266 [D loss: 0.634001, acc: 64.84%] [G loss: 2.025436]\n",
      "epoch:8 step:8267 [D loss: 0.645214, acc: 60.94%] [G loss: 1.987667]\n",
      "epoch:8 step:8268 [D loss: 0.661832, acc: 60.16%] [G loss: 2.204442]\n",
      "epoch:8 step:8269 [D loss: 0.644722, acc: 67.97%] [G loss: 2.118796]\n",
      "epoch:8 step:8270 [D loss: 0.613987, acc: 64.84%] [G loss: 2.611038]\n",
      "epoch:8 step:8271 [D loss: 0.603953, acc: 69.53%] [G loss: 2.433638]\n",
      "epoch:8 step:8272 [D loss: 0.614208, acc: 65.62%] [G loss: 2.230668]\n",
      "epoch:8 step:8273 [D loss: 0.620152, acc: 71.88%] [G loss: 2.146303]\n",
      "epoch:8 step:8274 [D loss: 0.684394, acc: 56.25%] [G loss: 2.010470]\n",
      "epoch:8 step:8275 [D loss: 0.606122, acc: 67.19%] [G loss: 2.164089]\n",
      "epoch:8 step:8276 [D loss: 0.600257, acc: 69.53%] [G loss: 2.195071]\n",
      "epoch:8 step:8277 [D loss: 0.595588, acc: 70.31%] [G loss: 2.432943]\n",
      "epoch:8 step:8278 [D loss: 0.614861, acc: 69.53%] [G loss: 2.277013]\n",
      "epoch:8 step:8279 [D loss: 0.655741, acc: 60.94%] [G loss: 2.137520]\n",
      "epoch:8 step:8280 [D loss: 0.679227, acc: 57.81%] [G loss: 1.982220]\n",
      "epoch:8 step:8281 [D loss: 0.590611, acc: 67.19%] [G loss: 2.149654]\n",
      "epoch:8 step:8282 [D loss: 0.593434, acc: 64.84%] [G loss: 2.262697]\n",
      "epoch:8 step:8283 [D loss: 0.646411, acc: 64.84%] [G loss: 2.062279]\n",
      "epoch:8 step:8284 [D loss: 0.697469, acc: 56.25%] [G loss: 1.944600]\n",
      "epoch:8 step:8285 [D loss: 0.636661, acc: 64.06%] [G loss: 1.991191]\n",
      "epoch:8 step:8286 [D loss: 0.689711, acc: 66.41%] [G loss: 2.125437]\n",
      "epoch:8 step:8287 [D loss: 0.578679, acc: 71.88%] [G loss: 1.953618]\n",
      "epoch:8 step:8288 [D loss: 0.572270, acc: 72.66%] [G loss: 2.355669]\n",
      "epoch:8 step:8289 [D loss: 0.643140, acc: 61.72%] [G loss: 2.114728]\n",
      "epoch:8 step:8290 [D loss: 0.707903, acc: 53.91%] [G loss: 1.940813]\n",
      "epoch:8 step:8291 [D loss: 0.616341, acc: 65.62%] [G loss: 2.090260]\n",
      "epoch:8 step:8292 [D loss: 0.605438, acc: 66.41%] [G loss: 2.187617]\n",
      "epoch:8 step:8293 [D loss: 0.626001, acc: 64.06%] [G loss: 1.986819]\n",
      "epoch:8 step:8294 [D loss: 0.603921, acc: 69.53%] [G loss: 2.103853]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:8 step:8295 [D loss: 0.582968, acc: 72.66%] [G loss: 2.173784]\n",
      "epoch:8 step:8296 [D loss: 0.658310, acc: 53.12%] [G loss: 1.843282]\n",
      "epoch:8 step:8297 [D loss: 0.653676, acc: 60.16%] [G loss: 1.977406]\n",
      "epoch:8 step:8298 [D loss: 0.663297, acc: 62.50%] [G loss: 2.268235]\n",
      "epoch:8 step:8299 [D loss: 0.610926, acc: 64.84%] [G loss: 2.140214]\n",
      "epoch:8 step:8300 [D loss: 0.655254, acc: 64.06%] [G loss: 2.169703]\n",
      "epoch:8 step:8301 [D loss: 0.667522, acc: 64.06%] [G loss: 2.426392]\n",
      "epoch:8 step:8302 [D loss: 0.587554, acc: 72.66%] [G loss: 2.273579]\n",
      "epoch:8 step:8303 [D loss: 0.630580, acc: 64.84%] [G loss: 2.178645]\n",
      "epoch:8 step:8304 [D loss: 0.592742, acc: 68.75%] [G loss: 2.207019]\n",
      "epoch:8 step:8305 [D loss: 0.582071, acc: 71.88%] [G loss: 2.303740]\n",
      "epoch:8 step:8306 [D loss: 0.633734, acc: 64.06%] [G loss: 2.261183]\n",
      "epoch:8 step:8307 [D loss: 0.623324, acc: 67.19%] [G loss: 2.093043]\n",
      "epoch:8 step:8308 [D loss: 0.657209, acc: 65.62%] [G loss: 2.013389]\n",
      "epoch:8 step:8309 [D loss: 0.673748, acc: 63.28%] [G loss: 2.029566]\n",
      "epoch:8 step:8310 [D loss: 0.659493, acc: 63.28%] [G loss: 2.027614]\n",
      "epoch:8 step:8311 [D loss: 0.620094, acc: 62.50%] [G loss: 2.508201]\n",
      "epoch:8 step:8312 [D loss: 0.570166, acc: 72.66%] [G loss: 2.173321]\n",
      "epoch:8 step:8313 [D loss: 0.601548, acc: 69.53%] [G loss: 2.089324]\n",
      "epoch:8 step:8314 [D loss: 0.688992, acc: 63.28%] [G loss: 2.072313]\n",
      "epoch:8 step:8315 [D loss: 0.606220, acc: 67.19%] [G loss: 2.158007]\n",
      "epoch:8 step:8316 [D loss: 0.691131, acc: 56.25%] [G loss: 1.961449]\n",
      "epoch:8 step:8317 [D loss: 0.650040, acc: 63.28%] [G loss: 2.107293]\n",
      "epoch:8 step:8318 [D loss: 0.545952, acc: 72.66%] [G loss: 2.111280]\n",
      "epoch:8 step:8319 [D loss: 0.631871, acc: 64.06%] [G loss: 2.186171]\n",
      "epoch:8 step:8320 [D loss: 0.676455, acc: 60.94%] [G loss: 2.081489]\n",
      "epoch:8 step:8321 [D loss: 0.537113, acc: 75.00%] [G loss: 2.236162]\n",
      "epoch:8 step:8322 [D loss: 0.716741, acc: 60.94%] [G loss: 2.098288]\n",
      "epoch:8 step:8323 [D loss: 0.655909, acc: 60.94%] [G loss: 2.065087]\n",
      "epoch:8 step:8324 [D loss: 0.647763, acc: 60.16%] [G loss: 1.987321]\n",
      "epoch:8 step:8325 [D loss: 0.638792, acc: 61.72%] [G loss: 2.101607]\n",
      "epoch:8 step:8326 [D loss: 0.573861, acc: 69.53%] [G loss: 2.059528]\n",
      "epoch:8 step:8327 [D loss: 0.624742, acc: 64.84%] [G loss: 2.174955]\n",
      "epoch:8 step:8328 [D loss: 0.555767, acc: 74.22%] [G loss: 1.973654]\n",
      "epoch:8 step:8329 [D loss: 0.629413, acc: 60.94%] [G loss: 2.067213]\n",
      "epoch:8 step:8330 [D loss: 0.628344, acc: 64.06%] [G loss: 2.119178]\n",
      "epoch:8 step:8331 [D loss: 0.660617, acc: 59.38%] [G loss: 2.022662]\n",
      "epoch:8 step:8332 [D loss: 0.628927, acc: 64.06%] [G loss: 2.050108]\n",
      "epoch:8 step:8333 [D loss: 0.580063, acc: 73.44%] [G loss: 2.123173]\n",
      "epoch:8 step:8334 [D loss: 0.596467, acc: 62.50%] [G loss: 2.219040]\n",
      "epoch:8 step:8335 [D loss: 0.571887, acc: 73.44%] [G loss: 2.134300]\n",
      "epoch:8 step:8336 [D loss: 0.590288, acc: 66.41%] [G loss: 2.073231]\n",
      "epoch:8 step:8337 [D loss: 0.621545, acc: 65.62%] [G loss: 2.023156]\n",
      "epoch:8 step:8338 [D loss: 0.575409, acc: 67.19%] [G loss: 2.151639]\n",
      "epoch:8 step:8339 [D loss: 0.689474, acc: 57.03%] [G loss: 2.117126]\n",
      "epoch:8 step:8340 [D loss: 0.644470, acc: 64.84%] [G loss: 2.249269]\n",
      "epoch:8 step:8341 [D loss: 0.647279, acc: 60.16%] [G loss: 2.123075]\n",
      "epoch:8 step:8342 [D loss: 0.672490, acc: 62.50%] [G loss: 2.052084]\n",
      "epoch:8 step:8343 [D loss: 0.662111, acc: 60.16%] [G loss: 2.177468]\n",
      "epoch:8 step:8344 [D loss: 0.655569, acc: 60.94%] [G loss: 2.108857]\n",
      "epoch:8 step:8345 [D loss: 0.566203, acc: 74.22%] [G loss: 2.190120]\n",
      "epoch:8 step:8346 [D loss: 0.648957, acc: 61.72%] [G loss: 2.319384]\n",
      "epoch:8 step:8347 [D loss: 0.609240, acc: 61.72%] [G loss: 2.184613]\n",
      "epoch:8 step:8348 [D loss: 0.585156, acc: 66.41%] [G loss: 2.031386]\n",
      "epoch:8 step:8349 [D loss: 0.646642, acc: 61.72%] [G loss: 2.060954]\n",
      "epoch:8 step:8350 [D loss: 0.599840, acc: 71.88%] [G loss: 2.172721]\n",
      "epoch:8 step:8351 [D loss: 0.672820, acc: 55.47%] [G loss: 1.996147]\n",
      "epoch:8 step:8352 [D loss: 0.601821, acc: 67.19%] [G loss: 2.027699]\n",
      "epoch:8 step:8353 [D loss: 0.613987, acc: 66.41%] [G loss: 2.095978]\n",
      "epoch:8 step:8354 [D loss: 0.693389, acc: 56.25%] [G loss: 2.028498]\n",
      "epoch:8 step:8355 [D loss: 0.645814, acc: 61.72%] [G loss: 2.041856]\n",
      "epoch:8 step:8356 [D loss: 0.601279, acc: 65.62%] [G loss: 2.214993]\n",
      "epoch:8 step:8357 [D loss: 0.651883, acc: 57.81%] [G loss: 2.031371]\n",
      "epoch:8 step:8358 [D loss: 0.609754, acc: 69.53%] [G loss: 2.172849]\n",
      "epoch:8 step:8359 [D loss: 0.676144, acc: 59.38%] [G loss: 1.980330]\n",
      "epoch:8 step:8360 [D loss: 0.574557, acc: 69.53%] [G loss: 2.234994]\n",
      "epoch:8 step:8361 [D loss: 0.589058, acc: 66.41%] [G loss: 2.370216]\n",
      "epoch:8 step:8362 [D loss: 0.624246, acc: 61.72%] [G loss: 1.978436]\n",
      "epoch:8 step:8363 [D loss: 0.639945, acc: 63.28%] [G loss: 2.293494]\n",
      "epoch:8 step:8364 [D loss: 0.633092, acc: 66.41%] [G loss: 2.388593]\n",
      "epoch:8 step:8365 [D loss: 0.610936, acc: 67.19%] [G loss: 2.075217]\n",
      "epoch:8 step:8366 [D loss: 0.614789, acc: 68.75%] [G loss: 2.143353]\n",
      "epoch:8 step:8367 [D loss: 0.663280, acc: 60.94%] [G loss: 1.978091]\n",
      "epoch:8 step:8368 [D loss: 0.662685, acc: 57.81%] [G loss: 2.142029]\n",
      "epoch:8 step:8369 [D loss: 0.657272, acc: 61.72%] [G loss: 1.961560]\n",
      "epoch:8 step:8370 [D loss: 0.665603, acc: 60.94%] [G loss: 2.096463]\n",
      "epoch:8 step:8371 [D loss: 0.554652, acc: 67.97%] [G loss: 2.260465]\n",
      "epoch:8 step:8372 [D loss: 0.616628, acc: 64.84%] [G loss: 2.120856]\n",
      "epoch:8 step:8373 [D loss: 0.724177, acc: 57.81%] [G loss: 2.011776]\n",
      "epoch:8 step:8374 [D loss: 0.611326, acc: 68.75%] [G loss: 2.141430]\n",
      "epoch:8 step:8375 [D loss: 0.645485, acc: 57.03%] [G loss: 2.011849]\n",
      "epoch:8 step:8376 [D loss: 0.590188, acc: 72.66%] [G loss: 2.037368]\n",
      "epoch:8 step:8377 [D loss: 0.630526, acc: 69.53%] [G loss: 2.287668]\n",
      "epoch:8 step:8378 [D loss: 0.683342, acc: 61.72%] [G loss: 2.215660]\n",
      "epoch:8 step:8379 [D loss: 0.615737, acc: 64.06%] [G loss: 2.208202]\n",
      "epoch:8 step:8380 [D loss: 0.592940, acc: 74.22%] [G loss: 2.379312]\n",
      "epoch:8 step:8381 [D loss: 0.620409, acc: 70.31%] [G loss: 2.077899]\n",
      "epoch:8 step:8382 [D loss: 0.636816, acc: 65.62%] [G loss: 2.378732]\n",
      "epoch:8 step:8383 [D loss: 0.593887, acc: 69.53%] [G loss: 2.329395]\n",
      "epoch:8 step:8384 [D loss: 0.643155, acc: 71.09%] [G loss: 2.157176]\n",
      "epoch:8 step:8385 [D loss: 0.579431, acc: 71.09%] [G loss: 2.357507]\n",
      "epoch:8 step:8386 [D loss: 0.662178, acc: 59.38%] [G loss: 2.263017]\n",
      "epoch:8 step:8387 [D loss: 0.687080, acc: 59.38%] [G loss: 2.094138]\n",
      "epoch:8 step:8388 [D loss: 0.655712, acc: 60.16%] [G loss: 2.069055]\n",
      "epoch:8 step:8389 [D loss: 0.651388, acc: 59.38%] [G loss: 2.062572]\n",
      "epoch:8 step:8390 [D loss: 0.623294, acc: 65.62%] [G loss: 2.316865]\n",
      "epoch:8 step:8391 [D loss: 0.651050, acc: 67.97%] [G loss: 2.165454]\n",
      "epoch:8 step:8392 [D loss: 0.632199, acc: 59.38%] [G loss: 2.040656]\n",
      "epoch:8 step:8393 [D loss: 0.612788, acc: 67.19%] [G loss: 2.049949]\n",
      "epoch:8 step:8394 [D loss: 0.607987, acc: 70.31%] [G loss: 2.247475]\n",
      "epoch:8 step:8395 [D loss: 0.571796, acc: 69.53%] [G loss: 2.333869]\n",
      "epoch:8 step:8396 [D loss: 0.610725, acc: 67.19%] [G loss: 2.225395]\n",
      "epoch:8 step:8397 [D loss: 0.658280, acc: 60.94%] [G loss: 2.050654]\n",
      "epoch:8 step:8398 [D loss: 0.678329, acc: 60.16%] [G loss: 2.012893]\n",
      "epoch:8 step:8399 [D loss: 0.641605, acc: 61.72%] [G loss: 2.260768]\n",
      "epoch:8 step:8400 [D loss: 0.597479, acc: 64.84%] [G loss: 2.211365]\n",
      "##############\n",
      "[2.39233109 1.54873871 6.31043191 5.04351413 3.84462846 5.73590018\n",
      " 4.49915754 4.84011676 4.74386702 3.54236038]\n",
      "##########\n",
      "epoch:8 step:8401 [D loss: 0.637349, acc: 63.28%] [G loss: 2.039426]\n",
      "epoch:8 step:8402 [D loss: 0.652304, acc: 61.72%] [G loss: 2.255543]\n",
      "epoch:8 step:8403 [D loss: 0.612309, acc: 65.62%] [G loss: 2.025485]\n",
      "epoch:8 step:8404 [D loss: 0.614243, acc: 64.06%] [G loss: 2.150291]\n",
      "epoch:8 step:8405 [D loss: 0.647534, acc: 64.84%] [G loss: 2.288220]\n",
      "epoch:8 step:8406 [D loss: 0.630267, acc: 71.09%] [G loss: 2.052396]\n",
      "epoch:8 step:8407 [D loss: 0.651218, acc: 61.72%] [G loss: 2.261326]\n",
      "epoch:8 step:8408 [D loss: 0.591346, acc: 67.19%] [G loss: 2.540865]\n",
      "epoch:8 step:8409 [D loss: 0.656336, acc: 62.50%] [G loss: 2.252862]\n",
      "epoch:8 step:8410 [D loss: 0.606258, acc: 64.06%] [G loss: 2.139317]\n",
      "epoch:8 step:8411 [D loss: 0.690188, acc: 65.62%] [G loss: 2.132621]\n",
      "epoch:8 step:8412 [D loss: 0.653048, acc: 57.03%] [G loss: 2.071874]\n",
      "epoch:8 step:8413 [D loss: 0.594691, acc: 66.41%] [G loss: 2.156299]\n",
      "epoch:8 step:8414 [D loss: 0.600847, acc: 67.19%] [G loss: 2.598315]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:8 step:8415 [D loss: 0.531154, acc: 78.12%] [G loss: 2.581491]\n",
      "epoch:8 step:8416 [D loss: 0.641831, acc: 60.94%] [G loss: 2.228263]\n",
      "epoch:8 step:8417 [D loss: 0.583557, acc: 68.75%] [G loss: 2.348822]\n",
      "epoch:8 step:8418 [D loss: 0.616681, acc: 67.19%] [G loss: 2.158240]\n",
      "epoch:8 step:8419 [D loss: 0.555676, acc: 75.78%] [G loss: 2.557402]\n",
      "epoch:8 step:8420 [D loss: 0.543636, acc: 71.88%] [G loss: 2.628408]\n",
      "epoch:8 step:8421 [D loss: 0.524031, acc: 72.66%] [G loss: 2.656159]\n",
      "epoch:8 step:8422 [D loss: 0.575716, acc: 66.41%] [G loss: 2.491192]\n",
      "epoch:8 step:8423 [D loss: 0.572421, acc: 67.19%] [G loss: 2.564438]\n",
      "epoch:8 step:8424 [D loss: 0.827809, acc: 54.69%] [G loss: 2.030049]\n",
      "epoch:8 step:8425 [D loss: 0.765417, acc: 45.31%] [G loss: 2.093276]\n",
      "epoch:8 step:8426 [D loss: 0.623475, acc: 68.75%] [G loss: 2.248405]\n",
      "epoch:8 step:8427 [D loss: 0.637206, acc: 67.97%] [G loss: 2.203204]\n",
      "epoch:8 step:8428 [D loss: 0.609076, acc: 67.97%] [G loss: 2.036128]\n",
      "epoch:8 step:8429 [D loss: 0.686655, acc: 64.84%] [G loss: 2.293404]\n",
      "epoch:8 step:8430 [D loss: 0.611246, acc: 66.41%] [G loss: 2.092272]\n",
      "epoch:8 step:8431 [D loss: 0.614425, acc: 64.84%] [G loss: 2.070464]\n",
      "epoch:8 step:8432 [D loss: 0.570448, acc: 75.00%] [G loss: 2.357519]\n",
      "epoch:8 step:8433 [D loss: 0.496795, acc: 78.12%] [G loss: 2.718796]\n",
      "epoch:9 step:8434 [D loss: 0.689750, acc: 56.25%] [G loss: 2.013415]\n",
      "epoch:9 step:8435 [D loss: 0.606258, acc: 69.53%] [G loss: 2.393927]\n",
      "epoch:9 step:8436 [D loss: 0.600042, acc: 69.53%] [G loss: 2.208344]\n",
      "epoch:9 step:8437 [D loss: 0.616666, acc: 65.62%] [G loss: 2.196251]\n",
      "epoch:9 step:8438 [D loss: 0.719027, acc: 55.47%] [G loss: 2.146798]\n",
      "epoch:9 step:8439 [D loss: 0.666308, acc: 61.72%] [G loss: 2.206639]\n",
      "epoch:9 step:8440 [D loss: 0.606450, acc: 60.16%] [G loss: 2.064385]\n",
      "epoch:9 step:8441 [D loss: 0.657860, acc: 62.50%] [G loss: 2.202519]\n",
      "epoch:9 step:8442 [D loss: 0.614226, acc: 62.50%] [G loss: 2.129445]\n",
      "epoch:9 step:8443 [D loss: 0.552454, acc: 71.88%] [G loss: 2.383914]\n",
      "epoch:9 step:8444 [D loss: 0.609629, acc: 64.84%] [G loss: 2.230031]\n",
      "epoch:9 step:8445 [D loss: 0.601439, acc: 67.19%] [G loss: 2.006399]\n",
      "epoch:9 step:8446 [D loss: 0.634510, acc: 67.19%] [G loss: 2.308295]\n",
      "epoch:9 step:8447 [D loss: 0.637086, acc: 67.97%] [G loss: 2.091564]\n",
      "epoch:9 step:8448 [D loss: 0.592868, acc: 69.53%] [G loss: 2.259507]\n",
      "epoch:9 step:8449 [D loss: 0.588149, acc: 67.97%] [G loss: 2.063857]\n",
      "epoch:9 step:8450 [D loss: 0.703514, acc: 59.38%] [G loss: 2.087031]\n",
      "epoch:9 step:8451 [D loss: 0.635133, acc: 62.50%] [G loss: 2.104502]\n",
      "epoch:9 step:8452 [D loss: 0.642824, acc: 60.16%] [G loss: 2.061761]\n",
      "epoch:9 step:8453 [D loss: 0.653685, acc: 60.16%] [G loss: 1.814747]\n",
      "epoch:9 step:8454 [D loss: 0.659674, acc: 62.50%] [G loss: 2.065298]\n",
      "epoch:9 step:8455 [D loss: 0.596186, acc: 64.06%] [G loss: 2.141877]\n",
      "epoch:9 step:8456 [D loss: 0.628509, acc: 68.75%] [G loss: 2.470229]\n",
      "epoch:9 step:8457 [D loss: 0.604807, acc: 67.97%] [G loss: 2.196349]\n",
      "epoch:9 step:8458 [D loss: 0.535129, acc: 77.34%] [G loss: 2.275977]\n",
      "epoch:9 step:8459 [D loss: 0.635232, acc: 66.41%] [G loss: 2.021876]\n",
      "epoch:9 step:8460 [D loss: 0.634037, acc: 61.72%] [G loss: 1.927466]\n",
      "epoch:9 step:8461 [D loss: 0.667143, acc: 57.81%] [G loss: 2.055728]\n",
      "epoch:9 step:8462 [D loss: 0.606377, acc: 66.41%] [G loss: 1.944496]\n",
      "epoch:9 step:8463 [D loss: 0.621767, acc: 67.19%] [G loss: 2.037630]\n",
      "epoch:9 step:8464 [D loss: 0.675068, acc: 59.38%] [G loss: 1.856490]\n",
      "epoch:9 step:8465 [D loss: 0.648277, acc: 60.16%] [G loss: 1.946551]\n",
      "epoch:9 step:8466 [D loss: 0.627959, acc: 65.62%] [G loss: 1.969680]\n",
      "epoch:9 step:8467 [D loss: 0.653697, acc: 63.28%] [G loss: 1.918510]\n",
      "epoch:9 step:8468 [D loss: 0.605855, acc: 66.41%] [G loss: 2.017189]\n",
      "epoch:9 step:8469 [D loss: 0.597863, acc: 65.62%] [G loss: 2.418259]\n",
      "epoch:9 step:8470 [D loss: 0.656860, acc: 71.88%] [G loss: 2.286669]\n",
      "epoch:9 step:8471 [D loss: 0.643042, acc: 65.62%] [G loss: 2.165715]\n",
      "epoch:9 step:8472 [D loss: 0.675065, acc: 61.72%] [G loss: 2.043825]\n",
      "epoch:9 step:8473 [D loss: 0.606012, acc: 68.75%] [G loss: 2.316695]\n",
      "epoch:9 step:8474 [D loss: 0.638584, acc: 61.72%] [G loss: 1.986414]\n",
      "epoch:9 step:8475 [D loss: 0.558490, acc: 71.88%] [G loss: 2.188961]\n",
      "epoch:9 step:8476 [D loss: 0.588861, acc: 67.97%] [G loss: 2.041705]\n",
      "epoch:9 step:8477 [D loss: 0.627509, acc: 66.41%] [G loss: 1.985738]\n",
      "epoch:9 step:8478 [D loss: 0.640144, acc: 61.72%] [G loss: 1.950165]\n",
      "epoch:9 step:8479 [D loss: 0.665413, acc: 60.94%] [G loss: 1.888859]\n",
      "epoch:9 step:8480 [D loss: 0.565860, acc: 76.56%] [G loss: 2.119638]\n",
      "epoch:9 step:8481 [D loss: 0.647867, acc: 61.72%] [G loss: 2.120212]\n",
      "epoch:9 step:8482 [D loss: 0.654589, acc: 60.16%] [G loss: 2.097723]\n",
      "epoch:9 step:8483 [D loss: 0.596802, acc: 68.75%] [G loss: 2.174326]\n",
      "epoch:9 step:8484 [D loss: 0.636836, acc: 64.06%] [G loss: 2.043874]\n",
      "epoch:9 step:8485 [D loss: 0.613194, acc: 62.50%] [G loss: 2.021435]\n",
      "epoch:9 step:8486 [D loss: 0.666322, acc: 63.28%] [G loss: 2.355260]\n",
      "epoch:9 step:8487 [D loss: 0.636941, acc: 67.97%] [G loss: 2.167370]\n",
      "epoch:9 step:8488 [D loss: 0.601496, acc: 62.50%] [G loss: 2.212749]\n",
      "epoch:9 step:8489 [D loss: 0.602924, acc: 64.06%] [G loss: 2.276396]\n",
      "epoch:9 step:8490 [D loss: 0.696754, acc: 55.47%] [G loss: 2.124373]\n",
      "epoch:9 step:8491 [D loss: 0.666024, acc: 68.75%] [G loss: 2.021171]\n",
      "epoch:9 step:8492 [D loss: 0.668846, acc: 64.06%] [G loss: 1.920102]\n",
      "epoch:9 step:8493 [D loss: 0.649914, acc: 59.38%] [G loss: 1.982129]\n",
      "epoch:9 step:8494 [D loss: 0.641335, acc: 63.28%] [G loss: 2.131864]\n",
      "epoch:9 step:8495 [D loss: 0.623823, acc: 64.84%] [G loss: 2.012932]\n",
      "epoch:9 step:8496 [D loss: 0.632327, acc: 61.72%] [G loss: 2.207751]\n",
      "epoch:9 step:8497 [D loss: 0.620815, acc: 64.06%] [G loss: 2.004659]\n",
      "epoch:9 step:8498 [D loss: 0.658548, acc: 56.25%] [G loss: 1.945470]\n",
      "epoch:9 step:8499 [D loss: 0.611115, acc: 68.75%] [G loss: 2.008308]\n",
      "epoch:9 step:8500 [D loss: 0.636181, acc: 62.50%] [G loss: 1.955055]\n",
      "epoch:9 step:8501 [D loss: 0.684789, acc: 59.38%] [G loss: 1.987964]\n",
      "epoch:9 step:8502 [D loss: 0.592272, acc: 72.66%] [G loss: 2.172313]\n",
      "epoch:9 step:8503 [D loss: 0.593214, acc: 69.53%] [G loss: 2.237458]\n",
      "epoch:9 step:8504 [D loss: 0.671347, acc: 59.38%] [G loss: 2.054607]\n",
      "epoch:9 step:8505 [D loss: 0.603842, acc: 67.97%] [G loss: 2.295762]\n",
      "epoch:9 step:8506 [D loss: 0.570008, acc: 71.09%] [G loss: 2.070774]\n",
      "epoch:9 step:8507 [D loss: 0.555824, acc: 71.09%] [G loss: 2.335876]\n",
      "epoch:9 step:8508 [D loss: 0.584779, acc: 71.09%] [G loss: 2.443382]\n",
      "epoch:9 step:8509 [D loss: 0.535437, acc: 75.00%] [G loss: 2.474995]\n",
      "epoch:9 step:8510 [D loss: 0.600297, acc: 67.19%] [G loss: 2.397523]\n",
      "epoch:9 step:8511 [D loss: 0.671724, acc: 60.94%] [G loss: 1.958136]\n",
      "epoch:9 step:8512 [D loss: 0.588105, acc: 72.66%] [G loss: 2.076612]\n",
      "epoch:9 step:8513 [D loss: 0.672691, acc: 62.50%] [G loss: 1.989398]\n",
      "epoch:9 step:8514 [D loss: 0.643909, acc: 61.72%] [G loss: 1.840086]\n",
      "epoch:9 step:8515 [D loss: 0.625612, acc: 60.94%] [G loss: 2.160928]\n",
      "epoch:9 step:8516 [D loss: 0.631762, acc: 63.28%] [G loss: 2.239781]\n",
      "epoch:9 step:8517 [D loss: 0.622285, acc: 61.72%] [G loss: 1.956636]\n",
      "epoch:9 step:8518 [D loss: 0.658241, acc: 67.19%] [G loss: 2.096076]\n",
      "epoch:9 step:8519 [D loss: 0.629535, acc: 67.19%] [G loss: 2.042069]\n",
      "epoch:9 step:8520 [D loss: 0.628746, acc: 63.28%] [G loss: 2.071768]\n",
      "epoch:9 step:8521 [D loss: 0.608757, acc: 67.19%] [G loss: 2.036878]\n",
      "epoch:9 step:8522 [D loss: 0.606264, acc: 67.97%] [G loss: 2.199889]\n",
      "epoch:9 step:8523 [D loss: 0.632362, acc: 64.06%] [G loss: 2.017827]\n",
      "epoch:9 step:8524 [D loss: 0.664465, acc: 67.97%] [G loss: 2.063027]\n",
      "epoch:9 step:8525 [D loss: 0.605290, acc: 70.31%] [G loss: 2.236317]\n",
      "epoch:9 step:8526 [D loss: 0.619606, acc: 62.50%] [G loss: 2.244247]\n",
      "epoch:9 step:8527 [D loss: 0.596454, acc: 67.97%] [G loss: 2.187807]\n",
      "epoch:9 step:8528 [D loss: 0.666889, acc: 62.50%] [G loss: 2.157657]\n",
      "epoch:9 step:8529 [D loss: 0.599683, acc: 68.75%] [G loss: 2.222604]\n",
      "epoch:9 step:8530 [D loss: 0.538964, acc: 71.88%] [G loss: 2.183551]\n",
      "epoch:9 step:8531 [D loss: 0.751427, acc: 52.34%] [G loss: 1.947991]\n",
      "epoch:9 step:8532 [D loss: 0.647579, acc: 61.72%] [G loss: 2.274454]\n",
      "epoch:9 step:8533 [D loss: 0.666354, acc: 53.12%] [G loss: 2.061968]\n",
      "epoch:9 step:8534 [D loss: 0.624571, acc: 63.28%] [G loss: 2.123209]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:9 step:8535 [D loss: 0.659114, acc: 68.75%] [G loss: 2.059071]\n",
      "epoch:9 step:8536 [D loss: 0.624308, acc: 64.84%] [G loss: 1.989091]\n",
      "epoch:9 step:8537 [D loss: 0.626544, acc: 64.06%] [G loss: 1.945338]\n",
      "epoch:9 step:8538 [D loss: 0.660253, acc: 66.41%] [G loss: 2.035116]\n",
      "epoch:9 step:8539 [D loss: 0.688742, acc: 58.59%] [G loss: 2.159886]\n",
      "epoch:9 step:8540 [D loss: 0.625051, acc: 65.62%] [G loss: 2.181881]\n",
      "epoch:9 step:8541 [D loss: 0.638587, acc: 61.72%] [G loss: 2.024693]\n",
      "epoch:9 step:8542 [D loss: 0.633870, acc: 60.16%] [G loss: 2.063114]\n",
      "epoch:9 step:8543 [D loss: 0.678753, acc: 58.59%] [G loss: 2.047331]\n",
      "epoch:9 step:8544 [D loss: 0.562428, acc: 64.84%] [G loss: 2.081179]\n",
      "epoch:9 step:8545 [D loss: 0.632580, acc: 66.41%] [G loss: 2.135522]\n",
      "epoch:9 step:8546 [D loss: 0.634241, acc: 64.06%] [G loss: 2.124035]\n",
      "epoch:9 step:8547 [D loss: 0.566967, acc: 72.66%] [G loss: 2.265284]\n",
      "epoch:9 step:8548 [D loss: 0.630846, acc: 70.31%] [G loss: 2.332092]\n",
      "epoch:9 step:8549 [D loss: 0.614681, acc: 65.62%] [G loss: 2.257619]\n",
      "epoch:9 step:8550 [D loss: 0.595499, acc: 71.09%] [G loss: 2.256056]\n",
      "epoch:9 step:8551 [D loss: 0.666400, acc: 64.06%] [G loss: 2.344975]\n",
      "epoch:9 step:8552 [D loss: 0.680228, acc: 69.53%] [G loss: 2.464990]\n",
      "epoch:9 step:8553 [D loss: 0.673987, acc: 60.94%] [G loss: 2.077128]\n",
      "epoch:9 step:8554 [D loss: 0.659061, acc: 66.41%] [G loss: 2.110301]\n",
      "epoch:9 step:8555 [D loss: 0.616082, acc: 60.94%] [G loss: 2.249680]\n",
      "epoch:9 step:8556 [D loss: 0.629622, acc: 64.06%] [G loss: 2.005645]\n",
      "epoch:9 step:8557 [D loss: 0.675525, acc: 59.38%] [G loss: 2.009288]\n",
      "epoch:9 step:8558 [D loss: 0.637292, acc: 61.72%] [G loss: 2.146500]\n",
      "epoch:9 step:8559 [D loss: 0.615960, acc: 61.72%] [G loss: 2.075596]\n",
      "epoch:9 step:8560 [D loss: 0.639812, acc: 65.62%] [G loss: 1.995904]\n",
      "epoch:9 step:8561 [D loss: 0.634979, acc: 68.75%] [G loss: 1.922179]\n",
      "epoch:9 step:8562 [D loss: 0.652184, acc: 57.03%] [G loss: 1.988133]\n",
      "epoch:9 step:8563 [D loss: 0.637811, acc: 64.06%] [G loss: 2.091368]\n",
      "epoch:9 step:8564 [D loss: 0.606344, acc: 64.84%] [G loss: 2.227974]\n",
      "epoch:9 step:8565 [D loss: 0.637958, acc: 62.50%] [G loss: 1.988084]\n",
      "epoch:9 step:8566 [D loss: 0.670782, acc: 56.25%] [G loss: 1.830334]\n",
      "epoch:9 step:8567 [D loss: 0.623357, acc: 66.41%] [G loss: 2.018424]\n",
      "epoch:9 step:8568 [D loss: 0.600149, acc: 71.88%] [G loss: 2.003823]\n",
      "epoch:9 step:8569 [D loss: 0.626226, acc: 64.06%] [G loss: 2.153407]\n",
      "epoch:9 step:8570 [D loss: 0.675574, acc: 60.16%] [G loss: 2.049441]\n",
      "epoch:9 step:8571 [D loss: 0.587450, acc: 67.19%] [G loss: 2.003231]\n",
      "epoch:9 step:8572 [D loss: 0.599701, acc: 68.75%] [G loss: 2.109316]\n",
      "epoch:9 step:8573 [D loss: 0.642502, acc: 62.50%] [G loss: 2.215962]\n",
      "epoch:9 step:8574 [D loss: 0.645135, acc: 63.28%] [G loss: 2.126272]\n",
      "epoch:9 step:8575 [D loss: 0.676522, acc: 58.59%] [G loss: 1.954597]\n",
      "epoch:9 step:8576 [D loss: 0.639179, acc: 60.16%] [G loss: 1.973851]\n",
      "epoch:9 step:8577 [D loss: 0.614455, acc: 67.97%] [G loss: 2.182619]\n",
      "epoch:9 step:8578 [D loss: 0.656545, acc: 62.50%] [G loss: 1.917428]\n",
      "epoch:9 step:8579 [D loss: 0.634885, acc: 60.16%] [G loss: 2.173155]\n",
      "epoch:9 step:8580 [D loss: 0.653665, acc: 64.06%] [G loss: 1.930625]\n",
      "epoch:9 step:8581 [D loss: 0.715194, acc: 56.25%] [G loss: 1.872584]\n",
      "epoch:9 step:8582 [D loss: 0.604684, acc: 67.19%] [G loss: 1.972596]\n",
      "epoch:9 step:8583 [D loss: 0.701929, acc: 60.16%] [G loss: 1.910588]\n",
      "epoch:9 step:8584 [D loss: 0.600731, acc: 66.41%] [G loss: 2.235989]\n",
      "epoch:9 step:8585 [D loss: 0.658676, acc: 61.72%] [G loss: 2.057502]\n",
      "epoch:9 step:8586 [D loss: 0.654094, acc: 58.59%] [G loss: 1.904442]\n",
      "epoch:9 step:8587 [D loss: 0.594734, acc: 71.88%] [G loss: 2.222566]\n",
      "epoch:9 step:8588 [D loss: 0.594234, acc: 72.66%] [G loss: 2.024071]\n",
      "epoch:9 step:8589 [D loss: 0.600116, acc: 62.50%] [G loss: 2.133219]\n",
      "epoch:9 step:8590 [D loss: 0.634722, acc: 62.50%] [G loss: 1.990252]\n",
      "epoch:9 step:8591 [D loss: 0.621833, acc: 65.62%] [G loss: 2.039231]\n",
      "epoch:9 step:8592 [D loss: 0.654969, acc: 62.50%] [G loss: 2.133846]\n",
      "epoch:9 step:8593 [D loss: 0.666371, acc: 57.03%] [G loss: 1.886294]\n",
      "epoch:9 step:8594 [D loss: 0.584889, acc: 71.09%] [G loss: 1.972781]\n",
      "epoch:9 step:8595 [D loss: 0.603099, acc: 68.75%] [G loss: 2.126926]\n",
      "epoch:9 step:8596 [D loss: 0.635687, acc: 61.72%] [G loss: 2.102567]\n",
      "epoch:9 step:8597 [D loss: 0.661338, acc: 63.28%] [G loss: 1.956998]\n",
      "epoch:9 step:8598 [D loss: 0.557586, acc: 71.88%] [G loss: 2.087048]\n",
      "epoch:9 step:8599 [D loss: 0.617437, acc: 64.06%] [G loss: 2.011695]\n",
      "epoch:9 step:8600 [D loss: 0.600063, acc: 68.75%] [G loss: 2.126730]\n",
      "##############\n",
      "[2.59031971 1.53067085 6.63670739 5.03787281 3.8551422  5.82386537\n",
      " 4.55208478 4.78483333 4.72765773 3.66174479]\n",
      "##########\n",
      "epoch:9 step:8601 [D loss: 0.618032, acc: 66.41%] [G loss: 2.176277]\n",
      "epoch:9 step:8602 [D loss: 0.606957, acc: 68.75%] [G loss: 2.196042]\n",
      "epoch:9 step:8603 [D loss: 0.691280, acc: 60.16%] [G loss: 2.118654]\n",
      "epoch:9 step:8604 [D loss: 0.602188, acc: 67.19%] [G loss: 2.124087]\n",
      "epoch:9 step:8605 [D loss: 0.645330, acc: 60.16%] [G loss: 2.047399]\n",
      "epoch:9 step:8606 [D loss: 0.609063, acc: 68.75%] [G loss: 2.140804]\n",
      "epoch:9 step:8607 [D loss: 0.622530, acc: 64.84%] [G loss: 2.095269]\n",
      "epoch:9 step:8608 [D loss: 0.612955, acc: 66.41%] [G loss: 2.008250]\n",
      "epoch:9 step:8609 [D loss: 0.692688, acc: 60.16%] [G loss: 2.024214]\n",
      "epoch:9 step:8610 [D loss: 0.709484, acc: 59.38%] [G loss: 1.939810]\n",
      "epoch:9 step:8611 [D loss: 0.633352, acc: 64.06%] [G loss: 2.019294]\n",
      "epoch:9 step:8612 [D loss: 0.642762, acc: 64.84%] [G loss: 1.890603]\n",
      "epoch:9 step:8613 [D loss: 0.668958, acc: 55.47%] [G loss: 1.924661]\n",
      "epoch:9 step:8614 [D loss: 0.602759, acc: 68.75%] [G loss: 1.888231]\n",
      "epoch:9 step:8615 [D loss: 0.641155, acc: 64.06%] [G loss: 1.931376]\n",
      "epoch:9 step:8616 [D loss: 0.646918, acc: 63.28%] [G loss: 2.082022]\n",
      "epoch:9 step:8617 [D loss: 0.635013, acc: 67.19%] [G loss: 1.954098]\n",
      "epoch:9 step:8618 [D loss: 0.615120, acc: 61.72%] [G loss: 2.025722]\n",
      "epoch:9 step:8619 [D loss: 0.636516, acc: 64.06%] [G loss: 1.969351]\n",
      "epoch:9 step:8620 [D loss: 0.694269, acc: 60.94%] [G loss: 1.977410]\n",
      "epoch:9 step:8621 [D loss: 0.619524, acc: 67.19%] [G loss: 1.893178]\n",
      "epoch:9 step:8622 [D loss: 0.642182, acc: 61.72%] [G loss: 2.026550]\n",
      "epoch:9 step:8623 [D loss: 0.612736, acc: 68.75%] [G loss: 2.041376]\n",
      "epoch:9 step:8624 [D loss: 0.613183, acc: 67.97%] [G loss: 2.192282]\n",
      "epoch:9 step:8625 [D loss: 0.567577, acc: 73.44%] [G loss: 2.125988]\n",
      "epoch:9 step:8626 [D loss: 0.686720, acc: 58.59%] [G loss: 2.215497]\n",
      "epoch:9 step:8627 [D loss: 0.526372, acc: 75.78%] [G loss: 2.466966]\n",
      "epoch:9 step:8628 [D loss: 0.645731, acc: 67.97%] [G loss: 1.877357]\n",
      "epoch:9 step:8629 [D loss: 0.597400, acc: 70.31%] [G loss: 2.006343]\n",
      "epoch:9 step:8630 [D loss: 0.603007, acc: 67.19%] [G loss: 2.292067]\n",
      "epoch:9 step:8631 [D loss: 0.642750, acc: 67.97%] [G loss: 2.247926]\n",
      "epoch:9 step:8632 [D loss: 0.626956, acc: 64.84%] [G loss: 2.223885]\n",
      "epoch:9 step:8633 [D loss: 0.685375, acc: 60.94%] [G loss: 1.884715]\n",
      "epoch:9 step:8634 [D loss: 0.643060, acc: 65.62%] [G loss: 2.041623]\n",
      "epoch:9 step:8635 [D loss: 0.664699, acc: 59.38%] [G loss: 2.267725]\n",
      "epoch:9 step:8636 [D loss: 0.612353, acc: 66.41%] [G loss: 2.084906]\n",
      "epoch:9 step:8637 [D loss: 0.639658, acc: 64.06%] [G loss: 2.164397]\n",
      "epoch:9 step:8638 [D loss: 0.693342, acc: 53.12%] [G loss: 2.007191]\n",
      "epoch:9 step:8639 [D loss: 0.618325, acc: 64.84%] [G loss: 2.196247]\n",
      "epoch:9 step:8640 [D loss: 0.589438, acc: 67.97%] [G loss: 2.464771]\n",
      "epoch:9 step:8641 [D loss: 0.586004, acc: 73.44%] [G loss: 2.436744]\n",
      "epoch:9 step:8642 [D loss: 0.585392, acc: 67.19%] [G loss: 2.469155]\n",
      "epoch:9 step:8643 [D loss: 0.709080, acc: 57.81%] [G loss: 1.936706]\n",
      "epoch:9 step:8644 [D loss: 0.731246, acc: 46.88%] [G loss: 1.856229]\n",
      "epoch:9 step:8645 [D loss: 0.637568, acc: 61.72%] [G loss: 1.940093]\n",
      "epoch:9 step:8646 [D loss: 0.692660, acc: 57.81%] [G loss: 2.016223]\n",
      "epoch:9 step:8647 [D loss: 0.624163, acc: 63.28%] [G loss: 1.929134]\n",
      "epoch:9 step:8648 [D loss: 0.690588, acc: 53.91%] [G loss: 1.911249]\n",
      "epoch:9 step:8649 [D loss: 0.595917, acc: 67.19%] [G loss: 2.188758]\n",
      "epoch:9 step:8650 [D loss: 0.588002, acc: 70.31%] [G loss: 2.044903]\n",
      "epoch:9 step:8651 [D loss: 0.579747, acc: 67.97%] [G loss: 2.415996]\n",
      "epoch:9 step:8652 [D loss: 0.611715, acc: 64.84%] [G loss: 2.252507]\n",
      "epoch:9 step:8653 [D loss: 0.707911, acc: 56.25%] [G loss: 1.899792]\n",
      "epoch:9 step:8654 [D loss: 0.697649, acc: 50.00%] [G loss: 1.971872]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:9 step:8655 [D loss: 0.651541, acc: 60.94%] [G loss: 2.114221]\n",
      "epoch:9 step:8656 [D loss: 0.591147, acc: 67.97%] [G loss: 2.076387]\n",
      "epoch:9 step:8657 [D loss: 0.660590, acc: 60.16%] [G loss: 2.046498]\n",
      "epoch:9 step:8658 [D loss: 0.680543, acc: 60.16%] [G loss: 2.015367]\n",
      "epoch:9 step:8659 [D loss: 0.639660, acc: 59.38%] [G loss: 2.042897]\n",
      "epoch:9 step:8660 [D loss: 0.666370, acc: 58.59%] [G loss: 1.898414]\n",
      "epoch:9 step:8661 [D loss: 0.709410, acc: 60.16%] [G loss: 1.819023]\n",
      "epoch:9 step:8662 [D loss: 0.603472, acc: 67.97%] [G loss: 2.138023]\n",
      "epoch:9 step:8663 [D loss: 0.590433, acc: 67.19%] [G loss: 2.375820]\n",
      "epoch:9 step:8664 [D loss: 0.533836, acc: 77.34%] [G loss: 2.605074]\n",
      "epoch:9 step:8665 [D loss: 0.548215, acc: 71.09%] [G loss: 2.376512]\n",
      "epoch:9 step:8666 [D loss: 0.698006, acc: 59.38%] [G loss: 2.016361]\n",
      "epoch:9 step:8667 [D loss: 0.611323, acc: 63.28%] [G loss: 2.008460]\n",
      "epoch:9 step:8668 [D loss: 0.609001, acc: 67.97%] [G loss: 1.932201]\n",
      "epoch:9 step:8669 [D loss: 0.597747, acc: 67.19%] [G loss: 2.137750]\n",
      "epoch:9 step:8670 [D loss: 0.650752, acc: 60.94%] [G loss: 1.915366]\n",
      "epoch:9 step:8671 [D loss: 0.603374, acc: 69.53%] [G loss: 1.939218]\n",
      "epoch:9 step:8672 [D loss: 0.597443, acc: 70.31%] [G loss: 2.110874]\n",
      "epoch:9 step:8673 [D loss: 0.585938, acc: 67.97%] [G loss: 2.074155]\n",
      "epoch:9 step:8674 [D loss: 0.585451, acc: 70.31%] [G loss: 2.274404]\n",
      "epoch:9 step:8675 [D loss: 0.658744, acc: 61.72%] [G loss: 2.304206]\n",
      "epoch:9 step:8676 [D loss: 0.618303, acc: 64.84%] [G loss: 1.946495]\n",
      "epoch:9 step:8677 [D loss: 0.654117, acc: 64.84%] [G loss: 2.136312]\n",
      "epoch:9 step:8678 [D loss: 0.665291, acc: 61.72%] [G loss: 2.090003]\n",
      "epoch:9 step:8679 [D loss: 0.623822, acc: 68.75%] [G loss: 2.059902]\n",
      "epoch:9 step:8680 [D loss: 0.690897, acc: 57.81%] [G loss: 2.306578]\n",
      "epoch:9 step:8681 [D loss: 0.617397, acc: 64.84%] [G loss: 2.038836]\n",
      "epoch:9 step:8682 [D loss: 0.648491, acc: 64.06%] [G loss: 2.082600]\n",
      "epoch:9 step:8683 [D loss: 0.692038, acc: 54.69%] [G loss: 1.964764]\n",
      "epoch:9 step:8684 [D loss: 0.714238, acc: 53.12%] [G loss: 1.884616]\n",
      "epoch:9 step:8685 [D loss: 0.604833, acc: 67.97%] [G loss: 1.814122]\n",
      "epoch:9 step:8686 [D loss: 0.663398, acc: 64.06%] [G loss: 1.976024]\n",
      "epoch:9 step:8687 [D loss: 0.622157, acc: 67.19%] [G loss: 1.892809]\n",
      "epoch:9 step:8688 [D loss: 0.657810, acc: 62.50%] [G loss: 2.025345]\n",
      "epoch:9 step:8689 [D loss: 0.629167, acc: 64.06%] [G loss: 1.890534]\n",
      "epoch:9 step:8690 [D loss: 0.709970, acc: 53.91%] [G loss: 1.890961]\n",
      "epoch:9 step:8691 [D loss: 0.643881, acc: 66.41%] [G loss: 1.930913]\n",
      "epoch:9 step:8692 [D loss: 0.593464, acc: 70.31%] [G loss: 2.073327]\n",
      "epoch:9 step:8693 [D loss: 0.602952, acc: 66.41%] [G loss: 1.992177]\n",
      "epoch:9 step:8694 [D loss: 0.615330, acc: 63.28%] [G loss: 2.199643]\n",
      "epoch:9 step:8695 [D loss: 0.610171, acc: 67.19%] [G loss: 2.254996]\n",
      "epoch:9 step:8696 [D loss: 0.607974, acc: 65.62%] [G loss: 2.024747]\n",
      "epoch:9 step:8697 [D loss: 0.531371, acc: 76.56%] [G loss: 2.232540]\n",
      "epoch:9 step:8698 [D loss: 0.673375, acc: 57.03%] [G loss: 1.980391]\n",
      "epoch:9 step:8699 [D loss: 0.635929, acc: 63.28%] [G loss: 2.172011]\n",
      "epoch:9 step:8700 [D loss: 0.640285, acc: 62.50%] [G loss: 2.214206]\n",
      "epoch:9 step:8701 [D loss: 0.670244, acc: 60.94%] [G loss: 2.207360]\n",
      "epoch:9 step:8702 [D loss: 0.663215, acc: 59.38%] [G loss: 2.115769]\n",
      "epoch:9 step:8703 [D loss: 0.632338, acc: 64.06%] [G loss: 2.126387]\n",
      "epoch:9 step:8704 [D loss: 0.694048, acc: 59.38%] [G loss: 2.030405]\n",
      "epoch:9 step:8705 [D loss: 0.638462, acc: 59.38%] [G loss: 2.068417]\n",
      "epoch:9 step:8706 [D loss: 0.619733, acc: 64.84%] [G loss: 2.236797]\n",
      "epoch:9 step:8707 [D loss: 0.589013, acc: 70.31%] [G loss: 2.204843]\n",
      "epoch:9 step:8708 [D loss: 0.615118, acc: 67.19%] [G loss: 2.151794]\n",
      "epoch:9 step:8709 [D loss: 0.563729, acc: 73.44%] [G loss: 2.345764]\n",
      "epoch:9 step:8710 [D loss: 0.628650, acc: 64.84%] [G loss: 1.993194]\n",
      "epoch:9 step:8711 [D loss: 0.643989, acc: 62.50%] [G loss: 2.066104]\n",
      "epoch:9 step:8712 [D loss: 0.661177, acc: 55.47%] [G loss: 2.052570]\n",
      "epoch:9 step:8713 [D loss: 0.654574, acc: 59.38%] [G loss: 2.065639]\n",
      "epoch:9 step:8714 [D loss: 0.652587, acc: 67.97%] [G loss: 1.885912]\n",
      "epoch:9 step:8715 [D loss: 0.651415, acc: 64.06%] [G loss: 1.877620]\n",
      "epoch:9 step:8716 [D loss: 0.607518, acc: 64.84%] [G loss: 2.051509]\n",
      "epoch:9 step:8717 [D loss: 0.606844, acc: 71.09%] [G loss: 2.109653]\n",
      "epoch:9 step:8718 [D loss: 0.640424, acc: 63.28%] [G loss: 2.004447]\n",
      "epoch:9 step:8719 [D loss: 0.657206, acc: 62.50%] [G loss: 2.230698]\n",
      "epoch:9 step:8720 [D loss: 0.628933, acc: 64.84%] [G loss: 2.046709]\n",
      "epoch:9 step:8721 [D loss: 0.661483, acc: 62.50%] [G loss: 2.068432]\n",
      "epoch:9 step:8722 [D loss: 0.644274, acc: 66.41%] [G loss: 1.965474]\n",
      "epoch:9 step:8723 [D loss: 0.658945, acc: 63.28%] [G loss: 2.029712]\n",
      "epoch:9 step:8724 [D loss: 0.598568, acc: 67.97%] [G loss: 2.247810]\n",
      "epoch:9 step:8725 [D loss: 0.630816, acc: 62.50%] [G loss: 1.943918]\n",
      "epoch:9 step:8726 [D loss: 0.628955, acc: 67.19%] [G loss: 1.941749]\n",
      "epoch:9 step:8727 [D loss: 0.632220, acc: 67.19%] [G loss: 2.117381]\n",
      "epoch:9 step:8728 [D loss: 0.728504, acc: 53.12%] [G loss: 2.151913]\n",
      "epoch:9 step:8729 [D loss: 0.554803, acc: 71.88%] [G loss: 2.073979]\n",
      "epoch:9 step:8730 [D loss: 0.608065, acc: 67.97%] [G loss: 1.933021]\n",
      "epoch:9 step:8731 [D loss: 0.655190, acc: 64.84%] [G loss: 2.136149]\n",
      "epoch:9 step:8732 [D loss: 0.569754, acc: 72.66%] [G loss: 2.262660]\n",
      "epoch:9 step:8733 [D loss: 0.628793, acc: 64.84%] [G loss: 2.302986]\n",
      "epoch:9 step:8734 [D loss: 0.683252, acc: 62.50%] [G loss: 1.880666]\n",
      "epoch:9 step:8735 [D loss: 0.677518, acc: 64.06%] [G loss: 2.047179]\n",
      "epoch:9 step:8736 [D loss: 0.637658, acc: 62.50%] [G loss: 2.077949]\n",
      "epoch:9 step:8737 [D loss: 0.603560, acc: 70.31%] [G loss: 1.980814]\n",
      "epoch:9 step:8738 [D loss: 0.589535, acc: 73.44%] [G loss: 1.970675]\n",
      "epoch:9 step:8739 [D loss: 0.651316, acc: 55.47%] [G loss: 2.036138]\n",
      "epoch:9 step:8740 [D loss: 0.681835, acc: 64.06%] [G loss: 2.035732]\n",
      "epoch:9 step:8741 [D loss: 0.617449, acc: 64.06%] [G loss: 2.215693]\n",
      "epoch:9 step:8742 [D loss: 0.581713, acc: 68.75%] [G loss: 2.297323]\n",
      "epoch:9 step:8743 [D loss: 0.622276, acc: 67.97%] [G loss: 2.106850]\n",
      "epoch:9 step:8744 [D loss: 0.593089, acc: 70.31%] [G loss: 2.120091]\n",
      "epoch:9 step:8745 [D loss: 0.510951, acc: 75.78%] [G loss: 2.899799]\n",
      "epoch:9 step:8746 [D loss: 0.577465, acc: 70.31%] [G loss: 2.763901]\n",
      "epoch:9 step:8747 [D loss: 0.596225, acc: 71.09%] [G loss: 2.791248]\n",
      "epoch:9 step:8748 [D loss: 0.533101, acc: 72.66%] [G loss: 2.703545]\n",
      "epoch:9 step:8749 [D loss: 0.743249, acc: 57.03%] [G loss: 2.012103]\n",
      "epoch:9 step:8750 [D loss: 0.647869, acc: 60.94%] [G loss: 2.074362]\n",
      "epoch:9 step:8751 [D loss: 0.572756, acc: 72.66%] [G loss: 2.200154]\n",
      "epoch:9 step:8752 [D loss: 0.711631, acc: 58.59%] [G loss: 1.952563]\n",
      "epoch:9 step:8753 [D loss: 0.679323, acc: 60.94%] [G loss: 2.020671]\n",
      "epoch:9 step:8754 [D loss: 0.635279, acc: 68.75%] [G loss: 2.295419]\n",
      "epoch:9 step:8755 [D loss: 0.659958, acc: 58.59%] [G loss: 2.104316]\n",
      "epoch:9 step:8756 [D loss: 0.638597, acc: 64.84%] [G loss: 1.934519]\n",
      "epoch:9 step:8757 [D loss: 0.678285, acc: 56.25%] [G loss: 1.891033]\n",
      "epoch:9 step:8758 [D loss: 0.659218, acc: 64.84%] [G loss: 2.162258]\n",
      "epoch:9 step:8759 [D loss: 0.659661, acc: 59.38%] [G loss: 1.830508]\n",
      "epoch:9 step:8760 [D loss: 0.628294, acc: 68.75%] [G loss: 2.041639]\n",
      "epoch:9 step:8761 [D loss: 0.674124, acc: 63.28%] [G loss: 2.097954]\n",
      "epoch:9 step:8762 [D loss: 0.559194, acc: 71.88%] [G loss: 2.108448]\n",
      "epoch:9 step:8763 [D loss: 0.667801, acc: 53.91%] [G loss: 1.967216]\n",
      "epoch:9 step:8764 [D loss: 0.611021, acc: 70.31%] [G loss: 2.161748]\n",
      "epoch:9 step:8765 [D loss: 0.675427, acc: 61.72%] [G loss: 2.194093]\n",
      "epoch:9 step:8766 [D loss: 0.594390, acc: 70.31%] [G loss: 2.237877]\n",
      "epoch:9 step:8767 [D loss: 0.719296, acc: 57.81%] [G loss: 1.968045]\n",
      "epoch:9 step:8768 [D loss: 0.658213, acc: 64.06%] [G loss: 2.086999]\n",
      "epoch:9 step:8769 [D loss: 0.614157, acc: 65.62%] [G loss: 2.057318]\n",
      "epoch:9 step:8770 [D loss: 0.651595, acc: 64.06%] [G loss: 2.085247]\n",
      "epoch:9 step:8771 [D loss: 0.582852, acc: 71.88%] [G loss: 1.995977]\n",
      "epoch:9 step:8772 [D loss: 0.627642, acc: 65.62%] [G loss: 2.172276]\n",
      "epoch:9 step:8773 [D loss: 0.637879, acc: 68.75%] [G loss: 1.989183]\n",
      "epoch:9 step:8774 [D loss: 0.694758, acc: 60.16%] [G loss: 1.997802]\n",
      "epoch:9 step:8775 [D loss: 0.675175, acc: 56.25%] [G loss: 1.882216]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:9 step:8776 [D loss: 0.621767, acc: 67.19%] [G loss: 1.926412]\n",
      "epoch:9 step:8777 [D loss: 0.592839, acc: 67.19%] [G loss: 1.990001]\n",
      "epoch:9 step:8778 [D loss: 0.534839, acc: 74.22%] [G loss: 2.372149]\n",
      "epoch:9 step:8779 [D loss: 0.569288, acc: 70.31%] [G loss: 2.450841]\n",
      "epoch:9 step:8780 [D loss: 0.529339, acc: 71.88%] [G loss: 2.491856]\n",
      "epoch:9 step:8781 [D loss: 0.662660, acc: 55.47%] [G loss: 2.023993]\n",
      "epoch:9 step:8782 [D loss: 0.684632, acc: 58.59%] [G loss: 1.882356]\n",
      "epoch:9 step:8783 [D loss: 0.606021, acc: 68.75%] [G loss: 1.901356]\n",
      "epoch:9 step:8784 [D loss: 0.598172, acc: 63.28%] [G loss: 2.103415]\n",
      "epoch:9 step:8785 [D loss: 0.674798, acc: 55.47%] [G loss: 1.943282]\n",
      "epoch:9 step:8786 [D loss: 0.599298, acc: 67.97%] [G loss: 2.210690]\n",
      "epoch:9 step:8787 [D loss: 0.633528, acc: 64.84%] [G loss: 2.148818]\n",
      "epoch:9 step:8788 [D loss: 0.691446, acc: 62.50%] [G loss: 1.925547]\n",
      "epoch:9 step:8789 [D loss: 0.635062, acc: 64.06%] [G loss: 2.048957]\n",
      "epoch:9 step:8790 [D loss: 0.589227, acc: 67.19%] [G loss: 2.271967]\n",
      "epoch:9 step:8791 [D loss: 0.601730, acc: 72.66%] [G loss: 2.190381]\n",
      "epoch:9 step:8792 [D loss: 0.618501, acc: 67.19%] [G loss: 2.433129]\n",
      "epoch:9 step:8793 [D loss: 0.584169, acc: 70.31%] [G loss: 2.137951]\n",
      "epoch:9 step:8794 [D loss: 0.687107, acc: 61.72%] [G loss: 2.124704]\n",
      "epoch:9 step:8795 [D loss: 0.597510, acc: 67.97%] [G loss: 1.849393]\n",
      "epoch:9 step:8796 [D loss: 0.633360, acc: 60.94%] [G loss: 1.980152]\n",
      "epoch:9 step:8797 [D loss: 0.592700, acc: 64.06%] [G loss: 2.258621]\n",
      "epoch:9 step:8798 [D loss: 0.618704, acc: 63.28%] [G loss: 2.182581]\n",
      "epoch:9 step:8799 [D loss: 0.640332, acc: 59.38%] [G loss: 2.287375]\n",
      "epoch:9 step:8800 [D loss: 0.643571, acc: 63.28%] [G loss: 2.152363]\n",
      "##############\n",
      "[2.53593058 1.18607861 6.45542272 5.00865337 3.75484089 5.85791136\n",
      " 4.64439171 4.66072146 4.81142877 3.62170059]\n",
      "##########\n",
      "epoch:9 step:8801 [D loss: 0.695513, acc: 59.38%] [G loss: 1.917083]\n",
      "epoch:9 step:8802 [D loss: 0.599509, acc: 70.31%] [G loss: 2.113596]\n",
      "epoch:9 step:8803 [D loss: 0.601577, acc: 70.31%] [G loss: 2.281423]\n",
      "epoch:9 step:8804 [D loss: 0.623844, acc: 67.19%] [G loss: 2.288758]\n",
      "epoch:9 step:8805 [D loss: 0.650523, acc: 60.94%] [G loss: 2.131464]\n",
      "epoch:9 step:8806 [D loss: 0.697797, acc: 56.25%] [G loss: 1.921417]\n",
      "epoch:9 step:8807 [D loss: 0.669155, acc: 66.41%] [G loss: 2.210097]\n",
      "epoch:9 step:8808 [D loss: 0.621486, acc: 68.75%] [G loss: 2.091719]\n",
      "epoch:9 step:8809 [D loss: 0.679544, acc: 57.81%] [G loss: 2.200515]\n",
      "epoch:9 step:8810 [D loss: 0.674499, acc: 54.69%] [G loss: 1.820110]\n",
      "epoch:9 step:8811 [D loss: 0.589730, acc: 71.09%] [G loss: 2.194077]\n",
      "epoch:9 step:8812 [D loss: 0.598192, acc: 71.88%] [G loss: 2.133198]\n",
      "epoch:9 step:8813 [D loss: 0.645388, acc: 60.94%] [G loss: 2.248459]\n",
      "epoch:9 step:8814 [D loss: 0.642889, acc: 63.28%] [G loss: 2.331543]\n",
      "epoch:9 step:8815 [D loss: 0.652057, acc: 63.28%] [G loss: 2.071341]\n",
      "epoch:9 step:8816 [D loss: 0.641389, acc: 65.62%] [G loss: 2.123631]\n",
      "epoch:9 step:8817 [D loss: 0.632293, acc: 61.72%] [G loss: 2.126883]\n",
      "epoch:9 step:8818 [D loss: 0.607356, acc: 68.75%] [G loss: 2.138412]\n",
      "epoch:9 step:8819 [D loss: 0.701653, acc: 55.47%] [G loss: 1.961590]\n",
      "epoch:9 step:8820 [D loss: 0.585304, acc: 69.53%] [G loss: 2.064084]\n",
      "epoch:9 step:8821 [D loss: 0.615122, acc: 71.88%] [G loss: 2.075368]\n",
      "epoch:9 step:8822 [D loss: 0.607880, acc: 62.50%] [G loss: 1.982502]\n",
      "epoch:9 step:8823 [D loss: 0.610653, acc: 63.28%] [G loss: 2.018361]\n",
      "epoch:9 step:8824 [D loss: 0.671397, acc: 60.94%] [G loss: 1.997573]\n",
      "epoch:9 step:8825 [D loss: 0.620787, acc: 64.84%] [G loss: 2.188980]\n",
      "epoch:9 step:8826 [D loss: 0.603296, acc: 69.53%] [G loss: 2.260619]\n",
      "epoch:9 step:8827 [D loss: 0.704377, acc: 57.81%] [G loss: 2.116482]\n",
      "epoch:9 step:8828 [D loss: 0.568946, acc: 76.56%] [G loss: 2.037676]\n",
      "epoch:9 step:8829 [D loss: 0.632917, acc: 60.94%] [G loss: 2.141115]\n",
      "epoch:9 step:8830 [D loss: 0.591009, acc: 66.41%] [G loss: 2.085448]\n",
      "epoch:9 step:8831 [D loss: 0.609998, acc: 70.31%] [G loss: 2.167363]\n",
      "epoch:9 step:8832 [D loss: 0.605924, acc: 70.31%] [G loss: 2.231045]\n",
      "epoch:9 step:8833 [D loss: 0.628165, acc: 65.62%] [G loss: 2.428117]\n",
      "epoch:9 step:8834 [D loss: 0.597463, acc: 67.19%] [G loss: 2.099933]\n",
      "epoch:9 step:8835 [D loss: 0.617607, acc: 62.50%] [G loss: 2.322742]\n",
      "epoch:9 step:8836 [D loss: 0.618262, acc: 65.62%] [G loss: 2.278279]\n",
      "epoch:9 step:8837 [D loss: 0.527515, acc: 75.00%] [G loss: 2.356241]\n",
      "epoch:9 step:8838 [D loss: 0.656506, acc: 60.94%] [G loss: 2.298510]\n",
      "epoch:9 step:8839 [D loss: 0.649967, acc: 61.72%] [G loss: 2.365501]\n",
      "epoch:9 step:8840 [D loss: 0.659075, acc: 60.94%] [G loss: 2.184918]\n",
      "epoch:9 step:8841 [D loss: 0.646240, acc: 66.41%] [G loss: 1.899574]\n",
      "epoch:9 step:8842 [D loss: 0.648636, acc: 61.72%] [G loss: 2.084783]\n",
      "epoch:9 step:8843 [D loss: 0.588146, acc: 72.66%] [G loss: 2.121960]\n",
      "epoch:9 step:8844 [D loss: 0.663747, acc: 62.50%] [G loss: 2.039610]\n",
      "epoch:9 step:8845 [D loss: 0.617377, acc: 64.84%] [G loss: 2.055658]\n",
      "epoch:9 step:8846 [D loss: 0.640589, acc: 65.62%] [G loss: 2.132621]\n",
      "epoch:9 step:8847 [D loss: 0.681591, acc: 66.41%] [G loss: 2.043637]\n",
      "epoch:9 step:8848 [D loss: 0.625505, acc: 65.62%] [G loss: 2.155089]\n",
      "epoch:9 step:8849 [D loss: 0.595886, acc: 70.31%] [G loss: 2.135815]\n",
      "epoch:9 step:8850 [D loss: 0.648237, acc: 64.06%] [G loss: 2.056449]\n",
      "epoch:9 step:8851 [D loss: 0.647962, acc: 64.06%] [G loss: 2.009990]\n",
      "epoch:9 step:8852 [D loss: 0.719428, acc: 53.91%] [G loss: 2.001287]\n",
      "epoch:9 step:8853 [D loss: 0.711659, acc: 48.44%] [G loss: 1.972945]\n",
      "epoch:9 step:8854 [D loss: 0.677679, acc: 61.72%] [G loss: 1.980269]\n",
      "epoch:9 step:8855 [D loss: 0.628966, acc: 66.41%] [G loss: 2.022135]\n",
      "epoch:9 step:8856 [D loss: 0.642740, acc: 64.06%] [G loss: 1.987996]\n",
      "epoch:9 step:8857 [D loss: 0.625365, acc: 64.84%] [G loss: 2.062063]\n",
      "epoch:9 step:8858 [D loss: 0.595300, acc: 70.31%] [G loss: 2.192807]\n",
      "epoch:9 step:8859 [D loss: 0.573780, acc: 68.75%] [G loss: 2.218805]\n",
      "epoch:9 step:8860 [D loss: 0.573244, acc: 72.66%] [G loss: 2.346112]\n",
      "epoch:9 step:8861 [D loss: 0.575581, acc: 72.66%] [G loss: 2.341637]\n",
      "epoch:9 step:8862 [D loss: 0.628417, acc: 68.75%] [G loss: 2.497369]\n",
      "epoch:9 step:8863 [D loss: 0.552142, acc: 72.66%] [G loss: 2.455706]\n",
      "epoch:9 step:8864 [D loss: 0.669918, acc: 58.59%] [G loss: 2.310959]\n",
      "epoch:9 step:8865 [D loss: 0.701071, acc: 58.59%] [G loss: 1.983987]\n",
      "epoch:9 step:8866 [D loss: 0.654428, acc: 60.94%] [G loss: 1.949493]\n",
      "epoch:9 step:8867 [D loss: 0.645525, acc: 63.28%] [G loss: 2.282644]\n",
      "epoch:9 step:8868 [D loss: 0.639300, acc: 60.94%] [G loss: 2.280374]\n",
      "epoch:9 step:8869 [D loss: 0.626118, acc: 66.41%] [G loss: 2.096033]\n",
      "epoch:9 step:8870 [D loss: 0.710241, acc: 53.91%] [G loss: 1.934603]\n",
      "epoch:9 step:8871 [D loss: 0.686296, acc: 60.94%] [G loss: 1.800458]\n",
      "epoch:9 step:8872 [D loss: 0.689040, acc: 55.47%] [G loss: 1.853856]\n",
      "epoch:9 step:8873 [D loss: 0.663681, acc: 60.94%] [G loss: 1.944877]\n",
      "epoch:9 step:8874 [D loss: 0.656413, acc: 57.81%] [G loss: 1.942480]\n",
      "epoch:9 step:8875 [D loss: 0.619976, acc: 67.19%] [G loss: 2.009593]\n",
      "epoch:9 step:8876 [D loss: 0.692152, acc: 56.25%] [G loss: 1.919482]\n",
      "epoch:9 step:8877 [D loss: 0.648072, acc: 67.19%] [G loss: 1.952429]\n",
      "epoch:9 step:8878 [D loss: 0.627088, acc: 65.62%] [G loss: 2.253470]\n",
      "epoch:9 step:8879 [D loss: 0.597024, acc: 70.31%] [G loss: 1.972936]\n",
      "epoch:9 step:8880 [D loss: 0.665771, acc: 62.50%] [G loss: 1.959663]\n",
      "epoch:9 step:8881 [D loss: 0.591894, acc: 71.09%] [G loss: 2.000124]\n",
      "epoch:9 step:8882 [D loss: 0.619776, acc: 63.28%] [G loss: 2.034516]\n",
      "epoch:9 step:8883 [D loss: 0.626188, acc: 65.62%] [G loss: 1.895859]\n",
      "epoch:9 step:8884 [D loss: 0.668456, acc: 58.59%] [G loss: 2.083455]\n",
      "epoch:9 step:8885 [D loss: 0.618617, acc: 67.19%] [G loss: 2.114983]\n",
      "epoch:9 step:8886 [D loss: 0.595939, acc: 67.97%] [G loss: 2.203428]\n",
      "epoch:9 step:8887 [D loss: 0.688869, acc: 54.69%] [G loss: 2.089997]\n",
      "epoch:9 step:8888 [D loss: 0.667183, acc: 61.72%] [G loss: 1.994855]\n",
      "epoch:9 step:8889 [D loss: 0.618646, acc: 63.28%] [G loss: 2.025382]\n",
      "epoch:9 step:8890 [D loss: 0.630185, acc: 68.75%] [G loss: 2.080023]\n",
      "epoch:9 step:8891 [D loss: 0.712263, acc: 57.03%] [G loss: 2.012369]\n",
      "epoch:9 step:8892 [D loss: 0.710377, acc: 48.44%] [G loss: 1.950911]\n",
      "epoch:9 step:8893 [D loss: 0.664837, acc: 63.28%] [G loss: 1.858828]\n",
      "epoch:9 step:8894 [D loss: 0.621969, acc: 60.94%] [G loss: 1.953258]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:9 step:8895 [D loss: 0.622614, acc: 64.06%] [G loss: 2.051542]\n",
      "epoch:9 step:8896 [D loss: 0.609060, acc: 68.75%] [G loss: 2.020582]\n",
      "epoch:9 step:8897 [D loss: 0.623623, acc: 63.28%] [G loss: 1.963964]\n",
      "epoch:9 step:8898 [D loss: 0.675895, acc: 53.91%] [G loss: 1.960897]\n",
      "epoch:9 step:8899 [D loss: 0.595828, acc: 69.53%] [G loss: 2.279258]\n",
      "epoch:9 step:8900 [D loss: 0.650175, acc: 67.97%] [G loss: 2.154199]\n",
      "epoch:9 step:8901 [D loss: 0.586810, acc: 66.41%] [G loss: 2.204541]\n",
      "epoch:9 step:8902 [D loss: 0.581948, acc: 71.09%] [G loss: 2.205342]\n",
      "epoch:9 step:8903 [D loss: 0.596891, acc: 68.75%] [G loss: 2.340036]\n",
      "epoch:9 step:8904 [D loss: 0.660141, acc: 70.31%] [G loss: 2.433506]\n",
      "epoch:9 step:8905 [D loss: 0.616532, acc: 63.28%] [G loss: 2.434433]\n",
      "epoch:9 step:8906 [D loss: 0.654165, acc: 60.94%] [G loss: 2.037021]\n",
      "epoch:9 step:8907 [D loss: 0.573596, acc: 75.78%] [G loss: 2.389487]\n",
      "epoch:9 step:8908 [D loss: 0.663169, acc: 58.59%] [G loss: 2.123164]\n",
      "epoch:9 step:8909 [D loss: 0.650489, acc: 64.06%] [G loss: 2.085763]\n",
      "epoch:9 step:8910 [D loss: 0.678933, acc: 60.16%] [G loss: 1.880313]\n",
      "epoch:9 step:8911 [D loss: 0.693050, acc: 62.50%] [G loss: 1.917096]\n",
      "epoch:9 step:8912 [D loss: 0.620140, acc: 67.19%] [G loss: 2.290017]\n",
      "epoch:9 step:8913 [D loss: 0.628738, acc: 64.84%] [G loss: 2.113145]\n",
      "epoch:9 step:8914 [D loss: 0.625138, acc: 70.31%] [G loss: 2.340403]\n",
      "epoch:9 step:8915 [D loss: 0.769477, acc: 50.00%] [G loss: 1.897625]\n",
      "epoch:9 step:8916 [D loss: 0.654082, acc: 60.94%] [G loss: 2.053906]\n",
      "epoch:9 step:8917 [D loss: 0.583703, acc: 68.75%] [G loss: 2.231175]\n",
      "epoch:9 step:8918 [D loss: 0.680151, acc: 60.94%] [G loss: 1.862785]\n",
      "epoch:9 step:8919 [D loss: 0.584801, acc: 70.31%] [G loss: 2.027768]\n",
      "epoch:9 step:8920 [D loss: 0.651112, acc: 61.72%] [G loss: 1.985987]\n",
      "epoch:9 step:8921 [D loss: 0.619267, acc: 74.22%] [G loss: 2.136205]\n",
      "epoch:9 step:8922 [D loss: 0.716095, acc: 57.03%] [G loss: 1.911182]\n",
      "epoch:9 step:8923 [D loss: 0.633556, acc: 60.16%] [G loss: 2.105217]\n",
      "epoch:9 step:8924 [D loss: 0.596979, acc: 68.75%] [G loss: 2.101523]\n",
      "epoch:9 step:8925 [D loss: 0.613951, acc: 64.84%] [G loss: 2.155353]\n",
      "epoch:9 step:8926 [D loss: 0.589606, acc: 67.97%] [G loss: 2.092064]\n",
      "epoch:9 step:8927 [D loss: 0.569932, acc: 69.53%] [G loss: 2.248140]\n",
      "epoch:9 step:8928 [D loss: 0.618795, acc: 64.06%] [G loss: 2.398257]\n",
      "epoch:9 step:8929 [D loss: 0.602525, acc: 68.75%] [G loss: 1.974254]\n",
      "epoch:9 step:8930 [D loss: 0.600266, acc: 73.44%] [G loss: 2.163226]\n",
      "epoch:9 step:8931 [D loss: 0.588679, acc: 67.19%] [G loss: 2.374803]\n",
      "epoch:9 step:8932 [D loss: 0.641447, acc: 65.62%] [G loss: 2.310480]\n",
      "epoch:9 step:8933 [D loss: 0.695122, acc: 59.38%] [G loss: 1.886917]\n",
      "epoch:9 step:8934 [D loss: 0.746532, acc: 52.34%] [G loss: 1.848007]\n",
      "epoch:9 step:8935 [D loss: 0.664015, acc: 60.16%] [G loss: 1.945912]\n",
      "epoch:9 step:8936 [D loss: 0.612859, acc: 66.41%] [G loss: 2.049654]\n",
      "epoch:9 step:8937 [D loss: 0.571521, acc: 70.31%] [G loss: 2.129922]\n",
      "epoch:9 step:8938 [D loss: 0.660708, acc: 64.06%] [G loss: 2.227906]\n",
      "epoch:9 step:8939 [D loss: 0.731457, acc: 53.91%] [G loss: 1.995350]\n",
      "epoch:9 step:8940 [D loss: 0.623000, acc: 67.97%] [G loss: 1.936222]\n",
      "epoch:9 step:8941 [D loss: 0.582503, acc: 67.97%] [G loss: 2.291247]\n",
      "epoch:9 step:8942 [D loss: 0.628291, acc: 62.50%] [G loss: 2.043944]\n",
      "epoch:9 step:8943 [D loss: 0.608339, acc: 68.75%] [G loss: 2.092425]\n",
      "epoch:9 step:8944 [D loss: 0.686161, acc: 57.81%] [G loss: 1.967235]\n",
      "epoch:9 step:8945 [D loss: 0.626737, acc: 64.84%] [G loss: 1.904102]\n",
      "epoch:9 step:8946 [D loss: 0.643149, acc: 60.94%] [G loss: 1.900364]\n",
      "epoch:9 step:8947 [D loss: 0.625600, acc: 64.06%] [G loss: 1.994486]\n",
      "epoch:9 step:8948 [D loss: 0.602976, acc: 64.84%] [G loss: 2.309846]\n",
      "epoch:9 step:8949 [D loss: 0.587769, acc: 70.31%] [G loss: 2.244101]\n",
      "epoch:9 step:8950 [D loss: 0.671028, acc: 59.38%] [G loss: 2.095963]\n",
      "epoch:9 step:8951 [D loss: 0.638285, acc: 67.19%] [G loss: 1.996670]\n",
      "epoch:9 step:8952 [D loss: 0.652161, acc: 55.47%] [G loss: 2.114580]\n",
      "epoch:9 step:8953 [D loss: 0.660677, acc: 62.50%] [G loss: 2.057202]\n",
      "epoch:9 step:8954 [D loss: 0.606670, acc: 64.84%] [G loss: 2.191984]\n",
      "epoch:9 step:8955 [D loss: 0.574832, acc: 68.75%] [G loss: 2.284016]\n",
      "epoch:9 step:8956 [D loss: 0.627724, acc: 67.19%] [G loss: 2.148056]\n",
      "epoch:9 step:8957 [D loss: 0.588251, acc: 71.88%] [G loss: 2.133025]\n",
      "epoch:9 step:8958 [D loss: 0.616948, acc: 67.97%] [G loss: 1.984380]\n",
      "epoch:9 step:8959 [D loss: 0.646797, acc: 62.50%] [G loss: 2.125097]\n",
      "epoch:9 step:8960 [D loss: 0.667480, acc: 62.50%] [G loss: 1.904006]\n",
      "epoch:9 step:8961 [D loss: 0.706439, acc: 53.12%] [G loss: 1.972821]\n",
      "epoch:9 step:8962 [D loss: 0.617399, acc: 65.62%] [G loss: 1.945115]\n",
      "epoch:9 step:8963 [D loss: 0.592328, acc: 69.53%] [G loss: 1.934770]\n",
      "epoch:9 step:8964 [D loss: 0.751897, acc: 57.81%] [G loss: 1.808964]\n",
      "epoch:9 step:8965 [D loss: 0.649610, acc: 61.72%] [G loss: 1.989263]\n",
      "epoch:9 step:8966 [D loss: 0.599751, acc: 65.62%] [G loss: 2.270784]\n",
      "epoch:9 step:8967 [D loss: 0.589372, acc: 71.09%] [G loss: 2.209365]\n",
      "epoch:9 step:8968 [D loss: 0.608620, acc: 66.41%] [G loss: 2.253565]\n",
      "epoch:9 step:8969 [D loss: 0.663414, acc: 65.62%] [G loss: 1.972555]\n",
      "epoch:9 step:8970 [D loss: 0.622923, acc: 64.84%] [G loss: 1.940472]\n",
      "epoch:9 step:8971 [D loss: 0.648359, acc: 61.72%] [G loss: 2.034478]\n",
      "epoch:9 step:8972 [D loss: 0.625757, acc: 67.97%] [G loss: 2.013445]\n",
      "epoch:9 step:8973 [D loss: 0.703317, acc: 59.38%] [G loss: 1.990992]\n",
      "epoch:9 step:8974 [D loss: 0.665849, acc: 60.16%] [G loss: 1.996836]\n",
      "epoch:9 step:8975 [D loss: 0.697352, acc: 57.03%] [G loss: 1.863109]\n",
      "epoch:9 step:8976 [D loss: 0.617813, acc: 68.75%] [G loss: 1.913890]\n",
      "epoch:9 step:8977 [D loss: 0.617367, acc: 67.97%] [G loss: 2.107610]\n",
      "epoch:9 step:8978 [D loss: 0.622601, acc: 63.28%] [G loss: 1.967099]\n",
      "epoch:9 step:8979 [D loss: 0.616927, acc: 67.97%] [G loss: 1.963628]\n",
      "epoch:9 step:8980 [D loss: 0.590038, acc: 69.53%] [G loss: 2.102320]\n",
      "epoch:9 step:8981 [D loss: 0.620606, acc: 64.84%] [G loss: 2.116920]\n",
      "epoch:9 step:8982 [D loss: 0.597419, acc: 70.31%] [G loss: 2.167453]\n",
      "epoch:9 step:8983 [D loss: 0.640848, acc: 66.41%] [G loss: 2.130992]\n",
      "epoch:9 step:8984 [D loss: 0.581269, acc: 64.84%] [G loss: 2.231206]\n",
      "epoch:9 step:8985 [D loss: 0.583944, acc: 71.88%] [G loss: 2.182228]\n",
      "epoch:9 step:8986 [D loss: 0.627778, acc: 67.19%] [G loss: 1.874966]\n",
      "epoch:9 step:8987 [D loss: 0.595410, acc: 66.41%] [G loss: 2.352890]\n",
      "epoch:9 step:8988 [D loss: 0.591295, acc: 68.75%] [G loss: 2.138928]\n",
      "epoch:9 step:8989 [D loss: 0.585938, acc: 68.75%] [G loss: 2.185091]\n",
      "epoch:9 step:8990 [D loss: 0.555441, acc: 73.44%] [G loss: 2.351197]\n",
      "epoch:9 step:8991 [D loss: 0.639765, acc: 64.84%] [G loss: 2.307940]\n",
      "epoch:9 step:8992 [D loss: 0.624927, acc: 64.06%] [G loss: 2.098853]\n",
      "epoch:9 step:8993 [D loss: 0.673123, acc: 60.16%] [G loss: 1.987726]\n",
      "epoch:9 step:8994 [D loss: 0.641415, acc: 63.28%] [G loss: 2.124787]\n",
      "epoch:9 step:8995 [D loss: 0.643219, acc: 61.72%] [G loss: 2.016706]\n",
      "epoch:9 step:8996 [D loss: 0.644568, acc: 65.62%] [G loss: 2.079339]\n",
      "epoch:9 step:8997 [D loss: 0.651552, acc: 54.69%] [G loss: 2.287424]\n",
      "epoch:9 step:8998 [D loss: 0.615863, acc: 68.75%] [G loss: 2.049155]\n",
      "epoch:9 step:8999 [D loss: 0.673736, acc: 60.16%] [G loss: 1.947862]\n",
      "epoch:9 step:9000 [D loss: 0.626717, acc: 64.06%] [G loss: 2.021423]\n",
      "##############\n",
      "[2.6289048  1.47638805 6.5350212  5.00413087 3.82296755 5.71636162\n",
      " 4.45909336 4.87626391 4.9513409  3.69099531]\n",
      "##########\n",
      "epoch:9 step:9001 [D loss: 0.683497, acc: 62.50%] [G loss: 2.099365]\n",
      "epoch:9 step:9002 [D loss: 0.731261, acc: 60.94%] [G loss: 2.007169]\n",
      "epoch:9 step:9003 [D loss: 0.641307, acc: 62.50%] [G loss: 2.030614]\n",
      "epoch:9 step:9004 [D loss: 0.601618, acc: 72.66%] [G loss: 2.094516]\n",
      "epoch:9 step:9005 [D loss: 0.599022, acc: 67.19%] [G loss: 1.926920]\n",
      "epoch:9 step:9006 [D loss: 0.596853, acc: 68.75%] [G loss: 1.957314]\n",
      "epoch:9 step:9007 [D loss: 0.612852, acc: 66.41%] [G loss: 1.967099]\n",
      "epoch:9 step:9008 [D loss: 0.610318, acc: 66.41%] [G loss: 2.064374]\n",
      "epoch:9 step:9009 [D loss: 0.674464, acc: 57.81%] [G loss: 1.947779]\n",
      "epoch:9 step:9010 [D loss: 0.733362, acc: 53.91%] [G loss: 1.916132]\n",
      "epoch:9 step:9011 [D loss: 0.617816, acc: 64.06%] [G loss: 1.955531]\n",
      "epoch:9 step:9012 [D loss: 0.639586, acc: 63.28%] [G loss: 1.905508]\n",
      "epoch:9 step:9013 [D loss: 0.645639, acc: 60.16%] [G loss: 2.048508]\n",
      "epoch:9 step:9014 [D loss: 0.620192, acc: 66.41%] [G loss: 1.953797]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:9 step:9015 [D loss: 0.603022, acc: 71.09%] [G loss: 2.110273]\n",
      "epoch:9 step:9016 [D loss: 0.665483, acc: 58.59%] [G loss: 1.979339]\n",
      "epoch:9 step:9017 [D loss: 0.589028, acc: 65.62%] [G loss: 2.092504]\n",
      "epoch:9 step:9018 [D loss: 0.630029, acc: 63.28%] [G loss: 1.990562]\n",
      "epoch:9 step:9019 [D loss: 0.628470, acc: 66.41%] [G loss: 1.937704]\n",
      "epoch:9 step:9020 [D loss: 0.613965, acc: 64.84%] [G loss: 2.043556]\n",
      "epoch:9 step:9021 [D loss: 0.648880, acc: 61.72%] [G loss: 2.081288]\n",
      "epoch:9 step:9022 [D loss: 0.647975, acc: 59.38%] [G loss: 2.142144]\n",
      "epoch:9 step:9023 [D loss: 0.696076, acc: 60.16%] [G loss: 1.979373]\n",
      "epoch:9 step:9024 [D loss: 0.608633, acc: 67.19%] [G loss: 2.125579]\n",
      "epoch:9 step:9025 [D loss: 0.674470, acc: 66.41%] [G loss: 2.004853]\n",
      "epoch:9 step:9026 [D loss: 0.632018, acc: 63.28%] [G loss: 1.983199]\n",
      "epoch:9 step:9027 [D loss: 0.676820, acc: 62.50%] [G loss: 1.954069]\n",
      "epoch:9 step:9028 [D loss: 0.639657, acc: 64.06%] [G loss: 2.033509]\n",
      "epoch:9 step:9029 [D loss: 0.595236, acc: 67.19%] [G loss: 2.020939]\n",
      "epoch:9 step:9030 [D loss: 0.639040, acc: 60.94%] [G loss: 2.007272]\n",
      "epoch:9 step:9031 [D loss: 0.623045, acc: 63.28%] [G loss: 2.159326]\n",
      "epoch:9 step:9032 [D loss: 0.621369, acc: 68.75%] [G loss: 2.116185]\n",
      "epoch:9 step:9033 [D loss: 0.738223, acc: 55.47%] [G loss: 2.109957]\n",
      "epoch:9 step:9034 [D loss: 0.706423, acc: 61.72%] [G loss: 1.937547]\n",
      "epoch:9 step:9035 [D loss: 0.618231, acc: 64.84%] [G loss: 2.056848]\n",
      "epoch:9 step:9036 [D loss: 0.584962, acc: 66.41%] [G loss: 2.124824]\n",
      "epoch:9 step:9037 [D loss: 0.629893, acc: 64.84%] [G loss: 2.019770]\n",
      "epoch:9 step:9038 [D loss: 0.668362, acc: 63.28%] [G loss: 2.132366]\n",
      "epoch:9 step:9039 [D loss: 0.659701, acc: 64.06%] [G loss: 2.040615]\n",
      "epoch:9 step:9040 [D loss: 0.653408, acc: 67.19%] [G loss: 1.968772]\n",
      "epoch:9 step:9041 [D loss: 0.617182, acc: 62.50%] [G loss: 2.149133]\n",
      "epoch:9 step:9042 [D loss: 0.617623, acc: 64.84%] [G loss: 2.053524]\n",
      "epoch:9 step:9043 [D loss: 0.653851, acc: 65.62%] [G loss: 1.970610]\n",
      "epoch:9 step:9044 [D loss: 0.626707, acc: 67.97%] [G loss: 1.792286]\n",
      "epoch:9 step:9045 [D loss: 0.600235, acc: 68.75%] [G loss: 2.055816]\n",
      "epoch:9 step:9046 [D loss: 0.622103, acc: 67.19%] [G loss: 2.164264]\n",
      "epoch:9 step:9047 [D loss: 0.683508, acc: 56.25%] [G loss: 1.989700]\n",
      "epoch:9 step:9048 [D loss: 0.697883, acc: 52.34%] [G loss: 1.859479]\n",
      "epoch:9 step:9049 [D loss: 0.641405, acc: 64.84%] [G loss: 1.961476]\n",
      "epoch:9 step:9050 [D loss: 0.624466, acc: 66.41%] [G loss: 2.061445]\n",
      "epoch:9 step:9051 [D loss: 0.601694, acc: 69.53%] [G loss: 1.984707]\n",
      "epoch:9 step:9052 [D loss: 0.623399, acc: 65.62%] [G loss: 1.932063]\n",
      "epoch:9 step:9053 [D loss: 0.602427, acc: 62.50%] [G loss: 1.927474]\n",
      "epoch:9 step:9054 [D loss: 0.637862, acc: 64.84%] [G loss: 1.938842]\n",
      "epoch:9 step:9055 [D loss: 0.683749, acc: 56.25%] [G loss: 2.069085]\n",
      "epoch:9 step:9056 [D loss: 0.648913, acc: 60.94%] [G loss: 2.000582]\n",
      "epoch:9 step:9057 [D loss: 0.567198, acc: 75.78%] [G loss: 2.117064]\n",
      "epoch:9 step:9058 [D loss: 0.686538, acc: 60.16%] [G loss: 1.925839]\n",
      "epoch:9 step:9059 [D loss: 0.686659, acc: 54.69%] [G loss: 2.058347]\n",
      "epoch:9 step:9060 [D loss: 0.641257, acc: 60.16%] [G loss: 1.960517]\n",
      "epoch:9 step:9061 [D loss: 0.677782, acc: 57.03%] [G loss: 1.855117]\n",
      "epoch:9 step:9062 [D loss: 0.675235, acc: 60.94%] [G loss: 1.864695]\n",
      "epoch:9 step:9063 [D loss: 0.569188, acc: 73.44%] [G loss: 1.995418]\n",
      "epoch:9 step:9064 [D loss: 0.610710, acc: 67.19%] [G loss: 2.158349]\n",
      "epoch:9 step:9065 [D loss: 0.588718, acc: 67.19%] [G loss: 2.257965]\n",
      "epoch:9 step:9066 [D loss: 0.631469, acc: 67.19%] [G loss: 2.048764]\n",
      "epoch:9 step:9067 [D loss: 0.612435, acc: 69.53%] [G loss: 2.128883]\n",
      "epoch:9 step:9068 [D loss: 0.584225, acc: 68.75%] [G loss: 2.217971]\n",
      "epoch:9 step:9069 [D loss: 0.612566, acc: 64.06%] [G loss: 2.089429]\n",
      "epoch:9 step:9070 [D loss: 0.614982, acc: 70.31%] [G loss: 2.372116]\n",
      "epoch:9 step:9071 [D loss: 0.613731, acc: 67.19%] [G loss: 2.028067]\n",
      "epoch:9 step:9072 [D loss: 0.634459, acc: 66.41%] [G loss: 2.028533]\n",
      "epoch:9 step:9073 [D loss: 0.639773, acc: 64.06%] [G loss: 2.088562]\n",
      "epoch:9 step:9074 [D loss: 0.602965, acc: 64.84%] [G loss: 2.212420]\n",
      "epoch:9 step:9075 [D loss: 0.589311, acc: 65.62%] [G loss: 2.344820]\n",
      "epoch:9 step:9076 [D loss: 0.678582, acc: 60.94%] [G loss: 2.121470]\n",
      "epoch:9 step:9077 [D loss: 0.626682, acc: 69.53%] [G loss: 1.982174]\n",
      "epoch:9 step:9078 [D loss: 0.638597, acc: 64.06%] [G loss: 1.987046]\n",
      "epoch:9 step:9079 [D loss: 0.645169, acc: 64.84%] [G loss: 2.096957]\n",
      "epoch:9 step:9080 [D loss: 0.641041, acc: 68.75%] [G loss: 2.101650]\n",
      "epoch:9 step:9081 [D loss: 0.521785, acc: 74.22%] [G loss: 2.489137]\n",
      "epoch:9 step:9082 [D loss: 0.615885, acc: 64.84%] [G loss: 2.464770]\n",
      "epoch:9 step:9083 [D loss: 0.536151, acc: 78.12%] [G loss: 2.203808]\n",
      "epoch:9 step:9084 [D loss: 0.643226, acc: 65.62%] [G loss: 2.154523]\n",
      "epoch:9 step:9085 [D loss: 0.599709, acc: 68.75%] [G loss: 2.103769]\n",
      "epoch:9 step:9086 [D loss: 0.614220, acc: 67.97%] [G loss: 2.031668]\n",
      "epoch:9 step:9087 [D loss: 0.619465, acc: 60.94%] [G loss: 2.199308]\n",
      "epoch:9 step:9088 [D loss: 0.626626, acc: 66.41%] [G loss: 1.992002]\n",
      "epoch:9 step:9089 [D loss: 0.633946, acc: 61.72%] [G loss: 2.124799]\n",
      "epoch:9 step:9090 [D loss: 0.659456, acc: 60.16%] [G loss: 1.917914]\n",
      "epoch:9 step:9091 [D loss: 0.656203, acc: 61.72%] [G loss: 1.936076]\n",
      "epoch:9 step:9092 [D loss: 0.608786, acc: 67.19%] [G loss: 2.017594]\n",
      "epoch:9 step:9093 [D loss: 0.633382, acc: 65.62%] [G loss: 2.043459]\n",
      "epoch:9 step:9094 [D loss: 0.651888, acc: 64.06%] [G loss: 2.097264]\n",
      "epoch:9 step:9095 [D loss: 0.627255, acc: 64.84%] [G loss: 2.023153]\n",
      "epoch:9 step:9096 [D loss: 0.625021, acc: 68.75%] [G loss: 2.095323]\n",
      "epoch:9 step:9097 [D loss: 0.624465, acc: 62.50%] [G loss: 2.116384]\n",
      "epoch:9 step:9098 [D loss: 0.685509, acc: 58.59%] [G loss: 2.003286]\n",
      "epoch:9 step:9099 [D loss: 0.633935, acc: 62.50%] [G loss: 1.786045]\n",
      "epoch:9 step:9100 [D loss: 0.726822, acc: 50.78%] [G loss: 1.795457]\n",
      "epoch:9 step:9101 [D loss: 0.641329, acc: 62.50%] [G loss: 1.882743]\n",
      "epoch:9 step:9102 [D loss: 0.643443, acc: 63.28%] [G loss: 1.931264]\n",
      "epoch:9 step:9103 [D loss: 0.653741, acc: 60.16%] [G loss: 1.976492]\n",
      "epoch:9 step:9104 [D loss: 0.618042, acc: 67.19%] [G loss: 1.915422]\n",
      "epoch:9 step:9105 [D loss: 0.591481, acc: 67.19%] [G loss: 1.956454]\n",
      "epoch:9 step:9106 [D loss: 0.650811, acc: 58.59%] [G loss: 2.025437]\n",
      "epoch:9 step:9107 [D loss: 0.667571, acc: 60.94%] [G loss: 2.060466]\n",
      "epoch:9 step:9108 [D loss: 0.653884, acc: 64.84%] [G loss: 2.026852]\n",
      "epoch:9 step:9109 [D loss: 0.607107, acc: 69.53%] [G loss: 2.061651]\n",
      "epoch:9 step:9110 [D loss: 0.566864, acc: 70.31%] [G loss: 2.154190]\n",
      "epoch:9 step:9111 [D loss: 0.614363, acc: 63.28%] [G loss: 2.104446]\n",
      "epoch:9 step:9112 [D loss: 0.603915, acc: 67.97%] [G loss: 2.223571]\n",
      "epoch:9 step:9113 [D loss: 0.613397, acc: 64.84%] [G loss: 2.107570]\n",
      "epoch:9 step:9114 [D loss: 0.601161, acc: 66.41%] [G loss: 2.204016]\n",
      "epoch:9 step:9115 [D loss: 0.660283, acc: 64.06%] [G loss: 2.015461]\n",
      "epoch:9 step:9116 [D loss: 0.693105, acc: 58.59%] [G loss: 2.007182]\n",
      "epoch:9 step:9117 [D loss: 0.632031, acc: 68.75%] [G loss: 2.049611]\n",
      "epoch:9 step:9118 [D loss: 0.655021, acc: 64.06%] [G loss: 1.971359]\n",
      "epoch:9 step:9119 [D loss: 0.606607, acc: 63.28%] [G loss: 2.109540]\n",
      "epoch:9 step:9120 [D loss: 0.637511, acc: 70.31%] [G loss: 2.066370]\n",
      "epoch:9 step:9121 [D loss: 0.635986, acc: 67.97%] [G loss: 1.989603]\n",
      "epoch:9 step:9122 [D loss: 0.669007, acc: 62.50%] [G loss: 2.063100]\n",
      "epoch:9 step:9123 [D loss: 0.606359, acc: 70.31%] [G loss: 2.222655]\n",
      "epoch:9 step:9124 [D loss: 0.619849, acc: 66.41%] [G loss: 2.238701]\n",
      "epoch:9 step:9125 [D loss: 0.624993, acc: 66.41%] [G loss: 2.266833]\n",
      "epoch:9 step:9126 [D loss: 0.591546, acc: 72.66%] [G loss: 2.046697]\n",
      "epoch:9 step:9127 [D loss: 0.585295, acc: 69.53%] [G loss: 2.421694]\n",
      "epoch:9 step:9128 [D loss: 0.618222, acc: 64.06%] [G loss: 2.276112]\n",
      "epoch:9 step:9129 [D loss: 0.618767, acc: 71.09%] [G loss: 2.166448]\n",
      "epoch:9 step:9130 [D loss: 0.620371, acc: 66.41%] [G loss: 2.030072]\n",
      "epoch:9 step:9131 [D loss: 0.624300, acc: 67.19%] [G loss: 2.244194]\n",
      "epoch:9 step:9132 [D loss: 0.535897, acc: 75.00%] [G loss: 2.290219]\n",
      "epoch:9 step:9133 [D loss: 0.679491, acc: 60.16%] [G loss: 1.979071]\n",
      "epoch:9 step:9134 [D loss: 0.590187, acc: 70.31%] [G loss: 2.026167]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:9 step:9135 [D loss: 0.591943, acc: 68.75%] [G loss: 2.020798]\n",
      "epoch:9 step:9136 [D loss: 0.636794, acc: 64.84%] [G loss: 1.928736]\n",
      "epoch:9 step:9137 [D loss: 0.664914, acc: 64.06%] [G loss: 1.982774]\n",
      "epoch:9 step:9138 [D loss: 0.631987, acc: 65.62%] [G loss: 2.013428]\n",
      "epoch:9 step:9139 [D loss: 0.614153, acc: 65.62%] [G loss: 2.226877]\n",
      "epoch:9 step:9140 [D loss: 0.613764, acc: 67.97%] [G loss: 2.181704]\n",
      "epoch:9 step:9141 [D loss: 0.515751, acc: 77.34%] [G loss: 2.365232]\n",
      "epoch:9 step:9142 [D loss: 0.648685, acc: 63.28%] [G loss: 2.235587]\n",
      "epoch:9 step:9143 [D loss: 0.704655, acc: 60.16%] [G loss: 1.973992]\n",
      "epoch:9 step:9144 [D loss: 0.623147, acc: 67.19%] [G loss: 2.108977]\n",
      "epoch:9 step:9145 [D loss: 0.648796, acc: 57.03%] [G loss: 2.122930]\n",
      "epoch:9 step:9146 [D loss: 0.651645, acc: 61.72%] [G loss: 2.067458]\n",
      "epoch:9 step:9147 [D loss: 0.603605, acc: 69.53%] [G loss: 1.981019]\n",
      "epoch:9 step:9148 [D loss: 0.635720, acc: 64.84%] [G loss: 2.045825]\n",
      "epoch:9 step:9149 [D loss: 0.673047, acc: 62.50%] [G loss: 1.834847]\n",
      "epoch:9 step:9150 [D loss: 0.634353, acc: 61.72%] [G loss: 2.024672]\n",
      "epoch:9 step:9151 [D loss: 0.663032, acc: 60.16%] [G loss: 2.017501]\n",
      "epoch:9 step:9152 [D loss: 0.620328, acc: 64.06%] [G loss: 2.120592]\n",
      "epoch:9 step:9153 [D loss: 0.661903, acc: 62.50%] [G loss: 2.074005]\n",
      "epoch:9 step:9154 [D loss: 0.623027, acc: 69.53%] [G loss: 2.252611]\n",
      "epoch:9 step:9155 [D loss: 0.683465, acc: 58.59%] [G loss: 1.923628]\n",
      "epoch:9 step:9156 [D loss: 0.623935, acc: 68.75%] [G loss: 2.040857]\n",
      "epoch:9 step:9157 [D loss: 0.571289, acc: 74.22%] [G loss: 2.200849]\n",
      "epoch:9 step:9158 [D loss: 0.575091, acc: 64.06%] [G loss: 2.172985]\n",
      "epoch:9 step:9159 [D loss: 0.692398, acc: 57.81%] [G loss: 2.052881]\n",
      "epoch:9 step:9160 [D loss: 0.616374, acc: 66.41%] [G loss: 1.998787]\n",
      "epoch:9 step:9161 [D loss: 0.588604, acc: 71.09%] [G loss: 2.221958]\n",
      "epoch:9 step:9162 [D loss: 0.671609, acc: 60.94%] [G loss: 2.101235]\n",
      "epoch:9 step:9163 [D loss: 0.715512, acc: 58.59%] [G loss: 2.118227]\n",
      "epoch:9 step:9164 [D loss: 0.616329, acc: 61.72%] [G loss: 2.006316]\n",
      "epoch:9 step:9165 [D loss: 0.643884, acc: 60.16%] [G loss: 1.987828]\n",
      "epoch:9 step:9166 [D loss: 0.664327, acc: 61.72%] [G loss: 2.082841]\n",
      "epoch:9 step:9167 [D loss: 0.687040, acc: 58.59%] [G loss: 1.936496]\n",
      "epoch:9 step:9168 [D loss: 0.697095, acc: 57.81%] [G loss: 1.975414]\n",
      "epoch:9 step:9169 [D loss: 0.547617, acc: 71.09%] [G loss: 2.080014]\n",
      "epoch:9 step:9170 [D loss: 0.631658, acc: 61.72%] [G loss: 1.897200]\n",
      "epoch:9 step:9171 [D loss: 0.565139, acc: 69.53%] [G loss: 1.990970]\n",
      "epoch:9 step:9172 [D loss: 0.677578, acc: 64.84%] [G loss: 2.080236]\n",
      "epoch:9 step:9173 [D loss: 0.583778, acc: 71.09%] [G loss: 2.141826]\n",
      "epoch:9 step:9174 [D loss: 0.673083, acc: 58.59%] [G loss: 1.964241]\n",
      "epoch:9 step:9175 [D loss: 0.690291, acc: 55.47%] [G loss: 1.862057]\n",
      "epoch:9 step:9176 [D loss: 0.622084, acc: 61.72%] [G loss: 1.817348]\n",
      "epoch:9 step:9177 [D loss: 0.727698, acc: 61.72%] [G loss: 1.918347]\n",
      "epoch:9 step:9178 [D loss: 0.631955, acc: 66.41%] [G loss: 2.132837]\n",
      "epoch:9 step:9179 [D loss: 0.656456, acc: 57.81%] [G loss: 2.151257]\n",
      "epoch:9 step:9180 [D loss: 0.586402, acc: 67.19%] [G loss: 2.076437]\n",
      "epoch:9 step:9181 [D loss: 0.626054, acc: 68.75%] [G loss: 2.047951]\n",
      "epoch:9 step:9182 [D loss: 0.649068, acc: 61.72%] [G loss: 2.005823]\n",
      "epoch:9 step:9183 [D loss: 0.677914, acc: 57.03%] [G loss: 1.865984]\n",
      "epoch:9 step:9184 [D loss: 0.632928, acc: 64.06%] [G loss: 1.904490]\n",
      "epoch:9 step:9185 [D loss: 0.661962, acc: 61.72%] [G loss: 1.925360]\n",
      "epoch:9 step:9186 [D loss: 0.613170, acc: 68.75%] [G loss: 2.069304]\n",
      "epoch:9 step:9187 [D loss: 0.560683, acc: 71.09%] [G loss: 2.056981]\n",
      "epoch:9 step:9188 [D loss: 0.625521, acc: 66.41%] [G loss: 2.125302]\n",
      "epoch:9 step:9189 [D loss: 0.644740, acc: 59.38%] [G loss: 2.175000]\n",
      "epoch:9 step:9190 [D loss: 0.656962, acc: 60.94%] [G loss: 2.028817]\n",
      "epoch:9 step:9191 [D loss: 0.618812, acc: 68.75%] [G loss: 1.820130]\n",
      "epoch:9 step:9192 [D loss: 0.657009, acc: 58.59%] [G loss: 2.190942]\n",
      "epoch:9 step:9193 [D loss: 0.607703, acc: 70.31%] [G loss: 1.934936]\n",
      "epoch:9 step:9194 [D loss: 0.641086, acc: 59.38%] [G loss: 2.031820]\n",
      "epoch:9 step:9195 [D loss: 0.639009, acc: 64.06%] [G loss: 1.926756]\n",
      "epoch:9 step:9196 [D loss: 0.688678, acc: 59.38%] [G loss: 2.068621]\n",
      "epoch:9 step:9197 [D loss: 0.686686, acc: 60.16%] [G loss: 2.000223]\n",
      "epoch:9 step:9198 [D loss: 0.654635, acc: 61.72%] [G loss: 1.878926]\n",
      "epoch:9 step:9199 [D loss: 0.658278, acc: 62.50%] [G loss: 1.951620]\n",
      "epoch:9 step:9200 [D loss: 0.643594, acc: 64.84%] [G loss: 1.933780]\n",
      "##############\n",
      "[2.50262154 1.43660636 6.52155697 4.9392905  3.69871584 5.77608599\n",
      " 4.62195868 4.55117585 4.88715051 3.43839142]\n",
      "##########\n",
      "epoch:9 step:9201 [D loss: 0.661148, acc: 61.72%] [G loss: 2.099388]\n",
      "epoch:9 step:9202 [D loss: 0.591142, acc: 65.62%] [G loss: 2.000868]\n",
      "epoch:9 step:9203 [D loss: 0.670761, acc: 60.94%] [G loss: 1.921931]\n",
      "epoch:9 step:9204 [D loss: 0.664207, acc: 60.94%] [G loss: 2.063960]\n",
      "epoch:9 step:9205 [D loss: 0.599895, acc: 70.31%] [G loss: 2.170919]\n",
      "epoch:9 step:9206 [D loss: 0.602428, acc: 67.97%] [G loss: 2.105692]\n",
      "epoch:9 step:9207 [D loss: 0.640131, acc: 60.94%] [G loss: 2.176153]\n",
      "epoch:9 step:9208 [D loss: 0.537963, acc: 76.56%] [G loss: 2.163036]\n",
      "epoch:9 step:9209 [D loss: 0.618666, acc: 67.19%] [G loss: 2.185713]\n",
      "epoch:9 step:9210 [D loss: 0.705304, acc: 61.72%] [G loss: 2.011050]\n",
      "epoch:9 step:9211 [D loss: 0.622450, acc: 68.75%] [G loss: 2.079581]\n",
      "epoch:9 step:9212 [D loss: 0.648834, acc: 61.72%] [G loss: 2.058199]\n",
      "epoch:9 step:9213 [D loss: 0.677999, acc: 65.62%] [G loss: 1.948924]\n",
      "epoch:9 step:9214 [D loss: 0.635532, acc: 62.50%] [G loss: 2.195568]\n",
      "epoch:9 step:9215 [D loss: 0.629324, acc: 66.41%] [G loss: 2.179922]\n",
      "epoch:9 step:9216 [D loss: 0.605127, acc: 68.75%] [G loss: 2.185330]\n",
      "epoch:9 step:9217 [D loss: 0.698281, acc: 57.81%] [G loss: 1.893865]\n",
      "epoch:9 step:9218 [D loss: 0.669824, acc: 52.34%] [G loss: 1.985408]\n",
      "epoch:9 step:9219 [D loss: 0.624116, acc: 64.84%] [G loss: 2.106650]\n",
      "epoch:9 step:9220 [D loss: 0.703409, acc: 59.38%] [G loss: 1.991313]\n",
      "epoch:9 step:9221 [D loss: 0.634879, acc: 66.41%] [G loss: 1.944506]\n",
      "epoch:9 step:9222 [D loss: 0.642843, acc: 64.06%] [G loss: 2.057667]\n",
      "epoch:9 step:9223 [D loss: 0.589522, acc: 67.97%] [G loss: 2.089727]\n",
      "epoch:9 step:9224 [D loss: 0.648168, acc: 64.06%] [G loss: 1.934931]\n",
      "epoch:9 step:9225 [D loss: 0.580825, acc: 72.66%] [G loss: 2.151129]\n",
      "epoch:9 step:9226 [D loss: 0.625209, acc: 61.72%] [G loss: 1.879264]\n",
      "epoch:9 step:9227 [D loss: 0.705216, acc: 55.47%] [G loss: 1.847028]\n",
      "epoch:9 step:9228 [D loss: 0.714265, acc: 52.34%] [G loss: 2.013361]\n",
      "epoch:9 step:9229 [D loss: 0.669222, acc: 56.25%] [G loss: 1.991368]\n",
      "epoch:9 step:9230 [D loss: 0.672877, acc: 61.72%] [G loss: 1.902756]\n",
      "epoch:9 step:9231 [D loss: 0.571928, acc: 72.66%] [G loss: 2.059265]\n",
      "epoch:9 step:9232 [D loss: 0.655857, acc: 60.16%] [G loss: 1.755246]\n",
      "epoch:9 step:9233 [D loss: 0.680600, acc: 56.25%] [G loss: 1.789127]\n",
      "epoch:9 step:9234 [D loss: 0.652591, acc: 59.38%] [G loss: 1.856089]\n",
      "epoch:9 step:9235 [D loss: 0.612025, acc: 65.62%] [G loss: 1.852178]\n",
      "epoch:9 step:9236 [D loss: 0.646381, acc: 64.06%] [G loss: 2.035851]\n",
      "epoch:9 step:9237 [D loss: 0.625123, acc: 64.06%] [G loss: 2.117762]\n",
      "epoch:9 step:9238 [D loss: 0.628669, acc: 63.28%] [G loss: 2.013351]\n",
      "epoch:9 step:9239 [D loss: 0.598583, acc: 67.97%] [G loss: 2.251648]\n",
      "epoch:9 step:9240 [D loss: 0.648537, acc: 62.50%] [G loss: 2.067464]\n",
      "epoch:9 step:9241 [D loss: 0.593778, acc: 64.06%] [G loss: 2.273770]\n",
      "epoch:9 step:9242 [D loss: 0.636092, acc: 60.94%] [G loss: 2.067386]\n",
      "epoch:9 step:9243 [D loss: 0.574675, acc: 69.53%] [G loss: 2.066307]\n",
      "epoch:9 step:9244 [D loss: 0.627688, acc: 64.06%] [G loss: 1.996686]\n",
      "epoch:9 step:9245 [D loss: 0.631398, acc: 62.50%] [G loss: 1.912629]\n",
      "epoch:9 step:9246 [D loss: 0.558421, acc: 71.88%] [G loss: 2.141815]\n",
      "epoch:9 step:9247 [D loss: 0.626564, acc: 66.41%] [G loss: 2.072430]\n",
      "epoch:9 step:9248 [D loss: 0.592172, acc: 74.22%] [G loss: 2.405997]\n",
      "epoch:9 step:9249 [D loss: 0.607165, acc: 70.31%] [G loss: 2.143029]\n",
      "epoch:9 step:9250 [D loss: 0.622411, acc: 68.75%] [G loss: 2.052340]\n",
      "epoch:9 step:9251 [D loss: 0.679162, acc: 56.25%] [G loss: 2.014268]\n",
      "epoch:9 step:9252 [D loss: 0.658530, acc: 60.16%] [G loss: 2.018272]\n",
      "epoch:9 step:9253 [D loss: 0.664768, acc: 59.38%] [G loss: 1.902503]\n",
      "epoch:9 step:9254 [D loss: 0.647888, acc: 61.72%] [G loss: 2.059204]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:9 step:9255 [D loss: 0.614354, acc: 68.75%] [G loss: 2.170072]\n",
      "epoch:9 step:9256 [D loss: 0.592189, acc: 71.88%] [G loss: 2.316797]\n",
      "epoch:9 step:9257 [D loss: 0.624781, acc: 66.41%] [G loss: 2.194863]\n",
      "epoch:9 step:9258 [D loss: 0.608929, acc: 67.97%] [G loss: 2.187648]\n",
      "epoch:9 step:9259 [D loss: 0.608140, acc: 62.50%] [G loss: 1.976524]\n",
      "epoch:9 step:9260 [D loss: 0.627537, acc: 64.84%] [G loss: 1.972570]\n",
      "epoch:9 step:9261 [D loss: 0.704646, acc: 53.91%] [G loss: 1.918225]\n",
      "epoch:9 step:9262 [D loss: 0.603539, acc: 64.06%] [G loss: 1.898886]\n",
      "epoch:9 step:9263 [D loss: 0.628893, acc: 65.62%] [G loss: 2.039666]\n",
      "epoch:9 step:9264 [D loss: 0.584313, acc: 72.66%] [G loss: 2.205429]\n",
      "epoch:9 step:9265 [D loss: 0.664178, acc: 60.94%] [G loss: 2.038896]\n",
      "epoch:9 step:9266 [D loss: 0.660641, acc: 65.62%] [G loss: 1.945194]\n",
      "epoch:9 step:9267 [D loss: 0.621983, acc: 64.06%] [G loss: 2.124049]\n",
      "epoch:9 step:9268 [D loss: 0.674978, acc: 57.03%] [G loss: 2.008230]\n",
      "epoch:9 step:9269 [D loss: 0.684487, acc: 57.81%] [G loss: 1.903408]\n",
      "epoch:9 step:9270 [D loss: 0.586779, acc: 67.19%] [G loss: 2.065516]\n",
      "epoch:9 step:9271 [D loss: 0.675832, acc: 55.47%] [G loss: 2.000559]\n",
      "epoch:9 step:9272 [D loss: 0.623200, acc: 60.16%] [G loss: 1.949882]\n",
      "epoch:9 step:9273 [D loss: 0.568657, acc: 70.31%] [G loss: 2.134597]\n",
      "epoch:9 step:9274 [D loss: 0.609659, acc: 67.97%] [G loss: 2.119414]\n",
      "epoch:9 step:9275 [D loss: 0.573581, acc: 66.41%] [G loss: 2.183399]\n",
      "epoch:9 step:9276 [D loss: 0.604866, acc: 66.41%] [G loss: 2.154735]\n",
      "epoch:9 step:9277 [D loss: 0.602592, acc: 63.28%] [G loss: 1.987694]\n",
      "epoch:9 step:9278 [D loss: 0.583736, acc: 67.97%] [G loss: 1.977604]\n",
      "epoch:9 step:9279 [D loss: 0.611700, acc: 70.31%] [G loss: 2.090770]\n",
      "epoch:9 step:9280 [D loss: 0.620030, acc: 63.28%] [G loss: 2.158681]\n",
      "epoch:9 step:9281 [D loss: 0.605241, acc: 71.88%] [G loss: 2.323198]\n",
      "epoch:9 step:9282 [D loss: 0.584305, acc: 71.09%] [G loss: 2.299654]\n",
      "epoch:9 step:9283 [D loss: 0.686234, acc: 63.28%] [G loss: 2.102284]\n",
      "epoch:9 step:9284 [D loss: 0.633074, acc: 60.16%] [G loss: 2.184630]\n",
      "epoch:9 step:9285 [D loss: 0.627970, acc: 60.94%] [G loss: 2.163903]\n",
      "epoch:9 step:9286 [D loss: 0.633356, acc: 67.19%] [G loss: 2.070451]\n",
      "epoch:9 step:9287 [D loss: 0.655415, acc: 58.59%] [G loss: 2.060982]\n",
      "epoch:9 step:9288 [D loss: 0.651460, acc: 59.38%] [G loss: 1.826023]\n",
      "epoch:9 step:9289 [D loss: 0.667875, acc: 60.16%] [G loss: 2.078760]\n",
      "epoch:9 step:9290 [D loss: 0.650612, acc: 61.72%] [G loss: 2.093153]\n",
      "epoch:9 step:9291 [D loss: 0.707523, acc: 61.72%] [G loss: 2.056084]\n",
      "epoch:9 step:9292 [D loss: 0.684401, acc: 63.28%] [G loss: 2.010189]\n",
      "epoch:9 step:9293 [D loss: 0.630393, acc: 60.94%] [G loss: 2.140266]\n",
      "epoch:9 step:9294 [D loss: 0.695972, acc: 60.94%] [G loss: 1.976380]\n",
      "epoch:9 step:9295 [D loss: 0.664098, acc: 59.38%] [G loss: 1.858413]\n",
      "epoch:9 step:9296 [D loss: 0.623229, acc: 65.62%] [G loss: 1.939584]\n",
      "epoch:9 step:9297 [D loss: 0.599519, acc: 64.84%] [G loss: 2.091455]\n",
      "epoch:9 step:9298 [D loss: 0.618537, acc: 66.41%] [G loss: 1.851801]\n",
      "epoch:9 step:9299 [D loss: 0.657128, acc: 67.97%] [G loss: 2.029850]\n",
      "epoch:9 step:9300 [D loss: 0.619314, acc: 63.28%] [G loss: 1.965131]\n",
      "epoch:9 step:9301 [D loss: 0.620332, acc: 66.41%] [G loss: 2.064329]\n",
      "epoch:9 step:9302 [D loss: 0.647941, acc: 60.94%] [G loss: 1.869098]\n",
      "epoch:9 step:9303 [D loss: 0.659582, acc: 57.81%] [G loss: 2.031417]\n",
      "epoch:9 step:9304 [D loss: 0.640967, acc: 65.62%] [G loss: 1.969206]\n",
      "epoch:9 step:9305 [D loss: 0.608763, acc: 66.41%] [G loss: 2.059234]\n",
      "epoch:9 step:9306 [D loss: 0.662142, acc: 61.72%] [G loss: 1.946962]\n",
      "epoch:9 step:9307 [D loss: 0.627765, acc: 67.97%] [G loss: 1.934086]\n",
      "epoch:9 step:9308 [D loss: 0.620205, acc: 69.53%] [G loss: 2.202616]\n",
      "epoch:9 step:9309 [D loss: 0.614608, acc: 67.19%] [G loss: 1.937689]\n",
      "epoch:9 step:9310 [D loss: 0.595098, acc: 67.19%] [G loss: 1.925221]\n",
      "epoch:9 step:9311 [D loss: 0.680260, acc: 57.03%] [G loss: 2.081170]\n",
      "epoch:9 step:9312 [D loss: 0.597919, acc: 67.97%] [G loss: 1.955593]\n",
      "epoch:9 step:9313 [D loss: 0.670015, acc: 59.38%] [G loss: 1.941151]\n",
      "epoch:9 step:9314 [D loss: 0.656148, acc: 63.28%] [G loss: 1.998368]\n",
      "epoch:9 step:9315 [D loss: 0.618362, acc: 64.84%] [G loss: 1.978865]\n",
      "epoch:9 step:9316 [D loss: 0.664638, acc: 63.28%] [G loss: 2.068504]\n",
      "epoch:9 step:9317 [D loss: 0.588452, acc: 66.41%] [G loss: 2.464370]\n",
      "epoch:9 step:9318 [D loss: 0.571899, acc: 71.88%] [G loss: 2.283553]\n",
      "epoch:9 step:9319 [D loss: 0.707253, acc: 65.62%] [G loss: 2.412793]\n",
      "epoch:9 step:9320 [D loss: 0.669120, acc: 60.94%] [G loss: 2.268562]\n",
      "epoch:9 step:9321 [D loss: 0.599569, acc: 70.31%] [G loss: 2.137593]\n",
      "epoch:9 step:9322 [D loss: 0.622737, acc: 64.84%] [G loss: 2.056512]\n",
      "epoch:9 step:9323 [D loss: 0.636588, acc: 60.16%] [G loss: 2.229209]\n",
      "epoch:9 step:9324 [D loss: 0.677566, acc: 60.94%] [G loss: 1.866104]\n",
      "epoch:9 step:9325 [D loss: 0.645729, acc: 67.19%] [G loss: 2.096313]\n",
      "epoch:9 step:9326 [D loss: 0.608513, acc: 71.09%] [G loss: 2.197208]\n",
      "epoch:9 step:9327 [D loss: 0.577145, acc: 69.53%] [G loss: 2.324912]\n",
      "epoch:9 step:9328 [D loss: 0.620210, acc: 71.88%] [G loss: 2.076195]\n",
      "epoch:9 step:9329 [D loss: 0.660146, acc: 58.59%] [G loss: 1.857171]\n",
      "epoch:9 step:9330 [D loss: 0.608085, acc: 67.97%] [G loss: 2.181968]\n",
      "epoch:9 step:9331 [D loss: 0.622335, acc: 67.19%] [G loss: 2.189154]\n",
      "epoch:9 step:9332 [D loss: 0.621444, acc: 72.66%] [G loss: 2.329868]\n",
      "epoch:9 step:9333 [D loss: 0.556548, acc: 71.88%] [G loss: 2.352459]\n",
      "epoch:9 step:9334 [D loss: 0.672650, acc: 60.94%] [G loss: 2.126516]\n",
      "epoch:9 step:9335 [D loss: 0.669836, acc: 64.06%] [G loss: 1.777484]\n",
      "epoch:9 step:9336 [D loss: 0.635476, acc: 70.31%] [G loss: 2.061506]\n",
      "epoch:9 step:9337 [D loss: 0.609538, acc: 66.41%] [G loss: 2.037668]\n",
      "epoch:9 step:9338 [D loss: 0.634109, acc: 62.50%] [G loss: 2.291337]\n",
      "epoch:9 step:9339 [D loss: 0.618940, acc: 67.97%] [G loss: 2.407689]\n",
      "epoch:9 step:9340 [D loss: 0.595578, acc: 65.62%] [G loss: 2.086226]\n",
      "epoch:9 step:9341 [D loss: 0.591172, acc: 70.31%] [G loss: 2.179780]\n",
      "epoch:9 step:9342 [D loss: 0.582090, acc: 66.41%] [G loss: 2.193559]\n",
      "epoch:9 step:9343 [D loss: 0.618173, acc: 65.62%] [G loss: 2.276026]\n",
      "epoch:9 step:9344 [D loss: 0.659093, acc: 64.84%] [G loss: 2.312510]\n",
      "epoch:9 step:9345 [D loss: 0.582867, acc: 74.22%] [G loss: 2.505560]\n",
      "epoch:9 step:9346 [D loss: 0.682020, acc: 60.16%] [G loss: 2.228790]\n",
      "epoch:9 step:9347 [D loss: 0.638198, acc: 63.28%] [G loss: 2.028933]\n",
      "epoch:9 step:9348 [D loss: 0.646626, acc: 63.28%] [G loss: 2.044652]\n",
      "epoch:9 step:9349 [D loss: 0.608317, acc: 69.53%] [G loss: 2.258024]\n",
      "epoch:9 step:9350 [D loss: 0.612052, acc: 65.62%] [G loss: 2.244411]\n",
      "epoch:9 step:9351 [D loss: 0.655164, acc: 63.28%] [G loss: 2.130042]\n",
      "epoch:9 step:9352 [D loss: 0.584898, acc: 64.84%] [G loss: 2.429137]\n",
      "epoch:9 step:9353 [D loss: 0.779537, acc: 52.34%] [G loss: 2.070071]\n",
      "epoch:9 step:9354 [D loss: 0.643497, acc: 64.84%] [G loss: 2.186910]\n",
      "epoch:9 step:9355 [D loss: 0.585707, acc: 66.41%] [G loss: 1.910828]\n",
      "epoch:9 step:9356 [D loss: 0.568573, acc: 73.44%] [G loss: 2.123804]\n",
      "epoch:9 step:9357 [D loss: 0.691909, acc: 60.16%] [G loss: 2.356006]\n",
      "epoch:9 step:9358 [D loss: 0.575139, acc: 70.31%] [G loss: 2.521036]\n",
      "epoch:9 step:9359 [D loss: 0.565519, acc: 67.97%] [G loss: 2.402224]\n",
      "epoch:9 step:9360 [D loss: 0.683541, acc: 57.03%] [G loss: 2.362415]\n",
      "epoch:9 step:9361 [D loss: 0.847665, acc: 49.22%] [G loss: 2.043255]\n",
      "epoch:9 step:9362 [D loss: 0.677769, acc: 58.59%] [G loss: 2.130937]\n",
      "epoch:9 step:9363 [D loss: 0.628050, acc: 68.75%] [G loss: 2.220306]\n",
      "epoch:9 step:9364 [D loss: 0.619617, acc: 61.72%] [G loss: 2.078928]\n",
      "epoch:9 step:9365 [D loss: 0.620037, acc: 68.75%] [G loss: 2.003627]\n",
      "epoch:9 step:9366 [D loss: 0.575011, acc: 71.88%] [G loss: 2.158914]\n",
      "epoch:9 step:9367 [D loss: 0.579892, acc: 71.09%] [G loss: 2.220967]\n",
      "epoch:9 step:9368 [D loss: 0.619761, acc: 70.31%] [G loss: 2.088563]\n",
      "epoch:9 step:9369 [D loss: 0.562141, acc: 69.53%] [G loss: 2.233627]\n",
      "epoch:9 step:9370 [D loss: 0.556517, acc: 72.66%] [G loss: 2.700641]\n",
      "epoch:10 step:9371 [D loss: 0.625255, acc: 68.75%] [G loss: 2.086416]\n",
      "epoch:10 step:9372 [D loss: 0.636834, acc: 61.72%] [G loss: 2.263086]\n",
      "epoch:10 step:9373 [D loss: 0.620772, acc: 67.19%] [G loss: 2.160537]\n",
      "epoch:10 step:9374 [D loss: 0.647065, acc: 64.06%] [G loss: 2.011540]\n",
      "epoch:10 step:9375 [D loss: 0.652917, acc: 62.50%] [G loss: 2.051341]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:10 step:9376 [D loss: 0.619725, acc: 64.84%] [G loss: 2.089653]\n",
      "epoch:10 step:9377 [D loss: 0.566219, acc: 74.22%] [G loss: 2.209844]\n",
      "epoch:10 step:9378 [D loss: 0.680361, acc: 61.72%] [G loss: 2.050166]\n",
      "epoch:10 step:9379 [D loss: 0.631854, acc: 64.84%] [G loss: 2.205900]\n",
      "epoch:10 step:9380 [D loss: 0.616870, acc: 63.28%] [G loss: 2.222367]\n",
      "epoch:10 step:9381 [D loss: 0.631849, acc: 64.84%] [G loss: 2.137781]\n",
      "epoch:10 step:9382 [D loss: 0.600688, acc: 69.53%] [G loss: 2.088207]\n",
      "epoch:10 step:9383 [D loss: 0.597857, acc: 70.31%] [G loss: 2.315943]\n",
      "epoch:10 step:9384 [D loss: 0.600776, acc: 68.75%] [G loss: 2.099743]\n",
      "epoch:10 step:9385 [D loss: 0.545232, acc: 75.00%] [G loss: 2.280872]\n",
      "epoch:10 step:9386 [D loss: 0.561613, acc: 69.53%] [G loss: 2.416505]\n",
      "epoch:10 step:9387 [D loss: 0.608029, acc: 67.97%] [G loss: 2.364290]\n",
      "epoch:10 step:9388 [D loss: 0.643322, acc: 64.84%] [G loss: 2.115880]\n",
      "epoch:10 step:9389 [D loss: 0.721321, acc: 60.16%] [G loss: 2.009052]\n",
      "epoch:10 step:9390 [D loss: 0.652807, acc: 58.59%] [G loss: 1.900206]\n",
      "epoch:10 step:9391 [D loss: 0.644465, acc: 64.06%] [G loss: 2.101173]\n",
      "epoch:10 step:9392 [D loss: 0.591215, acc: 67.97%] [G loss: 2.028903]\n",
      "epoch:10 step:9393 [D loss: 0.634853, acc: 60.16%] [G loss: 2.333947]\n",
      "epoch:10 step:9394 [D loss: 0.566664, acc: 69.53%] [G loss: 2.139542]\n",
      "epoch:10 step:9395 [D loss: 0.613997, acc: 67.19%] [G loss: 2.351527]\n",
      "epoch:10 step:9396 [D loss: 0.617148, acc: 62.50%] [G loss: 1.914168]\n",
      "epoch:10 step:9397 [D loss: 0.693879, acc: 58.59%] [G loss: 2.091745]\n",
      "epoch:10 step:9398 [D loss: 0.638105, acc: 64.06%] [G loss: 2.189153]\n",
      "epoch:10 step:9399 [D loss: 0.644901, acc: 64.84%] [G loss: 2.009381]\n",
      "epoch:10 step:9400 [D loss: 0.631572, acc: 57.81%] [G loss: 2.038564]\n",
      "##############\n",
      "[2.50904761 1.55486989 6.51393427 4.91693207 3.88905078 5.79146434\n",
      " 4.56586888 4.7846736  4.93950395 3.48059308]\n",
      "##########\n",
      "epoch:10 step:9401 [D loss: 0.640526, acc: 64.84%] [G loss: 1.944498]\n",
      "epoch:10 step:9402 [D loss: 0.628331, acc: 61.72%] [G loss: 2.059832]\n",
      "epoch:10 step:9403 [D loss: 0.623789, acc: 64.06%] [G loss: 2.102734]\n",
      "epoch:10 step:9404 [D loss: 0.588388, acc: 67.97%] [G loss: 2.249285]\n",
      "epoch:10 step:9405 [D loss: 0.689142, acc: 54.69%] [G loss: 1.933381]\n",
      "epoch:10 step:9406 [D loss: 0.658786, acc: 61.72%] [G loss: 2.089107]\n",
      "epoch:10 step:9407 [D loss: 0.578461, acc: 71.09%] [G loss: 2.027325]\n",
      "epoch:10 step:9408 [D loss: 0.671375, acc: 60.16%] [G loss: 2.153292]\n",
      "epoch:10 step:9409 [D loss: 0.590181, acc: 69.53%] [G loss: 2.396232]\n",
      "epoch:10 step:9410 [D loss: 0.565813, acc: 71.09%] [G loss: 2.368092]\n",
      "epoch:10 step:9411 [D loss: 0.689225, acc: 56.25%] [G loss: 1.946829]\n",
      "epoch:10 step:9412 [D loss: 0.597614, acc: 70.31%] [G loss: 2.201808]\n",
      "epoch:10 step:9413 [D loss: 0.560676, acc: 74.22%] [G loss: 2.133272]\n",
      "epoch:10 step:9414 [D loss: 0.617629, acc: 67.97%] [G loss: 2.182021]\n",
      "epoch:10 step:9415 [D loss: 0.607280, acc: 64.84%] [G loss: 2.265722]\n",
      "epoch:10 step:9416 [D loss: 0.640874, acc: 63.28%] [G loss: 2.046274]\n",
      "epoch:10 step:9417 [D loss: 0.648567, acc: 64.84%] [G loss: 2.121066]\n",
      "epoch:10 step:9418 [D loss: 0.637759, acc: 65.62%] [G loss: 2.212248]\n",
      "epoch:10 step:9419 [D loss: 0.590539, acc: 70.31%] [G loss: 2.200807]\n",
      "epoch:10 step:9420 [D loss: 0.594707, acc: 67.97%] [G loss: 2.198920]\n",
      "epoch:10 step:9421 [D loss: 0.608979, acc: 67.97%] [G loss: 2.089385]\n",
      "epoch:10 step:9422 [D loss: 0.678596, acc: 63.28%] [G loss: 2.104799]\n",
      "epoch:10 step:9423 [D loss: 0.552977, acc: 71.88%] [G loss: 2.248532]\n",
      "epoch:10 step:9424 [D loss: 0.578169, acc: 69.53%] [G loss: 2.217070]\n",
      "epoch:10 step:9425 [D loss: 0.546787, acc: 71.88%] [G loss: 2.496955]\n",
      "epoch:10 step:9426 [D loss: 0.633027, acc: 65.62%] [G loss: 2.351465]\n",
      "epoch:10 step:9427 [D loss: 0.651087, acc: 60.16%] [G loss: 2.142900]\n",
      "epoch:10 step:9428 [D loss: 0.666792, acc: 55.47%] [G loss: 2.177079]\n",
      "epoch:10 step:9429 [D loss: 0.622798, acc: 64.84%] [G loss: 2.164796]\n",
      "epoch:10 step:9430 [D loss: 0.609170, acc: 68.75%] [G loss: 2.079288]\n",
      "epoch:10 step:9431 [D loss: 0.613469, acc: 65.62%] [G loss: 2.131991]\n",
      "epoch:10 step:9432 [D loss: 0.654680, acc: 62.50%] [G loss: 2.058486]\n",
      "epoch:10 step:9433 [D loss: 0.625182, acc: 67.97%] [G loss: 2.111680]\n",
      "epoch:10 step:9434 [D loss: 0.673190, acc: 59.38%] [G loss: 2.093888]\n",
      "epoch:10 step:9435 [D loss: 0.691890, acc: 60.94%] [G loss: 1.913474]\n",
      "epoch:10 step:9436 [D loss: 0.671643, acc: 60.16%] [G loss: 1.908736]\n",
      "epoch:10 step:9437 [D loss: 0.620176, acc: 67.19%] [G loss: 2.026652]\n",
      "epoch:10 step:9438 [D loss: 0.606971, acc: 65.62%] [G loss: 2.181274]\n",
      "epoch:10 step:9439 [D loss: 0.624789, acc: 63.28%] [G loss: 2.264213]\n",
      "epoch:10 step:9440 [D loss: 0.669132, acc: 61.72%] [G loss: 2.068471]\n",
      "epoch:10 step:9441 [D loss: 0.629051, acc: 67.97%] [G loss: 2.026341]\n",
      "epoch:10 step:9442 [D loss: 0.581017, acc: 69.53%] [G loss: 2.090564]\n",
      "epoch:10 step:9443 [D loss: 0.655666, acc: 60.94%] [G loss: 1.746051]\n",
      "epoch:10 step:9444 [D loss: 0.623083, acc: 66.41%] [G loss: 2.290292]\n",
      "epoch:10 step:9445 [D loss: 0.581297, acc: 67.19%] [G loss: 2.242891]\n",
      "epoch:10 step:9446 [D loss: 0.535636, acc: 79.69%] [G loss: 2.400458]\n",
      "epoch:10 step:9447 [D loss: 0.678902, acc: 62.50%] [G loss: 2.515520]\n",
      "epoch:10 step:9448 [D loss: 0.652606, acc: 64.06%] [G loss: 1.980052]\n",
      "epoch:10 step:9449 [D loss: 0.649522, acc: 59.38%] [G loss: 2.002885]\n",
      "epoch:10 step:9450 [D loss: 0.676790, acc: 64.06%] [G loss: 1.939037]\n",
      "epoch:10 step:9451 [D loss: 0.681006, acc: 57.81%] [G loss: 1.791357]\n",
      "epoch:10 step:9452 [D loss: 0.606576, acc: 67.19%] [G loss: 2.111904]\n",
      "epoch:10 step:9453 [D loss: 0.610992, acc: 74.22%] [G loss: 2.219840]\n",
      "epoch:10 step:9454 [D loss: 0.584664, acc: 67.19%] [G loss: 2.086715]\n",
      "epoch:10 step:9455 [D loss: 0.664079, acc: 61.72%] [G loss: 1.919416]\n",
      "epoch:10 step:9456 [D loss: 0.669184, acc: 60.16%] [G loss: 1.999777]\n",
      "epoch:10 step:9457 [D loss: 0.597192, acc: 67.19%] [G loss: 2.087790]\n",
      "epoch:10 step:9458 [D loss: 0.653888, acc: 60.94%] [G loss: 1.882640]\n",
      "epoch:10 step:9459 [D loss: 0.624255, acc: 65.62%] [G loss: 1.869631]\n",
      "epoch:10 step:9460 [D loss: 0.623046, acc: 67.19%] [G loss: 2.133408]\n",
      "epoch:10 step:9461 [D loss: 0.662131, acc: 60.94%] [G loss: 1.971247]\n",
      "epoch:10 step:9462 [D loss: 0.639971, acc: 67.19%] [G loss: 2.010216]\n",
      "epoch:10 step:9463 [D loss: 0.601787, acc: 68.75%] [G loss: 2.372712]\n",
      "epoch:10 step:9464 [D loss: 0.615698, acc: 61.72%] [G loss: 2.100651]\n",
      "epoch:10 step:9465 [D loss: 0.670594, acc: 60.94%] [G loss: 1.959753]\n",
      "epoch:10 step:9466 [D loss: 0.598604, acc: 67.97%] [G loss: 2.036958]\n",
      "epoch:10 step:9467 [D loss: 0.618629, acc: 62.50%] [G loss: 2.023860]\n",
      "epoch:10 step:9468 [D loss: 0.666990, acc: 64.84%] [G loss: 1.950546]\n",
      "epoch:10 step:9469 [D loss: 0.680150, acc: 62.50%] [G loss: 1.816270]\n",
      "epoch:10 step:9470 [D loss: 0.591580, acc: 70.31%] [G loss: 2.058426]\n",
      "epoch:10 step:9471 [D loss: 0.558744, acc: 73.44%] [G loss: 2.047721]\n",
      "epoch:10 step:9472 [D loss: 0.709318, acc: 52.34%] [G loss: 2.175597]\n",
      "epoch:10 step:9473 [D loss: 0.602087, acc: 64.84%] [G loss: 2.044216]\n",
      "epoch:10 step:9474 [D loss: 0.638582, acc: 61.72%] [G loss: 1.980836]\n",
      "epoch:10 step:9475 [D loss: 0.638763, acc: 61.72%] [G loss: 2.082787]\n",
      "epoch:10 step:9476 [D loss: 0.617269, acc: 64.84%] [G loss: 2.260258]\n",
      "epoch:10 step:9477 [D loss: 0.585469, acc: 61.72%] [G loss: 2.311592]\n",
      "epoch:10 step:9478 [D loss: 0.671759, acc: 58.59%] [G loss: 1.839823]\n",
      "epoch:10 step:9479 [D loss: 0.694496, acc: 57.03%] [G loss: 1.957558]\n",
      "epoch:10 step:9480 [D loss: 0.623393, acc: 64.84%] [G loss: 2.035676]\n",
      "epoch:10 step:9481 [D loss: 0.567238, acc: 69.53%] [G loss: 2.258452]\n",
      "epoch:10 step:9482 [D loss: 0.626240, acc: 64.06%] [G loss: 1.965719]\n",
      "epoch:10 step:9483 [D loss: 0.633805, acc: 65.62%] [G loss: 2.102365]\n",
      "epoch:10 step:9484 [D loss: 0.643084, acc: 59.38%] [G loss: 2.124146]\n",
      "epoch:10 step:9485 [D loss: 0.622584, acc: 65.62%] [G loss: 2.249909]\n",
      "epoch:10 step:9486 [D loss: 0.588951, acc: 67.19%] [G loss: 2.163357]\n",
      "epoch:10 step:9487 [D loss: 0.697683, acc: 60.94%] [G loss: 2.197044]\n",
      "epoch:10 step:9488 [D loss: 0.638921, acc: 60.94%] [G loss: 2.040248]\n",
      "epoch:10 step:9489 [D loss: 0.528718, acc: 75.00%] [G loss: 2.583689]\n",
      "epoch:10 step:9490 [D loss: 0.661140, acc: 67.97%] [G loss: 2.192261]\n",
      "epoch:10 step:9491 [D loss: 0.681054, acc: 64.84%] [G loss: 2.053743]\n",
      "epoch:10 step:9492 [D loss: 0.611326, acc: 60.94%] [G loss: 2.194092]\n",
      "epoch:10 step:9493 [D loss: 0.642596, acc: 63.28%] [G loss: 2.089247]\n",
      "epoch:10 step:9494 [D loss: 0.662842, acc: 61.72%] [G loss: 2.017857]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:10 step:9495 [D loss: 0.722128, acc: 57.81%] [G loss: 1.879686]\n",
      "epoch:10 step:9496 [D loss: 0.629742, acc: 67.19%] [G loss: 1.954953]\n",
      "epoch:10 step:9497 [D loss: 0.588802, acc: 67.97%] [G loss: 1.909027]\n",
      "epoch:10 step:9498 [D loss: 0.646235, acc: 62.50%] [G loss: 1.981006]\n",
      "epoch:10 step:9499 [D loss: 0.619830, acc: 63.28%] [G loss: 1.969864]\n",
      "epoch:10 step:9500 [D loss: 0.607513, acc: 65.62%] [G loss: 2.094743]\n",
      "epoch:10 step:9501 [D loss: 0.643232, acc: 64.06%] [G loss: 2.086694]\n",
      "epoch:10 step:9502 [D loss: 0.719992, acc: 53.91%] [G loss: 1.855762]\n",
      "epoch:10 step:9503 [D loss: 0.711706, acc: 57.03%] [G loss: 1.956529]\n",
      "epoch:10 step:9504 [D loss: 0.625835, acc: 64.84%] [G loss: 1.983098]\n",
      "epoch:10 step:9505 [D loss: 0.674183, acc: 64.84%] [G loss: 1.968157]\n",
      "epoch:10 step:9506 [D loss: 0.665295, acc: 59.38%] [G loss: 1.999321]\n",
      "epoch:10 step:9507 [D loss: 0.644246, acc: 60.94%] [G loss: 1.941763]\n",
      "epoch:10 step:9508 [D loss: 0.650664, acc: 58.59%] [G loss: 1.931543]\n",
      "epoch:10 step:9509 [D loss: 0.615698, acc: 67.19%] [G loss: 2.011641]\n",
      "epoch:10 step:9510 [D loss: 0.630648, acc: 61.72%] [G loss: 1.883606]\n",
      "epoch:10 step:9511 [D loss: 0.661128, acc: 58.59%] [G loss: 1.971807]\n",
      "epoch:10 step:9512 [D loss: 0.697845, acc: 55.47%] [G loss: 2.036168]\n",
      "epoch:10 step:9513 [D loss: 0.654970, acc: 64.84%] [G loss: 1.940971]\n",
      "epoch:10 step:9514 [D loss: 0.645381, acc: 63.28%] [G loss: 2.129085]\n",
      "epoch:10 step:9515 [D loss: 0.622794, acc: 71.09%] [G loss: 2.000388]\n",
      "epoch:10 step:9516 [D loss: 0.609843, acc: 65.62%] [G loss: 1.992705]\n",
      "epoch:10 step:9517 [D loss: 0.647528, acc: 63.28%] [G loss: 1.975902]\n",
      "epoch:10 step:9518 [D loss: 0.617780, acc: 64.06%] [G loss: 1.968596]\n",
      "epoch:10 step:9519 [D loss: 0.615229, acc: 65.62%] [G loss: 2.154460]\n",
      "epoch:10 step:9520 [D loss: 0.638038, acc: 65.62%] [G loss: 2.187746]\n",
      "epoch:10 step:9521 [D loss: 0.558005, acc: 67.97%] [G loss: 2.413062]\n",
      "epoch:10 step:9522 [D loss: 0.662722, acc: 67.19%] [G loss: 2.113984]\n",
      "epoch:10 step:9523 [D loss: 0.601917, acc: 67.19%] [G loss: 1.882301]\n",
      "epoch:10 step:9524 [D loss: 0.647459, acc: 66.41%] [G loss: 2.002623]\n",
      "epoch:10 step:9525 [D loss: 0.644486, acc: 61.72%] [G loss: 2.114107]\n",
      "epoch:10 step:9526 [D loss: 0.603049, acc: 67.19%] [G loss: 2.114616]\n",
      "epoch:10 step:9527 [D loss: 0.644782, acc: 63.28%] [G loss: 2.105230]\n",
      "epoch:10 step:9528 [D loss: 0.610019, acc: 64.84%] [G loss: 1.986399]\n",
      "epoch:10 step:9529 [D loss: 0.694846, acc: 58.59%] [G loss: 2.088993]\n",
      "epoch:10 step:9530 [D loss: 0.671378, acc: 56.25%] [G loss: 1.872466]\n",
      "epoch:10 step:9531 [D loss: 0.623028, acc: 65.62%] [G loss: 2.127979]\n",
      "epoch:10 step:9532 [D loss: 0.613792, acc: 69.53%] [G loss: 1.967547]\n",
      "epoch:10 step:9533 [D loss: 0.651574, acc: 53.12%] [G loss: 1.890302]\n",
      "epoch:10 step:9534 [D loss: 0.601044, acc: 67.97%] [G loss: 1.920503]\n",
      "epoch:10 step:9535 [D loss: 0.710525, acc: 53.12%] [G loss: 1.848371]\n",
      "epoch:10 step:9536 [D loss: 0.665195, acc: 57.81%] [G loss: 1.937602]\n",
      "epoch:10 step:9537 [D loss: 0.632100, acc: 66.41%] [G loss: 2.116883]\n",
      "epoch:10 step:9538 [D loss: 0.555570, acc: 71.88%] [G loss: 2.037645]\n",
      "epoch:10 step:9539 [D loss: 0.719432, acc: 56.25%] [G loss: 1.989784]\n",
      "epoch:10 step:9540 [D loss: 0.586821, acc: 69.53%] [G loss: 2.028237]\n",
      "epoch:10 step:9541 [D loss: 0.587091, acc: 67.97%] [G loss: 2.041250]\n",
      "epoch:10 step:9542 [D loss: 0.601836, acc: 71.09%] [G loss: 2.182161]\n",
      "epoch:10 step:9543 [D loss: 0.662555, acc: 57.81%] [G loss: 1.913134]\n",
      "epoch:10 step:9544 [D loss: 0.668783, acc: 57.81%] [G loss: 2.006497]\n",
      "epoch:10 step:9545 [D loss: 0.627942, acc: 61.72%] [G loss: 1.960735]\n",
      "epoch:10 step:9546 [D loss: 0.640711, acc: 59.38%] [G loss: 1.981078]\n",
      "epoch:10 step:9547 [D loss: 0.638237, acc: 60.16%] [G loss: 1.876038]\n",
      "epoch:10 step:9548 [D loss: 0.604340, acc: 67.19%] [G loss: 1.959304]\n",
      "epoch:10 step:9549 [D loss: 0.582660, acc: 69.53%] [G loss: 1.888135]\n",
      "epoch:10 step:9550 [D loss: 0.683910, acc: 60.94%] [G loss: 2.115713]\n",
      "epoch:10 step:9551 [D loss: 0.582151, acc: 72.66%] [G loss: 2.185257]\n",
      "epoch:10 step:9552 [D loss: 0.659698, acc: 57.03%] [G loss: 1.902297]\n",
      "epoch:10 step:9553 [D loss: 0.679336, acc: 60.94%] [G loss: 1.933373]\n",
      "epoch:10 step:9554 [D loss: 0.609356, acc: 67.19%] [G loss: 1.978064]\n",
      "epoch:10 step:9555 [D loss: 0.623454, acc: 64.84%] [G loss: 2.197383]\n",
      "epoch:10 step:9556 [D loss: 0.609946, acc: 65.62%] [G loss: 2.088495]\n",
      "epoch:10 step:9557 [D loss: 0.615759, acc: 71.09%] [G loss: 2.463888]\n",
      "epoch:10 step:9558 [D loss: 0.645059, acc: 65.62%] [G loss: 2.067975]\n",
      "epoch:10 step:9559 [D loss: 0.628722, acc: 68.75%] [G loss: 1.978861]\n",
      "epoch:10 step:9560 [D loss: 0.625680, acc: 61.72%] [G loss: 2.183509]\n",
      "epoch:10 step:9561 [D loss: 0.695364, acc: 57.81%] [G loss: 1.990016]\n",
      "epoch:10 step:9562 [D loss: 0.613786, acc: 63.28%] [G loss: 2.107264]\n",
      "epoch:10 step:9563 [D loss: 0.652146, acc: 63.28%] [G loss: 2.224362]\n",
      "epoch:10 step:9564 [D loss: 0.641798, acc: 64.84%] [G loss: 2.097503]\n",
      "epoch:10 step:9565 [D loss: 0.657671, acc: 61.72%] [G loss: 1.985046]\n",
      "epoch:10 step:9566 [D loss: 0.665353, acc: 57.03%] [G loss: 2.033637]\n",
      "epoch:10 step:9567 [D loss: 0.604917, acc: 72.66%] [G loss: 2.198803]\n",
      "epoch:10 step:9568 [D loss: 0.596116, acc: 64.06%] [G loss: 2.277993]\n",
      "epoch:10 step:9569 [D loss: 0.615365, acc: 70.31%] [G loss: 2.017574]\n",
      "epoch:10 step:9570 [D loss: 0.703753, acc: 57.03%] [G loss: 1.895351]\n",
      "epoch:10 step:9571 [D loss: 0.636214, acc: 65.62%] [G loss: 2.227907]\n",
      "epoch:10 step:9572 [D loss: 0.649679, acc: 58.59%] [G loss: 1.864797]\n",
      "epoch:10 step:9573 [D loss: 0.701506, acc: 59.38%] [G loss: 1.958947]\n",
      "epoch:10 step:9574 [D loss: 0.642714, acc: 60.94%] [G loss: 1.921234]\n",
      "epoch:10 step:9575 [D loss: 0.609637, acc: 67.97%] [G loss: 2.059777]\n",
      "epoch:10 step:9576 [D loss: 0.586364, acc: 66.41%] [G loss: 2.315159]\n",
      "epoch:10 step:9577 [D loss: 0.578506, acc: 65.62%] [G loss: 2.352680]\n",
      "epoch:10 step:9578 [D loss: 0.548476, acc: 70.31%] [G loss: 2.488895]\n",
      "epoch:10 step:9579 [D loss: 0.573418, acc: 65.62%] [G loss: 2.341473]\n",
      "epoch:10 step:9580 [D loss: 0.657849, acc: 60.16%] [G loss: 2.083480]\n",
      "epoch:10 step:9581 [D loss: 0.648802, acc: 61.72%] [G loss: 1.867910]\n",
      "epoch:10 step:9582 [D loss: 0.667188, acc: 60.94%] [G loss: 1.951317]\n",
      "epoch:10 step:9583 [D loss: 0.643119, acc: 60.94%] [G loss: 1.864872]\n",
      "epoch:10 step:9584 [D loss: 0.659939, acc: 59.38%] [G loss: 1.913144]\n",
      "epoch:10 step:9585 [D loss: 0.698563, acc: 57.03%] [G loss: 1.854437]\n",
      "epoch:10 step:9586 [D loss: 0.657086, acc: 64.06%] [G loss: 1.929074]\n",
      "epoch:10 step:9587 [D loss: 0.608218, acc: 72.66%] [G loss: 2.159628]\n",
      "epoch:10 step:9588 [D loss: 0.603814, acc: 65.62%] [G loss: 2.384142]\n",
      "epoch:10 step:9589 [D loss: 0.518818, acc: 77.34%] [G loss: 2.405041]\n",
      "epoch:10 step:9590 [D loss: 0.717782, acc: 55.47%] [G loss: 1.814534]\n",
      "epoch:10 step:9591 [D loss: 0.674837, acc: 60.94%] [G loss: 2.018688]\n",
      "epoch:10 step:9592 [D loss: 0.589286, acc: 68.75%] [G loss: 2.270008]\n",
      "epoch:10 step:9593 [D loss: 0.576265, acc: 75.78%] [G loss: 2.103515]\n",
      "epoch:10 step:9594 [D loss: 0.703511, acc: 58.59%] [G loss: 1.951450]\n",
      "epoch:10 step:9595 [D loss: 0.690315, acc: 53.91%] [G loss: 2.002916]\n",
      "epoch:10 step:9596 [D loss: 0.665972, acc: 65.62%] [G loss: 2.034364]\n",
      "epoch:10 step:9597 [D loss: 0.617915, acc: 64.84%] [G loss: 1.989165]\n",
      "epoch:10 step:9598 [D loss: 0.689662, acc: 60.16%] [G loss: 2.109936]\n",
      "epoch:10 step:9599 [D loss: 0.606839, acc: 67.97%] [G loss: 2.004295]\n",
      "epoch:10 step:9600 [D loss: 0.615214, acc: 67.19%] [G loss: 2.288793]\n",
      "##############\n",
      "[2.41382028 1.50975588 6.53300533 4.68602487 3.97942403 5.68905648\n",
      " 4.45966673 4.777381   4.79725378 3.65314725]\n",
      "##########\n",
      "epoch:10 step:9601 [D loss: 0.568846, acc: 71.09%] [G loss: 2.515075]\n",
      "epoch:10 step:9602 [D loss: 0.603123, acc: 68.75%] [G loss: 2.165159]\n",
      "epoch:10 step:9603 [D loss: 0.666542, acc: 65.62%] [G loss: 1.871198]\n",
      "epoch:10 step:9604 [D loss: 0.655727, acc: 63.28%] [G loss: 2.117623]\n",
      "epoch:10 step:9605 [D loss: 0.604856, acc: 63.28%] [G loss: 2.047187]\n",
      "epoch:10 step:9606 [D loss: 0.622328, acc: 64.84%] [G loss: 2.000963]\n",
      "epoch:10 step:9607 [D loss: 0.607149, acc: 65.62%] [G loss: 2.100310]\n",
      "epoch:10 step:9608 [D loss: 0.636895, acc: 63.28%] [G loss: 1.970938]\n",
      "epoch:10 step:9609 [D loss: 0.552525, acc: 81.25%] [G loss: 1.988069]\n",
      "epoch:10 step:9610 [D loss: 0.646172, acc: 64.06%] [G loss: 2.088276]\n",
      "epoch:10 step:9611 [D loss: 0.629024, acc: 66.41%] [G loss: 2.104655]\n",
      "epoch:10 step:9612 [D loss: 0.643398, acc: 60.16%] [G loss: 2.077284]\n",
      "epoch:10 step:9613 [D loss: 0.622088, acc: 64.06%] [G loss: 2.226789]\n",
      "epoch:10 step:9614 [D loss: 0.600800, acc: 69.53%] [G loss: 2.147416]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:10 step:9615 [D loss: 0.602786, acc: 67.97%] [G loss: 2.253999]\n",
      "epoch:10 step:9616 [D loss: 0.639625, acc: 63.28%] [G loss: 2.194645]\n",
      "epoch:10 step:9617 [D loss: 0.606262, acc: 71.88%] [G loss: 2.169171]\n",
      "epoch:10 step:9618 [D loss: 0.603715, acc: 67.19%] [G loss: 2.201458]\n",
      "epoch:10 step:9619 [D loss: 0.632057, acc: 64.06%] [G loss: 2.082174]\n",
      "epoch:10 step:9620 [D loss: 0.667922, acc: 60.94%] [G loss: 1.872150]\n",
      "epoch:10 step:9621 [D loss: 0.636689, acc: 66.41%] [G loss: 1.938630]\n",
      "epoch:10 step:9622 [D loss: 0.637718, acc: 64.06%] [G loss: 1.905760]\n",
      "epoch:10 step:9623 [D loss: 0.639456, acc: 62.50%] [G loss: 1.987621]\n",
      "epoch:10 step:9624 [D loss: 0.695945, acc: 57.03%] [G loss: 2.065253]\n",
      "epoch:10 step:9625 [D loss: 0.646402, acc: 59.38%] [G loss: 2.013438]\n",
      "epoch:10 step:9626 [D loss: 0.635627, acc: 64.84%] [G loss: 1.991260]\n",
      "epoch:10 step:9627 [D loss: 0.638381, acc: 61.72%] [G loss: 1.939159]\n",
      "epoch:10 step:9628 [D loss: 0.607002, acc: 64.06%] [G loss: 1.909963]\n",
      "epoch:10 step:9629 [D loss: 0.605710, acc: 64.84%] [G loss: 2.080857]\n",
      "epoch:10 step:9630 [D loss: 0.612006, acc: 74.22%] [G loss: 2.227434]\n",
      "epoch:10 step:9631 [D loss: 0.573892, acc: 69.53%] [G loss: 2.111615]\n",
      "epoch:10 step:9632 [D loss: 0.611164, acc: 65.62%] [G loss: 2.177593]\n",
      "epoch:10 step:9633 [D loss: 0.689953, acc: 57.81%] [G loss: 1.967893]\n",
      "epoch:10 step:9634 [D loss: 0.625994, acc: 65.62%] [G loss: 2.207853]\n",
      "epoch:10 step:9635 [D loss: 0.682950, acc: 61.72%] [G loss: 1.862035]\n",
      "epoch:10 step:9636 [D loss: 0.649371, acc: 61.72%] [G loss: 2.035198]\n",
      "epoch:10 step:9637 [D loss: 0.633741, acc: 64.06%] [G loss: 2.003043]\n",
      "epoch:10 step:9638 [D loss: 0.634468, acc: 64.06%] [G loss: 2.029084]\n",
      "epoch:10 step:9639 [D loss: 0.593896, acc: 71.09%] [G loss: 2.082269]\n",
      "epoch:10 step:9640 [D loss: 0.676050, acc: 53.91%] [G loss: 2.123610]\n",
      "epoch:10 step:9641 [D loss: 0.618958, acc: 64.06%] [G loss: 2.140505]\n",
      "epoch:10 step:9642 [D loss: 0.610458, acc: 65.62%] [G loss: 2.256124]\n",
      "epoch:10 step:9643 [D loss: 0.623259, acc: 60.94%] [G loss: 2.021242]\n",
      "epoch:10 step:9644 [D loss: 0.600688, acc: 68.75%] [G loss: 2.105809]\n",
      "epoch:10 step:9645 [D loss: 0.637765, acc: 67.19%] [G loss: 1.994578]\n",
      "epoch:10 step:9646 [D loss: 0.634785, acc: 64.84%] [G loss: 2.077403]\n",
      "epoch:10 step:9647 [D loss: 0.668862, acc: 60.16%] [G loss: 1.887111]\n",
      "epoch:10 step:9648 [D loss: 0.660276, acc: 62.50%] [G loss: 1.888354]\n",
      "epoch:10 step:9649 [D loss: 0.620656, acc: 67.97%] [G loss: 2.009903]\n",
      "epoch:10 step:9650 [D loss: 0.588408, acc: 68.75%] [G loss: 1.987391]\n",
      "epoch:10 step:9651 [D loss: 0.687491, acc: 60.16%] [G loss: 1.951886]\n",
      "epoch:10 step:9652 [D loss: 0.642791, acc: 62.50%] [G loss: 2.007224]\n",
      "epoch:10 step:9653 [D loss: 0.575462, acc: 71.88%] [G loss: 1.904964]\n",
      "epoch:10 step:9654 [D loss: 0.648301, acc: 62.50%] [G loss: 2.064311]\n",
      "epoch:10 step:9655 [D loss: 0.644833, acc: 62.50%] [G loss: 2.056135]\n",
      "epoch:10 step:9656 [D loss: 0.583589, acc: 67.19%] [G loss: 2.224103]\n",
      "epoch:10 step:9657 [D loss: 0.647762, acc: 63.28%] [G loss: 2.030080]\n",
      "epoch:10 step:9658 [D loss: 0.657837, acc: 64.84%] [G loss: 1.966394]\n",
      "epoch:10 step:9659 [D loss: 0.636592, acc: 67.19%] [G loss: 2.096094]\n",
      "epoch:10 step:9660 [D loss: 0.719305, acc: 50.00%] [G loss: 1.994438]\n",
      "epoch:10 step:9661 [D loss: 0.632412, acc: 64.84%] [G loss: 2.176739]\n",
      "epoch:10 step:9662 [D loss: 0.655498, acc: 60.94%] [G loss: 1.874297]\n",
      "epoch:10 step:9663 [D loss: 0.569151, acc: 69.53%] [G loss: 2.055245]\n",
      "epoch:10 step:9664 [D loss: 0.678515, acc: 56.25%] [G loss: 2.003137]\n",
      "epoch:10 step:9665 [D loss: 0.622018, acc: 64.06%] [G loss: 2.057118]\n",
      "epoch:10 step:9666 [D loss: 0.634797, acc: 67.19%] [G loss: 2.165491]\n",
      "epoch:10 step:9667 [D loss: 0.632645, acc: 64.84%] [G loss: 1.909482]\n",
      "epoch:10 step:9668 [D loss: 0.584348, acc: 70.31%] [G loss: 2.109925]\n",
      "epoch:10 step:9669 [D loss: 0.624747, acc: 64.84%] [G loss: 2.192165]\n",
      "epoch:10 step:9670 [D loss: 0.586315, acc: 71.88%] [G loss: 2.050469]\n",
      "epoch:10 step:9671 [D loss: 0.715741, acc: 51.56%] [G loss: 1.917202]\n",
      "epoch:10 step:9672 [D loss: 0.685205, acc: 57.81%] [G loss: 2.050677]\n",
      "epoch:10 step:9673 [D loss: 0.590319, acc: 71.88%] [G loss: 2.087270]\n",
      "epoch:10 step:9674 [D loss: 0.667489, acc: 55.47%] [G loss: 1.974268]\n",
      "epoch:10 step:9675 [D loss: 0.647131, acc: 60.16%] [G loss: 2.112949]\n",
      "epoch:10 step:9676 [D loss: 0.604712, acc: 67.19%] [G loss: 2.087875]\n",
      "epoch:10 step:9677 [D loss: 0.606295, acc: 63.28%] [G loss: 1.855925]\n",
      "epoch:10 step:9678 [D loss: 0.627305, acc: 62.50%] [G loss: 2.032472]\n",
      "epoch:10 step:9679 [D loss: 0.607452, acc: 64.06%] [G loss: 2.075056]\n",
      "epoch:10 step:9680 [D loss: 0.658581, acc: 56.25%] [G loss: 1.958272]\n",
      "epoch:10 step:9681 [D loss: 0.651242, acc: 64.06%] [G loss: 2.004006]\n",
      "epoch:10 step:9682 [D loss: 0.532628, acc: 72.66%] [G loss: 2.538009]\n",
      "epoch:10 step:9683 [D loss: 0.618285, acc: 68.75%] [G loss: 2.271747]\n",
      "epoch:10 step:9684 [D loss: 0.512393, acc: 74.22%] [G loss: 2.331618]\n",
      "epoch:10 step:9685 [D loss: 0.531569, acc: 75.78%] [G loss: 2.374233]\n",
      "epoch:10 step:9686 [D loss: 0.641821, acc: 60.94%] [G loss: 1.942490]\n",
      "epoch:10 step:9687 [D loss: 0.676774, acc: 57.03%] [G loss: 2.013677]\n",
      "epoch:10 step:9688 [D loss: 0.642676, acc: 65.62%] [G loss: 2.062217]\n",
      "epoch:10 step:9689 [D loss: 0.631413, acc: 64.84%] [G loss: 1.918805]\n",
      "epoch:10 step:9690 [D loss: 0.680872, acc: 58.59%] [G loss: 1.980014]\n",
      "epoch:10 step:9691 [D loss: 0.654391, acc: 64.84%] [G loss: 2.129898]\n",
      "epoch:10 step:9692 [D loss: 0.561453, acc: 75.00%] [G loss: 2.024679]\n",
      "epoch:10 step:9693 [D loss: 0.670250, acc: 56.25%] [G loss: 2.021110]\n",
      "epoch:10 step:9694 [D loss: 0.665891, acc: 60.16%] [G loss: 1.994265]\n",
      "epoch:10 step:9695 [D loss: 0.598109, acc: 69.53%] [G loss: 1.978013]\n",
      "epoch:10 step:9696 [D loss: 0.663404, acc: 58.59%] [G loss: 2.149528]\n",
      "epoch:10 step:9697 [D loss: 0.704139, acc: 56.25%] [G loss: 1.907962]\n",
      "epoch:10 step:9698 [D loss: 0.661062, acc: 64.84%] [G loss: 2.066908]\n",
      "epoch:10 step:9699 [D loss: 0.612572, acc: 64.84%] [G loss: 2.131766]\n",
      "epoch:10 step:9700 [D loss: 0.601054, acc: 70.31%] [G loss: 2.020015]\n",
      "epoch:10 step:9701 [D loss: 0.597594, acc: 73.44%] [G loss: 2.170386]\n",
      "epoch:10 step:9702 [D loss: 0.578767, acc: 67.97%] [G loss: 2.018606]\n",
      "epoch:10 step:9703 [D loss: 0.575971, acc: 71.88%] [G loss: 2.192953]\n",
      "epoch:10 step:9704 [D loss: 0.640824, acc: 62.50%] [G loss: 2.123020]\n",
      "epoch:10 step:9705 [D loss: 0.660257, acc: 60.94%] [G loss: 2.170495]\n",
      "epoch:10 step:9706 [D loss: 0.660980, acc: 64.84%] [G loss: 2.200424]\n",
      "epoch:10 step:9707 [D loss: 0.630468, acc: 63.28%] [G loss: 1.997822]\n",
      "epoch:10 step:9708 [D loss: 0.590507, acc: 71.88%] [G loss: 2.144645]\n",
      "epoch:10 step:9709 [D loss: 0.576986, acc: 70.31%] [G loss: 2.124330]\n",
      "epoch:10 step:9710 [D loss: 0.624492, acc: 64.84%] [G loss: 2.195444]\n",
      "epoch:10 step:9711 [D loss: 0.741689, acc: 56.25%] [G loss: 1.982309]\n",
      "epoch:10 step:9712 [D loss: 0.648193, acc: 63.28%] [G loss: 2.039920]\n",
      "epoch:10 step:9713 [D loss: 0.686625, acc: 61.72%] [G loss: 1.990805]\n",
      "epoch:10 step:9714 [D loss: 0.563153, acc: 71.09%] [G loss: 2.302288]\n",
      "epoch:10 step:9715 [D loss: 0.537231, acc: 75.00%] [G loss: 2.450678]\n",
      "epoch:10 step:9716 [D loss: 0.536219, acc: 71.09%] [G loss: 2.645313]\n",
      "epoch:10 step:9717 [D loss: 0.537084, acc: 76.56%] [G loss: 2.522699]\n",
      "epoch:10 step:9718 [D loss: 0.770633, acc: 54.69%] [G loss: 1.841269]\n",
      "epoch:10 step:9719 [D loss: 0.725871, acc: 56.25%] [G loss: 1.926216]\n",
      "epoch:10 step:9720 [D loss: 0.649540, acc: 62.50%] [G loss: 2.133855]\n",
      "epoch:10 step:9721 [D loss: 0.632006, acc: 68.75%] [G loss: 1.993273]\n",
      "epoch:10 step:9722 [D loss: 0.648714, acc: 62.50%] [G loss: 1.951820]\n",
      "epoch:10 step:9723 [D loss: 0.638495, acc: 58.59%] [G loss: 2.108443]\n",
      "epoch:10 step:9724 [D loss: 0.617320, acc: 66.41%] [G loss: 2.181813]\n",
      "epoch:10 step:9725 [D loss: 0.683553, acc: 58.59%] [G loss: 1.927976]\n",
      "epoch:10 step:9726 [D loss: 0.627605, acc: 64.84%] [G loss: 2.074040]\n",
      "epoch:10 step:9727 [D loss: 0.611411, acc: 61.72%] [G loss: 2.356843]\n",
      "epoch:10 step:9728 [D loss: 0.618320, acc: 65.62%] [G loss: 2.198813]\n",
      "epoch:10 step:9729 [D loss: 0.604605, acc: 66.41%] [G loss: 2.263592]\n",
      "epoch:10 step:9730 [D loss: 0.600431, acc: 66.41%] [G loss: 1.970683]\n",
      "epoch:10 step:9731 [D loss: 0.612050, acc: 67.97%] [G loss: 2.050201]\n",
      "epoch:10 step:9732 [D loss: 0.659655, acc: 60.94%] [G loss: 1.991544]\n",
      "epoch:10 step:9733 [D loss: 0.661945, acc: 59.38%] [G loss: 2.024233]\n",
      "epoch:10 step:9734 [D loss: 0.682803, acc: 59.38%] [G loss: 2.081716]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:10 step:9735 [D loss: 0.605946, acc: 65.62%] [G loss: 2.030236]\n",
      "epoch:10 step:9736 [D loss: 0.608055, acc: 65.62%] [G loss: 2.118267]\n",
      "epoch:10 step:9737 [D loss: 0.604148, acc: 71.88%] [G loss: 2.129108]\n",
      "epoch:10 step:9738 [D loss: 0.664411, acc: 61.72%] [G loss: 1.991677]\n",
      "epoch:10 step:9739 [D loss: 0.618935, acc: 64.06%] [G loss: 2.040379]\n",
      "epoch:10 step:9740 [D loss: 0.602693, acc: 67.19%] [G loss: 2.231127]\n",
      "epoch:10 step:9741 [D loss: 0.623237, acc: 64.84%] [G loss: 2.172734]\n",
      "epoch:10 step:9742 [D loss: 0.641104, acc: 63.28%] [G loss: 2.190139]\n",
      "epoch:10 step:9743 [D loss: 0.625384, acc: 66.41%] [G loss: 1.844104]\n",
      "epoch:10 step:9744 [D loss: 0.582075, acc: 73.44%] [G loss: 2.148184]\n",
      "epoch:10 step:9745 [D loss: 0.649687, acc: 62.50%] [G loss: 1.824069]\n",
      "epoch:10 step:9746 [D loss: 0.694699, acc: 57.81%] [G loss: 2.055564]\n",
      "epoch:10 step:9747 [D loss: 0.719423, acc: 56.25%] [G loss: 1.850009]\n",
      "epoch:10 step:9748 [D loss: 0.711935, acc: 54.69%] [G loss: 1.947139]\n",
      "epoch:10 step:9749 [D loss: 0.583156, acc: 68.75%] [G loss: 2.106785]\n",
      "epoch:10 step:9750 [D loss: 0.625549, acc: 65.62%] [G loss: 2.212863]\n",
      "epoch:10 step:9751 [D loss: 0.556529, acc: 71.88%] [G loss: 2.214915]\n",
      "epoch:10 step:9752 [D loss: 0.621632, acc: 64.84%] [G loss: 2.091006]\n",
      "epoch:10 step:9753 [D loss: 0.706174, acc: 60.16%] [G loss: 1.926067]\n",
      "epoch:10 step:9754 [D loss: 0.611858, acc: 69.53%] [G loss: 2.150340]\n",
      "epoch:10 step:9755 [D loss: 0.590578, acc: 66.41%] [G loss: 2.097167]\n",
      "epoch:10 step:9756 [D loss: 0.646551, acc: 64.06%] [G loss: 2.007002]\n",
      "epoch:10 step:9757 [D loss: 0.627661, acc: 62.50%] [G loss: 1.903599]\n",
      "epoch:10 step:9758 [D loss: 0.654993, acc: 64.06%] [G loss: 2.086890]\n",
      "epoch:10 step:9759 [D loss: 0.637764, acc: 64.84%] [G loss: 1.929375]\n",
      "epoch:10 step:9760 [D loss: 0.625791, acc: 69.53%] [G loss: 2.012931]\n",
      "epoch:10 step:9761 [D loss: 0.644105, acc: 64.84%] [G loss: 1.847073]\n",
      "epoch:10 step:9762 [D loss: 0.626236, acc: 67.97%] [G loss: 2.114249]\n",
      "epoch:10 step:9763 [D loss: 0.634582, acc: 64.84%] [G loss: 2.020938]\n",
      "epoch:10 step:9764 [D loss: 0.645797, acc: 65.62%] [G loss: 2.003996]\n",
      "epoch:10 step:9765 [D loss: 0.650904, acc: 63.28%] [G loss: 1.899570]\n",
      "epoch:10 step:9766 [D loss: 0.685199, acc: 51.56%] [G loss: 1.951881]\n",
      "epoch:10 step:9767 [D loss: 0.671002, acc: 60.16%] [G loss: 1.881444]\n",
      "epoch:10 step:9768 [D loss: 0.597465, acc: 66.41%] [G loss: 2.123542]\n",
      "epoch:10 step:9769 [D loss: 0.584368, acc: 71.88%] [G loss: 2.081410]\n",
      "epoch:10 step:9770 [D loss: 0.639276, acc: 60.94%] [G loss: 2.088840]\n",
      "epoch:10 step:9771 [D loss: 0.629367, acc: 60.94%] [G loss: 2.053124]\n",
      "epoch:10 step:9772 [D loss: 0.655580, acc: 65.62%] [G loss: 2.096265]\n",
      "epoch:10 step:9773 [D loss: 0.646108, acc: 64.84%] [G loss: 2.012534]\n",
      "epoch:10 step:9774 [D loss: 0.631467, acc: 67.19%] [G loss: 2.104235]\n",
      "epoch:10 step:9775 [D loss: 0.606665, acc: 64.06%] [G loss: 2.203454]\n",
      "epoch:10 step:9776 [D loss: 0.536718, acc: 78.12%] [G loss: 2.300181]\n",
      "epoch:10 step:9777 [D loss: 0.663644, acc: 60.16%] [G loss: 2.123583]\n",
      "epoch:10 step:9778 [D loss: 0.630541, acc: 62.50%] [G loss: 1.959903]\n",
      "epoch:10 step:9779 [D loss: 0.597769, acc: 67.97%] [G loss: 2.051617]\n",
      "epoch:10 step:9780 [D loss: 0.652823, acc: 65.62%] [G loss: 2.092336]\n",
      "epoch:10 step:9781 [D loss: 0.671339, acc: 54.69%] [G loss: 1.850522]\n",
      "epoch:10 step:9782 [D loss: 0.635251, acc: 63.28%] [G loss: 1.973464]\n",
      "epoch:10 step:9783 [D loss: 0.612844, acc: 66.41%] [G loss: 2.042891]\n",
      "epoch:10 step:9784 [D loss: 0.598765, acc: 70.31%] [G loss: 2.318145]\n",
      "epoch:10 step:9785 [D loss: 0.660675, acc: 64.06%] [G loss: 2.300834]\n",
      "epoch:10 step:9786 [D loss: 0.589928, acc: 67.97%] [G loss: 2.343564]\n",
      "epoch:10 step:9787 [D loss: 0.671925, acc: 60.94%] [G loss: 2.147287]\n",
      "epoch:10 step:9788 [D loss: 0.662585, acc: 63.28%] [G loss: 2.134054]\n",
      "epoch:10 step:9789 [D loss: 0.666561, acc: 60.94%] [G loss: 1.974249]\n",
      "epoch:10 step:9790 [D loss: 0.677133, acc: 57.81%] [G loss: 1.942097]\n",
      "epoch:10 step:9791 [D loss: 0.640910, acc: 65.62%] [G loss: 1.927128]\n",
      "epoch:10 step:9792 [D loss: 0.658787, acc: 62.50%] [G loss: 1.906480]\n",
      "epoch:10 step:9793 [D loss: 0.644243, acc: 59.38%] [G loss: 2.065058]\n",
      "epoch:10 step:9794 [D loss: 0.672768, acc: 61.72%] [G loss: 2.233984]\n",
      "epoch:10 step:9795 [D loss: 0.656138, acc: 59.38%] [G loss: 1.955546]\n",
      "epoch:10 step:9796 [D loss: 0.616698, acc: 64.06%] [G loss: 1.890123]\n",
      "epoch:10 step:9797 [D loss: 0.613049, acc: 65.62%] [G loss: 2.021271]\n",
      "epoch:10 step:9798 [D loss: 0.615791, acc: 64.06%] [G loss: 2.289337]\n",
      "epoch:10 step:9799 [D loss: 0.605197, acc: 67.97%] [G loss: 2.280217]\n",
      "epoch:10 step:9800 [D loss: 0.615530, acc: 64.06%] [G loss: 2.209100]\n",
      "##############\n",
      "[2.60978186 1.54316304 6.40655065 5.09226675 3.85036343 5.63396072\n",
      " 4.41659567 4.84898362 5.01514631 3.50479642]\n",
      "##########\n",
      "epoch:10 step:9801 [D loss: 0.666068, acc: 64.84%] [G loss: 1.989199]\n",
      "epoch:10 step:9802 [D loss: 0.726710, acc: 56.25%] [G loss: 1.870479]\n",
      "epoch:10 step:9803 [D loss: 0.649465, acc: 68.75%] [G loss: 2.056468]\n",
      "epoch:10 step:9804 [D loss: 0.655730, acc: 61.72%] [G loss: 2.000064]\n",
      "epoch:10 step:9805 [D loss: 0.644381, acc: 64.06%] [G loss: 1.993796]\n",
      "epoch:10 step:9806 [D loss: 0.644861, acc: 64.06%] [G loss: 2.020987]\n",
      "epoch:10 step:9807 [D loss: 0.790864, acc: 43.75%] [G loss: 1.809259]\n",
      "epoch:10 step:9808 [D loss: 0.647700, acc: 60.16%] [G loss: 1.841405]\n",
      "epoch:10 step:9809 [D loss: 0.664664, acc: 58.59%] [G loss: 1.974042]\n",
      "epoch:10 step:9810 [D loss: 0.617997, acc: 64.84%] [G loss: 2.024765]\n",
      "epoch:10 step:9811 [D loss: 0.626407, acc: 64.84%] [G loss: 1.976469]\n",
      "epoch:10 step:9812 [D loss: 0.673867, acc: 60.16%] [G loss: 1.926201]\n",
      "epoch:10 step:9813 [D loss: 0.645440, acc: 63.28%] [G loss: 1.879478]\n",
      "epoch:10 step:9814 [D loss: 0.679665, acc: 52.34%] [G loss: 1.877232]\n",
      "epoch:10 step:9815 [D loss: 0.621286, acc: 67.19%] [G loss: 1.929882]\n",
      "epoch:10 step:9816 [D loss: 0.626064, acc: 61.72%] [G loss: 1.896253]\n",
      "epoch:10 step:9817 [D loss: 0.649927, acc: 65.62%] [G loss: 1.994925]\n",
      "epoch:10 step:9818 [D loss: 0.626634, acc: 64.84%] [G loss: 1.983631]\n",
      "epoch:10 step:9819 [D loss: 0.630259, acc: 64.84%] [G loss: 1.943486]\n",
      "epoch:10 step:9820 [D loss: 0.612065, acc: 71.88%] [G loss: 2.167581]\n",
      "epoch:10 step:9821 [D loss: 0.619755, acc: 64.06%] [G loss: 2.084579]\n",
      "epoch:10 step:9822 [D loss: 0.658138, acc: 60.16%] [G loss: 1.872285]\n",
      "epoch:10 step:9823 [D loss: 0.609917, acc: 65.62%] [G loss: 2.026420]\n",
      "epoch:10 step:9824 [D loss: 0.622152, acc: 63.28%] [G loss: 2.061094]\n",
      "epoch:10 step:9825 [D loss: 0.688414, acc: 54.69%] [G loss: 2.014225]\n",
      "epoch:10 step:9826 [D loss: 0.573627, acc: 71.88%] [G loss: 1.956199]\n",
      "epoch:10 step:9827 [D loss: 0.645253, acc: 63.28%] [G loss: 2.027556]\n",
      "epoch:10 step:9828 [D loss: 0.660031, acc: 58.59%] [G loss: 1.833638]\n",
      "epoch:10 step:9829 [D loss: 0.656864, acc: 62.50%] [G loss: 1.879669]\n",
      "epoch:10 step:9830 [D loss: 0.683924, acc: 59.38%] [G loss: 1.956820]\n",
      "epoch:10 step:9831 [D loss: 0.629487, acc: 65.62%] [G loss: 1.843513]\n",
      "epoch:10 step:9832 [D loss: 0.671213, acc: 58.59%] [G loss: 1.851897]\n",
      "epoch:10 step:9833 [D loss: 0.647500, acc: 62.50%] [G loss: 1.995124]\n",
      "epoch:10 step:9834 [D loss: 0.636470, acc: 64.84%] [G loss: 2.093744]\n",
      "epoch:10 step:9835 [D loss: 0.706904, acc: 49.22%] [G loss: 1.959182]\n",
      "epoch:10 step:9836 [D loss: 0.620996, acc: 64.06%] [G loss: 1.959908]\n",
      "epoch:10 step:9837 [D loss: 0.627347, acc: 70.31%] [G loss: 1.967987]\n",
      "epoch:10 step:9838 [D loss: 0.645020, acc: 64.84%] [G loss: 2.007919]\n",
      "epoch:10 step:9839 [D loss: 0.590472, acc: 71.88%] [G loss: 2.157120]\n",
      "epoch:10 step:9840 [D loss: 0.603597, acc: 67.19%] [G loss: 2.084531]\n",
      "epoch:10 step:9841 [D loss: 0.597115, acc: 70.31%] [G loss: 2.383612]\n",
      "epoch:10 step:9842 [D loss: 0.638043, acc: 64.84%] [G loss: 2.187059]\n",
      "epoch:10 step:9843 [D loss: 0.624367, acc: 66.41%] [G loss: 2.046528]\n",
      "epoch:10 step:9844 [D loss: 0.629974, acc: 68.75%] [G loss: 2.019945]\n",
      "epoch:10 step:9845 [D loss: 0.624400, acc: 65.62%] [G loss: 2.179975]\n",
      "epoch:10 step:9846 [D loss: 0.639622, acc: 58.59%] [G loss: 2.100475]\n",
      "epoch:10 step:9847 [D loss: 0.662195, acc: 61.72%] [G loss: 1.894608]\n",
      "epoch:10 step:9848 [D loss: 0.659649, acc: 59.38%] [G loss: 1.948183]\n",
      "epoch:10 step:9849 [D loss: 0.634357, acc: 60.16%] [G loss: 2.301682]\n",
      "epoch:10 step:9850 [D loss: 0.657312, acc: 63.28%] [G loss: 2.000710]\n",
      "epoch:10 step:9851 [D loss: 0.600583, acc: 72.66%] [G loss: 2.186224]\n",
      "epoch:10 step:9852 [D loss: 0.720253, acc: 51.56%] [G loss: 1.934915]\n",
      "epoch:10 step:9853 [D loss: 0.655090, acc: 60.94%] [G loss: 2.045315]\n",
      "epoch:10 step:9854 [D loss: 0.619611, acc: 64.84%] [G loss: 2.127548]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:10 step:9855 [D loss: 0.693452, acc: 54.69%] [G loss: 2.014304]\n",
      "epoch:10 step:9856 [D loss: 0.662633, acc: 65.62%] [G loss: 1.942056]\n",
      "epoch:10 step:9857 [D loss: 0.649209, acc: 64.06%] [G loss: 1.849146]\n",
      "epoch:10 step:9858 [D loss: 0.604760, acc: 64.84%] [G loss: 2.163901]\n",
      "epoch:10 step:9859 [D loss: 0.670507, acc: 60.94%] [G loss: 1.920443]\n",
      "epoch:10 step:9860 [D loss: 0.596071, acc: 66.41%] [G loss: 2.029730]\n",
      "epoch:10 step:9861 [D loss: 0.623154, acc: 67.97%] [G loss: 2.034349]\n",
      "epoch:10 step:9862 [D loss: 0.611111, acc: 65.62%] [G loss: 1.939480]\n",
      "epoch:10 step:9863 [D loss: 0.629235, acc: 67.97%] [G loss: 2.003072]\n",
      "epoch:10 step:9864 [D loss: 0.606982, acc: 66.41%] [G loss: 2.094398]\n",
      "epoch:10 step:9865 [D loss: 0.568583, acc: 73.44%] [G loss: 2.185840]\n",
      "epoch:10 step:9866 [D loss: 0.601522, acc: 65.62%] [G loss: 1.923266]\n",
      "epoch:10 step:9867 [D loss: 0.608869, acc: 65.62%] [G loss: 2.232499]\n",
      "epoch:10 step:9868 [D loss: 0.553478, acc: 72.66%] [G loss: 2.305657]\n",
      "epoch:10 step:9869 [D loss: 0.536167, acc: 77.34%] [G loss: 2.468644]\n",
      "epoch:10 step:9870 [D loss: 0.741964, acc: 56.25%] [G loss: 1.837322]\n",
      "epoch:10 step:9871 [D loss: 0.714034, acc: 56.25%] [G loss: 1.926287]\n",
      "epoch:10 step:9872 [D loss: 0.687229, acc: 57.81%] [G loss: 1.952340]\n",
      "epoch:10 step:9873 [D loss: 0.598834, acc: 69.53%] [G loss: 2.024754]\n",
      "epoch:10 step:9874 [D loss: 0.541102, acc: 71.88%] [G loss: 2.242222]\n",
      "epoch:10 step:9875 [D loss: 0.586108, acc: 69.53%] [G loss: 2.010073]\n",
      "epoch:10 step:9876 [D loss: 0.676050, acc: 57.81%] [G loss: 2.082854]\n",
      "epoch:10 step:9877 [D loss: 0.618336, acc: 63.28%] [G loss: 2.071493]\n",
      "epoch:10 step:9878 [D loss: 0.574607, acc: 70.31%] [G loss: 2.248274]\n",
      "epoch:10 step:9879 [D loss: 0.672837, acc: 64.84%] [G loss: 1.980844]\n",
      "epoch:10 step:9880 [D loss: 0.644780, acc: 64.84%] [G loss: 1.897208]\n",
      "epoch:10 step:9881 [D loss: 0.729107, acc: 51.56%] [G loss: 1.922822]\n",
      "epoch:10 step:9882 [D loss: 0.588550, acc: 71.88%] [G loss: 1.898331]\n",
      "epoch:10 step:9883 [D loss: 0.670224, acc: 61.72%] [G loss: 2.071044]\n",
      "epoch:10 step:9884 [D loss: 0.645268, acc: 56.25%] [G loss: 2.033931]\n",
      "epoch:10 step:9885 [D loss: 0.613037, acc: 63.28%] [G loss: 2.001049]\n",
      "epoch:10 step:9886 [D loss: 0.561539, acc: 71.88%] [G loss: 2.236504]\n",
      "epoch:10 step:9887 [D loss: 0.591865, acc: 65.62%] [G loss: 2.168021]\n",
      "epoch:10 step:9888 [D loss: 0.604054, acc: 61.72%] [G loss: 2.079354]\n",
      "epoch:10 step:9889 [D loss: 0.612874, acc: 65.62%] [G loss: 2.166078]\n",
      "epoch:10 step:9890 [D loss: 0.602097, acc: 66.41%] [G loss: 2.237008]\n",
      "epoch:10 step:9891 [D loss: 0.621893, acc: 63.28%] [G loss: 2.172425]\n",
      "epoch:10 step:9892 [D loss: 0.615266, acc: 64.06%] [G loss: 2.053889]\n",
      "epoch:10 step:9893 [D loss: 0.609741, acc: 67.19%] [G loss: 2.218329]\n",
      "epoch:10 step:9894 [D loss: 0.612767, acc: 59.38%] [G loss: 2.065451]\n",
      "epoch:10 step:9895 [D loss: 0.638924, acc: 67.97%] [G loss: 2.069731]\n",
      "epoch:10 step:9896 [D loss: 0.597648, acc: 71.09%] [G loss: 2.145608]\n",
      "epoch:10 step:9897 [D loss: 0.668696, acc: 59.38%] [G loss: 2.008667]\n",
      "epoch:10 step:9898 [D loss: 0.706663, acc: 54.69%] [G loss: 1.972080]\n",
      "epoch:10 step:9899 [D loss: 0.733515, acc: 54.69%] [G loss: 1.791466]\n",
      "epoch:10 step:9900 [D loss: 0.646833, acc: 60.16%] [G loss: 1.870595]\n",
      "epoch:10 step:9901 [D loss: 0.639247, acc: 65.62%] [G loss: 1.882410]\n",
      "epoch:10 step:9902 [D loss: 0.609828, acc: 71.88%] [G loss: 2.170413]\n",
      "epoch:10 step:9903 [D loss: 0.644108, acc: 59.38%] [G loss: 2.051675]\n",
      "epoch:10 step:9904 [D loss: 0.609184, acc: 67.97%] [G loss: 2.020348]\n",
      "epoch:10 step:9905 [D loss: 0.618414, acc: 68.75%] [G loss: 2.058314]\n",
      "epoch:10 step:9906 [D loss: 0.608138, acc: 62.50%] [G loss: 2.123475]\n",
      "epoch:10 step:9907 [D loss: 0.601965, acc: 63.28%] [G loss: 1.906453]\n",
      "epoch:10 step:9908 [D loss: 0.640669, acc: 64.06%] [G loss: 1.914470]\n",
      "epoch:10 step:9909 [D loss: 0.614991, acc: 69.53%] [G loss: 1.918112]\n",
      "epoch:10 step:9910 [D loss: 0.635358, acc: 65.62%] [G loss: 1.892091]\n",
      "epoch:10 step:9911 [D loss: 0.676023, acc: 60.94%] [G loss: 1.905284]\n",
      "epoch:10 step:9912 [D loss: 0.653287, acc: 60.16%] [G loss: 2.006834]\n",
      "epoch:10 step:9913 [D loss: 0.709969, acc: 56.25%] [G loss: 2.026206]\n",
      "epoch:10 step:9914 [D loss: 0.709475, acc: 57.03%] [G loss: 1.926267]\n",
      "epoch:10 step:9915 [D loss: 0.620136, acc: 65.62%] [G loss: 2.119582]\n",
      "epoch:10 step:9916 [D loss: 0.617364, acc: 64.06%] [G loss: 1.960842]\n",
      "epoch:10 step:9917 [D loss: 0.609139, acc: 60.94%] [G loss: 2.120926]\n",
      "epoch:10 step:9918 [D loss: 0.629157, acc: 61.72%] [G loss: 2.238201]\n",
      "epoch:10 step:9919 [D loss: 0.589134, acc: 71.09%] [G loss: 2.117923]\n",
      "epoch:10 step:9920 [D loss: 0.661331, acc: 60.16%] [G loss: 2.108605]\n",
      "epoch:10 step:9921 [D loss: 0.609936, acc: 67.19%] [G loss: 2.385343]\n",
      "epoch:10 step:9922 [D loss: 0.622717, acc: 66.41%] [G loss: 2.240832]\n",
      "epoch:10 step:9923 [D loss: 0.628688, acc: 63.28%] [G loss: 1.878533]\n",
      "epoch:10 step:9924 [D loss: 0.580227, acc: 65.62%] [G loss: 2.190868]\n",
      "epoch:10 step:9925 [D loss: 0.586768, acc: 71.88%] [G loss: 1.971983]\n",
      "epoch:10 step:9926 [D loss: 0.608339, acc: 65.62%] [G loss: 2.271028]\n",
      "epoch:10 step:9927 [D loss: 0.648512, acc: 60.94%] [G loss: 2.240450]\n",
      "epoch:10 step:9928 [D loss: 0.614496, acc: 62.50%] [G loss: 2.207605]\n",
      "epoch:10 step:9929 [D loss: 0.651517, acc: 57.81%] [G loss: 1.937413]\n",
      "epoch:10 step:9930 [D loss: 0.650580, acc: 59.38%] [G loss: 1.877919]\n",
      "epoch:10 step:9931 [D loss: 0.677331, acc: 59.38%] [G loss: 1.944896]\n",
      "epoch:10 step:9932 [D loss: 0.637849, acc: 65.62%] [G loss: 1.905481]\n",
      "epoch:10 step:9933 [D loss: 0.598683, acc: 71.88%] [G loss: 2.036854]\n",
      "epoch:10 step:9934 [D loss: 0.573778, acc: 72.66%] [G loss: 2.362364]\n",
      "epoch:10 step:9935 [D loss: 0.667806, acc: 58.59%] [G loss: 2.093143]\n",
      "epoch:10 step:9936 [D loss: 0.715735, acc: 53.12%] [G loss: 1.882107]\n",
      "epoch:10 step:9937 [D loss: 0.658019, acc: 63.28%] [G loss: 1.984680]\n",
      "epoch:10 step:9938 [D loss: 0.627131, acc: 66.41%] [G loss: 1.868236]\n",
      "epoch:10 step:9939 [D loss: 0.611142, acc: 71.88%] [G loss: 2.133534]\n",
      "epoch:10 step:9940 [D loss: 0.624341, acc: 61.72%] [G loss: 2.039348]\n",
      "epoch:10 step:9941 [D loss: 0.632813, acc: 65.62%] [G loss: 2.088422]\n",
      "epoch:10 step:9942 [D loss: 0.674805, acc: 64.06%] [G loss: 1.765741]\n",
      "epoch:10 step:9943 [D loss: 0.664767, acc: 62.50%] [G loss: 2.020183]\n",
      "epoch:10 step:9944 [D loss: 0.602723, acc: 69.53%] [G loss: 2.098132]\n",
      "epoch:10 step:9945 [D loss: 0.641475, acc: 63.28%] [G loss: 1.960390]\n",
      "epoch:10 step:9946 [D loss: 0.666894, acc: 61.72%] [G loss: 1.985452]\n",
      "epoch:10 step:9947 [D loss: 0.659262, acc: 58.59%] [G loss: 1.974662]\n",
      "epoch:10 step:9948 [D loss: 0.619192, acc: 64.06%] [G loss: 2.051842]\n",
      "epoch:10 step:9949 [D loss: 0.643650, acc: 64.84%] [G loss: 1.989561]\n",
      "epoch:10 step:9950 [D loss: 0.624858, acc: 63.28%] [G loss: 1.990916]\n",
      "epoch:10 step:9951 [D loss: 0.617367, acc: 65.62%] [G loss: 1.962885]\n",
      "epoch:10 step:9952 [D loss: 0.638738, acc: 64.06%] [G loss: 2.177753]\n",
      "epoch:10 step:9953 [D loss: 0.698391, acc: 56.25%] [G loss: 1.919481]\n",
      "epoch:10 step:9954 [D loss: 0.639544, acc: 64.84%] [G loss: 1.998921]\n",
      "epoch:10 step:9955 [D loss: 0.627189, acc: 61.72%] [G loss: 1.951601]\n",
      "epoch:10 step:9956 [D loss: 0.663265, acc: 61.72%] [G loss: 1.959982]\n",
      "epoch:10 step:9957 [D loss: 0.679515, acc: 60.16%] [G loss: 2.017475]\n",
      "epoch:10 step:9958 [D loss: 0.628884, acc: 61.72%] [G loss: 2.059703]\n",
      "epoch:10 step:9959 [D loss: 0.619470, acc: 67.19%] [G loss: 2.126091]\n",
      "epoch:10 step:9960 [D loss: 0.595587, acc: 71.88%] [G loss: 2.119455]\n",
      "epoch:10 step:9961 [D loss: 0.624615, acc: 66.41%] [G loss: 2.051518]\n",
      "epoch:10 step:9962 [D loss: 0.609112, acc: 65.62%] [G loss: 2.107376]\n",
      "epoch:10 step:9963 [D loss: 0.625904, acc: 64.84%] [G loss: 1.865274]\n",
      "epoch:10 step:9964 [D loss: 0.631221, acc: 67.19%] [G loss: 2.075760]\n",
      "epoch:10 step:9965 [D loss: 0.637729, acc: 65.62%] [G loss: 1.998668]\n",
      "epoch:10 step:9966 [D loss: 0.618863, acc: 64.84%] [G loss: 2.006360]\n",
      "epoch:10 step:9967 [D loss: 0.626218, acc: 67.97%] [G loss: 2.034204]\n",
      "epoch:10 step:9968 [D loss: 0.593472, acc: 65.62%] [G loss: 2.162417]\n",
      "epoch:10 step:9969 [D loss: 0.680018, acc: 57.03%] [G loss: 1.901005]\n",
      "epoch:10 step:9970 [D loss: 0.686146, acc: 56.25%] [G loss: 1.780515]\n",
      "epoch:10 step:9971 [D loss: 0.646736, acc: 63.28%] [G loss: 1.937284]\n",
      "epoch:10 step:9972 [D loss: 0.679543, acc: 59.38%] [G loss: 1.820135]\n",
      "epoch:10 step:9973 [D loss: 0.653466, acc: 60.16%] [G loss: 1.923823]\n",
      "epoch:10 step:9974 [D loss: 0.632628, acc: 68.75%] [G loss: 1.936132]\n",
      "epoch:10 step:9975 [D loss: 0.647619, acc: 65.62%] [G loss: 2.226076]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:10 step:9976 [D loss: 0.617388, acc: 60.16%] [G loss: 2.038990]\n",
      "epoch:10 step:9977 [D loss: 0.607525, acc: 64.06%] [G loss: 1.980996]\n",
      "epoch:10 step:9978 [D loss: 0.612839, acc: 68.75%] [G loss: 1.965862]\n",
      "epoch:10 step:9979 [D loss: 0.595526, acc: 69.53%] [G loss: 2.052065]\n",
      "epoch:10 step:9980 [D loss: 0.624349, acc: 67.97%] [G loss: 1.992836]\n",
      "epoch:10 step:9981 [D loss: 0.677762, acc: 62.50%] [G loss: 1.943129]\n",
      "epoch:10 step:9982 [D loss: 0.675653, acc: 57.03%] [G loss: 1.840435]\n",
      "epoch:10 step:9983 [D loss: 0.639382, acc: 60.16%] [G loss: 1.848100]\n",
      "epoch:10 step:9984 [D loss: 0.633015, acc: 65.62%] [G loss: 1.861422]\n",
      "epoch:10 step:9985 [D loss: 0.697792, acc: 58.59%] [G loss: 1.858520]\n",
      "epoch:10 step:9986 [D loss: 0.643422, acc: 64.06%] [G loss: 1.955976]\n",
      "epoch:10 step:9987 [D loss: 0.658130, acc: 59.38%] [G loss: 2.069529]\n",
      "epoch:10 step:9988 [D loss: 0.678731, acc: 61.72%] [G loss: 1.977616]\n",
      "epoch:10 step:9989 [D loss: 0.671820, acc: 55.47%] [G loss: 1.986858]\n",
      "epoch:10 step:9990 [D loss: 0.632708, acc: 64.84%] [G loss: 1.945883]\n",
      "epoch:10 step:9991 [D loss: 0.653592, acc: 61.72%] [G loss: 1.983537]\n",
      "epoch:10 step:9992 [D loss: 0.597437, acc: 67.19%] [G loss: 2.025283]\n",
      "epoch:10 step:9993 [D loss: 0.695430, acc: 57.81%] [G loss: 1.924401]\n",
      "epoch:10 step:9994 [D loss: 0.586531, acc: 71.09%] [G loss: 2.265569]\n",
      "epoch:10 step:9995 [D loss: 0.656335, acc: 62.50%] [G loss: 1.876471]\n",
      "epoch:10 step:9996 [D loss: 0.664501, acc: 60.94%] [G loss: 1.975706]\n",
      "epoch:10 step:9997 [D loss: 0.624432, acc: 67.97%] [G loss: 1.989691]\n",
      "epoch:10 step:9998 [D loss: 0.623677, acc: 64.84%] [G loss: 1.986609]\n",
      "epoch:10 step:9999 [D loss: 0.602197, acc: 75.78%] [G loss: 2.180532]\n",
      "epoch:10 step:10000 [D loss: 0.651089, acc: 68.75%] [G loss: 1.996038]\n",
      "##############\n",
      "[2.53746338 1.44353695 6.47398821 4.90775739 3.76108346 5.78324381\n",
      " 4.5340593  4.74709125 4.89826665 3.78717066]\n",
      "##########\n",
      "epoch:10 step:10001 [D loss: 0.585187, acc: 69.53%] [G loss: 2.204251]\n",
      "epoch:10 step:10002 [D loss: 0.607946, acc: 68.75%] [G loss: 2.039029]\n",
      "epoch:10 step:10003 [D loss: 0.662225, acc: 57.81%] [G loss: 2.129888]\n",
      "epoch:10 step:10004 [D loss: 0.590907, acc: 67.19%] [G loss: 2.173131]\n",
      "epoch:10 step:10005 [D loss: 0.581927, acc: 66.41%] [G loss: 2.059751]\n",
      "epoch:10 step:10006 [D loss: 0.606272, acc: 64.06%] [G loss: 2.094896]\n",
      "epoch:10 step:10007 [D loss: 0.605084, acc: 65.62%] [G loss: 2.176317]\n",
      "epoch:10 step:10008 [D loss: 0.579781, acc: 66.41%] [G loss: 2.005000]\n",
      "epoch:10 step:10009 [D loss: 0.630766, acc: 64.06%] [G loss: 2.079019]\n",
      "epoch:10 step:10010 [D loss: 0.683445, acc: 60.16%] [G loss: 2.256026]\n",
      "epoch:10 step:10011 [D loss: 0.562648, acc: 68.75%] [G loss: 2.313129]\n",
      "epoch:10 step:10012 [D loss: 0.626592, acc: 60.16%] [G loss: 2.115181]\n",
      "epoch:10 step:10013 [D loss: 0.670587, acc: 64.06%] [G loss: 2.005724]\n",
      "epoch:10 step:10014 [D loss: 0.602314, acc: 67.97%] [G loss: 2.058234]\n",
      "epoch:10 step:10015 [D loss: 0.691353, acc: 60.94%] [G loss: 1.963535]\n",
      "epoch:10 step:10016 [D loss: 0.700522, acc: 64.06%] [G loss: 1.989271]\n",
      "epoch:10 step:10017 [D loss: 0.595525, acc: 68.75%] [G loss: 2.349074]\n",
      "epoch:10 step:10018 [D loss: 0.621618, acc: 67.19%] [G loss: 2.332820]\n",
      "epoch:10 step:10019 [D loss: 0.507650, acc: 80.47%] [G loss: 2.487796]\n",
      "epoch:10 step:10020 [D loss: 0.598644, acc: 65.62%] [G loss: 2.219992]\n",
      "epoch:10 step:10021 [D loss: 0.601264, acc: 70.31%] [G loss: 2.139227]\n",
      "epoch:10 step:10022 [D loss: 0.641563, acc: 66.41%] [G loss: 2.239912]\n",
      "epoch:10 step:10023 [D loss: 0.661999, acc: 59.38%] [G loss: 1.979738]\n",
      "epoch:10 step:10024 [D loss: 0.636201, acc: 64.06%] [G loss: 2.101328]\n",
      "epoch:10 step:10025 [D loss: 0.626851, acc: 60.16%] [G loss: 2.073512]\n",
      "epoch:10 step:10026 [D loss: 0.646128, acc: 57.81%] [G loss: 1.959825]\n",
      "epoch:10 step:10027 [D loss: 0.710240, acc: 61.72%] [G loss: 1.929606]\n",
      "epoch:10 step:10028 [D loss: 0.699281, acc: 54.69%] [G loss: 1.816290]\n",
      "epoch:10 step:10029 [D loss: 0.649840, acc: 60.94%] [G loss: 2.002241]\n",
      "epoch:10 step:10030 [D loss: 0.622015, acc: 64.84%] [G loss: 1.890740]\n",
      "epoch:10 step:10031 [D loss: 0.649625, acc: 62.50%] [G loss: 2.070525]\n",
      "epoch:10 step:10032 [D loss: 0.628957, acc: 66.41%] [G loss: 2.075583]\n",
      "epoch:10 step:10033 [D loss: 0.633013, acc: 63.28%] [G loss: 1.923293]\n",
      "epoch:10 step:10034 [D loss: 0.674069, acc: 60.16%] [G loss: 1.939842]\n",
      "epoch:10 step:10035 [D loss: 0.623775, acc: 63.28%] [G loss: 2.134140]\n",
      "epoch:10 step:10036 [D loss: 0.626410, acc: 63.28%] [G loss: 1.960645]\n",
      "epoch:10 step:10037 [D loss: 0.718902, acc: 57.81%] [G loss: 1.874965]\n",
      "epoch:10 step:10038 [D loss: 0.660725, acc: 59.38%] [G loss: 2.153450]\n",
      "epoch:10 step:10039 [D loss: 0.726498, acc: 52.34%] [G loss: 1.864079]\n",
      "epoch:10 step:10040 [D loss: 0.674091, acc: 60.16%] [G loss: 1.865407]\n",
      "epoch:10 step:10041 [D loss: 0.647622, acc: 60.94%] [G loss: 1.978962]\n",
      "epoch:10 step:10042 [D loss: 0.604631, acc: 67.19%] [G loss: 1.986254]\n",
      "epoch:10 step:10043 [D loss: 0.596037, acc: 71.09%] [G loss: 1.984904]\n",
      "epoch:10 step:10044 [D loss: 0.650177, acc: 63.28%] [G loss: 1.876063]\n",
      "epoch:10 step:10045 [D loss: 0.632969, acc: 64.06%] [G loss: 1.990604]\n",
      "epoch:10 step:10046 [D loss: 0.654346, acc: 64.84%] [G loss: 1.949309]\n",
      "epoch:10 step:10047 [D loss: 0.631951, acc: 62.50%] [G loss: 2.072889]\n",
      "epoch:10 step:10048 [D loss: 0.627895, acc: 65.62%] [G loss: 2.006097]\n",
      "epoch:10 step:10049 [D loss: 0.673652, acc: 61.72%] [G loss: 1.982132]\n",
      "epoch:10 step:10050 [D loss: 0.617911, acc: 64.84%] [G loss: 1.864341]\n",
      "epoch:10 step:10051 [D loss: 0.619832, acc: 70.31%] [G loss: 2.286534]\n",
      "epoch:10 step:10052 [D loss: 0.653731, acc: 63.28%] [G loss: 1.985696]\n",
      "epoch:10 step:10053 [D loss: 0.644798, acc: 61.72%] [G loss: 1.938703]\n",
      "epoch:10 step:10054 [D loss: 0.603880, acc: 67.19%] [G loss: 1.875759]\n",
      "epoch:10 step:10055 [D loss: 0.620755, acc: 60.94%] [G loss: 1.920460]\n",
      "epoch:10 step:10056 [D loss: 0.599883, acc: 75.78%] [G loss: 2.000473]\n",
      "epoch:10 step:10057 [D loss: 0.648431, acc: 63.28%] [G loss: 1.966715]\n",
      "epoch:10 step:10058 [D loss: 0.706548, acc: 62.50%] [G loss: 2.086684]\n",
      "epoch:10 step:10059 [D loss: 0.652988, acc: 64.06%] [G loss: 2.109253]\n",
      "epoch:10 step:10060 [D loss: 0.617411, acc: 64.84%] [G loss: 1.859993]\n",
      "epoch:10 step:10061 [D loss: 0.634469, acc: 64.06%] [G loss: 2.153760]\n",
      "epoch:10 step:10062 [D loss: 0.594793, acc: 71.09%] [G loss: 2.315633]\n",
      "epoch:10 step:10063 [D loss: 0.653741, acc: 60.94%] [G loss: 2.041493]\n",
      "epoch:10 step:10064 [D loss: 0.542473, acc: 73.44%] [G loss: 2.309373]\n",
      "epoch:10 step:10065 [D loss: 0.654297, acc: 60.94%] [G loss: 2.147289]\n",
      "epoch:10 step:10066 [D loss: 0.677983, acc: 61.72%] [G loss: 2.085011]\n",
      "epoch:10 step:10067 [D loss: 0.617716, acc: 66.41%] [G loss: 2.156554]\n",
      "epoch:10 step:10068 [D loss: 0.676415, acc: 55.47%] [G loss: 1.998722]\n",
      "epoch:10 step:10069 [D loss: 0.640339, acc: 61.72%] [G loss: 2.094472]\n",
      "epoch:10 step:10070 [D loss: 0.674555, acc: 62.50%] [G loss: 2.052025]\n",
      "epoch:10 step:10071 [D loss: 0.643453, acc: 66.41%] [G loss: 2.070622]\n",
      "epoch:10 step:10072 [D loss: 0.678473, acc: 61.72%] [G loss: 1.917134]\n",
      "epoch:10 step:10073 [D loss: 0.631104, acc: 61.72%] [G loss: 2.097485]\n",
      "epoch:10 step:10074 [D loss: 0.680870, acc: 61.72%] [G loss: 1.923024]\n",
      "epoch:10 step:10075 [D loss: 0.639556, acc: 60.94%] [G loss: 1.760922]\n",
      "epoch:10 step:10076 [D loss: 0.595456, acc: 65.62%] [G loss: 2.043696]\n",
      "epoch:10 step:10077 [D loss: 0.584015, acc: 70.31%] [G loss: 2.092927]\n",
      "epoch:10 step:10078 [D loss: 0.618357, acc: 64.84%] [G loss: 2.237267]\n",
      "epoch:10 step:10079 [D loss: 0.570917, acc: 71.88%] [G loss: 2.200430]\n",
      "epoch:10 step:10080 [D loss: 0.614943, acc: 70.31%] [G loss: 1.973048]\n",
      "epoch:10 step:10081 [D loss: 0.624422, acc: 64.84%] [G loss: 2.137391]\n",
      "epoch:10 step:10082 [D loss: 0.647732, acc: 62.50%] [G loss: 2.144551]\n",
      "epoch:10 step:10083 [D loss: 0.672591, acc: 57.03%] [G loss: 2.066767]\n",
      "epoch:10 step:10084 [D loss: 0.598757, acc: 71.88%] [G loss: 2.039408]\n",
      "epoch:10 step:10085 [D loss: 0.670460, acc: 61.72%] [G loss: 2.121075]\n",
      "epoch:10 step:10086 [D loss: 0.610966, acc: 67.19%] [G loss: 1.768389]\n",
      "epoch:10 step:10087 [D loss: 0.664056, acc: 57.81%] [G loss: 1.959725]\n",
      "epoch:10 step:10088 [D loss: 0.658331, acc: 61.72%] [G loss: 1.986961]\n",
      "epoch:10 step:10089 [D loss: 0.644502, acc: 63.28%] [G loss: 2.152589]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:10 step:10090 [D loss: 0.603909, acc: 68.75%] [G loss: 2.090623]\n",
      "epoch:10 step:10091 [D loss: 0.630272, acc: 64.06%] [G loss: 1.983142]\n",
      "epoch:10 step:10092 [D loss: 0.654068, acc: 62.50%] [G loss: 1.835582]\n",
      "epoch:10 step:10093 [D loss: 0.692495, acc: 53.12%] [G loss: 1.921083]\n",
      "epoch:10 step:10094 [D loss: 0.620389, acc: 62.50%] [G loss: 1.945020]\n",
      "epoch:10 step:10095 [D loss: 0.609078, acc: 65.62%] [G loss: 2.159382]\n",
      "epoch:10 step:10096 [D loss: 0.635924, acc: 62.50%] [G loss: 2.052863]\n",
      "epoch:10 step:10097 [D loss: 0.637067, acc: 67.97%] [G loss: 2.047794]\n",
      "epoch:10 step:10098 [D loss: 0.594644, acc: 65.62%] [G loss: 2.042523]\n",
      "epoch:10 step:10099 [D loss: 0.586560, acc: 68.75%] [G loss: 1.928346]\n",
      "epoch:10 step:10100 [D loss: 0.639889, acc: 64.84%] [G loss: 1.941369]\n",
      "epoch:10 step:10101 [D loss: 0.599125, acc: 72.66%] [G loss: 2.130980]\n",
      "epoch:10 step:10102 [D loss: 0.640912, acc: 61.72%] [G loss: 2.068796]\n",
      "epoch:10 step:10103 [D loss: 0.619921, acc: 64.84%] [G loss: 2.042855]\n",
      "epoch:10 step:10104 [D loss: 0.637302, acc: 63.28%] [G loss: 1.978682]\n",
      "epoch:10 step:10105 [D loss: 0.645029, acc: 64.84%] [G loss: 2.013371]\n",
      "epoch:10 step:10106 [D loss: 0.583284, acc: 66.41%] [G loss: 1.978377]\n",
      "epoch:10 step:10107 [D loss: 0.723181, acc: 56.25%] [G loss: 2.023251]\n",
      "epoch:10 step:10108 [D loss: 0.643467, acc: 62.50%] [G loss: 2.003713]\n",
      "epoch:10 step:10109 [D loss: 0.647375, acc: 64.06%] [G loss: 2.088996]\n",
      "epoch:10 step:10110 [D loss: 0.609515, acc: 65.62%] [G loss: 1.953763]\n",
      "epoch:10 step:10111 [D loss: 0.700791, acc: 57.03%] [G loss: 1.828663]\n",
      "epoch:10 step:10112 [D loss: 0.693091, acc: 62.50%] [G loss: 1.900416]\n",
      "epoch:10 step:10113 [D loss: 0.592818, acc: 66.41%] [G loss: 1.969184]\n",
      "epoch:10 step:10114 [D loss: 0.704879, acc: 60.16%] [G loss: 1.806784]\n",
      "epoch:10 step:10115 [D loss: 0.673182, acc: 62.50%] [G loss: 1.976524]\n",
      "epoch:10 step:10116 [D loss: 0.632369, acc: 64.84%] [G loss: 2.156109]\n",
      "epoch:10 step:10117 [D loss: 0.643460, acc: 63.28%] [G loss: 2.086466]\n",
      "epoch:10 step:10118 [D loss: 0.660588, acc: 60.94%] [G loss: 1.945033]\n",
      "epoch:10 step:10119 [D loss: 0.649503, acc: 59.38%] [G loss: 1.877730]\n",
      "epoch:10 step:10120 [D loss: 0.613200, acc: 69.53%] [G loss: 1.988300]\n",
      "epoch:10 step:10121 [D loss: 0.649579, acc: 64.06%] [G loss: 1.847101]\n",
      "epoch:10 step:10122 [D loss: 0.635711, acc: 64.06%] [G loss: 1.963123]\n",
      "epoch:10 step:10123 [D loss: 0.641862, acc: 62.50%] [G loss: 2.003599]\n",
      "epoch:10 step:10124 [D loss: 0.593767, acc: 71.88%] [G loss: 1.912030]\n",
      "epoch:10 step:10125 [D loss: 0.586545, acc: 71.09%] [G loss: 2.029820]\n",
      "epoch:10 step:10126 [D loss: 0.596662, acc: 66.41%] [G loss: 2.112943]\n",
      "epoch:10 step:10127 [D loss: 0.684672, acc: 58.59%] [G loss: 1.788979]\n",
      "epoch:10 step:10128 [D loss: 0.677721, acc: 61.72%] [G loss: 1.865802]\n",
      "epoch:10 step:10129 [D loss: 0.569699, acc: 70.31%] [G loss: 2.021163]\n",
      "epoch:10 step:10130 [D loss: 0.591790, acc: 67.97%] [G loss: 2.154500]\n",
      "epoch:10 step:10131 [D loss: 0.628631, acc: 65.62%] [G loss: 2.092139]\n",
      "epoch:10 step:10132 [D loss: 0.672547, acc: 58.59%] [G loss: 1.989057]\n",
      "epoch:10 step:10133 [D loss: 0.617142, acc: 64.84%] [G loss: 1.945045]\n",
      "epoch:10 step:10134 [D loss: 0.666672, acc: 60.16%] [G loss: 2.011028]\n",
      "epoch:10 step:10135 [D loss: 0.753253, acc: 53.12%] [G loss: 1.844377]\n",
      "epoch:10 step:10136 [D loss: 0.653994, acc: 65.62%] [G loss: 1.767488]\n",
      "epoch:10 step:10137 [D loss: 0.649348, acc: 63.28%] [G loss: 1.850003]\n",
      "epoch:10 step:10138 [D loss: 0.642822, acc: 65.62%] [G loss: 1.961549]\n",
      "epoch:10 step:10139 [D loss: 0.607954, acc: 68.75%] [G loss: 2.108156]\n",
      "epoch:10 step:10140 [D loss: 0.628747, acc: 64.84%] [G loss: 1.974236]\n",
      "epoch:10 step:10141 [D loss: 0.675963, acc: 61.72%] [G loss: 1.915216]\n",
      "epoch:10 step:10142 [D loss: 0.605888, acc: 68.75%] [G loss: 1.962678]\n",
      "epoch:10 step:10143 [D loss: 0.610077, acc: 63.28%] [G loss: 2.131384]\n",
      "epoch:10 step:10144 [D loss: 0.615938, acc: 63.28%] [G loss: 2.071909]\n",
      "epoch:10 step:10145 [D loss: 0.627833, acc: 66.41%] [G loss: 2.153555]\n",
      "epoch:10 step:10146 [D loss: 0.665332, acc: 58.59%] [G loss: 1.959289]\n",
      "epoch:10 step:10147 [D loss: 0.672936, acc: 58.59%] [G loss: 1.905068]\n",
      "epoch:10 step:10148 [D loss: 0.636473, acc: 66.41%] [G loss: 1.928333]\n",
      "epoch:10 step:10149 [D loss: 0.705159, acc: 56.25%] [G loss: 1.932671]\n",
      "epoch:10 step:10150 [D loss: 0.577865, acc: 70.31%] [G loss: 2.371746]\n",
      "epoch:10 step:10151 [D loss: 0.598065, acc: 67.97%] [G loss: 2.189440]\n",
      "epoch:10 step:10152 [D loss: 0.622960, acc: 64.84%] [G loss: 2.212712]\n",
      "epoch:10 step:10153 [D loss: 0.633046, acc: 62.50%] [G loss: 2.171447]\n",
      "epoch:10 step:10154 [D loss: 0.702141, acc: 55.47%] [G loss: 2.010037]\n",
      "epoch:10 step:10155 [D loss: 0.693030, acc: 57.81%] [G loss: 1.956205]\n",
      "epoch:10 step:10156 [D loss: 0.630901, acc: 66.41%] [G loss: 2.062617]\n",
      "epoch:10 step:10157 [D loss: 0.640261, acc: 64.84%] [G loss: 2.044359]\n",
      "epoch:10 step:10158 [D loss: 0.644235, acc: 65.62%] [G loss: 1.852273]\n",
      "epoch:10 step:10159 [D loss: 0.653742, acc: 61.72%] [G loss: 1.964688]\n",
      "epoch:10 step:10160 [D loss: 0.624567, acc: 67.97%] [G loss: 2.045122]\n",
      "epoch:10 step:10161 [D loss: 0.606901, acc: 66.41%] [G loss: 1.895466]\n",
      "epoch:10 step:10162 [D loss: 0.645935, acc: 58.59%] [G loss: 2.251835]\n",
      "epoch:10 step:10163 [D loss: 0.626489, acc: 62.50%] [G loss: 2.003674]\n",
      "epoch:10 step:10164 [D loss: 0.702960, acc: 57.03%] [G loss: 1.817373]\n",
      "epoch:10 step:10165 [D loss: 0.673450, acc: 57.81%] [G loss: 1.851020]\n",
      "epoch:10 step:10166 [D loss: 0.648959, acc: 59.38%] [G loss: 1.924984]\n",
      "epoch:10 step:10167 [D loss: 0.704973, acc: 55.47%] [G loss: 1.857329]\n",
      "epoch:10 step:10168 [D loss: 0.637048, acc: 62.50%] [G loss: 2.002202]\n",
      "epoch:10 step:10169 [D loss: 0.640984, acc: 64.06%] [G loss: 1.863276]\n",
      "epoch:10 step:10170 [D loss: 0.679408, acc: 60.16%] [G loss: 1.892578]\n",
      "epoch:10 step:10171 [D loss: 0.717994, acc: 55.47%] [G loss: 1.871479]\n",
      "epoch:10 step:10172 [D loss: 0.659561, acc: 64.06%] [G loss: 2.026677]\n",
      "epoch:10 step:10173 [D loss: 0.585095, acc: 71.88%] [G loss: 2.101307]\n",
      "epoch:10 step:10174 [D loss: 0.637705, acc: 64.84%] [G loss: 1.934833]\n",
      "epoch:10 step:10175 [D loss: 0.602410, acc: 67.19%] [G loss: 2.116178]\n",
      "epoch:10 step:10176 [D loss: 0.676643, acc: 60.16%] [G loss: 1.998818]\n",
      "epoch:10 step:10177 [D loss: 0.618754, acc: 65.62%] [G loss: 2.087948]\n",
      "epoch:10 step:10178 [D loss: 0.620021, acc: 64.06%] [G loss: 2.244228]\n",
      "epoch:10 step:10179 [D loss: 0.657872, acc: 60.94%] [G loss: 2.021286]\n",
      "epoch:10 step:10180 [D loss: 0.572260, acc: 71.88%] [G loss: 2.167241]\n",
      "epoch:10 step:10181 [D loss: 0.568863, acc: 74.22%] [G loss: 2.048142]\n",
      "epoch:10 step:10182 [D loss: 0.647402, acc: 63.28%] [G loss: 1.943136]\n",
      "epoch:10 step:10183 [D loss: 0.599234, acc: 73.44%] [G loss: 2.011225]\n",
      "epoch:10 step:10184 [D loss: 0.661723, acc: 57.81%] [G loss: 2.243292]\n",
      "epoch:10 step:10185 [D loss: 0.546759, acc: 69.53%] [G loss: 2.449608]\n",
      "epoch:10 step:10186 [D loss: 0.550071, acc: 70.31%] [G loss: 2.210854]\n",
      "epoch:10 step:10187 [D loss: 0.709555, acc: 57.81%] [G loss: 1.987725]\n",
      "epoch:10 step:10188 [D loss: 0.619552, acc: 64.84%] [G loss: 1.958043]\n",
      "epoch:10 step:10189 [D loss: 0.627090, acc: 61.72%] [G loss: 2.012593]\n",
      "epoch:10 step:10190 [D loss: 0.722365, acc: 53.12%] [G loss: 1.840719]\n",
      "epoch:10 step:10191 [D loss: 0.610329, acc: 64.06%] [G loss: 2.089614]\n",
      "epoch:10 step:10192 [D loss: 0.648924, acc: 68.75%] [G loss: 1.931248]\n",
      "epoch:10 step:10193 [D loss: 0.606837, acc: 65.62%] [G loss: 2.200144]\n",
      "epoch:10 step:10194 [D loss: 0.623367, acc: 67.97%] [G loss: 2.159157]\n",
      "epoch:10 step:10195 [D loss: 0.640780, acc: 61.72%] [G loss: 2.303906]\n",
      "epoch:10 step:10196 [D loss: 0.644936, acc: 64.84%] [G loss: 2.110470]\n",
      "epoch:10 step:10197 [D loss: 0.668068, acc: 64.06%] [G loss: 2.053083]\n",
      "epoch:10 step:10198 [D loss: 0.673327, acc: 55.47%] [G loss: 2.013028]\n",
      "epoch:10 step:10199 [D loss: 0.692402, acc: 57.81%] [G loss: 2.027268]\n",
      "epoch:10 step:10200 [D loss: 0.621662, acc: 65.62%] [G loss: 1.804209]\n",
      "##############\n",
      "[2.64274219 1.33012856 6.3903171  5.0391652  3.92994435 5.80021421\n",
      " 4.53200094 4.75603164 4.75090561 3.62776302]\n",
      "##########\n",
      "epoch:10 step:10201 [D loss: 0.618956, acc: 70.31%] [G loss: 2.103502]\n",
      "epoch:10 step:10202 [D loss: 0.692345, acc: 57.03%] [G loss: 1.912453]\n",
      "epoch:10 step:10203 [D loss: 0.599693, acc: 74.22%] [G loss: 2.024242]\n",
      "epoch:10 step:10204 [D loss: 0.590094, acc: 68.75%] [G loss: 2.078182]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:10 step:10205 [D loss: 0.631596, acc: 67.19%] [G loss: 1.823067]\n",
      "epoch:10 step:10206 [D loss: 0.671559, acc: 53.91%] [G loss: 1.892095]\n",
      "epoch:10 step:10207 [D loss: 0.633308, acc: 63.28%] [G loss: 1.991616]\n",
      "epoch:10 step:10208 [D loss: 0.635504, acc: 67.19%] [G loss: 1.917794]\n",
      "epoch:10 step:10209 [D loss: 0.604618, acc: 66.41%] [G loss: 2.066360]\n",
      "epoch:10 step:10210 [D loss: 0.711969, acc: 55.47%] [G loss: 2.058888]\n",
      "epoch:10 step:10211 [D loss: 0.626663, acc: 69.53%] [G loss: 1.948546]\n",
      "epoch:10 step:10212 [D loss: 0.586309, acc: 68.75%] [G loss: 2.161706]\n",
      "epoch:10 step:10213 [D loss: 0.563603, acc: 71.88%] [G loss: 2.058002]\n",
      "epoch:10 step:10214 [D loss: 0.632825, acc: 63.28%] [G loss: 1.898193]\n",
      "epoch:10 step:10215 [D loss: 0.636156, acc: 65.62%] [G loss: 2.031278]\n",
      "epoch:10 step:10216 [D loss: 0.622629, acc: 64.84%] [G loss: 1.911179]\n",
      "epoch:10 step:10217 [D loss: 0.578905, acc: 71.88%] [G loss: 2.075416]\n",
      "epoch:10 step:10218 [D loss: 0.581977, acc: 69.53%] [G loss: 2.099927]\n",
      "epoch:10 step:10219 [D loss: 0.561074, acc: 70.31%] [G loss: 2.158850]\n",
      "epoch:10 step:10220 [D loss: 0.663359, acc: 57.03%] [G loss: 2.027684]\n",
      "epoch:10 step:10221 [D loss: 0.613710, acc: 63.28%] [G loss: 2.051471]\n",
      "epoch:10 step:10222 [D loss: 0.678238, acc: 59.38%] [G loss: 1.915295]\n",
      "epoch:10 step:10223 [D loss: 0.682529, acc: 55.47%] [G loss: 1.919206]\n",
      "epoch:10 step:10224 [D loss: 0.623949, acc: 66.41%] [G loss: 1.962871]\n",
      "epoch:10 step:10225 [D loss: 0.644806, acc: 58.59%] [G loss: 2.026166]\n",
      "epoch:10 step:10226 [D loss: 0.687424, acc: 61.72%] [G loss: 2.035699]\n",
      "epoch:10 step:10227 [D loss: 0.638462, acc: 61.72%] [G loss: 2.114260]\n",
      "epoch:10 step:10228 [D loss: 0.722446, acc: 55.47%] [G loss: 1.990033]\n",
      "epoch:10 step:10229 [D loss: 0.716849, acc: 58.59%] [G loss: 1.971630]\n",
      "epoch:10 step:10230 [D loss: 0.618182, acc: 62.50%] [G loss: 2.128477]\n",
      "epoch:10 step:10231 [D loss: 0.667049, acc: 62.50%] [G loss: 1.963847]\n",
      "epoch:10 step:10232 [D loss: 0.672373, acc: 62.50%] [G loss: 1.938447]\n",
      "epoch:10 step:10233 [D loss: 0.642274, acc: 60.16%] [G loss: 2.116182]\n",
      "epoch:10 step:10234 [D loss: 0.672650, acc: 59.38%] [G loss: 1.987435]\n",
      "epoch:10 step:10235 [D loss: 0.604654, acc: 66.41%] [G loss: 1.955382]\n",
      "epoch:10 step:10236 [D loss: 0.617329, acc: 64.84%] [G loss: 1.932584]\n",
      "epoch:10 step:10237 [D loss: 0.611397, acc: 64.84%] [G loss: 1.965010]\n",
      "epoch:10 step:10238 [D loss: 0.589611, acc: 68.75%] [G loss: 2.012280]\n",
      "epoch:10 step:10239 [D loss: 0.642175, acc: 64.84%] [G loss: 2.006264]\n",
      "epoch:10 step:10240 [D loss: 0.604223, acc: 67.97%] [G loss: 1.998182]\n",
      "epoch:10 step:10241 [D loss: 0.655190, acc: 61.72%] [G loss: 1.867677]\n",
      "epoch:10 step:10242 [D loss: 0.638587, acc: 67.97%] [G loss: 1.813625]\n",
      "epoch:10 step:10243 [D loss: 0.632222, acc: 64.06%] [G loss: 2.051152]\n",
      "epoch:10 step:10244 [D loss: 0.661877, acc: 60.16%] [G loss: 1.918497]\n",
      "epoch:10 step:10245 [D loss: 0.575436, acc: 72.66%] [G loss: 2.164379]\n",
      "epoch:10 step:10246 [D loss: 0.683589, acc: 59.38%] [G loss: 2.149117]\n",
      "epoch:10 step:10247 [D loss: 0.632676, acc: 65.62%] [G loss: 2.103451]\n",
      "epoch:10 step:10248 [D loss: 0.618047, acc: 63.28%] [G loss: 1.983468]\n",
      "epoch:10 step:10249 [D loss: 0.678666, acc: 57.03%] [G loss: 1.926484]\n",
      "epoch:10 step:10250 [D loss: 0.665957, acc: 59.38%] [G loss: 1.788728]\n",
      "epoch:10 step:10251 [D loss: 0.627024, acc: 63.28%] [G loss: 2.073012]\n",
      "epoch:10 step:10252 [D loss: 0.694113, acc: 57.81%] [G loss: 1.941966]\n",
      "epoch:10 step:10253 [D loss: 0.655230, acc: 64.84%] [G loss: 1.956139]\n",
      "epoch:10 step:10254 [D loss: 0.632550, acc: 62.50%] [G loss: 2.073453]\n",
      "epoch:10 step:10255 [D loss: 0.614264, acc: 65.62%] [G loss: 1.995192]\n",
      "epoch:10 step:10256 [D loss: 0.628232, acc: 65.62%] [G loss: 2.201661]\n",
      "epoch:10 step:10257 [D loss: 0.634712, acc: 63.28%] [G loss: 2.068961]\n",
      "epoch:10 step:10258 [D loss: 0.641021, acc: 67.97%] [G loss: 1.904732]\n",
      "epoch:10 step:10259 [D loss: 0.663216, acc: 57.81%] [G loss: 2.093188]\n",
      "epoch:10 step:10260 [D loss: 0.660266, acc: 66.41%] [G loss: 2.106167]\n",
      "epoch:10 step:10261 [D loss: 0.634951, acc: 67.19%] [G loss: 2.024679]\n",
      "epoch:10 step:10262 [D loss: 0.690121, acc: 56.25%] [G loss: 1.920043]\n",
      "epoch:10 step:10263 [D loss: 0.607916, acc: 63.28%] [G loss: 1.978588]\n",
      "epoch:10 step:10264 [D loss: 0.629171, acc: 62.50%] [G loss: 2.048908]\n",
      "epoch:10 step:10265 [D loss: 0.626652, acc: 66.41%] [G loss: 2.108112]\n",
      "epoch:10 step:10266 [D loss: 0.659714, acc: 58.59%] [G loss: 2.074067]\n",
      "epoch:10 step:10267 [D loss: 0.595247, acc: 64.84%] [G loss: 2.185958]\n",
      "epoch:10 step:10268 [D loss: 0.631349, acc: 67.97%] [G loss: 1.950369]\n",
      "epoch:10 step:10269 [D loss: 0.602045, acc: 67.19%] [G loss: 2.162450]\n",
      "epoch:10 step:10270 [D loss: 0.651451, acc: 61.72%] [G loss: 2.147928]\n",
      "epoch:10 step:10271 [D loss: 0.637120, acc: 63.28%] [G loss: 2.025371]\n",
      "epoch:10 step:10272 [D loss: 0.629967, acc: 66.41%] [G loss: 2.062315]\n",
      "epoch:10 step:10273 [D loss: 0.633743, acc: 64.84%] [G loss: 2.127011]\n",
      "epoch:10 step:10274 [D loss: 0.631062, acc: 67.19%] [G loss: 1.908268]\n",
      "epoch:10 step:10275 [D loss: 0.612176, acc: 67.97%] [G loss: 2.009215]\n",
      "epoch:10 step:10276 [D loss: 0.629974, acc: 67.97%] [G loss: 2.100953]\n",
      "epoch:10 step:10277 [D loss: 0.661518, acc: 62.50%] [G loss: 2.086892]\n",
      "epoch:10 step:10278 [D loss: 0.563188, acc: 76.56%] [G loss: 2.034364]\n",
      "epoch:10 step:10279 [D loss: 0.578795, acc: 70.31%] [G loss: 2.332254]\n",
      "epoch:10 step:10280 [D loss: 0.649652, acc: 66.41%] [G loss: 2.125500]\n",
      "epoch:10 step:10281 [D loss: 0.614447, acc: 66.41%] [G loss: 2.294874]\n",
      "epoch:10 step:10282 [D loss: 0.569391, acc: 68.75%] [G loss: 2.629819]\n",
      "epoch:10 step:10283 [D loss: 0.687773, acc: 55.47%] [G loss: 1.949130]\n",
      "epoch:10 step:10284 [D loss: 0.580145, acc: 73.44%] [G loss: 1.979375]\n",
      "epoch:10 step:10285 [D loss: 0.654429, acc: 62.50%] [G loss: 2.178790]\n",
      "epoch:10 step:10286 [D loss: 0.691187, acc: 57.03%] [G loss: 2.057276]\n",
      "epoch:10 step:10287 [D loss: 0.583148, acc: 69.53%] [G loss: 2.155725]\n",
      "epoch:10 step:10288 [D loss: 0.613345, acc: 69.53%] [G loss: 2.049278]\n",
      "epoch:10 step:10289 [D loss: 0.584311, acc: 72.66%] [G loss: 2.607087]\n",
      "epoch:10 step:10290 [D loss: 0.753173, acc: 56.25%] [G loss: 2.039032]\n",
      "epoch:10 step:10291 [D loss: 0.675999, acc: 60.94%] [G loss: 2.231627]\n",
      "epoch:10 step:10292 [D loss: 0.670935, acc: 61.72%] [G loss: 2.111396]\n",
      "epoch:10 step:10293 [D loss: 0.560355, acc: 75.78%] [G loss: 2.234222]\n",
      "epoch:10 step:10294 [D loss: 0.564956, acc: 76.56%] [G loss: 2.408599]\n",
      "epoch:10 step:10295 [D loss: 0.618231, acc: 62.50%] [G loss: 2.132274]\n",
      "epoch:10 step:10296 [D loss: 0.655993, acc: 58.59%] [G loss: 2.262965]\n",
      "epoch:10 step:10297 [D loss: 0.594438, acc: 71.88%] [G loss: 2.269977]\n",
      "epoch:10 step:10298 [D loss: 0.814415, acc: 47.66%] [G loss: 1.976661]\n",
      "epoch:10 step:10299 [D loss: 0.618701, acc: 64.06%] [G loss: 2.179812]\n",
      "epoch:10 step:10300 [D loss: 0.595850, acc: 68.75%] [G loss: 2.266099]\n",
      "epoch:10 step:10301 [D loss: 0.635331, acc: 64.84%] [G loss: 2.236011]\n",
      "epoch:10 step:10302 [D loss: 0.629004, acc: 65.62%] [G loss: 1.996203]\n",
      "epoch:10 step:10303 [D loss: 0.536787, acc: 75.00%] [G loss: 2.181917]\n",
      "epoch:10 step:10304 [D loss: 0.671074, acc: 60.94%] [G loss: 2.291860]\n",
      "epoch:10 step:10305 [D loss: 0.551103, acc: 74.22%] [G loss: 2.282410]\n",
      "epoch:10 step:10306 [D loss: 0.646008, acc: 66.41%] [G loss: 2.307948]\n",
      "epoch:10 step:10307 [D loss: 0.495584, acc: 80.47%] [G loss: 2.850962]\n",
      "epoch:11 step:10308 [D loss: 0.666833, acc: 61.72%] [G loss: 2.105133]\n",
      "epoch:11 step:10309 [D loss: 0.679375, acc: 62.50%] [G loss: 1.952643]\n",
      "epoch:11 step:10310 [D loss: 0.646312, acc: 61.72%] [G loss: 2.080020]\n",
      "epoch:11 step:10311 [D loss: 0.659191, acc: 63.28%] [G loss: 2.082824]\n",
      "epoch:11 step:10312 [D loss: 0.642472, acc: 62.50%] [G loss: 2.046922]\n",
      "epoch:11 step:10313 [D loss: 0.652000, acc: 58.59%] [G loss: 2.053005]\n",
      "epoch:11 step:10314 [D loss: 0.592655, acc: 68.75%] [G loss: 2.216284]\n",
      "epoch:11 step:10315 [D loss: 0.628386, acc: 71.88%] [G loss: 2.284291]\n",
      "epoch:11 step:10316 [D loss: 0.613109, acc: 69.53%] [G loss: 2.209588]\n",
      "epoch:11 step:10317 [D loss: 0.607127, acc: 67.19%] [G loss: 2.251195]\n",
      "epoch:11 step:10318 [D loss: 0.632914, acc: 65.62%] [G loss: 2.090045]\n",
      "epoch:11 step:10319 [D loss: 0.629652, acc: 65.62%] [G loss: 2.095470]\n",
      "epoch:11 step:10320 [D loss: 0.600197, acc: 65.62%] [G loss: 2.216431]\n",
      "epoch:11 step:10321 [D loss: 0.623045, acc: 65.62%] [G loss: 2.013147]\n",
      "epoch:11 step:10322 [D loss: 0.583936, acc: 69.53%] [G loss: 2.231855]\n",
      "epoch:11 step:10323 [D loss: 0.642371, acc: 62.50%] [G loss: 2.261392]\n",
      "epoch:11 step:10324 [D loss: 0.611544, acc: 67.97%] [G loss: 1.996501]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:11 step:10325 [D loss: 0.643049, acc: 62.50%] [G loss: 2.132004]\n",
      "epoch:11 step:10326 [D loss: 0.621454, acc: 64.06%] [G loss: 2.055035]\n",
      "epoch:11 step:10327 [D loss: 0.720065, acc: 56.25%] [G loss: 1.817260]\n",
      "epoch:11 step:10328 [D loss: 0.743202, acc: 48.44%] [G loss: 2.020168]\n",
      "epoch:11 step:10329 [D loss: 0.593213, acc: 65.62%] [G loss: 1.965754]\n",
      "epoch:11 step:10330 [D loss: 0.599435, acc: 64.06%] [G loss: 2.142738]\n",
      "epoch:11 step:10331 [D loss: 0.650807, acc: 60.94%] [G loss: 2.214859]\n",
      "epoch:11 step:10332 [D loss: 0.585176, acc: 70.31%] [G loss: 2.145722]\n",
      "epoch:11 step:10333 [D loss: 0.608575, acc: 67.97%] [G loss: 1.993041]\n",
      "epoch:11 step:10334 [D loss: 0.666109, acc: 57.03%] [G loss: 1.878370]\n",
      "epoch:11 step:10335 [D loss: 0.617773, acc: 69.53%] [G loss: 1.975224]\n",
      "epoch:11 step:10336 [D loss: 0.657684, acc: 59.38%] [G loss: 2.072244]\n",
      "epoch:11 step:10337 [D loss: 0.671477, acc: 61.72%] [G loss: 1.894608]\n",
      "epoch:11 step:10338 [D loss: 0.691101, acc: 62.50%] [G loss: 1.840150]\n",
      "epoch:11 step:10339 [D loss: 0.647193, acc: 57.03%] [G loss: 1.917820]\n",
      "epoch:11 step:10340 [D loss: 0.632763, acc: 67.97%] [G loss: 1.889472]\n",
      "epoch:11 step:10341 [D loss: 0.600641, acc: 69.53%] [G loss: 2.097456]\n",
      "epoch:11 step:10342 [D loss: 0.666817, acc: 64.84%] [G loss: 1.910367]\n",
      "epoch:11 step:10343 [D loss: 0.592468, acc: 71.09%] [G loss: 2.291617]\n",
      "epoch:11 step:10344 [D loss: 0.632339, acc: 63.28%] [G loss: 2.107953]\n",
      "epoch:11 step:10345 [D loss: 0.658167, acc: 59.38%] [G loss: 2.075489]\n",
      "epoch:11 step:10346 [D loss: 0.643725, acc: 65.62%] [G loss: 2.268534]\n",
      "epoch:11 step:10347 [D loss: 0.583957, acc: 68.75%] [G loss: 2.286321]\n",
      "epoch:11 step:10348 [D loss: 0.631747, acc: 65.62%] [G loss: 2.010176]\n",
      "epoch:11 step:10349 [D loss: 0.593795, acc: 65.62%] [G loss: 2.180710]\n",
      "epoch:11 step:10350 [D loss: 0.597845, acc: 67.19%] [G loss: 1.960431]\n",
      "epoch:11 step:10351 [D loss: 0.611456, acc: 68.75%] [G loss: 1.896091]\n",
      "epoch:11 step:10352 [D loss: 0.636108, acc: 65.62%] [G loss: 2.107518]\n",
      "epoch:11 step:10353 [D loss: 0.670977, acc: 62.50%] [G loss: 1.908748]\n",
      "epoch:11 step:10354 [D loss: 0.565574, acc: 75.00%] [G loss: 2.155913]\n",
      "epoch:11 step:10355 [D loss: 0.572673, acc: 73.44%] [G loss: 2.054662]\n",
      "epoch:11 step:10356 [D loss: 0.625997, acc: 66.41%] [G loss: 2.109887]\n",
      "epoch:11 step:10357 [D loss: 0.603366, acc: 69.53%] [G loss: 2.075008]\n",
      "epoch:11 step:10358 [D loss: 0.672399, acc: 59.38%] [G loss: 2.044963]\n",
      "epoch:11 step:10359 [D loss: 0.698437, acc: 54.69%] [G loss: 1.997378]\n",
      "epoch:11 step:10360 [D loss: 0.661928, acc: 63.28%] [G loss: 2.138267]\n",
      "epoch:11 step:10361 [D loss: 0.582991, acc: 66.41%] [G loss: 2.278029]\n",
      "epoch:11 step:10362 [D loss: 0.664544, acc: 62.50%] [G loss: 2.067477]\n",
      "epoch:11 step:10363 [D loss: 0.610921, acc: 64.84%] [G loss: 2.039661]\n",
      "epoch:11 step:10364 [D loss: 0.642888, acc: 60.94%] [G loss: 2.057598]\n",
      "epoch:11 step:10365 [D loss: 0.593408, acc: 70.31%] [G loss: 2.009046]\n",
      "epoch:11 step:10366 [D loss: 0.586170, acc: 64.06%] [G loss: 2.113148]\n",
      "epoch:11 step:10367 [D loss: 0.625578, acc: 65.62%] [G loss: 2.066930]\n",
      "epoch:11 step:10368 [D loss: 0.653766, acc: 60.16%] [G loss: 1.905243]\n",
      "epoch:11 step:10369 [D loss: 0.625532, acc: 59.38%] [G loss: 2.056213]\n",
      "epoch:11 step:10370 [D loss: 0.700163, acc: 53.91%] [G loss: 2.074925]\n",
      "epoch:11 step:10371 [D loss: 0.599859, acc: 64.84%] [G loss: 2.180802]\n",
      "epoch:11 step:10372 [D loss: 0.677163, acc: 60.16%] [G loss: 2.148490]\n",
      "epoch:11 step:10373 [D loss: 0.576806, acc: 67.97%] [G loss: 2.116570]\n",
      "epoch:11 step:10374 [D loss: 0.653193, acc: 66.41%] [G loss: 1.997077]\n",
      "epoch:11 step:10375 [D loss: 0.606612, acc: 69.53%] [G loss: 2.085596]\n",
      "epoch:11 step:10376 [D loss: 0.575778, acc: 70.31%] [G loss: 2.106722]\n",
      "epoch:11 step:10377 [D loss: 0.639105, acc: 60.94%] [G loss: 2.124588]\n",
      "epoch:11 step:10378 [D loss: 0.659899, acc: 66.41%] [G loss: 1.972435]\n",
      "epoch:11 step:10379 [D loss: 0.614787, acc: 66.41%] [G loss: 2.112529]\n",
      "epoch:11 step:10380 [D loss: 0.611132, acc: 70.31%] [G loss: 2.042271]\n",
      "epoch:11 step:10381 [D loss: 0.644399, acc: 67.19%] [G loss: 2.050250]\n",
      "epoch:11 step:10382 [D loss: 0.622781, acc: 62.50%] [G loss: 2.168540]\n",
      "epoch:11 step:10383 [D loss: 0.632035, acc: 68.75%] [G loss: 2.242702]\n",
      "epoch:11 step:10384 [D loss: 0.519402, acc: 79.69%] [G loss: 2.388170]\n",
      "epoch:11 step:10385 [D loss: 0.654929, acc: 57.81%] [G loss: 2.063195]\n",
      "epoch:11 step:10386 [D loss: 0.627246, acc: 64.84%] [G loss: 2.020798]\n",
      "epoch:11 step:10387 [D loss: 0.636609, acc: 64.84%] [G loss: 1.869889]\n",
      "epoch:11 step:10388 [D loss: 0.688163, acc: 60.94%] [G loss: 2.001455]\n",
      "epoch:11 step:10389 [D loss: 0.645634, acc: 56.25%] [G loss: 2.012397]\n",
      "epoch:11 step:10390 [D loss: 0.629099, acc: 69.53%] [G loss: 2.088028]\n",
      "epoch:11 step:10391 [D loss: 0.623710, acc: 62.50%] [G loss: 1.944497]\n",
      "epoch:11 step:10392 [D loss: 0.647242, acc: 62.50%] [G loss: 1.895825]\n",
      "epoch:11 step:10393 [D loss: 0.644590, acc: 63.28%] [G loss: 1.808881]\n",
      "epoch:11 step:10394 [D loss: 0.602151, acc: 68.75%] [G loss: 1.916148]\n",
      "epoch:11 step:10395 [D loss: 0.619011, acc: 63.28%] [G loss: 2.149450]\n",
      "epoch:11 step:10396 [D loss: 0.609561, acc: 68.75%] [G loss: 2.214918]\n",
      "epoch:11 step:10397 [D loss: 0.623331, acc: 63.28%] [G loss: 2.028456]\n",
      "epoch:11 step:10398 [D loss: 0.644154, acc: 64.84%] [G loss: 2.068188]\n",
      "epoch:11 step:10399 [D loss: 0.631845, acc: 64.84%] [G loss: 2.172478]\n",
      "epoch:11 step:10400 [D loss: 0.585651, acc: 68.75%] [G loss: 2.144129]\n",
      "##############\n",
      "[2.37259148 1.29705118 6.5860816  4.77143644 3.68568458 5.54991978\n",
      " 4.33317618 4.7733721  4.68226164 3.50778279]\n",
      "##########\n",
      "epoch:11 step:10401 [D loss: 0.664442, acc: 63.28%] [G loss: 2.139728]\n",
      "epoch:11 step:10402 [D loss: 0.600154, acc: 73.44%] [G loss: 1.873301]\n",
      "epoch:11 step:10403 [D loss: 0.654848, acc: 58.59%] [G loss: 2.061139]\n",
      "epoch:11 step:10404 [D loss: 0.626638, acc: 64.84%] [G loss: 2.116106]\n",
      "epoch:11 step:10405 [D loss: 0.668388, acc: 66.41%] [G loss: 2.073323]\n",
      "epoch:11 step:10406 [D loss: 0.673447, acc: 57.81%] [G loss: 1.988110]\n",
      "epoch:11 step:10407 [D loss: 0.644276, acc: 63.28%] [G loss: 2.100127]\n",
      "epoch:11 step:10408 [D loss: 0.627493, acc: 66.41%] [G loss: 2.014700]\n",
      "epoch:11 step:10409 [D loss: 0.640058, acc: 62.50%] [G loss: 2.006187]\n",
      "epoch:11 step:10410 [D loss: 0.647540, acc: 61.72%] [G loss: 2.001702]\n",
      "epoch:11 step:10411 [D loss: 0.647515, acc: 61.72%] [G loss: 1.955143]\n",
      "epoch:11 step:10412 [D loss: 0.654495, acc: 60.94%] [G loss: 1.933059]\n",
      "epoch:11 step:10413 [D loss: 0.637284, acc: 60.94%] [G loss: 2.021593]\n",
      "epoch:11 step:10414 [D loss: 0.616131, acc: 61.72%] [G loss: 2.145358]\n",
      "epoch:11 step:10415 [D loss: 0.676234, acc: 60.16%] [G loss: 1.798909]\n",
      "epoch:11 step:10416 [D loss: 0.696107, acc: 60.16%] [G loss: 1.859145]\n",
      "epoch:11 step:10417 [D loss: 0.685095, acc: 60.16%] [G loss: 1.850322]\n",
      "epoch:11 step:10418 [D loss: 0.638153, acc: 64.06%] [G loss: 2.064317]\n",
      "epoch:11 step:10419 [D loss: 0.636860, acc: 64.84%] [G loss: 1.975410]\n",
      "epoch:11 step:10420 [D loss: 0.620996, acc: 65.62%] [G loss: 1.976733]\n",
      "epoch:11 step:10421 [D loss: 0.612679, acc: 66.41%] [G loss: 1.964288]\n",
      "epoch:11 step:10422 [D loss: 0.618670, acc: 64.84%] [G loss: 2.110317]\n",
      "epoch:11 step:10423 [D loss: 0.613601, acc: 64.84%] [G loss: 2.096522]\n",
      "epoch:11 step:10424 [D loss: 0.642419, acc: 60.94%] [G loss: 2.081597]\n",
      "epoch:11 step:10425 [D loss: 0.628754, acc: 67.97%] [G loss: 2.085732]\n",
      "epoch:11 step:10426 [D loss: 0.571735, acc: 68.75%] [G loss: 2.294617]\n",
      "epoch:11 step:10427 [D loss: 0.665779, acc: 58.59%] [G loss: 2.018031]\n",
      "epoch:11 step:10428 [D loss: 0.627814, acc: 64.84%] [G loss: 1.977318]\n",
      "epoch:11 step:10429 [D loss: 0.574459, acc: 64.84%] [G loss: 2.314112]\n",
      "epoch:11 step:10430 [D loss: 0.690031, acc: 60.16%] [G loss: 2.103301]\n",
      "epoch:11 step:10431 [D loss: 0.626369, acc: 66.41%] [G loss: 2.093245]\n",
      "epoch:11 step:10432 [D loss: 0.680920, acc: 57.81%] [G loss: 1.990927]\n",
      "epoch:11 step:10433 [D loss: 0.687128, acc: 57.03%] [G loss: 2.000882]\n",
      "epoch:11 step:10434 [D loss: 0.645525, acc: 61.72%] [G loss: 1.919474]\n",
      "epoch:11 step:10435 [D loss: 0.604908, acc: 68.75%] [G loss: 1.884139]\n",
      "epoch:11 step:10436 [D loss: 0.709757, acc: 57.03%] [G loss: 1.877609]\n",
      "epoch:11 step:10437 [D loss: 0.644624, acc: 65.62%] [G loss: 1.967182]\n",
      "epoch:11 step:10438 [D loss: 0.642403, acc: 61.72%] [G loss: 2.025738]\n",
      "epoch:11 step:10439 [D loss: 0.588428, acc: 67.19%] [G loss: 1.947959]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:11 step:10440 [D loss: 0.692454, acc: 52.34%] [G loss: 1.918800]\n",
      "epoch:11 step:10441 [D loss: 0.704054, acc: 53.12%] [G loss: 2.054263]\n",
      "epoch:11 step:10442 [D loss: 0.636636, acc: 64.84%] [G loss: 2.029968]\n",
      "epoch:11 step:10443 [D loss: 0.624935, acc: 66.41%] [G loss: 2.038550]\n",
      "epoch:11 step:10444 [D loss: 0.661540, acc: 60.16%] [G loss: 1.891906]\n",
      "epoch:11 step:10445 [D loss: 0.649069, acc: 65.62%] [G loss: 1.937847]\n",
      "epoch:11 step:10446 [D loss: 0.631795, acc: 61.72%] [G loss: 1.969832]\n",
      "epoch:11 step:10447 [D loss: 0.623524, acc: 67.97%] [G loss: 1.915128]\n",
      "epoch:11 step:10448 [D loss: 0.648739, acc: 63.28%] [G loss: 2.054263]\n",
      "epoch:11 step:10449 [D loss: 0.650519, acc: 63.28%] [G loss: 1.991643]\n",
      "epoch:11 step:10450 [D loss: 0.640424, acc: 65.62%] [G loss: 2.007782]\n",
      "epoch:11 step:10451 [D loss: 0.633987, acc: 64.06%] [G loss: 2.139387]\n",
      "epoch:11 step:10452 [D loss: 0.647150, acc: 70.31%] [G loss: 1.983393]\n",
      "epoch:11 step:10453 [D loss: 0.593824, acc: 71.09%] [G loss: 1.993639]\n",
      "epoch:11 step:10454 [D loss: 0.683825, acc: 57.03%] [G loss: 2.012831]\n",
      "epoch:11 step:10455 [D loss: 0.655966, acc: 60.94%] [G loss: 2.007335]\n",
      "epoch:11 step:10456 [D loss: 0.648941, acc: 61.72%] [G loss: 1.972412]\n",
      "epoch:11 step:10457 [D loss: 0.572030, acc: 75.78%] [G loss: 2.079859]\n",
      "epoch:11 step:10458 [D loss: 0.593691, acc: 70.31%] [G loss: 2.350844]\n",
      "epoch:11 step:10459 [D loss: 0.625599, acc: 62.50%] [G loss: 2.091131]\n",
      "epoch:11 step:10460 [D loss: 0.669050, acc: 61.72%] [G loss: 1.963811]\n",
      "epoch:11 step:10461 [D loss: 0.619692, acc: 70.31%] [G loss: 2.101796]\n",
      "epoch:11 step:10462 [D loss: 0.636126, acc: 64.06%] [G loss: 1.992543]\n",
      "epoch:11 step:10463 [D loss: 0.541076, acc: 77.34%] [G loss: 2.133347]\n",
      "epoch:11 step:10464 [D loss: 0.578893, acc: 66.41%] [G loss: 1.837388]\n",
      "epoch:11 step:10465 [D loss: 0.599822, acc: 67.97%] [G loss: 2.118261]\n",
      "epoch:11 step:10466 [D loss: 0.659986, acc: 64.06%] [G loss: 2.087481]\n",
      "epoch:11 step:10467 [D loss: 0.641048, acc: 64.84%] [G loss: 2.056314]\n",
      "epoch:11 step:10468 [D loss: 0.650260, acc: 64.84%] [G loss: 1.882157]\n",
      "epoch:11 step:10469 [D loss: 0.643589, acc: 65.62%] [G loss: 2.055223]\n",
      "epoch:11 step:10470 [D loss: 0.629090, acc: 63.28%] [G loss: 1.928264]\n",
      "epoch:11 step:10471 [D loss: 0.655109, acc: 63.28%] [G loss: 1.975041]\n",
      "epoch:11 step:10472 [D loss: 0.653724, acc: 60.94%] [G loss: 1.975105]\n",
      "epoch:11 step:10473 [D loss: 0.641372, acc: 66.41%] [G loss: 1.930117]\n",
      "epoch:11 step:10474 [D loss: 0.671420, acc: 62.50%] [G loss: 1.928210]\n",
      "epoch:11 step:10475 [D loss: 0.668974, acc: 65.62%] [G loss: 2.054196]\n",
      "epoch:11 step:10476 [D loss: 0.669200, acc: 54.69%] [G loss: 2.063241]\n",
      "epoch:11 step:10477 [D loss: 0.668718, acc: 63.28%] [G loss: 1.910259]\n",
      "epoch:11 step:10478 [D loss: 0.639373, acc: 64.84%] [G loss: 2.114977]\n",
      "epoch:11 step:10479 [D loss: 0.634519, acc: 60.94%] [G loss: 2.082200]\n",
      "epoch:11 step:10480 [D loss: 0.646626, acc: 63.28%] [G loss: 2.013098]\n",
      "epoch:11 step:10481 [D loss: 0.642904, acc: 64.06%] [G loss: 1.981103]\n",
      "epoch:11 step:10482 [D loss: 0.690282, acc: 58.59%] [G loss: 1.890470]\n",
      "epoch:11 step:10483 [D loss: 0.645370, acc: 56.25%] [G loss: 1.916839]\n",
      "epoch:11 step:10484 [D loss: 0.636770, acc: 63.28%] [G loss: 2.022277]\n",
      "epoch:11 step:10485 [D loss: 0.614702, acc: 67.97%] [G loss: 1.852627]\n",
      "epoch:11 step:10486 [D loss: 0.599707, acc: 68.75%] [G loss: 1.845283]\n",
      "epoch:11 step:10487 [D loss: 0.655595, acc: 64.06%] [G loss: 2.062690]\n",
      "epoch:11 step:10488 [D loss: 0.598697, acc: 68.75%] [G loss: 2.099124]\n",
      "epoch:11 step:10489 [D loss: 0.687659, acc: 59.38%] [G loss: 1.899193]\n",
      "epoch:11 step:10490 [D loss: 0.605846, acc: 65.62%] [G loss: 1.953046]\n",
      "epoch:11 step:10491 [D loss: 0.683574, acc: 60.94%] [G loss: 1.896372]\n",
      "epoch:11 step:10492 [D loss: 0.648758, acc: 66.41%] [G loss: 2.069491]\n",
      "epoch:11 step:10493 [D loss: 0.636624, acc: 61.72%] [G loss: 2.057697]\n",
      "epoch:11 step:10494 [D loss: 0.658625, acc: 57.03%] [G loss: 2.085586]\n",
      "epoch:11 step:10495 [D loss: 0.609325, acc: 66.41%] [G loss: 2.105914]\n",
      "epoch:11 step:10496 [D loss: 0.595675, acc: 70.31%] [G loss: 1.865179]\n",
      "epoch:11 step:10497 [D loss: 0.574561, acc: 67.19%] [G loss: 2.229427]\n",
      "epoch:11 step:10498 [D loss: 0.652095, acc: 66.41%] [G loss: 1.981923]\n",
      "epoch:11 step:10499 [D loss: 0.637814, acc: 64.84%] [G loss: 2.197040]\n",
      "epoch:11 step:10500 [D loss: 0.718661, acc: 52.34%] [G loss: 2.051453]\n",
      "epoch:11 step:10501 [D loss: 0.592506, acc: 66.41%] [G loss: 2.226255]\n",
      "epoch:11 step:10502 [D loss: 0.650211, acc: 60.16%] [G loss: 2.172843]\n",
      "epoch:11 step:10503 [D loss: 0.678922, acc: 60.94%] [G loss: 1.943246]\n",
      "epoch:11 step:10504 [D loss: 0.612633, acc: 65.62%] [G loss: 2.158824]\n",
      "epoch:11 step:10505 [D loss: 0.622940, acc: 61.72%] [G loss: 2.073479]\n",
      "epoch:11 step:10506 [D loss: 0.624358, acc: 61.72%] [G loss: 1.939530]\n",
      "epoch:11 step:10507 [D loss: 0.694338, acc: 61.72%] [G loss: 1.837672]\n",
      "epoch:11 step:10508 [D loss: 0.621793, acc: 69.53%] [G loss: 2.220277]\n",
      "epoch:11 step:10509 [D loss: 0.619205, acc: 64.06%] [G loss: 2.141020]\n",
      "epoch:11 step:10510 [D loss: 0.673528, acc: 56.25%] [G loss: 1.855140]\n",
      "epoch:11 step:10511 [D loss: 0.629055, acc: 64.06%] [G loss: 2.145054]\n",
      "epoch:11 step:10512 [D loss: 0.708529, acc: 51.56%] [G loss: 1.922313]\n",
      "epoch:11 step:10513 [D loss: 0.677653, acc: 64.06%] [G loss: 2.163720]\n",
      "epoch:11 step:10514 [D loss: 0.556340, acc: 71.88%] [G loss: 2.296948]\n",
      "epoch:11 step:10515 [D loss: 0.571990, acc: 71.88%] [G loss: 2.314985]\n",
      "epoch:11 step:10516 [D loss: 0.566924, acc: 68.75%] [G loss: 2.279374]\n",
      "epoch:11 step:10517 [D loss: 0.654225, acc: 60.16%] [G loss: 1.966525]\n",
      "epoch:11 step:10518 [D loss: 0.669251, acc: 60.16%] [G loss: 1.806169]\n",
      "epoch:11 step:10519 [D loss: 0.691408, acc: 56.25%] [G loss: 1.821628]\n",
      "epoch:11 step:10520 [D loss: 0.636991, acc: 69.53%] [G loss: 1.862735]\n",
      "epoch:11 step:10521 [D loss: 0.666017, acc: 58.59%] [G loss: 1.902958]\n",
      "epoch:11 step:10522 [D loss: 0.685894, acc: 54.69%] [G loss: 2.037246]\n",
      "epoch:11 step:10523 [D loss: 0.578226, acc: 67.19%] [G loss: 2.134951]\n",
      "epoch:11 step:10524 [D loss: 0.636763, acc: 66.41%] [G loss: 2.095810]\n",
      "epoch:11 step:10525 [D loss: 0.566614, acc: 67.97%] [G loss: 2.132244]\n",
      "epoch:11 step:10526 [D loss: 0.620978, acc: 65.62%] [G loss: 2.425601]\n",
      "epoch:11 step:10527 [D loss: 0.792101, acc: 53.91%] [G loss: 1.849447]\n",
      "epoch:11 step:10528 [D loss: 0.669596, acc: 57.03%] [G loss: 2.126440]\n",
      "epoch:11 step:10529 [D loss: 0.601442, acc: 68.75%] [G loss: 1.965526]\n",
      "epoch:11 step:10530 [D loss: 0.631408, acc: 67.97%] [G loss: 2.109203]\n",
      "epoch:11 step:10531 [D loss: 0.616940, acc: 66.41%] [G loss: 2.049612]\n",
      "epoch:11 step:10532 [D loss: 0.669490, acc: 60.16%] [G loss: 1.876247]\n",
      "epoch:11 step:10533 [D loss: 0.618580, acc: 65.62%] [G loss: 1.960836]\n",
      "epoch:11 step:10534 [D loss: 0.622901, acc: 70.31%] [G loss: 1.946926]\n",
      "epoch:11 step:10535 [D loss: 0.721512, acc: 51.56%] [G loss: 1.777456]\n",
      "epoch:11 step:10536 [D loss: 0.574131, acc: 71.09%] [G loss: 2.477494]\n",
      "epoch:11 step:10537 [D loss: 0.690824, acc: 60.16%] [G loss: 2.203394]\n",
      "epoch:11 step:10538 [D loss: 0.636121, acc: 64.06%] [G loss: 2.334751]\n",
      "epoch:11 step:10539 [D loss: 0.510159, acc: 79.69%] [G loss: 2.516524]\n",
      "epoch:11 step:10540 [D loss: 0.731251, acc: 52.34%] [G loss: 1.890475]\n",
      "epoch:11 step:10541 [D loss: 0.639274, acc: 62.50%] [G loss: 1.945675]\n",
      "epoch:11 step:10542 [D loss: 0.658460, acc: 60.94%] [G loss: 1.983736]\n",
      "epoch:11 step:10543 [D loss: 0.625483, acc: 64.06%] [G loss: 2.070755]\n",
      "epoch:11 step:10544 [D loss: 0.615154, acc: 65.62%] [G loss: 1.968253]\n",
      "epoch:11 step:10545 [D loss: 0.674028, acc: 62.50%] [G loss: 2.026147]\n",
      "epoch:11 step:10546 [D loss: 0.598437, acc: 72.66%] [G loss: 2.044416]\n",
      "epoch:11 step:10547 [D loss: 0.659711, acc: 64.84%] [G loss: 2.014384]\n",
      "epoch:11 step:10548 [D loss: 0.626401, acc: 67.19%] [G loss: 2.079855]\n",
      "epoch:11 step:10549 [D loss: 0.636709, acc: 65.62%] [G loss: 2.004054]\n",
      "epoch:11 step:10550 [D loss: 0.612377, acc: 70.31%] [G loss: 2.036823]\n",
      "epoch:11 step:10551 [D loss: 0.621027, acc: 62.50%] [G loss: 2.094777]\n",
      "epoch:11 step:10552 [D loss: 0.572448, acc: 73.44%] [G loss: 2.113099]\n",
      "epoch:11 step:10553 [D loss: 0.660278, acc: 60.94%] [G loss: 2.010731]\n",
      "epoch:11 step:10554 [D loss: 0.618767, acc: 66.41%] [G loss: 2.044357]\n",
      "epoch:11 step:10555 [D loss: 0.613566, acc: 67.97%] [G loss: 2.216201]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:11 step:10556 [D loss: 0.695319, acc: 57.03%] [G loss: 2.059736]\n",
      "epoch:11 step:10557 [D loss: 0.698066, acc: 57.81%] [G loss: 1.884291]\n",
      "epoch:11 step:10558 [D loss: 0.682649, acc: 59.38%] [G loss: 1.930339]\n",
      "epoch:11 step:10559 [D loss: 0.668611, acc: 57.81%] [G loss: 1.920591]\n",
      "epoch:11 step:10560 [D loss: 0.611961, acc: 65.62%] [G loss: 2.124664]\n",
      "epoch:11 step:10561 [D loss: 0.645326, acc: 64.84%] [G loss: 1.968718]\n",
      "epoch:11 step:10562 [D loss: 0.687115, acc: 60.16%] [G loss: 1.956019]\n",
      "epoch:11 step:10563 [D loss: 0.607455, acc: 67.19%] [G loss: 1.994160]\n",
      "epoch:11 step:10564 [D loss: 0.658540, acc: 57.03%] [G loss: 1.829781]\n",
      "epoch:11 step:10565 [D loss: 0.646767, acc: 61.72%] [G loss: 1.888039]\n",
      "epoch:11 step:10566 [D loss: 0.589973, acc: 70.31%] [G loss: 1.902914]\n",
      "epoch:11 step:10567 [D loss: 0.664759, acc: 62.50%] [G loss: 2.046207]\n",
      "epoch:11 step:10568 [D loss: 0.610168, acc: 68.75%] [G loss: 2.111173]\n",
      "epoch:11 step:10569 [D loss: 0.587565, acc: 67.19%] [G loss: 2.152488]\n",
      "epoch:11 step:10570 [D loss: 0.643006, acc: 64.06%] [G loss: 1.881996]\n",
      "epoch:11 step:10571 [D loss: 0.567985, acc: 73.44%] [G loss: 2.265602]\n",
      "epoch:11 step:10572 [D loss: 0.665061, acc: 63.28%] [G loss: 2.053356]\n",
      "epoch:11 step:10573 [D loss: 0.653085, acc: 60.94%] [G loss: 1.986259]\n",
      "epoch:11 step:10574 [D loss: 0.596385, acc: 68.75%] [G loss: 2.015928]\n",
      "epoch:11 step:10575 [D loss: 0.633560, acc: 64.06%] [G loss: 2.009808]\n",
      "epoch:11 step:10576 [D loss: 0.603002, acc: 67.19%] [G loss: 2.014411]\n",
      "epoch:11 step:10577 [D loss: 0.594900, acc: 71.09%] [G loss: 2.128364]\n",
      "epoch:11 step:10578 [D loss: 0.601806, acc: 69.53%] [G loss: 2.135197]\n",
      "epoch:11 step:10579 [D loss: 0.560646, acc: 72.66%] [G loss: 2.103635]\n",
      "epoch:11 step:10580 [D loss: 0.602694, acc: 65.62%] [G loss: 1.961885]\n",
      "epoch:11 step:10581 [D loss: 0.570983, acc: 71.09%] [G loss: 2.314615]\n",
      "epoch:11 step:10582 [D loss: 0.608653, acc: 70.31%] [G loss: 2.088790]\n",
      "epoch:11 step:10583 [D loss: 0.623249, acc: 64.06%] [G loss: 2.164397]\n",
      "epoch:11 step:10584 [D loss: 0.688232, acc: 55.47%] [G loss: 2.038126]\n",
      "epoch:11 step:10585 [D loss: 0.649285, acc: 64.06%] [G loss: 1.945380]\n",
      "epoch:11 step:10586 [D loss: 0.590505, acc: 70.31%] [G loss: 2.047292]\n",
      "epoch:11 step:10587 [D loss: 0.584073, acc: 73.44%] [G loss: 2.072565]\n",
      "epoch:11 step:10588 [D loss: 0.709725, acc: 50.78%] [G loss: 1.860561]\n",
      "epoch:11 step:10589 [D loss: 0.580125, acc: 70.31%] [G loss: 2.039269]\n",
      "epoch:11 step:10590 [D loss: 0.617453, acc: 64.06%] [G loss: 2.025225]\n",
      "epoch:11 step:10591 [D loss: 0.584938, acc: 68.75%] [G loss: 2.008583]\n",
      "epoch:11 step:10592 [D loss: 0.588909, acc: 67.19%] [G loss: 2.124961]\n",
      "epoch:11 step:10593 [D loss: 0.681125, acc: 57.03%] [G loss: 2.195420]\n",
      "epoch:11 step:10594 [D loss: 0.621976, acc: 66.41%] [G loss: 2.169069]\n",
      "epoch:11 step:10595 [D loss: 0.699280, acc: 58.59%] [G loss: 1.987167]\n",
      "epoch:11 step:10596 [D loss: 0.623145, acc: 67.97%] [G loss: 2.048413]\n",
      "epoch:11 step:10597 [D loss: 0.607767, acc: 64.84%] [G loss: 2.000353]\n",
      "epoch:11 step:10598 [D loss: 0.668953, acc: 59.38%] [G loss: 1.935780]\n",
      "epoch:11 step:10599 [D loss: 0.628765, acc: 67.19%] [G loss: 1.942485]\n",
      "epoch:11 step:10600 [D loss: 0.620559, acc: 62.50%] [G loss: 2.173198]\n",
      "##############\n",
      "[2.59626818 1.22587779 6.51735712 4.8744692  3.7347735  5.72150226\n",
      " 4.34244744 4.8603322  4.85865491 3.52267293]\n",
      "##########\n",
      "epoch:11 step:10601 [D loss: 0.629103, acc: 61.72%] [G loss: 2.120406]\n",
      "epoch:11 step:10602 [D loss: 0.665069, acc: 58.59%] [G loss: 1.972683]\n",
      "epoch:11 step:10603 [D loss: 0.620527, acc: 64.84%] [G loss: 2.082607]\n",
      "epoch:11 step:10604 [D loss: 0.600928, acc: 65.62%] [G loss: 2.005143]\n",
      "epoch:11 step:10605 [D loss: 0.625213, acc: 68.75%] [G loss: 2.280707]\n",
      "epoch:11 step:10606 [D loss: 0.612176, acc: 67.19%] [G loss: 2.144556]\n",
      "epoch:11 step:10607 [D loss: 0.588125, acc: 68.75%] [G loss: 2.196275]\n",
      "epoch:11 step:10608 [D loss: 0.703758, acc: 57.03%] [G loss: 1.874707]\n",
      "epoch:11 step:10609 [D loss: 0.643131, acc: 65.62%] [G loss: 2.015516]\n",
      "epoch:11 step:10610 [D loss: 0.636415, acc: 66.41%] [G loss: 2.104936]\n",
      "epoch:11 step:10611 [D loss: 0.620469, acc: 63.28%] [G loss: 1.924921]\n",
      "epoch:11 step:10612 [D loss: 0.607854, acc: 65.62%] [G loss: 1.952124]\n",
      "epoch:11 step:10613 [D loss: 0.632751, acc: 65.62%] [G loss: 1.968994]\n",
      "epoch:11 step:10614 [D loss: 0.717488, acc: 56.25%] [G loss: 1.869256]\n",
      "epoch:11 step:10615 [D loss: 0.653169, acc: 63.28%] [G loss: 1.988632]\n",
      "epoch:11 step:10616 [D loss: 0.600230, acc: 70.31%] [G loss: 1.975852]\n",
      "epoch:11 step:10617 [D loss: 0.603886, acc: 70.31%] [G loss: 2.010945]\n",
      "epoch:11 step:10618 [D loss: 0.624382, acc: 66.41%] [G loss: 2.036441]\n",
      "epoch:11 step:10619 [D loss: 0.627011, acc: 64.06%] [G loss: 2.460903]\n",
      "epoch:11 step:10620 [D loss: 0.567950, acc: 71.09%] [G loss: 2.309153]\n",
      "epoch:11 step:10621 [D loss: 0.526419, acc: 79.69%] [G loss: 2.435105]\n",
      "epoch:11 step:10622 [D loss: 0.580307, acc: 68.75%] [G loss: 2.454027]\n",
      "epoch:11 step:10623 [D loss: 0.719597, acc: 56.25%] [G loss: 1.855747]\n",
      "epoch:11 step:10624 [D loss: 0.670153, acc: 60.16%] [G loss: 1.964612]\n",
      "epoch:11 step:10625 [D loss: 0.679652, acc: 57.81%] [G loss: 2.117726]\n",
      "epoch:11 step:10626 [D loss: 0.605912, acc: 69.53%] [G loss: 1.945459]\n",
      "epoch:11 step:10627 [D loss: 0.607225, acc: 67.97%] [G loss: 2.075263]\n",
      "epoch:11 step:10628 [D loss: 0.634431, acc: 65.62%] [G loss: 2.174324]\n",
      "epoch:11 step:10629 [D loss: 0.593607, acc: 67.97%] [G loss: 2.054224]\n",
      "epoch:11 step:10630 [D loss: 0.719085, acc: 50.00%] [G loss: 1.920939]\n",
      "epoch:11 step:10631 [D loss: 0.638175, acc: 64.06%] [G loss: 1.981839]\n",
      "epoch:11 step:10632 [D loss: 0.616172, acc: 64.06%] [G loss: 1.972313]\n",
      "epoch:11 step:10633 [D loss: 0.644955, acc: 63.28%] [G loss: 1.912042]\n",
      "epoch:11 step:10634 [D loss: 0.647812, acc: 67.97%] [G loss: 2.024633]\n",
      "epoch:11 step:10635 [D loss: 0.672769, acc: 60.94%] [G loss: 1.889390]\n",
      "epoch:11 step:10636 [D loss: 0.616265, acc: 68.75%] [G loss: 1.979412]\n",
      "epoch:11 step:10637 [D loss: 0.599396, acc: 67.19%] [G loss: 2.159351]\n",
      "epoch:11 step:10638 [D loss: 0.635274, acc: 64.84%] [G loss: 2.116372]\n",
      "epoch:11 step:10639 [D loss: 0.578319, acc: 71.09%] [G loss: 2.020954]\n",
      "epoch:11 step:10640 [D loss: 0.602024, acc: 70.31%] [G loss: 2.093940]\n",
      "epoch:11 step:10641 [D loss: 0.637657, acc: 64.84%] [G loss: 2.022305]\n",
      "epoch:11 step:10642 [D loss: 0.648271, acc: 63.28%] [G loss: 2.039796]\n",
      "epoch:11 step:10643 [D loss: 0.632544, acc: 67.19%] [G loss: 2.027736]\n",
      "epoch:11 step:10644 [D loss: 0.676269, acc: 61.72%] [G loss: 2.104875]\n",
      "epoch:11 step:10645 [D loss: 0.645627, acc: 60.16%] [G loss: 2.086579]\n",
      "epoch:11 step:10646 [D loss: 0.635177, acc: 65.62%] [G loss: 2.106252]\n",
      "epoch:11 step:10647 [D loss: 0.599957, acc: 67.97%] [G loss: 1.989424]\n",
      "epoch:11 step:10648 [D loss: 0.692513, acc: 58.59%] [G loss: 1.844683]\n",
      "epoch:11 step:10649 [D loss: 0.638385, acc: 61.72%] [G loss: 2.048620]\n",
      "epoch:11 step:10650 [D loss: 0.667690, acc: 56.25%] [G loss: 2.047942]\n",
      "epoch:11 step:10651 [D loss: 0.650177, acc: 62.50%] [G loss: 2.042634]\n",
      "epoch:11 step:10652 [D loss: 0.634618, acc: 62.50%] [G loss: 2.179145]\n",
      "epoch:11 step:10653 [D loss: 0.616829, acc: 60.16%] [G loss: 2.295430]\n",
      "epoch:11 step:10654 [D loss: 0.606821, acc: 63.28%] [G loss: 2.446303]\n",
      "epoch:11 step:10655 [D loss: 0.651108, acc: 64.06%] [G loss: 1.959032]\n",
      "epoch:11 step:10656 [D loss: 0.668393, acc: 57.81%] [G loss: 1.758153]\n",
      "epoch:11 step:10657 [D loss: 0.620819, acc: 67.97%] [G loss: 2.034236]\n",
      "epoch:11 step:10658 [D loss: 0.613863, acc: 67.19%] [G loss: 2.142089]\n",
      "epoch:11 step:10659 [D loss: 0.698369, acc: 55.47%] [G loss: 1.991090]\n",
      "epoch:11 step:10660 [D loss: 0.599607, acc: 65.62%] [G loss: 2.274319]\n",
      "epoch:11 step:10661 [D loss: 0.600006, acc: 64.84%] [G loss: 1.981335]\n",
      "epoch:11 step:10662 [D loss: 0.686487, acc: 61.72%] [G loss: 1.997429]\n",
      "epoch:11 step:10663 [D loss: 0.681739, acc: 62.50%] [G loss: 1.947346]\n",
      "epoch:11 step:10664 [D loss: 0.652015, acc: 61.72%] [G loss: 2.073720]\n",
      "epoch:11 step:10665 [D loss: 0.643380, acc: 64.06%] [G loss: 2.065838]\n",
      "epoch:11 step:10666 [D loss: 0.595849, acc: 66.41%] [G loss: 2.127176]\n",
      "epoch:11 step:10667 [D loss: 0.684336, acc: 59.38%] [G loss: 2.017452]\n",
      "epoch:11 step:10668 [D loss: 0.616042, acc: 60.94%] [G loss: 1.944994]\n",
      "epoch:11 step:10669 [D loss: 0.660153, acc: 62.50%] [G loss: 1.822600]\n",
      "epoch:11 step:10670 [D loss: 0.628797, acc: 69.53%] [G loss: 1.901783]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:11 step:10671 [D loss: 0.563426, acc: 72.66%] [G loss: 1.997600]\n",
      "epoch:11 step:10672 [D loss: 0.593097, acc: 69.53%] [G loss: 2.047967]\n",
      "epoch:11 step:10673 [D loss: 0.602967, acc: 64.06%] [G loss: 2.153068]\n",
      "epoch:11 step:10674 [D loss: 0.595738, acc: 67.19%] [G loss: 2.143851]\n",
      "epoch:11 step:10675 [D loss: 0.685871, acc: 57.81%] [G loss: 1.936091]\n",
      "epoch:11 step:10676 [D loss: 0.653989, acc: 57.81%] [G loss: 2.129322]\n",
      "epoch:11 step:10677 [D loss: 0.638122, acc: 63.28%] [G loss: 1.998582]\n",
      "epoch:11 step:10678 [D loss: 0.618681, acc: 65.62%] [G loss: 2.003728]\n",
      "epoch:11 step:10679 [D loss: 0.621522, acc: 66.41%] [G loss: 1.897774]\n",
      "epoch:11 step:10680 [D loss: 0.665814, acc: 57.81%] [G loss: 1.919996]\n",
      "epoch:11 step:10681 [D loss: 0.644254, acc: 58.59%] [G loss: 2.103017]\n",
      "epoch:11 step:10682 [D loss: 0.653486, acc: 64.06%] [G loss: 2.047618]\n",
      "epoch:11 step:10683 [D loss: 0.687735, acc: 63.28%] [G loss: 1.916254]\n",
      "epoch:11 step:10684 [D loss: 0.680866, acc: 61.72%] [G loss: 1.938795]\n",
      "epoch:11 step:10685 [D loss: 0.629086, acc: 66.41%] [G loss: 1.999966]\n",
      "epoch:11 step:10686 [D loss: 0.610794, acc: 71.09%] [G loss: 2.019958]\n",
      "epoch:11 step:10687 [D loss: 0.638593, acc: 60.94%] [G loss: 2.069975]\n",
      "epoch:11 step:10688 [D loss: 0.588990, acc: 70.31%] [G loss: 1.992259]\n",
      "epoch:11 step:10689 [D loss: 0.620325, acc: 60.16%] [G loss: 1.872616]\n",
      "epoch:11 step:10690 [D loss: 0.667118, acc: 57.03%] [G loss: 1.973489]\n",
      "epoch:11 step:10691 [D loss: 0.616344, acc: 63.28%] [G loss: 2.092213]\n",
      "epoch:11 step:10692 [D loss: 0.632623, acc: 64.06%] [G loss: 2.152442]\n",
      "epoch:11 step:10693 [D loss: 0.661990, acc: 55.47%] [G loss: 1.882665]\n",
      "epoch:11 step:10694 [D loss: 0.647709, acc: 60.94%] [G loss: 1.904357]\n",
      "epoch:11 step:10695 [D loss: 0.635825, acc: 59.38%] [G loss: 1.976997]\n",
      "epoch:11 step:10696 [D loss: 0.645113, acc: 64.84%] [G loss: 1.800259]\n",
      "epoch:11 step:10697 [D loss: 0.698789, acc: 55.47%] [G loss: 2.004936]\n",
      "epoch:11 step:10698 [D loss: 0.656475, acc: 64.84%] [G loss: 1.853669]\n",
      "epoch:11 step:10699 [D loss: 0.622155, acc: 64.84%] [G loss: 2.063408]\n",
      "epoch:11 step:10700 [D loss: 0.627025, acc: 60.16%] [G loss: 1.952090]\n",
      "epoch:11 step:10701 [D loss: 0.669985, acc: 57.03%] [G loss: 1.967955]\n",
      "epoch:11 step:10702 [D loss: 0.619807, acc: 61.72%] [G loss: 1.989243]\n",
      "epoch:11 step:10703 [D loss: 0.705043, acc: 53.91%] [G loss: 1.964464]\n",
      "epoch:11 step:10704 [D loss: 0.576730, acc: 78.12%] [G loss: 2.018565]\n",
      "epoch:11 step:10705 [D loss: 0.640078, acc: 62.50%] [G loss: 2.129442]\n",
      "epoch:11 step:10706 [D loss: 0.640290, acc: 59.38%] [G loss: 2.008553]\n",
      "epoch:11 step:10707 [D loss: 0.660197, acc: 61.72%] [G loss: 1.906007]\n",
      "epoch:11 step:10708 [D loss: 0.658955, acc: 67.19%] [G loss: 1.932746]\n",
      "epoch:11 step:10709 [D loss: 0.578094, acc: 72.66%] [G loss: 2.031489]\n",
      "epoch:11 step:10710 [D loss: 0.652407, acc: 60.16%] [G loss: 1.988593]\n",
      "epoch:11 step:10711 [D loss: 0.659295, acc: 61.72%] [G loss: 1.994659]\n",
      "epoch:11 step:10712 [D loss: 0.605807, acc: 65.62%] [G loss: 2.100493]\n",
      "epoch:11 step:10713 [D loss: 0.599823, acc: 71.09%] [G loss: 2.207807]\n",
      "epoch:11 step:10714 [D loss: 0.665187, acc: 58.59%] [G loss: 1.869813]\n",
      "epoch:11 step:10715 [D loss: 0.667396, acc: 57.81%] [G loss: 2.068369]\n",
      "epoch:11 step:10716 [D loss: 0.666294, acc: 61.72%] [G loss: 1.997002]\n",
      "epoch:11 step:10717 [D loss: 0.642305, acc: 60.16%] [G loss: 2.076651]\n",
      "epoch:11 step:10718 [D loss: 0.602125, acc: 70.31%] [G loss: 2.091649]\n",
      "epoch:11 step:10719 [D loss: 0.615968, acc: 70.31%] [G loss: 1.911426]\n",
      "epoch:11 step:10720 [D loss: 0.661021, acc: 64.84%] [G loss: 2.084531]\n",
      "epoch:11 step:10721 [D loss: 0.629682, acc: 64.84%] [G loss: 2.019040]\n",
      "epoch:11 step:10722 [D loss: 0.648113, acc: 65.62%] [G loss: 1.990672]\n",
      "epoch:11 step:10723 [D loss: 0.665285, acc: 65.62%] [G loss: 2.111099]\n",
      "epoch:11 step:10724 [D loss: 0.644035, acc: 60.94%] [G loss: 2.098340]\n",
      "epoch:11 step:10725 [D loss: 0.651906, acc: 63.28%] [G loss: 1.869808]\n",
      "epoch:11 step:10726 [D loss: 0.620571, acc: 66.41%] [G loss: 1.991020]\n",
      "epoch:11 step:10727 [D loss: 0.695439, acc: 58.59%] [G loss: 1.817876]\n",
      "epoch:11 step:10728 [D loss: 0.655489, acc: 64.06%] [G loss: 1.916356]\n",
      "epoch:11 step:10729 [D loss: 0.639667, acc: 62.50%] [G loss: 1.976306]\n",
      "epoch:11 step:10730 [D loss: 0.613595, acc: 69.53%] [G loss: 2.025731]\n",
      "epoch:11 step:10731 [D loss: 0.679351, acc: 57.03%] [G loss: 2.038798]\n",
      "epoch:11 step:10732 [D loss: 0.635136, acc: 64.84%] [G loss: 1.969767]\n",
      "epoch:11 step:10733 [D loss: 0.623798, acc: 67.19%] [G loss: 1.936308]\n",
      "epoch:11 step:10734 [D loss: 0.602843, acc: 69.53%] [G loss: 2.210621]\n",
      "epoch:11 step:10735 [D loss: 0.579713, acc: 66.41%] [G loss: 2.188545]\n",
      "epoch:11 step:10736 [D loss: 0.585026, acc: 69.53%] [G loss: 2.283493]\n",
      "epoch:11 step:10737 [D loss: 0.604082, acc: 69.53%] [G loss: 2.195125]\n",
      "epoch:11 step:10738 [D loss: 0.648252, acc: 62.50%] [G loss: 1.922567]\n",
      "epoch:11 step:10739 [D loss: 0.623158, acc: 67.19%] [G loss: 2.024847]\n",
      "epoch:11 step:10740 [D loss: 0.646853, acc: 67.97%] [G loss: 2.095952]\n",
      "epoch:11 step:10741 [D loss: 0.622101, acc: 66.41%] [G loss: 2.052944]\n",
      "epoch:11 step:10742 [D loss: 0.650242, acc: 61.72%] [G loss: 2.019763]\n",
      "epoch:11 step:10743 [D loss: 0.620528, acc: 64.06%] [G loss: 2.099012]\n",
      "epoch:11 step:10744 [D loss: 0.720067, acc: 54.69%] [G loss: 1.810604]\n",
      "epoch:11 step:10745 [D loss: 0.656993, acc: 60.94%] [G loss: 1.959519]\n",
      "epoch:11 step:10746 [D loss: 0.617527, acc: 67.97%] [G loss: 1.931448]\n",
      "epoch:11 step:10747 [D loss: 0.695977, acc: 61.72%] [G loss: 2.072276]\n",
      "epoch:11 step:10748 [D loss: 0.680275, acc: 56.25%] [G loss: 1.894621]\n",
      "epoch:11 step:10749 [D loss: 0.681020, acc: 57.03%] [G loss: 1.810642]\n",
      "epoch:11 step:10750 [D loss: 0.657236, acc: 63.28%] [G loss: 1.911098]\n",
      "epoch:11 step:10751 [D loss: 0.695552, acc: 56.25%] [G loss: 1.873626]\n",
      "epoch:11 step:10752 [D loss: 0.629804, acc: 64.06%] [G loss: 2.091419]\n",
      "epoch:11 step:10753 [D loss: 0.628722, acc: 67.19%] [G loss: 1.960032]\n",
      "epoch:11 step:10754 [D loss: 0.615487, acc: 64.06%] [G loss: 2.067295]\n",
      "epoch:11 step:10755 [D loss: 0.673661, acc: 60.16%] [G loss: 1.964962]\n",
      "epoch:11 step:10756 [D loss: 0.616480, acc: 67.97%] [G loss: 1.944834]\n",
      "epoch:11 step:10757 [D loss: 0.648965, acc: 62.50%] [G loss: 2.016075]\n",
      "epoch:11 step:10758 [D loss: 0.626443, acc: 63.28%] [G loss: 2.029400]\n",
      "epoch:11 step:10759 [D loss: 0.612200, acc: 64.06%] [G loss: 2.091436]\n",
      "epoch:11 step:10760 [D loss: 0.617930, acc: 58.59%] [G loss: 2.034178]\n",
      "epoch:11 step:10761 [D loss: 0.680219, acc: 58.59%] [G loss: 2.057358]\n",
      "epoch:11 step:10762 [D loss: 0.670086, acc: 62.50%] [G loss: 1.863238]\n",
      "epoch:11 step:10763 [D loss: 0.601914, acc: 62.50%] [G loss: 2.080405]\n",
      "epoch:11 step:10764 [D loss: 0.594424, acc: 67.97%] [G loss: 2.069082]\n",
      "epoch:11 step:10765 [D loss: 0.642136, acc: 66.41%] [G loss: 1.901216]\n",
      "epoch:11 step:10766 [D loss: 0.679028, acc: 54.69%] [G loss: 1.962722]\n",
      "epoch:11 step:10767 [D loss: 0.681164, acc: 57.03%] [G loss: 1.966917]\n",
      "epoch:11 step:10768 [D loss: 0.634439, acc: 60.94%] [G loss: 1.959825]\n",
      "epoch:11 step:10769 [D loss: 0.646490, acc: 65.62%] [G loss: 2.016170]\n",
      "epoch:11 step:10770 [D loss: 0.706548, acc: 56.25%] [G loss: 1.838963]\n",
      "epoch:11 step:10771 [D loss: 0.614263, acc: 66.41%] [G loss: 1.894612]\n",
      "epoch:11 step:10772 [D loss: 0.657484, acc: 63.28%] [G loss: 1.913368]\n",
      "epoch:11 step:10773 [D loss: 0.648311, acc: 67.19%] [G loss: 2.048171]\n",
      "epoch:11 step:10774 [D loss: 0.638035, acc: 66.41%] [G loss: 1.935114]\n",
      "epoch:11 step:10775 [D loss: 0.606084, acc: 64.84%] [G loss: 2.020976]\n",
      "epoch:11 step:10776 [D loss: 0.604712, acc: 61.72%] [G loss: 2.211914]\n",
      "epoch:11 step:10777 [D loss: 0.654039, acc: 65.62%] [G loss: 2.228410]\n",
      "epoch:11 step:10778 [D loss: 0.598834, acc: 67.97%] [G loss: 2.310997]\n",
      "epoch:11 step:10779 [D loss: 0.594519, acc: 72.66%] [G loss: 2.129854]\n",
      "epoch:11 step:10780 [D loss: 0.719432, acc: 54.69%] [G loss: 1.692058]\n",
      "epoch:11 step:10781 [D loss: 0.654384, acc: 60.16%] [G loss: 2.126930]\n",
      "epoch:11 step:10782 [D loss: 0.594969, acc: 71.88%] [G loss: 2.146657]\n",
      "epoch:11 step:10783 [D loss: 0.653953, acc: 62.50%] [G loss: 2.129153]\n",
      "epoch:11 step:10784 [D loss: 0.707471, acc: 55.47%] [G loss: 1.772054]\n",
      "epoch:11 step:10785 [D loss: 0.628265, acc: 62.50%] [G loss: 1.894710]\n",
      "epoch:11 step:10786 [D loss: 0.636114, acc: 62.50%] [G loss: 2.052659]\n",
      "epoch:11 step:10787 [D loss: 0.646564, acc: 67.19%] [G loss: 1.873421]\n",
      "epoch:11 step:10788 [D loss: 0.559609, acc: 75.00%] [G loss: 2.125246]\n",
      "epoch:11 step:10789 [D loss: 0.702830, acc: 57.03%] [G loss: 1.887406]\n",
      "epoch:11 step:10790 [D loss: 0.678963, acc: 60.16%] [G loss: 1.902859]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:11 step:10791 [D loss: 0.622391, acc: 64.06%] [G loss: 1.897746]\n",
      "epoch:11 step:10792 [D loss: 0.687652, acc: 61.72%] [G loss: 1.802552]\n",
      "epoch:11 step:10793 [D loss: 0.643634, acc: 60.16%] [G loss: 1.937624]\n",
      "epoch:11 step:10794 [D loss: 0.666544, acc: 63.28%] [G loss: 1.839713]\n",
      "epoch:11 step:10795 [D loss: 0.622278, acc: 64.06%] [G loss: 2.077904]\n",
      "epoch:11 step:10796 [D loss: 0.633167, acc: 67.19%] [G loss: 1.974482]\n",
      "epoch:11 step:10797 [D loss: 0.601494, acc: 68.75%] [G loss: 2.019922]\n",
      "epoch:11 step:10798 [D loss: 0.615722, acc: 66.41%] [G loss: 2.109352]\n",
      "epoch:11 step:10799 [D loss: 0.657775, acc: 62.50%] [G loss: 1.991332]\n",
      "epoch:11 step:10800 [D loss: 0.651965, acc: 59.38%] [G loss: 1.860116]\n",
      "##############\n",
      "[2.38339864 1.29138937 6.20644006 4.95497951 3.71554633 5.73459682\n",
      " 4.42263556 4.57940729 4.67049601 3.6100059 ]\n",
      "##########\n",
      "epoch:11 step:10801 [D loss: 0.640431, acc: 64.06%] [G loss: 2.121612]\n",
      "epoch:11 step:10802 [D loss: 0.633925, acc: 63.28%] [G loss: 2.168243]\n",
      "epoch:11 step:10803 [D loss: 0.623956, acc: 64.06%] [G loss: 1.841314]\n",
      "epoch:11 step:10804 [D loss: 0.580154, acc: 73.44%] [G loss: 2.214491]\n",
      "epoch:11 step:10805 [D loss: 0.608190, acc: 73.44%] [G loss: 2.272946]\n",
      "epoch:11 step:10806 [D loss: 0.557059, acc: 75.78%] [G loss: 2.229713]\n",
      "epoch:11 step:10807 [D loss: 0.726540, acc: 53.91%] [G loss: 1.835654]\n",
      "epoch:11 step:10808 [D loss: 0.752051, acc: 53.91%] [G loss: 1.715547]\n",
      "epoch:11 step:10809 [D loss: 0.700056, acc: 56.25%] [G loss: 1.842652]\n",
      "epoch:11 step:10810 [D loss: 0.646862, acc: 57.03%] [G loss: 2.128268]\n",
      "epoch:11 step:10811 [D loss: 0.542426, acc: 73.44%] [G loss: 2.157846]\n",
      "epoch:11 step:10812 [D loss: 0.619776, acc: 71.09%] [G loss: 2.037723]\n",
      "epoch:11 step:10813 [D loss: 0.665216, acc: 61.72%] [G loss: 1.839545]\n",
      "epoch:11 step:10814 [D loss: 0.691865, acc: 56.25%] [G loss: 1.842848]\n",
      "epoch:11 step:10815 [D loss: 0.581245, acc: 68.75%] [G loss: 2.038723]\n",
      "epoch:11 step:10816 [D loss: 0.697642, acc: 57.03%] [G loss: 2.001811]\n",
      "epoch:11 step:10817 [D loss: 0.660184, acc: 63.28%] [G loss: 1.791445]\n",
      "epoch:11 step:10818 [D loss: 0.706043, acc: 59.38%] [G loss: 1.796369]\n",
      "epoch:11 step:10819 [D loss: 0.625266, acc: 59.38%] [G loss: 1.895006]\n",
      "epoch:11 step:10820 [D loss: 0.624426, acc: 65.62%] [G loss: 1.836798]\n",
      "epoch:11 step:10821 [D loss: 0.658315, acc: 57.81%] [G loss: 1.956669]\n",
      "epoch:11 step:10822 [D loss: 0.595276, acc: 67.97%] [G loss: 1.931317]\n",
      "epoch:11 step:10823 [D loss: 0.644026, acc: 65.62%] [G loss: 2.039460]\n",
      "epoch:11 step:10824 [D loss: 0.590024, acc: 66.41%] [G loss: 1.899884]\n",
      "epoch:11 step:10825 [D loss: 0.627578, acc: 66.41%] [G loss: 1.870714]\n",
      "epoch:11 step:10826 [D loss: 0.598478, acc: 68.75%] [G loss: 2.030452]\n",
      "epoch:11 step:10827 [D loss: 0.570404, acc: 69.53%] [G loss: 2.151345]\n",
      "epoch:11 step:10828 [D loss: 0.689802, acc: 57.81%] [G loss: 1.936618]\n",
      "epoch:11 step:10829 [D loss: 0.602021, acc: 66.41%] [G loss: 2.056340]\n",
      "epoch:11 step:10830 [D loss: 0.595069, acc: 73.44%] [G loss: 2.220613]\n",
      "epoch:11 step:10831 [D loss: 0.604700, acc: 71.09%] [G loss: 1.976228]\n",
      "epoch:11 step:10832 [D loss: 0.645060, acc: 56.25%] [G loss: 2.130828]\n",
      "epoch:11 step:10833 [D loss: 0.643603, acc: 58.59%] [G loss: 1.989122]\n",
      "epoch:11 step:10834 [D loss: 0.606500, acc: 65.62%] [G loss: 1.934530]\n",
      "epoch:11 step:10835 [D loss: 0.662993, acc: 62.50%] [G loss: 1.827405]\n",
      "epoch:11 step:10836 [D loss: 0.676243, acc: 64.84%] [G loss: 1.898694]\n",
      "epoch:11 step:10837 [D loss: 0.626353, acc: 66.41%] [G loss: 1.813104]\n",
      "epoch:11 step:10838 [D loss: 0.672051, acc: 59.38%] [G loss: 1.821583]\n",
      "epoch:11 step:10839 [D loss: 0.614655, acc: 67.19%] [G loss: 2.210512]\n",
      "epoch:11 step:10840 [D loss: 0.624158, acc: 67.97%] [G loss: 2.028031]\n",
      "epoch:11 step:10841 [D loss: 0.615400, acc: 71.88%] [G loss: 2.084275]\n",
      "epoch:11 step:10842 [D loss: 0.660275, acc: 67.19%] [G loss: 1.994999]\n",
      "epoch:11 step:10843 [D loss: 0.608283, acc: 67.19%] [G loss: 1.995370]\n",
      "epoch:11 step:10844 [D loss: 0.651224, acc: 68.75%] [G loss: 1.902614]\n",
      "epoch:11 step:10845 [D loss: 0.670572, acc: 59.38%] [G loss: 2.002572]\n",
      "epoch:11 step:10846 [D loss: 0.676731, acc: 59.38%] [G loss: 1.956904]\n",
      "epoch:11 step:10847 [D loss: 0.653507, acc: 62.50%] [G loss: 2.073658]\n",
      "epoch:11 step:10848 [D loss: 0.732767, acc: 55.47%] [G loss: 1.774911]\n",
      "epoch:11 step:10849 [D loss: 0.663206, acc: 57.81%] [G loss: 1.750925]\n",
      "epoch:11 step:10850 [D loss: 0.642380, acc: 63.28%] [G loss: 1.906566]\n",
      "epoch:11 step:10851 [D loss: 0.670405, acc: 60.94%] [G loss: 1.902221]\n",
      "epoch:11 step:10852 [D loss: 0.647065, acc: 62.50%] [G loss: 2.002527]\n",
      "epoch:11 step:10853 [D loss: 0.629810, acc: 65.62%] [G loss: 2.084504]\n",
      "epoch:11 step:10854 [D loss: 0.667147, acc: 65.62%] [G loss: 1.936965]\n",
      "epoch:11 step:10855 [D loss: 0.607121, acc: 69.53%] [G loss: 1.952211]\n",
      "epoch:11 step:10856 [D loss: 0.600199, acc: 65.62%] [G loss: 2.208561]\n",
      "epoch:11 step:10857 [D loss: 0.600443, acc: 69.53%] [G loss: 2.139028]\n",
      "epoch:11 step:10858 [D loss: 0.574918, acc: 71.88%] [G loss: 2.190228]\n",
      "epoch:11 step:10859 [D loss: 0.655640, acc: 62.50%] [G loss: 2.102858]\n",
      "epoch:11 step:10860 [D loss: 0.585495, acc: 72.66%] [G loss: 1.943228]\n",
      "epoch:11 step:10861 [D loss: 0.639800, acc: 69.53%] [G loss: 2.162626]\n",
      "epoch:11 step:10862 [D loss: 0.625156, acc: 67.97%] [G loss: 1.936942]\n",
      "epoch:11 step:10863 [D loss: 0.592945, acc: 69.53%] [G loss: 2.175329]\n",
      "epoch:11 step:10864 [D loss: 0.615664, acc: 66.41%] [G loss: 2.088279]\n",
      "epoch:11 step:10865 [D loss: 0.659063, acc: 60.16%] [G loss: 2.103513]\n",
      "epoch:11 step:10866 [D loss: 0.720434, acc: 58.59%] [G loss: 1.894479]\n",
      "epoch:11 step:10867 [D loss: 0.669727, acc: 57.03%] [G loss: 1.956606]\n",
      "epoch:11 step:10868 [D loss: 0.618829, acc: 67.19%] [G loss: 2.053612]\n",
      "epoch:11 step:10869 [D loss: 0.667171, acc: 58.59%] [G loss: 1.926484]\n",
      "epoch:11 step:10870 [D loss: 0.614189, acc: 65.62%] [G loss: 2.005777]\n",
      "epoch:11 step:10871 [D loss: 0.617811, acc: 70.31%] [G loss: 2.052242]\n",
      "epoch:11 step:10872 [D loss: 0.638580, acc: 65.62%] [G loss: 1.828091]\n",
      "epoch:11 step:10873 [D loss: 0.655944, acc: 61.72%] [G loss: 1.949168]\n",
      "epoch:11 step:10874 [D loss: 0.680678, acc: 59.38%] [G loss: 1.886573]\n",
      "epoch:11 step:10875 [D loss: 0.667700, acc: 62.50%] [G loss: 1.994001]\n",
      "epoch:11 step:10876 [D loss: 0.658238, acc: 64.06%] [G loss: 1.892349]\n",
      "epoch:11 step:10877 [D loss: 0.673173, acc: 58.59%] [G loss: 1.949037]\n",
      "epoch:11 step:10878 [D loss: 0.647730, acc: 64.84%] [G loss: 1.803408]\n",
      "epoch:11 step:10879 [D loss: 0.626943, acc: 61.72%] [G loss: 1.759560]\n",
      "epoch:11 step:10880 [D loss: 0.622495, acc: 64.84%] [G loss: 1.915817]\n",
      "epoch:11 step:10881 [D loss: 0.641925, acc: 63.28%] [G loss: 1.949323]\n",
      "epoch:11 step:10882 [D loss: 0.616028, acc: 66.41%] [G loss: 1.944056]\n",
      "epoch:11 step:10883 [D loss: 0.663010, acc: 58.59%] [G loss: 1.953888]\n",
      "epoch:11 step:10884 [D loss: 0.636479, acc: 66.41%] [G loss: 1.867251]\n",
      "epoch:11 step:10885 [D loss: 0.650653, acc: 67.19%] [G loss: 2.024464]\n",
      "epoch:11 step:10886 [D loss: 0.644827, acc: 61.72%] [G loss: 1.805148]\n",
      "epoch:11 step:10887 [D loss: 0.610931, acc: 65.62%] [G loss: 2.042507]\n",
      "epoch:11 step:10888 [D loss: 0.589275, acc: 71.09%] [G loss: 1.941798]\n",
      "epoch:11 step:10889 [D loss: 0.661361, acc: 62.50%] [G loss: 2.029377]\n",
      "epoch:11 step:10890 [D loss: 0.668931, acc: 61.72%] [G loss: 1.884822]\n",
      "epoch:11 step:10891 [D loss: 0.647016, acc: 62.50%] [G loss: 1.957158]\n",
      "epoch:11 step:10892 [D loss: 0.591544, acc: 64.84%] [G loss: 2.006687]\n",
      "epoch:11 step:10893 [D loss: 0.648186, acc: 63.28%] [G loss: 1.932653]\n",
      "epoch:11 step:10894 [D loss: 0.654373, acc: 61.72%] [G loss: 2.067671]\n",
      "epoch:11 step:10895 [D loss: 0.631760, acc: 65.62%] [G loss: 2.085686]\n",
      "epoch:11 step:10896 [D loss: 0.587446, acc: 64.84%] [G loss: 1.969090]\n",
      "epoch:11 step:10897 [D loss: 0.667795, acc: 57.81%] [G loss: 2.056939]\n",
      "epoch:11 step:10898 [D loss: 0.737834, acc: 56.25%] [G loss: 2.006445]\n",
      "epoch:11 step:10899 [D loss: 0.594315, acc: 72.66%] [G loss: 2.023157]\n",
      "epoch:11 step:10900 [D loss: 0.590160, acc: 70.31%] [G loss: 2.005761]\n",
      "epoch:11 step:10901 [D loss: 0.692502, acc: 56.25%] [G loss: 1.976351]\n",
      "epoch:11 step:10902 [D loss: 0.628346, acc: 67.97%] [G loss: 1.979898]\n",
      "epoch:11 step:10903 [D loss: 0.655577, acc: 63.28%] [G loss: 1.942904]\n",
      "epoch:11 step:10904 [D loss: 0.633740, acc: 62.50%] [G loss: 1.875197]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:11 step:10905 [D loss: 0.631228, acc: 64.06%] [G loss: 2.200266]\n",
      "epoch:11 step:10906 [D loss: 0.650850, acc: 60.16%] [G loss: 1.904652]\n",
      "epoch:11 step:10907 [D loss: 0.608438, acc: 66.41%] [G loss: 2.060808]\n",
      "epoch:11 step:10908 [D loss: 0.592938, acc: 71.09%] [G loss: 1.976116]\n",
      "epoch:11 step:10909 [D loss: 0.648662, acc: 58.59%] [G loss: 2.021900]\n",
      "epoch:11 step:10910 [D loss: 0.638133, acc: 64.06%] [G loss: 2.025814]\n",
      "epoch:11 step:10911 [D loss: 0.621258, acc: 63.28%] [G loss: 1.897854]\n",
      "epoch:11 step:10912 [D loss: 0.628582, acc: 60.16%] [G loss: 2.027374]\n",
      "epoch:11 step:10913 [D loss: 0.687678, acc: 54.69%] [G loss: 1.849291]\n",
      "epoch:11 step:10914 [D loss: 0.677446, acc: 60.94%] [G loss: 2.092428]\n",
      "epoch:11 step:10915 [D loss: 0.597852, acc: 67.97%] [G loss: 2.081202]\n",
      "epoch:11 step:10916 [D loss: 0.659273, acc: 60.16%] [G loss: 1.939047]\n",
      "epoch:11 step:10917 [D loss: 0.660448, acc: 61.72%] [G loss: 2.042683]\n",
      "epoch:11 step:10918 [D loss: 0.665554, acc: 63.28%] [G loss: 1.913598]\n",
      "epoch:11 step:10919 [D loss: 0.680207, acc: 58.59%] [G loss: 1.915054]\n",
      "epoch:11 step:10920 [D loss: 0.626892, acc: 65.62%] [G loss: 1.990655]\n",
      "epoch:11 step:10921 [D loss: 0.655858, acc: 60.16%] [G loss: 1.812398]\n",
      "epoch:11 step:10922 [D loss: 0.698669, acc: 53.12%] [G loss: 1.901835]\n",
      "epoch:11 step:10923 [D loss: 0.649048, acc: 64.84%] [G loss: 1.907278]\n",
      "epoch:11 step:10924 [D loss: 0.631602, acc: 60.94%] [G loss: 2.018449]\n",
      "epoch:11 step:10925 [D loss: 0.610193, acc: 61.72%] [G loss: 1.968948]\n",
      "epoch:11 step:10926 [D loss: 0.602600, acc: 71.09%] [G loss: 1.979179]\n",
      "epoch:11 step:10927 [D loss: 0.653849, acc: 59.38%] [G loss: 2.034297]\n",
      "epoch:11 step:10928 [D loss: 0.628580, acc: 66.41%] [G loss: 1.907629]\n",
      "epoch:11 step:10929 [D loss: 0.677161, acc: 62.50%] [G loss: 1.934778]\n",
      "epoch:11 step:10930 [D loss: 0.637726, acc: 61.72%] [G loss: 1.858083]\n",
      "epoch:11 step:10931 [D loss: 0.584810, acc: 68.75%] [G loss: 2.009668]\n",
      "epoch:11 step:10932 [D loss: 0.696890, acc: 56.25%] [G loss: 1.815451]\n",
      "epoch:11 step:10933 [D loss: 0.645858, acc: 61.72%] [G loss: 1.886577]\n",
      "epoch:11 step:10934 [D loss: 0.627410, acc: 70.31%] [G loss: 1.918957]\n",
      "epoch:11 step:10935 [D loss: 0.654383, acc: 62.50%] [G loss: 1.917193]\n",
      "epoch:11 step:10936 [D loss: 0.632236, acc: 65.62%] [G loss: 2.008140]\n",
      "epoch:11 step:10937 [D loss: 0.661667, acc: 62.50%] [G loss: 2.064819]\n",
      "epoch:11 step:10938 [D loss: 0.596867, acc: 65.62%] [G loss: 2.050431]\n",
      "epoch:11 step:10939 [D loss: 0.552147, acc: 75.78%] [G loss: 2.085276]\n",
      "epoch:11 step:10940 [D loss: 0.664003, acc: 64.84%] [G loss: 2.035629]\n",
      "epoch:11 step:10941 [D loss: 0.655329, acc: 60.16%] [G loss: 2.091079]\n",
      "epoch:11 step:10942 [D loss: 0.601674, acc: 67.19%] [G loss: 2.148121]\n",
      "epoch:11 step:10943 [D loss: 0.665936, acc: 59.38%] [G loss: 2.062546]\n",
      "epoch:11 step:10944 [D loss: 0.654514, acc: 64.84%] [G loss: 2.079637]\n",
      "epoch:11 step:10945 [D loss: 0.664903, acc: 60.16%] [G loss: 1.956371]\n",
      "epoch:11 step:10946 [D loss: 0.584831, acc: 68.75%] [G loss: 2.022941]\n",
      "epoch:11 step:10947 [D loss: 0.571061, acc: 71.88%] [G loss: 1.923113]\n",
      "epoch:11 step:10948 [D loss: 0.585534, acc: 65.62%] [G loss: 2.116682]\n",
      "epoch:11 step:10949 [D loss: 0.598183, acc: 68.75%] [G loss: 2.155682]\n",
      "epoch:11 step:10950 [D loss: 0.600124, acc: 68.75%] [G loss: 1.956950]\n",
      "epoch:11 step:10951 [D loss: 0.615088, acc: 67.19%] [G loss: 2.065905]\n",
      "epoch:11 step:10952 [D loss: 0.624484, acc: 64.84%] [G loss: 2.062844]\n",
      "epoch:11 step:10953 [D loss: 0.646795, acc: 64.06%] [G loss: 2.239633]\n",
      "epoch:11 step:10954 [D loss: 0.548313, acc: 72.66%] [G loss: 2.210799]\n",
      "epoch:11 step:10955 [D loss: 0.577849, acc: 71.09%] [G loss: 2.493800]\n",
      "epoch:11 step:10956 [D loss: 0.545088, acc: 70.31%] [G loss: 2.271086]\n",
      "epoch:11 step:10957 [D loss: 0.615300, acc: 66.41%] [G loss: 2.207898]\n",
      "epoch:11 step:10958 [D loss: 0.616581, acc: 64.06%] [G loss: 2.043500]\n",
      "epoch:11 step:10959 [D loss: 0.658712, acc: 61.72%] [G loss: 2.134788]\n",
      "epoch:11 step:10960 [D loss: 0.660140, acc: 61.72%] [G loss: 2.162664]\n",
      "epoch:11 step:10961 [D loss: 0.637625, acc: 65.62%] [G loss: 2.202603]\n",
      "epoch:11 step:10962 [D loss: 0.738145, acc: 53.91%] [G loss: 1.986766]\n",
      "epoch:11 step:10963 [D loss: 0.717390, acc: 58.59%] [G loss: 1.846444]\n",
      "epoch:11 step:10964 [D loss: 0.656552, acc: 59.38%] [G loss: 1.934110]\n",
      "epoch:11 step:10965 [D loss: 0.647614, acc: 62.50%] [G loss: 1.960391]\n",
      "epoch:11 step:10966 [D loss: 0.609147, acc: 66.41%] [G loss: 1.986493]\n",
      "epoch:11 step:10967 [D loss: 0.628419, acc: 67.19%] [G loss: 1.984264]\n",
      "epoch:11 step:10968 [D loss: 0.629452, acc: 59.38%] [G loss: 2.151263]\n",
      "epoch:11 step:10969 [D loss: 0.589072, acc: 71.88%] [G loss: 2.150524]\n",
      "epoch:11 step:10970 [D loss: 0.606336, acc: 69.53%] [G loss: 2.095143]\n",
      "epoch:11 step:10971 [D loss: 0.683834, acc: 57.81%] [G loss: 2.100010]\n",
      "epoch:11 step:10972 [D loss: 0.640825, acc: 62.50%] [G loss: 2.068389]\n",
      "epoch:11 step:10973 [D loss: 0.675628, acc: 56.25%] [G loss: 1.984107]\n",
      "epoch:11 step:10974 [D loss: 0.649876, acc: 65.62%] [G loss: 1.912025]\n",
      "epoch:11 step:10975 [D loss: 0.650470, acc: 64.06%] [G loss: 2.008647]\n",
      "epoch:11 step:10976 [D loss: 0.635187, acc: 67.97%] [G loss: 1.948658]\n",
      "epoch:11 step:10977 [D loss: 0.622789, acc: 63.28%] [G loss: 2.038514]\n",
      "epoch:11 step:10978 [D loss: 0.671196, acc: 62.50%] [G loss: 1.848163]\n",
      "epoch:11 step:10979 [D loss: 0.632585, acc: 63.28%] [G loss: 1.860327]\n",
      "epoch:11 step:10980 [D loss: 0.614396, acc: 67.97%] [G loss: 2.104796]\n",
      "epoch:11 step:10981 [D loss: 0.650147, acc: 60.16%] [G loss: 2.012379]\n",
      "epoch:11 step:10982 [D loss: 0.609813, acc: 64.06%] [G loss: 2.055440]\n",
      "epoch:11 step:10983 [D loss: 0.661069, acc: 58.59%] [G loss: 1.813566]\n",
      "epoch:11 step:10984 [D loss: 0.620448, acc: 65.62%] [G loss: 2.101330]\n",
      "epoch:11 step:10985 [D loss: 0.587989, acc: 65.62%] [G loss: 1.882282]\n",
      "epoch:11 step:10986 [D loss: 0.651612, acc: 61.72%] [G loss: 1.974213]\n",
      "epoch:11 step:10987 [D loss: 0.627636, acc: 64.84%] [G loss: 2.171162]\n",
      "epoch:11 step:10988 [D loss: 0.611921, acc: 60.94%] [G loss: 2.001948]\n",
      "epoch:11 step:10989 [D loss: 0.645074, acc: 63.28%] [G loss: 1.971364]\n",
      "epoch:11 step:10990 [D loss: 0.615660, acc: 67.19%] [G loss: 1.887642]\n",
      "epoch:11 step:10991 [D loss: 0.663305, acc: 60.94%] [G loss: 1.909198]\n",
      "epoch:11 step:10992 [D loss: 0.600427, acc: 64.84%] [G loss: 1.910342]\n",
      "epoch:11 step:10993 [D loss: 0.688508, acc: 61.72%] [G loss: 1.983574]\n",
      "epoch:11 step:10994 [D loss: 0.623973, acc: 67.19%] [G loss: 1.898866]\n",
      "epoch:11 step:10995 [D loss: 0.629964, acc: 65.62%] [G loss: 2.024475]\n",
      "epoch:11 step:10996 [D loss: 0.612493, acc: 63.28%] [G loss: 2.063675]\n",
      "epoch:11 step:10997 [D loss: 0.611272, acc: 65.62%] [G loss: 2.079377]\n",
      "epoch:11 step:10998 [D loss: 0.578572, acc: 72.66%] [G loss: 2.047621]\n",
      "epoch:11 step:10999 [D loss: 0.562430, acc: 69.53%] [G loss: 2.259271]\n",
      "epoch:11 step:11000 [D loss: 0.561410, acc: 70.31%] [G loss: 2.255742]\n",
      "##############\n",
      "[2.51517997 1.38944074 6.33354158 4.79404858 3.88052891 5.81297088\n",
      " 4.47614043 4.76474152 4.7418443  3.71496457]\n",
      "##########\n",
      "epoch:11 step:11001 [D loss: 0.635662, acc: 65.62%] [G loss: 2.294366]\n",
      "epoch:11 step:11002 [D loss: 0.646434, acc: 62.50%] [G loss: 2.225507]\n",
      "epoch:11 step:11003 [D loss: 0.659595, acc: 62.50%] [G loss: 1.987437]\n",
      "epoch:11 step:11004 [D loss: 0.620246, acc: 60.94%] [G loss: 1.957824]\n",
      "epoch:11 step:11005 [D loss: 0.627612, acc: 65.62%] [G loss: 1.922771]\n",
      "epoch:11 step:11006 [D loss: 0.627572, acc: 66.41%] [G loss: 2.128634]\n",
      "epoch:11 step:11007 [D loss: 0.588276, acc: 70.31%] [G loss: 2.211707]\n",
      "epoch:11 step:11008 [D loss: 0.645000, acc: 64.84%] [G loss: 2.254982]\n",
      "epoch:11 step:11009 [D loss: 0.642309, acc: 57.81%] [G loss: 1.992070]\n",
      "epoch:11 step:11010 [D loss: 0.630730, acc: 64.84%] [G loss: 1.860271]\n",
      "epoch:11 step:11011 [D loss: 0.640699, acc: 64.06%] [G loss: 2.021037]\n",
      "epoch:11 step:11012 [D loss: 0.692780, acc: 58.59%] [G loss: 2.111630]\n",
      "epoch:11 step:11013 [D loss: 0.633828, acc: 70.31%] [G loss: 2.028877]\n",
      "epoch:11 step:11014 [D loss: 0.593976, acc: 67.97%] [G loss: 2.234618]\n",
      "epoch:11 step:11015 [D loss: 0.574854, acc: 67.97%] [G loss: 2.278236]\n",
      "epoch:11 step:11016 [D loss: 0.589148, acc: 67.19%] [G loss: 2.242521]\n",
      "epoch:11 step:11017 [D loss: 0.681543, acc: 65.62%] [G loss: 1.978566]\n",
      "epoch:11 step:11018 [D loss: 0.598988, acc: 66.41%] [G loss: 2.046960]\n",
      "epoch:11 step:11019 [D loss: 0.590926, acc: 68.75%] [G loss: 2.114445]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:11 step:11020 [D loss: 0.602591, acc: 67.19%] [G loss: 1.993942]\n",
      "epoch:11 step:11021 [D loss: 0.624281, acc: 66.41%] [G loss: 2.039578]\n",
      "epoch:11 step:11022 [D loss: 0.588246, acc: 69.53%] [G loss: 2.031989]\n",
      "epoch:11 step:11023 [D loss: 0.781336, acc: 49.22%] [G loss: 1.807129]\n",
      "epoch:11 step:11024 [D loss: 0.673163, acc: 59.38%] [G loss: 2.010232]\n",
      "epoch:11 step:11025 [D loss: 0.682576, acc: 53.12%] [G loss: 2.083836]\n",
      "epoch:11 step:11026 [D loss: 0.637019, acc: 63.28%] [G loss: 2.099477]\n",
      "epoch:11 step:11027 [D loss: 0.611271, acc: 64.06%] [G loss: 1.866117]\n",
      "epoch:11 step:11028 [D loss: 0.685288, acc: 54.69%] [G loss: 2.042237]\n",
      "epoch:11 step:11029 [D loss: 0.678127, acc: 57.81%] [G loss: 1.862129]\n",
      "epoch:11 step:11030 [D loss: 0.649410, acc: 57.81%] [G loss: 1.919365]\n",
      "epoch:11 step:11031 [D loss: 0.634624, acc: 64.84%] [G loss: 2.079882]\n",
      "epoch:11 step:11032 [D loss: 0.674756, acc: 58.59%] [G loss: 1.973469]\n",
      "epoch:11 step:11033 [D loss: 0.711707, acc: 57.03%] [G loss: 2.076162]\n",
      "epoch:11 step:11034 [D loss: 0.637098, acc: 66.41%] [G loss: 1.856283]\n",
      "epoch:11 step:11035 [D loss: 0.680313, acc: 58.59%] [G loss: 1.991316]\n",
      "epoch:11 step:11036 [D loss: 0.627370, acc: 64.06%] [G loss: 2.021072]\n",
      "epoch:11 step:11037 [D loss: 0.662175, acc: 65.62%] [G loss: 1.930047]\n",
      "epoch:11 step:11038 [D loss: 0.663060, acc: 60.16%] [G loss: 1.955938]\n",
      "epoch:11 step:11039 [D loss: 0.651424, acc: 63.28%] [G loss: 1.886130]\n",
      "epoch:11 step:11040 [D loss: 0.661011, acc: 64.06%] [G loss: 1.928846]\n",
      "epoch:11 step:11041 [D loss: 0.657990, acc: 64.06%] [G loss: 1.805545]\n",
      "epoch:11 step:11042 [D loss: 0.645836, acc: 66.41%] [G loss: 1.952557]\n",
      "epoch:11 step:11043 [D loss: 0.603239, acc: 67.19%] [G loss: 1.937702]\n",
      "epoch:11 step:11044 [D loss: 0.638065, acc: 65.62%] [G loss: 1.922827]\n",
      "epoch:11 step:11045 [D loss: 0.661167, acc: 59.38%] [G loss: 1.869210]\n",
      "epoch:11 step:11046 [D loss: 0.639305, acc: 60.16%] [G loss: 1.851805]\n",
      "epoch:11 step:11047 [D loss: 0.637874, acc: 57.81%] [G loss: 1.991550]\n",
      "epoch:11 step:11048 [D loss: 0.687280, acc: 64.84%] [G loss: 1.855344]\n",
      "epoch:11 step:11049 [D loss: 0.672797, acc: 60.94%] [G loss: 1.845433]\n",
      "epoch:11 step:11050 [D loss: 0.649453, acc: 59.38%] [G loss: 1.975001]\n",
      "epoch:11 step:11051 [D loss: 0.570334, acc: 68.75%] [G loss: 2.022614]\n",
      "epoch:11 step:11052 [D loss: 0.636044, acc: 62.50%] [G loss: 2.020903]\n",
      "epoch:11 step:11053 [D loss: 0.632188, acc: 66.41%] [G loss: 2.013267]\n",
      "epoch:11 step:11054 [D loss: 0.576229, acc: 69.53%] [G loss: 1.859197]\n",
      "epoch:11 step:11055 [D loss: 0.653041, acc: 60.94%] [G loss: 1.992679]\n",
      "epoch:11 step:11056 [D loss: 0.623519, acc: 62.50%] [G loss: 1.944228]\n",
      "epoch:11 step:11057 [D loss: 0.654655, acc: 54.69%] [G loss: 2.002592]\n",
      "epoch:11 step:11058 [D loss: 0.695027, acc: 57.81%] [G loss: 1.838620]\n",
      "epoch:11 step:11059 [D loss: 0.669570, acc: 64.06%] [G loss: 1.979276]\n",
      "epoch:11 step:11060 [D loss: 0.699111, acc: 60.94%] [G loss: 1.965889]\n",
      "epoch:11 step:11061 [D loss: 0.619119, acc: 65.62%] [G loss: 1.996356]\n",
      "epoch:11 step:11062 [D loss: 0.591336, acc: 66.41%] [G loss: 1.907770]\n",
      "epoch:11 step:11063 [D loss: 0.642767, acc: 60.94%] [G loss: 1.907754]\n",
      "epoch:11 step:11064 [D loss: 0.597809, acc: 69.53%] [G loss: 2.007154]\n",
      "epoch:11 step:11065 [D loss: 0.650085, acc: 61.72%] [G loss: 1.958615]\n",
      "epoch:11 step:11066 [D loss: 0.591657, acc: 68.75%] [G loss: 1.912710]\n",
      "epoch:11 step:11067 [D loss: 0.630430, acc: 60.94%] [G loss: 1.910701]\n",
      "epoch:11 step:11068 [D loss: 0.639908, acc: 59.38%] [G loss: 1.894603]\n",
      "epoch:11 step:11069 [D loss: 0.621222, acc: 68.75%] [G loss: 1.858076]\n",
      "epoch:11 step:11070 [D loss: 0.647978, acc: 59.38%] [G loss: 1.930101]\n",
      "epoch:11 step:11071 [D loss: 0.639357, acc: 62.50%] [G loss: 1.846963]\n",
      "epoch:11 step:11072 [D loss: 0.675710, acc: 60.16%] [G loss: 1.902792]\n",
      "epoch:11 step:11073 [D loss: 0.703026, acc: 53.91%] [G loss: 1.784484]\n",
      "epoch:11 step:11074 [D loss: 0.639413, acc: 65.62%] [G loss: 1.897018]\n",
      "epoch:11 step:11075 [D loss: 0.615513, acc: 67.97%] [G loss: 1.887077]\n",
      "epoch:11 step:11076 [D loss: 0.664858, acc: 60.16%] [G loss: 2.120064]\n",
      "epoch:11 step:11077 [D loss: 0.663187, acc: 60.16%] [G loss: 1.822785]\n",
      "epoch:11 step:11078 [D loss: 0.595128, acc: 63.28%] [G loss: 2.074227]\n",
      "epoch:11 step:11079 [D loss: 0.692286, acc: 58.59%] [G loss: 1.881863]\n",
      "epoch:11 step:11080 [D loss: 0.629727, acc: 61.72%] [G loss: 1.955570]\n",
      "epoch:11 step:11081 [D loss: 0.580087, acc: 74.22%] [G loss: 2.292362]\n",
      "epoch:11 step:11082 [D loss: 0.646125, acc: 64.84%] [G loss: 2.261153]\n",
      "epoch:11 step:11083 [D loss: 0.611851, acc: 71.88%] [G loss: 2.037907]\n",
      "epoch:11 step:11084 [D loss: 0.632007, acc: 64.06%] [G loss: 1.947453]\n",
      "epoch:11 step:11085 [D loss: 0.674124, acc: 59.38%] [G loss: 1.833662]\n",
      "epoch:11 step:11086 [D loss: 0.672802, acc: 55.47%] [G loss: 1.867524]\n",
      "epoch:11 step:11087 [D loss: 0.625125, acc: 63.28%] [G loss: 2.205554]\n",
      "epoch:11 step:11088 [D loss: 0.599754, acc: 67.97%] [G loss: 2.226486]\n",
      "epoch:11 step:11089 [D loss: 0.563602, acc: 73.44%] [G loss: 2.212856]\n",
      "epoch:11 step:11090 [D loss: 0.649728, acc: 61.72%] [G loss: 2.028889]\n",
      "epoch:11 step:11091 [D loss: 0.709693, acc: 51.56%] [G loss: 1.724475]\n",
      "epoch:11 step:11092 [D loss: 0.631815, acc: 61.72%] [G loss: 2.030051]\n",
      "epoch:11 step:11093 [D loss: 0.618949, acc: 65.62%] [G loss: 2.122939]\n",
      "epoch:11 step:11094 [D loss: 0.702933, acc: 60.94%] [G loss: 1.837038]\n",
      "epoch:11 step:11095 [D loss: 0.675389, acc: 57.03%] [G loss: 1.877200]\n",
      "epoch:11 step:11096 [D loss: 0.666322, acc: 59.38%] [G loss: 2.088784]\n",
      "epoch:11 step:11097 [D loss: 0.602524, acc: 69.53%] [G loss: 1.892522]\n",
      "epoch:11 step:11098 [D loss: 0.572807, acc: 66.41%] [G loss: 1.881245]\n",
      "epoch:11 step:11099 [D loss: 0.582219, acc: 69.53%] [G loss: 2.064176]\n",
      "epoch:11 step:11100 [D loss: 0.608733, acc: 66.41%] [G loss: 2.248229]\n",
      "epoch:11 step:11101 [D loss: 0.650034, acc: 66.41%] [G loss: 1.955603]\n",
      "epoch:11 step:11102 [D loss: 0.677408, acc: 58.59%] [G loss: 1.859477]\n",
      "epoch:11 step:11103 [D loss: 0.667809, acc: 58.59%] [G loss: 1.981878]\n",
      "epoch:11 step:11104 [D loss: 0.622332, acc: 64.06%] [G loss: 1.861084]\n",
      "epoch:11 step:11105 [D loss: 0.677963, acc: 57.03%] [G loss: 1.841245]\n",
      "epoch:11 step:11106 [D loss: 0.591167, acc: 71.88%] [G loss: 2.028452]\n",
      "epoch:11 step:11107 [D loss: 0.729647, acc: 58.59%] [G loss: 1.802085]\n",
      "epoch:11 step:11108 [D loss: 0.681690, acc: 56.25%] [G loss: 1.844615]\n",
      "epoch:11 step:11109 [D loss: 0.638806, acc: 66.41%] [G loss: 1.862753]\n",
      "epoch:11 step:11110 [D loss: 0.590342, acc: 71.09%] [G loss: 1.975533]\n",
      "epoch:11 step:11111 [D loss: 0.590723, acc: 67.19%] [G loss: 1.916513]\n",
      "epoch:11 step:11112 [D loss: 0.611870, acc: 64.06%] [G loss: 1.931061]\n",
      "epoch:11 step:11113 [D loss: 0.600366, acc: 67.97%] [G loss: 2.273990]\n",
      "epoch:11 step:11114 [D loss: 0.622699, acc: 59.38%] [G loss: 2.140549]\n",
      "epoch:11 step:11115 [D loss: 0.659560, acc: 56.25%] [G loss: 1.922891]\n",
      "epoch:11 step:11116 [D loss: 0.602409, acc: 66.41%] [G loss: 1.854851]\n",
      "epoch:11 step:11117 [D loss: 0.581018, acc: 69.53%] [G loss: 1.964008]\n",
      "epoch:11 step:11118 [D loss: 0.660485, acc: 57.03%] [G loss: 1.919802]\n",
      "epoch:11 step:11119 [D loss: 0.627980, acc: 58.59%] [G loss: 1.841049]\n",
      "epoch:11 step:11120 [D loss: 0.646115, acc: 62.50%] [G loss: 1.969553]\n",
      "epoch:11 step:11121 [D loss: 0.644773, acc: 63.28%] [G loss: 2.119695]\n",
      "epoch:11 step:11122 [D loss: 0.575880, acc: 67.97%] [G loss: 2.184344]\n",
      "epoch:11 step:11123 [D loss: 0.579126, acc: 70.31%] [G loss: 2.167016]\n",
      "epoch:11 step:11124 [D loss: 0.665210, acc: 60.94%] [G loss: 2.137192]\n",
      "epoch:11 step:11125 [D loss: 0.679018, acc: 57.81%] [G loss: 1.842884]\n",
      "epoch:11 step:11126 [D loss: 0.601655, acc: 72.66%] [G loss: 2.096652]\n",
      "epoch:11 step:11127 [D loss: 0.676706, acc: 58.59%] [G loss: 1.835482]\n",
      "epoch:11 step:11128 [D loss: 0.607774, acc: 72.66%] [G loss: 1.950609]\n",
      "epoch:11 step:11129 [D loss: 0.598453, acc: 66.41%] [G loss: 1.986801]\n",
      "epoch:11 step:11130 [D loss: 0.597253, acc: 67.19%] [G loss: 2.130754]\n",
      "epoch:11 step:11131 [D loss: 0.672292, acc: 64.06%] [G loss: 1.915640]\n",
      "epoch:11 step:11132 [D loss: 0.620946, acc: 66.41%] [G loss: 2.159337]\n",
      "epoch:11 step:11133 [D loss: 0.663362, acc: 63.28%] [G loss: 1.964923]\n",
      "epoch:11 step:11134 [D loss: 0.664804, acc: 62.50%] [G loss: 1.790512]\n",
      "epoch:11 step:11135 [D loss: 0.657336, acc: 61.72%] [G loss: 1.962614]\n",
      "epoch:11 step:11136 [D loss: 0.664717, acc: 62.50%] [G loss: 1.939121]\n",
      "epoch:11 step:11137 [D loss: 0.706657, acc: 56.25%] [G loss: 1.887087]\n",
      "epoch:11 step:11138 [D loss: 0.642770, acc: 66.41%] [G loss: 2.106709]\n",
      "epoch:11 step:11139 [D loss: 0.633457, acc: 64.06%] [G loss: 1.904175]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:11 step:11140 [D loss: 0.555747, acc: 67.19%] [G loss: 1.982230]\n",
      "epoch:11 step:11141 [D loss: 0.645602, acc: 60.94%] [G loss: 2.008367]\n",
      "epoch:11 step:11142 [D loss: 0.642521, acc: 64.06%] [G loss: 1.912495]\n",
      "epoch:11 step:11143 [D loss: 0.618553, acc: 63.28%] [G loss: 1.854928]\n",
      "epoch:11 step:11144 [D loss: 0.635471, acc: 63.28%] [G loss: 1.995682]\n",
      "epoch:11 step:11145 [D loss: 0.650921, acc: 64.84%] [G loss: 1.981567]\n",
      "epoch:11 step:11146 [D loss: 0.594015, acc: 65.62%] [G loss: 2.015031]\n",
      "epoch:11 step:11147 [D loss: 0.622028, acc: 61.72%] [G loss: 2.058795]\n",
      "epoch:11 step:11148 [D loss: 0.600810, acc: 69.53%] [G loss: 1.963285]\n",
      "epoch:11 step:11149 [D loss: 0.563617, acc: 71.88%] [G loss: 2.091689]\n",
      "epoch:11 step:11150 [D loss: 0.617743, acc: 66.41%] [G loss: 2.066638]\n",
      "epoch:11 step:11151 [D loss: 0.576378, acc: 69.53%] [G loss: 2.193813]\n",
      "epoch:11 step:11152 [D loss: 0.615114, acc: 63.28%] [G loss: 2.105872]\n",
      "epoch:11 step:11153 [D loss: 0.616383, acc: 64.84%] [G loss: 2.072252]\n",
      "epoch:11 step:11154 [D loss: 0.624983, acc: 67.97%] [G loss: 2.128294]\n",
      "epoch:11 step:11155 [D loss: 0.603830, acc: 70.31%] [G loss: 2.186925]\n",
      "epoch:11 step:11156 [D loss: 0.650345, acc: 64.84%] [G loss: 2.099784]\n",
      "epoch:11 step:11157 [D loss: 0.698506, acc: 62.50%] [G loss: 1.867641]\n",
      "epoch:11 step:11158 [D loss: 0.631539, acc: 62.50%] [G loss: 1.908002]\n",
      "epoch:11 step:11159 [D loss: 0.628406, acc: 63.28%] [G loss: 2.050855]\n",
      "epoch:11 step:11160 [D loss: 0.647775, acc: 61.72%] [G loss: 2.139633]\n",
      "epoch:11 step:11161 [D loss: 0.617377, acc: 67.19%] [G loss: 2.106117]\n",
      "epoch:11 step:11162 [D loss: 0.668744, acc: 60.94%] [G loss: 1.922047]\n",
      "epoch:11 step:11163 [D loss: 0.680613, acc: 62.50%] [G loss: 1.861150]\n",
      "epoch:11 step:11164 [D loss: 0.641808, acc: 64.06%] [G loss: 2.091908]\n",
      "epoch:11 step:11165 [D loss: 0.789178, acc: 43.75%] [G loss: 1.752808]\n",
      "epoch:11 step:11166 [D loss: 0.707899, acc: 57.81%] [G loss: 1.842018]\n",
      "epoch:11 step:11167 [D loss: 0.590476, acc: 64.84%] [G loss: 1.923904]\n",
      "epoch:11 step:11168 [D loss: 0.684081, acc: 58.59%] [G loss: 1.914951]\n",
      "epoch:11 step:11169 [D loss: 0.640067, acc: 59.38%] [G loss: 1.862185]\n",
      "epoch:11 step:11170 [D loss: 0.618660, acc: 65.62%] [G loss: 2.028933]\n",
      "epoch:11 step:11171 [D loss: 0.611638, acc: 64.84%] [G loss: 1.979774]\n",
      "epoch:11 step:11172 [D loss: 0.632948, acc: 65.62%] [G loss: 1.934326]\n",
      "epoch:11 step:11173 [D loss: 0.639184, acc: 62.50%] [G loss: 1.854843]\n",
      "epoch:11 step:11174 [D loss: 0.604873, acc: 62.50%] [G loss: 1.804633]\n",
      "epoch:11 step:11175 [D loss: 0.617675, acc: 67.19%] [G loss: 2.065558]\n",
      "epoch:11 step:11176 [D loss: 0.628971, acc: 66.41%] [G loss: 1.895032]\n",
      "epoch:11 step:11177 [D loss: 0.670934, acc: 62.50%] [G loss: 2.012072]\n",
      "epoch:11 step:11178 [D loss: 0.628148, acc: 64.06%] [G loss: 1.897276]\n",
      "epoch:11 step:11179 [D loss: 0.680783, acc: 59.38%] [G loss: 2.022680]\n",
      "epoch:11 step:11180 [D loss: 0.615511, acc: 64.06%] [G loss: 1.943435]\n",
      "epoch:11 step:11181 [D loss: 0.702688, acc: 51.56%] [G loss: 1.964687]\n",
      "epoch:11 step:11182 [D loss: 0.625821, acc: 65.62%] [G loss: 1.971763]\n",
      "epoch:11 step:11183 [D loss: 0.652411, acc: 60.94%] [G loss: 1.920673]\n",
      "epoch:11 step:11184 [D loss: 0.698600, acc: 53.12%] [G loss: 1.995175]\n",
      "epoch:11 step:11185 [D loss: 0.713457, acc: 61.72%] [G loss: 1.886355]\n",
      "epoch:11 step:11186 [D loss: 0.656521, acc: 59.38%] [G loss: 1.805637]\n",
      "epoch:11 step:11187 [D loss: 0.661233, acc: 63.28%] [G loss: 1.900114]\n",
      "epoch:11 step:11188 [D loss: 0.680524, acc: 60.16%] [G loss: 1.875277]\n",
      "epoch:11 step:11189 [D loss: 0.661582, acc: 63.28%] [G loss: 1.902992]\n",
      "epoch:11 step:11190 [D loss: 0.618376, acc: 61.72%] [G loss: 1.896700]\n",
      "epoch:11 step:11191 [D loss: 0.636468, acc: 65.62%] [G loss: 2.090161]\n",
      "epoch:11 step:11192 [D loss: 0.606992, acc: 67.19%] [G loss: 2.188982]\n",
      "epoch:11 step:11193 [D loss: 0.582372, acc: 70.31%] [G loss: 2.182411]\n",
      "epoch:11 step:11194 [D loss: 0.612596, acc: 62.50%] [G loss: 2.081163]\n",
      "epoch:11 step:11195 [D loss: 0.617497, acc: 68.75%] [G loss: 2.018479]\n",
      "epoch:11 step:11196 [D loss: 0.618073, acc: 60.16%] [G loss: 2.069948]\n",
      "epoch:11 step:11197 [D loss: 0.666691, acc: 62.50%] [G loss: 1.925308]\n",
      "epoch:11 step:11198 [D loss: 0.679165, acc: 54.69%] [G loss: 1.901962]\n",
      "epoch:11 step:11199 [D loss: 0.687953, acc: 57.81%] [G loss: 1.929244]\n",
      "epoch:11 step:11200 [D loss: 0.667864, acc: 64.84%] [G loss: 1.939562]\n",
      "##############\n",
      "[2.58034075 1.42454422 6.27033187 4.80513993 3.63453086 5.58156474\n",
      " 4.37734697 4.62604451 4.59473237 3.53608783]\n",
      "##########\n",
      "epoch:11 step:11201 [D loss: 0.571807, acc: 71.09%] [G loss: 1.942500]\n",
      "epoch:11 step:11202 [D loss: 0.619594, acc: 63.28%] [G loss: 2.033233]\n",
      "epoch:11 step:11203 [D loss: 0.638477, acc: 64.84%] [G loss: 1.916874]\n",
      "epoch:11 step:11204 [D loss: 0.652425, acc: 61.72%] [G loss: 1.996967]\n",
      "epoch:11 step:11205 [D loss: 0.616160, acc: 64.84%] [G loss: 1.985537]\n",
      "epoch:11 step:11206 [D loss: 0.603385, acc: 63.28%] [G loss: 2.187854]\n",
      "epoch:11 step:11207 [D loss: 0.609346, acc: 67.97%] [G loss: 2.073835]\n",
      "epoch:11 step:11208 [D loss: 0.576680, acc: 69.53%] [G loss: 1.939238]\n",
      "epoch:11 step:11209 [D loss: 0.631654, acc: 64.06%] [G loss: 2.063702]\n",
      "epoch:11 step:11210 [D loss: 0.634307, acc: 62.50%] [G loss: 2.027170]\n",
      "epoch:11 step:11211 [D loss: 0.651010, acc: 64.84%] [G loss: 2.142151]\n",
      "epoch:11 step:11212 [D loss: 0.573099, acc: 70.31%] [G loss: 1.942185]\n",
      "epoch:11 step:11213 [D loss: 0.657535, acc: 57.81%] [G loss: 2.109043]\n",
      "epoch:11 step:11214 [D loss: 0.582024, acc: 69.53%] [G loss: 2.036366]\n",
      "epoch:11 step:11215 [D loss: 0.634616, acc: 61.72%] [G loss: 2.079538]\n",
      "epoch:11 step:11216 [D loss: 0.603226, acc: 67.19%] [G loss: 2.179631]\n",
      "epoch:11 step:11217 [D loss: 0.602353, acc: 69.53%] [G loss: 2.053135]\n",
      "epoch:11 step:11218 [D loss: 0.622949, acc: 64.06%] [G loss: 2.004158]\n",
      "epoch:11 step:11219 [D loss: 0.561438, acc: 71.88%] [G loss: 2.314867]\n",
      "epoch:11 step:11220 [D loss: 0.659108, acc: 55.47%] [G loss: 2.073726]\n",
      "epoch:11 step:11221 [D loss: 0.648543, acc: 63.28%] [G loss: 2.131241]\n",
      "epoch:11 step:11222 [D loss: 0.678268, acc: 57.81%] [G loss: 2.123328]\n",
      "epoch:11 step:11223 [D loss: 0.626568, acc: 64.84%] [G loss: 2.032168]\n",
      "epoch:11 step:11224 [D loss: 0.553250, acc: 73.44%] [G loss: 2.190138]\n",
      "epoch:11 step:11225 [D loss: 0.599460, acc: 66.41%] [G loss: 2.265341]\n",
      "epoch:11 step:11226 [D loss: 0.650771, acc: 60.94%] [G loss: 2.195837]\n",
      "epoch:11 step:11227 [D loss: 0.730963, acc: 59.38%] [G loss: 1.962934]\n",
      "epoch:11 step:11228 [D loss: 0.639634, acc: 61.72%] [G loss: 2.247271]\n",
      "epoch:11 step:11229 [D loss: 0.605603, acc: 67.97%] [G loss: 2.141015]\n",
      "epoch:11 step:11230 [D loss: 0.579925, acc: 73.44%] [G loss: 2.125257]\n",
      "epoch:11 step:11231 [D loss: 0.504401, acc: 78.91%] [G loss: 2.155753]\n",
      "epoch:11 step:11232 [D loss: 0.592546, acc: 68.75%] [G loss: 2.301375]\n",
      "epoch:11 step:11233 [D loss: 0.596840, acc: 67.19%] [G loss: 2.455305]\n",
      "epoch:11 step:11234 [D loss: 0.559604, acc: 69.53%] [G loss: 2.217742]\n",
      "epoch:11 step:11235 [D loss: 0.747806, acc: 51.56%] [G loss: 1.898568]\n",
      "epoch:11 step:11236 [D loss: 0.716169, acc: 51.56%] [G loss: 2.191163]\n",
      "epoch:11 step:11237 [D loss: 0.655614, acc: 61.72%] [G loss: 2.064121]\n",
      "epoch:11 step:11238 [D loss: 0.607598, acc: 70.31%] [G loss: 2.161554]\n",
      "epoch:11 step:11239 [D loss: 0.615703, acc: 67.19%] [G loss: 2.032235]\n",
      "epoch:11 step:11240 [D loss: 0.602163, acc: 68.75%] [G loss: 2.042166]\n",
      "epoch:11 step:11241 [D loss: 0.646052, acc: 64.06%] [G loss: 2.342894]\n",
      "epoch:11 step:11242 [D loss: 0.580378, acc: 70.31%] [G loss: 2.132768]\n",
      "epoch:11 step:11243 [D loss: 0.559560, acc: 75.00%] [G loss: 2.222991]\n",
      "epoch:11 step:11244 [D loss: 0.519207, acc: 77.34%] [G loss: 2.621196]\n",
      "epoch:12 step:11245 [D loss: 0.709878, acc: 57.81%] [G loss: 2.077730]\n",
      "epoch:12 step:11246 [D loss: 0.677881, acc: 63.28%] [G loss: 2.103087]\n",
      "epoch:12 step:11247 [D loss: 0.590447, acc: 67.19%] [G loss: 2.222279]\n",
      "epoch:12 step:11248 [D loss: 0.636894, acc: 62.50%] [G loss: 2.063875]\n",
      "epoch:12 step:11249 [D loss: 0.621346, acc: 65.62%] [G loss: 2.103116]\n",
      "epoch:12 step:11250 [D loss: 0.651257, acc: 65.62%] [G loss: 2.104527]\n",
      "epoch:12 step:11251 [D loss: 0.558648, acc: 73.44%] [G loss: 2.242211]\n",
      "epoch:12 step:11252 [D loss: 0.630166, acc: 64.84%] [G loss: 2.384545]\n",
      "epoch:12 step:11253 [D loss: 0.538146, acc: 75.78%] [G loss: 2.178883]\n",
      "epoch:12 step:11254 [D loss: 0.607364, acc: 66.41%] [G loss: 2.217737]\n",
      "epoch:12 step:11255 [D loss: 0.616795, acc: 65.62%] [G loss: 2.300000]\n",
      "epoch:12 step:11256 [D loss: 0.666600, acc: 60.16%] [G loss: 2.147332]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:12 step:11257 [D loss: 0.583044, acc: 66.41%] [G loss: 2.135431]\n",
      "epoch:12 step:11258 [D loss: 0.609819, acc: 63.28%] [G loss: 2.172879]\n",
      "epoch:12 step:11259 [D loss: 0.571787, acc: 72.66%] [G loss: 2.233210]\n",
      "epoch:12 step:11260 [D loss: 0.584055, acc: 67.19%] [G loss: 2.356968]\n",
      "epoch:12 step:11261 [D loss: 0.683950, acc: 60.16%] [G loss: 2.145588]\n",
      "epoch:12 step:11262 [D loss: 0.701061, acc: 59.38%] [G loss: 2.140128]\n",
      "epoch:12 step:11263 [D loss: 0.675656, acc: 57.03%] [G loss: 1.934097]\n",
      "epoch:12 step:11264 [D loss: 0.681851, acc: 58.59%] [G loss: 1.794513]\n",
      "epoch:12 step:11265 [D loss: 0.633999, acc: 62.50%] [G loss: 2.041678]\n",
      "epoch:12 step:11266 [D loss: 0.644527, acc: 63.28%] [G loss: 1.970145]\n",
      "epoch:12 step:11267 [D loss: 0.606083, acc: 66.41%] [G loss: 2.129725]\n",
      "epoch:12 step:11268 [D loss: 0.596287, acc: 67.97%] [G loss: 2.135202]\n",
      "epoch:12 step:11269 [D loss: 0.582610, acc: 69.53%] [G loss: 2.157851]\n",
      "epoch:12 step:11270 [D loss: 0.654509, acc: 65.62%] [G loss: 1.847433]\n",
      "epoch:12 step:11271 [D loss: 0.677078, acc: 62.50%] [G loss: 1.982774]\n",
      "epoch:12 step:11272 [D loss: 0.622458, acc: 64.06%] [G loss: 2.035693]\n",
      "epoch:12 step:11273 [D loss: 0.616014, acc: 67.97%] [G loss: 1.842908]\n",
      "epoch:12 step:11274 [D loss: 0.628922, acc: 67.19%] [G loss: 2.047926]\n",
      "epoch:12 step:11275 [D loss: 0.666436, acc: 63.28%] [G loss: 1.879549]\n",
      "epoch:12 step:11276 [D loss: 0.653346, acc: 60.94%] [G loss: 2.013970]\n",
      "epoch:12 step:11277 [D loss: 0.566403, acc: 75.00%] [G loss: 2.004946]\n",
      "epoch:12 step:11278 [D loss: 0.664051, acc: 60.94%] [G loss: 1.916001]\n",
      "epoch:12 step:11279 [D loss: 0.592012, acc: 64.84%] [G loss: 2.120806]\n",
      "epoch:12 step:11280 [D loss: 0.628756, acc: 65.62%] [G loss: 2.158304]\n",
      "epoch:12 step:11281 [D loss: 0.608428, acc: 70.31%] [G loss: 2.420242]\n",
      "epoch:12 step:11282 [D loss: 0.694690, acc: 57.03%] [G loss: 2.287115]\n",
      "epoch:12 step:11283 [D loss: 0.638777, acc: 61.72%] [G loss: 1.962491]\n",
      "epoch:12 step:11284 [D loss: 0.590603, acc: 71.88%] [G loss: 2.158310]\n",
      "epoch:12 step:11285 [D loss: 0.653237, acc: 57.03%] [G loss: 1.872317]\n",
      "epoch:12 step:11286 [D loss: 0.581206, acc: 68.75%] [G loss: 2.057242]\n",
      "epoch:12 step:11287 [D loss: 0.632426, acc: 63.28%] [G loss: 2.021159]\n",
      "epoch:12 step:11288 [D loss: 0.629715, acc: 65.62%] [G loss: 1.927814]\n",
      "epoch:12 step:11289 [D loss: 0.643630, acc: 61.72%] [G loss: 2.050918]\n",
      "epoch:12 step:11290 [D loss: 0.658338, acc: 62.50%] [G loss: 1.960964]\n",
      "epoch:12 step:11291 [D loss: 0.594340, acc: 71.09%] [G loss: 2.003549]\n",
      "epoch:12 step:11292 [D loss: 0.595611, acc: 71.88%] [G loss: 2.218739]\n",
      "epoch:12 step:11293 [D loss: 0.632424, acc: 65.62%] [G loss: 2.038763]\n",
      "epoch:12 step:11294 [D loss: 0.598341, acc: 67.19%] [G loss: 2.083643]\n",
      "epoch:12 step:11295 [D loss: 0.657650, acc: 63.28%] [G loss: 2.170831]\n",
      "epoch:12 step:11296 [D loss: 0.641695, acc: 66.41%] [G loss: 2.148066]\n",
      "epoch:12 step:11297 [D loss: 0.629546, acc: 64.84%] [G loss: 2.237845]\n",
      "epoch:12 step:11298 [D loss: 0.640532, acc: 60.94%] [G loss: 2.019312]\n",
      "epoch:12 step:11299 [D loss: 0.639898, acc: 65.62%] [G loss: 2.039405]\n",
      "epoch:12 step:11300 [D loss: 0.622480, acc: 59.38%] [G loss: 2.202220]\n",
      "epoch:12 step:11301 [D loss: 0.631930, acc: 63.28%] [G loss: 2.054193]\n",
      "epoch:12 step:11302 [D loss: 0.593736, acc: 65.62%] [G loss: 2.136309]\n",
      "epoch:12 step:11303 [D loss: 0.687716, acc: 60.16%] [G loss: 1.922101]\n",
      "epoch:12 step:11304 [D loss: 0.636280, acc: 59.38%] [G loss: 1.894545]\n",
      "epoch:12 step:11305 [D loss: 0.620512, acc: 66.41%] [G loss: 1.989717]\n",
      "epoch:12 step:11306 [D loss: 0.645359, acc: 65.62%] [G loss: 2.084374]\n",
      "epoch:12 step:11307 [D loss: 0.655448, acc: 62.50%] [G loss: 1.893630]\n",
      "epoch:12 step:11308 [D loss: 0.683362, acc: 60.94%] [G loss: 1.867952]\n",
      "epoch:12 step:11309 [D loss: 0.634673, acc: 64.84%] [G loss: 1.921471]\n",
      "epoch:12 step:11310 [D loss: 0.615290, acc: 64.06%] [G loss: 1.986582]\n",
      "epoch:12 step:11311 [D loss: 0.673316, acc: 59.38%] [G loss: 2.117181]\n",
      "epoch:12 step:11312 [D loss: 0.604432, acc: 67.19%] [G loss: 2.071171]\n",
      "epoch:12 step:11313 [D loss: 0.608798, acc: 70.31%] [G loss: 2.141872]\n",
      "epoch:12 step:11314 [D loss: 0.587253, acc: 67.97%] [G loss: 2.212890]\n",
      "epoch:12 step:11315 [D loss: 0.671733, acc: 61.72%] [G loss: 2.034617]\n",
      "epoch:12 step:11316 [D loss: 0.628192, acc: 64.84%] [G loss: 2.057160]\n",
      "epoch:12 step:11317 [D loss: 0.627200, acc: 63.28%] [G loss: 2.049531]\n",
      "epoch:12 step:11318 [D loss: 0.548747, acc: 74.22%] [G loss: 2.145782]\n",
      "epoch:12 step:11319 [D loss: 0.610091, acc: 68.75%] [G loss: 2.382375]\n",
      "epoch:12 step:11320 [D loss: 0.597132, acc: 68.75%] [G loss: 2.270283]\n",
      "epoch:12 step:11321 [D loss: 0.577403, acc: 66.41%] [G loss: 2.397606]\n",
      "epoch:12 step:11322 [D loss: 0.673846, acc: 58.59%] [G loss: 2.213954]\n",
      "epoch:12 step:11323 [D loss: 0.620978, acc: 67.19%] [G loss: 2.054195]\n",
      "epoch:12 step:11324 [D loss: 0.657603, acc: 64.06%] [G loss: 1.983365]\n",
      "epoch:12 step:11325 [D loss: 0.647736, acc: 64.06%] [G loss: 1.816249]\n",
      "epoch:12 step:11326 [D loss: 0.635022, acc: 62.50%] [G loss: 2.031568]\n",
      "epoch:12 step:11327 [D loss: 0.616407, acc: 64.06%] [G loss: 2.204090]\n",
      "epoch:12 step:11328 [D loss: 0.671965, acc: 57.03%] [G loss: 1.986475]\n",
      "epoch:12 step:11329 [D loss: 0.640568, acc: 64.84%] [G loss: 1.903263]\n",
      "epoch:12 step:11330 [D loss: 0.644898, acc: 60.94%] [G loss: 1.937392]\n",
      "epoch:12 step:11331 [D loss: 0.657592, acc: 59.38%] [G loss: 1.883301]\n",
      "epoch:12 step:11332 [D loss: 0.660044, acc: 60.16%] [G loss: 2.107573]\n",
      "epoch:12 step:11333 [D loss: 0.670858, acc: 60.94%] [G loss: 1.943284]\n",
      "epoch:12 step:11334 [D loss: 0.607436, acc: 70.31%] [G loss: 1.969337]\n",
      "epoch:12 step:11335 [D loss: 0.644058, acc: 67.19%] [G loss: 1.972202]\n",
      "epoch:12 step:11336 [D loss: 0.659497, acc: 63.28%] [G loss: 2.067180]\n",
      "epoch:12 step:11337 [D loss: 0.595803, acc: 65.62%] [G loss: 1.976561]\n",
      "epoch:12 step:11338 [D loss: 0.620857, acc: 65.62%] [G loss: 2.026606]\n",
      "epoch:12 step:11339 [D loss: 0.674620, acc: 57.81%] [G loss: 1.861044]\n",
      "epoch:12 step:11340 [D loss: 0.654796, acc: 67.97%] [G loss: 1.979635]\n",
      "epoch:12 step:11341 [D loss: 0.642882, acc: 60.94%] [G loss: 1.987467]\n",
      "epoch:12 step:11342 [D loss: 0.652416, acc: 58.59%] [G loss: 1.880257]\n",
      "epoch:12 step:11343 [D loss: 0.684165, acc: 60.94%] [G loss: 1.892851]\n",
      "epoch:12 step:11344 [D loss: 0.594633, acc: 73.44%] [G loss: 2.037192]\n",
      "epoch:12 step:11345 [D loss: 0.605783, acc: 66.41%] [G loss: 2.019157]\n",
      "epoch:12 step:11346 [D loss: 0.682265, acc: 57.81%] [G loss: 1.942023]\n",
      "epoch:12 step:11347 [D loss: 0.630558, acc: 67.19%] [G loss: 2.084615]\n",
      "epoch:12 step:11348 [D loss: 0.660605, acc: 64.06%] [G loss: 2.108528]\n",
      "epoch:12 step:11349 [D loss: 0.682786, acc: 55.47%] [G loss: 2.026948]\n",
      "epoch:12 step:11350 [D loss: 0.654223, acc: 62.50%] [G loss: 1.998049]\n",
      "epoch:12 step:11351 [D loss: 0.619141, acc: 64.84%] [G loss: 2.042170]\n",
      "epoch:12 step:11352 [D loss: 0.724126, acc: 53.12%] [G loss: 1.759726]\n",
      "epoch:12 step:11353 [D loss: 0.644170, acc: 61.72%] [G loss: 1.862493]\n",
      "epoch:12 step:11354 [D loss: 0.592455, acc: 70.31%] [G loss: 1.791960]\n",
      "epoch:12 step:11355 [D loss: 0.627945, acc: 59.38%] [G loss: 2.143682]\n",
      "epoch:12 step:11356 [D loss: 0.659420, acc: 60.94%] [G loss: 1.906736]\n",
      "epoch:12 step:11357 [D loss: 0.624431, acc: 63.28%] [G loss: 2.078858]\n",
      "epoch:12 step:11358 [D loss: 0.601748, acc: 69.53%] [G loss: 1.957914]\n",
      "epoch:12 step:11359 [D loss: 0.617678, acc: 64.06%] [G loss: 2.107754]\n",
      "epoch:12 step:11360 [D loss: 0.653066, acc: 62.50%] [G loss: 2.000349]\n",
      "epoch:12 step:11361 [D loss: 0.609466, acc: 65.62%] [G loss: 2.249201]\n",
      "epoch:12 step:11362 [D loss: 0.629242, acc: 67.19%] [G loss: 2.279316]\n",
      "epoch:12 step:11363 [D loss: 0.608669, acc: 67.97%] [G loss: 2.355433]\n",
      "epoch:12 step:11364 [D loss: 0.682337, acc: 58.59%] [G loss: 2.062745]\n",
      "epoch:12 step:11365 [D loss: 0.655914, acc: 60.16%] [G loss: 1.990774]\n",
      "epoch:12 step:11366 [D loss: 0.625102, acc: 58.59%] [G loss: 2.198371]\n",
      "epoch:12 step:11367 [D loss: 0.623059, acc: 64.84%] [G loss: 1.990564]\n",
      "epoch:12 step:11368 [D loss: 0.628552, acc: 57.03%] [G loss: 2.160704]\n",
      "epoch:12 step:11369 [D loss: 0.670442, acc: 60.94%] [G loss: 1.871103]\n",
      "epoch:12 step:11370 [D loss: 0.624567, acc: 64.84%] [G loss: 2.039094]\n",
      "epoch:12 step:11371 [D loss: 0.705279, acc: 56.25%] [G loss: 1.944985]\n",
      "epoch:12 step:11372 [D loss: 0.638328, acc: 63.28%] [G loss: 1.903409]\n",
      "epoch:12 step:11373 [D loss: 0.620786, acc: 62.50%] [G loss: 1.801849]\n",
      "epoch:12 step:11374 [D loss: 0.638274, acc: 57.03%] [G loss: 1.988334]\n",
      "epoch:12 step:11375 [D loss: 0.595565, acc: 65.62%] [G loss: 2.083013]\n",
      "epoch:12 step:11376 [D loss: 0.620770, acc: 63.28%] [G loss: 1.866587]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:12 step:11377 [D loss: 0.704303, acc: 57.81%] [G loss: 1.824175]\n",
      "epoch:12 step:11378 [D loss: 0.702419, acc: 58.59%] [G loss: 1.848922]\n",
      "epoch:12 step:11379 [D loss: 0.645972, acc: 61.72%] [G loss: 2.074935]\n",
      "epoch:12 step:11380 [D loss: 0.707530, acc: 53.12%] [G loss: 1.725898]\n",
      "epoch:12 step:11381 [D loss: 0.666730, acc: 60.94%] [G loss: 1.953430]\n",
      "epoch:12 step:11382 [D loss: 0.667819, acc: 60.94%] [G loss: 1.857758]\n",
      "epoch:12 step:11383 [D loss: 0.642710, acc: 64.06%] [G loss: 1.957124]\n",
      "epoch:12 step:11384 [D loss: 0.640779, acc: 64.06%] [G loss: 1.841931]\n",
      "epoch:12 step:11385 [D loss: 0.647062, acc: 60.16%] [G loss: 1.958625]\n",
      "epoch:12 step:11386 [D loss: 0.637357, acc: 62.50%] [G loss: 1.842277]\n",
      "epoch:12 step:11387 [D loss: 0.660857, acc: 61.72%] [G loss: 1.919171]\n",
      "epoch:12 step:11388 [D loss: 0.624041, acc: 67.19%] [G loss: 2.153282]\n",
      "epoch:12 step:11389 [D loss: 0.642322, acc: 60.16%] [G loss: 1.803891]\n",
      "epoch:12 step:11390 [D loss: 0.612689, acc: 68.75%] [G loss: 2.009587]\n",
      "epoch:12 step:11391 [D loss: 0.679677, acc: 58.59%] [G loss: 1.793255]\n",
      "epoch:12 step:11392 [D loss: 0.640502, acc: 60.16%] [G loss: 1.961276]\n",
      "epoch:12 step:11393 [D loss: 0.682775, acc: 60.16%] [G loss: 2.029456]\n",
      "epoch:12 step:11394 [D loss: 0.635217, acc: 63.28%] [G loss: 2.039001]\n",
      "epoch:12 step:11395 [D loss: 0.574341, acc: 69.53%] [G loss: 2.257728]\n",
      "epoch:12 step:11396 [D loss: 0.616665, acc: 70.31%] [G loss: 2.101470]\n",
      "epoch:12 step:11397 [D loss: 0.622554, acc: 67.19%] [G loss: 2.051053]\n",
      "epoch:12 step:11398 [D loss: 0.596021, acc: 65.62%] [G loss: 2.200826]\n",
      "epoch:12 step:11399 [D loss: 0.602227, acc: 67.19%] [G loss: 2.030077]\n",
      "epoch:12 step:11400 [D loss: 0.621810, acc: 62.50%] [G loss: 1.945886]\n",
      "##############\n",
      "[2.44120656 1.31796395 6.35918018 4.89403129 3.78209281 5.87882323\n",
      " 4.32817766 4.93864425 4.51083095 3.6801121 ]\n",
      "##########\n",
      "epoch:12 step:11401 [D loss: 0.644869, acc: 57.03%] [G loss: 1.937660]\n",
      "epoch:12 step:11402 [D loss: 0.599623, acc: 69.53%] [G loss: 2.084868]\n",
      "epoch:12 step:11403 [D loss: 0.586243, acc: 67.19%] [G loss: 2.097337]\n",
      "epoch:12 step:11404 [D loss: 0.700067, acc: 57.81%] [G loss: 1.914343]\n",
      "epoch:12 step:11405 [D loss: 0.688030, acc: 59.38%] [G loss: 1.890329]\n",
      "epoch:12 step:11406 [D loss: 0.660740, acc: 63.28%] [G loss: 1.989445]\n",
      "epoch:12 step:11407 [D loss: 0.624261, acc: 67.19%] [G loss: 1.944495]\n",
      "epoch:12 step:11408 [D loss: 0.620450, acc: 66.41%] [G loss: 1.872142]\n",
      "epoch:12 step:11409 [D loss: 0.601837, acc: 75.00%] [G loss: 2.154469]\n",
      "epoch:12 step:11410 [D loss: 0.606142, acc: 67.19%] [G loss: 2.079541]\n",
      "epoch:12 step:11411 [D loss: 0.691502, acc: 64.06%] [G loss: 1.907372]\n",
      "epoch:12 step:11412 [D loss: 0.608388, acc: 66.41%] [G loss: 2.097318]\n",
      "epoch:12 step:11413 [D loss: 0.709020, acc: 55.47%] [G loss: 1.934980]\n",
      "epoch:12 step:11414 [D loss: 0.637395, acc: 64.06%] [G loss: 2.046020]\n",
      "epoch:12 step:11415 [D loss: 0.653705, acc: 67.97%] [G loss: 1.981655]\n",
      "epoch:12 step:11416 [D loss: 0.706980, acc: 59.38%] [G loss: 1.951334]\n",
      "epoch:12 step:11417 [D loss: 0.624671, acc: 67.19%] [G loss: 1.893304]\n",
      "epoch:12 step:11418 [D loss: 0.648210, acc: 62.50%] [G loss: 1.881642]\n",
      "epoch:12 step:11419 [D loss: 0.610244, acc: 69.53%] [G loss: 1.855385]\n",
      "epoch:12 step:11420 [D loss: 0.593829, acc: 66.41%] [G loss: 2.038951]\n",
      "epoch:12 step:11421 [D loss: 0.630009, acc: 65.62%] [G loss: 1.998960]\n",
      "epoch:12 step:11422 [D loss: 0.690800, acc: 62.50%] [G loss: 2.031976]\n",
      "epoch:12 step:11423 [D loss: 0.670690, acc: 62.50%] [G loss: 1.775331]\n",
      "epoch:12 step:11424 [D loss: 0.637954, acc: 58.59%] [G loss: 1.842621]\n",
      "epoch:12 step:11425 [D loss: 0.656272, acc: 59.38%] [G loss: 1.924472]\n",
      "epoch:12 step:11426 [D loss: 0.674949, acc: 61.72%] [G loss: 1.982100]\n",
      "epoch:12 step:11427 [D loss: 0.641615, acc: 60.94%] [G loss: 1.840845]\n",
      "epoch:12 step:11428 [D loss: 0.586937, acc: 72.66%] [G loss: 1.930152]\n",
      "epoch:12 step:11429 [D loss: 0.581706, acc: 68.75%] [G loss: 2.098931]\n",
      "epoch:12 step:11430 [D loss: 0.673436, acc: 56.25%] [G loss: 1.921517]\n",
      "epoch:12 step:11431 [D loss: 0.646702, acc: 62.50%] [G loss: 2.158467]\n",
      "epoch:12 step:11432 [D loss: 0.696189, acc: 53.91%] [G loss: 1.912253]\n",
      "epoch:12 step:11433 [D loss: 0.669910, acc: 64.06%] [G loss: 1.819896]\n",
      "epoch:12 step:11434 [D loss: 0.658803, acc: 56.25%] [G loss: 1.935154]\n",
      "epoch:12 step:11435 [D loss: 0.581292, acc: 68.75%] [G loss: 2.060694]\n",
      "epoch:12 step:11436 [D loss: 0.613379, acc: 70.31%] [G loss: 1.998781]\n",
      "epoch:12 step:11437 [D loss: 0.639723, acc: 60.94%] [G loss: 2.279265]\n",
      "epoch:12 step:11438 [D loss: 0.604613, acc: 65.62%] [G loss: 2.329525]\n",
      "epoch:12 step:11439 [D loss: 0.685710, acc: 55.47%] [G loss: 2.007438]\n",
      "epoch:12 step:11440 [D loss: 0.628938, acc: 65.62%] [G loss: 1.881415]\n",
      "epoch:12 step:11441 [D loss: 0.646696, acc: 60.16%] [G loss: 2.074564]\n",
      "epoch:12 step:11442 [D loss: 0.635424, acc: 63.28%] [G loss: 1.987668]\n",
      "epoch:12 step:11443 [D loss: 0.683530, acc: 60.94%] [G loss: 2.142003]\n",
      "epoch:12 step:11444 [D loss: 0.688225, acc: 58.59%] [G loss: 1.740443]\n",
      "epoch:12 step:11445 [D loss: 0.667019, acc: 60.94%] [G loss: 1.933733]\n",
      "epoch:12 step:11446 [D loss: 0.632242, acc: 65.62%] [G loss: 2.027571]\n",
      "epoch:12 step:11447 [D loss: 0.661766, acc: 57.03%] [G loss: 1.910565]\n",
      "epoch:12 step:11448 [D loss: 0.654172, acc: 57.03%] [G loss: 1.871298]\n",
      "epoch:12 step:11449 [D loss: 0.610576, acc: 66.41%] [G loss: 2.209153]\n",
      "epoch:12 step:11450 [D loss: 0.603341, acc: 66.41%] [G loss: 2.166173]\n",
      "epoch:12 step:11451 [D loss: 0.568703, acc: 69.53%] [G loss: 2.293499]\n",
      "epoch:12 step:11452 [D loss: 0.579957, acc: 66.41%] [G loss: 2.331080]\n",
      "epoch:12 step:11453 [D loss: 0.592944, acc: 64.84%] [G loss: 2.179746]\n",
      "epoch:12 step:11454 [D loss: 0.679137, acc: 64.06%] [G loss: 1.950493]\n",
      "epoch:12 step:11455 [D loss: 0.679447, acc: 57.81%] [G loss: 1.952198]\n",
      "epoch:12 step:11456 [D loss: 0.679207, acc: 56.25%] [G loss: 1.915001]\n",
      "epoch:12 step:11457 [D loss: 0.657362, acc: 57.03%] [G loss: 2.018877]\n",
      "epoch:12 step:11458 [D loss: 0.665540, acc: 62.50%] [G loss: 1.792050]\n",
      "epoch:12 step:11459 [D loss: 0.676479, acc: 60.16%] [G loss: 1.755662]\n",
      "epoch:12 step:11460 [D loss: 0.628784, acc: 64.06%] [G loss: 2.079957]\n",
      "epoch:12 step:11461 [D loss: 0.650742, acc: 64.06%] [G loss: 2.053258]\n",
      "epoch:12 step:11462 [D loss: 0.622514, acc: 71.88%] [G loss: 2.190621]\n",
      "epoch:12 step:11463 [D loss: 0.606314, acc: 69.53%] [G loss: 2.132828]\n",
      "epoch:12 step:11464 [D loss: 0.748297, acc: 53.91%] [G loss: 1.919393]\n",
      "epoch:12 step:11465 [D loss: 0.638049, acc: 62.50%] [G loss: 1.998392]\n",
      "epoch:12 step:11466 [D loss: 0.578721, acc: 70.31%] [G loss: 1.973641]\n",
      "epoch:12 step:11467 [D loss: 0.681737, acc: 63.28%] [G loss: 1.951248]\n",
      "epoch:12 step:11468 [D loss: 0.674638, acc: 58.59%] [G loss: 1.937972]\n",
      "epoch:12 step:11469 [D loss: 0.671072, acc: 54.69%] [G loss: 1.983375]\n",
      "epoch:12 step:11470 [D loss: 0.616712, acc: 66.41%] [G loss: 1.830312]\n",
      "epoch:12 step:11471 [D loss: 0.683706, acc: 60.94%] [G loss: 1.869199]\n",
      "epoch:12 step:11472 [D loss: 0.610472, acc: 69.53%] [G loss: 1.902614]\n",
      "epoch:12 step:11473 [D loss: 0.578041, acc: 70.31%] [G loss: 2.138457]\n",
      "epoch:12 step:11474 [D loss: 0.591479, acc: 67.19%] [G loss: 2.329185]\n",
      "epoch:12 step:11475 [D loss: 0.526325, acc: 67.97%] [G loss: 2.694516]\n",
      "epoch:12 step:11476 [D loss: 0.580260, acc: 69.53%] [G loss: 2.471405]\n",
      "epoch:12 step:11477 [D loss: 0.680414, acc: 60.94%] [G loss: 2.035659]\n",
      "epoch:12 step:11478 [D loss: 0.622625, acc: 61.72%] [G loss: 2.144077]\n",
      "epoch:12 step:11479 [D loss: 0.737003, acc: 56.25%] [G loss: 1.897697]\n",
      "epoch:12 step:11480 [D loss: 0.640358, acc: 61.72%] [G loss: 1.934432]\n",
      "epoch:12 step:11481 [D loss: 0.633451, acc: 64.84%] [G loss: 2.086908]\n",
      "epoch:12 step:11482 [D loss: 0.653304, acc: 62.50%] [G loss: 2.056182]\n",
      "epoch:12 step:11483 [D loss: 0.620604, acc: 66.41%] [G loss: 2.054950]\n",
      "epoch:12 step:11484 [D loss: 0.714183, acc: 54.69%] [G loss: 1.865225]\n",
      "epoch:12 step:11485 [D loss: 0.637747, acc: 68.75%] [G loss: 2.081972]\n",
      "epoch:12 step:11486 [D loss: 0.599618, acc: 69.53%] [G loss: 2.021621]\n",
      "epoch:12 step:11487 [D loss: 0.630369, acc: 68.75%] [G loss: 1.986928]\n",
      "epoch:12 step:11488 [D loss: 0.659729, acc: 59.38%] [G loss: 1.961104]\n",
      "epoch:12 step:11489 [D loss: 0.635990, acc: 66.41%] [G loss: 2.073669]\n",
      "epoch:12 step:11490 [D loss: 0.674134, acc: 58.59%] [G loss: 2.078701]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:12 step:11491 [D loss: 0.705000, acc: 57.03%] [G loss: 1.934765]\n",
      "epoch:12 step:11492 [D loss: 0.616667, acc: 67.97%] [G loss: 2.134809]\n",
      "epoch:12 step:11493 [D loss: 0.667769, acc: 59.38%] [G loss: 1.844072]\n",
      "epoch:12 step:11494 [D loss: 0.723092, acc: 53.12%] [G loss: 1.757441]\n",
      "epoch:12 step:11495 [D loss: 0.619110, acc: 67.19%] [G loss: 1.908729]\n",
      "epoch:12 step:11496 [D loss: 0.731215, acc: 57.81%] [G loss: 1.795986]\n",
      "epoch:12 step:11497 [D loss: 0.638695, acc: 66.41%] [G loss: 1.854002]\n",
      "epoch:12 step:11498 [D loss: 0.631502, acc: 60.94%] [G loss: 1.804906]\n",
      "epoch:12 step:11499 [D loss: 0.611953, acc: 65.62%] [G loss: 1.945162]\n",
      "epoch:12 step:11500 [D loss: 0.676653, acc: 60.16%] [G loss: 1.985776]\n",
      "epoch:12 step:11501 [D loss: 0.635865, acc: 64.84%] [G loss: 1.883343]\n",
      "epoch:12 step:11502 [D loss: 0.656344, acc: 62.50%] [G loss: 1.875278]\n",
      "epoch:12 step:11503 [D loss: 0.666909, acc: 61.72%] [G loss: 1.854157]\n",
      "epoch:12 step:11504 [D loss: 0.589521, acc: 68.75%] [G loss: 1.746707]\n",
      "epoch:12 step:11505 [D loss: 0.580461, acc: 69.53%] [G loss: 2.015179]\n",
      "epoch:12 step:11506 [D loss: 0.601918, acc: 66.41%] [G loss: 2.115077]\n",
      "epoch:12 step:11507 [D loss: 0.701253, acc: 57.81%] [G loss: 1.994179]\n",
      "epoch:12 step:11508 [D loss: 0.638099, acc: 58.59%] [G loss: 1.994433]\n",
      "epoch:12 step:11509 [D loss: 0.650499, acc: 61.72%] [G loss: 1.840339]\n",
      "epoch:12 step:11510 [D loss: 0.658230, acc: 64.06%] [G loss: 2.054484]\n",
      "epoch:12 step:11511 [D loss: 0.632064, acc: 64.06%] [G loss: 2.023875]\n",
      "epoch:12 step:11512 [D loss: 0.606470, acc: 65.62%] [G loss: 2.000831]\n",
      "epoch:12 step:11513 [D loss: 0.639333, acc: 61.72%] [G loss: 1.989083]\n",
      "epoch:12 step:11514 [D loss: 0.608816, acc: 67.19%] [G loss: 2.075524]\n",
      "epoch:12 step:11515 [D loss: 0.634272, acc: 61.72%] [G loss: 2.136633]\n",
      "epoch:12 step:11516 [D loss: 0.626280, acc: 64.06%] [G loss: 2.006484]\n",
      "epoch:12 step:11517 [D loss: 0.587025, acc: 73.44%] [G loss: 2.017286]\n",
      "epoch:12 step:11518 [D loss: 0.646028, acc: 62.50%] [G loss: 1.990560]\n",
      "epoch:12 step:11519 [D loss: 0.600546, acc: 68.75%] [G loss: 2.034826]\n",
      "epoch:12 step:11520 [D loss: 0.607867, acc: 66.41%] [G loss: 2.000591]\n",
      "epoch:12 step:11521 [D loss: 0.631823, acc: 65.62%] [G loss: 2.066699]\n",
      "epoch:12 step:11522 [D loss: 0.625685, acc: 64.06%] [G loss: 2.065477]\n",
      "epoch:12 step:11523 [D loss: 0.636879, acc: 58.59%] [G loss: 1.964713]\n",
      "epoch:12 step:11524 [D loss: 0.614407, acc: 64.06%] [G loss: 2.093640]\n",
      "epoch:12 step:11525 [D loss: 0.693641, acc: 57.03%] [G loss: 1.800511]\n",
      "epoch:12 step:11526 [D loss: 0.678180, acc: 58.59%] [G loss: 1.895394]\n",
      "epoch:12 step:11527 [D loss: 0.589592, acc: 70.31%] [G loss: 1.983138]\n",
      "epoch:12 step:11528 [D loss: 0.628045, acc: 61.72%] [G loss: 1.944615]\n",
      "epoch:12 step:11529 [D loss: 0.625942, acc: 64.06%] [G loss: 1.917679]\n",
      "epoch:12 step:11530 [D loss: 0.586815, acc: 64.06%] [G loss: 1.900723]\n",
      "epoch:12 step:11531 [D loss: 0.665606, acc: 70.31%] [G loss: 2.046232]\n",
      "epoch:12 step:11532 [D loss: 0.710592, acc: 55.47%] [G loss: 1.960157]\n",
      "epoch:12 step:11533 [D loss: 0.646763, acc: 60.16%] [G loss: 1.934621]\n",
      "epoch:12 step:11534 [D loss: 0.627936, acc: 65.62%] [G loss: 1.995190]\n",
      "epoch:12 step:11535 [D loss: 0.670711, acc: 60.94%] [G loss: 2.115274]\n",
      "epoch:12 step:11536 [D loss: 0.609352, acc: 64.84%] [G loss: 2.041308]\n",
      "epoch:12 step:11537 [D loss: 0.619560, acc: 64.84%] [G loss: 2.243952]\n",
      "epoch:12 step:11538 [D loss: 0.597312, acc: 66.41%] [G loss: 1.970097]\n",
      "epoch:12 step:11539 [D loss: 0.612507, acc: 64.06%] [G loss: 1.813763]\n",
      "epoch:12 step:11540 [D loss: 0.590799, acc: 71.88%] [G loss: 2.140132]\n",
      "epoch:12 step:11541 [D loss: 0.689554, acc: 57.81%] [G loss: 1.811722]\n",
      "epoch:12 step:11542 [D loss: 0.580379, acc: 72.66%] [G loss: 2.002995]\n",
      "epoch:12 step:11543 [D loss: 0.588225, acc: 67.19%] [G loss: 2.023705]\n",
      "epoch:12 step:11544 [D loss: 0.620041, acc: 64.84%] [G loss: 2.074693]\n",
      "epoch:12 step:11545 [D loss: 0.606828, acc: 67.97%] [G loss: 1.942531]\n",
      "epoch:12 step:11546 [D loss: 0.649531, acc: 60.16%] [G loss: 2.039093]\n",
      "epoch:12 step:11547 [D loss: 0.597976, acc: 71.09%] [G loss: 1.879187]\n",
      "epoch:12 step:11548 [D loss: 0.642811, acc: 61.72%] [G loss: 1.901876]\n",
      "epoch:12 step:11549 [D loss: 0.603593, acc: 62.50%] [G loss: 1.993229]\n",
      "epoch:12 step:11550 [D loss: 0.638301, acc: 67.97%] [G loss: 1.881487]\n",
      "epoch:12 step:11551 [D loss: 0.649113, acc: 59.38%] [G loss: 2.043106]\n",
      "epoch:12 step:11552 [D loss: 0.573665, acc: 71.88%] [G loss: 2.042178]\n",
      "epoch:12 step:11553 [D loss: 0.586365, acc: 71.88%] [G loss: 2.178897]\n",
      "epoch:12 step:11554 [D loss: 0.641636, acc: 65.62%] [G loss: 1.947006]\n",
      "epoch:12 step:11555 [D loss: 0.613070, acc: 68.75%] [G loss: 2.025815]\n",
      "epoch:12 step:11556 [D loss: 0.552036, acc: 73.44%] [G loss: 2.285739]\n",
      "epoch:12 step:11557 [D loss: 0.617913, acc: 67.19%] [G loss: 2.186093]\n",
      "epoch:12 step:11558 [D loss: 0.538979, acc: 73.44%] [G loss: 2.305717]\n",
      "epoch:12 step:11559 [D loss: 0.566076, acc: 78.91%] [G loss: 2.389175]\n",
      "epoch:12 step:11560 [D loss: 0.692197, acc: 57.81%] [G loss: 1.898679]\n",
      "epoch:12 step:11561 [D loss: 0.715139, acc: 58.59%] [G loss: 1.905137]\n",
      "epoch:12 step:11562 [D loss: 0.584198, acc: 67.97%] [G loss: 2.161373]\n",
      "epoch:12 step:11563 [D loss: 0.616508, acc: 62.50%] [G loss: 1.942524]\n",
      "epoch:12 step:11564 [D loss: 0.590315, acc: 67.97%] [G loss: 1.982326]\n",
      "epoch:12 step:11565 [D loss: 0.610587, acc: 70.31%] [G loss: 2.044977]\n",
      "epoch:12 step:11566 [D loss: 0.617583, acc: 63.28%] [G loss: 1.996069]\n",
      "epoch:12 step:11567 [D loss: 0.653743, acc: 60.16%] [G loss: 1.817617]\n",
      "epoch:12 step:11568 [D loss: 0.612605, acc: 66.41%] [G loss: 2.025177]\n",
      "epoch:12 step:11569 [D loss: 0.648430, acc: 62.50%] [G loss: 2.099352]\n",
      "epoch:12 step:11570 [D loss: 0.665221, acc: 63.28%] [G loss: 1.853925]\n",
      "epoch:12 step:11571 [D loss: 0.657894, acc: 60.94%] [G loss: 2.053462]\n",
      "epoch:12 step:11572 [D loss: 0.630977, acc: 64.84%] [G loss: 2.011587]\n",
      "epoch:12 step:11573 [D loss: 0.632041, acc: 65.62%] [G loss: 1.966695]\n",
      "epoch:12 step:11574 [D loss: 0.608618, acc: 70.31%] [G loss: 2.089978]\n",
      "epoch:12 step:11575 [D loss: 0.672090, acc: 64.06%] [G loss: 1.831393]\n",
      "epoch:12 step:11576 [D loss: 0.587766, acc: 69.53%] [G loss: 2.089614]\n",
      "epoch:12 step:11577 [D loss: 0.637992, acc: 68.75%] [G loss: 2.006271]\n",
      "epoch:12 step:11578 [D loss: 0.611133, acc: 67.97%] [G loss: 2.141368]\n",
      "epoch:12 step:11579 [D loss: 0.594817, acc: 69.53%] [G loss: 2.102923]\n",
      "epoch:12 step:11580 [D loss: 0.646241, acc: 59.38%] [G loss: 2.101018]\n",
      "epoch:12 step:11581 [D loss: 0.620891, acc: 66.41%] [G loss: 2.045385]\n",
      "epoch:12 step:11582 [D loss: 0.639463, acc: 67.19%] [G loss: 2.205701]\n",
      "epoch:12 step:11583 [D loss: 0.590237, acc: 67.97%] [G loss: 2.067312]\n",
      "epoch:12 step:11584 [D loss: 0.572690, acc: 71.88%] [G loss: 2.079535]\n",
      "epoch:12 step:11585 [D loss: 0.700249, acc: 57.81%] [G loss: 1.819147]\n",
      "epoch:12 step:11586 [D loss: 0.692356, acc: 53.91%] [G loss: 1.841272]\n",
      "epoch:12 step:11587 [D loss: 0.599979, acc: 64.06%] [G loss: 2.131348]\n",
      "epoch:12 step:11588 [D loss: 0.616368, acc: 60.94%] [G loss: 2.121732]\n",
      "epoch:12 step:11589 [D loss: 0.590980, acc: 67.19%] [G loss: 2.241505]\n",
      "epoch:12 step:11590 [D loss: 0.565844, acc: 71.88%] [G loss: 2.248212]\n",
      "epoch:12 step:11591 [D loss: 0.559379, acc: 70.31%] [G loss: 2.691292]\n",
      "epoch:12 step:11592 [D loss: 0.621496, acc: 67.19%] [G loss: 2.176325]\n",
      "epoch:12 step:11593 [D loss: 0.681673, acc: 55.47%] [G loss: 1.924552]\n",
      "epoch:12 step:11594 [D loss: 0.634918, acc: 62.50%] [G loss: 2.152693]\n",
      "epoch:12 step:11595 [D loss: 0.659775, acc: 57.03%] [G loss: 1.971139]\n",
      "epoch:12 step:11596 [D loss: 0.671599, acc: 62.50%] [G loss: 1.971677]\n",
      "epoch:12 step:11597 [D loss: 0.619349, acc: 63.28%] [G loss: 2.145414]\n",
      "epoch:12 step:11598 [D loss: 0.603948, acc: 64.06%] [G loss: 2.296526]\n",
      "epoch:12 step:11599 [D loss: 0.658669, acc: 59.38%] [G loss: 1.857426]\n",
      "epoch:12 step:11600 [D loss: 0.628103, acc: 65.62%] [G loss: 1.921938]\n",
      "##############\n",
      "[2.58656265 1.14246662 6.55888437 4.82232612 3.72455555 5.83561625\n",
      " 4.45966602 4.76343075 4.73362947 3.80630666]\n",
      "##########\n",
      "epoch:12 step:11601 [D loss: 0.641412, acc: 62.50%] [G loss: 2.123749]\n",
      "epoch:12 step:11602 [D loss: 0.547439, acc: 74.22%] [G loss: 2.249820]\n",
      "epoch:12 step:11603 [D loss: 0.574221, acc: 73.44%] [G loss: 2.188500]\n",
      "epoch:12 step:11604 [D loss: 0.618721, acc: 68.75%] [G loss: 1.932329]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:12 step:11605 [D loss: 0.612686, acc: 64.84%] [G loss: 2.172250]\n",
      "epoch:12 step:11606 [D loss: 0.633602, acc: 65.62%] [G loss: 2.029530]\n",
      "epoch:12 step:11607 [D loss: 0.646711, acc: 61.72%] [G loss: 1.976748]\n",
      "epoch:12 step:11608 [D loss: 0.555765, acc: 71.09%] [G loss: 2.238018]\n",
      "epoch:12 step:11609 [D loss: 0.600582, acc: 66.41%] [G loss: 2.189407]\n",
      "epoch:12 step:11610 [D loss: 0.615357, acc: 63.28%] [G loss: 2.039110]\n",
      "epoch:12 step:11611 [D loss: 0.645769, acc: 67.19%] [G loss: 2.095759]\n",
      "epoch:12 step:11612 [D loss: 0.651565, acc: 61.72%] [G loss: 1.999654]\n",
      "epoch:12 step:11613 [D loss: 0.590281, acc: 67.19%] [G loss: 2.104991]\n",
      "epoch:12 step:11614 [D loss: 0.630219, acc: 71.09%] [G loss: 2.052358]\n",
      "epoch:12 step:11615 [D loss: 0.625699, acc: 61.72%] [G loss: 2.099860]\n",
      "epoch:12 step:11616 [D loss: 0.609529, acc: 69.53%] [G loss: 1.870826]\n",
      "epoch:12 step:11617 [D loss: 0.721949, acc: 51.56%] [G loss: 1.874503]\n",
      "epoch:12 step:11618 [D loss: 0.675450, acc: 57.81%] [G loss: 2.043650]\n",
      "epoch:12 step:11619 [D loss: 0.637397, acc: 64.06%] [G loss: 2.027910]\n",
      "epoch:12 step:11620 [D loss: 0.684647, acc: 59.38%] [G loss: 2.005947]\n",
      "epoch:12 step:11621 [D loss: 0.679445, acc: 62.50%] [G loss: 1.807784]\n",
      "epoch:12 step:11622 [D loss: 0.677352, acc: 59.38%] [G loss: 1.858081]\n",
      "epoch:12 step:11623 [D loss: 0.614844, acc: 63.28%] [G loss: 1.899668]\n",
      "epoch:12 step:11624 [D loss: 0.556285, acc: 71.88%] [G loss: 2.132601]\n",
      "epoch:12 step:11625 [D loss: 0.643616, acc: 61.72%] [G loss: 2.096374]\n",
      "epoch:12 step:11626 [D loss: 0.690086, acc: 60.16%] [G loss: 1.791742]\n",
      "epoch:12 step:11627 [D loss: 0.694570, acc: 56.25%] [G loss: 1.908033]\n",
      "epoch:12 step:11628 [D loss: 0.653028, acc: 59.38%] [G loss: 2.059956]\n",
      "epoch:12 step:11629 [D loss: 0.577700, acc: 70.31%] [G loss: 1.931627]\n",
      "epoch:12 step:11630 [D loss: 0.656881, acc: 62.50%] [G loss: 1.942600]\n",
      "epoch:12 step:11631 [D loss: 0.626106, acc: 64.06%] [G loss: 1.950552]\n",
      "epoch:12 step:11632 [D loss: 0.598198, acc: 73.44%] [G loss: 2.031549]\n",
      "epoch:12 step:11633 [D loss: 0.621999, acc: 63.28%] [G loss: 1.957447]\n",
      "epoch:12 step:11634 [D loss: 0.629926, acc: 65.62%] [G loss: 1.974660]\n",
      "epoch:12 step:11635 [D loss: 0.680371, acc: 57.81%] [G loss: 1.947483]\n",
      "epoch:12 step:11636 [D loss: 0.646394, acc: 59.38%] [G loss: 1.899514]\n",
      "epoch:12 step:11637 [D loss: 0.652470, acc: 58.59%] [G loss: 1.923283]\n",
      "epoch:12 step:11638 [D loss: 0.708433, acc: 50.78%] [G loss: 2.046519]\n",
      "epoch:12 step:11639 [D loss: 0.656296, acc: 60.94%] [G loss: 2.073219]\n",
      "epoch:12 step:11640 [D loss: 0.712268, acc: 53.12%] [G loss: 1.903658]\n",
      "epoch:12 step:11641 [D loss: 0.684523, acc: 56.25%] [G loss: 1.961181]\n",
      "epoch:12 step:11642 [D loss: 0.610471, acc: 68.75%] [G loss: 2.028873]\n",
      "epoch:12 step:11643 [D loss: 0.616145, acc: 64.06%] [G loss: 1.891935]\n",
      "epoch:12 step:11644 [D loss: 0.613383, acc: 63.28%] [G loss: 1.909754]\n",
      "epoch:12 step:11645 [D loss: 0.644714, acc: 60.16%] [G loss: 1.980232]\n",
      "epoch:12 step:11646 [D loss: 0.626908, acc: 64.84%] [G loss: 2.032901]\n",
      "epoch:12 step:11647 [D loss: 0.666060, acc: 58.59%] [G loss: 2.196205]\n",
      "epoch:12 step:11648 [D loss: 0.681266, acc: 58.59%] [G loss: 1.907597]\n",
      "epoch:12 step:11649 [D loss: 0.598403, acc: 70.31%] [G loss: 2.070371]\n",
      "epoch:12 step:11650 [D loss: 0.651477, acc: 62.50%] [G loss: 2.043986]\n",
      "epoch:12 step:11651 [D loss: 0.669729, acc: 60.16%] [G loss: 1.927465]\n",
      "epoch:12 step:11652 [D loss: 0.644569, acc: 67.97%] [G loss: 1.980821]\n",
      "epoch:12 step:11653 [D loss: 0.629413, acc: 62.50%] [G loss: 2.060127]\n",
      "epoch:12 step:11654 [D loss: 0.645283, acc: 57.81%] [G loss: 1.949635]\n",
      "epoch:12 step:11655 [D loss: 0.606368, acc: 65.62%] [G loss: 1.993018]\n",
      "epoch:12 step:11656 [D loss: 0.634607, acc: 63.28%] [G loss: 2.018445]\n",
      "epoch:12 step:11657 [D loss: 0.565856, acc: 70.31%] [G loss: 2.116463]\n",
      "epoch:12 step:11658 [D loss: 0.649324, acc: 63.28%] [G loss: 2.053558]\n",
      "epoch:12 step:11659 [D loss: 0.643745, acc: 63.28%] [G loss: 2.019582]\n",
      "epoch:12 step:11660 [D loss: 0.668735, acc: 62.50%] [G loss: 2.033328]\n",
      "epoch:12 step:11661 [D loss: 0.649367, acc: 62.50%] [G loss: 2.267010]\n",
      "epoch:12 step:11662 [D loss: 0.629259, acc: 67.97%] [G loss: 1.916047]\n",
      "epoch:12 step:11663 [D loss: 0.635272, acc: 61.72%] [G loss: 2.002545]\n",
      "epoch:12 step:11664 [D loss: 0.673602, acc: 60.94%] [G loss: 2.043379]\n",
      "epoch:12 step:11665 [D loss: 0.616421, acc: 71.88%] [G loss: 1.910774]\n",
      "epoch:12 step:11666 [D loss: 0.646792, acc: 65.62%] [G loss: 2.046761]\n",
      "epoch:12 step:11667 [D loss: 0.663431, acc: 64.06%] [G loss: 1.881875]\n",
      "epoch:12 step:11668 [D loss: 0.616744, acc: 64.84%] [G loss: 1.887859]\n",
      "epoch:12 step:11669 [D loss: 0.697094, acc: 59.38%] [G loss: 1.995214]\n",
      "epoch:12 step:11670 [D loss: 0.648794, acc: 60.94%] [G loss: 1.862148]\n",
      "epoch:12 step:11671 [D loss: 0.649804, acc: 59.38%] [G loss: 2.104426]\n",
      "epoch:12 step:11672 [D loss: 0.610710, acc: 68.75%] [G loss: 2.162646]\n",
      "epoch:12 step:11673 [D loss: 0.619228, acc: 67.97%] [G loss: 2.189651]\n",
      "epoch:12 step:11674 [D loss: 0.578148, acc: 69.53%] [G loss: 2.115803]\n",
      "epoch:12 step:11675 [D loss: 0.634697, acc: 58.59%] [G loss: 2.068706]\n",
      "epoch:12 step:11676 [D loss: 0.677990, acc: 57.81%] [G loss: 2.006066]\n",
      "epoch:12 step:11677 [D loss: 0.652989, acc: 58.59%] [G loss: 1.960225]\n",
      "epoch:12 step:11678 [D loss: 0.613129, acc: 70.31%] [G loss: 2.193602]\n",
      "epoch:12 step:11679 [D loss: 0.642053, acc: 63.28%] [G loss: 2.173932]\n",
      "epoch:12 step:11680 [D loss: 0.613541, acc: 61.72%] [G loss: 1.948227]\n",
      "epoch:12 step:11681 [D loss: 0.725222, acc: 50.00%] [G loss: 1.818828]\n",
      "epoch:12 step:11682 [D loss: 0.634539, acc: 69.53%] [G loss: 1.874844]\n",
      "epoch:12 step:11683 [D loss: 0.650251, acc: 59.38%] [G loss: 1.939752]\n",
      "epoch:12 step:11684 [D loss: 0.649277, acc: 60.16%] [G loss: 1.903803]\n",
      "epoch:12 step:11685 [D loss: 0.673522, acc: 61.72%] [G loss: 1.852508]\n",
      "epoch:12 step:11686 [D loss: 0.673424, acc: 55.47%] [G loss: 1.953928]\n",
      "epoch:12 step:11687 [D loss: 0.688525, acc: 60.94%] [G loss: 1.934926]\n",
      "epoch:12 step:11688 [D loss: 0.579631, acc: 68.75%] [G loss: 1.759404]\n",
      "epoch:12 step:11689 [D loss: 0.637683, acc: 62.50%] [G loss: 2.056754]\n",
      "epoch:12 step:11690 [D loss: 0.642276, acc: 60.94%] [G loss: 2.004071]\n",
      "epoch:12 step:11691 [D loss: 0.573990, acc: 69.53%] [G loss: 2.085591]\n",
      "epoch:12 step:11692 [D loss: 0.671650, acc: 60.94%] [G loss: 1.790316]\n",
      "epoch:12 step:11693 [D loss: 0.591943, acc: 69.53%] [G loss: 1.974636]\n",
      "epoch:12 step:11694 [D loss: 0.636981, acc: 64.84%] [G loss: 2.010176]\n",
      "epoch:12 step:11695 [D loss: 0.652155, acc: 64.06%] [G loss: 2.176613]\n",
      "epoch:12 step:11696 [D loss: 0.643514, acc: 56.25%] [G loss: 2.040121]\n",
      "epoch:12 step:11697 [D loss: 0.592240, acc: 64.84%] [G loss: 2.008852]\n",
      "epoch:12 step:11698 [D loss: 0.622363, acc: 61.72%] [G loss: 1.932290]\n",
      "epoch:12 step:11699 [D loss: 0.647545, acc: 64.06%] [G loss: 1.912743]\n",
      "epoch:12 step:11700 [D loss: 0.621011, acc: 68.75%] [G loss: 1.966235]\n",
      "epoch:12 step:11701 [D loss: 0.593586, acc: 64.84%] [G loss: 1.957594]\n",
      "epoch:12 step:11702 [D loss: 0.682019, acc: 62.50%] [G loss: 1.950810]\n",
      "epoch:12 step:11703 [D loss: 0.688050, acc: 59.38%] [G loss: 1.880969]\n",
      "epoch:12 step:11704 [D loss: 0.699154, acc: 54.69%] [G loss: 1.766371]\n",
      "epoch:12 step:11705 [D loss: 0.665180, acc: 60.94%] [G loss: 1.910450]\n",
      "epoch:12 step:11706 [D loss: 0.639403, acc: 64.06%] [G loss: 1.911767]\n",
      "epoch:12 step:11707 [D loss: 0.587799, acc: 67.97%] [G loss: 1.855482]\n",
      "epoch:12 step:11708 [D loss: 0.603499, acc: 68.75%] [G loss: 2.021012]\n",
      "epoch:12 step:11709 [D loss: 0.664119, acc: 58.59%] [G loss: 1.947853]\n",
      "epoch:12 step:11710 [D loss: 0.710649, acc: 54.69%] [G loss: 1.911695]\n",
      "epoch:12 step:11711 [D loss: 0.649024, acc: 64.06%] [G loss: 2.006678]\n",
      "epoch:12 step:11712 [D loss: 0.675193, acc: 56.25%] [G loss: 2.240248]\n",
      "epoch:12 step:11713 [D loss: 0.572651, acc: 70.31%] [G loss: 1.993430]\n",
      "epoch:12 step:11714 [D loss: 0.616030, acc: 64.84%] [G loss: 2.113301]\n",
      "epoch:12 step:11715 [D loss: 0.597308, acc: 67.97%] [G loss: 2.187750]\n",
      "epoch:12 step:11716 [D loss: 0.605556, acc: 64.06%] [G loss: 2.269863]\n",
      "epoch:12 step:11717 [D loss: 0.666450, acc: 64.84%] [G loss: 1.963578]\n",
      "epoch:12 step:11718 [D loss: 0.653237, acc: 60.16%] [G loss: 1.865260]\n",
      "epoch:12 step:11719 [D loss: 0.688233, acc: 57.81%] [G loss: 2.074339]\n",
      "epoch:12 step:11720 [D loss: 0.640873, acc: 62.50%] [G loss: 1.916582]\n",
      "epoch:12 step:11721 [D loss: 0.682944, acc: 58.59%] [G loss: 1.993333]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:12 step:11722 [D loss: 0.617300, acc: 68.75%] [G loss: 1.816905]\n",
      "epoch:12 step:11723 [D loss: 0.599092, acc: 70.31%] [G loss: 1.957890]\n",
      "epoch:12 step:11724 [D loss: 0.650079, acc: 63.28%] [G loss: 2.039330]\n",
      "epoch:12 step:11725 [D loss: 0.581624, acc: 70.31%] [G loss: 2.149153]\n",
      "epoch:12 step:11726 [D loss: 0.687579, acc: 56.25%] [G loss: 1.938414]\n",
      "epoch:12 step:11727 [D loss: 0.624713, acc: 65.62%] [G loss: 1.964076]\n",
      "epoch:12 step:11728 [D loss: 0.657677, acc: 58.59%] [G loss: 1.949056]\n",
      "epoch:12 step:11729 [D loss: 0.653717, acc: 57.81%] [G loss: 1.927356]\n",
      "epoch:12 step:11730 [D loss: 0.673405, acc: 64.06%] [G loss: 1.837045]\n",
      "epoch:12 step:11731 [D loss: 0.712018, acc: 59.38%] [G loss: 1.790025]\n",
      "epoch:12 step:11732 [D loss: 0.607046, acc: 67.97%] [G loss: 2.027426]\n",
      "epoch:12 step:11733 [D loss: 0.619249, acc: 71.09%] [G loss: 1.923503]\n",
      "epoch:12 step:11734 [D loss: 0.671788, acc: 59.38%] [G loss: 1.895783]\n",
      "epoch:12 step:11735 [D loss: 0.685081, acc: 61.72%] [G loss: 2.003926]\n",
      "epoch:12 step:11736 [D loss: 0.636989, acc: 63.28%] [G loss: 1.844058]\n",
      "epoch:12 step:11737 [D loss: 0.652237, acc: 65.62%] [G loss: 1.890821]\n",
      "epoch:12 step:11738 [D loss: 0.652387, acc: 64.06%] [G loss: 2.057272]\n",
      "epoch:12 step:11739 [D loss: 0.589484, acc: 71.88%] [G loss: 2.050475]\n",
      "epoch:12 step:11740 [D loss: 0.586213, acc: 68.75%] [G loss: 1.981954]\n",
      "epoch:12 step:11741 [D loss: 0.677656, acc: 60.16%] [G loss: 2.083067]\n",
      "epoch:12 step:11742 [D loss: 0.619784, acc: 64.84%] [G loss: 1.999818]\n",
      "epoch:12 step:11743 [D loss: 0.562299, acc: 71.88%] [G loss: 2.210079]\n",
      "epoch:12 step:11744 [D loss: 0.737390, acc: 57.81%] [G loss: 1.779127]\n",
      "epoch:12 step:11745 [D loss: 0.745419, acc: 52.34%] [G loss: 1.805838]\n",
      "epoch:12 step:11746 [D loss: 0.693587, acc: 54.69%] [G loss: 1.784119]\n",
      "epoch:12 step:11747 [D loss: 0.670409, acc: 60.94%] [G loss: 1.908599]\n",
      "epoch:12 step:11748 [D loss: 0.612937, acc: 67.97%] [G loss: 2.035930]\n",
      "epoch:12 step:11749 [D loss: 0.611045, acc: 69.53%] [G loss: 1.984237]\n",
      "epoch:12 step:11750 [D loss: 0.623210, acc: 65.62%] [G loss: 1.784811]\n",
      "epoch:12 step:11751 [D loss: 0.670960, acc: 56.25%] [G loss: 1.788768]\n",
      "epoch:12 step:11752 [D loss: 0.601573, acc: 73.44%] [G loss: 2.140969]\n",
      "epoch:12 step:11753 [D loss: 0.610013, acc: 71.88%] [G loss: 1.938203]\n",
      "epoch:12 step:11754 [D loss: 0.742558, acc: 51.56%] [G loss: 1.940371]\n",
      "epoch:12 step:11755 [D loss: 0.649973, acc: 64.84%] [G loss: 1.748645]\n",
      "epoch:12 step:11756 [D loss: 0.620666, acc: 64.84%] [G loss: 1.832593]\n",
      "epoch:12 step:11757 [D loss: 0.610649, acc: 63.28%] [G loss: 1.983923]\n",
      "epoch:12 step:11758 [D loss: 0.676307, acc: 64.06%] [G loss: 1.876087]\n",
      "epoch:12 step:11759 [D loss: 0.630860, acc: 64.06%] [G loss: 2.007462]\n",
      "epoch:12 step:11760 [D loss: 0.575290, acc: 65.62%] [G loss: 2.010431]\n",
      "epoch:12 step:11761 [D loss: 0.660398, acc: 59.38%] [G loss: 1.947214]\n",
      "epoch:12 step:11762 [D loss: 0.634072, acc: 65.62%] [G loss: 2.034901]\n",
      "epoch:12 step:11763 [D loss: 0.627973, acc: 61.72%] [G loss: 1.924506]\n",
      "epoch:12 step:11764 [D loss: 0.642335, acc: 60.16%] [G loss: 1.991793]\n",
      "epoch:12 step:11765 [D loss: 0.686062, acc: 57.03%] [G loss: 1.946493]\n",
      "epoch:12 step:11766 [D loss: 0.611103, acc: 67.19%] [G loss: 1.986683]\n",
      "epoch:12 step:11767 [D loss: 0.602270, acc: 70.31%] [G loss: 2.006149]\n",
      "epoch:12 step:11768 [D loss: 0.632751, acc: 64.06%] [G loss: 2.065480]\n",
      "epoch:12 step:11769 [D loss: 0.661408, acc: 57.81%] [G loss: 1.921467]\n",
      "epoch:12 step:11770 [D loss: 0.582802, acc: 67.97%] [G loss: 1.879259]\n",
      "epoch:12 step:11771 [D loss: 0.627424, acc: 64.06%] [G loss: 1.938236]\n",
      "epoch:12 step:11772 [D loss: 0.643219, acc: 63.28%] [G loss: 1.937982]\n",
      "epoch:12 step:11773 [D loss: 0.679487, acc: 64.84%] [G loss: 1.868349]\n",
      "epoch:12 step:11774 [D loss: 0.621924, acc: 69.53%] [G loss: 1.939935]\n",
      "epoch:12 step:11775 [D loss: 0.683654, acc: 53.91%] [G loss: 1.926473]\n",
      "epoch:12 step:11776 [D loss: 0.606265, acc: 66.41%] [G loss: 2.234939]\n",
      "epoch:12 step:11777 [D loss: 0.688519, acc: 55.47%] [G loss: 2.109059]\n",
      "epoch:12 step:11778 [D loss: 0.654828, acc: 62.50%] [G loss: 2.059567]\n",
      "epoch:12 step:11779 [D loss: 0.655910, acc: 59.38%] [G loss: 1.874007]\n",
      "epoch:12 step:11780 [D loss: 0.612196, acc: 67.97%] [G loss: 2.234370]\n",
      "epoch:12 step:11781 [D loss: 0.657088, acc: 64.06%] [G loss: 1.901198]\n",
      "epoch:12 step:11782 [D loss: 0.663766, acc: 61.72%] [G loss: 1.963181]\n",
      "epoch:12 step:11783 [D loss: 0.585548, acc: 67.19%] [G loss: 2.041016]\n",
      "epoch:12 step:11784 [D loss: 0.626768, acc: 70.31%] [G loss: 2.158415]\n",
      "epoch:12 step:11785 [D loss: 0.659789, acc: 60.94%] [G loss: 1.937820]\n",
      "epoch:12 step:11786 [D loss: 0.636146, acc: 65.62%] [G loss: 1.859828]\n",
      "epoch:12 step:11787 [D loss: 0.683732, acc: 57.03%] [G loss: 1.948440]\n",
      "epoch:12 step:11788 [D loss: 0.631259, acc: 60.94%] [G loss: 2.027767]\n",
      "epoch:12 step:11789 [D loss: 0.637063, acc: 65.62%] [G loss: 2.246948]\n",
      "epoch:12 step:11790 [D loss: 0.640284, acc: 67.97%] [G loss: 2.141322]\n",
      "epoch:12 step:11791 [D loss: 0.626111, acc: 66.41%] [G loss: 1.949841]\n",
      "epoch:12 step:11792 [D loss: 0.676895, acc: 62.50%] [G loss: 2.098492]\n",
      "epoch:12 step:11793 [D loss: 0.624823, acc: 68.75%] [G loss: 2.016386]\n",
      "epoch:12 step:11794 [D loss: 0.641952, acc: 58.59%] [G loss: 2.022917]\n",
      "epoch:12 step:11795 [D loss: 0.574454, acc: 70.31%] [G loss: 2.031559]\n",
      "epoch:12 step:11796 [D loss: 0.645006, acc: 59.38%] [G loss: 2.041736]\n",
      "epoch:12 step:11797 [D loss: 0.638253, acc: 61.72%] [G loss: 1.988548]\n",
      "epoch:12 step:11798 [D loss: 0.632568, acc: 70.31%] [G loss: 2.190440]\n",
      "epoch:12 step:11799 [D loss: 0.644806, acc: 67.19%] [G loss: 2.149871]\n",
      "epoch:12 step:11800 [D loss: 0.640318, acc: 64.06%] [G loss: 2.081628]\n",
      "##############\n",
      "[2.2517443  1.39071799 6.5803     4.87187062 3.52475322 5.71212323\n",
      " 4.49951282 4.68781086 4.76651993 3.56968533]\n",
      "##########\n",
      "epoch:12 step:11801 [D loss: 0.625668, acc: 63.28%] [G loss: 2.251781]\n",
      "epoch:12 step:11802 [D loss: 0.616054, acc: 67.19%] [G loss: 2.181136]\n",
      "epoch:12 step:11803 [D loss: 0.688852, acc: 57.03%] [G loss: 1.926548]\n",
      "epoch:12 step:11804 [D loss: 0.646554, acc: 61.72%] [G loss: 2.066321]\n",
      "epoch:12 step:11805 [D loss: 0.616398, acc: 64.06%] [G loss: 2.121486]\n",
      "epoch:12 step:11806 [D loss: 0.672587, acc: 58.59%] [G loss: 1.839360]\n",
      "epoch:12 step:11807 [D loss: 0.681828, acc: 59.38%] [G loss: 2.055285]\n",
      "epoch:12 step:11808 [D loss: 0.568711, acc: 71.09%] [G loss: 2.182760]\n",
      "epoch:12 step:11809 [D loss: 0.673433, acc: 60.16%] [G loss: 1.973596]\n",
      "epoch:12 step:11810 [D loss: 0.724251, acc: 50.78%] [G loss: 1.809734]\n",
      "epoch:12 step:11811 [D loss: 0.656765, acc: 58.59%] [G loss: 1.832238]\n",
      "epoch:12 step:11812 [D loss: 0.631584, acc: 71.09%] [G loss: 2.012726]\n",
      "epoch:12 step:11813 [D loss: 0.684992, acc: 59.38%] [G loss: 1.810759]\n",
      "epoch:12 step:11814 [D loss: 0.637458, acc: 63.28%] [G loss: 1.837637]\n",
      "epoch:12 step:11815 [D loss: 0.711503, acc: 53.91%] [G loss: 1.788060]\n",
      "epoch:12 step:11816 [D loss: 0.643647, acc: 64.84%] [G loss: 1.984361]\n",
      "epoch:12 step:11817 [D loss: 0.652174, acc: 65.62%] [G loss: 1.925201]\n",
      "epoch:12 step:11818 [D loss: 0.617770, acc: 67.19%] [G loss: 1.980097]\n",
      "epoch:12 step:11819 [D loss: 0.604630, acc: 67.19%] [G loss: 2.002632]\n",
      "epoch:12 step:11820 [D loss: 0.645256, acc: 60.16%] [G loss: 1.846804]\n",
      "epoch:12 step:11821 [D loss: 0.638665, acc: 62.50%] [G loss: 1.813953]\n",
      "epoch:12 step:11822 [D loss: 0.654525, acc: 60.94%] [G loss: 1.830539]\n",
      "epoch:12 step:11823 [D loss: 0.590795, acc: 69.53%] [G loss: 1.837893]\n",
      "epoch:12 step:11824 [D loss: 0.647919, acc: 57.03%] [G loss: 1.890589]\n",
      "epoch:12 step:11825 [D loss: 0.634221, acc: 61.72%] [G loss: 1.899207]\n",
      "epoch:12 step:11826 [D loss: 0.679244, acc: 63.28%] [G loss: 1.940194]\n",
      "epoch:12 step:11827 [D loss: 0.659971, acc: 58.59%] [G loss: 1.829193]\n",
      "epoch:12 step:11828 [D loss: 0.605590, acc: 70.31%] [G loss: 1.853099]\n",
      "epoch:12 step:11829 [D loss: 0.638448, acc: 60.16%] [G loss: 1.968377]\n",
      "epoch:12 step:11830 [D loss: 0.618062, acc: 67.19%] [G loss: 1.918088]\n",
      "epoch:12 step:11831 [D loss: 0.663837, acc: 58.59%] [G loss: 1.902561]\n",
      "epoch:12 step:11832 [D loss: 0.615134, acc: 67.97%] [G loss: 2.032954]\n",
      "epoch:12 step:11833 [D loss: 0.633994, acc: 64.06%] [G loss: 2.062515]\n",
      "epoch:12 step:11834 [D loss: 0.694335, acc: 53.91%] [G loss: 1.937913]\n",
      "epoch:12 step:11835 [D loss: 0.689521, acc: 54.69%] [G loss: 1.921324]\n",
      "epoch:12 step:11836 [D loss: 0.656302, acc: 56.25%] [G loss: 1.881988]\n",
      "epoch:12 step:11837 [D loss: 0.644167, acc: 64.06%] [G loss: 1.940099]\n",
      "epoch:12 step:11838 [D loss: 0.595081, acc: 68.75%] [G loss: 1.943328]\n",
      "epoch:12 step:11839 [D loss: 0.628623, acc: 68.75%] [G loss: 1.950647]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:12 step:11840 [D loss: 0.648636, acc: 61.72%] [G loss: 1.866756]\n",
      "epoch:12 step:11841 [D loss: 0.647203, acc: 64.06%] [G loss: 1.907597]\n",
      "epoch:12 step:11842 [D loss: 0.642050, acc: 61.72%] [G loss: 1.905955]\n",
      "epoch:12 step:11843 [D loss: 0.649827, acc: 65.62%] [G loss: 1.886391]\n",
      "epoch:12 step:11844 [D loss: 0.722873, acc: 58.59%] [G loss: 1.889820]\n",
      "epoch:12 step:11845 [D loss: 0.605769, acc: 67.97%] [G loss: 2.073540]\n",
      "epoch:12 step:11846 [D loss: 0.655737, acc: 60.94%] [G loss: 1.964225]\n",
      "epoch:12 step:11847 [D loss: 0.627308, acc: 60.16%] [G loss: 1.971717]\n",
      "epoch:12 step:11848 [D loss: 0.631144, acc: 62.50%] [G loss: 1.999972]\n",
      "epoch:12 step:11849 [D loss: 0.666198, acc: 60.94%] [G loss: 1.978842]\n",
      "epoch:12 step:11850 [D loss: 0.662329, acc: 64.06%] [G loss: 1.891319]\n",
      "epoch:12 step:11851 [D loss: 0.625757, acc: 65.62%] [G loss: 1.925877]\n",
      "epoch:12 step:11852 [D loss: 0.586775, acc: 69.53%] [G loss: 1.910358]\n",
      "epoch:12 step:11853 [D loss: 0.620596, acc: 63.28%] [G loss: 2.026747]\n",
      "epoch:12 step:11854 [D loss: 0.685651, acc: 64.06%] [G loss: 1.988983]\n",
      "epoch:12 step:11855 [D loss: 0.656367, acc: 58.59%] [G loss: 2.017571]\n",
      "epoch:12 step:11856 [D loss: 0.619399, acc: 63.28%] [G loss: 2.020559]\n",
      "epoch:12 step:11857 [D loss: 0.738034, acc: 57.81%] [G loss: 2.031613]\n",
      "epoch:12 step:11858 [D loss: 0.628933, acc: 66.41%] [G loss: 1.849432]\n",
      "epoch:12 step:11859 [D loss: 0.678141, acc: 57.81%] [G loss: 1.962345]\n",
      "epoch:12 step:11860 [D loss: 0.688991, acc: 61.72%] [G loss: 1.858115]\n",
      "epoch:12 step:11861 [D loss: 0.631466, acc: 61.72%] [G loss: 1.833047]\n",
      "epoch:12 step:11862 [D loss: 0.582119, acc: 66.41%] [G loss: 2.019941]\n",
      "epoch:12 step:11863 [D loss: 0.662770, acc: 60.16%] [G loss: 1.909858]\n",
      "epoch:12 step:11864 [D loss: 0.603633, acc: 71.88%] [G loss: 1.956141]\n",
      "epoch:12 step:11865 [D loss: 0.642955, acc: 60.94%] [G loss: 1.938920]\n",
      "epoch:12 step:11866 [D loss: 0.584923, acc: 70.31%] [G loss: 2.023975]\n",
      "epoch:12 step:11867 [D loss: 0.675504, acc: 56.25%] [G loss: 1.977166]\n",
      "epoch:12 step:11868 [D loss: 0.609024, acc: 65.62%] [G loss: 1.960029]\n",
      "epoch:12 step:11869 [D loss: 0.695062, acc: 58.59%] [G loss: 1.886132]\n",
      "epoch:12 step:11870 [D loss: 0.651234, acc: 62.50%] [G loss: 1.880939]\n",
      "epoch:12 step:11871 [D loss: 0.657246, acc: 59.38%] [G loss: 1.838607]\n",
      "epoch:12 step:11872 [D loss: 0.667576, acc: 60.16%] [G loss: 1.835868]\n",
      "epoch:12 step:11873 [D loss: 0.626565, acc: 67.19%] [G loss: 1.818459]\n",
      "epoch:12 step:11874 [D loss: 0.605885, acc: 67.19%] [G loss: 1.821546]\n",
      "epoch:12 step:11875 [D loss: 0.596275, acc: 64.84%] [G loss: 2.051189]\n",
      "epoch:12 step:11876 [D loss: 0.594589, acc: 66.41%] [G loss: 1.982993]\n",
      "epoch:12 step:11877 [D loss: 0.629913, acc: 63.28%] [G loss: 2.094477]\n",
      "epoch:12 step:11878 [D loss: 0.601573, acc: 67.19%] [G loss: 2.045172]\n",
      "epoch:12 step:11879 [D loss: 0.614260, acc: 64.06%] [G loss: 2.035757]\n",
      "epoch:12 step:11880 [D loss: 0.634109, acc: 60.94%] [G loss: 1.929828]\n",
      "epoch:12 step:11881 [D loss: 0.654320, acc: 59.38%] [G loss: 2.046777]\n",
      "epoch:12 step:11882 [D loss: 0.618436, acc: 59.38%] [G loss: 2.056804]\n",
      "epoch:12 step:11883 [D loss: 0.679180, acc: 57.81%] [G loss: 1.997867]\n",
      "epoch:12 step:11884 [D loss: 0.627833, acc: 64.06%] [G loss: 2.029675]\n",
      "epoch:12 step:11885 [D loss: 0.609368, acc: 66.41%] [G loss: 2.138373]\n",
      "epoch:12 step:11886 [D loss: 0.603977, acc: 68.75%] [G loss: 2.165503]\n",
      "epoch:12 step:11887 [D loss: 0.612510, acc: 67.19%] [G loss: 1.974841]\n",
      "epoch:12 step:11888 [D loss: 0.640903, acc: 64.84%] [G loss: 2.069515]\n",
      "epoch:12 step:11889 [D loss: 0.689221, acc: 61.72%] [G loss: 2.215457]\n",
      "epoch:12 step:11890 [D loss: 0.616940, acc: 71.88%] [G loss: 2.047140]\n",
      "epoch:12 step:11891 [D loss: 0.595963, acc: 64.84%] [G loss: 2.392196]\n",
      "epoch:12 step:11892 [D loss: 0.622069, acc: 65.62%] [G loss: 2.642942]\n",
      "epoch:12 step:11893 [D loss: 0.578224, acc: 63.28%] [G loss: 2.079733]\n",
      "epoch:12 step:11894 [D loss: 0.614791, acc: 64.06%] [G loss: 2.238669]\n",
      "epoch:12 step:11895 [D loss: 0.640890, acc: 64.06%] [G loss: 1.922570]\n",
      "epoch:12 step:11896 [D loss: 0.687034, acc: 54.69%] [G loss: 1.977349]\n",
      "epoch:12 step:11897 [D loss: 0.721871, acc: 53.12%] [G loss: 1.924214]\n",
      "epoch:12 step:11898 [D loss: 0.650934, acc: 64.06%] [G loss: 2.033212]\n",
      "epoch:12 step:11899 [D loss: 0.673493, acc: 67.19%] [G loss: 1.973626]\n",
      "epoch:12 step:11900 [D loss: 0.678309, acc: 57.81%] [G loss: 1.919796]\n",
      "epoch:12 step:11901 [D loss: 0.660326, acc: 59.38%] [G loss: 1.804178]\n",
      "epoch:12 step:11902 [D loss: 0.679288, acc: 60.16%] [G loss: 1.868480]\n",
      "epoch:12 step:11903 [D loss: 0.651080, acc: 63.28%] [G loss: 1.873481]\n",
      "epoch:12 step:11904 [D loss: 0.584782, acc: 71.88%] [G loss: 1.973184]\n",
      "epoch:12 step:11905 [D loss: 0.708289, acc: 55.47%] [G loss: 1.879766]\n",
      "epoch:12 step:11906 [D loss: 0.656405, acc: 63.28%] [G loss: 2.011804]\n",
      "epoch:12 step:11907 [D loss: 0.619572, acc: 64.06%] [G loss: 1.934648]\n",
      "epoch:12 step:11908 [D loss: 0.656344, acc: 60.94%] [G loss: 1.941725]\n",
      "epoch:12 step:11909 [D loss: 0.638456, acc: 63.28%] [G loss: 1.901750]\n",
      "epoch:12 step:11910 [D loss: 0.686577, acc: 60.16%] [G loss: 1.856699]\n",
      "epoch:12 step:11911 [D loss: 0.708233, acc: 53.91%] [G loss: 1.730131]\n",
      "epoch:12 step:11912 [D loss: 0.667893, acc: 57.03%] [G loss: 1.933327]\n",
      "epoch:12 step:11913 [D loss: 0.650257, acc: 61.72%] [G loss: 1.739346]\n",
      "epoch:12 step:11914 [D loss: 0.697429, acc: 57.81%] [G loss: 1.872084]\n",
      "epoch:12 step:11915 [D loss: 0.644733, acc: 61.72%] [G loss: 1.843213]\n",
      "epoch:12 step:11916 [D loss: 0.660160, acc: 66.41%] [G loss: 1.926734]\n",
      "epoch:12 step:11917 [D loss: 0.595914, acc: 71.88%] [G loss: 1.917318]\n",
      "epoch:12 step:11918 [D loss: 0.645108, acc: 66.41%] [G loss: 1.823204]\n",
      "epoch:12 step:11919 [D loss: 0.641792, acc: 58.59%] [G loss: 1.768023]\n",
      "epoch:12 step:11920 [D loss: 0.640026, acc: 64.84%] [G loss: 1.929469]\n",
      "epoch:12 step:11921 [D loss: 0.622627, acc: 61.72%] [G loss: 1.898146]\n",
      "epoch:12 step:11922 [D loss: 0.618533, acc: 70.31%] [G loss: 2.016081]\n",
      "epoch:12 step:11923 [D loss: 0.663342, acc: 62.50%] [G loss: 2.049903]\n",
      "epoch:12 step:11924 [D loss: 0.594478, acc: 67.19%] [G loss: 2.117137]\n",
      "epoch:12 step:11925 [D loss: 0.562597, acc: 75.78%] [G loss: 2.151582]\n",
      "epoch:12 step:11926 [D loss: 0.640603, acc: 63.28%] [G loss: 1.913101]\n",
      "epoch:12 step:11927 [D loss: 0.661171, acc: 60.16%] [G loss: 1.985317]\n",
      "epoch:12 step:11928 [D loss: 0.660560, acc: 62.50%] [G loss: 1.830276]\n",
      "epoch:12 step:11929 [D loss: 0.622437, acc: 64.84%] [G loss: 2.039077]\n",
      "epoch:12 step:11930 [D loss: 0.633317, acc: 61.72%] [G loss: 1.937745]\n",
      "epoch:12 step:11931 [D loss: 0.653703, acc: 62.50%] [G loss: 1.857117]\n",
      "epoch:12 step:11932 [D loss: 0.598061, acc: 67.19%] [G loss: 2.177678]\n",
      "epoch:12 step:11933 [D loss: 0.681661, acc: 57.81%] [G loss: 1.900524]\n",
      "epoch:12 step:11934 [D loss: 0.683081, acc: 57.81%] [G loss: 1.897304]\n",
      "epoch:12 step:11935 [D loss: 0.607566, acc: 68.75%] [G loss: 2.128419]\n",
      "epoch:12 step:11936 [D loss: 0.609857, acc: 67.19%] [G loss: 2.107329]\n",
      "epoch:12 step:11937 [D loss: 0.646161, acc: 60.94%] [G loss: 2.142850]\n",
      "epoch:12 step:11938 [D loss: 0.594257, acc: 67.97%] [G loss: 2.119787]\n",
      "epoch:12 step:11939 [D loss: 0.617001, acc: 64.84%] [G loss: 2.190238]\n",
      "epoch:12 step:11940 [D loss: 0.622707, acc: 67.19%] [G loss: 2.057005]\n",
      "epoch:12 step:11941 [D loss: 0.616314, acc: 65.62%] [G loss: 1.918920]\n",
      "epoch:12 step:11942 [D loss: 0.681959, acc: 60.16%] [G loss: 1.868277]\n",
      "epoch:12 step:11943 [D loss: 0.658771, acc: 62.50%] [G loss: 2.076423]\n",
      "epoch:12 step:11944 [D loss: 0.620987, acc: 70.31%] [G loss: 1.984560]\n",
      "epoch:12 step:11945 [D loss: 0.681420, acc: 63.28%] [G loss: 2.009494]\n",
      "epoch:12 step:11946 [D loss: 0.674062, acc: 64.06%] [G loss: 1.818006]\n",
      "epoch:12 step:11947 [D loss: 0.697326, acc: 56.25%] [G loss: 1.852306]\n",
      "epoch:12 step:11948 [D loss: 0.682401, acc: 55.47%] [G loss: 1.793842]\n",
      "epoch:12 step:11949 [D loss: 0.607807, acc: 67.19%] [G loss: 1.924102]\n",
      "epoch:12 step:11950 [D loss: 0.639805, acc: 59.38%] [G loss: 2.016530]\n",
      "epoch:12 step:11951 [D loss: 0.577357, acc: 68.75%] [G loss: 2.075454]\n",
      "epoch:12 step:11952 [D loss: 0.610947, acc: 67.97%] [G loss: 2.296782]\n",
      "epoch:12 step:11953 [D loss: 0.622006, acc: 63.28%] [G loss: 1.958488]\n",
      "epoch:12 step:11954 [D loss: 0.659044, acc: 60.16%] [G loss: 1.967764]\n",
      "epoch:12 step:11955 [D loss: 0.610574, acc: 64.06%] [G loss: 1.904141]\n",
      "epoch:12 step:11956 [D loss: 0.576594, acc: 69.53%] [G loss: 1.966189]\n",
      "epoch:12 step:11957 [D loss: 0.655034, acc: 64.06%] [G loss: 1.988256]\n",
      "epoch:12 step:11958 [D loss: 0.629696, acc: 60.94%] [G loss: 1.913451]\n",
      "epoch:12 step:11959 [D loss: 0.711931, acc: 56.25%] [G loss: 1.945296]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:12 step:11960 [D loss: 0.638246, acc: 62.50%] [G loss: 1.810337]\n",
      "epoch:12 step:11961 [D loss: 0.635653, acc: 59.38%] [G loss: 1.949256]\n",
      "epoch:12 step:11962 [D loss: 0.653272, acc: 61.72%] [G loss: 2.260640]\n",
      "epoch:12 step:11963 [D loss: 0.630085, acc: 63.28%] [G loss: 1.900510]\n",
      "epoch:12 step:11964 [D loss: 0.652026, acc: 60.16%] [G loss: 1.932533]\n",
      "epoch:12 step:11965 [D loss: 0.738414, acc: 53.91%] [G loss: 1.930525]\n",
      "epoch:12 step:11966 [D loss: 0.679988, acc: 57.81%] [G loss: 1.835829]\n",
      "epoch:12 step:11967 [D loss: 0.622274, acc: 63.28%] [G loss: 1.918243]\n",
      "epoch:12 step:11968 [D loss: 0.633363, acc: 64.84%] [G loss: 1.916838]\n",
      "epoch:12 step:11969 [D loss: 0.673555, acc: 67.19%] [G loss: 1.909178]\n",
      "epoch:12 step:11970 [D loss: 0.631672, acc: 66.41%] [G loss: 1.864958]\n",
      "epoch:12 step:11971 [D loss: 0.746276, acc: 50.78%] [G loss: 1.853118]\n",
      "epoch:12 step:11972 [D loss: 0.657330, acc: 60.94%] [G loss: 1.914016]\n",
      "epoch:12 step:11973 [D loss: 0.588656, acc: 71.88%] [G loss: 1.835559]\n",
      "epoch:12 step:11974 [D loss: 0.661492, acc: 65.62%] [G loss: 1.864587]\n",
      "epoch:12 step:11975 [D loss: 0.637386, acc: 53.91%] [G loss: 2.085116]\n",
      "epoch:12 step:11976 [D loss: 0.660478, acc: 66.41%] [G loss: 2.054240]\n",
      "epoch:12 step:11977 [D loss: 0.591171, acc: 66.41%] [G loss: 1.941762]\n",
      "epoch:12 step:11978 [D loss: 0.593863, acc: 72.66%] [G loss: 1.864016]\n",
      "epoch:12 step:11979 [D loss: 0.650118, acc: 57.81%] [G loss: 1.998386]\n",
      "epoch:12 step:11980 [D loss: 0.605310, acc: 62.50%] [G loss: 2.090690]\n",
      "epoch:12 step:11981 [D loss: 0.608644, acc: 68.75%] [G loss: 1.918253]\n",
      "epoch:12 step:11982 [D loss: 0.599160, acc: 67.97%] [G loss: 1.833825]\n",
      "epoch:12 step:11983 [D loss: 0.677379, acc: 58.59%] [G loss: 1.859801]\n",
      "epoch:12 step:11984 [D loss: 0.641720, acc: 65.62%] [G loss: 1.946456]\n",
      "epoch:12 step:11985 [D loss: 0.671351, acc: 60.94%] [G loss: 1.957291]\n",
      "epoch:12 step:11986 [D loss: 0.663224, acc: 64.06%] [G loss: 1.884276]\n",
      "epoch:12 step:11987 [D loss: 0.649283, acc: 61.72%] [G loss: 2.132396]\n",
      "epoch:12 step:11988 [D loss: 0.717952, acc: 58.59%] [G loss: 1.844203]\n",
      "epoch:12 step:11989 [D loss: 0.648916, acc: 60.94%] [G loss: 1.969733]\n",
      "epoch:12 step:11990 [D loss: 0.645507, acc: 61.72%] [G loss: 2.172457]\n",
      "epoch:12 step:11991 [D loss: 0.610934, acc: 63.28%] [G loss: 2.122767]\n",
      "epoch:12 step:11992 [D loss: 0.674839, acc: 58.59%] [G loss: 1.916597]\n",
      "epoch:12 step:11993 [D loss: 0.687649, acc: 53.91%] [G loss: 1.848998]\n",
      "epoch:12 step:11994 [D loss: 0.651840, acc: 62.50%] [G loss: 1.796295]\n",
      "epoch:12 step:11995 [D loss: 0.634788, acc: 61.72%] [G loss: 1.828759]\n",
      "epoch:12 step:11996 [D loss: 0.646931, acc: 61.72%] [G loss: 1.761719]\n",
      "epoch:12 step:11997 [D loss: 0.638288, acc: 62.50%] [G loss: 1.959132]\n",
      "epoch:12 step:11998 [D loss: 0.612755, acc: 68.75%] [G loss: 1.874537]\n",
      "epoch:12 step:11999 [D loss: 0.613122, acc: 62.50%] [G loss: 1.870900]\n",
      "epoch:12 step:12000 [D loss: 0.642582, acc: 67.19%] [G loss: 1.869951]\n",
      "##############\n",
      "[2.400985   1.48694938 6.3629143  4.68354964 3.72812441 5.77387897\n",
      " 4.48109687 4.76368801 4.85182517 3.6839799 ]\n",
      "##########\n",
      "epoch:12 step:12001 [D loss: 0.622293, acc: 71.09%] [G loss: 2.121896]\n",
      "epoch:12 step:12002 [D loss: 0.678202, acc: 57.03%] [G loss: 1.820735]\n",
      "epoch:12 step:12003 [D loss: 0.656196, acc: 61.72%] [G loss: 1.928759]\n",
      "epoch:12 step:12004 [D loss: 0.652046, acc: 60.94%] [G loss: 1.901715]\n",
      "epoch:12 step:12005 [D loss: 0.618610, acc: 69.53%] [G loss: 1.877278]\n",
      "epoch:12 step:12006 [D loss: 0.595468, acc: 70.31%] [G loss: 1.948106]\n",
      "epoch:12 step:12007 [D loss: 0.650852, acc: 57.81%] [G loss: 2.022066]\n",
      "epoch:12 step:12008 [D loss: 0.634691, acc: 64.06%] [G loss: 1.873601]\n",
      "epoch:12 step:12009 [D loss: 0.688599, acc: 57.81%] [G loss: 1.813446]\n",
      "epoch:12 step:12010 [D loss: 0.687527, acc: 57.03%] [G loss: 1.831288]\n",
      "epoch:12 step:12011 [D loss: 0.692116, acc: 58.59%] [G loss: 1.799199]\n",
      "epoch:12 step:12012 [D loss: 0.681046, acc: 57.81%] [G loss: 1.814221]\n",
      "epoch:12 step:12013 [D loss: 0.615183, acc: 63.28%] [G loss: 1.963757]\n",
      "epoch:12 step:12014 [D loss: 0.657149, acc: 62.50%] [G loss: 1.948745]\n",
      "epoch:12 step:12015 [D loss: 0.653907, acc: 57.81%] [G loss: 1.893867]\n",
      "epoch:12 step:12016 [D loss: 0.720220, acc: 56.25%] [G loss: 1.851780]\n",
      "epoch:12 step:12017 [D loss: 0.616358, acc: 66.41%] [G loss: 1.886551]\n",
      "epoch:12 step:12018 [D loss: 0.613915, acc: 65.62%] [G loss: 1.993688]\n",
      "epoch:12 step:12019 [D loss: 0.638169, acc: 59.38%] [G loss: 2.134840]\n",
      "epoch:12 step:12020 [D loss: 0.620765, acc: 67.19%] [G loss: 1.944913]\n",
      "epoch:12 step:12021 [D loss: 0.611224, acc: 67.97%] [G loss: 2.031071]\n",
      "epoch:12 step:12022 [D loss: 0.670930, acc: 64.06%] [G loss: 2.039307]\n",
      "epoch:12 step:12023 [D loss: 0.667231, acc: 60.94%] [G loss: 1.899804]\n",
      "epoch:12 step:12024 [D loss: 0.545930, acc: 78.91%] [G loss: 2.108664]\n",
      "epoch:12 step:12025 [D loss: 0.612695, acc: 69.53%] [G loss: 2.239559]\n",
      "epoch:12 step:12026 [D loss: 0.591628, acc: 73.44%] [G loss: 2.355836]\n",
      "epoch:12 step:12027 [D loss: 0.624643, acc: 66.41%] [G loss: 1.993683]\n",
      "epoch:12 step:12028 [D loss: 0.696508, acc: 53.91%] [G loss: 1.862100]\n",
      "epoch:12 step:12029 [D loss: 0.635883, acc: 67.19%] [G loss: 1.955225]\n",
      "epoch:12 step:12030 [D loss: 0.665442, acc: 61.72%] [G loss: 2.003277]\n",
      "epoch:12 step:12031 [D loss: 0.602878, acc: 65.62%] [G loss: 2.045671]\n",
      "epoch:12 step:12032 [D loss: 0.639684, acc: 66.41%] [G loss: 1.960867]\n",
      "epoch:12 step:12033 [D loss: 0.664619, acc: 63.28%] [G loss: 1.940713]\n",
      "epoch:12 step:12034 [D loss: 0.606849, acc: 72.66%] [G loss: 2.067460]\n",
      "epoch:12 step:12035 [D loss: 0.677014, acc: 63.28%] [G loss: 1.887583]\n",
      "epoch:12 step:12036 [D loss: 0.631120, acc: 60.16%] [G loss: 2.104803]\n",
      "epoch:12 step:12037 [D loss: 0.696018, acc: 59.38%] [G loss: 1.960506]\n",
      "epoch:12 step:12038 [D loss: 0.701780, acc: 52.34%] [G loss: 1.765963]\n",
      "epoch:12 step:12039 [D loss: 0.669158, acc: 64.06%] [G loss: 2.159845]\n",
      "epoch:12 step:12040 [D loss: 0.655041, acc: 60.16%] [G loss: 1.979964]\n",
      "epoch:12 step:12041 [D loss: 0.695055, acc: 65.62%] [G loss: 1.852401]\n",
      "epoch:12 step:12042 [D loss: 0.638779, acc: 67.19%] [G loss: 1.846239]\n",
      "epoch:12 step:12043 [D loss: 0.683912, acc: 56.25%] [G loss: 1.902077]\n",
      "epoch:12 step:12044 [D loss: 0.684194, acc: 60.94%] [G loss: 1.802630]\n",
      "epoch:12 step:12045 [D loss: 0.639841, acc: 63.28%] [G loss: 1.804534]\n",
      "epoch:12 step:12046 [D loss: 0.678155, acc: 57.03%] [G loss: 1.871616]\n",
      "epoch:12 step:12047 [D loss: 0.593517, acc: 67.19%] [G loss: 1.895632]\n",
      "epoch:12 step:12048 [D loss: 0.670654, acc: 58.59%] [G loss: 1.835130]\n",
      "epoch:12 step:12049 [D loss: 0.612200, acc: 68.75%] [G loss: 2.076803]\n",
      "epoch:12 step:12050 [D loss: 0.596900, acc: 75.78%] [G loss: 2.057865]\n",
      "epoch:12 step:12051 [D loss: 0.587031, acc: 67.97%] [G loss: 1.903861]\n",
      "epoch:12 step:12052 [D loss: 0.628054, acc: 63.28%] [G loss: 1.931013]\n",
      "epoch:12 step:12053 [D loss: 0.590349, acc: 70.31%] [G loss: 1.905005]\n",
      "epoch:12 step:12054 [D loss: 0.592341, acc: 71.88%] [G loss: 2.061320]\n",
      "epoch:12 step:12055 [D loss: 0.593420, acc: 70.31%] [G loss: 1.971645]\n",
      "epoch:12 step:12056 [D loss: 0.659832, acc: 58.59%] [G loss: 1.831097]\n",
      "epoch:12 step:12057 [D loss: 0.614198, acc: 64.84%] [G loss: 2.063291]\n",
      "epoch:12 step:12058 [D loss: 0.596885, acc: 71.09%] [G loss: 2.045932]\n",
      "epoch:12 step:12059 [D loss: 0.599856, acc: 65.62%] [G loss: 2.155418]\n",
      "epoch:12 step:12060 [D loss: 0.611866, acc: 64.06%] [G loss: 2.189667]\n",
      "epoch:12 step:12061 [D loss: 0.671108, acc: 63.28%] [G loss: 2.048338]\n",
      "epoch:12 step:12062 [D loss: 0.645750, acc: 66.41%] [G loss: 2.030263]\n",
      "epoch:12 step:12063 [D loss: 0.636312, acc: 63.28%] [G loss: 2.014805]\n",
      "epoch:12 step:12064 [D loss: 0.723903, acc: 50.78%] [G loss: 1.804493]\n",
      "epoch:12 step:12065 [D loss: 0.625428, acc: 65.62%] [G loss: 1.930339]\n",
      "epoch:12 step:12066 [D loss: 0.650054, acc: 58.59%] [G loss: 2.142764]\n",
      "epoch:12 step:12067 [D loss: 0.610002, acc: 66.41%] [G loss: 2.087295]\n",
      "epoch:12 step:12068 [D loss: 0.695898, acc: 58.59%] [G loss: 1.813900]\n",
      "epoch:12 step:12069 [D loss: 0.666772, acc: 62.50%] [G loss: 2.095189]\n",
      "epoch:12 step:12070 [D loss: 0.635587, acc: 62.50%] [G loss: 2.073103]\n",
      "epoch:12 step:12071 [D loss: 0.659920, acc: 60.16%] [G loss: 1.901151]\n",
      "epoch:12 step:12072 [D loss: 0.697585, acc: 60.94%] [G loss: 1.857204]\n",
      "epoch:12 step:12073 [D loss: 0.600920, acc: 65.62%] [G loss: 1.785120]\n",
      "epoch:12 step:12074 [D loss: 0.663693, acc: 63.28%] [G loss: 1.846759]\n",
      "epoch:12 step:12075 [D loss: 0.658466, acc: 61.72%] [G loss: 1.863479]\n",
      "epoch:12 step:12076 [D loss: 0.639766, acc: 62.50%] [G loss: 1.894807]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:12 step:12077 [D loss: 0.659072, acc: 60.94%] [G loss: 1.891512]\n",
      "epoch:12 step:12078 [D loss: 0.619635, acc: 68.75%] [G loss: 1.995441]\n",
      "epoch:12 step:12079 [D loss: 0.644456, acc: 62.50%] [G loss: 1.920884]\n",
      "epoch:12 step:12080 [D loss: 0.593676, acc: 66.41%] [G loss: 1.967830]\n",
      "epoch:12 step:12081 [D loss: 0.668988, acc: 58.59%] [G loss: 1.911976]\n",
      "epoch:12 step:12082 [D loss: 0.564520, acc: 74.22%] [G loss: 1.945751]\n",
      "epoch:12 step:12083 [D loss: 0.670420, acc: 60.16%] [G loss: 2.019186]\n",
      "epoch:12 step:12084 [D loss: 0.658655, acc: 62.50%] [G loss: 2.024801]\n",
      "epoch:12 step:12085 [D loss: 0.626249, acc: 62.50%] [G loss: 1.902048]\n",
      "epoch:12 step:12086 [D loss: 0.659783, acc: 61.72%] [G loss: 1.978237]\n",
      "epoch:12 step:12087 [D loss: 0.667801, acc: 62.50%] [G loss: 1.918040]\n",
      "epoch:12 step:12088 [D loss: 0.636547, acc: 64.84%] [G loss: 1.999220]\n",
      "epoch:12 step:12089 [D loss: 0.638315, acc: 60.94%] [G loss: 2.063508]\n",
      "epoch:12 step:12090 [D loss: 0.594407, acc: 69.53%] [G loss: 1.974818]\n",
      "epoch:12 step:12091 [D loss: 0.613493, acc: 69.53%] [G loss: 1.918128]\n",
      "epoch:12 step:12092 [D loss: 0.644306, acc: 63.28%] [G loss: 2.047276]\n",
      "epoch:12 step:12093 [D loss: 0.655850, acc: 61.72%] [G loss: 1.972153]\n",
      "epoch:12 step:12094 [D loss: 0.617073, acc: 66.41%] [G loss: 1.950432]\n",
      "epoch:12 step:12095 [D loss: 0.588169, acc: 67.97%] [G loss: 1.911125]\n",
      "epoch:12 step:12096 [D loss: 0.612194, acc: 61.72%] [G loss: 2.036670]\n",
      "epoch:12 step:12097 [D loss: 0.624125, acc: 64.84%] [G loss: 2.004687]\n",
      "epoch:12 step:12098 [D loss: 0.668603, acc: 64.06%] [G loss: 1.839752]\n",
      "epoch:12 step:12099 [D loss: 0.676922, acc: 60.94%] [G loss: 1.795809]\n",
      "epoch:12 step:12100 [D loss: 0.752260, acc: 52.34%] [G loss: 1.780099]\n",
      "epoch:12 step:12101 [D loss: 0.630571, acc: 64.84%] [G loss: 2.042720]\n",
      "epoch:12 step:12102 [D loss: 0.657859, acc: 61.72%] [G loss: 1.903038]\n",
      "epoch:12 step:12103 [D loss: 0.643556, acc: 66.41%] [G loss: 1.932249]\n",
      "epoch:12 step:12104 [D loss: 0.610027, acc: 67.97%] [G loss: 2.031399]\n",
      "epoch:12 step:12105 [D loss: 0.710074, acc: 59.38%] [G loss: 1.797493]\n",
      "epoch:12 step:12106 [D loss: 0.624375, acc: 65.62%] [G loss: 1.824262]\n",
      "epoch:12 step:12107 [D loss: 0.655249, acc: 60.16%] [G loss: 1.941547]\n",
      "epoch:12 step:12108 [D loss: 0.631935, acc: 60.16%] [G loss: 1.956203]\n",
      "epoch:12 step:12109 [D loss: 0.618573, acc: 64.06%] [G loss: 1.874231]\n",
      "epoch:12 step:12110 [D loss: 0.665380, acc: 62.50%] [G loss: 2.014896]\n",
      "epoch:12 step:12111 [D loss: 0.661825, acc: 57.81%] [G loss: 1.892052]\n",
      "epoch:12 step:12112 [D loss: 0.590661, acc: 68.75%] [G loss: 1.973519]\n",
      "epoch:12 step:12113 [D loss: 0.625506, acc: 66.41%] [G loss: 1.946600]\n",
      "epoch:12 step:12114 [D loss: 0.649047, acc: 62.50%] [G loss: 2.163343]\n",
      "epoch:12 step:12115 [D loss: 0.718832, acc: 57.03%] [G loss: 1.820284]\n",
      "epoch:12 step:12116 [D loss: 0.661735, acc: 61.72%] [G loss: 1.875897]\n",
      "epoch:12 step:12117 [D loss: 0.705400, acc: 53.91%] [G loss: 1.766063]\n",
      "epoch:12 step:12118 [D loss: 0.679724, acc: 53.91%] [G loss: 1.793580]\n",
      "epoch:12 step:12119 [D loss: 0.636252, acc: 68.75%] [G loss: 1.994583]\n",
      "epoch:12 step:12120 [D loss: 0.623124, acc: 65.62%] [G loss: 1.804086]\n",
      "epoch:12 step:12121 [D loss: 0.603673, acc: 67.97%] [G loss: 1.849599]\n",
      "epoch:12 step:12122 [D loss: 0.628944, acc: 62.50%] [G loss: 1.918807]\n",
      "epoch:12 step:12123 [D loss: 0.643134, acc: 62.50%] [G loss: 1.883500]\n",
      "epoch:12 step:12124 [D loss: 0.647801, acc: 59.38%] [G loss: 1.869704]\n",
      "epoch:12 step:12125 [D loss: 0.612155, acc: 69.53%] [G loss: 1.897700]\n",
      "epoch:12 step:12126 [D loss: 0.582560, acc: 73.44%] [G loss: 1.999627]\n",
      "epoch:12 step:12127 [D loss: 0.642568, acc: 60.16%] [G loss: 2.042961]\n",
      "epoch:12 step:12128 [D loss: 0.618724, acc: 68.75%] [G loss: 1.963754]\n",
      "epoch:12 step:12129 [D loss: 0.659637, acc: 61.72%] [G loss: 2.050133]\n",
      "epoch:12 step:12130 [D loss: 0.626479, acc: 64.06%] [G loss: 2.005364]\n",
      "epoch:12 step:12131 [D loss: 0.642424, acc: 60.16%] [G loss: 1.994732]\n",
      "epoch:12 step:12132 [D loss: 0.644650, acc: 65.62%] [G loss: 1.889719]\n",
      "epoch:12 step:12133 [D loss: 0.645217, acc: 59.38%] [G loss: 2.013282]\n",
      "epoch:12 step:12134 [D loss: 0.601002, acc: 67.19%] [G loss: 1.981019]\n",
      "epoch:12 step:12135 [D loss: 0.687776, acc: 60.16%] [G loss: 1.973321]\n",
      "epoch:12 step:12136 [D loss: 0.703988, acc: 55.47%] [G loss: 1.878017]\n",
      "epoch:12 step:12137 [D loss: 0.650077, acc: 61.72%] [G loss: 1.960572]\n",
      "epoch:12 step:12138 [D loss: 0.617475, acc: 63.28%] [G loss: 2.012538]\n",
      "epoch:12 step:12139 [D loss: 0.669007, acc: 58.59%] [G loss: 1.992471]\n",
      "epoch:12 step:12140 [D loss: 0.666405, acc: 63.28%] [G loss: 1.815908]\n",
      "epoch:12 step:12141 [D loss: 0.656358, acc: 64.84%] [G loss: 2.176484]\n",
      "epoch:12 step:12142 [D loss: 0.751921, acc: 46.09%] [G loss: 1.896174]\n",
      "epoch:12 step:12143 [D loss: 0.626590, acc: 65.62%] [G loss: 2.160666]\n",
      "epoch:12 step:12144 [D loss: 0.583046, acc: 67.97%] [G loss: 1.913730]\n",
      "epoch:12 step:12145 [D loss: 0.582172, acc: 67.19%] [G loss: 1.898726]\n",
      "epoch:12 step:12146 [D loss: 0.699164, acc: 57.03%] [G loss: 1.899982]\n",
      "epoch:12 step:12147 [D loss: 0.611726, acc: 67.97%] [G loss: 2.022640]\n",
      "epoch:12 step:12148 [D loss: 0.684033, acc: 61.72%] [G loss: 1.943343]\n",
      "epoch:12 step:12149 [D loss: 0.660130, acc: 62.50%] [G loss: 1.964090]\n",
      "epoch:12 step:12150 [D loss: 0.596485, acc: 66.41%] [G loss: 2.061319]\n",
      "epoch:12 step:12151 [D loss: 0.659897, acc: 59.38%] [G loss: 2.009775]\n",
      "epoch:12 step:12152 [D loss: 0.657330, acc: 60.16%] [G loss: 1.922569]\n",
      "epoch:12 step:12153 [D loss: 0.580996, acc: 71.88%] [G loss: 2.071071]\n",
      "epoch:12 step:12154 [D loss: 0.605143, acc: 71.09%] [G loss: 1.997971]\n",
      "epoch:12 step:12155 [D loss: 0.618014, acc: 67.19%] [G loss: 2.137167]\n",
      "epoch:12 step:12156 [D loss: 0.629808, acc: 67.19%] [G loss: 2.158654]\n",
      "epoch:12 step:12157 [D loss: 0.677564, acc: 60.16%] [G loss: 2.118917]\n",
      "epoch:12 step:12158 [D loss: 0.609956, acc: 64.06%] [G loss: 2.142029]\n",
      "epoch:12 step:12159 [D loss: 0.688676, acc: 57.81%] [G loss: 2.099529]\n",
      "epoch:12 step:12160 [D loss: 0.625613, acc: 62.50%] [G loss: 2.196998]\n",
      "epoch:12 step:12161 [D loss: 0.686409, acc: 61.72%] [G loss: 2.065928]\n",
      "epoch:12 step:12162 [D loss: 0.588727, acc: 74.22%] [G loss: 2.185954]\n",
      "epoch:12 step:12163 [D loss: 0.590310, acc: 66.41%] [G loss: 2.448105]\n",
      "epoch:12 step:12164 [D loss: 0.779748, acc: 52.34%] [G loss: 1.844959]\n",
      "epoch:12 step:12165 [D loss: 0.675241, acc: 53.91%] [G loss: 2.041811]\n",
      "epoch:12 step:12166 [D loss: 0.608468, acc: 63.28%] [G loss: 2.116034]\n",
      "epoch:12 step:12167 [D loss: 0.594713, acc: 69.53%] [G loss: 2.043504]\n",
      "epoch:12 step:12168 [D loss: 0.609708, acc: 65.62%] [G loss: 2.204592]\n",
      "epoch:12 step:12169 [D loss: 0.602319, acc: 64.84%] [G loss: 2.218398]\n",
      "epoch:12 step:12170 [D loss: 0.600302, acc: 66.41%] [G loss: 2.109756]\n",
      "epoch:12 step:12171 [D loss: 0.656140, acc: 66.41%] [G loss: 2.060547]\n",
      "epoch:12 step:12172 [D loss: 0.791477, acc: 49.22%] [G loss: 1.852284]\n",
      "epoch:12 step:12173 [D loss: 0.747545, acc: 50.00%] [G loss: 1.773281]\n",
      "epoch:12 step:12174 [D loss: 0.581717, acc: 68.75%] [G loss: 2.065609]\n",
      "epoch:12 step:12175 [D loss: 0.546833, acc: 68.75%] [G loss: 1.982708]\n",
      "epoch:12 step:12176 [D loss: 0.651580, acc: 63.28%] [G loss: 1.992013]\n",
      "epoch:12 step:12177 [D loss: 0.674372, acc: 67.97%] [G loss: 1.976545]\n",
      "epoch:12 step:12178 [D loss: 0.632303, acc: 64.84%] [G loss: 2.233200]\n",
      "epoch:12 step:12179 [D loss: 0.659296, acc: 62.50%] [G loss: 2.112137]\n",
      "epoch:12 step:12180 [D loss: 0.566939, acc: 74.22%] [G loss: 2.258186]\n",
      "epoch:12 step:12181 [D loss: 0.607517, acc: 65.62%] [G loss: 2.473123]\n",
      "epoch:13 step:12182 [D loss: 0.593204, acc: 69.53%] [G loss: 2.039077]\n",
      "epoch:13 step:12183 [D loss: 0.622470, acc: 65.62%] [G loss: 2.014784]\n",
      "epoch:13 step:12184 [D loss: 0.668176, acc: 61.72%] [G loss: 1.980578]\n",
      "epoch:13 step:12185 [D loss: 0.605611, acc: 61.72%] [G loss: 2.057690]\n",
      "epoch:13 step:12186 [D loss: 0.625867, acc: 62.50%] [G loss: 2.025732]\n",
      "epoch:13 step:12187 [D loss: 0.643511, acc: 64.84%] [G loss: 1.999982]\n",
      "epoch:13 step:12188 [D loss: 0.608811, acc: 68.75%] [G loss: 2.103205]\n",
      "epoch:13 step:12189 [D loss: 0.598661, acc: 66.41%] [G loss: 2.031885]\n",
      "epoch:13 step:12190 [D loss: 0.566538, acc: 70.31%] [G loss: 2.053125]\n",
      "epoch:13 step:12191 [D loss: 0.680802, acc: 61.72%] [G loss: 2.231217]\n",
      "epoch:13 step:12192 [D loss: 0.633490, acc: 65.62%] [G loss: 2.108948]\n",
      "epoch:13 step:12193 [D loss: 0.629758, acc: 64.06%] [G loss: 1.905685]\n",
      "epoch:13 step:12194 [D loss: 0.620618, acc: 64.06%] [G loss: 2.003869]\n",
      "epoch:13 step:12195 [D loss: 0.642039, acc: 64.06%] [G loss: 2.085645]\n",
      "epoch:13 step:12196 [D loss: 0.610008, acc: 73.44%] [G loss: 2.145719]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:13 step:12197 [D loss: 0.612668, acc: 61.72%] [G loss: 2.114880]\n",
      "epoch:13 step:12198 [D loss: 0.751898, acc: 51.56%] [G loss: 2.041431]\n",
      "epoch:13 step:12199 [D loss: 0.593270, acc: 66.41%] [G loss: 1.931769]\n",
      "epoch:13 step:12200 [D loss: 0.636386, acc: 65.62%] [G loss: 1.956231]\n",
      "##############\n",
      "[2.41653113 1.44506729 6.28564517 4.64866876 3.70841236 5.58477205\n",
      " 4.49426107 4.82905118 4.46545217 3.70257905]\n",
      "##########\n",
      "epoch:13 step:12201 [D loss: 0.751634, acc: 50.00%] [G loss: 1.828419]\n",
      "epoch:13 step:12202 [D loss: 0.638392, acc: 64.06%] [G loss: 1.888735]\n",
      "epoch:13 step:12203 [D loss: 0.628878, acc: 65.62%] [G loss: 2.018726]\n",
      "epoch:13 step:12204 [D loss: 0.613571, acc: 67.19%] [G loss: 1.942761]\n",
      "epoch:13 step:12205 [D loss: 0.576573, acc: 72.66%] [G loss: 1.957686]\n",
      "epoch:13 step:12206 [D loss: 0.610319, acc: 71.09%] [G loss: 2.119317]\n",
      "epoch:13 step:12207 [D loss: 0.666478, acc: 58.59%] [G loss: 1.933303]\n",
      "epoch:13 step:12208 [D loss: 0.697289, acc: 54.69%] [G loss: 1.824444]\n",
      "epoch:13 step:12209 [D loss: 0.609618, acc: 67.97%] [G loss: 1.893991]\n",
      "epoch:13 step:12210 [D loss: 0.629438, acc: 63.28%] [G loss: 1.995437]\n",
      "epoch:13 step:12211 [D loss: 0.655447, acc: 58.59%] [G loss: 1.973215]\n",
      "epoch:13 step:12212 [D loss: 0.721042, acc: 49.22%] [G loss: 1.721769]\n",
      "epoch:13 step:12213 [D loss: 0.663662, acc: 60.94%] [G loss: 1.873150]\n",
      "epoch:13 step:12214 [D loss: 0.596195, acc: 68.75%] [G loss: 1.862445]\n",
      "epoch:13 step:12215 [D loss: 0.629285, acc: 59.38%] [G loss: 1.819822]\n",
      "epoch:13 step:12216 [D loss: 0.678133, acc: 61.72%] [G loss: 1.929189]\n",
      "epoch:13 step:12217 [D loss: 0.610559, acc: 66.41%] [G loss: 2.106112]\n",
      "epoch:13 step:12218 [D loss: 0.625995, acc: 63.28%] [G loss: 1.908679]\n",
      "epoch:13 step:12219 [D loss: 0.651761, acc: 62.50%] [G loss: 1.890754]\n",
      "epoch:13 step:12220 [D loss: 0.630215, acc: 63.28%] [G loss: 2.069183]\n",
      "epoch:13 step:12221 [D loss: 0.626871, acc: 66.41%] [G loss: 1.981266]\n",
      "epoch:13 step:12222 [D loss: 0.690054, acc: 57.81%] [G loss: 1.824496]\n",
      "epoch:13 step:12223 [D loss: 0.590585, acc: 71.09%] [G loss: 1.925802]\n",
      "epoch:13 step:12224 [D loss: 0.572163, acc: 66.41%] [G loss: 2.040277]\n",
      "epoch:13 step:12225 [D loss: 0.622144, acc: 66.41%] [G loss: 2.003656]\n",
      "epoch:13 step:12226 [D loss: 0.607799, acc: 68.75%] [G loss: 1.872655]\n",
      "epoch:13 step:12227 [D loss: 0.653109, acc: 63.28%] [G loss: 1.848723]\n",
      "epoch:13 step:12228 [D loss: 0.565915, acc: 73.44%] [G loss: 2.121773]\n",
      "epoch:13 step:12229 [D loss: 0.585690, acc: 74.22%] [G loss: 2.064563]\n",
      "epoch:13 step:12230 [D loss: 0.684854, acc: 57.03%] [G loss: 1.918208]\n",
      "epoch:13 step:12231 [D loss: 0.609454, acc: 65.62%] [G loss: 2.151056]\n",
      "epoch:13 step:12232 [D loss: 0.614414, acc: 62.50%] [G loss: 2.036786]\n",
      "epoch:13 step:12233 [D loss: 0.605980, acc: 67.19%] [G loss: 2.107939]\n",
      "epoch:13 step:12234 [D loss: 0.599811, acc: 71.09%] [G loss: 1.966487]\n",
      "epoch:13 step:12235 [D loss: 0.570102, acc: 71.09%] [G loss: 2.127871]\n",
      "epoch:13 step:12236 [D loss: 0.627937, acc: 65.62%] [G loss: 2.174924]\n",
      "epoch:13 step:12237 [D loss: 0.573787, acc: 67.97%] [G loss: 2.019980]\n",
      "epoch:13 step:12238 [D loss: 0.675540, acc: 61.72%] [G loss: 1.953693]\n",
      "epoch:13 step:12239 [D loss: 0.619342, acc: 65.62%] [G loss: 2.092056]\n",
      "epoch:13 step:12240 [D loss: 0.640143, acc: 62.50%] [G loss: 1.960724]\n",
      "epoch:13 step:12241 [D loss: 0.661348, acc: 63.28%] [G loss: 1.876864]\n",
      "epoch:13 step:12242 [D loss: 0.620342, acc: 62.50%] [G loss: 2.091058]\n",
      "epoch:13 step:12243 [D loss: 0.642981, acc: 64.84%] [G loss: 1.959684]\n",
      "epoch:13 step:12244 [D loss: 0.609419, acc: 69.53%] [G loss: 1.943788]\n",
      "epoch:13 step:12245 [D loss: 0.675526, acc: 58.59%] [G loss: 1.949268]\n",
      "epoch:13 step:12246 [D loss: 0.672451, acc: 60.16%] [G loss: 1.838810]\n",
      "epoch:13 step:12247 [D loss: 0.592839, acc: 67.19%] [G loss: 1.919418]\n",
      "epoch:13 step:12248 [D loss: 0.659253, acc: 59.38%] [G loss: 1.945342]\n",
      "epoch:13 step:12249 [D loss: 0.594907, acc: 71.88%] [G loss: 2.016797]\n",
      "epoch:13 step:12250 [D loss: 0.653679, acc: 65.62%] [G loss: 2.069546]\n",
      "epoch:13 step:12251 [D loss: 0.575295, acc: 71.09%] [G loss: 2.013865]\n",
      "epoch:13 step:12252 [D loss: 0.591990, acc: 67.19%] [G loss: 1.922753]\n",
      "epoch:13 step:12253 [D loss: 0.646881, acc: 61.72%] [G loss: 1.943791]\n",
      "epoch:13 step:12254 [D loss: 0.625719, acc: 70.31%] [G loss: 1.892458]\n",
      "epoch:13 step:12255 [D loss: 0.609587, acc: 67.97%] [G loss: 2.110806]\n",
      "epoch:13 step:12256 [D loss: 0.665367, acc: 64.84%] [G loss: 1.998414]\n",
      "epoch:13 step:12257 [D loss: 0.620616, acc: 64.84%] [G loss: 2.215775]\n",
      "epoch:13 step:12258 [D loss: 0.531965, acc: 78.91%] [G loss: 2.161940]\n",
      "epoch:13 step:12259 [D loss: 0.654235, acc: 60.16%] [G loss: 1.959707]\n",
      "epoch:13 step:12260 [D loss: 0.637546, acc: 67.97%] [G loss: 1.967489]\n",
      "epoch:13 step:12261 [D loss: 0.682171, acc: 58.59%] [G loss: 1.973244]\n",
      "epoch:13 step:12262 [D loss: 0.655036, acc: 60.16%] [G loss: 1.880497]\n",
      "epoch:13 step:12263 [D loss: 0.638497, acc: 60.94%] [G loss: 1.866475]\n",
      "epoch:13 step:12264 [D loss: 0.632341, acc: 64.06%] [G loss: 1.929332]\n",
      "epoch:13 step:12265 [D loss: 0.665299, acc: 62.50%] [G loss: 1.978877]\n",
      "epoch:13 step:12266 [D loss: 0.660905, acc: 58.59%] [G loss: 1.829311]\n",
      "epoch:13 step:12267 [D loss: 0.653744, acc: 59.38%] [G loss: 1.805743]\n",
      "epoch:13 step:12268 [D loss: 0.610118, acc: 66.41%] [G loss: 1.871007]\n",
      "epoch:13 step:12269 [D loss: 0.621776, acc: 62.50%] [G loss: 2.012151]\n",
      "epoch:13 step:12270 [D loss: 0.594695, acc: 67.19%] [G loss: 2.029361]\n",
      "epoch:13 step:12271 [D loss: 0.620558, acc: 68.75%] [G loss: 2.040736]\n",
      "epoch:13 step:12272 [D loss: 0.681984, acc: 60.16%] [G loss: 1.968903]\n",
      "epoch:13 step:12273 [D loss: 0.597263, acc: 65.62%] [G loss: 2.087125]\n",
      "epoch:13 step:12274 [D loss: 0.648975, acc: 56.25%] [G loss: 2.159509]\n",
      "epoch:13 step:12275 [D loss: 0.636999, acc: 64.84%] [G loss: 2.012433]\n",
      "epoch:13 step:12276 [D loss: 0.640132, acc: 67.19%] [G loss: 1.789473]\n",
      "epoch:13 step:12277 [D loss: 0.606388, acc: 69.53%] [G loss: 1.936029]\n",
      "epoch:13 step:12278 [D loss: 0.628325, acc: 64.84%] [G loss: 2.080010]\n",
      "epoch:13 step:12279 [D loss: 0.679385, acc: 58.59%] [G loss: 1.869162]\n",
      "epoch:13 step:12280 [D loss: 0.626709, acc: 67.19%] [G loss: 1.766530]\n",
      "epoch:13 step:12281 [D loss: 0.598151, acc: 68.75%] [G loss: 1.971645]\n",
      "epoch:13 step:12282 [D loss: 0.634152, acc: 61.72%] [G loss: 1.947714]\n",
      "epoch:13 step:12283 [D loss: 0.610254, acc: 65.62%] [G loss: 1.997291]\n",
      "epoch:13 step:12284 [D loss: 0.586807, acc: 69.53%] [G loss: 1.921887]\n",
      "epoch:13 step:12285 [D loss: 0.632459, acc: 64.06%] [G loss: 1.852621]\n",
      "epoch:13 step:12286 [D loss: 0.683504, acc: 68.75%] [G loss: 1.874637]\n",
      "epoch:13 step:12287 [D loss: 0.649511, acc: 63.28%] [G loss: 2.005281]\n",
      "epoch:13 step:12288 [D loss: 0.616494, acc: 65.62%] [G loss: 2.156494]\n",
      "epoch:13 step:12289 [D loss: 0.668944, acc: 54.69%] [G loss: 1.827898]\n",
      "epoch:13 step:12290 [D loss: 0.686616, acc: 56.25%] [G loss: 1.843476]\n",
      "epoch:13 step:12291 [D loss: 0.647886, acc: 60.94%] [G loss: 1.918698]\n",
      "epoch:13 step:12292 [D loss: 0.595614, acc: 71.09%] [G loss: 1.991116]\n",
      "epoch:13 step:12293 [D loss: 0.606243, acc: 64.84%] [G loss: 2.032344]\n",
      "epoch:13 step:12294 [D loss: 0.613986, acc: 64.84%] [G loss: 2.105376]\n",
      "epoch:13 step:12295 [D loss: 0.615337, acc: 66.41%] [G loss: 2.132007]\n",
      "epoch:13 step:12296 [D loss: 0.568494, acc: 71.88%] [G loss: 2.067388]\n",
      "epoch:13 step:12297 [D loss: 0.628281, acc: 65.62%] [G loss: 2.195189]\n",
      "epoch:13 step:12298 [D loss: 0.661567, acc: 59.38%] [G loss: 2.186907]\n",
      "epoch:13 step:12299 [D loss: 0.593554, acc: 69.53%] [G loss: 2.143003]\n",
      "epoch:13 step:12300 [D loss: 0.619358, acc: 64.84%] [G loss: 2.320259]\n",
      "epoch:13 step:12301 [D loss: 0.630203, acc: 63.28%] [G loss: 1.977803]\n",
      "epoch:13 step:12302 [D loss: 0.659868, acc: 63.28%] [G loss: 1.993598]\n",
      "epoch:13 step:12303 [D loss: 0.667220, acc: 63.28%] [G loss: 2.097625]\n",
      "epoch:13 step:12304 [D loss: 0.629749, acc: 60.16%] [G loss: 2.210841]\n",
      "epoch:13 step:12305 [D loss: 0.624040, acc: 64.06%] [G loss: 2.019877]\n",
      "epoch:13 step:12306 [D loss: 0.722480, acc: 52.34%] [G loss: 1.915173]\n",
      "epoch:13 step:12307 [D loss: 0.603097, acc: 65.62%] [G loss: 2.005698]\n",
      "epoch:13 step:12308 [D loss: 0.652337, acc: 60.16%] [G loss: 1.924740]\n",
      "epoch:13 step:12309 [D loss: 0.659361, acc: 58.59%] [G loss: 1.945796]\n",
      "epoch:13 step:12310 [D loss: 0.625912, acc: 67.19%] [G loss: 1.740038]\n",
      "epoch:13 step:12311 [D loss: 0.682280, acc: 61.72%] [G loss: 2.256062]\n",
      "epoch:13 step:12312 [D loss: 0.636589, acc: 63.28%] [G loss: 1.975364]\n",
      "epoch:13 step:12313 [D loss: 0.619347, acc: 69.53%] [G loss: 2.036162]\n",
      "epoch:13 step:12314 [D loss: 0.653655, acc: 60.16%] [G loss: 1.889626]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:13 step:12315 [D loss: 0.669412, acc: 62.50%] [G loss: 1.872327]\n",
      "epoch:13 step:12316 [D loss: 0.656807, acc: 62.50%] [G loss: 1.773605]\n",
      "epoch:13 step:12317 [D loss: 0.650156, acc: 56.25%] [G loss: 1.967368]\n",
      "epoch:13 step:12318 [D loss: 0.704059, acc: 57.03%] [G loss: 1.997128]\n",
      "epoch:13 step:12319 [D loss: 0.640454, acc: 63.28%] [G loss: 1.854353]\n",
      "epoch:13 step:12320 [D loss: 0.638556, acc: 64.06%] [G loss: 1.925762]\n",
      "epoch:13 step:12321 [D loss: 0.613786, acc: 65.62%] [G loss: 1.840665]\n",
      "epoch:13 step:12322 [D loss: 0.579119, acc: 71.09%] [G loss: 1.999583]\n",
      "epoch:13 step:12323 [D loss: 0.669044, acc: 58.59%] [G loss: 1.798974]\n",
      "epoch:13 step:12324 [D loss: 0.634045, acc: 61.72%] [G loss: 1.933544]\n",
      "epoch:13 step:12325 [D loss: 0.677246, acc: 56.25%] [G loss: 1.925674]\n",
      "epoch:13 step:12326 [D loss: 0.663450, acc: 57.81%] [G loss: 1.988016]\n",
      "epoch:13 step:12327 [D loss: 0.626109, acc: 63.28%] [G loss: 2.089927]\n",
      "epoch:13 step:12328 [D loss: 0.676115, acc: 57.81%] [G loss: 2.033736]\n",
      "epoch:13 step:12329 [D loss: 0.693753, acc: 55.47%] [G loss: 1.821395]\n",
      "epoch:13 step:12330 [D loss: 0.608857, acc: 65.62%] [G loss: 2.115590]\n",
      "epoch:13 step:12331 [D loss: 0.654668, acc: 64.84%] [G loss: 1.855560]\n",
      "epoch:13 step:12332 [D loss: 0.565920, acc: 70.31%] [G loss: 2.049396]\n",
      "epoch:13 step:12333 [D loss: 0.583297, acc: 70.31%] [G loss: 2.152323]\n",
      "epoch:13 step:12334 [D loss: 0.628241, acc: 67.97%] [G loss: 2.093677]\n",
      "epoch:13 step:12335 [D loss: 0.661422, acc: 57.81%] [G loss: 2.058835]\n",
      "epoch:13 step:12336 [D loss: 0.580770, acc: 68.75%] [G loss: 1.917097]\n",
      "epoch:13 step:12337 [D loss: 0.623213, acc: 68.75%] [G loss: 1.932619]\n",
      "epoch:13 step:12338 [D loss: 0.620347, acc: 66.41%] [G loss: 1.919375]\n",
      "epoch:13 step:12339 [D loss: 0.665062, acc: 60.16%] [G loss: 1.934030]\n",
      "epoch:13 step:12340 [D loss: 0.722130, acc: 57.03%] [G loss: 1.961902]\n",
      "epoch:13 step:12341 [D loss: 0.618992, acc: 64.06%] [G loss: 1.926818]\n",
      "epoch:13 step:12342 [D loss: 0.664236, acc: 64.84%] [G loss: 1.918491]\n",
      "epoch:13 step:12343 [D loss: 0.580129, acc: 71.09%] [G loss: 1.902867]\n",
      "epoch:13 step:12344 [D loss: 0.696054, acc: 57.81%] [G loss: 1.898300]\n",
      "epoch:13 step:12345 [D loss: 0.659889, acc: 60.16%] [G loss: 1.933077]\n",
      "epoch:13 step:12346 [D loss: 0.692782, acc: 58.59%] [G loss: 1.771702]\n",
      "epoch:13 step:12347 [D loss: 0.643191, acc: 64.84%] [G loss: 1.847505]\n",
      "epoch:13 step:12348 [D loss: 0.647769, acc: 64.84%] [G loss: 1.942851]\n",
      "epoch:13 step:12349 [D loss: 0.633872, acc: 64.84%] [G loss: 1.883290]\n",
      "epoch:13 step:12350 [D loss: 0.648613, acc: 61.72%] [G loss: 2.012542]\n",
      "epoch:13 step:12351 [D loss: 0.614118, acc: 61.72%] [G loss: 1.774632]\n",
      "epoch:13 step:12352 [D loss: 0.649027, acc: 65.62%] [G loss: 2.191468]\n",
      "epoch:13 step:12353 [D loss: 0.637901, acc: 61.72%] [G loss: 1.916591]\n",
      "epoch:13 step:12354 [D loss: 0.657711, acc: 59.38%] [G loss: 1.748737]\n",
      "epoch:13 step:12355 [D loss: 0.671309, acc: 58.59%] [G loss: 1.869081]\n",
      "epoch:13 step:12356 [D loss: 0.682265, acc: 56.25%] [G loss: 1.885329]\n",
      "epoch:13 step:12357 [D loss: 0.667532, acc: 57.81%] [G loss: 1.835086]\n",
      "epoch:13 step:12358 [D loss: 0.623638, acc: 68.75%] [G loss: 1.839891]\n",
      "epoch:13 step:12359 [D loss: 0.653603, acc: 61.72%] [G loss: 2.059206]\n",
      "epoch:13 step:12360 [D loss: 0.669681, acc: 60.16%] [G loss: 1.874468]\n",
      "epoch:13 step:12361 [D loss: 0.672889, acc: 61.72%] [G loss: 1.841518]\n",
      "epoch:13 step:12362 [D loss: 0.658196, acc: 64.06%] [G loss: 1.712009]\n",
      "epoch:13 step:12363 [D loss: 0.663964, acc: 61.72%] [G loss: 1.922747]\n",
      "epoch:13 step:12364 [D loss: 0.713004, acc: 56.25%] [G loss: 1.876386]\n",
      "epoch:13 step:12365 [D loss: 0.622587, acc: 64.84%] [G loss: 1.890721]\n",
      "epoch:13 step:12366 [D loss: 0.660138, acc: 57.03%] [G loss: 1.926856]\n",
      "epoch:13 step:12367 [D loss: 0.696004, acc: 55.47%] [G loss: 1.786500]\n",
      "epoch:13 step:12368 [D loss: 0.626200, acc: 65.62%] [G loss: 1.975860]\n",
      "epoch:13 step:12369 [D loss: 0.638634, acc: 61.72%] [G loss: 1.979781]\n",
      "epoch:13 step:12370 [D loss: 0.658362, acc: 65.62%] [G loss: 1.956639]\n",
      "epoch:13 step:12371 [D loss: 0.646975, acc: 61.72%] [G loss: 1.774646]\n",
      "epoch:13 step:12372 [D loss: 0.687363, acc: 57.81%] [G loss: 1.923475]\n",
      "epoch:13 step:12373 [D loss: 0.622037, acc: 66.41%] [G loss: 1.911982]\n",
      "epoch:13 step:12374 [D loss: 0.597727, acc: 64.06%] [G loss: 1.864061]\n",
      "epoch:13 step:12375 [D loss: 0.568334, acc: 71.88%] [G loss: 2.086208]\n",
      "epoch:13 step:12376 [D loss: 0.632861, acc: 59.38%] [G loss: 1.978221]\n",
      "epoch:13 step:12377 [D loss: 0.640305, acc: 65.62%] [G loss: 1.940393]\n",
      "epoch:13 step:12378 [D loss: 0.599556, acc: 64.84%] [G loss: 2.055084]\n",
      "epoch:13 step:12379 [D loss: 0.698123, acc: 56.25%] [G loss: 1.922149]\n",
      "epoch:13 step:12380 [D loss: 0.669478, acc: 60.16%] [G loss: 2.070282]\n",
      "epoch:13 step:12381 [D loss: 0.741800, acc: 52.34%] [G loss: 1.765812]\n",
      "epoch:13 step:12382 [D loss: 0.631908, acc: 61.72%] [G loss: 2.058874]\n",
      "epoch:13 step:12383 [D loss: 0.639930, acc: 57.81%] [G loss: 1.769402]\n",
      "epoch:13 step:12384 [D loss: 0.639618, acc: 64.06%] [G loss: 2.029123]\n",
      "epoch:13 step:12385 [D loss: 0.611279, acc: 64.84%] [G loss: 1.900592]\n",
      "epoch:13 step:12386 [D loss: 0.653970, acc: 64.84%] [G loss: 2.009482]\n",
      "epoch:13 step:12387 [D loss: 0.607817, acc: 64.84%] [G loss: 2.014154]\n",
      "epoch:13 step:12388 [D loss: 0.705418, acc: 58.59%] [G loss: 2.099598]\n",
      "epoch:13 step:12389 [D loss: 0.582206, acc: 69.53%] [G loss: 2.059721]\n",
      "epoch:13 step:12390 [D loss: 0.611563, acc: 68.75%] [G loss: 2.044208]\n",
      "epoch:13 step:12391 [D loss: 0.664346, acc: 59.38%] [G loss: 1.893003]\n",
      "epoch:13 step:12392 [D loss: 0.671091, acc: 61.72%] [G loss: 1.845520]\n",
      "epoch:13 step:12393 [D loss: 0.637409, acc: 60.16%] [G loss: 1.961178]\n",
      "epoch:13 step:12394 [D loss: 0.645461, acc: 62.50%] [G loss: 1.898420]\n",
      "epoch:13 step:12395 [D loss: 0.684455, acc: 57.81%] [G loss: 1.784990]\n",
      "epoch:13 step:12396 [D loss: 0.677622, acc: 59.38%] [G loss: 1.825604]\n",
      "epoch:13 step:12397 [D loss: 0.643301, acc: 60.16%] [G loss: 2.227159]\n",
      "epoch:13 step:12398 [D loss: 0.593450, acc: 70.31%] [G loss: 1.974428]\n",
      "epoch:13 step:12399 [D loss: 0.624387, acc: 67.97%] [G loss: 1.996118]\n",
      "epoch:13 step:12400 [D loss: 0.587962, acc: 67.19%] [G loss: 2.168422]\n",
      "##############\n",
      "[2.47872018 1.44286283 6.35433436 4.79595751 3.65516214 5.66107431\n",
      " 4.43668426 4.81513423 4.59927566 3.54732425]\n",
      "##########\n",
      "epoch:13 step:12401 [D loss: 0.726111, acc: 54.69%] [G loss: 1.812593]\n",
      "epoch:13 step:12402 [D loss: 0.677279, acc: 57.03%] [G loss: 1.837951]\n",
      "epoch:13 step:12403 [D loss: 0.666354, acc: 57.81%] [G loss: 1.940661]\n",
      "epoch:13 step:12404 [D loss: 0.626018, acc: 66.41%] [G loss: 2.052622]\n",
      "epoch:13 step:12405 [D loss: 0.698397, acc: 57.03%] [G loss: 1.971324]\n",
      "epoch:13 step:12406 [D loss: 0.653497, acc: 63.28%] [G loss: 1.959097]\n",
      "epoch:13 step:12407 [D loss: 0.670393, acc: 60.16%] [G loss: 1.904912]\n",
      "epoch:13 step:12408 [D loss: 0.660044, acc: 63.28%] [G loss: 1.854779]\n",
      "epoch:13 step:12409 [D loss: 0.696333, acc: 51.56%] [G loss: 1.909596]\n",
      "epoch:13 step:12410 [D loss: 0.583711, acc: 70.31%] [G loss: 2.055091]\n",
      "epoch:13 step:12411 [D loss: 0.568273, acc: 68.75%] [G loss: 2.102834]\n",
      "epoch:13 step:12412 [D loss: 0.579307, acc: 64.84%] [G loss: 2.295404]\n",
      "epoch:13 step:12413 [D loss: 0.566997, acc: 71.09%] [G loss: 2.420205]\n",
      "epoch:13 step:12414 [D loss: 0.599231, acc: 67.19%] [G loss: 1.852454]\n",
      "epoch:13 step:12415 [D loss: 0.621242, acc: 64.06%] [G loss: 1.998055]\n",
      "epoch:13 step:12416 [D loss: 0.688420, acc: 58.59%] [G loss: 2.027477]\n",
      "epoch:13 step:12417 [D loss: 0.581107, acc: 77.34%] [G loss: 2.031174]\n",
      "epoch:13 step:12418 [D loss: 0.676738, acc: 54.69%] [G loss: 2.045973]\n",
      "epoch:13 step:12419 [D loss: 0.632629, acc: 63.28%] [G loss: 2.019769]\n",
      "epoch:13 step:12420 [D loss: 0.624249, acc: 64.06%] [G loss: 1.980510]\n",
      "epoch:13 step:12421 [D loss: 0.648203, acc: 60.94%] [G loss: 1.933107]\n",
      "epoch:13 step:12422 [D loss: 0.613415, acc: 66.41%] [G loss: 1.995480]\n",
      "epoch:13 step:12423 [D loss: 0.613487, acc: 65.62%] [G loss: 2.198953]\n",
      "epoch:13 step:12424 [D loss: 0.610158, acc: 67.97%] [G loss: 2.034227]\n",
      "epoch:13 step:12425 [D loss: 0.632631, acc: 64.06%] [G loss: 2.039510]\n",
      "epoch:13 step:12426 [D loss: 0.602706, acc: 67.97%] [G loss: 2.045651]\n",
      "epoch:13 step:12427 [D loss: 0.614490, acc: 61.72%] [G loss: 1.961050]\n",
      "epoch:13 step:12428 [D loss: 0.600416, acc: 70.31%] [G loss: 2.060104]\n",
      "epoch:13 step:12429 [D loss: 0.631993, acc: 64.06%] [G loss: 2.103006]\n",
      "epoch:13 step:12430 [D loss: 0.674832, acc: 57.03%] [G loss: 1.969418]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:13 step:12431 [D loss: 0.716307, acc: 50.78%] [G loss: 1.821111]\n",
      "epoch:13 step:12432 [D loss: 0.623560, acc: 67.97%] [G loss: 1.924446]\n",
      "epoch:13 step:12433 [D loss: 0.621652, acc: 64.84%] [G loss: 1.922749]\n",
      "epoch:13 step:12434 [D loss: 0.619918, acc: 71.09%] [G loss: 1.803367]\n",
      "epoch:13 step:12435 [D loss: 0.657009, acc: 63.28%] [G loss: 1.851808]\n",
      "epoch:13 step:12436 [D loss: 0.660979, acc: 60.94%] [G loss: 1.959841]\n",
      "epoch:13 step:12437 [D loss: 0.693629, acc: 57.03%] [G loss: 1.746785]\n",
      "epoch:13 step:12438 [D loss: 0.668162, acc: 60.94%] [G loss: 1.843439]\n",
      "epoch:13 step:12439 [D loss: 0.678998, acc: 61.72%] [G loss: 2.020921]\n",
      "epoch:13 step:12440 [D loss: 0.636921, acc: 59.38%] [G loss: 2.077263]\n",
      "epoch:13 step:12441 [D loss: 0.659046, acc: 64.06%] [G loss: 1.879807]\n",
      "epoch:13 step:12442 [D loss: 0.614914, acc: 68.75%] [G loss: 2.107264]\n",
      "epoch:13 step:12443 [D loss: 0.622858, acc: 67.19%] [G loss: 1.928070]\n",
      "epoch:13 step:12444 [D loss: 0.638470, acc: 64.84%] [G loss: 2.010803]\n",
      "epoch:13 step:12445 [D loss: 0.582554, acc: 72.66%] [G loss: 2.013323]\n",
      "epoch:13 step:12446 [D loss: 0.657020, acc: 60.94%] [G loss: 1.920623]\n",
      "epoch:13 step:12447 [D loss: 0.684353, acc: 59.38%] [G loss: 2.034662]\n",
      "epoch:13 step:12448 [D loss: 0.582973, acc: 67.97%] [G loss: 2.030832]\n",
      "epoch:13 step:12449 [D loss: 0.654325, acc: 60.16%] [G loss: 1.863390]\n",
      "epoch:13 step:12450 [D loss: 0.704077, acc: 53.12%] [G loss: 1.916416]\n",
      "epoch:13 step:12451 [D loss: 0.605075, acc: 67.19%] [G loss: 1.851142]\n",
      "epoch:13 step:12452 [D loss: 0.651936, acc: 60.94%] [G loss: 1.953681]\n",
      "epoch:13 step:12453 [D loss: 0.604745, acc: 67.19%] [G loss: 1.833836]\n",
      "epoch:13 step:12454 [D loss: 0.607012, acc: 68.75%] [G loss: 1.998995]\n",
      "epoch:13 step:12455 [D loss: 0.628071, acc: 65.62%] [G loss: 2.172018]\n",
      "epoch:13 step:12456 [D loss: 0.575279, acc: 66.41%] [G loss: 2.025246]\n",
      "epoch:13 step:12457 [D loss: 0.693456, acc: 59.38%] [G loss: 2.026167]\n",
      "epoch:13 step:12458 [D loss: 0.620774, acc: 64.84%] [G loss: 2.048228]\n",
      "epoch:13 step:12459 [D loss: 0.674724, acc: 59.38%] [G loss: 1.812783]\n",
      "epoch:13 step:12460 [D loss: 0.631745, acc: 62.50%] [G loss: 1.930944]\n",
      "epoch:13 step:12461 [D loss: 0.613185, acc: 68.75%] [G loss: 2.064458]\n",
      "epoch:13 step:12462 [D loss: 0.623816, acc: 69.53%] [G loss: 1.801385]\n",
      "epoch:13 step:12463 [D loss: 0.683352, acc: 60.94%] [G loss: 1.994251]\n",
      "epoch:13 step:12464 [D loss: 0.614002, acc: 69.53%] [G loss: 2.004917]\n",
      "epoch:13 step:12465 [D loss: 0.603840, acc: 65.62%] [G loss: 1.945787]\n",
      "epoch:13 step:12466 [D loss: 0.635636, acc: 67.19%] [G loss: 2.128800]\n",
      "epoch:13 step:12467 [D loss: 0.611359, acc: 73.44%] [G loss: 2.085478]\n",
      "epoch:13 step:12468 [D loss: 0.703140, acc: 57.03%] [G loss: 2.023876]\n",
      "epoch:13 step:12469 [D loss: 0.674981, acc: 62.50%] [G loss: 1.950599]\n",
      "epoch:13 step:12470 [D loss: 0.644839, acc: 59.38%] [G loss: 1.788892]\n",
      "epoch:13 step:12471 [D loss: 0.623497, acc: 64.84%] [G loss: 1.895243]\n",
      "epoch:13 step:12472 [D loss: 0.675366, acc: 57.03%] [G loss: 1.882802]\n",
      "epoch:13 step:12473 [D loss: 0.648103, acc: 61.72%] [G loss: 1.941372]\n",
      "epoch:13 step:12474 [D loss: 0.608516, acc: 67.97%] [G loss: 2.060685]\n",
      "epoch:13 step:12475 [D loss: 0.618227, acc: 63.28%] [G loss: 1.994138]\n",
      "epoch:13 step:12476 [D loss: 0.642815, acc: 67.97%] [G loss: 2.000466]\n",
      "epoch:13 step:12477 [D loss: 0.608432, acc: 65.62%] [G loss: 1.956173]\n",
      "epoch:13 step:12478 [D loss: 0.647106, acc: 61.72%] [G loss: 1.928329]\n",
      "epoch:13 step:12479 [D loss: 0.646013, acc: 67.19%] [G loss: 2.167755]\n",
      "epoch:13 step:12480 [D loss: 0.601076, acc: 64.06%] [G loss: 2.028161]\n",
      "epoch:13 step:12481 [D loss: 0.620997, acc: 67.19%] [G loss: 2.112329]\n",
      "epoch:13 step:12482 [D loss: 0.658929, acc: 59.38%] [G loss: 1.929830]\n",
      "epoch:13 step:12483 [D loss: 0.649446, acc: 63.28%] [G loss: 2.027697]\n",
      "epoch:13 step:12484 [D loss: 0.607290, acc: 71.09%] [G loss: 1.909986]\n",
      "epoch:13 step:12485 [D loss: 0.639756, acc: 67.19%] [G loss: 1.982669]\n",
      "epoch:13 step:12486 [D loss: 0.596489, acc: 65.62%] [G loss: 1.966297]\n",
      "epoch:13 step:12487 [D loss: 0.669015, acc: 56.25%] [G loss: 2.002749]\n",
      "epoch:13 step:12488 [D loss: 0.613819, acc: 67.97%] [G loss: 2.119116]\n",
      "epoch:13 step:12489 [D loss: 0.626962, acc: 64.84%] [G loss: 1.878093]\n",
      "epoch:13 step:12490 [D loss: 0.602235, acc: 63.28%] [G loss: 2.223330]\n",
      "epoch:13 step:12491 [D loss: 0.605060, acc: 71.09%] [G loss: 1.906132]\n",
      "epoch:13 step:12492 [D loss: 0.678588, acc: 57.03%] [G loss: 1.986973]\n",
      "epoch:13 step:12493 [D loss: 0.560748, acc: 74.22%] [G loss: 2.351360]\n",
      "epoch:13 step:12494 [D loss: 0.619548, acc: 64.84%] [G loss: 2.368890]\n",
      "epoch:13 step:12495 [D loss: 0.567698, acc: 75.00%] [G loss: 2.357094]\n",
      "epoch:13 step:12496 [D loss: 0.549674, acc: 71.88%] [G loss: 2.361731]\n",
      "epoch:13 step:12497 [D loss: 0.690963, acc: 57.03%] [G loss: 1.907511]\n",
      "epoch:13 step:12498 [D loss: 0.659062, acc: 58.59%] [G loss: 1.922977]\n",
      "epoch:13 step:12499 [D loss: 0.625856, acc: 63.28%] [G loss: 2.275960]\n",
      "epoch:13 step:12500 [D loss: 0.646647, acc: 64.84%] [G loss: 2.052582]\n",
      "epoch:13 step:12501 [D loss: 0.678015, acc: 54.69%] [G loss: 1.932100]\n",
      "epoch:13 step:12502 [D loss: 0.602553, acc: 71.88%] [G loss: 2.151759]\n",
      "epoch:13 step:12503 [D loss: 0.624360, acc: 67.97%] [G loss: 1.931926]\n",
      "epoch:13 step:12504 [D loss: 0.647480, acc: 60.16%] [G loss: 1.913654]\n",
      "epoch:13 step:12505 [D loss: 0.669963, acc: 61.72%] [G loss: 1.929498]\n",
      "epoch:13 step:12506 [D loss: 0.606124, acc: 69.53%] [G loss: 2.188911]\n",
      "epoch:13 step:12507 [D loss: 0.608312, acc: 66.41%] [G loss: 1.983504]\n",
      "epoch:13 step:12508 [D loss: 0.656943, acc: 61.72%] [G loss: 1.916175]\n",
      "epoch:13 step:12509 [D loss: 0.674847, acc: 60.16%] [G loss: 2.070281]\n",
      "epoch:13 step:12510 [D loss: 0.590125, acc: 65.62%] [G loss: 2.037210]\n",
      "epoch:13 step:12511 [D loss: 0.636283, acc: 64.06%] [G loss: 2.005574]\n",
      "epoch:13 step:12512 [D loss: 0.635387, acc: 65.62%] [G loss: 1.995142]\n",
      "epoch:13 step:12513 [D loss: 0.579149, acc: 71.09%] [G loss: 2.054336]\n",
      "epoch:13 step:12514 [D loss: 0.636818, acc: 59.38%] [G loss: 1.975118]\n",
      "epoch:13 step:12515 [D loss: 0.618192, acc: 67.19%] [G loss: 2.081574]\n",
      "epoch:13 step:12516 [D loss: 0.647551, acc: 63.28%] [G loss: 2.090012]\n",
      "epoch:13 step:12517 [D loss: 0.677777, acc: 57.81%] [G loss: 1.853590]\n",
      "epoch:13 step:12518 [D loss: 0.642304, acc: 59.38%] [G loss: 2.107815]\n",
      "epoch:13 step:12519 [D loss: 0.613236, acc: 66.41%] [G loss: 2.124778]\n",
      "epoch:13 step:12520 [D loss: 0.606560, acc: 69.53%] [G loss: 1.938079]\n",
      "epoch:13 step:12521 [D loss: 0.561242, acc: 76.56%] [G loss: 2.198493]\n",
      "epoch:13 step:12522 [D loss: 0.709739, acc: 57.03%] [G loss: 1.915269]\n",
      "epoch:13 step:12523 [D loss: 0.708581, acc: 56.25%] [G loss: 1.826154]\n",
      "epoch:13 step:12524 [D loss: 0.698259, acc: 59.38%] [G loss: 1.997864]\n",
      "epoch:13 step:12525 [D loss: 0.701091, acc: 64.06%] [G loss: 1.966861]\n",
      "epoch:13 step:12526 [D loss: 0.616348, acc: 65.62%] [G loss: 2.126115]\n",
      "epoch:13 step:12527 [D loss: 0.596014, acc: 71.88%] [G loss: 2.018624]\n",
      "epoch:13 step:12528 [D loss: 0.610450, acc: 64.06%] [G loss: 2.254698]\n",
      "epoch:13 step:12529 [D loss: 0.702299, acc: 57.03%] [G loss: 1.857260]\n",
      "epoch:13 step:12530 [D loss: 0.721493, acc: 54.69%] [G loss: 1.876767]\n",
      "epoch:13 step:12531 [D loss: 0.661430, acc: 58.59%] [G loss: 1.890109]\n",
      "epoch:13 step:12532 [D loss: 0.651839, acc: 64.84%] [G loss: 1.832400]\n",
      "epoch:13 step:12533 [D loss: 0.680298, acc: 59.38%] [G loss: 1.763131]\n",
      "epoch:13 step:12534 [D loss: 0.671990, acc: 58.59%] [G loss: 1.956133]\n",
      "epoch:13 step:12535 [D loss: 0.651839, acc: 67.19%] [G loss: 2.141767]\n",
      "epoch:13 step:12536 [D loss: 0.657097, acc: 61.72%] [G loss: 1.882459]\n",
      "epoch:13 step:12537 [D loss: 0.687015, acc: 60.16%] [G loss: 1.999339]\n",
      "epoch:13 step:12538 [D loss: 0.627041, acc: 65.62%] [G loss: 1.985570]\n",
      "epoch:13 step:12539 [D loss: 0.565023, acc: 75.78%] [G loss: 2.052950]\n",
      "epoch:13 step:12540 [D loss: 0.579877, acc: 71.09%] [G loss: 2.158211]\n",
      "epoch:13 step:12541 [D loss: 0.642282, acc: 63.28%] [G loss: 2.200360]\n",
      "epoch:13 step:12542 [D loss: 0.644931, acc: 60.16%] [G loss: 1.820842]\n",
      "epoch:13 step:12543 [D loss: 0.634141, acc: 60.16%] [G loss: 1.937271]\n",
      "epoch:13 step:12544 [D loss: 0.683405, acc: 59.38%] [G loss: 1.841716]\n",
      "epoch:13 step:12545 [D loss: 0.613735, acc: 62.50%] [G loss: 2.090127]\n",
      "epoch:13 step:12546 [D loss: 0.678004, acc: 53.91%] [G loss: 2.034884]\n",
      "epoch:13 step:12547 [D loss: 0.621730, acc: 61.72%] [G loss: 2.100873]\n",
      "epoch:13 step:12548 [D loss: 0.647850, acc: 64.84%] [G loss: 2.139255]\n",
      "epoch:13 step:12549 [D loss: 0.611288, acc: 70.31%] [G loss: 2.068059]\n",
      "epoch:13 step:12550 [D loss: 0.673771, acc: 56.25%] [G loss: 1.915140]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:13 step:12551 [D loss: 0.601019, acc: 68.75%] [G loss: 1.938886]\n",
      "epoch:13 step:12552 [D loss: 0.614761, acc: 67.97%] [G loss: 2.199008]\n",
      "epoch:13 step:12553 [D loss: 0.665267, acc: 60.16%] [G loss: 1.983063]\n",
      "epoch:13 step:12554 [D loss: 0.669709, acc: 60.16%] [G loss: 1.913970]\n",
      "epoch:13 step:12555 [D loss: 0.607728, acc: 65.62%] [G loss: 2.021909]\n",
      "epoch:13 step:12556 [D loss: 0.688620, acc: 57.03%] [G loss: 1.911314]\n",
      "epoch:13 step:12557 [D loss: 0.736114, acc: 50.78%] [G loss: 1.940527]\n",
      "epoch:13 step:12558 [D loss: 0.659004, acc: 60.16%] [G loss: 1.835315]\n",
      "epoch:13 step:12559 [D loss: 0.635471, acc: 70.31%] [G loss: 2.031040]\n",
      "epoch:13 step:12560 [D loss: 0.640093, acc: 57.81%] [G loss: 1.986837]\n",
      "epoch:13 step:12561 [D loss: 0.622782, acc: 64.84%] [G loss: 1.978213]\n",
      "epoch:13 step:12562 [D loss: 0.589702, acc: 70.31%] [G loss: 2.090190]\n",
      "epoch:13 step:12563 [D loss: 0.630742, acc: 64.06%] [G loss: 1.894986]\n",
      "epoch:13 step:12564 [D loss: 0.588019, acc: 71.09%] [G loss: 1.930728]\n",
      "epoch:13 step:12565 [D loss: 0.683268, acc: 60.94%] [G loss: 1.916221]\n",
      "epoch:13 step:12566 [D loss: 0.623354, acc: 64.06%] [G loss: 1.876447]\n",
      "epoch:13 step:12567 [D loss: 0.651264, acc: 60.16%] [G loss: 1.918664]\n",
      "epoch:13 step:12568 [D loss: 0.654445, acc: 63.28%] [G loss: 1.890412]\n",
      "epoch:13 step:12569 [D loss: 0.677032, acc: 53.91%] [G loss: 1.820742]\n",
      "epoch:13 step:12570 [D loss: 0.630772, acc: 63.28%] [G loss: 1.850354]\n",
      "epoch:13 step:12571 [D loss: 0.708496, acc: 60.94%] [G loss: 1.835856]\n",
      "epoch:13 step:12572 [D loss: 0.713502, acc: 53.91%] [G loss: 1.847492]\n",
      "epoch:13 step:12573 [D loss: 0.607939, acc: 67.19%] [G loss: 1.909898]\n",
      "epoch:13 step:12574 [D loss: 0.608878, acc: 71.09%] [G loss: 1.903637]\n",
      "epoch:13 step:12575 [D loss: 0.621741, acc: 63.28%] [G loss: 1.995307]\n",
      "epoch:13 step:12576 [D loss: 0.693992, acc: 57.03%] [G loss: 1.881401]\n",
      "epoch:13 step:12577 [D loss: 0.721544, acc: 55.47%] [G loss: 1.776389]\n",
      "epoch:13 step:12578 [D loss: 0.648880, acc: 63.28%] [G loss: 1.748581]\n",
      "epoch:13 step:12579 [D loss: 0.596197, acc: 69.53%] [G loss: 1.843123]\n",
      "epoch:13 step:12580 [D loss: 0.640687, acc: 64.06%] [G loss: 1.880576]\n",
      "epoch:13 step:12581 [D loss: 0.673485, acc: 52.34%] [G loss: 1.843295]\n",
      "epoch:13 step:12582 [D loss: 0.653353, acc: 62.50%] [G loss: 1.919205]\n",
      "epoch:13 step:12583 [D loss: 0.635622, acc: 67.97%] [G loss: 1.843735]\n",
      "epoch:13 step:12584 [D loss: 0.604819, acc: 65.62%] [G loss: 2.101638]\n",
      "epoch:13 step:12585 [D loss: 0.669598, acc: 60.16%] [G loss: 1.937514]\n",
      "epoch:13 step:12586 [D loss: 0.602649, acc: 65.62%] [G loss: 2.101102]\n",
      "epoch:13 step:12587 [D loss: 0.620412, acc: 64.84%] [G loss: 2.269110]\n",
      "epoch:13 step:12588 [D loss: 0.658731, acc: 59.38%] [G loss: 1.901164]\n",
      "epoch:13 step:12589 [D loss: 0.639956, acc: 64.06%] [G loss: 1.872859]\n",
      "epoch:13 step:12590 [D loss: 0.643720, acc: 59.38%] [G loss: 1.895098]\n",
      "epoch:13 step:12591 [D loss: 0.630549, acc: 65.62%] [G loss: 2.117551]\n",
      "epoch:13 step:12592 [D loss: 0.660185, acc: 64.06%] [G loss: 1.908911]\n",
      "epoch:13 step:12593 [D loss: 0.650632, acc: 62.50%] [G loss: 1.864420]\n",
      "epoch:13 step:12594 [D loss: 0.642842, acc: 62.50%] [G loss: 2.004533]\n",
      "epoch:13 step:12595 [D loss: 0.639338, acc: 65.62%] [G loss: 1.931002]\n",
      "epoch:13 step:12596 [D loss: 0.676789, acc: 60.94%] [G loss: 1.936264]\n",
      "epoch:13 step:12597 [D loss: 0.604129, acc: 66.41%] [G loss: 1.939228]\n",
      "epoch:13 step:12598 [D loss: 0.666114, acc: 61.72%] [G loss: 1.934971]\n",
      "epoch:13 step:12599 [D loss: 0.636688, acc: 70.31%] [G loss: 1.891479]\n",
      "epoch:13 step:12600 [D loss: 0.590683, acc: 69.53%] [G loss: 1.911172]\n",
      "##############\n",
      "[2.51940652 1.36426342 6.26973673 4.87909893 3.61130708 5.62159458\n",
      " 4.37285219 4.60657478 4.77877209 3.64116246]\n",
      "##########\n",
      "epoch:13 step:12601 [D loss: 0.693465, acc: 63.28%] [G loss: 1.815380]\n",
      "epoch:13 step:12602 [D loss: 0.602829, acc: 67.97%] [G loss: 1.986414]\n",
      "epoch:13 step:12603 [D loss: 0.648782, acc: 61.72%] [G loss: 1.882389]\n",
      "epoch:13 step:12604 [D loss: 0.618191, acc: 66.41%] [G loss: 1.946650]\n",
      "epoch:13 step:12605 [D loss: 0.655284, acc: 60.16%] [G loss: 1.898817]\n",
      "epoch:13 step:12606 [D loss: 0.670179, acc: 62.50%] [G loss: 2.077636]\n",
      "epoch:13 step:12607 [D loss: 0.646731, acc: 64.84%] [G loss: 1.894160]\n",
      "epoch:13 step:12608 [D loss: 0.601636, acc: 64.84%] [G loss: 2.186502]\n",
      "epoch:13 step:12609 [D loss: 0.593658, acc: 71.88%] [G loss: 2.188355]\n",
      "epoch:13 step:12610 [D loss: 0.601541, acc: 68.75%] [G loss: 2.112084]\n",
      "epoch:13 step:12611 [D loss: 0.597500, acc: 68.75%] [G loss: 2.207789]\n",
      "epoch:13 step:12612 [D loss: 0.626555, acc: 62.50%] [G loss: 2.050149]\n",
      "epoch:13 step:12613 [D loss: 0.646433, acc: 64.06%] [G loss: 1.940459]\n",
      "epoch:13 step:12614 [D loss: 0.614483, acc: 67.19%] [G loss: 1.923681]\n",
      "epoch:13 step:12615 [D loss: 0.653691, acc: 60.94%] [G loss: 2.026082]\n",
      "epoch:13 step:12616 [D loss: 0.623553, acc: 67.97%] [G loss: 1.903715]\n",
      "epoch:13 step:12617 [D loss: 0.621026, acc: 60.94%] [G loss: 1.926721]\n",
      "epoch:13 step:12618 [D loss: 0.723652, acc: 53.91%] [G loss: 1.786261]\n",
      "epoch:13 step:12619 [D loss: 0.641120, acc: 62.50%] [G loss: 1.721971]\n",
      "epoch:13 step:12620 [D loss: 0.663685, acc: 57.03%] [G loss: 1.937064]\n",
      "epoch:13 step:12621 [D loss: 0.607540, acc: 67.19%] [G loss: 1.968389]\n",
      "epoch:13 step:12622 [D loss: 0.640121, acc: 63.28%] [G loss: 1.862136]\n",
      "epoch:13 step:12623 [D loss: 0.634998, acc: 67.19%] [G loss: 1.877437]\n",
      "epoch:13 step:12624 [D loss: 0.652952, acc: 61.72%] [G loss: 1.963810]\n",
      "epoch:13 step:12625 [D loss: 0.666195, acc: 62.50%] [G loss: 1.931815]\n",
      "epoch:13 step:12626 [D loss: 0.625911, acc: 64.06%] [G loss: 2.039768]\n",
      "epoch:13 step:12627 [D loss: 0.647360, acc: 62.50%] [G loss: 1.946877]\n",
      "epoch:13 step:12628 [D loss: 0.605077, acc: 71.09%] [G loss: 2.093554]\n",
      "epoch:13 step:12629 [D loss: 0.637142, acc: 64.84%] [G loss: 1.785028]\n",
      "epoch:13 step:12630 [D loss: 0.671835, acc: 60.16%] [G loss: 1.947806]\n",
      "epoch:13 step:12631 [D loss: 0.614766, acc: 68.75%] [G loss: 1.981012]\n",
      "epoch:13 step:12632 [D loss: 0.611180, acc: 71.09%] [G loss: 1.920786]\n",
      "epoch:13 step:12633 [D loss: 0.650263, acc: 60.94%] [G loss: 2.024271]\n",
      "epoch:13 step:12634 [D loss: 0.640695, acc: 63.28%] [G loss: 2.001171]\n",
      "epoch:13 step:12635 [D loss: 0.639384, acc: 66.41%] [G loss: 1.998407]\n",
      "epoch:13 step:12636 [D loss: 0.687194, acc: 54.69%] [G loss: 2.018956]\n",
      "epoch:13 step:12637 [D loss: 0.625659, acc: 61.72%] [G loss: 1.959459]\n",
      "epoch:13 step:12638 [D loss: 0.620488, acc: 70.31%] [G loss: 1.970296]\n",
      "epoch:13 step:12639 [D loss: 0.629514, acc: 62.50%] [G loss: 1.881369]\n",
      "epoch:13 step:12640 [D loss: 0.653988, acc: 60.94%] [G loss: 1.904791]\n",
      "epoch:13 step:12641 [D loss: 0.666751, acc: 57.03%] [G loss: 1.952694]\n",
      "epoch:13 step:12642 [D loss: 0.659551, acc: 65.62%] [G loss: 1.983660]\n",
      "epoch:13 step:12643 [D loss: 0.620486, acc: 64.84%] [G loss: 1.852696]\n",
      "epoch:13 step:12644 [D loss: 0.677934, acc: 59.38%] [G loss: 1.855310]\n",
      "epoch:13 step:12645 [D loss: 0.710245, acc: 53.12%] [G loss: 1.748380]\n",
      "epoch:13 step:12646 [D loss: 0.650704, acc: 62.50%] [G loss: 1.943026]\n",
      "epoch:13 step:12647 [D loss: 0.625271, acc: 66.41%] [G loss: 1.809974]\n",
      "epoch:13 step:12648 [D loss: 0.638442, acc: 65.62%] [G loss: 1.814444]\n",
      "epoch:13 step:12649 [D loss: 0.631864, acc: 62.50%] [G loss: 2.000135]\n",
      "epoch:13 step:12650 [D loss: 0.634118, acc: 64.06%] [G loss: 2.123995]\n",
      "epoch:13 step:12651 [D loss: 0.661859, acc: 60.94%] [G loss: 2.029281]\n",
      "epoch:13 step:12652 [D loss: 0.619730, acc: 67.19%] [G loss: 2.164897]\n",
      "epoch:13 step:12653 [D loss: 0.589800, acc: 70.31%] [G loss: 2.015418]\n",
      "epoch:13 step:12654 [D loss: 0.672547, acc: 59.38%] [G loss: 1.822539]\n",
      "epoch:13 step:12655 [D loss: 0.624669, acc: 66.41%] [G loss: 2.021659]\n",
      "epoch:13 step:12656 [D loss: 0.630150, acc: 66.41%] [G loss: 2.194895]\n",
      "epoch:13 step:12657 [D loss: 0.656646, acc: 58.59%] [G loss: 2.075800]\n",
      "epoch:13 step:12658 [D loss: 0.665003, acc: 60.16%] [G loss: 1.839223]\n",
      "epoch:13 step:12659 [D loss: 0.706267, acc: 52.34%] [G loss: 1.877772]\n",
      "epoch:13 step:12660 [D loss: 0.615635, acc: 63.28%] [G loss: 1.951828]\n",
      "epoch:13 step:12661 [D loss: 0.646739, acc: 67.19%] [G loss: 2.085088]\n",
      "epoch:13 step:12662 [D loss: 0.629630, acc: 64.84%] [G loss: 1.934940]\n",
      "epoch:13 step:12663 [D loss: 0.676333, acc: 57.81%] [G loss: 1.710989]\n",
      "epoch:13 step:12664 [D loss: 0.683115, acc: 56.25%] [G loss: 1.825541]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:13 step:12665 [D loss: 0.588137, acc: 68.75%] [G loss: 2.049093]\n",
      "epoch:13 step:12666 [D loss: 0.699736, acc: 57.81%] [G loss: 1.735751]\n",
      "epoch:13 step:12667 [D loss: 0.577627, acc: 75.00%] [G loss: 2.002677]\n",
      "epoch:13 step:12668 [D loss: 0.638878, acc: 58.59%] [G loss: 1.988443]\n",
      "epoch:13 step:12669 [D loss: 0.628211, acc: 68.75%] [G loss: 2.181395]\n",
      "epoch:13 step:12670 [D loss: 0.596525, acc: 71.88%] [G loss: 1.913841]\n",
      "epoch:13 step:12671 [D loss: 0.618777, acc: 71.09%] [G loss: 2.089396]\n",
      "epoch:13 step:12672 [D loss: 0.589364, acc: 69.53%] [G loss: 1.950968]\n",
      "epoch:13 step:12673 [D loss: 0.637205, acc: 64.06%] [G loss: 1.916934]\n",
      "epoch:13 step:12674 [D loss: 0.571298, acc: 71.88%] [G loss: 2.010511]\n",
      "epoch:13 step:12675 [D loss: 0.576518, acc: 70.31%] [G loss: 2.014610]\n",
      "epoch:13 step:12676 [D loss: 0.657456, acc: 57.03%] [G loss: 2.088930]\n",
      "epoch:13 step:12677 [D loss: 0.655281, acc: 62.50%] [G loss: 1.984561]\n",
      "epoch:13 step:12678 [D loss: 0.630608, acc: 61.72%] [G loss: 2.183096]\n",
      "epoch:13 step:12679 [D loss: 0.647727, acc: 64.06%] [G loss: 2.125108]\n",
      "epoch:13 step:12680 [D loss: 0.602093, acc: 67.19%] [G loss: 2.315257]\n",
      "epoch:13 step:12681 [D loss: 0.736283, acc: 53.91%] [G loss: 1.846953]\n",
      "epoch:13 step:12682 [D loss: 0.644478, acc: 64.06%] [G loss: 1.859455]\n",
      "epoch:13 step:12683 [D loss: 0.661286, acc: 64.84%] [G loss: 1.751457]\n",
      "epoch:13 step:12684 [D loss: 0.617872, acc: 67.97%] [G loss: 1.884356]\n",
      "epoch:13 step:12685 [D loss: 0.630919, acc: 62.50%] [G loss: 2.215742]\n",
      "epoch:13 step:12686 [D loss: 0.596851, acc: 69.53%] [G loss: 2.021190]\n",
      "epoch:13 step:12687 [D loss: 0.690365, acc: 56.25%] [G loss: 1.769525]\n",
      "epoch:13 step:12688 [D loss: 0.660683, acc: 58.59%] [G loss: 1.913227]\n",
      "epoch:13 step:12689 [D loss: 0.594772, acc: 72.66%] [G loss: 2.010538]\n",
      "epoch:13 step:12690 [D loss: 0.620888, acc: 69.53%] [G loss: 1.904208]\n",
      "epoch:13 step:12691 [D loss: 0.657465, acc: 63.28%] [G loss: 1.833902]\n",
      "epoch:13 step:12692 [D loss: 0.691845, acc: 53.12%] [G loss: 1.798842]\n",
      "epoch:13 step:12693 [D loss: 0.621531, acc: 66.41%] [G loss: 1.800839]\n",
      "epoch:13 step:12694 [D loss: 0.619225, acc: 64.06%] [G loss: 2.032405]\n",
      "epoch:13 step:12695 [D loss: 0.657213, acc: 60.16%] [G loss: 2.048757]\n",
      "epoch:13 step:12696 [D loss: 0.638132, acc: 60.94%] [G loss: 1.889772]\n",
      "epoch:13 step:12697 [D loss: 0.563443, acc: 71.09%] [G loss: 2.080934]\n",
      "epoch:13 step:12698 [D loss: 0.697381, acc: 59.38%] [G loss: 2.021874]\n",
      "epoch:13 step:12699 [D loss: 0.666773, acc: 61.72%] [G loss: 1.901139]\n",
      "epoch:13 step:12700 [D loss: 0.663533, acc: 60.16%] [G loss: 1.852627]\n",
      "epoch:13 step:12701 [D loss: 0.649604, acc: 60.16%] [G loss: 1.973191]\n",
      "epoch:13 step:12702 [D loss: 0.612975, acc: 66.41%] [G loss: 1.972275]\n",
      "epoch:13 step:12703 [D loss: 0.620683, acc: 65.62%] [G loss: 1.959535]\n",
      "epoch:13 step:12704 [D loss: 0.671463, acc: 65.62%] [G loss: 1.963657]\n",
      "epoch:13 step:12705 [D loss: 0.652335, acc: 63.28%] [G loss: 2.032194]\n",
      "epoch:13 step:12706 [D loss: 0.631026, acc: 67.19%] [G loss: 2.009465]\n",
      "epoch:13 step:12707 [D loss: 0.615674, acc: 64.84%] [G loss: 1.985843]\n",
      "epoch:13 step:12708 [D loss: 0.631548, acc: 65.62%] [G loss: 1.912915]\n",
      "epoch:13 step:12709 [D loss: 0.688486, acc: 59.38%] [G loss: 1.892985]\n",
      "epoch:13 step:12710 [D loss: 0.691643, acc: 56.25%] [G loss: 1.858797]\n",
      "epoch:13 step:12711 [D loss: 0.641420, acc: 63.28%] [G loss: 1.913871]\n",
      "epoch:13 step:12712 [D loss: 0.654454, acc: 60.16%] [G loss: 1.815466]\n",
      "epoch:13 step:12713 [D loss: 0.659128, acc: 63.28%] [G loss: 2.049781]\n",
      "epoch:13 step:12714 [D loss: 0.656215, acc: 62.50%] [G loss: 1.977650]\n",
      "epoch:13 step:12715 [D loss: 0.552820, acc: 72.66%] [G loss: 2.203529]\n",
      "epoch:13 step:12716 [D loss: 0.641058, acc: 63.28%] [G loss: 1.826184]\n",
      "epoch:13 step:12717 [D loss: 0.617586, acc: 67.19%] [G loss: 2.137985]\n",
      "epoch:13 step:12718 [D loss: 0.691376, acc: 55.47%] [G loss: 1.877938]\n",
      "epoch:13 step:12719 [D loss: 0.648102, acc: 60.94%] [G loss: 1.829359]\n",
      "epoch:13 step:12720 [D loss: 0.634363, acc: 64.84%] [G loss: 1.921461]\n",
      "epoch:13 step:12721 [D loss: 0.657550, acc: 60.94%] [G loss: 1.892387]\n",
      "epoch:13 step:12722 [D loss: 0.688966, acc: 55.47%] [G loss: 1.790717]\n",
      "epoch:13 step:12723 [D loss: 0.633898, acc: 59.38%] [G loss: 1.864696]\n",
      "epoch:13 step:12724 [D loss: 0.671217, acc: 58.59%] [G loss: 1.852880]\n",
      "epoch:13 step:12725 [D loss: 0.660547, acc: 62.50%] [G loss: 1.946448]\n",
      "epoch:13 step:12726 [D loss: 0.597589, acc: 62.50%] [G loss: 2.039911]\n",
      "epoch:13 step:12727 [D loss: 0.664550, acc: 61.72%] [G loss: 1.871991]\n",
      "epoch:13 step:12728 [D loss: 0.645194, acc: 62.50%] [G loss: 2.056318]\n",
      "epoch:13 step:12729 [D loss: 0.589114, acc: 71.09%] [G loss: 1.996805]\n",
      "epoch:13 step:12730 [D loss: 0.631640, acc: 67.19%] [G loss: 2.053740]\n",
      "epoch:13 step:12731 [D loss: 0.600403, acc: 66.41%] [G loss: 2.125307]\n",
      "epoch:13 step:12732 [D loss: 0.580068, acc: 71.88%] [G loss: 2.138608]\n",
      "epoch:13 step:12733 [D loss: 0.585555, acc: 64.84%] [G loss: 2.090733]\n",
      "epoch:13 step:12734 [D loss: 0.690147, acc: 54.69%] [G loss: 1.895121]\n",
      "epoch:13 step:12735 [D loss: 0.587936, acc: 67.97%] [G loss: 2.116146]\n",
      "epoch:13 step:12736 [D loss: 0.627423, acc: 65.62%] [G loss: 2.029708]\n",
      "epoch:13 step:12737 [D loss: 0.610204, acc: 70.31%] [G loss: 2.055817]\n",
      "epoch:13 step:12738 [D loss: 0.557154, acc: 73.44%] [G loss: 2.090799]\n",
      "epoch:13 step:12739 [D loss: 0.636558, acc: 60.94%] [G loss: 2.192944]\n",
      "epoch:13 step:12740 [D loss: 0.700901, acc: 54.69%] [G loss: 1.912909]\n",
      "epoch:13 step:12741 [D loss: 0.663745, acc: 61.72%] [G loss: 1.927130]\n",
      "epoch:13 step:12742 [D loss: 0.657372, acc: 63.28%] [G loss: 1.917171]\n",
      "epoch:13 step:12743 [D loss: 0.627816, acc: 60.16%] [G loss: 2.060686]\n",
      "epoch:13 step:12744 [D loss: 0.684384, acc: 63.28%] [G loss: 1.851085]\n",
      "epoch:13 step:12745 [D loss: 0.652209, acc: 63.28%] [G loss: 2.110169]\n",
      "epoch:13 step:12746 [D loss: 0.649801, acc: 62.50%] [G loss: 1.966522]\n",
      "epoch:13 step:12747 [D loss: 0.692825, acc: 60.16%] [G loss: 1.833203]\n",
      "epoch:13 step:12748 [D loss: 0.614506, acc: 65.62%] [G loss: 2.010884]\n",
      "epoch:13 step:12749 [D loss: 0.662058, acc: 60.94%] [G loss: 1.762099]\n",
      "epoch:13 step:12750 [D loss: 0.647938, acc: 61.72%] [G loss: 1.947510]\n",
      "epoch:13 step:12751 [D loss: 0.615748, acc: 65.62%] [G loss: 1.946245]\n",
      "epoch:13 step:12752 [D loss: 0.652378, acc: 58.59%] [G loss: 1.905429]\n",
      "epoch:13 step:12753 [D loss: 0.663849, acc: 59.38%] [G loss: 1.862360]\n",
      "epoch:13 step:12754 [D loss: 0.650347, acc: 64.06%] [G loss: 1.877480]\n",
      "epoch:13 step:12755 [D loss: 0.569850, acc: 74.22%] [G loss: 1.985140]\n",
      "epoch:13 step:12756 [D loss: 0.640333, acc: 61.72%] [G loss: 2.036734]\n",
      "epoch:13 step:12757 [D loss: 0.678382, acc: 55.47%] [G loss: 1.835755]\n",
      "epoch:13 step:12758 [D loss: 0.666615, acc: 61.72%] [G loss: 1.843546]\n",
      "epoch:13 step:12759 [D loss: 0.625644, acc: 64.84%] [G loss: 1.982084]\n",
      "epoch:13 step:12760 [D loss: 0.658611, acc: 60.94%] [G loss: 1.903864]\n",
      "epoch:13 step:12761 [D loss: 0.648005, acc: 59.38%] [G loss: 1.970236]\n",
      "epoch:13 step:12762 [D loss: 0.603173, acc: 67.19%] [G loss: 1.905654]\n",
      "epoch:13 step:12763 [D loss: 0.618271, acc: 64.06%] [G loss: 1.895149]\n",
      "epoch:13 step:12764 [D loss: 0.667074, acc: 57.03%] [G loss: 1.814366]\n",
      "epoch:13 step:12765 [D loss: 0.621396, acc: 68.75%] [G loss: 1.819591]\n",
      "epoch:13 step:12766 [D loss: 0.623374, acc: 62.50%] [G loss: 1.932715]\n",
      "epoch:13 step:12767 [D loss: 0.641613, acc: 61.72%] [G loss: 1.972609]\n",
      "epoch:13 step:12768 [D loss: 0.642426, acc: 66.41%] [G loss: 1.883846]\n",
      "epoch:13 step:12769 [D loss: 0.643094, acc: 64.06%] [G loss: 2.013167]\n",
      "epoch:13 step:12770 [D loss: 0.683712, acc: 56.25%] [G loss: 2.046754]\n",
      "epoch:13 step:12771 [D loss: 0.691211, acc: 56.25%] [G loss: 2.051557]\n",
      "epoch:13 step:12772 [D loss: 0.645249, acc: 58.59%] [G loss: 1.835149]\n",
      "epoch:13 step:12773 [D loss: 0.622219, acc: 62.50%] [G loss: 1.831277]\n",
      "epoch:13 step:12774 [D loss: 0.669146, acc: 58.59%] [G loss: 2.077921]\n",
      "epoch:13 step:12775 [D loss: 0.645918, acc: 62.50%] [G loss: 1.958549]\n",
      "epoch:13 step:12776 [D loss: 0.638220, acc: 64.06%] [G loss: 2.049677]\n",
      "epoch:13 step:12777 [D loss: 0.715559, acc: 51.56%] [G loss: 1.798038]\n",
      "epoch:13 step:12778 [D loss: 0.655849, acc: 60.16%] [G loss: 1.933177]\n",
      "epoch:13 step:12779 [D loss: 0.606610, acc: 71.88%] [G loss: 1.921790]\n",
      "epoch:13 step:12780 [D loss: 0.685867, acc: 58.59%] [G loss: 1.848466]\n",
      "epoch:13 step:12781 [D loss: 0.703830, acc: 57.03%] [G loss: 1.891722]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:13 step:12782 [D loss: 0.647940, acc: 66.41%] [G loss: 1.848969]\n",
      "epoch:13 step:12783 [D loss: 0.612013, acc: 66.41%] [G loss: 1.783900]\n",
      "epoch:13 step:12784 [D loss: 0.642008, acc: 60.94%] [G loss: 2.008873]\n",
      "epoch:13 step:12785 [D loss: 0.649196, acc: 60.16%] [G loss: 1.918188]\n",
      "epoch:13 step:12786 [D loss: 0.597647, acc: 64.84%] [G loss: 2.057150]\n",
      "epoch:13 step:12787 [D loss: 0.662480, acc: 66.41%] [G loss: 1.991760]\n",
      "epoch:13 step:12788 [D loss: 0.647551, acc: 61.72%] [G loss: 1.876491]\n",
      "epoch:13 step:12789 [D loss: 0.583970, acc: 71.09%] [G loss: 2.030821]\n",
      "epoch:13 step:12790 [D loss: 0.645411, acc: 61.72%] [G loss: 1.961200]\n",
      "epoch:13 step:12791 [D loss: 0.641644, acc: 62.50%] [G loss: 1.860088]\n",
      "epoch:13 step:12792 [D loss: 0.613425, acc: 60.16%] [G loss: 1.839313]\n",
      "epoch:13 step:12793 [D loss: 0.675584, acc: 58.59%] [G loss: 2.050842]\n",
      "epoch:13 step:12794 [D loss: 0.643271, acc: 64.06%] [G loss: 1.958802]\n",
      "epoch:13 step:12795 [D loss: 0.664492, acc: 58.59%] [G loss: 1.743058]\n",
      "epoch:13 step:12796 [D loss: 0.692113, acc: 53.12%] [G loss: 1.865521]\n",
      "epoch:13 step:12797 [D loss: 0.659328, acc: 64.84%] [G loss: 1.927912]\n",
      "epoch:13 step:12798 [D loss: 0.645083, acc: 54.69%] [G loss: 1.847897]\n",
      "epoch:13 step:12799 [D loss: 0.658436, acc: 60.94%] [G loss: 1.862423]\n",
      "epoch:13 step:12800 [D loss: 0.642326, acc: 62.50%] [G loss: 1.877335]\n",
      "##############\n",
      "[2.52563734 1.33031952 6.28540902 4.93398766 3.60932314 5.75449791\n",
      " 4.31884817 4.71112489 4.57955035 3.50642365]\n",
      "##########\n",
      "epoch:13 step:12801 [D loss: 0.675079, acc: 58.59%] [G loss: 1.789195]\n",
      "epoch:13 step:12802 [D loss: 0.640005, acc: 61.72%] [G loss: 1.983257]\n",
      "epoch:13 step:12803 [D loss: 0.652500, acc: 60.16%] [G loss: 1.866606]\n",
      "epoch:13 step:12804 [D loss: 0.644577, acc: 65.62%] [G loss: 1.976484]\n",
      "epoch:13 step:12805 [D loss: 0.624291, acc: 71.09%] [G loss: 1.991335]\n",
      "epoch:13 step:12806 [D loss: 0.697714, acc: 51.56%] [G loss: 1.847636]\n",
      "epoch:13 step:12807 [D loss: 0.650443, acc: 61.72%] [G loss: 1.875680]\n",
      "epoch:13 step:12808 [D loss: 0.688636, acc: 58.59%] [G loss: 1.831550]\n",
      "epoch:13 step:12809 [D loss: 0.653938, acc: 60.94%] [G loss: 1.884178]\n",
      "epoch:13 step:12810 [D loss: 0.596235, acc: 69.53%] [G loss: 2.003304]\n",
      "epoch:13 step:12811 [D loss: 0.648487, acc: 64.84%] [G loss: 1.947420]\n",
      "epoch:13 step:12812 [D loss: 0.616459, acc: 65.62%] [G loss: 2.154149]\n",
      "epoch:13 step:12813 [D loss: 0.603500, acc: 69.53%] [G loss: 1.961126]\n",
      "epoch:13 step:12814 [D loss: 0.655379, acc: 62.50%] [G loss: 1.891442]\n",
      "epoch:13 step:12815 [D loss: 0.632352, acc: 60.94%] [G loss: 2.078954]\n",
      "epoch:13 step:12816 [D loss: 0.573787, acc: 66.41%] [G loss: 1.987915]\n",
      "epoch:13 step:12817 [D loss: 0.634614, acc: 62.50%] [G loss: 1.885535]\n",
      "epoch:13 step:12818 [D loss: 0.629628, acc: 64.06%] [G loss: 1.927848]\n",
      "epoch:13 step:12819 [D loss: 0.586119, acc: 70.31%] [G loss: 1.891717]\n",
      "epoch:13 step:12820 [D loss: 0.594350, acc: 68.75%] [G loss: 2.041706]\n",
      "epoch:13 step:12821 [D loss: 0.640060, acc: 63.28%] [G loss: 1.999922]\n",
      "epoch:13 step:12822 [D loss: 0.601891, acc: 66.41%] [G loss: 2.023741]\n",
      "epoch:13 step:12823 [D loss: 0.595398, acc: 64.84%] [G loss: 2.046261]\n",
      "epoch:13 step:12824 [D loss: 0.647023, acc: 59.38%] [G loss: 1.995837]\n",
      "epoch:13 step:12825 [D loss: 0.648091, acc: 61.72%] [G loss: 1.867579]\n",
      "epoch:13 step:12826 [D loss: 0.596649, acc: 71.09%] [G loss: 2.086273]\n",
      "epoch:13 step:12827 [D loss: 0.637177, acc: 64.84%] [G loss: 2.073411]\n",
      "epoch:13 step:12828 [D loss: 0.610854, acc: 61.72%] [G loss: 2.093590]\n",
      "epoch:13 step:12829 [D loss: 0.576238, acc: 64.06%] [G loss: 2.218953]\n",
      "epoch:13 step:12830 [D loss: 0.591349, acc: 68.75%] [G loss: 2.244677]\n",
      "epoch:13 step:12831 [D loss: 0.604903, acc: 65.62%] [G loss: 2.127730]\n",
      "epoch:13 step:12832 [D loss: 0.598071, acc: 67.97%] [G loss: 2.095035]\n",
      "epoch:13 step:12833 [D loss: 0.636770, acc: 62.50%] [G loss: 1.997307]\n",
      "epoch:13 step:12834 [D loss: 0.685793, acc: 57.03%] [G loss: 2.139488]\n",
      "epoch:13 step:12835 [D loss: 0.564953, acc: 72.66%] [G loss: 2.105907]\n",
      "epoch:13 step:12836 [D loss: 0.661796, acc: 60.16%] [G loss: 1.923717]\n",
      "epoch:13 step:12837 [D loss: 0.647830, acc: 64.06%] [G loss: 2.046040]\n",
      "epoch:13 step:12838 [D loss: 0.648148, acc: 56.25%] [G loss: 1.802169]\n",
      "epoch:13 step:12839 [D loss: 0.698484, acc: 55.47%] [G loss: 1.849521]\n",
      "epoch:13 step:12840 [D loss: 0.620942, acc: 64.06%] [G loss: 1.955277]\n",
      "epoch:13 step:12841 [D loss: 0.676100, acc: 61.72%] [G loss: 1.964926]\n",
      "epoch:13 step:12842 [D loss: 0.641140, acc: 60.94%] [G loss: 1.915459]\n",
      "epoch:13 step:12843 [D loss: 0.664149, acc: 60.94%] [G loss: 2.038900]\n",
      "epoch:13 step:12844 [D loss: 0.624878, acc: 64.06%] [G loss: 1.899204]\n",
      "epoch:13 step:12845 [D loss: 0.639936, acc: 64.06%] [G loss: 1.975751]\n",
      "epoch:13 step:12846 [D loss: 0.602164, acc: 60.16%] [G loss: 2.043003]\n",
      "epoch:13 step:12847 [D loss: 0.618606, acc: 64.06%] [G loss: 1.930722]\n",
      "epoch:13 step:12848 [D loss: 0.690114, acc: 57.03%] [G loss: 1.798923]\n",
      "epoch:13 step:12849 [D loss: 0.654511, acc: 59.38%] [G loss: 1.976417]\n",
      "epoch:13 step:12850 [D loss: 0.625822, acc: 61.72%] [G loss: 1.813203]\n",
      "epoch:13 step:12851 [D loss: 0.637084, acc: 65.62%] [G loss: 1.888003]\n",
      "epoch:13 step:12852 [D loss: 0.662328, acc: 60.16%] [G loss: 1.953583]\n",
      "epoch:13 step:12853 [D loss: 0.642802, acc: 62.50%] [G loss: 1.944677]\n",
      "epoch:13 step:12854 [D loss: 0.625346, acc: 64.06%] [G loss: 1.883337]\n",
      "epoch:13 step:12855 [D loss: 0.693957, acc: 57.81%] [G loss: 1.969022]\n",
      "epoch:13 step:12856 [D loss: 0.677082, acc: 58.59%] [G loss: 1.770024]\n",
      "epoch:13 step:12857 [D loss: 0.672535, acc: 60.94%] [G loss: 1.968622]\n",
      "epoch:13 step:12858 [D loss: 0.618633, acc: 68.75%] [G loss: 1.998732]\n",
      "epoch:13 step:12859 [D loss: 0.641283, acc: 60.94%] [G loss: 2.014802]\n",
      "epoch:13 step:12860 [D loss: 0.638054, acc: 61.72%] [G loss: 1.867305]\n",
      "epoch:13 step:12861 [D loss: 0.676105, acc: 62.50%] [G loss: 1.781076]\n",
      "epoch:13 step:12862 [D loss: 0.584293, acc: 71.88%] [G loss: 2.068018]\n",
      "epoch:13 step:12863 [D loss: 0.621055, acc: 64.06%] [G loss: 1.872244]\n",
      "epoch:13 step:12864 [D loss: 0.632609, acc: 60.94%] [G loss: 1.912286]\n",
      "epoch:13 step:12865 [D loss: 0.657321, acc: 59.38%] [G loss: 1.850281]\n",
      "epoch:13 step:12866 [D loss: 0.661198, acc: 61.72%] [G loss: 1.865120]\n",
      "epoch:13 step:12867 [D loss: 0.651340, acc: 60.94%] [G loss: 1.945143]\n",
      "epoch:13 step:12868 [D loss: 0.642696, acc: 64.84%] [G loss: 1.983316]\n",
      "epoch:13 step:12869 [D loss: 0.648454, acc: 63.28%] [G loss: 2.010911]\n",
      "epoch:13 step:12870 [D loss: 0.645011, acc: 56.25%] [G loss: 2.073029]\n",
      "epoch:13 step:12871 [D loss: 0.641815, acc: 63.28%] [G loss: 1.923935]\n",
      "epoch:13 step:12872 [D loss: 0.585150, acc: 67.19%] [G loss: 2.136725]\n",
      "epoch:13 step:12873 [D loss: 0.616530, acc: 65.62%] [G loss: 2.097455]\n",
      "epoch:13 step:12874 [D loss: 0.627123, acc: 62.50%] [G loss: 2.095932]\n",
      "epoch:13 step:12875 [D loss: 0.603077, acc: 65.62%] [G loss: 2.042402]\n",
      "epoch:13 step:12876 [D loss: 0.628996, acc: 66.41%] [G loss: 2.234672]\n",
      "epoch:13 step:12877 [D loss: 0.693008, acc: 56.25%] [G loss: 1.785749]\n",
      "epoch:13 step:12878 [D loss: 0.609196, acc: 63.28%] [G loss: 2.024021]\n",
      "epoch:13 step:12879 [D loss: 0.667682, acc: 61.72%] [G loss: 1.938641]\n",
      "epoch:13 step:12880 [D loss: 0.654966, acc: 60.16%] [G loss: 1.982802]\n",
      "epoch:13 step:12881 [D loss: 0.636495, acc: 58.59%] [G loss: 2.028873]\n",
      "epoch:13 step:12882 [D loss: 0.589910, acc: 68.75%] [G loss: 2.157773]\n",
      "epoch:13 step:12883 [D loss: 0.672772, acc: 53.91%] [G loss: 2.016762]\n",
      "epoch:13 step:12884 [D loss: 0.649609, acc: 64.06%] [G loss: 1.897663]\n",
      "epoch:13 step:12885 [D loss: 0.647550, acc: 64.84%] [G loss: 1.880466]\n",
      "epoch:13 step:12886 [D loss: 0.639356, acc: 66.41%] [G loss: 1.870198]\n",
      "epoch:13 step:12887 [D loss: 0.604059, acc: 65.62%] [G loss: 1.980550]\n",
      "epoch:13 step:12888 [D loss: 0.640496, acc: 66.41%] [G loss: 2.046112]\n",
      "epoch:13 step:12889 [D loss: 0.625122, acc: 61.72%] [G loss: 2.114153]\n",
      "epoch:13 step:12890 [D loss: 0.578695, acc: 70.31%] [G loss: 2.214484]\n",
      "epoch:13 step:12891 [D loss: 0.740300, acc: 48.44%] [G loss: 1.789330]\n",
      "epoch:13 step:12892 [D loss: 0.647853, acc: 63.28%] [G loss: 2.078407]\n",
      "epoch:13 step:12893 [D loss: 0.588604, acc: 70.31%] [G loss: 2.060447]\n",
      "epoch:13 step:12894 [D loss: 0.642375, acc: 62.50%] [G loss: 1.995054]\n",
      "epoch:13 step:12895 [D loss: 0.645257, acc: 64.06%] [G loss: 2.023704]\n",
      "epoch:13 step:12896 [D loss: 0.608567, acc: 67.19%] [G loss: 1.961542]\n",
      "epoch:13 step:12897 [D loss: 0.651668, acc: 62.50%] [G loss: 1.793273]\n",
      "epoch:13 step:12898 [D loss: 0.654897, acc: 61.72%] [G loss: 1.853656]\n",
      "epoch:13 step:12899 [D loss: 0.717438, acc: 52.34%] [G loss: 1.965370]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:13 step:12900 [D loss: 0.657654, acc: 59.38%] [G loss: 2.086703]\n",
      "epoch:13 step:12901 [D loss: 0.646658, acc: 61.72%] [G loss: 1.879489]\n",
      "epoch:13 step:12902 [D loss: 0.621702, acc: 63.28%] [G loss: 1.971162]\n",
      "epoch:13 step:12903 [D loss: 0.675824, acc: 59.38%] [G loss: 1.863596]\n",
      "epoch:13 step:12904 [D loss: 0.640636, acc: 61.72%] [G loss: 1.964051]\n",
      "epoch:13 step:12905 [D loss: 0.647796, acc: 61.72%] [G loss: 2.072388]\n",
      "epoch:13 step:12906 [D loss: 0.679202, acc: 60.16%] [G loss: 1.894367]\n",
      "epoch:13 step:12907 [D loss: 0.662203, acc: 59.38%] [G loss: 1.889296]\n",
      "epoch:13 step:12908 [D loss: 0.658283, acc: 63.28%] [G loss: 1.851987]\n",
      "epoch:13 step:12909 [D loss: 0.619175, acc: 69.53%] [G loss: 2.009337]\n",
      "epoch:13 step:12910 [D loss: 0.654087, acc: 60.16%] [G loss: 1.774655]\n",
      "epoch:13 step:12911 [D loss: 0.659281, acc: 58.59%] [G loss: 1.932553]\n",
      "epoch:13 step:12912 [D loss: 0.710321, acc: 53.12%] [G loss: 1.875326]\n",
      "epoch:13 step:12913 [D loss: 0.620513, acc: 62.50%] [G loss: 2.035966]\n",
      "epoch:13 step:12914 [D loss: 0.663034, acc: 60.16%] [G loss: 1.939492]\n",
      "epoch:13 step:12915 [D loss: 0.678571, acc: 62.50%] [G loss: 1.841897]\n",
      "epoch:13 step:12916 [D loss: 0.652727, acc: 59.38%] [G loss: 2.051418]\n",
      "epoch:13 step:12917 [D loss: 0.663717, acc: 59.38%] [G loss: 1.780712]\n",
      "epoch:13 step:12918 [D loss: 0.662813, acc: 61.72%] [G loss: 1.919098]\n",
      "epoch:13 step:12919 [D loss: 0.653865, acc: 59.38%] [G loss: 1.879212]\n",
      "epoch:13 step:12920 [D loss: 0.689253, acc: 60.16%] [G loss: 1.732247]\n",
      "epoch:13 step:12921 [D loss: 0.640377, acc: 61.72%] [G loss: 1.837250]\n",
      "epoch:13 step:12922 [D loss: 0.655661, acc: 61.72%] [G loss: 1.814391]\n",
      "epoch:13 step:12923 [D loss: 0.671213, acc: 57.81%] [G loss: 1.844795]\n",
      "epoch:13 step:12924 [D loss: 0.631864, acc: 65.62%] [G loss: 2.025425]\n",
      "epoch:13 step:12925 [D loss: 0.687114, acc: 55.47%] [G loss: 1.803850]\n",
      "epoch:13 step:12926 [D loss: 0.688158, acc: 54.69%] [G loss: 1.887201]\n",
      "epoch:13 step:12927 [D loss: 0.578220, acc: 69.53%] [G loss: 1.885141]\n",
      "epoch:13 step:12928 [D loss: 0.650424, acc: 63.28%] [G loss: 2.174449]\n",
      "epoch:13 step:12929 [D loss: 0.622666, acc: 64.06%] [G loss: 1.850280]\n",
      "epoch:13 step:12930 [D loss: 0.663704, acc: 60.16%] [G loss: 1.686445]\n",
      "epoch:13 step:12931 [D loss: 0.647935, acc: 65.62%] [G loss: 1.864442]\n",
      "epoch:13 step:12932 [D loss: 0.638927, acc: 68.75%] [G loss: 1.849302]\n",
      "epoch:13 step:12933 [D loss: 0.627779, acc: 67.19%] [G loss: 1.896302]\n",
      "epoch:13 step:12934 [D loss: 0.639455, acc: 63.28%] [G loss: 1.841828]\n",
      "epoch:13 step:12935 [D loss: 0.641254, acc: 58.59%] [G loss: 2.004697]\n",
      "epoch:13 step:12936 [D loss: 0.564550, acc: 72.66%] [G loss: 1.967002]\n",
      "epoch:13 step:12937 [D loss: 0.633971, acc: 65.62%] [G loss: 1.993183]\n",
      "epoch:13 step:12938 [D loss: 0.634382, acc: 62.50%] [G loss: 1.968162]\n",
      "epoch:13 step:12939 [D loss: 0.693322, acc: 58.59%] [G loss: 1.779661]\n",
      "epoch:13 step:12940 [D loss: 0.657066, acc: 61.72%] [G loss: 1.895557]\n",
      "epoch:13 step:12941 [D loss: 0.633238, acc: 61.72%] [G loss: 1.916647]\n",
      "epoch:13 step:12942 [D loss: 0.606603, acc: 66.41%] [G loss: 1.884983]\n",
      "epoch:13 step:12943 [D loss: 0.681061, acc: 53.12%] [G loss: 1.810692]\n",
      "epoch:13 step:12944 [D loss: 0.631421, acc: 58.59%] [G loss: 1.910946]\n",
      "epoch:13 step:12945 [D loss: 0.655600, acc: 58.59%] [G loss: 1.813457]\n",
      "epoch:13 step:12946 [D loss: 0.793766, acc: 51.56%] [G loss: 1.794130]\n",
      "epoch:13 step:12947 [D loss: 0.686458, acc: 63.28%] [G loss: 1.856131]\n",
      "epoch:13 step:12948 [D loss: 0.653712, acc: 64.84%] [G loss: 1.899025]\n",
      "epoch:13 step:12949 [D loss: 0.706697, acc: 55.47%] [G loss: 1.787734]\n",
      "epoch:13 step:12950 [D loss: 0.633221, acc: 64.84%] [G loss: 1.890849]\n",
      "epoch:13 step:12951 [D loss: 0.641563, acc: 61.72%] [G loss: 1.841562]\n",
      "epoch:13 step:12952 [D loss: 0.600377, acc: 67.19%] [G loss: 2.039636]\n",
      "epoch:13 step:12953 [D loss: 0.640574, acc: 60.94%] [G loss: 1.881628]\n",
      "epoch:13 step:12954 [D loss: 0.663361, acc: 64.84%] [G loss: 1.900013]\n",
      "epoch:13 step:12955 [D loss: 0.659036, acc: 61.72%] [G loss: 1.952659]\n",
      "epoch:13 step:12956 [D loss: 0.626141, acc: 66.41%] [G loss: 2.060946]\n",
      "epoch:13 step:12957 [D loss: 0.640347, acc: 61.72%] [G loss: 1.991455]\n",
      "epoch:13 step:12958 [D loss: 0.672550, acc: 60.16%] [G loss: 2.000048]\n",
      "epoch:13 step:12959 [D loss: 0.657908, acc: 64.84%] [G loss: 1.959258]\n",
      "epoch:13 step:12960 [D loss: 0.622120, acc: 62.50%] [G loss: 1.923652]\n",
      "epoch:13 step:12961 [D loss: 0.592131, acc: 66.41%] [G loss: 2.010696]\n",
      "epoch:13 step:12962 [D loss: 0.566631, acc: 75.00%] [G loss: 2.178608]\n",
      "epoch:13 step:12963 [D loss: 0.564461, acc: 75.00%] [G loss: 2.247849]\n",
      "epoch:13 step:12964 [D loss: 0.638449, acc: 67.97%] [G loss: 2.018201]\n",
      "epoch:13 step:12965 [D loss: 0.710411, acc: 57.81%] [G loss: 1.771693]\n",
      "epoch:13 step:12966 [D loss: 0.633566, acc: 63.28%] [G loss: 1.903743]\n",
      "epoch:13 step:12967 [D loss: 0.615604, acc: 65.62%] [G loss: 2.056312]\n",
      "epoch:13 step:12968 [D loss: 0.618800, acc: 66.41%] [G loss: 2.103562]\n",
      "epoch:13 step:12969 [D loss: 0.669374, acc: 56.25%] [G loss: 1.800947]\n",
      "epoch:13 step:12970 [D loss: 0.689722, acc: 60.16%] [G loss: 1.996897]\n",
      "epoch:13 step:12971 [D loss: 0.556823, acc: 74.22%] [G loss: 2.090992]\n",
      "epoch:13 step:12972 [D loss: 0.587499, acc: 71.09%] [G loss: 2.031542]\n",
      "epoch:13 step:12973 [D loss: 0.593732, acc: 67.97%] [G loss: 2.053023]\n",
      "epoch:13 step:12974 [D loss: 0.597072, acc: 63.28%] [G loss: 1.998577]\n",
      "epoch:13 step:12975 [D loss: 0.743348, acc: 52.34%] [G loss: 1.815039]\n",
      "epoch:13 step:12976 [D loss: 0.715831, acc: 54.69%] [G loss: 1.754766]\n",
      "epoch:13 step:12977 [D loss: 0.662554, acc: 57.81%] [G loss: 1.881558]\n",
      "epoch:13 step:12978 [D loss: 0.637875, acc: 60.94%] [G loss: 1.927199]\n",
      "epoch:13 step:12979 [D loss: 0.596809, acc: 64.84%] [G loss: 1.937335]\n",
      "epoch:13 step:12980 [D loss: 0.706809, acc: 58.59%] [G loss: 1.835919]\n",
      "epoch:13 step:12981 [D loss: 0.740377, acc: 52.34%] [G loss: 1.954832]\n",
      "epoch:13 step:12982 [D loss: 0.667933, acc: 54.69%] [G loss: 1.791772]\n",
      "epoch:13 step:12983 [D loss: 0.628943, acc: 63.28%] [G loss: 1.880322]\n",
      "epoch:13 step:12984 [D loss: 0.633454, acc: 67.19%] [G loss: 1.947331]\n",
      "epoch:13 step:12985 [D loss: 0.627449, acc: 64.84%] [G loss: 1.936641]\n",
      "epoch:13 step:12986 [D loss: 0.606090, acc: 65.62%] [G loss: 2.019679]\n",
      "epoch:13 step:12987 [D loss: 0.698516, acc: 57.03%] [G loss: 1.855464]\n",
      "epoch:13 step:12988 [D loss: 0.597588, acc: 67.97%] [G loss: 1.991320]\n",
      "epoch:13 step:12989 [D loss: 0.620153, acc: 67.97%] [G loss: 1.883044]\n",
      "epoch:13 step:12990 [D loss: 0.618670, acc: 61.72%] [G loss: 1.953995]\n",
      "epoch:13 step:12991 [D loss: 0.658904, acc: 62.50%] [G loss: 1.928594]\n",
      "epoch:13 step:12992 [D loss: 0.612744, acc: 68.75%] [G loss: 1.923682]\n",
      "epoch:13 step:12993 [D loss: 0.671139, acc: 57.03%] [G loss: 1.868353]\n",
      "epoch:13 step:12994 [D loss: 0.627010, acc: 64.06%] [G loss: 1.964852]\n",
      "epoch:13 step:12995 [D loss: 0.668837, acc: 60.94%] [G loss: 1.763571]\n",
      "epoch:13 step:12996 [D loss: 0.594059, acc: 71.09%] [G loss: 2.008105]\n",
      "epoch:13 step:12997 [D loss: 0.552601, acc: 75.00%] [G loss: 1.943463]\n",
      "epoch:13 step:12998 [D loss: 0.654047, acc: 61.72%] [G loss: 1.950253]\n",
      "epoch:13 step:12999 [D loss: 0.703119, acc: 53.91%] [G loss: 1.824548]\n",
      "epoch:13 step:13000 [D loss: 0.642559, acc: 57.81%] [G loss: 1.861425]\n",
      "##############\n",
      "[2.45283801 1.3774339  6.35795689 4.83733792 3.71297103 5.59551892\n",
      " 4.38101113 4.67522284 4.73181539 3.52759962]\n",
      "##########\n",
      "epoch:13 step:13001 [D loss: 0.697378, acc: 53.12%] [G loss: 1.756755]\n",
      "epoch:13 step:13002 [D loss: 0.664151, acc: 57.03%] [G loss: 1.829870]\n",
      "epoch:13 step:13003 [D loss: 0.643659, acc: 55.47%] [G loss: 2.094090]\n",
      "epoch:13 step:13004 [D loss: 0.655656, acc: 63.28%] [G loss: 1.908817]\n",
      "epoch:13 step:13005 [D loss: 0.680318, acc: 60.94%] [G loss: 1.932342]\n",
      "epoch:13 step:13006 [D loss: 0.611800, acc: 75.00%] [G loss: 2.113793]\n",
      "epoch:13 step:13007 [D loss: 0.640050, acc: 60.94%] [G loss: 2.006138]\n",
      "epoch:13 step:13008 [D loss: 0.663456, acc: 64.06%] [G loss: 1.683455]\n",
      "epoch:13 step:13009 [D loss: 0.678606, acc: 54.69%] [G loss: 1.710267]\n",
      "epoch:13 step:13010 [D loss: 0.649465, acc: 57.81%] [G loss: 1.829986]\n",
      "epoch:13 step:13011 [D loss: 0.626459, acc: 64.84%] [G loss: 1.881700]\n",
      "epoch:13 step:13012 [D loss: 0.630518, acc: 64.06%] [G loss: 2.010535]\n",
      "epoch:13 step:13013 [D loss: 0.611372, acc: 66.41%] [G loss: 1.930621]\n",
      "epoch:13 step:13014 [D loss: 0.669915, acc: 57.81%] [G loss: 1.849134]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:13 step:13015 [D loss: 0.573032, acc: 71.88%] [G loss: 1.908156]\n",
      "epoch:13 step:13016 [D loss: 0.602367, acc: 70.31%] [G loss: 2.036903]\n",
      "epoch:13 step:13017 [D loss: 0.644932, acc: 57.03%] [G loss: 1.972071]\n",
      "epoch:13 step:13018 [D loss: 0.615421, acc: 71.88%] [G loss: 2.043107]\n",
      "epoch:13 step:13019 [D loss: 0.585261, acc: 67.97%] [G loss: 1.853709]\n",
      "epoch:13 step:13020 [D loss: 0.652973, acc: 64.84%] [G loss: 1.910134]\n",
      "epoch:13 step:13021 [D loss: 0.702916, acc: 53.12%] [G loss: 1.861043]\n",
      "epoch:13 step:13022 [D loss: 0.625579, acc: 65.62%] [G loss: 2.044207]\n",
      "epoch:13 step:13023 [D loss: 0.646332, acc: 62.50%] [G loss: 1.932441]\n",
      "epoch:13 step:13024 [D loss: 0.684377, acc: 64.84%] [G loss: 1.898180]\n",
      "epoch:13 step:13025 [D loss: 0.628990, acc: 64.84%] [G loss: 2.085126]\n",
      "epoch:13 step:13026 [D loss: 0.669201, acc: 54.69%] [G loss: 1.947706]\n",
      "epoch:13 step:13027 [D loss: 0.653324, acc: 64.84%] [G loss: 1.910900]\n",
      "epoch:13 step:13028 [D loss: 0.661620, acc: 56.25%] [G loss: 1.816231]\n",
      "epoch:13 step:13029 [D loss: 0.668028, acc: 62.50%] [G loss: 1.860944]\n",
      "epoch:13 step:13030 [D loss: 0.629054, acc: 63.28%] [G loss: 2.104460]\n",
      "epoch:13 step:13031 [D loss: 0.702694, acc: 52.34%] [G loss: 1.902947]\n",
      "epoch:13 step:13032 [D loss: 0.617684, acc: 68.75%] [G loss: 1.954055]\n",
      "epoch:13 step:13033 [D loss: 0.651308, acc: 64.06%] [G loss: 2.081813]\n",
      "epoch:13 step:13034 [D loss: 0.645453, acc: 60.16%] [G loss: 1.879512]\n",
      "epoch:13 step:13035 [D loss: 0.628169, acc: 63.28%] [G loss: 1.905643]\n",
      "epoch:13 step:13036 [D loss: 0.614613, acc: 69.53%] [G loss: 1.904007]\n",
      "epoch:13 step:13037 [D loss: 0.638280, acc: 65.62%] [G loss: 1.790065]\n",
      "epoch:13 step:13038 [D loss: 0.634623, acc: 62.50%] [G loss: 1.964734]\n",
      "epoch:13 step:13039 [D loss: 0.688824, acc: 60.94%] [G loss: 1.792236]\n",
      "epoch:13 step:13040 [D loss: 0.659562, acc: 60.94%] [G loss: 1.798489]\n",
      "epoch:13 step:13041 [D loss: 0.646393, acc: 66.41%] [G loss: 2.063110]\n",
      "epoch:13 step:13042 [D loss: 0.679908, acc: 59.38%] [G loss: 1.881298]\n",
      "epoch:13 step:13043 [D loss: 0.638915, acc: 59.38%] [G loss: 1.791074]\n",
      "epoch:13 step:13044 [D loss: 0.641392, acc: 59.38%] [G loss: 1.874350]\n",
      "epoch:13 step:13045 [D loss: 0.612581, acc: 71.09%] [G loss: 1.832404]\n",
      "epoch:13 step:13046 [D loss: 0.587901, acc: 67.19%] [G loss: 1.914104]\n",
      "epoch:13 step:13047 [D loss: 0.698938, acc: 57.03%] [G loss: 1.849687]\n",
      "epoch:13 step:13048 [D loss: 0.674994, acc: 63.28%] [G loss: 1.873459]\n",
      "epoch:13 step:13049 [D loss: 0.675634, acc: 60.94%] [G loss: 1.891576]\n",
      "epoch:13 step:13050 [D loss: 0.666788, acc: 57.03%] [G loss: 1.904633]\n",
      "epoch:13 step:13051 [D loss: 0.654255, acc: 64.84%] [G loss: 1.890762]\n",
      "epoch:13 step:13052 [D loss: 0.627390, acc: 64.84%] [G loss: 1.834185]\n",
      "epoch:13 step:13053 [D loss: 0.623776, acc: 61.72%] [G loss: 1.943918]\n",
      "epoch:13 step:13054 [D loss: 0.695390, acc: 57.81%] [G loss: 1.748660]\n",
      "epoch:13 step:13055 [D loss: 0.631540, acc: 64.84%] [G loss: 1.962527]\n",
      "epoch:13 step:13056 [D loss: 0.555200, acc: 74.22%] [G loss: 1.979760]\n",
      "epoch:13 step:13057 [D loss: 0.640181, acc: 61.72%] [G loss: 1.947256]\n",
      "epoch:13 step:13058 [D loss: 0.616268, acc: 64.06%] [G loss: 2.065043]\n",
      "epoch:13 step:13059 [D loss: 0.646818, acc: 61.72%] [G loss: 1.896284]\n",
      "epoch:13 step:13060 [D loss: 0.664583, acc: 57.81%] [G loss: 1.780224]\n",
      "epoch:13 step:13061 [D loss: 0.623275, acc: 64.84%] [G loss: 1.923937]\n",
      "epoch:13 step:13062 [D loss: 0.639757, acc: 62.50%] [G loss: 1.911927]\n",
      "epoch:13 step:13063 [D loss: 0.659754, acc: 62.50%] [G loss: 1.981116]\n",
      "epoch:13 step:13064 [D loss: 0.610673, acc: 61.72%] [G loss: 2.098261]\n",
      "epoch:13 step:13065 [D loss: 0.600601, acc: 64.84%] [G loss: 2.100538]\n",
      "epoch:13 step:13066 [D loss: 0.659085, acc: 59.38%] [G loss: 1.814797]\n",
      "epoch:13 step:13067 [D loss: 0.600428, acc: 70.31%] [G loss: 2.090691]\n",
      "epoch:13 step:13068 [D loss: 0.615339, acc: 65.62%] [G loss: 2.065774]\n",
      "epoch:13 step:13069 [D loss: 0.637927, acc: 64.84%] [G loss: 1.987519]\n",
      "epoch:13 step:13070 [D loss: 0.715519, acc: 51.56%] [G loss: 1.873248]\n",
      "epoch:13 step:13071 [D loss: 0.613105, acc: 67.97%] [G loss: 1.807259]\n",
      "epoch:13 step:13072 [D loss: 0.644099, acc: 60.94%] [G loss: 1.833491]\n",
      "epoch:13 step:13073 [D loss: 0.643728, acc: 64.06%] [G loss: 1.938579]\n",
      "epoch:13 step:13074 [D loss: 0.688511, acc: 59.38%] [G loss: 1.942279]\n",
      "epoch:13 step:13075 [D loss: 0.654664, acc: 64.84%] [G loss: 1.960439]\n",
      "epoch:13 step:13076 [D loss: 0.673487, acc: 53.91%] [G loss: 1.934437]\n",
      "epoch:13 step:13077 [D loss: 0.662545, acc: 60.94%] [G loss: 1.825946]\n",
      "epoch:13 step:13078 [D loss: 0.665657, acc: 56.25%] [G loss: 1.872323]\n",
      "epoch:13 step:13079 [D loss: 0.662140, acc: 66.41%] [G loss: 1.776145]\n",
      "epoch:13 step:13080 [D loss: 0.606238, acc: 66.41%] [G loss: 2.086995]\n",
      "epoch:13 step:13081 [D loss: 0.546426, acc: 71.88%] [G loss: 2.032061]\n",
      "epoch:13 step:13082 [D loss: 0.667868, acc: 57.81%] [G loss: 1.877922]\n",
      "epoch:13 step:13083 [D loss: 0.664857, acc: 58.59%] [G loss: 1.917837]\n",
      "epoch:13 step:13084 [D loss: 0.635246, acc: 66.41%] [G loss: 1.882531]\n",
      "epoch:13 step:13085 [D loss: 0.623338, acc: 70.31%] [G loss: 2.014848]\n",
      "epoch:13 step:13086 [D loss: 0.681808, acc: 60.16%] [G loss: 1.923199]\n",
      "epoch:13 step:13087 [D loss: 0.628526, acc: 67.19%] [G loss: 2.198534]\n",
      "epoch:13 step:13088 [D loss: 0.639948, acc: 61.72%] [G loss: 1.950088]\n",
      "epoch:13 step:13089 [D loss: 0.610830, acc: 64.84%] [G loss: 2.024963]\n",
      "epoch:13 step:13090 [D loss: 0.645237, acc: 65.62%] [G loss: 2.048687]\n",
      "epoch:13 step:13091 [D loss: 0.636714, acc: 59.38%] [G loss: 2.043752]\n",
      "epoch:13 step:13092 [D loss: 0.624585, acc: 61.72%] [G loss: 1.972237]\n",
      "epoch:13 step:13093 [D loss: 0.671878, acc: 62.50%] [G loss: 2.017524]\n",
      "epoch:13 step:13094 [D loss: 0.666561, acc: 61.72%] [G loss: 2.141419]\n",
      "epoch:13 step:13095 [D loss: 0.661698, acc: 59.38%] [G loss: 2.071075]\n",
      "epoch:13 step:13096 [D loss: 0.672214, acc: 62.50%] [G loss: 2.039059]\n",
      "epoch:13 step:13097 [D loss: 0.690825, acc: 56.25%] [G loss: 1.983027]\n",
      "epoch:13 step:13098 [D loss: 0.663628, acc: 56.25%] [G loss: 1.987764]\n",
      "epoch:13 step:13099 [D loss: 0.615615, acc: 64.84%] [G loss: 2.106098]\n",
      "epoch:13 step:13100 [D loss: 0.636190, acc: 64.06%] [G loss: 2.212405]\n",
      "epoch:13 step:13101 [D loss: 0.713178, acc: 53.12%] [G loss: 1.787363]\n",
      "epoch:13 step:13102 [D loss: 0.692811, acc: 60.16%] [G loss: 2.017988]\n",
      "epoch:13 step:13103 [D loss: 0.661608, acc: 57.81%] [G loss: 2.002718]\n",
      "epoch:13 step:13104 [D loss: 0.625455, acc: 63.28%] [G loss: 2.046258]\n",
      "epoch:13 step:13105 [D loss: 0.633692, acc: 63.28%] [G loss: 2.181750]\n",
      "epoch:13 step:13106 [D loss: 0.619081, acc: 64.84%] [G loss: 2.074646]\n",
      "epoch:13 step:13107 [D loss: 0.610439, acc: 66.41%] [G loss: 2.104674]\n",
      "epoch:13 step:13108 [D loss: 0.627549, acc: 63.28%] [G loss: 2.027136]\n",
      "epoch:13 step:13109 [D loss: 0.707852, acc: 57.81%] [G loss: 1.920469]\n",
      "epoch:13 step:13110 [D loss: 0.709723, acc: 53.91%] [G loss: 1.888402]\n",
      "epoch:13 step:13111 [D loss: 0.583283, acc: 68.75%] [G loss: 2.089827]\n",
      "epoch:13 step:13112 [D loss: 0.642105, acc: 61.72%] [G loss: 1.927332]\n",
      "epoch:13 step:13113 [D loss: 0.626061, acc: 59.38%] [G loss: 1.905254]\n",
      "epoch:13 step:13114 [D loss: 0.608085, acc: 69.53%] [G loss: 1.929440]\n",
      "epoch:13 step:13115 [D loss: 0.643561, acc: 61.72%] [G loss: 1.932352]\n",
      "epoch:13 step:13116 [D loss: 0.611210, acc: 65.62%] [G loss: 2.031692]\n",
      "epoch:13 step:13117 [D loss: 0.621749, acc: 70.31%] [G loss: 2.033588]\n",
      "epoch:13 step:13118 [D loss: 0.543490, acc: 75.78%] [G loss: 2.387251]\n",
      "epoch:14 step:13119 [D loss: 0.641541, acc: 60.16%] [G loss: 1.977862]\n",
      "epoch:14 step:13120 [D loss: 0.623788, acc: 63.28%] [G loss: 1.914532]\n",
      "epoch:14 step:13121 [D loss: 0.702722, acc: 53.91%] [G loss: 1.923390]\n",
      "epoch:14 step:13122 [D loss: 0.645308, acc: 58.59%] [G loss: 1.804720]\n",
      "epoch:14 step:13123 [D loss: 0.603525, acc: 70.31%] [G loss: 1.953744]\n",
      "epoch:14 step:13124 [D loss: 0.654215, acc: 66.41%] [G loss: 2.031116]\n",
      "epoch:14 step:13125 [D loss: 0.645968, acc: 63.28%] [G loss: 2.024890]\n",
      "epoch:14 step:13126 [D loss: 0.613039, acc: 65.62%] [G loss: 1.929088]\n",
      "epoch:14 step:13127 [D loss: 0.633345, acc: 62.50%] [G loss: 1.892235]\n",
      "epoch:14 step:13128 [D loss: 0.607798, acc: 68.75%] [G loss: 1.936146]\n",
      "epoch:14 step:13129 [D loss: 0.617580, acc: 60.94%] [G loss: 1.948638]\n",
      "epoch:14 step:13130 [D loss: 0.643911, acc: 64.84%] [G loss: 2.036822]\n",
      "epoch:14 step:13131 [D loss: 0.662463, acc: 65.62%] [G loss: 2.017223]\n",
      "epoch:14 step:13132 [D loss: 0.621151, acc: 63.28%] [G loss: 1.950807]\n",
      "epoch:14 step:13133 [D loss: 0.554508, acc: 72.66%] [G loss: 2.040239]\n",
      "epoch:14 step:13134 [D loss: 0.606442, acc: 63.28%] [G loss: 2.175565]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:14 step:13135 [D loss: 0.632565, acc: 61.72%] [G loss: 1.836679]\n",
      "epoch:14 step:13136 [D loss: 0.695013, acc: 58.59%] [G loss: 1.890232]\n",
      "epoch:14 step:13137 [D loss: 0.637675, acc: 64.84%] [G loss: 1.957777]\n",
      "epoch:14 step:13138 [D loss: 0.699279, acc: 56.25%] [G loss: 1.826018]\n",
      "epoch:14 step:13139 [D loss: 0.649604, acc: 55.47%] [G loss: 1.868879]\n",
      "epoch:14 step:13140 [D loss: 0.658762, acc: 58.59%] [G loss: 1.947251]\n",
      "epoch:14 step:13141 [D loss: 0.581730, acc: 72.66%] [G loss: 2.050323]\n",
      "epoch:14 step:13142 [D loss: 0.590185, acc: 74.22%] [G loss: 1.959876]\n",
      "epoch:14 step:13143 [D loss: 0.600708, acc: 70.31%] [G loss: 2.158993]\n",
      "epoch:14 step:13144 [D loss: 0.617349, acc: 66.41%] [G loss: 1.852725]\n",
      "epoch:14 step:13145 [D loss: 0.622468, acc: 60.94%] [G loss: 1.999574]\n",
      "epoch:14 step:13146 [D loss: 0.591981, acc: 70.31%] [G loss: 1.965946]\n",
      "epoch:14 step:13147 [D loss: 0.658471, acc: 61.72%] [G loss: 1.865274]\n",
      "epoch:14 step:13148 [D loss: 0.655509, acc: 60.16%] [G loss: 2.048189]\n",
      "epoch:14 step:13149 [D loss: 0.663598, acc: 57.03%] [G loss: 1.775575]\n",
      "epoch:14 step:13150 [D loss: 0.662402, acc: 64.06%] [G loss: 1.830711]\n",
      "epoch:14 step:13151 [D loss: 0.652124, acc: 62.50%] [G loss: 1.923643]\n",
      "epoch:14 step:13152 [D loss: 0.634786, acc: 58.59%] [G loss: 1.950742]\n",
      "epoch:14 step:13153 [D loss: 0.667970, acc: 60.94%] [G loss: 2.022552]\n",
      "epoch:14 step:13154 [D loss: 0.637646, acc: 64.06%] [G loss: 1.947306]\n",
      "epoch:14 step:13155 [D loss: 0.640730, acc: 66.41%] [G loss: 1.895246]\n",
      "epoch:14 step:13156 [D loss: 0.681089, acc: 60.16%] [G loss: 2.105300]\n",
      "epoch:14 step:13157 [D loss: 0.642888, acc: 63.28%] [G loss: 2.094253]\n",
      "epoch:14 step:13158 [D loss: 0.630442, acc: 61.72%] [G loss: 2.070102]\n",
      "epoch:14 step:13159 [D loss: 0.648793, acc: 59.38%] [G loss: 1.784366]\n",
      "epoch:14 step:13160 [D loss: 0.619440, acc: 67.19%] [G loss: 2.010922]\n",
      "epoch:14 step:13161 [D loss: 0.648377, acc: 62.50%] [G loss: 1.832463]\n",
      "epoch:14 step:13162 [D loss: 0.636494, acc: 62.50%] [G loss: 1.921832]\n",
      "epoch:14 step:13163 [D loss: 0.571712, acc: 71.88%] [G loss: 1.925736]\n",
      "epoch:14 step:13164 [D loss: 0.674225, acc: 58.59%] [G loss: 1.903082]\n",
      "epoch:14 step:13165 [D loss: 0.613661, acc: 67.97%] [G loss: 1.951410]\n",
      "epoch:14 step:13166 [D loss: 0.595294, acc: 71.88%] [G loss: 2.080781]\n",
      "epoch:14 step:13167 [D loss: 0.581983, acc: 67.19%] [G loss: 2.048837]\n",
      "epoch:14 step:13168 [D loss: 0.607588, acc: 64.06%] [G loss: 1.980594]\n",
      "epoch:14 step:13169 [D loss: 0.651028, acc: 64.06%] [G loss: 1.781686]\n",
      "epoch:14 step:13170 [D loss: 0.620855, acc: 67.19%] [G loss: 1.891540]\n",
      "epoch:14 step:13171 [D loss: 0.637659, acc: 64.84%] [G loss: 2.231239]\n",
      "epoch:14 step:13172 [D loss: 0.655445, acc: 62.50%] [G loss: 2.063305]\n",
      "epoch:14 step:13173 [D loss: 0.590947, acc: 68.75%] [G loss: 2.051954]\n",
      "epoch:14 step:13174 [D loss: 0.583693, acc: 66.41%] [G loss: 1.987094]\n",
      "epoch:14 step:13175 [D loss: 0.649231, acc: 61.72%] [G loss: 2.248955]\n",
      "epoch:14 step:13176 [D loss: 0.607413, acc: 64.06%] [G loss: 1.839875]\n",
      "epoch:14 step:13177 [D loss: 0.625337, acc: 61.72%] [G loss: 2.003640]\n",
      "epoch:14 step:13178 [D loss: 0.654585, acc: 60.16%] [G loss: 2.048609]\n",
      "epoch:14 step:13179 [D loss: 0.652267, acc: 65.62%] [G loss: 1.882137]\n",
      "epoch:14 step:13180 [D loss: 0.693707, acc: 55.47%] [G loss: 1.864038]\n",
      "epoch:14 step:13181 [D loss: 0.626154, acc: 65.62%] [G loss: 1.872879]\n",
      "epoch:14 step:13182 [D loss: 0.667475, acc: 60.16%] [G loss: 1.930704]\n",
      "epoch:14 step:13183 [D loss: 0.608331, acc: 65.62%] [G loss: 1.908287]\n",
      "epoch:14 step:13184 [D loss: 0.626079, acc: 60.16%] [G loss: 1.904140]\n",
      "epoch:14 step:13185 [D loss: 0.719396, acc: 58.59%] [G loss: 1.973342]\n",
      "epoch:14 step:13186 [D loss: 0.615898, acc: 63.28%] [G loss: 1.930647]\n",
      "epoch:14 step:13187 [D loss: 0.658096, acc: 59.38%] [G loss: 2.172313]\n",
      "epoch:14 step:13188 [D loss: 0.644041, acc: 60.16%] [G loss: 1.954038]\n",
      "epoch:14 step:13189 [D loss: 0.622392, acc: 67.19%] [G loss: 2.034146]\n",
      "epoch:14 step:13190 [D loss: 0.628081, acc: 65.62%] [G loss: 2.004511]\n",
      "epoch:14 step:13191 [D loss: 0.632365, acc: 64.06%] [G loss: 1.811524]\n",
      "epoch:14 step:13192 [D loss: 0.652827, acc: 59.38%] [G loss: 1.942217]\n",
      "epoch:14 step:13193 [D loss: 0.556199, acc: 78.12%] [G loss: 2.216920]\n",
      "epoch:14 step:13194 [D loss: 0.613307, acc: 67.97%] [G loss: 2.203898]\n",
      "epoch:14 step:13195 [D loss: 0.599937, acc: 68.75%] [G loss: 2.168013]\n",
      "epoch:14 step:13196 [D loss: 0.633936, acc: 66.41%] [G loss: 1.990735]\n",
      "epoch:14 step:13197 [D loss: 0.669206, acc: 60.16%] [G loss: 1.808906]\n",
      "epoch:14 step:13198 [D loss: 0.654574, acc: 64.84%] [G loss: 1.930272]\n",
      "epoch:14 step:13199 [D loss: 0.717696, acc: 50.78%] [G loss: 1.759038]\n",
      "epoch:14 step:13200 [D loss: 0.677486, acc: 60.16%] [G loss: 1.869846]\n",
      "##############\n",
      "[2.44542515 1.42148108 6.25875407 4.72919218 3.66374859 5.61050913\n",
      " 4.49035066 4.74206591 4.64572724 3.69280045]\n",
      "##########\n",
      "epoch:14 step:13201 [D loss: 0.588955, acc: 70.31%] [G loss: 1.976031]\n",
      "epoch:14 step:13202 [D loss: 0.606809, acc: 65.62%] [G loss: 1.991474]\n",
      "epoch:14 step:13203 [D loss: 0.668472, acc: 60.16%] [G loss: 1.812389]\n",
      "epoch:14 step:13204 [D loss: 0.654123, acc: 60.16%] [G loss: 1.851870]\n",
      "epoch:14 step:13205 [D loss: 0.666243, acc: 60.16%] [G loss: 1.867476]\n",
      "epoch:14 step:13206 [D loss: 0.665794, acc: 63.28%] [G loss: 1.967889]\n",
      "epoch:14 step:13207 [D loss: 0.577124, acc: 72.66%] [G loss: 1.889672]\n",
      "epoch:14 step:13208 [D loss: 0.652599, acc: 66.41%] [G loss: 2.034748]\n",
      "epoch:14 step:13209 [D loss: 0.652946, acc: 60.16%] [G loss: 1.821880]\n",
      "epoch:14 step:13210 [D loss: 0.585009, acc: 67.19%] [G loss: 1.941109]\n",
      "epoch:14 step:13211 [D loss: 0.664649, acc: 62.50%] [G loss: 2.088886]\n",
      "epoch:14 step:13212 [D loss: 0.615421, acc: 67.97%] [G loss: 2.115511]\n",
      "epoch:14 step:13213 [D loss: 0.647709, acc: 62.50%] [G loss: 1.893416]\n",
      "epoch:14 step:13214 [D loss: 0.606821, acc: 68.75%] [G loss: 2.102529]\n",
      "epoch:14 step:13215 [D loss: 0.631678, acc: 61.72%] [G loss: 2.054040]\n",
      "epoch:14 step:13216 [D loss: 0.674688, acc: 61.72%] [G loss: 1.805845]\n",
      "epoch:14 step:13217 [D loss: 0.607249, acc: 64.84%] [G loss: 1.975718]\n",
      "epoch:14 step:13218 [D loss: 0.619736, acc: 66.41%] [G loss: 1.905848]\n",
      "epoch:14 step:13219 [D loss: 0.656831, acc: 58.59%] [G loss: 1.796080]\n",
      "epoch:14 step:13220 [D loss: 0.610573, acc: 64.06%] [G loss: 1.967338]\n",
      "epoch:14 step:13221 [D loss: 0.603535, acc: 61.72%] [G loss: 2.097367]\n",
      "epoch:14 step:13222 [D loss: 0.649487, acc: 60.94%] [G loss: 1.862524]\n",
      "epoch:14 step:13223 [D loss: 0.705384, acc: 53.91%] [G loss: 1.883017]\n",
      "epoch:14 step:13224 [D loss: 0.633796, acc: 61.72%] [G loss: 2.046433]\n",
      "epoch:14 step:13225 [D loss: 0.659418, acc: 58.59%] [G loss: 1.966545]\n",
      "epoch:14 step:13226 [D loss: 0.739109, acc: 53.12%] [G loss: 1.881736]\n",
      "epoch:14 step:13227 [D loss: 0.688497, acc: 55.47%] [G loss: 1.718739]\n",
      "epoch:14 step:13228 [D loss: 0.673111, acc: 55.47%] [G loss: 1.896277]\n",
      "epoch:14 step:13229 [D loss: 0.602352, acc: 64.06%] [G loss: 2.073542]\n",
      "epoch:14 step:13230 [D loss: 0.589793, acc: 71.88%] [G loss: 1.978930]\n",
      "epoch:14 step:13231 [D loss: 0.630795, acc: 64.06%] [G loss: 2.054780]\n",
      "epoch:14 step:13232 [D loss: 0.611434, acc: 74.22%] [G loss: 2.024207]\n",
      "epoch:14 step:13233 [D loss: 0.605556, acc: 72.66%] [G loss: 2.126835]\n",
      "epoch:14 step:13234 [D loss: 0.644333, acc: 64.06%] [G loss: 2.195839]\n",
      "epoch:14 step:13235 [D loss: 0.539038, acc: 73.44%] [G loss: 2.199663]\n",
      "epoch:14 step:13236 [D loss: 0.655778, acc: 67.19%] [G loss: 2.074572]\n",
      "epoch:14 step:13237 [D loss: 0.612828, acc: 64.84%] [G loss: 2.414959]\n",
      "epoch:14 step:13238 [D loss: 0.648341, acc: 65.62%] [G loss: 2.063222]\n",
      "epoch:14 step:13239 [D loss: 0.687385, acc: 57.03%] [G loss: 1.993332]\n",
      "epoch:14 step:13240 [D loss: 0.616750, acc: 69.53%] [G loss: 2.248082]\n",
      "epoch:14 step:13241 [D loss: 0.636119, acc: 63.28%] [G loss: 2.023143]\n",
      "epoch:14 step:13242 [D loss: 0.671169, acc: 59.38%] [G loss: 1.973014]\n",
      "epoch:14 step:13243 [D loss: 0.694556, acc: 56.25%] [G loss: 1.762214]\n",
      "epoch:14 step:13244 [D loss: 0.629918, acc: 62.50%] [G loss: 1.955733]\n",
      "epoch:14 step:13245 [D loss: 0.658579, acc: 59.38%] [G loss: 1.917804]\n",
      "epoch:14 step:13246 [D loss: 0.635224, acc: 59.38%] [G loss: 1.966471]\n",
      "epoch:14 step:13247 [D loss: 0.657206, acc: 61.72%] [G loss: 1.926727]\n",
      "epoch:14 step:13248 [D loss: 0.598048, acc: 68.75%] [G loss: 1.993595]\n",
      "epoch:14 step:13249 [D loss: 0.658061, acc: 64.06%] [G loss: 2.007165]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:14 step:13250 [D loss: 0.635479, acc: 62.50%] [G loss: 1.861275]\n",
      "epoch:14 step:13251 [D loss: 0.677518, acc: 59.38%] [G loss: 1.892507]\n",
      "epoch:14 step:13252 [D loss: 0.638793, acc: 65.62%] [G loss: 1.835513]\n",
      "epoch:14 step:13253 [D loss: 0.664740, acc: 58.59%] [G loss: 1.752740]\n",
      "epoch:14 step:13254 [D loss: 0.676494, acc: 59.38%] [G loss: 1.840611]\n",
      "epoch:14 step:13255 [D loss: 0.644765, acc: 64.84%] [G loss: 1.883000]\n",
      "epoch:14 step:13256 [D loss: 0.641030, acc: 64.84%] [G loss: 1.995234]\n",
      "epoch:14 step:13257 [D loss: 0.631384, acc: 64.84%] [G loss: 2.060601]\n",
      "epoch:14 step:13258 [D loss: 0.677141, acc: 57.03%] [G loss: 1.882568]\n",
      "epoch:14 step:13259 [D loss: 0.660256, acc: 60.94%] [G loss: 1.856242]\n",
      "epoch:14 step:13260 [D loss: 0.680534, acc: 57.03%] [G loss: 1.829591]\n",
      "epoch:14 step:13261 [D loss: 0.715154, acc: 50.00%] [G loss: 1.824429]\n",
      "epoch:14 step:13262 [D loss: 0.641796, acc: 64.84%] [G loss: 1.925864]\n",
      "epoch:14 step:13263 [D loss: 0.654333, acc: 58.59%] [G loss: 1.851428]\n",
      "epoch:14 step:13264 [D loss: 0.650444, acc: 61.72%] [G loss: 1.927986]\n",
      "epoch:14 step:13265 [D loss: 0.702046, acc: 57.81%] [G loss: 1.735219]\n",
      "epoch:14 step:13266 [D loss: 0.655879, acc: 59.38%] [G loss: 1.802709]\n",
      "epoch:14 step:13267 [D loss: 0.651014, acc: 59.38%] [G loss: 1.844012]\n",
      "epoch:14 step:13268 [D loss: 0.651036, acc: 65.62%] [G loss: 1.830869]\n",
      "epoch:14 step:13269 [D loss: 0.603905, acc: 66.41%] [G loss: 1.888141]\n",
      "epoch:14 step:13270 [D loss: 0.633956, acc: 58.59%] [G loss: 2.169968]\n",
      "epoch:14 step:13271 [D loss: 0.670721, acc: 64.84%] [G loss: 1.816306]\n",
      "epoch:14 step:13272 [D loss: 0.637000, acc: 66.41%] [G loss: 1.874477]\n",
      "epoch:14 step:13273 [D loss: 0.592411, acc: 72.66%] [G loss: 1.913995]\n",
      "epoch:14 step:13274 [D loss: 0.622956, acc: 67.97%] [G loss: 1.840466]\n",
      "epoch:14 step:13275 [D loss: 0.651089, acc: 58.59%] [G loss: 1.932185]\n",
      "epoch:14 step:13276 [D loss: 0.645482, acc: 64.06%] [G loss: 1.804561]\n",
      "epoch:14 step:13277 [D loss: 0.609066, acc: 67.19%] [G loss: 1.837521]\n",
      "epoch:14 step:13278 [D loss: 0.655425, acc: 63.28%] [G loss: 1.979704]\n",
      "epoch:14 step:13279 [D loss: 0.691547, acc: 57.81%] [G loss: 1.914793]\n",
      "epoch:14 step:13280 [D loss: 0.647298, acc: 60.94%] [G loss: 1.930928]\n",
      "epoch:14 step:13281 [D loss: 0.660192, acc: 57.81%] [G loss: 1.895635]\n",
      "epoch:14 step:13282 [D loss: 0.685998, acc: 55.47%] [G loss: 1.875505]\n",
      "epoch:14 step:13283 [D loss: 0.651838, acc: 59.38%] [G loss: 1.894055]\n",
      "epoch:14 step:13284 [D loss: 0.615347, acc: 64.06%] [G loss: 1.864592]\n",
      "epoch:14 step:13285 [D loss: 0.642015, acc: 58.59%] [G loss: 1.855720]\n",
      "epoch:14 step:13286 [D loss: 0.608058, acc: 65.62%] [G loss: 1.908751]\n",
      "epoch:14 step:13287 [D loss: 0.614349, acc: 67.97%] [G loss: 1.939245]\n",
      "epoch:14 step:13288 [D loss: 0.696879, acc: 57.81%] [G loss: 1.871248]\n",
      "epoch:14 step:13289 [D loss: 0.624509, acc: 64.84%] [G loss: 1.794354]\n",
      "epoch:14 step:13290 [D loss: 0.634111, acc: 67.97%] [G loss: 1.855486]\n",
      "epoch:14 step:13291 [D loss: 0.688532, acc: 59.38%] [G loss: 1.852958]\n",
      "epoch:14 step:13292 [D loss: 0.621669, acc: 63.28%] [G loss: 1.878540]\n",
      "epoch:14 step:13293 [D loss: 0.663816, acc: 54.69%] [G loss: 1.848432]\n",
      "epoch:14 step:13294 [D loss: 0.695372, acc: 53.91%] [G loss: 1.916361]\n",
      "epoch:14 step:13295 [D loss: 0.677425, acc: 57.81%] [G loss: 1.833436]\n",
      "epoch:14 step:13296 [D loss: 0.713366, acc: 58.59%] [G loss: 1.928207]\n",
      "epoch:14 step:13297 [D loss: 0.643064, acc: 59.38%] [G loss: 1.852214]\n",
      "epoch:14 step:13298 [D loss: 0.671696, acc: 61.72%] [G loss: 1.841949]\n",
      "epoch:14 step:13299 [D loss: 0.599364, acc: 64.84%] [G loss: 1.932998]\n",
      "epoch:14 step:13300 [D loss: 0.618912, acc: 67.19%] [G loss: 1.915196]\n",
      "epoch:14 step:13301 [D loss: 0.618640, acc: 73.44%] [G loss: 2.040352]\n",
      "epoch:14 step:13302 [D loss: 0.609042, acc: 64.06%] [G loss: 1.939567]\n",
      "epoch:14 step:13303 [D loss: 0.638646, acc: 60.16%] [G loss: 1.949984]\n",
      "epoch:14 step:13304 [D loss: 0.676600, acc: 64.84%] [G loss: 1.831130]\n",
      "epoch:14 step:13305 [D loss: 0.633473, acc: 64.84%] [G loss: 1.930408]\n",
      "epoch:14 step:13306 [D loss: 0.642205, acc: 63.28%] [G loss: 1.858967]\n",
      "epoch:14 step:13307 [D loss: 0.688410, acc: 58.59%] [G loss: 1.943741]\n",
      "epoch:14 step:13308 [D loss: 0.595187, acc: 68.75%] [G loss: 1.809297]\n",
      "epoch:14 step:13309 [D loss: 0.600993, acc: 67.19%] [G loss: 2.030083]\n",
      "epoch:14 step:13310 [D loss: 0.630280, acc: 67.19%] [G loss: 2.030871]\n",
      "epoch:14 step:13311 [D loss: 0.632815, acc: 65.62%] [G loss: 1.962995]\n",
      "epoch:14 step:13312 [D loss: 0.642386, acc: 60.94%] [G loss: 2.072690]\n",
      "epoch:14 step:13313 [D loss: 0.631902, acc: 66.41%] [G loss: 1.788292]\n",
      "epoch:14 step:13314 [D loss: 0.707669, acc: 57.81%] [G loss: 1.784999]\n",
      "epoch:14 step:13315 [D loss: 0.645248, acc: 63.28%] [G loss: 1.907514]\n",
      "epoch:14 step:13316 [D loss: 0.661767, acc: 57.03%] [G loss: 1.997275]\n",
      "epoch:14 step:13317 [D loss: 0.687588, acc: 53.12%] [G loss: 1.913419]\n",
      "epoch:14 step:13318 [D loss: 0.699891, acc: 56.25%] [G loss: 1.855000]\n",
      "epoch:14 step:13319 [D loss: 0.608152, acc: 68.75%] [G loss: 1.890826]\n",
      "epoch:14 step:13320 [D loss: 0.651651, acc: 60.16%] [G loss: 1.927695]\n",
      "epoch:14 step:13321 [D loss: 0.650897, acc: 65.62%] [G loss: 1.955299]\n",
      "epoch:14 step:13322 [D loss: 0.639483, acc: 65.62%] [G loss: 1.944652]\n",
      "epoch:14 step:13323 [D loss: 0.677435, acc: 52.34%] [G loss: 1.959454]\n",
      "epoch:14 step:13324 [D loss: 0.589690, acc: 67.97%] [G loss: 2.120430]\n",
      "epoch:14 step:13325 [D loss: 0.594785, acc: 70.31%] [G loss: 2.302833]\n",
      "epoch:14 step:13326 [D loss: 0.567974, acc: 71.09%] [G loss: 2.165307]\n",
      "epoch:14 step:13327 [D loss: 0.563465, acc: 71.88%] [G loss: 2.323773]\n",
      "epoch:14 step:13328 [D loss: 0.630590, acc: 66.41%] [G loss: 1.960454]\n",
      "epoch:14 step:13329 [D loss: 0.658759, acc: 60.94%] [G loss: 1.919309]\n",
      "epoch:14 step:13330 [D loss: 0.683321, acc: 57.81%] [G loss: 1.868599]\n",
      "epoch:14 step:13331 [D loss: 0.643010, acc: 65.62%] [G loss: 1.770596]\n",
      "epoch:14 step:13332 [D loss: 0.699529, acc: 60.16%] [G loss: 1.856033]\n",
      "epoch:14 step:13333 [D loss: 0.731509, acc: 50.78%] [G loss: 1.761979]\n",
      "epoch:14 step:13334 [D loss: 0.655874, acc: 62.50%] [G loss: 1.941612]\n",
      "epoch:14 step:13335 [D loss: 0.620575, acc: 67.19%] [G loss: 1.850671]\n",
      "epoch:14 step:13336 [D loss: 0.605658, acc: 64.84%] [G loss: 1.918955]\n",
      "epoch:14 step:13337 [D loss: 0.645210, acc: 62.50%] [G loss: 2.162681]\n",
      "epoch:14 step:13338 [D loss: 0.723211, acc: 57.03%] [G loss: 1.811502]\n",
      "epoch:14 step:13339 [D loss: 0.704616, acc: 56.25%] [G loss: 1.980156]\n",
      "epoch:14 step:13340 [D loss: 0.597087, acc: 67.97%] [G loss: 1.992714]\n",
      "epoch:14 step:13341 [D loss: 0.623388, acc: 67.97%] [G loss: 1.998702]\n",
      "epoch:14 step:13342 [D loss: 0.636710, acc: 64.06%] [G loss: 1.740999]\n",
      "epoch:14 step:13343 [D loss: 0.605500, acc: 72.66%] [G loss: 1.916864]\n",
      "epoch:14 step:13344 [D loss: 0.665092, acc: 50.78%] [G loss: 1.820891]\n",
      "epoch:14 step:13345 [D loss: 0.645437, acc: 60.16%] [G loss: 1.877122]\n",
      "epoch:14 step:13346 [D loss: 0.676417, acc: 61.72%] [G loss: 1.831817]\n",
      "epoch:14 step:13347 [D loss: 0.584588, acc: 71.09%] [G loss: 1.965072]\n",
      "epoch:14 step:13348 [D loss: 0.588174, acc: 66.41%] [G loss: 2.167491]\n",
      "epoch:14 step:13349 [D loss: 0.565718, acc: 71.09%] [G loss: 2.462372]\n",
      "epoch:14 step:13350 [D loss: 0.577792, acc: 72.66%] [G loss: 2.507379]\n",
      "epoch:14 step:13351 [D loss: 0.638152, acc: 64.06%] [G loss: 2.060112]\n",
      "epoch:14 step:13352 [D loss: 0.663216, acc: 59.38%] [G loss: 2.032120]\n",
      "epoch:14 step:13353 [D loss: 0.637574, acc: 62.50%] [G loss: 1.826256]\n",
      "epoch:14 step:13354 [D loss: 0.719830, acc: 59.38%] [G loss: 1.915383]\n",
      "epoch:14 step:13355 [D loss: 0.630771, acc: 67.19%] [G loss: 1.912121]\n",
      "epoch:14 step:13356 [D loss: 0.606858, acc: 66.41%] [G loss: 1.857924]\n",
      "epoch:14 step:13357 [D loss: 0.617252, acc: 65.62%] [G loss: 1.975380]\n",
      "epoch:14 step:13358 [D loss: 0.620307, acc: 61.72%] [G loss: 2.157567]\n",
      "epoch:14 step:13359 [D loss: 0.573529, acc: 75.00%] [G loss: 1.972806]\n",
      "epoch:14 step:13360 [D loss: 0.572515, acc: 75.00%] [G loss: 2.068316]\n",
      "epoch:14 step:13361 [D loss: 0.657581, acc: 58.59%] [G loss: 2.098706]\n",
      "epoch:14 step:13362 [D loss: 0.649022, acc: 61.72%] [G loss: 1.873615]\n",
      "epoch:14 step:13363 [D loss: 0.668380, acc: 54.69%] [G loss: 1.939499]\n",
      "epoch:14 step:13364 [D loss: 0.663095, acc: 60.16%] [G loss: 1.980425]\n",
      "epoch:14 step:13365 [D loss: 0.654765, acc: 57.81%] [G loss: 1.962624]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:14 step:13366 [D loss: 0.581771, acc: 64.84%] [G loss: 2.023653]\n",
      "epoch:14 step:13367 [D loss: 0.667835, acc: 63.28%] [G loss: 2.064343]\n",
      "epoch:14 step:13368 [D loss: 0.701745, acc: 58.59%] [G loss: 1.792627]\n",
      "epoch:14 step:13369 [D loss: 0.708972, acc: 50.00%] [G loss: 1.847322]\n",
      "epoch:14 step:13370 [D loss: 0.670364, acc: 62.50%] [G loss: 1.697089]\n",
      "epoch:14 step:13371 [D loss: 0.604905, acc: 66.41%] [G loss: 1.759969]\n",
      "epoch:14 step:13372 [D loss: 0.667473, acc: 59.38%] [G loss: 1.857494]\n",
      "epoch:14 step:13373 [D loss: 0.673539, acc: 55.47%] [G loss: 1.834776]\n",
      "epoch:14 step:13374 [D loss: 0.643444, acc: 59.38%] [G loss: 1.925388]\n",
      "epoch:14 step:13375 [D loss: 0.694301, acc: 54.69%] [G loss: 1.672627]\n",
      "epoch:14 step:13376 [D loss: 0.607664, acc: 71.09%] [G loss: 1.763345]\n",
      "epoch:14 step:13377 [D loss: 0.653468, acc: 66.41%] [G loss: 1.870072]\n",
      "epoch:14 step:13378 [D loss: 0.647614, acc: 59.38%] [G loss: 1.906196]\n",
      "epoch:14 step:13379 [D loss: 0.626080, acc: 61.72%] [G loss: 2.029096]\n",
      "epoch:14 step:13380 [D loss: 0.645460, acc: 67.19%] [G loss: 2.037409]\n",
      "epoch:14 step:13381 [D loss: 0.669121, acc: 65.62%] [G loss: 1.947693]\n",
      "epoch:14 step:13382 [D loss: 0.628823, acc: 71.09%] [G loss: 1.957483]\n",
      "epoch:14 step:13383 [D loss: 0.630379, acc: 66.41%] [G loss: 1.816847]\n",
      "epoch:14 step:13384 [D loss: 0.630982, acc: 65.62%] [G loss: 1.858271]\n",
      "epoch:14 step:13385 [D loss: 0.665651, acc: 58.59%] [G loss: 1.901973]\n",
      "epoch:14 step:13386 [D loss: 0.691939, acc: 53.91%] [G loss: 1.795846]\n",
      "epoch:14 step:13387 [D loss: 0.657889, acc: 60.16%] [G loss: 1.851549]\n",
      "epoch:14 step:13388 [D loss: 0.653353, acc: 59.38%] [G loss: 2.030518]\n",
      "epoch:14 step:13389 [D loss: 0.628094, acc: 64.06%] [G loss: 1.984449]\n",
      "epoch:14 step:13390 [D loss: 0.601867, acc: 70.31%] [G loss: 1.956180]\n",
      "epoch:14 step:13391 [D loss: 0.646655, acc: 63.28%] [G loss: 1.908234]\n",
      "epoch:14 step:13392 [D loss: 0.596016, acc: 68.75%] [G loss: 2.041631]\n",
      "epoch:14 step:13393 [D loss: 0.619038, acc: 67.19%] [G loss: 2.203625]\n",
      "epoch:14 step:13394 [D loss: 0.634283, acc: 70.31%] [G loss: 2.134167]\n",
      "epoch:14 step:13395 [D loss: 0.614092, acc: 61.72%] [G loss: 2.097375]\n",
      "epoch:14 step:13396 [D loss: 0.662591, acc: 57.81%] [G loss: 1.867391]\n",
      "epoch:14 step:13397 [D loss: 0.643427, acc: 67.19%] [G loss: 1.955208]\n",
      "epoch:14 step:13398 [D loss: 0.612382, acc: 63.28%] [G loss: 1.997478]\n",
      "epoch:14 step:13399 [D loss: 0.665286, acc: 60.94%] [G loss: 1.834580]\n",
      "epoch:14 step:13400 [D loss: 0.658099, acc: 64.06%] [G loss: 1.820423]\n",
      "##############\n",
      "[2.53811512 1.54678386 6.47103081 4.89533715 3.74739196 5.85624453\n",
      " 4.4731873  4.72057089 4.81766288 3.7912263 ]\n",
      "##########\n",
      "epoch:14 step:13401 [D loss: 0.611702, acc: 64.06%] [G loss: 1.901421]\n",
      "epoch:14 step:13402 [D loss: 0.639766, acc: 57.81%] [G loss: 1.947208]\n",
      "epoch:14 step:13403 [D loss: 0.625850, acc: 67.97%] [G loss: 2.072171]\n",
      "epoch:14 step:13404 [D loss: 0.586719, acc: 67.97%] [G loss: 2.022461]\n",
      "epoch:14 step:13405 [D loss: 0.648692, acc: 61.72%] [G loss: 1.934326]\n",
      "epoch:14 step:13406 [D loss: 0.670260, acc: 60.16%] [G loss: 2.014676]\n",
      "epoch:14 step:13407 [D loss: 0.655654, acc: 58.59%] [G loss: 2.018929]\n",
      "epoch:14 step:13408 [D loss: 0.626982, acc: 64.06%] [G loss: 1.928548]\n",
      "epoch:14 step:13409 [D loss: 0.644904, acc: 63.28%] [G loss: 1.983335]\n",
      "epoch:14 step:13410 [D loss: 0.674297, acc: 57.81%] [G loss: 1.787233]\n",
      "epoch:14 step:13411 [D loss: 0.607915, acc: 64.06%] [G loss: 2.050954]\n",
      "epoch:14 step:13412 [D loss: 0.656557, acc: 64.84%] [G loss: 1.978290]\n",
      "epoch:14 step:13413 [D loss: 0.657356, acc: 60.94%] [G loss: 1.873739]\n",
      "epoch:14 step:13414 [D loss: 0.609234, acc: 70.31%] [G loss: 2.104353]\n",
      "epoch:14 step:13415 [D loss: 0.619707, acc: 62.50%] [G loss: 1.967765]\n",
      "epoch:14 step:13416 [D loss: 0.643776, acc: 64.84%] [G loss: 2.074345]\n",
      "epoch:14 step:13417 [D loss: 0.647288, acc: 62.50%] [G loss: 1.860921]\n",
      "epoch:14 step:13418 [D loss: 0.624693, acc: 67.19%] [G loss: 1.883728]\n",
      "epoch:14 step:13419 [D loss: 0.666581, acc: 64.06%] [G loss: 1.818245]\n",
      "epoch:14 step:13420 [D loss: 0.641201, acc: 62.50%] [G loss: 1.981631]\n",
      "epoch:14 step:13421 [D loss: 0.623166, acc: 64.06%] [G loss: 1.799912]\n",
      "epoch:14 step:13422 [D loss: 0.709838, acc: 57.81%] [G loss: 1.781687]\n",
      "epoch:14 step:13423 [D loss: 0.612551, acc: 64.06%] [G loss: 1.872901]\n",
      "epoch:14 step:13424 [D loss: 0.660418, acc: 59.38%] [G loss: 1.774079]\n",
      "epoch:14 step:13425 [D loss: 0.626203, acc: 63.28%] [G loss: 1.872483]\n",
      "epoch:14 step:13426 [D loss: 0.615550, acc: 67.97%] [G loss: 1.799899]\n",
      "epoch:14 step:13427 [D loss: 0.611229, acc: 67.97%] [G loss: 1.991530]\n",
      "epoch:14 step:13428 [D loss: 0.646089, acc: 60.94%] [G loss: 1.984211]\n",
      "epoch:14 step:13429 [D loss: 0.607999, acc: 67.97%] [G loss: 1.876964]\n",
      "epoch:14 step:13430 [D loss: 0.592444, acc: 71.09%] [G loss: 2.241726]\n",
      "epoch:14 step:13431 [D loss: 0.610063, acc: 64.06%] [G loss: 2.126878]\n",
      "epoch:14 step:13432 [D loss: 0.591729, acc: 67.19%] [G loss: 2.235488]\n",
      "epoch:14 step:13433 [D loss: 0.573899, acc: 68.75%] [G loss: 2.216703]\n",
      "epoch:14 step:13434 [D loss: 0.659895, acc: 62.50%] [G loss: 1.757287]\n",
      "epoch:14 step:13435 [D loss: 0.631400, acc: 62.50%] [G loss: 1.898656]\n",
      "epoch:14 step:13436 [D loss: 0.637230, acc: 67.19%] [G loss: 2.241733]\n",
      "epoch:14 step:13437 [D loss: 0.660293, acc: 67.19%] [G loss: 2.046563]\n",
      "epoch:14 step:13438 [D loss: 0.645152, acc: 66.41%] [G loss: 1.932267]\n",
      "epoch:14 step:13439 [D loss: 0.638638, acc: 57.03%] [G loss: 2.024379]\n",
      "epoch:14 step:13440 [D loss: 0.593297, acc: 70.31%] [G loss: 1.919681]\n",
      "epoch:14 step:13441 [D loss: 0.656738, acc: 62.50%] [G loss: 1.857018]\n",
      "epoch:14 step:13442 [D loss: 0.694009, acc: 58.59%] [G loss: 1.834305]\n",
      "epoch:14 step:13443 [D loss: 0.670103, acc: 60.94%] [G loss: 1.919977]\n",
      "epoch:14 step:13444 [D loss: 0.626141, acc: 65.62%] [G loss: 2.019338]\n",
      "epoch:14 step:13445 [D loss: 0.718647, acc: 54.69%] [G loss: 1.803753]\n",
      "epoch:14 step:13446 [D loss: 0.651992, acc: 60.94%] [G loss: 1.921697]\n",
      "epoch:14 step:13447 [D loss: 0.623322, acc: 69.53%] [G loss: 1.953415]\n",
      "epoch:14 step:13448 [D loss: 0.618261, acc: 64.06%] [G loss: 2.035051]\n",
      "epoch:14 step:13449 [D loss: 0.676421, acc: 56.25%] [G loss: 1.994817]\n",
      "epoch:14 step:13450 [D loss: 0.646076, acc: 59.38%] [G loss: 2.032522]\n",
      "epoch:14 step:13451 [D loss: 0.654078, acc: 62.50%] [G loss: 1.934664]\n",
      "epoch:14 step:13452 [D loss: 0.641459, acc: 60.94%] [G loss: 1.891773]\n",
      "epoch:14 step:13453 [D loss: 0.623458, acc: 67.97%] [G loss: 2.009983]\n",
      "epoch:14 step:13454 [D loss: 0.667131, acc: 62.50%] [G loss: 2.020874]\n",
      "epoch:14 step:13455 [D loss: 0.606524, acc: 69.53%] [G loss: 2.015767]\n",
      "epoch:14 step:13456 [D loss: 0.656177, acc: 60.94%] [G loss: 2.076347]\n",
      "epoch:14 step:13457 [D loss: 0.635341, acc: 67.19%] [G loss: 2.020767]\n",
      "epoch:14 step:13458 [D loss: 0.639557, acc: 64.06%] [G loss: 1.992336]\n",
      "epoch:14 step:13459 [D loss: 0.726682, acc: 52.34%] [G loss: 1.857913]\n",
      "epoch:14 step:13460 [D loss: 0.704374, acc: 57.03%] [G loss: 1.863184]\n",
      "epoch:14 step:13461 [D loss: 0.657353, acc: 59.38%] [G loss: 1.912506]\n",
      "epoch:14 step:13462 [D loss: 0.644296, acc: 58.59%] [G loss: 1.924419]\n",
      "epoch:14 step:13463 [D loss: 0.611229, acc: 64.84%] [G loss: 2.233266]\n",
      "epoch:14 step:13464 [D loss: 0.627292, acc: 64.06%] [G loss: 2.314497]\n",
      "epoch:14 step:13465 [D loss: 0.589281, acc: 67.19%] [G loss: 2.406271]\n",
      "epoch:14 step:13466 [D loss: 0.711462, acc: 55.47%] [G loss: 1.882657]\n",
      "epoch:14 step:13467 [D loss: 0.750240, acc: 53.91%] [G loss: 1.752836]\n",
      "epoch:14 step:13468 [D loss: 0.703047, acc: 59.38%] [G loss: 1.957536]\n",
      "epoch:14 step:13469 [D loss: 0.659559, acc: 62.50%] [G loss: 1.764023]\n",
      "epoch:14 step:13470 [D loss: 0.682261, acc: 57.81%] [G loss: 1.818879]\n",
      "epoch:14 step:13471 [D loss: 0.636748, acc: 66.41%] [G loss: 1.814860]\n",
      "epoch:14 step:13472 [D loss: 0.632836, acc: 61.72%] [G loss: 2.048440]\n",
      "epoch:14 step:13473 [D loss: 0.637620, acc: 65.62%] [G loss: 1.830192]\n",
      "epoch:14 step:13474 [D loss: 0.674583, acc: 56.25%] [G loss: 1.845154]\n",
      "epoch:14 step:13475 [D loss: 0.612856, acc: 67.19%] [G loss: 2.018137]\n",
      "epoch:14 step:13476 [D loss: 0.629101, acc: 67.19%] [G loss: 1.951781]\n",
      "epoch:14 step:13477 [D loss: 0.648884, acc: 62.50%] [G loss: 1.929690]\n",
      "epoch:14 step:13478 [D loss: 0.634946, acc: 64.84%] [G loss: 1.927388]\n",
      "epoch:14 step:13479 [D loss: 0.641994, acc: 63.28%] [G loss: 1.788691]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:14 step:13480 [D loss: 0.639369, acc: 59.38%] [G loss: 1.943570]\n",
      "epoch:14 step:13481 [D loss: 0.626291, acc: 67.19%] [G loss: 1.931956]\n",
      "epoch:14 step:13482 [D loss: 0.603537, acc: 66.41%] [G loss: 1.909032]\n",
      "epoch:14 step:13483 [D loss: 0.638835, acc: 60.94%] [G loss: 1.949801]\n",
      "epoch:14 step:13484 [D loss: 0.710060, acc: 55.47%] [G loss: 1.980501]\n",
      "epoch:14 step:13485 [D loss: 0.623683, acc: 64.84%] [G loss: 2.028000]\n",
      "epoch:14 step:13486 [D loss: 0.620451, acc: 64.84%] [G loss: 1.914302]\n",
      "epoch:14 step:13487 [D loss: 0.608196, acc: 69.53%] [G loss: 1.920640]\n",
      "epoch:14 step:13488 [D loss: 0.593400, acc: 68.75%] [G loss: 2.023082]\n",
      "epoch:14 step:13489 [D loss: 0.631867, acc: 63.28%] [G loss: 1.999274]\n",
      "epoch:14 step:13490 [D loss: 0.597940, acc: 64.84%] [G loss: 1.912167]\n",
      "epoch:14 step:13491 [D loss: 0.646371, acc: 56.25%] [G loss: 1.826830]\n",
      "epoch:14 step:13492 [D loss: 0.533770, acc: 78.91%] [G loss: 1.949012]\n",
      "epoch:14 step:13493 [D loss: 0.680729, acc: 61.72%] [G loss: 1.892792]\n",
      "epoch:14 step:13494 [D loss: 0.626211, acc: 69.53%] [G loss: 1.792112]\n",
      "epoch:14 step:13495 [D loss: 0.619959, acc: 64.84%] [G loss: 1.787390]\n",
      "epoch:14 step:13496 [D loss: 0.647395, acc: 62.50%] [G loss: 1.935790]\n",
      "epoch:14 step:13497 [D loss: 0.701802, acc: 57.81%] [G loss: 1.867384]\n",
      "epoch:14 step:13498 [D loss: 0.671663, acc: 58.59%] [G loss: 1.987983]\n",
      "epoch:14 step:13499 [D loss: 0.645665, acc: 64.06%] [G loss: 1.993422]\n",
      "epoch:14 step:13500 [D loss: 0.658038, acc: 61.72%] [G loss: 1.873085]\n",
      "epoch:14 step:13501 [D loss: 0.637847, acc: 64.84%] [G loss: 1.963309]\n",
      "epoch:14 step:13502 [D loss: 0.599915, acc: 71.09%] [G loss: 2.044495]\n",
      "epoch:14 step:13503 [D loss: 0.619026, acc: 67.19%] [G loss: 1.957449]\n",
      "epoch:14 step:13504 [D loss: 0.680753, acc: 58.59%] [G loss: 1.815643]\n",
      "epoch:14 step:13505 [D loss: 0.643687, acc: 62.50%] [G loss: 1.907906]\n",
      "epoch:14 step:13506 [D loss: 0.672425, acc: 62.50%] [G loss: 1.957543]\n",
      "epoch:14 step:13507 [D loss: 0.658272, acc: 61.72%] [G loss: 1.811819]\n",
      "epoch:14 step:13508 [D loss: 0.625900, acc: 62.50%] [G loss: 1.880682]\n",
      "epoch:14 step:13509 [D loss: 0.710919, acc: 56.25%] [G loss: 1.797310]\n",
      "epoch:14 step:13510 [D loss: 0.623007, acc: 66.41%] [G loss: 1.798129]\n",
      "epoch:14 step:13511 [D loss: 0.710372, acc: 53.91%] [G loss: 1.710236]\n",
      "epoch:14 step:13512 [D loss: 0.656551, acc: 62.50%] [G loss: 1.811474]\n",
      "epoch:14 step:13513 [D loss: 0.627761, acc: 66.41%] [G loss: 1.896917]\n",
      "epoch:14 step:13514 [D loss: 0.693969, acc: 54.69%] [G loss: 1.776761]\n",
      "epoch:14 step:13515 [D loss: 0.660820, acc: 60.16%] [G loss: 1.791734]\n",
      "epoch:14 step:13516 [D loss: 0.670177, acc: 59.38%] [G loss: 1.822899]\n",
      "epoch:14 step:13517 [D loss: 0.644144, acc: 61.72%] [G loss: 1.865394]\n",
      "epoch:14 step:13518 [D loss: 0.647044, acc: 61.72%] [G loss: 1.801723]\n",
      "epoch:14 step:13519 [D loss: 0.629995, acc: 61.72%] [G loss: 1.763908]\n",
      "epoch:14 step:13520 [D loss: 0.622985, acc: 66.41%] [G loss: 1.878065]\n",
      "epoch:14 step:13521 [D loss: 0.642187, acc: 65.62%] [G loss: 1.955193]\n",
      "epoch:14 step:13522 [D loss: 0.703190, acc: 57.03%] [G loss: 2.009058]\n",
      "epoch:14 step:13523 [D loss: 0.609437, acc: 67.19%] [G loss: 2.150809]\n",
      "epoch:14 step:13524 [D loss: 0.599466, acc: 74.22%] [G loss: 2.203967]\n",
      "epoch:14 step:13525 [D loss: 0.646813, acc: 59.38%] [G loss: 2.019051]\n",
      "epoch:14 step:13526 [D loss: 0.673106, acc: 57.81%] [G loss: 1.981031]\n",
      "epoch:14 step:13527 [D loss: 0.711523, acc: 55.47%] [G loss: 1.925650]\n",
      "epoch:14 step:13528 [D loss: 0.682726, acc: 60.94%] [G loss: 1.848546]\n",
      "epoch:14 step:13529 [D loss: 0.633029, acc: 63.28%] [G loss: 1.884762]\n",
      "epoch:14 step:13530 [D loss: 0.644582, acc: 64.84%] [G loss: 1.945907]\n",
      "epoch:14 step:13531 [D loss: 0.644626, acc: 58.59%] [G loss: 1.741943]\n",
      "epoch:14 step:13532 [D loss: 0.638369, acc: 67.19%] [G loss: 1.962347]\n",
      "epoch:14 step:13533 [D loss: 0.599271, acc: 66.41%] [G loss: 1.946666]\n",
      "epoch:14 step:13534 [D loss: 0.612386, acc: 64.84%] [G loss: 2.117983]\n",
      "epoch:14 step:13535 [D loss: 0.638880, acc: 62.50%] [G loss: 2.008612]\n",
      "epoch:14 step:13536 [D loss: 0.693449, acc: 58.59%] [G loss: 1.826761]\n",
      "epoch:14 step:13537 [D loss: 0.652004, acc: 64.84%] [G loss: 1.905522]\n",
      "epoch:14 step:13538 [D loss: 0.643390, acc: 63.28%] [G loss: 2.047985]\n",
      "epoch:14 step:13539 [D loss: 0.692835, acc: 58.59%] [G loss: 1.854492]\n",
      "epoch:14 step:13540 [D loss: 0.656453, acc: 60.16%] [G loss: 1.758940]\n",
      "epoch:14 step:13541 [D loss: 0.650329, acc: 57.81%] [G loss: 1.908286]\n",
      "epoch:14 step:13542 [D loss: 0.699520, acc: 54.69%] [G loss: 1.792188]\n",
      "epoch:14 step:13543 [D loss: 0.671892, acc: 61.72%] [G loss: 1.915436]\n",
      "epoch:14 step:13544 [D loss: 0.642132, acc: 58.59%] [G loss: 1.896715]\n",
      "epoch:14 step:13545 [D loss: 0.652088, acc: 60.16%] [G loss: 1.955311]\n",
      "epoch:14 step:13546 [D loss: 0.609858, acc: 64.84%] [G loss: 2.142352]\n",
      "epoch:14 step:13547 [D loss: 0.584810, acc: 68.75%] [G loss: 2.108585]\n",
      "epoch:14 step:13548 [D loss: 0.599499, acc: 68.75%] [G loss: 2.065973]\n",
      "epoch:14 step:13549 [D loss: 0.612661, acc: 65.62%] [G loss: 1.946938]\n",
      "epoch:14 step:13550 [D loss: 0.663627, acc: 62.50%] [G loss: 1.819642]\n",
      "epoch:14 step:13551 [D loss: 0.651467, acc: 57.03%] [G loss: 1.815675]\n",
      "epoch:14 step:13552 [D loss: 0.616007, acc: 67.97%] [G loss: 1.999398]\n",
      "epoch:14 step:13553 [D loss: 0.664121, acc: 67.19%] [G loss: 1.913190]\n",
      "epoch:14 step:13554 [D loss: 0.645224, acc: 60.94%] [G loss: 2.090581]\n",
      "epoch:14 step:13555 [D loss: 0.711772, acc: 51.56%] [G loss: 1.671363]\n",
      "epoch:14 step:13556 [D loss: 0.708349, acc: 53.91%] [G loss: 1.759318]\n",
      "epoch:14 step:13557 [D loss: 0.675230, acc: 62.50%] [G loss: 1.754158]\n",
      "epoch:14 step:13558 [D loss: 0.638203, acc: 63.28%] [G loss: 1.798369]\n",
      "epoch:14 step:13559 [D loss: 0.652153, acc: 58.59%] [G loss: 1.792223]\n",
      "epoch:14 step:13560 [D loss: 0.666148, acc: 64.84%] [G loss: 1.792449]\n",
      "epoch:14 step:13561 [D loss: 0.631937, acc: 64.06%] [G loss: 1.852628]\n",
      "epoch:14 step:13562 [D loss: 0.679388, acc: 57.81%] [G loss: 1.725305]\n",
      "epoch:14 step:13563 [D loss: 0.657602, acc: 60.16%] [G loss: 1.836600]\n",
      "epoch:14 step:13564 [D loss: 0.656356, acc: 60.16%] [G loss: 1.687491]\n",
      "epoch:14 step:13565 [D loss: 0.663977, acc: 55.47%] [G loss: 1.835173]\n",
      "epoch:14 step:13566 [D loss: 0.702041, acc: 57.81%] [G loss: 1.830992]\n",
      "epoch:14 step:13567 [D loss: 0.633611, acc: 62.50%] [G loss: 1.964894]\n",
      "epoch:14 step:13568 [D loss: 0.628744, acc: 67.19%] [G loss: 1.707853]\n",
      "epoch:14 step:13569 [D loss: 0.635206, acc: 66.41%] [G loss: 1.964806]\n",
      "epoch:14 step:13570 [D loss: 0.626584, acc: 67.97%] [G loss: 1.798944]\n",
      "epoch:14 step:13571 [D loss: 0.629757, acc: 65.62%] [G loss: 2.001235]\n",
      "epoch:14 step:13572 [D loss: 0.643437, acc: 64.06%] [G loss: 1.979372]\n",
      "epoch:14 step:13573 [D loss: 0.656261, acc: 60.94%] [G loss: 1.808407]\n",
      "epoch:14 step:13574 [D loss: 0.605367, acc: 71.88%] [G loss: 1.936628]\n",
      "epoch:14 step:13575 [D loss: 0.612926, acc: 70.31%] [G loss: 1.982136]\n",
      "epoch:14 step:13576 [D loss: 0.691822, acc: 57.81%] [G loss: 1.823042]\n",
      "epoch:14 step:13577 [D loss: 0.656718, acc: 63.28%] [G loss: 1.743899]\n",
      "epoch:14 step:13578 [D loss: 0.673255, acc: 59.38%] [G loss: 1.813663]\n",
      "epoch:14 step:13579 [D loss: 0.657042, acc: 60.16%] [G loss: 1.858566]\n",
      "epoch:14 step:13580 [D loss: 0.618422, acc: 67.97%] [G loss: 1.813295]\n",
      "epoch:14 step:13581 [D loss: 0.619061, acc: 67.97%] [G loss: 2.066482]\n",
      "epoch:14 step:13582 [D loss: 0.670131, acc: 58.59%] [G loss: 1.910489]\n",
      "epoch:14 step:13583 [D loss: 0.652881, acc: 60.16%] [G loss: 1.872458]\n",
      "epoch:14 step:13584 [D loss: 0.685206, acc: 56.25%] [G loss: 1.817946]\n",
      "epoch:14 step:13585 [D loss: 0.680680, acc: 65.62%] [G loss: 1.818289]\n",
      "epoch:14 step:13586 [D loss: 0.639095, acc: 66.41%] [G loss: 2.014865]\n",
      "epoch:14 step:13587 [D loss: 0.602439, acc: 67.97%] [G loss: 2.146992]\n",
      "epoch:14 step:13588 [D loss: 0.618042, acc: 67.19%] [G loss: 2.123240]\n",
      "epoch:14 step:13589 [D loss: 0.612110, acc: 64.06%] [G loss: 2.293406]\n",
      "epoch:14 step:13590 [D loss: 0.637990, acc: 62.50%] [G loss: 1.979483]\n",
      "epoch:14 step:13591 [D loss: 0.699524, acc: 54.69%] [G loss: 1.883711]\n",
      "epoch:14 step:13592 [D loss: 0.648373, acc: 61.72%] [G loss: 1.933351]\n",
      "epoch:14 step:13593 [D loss: 0.610458, acc: 69.53%] [G loss: 1.916856]\n",
      "epoch:14 step:13594 [D loss: 0.673538, acc: 59.38%] [G loss: 1.895533]\n",
      "epoch:14 step:13595 [D loss: 0.650168, acc: 60.94%] [G loss: 1.877253]\n",
      "epoch:14 step:13596 [D loss: 0.689649, acc: 54.69%] [G loss: 1.893861]\n",
      "epoch:14 step:13597 [D loss: 0.634609, acc: 63.28%] [G loss: 1.969448]\n",
      "epoch:14 step:13598 [D loss: 0.611331, acc: 64.06%] [G loss: 2.124329]\n",
      "epoch:14 step:13599 [D loss: 0.645288, acc: 63.28%] [G loss: 2.192302]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:14 step:13600 [D loss: 0.694025, acc: 55.47%] [G loss: 1.766780]\n",
      "##############\n",
      "[2.38507809 1.40671956 6.23860222 4.81406669 3.60558349 5.6526335\n",
      " 4.37386256 4.8080394  4.56793327 3.52091113]\n",
      "##########\n",
      "epoch:14 step:13601 [D loss: 0.636401, acc: 61.72%] [G loss: 1.856122]\n",
      "epoch:14 step:13602 [D loss: 0.620480, acc: 66.41%] [G loss: 1.852605]\n",
      "epoch:14 step:13603 [D loss: 0.663959, acc: 57.81%] [G loss: 1.867778]\n",
      "epoch:14 step:13604 [D loss: 0.675365, acc: 58.59%] [G loss: 1.765559]\n",
      "epoch:14 step:13605 [D loss: 0.615368, acc: 65.62%] [G loss: 1.939237]\n",
      "epoch:14 step:13606 [D loss: 0.620297, acc: 62.50%] [G loss: 1.943374]\n",
      "epoch:14 step:13607 [D loss: 0.635191, acc: 66.41%] [G loss: 1.935999]\n",
      "epoch:14 step:13608 [D loss: 0.573830, acc: 67.19%] [G loss: 1.904210]\n",
      "epoch:14 step:13609 [D loss: 0.625835, acc: 64.84%] [G loss: 1.973572]\n",
      "epoch:14 step:13610 [D loss: 0.693838, acc: 60.16%] [G loss: 1.903117]\n",
      "epoch:14 step:13611 [D loss: 0.615828, acc: 63.28%] [G loss: 1.949924]\n",
      "epoch:14 step:13612 [D loss: 0.596243, acc: 68.75%] [G loss: 2.282545]\n",
      "epoch:14 step:13613 [D loss: 0.612772, acc: 64.06%] [G loss: 2.346090]\n",
      "epoch:14 step:13614 [D loss: 0.651301, acc: 62.50%] [G loss: 2.007713]\n",
      "epoch:14 step:13615 [D loss: 0.587227, acc: 69.53%] [G loss: 2.196798]\n",
      "epoch:14 step:13616 [D loss: 0.626091, acc: 67.19%] [G loss: 2.252075]\n",
      "epoch:14 step:13617 [D loss: 0.600890, acc: 71.09%] [G loss: 2.060029]\n",
      "epoch:14 step:13618 [D loss: 0.679542, acc: 65.62%] [G loss: 1.812792]\n",
      "epoch:14 step:13619 [D loss: 0.701132, acc: 55.47%] [G loss: 1.721291]\n",
      "epoch:14 step:13620 [D loss: 0.681739, acc: 60.94%] [G loss: 1.727604]\n",
      "epoch:14 step:13621 [D loss: 0.630360, acc: 65.62%] [G loss: 1.986812]\n",
      "epoch:14 step:13622 [D loss: 0.594608, acc: 74.22%] [G loss: 2.171926]\n",
      "epoch:14 step:13623 [D loss: 0.691569, acc: 55.47%] [G loss: 1.976037]\n",
      "epoch:14 step:13624 [D loss: 0.646730, acc: 64.06%] [G loss: 1.736426]\n",
      "epoch:14 step:13625 [D loss: 0.624877, acc: 67.97%] [G loss: 1.870973]\n",
      "epoch:14 step:13626 [D loss: 0.683137, acc: 58.59%] [G loss: 1.985673]\n",
      "epoch:14 step:13627 [D loss: 0.648552, acc: 60.16%] [G loss: 1.969664]\n",
      "epoch:14 step:13628 [D loss: 0.726290, acc: 50.78%] [G loss: 1.702678]\n",
      "epoch:14 step:13629 [D loss: 0.723964, acc: 55.47%] [G loss: 1.753778]\n",
      "epoch:14 step:13630 [D loss: 0.664277, acc: 57.81%] [G loss: 1.796651]\n",
      "epoch:14 step:13631 [D loss: 0.623209, acc: 68.75%] [G loss: 1.842429]\n",
      "epoch:14 step:13632 [D loss: 0.604707, acc: 70.31%] [G loss: 1.949774]\n",
      "epoch:14 step:13633 [D loss: 0.658723, acc: 60.94%] [G loss: 2.023250]\n",
      "epoch:14 step:13634 [D loss: 0.609477, acc: 68.75%] [G loss: 1.985186]\n",
      "epoch:14 step:13635 [D loss: 0.642908, acc: 64.06%] [G loss: 1.967093]\n",
      "epoch:14 step:13636 [D loss: 0.629057, acc: 66.41%] [G loss: 1.808267]\n",
      "epoch:14 step:13637 [D loss: 0.614116, acc: 71.88%] [G loss: 1.929891]\n",
      "epoch:14 step:13638 [D loss: 0.617183, acc: 64.84%] [G loss: 1.974856]\n",
      "epoch:14 step:13639 [D loss: 0.626874, acc: 65.62%] [G loss: 1.909787]\n",
      "epoch:14 step:13640 [D loss: 0.619433, acc: 62.50%] [G loss: 2.028425]\n",
      "epoch:14 step:13641 [D loss: 0.608900, acc: 67.19%] [G loss: 2.049402]\n",
      "epoch:14 step:13642 [D loss: 0.607657, acc: 67.19%] [G loss: 1.949635]\n",
      "epoch:14 step:13643 [D loss: 0.629441, acc: 64.06%] [G loss: 1.844579]\n",
      "epoch:14 step:13644 [D loss: 0.654637, acc: 57.81%] [G loss: 1.929147]\n",
      "epoch:14 step:13645 [D loss: 0.640089, acc: 64.06%] [G loss: 1.924743]\n",
      "epoch:14 step:13646 [D loss: 0.707849, acc: 56.25%] [G loss: 1.823166]\n",
      "epoch:14 step:13647 [D loss: 0.692009, acc: 57.03%] [G loss: 1.769800]\n",
      "epoch:14 step:13648 [D loss: 0.669018, acc: 55.47%] [G loss: 1.840141]\n",
      "epoch:14 step:13649 [D loss: 0.636378, acc: 67.97%] [G loss: 1.730514]\n",
      "epoch:14 step:13650 [D loss: 0.623426, acc: 65.62%] [G loss: 1.998188]\n",
      "epoch:14 step:13651 [D loss: 0.634742, acc: 65.62%] [G loss: 1.951273]\n",
      "epoch:14 step:13652 [D loss: 0.633742, acc: 60.16%] [G loss: 2.048185]\n",
      "epoch:14 step:13653 [D loss: 0.627579, acc: 67.19%] [G loss: 2.029799]\n",
      "epoch:14 step:13654 [D loss: 0.637606, acc: 67.97%] [G loss: 1.946436]\n",
      "epoch:14 step:13655 [D loss: 0.653412, acc: 64.84%] [G loss: 1.894693]\n",
      "epoch:14 step:13656 [D loss: 0.652818, acc: 63.28%] [G loss: 1.748915]\n",
      "epoch:14 step:13657 [D loss: 0.645996, acc: 57.81%] [G loss: 1.776972]\n",
      "epoch:14 step:13658 [D loss: 0.660056, acc: 56.25%] [G loss: 1.945143]\n",
      "epoch:14 step:13659 [D loss: 0.640593, acc: 57.81%] [G loss: 1.781109]\n",
      "epoch:14 step:13660 [D loss: 0.649030, acc: 64.06%] [G loss: 1.821786]\n",
      "epoch:14 step:13661 [D loss: 0.666142, acc: 60.94%] [G loss: 1.837343]\n",
      "epoch:14 step:13662 [D loss: 0.618674, acc: 65.62%] [G loss: 1.976801]\n",
      "epoch:14 step:13663 [D loss: 0.598895, acc: 64.06%] [G loss: 2.033342]\n",
      "epoch:14 step:13664 [D loss: 0.641598, acc: 60.94%] [G loss: 1.982088]\n",
      "epoch:14 step:13665 [D loss: 0.638815, acc: 64.06%] [G loss: 1.971158]\n",
      "epoch:14 step:13666 [D loss: 0.618835, acc: 67.19%] [G loss: 1.936910]\n",
      "epoch:14 step:13667 [D loss: 0.614297, acc: 66.41%] [G loss: 1.989412]\n",
      "epoch:14 step:13668 [D loss: 0.639060, acc: 67.19%] [G loss: 1.952737]\n",
      "epoch:14 step:13669 [D loss: 0.605023, acc: 67.97%] [G loss: 1.953203]\n",
      "epoch:14 step:13670 [D loss: 0.618596, acc: 65.62%] [G loss: 2.024446]\n",
      "epoch:14 step:13671 [D loss: 0.658473, acc: 59.38%] [G loss: 1.907752]\n",
      "epoch:14 step:13672 [D loss: 0.634642, acc: 64.06%] [G loss: 2.265954]\n",
      "epoch:14 step:13673 [D loss: 0.617429, acc: 66.41%] [G loss: 2.011398]\n",
      "epoch:14 step:13674 [D loss: 0.636613, acc: 62.50%] [G loss: 1.966899]\n",
      "epoch:14 step:13675 [D loss: 0.623190, acc: 64.06%] [G loss: 2.006863]\n",
      "epoch:14 step:13676 [D loss: 0.638021, acc: 64.84%] [G loss: 2.092331]\n",
      "epoch:14 step:13677 [D loss: 0.685040, acc: 57.81%] [G loss: 1.834972]\n",
      "epoch:14 step:13678 [D loss: 0.718818, acc: 55.47%] [G loss: 1.781280]\n",
      "epoch:14 step:13679 [D loss: 0.652290, acc: 62.50%] [G loss: 1.840666]\n",
      "epoch:14 step:13680 [D loss: 0.640465, acc: 61.72%] [G loss: 1.849453]\n",
      "epoch:14 step:13681 [D loss: 0.624680, acc: 63.28%] [G loss: 1.926363]\n",
      "epoch:14 step:13682 [D loss: 0.562367, acc: 76.56%] [G loss: 2.152586]\n",
      "epoch:14 step:13683 [D loss: 0.636649, acc: 65.62%] [G loss: 1.948386]\n",
      "epoch:14 step:13684 [D loss: 0.734004, acc: 50.78%] [G loss: 1.764035]\n",
      "epoch:14 step:13685 [D loss: 0.709681, acc: 53.12%] [G loss: 1.804223]\n",
      "epoch:14 step:13686 [D loss: 0.632871, acc: 61.72%] [G loss: 1.911572]\n",
      "epoch:14 step:13687 [D loss: 0.630623, acc: 67.19%] [G loss: 1.835170]\n",
      "epoch:14 step:13688 [D loss: 0.671634, acc: 56.25%] [G loss: 1.815842]\n",
      "epoch:14 step:13689 [D loss: 0.637367, acc: 63.28%] [G loss: 1.907712]\n",
      "epoch:14 step:13690 [D loss: 0.657286, acc: 62.50%] [G loss: 1.782568]\n",
      "epoch:14 step:13691 [D loss: 0.651796, acc: 63.28%] [G loss: 1.955333]\n",
      "epoch:14 step:13692 [D loss: 0.648601, acc: 59.38%] [G loss: 1.900655]\n",
      "epoch:14 step:13693 [D loss: 0.627260, acc: 64.06%] [G loss: 1.872918]\n",
      "epoch:14 step:13694 [D loss: 0.621502, acc: 67.19%] [G loss: 1.811800]\n",
      "epoch:14 step:13695 [D loss: 0.689605, acc: 53.91%] [G loss: 1.744214]\n",
      "epoch:14 step:13696 [D loss: 0.643722, acc: 64.06%] [G loss: 1.799038]\n",
      "epoch:14 step:13697 [D loss: 0.662652, acc: 57.81%] [G loss: 1.711809]\n",
      "epoch:14 step:13698 [D loss: 0.657730, acc: 59.38%] [G loss: 1.892225]\n",
      "epoch:14 step:13699 [D loss: 0.611490, acc: 64.06%] [G loss: 1.810531]\n",
      "epoch:14 step:13700 [D loss: 0.549556, acc: 75.78%] [G loss: 1.840816]\n",
      "epoch:14 step:13701 [D loss: 0.696631, acc: 57.81%] [G loss: 1.806449]\n",
      "epoch:14 step:13702 [D loss: 0.630744, acc: 62.50%] [G loss: 1.818951]\n",
      "epoch:14 step:13703 [D loss: 0.626864, acc: 60.16%] [G loss: 1.942652]\n",
      "epoch:14 step:13704 [D loss: 0.646080, acc: 60.16%] [G loss: 1.954760]\n",
      "epoch:14 step:13705 [D loss: 0.672256, acc: 58.59%] [G loss: 1.905756]\n",
      "epoch:14 step:13706 [D loss: 0.621249, acc: 67.19%] [G loss: 2.033217]\n",
      "epoch:14 step:13707 [D loss: 0.604025, acc: 69.53%] [G loss: 2.098679]\n",
      "epoch:14 step:13708 [D loss: 0.641988, acc: 59.38%] [G loss: 1.962990]\n",
      "epoch:14 step:13709 [D loss: 0.717583, acc: 49.22%] [G loss: 1.951110]\n",
      "epoch:14 step:13710 [D loss: 0.625047, acc: 67.19%] [G loss: 1.793277]\n",
      "epoch:14 step:13711 [D loss: 0.631757, acc: 66.41%] [G loss: 1.996838]\n",
      "epoch:14 step:13712 [D loss: 0.708116, acc: 57.03%] [G loss: 1.827861]\n",
      "epoch:14 step:13713 [D loss: 0.674238, acc: 61.72%] [G loss: 1.777959]\n",
      "epoch:14 step:13714 [D loss: 0.668497, acc: 59.38%] [G loss: 1.993181]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:14 step:13715 [D loss: 0.696117, acc: 60.94%] [G loss: 1.839091]\n",
      "epoch:14 step:13716 [D loss: 0.618236, acc: 64.06%] [G loss: 2.008341]\n",
      "epoch:14 step:13717 [D loss: 0.702357, acc: 57.81%] [G loss: 1.865713]\n",
      "epoch:14 step:13718 [D loss: 0.650968, acc: 55.47%] [G loss: 1.914999]\n",
      "epoch:14 step:13719 [D loss: 0.652528, acc: 64.06%] [G loss: 1.978469]\n",
      "epoch:14 step:13720 [D loss: 0.665713, acc: 57.81%] [G loss: 1.949319]\n",
      "epoch:14 step:13721 [D loss: 0.643305, acc: 65.62%] [G loss: 2.069837]\n",
      "epoch:14 step:13722 [D loss: 0.678198, acc: 60.16%] [G loss: 1.869767]\n",
      "epoch:14 step:13723 [D loss: 0.635593, acc: 61.72%] [G loss: 1.872322]\n",
      "epoch:14 step:13724 [D loss: 0.635196, acc: 63.28%] [G loss: 1.916523]\n",
      "epoch:14 step:13725 [D loss: 0.638832, acc: 63.28%] [G loss: 1.903232]\n",
      "epoch:14 step:13726 [D loss: 0.627947, acc: 64.06%] [G loss: 1.763411]\n",
      "epoch:14 step:13727 [D loss: 0.586545, acc: 68.75%] [G loss: 1.922434]\n",
      "epoch:14 step:13728 [D loss: 0.669838, acc: 57.81%] [G loss: 1.857226]\n",
      "epoch:14 step:13729 [D loss: 0.670910, acc: 62.50%] [G loss: 1.809729]\n",
      "epoch:14 step:13730 [D loss: 0.663522, acc: 60.94%] [G loss: 1.893516]\n",
      "epoch:14 step:13731 [D loss: 0.642955, acc: 65.62%] [G loss: 1.997321]\n",
      "epoch:14 step:13732 [D loss: 0.729237, acc: 55.47%] [G loss: 1.884256]\n",
      "epoch:14 step:13733 [D loss: 0.702459, acc: 55.47%] [G loss: 1.786788]\n",
      "epoch:14 step:13734 [D loss: 0.677324, acc: 62.50%] [G loss: 1.930597]\n",
      "epoch:14 step:13735 [D loss: 0.686277, acc: 57.03%] [G loss: 1.676152]\n",
      "epoch:14 step:13736 [D loss: 0.599060, acc: 67.19%] [G loss: 1.837410]\n",
      "epoch:14 step:13737 [D loss: 0.670353, acc: 63.28%] [G loss: 1.811429]\n",
      "epoch:14 step:13738 [D loss: 0.653144, acc: 59.38%] [G loss: 1.890079]\n",
      "epoch:14 step:13739 [D loss: 0.649758, acc: 64.84%] [G loss: 1.711851]\n",
      "epoch:14 step:13740 [D loss: 0.646790, acc: 60.94%] [G loss: 1.974726]\n",
      "epoch:14 step:13741 [D loss: 0.628986, acc: 69.53%] [G loss: 1.987075]\n",
      "epoch:14 step:13742 [D loss: 0.636562, acc: 66.41%] [G loss: 2.059724]\n",
      "epoch:14 step:13743 [D loss: 0.652362, acc: 58.59%] [G loss: 1.880230]\n",
      "epoch:14 step:13744 [D loss: 0.624577, acc: 65.62%] [G loss: 1.859461]\n",
      "epoch:14 step:13745 [D loss: 0.664870, acc: 60.94%] [G loss: 1.836100]\n",
      "epoch:14 step:13746 [D loss: 0.617029, acc: 66.41%] [G loss: 1.827539]\n",
      "epoch:14 step:13747 [D loss: 0.641492, acc: 60.94%] [G loss: 2.026552]\n",
      "epoch:14 step:13748 [D loss: 0.628198, acc: 63.28%] [G loss: 1.886605]\n",
      "epoch:14 step:13749 [D loss: 0.619715, acc: 67.19%] [G loss: 1.948404]\n",
      "epoch:14 step:13750 [D loss: 0.678484, acc: 60.94%] [G loss: 1.957020]\n",
      "epoch:14 step:13751 [D loss: 0.627380, acc: 60.94%] [G loss: 2.056721]\n",
      "epoch:14 step:13752 [D loss: 0.544007, acc: 77.34%] [G loss: 1.954681]\n",
      "epoch:14 step:13753 [D loss: 0.629218, acc: 67.97%] [G loss: 1.999962]\n",
      "epoch:14 step:13754 [D loss: 0.579140, acc: 69.53%] [G loss: 1.998425]\n",
      "epoch:14 step:13755 [D loss: 0.589011, acc: 69.53%] [G loss: 2.190306]\n",
      "epoch:14 step:13756 [D loss: 0.583464, acc: 69.53%] [G loss: 2.128049]\n",
      "epoch:14 step:13757 [D loss: 0.680043, acc: 58.59%] [G loss: 1.928496]\n",
      "epoch:14 step:13758 [D loss: 0.668449, acc: 64.84%] [G loss: 1.871621]\n",
      "epoch:14 step:13759 [D loss: 0.614306, acc: 69.53%] [G loss: 1.999477]\n",
      "epoch:14 step:13760 [D loss: 0.588756, acc: 69.53%] [G loss: 2.070538]\n",
      "epoch:14 step:13761 [D loss: 0.654448, acc: 64.06%] [G loss: 2.034174]\n",
      "epoch:14 step:13762 [D loss: 0.604466, acc: 65.62%] [G loss: 2.142360]\n",
      "epoch:14 step:13763 [D loss: 0.648594, acc: 60.16%] [G loss: 2.067060]\n",
      "epoch:14 step:13764 [D loss: 0.593589, acc: 70.31%] [G loss: 1.959443]\n",
      "epoch:14 step:13765 [D loss: 0.580608, acc: 71.88%] [G loss: 2.102477]\n",
      "epoch:14 step:13766 [D loss: 0.606907, acc: 65.62%] [G loss: 2.367690]\n",
      "epoch:14 step:13767 [D loss: 0.613652, acc: 68.75%] [G loss: 2.043506]\n",
      "epoch:14 step:13768 [D loss: 0.579193, acc: 67.97%] [G loss: 2.034972]\n",
      "epoch:14 step:13769 [D loss: 0.649487, acc: 60.16%] [G loss: 2.073311]\n",
      "epoch:14 step:13770 [D loss: 0.621599, acc: 66.41%] [G loss: 2.043337]\n",
      "epoch:14 step:13771 [D loss: 0.620162, acc: 65.62%] [G loss: 2.095377]\n",
      "epoch:14 step:13772 [D loss: 0.611432, acc: 66.41%] [G loss: 2.185989]\n",
      "epoch:14 step:13773 [D loss: 0.673439, acc: 61.72%] [G loss: 2.044319]\n",
      "epoch:14 step:13774 [D loss: 0.650515, acc: 60.94%] [G loss: 1.830471]\n",
      "epoch:14 step:13775 [D loss: 0.625633, acc: 64.84%] [G loss: 1.855294]\n",
      "epoch:14 step:13776 [D loss: 0.632938, acc: 63.28%] [G loss: 1.859551]\n",
      "epoch:14 step:13777 [D loss: 0.705504, acc: 50.00%] [G loss: 1.916852]\n",
      "epoch:14 step:13778 [D loss: 0.623287, acc: 63.28%] [G loss: 1.812509]\n",
      "epoch:14 step:13779 [D loss: 0.603161, acc: 70.31%] [G loss: 1.856977]\n",
      "epoch:14 step:13780 [D loss: 0.630489, acc: 66.41%] [G loss: 1.955140]\n",
      "epoch:14 step:13781 [D loss: 0.655642, acc: 60.94%] [G loss: 1.952704]\n",
      "epoch:14 step:13782 [D loss: 0.611724, acc: 69.53%] [G loss: 1.993257]\n",
      "epoch:14 step:13783 [D loss: 0.662837, acc: 57.03%] [G loss: 2.116318]\n",
      "epoch:14 step:13784 [D loss: 0.648477, acc: 64.06%] [G loss: 1.853936]\n",
      "epoch:14 step:13785 [D loss: 0.636308, acc: 64.06%] [G loss: 1.882753]\n",
      "epoch:14 step:13786 [D loss: 0.640032, acc: 64.84%] [G loss: 1.964046]\n",
      "epoch:14 step:13787 [D loss: 0.698295, acc: 54.69%] [G loss: 1.808200]\n",
      "epoch:14 step:13788 [D loss: 0.718263, acc: 56.25%] [G loss: 1.772281]\n",
      "epoch:14 step:13789 [D loss: 0.620853, acc: 67.19%] [G loss: 1.906678]\n",
      "epoch:14 step:13790 [D loss: 0.728301, acc: 51.56%] [G loss: 1.876027]\n",
      "epoch:14 step:13791 [D loss: 0.705959, acc: 56.25%] [G loss: 1.839654]\n",
      "epoch:14 step:13792 [D loss: 0.634969, acc: 67.97%] [G loss: 1.797883]\n",
      "epoch:14 step:13793 [D loss: 0.626915, acc: 64.84%] [G loss: 1.890181]\n",
      "epoch:14 step:13794 [D loss: 0.615779, acc: 67.19%] [G loss: 1.881523]\n",
      "epoch:14 step:13795 [D loss: 0.605733, acc: 66.41%] [G loss: 2.064813]\n",
      "epoch:14 step:13796 [D loss: 0.692037, acc: 56.25%] [G loss: 1.805109]\n",
      "epoch:14 step:13797 [D loss: 0.646866, acc: 61.72%] [G loss: 1.939987]\n",
      "epoch:14 step:13798 [D loss: 0.627693, acc: 65.62%] [G loss: 2.035390]\n",
      "epoch:14 step:13799 [D loss: 0.584993, acc: 71.88%] [G loss: 1.989978]\n",
      "epoch:14 step:13800 [D loss: 0.660968, acc: 60.94%] [G loss: 1.995712]\n",
      "##############\n",
      "[2.44276058 1.31196119 6.39140017 4.89847433 3.67070504 5.73085857\n",
      " 4.46443929 4.85439622 4.72231742 3.50520641]\n",
      "##########\n",
      "epoch:14 step:13801 [D loss: 0.627324, acc: 62.50%] [G loss: 1.826777]\n",
      "epoch:14 step:13802 [D loss: 0.630560, acc: 65.62%] [G loss: 1.947491]\n",
      "epoch:14 step:13803 [D loss: 0.635947, acc: 64.84%] [G loss: 2.014720]\n",
      "epoch:14 step:13804 [D loss: 0.636046, acc: 64.06%] [G loss: 1.931832]\n",
      "epoch:14 step:13805 [D loss: 0.600712, acc: 68.75%] [G loss: 2.043244]\n",
      "epoch:14 step:13806 [D loss: 0.617069, acc: 67.19%] [G loss: 2.036929]\n",
      "epoch:14 step:13807 [D loss: 0.631018, acc: 67.19%] [G loss: 1.840140]\n",
      "epoch:14 step:13808 [D loss: 0.591333, acc: 73.44%] [G loss: 2.083615]\n",
      "epoch:14 step:13809 [D loss: 0.614141, acc: 66.41%] [G loss: 2.082942]\n",
      "epoch:14 step:13810 [D loss: 0.631959, acc: 66.41%] [G loss: 2.028045]\n",
      "epoch:14 step:13811 [D loss: 0.562959, acc: 71.88%] [G loss: 2.179349]\n",
      "epoch:14 step:13812 [D loss: 0.603584, acc: 62.50%] [G loss: 2.183550]\n",
      "epoch:14 step:13813 [D loss: 0.610219, acc: 62.50%] [G loss: 2.048877]\n",
      "epoch:14 step:13814 [D loss: 0.650807, acc: 62.50%] [G loss: 1.951002]\n",
      "epoch:14 step:13815 [D loss: 0.607626, acc: 67.19%] [G loss: 2.071085]\n",
      "epoch:14 step:13816 [D loss: 0.594746, acc: 67.19%] [G loss: 2.026558]\n",
      "epoch:14 step:13817 [D loss: 0.651575, acc: 60.94%] [G loss: 2.079358]\n",
      "epoch:14 step:13818 [D loss: 0.633739, acc: 61.72%] [G loss: 2.046251]\n",
      "epoch:14 step:13819 [D loss: 0.688487, acc: 58.59%] [G loss: 2.023083]\n",
      "epoch:14 step:13820 [D loss: 0.696636, acc: 54.69%] [G loss: 1.857044]\n",
      "epoch:14 step:13821 [D loss: 0.679794, acc: 50.78%] [G loss: 1.906163]\n",
      "epoch:14 step:13822 [D loss: 0.667553, acc: 59.38%] [G loss: 1.947938]\n",
      "epoch:14 step:13823 [D loss: 0.650638, acc: 66.41%] [G loss: 1.848934]\n",
      "epoch:14 step:13824 [D loss: 0.622750, acc: 63.28%] [G loss: 1.873950]\n",
      "epoch:14 step:13825 [D loss: 0.635896, acc: 60.16%] [G loss: 2.002729]\n",
      "epoch:14 step:13826 [D loss: 0.583089, acc: 67.97%] [G loss: 2.259204]\n",
      "epoch:14 step:13827 [D loss: 0.582056, acc: 71.09%] [G loss: 2.035334]\n",
      "epoch:14 step:13828 [D loss: 0.676731, acc: 64.06%] [G loss: 1.872413]\n",
      "epoch:14 step:13829 [D loss: 0.638112, acc: 63.28%] [G loss: 2.001187]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:14 step:13830 [D loss: 0.616418, acc: 67.97%] [G loss: 2.063368]\n",
      "epoch:14 step:13831 [D loss: 0.636807, acc: 66.41%] [G loss: 1.894174]\n",
      "epoch:14 step:13832 [D loss: 0.582293, acc: 73.44%] [G loss: 1.963516]\n",
      "epoch:14 step:13833 [D loss: 0.633200, acc: 60.94%] [G loss: 1.855657]\n",
      "epoch:14 step:13834 [D loss: 0.711874, acc: 53.91%] [G loss: 1.729996]\n",
      "epoch:14 step:13835 [D loss: 0.622941, acc: 70.31%] [G loss: 1.945389]\n",
      "epoch:14 step:13836 [D loss: 0.633827, acc: 62.50%] [G loss: 2.136670]\n",
      "epoch:14 step:13837 [D loss: 0.584195, acc: 71.09%] [G loss: 2.197342]\n",
      "epoch:14 step:13838 [D loss: 0.632069, acc: 63.28%] [G loss: 2.094368]\n",
      "epoch:14 step:13839 [D loss: 0.625824, acc: 67.97%] [G loss: 2.055964]\n",
      "epoch:14 step:13840 [D loss: 0.633787, acc: 61.72%] [G loss: 1.911359]\n",
      "epoch:14 step:13841 [D loss: 0.656986, acc: 59.38%] [G loss: 1.937231]\n",
      "epoch:14 step:13842 [D loss: 0.642461, acc: 63.28%] [G loss: 1.881632]\n",
      "epoch:14 step:13843 [D loss: 0.654135, acc: 63.28%] [G loss: 2.005002]\n",
      "epoch:14 step:13844 [D loss: 0.703949, acc: 53.12%] [G loss: 2.111742]\n",
      "epoch:14 step:13845 [D loss: 0.678582, acc: 59.38%] [G loss: 1.990809]\n",
      "epoch:14 step:13846 [D loss: 0.628150, acc: 65.62%] [G loss: 1.946747]\n",
      "epoch:14 step:13847 [D loss: 0.652979, acc: 57.81%] [G loss: 1.824022]\n",
      "epoch:14 step:13848 [D loss: 0.618285, acc: 67.19%] [G loss: 1.942726]\n",
      "epoch:14 step:13849 [D loss: 0.653922, acc: 57.03%] [G loss: 1.832106]\n",
      "epoch:14 step:13850 [D loss: 0.631317, acc: 60.16%] [G loss: 1.724916]\n",
      "epoch:14 step:13851 [D loss: 0.659804, acc: 58.59%] [G loss: 1.891770]\n",
      "epoch:14 step:13852 [D loss: 0.629646, acc: 69.53%] [G loss: 1.929638]\n",
      "epoch:14 step:13853 [D loss: 0.603380, acc: 61.72%] [G loss: 1.864610]\n",
      "epoch:14 step:13854 [D loss: 0.595139, acc: 68.75%] [G loss: 1.971451]\n",
      "epoch:14 step:13855 [D loss: 0.631436, acc: 63.28%] [G loss: 1.768635]\n",
      "epoch:14 step:13856 [D loss: 0.638178, acc: 64.06%] [G loss: 1.883273]\n",
      "epoch:14 step:13857 [D loss: 0.653596, acc: 66.41%] [G loss: 1.985869]\n",
      "epoch:14 step:13858 [D loss: 0.671263, acc: 60.94%] [G loss: 1.956431]\n",
      "epoch:14 step:13859 [D loss: 0.650624, acc: 60.16%] [G loss: 1.936812]\n",
      "epoch:14 step:13860 [D loss: 0.625539, acc: 64.84%] [G loss: 2.005368]\n",
      "epoch:14 step:13861 [D loss: 0.697743, acc: 56.25%] [G loss: 1.923892]\n",
      "epoch:14 step:13862 [D loss: 0.698808, acc: 53.12%] [G loss: 1.925910]\n",
      "epoch:14 step:13863 [D loss: 0.686372, acc: 54.69%] [G loss: 1.942873]\n",
      "epoch:14 step:13864 [D loss: 0.576802, acc: 75.00%] [G loss: 1.989468]\n",
      "epoch:14 step:13865 [D loss: 0.598381, acc: 65.62%] [G loss: 2.078366]\n",
      "epoch:14 step:13866 [D loss: 0.618986, acc: 65.62%] [G loss: 1.879645]\n",
      "epoch:14 step:13867 [D loss: 0.616972, acc: 65.62%] [G loss: 1.846502]\n",
      "epoch:14 step:13868 [D loss: 0.640683, acc: 62.50%] [G loss: 1.972561]\n",
      "epoch:14 step:13869 [D loss: 0.705979, acc: 59.38%] [G loss: 1.854243]\n",
      "epoch:14 step:13870 [D loss: 0.664563, acc: 58.59%] [G loss: 1.913118]\n",
      "epoch:14 step:13871 [D loss: 0.643360, acc: 57.81%] [G loss: 1.960029]\n",
      "epoch:14 step:13872 [D loss: 0.626289, acc: 66.41%] [G loss: 1.945860]\n",
      "epoch:14 step:13873 [D loss: 0.652349, acc: 62.50%] [G loss: 1.881919]\n",
      "epoch:14 step:13874 [D loss: 0.642240, acc: 60.16%] [G loss: 1.940092]\n",
      "epoch:14 step:13875 [D loss: 0.710642, acc: 55.47%] [G loss: 2.011662]\n",
      "epoch:14 step:13876 [D loss: 0.641272, acc: 61.72%] [G loss: 1.910616]\n",
      "epoch:14 step:13877 [D loss: 0.588284, acc: 70.31%] [G loss: 2.009016]\n",
      "epoch:14 step:13878 [D loss: 0.646133, acc: 60.94%] [G loss: 2.026335]\n",
      "epoch:14 step:13879 [D loss: 0.668462, acc: 57.81%] [G loss: 2.070978]\n",
      "epoch:14 step:13880 [D loss: 0.640988, acc: 62.50%] [G loss: 1.959457]\n",
      "epoch:14 step:13881 [D loss: 0.618079, acc: 61.72%] [G loss: 1.856488]\n",
      "epoch:14 step:13882 [D loss: 0.730043, acc: 54.69%] [G loss: 1.944146]\n",
      "epoch:14 step:13883 [D loss: 0.725010, acc: 53.91%] [G loss: 1.814794]\n",
      "epoch:14 step:13884 [D loss: 0.634673, acc: 64.06%] [G loss: 1.974085]\n",
      "epoch:14 step:13885 [D loss: 0.690286, acc: 60.16%] [G loss: 1.906512]\n",
      "epoch:14 step:13886 [D loss: 0.654429, acc: 61.72%] [G loss: 1.955838]\n",
      "epoch:14 step:13887 [D loss: 0.624265, acc: 65.62%] [G loss: 1.997856]\n",
      "epoch:14 step:13888 [D loss: 0.663404, acc: 66.41%] [G loss: 1.935423]\n",
      "epoch:14 step:13889 [D loss: 0.598541, acc: 71.09%] [G loss: 2.033568]\n",
      "epoch:14 step:13890 [D loss: 0.694454, acc: 59.38%] [G loss: 1.923866]\n",
      "epoch:14 step:13891 [D loss: 0.646730, acc: 59.38%] [G loss: 1.893957]\n",
      "epoch:14 step:13892 [D loss: 0.597026, acc: 70.31%] [G loss: 1.989791]\n",
      "epoch:14 step:13893 [D loss: 0.537043, acc: 79.69%] [G loss: 2.254708]\n",
      "epoch:14 step:13894 [D loss: 0.595015, acc: 67.97%] [G loss: 1.969795]\n",
      "epoch:14 step:13895 [D loss: 0.648224, acc: 58.59%] [G loss: 2.001193]\n",
      "epoch:14 step:13896 [D loss: 0.663929, acc: 57.81%] [G loss: 2.051610]\n",
      "epoch:14 step:13897 [D loss: 0.676070, acc: 64.06%] [G loss: 1.972181]\n",
      "epoch:14 step:13898 [D loss: 0.600898, acc: 69.53%] [G loss: 2.104472]\n",
      "epoch:14 step:13899 [D loss: 0.574762, acc: 71.88%] [G loss: 2.083822]\n",
      "epoch:14 step:13900 [D loss: 0.645562, acc: 60.94%] [G loss: 2.025333]\n",
      "epoch:14 step:13901 [D loss: 0.648766, acc: 59.38%] [G loss: 1.993052]\n",
      "epoch:14 step:13902 [D loss: 0.668749, acc: 58.59%] [G loss: 1.795125]\n",
      "epoch:14 step:13903 [D loss: 0.668727, acc: 53.12%] [G loss: 1.823958]\n",
      "epoch:14 step:13904 [D loss: 0.600403, acc: 74.22%] [G loss: 1.954340]\n",
      "epoch:14 step:13905 [D loss: 0.640130, acc: 65.62%] [G loss: 1.924479]\n",
      "epoch:14 step:13906 [D loss: 0.657780, acc: 57.03%] [G loss: 1.884318]\n",
      "epoch:14 step:13907 [D loss: 0.633939, acc: 64.84%] [G loss: 2.130176]\n",
      "epoch:14 step:13908 [D loss: 0.667601, acc: 58.59%] [G loss: 2.028993]\n",
      "epoch:14 step:13909 [D loss: 0.631567, acc: 62.50%] [G loss: 1.924538]\n",
      "epoch:14 step:13910 [D loss: 0.665029, acc: 57.03%] [G loss: 2.108406]\n",
      "epoch:14 step:13911 [D loss: 0.640922, acc: 60.94%] [G loss: 1.973185]\n",
      "epoch:14 step:13912 [D loss: 0.728802, acc: 51.56%] [G loss: 1.773815]\n",
      "epoch:14 step:13913 [D loss: 0.655906, acc: 62.50%] [G loss: 1.864340]\n",
      "epoch:14 step:13914 [D loss: 0.600792, acc: 70.31%] [G loss: 1.906595]\n",
      "epoch:14 step:13915 [D loss: 0.654513, acc: 64.06%] [G loss: 1.827263]\n",
      "epoch:14 step:13916 [D loss: 0.642747, acc: 59.38%] [G loss: 1.881365]\n",
      "epoch:14 step:13917 [D loss: 0.632319, acc: 65.62%] [G loss: 1.894993]\n",
      "epoch:14 step:13918 [D loss: 0.724597, acc: 50.00%] [G loss: 1.737404]\n",
      "epoch:14 step:13919 [D loss: 0.667672, acc: 60.94%] [G loss: 1.882630]\n",
      "epoch:14 step:13920 [D loss: 0.637789, acc: 61.72%] [G loss: 1.841106]\n",
      "epoch:14 step:13921 [D loss: 0.614697, acc: 66.41%] [G loss: 1.869392]\n",
      "epoch:14 step:13922 [D loss: 0.600371, acc: 70.31%] [G loss: 1.952958]\n",
      "epoch:14 step:13923 [D loss: 0.595408, acc: 64.06%] [G loss: 2.011340]\n",
      "epoch:14 step:13924 [D loss: 0.690794, acc: 55.47%] [G loss: 1.971040]\n",
      "epoch:14 step:13925 [D loss: 0.645886, acc: 59.38%] [G loss: 1.828038]\n",
      "epoch:14 step:13926 [D loss: 0.630845, acc: 63.28%] [G loss: 2.038865]\n",
      "epoch:14 step:13927 [D loss: 0.620811, acc: 67.19%] [G loss: 1.800691]\n",
      "epoch:14 step:13928 [D loss: 0.648775, acc: 62.50%] [G loss: 1.820612]\n",
      "epoch:14 step:13929 [D loss: 0.641680, acc: 63.28%] [G loss: 1.932232]\n",
      "epoch:14 step:13930 [D loss: 0.683554, acc: 56.25%] [G loss: 1.944684]\n",
      "epoch:14 step:13931 [D loss: 0.630031, acc: 66.41%] [G loss: 1.927006]\n",
      "epoch:14 step:13932 [D loss: 0.665347, acc: 54.69%] [G loss: 2.020267]\n",
      "epoch:14 step:13933 [D loss: 0.557032, acc: 71.88%] [G loss: 2.246512]\n",
      "epoch:14 step:13934 [D loss: 0.656357, acc: 60.94%] [G loss: 1.957429]\n",
      "epoch:14 step:13935 [D loss: 0.616248, acc: 67.97%] [G loss: 1.892716]\n",
      "epoch:14 step:13936 [D loss: 0.679363, acc: 60.94%] [G loss: 1.748104]\n",
      "epoch:14 step:13937 [D loss: 0.659700, acc: 60.94%] [G loss: 1.873935]\n",
      "epoch:14 step:13938 [D loss: 0.680853, acc: 53.12%] [G loss: 1.869504]\n",
      "epoch:14 step:13939 [D loss: 0.637693, acc: 59.38%] [G loss: 1.841697]\n",
      "epoch:14 step:13940 [D loss: 0.644655, acc: 60.94%] [G loss: 1.968191]\n",
      "epoch:14 step:13941 [D loss: 0.590123, acc: 71.88%] [G loss: 1.846662]\n",
      "epoch:14 step:13942 [D loss: 0.683850, acc: 53.12%] [G loss: 1.840591]\n",
      "epoch:14 step:13943 [D loss: 0.656759, acc: 58.59%] [G loss: 2.003293]\n",
      "epoch:14 step:13944 [D loss: 0.640976, acc: 57.03%] [G loss: 1.827698]\n",
      "epoch:14 step:13945 [D loss: 0.664237, acc: 55.47%] [G loss: 1.802471]\n",
      "epoch:14 step:13946 [D loss: 0.726030, acc: 54.69%] [G loss: 1.714118]\n",
      "epoch:14 step:13947 [D loss: 0.693618, acc: 56.25%] [G loss: 1.767462]\n",
      "epoch:14 step:13948 [D loss: 0.664988, acc: 59.38%] [G loss: 1.726541]\n",
      "epoch:14 step:13949 [D loss: 0.661976, acc: 57.81%] [G loss: 1.924588]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:14 step:13950 [D loss: 0.662219, acc: 64.06%] [G loss: 1.808731]\n",
      "epoch:14 step:13951 [D loss: 0.617149, acc: 71.88%] [G loss: 2.011691]\n",
      "epoch:14 step:13952 [D loss: 0.664822, acc: 58.59%] [G loss: 1.842320]\n",
      "epoch:14 step:13953 [D loss: 0.664507, acc: 55.47%] [G loss: 1.780446]\n",
      "epoch:14 step:13954 [D loss: 0.634076, acc: 64.84%] [G loss: 1.915241]\n",
      "epoch:14 step:13955 [D loss: 0.580890, acc: 69.53%] [G loss: 1.987502]\n",
      "epoch:14 step:13956 [D loss: 0.660590, acc: 59.38%] [G loss: 1.783201]\n",
      "epoch:14 step:13957 [D loss: 0.615076, acc: 65.62%] [G loss: 1.906081]\n",
      "epoch:14 step:13958 [D loss: 0.648124, acc: 58.59%] [G loss: 1.851713]\n",
      "epoch:14 step:13959 [D loss: 0.689336, acc: 60.16%] [G loss: 1.773083]\n",
      "epoch:14 step:13960 [D loss: 0.619726, acc: 67.97%] [G loss: 1.983470]\n",
      "epoch:14 step:13961 [D loss: 0.630592, acc: 63.28%] [G loss: 1.951009]\n",
      "epoch:14 step:13962 [D loss: 0.641208, acc: 67.97%] [G loss: 2.027520]\n",
      "epoch:14 step:13963 [D loss: 0.667186, acc: 58.59%] [G loss: 1.923187]\n",
      "epoch:14 step:13964 [D loss: 0.637584, acc: 60.94%] [G loss: 1.941684]\n",
      "epoch:14 step:13965 [D loss: 0.622483, acc: 65.62%] [G loss: 1.995091]\n",
      "epoch:14 step:13966 [D loss: 0.620993, acc: 62.50%] [G loss: 1.949047]\n",
      "epoch:14 step:13967 [D loss: 0.643749, acc: 60.94%] [G loss: 1.917534]\n",
      "epoch:14 step:13968 [D loss: 0.664004, acc: 62.50%] [G loss: 1.843376]\n",
      "epoch:14 step:13969 [D loss: 0.679221, acc: 63.28%] [G loss: 1.859010]\n",
      "epoch:14 step:13970 [D loss: 0.590859, acc: 71.09%] [G loss: 1.996368]\n",
      "epoch:14 step:13971 [D loss: 0.693117, acc: 57.03%] [G loss: 1.837109]\n",
      "epoch:14 step:13972 [D loss: 0.558085, acc: 73.44%] [G loss: 1.903658]\n",
      "epoch:14 step:13973 [D loss: 0.648312, acc: 58.59%] [G loss: 1.848281]\n",
      "epoch:14 step:13974 [D loss: 0.691511, acc: 54.69%] [G loss: 1.846244]\n",
      "epoch:14 step:13975 [D loss: 0.628376, acc: 68.75%] [G loss: 2.025122]\n",
      "epoch:14 step:13976 [D loss: 0.702229, acc: 57.03%] [G loss: 1.774444]\n",
      "epoch:14 step:13977 [D loss: 0.706539, acc: 53.91%] [G loss: 1.718191]\n",
      "epoch:14 step:13978 [D loss: 0.621850, acc: 62.50%] [G loss: 1.853951]\n",
      "epoch:14 step:13979 [D loss: 0.667074, acc: 58.59%] [G loss: 1.824100]\n",
      "epoch:14 step:13980 [D loss: 0.668012, acc: 56.25%] [G loss: 1.810203]\n",
      "epoch:14 step:13981 [D loss: 0.635723, acc: 60.94%] [G loss: 1.942666]\n",
      "epoch:14 step:13982 [D loss: 0.688728, acc: 60.16%] [G loss: 1.797697]\n",
      "epoch:14 step:13983 [D loss: 0.650382, acc: 61.72%] [G loss: 1.754782]\n",
      "epoch:14 step:13984 [D loss: 0.631663, acc: 64.06%] [G loss: 1.812151]\n",
      "epoch:14 step:13985 [D loss: 0.665210, acc: 55.47%] [G loss: 1.862519]\n",
      "epoch:14 step:13986 [D loss: 0.680439, acc: 58.59%] [G loss: 1.896576]\n",
      "epoch:14 step:13987 [D loss: 0.662971, acc: 60.16%] [G loss: 1.885032]\n",
      "epoch:14 step:13988 [D loss: 0.682030, acc: 57.03%] [G loss: 1.909851]\n",
      "epoch:14 step:13989 [D loss: 0.664770, acc: 60.16%] [G loss: 1.790137]\n",
      "epoch:14 step:13990 [D loss: 0.628767, acc: 61.72%] [G loss: 1.821075]\n",
      "epoch:14 step:13991 [D loss: 0.631501, acc: 67.19%] [G loss: 1.678063]\n",
      "epoch:14 step:13992 [D loss: 0.695657, acc: 51.56%] [G loss: 2.013429]\n",
      "epoch:14 step:13993 [D loss: 0.654586, acc: 61.72%] [G loss: 1.977644]\n",
      "epoch:14 step:13994 [D loss: 0.636068, acc: 57.81%] [G loss: 1.940015]\n",
      "epoch:14 step:13995 [D loss: 0.665785, acc: 53.91%] [G loss: 1.895138]\n",
      "epoch:14 step:13996 [D loss: 0.616533, acc: 64.84%] [G loss: 1.926448]\n",
      "epoch:14 step:13997 [D loss: 0.646166, acc: 62.50%] [G loss: 1.818684]\n",
      "epoch:14 step:13998 [D loss: 0.649095, acc: 59.38%] [G loss: 1.892134]\n",
      "epoch:14 step:13999 [D loss: 0.689651, acc: 58.59%] [G loss: 1.885357]\n",
      "epoch:14 step:14000 [D loss: 0.617088, acc: 70.31%] [G loss: 1.929198]\n",
      "##############\n",
      "[2.54995315 1.45189144 6.1642428  4.55524409 3.67110118 5.68763434\n",
      " 4.31320476 4.57859306 4.59490067 3.61006632]\n",
      "##########\n",
      "epoch:14 step:14001 [D loss: 0.716691, acc: 57.81%] [G loss: 1.955881]\n",
      "epoch:14 step:14002 [D loss: 0.647219, acc: 64.06%] [G loss: 2.032284]\n",
      "epoch:14 step:14003 [D loss: 0.672235, acc: 58.59%] [G loss: 1.867484]\n",
      "epoch:14 step:14004 [D loss: 0.606029, acc: 64.06%] [G loss: 1.888772]\n",
      "epoch:14 step:14005 [D loss: 0.588494, acc: 68.75%] [G loss: 1.998055]\n",
      "epoch:14 step:14006 [D loss: 0.651385, acc: 60.16%] [G loss: 1.856802]\n",
      "epoch:14 step:14007 [D loss: 0.603156, acc: 63.28%] [G loss: 1.930902]\n",
      "epoch:14 step:14008 [D loss: 0.705663, acc: 57.03%] [G loss: 2.013564]\n",
      "epoch:14 step:14009 [D loss: 0.633406, acc: 58.59%] [G loss: 1.892906]\n",
      "epoch:14 step:14010 [D loss: 0.698096, acc: 57.81%] [G loss: 1.805693]\n",
      "epoch:14 step:14011 [D loss: 0.648511, acc: 59.38%] [G loss: 2.090258]\n",
      "epoch:14 step:14012 [D loss: 0.595225, acc: 71.09%] [G loss: 1.910147]\n",
      "epoch:14 step:14013 [D loss: 0.699020, acc: 54.69%] [G loss: 1.877698]\n",
      "epoch:14 step:14014 [D loss: 0.704780, acc: 50.78%] [G loss: 1.847394]\n",
      "epoch:14 step:14015 [D loss: 0.594665, acc: 67.97%] [G loss: 1.919219]\n",
      "epoch:14 step:14016 [D loss: 0.682465, acc: 57.03%] [G loss: 1.769247]\n",
      "epoch:14 step:14017 [D loss: 0.606368, acc: 65.62%] [G loss: 1.911236]\n",
      "epoch:14 step:14018 [D loss: 0.615338, acc: 62.50%] [G loss: 1.877394]\n",
      "epoch:14 step:14019 [D loss: 0.612372, acc: 67.19%] [G loss: 1.969872]\n",
      "epoch:14 step:14020 [D loss: 0.634109, acc: 62.50%] [G loss: 1.864525]\n",
      "epoch:14 step:14021 [D loss: 0.633097, acc: 63.28%] [G loss: 1.952493]\n",
      "epoch:14 step:14022 [D loss: 0.605297, acc: 69.53%] [G loss: 1.859948]\n",
      "epoch:14 step:14023 [D loss: 0.608920, acc: 68.75%] [G loss: 2.006542]\n",
      "epoch:14 step:14024 [D loss: 0.669041, acc: 60.16%] [G loss: 2.037999]\n",
      "epoch:14 step:14025 [D loss: 0.608938, acc: 67.19%] [G loss: 1.882927]\n",
      "epoch:14 step:14026 [D loss: 0.657002, acc: 57.81%] [G loss: 2.115386]\n",
      "epoch:14 step:14027 [D loss: 0.568646, acc: 73.44%] [G loss: 2.208616]\n",
      "epoch:14 step:14028 [D loss: 0.638237, acc: 61.72%] [G loss: 1.970942]\n",
      "epoch:14 step:14029 [D loss: 0.622820, acc: 64.06%] [G loss: 2.141377]\n",
      "epoch:14 step:14030 [D loss: 0.616730, acc: 66.41%] [G loss: 2.372443]\n",
      "epoch:14 step:14031 [D loss: 0.657200, acc: 58.59%] [G loss: 2.134140]\n",
      "epoch:14 step:14032 [D loss: 0.664864, acc: 57.81%] [G loss: 2.149097]\n",
      "epoch:14 step:14033 [D loss: 0.694615, acc: 57.03%] [G loss: 2.020223]\n",
      "epoch:14 step:14034 [D loss: 0.641740, acc: 60.16%] [G loss: 2.136589]\n",
      "epoch:14 step:14035 [D loss: 0.608756, acc: 71.88%] [G loss: 2.057280]\n",
      "epoch:14 step:14036 [D loss: 0.658405, acc: 64.06%] [G loss: 2.165445]\n",
      "epoch:14 step:14037 [D loss: 0.504266, acc: 75.78%] [G loss: 2.169985]\n",
      "epoch:14 step:14038 [D loss: 0.758770, acc: 50.00%] [G loss: 1.770827]\n",
      "epoch:14 step:14039 [D loss: 0.631280, acc: 65.62%] [G loss: 2.045127]\n",
      "epoch:14 step:14040 [D loss: 0.640561, acc: 60.16%] [G loss: 2.056976]\n",
      "epoch:14 step:14041 [D loss: 0.577120, acc: 71.88%] [G loss: 2.169047]\n",
      "epoch:14 step:14042 [D loss: 0.583974, acc: 72.66%] [G loss: 2.212391]\n",
      "epoch:14 step:14043 [D loss: 0.567092, acc: 70.31%] [G loss: 2.320781]\n",
      "epoch:14 step:14044 [D loss: 0.636119, acc: 68.75%] [G loss: 2.039535]\n",
      "epoch:14 step:14045 [D loss: 0.692237, acc: 59.38%] [G loss: 2.109953]\n",
      "epoch:14 step:14046 [D loss: 0.754373, acc: 53.12%] [G loss: 1.878495]\n",
      "epoch:14 step:14047 [D loss: 0.744941, acc: 49.22%] [G loss: 1.927521]\n",
      "epoch:14 step:14048 [D loss: 0.614056, acc: 69.53%] [G loss: 1.995654]\n",
      "epoch:14 step:14049 [D loss: 0.643290, acc: 63.28%] [G loss: 2.008052]\n",
      "epoch:14 step:14050 [D loss: 0.646482, acc: 61.72%] [G loss: 1.921479]\n",
      "epoch:14 step:14051 [D loss: 0.616593, acc: 65.62%] [G loss: 1.913541]\n",
      "epoch:14 step:14052 [D loss: 0.621042, acc: 64.84%] [G loss: 1.992817]\n",
      "epoch:14 step:14053 [D loss: 0.645529, acc: 64.06%] [G loss: 2.045579]\n",
      "epoch:14 step:14054 [D loss: 0.601624, acc: 73.44%] [G loss: 1.996672]\n",
      "epoch:14 step:14055 [D loss: 0.561601, acc: 75.78%] [G loss: 2.653406]\n",
      "epoch:15 step:14056 [D loss: 0.668359, acc: 66.41%] [G loss: 1.931771]\n",
      "epoch:15 step:14057 [D loss: 0.674346, acc: 56.25%] [G loss: 1.912073]\n",
      "epoch:15 step:14058 [D loss: 0.635219, acc: 61.72%] [G loss: 1.925701]\n",
      "epoch:15 step:14059 [D loss: 0.657648, acc: 64.06%] [G loss: 1.926659]\n",
      "epoch:15 step:14060 [D loss: 0.648569, acc: 60.94%] [G loss: 1.944772]\n",
      "epoch:15 step:14061 [D loss: 0.639167, acc: 61.72%] [G loss: 1.926707]\n",
      "epoch:15 step:14062 [D loss: 0.598512, acc: 69.53%] [G loss: 1.970938]\n",
      "epoch:15 step:14063 [D loss: 0.639027, acc: 59.38%] [G loss: 1.927383]\n",
      "epoch:15 step:14064 [D loss: 0.596134, acc: 70.31%] [G loss: 2.119517]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:15 step:14065 [D loss: 0.677195, acc: 58.59%] [G loss: 2.102893]\n",
      "epoch:15 step:14066 [D loss: 0.580881, acc: 69.53%] [G loss: 2.054252]\n",
      "epoch:15 step:14067 [D loss: 0.660404, acc: 59.38%] [G loss: 1.983654]\n",
      "epoch:15 step:14068 [D loss: 0.598411, acc: 68.75%] [G loss: 1.976331]\n",
      "epoch:15 step:14069 [D loss: 0.619702, acc: 63.28%] [G loss: 1.929838]\n",
      "epoch:15 step:14070 [D loss: 0.565587, acc: 70.31%] [G loss: 2.268577]\n",
      "epoch:15 step:14071 [D loss: 0.600317, acc: 70.31%] [G loss: 2.228895]\n",
      "epoch:15 step:14072 [D loss: 0.632044, acc: 65.62%] [G loss: 1.962325]\n",
      "epoch:15 step:14073 [D loss: 0.591239, acc: 64.84%] [G loss: 2.028198]\n",
      "epoch:15 step:14074 [D loss: 0.678539, acc: 66.41%] [G loss: 1.925689]\n",
      "epoch:15 step:14075 [D loss: 0.672759, acc: 57.03%] [G loss: 1.883847]\n",
      "epoch:15 step:14076 [D loss: 0.670036, acc: 59.38%] [G loss: 1.992540]\n",
      "epoch:15 step:14077 [D loss: 0.681323, acc: 54.69%] [G loss: 1.871016]\n",
      "epoch:15 step:14078 [D loss: 0.624858, acc: 62.50%] [G loss: 1.950651]\n",
      "epoch:15 step:14079 [D loss: 0.607211, acc: 65.62%] [G loss: 2.064681]\n",
      "epoch:15 step:14080 [D loss: 0.594498, acc: 67.97%] [G loss: 2.201848]\n",
      "epoch:15 step:14081 [D loss: 0.672955, acc: 60.94%] [G loss: 1.923489]\n",
      "epoch:15 step:14082 [D loss: 0.636881, acc: 64.84%] [G loss: 1.888943]\n",
      "epoch:15 step:14083 [D loss: 0.643373, acc: 65.62%] [G loss: 1.894253]\n",
      "epoch:15 step:14084 [D loss: 0.637206, acc: 64.06%] [G loss: 1.909734]\n",
      "epoch:15 step:14085 [D loss: 0.616172, acc: 64.06%] [G loss: 1.988244]\n",
      "epoch:15 step:14086 [D loss: 0.706096, acc: 54.69%] [G loss: 1.686396]\n",
      "epoch:15 step:14087 [D loss: 0.692590, acc: 56.25%] [G loss: 1.941403]\n",
      "epoch:15 step:14088 [D loss: 0.625742, acc: 66.41%] [G loss: 1.845718]\n",
      "epoch:15 step:14089 [D loss: 0.606704, acc: 68.75%] [G loss: 1.933938]\n",
      "epoch:15 step:14090 [D loss: 0.663328, acc: 60.16%] [G loss: 1.844565]\n",
      "epoch:15 step:14091 [D loss: 0.641474, acc: 62.50%] [G loss: 1.884714]\n",
      "epoch:15 step:14092 [D loss: 0.673681, acc: 57.03%] [G loss: 2.030580]\n",
      "epoch:15 step:14093 [D loss: 0.658799, acc: 60.16%] [G loss: 1.975866]\n",
      "epoch:15 step:14094 [D loss: 0.658635, acc: 58.59%] [G loss: 2.048707]\n",
      "epoch:15 step:14095 [D loss: 0.582033, acc: 68.75%] [G loss: 2.238137]\n",
      "epoch:15 step:14096 [D loss: 0.627695, acc: 62.50%] [G loss: 1.886457]\n",
      "epoch:15 step:14097 [D loss: 0.640170, acc: 57.03%] [G loss: 1.952111]\n",
      "epoch:15 step:14098 [D loss: 0.628179, acc: 67.97%] [G loss: 2.093537]\n",
      "epoch:15 step:14099 [D loss: 0.657616, acc: 60.16%] [G loss: 1.794300]\n",
      "epoch:15 step:14100 [D loss: 0.663240, acc: 60.16%] [G loss: 1.925213]\n",
      "epoch:15 step:14101 [D loss: 0.633090, acc: 63.28%] [G loss: 1.761981]\n",
      "epoch:15 step:14102 [D loss: 0.636115, acc: 66.41%] [G loss: 2.060472]\n",
      "epoch:15 step:14103 [D loss: 0.635725, acc: 61.72%] [G loss: 1.947670]\n",
      "epoch:15 step:14104 [D loss: 0.661563, acc: 60.16%] [G loss: 2.000955]\n",
      "epoch:15 step:14105 [D loss: 0.657705, acc: 61.72%] [G loss: 1.952037]\n",
      "epoch:15 step:14106 [D loss: 0.623998, acc: 67.97%] [G loss: 1.933443]\n",
      "epoch:15 step:14107 [D loss: 0.644985, acc: 58.59%] [G loss: 1.914101]\n",
      "epoch:15 step:14108 [D loss: 0.630893, acc: 68.75%] [G loss: 1.960962]\n",
      "epoch:15 step:14109 [D loss: 0.697189, acc: 63.28%] [G loss: 1.988623]\n",
      "epoch:15 step:14110 [D loss: 0.660505, acc: 63.28%] [G loss: 1.868229]\n",
      "epoch:15 step:14111 [D loss: 0.637066, acc: 65.62%] [G loss: 2.162679]\n",
      "epoch:15 step:14112 [D loss: 0.689059, acc: 58.59%] [G loss: 1.887392]\n",
      "epoch:15 step:14113 [D loss: 0.678293, acc: 51.56%] [G loss: 1.848290]\n",
      "epoch:15 step:14114 [D loss: 0.676387, acc: 57.81%] [G loss: 1.836995]\n",
      "epoch:15 step:14115 [D loss: 0.662426, acc: 66.41%] [G loss: 1.862607]\n",
      "epoch:15 step:14116 [D loss: 0.694115, acc: 60.94%] [G loss: 1.778401]\n",
      "epoch:15 step:14117 [D loss: 0.658483, acc: 64.06%] [G loss: 1.914199]\n",
      "epoch:15 step:14118 [D loss: 0.614279, acc: 68.75%] [G loss: 1.947552]\n",
      "epoch:15 step:14119 [D loss: 0.648015, acc: 64.84%] [G loss: 1.934804]\n",
      "epoch:15 step:14120 [D loss: 0.636227, acc: 64.06%] [G loss: 1.878456]\n",
      "epoch:15 step:14121 [D loss: 0.617347, acc: 68.75%] [G loss: 1.797748]\n",
      "epoch:15 step:14122 [D loss: 0.672770, acc: 62.50%] [G loss: 1.807905]\n",
      "epoch:15 step:14123 [D loss: 0.671989, acc: 61.72%] [G loss: 1.948032]\n",
      "epoch:15 step:14124 [D loss: 0.624517, acc: 63.28%] [G loss: 2.081081]\n",
      "epoch:15 step:14125 [D loss: 0.637822, acc: 67.97%] [G loss: 1.917594]\n",
      "epoch:15 step:14126 [D loss: 0.672544, acc: 58.59%] [G loss: 1.747949]\n",
      "epoch:15 step:14127 [D loss: 0.660621, acc: 59.38%] [G loss: 1.930768]\n",
      "epoch:15 step:14128 [D loss: 0.679758, acc: 59.38%] [G loss: 1.813055]\n",
      "epoch:15 step:14129 [D loss: 0.630322, acc: 64.06%] [G loss: 1.858029]\n",
      "epoch:15 step:14130 [D loss: 0.629420, acc: 62.50%] [G loss: 1.907828]\n",
      "epoch:15 step:14131 [D loss: 0.638439, acc: 63.28%] [G loss: 2.087024]\n",
      "epoch:15 step:14132 [D loss: 0.579510, acc: 72.66%] [G loss: 2.043826]\n",
      "epoch:15 step:14133 [D loss: 0.684518, acc: 57.03%] [G loss: 1.863091]\n",
      "epoch:15 step:14134 [D loss: 0.617437, acc: 71.09%] [G loss: 1.871555]\n",
      "epoch:15 step:14135 [D loss: 0.626877, acc: 64.84%] [G loss: 1.935662]\n",
      "epoch:15 step:14136 [D loss: 0.684440, acc: 56.25%] [G loss: 1.923864]\n",
      "epoch:15 step:14137 [D loss: 0.666055, acc: 60.94%] [G loss: 1.879434]\n",
      "epoch:15 step:14138 [D loss: 0.597874, acc: 70.31%] [G loss: 1.998638]\n",
      "epoch:15 step:14139 [D loss: 0.631046, acc: 60.94%] [G loss: 2.038784]\n",
      "epoch:15 step:14140 [D loss: 0.666244, acc: 64.06%] [G loss: 1.734298]\n",
      "epoch:15 step:14141 [D loss: 0.643600, acc: 64.06%] [G loss: 1.941686]\n",
      "epoch:15 step:14142 [D loss: 0.667430, acc: 64.06%] [G loss: 1.987058]\n",
      "epoch:15 step:14143 [D loss: 0.611840, acc: 69.53%] [G loss: 1.937281]\n",
      "epoch:15 step:14144 [D loss: 0.649105, acc: 67.97%] [G loss: 1.982009]\n",
      "epoch:15 step:14145 [D loss: 0.617688, acc: 66.41%] [G loss: 1.930028]\n",
      "epoch:15 step:14146 [D loss: 0.637592, acc: 64.84%] [G loss: 1.900366]\n",
      "epoch:15 step:14147 [D loss: 0.620236, acc: 68.75%] [G loss: 1.938599]\n",
      "epoch:15 step:14148 [D loss: 0.613718, acc: 68.75%] [G loss: 1.891361]\n",
      "epoch:15 step:14149 [D loss: 0.597801, acc: 73.44%] [G loss: 1.987296]\n",
      "epoch:15 step:14150 [D loss: 0.644064, acc: 64.06%] [G loss: 1.886252]\n",
      "epoch:15 step:14151 [D loss: 0.648702, acc: 61.72%] [G loss: 2.033113]\n",
      "epoch:15 step:14152 [D loss: 0.601980, acc: 64.84%] [G loss: 1.993748]\n",
      "epoch:15 step:14153 [D loss: 0.694581, acc: 60.94%] [G loss: 1.865244]\n",
      "epoch:15 step:14154 [D loss: 0.640411, acc: 66.41%] [G loss: 1.942019]\n",
      "epoch:15 step:14155 [D loss: 0.671560, acc: 59.38%] [G loss: 1.997878]\n",
      "epoch:15 step:14156 [D loss: 0.606667, acc: 69.53%] [G loss: 2.069395]\n",
      "epoch:15 step:14157 [D loss: 0.700627, acc: 57.03%] [G loss: 1.946578]\n",
      "epoch:15 step:14158 [D loss: 0.602118, acc: 63.28%] [G loss: 2.056660]\n",
      "epoch:15 step:14159 [D loss: 0.678378, acc: 58.59%] [G loss: 1.854963]\n",
      "epoch:15 step:14160 [D loss: 0.699475, acc: 62.50%] [G loss: 1.755099]\n",
      "epoch:15 step:14161 [D loss: 0.663084, acc: 54.69%] [G loss: 1.960486]\n",
      "epoch:15 step:14162 [D loss: 0.585981, acc: 72.66%] [G loss: 1.968257]\n",
      "epoch:15 step:14163 [D loss: 0.630213, acc: 64.06%] [G loss: 1.836167]\n",
      "epoch:15 step:14164 [D loss: 0.704079, acc: 53.91%] [G loss: 1.897496]\n",
      "epoch:15 step:14165 [D loss: 0.708159, acc: 53.12%] [G loss: 1.816049]\n",
      "epoch:15 step:14166 [D loss: 0.626541, acc: 66.41%] [G loss: 1.894376]\n",
      "epoch:15 step:14167 [D loss: 0.583854, acc: 68.75%] [G loss: 2.144320]\n",
      "epoch:15 step:14168 [D loss: 0.590582, acc: 68.75%] [G loss: 1.972513]\n",
      "epoch:15 step:14169 [D loss: 0.645349, acc: 59.38%] [G loss: 2.137661]\n",
      "epoch:15 step:14170 [D loss: 0.638529, acc: 71.88%] [G loss: 2.043287]\n",
      "epoch:15 step:14171 [D loss: 0.649883, acc: 59.38%] [G loss: 2.136825]\n",
      "epoch:15 step:14172 [D loss: 0.620442, acc: 61.72%] [G loss: 2.126107]\n",
      "epoch:15 step:14173 [D loss: 0.670424, acc: 62.50%] [G loss: 1.922643]\n",
      "epoch:15 step:14174 [D loss: 0.655716, acc: 61.72%] [G loss: 2.202451]\n",
      "epoch:15 step:14175 [D loss: 0.641950, acc: 60.16%] [G loss: 2.006805]\n",
      "epoch:15 step:14176 [D loss: 0.694766, acc: 63.28%] [G loss: 1.742149]\n",
      "epoch:15 step:14177 [D loss: 0.612084, acc: 71.09%] [G loss: 1.886765]\n",
      "epoch:15 step:14178 [D loss: 0.664307, acc: 58.59%] [G loss: 1.896212]\n",
      "epoch:15 step:14179 [D loss: 0.712844, acc: 57.03%] [G loss: 1.872615]\n",
      "epoch:15 step:14180 [D loss: 0.660166, acc: 58.59%] [G loss: 1.737175]\n",
      "epoch:15 step:14181 [D loss: 0.636680, acc: 63.28%] [G loss: 1.912832]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:15 step:14182 [D loss: 0.647228, acc: 56.25%] [G loss: 1.883129]\n",
      "epoch:15 step:14183 [D loss: 0.652223, acc: 64.84%] [G loss: 1.854642]\n",
      "epoch:15 step:14184 [D loss: 0.639436, acc: 69.53%] [G loss: 1.835247]\n",
      "epoch:15 step:14185 [D loss: 0.615744, acc: 71.88%] [G loss: 1.984862]\n",
      "epoch:15 step:14186 [D loss: 0.606074, acc: 67.19%] [G loss: 2.054891]\n",
      "epoch:15 step:14187 [D loss: 0.646675, acc: 62.50%] [G loss: 1.921552]\n",
      "epoch:15 step:14188 [D loss: 0.698094, acc: 55.47%] [G loss: 1.732435]\n",
      "epoch:15 step:14189 [D loss: 0.685555, acc: 60.94%] [G loss: 1.875046]\n",
      "epoch:15 step:14190 [D loss: 0.638538, acc: 62.50%] [G loss: 1.802386]\n",
      "epoch:15 step:14191 [D loss: 0.698991, acc: 58.59%] [G loss: 1.814313]\n",
      "epoch:15 step:14192 [D loss: 0.612966, acc: 67.19%] [G loss: 1.930857]\n",
      "epoch:15 step:14193 [D loss: 0.642505, acc: 63.28%] [G loss: 1.887744]\n",
      "epoch:15 step:14194 [D loss: 0.616049, acc: 67.19%] [G loss: 2.016674]\n",
      "epoch:15 step:14195 [D loss: 0.624130, acc: 64.84%] [G loss: 1.844087]\n",
      "epoch:15 step:14196 [D loss: 0.576238, acc: 71.88%] [G loss: 1.960123]\n",
      "epoch:15 step:14197 [D loss: 0.603877, acc: 69.53%] [G loss: 1.976226]\n",
      "epoch:15 step:14198 [D loss: 0.703940, acc: 52.34%] [G loss: 1.852904]\n",
      "epoch:15 step:14199 [D loss: 0.629877, acc: 60.94%] [G loss: 2.028172]\n",
      "epoch:15 step:14200 [D loss: 0.618798, acc: 58.59%] [G loss: 1.956120]\n",
      "##############\n",
      "[2.36379973 1.35416567 6.02493135 4.71027418 3.51419907 5.69558182\n",
      " 4.14407246 4.85745857 4.54797741 3.47909943]\n",
      "##########\n",
      "epoch:15 step:14201 [D loss: 0.594107, acc: 67.19%] [G loss: 1.901325]\n",
      "epoch:15 step:14202 [D loss: 0.653971, acc: 59.38%] [G loss: 2.077963]\n",
      "epoch:15 step:14203 [D loss: 0.669057, acc: 64.84%] [G loss: 1.847602]\n",
      "epoch:15 step:14204 [D loss: 0.662056, acc: 64.06%] [G loss: 1.831659]\n",
      "epoch:15 step:14205 [D loss: 0.619143, acc: 61.72%] [G loss: 1.897758]\n",
      "epoch:15 step:14206 [D loss: 0.664352, acc: 60.94%] [G loss: 2.071168]\n",
      "epoch:15 step:14207 [D loss: 0.641378, acc: 66.41%] [G loss: 1.979203]\n",
      "epoch:15 step:14208 [D loss: 0.667341, acc: 59.38%] [G loss: 1.899370]\n",
      "epoch:15 step:14209 [D loss: 0.651623, acc: 60.16%] [G loss: 2.041047]\n",
      "epoch:15 step:14210 [D loss: 0.667119, acc: 58.59%] [G loss: 1.914745]\n",
      "epoch:15 step:14211 [D loss: 0.663881, acc: 58.59%] [G loss: 1.934212]\n",
      "epoch:15 step:14212 [D loss: 0.601982, acc: 65.62%] [G loss: 2.039859]\n",
      "epoch:15 step:14213 [D loss: 0.734892, acc: 53.91%] [G loss: 1.738710]\n",
      "epoch:15 step:14214 [D loss: 0.693367, acc: 59.38%] [G loss: 1.963918]\n",
      "epoch:15 step:14215 [D loss: 0.671346, acc: 53.91%] [G loss: 1.801049]\n",
      "epoch:15 step:14216 [D loss: 0.673999, acc: 64.84%] [G loss: 1.991859]\n",
      "epoch:15 step:14217 [D loss: 0.604700, acc: 63.28%] [G loss: 1.889683]\n",
      "epoch:15 step:14218 [D loss: 0.650720, acc: 60.94%] [G loss: 1.852054]\n",
      "epoch:15 step:14219 [D loss: 0.636327, acc: 63.28%] [G loss: 1.836181]\n",
      "epoch:15 step:14220 [D loss: 0.629055, acc: 66.41%] [G loss: 1.878690]\n",
      "epoch:15 step:14221 [D loss: 0.663182, acc: 60.16%] [G loss: 1.737857]\n",
      "epoch:15 step:14222 [D loss: 0.658948, acc: 57.81%] [G loss: 1.824732]\n",
      "epoch:15 step:14223 [D loss: 0.623820, acc: 64.84%] [G loss: 1.783685]\n",
      "epoch:15 step:14224 [D loss: 0.651843, acc: 59.38%] [G loss: 1.877105]\n",
      "epoch:15 step:14225 [D loss: 0.653187, acc: 63.28%] [G loss: 1.777304]\n",
      "epoch:15 step:14226 [D loss: 0.644934, acc: 64.84%] [G loss: 1.774568]\n",
      "epoch:15 step:14227 [D loss: 0.611341, acc: 65.62%] [G loss: 1.860778]\n",
      "epoch:15 step:14228 [D loss: 0.619019, acc: 69.53%] [G loss: 1.893371]\n",
      "epoch:15 step:14229 [D loss: 0.633832, acc: 65.62%] [G loss: 1.752276]\n",
      "epoch:15 step:14230 [D loss: 0.633452, acc: 65.62%] [G loss: 1.768727]\n",
      "epoch:15 step:14231 [D loss: 0.648592, acc: 58.59%] [G loss: 1.821943]\n",
      "epoch:15 step:14232 [D loss: 0.657996, acc: 58.59%] [G loss: 1.944704]\n",
      "epoch:15 step:14233 [D loss: 0.671656, acc: 61.72%] [G loss: 1.859860]\n",
      "epoch:15 step:14234 [D loss: 0.651578, acc: 59.38%] [G loss: 1.965435]\n",
      "epoch:15 step:14235 [D loss: 0.731600, acc: 51.56%] [G loss: 1.782805]\n",
      "epoch:15 step:14236 [D loss: 0.668253, acc: 58.59%] [G loss: 1.668520]\n",
      "epoch:15 step:14237 [D loss: 0.641919, acc: 66.41%] [G loss: 1.929110]\n",
      "epoch:15 step:14238 [D loss: 0.618790, acc: 66.41%] [G loss: 1.822870]\n",
      "epoch:15 step:14239 [D loss: 0.672689, acc: 61.72%] [G loss: 1.786536]\n",
      "epoch:15 step:14240 [D loss: 0.648475, acc: 62.50%] [G loss: 1.939481]\n",
      "epoch:15 step:14241 [D loss: 0.649683, acc: 60.94%] [G loss: 1.856730]\n",
      "epoch:15 step:14242 [D loss: 0.675829, acc: 57.81%] [G loss: 1.867673]\n",
      "epoch:15 step:14243 [D loss: 0.651116, acc: 60.94%] [G loss: 1.872045]\n",
      "epoch:15 step:14244 [D loss: 0.602595, acc: 70.31%] [G loss: 1.921814]\n",
      "epoch:15 step:14245 [D loss: 0.654379, acc: 59.38%] [G loss: 2.020930]\n",
      "epoch:15 step:14246 [D loss: 0.623058, acc: 64.84%] [G loss: 1.922482]\n",
      "epoch:15 step:14247 [D loss: 0.603469, acc: 68.75%] [G loss: 2.006858]\n",
      "epoch:15 step:14248 [D loss: 0.682127, acc: 55.47%] [G loss: 1.734676]\n",
      "epoch:15 step:14249 [D loss: 0.661502, acc: 62.50%] [G loss: 1.944264]\n",
      "epoch:15 step:14250 [D loss: 0.629599, acc: 63.28%] [G loss: 1.806259]\n",
      "epoch:15 step:14251 [D loss: 0.635415, acc: 62.50%] [G loss: 1.757814]\n",
      "epoch:15 step:14252 [D loss: 0.628629, acc: 61.72%] [G loss: 1.912058]\n",
      "epoch:15 step:14253 [D loss: 0.646015, acc: 60.16%] [G loss: 1.980058]\n",
      "epoch:15 step:14254 [D loss: 0.633705, acc: 65.62%] [G loss: 1.866311]\n",
      "epoch:15 step:14255 [D loss: 0.676880, acc: 63.28%] [G loss: 1.769597]\n",
      "epoch:15 step:14256 [D loss: 0.639776, acc: 63.28%] [G loss: 1.993166]\n",
      "epoch:15 step:14257 [D loss: 0.657608, acc: 58.59%] [G loss: 1.919256]\n",
      "epoch:15 step:14258 [D loss: 0.624979, acc: 63.28%] [G loss: 1.865029]\n",
      "epoch:15 step:14259 [D loss: 0.621809, acc: 68.75%] [G loss: 1.907545]\n",
      "epoch:15 step:14260 [D loss: 0.693450, acc: 56.25%] [G loss: 1.811346]\n",
      "epoch:15 step:14261 [D loss: 0.623560, acc: 62.50%] [G loss: 1.991910]\n",
      "epoch:15 step:14262 [D loss: 0.619869, acc: 65.62%] [G loss: 2.066151]\n",
      "epoch:15 step:14263 [D loss: 0.614300, acc: 69.53%] [G loss: 2.056641]\n",
      "epoch:15 step:14264 [D loss: 0.603342, acc: 65.62%] [G loss: 2.126084]\n",
      "epoch:15 step:14265 [D loss: 0.741418, acc: 46.09%] [G loss: 1.761445]\n",
      "epoch:15 step:14266 [D loss: 0.624412, acc: 68.75%] [G loss: 1.748223]\n",
      "epoch:15 step:14267 [D loss: 0.651081, acc: 60.16%] [G loss: 1.946011]\n",
      "epoch:15 step:14268 [D loss: 0.690281, acc: 52.34%] [G loss: 1.903674]\n",
      "epoch:15 step:14269 [D loss: 0.678882, acc: 60.16%] [G loss: 1.695379]\n",
      "epoch:15 step:14270 [D loss: 0.674500, acc: 60.16%] [G loss: 1.941717]\n",
      "epoch:15 step:14271 [D loss: 0.628538, acc: 60.94%] [G loss: 1.990752]\n",
      "epoch:15 step:14272 [D loss: 0.630550, acc: 64.84%] [G loss: 1.840629]\n",
      "epoch:15 step:14273 [D loss: 0.617644, acc: 69.53%] [G loss: 1.948910]\n",
      "epoch:15 step:14274 [D loss: 0.590983, acc: 69.53%] [G loss: 2.213932]\n",
      "epoch:15 step:14275 [D loss: 0.721444, acc: 48.44%] [G loss: 1.785973]\n",
      "epoch:15 step:14276 [D loss: 0.653608, acc: 62.50%] [G loss: 1.877130]\n",
      "epoch:15 step:14277 [D loss: 0.611796, acc: 64.06%] [G loss: 2.040013]\n",
      "epoch:15 step:14278 [D loss: 0.642638, acc: 63.28%] [G loss: 1.960593]\n",
      "epoch:15 step:14279 [D loss: 0.662014, acc: 62.50%] [G loss: 1.845029]\n",
      "epoch:15 step:14280 [D loss: 0.678389, acc: 62.50%] [G loss: 1.926044]\n",
      "epoch:15 step:14281 [D loss: 0.650185, acc: 61.72%] [G loss: 1.821266]\n",
      "epoch:15 step:14282 [D loss: 0.663831, acc: 58.59%] [G loss: 1.885237]\n",
      "epoch:15 step:14283 [D loss: 0.673600, acc: 57.03%] [G loss: 1.848593]\n",
      "epoch:15 step:14284 [D loss: 0.676440, acc: 56.25%] [G loss: 2.133365]\n",
      "epoch:15 step:14285 [D loss: 0.548555, acc: 71.09%] [G loss: 2.073081]\n",
      "epoch:15 step:14286 [D loss: 0.523624, acc: 76.56%] [G loss: 2.372208]\n",
      "epoch:15 step:14287 [D loss: 0.544093, acc: 73.44%] [G loss: 2.425840]\n",
      "epoch:15 step:14288 [D loss: 0.735170, acc: 53.12%] [G loss: 1.878428]\n",
      "epoch:15 step:14289 [D loss: 0.629697, acc: 62.50%] [G loss: 1.912840]\n",
      "epoch:15 step:14290 [D loss: 0.638017, acc: 64.84%] [G loss: 1.824022]\n",
      "epoch:15 step:14291 [D loss: 0.646694, acc: 54.69%] [G loss: 1.931624]\n",
      "epoch:15 step:14292 [D loss: 0.578250, acc: 70.31%] [G loss: 1.864444]\n",
      "epoch:15 step:14293 [D loss: 0.616915, acc: 70.31%] [G loss: 2.045737]\n",
      "epoch:15 step:14294 [D loss: 0.656160, acc: 61.72%] [G loss: 1.759787]\n",
      "epoch:15 step:14295 [D loss: 0.644023, acc: 63.28%] [G loss: 1.941319]\n",
      "epoch:15 step:14296 [D loss: 0.609014, acc: 67.97%] [G loss: 1.801238]\n",
      "epoch:15 step:14297 [D loss: 0.636102, acc: 62.50%] [G loss: 2.210075]\n",
      "epoch:15 step:14298 [D loss: 0.589234, acc: 69.53%] [G loss: 1.979950]\n",
      "epoch:15 step:14299 [D loss: 0.618919, acc: 66.41%] [G loss: 2.048174]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:15 step:14300 [D loss: 0.696851, acc: 59.38%] [G loss: 1.959278]\n",
      "epoch:15 step:14301 [D loss: 0.633167, acc: 62.50%] [G loss: 1.962901]\n",
      "epoch:15 step:14302 [D loss: 0.653892, acc: 61.72%] [G loss: 1.915676]\n",
      "epoch:15 step:14303 [D loss: 0.653429, acc: 60.16%] [G loss: 1.996828]\n",
      "epoch:15 step:14304 [D loss: 0.667656, acc: 66.41%] [G loss: 1.926552]\n",
      "epoch:15 step:14305 [D loss: 0.697625, acc: 60.16%] [G loss: 1.753152]\n",
      "epoch:15 step:14306 [D loss: 0.682969, acc: 57.81%] [G loss: 1.808752]\n",
      "epoch:15 step:14307 [D loss: 0.673934, acc: 58.59%] [G loss: 1.757105]\n",
      "epoch:15 step:14308 [D loss: 0.665869, acc: 53.91%] [G loss: 1.736184]\n",
      "epoch:15 step:14309 [D loss: 0.654048, acc: 59.38%] [G loss: 1.833513]\n",
      "epoch:15 step:14310 [D loss: 0.681146, acc: 62.50%] [G loss: 1.758986]\n",
      "epoch:15 step:14311 [D loss: 0.660237, acc: 61.72%] [G loss: 1.716864]\n",
      "epoch:15 step:14312 [D loss: 0.649664, acc: 62.50%] [G loss: 1.857457]\n",
      "epoch:15 step:14313 [D loss: 0.671674, acc: 57.03%] [G loss: 1.844690]\n",
      "epoch:15 step:14314 [D loss: 0.651938, acc: 64.06%] [G loss: 1.798641]\n",
      "epoch:15 step:14315 [D loss: 0.642265, acc: 60.16%] [G loss: 1.784881]\n",
      "epoch:15 step:14316 [D loss: 0.619328, acc: 64.84%] [G loss: 1.966193]\n",
      "epoch:15 step:14317 [D loss: 0.635980, acc: 61.72%] [G loss: 1.934255]\n",
      "epoch:15 step:14318 [D loss: 0.639286, acc: 64.84%] [G loss: 1.945485]\n",
      "epoch:15 step:14319 [D loss: 0.653137, acc: 64.06%] [G loss: 2.071475]\n",
      "epoch:15 step:14320 [D loss: 0.623920, acc: 62.50%] [G loss: 1.917094]\n",
      "epoch:15 step:14321 [D loss: 0.619325, acc: 64.84%] [G loss: 1.935131]\n",
      "epoch:15 step:14322 [D loss: 0.638193, acc: 60.16%] [G loss: 1.944091]\n",
      "epoch:15 step:14323 [D loss: 0.695519, acc: 57.03%] [G loss: 1.822497]\n",
      "epoch:15 step:14324 [D loss: 0.621187, acc: 63.28%] [G loss: 1.856655]\n",
      "epoch:15 step:14325 [D loss: 0.636069, acc: 64.84%] [G loss: 2.074344]\n",
      "epoch:15 step:14326 [D loss: 0.630273, acc: 64.06%] [G loss: 1.778466]\n",
      "epoch:15 step:14327 [D loss: 0.622070, acc: 61.72%] [G loss: 1.940503]\n",
      "epoch:15 step:14328 [D loss: 0.647140, acc: 67.97%] [G loss: 1.779590]\n",
      "epoch:15 step:14329 [D loss: 0.557805, acc: 76.56%] [G loss: 1.935300]\n",
      "epoch:15 step:14330 [D loss: 0.654342, acc: 60.16%] [G loss: 2.027689]\n",
      "epoch:15 step:14331 [D loss: 0.715185, acc: 57.81%] [G loss: 2.143448]\n",
      "epoch:15 step:14332 [D loss: 0.664695, acc: 57.81%] [G loss: 1.986451]\n",
      "epoch:15 step:14333 [D loss: 0.703547, acc: 53.91%] [G loss: 1.940669]\n",
      "epoch:15 step:14334 [D loss: 0.628378, acc: 64.06%] [G loss: 1.812773]\n",
      "epoch:15 step:14335 [D loss: 0.615088, acc: 64.06%] [G loss: 1.957785]\n",
      "epoch:15 step:14336 [D loss: 0.694758, acc: 56.25%] [G loss: 1.771128]\n",
      "epoch:15 step:14337 [D loss: 0.633254, acc: 66.41%] [G loss: 1.824337]\n",
      "epoch:15 step:14338 [D loss: 0.550588, acc: 75.00%] [G loss: 2.069595]\n",
      "epoch:15 step:14339 [D loss: 0.685960, acc: 60.16%] [G loss: 1.872490]\n",
      "epoch:15 step:14340 [D loss: 0.649187, acc: 60.94%] [G loss: 1.974557]\n",
      "epoch:15 step:14341 [D loss: 0.647902, acc: 63.28%] [G loss: 1.906835]\n",
      "epoch:15 step:14342 [D loss: 0.688310, acc: 51.56%] [G loss: 1.833205]\n",
      "epoch:15 step:14343 [D loss: 0.664513, acc: 57.81%] [G loss: 1.896126]\n",
      "epoch:15 step:14344 [D loss: 0.660173, acc: 55.47%] [G loss: 1.871773]\n",
      "epoch:15 step:14345 [D loss: 0.660015, acc: 59.38%] [G loss: 1.797387]\n",
      "epoch:15 step:14346 [D loss: 0.677131, acc: 60.16%] [G loss: 1.779356]\n",
      "epoch:15 step:14347 [D loss: 0.634038, acc: 68.75%] [G loss: 1.789684]\n",
      "epoch:15 step:14348 [D loss: 0.625758, acc: 61.72%] [G loss: 1.953039]\n",
      "epoch:15 step:14349 [D loss: 0.712823, acc: 55.47%] [G loss: 1.847354]\n",
      "epoch:15 step:14350 [D loss: 0.622461, acc: 67.19%] [G loss: 1.863106]\n",
      "epoch:15 step:14351 [D loss: 0.654330, acc: 60.94%] [G loss: 1.901646]\n",
      "epoch:15 step:14352 [D loss: 0.641627, acc: 63.28%] [G loss: 1.815574]\n",
      "epoch:15 step:14353 [D loss: 0.630938, acc: 64.06%] [G loss: 2.038532]\n",
      "epoch:15 step:14354 [D loss: 0.608837, acc: 67.97%] [G loss: 1.869094]\n",
      "epoch:15 step:14355 [D loss: 0.607994, acc: 65.62%] [G loss: 1.859379]\n",
      "epoch:15 step:14356 [D loss: 0.671425, acc: 58.59%] [G loss: 1.788805]\n",
      "epoch:15 step:14357 [D loss: 0.639141, acc: 61.72%] [G loss: 1.878901]\n",
      "epoch:15 step:14358 [D loss: 0.725719, acc: 53.91%] [G loss: 1.976667]\n",
      "epoch:15 step:14359 [D loss: 0.656760, acc: 55.47%] [G loss: 1.706721]\n",
      "epoch:15 step:14360 [D loss: 0.652054, acc: 60.94%] [G loss: 1.842773]\n",
      "epoch:15 step:14361 [D loss: 0.636843, acc: 60.94%] [G loss: 1.770760]\n",
      "epoch:15 step:14362 [D loss: 0.648773, acc: 64.06%] [G loss: 1.794151]\n",
      "epoch:15 step:14363 [D loss: 0.673193, acc: 59.38%] [G loss: 1.737016]\n",
      "epoch:15 step:14364 [D loss: 0.619099, acc: 66.41%] [G loss: 1.831025]\n",
      "epoch:15 step:14365 [D loss: 0.658598, acc: 58.59%] [G loss: 1.946342]\n",
      "epoch:15 step:14366 [D loss: 0.610627, acc: 64.84%] [G loss: 1.962580]\n",
      "epoch:15 step:14367 [D loss: 0.631068, acc: 69.53%] [G loss: 2.111761]\n",
      "epoch:15 step:14368 [D loss: 0.613000, acc: 69.53%] [G loss: 2.202376]\n",
      "epoch:15 step:14369 [D loss: 0.559592, acc: 73.44%] [G loss: 2.134352]\n",
      "epoch:15 step:14370 [D loss: 0.582986, acc: 71.09%] [G loss: 2.144884]\n",
      "epoch:15 step:14371 [D loss: 0.686199, acc: 56.25%] [G loss: 1.882071]\n",
      "epoch:15 step:14372 [D loss: 0.637221, acc: 61.72%] [G loss: 1.926204]\n",
      "epoch:15 step:14373 [D loss: 0.642717, acc: 65.62%] [G loss: 1.960477]\n",
      "epoch:15 step:14374 [D loss: 0.652216, acc: 58.59%] [G loss: 1.834921]\n",
      "epoch:15 step:14375 [D loss: 0.647758, acc: 59.38%] [G loss: 1.857683]\n",
      "epoch:15 step:14376 [D loss: 0.643876, acc: 62.50%] [G loss: 1.854261]\n",
      "epoch:15 step:14377 [D loss: 0.707859, acc: 54.69%] [G loss: 1.858435]\n",
      "epoch:15 step:14378 [D loss: 0.693210, acc: 60.16%] [G loss: 1.856248]\n",
      "epoch:15 step:14379 [D loss: 0.667948, acc: 60.94%] [G loss: 1.830561]\n",
      "epoch:15 step:14380 [D loss: 0.640538, acc: 64.06%] [G loss: 1.867095]\n",
      "epoch:15 step:14381 [D loss: 0.688139, acc: 60.16%] [G loss: 1.817238]\n",
      "epoch:15 step:14382 [D loss: 0.701893, acc: 53.12%] [G loss: 1.777773]\n",
      "epoch:15 step:14383 [D loss: 0.648049, acc: 60.16%] [G loss: 1.798623]\n",
      "epoch:15 step:14384 [D loss: 0.640329, acc: 65.62%] [G loss: 1.857286]\n",
      "epoch:15 step:14385 [D loss: 0.654864, acc: 58.59%] [G loss: 1.877706]\n",
      "epoch:15 step:14386 [D loss: 0.644041, acc: 64.06%] [G loss: 1.943913]\n",
      "epoch:15 step:14387 [D loss: 0.606338, acc: 64.06%] [G loss: 1.976430]\n",
      "epoch:15 step:14388 [D loss: 0.633727, acc: 64.84%] [G loss: 1.977328]\n",
      "epoch:15 step:14389 [D loss: 0.656960, acc: 57.03%] [G loss: 1.969495]\n",
      "epoch:15 step:14390 [D loss: 0.644597, acc: 60.16%] [G loss: 1.922710]\n",
      "epoch:15 step:14391 [D loss: 0.610049, acc: 67.19%] [G loss: 2.050060]\n",
      "epoch:15 step:14392 [D loss: 0.556377, acc: 73.44%] [G loss: 2.029623]\n",
      "epoch:15 step:14393 [D loss: 0.682049, acc: 61.72%] [G loss: 2.043811]\n",
      "epoch:15 step:14394 [D loss: 0.635756, acc: 64.06%] [G loss: 1.875568]\n",
      "epoch:15 step:14395 [D loss: 0.638695, acc: 64.84%] [G loss: 1.967923]\n",
      "epoch:15 step:14396 [D loss: 0.691075, acc: 59.38%] [G loss: 1.878389]\n",
      "epoch:15 step:14397 [D loss: 0.687360, acc: 59.38%] [G loss: 1.861537]\n",
      "epoch:15 step:14398 [D loss: 0.672430, acc: 54.69%] [G loss: 1.875839]\n",
      "epoch:15 step:14399 [D loss: 0.613667, acc: 64.84%] [G loss: 1.913166]\n",
      "epoch:15 step:14400 [D loss: 0.637892, acc: 66.41%] [G loss: 2.139243]\n",
      "##############\n",
      "[2.48800992 1.08744637 6.30053906 4.60611805 3.61661621 5.60461712\n",
      " 4.41004359 4.79416566 4.65693272 3.61080777]\n",
      "##########\n",
      "epoch:15 step:14401 [D loss: 0.611139, acc: 67.97%] [G loss: 2.177029]\n",
      "epoch:15 step:14402 [D loss: 0.577537, acc: 71.09%] [G loss: 2.193123]\n",
      "epoch:15 step:14403 [D loss: 0.683016, acc: 61.72%] [G loss: 1.763674]\n",
      "epoch:15 step:14404 [D loss: 0.687039, acc: 57.03%] [G loss: 1.752895]\n",
      "epoch:15 step:14405 [D loss: 0.618184, acc: 67.97%] [G loss: 1.862137]\n",
      "epoch:15 step:14406 [D loss: 0.697156, acc: 53.91%] [G loss: 1.806589]\n",
      "epoch:15 step:14407 [D loss: 0.682321, acc: 53.91%] [G loss: 1.878258]\n",
      "epoch:15 step:14408 [D loss: 0.626882, acc: 65.62%] [G loss: 1.988049]\n",
      "epoch:15 step:14409 [D loss: 0.605573, acc: 67.97%] [G loss: 1.978743]\n",
      "epoch:15 step:14410 [D loss: 0.637775, acc: 66.41%] [G loss: 1.909132]\n",
      "epoch:15 step:14411 [D loss: 0.683020, acc: 59.38%] [G loss: 1.856242]\n",
      "epoch:15 step:14412 [D loss: 0.615036, acc: 67.97%] [G loss: 1.871728]\n",
      "epoch:15 step:14413 [D loss: 0.614001, acc: 66.41%] [G loss: 2.164576]\n",
      "epoch:15 step:14414 [D loss: 0.616523, acc: 69.53%] [G loss: 2.060037]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:15 step:14415 [D loss: 0.633678, acc: 64.06%] [G loss: 1.929986]\n",
      "epoch:15 step:14416 [D loss: 0.631870, acc: 63.28%] [G loss: 1.996964]\n",
      "epoch:15 step:14417 [D loss: 0.592723, acc: 66.41%] [G loss: 2.039880]\n",
      "epoch:15 step:14418 [D loss: 0.628778, acc: 64.84%] [G loss: 1.851587]\n",
      "epoch:15 step:14419 [D loss: 0.705628, acc: 50.78%] [G loss: 1.971823]\n",
      "epoch:15 step:14420 [D loss: 0.625180, acc: 62.50%] [G loss: 1.910395]\n",
      "epoch:15 step:14421 [D loss: 0.606723, acc: 65.62%] [G loss: 1.890704]\n",
      "epoch:15 step:14422 [D loss: 0.609657, acc: 62.50%] [G loss: 1.949304]\n",
      "epoch:15 step:14423 [D loss: 0.619915, acc: 68.75%] [G loss: 1.858229]\n",
      "epoch:15 step:14424 [D loss: 0.674170, acc: 59.38%] [G loss: 2.087283]\n",
      "epoch:15 step:14425 [D loss: 0.650733, acc: 67.19%] [G loss: 1.855045]\n",
      "epoch:15 step:14426 [D loss: 0.657507, acc: 60.94%] [G loss: 2.099322]\n",
      "epoch:15 step:14427 [D loss: 0.608243, acc: 71.09%] [G loss: 1.870706]\n",
      "epoch:15 step:14428 [D loss: 0.673903, acc: 54.69%] [G loss: 1.691575]\n",
      "epoch:15 step:14429 [D loss: 0.644633, acc: 62.50%] [G loss: 1.977364]\n",
      "epoch:15 step:14430 [D loss: 0.678585, acc: 53.12%] [G loss: 1.811546]\n",
      "epoch:15 step:14431 [D loss: 0.668616, acc: 54.69%] [G loss: 1.914904]\n",
      "epoch:15 step:14432 [D loss: 0.694126, acc: 57.81%] [G loss: 1.798547]\n",
      "epoch:15 step:14433 [D loss: 0.634700, acc: 61.72%] [G loss: 1.938182]\n",
      "epoch:15 step:14434 [D loss: 0.630427, acc: 70.31%] [G loss: 1.999938]\n",
      "epoch:15 step:14435 [D loss: 0.662738, acc: 64.84%] [G loss: 1.834645]\n",
      "epoch:15 step:14436 [D loss: 0.659817, acc: 65.62%] [G loss: 1.982329]\n",
      "epoch:15 step:14437 [D loss: 0.659369, acc: 54.69%] [G loss: 1.842035]\n",
      "epoch:15 step:14438 [D loss: 0.658313, acc: 60.16%] [G loss: 1.840457]\n",
      "epoch:15 step:14439 [D loss: 0.667758, acc: 60.94%] [G loss: 1.833958]\n",
      "epoch:15 step:14440 [D loss: 0.588749, acc: 70.31%] [G loss: 1.871270]\n",
      "epoch:15 step:14441 [D loss: 0.642592, acc: 63.28%] [G loss: 1.837317]\n",
      "epoch:15 step:14442 [D loss: 0.699603, acc: 54.69%] [G loss: 1.809576]\n",
      "epoch:15 step:14443 [D loss: 0.642686, acc: 60.94%] [G loss: 1.958909]\n",
      "epoch:15 step:14444 [D loss: 0.619871, acc: 65.62%] [G loss: 1.886726]\n",
      "epoch:15 step:14445 [D loss: 0.682464, acc: 63.28%] [G loss: 1.909881]\n",
      "epoch:15 step:14446 [D loss: 0.678460, acc: 55.47%] [G loss: 1.796811]\n",
      "epoch:15 step:14447 [D loss: 0.600163, acc: 68.75%] [G loss: 1.821411]\n",
      "epoch:15 step:14448 [D loss: 0.671537, acc: 67.19%] [G loss: 1.873579]\n",
      "epoch:15 step:14449 [D loss: 0.657644, acc: 59.38%] [G loss: 1.769264]\n",
      "epoch:15 step:14450 [D loss: 0.661427, acc: 60.94%] [G loss: 1.835511]\n",
      "epoch:15 step:14451 [D loss: 0.688691, acc: 55.47%] [G loss: 1.787938]\n",
      "epoch:15 step:14452 [D loss: 0.625118, acc: 62.50%] [G loss: 1.716709]\n",
      "epoch:15 step:14453 [D loss: 0.635517, acc: 64.06%] [G loss: 1.897574]\n",
      "epoch:15 step:14454 [D loss: 0.599056, acc: 67.19%] [G loss: 2.014351]\n",
      "epoch:15 step:14455 [D loss: 0.665273, acc: 60.16%] [G loss: 1.965089]\n",
      "epoch:15 step:14456 [D loss: 0.675251, acc: 59.38%] [G loss: 1.820443]\n",
      "epoch:15 step:14457 [D loss: 0.647457, acc: 63.28%] [G loss: 1.850416]\n",
      "epoch:15 step:14458 [D loss: 0.669326, acc: 61.72%] [G loss: 1.861616]\n",
      "epoch:15 step:14459 [D loss: 0.684366, acc: 60.94%] [G loss: 1.895722]\n",
      "epoch:15 step:14460 [D loss: 0.641895, acc: 63.28%] [G loss: 1.782287]\n",
      "epoch:15 step:14461 [D loss: 0.649075, acc: 60.94%] [G loss: 1.922852]\n",
      "epoch:15 step:14462 [D loss: 0.630974, acc: 68.75%] [G loss: 1.920006]\n",
      "epoch:15 step:14463 [D loss: 0.633837, acc: 67.97%] [G loss: 1.823387]\n",
      "epoch:15 step:14464 [D loss: 0.641407, acc: 60.94%] [G loss: 1.833791]\n",
      "epoch:15 step:14465 [D loss: 0.689972, acc: 58.59%] [G loss: 2.025195]\n",
      "epoch:15 step:14466 [D loss: 0.612703, acc: 65.62%] [G loss: 1.991631]\n",
      "epoch:15 step:14467 [D loss: 0.644798, acc: 60.94%] [G loss: 1.918829]\n",
      "epoch:15 step:14468 [D loss: 0.612286, acc: 67.97%] [G loss: 1.982428]\n",
      "epoch:15 step:14469 [D loss: 0.628005, acc: 67.97%] [G loss: 2.095558]\n",
      "epoch:15 step:14470 [D loss: 0.649277, acc: 64.06%] [G loss: 2.078684]\n",
      "epoch:15 step:14471 [D loss: 0.618735, acc: 64.84%] [G loss: 2.023084]\n",
      "epoch:15 step:14472 [D loss: 0.668388, acc: 61.72%] [G loss: 1.905648]\n",
      "epoch:15 step:14473 [D loss: 0.677442, acc: 57.81%] [G loss: 1.778632]\n",
      "epoch:15 step:14474 [D loss: 0.668985, acc: 60.16%] [G loss: 1.816926]\n",
      "epoch:15 step:14475 [D loss: 0.642959, acc: 61.72%] [G loss: 1.737769]\n",
      "epoch:15 step:14476 [D loss: 0.687308, acc: 60.16%] [G loss: 1.821898]\n",
      "epoch:15 step:14477 [D loss: 0.675755, acc: 60.94%] [G loss: 1.914103]\n",
      "epoch:15 step:14478 [D loss: 0.668074, acc: 60.16%] [G loss: 1.968175]\n",
      "epoch:15 step:14479 [D loss: 0.622591, acc: 64.06%] [G loss: 1.716051]\n",
      "epoch:15 step:14480 [D loss: 0.656323, acc: 59.38%] [G loss: 1.952651]\n",
      "epoch:15 step:14481 [D loss: 0.652777, acc: 58.59%] [G loss: 1.845202]\n",
      "epoch:15 step:14482 [D loss: 0.656222, acc: 57.81%] [G loss: 1.848095]\n",
      "epoch:15 step:14483 [D loss: 0.573599, acc: 68.75%] [G loss: 2.154177]\n",
      "epoch:15 step:14484 [D loss: 0.557691, acc: 75.00%] [G loss: 2.173359]\n",
      "epoch:15 step:14485 [D loss: 0.553230, acc: 76.56%] [G loss: 2.183239]\n",
      "epoch:15 step:14486 [D loss: 0.670019, acc: 55.47%] [G loss: 1.959530]\n",
      "epoch:15 step:14487 [D loss: 0.691949, acc: 57.81%] [G loss: 1.903369]\n",
      "epoch:15 step:14488 [D loss: 0.609242, acc: 69.53%] [G loss: 2.001667]\n",
      "epoch:15 step:14489 [D loss: 0.619596, acc: 66.41%] [G loss: 2.008050]\n",
      "epoch:15 step:14490 [D loss: 0.567463, acc: 75.78%] [G loss: 1.986317]\n",
      "epoch:15 step:14491 [D loss: 0.597002, acc: 69.53%] [G loss: 2.205940]\n",
      "epoch:15 step:14492 [D loss: 0.716290, acc: 50.78%] [G loss: 1.816189]\n",
      "epoch:15 step:14493 [D loss: 0.686711, acc: 59.38%] [G loss: 1.736350]\n",
      "epoch:15 step:14494 [D loss: 0.650915, acc: 62.50%] [G loss: 1.857787]\n",
      "epoch:15 step:14495 [D loss: 0.667628, acc: 62.50%] [G loss: 1.797024]\n",
      "epoch:15 step:14496 [D loss: 0.687168, acc: 57.81%] [G loss: 1.820045]\n",
      "epoch:15 step:14497 [D loss: 0.677361, acc: 57.03%] [G loss: 1.787785]\n",
      "epoch:15 step:14498 [D loss: 0.648106, acc: 60.16%] [G loss: 1.766951]\n",
      "epoch:15 step:14499 [D loss: 0.673303, acc: 64.06%] [G loss: 1.798662]\n",
      "epoch:15 step:14500 [D loss: 0.596831, acc: 65.62%] [G loss: 1.867583]\n",
      "epoch:15 step:14501 [D loss: 0.681668, acc: 60.16%] [G loss: 1.731057]\n",
      "epoch:15 step:14502 [D loss: 0.644262, acc: 64.84%] [G loss: 1.869225]\n",
      "epoch:15 step:14503 [D loss: 0.640648, acc: 64.06%] [G loss: 1.910627]\n",
      "epoch:15 step:14504 [D loss: 0.647292, acc: 63.28%] [G loss: 1.863191]\n",
      "epoch:15 step:14505 [D loss: 0.634570, acc: 68.75%] [G loss: 1.932997]\n",
      "epoch:15 step:14506 [D loss: 0.619074, acc: 67.97%] [G loss: 2.099830]\n",
      "epoch:15 step:14507 [D loss: 0.651146, acc: 58.59%] [G loss: 1.797366]\n",
      "epoch:15 step:14508 [D loss: 0.671285, acc: 53.91%] [G loss: 2.175802]\n",
      "epoch:15 step:14509 [D loss: 0.653537, acc: 57.81%] [G loss: 1.962433]\n",
      "epoch:15 step:14510 [D loss: 0.645465, acc: 68.75%] [G loss: 1.905943]\n",
      "epoch:15 step:14511 [D loss: 0.687403, acc: 60.16%] [G loss: 1.894879]\n",
      "epoch:15 step:14512 [D loss: 0.614159, acc: 67.19%] [G loss: 1.997304]\n",
      "epoch:15 step:14513 [D loss: 0.688104, acc: 60.16%] [G loss: 1.724330]\n",
      "epoch:15 step:14514 [D loss: 0.678558, acc: 58.59%] [G loss: 1.788057]\n",
      "epoch:15 step:14515 [D loss: 0.663467, acc: 58.59%] [G loss: 1.852373]\n",
      "epoch:15 step:14516 [D loss: 0.630302, acc: 60.94%] [G loss: 1.959651]\n",
      "epoch:15 step:14517 [D loss: 0.635687, acc: 60.94%] [G loss: 1.943423]\n",
      "epoch:15 step:14518 [D loss: 0.613931, acc: 67.19%] [G loss: 1.960572]\n",
      "epoch:15 step:14519 [D loss: 0.659615, acc: 61.72%] [G loss: 1.901727]\n",
      "epoch:15 step:14520 [D loss: 0.693779, acc: 55.47%] [G loss: 1.758151]\n",
      "epoch:15 step:14521 [D loss: 0.712585, acc: 56.25%] [G loss: 1.817574]\n",
      "epoch:15 step:14522 [D loss: 0.624928, acc: 71.88%] [G loss: 1.875707]\n",
      "epoch:15 step:14523 [D loss: 0.628856, acc: 61.72%] [G loss: 2.014938]\n",
      "epoch:15 step:14524 [D loss: 0.614440, acc: 67.97%] [G loss: 2.241834]\n",
      "epoch:15 step:14525 [D loss: 0.604327, acc: 67.97%] [G loss: 2.157660]\n",
      "epoch:15 step:14526 [D loss: 0.568786, acc: 71.88%] [G loss: 2.253543]\n",
      "epoch:15 step:14527 [D loss: 0.602026, acc: 67.97%] [G loss: 2.272682]\n",
      "epoch:15 step:14528 [D loss: 0.769066, acc: 46.88%] [G loss: 1.923968]\n",
      "epoch:15 step:14529 [D loss: 0.679207, acc: 57.81%] [G loss: 1.919434]\n",
      "epoch:15 step:14530 [D loss: 0.654502, acc: 63.28%] [G loss: 1.923353]\n",
      "epoch:15 step:14531 [D loss: 0.629637, acc: 62.50%] [G loss: 1.945235]\n",
      "epoch:15 step:14532 [D loss: 0.679061, acc: 56.25%] [G loss: 1.792651]\n",
      "epoch:15 step:14533 [D loss: 0.712860, acc: 50.78%] [G loss: 1.858807]\n",
      "epoch:15 step:14534 [D loss: 0.624362, acc: 66.41%] [G loss: 2.028143]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:15 step:14535 [D loss: 0.671769, acc: 64.06%] [G loss: 1.962714]\n",
      "epoch:15 step:14536 [D loss: 0.595786, acc: 68.75%] [G loss: 2.030184]\n",
      "epoch:15 step:14537 [D loss: 0.668362, acc: 57.81%] [G loss: 1.785027]\n",
      "epoch:15 step:14538 [D loss: 0.693105, acc: 55.47%] [G loss: 1.693459]\n",
      "epoch:15 step:14539 [D loss: 0.657465, acc: 60.16%] [G loss: 1.918261]\n",
      "epoch:15 step:14540 [D loss: 0.671397, acc: 58.59%] [G loss: 1.811363]\n",
      "epoch:15 step:14541 [D loss: 0.700759, acc: 59.38%] [G loss: 1.911656]\n",
      "epoch:15 step:14542 [D loss: 0.656399, acc: 64.06%] [G loss: 1.828389]\n",
      "epoch:15 step:14543 [D loss: 0.639887, acc: 62.50%] [G loss: 1.909857]\n",
      "epoch:15 step:14544 [D loss: 0.641212, acc: 64.06%] [G loss: 1.788975]\n",
      "epoch:15 step:14545 [D loss: 0.670751, acc: 63.28%] [G loss: 1.869829]\n",
      "epoch:15 step:14546 [D loss: 0.644324, acc: 64.84%] [G loss: 2.036651]\n",
      "epoch:15 step:14547 [D loss: 0.698529, acc: 58.59%] [G loss: 1.806149]\n",
      "epoch:15 step:14548 [D loss: 0.696320, acc: 53.12%] [G loss: 1.877650]\n",
      "epoch:15 step:14549 [D loss: 0.630476, acc: 61.72%] [G loss: 1.884911]\n",
      "epoch:15 step:14550 [D loss: 0.613757, acc: 66.41%] [G loss: 1.885614]\n",
      "epoch:15 step:14551 [D loss: 0.632271, acc: 67.97%] [G loss: 1.902058]\n",
      "epoch:15 step:14552 [D loss: 0.642773, acc: 67.97%] [G loss: 1.949025]\n",
      "epoch:15 step:14553 [D loss: 0.625956, acc: 60.94%] [G loss: 1.913397]\n",
      "epoch:15 step:14554 [D loss: 0.585423, acc: 70.31%] [G loss: 2.183439]\n",
      "epoch:15 step:14555 [D loss: 0.691366, acc: 55.47%] [G loss: 1.759730]\n",
      "epoch:15 step:14556 [D loss: 0.701299, acc: 54.69%] [G loss: 1.700274]\n",
      "epoch:15 step:14557 [D loss: 0.674095, acc: 57.81%] [G loss: 1.606437]\n",
      "epoch:15 step:14558 [D loss: 0.639153, acc: 64.06%] [G loss: 1.815858]\n",
      "epoch:15 step:14559 [D loss: 0.676753, acc: 63.28%] [G loss: 1.998144]\n",
      "epoch:15 step:14560 [D loss: 0.675901, acc: 57.81%] [G loss: 1.734451]\n",
      "epoch:15 step:14561 [D loss: 0.646963, acc: 61.72%] [G loss: 1.844737]\n",
      "epoch:15 step:14562 [D loss: 0.660808, acc: 60.16%] [G loss: 1.756826]\n",
      "epoch:15 step:14563 [D loss: 0.642728, acc: 64.06%] [G loss: 1.917277]\n",
      "epoch:15 step:14564 [D loss: 0.667399, acc: 59.38%] [G loss: 1.831513]\n",
      "epoch:15 step:14565 [D loss: 0.643487, acc: 60.16%] [G loss: 1.778749]\n",
      "epoch:15 step:14566 [D loss: 0.591342, acc: 71.09%] [G loss: 1.913440]\n",
      "epoch:15 step:14567 [D loss: 0.614117, acc: 67.97%] [G loss: 1.857503]\n",
      "epoch:15 step:14568 [D loss: 0.615462, acc: 64.84%] [G loss: 1.842033]\n",
      "epoch:15 step:14569 [D loss: 0.597983, acc: 67.19%] [G loss: 1.986444]\n",
      "epoch:15 step:14570 [D loss: 0.635543, acc: 60.16%] [G loss: 2.073983]\n",
      "epoch:15 step:14571 [D loss: 0.575452, acc: 73.44%] [G loss: 2.036266]\n",
      "epoch:15 step:14572 [D loss: 0.642780, acc: 64.06%] [G loss: 1.949507]\n",
      "epoch:15 step:14573 [D loss: 0.697264, acc: 56.25%] [G loss: 1.762590]\n",
      "epoch:15 step:14574 [D loss: 0.616699, acc: 65.62%] [G loss: 1.852317]\n",
      "epoch:15 step:14575 [D loss: 0.638867, acc: 61.72%] [G loss: 1.902242]\n",
      "epoch:15 step:14576 [D loss: 0.673105, acc: 65.62%] [G loss: 1.846744]\n",
      "epoch:15 step:14577 [D loss: 0.640764, acc: 66.41%] [G loss: 1.945854]\n",
      "epoch:15 step:14578 [D loss: 0.641190, acc: 61.72%] [G loss: 2.055938]\n",
      "epoch:15 step:14579 [D loss: 0.617698, acc: 66.41%] [G loss: 1.921273]\n",
      "epoch:15 step:14580 [D loss: 0.655208, acc: 61.72%] [G loss: 1.904110]\n",
      "epoch:15 step:14581 [D loss: 0.616473, acc: 67.97%] [G loss: 1.814050]\n",
      "epoch:15 step:14582 [D loss: 0.647440, acc: 62.50%] [G loss: 1.885173]\n",
      "epoch:15 step:14583 [D loss: 0.694026, acc: 56.25%] [G loss: 1.745452]\n",
      "epoch:15 step:14584 [D loss: 0.737018, acc: 48.44%] [G loss: 1.593414]\n",
      "epoch:15 step:14585 [D loss: 0.610173, acc: 66.41%] [G loss: 1.785166]\n",
      "epoch:15 step:14586 [D loss: 0.711576, acc: 57.81%] [G loss: 1.726312]\n",
      "epoch:15 step:14587 [D loss: 0.599345, acc: 75.00%] [G loss: 1.867615]\n",
      "epoch:15 step:14588 [D loss: 0.626151, acc: 68.75%] [G loss: 1.952897]\n",
      "epoch:15 step:14589 [D loss: 0.639483, acc: 65.62%] [G loss: 1.931570]\n",
      "epoch:15 step:14590 [D loss: 0.644651, acc: 62.50%] [G loss: 1.796599]\n",
      "epoch:15 step:14591 [D loss: 0.623095, acc: 64.84%] [G loss: 1.968906]\n",
      "epoch:15 step:14592 [D loss: 0.706723, acc: 53.91%] [G loss: 1.808970]\n",
      "epoch:15 step:14593 [D loss: 0.618371, acc: 70.31%] [G loss: 1.753095]\n",
      "epoch:15 step:14594 [D loss: 0.647576, acc: 63.28%] [G loss: 1.958606]\n",
      "epoch:15 step:14595 [D loss: 0.650792, acc: 61.72%] [G loss: 1.975510]\n",
      "epoch:15 step:14596 [D loss: 0.631011, acc: 65.62%] [G loss: 1.790231]\n",
      "epoch:15 step:14597 [D loss: 0.743669, acc: 56.25%] [G loss: 1.916929]\n",
      "epoch:15 step:14598 [D loss: 0.681635, acc: 58.59%] [G loss: 1.874462]\n",
      "epoch:15 step:14599 [D loss: 0.635748, acc: 62.50%] [G loss: 1.849764]\n",
      "epoch:15 step:14600 [D loss: 0.643274, acc: 64.06%] [G loss: 1.892402]\n",
      "##############\n",
      "[2.48826759 1.43826836 6.23174615 4.81323986 3.71867947 5.68277396\n",
      " 4.48013183 4.67578833 4.7640353  3.60369092]\n",
      "##########\n",
      "epoch:15 step:14601 [D loss: 0.613973, acc: 65.62%] [G loss: 1.807987]\n",
      "epoch:15 step:14602 [D loss: 0.653954, acc: 62.50%] [G loss: 1.886592]\n",
      "epoch:15 step:14603 [D loss: 0.608725, acc: 62.50%] [G loss: 1.995808]\n",
      "epoch:15 step:14604 [D loss: 0.607266, acc: 60.16%] [G loss: 1.863516]\n",
      "epoch:15 step:14605 [D loss: 0.628600, acc: 66.41%] [G loss: 1.911262]\n",
      "epoch:15 step:14606 [D loss: 0.582667, acc: 75.00%] [G loss: 1.965101]\n",
      "epoch:15 step:14607 [D loss: 0.612585, acc: 70.31%] [G loss: 2.042175]\n",
      "epoch:15 step:14608 [D loss: 0.641563, acc: 60.94%] [G loss: 1.881724]\n",
      "epoch:15 step:14609 [D loss: 0.600899, acc: 64.84%] [G loss: 2.159157]\n",
      "epoch:15 step:14610 [D loss: 0.656766, acc: 61.72%] [G loss: 1.857150]\n",
      "epoch:15 step:14611 [D loss: 0.639012, acc: 65.62%] [G loss: 2.063692]\n",
      "epoch:15 step:14612 [D loss: 0.634765, acc: 67.19%] [G loss: 1.997369]\n",
      "epoch:15 step:14613 [D loss: 0.587013, acc: 72.66%] [G loss: 1.968270]\n",
      "epoch:15 step:14614 [D loss: 0.690215, acc: 56.25%] [G loss: 1.877680]\n",
      "epoch:15 step:14615 [D loss: 0.667108, acc: 57.81%] [G loss: 1.977352]\n",
      "epoch:15 step:14616 [D loss: 0.584606, acc: 69.53%] [G loss: 1.898467]\n",
      "epoch:15 step:14617 [D loss: 0.615615, acc: 64.06%] [G loss: 1.927347]\n",
      "epoch:15 step:14618 [D loss: 0.619471, acc: 64.84%] [G loss: 1.970983]\n",
      "epoch:15 step:14619 [D loss: 0.568042, acc: 72.66%] [G loss: 2.037205]\n",
      "epoch:15 step:14620 [D loss: 0.670162, acc: 59.38%] [G loss: 1.784786]\n",
      "epoch:15 step:14621 [D loss: 0.692118, acc: 57.03%] [G loss: 1.776935]\n",
      "epoch:15 step:14622 [D loss: 0.679006, acc: 57.03%] [G loss: 1.922792]\n",
      "epoch:15 step:14623 [D loss: 0.655078, acc: 57.81%] [G loss: 1.882146]\n",
      "epoch:15 step:14624 [D loss: 0.619508, acc: 71.88%] [G loss: 1.833836]\n",
      "epoch:15 step:14625 [D loss: 0.625594, acc: 65.62%] [G loss: 1.933939]\n",
      "epoch:15 step:14626 [D loss: 0.620035, acc: 66.41%] [G loss: 1.876552]\n",
      "epoch:15 step:14627 [D loss: 0.646304, acc: 62.50%] [G loss: 1.986825]\n",
      "epoch:15 step:14628 [D loss: 0.651683, acc: 60.94%] [G loss: 2.044307]\n",
      "epoch:15 step:14629 [D loss: 0.590831, acc: 73.44%] [G loss: 2.137529]\n",
      "epoch:15 step:14630 [D loss: 0.597195, acc: 68.75%] [G loss: 2.017230]\n",
      "epoch:15 step:14631 [D loss: 0.667544, acc: 58.59%] [G loss: 1.971685]\n",
      "epoch:15 step:14632 [D loss: 0.694457, acc: 55.47%] [G loss: 1.784242]\n",
      "epoch:15 step:14633 [D loss: 0.637167, acc: 57.81%] [G loss: 2.046528]\n",
      "epoch:15 step:14634 [D loss: 0.688418, acc: 57.81%] [G loss: 1.846675]\n",
      "epoch:15 step:14635 [D loss: 0.662214, acc: 57.81%] [G loss: 1.809589]\n",
      "epoch:15 step:14636 [D loss: 0.658143, acc: 58.59%] [G loss: 1.950669]\n",
      "epoch:15 step:14637 [D loss: 0.603007, acc: 68.75%] [G loss: 2.031148]\n",
      "epoch:15 step:14638 [D loss: 0.706275, acc: 60.16%] [G loss: 1.758422]\n",
      "epoch:15 step:14639 [D loss: 0.676584, acc: 60.94%] [G loss: 1.780963]\n",
      "epoch:15 step:14640 [D loss: 0.678364, acc: 60.16%] [G loss: 1.850761]\n",
      "epoch:15 step:14641 [D loss: 0.647885, acc: 64.06%] [G loss: 1.816426]\n",
      "epoch:15 step:14642 [D loss: 0.646657, acc: 64.06%] [G loss: 1.758507]\n",
      "epoch:15 step:14643 [D loss: 0.665203, acc: 61.72%] [G loss: 1.845285]\n",
      "epoch:15 step:14644 [D loss: 0.634214, acc: 64.84%] [G loss: 1.907994]\n",
      "epoch:15 step:14645 [D loss: 0.677514, acc: 60.16%] [G loss: 1.783710]\n",
      "epoch:15 step:14646 [D loss: 0.621675, acc: 65.62%] [G loss: 1.956898]\n",
      "epoch:15 step:14647 [D loss: 0.669995, acc: 64.06%] [G loss: 1.914953]\n",
      "epoch:15 step:14648 [D loss: 0.672225, acc: 60.16%] [G loss: 1.853933]\n",
      "epoch:15 step:14649 [D loss: 0.662687, acc: 60.16%] [G loss: 1.752673]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:15 step:14650 [D loss: 0.626908, acc: 60.94%] [G loss: 1.794136]\n",
      "epoch:15 step:14651 [D loss: 0.654370, acc: 58.59%] [G loss: 1.886635]\n",
      "epoch:15 step:14652 [D loss: 0.704236, acc: 51.56%] [G loss: 1.929966]\n",
      "epoch:15 step:14653 [D loss: 0.650635, acc: 64.06%] [G loss: 1.918908]\n",
      "epoch:15 step:14654 [D loss: 0.639447, acc: 61.72%] [G loss: 1.967848]\n",
      "epoch:15 step:14655 [D loss: 0.624037, acc: 67.97%] [G loss: 1.835675]\n",
      "epoch:15 step:14656 [D loss: 0.668635, acc: 60.16%] [G loss: 1.923257]\n",
      "epoch:15 step:14657 [D loss: 0.623745, acc: 63.28%] [G loss: 1.842326]\n",
      "epoch:15 step:14658 [D loss: 0.600394, acc: 68.75%] [G loss: 2.009774]\n",
      "epoch:15 step:14659 [D loss: 0.631084, acc: 66.41%] [G loss: 2.066090]\n",
      "epoch:15 step:14660 [D loss: 0.694659, acc: 52.34%] [G loss: 1.908851]\n",
      "epoch:15 step:14661 [D loss: 0.660821, acc: 60.94%] [G loss: 1.845593]\n",
      "epoch:15 step:14662 [D loss: 0.642383, acc: 62.50%] [G loss: 1.937941]\n",
      "epoch:15 step:14663 [D loss: 0.615476, acc: 69.53%] [G loss: 1.899942]\n",
      "epoch:15 step:14664 [D loss: 0.645289, acc: 66.41%] [G loss: 1.883680]\n",
      "epoch:15 step:14665 [D loss: 0.615950, acc: 67.19%] [G loss: 1.886025]\n",
      "epoch:15 step:14666 [D loss: 0.648169, acc: 57.03%] [G loss: 1.779419]\n",
      "epoch:15 step:14667 [D loss: 0.619532, acc: 64.06%] [G loss: 1.818214]\n",
      "epoch:15 step:14668 [D loss: 0.622121, acc: 63.28%] [G loss: 1.963544]\n",
      "epoch:15 step:14669 [D loss: 0.683470, acc: 55.47%] [G loss: 1.751623]\n",
      "epoch:15 step:14670 [D loss: 0.687493, acc: 56.25%] [G loss: 1.766238]\n",
      "epoch:15 step:14671 [D loss: 0.623716, acc: 68.75%] [G loss: 1.883574]\n",
      "epoch:15 step:14672 [D loss: 0.636822, acc: 67.97%] [G loss: 1.853080]\n",
      "epoch:15 step:14673 [D loss: 0.625024, acc: 67.97%] [G loss: 1.831746]\n",
      "epoch:15 step:14674 [D loss: 0.620880, acc: 68.75%] [G loss: 1.830264]\n",
      "epoch:15 step:14675 [D loss: 0.639809, acc: 67.97%] [G loss: 2.000267]\n",
      "epoch:15 step:14676 [D loss: 0.656132, acc: 60.16%] [G loss: 1.826288]\n",
      "epoch:15 step:14677 [D loss: 0.689399, acc: 57.03%] [G loss: 2.034207]\n",
      "epoch:15 step:14678 [D loss: 0.659043, acc: 60.16%] [G loss: 1.951181]\n",
      "epoch:15 step:14679 [D loss: 0.621479, acc: 69.53%] [G loss: 2.142380]\n",
      "epoch:15 step:14680 [D loss: 0.666263, acc: 60.94%] [G loss: 1.810975]\n",
      "epoch:15 step:14681 [D loss: 0.688564, acc: 57.81%] [G loss: 1.873220]\n",
      "epoch:15 step:14682 [D loss: 0.667998, acc: 59.38%] [G loss: 1.706142]\n",
      "epoch:15 step:14683 [D loss: 0.677517, acc: 58.59%] [G loss: 1.767326]\n",
      "epoch:15 step:14684 [D loss: 0.666572, acc: 63.28%] [G loss: 1.936878]\n",
      "epoch:15 step:14685 [D loss: 0.676295, acc: 61.72%] [G loss: 1.832138]\n",
      "epoch:15 step:14686 [D loss: 0.619729, acc: 64.06%] [G loss: 1.891098]\n",
      "epoch:15 step:14687 [D loss: 0.608580, acc: 74.22%] [G loss: 1.897371]\n",
      "epoch:15 step:14688 [D loss: 0.662605, acc: 62.50%] [G loss: 1.931532]\n",
      "epoch:15 step:14689 [D loss: 0.597361, acc: 67.97%] [G loss: 1.967711]\n",
      "epoch:15 step:14690 [D loss: 0.665224, acc: 57.03%] [G loss: 1.900546]\n",
      "epoch:15 step:14691 [D loss: 0.663464, acc: 57.03%] [G loss: 1.750258]\n",
      "epoch:15 step:14692 [D loss: 0.659244, acc: 58.59%] [G loss: 1.873884]\n",
      "epoch:15 step:14693 [D loss: 0.614133, acc: 65.62%] [G loss: 1.965712]\n",
      "epoch:15 step:14694 [D loss: 0.644206, acc: 53.91%] [G loss: 1.860887]\n",
      "epoch:15 step:14695 [D loss: 0.718851, acc: 55.47%] [G loss: 1.883079]\n",
      "epoch:15 step:14696 [D loss: 0.633768, acc: 64.06%] [G loss: 1.915273]\n",
      "epoch:15 step:14697 [D loss: 0.588511, acc: 67.97%] [G loss: 1.933568]\n",
      "epoch:15 step:14698 [D loss: 0.627498, acc: 64.06%] [G loss: 1.922486]\n",
      "epoch:15 step:14699 [D loss: 0.630853, acc: 64.06%] [G loss: 2.037016]\n",
      "epoch:15 step:14700 [D loss: 0.627229, acc: 67.19%] [G loss: 1.945360]\n",
      "epoch:15 step:14701 [D loss: 0.629261, acc: 67.19%] [G loss: 2.135473]\n",
      "epoch:15 step:14702 [D loss: 0.632406, acc: 64.84%] [G loss: 2.087554]\n",
      "epoch:15 step:14703 [D loss: 0.609086, acc: 68.75%] [G loss: 2.222088]\n",
      "epoch:15 step:14704 [D loss: 0.623890, acc: 63.28%] [G loss: 2.038585]\n",
      "epoch:15 step:14705 [D loss: 0.604761, acc: 65.62%] [G loss: 2.174698]\n",
      "epoch:15 step:14706 [D loss: 0.661202, acc: 60.94%] [G loss: 2.037336]\n",
      "epoch:15 step:14707 [D loss: 0.651587, acc: 63.28%] [G loss: 1.874154]\n",
      "epoch:15 step:14708 [D loss: 0.622638, acc: 63.28%] [G loss: 1.928115]\n",
      "epoch:15 step:14709 [D loss: 0.572323, acc: 73.44%] [G loss: 2.095491]\n",
      "epoch:15 step:14710 [D loss: 0.675932, acc: 61.72%] [G loss: 1.940679]\n",
      "epoch:15 step:14711 [D loss: 0.674322, acc: 64.06%] [G loss: 1.866810]\n",
      "epoch:15 step:14712 [D loss: 0.653314, acc: 57.03%] [G loss: 1.877872]\n",
      "epoch:15 step:14713 [D loss: 0.678863, acc: 57.81%] [G loss: 1.737387]\n",
      "epoch:15 step:14714 [D loss: 0.675776, acc: 54.69%] [G loss: 1.801404]\n",
      "epoch:15 step:14715 [D loss: 0.684486, acc: 60.16%] [G loss: 1.848125]\n",
      "epoch:15 step:14716 [D loss: 0.684943, acc: 61.72%] [G loss: 1.905994]\n",
      "epoch:15 step:14717 [D loss: 0.631163, acc: 63.28%] [G loss: 1.907470]\n",
      "epoch:15 step:14718 [D loss: 0.629371, acc: 64.84%] [G loss: 1.928744]\n",
      "epoch:15 step:14719 [D loss: 0.622931, acc: 65.62%] [G loss: 1.756654]\n",
      "epoch:15 step:14720 [D loss: 0.616045, acc: 67.97%] [G loss: 1.913708]\n",
      "epoch:15 step:14721 [D loss: 0.672883, acc: 51.56%] [G loss: 1.830178]\n",
      "epoch:15 step:14722 [D loss: 0.652881, acc: 61.72%] [G loss: 1.761602]\n",
      "epoch:15 step:14723 [D loss: 0.673229, acc: 59.38%] [G loss: 1.811608]\n",
      "epoch:15 step:14724 [D loss: 0.638844, acc: 64.84%] [G loss: 1.757042]\n",
      "epoch:15 step:14725 [D loss: 0.592085, acc: 68.75%] [G loss: 1.899376]\n",
      "epoch:15 step:14726 [D loss: 0.711261, acc: 58.59%] [G loss: 1.974784]\n",
      "epoch:15 step:14727 [D loss: 0.690176, acc: 58.59%] [G loss: 1.860004]\n",
      "epoch:15 step:14728 [D loss: 0.634461, acc: 62.50%] [G loss: 1.896904]\n",
      "epoch:15 step:14729 [D loss: 0.655872, acc: 61.72%] [G loss: 1.961098]\n",
      "epoch:15 step:14730 [D loss: 0.667437, acc: 59.38%] [G loss: 1.904202]\n",
      "epoch:15 step:14731 [D loss: 0.677246, acc: 60.94%] [G loss: 1.881714]\n",
      "epoch:15 step:14732 [D loss: 0.653663, acc: 58.59%] [G loss: 1.795867]\n",
      "epoch:15 step:14733 [D loss: 0.670601, acc: 54.69%] [G loss: 1.886817]\n",
      "epoch:15 step:14734 [D loss: 0.652789, acc: 61.72%] [G loss: 1.885617]\n",
      "epoch:15 step:14735 [D loss: 0.592279, acc: 71.09%] [G loss: 1.980154]\n",
      "epoch:15 step:14736 [D loss: 0.646543, acc: 61.72%] [G loss: 2.113097]\n",
      "epoch:15 step:14737 [D loss: 0.678435, acc: 60.16%] [G loss: 1.749249]\n",
      "epoch:15 step:14738 [D loss: 0.617204, acc: 65.62%] [G loss: 1.687721]\n",
      "epoch:15 step:14739 [D loss: 0.612292, acc: 67.97%] [G loss: 1.807211]\n",
      "epoch:15 step:14740 [D loss: 0.675786, acc: 61.72%] [G loss: 1.777104]\n",
      "epoch:15 step:14741 [D loss: 0.606396, acc: 67.97%] [G loss: 1.874270]\n",
      "epoch:15 step:14742 [D loss: 0.653440, acc: 59.38%] [G loss: 1.847956]\n",
      "epoch:15 step:14743 [D loss: 0.634571, acc: 66.41%] [G loss: 1.960508]\n",
      "epoch:15 step:14744 [D loss: 0.605679, acc: 62.50%] [G loss: 1.982446]\n",
      "epoch:15 step:14745 [D loss: 0.592782, acc: 69.53%] [G loss: 2.057798]\n",
      "epoch:15 step:14746 [D loss: 0.652258, acc: 54.69%] [G loss: 2.042798]\n",
      "epoch:15 step:14747 [D loss: 0.604227, acc: 61.72%] [G loss: 1.993505]\n",
      "epoch:15 step:14748 [D loss: 0.591352, acc: 66.41%] [G loss: 1.996613]\n",
      "epoch:15 step:14749 [D loss: 0.612472, acc: 67.19%] [G loss: 2.182029]\n",
      "epoch:15 step:14750 [D loss: 0.621097, acc: 60.94%] [G loss: 2.140781]\n",
      "epoch:15 step:14751 [D loss: 0.666056, acc: 57.03%] [G loss: 1.877272]\n",
      "epoch:15 step:14752 [D loss: 0.657851, acc: 60.16%] [G loss: 1.851737]\n",
      "epoch:15 step:14753 [D loss: 0.684250, acc: 56.25%] [G loss: 1.797574]\n",
      "epoch:15 step:14754 [D loss: 0.680627, acc: 60.94%] [G loss: 1.997385]\n",
      "epoch:15 step:14755 [D loss: 0.667413, acc: 53.91%] [G loss: 1.871337]\n",
      "epoch:15 step:14756 [D loss: 0.665919, acc: 57.81%] [G loss: 2.059861]\n",
      "epoch:15 step:14757 [D loss: 0.671286, acc: 66.41%] [G loss: 1.952333]\n",
      "epoch:15 step:14758 [D loss: 0.679956, acc: 59.38%] [G loss: 1.807163]\n",
      "epoch:15 step:14759 [D loss: 0.658492, acc: 58.59%] [G loss: 1.781662]\n",
      "epoch:15 step:14760 [D loss: 0.619508, acc: 67.97%] [G loss: 1.782772]\n",
      "epoch:15 step:14761 [D loss: 0.627910, acc: 60.16%] [G loss: 1.873038]\n",
      "epoch:15 step:14762 [D loss: 0.608773, acc: 63.28%] [G loss: 1.950490]\n",
      "epoch:15 step:14763 [D loss: 0.604776, acc: 72.66%] [G loss: 2.100751]\n",
      "epoch:15 step:14764 [D loss: 0.637885, acc: 62.50%] [G loss: 1.942625]\n",
      "epoch:15 step:14765 [D loss: 0.667373, acc: 55.47%] [G loss: 1.754381]\n",
      "epoch:15 step:14766 [D loss: 0.659263, acc: 64.84%] [G loss: 1.983334]\n",
      "epoch:15 step:14767 [D loss: 0.656476, acc: 65.62%] [G loss: 1.986527]\n",
      "epoch:15 step:14768 [D loss: 0.655496, acc: 60.94%] [G loss: 1.954819]\n",
      "epoch:15 step:14769 [D loss: 0.685568, acc: 58.59%] [G loss: 1.923693]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:15 step:14770 [D loss: 0.598697, acc: 66.41%] [G loss: 1.860734]\n",
      "epoch:15 step:14771 [D loss: 0.657548, acc: 62.50%] [G loss: 1.699251]\n",
      "epoch:15 step:14772 [D loss: 0.651945, acc: 63.28%] [G loss: 1.850074]\n",
      "epoch:15 step:14773 [D loss: 0.651669, acc: 61.72%] [G loss: 1.874673]\n",
      "epoch:15 step:14774 [D loss: 0.604103, acc: 72.66%] [G loss: 2.011669]\n",
      "epoch:15 step:14775 [D loss: 0.662390, acc: 55.47%] [G loss: 1.915015]\n",
      "epoch:15 step:14776 [D loss: 0.659295, acc: 60.16%] [G loss: 1.947430]\n",
      "epoch:15 step:14777 [D loss: 0.673340, acc: 56.25%] [G loss: 1.889220]\n",
      "epoch:15 step:14778 [D loss: 0.601685, acc: 75.00%] [G loss: 1.995531]\n",
      "epoch:15 step:14779 [D loss: 0.662827, acc: 63.28%] [G loss: 1.915323]\n",
      "epoch:15 step:14780 [D loss: 0.665111, acc: 62.50%] [G loss: 1.921308]\n",
      "epoch:15 step:14781 [D loss: 0.613577, acc: 63.28%] [G loss: 2.018699]\n",
      "epoch:15 step:14782 [D loss: 0.635344, acc: 61.72%] [G loss: 1.903274]\n",
      "epoch:15 step:14783 [D loss: 0.689185, acc: 55.47%] [G loss: 1.987784]\n",
      "epoch:15 step:14784 [D loss: 0.629884, acc: 64.06%] [G loss: 1.996058]\n",
      "epoch:15 step:14785 [D loss: 0.638197, acc: 63.28%] [G loss: 1.891086]\n",
      "epoch:15 step:14786 [D loss: 0.643799, acc: 63.28%] [G loss: 1.918737]\n",
      "epoch:15 step:14787 [D loss: 0.634641, acc: 63.28%] [G loss: 1.999488]\n",
      "epoch:15 step:14788 [D loss: 0.632124, acc: 65.62%] [G loss: 1.970404]\n",
      "epoch:15 step:14789 [D loss: 0.665919, acc: 65.62%] [G loss: 1.870129]\n",
      "epoch:15 step:14790 [D loss: 0.655428, acc: 61.72%] [G loss: 1.980598]\n",
      "epoch:15 step:14791 [D loss: 0.622022, acc: 64.06%] [G loss: 1.916420]\n",
      "epoch:15 step:14792 [D loss: 0.669739, acc: 60.94%] [G loss: 1.835436]\n",
      "epoch:15 step:14793 [D loss: 0.619487, acc: 65.62%] [G loss: 1.938278]\n",
      "epoch:15 step:14794 [D loss: 0.653217, acc: 64.84%] [G loss: 1.923415]\n",
      "epoch:15 step:14795 [D loss: 0.654120, acc: 61.72%] [G loss: 1.857819]\n",
      "epoch:15 step:14796 [D loss: 0.647197, acc: 64.84%] [G loss: 1.864572]\n",
      "epoch:15 step:14797 [D loss: 0.647681, acc: 62.50%] [G loss: 1.999246]\n",
      "epoch:15 step:14798 [D loss: 0.637665, acc: 67.19%] [G loss: 1.816537]\n",
      "epoch:15 step:14799 [D loss: 0.620196, acc: 66.41%] [G loss: 2.101140]\n",
      "epoch:15 step:14800 [D loss: 0.688485, acc: 54.69%] [G loss: 1.946241]\n",
      "##############\n",
      "[2.42508441 1.50679191 6.56935437 4.80381045 3.67447371 5.70637658\n",
      " 4.47339206 4.72442571 4.5725484  3.56523162]\n",
      "##########\n",
      "epoch:15 step:14801 [D loss: 0.607898, acc: 69.53%] [G loss: 2.061339]\n",
      "epoch:15 step:14802 [D loss: 0.584176, acc: 70.31%] [G loss: 2.048981]\n",
      "epoch:15 step:14803 [D loss: 0.601233, acc: 63.28%] [G loss: 2.007773]\n",
      "epoch:15 step:14804 [D loss: 0.608135, acc: 64.84%] [G loss: 1.946522]\n",
      "epoch:15 step:14805 [D loss: 0.688502, acc: 57.03%] [G loss: 1.940701]\n",
      "epoch:15 step:14806 [D loss: 0.650166, acc: 64.06%] [G loss: 1.964125]\n",
      "epoch:15 step:14807 [D loss: 0.673624, acc: 62.50%] [G loss: 1.838083]\n",
      "epoch:15 step:14808 [D loss: 0.628485, acc: 64.84%] [G loss: 1.884569]\n",
      "epoch:15 step:14809 [D loss: 0.596721, acc: 71.88%] [G loss: 1.916072]\n",
      "epoch:15 step:14810 [D loss: 0.644148, acc: 64.06%] [G loss: 1.911188]\n",
      "epoch:15 step:14811 [D loss: 0.659946, acc: 60.94%] [G loss: 2.051158]\n",
      "epoch:15 step:14812 [D loss: 0.624020, acc: 60.94%] [G loss: 1.906782]\n",
      "epoch:15 step:14813 [D loss: 0.640913, acc: 64.06%] [G loss: 1.955423]\n",
      "epoch:15 step:14814 [D loss: 0.631344, acc: 56.25%] [G loss: 1.845887]\n",
      "epoch:15 step:14815 [D loss: 0.620637, acc: 64.06%] [G loss: 1.806004]\n",
      "epoch:15 step:14816 [D loss: 0.694952, acc: 53.91%] [G loss: 1.800453]\n",
      "epoch:15 step:14817 [D loss: 0.628083, acc: 60.94%] [G loss: 1.978878]\n",
      "epoch:15 step:14818 [D loss: 0.673460, acc: 58.59%] [G loss: 1.929979]\n",
      "epoch:15 step:14819 [D loss: 0.688805, acc: 52.34%] [G loss: 1.838670]\n",
      "epoch:15 step:14820 [D loss: 0.712060, acc: 53.91%] [G loss: 1.749081]\n",
      "epoch:15 step:14821 [D loss: 0.656958, acc: 59.38%] [G loss: 1.778344]\n",
      "epoch:15 step:14822 [D loss: 0.660298, acc: 61.72%] [G loss: 1.928202]\n",
      "epoch:15 step:14823 [D loss: 0.654748, acc: 67.19%] [G loss: 1.865286]\n",
      "epoch:15 step:14824 [D loss: 0.630747, acc: 61.72%] [G loss: 1.898775]\n",
      "epoch:15 step:14825 [D loss: 0.636977, acc: 68.75%] [G loss: 1.877203]\n",
      "epoch:15 step:14826 [D loss: 0.679678, acc: 60.94%] [G loss: 1.814408]\n",
      "epoch:15 step:14827 [D loss: 0.623193, acc: 63.28%] [G loss: 1.877119]\n",
      "epoch:15 step:14828 [D loss: 0.660787, acc: 62.50%] [G loss: 1.769503]\n",
      "epoch:15 step:14829 [D loss: 0.650854, acc: 60.94%] [G loss: 1.820141]\n",
      "epoch:15 step:14830 [D loss: 0.588218, acc: 71.88%] [G loss: 2.030265]\n",
      "epoch:15 step:14831 [D loss: 0.641662, acc: 60.16%] [G loss: 1.965613]\n",
      "epoch:15 step:14832 [D loss: 0.619750, acc: 67.19%] [G loss: 1.945287]\n",
      "epoch:15 step:14833 [D loss: 0.683443, acc: 57.03%] [G loss: 1.831332]\n",
      "epoch:15 step:14834 [D loss: 0.668778, acc: 64.06%] [G loss: 1.915180]\n",
      "epoch:15 step:14835 [D loss: 0.644694, acc: 64.84%] [G loss: 2.100142]\n",
      "epoch:15 step:14836 [D loss: 0.542123, acc: 75.78%] [G loss: 2.269204]\n",
      "epoch:15 step:14837 [D loss: 0.598516, acc: 64.84%] [G loss: 2.067531]\n",
      "epoch:15 step:14838 [D loss: 0.640719, acc: 60.16%] [G loss: 1.988238]\n",
      "epoch:15 step:14839 [D loss: 0.720241, acc: 54.69%] [G loss: 1.811380]\n",
      "epoch:15 step:14840 [D loss: 0.666312, acc: 57.81%] [G loss: 1.860548]\n",
      "epoch:15 step:14841 [D loss: 0.583340, acc: 67.97%] [G loss: 2.084977]\n",
      "epoch:15 step:14842 [D loss: 0.655598, acc: 66.41%] [G loss: 1.871188]\n",
      "epoch:15 step:14843 [D loss: 0.653231, acc: 59.38%] [G loss: 1.896939]\n",
      "epoch:15 step:14844 [D loss: 0.654162, acc: 63.28%] [G loss: 2.016475]\n",
      "epoch:15 step:14845 [D loss: 0.660147, acc: 60.94%] [G loss: 2.046040]\n",
      "epoch:15 step:14846 [D loss: 0.652204, acc: 61.72%] [G loss: 1.917579]\n",
      "epoch:15 step:14847 [D loss: 0.584586, acc: 67.97%] [G loss: 2.002841]\n",
      "epoch:15 step:14848 [D loss: 0.619335, acc: 67.19%] [G loss: 1.988267]\n",
      "epoch:15 step:14849 [D loss: 0.637152, acc: 58.59%] [G loss: 1.838446]\n",
      "epoch:15 step:14850 [D loss: 0.703389, acc: 56.25%] [G loss: 1.831348]\n",
      "epoch:15 step:14851 [D loss: 0.657814, acc: 60.94%] [G loss: 1.962648]\n",
      "epoch:15 step:14852 [D loss: 0.669568, acc: 64.06%] [G loss: 1.931234]\n",
      "epoch:15 step:14853 [D loss: 0.678830, acc: 58.59%] [G loss: 1.957797]\n",
      "epoch:15 step:14854 [D loss: 0.653822, acc: 55.47%] [G loss: 1.937132]\n",
      "epoch:15 step:14855 [D loss: 0.658216, acc: 63.28%] [G loss: 1.852465]\n",
      "epoch:15 step:14856 [D loss: 0.639042, acc: 64.06%] [G loss: 1.896290]\n",
      "epoch:15 step:14857 [D loss: 0.699663, acc: 59.38%] [G loss: 1.867118]\n",
      "epoch:15 step:14858 [D loss: 0.659450, acc: 61.72%] [G loss: 1.929340]\n",
      "epoch:15 step:14859 [D loss: 0.650297, acc: 64.84%] [G loss: 1.978875]\n",
      "epoch:15 step:14860 [D loss: 0.619012, acc: 62.50%] [G loss: 2.080517]\n",
      "epoch:15 step:14861 [D loss: 0.640864, acc: 64.06%] [G loss: 2.056205]\n",
      "epoch:15 step:14862 [D loss: 0.615688, acc: 65.62%] [G loss: 1.933804]\n",
      "epoch:15 step:14863 [D loss: 0.644229, acc: 55.47%] [G loss: 2.095949]\n",
      "epoch:15 step:14864 [D loss: 0.648552, acc: 64.06%] [G loss: 1.987085]\n",
      "epoch:15 step:14865 [D loss: 0.628501, acc: 62.50%] [G loss: 1.725982]\n",
      "epoch:15 step:14866 [D loss: 0.631890, acc: 70.31%] [G loss: 1.938513]\n",
      "epoch:15 step:14867 [D loss: 0.622851, acc: 64.06%] [G loss: 1.996073]\n",
      "epoch:15 step:14868 [D loss: 0.663417, acc: 60.16%] [G loss: 1.947382]\n",
      "epoch:15 step:14869 [D loss: 0.626928, acc: 67.19%] [G loss: 2.063607]\n",
      "epoch:15 step:14870 [D loss: 0.654228, acc: 62.50%] [G loss: 2.225626]\n",
      "epoch:15 step:14871 [D loss: 0.674769, acc: 59.38%] [G loss: 1.989040]\n",
      "epoch:15 step:14872 [D loss: 0.684877, acc: 58.59%] [G loss: 1.824922]\n",
      "epoch:15 step:14873 [D loss: 0.711140, acc: 47.66%] [G loss: 1.757951]\n",
      "epoch:15 step:14874 [D loss: 0.672007, acc: 57.81%] [G loss: 1.788561]\n",
      "epoch:15 step:14875 [D loss: 0.728263, acc: 51.56%] [G loss: 1.726301]\n",
      "epoch:15 step:14876 [D loss: 0.641197, acc: 60.94%] [G loss: 1.807991]\n",
      "epoch:15 step:14877 [D loss: 0.625826, acc: 60.94%] [G loss: 1.981359]\n",
      "epoch:15 step:14878 [D loss: 0.653344, acc: 61.72%] [G loss: 1.926134]\n",
      "epoch:15 step:14879 [D loss: 0.666478, acc: 58.59%] [G loss: 1.819191]\n",
      "epoch:15 step:14880 [D loss: 0.612954, acc: 67.19%] [G loss: 1.877287]\n",
      "epoch:15 step:14881 [D loss: 0.683745, acc: 57.03%] [G loss: 1.888219]\n",
      "epoch:15 step:14882 [D loss: 0.700234, acc: 52.34%] [G loss: 1.764687]\n",
      "epoch:15 step:14883 [D loss: 0.686203, acc: 58.59%] [G loss: 1.635689]\n",
      "epoch:15 step:14884 [D loss: 0.682672, acc: 55.47%] [G loss: 1.797499]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:15 step:14885 [D loss: 0.637181, acc: 63.28%] [G loss: 1.657597]\n",
      "epoch:15 step:14886 [D loss: 0.654052, acc: 60.16%] [G loss: 2.003966]\n",
      "epoch:15 step:14887 [D loss: 0.640956, acc: 60.94%] [G loss: 1.792533]\n",
      "epoch:15 step:14888 [D loss: 0.611021, acc: 64.84%] [G loss: 1.841030]\n",
      "epoch:15 step:14889 [D loss: 0.648147, acc: 64.06%] [G loss: 1.909755]\n",
      "epoch:15 step:14890 [D loss: 0.646526, acc: 63.28%] [G loss: 1.778075]\n",
      "epoch:15 step:14891 [D loss: 0.614183, acc: 64.84%] [G loss: 1.897689]\n",
      "epoch:15 step:14892 [D loss: 0.625593, acc: 65.62%] [G loss: 1.848060]\n",
      "epoch:15 step:14893 [D loss: 0.657400, acc: 58.59%] [G loss: 1.799729]\n",
      "epoch:15 step:14894 [D loss: 0.588834, acc: 70.31%] [G loss: 1.803789]\n",
      "epoch:15 step:14895 [D loss: 0.671903, acc: 60.16%] [G loss: 1.838431]\n",
      "epoch:15 step:14896 [D loss: 0.596861, acc: 70.31%] [G loss: 1.979732]\n",
      "epoch:15 step:14897 [D loss: 0.620665, acc: 67.19%] [G loss: 2.031256]\n",
      "epoch:15 step:14898 [D loss: 0.633228, acc: 64.06%] [G loss: 1.985767]\n",
      "epoch:15 step:14899 [D loss: 0.630230, acc: 69.53%] [G loss: 1.931570]\n",
      "epoch:15 step:14900 [D loss: 0.667627, acc: 56.25%] [G loss: 1.874041]\n",
      "epoch:15 step:14901 [D loss: 0.700683, acc: 61.72%] [G loss: 1.899853]\n",
      "epoch:15 step:14902 [D loss: 0.672340, acc: 57.81%] [G loss: 1.856820]\n",
      "epoch:15 step:14903 [D loss: 0.620637, acc: 66.41%] [G loss: 1.824412]\n",
      "epoch:15 step:14904 [D loss: 0.633794, acc: 63.28%] [G loss: 1.954097]\n",
      "epoch:15 step:14905 [D loss: 0.690418, acc: 55.47%] [G loss: 1.902839]\n",
      "epoch:15 step:14906 [D loss: 0.720420, acc: 54.69%] [G loss: 1.751591]\n",
      "epoch:15 step:14907 [D loss: 0.619116, acc: 64.84%] [G loss: 1.872520]\n",
      "epoch:15 step:14908 [D loss: 0.664873, acc: 55.47%] [G loss: 2.008170]\n",
      "epoch:15 step:14909 [D loss: 0.666515, acc: 64.84%] [G loss: 1.873271]\n",
      "epoch:15 step:14910 [D loss: 0.661424, acc: 61.72%] [G loss: 1.890612]\n",
      "epoch:15 step:14911 [D loss: 0.686904, acc: 51.56%] [G loss: 1.760537]\n",
      "epoch:15 step:14912 [D loss: 0.625284, acc: 63.28%] [G loss: 1.987378]\n",
      "epoch:15 step:14913 [D loss: 0.683722, acc: 64.06%] [G loss: 1.917545]\n",
      "epoch:15 step:14914 [D loss: 0.710909, acc: 51.56%] [G loss: 1.830113]\n",
      "epoch:15 step:14915 [D loss: 0.638124, acc: 64.06%] [G loss: 1.793457]\n",
      "epoch:15 step:14916 [D loss: 0.632862, acc: 65.62%] [G loss: 1.771251]\n",
      "epoch:15 step:14917 [D loss: 0.662941, acc: 63.28%] [G loss: 1.778798]\n",
      "epoch:15 step:14918 [D loss: 0.616373, acc: 64.84%] [G loss: 1.858465]\n",
      "epoch:15 step:14919 [D loss: 0.626918, acc: 64.84%] [G loss: 1.874263]\n",
      "epoch:15 step:14920 [D loss: 0.683673, acc: 61.72%] [G loss: 1.734862]\n",
      "epoch:15 step:14921 [D loss: 0.656433, acc: 60.94%] [G loss: 1.746119]\n",
      "epoch:15 step:14922 [D loss: 0.701187, acc: 54.69%] [G loss: 1.822339]\n",
      "epoch:15 step:14923 [D loss: 0.611439, acc: 64.84%] [G loss: 1.827640]\n",
      "epoch:15 step:14924 [D loss: 0.632326, acc: 62.50%] [G loss: 1.820020]\n",
      "epoch:15 step:14925 [D loss: 0.622186, acc: 64.06%] [G loss: 1.855621]\n",
      "epoch:15 step:14926 [D loss: 0.642948, acc: 64.84%] [G loss: 1.995106]\n",
      "epoch:15 step:14927 [D loss: 0.671885, acc: 60.94%] [G loss: 1.804364]\n",
      "epoch:15 step:14928 [D loss: 0.635347, acc: 66.41%] [G loss: 1.784590]\n",
      "epoch:15 step:14929 [D loss: 0.659937, acc: 60.16%] [G loss: 1.747475]\n",
      "epoch:15 step:14930 [D loss: 0.614161, acc: 65.62%] [G loss: 2.008457]\n",
      "epoch:15 step:14931 [D loss: 0.646674, acc: 63.28%] [G loss: 1.764875]\n",
      "epoch:15 step:14932 [D loss: 0.646911, acc: 61.72%] [G loss: 1.828204]\n",
      "epoch:15 step:14933 [D loss: 0.665841, acc: 61.72%] [G loss: 1.966356]\n",
      "epoch:15 step:14934 [D loss: 0.724531, acc: 57.03%] [G loss: 1.927809]\n",
      "epoch:15 step:14935 [D loss: 0.666609, acc: 63.28%] [G loss: 1.772167]\n",
      "epoch:15 step:14936 [D loss: 0.670488, acc: 63.28%] [G loss: 1.887361]\n",
      "epoch:15 step:14937 [D loss: 0.624092, acc: 64.06%] [G loss: 1.928171]\n",
      "epoch:15 step:14938 [D loss: 0.623453, acc: 65.62%] [G loss: 1.878238]\n",
      "epoch:15 step:14939 [D loss: 0.617580, acc: 64.84%] [G loss: 1.896355]\n",
      "epoch:15 step:14940 [D loss: 0.669795, acc: 60.16%] [G loss: 1.801110]\n",
      "epoch:15 step:14941 [D loss: 0.611351, acc: 63.28%] [G loss: 2.011698]\n",
      "epoch:15 step:14942 [D loss: 0.631990, acc: 64.84%] [G loss: 2.066786]\n",
      "epoch:15 step:14943 [D loss: 0.667819, acc: 57.03%] [G loss: 2.020235]\n",
      "epoch:15 step:14944 [D loss: 0.622760, acc: 65.62%] [G loss: 1.874621]\n",
      "epoch:15 step:14945 [D loss: 0.597249, acc: 69.53%] [G loss: 1.959476]\n",
      "epoch:15 step:14946 [D loss: 0.646267, acc: 62.50%] [G loss: 1.727069]\n",
      "epoch:15 step:14947 [D loss: 0.701701, acc: 56.25%] [G loss: 1.917119]\n",
      "epoch:15 step:14948 [D loss: 0.734796, acc: 57.81%] [G loss: 1.908636]\n",
      "epoch:15 step:14949 [D loss: 0.557180, acc: 75.78%] [G loss: 2.114270]\n",
      "epoch:15 step:14950 [D loss: 0.693908, acc: 57.81%] [G loss: 1.873211]\n",
      "epoch:15 step:14951 [D loss: 0.696308, acc: 52.34%] [G loss: 1.848102]\n",
      "epoch:15 step:14952 [D loss: 0.640245, acc: 60.16%] [G loss: 1.913691]\n",
      "epoch:15 step:14953 [D loss: 0.638674, acc: 64.06%] [G loss: 1.893615]\n",
      "epoch:15 step:14954 [D loss: 0.573086, acc: 69.53%] [G loss: 2.090927]\n",
      "epoch:15 step:14955 [D loss: 0.627683, acc: 68.75%] [G loss: 2.113845]\n",
      "epoch:15 step:14956 [D loss: 0.629147, acc: 61.72%] [G loss: 1.974586]\n",
      "epoch:15 step:14957 [D loss: 0.634654, acc: 64.06%] [G loss: 1.921109]\n",
      "epoch:15 step:14958 [D loss: 0.660523, acc: 64.84%] [G loss: 1.916057]\n",
      "epoch:15 step:14959 [D loss: 0.624396, acc: 64.06%] [G loss: 2.022950]\n",
      "epoch:15 step:14960 [D loss: 0.592379, acc: 67.97%] [G loss: 2.013263]\n",
      "epoch:15 step:14961 [D loss: 0.669171, acc: 59.38%] [G loss: 1.980563]\n",
      "epoch:15 step:14962 [D loss: 0.614745, acc: 67.19%] [G loss: 2.093673]\n",
      "epoch:15 step:14963 [D loss: 0.611243, acc: 67.19%] [G loss: 1.878418]\n",
      "epoch:15 step:14964 [D loss: 0.625438, acc: 64.06%] [G loss: 1.978043]\n",
      "epoch:15 step:14965 [D loss: 0.661523, acc: 64.06%] [G loss: 1.944734]\n",
      "epoch:15 step:14966 [D loss: 0.589825, acc: 68.75%] [G loss: 1.973250]\n",
      "epoch:15 step:14967 [D loss: 0.603557, acc: 69.53%] [G loss: 2.168221]\n",
      "epoch:15 step:14968 [D loss: 0.755902, acc: 51.56%] [G loss: 1.934753]\n",
      "epoch:15 step:14969 [D loss: 0.688678, acc: 61.72%] [G loss: 1.849582]\n",
      "epoch:15 step:14970 [D loss: 0.632035, acc: 63.28%] [G loss: 1.887954]\n",
      "epoch:15 step:14971 [D loss: 0.620344, acc: 64.06%] [G loss: 1.970308]\n",
      "epoch:15 step:14972 [D loss: 0.707934, acc: 60.16%] [G loss: 1.916485]\n",
      "epoch:15 step:14973 [D loss: 0.575252, acc: 72.66%] [G loss: 2.110115]\n",
      "epoch:15 step:14974 [D loss: 0.531035, acc: 74.22%] [G loss: 2.196701]\n",
      "epoch:15 step:14975 [D loss: 0.759293, acc: 47.66%] [G loss: 1.870118]\n",
      "epoch:15 step:14976 [D loss: 0.604267, acc: 69.53%] [G loss: 1.918161]\n",
      "epoch:15 step:14977 [D loss: 0.671978, acc: 62.50%] [G loss: 1.992665]\n",
      "epoch:15 step:14978 [D loss: 0.610230, acc: 64.84%] [G loss: 2.040473]\n",
      "epoch:15 step:14979 [D loss: 0.595196, acc: 69.53%] [G loss: 2.227352]\n",
      "epoch:15 step:14980 [D loss: 0.611239, acc: 66.41%] [G loss: 2.167446]\n",
      "epoch:15 step:14981 [D loss: 0.623528, acc: 73.44%] [G loss: 2.083299]\n",
      "epoch:15 step:14982 [D loss: 0.600849, acc: 64.84%] [G loss: 2.140312]\n",
      "epoch:15 step:14983 [D loss: 0.740422, acc: 48.44%] [G loss: 1.886339]\n",
      "epoch:15 step:14984 [D loss: 0.812273, acc: 39.84%] [G loss: 1.702293]\n",
      "epoch:15 step:14985 [D loss: 0.566240, acc: 67.19%] [G loss: 2.074674]\n",
      "epoch:15 step:14986 [D loss: 0.606468, acc: 68.75%] [G loss: 2.085483]\n",
      "epoch:15 step:14987 [D loss: 0.639000, acc: 64.84%] [G loss: 1.930718]\n",
      "epoch:15 step:14988 [D loss: 0.631172, acc: 66.41%] [G loss: 1.976584]\n",
      "epoch:15 step:14989 [D loss: 0.607071, acc: 67.19%] [G loss: 2.098506]\n",
      "epoch:15 step:14990 [D loss: 0.574357, acc: 75.78%] [G loss: 2.114872]\n",
      "epoch:15 step:14991 [D loss: 0.577860, acc: 66.41%] [G loss: 2.260351]\n",
      "epoch:15 step:14992 [D loss: 0.614162, acc: 67.97%] [G loss: 2.308316]\n",
      "epoch:16 step:14993 [D loss: 0.684856, acc: 65.62%] [G loss: 1.781620]\n",
      "epoch:16 step:14994 [D loss: 0.695520, acc: 55.47%] [G loss: 1.943817]\n",
      "epoch:16 step:14995 [D loss: 0.687354, acc: 57.03%] [G loss: 1.840955]\n",
      "epoch:16 step:14996 [D loss: 0.634482, acc: 65.62%] [G loss: 1.980383]\n",
      "epoch:16 step:14997 [D loss: 0.646060, acc: 64.06%] [G loss: 1.838276]\n",
      "epoch:16 step:14998 [D loss: 0.672332, acc: 62.50%] [G loss: 1.798706]\n",
      "epoch:16 step:14999 [D loss: 0.617134, acc: 67.97%] [G loss: 1.908329]\n",
      "epoch:16 step:15000 [D loss: 0.649340, acc: 64.84%] [G loss: 1.914417]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##############\n",
      "[2.36059017 1.44118362 6.16881383 4.66849136 3.71531302 5.44103866\n",
      " 4.44076446 4.62222237 4.55135964 3.65946632]\n",
      "##########\n",
      "epoch:16 step:15001 [D loss: 0.695360, acc: 58.59%] [G loss: 1.968977]\n",
      "epoch:16 step:15002 [D loss: 0.576155, acc: 69.53%] [G loss: 2.055745]\n",
      "epoch:16 step:15003 [D loss: 0.639125, acc: 60.16%] [G loss: 2.043693]\n",
      "epoch:16 step:15004 [D loss: 0.710897, acc: 57.03%] [G loss: 1.958652]\n",
      "epoch:16 step:15005 [D loss: 0.645089, acc: 65.62%] [G loss: 2.113676]\n",
      "epoch:16 step:15006 [D loss: 0.599071, acc: 67.97%] [G loss: 1.984710]\n",
      "epoch:16 step:15007 [D loss: 0.589454, acc: 70.31%] [G loss: 2.071858]\n",
      "epoch:16 step:15008 [D loss: 0.604141, acc: 66.41%] [G loss: 2.151955]\n",
      "epoch:16 step:15009 [D loss: 0.646896, acc: 63.28%] [G loss: 2.035349]\n",
      "epoch:16 step:15010 [D loss: 0.598385, acc: 70.31%] [G loss: 1.890068]\n",
      "epoch:16 step:15011 [D loss: 0.683016, acc: 54.69%] [G loss: 1.879659]\n",
      "epoch:16 step:15012 [D loss: 0.663268, acc: 59.38%] [G loss: 1.826693]\n",
      "epoch:16 step:15013 [D loss: 0.683625, acc: 58.59%] [G loss: 1.858797]\n",
      "epoch:16 step:15014 [D loss: 0.677003, acc: 61.72%] [G loss: 1.721234]\n",
      "epoch:16 step:15015 [D loss: 0.625663, acc: 68.75%] [G loss: 2.065061]\n",
      "epoch:16 step:15016 [D loss: 0.615954, acc: 65.62%] [G loss: 1.854082]\n",
      "epoch:16 step:15017 [D loss: 0.577075, acc: 72.66%] [G loss: 1.976733]\n",
      "epoch:16 step:15018 [D loss: 0.679790, acc: 56.25%] [G loss: 1.767556]\n",
      "epoch:16 step:15019 [D loss: 0.654818, acc: 58.59%] [G loss: 1.876112]\n",
      "epoch:16 step:15020 [D loss: 0.595742, acc: 69.53%] [G loss: 1.861672]\n",
      "epoch:16 step:15021 [D loss: 0.629713, acc: 71.09%] [G loss: 1.924061]\n",
      "epoch:16 step:15022 [D loss: 0.631125, acc: 64.84%] [G loss: 1.751073]\n",
      "epoch:16 step:15023 [D loss: 0.719787, acc: 53.91%] [G loss: 1.921059]\n",
      "epoch:16 step:15024 [D loss: 0.625642, acc: 67.19%] [G loss: 1.759136]\n",
      "epoch:16 step:15025 [D loss: 0.615099, acc: 67.97%] [G loss: 1.885452]\n",
      "epoch:16 step:15026 [D loss: 0.704900, acc: 50.00%] [G loss: 1.823687]\n",
      "epoch:16 step:15027 [D loss: 0.649590, acc: 67.19%] [G loss: 1.823321]\n",
      "epoch:16 step:15028 [D loss: 0.603047, acc: 67.97%] [G loss: 1.960556]\n",
      "epoch:16 step:15029 [D loss: 0.602619, acc: 71.88%] [G loss: 2.069045]\n",
      "epoch:16 step:15030 [D loss: 0.646826, acc: 66.41%] [G loss: 1.976847]\n",
      "epoch:16 step:15031 [D loss: 0.660715, acc: 63.28%] [G loss: 2.075770]\n",
      "epoch:16 step:15032 [D loss: 0.596131, acc: 70.31%] [G loss: 2.194722]\n",
      "epoch:16 step:15033 [D loss: 0.653382, acc: 60.94%] [G loss: 1.940636]\n",
      "epoch:16 step:15034 [D loss: 0.642471, acc: 62.50%] [G loss: 2.177117]\n",
      "epoch:16 step:15035 [D loss: 0.654971, acc: 62.50%] [G loss: 1.952769]\n",
      "epoch:16 step:15036 [D loss: 0.659642, acc: 59.38%] [G loss: 1.839150]\n",
      "epoch:16 step:15037 [D loss: 0.668432, acc: 63.28%] [G loss: 1.857323]\n",
      "epoch:16 step:15038 [D loss: 0.628596, acc: 62.50%] [G loss: 1.881756]\n",
      "epoch:16 step:15039 [D loss: 0.630262, acc: 61.72%] [G loss: 1.868830]\n",
      "epoch:16 step:15040 [D loss: 0.628641, acc: 63.28%] [G loss: 1.998176]\n",
      "epoch:16 step:15041 [D loss: 0.622928, acc: 63.28%] [G loss: 2.030732]\n",
      "epoch:16 step:15042 [D loss: 0.610381, acc: 66.41%] [G loss: 2.046999]\n",
      "epoch:16 step:15043 [D loss: 0.666085, acc: 61.72%] [G loss: 2.017406]\n",
      "epoch:16 step:15044 [D loss: 0.737933, acc: 57.81%] [G loss: 1.893132]\n",
      "epoch:16 step:15045 [D loss: 0.654360, acc: 64.06%] [G loss: 1.869257]\n",
      "epoch:16 step:15046 [D loss: 0.649092, acc: 61.72%] [G loss: 1.905247]\n",
      "epoch:16 step:15047 [D loss: 0.633496, acc: 64.06%] [G loss: 2.024922]\n",
      "epoch:16 step:15048 [D loss: 0.630974, acc: 67.19%] [G loss: 1.912220]\n",
      "epoch:16 step:15049 [D loss: 0.694121, acc: 53.12%] [G loss: 1.855426]\n",
      "epoch:16 step:15050 [D loss: 0.647627, acc: 58.59%] [G loss: 1.771434]\n",
      "epoch:16 step:15051 [D loss: 0.636261, acc: 66.41%] [G loss: 1.811847]\n",
      "epoch:16 step:15052 [D loss: 0.675888, acc: 62.50%] [G loss: 1.841495]\n",
      "epoch:16 step:15053 [D loss: 0.655571, acc: 66.41%] [G loss: 1.761147]\n",
      "epoch:16 step:15054 [D loss: 0.647168, acc: 62.50%] [G loss: 1.925238]\n",
      "epoch:16 step:15055 [D loss: 0.644484, acc: 64.06%] [G loss: 1.729504]\n",
      "epoch:16 step:15056 [D loss: 0.685339, acc: 56.25%] [G loss: 1.923943]\n",
      "epoch:16 step:15057 [D loss: 0.635948, acc: 62.50%] [G loss: 1.930825]\n",
      "epoch:16 step:15058 [D loss: 0.641592, acc: 61.72%] [G loss: 1.910877]\n",
      "epoch:16 step:15059 [D loss: 0.645343, acc: 64.06%] [G loss: 1.780431]\n",
      "epoch:16 step:15060 [D loss: 0.653826, acc: 59.38%] [G loss: 1.777329]\n",
      "epoch:16 step:15061 [D loss: 0.648977, acc: 66.41%] [G loss: 1.995450]\n",
      "epoch:16 step:15062 [D loss: 0.641307, acc: 62.50%] [G loss: 1.935405]\n",
      "epoch:16 step:15063 [D loss: 0.623732, acc: 66.41%] [G loss: 1.830924]\n",
      "epoch:16 step:15064 [D loss: 0.617235, acc: 68.75%] [G loss: 1.895521]\n",
      "epoch:16 step:15065 [D loss: 0.633080, acc: 61.72%] [G loss: 1.845985]\n",
      "epoch:16 step:15066 [D loss: 0.641219, acc: 62.50%] [G loss: 1.834301]\n",
      "epoch:16 step:15067 [D loss: 0.643036, acc: 61.72%] [G loss: 1.913208]\n",
      "epoch:16 step:15068 [D loss: 0.667375, acc: 58.59%] [G loss: 2.033914]\n",
      "epoch:16 step:15069 [D loss: 0.635002, acc: 64.06%] [G loss: 2.049418]\n",
      "epoch:16 step:15070 [D loss: 0.690459, acc: 57.03%] [G loss: 1.813206]\n",
      "epoch:16 step:15071 [D loss: 0.603265, acc: 73.44%] [G loss: 1.793855]\n",
      "epoch:16 step:15072 [D loss: 0.619758, acc: 62.50%] [G loss: 1.836193]\n",
      "epoch:16 step:15073 [D loss: 0.749358, acc: 52.34%] [G loss: 1.759889]\n",
      "epoch:16 step:15074 [D loss: 0.624031, acc: 64.06%] [G loss: 1.832363]\n",
      "epoch:16 step:15075 [D loss: 0.607117, acc: 67.19%] [G loss: 1.984518]\n",
      "epoch:16 step:15076 [D loss: 0.668867, acc: 60.16%] [G loss: 2.047896]\n",
      "epoch:16 step:15077 [D loss: 0.647101, acc: 59.38%] [G loss: 1.800352]\n",
      "epoch:16 step:15078 [D loss: 0.652885, acc: 63.28%] [G loss: 1.794995]\n",
      "epoch:16 step:15079 [D loss: 0.635191, acc: 64.84%] [G loss: 1.964311]\n",
      "epoch:16 step:15080 [D loss: 0.621543, acc: 67.97%] [G loss: 2.013695]\n",
      "epoch:16 step:15081 [D loss: 0.644367, acc: 67.97%] [G loss: 1.996916]\n",
      "epoch:16 step:15082 [D loss: 0.656751, acc: 57.81%] [G loss: 1.822340]\n",
      "epoch:16 step:15083 [D loss: 0.700966, acc: 54.69%] [G loss: 1.850775]\n",
      "epoch:16 step:15084 [D loss: 0.635337, acc: 65.62%] [G loss: 1.919081]\n",
      "epoch:16 step:15085 [D loss: 0.594332, acc: 71.88%] [G loss: 2.082255]\n",
      "epoch:16 step:15086 [D loss: 0.648173, acc: 64.06%] [G loss: 1.985533]\n",
      "epoch:16 step:15087 [D loss: 0.647051, acc: 64.06%] [G loss: 1.791117]\n",
      "epoch:16 step:15088 [D loss: 0.622726, acc: 67.97%] [G loss: 1.829541]\n",
      "epoch:16 step:15089 [D loss: 0.644703, acc: 63.28%] [G loss: 1.789705]\n",
      "epoch:16 step:15090 [D loss: 0.719480, acc: 55.47%] [G loss: 1.762779]\n",
      "epoch:16 step:15091 [D loss: 0.680676, acc: 57.03%] [G loss: 1.822904]\n",
      "epoch:16 step:15092 [D loss: 0.640507, acc: 64.84%] [G loss: 2.047712]\n",
      "epoch:16 step:15093 [D loss: 0.618421, acc: 64.06%] [G loss: 2.025003]\n",
      "epoch:16 step:15094 [D loss: 0.645950, acc: 58.59%] [G loss: 1.830658]\n",
      "epoch:16 step:15095 [D loss: 0.594137, acc: 73.44%] [G loss: 1.980286]\n",
      "epoch:16 step:15096 [D loss: 0.702799, acc: 56.25%] [G loss: 1.931789]\n",
      "epoch:16 step:15097 [D loss: 0.682119, acc: 59.38%] [G loss: 1.731401]\n",
      "epoch:16 step:15098 [D loss: 0.659901, acc: 57.81%] [G loss: 2.006501]\n",
      "epoch:16 step:15099 [D loss: 0.570473, acc: 72.66%] [G loss: 1.976103]\n",
      "epoch:16 step:15100 [D loss: 0.651990, acc: 61.72%] [G loss: 1.814375]\n",
      "epoch:16 step:15101 [D loss: 0.640570, acc: 64.06%] [G loss: 1.815803]\n",
      "epoch:16 step:15102 [D loss: 0.630660, acc: 66.41%] [G loss: 1.959156]\n",
      "epoch:16 step:15103 [D loss: 0.640531, acc: 67.97%] [G loss: 1.859791]\n",
      "epoch:16 step:15104 [D loss: 0.630609, acc: 65.62%] [G loss: 1.934705]\n",
      "epoch:16 step:15105 [D loss: 0.608207, acc: 67.19%] [G loss: 1.864155]\n",
      "epoch:16 step:15106 [D loss: 0.632830, acc: 66.41%] [G loss: 1.893014]\n",
      "epoch:16 step:15107 [D loss: 0.590400, acc: 68.75%] [G loss: 1.981385]\n",
      "epoch:16 step:15108 [D loss: 0.597480, acc: 74.22%] [G loss: 2.122098]\n",
      "epoch:16 step:15109 [D loss: 0.623872, acc: 61.72%] [G loss: 2.088656]\n",
      "epoch:16 step:15110 [D loss: 0.660488, acc: 55.47%] [G loss: 1.943063]\n",
      "epoch:16 step:15111 [D loss: 0.634826, acc: 65.62%] [G loss: 2.357921]\n",
      "epoch:16 step:15112 [D loss: 0.696085, acc: 53.91%] [G loss: 1.907723]\n",
      "epoch:16 step:15113 [D loss: 0.656870, acc: 64.84%] [G loss: 2.047368]\n",
      "epoch:16 step:15114 [D loss: 0.659784, acc: 62.50%] [G loss: 1.995276]\n",
      "epoch:16 step:15115 [D loss: 0.664183, acc: 60.16%] [G loss: 1.986595]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:16 step:15116 [D loss: 0.710160, acc: 54.69%] [G loss: 1.853577]\n",
      "epoch:16 step:15117 [D loss: 0.698216, acc: 53.91%] [G loss: 1.643334]\n",
      "epoch:16 step:15118 [D loss: 0.629663, acc: 63.28%] [G loss: 1.865360]\n",
      "epoch:16 step:15119 [D loss: 0.677381, acc: 57.03%] [G loss: 1.841186]\n",
      "epoch:16 step:15120 [D loss: 0.663485, acc: 61.72%] [G loss: 1.837523]\n",
      "epoch:16 step:15121 [D loss: 0.667954, acc: 62.50%] [G loss: 1.809678]\n",
      "epoch:16 step:15122 [D loss: 0.624460, acc: 67.19%] [G loss: 1.933592]\n",
      "epoch:16 step:15123 [D loss: 0.691952, acc: 54.69%] [G loss: 1.853572]\n",
      "epoch:16 step:15124 [D loss: 0.614779, acc: 65.62%] [G loss: 1.802293]\n",
      "epoch:16 step:15125 [D loss: 0.635249, acc: 67.19%] [G loss: 1.851860]\n",
      "epoch:16 step:15126 [D loss: 0.663679, acc: 63.28%] [G loss: 1.842273]\n",
      "epoch:16 step:15127 [D loss: 0.657508, acc: 62.50%] [G loss: 1.929037]\n",
      "epoch:16 step:15128 [D loss: 0.673601, acc: 56.25%] [G loss: 1.848296]\n",
      "epoch:16 step:15129 [D loss: 0.597048, acc: 67.19%] [G loss: 1.813513]\n",
      "epoch:16 step:15130 [D loss: 0.662363, acc: 59.38%] [G loss: 1.849692]\n",
      "epoch:16 step:15131 [D loss: 0.675444, acc: 54.69%] [G loss: 1.833210]\n",
      "epoch:16 step:15132 [D loss: 0.650181, acc: 60.16%] [G loss: 1.924120]\n",
      "epoch:16 step:15133 [D loss: 0.634532, acc: 67.97%] [G loss: 1.811759]\n",
      "epoch:16 step:15134 [D loss: 0.637118, acc: 59.38%] [G loss: 1.891561]\n",
      "epoch:16 step:15135 [D loss: 0.654303, acc: 59.38%] [G loss: 1.784596]\n",
      "epoch:16 step:15136 [D loss: 0.634706, acc: 58.59%] [G loss: 1.929294]\n",
      "epoch:16 step:15137 [D loss: 0.655682, acc: 56.25%] [G loss: 1.885683]\n",
      "epoch:16 step:15138 [D loss: 0.609713, acc: 71.09%] [G loss: 1.825276]\n",
      "epoch:16 step:15139 [D loss: 0.651022, acc: 60.94%] [G loss: 1.938619]\n",
      "epoch:16 step:15140 [D loss: 0.693193, acc: 57.81%] [G loss: 1.775261]\n",
      "epoch:16 step:15141 [D loss: 0.711123, acc: 60.94%] [G loss: 1.718288]\n",
      "epoch:16 step:15142 [D loss: 0.656417, acc: 62.50%] [G loss: 1.829350]\n",
      "epoch:16 step:15143 [D loss: 0.594102, acc: 66.41%] [G loss: 1.961609]\n",
      "epoch:16 step:15144 [D loss: 0.656390, acc: 60.94%] [G loss: 1.961584]\n",
      "epoch:16 step:15145 [D loss: 0.623409, acc: 58.59%] [G loss: 1.803592]\n",
      "epoch:16 step:15146 [D loss: 0.654723, acc: 62.50%] [G loss: 2.059448]\n",
      "epoch:16 step:15147 [D loss: 0.610652, acc: 62.50%] [G loss: 1.955047]\n",
      "epoch:16 step:15148 [D loss: 0.589346, acc: 67.97%] [G loss: 1.910304]\n",
      "epoch:16 step:15149 [D loss: 0.658119, acc: 61.72%] [G loss: 1.903799]\n",
      "epoch:16 step:15150 [D loss: 0.705246, acc: 60.16%] [G loss: 1.889212]\n",
      "epoch:16 step:15151 [D loss: 0.648673, acc: 62.50%] [G loss: 1.856165]\n",
      "epoch:16 step:15152 [D loss: 0.646977, acc: 57.03%] [G loss: 1.897098]\n",
      "epoch:16 step:15153 [D loss: 0.678084, acc: 60.94%] [G loss: 1.922955]\n",
      "epoch:16 step:15154 [D loss: 0.651990, acc: 58.59%] [G loss: 1.882633]\n",
      "epoch:16 step:15155 [D loss: 0.643951, acc: 59.38%] [G loss: 1.949988]\n",
      "epoch:16 step:15156 [D loss: 0.620385, acc: 60.16%] [G loss: 1.844296]\n",
      "epoch:16 step:15157 [D loss: 0.675149, acc: 55.47%] [G loss: 1.840159]\n",
      "epoch:16 step:15158 [D loss: 0.635300, acc: 66.41%] [G loss: 1.800767]\n",
      "epoch:16 step:15159 [D loss: 0.596480, acc: 71.09%] [G loss: 1.925907]\n",
      "epoch:16 step:15160 [D loss: 0.604867, acc: 68.75%] [G loss: 1.948149]\n",
      "epoch:16 step:15161 [D loss: 0.625650, acc: 67.19%] [G loss: 1.842571]\n",
      "epoch:16 step:15162 [D loss: 0.666998, acc: 64.84%] [G loss: 1.867020]\n",
      "epoch:16 step:15163 [D loss: 0.640932, acc: 67.19%] [G loss: 1.872896]\n",
      "epoch:16 step:15164 [D loss: 0.709654, acc: 56.25%] [G loss: 1.867651]\n",
      "epoch:16 step:15165 [D loss: 0.667604, acc: 53.12%] [G loss: 1.641758]\n",
      "epoch:16 step:15166 [D loss: 0.653346, acc: 66.41%] [G loss: 1.863535]\n",
      "epoch:16 step:15167 [D loss: 0.637297, acc: 59.38%] [G loss: 1.842992]\n",
      "epoch:16 step:15168 [D loss: 0.672774, acc: 54.69%] [G loss: 1.822724]\n",
      "epoch:16 step:15169 [D loss: 0.634371, acc: 65.62%] [G loss: 1.804237]\n",
      "epoch:16 step:15170 [D loss: 0.616636, acc: 65.62%] [G loss: 1.839057]\n",
      "epoch:16 step:15171 [D loss: 0.656031, acc: 60.16%] [G loss: 1.792053]\n",
      "epoch:16 step:15172 [D loss: 0.673108, acc: 63.28%] [G loss: 1.819701]\n",
      "epoch:16 step:15173 [D loss: 0.651097, acc: 61.72%] [G loss: 1.825409]\n",
      "epoch:16 step:15174 [D loss: 0.726711, acc: 54.69%] [G loss: 1.777768]\n",
      "epoch:16 step:15175 [D loss: 0.674172, acc: 60.16%] [G loss: 1.772576]\n",
      "epoch:16 step:15176 [D loss: 0.583032, acc: 71.88%] [G loss: 2.013287]\n",
      "epoch:16 step:15177 [D loss: 0.637531, acc: 65.62%] [G loss: 1.841668]\n",
      "epoch:16 step:15178 [D loss: 0.667821, acc: 54.69%] [G loss: 1.858609]\n",
      "epoch:16 step:15179 [D loss: 0.649327, acc: 61.72%] [G loss: 1.964025]\n",
      "epoch:16 step:15180 [D loss: 0.647940, acc: 64.06%] [G loss: 1.799497]\n",
      "epoch:16 step:15181 [D loss: 0.670100, acc: 62.50%] [G loss: 1.783654]\n",
      "epoch:16 step:15182 [D loss: 0.676468, acc: 60.16%] [G loss: 1.749121]\n",
      "epoch:16 step:15183 [D loss: 0.627117, acc: 60.94%] [G loss: 1.861110]\n",
      "epoch:16 step:15184 [D loss: 0.643104, acc: 65.62%] [G loss: 2.021107]\n",
      "epoch:16 step:15185 [D loss: 0.648696, acc: 64.06%] [G loss: 1.849180]\n",
      "epoch:16 step:15186 [D loss: 0.627496, acc: 66.41%] [G loss: 2.016799]\n",
      "epoch:16 step:15187 [D loss: 0.673643, acc: 60.16%] [G loss: 1.908603]\n",
      "epoch:16 step:15188 [D loss: 0.635631, acc: 65.62%] [G loss: 1.835884]\n",
      "epoch:16 step:15189 [D loss: 0.636582, acc: 68.75%] [G loss: 1.961900]\n",
      "epoch:16 step:15190 [D loss: 0.631368, acc: 67.19%] [G loss: 1.901357]\n",
      "epoch:16 step:15191 [D loss: 0.669550, acc: 61.72%] [G loss: 1.935065]\n",
      "epoch:16 step:15192 [D loss: 0.670397, acc: 59.38%] [G loss: 1.748945]\n",
      "epoch:16 step:15193 [D loss: 0.668648, acc: 57.81%] [G loss: 1.858433]\n",
      "epoch:16 step:15194 [D loss: 0.723683, acc: 55.47%] [G loss: 1.783520]\n",
      "epoch:16 step:15195 [D loss: 0.658680, acc: 61.72%] [G loss: 1.914837]\n",
      "epoch:16 step:15196 [D loss: 0.640252, acc: 60.94%] [G loss: 1.757561]\n",
      "epoch:16 step:15197 [D loss: 0.679072, acc: 57.03%] [G loss: 1.867320]\n",
      "epoch:16 step:15198 [D loss: 0.595641, acc: 70.31%] [G loss: 2.018966]\n",
      "epoch:16 step:15199 [D loss: 0.580267, acc: 72.66%] [G loss: 2.088299]\n",
      "epoch:16 step:15200 [D loss: 0.602904, acc: 64.84%] [G loss: 2.207851]\n",
      "##############\n",
      "[2.43136774 1.28693374 6.33594433 4.80374425 3.55561727 5.68102275\n",
      " 4.34460316 4.64984769 4.54635626 3.51720926]\n",
      "##########\n",
      "epoch:16 step:15201 [D loss: 0.573383, acc: 66.41%] [G loss: 2.247107]\n",
      "epoch:16 step:15202 [D loss: 0.672963, acc: 55.47%] [G loss: 1.853269]\n",
      "epoch:16 step:15203 [D loss: 0.657403, acc: 61.72%] [G loss: 1.698541]\n",
      "epoch:16 step:15204 [D loss: 0.701417, acc: 55.47%] [G loss: 1.716686]\n",
      "epoch:16 step:15205 [D loss: 0.671092, acc: 62.50%] [G loss: 1.790002]\n",
      "epoch:16 step:15206 [D loss: 0.686204, acc: 58.59%] [G loss: 1.807484]\n",
      "epoch:16 step:15207 [D loss: 0.690375, acc: 60.94%] [G loss: 1.821969]\n",
      "epoch:16 step:15208 [D loss: 0.638946, acc: 59.38%] [G loss: 1.915315]\n",
      "epoch:16 step:15209 [D loss: 0.612651, acc: 71.88%] [G loss: 1.926128]\n",
      "epoch:16 step:15210 [D loss: 0.560965, acc: 71.09%] [G loss: 2.199221]\n",
      "epoch:16 step:15211 [D loss: 0.543414, acc: 74.22%] [G loss: 2.036511]\n",
      "epoch:16 step:15212 [D loss: 0.741898, acc: 51.56%] [G loss: 1.744394]\n",
      "epoch:16 step:15213 [D loss: 0.694291, acc: 57.81%] [G loss: 2.069947]\n",
      "epoch:16 step:15214 [D loss: 0.718930, acc: 57.81%] [G loss: 1.839740]\n",
      "epoch:16 step:15215 [D loss: 0.644710, acc: 56.25%] [G loss: 1.817572]\n",
      "epoch:16 step:15216 [D loss: 0.652263, acc: 62.50%] [G loss: 1.776576]\n",
      "epoch:16 step:15217 [D loss: 0.664082, acc: 61.72%] [G loss: 1.927271]\n",
      "epoch:16 step:15218 [D loss: 0.653714, acc: 60.94%] [G loss: 1.813848]\n",
      "epoch:16 step:15219 [D loss: 0.619979, acc: 67.97%] [G loss: 1.792666]\n",
      "epoch:16 step:15220 [D loss: 0.638258, acc: 61.72%] [G loss: 1.722926]\n",
      "epoch:16 step:15221 [D loss: 0.599584, acc: 68.75%] [G loss: 2.019267]\n",
      "epoch:16 step:15222 [D loss: 0.588889, acc: 67.97%] [G loss: 1.994947]\n",
      "epoch:16 step:15223 [D loss: 0.607363, acc: 67.19%] [G loss: 2.402223]\n",
      "epoch:16 step:15224 [D loss: 0.564328, acc: 75.78%] [G loss: 2.436850]\n",
      "epoch:16 step:15225 [D loss: 0.654481, acc: 57.81%] [G loss: 1.799505]\n",
      "epoch:16 step:15226 [D loss: 0.644853, acc: 60.94%] [G loss: 1.867520]\n",
      "epoch:16 step:15227 [D loss: 0.691003, acc: 60.16%] [G loss: 1.802736]\n",
      "epoch:16 step:15228 [D loss: 0.657682, acc: 61.72%] [G loss: 1.856359]\n",
      "epoch:16 step:15229 [D loss: 0.648950, acc: 61.72%] [G loss: 1.991111]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:16 step:15230 [D loss: 0.665196, acc: 64.06%] [G loss: 1.848326]\n",
      "epoch:16 step:15231 [D loss: 0.657802, acc: 60.94%] [G loss: 1.787917]\n",
      "epoch:16 step:15232 [D loss: 0.600000, acc: 66.41%] [G loss: 1.886397]\n",
      "epoch:16 step:15233 [D loss: 0.608695, acc: 69.53%] [G loss: 2.076263]\n",
      "epoch:16 step:15234 [D loss: 0.565534, acc: 75.78%] [G loss: 2.028958]\n",
      "epoch:16 step:15235 [D loss: 0.659231, acc: 63.28%] [G loss: 2.041100]\n",
      "epoch:16 step:15236 [D loss: 0.661454, acc: 65.62%] [G loss: 2.007133]\n",
      "epoch:16 step:15237 [D loss: 0.619254, acc: 66.41%] [G loss: 2.007228]\n",
      "epoch:16 step:15238 [D loss: 0.636087, acc: 66.41%] [G loss: 1.867885]\n",
      "epoch:16 step:15239 [D loss: 0.639022, acc: 61.72%] [G loss: 2.024941]\n",
      "epoch:16 step:15240 [D loss: 0.601309, acc: 71.88%] [G loss: 1.930055]\n",
      "epoch:16 step:15241 [D loss: 0.671201, acc: 63.28%] [G loss: 1.845369]\n",
      "epoch:16 step:15242 [D loss: 0.694514, acc: 56.25%] [G loss: 1.760321]\n",
      "epoch:16 step:15243 [D loss: 0.650978, acc: 60.16%] [G loss: 1.599914]\n",
      "epoch:16 step:15244 [D loss: 0.663820, acc: 59.38%] [G loss: 1.761758]\n",
      "epoch:16 step:15245 [D loss: 0.634714, acc: 62.50%] [G loss: 1.803524]\n",
      "epoch:16 step:15246 [D loss: 0.618732, acc: 67.97%] [G loss: 1.900902]\n",
      "epoch:16 step:15247 [D loss: 0.658399, acc: 60.94%] [G loss: 1.789322]\n",
      "epoch:16 step:15248 [D loss: 0.655324, acc: 58.59%] [G loss: 1.910577]\n",
      "epoch:16 step:15249 [D loss: 0.650751, acc: 66.41%] [G loss: 1.858142]\n",
      "epoch:16 step:15250 [D loss: 0.683758, acc: 58.59%] [G loss: 1.960963]\n",
      "epoch:16 step:15251 [D loss: 0.632044, acc: 60.16%] [G loss: 1.857784]\n",
      "epoch:16 step:15252 [D loss: 0.626081, acc: 63.28%] [G loss: 1.838310]\n",
      "epoch:16 step:15253 [D loss: 0.664864, acc: 60.94%] [G loss: 1.877410]\n",
      "epoch:16 step:15254 [D loss: 0.612589, acc: 67.97%] [G loss: 1.980473]\n",
      "epoch:16 step:15255 [D loss: 0.673151, acc: 64.06%] [G loss: 2.005396]\n",
      "epoch:16 step:15256 [D loss: 0.653102, acc: 60.16%] [G loss: 2.258725]\n",
      "epoch:16 step:15257 [D loss: 0.670636, acc: 58.59%] [G loss: 1.897197]\n",
      "epoch:16 step:15258 [D loss: 0.675183, acc: 61.72%] [G loss: 2.051829]\n",
      "epoch:16 step:15259 [D loss: 0.627689, acc: 60.16%] [G loss: 1.831849]\n",
      "epoch:16 step:15260 [D loss: 0.665329, acc: 60.16%] [G loss: 1.843029]\n",
      "epoch:16 step:15261 [D loss: 0.666317, acc: 61.72%] [G loss: 2.011076]\n",
      "epoch:16 step:15262 [D loss: 0.607690, acc: 67.97%] [G loss: 1.884262]\n",
      "epoch:16 step:15263 [D loss: 0.685970, acc: 53.91%] [G loss: 1.915527]\n",
      "epoch:16 step:15264 [D loss: 0.616307, acc: 67.19%] [G loss: 1.959046]\n",
      "epoch:16 step:15265 [D loss: 0.689332, acc: 54.69%] [G loss: 1.866978]\n",
      "epoch:16 step:15266 [D loss: 0.607120, acc: 65.62%] [G loss: 2.038267]\n",
      "epoch:16 step:15267 [D loss: 0.675919, acc: 56.25%] [G loss: 1.972042]\n",
      "epoch:16 step:15268 [D loss: 0.643734, acc: 62.50%] [G loss: 1.982937]\n",
      "epoch:16 step:15269 [D loss: 0.718864, acc: 57.81%] [G loss: 1.907205]\n",
      "epoch:16 step:15270 [D loss: 0.658579, acc: 65.62%] [G loss: 1.748834]\n",
      "epoch:16 step:15271 [D loss: 0.664094, acc: 58.59%] [G loss: 1.926962]\n",
      "epoch:16 step:15272 [D loss: 0.674647, acc: 58.59%] [G loss: 1.822652]\n",
      "epoch:16 step:15273 [D loss: 0.681198, acc: 55.47%] [G loss: 1.908341]\n",
      "epoch:16 step:15274 [D loss: 0.625186, acc: 66.41%] [G loss: 1.843977]\n",
      "epoch:16 step:15275 [D loss: 0.690989, acc: 59.38%] [G loss: 1.874182]\n",
      "epoch:16 step:15276 [D loss: 0.644320, acc: 58.59%] [G loss: 1.823223]\n",
      "epoch:16 step:15277 [D loss: 0.634021, acc: 62.50%] [G loss: 1.862659]\n",
      "epoch:16 step:15278 [D loss: 0.641757, acc: 64.84%] [G loss: 1.909837]\n",
      "epoch:16 step:15279 [D loss: 0.660019, acc: 59.38%] [G loss: 1.856339]\n",
      "epoch:16 step:15280 [D loss: 0.596701, acc: 69.53%] [G loss: 1.861743]\n",
      "epoch:16 step:15281 [D loss: 0.663139, acc: 62.50%] [G loss: 1.842608]\n",
      "epoch:16 step:15282 [D loss: 0.721680, acc: 52.34%] [G loss: 1.856775]\n",
      "epoch:16 step:15283 [D loss: 0.665630, acc: 61.72%] [G loss: 1.752715]\n",
      "epoch:16 step:15284 [D loss: 0.680459, acc: 62.50%] [G loss: 1.802911]\n",
      "epoch:16 step:15285 [D loss: 0.587432, acc: 70.31%] [G loss: 1.912788]\n",
      "epoch:16 step:15286 [D loss: 0.632888, acc: 63.28%] [G loss: 1.908257]\n",
      "epoch:16 step:15287 [D loss: 0.677198, acc: 57.03%] [G loss: 1.931274]\n",
      "epoch:16 step:15288 [D loss: 0.630834, acc: 67.19%] [G loss: 1.962244]\n",
      "epoch:16 step:15289 [D loss: 0.643220, acc: 57.03%] [G loss: 1.858023]\n",
      "epoch:16 step:15290 [D loss: 0.628474, acc: 65.62%] [G loss: 1.879698]\n",
      "epoch:16 step:15291 [D loss: 0.641097, acc: 64.84%] [G loss: 1.883055]\n",
      "epoch:16 step:15292 [D loss: 0.623882, acc: 65.62%] [G loss: 1.876504]\n",
      "epoch:16 step:15293 [D loss: 0.656652, acc: 57.81%] [G loss: 1.884179]\n",
      "epoch:16 step:15294 [D loss: 0.658383, acc: 59.38%] [G loss: 1.937075]\n",
      "epoch:16 step:15295 [D loss: 0.687079, acc: 53.91%] [G loss: 1.822112]\n",
      "epoch:16 step:15296 [D loss: 0.638642, acc: 59.38%] [G loss: 1.737752]\n",
      "epoch:16 step:15297 [D loss: 0.647113, acc: 62.50%] [G loss: 1.891925]\n",
      "epoch:16 step:15298 [D loss: 0.638758, acc: 66.41%] [G loss: 1.841618]\n",
      "epoch:16 step:15299 [D loss: 0.615059, acc: 66.41%] [G loss: 1.781249]\n",
      "epoch:16 step:15300 [D loss: 0.700747, acc: 53.12%] [G loss: 1.790363]\n",
      "epoch:16 step:15301 [D loss: 0.639149, acc: 61.72%] [G loss: 1.956537]\n",
      "epoch:16 step:15302 [D loss: 0.605517, acc: 65.62%] [G loss: 1.872457]\n",
      "epoch:16 step:15303 [D loss: 0.638672, acc: 66.41%] [G loss: 1.921026]\n",
      "epoch:16 step:15304 [D loss: 0.586903, acc: 69.53%] [G loss: 2.334116]\n",
      "epoch:16 step:15305 [D loss: 0.574297, acc: 71.88%] [G loss: 2.185633]\n",
      "epoch:16 step:15306 [D loss: 0.623190, acc: 64.06%] [G loss: 2.213161]\n",
      "epoch:16 step:15307 [D loss: 0.532606, acc: 76.56%] [G loss: 2.284935]\n",
      "epoch:16 step:15308 [D loss: 0.740282, acc: 52.34%] [G loss: 1.778874]\n",
      "epoch:16 step:15309 [D loss: 0.721155, acc: 55.47%] [G loss: 1.925459]\n",
      "epoch:16 step:15310 [D loss: 0.636734, acc: 64.84%] [G loss: 1.937010]\n",
      "epoch:16 step:15311 [D loss: 0.655440, acc: 62.50%] [G loss: 1.839095]\n",
      "epoch:16 step:15312 [D loss: 0.629456, acc: 63.28%] [G loss: 1.897919]\n",
      "epoch:16 step:15313 [D loss: 0.631874, acc: 65.62%] [G loss: 2.055017]\n",
      "epoch:16 step:15314 [D loss: 0.649957, acc: 66.41%] [G loss: 1.980070]\n",
      "epoch:16 step:15315 [D loss: 0.728881, acc: 53.91%] [G loss: 1.723139]\n",
      "epoch:16 step:15316 [D loss: 0.627675, acc: 66.41%] [G loss: 1.809111]\n",
      "epoch:16 step:15317 [D loss: 0.620618, acc: 63.28%] [G loss: 1.893645]\n",
      "epoch:16 step:15318 [D loss: 0.655017, acc: 57.81%] [G loss: 1.920752]\n",
      "epoch:16 step:15319 [D loss: 0.630300, acc: 66.41%] [G loss: 1.856498]\n",
      "epoch:16 step:15320 [D loss: 0.645540, acc: 59.38%] [G loss: 1.988630]\n",
      "epoch:16 step:15321 [D loss: 0.676838, acc: 58.59%] [G loss: 1.844890]\n",
      "epoch:16 step:15322 [D loss: 0.663460, acc: 60.94%] [G loss: 2.107405]\n",
      "epoch:16 step:15323 [D loss: 0.628220, acc: 64.06%] [G loss: 1.918667]\n",
      "epoch:16 step:15324 [D loss: 0.618758, acc: 64.06%] [G loss: 2.202067]\n",
      "epoch:16 step:15325 [D loss: 0.683307, acc: 61.72%] [G loss: 1.873468]\n",
      "epoch:16 step:15326 [D loss: 0.608845, acc: 67.19%] [G loss: 2.103719]\n",
      "epoch:16 step:15327 [D loss: 0.664354, acc: 60.16%] [G loss: 2.047860]\n",
      "epoch:16 step:15328 [D loss: 0.644321, acc: 61.72%] [G loss: 1.980013]\n",
      "epoch:16 step:15329 [D loss: 0.585947, acc: 74.22%] [G loss: 1.975554]\n",
      "epoch:16 step:15330 [D loss: 0.676827, acc: 57.81%] [G loss: 1.949824]\n",
      "epoch:16 step:15331 [D loss: 0.620511, acc: 67.97%] [G loss: 2.041919]\n",
      "epoch:16 step:15332 [D loss: 0.611495, acc: 67.19%] [G loss: 1.841104]\n",
      "epoch:16 step:15333 [D loss: 0.652660, acc: 60.16%] [G loss: 1.757009]\n",
      "epoch:16 step:15334 [D loss: 0.672537, acc: 53.91%] [G loss: 1.711625]\n",
      "epoch:16 step:15335 [D loss: 0.669893, acc: 58.59%] [G loss: 1.840516]\n",
      "epoch:16 step:15336 [D loss: 0.609711, acc: 63.28%] [G loss: 1.983187]\n",
      "epoch:16 step:15337 [D loss: 0.644673, acc: 60.94%] [G loss: 2.328038]\n",
      "epoch:16 step:15338 [D loss: 0.597602, acc: 70.31%] [G loss: 2.162580]\n",
      "epoch:16 step:15339 [D loss: 0.590441, acc: 65.62%] [G loss: 2.326229]\n",
      "epoch:16 step:15340 [D loss: 0.701953, acc: 58.59%] [G loss: 1.916759]\n",
      "epoch:16 step:15341 [D loss: 0.709946, acc: 53.12%] [G loss: 1.672450]\n",
      "epoch:16 step:15342 [D loss: 0.611489, acc: 70.31%] [G loss: 1.901535]\n",
      "epoch:16 step:15343 [D loss: 0.661872, acc: 61.72%] [G loss: 1.937868]\n",
      "epoch:16 step:15344 [D loss: 0.641782, acc: 62.50%] [G loss: 1.832993]\n",
      "epoch:16 step:15345 [D loss: 0.638146, acc: 66.41%] [G loss: 1.840503]\n",
      "epoch:16 step:15346 [D loss: 0.696000, acc: 55.47%] [G loss: 1.947539]\n",
      "epoch:16 step:15347 [D loss: 0.631468, acc: 62.50%] [G loss: 1.903580]\n",
      "epoch:16 step:15348 [D loss: 0.664792, acc: 57.03%] [G loss: 1.727386]\n",
      "epoch:16 step:15349 [D loss: 0.626853, acc: 63.28%] [G loss: 1.888463]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:16 step:15350 [D loss: 0.624760, acc: 68.75%] [G loss: 2.036311]\n",
      "epoch:16 step:15351 [D loss: 0.658825, acc: 60.94%] [G loss: 1.960087]\n",
      "epoch:16 step:15352 [D loss: 0.647985, acc: 60.16%] [G loss: 1.926000]\n",
      "epoch:16 step:15353 [D loss: 0.671401, acc: 60.94%] [G loss: 1.710974]\n",
      "epoch:16 step:15354 [D loss: 0.650382, acc: 59.38%] [G loss: 1.752220]\n",
      "epoch:16 step:15355 [D loss: 0.626117, acc: 62.50%] [G loss: 1.885546]\n",
      "epoch:16 step:15356 [D loss: 0.642156, acc: 63.28%] [G loss: 1.833717]\n",
      "epoch:16 step:15357 [D loss: 0.680824, acc: 55.47%] [G loss: 1.752052]\n",
      "epoch:16 step:15358 [D loss: 0.668095, acc: 59.38%] [G loss: 2.000898]\n",
      "epoch:16 step:15359 [D loss: 0.613880, acc: 62.50%] [G loss: 1.923181]\n",
      "epoch:16 step:15360 [D loss: 0.664521, acc: 58.59%] [G loss: 1.958367]\n",
      "epoch:16 step:15361 [D loss: 0.673693, acc: 60.16%] [G loss: 1.798532]\n",
      "epoch:16 step:15362 [D loss: 0.656892, acc: 65.62%] [G loss: 1.978348]\n",
      "epoch:16 step:15363 [D loss: 0.600579, acc: 70.31%] [G loss: 1.930312]\n",
      "epoch:16 step:15364 [D loss: 0.655663, acc: 58.59%] [G loss: 1.814790]\n",
      "epoch:16 step:15365 [D loss: 0.673928, acc: 55.47%] [G loss: 1.773906]\n",
      "epoch:16 step:15366 [D loss: 0.631688, acc: 60.94%] [G loss: 1.900202]\n",
      "epoch:16 step:15367 [D loss: 0.653956, acc: 61.72%] [G loss: 1.890816]\n",
      "epoch:16 step:15368 [D loss: 0.719545, acc: 50.78%] [G loss: 1.614632]\n",
      "epoch:16 step:15369 [D loss: 0.704754, acc: 57.81%] [G loss: 1.755880]\n",
      "epoch:16 step:15370 [D loss: 0.655911, acc: 64.84%] [G loss: 1.786594]\n",
      "epoch:16 step:15371 [D loss: 0.597220, acc: 67.97%] [G loss: 1.895929]\n",
      "epoch:16 step:15372 [D loss: 0.671920, acc: 57.03%] [G loss: 1.779107]\n",
      "epoch:16 step:15373 [D loss: 0.653502, acc: 58.59%] [G loss: 1.784443]\n",
      "epoch:16 step:15374 [D loss: 0.657416, acc: 59.38%] [G loss: 1.823733]\n",
      "epoch:16 step:15375 [D loss: 0.631851, acc: 67.97%] [G loss: 2.011447]\n",
      "epoch:16 step:15376 [D loss: 0.624801, acc: 71.88%] [G loss: 1.998556]\n",
      "epoch:16 step:15377 [D loss: 0.637807, acc: 64.84%] [G loss: 2.057673]\n",
      "epoch:16 step:15378 [D loss: 0.691164, acc: 59.38%] [G loss: 1.786043]\n",
      "epoch:16 step:15379 [D loss: 0.637688, acc: 63.28%] [G loss: 1.823780]\n",
      "epoch:16 step:15380 [D loss: 0.645046, acc: 64.06%] [G loss: 1.815933]\n",
      "epoch:16 step:15381 [D loss: 0.686481, acc: 56.25%] [G loss: 1.872881]\n",
      "epoch:16 step:15382 [D loss: 0.707054, acc: 55.47%] [G loss: 1.809708]\n",
      "epoch:16 step:15383 [D loss: 0.701513, acc: 55.47%] [G loss: 1.746104]\n",
      "epoch:16 step:15384 [D loss: 0.635386, acc: 62.50%] [G loss: 1.842298]\n",
      "epoch:16 step:15385 [D loss: 0.656268, acc: 61.72%] [G loss: 1.841759]\n",
      "epoch:16 step:15386 [D loss: 0.647915, acc: 62.50%] [G loss: 1.814209]\n",
      "epoch:16 step:15387 [D loss: 0.672718, acc: 61.72%] [G loss: 1.827753]\n",
      "epoch:16 step:15388 [D loss: 0.633051, acc: 61.72%] [G loss: 1.842615]\n",
      "epoch:16 step:15389 [D loss: 0.663372, acc: 57.81%] [G loss: 1.774041]\n",
      "epoch:16 step:15390 [D loss: 0.614288, acc: 67.19%] [G loss: 1.901468]\n",
      "epoch:16 step:15391 [D loss: 0.636602, acc: 64.06%] [G loss: 1.874456]\n",
      "epoch:16 step:15392 [D loss: 0.650441, acc: 60.94%] [G loss: 1.853112]\n",
      "epoch:16 step:15393 [D loss: 0.684078, acc: 54.69%] [G loss: 1.839420]\n",
      "epoch:16 step:15394 [D loss: 0.625006, acc: 62.50%] [G loss: 1.863524]\n",
      "epoch:16 step:15395 [D loss: 0.630743, acc: 67.19%] [G loss: 1.927391]\n",
      "epoch:16 step:15396 [D loss: 0.553223, acc: 74.22%] [G loss: 1.987858]\n",
      "epoch:16 step:15397 [D loss: 0.656872, acc: 58.59%] [G loss: 1.940084]\n",
      "epoch:16 step:15398 [D loss: 0.596236, acc: 64.84%] [G loss: 2.081175]\n",
      "epoch:16 step:15399 [D loss: 0.635591, acc: 66.41%] [G loss: 2.013679]\n",
      "epoch:16 step:15400 [D loss: 0.690647, acc: 54.69%] [G loss: 1.883911]\n",
      "##############\n",
      "[2.5144748  1.50671581 6.47486508 4.75829113 3.53770466 5.60891832\n",
      " 4.33034216 4.69614804 4.53418156 3.44543878]\n",
      "##########\n",
      "epoch:16 step:15401 [D loss: 0.608711, acc: 67.19%] [G loss: 1.895299]\n",
      "epoch:16 step:15402 [D loss: 0.632339, acc: 64.84%] [G loss: 1.855042]\n",
      "epoch:16 step:15403 [D loss: 0.652395, acc: 61.72%] [G loss: 1.800034]\n",
      "epoch:16 step:15404 [D loss: 0.634253, acc: 64.84%] [G loss: 2.007732]\n",
      "epoch:16 step:15405 [D loss: 0.676128, acc: 57.81%] [G loss: 1.926611]\n",
      "epoch:16 step:15406 [D loss: 0.610459, acc: 72.66%] [G loss: 1.901532]\n",
      "epoch:16 step:15407 [D loss: 0.644044, acc: 62.50%] [G loss: 2.027894]\n",
      "epoch:16 step:15408 [D loss: 0.643649, acc: 65.62%] [G loss: 2.092088]\n",
      "epoch:16 step:15409 [D loss: 0.644642, acc: 63.28%] [G loss: 1.984031]\n",
      "epoch:16 step:15410 [D loss: 0.669039, acc: 54.69%] [G loss: 1.977123]\n",
      "epoch:16 step:15411 [D loss: 0.665720, acc: 61.72%] [G loss: 1.965721]\n",
      "epoch:16 step:15412 [D loss: 0.612382, acc: 67.97%] [G loss: 1.740849]\n",
      "epoch:16 step:15413 [D loss: 0.617783, acc: 66.41%] [G loss: 2.028435]\n",
      "epoch:16 step:15414 [D loss: 0.651356, acc: 65.62%] [G loss: 2.014548]\n",
      "epoch:16 step:15415 [D loss: 0.681551, acc: 54.69%] [G loss: 1.838091]\n",
      "epoch:16 step:15416 [D loss: 0.620141, acc: 67.97%] [G loss: 1.804152]\n",
      "epoch:16 step:15417 [D loss: 0.701034, acc: 56.25%] [G loss: 1.924651]\n",
      "epoch:16 step:15418 [D loss: 0.633416, acc: 60.94%] [G loss: 2.018289]\n",
      "epoch:16 step:15419 [D loss: 0.616826, acc: 67.97%] [G loss: 2.011834]\n",
      "epoch:16 step:15420 [D loss: 0.569131, acc: 68.75%] [G loss: 2.015254]\n",
      "epoch:16 step:15421 [D loss: 0.512986, acc: 78.91%] [G loss: 2.286360]\n",
      "epoch:16 step:15422 [D loss: 0.585120, acc: 68.75%] [G loss: 2.247365]\n",
      "epoch:16 step:15423 [D loss: 0.644099, acc: 64.06%] [G loss: 1.971053]\n",
      "epoch:16 step:15424 [D loss: 0.669486, acc: 60.16%] [G loss: 1.897841]\n",
      "epoch:16 step:15425 [D loss: 0.610743, acc: 73.44%] [G loss: 1.918882]\n",
      "epoch:16 step:15426 [D loss: 0.659707, acc: 67.97%] [G loss: 1.927969]\n",
      "epoch:16 step:15427 [D loss: 0.610369, acc: 62.50%] [G loss: 2.016337]\n",
      "epoch:16 step:15428 [D loss: 0.605551, acc: 65.62%] [G loss: 2.053387]\n",
      "epoch:16 step:15429 [D loss: 0.686685, acc: 57.81%] [G loss: 1.693961]\n",
      "epoch:16 step:15430 [D loss: 0.680326, acc: 57.03%] [G loss: 1.845908]\n",
      "epoch:16 step:15431 [D loss: 0.681405, acc: 57.03%] [G loss: 1.868272]\n",
      "epoch:16 step:15432 [D loss: 0.648692, acc: 64.06%] [G loss: 1.790618]\n",
      "epoch:16 step:15433 [D loss: 0.662658, acc: 57.03%] [G loss: 1.817510]\n",
      "epoch:16 step:15434 [D loss: 0.658442, acc: 60.16%] [G loss: 1.901596]\n",
      "epoch:16 step:15435 [D loss: 0.688102, acc: 58.59%] [G loss: 1.812881]\n",
      "epoch:16 step:15436 [D loss: 0.627630, acc: 63.28%] [G loss: 1.814205]\n",
      "epoch:16 step:15437 [D loss: 0.685159, acc: 54.69%] [G loss: 1.851710]\n",
      "epoch:16 step:15438 [D loss: 0.628147, acc: 67.19%] [G loss: 1.736048]\n",
      "epoch:16 step:15439 [D loss: 0.619180, acc: 67.19%] [G loss: 1.958180]\n",
      "epoch:16 step:15440 [D loss: 0.695424, acc: 55.47%] [G loss: 1.816634]\n",
      "epoch:16 step:15441 [D loss: 0.652359, acc: 61.72%] [G loss: 1.937198]\n",
      "epoch:16 step:15442 [D loss: 0.633984, acc: 67.19%] [G loss: 1.787761]\n",
      "epoch:16 step:15443 [D loss: 0.673152, acc: 62.50%] [G loss: 1.859296]\n",
      "epoch:16 step:15444 [D loss: 0.677456, acc: 60.94%] [G loss: 1.884036]\n",
      "epoch:16 step:15445 [D loss: 0.634982, acc: 64.06%] [G loss: 1.844655]\n",
      "epoch:16 step:15446 [D loss: 0.630467, acc: 67.19%] [G loss: 1.891988]\n",
      "epoch:16 step:15447 [D loss: 0.676523, acc: 57.03%] [G loss: 1.837371]\n",
      "epoch:16 step:15448 [D loss: 0.679937, acc: 59.38%] [G loss: 2.093444]\n",
      "epoch:16 step:15449 [D loss: 0.640478, acc: 64.06%] [G loss: 1.914830]\n",
      "epoch:16 step:15450 [D loss: 0.666073, acc: 60.94%] [G loss: 1.869569]\n",
      "epoch:16 step:15451 [D loss: 0.640466, acc: 60.16%] [G loss: 1.891623]\n",
      "epoch:16 step:15452 [D loss: 0.670101, acc: 60.94%] [G loss: 1.808061]\n",
      "epoch:16 step:15453 [D loss: 0.627140, acc: 64.06%] [G loss: 1.925298]\n",
      "epoch:16 step:15454 [D loss: 0.685401, acc: 55.47%] [G loss: 1.843378]\n",
      "epoch:16 step:15455 [D loss: 0.635083, acc: 64.06%] [G loss: 1.873025]\n",
      "epoch:16 step:15456 [D loss: 0.654306, acc: 62.50%] [G loss: 1.795119]\n",
      "epoch:16 step:15457 [D loss: 0.668723, acc: 58.59%] [G loss: 1.821633]\n",
      "epoch:16 step:15458 [D loss: 0.679224, acc: 58.59%] [G loss: 1.795928]\n",
      "epoch:16 step:15459 [D loss: 0.611009, acc: 68.75%] [G loss: 1.964233]\n",
      "epoch:16 step:15460 [D loss: 0.664125, acc: 62.50%] [G loss: 2.017833]\n",
      "epoch:16 step:15461 [D loss: 0.628822, acc: 64.06%] [G loss: 2.185177]\n",
      "epoch:16 step:15462 [D loss: 0.658596, acc: 60.16%] [G loss: 2.246283]\n",
      "epoch:16 step:15463 [D loss: 0.643359, acc: 66.41%] [G loss: 2.211176]\n",
      "epoch:16 step:15464 [D loss: 0.618158, acc: 67.97%] [G loss: 1.988690]\n",
      "epoch:16 step:15465 [D loss: 0.672921, acc: 53.12%] [G loss: 1.848315]\n",
      "epoch:16 step:15466 [D loss: 0.664966, acc: 64.06%] [G loss: 1.878984]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:16 step:15467 [D loss: 0.642838, acc: 65.62%] [G loss: 2.004575]\n",
      "epoch:16 step:15468 [D loss: 0.652078, acc: 63.28%] [G loss: 1.982514]\n",
      "epoch:16 step:15469 [D loss: 0.685665, acc: 56.25%] [G loss: 1.828524]\n",
      "epoch:16 step:15470 [D loss: 0.671408, acc: 58.59%] [G loss: 1.819001]\n",
      "epoch:16 step:15471 [D loss: 0.623480, acc: 65.62%] [G loss: 1.985566]\n",
      "epoch:16 step:15472 [D loss: 0.600760, acc: 71.09%] [G loss: 1.948381]\n",
      "epoch:16 step:15473 [D loss: 0.619507, acc: 63.28%] [G loss: 1.975527]\n",
      "epoch:16 step:15474 [D loss: 0.676487, acc: 56.25%] [G loss: 1.765335]\n",
      "epoch:16 step:15475 [D loss: 0.683753, acc: 57.03%] [G loss: 1.814185]\n",
      "epoch:16 step:15476 [D loss: 0.669086, acc: 62.50%] [G loss: 1.854065]\n",
      "epoch:16 step:15477 [D loss: 0.629000, acc: 67.19%] [G loss: 1.877935]\n",
      "epoch:16 step:15478 [D loss: 0.644435, acc: 70.31%] [G loss: 1.823373]\n",
      "epoch:16 step:15479 [D loss: 0.659644, acc: 66.41%] [G loss: 1.864880]\n",
      "epoch:16 step:15480 [D loss: 0.633155, acc: 65.62%] [G loss: 2.022426]\n",
      "epoch:16 step:15481 [D loss: 0.659932, acc: 58.59%] [G loss: 1.913253]\n",
      "epoch:16 step:15482 [D loss: 0.636911, acc: 66.41%] [G loss: 1.894250]\n",
      "epoch:16 step:15483 [D loss: 0.644977, acc: 62.50%] [G loss: 2.021646]\n",
      "epoch:16 step:15484 [D loss: 0.650641, acc: 61.72%] [G loss: 1.786408]\n",
      "epoch:16 step:15485 [D loss: 0.651887, acc: 57.81%] [G loss: 1.877522]\n",
      "epoch:16 step:15486 [D loss: 0.614080, acc: 69.53%] [G loss: 1.849460]\n",
      "epoch:16 step:15487 [D loss: 0.588508, acc: 74.22%] [G loss: 1.948958]\n",
      "epoch:16 step:15488 [D loss: 0.595761, acc: 66.41%] [G loss: 1.817388]\n",
      "epoch:16 step:15489 [D loss: 0.583774, acc: 73.44%] [G loss: 1.909820]\n",
      "epoch:16 step:15490 [D loss: 0.646213, acc: 63.28%] [G loss: 2.184617]\n",
      "epoch:16 step:15491 [D loss: 0.663879, acc: 64.06%] [G loss: 2.036129]\n",
      "epoch:16 step:15492 [D loss: 0.759316, acc: 50.78%] [G loss: 1.678333]\n",
      "epoch:16 step:15493 [D loss: 0.699470, acc: 56.25%] [G loss: 1.688899]\n",
      "epoch:16 step:15494 [D loss: 0.662453, acc: 59.38%] [G loss: 1.789339]\n",
      "epoch:16 step:15495 [D loss: 0.626822, acc: 65.62%] [G loss: 2.024648]\n",
      "epoch:16 step:15496 [D loss: 0.593840, acc: 74.22%] [G loss: 2.082896]\n",
      "epoch:16 step:15497 [D loss: 0.615999, acc: 64.84%] [G loss: 1.902926]\n",
      "epoch:16 step:15498 [D loss: 0.661019, acc: 60.94%] [G loss: 1.851987]\n",
      "epoch:16 step:15499 [D loss: 0.659934, acc: 59.38%] [G loss: 1.870902]\n",
      "epoch:16 step:15500 [D loss: 0.603154, acc: 63.28%] [G loss: 2.058411]\n",
      "epoch:16 step:15501 [D loss: 0.670335, acc: 60.16%] [G loss: 1.805825]\n",
      "epoch:16 step:15502 [D loss: 0.661586, acc: 60.94%] [G loss: 1.844209]\n",
      "epoch:16 step:15503 [D loss: 0.658684, acc: 58.59%] [G loss: 1.810593]\n",
      "epoch:16 step:15504 [D loss: 0.659257, acc: 61.72%] [G loss: 1.858333]\n",
      "epoch:16 step:15505 [D loss: 0.651073, acc: 59.38%] [G loss: 1.923086]\n",
      "epoch:16 step:15506 [D loss: 0.678150, acc: 61.72%] [G loss: 1.948297]\n",
      "epoch:16 step:15507 [D loss: 0.638701, acc: 66.41%] [G loss: 1.745054]\n",
      "epoch:16 step:15508 [D loss: 0.601995, acc: 73.44%] [G loss: 2.094230]\n",
      "epoch:16 step:15509 [D loss: 0.615214, acc: 64.84%] [G loss: 2.004569]\n",
      "epoch:16 step:15510 [D loss: 0.666343, acc: 56.25%] [G loss: 1.836630]\n",
      "epoch:16 step:15511 [D loss: 0.636719, acc: 66.41%] [G loss: 1.944675]\n",
      "epoch:16 step:15512 [D loss: 0.641521, acc: 61.72%] [G loss: 1.829337]\n",
      "epoch:16 step:15513 [D loss: 0.582798, acc: 67.97%] [G loss: 1.880445]\n",
      "epoch:16 step:15514 [D loss: 0.577412, acc: 71.09%] [G loss: 1.879077]\n",
      "epoch:16 step:15515 [D loss: 0.602852, acc: 67.19%] [G loss: 2.015873]\n",
      "epoch:16 step:15516 [D loss: 0.659366, acc: 58.59%] [G loss: 1.945331]\n",
      "epoch:16 step:15517 [D loss: 0.663573, acc: 60.16%] [G loss: 1.770554]\n",
      "epoch:16 step:15518 [D loss: 0.631628, acc: 68.75%] [G loss: 1.881056]\n",
      "epoch:16 step:15519 [D loss: 0.697393, acc: 55.47%] [G loss: 1.843927]\n",
      "epoch:16 step:15520 [D loss: 0.670223, acc: 58.59%] [G loss: 1.835949]\n",
      "epoch:16 step:15521 [D loss: 0.710214, acc: 52.34%] [G loss: 1.714411]\n",
      "epoch:16 step:15522 [D loss: 0.662526, acc: 59.38%] [G loss: 1.723842]\n",
      "epoch:16 step:15523 [D loss: 0.664047, acc: 60.16%] [G loss: 1.867151]\n",
      "epoch:16 step:15524 [D loss: 0.618294, acc: 64.06%] [G loss: 2.143441]\n",
      "epoch:16 step:15525 [D loss: 0.648362, acc: 61.72%] [G loss: 1.911500]\n",
      "epoch:16 step:15526 [D loss: 0.616046, acc: 70.31%] [G loss: 1.956668]\n",
      "epoch:16 step:15527 [D loss: 0.662101, acc: 62.50%] [G loss: 1.801330]\n",
      "epoch:16 step:15528 [D loss: 0.600223, acc: 70.31%] [G loss: 2.032074]\n",
      "epoch:16 step:15529 [D loss: 0.647252, acc: 60.94%] [G loss: 1.843198]\n",
      "epoch:16 step:15530 [D loss: 0.636626, acc: 69.53%] [G loss: 1.831831]\n",
      "epoch:16 step:15531 [D loss: 0.620605, acc: 67.97%] [G loss: 2.038214]\n",
      "epoch:16 step:15532 [D loss: 0.638901, acc: 64.06%] [G loss: 1.996033]\n",
      "epoch:16 step:15533 [D loss: 0.696108, acc: 62.50%] [G loss: 1.854974]\n",
      "epoch:16 step:15534 [D loss: 0.712477, acc: 59.38%] [G loss: 1.727791]\n",
      "epoch:16 step:15535 [D loss: 0.690866, acc: 55.47%] [G loss: 1.890179]\n",
      "epoch:16 step:15536 [D loss: 0.646770, acc: 68.75%] [G loss: 1.817965]\n",
      "epoch:16 step:15537 [D loss: 0.618112, acc: 65.62%] [G loss: 1.812697]\n",
      "epoch:16 step:15538 [D loss: 0.611338, acc: 65.62%] [G loss: 1.845367]\n",
      "epoch:16 step:15539 [D loss: 0.603082, acc: 67.19%] [G loss: 1.872537]\n",
      "epoch:16 step:15540 [D loss: 0.572223, acc: 68.75%] [G loss: 1.999919]\n",
      "epoch:16 step:15541 [D loss: 0.597621, acc: 70.31%] [G loss: 1.943620]\n",
      "epoch:16 step:15542 [D loss: 0.577654, acc: 71.09%] [G loss: 2.102563]\n",
      "epoch:16 step:15543 [D loss: 0.639041, acc: 62.50%] [G loss: 2.055383]\n",
      "epoch:16 step:15544 [D loss: 0.637594, acc: 68.75%] [G loss: 1.872669]\n",
      "epoch:16 step:15545 [D loss: 0.644386, acc: 66.41%] [G loss: 1.915811]\n",
      "epoch:16 step:15546 [D loss: 0.626636, acc: 69.53%] [G loss: 2.027131]\n",
      "epoch:16 step:15547 [D loss: 0.629146, acc: 60.16%] [G loss: 1.929628]\n",
      "epoch:16 step:15548 [D loss: 0.586721, acc: 70.31%] [G loss: 2.102674]\n",
      "epoch:16 step:15549 [D loss: 0.615251, acc: 67.19%] [G loss: 2.178151]\n",
      "epoch:16 step:15550 [D loss: 0.654858, acc: 57.81%] [G loss: 2.091703]\n",
      "epoch:16 step:15551 [D loss: 0.687499, acc: 61.72%] [G loss: 1.693403]\n",
      "epoch:16 step:15552 [D loss: 0.657026, acc: 57.81%] [G loss: 1.749424]\n",
      "epoch:16 step:15553 [D loss: 0.610563, acc: 67.97%] [G loss: 1.971941]\n",
      "epoch:16 step:15554 [D loss: 0.675089, acc: 60.16%] [G loss: 1.891063]\n",
      "epoch:16 step:15555 [D loss: 0.612306, acc: 67.97%] [G loss: 1.992829]\n",
      "epoch:16 step:15556 [D loss: 0.631057, acc: 67.97%] [G loss: 1.890840]\n",
      "epoch:16 step:15557 [D loss: 0.657000, acc: 58.59%] [G loss: 1.957022]\n",
      "epoch:16 step:15558 [D loss: 0.669743, acc: 56.25%] [G loss: 1.831273]\n",
      "epoch:16 step:15559 [D loss: 0.643301, acc: 59.38%] [G loss: 2.015133]\n",
      "epoch:16 step:15560 [D loss: 0.648517, acc: 61.72%] [G loss: 1.858719]\n",
      "epoch:16 step:15561 [D loss: 0.657003, acc: 59.38%] [G loss: 1.822508]\n",
      "epoch:16 step:15562 [D loss: 0.695984, acc: 53.91%] [G loss: 1.909540]\n",
      "epoch:16 step:15563 [D loss: 0.624712, acc: 64.84%] [G loss: 1.814949]\n",
      "epoch:16 step:15564 [D loss: 0.656796, acc: 60.16%] [G loss: 1.826457]\n",
      "epoch:16 step:15565 [D loss: 0.682614, acc: 63.28%] [G loss: 1.744632]\n",
      "epoch:16 step:15566 [D loss: 0.645393, acc: 61.72%] [G loss: 2.048984]\n",
      "epoch:16 step:15567 [D loss: 0.622250, acc: 64.06%] [G loss: 1.975446]\n",
      "epoch:16 step:15568 [D loss: 0.672044, acc: 56.25%] [G loss: 1.731135]\n",
      "epoch:16 step:15569 [D loss: 0.653083, acc: 59.38%] [G loss: 1.862613]\n",
      "epoch:16 step:15570 [D loss: 0.647875, acc: 62.50%] [G loss: 1.896561]\n",
      "epoch:16 step:15571 [D loss: 0.643111, acc: 60.94%] [G loss: 1.876314]\n",
      "epoch:16 step:15572 [D loss: 0.672415, acc: 61.72%] [G loss: 1.916064]\n",
      "epoch:16 step:15573 [D loss: 0.619081, acc: 67.19%] [G loss: 1.823703]\n",
      "epoch:16 step:15574 [D loss: 0.603114, acc: 69.53%] [G loss: 1.882887]\n",
      "epoch:16 step:15575 [D loss: 0.634186, acc: 64.06%] [G loss: 1.771055]\n",
      "epoch:16 step:15576 [D loss: 0.711673, acc: 53.91%] [G loss: 1.780142]\n",
      "epoch:16 step:15577 [D loss: 0.680075, acc: 57.81%] [G loss: 1.855063]\n",
      "epoch:16 step:15578 [D loss: 0.648755, acc: 59.38%] [G loss: 1.835967]\n",
      "epoch:16 step:15579 [D loss: 0.648602, acc: 60.16%] [G loss: 1.900695]\n",
      "epoch:16 step:15580 [D loss: 0.643249, acc: 65.62%] [G loss: 1.887041]\n",
      "epoch:16 step:15581 [D loss: 0.633321, acc: 64.06%] [G loss: 1.964055]\n",
      "epoch:16 step:15582 [D loss: 0.688133, acc: 60.16%] [G loss: 1.881948]\n",
      "epoch:16 step:15583 [D loss: 0.646193, acc: 60.94%] [G loss: 1.959295]\n",
      "epoch:16 step:15584 [D loss: 0.685269, acc: 59.38%] [G loss: 2.159058]\n",
      "epoch:16 step:15585 [D loss: 0.639833, acc: 67.19%] [G loss: 1.877165]\n",
      "epoch:16 step:15586 [D loss: 0.678426, acc: 60.16%] [G loss: 1.808246]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:16 step:15587 [D loss: 0.626866, acc: 67.97%] [G loss: 1.814890]\n",
      "epoch:16 step:15588 [D loss: 0.628135, acc: 62.50%] [G loss: 1.862961]\n",
      "epoch:16 step:15589 [D loss: 0.674062, acc: 60.94%] [G loss: 1.884751]\n",
      "epoch:16 step:15590 [D loss: 0.650048, acc: 57.03%] [G loss: 1.849934]\n",
      "epoch:16 step:15591 [D loss: 0.638600, acc: 62.50%] [G loss: 1.883770]\n",
      "epoch:16 step:15592 [D loss: 0.692112, acc: 58.59%] [G loss: 1.984978]\n",
      "epoch:16 step:15593 [D loss: 0.628405, acc: 64.84%] [G loss: 1.943322]\n",
      "epoch:16 step:15594 [D loss: 0.683094, acc: 58.59%] [G loss: 1.779210]\n",
      "epoch:16 step:15595 [D loss: 0.633857, acc: 64.06%] [G loss: 1.969717]\n",
      "epoch:16 step:15596 [D loss: 0.652425, acc: 60.94%] [G loss: 1.944935]\n",
      "epoch:16 step:15597 [D loss: 0.655233, acc: 60.16%] [G loss: 1.864515]\n",
      "epoch:16 step:15598 [D loss: 0.665454, acc: 61.72%] [G loss: 1.804025]\n",
      "epoch:16 step:15599 [D loss: 0.604878, acc: 67.97%] [G loss: 1.870510]\n",
      "epoch:16 step:15600 [D loss: 0.667075, acc: 56.25%] [G loss: 1.898755]\n",
      "##############\n",
      "[2.50919661 1.32688941 6.23303981 4.66696546 3.51552025 5.72774613\n",
      " 4.41487563 4.86427979 4.55396901 3.87648022]\n",
      "##########\n",
      "epoch:16 step:15601 [D loss: 0.630568, acc: 62.50%] [G loss: 1.956215]\n",
      "epoch:16 step:15602 [D loss: 0.631794, acc: 64.84%] [G loss: 1.794093]\n",
      "epoch:16 step:15603 [D loss: 0.678471, acc: 57.81%] [G loss: 1.764382]\n",
      "epoch:16 step:15604 [D loss: 0.659022, acc: 54.69%] [G loss: 1.837863]\n",
      "epoch:16 step:15605 [D loss: 0.612497, acc: 66.41%] [G loss: 1.851585]\n",
      "epoch:16 step:15606 [D loss: 0.649107, acc: 57.81%] [G loss: 1.782773]\n",
      "epoch:16 step:15607 [D loss: 0.683123, acc: 57.03%] [G loss: 1.843979]\n",
      "epoch:16 step:15608 [D loss: 0.641281, acc: 63.28%] [G loss: 1.942775]\n",
      "epoch:16 step:15609 [D loss: 0.642373, acc: 61.72%] [G loss: 1.908523]\n",
      "epoch:16 step:15610 [D loss: 0.607771, acc: 67.97%] [G loss: 1.792023]\n",
      "epoch:16 step:15611 [D loss: 0.612319, acc: 62.50%] [G loss: 1.885825]\n",
      "epoch:16 step:15612 [D loss: 0.642142, acc: 58.59%] [G loss: 1.998700]\n",
      "epoch:16 step:15613 [D loss: 0.663038, acc: 59.38%] [G loss: 1.857333]\n",
      "epoch:16 step:15614 [D loss: 0.631938, acc: 65.62%] [G loss: 1.963179]\n",
      "epoch:16 step:15615 [D loss: 0.624181, acc: 65.62%] [G loss: 1.867497]\n",
      "epoch:16 step:15616 [D loss: 0.598386, acc: 65.62%] [G loss: 2.118715]\n",
      "epoch:16 step:15617 [D loss: 0.693426, acc: 60.94%] [G loss: 1.757272]\n",
      "epoch:16 step:15618 [D loss: 0.659073, acc: 59.38%] [G loss: 1.958867]\n",
      "epoch:16 step:15619 [D loss: 0.663034, acc: 65.62%] [G loss: 1.814340]\n",
      "epoch:16 step:15620 [D loss: 0.663545, acc: 58.59%] [G loss: 1.787063]\n",
      "epoch:16 step:15621 [D loss: 0.647057, acc: 64.06%] [G loss: 1.809508]\n",
      "epoch:16 step:15622 [D loss: 0.668976, acc: 62.50%] [G loss: 1.925160]\n",
      "epoch:16 step:15623 [D loss: 0.579712, acc: 73.44%] [G loss: 1.938417]\n",
      "epoch:16 step:15624 [D loss: 0.576154, acc: 68.75%] [G loss: 2.055987]\n",
      "epoch:16 step:15625 [D loss: 0.616837, acc: 67.97%] [G loss: 2.016287]\n",
      "epoch:16 step:15626 [D loss: 0.609101, acc: 64.84%] [G loss: 1.849414]\n",
      "epoch:16 step:15627 [D loss: 0.604936, acc: 67.19%] [G loss: 1.960726]\n",
      "epoch:16 step:15628 [D loss: 0.652666, acc: 62.50%] [G loss: 1.952627]\n",
      "epoch:16 step:15629 [D loss: 0.629140, acc: 62.50%] [G loss: 2.135491]\n",
      "epoch:16 step:15630 [D loss: 0.651353, acc: 64.06%] [G loss: 2.044960]\n",
      "epoch:16 step:15631 [D loss: 0.662504, acc: 67.19%] [G loss: 1.963770]\n",
      "epoch:16 step:15632 [D loss: 0.610117, acc: 64.84%] [G loss: 1.981289]\n",
      "epoch:16 step:15633 [D loss: 0.653671, acc: 58.59%] [G loss: 2.058654]\n",
      "epoch:16 step:15634 [D loss: 0.619627, acc: 66.41%] [G loss: 2.020352]\n",
      "epoch:16 step:15635 [D loss: 0.630163, acc: 72.66%] [G loss: 1.930754]\n",
      "epoch:16 step:15636 [D loss: 0.637610, acc: 61.72%] [G loss: 1.997913]\n",
      "epoch:16 step:15637 [D loss: 0.656162, acc: 64.84%] [G loss: 1.997171]\n",
      "epoch:16 step:15638 [D loss: 0.656652, acc: 64.84%] [G loss: 1.892609]\n",
      "epoch:16 step:15639 [D loss: 0.611430, acc: 64.84%] [G loss: 2.098409]\n",
      "epoch:16 step:15640 [D loss: 0.575058, acc: 69.53%] [G loss: 2.321423]\n",
      "epoch:16 step:15641 [D loss: 0.628096, acc: 67.19%] [G loss: 2.079065]\n",
      "epoch:16 step:15642 [D loss: 0.596897, acc: 67.97%] [G loss: 2.127013]\n",
      "epoch:16 step:15643 [D loss: 0.577783, acc: 68.75%] [G loss: 2.134927]\n",
      "epoch:16 step:15644 [D loss: 0.667274, acc: 62.50%] [G loss: 1.920419]\n",
      "epoch:16 step:15645 [D loss: 0.619034, acc: 61.72%] [G loss: 1.976971]\n",
      "epoch:16 step:15646 [D loss: 0.627288, acc: 64.06%] [G loss: 2.001950]\n",
      "epoch:16 step:15647 [D loss: 0.660582, acc: 59.38%] [G loss: 1.879470]\n",
      "epoch:16 step:15648 [D loss: 0.663543, acc: 61.72%] [G loss: 1.831048]\n",
      "epoch:16 step:15649 [D loss: 0.656727, acc: 61.72%] [G loss: 1.841928]\n",
      "epoch:16 step:15650 [D loss: 0.659864, acc: 59.38%] [G loss: 1.743490]\n",
      "epoch:16 step:15651 [D loss: 0.649772, acc: 54.69%] [G loss: 1.808264]\n",
      "epoch:16 step:15652 [D loss: 0.669705, acc: 57.81%] [G loss: 1.818105]\n",
      "epoch:16 step:15653 [D loss: 0.635063, acc: 64.84%] [G loss: 1.745758]\n",
      "epoch:16 step:15654 [D loss: 0.668644, acc: 64.06%] [G loss: 1.766133]\n",
      "epoch:16 step:15655 [D loss: 0.663273, acc: 61.72%] [G loss: 1.889623]\n",
      "epoch:16 step:15656 [D loss: 0.619338, acc: 67.97%] [G loss: 1.883069]\n",
      "epoch:16 step:15657 [D loss: 0.667434, acc: 60.94%] [G loss: 1.777994]\n",
      "epoch:16 step:15658 [D loss: 0.655643, acc: 62.50%] [G loss: 1.881032]\n",
      "epoch:16 step:15659 [D loss: 0.695308, acc: 57.03%] [G loss: 1.909955]\n",
      "epoch:16 step:15660 [D loss: 0.644719, acc: 60.16%] [G loss: 1.894045]\n",
      "epoch:16 step:15661 [D loss: 0.679206, acc: 60.94%] [G loss: 1.783024]\n",
      "epoch:16 step:15662 [D loss: 0.689809, acc: 60.16%] [G loss: 1.710284]\n",
      "epoch:16 step:15663 [D loss: 0.690632, acc: 63.28%] [G loss: 1.790679]\n",
      "epoch:16 step:15664 [D loss: 0.684062, acc: 60.16%] [G loss: 1.884911]\n",
      "epoch:16 step:15665 [D loss: 0.640340, acc: 63.28%] [G loss: 1.749316]\n",
      "epoch:16 step:15666 [D loss: 0.670422, acc: 64.06%] [G loss: 1.846495]\n",
      "epoch:16 step:15667 [D loss: 0.670078, acc: 54.69%] [G loss: 1.843537]\n",
      "epoch:16 step:15668 [D loss: 0.666899, acc: 59.38%] [G loss: 1.805061]\n",
      "epoch:16 step:15669 [D loss: 0.608386, acc: 63.28%] [G loss: 1.892819]\n",
      "epoch:16 step:15670 [D loss: 0.620864, acc: 65.62%] [G loss: 2.023588]\n",
      "epoch:16 step:15671 [D loss: 0.631866, acc: 67.19%] [G loss: 1.930918]\n",
      "epoch:16 step:15672 [D loss: 0.650583, acc: 64.84%] [G loss: 1.840292]\n",
      "epoch:16 step:15673 [D loss: 0.608381, acc: 67.97%] [G loss: 1.888305]\n",
      "epoch:16 step:15674 [D loss: 0.692614, acc: 54.69%] [G loss: 1.918516]\n",
      "epoch:16 step:15675 [D loss: 0.635813, acc: 63.28%] [G loss: 1.966249]\n",
      "epoch:16 step:15676 [D loss: 0.645890, acc: 65.62%] [G loss: 1.825533]\n",
      "epoch:16 step:15677 [D loss: 0.627210, acc: 62.50%] [G loss: 1.936942]\n",
      "epoch:16 step:15678 [D loss: 0.645977, acc: 66.41%] [G loss: 1.887174]\n",
      "epoch:16 step:15679 [D loss: 0.599908, acc: 67.97%] [G loss: 1.908558]\n",
      "epoch:16 step:15680 [D loss: 0.633721, acc: 67.97%] [G loss: 2.080076]\n",
      "epoch:16 step:15681 [D loss: 0.619109, acc: 64.06%] [G loss: 1.917670]\n",
      "epoch:16 step:15682 [D loss: 0.655735, acc: 58.59%] [G loss: 1.945882]\n",
      "epoch:16 step:15683 [D loss: 0.660496, acc: 61.72%] [G loss: 2.038131]\n",
      "epoch:16 step:15684 [D loss: 0.643941, acc: 63.28%] [G loss: 2.015609]\n",
      "epoch:16 step:15685 [D loss: 0.623438, acc: 61.72%] [G loss: 2.117314]\n",
      "epoch:16 step:15686 [D loss: 0.592428, acc: 65.62%] [G loss: 2.069624]\n",
      "epoch:16 step:15687 [D loss: 0.673709, acc: 59.38%] [G loss: 2.004152]\n",
      "epoch:16 step:15688 [D loss: 0.740156, acc: 49.22%] [G loss: 1.745210]\n",
      "epoch:16 step:15689 [D loss: 0.680229, acc: 57.03%] [G loss: 1.808775]\n",
      "epoch:16 step:15690 [D loss: 0.665970, acc: 58.59%] [G loss: 1.799451]\n",
      "epoch:16 step:15691 [D loss: 0.621206, acc: 62.50%] [G loss: 1.954612]\n",
      "epoch:16 step:15692 [D loss: 0.667691, acc: 57.81%] [G loss: 1.810263]\n",
      "epoch:16 step:15693 [D loss: 0.664183, acc: 62.50%] [G loss: 1.962762]\n",
      "epoch:16 step:15694 [D loss: 0.678301, acc: 56.25%] [G loss: 1.880448]\n",
      "epoch:16 step:15695 [D loss: 0.685072, acc: 57.03%] [G loss: 1.818529]\n",
      "epoch:16 step:15696 [D loss: 0.644009, acc: 67.19%] [G loss: 1.795441]\n",
      "epoch:16 step:15697 [D loss: 0.638850, acc: 60.94%] [G loss: 1.816206]\n",
      "epoch:16 step:15698 [D loss: 0.643562, acc: 65.62%] [G loss: 1.940481]\n",
      "epoch:16 step:15699 [D loss: 0.606388, acc: 67.19%] [G loss: 1.891324]\n",
      "epoch:16 step:15700 [D loss: 0.565764, acc: 75.00%] [G loss: 2.177237]\n",
      "epoch:16 step:15701 [D loss: 0.631749, acc: 66.41%] [G loss: 2.139418]\n",
      "epoch:16 step:15702 [D loss: 0.707810, acc: 55.47%] [G loss: 1.811743]\n",
      "epoch:16 step:15703 [D loss: 0.647305, acc: 69.53%] [G loss: 1.882539]\n",
      "epoch:16 step:15704 [D loss: 0.627983, acc: 61.72%] [G loss: 1.921280]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:16 step:15705 [D loss: 0.666024, acc: 54.69%] [G loss: 1.911815]\n",
      "epoch:16 step:15706 [D loss: 0.705385, acc: 54.69%] [G loss: 1.900734]\n",
      "epoch:16 step:15707 [D loss: 0.661666, acc: 58.59%] [G loss: 1.929958]\n",
      "epoch:16 step:15708 [D loss: 0.735364, acc: 50.00%] [G loss: 1.862951]\n",
      "epoch:16 step:15709 [D loss: 0.706082, acc: 52.34%] [G loss: 1.720985]\n",
      "epoch:16 step:15710 [D loss: 0.704503, acc: 57.03%] [G loss: 1.765376]\n",
      "epoch:16 step:15711 [D loss: 0.632871, acc: 67.19%] [G loss: 1.879522]\n",
      "epoch:16 step:15712 [D loss: 0.681285, acc: 59.38%] [G loss: 1.961730]\n",
      "epoch:16 step:15713 [D loss: 0.661558, acc: 58.59%] [G loss: 1.793417]\n",
      "epoch:16 step:15714 [D loss: 0.684907, acc: 60.16%] [G loss: 1.733009]\n",
      "epoch:16 step:15715 [D loss: 0.669715, acc: 56.25%] [G loss: 1.838807]\n",
      "epoch:16 step:15716 [D loss: 0.663796, acc: 64.06%] [G loss: 1.842961]\n",
      "epoch:16 step:15717 [D loss: 0.627909, acc: 63.28%] [G loss: 1.843486]\n",
      "epoch:16 step:15718 [D loss: 0.653779, acc: 61.72%] [G loss: 1.913888]\n",
      "epoch:16 step:15719 [D loss: 0.652264, acc: 60.94%] [G loss: 1.753758]\n",
      "epoch:16 step:15720 [D loss: 0.645106, acc: 58.59%] [G loss: 1.943888]\n",
      "epoch:16 step:15721 [D loss: 0.682939, acc: 60.16%] [G loss: 1.788071]\n",
      "epoch:16 step:15722 [D loss: 0.662005, acc: 60.16%] [G loss: 1.761606]\n",
      "epoch:16 step:15723 [D loss: 0.625602, acc: 64.84%] [G loss: 1.866052]\n",
      "epoch:16 step:15724 [D loss: 0.679269, acc: 53.12%] [G loss: 1.830382]\n",
      "epoch:16 step:15725 [D loss: 0.611640, acc: 61.72%] [G loss: 1.934432]\n",
      "epoch:16 step:15726 [D loss: 0.662383, acc: 58.59%] [G loss: 1.785391]\n",
      "epoch:16 step:15727 [D loss: 0.629357, acc: 63.28%] [G loss: 1.957607]\n",
      "epoch:16 step:15728 [D loss: 0.667572, acc: 60.16%] [G loss: 1.981227]\n",
      "epoch:16 step:15729 [D loss: 0.605642, acc: 71.09%] [G loss: 1.820728]\n",
      "epoch:16 step:15730 [D loss: 0.614625, acc: 69.53%] [G loss: 1.913770]\n",
      "epoch:16 step:15731 [D loss: 0.634080, acc: 64.84%] [G loss: 1.796509]\n",
      "epoch:16 step:15732 [D loss: 0.634486, acc: 67.97%] [G loss: 1.931305]\n",
      "epoch:16 step:15733 [D loss: 0.682452, acc: 58.59%] [G loss: 1.816972]\n",
      "epoch:16 step:15734 [D loss: 0.669967, acc: 60.94%] [G loss: 1.806119]\n",
      "epoch:16 step:15735 [D loss: 0.611220, acc: 64.84%] [G loss: 1.866328]\n",
      "epoch:16 step:15736 [D loss: 0.665177, acc: 60.94%] [G loss: 1.974944]\n",
      "epoch:16 step:15737 [D loss: 0.708664, acc: 56.25%] [G loss: 1.864068]\n",
      "epoch:16 step:15738 [D loss: 0.645069, acc: 60.94%] [G loss: 1.894333]\n",
      "epoch:16 step:15739 [D loss: 0.575779, acc: 74.22%] [G loss: 1.840451]\n",
      "epoch:16 step:15740 [D loss: 0.693767, acc: 57.81%] [G loss: 1.853079]\n",
      "epoch:16 step:15741 [D loss: 0.626913, acc: 63.28%] [G loss: 1.777566]\n",
      "epoch:16 step:15742 [D loss: 0.683892, acc: 55.47%] [G loss: 1.901489]\n",
      "epoch:16 step:15743 [D loss: 0.670570, acc: 59.38%] [G loss: 1.785918]\n",
      "epoch:16 step:15744 [D loss: 0.653296, acc: 60.94%] [G loss: 1.731846]\n",
      "epoch:16 step:15745 [D loss: 0.625775, acc: 63.28%] [G loss: 1.820585]\n",
      "epoch:16 step:15746 [D loss: 0.629910, acc: 64.84%] [G loss: 1.648892]\n",
      "epoch:16 step:15747 [D loss: 0.635643, acc: 59.38%] [G loss: 1.864953]\n",
      "epoch:16 step:15748 [D loss: 0.639420, acc: 62.50%] [G loss: 1.945088]\n",
      "epoch:16 step:15749 [D loss: 0.671866, acc: 57.81%] [G loss: 1.826945]\n",
      "epoch:16 step:15750 [D loss: 0.641145, acc: 65.62%] [G loss: 1.703974]\n",
      "epoch:16 step:15751 [D loss: 0.669775, acc: 62.50%] [G loss: 1.698560]\n",
      "epoch:16 step:15752 [D loss: 0.650277, acc: 57.03%] [G loss: 1.754802]\n",
      "epoch:16 step:15753 [D loss: 0.630105, acc: 64.06%] [G loss: 1.871604]\n",
      "epoch:16 step:15754 [D loss: 0.681008, acc: 58.59%] [G loss: 1.880108]\n",
      "epoch:16 step:15755 [D loss: 0.653463, acc: 59.38%] [G loss: 1.781943]\n",
      "epoch:16 step:15756 [D loss: 0.665230, acc: 60.16%] [G loss: 1.777167]\n",
      "epoch:16 step:15757 [D loss: 0.708697, acc: 57.03%] [G loss: 1.783333]\n",
      "epoch:16 step:15758 [D loss: 0.673787, acc: 64.06%] [G loss: 1.752133]\n",
      "epoch:16 step:15759 [D loss: 0.684792, acc: 59.38%] [G loss: 1.822963]\n",
      "epoch:16 step:15760 [D loss: 0.665213, acc: 64.06%] [G loss: 1.860090]\n",
      "epoch:16 step:15761 [D loss: 0.673315, acc: 62.50%] [G loss: 1.961054]\n",
      "epoch:16 step:15762 [D loss: 0.625209, acc: 62.50%] [G loss: 1.833534]\n",
      "epoch:16 step:15763 [D loss: 0.614805, acc: 61.72%] [G loss: 1.748496]\n",
      "epoch:16 step:15764 [D loss: 0.686776, acc: 60.94%] [G loss: 1.932359]\n",
      "epoch:16 step:15765 [D loss: 0.640785, acc: 66.41%] [G loss: 1.700469]\n",
      "epoch:16 step:15766 [D loss: 0.602971, acc: 65.62%] [G loss: 1.974429]\n",
      "epoch:16 step:15767 [D loss: 0.618734, acc: 67.19%] [G loss: 2.111623]\n",
      "epoch:16 step:15768 [D loss: 0.621391, acc: 67.97%] [G loss: 1.980819]\n",
      "epoch:16 step:15769 [D loss: 0.687558, acc: 57.81%] [G loss: 1.887676]\n",
      "epoch:16 step:15770 [D loss: 0.679693, acc: 60.94%] [G loss: 1.870014]\n",
      "epoch:16 step:15771 [D loss: 0.642990, acc: 60.94%] [G loss: 1.767048]\n",
      "epoch:16 step:15772 [D loss: 0.651958, acc: 66.41%] [G loss: 2.015681]\n",
      "epoch:16 step:15773 [D loss: 0.588545, acc: 71.88%] [G loss: 2.051517]\n",
      "epoch:16 step:15774 [D loss: 0.634970, acc: 65.62%] [G loss: 2.112093]\n",
      "epoch:16 step:15775 [D loss: 0.724012, acc: 59.38%] [G loss: 1.870187]\n",
      "epoch:16 step:15776 [D loss: 0.647944, acc: 67.19%] [G loss: 1.711545]\n",
      "epoch:16 step:15777 [D loss: 0.678895, acc: 56.25%] [G loss: 1.812448]\n",
      "epoch:16 step:15778 [D loss: 0.599622, acc: 65.62%] [G loss: 1.957170]\n",
      "epoch:16 step:15779 [D loss: 0.662397, acc: 60.94%] [G loss: 1.867664]\n",
      "epoch:16 step:15780 [D loss: 0.619658, acc: 67.97%] [G loss: 1.748788]\n",
      "epoch:16 step:15781 [D loss: 0.612052, acc: 69.53%] [G loss: 1.973585]\n",
      "epoch:16 step:15782 [D loss: 0.614534, acc: 67.97%] [G loss: 2.091651]\n",
      "epoch:16 step:15783 [D loss: 0.615584, acc: 62.50%] [G loss: 1.883694]\n",
      "epoch:16 step:15784 [D loss: 0.584059, acc: 68.75%] [G loss: 2.024445]\n",
      "epoch:16 step:15785 [D loss: 0.649204, acc: 66.41%] [G loss: 2.091973]\n",
      "epoch:16 step:15786 [D loss: 0.688483, acc: 59.38%] [G loss: 1.899946]\n",
      "epoch:16 step:15787 [D loss: 0.676124, acc: 57.03%] [G loss: 1.688357]\n",
      "epoch:16 step:15788 [D loss: 0.720259, acc: 52.34%] [G loss: 1.949049]\n",
      "epoch:16 step:15789 [D loss: 0.674147, acc: 58.59%] [G loss: 1.825912]\n",
      "epoch:16 step:15790 [D loss: 0.641834, acc: 67.97%] [G loss: 1.786938]\n",
      "epoch:16 step:15791 [D loss: 0.663185, acc: 60.94%] [G loss: 1.943531]\n",
      "epoch:16 step:15792 [D loss: 0.712776, acc: 51.56%] [G loss: 1.837221]\n",
      "epoch:16 step:15793 [D loss: 0.646605, acc: 67.97%] [G loss: 1.815529]\n",
      "epoch:16 step:15794 [D loss: 0.643855, acc: 62.50%] [G loss: 1.909487]\n",
      "epoch:16 step:15795 [D loss: 0.621649, acc: 69.53%] [G loss: 1.876693]\n",
      "epoch:16 step:15796 [D loss: 0.651563, acc: 59.38%] [G loss: 1.819899]\n",
      "epoch:16 step:15797 [D loss: 0.645495, acc: 61.72%] [G loss: 2.040218]\n",
      "epoch:16 step:15798 [D loss: 0.667795, acc: 57.81%] [G loss: 1.778215]\n",
      "epoch:16 step:15799 [D loss: 0.678560, acc: 57.03%] [G loss: 1.837486]\n",
      "epoch:16 step:15800 [D loss: 0.626872, acc: 60.16%] [G loss: 1.894210]\n",
      "##############\n",
      "[2.30953604 1.46682673 6.20453551 4.77750552 3.52929067 5.63447083\n",
      " 4.63983328 4.33405955 4.41715583 3.4771984 ]\n",
      "##########\n",
      "epoch:16 step:15801 [D loss: 0.657229, acc: 57.03%] [G loss: 1.965599]\n",
      "epoch:16 step:15802 [D loss: 0.620353, acc: 63.28%] [G loss: 1.753788]\n",
      "epoch:16 step:15803 [D loss: 0.662553, acc: 59.38%] [G loss: 1.868365]\n",
      "epoch:16 step:15804 [D loss: 0.682416, acc: 55.47%] [G loss: 1.842573]\n",
      "epoch:16 step:15805 [D loss: 0.664826, acc: 57.03%] [G loss: 1.867387]\n",
      "epoch:16 step:15806 [D loss: 0.641551, acc: 65.62%] [G loss: 1.893707]\n",
      "epoch:16 step:15807 [D loss: 0.587077, acc: 67.97%] [G loss: 2.002197]\n",
      "epoch:16 step:15808 [D loss: 0.617554, acc: 64.84%] [G loss: 2.087239]\n",
      "epoch:16 step:15809 [D loss: 0.689149, acc: 55.47%] [G loss: 1.766169]\n",
      "epoch:16 step:15810 [D loss: 0.691122, acc: 53.91%] [G loss: 1.768046]\n",
      "epoch:16 step:15811 [D loss: 0.648920, acc: 56.25%] [G loss: 1.903155]\n",
      "epoch:16 step:15812 [D loss: 0.710926, acc: 48.44%] [G loss: 1.688604]\n",
      "epoch:16 step:15813 [D loss: 0.661228, acc: 61.72%] [G loss: 1.728520]\n",
      "epoch:16 step:15814 [D loss: 0.611304, acc: 67.97%] [G loss: 1.870025]\n",
      "epoch:16 step:15815 [D loss: 0.645275, acc: 61.72%] [G loss: 1.858923]\n",
      "epoch:16 step:15816 [D loss: 0.716631, acc: 53.12%] [G loss: 1.822868]\n",
      "epoch:16 step:15817 [D loss: 0.628157, acc: 63.28%] [G loss: 1.885277]\n",
      "epoch:16 step:15818 [D loss: 0.663392, acc: 59.38%] [G loss: 1.813909]\n",
      "epoch:16 step:15819 [D loss: 0.685137, acc: 56.25%] [G loss: 1.755102]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:16 step:15820 [D loss: 0.695968, acc: 55.47%] [G loss: 1.734452]\n",
      "epoch:16 step:15821 [D loss: 0.638792, acc: 61.72%] [G loss: 1.769547]\n",
      "epoch:16 step:15822 [D loss: 0.637575, acc: 64.06%] [G loss: 1.730608]\n",
      "epoch:16 step:15823 [D loss: 0.626134, acc: 66.41%] [G loss: 1.794314]\n",
      "epoch:16 step:15824 [D loss: 0.618165, acc: 65.62%] [G loss: 1.796519]\n",
      "epoch:16 step:15825 [D loss: 0.611100, acc: 66.41%] [G loss: 1.842347]\n",
      "epoch:16 step:15826 [D loss: 0.631100, acc: 67.19%] [G loss: 1.872250]\n",
      "epoch:16 step:15827 [D loss: 0.605795, acc: 69.53%] [G loss: 2.010486]\n",
      "epoch:16 step:15828 [D loss: 0.664836, acc: 59.38%] [G loss: 2.006837]\n",
      "epoch:16 step:15829 [D loss: 0.644292, acc: 60.94%] [G loss: 1.925178]\n",
      "epoch:16 step:15830 [D loss: 0.635892, acc: 64.84%] [G loss: 1.907056]\n",
      "epoch:16 step:15831 [D loss: 0.634038, acc: 63.28%] [G loss: 1.817203]\n",
      "epoch:16 step:15832 [D loss: 0.636692, acc: 64.84%] [G loss: 1.847718]\n",
      "epoch:16 step:15833 [D loss: 0.622495, acc: 64.84%] [G loss: 1.979726]\n",
      "epoch:16 step:15834 [D loss: 0.611190, acc: 64.84%] [G loss: 2.002003]\n",
      "epoch:16 step:15835 [D loss: 0.631286, acc: 64.84%] [G loss: 1.879026]\n",
      "epoch:16 step:15836 [D loss: 0.677318, acc: 60.94%] [G loss: 1.936243]\n",
      "epoch:16 step:15837 [D loss: 0.635412, acc: 67.97%] [G loss: 1.967423]\n",
      "epoch:16 step:15838 [D loss: 0.639510, acc: 62.50%] [G loss: 1.899883]\n",
      "epoch:16 step:15839 [D loss: 0.626555, acc: 64.06%] [G loss: 1.898444]\n",
      "epoch:16 step:15840 [D loss: 0.658103, acc: 63.28%] [G loss: 1.894784]\n",
      "epoch:16 step:15841 [D loss: 0.662372, acc: 60.94%] [G loss: 1.822682]\n",
      "epoch:16 step:15842 [D loss: 0.676637, acc: 55.47%] [G loss: 1.938148]\n",
      "epoch:16 step:15843 [D loss: 0.656301, acc: 64.06%] [G loss: 1.748085]\n",
      "epoch:16 step:15844 [D loss: 0.675038, acc: 58.59%] [G loss: 1.924173]\n",
      "epoch:16 step:15845 [D loss: 0.647607, acc: 57.81%] [G loss: 1.774556]\n",
      "epoch:16 step:15846 [D loss: 0.681224, acc: 59.38%] [G loss: 1.865048]\n",
      "epoch:16 step:15847 [D loss: 0.677680, acc: 54.69%] [G loss: 1.865064]\n",
      "epoch:16 step:15848 [D loss: 0.717482, acc: 57.81%] [G loss: 1.823690]\n",
      "epoch:16 step:15849 [D loss: 0.665898, acc: 61.72%] [G loss: 1.908167]\n",
      "epoch:16 step:15850 [D loss: 0.715299, acc: 57.03%] [G loss: 1.883837]\n",
      "epoch:16 step:15851 [D loss: 0.641823, acc: 67.19%] [G loss: 1.808660]\n",
      "epoch:16 step:15852 [D loss: 0.611153, acc: 72.66%] [G loss: 1.977888]\n",
      "epoch:16 step:15853 [D loss: 0.701366, acc: 50.78%] [G loss: 1.652085]\n",
      "epoch:16 step:15854 [D loss: 0.683508, acc: 59.38%] [G loss: 1.761557]\n",
      "epoch:16 step:15855 [D loss: 0.656484, acc: 56.25%] [G loss: 1.778308]\n",
      "epoch:16 step:15856 [D loss: 0.696998, acc: 61.72%] [G loss: 1.825207]\n",
      "epoch:16 step:15857 [D loss: 0.648475, acc: 60.94%] [G loss: 1.800258]\n",
      "epoch:16 step:15858 [D loss: 0.608072, acc: 71.09%] [G loss: 1.805684]\n",
      "epoch:16 step:15859 [D loss: 0.646478, acc: 63.28%] [G loss: 1.752123]\n",
      "epoch:16 step:15860 [D loss: 0.671742, acc: 58.59%] [G loss: 1.700931]\n",
      "epoch:16 step:15861 [D loss: 0.649218, acc: 60.94%] [G loss: 1.855048]\n",
      "epoch:16 step:15862 [D loss: 0.667535, acc: 59.38%] [G loss: 1.865020]\n",
      "epoch:16 step:15863 [D loss: 0.653041, acc: 63.28%] [G loss: 1.805208]\n",
      "epoch:16 step:15864 [D loss: 0.647439, acc: 61.72%] [G loss: 1.762165]\n",
      "epoch:16 step:15865 [D loss: 0.686614, acc: 54.69%] [G loss: 1.730921]\n",
      "epoch:16 step:15866 [D loss: 0.649209, acc: 61.72%] [G loss: 1.765235]\n",
      "epoch:16 step:15867 [D loss: 0.628457, acc: 63.28%] [G loss: 1.868350]\n",
      "epoch:16 step:15868 [D loss: 0.645679, acc: 60.94%] [G loss: 1.840545]\n",
      "epoch:16 step:15869 [D loss: 0.678203, acc: 62.50%] [G loss: 1.728161]\n",
      "epoch:16 step:15870 [D loss: 0.669736, acc: 60.94%] [G loss: 1.763826]\n",
      "epoch:16 step:15871 [D loss: 0.640583, acc: 63.28%] [G loss: 1.878858]\n",
      "epoch:16 step:15872 [D loss: 0.574299, acc: 75.00%] [G loss: 1.859818]\n",
      "epoch:16 step:15873 [D loss: 0.635206, acc: 64.06%] [G loss: 1.812889]\n",
      "epoch:16 step:15874 [D loss: 0.639534, acc: 63.28%] [G loss: 1.890260]\n",
      "epoch:16 step:15875 [D loss: 0.590305, acc: 67.19%] [G loss: 1.829436]\n",
      "epoch:16 step:15876 [D loss: 0.654900, acc: 60.16%] [G loss: 1.985911]\n",
      "epoch:16 step:15877 [D loss: 0.632840, acc: 61.72%] [G loss: 2.005755]\n",
      "epoch:16 step:15878 [D loss: 0.665746, acc: 62.50%] [G loss: 2.069626]\n",
      "epoch:16 step:15879 [D loss: 0.612533, acc: 64.06%] [G loss: 1.938999]\n",
      "epoch:16 step:15880 [D loss: 0.593323, acc: 68.75%] [G loss: 2.035297]\n",
      "epoch:16 step:15881 [D loss: 0.607640, acc: 66.41%] [G loss: 1.963543]\n",
      "epoch:16 step:15882 [D loss: 0.644952, acc: 66.41%] [G loss: 1.907857]\n",
      "epoch:16 step:15883 [D loss: 0.659156, acc: 57.81%] [G loss: 1.767679]\n",
      "epoch:16 step:15884 [D loss: 0.621905, acc: 64.84%] [G loss: 2.004192]\n",
      "epoch:16 step:15885 [D loss: 0.668316, acc: 56.25%] [G loss: 1.880460]\n",
      "epoch:16 step:15886 [D loss: 0.601875, acc: 67.97%] [G loss: 1.969236]\n",
      "epoch:16 step:15887 [D loss: 0.680069, acc: 56.25%] [G loss: 1.956382]\n",
      "epoch:16 step:15888 [D loss: 0.671132, acc: 58.59%] [G loss: 1.815580]\n",
      "epoch:16 step:15889 [D loss: 0.568806, acc: 71.88%] [G loss: 2.003986]\n",
      "epoch:16 step:15890 [D loss: 0.671497, acc: 60.94%] [G loss: 1.851542]\n",
      "epoch:16 step:15891 [D loss: 0.640606, acc: 61.72%] [G loss: 2.119677]\n",
      "epoch:16 step:15892 [D loss: 0.601678, acc: 64.84%] [G loss: 2.103041]\n",
      "epoch:16 step:15893 [D loss: 0.624699, acc: 64.06%] [G loss: 1.912297]\n",
      "epoch:16 step:15894 [D loss: 0.619024, acc: 60.94%] [G loss: 1.885452]\n",
      "epoch:16 step:15895 [D loss: 0.655830, acc: 57.81%] [G loss: 1.983357]\n",
      "epoch:16 step:15896 [D loss: 0.649357, acc: 63.28%] [G loss: 1.965339]\n",
      "epoch:16 step:15897 [D loss: 0.612476, acc: 64.06%] [G loss: 1.982589]\n",
      "epoch:16 step:15898 [D loss: 0.661715, acc: 57.03%] [G loss: 1.969267]\n",
      "epoch:16 step:15899 [D loss: 0.583778, acc: 72.66%] [G loss: 1.880744]\n",
      "epoch:16 step:15900 [D loss: 0.644247, acc: 64.84%] [G loss: 2.076394]\n",
      "epoch:16 step:15901 [D loss: 0.714025, acc: 50.78%] [G loss: 2.123590]\n",
      "epoch:16 step:15902 [D loss: 0.648061, acc: 63.28%] [G loss: 1.856863]\n",
      "epoch:16 step:15903 [D loss: 0.664197, acc: 59.38%] [G loss: 1.997404]\n",
      "epoch:16 step:15904 [D loss: 0.579911, acc: 69.53%] [G loss: 2.183464]\n",
      "epoch:16 step:15905 [D loss: 0.697288, acc: 60.16%] [G loss: 1.877026]\n",
      "epoch:16 step:15906 [D loss: 0.712173, acc: 51.56%] [G loss: 1.763704]\n",
      "epoch:16 step:15907 [D loss: 0.611949, acc: 67.19%] [G loss: 2.003096]\n",
      "epoch:16 step:15908 [D loss: 0.608320, acc: 65.62%] [G loss: 1.915897]\n",
      "epoch:16 step:15909 [D loss: 0.641144, acc: 63.28%] [G loss: 1.954761]\n",
      "epoch:16 step:15910 [D loss: 0.545335, acc: 75.00%] [G loss: 2.089975]\n",
      "epoch:16 step:15911 [D loss: 0.625385, acc: 64.84%] [G loss: 2.206415]\n",
      "epoch:16 step:15912 [D loss: 0.711855, acc: 53.91%] [G loss: 1.672368]\n",
      "epoch:16 step:15913 [D loss: 0.704023, acc: 51.56%] [G loss: 1.908910]\n",
      "epoch:16 step:15914 [D loss: 0.577454, acc: 71.09%] [G loss: 1.968783]\n",
      "epoch:16 step:15915 [D loss: 0.588668, acc: 70.31%] [G loss: 2.067437]\n",
      "epoch:16 step:15916 [D loss: 0.563331, acc: 77.34%] [G loss: 2.114855]\n",
      "epoch:16 step:15917 [D loss: 0.587825, acc: 70.31%] [G loss: 2.368325]\n",
      "epoch:16 step:15918 [D loss: 0.609827, acc: 68.75%] [G loss: 2.225498]\n",
      "epoch:16 step:15919 [D loss: 0.612273, acc: 66.41%] [G loss: 2.153473]\n",
      "epoch:16 step:15920 [D loss: 0.747193, acc: 49.22%] [G loss: 1.828449]\n",
      "epoch:16 step:15921 [D loss: 0.763674, acc: 46.09%] [G loss: 1.981614]\n",
      "epoch:16 step:15922 [D loss: 0.569703, acc: 72.66%] [G loss: 2.132958]\n",
      "epoch:16 step:15923 [D loss: 0.681005, acc: 58.59%] [G loss: 2.117967]\n",
      "epoch:16 step:15924 [D loss: 0.631896, acc: 57.03%] [G loss: 1.915248]\n",
      "epoch:16 step:15925 [D loss: 0.655944, acc: 60.16%] [G loss: 1.994916]\n",
      "epoch:16 step:15926 [D loss: 0.598784, acc: 65.62%] [G loss: 1.881136]\n",
      "epoch:16 step:15927 [D loss: 0.662009, acc: 62.50%] [G loss: 1.830315]\n",
      "epoch:16 step:15928 [D loss: 0.606543, acc: 68.75%] [G loss: 2.075669]\n",
      "epoch:16 step:15929 [D loss: 0.603865, acc: 71.88%] [G loss: 2.369969]\n",
      "epoch:17 step:15930 [D loss: 0.638833, acc: 59.38%] [G loss: 1.887017]\n",
      "epoch:17 step:15931 [D loss: 0.643507, acc: 64.84%] [G loss: 2.016519]\n",
      "epoch:17 step:15932 [D loss: 0.604696, acc: 69.53%] [G loss: 1.956656]\n",
      "epoch:17 step:15933 [D loss: 0.645577, acc: 57.81%] [G loss: 1.790413]\n",
      "epoch:17 step:15934 [D loss: 0.622173, acc: 69.53%] [G loss: 2.101251]\n",
      "epoch:17 step:15935 [D loss: 0.648213, acc: 62.50%] [G loss: 1.925362]\n",
      "epoch:17 step:15936 [D loss: 0.627533, acc: 64.06%] [G loss: 1.965555]\n",
      "epoch:17 step:15937 [D loss: 0.638902, acc: 65.62%] [G loss: 1.897793]\n",
      "epoch:17 step:15938 [D loss: 0.616612, acc: 67.19%] [G loss: 2.024633]\n",
      "epoch:17 step:15939 [D loss: 0.582671, acc: 71.88%] [G loss: 2.138236]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:17 step:15940 [D loss: 0.649062, acc: 60.94%] [G loss: 2.090559]\n",
      "epoch:17 step:15941 [D loss: 0.616393, acc: 63.28%] [G loss: 1.901696]\n",
      "epoch:17 step:15942 [D loss: 0.609439, acc: 68.75%] [G loss: 2.005335]\n",
      "epoch:17 step:15943 [D loss: 0.640941, acc: 64.06%] [G loss: 1.986151]\n",
      "epoch:17 step:15944 [D loss: 0.638321, acc: 64.84%] [G loss: 2.088758]\n",
      "epoch:17 step:15945 [D loss: 0.593039, acc: 70.31%] [G loss: 2.097357]\n",
      "epoch:17 step:15946 [D loss: 0.633112, acc: 66.41%] [G loss: 1.975778]\n",
      "epoch:17 step:15947 [D loss: 0.594406, acc: 68.75%] [G loss: 2.063757]\n",
      "epoch:17 step:15948 [D loss: 0.684378, acc: 58.59%] [G loss: 1.979731]\n",
      "epoch:17 step:15949 [D loss: 0.680701, acc: 56.25%] [G loss: 1.764090]\n",
      "epoch:17 step:15950 [D loss: 0.689252, acc: 60.16%] [G loss: 1.837651]\n",
      "epoch:17 step:15951 [D loss: 0.669330, acc: 56.25%] [G loss: 1.829002]\n",
      "epoch:17 step:15952 [D loss: 0.607281, acc: 69.53%] [G loss: 2.007470]\n",
      "epoch:17 step:15953 [D loss: 0.616758, acc: 67.97%] [G loss: 1.949102]\n",
      "epoch:17 step:15954 [D loss: 0.654689, acc: 60.16%] [G loss: 1.936481]\n",
      "epoch:17 step:15955 [D loss: 0.683260, acc: 57.81%] [G loss: 1.782323]\n",
      "epoch:17 step:15956 [D loss: 0.638275, acc: 63.28%] [G loss: 1.833176]\n",
      "epoch:17 step:15957 [D loss: 0.633890, acc: 64.06%] [G loss: 1.895279]\n",
      "epoch:17 step:15958 [D loss: 0.614724, acc: 69.53%] [G loss: 1.922295]\n",
      "epoch:17 step:15959 [D loss: 0.671498, acc: 62.50%] [G loss: 2.018910]\n",
      "epoch:17 step:15960 [D loss: 0.701300, acc: 54.69%] [G loss: 1.657469]\n",
      "epoch:17 step:15961 [D loss: 0.724931, acc: 50.78%] [G loss: 1.725400]\n",
      "epoch:17 step:15962 [D loss: 0.639835, acc: 63.28%] [G loss: 1.819143]\n",
      "epoch:17 step:15963 [D loss: 0.623106, acc: 64.84%] [G loss: 1.794368]\n",
      "epoch:17 step:15964 [D loss: 0.634531, acc: 61.72%] [G loss: 1.879696]\n",
      "epoch:17 step:15965 [D loss: 0.594788, acc: 68.75%] [G loss: 1.820112]\n",
      "epoch:17 step:15966 [D loss: 0.593451, acc: 69.53%] [G loss: 1.996492]\n",
      "epoch:17 step:15967 [D loss: 0.682599, acc: 56.25%] [G loss: 1.850576]\n",
      "epoch:17 step:15968 [D loss: 0.634835, acc: 63.28%] [G loss: 2.037689]\n",
      "epoch:17 step:15969 [D loss: 0.631868, acc: 64.84%] [G loss: 2.109960]\n",
      "epoch:17 step:15970 [D loss: 0.664984, acc: 60.94%] [G loss: 1.950218]\n",
      "epoch:17 step:15971 [D loss: 0.599107, acc: 65.62%] [G loss: 2.014690]\n",
      "epoch:17 step:15972 [D loss: 0.638175, acc: 68.75%] [G loss: 1.976619]\n",
      "epoch:17 step:15973 [D loss: 0.684798, acc: 56.25%] [G loss: 1.869722]\n",
      "epoch:17 step:15974 [D loss: 0.611628, acc: 69.53%] [G loss: 1.972087]\n",
      "epoch:17 step:15975 [D loss: 0.670173, acc: 55.47%] [G loss: 1.929189]\n",
      "epoch:17 step:15976 [D loss: 0.663580, acc: 60.16%] [G loss: 1.921664]\n",
      "epoch:17 step:15977 [D loss: 0.610425, acc: 62.50%] [G loss: 1.918479]\n",
      "epoch:17 step:15978 [D loss: 0.651266, acc: 60.16%] [G loss: 2.006785]\n",
      "epoch:17 step:15979 [D loss: 0.634522, acc: 60.94%] [G loss: 1.919668]\n",
      "epoch:17 step:15980 [D loss: 0.634564, acc: 65.62%] [G loss: 1.895910]\n",
      "epoch:17 step:15981 [D loss: 0.616223, acc: 67.97%] [G loss: 2.014370]\n",
      "epoch:17 step:15982 [D loss: 0.630242, acc: 64.84%] [G loss: 1.953226]\n",
      "epoch:17 step:15983 [D loss: 0.602535, acc: 67.19%] [G loss: 2.017158]\n",
      "epoch:17 step:15984 [D loss: 0.638800, acc: 68.75%] [G loss: 1.950977]\n",
      "epoch:17 step:15985 [D loss: 0.657788, acc: 64.06%] [G loss: 2.043965]\n",
      "epoch:17 step:15986 [D loss: 0.637441, acc: 66.41%] [G loss: 1.862246]\n",
      "epoch:17 step:15987 [D loss: 0.646842, acc: 60.94%] [G loss: 1.977190]\n",
      "epoch:17 step:15988 [D loss: 0.604370, acc: 67.19%] [G loss: 2.002075]\n",
      "epoch:17 step:15989 [D loss: 0.630919, acc: 66.41%] [G loss: 1.904955]\n",
      "epoch:17 step:15990 [D loss: 0.674607, acc: 57.81%] [G loss: 1.873161]\n",
      "epoch:17 step:15991 [D loss: 0.664076, acc: 64.06%] [G loss: 1.944478]\n",
      "epoch:17 step:15992 [D loss: 0.612295, acc: 66.41%] [G loss: 1.825912]\n",
      "epoch:17 step:15993 [D loss: 0.606556, acc: 63.28%] [G loss: 1.943624]\n",
      "epoch:17 step:15994 [D loss: 0.605896, acc: 71.88%] [G loss: 1.962016]\n",
      "epoch:17 step:15995 [D loss: 0.669536, acc: 57.81%] [G loss: 1.869618]\n",
      "epoch:17 step:15996 [D loss: 0.585515, acc: 67.97%] [G loss: 1.960050]\n",
      "epoch:17 step:15997 [D loss: 0.617745, acc: 65.62%] [G loss: 2.033864]\n",
      "epoch:17 step:15998 [D loss: 0.640019, acc: 63.28%] [G loss: 2.133638]\n",
      "epoch:17 step:15999 [D loss: 0.637629, acc: 62.50%] [G loss: 1.997254]\n",
      "epoch:17 step:16000 [D loss: 0.645956, acc: 63.28%] [G loss: 1.881992]\n",
      "##############\n",
      "[2.49682803 1.4704431  6.15056726 4.67738932 3.63689968 5.74592536\n",
      " 4.32995887 4.67601281 4.68175069 3.65400443]\n",
      "##########\n",
      "epoch:17 step:16001 [D loss: 0.593549, acc: 71.88%] [G loss: 1.930457]\n",
      "epoch:17 step:16002 [D loss: 0.643879, acc: 66.41%] [G loss: 1.802598]\n",
      "epoch:17 step:16003 [D loss: 0.637673, acc: 61.72%] [G loss: 2.127673]\n",
      "epoch:17 step:16004 [D loss: 0.675578, acc: 60.94%] [G loss: 2.039536]\n",
      "epoch:17 step:16005 [D loss: 0.650133, acc: 62.50%] [G loss: 2.017259]\n",
      "epoch:17 step:16006 [D loss: 0.530162, acc: 76.56%] [G loss: 2.285106]\n",
      "epoch:17 step:16007 [D loss: 0.632266, acc: 64.06%] [G loss: 1.887010]\n",
      "epoch:17 step:16008 [D loss: 0.664404, acc: 58.59%] [G loss: 1.945560]\n",
      "epoch:17 step:16009 [D loss: 0.676982, acc: 60.94%] [G loss: 1.937821]\n",
      "epoch:17 step:16010 [D loss: 0.695395, acc: 59.38%] [G loss: 1.789808]\n",
      "epoch:17 step:16011 [D loss: 0.651590, acc: 59.38%] [G loss: 1.876055]\n",
      "epoch:17 step:16012 [D loss: 0.670361, acc: 62.50%] [G loss: 1.934104]\n",
      "epoch:17 step:16013 [D loss: 0.621322, acc: 62.50%] [G loss: 1.974301]\n",
      "epoch:17 step:16014 [D loss: 0.667298, acc: 61.72%] [G loss: 1.812968]\n",
      "epoch:17 step:16015 [D loss: 0.680575, acc: 60.94%] [G loss: 1.819710]\n",
      "epoch:17 step:16016 [D loss: 0.598182, acc: 71.88%] [G loss: 1.936412]\n",
      "epoch:17 step:16017 [D loss: 0.626158, acc: 66.41%] [G loss: 1.891938]\n",
      "epoch:17 step:16018 [D loss: 0.568718, acc: 74.22%] [G loss: 1.993483]\n",
      "epoch:17 step:16019 [D loss: 0.652135, acc: 64.06%] [G loss: 1.906663]\n",
      "epoch:17 step:16020 [D loss: 0.710108, acc: 56.25%] [G loss: 1.864367]\n",
      "epoch:17 step:16021 [D loss: 0.575238, acc: 70.31%] [G loss: 1.994912]\n",
      "epoch:17 step:16022 [D loss: 0.636961, acc: 63.28%] [G loss: 2.012377]\n",
      "epoch:17 step:16023 [D loss: 0.596124, acc: 67.97%] [G loss: 1.938847]\n",
      "epoch:17 step:16024 [D loss: 0.661788, acc: 62.50%] [G loss: 1.849025]\n",
      "epoch:17 step:16025 [D loss: 0.621522, acc: 64.84%] [G loss: 1.997315]\n",
      "epoch:17 step:16026 [D loss: 0.607235, acc: 67.97%] [G loss: 2.058333]\n",
      "epoch:17 step:16027 [D loss: 0.689341, acc: 60.16%] [G loss: 1.763683]\n",
      "epoch:17 step:16028 [D loss: 0.629977, acc: 66.41%] [G loss: 1.821135]\n",
      "epoch:17 step:16029 [D loss: 0.641871, acc: 62.50%] [G loss: 1.787945]\n",
      "epoch:17 step:16030 [D loss: 0.629560, acc: 64.06%] [G loss: 1.963534]\n",
      "epoch:17 step:16031 [D loss: 0.620770, acc: 64.84%] [G loss: 1.874999]\n",
      "epoch:17 step:16032 [D loss: 0.605649, acc: 69.53%] [G loss: 1.920250]\n",
      "epoch:17 step:16033 [D loss: 0.711088, acc: 60.16%] [G loss: 1.955275]\n",
      "epoch:17 step:16034 [D loss: 0.669250, acc: 60.16%] [G loss: 1.910870]\n",
      "epoch:17 step:16035 [D loss: 0.623421, acc: 60.94%] [G loss: 2.026220]\n",
      "epoch:17 step:16036 [D loss: 0.605273, acc: 66.41%] [G loss: 2.228266]\n",
      "epoch:17 step:16037 [D loss: 0.683299, acc: 57.03%] [G loss: 1.885200]\n",
      "epoch:17 step:16038 [D loss: 0.718411, acc: 53.12%] [G loss: 1.743360]\n",
      "epoch:17 step:16039 [D loss: 0.632735, acc: 64.84%] [G loss: 1.706272]\n",
      "epoch:17 step:16040 [D loss: 0.594304, acc: 69.53%] [G loss: 1.984538]\n",
      "epoch:17 step:16041 [D loss: 0.652739, acc: 69.53%] [G loss: 1.888500]\n",
      "epoch:17 step:16042 [D loss: 0.611310, acc: 67.97%] [G loss: 2.019647]\n",
      "epoch:17 step:16043 [D loss: 0.655119, acc: 64.06%] [G loss: 2.083126]\n",
      "epoch:17 step:16044 [D loss: 0.602764, acc: 70.31%] [G loss: 2.160072]\n",
      "epoch:17 step:16045 [D loss: 0.599494, acc: 66.41%] [G loss: 2.058075]\n",
      "epoch:17 step:16046 [D loss: 0.619220, acc: 64.06%] [G loss: 1.987009]\n",
      "epoch:17 step:16047 [D loss: 0.638316, acc: 65.62%] [G loss: 1.878540]\n",
      "epoch:17 step:16048 [D loss: 0.620140, acc: 67.97%] [G loss: 2.350532]\n",
      "epoch:17 step:16049 [D loss: 0.672355, acc: 60.16%] [G loss: 1.856178]\n",
      "epoch:17 step:16050 [D loss: 0.691587, acc: 62.50%] [G loss: 2.007753]\n",
      "epoch:17 step:16051 [D loss: 0.685763, acc: 54.69%] [G loss: 2.117278]\n",
      "epoch:17 step:16052 [D loss: 0.622102, acc: 66.41%] [G loss: 1.770700]\n",
      "epoch:17 step:16053 [D loss: 0.751551, acc: 52.34%] [G loss: 1.967693]\n",
      "epoch:17 step:16054 [D loss: 0.698405, acc: 51.56%] [G loss: 1.726173]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:17 step:16055 [D loss: 0.619349, acc: 64.84%] [G loss: 1.951694]\n",
      "epoch:17 step:16056 [D loss: 0.663937, acc: 58.59%] [G loss: 1.786515]\n",
      "epoch:17 step:16057 [D loss: 0.666987, acc: 65.62%] [G loss: 1.856710]\n",
      "epoch:17 step:16058 [D loss: 0.648540, acc: 61.72%] [G loss: 1.810384]\n",
      "epoch:17 step:16059 [D loss: 0.659916, acc: 56.25%] [G loss: 2.036775]\n",
      "epoch:17 step:16060 [D loss: 0.622119, acc: 65.62%] [G loss: 1.938620]\n",
      "epoch:17 step:16061 [D loss: 0.670824, acc: 57.03%] [G loss: 1.915883]\n",
      "epoch:17 step:16062 [D loss: 0.679907, acc: 57.81%] [G loss: 1.765019]\n",
      "epoch:17 step:16063 [D loss: 0.676693, acc: 63.28%] [G loss: 1.745851]\n",
      "epoch:17 step:16064 [D loss: 0.633861, acc: 64.84%] [G loss: 1.760025]\n",
      "epoch:17 step:16065 [D loss: 0.658008, acc: 63.28%] [G loss: 1.781686]\n",
      "epoch:17 step:16066 [D loss: 0.658427, acc: 62.50%] [G loss: 1.911361]\n",
      "epoch:17 step:16067 [D loss: 0.671907, acc: 59.38%] [G loss: 1.977608]\n",
      "epoch:17 step:16068 [D loss: 0.674754, acc: 60.94%] [G loss: 1.896671]\n",
      "epoch:17 step:16069 [D loss: 0.619498, acc: 69.53%] [G loss: 1.980847]\n",
      "epoch:17 step:16070 [D loss: 0.643988, acc: 62.50%] [G loss: 1.873004]\n",
      "epoch:17 step:16071 [D loss: 0.669810, acc: 59.38%] [G loss: 1.811043]\n",
      "epoch:17 step:16072 [D loss: 0.677025, acc: 59.38%] [G loss: 1.920445]\n",
      "epoch:17 step:16073 [D loss: 0.687382, acc: 57.81%] [G loss: 1.948097]\n",
      "epoch:17 step:16074 [D loss: 0.612006, acc: 68.75%] [G loss: 1.945786]\n",
      "epoch:17 step:16075 [D loss: 0.621582, acc: 64.06%] [G loss: 2.049030]\n",
      "epoch:17 step:16076 [D loss: 0.642776, acc: 64.06%] [G loss: 1.957010]\n",
      "epoch:17 step:16077 [D loss: 0.640893, acc: 63.28%] [G loss: 1.881162]\n",
      "epoch:17 step:16078 [D loss: 0.659617, acc: 61.72%] [G loss: 1.986013]\n",
      "epoch:17 step:16079 [D loss: 0.623394, acc: 62.50%] [G loss: 1.978985]\n",
      "epoch:17 step:16080 [D loss: 0.635007, acc: 64.06%] [G loss: 2.017071]\n",
      "epoch:17 step:16081 [D loss: 0.632073, acc: 62.50%] [G loss: 2.015977]\n",
      "epoch:17 step:16082 [D loss: 0.647185, acc: 58.59%] [G loss: 1.796299]\n",
      "epoch:17 step:16083 [D loss: 0.673383, acc: 57.03%] [G loss: 1.876695]\n",
      "epoch:17 step:16084 [D loss: 0.623517, acc: 60.16%] [G loss: 1.905336]\n",
      "epoch:17 step:16085 [D loss: 0.624109, acc: 64.84%] [G loss: 2.055410]\n",
      "epoch:17 step:16086 [D loss: 0.654779, acc: 60.94%] [G loss: 1.861184]\n",
      "epoch:17 step:16087 [D loss: 0.661915, acc: 57.81%] [G loss: 1.807463]\n",
      "epoch:17 step:16088 [D loss: 0.670704, acc: 60.16%] [G loss: 1.969839]\n",
      "epoch:17 step:16089 [D loss: 0.713048, acc: 54.69%] [G loss: 1.685645]\n",
      "epoch:17 step:16090 [D loss: 0.653471, acc: 64.84%] [G loss: 1.861641]\n",
      "epoch:17 step:16091 [D loss: 0.691776, acc: 57.81%] [G loss: 1.888980]\n",
      "epoch:17 step:16092 [D loss: 0.637574, acc: 65.62%] [G loss: 1.807501]\n",
      "epoch:17 step:16093 [D loss: 0.659533, acc: 56.25%] [G loss: 1.835219]\n",
      "epoch:17 step:16094 [D loss: 0.627483, acc: 60.16%] [G loss: 1.876533]\n",
      "epoch:17 step:16095 [D loss: 0.661587, acc: 60.94%] [G loss: 1.810148]\n",
      "epoch:17 step:16096 [D loss: 0.633922, acc: 65.62%] [G loss: 1.856312]\n",
      "epoch:17 step:16097 [D loss: 0.663624, acc: 62.50%] [G loss: 1.813727]\n",
      "epoch:17 step:16098 [D loss: 0.649772, acc: 62.50%] [G loss: 1.890645]\n",
      "epoch:17 step:16099 [D loss: 0.633457, acc: 64.06%] [G loss: 1.836253]\n",
      "epoch:17 step:16100 [D loss: 0.670294, acc: 57.03%] [G loss: 1.870907]\n",
      "epoch:17 step:16101 [D loss: 0.634955, acc: 59.38%] [G loss: 1.833768]\n",
      "epoch:17 step:16102 [D loss: 0.654936, acc: 56.25%] [G loss: 1.914853]\n",
      "epoch:17 step:16103 [D loss: 0.652561, acc: 57.81%] [G loss: 1.767440]\n",
      "epoch:17 step:16104 [D loss: 0.650077, acc: 62.50%] [G loss: 1.860521]\n",
      "epoch:17 step:16105 [D loss: 0.673930, acc: 58.59%] [G loss: 1.889528]\n",
      "epoch:17 step:16106 [D loss: 0.643099, acc: 64.84%] [G loss: 1.836956]\n",
      "epoch:17 step:16107 [D loss: 0.666542, acc: 59.38%] [G loss: 1.761374]\n",
      "epoch:17 step:16108 [D loss: 0.602297, acc: 67.19%] [G loss: 1.742151]\n",
      "epoch:17 step:16109 [D loss: 0.664593, acc: 60.94%] [G loss: 1.798685]\n",
      "epoch:17 step:16110 [D loss: 0.658696, acc: 60.16%] [G loss: 1.831508]\n",
      "epoch:17 step:16111 [D loss: 0.677898, acc: 57.81%] [G loss: 1.760576]\n",
      "epoch:17 step:16112 [D loss: 0.690812, acc: 51.56%] [G loss: 1.831390]\n",
      "epoch:17 step:16113 [D loss: 0.641912, acc: 67.97%] [G loss: 1.720770]\n",
      "epoch:17 step:16114 [D loss: 0.655944, acc: 62.50%] [G loss: 1.813630]\n",
      "epoch:17 step:16115 [D loss: 0.637576, acc: 67.97%] [G loss: 1.891506]\n",
      "epoch:17 step:16116 [D loss: 0.668434, acc: 62.50%] [G loss: 1.827688]\n",
      "epoch:17 step:16117 [D loss: 0.668100, acc: 59.38%] [G loss: 1.880929]\n",
      "epoch:17 step:16118 [D loss: 0.654353, acc: 57.03%] [G loss: 1.921065]\n",
      "epoch:17 step:16119 [D loss: 0.639579, acc: 65.62%] [G loss: 1.804207]\n",
      "epoch:17 step:16120 [D loss: 0.625068, acc: 62.50%] [G loss: 1.828858]\n",
      "epoch:17 step:16121 [D loss: 0.632574, acc: 67.19%] [G loss: 1.923303]\n",
      "epoch:17 step:16122 [D loss: 0.651617, acc: 63.28%] [G loss: 1.932737]\n",
      "epoch:17 step:16123 [D loss: 0.568150, acc: 69.53%] [G loss: 2.056621]\n",
      "epoch:17 step:16124 [D loss: 0.637424, acc: 62.50%] [G loss: 1.886403]\n",
      "epoch:17 step:16125 [D loss: 0.687377, acc: 60.94%] [G loss: 1.857422]\n",
      "epoch:17 step:16126 [D loss: 0.660648, acc: 65.62%] [G loss: 2.034446]\n",
      "epoch:17 step:16127 [D loss: 0.606754, acc: 71.09%] [G loss: 1.926652]\n",
      "epoch:17 step:16128 [D loss: 0.641810, acc: 61.72%] [G loss: 1.935834]\n",
      "epoch:17 step:16129 [D loss: 0.717269, acc: 57.03%] [G loss: 1.750875]\n",
      "epoch:17 step:16130 [D loss: 0.642713, acc: 62.50%] [G loss: 1.944245]\n",
      "epoch:17 step:16131 [D loss: 0.623963, acc: 62.50%] [G loss: 1.997423]\n",
      "epoch:17 step:16132 [D loss: 0.695050, acc: 57.81%] [G loss: 1.902725]\n",
      "epoch:17 step:16133 [D loss: 0.624470, acc: 64.84%] [G loss: 1.881875]\n",
      "epoch:17 step:16134 [D loss: 0.702929, acc: 51.56%] [G loss: 1.883901]\n",
      "epoch:17 step:16135 [D loss: 0.651419, acc: 60.16%] [G loss: 1.901101]\n",
      "epoch:17 step:16136 [D loss: 0.604541, acc: 68.75%] [G loss: 2.054255]\n",
      "epoch:17 step:16137 [D loss: 0.600655, acc: 64.84%] [G loss: 2.084878]\n",
      "epoch:17 step:16138 [D loss: 0.591060, acc: 63.28%] [G loss: 2.147544]\n",
      "epoch:17 step:16139 [D loss: 0.685252, acc: 59.38%] [G loss: 1.902183]\n",
      "epoch:17 step:16140 [D loss: 0.671714, acc: 60.94%] [G loss: 1.733554]\n",
      "epoch:17 step:16141 [D loss: 0.673297, acc: 52.34%] [G loss: 1.872773]\n",
      "epoch:17 step:16142 [D loss: 0.670592, acc: 61.72%] [G loss: 1.840143]\n",
      "epoch:17 step:16143 [D loss: 0.638108, acc: 64.06%] [G loss: 1.838566]\n",
      "epoch:17 step:16144 [D loss: 0.684187, acc: 58.59%] [G loss: 1.700531]\n",
      "epoch:17 step:16145 [D loss: 0.604887, acc: 63.28%] [G loss: 1.974139]\n",
      "epoch:17 step:16146 [D loss: 0.681374, acc: 53.91%] [G loss: 1.821143]\n",
      "epoch:17 step:16147 [D loss: 0.592619, acc: 65.62%] [G loss: 2.073751]\n",
      "epoch:17 step:16148 [D loss: 0.647738, acc: 63.28%] [G loss: 1.965780]\n",
      "epoch:17 step:16149 [D loss: 0.655893, acc: 60.16%] [G loss: 1.771955]\n",
      "epoch:17 step:16150 [D loss: 0.656080, acc: 62.50%] [G loss: 1.933587]\n",
      "epoch:17 step:16151 [D loss: 0.645507, acc: 58.59%] [G loss: 1.852809]\n",
      "epoch:17 step:16152 [D loss: 0.684419, acc: 60.94%] [G loss: 1.906426]\n",
      "epoch:17 step:16153 [D loss: 0.683870, acc: 59.38%] [G loss: 1.774830]\n",
      "epoch:17 step:16154 [D loss: 0.676714, acc: 64.06%] [G loss: 1.919641]\n",
      "epoch:17 step:16155 [D loss: 0.651428, acc: 62.50%] [G loss: 1.823011]\n",
      "epoch:17 step:16156 [D loss: 0.623613, acc: 65.62%] [G loss: 1.791258]\n",
      "epoch:17 step:16157 [D loss: 0.691744, acc: 57.03%] [G loss: 1.729693]\n",
      "epoch:17 step:16158 [D loss: 0.601749, acc: 64.84%] [G loss: 1.965626]\n",
      "epoch:17 step:16159 [D loss: 0.576473, acc: 71.88%] [G loss: 2.090941]\n",
      "epoch:17 step:16160 [D loss: 0.595150, acc: 66.41%] [G loss: 2.168264]\n",
      "epoch:17 step:16161 [D loss: 0.591361, acc: 67.19%] [G loss: 2.213207]\n",
      "epoch:17 step:16162 [D loss: 0.671657, acc: 59.38%] [G loss: 1.855103]\n",
      "epoch:17 step:16163 [D loss: 0.691730, acc: 55.47%] [G loss: 1.841724]\n",
      "epoch:17 step:16164 [D loss: 0.651557, acc: 63.28%] [G loss: 1.765831]\n",
      "epoch:17 step:16165 [D loss: 0.648195, acc: 64.06%] [G loss: 1.841718]\n",
      "epoch:17 step:16166 [D loss: 0.612401, acc: 67.19%] [G loss: 1.935354]\n",
      "epoch:17 step:16167 [D loss: 0.710270, acc: 54.69%] [G loss: 1.857321]\n",
      "epoch:17 step:16168 [D loss: 0.682348, acc: 58.59%] [G loss: 1.851366]\n",
      "epoch:17 step:16169 [D loss: 0.691608, acc: 56.25%] [G loss: 1.846446]\n",
      "epoch:17 step:16170 [D loss: 0.637600, acc: 60.94%] [G loss: 1.832449]\n",
      "epoch:17 step:16171 [D loss: 0.611917, acc: 64.84%] [G loss: 1.850435]\n",
      "epoch:17 step:16172 [D loss: 0.621333, acc: 64.84%] [G loss: 1.896021]\n",
      "epoch:17 step:16173 [D loss: 0.637752, acc: 64.06%] [G loss: 1.972791]\n",
      "epoch:17 step:16174 [D loss: 0.622865, acc: 65.62%] [G loss: 1.910746]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:17 step:16175 [D loss: 0.630523, acc: 64.06%] [G loss: 1.983185]\n",
      "epoch:17 step:16176 [D loss: 0.654014, acc: 60.94%] [G loss: 1.860843]\n",
      "epoch:17 step:16177 [D loss: 0.602172, acc: 63.28%] [G loss: 2.061653]\n",
      "epoch:17 step:16178 [D loss: 0.671318, acc: 54.69%] [G loss: 1.902628]\n",
      "epoch:17 step:16179 [D loss: 0.688904, acc: 52.34%] [G loss: 1.611462]\n",
      "epoch:17 step:16180 [D loss: 0.707955, acc: 51.56%] [G loss: 1.783606]\n",
      "epoch:17 step:16181 [D loss: 0.684600, acc: 59.38%] [G loss: 1.824474]\n",
      "epoch:17 step:16182 [D loss: 0.692046, acc: 64.84%] [G loss: 1.844604]\n",
      "epoch:17 step:16183 [D loss: 0.632816, acc: 60.94%] [G loss: 1.750140]\n",
      "epoch:17 step:16184 [D loss: 0.614947, acc: 69.53%] [G loss: 1.905048]\n",
      "epoch:17 step:16185 [D loss: 0.638446, acc: 63.28%] [G loss: 1.743492]\n",
      "epoch:17 step:16186 [D loss: 0.688660, acc: 56.25%] [G loss: 1.804446]\n",
      "epoch:17 step:16187 [D loss: 0.603630, acc: 70.31%] [G loss: 1.905200]\n",
      "epoch:17 step:16188 [D loss: 0.695506, acc: 54.69%] [G loss: 1.856653]\n",
      "epoch:17 step:16189 [D loss: 0.660059, acc: 61.72%] [G loss: 1.820095]\n",
      "epoch:17 step:16190 [D loss: 0.639986, acc: 65.62%] [G loss: 1.842637]\n",
      "epoch:17 step:16191 [D loss: 0.622166, acc: 63.28%] [G loss: 2.000716]\n",
      "epoch:17 step:16192 [D loss: 0.652131, acc: 59.38%] [G loss: 2.073457]\n",
      "epoch:17 step:16193 [D loss: 0.601709, acc: 68.75%] [G loss: 2.111352]\n",
      "epoch:17 step:16194 [D loss: 0.673307, acc: 60.16%] [G loss: 1.973465]\n",
      "epoch:17 step:16195 [D loss: 0.641436, acc: 64.06%] [G loss: 1.860899]\n",
      "epoch:17 step:16196 [D loss: 0.618662, acc: 67.97%] [G loss: 1.888499]\n",
      "epoch:17 step:16197 [D loss: 0.676376, acc: 60.16%] [G loss: 1.807110]\n",
      "epoch:17 step:16198 [D loss: 0.658459, acc: 62.50%] [G loss: 1.989078]\n",
      "epoch:17 step:16199 [D loss: 0.637446, acc: 62.50%] [G loss: 1.839917]\n",
      "epoch:17 step:16200 [D loss: 0.613646, acc: 64.84%] [G loss: 1.954004]\n",
      "##############\n",
      "[2.54967597 1.38624906 6.42235652 4.71862503 3.69230468 5.94883685\n",
      " 4.32036351 4.77622251 4.6090769  3.69926524]\n",
      "##########\n",
      "epoch:17 step:16201 [D loss: 0.593671, acc: 71.88%] [G loss: 1.989261]\n",
      "epoch:17 step:16202 [D loss: 0.574186, acc: 68.75%] [G loss: 1.978803]\n",
      "epoch:17 step:16203 [D loss: 0.599479, acc: 66.41%] [G loss: 2.103793]\n",
      "epoch:17 step:16204 [D loss: 0.644628, acc: 65.62%] [G loss: 1.929245]\n",
      "epoch:17 step:16205 [D loss: 0.602789, acc: 73.44%] [G loss: 2.076513]\n",
      "epoch:17 step:16206 [D loss: 0.642309, acc: 60.16%] [G loss: 1.903346]\n",
      "epoch:17 step:16207 [D loss: 0.637912, acc: 72.66%] [G loss: 1.889243]\n",
      "epoch:17 step:16208 [D loss: 0.605469, acc: 68.75%] [G loss: 1.882170]\n",
      "epoch:17 step:16209 [D loss: 0.621800, acc: 66.41%] [G loss: 1.938127]\n",
      "epoch:17 step:16210 [D loss: 0.693164, acc: 54.69%] [G loss: 1.875797]\n",
      "epoch:17 step:16211 [D loss: 0.640227, acc: 66.41%] [G loss: 2.025118]\n",
      "epoch:17 step:16212 [D loss: 0.664373, acc: 56.25%] [G loss: 1.901039]\n",
      "epoch:17 step:16213 [D loss: 0.597120, acc: 67.97%] [G loss: 1.963933]\n",
      "epoch:17 step:16214 [D loss: 0.596943, acc: 66.41%] [G loss: 1.996184]\n",
      "epoch:17 step:16215 [D loss: 0.588132, acc: 70.31%] [G loss: 1.973324]\n",
      "epoch:17 step:16216 [D loss: 0.635479, acc: 65.62%] [G loss: 1.769497]\n",
      "epoch:17 step:16217 [D loss: 0.728492, acc: 51.56%] [G loss: 1.998406]\n",
      "epoch:17 step:16218 [D loss: 0.658281, acc: 60.94%] [G loss: 1.906774]\n",
      "epoch:17 step:16219 [D loss: 0.646629, acc: 60.16%] [G loss: 1.957579]\n",
      "epoch:17 step:16220 [D loss: 0.692464, acc: 57.81%] [G loss: 1.887691]\n",
      "epoch:17 step:16221 [D loss: 0.594368, acc: 72.66%] [G loss: 1.954625]\n",
      "epoch:17 step:16222 [D loss: 0.657524, acc: 60.94%] [G loss: 1.972237]\n",
      "epoch:17 step:16223 [D loss: 0.643554, acc: 59.38%] [G loss: 1.887615]\n",
      "epoch:17 step:16224 [D loss: 0.658428, acc: 58.59%] [G loss: 1.901887]\n",
      "epoch:17 step:16225 [D loss: 0.621975, acc: 61.72%] [G loss: 1.966660]\n",
      "epoch:17 step:16226 [D loss: 0.634095, acc: 65.62%] [G loss: 1.817026]\n",
      "epoch:17 step:16227 [D loss: 0.614066, acc: 63.28%] [G loss: 1.985020]\n",
      "epoch:17 step:16228 [D loss: 0.598518, acc: 66.41%] [G loss: 1.982365]\n",
      "epoch:17 step:16229 [D loss: 0.691231, acc: 63.28%] [G loss: 2.034840]\n",
      "epoch:17 step:16230 [D loss: 0.650696, acc: 57.03%] [G loss: 1.775345]\n",
      "epoch:17 step:16231 [D loss: 0.660840, acc: 61.72%] [G loss: 1.943441]\n",
      "epoch:17 step:16232 [D loss: 0.668214, acc: 59.38%] [G loss: 1.936317]\n",
      "epoch:17 step:16233 [D loss: 0.667431, acc: 63.28%] [G loss: 1.714992]\n",
      "epoch:17 step:16234 [D loss: 0.597201, acc: 67.97%] [G loss: 1.913128]\n",
      "epoch:17 step:16235 [D loss: 0.647277, acc: 61.72%] [G loss: 1.904645]\n",
      "epoch:17 step:16236 [D loss: 0.597533, acc: 75.78%] [G loss: 1.884988]\n",
      "epoch:17 step:16237 [D loss: 0.657619, acc: 60.16%] [G loss: 1.882092]\n",
      "epoch:17 step:16238 [D loss: 0.637266, acc: 65.62%] [G loss: 1.868871]\n",
      "epoch:17 step:16239 [D loss: 0.596795, acc: 67.97%] [G loss: 1.858443]\n",
      "epoch:17 step:16240 [D loss: 0.619725, acc: 63.28%] [G loss: 1.823107]\n",
      "epoch:17 step:16241 [D loss: 0.571261, acc: 71.88%] [G loss: 2.183773]\n",
      "epoch:17 step:16242 [D loss: 0.579819, acc: 67.19%] [G loss: 2.319793]\n",
      "epoch:17 step:16243 [D loss: 0.573921, acc: 71.88%] [G loss: 2.329621]\n",
      "epoch:17 step:16244 [D loss: 0.566225, acc: 71.88%] [G loss: 2.299274]\n",
      "epoch:17 step:16245 [D loss: 0.701544, acc: 53.12%] [G loss: 1.821234]\n",
      "epoch:17 step:16246 [D loss: 0.627844, acc: 63.28%] [G loss: 1.909187]\n",
      "epoch:17 step:16247 [D loss: 0.626451, acc: 65.62%] [G loss: 1.918492]\n",
      "epoch:17 step:16248 [D loss: 0.646325, acc: 59.38%] [G loss: 1.843950]\n",
      "epoch:17 step:16249 [D loss: 0.655968, acc: 63.28%] [G loss: 2.001453]\n",
      "epoch:17 step:16250 [D loss: 0.630115, acc: 61.72%] [G loss: 2.014443]\n",
      "epoch:17 step:16251 [D loss: 0.680566, acc: 57.81%] [G loss: 1.855097]\n",
      "epoch:17 step:16252 [D loss: 0.661666, acc: 64.84%] [G loss: 1.928929]\n",
      "epoch:17 step:16253 [D loss: 0.654805, acc: 65.62%] [G loss: 1.835618]\n",
      "epoch:17 step:16254 [D loss: 0.677365, acc: 58.59%] [G loss: 1.945006]\n",
      "epoch:17 step:16255 [D loss: 0.721177, acc: 50.78%] [G loss: 1.849056]\n",
      "epoch:17 step:16256 [D loss: 0.711917, acc: 47.66%] [G loss: 1.844553]\n",
      "epoch:17 step:16257 [D loss: 0.638695, acc: 60.94%] [G loss: 1.912028]\n",
      "epoch:17 step:16258 [D loss: 0.641720, acc: 64.84%] [G loss: 1.935866]\n",
      "epoch:17 step:16259 [D loss: 0.669788, acc: 60.16%] [G loss: 1.881857]\n",
      "epoch:17 step:16260 [D loss: 0.623458, acc: 67.97%] [G loss: 1.895089]\n",
      "epoch:17 step:16261 [D loss: 0.634895, acc: 62.50%] [G loss: 1.912684]\n",
      "epoch:17 step:16262 [D loss: 0.635650, acc: 59.38%] [G loss: 1.886403]\n",
      "epoch:17 step:16263 [D loss: 0.628449, acc: 63.28%] [G loss: 1.959708]\n",
      "epoch:17 step:16264 [D loss: 0.676978, acc: 57.81%] [G loss: 1.968251]\n",
      "epoch:17 step:16265 [D loss: 0.657347, acc: 63.28%] [G loss: 1.919109]\n",
      "epoch:17 step:16266 [D loss: 0.632912, acc: 67.97%] [G loss: 1.973264]\n",
      "epoch:17 step:16267 [D loss: 0.564453, acc: 71.88%] [G loss: 1.979729]\n",
      "epoch:17 step:16268 [D loss: 0.678954, acc: 56.25%] [G loss: 2.085157]\n",
      "epoch:17 step:16269 [D loss: 0.647300, acc: 60.94%] [G loss: 1.973197]\n",
      "epoch:17 step:16270 [D loss: 0.691059, acc: 64.06%] [G loss: 1.674928]\n",
      "epoch:17 step:16271 [D loss: 0.672107, acc: 59.38%] [G loss: 2.046050]\n",
      "epoch:17 step:16272 [D loss: 0.643883, acc: 59.38%] [G loss: 1.880586]\n",
      "epoch:17 step:16273 [D loss: 0.674265, acc: 57.03%] [G loss: 1.799713]\n",
      "epoch:17 step:16274 [D loss: 0.596394, acc: 71.09%] [G loss: 2.109527]\n",
      "epoch:17 step:16275 [D loss: 0.561147, acc: 75.78%] [G loss: 2.199187]\n",
      "epoch:17 step:16276 [D loss: 0.565181, acc: 71.09%] [G loss: 2.278528]\n",
      "epoch:17 step:16277 [D loss: 0.669914, acc: 54.69%] [G loss: 1.800747]\n",
      "epoch:17 step:16278 [D loss: 0.740004, acc: 54.69%] [G loss: 1.613068]\n",
      "epoch:17 step:16279 [D loss: 0.649737, acc: 60.94%] [G loss: 1.916650]\n",
      "epoch:17 step:16280 [D loss: 0.647114, acc: 63.28%] [G loss: 1.840787]\n",
      "epoch:17 step:16281 [D loss: 0.683831, acc: 57.81%] [G loss: 1.711624]\n",
      "epoch:17 step:16282 [D loss: 0.631391, acc: 62.50%] [G loss: 1.900553]\n",
      "epoch:17 step:16283 [D loss: 0.618641, acc: 66.41%] [G loss: 1.877472]\n",
      "epoch:17 step:16284 [D loss: 0.639762, acc: 66.41%] [G loss: 1.869487]\n",
      "epoch:17 step:16285 [D loss: 0.643646, acc: 64.84%] [G loss: 1.786415]\n",
      "epoch:17 step:16286 [D loss: 0.646422, acc: 61.72%] [G loss: 1.946316]\n",
      "epoch:17 step:16287 [D loss: 0.611791, acc: 67.97%] [G loss: 2.063366]\n",
      "epoch:17 step:16288 [D loss: 0.649199, acc: 61.72%] [G loss: 1.906125]\n",
      "epoch:17 step:16289 [D loss: 0.648252, acc: 61.72%] [G loss: 1.988991]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:17 step:16290 [D loss: 0.670581, acc: 61.72%] [G loss: 1.898526]\n",
      "epoch:17 step:16291 [D loss: 0.661157, acc: 61.72%] [G loss: 1.922547]\n",
      "epoch:17 step:16292 [D loss: 0.628776, acc: 63.28%] [G loss: 1.882126]\n",
      "epoch:17 step:16293 [D loss: 0.638553, acc: 61.72%] [G loss: 1.997785]\n",
      "epoch:17 step:16294 [D loss: 0.699057, acc: 55.47%] [G loss: 1.909631]\n",
      "epoch:17 step:16295 [D loss: 0.657256, acc: 56.25%] [G loss: 1.960952]\n",
      "epoch:17 step:16296 [D loss: 0.643665, acc: 63.28%] [G loss: 1.950237]\n",
      "epoch:17 step:16297 [D loss: 0.666917, acc: 59.38%] [G loss: 1.832044]\n",
      "epoch:17 step:16298 [D loss: 0.650681, acc: 65.62%] [G loss: 1.962819]\n",
      "epoch:17 step:16299 [D loss: 0.622684, acc: 71.88%] [G loss: 2.012322]\n",
      "epoch:17 step:16300 [D loss: 0.629863, acc: 66.41%] [G loss: 2.114965]\n",
      "epoch:17 step:16301 [D loss: 0.635999, acc: 61.72%] [G loss: 1.933179]\n",
      "epoch:17 step:16302 [D loss: 0.691819, acc: 53.91%] [G loss: 1.785989]\n",
      "epoch:17 step:16303 [D loss: 0.621750, acc: 62.50%] [G loss: 2.030096]\n",
      "epoch:17 step:16304 [D loss: 0.605782, acc: 67.19%] [G loss: 1.915156]\n",
      "epoch:17 step:16305 [D loss: 0.679826, acc: 55.47%] [G loss: 1.931075]\n",
      "epoch:17 step:16306 [D loss: 0.709531, acc: 57.03%] [G loss: 1.830260]\n",
      "epoch:17 step:16307 [D loss: 0.652997, acc: 59.38%] [G loss: 1.665658]\n",
      "epoch:17 step:16308 [D loss: 0.656062, acc: 61.72%] [G loss: 1.851654]\n",
      "epoch:17 step:16309 [D loss: 0.650110, acc: 60.16%] [G loss: 1.861359]\n",
      "epoch:17 step:16310 [D loss: 0.612305, acc: 66.41%] [G loss: 1.942752]\n",
      "epoch:17 step:16311 [D loss: 0.624651, acc: 57.81%] [G loss: 1.821459]\n",
      "epoch:17 step:16312 [D loss: 0.638778, acc: 64.84%] [G loss: 1.806578]\n",
      "epoch:17 step:16313 [D loss: 0.654486, acc: 59.38%] [G loss: 2.082929]\n",
      "epoch:17 step:16314 [D loss: 0.578165, acc: 68.75%] [G loss: 1.928872]\n",
      "epoch:17 step:16315 [D loss: 0.675119, acc: 63.28%] [G loss: 1.888611]\n",
      "epoch:17 step:16316 [D loss: 0.647125, acc: 60.16%] [G loss: 1.910260]\n",
      "epoch:17 step:16317 [D loss: 0.609520, acc: 71.09%] [G loss: 1.841326]\n",
      "epoch:17 step:16318 [D loss: 0.659393, acc: 64.06%] [G loss: 1.886820]\n",
      "epoch:17 step:16319 [D loss: 0.616400, acc: 66.41%] [G loss: 2.051130]\n",
      "epoch:17 step:16320 [D loss: 0.688796, acc: 57.81%] [G loss: 1.827718]\n",
      "epoch:17 step:16321 [D loss: 0.673675, acc: 58.59%] [G loss: 1.975298]\n",
      "epoch:17 step:16322 [D loss: 0.627822, acc: 64.06%] [G loss: 1.950120]\n",
      "epoch:17 step:16323 [D loss: 0.654642, acc: 61.72%] [G loss: 1.857844]\n",
      "epoch:17 step:16324 [D loss: 0.651533, acc: 60.16%] [G loss: 2.024820]\n",
      "epoch:17 step:16325 [D loss: 0.660583, acc: 61.72%] [G loss: 1.867088]\n",
      "epoch:17 step:16326 [D loss: 0.692883, acc: 57.81%] [G loss: 1.883763]\n",
      "epoch:17 step:16327 [D loss: 0.606049, acc: 67.97%] [G loss: 1.880573]\n",
      "epoch:17 step:16328 [D loss: 0.665902, acc: 59.38%] [G loss: 2.012938]\n",
      "epoch:17 step:16329 [D loss: 0.628321, acc: 65.62%] [G loss: 1.944976]\n",
      "epoch:17 step:16330 [D loss: 0.667242, acc: 62.50%] [G loss: 1.718069]\n",
      "epoch:17 step:16331 [D loss: 0.665306, acc: 60.94%] [G loss: 1.954778]\n",
      "epoch:17 step:16332 [D loss: 0.683174, acc: 57.03%] [G loss: 1.745112]\n",
      "epoch:17 step:16333 [D loss: 0.626370, acc: 69.53%] [G loss: 1.991262]\n",
      "epoch:17 step:16334 [D loss: 0.598840, acc: 65.62%] [G loss: 2.059491]\n",
      "epoch:17 step:16335 [D loss: 0.655475, acc: 63.28%] [G loss: 1.954189]\n",
      "epoch:17 step:16336 [D loss: 0.620855, acc: 63.28%] [G loss: 1.881874]\n",
      "epoch:17 step:16337 [D loss: 0.725660, acc: 57.03%] [G loss: 1.832984]\n",
      "epoch:17 step:16338 [D loss: 0.657067, acc: 62.50%] [G loss: 1.863776]\n",
      "epoch:17 step:16339 [D loss: 0.644300, acc: 62.50%] [G loss: 1.873072]\n",
      "epoch:17 step:16340 [D loss: 0.646748, acc: 60.94%] [G loss: 1.804523]\n",
      "epoch:17 step:16341 [D loss: 0.668643, acc: 59.38%] [G loss: 1.918235]\n",
      "epoch:17 step:16342 [D loss: 0.643702, acc: 65.62%] [G loss: 1.847748]\n",
      "epoch:17 step:16343 [D loss: 0.600839, acc: 68.75%] [G loss: 2.081740]\n",
      "epoch:17 step:16344 [D loss: 0.655849, acc: 60.16%] [G loss: 2.030664]\n",
      "epoch:17 step:16345 [D loss: 0.592253, acc: 67.97%] [G loss: 2.064245]\n",
      "epoch:17 step:16346 [D loss: 0.607632, acc: 70.31%] [G loss: 1.981313]\n",
      "epoch:17 step:16347 [D loss: 0.639362, acc: 60.16%] [G loss: 1.872432]\n",
      "epoch:17 step:16348 [D loss: 0.674132, acc: 67.19%] [G loss: 1.885305]\n",
      "epoch:17 step:16349 [D loss: 0.649351, acc: 64.06%] [G loss: 2.014249]\n",
      "epoch:17 step:16350 [D loss: 0.661895, acc: 54.69%] [G loss: 1.958452]\n",
      "epoch:17 step:16351 [D loss: 0.642359, acc: 61.72%] [G loss: 1.870211]\n",
      "epoch:17 step:16352 [D loss: 0.661905, acc: 59.38%] [G loss: 1.846079]\n",
      "epoch:17 step:16353 [D loss: 0.705621, acc: 56.25%] [G loss: 1.879634]\n",
      "epoch:17 step:16354 [D loss: 0.637186, acc: 65.62%] [G loss: 1.966506]\n",
      "epoch:17 step:16355 [D loss: 0.667010, acc: 60.94%] [G loss: 1.943822]\n",
      "epoch:17 step:16356 [D loss: 0.608129, acc: 68.75%] [G loss: 2.054679]\n",
      "epoch:17 step:16357 [D loss: 0.539607, acc: 77.34%] [G loss: 2.097564]\n",
      "epoch:17 step:16358 [D loss: 0.630563, acc: 66.41%] [G loss: 2.282776]\n",
      "epoch:17 step:16359 [D loss: 0.578576, acc: 66.41%] [G loss: 2.190345]\n",
      "epoch:17 step:16360 [D loss: 0.688151, acc: 60.16%] [G loss: 1.870800]\n",
      "epoch:17 step:16361 [D loss: 0.651550, acc: 63.28%] [G loss: 1.777879]\n",
      "epoch:17 step:16362 [D loss: 0.679050, acc: 55.47%] [G loss: 1.848395]\n",
      "epoch:17 step:16363 [D loss: 0.603768, acc: 67.97%] [G loss: 2.115069]\n",
      "epoch:17 step:16364 [D loss: 0.618183, acc: 65.62%] [G loss: 1.947961]\n",
      "epoch:17 step:16365 [D loss: 0.620390, acc: 66.41%] [G loss: 1.971565]\n",
      "epoch:17 step:16366 [D loss: 0.738200, acc: 51.56%] [G loss: 1.712747]\n",
      "epoch:17 step:16367 [D loss: 0.671855, acc: 59.38%] [G loss: 1.885776]\n",
      "epoch:17 step:16368 [D loss: 0.653882, acc: 60.16%] [G loss: 1.801093]\n",
      "epoch:17 step:16369 [D loss: 0.709793, acc: 56.25%] [G loss: 1.801953]\n",
      "epoch:17 step:16370 [D loss: 0.662257, acc: 59.38%] [G loss: 1.999641]\n",
      "epoch:17 step:16371 [D loss: 0.680658, acc: 56.25%] [G loss: 1.823051]\n",
      "epoch:17 step:16372 [D loss: 0.662206, acc: 57.03%] [G loss: 1.760329]\n",
      "epoch:17 step:16373 [D loss: 0.643858, acc: 63.28%] [G loss: 1.723903]\n",
      "epoch:17 step:16374 [D loss: 0.606094, acc: 68.75%] [G loss: 1.910974]\n",
      "epoch:17 step:16375 [D loss: 0.668661, acc: 60.94%] [G loss: 1.810511]\n",
      "epoch:17 step:16376 [D loss: 0.622559, acc: 65.62%] [G loss: 1.813920]\n",
      "epoch:17 step:16377 [D loss: 0.662897, acc: 59.38%] [G loss: 1.828395]\n",
      "epoch:17 step:16378 [D loss: 0.651136, acc: 66.41%] [G loss: 1.807458]\n",
      "epoch:17 step:16379 [D loss: 0.599361, acc: 67.19%] [G loss: 1.916628]\n",
      "epoch:17 step:16380 [D loss: 0.637811, acc: 62.50%] [G loss: 1.760225]\n",
      "epoch:17 step:16381 [D loss: 0.675153, acc: 54.69%] [G loss: 1.835998]\n",
      "epoch:17 step:16382 [D loss: 0.653403, acc: 63.28%] [G loss: 1.919437]\n",
      "epoch:17 step:16383 [D loss: 0.631515, acc: 62.50%] [G loss: 1.874327]\n",
      "epoch:17 step:16384 [D loss: 0.623973, acc: 62.50%] [G loss: 1.938434]\n",
      "epoch:17 step:16385 [D loss: 0.602476, acc: 73.44%] [G loss: 1.885373]\n",
      "epoch:17 step:16386 [D loss: 0.627935, acc: 61.72%] [G loss: 2.013929]\n",
      "epoch:17 step:16387 [D loss: 0.699107, acc: 50.78%] [G loss: 1.796579]\n",
      "epoch:17 step:16388 [D loss: 0.672726, acc: 58.59%] [G loss: 1.727761]\n",
      "epoch:17 step:16389 [D loss: 0.710014, acc: 53.91%] [G loss: 1.679011]\n",
      "epoch:17 step:16390 [D loss: 0.666830, acc: 56.25%] [G loss: 1.755613]\n",
      "epoch:17 step:16391 [D loss: 0.688150, acc: 52.34%] [G loss: 1.851310]\n",
      "epoch:17 step:16392 [D loss: 0.690301, acc: 57.81%] [G loss: 1.854092]\n",
      "epoch:17 step:16393 [D loss: 0.644559, acc: 63.28%] [G loss: 1.815596]\n",
      "epoch:17 step:16394 [D loss: 0.647563, acc: 62.50%] [G loss: 1.833169]\n",
      "epoch:17 step:16395 [D loss: 0.640581, acc: 67.97%] [G loss: 1.858134]\n",
      "epoch:17 step:16396 [D loss: 0.647654, acc: 66.41%] [G loss: 1.872848]\n",
      "epoch:17 step:16397 [D loss: 0.658355, acc: 56.25%] [G loss: 1.860082]\n",
      "epoch:17 step:16398 [D loss: 0.628035, acc: 67.97%] [G loss: 1.951242]\n",
      "epoch:17 step:16399 [D loss: 0.626169, acc: 65.62%] [G loss: 2.054455]\n",
      "epoch:17 step:16400 [D loss: 0.609169, acc: 71.09%] [G loss: 2.179516]\n",
      "##############\n",
      "[2.37714514 1.51661776 6.41612042 4.56824672 3.49141297 5.63778235\n",
      " 4.24289108 4.53477182 4.60067971 3.46093315]\n",
      "##########\n",
      "epoch:17 step:16401 [D loss: 0.632154, acc: 66.41%] [G loss: 2.127431]\n",
      "epoch:17 step:16402 [D loss: 0.723030, acc: 57.03%] [G loss: 1.847754]\n",
      "epoch:17 step:16403 [D loss: 0.618291, acc: 67.19%] [G loss: 1.870273]\n",
      "epoch:17 step:16404 [D loss: 0.653147, acc: 62.50%] [G loss: 1.943366]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:17 step:16405 [D loss: 0.637578, acc: 60.94%] [G loss: 1.976440]\n",
      "epoch:17 step:16406 [D loss: 0.661648, acc: 60.16%] [G loss: 1.890762]\n",
      "epoch:17 step:16407 [D loss: 0.657582, acc: 63.28%] [G loss: 1.806397]\n",
      "epoch:17 step:16408 [D loss: 0.581726, acc: 74.22%] [G loss: 2.140348]\n",
      "epoch:17 step:16409 [D loss: 0.614442, acc: 67.19%] [G loss: 1.832348]\n",
      "epoch:17 step:16410 [D loss: 0.596948, acc: 66.41%] [G loss: 2.078863]\n",
      "epoch:17 step:16411 [D loss: 0.672558, acc: 58.59%] [G loss: 1.833996]\n",
      "epoch:17 step:16412 [D loss: 0.711253, acc: 52.34%] [G loss: 1.768653]\n",
      "epoch:17 step:16413 [D loss: 0.672472, acc: 57.03%] [G loss: 1.888381]\n",
      "epoch:17 step:16414 [D loss: 0.660742, acc: 58.59%] [G loss: 1.996153]\n",
      "epoch:17 step:16415 [D loss: 0.683359, acc: 57.81%] [G loss: 1.859596]\n",
      "epoch:17 step:16416 [D loss: 0.635231, acc: 63.28%] [G loss: 1.859287]\n",
      "epoch:17 step:16417 [D loss: 0.618993, acc: 62.50%] [G loss: 2.015089]\n",
      "epoch:17 step:16418 [D loss: 0.643649, acc: 60.16%] [G loss: 1.893764]\n",
      "epoch:17 step:16419 [D loss: 0.625347, acc: 65.62%] [G loss: 1.974723]\n",
      "epoch:17 step:16420 [D loss: 0.647676, acc: 62.50%] [G loss: 1.962983]\n",
      "epoch:17 step:16421 [D loss: 0.627297, acc: 65.62%] [G loss: 1.780648]\n",
      "epoch:17 step:16422 [D loss: 0.652859, acc: 61.72%] [G loss: 1.855365]\n",
      "epoch:17 step:16423 [D loss: 0.627177, acc: 58.59%] [G loss: 2.066745]\n",
      "epoch:17 step:16424 [D loss: 0.633045, acc: 63.28%] [G loss: 2.109952]\n",
      "epoch:17 step:16425 [D loss: 0.623608, acc: 65.62%] [G loss: 1.955887]\n",
      "epoch:17 step:16426 [D loss: 0.677390, acc: 64.84%] [G loss: 1.935592]\n",
      "epoch:17 step:16427 [D loss: 0.633964, acc: 66.41%] [G loss: 2.171793]\n",
      "epoch:17 step:16428 [D loss: 0.590565, acc: 69.53%] [G loss: 1.945605]\n",
      "epoch:17 step:16429 [D loss: 0.691272, acc: 55.47%] [G loss: 1.827464]\n",
      "epoch:17 step:16430 [D loss: 0.681847, acc: 59.38%] [G loss: 1.837177]\n",
      "epoch:17 step:16431 [D loss: 0.682315, acc: 60.94%] [G loss: 1.786388]\n",
      "epoch:17 step:16432 [D loss: 0.669149, acc: 66.41%] [G loss: 1.954836]\n",
      "epoch:17 step:16433 [D loss: 0.635215, acc: 64.06%] [G loss: 2.208425]\n",
      "epoch:17 step:16434 [D loss: 0.626083, acc: 64.84%] [G loss: 1.901882]\n",
      "epoch:17 step:16435 [D loss: 0.700858, acc: 50.00%] [G loss: 1.765347]\n",
      "epoch:17 step:16436 [D loss: 0.643538, acc: 58.59%] [G loss: 1.810597]\n",
      "epoch:17 step:16437 [D loss: 0.605720, acc: 67.19%] [G loss: 2.098353]\n",
      "epoch:17 step:16438 [D loss: 0.662679, acc: 64.84%] [G loss: 1.851152]\n",
      "epoch:17 step:16439 [D loss: 0.650104, acc: 63.28%] [G loss: 1.857080]\n",
      "epoch:17 step:16440 [D loss: 0.742095, acc: 50.00%] [G loss: 1.651523]\n",
      "epoch:17 step:16441 [D loss: 0.645677, acc: 64.06%] [G loss: 1.818807]\n",
      "epoch:17 step:16442 [D loss: 0.605282, acc: 68.75%] [G loss: 1.875799]\n",
      "epoch:17 step:16443 [D loss: 0.634017, acc: 64.84%] [G loss: 1.839165]\n",
      "epoch:17 step:16444 [D loss: 0.625558, acc: 67.19%] [G loss: 1.893246]\n",
      "epoch:17 step:16445 [D loss: 0.618298, acc: 64.06%] [G loss: 1.903852]\n",
      "epoch:17 step:16446 [D loss: 0.611633, acc: 65.62%] [G loss: 2.103982]\n",
      "epoch:17 step:16447 [D loss: 0.616240, acc: 67.19%] [G loss: 1.813540]\n",
      "epoch:17 step:16448 [D loss: 0.651927, acc: 61.72%] [G loss: 1.913759]\n",
      "epoch:17 step:16449 [D loss: 0.592277, acc: 67.97%] [G loss: 1.955376]\n",
      "epoch:17 step:16450 [D loss: 0.632948, acc: 63.28%] [G loss: 2.095044]\n",
      "epoch:17 step:16451 [D loss: 0.625130, acc: 71.09%] [G loss: 2.081619]\n",
      "epoch:17 step:16452 [D loss: 0.639660, acc: 64.84%] [G loss: 2.102773]\n",
      "epoch:17 step:16453 [D loss: 0.728765, acc: 59.38%] [G loss: 1.792976]\n",
      "epoch:17 step:16454 [D loss: 0.697412, acc: 55.47%] [G loss: 1.710050]\n",
      "epoch:17 step:16455 [D loss: 0.631430, acc: 67.97%] [G loss: 1.748094]\n",
      "epoch:17 step:16456 [D loss: 0.705279, acc: 57.03%] [G loss: 1.828159]\n",
      "epoch:17 step:16457 [D loss: 0.741162, acc: 56.25%] [G loss: 1.722638]\n",
      "epoch:17 step:16458 [D loss: 0.682424, acc: 56.25%] [G loss: 1.626864]\n",
      "epoch:17 step:16459 [D loss: 0.614240, acc: 67.97%] [G loss: 1.703226]\n",
      "epoch:17 step:16460 [D loss: 0.682024, acc: 58.59%] [G loss: 1.674554]\n",
      "epoch:17 step:16461 [D loss: 0.628961, acc: 58.59%] [G loss: 1.960674]\n",
      "epoch:17 step:16462 [D loss: 0.617075, acc: 63.28%] [G loss: 1.886708]\n",
      "epoch:17 step:16463 [D loss: 0.590051, acc: 69.53%] [G loss: 1.933999]\n",
      "epoch:17 step:16464 [D loss: 0.650693, acc: 64.84%] [G loss: 1.872851]\n",
      "epoch:17 step:16465 [D loss: 0.616811, acc: 63.28%] [G loss: 2.016875]\n",
      "epoch:17 step:16466 [D loss: 0.671644, acc: 57.81%] [G loss: 1.875656]\n",
      "epoch:17 step:16467 [D loss: 0.622756, acc: 65.62%] [G loss: 1.829650]\n",
      "epoch:17 step:16468 [D loss: 0.671402, acc: 58.59%] [G loss: 1.841902]\n",
      "epoch:17 step:16469 [D loss: 0.675134, acc: 56.25%] [G loss: 1.816799]\n",
      "epoch:17 step:16470 [D loss: 0.687858, acc: 55.47%] [G loss: 1.773888]\n",
      "epoch:17 step:16471 [D loss: 0.664540, acc: 59.38%] [G loss: 1.764257]\n",
      "epoch:17 step:16472 [D loss: 0.643791, acc: 63.28%] [G loss: 1.796692]\n",
      "epoch:17 step:16473 [D loss: 0.639910, acc: 64.84%] [G loss: 1.898476]\n",
      "epoch:17 step:16474 [D loss: 0.648513, acc: 64.06%] [G loss: 1.888726]\n",
      "epoch:17 step:16475 [D loss: 0.609132, acc: 65.62%] [G loss: 1.915631]\n",
      "epoch:17 step:16476 [D loss: 0.636401, acc: 65.62%] [G loss: 2.015771]\n",
      "epoch:17 step:16477 [D loss: 0.675109, acc: 60.16%] [G loss: 2.018548]\n",
      "epoch:17 step:16478 [D loss: 0.617809, acc: 67.19%] [G loss: 1.908716]\n",
      "epoch:17 step:16479 [D loss: 0.637608, acc: 60.16%] [G loss: 2.031322]\n",
      "epoch:17 step:16480 [D loss: 0.642696, acc: 62.50%] [G loss: 2.102034]\n",
      "epoch:17 step:16481 [D loss: 0.612173, acc: 64.84%] [G loss: 1.986280]\n",
      "epoch:17 step:16482 [D loss: 0.659247, acc: 59.38%] [G loss: 1.858083]\n",
      "epoch:17 step:16483 [D loss: 0.585207, acc: 60.94%] [G loss: 2.064266]\n",
      "epoch:17 step:16484 [D loss: 0.620696, acc: 64.06%] [G loss: 1.952727]\n",
      "epoch:17 step:16485 [D loss: 0.582344, acc: 71.09%] [G loss: 2.178641]\n",
      "epoch:17 step:16486 [D loss: 0.626053, acc: 67.19%] [G loss: 2.274811]\n",
      "epoch:17 step:16487 [D loss: 0.648476, acc: 57.03%] [G loss: 2.149102]\n",
      "epoch:17 step:16488 [D loss: 0.666909, acc: 59.38%] [G loss: 1.849569]\n",
      "epoch:17 step:16489 [D loss: 0.646391, acc: 60.16%] [G loss: 1.896294]\n",
      "epoch:17 step:16490 [D loss: 0.641791, acc: 59.38%] [G loss: 1.918910]\n",
      "epoch:17 step:16491 [D loss: 0.699552, acc: 57.03%] [G loss: 1.783186]\n",
      "epoch:17 step:16492 [D loss: 0.683432, acc: 60.94%] [G loss: 1.896609]\n",
      "epoch:17 step:16493 [D loss: 0.597286, acc: 69.53%] [G loss: 2.051107]\n",
      "epoch:17 step:16494 [D loss: 0.661336, acc: 62.50%] [G loss: 1.892690]\n",
      "epoch:17 step:16495 [D loss: 0.759723, acc: 48.44%] [G loss: 1.670575]\n",
      "epoch:17 step:16496 [D loss: 0.664264, acc: 59.38%] [G loss: 1.682288]\n",
      "epoch:17 step:16497 [D loss: 0.688086, acc: 60.94%] [G loss: 1.934050]\n",
      "epoch:17 step:16498 [D loss: 0.643654, acc: 65.62%] [G loss: 1.781021]\n",
      "epoch:17 step:16499 [D loss: 0.620066, acc: 66.41%] [G loss: 1.814862]\n",
      "epoch:17 step:16500 [D loss: 0.617484, acc: 66.41%] [G loss: 1.908852]\n",
      "epoch:17 step:16501 [D loss: 0.683476, acc: 58.59%] [G loss: 1.819651]\n",
      "epoch:17 step:16502 [D loss: 0.650908, acc: 61.72%] [G loss: 1.821288]\n",
      "epoch:17 step:16503 [D loss: 0.614916, acc: 67.97%] [G loss: 1.858987]\n",
      "epoch:17 step:16504 [D loss: 0.629384, acc: 64.84%] [G loss: 1.908401]\n",
      "epoch:17 step:16505 [D loss: 0.684079, acc: 58.59%] [G loss: 1.890446]\n",
      "epoch:17 step:16506 [D loss: 0.714149, acc: 53.91%] [G loss: 1.650707]\n",
      "epoch:17 step:16507 [D loss: 0.661348, acc: 64.06%] [G loss: 1.827113]\n",
      "epoch:17 step:16508 [D loss: 0.673620, acc: 57.81%] [G loss: 1.699836]\n",
      "epoch:17 step:16509 [D loss: 0.708481, acc: 53.91%] [G loss: 1.804096]\n",
      "epoch:17 step:16510 [D loss: 0.646230, acc: 63.28%] [G loss: 1.725671]\n",
      "epoch:17 step:16511 [D loss: 0.629309, acc: 68.75%] [G loss: 1.838441]\n",
      "epoch:17 step:16512 [D loss: 0.645287, acc: 63.28%] [G loss: 1.783694]\n",
      "epoch:17 step:16513 [D loss: 0.642381, acc: 63.28%] [G loss: 1.791535]\n",
      "epoch:17 step:16514 [D loss: 0.658251, acc: 60.16%] [G loss: 1.940969]\n",
      "epoch:17 step:16515 [D loss: 0.669523, acc: 60.16%] [G loss: 1.800876]\n",
      "epoch:17 step:16516 [D loss: 0.667045, acc: 57.03%] [G loss: 1.685148]\n",
      "epoch:17 step:16517 [D loss: 0.667977, acc: 61.72%] [G loss: 1.909912]\n",
      "epoch:17 step:16518 [D loss: 0.665812, acc: 59.38%] [G loss: 1.890129]\n",
      "epoch:17 step:16519 [D loss: 0.635855, acc: 64.06%] [G loss: 1.793972]\n",
      "epoch:17 step:16520 [D loss: 0.644806, acc: 59.38%] [G loss: 1.837228]\n",
      "epoch:17 step:16521 [D loss: 0.650846, acc: 60.16%] [G loss: 1.945642]\n",
      "epoch:17 step:16522 [D loss: 0.628248, acc: 63.28%] [G loss: 1.793423]\n",
      "epoch:17 step:16523 [D loss: 0.654918, acc: 57.03%] [G loss: 1.883557]\n",
      "epoch:17 step:16524 [D loss: 0.615074, acc: 67.19%] [G loss: 1.820305]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:17 step:16525 [D loss: 0.669051, acc: 61.72%] [G loss: 1.891376]\n",
      "epoch:17 step:16526 [D loss: 0.658263, acc: 56.25%] [G loss: 1.771055]\n",
      "epoch:17 step:16527 [D loss: 0.649296, acc: 66.41%] [G loss: 2.027473]\n",
      "epoch:17 step:16528 [D loss: 0.673025, acc: 61.72%] [G loss: 1.794770]\n",
      "epoch:17 step:16529 [D loss: 0.610095, acc: 61.72%] [G loss: 1.838315]\n",
      "epoch:17 step:16530 [D loss: 0.639146, acc: 60.94%] [G loss: 1.912460]\n",
      "epoch:17 step:16531 [D loss: 0.651885, acc: 60.16%] [G loss: 1.976552]\n",
      "epoch:17 step:16532 [D loss: 0.626059, acc: 66.41%] [G loss: 2.160648]\n",
      "epoch:17 step:16533 [D loss: 0.655082, acc: 60.94%] [G loss: 1.887352]\n",
      "epoch:17 step:16534 [D loss: 0.595071, acc: 70.31%] [G loss: 1.872541]\n",
      "epoch:17 step:16535 [D loss: 0.688200, acc: 57.03%] [G loss: 1.881397]\n",
      "epoch:17 step:16536 [D loss: 0.607228, acc: 68.75%] [G loss: 1.831311]\n",
      "epoch:17 step:16537 [D loss: 0.664359, acc: 55.47%] [G loss: 1.912320]\n",
      "epoch:17 step:16538 [D loss: 0.633521, acc: 65.62%] [G loss: 1.798237]\n",
      "epoch:17 step:16539 [D loss: 0.609350, acc: 71.09%] [G loss: 1.915267]\n",
      "epoch:17 step:16540 [D loss: 0.638045, acc: 67.19%] [G loss: 1.861478]\n",
      "epoch:17 step:16541 [D loss: 0.677342, acc: 60.16%] [G loss: 1.772184]\n",
      "epoch:17 step:16542 [D loss: 0.636285, acc: 64.06%] [G loss: 1.866431]\n",
      "epoch:17 step:16543 [D loss: 0.680552, acc: 55.47%] [G loss: 1.794950]\n",
      "epoch:17 step:16544 [D loss: 0.731963, acc: 51.56%] [G loss: 1.739446]\n",
      "epoch:17 step:16545 [D loss: 0.666207, acc: 60.94%] [G loss: 1.773442]\n",
      "epoch:17 step:16546 [D loss: 0.691394, acc: 58.59%] [G loss: 1.818880]\n",
      "epoch:17 step:16547 [D loss: 0.642775, acc: 66.41%] [G loss: 1.821679]\n",
      "epoch:17 step:16548 [D loss: 0.643212, acc: 66.41%] [G loss: 1.957045]\n",
      "epoch:17 step:16549 [D loss: 0.707987, acc: 55.47%] [G loss: 1.810579]\n",
      "epoch:17 step:16550 [D loss: 0.632457, acc: 65.62%] [G loss: 1.890563]\n",
      "epoch:17 step:16551 [D loss: 0.621330, acc: 62.50%] [G loss: 1.792242]\n",
      "epoch:17 step:16552 [D loss: 0.600567, acc: 69.53%] [G loss: 1.931830]\n",
      "epoch:17 step:16553 [D loss: 0.601361, acc: 70.31%] [G loss: 2.126684]\n",
      "epoch:17 step:16554 [D loss: 0.649043, acc: 63.28%] [G loss: 1.698098]\n",
      "epoch:17 step:16555 [D loss: 0.659692, acc: 60.16%] [G loss: 1.898024]\n",
      "epoch:17 step:16556 [D loss: 0.642861, acc: 67.19%] [G loss: 1.831927]\n",
      "epoch:17 step:16557 [D loss: 0.649584, acc: 63.28%] [G loss: 1.759317]\n",
      "epoch:17 step:16558 [D loss: 0.646909, acc: 60.16%] [G loss: 1.972455]\n",
      "epoch:17 step:16559 [D loss: 0.654894, acc: 68.75%] [G loss: 1.954343]\n",
      "epoch:17 step:16560 [D loss: 0.618279, acc: 70.31%] [G loss: 1.984231]\n",
      "epoch:17 step:16561 [D loss: 0.681108, acc: 58.59%] [G loss: 1.893912]\n",
      "epoch:17 step:16562 [D loss: 0.663093, acc: 57.03%] [G loss: 1.959025]\n",
      "epoch:17 step:16563 [D loss: 0.641399, acc: 66.41%] [G loss: 1.869589]\n",
      "epoch:17 step:16564 [D loss: 0.620585, acc: 64.06%] [G loss: 2.013851]\n",
      "epoch:17 step:16565 [D loss: 0.680616, acc: 56.25%] [G loss: 1.788099]\n",
      "epoch:17 step:16566 [D loss: 0.678110, acc: 60.16%] [G loss: 1.930063]\n",
      "epoch:17 step:16567 [D loss: 0.623366, acc: 69.53%] [G loss: 1.946723]\n",
      "epoch:17 step:16568 [D loss: 0.605449, acc: 65.62%] [G loss: 1.904024]\n",
      "epoch:17 step:16569 [D loss: 0.656043, acc: 59.38%] [G loss: 1.939755]\n",
      "epoch:17 step:16570 [D loss: 0.614666, acc: 69.53%] [G loss: 1.938583]\n",
      "epoch:17 step:16571 [D loss: 0.634586, acc: 62.50%] [G loss: 1.844179]\n",
      "epoch:17 step:16572 [D loss: 0.677918, acc: 57.03%] [G loss: 1.807412]\n",
      "epoch:17 step:16573 [D loss: 0.628815, acc: 62.50%] [G loss: 1.881780]\n",
      "epoch:17 step:16574 [D loss: 0.667135, acc: 62.50%] [G loss: 1.852859]\n",
      "epoch:17 step:16575 [D loss: 0.621618, acc: 62.50%] [G loss: 1.869449]\n",
      "epoch:17 step:16576 [D loss: 0.648239, acc: 61.72%] [G loss: 2.019246]\n",
      "epoch:17 step:16577 [D loss: 0.543658, acc: 75.00%] [G loss: 2.444155]\n",
      "epoch:17 step:16578 [D loss: 0.615847, acc: 64.06%] [G loss: 2.052781]\n",
      "epoch:17 step:16579 [D loss: 0.607827, acc: 62.50%] [G loss: 2.139618]\n",
      "epoch:17 step:16580 [D loss: 0.629183, acc: 64.06%] [G loss: 1.954887]\n",
      "epoch:17 step:16581 [D loss: 0.643450, acc: 60.94%] [G loss: 1.981038]\n",
      "epoch:17 step:16582 [D loss: 0.670394, acc: 60.16%] [G loss: 1.928105]\n",
      "epoch:17 step:16583 [D loss: 0.632876, acc: 59.38%] [G loss: 2.076544]\n",
      "epoch:17 step:16584 [D loss: 0.756360, acc: 50.78%] [G loss: 1.880018]\n",
      "epoch:17 step:16585 [D loss: 0.697520, acc: 58.59%] [G loss: 1.800072]\n",
      "epoch:17 step:16586 [D loss: 0.717084, acc: 56.25%] [G loss: 1.777842]\n",
      "epoch:17 step:16587 [D loss: 0.672991, acc: 57.81%] [G loss: 1.675234]\n",
      "epoch:17 step:16588 [D loss: 0.740492, acc: 50.78%] [G loss: 1.772399]\n",
      "epoch:17 step:16589 [D loss: 0.660541, acc: 62.50%] [G loss: 1.734621]\n",
      "epoch:17 step:16590 [D loss: 0.684493, acc: 60.16%] [G loss: 1.849344]\n",
      "epoch:17 step:16591 [D loss: 0.646323, acc: 64.06%] [G loss: 1.716381]\n",
      "epoch:17 step:16592 [D loss: 0.637739, acc: 61.72%] [G loss: 1.849718]\n",
      "epoch:17 step:16593 [D loss: 0.659864, acc: 54.69%] [G loss: 1.829342]\n",
      "epoch:17 step:16594 [D loss: 0.594628, acc: 67.19%] [G loss: 1.814395]\n",
      "epoch:17 step:16595 [D loss: 0.659350, acc: 59.38%] [G loss: 1.788302]\n",
      "epoch:17 step:16596 [D loss: 0.660974, acc: 60.16%] [G loss: 1.684149]\n",
      "epoch:17 step:16597 [D loss: 0.645554, acc: 64.06%] [G loss: 1.820002]\n",
      "epoch:17 step:16598 [D loss: 0.665557, acc: 57.81%] [G loss: 1.800888]\n",
      "epoch:17 step:16599 [D loss: 0.644422, acc: 60.16%] [G loss: 1.785517]\n",
      "epoch:17 step:16600 [D loss: 0.657543, acc: 53.91%] [G loss: 1.902067]\n",
      "##############\n",
      "[2.49690261 1.41919563 6.20360466 4.9718229  3.76583564 5.54534421\n",
      " 4.40261297 4.66816745 4.73441664 3.69094136]\n",
      "##########\n",
      "epoch:17 step:16601 [D loss: 0.637454, acc: 59.38%] [G loss: 1.857439]\n",
      "epoch:17 step:16602 [D loss: 0.649549, acc: 63.28%] [G loss: 2.018546]\n",
      "epoch:17 step:16603 [D loss: 0.587718, acc: 72.66%] [G loss: 1.980142]\n",
      "epoch:17 step:16604 [D loss: 0.695280, acc: 55.47%] [G loss: 1.770774]\n",
      "epoch:17 step:16605 [D loss: 0.617369, acc: 63.28%] [G loss: 1.895237]\n",
      "epoch:17 step:16606 [D loss: 0.601976, acc: 66.41%] [G loss: 2.007000]\n",
      "epoch:17 step:16607 [D loss: 0.624707, acc: 62.50%] [G loss: 1.967713]\n",
      "epoch:17 step:16608 [D loss: 0.636147, acc: 60.16%] [G loss: 2.046209]\n",
      "epoch:17 step:16609 [D loss: 0.637244, acc: 58.59%] [G loss: 2.102674]\n",
      "epoch:17 step:16610 [D loss: 0.635124, acc: 59.38%] [G loss: 1.917602]\n",
      "epoch:17 step:16611 [D loss: 0.648504, acc: 64.84%] [G loss: 1.941555]\n",
      "epoch:17 step:16612 [D loss: 0.677791, acc: 56.25%] [G loss: 1.802180]\n",
      "epoch:17 step:16613 [D loss: 0.670385, acc: 60.16%] [G loss: 1.855119]\n",
      "epoch:17 step:16614 [D loss: 0.638440, acc: 59.38%] [G loss: 1.840886]\n",
      "epoch:17 step:16615 [D loss: 0.659574, acc: 61.72%] [G loss: 1.917187]\n",
      "epoch:17 step:16616 [D loss: 0.633249, acc: 59.38%] [G loss: 1.841792]\n",
      "epoch:17 step:16617 [D loss: 0.640564, acc: 59.38%] [G loss: 1.891612]\n",
      "epoch:17 step:16618 [D loss: 0.640954, acc: 64.06%] [G loss: 1.873993]\n",
      "epoch:17 step:16619 [D loss: 0.594766, acc: 71.88%] [G loss: 2.000519]\n",
      "epoch:17 step:16620 [D loss: 0.632282, acc: 64.84%] [G loss: 2.012389]\n",
      "epoch:17 step:16621 [D loss: 0.670583, acc: 57.03%] [G loss: 1.887173]\n",
      "epoch:17 step:16622 [D loss: 0.601323, acc: 64.06%] [G loss: 2.034647]\n",
      "epoch:17 step:16623 [D loss: 0.599864, acc: 64.06%] [G loss: 2.182832]\n",
      "epoch:17 step:16624 [D loss: 0.636664, acc: 64.06%] [G loss: 1.937362]\n",
      "epoch:17 step:16625 [D loss: 0.694587, acc: 56.25%] [G loss: 1.964128]\n",
      "epoch:17 step:16626 [D loss: 0.632454, acc: 59.38%] [G loss: 1.917727]\n",
      "epoch:17 step:16627 [D loss: 0.666037, acc: 53.91%] [G loss: 1.859319]\n",
      "epoch:17 step:16628 [D loss: 0.608727, acc: 61.72%] [G loss: 2.067990]\n",
      "epoch:17 step:16629 [D loss: 0.663441, acc: 57.03%] [G loss: 1.880290]\n",
      "epoch:17 step:16630 [D loss: 0.651158, acc: 64.06%] [G loss: 1.918169]\n",
      "epoch:17 step:16631 [D loss: 0.663585, acc: 57.81%] [G loss: 1.769620]\n",
      "epoch:17 step:16632 [D loss: 0.683126, acc: 55.47%] [G loss: 1.792520]\n",
      "epoch:17 step:16633 [D loss: 0.675278, acc: 54.69%] [G loss: 1.803250]\n",
      "epoch:17 step:16634 [D loss: 0.644336, acc: 63.28%] [G loss: 1.716855]\n",
      "epoch:17 step:16635 [D loss: 0.617632, acc: 66.41%] [G loss: 1.900011]\n",
      "epoch:17 step:16636 [D loss: 0.665569, acc: 54.69%] [G loss: 1.927569]\n",
      "epoch:17 step:16637 [D loss: 0.649816, acc: 58.59%] [G loss: 2.089528]\n",
      "epoch:17 step:16638 [D loss: 0.572650, acc: 73.44%] [G loss: 2.059081]\n",
      "epoch:17 step:16639 [D loss: 0.649162, acc: 65.62%] [G loss: 2.074408]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:17 step:16640 [D loss: 0.653422, acc: 63.28%] [G loss: 1.964274]\n",
      "epoch:17 step:16641 [D loss: 0.621606, acc: 71.09%] [G loss: 1.978590]\n",
      "epoch:17 step:16642 [D loss: 0.620691, acc: 65.62%] [G loss: 1.842188]\n",
      "epoch:17 step:16643 [D loss: 0.633001, acc: 64.84%] [G loss: 2.024169]\n",
      "epoch:17 step:16644 [D loss: 0.621010, acc: 62.50%] [G loss: 1.886018]\n",
      "epoch:17 step:16645 [D loss: 0.671140, acc: 59.38%] [G loss: 1.729782]\n",
      "epoch:17 step:16646 [D loss: 0.696306, acc: 55.47%] [G loss: 1.822400]\n",
      "epoch:17 step:16647 [D loss: 0.686019, acc: 57.03%] [G loss: 1.986456]\n",
      "epoch:17 step:16648 [D loss: 0.607706, acc: 71.09%] [G loss: 1.921288]\n",
      "epoch:17 step:16649 [D loss: 0.634089, acc: 61.72%] [G loss: 1.972389]\n",
      "epoch:17 step:16650 [D loss: 0.621008, acc: 71.09%] [G loss: 1.889656]\n",
      "epoch:17 step:16651 [D loss: 0.685692, acc: 49.22%] [G loss: 1.814605]\n",
      "epoch:17 step:16652 [D loss: 0.656186, acc: 60.16%] [G loss: 1.931914]\n",
      "epoch:17 step:16653 [D loss: 0.697771, acc: 56.25%] [G loss: 1.912396]\n",
      "epoch:17 step:16654 [D loss: 0.697901, acc: 56.25%] [G loss: 1.914217]\n",
      "epoch:17 step:16655 [D loss: 0.631297, acc: 62.50%] [G loss: 2.010151]\n",
      "epoch:17 step:16656 [D loss: 0.635809, acc: 62.50%] [G loss: 1.828790]\n",
      "epoch:17 step:16657 [D loss: 0.657842, acc: 63.28%] [G loss: 1.929459]\n",
      "epoch:17 step:16658 [D loss: 0.621772, acc: 69.53%] [G loss: 1.791005]\n",
      "epoch:17 step:16659 [D loss: 0.657116, acc: 62.50%] [G loss: 1.983832]\n",
      "epoch:17 step:16660 [D loss: 0.670586, acc: 57.03%] [G loss: 1.895487]\n",
      "epoch:17 step:16661 [D loss: 0.675960, acc: 60.94%] [G loss: 1.877090]\n",
      "epoch:17 step:16662 [D loss: 0.629351, acc: 65.62%] [G loss: 1.896465]\n",
      "epoch:17 step:16663 [D loss: 0.657417, acc: 57.81%] [G loss: 1.837686]\n",
      "epoch:17 step:16664 [D loss: 0.656301, acc: 55.47%] [G loss: 1.927262]\n",
      "epoch:17 step:16665 [D loss: 0.623561, acc: 65.62%] [G loss: 1.880020]\n",
      "epoch:17 step:16666 [D loss: 0.625857, acc: 64.06%] [G loss: 1.946795]\n",
      "epoch:17 step:16667 [D loss: 0.612637, acc: 62.50%] [G loss: 1.759519]\n",
      "epoch:17 step:16668 [D loss: 0.658255, acc: 61.72%] [G loss: 1.832097]\n",
      "epoch:17 step:16669 [D loss: 0.636962, acc: 67.19%] [G loss: 1.894551]\n",
      "epoch:17 step:16670 [D loss: 0.654114, acc: 67.19%] [G loss: 1.856130]\n",
      "epoch:17 step:16671 [D loss: 0.657530, acc: 65.62%] [G loss: 1.850176]\n",
      "epoch:17 step:16672 [D loss: 0.642874, acc: 63.28%] [G loss: 1.901399]\n",
      "epoch:17 step:16673 [D loss: 0.624757, acc: 61.72%] [G loss: 1.881806]\n",
      "epoch:17 step:16674 [D loss: 0.655284, acc: 57.03%] [G loss: 1.918204]\n",
      "epoch:17 step:16675 [D loss: 0.624900, acc: 67.19%] [G loss: 1.906434]\n",
      "epoch:17 step:16676 [D loss: 0.625122, acc: 62.50%] [G loss: 1.997686]\n",
      "epoch:17 step:16677 [D loss: 0.595903, acc: 71.09%] [G loss: 2.005818]\n",
      "epoch:17 step:16678 [D loss: 0.669970, acc: 58.59%] [G loss: 1.938124]\n",
      "epoch:17 step:16679 [D loss: 0.606185, acc: 64.06%] [G loss: 2.026142]\n",
      "epoch:17 step:16680 [D loss: 0.636694, acc: 62.50%] [G loss: 1.919184]\n",
      "epoch:17 step:16681 [D loss: 0.654625, acc: 60.16%] [G loss: 1.819946]\n",
      "epoch:17 step:16682 [D loss: 0.661842, acc: 56.25%] [G loss: 1.878643]\n",
      "epoch:17 step:16683 [D loss: 0.623459, acc: 68.75%] [G loss: 1.923848]\n",
      "epoch:17 step:16684 [D loss: 0.705085, acc: 55.47%] [G loss: 1.933927]\n",
      "epoch:17 step:16685 [D loss: 0.648723, acc: 63.28%] [G loss: 1.951423]\n",
      "epoch:17 step:16686 [D loss: 0.659266, acc: 60.16%] [G loss: 1.963331]\n",
      "epoch:17 step:16687 [D loss: 0.699030, acc: 59.38%] [G loss: 1.932580]\n",
      "epoch:17 step:16688 [D loss: 0.669159, acc: 60.16%] [G loss: 1.879957]\n",
      "epoch:17 step:16689 [D loss: 0.628255, acc: 64.06%] [G loss: 1.848681]\n",
      "epoch:17 step:16690 [D loss: 0.685287, acc: 57.81%] [G loss: 1.864253]\n",
      "epoch:17 step:16691 [D loss: 0.687582, acc: 57.81%] [G loss: 1.822444]\n",
      "epoch:17 step:16692 [D loss: 0.603452, acc: 68.75%] [G loss: 1.863753]\n",
      "epoch:17 step:16693 [D loss: 0.622207, acc: 64.84%] [G loss: 1.776509]\n",
      "epoch:17 step:16694 [D loss: 0.653660, acc: 60.16%] [G loss: 1.827625]\n",
      "epoch:17 step:16695 [D loss: 0.662970, acc: 63.28%] [G loss: 1.821225]\n",
      "epoch:17 step:16696 [D loss: 0.666402, acc: 62.50%] [G loss: 1.839309]\n",
      "epoch:17 step:16697 [D loss: 0.666287, acc: 63.28%] [G loss: 1.876568]\n",
      "epoch:17 step:16698 [D loss: 0.616076, acc: 61.72%] [G loss: 1.943274]\n",
      "epoch:17 step:16699 [D loss: 0.628851, acc: 66.41%] [G loss: 1.994902]\n",
      "epoch:17 step:16700 [D loss: 0.668930, acc: 63.28%] [G loss: 1.866045]\n",
      "epoch:17 step:16701 [D loss: 0.669332, acc: 55.47%] [G loss: 1.860975]\n",
      "epoch:17 step:16702 [D loss: 0.628201, acc: 67.97%] [G loss: 1.847162]\n",
      "epoch:17 step:16703 [D loss: 0.627765, acc: 64.84%] [G loss: 2.167582]\n",
      "epoch:17 step:16704 [D loss: 0.639760, acc: 67.97%] [G loss: 1.992501]\n",
      "epoch:17 step:16705 [D loss: 0.622425, acc: 64.84%] [G loss: 1.889281]\n",
      "epoch:17 step:16706 [D loss: 0.647587, acc: 63.28%] [G loss: 1.916579]\n",
      "epoch:17 step:16707 [D loss: 0.668075, acc: 63.28%] [G loss: 1.832940]\n",
      "epoch:17 step:16708 [D loss: 0.656017, acc: 58.59%] [G loss: 1.788521]\n",
      "epoch:17 step:16709 [D loss: 0.619858, acc: 66.41%] [G loss: 2.126465]\n",
      "epoch:17 step:16710 [D loss: 0.569653, acc: 71.09%] [G loss: 2.335080]\n",
      "epoch:17 step:16711 [D loss: 0.629737, acc: 67.19%] [G loss: 2.109038]\n",
      "epoch:17 step:16712 [D loss: 0.635364, acc: 64.84%] [G loss: 1.939274]\n",
      "epoch:17 step:16713 [D loss: 0.679679, acc: 56.25%] [G loss: 1.702162]\n",
      "epoch:17 step:16714 [D loss: 0.624037, acc: 61.72%] [G loss: 1.849255]\n",
      "epoch:17 step:16715 [D loss: 0.649429, acc: 64.06%] [G loss: 2.002212]\n",
      "epoch:17 step:16716 [D loss: 0.703900, acc: 57.03%] [G loss: 1.987900]\n",
      "epoch:17 step:16717 [D loss: 0.667941, acc: 60.16%] [G loss: 1.823937]\n",
      "epoch:17 step:16718 [D loss: 0.676053, acc: 59.38%] [G loss: 1.874628]\n",
      "epoch:17 step:16719 [D loss: 0.606179, acc: 64.06%] [G loss: 2.026247]\n",
      "epoch:17 step:16720 [D loss: 0.722603, acc: 52.34%] [G loss: 1.933060]\n",
      "epoch:17 step:16721 [D loss: 0.595044, acc: 66.41%] [G loss: 1.857484]\n",
      "epoch:17 step:16722 [D loss: 0.657239, acc: 61.72%] [G loss: 1.927488]\n",
      "epoch:17 step:16723 [D loss: 0.683223, acc: 57.03%] [G loss: 1.929208]\n",
      "epoch:17 step:16724 [D loss: 0.718323, acc: 54.69%] [G loss: 1.779968]\n",
      "epoch:17 step:16725 [D loss: 0.665367, acc: 56.25%] [G loss: 1.866648]\n",
      "epoch:17 step:16726 [D loss: 0.686745, acc: 53.91%] [G loss: 1.763574]\n",
      "epoch:17 step:16727 [D loss: 0.686625, acc: 55.47%] [G loss: 1.787998]\n",
      "epoch:17 step:16728 [D loss: 0.660227, acc: 63.28%] [G loss: 1.842467]\n",
      "epoch:17 step:16729 [D loss: 0.693385, acc: 57.81%] [G loss: 1.758285]\n",
      "epoch:17 step:16730 [D loss: 0.669999, acc: 62.50%] [G loss: 1.913425]\n",
      "epoch:17 step:16731 [D loss: 0.637800, acc: 64.84%] [G loss: 1.793245]\n",
      "epoch:17 step:16732 [D loss: 0.638291, acc: 60.94%] [G loss: 1.983154]\n",
      "epoch:17 step:16733 [D loss: 0.638670, acc: 62.50%] [G loss: 1.842313]\n",
      "epoch:17 step:16734 [D loss: 0.641402, acc: 60.16%] [G loss: 1.927222]\n",
      "epoch:17 step:16735 [D loss: 0.564194, acc: 72.66%] [G loss: 1.862025]\n",
      "epoch:17 step:16736 [D loss: 0.618473, acc: 67.19%] [G loss: 1.892137]\n",
      "epoch:17 step:16737 [D loss: 0.647672, acc: 61.72%] [G loss: 1.955085]\n",
      "epoch:17 step:16738 [D loss: 0.707062, acc: 50.00%] [G loss: 1.833225]\n",
      "epoch:17 step:16739 [D loss: 0.644028, acc: 62.50%] [G loss: 1.821144]\n",
      "epoch:17 step:16740 [D loss: 0.625456, acc: 65.62%] [G loss: 1.893510]\n",
      "epoch:17 step:16741 [D loss: 0.676919, acc: 64.06%] [G loss: 1.708959]\n",
      "epoch:17 step:16742 [D loss: 0.641458, acc: 67.97%] [G loss: 1.962195]\n",
      "epoch:17 step:16743 [D loss: 0.691038, acc: 57.03%] [G loss: 1.890298]\n",
      "epoch:17 step:16744 [D loss: 0.614105, acc: 67.19%] [G loss: 2.149648]\n",
      "epoch:17 step:16745 [D loss: 0.614212, acc: 67.19%] [G loss: 1.982560]\n",
      "epoch:17 step:16746 [D loss: 0.685821, acc: 59.38%] [G loss: 1.846188]\n",
      "epoch:17 step:16747 [D loss: 0.647038, acc: 61.72%] [G loss: 1.781835]\n",
      "epoch:17 step:16748 [D loss: 0.685082, acc: 59.38%] [G loss: 1.867498]\n",
      "epoch:17 step:16749 [D loss: 0.676320, acc: 57.81%] [G loss: 1.644408]\n",
      "epoch:17 step:16750 [D loss: 0.645834, acc: 64.06%] [G loss: 1.853537]\n",
      "epoch:17 step:16751 [D loss: 0.602540, acc: 70.31%] [G loss: 1.936807]\n",
      "epoch:17 step:16752 [D loss: 0.609403, acc: 67.19%] [G loss: 1.945528]\n",
      "epoch:17 step:16753 [D loss: 0.629857, acc: 60.16%] [G loss: 1.902984]\n",
      "epoch:17 step:16754 [D loss: 0.596772, acc: 67.19%] [G loss: 2.096067]\n",
      "epoch:17 step:16755 [D loss: 0.655704, acc: 57.81%] [G loss: 1.791651]\n",
      "epoch:17 step:16756 [D loss: 0.657960, acc: 58.59%] [G loss: 1.755683]\n",
      "epoch:17 step:16757 [D loss: 0.680377, acc: 61.72%] [G loss: 1.837829]\n",
      "epoch:17 step:16758 [D loss: 0.668938, acc: 58.59%] [G loss: 1.837805]\n",
      "epoch:17 step:16759 [D loss: 0.649308, acc: 65.62%] [G loss: 1.738241]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:17 step:16760 [D loss: 0.638215, acc: 60.16%] [G loss: 1.860864]\n",
      "epoch:17 step:16761 [D loss: 0.646161, acc: 64.84%] [G loss: 2.004211]\n",
      "epoch:17 step:16762 [D loss: 0.658336, acc: 62.50%] [G loss: 1.923235]\n",
      "epoch:17 step:16763 [D loss: 0.628309, acc: 61.72%] [G loss: 1.825239]\n",
      "epoch:17 step:16764 [D loss: 0.671799, acc: 64.84%] [G loss: 1.806049]\n",
      "epoch:17 step:16765 [D loss: 0.668900, acc: 58.59%] [G loss: 1.900355]\n",
      "epoch:17 step:16766 [D loss: 0.614332, acc: 67.97%] [G loss: 1.814622]\n",
      "epoch:17 step:16767 [D loss: 0.618965, acc: 67.19%] [G loss: 1.869184]\n",
      "epoch:17 step:16768 [D loss: 0.684301, acc: 56.25%] [G loss: 1.835908]\n",
      "epoch:17 step:16769 [D loss: 0.671850, acc: 58.59%] [G loss: 1.823443]\n",
      "epoch:17 step:16770 [D loss: 0.586530, acc: 68.75%] [G loss: 1.925550]\n",
      "epoch:17 step:16771 [D loss: 0.639953, acc: 63.28%] [G loss: 2.050101]\n",
      "epoch:17 step:16772 [D loss: 0.703400, acc: 56.25%] [G loss: 1.867557]\n",
      "epoch:17 step:16773 [D loss: 0.671324, acc: 57.81%] [G loss: 2.027590]\n",
      "epoch:17 step:16774 [D loss: 0.660444, acc: 59.38%] [G loss: 1.779207]\n",
      "epoch:17 step:16775 [D loss: 0.627292, acc: 64.84%] [G loss: 1.695967]\n",
      "epoch:17 step:16776 [D loss: 0.628395, acc: 58.59%] [G loss: 1.945877]\n",
      "epoch:17 step:16777 [D loss: 0.620207, acc: 67.97%] [G loss: 1.922456]\n",
      "epoch:17 step:16778 [D loss: 0.668287, acc: 61.72%] [G loss: 1.929559]\n",
      "epoch:17 step:16779 [D loss: 0.689508, acc: 53.12%] [G loss: 1.849249]\n",
      "epoch:17 step:16780 [D loss: 0.620086, acc: 64.06%] [G loss: 1.803014]\n",
      "epoch:17 step:16781 [D loss: 0.649547, acc: 63.28%] [G loss: 1.978687]\n",
      "epoch:17 step:16782 [D loss: 0.645853, acc: 59.38%] [G loss: 1.924240]\n",
      "epoch:17 step:16783 [D loss: 0.623914, acc: 64.84%] [G loss: 1.829701]\n",
      "epoch:17 step:16784 [D loss: 0.695461, acc: 54.69%] [G loss: 1.803054]\n",
      "epoch:17 step:16785 [D loss: 0.626483, acc: 62.50%] [G loss: 1.737659]\n",
      "epoch:17 step:16786 [D loss: 0.667365, acc: 59.38%] [G loss: 1.851888]\n",
      "epoch:17 step:16787 [D loss: 0.720486, acc: 47.66%] [G loss: 1.804655]\n",
      "epoch:17 step:16788 [D loss: 0.642784, acc: 57.81%] [G loss: 1.773234]\n",
      "epoch:17 step:16789 [D loss: 0.642997, acc: 60.94%] [G loss: 1.935240]\n",
      "epoch:17 step:16790 [D loss: 0.639549, acc: 67.19%] [G loss: 1.775172]\n",
      "epoch:17 step:16791 [D loss: 0.653405, acc: 60.94%] [G loss: 1.941087]\n",
      "epoch:17 step:16792 [D loss: 0.686974, acc: 64.06%] [G loss: 1.854532]\n",
      "epoch:17 step:16793 [D loss: 0.648395, acc: 65.62%] [G loss: 1.940938]\n",
      "epoch:17 step:16794 [D loss: 0.632225, acc: 65.62%] [G loss: 1.833492]\n",
      "epoch:17 step:16795 [D loss: 0.672663, acc: 62.50%] [G loss: 1.838679]\n",
      "epoch:17 step:16796 [D loss: 0.643370, acc: 61.72%] [G loss: 1.826397]\n",
      "epoch:17 step:16797 [D loss: 0.626629, acc: 64.84%] [G loss: 1.941131]\n",
      "epoch:17 step:16798 [D loss: 0.700553, acc: 53.12%] [G loss: 1.913385]\n",
      "epoch:17 step:16799 [D loss: 0.656962, acc: 60.16%] [G loss: 1.914235]\n",
      "epoch:17 step:16800 [D loss: 0.658870, acc: 62.50%] [G loss: 1.814230]\n",
      "##############\n",
      "[2.36507853 1.4896458  6.14477356 4.79927723 3.53584581 5.8200234\n",
      " 4.20275645 4.71564481 4.68189937 3.61922724]\n",
      "##########\n",
      "epoch:17 step:16801 [D loss: 0.625087, acc: 64.06%] [G loss: 1.818217]\n",
      "epoch:17 step:16802 [D loss: 0.661970, acc: 58.59%] [G loss: 1.759346]\n",
      "epoch:17 step:16803 [D loss: 0.634765, acc: 65.62%] [G loss: 1.843601]\n",
      "epoch:17 step:16804 [D loss: 0.635769, acc: 67.19%] [G loss: 1.869254]\n",
      "epoch:17 step:16805 [D loss: 0.623372, acc: 64.84%] [G loss: 1.917684]\n",
      "epoch:17 step:16806 [D loss: 0.610107, acc: 64.84%] [G loss: 1.758694]\n",
      "epoch:17 step:16807 [D loss: 0.645609, acc: 62.50%] [G loss: 1.830811]\n",
      "epoch:17 step:16808 [D loss: 0.670801, acc: 59.38%] [G loss: 1.888943]\n",
      "epoch:17 step:16809 [D loss: 0.630917, acc: 65.62%] [G loss: 1.852057]\n",
      "epoch:17 step:16810 [D loss: 0.664578, acc: 61.72%] [G loss: 1.933400]\n",
      "epoch:17 step:16811 [D loss: 0.600988, acc: 67.97%] [G loss: 1.965736]\n",
      "epoch:17 step:16812 [D loss: 0.650679, acc: 56.25%] [G loss: 1.884256]\n",
      "epoch:17 step:16813 [D loss: 0.623873, acc: 59.38%] [G loss: 2.148114]\n",
      "epoch:17 step:16814 [D loss: 0.641927, acc: 63.28%] [G loss: 1.867608]\n",
      "epoch:17 step:16815 [D loss: 0.612521, acc: 65.62%] [G loss: 1.950080]\n",
      "epoch:17 step:16816 [D loss: 0.616619, acc: 68.75%] [G loss: 1.853480]\n",
      "epoch:17 step:16817 [D loss: 0.648941, acc: 57.81%] [G loss: 1.916561]\n",
      "epoch:17 step:16818 [D loss: 0.652134, acc: 64.84%] [G loss: 1.964596]\n",
      "epoch:17 step:16819 [D loss: 0.693900, acc: 50.00%] [G loss: 1.934155]\n",
      "epoch:17 step:16820 [D loss: 0.656574, acc: 57.81%] [G loss: 1.816018]\n",
      "epoch:17 step:16821 [D loss: 0.703500, acc: 60.94%] [G loss: 1.800740]\n",
      "epoch:17 step:16822 [D loss: 0.596188, acc: 68.75%] [G loss: 1.839296]\n",
      "epoch:17 step:16823 [D loss: 0.642033, acc: 62.50%] [G loss: 1.893016]\n",
      "epoch:17 step:16824 [D loss: 0.658634, acc: 60.16%] [G loss: 2.023055]\n",
      "epoch:17 step:16825 [D loss: 0.685872, acc: 57.81%] [G loss: 1.753936]\n",
      "epoch:17 step:16826 [D loss: 0.655046, acc: 63.28%] [G loss: 1.905973]\n",
      "epoch:17 step:16827 [D loss: 0.636803, acc: 65.62%] [G loss: 1.996923]\n",
      "epoch:17 step:16828 [D loss: 0.620668, acc: 66.41%] [G loss: 1.996099]\n",
      "epoch:17 step:16829 [D loss: 0.690182, acc: 57.81%] [G loss: 1.844745]\n",
      "epoch:17 step:16830 [D loss: 0.598246, acc: 67.19%] [G loss: 1.927817]\n",
      "epoch:17 step:16831 [D loss: 0.643460, acc: 65.62%] [G loss: 1.941592]\n",
      "epoch:17 step:16832 [D loss: 0.630718, acc: 65.62%] [G loss: 1.929488]\n",
      "epoch:17 step:16833 [D loss: 0.649595, acc: 65.62%] [G loss: 1.851529]\n",
      "epoch:17 step:16834 [D loss: 0.629549, acc: 65.62%] [G loss: 2.005350]\n",
      "epoch:17 step:16835 [D loss: 0.647600, acc: 65.62%] [G loss: 2.026228]\n",
      "epoch:17 step:16836 [D loss: 0.661279, acc: 61.72%] [G loss: 2.063241]\n",
      "epoch:17 step:16837 [D loss: 0.592249, acc: 70.31%] [G loss: 1.960134]\n",
      "epoch:17 step:16838 [D loss: 0.576461, acc: 72.66%] [G loss: 1.975260]\n",
      "epoch:17 step:16839 [D loss: 0.640782, acc: 57.81%] [G loss: 1.976012]\n",
      "epoch:17 step:16840 [D loss: 0.627062, acc: 65.62%] [G loss: 1.924792]\n",
      "epoch:17 step:16841 [D loss: 0.703390, acc: 57.81%] [G loss: 1.932662]\n",
      "epoch:17 step:16842 [D loss: 0.677436, acc: 61.72%] [G loss: 1.865116]\n",
      "epoch:17 step:16843 [D loss: 0.652181, acc: 59.38%] [G loss: 1.892346]\n",
      "epoch:17 step:16844 [D loss: 0.607476, acc: 67.19%] [G loss: 1.885451]\n",
      "epoch:17 step:16845 [D loss: 0.637113, acc: 64.84%] [G loss: 2.197529]\n",
      "epoch:17 step:16846 [D loss: 0.640290, acc: 64.84%] [G loss: 1.994365]\n",
      "epoch:17 step:16847 [D loss: 0.615851, acc: 65.62%] [G loss: 1.965176]\n",
      "epoch:17 step:16848 [D loss: 0.592156, acc: 70.31%] [G loss: 2.136601]\n",
      "epoch:17 step:16849 [D loss: 0.729382, acc: 59.38%] [G loss: 1.779138]\n",
      "epoch:17 step:16850 [D loss: 0.667751, acc: 62.50%] [G loss: 1.905948]\n",
      "epoch:17 step:16851 [D loss: 0.579637, acc: 74.22%] [G loss: 1.977579]\n",
      "epoch:17 step:16852 [D loss: 0.661909, acc: 60.16%] [G loss: 2.051415]\n",
      "epoch:17 step:16853 [D loss: 0.606151, acc: 71.09%] [G loss: 2.137452]\n",
      "epoch:17 step:16854 [D loss: 0.617097, acc: 67.19%] [G loss: 1.947843]\n",
      "epoch:17 step:16855 [D loss: 0.613414, acc: 67.97%] [G loss: 2.071050]\n",
      "epoch:17 step:16856 [D loss: 0.608936, acc: 65.62%] [G loss: 1.909212]\n",
      "epoch:17 step:16857 [D loss: 0.729844, acc: 52.34%] [G loss: 1.814898]\n",
      "epoch:17 step:16858 [D loss: 0.702712, acc: 53.12%] [G loss: 1.907951]\n",
      "epoch:17 step:16859 [D loss: 0.611175, acc: 64.84%] [G loss: 2.125236]\n",
      "epoch:17 step:16860 [D loss: 0.671465, acc: 60.94%] [G loss: 2.044492]\n",
      "epoch:17 step:16861 [D loss: 0.635189, acc: 63.28%] [G loss: 1.867381]\n",
      "epoch:17 step:16862 [D loss: 0.607695, acc: 70.31%] [G loss: 2.077740]\n",
      "epoch:17 step:16863 [D loss: 0.662114, acc: 60.94%] [G loss: 1.938988]\n",
      "epoch:17 step:16864 [D loss: 0.612974, acc: 60.16%] [G loss: 1.942876]\n",
      "epoch:17 step:16865 [D loss: 0.597861, acc: 70.31%] [G loss: 2.051918]\n",
      "epoch:17 step:16866 [D loss: 0.612519, acc: 66.41%] [G loss: 2.372858]\n",
      "epoch:18 step:16867 [D loss: 0.677875, acc: 59.38%] [G loss: 1.904176]\n",
      "epoch:18 step:16868 [D loss: 0.652134, acc: 59.38%] [G loss: 1.863263]\n",
      "epoch:18 step:16869 [D loss: 0.607885, acc: 66.41%] [G loss: 1.930337]\n",
      "epoch:18 step:16870 [D loss: 0.670077, acc: 64.84%] [G loss: 1.939854]\n",
      "epoch:18 step:16871 [D loss: 0.628272, acc: 62.50%] [G loss: 2.000796]\n",
      "epoch:18 step:16872 [D loss: 0.618698, acc: 65.62%] [G loss: 2.022734]\n",
      "epoch:18 step:16873 [D loss: 0.610928, acc: 64.06%] [G loss: 2.023057]\n",
      "epoch:18 step:16874 [D loss: 0.702193, acc: 57.03%] [G loss: 1.991640]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:18 step:16875 [D loss: 0.604247, acc: 65.62%] [G loss: 1.993162]\n",
      "epoch:18 step:16876 [D loss: 0.659874, acc: 57.03%] [G loss: 1.943341]\n",
      "epoch:18 step:16877 [D loss: 0.622348, acc: 65.62%] [G loss: 1.888355]\n",
      "epoch:18 step:16878 [D loss: 0.667414, acc: 57.81%] [G loss: 1.848229]\n",
      "epoch:18 step:16879 [D loss: 0.620584, acc: 67.97%] [G loss: 1.927254]\n",
      "epoch:18 step:16880 [D loss: 0.647209, acc: 62.50%] [G loss: 1.931776]\n",
      "epoch:18 step:16881 [D loss: 0.598152, acc: 68.75%] [G loss: 2.004188]\n",
      "epoch:18 step:16882 [D loss: 0.611462, acc: 62.50%] [G loss: 2.094891]\n",
      "epoch:18 step:16883 [D loss: 0.637709, acc: 58.59%] [G loss: 1.972530]\n",
      "epoch:18 step:16884 [D loss: 0.639572, acc: 67.19%] [G loss: 1.845954]\n",
      "epoch:18 step:16885 [D loss: 0.647714, acc: 64.06%] [G loss: 1.897882]\n",
      "epoch:18 step:16886 [D loss: 0.741836, acc: 50.78%] [G loss: 1.670716]\n",
      "epoch:18 step:16887 [D loss: 0.658026, acc: 60.16%] [G loss: 1.826388]\n",
      "epoch:18 step:16888 [D loss: 0.713624, acc: 53.12%] [G loss: 1.772057]\n",
      "epoch:18 step:16889 [D loss: 0.616689, acc: 67.19%] [G loss: 1.988473]\n",
      "epoch:18 step:16890 [D loss: 0.600353, acc: 68.75%] [G loss: 1.974902]\n",
      "epoch:18 step:16891 [D loss: 0.628231, acc: 66.41%] [G loss: 2.147089]\n",
      "epoch:18 step:16892 [D loss: 0.658060, acc: 59.38%] [G loss: 1.871928]\n",
      "epoch:18 step:16893 [D loss: 0.657021, acc: 60.16%] [G loss: 1.937270]\n",
      "epoch:18 step:16894 [D loss: 0.648438, acc: 63.28%] [G loss: 1.833495]\n",
      "epoch:18 step:16895 [D loss: 0.662359, acc: 60.94%] [G loss: 2.015748]\n",
      "epoch:18 step:16896 [D loss: 0.657685, acc: 60.16%] [G loss: 1.952050]\n",
      "epoch:18 step:16897 [D loss: 0.647553, acc: 61.72%] [G loss: 1.850023]\n",
      "epoch:18 step:16898 [D loss: 0.664543, acc: 60.94%] [G loss: 1.811801]\n",
      "epoch:18 step:16899 [D loss: 0.642819, acc: 65.62%] [G loss: 1.831018]\n",
      "epoch:18 step:16900 [D loss: 0.603432, acc: 66.41%] [G loss: 1.915741]\n",
      "epoch:18 step:16901 [D loss: 0.625960, acc: 62.50%] [G loss: 1.896197]\n",
      "epoch:18 step:16902 [D loss: 0.611435, acc: 65.62%] [G loss: 1.927403]\n",
      "epoch:18 step:16903 [D loss: 0.611805, acc: 66.41%] [G loss: 2.095779]\n",
      "epoch:18 step:16904 [D loss: 0.647043, acc: 58.59%] [G loss: 1.948125]\n",
      "epoch:18 step:16905 [D loss: 0.583182, acc: 68.75%] [G loss: 1.960932]\n",
      "epoch:18 step:16906 [D loss: 0.563521, acc: 71.09%] [G loss: 2.075550]\n",
      "epoch:18 step:16907 [D loss: 0.630905, acc: 64.84%] [G loss: 1.891508]\n",
      "epoch:18 step:16908 [D loss: 0.593856, acc: 68.75%] [G loss: 2.045126]\n",
      "epoch:18 step:16909 [D loss: 0.587246, acc: 67.97%] [G loss: 1.979110]\n",
      "epoch:18 step:16910 [D loss: 0.630051, acc: 60.94%] [G loss: 1.951175]\n",
      "epoch:18 step:16911 [D loss: 0.675245, acc: 57.03%] [G loss: 1.995696]\n",
      "epoch:18 step:16912 [D loss: 0.680979, acc: 58.59%] [G loss: 1.751533]\n",
      "epoch:18 step:16913 [D loss: 0.633240, acc: 66.41%] [G loss: 2.077710]\n",
      "epoch:18 step:16914 [D loss: 0.608101, acc: 64.06%] [G loss: 2.009020]\n",
      "epoch:18 step:16915 [D loss: 0.679543, acc: 57.03%] [G loss: 1.868981]\n",
      "epoch:18 step:16916 [D loss: 0.594836, acc: 68.75%] [G loss: 1.889733]\n",
      "epoch:18 step:16917 [D loss: 0.677923, acc: 62.50%] [G loss: 1.976080]\n",
      "epoch:18 step:16918 [D loss: 0.625854, acc: 64.84%] [G loss: 1.891813]\n",
      "epoch:18 step:16919 [D loss: 0.619334, acc: 64.06%] [G loss: 1.934921]\n",
      "epoch:18 step:16920 [D loss: 0.638524, acc: 59.38%] [G loss: 1.972787]\n",
      "epoch:18 step:16921 [D loss: 0.639969, acc: 62.50%] [G loss: 1.963469]\n",
      "epoch:18 step:16922 [D loss: 0.659821, acc: 62.50%] [G loss: 2.041589]\n",
      "epoch:18 step:16923 [D loss: 0.682706, acc: 59.38%] [G loss: 1.999817]\n",
      "epoch:18 step:16924 [D loss: 0.649404, acc: 66.41%] [G loss: 2.057078]\n",
      "epoch:18 step:16925 [D loss: 0.658035, acc: 60.16%] [G loss: 2.027877]\n",
      "epoch:18 step:16926 [D loss: 0.658510, acc: 52.34%] [G loss: 1.886272]\n",
      "epoch:18 step:16927 [D loss: 0.670343, acc: 58.59%] [G loss: 1.898008]\n",
      "epoch:18 step:16928 [D loss: 0.648436, acc: 63.28%] [G loss: 1.873704]\n",
      "epoch:18 step:16929 [D loss: 0.600835, acc: 64.84%] [G loss: 1.865009]\n",
      "epoch:18 step:16930 [D loss: 0.643647, acc: 57.03%] [G loss: 2.057182]\n",
      "epoch:18 step:16931 [D loss: 0.591066, acc: 67.19%] [G loss: 2.000176]\n",
      "epoch:18 step:16932 [D loss: 0.611592, acc: 67.19%] [G loss: 1.823035]\n",
      "epoch:18 step:16933 [D loss: 0.655818, acc: 61.72%] [G loss: 1.913058]\n",
      "epoch:18 step:16934 [D loss: 0.596388, acc: 70.31%] [G loss: 1.839477]\n",
      "epoch:18 step:16935 [D loss: 0.573828, acc: 72.66%] [G loss: 2.149265]\n",
      "epoch:18 step:16936 [D loss: 0.593526, acc: 67.19%] [G loss: 2.112180]\n",
      "epoch:18 step:16937 [D loss: 0.650586, acc: 64.84%] [G loss: 1.894645]\n",
      "epoch:18 step:16938 [D loss: 0.646608, acc: 67.97%] [G loss: 1.975354]\n",
      "epoch:18 step:16939 [D loss: 0.631772, acc: 67.97%] [G loss: 2.009848]\n",
      "epoch:18 step:16940 [D loss: 0.638909, acc: 64.84%] [G loss: 2.038723]\n",
      "epoch:18 step:16941 [D loss: 0.639473, acc: 60.94%] [G loss: 2.249699]\n",
      "epoch:18 step:16942 [D loss: 0.612997, acc: 66.41%] [G loss: 2.253581]\n",
      "epoch:18 step:16943 [D loss: 0.647044, acc: 65.62%] [G loss: 2.165455]\n",
      "epoch:18 step:16944 [D loss: 0.644250, acc: 64.06%] [G loss: 1.945268]\n",
      "epoch:18 step:16945 [D loss: 0.662533, acc: 55.47%] [G loss: 1.824200]\n",
      "epoch:18 step:16946 [D loss: 0.664457, acc: 60.94%] [G loss: 1.867406]\n",
      "epoch:18 step:16947 [D loss: 0.669826, acc: 53.12%] [G loss: 1.775528]\n",
      "epoch:18 step:16948 [D loss: 0.642166, acc: 57.81%] [G loss: 1.922914]\n",
      "epoch:18 step:16949 [D loss: 0.651488, acc: 64.06%] [G loss: 1.981767]\n",
      "epoch:18 step:16950 [D loss: 0.606700, acc: 65.62%] [G loss: 1.949727]\n",
      "epoch:18 step:16951 [D loss: 0.655578, acc: 61.72%] [G loss: 1.881167]\n",
      "epoch:18 step:16952 [D loss: 0.660280, acc: 62.50%] [G loss: 1.825766]\n",
      "epoch:18 step:16953 [D loss: 0.642289, acc: 62.50%] [G loss: 1.892020]\n",
      "epoch:18 step:16954 [D loss: 0.644079, acc: 63.28%] [G loss: 1.995471]\n",
      "epoch:18 step:16955 [D loss: 0.613388, acc: 67.97%] [G loss: 1.996828]\n",
      "epoch:18 step:16956 [D loss: 0.687482, acc: 57.03%] [G loss: 1.812400]\n",
      "epoch:18 step:16957 [D loss: 0.643324, acc: 60.94%] [G loss: 2.029672]\n",
      "epoch:18 step:16958 [D loss: 0.616990, acc: 63.28%] [G loss: 1.984216]\n",
      "epoch:18 step:16959 [D loss: 0.617667, acc: 62.50%] [G loss: 1.991312]\n",
      "epoch:18 step:16960 [D loss: 0.642083, acc: 57.81%] [G loss: 2.005210]\n",
      "epoch:18 step:16961 [D loss: 0.654887, acc: 65.62%] [G loss: 1.786846]\n",
      "epoch:18 step:16962 [D loss: 0.653057, acc: 64.84%] [G loss: 1.848196]\n",
      "epoch:18 step:16963 [D loss: 0.672141, acc: 55.47%] [G loss: 1.937850]\n",
      "epoch:18 step:16964 [D loss: 0.732008, acc: 53.12%] [G loss: 1.777329]\n",
      "epoch:18 step:16965 [D loss: 0.667968, acc: 59.38%] [G loss: 1.782138]\n",
      "epoch:18 step:16966 [D loss: 0.650124, acc: 60.94%] [G loss: 1.822957]\n",
      "epoch:18 step:16967 [D loss: 0.597610, acc: 67.97%] [G loss: 1.796047]\n",
      "epoch:18 step:16968 [D loss: 0.643071, acc: 66.41%] [G loss: 1.956663]\n",
      "epoch:18 step:16969 [D loss: 0.624836, acc: 63.28%] [G loss: 1.992454]\n",
      "epoch:18 step:16970 [D loss: 0.672350, acc: 60.94%] [G loss: 1.866675]\n",
      "epoch:18 step:16971 [D loss: 0.689594, acc: 58.59%] [G loss: 1.903817]\n",
      "epoch:18 step:16972 [D loss: 0.658351, acc: 57.81%] [G loss: 2.102885]\n",
      "epoch:18 step:16973 [D loss: 0.606076, acc: 65.62%] [G loss: 2.174646]\n",
      "epoch:18 step:16974 [D loss: 0.672894, acc: 61.72%] [G loss: 1.779544]\n",
      "epoch:18 step:16975 [D loss: 0.677817, acc: 60.94%] [G loss: 1.879061]\n",
      "epoch:18 step:16976 [D loss: 0.728826, acc: 51.56%] [G loss: 1.817026]\n",
      "epoch:18 step:16977 [D loss: 0.552385, acc: 75.00%] [G loss: 1.816671]\n",
      "epoch:18 step:16978 [D loss: 0.595126, acc: 70.31%] [G loss: 1.876453]\n",
      "epoch:18 step:16979 [D loss: 0.663748, acc: 57.81%] [G loss: 1.860374]\n",
      "epoch:18 step:16980 [D loss: 0.614872, acc: 66.41%] [G loss: 2.086545]\n",
      "epoch:18 step:16981 [D loss: 0.646867, acc: 61.72%] [G loss: 1.965001]\n",
      "epoch:18 step:16982 [D loss: 0.638922, acc: 63.28%] [G loss: 2.025213]\n",
      "epoch:18 step:16983 [D loss: 0.608957, acc: 64.84%] [G loss: 2.000080]\n",
      "epoch:18 step:16984 [D loss: 0.671245, acc: 57.81%] [G loss: 1.952266]\n",
      "epoch:18 step:16985 [D loss: 0.637318, acc: 62.50%] [G loss: 2.236487]\n",
      "epoch:18 step:16986 [D loss: 0.652131, acc: 65.62%] [G loss: 1.972619]\n",
      "epoch:18 step:16987 [D loss: 0.675524, acc: 57.03%] [G loss: 1.929426]\n",
      "epoch:18 step:16988 [D loss: 0.670037, acc: 55.47%] [G loss: 2.043034]\n",
      "epoch:18 step:16989 [D loss: 0.699036, acc: 55.47%] [G loss: 1.956857]\n",
      "epoch:18 step:16990 [D loss: 0.640084, acc: 67.19%] [G loss: 2.011818]\n",
      "epoch:18 step:16991 [D loss: 0.726614, acc: 56.25%] [G loss: 1.799634]\n",
      "epoch:18 step:16992 [D loss: 0.677749, acc: 57.81%] [G loss: 1.913986]\n",
      "epoch:18 step:16993 [D loss: 0.637094, acc: 60.94%] [G loss: 1.808324]\n",
      "epoch:18 step:16994 [D loss: 0.654688, acc: 62.50%] [G loss: 1.935816]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:18 step:16995 [D loss: 0.651056, acc: 57.81%] [G loss: 1.872315]\n",
      "epoch:18 step:16996 [D loss: 0.635253, acc: 62.50%] [G loss: 1.904615]\n",
      "epoch:18 step:16997 [D loss: 0.688957, acc: 59.38%] [G loss: 1.891292]\n",
      "epoch:18 step:16998 [D loss: 0.643203, acc: 63.28%] [G loss: 1.850229]\n",
      "epoch:18 step:16999 [D loss: 0.672751, acc: 54.69%] [G loss: 1.805745]\n",
      "epoch:18 step:17000 [D loss: 0.660056, acc: 53.91%] [G loss: 1.758544]\n",
      "##############\n",
      "[2.40931046 1.40454381 6.29139798 4.63497815 3.5475797  5.75435674\n",
      " 4.25153861 4.66636294 4.43214666 3.4602895 ]\n",
      "##########\n",
      "epoch:18 step:17001 [D loss: 0.650879, acc: 66.41%] [G loss: 1.860416]\n",
      "epoch:18 step:17002 [D loss: 0.628448, acc: 64.84%] [G loss: 1.774085]\n",
      "epoch:18 step:17003 [D loss: 0.657253, acc: 63.28%] [G loss: 1.790895]\n",
      "epoch:18 step:17004 [D loss: 0.680255, acc: 54.69%] [G loss: 1.878984]\n",
      "epoch:18 step:17005 [D loss: 0.654581, acc: 62.50%] [G loss: 1.805509]\n",
      "epoch:18 step:17006 [D loss: 0.664753, acc: 59.38%] [G loss: 1.742644]\n",
      "epoch:18 step:17007 [D loss: 0.653114, acc: 57.81%] [G loss: 1.705156]\n",
      "epoch:18 step:17008 [D loss: 0.618714, acc: 64.84%] [G loss: 1.879623]\n",
      "epoch:18 step:17009 [D loss: 0.632808, acc: 64.84%] [G loss: 1.860420]\n",
      "epoch:18 step:17010 [D loss: 0.618895, acc: 64.06%] [G loss: 1.881668]\n",
      "epoch:18 step:17011 [D loss: 0.660461, acc: 58.59%] [G loss: 1.829859]\n",
      "epoch:18 step:17012 [D loss: 0.623817, acc: 67.97%] [G loss: 2.014889]\n",
      "epoch:18 step:17013 [D loss: 0.622877, acc: 66.41%] [G loss: 1.901873]\n",
      "epoch:18 step:17014 [D loss: 0.719800, acc: 51.56%] [G loss: 1.870545]\n",
      "epoch:18 step:17015 [D loss: 0.644500, acc: 63.28%] [G loss: 1.908662]\n",
      "epoch:18 step:17016 [D loss: 0.696691, acc: 57.81%] [G loss: 1.825784]\n",
      "epoch:18 step:17017 [D loss: 0.622728, acc: 62.50%] [G loss: 1.959669]\n",
      "epoch:18 step:17018 [D loss: 0.689258, acc: 54.69%] [G loss: 1.991995]\n",
      "epoch:18 step:17019 [D loss: 0.673740, acc: 59.38%] [G loss: 1.935953]\n",
      "epoch:18 step:17020 [D loss: 0.620866, acc: 65.62%] [G loss: 1.989367]\n",
      "epoch:18 step:17021 [D loss: 0.669172, acc: 61.72%] [G loss: 1.841397]\n",
      "epoch:18 step:17022 [D loss: 0.630835, acc: 60.94%] [G loss: 2.045699]\n",
      "epoch:18 step:17023 [D loss: 0.643627, acc: 60.94%] [G loss: 1.858613]\n",
      "epoch:18 step:17024 [D loss: 0.636704, acc: 65.62%] [G loss: 1.895448]\n",
      "epoch:18 step:17025 [D loss: 0.663441, acc: 61.72%] [G loss: 1.948784]\n",
      "epoch:18 step:17026 [D loss: 0.715581, acc: 53.12%] [G loss: 1.687249]\n",
      "epoch:18 step:17027 [D loss: 0.670076, acc: 56.25%] [G loss: 1.793296]\n",
      "epoch:18 step:17028 [D loss: 0.661857, acc: 52.34%] [G loss: 1.896184]\n",
      "epoch:18 step:17029 [D loss: 0.655942, acc: 56.25%] [G loss: 1.778564]\n",
      "epoch:18 step:17030 [D loss: 0.629177, acc: 63.28%] [G loss: 1.946411]\n",
      "epoch:18 step:17031 [D loss: 0.599545, acc: 71.88%] [G loss: 1.892505]\n",
      "epoch:18 step:17032 [D loss: 0.647114, acc: 59.38%] [G loss: 1.931828]\n",
      "epoch:18 step:17033 [D loss: 0.609513, acc: 71.09%] [G loss: 1.842497]\n",
      "epoch:18 step:17034 [D loss: 0.646055, acc: 60.16%] [G loss: 1.874929]\n",
      "epoch:18 step:17035 [D loss: 0.627174, acc: 63.28%] [G loss: 1.947577]\n",
      "epoch:18 step:17036 [D loss: 0.718726, acc: 56.25%] [G loss: 1.861813]\n",
      "epoch:18 step:17037 [D loss: 0.614943, acc: 64.84%] [G loss: 1.930575]\n",
      "epoch:18 step:17038 [D loss: 0.676636, acc: 60.16%] [G loss: 1.871733]\n",
      "epoch:18 step:17039 [D loss: 0.655461, acc: 64.84%] [G loss: 1.803005]\n",
      "epoch:18 step:17040 [D loss: 0.706341, acc: 57.81%] [G loss: 1.760047]\n",
      "epoch:18 step:17041 [D loss: 0.673746, acc: 54.69%] [G loss: 1.754870]\n",
      "epoch:18 step:17042 [D loss: 0.658547, acc: 62.50%] [G loss: 1.757360]\n",
      "epoch:18 step:17043 [D loss: 0.637424, acc: 60.16%] [G loss: 1.938196]\n",
      "epoch:18 step:17044 [D loss: 0.626121, acc: 65.62%] [G loss: 1.779227]\n",
      "epoch:18 step:17045 [D loss: 0.709292, acc: 57.03%] [G loss: 1.707518]\n",
      "epoch:18 step:17046 [D loss: 0.733249, acc: 53.12%] [G loss: 1.765870]\n",
      "epoch:18 step:17047 [D loss: 0.660662, acc: 62.50%] [G loss: 1.773903]\n",
      "epoch:18 step:17048 [D loss: 0.715085, acc: 58.59%] [G loss: 1.783141]\n",
      "epoch:18 step:17049 [D loss: 0.687532, acc: 59.38%] [G loss: 1.761866]\n",
      "epoch:18 step:17050 [D loss: 0.641948, acc: 64.06%] [G loss: 1.960800]\n",
      "epoch:18 step:17051 [D loss: 0.645785, acc: 63.28%] [G loss: 1.817385]\n",
      "epoch:18 step:17052 [D loss: 0.608764, acc: 66.41%] [G loss: 1.933439]\n",
      "epoch:18 step:17053 [D loss: 0.654768, acc: 61.72%] [G loss: 1.866009]\n",
      "epoch:18 step:17054 [D loss: 0.634820, acc: 64.84%] [G loss: 1.771247]\n",
      "epoch:18 step:17055 [D loss: 0.663859, acc: 58.59%] [G loss: 1.914859]\n",
      "epoch:18 step:17056 [D loss: 0.623017, acc: 61.72%] [G loss: 1.864091]\n",
      "epoch:18 step:17057 [D loss: 0.631677, acc: 59.38%] [G loss: 1.870294]\n",
      "epoch:18 step:17058 [D loss: 0.641104, acc: 58.59%] [G loss: 1.909792]\n",
      "epoch:18 step:17059 [D loss: 0.628935, acc: 67.97%] [G loss: 1.874866]\n",
      "epoch:18 step:17060 [D loss: 0.613821, acc: 60.94%] [G loss: 2.077265]\n",
      "epoch:18 step:17061 [D loss: 0.643507, acc: 60.94%] [G loss: 1.951297]\n",
      "epoch:18 step:17062 [D loss: 0.645349, acc: 64.06%] [G loss: 1.813105]\n",
      "epoch:18 step:17063 [D loss: 0.611355, acc: 70.31%] [G loss: 1.943469]\n",
      "epoch:18 step:17064 [D loss: 0.677288, acc: 55.47%] [G loss: 1.947577]\n",
      "epoch:18 step:17065 [D loss: 0.676803, acc: 61.72%] [G loss: 1.879756]\n",
      "epoch:18 step:17066 [D loss: 0.628777, acc: 62.50%] [G loss: 1.800396]\n",
      "epoch:18 step:17067 [D loss: 0.651547, acc: 60.16%] [G loss: 1.959765]\n",
      "epoch:18 step:17068 [D loss: 0.658762, acc: 62.50%] [G loss: 1.932430]\n",
      "epoch:18 step:17069 [D loss: 0.685092, acc: 53.91%] [G loss: 1.906525]\n",
      "epoch:18 step:17070 [D loss: 0.697861, acc: 59.38%] [G loss: 1.880061]\n",
      "epoch:18 step:17071 [D loss: 0.719514, acc: 52.34%] [G loss: 1.821917]\n",
      "epoch:18 step:17072 [D loss: 0.628519, acc: 63.28%] [G loss: 2.054673]\n",
      "epoch:18 step:17073 [D loss: 0.601755, acc: 67.97%] [G loss: 2.032035]\n",
      "epoch:18 step:17074 [D loss: 0.631446, acc: 61.72%] [G loss: 1.981967]\n",
      "epoch:18 step:17075 [D loss: 0.602052, acc: 67.19%] [G loss: 2.058441]\n",
      "epoch:18 step:17076 [D loss: 0.666695, acc: 55.47%] [G loss: 1.829431]\n",
      "epoch:18 step:17077 [D loss: 0.674914, acc: 59.38%] [G loss: 1.879350]\n",
      "epoch:18 step:17078 [D loss: 0.644278, acc: 63.28%] [G loss: 1.716988]\n",
      "epoch:18 step:17079 [D loss: 0.623699, acc: 65.62%] [G loss: 1.845889]\n",
      "epoch:18 step:17080 [D loss: 0.691019, acc: 53.12%] [G loss: 1.757576]\n",
      "epoch:18 step:17081 [D loss: 0.675410, acc: 57.81%] [G loss: 1.869502]\n",
      "epoch:18 step:17082 [D loss: 0.665366, acc: 59.38%] [G loss: 1.826785]\n",
      "epoch:18 step:17083 [D loss: 0.615650, acc: 70.31%] [G loss: 1.983154]\n",
      "epoch:18 step:17084 [D loss: 0.641998, acc: 60.94%] [G loss: 2.065842]\n",
      "epoch:18 step:17085 [D loss: 0.607849, acc: 68.75%] [G loss: 2.063350]\n",
      "epoch:18 step:17086 [D loss: 0.666306, acc: 58.59%] [G loss: 1.763737]\n",
      "epoch:18 step:17087 [D loss: 0.610828, acc: 65.62%] [G loss: 1.946697]\n",
      "epoch:18 step:17088 [D loss: 0.679637, acc: 59.38%] [G loss: 1.854280]\n",
      "epoch:18 step:17089 [D loss: 0.636994, acc: 64.84%] [G loss: 1.969851]\n",
      "epoch:18 step:17090 [D loss: 0.650354, acc: 59.38%] [G loss: 1.873122]\n",
      "epoch:18 step:17091 [D loss: 0.678264, acc: 60.16%] [G loss: 1.798639]\n",
      "epoch:18 step:17092 [D loss: 0.625097, acc: 63.28%] [G loss: 1.672233]\n",
      "epoch:18 step:17093 [D loss: 0.669460, acc: 58.59%] [G loss: 1.798620]\n",
      "epoch:18 step:17094 [D loss: 0.675313, acc: 55.47%] [G loss: 1.758952]\n",
      "epoch:18 step:17095 [D loss: 0.593148, acc: 68.75%] [G loss: 1.947694]\n",
      "epoch:18 step:17096 [D loss: 0.631104, acc: 60.94%] [G loss: 2.260305]\n",
      "epoch:18 step:17097 [D loss: 0.580436, acc: 66.41%] [G loss: 2.334924]\n",
      "epoch:18 step:17098 [D loss: 0.508039, acc: 77.34%] [G loss: 2.500804]\n",
      "epoch:18 step:17099 [D loss: 0.728991, acc: 47.66%] [G loss: 1.835052]\n",
      "epoch:18 step:17100 [D loss: 0.655154, acc: 63.28%] [G loss: 1.829304]\n",
      "epoch:18 step:17101 [D loss: 0.647180, acc: 64.06%] [G loss: 1.794355]\n",
      "epoch:18 step:17102 [D loss: 0.638554, acc: 66.41%] [G loss: 1.850065]\n",
      "epoch:18 step:17103 [D loss: 0.648049, acc: 60.94%] [G loss: 1.875525]\n",
      "epoch:18 step:17104 [D loss: 0.655955, acc: 66.41%] [G loss: 1.834974]\n",
      "epoch:18 step:17105 [D loss: 0.658949, acc: 64.06%] [G loss: 1.867079]\n",
      "epoch:18 step:17106 [D loss: 0.730107, acc: 58.59%] [G loss: 1.846638]\n",
      "epoch:18 step:17107 [D loss: 0.618393, acc: 67.19%] [G loss: 1.977265]\n",
      "epoch:18 step:17108 [D loss: 0.618423, acc: 61.72%] [G loss: 1.959117]\n",
      "epoch:18 step:17109 [D loss: 0.631415, acc: 61.72%] [G loss: 1.959517]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:18 step:17110 [D loss: 0.673439, acc: 60.16%] [G loss: 1.857229]\n",
      "epoch:18 step:17111 [D loss: 0.612674, acc: 67.97%] [G loss: 1.875036]\n",
      "epoch:18 step:17112 [D loss: 0.653632, acc: 58.59%] [G loss: 1.841706]\n",
      "epoch:18 step:17113 [D loss: 0.663779, acc: 57.03%] [G loss: 1.947098]\n",
      "epoch:18 step:17114 [D loss: 0.614099, acc: 67.19%] [G loss: 2.013257]\n",
      "epoch:18 step:17115 [D loss: 0.668227, acc: 59.38%] [G loss: 1.866529]\n",
      "epoch:18 step:17116 [D loss: 0.728696, acc: 51.56%] [G loss: 1.727885]\n",
      "epoch:18 step:17117 [D loss: 0.671853, acc: 60.16%] [G loss: 1.889409]\n",
      "epoch:18 step:17118 [D loss: 0.689756, acc: 60.16%] [G loss: 1.746489]\n",
      "epoch:18 step:17119 [D loss: 0.633355, acc: 61.72%] [G loss: 1.919076]\n",
      "epoch:18 step:17120 [D loss: 0.650864, acc: 60.94%] [G loss: 1.856622]\n",
      "epoch:18 step:17121 [D loss: 0.666011, acc: 64.06%] [G loss: 1.774624]\n",
      "epoch:18 step:17122 [D loss: 0.621576, acc: 66.41%] [G loss: 1.881290]\n",
      "epoch:18 step:17123 [D loss: 0.636163, acc: 68.75%] [G loss: 2.030189]\n",
      "epoch:18 step:17124 [D loss: 0.642795, acc: 58.59%] [G loss: 1.789819]\n",
      "epoch:18 step:17125 [D loss: 0.618070, acc: 68.75%] [G loss: 1.974550]\n",
      "epoch:18 step:17126 [D loss: 0.675379, acc: 60.94%] [G loss: 1.814930]\n",
      "epoch:18 step:17127 [D loss: 0.617103, acc: 63.28%] [G loss: 2.015924]\n",
      "epoch:18 step:17128 [D loss: 0.661511, acc: 59.38%] [G loss: 1.856798]\n",
      "epoch:18 step:17129 [D loss: 0.680599, acc: 60.94%] [G loss: 1.866405]\n",
      "epoch:18 step:17130 [D loss: 0.605731, acc: 68.75%] [G loss: 2.009818]\n",
      "epoch:18 step:17131 [D loss: 0.671298, acc: 63.28%] [G loss: 1.734900]\n",
      "epoch:18 step:17132 [D loss: 0.616625, acc: 62.50%] [G loss: 1.836022]\n",
      "epoch:18 step:17133 [D loss: 0.674202, acc: 62.50%] [G loss: 1.875751]\n",
      "epoch:18 step:17134 [D loss: 0.655565, acc: 58.59%] [G loss: 1.804542]\n",
      "epoch:18 step:17135 [D loss: 0.634926, acc: 62.50%] [G loss: 1.847832]\n",
      "epoch:18 step:17136 [D loss: 0.651232, acc: 60.16%] [G loss: 1.764345]\n",
      "epoch:18 step:17137 [D loss: 0.659589, acc: 65.62%] [G loss: 1.901767]\n",
      "epoch:18 step:17138 [D loss: 0.598904, acc: 68.75%] [G loss: 1.998911]\n",
      "epoch:18 step:17139 [D loss: 0.642193, acc: 64.06%] [G loss: 1.882696]\n",
      "epoch:18 step:17140 [D loss: 0.625718, acc: 61.72%] [G loss: 2.056934]\n",
      "epoch:18 step:17141 [D loss: 0.631604, acc: 62.50%] [G loss: 2.126367]\n",
      "epoch:18 step:17142 [D loss: 0.637692, acc: 60.16%] [G loss: 2.278261]\n",
      "epoch:18 step:17143 [D loss: 0.612549, acc: 65.62%] [G loss: 1.949195]\n",
      "epoch:18 step:17144 [D loss: 0.693508, acc: 57.81%] [G loss: 1.927919]\n",
      "epoch:18 step:17145 [D loss: 0.665055, acc: 60.16%] [G loss: 1.945766]\n",
      "epoch:18 step:17146 [D loss: 0.586237, acc: 66.41%] [G loss: 1.879686]\n",
      "epoch:18 step:17147 [D loss: 0.721165, acc: 57.03%] [G loss: 1.817069]\n",
      "epoch:18 step:17148 [D loss: 0.658617, acc: 62.50%] [G loss: 1.740948]\n",
      "epoch:18 step:17149 [D loss: 0.653569, acc: 68.75%] [G loss: 1.925004]\n",
      "epoch:18 step:17150 [D loss: 0.644050, acc: 64.06%] [G loss: 1.806136]\n",
      "epoch:18 step:17151 [D loss: 0.652217, acc: 65.62%] [G loss: 1.870371]\n",
      "epoch:18 step:17152 [D loss: 0.650251, acc: 60.94%] [G loss: 1.821608]\n",
      "epoch:18 step:17153 [D loss: 0.644107, acc: 58.59%] [G loss: 1.852682]\n",
      "epoch:18 step:17154 [D loss: 0.659654, acc: 60.94%] [G loss: 1.919517]\n",
      "epoch:18 step:17155 [D loss: 0.605390, acc: 71.88%] [G loss: 1.937509]\n",
      "epoch:18 step:17156 [D loss: 0.646420, acc: 60.16%] [G loss: 1.856144]\n",
      "epoch:18 step:17157 [D loss: 0.692333, acc: 60.16%] [G loss: 1.848051]\n",
      "epoch:18 step:17158 [D loss: 0.654613, acc: 57.03%] [G loss: 1.909208]\n",
      "epoch:18 step:17159 [D loss: 0.640911, acc: 64.06%] [G loss: 1.831173]\n",
      "epoch:18 step:17160 [D loss: 0.671081, acc: 60.16%] [G loss: 1.882387]\n",
      "epoch:18 step:17161 [D loss: 0.611304, acc: 66.41%] [G loss: 1.856756]\n",
      "epoch:18 step:17162 [D loss: 0.678643, acc: 55.47%] [G loss: 2.014554]\n",
      "epoch:18 step:17163 [D loss: 0.679514, acc: 58.59%] [G loss: 1.853108]\n",
      "epoch:18 step:17164 [D loss: 0.613480, acc: 65.62%] [G loss: 1.874520]\n",
      "epoch:18 step:17165 [D loss: 0.596414, acc: 67.19%] [G loss: 1.877761]\n",
      "epoch:18 step:17166 [D loss: 0.615149, acc: 66.41%] [G loss: 2.051958]\n",
      "epoch:18 step:17167 [D loss: 0.657610, acc: 62.50%] [G loss: 1.857924]\n",
      "epoch:18 step:17168 [D loss: 0.642417, acc: 66.41%] [G loss: 1.888756]\n",
      "epoch:18 step:17169 [D loss: 0.645279, acc: 62.50%] [G loss: 1.899308]\n",
      "epoch:18 step:17170 [D loss: 0.698340, acc: 57.81%] [G loss: 1.847339]\n",
      "epoch:18 step:17171 [D loss: 0.624246, acc: 63.28%] [G loss: 1.851912]\n",
      "epoch:18 step:17172 [D loss: 0.619478, acc: 70.31%] [G loss: 1.934132]\n",
      "epoch:18 step:17173 [D loss: 0.638756, acc: 62.50%] [G loss: 1.707613]\n",
      "epoch:18 step:17174 [D loss: 0.687795, acc: 60.16%] [G loss: 1.846904]\n",
      "epoch:18 step:17175 [D loss: 0.616541, acc: 66.41%] [G loss: 1.879736]\n",
      "epoch:18 step:17176 [D loss: 0.641920, acc: 60.94%] [G loss: 1.763303]\n",
      "epoch:18 step:17177 [D loss: 0.671265, acc: 60.94%] [G loss: 1.931493]\n",
      "epoch:18 step:17178 [D loss: 0.575224, acc: 70.31%] [G loss: 2.161704]\n",
      "epoch:18 step:17179 [D loss: 0.580431, acc: 71.09%] [G loss: 2.037727]\n",
      "epoch:18 step:17180 [D loss: 0.573375, acc: 67.97%] [G loss: 2.017265]\n",
      "epoch:18 step:17181 [D loss: 0.575907, acc: 68.75%] [G loss: 2.089530]\n",
      "epoch:18 step:17182 [D loss: 0.663718, acc: 58.59%] [G loss: 1.783130]\n",
      "epoch:18 step:17183 [D loss: 0.701630, acc: 53.91%] [G loss: 1.802105]\n",
      "epoch:18 step:17184 [D loss: 0.628738, acc: 64.84%] [G loss: 1.986637]\n",
      "epoch:18 step:17185 [D loss: 0.710230, acc: 52.34%] [G loss: 1.774923]\n",
      "epoch:18 step:17186 [D loss: 0.666751, acc: 63.28%] [G loss: 1.900060]\n",
      "epoch:18 step:17187 [D loss: 0.681124, acc: 57.81%] [G loss: 1.994271]\n",
      "epoch:18 step:17188 [D loss: 0.671903, acc: 62.50%] [G loss: 1.859829]\n",
      "epoch:18 step:17189 [D loss: 0.706566, acc: 55.47%] [G loss: 1.738424]\n",
      "epoch:18 step:17190 [D loss: 0.666078, acc: 59.38%] [G loss: 1.847805]\n",
      "epoch:18 step:17191 [D loss: 0.655604, acc: 62.50%] [G loss: 1.923057]\n",
      "epoch:18 step:17192 [D loss: 0.644385, acc: 68.75%] [G loss: 1.925758]\n",
      "epoch:18 step:17193 [D loss: 0.656904, acc: 63.28%] [G loss: 1.822693]\n",
      "epoch:18 step:17194 [D loss: 0.593554, acc: 71.09%] [G loss: 1.849569]\n",
      "epoch:18 step:17195 [D loss: 0.642218, acc: 60.94%] [G loss: 1.878257]\n",
      "epoch:18 step:17196 [D loss: 0.636005, acc: 64.06%] [G loss: 1.936837]\n",
      "epoch:18 step:17197 [D loss: 0.624907, acc: 67.19%] [G loss: 1.957082]\n",
      "epoch:18 step:17198 [D loss: 0.626879, acc: 62.50%] [G loss: 2.008545]\n",
      "epoch:18 step:17199 [D loss: 0.634559, acc: 67.19%] [G loss: 1.853591]\n",
      "epoch:18 step:17200 [D loss: 0.665898, acc: 62.50%] [G loss: 1.929551]\n",
      "##############\n",
      "[2.48817246 1.17085745 6.22146577 4.51103933 3.61978297 5.72164252\n",
      " 4.38471471 4.8289966  4.49115385 3.68211289]\n",
      "##########\n",
      "epoch:18 step:17201 [D loss: 0.656785, acc: 63.28%] [G loss: 1.972588]\n",
      "epoch:18 step:17202 [D loss: 0.615066, acc: 69.53%] [G loss: 1.941308]\n",
      "epoch:18 step:17203 [D loss: 0.660900, acc: 60.94%] [G loss: 1.921106]\n",
      "epoch:18 step:17204 [D loss: 0.697179, acc: 57.81%] [G loss: 2.134472]\n",
      "epoch:18 step:17205 [D loss: 0.616341, acc: 67.97%] [G loss: 1.881532]\n",
      "epoch:18 step:17206 [D loss: 0.639070, acc: 64.84%] [G loss: 1.937263]\n",
      "epoch:18 step:17207 [D loss: 0.699106, acc: 55.47%] [G loss: 1.660921]\n",
      "epoch:18 step:17208 [D loss: 0.697207, acc: 57.81%] [G loss: 1.711084]\n",
      "epoch:18 step:17209 [D loss: 0.674781, acc: 60.16%] [G loss: 1.737475]\n",
      "epoch:18 step:17210 [D loss: 0.686453, acc: 55.47%] [G loss: 1.846078]\n",
      "epoch:18 step:17211 [D loss: 0.544664, acc: 71.09%] [G loss: 2.172998]\n",
      "epoch:18 step:17212 [D loss: 0.602032, acc: 67.19%] [G loss: 1.981076]\n",
      "epoch:18 step:17213 [D loss: 0.565346, acc: 74.22%] [G loss: 2.409772]\n",
      "epoch:18 step:17214 [D loss: 0.682203, acc: 61.72%] [G loss: 1.793973]\n",
      "epoch:18 step:17215 [D loss: 0.707567, acc: 53.12%] [G loss: 1.779125]\n",
      "epoch:18 step:17216 [D loss: 0.623940, acc: 64.84%] [G loss: 1.783452]\n",
      "epoch:18 step:17217 [D loss: 0.668842, acc: 57.81%] [G loss: 1.788397]\n",
      "epoch:18 step:17218 [D loss: 0.699715, acc: 53.91%] [G loss: 1.793706]\n",
      "epoch:18 step:17219 [D loss: 0.628457, acc: 64.84%] [G loss: 1.853758]\n",
      "epoch:18 step:17220 [D loss: 0.594001, acc: 71.88%] [G loss: 1.875563]\n",
      "epoch:18 step:17221 [D loss: 0.683211, acc: 58.59%] [G loss: 1.867318]\n",
      "epoch:18 step:17222 [D loss: 0.685486, acc: 59.38%] [G loss: 1.726072]\n",
      "epoch:18 step:17223 [D loss: 0.611262, acc: 70.31%] [G loss: 1.918165]\n",
      "epoch:18 step:17224 [D loss: 0.610220, acc: 62.50%] [G loss: 1.988922]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:18 step:17225 [D loss: 0.600218, acc: 67.19%] [G loss: 2.105376]\n",
      "epoch:18 step:17226 [D loss: 0.624301, acc: 66.41%] [G loss: 2.001944]\n",
      "epoch:18 step:17227 [D loss: 0.676664, acc: 60.94%] [G loss: 1.781399]\n",
      "epoch:18 step:17228 [D loss: 0.681205, acc: 53.91%] [G loss: 1.686364]\n",
      "epoch:18 step:17229 [D loss: 0.623211, acc: 65.62%] [G loss: 1.851110]\n",
      "epoch:18 step:17230 [D loss: 0.601912, acc: 72.66%] [G loss: 2.014760]\n",
      "epoch:18 step:17231 [D loss: 0.679084, acc: 59.38%] [G loss: 1.776191]\n",
      "epoch:18 step:17232 [D loss: 0.628474, acc: 66.41%] [G loss: 2.002392]\n",
      "epoch:18 step:17233 [D loss: 0.750193, acc: 53.91%] [G loss: 1.893574]\n",
      "epoch:18 step:17234 [D loss: 0.611284, acc: 67.19%] [G loss: 1.729754]\n",
      "epoch:18 step:17235 [D loss: 0.648476, acc: 59.38%] [G loss: 1.789201]\n",
      "epoch:18 step:17236 [D loss: 0.619038, acc: 64.84%] [G loss: 2.037381]\n",
      "epoch:18 step:17237 [D loss: 0.648126, acc: 60.16%] [G loss: 2.080117]\n",
      "epoch:18 step:17238 [D loss: 0.646169, acc: 57.81%] [G loss: 1.885986]\n",
      "epoch:18 step:17239 [D loss: 0.660686, acc: 60.94%] [G loss: 1.854860]\n",
      "epoch:18 step:17240 [D loss: 0.596098, acc: 67.97%] [G loss: 2.118999]\n",
      "epoch:18 step:17241 [D loss: 0.658364, acc: 60.16%] [G loss: 1.767151]\n",
      "epoch:18 step:17242 [D loss: 0.672228, acc: 60.16%] [G loss: 1.743347]\n",
      "epoch:18 step:17243 [D loss: 0.709521, acc: 53.12%] [G loss: 1.769994]\n",
      "epoch:18 step:17244 [D loss: 0.652285, acc: 60.94%] [G loss: 1.841919]\n",
      "epoch:18 step:17245 [D loss: 0.588683, acc: 67.19%] [G loss: 1.879349]\n",
      "epoch:18 step:17246 [D loss: 0.629699, acc: 62.50%] [G loss: 1.847010]\n",
      "epoch:18 step:17247 [D loss: 0.599308, acc: 70.31%] [G loss: 1.886919]\n",
      "epoch:18 step:17248 [D loss: 0.651748, acc: 61.72%] [G loss: 1.780114]\n",
      "epoch:18 step:17249 [D loss: 0.657839, acc: 60.16%] [G loss: 2.026320]\n",
      "epoch:18 step:17250 [D loss: 0.643165, acc: 60.16%] [G loss: 1.971723]\n",
      "epoch:18 step:17251 [D loss: 0.668249, acc: 57.81%] [G loss: 1.931098]\n",
      "epoch:18 step:17252 [D loss: 0.684827, acc: 57.03%] [G loss: 1.869532]\n",
      "epoch:18 step:17253 [D loss: 0.721651, acc: 55.47%] [G loss: 1.803812]\n",
      "epoch:18 step:17254 [D loss: 0.596700, acc: 70.31%] [G loss: 2.022051]\n",
      "epoch:18 step:17255 [D loss: 0.649973, acc: 66.41%] [G loss: 1.863667]\n",
      "epoch:18 step:17256 [D loss: 0.690773, acc: 64.06%] [G loss: 1.896206]\n",
      "epoch:18 step:17257 [D loss: 0.691205, acc: 53.91%] [G loss: 1.711977]\n",
      "epoch:18 step:17258 [D loss: 0.646283, acc: 63.28%] [G loss: 1.972754]\n",
      "epoch:18 step:17259 [D loss: 0.682109, acc: 63.28%] [G loss: 1.724053]\n",
      "epoch:18 step:17260 [D loss: 0.625186, acc: 65.62%] [G loss: 1.844151]\n",
      "epoch:18 step:17261 [D loss: 0.652483, acc: 58.59%] [G loss: 1.953707]\n",
      "epoch:18 step:17262 [D loss: 0.718802, acc: 51.56%] [G loss: 1.797364]\n",
      "epoch:18 step:17263 [D loss: 0.641237, acc: 61.72%] [G loss: 1.773551]\n",
      "epoch:18 step:17264 [D loss: 0.659854, acc: 62.50%] [G loss: 1.828360]\n",
      "epoch:18 step:17265 [D loss: 0.626114, acc: 67.97%] [G loss: 1.855304]\n",
      "epoch:18 step:17266 [D loss: 0.688284, acc: 59.38%] [G loss: 1.881674]\n",
      "epoch:18 step:17267 [D loss: 0.642050, acc: 60.94%] [G loss: 1.817637]\n",
      "epoch:18 step:17268 [D loss: 0.646701, acc: 63.28%] [G loss: 1.804810]\n",
      "epoch:18 step:17269 [D loss: 0.688687, acc: 57.81%] [G loss: 1.915078]\n",
      "epoch:18 step:17270 [D loss: 0.664877, acc: 59.38%] [G loss: 1.823479]\n",
      "epoch:18 step:17271 [D loss: 0.623324, acc: 63.28%] [G loss: 1.973611]\n",
      "epoch:18 step:17272 [D loss: 0.643598, acc: 61.72%] [G loss: 1.747740]\n",
      "epoch:18 step:17273 [D loss: 0.682242, acc: 64.84%] [G loss: 1.948454]\n",
      "epoch:18 step:17274 [D loss: 0.651988, acc: 59.38%] [G loss: 1.760503]\n",
      "epoch:18 step:17275 [D loss: 0.680255, acc: 56.25%] [G loss: 1.769493]\n",
      "epoch:18 step:17276 [D loss: 0.611266, acc: 70.31%] [G loss: 1.864173]\n",
      "epoch:18 step:17277 [D loss: 0.684099, acc: 61.72%] [G loss: 1.926216]\n",
      "epoch:18 step:17278 [D loss: 0.619622, acc: 65.62%] [G loss: 1.806612]\n",
      "epoch:18 step:17279 [D loss: 0.665338, acc: 60.16%] [G loss: 1.897092]\n",
      "epoch:18 step:17280 [D loss: 0.615773, acc: 66.41%] [G loss: 1.871888]\n",
      "epoch:18 step:17281 [D loss: 0.640229, acc: 70.31%] [G loss: 1.871463]\n",
      "epoch:18 step:17282 [D loss: 0.664436, acc: 55.47%] [G loss: 1.990010]\n",
      "epoch:18 step:17283 [D loss: 0.620106, acc: 67.97%] [G loss: 1.950097]\n",
      "epoch:18 step:17284 [D loss: 0.686185, acc: 60.16%] [G loss: 1.814943]\n",
      "epoch:18 step:17285 [D loss: 0.584459, acc: 68.75%] [G loss: 2.019025]\n",
      "epoch:18 step:17286 [D loss: 0.650198, acc: 61.72%] [G loss: 1.945226]\n",
      "epoch:18 step:17287 [D loss: 0.648441, acc: 63.28%] [G loss: 1.900065]\n",
      "epoch:18 step:17288 [D loss: 0.668759, acc: 58.59%] [G loss: 1.799142]\n",
      "epoch:18 step:17289 [D loss: 0.673072, acc: 61.72%] [G loss: 1.791744]\n",
      "epoch:18 step:17290 [D loss: 0.630450, acc: 61.72%] [G loss: 1.866813]\n",
      "epoch:18 step:17291 [D loss: 0.679589, acc: 57.03%] [G loss: 1.805915]\n",
      "epoch:18 step:17292 [D loss: 0.641420, acc: 60.94%] [G loss: 2.121826]\n",
      "epoch:18 step:17293 [D loss: 0.574941, acc: 71.88%] [G loss: 1.943551]\n",
      "epoch:18 step:17294 [D loss: 0.557173, acc: 76.56%] [G loss: 2.013451]\n",
      "epoch:18 step:17295 [D loss: 0.569595, acc: 71.09%] [G loss: 2.178128]\n",
      "epoch:18 step:17296 [D loss: 0.579081, acc: 72.66%] [G loss: 2.300394]\n",
      "epoch:18 step:17297 [D loss: 0.671590, acc: 60.94%] [G loss: 1.963059]\n",
      "epoch:18 step:17298 [D loss: 0.680042, acc: 57.81%] [G loss: 1.849826]\n",
      "epoch:18 step:17299 [D loss: 0.665601, acc: 62.50%] [G loss: 1.936522]\n",
      "epoch:18 step:17300 [D loss: 0.623275, acc: 65.62%] [G loss: 1.889159]\n",
      "epoch:18 step:17301 [D loss: 0.652790, acc: 60.16%] [G loss: 2.081581]\n",
      "epoch:18 step:17302 [D loss: 0.700647, acc: 63.28%] [G loss: 1.952310]\n",
      "epoch:18 step:17303 [D loss: 0.708574, acc: 53.91%] [G loss: 1.816583]\n",
      "epoch:18 step:17304 [D loss: 0.645677, acc: 62.50%] [G loss: 1.781065]\n",
      "epoch:18 step:17305 [D loss: 0.637740, acc: 65.62%] [G loss: 1.752266]\n",
      "epoch:18 step:17306 [D loss: 0.672197, acc: 60.94%] [G loss: 1.854273]\n",
      "epoch:18 step:17307 [D loss: 0.699707, acc: 55.47%] [G loss: 1.834254]\n",
      "epoch:18 step:17308 [D loss: 0.672708, acc: 59.38%] [G loss: 1.782370]\n",
      "epoch:18 step:17309 [D loss: 0.657007, acc: 67.97%] [G loss: 1.859817]\n",
      "epoch:18 step:17310 [D loss: 0.672093, acc: 60.16%] [G loss: 1.674658]\n",
      "epoch:18 step:17311 [D loss: 0.650181, acc: 63.28%] [G loss: 1.820200]\n",
      "epoch:18 step:17312 [D loss: 0.637716, acc: 64.06%] [G loss: 1.821445]\n",
      "epoch:18 step:17313 [D loss: 0.665407, acc: 59.38%] [G loss: 1.825695]\n",
      "epoch:18 step:17314 [D loss: 0.668194, acc: 58.59%] [G loss: 1.756752]\n",
      "epoch:18 step:17315 [D loss: 0.621970, acc: 67.97%] [G loss: 1.800363]\n",
      "epoch:18 step:17316 [D loss: 0.643053, acc: 60.94%] [G loss: 1.724652]\n",
      "epoch:18 step:17317 [D loss: 0.651244, acc: 60.94%] [G loss: 1.934439]\n",
      "epoch:18 step:17318 [D loss: 0.680314, acc: 61.72%] [G loss: 1.751771]\n",
      "epoch:18 step:17319 [D loss: 0.613566, acc: 65.62%] [G loss: 2.054408]\n",
      "epoch:18 step:17320 [D loss: 0.654197, acc: 57.03%] [G loss: 1.865752]\n",
      "epoch:18 step:17321 [D loss: 0.619103, acc: 65.62%] [G loss: 1.846132]\n",
      "epoch:18 step:17322 [D loss: 0.653915, acc: 60.16%] [G loss: 2.010345]\n",
      "epoch:18 step:17323 [D loss: 0.641949, acc: 64.84%] [G loss: 1.882918]\n",
      "epoch:18 step:17324 [D loss: 0.655637, acc: 56.25%] [G loss: 1.808101]\n",
      "epoch:18 step:17325 [D loss: 0.706670, acc: 52.34%] [G loss: 1.843409]\n",
      "epoch:18 step:17326 [D loss: 0.680739, acc: 58.59%] [G loss: 1.802470]\n",
      "epoch:18 step:17327 [D loss: 0.691125, acc: 57.03%] [G loss: 1.805887]\n",
      "epoch:18 step:17328 [D loss: 0.638927, acc: 66.41%] [G loss: 1.786945]\n",
      "epoch:18 step:17329 [D loss: 0.642840, acc: 65.62%] [G loss: 1.861065]\n",
      "epoch:18 step:17330 [D loss: 0.611547, acc: 67.19%] [G loss: 1.885551]\n",
      "epoch:18 step:17331 [D loss: 0.636379, acc: 65.62%] [G loss: 1.959096]\n",
      "epoch:18 step:17332 [D loss: 0.660352, acc: 57.03%] [G loss: 1.830577]\n",
      "epoch:18 step:17333 [D loss: 0.697106, acc: 55.47%] [G loss: 1.870165]\n",
      "epoch:18 step:17334 [D loss: 0.616815, acc: 64.84%] [G loss: 1.952241]\n",
      "epoch:18 step:17335 [D loss: 0.576306, acc: 77.34%] [G loss: 2.091999]\n",
      "epoch:18 step:17336 [D loss: 0.623186, acc: 67.97%] [G loss: 1.954335]\n",
      "epoch:18 step:17337 [D loss: 0.579394, acc: 65.62%] [G loss: 2.182391]\n",
      "epoch:18 step:17338 [D loss: 0.660966, acc: 60.94%] [G loss: 2.085577]\n",
      "epoch:18 step:17339 [D loss: 0.699073, acc: 55.47%] [G loss: 1.759599]\n",
      "epoch:18 step:17340 [D loss: 0.682017, acc: 61.72%] [G loss: 1.751250]\n",
      "epoch:18 step:17341 [D loss: 0.649488, acc: 57.03%] [G loss: 1.934723]\n",
      "epoch:18 step:17342 [D loss: 0.646633, acc: 60.16%] [G loss: 1.975450]\n",
      "epoch:18 step:17343 [D loss: 0.664949, acc: 53.91%] [G loss: 1.782200]\n",
      "epoch:18 step:17344 [D loss: 0.691876, acc: 53.91%] [G loss: 1.760482]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:18 step:17345 [D loss: 0.619707, acc: 72.66%] [G loss: 1.919633]\n",
      "epoch:18 step:17346 [D loss: 0.628690, acc: 64.06%] [G loss: 1.890523]\n",
      "epoch:18 step:17347 [D loss: 0.607796, acc: 68.75%] [G loss: 2.175220]\n",
      "epoch:18 step:17348 [D loss: 0.717908, acc: 52.34%] [G loss: 1.823678]\n",
      "epoch:18 step:17349 [D loss: 0.656563, acc: 60.94%] [G loss: 1.761079]\n",
      "epoch:18 step:17350 [D loss: 0.663769, acc: 57.03%] [G loss: 1.876974]\n",
      "epoch:18 step:17351 [D loss: 0.637963, acc: 60.16%] [G loss: 1.833119]\n",
      "epoch:18 step:17352 [D loss: 0.630602, acc: 58.59%] [G loss: 1.813599]\n",
      "epoch:18 step:17353 [D loss: 0.676558, acc: 57.03%] [G loss: 1.895789]\n",
      "epoch:18 step:17354 [D loss: 0.611023, acc: 71.09%] [G loss: 1.939211]\n",
      "epoch:18 step:17355 [D loss: 0.671443, acc: 61.72%] [G loss: 1.832508]\n",
      "epoch:18 step:17356 [D loss: 0.696508, acc: 56.25%] [G loss: 1.788684]\n",
      "epoch:18 step:17357 [D loss: 0.660581, acc: 66.41%] [G loss: 1.946222]\n",
      "epoch:18 step:17358 [D loss: 0.719356, acc: 52.34%] [G loss: 1.766902]\n",
      "epoch:18 step:17359 [D loss: 0.642273, acc: 66.41%] [G loss: 1.785037]\n",
      "epoch:18 step:17360 [D loss: 0.576870, acc: 67.19%] [G loss: 1.985016]\n",
      "epoch:18 step:17361 [D loss: 0.596222, acc: 67.19%] [G loss: 2.056681]\n",
      "epoch:18 step:17362 [D loss: 0.648061, acc: 59.38%] [G loss: 1.886537]\n",
      "epoch:18 step:17363 [D loss: 0.664831, acc: 63.28%] [G loss: 1.908592]\n",
      "epoch:18 step:17364 [D loss: 0.594442, acc: 64.06%] [G loss: 1.952836]\n",
      "epoch:18 step:17365 [D loss: 0.580854, acc: 71.88%] [G loss: 2.110795]\n",
      "epoch:18 step:17366 [D loss: 0.726732, acc: 55.47%] [G loss: 1.800640]\n",
      "epoch:18 step:17367 [D loss: 0.702344, acc: 61.72%] [G loss: 1.807465]\n",
      "epoch:18 step:17368 [D loss: 0.706151, acc: 51.56%] [G loss: 1.707320]\n",
      "epoch:18 step:17369 [D loss: 0.612827, acc: 67.19%] [G loss: 1.917533]\n",
      "epoch:18 step:17370 [D loss: 0.676688, acc: 53.91%] [G loss: 2.025064]\n",
      "epoch:18 step:17371 [D loss: 0.630189, acc: 68.75%] [G loss: 1.911632]\n",
      "epoch:18 step:17372 [D loss: 0.654044, acc: 60.94%] [G loss: 1.821738]\n",
      "epoch:18 step:17373 [D loss: 0.654903, acc: 64.84%] [G loss: 1.941067]\n",
      "epoch:18 step:17374 [D loss: 0.601794, acc: 64.84%] [G loss: 2.067697]\n",
      "epoch:18 step:17375 [D loss: 0.684722, acc: 56.25%] [G loss: 1.836209]\n",
      "epoch:18 step:17376 [D loss: 0.678115, acc: 62.50%] [G loss: 1.795248]\n",
      "epoch:18 step:17377 [D loss: 0.684121, acc: 54.69%] [G loss: 1.711972]\n",
      "epoch:18 step:17378 [D loss: 0.631808, acc: 63.28%] [G loss: 1.903300]\n",
      "epoch:18 step:17379 [D loss: 0.620828, acc: 65.62%] [G loss: 1.883811]\n",
      "epoch:18 step:17380 [D loss: 0.664690, acc: 57.03%] [G loss: 1.842524]\n",
      "epoch:18 step:17381 [D loss: 0.602950, acc: 67.97%] [G loss: 1.897826]\n",
      "epoch:18 step:17382 [D loss: 0.672962, acc: 66.41%] [G loss: 1.956587]\n",
      "epoch:18 step:17383 [D loss: 0.643540, acc: 64.84%] [G loss: 1.986505]\n",
      "epoch:18 step:17384 [D loss: 0.688418, acc: 60.94%] [G loss: 1.775660]\n",
      "epoch:18 step:17385 [D loss: 0.624927, acc: 62.50%] [G loss: 1.936517]\n",
      "epoch:18 step:17386 [D loss: 0.634952, acc: 65.62%] [G loss: 1.895175]\n",
      "epoch:18 step:17387 [D loss: 0.630485, acc: 64.06%] [G loss: 1.889868]\n",
      "epoch:18 step:17388 [D loss: 0.596588, acc: 70.31%] [G loss: 2.029643]\n",
      "epoch:18 step:17389 [D loss: 0.642483, acc: 62.50%] [G loss: 1.994941]\n",
      "epoch:18 step:17390 [D loss: 0.617037, acc: 64.06%] [G loss: 1.856143]\n",
      "epoch:18 step:17391 [D loss: 0.686593, acc: 56.25%] [G loss: 1.813069]\n",
      "epoch:18 step:17392 [D loss: 0.624160, acc: 66.41%] [G loss: 1.926659]\n",
      "epoch:18 step:17393 [D loss: 0.662596, acc: 59.38%] [G loss: 1.866776]\n",
      "epoch:18 step:17394 [D loss: 0.692485, acc: 53.12%] [G loss: 1.720955]\n",
      "epoch:18 step:17395 [D loss: 0.668597, acc: 57.03%] [G loss: 1.656696]\n",
      "epoch:18 step:17396 [D loss: 0.702274, acc: 59.38%] [G loss: 1.692650]\n",
      "epoch:18 step:17397 [D loss: 0.679861, acc: 55.47%] [G loss: 1.698028]\n",
      "epoch:18 step:17398 [D loss: 0.617497, acc: 65.62%] [G loss: 1.871822]\n",
      "epoch:18 step:17399 [D loss: 0.585828, acc: 71.09%] [G loss: 1.980515]\n",
      "epoch:18 step:17400 [D loss: 0.621939, acc: 65.62%] [G loss: 2.011024]\n",
      "##############\n",
      "[2.30866798 1.63599662 6.26187062 4.86430997 3.61367632 5.69852761\n",
      " 4.28433394 4.6859982  4.50934442 3.57820898]\n",
      "##########\n",
      "epoch:18 step:17401 [D loss: 0.649595, acc: 62.50%] [G loss: 1.916200]\n",
      "epoch:18 step:17402 [D loss: 0.625649, acc: 65.62%] [G loss: 1.932641]\n",
      "epoch:18 step:17403 [D loss: 0.639803, acc: 64.06%] [G loss: 1.807625]\n",
      "epoch:18 step:17404 [D loss: 0.685429, acc: 58.59%] [G loss: 1.809832]\n",
      "epoch:18 step:17405 [D loss: 0.647243, acc: 62.50%] [G loss: 1.942722]\n",
      "epoch:18 step:17406 [D loss: 0.670120, acc: 65.62%] [G loss: 1.932784]\n",
      "epoch:18 step:17407 [D loss: 0.639651, acc: 64.06%] [G loss: 1.901813]\n",
      "epoch:18 step:17408 [D loss: 0.725391, acc: 48.44%] [G loss: 1.816370]\n",
      "epoch:18 step:17409 [D loss: 0.629807, acc: 67.97%] [G loss: 2.063209]\n",
      "epoch:18 step:17410 [D loss: 0.638266, acc: 61.72%] [G loss: 1.845378]\n",
      "epoch:18 step:17411 [D loss: 0.668002, acc: 62.50%] [G loss: 1.934099]\n",
      "epoch:18 step:17412 [D loss: 0.647162, acc: 63.28%] [G loss: 1.929084]\n",
      "epoch:18 step:17413 [D loss: 0.668804, acc: 62.50%] [G loss: 1.979531]\n",
      "epoch:18 step:17414 [D loss: 0.622113, acc: 64.84%] [G loss: 1.876276]\n",
      "epoch:18 step:17415 [D loss: 0.552312, acc: 73.44%] [G loss: 2.064573]\n",
      "epoch:18 step:17416 [D loss: 0.595621, acc: 71.88%] [G loss: 1.922075]\n",
      "epoch:18 step:17417 [D loss: 0.615379, acc: 63.28%] [G loss: 2.024572]\n",
      "epoch:18 step:17418 [D loss: 0.574695, acc: 73.44%] [G loss: 1.883412]\n",
      "epoch:18 step:17419 [D loss: 0.665487, acc: 59.38%] [G loss: 1.895835]\n",
      "epoch:18 step:17420 [D loss: 0.623416, acc: 64.84%] [G loss: 2.114274]\n",
      "epoch:18 step:17421 [D loss: 0.680251, acc: 57.81%] [G loss: 1.856017]\n",
      "epoch:18 step:17422 [D loss: 0.679941, acc: 58.59%] [G loss: 1.972437]\n",
      "epoch:18 step:17423 [D loss: 0.607596, acc: 67.97%] [G loss: 2.003983]\n",
      "epoch:18 step:17424 [D loss: 0.649813, acc: 61.72%] [G loss: 1.932242]\n",
      "epoch:18 step:17425 [D loss: 0.676625, acc: 58.59%] [G loss: 1.834936]\n",
      "epoch:18 step:17426 [D loss: 0.630750, acc: 63.28%] [G loss: 1.763077]\n",
      "epoch:18 step:17427 [D loss: 0.644565, acc: 63.28%] [G loss: 1.917370]\n",
      "epoch:18 step:17428 [D loss: 0.657311, acc: 60.94%] [G loss: 1.939012]\n",
      "epoch:18 step:17429 [D loss: 0.615368, acc: 66.41%] [G loss: 2.124172]\n",
      "epoch:18 step:17430 [D loss: 0.588447, acc: 69.53%] [G loss: 2.019894]\n",
      "epoch:18 step:17431 [D loss: 0.618291, acc: 68.75%] [G loss: 1.809634]\n",
      "epoch:18 step:17432 [D loss: 0.704438, acc: 52.34%] [G loss: 1.762806]\n",
      "epoch:18 step:17433 [D loss: 0.664865, acc: 55.47%] [G loss: 1.790393]\n",
      "epoch:18 step:17434 [D loss: 0.633072, acc: 62.50%] [G loss: 1.977713]\n",
      "epoch:18 step:17435 [D loss: 0.668218, acc: 59.38%] [G loss: 1.855180]\n",
      "epoch:18 step:17436 [D loss: 0.631217, acc: 61.72%] [G loss: 2.053397]\n",
      "epoch:18 step:17437 [D loss: 0.588126, acc: 72.66%] [G loss: 2.065763]\n",
      "epoch:18 step:17438 [D loss: 0.667734, acc: 61.72%] [G loss: 1.824669]\n",
      "epoch:18 step:17439 [D loss: 0.634192, acc: 63.28%] [G loss: 1.714465]\n",
      "epoch:18 step:17440 [D loss: 0.603299, acc: 68.75%] [G loss: 2.035213]\n",
      "epoch:18 step:17441 [D loss: 0.660307, acc: 61.72%] [G loss: 1.919358]\n",
      "epoch:18 step:17442 [D loss: 0.645076, acc: 64.84%] [G loss: 1.725068]\n",
      "epoch:18 step:17443 [D loss: 0.626529, acc: 70.31%] [G loss: 1.734741]\n",
      "epoch:18 step:17444 [D loss: 0.616081, acc: 67.19%] [G loss: 1.835374]\n",
      "epoch:18 step:17445 [D loss: 0.655222, acc: 61.72%] [G loss: 1.778112]\n",
      "epoch:18 step:17446 [D loss: 0.644002, acc: 60.16%] [G loss: 1.944424]\n",
      "epoch:18 step:17447 [D loss: 0.589605, acc: 67.97%] [G loss: 1.888821]\n",
      "epoch:18 step:17448 [D loss: 0.610152, acc: 71.88%] [G loss: 1.939134]\n",
      "epoch:18 step:17449 [D loss: 0.661722, acc: 56.25%] [G loss: 1.881242]\n",
      "epoch:18 step:17450 [D loss: 0.662028, acc: 65.62%] [G loss: 1.825236]\n",
      "epoch:18 step:17451 [D loss: 0.628974, acc: 60.94%] [G loss: 1.915461]\n",
      "epoch:18 step:17452 [D loss: 0.710205, acc: 51.56%] [G loss: 1.878904]\n",
      "epoch:18 step:17453 [D loss: 0.643088, acc: 67.19%] [G loss: 1.846912]\n",
      "epoch:18 step:17454 [D loss: 0.622186, acc: 66.41%] [G loss: 2.036884]\n",
      "epoch:18 step:17455 [D loss: 0.663499, acc: 64.84%] [G loss: 1.951385]\n",
      "epoch:18 step:17456 [D loss: 0.628388, acc: 60.94%] [G loss: 1.692670]\n",
      "epoch:18 step:17457 [D loss: 0.634933, acc: 61.72%] [G loss: 1.838509]\n",
      "epoch:18 step:17458 [D loss: 0.670525, acc: 60.94%] [G loss: 1.881577]\n",
      "epoch:18 step:17459 [D loss: 0.627338, acc: 64.06%] [G loss: 1.897989]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:18 step:17460 [D loss: 0.646643, acc: 60.16%] [G loss: 1.794824]\n",
      "epoch:18 step:17461 [D loss: 0.643898, acc: 62.50%] [G loss: 1.941363]\n",
      "epoch:18 step:17462 [D loss: 0.669379, acc: 62.50%] [G loss: 1.845439]\n",
      "epoch:18 step:17463 [D loss: 0.679034, acc: 58.59%] [G loss: 1.781032]\n",
      "epoch:18 step:17464 [D loss: 0.618156, acc: 68.75%] [G loss: 1.963369]\n",
      "epoch:18 step:17465 [D loss: 0.667156, acc: 61.72%] [G loss: 1.824265]\n",
      "epoch:18 step:17466 [D loss: 0.632057, acc: 64.06%] [G loss: 1.862704]\n",
      "epoch:18 step:17467 [D loss: 0.677636, acc: 56.25%] [G loss: 1.892941]\n",
      "epoch:18 step:17468 [D loss: 0.613436, acc: 64.06%] [G loss: 1.838789]\n",
      "epoch:18 step:17469 [D loss: 0.652403, acc: 62.50%] [G loss: 1.985947]\n",
      "epoch:18 step:17470 [D loss: 0.634425, acc: 64.06%] [G loss: 1.970548]\n",
      "epoch:18 step:17471 [D loss: 0.620753, acc: 64.84%] [G loss: 2.116676]\n",
      "epoch:18 step:17472 [D loss: 0.638331, acc: 66.41%] [G loss: 1.788020]\n",
      "epoch:18 step:17473 [D loss: 0.608913, acc: 67.19%] [G loss: 2.031789]\n",
      "epoch:18 step:17474 [D loss: 0.692340, acc: 57.81%] [G loss: 1.817469]\n",
      "epoch:18 step:17475 [D loss: 0.666935, acc: 57.81%] [G loss: 1.820892]\n",
      "epoch:18 step:17476 [D loss: 0.633710, acc: 63.28%] [G loss: 1.902673]\n",
      "epoch:18 step:17477 [D loss: 0.621320, acc: 66.41%] [G loss: 1.778995]\n",
      "epoch:18 step:17478 [D loss: 0.686027, acc: 50.00%] [G loss: 1.868190]\n",
      "epoch:18 step:17479 [D loss: 0.630663, acc: 60.16%] [G loss: 2.040214]\n",
      "epoch:18 step:17480 [D loss: 0.701761, acc: 54.69%] [G loss: 1.782937]\n",
      "epoch:18 step:17481 [D loss: 0.659304, acc: 60.16%] [G loss: 1.813857]\n",
      "epoch:18 step:17482 [D loss: 0.633911, acc: 64.84%] [G loss: 1.773585]\n",
      "epoch:18 step:17483 [D loss: 0.608247, acc: 67.19%] [G loss: 1.888603]\n",
      "epoch:18 step:17484 [D loss: 0.650805, acc: 64.06%] [G loss: 1.953353]\n",
      "epoch:18 step:17485 [D loss: 0.607924, acc: 67.19%] [G loss: 1.792106]\n",
      "epoch:18 step:17486 [D loss: 0.689264, acc: 60.16%] [G loss: 1.799950]\n",
      "epoch:18 step:17487 [D loss: 0.642290, acc: 67.97%] [G loss: 1.911031]\n",
      "epoch:18 step:17488 [D loss: 0.642083, acc: 61.72%] [G loss: 2.067632]\n",
      "epoch:18 step:17489 [D loss: 0.591223, acc: 69.53%] [G loss: 1.914781]\n",
      "epoch:18 step:17490 [D loss: 0.588890, acc: 74.22%] [G loss: 2.147591]\n",
      "epoch:18 step:17491 [D loss: 0.682291, acc: 57.03%] [G loss: 1.811444]\n",
      "epoch:18 step:17492 [D loss: 0.639027, acc: 60.94%] [G loss: 1.983485]\n",
      "epoch:18 step:17493 [D loss: 0.662538, acc: 61.72%] [G loss: 1.911311]\n",
      "epoch:18 step:17494 [D loss: 0.667826, acc: 60.94%] [G loss: 1.776797]\n",
      "epoch:18 step:17495 [D loss: 0.622777, acc: 67.19%] [G loss: 1.798397]\n",
      "epoch:18 step:17496 [D loss: 0.622655, acc: 69.53%] [G loss: 1.879457]\n",
      "epoch:18 step:17497 [D loss: 0.615930, acc: 66.41%] [G loss: 1.991420]\n",
      "epoch:18 step:17498 [D loss: 0.600696, acc: 68.75%] [G loss: 2.052524]\n",
      "epoch:18 step:17499 [D loss: 0.603596, acc: 69.53%] [G loss: 1.936935]\n",
      "epoch:18 step:17500 [D loss: 0.639527, acc: 66.41%] [G loss: 1.999837]\n",
      "epoch:18 step:17501 [D loss: 0.667743, acc: 60.94%] [G loss: 1.995290]\n",
      "epoch:18 step:17502 [D loss: 0.619818, acc: 63.28%] [G loss: 1.815980]\n",
      "epoch:18 step:17503 [D loss: 0.604673, acc: 61.72%] [G loss: 2.017163]\n",
      "epoch:18 step:17504 [D loss: 0.705399, acc: 59.38%] [G loss: 2.001081]\n",
      "epoch:18 step:17505 [D loss: 0.678897, acc: 58.59%] [G loss: 1.933124]\n",
      "epoch:18 step:17506 [D loss: 0.655856, acc: 63.28%] [G loss: 1.930367]\n",
      "epoch:18 step:17507 [D loss: 0.624725, acc: 66.41%] [G loss: 1.922485]\n",
      "epoch:18 step:17508 [D loss: 0.620148, acc: 67.97%] [G loss: 2.068673]\n",
      "epoch:18 step:17509 [D loss: 0.660252, acc: 58.59%] [G loss: 1.984986]\n",
      "epoch:18 step:17510 [D loss: 0.674560, acc: 60.16%] [G loss: 2.001797]\n",
      "epoch:18 step:17511 [D loss: 0.681352, acc: 60.16%] [G loss: 1.864081]\n",
      "epoch:18 step:17512 [D loss: 0.705493, acc: 56.25%] [G loss: 1.947545]\n",
      "epoch:18 step:17513 [D loss: 0.624928, acc: 61.72%] [G loss: 2.176714]\n",
      "epoch:18 step:17514 [D loss: 0.618352, acc: 67.19%] [G loss: 2.226090]\n",
      "epoch:18 step:17515 [D loss: 0.607825, acc: 69.53%] [G loss: 2.054334]\n",
      "epoch:18 step:17516 [D loss: 0.537385, acc: 75.78%] [G loss: 2.254469]\n",
      "epoch:18 step:17517 [D loss: 0.652356, acc: 63.28%] [G loss: 2.072683]\n",
      "epoch:18 step:17518 [D loss: 0.652564, acc: 60.16%] [G loss: 1.940011]\n",
      "epoch:18 step:17519 [D loss: 0.612361, acc: 67.97%] [G loss: 2.210608]\n",
      "epoch:18 step:17520 [D loss: 0.642356, acc: 62.50%] [G loss: 2.025480]\n",
      "epoch:18 step:17521 [D loss: 0.652682, acc: 64.06%] [G loss: 1.859397]\n",
      "epoch:18 step:17522 [D loss: 0.644693, acc: 65.62%] [G loss: 1.769274]\n",
      "epoch:18 step:17523 [D loss: 0.693346, acc: 61.72%] [G loss: 1.767573]\n",
      "epoch:18 step:17524 [D loss: 0.681208, acc: 57.03%] [G loss: 1.780288]\n",
      "epoch:18 step:17525 [D loss: 0.672109, acc: 60.94%] [G loss: 1.906771]\n",
      "epoch:18 step:17526 [D loss: 0.618609, acc: 67.97%] [G loss: 1.815314]\n",
      "epoch:18 step:17527 [D loss: 0.645533, acc: 65.62%] [G loss: 1.862086]\n",
      "epoch:18 step:17528 [D loss: 0.688763, acc: 58.59%] [G loss: 1.893910]\n",
      "epoch:18 step:17529 [D loss: 0.693450, acc: 55.47%] [G loss: 1.814347]\n",
      "epoch:18 step:17530 [D loss: 0.655561, acc: 53.91%] [G loss: 1.998890]\n",
      "epoch:18 step:17531 [D loss: 0.644862, acc: 61.72%] [G loss: 1.860553]\n",
      "epoch:18 step:17532 [D loss: 0.647412, acc: 60.16%] [G loss: 1.875592]\n",
      "epoch:18 step:17533 [D loss: 0.701383, acc: 53.91%] [G loss: 1.674128]\n",
      "epoch:18 step:17534 [D loss: 0.688271, acc: 53.91%] [G loss: 1.901299]\n",
      "epoch:18 step:17535 [D loss: 0.704291, acc: 51.56%] [G loss: 1.781415]\n",
      "epoch:18 step:17536 [D loss: 0.611494, acc: 67.97%] [G loss: 1.886938]\n",
      "epoch:18 step:17537 [D loss: 0.658071, acc: 56.25%] [G loss: 1.866664]\n",
      "epoch:18 step:17538 [D loss: 0.631757, acc: 59.38%] [G loss: 1.865907]\n",
      "epoch:18 step:17539 [D loss: 0.668404, acc: 59.38%] [G loss: 1.841149]\n",
      "epoch:18 step:17540 [D loss: 0.658708, acc: 63.28%] [G loss: 1.843636]\n",
      "epoch:18 step:17541 [D loss: 0.654076, acc: 64.06%] [G loss: 1.802966]\n",
      "epoch:18 step:17542 [D loss: 0.626604, acc: 60.16%] [G loss: 1.805304]\n",
      "epoch:18 step:17543 [D loss: 0.604864, acc: 65.62%] [G loss: 1.889776]\n",
      "epoch:18 step:17544 [D loss: 0.649221, acc: 61.72%] [G loss: 1.843730]\n",
      "epoch:18 step:17545 [D loss: 0.636599, acc: 64.06%] [G loss: 1.886104]\n",
      "epoch:18 step:17546 [D loss: 0.618584, acc: 57.81%] [G loss: 1.967389]\n",
      "epoch:18 step:17547 [D loss: 0.607136, acc: 67.97%] [G loss: 2.085328]\n",
      "epoch:18 step:17548 [D loss: 0.590490, acc: 65.62%] [G loss: 1.886340]\n",
      "epoch:18 step:17549 [D loss: 0.656222, acc: 64.06%] [G loss: 1.770322]\n",
      "epoch:18 step:17550 [D loss: 0.658566, acc: 56.25%] [G loss: 1.868679]\n",
      "epoch:18 step:17551 [D loss: 0.648565, acc: 63.28%] [G loss: 1.958084]\n",
      "epoch:18 step:17552 [D loss: 0.668348, acc: 60.94%] [G loss: 1.812907]\n",
      "epoch:18 step:17553 [D loss: 0.634920, acc: 63.28%] [G loss: 1.801340]\n",
      "epoch:18 step:17554 [D loss: 0.663134, acc: 58.59%] [G loss: 1.890325]\n",
      "epoch:18 step:17555 [D loss: 0.682954, acc: 59.38%] [G loss: 1.813132]\n",
      "epoch:18 step:17556 [D loss: 0.664685, acc: 56.25%] [G loss: 1.889105]\n",
      "epoch:18 step:17557 [D loss: 0.614554, acc: 66.41%] [G loss: 2.074550]\n",
      "epoch:18 step:17558 [D loss: 0.624353, acc: 64.84%] [G loss: 1.951241]\n",
      "epoch:18 step:17559 [D loss: 0.637555, acc: 63.28%] [G loss: 1.972414]\n",
      "epoch:18 step:17560 [D loss: 0.587560, acc: 72.66%] [G loss: 2.229941]\n",
      "epoch:18 step:17561 [D loss: 0.622232, acc: 63.28%] [G loss: 2.059103]\n",
      "epoch:18 step:17562 [D loss: 0.641801, acc: 60.94%] [G loss: 1.781058]\n",
      "epoch:18 step:17563 [D loss: 0.658566, acc: 60.16%] [G loss: 1.787578]\n",
      "epoch:18 step:17564 [D loss: 0.671162, acc: 57.81%] [G loss: 1.821129]\n",
      "epoch:18 step:17565 [D loss: 0.591864, acc: 73.44%] [G loss: 1.990874]\n",
      "epoch:18 step:17566 [D loss: 0.666662, acc: 60.94%] [G loss: 1.760833]\n",
      "epoch:18 step:17567 [D loss: 0.602756, acc: 66.41%] [G loss: 1.916442]\n",
      "epoch:18 step:17568 [D loss: 0.684140, acc: 59.38%] [G loss: 1.791740]\n",
      "epoch:18 step:17569 [D loss: 0.687935, acc: 56.25%] [G loss: 1.805390]\n",
      "epoch:18 step:17570 [D loss: 0.705414, acc: 55.47%] [G loss: 1.725563]\n",
      "epoch:18 step:17571 [D loss: 0.631883, acc: 64.84%] [G loss: 1.981990]\n",
      "epoch:18 step:17572 [D loss: 0.637117, acc: 61.72%] [G loss: 1.913034]\n",
      "epoch:18 step:17573 [D loss: 0.562800, acc: 75.78%] [G loss: 1.937495]\n",
      "epoch:18 step:17574 [D loss: 0.573493, acc: 67.97%] [G loss: 2.074031]\n",
      "epoch:18 step:17575 [D loss: 0.632050, acc: 64.84%] [G loss: 1.955670]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:18 step:17576 [D loss: 0.661396, acc: 62.50%] [G loss: 1.754708]\n",
      "epoch:18 step:17577 [D loss: 0.660912, acc: 60.16%] [G loss: 2.031086]\n",
      "epoch:18 step:17578 [D loss: 0.689430, acc: 57.81%] [G loss: 1.926644]\n",
      "epoch:18 step:17579 [D loss: 0.625132, acc: 67.97%] [G loss: 1.904773]\n",
      "epoch:18 step:17580 [D loss: 0.650390, acc: 63.28%] [G loss: 1.922390]\n",
      "epoch:18 step:17581 [D loss: 0.670468, acc: 60.94%] [G loss: 1.938258]\n",
      "epoch:18 step:17582 [D loss: 0.652112, acc: 63.28%] [G loss: 1.783899]\n",
      "epoch:18 step:17583 [D loss: 0.584951, acc: 68.75%] [G loss: 2.001793]\n",
      "epoch:18 step:17584 [D loss: 0.614287, acc: 64.84%] [G loss: 1.955481]\n",
      "epoch:18 step:17585 [D loss: 0.631622, acc: 64.84%] [G loss: 1.991524]\n",
      "epoch:18 step:17586 [D loss: 0.646599, acc: 63.28%] [G loss: 2.035566]\n",
      "epoch:18 step:17587 [D loss: 0.675069, acc: 52.34%] [G loss: 1.929390]\n",
      "epoch:18 step:17588 [D loss: 0.626831, acc: 63.28%] [G loss: 1.876069]\n",
      "epoch:18 step:17589 [D loss: 0.656157, acc: 60.94%] [G loss: 1.842749]\n",
      "epoch:18 step:17590 [D loss: 0.624053, acc: 63.28%] [G loss: 2.071154]\n",
      "epoch:18 step:17591 [D loss: 0.609733, acc: 68.75%] [G loss: 1.858479]\n",
      "epoch:18 step:17592 [D loss: 0.665629, acc: 62.50%] [G loss: 2.022968]\n",
      "epoch:18 step:17593 [D loss: 0.665196, acc: 62.50%] [G loss: 1.917695]\n",
      "epoch:18 step:17594 [D loss: 0.672841, acc: 57.03%] [G loss: 1.847468]\n",
      "epoch:18 step:17595 [D loss: 0.645513, acc: 60.94%] [G loss: 1.853156]\n",
      "epoch:18 step:17596 [D loss: 0.639273, acc: 67.19%] [G loss: 1.892426]\n",
      "epoch:18 step:17597 [D loss: 0.656686, acc: 60.16%] [G loss: 1.873473]\n",
      "epoch:18 step:17598 [D loss: 0.646525, acc: 53.91%] [G loss: 1.841676]\n",
      "epoch:18 step:17599 [D loss: 0.639327, acc: 64.06%] [G loss: 1.948438]\n",
      "epoch:18 step:17600 [D loss: 0.576914, acc: 75.00%] [G loss: 2.011766]\n",
      "##############\n",
      "[2.43002451 1.47105492 6.30840362 4.79543587 3.6709247  5.69265352\n",
      " 4.30109252 4.81689655 4.70547076 3.83398436]\n",
      "##########\n",
      "epoch:18 step:17601 [D loss: 0.659194, acc: 60.94%] [G loss: 2.073484]\n",
      "epoch:18 step:17602 [D loss: 0.674750, acc: 59.38%] [G loss: 1.882488]\n",
      "epoch:18 step:17603 [D loss: 0.688467, acc: 59.38%] [G loss: 1.918107]\n",
      "epoch:18 step:17604 [D loss: 0.623762, acc: 64.06%] [G loss: 1.984409]\n",
      "epoch:18 step:17605 [D loss: 0.634549, acc: 62.50%] [G loss: 1.883484]\n",
      "epoch:18 step:17606 [D loss: 0.590451, acc: 74.22%] [G loss: 1.883851]\n",
      "epoch:18 step:17607 [D loss: 0.673924, acc: 63.28%] [G loss: 1.883706]\n",
      "epoch:18 step:17608 [D loss: 0.645801, acc: 66.41%] [G loss: 1.851871]\n",
      "epoch:18 step:17609 [D loss: 0.664299, acc: 57.81%] [G loss: 1.992820]\n",
      "epoch:18 step:17610 [D loss: 0.631434, acc: 69.53%] [G loss: 1.906437]\n",
      "epoch:18 step:17611 [D loss: 0.659399, acc: 53.91%] [G loss: 1.924922]\n",
      "epoch:18 step:17612 [D loss: 0.598734, acc: 72.66%] [G loss: 2.123339]\n",
      "epoch:18 step:17613 [D loss: 0.656965, acc: 64.84%] [G loss: 1.916952]\n",
      "epoch:18 step:17614 [D loss: 0.656943, acc: 63.28%] [G loss: 1.915992]\n",
      "epoch:18 step:17615 [D loss: 0.678757, acc: 57.81%] [G loss: 1.921310]\n",
      "epoch:18 step:17616 [D loss: 0.626543, acc: 64.06%] [G loss: 1.807582]\n",
      "epoch:18 step:17617 [D loss: 0.621304, acc: 64.84%] [G loss: 1.931509]\n",
      "epoch:18 step:17618 [D loss: 0.643729, acc: 60.16%] [G loss: 1.798476]\n",
      "epoch:18 step:17619 [D loss: 0.670084, acc: 55.47%] [G loss: 2.002846]\n",
      "epoch:18 step:17620 [D loss: 0.701680, acc: 58.59%] [G loss: 1.938830]\n",
      "epoch:18 step:17621 [D loss: 0.648790, acc: 64.06%] [G loss: 1.898266]\n",
      "epoch:18 step:17622 [D loss: 0.613954, acc: 64.06%] [G loss: 1.845588]\n",
      "epoch:18 step:17623 [D loss: 0.674325, acc: 56.25%] [G loss: 1.860601]\n",
      "epoch:18 step:17624 [D loss: 0.631584, acc: 61.72%] [G loss: 1.737679]\n",
      "epoch:18 step:17625 [D loss: 0.690242, acc: 53.12%] [G loss: 1.735154]\n",
      "epoch:18 step:17626 [D loss: 0.623552, acc: 67.19%] [G loss: 1.723893]\n",
      "epoch:18 step:17627 [D loss: 0.654390, acc: 62.50%] [G loss: 1.865997]\n",
      "epoch:18 step:17628 [D loss: 0.691921, acc: 55.47%] [G loss: 1.808721]\n",
      "epoch:18 step:17629 [D loss: 0.633087, acc: 66.41%] [G loss: 1.932953]\n",
      "epoch:18 step:17630 [D loss: 0.670698, acc: 55.47%] [G loss: 1.834105]\n",
      "epoch:18 step:17631 [D loss: 0.718799, acc: 55.47%] [G loss: 1.687927]\n",
      "epoch:18 step:17632 [D loss: 0.672692, acc: 62.50%] [G loss: 1.828648]\n",
      "epoch:18 step:17633 [D loss: 0.679181, acc: 61.72%] [G loss: 1.845984]\n",
      "epoch:18 step:17634 [D loss: 0.674715, acc: 57.81%] [G loss: 1.874098]\n",
      "epoch:18 step:17635 [D loss: 0.655657, acc: 60.94%] [G loss: 1.810464]\n",
      "epoch:18 step:17636 [D loss: 0.628816, acc: 64.84%] [G loss: 1.831730]\n",
      "epoch:18 step:17637 [D loss: 0.664444, acc: 62.50%] [G loss: 1.766145]\n",
      "epoch:18 step:17638 [D loss: 0.644356, acc: 64.06%] [G loss: 1.962850]\n",
      "epoch:18 step:17639 [D loss: 0.641946, acc: 60.94%] [G loss: 1.881452]\n",
      "epoch:18 step:17640 [D loss: 0.633540, acc: 71.09%] [G loss: 1.984409]\n",
      "epoch:18 step:17641 [D loss: 0.650390, acc: 61.72%] [G loss: 1.922219]\n",
      "epoch:18 step:17642 [D loss: 0.665288, acc: 59.38%] [G loss: 1.924552]\n",
      "epoch:18 step:17643 [D loss: 0.650181, acc: 59.38%] [G loss: 1.891539]\n",
      "epoch:18 step:17644 [D loss: 0.676838, acc: 59.38%] [G loss: 1.924681]\n",
      "epoch:18 step:17645 [D loss: 0.693071, acc: 54.69%] [G loss: 1.806006]\n",
      "epoch:18 step:17646 [D loss: 0.687832, acc: 54.69%] [G loss: 1.986121]\n",
      "epoch:18 step:17647 [D loss: 0.657806, acc: 64.84%] [G loss: 2.014696]\n",
      "epoch:18 step:17648 [D loss: 0.644447, acc: 63.28%] [G loss: 1.994490]\n",
      "epoch:18 step:17649 [D loss: 0.647967, acc: 66.41%] [G loss: 1.940881]\n",
      "epoch:18 step:17650 [D loss: 0.734891, acc: 57.03%] [G loss: 1.716238]\n",
      "epoch:18 step:17651 [D loss: 0.646677, acc: 64.06%] [G loss: 1.864470]\n",
      "epoch:18 step:17652 [D loss: 0.639876, acc: 64.06%] [G loss: 2.014322]\n",
      "epoch:18 step:17653 [D loss: 0.649685, acc: 58.59%] [G loss: 1.867384]\n",
      "epoch:18 step:17654 [D loss: 0.709600, acc: 56.25%] [G loss: 1.740382]\n",
      "epoch:18 step:17655 [D loss: 0.643200, acc: 64.84%] [G loss: 1.841262]\n",
      "epoch:18 step:17656 [D loss: 0.654978, acc: 64.06%] [G loss: 1.824965]\n",
      "epoch:18 step:17657 [D loss: 0.643491, acc: 61.72%] [G loss: 1.901553]\n",
      "epoch:18 step:17658 [D loss: 0.652900, acc: 59.38%] [G loss: 1.992565]\n",
      "epoch:18 step:17659 [D loss: 0.627614, acc: 62.50%] [G loss: 1.976267]\n",
      "epoch:18 step:17660 [D loss: 0.681756, acc: 57.81%] [G loss: 1.910053]\n",
      "epoch:18 step:17661 [D loss: 0.665481, acc: 59.38%] [G loss: 1.785175]\n",
      "epoch:18 step:17662 [D loss: 0.656477, acc: 58.59%] [G loss: 1.808328]\n",
      "epoch:18 step:17663 [D loss: 0.642051, acc: 62.50%] [G loss: 1.817233]\n",
      "epoch:18 step:17664 [D loss: 0.610832, acc: 67.19%] [G loss: 1.760971]\n",
      "epoch:18 step:17665 [D loss: 0.639461, acc: 60.94%] [G loss: 1.744696]\n",
      "epoch:18 step:17666 [D loss: 0.732177, acc: 54.69%] [G loss: 1.683122]\n",
      "epoch:18 step:17667 [D loss: 0.671461, acc: 57.03%] [G loss: 1.713272]\n",
      "epoch:18 step:17668 [D loss: 0.651451, acc: 63.28%] [G loss: 1.885685]\n",
      "epoch:18 step:17669 [D loss: 0.656517, acc: 63.28%] [G loss: 1.786784]\n",
      "epoch:18 step:17670 [D loss: 0.571045, acc: 70.31%] [G loss: 1.939708]\n",
      "epoch:18 step:17671 [D loss: 0.624959, acc: 64.06%] [G loss: 1.884381]\n",
      "epoch:18 step:17672 [D loss: 0.607987, acc: 72.66%] [G loss: 1.916819]\n",
      "epoch:18 step:17673 [D loss: 0.619439, acc: 68.75%] [G loss: 1.823793]\n",
      "epoch:18 step:17674 [D loss: 0.631993, acc: 66.41%] [G loss: 1.910246]\n",
      "epoch:18 step:17675 [D loss: 0.619403, acc: 67.97%] [G loss: 1.911777]\n",
      "epoch:18 step:17676 [D loss: 0.629400, acc: 63.28%] [G loss: 1.907857]\n",
      "epoch:18 step:17677 [D loss: 0.630145, acc: 65.62%] [G loss: 1.943183]\n",
      "epoch:18 step:17678 [D loss: 0.626443, acc: 64.84%] [G loss: 1.868629]\n",
      "epoch:18 step:17679 [D loss: 0.687436, acc: 57.03%] [G loss: 1.992495]\n",
      "epoch:18 step:17680 [D loss: 0.624272, acc: 67.97%] [G loss: 2.010330]\n",
      "epoch:18 step:17681 [D loss: 0.625812, acc: 65.62%] [G loss: 2.193775]\n",
      "epoch:18 step:17682 [D loss: 0.634354, acc: 64.06%] [G loss: 2.138931]\n",
      "epoch:18 step:17683 [D loss: 0.698709, acc: 59.38%] [G loss: 1.967013]\n",
      "epoch:18 step:17684 [D loss: 0.638209, acc: 62.50%] [G loss: 1.835779]\n",
      "epoch:18 step:17685 [D loss: 0.580881, acc: 76.56%] [G loss: 1.929186]\n",
      "epoch:18 step:17686 [D loss: 0.729938, acc: 49.22%] [G loss: 1.748403]\n",
      "epoch:18 step:17687 [D loss: 0.662311, acc: 60.94%] [G loss: 1.952188]\n",
      "epoch:18 step:17688 [D loss: 0.661592, acc: 57.03%] [G loss: 1.959296]\n",
      "epoch:18 step:17689 [D loss: 0.617319, acc: 70.31%] [G loss: 1.980796]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:18 step:17690 [D loss: 0.655823, acc: 60.16%] [G loss: 1.894619]\n",
      "epoch:18 step:17691 [D loss: 0.623670, acc: 67.97%] [G loss: 1.964346]\n",
      "epoch:18 step:17692 [D loss: 0.629020, acc: 64.06%] [G loss: 1.903596]\n",
      "epoch:18 step:17693 [D loss: 0.639717, acc: 67.19%] [G loss: 1.774004]\n",
      "epoch:18 step:17694 [D loss: 0.633019, acc: 64.84%] [G loss: 1.757739]\n",
      "epoch:18 step:17695 [D loss: 0.695725, acc: 54.69%] [G loss: 1.721320]\n",
      "epoch:18 step:17696 [D loss: 0.646309, acc: 64.06%] [G loss: 1.846584]\n",
      "epoch:18 step:17697 [D loss: 0.594522, acc: 69.53%] [G loss: 2.041915]\n",
      "epoch:18 step:17698 [D loss: 0.632141, acc: 62.50%] [G loss: 1.933511]\n",
      "epoch:18 step:17699 [D loss: 0.609934, acc: 69.53%] [G loss: 2.004029]\n",
      "epoch:18 step:17700 [D loss: 0.655188, acc: 64.84%] [G loss: 1.856758]\n",
      "epoch:18 step:17701 [D loss: 0.630648, acc: 63.28%] [G loss: 1.871048]\n",
      "epoch:18 step:17702 [D loss: 0.643354, acc: 61.72%] [G loss: 1.895298]\n",
      "epoch:18 step:17703 [D loss: 0.652482, acc: 60.94%] [G loss: 1.759346]\n",
      "epoch:18 step:17704 [D loss: 0.658356, acc: 62.50%] [G loss: 1.943245]\n",
      "epoch:18 step:17705 [D loss: 0.608220, acc: 64.06%] [G loss: 1.874726]\n",
      "epoch:18 step:17706 [D loss: 0.619853, acc: 65.62%] [G loss: 1.936737]\n",
      "epoch:18 step:17707 [D loss: 0.596717, acc: 66.41%] [G loss: 2.030767]\n",
      "epoch:18 step:17708 [D loss: 0.644935, acc: 61.72%] [G loss: 1.958700]\n",
      "epoch:18 step:17709 [D loss: 0.646461, acc: 60.16%] [G loss: 1.843262]\n",
      "epoch:18 step:17710 [D loss: 0.675812, acc: 59.38%] [G loss: 1.845029]\n",
      "epoch:18 step:17711 [D loss: 0.706411, acc: 51.56%] [G loss: 1.980607]\n",
      "epoch:18 step:17712 [D loss: 0.652016, acc: 57.81%] [G loss: 1.822568]\n",
      "epoch:18 step:17713 [D loss: 0.653877, acc: 65.62%] [G loss: 1.812158]\n",
      "epoch:18 step:17714 [D loss: 0.643054, acc: 63.28%] [G loss: 1.901891]\n",
      "epoch:18 step:17715 [D loss: 0.679607, acc: 53.91%] [G loss: 1.839844]\n",
      "epoch:18 step:17716 [D loss: 0.711266, acc: 50.78%] [G loss: 1.841696]\n",
      "epoch:18 step:17717 [D loss: 0.639378, acc: 62.50%] [G loss: 1.739593]\n",
      "epoch:18 step:17718 [D loss: 0.630646, acc: 61.72%] [G loss: 1.854636]\n",
      "epoch:18 step:17719 [D loss: 0.644164, acc: 59.38%] [G loss: 1.919472]\n",
      "epoch:18 step:17720 [D loss: 0.602699, acc: 71.09%] [G loss: 1.935387]\n",
      "epoch:18 step:17721 [D loss: 0.663691, acc: 62.50%] [G loss: 1.768641]\n",
      "epoch:18 step:17722 [D loss: 0.671176, acc: 60.16%] [G loss: 1.794036]\n",
      "epoch:18 step:17723 [D loss: 0.632771, acc: 60.94%] [G loss: 1.831686]\n",
      "epoch:18 step:17724 [D loss: 0.674706, acc: 59.38%] [G loss: 1.826832]\n",
      "epoch:18 step:17725 [D loss: 0.666130, acc: 59.38%] [G loss: 1.862568]\n",
      "epoch:18 step:17726 [D loss: 0.647081, acc: 60.16%] [G loss: 1.979807]\n",
      "epoch:18 step:17727 [D loss: 0.632678, acc: 66.41%] [G loss: 1.858268]\n",
      "epoch:18 step:17728 [D loss: 0.646740, acc: 63.28%] [G loss: 1.881792]\n",
      "epoch:18 step:17729 [D loss: 0.629440, acc: 67.97%] [G loss: 1.844158]\n",
      "epoch:18 step:17730 [D loss: 0.628387, acc: 65.62%] [G loss: 1.846542]\n",
      "epoch:18 step:17731 [D loss: 0.612631, acc: 69.53%] [G loss: 1.693140]\n",
      "epoch:18 step:17732 [D loss: 0.628714, acc: 69.53%] [G loss: 1.950875]\n",
      "epoch:18 step:17733 [D loss: 0.665812, acc: 56.25%] [G loss: 1.700502]\n",
      "epoch:18 step:17734 [D loss: 0.631323, acc: 64.84%] [G loss: 2.029515]\n",
      "epoch:18 step:17735 [D loss: 0.687191, acc: 50.78%] [G loss: 1.829540]\n",
      "epoch:18 step:17736 [D loss: 0.626504, acc: 61.72%] [G loss: 1.839379]\n",
      "epoch:18 step:17737 [D loss: 0.640862, acc: 67.97%] [G loss: 1.829633]\n",
      "epoch:18 step:17738 [D loss: 0.614301, acc: 67.19%] [G loss: 1.833702]\n",
      "epoch:18 step:17739 [D loss: 0.685621, acc: 53.91%] [G loss: 1.669560]\n",
      "epoch:18 step:17740 [D loss: 0.663403, acc: 57.81%] [G loss: 1.816850]\n",
      "epoch:18 step:17741 [D loss: 0.622092, acc: 64.84%] [G loss: 1.971877]\n",
      "epoch:18 step:17742 [D loss: 0.677783, acc: 54.69%] [G loss: 1.889316]\n",
      "epoch:18 step:17743 [D loss: 0.633437, acc: 63.28%] [G loss: 1.861739]\n",
      "epoch:18 step:17744 [D loss: 0.674971, acc: 60.16%] [G loss: 1.946510]\n",
      "epoch:18 step:17745 [D loss: 0.647034, acc: 65.62%] [G loss: 1.821039]\n",
      "epoch:18 step:17746 [D loss: 0.616617, acc: 66.41%] [G loss: 1.719408]\n",
      "epoch:18 step:17747 [D loss: 0.657964, acc: 67.19%] [G loss: 1.878721]\n",
      "epoch:18 step:17748 [D loss: 0.609575, acc: 67.19%] [G loss: 1.958096]\n",
      "epoch:18 step:17749 [D loss: 0.647502, acc: 61.72%] [G loss: 1.879849]\n",
      "epoch:18 step:17750 [D loss: 0.639682, acc: 66.41%] [G loss: 2.083510]\n",
      "epoch:18 step:17751 [D loss: 0.646991, acc: 64.84%] [G loss: 1.878985]\n",
      "epoch:18 step:17752 [D loss: 0.593538, acc: 72.66%] [G loss: 2.091846]\n",
      "epoch:18 step:17753 [D loss: 0.625808, acc: 67.97%] [G loss: 1.818511]\n",
      "epoch:18 step:17754 [D loss: 0.631557, acc: 65.62%] [G loss: 1.917165]\n",
      "epoch:18 step:17755 [D loss: 0.671510, acc: 60.94%] [G loss: 1.881697]\n",
      "epoch:18 step:17756 [D loss: 0.619525, acc: 68.75%] [G loss: 1.996296]\n",
      "epoch:18 step:17757 [D loss: 0.693065, acc: 53.91%] [G loss: 1.846748]\n",
      "epoch:18 step:17758 [D loss: 0.685714, acc: 57.81%] [G loss: 1.909536]\n",
      "epoch:18 step:17759 [D loss: 0.612277, acc: 72.66%] [G loss: 1.967659]\n",
      "epoch:18 step:17760 [D loss: 0.602601, acc: 65.62%] [G loss: 1.943098]\n",
      "epoch:18 step:17761 [D loss: 0.655930, acc: 64.84%] [G loss: 1.842614]\n",
      "epoch:18 step:17762 [D loss: 0.660835, acc: 63.28%] [G loss: 1.919478]\n",
      "epoch:18 step:17763 [D loss: 0.648884, acc: 59.38%] [G loss: 1.981623]\n",
      "epoch:18 step:17764 [D loss: 0.662240, acc: 66.41%] [G loss: 1.957094]\n",
      "epoch:18 step:17765 [D loss: 0.594672, acc: 67.19%] [G loss: 2.200041]\n",
      "epoch:18 step:17766 [D loss: 0.621676, acc: 67.19%] [G loss: 1.962234]\n",
      "epoch:18 step:17767 [D loss: 0.642787, acc: 62.50%] [G loss: 1.894490]\n",
      "epoch:18 step:17768 [D loss: 0.635850, acc: 62.50%] [G loss: 1.968165]\n",
      "epoch:18 step:17769 [D loss: 0.649116, acc: 60.94%] [G loss: 1.880511]\n",
      "epoch:18 step:17770 [D loss: 0.668819, acc: 61.72%] [G loss: 1.905970]\n",
      "epoch:18 step:17771 [D loss: 0.657500, acc: 57.03%] [G loss: 1.983454]\n",
      "epoch:18 step:17772 [D loss: 0.637690, acc: 64.06%] [G loss: 1.957016]\n",
      "epoch:18 step:17773 [D loss: 0.631279, acc: 67.97%] [G loss: 1.976188]\n",
      "epoch:18 step:17774 [D loss: 0.634287, acc: 67.97%] [G loss: 1.955155]\n",
      "epoch:18 step:17775 [D loss: 0.614358, acc: 71.09%] [G loss: 2.051832]\n",
      "epoch:18 step:17776 [D loss: 0.667742, acc: 61.72%] [G loss: 1.960687]\n",
      "epoch:18 step:17777 [D loss: 0.684468, acc: 62.50%] [G loss: 1.925589]\n",
      "epoch:18 step:17778 [D loss: 0.608332, acc: 71.88%] [G loss: 2.144297]\n",
      "epoch:18 step:17779 [D loss: 0.690240, acc: 55.47%] [G loss: 1.885409]\n",
      "epoch:18 step:17780 [D loss: 0.628477, acc: 63.28%] [G loss: 1.993607]\n",
      "epoch:18 step:17781 [D loss: 0.619447, acc: 69.53%] [G loss: 1.915336]\n",
      "epoch:18 step:17782 [D loss: 0.645499, acc: 64.84%] [G loss: 1.997736]\n",
      "epoch:18 step:17783 [D loss: 0.631738, acc: 60.94%] [G loss: 2.115325]\n",
      "epoch:18 step:17784 [D loss: 0.633440, acc: 64.84%] [G loss: 2.210103]\n",
      "epoch:18 step:17785 [D loss: 0.538984, acc: 71.09%] [G loss: 2.162880]\n",
      "epoch:18 step:17786 [D loss: 0.733894, acc: 51.56%] [G loss: 1.785222]\n",
      "epoch:18 step:17787 [D loss: 0.663173, acc: 58.59%] [G loss: 1.917057]\n",
      "epoch:18 step:17788 [D loss: 0.625597, acc: 66.41%] [G loss: 2.041996]\n",
      "epoch:18 step:17789 [D loss: 0.661072, acc: 63.28%] [G loss: 2.158268]\n",
      "epoch:18 step:17790 [D loss: 0.612362, acc: 66.41%] [G loss: 2.104884]\n",
      "epoch:18 step:17791 [D loss: 0.585067, acc: 69.53%] [G loss: 2.208948]\n",
      "epoch:18 step:17792 [D loss: 0.611306, acc: 68.75%] [G loss: 2.176085]\n",
      "epoch:18 step:17793 [D loss: 0.687447, acc: 57.81%] [G loss: 2.022032]\n",
      "epoch:18 step:17794 [D loss: 0.748362, acc: 56.25%] [G loss: 1.764400]\n",
      "epoch:18 step:17795 [D loss: 0.714973, acc: 54.69%] [G loss: 1.913149]\n",
      "epoch:18 step:17796 [D loss: 0.598133, acc: 69.53%] [G loss: 2.049440]\n",
      "epoch:18 step:17797 [D loss: 0.646007, acc: 60.16%] [G loss: 1.893102]\n",
      "epoch:18 step:17798 [D loss: 0.640186, acc: 64.84%] [G loss: 1.915074]\n",
      "epoch:18 step:17799 [D loss: 0.620802, acc: 70.31%] [G loss: 1.938069]\n",
      "epoch:18 step:17800 [D loss: 0.645437, acc: 60.16%] [G loss: 1.900344]\n",
      "##############\n",
      "[2.40081064 1.47129587 6.14387702 4.70795765 3.66855061 5.61489169\n",
      " 4.60256869 4.94201619 4.57885899 3.57577622]\n",
      "##########\n",
      "epoch:18 step:17801 [D loss: 0.639409, acc: 60.94%] [G loss: 1.821463]\n",
      "epoch:18 step:17802 [D loss: 0.604638, acc: 66.41%] [G loss: 2.113902]\n",
      "epoch:18 step:17803 [D loss: 0.624101, acc: 60.16%] [G loss: 2.434858]\n",
      "epoch:19 step:17804 [D loss: 0.654845, acc: 68.75%] [G loss: 1.874219]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:19 step:17805 [D loss: 0.622339, acc: 66.41%] [G loss: 2.008242]\n",
      "epoch:19 step:17806 [D loss: 0.624903, acc: 66.41%] [G loss: 1.976018]\n",
      "epoch:19 step:17807 [D loss: 0.701301, acc: 60.16%] [G loss: 1.904799]\n",
      "epoch:19 step:17808 [D loss: 0.634049, acc: 62.50%] [G loss: 1.966408]\n",
      "epoch:19 step:17809 [D loss: 0.704706, acc: 56.25%] [G loss: 1.940802]\n",
      "epoch:19 step:17810 [D loss: 0.584557, acc: 70.31%] [G loss: 2.122067]\n",
      "epoch:19 step:17811 [D loss: 0.653916, acc: 61.72%] [G loss: 1.986563]\n",
      "epoch:19 step:17812 [D loss: 0.592018, acc: 72.66%] [G loss: 2.016768]\n",
      "epoch:19 step:17813 [D loss: 0.667430, acc: 57.03%] [G loss: 2.035474]\n",
      "epoch:19 step:17814 [D loss: 0.594392, acc: 71.09%] [G loss: 2.082002]\n",
      "epoch:19 step:17815 [D loss: 0.625172, acc: 64.84%] [G loss: 1.930813]\n",
      "epoch:19 step:17816 [D loss: 0.622542, acc: 63.28%] [G loss: 1.970106]\n",
      "epoch:19 step:17817 [D loss: 0.593106, acc: 65.62%] [G loss: 1.932538]\n",
      "epoch:19 step:17818 [D loss: 0.595428, acc: 71.09%] [G loss: 2.143434]\n",
      "epoch:19 step:17819 [D loss: 0.578210, acc: 71.88%] [G loss: 2.243999]\n",
      "epoch:19 step:17820 [D loss: 0.632062, acc: 67.19%] [G loss: 2.018852]\n",
      "epoch:19 step:17821 [D loss: 0.692483, acc: 61.72%] [G loss: 1.824225]\n",
      "epoch:19 step:17822 [D loss: 0.691518, acc: 64.06%] [G loss: 1.835747]\n",
      "epoch:19 step:17823 [D loss: 0.684530, acc: 57.03%] [G loss: 1.714146]\n",
      "epoch:19 step:17824 [D loss: 0.701558, acc: 53.91%] [G loss: 1.792715]\n",
      "epoch:19 step:17825 [D loss: 0.720010, acc: 50.78%] [G loss: 1.906986]\n",
      "epoch:19 step:17826 [D loss: 0.619140, acc: 66.41%] [G loss: 2.040498]\n",
      "epoch:19 step:17827 [D loss: 0.662238, acc: 59.38%] [G loss: 1.784082]\n",
      "epoch:19 step:17828 [D loss: 0.568917, acc: 73.44%] [G loss: 1.945622]\n",
      "epoch:19 step:17829 [D loss: 0.637844, acc: 64.06%] [G loss: 1.839656]\n",
      "epoch:19 step:17830 [D loss: 0.668647, acc: 62.50%] [G loss: 1.853957]\n",
      "epoch:19 step:17831 [D loss: 0.655784, acc: 60.16%] [G loss: 1.923537]\n",
      "epoch:19 step:17832 [D loss: 0.635698, acc: 58.59%] [G loss: 1.824673]\n",
      "epoch:19 step:17833 [D loss: 0.687903, acc: 61.72%] [G loss: 1.867125]\n",
      "epoch:19 step:17834 [D loss: 0.665428, acc: 57.03%] [G loss: 1.859798]\n",
      "epoch:19 step:17835 [D loss: 0.656584, acc: 66.41%] [G loss: 1.831120]\n",
      "epoch:19 step:17836 [D loss: 0.622165, acc: 62.50%] [G loss: 1.734166]\n",
      "epoch:19 step:17837 [D loss: 0.640100, acc: 62.50%] [G loss: 1.774319]\n",
      "epoch:19 step:17838 [D loss: 0.645634, acc: 60.94%] [G loss: 1.814433]\n",
      "epoch:19 step:17839 [D loss: 0.622324, acc: 64.84%] [G loss: 1.937846]\n",
      "epoch:19 step:17840 [D loss: 0.624292, acc: 67.97%] [G loss: 1.877018]\n",
      "epoch:19 step:17841 [D loss: 0.676031, acc: 60.16%] [G loss: 1.885878]\n",
      "epoch:19 step:17842 [D loss: 0.639494, acc: 64.84%] [G loss: 1.946148]\n",
      "epoch:19 step:17843 [D loss: 0.557276, acc: 72.66%] [G loss: 2.136394]\n",
      "epoch:19 step:17844 [D loss: 0.648962, acc: 57.81%] [G loss: 1.926003]\n",
      "epoch:19 step:17845 [D loss: 0.629797, acc: 67.97%] [G loss: 1.915049]\n",
      "epoch:19 step:17846 [D loss: 0.643796, acc: 60.94%] [G loss: 1.935914]\n",
      "epoch:19 step:17847 [D loss: 0.647562, acc: 64.06%] [G loss: 1.876301]\n",
      "epoch:19 step:17848 [D loss: 0.628800, acc: 66.41%] [G loss: 1.877348]\n",
      "epoch:19 step:17849 [D loss: 0.657110, acc: 56.25%] [G loss: 1.814559]\n",
      "epoch:19 step:17850 [D loss: 0.619150, acc: 62.50%] [G loss: 1.822397]\n",
      "epoch:19 step:17851 [D loss: 0.626316, acc: 64.06%] [G loss: 1.858688]\n",
      "epoch:19 step:17852 [D loss: 0.611658, acc: 69.53%] [G loss: 1.975250]\n",
      "epoch:19 step:17853 [D loss: 0.654778, acc: 60.16%] [G loss: 1.970250]\n",
      "epoch:19 step:17854 [D loss: 0.698300, acc: 50.00%] [G loss: 1.915309]\n",
      "epoch:19 step:17855 [D loss: 0.668846, acc: 61.72%] [G loss: 1.858708]\n",
      "epoch:19 step:17856 [D loss: 0.707520, acc: 57.03%] [G loss: 1.853237]\n",
      "epoch:19 step:17857 [D loss: 0.631624, acc: 62.50%] [G loss: 2.034075]\n",
      "epoch:19 step:17858 [D loss: 0.643484, acc: 67.19%] [G loss: 1.905370]\n",
      "epoch:19 step:17859 [D loss: 0.650896, acc: 58.59%] [G loss: 2.040705]\n",
      "epoch:19 step:17860 [D loss: 0.618795, acc: 67.19%] [G loss: 2.032665]\n",
      "epoch:19 step:17861 [D loss: 0.674844, acc: 60.16%] [G loss: 1.889783]\n",
      "epoch:19 step:17862 [D loss: 0.588034, acc: 71.88%] [G loss: 2.045074]\n",
      "epoch:19 step:17863 [D loss: 0.656702, acc: 58.59%] [G loss: 1.830314]\n",
      "epoch:19 step:17864 [D loss: 0.670907, acc: 54.69%] [G loss: 1.838591]\n",
      "epoch:19 step:17865 [D loss: 0.609278, acc: 67.97%] [G loss: 1.816961]\n",
      "epoch:19 step:17866 [D loss: 0.643522, acc: 60.94%] [G loss: 2.090967]\n",
      "epoch:19 step:17867 [D loss: 0.737613, acc: 53.12%] [G loss: 1.828384]\n",
      "epoch:19 step:17868 [D loss: 0.646519, acc: 67.97%] [G loss: 1.840024]\n",
      "epoch:19 step:17869 [D loss: 0.629060, acc: 66.41%] [G loss: 1.910387]\n",
      "epoch:19 step:17870 [D loss: 0.656769, acc: 64.06%] [G loss: 1.971102]\n",
      "epoch:19 step:17871 [D loss: 0.657862, acc: 62.50%] [G loss: 1.851867]\n",
      "epoch:19 step:17872 [D loss: 0.615909, acc: 68.75%] [G loss: 2.033435]\n",
      "epoch:19 step:17873 [D loss: 0.582946, acc: 68.75%] [G loss: 2.144680]\n",
      "epoch:19 step:17874 [D loss: 0.613018, acc: 70.31%] [G loss: 1.840834]\n",
      "epoch:19 step:17875 [D loss: 0.616298, acc: 67.97%] [G loss: 1.901818]\n",
      "epoch:19 step:17876 [D loss: 0.625701, acc: 64.06%] [G loss: 1.951527]\n",
      "epoch:19 step:17877 [D loss: 0.660776, acc: 57.81%] [G loss: 2.066868]\n",
      "epoch:19 step:17878 [D loss: 0.659048, acc: 60.94%] [G loss: 2.197261]\n",
      "epoch:19 step:17879 [D loss: 0.607786, acc: 67.19%] [G loss: 2.014474]\n",
      "epoch:19 step:17880 [D loss: 0.611815, acc: 64.06%] [G loss: 2.015869]\n",
      "epoch:19 step:17881 [D loss: 0.666263, acc: 59.38%] [G loss: 1.822765]\n",
      "epoch:19 step:17882 [D loss: 0.634710, acc: 67.19%] [G loss: 1.855556]\n",
      "epoch:19 step:17883 [D loss: 0.651095, acc: 60.16%] [G loss: 1.766162]\n",
      "epoch:19 step:17884 [D loss: 0.697130, acc: 52.34%] [G loss: 1.805329]\n",
      "epoch:19 step:17885 [D loss: 0.612151, acc: 66.41%] [G loss: 1.983336]\n",
      "epoch:19 step:17886 [D loss: 0.647821, acc: 63.28%] [G loss: 2.098227]\n",
      "epoch:19 step:17887 [D loss: 0.648666, acc: 62.50%] [G loss: 2.030958]\n",
      "epoch:19 step:17888 [D loss: 0.637854, acc: 62.50%] [G loss: 1.872205]\n",
      "epoch:19 step:17889 [D loss: 0.667808, acc: 63.28%] [G loss: 1.867198]\n",
      "epoch:19 step:17890 [D loss: 0.659300, acc: 60.16%] [G loss: 1.895192]\n",
      "epoch:19 step:17891 [D loss: 0.665759, acc: 60.16%] [G loss: 1.816533]\n",
      "epoch:19 step:17892 [D loss: 0.632072, acc: 65.62%] [G loss: 1.964024]\n",
      "epoch:19 step:17893 [D loss: 0.599073, acc: 66.41%] [G loss: 1.874575]\n",
      "epoch:19 step:17894 [D loss: 0.661480, acc: 60.16%] [G loss: 1.905482]\n",
      "epoch:19 step:17895 [D loss: 0.639380, acc: 61.72%] [G loss: 2.061267]\n",
      "epoch:19 step:17896 [D loss: 0.592036, acc: 66.41%] [G loss: 2.022556]\n",
      "epoch:19 step:17897 [D loss: 0.633963, acc: 66.41%] [G loss: 1.883466]\n",
      "epoch:19 step:17898 [D loss: 0.659543, acc: 64.84%] [G loss: 1.807989]\n",
      "epoch:19 step:17899 [D loss: 0.587803, acc: 71.09%] [G loss: 1.998039]\n",
      "epoch:19 step:17900 [D loss: 0.663680, acc: 63.28%] [G loss: 1.877348]\n",
      "epoch:19 step:17901 [D loss: 0.701025, acc: 48.44%] [G loss: 1.780412]\n",
      "epoch:19 step:17902 [D loss: 0.665991, acc: 60.94%] [G loss: 1.890840]\n",
      "epoch:19 step:17903 [D loss: 0.611379, acc: 68.75%] [G loss: 1.966915]\n",
      "epoch:19 step:17904 [D loss: 0.647197, acc: 64.06%] [G loss: 2.073763]\n",
      "epoch:19 step:17905 [D loss: 0.648458, acc: 60.16%] [G loss: 1.754800]\n",
      "epoch:19 step:17906 [D loss: 0.632564, acc: 60.94%] [G loss: 1.920967]\n",
      "epoch:19 step:17907 [D loss: 0.673971, acc: 56.25%] [G loss: 1.788248]\n",
      "epoch:19 step:17908 [D loss: 0.719180, acc: 51.56%] [G loss: 1.859650]\n",
      "epoch:19 step:17909 [D loss: 0.658691, acc: 65.62%] [G loss: 1.973163]\n",
      "epoch:19 step:17910 [D loss: 0.608434, acc: 65.62%] [G loss: 2.093164]\n",
      "epoch:19 step:17911 [D loss: 0.652501, acc: 62.50%] [G loss: 1.898385]\n",
      "epoch:19 step:17912 [D loss: 0.685282, acc: 57.81%] [G loss: 1.753664]\n",
      "epoch:19 step:17913 [D loss: 0.735841, acc: 54.69%] [G loss: 1.831779]\n",
      "epoch:19 step:17914 [D loss: 0.626533, acc: 60.16%] [G loss: 2.053334]\n",
      "epoch:19 step:17915 [D loss: 0.564987, acc: 71.88%] [G loss: 1.924939]\n",
      "epoch:19 step:17916 [D loss: 0.633627, acc: 62.50%] [G loss: 2.009950]\n",
      "epoch:19 step:17917 [D loss: 0.626835, acc: 64.06%] [G loss: 1.950518]\n",
      "epoch:19 step:17918 [D loss: 0.618877, acc: 64.06%] [G loss: 2.223675]\n",
      "epoch:19 step:17919 [D loss: 0.638266, acc: 62.50%] [G loss: 2.017329]\n",
      "epoch:19 step:17920 [D loss: 0.578437, acc: 65.62%] [G loss: 2.057981]\n",
      "epoch:19 step:17921 [D loss: 0.698217, acc: 57.81%] [G loss: 1.994547]\n",
      "epoch:19 step:17922 [D loss: 0.609971, acc: 67.19%] [G loss: 2.135778]\n",
      "epoch:19 step:17923 [D loss: 0.682059, acc: 56.25%] [G loss: 1.990321]\n",
      "epoch:19 step:17924 [D loss: 0.666889, acc: 58.59%] [G loss: 1.996239]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:19 step:17925 [D loss: 0.634152, acc: 67.19%] [G loss: 2.053729]\n",
      "epoch:19 step:17926 [D loss: 0.610708, acc: 60.94%] [G loss: 1.975648]\n",
      "epoch:19 step:17927 [D loss: 0.665562, acc: 57.03%] [G loss: 1.942930]\n",
      "epoch:19 step:17928 [D loss: 0.682629, acc: 56.25%] [G loss: 1.897723]\n",
      "epoch:19 step:17929 [D loss: 0.687968, acc: 59.38%] [G loss: 1.878765]\n",
      "epoch:19 step:17930 [D loss: 0.678642, acc: 54.69%] [G loss: 1.754264]\n",
      "epoch:19 step:17931 [D loss: 0.670860, acc: 58.59%] [G loss: 1.864291]\n",
      "epoch:19 step:17932 [D loss: 0.676394, acc: 53.91%] [G loss: 1.817341]\n",
      "epoch:19 step:17933 [D loss: 0.655141, acc: 60.16%] [G loss: 2.011359]\n",
      "epoch:19 step:17934 [D loss: 0.615375, acc: 62.50%] [G loss: 1.862489]\n",
      "epoch:19 step:17935 [D loss: 0.677668, acc: 55.47%] [G loss: 1.880333]\n",
      "epoch:19 step:17936 [D loss: 0.682962, acc: 54.69%] [G loss: 1.706553]\n",
      "epoch:19 step:17937 [D loss: 0.658306, acc: 64.06%] [G loss: 1.935547]\n",
      "epoch:19 step:17938 [D loss: 0.648786, acc: 63.28%] [G loss: 1.985676]\n",
      "epoch:19 step:17939 [D loss: 0.618087, acc: 63.28%] [G loss: 1.809487]\n",
      "epoch:19 step:17940 [D loss: 0.661487, acc: 64.06%] [G loss: 1.799479]\n",
      "epoch:19 step:17941 [D loss: 0.608049, acc: 63.28%] [G loss: 1.896541]\n",
      "epoch:19 step:17942 [D loss: 0.650470, acc: 63.28%] [G loss: 1.909694]\n",
      "epoch:19 step:17943 [D loss: 0.670121, acc: 60.16%] [G loss: 1.903611]\n",
      "epoch:19 step:17944 [D loss: 0.638215, acc: 67.97%] [G loss: 1.854050]\n",
      "epoch:19 step:17945 [D loss: 0.622666, acc: 70.31%] [G loss: 1.779190]\n",
      "epoch:19 step:17946 [D loss: 0.658379, acc: 55.47%] [G loss: 1.932591]\n",
      "epoch:19 step:17947 [D loss: 0.641470, acc: 65.62%] [G loss: 1.928780]\n",
      "epoch:19 step:17948 [D loss: 0.620536, acc: 63.28%] [G loss: 1.930364]\n",
      "epoch:19 step:17949 [D loss: 0.643242, acc: 62.50%] [G loss: 1.856345]\n",
      "epoch:19 step:17950 [D loss: 0.636356, acc: 64.06%] [G loss: 2.030511]\n",
      "epoch:19 step:17951 [D loss: 0.634837, acc: 65.62%] [G loss: 1.867009]\n",
      "epoch:19 step:17952 [D loss: 0.631372, acc: 62.50%] [G loss: 2.007802]\n",
      "epoch:19 step:17953 [D loss: 0.670797, acc: 60.94%] [G loss: 1.948661]\n",
      "epoch:19 step:17954 [D loss: 0.682837, acc: 55.47%] [G loss: 1.887111]\n",
      "epoch:19 step:17955 [D loss: 0.607598, acc: 71.09%] [G loss: 1.954766]\n",
      "epoch:19 step:17956 [D loss: 0.703363, acc: 59.38%] [G loss: 1.790542]\n",
      "epoch:19 step:17957 [D loss: 0.651178, acc: 63.28%] [G loss: 1.868266]\n",
      "epoch:19 step:17958 [D loss: 0.612937, acc: 66.41%] [G loss: 2.005397]\n",
      "epoch:19 step:17959 [D loss: 0.620196, acc: 65.62%] [G loss: 1.880544]\n",
      "epoch:19 step:17960 [D loss: 0.650310, acc: 67.97%] [G loss: 1.871535]\n",
      "epoch:19 step:17961 [D loss: 0.646737, acc: 66.41%] [G loss: 1.823895]\n",
      "epoch:19 step:17962 [D loss: 0.643227, acc: 66.41%] [G loss: 1.876955]\n",
      "epoch:19 step:17963 [D loss: 0.690723, acc: 57.03%] [G loss: 1.701741]\n",
      "epoch:19 step:17964 [D loss: 0.704658, acc: 52.34%] [G loss: 1.819050]\n",
      "epoch:19 step:17965 [D loss: 0.592696, acc: 64.06%] [G loss: 1.998475]\n",
      "epoch:19 step:17966 [D loss: 0.618528, acc: 61.72%] [G loss: 1.905246]\n",
      "epoch:19 step:17967 [D loss: 0.623733, acc: 62.50%] [G loss: 1.746632]\n",
      "epoch:19 step:17968 [D loss: 0.705159, acc: 51.56%] [G loss: 1.942043]\n",
      "epoch:19 step:17969 [D loss: 0.632615, acc: 67.19%] [G loss: 1.782633]\n",
      "epoch:19 step:17970 [D loss: 0.683272, acc: 55.47%] [G loss: 1.848362]\n",
      "epoch:19 step:17971 [D loss: 0.657904, acc: 60.16%] [G loss: 1.923871]\n",
      "epoch:19 step:17972 [D loss: 0.652455, acc: 58.59%] [G loss: 1.858516]\n",
      "epoch:19 step:17973 [D loss: 0.667970, acc: 61.72%] [G loss: 1.791913]\n",
      "epoch:19 step:17974 [D loss: 0.681590, acc: 59.38%] [G loss: 1.800405]\n",
      "epoch:19 step:17975 [D loss: 0.634869, acc: 60.94%] [G loss: 1.827527]\n",
      "epoch:19 step:17976 [D loss: 0.646030, acc: 63.28%] [G loss: 1.879008]\n",
      "epoch:19 step:17977 [D loss: 0.606025, acc: 65.62%] [G loss: 1.850541]\n",
      "epoch:19 step:17978 [D loss: 0.659736, acc: 62.50%] [G loss: 1.796222]\n",
      "epoch:19 step:17979 [D loss: 0.657603, acc: 64.06%] [G loss: 1.755580]\n",
      "epoch:19 step:17980 [D loss: 0.649306, acc: 61.72%] [G loss: 1.873534]\n",
      "epoch:19 step:17981 [D loss: 0.715330, acc: 57.81%] [G loss: 1.793120]\n",
      "epoch:19 step:17982 [D loss: 0.669290, acc: 61.72%] [G loss: 1.842870]\n",
      "epoch:19 step:17983 [D loss: 0.660458, acc: 60.94%] [G loss: 1.713447]\n",
      "epoch:19 step:17984 [D loss: 0.692173, acc: 55.47%] [G loss: 1.689839]\n",
      "epoch:19 step:17985 [D loss: 0.684618, acc: 54.69%] [G loss: 1.804618]\n",
      "epoch:19 step:17986 [D loss: 0.630690, acc: 58.59%] [G loss: 1.769387]\n",
      "epoch:19 step:17987 [D loss: 0.645020, acc: 63.28%] [G loss: 1.822146]\n",
      "epoch:19 step:17988 [D loss: 0.636393, acc: 59.38%] [G loss: 1.918045]\n",
      "epoch:19 step:17989 [D loss: 0.677567, acc: 57.81%] [G loss: 1.876631]\n",
      "epoch:19 step:17990 [D loss: 0.706211, acc: 57.81%] [G loss: 1.696797]\n",
      "epoch:19 step:17991 [D loss: 0.650164, acc: 62.50%] [G loss: 1.811611]\n",
      "epoch:19 step:17992 [D loss: 0.661088, acc: 61.72%] [G loss: 1.854813]\n",
      "epoch:19 step:17993 [D loss: 0.626277, acc: 62.50%] [G loss: 1.783419]\n",
      "epoch:19 step:17994 [D loss: 0.659872, acc: 60.94%] [G loss: 2.002951]\n",
      "epoch:19 step:17995 [D loss: 0.616989, acc: 60.16%] [G loss: 1.929744]\n",
      "epoch:19 step:17996 [D loss: 0.653377, acc: 61.72%] [G loss: 1.864600]\n",
      "epoch:19 step:17997 [D loss: 0.632189, acc: 66.41%] [G loss: 2.023582]\n",
      "epoch:19 step:17998 [D loss: 0.646600, acc: 63.28%] [G loss: 1.877788]\n",
      "epoch:19 step:17999 [D loss: 0.676742, acc: 65.62%] [G loss: 1.764660]\n",
      "epoch:19 step:18000 [D loss: 0.685452, acc: 55.47%] [G loss: 1.967039]\n",
      "##############\n",
      "[2.34709665 1.25218032 6.29617245 4.77399698 3.71456032 5.73631156\n",
      " 4.48922943 4.78673627 4.41866301 3.59409556]\n",
      "##########\n",
      "epoch:19 step:18001 [D loss: 0.667640, acc: 59.38%] [G loss: 1.967386]\n",
      "epoch:19 step:18002 [D loss: 0.638192, acc: 62.50%] [G loss: 1.945538]\n",
      "epoch:19 step:18003 [D loss: 0.687656, acc: 57.81%] [G loss: 1.698076]\n",
      "epoch:19 step:18004 [D loss: 0.599555, acc: 66.41%] [G loss: 1.912735]\n",
      "epoch:19 step:18005 [D loss: 0.634376, acc: 67.19%] [G loss: 1.791023]\n",
      "epoch:19 step:18006 [D loss: 0.671302, acc: 57.81%] [G loss: 1.843905]\n",
      "epoch:19 step:18007 [D loss: 0.638064, acc: 68.75%] [G loss: 1.866137]\n",
      "epoch:19 step:18008 [D loss: 0.703184, acc: 56.25%] [G loss: 1.788536]\n",
      "epoch:19 step:18009 [D loss: 0.654092, acc: 62.50%] [G loss: 1.903825]\n",
      "epoch:19 step:18010 [D loss: 0.649348, acc: 66.41%] [G loss: 2.045977]\n",
      "epoch:19 step:18011 [D loss: 0.627559, acc: 66.41%] [G loss: 2.032419]\n",
      "epoch:19 step:18012 [D loss: 0.601459, acc: 65.62%] [G loss: 2.079382]\n",
      "epoch:19 step:18013 [D loss: 0.669705, acc: 63.28%] [G loss: 1.835105]\n",
      "epoch:19 step:18014 [D loss: 0.674677, acc: 57.81%] [G loss: 1.725122]\n",
      "epoch:19 step:18015 [D loss: 0.651205, acc: 60.94%] [G loss: 1.777455]\n",
      "epoch:19 step:18016 [D loss: 0.646020, acc: 65.62%] [G loss: 1.922872]\n",
      "epoch:19 step:18017 [D loss: 0.751820, acc: 49.22%] [G loss: 1.754103]\n",
      "epoch:19 step:18018 [D loss: 0.640066, acc: 61.72%] [G loss: 1.858203]\n",
      "epoch:19 step:18019 [D loss: 0.609359, acc: 63.28%] [G loss: 2.002485]\n",
      "epoch:19 step:18020 [D loss: 0.625232, acc: 67.19%] [G loss: 1.876292]\n",
      "epoch:19 step:18021 [D loss: 0.583925, acc: 69.53%] [G loss: 2.067740]\n",
      "epoch:19 step:18022 [D loss: 0.635875, acc: 65.62%] [G loss: 2.020039]\n",
      "epoch:19 step:18023 [D loss: 0.716352, acc: 47.66%] [G loss: 1.817364]\n",
      "epoch:19 step:18024 [D loss: 0.656439, acc: 57.03%] [G loss: 1.945133]\n",
      "epoch:19 step:18025 [D loss: 0.669983, acc: 58.59%] [G loss: 1.875686]\n",
      "epoch:19 step:18026 [D loss: 0.605993, acc: 69.53%] [G loss: 1.985747]\n",
      "epoch:19 step:18027 [D loss: 0.665553, acc: 61.72%] [G loss: 1.823173]\n",
      "epoch:19 step:18028 [D loss: 0.680883, acc: 54.69%] [G loss: 1.913119]\n",
      "epoch:19 step:18029 [D loss: 0.623180, acc: 64.84%] [G loss: 1.949130]\n",
      "epoch:19 step:18030 [D loss: 0.633334, acc: 68.75%] [G loss: 1.815567]\n",
      "epoch:19 step:18031 [D loss: 0.707551, acc: 53.12%] [G loss: 1.793887]\n",
      "epoch:19 step:18032 [D loss: 0.608829, acc: 67.19%] [G loss: 2.194737]\n",
      "epoch:19 step:18033 [D loss: 0.621093, acc: 67.19%] [G loss: 2.083017]\n",
      "epoch:19 step:18034 [D loss: 0.570666, acc: 69.53%] [G loss: 2.270357]\n",
      "epoch:19 step:18035 [D loss: 0.581906, acc: 67.97%] [G loss: 2.438498]\n",
      "epoch:19 step:18036 [D loss: 0.630329, acc: 67.19%] [G loss: 1.893765]\n",
      "epoch:19 step:18037 [D loss: 0.716695, acc: 55.47%] [G loss: 1.856691]\n",
      "epoch:19 step:18038 [D loss: 0.682627, acc: 60.16%] [G loss: 1.846547]\n",
      "epoch:19 step:18039 [D loss: 0.647290, acc: 60.94%] [G loss: 1.896268]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:19 step:18040 [D loss: 0.654641, acc: 63.28%] [G loss: 1.852774]\n",
      "epoch:19 step:18041 [D loss: 0.667510, acc: 64.84%] [G loss: 1.848629]\n",
      "epoch:19 step:18042 [D loss: 0.630231, acc: 57.03%] [G loss: 1.827617]\n",
      "epoch:19 step:18043 [D loss: 0.673167, acc: 61.72%] [G loss: 1.987240]\n",
      "epoch:19 step:18044 [D loss: 0.659647, acc: 61.72%] [G loss: 1.954384]\n",
      "epoch:19 step:18045 [D loss: 0.570416, acc: 71.88%] [G loss: 1.945364]\n",
      "epoch:19 step:18046 [D loss: 0.622522, acc: 65.62%] [G loss: 1.921747]\n",
      "epoch:19 step:18047 [D loss: 0.672340, acc: 61.72%] [G loss: 1.884795]\n",
      "epoch:19 step:18048 [D loss: 0.650834, acc: 61.72%] [G loss: 1.894287]\n",
      "epoch:19 step:18049 [D loss: 0.660581, acc: 62.50%] [G loss: 1.924134]\n",
      "epoch:19 step:18050 [D loss: 0.655723, acc: 63.28%] [G loss: 1.871000]\n",
      "epoch:19 step:18051 [D loss: 0.588508, acc: 67.97%] [G loss: 2.152558]\n",
      "epoch:19 step:18052 [D loss: 0.714297, acc: 59.38%] [G loss: 1.828162]\n",
      "epoch:19 step:18053 [D loss: 0.732922, acc: 50.78%] [G loss: 1.741742]\n",
      "epoch:19 step:18054 [D loss: 0.703292, acc: 57.81%] [G loss: 1.714211]\n",
      "epoch:19 step:18055 [D loss: 0.686002, acc: 58.59%] [G loss: 1.881501]\n",
      "epoch:19 step:18056 [D loss: 0.640337, acc: 59.38%] [G loss: 1.647345]\n",
      "epoch:19 step:18057 [D loss: 0.678791, acc: 55.47%] [G loss: 1.863057]\n",
      "epoch:19 step:18058 [D loss: 0.662893, acc: 58.59%] [G loss: 1.853196]\n",
      "epoch:19 step:18059 [D loss: 0.617049, acc: 65.62%] [G loss: 1.764759]\n",
      "epoch:19 step:18060 [D loss: 0.647795, acc: 64.84%] [G loss: 1.829490]\n",
      "epoch:19 step:18061 [D loss: 0.667237, acc: 62.50%] [G loss: 1.810828]\n",
      "epoch:19 step:18062 [D loss: 0.652182, acc: 58.59%] [G loss: 1.799967]\n",
      "epoch:19 step:18063 [D loss: 0.656189, acc: 60.16%] [G loss: 1.838833]\n",
      "epoch:19 step:18064 [D loss: 0.632408, acc: 67.97%] [G loss: 2.043490]\n",
      "epoch:19 step:18065 [D loss: 0.616194, acc: 60.16%] [G loss: 2.031940]\n",
      "epoch:19 step:18066 [D loss: 0.609466, acc: 68.75%] [G loss: 1.927999]\n",
      "epoch:19 step:18067 [D loss: 0.647306, acc: 62.50%] [G loss: 2.094659]\n",
      "epoch:19 step:18068 [D loss: 0.637600, acc: 67.97%] [G loss: 1.676250]\n",
      "epoch:19 step:18069 [D loss: 0.614121, acc: 66.41%] [G loss: 1.876345]\n",
      "epoch:19 step:18070 [D loss: 0.639114, acc: 62.50%] [G loss: 1.944837]\n",
      "epoch:19 step:18071 [D loss: 0.712243, acc: 48.44%] [G loss: 1.853878]\n",
      "epoch:19 step:18072 [D loss: 0.630605, acc: 66.41%] [G loss: 1.889656]\n",
      "epoch:19 step:18073 [D loss: 0.621027, acc: 71.88%] [G loss: 2.005037]\n",
      "epoch:19 step:18074 [D loss: 0.698753, acc: 53.91%] [G loss: 1.912351]\n",
      "epoch:19 step:18075 [D loss: 0.632843, acc: 62.50%] [G loss: 1.874067]\n",
      "epoch:19 step:18076 [D loss: 0.656990, acc: 59.38%] [G loss: 1.886561]\n",
      "epoch:19 step:18077 [D loss: 0.588729, acc: 67.97%] [G loss: 2.068876]\n",
      "epoch:19 step:18078 [D loss: 0.661947, acc: 57.81%] [G loss: 2.150005]\n",
      "epoch:19 step:18079 [D loss: 0.623902, acc: 64.84%] [G loss: 2.290737]\n",
      "epoch:19 step:18080 [D loss: 0.652836, acc: 54.69%] [G loss: 1.888881]\n",
      "epoch:19 step:18081 [D loss: 0.610240, acc: 72.66%] [G loss: 1.894943]\n",
      "epoch:19 step:18082 [D loss: 0.685533, acc: 62.50%] [G loss: 1.930625]\n",
      "epoch:19 step:18083 [D loss: 0.641115, acc: 63.28%] [G loss: 1.896868]\n",
      "epoch:19 step:18084 [D loss: 0.684704, acc: 54.69%] [G loss: 1.761122]\n",
      "epoch:19 step:18085 [D loss: 0.654554, acc: 63.28%] [G loss: 1.836388]\n",
      "epoch:19 step:18086 [D loss: 0.638738, acc: 64.84%] [G loss: 1.889102]\n",
      "epoch:19 step:18087 [D loss: 0.675442, acc: 56.25%] [G loss: 1.843482]\n",
      "epoch:19 step:18088 [D loss: 0.632247, acc: 67.97%] [G loss: 1.850623]\n",
      "epoch:19 step:18089 [D loss: 0.627013, acc: 59.38%] [G loss: 1.927727]\n",
      "epoch:19 step:18090 [D loss: 0.671310, acc: 60.94%] [G loss: 1.767221]\n",
      "epoch:19 step:18091 [D loss: 0.665978, acc: 57.81%] [G loss: 1.740754]\n",
      "epoch:19 step:18092 [D loss: 0.617292, acc: 71.88%] [G loss: 1.919644]\n",
      "epoch:19 step:18093 [D loss: 0.617513, acc: 66.41%] [G loss: 1.873670]\n",
      "epoch:19 step:18094 [D loss: 0.701637, acc: 59.38%] [G loss: 1.792859]\n",
      "epoch:19 step:18095 [D loss: 0.682081, acc: 56.25%] [G loss: 1.761661]\n",
      "epoch:19 step:18096 [D loss: 0.689604, acc: 61.72%] [G loss: 1.831080]\n",
      "epoch:19 step:18097 [D loss: 0.636244, acc: 64.84%] [G loss: 1.775542]\n",
      "epoch:19 step:18098 [D loss: 0.675127, acc: 64.84%] [G loss: 1.839528]\n",
      "epoch:19 step:18099 [D loss: 0.673279, acc: 57.03%] [G loss: 1.813822]\n",
      "epoch:19 step:18100 [D loss: 0.633796, acc: 64.84%] [G loss: 1.724812]\n",
      "epoch:19 step:18101 [D loss: 0.625165, acc: 65.62%] [G loss: 2.009442]\n",
      "epoch:19 step:18102 [D loss: 0.630545, acc: 65.62%] [G loss: 1.919289]\n",
      "epoch:19 step:18103 [D loss: 0.595505, acc: 70.31%] [G loss: 1.969499]\n",
      "epoch:19 step:18104 [D loss: 0.619744, acc: 62.50%] [G loss: 1.878332]\n",
      "epoch:19 step:18105 [D loss: 0.653222, acc: 64.06%] [G loss: 1.953763]\n",
      "epoch:19 step:18106 [D loss: 0.640576, acc: 58.59%] [G loss: 1.802031]\n",
      "epoch:19 step:18107 [D loss: 0.653913, acc: 56.25%] [G loss: 1.780199]\n",
      "epoch:19 step:18108 [D loss: 0.698639, acc: 60.94%] [G loss: 1.917921]\n",
      "epoch:19 step:18109 [D loss: 0.683164, acc: 59.38%] [G loss: 1.790667]\n",
      "epoch:19 step:18110 [D loss: 0.672741, acc: 58.59%] [G loss: 1.840991]\n",
      "epoch:19 step:18111 [D loss: 0.612400, acc: 66.41%] [G loss: 1.859379]\n",
      "epoch:19 step:18112 [D loss: 0.652993, acc: 59.38%] [G loss: 1.838884]\n",
      "epoch:19 step:18113 [D loss: 0.689363, acc: 57.81%] [G loss: 1.956323]\n",
      "epoch:19 step:18114 [D loss: 0.648237, acc: 55.47%] [G loss: 1.943843]\n",
      "epoch:19 step:18115 [D loss: 0.660566, acc: 63.28%] [G loss: 2.011606]\n",
      "epoch:19 step:18116 [D loss: 0.578524, acc: 70.31%] [G loss: 2.114467]\n",
      "epoch:19 step:18117 [D loss: 0.592553, acc: 67.97%] [G loss: 2.188393]\n",
      "epoch:19 step:18118 [D loss: 0.585322, acc: 71.09%] [G loss: 2.126156]\n",
      "epoch:19 step:18119 [D loss: 0.741757, acc: 49.22%] [G loss: 1.869628]\n",
      "epoch:19 step:18120 [D loss: 0.604969, acc: 70.31%] [G loss: 1.771961]\n",
      "epoch:19 step:18121 [D loss: 0.634217, acc: 66.41%] [G loss: 1.846555]\n",
      "epoch:19 step:18122 [D loss: 0.719856, acc: 50.00%] [G loss: 1.793450]\n",
      "epoch:19 step:18123 [D loss: 0.693851, acc: 57.03%] [G loss: 1.812089]\n",
      "epoch:19 step:18124 [D loss: 0.632178, acc: 65.62%] [G loss: 1.915559]\n",
      "epoch:19 step:18125 [D loss: 0.658180, acc: 62.50%] [G loss: 1.787224]\n",
      "epoch:19 step:18126 [D loss: 0.678201, acc: 58.59%] [G loss: 1.864999]\n",
      "epoch:19 step:18127 [D loss: 0.664921, acc: 61.72%] [G loss: 1.832547]\n",
      "epoch:19 step:18128 [D loss: 0.650982, acc: 56.25%] [G loss: 1.860094]\n",
      "epoch:19 step:18129 [D loss: 0.672607, acc: 57.81%] [G loss: 1.743545]\n",
      "epoch:19 step:18130 [D loss: 0.602469, acc: 71.88%] [G loss: 1.770622]\n",
      "epoch:19 step:18131 [D loss: 0.663670, acc: 64.84%] [G loss: 1.841119]\n",
      "epoch:19 step:18132 [D loss: 0.637510, acc: 64.84%] [G loss: 1.817247]\n",
      "epoch:19 step:18133 [D loss: 0.590258, acc: 67.97%] [G loss: 2.004064]\n",
      "epoch:19 step:18134 [D loss: 0.653012, acc: 60.94%] [G loss: 2.002429]\n",
      "epoch:19 step:18135 [D loss: 0.571329, acc: 71.88%] [G loss: 1.947489]\n",
      "epoch:19 step:18136 [D loss: 0.638947, acc: 62.50%] [G loss: 1.968938]\n",
      "epoch:19 step:18137 [D loss: 0.620555, acc: 67.97%] [G loss: 1.957088]\n",
      "epoch:19 step:18138 [D loss: 0.609796, acc: 65.62%] [G loss: 1.845705]\n",
      "epoch:19 step:18139 [D loss: 0.631510, acc: 63.28%] [G loss: 2.008058]\n",
      "epoch:19 step:18140 [D loss: 0.695535, acc: 60.94%] [G loss: 2.132603]\n",
      "epoch:19 step:18141 [D loss: 0.649885, acc: 61.72%] [G loss: 1.962342]\n",
      "epoch:19 step:18142 [D loss: 0.619006, acc: 68.75%] [G loss: 2.037823]\n",
      "epoch:19 step:18143 [D loss: 0.652071, acc: 64.06%] [G loss: 1.993055]\n",
      "epoch:19 step:18144 [D loss: 0.678308, acc: 56.25%] [G loss: 1.772263]\n",
      "epoch:19 step:18145 [D loss: 0.688500, acc: 55.47%] [G loss: 1.789630]\n",
      "epoch:19 step:18146 [D loss: 0.658117, acc: 59.38%] [G loss: 1.826964]\n",
      "epoch:19 step:18147 [D loss: 0.684580, acc: 54.69%] [G loss: 1.871391]\n",
      "epoch:19 step:18148 [D loss: 0.602878, acc: 69.53%] [G loss: 2.097369]\n",
      "epoch:19 step:18149 [D loss: 0.608491, acc: 69.53%] [G loss: 2.164551]\n",
      "epoch:19 step:18150 [D loss: 0.581269, acc: 70.31%] [G loss: 2.130109]\n",
      "epoch:19 step:18151 [D loss: 0.619975, acc: 67.97%] [G loss: 1.806881]\n",
      "epoch:19 step:18152 [D loss: 0.748416, acc: 46.09%] [G loss: 1.680805]\n",
      "epoch:19 step:18153 [D loss: 0.649633, acc: 64.84%] [G loss: 1.929887]\n",
      "epoch:19 step:18154 [D loss: 0.696257, acc: 54.69%] [G loss: 1.766223]\n",
      "epoch:19 step:18155 [D loss: 0.690237, acc: 57.03%] [G loss: 1.849505]\n",
      "epoch:19 step:18156 [D loss: 0.596449, acc: 68.75%] [G loss: 1.986202]\n",
      "epoch:19 step:18157 [D loss: 0.624083, acc: 65.62%] [G loss: 2.066784]\n",
      "epoch:19 step:18158 [D loss: 0.629481, acc: 65.62%] [G loss: 1.875654]\n",
      "epoch:19 step:18159 [D loss: 0.619601, acc: 64.06%] [G loss: 1.845563]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:19 step:18160 [D loss: 0.639617, acc: 63.28%] [G loss: 1.936535]\n",
      "epoch:19 step:18161 [D loss: 0.632454, acc: 64.84%] [G loss: 2.068744]\n",
      "epoch:19 step:18162 [D loss: 0.616985, acc: 61.72%] [G loss: 1.990974]\n",
      "epoch:19 step:18163 [D loss: 0.644624, acc: 62.50%] [G loss: 1.940233]\n",
      "epoch:19 step:18164 [D loss: 0.624630, acc: 66.41%] [G loss: 1.846330]\n",
      "epoch:19 step:18165 [D loss: 0.620992, acc: 69.53%] [G loss: 1.786051]\n",
      "epoch:19 step:18166 [D loss: 0.716339, acc: 53.91%] [G loss: 1.882713]\n",
      "epoch:19 step:18167 [D loss: 0.582079, acc: 72.66%] [G loss: 1.835944]\n",
      "epoch:19 step:18168 [D loss: 0.656511, acc: 67.19%] [G loss: 1.800808]\n",
      "epoch:19 step:18169 [D loss: 0.650273, acc: 61.72%] [G loss: 2.028028]\n",
      "epoch:19 step:18170 [D loss: 0.645534, acc: 64.06%] [G loss: 1.845289]\n",
      "epoch:19 step:18171 [D loss: 0.672276, acc: 62.50%] [G loss: 1.832074]\n",
      "epoch:19 step:18172 [D loss: 0.653364, acc: 63.28%] [G loss: 1.814402]\n",
      "epoch:19 step:18173 [D loss: 0.611806, acc: 66.41%] [G loss: 1.890709]\n",
      "epoch:19 step:18174 [D loss: 0.668501, acc: 58.59%] [G loss: 2.006021]\n",
      "epoch:19 step:18175 [D loss: 0.634175, acc: 60.16%] [G loss: 1.967223]\n",
      "epoch:19 step:18176 [D loss: 0.726639, acc: 53.12%] [G loss: 1.795474]\n",
      "epoch:19 step:18177 [D loss: 0.651021, acc: 60.94%] [G loss: 1.912139]\n",
      "epoch:19 step:18178 [D loss: 0.674948, acc: 59.38%] [G loss: 1.756222]\n",
      "epoch:19 step:18179 [D loss: 0.644407, acc: 57.03%] [G loss: 1.722838]\n",
      "epoch:19 step:18180 [D loss: 0.733216, acc: 51.56%] [G loss: 1.737328]\n",
      "epoch:19 step:18181 [D loss: 0.671930, acc: 53.91%] [G loss: 1.879305]\n",
      "epoch:19 step:18182 [D loss: 0.602796, acc: 68.75%] [G loss: 1.830391]\n",
      "epoch:19 step:18183 [D loss: 0.621134, acc: 65.62%] [G loss: 1.952599]\n",
      "epoch:19 step:18184 [D loss: 0.602986, acc: 72.66%] [G loss: 2.069628]\n",
      "epoch:19 step:18185 [D loss: 0.581388, acc: 71.09%] [G loss: 1.965954]\n",
      "epoch:19 step:18186 [D loss: 0.617764, acc: 70.31%] [G loss: 1.827112]\n",
      "epoch:19 step:18187 [D loss: 0.633144, acc: 63.28%] [G loss: 1.957675]\n",
      "epoch:19 step:18188 [D loss: 0.617452, acc: 67.19%] [G loss: 1.826644]\n",
      "epoch:19 step:18189 [D loss: 0.647797, acc: 59.38%] [G loss: 1.856479]\n",
      "epoch:19 step:18190 [D loss: 0.720341, acc: 51.56%] [G loss: 1.872295]\n",
      "epoch:19 step:18191 [D loss: 0.638470, acc: 61.72%] [G loss: 1.859899]\n",
      "epoch:19 step:18192 [D loss: 0.607059, acc: 70.31%] [G loss: 2.049884]\n",
      "epoch:19 step:18193 [D loss: 0.635702, acc: 67.19%] [G loss: 1.869805]\n",
      "epoch:19 step:18194 [D loss: 0.635917, acc: 63.28%] [G loss: 1.861313]\n",
      "epoch:19 step:18195 [D loss: 0.654131, acc: 60.16%] [G loss: 1.897789]\n",
      "epoch:19 step:18196 [D loss: 0.672019, acc: 60.94%] [G loss: 1.900260]\n",
      "epoch:19 step:18197 [D loss: 0.648630, acc: 60.16%] [G loss: 1.898076]\n",
      "epoch:19 step:18198 [D loss: 0.636515, acc: 63.28%] [G loss: 1.982656]\n",
      "epoch:19 step:18199 [D loss: 0.669179, acc: 61.72%] [G loss: 1.732487]\n",
      "epoch:19 step:18200 [D loss: 0.671114, acc: 58.59%] [G loss: 1.748731]\n",
      "##############\n",
      "[2.49668566 1.26718748 6.15888784 4.74964003 3.54811447 5.76384912\n",
      " 4.32824939 4.7044733  4.64628461 3.68371124]\n",
      "##########\n",
      "epoch:19 step:18201 [D loss: 0.638806, acc: 62.50%] [G loss: 1.772688]\n",
      "epoch:19 step:18202 [D loss: 0.603263, acc: 64.84%] [G loss: 1.970071]\n",
      "epoch:19 step:18203 [D loss: 0.651942, acc: 60.94%] [G loss: 1.890631]\n",
      "epoch:19 step:18204 [D loss: 0.600565, acc: 66.41%] [G loss: 1.910664]\n",
      "epoch:19 step:18205 [D loss: 0.600037, acc: 69.53%] [G loss: 1.939807]\n",
      "epoch:19 step:18206 [D loss: 0.702802, acc: 52.34%] [G loss: 1.901481]\n",
      "epoch:19 step:18207 [D loss: 0.651146, acc: 61.72%] [G loss: 2.020397]\n",
      "epoch:19 step:18208 [D loss: 0.608560, acc: 67.19%] [G loss: 1.970990]\n",
      "epoch:19 step:18209 [D loss: 0.605573, acc: 60.94%] [G loss: 2.148315]\n",
      "epoch:19 step:18210 [D loss: 0.665055, acc: 64.84%] [G loss: 1.874756]\n",
      "epoch:19 step:18211 [D loss: 0.640111, acc: 60.16%] [G loss: 1.888437]\n",
      "epoch:19 step:18212 [D loss: 0.598559, acc: 67.97%] [G loss: 1.999959]\n",
      "epoch:19 step:18213 [D loss: 0.696005, acc: 58.59%] [G loss: 1.798613]\n",
      "epoch:19 step:18214 [D loss: 0.657932, acc: 60.94%] [G loss: 1.926228]\n",
      "epoch:19 step:18215 [D loss: 0.622247, acc: 64.84%] [G loss: 1.982640]\n",
      "epoch:19 step:18216 [D loss: 0.672784, acc: 58.59%] [G loss: 1.967932]\n",
      "epoch:19 step:18217 [D loss: 0.658677, acc: 59.38%] [G loss: 1.927233]\n",
      "epoch:19 step:18218 [D loss: 0.646165, acc: 64.84%] [G loss: 2.042661]\n",
      "epoch:19 step:18219 [D loss: 0.632757, acc: 65.62%] [G loss: 1.895820]\n",
      "epoch:19 step:18220 [D loss: 0.634965, acc: 64.06%] [G loss: 1.954213]\n",
      "epoch:19 step:18221 [D loss: 0.680118, acc: 61.72%] [G loss: 1.842294]\n",
      "epoch:19 step:18222 [D loss: 0.693484, acc: 58.59%] [G loss: 1.794383]\n",
      "epoch:19 step:18223 [D loss: 0.686971, acc: 55.47%] [G loss: 1.844593]\n",
      "epoch:19 step:18224 [D loss: 0.681603, acc: 59.38%] [G loss: 1.798400]\n",
      "epoch:19 step:18225 [D loss: 0.682451, acc: 56.25%] [G loss: 1.811724]\n",
      "epoch:19 step:18226 [D loss: 0.624114, acc: 67.97%] [G loss: 1.759710]\n",
      "epoch:19 step:18227 [D loss: 0.627318, acc: 70.31%] [G loss: 1.878305]\n",
      "epoch:19 step:18228 [D loss: 0.610802, acc: 67.19%] [G loss: 1.911486]\n",
      "epoch:19 step:18229 [D loss: 0.643226, acc: 65.62%] [G loss: 1.812849]\n",
      "epoch:19 step:18230 [D loss: 0.642008, acc: 61.72%] [G loss: 1.853742]\n",
      "epoch:19 step:18231 [D loss: 0.592900, acc: 60.94%] [G loss: 1.958894]\n",
      "epoch:19 step:18232 [D loss: 0.634824, acc: 64.84%] [G loss: 2.097498]\n",
      "epoch:19 step:18233 [D loss: 0.596381, acc: 71.09%] [G loss: 2.182520]\n",
      "epoch:19 step:18234 [D loss: 0.650913, acc: 63.28%] [G loss: 1.847286]\n",
      "epoch:19 step:18235 [D loss: 0.667059, acc: 56.25%] [G loss: 1.788135]\n",
      "epoch:19 step:18236 [D loss: 0.668241, acc: 60.16%] [G loss: 1.718837]\n",
      "epoch:19 step:18237 [D loss: 0.618694, acc: 64.06%] [G loss: 2.084137]\n",
      "epoch:19 step:18238 [D loss: 0.602754, acc: 68.75%] [G loss: 1.979778]\n",
      "epoch:19 step:18239 [D loss: 0.661244, acc: 60.94%] [G loss: 1.954652]\n",
      "epoch:19 step:18240 [D loss: 0.709252, acc: 58.59%] [G loss: 1.807862]\n",
      "epoch:19 step:18241 [D loss: 0.675847, acc: 57.03%] [G loss: 1.785230]\n",
      "epoch:19 step:18242 [D loss: 0.641789, acc: 60.94%] [G loss: 1.716492]\n",
      "epoch:19 step:18243 [D loss: 0.681910, acc: 53.91%] [G loss: 1.784553]\n",
      "epoch:19 step:18244 [D loss: 0.628414, acc: 60.94%] [G loss: 1.814237]\n",
      "epoch:19 step:18245 [D loss: 0.660875, acc: 60.16%] [G loss: 1.750476]\n",
      "epoch:19 step:18246 [D loss: 0.643698, acc: 64.06%] [G loss: 1.756821]\n",
      "epoch:19 step:18247 [D loss: 0.661802, acc: 55.47%] [G loss: 1.713099]\n",
      "epoch:19 step:18248 [D loss: 0.647163, acc: 62.50%] [G loss: 1.856936]\n",
      "epoch:19 step:18249 [D loss: 0.598963, acc: 67.97%] [G loss: 1.830083]\n",
      "epoch:19 step:18250 [D loss: 0.662512, acc: 57.81%] [G loss: 1.816881]\n",
      "epoch:19 step:18251 [D loss: 0.647994, acc: 59.38%] [G loss: 1.702578]\n",
      "epoch:19 step:18252 [D loss: 0.659102, acc: 60.94%] [G loss: 1.943187]\n",
      "epoch:19 step:18253 [D loss: 0.614752, acc: 71.09%] [G loss: 1.852042]\n",
      "epoch:19 step:18254 [D loss: 0.621159, acc: 62.50%] [G loss: 1.943552]\n",
      "epoch:19 step:18255 [D loss: 0.672099, acc: 54.69%] [G loss: 1.830190]\n",
      "epoch:19 step:18256 [D loss: 0.582508, acc: 72.66%] [G loss: 1.993357]\n",
      "epoch:19 step:18257 [D loss: 0.682651, acc: 56.25%] [G loss: 1.985789]\n",
      "epoch:19 step:18258 [D loss: 0.652413, acc: 64.06%] [G loss: 1.939461]\n",
      "epoch:19 step:18259 [D loss: 0.632255, acc: 62.50%] [G loss: 1.917443]\n",
      "epoch:19 step:18260 [D loss: 0.615481, acc: 65.62%] [G loss: 1.916541]\n",
      "epoch:19 step:18261 [D loss: 0.734149, acc: 55.47%] [G loss: 1.923613]\n",
      "epoch:19 step:18262 [D loss: 0.681066, acc: 53.12%] [G loss: 1.771959]\n",
      "epoch:19 step:18263 [D loss: 0.680463, acc: 53.12%] [G loss: 1.745613]\n",
      "epoch:19 step:18264 [D loss: 0.694587, acc: 60.16%] [G loss: 1.884122]\n",
      "epoch:19 step:18265 [D loss: 0.648098, acc: 64.06%] [G loss: 1.866299]\n",
      "epoch:19 step:18266 [D loss: 0.656988, acc: 62.50%] [G loss: 1.851203]\n",
      "epoch:19 step:18267 [D loss: 0.626379, acc: 64.84%] [G loss: 1.799621]\n",
      "epoch:19 step:18268 [D loss: 0.686161, acc: 60.16%] [G loss: 1.904285]\n",
      "epoch:19 step:18269 [D loss: 0.627827, acc: 64.84%] [G loss: 1.850195]\n",
      "epoch:19 step:18270 [D loss: 0.670613, acc: 65.62%] [G loss: 1.884773]\n",
      "epoch:19 step:18271 [D loss: 0.590847, acc: 64.84%] [G loss: 2.026913]\n",
      "epoch:19 step:18272 [D loss: 0.588206, acc: 68.75%] [G loss: 2.096799]\n",
      "epoch:19 step:18273 [D loss: 0.602650, acc: 67.19%] [G loss: 2.119130]\n",
      "epoch:19 step:18274 [D loss: 0.578106, acc: 65.62%] [G loss: 2.395061]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:19 step:18275 [D loss: 0.626501, acc: 68.75%] [G loss: 2.218304]\n",
      "epoch:19 step:18276 [D loss: 0.724130, acc: 55.47%] [G loss: 1.711248]\n",
      "epoch:19 step:18277 [D loss: 0.713264, acc: 53.91%] [G loss: 1.894224]\n",
      "epoch:19 step:18278 [D loss: 0.734326, acc: 57.03%] [G loss: 1.748183]\n",
      "epoch:19 step:18279 [D loss: 0.679582, acc: 56.25%] [G loss: 1.865994]\n",
      "epoch:19 step:18280 [D loss: 0.660807, acc: 59.38%] [G loss: 1.734234]\n",
      "epoch:19 step:18281 [D loss: 0.684449, acc: 56.25%] [G loss: 1.759423]\n",
      "epoch:19 step:18282 [D loss: 0.668158, acc: 63.28%] [G loss: 1.825699]\n",
      "epoch:19 step:18283 [D loss: 0.657398, acc: 60.16%] [G loss: 1.963231]\n",
      "epoch:19 step:18284 [D loss: 0.587383, acc: 65.62%] [G loss: 2.066200]\n",
      "epoch:19 step:18285 [D loss: 0.675806, acc: 55.47%] [G loss: 1.823697]\n",
      "epoch:19 step:18286 [D loss: 0.678660, acc: 64.84%] [G loss: 1.832137]\n",
      "epoch:19 step:18287 [D loss: 0.628510, acc: 67.19%] [G loss: 1.765592]\n",
      "epoch:19 step:18288 [D loss: 0.664268, acc: 63.28%] [G loss: 1.786587]\n",
      "epoch:19 step:18289 [D loss: 0.644132, acc: 60.94%] [G loss: 1.890351]\n",
      "epoch:19 step:18290 [D loss: 0.628987, acc: 61.72%] [G loss: 1.873040]\n",
      "epoch:19 step:18291 [D loss: 0.616395, acc: 68.75%] [G loss: 1.998388]\n",
      "epoch:19 step:18292 [D loss: 0.656904, acc: 64.84%] [G loss: 1.842625]\n",
      "epoch:19 step:18293 [D loss: 0.633092, acc: 63.28%] [G loss: 1.920073]\n",
      "epoch:19 step:18294 [D loss: 0.647710, acc: 60.94%] [G loss: 1.892636]\n",
      "epoch:19 step:18295 [D loss: 0.651587, acc: 62.50%] [G loss: 1.735750]\n",
      "epoch:19 step:18296 [D loss: 0.659029, acc: 60.94%] [G loss: 1.878560]\n",
      "epoch:19 step:18297 [D loss: 0.588912, acc: 68.75%] [G loss: 2.018454]\n",
      "epoch:19 step:18298 [D loss: 0.645416, acc: 62.50%] [G loss: 1.977828]\n",
      "epoch:19 step:18299 [D loss: 0.633217, acc: 63.28%] [G loss: 1.827221]\n",
      "epoch:19 step:18300 [D loss: 0.641123, acc: 64.06%] [G loss: 1.908413]\n",
      "epoch:19 step:18301 [D loss: 0.612215, acc: 62.50%] [G loss: 1.970407]\n",
      "epoch:19 step:18302 [D loss: 0.590119, acc: 68.75%] [G loss: 2.143548]\n",
      "epoch:19 step:18303 [D loss: 0.709559, acc: 51.56%] [G loss: 1.921828]\n",
      "epoch:19 step:18304 [D loss: 0.696097, acc: 56.25%] [G loss: 1.830909]\n",
      "epoch:19 step:18305 [D loss: 0.706428, acc: 52.34%] [G loss: 1.784768]\n",
      "epoch:19 step:18306 [D loss: 0.637105, acc: 65.62%] [G loss: 1.899553]\n",
      "epoch:19 step:18307 [D loss: 0.644399, acc: 66.41%] [G loss: 2.050902]\n",
      "epoch:19 step:18308 [D loss: 0.659770, acc: 57.03%] [G loss: 1.954699]\n",
      "epoch:19 step:18309 [D loss: 0.720711, acc: 53.12%] [G loss: 1.775103]\n",
      "epoch:19 step:18310 [D loss: 0.686876, acc: 58.59%] [G loss: 1.765096]\n",
      "epoch:19 step:18311 [D loss: 0.623406, acc: 67.19%] [G loss: 2.083050]\n",
      "epoch:19 step:18312 [D loss: 0.685092, acc: 57.03%] [G loss: 1.836615]\n",
      "epoch:19 step:18313 [D loss: 0.672934, acc: 60.94%] [G loss: 1.845089]\n",
      "epoch:19 step:18314 [D loss: 0.666470, acc: 66.41%] [G loss: 1.739472]\n",
      "epoch:19 step:18315 [D loss: 0.663919, acc: 58.59%] [G loss: 1.805428]\n",
      "epoch:19 step:18316 [D loss: 0.607205, acc: 62.50%] [G loss: 1.942713]\n",
      "epoch:19 step:18317 [D loss: 0.604912, acc: 71.09%] [G loss: 1.974777]\n",
      "epoch:19 step:18318 [D loss: 0.671182, acc: 64.06%] [G loss: 1.827658]\n",
      "epoch:19 step:18319 [D loss: 0.576338, acc: 67.97%] [G loss: 1.931703]\n",
      "epoch:19 step:18320 [D loss: 0.620122, acc: 66.41%] [G loss: 2.023063]\n",
      "epoch:19 step:18321 [D loss: 0.657967, acc: 60.16%] [G loss: 1.810884]\n",
      "epoch:19 step:18322 [D loss: 0.632103, acc: 59.38%] [G loss: 1.954241]\n",
      "epoch:19 step:18323 [D loss: 0.616395, acc: 65.62%] [G loss: 1.889589]\n",
      "epoch:19 step:18324 [D loss: 0.618855, acc: 68.75%] [G loss: 1.964929]\n",
      "epoch:19 step:18325 [D loss: 0.627343, acc: 67.19%] [G loss: 2.129180]\n",
      "epoch:19 step:18326 [D loss: 0.596985, acc: 67.19%] [G loss: 2.063004]\n",
      "epoch:19 step:18327 [D loss: 0.619850, acc: 67.19%] [G loss: 2.057988]\n",
      "epoch:19 step:18328 [D loss: 0.676948, acc: 64.06%] [G loss: 1.815467]\n",
      "epoch:19 step:18329 [D loss: 0.652406, acc: 61.72%] [G loss: 1.856052]\n",
      "epoch:19 step:18330 [D loss: 0.708902, acc: 55.47%] [G loss: 1.818317]\n",
      "epoch:19 step:18331 [D loss: 0.670639, acc: 64.84%] [G loss: 1.786916]\n",
      "epoch:19 step:18332 [D loss: 0.721864, acc: 54.69%] [G loss: 1.703841]\n",
      "epoch:19 step:18333 [D loss: 0.721775, acc: 51.56%] [G loss: 1.742055]\n",
      "epoch:19 step:18334 [D loss: 0.628130, acc: 65.62%] [G loss: 1.713850]\n",
      "epoch:19 step:18335 [D loss: 0.660096, acc: 65.62%] [G loss: 1.983147]\n",
      "epoch:19 step:18336 [D loss: 0.667649, acc: 57.81%] [G loss: 1.890711]\n",
      "epoch:19 step:18337 [D loss: 0.670536, acc: 60.16%] [G loss: 1.953715]\n",
      "epoch:19 step:18338 [D loss: 0.698696, acc: 54.69%] [G loss: 1.832095]\n",
      "epoch:19 step:18339 [D loss: 0.610871, acc: 72.66%] [G loss: 2.015325]\n",
      "epoch:19 step:18340 [D loss: 0.636552, acc: 64.06%] [G loss: 1.749493]\n",
      "epoch:19 step:18341 [D loss: 0.632203, acc: 64.06%] [G loss: 1.869970]\n",
      "epoch:19 step:18342 [D loss: 0.660958, acc: 57.03%] [G loss: 1.863917]\n",
      "epoch:19 step:18343 [D loss: 0.667362, acc: 57.81%] [G loss: 1.870157]\n",
      "epoch:19 step:18344 [D loss: 0.632348, acc: 60.16%] [G loss: 1.793111]\n",
      "epoch:19 step:18345 [D loss: 0.715892, acc: 57.81%] [G loss: 1.921742]\n",
      "epoch:19 step:18346 [D loss: 0.607378, acc: 62.50%] [G loss: 1.865574]\n",
      "epoch:19 step:18347 [D loss: 0.648166, acc: 61.72%] [G loss: 1.869305]\n",
      "epoch:19 step:18348 [D loss: 0.602642, acc: 62.50%] [G loss: 1.947409]\n",
      "epoch:19 step:18349 [D loss: 0.647374, acc: 64.06%] [G loss: 1.871626]\n",
      "epoch:19 step:18350 [D loss: 0.643466, acc: 60.16%] [G loss: 1.934695]\n",
      "epoch:19 step:18351 [D loss: 0.647555, acc: 58.59%] [G loss: 1.833874]\n",
      "epoch:19 step:18352 [D loss: 0.587413, acc: 72.66%] [G loss: 1.954149]\n",
      "epoch:19 step:18353 [D loss: 0.636180, acc: 67.19%] [G loss: 1.906624]\n",
      "epoch:19 step:18354 [D loss: 0.634459, acc: 62.50%] [G loss: 2.077785]\n",
      "epoch:19 step:18355 [D loss: 0.643043, acc: 64.84%] [G loss: 1.913949]\n",
      "epoch:19 step:18356 [D loss: 0.669783, acc: 60.16%] [G loss: 1.854272]\n",
      "epoch:19 step:18357 [D loss: 0.564615, acc: 72.66%] [G loss: 2.051159]\n",
      "epoch:19 step:18358 [D loss: 0.624629, acc: 73.44%] [G loss: 1.981060]\n",
      "epoch:19 step:18359 [D loss: 0.567989, acc: 71.09%] [G loss: 2.114100]\n",
      "epoch:19 step:18360 [D loss: 0.654498, acc: 60.16%] [G loss: 2.093781]\n",
      "epoch:19 step:18361 [D loss: 0.659527, acc: 62.50%] [G loss: 2.000056]\n",
      "epoch:19 step:18362 [D loss: 0.673990, acc: 52.34%] [G loss: 1.777512]\n",
      "epoch:19 step:18363 [D loss: 0.667988, acc: 56.25%] [G loss: 1.929992]\n",
      "epoch:19 step:18364 [D loss: 0.618499, acc: 67.97%] [G loss: 2.006470]\n",
      "epoch:19 step:18365 [D loss: 0.663952, acc: 57.03%] [G loss: 1.898947]\n",
      "epoch:19 step:18366 [D loss: 0.616475, acc: 69.53%] [G loss: 1.969330]\n",
      "epoch:19 step:18367 [D loss: 0.576602, acc: 69.53%] [G loss: 2.128385]\n",
      "epoch:19 step:18368 [D loss: 0.645715, acc: 64.06%] [G loss: 1.829896]\n",
      "epoch:19 step:18369 [D loss: 0.724870, acc: 53.91%] [G loss: 1.879243]\n",
      "epoch:19 step:18370 [D loss: 0.645366, acc: 60.94%] [G loss: 1.837765]\n",
      "epoch:19 step:18371 [D loss: 0.643951, acc: 64.84%] [G loss: 1.711302]\n",
      "epoch:19 step:18372 [D loss: 0.594290, acc: 63.28%] [G loss: 1.831726]\n",
      "epoch:19 step:18373 [D loss: 0.682041, acc: 59.38%] [G loss: 1.907740]\n",
      "epoch:19 step:18374 [D loss: 0.672210, acc: 64.84%] [G loss: 1.971781]\n",
      "epoch:19 step:18375 [D loss: 0.684549, acc: 55.47%] [G loss: 1.769312]\n",
      "epoch:19 step:18376 [D loss: 0.608120, acc: 65.62%] [G loss: 1.850497]\n",
      "epoch:19 step:18377 [D loss: 0.633356, acc: 63.28%] [G loss: 1.890649]\n",
      "epoch:19 step:18378 [D loss: 0.617497, acc: 64.84%] [G loss: 1.765176]\n",
      "epoch:19 step:18379 [D loss: 0.639296, acc: 63.28%] [G loss: 1.904066]\n",
      "epoch:19 step:18380 [D loss: 0.642711, acc: 60.94%] [G loss: 1.803853]\n",
      "epoch:19 step:18381 [D loss: 0.691138, acc: 56.25%] [G loss: 1.950501]\n",
      "epoch:19 step:18382 [D loss: 0.662086, acc: 61.72%] [G loss: 1.894365]\n",
      "epoch:19 step:18383 [D loss: 0.633730, acc: 64.84%] [G loss: 1.852647]\n",
      "epoch:19 step:18384 [D loss: 0.636369, acc: 64.84%] [G loss: 1.724678]\n",
      "epoch:19 step:18385 [D loss: 0.609344, acc: 69.53%] [G loss: 2.014887]\n",
      "epoch:19 step:18386 [D loss: 0.633378, acc: 62.50%] [G loss: 1.866543]\n",
      "epoch:19 step:18387 [D loss: 0.660327, acc: 60.16%] [G loss: 2.014953]\n",
      "epoch:19 step:18388 [D loss: 0.679267, acc: 57.03%] [G loss: 1.894775]\n",
      "epoch:19 step:18389 [D loss: 0.686742, acc: 60.16%] [G loss: 1.789124]\n",
      "epoch:19 step:18390 [D loss: 0.648805, acc: 61.72%] [G loss: 1.911857]\n",
      "epoch:19 step:18391 [D loss: 0.593946, acc: 73.44%] [G loss: 1.842088]\n",
      "epoch:19 step:18392 [D loss: 0.612266, acc: 66.41%] [G loss: 2.069183]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:19 step:18393 [D loss: 0.737689, acc: 48.44%] [G loss: 1.912469]\n",
      "epoch:19 step:18394 [D loss: 0.652246, acc: 61.72%] [G loss: 1.835539]\n",
      "epoch:19 step:18395 [D loss: 0.643080, acc: 64.84%] [G loss: 1.886389]\n",
      "epoch:19 step:18396 [D loss: 0.693467, acc: 57.03%] [G loss: 1.830771]\n",
      "epoch:19 step:18397 [D loss: 0.625384, acc: 60.16%] [G loss: 1.837853]\n",
      "epoch:19 step:18398 [D loss: 0.677961, acc: 57.81%] [G loss: 1.802894]\n",
      "epoch:19 step:18399 [D loss: 0.647145, acc: 64.06%] [G loss: 1.808281]\n",
      "epoch:19 step:18400 [D loss: 0.596471, acc: 67.19%] [G loss: 1.900699]\n",
      "##############\n",
      "[2.29516091 1.33627677 6.30203542 4.75408295 3.6498523  5.69153928\n",
      " 4.4533535  4.62645853 4.61215584 3.69373108]\n",
      "##########\n",
      "epoch:19 step:18401 [D loss: 0.690851, acc: 60.16%] [G loss: 1.975456]\n",
      "epoch:19 step:18402 [D loss: 0.655217, acc: 60.94%] [G loss: 1.881404]\n",
      "epoch:19 step:18403 [D loss: 0.603464, acc: 68.75%] [G loss: 1.857203]\n",
      "epoch:19 step:18404 [D loss: 0.694030, acc: 56.25%] [G loss: 1.880833]\n",
      "epoch:19 step:18405 [D loss: 0.699086, acc: 57.03%] [G loss: 1.845666]\n",
      "epoch:19 step:18406 [D loss: 0.662905, acc: 57.81%] [G loss: 1.772760]\n",
      "epoch:19 step:18407 [D loss: 0.622578, acc: 62.50%] [G loss: 1.980480]\n",
      "epoch:19 step:18408 [D loss: 0.658013, acc: 58.59%] [G loss: 1.963236]\n",
      "epoch:19 step:18409 [D loss: 0.635759, acc: 60.94%] [G loss: 1.771478]\n",
      "epoch:19 step:18410 [D loss: 0.653344, acc: 58.59%] [G loss: 2.035269]\n",
      "epoch:19 step:18411 [D loss: 0.623741, acc: 64.84%] [G loss: 1.860232]\n",
      "epoch:19 step:18412 [D loss: 0.618130, acc: 66.41%] [G loss: 1.956556]\n",
      "epoch:19 step:18413 [D loss: 0.661675, acc: 64.06%] [G loss: 1.867020]\n",
      "epoch:19 step:18414 [D loss: 0.651913, acc: 64.06%] [G loss: 1.920555]\n",
      "epoch:19 step:18415 [D loss: 0.657941, acc: 65.62%] [G loss: 1.791755]\n",
      "epoch:19 step:18416 [D loss: 0.637308, acc: 60.94%] [G loss: 1.995373]\n",
      "epoch:19 step:18417 [D loss: 0.656422, acc: 54.69%] [G loss: 1.752440]\n",
      "epoch:19 step:18418 [D loss: 0.695914, acc: 56.25%] [G loss: 1.883607]\n",
      "epoch:19 step:18419 [D loss: 0.682230, acc: 53.12%] [G loss: 1.783976]\n",
      "epoch:19 step:18420 [D loss: 0.676817, acc: 54.69%] [G loss: 1.854403]\n",
      "epoch:19 step:18421 [D loss: 0.608336, acc: 69.53%] [G loss: 1.952711]\n",
      "epoch:19 step:18422 [D loss: 0.649250, acc: 61.72%] [G loss: 1.861653]\n",
      "epoch:19 step:18423 [D loss: 0.628265, acc: 65.62%] [G loss: 1.922098]\n",
      "epoch:19 step:18424 [D loss: 0.633118, acc: 68.75%] [G loss: 2.002088]\n",
      "epoch:19 step:18425 [D loss: 0.634255, acc: 58.59%] [G loss: 1.949615]\n",
      "epoch:19 step:18426 [D loss: 0.648191, acc: 64.84%] [G loss: 2.005525]\n",
      "epoch:19 step:18427 [D loss: 0.629287, acc: 65.62%] [G loss: 2.090117]\n",
      "epoch:19 step:18428 [D loss: 0.706584, acc: 53.91%] [G loss: 1.771326]\n",
      "epoch:19 step:18429 [D loss: 0.687456, acc: 60.94%] [G loss: 1.798949]\n",
      "epoch:19 step:18430 [D loss: 0.620045, acc: 66.41%] [G loss: 1.780634]\n",
      "epoch:19 step:18431 [D loss: 0.625670, acc: 62.50%] [G loss: 1.741669]\n",
      "epoch:19 step:18432 [D loss: 0.624152, acc: 67.19%] [G loss: 1.919889]\n",
      "epoch:19 step:18433 [D loss: 0.677712, acc: 64.06%] [G loss: 1.856782]\n",
      "epoch:19 step:18434 [D loss: 0.675533, acc: 57.81%] [G loss: 1.937175]\n",
      "epoch:19 step:18435 [D loss: 0.679734, acc: 55.47%] [G loss: 1.852282]\n",
      "epoch:19 step:18436 [D loss: 0.662745, acc: 57.81%] [G loss: 1.919384]\n",
      "epoch:19 step:18437 [D loss: 0.651353, acc: 60.94%] [G loss: 1.922468]\n",
      "epoch:19 step:18438 [D loss: 0.634327, acc: 63.28%] [G loss: 1.935344]\n",
      "epoch:19 step:18439 [D loss: 0.640301, acc: 60.94%] [G loss: 1.973315]\n",
      "epoch:19 step:18440 [D loss: 0.653866, acc: 62.50%] [G loss: 2.008041]\n",
      "epoch:19 step:18441 [D loss: 0.672381, acc: 64.84%] [G loss: 1.838913]\n",
      "epoch:19 step:18442 [D loss: 0.651931, acc: 62.50%] [G loss: 1.955792]\n",
      "epoch:19 step:18443 [D loss: 0.631877, acc: 70.31%] [G loss: 1.912202]\n",
      "epoch:19 step:18444 [D loss: 0.616716, acc: 64.84%] [G loss: 1.826310]\n",
      "epoch:19 step:18445 [D loss: 0.613193, acc: 65.62%] [G loss: 1.942560]\n",
      "epoch:19 step:18446 [D loss: 0.619812, acc: 66.41%] [G loss: 1.942357]\n",
      "epoch:19 step:18447 [D loss: 0.599894, acc: 67.97%] [G loss: 2.061432]\n",
      "epoch:19 step:18448 [D loss: 0.629873, acc: 67.97%] [G loss: 1.934748]\n",
      "epoch:19 step:18449 [D loss: 0.559458, acc: 70.31%] [G loss: 1.875730]\n",
      "epoch:19 step:18450 [D loss: 0.653940, acc: 63.28%] [G loss: 2.158833]\n",
      "epoch:19 step:18451 [D loss: 0.575382, acc: 64.84%] [G loss: 2.366581]\n",
      "epoch:19 step:18452 [D loss: 0.574373, acc: 73.44%] [G loss: 2.153095]\n",
      "epoch:19 step:18453 [D loss: 0.615961, acc: 65.62%] [G loss: 2.072247]\n",
      "epoch:19 step:18454 [D loss: 0.642591, acc: 61.72%] [G loss: 1.910559]\n",
      "epoch:19 step:18455 [D loss: 0.671972, acc: 62.50%] [G loss: 1.886712]\n",
      "epoch:19 step:18456 [D loss: 0.606184, acc: 64.06%] [G loss: 2.023483]\n",
      "epoch:19 step:18457 [D loss: 0.633132, acc: 64.06%] [G loss: 2.189640]\n",
      "epoch:19 step:18458 [D loss: 0.670736, acc: 64.06%] [G loss: 1.839937]\n",
      "epoch:19 step:18459 [D loss: 0.626994, acc: 64.06%] [G loss: 1.961087]\n",
      "epoch:19 step:18460 [D loss: 0.682399, acc: 57.03%] [G loss: 1.796692]\n",
      "epoch:19 step:18461 [D loss: 0.738703, acc: 47.66%] [G loss: 1.726722]\n",
      "epoch:19 step:18462 [D loss: 0.665534, acc: 57.03%] [G loss: 1.809967]\n",
      "epoch:19 step:18463 [D loss: 0.587096, acc: 69.53%] [G loss: 1.833267]\n",
      "epoch:19 step:18464 [D loss: 0.644180, acc: 65.62%] [G loss: 1.948962]\n",
      "epoch:19 step:18465 [D loss: 0.654910, acc: 58.59%] [G loss: 1.892107]\n",
      "epoch:19 step:18466 [D loss: 0.661734, acc: 62.50%] [G loss: 1.946456]\n",
      "epoch:19 step:18467 [D loss: 0.659957, acc: 60.94%] [G loss: 1.969152]\n",
      "epoch:19 step:18468 [D loss: 0.688865, acc: 57.03%] [G loss: 1.859918]\n",
      "epoch:19 step:18469 [D loss: 0.620812, acc: 71.09%] [G loss: 1.916981]\n",
      "epoch:19 step:18470 [D loss: 0.674995, acc: 57.81%] [G loss: 1.812498]\n",
      "epoch:19 step:18471 [D loss: 0.637003, acc: 64.84%] [G loss: 1.878818]\n",
      "epoch:19 step:18472 [D loss: 0.644592, acc: 66.41%] [G loss: 1.797833]\n",
      "epoch:19 step:18473 [D loss: 0.630996, acc: 70.31%] [G loss: 1.903342]\n",
      "epoch:19 step:18474 [D loss: 0.662701, acc: 60.16%] [G loss: 1.779436]\n",
      "epoch:19 step:18475 [D loss: 0.652246, acc: 59.38%] [G loss: 1.902117]\n",
      "epoch:19 step:18476 [D loss: 0.608390, acc: 65.62%] [G loss: 1.926985]\n",
      "epoch:19 step:18477 [D loss: 0.656673, acc: 59.38%] [G loss: 1.931586]\n",
      "epoch:19 step:18478 [D loss: 0.632011, acc: 63.28%] [G loss: 1.848726]\n",
      "epoch:19 step:18479 [D loss: 0.624254, acc: 64.06%] [G loss: 1.858066]\n",
      "epoch:19 step:18480 [D loss: 0.594334, acc: 68.75%] [G loss: 1.932018]\n",
      "epoch:19 step:18481 [D loss: 0.672586, acc: 58.59%] [G loss: 1.977107]\n",
      "epoch:19 step:18482 [D loss: 0.620235, acc: 68.75%] [G loss: 1.942518]\n",
      "epoch:19 step:18483 [D loss: 0.664457, acc: 60.94%] [G loss: 2.027582]\n",
      "epoch:19 step:18484 [D loss: 0.694767, acc: 57.81%] [G loss: 2.138987]\n",
      "epoch:19 step:18485 [D loss: 0.665517, acc: 60.16%] [G loss: 1.827680]\n",
      "epoch:19 step:18486 [D loss: 0.635297, acc: 58.59%] [G loss: 1.856175]\n",
      "epoch:19 step:18487 [D loss: 0.640223, acc: 64.06%] [G loss: 1.817501]\n",
      "epoch:19 step:18488 [D loss: 0.641421, acc: 65.62%] [G loss: 1.993734]\n",
      "epoch:19 step:18489 [D loss: 0.639155, acc: 64.84%] [G loss: 1.937909]\n",
      "epoch:19 step:18490 [D loss: 0.641310, acc: 63.28%] [G loss: 1.898981]\n",
      "epoch:19 step:18491 [D loss: 0.622445, acc: 63.28%] [G loss: 2.043600]\n",
      "epoch:19 step:18492 [D loss: 0.646416, acc: 64.84%] [G loss: 1.975584]\n",
      "epoch:19 step:18493 [D loss: 0.589616, acc: 71.88%] [G loss: 2.122137]\n",
      "epoch:19 step:18494 [D loss: 0.629867, acc: 66.41%] [G loss: 2.111811]\n",
      "epoch:19 step:18495 [D loss: 0.696661, acc: 67.19%] [G loss: 1.940328]\n",
      "epoch:19 step:18496 [D loss: 0.593476, acc: 69.53%] [G loss: 2.108900]\n",
      "epoch:19 step:18497 [D loss: 0.605509, acc: 64.84%] [G loss: 2.153826]\n",
      "epoch:19 step:18498 [D loss: 0.551286, acc: 74.22%] [G loss: 1.971457]\n",
      "epoch:19 step:18499 [D loss: 0.668057, acc: 60.94%] [G loss: 1.897287]\n",
      "epoch:19 step:18500 [D loss: 0.629687, acc: 64.06%] [G loss: 2.073200]\n",
      "epoch:19 step:18501 [D loss: 0.684787, acc: 54.69%] [G loss: 1.957887]\n",
      "epoch:19 step:18502 [D loss: 0.635396, acc: 64.06%] [G loss: 2.043283]\n",
      "epoch:19 step:18503 [D loss: 0.630735, acc: 65.62%] [G loss: 1.957060]\n",
      "epoch:19 step:18504 [D loss: 0.637037, acc: 67.97%] [G loss: 2.034381]\n",
      "epoch:19 step:18505 [D loss: 0.662132, acc: 60.94%] [G loss: 2.018120]\n",
      "epoch:19 step:18506 [D loss: 0.693524, acc: 56.25%] [G loss: 1.936581]\n",
      "epoch:19 step:18507 [D loss: 0.626311, acc: 65.62%] [G loss: 1.923913]\n",
      "epoch:19 step:18508 [D loss: 0.677542, acc: 53.12%] [G loss: 1.945834]\n",
      "epoch:19 step:18509 [D loss: 0.654457, acc: 60.94%] [G loss: 1.987531]\n",
      "epoch:19 step:18510 [D loss: 0.638077, acc: 70.31%] [G loss: 1.970039]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:19 step:18511 [D loss: 0.559787, acc: 74.22%] [G loss: 1.971739]\n",
      "epoch:19 step:18512 [D loss: 0.619814, acc: 70.31%] [G loss: 2.014655]\n",
      "epoch:19 step:18513 [D loss: 0.649864, acc: 63.28%] [G loss: 1.922763]\n",
      "epoch:19 step:18514 [D loss: 0.656466, acc: 60.16%] [G loss: 1.993663]\n",
      "epoch:19 step:18515 [D loss: 0.724831, acc: 53.91%] [G loss: 1.858594]\n",
      "epoch:19 step:18516 [D loss: 0.628717, acc: 59.38%] [G loss: 1.961438]\n",
      "epoch:19 step:18517 [D loss: 0.681243, acc: 57.81%] [G loss: 1.940745]\n",
      "epoch:19 step:18518 [D loss: 0.612700, acc: 63.28%] [G loss: 1.898466]\n",
      "epoch:19 step:18519 [D loss: 0.680895, acc: 60.16%] [G loss: 1.846894]\n",
      "epoch:19 step:18520 [D loss: 0.664130, acc: 57.81%] [G loss: 1.965126]\n",
      "epoch:19 step:18521 [D loss: 0.653650, acc: 54.69%] [G loss: 2.060606]\n",
      "epoch:19 step:18522 [D loss: 0.578151, acc: 71.88%] [G loss: 2.022306]\n",
      "epoch:19 step:18523 [D loss: 0.674436, acc: 58.59%] [G loss: 1.903730]\n",
      "epoch:19 step:18524 [D loss: 0.612441, acc: 69.53%] [G loss: 1.998511]\n",
      "epoch:19 step:18525 [D loss: 0.681133, acc: 58.59%] [G loss: 1.982449]\n",
      "epoch:19 step:18526 [D loss: 0.689222, acc: 56.25%] [G loss: 1.769139]\n",
      "epoch:19 step:18527 [D loss: 0.617412, acc: 64.06%] [G loss: 1.844714]\n",
      "epoch:19 step:18528 [D loss: 0.618259, acc: 65.62%] [G loss: 1.902586]\n",
      "epoch:19 step:18529 [D loss: 0.700466, acc: 53.12%] [G loss: 1.868789]\n",
      "epoch:19 step:18530 [D loss: 0.617840, acc: 66.41%] [G loss: 2.005430]\n",
      "epoch:19 step:18531 [D loss: 0.619930, acc: 70.31%] [G loss: 1.951431]\n",
      "epoch:19 step:18532 [D loss: 0.636427, acc: 64.06%] [G loss: 1.774299]\n",
      "epoch:19 step:18533 [D loss: 0.632855, acc: 62.50%] [G loss: 1.949957]\n",
      "epoch:19 step:18534 [D loss: 0.677140, acc: 57.81%] [G loss: 1.842390]\n",
      "epoch:19 step:18535 [D loss: 0.636719, acc: 67.19%] [G loss: 2.032198]\n",
      "epoch:19 step:18536 [D loss: 0.688478, acc: 62.50%] [G loss: 1.929506]\n",
      "epoch:19 step:18537 [D loss: 0.606831, acc: 71.09%] [G loss: 1.759994]\n",
      "epoch:19 step:18538 [D loss: 0.676029, acc: 58.59%] [G loss: 1.926180]\n",
      "epoch:19 step:18539 [D loss: 0.573057, acc: 72.66%] [G loss: 1.912339]\n",
      "epoch:19 step:18540 [D loss: 0.599736, acc: 64.84%] [G loss: 1.839459]\n",
      "epoch:19 step:18541 [D loss: 0.613035, acc: 64.06%] [G loss: 1.895648]\n",
      "epoch:19 step:18542 [D loss: 0.658107, acc: 57.03%] [G loss: 1.886592]\n",
      "epoch:19 step:18543 [D loss: 0.698565, acc: 60.94%] [G loss: 1.835931]\n",
      "epoch:19 step:18544 [D loss: 0.650480, acc: 64.06%] [G loss: 1.775078]\n",
      "epoch:19 step:18545 [D loss: 0.705959, acc: 60.16%] [G loss: 1.820650]\n",
      "epoch:19 step:18546 [D loss: 0.661939, acc: 60.16%] [G loss: 1.874485]\n",
      "epoch:19 step:18547 [D loss: 0.694371, acc: 60.94%] [G loss: 1.789673]\n",
      "epoch:19 step:18548 [D loss: 0.670100, acc: 58.59%] [G loss: 1.822211]\n",
      "epoch:19 step:18549 [D loss: 0.649618, acc: 60.16%] [G loss: 1.863351]\n",
      "epoch:19 step:18550 [D loss: 0.620309, acc: 68.75%] [G loss: 1.951664]\n",
      "epoch:19 step:18551 [D loss: 0.680250, acc: 60.16%] [G loss: 1.930497]\n",
      "epoch:19 step:18552 [D loss: 0.642725, acc: 64.06%] [G loss: 1.735503]\n",
      "epoch:19 step:18553 [D loss: 0.630881, acc: 64.06%] [G loss: 1.812186]\n",
      "epoch:19 step:18554 [D loss: 0.655416, acc: 57.03%] [G loss: 1.758194]\n",
      "epoch:19 step:18555 [D loss: 0.670089, acc: 57.03%] [G loss: 1.745975]\n",
      "epoch:19 step:18556 [D loss: 0.671047, acc: 53.12%] [G loss: 1.864011]\n",
      "epoch:19 step:18557 [D loss: 0.604058, acc: 68.75%] [G loss: 1.985168]\n",
      "epoch:19 step:18558 [D loss: 0.662851, acc: 58.59%] [G loss: 1.797161]\n",
      "epoch:19 step:18559 [D loss: 0.685758, acc: 57.03%] [G loss: 1.781845]\n",
      "epoch:19 step:18560 [D loss: 0.602103, acc: 66.41%] [G loss: 1.951402]\n",
      "epoch:19 step:18561 [D loss: 0.690321, acc: 53.12%] [G loss: 1.717757]\n",
      "epoch:19 step:18562 [D loss: 0.640493, acc: 65.62%] [G loss: 1.807527]\n",
      "epoch:19 step:18563 [D loss: 0.671825, acc: 61.72%] [G loss: 1.890345]\n",
      "epoch:19 step:18564 [D loss: 0.635028, acc: 64.84%] [G loss: 1.921876]\n",
      "epoch:19 step:18565 [D loss: 0.655171, acc: 60.16%] [G loss: 1.894034]\n",
      "epoch:19 step:18566 [D loss: 0.655631, acc: 60.16%] [G loss: 1.836819]\n",
      "epoch:19 step:18567 [D loss: 0.646168, acc: 64.06%] [G loss: 1.903391]\n",
      "epoch:19 step:18568 [D loss: 0.687347, acc: 57.03%] [G loss: 1.785099]\n",
      "epoch:19 step:18569 [D loss: 0.675914, acc: 56.25%] [G loss: 1.755840]\n",
      "epoch:19 step:18570 [D loss: 0.670493, acc: 57.81%] [G loss: 1.817133]\n",
      "epoch:19 step:18571 [D loss: 0.689642, acc: 63.28%] [G loss: 1.794595]\n",
      "epoch:19 step:18572 [D loss: 0.661938, acc: 60.94%] [G loss: 1.892152]\n",
      "epoch:19 step:18573 [D loss: 0.624892, acc: 63.28%] [G loss: 1.972611]\n",
      "epoch:19 step:18574 [D loss: 0.677057, acc: 58.59%] [G loss: 1.884268]\n",
      "epoch:19 step:18575 [D loss: 0.628913, acc: 65.62%] [G loss: 1.873900]\n",
      "epoch:19 step:18576 [D loss: 0.637351, acc: 58.59%] [G loss: 1.935468]\n",
      "epoch:19 step:18577 [D loss: 0.607680, acc: 65.62%] [G loss: 1.928965]\n",
      "epoch:19 step:18578 [D loss: 0.629705, acc: 63.28%] [G loss: 2.144535]\n",
      "epoch:19 step:18579 [D loss: 0.632681, acc: 67.97%] [G loss: 1.970813]\n",
      "epoch:19 step:18580 [D loss: 0.621007, acc: 66.41%] [G loss: 1.926736]\n",
      "epoch:19 step:18581 [D loss: 0.621292, acc: 62.50%] [G loss: 1.889821]\n",
      "epoch:19 step:18582 [D loss: 0.665208, acc: 62.50%] [G loss: 1.941020]\n",
      "epoch:19 step:18583 [D loss: 0.609776, acc: 69.53%] [G loss: 1.948881]\n",
      "epoch:19 step:18584 [D loss: 0.601608, acc: 71.09%] [G loss: 2.154492]\n",
      "epoch:19 step:18585 [D loss: 0.600931, acc: 67.97%] [G loss: 2.096548]\n",
      "epoch:19 step:18586 [D loss: 0.674452, acc: 60.16%] [G loss: 2.117123]\n",
      "epoch:19 step:18587 [D loss: 0.653213, acc: 65.62%] [G loss: 1.800722]\n",
      "epoch:19 step:18588 [D loss: 0.600767, acc: 72.66%] [G loss: 1.949582]\n",
      "epoch:19 step:18589 [D loss: 0.572927, acc: 73.44%] [G loss: 2.073219]\n",
      "epoch:19 step:18590 [D loss: 0.616218, acc: 62.50%] [G loss: 1.987220]\n",
      "epoch:19 step:18591 [D loss: 0.697501, acc: 58.59%] [G loss: 1.925994]\n",
      "epoch:19 step:18592 [D loss: 0.661259, acc: 63.28%] [G loss: 2.016132]\n",
      "epoch:19 step:18593 [D loss: 0.631875, acc: 62.50%] [G loss: 1.911449]\n",
      "epoch:19 step:18594 [D loss: 0.616994, acc: 62.50%] [G loss: 1.991570]\n",
      "epoch:19 step:18595 [D loss: 0.643274, acc: 60.16%] [G loss: 2.120277]\n",
      "epoch:19 step:18596 [D loss: 0.603703, acc: 67.19%] [G loss: 1.979034]\n",
      "epoch:19 step:18597 [D loss: 0.715366, acc: 57.03%] [G loss: 1.728031]\n",
      "epoch:19 step:18598 [D loss: 0.691819, acc: 63.28%] [G loss: 1.794901]\n",
      "epoch:19 step:18599 [D loss: 0.684410, acc: 59.38%] [G loss: 1.822990]\n",
      "epoch:19 step:18600 [D loss: 0.676254, acc: 55.47%] [G loss: 1.846864]\n",
      "##############\n",
      "[2.40856166 1.50906527 6.43689031 4.79386456 3.55755347 5.6834734\n",
      " 4.44581342 4.65294218 4.63577815 3.87569859]\n",
      "##########\n",
      "epoch:19 step:18601 [D loss: 0.684885, acc: 55.47%] [G loss: 1.902097]\n",
      "epoch:19 step:18602 [D loss: 0.661087, acc: 62.50%] [G loss: 1.807454]\n",
      "epoch:19 step:18603 [D loss: 0.712822, acc: 52.34%] [G loss: 1.734133]\n",
      "epoch:19 step:18604 [D loss: 0.658370, acc: 64.84%] [G loss: 1.797903]\n",
      "epoch:19 step:18605 [D loss: 0.672304, acc: 53.91%] [G loss: 1.895867]\n",
      "epoch:19 step:18606 [D loss: 0.627426, acc: 64.84%] [G loss: 1.970792]\n",
      "epoch:19 step:18607 [D loss: 0.603801, acc: 67.19%] [G loss: 2.105975]\n",
      "epoch:19 step:18608 [D loss: 0.646917, acc: 66.41%] [G loss: 1.843427]\n",
      "epoch:19 step:18609 [D loss: 0.626347, acc: 67.19%] [G loss: 2.000332]\n",
      "epoch:19 step:18610 [D loss: 0.609845, acc: 64.84%] [G loss: 1.924773]\n",
      "epoch:19 step:18611 [D loss: 0.659847, acc: 60.94%] [G loss: 1.874864]\n",
      "epoch:19 step:18612 [D loss: 0.654415, acc: 58.59%] [G loss: 2.028726]\n",
      "epoch:19 step:18613 [D loss: 0.652840, acc: 58.59%] [G loss: 1.867626]\n",
      "epoch:19 step:18614 [D loss: 0.576463, acc: 71.88%] [G loss: 1.709951]\n",
      "epoch:19 step:18615 [D loss: 0.667911, acc: 60.94%] [G loss: 1.901136]\n",
      "epoch:19 step:18616 [D loss: 0.667222, acc: 62.50%] [G loss: 1.873871]\n",
      "epoch:19 step:18617 [D loss: 0.656911, acc: 59.38%] [G loss: 1.869804]\n",
      "epoch:19 step:18618 [D loss: 0.661780, acc: 62.50%] [G loss: 2.102046]\n",
      "epoch:19 step:18619 [D loss: 0.596454, acc: 66.41%] [G loss: 1.869267]\n",
      "epoch:19 step:18620 [D loss: 0.692832, acc: 57.03%] [G loss: 1.704240]\n",
      "epoch:19 step:18621 [D loss: 0.716863, acc: 56.25%] [G loss: 1.807810]\n",
      "epoch:19 step:18622 [D loss: 0.657890, acc: 56.25%] [G loss: 1.678967]\n",
      "epoch:19 step:18623 [D loss: 0.703335, acc: 52.34%] [G loss: 1.698031]\n",
      "epoch:19 step:18624 [D loss: 0.619459, acc: 62.50%] [G loss: 1.698260]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:19 step:18625 [D loss: 0.672583, acc: 58.59%] [G loss: 1.786650]\n",
      "epoch:19 step:18626 [D loss: 0.626612, acc: 60.94%] [G loss: 1.893519]\n",
      "epoch:19 step:18627 [D loss: 0.660893, acc: 59.38%] [G loss: 1.775574]\n",
      "epoch:19 step:18628 [D loss: 0.632295, acc: 61.72%] [G loss: 1.937885]\n",
      "epoch:19 step:18629 [D loss: 0.661250, acc: 60.16%] [G loss: 1.965245]\n",
      "epoch:19 step:18630 [D loss: 0.660053, acc: 58.59%] [G loss: 1.740542]\n",
      "epoch:19 step:18631 [D loss: 0.694570, acc: 53.91%] [G loss: 1.806669]\n",
      "epoch:19 step:18632 [D loss: 0.643616, acc: 60.94%] [G loss: 1.723626]\n",
      "epoch:19 step:18633 [D loss: 0.672244, acc: 55.47%] [G loss: 1.778527]\n",
      "epoch:19 step:18634 [D loss: 0.660585, acc: 59.38%] [G loss: 1.998375]\n",
      "epoch:19 step:18635 [D loss: 0.633803, acc: 63.28%] [G loss: 1.889844]\n",
      "epoch:19 step:18636 [D loss: 0.590811, acc: 67.97%] [G loss: 1.888075]\n",
      "epoch:19 step:18637 [D loss: 0.658609, acc: 65.62%] [G loss: 1.957170]\n",
      "epoch:19 step:18638 [D loss: 0.620446, acc: 60.16%] [G loss: 1.824514]\n",
      "epoch:19 step:18639 [D loss: 0.640913, acc: 61.72%] [G loss: 1.918588]\n",
      "epoch:19 step:18640 [D loss: 0.589822, acc: 69.53%] [G loss: 1.959758]\n",
      "epoch:19 step:18641 [D loss: 0.681318, acc: 61.72%] [G loss: 1.823564]\n",
      "epoch:19 step:18642 [D loss: 0.679213, acc: 57.81%] [G loss: 1.873348]\n",
      "epoch:19 step:18643 [D loss: 0.660772, acc: 61.72%] [G loss: 1.870663]\n",
      "epoch:19 step:18644 [D loss: 0.622848, acc: 60.94%] [G loss: 1.915271]\n",
      "epoch:19 step:18645 [D loss: 0.571418, acc: 69.53%] [G loss: 1.899500]\n",
      "epoch:19 step:18646 [D loss: 0.677827, acc: 59.38%] [G loss: 1.808644]\n",
      "epoch:19 step:18647 [D loss: 0.660832, acc: 60.94%] [G loss: 1.830686]\n",
      "epoch:19 step:18648 [D loss: 0.695264, acc: 56.25%] [G loss: 1.939412]\n",
      "epoch:19 step:18649 [D loss: 0.636788, acc: 63.28%] [G loss: 1.865393]\n",
      "epoch:19 step:18650 [D loss: 0.616489, acc: 67.97%] [G loss: 2.013194]\n",
      "epoch:19 step:18651 [D loss: 0.646380, acc: 67.19%] [G loss: 1.948235]\n",
      "epoch:19 step:18652 [D loss: 0.605033, acc: 67.97%] [G loss: 1.872171]\n",
      "epoch:19 step:18653 [D loss: 0.687291, acc: 60.16%] [G loss: 1.798780]\n",
      "epoch:19 step:18654 [D loss: 0.616858, acc: 64.06%] [G loss: 1.880675]\n",
      "epoch:19 step:18655 [D loss: 0.630998, acc: 63.28%] [G loss: 1.905207]\n",
      "epoch:19 step:18656 [D loss: 0.614814, acc: 70.31%] [G loss: 1.970946]\n",
      "epoch:19 step:18657 [D loss: 0.639038, acc: 57.81%] [G loss: 1.843747]\n",
      "epoch:19 step:18658 [D loss: 0.628666, acc: 60.16%] [G loss: 1.807723]\n",
      "epoch:19 step:18659 [D loss: 0.616334, acc: 62.50%] [G loss: 1.844288]\n",
      "epoch:19 step:18660 [D loss: 0.637835, acc: 64.84%] [G loss: 1.874254]\n",
      "epoch:19 step:18661 [D loss: 0.680005, acc: 59.38%] [G loss: 1.939057]\n",
      "epoch:19 step:18662 [D loss: 0.665036, acc: 63.28%] [G loss: 1.914300]\n",
      "epoch:19 step:18663 [D loss: 0.665835, acc: 62.50%] [G loss: 1.917619]\n",
      "epoch:19 step:18664 [D loss: 0.592060, acc: 68.75%] [G loss: 1.826966]\n",
      "epoch:19 step:18665 [D loss: 0.626076, acc: 67.97%] [G loss: 1.819937]\n",
      "epoch:19 step:18666 [D loss: 0.672571, acc: 62.50%] [G loss: 1.862504]\n",
      "epoch:19 step:18667 [D loss: 0.613786, acc: 71.88%] [G loss: 1.927092]\n",
      "epoch:19 step:18668 [D loss: 0.634486, acc: 67.97%] [G loss: 1.798056]\n",
      "epoch:19 step:18669 [D loss: 0.654126, acc: 56.25%] [G loss: 1.921411]\n",
      "epoch:19 step:18670 [D loss: 0.635759, acc: 66.41%] [G loss: 1.747881]\n",
      "epoch:19 step:18671 [D loss: 0.616090, acc: 63.28%] [G loss: 2.007518]\n",
      "epoch:19 step:18672 [D loss: 0.662510, acc: 57.81%] [G loss: 1.936589]\n",
      "epoch:19 step:18673 [D loss: 0.649021, acc: 58.59%] [G loss: 1.771532]\n",
      "epoch:19 step:18674 [D loss: 0.676093, acc: 68.75%] [G loss: 1.836484]\n",
      "epoch:19 step:18675 [D loss: 0.645804, acc: 63.28%] [G loss: 1.906779]\n",
      "epoch:19 step:18676 [D loss: 0.656135, acc: 63.28%] [G loss: 1.775906]\n",
      "epoch:19 step:18677 [D loss: 0.676374, acc: 58.59%] [G loss: 1.747712]\n",
      "epoch:19 step:18678 [D loss: 0.590583, acc: 67.19%] [G loss: 1.975189]\n",
      "epoch:19 step:18679 [D loss: 0.671207, acc: 59.38%] [G loss: 1.864243]\n",
      "epoch:19 step:18680 [D loss: 0.640219, acc: 65.62%] [G loss: 1.852803]\n",
      "epoch:19 step:18681 [D loss: 0.678435, acc: 57.03%] [G loss: 1.900546]\n",
      "epoch:19 step:18682 [D loss: 0.700923, acc: 60.94%] [G loss: 1.893361]\n",
      "epoch:19 step:18683 [D loss: 0.594334, acc: 68.75%] [G loss: 1.877960]\n",
      "epoch:19 step:18684 [D loss: 0.660547, acc: 61.72%] [G loss: 1.850788]\n",
      "epoch:19 step:18685 [D loss: 0.645984, acc: 62.50%] [G loss: 1.866185]\n",
      "epoch:19 step:18686 [D loss: 0.637762, acc: 67.19%] [G loss: 1.901073]\n",
      "epoch:19 step:18687 [D loss: 0.611729, acc: 67.97%] [G loss: 1.901107]\n",
      "epoch:19 step:18688 [D loss: 0.635208, acc: 61.72%] [G loss: 1.916474]\n",
      "epoch:19 step:18689 [D loss: 0.663568, acc: 64.84%] [G loss: 1.970886]\n",
      "epoch:19 step:18690 [D loss: 0.629560, acc: 64.84%] [G loss: 1.748856]\n",
      "epoch:19 step:18691 [D loss: 0.624391, acc: 65.62%] [G loss: 1.860572]\n",
      "epoch:19 step:18692 [D loss: 0.680515, acc: 58.59%] [G loss: 1.893145]\n",
      "epoch:19 step:18693 [D loss: 0.661331, acc: 59.38%] [G loss: 1.927547]\n",
      "epoch:19 step:18694 [D loss: 0.618987, acc: 65.62%] [G loss: 1.929097]\n",
      "epoch:19 step:18695 [D loss: 0.660777, acc: 61.72%] [G loss: 1.790202]\n",
      "epoch:19 step:18696 [D loss: 0.629234, acc: 65.62%] [G loss: 1.872334]\n",
      "epoch:19 step:18697 [D loss: 0.602860, acc: 67.19%] [G loss: 1.871316]\n",
      "epoch:19 step:18698 [D loss: 0.630660, acc: 62.50%] [G loss: 1.822348]\n",
      "epoch:19 step:18699 [D loss: 0.635380, acc: 63.28%] [G loss: 1.779454]\n",
      "epoch:19 step:18700 [D loss: 0.705585, acc: 56.25%] [G loss: 1.791809]\n",
      "epoch:19 step:18701 [D loss: 0.667125, acc: 63.28%] [G loss: 1.926687]\n",
      "epoch:19 step:18702 [D loss: 0.647299, acc: 55.47%] [G loss: 1.992280]\n",
      "epoch:19 step:18703 [D loss: 0.659891, acc: 59.38%] [G loss: 1.915680]\n",
      "epoch:19 step:18704 [D loss: 0.632982, acc: 58.59%] [G loss: 1.909812]\n",
      "epoch:19 step:18705 [D loss: 0.613311, acc: 66.41%] [G loss: 1.802547]\n",
      "epoch:19 step:18706 [D loss: 0.618927, acc: 68.75%] [G loss: 1.896824]\n",
      "epoch:19 step:18707 [D loss: 0.678926, acc: 60.94%] [G loss: 1.789311]\n",
      "epoch:19 step:18708 [D loss: 0.636848, acc: 62.50%] [G loss: 1.954398]\n",
      "epoch:19 step:18709 [D loss: 0.645218, acc: 61.72%] [G loss: 1.939740]\n",
      "epoch:19 step:18710 [D loss: 0.624170, acc: 61.72%] [G loss: 1.883099]\n",
      "epoch:19 step:18711 [D loss: 0.626815, acc: 62.50%] [G loss: 2.017931]\n",
      "epoch:19 step:18712 [D loss: 0.570262, acc: 75.78%] [G loss: 2.150582]\n",
      "epoch:19 step:18713 [D loss: 0.685846, acc: 54.69%] [G loss: 2.024176]\n",
      "epoch:19 step:18714 [D loss: 0.639821, acc: 59.38%] [G loss: 2.096075]\n",
      "epoch:19 step:18715 [D loss: 0.595017, acc: 68.75%] [G loss: 2.370539]\n",
      "epoch:19 step:18716 [D loss: 0.693542, acc: 54.69%] [G loss: 1.968471]\n",
      "epoch:19 step:18717 [D loss: 0.652918, acc: 60.16%] [G loss: 1.989027]\n",
      "epoch:19 step:18718 [D loss: 0.631698, acc: 63.28%] [G loss: 1.912072]\n",
      "epoch:19 step:18719 [D loss: 0.620533, acc: 67.19%] [G loss: 2.180303]\n",
      "epoch:19 step:18720 [D loss: 0.660447, acc: 58.59%] [G loss: 1.897621]\n",
      "epoch:19 step:18721 [D loss: 0.576239, acc: 73.44%] [G loss: 2.395754]\n",
      "epoch:19 step:18722 [D loss: 0.532071, acc: 76.56%] [G loss: 2.264915]\n",
      "epoch:19 step:18723 [D loss: 0.718698, acc: 51.56%] [G loss: 1.886014]\n",
      "epoch:19 step:18724 [D loss: 0.589748, acc: 67.97%] [G loss: 2.159112]\n",
      "epoch:19 step:18725 [D loss: 0.629787, acc: 64.84%] [G loss: 2.129767]\n",
      "epoch:19 step:18726 [D loss: 0.599503, acc: 66.41%] [G loss: 2.079709]\n",
      "epoch:19 step:18727 [D loss: 0.574996, acc: 69.53%] [G loss: 2.143738]\n",
      "epoch:19 step:18728 [D loss: 0.588068, acc: 69.53%] [G loss: 2.196464]\n",
      "epoch:19 step:18729 [D loss: 0.611824, acc: 67.19%] [G loss: 2.206269]\n",
      "epoch:19 step:18730 [D loss: 0.639410, acc: 64.06%] [G loss: 2.088392]\n",
      "epoch:19 step:18731 [D loss: 0.801565, acc: 46.88%] [G loss: 1.760950]\n",
      "epoch:19 step:18732 [D loss: 0.795454, acc: 43.75%] [G loss: 1.910422]\n",
      "epoch:19 step:18733 [D loss: 0.599423, acc: 71.88%] [G loss: 2.054762]\n",
      "epoch:19 step:18734 [D loss: 0.613875, acc: 66.41%] [G loss: 2.022271]\n",
      "epoch:19 step:18735 [D loss: 0.663281, acc: 59.38%] [G loss: 1.821038]\n",
      "epoch:19 step:18736 [D loss: 0.697432, acc: 57.03%] [G loss: 1.957027]\n",
      "epoch:19 step:18737 [D loss: 0.589109, acc: 74.22%] [G loss: 1.879871]\n",
      "epoch:19 step:18738 [D loss: 0.643775, acc: 63.28%] [G loss: 1.950534]\n",
      "epoch:19 step:18739 [D loss: 0.612860, acc: 71.88%] [G loss: 2.082623]\n",
      "epoch:19 step:18740 [D loss: 0.573307, acc: 72.66%] [G loss: 2.448801]\n",
      "epoch:20 step:18741 [D loss: 0.669416, acc: 60.94%] [G loss: 1.938306]\n",
      "epoch:20 step:18742 [D loss: 0.700423, acc: 60.16%] [G loss: 1.929515]\n",
      "epoch:20 step:18743 [D loss: 0.682413, acc: 60.16%] [G loss: 1.989677]\n",
      "epoch:20 step:18744 [D loss: 0.688370, acc: 57.03%] [G loss: 1.722496]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:20 step:18745 [D loss: 0.585246, acc: 67.19%] [G loss: 1.846279]\n",
      "epoch:20 step:18746 [D loss: 0.671290, acc: 57.81%] [G loss: 1.948005]\n",
      "epoch:20 step:18747 [D loss: 0.634478, acc: 62.50%] [G loss: 1.935237]\n",
      "epoch:20 step:18748 [D loss: 0.677613, acc: 57.81%] [G loss: 1.882037]\n",
      "epoch:20 step:18749 [D loss: 0.618314, acc: 64.06%] [G loss: 2.029508]\n",
      "epoch:20 step:18750 [D loss: 0.641345, acc: 60.16%] [G loss: 1.967173]\n",
      "epoch:20 step:18751 [D loss: 0.632392, acc: 64.06%] [G loss: 2.019575]\n",
      "epoch:20 step:18752 [D loss: 0.675390, acc: 57.03%] [G loss: 1.720056]\n",
      "epoch:20 step:18753 [D loss: 0.639973, acc: 63.28%] [G loss: 1.978187]\n",
      "epoch:20 step:18754 [D loss: 0.626841, acc: 62.50%] [G loss: 1.955348]\n",
      "epoch:20 step:18755 [D loss: 0.597887, acc: 69.53%] [G loss: 2.122811]\n",
      "epoch:20 step:18756 [D loss: 0.644369, acc: 63.28%] [G loss: 1.986695]\n",
      "epoch:20 step:18757 [D loss: 0.678347, acc: 53.12%] [G loss: 1.832995]\n",
      "epoch:20 step:18758 [D loss: 0.665245, acc: 62.50%] [G loss: 1.903716]\n",
      "epoch:20 step:18759 [D loss: 0.670688, acc: 57.03%] [G loss: 1.917941]\n",
      "epoch:20 step:18760 [D loss: 0.684961, acc: 57.03%] [G loss: 1.745582]\n",
      "epoch:20 step:18761 [D loss: 0.637547, acc: 57.81%] [G loss: 1.848576]\n",
      "epoch:20 step:18762 [D loss: 0.705090, acc: 56.25%] [G loss: 1.823648]\n",
      "epoch:20 step:18763 [D loss: 0.585349, acc: 67.97%] [G loss: 1.933212]\n",
      "epoch:20 step:18764 [D loss: 0.612507, acc: 66.41%] [G loss: 1.951053]\n",
      "epoch:20 step:18765 [D loss: 0.602998, acc: 66.41%] [G loss: 1.993640]\n",
      "epoch:20 step:18766 [D loss: 0.647859, acc: 66.41%] [G loss: 1.734892]\n",
      "epoch:20 step:18767 [D loss: 0.668631, acc: 64.84%] [G loss: 1.813259]\n",
      "epoch:20 step:18768 [D loss: 0.643899, acc: 66.41%] [G loss: 1.854966]\n",
      "epoch:20 step:18769 [D loss: 0.607255, acc: 71.09%] [G loss: 1.938375]\n",
      "epoch:20 step:18770 [D loss: 0.630703, acc: 63.28%] [G loss: 1.915898]\n",
      "epoch:20 step:18771 [D loss: 0.674238, acc: 53.12%] [G loss: 1.748219]\n",
      "epoch:20 step:18772 [D loss: 0.650112, acc: 62.50%] [G loss: 1.906610]\n",
      "epoch:20 step:18773 [D loss: 0.611339, acc: 64.06%] [G loss: 1.987460]\n",
      "epoch:20 step:18774 [D loss: 0.669745, acc: 54.69%] [G loss: 1.884440]\n",
      "epoch:20 step:18775 [D loss: 0.653714, acc: 62.50%] [G loss: 1.946514]\n",
      "epoch:20 step:18776 [D loss: 0.601683, acc: 67.97%] [G loss: 1.938995]\n",
      "epoch:20 step:18777 [D loss: 0.617421, acc: 66.41%] [G loss: 1.872837]\n",
      "epoch:20 step:18778 [D loss: 0.681383, acc: 56.25%] [G loss: 1.957000]\n",
      "epoch:20 step:18779 [D loss: 0.642466, acc: 61.72%] [G loss: 1.903403]\n",
      "epoch:20 step:18780 [D loss: 0.606284, acc: 61.72%] [G loss: 2.186239]\n",
      "epoch:20 step:18781 [D loss: 0.648769, acc: 62.50%] [G loss: 1.873707]\n",
      "epoch:20 step:18782 [D loss: 0.629024, acc: 67.19%] [G loss: 1.899651]\n",
      "epoch:20 step:18783 [D loss: 0.608211, acc: 68.75%] [G loss: 1.939327]\n",
      "epoch:20 step:18784 [D loss: 0.684839, acc: 58.59%] [G loss: 1.908859]\n",
      "epoch:20 step:18785 [D loss: 0.629388, acc: 60.94%] [G loss: 2.088123]\n",
      "epoch:20 step:18786 [D loss: 0.618791, acc: 62.50%] [G loss: 1.825694]\n",
      "epoch:20 step:18787 [D loss: 0.624195, acc: 70.31%] [G loss: 1.984367]\n",
      "epoch:20 step:18788 [D loss: 0.601753, acc: 68.75%] [G loss: 1.860190]\n",
      "epoch:20 step:18789 [D loss: 0.643618, acc: 66.41%] [G loss: 2.035709]\n",
      "epoch:20 step:18790 [D loss: 0.630427, acc: 66.41%] [G loss: 2.000989]\n",
      "epoch:20 step:18791 [D loss: 0.647573, acc: 67.97%] [G loss: 1.909230]\n",
      "epoch:20 step:18792 [D loss: 0.621037, acc: 66.41%] [G loss: 1.909488]\n",
      "epoch:20 step:18793 [D loss: 0.642219, acc: 64.84%] [G loss: 1.965709]\n",
      "epoch:20 step:18794 [D loss: 0.641117, acc: 59.38%] [G loss: 1.948553]\n",
      "epoch:20 step:18795 [D loss: 0.651387, acc: 61.72%] [G loss: 1.882317]\n",
      "epoch:20 step:18796 [D loss: 0.611071, acc: 64.06%] [G loss: 2.051030]\n",
      "epoch:20 step:18797 [D loss: 0.704809, acc: 59.38%] [G loss: 1.984304]\n",
      "epoch:20 step:18798 [D loss: 0.656379, acc: 66.41%] [G loss: 1.725076]\n",
      "epoch:20 step:18799 [D loss: 0.674039, acc: 59.38%] [G loss: 1.923381]\n",
      "epoch:20 step:18800 [D loss: 0.678177, acc: 58.59%] [G loss: 1.905530]\n",
      "##############\n",
      "[2.45695014 1.5889271  6.2010709  4.88043863 3.54189486 5.768654\n",
      " 4.26927691 4.64052656 4.44709244 3.66194775]\n",
      "##########\n",
      "epoch:20 step:18801 [D loss: 0.646726, acc: 60.94%] [G loss: 1.718247]\n",
      "epoch:20 step:18802 [D loss: 0.661097, acc: 60.94%] [G loss: 1.896944]\n",
      "epoch:20 step:18803 [D loss: 0.608151, acc: 67.19%] [G loss: 1.832834]\n",
      "epoch:20 step:18804 [D loss: 0.666405, acc: 53.12%] [G loss: 1.834753]\n",
      "epoch:20 step:18805 [D loss: 0.679979, acc: 58.59%] [G loss: 1.856023]\n",
      "epoch:20 step:18806 [D loss: 0.669901, acc: 55.47%] [G loss: 1.874167]\n",
      "epoch:20 step:18807 [D loss: 0.655843, acc: 58.59%] [G loss: 1.707144]\n",
      "epoch:20 step:18808 [D loss: 0.687099, acc: 60.16%] [G loss: 1.887465]\n",
      "epoch:20 step:18809 [D loss: 0.635635, acc: 67.19%] [G loss: 1.904809]\n",
      "epoch:20 step:18810 [D loss: 0.645714, acc: 63.28%] [G loss: 1.846449]\n",
      "epoch:20 step:18811 [D loss: 0.648459, acc: 64.84%] [G loss: 1.816082]\n",
      "epoch:20 step:18812 [D loss: 0.631372, acc: 62.50%] [G loss: 1.784357]\n",
      "epoch:20 step:18813 [D loss: 0.695165, acc: 53.12%] [G loss: 1.604694]\n",
      "epoch:20 step:18814 [D loss: 0.584489, acc: 67.97%] [G loss: 1.992647]\n",
      "epoch:20 step:18815 [D loss: 0.613152, acc: 69.53%] [G loss: 1.891429]\n",
      "epoch:20 step:18816 [D loss: 0.602402, acc: 72.66%] [G loss: 2.015506]\n",
      "epoch:20 step:18817 [D loss: 0.655676, acc: 61.72%] [G loss: 1.983796]\n",
      "epoch:20 step:18818 [D loss: 0.651004, acc: 60.16%] [G loss: 1.744283]\n",
      "epoch:20 step:18819 [D loss: 0.685274, acc: 58.59%] [G loss: 1.842880]\n",
      "epoch:20 step:18820 [D loss: 0.693660, acc: 54.69%] [G loss: 1.791678]\n",
      "epoch:20 step:18821 [D loss: 0.671059, acc: 63.28%] [G loss: 1.785817]\n",
      "epoch:20 step:18822 [D loss: 0.662281, acc: 58.59%] [G loss: 1.905127]\n",
      "epoch:20 step:18823 [D loss: 0.646508, acc: 61.72%] [G loss: 2.018237]\n",
      "epoch:20 step:18824 [D loss: 0.647649, acc: 60.16%] [G loss: 1.946914]\n",
      "epoch:20 step:18825 [D loss: 0.760896, acc: 51.56%] [G loss: 1.933791]\n",
      "epoch:20 step:18826 [D loss: 0.669862, acc: 60.16%] [G loss: 1.860091]\n",
      "epoch:20 step:18827 [D loss: 0.617667, acc: 60.16%] [G loss: 1.920677]\n",
      "epoch:20 step:18828 [D loss: 0.637535, acc: 63.28%] [G loss: 1.848220]\n",
      "epoch:20 step:18829 [D loss: 0.666759, acc: 59.38%] [G loss: 1.909083]\n",
      "epoch:20 step:18830 [D loss: 0.625686, acc: 66.41%] [G loss: 1.929520]\n",
      "epoch:20 step:18831 [D loss: 0.648456, acc: 62.50%] [G loss: 1.952622]\n",
      "epoch:20 step:18832 [D loss: 0.624811, acc: 69.53%] [G loss: 2.066180]\n",
      "epoch:20 step:18833 [D loss: 0.612822, acc: 68.75%] [G loss: 1.925744]\n",
      "epoch:20 step:18834 [D loss: 0.631061, acc: 61.72%] [G loss: 1.860810]\n",
      "epoch:20 step:18835 [D loss: 0.654865, acc: 60.94%] [G loss: 1.938005]\n",
      "epoch:20 step:18836 [D loss: 0.655492, acc: 59.38%] [G loss: 1.780301]\n",
      "epoch:20 step:18837 [D loss: 0.630748, acc: 64.06%] [G loss: 1.831644]\n",
      "epoch:20 step:18838 [D loss: 0.632255, acc: 65.62%] [G loss: 1.719088]\n",
      "epoch:20 step:18839 [D loss: 0.597999, acc: 65.62%] [G loss: 1.893452]\n",
      "epoch:20 step:18840 [D loss: 0.650430, acc: 62.50%] [G loss: 2.038289]\n",
      "epoch:20 step:18841 [D loss: 0.656150, acc: 58.59%] [G loss: 1.934244]\n",
      "epoch:20 step:18842 [D loss: 0.669773, acc: 54.69%] [G loss: 1.882811]\n",
      "epoch:20 step:18843 [D loss: 0.595578, acc: 67.19%] [G loss: 1.867304]\n",
      "epoch:20 step:18844 [D loss: 0.673992, acc: 53.91%] [G loss: 1.926319]\n",
      "epoch:20 step:18845 [D loss: 0.677813, acc: 55.47%] [G loss: 1.859912]\n",
      "epoch:20 step:18846 [D loss: 0.603796, acc: 64.84%] [G loss: 2.019157]\n",
      "epoch:20 step:18847 [D loss: 0.619963, acc: 64.84%] [G loss: 2.118295]\n",
      "epoch:20 step:18848 [D loss: 0.685087, acc: 56.25%] [G loss: 1.900613]\n",
      "epoch:20 step:18849 [D loss: 0.660047, acc: 57.03%] [G loss: 1.648304]\n",
      "epoch:20 step:18850 [D loss: 0.650779, acc: 55.47%] [G loss: 1.751526]\n",
      "epoch:20 step:18851 [D loss: 0.657835, acc: 62.50%] [G loss: 1.812494]\n",
      "epoch:20 step:18852 [D loss: 0.690302, acc: 58.59%] [G loss: 1.917640]\n",
      "epoch:20 step:18853 [D loss: 0.636088, acc: 64.06%] [G loss: 1.961518]\n",
      "epoch:20 step:18854 [D loss: 0.649025, acc: 60.16%] [G loss: 1.965738]\n",
      "epoch:20 step:18855 [D loss: 0.629119, acc: 62.50%] [G loss: 2.122357]\n",
      "epoch:20 step:18856 [D loss: 0.579328, acc: 67.97%] [G loss: 2.021803]\n",
      "epoch:20 step:18857 [D loss: 0.678138, acc: 52.34%] [G loss: 2.028177]\n",
      "epoch:20 step:18858 [D loss: 0.642910, acc: 66.41%] [G loss: 1.958726]\n",
      "epoch:20 step:18859 [D loss: 0.627835, acc: 64.06%] [G loss: 2.091413]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:20 step:18860 [D loss: 0.704105, acc: 60.16%] [G loss: 1.851079]\n",
      "epoch:20 step:18861 [D loss: 0.658326, acc: 58.59%] [G loss: 1.883042]\n",
      "epoch:20 step:18862 [D loss: 0.647087, acc: 63.28%] [G loss: 1.915805]\n",
      "epoch:20 step:18863 [D loss: 0.652660, acc: 62.50%] [G loss: 1.942503]\n",
      "epoch:20 step:18864 [D loss: 0.651779, acc: 58.59%] [G loss: 1.814066]\n",
      "epoch:20 step:18865 [D loss: 0.653928, acc: 57.81%] [G loss: 1.814483]\n",
      "epoch:20 step:18866 [D loss: 0.630413, acc: 57.81%] [G loss: 2.031379]\n",
      "epoch:20 step:18867 [D loss: 0.685406, acc: 59.38%] [G loss: 1.669849]\n",
      "epoch:20 step:18868 [D loss: 0.643994, acc: 60.94%] [G loss: 1.876679]\n",
      "epoch:20 step:18869 [D loss: 0.664514, acc: 57.03%] [G loss: 1.863800]\n",
      "epoch:20 step:18870 [D loss: 0.649389, acc: 60.16%] [G loss: 1.873269]\n",
      "epoch:20 step:18871 [D loss: 0.653262, acc: 57.03%] [G loss: 1.894403]\n",
      "epoch:20 step:18872 [D loss: 0.628224, acc: 60.16%] [G loss: 1.756859]\n",
      "epoch:20 step:18873 [D loss: 0.712306, acc: 51.56%] [G loss: 1.873213]\n",
      "epoch:20 step:18874 [D loss: 0.719298, acc: 51.56%] [G loss: 1.797709]\n",
      "epoch:20 step:18875 [D loss: 0.637051, acc: 61.72%] [G loss: 1.865156]\n",
      "epoch:20 step:18876 [D loss: 0.647848, acc: 63.28%] [G loss: 1.889676]\n",
      "epoch:20 step:18877 [D loss: 0.641487, acc: 59.38%] [G loss: 1.869559]\n",
      "epoch:20 step:18878 [D loss: 0.649570, acc: 61.72%] [G loss: 1.773252]\n",
      "epoch:20 step:18879 [D loss: 0.654717, acc: 60.94%] [G loss: 2.012956]\n",
      "epoch:20 step:18880 [D loss: 0.613477, acc: 74.22%] [G loss: 1.758087]\n",
      "epoch:20 step:18881 [D loss: 0.593362, acc: 73.44%] [G loss: 1.897423]\n",
      "epoch:20 step:18882 [D loss: 0.652630, acc: 67.97%] [G loss: 1.814808]\n",
      "epoch:20 step:18883 [D loss: 0.644110, acc: 58.59%] [G loss: 1.933684]\n",
      "epoch:20 step:18884 [D loss: 0.648519, acc: 60.94%] [G loss: 1.968121]\n",
      "epoch:20 step:18885 [D loss: 0.631221, acc: 64.06%] [G loss: 1.905180]\n",
      "epoch:20 step:18886 [D loss: 0.633809, acc: 69.53%] [G loss: 1.930166]\n",
      "epoch:20 step:18887 [D loss: 0.661678, acc: 59.38%] [G loss: 1.836875]\n",
      "epoch:20 step:18888 [D loss: 0.639675, acc: 66.41%] [G loss: 1.746506]\n",
      "epoch:20 step:18889 [D loss: 0.638087, acc: 59.38%] [G loss: 2.018592]\n",
      "epoch:20 step:18890 [D loss: 0.653400, acc: 55.47%] [G loss: 1.810719]\n",
      "epoch:20 step:18891 [D loss: 0.675577, acc: 55.47%] [G loss: 2.049705]\n",
      "epoch:20 step:18892 [D loss: 0.711188, acc: 57.03%] [G loss: 1.878884]\n",
      "epoch:20 step:18893 [D loss: 0.644395, acc: 58.59%] [G loss: 1.764961]\n",
      "epoch:20 step:18894 [D loss: 0.655237, acc: 57.81%] [G loss: 1.852912]\n",
      "epoch:20 step:18895 [D loss: 0.677506, acc: 54.69%] [G loss: 1.900793]\n",
      "epoch:20 step:18896 [D loss: 0.679771, acc: 54.69%] [G loss: 1.901698]\n",
      "epoch:20 step:18897 [D loss: 0.674738, acc: 59.38%] [G loss: 1.779374]\n",
      "epoch:20 step:18898 [D loss: 0.741566, acc: 50.78%] [G loss: 1.677996]\n",
      "epoch:20 step:18899 [D loss: 0.645727, acc: 60.94%] [G loss: 1.855623]\n",
      "epoch:20 step:18900 [D loss: 0.700716, acc: 53.91%] [G loss: 1.616731]\n",
      "epoch:20 step:18901 [D loss: 0.682152, acc: 60.16%] [G loss: 1.719125]\n",
      "epoch:20 step:18902 [D loss: 0.655883, acc: 62.50%] [G loss: 1.828887]\n",
      "epoch:20 step:18903 [D loss: 0.591531, acc: 70.31%] [G loss: 1.781124]\n",
      "epoch:20 step:18904 [D loss: 0.657575, acc: 60.16%] [G loss: 1.812068]\n",
      "epoch:20 step:18905 [D loss: 0.610024, acc: 67.97%] [G loss: 1.803858]\n",
      "epoch:20 step:18906 [D loss: 0.626737, acc: 67.97%] [G loss: 1.724777]\n",
      "epoch:20 step:18907 [D loss: 0.651920, acc: 60.16%] [G loss: 1.885648]\n",
      "epoch:20 step:18908 [D loss: 0.668361, acc: 59.38%] [G loss: 1.891103]\n",
      "epoch:20 step:18909 [D loss: 0.675947, acc: 59.38%] [G loss: 1.971870]\n",
      "epoch:20 step:18910 [D loss: 0.653589, acc: 60.16%] [G loss: 1.629357]\n",
      "epoch:20 step:18911 [D loss: 0.614008, acc: 64.06%] [G loss: 1.875406]\n",
      "epoch:20 step:18912 [D loss: 0.653495, acc: 56.25%] [G loss: 1.993412]\n",
      "epoch:20 step:18913 [D loss: 0.669438, acc: 57.03%] [G loss: 1.786569]\n",
      "epoch:20 step:18914 [D loss: 0.669290, acc: 57.81%] [G loss: 1.727403]\n",
      "epoch:20 step:18915 [D loss: 0.678355, acc: 57.81%] [G loss: 1.697760]\n",
      "epoch:20 step:18916 [D loss: 0.695089, acc: 59.38%] [G loss: 1.710268]\n",
      "epoch:20 step:18917 [D loss: 0.690668, acc: 55.47%] [G loss: 1.652024]\n",
      "epoch:20 step:18918 [D loss: 0.709639, acc: 50.78%] [G loss: 1.730446]\n",
      "epoch:20 step:18919 [D loss: 0.619160, acc: 69.53%] [G loss: 1.766348]\n",
      "epoch:20 step:18920 [D loss: 0.643403, acc: 67.97%] [G loss: 1.875234]\n",
      "epoch:20 step:18921 [D loss: 0.618381, acc: 70.31%] [G loss: 1.850587]\n",
      "epoch:20 step:18922 [D loss: 0.583615, acc: 73.44%] [G loss: 1.820103]\n",
      "epoch:20 step:18923 [D loss: 0.658273, acc: 57.81%] [G loss: 1.861147]\n",
      "epoch:20 step:18924 [D loss: 0.617731, acc: 66.41%] [G loss: 1.900502]\n",
      "epoch:20 step:18925 [D loss: 0.619671, acc: 67.19%] [G loss: 1.841375]\n",
      "epoch:20 step:18926 [D loss: 0.645055, acc: 60.16%] [G loss: 1.834679]\n",
      "epoch:20 step:18927 [D loss: 0.646000, acc: 60.94%] [G loss: 1.843614]\n",
      "epoch:20 step:18928 [D loss: 0.588065, acc: 69.53%] [G loss: 1.979974]\n",
      "epoch:20 step:18929 [D loss: 0.731453, acc: 53.91%] [G loss: 1.744104]\n",
      "epoch:20 step:18930 [D loss: 0.628119, acc: 62.50%] [G loss: 1.834335]\n",
      "epoch:20 step:18931 [D loss: 0.670144, acc: 61.72%] [G loss: 1.901707]\n",
      "epoch:20 step:18932 [D loss: 0.639690, acc: 63.28%] [G loss: 1.982434]\n",
      "epoch:20 step:18933 [D loss: 0.660088, acc: 60.16%] [G loss: 1.898674]\n",
      "epoch:20 step:18934 [D loss: 0.603940, acc: 71.09%] [G loss: 2.090619]\n",
      "epoch:20 step:18935 [D loss: 0.655396, acc: 58.59%] [G loss: 1.918283]\n",
      "epoch:20 step:18936 [D loss: 0.693304, acc: 56.25%] [G loss: 1.775835]\n",
      "epoch:20 step:18937 [D loss: 0.619006, acc: 67.19%] [G loss: 1.985917]\n",
      "epoch:20 step:18938 [D loss: 0.633067, acc: 62.50%] [G loss: 2.013999]\n",
      "epoch:20 step:18939 [D loss: 0.683800, acc: 55.47%] [G loss: 1.817454]\n",
      "epoch:20 step:18940 [D loss: 0.668669, acc: 55.47%] [G loss: 1.827668]\n",
      "epoch:20 step:18941 [D loss: 0.682366, acc: 53.12%] [G loss: 1.886506]\n",
      "epoch:20 step:18942 [D loss: 0.659947, acc: 58.59%] [G loss: 1.737222]\n",
      "epoch:20 step:18943 [D loss: 0.639954, acc: 61.72%] [G loss: 1.808019]\n",
      "epoch:20 step:18944 [D loss: 0.655889, acc: 64.06%] [G loss: 1.865680]\n",
      "epoch:20 step:18945 [D loss: 0.683317, acc: 59.38%] [G loss: 1.781632]\n",
      "epoch:20 step:18946 [D loss: 0.594981, acc: 71.09%] [G loss: 1.986477]\n",
      "epoch:20 step:18947 [D loss: 0.589846, acc: 69.53%] [G loss: 2.109094]\n",
      "epoch:20 step:18948 [D loss: 0.635150, acc: 64.06%] [G loss: 2.055910]\n",
      "epoch:20 step:18949 [D loss: 0.570401, acc: 66.41%] [G loss: 2.049856]\n",
      "epoch:20 step:18950 [D loss: 0.613620, acc: 70.31%] [G loss: 1.937504]\n",
      "epoch:20 step:18951 [D loss: 0.682400, acc: 60.16%] [G loss: 1.722754]\n",
      "epoch:20 step:18952 [D loss: 0.643553, acc: 60.94%] [G loss: 1.874933]\n",
      "epoch:20 step:18953 [D loss: 0.667066, acc: 57.03%] [G loss: 1.999621]\n",
      "epoch:20 step:18954 [D loss: 0.636914, acc: 67.97%] [G loss: 1.874032]\n",
      "epoch:20 step:18955 [D loss: 0.637200, acc: 62.50%] [G loss: 1.923829]\n",
      "epoch:20 step:18956 [D loss: 0.665467, acc: 54.69%] [G loss: 1.891591]\n",
      "epoch:20 step:18957 [D loss: 0.655607, acc: 59.38%] [G loss: 1.934857]\n",
      "epoch:20 step:18958 [D loss: 0.616689, acc: 66.41%] [G loss: 2.132702]\n",
      "epoch:20 step:18959 [D loss: 0.632485, acc: 66.41%] [G loss: 2.135880]\n",
      "epoch:20 step:18960 [D loss: 0.757732, acc: 48.44%] [G loss: 1.707515]\n",
      "epoch:20 step:18961 [D loss: 0.656726, acc: 57.81%] [G loss: 1.872622]\n",
      "epoch:20 step:18962 [D loss: 0.644687, acc: 62.50%] [G loss: 1.897997]\n",
      "epoch:20 step:18963 [D loss: 0.647801, acc: 59.38%] [G loss: 1.876804]\n",
      "epoch:20 step:18964 [D loss: 0.687455, acc: 57.03%] [G loss: 1.999132]\n",
      "epoch:20 step:18965 [D loss: 0.614500, acc: 66.41%] [G loss: 2.038995]\n",
      "epoch:20 step:18966 [D loss: 0.628371, acc: 64.06%] [G loss: 1.809629]\n",
      "epoch:20 step:18967 [D loss: 0.650783, acc: 62.50%] [G loss: 1.858548]\n",
      "epoch:20 step:18968 [D loss: 0.718562, acc: 53.91%] [G loss: 1.773590]\n",
      "epoch:20 step:18969 [D loss: 0.574008, acc: 68.75%] [G loss: 2.178513]\n",
      "epoch:20 step:18970 [D loss: 0.563009, acc: 76.56%] [G loss: 2.054108]\n",
      "epoch:20 step:18971 [D loss: 0.522227, acc: 78.12%] [G loss: 2.246142]\n",
      "epoch:20 step:18972 [D loss: 0.615938, acc: 69.53%] [G loss: 2.410182]\n",
      "epoch:20 step:18973 [D loss: 0.673436, acc: 60.16%] [G loss: 1.861592]\n",
      "epoch:20 step:18974 [D loss: 0.682112, acc: 60.16%] [G loss: 1.901637]\n",
      "epoch:20 step:18975 [D loss: 0.619483, acc: 67.19%] [G loss: 1.879629]\n",
      "epoch:20 step:18976 [D loss: 0.632510, acc: 60.94%] [G loss: 1.912876]\n",
      "epoch:20 step:18977 [D loss: 0.669658, acc: 57.81%] [G loss: 1.950915]\n",
      "epoch:20 step:18978 [D loss: 0.637018, acc: 68.75%] [G loss: 1.944785]\n",
      "epoch:20 step:18979 [D loss: 0.639195, acc: 60.16%] [G loss: 1.991564]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:20 step:18980 [D loss: 0.615257, acc: 62.50%] [G loss: 1.995428]\n",
      "epoch:20 step:18981 [D loss: 0.629459, acc: 65.62%] [G loss: 1.986695]\n",
      "epoch:20 step:18982 [D loss: 0.673041, acc: 61.72%] [G loss: 1.948572]\n",
      "epoch:20 step:18983 [D loss: 0.694273, acc: 57.03%] [G loss: 1.883814]\n",
      "epoch:20 step:18984 [D loss: 0.686853, acc: 58.59%] [G loss: 1.906168]\n",
      "epoch:20 step:18985 [D loss: 0.584614, acc: 71.88%] [G loss: 2.094215]\n",
      "epoch:20 step:18986 [D loss: 0.651361, acc: 63.28%] [G loss: 1.970838]\n",
      "epoch:20 step:18987 [D loss: 0.672034, acc: 58.59%] [G loss: 1.871906]\n",
      "epoch:20 step:18988 [D loss: 0.599545, acc: 69.53%] [G loss: 2.057066]\n",
      "epoch:20 step:18989 [D loss: 0.653685, acc: 55.47%] [G loss: 1.780980]\n",
      "epoch:20 step:18990 [D loss: 0.703987, acc: 54.69%] [G loss: 1.687822]\n",
      "epoch:20 step:18991 [D loss: 0.741486, acc: 53.12%] [G loss: 1.705696]\n",
      "epoch:20 step:18992 [D loss: 0.690404, acc: 57.81%] [G loss: 1.809787]\n",
      "epoch:20 step:18993 [D loss: 0.662867, acc: 61.72%] [G loss: 1.728101]\n",
      "epoch:20 step:18994 [D loss: 0.640227, acc: 64.06%] [G loss: 1.867030]\n",
      "epoch:20 step:18995 [D loss: 0.621086, acc: 65.62%] [G loss: 1.827124]\n",
      "epoch:20 step:18996 [D loss: 0.671007, acc: 57.81%] [G loss: 1.801640]\n",
      "epoch:20 step:18997 [D loss: 0.646035, acc: 60.94%] [G loss: 1.833995]\n",
      "epoch:20 step:18998 [D loss: 0.665953, acc: 60.16%] [G loss: 1.804926]\n",
      "epoch:20 step:18999 [D loss: 0.658759, acc: 62.50%] [G loss: 1.787911]\n",
      "epoch:20 step:19000 [D loss: 0.597764, acc: 68.75%] [G loss: 1.849571]\n",
      "##############\n",
      "[2.53233565 1.51844078 6.47006724 4.74935103 3.60513068 5.59910982\n",
      " 4.37638198 4.75039426 4.8384458  3.69510073]\n",
      "##########\n",
      "epoch:20 step:19001 [D loss: 0.641261, acc: 61.72%] [G loss: 1.908515]\n",
      "epoch:20 step:19002 [D loss: 0.548676, acc: 77.34%] [G loss: 1.921196]\n",
      "epoch:20 step:19003 [D loss: 0.655324, acc: 63.28%] [G loss: 1.829812]\n",
      "epoch:20 step:19004 [D loss: 0.599891, acc: 67.19%] [G loss: 1.992281]\n",
      "epoch:20 step:19005 [D loss: 0.701729, acc: 53.91%] [G loss: 1.820968]\n",
      "epoch:20 step:19006 [D loss: 0.647036, acc: 60.16%] [G loss: 1.898617]\n",
      "epoch:20 step:19007 [D loss: 0.644574, acc: 63.28%] [G loss: 1.870732]\n",
      "epoch:20 step:19008 [D loss: 0.643712, acc: 58.59%] [G loss: 1.783572]\n",
      "epoch:20 step:19009 [D loss: 0.643459, acc: 59.38%] [G loss: 1.860072]\n",
      "epoch:20 step:19010 [D loss: 0.632216, acc: 62.50%] [G loss: 1.897077]\n",
      "epoch:20 step:19011 [D loss: 0.622538, acc: 64.06%] [G loss: 1.929447]\n",
      "epoch:20 step:19012 [D loss: 0.598544, acc: 67.19%] [G loss: 1.960218]\n",
      "epoch:20 step:19013 [D loss: 0.696617, acc: 57.81%] [G loss: 1.880640]\n",
      "epoch:20 step:19014 [D loss: 0.600021, acc: 73.44%] [G loss: 1.866616]\n",
      "epoch:20 step:19015 [D loss: 0.622063, acc: 62.50%] [G loss: 2.016556]\n",
      "epoch:20 step:19016 [D loss: 0.560939, acc: 69.53%] [G loss: 2.380068]\n",
      "epoch:20 step:19017 [D loss: 0.611478, acc: 66.41%] [G loss: 1.920807]\n",
      "epoch:20 step:19018 [D loss: 0.673974, acc: 63.28%] [G loss: 1.902001]\n",
      "epoch:20 step:19019 [D loss: 0.681774, acc: 61.72%] [G loss: 1.978902]\n",
      "epoch:20 step:19020 [D loss: 0.613158, acc: 68.75%] [G loss: 2.032086]\n",
      "epoch:20 step:19021 [D loss: 0.682327, acc: 59.38%] [G loss: 1.901465]\n",
      "epoch:20 step:19022 [D loss: 0.657539, acc: 66.41%] [G loss: 1.902655]\n",
      "epoch:20 step:19023 [D loss: 0.637017, acc: 64.06%] [G loss: 1.884600]\n",
      "epoch:20 step:19024 [D loss: 0.667052, acc: 63.28%] [G loss: 1.994631]\n",
      "epoch:20 step:19025 [D loss: 0.672578, acc: 60.94%] [G loss: 1.896815]\n",
      "epoch:20 step:19026 [D loss: 0.629464, acc: 60.16%] [G loss: 2.037786]\n",
      "epoch:20 step:19027 [D loss: 0.644647, acc: 64.06%] [G loss: 1.930844]\n",
      "epoch:20 step:19028 [D loss: 0.662698, acc: 60.16%] [G loss: 1.833065]\n",
      "epoch:20 step:19029 [D loss: 0.627527, acc: 60.94%] [G loss: 1.844229]\n",
      "epoch:20 step:19030 [D loss: 0.649502, acc: 60.16%] [G loss: 1.947178]\n",
      "epoch:20 step:19031 [D loss: 0.699800, acc: 54.69%] [G loss: 1.768275]\n",
      "epoch:20 step:19032 [D loss: 0.672453, acc: 57.81%] [G loss: 1.908096]\n",
      "epoch:20 step:19033 [D loss: 0.637216, acc: 64.84%] [G loss: 1.910551]\n",
      "epoch:20 step:19034 [D loss: 0.634775, acc: 63.28%] [G loss: 1.857573]\n",
      "epoch:20 step:19035 [D loss: 0.638946, acc: 59.38%] [G loss: 1.822561]\n",
      "epoch:20 step:19036 [D loss: 0.630997, acc: 61.72%] [G loss: 1.990577]\n",
      "epoch:20 step:19037 [D loss: 0.611908, acc: 66.41%] [G loss: 1.774868]\n",
      "epoch:20 step:19038 [D loss: 0.663952, acc: 58.59%] [G loss: 1.912970]\n",
      "epoch:20 step:19039 [D loss: 0.694776, acc: 60.16%] [G loss: 1.918345]\n",
      "epoch:20 step:19040 [D loss: 0.600803, acc: 70.31%] [G loss: 1.809031]\n",
      "epoch:20 step:19041 [D loss: 0.635832, acc: 64.84%] [G loss: 1.939080]\n",
      "epoch:20 step:19042 [D loss: 0.646197, acc: 60.16%] [G loss: 1.841611]\n",
      "epoch:20 step:19043 [D loss: 0.680359, acc: 60.16%] [G loss: 1.762321]\n",
      "epoch:20 step:19044 [D loss: 0.621210, acc: 63.28%] [G loss: 1.808159]\n",
      "epoch:20 step:19045 [D loss: 0.695095, acc: 53.91%] [G loss: 1.836579]\n",
      "epoch:20 step:19046 [D loss: 0.677161, acc: 55.47%] [G loss: 1.748889]\n",
      "epoch:20 step:19047 [D loss: 0.634864, acc: 61.72%] [G loss: 1.797864]\n",
      "epoch:20 step:19048 [D loss: 0.668182, acc: 56.25%] [G loss: 1.767604]\n",
      "epoch:20 step:19049 [D loss: 0.621742, acc: 64.84%] [G loss: 1.823870]\n",
      "epoch:20 step:19050 [D loss: 0.666950, acc: 59.38%] [G loss: 1.882249]\n",
      "epoch:20 step:19051 [D loss: 0.631947, acc: 65.62%] [G loss: 1.880204]\n",
      "epoch:20 step:19052 [D loss: 0.635920, acc: 65.62%] [G loss: 2.109705]\n",
      "epoch:20 step:19053 [D loss: 0.613068, acc: 64.84%] [G loss: 2.017595]\n",
      "epoch:20 step:19054 [D loss: 0.580655, acc: 71.88%] [G loss: 2.144048]\n",
      "epoch:20 step:19055 [D loss: 0.608380, acc: 67.97%] [G loss: 2.290657]\n",
      "epoch:20 step:19056 [D loss: 0.731954, acc: 60.94%] [G loss: 1.805658]\n",
      "epoch:20 step:19057 [D loss: 0.655680, acc: 66.41%] [G loss: 1.811040]\n",
      "epoch:20 step:19058 [D loss: 0.603503, acc: 64.84%] [G loss: 1.962136]\n",
      "epoch:20 step:19059 [D loss: 0.655038, acc: 62.50%] [G loss: 1.777196]\n",
      "epoch:20 step:19060 [D loss: 0.695013, acc: 53.91%] [G loss: 1.839279]\n",
      "epoch:20 step:19061 [D loss: 0.651647, acc: 60.94%] [G loss: 1.882043]\n",
      "epoch:20 step:19062 [D loss: 0.642812, acc: 60.94%] [G loss: 1.957257]\n",
      "epoch:20 step:19063 [D loss: 0.667661, acc: 61.72%] [G loss: 1.795146]\n",
      "epoch:20 step:19064 [D loss: 0.688190, acc: 59.38%] [G loss: 1.718639]\n",
      "epoch:20 step:19065 [D loss: 0.647704, acc: 64.06%] [G loss: 1.958198]\n",
      "epoch:20 step:19066 [D loss: 0.686887, acc: 59.38%] [G loss: 1.849156]\n",
      "epoch:20 step:19067 [D loss: 0.679122, acc: 61.72%] [G loss: 1.733047]\n",
      "epoch:20 step:19068 [D loss: 0.686684, acc: 55.47%] [G loss: 1.786548]\n",
      "epoch:20 step:19069 [D loss: 0.678822, acc: 56.25%] [G loss: 1.835878]\n",
      "epoch:20 step:19070 [D loss: 0.656824, acc: 61.72%] [G loss: 1.933103]\n",
      "epoch:20 step:19071 [D loss: 0.608280, acc: 67.97%] [G loss: 1.819924]\n",
      "epoch:20 step:19072 [D loss: 0.616224, acc: 66.41%] [G loss: 2.046677]\n",
      "epoch:20 step:19073 [D loss: 0.624435, acc: 67.97%] [G loss: 2.021575]\n",
      "epoch:20 step:19074 [D loss: 0.634011, acc: 71.09%] [G loss: 2.066639]\n",
      "epoch:20 step:19075 [D loss: 0.627029, acc: 61.72%] [G loss: 1.981934]\n",
      "epoch:20 step:19076 [D loss: 0.599876, acc: 71.09%] [G loss: 1.992913]\n",
      "epoch:20 step:19077 [D loss: 0.665509, acc: 66.41%] [G loss: 1.955642]\n",
      "epoch:20 step:19078 [D loss: 0.631682, acc: 62.50%] [G loss: 2.124437]\n",
      "epoch:20 step:19079 [D loss: 0.600364, acc: 68.75%] [G loss: 1.939897]\n",
      "epoch:20 step:19080 [D loss: 0.640059, acc: 67.19%] [G loss: 1.997675]\n",
      "epoch:20 step:19081 [D loss: 0.727170, acc: 50.78%] [G loss: 1.797464]\n",
      "epoch:20 step:19082 [D loss: 0.637070, acc: 61.72%] [G loss: 1.888836]\n",
      "epoch:20 step:19083 [D loss: 0.669063, acc: 55.47%] [G loss: 1.859312]\n",
      "epoch:20 step:19084 [D loss: 0.621639, acc: 64.06%] [G loss: 1.822456]\n",
      "epoch:20 step:19085 [D loss: 0.632267, acc: 65.62%] [G loss: 2.058499]\n",
      "epoch:20 step:19086 [D loss: 0.572301, acc: 70.31%] [G loss: 2.219783]\n",
      "epoch:20 step:19087 [D loss: 0.512601, acc: 77.34%] [G loss: 2.580336]\n",
      "epoch:20 step:19088 [D loss: 0.714883, acc: 60.16%] [G loss: 2.001634]\n",
      "epoch:20 step:19089 [D loss: 0.805257, acc: 46.09%] [G loss: 1.692015]\n",
      "epoch:20 step:19090 [D loss: 0.659793, acc: 58.59%] [G loss: 1.776860]\n",
      "epoch:20 step:19091 [D loss: 0.668988, acc: 54.69%] [G loss: 1.832352]\n",
      "epoch:20 step:19092 [D loss: 0.663057, acc: 61.72%] [G loss: 1.925600]\n",
      "epoch:20 step:19093 [D loss: 0.718379, acc: 57.81%] [G loss: 1.906140]\n",
      "epoch:20 step:19094 [D loss: 0.664791, acc: 60.16%] [G loss: 1.841339]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:20 step:19095 [D loss: 0.736468, acc: 51.56%] [G loss: 1.767090]\n",
      "epoch:20 step:19096 [D loss: 0.661968, acc: 59.38%] [G loss: 1.696201]\n",
      "epoch:20 step:19097 [D loss: 0.632451, acc: 66.41%] [G loss: 1.840143]\n",
      "epoch:20 step:19098 [D loss: 0.613387, acc: 62.50%] [G loss: 1.917530]\n",
      "epoch:20 step:19099 [D loss: 0.632098, acc: 60.94%] [G loss: 1.911529]\n",
      "epoch:20 step:19100 [D loss: 0.637403, acc: 63.28%] [G loss: 1.908186]\n",
      "epoch:20 step:19101 [D loss: 0.665252, acc: 60.16%] [G loss: 1.776403]\n",
      "epoch:20 step:19102 [D loss: 0.705392, acc: 53.91%] [G loss: 1.823287]\n",
      "epoch:20 step:19103 [D loss: 0.640656, acc: 64.06%] [G loss: 1.831818]\n",
      "epoch:20 step:19104 [D loss: 0.584314, acc: 70.31%] [G loss: 1.831223]\n",
      "epoch:20 step:19105 [D loss: 0.636912, acc: 62.50%] [G loss: 1.827290]\n",
      "epoch:20 step:19106 [D loss: 0.611701, acc: 67.97%] [G loss: 1.916701]\n",
      "epoch:20 step:19107 [D loss: 0.648114, acc: 60.16%] [G loss: 2.017976]\n",
      "epoch:20 step:19108 [D loss: 0.656331, acc: 59.38%] [G loss: 1.746457]\n",
      "epoch:20 step:19109 [D loss: 0.663721, acc: 58.59%] [G loss: 1.878754]\n",
      "epoch:20 step:19110 [D loss: 0.647726, acc: 63.28%] [G loss: 1.948428]\n",
      "epoch:20 step:19111 [D loss: 0.640345, acc: 60.16%] [G loss: 2.039246]\n",
      "epoch:20 step:19112 [D loss: 0.663431, acc: 56.25%] [G loss: 1.925982]\n",
      "epoch:20 step:19113 [D loss: 0.702751, acc: 51.56%] [G loss: 1.728231]\n",
      "epoch:20 step:19114 [D loss: 0.639024, acc: 60.94%] [G loss: 1.899870]\n",
      "epoch:20 step:19115 [D loss: 0.652405, acc: 56.25%] [G loss: 1.864948]\n",
      "epoch:20 step:19116 [D loss: 0.642427, acc: 61.72%] [G loss: 1.831207]\n",
      "epoch:20 step:19117 [D loss: 0.679056, acc: 55.47%] [G loss: 1.727743]\n",
      "epoch:20 step:19118 [D loss: 0.630846, acc: 64.06%] [G loss: 1.818085]\n",
      "epoch:20 step:19119 [D loss: 0.665406, acc: 60.94%] [G loss: 1.864493]\n",
      "epoch:20 step:19120 [D loss: 0.656490, acc: 57.81%] [G loss: 1.899329]\n",
      "epoch:20 step:19121 [D loss: 0.616027, acc: 64.06%] [G loss: 2.173678]\n",
      "epoch:20 step:19122 [D loss: 0.667836, acc: 62.50%] [G loss: 1.912193]\n",
      "epoch:20 step:19123 [D loss: 0.633665, acc: 60.16%] [G loss: 2.000469]\n",
      "epoch:20 step:19124 [D loss: 0.634321, acc: 62.50%] [G loss: 1.892659]\n",
      "epoch:20 step:19125 [D loss: 0.649553, acc: 65.62%] [G loss: 1.908639]\n",
      "epoch:20 step:19126 [D loss: 0.703193, acc: 50.00%] [G loss: 1.658424]\n",
      "epoch:20 step:19127 [D loss: 0.666888, acc: 60.94%] [G loss: 1.858575]\n",
      "epoch:20 step:19128 [D loss: 0.655682, acc: 61.72%] [G loss: 1.904902]\n",
      "epoch:20 step:19129 [D loss: 0.684423, acc: 60.16%] [G loss: 1.879841]\n",
      "epoch:20 step:19130 [D loss: 0.676439, acc: 61.72%] [G loss: 1.909136]\n",
      "epoch:20 step:19131 [D loss: 0.640698, acc: 63.28%] [G loss: 1.741900]\n",
      "epoch:20 step:19132 [D loss: 0.605430, acc: 65.62%] [G loss: 1.845749]\n",
      "epoch:20 step:19133 [D loss: 0.655559, acc: 57.03%] [G loss: 1.789905]\n",
      "epoch:20 step:19134 [D loss: 0.677509, acc: 56.25%] [G loss: 1.903141]\n",
      "epoch:20 step:19135 [D loss: 0.613719, acc: 72.66%] [G loss: 1.850168]\n",
      "epoch:20 step:19136 [D loss: 0.677678, acc: 60.94%] [G loss: 1.721877]\n",
      "epoch:20 step:19137 [D loss: 0.645682, acc: 64.06%] [G loss: 1.863592]\n",
      "epoch:20 step:19138 [D loss: 0.625880, acc: 69.53%] [G loss: 1.814248]\n",
      "epoch:20 step:19139 [D loss: 0.650193, acc: 60.16%] [G loss: 1.789776]\n",
      "epoch:20 step:19140 [D loss: 0.630919, acc: 65.62%] [G loss: 1.808643]\n",
      "epoch:20 step:19141 [D loss: 0.640152, acc: 63.28%] [G loss: 1.909133]\n",
      "epoch:20 step:19142 [D loss: 0.604223, acc: 68.75%] [G loss: 1.810919]\n",
      "epoch:20 step:19143 [D loss: 0.658513, acc: 59.38%] [G loss: 1.946272]\n",
      "epoch:20 step:19144 [D loss: 0.613546, acc: 63.28%] [G loss: 1.903493]\n",
      "epoch:20 step:19145 [D loss: 0.584976, acc: 72.66%] [G loss: 2.038758]\n",
      "epoch:20 step:19146 [D loss: 0.611685, acc: 64.84%] [G loss: 2.110284]\n",
      "epoch:20 step:19147 [D loss: 0.604232, acc: 72.66%] [G loss: 1.991508]\n",
      "epoch:20 step:19148 [D loss: 0.613802, acc: 64.06%] [G loss: 1.833326]\n",
      "epoch:20 step:19149 [D loss: 0.670457, acc: 60.16%] [G loss: 1.964121]\n",
      "epoch:20 step:19150 [D loss: 0.620818, acc: 66.41%] [G loss: 1.868333]\n",
      "epoch:20 step:19151 [D loss: 0.655757, acc: 57.03%] [G loss: 1.899995]\n",
      "epoch:20 step:19152 [D loss: 0.629358, acc: 66.41%] [G loss: 1.975051]\n",
      "epoch:20 step:19153 [D loss: 0.653993, acc: 61.72%] [G loss: 1.883211]\n",
      "epoch:20 step:19154 [D loss: 0.674346, acc: 54.69%] [G loss: 1.872307]\n",
      "epoch:20 step:19155 [D loss: 0.650798, acc: 60.94%] [G loss: 2.064686]\n",
      "epoch:20 step:19156 [D loss: 0.594229, acc: 68.75%] [G loss: 1.967478]\n",
      "epoch:20 step:19157 [D loss: 0.614094, acc: 68.75%] [G loss: 1.955019]\n",
      "epoch:20 step:19158 [D loss: 0.639327, acc: 64.06%] [G loss: 1.903012]\n",
      "epoch:20 step:19159 [D loss: 0.651361, acc: 60.16%] [G loss: 2.060062]\n",
      "epoch:20 step:19160 [D loss: 0.707589, acc: 54.69%] [G loss: 1.897104]\n",
      "epoch:20 step:19161 [D loss: 0.715294, acc: 57.03%] [G loss: 1.930085]\n",
      "epoch:20 step:19162 [D loss: 0.637307, acc: 64.06%] [G loss: 1.851216]\n",
      "epoch:20 step:19163 [D loss: 0.670873, acc: 63.28%] [G loss: 1.891082]\n",
      "epoch:20 step:19164 [D loss: 0.685139, acc: 59.38%] [G loss: 1.828256]\n",
      "epoch:20 step:19165 [D loss: 0.634839, acc: 63.28%] [G loss: 1.871979]\n",
      "epoch:20 step:19166 [D loss: 0.670113, acc: 56.25%] [G loss: 1.866991]\n",
      "epoch:20 step:19167 [D loss: 0.695145, acc: 58.59%] [G loss: 1.951719]\n",
      "epoch:20 step:19168 [D loss: 0.622562, acc: 61.72%] [G loss: 2.000854]\n",
      "epoch:20 step:19169 [D loss: 0.618417, acc: 64.84%] [G loss: 1.915806]\n",
      "epoch:20 step:19170 [D loss: 0.602166, acc: 70.31%] [G loss: 2.122799]\n",
      "epoch:20 step:19171 [D loss: 0.650127, acc: 61.72%] [G loss: 1.944945]\n",
      "epoch:20 step:19172 [D loss: 0.636632, acc: 64.06%] [G loss: 1.832100]\n",
      "epoch:20 step:19173 [D loss: 0.645824, acc: 64.84%] [G loss: 1.803323]\n",
      "epoch:20 step:19174 [D loss: 0.605684, acc: 67.97%] [G loss: 1.986492]\n",
      "epoch:20 step:19175 [D loss: 0.657670, acc: 63.28%] [G loss: 1.992226]\n",
      "epoch:20 step:19176 [D loss: 0.626069, acc: 65.62%] [G loss: 1.887833]\n",
      "epoch:20 step:19177 [D loss: 0.741325, acc: 50.78%] [G loss: 1.792613]\n",
      "epoch:20 step:19178 [D loss: 0.698645, acc: 57.03%] [G loss: 1.680711]\n",
      "epoch:20 step:19179 [D loss: 0.683761, acc: 57.81%] [G loss: 1.878741]\n",
      "epoch:20 step:19180 [D loss: 0.684305, acc: 54.69%] [G loss: 1.770292]\n",
      "epoch:20 step:19181 [D loss: 0.703597, acc: 53.91%] [G loss: 1.814671]\n",
      "epoch:20 step:19182 [D loss: 0.680350, acc: 56.25%] [G loss: 1.785194]\n",
      "epoch:20 step:19183 [D loss: 0.646312, acc: 58.59%] [G loss: 1.932694]\n",
      "epoch:20 step:19184 [D loss: 0.684738, acc: 56.25%] [G loss: 1.737883]\n",
      "epoch:20 step:19185 [D loss: 0.614934, acc: 65.62%] [G loss: 1.767524]\n",
      "epoch:20 step:19186 [D loss: 0.643569, acc: 65.62%] [G loss: 1.806534]\n",
      "epoch:20 step:19187 [D loss: 0.666351, acc: 56.25%] [G loss: 1.792841]\n",
      "epoch:20 step:19188 [D loss: 0.663911, acc: 65.62%] [G loss: 1.692601]\n",
      "epoch:20 step:19189 [D loss: 0.681068, acc: 65.62%] [G loss: 1.748886]\n",
      "epoch:20 step:19190 [D loss: 0.644256, acc: 63.28%] [G loss: 1.812757]\n",
      "epoch:20 step:19191 [D loss: 0.624491, acc: 66.41%] [G loss: 1.807575]\n",
      "epoch:20 step:19192 [D loss: 0.634791, acc: 65.62%] [G loss: 1.860271]\n",
      "epoch:20 step:19193 [D loss: 0.632356, acc: 64.84%] [G loss: 1.915859]\n",
      "epoch:20 step:19194 [D loss: 0.635462, acc: 62.50%] [G loss: 1.776144]\n",
      "epoch:20 step:19195 [D loss: 0.631584, acc: 63.28%] [G loss: 1.708958]\n",
      "epoch:20 step:19196 [D loss: 0.681683, acc: 57.81%] [G loss: 1.918890]\n",
      "epoch:20 step:19197 [D loss: 0.577301, acc: 74.22%] [G loss: 2.075810]\n",
      "epoch:20 step:19198 [D loss: 0.686526, acc: 57.03%] [G loss: 1.693321]\n",
      "epoch:20 step:19199 [D loss: 0.650327, acc: 64.84%] [G loss: 1.739830]\n",
      "epoch:20 step:19200 [D loss: 0.697968, acc: 57.81%] [G loss: 1.781183]\n",
      "##############\n",
      "[2.36018173 1.27135491 6.36073339 4.92895773 3.51809357 5.50995417\n",
      " 4.29852594 4.6608894  4.63950151 3.63676587]\n",
      "##########\n",
      "epoch:20 step:19201 [D loss: 0.653382, acc: 60.16%] [G loss: 1.851540]\n",
      "epoch:20 step:19202 [D loss: 0.655894, acc: 58.59%] [G loss: 1.808970]\n",
      "epoch:20 step:19203 [D loss: 0.630078, acc: 65.62%] [G loss: 1.889558]\n",
      "epoch:20 step:19204 [D loss: 0.614852, acc: 71.09%] [G loss: 1.787260]\n",
      "epoch:20 step:19205 [D loss: 0.715437, acc: 60.94%] [G loss: 1.741365]\n",
      "epoch:20 step:19206 [D loss: 0.652967, acc: 58.59%] [G loss: 1.944757]\n",
      "epoch:20 step:19207 [D loss: 0.677539, acc: 61.72%] [G loss: 1.860740]\n",
      "epoch:20 step:19208 [D loss: 0.679490, acc: 58.59%] [G loss: 1.834120]\n",
      "epoch:20 step:19209 [D loss: 0.661014, acc: 59.38%] [G loss: 1.922035]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:20 step:19210 [D loss: 0.610479, acc: 65.62%] [G loss: 1.913522]\n",
      "epoch:20 step:19211 [D loss: 0.591398, acc: 69.53%] [G loss: 2.207455]\n",
      "epoch:20 step:19212 [D loss: 0.546924, acc: 77.34%] [G loss: 2.172230]\n",
      "epoch:20 step:19213 [D loss: 0.711035, acc: 50.78%] [G loss: 1.780437]\n",
      "epoch:20 step:19214 [D loss: 0.669421, acc: 66.41%] [G loss: 1.829367]\n",
      "epoch:20 step:19215 [D loss: 0.671154, acc: 53.12%] [G loss: 1.957870]\n",
      "epoch:20 step:19216 [D loss: 0.632109, acc: 66.41%] [G loss: 1.827192]\n",
      "epoch:20 step:19217 [D loss: 0.653034, acc: 63.28%] [G loss: 1.865908]\n",
      "epoch:20 step:19218 [D loss: 0.627910, acc: 67.19%] [G loss: 1.828253]\n",
      "epoch:20 step:19219 [D loss: 0.642758, acc: 60.94%] [G loss: 2.129879]\n",
      "epoch:20 step:19220 [D loss: 0.650719, acc: 58.59%] [G loss: 1.866179]\n",
      "epoch:20 step:19221 [D loss: 0.611060, acc: 67.97%] [G loss: 2.052113]\n",
      "epoch:20 step:19222 [D loss: 0.672435, acc: 59.38%] [G loss: 1.770121]\n",
      "epoch:20 step:19223 [D loss: 0.686436, acc: 56.25%] [G loss: 1.725316]\n",
      "epoch:20 step:19224 [D loss: 0.667077, acc: 61.72%] [G loss: 1.897799]\n",
      "epoch:20 step:19225 [D loss: 0.679049, acc: 59.38%] [G loss: 1.867377]\n",
      "epoch:20 step:19226 [D loss: 0.622627, acc: 68.75%] [G loss: 1.874869]\n",
      "epoch:20 step:19227 [D loss: 0.661060, acc: 60.94%] [G loss: 1.834195]\n",
      "epoch:20 step:19228 [D loss: 0.604250, acc: 72.66%] [G loss: 1.995910]\n",
      "epoch:20 step:19229 [D loss: 0.601720, acc: 71.09%] [G loss: 1.871567]\n",
      "epoch:20 step:19230 [D loss: 0.657675, acc: 64.06%] [G loss: 1.880390]\n",
      "epoch:20 step:19231 [D loss: 0.637163, acc: 64.06%] [G loss: 2.025624]\n",
      "epoch:20 step:19232 [D loss: 0.692503, acc: 56.25%] [G loss: 1.840305]\n",
      "epoch:20 step:19233 [D loss: 0.659148, acc: 60.16%] [G loss: 1.803263]\n",
      "epoch:20 step:19234 [D loss: 0.626981, acc: 61.72%] [G loss: 2.033383]\n",
      "epoch:20 step:19235 [D loss: 0.598561, acc: 70.31%] [G loss: 1.967388]\n",
      "epoch:20 step:19236 [D loss: 0.631007, acc: 64.84%] [G loss: 1.876403]\n",
      "epoch:20 step:19237 [D loss: 0.615656, acc: 64.84%] [G loss: 2.104966]\n",
      "epoch:20 step:19238 [D loss: 0.585132, acc: 71.09%] [G loss: 1.969957]\n",
      "epoch:20 step:19239 [D loss: 0.594037, acc: 64.84%] [G loss: 2.099707]\n",
      "epoch:20 step:19240 [D loss: 0.734242, acc: 47.66%] [G loss: 1.749505]\n",
      "epoch:20 step:19241 [D loss: 0.721285, acc: 55.47%] [G loss: 1.759057]\n",
      "epoch:20 step:19242 [D loss: 0.652292, acc: 63.28%] [G loss: 1.683937]\n",
      "epoch:20 step:19243 [D loss: 0.646434, acc: 66.41%] [G loss: 2.014688]\n",
      "epoch:20 step:19244 [D loss: 0.638932, acc: 67.19%] [G loss: 1.921687]\n",
      "epoch:20 step:19245 [D loss: 0.641001, acc: 67.97%] [G loss: 1.848378]\n",
      "epoch:20 step:19246 [D loss: 0.670967, acc: 54.69%] [G loss: 1.864433]\n",
      "epoch:20 step:19247 [D loss: 0.635374, acc: 60.94%] [G loss: 1.818822]\n",
      "epoch:20 step:19248 [D loss: 0.586701, acc: 69.53%] [G loss: 1.971107]\n",
      "epoch:20 step:19249 [D loss: 0.651680, acc: 60.16%] [G loss: 1.941306]\n",
      "epoch:20 step:19250 [D loss: 0.656493, acc: 59.38%] [G loss: 1.879195]\n",
      "epoch:20 step:19251 [D loss: 0.679501, acc: 60.94%] [G loss: 1.784339]\n",
      "epoch:20 step:19252 [D loss: 0.659267, acc: 63.28%] [G loss: 1.909057]\n",
      "epoch:20 step:19253 [D loss: 0.627936, acc: 64.06%] [G loss: 1.888549]\n",
      "epoch:20 step:19254 [D loss: 0.628742, acc: 64.84%] [G loss: 1.955516]\n",
      "epoch:20 step:19255 [D loss: 0.639156, acc: 55.47%] [G loss: 1.868822]\n",
      "epoch:20 step:19256 [D loss: 0.616562, acc: 65.62%] [G loss: 2.049458]\n",
      "epoch:20 step:19257 [D loss: 0.603886, acc: 72.66%] [G loss: 1.884476]\n",
      "epoch:20 step:19258 [D loss: 0.672171, acc: 53.91%] [G loss: 1.825773]\n",
      "epoch:20 step:19259 [D loss: 0.632128, acc: 64.84%] [G loss: 1.959044]\n",
      "epoch:20 step:19260 [D loss: 0.598155, acc: 64.84%] [G loss: 1.943665]\n",
      "epoch:20 step:19261 [D loss: 0.612290, acc: 64.06%] [G loss: 2.031400]\n",
      "epoch:20 step:19262 [D loss: 0.726192, acc: 50.78%] [G loss: 2.022001]\n",
      "epoch:20 step:19263 [D loss: 0.595730, acc: 71.88%] [G loss: 1.938138]\n",
      "epoch:20 step:19264 [D loss: 0.715533, acc: 56.25%] [G loss: 1.966058]\n",
      "epoch:20 step:19265 [D loss: 0.694197, acc: 55.47%] [G loss: 1.910701]\n",
      "epoch:20 step:19266 [D loss: 0.677091, acc: 59.38%] [G loss: 1.785769]\n",
      "epoch:20 step:19267 [D loss: 0.635020, acc: 63.28%] [G loss: 1.917416]\n",
      "epoch:20 step:19268 [D loss: 0.702206, acc: 51.56%] [G loss: 1.673514]\n",
      "epoch:20 step:19269 [D loss: 0.650795, acc: 57.03%] [G loss: 1.685830]\n",
      "epoch:20 step:19270 [D loss: 0.677851, acc: 59.38%] [G loss: 1.718787]\n",
      "epoch:20 step:19271 [D loss: 0.637536, acc: 64.06%] [G loss: 1.916253]\n",
      "epoch:20 step:19272 [D loss: 0.623403, acc: 64.06%] [G loss: 2.097188]\n",
      "epoch:20 step:19273 [D loss: 0.655353, acc: 61.72%] [G loss: 2.078557]\n",
      "epoch:20 step:19274 [D loss: 0.616466, acc: 63.28%] [G loss: 2.027122]\n",
      "epoch:20 step:19275 [D loss: 0.620281, acc: 61.72%] [G loss: 1.820867]\n",
      "epoch:20 step:19276 [D loss: 0.637453, acc: 67.19%] [G loss: 1.759486]\n",
      "epoch:20 step:19277 [D loss: 0.685732, acc: 57.81%] [G loss: 1.816515]\n",
      "epoch:20 step:19278 [D loss: 0.642579, acc: 62.50%] [G loss: 1.812879]\n",
      "epoch:20 step:19279 [D loss: 0.595799, acc: 67.97%] [G loss: 1.824298]\n",
      "epoch:20 step:19280 [D loss: 0.665895, acc: 61.72%] [G loss: 1.842096]\n",
      "epoch:20 step:19281 [D loss: 0.690394, acc: 57.81%] [G loss: 1.725993]\n",
      "epoch:20 step:19282 [D loss: 0.645067, acc: 61.72%] [G loss: 1.860741]\n",
      "epoch:20 step:19283 [D loss: 0.670530, acc: 62.50%] [G loss: 1.825667]\n",
      "epoch:20 step:19284 [D loss: 0.616504, acc: 67.19%] [G loss: 1.757224]\n",
      "epoch:20 step:19285 [D loss: 0.658850, acc: 57.81%] [G loss: 1.842101]\n",
      "epoch:20 step:19286 [D loss: 0.651126, acc: 60.94%] [G loss: 1.748458]\n",
      "epoch:20 step:19287 [D loss: 0.648650, acc: 64.84%] [G loss: 1.957673]\n",
      "epoch:20 step:19288 [D loss: 0.612431, acc: 71.09%] [G loss: 2.050628]\n",
      "epoch:20 step:19289 [D loss: 0.572919, acc: 71.88%] [G loss: 2.092557]\n",
      "epoch:20 step:19290 [D loss: 0.600755, acc: 63.28%] [G loss: 2.116519]\n",
      "epoch:20 step:19291 [D loss: 0.593885, acc: 67.97%] [G loss: 2.022846]\n",
      "epoch:20 step:19292 [D loss: 0.603070, acc: 70.31%] [G loss: 1.989585]\n",
      "epoch:20 step:19293 [D loss: 0.652340, acc: 59.38%] [G loss: 1.903048]\n",
      "epoch:20 step:19294 [D loss: 0.648952, acc: 65.62%] [G loss: 2.134155]\n",
      "epoch:20 step:19295 [D loss: 0.621060, acc: 60.16%] [G loss: 1.965016]\n",
      "epoch:20 step:19296 [D loss: 0.650496, acc: 66.41%] [G loss: 2.044452]\n",
      "epoch:20 step:19297 [D loss: 0.592218, acc: 67.97%] [G loss: 1.944061]\n",
      "epoch:20 step:19298 [D loss: 0.673331, acc: 64.06%] [G loss: 2.075598]\n",
      "epoch:20 step:19299 [D loss: 0.679192, acc: 51.56%] [G loss: 1.869904]\n",
      "epoch:20 step:19300 [D loss: 0.669169, acc: 60.94%] [G loss: 1.971194]\n",
      "epoch:20 step:19301 [D loss: 0.586518, acc: 69.53%] [G loss: 1.961619]\n",
      "epoch:20 step:19302 [D loss: 0.620546, acc: 67.19%] [G loss: 2.019176]\n",
      "epoch:20 step:19303 [D loss: 0.657082, acc: 61.72%] [G loss: 1.917492]\n",
      "epoch:20 step:19304 [D loss: 0.620446, acc: 65.62%] [G loss: 2.164493]\n",
      "epoch:20 step:19305 [D loss: 0.679644, acc: 53.91%] [G loss: 1.902773]\n",
      "epoch:20 step:19306 [D loss: 0.720550, acc: 53.12%] [G loss: 1.689668]\n",
      "epoch:20 step:19307 [D loss: 0.667084, acc: 57.03%] [G loss: 1.817708]\n",
      "epoch:20 step:19308 [D loss: 0.692549, acc: 58.59%] [G loss: 1.883060]\n",
      "epoch:20 step:19309 [D loss: 0.669713, acc: 60.16%] [G loss: 1.861069]\n",
      "epoch:20 step:19310 [D loss: 0.590864, acc: 68.75%] [G loss: 1.881431]\n",
      "epoch:20 step:19311 [D loss: 0.609727, acc: 67.19%] [G loss: 1.861238]\n",
      "epoch:20 step:19312 [D loss: 0.712901, acc: 53.12%] [G loss: 1.866161]\n",
      "epoch:20 step:19313 [D loss: 0.672808, acc: 65.62%] [G loss: 1.836775]\n",
      "epoch:20 step:19314 [D loss: 0.648656, acc: 59.38%] [G loss: 1.929342]\n",
      "epoch:20 step:19315 [D loss: 0.671338, acc: 63.28%] [G loss: 1.860859]\n",
      "epoch:20 step:19316 [D loss: 0.670242, acc: 60.94%] [G loss: 1.749571]\n",
      "epoch:20 step:19317 [D loss: 0.643720, acc: 60.16%] [G loss: 1.765387]\n",
      "epoch:20 step:19318 [D loss: 0.618672, acc: 67.19%] [G loss: 1.914296]\n",
      "epoch:20 step:19319 [D loss: 0.670679, acc: 58.59%] [G loss: 1.761540]\n",
      "epoch:20 step:19320 [D loss: 0.610623, acc: 66.41%] [G loss: 1.767721]\n",
      "epoch:20 step:19321 [D loss: 0.655290, acc: 63.28%] [G loss: 1.898542]\n",
      "epoch:20 step:19322 [D loss: 0.625634, acc: 64.06%] [G loss: 2.142900]\n",
      "epoch:20 step:19323 [D loss: 0.598448, acc: 70.31%] [G loss: 1.767125]\n",
      "epoch:20 step:19324 [D loss: 0.694533, acc: 57.03%] [G loss: 1.770465]\n",
      "epoch:20 step:19325 [D loss: 0.653858, acc: 57.81%] [G loss: 1.861380]\n",
      "epoch:20 step:19326 [D loss: 0.685744, acc: 51.56%] [G loss: 1.757164]\n",
      "epoch:20 step:19327 [D loss: 0.639985, acc: 61.72%] [G loss: 1.889261]\n",
      "epoch:20 step:19328 [D loss: 0.631169, acc: 63.28%] [G loss: 1.926688]\n",
      "epoch:20 step:19329 [D loss: 0.628724, acc: 67.19%] [G loss: 1.985592]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:20 step:19330 [D loss: 0.664192, acc: 60.94%] [G loss: 1.864398]\n",
      "epoch:20 step:19331 [D loss: 0.653148, acc: 64.84%] [G loss: 1.878846]\n",
      "epoch:20 step:19332 [D loss: 0.621600, acc: 66.41%] [G loss: 1.904220]\n",
      "epoch:20 step:19333 [D loss: 0.681324, acc: 53.91%] [G loss: 1.896288]\n",
      "epoch:20 step:19334 [D loss: 0.631376, acc: 64.06%] [G loss: 1.838031]\n",
      "epoch:20 step:19335 [D loss: 0.645401, acc: 65.62%] [G loss: 1.884762]\n",
      "epoch:20 step:19336 [D loss: 0.664234, acc: 58.59%] [G loss: 1.789574]\n",
      "epoch:20 step:19337 [D loss: 0.676665, acc: 55.47%] [G loss: 1.807778]\n",
      "epoch:20 step:19338 [D loss: 0.612773, acc: 69.53%] [G loss: 2.021916]\n",
      "epoch:20 step:19339 [D loss: 0.658000, acc: 61.72%] [G loss: 1.902406]\n",
      "epoch:20 step:19340 [D loss: 0.704215, acc: 56.25%] [G loss: 1.779337]\n",
      "epoch:20 step:19341 [D loss: 0.645026, acc: 63.28%] [G loss: 1.925220]\n",
      "epoch:20 step:19342 [D loss: 0.657279, acc: 60.94%] [G loss: 1.787816]\n",
      "epoch:20 step:19343 [D loss: 0.617009, acc: 67.19%] [G loss: 1.970029]\n",
      "epoch:20 step:19344 [D loss: 0.697711, acc: 50.78%] [G loss: 1.887319]\n",
      "epoch:20 step:19345 [D loss: 0.632636, acc: 64.84%] [G loss: 1.980266]\n",
      "epoch:20 step:19346 [D loss: 0.665412, acc: 57.03%] [G loss: 1.860998]\n",
      "epoch:20 step:19347 [D loss: 0.605262, acc: 67.19%] [G loss: 1.913702]\n",
      "epoch:20 step:19348 [D loss: 0.659149, acc: 54.69%] [G loss: 1.861394]\n",
      "epoch:20 step:19349 [D loss: 0.651825, acc: 62.50%] [G loss: 1.812840]\n",
      "epoch:20 step:19350 [D loss: 0.700469, acc: 57.81%] [G loss: 1.748866]\n",
      "epoch:20 step:19351 [D loss: 0.662693, acc: 60.16%] [G loss: 1.890538]\n",
      "epoch:20 step:19352 [D loss: 0.654977, acc: 60.16%] [G loss: 1.767447]\n",
      "epoch:20 step:19353 [D loss: 0.639730, acc: 62.50%] [G loss: 1.880449]\n",
      "epoch:20 step:19354 [D loss: 0.686734, acc: 55.47%] [G loss: 1.706261]\n",
      "epoch:20 step:19355 [D loss: 0.732526, acc: 50.00%] [G loss: 1.715831]\n",
      "epoch:20 step:19356 [D loss: 0.663764, acc: 57.81%] [G loss: 1.751140]\n",
      "epoch:20 step:19357 [D loss: 0.673851, acc: 60.94%] [G loss: 1.781931]\n",
      "epoch:20 step:19358 [D loss: 0.652417, acc: 60.94%] [G loss: 1.819869]\n",
      "epoch:20 step:19359 [D loss: 0.658039, acc: 59.38%] [G loss: 1.830352]\n",
      "epoch:20 step:19360 [D loss: 0.652807, acc: 59.38%] [G loss: 1.816528]\n",
      "epoch:20 step:19361 [D loss: 0.643144, acc: 63.28%] [G loss: 1.925107]\n",
      "epoch:20 step:19362 [D loss: 0.679041, acc: 56.25%] [G loss: 1.918466]\n",
      "epoch:20 step:19363 [D loss: 0.679413, acc: 58.59%] [G loss: 1.899054]\n",
      "epoch:20 step:19364 [D loss: 0.616130, acc: 70.31%] [G loss: 2.035984]\n",
      "epoch:20 step:19365 [D loss: 0.658636, acc: 61.72%] [G loss: 1.786488]\n",
      "epoch:20 step:19366 [D loss: 0.637930, acc: 65.62%] [G loss: 1.756834]\n",
      "epoch:20 step:19367 [D loss: 0.628646, acc: 66.41%] [G loss: 1.939479]\n",
      "epoch:20 step:19368 [D loss: 0.669178, acc: 55.47%] [G loss: 1.747977]\n",
      "epoch:20 step:19369 [D loss: 0.664100, acc: 60.16%] [G loss: 1.817809]\n",
      "epoch:20 step:19370 [D loss: 0.631525, acc: 63.28%] [G loss: 1.862991]\n",
      "epoch:20 step:19371 [D loss: 0.594677, acc: 67.97%] [G loss: 1.958929]\n",
      "epoch:20 step:19372 [D loss: 0.611185, acc: 66.41%] [G loss: 1.954197]\n",
      "epoch:20 step:19373 [D loss: 0.640582, acc: 63.28%] [G loss: 1.930800]\n",
      "epoch:20 step:19374 [D loss: 0.622097, acc: 67.19%] [G loss: 2.055631]\n",
      "epoch:20 step:19375 [D loss: 0.667046, acc: 60.94%] [G loss: 2.004969]\n",
      "epoch:20 step:19376 [D loss: 0.645521, acc: 57.03%] [G loss: 1.751010]\n",
      "epoch:20 step:19377 [D loss: 0.693525, acc: 63.28%] [G loss: 2.040874]\n",
      "epoch:20 step:19378 [D loss: 0.640814, acc: 64.84%] [G loss: 1.963959]\n",
      "epoch:20 step:19379 [D loss: 0.692352, acc: 56.25%] [G loss: 1.915498]\n",
      "epoch:20 step:19380 [D loss: 0.632139, acc: 64.06%] [G loss: 1.858063]\n",
      "epoch:20 step:19381 [D loss: 0.623944, acc: 68.75%] [G loss: 1.966310]\n",
      "epoch:20 step:19382 [D loss: 0.611891, acc: 67.97%] [G loss: 2.047921]\n",
      "epoch:20 step:19383 [D loss: 0.654700, acc: 63.28%] [G loss: 1.804500]\n",
      "epoch:20 step:19384 [D loss: 0.662974, acc: 64.06%] [G loss: 1.847904]\n",
      "epoch:20 step:19385 [D loss: 0.632655, acc: 64.06%] [G loss: 1.952819]\n",
      "epoch:20 step:19386 [D loss: 0.602958, acc: 66.41%] [G loss: 2.134753]\n",
      "epoch:20 step:19387 [D loss: 0.587319, acc: 66.41%] [G loss: 2.244505]\n",
      "epoch:20 step:19388 [D loss: 0.586264, acc: 64.06%] [G loss: 2.217337]\n",
      "epoch:20 step:19389 [D loss: 0.583694, acc: 73.44%] [G loss: 2.132405]\n",
      "epoch:20 step:19390 [D loss: 0.617960, acc: 66.41%] [G loss: 2.165556]\n",
      "epoch:20 step:19391 [D loss: 0.645546, acc: 63.28%] [G loss: 1.875746]\n",
      "epoch:20 step:19392 [D loss: 0.636186, acc: 64.84%] [G loss: 1.889400]\n",
      "epoch:20 step:19393 [D loss: 0.617496, acc: 66.41%] [G loss: 1.982352]\n",
      "epoch:20 step:19394 [D loss: 0.663488, acc: 66.41%] [G loss: 1.922740]\n",
      "epoch:20 step:19395 [D loss: 0.662428, acc: 59.38%] [G loss: 1.936985]\n",
      "epoch:20 step:19396 [D loss: 0.670483, acc: 61.72%] [G loss: 1.803726]\n",
      "epoch:20 step:19397 [D loss: 0.695235, acc: 53.91%] [G loss: 1.844271]\n",
      "epoch:20 step:19398 [D loss: 0.718544, acc: 58.59%] [G loss: 1.726567]\n",
      "epoch:20 step:19399 [D loss: 0.670771, acc: 55.47%] [G loss: 1.787646]\n",
      "epoch:20 step:19400 [D loss: 0.626901, acc: 62.50%] [G loss: 1.811853]\n",
      "##############\n",
      "[2.52369063 1.42883012 6.27840739 4.75993519 3.63908436 5.64643647\n",
      " 4.33117743 4.6777447  4.53565383 3.49613976]\n",
      "##########\n",
      "epoch:20 step:19401 [D loss: 0.650043, acc: 60.94%] [G loss: 1.853930]\n",
      "epoch:20 step:19402 [D loss: 0.675646, acc: 60.16%] [G loss: 1.785081]\n",
      "epoch:20 step:19403 [D loss: 0.619594, acc: 64.06%] [G loss: 1.899063]\n",
      "epoch:20 step:19404 [D loss: 0.669243, acc: 61.72%] [G loss: 1.796319]\n",
      "epoch:20 step:19405 [D loss: 0.674630, acc: 59.38%] [G loss: 1.742083]\n",
      "epoch:20 step:19406 [D loss: 0.634904, acc: 61.72%] [G loss: 1.855054]\n",
      "epoch:20 step:19407 [D loss: 0.683502, acc: 59.38%] [G loss: 1.681480]\n",
      "epoch:20 step:19408 [D loss: 0.685079, acc: 56.25%] [G loss: 1.814507]\n",
      "epoch:20 step:19409 [D loss: 0.637718, acc: 64.06%] [G loss: 1.807710]\n",
      "epoch:20 step:19410 [D loss: 0.619154, acc: 64.84%] [G loss: 1.861659]\n",
      "epoch:20 step:19411 [D loss: 0.673994, acc: 58.59%] [G loss: 1.823374]\n",
      "epoch:20 step:19412 [D loss: 0.639199, acc: 64.84%] [G loss: 1.893817]\n",
      "epoch:20 step:19413 [D loss: 0.638586, acc: 64.06%] [G loss: 1.752059]\n",
      "epoch:20 step:19414 [D loss: 0.644949, acc: 63.28%] [G loss: 1.861610]\n",
      "epoch:20 step:19415 [D loss: 0.668071, acc: 58.59%] [G loss: 1.756339]\n",
      "epoch:20 step:19416 [D loss: 0.663878, acc: 59.38%] [G loss: 1.822102]\n",
      "epoch:20 step:19417 [D loss: 0.662477, acc: 61.72%] [G loss: 1.945095]\n",
      "epoch:20 step:19418 [D loss: 0.640586, acc: 65.62%] [G loss: 1.870347]\n",
      "epoch:20 step:19419 [D loss: 0.669538, acc: 60.16%] [G loss: 1.962508]\n",
      "epoch:20 step:19420 [D loss: 0.629358, acc: 70.31%] [G loss: 1.846666]\n",
      "epoch:20 step:19421 [D loss: 0.598990, acc: 70.31%] [G loss: 1.968677]\n",
      "epoch:20 step:19422 [D loss: 0.678982, acc: 57.03%] [G loss: 1.791704]\n",
      "epoch:20 step:19423 [D loss: 0.669254, acc: 60.16%] [G loss: 1.947119]\n",
      "epoch:20 step:19424 [D loss: 0.644549, acc: 60.16%] [G loss: 1.835831]\n",
      "epoch:20 step:19425 [D loss: 0.599688, acc: 73.44%] [G loss: 1.920418]\n",
      "epoch:20 step:19426 [D loss: 0.644424, acc: 63.28%] [G loss: 1.996976]\n",
      "epoch:20 step:19427 [D loss: 0.672678, acc: 57.03%] [G loss: 1.799568]\n",
      "epoch:20 step:19428 [D loss: 0.649092, acc: 60.94%] [G loss: 1.860612]\n",
      "epoch:20 step:19429 [D loss: 0.640905, acc: 64.84%] [G loss: 2.053341]\n",
      "epoch:20 step:19430 [D loss: 0.559315, acc: 71.09%] [G loss: 2.079237]\n",
      "epoch:20 step:19431 [D loss: 0.646560, acc: 64.06%] [G loss: 1.839449]\n",
      "epoch:20 step:19432 [D loss: 0.642932, acc: 62.50%] [G loss: 1.946514]\n",
      "epoch:20 step:19433 [D loss: 0.610595, acc: 67.19%] [G loss: 2.088506]\n",
      "epoch:20 step:19434 [D loss: 0.639539, acc: 62.50%] [G loss: 1.993820]\n",
      "epoch:20 step:19435 [D loss: 0.626266, acc: 60.16%] [G loss: 2.007775]\n",
      "epoch:20 step:19436 [D loss: 0.733462, acc: 54.69%] [G loss: 1.929105]\n",
      "epoch:20 step:19437 [D loss: 0.669019, acc: 57.81%] [G loss: 1.928674]\n",
      "epoch:20 step:19438 [D loss: 0.666434, acc: 61.72%] [G loss: 1.776773]\n",
      "epoch:20 step:19439 [D loss: 0.581067, acc: 67.19%] [G loss: 1.937092]\n",
      "epoch:20 step:19440 [D loss: 0.635879, acc: 62.50%] [G loss: 1.879694]\n",
      "epoch:20 step:19441 [D loss: 0.657369, acc: 60.16%] [G loss: 2.010478]\n",
      "epoch:20 step:19442 [D loss: 0.711473, acc: 57.81%] [G loss: 1.828592]\n",
      "epoch:20 step:19443 [D loss: 0.710774, acc: 56.25%] [G loss: 1.796621]\n",
      "epoch:20 step:19444 [D loss: 0.676757, acc: 59.38%] [G loss: 1.764592]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:20 step:19445 [D loss: 0.657834, acc: 63.28%] [G loss: 1.791614]\n",
      "epoch:20 step:19446 [D loss: 0.614087, acc: 67.19%] [G loss: 1.849945]\n",
      "epoch:20 step:19447 [D loss: 0.683040, acc: 57.03%] [G loss: 1.872573]\n",
      "epoch:20 step:19448 [D loss: 0.629315, acc: 67.19%] [G loss: 1.979980]\n",
      "epoch:20 step:19449 [D loss: 0.602852, acc: 67.19%] [G loss: 1.942231]\n",
      "epoch:20 step:19450 [D loss: 0.613253, acc: 70.31%] [G loss: 1.919972]\n",
      "epoch:20 step:19451 [D loss: 0.634323, acc: 68.75%] [G loss: 2.075475]\n",
      "epoch:20 step:19452 [D loss: 0.627788, acc: 60.94%] [G loss: 2.050273]\n",
      "epoch:20 step:19453 [D loss: 0.661228, acc: 65.62%] [G loss: 1.846394]\n",
      "epoch:20 step:19454 [D loss: 0.664310, acc: 58.59%] [G loss: 1.905315]\n",
      "epoch:20 step:19455 [D loss: 0.657826, acc: 57.03%] [G loss: 1.927888]\n",
      "epoch:20 step:19456 [D loss: 0.667480, acc: 57.81%] [G loss: 1.911245]\n",
      "epoch:20 step:19457 [D loss: 0.630105, acc: 63.28%] [G loss: 1.918676]\n",
      "epoch:20 step:19458 [D loss: 0.645661, acc: 64.84%] [G loss: 1.874134]\n",
      "epoch:20 step:19459 [D loss: 0.598583, acc: 70.31%] [G loss: 1.880177]\n",
      "epoch:20 step:19460 [D loss: 0.601247, acc: 67.97%] [G loss: 1.961939]\n",
      "epoch:20 step:19461 [D loss: 0.663652, acc: 59.38%] [G loss: 2.080427]\n",
      "epoch:20 step:19462 [D loss: 0.624263, acc: 65.62%] [G loss: 1.941625]\n",
      "epoch:20 step:19463 [D loss: 0.674536, acc: 59.38%] [G loss: 1.862843]\n",
      "epoch:20 step:19464 [D loss: 0.653059, acc: 59.38%] [G loss: 2.008677]\n",
      "epoch:20 step:19465 [D loss: 0.641212, acc: 66.41%] [G loss: 1.870172]\n",
      "epoch:20 step:19466 [D loss: 0.618192, acc: 64.84%] [G loss: 1.930520]\n",
      "epoch:20 step:19467 [D loss: 0.675421, acc: 61.72%] [G loss: 1.722481]\n",
      "epoch:20 step:19468 [D loss: 0.710616, acc: 56.25%] [G loss: 1.934675]\n",
      "epoch:20 step:19469 [D loss: 0.609744, acc: 68.75%] [G loss: 1.737428]\n",
      "epoch:20 step:19470 [D loss: 0.655591, acc: 53.12%] [G loss: 1.785442]\n",
      "epoch:20 step:19471 [D loss: 0.672831, acc: 59.38%] [G loss: 1.783532]\n",
      "epoch:20 step:19472 [D loss: 0.689384, acc: 54.69%] [G loss: 1.670369]\n",
      "epoch:20 step:19473 [D loss: 0.599652, acc: 71.88%] [G loss: 1.897680]\n",
      "epoch:20 step:19474 [D loss: 0.661279, acc: 61.72%] [G loss: 1.926229]\n",
      "epoch:20 step:19475 [D loss: 0.575598, acc: 72.66%] [G loss: 1.911948]\n",
      "epoch:20 step:19476 [D loss: 0.656144, acc: 60.94%] [G loss: 1.870051]\n",
      "epoch:20 step:19477 [D loss: 0.688950, acc: 60.16%] [G loss: 1.796268]\n",
      "epoch:20 step:19478 [D loss: 0.667084, acc: 54.69%] [G loss: 1.743745]\n",
      "epoch:20 step:19479 [D loss: 0.685167, acc: 53.91%] [G loss: 1.749201]\n",
      "epoch:20 step:19480 [D loss: 0.671443, acc: 56.25%] [G loss: 1.678601]\n",
      "epoch:20 step:19481 [D loss: 0.675420, acc: 58.59%] [G loss: 1.804109]\n",
      "epoch:20 step:19482 [D loss: 0.631947, acc: 67.19%] [G loss: 1.919322]\n",
      "epoch:20 step:19483 [D loss: 0.648682, acc: 60.94%] [G loss: 1.847193]\n",
      "epoch:20 step:19484 [D loss: 0.626269, acc: 61.72%] [G loss: 1.876061]\n",
      "epoch:20 step:19485 [D loss: 0.671906, acc: 59.38%] [G loss: 1.889832]\n",
      "epoch:20 step:19486 [D loss: 0.646482, acc: 57.03%] [G loss: 1.938403]\n",
      "epoch:20 step:19487 [D loss: 0.611690, acc: 64.06%] [G loss: 1.906067]\n",
      "epoch:20 step:19488 [D loss: 0.651692, acc: 57.81%] [G loss: 1.838786]\n",
      "epoch:20 step:19489 [D loss: 0.662086, acc: 57.81%] [G loss: 1.905440]\n",
      "epoch:20 step:19490 [D loss: 0.619562, acc: 61.72%] [G loss: 1.853095]\n",
      "epoch:20 step:19491 [D loss: 0.678234, acc: 58.59%] [G loss: 1.790973]\n",
      "epoch:20 step:19492 [D loss: 0.682217, acc: 59.38%] [G loss: 1.735815]\n",
      "epoch:20 step:19493 [D loss: 0.635199, acc: 65.62%] [G loss: 1.794657]\n",
      "epoch:20 step:19494 [D loss: 0.599586, acc: 67.19%] [G loss: 1.922150]\n",
      "epoch:20 step:19495 [D loss: 0.639297, acc: 65.62%] [G loss: 1.841349]\n",
      "epoch:20 step:19496 [D loss: 0.656638, acc: 63.28%] [G loss: 1.960545]\n",
      "epoch:20 step:19497 [D loss: 0.639955, acc: 66.41%] [G loss: 1.835670]\n",
      "epoch:20 step:19498 [D loss: 0.634577, acc: 66.41%] [G loss: 1.732758]\n",
      "epoch:20 step:19499 [D loss: 0.718782, acc: 46.88%] [G loss: 1.814550]\n",
      "epoch:20 step:19500 [D loss: 0.668224, acc: 60.94%] [G loss: 1.786718]\n",
      "epoch:20 step:19501 [D loss: 0.721696, acc: 54.69%] [G loss: 1.780877]\n",
      "epoch:20 step:19502 [D loss: 0.646234, acc: 61.72%] [G loss: 1.752662]\n",
      "epoch:20 step:19503 [D loss: 0.719141, acc: 50.00%] [G loss: 1.752494]\n",
      "epoch:20 step:19504 [D loss: 0.663125, acc: 59.38%] [G loss: 1.895815]\n",
      "epoch:20 step:19505 [D loss: 0.736118, acc: 49.22%] [G loss: 1.765472]\n",
      "epoch:20 step:19506 [D loss: 0.711162, acc: 53.91%] [G loss: 1.721951]\n",
      "epoch:20 step:19507 [D loss: 0.682400, acc: 59.38%] [G loss: 1.784984]\n",
      "epoch:20 step:19508 [D loss: 0.667137, acc: 58.59%] [G loss: 1.805400]\n",
      "epoch:20 step:19509 [D loss: 0.647760, acc: 58.59%] [G loss: 1.875579]\n",
      "epoch:20 step:19510 [D loss: 0.629940, acc: 62.50%] [G loss: 1.795304]\n",
      "epoch:20 step:19511 [D loss: 0.651173, acc: 67.19%] [G loss: 1.835652]\n",
      "epoch:20 step:19512 [D loss: 0.661047, acc: 58.59%] [G loss: 1.793001]\n",
      "epoch:20 step:19513 [D loss: 0.598880, acc: 71.09%] [G loss: 1.867943]\n",
      "epoch:20 step:19514 [D loss: 0.642258, acc: 61.72%] [G loss: 1.933704]\n",
      "epoch:20 step:19515 [D loss: 0.586296, acc: 67.97%] [G loss: 1.947307]\n",
      "epoch:20 step:19516 [D loss: 0.652921, acc: 59.38%] [G loss: 1.981122]\n",
      "epoch:20 step:19517 [D loss: 0.647001, acc: 64.84%] [G loss: 1.860802]\n",
      "epoch:20 step:19518 [D loss: 0.651258, acc: 60.16%] [G loss: 1.921454]\n",
      "epoch:20 step:19519 [D loss: 0.644623, acc: 61.72%] [G loss: 1.795815]\n",
      "epoch:20 step:19520 [D loss: 0.596846, acc: 69.53%] [G loss: 1.947431]\n",
      "epoch:20 step:19521 [D loss: 0.607635, acc: 67.97%] [G loss: 2.141171]\n",
      "epoch:20 step:19522 [D loss: 0.604946, acc: 64.84%] [G loss: 2.118334]\n",
      "epoch:20 step:19523 [D loss: 0.676072, acc: 61.72%] [G loss: 1.884122]\n",
      "epoch:20 step:19524 [D loss: 0.716269, acc: 53.91%] [G loss: 1.755205]\n",
      "epoch:20 step:19525 [D loss: 0.660577, acc: 60.94%] [G loss: 1.869474]\n",
      "epoch:20 step:19526 [D loss: 0.611837, acc: 65.62%] [G loss: 2.005731]\n",
      "epoch:20 step:19527 [D loss: 0.663850, acc: 64.06%] [G loss: 1.778737]\n",
      "epoch:20 step:19528 [D loss: 0.672485, acc: 57.03%] [G loss: 1.809209]\n",
      "epoch:20 step:19529 [D loss: 0.641215, acc: 70.31%] [G loss: 1.820234]\n",
      "epoch:20 step:19530 [D loss: 0.646012, acc: 60.16%] [G loss: 1.967540]\n",
      "epoch:20 step:19531 [D loss: 0.658838, acc: 58.59%] [G loss: 1.847646]\n",
      "epoch:20 step:19532 [D loss: 0.618454, acc: 65.62%] [G loss: 2.081949]\n",
      "epoch:20 step:19533 [D loss: 0.627166, acc: 63.28%] [G loss: 1.849210]\n",
      "epoch:20 step:19534 [D loss: 0.693145, acc: 54.69%] [G loss: 1.747272]\n",
      "epoch:20 step:19535 [D loss: 0.644780, acc: 57.03%] [G loss: 1.853094]\n",
      "epoch:20 step:19536 [D loss: 0.673110, acc: 59.38%] [G loss: 1.805960]\n",
      "epoch:20 step:19537 [D loss: 0.660133, acc: 60.16%] [G loss: 1.735111]\n",
      "epoch:20 step:19538 [D loss: 0.654770, acc: 60.16%] [G loss: 1.871289]\n",
      "epoch:20 step:19539 [D loss: 0.676496, acc: 52.34%] [G loss: 1.718510]\n",
      "epoch:20 step:19540 [D loss: 0.670378, acc: 60.16%] [G loss: 1.818042]\n",
      "epoch:20 step:19541 [D loss: 0.684846, acc: 58.59%] [G loss: 1.827250]\n",
      "epoch:20 step:19542 [D loss: 0.672080, acc: 57.81%] [G loss: 1.930950]\n",
      "epoch:20 step:19543 [D loss: 0.612415, acc: 68.75%] [G loss: 1.905567]\n",
      "epoch:20 step:19544 [D loss: 0.632444, acc: 61.72%] [G loss: 2.097850]\n",
      "epoch:20 step:19545 [D loss: 0.629923, acc: 65.62%] [G loss: 2.026753]\n",
      "epoch:20 step:19546 [D loss: 0.604264, acc: 67.97%] [G loss: 2.012417]\n",
      "epoch:20 step:19547 [D loss: 0.629373, acc: 60.94%] [G loss: 1.944548]\n",
      "epoch:20 step:19548 [D loss: 0.616314, acc: 67.19%] [G loss: 1.986638]\n",
      "epoch:20 step:19549 [D loss: 0.642664, acc: 67.19%] [G loss: 1.891432]\n",
      "epoch:20 step:19550 [D loss: 0.620326, acc: 64.06%] [G loss: 1.850559]\n",
      "epoch:20 step:19551 [D loss: 0.621222, acc: 64.84%] [G loss: 1.893367]\n",
      "epoch:20 step:19552 [D loss: 0.652334, acc: 57.81%] [G loss: 1.818981]\n",
      "epoch:20 step:19553 [D loss: 0.674738, acc: 57.81%] [G loss: 1.915341]\n",
      "epoch:20 step:19554 [D loss: 0.653823, acc: 60.94%] [G loss: 1.988012]\n",
      "epoch:20 step:19555 [D loss: 0.616468, acc: 70.31%] [G loss: 2.166446]\n",
      "epoch:20 step:19556 [D loss: 0.630464, acc: 65.62%] [G loss: 2.065409]\n",
      "epoch:20 step:19557 [D loss: 0.697162, acc: 58.59%] [G loss: 1.745456]\n",
      "epoch:20 step:19558 [D loss: 0.700075, acc: 51.56%] [G loss: 1.782279]\n",
      "epoch:20 step:19559 [D loss: 0.617819, acc: 67.97%] [G loss: 1.972081]\n",
      "epoch:20 step:19560 [D loss: 0.685827, acc: 57.03%] [G loss: 1.769086]\n",
      "epoch:20 step:19561 [D loss: 0.649820, acc: 62.50%] [G loss: 1.785927]\n",
      "epoch:20 step:19562 [D loss: 0.683447, acc: 60.16%] [G loss: 1.959069]\n",
      "epoch:20 step:19563 [D loss: 0.607688, acc: 67.19%] [G loss: 2.013776]\n",
      "epoch:20 step:19564 [D loss: 0.673069, acc: 60.94%] [G loss: 1.692045]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:20 step:19565 [D loss: 0.594143, acc: 66.41%] [G loss: 1.927913]\n",
      "epoch:20 step:19566 [D loss: 0.667531, acc: 58.59%] [G loss: 1.799913]\n",
      "epoch:20 step:19567 [D loss: 0.696138, acc: 57.03%] [G loss: 1.695389]\n",
      "epoch:20 step:19568 [D loss: 0.700753, acc: 57.03%] [G loss: 1.653047]\n",
      "epoch:20 step:19569 [D loss: 0.726287, acc: 50.00%] [G loss: 1.777650]\n",
      "epoch:20 step:19570 [D loss: 0.663273, acc: 59.38%] [G loss: 1.749263]\n",
      "epoch:20 step:19571 [D loss: 0.631167, acc: 71.09%] [G loss: 1.845562]\n",
      "epoch:20 step:19572 [D loss: 0.632633, acc: 60.94%] [G loss: 1.835503]\n",
      "epoch:20 step:19573 [D loss: 0.621539, acc: 65.62%] [G loss: 1.918342]\n",
      "epoch:20 step:19574 [D loss: 0.649990, acc: 60.16%] [G loss: 1.845616]\n",
      "epoch:20 step:19575 [D loss: 0.639053, acc: 64.84%] [G loss: 1.884724]\n",
      "epoch:20 step:19576 [D loss: 0.694147, acc: 57.81%] [G loss: 1.891920]\n",
      "epoch:20 step:19577 [D loss: 0.623794, acc: 64.06%] [G loss: 1.769533]\n",
      "epoch:20 step:19578 [D loss: 0.635418, acc: 64.06%] [G loss: 1.876475]\n",
      "epoch:20 step:19579 [D loss: 0.669794, acc: 58.59%] [G loss: 1.787960]\n",
      "epoch:20 step:19580 [D loss: 0.678403, acc: 58.59%] [G loss: 1.795228]\n",
      "epoch:20 step:19581 [D loss: 0.608213, acc: 71.88%] [G loss: 1.757482]\n",
      "epoch:20 step:19582 [D loss: 0.645039, acc: 66.41%] [G loss: 1.870796]\n",
      "epoch:20 step:19583 [D loss: 0.653512, acc: 60.94%] [G loss: 1.802242]\n",
      "epoch:20 step:19584 [D loss: 0.632359, acc: 63.28%] [G loss: 1.944392]\n",
      "epoch:20 step:19585 [D loss: 0.628891, acc: 71.09%] [G loss: 2.058950]\n",
      "epoch:20 step:19586 [D loss: 0.602424, acc: 63.28%] [G loss: 2.010854]\n",
      "epoch:20 step:19587 [D loss: 0.593310, acc: 68.75%] [G loss: 1.992877]\n",
      "epoch:20 step:19588 [D loss: 0.592695, acc: 71.09%] [G loss: 2.000868]\n",
      "epoch:20 step:19589 [D loss: 0.575218, acc: 71.88%] [G loss: 2.092109]\n",
      "epoch:20 step:19590 [D loss: 0.632376, acc: 64.06%] [G loss: 1.920319]\n",
      "epoch:20 step:19591 [D loss: 0.607714, acc: 65.62%] [G loss: 1.952805]\n",
      "epoch:20 step:19592 [D loss: 0.684794, acc: 55.47%] [G loss: 1.822019]\n",
      "epoch:20 step:19593 [D loss: 0.621002, acc: 64.06%] [G loss: 1.813565]\n",
      "epoch:20 step:19594 [D loss: 0.668557, acc: 64.06%] [G loss: 1.815493]\n",
      "epoch:20 step:19595 [D loss: 0.659132, acc: 60.94%] [G loss: 1.841243]\n",
      "epoch:20 step:19596 [D loss: 0.638865, acc: 64.84%] [G loss: 1.921689]\n",
      "epoch:20 step:19597 [D loss: 0.646850, acc: 63.28%] [G loss: 1.807768]\n",
      "epoch:20 step:19598 [D loss: 0.737484, acc: 54.69%] [G loss: 1.878590]\n",
      "epoch:20 step:19599 [D loss: 0.693276, acc: 57.81%] [G loss: 1.820275]\n",
      "epoch:20 step:19600 [D loss: 0.636036, acc: 66.41%] [G loss: 1.776641]\n",
      "##############\n",
      "[2.46519352 1.36470348 6.20279418 4.67399279 3.63222658 5.63913637\n",
      " 4.32331063 4.71736152 4.50374578 3.59042873]\n",
      "##########\n",
      "epoch:20 step:19601 [D loss: 0.656923, acc: 61.72%] [G loss: 1.664253]\n",
      "epoch:20 step:19602 [D loss: 0.634899, acc: 70.31%] [G loss: 1.693720]\n",
      "epoch:20 step:19603 [D loss: 0.665228, acc: 58.59%] [G loss: 1.761755]\n",
      "epoch:20 step:19604 [D loss: 0.687546, acc: 53.12%] [G loss: 1.750155]\n",
      "epoch:20 step:19605 [D loss: 0.623095, acc: 63.28%] [G loss: 1.829427]\n",
      "epoch:20 step:19606 [D loss: 0.676714, acc: 60.16%] [G loss: 1.737896]\n",
      "epoch:20 step:19607 [D loss: 0.659823, acc: 53.91%] [G loss: 1.706931]\n",
      "epoch:20 step:19608 [D loss: 0.667529, acc: 62.50%] [G loss: 1.899678]\n",
      "epoch:20 step:19609 [D loss: 0.583868, acc: 71.88%] [G loss: 1.797598]\n",
      "epoch:20 step:19610 [D loss: 0.690960, acc: 52.34%] [G loss: 1.949054]\n",
      "epoch:20 step:19611 [D loss: 0.609146, acc: 70.31%] [G loss: 1.806769]\n",
      "epoch:20 step:19612 [D loss: 0.683356, acc: 57.81%] [G loss: 1.810429]\n",
      "epoch:20 step:19613 [D loss: 0.656969, acc: 60.94%] [G loss: 1.845602]\n",
      "epoch:20 step:19614 [D loss: 0.660020, acc: 60.94%] [G loss: 1.914520]\n",
      "epoch:20 step:19615 [D loss: 0.647781, acc: 68.75%] [G loss: 1.985050]\n",
      "epoch:20 step:19616 [D loss: 0.640859, acc: 63.28%] [G loss: 1.954114]\n",
      "epoch:20 step:19617 [D loss: 0.632472, acc: 66.41%] [G loss: 1.853618]\n",
      "epoch:20 step:19618 [D loss: 0.658258, acc: 59.38%] [G loss: 1.904693]\n",
      "epoch:20 step:19619 [D loss: 0.603057, acc: 66.41%] [G loss: 1.745972]\n",
      "epoch:20 step:19620 [D loss: 0.657992, acc: 58.59%] [G loss: 1.869218]\n",
      "epoch:20 step:19621 [D loss: 0.650757, acc: 63.28%] [G loss: 1.854424]\n",
      "epoch:20 step:19622 [D loss: 0.633191, acc: 62.50%] [G loss: 1.881621]\n",
      "epoch:20 step:19623 [D loss: 0.644983, acc: 67.19%] [G loss: 1.761661]\n",
      "epoch:20 step:19624 [D loss: 0.583850, acc: 69.53%] [G loss: 1.962811]\n",
      "epoch:20 step:19625 [D loss: 0.636984, acc: 63.28%] [G loss: 1.901705]\n",
      "epoch:20 step:19626 [D loss: 0.605404, acc: 65.62%] [G loss: 1.948668]\n",
      "epoch:20 step:19627 [D loss: 0.632455, acc: 64.06%] [G loss: 1.975230]\n",
      "epoch:20 step:19628 [D loss: 0.658588, acc: 64.84%] [G loss: 1.819012]\n",
      "epoch:20 step:19629 [D loss: 0.607489, acc: 66.41%] [G loss: 1.791059]\n",
      "epoch:20 step:19630 [D loss: 0.669242, acc: 57.03%] [G loss: 1.994013]\n",
      "epoch:20 step:19631 [D loss: 0.683057, acc: 59.38%] [G loss: 1.693562]\n",
      "epoch:20 step:19632 [D loss: 0.708713, acc: 57.81%] [G loss: 1.884127]\n",
      "epoch:20 step:19633 [D loss: 0.668909, acc: 59.38%] [G loss: 1.851107]\n",
      "epoch:20 step:19634 [D loss: 0.622191, acc: 60.94%] [G loss: 1.910830]\n",
      "epoch:20 step:19635 [D loss: 0.681759, acc: 59.38%] [G loss: 1.734926]\n",
      "epoch:20 step:19636 [D loss: 0.621236, acc: 63.28%] [G loss: 1.774921]\n",
      "epoch:20 step:19637 [D loss: 0.675391, acc: 60.94%] [G loss: 1.896399]\n",
      "epoch:20 step:19638 [D loss: 0.597469, acc: 67.19%] [G loss: 1.873222]\n",
      "epoch:20 step:19639 [D loss: 0.652882, acc: 59.38%] [G loss: 2.108123]\n",
      "epoch:20 step:19640 [D loss: 0.603880, acc: 64.84%] [G loss: 1.918462]\n",
      "epoch:20 step:19641 [D loss: 0.611253, acc: 68.75%] [G loss: 1.997679]\n",
      "epoch:20 step:19642 [D loss: 0.666129, acc: 60.94%] [G loss: 1.894382]\n",
      "epoch:20 step:19643 [D loss: 0.640645, acc: 62.50%] [G loss: 1.930530]\n",
      "epoch:20 step:19644 [D loss: 0.607723, acc: 66.41%] [G loss: 2.061787]\n",
      "epoch:20 step:19645 [D loss: 0.679042, acc: 58.59%] [G loss: 1.987957]\n",
      "epoch:20 step:19646 [D loss: 0.598339, acc: 67.97%] [G loss: 2.200872]\n",
      "epoch:20 step:19647 [D loss: 0.624549, acc: 68.75%] [G loss: 1.911010]\n",
      "epoch:20 step:19648 [D loss: 0.699309, acc: 61.72%] [G loss: 1.854090]\n",
      "epoch:20 step:19649 [D loss: 0.572292, acc: 71.09%] [G loss: 1.999247]\n",
      "epoch:20 step:19650 [D loss: 0.615970, acc: 62.50%] [G loss: 2.013504]\n",
      "epoch:20 step:19651 [D loss: 0.647138, acc: 69.53%] [G loss: 1.987412]\n",
      "epoch:20 step:19652 [D loss: 0.600310, acc: 69.53%] [G loss: 2.149139]\n",
      "epoch:20 step:19653 [D loss: 0.677307, acc: 56.25%] [G loss: 1.898679]\n",
      "epoch:20 step:19654 [D loss: 0.704736, acc: 55.47%] [G loss: 1.832009]\n",
      "epoch:20 step:19655 [D loss: 0.666080, acc: 61.72%] [G loss: 1.833065]\n",
      "epoch:20 step:19656 [D loss: 0.638725, acc: 60.94%] [G loss: 2.057093]\n",
      "epoch:20 step:19657 [D loss: 0.676910, acc: 57.81%] [G loss: 2.097445]\n",
      "epoch:20 step:19658 [D loss: 0.572578, acc: 67.19%] [G loss: 2.043810]\n",
      "epoch:20 step:19659 [D loss: 0.574873, acc: 68.75%] [G loss: 2.161237]\n",
      "epoch:20 step:19660 [D loss: 0.713715, acc: 57.81%] [G loss: 1.786638]\n",
      "epoch:20 step:19661 [D loss: 0.631407, acc: 63.28%] [G loss: 1.945638]\n",
      "epoch:20 step:19662 [D loss: 0.586525, acc: 70.31%] [G loss: 1.921773]\n",
      "epoch:20 step:19663 [D loss: 0.540312, acc: 75.78%] [G loss: 2.127648]\n",
      "epoch:20 step:19664 [D loss: 0.618206, acc: 64.84%] [G loss: 2.256099]\n",
      "epoch:20 step:19665 [D loss: 0.639877, acc: 64.84%] [G loss: 2.121904]\n",
      "epoch:20 step:19666 [D loss: 0.594248, acc: 67.97%] [G loss: 2.236502]\n",
      "epoch:20 step:19667 [D loss: 0.623996, acc: 63.28%] [G loss: 2.099286]\n",
      "epoch:20 step:19668 [D loss: 0.773540, acc: 47.66%] [G loss: 1.851871]\n",
      "epoch:20 step:19669 [D loss: 0.757949, acc: 47.66%] [G loss: 1.906781]\n",
      "epoch:20 step:19670 [D loss: 0.632633, acc: 61.72%] [G loss: 1.982741]\n",
      "epoch:20 step:19671 [D loss: 0.661060, acc: 66.41%] [G loss: 1.982350]\n",
      "epoch:20 step:19672 [D loss: 0.662925, acc: 58.59%] [G loss: 1.955891]\n",
      "epoch:20 step:19673 [D loss: 0.658959, acc: 65.62%] [G loss: 1.864008]\n",
      "epoch:20 step:19674 [D loss: 0.623008, acc: 68.75%] [G loss: 1.998412]\n",
      "epoch:20 step:19675 [D loss: 0.607827, acc: 66.41%] [G loss: 1.937160]\n",
      "epoch:20 step:19676 [D loss: 0.564377, acc: 79.69%] [G loss: 2.124370]\n",
      "epoch:20 step:19677 [D loss: 0.569829, acc: 70.31%] [G loss: 2.534809]\n",
      "epoch:21 step:19678 [D loss: 0.656377, acc: 64.06%] [G loss: 1.952360]\n",
      "epoch:21 step:19679 [D loss: 0.675367, acc: 57.81%] [G loss: 2.027183]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:21 step:19680 [D loss: 0.625303, acc: 68.75%] [G loss: 1.866477]\n",
      "epoch:21 step:19681 [D loss: 0.626716, acc: 64.84%] [G loss: 1.857325]\n",
      "epoch:21 step:19682 [D loss: 0.735705, acc: 54.69%] [G loss: 1.879924]\n",
      "epoch:21 step:19683 [D loss: 0.586959, acc: 73.44%] [G loss: 2.045804]\n",
      "epoch:21 step:19684 [D loss: 0.652874, acc: 62.50%] [G loss: 1.953191]\n",
      "epoch:21 step:19685 [D loss: 0.628430, acc: 65.62%] [G loss: 1.908072]\n",
      "epoch:21 step:19686 [D loss: 0.686318, acc: 61.72%] [G loss: 2.073520]\n",
      "epoch:21 step:19687 [D loss: 0.570025, acc: 69.53%] [G loss: 2.046648]\n",
      "epoch:21 step:19688 [D loss: 0.607250, acc: 65.62%] [G loss: 1.864863]\n",
      "epoch:21 step:19689 [D loss: 0.645099, acc: 60.94%] [G loss: 1.961503]\n",
      "epoch:21 step:19690 [D loss: 0.617985, acc: 67.19%] [G loss: 2.039444]\n",
      "epoch:21 step:19691 [D loss: 0.576531, acc: 71.88%] [G loss: 2.040684]\n",
      "epoch:21 step:19692 [D loss: 0.599090, acc: 67.19%] [G loss: 2.285095]\n",
      "epoch:21 step:19693 [D loss: 0.580215, acc: 67.97%] [G loss: 2.196918]\n",
      "epoch:21 step:19694 [D loss: 0.679614, acc: 60.16%] [G loss: 1.928963]\n",
      "epoch:21 step:19695 [D loss: 0.632391, acc: 64.06%] [G loss: 1.866237]\n",
      "epoch:21 step:19696 [D loss: 0.627798, acc: 62.50%] [G loss: 2.023168]\n",
      "epoch:21 step:19697 [D loss: 0.698997, acc: 53.91%] [G loss: 1.759377]\n",
      "epoch:21 step:19698 [D loss: 0.658645, acc: 60.94%] [G loss: 1.881417]\n",
      "epoch:21 step:19699 [D loss: 0.664021, acc: 58.59%] [G loss: 1.770795]\n",
      "epoch:21 step:19700 [D loss: 0.594261, acc: 75.78%] [G loss: 1.951418]\n",
      "epoch:21 step:19701 [D loss: 0.597506, acc: 68.75%] [G loss: 2.098084]\n",
      "epoch:21 step:19702 [D loss: 0.587464, acc: 67.97%] [G loss: 2.035904]\n",
      "epoch:21 step:19703 [D loss: 0.629212, acc: 64.84%] [G loss: 1.847647]\n",
      "epoch:21 step:19704 [D loss: 0.643164, acc: 64.84%] [G loss: 1.969555]\n",
      "epoch:21 step:19705 [D loss: 0.681547, acc: 60.94%] [G loss: 2.012240]\n",
      "epoch:21 step:19706 [D loss: 0.605853, acc: 64.84%] [G loss: 1.980543]\n",
      "epoch:21 step:19707 [D loss: 0.636081, acc: 66.41%] [G loss: 1.984555]\n",
      "epoch:21 step:19708 [D loss: 0.694205, acc: 57.81%] [G loss: 1.791718]\n",
      "epoch:21 step:19709 [D loss: 0.678989, acc: 57.03%] [G loss: 1.890539]\n",
      "epoch:21 step:19710 [D loss: 0.602408, acc: 70.31%] [G loss: 1.845186]\n",
      "epoch:21 step:19711 [D loss: 0.628084, acc: 63.28%] [G loss: 1.886888]\n",
      "epoch:21 step:19712 [D loss: 0.618394, acc: 63.28%] [G loss: 2.102463]\n",
      "epoch:21 step:19713 [D loss: 0.600853, acc: 68.75%] [G loss: 2.068023]\n",
      "epoch:21 step:19714 [D loss: 0.657596, acc: 62.50%] [G loss: 1.793756]\n",
      "epoch:21 step:19715 [D loss: 0.667810, acc: 60.94%] [G loss: 1.761918]\n",
      "epoch:21 step:19716 [D loss: 0.585599, acc: 67.97%] [G loss: 1.904077]\n",
      "epoch:21 step:19717 [D loss: 0.586236, acc: 69.53%] [G loss: 2.085486]\n",
      "epoch:21 step:19718 [D loss: 0.651710, acc: 57.03%] [G loss: 1.838700]\n",
      "epoch:21 step:19719 [D loss: 0.659518, acc: 60.16%] [G loss: 1.943752]\n",
      "epoch:21 step:19720 [D loss: 0.642196, acc: 62.50%] [G loss: 1.906606]\n",
      "epoch:21 step:19721 [D loss: 0.635123, acc: 57.81%] [G loss: 1.906367]\n",
      "epoch:21 step:19722 [D loss: 0.657592, acc: 66.41%] [G loss: 1.811114]\n",
      "epoch:21 step:19723 [D loss: 0.656369, acc: 59.38%] [G loss: 2.061307]\n",
      "epoch:21 step:19724 [D loss: 0.655067, acc: 60.16%] [G loss: 1.924419]\n",
      "epoch:21 step:19725 [D loss: 0.552492, acc: 78.12%] [G loss: 1.939595]\n",
      "epoch:21 step:19726 [D loss: 0.641092, acc: 60.94%] [G loss: 2.031848]\n",
      "epoch:21 step:19727 [D loss: 0.665280, acc: 64.06%] [G loss: 1.867077]\n",
      "epoch:21 step:19728 [D loss: 0.637410, acc: 64.06%] [G loss: 2.031939]\n",
      "epoch:21 step:19729 [D loss: 0.628776, acc: 62.50%] [G loss: 1.934379]\n",
      "epoch:21 step:19730 [D loss: 0.617440, acc: 64.84%] [G loss: 1.899761]\n",
      "epoch:21 step:19731 [D loss: 0.580508, acc: 73.44%] [G loss: 1.922302]\n",
      "epoch:21 step:19732 [D loss: 0.603025, acc: 69.53%] [G loss: 2.183492]\n",
      "epoch:21 step:19733 [D loss: 0.623137, acc: 64.06%] [G loss: 2.030916]\n",
      "epoch:21 step:19734 [D loss: 0.646867, acc: 59.38%] [G loss: 2.001663]\n",
      "epoch:21 step:19735 [D loss: 0.691449, acc: 59.38%] [G loss: 1.970523]\n",
      "epoch:21 step:19736 [D loss: 0.653220, acc: 60.94%] [G loss: 1.990397]\n",
      "epoch:21 step:19737 [D loss: 0.655928, acc: 62.50%] [G loss: 1.773145]\n",
      "epoch:21 step:19738 [D loss: 0.602808, acc: 65.62%] [G loss: 1.876295]\n",
      "epoch:21 step:19739 [D loss: 0.652140, acc: 60.94%] [G loss: 1.787584]\n",
      "epoch:21 step:19740 [D loss: 0.646591, acc: 61.72%] [G loss: 1.860451]\n",
      "epoch:21 step:19741 [D loss: 0.589709, acc: 64.84%] [G loss: 2.105430]\n",
      "epoch:21 step:19742 [D loss: 0.633729, acc: 64.84%] [G loss: 2.117794]\n",
      "epoch:21 step:19743 [D loss: 0.642429, acc: 67.19%] [G loss: 1.898530]\n",
      "epoch:21 step:19744 [D loss: 0.669243, acc: 60.16%] [G loss: 1.984949]\n",
      "epoch:21 step:19745 [D loss: 0.692808, acc: 60.16%] [G loss: 1.979927]\n",
      "epoch:21 step:19746 [D loss: 0.648355, acc: 68.75%] [G loss: 1.956495]\n",
      "epoch:21 step:19747 [D loss: 0.611756, acc: 65.62%] [G loss: 2.015723]\n",
      "epoch:21 step:19748 [D loss: 0.703844, acc: 60.16%] [G loss: 1.792439]\n",
      "epoch:21 step:19749 [D loss: 0.646409, acc: 67.19%] [G loss: 1.953324]\n",
      "epoch:21 step:19750 [D loss: 0.636827, acc: 64.84%] [G loss: 1.872589]\n",
      "epoch:21 step:19751 [D loss: 0.653543, acc: 62.50%] [G loss: 2.014735]\n",
      "epoch:21 step:19752 [D loss: 0.635638, acc: 64.84%] [G loss: 2.066279]\n",
      "epoch:21 step:19753 [D loss: 0.601675, acc: 70.31%] [G loss: 2.049992]\n",
      "epoch:21 step:19754 [D loss: 0.644271, acc: 61.72%] [G loss: 2.034334]\n",
      "epoch:21 step:19755 [D loss: 0.681912, acc: 56.25%] [G loss: 1.938234]\n",
      "epoch:21 step:19756 [D loss: 0.649763, acc: 60.16%] [G loss: 1.804797]\n",
      "epoch:21 step:19757 [D loss: 0.703581, acc: 53.12%] [G loss: 1.819731]\n",
      "epoch:21 step:19758 [D loss: 0.711614, acc: 55.47%] [G loss: 1.725220]\n",
      "epoch:21 step:19759 [D loss: 0.650756, acc: 60.16%] [G loss: 1.827088]\n",
      "epoch:21 step:19760 [D loss: 0.680653, acc: 55.47%] [G loss: 1.908050]\n",
      "epoch:21 step:19761 [D loss: 0.638649, acc: 64.84%] [G loss: 1.792252]\n",
      "epoch:21 step:19762 [D loss: 0.632055, acc: 62.50%] [G loss: 1.804099]\n",
      "epoch:21 step:19763 [D loss: 0.630322, acc: 67.97%] [G loss: 1.778096]\n",
      "epoch:21 step:19764 [D loss: 0.617816, acc: 65.62%] [G loss: 1.881188]\n",
      "epoch:21 step:19765 [D loss: 0.626424, acc: 65.62%] [G loss: 1.957166]\n",
      "epoch:21 step:19766 [D loss: 0.611950, acc: 68.75%] [G loss: 1.937712]\n",
      "epoch:21 step:19767 [D loss: 0.598134, acc: 67.97%] [G loss: 1.867865]\n",
      "epoch:21 step:19768 [D loss: 0.672802, acc: 61.72%] [G loss: 1.896854]\n",
      "epoch:21 step:19769 [D loss: 0.637739, acc: 68.75%] [G loss: 1.938749]\n",
      "epoch:21 step:19770 [D loss: 0.618486, acc: 67.19%] [G loss: 2.133264]\n",
      "epoch:21 step:19771 [D loss: 0.630308, acc: 61.72%] [G loss: 1.965509]\n",
      "epoch:21 step:19772 [D loss: 0.657561, acc: 61.72%] [G loss: 1.812972]\n",
      "epoch:21 step:19773 [D loss: 0.655316, acc: 63.28%] [G loss: 1.812388]\n",
      "epoch:21 step:19774 [D loss: 0.653017, acc: 61.72%] [G loss: 1.919934]\n",
      "epoch:21 step:19775 [D loss: 0.709960, acc: 51.56%] [G loss: 1.907198]\n",
      "epoch:21 step:19776 [D loss: 0.656051, acc: 58.59%] [G loss: 1.778673]\n",
      "epoch:21 step:19777 [D loss: 0.611532, acc: 67.97%] [G loss: 1.789601]\n",
      "epoch:21 step:19778 [D loss: 0.653870, acc: 61.72%] [G loss: 1.965636]\n",
      "epoch:21 step:19779 [D loss: 0.677407, acc: 58.59%] [G loss: 1.925657]\n",
      "epoch:21 step:19780 [D loss: 0.635669, acc: 60.94%] [G loss: 1.970797]\n",
      "epoch:21 step:19781 [D loss: 0.638366, acc: 62.50%] [G loss: 1.894810]\n",
      "epoch:21 step:19782 [D loss: 0.678880, acc: 55.47%] [G loss: 1.947699]\n",
      "epoch:21 step:19783 [D loss: 0.607930, acc: 64.06%] [G loss: 1.942061]\n",
      "epoch:21 step:19784 [D loss: 0.629292, acc: 64.84%] [G loss: 2.091507]\n",
      "epoch:21 step:19785 [D loss: 0.780463, acc: 51.56%] [G loss: 1.693664]\n",
      "epoch:21 step:19786 [D loss: 0.659932, acc: 63.28%] [G loss: 1.746043]\n",
      "epoch:21 step:19787 [D loss: 0.713984, acc: 54.69%] [G loss: 1.753089]\n",
      "epoch:21 step:19788 [D loss: 0.665796, acc: 63.28%] [G loss: 1.981558]\n",
      "epoch:21 step:19789 [D loss: 0.558780, acc: 75.78%] [G loss: 1.876159]\n",
      "epoch:21 step:19790 [D loss: 0.698111, acc: 57.81%] [G loss: 2.068593]\n",
      "epoch:21 step:19791 [D loss: 0.660182, acc: 66.41%] [G loss: 2.014355]\n",
      "epoch:21 step:19792 [D loss: 0.613169, acc: 67.97%] [G loss: 2.048887]\n",
      "epoch:21 step:19793 [D loss: 0.599240, acc: 72.66%] [G loss: 2.073403]\n",
      "epoch:21 step:19794 [D loss: 0.637730, acc: 61.72%] [G loss: 1.979014]\n",
      "epoch:21 step:19795 [D loss: 0.658330, acc: 60.94%] [G loss: 1.901836]\n",
      "epoch:21 step:19796 [D loss: 0.576668, acc: 70.31%] [G loss: 2.276906]\n",
      "epoch:21 step:19797 [D loss: 0.649016, acc: 63.28%] [G loss: 1.865621]\n",
      "epoch:21 step:19798 [D loss: 0.652134, acc: 67.97%] [G loss: 1.982029]\n",
      "epoch:21 step:19799 [D loss: 0.627655, acc: 62.50%] [G loss: 2.178907]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:21 step:19800 [D loss: 0.654894, acc: 62.50%] [G loss: 1.866255]\n",
      "##############\n",
      "[2.45438655 1.72231133 6.26876695 4.86095181 3.56975652 5.59031586\n",
      " 4.21894491 4.8048865  4.54658188 3.68899583]\n",
      "##########\n",
      "epoch:21 step:19801 [D loss: 0.680825, acc: 56.25%] [G loss: 1.849073]\n",
      "epoch:21 step:19802 [D loss: 0.729205, acc: 54.69%] [G loss: 1.723916]\n",
      "epoch:21 step:19803 [D loss: 0.640920, acc: 62.50%] [G loss: 1.854250]\n",
      "epoch:21 step:19804 [D loss: 0.630370, acc: 67.19%] [G loss: 1.849143]\n",
      "epoch:21 step:19805 [D loss: 0.679819, acc: 61.72%] [G loss: 1.713469]\n",
      "epoch:21 step:19806 [D loss: 0.637464, acc: 64.06%] [G loss: 1.808914]\n",
      "epoch:21 step:19807 [D loss: 0.666622, acc: 65.62%] [G loss: 1.808717]\n",
      "epoch:21 step:19808 [D loss: 0.625638, acc: 63.28%] [G loss: 1.863949]\n",
      "epoch:21 step:19809 [D loss: 0.653488, acc: 61.72%] [G loss: 1.742726]\n",
      "epoch:21 step:19810 [D loss: 0.650900, acc: 58.59%] [G loss: 1.816204]\n",
      "epoch:21 step:19811 [D loss: 0.666064, acc: 61.72%] [G loss: 1.774458]\n",
      "epoch:21 step:19812 [D loss: 0.670601, acc: 60.94%] [G loss: 1.791962]\n",
      "epoch:21 step:19813 [D loss: 0.651039, acc: 57.03%] [G loss: 1.776517]\n",
      "epoch:21 step:19814 [D loss: 0.678555, acc: 63.28%] [G loss: 1.674008]\n",
      "epoch:21 step:19815 [D loss: 0.640531, acc: 60.16%] [G loss: 1.795153]\n",
      "epoch:21 step:19816 [D loss: 0.645082, acc: 62.50%] [G loss: 1.920748]\n",
      "epoch:21 step:19817 [D loss: 0.715607, acc: 52.34%] [G loss: 1.898743]\n",
      "epoch:21 step:19818 [D loss: 0.669387, acc: 60.16%] [G loss: 1.858252]\n",
      "epoch:21 step:19819 [D loss: 0.666599, acc: 61.72%] [G loss: 1.799464]\n",
      "epoch:21 step:19820 [D loss: 0.687604, acc: 50.00%] [G loss: 1.754707]\n",
      "epoch:21 step:19821 [D loss: 0.665440, acc: 60.16%] [G loss: 1.876122]\n",
      "epoch:21 step:19822 [D loss: 0.637765, acc: 63.28%] [G loss: 1.847842]\n",
      "epoch:21 step:19823 [D loss: 0.641404, acc: 66.41%] [G loss: 1.868765]\n",
      "epoch:21 step:19824 [D loss: 0.691206, acc: 63.28%] [G loss: 1.866736]\n",
      "epoch:21 step:19825 [D loss: 0.683421, acc: 60.94%] [G loss: 1.709858]\n",
      "epoch:21 step:19826 [D loss: 0.623508, acc: 69.53%] [G loss: 1.851147]\n",
      "epoch:21 step:19827 [D loss: 0.608588, acc: 69.53%] [G loss: 1.890327]\n",
      "epoch:21 step:19828 [D loss: 0.658524, acc: 59.38%] [G loss: 1.897049]\n",
      "epoch:21 step:19829 [D loss: 0.645191, acc: 63.28%] [G loss: 1.816731]\n",
      "epoch:21 step:19830 [D loss: 0.664822, acc: 63.28%] [G loss: 1.826843]\n",
      "epoch:21 step:19831 [D loss: 0.638834, acc: 65.62%] [G loss: 1.909781]\n",
      "epoch:21 step:19832 [D loss: 0.639623, acc: 64.06%] [G loss: 1.912809]\n",
      "epoch:21 step:19833 [D loss: 0.685580, acc: 53.91%] [G loss: 1.842572]\n",
      "epoch:21 step:19834 [D loss: 0.708888, acc: 56.25%] [G loss: 1.783139]\n",
      "epoch:21 step:19835 [D loss: 0.667466, acc: 60.94%] [G loss: 1.816024]\n",
      "epoch:21 step:19836 [D loss: 0.619377, acc: 60.94%] [G loss: 1.861537]\n",
      "epoch:21 step:19837 [D loss: 0.684479, acc: 57.81%] [G loss: 1.753345]\n",
      "epoch:21 step:19838 [D loss: 0.631485, acc: 64.84%] [G loss: 1.892750]\n",
      "epoch:21 step:19839 [D loss: 0.638788, acc: 60.16%] [G loss: 1.843426]\n",
      "epoch:21 step:19840 [D loss: 0.628321, acc: 64.06%] [G loss: 1.892883]\n",
      "epoch:21 step:19841 [D loss: 0.610376, acc: 71.88%] [G loss: 1.958233]\n",
      "epoch:21 step:19842 [D loss: 0.647583, acc: 62.50%] [G loss: 1.801645]\n",
      "epoch:21 step:19843 [D loss: 0.626418, acc: 64.84%] [G loss: 1.866933]\n",
      "epoch:21 step:19844 [D loss: 0.600838, acc: 66.41%] [G loss: 1.912705]\n",
      "epoch:21 step:19845 [D loss: 0.652224, acc: 60.16%] [G loss: 1.889695]\n",
      "epoch:21 step:19846 [D loss: 0.636023, acc: 60.94%] [G loss: 1.881330]\n",
      "epoch:21 step:19847 [D loss: 0.643696, acc: 62.50%] [G loss: 1.818128]\n",
      "epoch:21 step:19848 [D loss: 0.657163, acc: 58.59%] [G loss: 1.800385]\n",
      "epoch:21 step:19849 [D loss: 0.675707, acc: 60.16%] [G loss: 1.827457]\n",
      "epoch:21 step:19850 [D loss: 0.644043, acc: 62.50%] [G loss: 1.706196]\n",
      "epoch:21 step:19851 [D loss: 0.637825, acc: 60.94%] [G loss: 1.771752]\n",
      "epoch:21 step:19852 [D loss: 0.641625, acc: 62.50%] [G loss: 1.833714]\n",
      "epoch:21 step:19853 [D loss: 0.675609, acc: 60.16%] [G loss: 1.769231]\n",
      "epoch:21 step:19854 [D loss: 0.645285, acc: 64.06%] [G loss: 1.839379]\n",
      "epoch:21 step:19855 [D loss: 0.650424, acc: 60.94%] [G loss: 1.718907]\n",
      "epoch:21 step:19856 [D loss: 0.640137, acc: 58.59%] [G loss: 1.820628]\n",
      "epoch:21 step:19857 [D loss: 0.608539, acc: 65.62%] [G loss: 1.798268]\n",
      "epoch:21 step:19858 [D loss: 0.667562, acc: 60.16%] [G loss: 1.707077]\n",
      "epoch:21 step:19859 [D loss: 0.686343, acc: 62.50%] [G loss: 1.808931]\n",
      "epoch:21 step:19860 [D loss: 0.642719, acc: 65.62%] [G loss: 1.771733]\n",
      "epoch:21 step:19861 [D loss: 0.621094, acc: 65.62%] [G loss: 1.937006]\n",
      "epoch:21 step:19862 [D loss: 0.634546, acc: 64.06%] [G loss: 1.903728]\n",
      "epoch:21 step:19863 [D loss: 0.653650, acc: 59.38%] [G loss: 1.961208]\n",
      "epoch:21 step:19864 [D loss: 0.645290, acc: 64.06%] [G loss: 1.986258]\n",
      "epoch:21 step:19865 [D loss: 0.649512, acc: 58.59%] [G loss: 1.924884]\n",
      "epoch:21 step:19866 [D loss: 0.663747, acc: 61.72%] [G loss: 1.698913]\n",
      "epoch:21 step:19867 [D loss: 0.640797, acc: 64.84%] [G loss: 1.888357]\n",
      "epoch:21 step:19868 [D loss: 0.637660, acc: 63.28%] [G loss: 1.933328]\n",
      "epoch:21 step:19869 [D loss: 0.661828, acc: 60.94%] [G loss: 1.849889]\n",
      "epoch:21 step:19870 [D loss: 0.636079, acc: 60.94%] [G loss: 1.898513]\n",
      "epoch:21 step:19871 [D loss: 0.593949, acc: 72.66%] [G loss: 1.958215]\n",
      "epoch:21 step:19872 [D loss: 0.656122, acc: 62.50%] [G loss: 1.794527]\n",
      "epoch:21 step:19873 [D loss: 0.625214, acc: 64.84%] [G loss: 1.959098]\n",
      "epoch:21 step:19874 [D loss: 0.647683, acc: 62.50%] [G loss: 2.018967]\n",
      "epoch:21 step:19875 [D loss: 0.593551, acc: 67.19%] [G loss: 1.905409]\n",
      "epoch:21 step:19876 [D loss: 0.642788, acc: 64.06%] [G loss: 1.840360]\n",
      "epoch:21 step:19877 [D loss: 0.708383, acc: 57.81%] [G loss: 1.809862]\n",
      "epoch:21 step:19878 [D loss: 0.604141, acc: 67.19%] [G loss: 1.853533]\n",
      "epoch:21 step:19879 [D loss: 0.652770, acc: 60.94%] [G loss: 1.909887]\n",
      "epoch:21 step:19880 [D loss: 0.597668, acc: 75.00%] [G loss: 1.831952]\n",
      "epoch:21 step:19881 [D loss: 0.639081, acc: 64.06%] [G loss: 1.888558]\n",
      "epoch:21 step:19882 [D loss: 0.684892, acc: 57.81%] [G loss: 1.915166]\n",
      "epoch:21 step:19883 [D loss: 0.638792, acc: 64.06%] [G loss: 1.890357]\n",
      "epoch:21 step:19884 [D loss: 0.679421, acc: 60.94%] [G loss: 2.058964]\n",
      "epoch:21 step:19885 [D loss: 0.669483, acc: 64.06%] [G loss: 2.109432]\n",
      "epoch:21 step:19886 [D loss: 0.574787, acc: 70.31%] [G loss: 2.022566]\n",
      "epoch:21 step:19887 [D loss: 0.716056, acc: 53.91%] [G loss: 1.888158]\n",
      "epoch:21 step:19888 [D loss: 0.679912, acc: 57.03%] [G loss: 1.705620]\n",
      "epoch:21 step:19889 [D loss: 0.656068, acc: 60.16%] [G loss: 1.765757]\n",
      "epoch:21 step:19890 [D loss: 0.666425, acc: 64.84%] [G loss: 1.649510]\n",
      "epoch:21 step:19891 [D loss: 0.700444, acc: 61.72%] [G loss: 1.768000]\n",
      "epoch:21 step:19892 [D loss: 0.672847, acc: 64.06%] [G loss: 1.735480]\n",
      "epoch:21 step:19893 [D loss: 0.692879, acc: 50.78%] [G loss: 1.888697]\n",
      "epoch:21 step:19894 [D loss: 0.592388, acc: 75.00%] [G loss: 1.904974]\n",
      "epoch:21 step:19895 [D loss: 0.582934, acc: 69.53%] [G loss: 2.050336]\n",
      "epoch:21 step:19896 [D loss: 0.584480, acc: 75.00%] [G loss: 1.999829]\n",
      "epoch:21 step:19897 [D loss: 0.663022, acc: 66.41%] [G loss: 1.882538]\n",
      "epoch:21 step:19898 [D loss: 0.704122, acc: 60.16%] [G loss: 2.006995]\n",
      "epoch:21 step:19899 [D loss: 0.647674, acc: 59.38%] [G loss: 1.855317]\n",
      "epoch:21 step:19900 [D loss: 0.625962, acc: 65.62%] [G loss: 1.834038]\n",
      "epoch:21 step:19901 [D loss: 0.680356, acc: 59.38%] [G loss: 1.794412]\n",
      "epoch:21 step:19902 [D loss: 0.646595, acc: 64.84%] [G loss: 1.946456]\n",
      "epoch:21 step:19903 [D loss: 0.637592, acc: 60.16%] [G loss: 1.695249]\n",
      "epoch:21 step:19904 [D loss: 0.662755, acc: 61.72%] [G loss: 1.755188]\n",
      "epoch:21 step:19905 [D loss: 0.678192, acc: 57.03%] [G loss: 1.824353]\n",
      "epoch:21 step:19906 [D loss: 0.567625, acc: 70.31%] [G loss: 2.148333]\n",
      "epoch:21 step:19907 [D loss: 0.634251, acc: 62.50%] [G loss: 2.249188]\n",
      "epoch:21 step:19908 [D loss: 0.610683, acc: 67.97%] [G loss: 2.209875]\n",
      "epoch:21 step:19909 [D loss: 0.552434, acc: 75.00%] [G loss: 2.336666]\n",
      "epoch:21 step:19910 [D loss: 0.717585, acc: 57.03%] [G loss: 1.950372]\n",
      "epoch:21 step:19911 [D loss: 0.661792, acc: 59.38%] [G loss: 1.768569]\n",
      "epoch:21 step:19912 [D loss: 0.667720, acc: 60.94%] [G loss: 1.824165]\n",
      "epoch:21 step:19913 [D loss: 0.658002, acc: 61.72%] [G loss: 1.890617]\n",
      "epoch:21 step:19914 [D loss: 0.651328, acc: 63.28%] [G loss: 1.791578]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:21 step:19915 [D loss: 0.664608, acc: 51.56%] [G loss: 1.808282]\n",
      "epoch:21 step:19916 [D loss: 0.635526, acc: 64.84%] [G loss: 1.724422]\n",
      "epoch:21 step:19917 [D loss: 0.654779, acc: 60.94%] [G loss: 1.861161]\n",
      "epoch:21 step:19918 [D loss: 0.623289, acc: 70.31%] [G loss: 2.042842]\n",
      "epoch:21 step:19919 [D loss: 0.659612, acc: 60.94%] [G loss: 1.870692]\n",
      "epoch:21 step:19920 [D loss: 0.648059, acc: 65.62%] [G loss: 1.785452]\n",
      "epoch:21 step:19921 [D loss: 0.675548, acc: 60.94%] [G loss: 1.918342]\n",
      "epoch:21 step:19922 [D loss: 0.675526, acc: 57.03%] [G loss: 1.815706]\n",
      "epoch:21 step:19923 [D loss: 0.610252, acc: 63.28%] [G loss: 1.986522]\n",
      "epoch:21 step:19924 [D loss: 0.663440, acc: 60.94%] [G loss: 1.928626]\n",
      "epoch:21 step:19925 [D loss: 0.602945, acc: 68.75%] [G loss: 2.110295]\n",
      "epoch:21 step:19926 [D loss: 0.686730, acc: 59.38%] [G loss: 1.756427]\n",
      "epoch:21 step:19927 [D loss: 0.730218, acc: 53.91%] [G loss: 1.770342]\n",
      "epoch:21 step:19928 [D loss: 0.692783, acc: 59.38%] [G loss: 1.793392]\n",
      "epoch:21 step:19929 [D loss: 0.665049, acc: 59.38%] [G loss: 1.661215]\n",
      "epoch:21 step:19930 [D loss: 0.663255, acc: 60.94%] [G loss: 1.846925]\n",
      "epoch:21 step:19931 [D loss: 0.676855, acc: 59.38%] [G loss: 1.767972]\n",
      "epoch:21 step:19932 [D loss: 0.661955, acc: 60.94%] [G loss: 1.785254]\n",
      "epoch:21 step:19933 [D loss: 0.623777, acc: 67.19%] [G loss: 1.903767]\n",
      "epoch:21 step:19934 [D loss: 0.661499, acc: 60.16%] [G loss: 1.789569]\n",
      "epoch:21 step:19935 [D loss: 0.663646, acc: 59.38%] [G loss: 1.795715]\n",
      "epoch:21 step:19936 [D loss: 0.659178, acc: 60.16%] [G loss: 1.718118]\n",
      "epoch:21 step:19937 [D loss: 0.621997, acc: 65.62%] [G loss: 1.766652]\n",
      "epoch:21 step:19938 [D loss: 0.651842, acc: 60.16%] [G loss: 1.820477]\n",
      "epoch:21 step:19939 [D loss: 0.622704, acc: 71.88%] [G loss: 2.008674]\n",
      "epoch:21 step:19940 [D loss: 0.678581, acc: 57.03%] [G loss: 1.874169]\n",
      "epoch:21 step:19941 [D loss: 0.629097, acc: 64.84%] [G loss: 2.083529]\n",
      "epoch:21 step:19942 [D loss: 0.650901, acc: 65.62%] [G loss: 1.820827]\n",
      "epoch:21 step:19943 [D loss: 0.647490, acc: 58.59%] [G loss: 1.985560]\n",
      "epoch:21 step:19944 [D loss: 0.669834, acc: 57.03%] [G loss: 1.915492]\n",
      "epoch:21 step:19945 [D loss: 0.637863, acc: 60.16%] [G loss: 1.947757]\n",
      "epoch:21 step:19946 [D loss: 0.632118, acc: 68.75%] [G loss: 1.819110]\n",
      "epoch:21 step:19947 [D loss: 0.657928, acc: 63.28%] [G loss: 1.960834]\n",
      "epoch:21 step:19948 [D loss: 0.676154, acc: 60.94%] [G loss: 1.908689]\n",
      "epoch:21 step:19949 [D loss: 0.655251, acc: 62.50%] [G loss: 1.896567]\n",
      "epoch:21 step:19950 [D loss: 0.629842, acc: 64.06%] [G loss: 1.744347]\n",
      "epoch:21 step:19951 [D loss: 0.565375, acc: 74.22%] [G loss: 2.091789]\n",
      "epoch:21 step:19952 [D loss: 0.633470, acc: 64.06%] [G loss: 2.079751]\n",
      "epoch:21 step:19953 [D loss: 0.578076, acc: 71.09%] [G loss: 2.245556]\n",
      "epoch:21 step:19954 [D loss: 0.670607, acc: 58.59%] [G loss: 1.851088]\n",
      "epoch:21 step:19955 [D loss: 0.677471, acc: 60.94%] [G loss: 1.980736]\n",
      "epoch:21 step:19956 [D loss: 0.665869, acc: 58.59%] [G loss: 1.941204]\n",
      "epoch:21 step:19957 [D loss: 0.617182, acc: 66.41%] [G loss: 2.000447]\n",
      "epoch:21 step:19958 [D loss: 0.651969, acc: 60.16%] [G loss: 1.891415]\n",
      "epoch:21 step:19959 [D loss: 0.625951, acc: 61.72%] [G loss: 1.880507]\n",
      "epoch:21 step:19960 [D loss: 0.628555, acc: 63.28%] [G loss: 1.869115]\n",
      "epoch:21 step:19961 [D loss: 0.620767, acc: 65.62%] [G loss: 1.839484]\n",
      "epoch:21 step:19962 [D loss: 0.678413, acc: 60.94%] [G loss: 1.814838]\n",
      "epoch:21 step:19963 [D loss: 0.625127, acc: 59.38%] [G loss: 2.009162]\n",
      "epoch:21 step:19964 [D loss: 0.686668, acc: 58.59%] [G loss: 1.922014]\n",
      "epoch:21 step:19965 [D loss: 0.653206, acc: 64.84%] [G loss: 1.865567]\n",
      "epoch:21 step:19966 [D loss: 0.697591, acc: 60.16%] [G loss: 1.911723]\n",
      "epoch:21 step:19967 [D loss: 0.671749, acc: 57.03%] [G loss: 1.850616]\n",
      "epoch:21 step:19968 [D loss: 0.637918, acc: 65.62%] [G loss: 1.790006]\n",
      "epoch:21 step:19969 [D loss: 0.625200, acc: 64.06%] [G loss: 1.849096]\n",
      "epoch:21 step:19970 [D loss: 0.610103, acc: 64.06%] [G loss: 1.916631]\n",
      "epoch:21 step:19971 [D loss: 0.638665, acc: 67.19%] [G loss: 1.798622]\n",
      "epoch:21 step:19972 [D loss: 0.650616, acc: 61.72%] [G loss: 1.853361]\n",
      "epoch:21 step:19973 [D loss: 0.626871, acc: 66.41%] [G loss: 1.927614]\n",
      "epoch:21 step:19974 [D loss: 0.679160, acc: 55.47%] [G loss: 1.823310]\n",
      "epoch:21 step:19975 [D loss: 0.655977, acc: 57.03%] [G loss: 1.852332]\n",
      "epoch:21 step:19976 [D loss: 0.613391, acc: 65.62%] [G loss: 1.962006]\n",
      "epoch:21 step:19977 [D loss: 0.651391, acc: 60.94%] [G loss: 1.869676]\n",
      "epoch:21 step:19978 [D loss: 0.659645, acc: 58.59%] [G loss: 1.774131]\n",
      "epoch:21 step:19979 [D loss: 0.623190, acc: 64.84%] [G loss: 1.781801]\n",
      "epoch:21 step:19980 [D loss: 0.665997, acc: 58.59%] [G loss: 1.862603]\n",
      "epoch:21 step:19981 [D loss: 0.709833, acc: 52.34%] [G loss: 1.902103]\n",
      "epoch:21 step:19982 [D loss: 0.678093, acc: 59.38%] [G loss: 1.836580]\n",
      "epoch:21 step:19983 [D loss: 0.681500, acc: 60.16%] [G loss: 1.773406]\n",
      "epoch:21 step:19984 [D loss: 0.650819, acc: 58.59%] [G loss: 1.759980]\n",
      "epoch:21 step:19985 [D loss: 0.623510, acc: 64.06%] [G loss: 1.762512]\n",
      "epoch:21 step:19986 [D loss: 0.643216, acc: 60.16%] [G loss: 1.830173]\n",
      "epoch:21 step:19987 [D loss: 0.640745, acc: 61.72%] [G loss: 1.829848]\n",
      "epoch:21 step:19988 [D loss: 0.663285, acc: 54.69%] [G loss: 1.913193]\n",
      "epoch:21 step:19989 [D loss: 0.588292, acc: 71.09%] [G loss: 2.207265]\n",
      "epoch:21 step:19990 [D loss: 0.574418, acc: 74.22%] [G loss: 2.109904]\n",
      "epoch:21 step:19991 [D loss: 0.576506, acc: 72.66%] [G loss: 2.307453]\n",
      "epoch:21 step:19992 [D loss: 0.568586, acc: 69.53%] [G loss: 2.080325]\n",
      "epoch:21 step:19993 [D loss: 0.655563, acc: 63.28%] [G loss: 1.805160]\n",
      "epoch:21 step:19994 [D loss: 0.738743, acc: 52.34%] [G loss: 1.903789]\n",
      "epoch:21 step:19995 [D loss: 0.590983, acc: 72.66%] [G loss: 2.132593]\n",
      "epoch:21 step:19996 [D loss: 0.699919, acc: 52.34%] [G loss: 1.832625]\n",
      "epoch:21 step:19997 [D loss: 0.621537, acc: 62.50%] [G loss: 1.787545]\n",
      "epoch:21 step:19998 [D loss: 0.619286, acc: 61.72%] [G loss: 1.910940]\n",
      "epoch:21 step:19999 [D loss: 0.601919, acc: 68.75%] [G loss: 1.981681]\n",
      "epoch:21 step:20000 [D loss: 0.691404, acc: 51.56%] [G loss: 1.851703]\n",
      "##############\n",
      "[2.40095903 1.1508169  6.32082256 4.60165357 3.52638747 5.54091341\n",
      " 4.47644688 4.69863683 4.48081673 3.49244355]\n",
      "##########\n",
      "epoch:21 step:20001 [D loss: 0.603954, acc: 72.66%] [G loss: 1.920988]\n",
      "epoch:21 step:20002 [D loss: 0.662317, acc: 63.28%] [G loss: 1.692189]\n",
      "epoch:21 step:20003 [D loss: 0.609659, acc: 64.84%] [G loss: 1.955011]\n",
      "epoch:21 step:20004 [D loss: 0.629567, acc: 62.50%] [G loss: 1.938981]\n",
      "epoch:21 step:20005 [D loss: 0.676033, acc: 57.03%] [G loss: 1.926691]\n",
      "epoch:21 step:20006 [D loss: 0.648011, acc: 66.41%] [G loss: 1.994481]\n",
      "epoch:21 step:20007 [D loss: 0.606167, acc: 66.41%] [G loss: 1.941394]\n",
      "epoch:21 step:20008 [D loss: 0.633183, acc: 64.84%] [G loss: 1.745838]\n",
      "epoch:21 step:20009 [D loss: 0.612702, acc: 67.97%] [G loss: 1.952482]\n",
      "epoch:21 step:20010 [D loss: 0.638215, acc: 67.19%] [G loss: 2.026956]\n",
      "epoch:21 step:20011 [D loss: 0.576283, acc: 72.66%] [G loss: 2.062844]\n",
      "epoch:21 step:20012 [D loss: 0.639916, acc: 60.16%] [G loss: 2.033696]\n",
      "epoch:21 step:20013 [D loss: 0.590358, acc: 67.19%] [G loss: 2.005419]\n",
      "epoch:21 step:20014 [D loss: 0.620285, acc: 65.62%] [G loss: 2.108641]\n",
      "epoch:21 step:20015 [D loss: 0.667272, acc: 60.16%] [G loss: 2.001056]\n",
      "epoch:21 step:20016 [D loss: 0.596818, acc: 70.31%] [G loss: 2.025348]\n",
      "epoch:21 step:20017 [D loss: 0.591304, acc: 66.41%] [G loss: 2.111476]\n",
      "epoch:21 step:20018 [D loss: 0.763170, acc: 50.78%] [G loss: 1.862594]\n",
      "epoch:21 step:20019 [D loss: 0.665486, acc: 62.50%] [G loss: 2.020572]\n",
      "epoch:21 step:20020 [D loss: 0.697108, acc: 49.22%] [G loss: 1.799250]\n",
      "epoch:21 step:20021 [D loss: 0.673815, acc: 59.38%] [G loss: 1.667940]\n",
      "epoch:21 step:20022 [D loss: 0.560194, acc: 72.66%] [G loss: 2.107474]\n",
      "epoch:21 step:20023 [D loss: 0.613925, acc: 67.19%] [G loss: 2.179712]\n",
      "epoch:21 step:20024 [D loss: 0.580759, acc: 68.75%] [G loss: 2.387880]\n",
      "epoch:21 step:20025 [D loss: 0.645526, acc: 63.28%] [G loss: 1.752177]\n",
      "epoch:21 step:20026 [D loss: 0.686610, acc: 61.72%] [G loss: 1.741491]\n",
      "epoch:21 step:20027 [D loss: 0.664052, acc: 60.94%] [G loss: 1.886235]\n",
      "epoch:21 step:20028 [D loss: 0.723967, acc: 50.78%] [G loss: 1.966199]\n",
      "epoch:21 step:20029 [D loss: 0.647090, acc: 61.72%] [G loss: 1.910746]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:21 step:20030 [D loss: 0.609176, acc: 66.41%] [G loss: 1.998835]\n",
      "epoch:21 step:20031 [D loss: 0.674914, acc: 60.16%] [G loss: 1.971081]\n",
      "epoch:21 step:20032 [D loss: 0.708372, acc: 50.78%] [G loss: 1.729242]\n",
      "epoch:21 step:20033 [D loss: 0.637564, acc: 63.28%] [G loss: 1.721783]\n",
      "epoch:21 step:20034 [D loss: 0.601074, acc: 67.19%] [G loss: 1.888194]\n",
      "epoch:21 step:20035 [D loss: 0.682734, acc: 57.03%] [G loss: 1.855159]\n",
      "epoch:21 step:20036 [D loss: 0.609821, acc: 71.09%] [G loss: 1.948429]\n",
      "epoch:21 step:20037 [D loss: 0.640858, acc: 63.28%] [G loss: 1.854399]\n",
      "epoch:21 step:20038 [D loss: 0.612454, acc: 69.53%] [G loss: 1.816904]\n",
      "epoch:21 step:20039 [D loss: 0.679548, acc: 55.47%] [G loss: 1.840669]\n",
      "epoch:21 step:20040 [D loss: 0.656611, acc: 60.94%] [G loss: 1.912185]\n",
      "epoch:21 step:20041 [D loss: 0.662883, acc: 59.38%] [G loss: 1.932094]\n",
      "epoch:21 step:20042 [D loss: 0.635482, acc: 61.72%] [G loss: 1.743353]\n",
      "epoch:21 step:20043 [D loss: 0.662097, acc: 56.25%] [G loss: 1.799512]\n",
      "epoch:21 step:20044 [D loss: 0.634739, acc: 60.16%] [G loss: 2.017713]\n",
      "epoch:21 step:20045 [D loss: 0.653769, acc: 60.94%] [G loss: 1.753341]\n",
      "epoch:21 step:20046 [D loss: 0.679587, acc: 57.03%] [G loss: 1.967745]\n",
      "epoch:21 step:20047 [D loss: 0.663280, acc: 56.25%] [G loss: 1.958370]\n",
      "epoch:21 step:20048 [D loss: 0.632593, acc: 60.94%] [G loss: 1.944032]\n",
      "epoch:21 step:20049 [D loss: 0.661052, acc: 59.38%] [G loss: 1.864151]\n",
      "epoch:21 step:20050 [D loss: 0.669019, acc: 58.59%] [G loss: 1.851766]\n",
      "epoch:21 step:20051 [D loss: 0.671590, acc: 57.81%] [G loss: 1.892087]\n",
      "epoch:21 step:20052 [D loss: 0.689828, acc: 54.69%] [G loss: 1.865342]\n",
      "epoch:21 step:20053 [D loss: 0.692485, acc: 53.12%] [G loss: 1.829185]\n",
      "epoch:21 step:20054 [D loss: 0.688907, acc: 50.78%] [G loss: 1.765942]\n",
      "epoch:21 step:20055 [D loss: 0.662578, acc: 64.06%] [G loss: 1.916333]\n",
      "epoch:21 step:20056 [D loss: 0.645010, acc: 62.50%] [G loss: 1.917317]\n",
      "epoch:21 step:20057 [D loss: 0.625588, acc: 70.31%] [G loss: 1.810259]\n",
      "epoch:21 step:20058 [D loss: 0.584546, acc: 71.09%] [G loss: 1.993736]\n",
      "epoch:21 step:20059 [D loss: 0.659315, acc: 57.81%] [G loss: 1.795468]\n",
      "epoch:21 step:20060 [D loss: 0.625353, acc: 63.28%] [G loss: 1.852182]\n",
      "epoch:21 step:20061 [D loss: 0.600351, acc: 67.97%] [G loss: 1.928589]\n",
      "epoch:21 step:20062 [D loss: 0.646531, acc: 64.06%] [G loss: 1.879185]\n",
      "epoch:21 step:20063 [D loss: 0.713812, acc: 52.34%] [G loss: 1.789519]\n",
      "epoch:21 step:20064 [D loss: 0.659668, acc: 60.16%] [G loss: 1.804242]\n",
      "epoch:21 step:20065 [D loss: 0.646991, acc: 62.50%] [G loss: 1.764493]\n",
      "epoch:21 step:20066 [D loss: 0.647084, acc: 67.19%] [G loss: 1.770091]\n",
      "epoch:21 step:20067 [D loss: 0.654350, acc: 61.72%] [G loss: 1.865985]\n",
      "epoch:21 step:20068 [D loss: 0.677833, acc: 54.69%] [G loss: 1.782030]\n",
      "epoch:21 step:20069 [D loss: 0.642518, acc: 60.16%] [G loss: 1.908699]\n",
      "epoch:21 step:20070 [D loss: 0.697147, acc: 61.72%] [G loss: 1.816253]\n",
      "epoch:21 step:20071 [D loss: 0.646339, acc: 64.84%] [G loss: 1.903852]\n",
      "epoch:21 step:20072 [D loss: 0.599095, acc: 63.28%] [G loss: 1.983482]\n",
      "epoch:21 step:20073 [D loss: 0.675939, acc: 59.38%] [G loss: 1.711889]\n",
      "epoch:21 step:20074 [D loss: 0.672902, acc: 60.16%] [G loss: 1.722453]\n",
      "epoch:21 step:20075 [D loss: 0.599967, acc: 69.53%] [G loss: 1.971235]\n",
      "epoch:21 step:20076 [D loss: 0.667431, acc: 60.94%] [G loss: 1.877561]\n",
      "epoch:21 step:20077 [D loss: 0.675752, acc: 55.47%] [G loss: 1.805620]\n",
      "epoch:21 step:20078 [D loss: 0.625887, acc: 69.53%] [G loss: 1.911452]\n",
      "epoch:21 step:20079 [D loss: 0.655594, acc: 62.50%] [G loss: 1.870892]\n",
      "epoch:21 step:20080 [D loss: 0.652925, acc: 60.94%] [G loss: 1.926582]\n",
      "epoch:21 step:20081 [D loss: 0.625223, acc: 60.94%] [G loss: 2.015051]\n",
      "epoch:21 step:20082 [D loss: 0.610395, acc: 65.62%] [G loss: 2.060761]\n",
      "epoch:21 step:20083 [D loss: 0.641205, acc: 61.72%] [G loss: 1.918794]\n",
      "epoch:21 step:20084 [D loss: 0.634287, acc: 63.28%] [G loss: 1.846185]\n",
      "epoch:21 step:20085 [D loss: 0.650505, acc: 65.62%] [G loss: 2.048457]\n",
      "epoch:21 step:20086 [D loss: 0.602557, acc: 69.53%] [G loss: 1.805639]\n",
      "epoch:21 step:20087 [D loss: 0.688883, acc: 60.16%] [G loss: 1.687315]\n",
      "epoch:21 step:20088 [D loss: 0.730821, acc: 55.47%] [G loss: 1.868316]\n",
      "epoch:21 step:20089 [D loss: 0.682368, acc: 53.12%] [G loss: 2.004874]\n",
      "epoch:21 step:20090 [D loss: 0.665091, acc: 64.06%] [G loss: 1.908187]\n",
      "epoch:21 step:20091 [D loss: 0.673733, acc: 62.50%] [G loss: 1.905077]\n",
      "epoch:21 step:20092 [D loss: 0.605948, acc: 67.19%] [G loss: 2.004422]\n",
      "epoch:21 step:20093 [D loss: 0.608040, acc: 66.41%] [G loss: 1.986759]\n",
      "epoch:21 step:20094 [D loss: 0.646831, acc: 65.62%] [G loss: 1.976512]\n",
      "epoch:21 step:20095 [D loss: 0.663015, acc: 57.81%] [G loss: 1.731986]\n",
      "epoch:21 step:20096 [D loss: 0.692025, acc: 59.38%] [G loss: 1.857444]\n",
      "epoch:21 step:20097 [D loss: 0.647434, acc: 61.72%] [G loss: 1.849924]\n",
      "epoch:21 step:20098 [D loss: 0.710318, acc: 54.69%] [G loss: 1.883163]\n",
      "epoch:21 step:20099 [D loss: 0.687956, acc: 64.06%] [G loss: 1.842271]\n",
      "epoch:21 step:20100 [D loss: 0.623442, acc: 67.97%] [G loss: 1.817562]\n",
      "epoch:21 step:20101 [D loss: 0.604510, acc: 67.97%] [G loss: 1.841199]\n",
      "epoch:21 step:20102 [D loss: 0.668174, acc: 60.94%] [G loss: 1.889195]\n",
      "epoch:21 step:20103 [D loss: 0.635414, acc: 61.72%] [G loss: 2.009826]\n",
      "epoch:21 step:20104 [D loss: 0.660188, acc: 61.72%] [G loss: 1.826023]\n",
      "epoch:21 step:20105 [D loss: 0.600493, acc: 68.75%] [G loss: 2.037533]\n",
      "epoch:21 step:20106 [D loss: 0.652937, acc: 60.94%] [G loss: 2.101732]\n",
      "epoch:21 step:20107 [D loss: 0.603609, acc: 70.31%] [G loss: 2.084055]\n",
      "epoch:21 step:20108 [D loss: 0.702674, acc: 60.16%] [G loss: 1.818197]\n",
      "epoch:21 step:20109 [D loss: 0.677858, acc: 61.72%] [G loss: 1.816999]\n",
      "epoch:21 step:20110 [D loss: 0.626958, acc: 60.16%] [G loss: 1.770943]\n",
      "epoch:21 step:20111 [D loss: 0.631727, acc: 59.38%] [G loss: 2.031456]\n",
      "epoch:21 step:20112 [D loss: 0.632631, acc: 64.84%] [G loss: 1.881120]\n",
      "epoch:21 step:20113 [D loss: 0.600910, acc: 65.62%] [G loss: 1.948406]\n",
      "epoch:21 step:20114 [D loss: 0.680074, acc: 57.03%] [G loss: 1.668770]\n",
      "epoch:21 step:20115 [D loss: 0.653654, acc: 62.50%] [G loss: 1.769102]\n",
      "epoch:21 step:20116 [D loss: 0.729328, acc: 54.69%] [G loss: 1.738642]\n",
      "epoch:21 step:20117 [D loss: 0.646813, acc: 60.16%] [G loss: 1.677162]\n",
      "epoch:21 step:20118 [D loss: 0.604118, acc: 69.53%] [G loss: 1.741496]\n",
      "epoch:21 step:20119 [D loss: 0.658609, acc: 57.81%] [G loss: 1.792301]\n",
      "epoch:21 step:20120 [D loss: 0.704544, acc: 58.59%] [G loss: 1.777801]\n",
      "epoch:21 step:20121 [D loss: 0.678337, acc: 60.16%] [G loss: 1.678052]\n",
      "epoch:21 step:20122 [D loss: 0.655791, acc: 58.59%] [G loss: 1.754952]\n",
      "epoch:21 step:20123 [D loss: 0.680388, acc: 57.81%] [G loss: 1.729599]\n",
      "epoch:21 step:20124 [D loss: 0.647054, acc: 64.06%] [G loss: 1.775373]\n",
      "epoch:21 step:20125 [D loss: 0.671475, acc: 60.94%] [G loss: 1.747464]\n",
      "epoch:21 step:20126 [D loss: 0.674074, acc: 63.28%] [G loss: 1.796371]\n",
      "epoch:21 step:20127 [D loss: 0.570266, acc: 72.66%] [G loss: 1.902221]\n",
      "epoch:21 step:20128 [D loss: 0.633504, acc: 62.50%] [G loss: 1.788294]\n",
      "epoch:21 step:20129 [D loss: 0.643966, acc: 65.62%] [G loss: 1.835744]\n",
      "epoch:21 step:20130 [D loss: 0.643712, acc: 64.84%] [G loss: 1.926034]\n",
      "epoch:21 step:20131 [D loss: 0.622528, acc: 61.72%] [G loss: 1.852043]\n",
      "epoch:21 step:20132 [D loss: 0.620158, acc: 65.62%] [G loss: 1.883419]\n",
      "epoch:21 step:20133 [D loss: 0.623766, acc: 65.62%] [G loss: 1.890596]\n",
      "epoch:21 step:20134 [D loss: 0.636649, acc: 61.72%] [G loss: 1.996551]\n",
      "epoch:21 step:20135 [D loss: 0.659017, acc: 59.38%] [G loss: 1.814268]\n",
      "epoch:21 step:20136 [D loss: 0.641102, acc: 61.72%] [G loss: 1.790344]\n",
      "epoch:21 step:20137 [D loss: 0.682106, acc: 59.38%] [G loss: 1.836491]\n",
      "epoch:21 step:20138 [D loss: 0.655727, acc: 59.38%] [G loss: 1.927010]\n",
      "epoch:21 step:20139 [D loss: 0.645433, acc: 68.75%] [G loss: 1.810137]\n",
      "epoch:21 step:20140 [D loss: 0.592553, acc: 67.97%] [G loss: 1.922626]\n",
      "epoch:21 step:20141 [D loss: 0.587328, acc: 69.53%] [G loss: 1.932947]\n",
      "epoch:21 step:20142 [D loss: 0.671015, acc: 62.50%] [G loss: 1.889121]\n",
      "epoch:21 step:20143 [D loss: 0.663996, acc: 60.94%] [G loss: 1.866623]\n",
      "epoch:21 step:20144 [D loss: 0.617746, acc: 67.97%] [G loss: 1.880694]\n",
      "epoch:21 step:20145 [D loss: 0.596943, acc: 62.50%] [G loss: 1.997549]\n",
      "epoch:21 step:20146 [D loss: 0.557201, acc: 76.56%] [G loss: 2.123640]\n",
      "epoch:21 step:20147 [D loss: 0.633064, acc: 71.88%] [G loss: 2.082573]\n",
      "epoch:21 step:20148 [D loss: 0.639198, acc: 59.38%] [G loss: 2.136066]\n",
      "epoch:21 step:20149 [D loss: 0.618482, acc: 62.50%] [G loss: 2.039312]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:21 step:20150 [D loss: 0.721223, acc: 55.47%] [G loss: 1.803093]\n",
      "epoch:21 step:20151 [D loss: 0.649588, acc: 63.28%] [G loss: 1.853379]\n",
      "epoch:21 step:20152 [D loss: 0.657503, acc: 61.72%] [G loss: 1.884911]\n",
      "epoch:21 step:20153 [D loss: 0.606622, acc: 68.75%] [G loss: 1.890504]\n",
      "epoch:21 step:20154 [D loss: 0.675468, acc: 55.47%] [G loss: 1.764187]\n",
      "epoch:21 step:20155 [D loss: 0.635261, acc: 64.06%] [G loss: 1.890824]\n",
      "epoch:21 step:20156 [D loss: 0.627498, acc: 62.50%] [G loss: 2.049190]\n",
      "epoch:21 step:20157 [D loss: 0.662656, acc: 56.25%] [G loss: 1.996301]\n",
      "epoch:21 step:20158 [D loss: 0.605020, acc: 67.19%] [G loss: 2.041881]\n",
      "epoch:21 step:20159 [D loss: 0.676769, acc: 59.38%] [G loss: 1.883268]\n",
      "epoch:21 step:20160 [D loss: 0.699527, acc: 58.59%] [G loss: 1.863156]\n",
      "epoch:21 step:20161 [D loss: 0.614416, acc: 66.41%] [G loss: 1.776015]\n",
      "epoch:21 step:20162 [D loss: 0.658986, acc: 61.72%] [G loss: 1.871132]\n",
      "epoch:21 step:20163 [D loss: 0.644560, acc: 65.62%] [G loss: 1.823517]\n",
      "epoch:21 step:20164 [D loss: 0.657974, acc: 61.72%] [G loss: 1.937152]\n",
      "epoch:21 step:20165 [D loss: 0.625161, acc: 67.19%] [G loss: 1.966195]\n",
      "epoch:21 step:20166 [D loss: 0.626326, acc: 64.06%] [G loss: 1.920761]\n",
      "epoch:21 step:20167 [D loss: 0.663189, acc: 64.06%] [G loss: 1.874455]\n",
      "epoch:21 step:20168 [D loss: 0.640578, acc: 63.28%] [G loss: 2.012527]\n",
      "epoch:21 step:20169 [D loss: 0.679619, acc: 57.81%] [G loss: 1.865680]\n",
      "epoch:21 step:20170 [D loss: 0.591749, acc: 70.31%] [G loss: 1.925667]\n",
      "epoch:21 step:20171 [D loss: 0.625422, acc: 62.50%] [G loss: 1.835599]\n",
      "epoch:21 step:20172 [D loss: 0.573353, acc: 70.31%] [G loss: 2.066290]\n",
      "epoch:21 step:20173 [D loss: 0.672198, acc: 59.38%] [G loss: 1.872936]\n",
      "epoch:21 step:20174 [D loss: 0.606619, acc: 69.53%] [G loss: 2.032330]\n",
      "epoch:21 step:20175 [D loss: 0.603269, acc: 68.75%] [G loss: 2.239463]\n",
      "epoch:21 step:20176 [D loss: 0.605348, acc: 67.97%] [G loss: 2.073111]\n",
      "epoch:21 step:20177 [D loss: 0.740236, acc: 52.34%] [G loss: 1.822161]\n",
      "epoch:21 step:20178 [D loss: 0.702990, acc: 57.81%] [G loss: 1.798907]\n",
      "epoch:21 step:20179 [D loss: 0.708392, acc: 50.00%] [G loss: 1.711237]\n",
      "epoch:21 step:20180 [D loss: 0.637480, acc: 64.06%] [G loss: 1.781400]\n",
      "epoch:21 step:20181 [D loss: 0.664696, acc: 55.47%] [G loss: 2.070457]\n",
      "epoch:21 step:20182 [D loss: 0.616880, acc: 67.97%] [G loss: 1.796651]\n",
      "epoch:21 step:20183 [D loss: 0.675232, acc: 57.81%] [G loss: 1.844631]\n",
      "epoch:21 step:20184 [D loss: 0.704279, acc: 51.56%] [G loss: 1.711626]\n",
      "epoch:21 step:20185 [D loss: 0.609071, acc: 63.28%] [G loss: 1.958322]\n",
      "epoch:21 step:20186 [D loss: 0.656007, acc: 64.06%] [G loss: 1.767984]\n",
      "epoch:21 step:20187 [D loss: 0.644844, acc: 58.59%] [G loss: 1.847787]\n",
      "epoch:21 step:20188 [D loss: 0.691100, acc: 56.25%] [G loss: 1.727238]\n",
      "epoch:21 step:20189 [D loss: 0.644361, acc: 64.84%] [G loss: 1.873849]\n",
      "epoch:21 step:20190 [D loss: 0.630898, acc: 64.84%] [G loss: 1.854850]\n",
      "epoch:21 step:20191 [D loss: 0.656900, acc: 66.41%] [G loss: 1.784221]\n",
      "epoch:21 step:20192 [D loss: 0.654906, acc: 63.28%] [G loss: 1.761434]\n",
      "epoch:21 step:20193 [D loss: 0.637105, acc: 62.50%] [G loss: 1.938918]\n",
      "epoch:21 step:20194 [D loss: 0.631890, acc: 71.09%] [G loss: 1.970467]\n",
      "epoch:21 step:20195 [D loss: 0.635410, acc: 67.19%] [G loss: 1.857013]\n",
      "epoch:21 step:20196 [D loss: 0.596710, acc: 71.09%] [G loss: 1.901297]\n",
      "epoch:21 step:20197 [D loss: 0.669143, acc: 62.50%] [G loss: 1.858100]\n",
      "epoch:21 step:20198 [D loss: 0.624832, acc: 66.41%] [G loss: 1.971576]\n",
      "epoch:21 step:20199 [D loss: 0.599294, acc: 66.41%] [G loss: 1.932248]\n",
      "epoch:21 step:20200 [D loss: 0.599396, acc: 67.97%] [G loss: 2.025628]\n",
      "##############\n",
      "[2.48216313 1.63557643 6.28364394 4.74232828 3.57260055 5.74734617\n",
      " 4.18945692 4.57853566 4.46791842 3.53044453]\n",
      "##########\n",
      "epoch:21 step:20201 [D loss: 0.714981, acc: 53.91%] [G loss: 1.753728]\n",
      "epoch:21 step:20202 [D loss: 0.626828, acc: 67.19%] [G loss: 1.743706]\n",
      "epoch:21 step:20203 [D loss: 0.673556, acc: 58.59%] [G loss: 1.747525]\n",
      "epoch:21 step:20204 [D loss: 0.671864, acc: 60.16%] [G loss: 1.862301]\n",
      "epoch:21 step:20205 [D loss: 0.678627, acc: 60.94%] [G loss: 1.707666]\n",
      "epoch:21 step:20206 [D loss: 0.703637, acc: 57.03%] [G loss: 1.709490]\n",
      "epoch:21 step:20207 [D loss: 0.669546, acc: 54.69%] [G loss: 1.827106]\n",
      "epoch:21 step:20208 [D loss: 0.627869, acc: 64.84%] [G loss: 1.886812]\n",
      "epoch:21 step:20209 [D loss: 0.608415, acc: 71.88%] [G loss: 2.003064]\n",
      "epoch:21 step:20210 [D loss: 0.689442, acc: 57.81%] [G loss: 1.896120]\n",
      "epoch:21 step:20211 [D loss: 0.588477, acc: 71.88%] [G loss: 1.992464]\n",
      "epoch:21 step:20212 [D loss: 0.677765, acc: 54.69%] [G loss: 1.788735]\n",
      "epoch:21 step:20213 [D loss: 0.590491, acc: 67.97%] [G loss: 1.992594]\n",
      "epoch:21 step:20214 [D loss: 0.656197, acc: 64.84%] [G loss: 1.775835]\n",
      "epoch:21 step:20215 [D loss: 0.678901, acc: 58.59%] [G loss: 1.913143]\n",
      "epoch:21 step:20216 [D loss: 0.658579, acc: 64.06%] [G loss: 1.911573]\n",
      "epoch:21 step:20217 [D loss: 0.608738, acc: 66.41%] [G loss: 1.749147]\n",
      "epoch:21 step:20218 [D loss: 0.647871, acc: 64.84%] [G loss: 1.804435]\n",
      "epoch:21 step:20219 [D loss: 0.658637, acc: 66.41%] [G loss: 1.868810]\n",
      "epoch:21 step:20220 [D loss: 0.704635, acc: 52.34%] [G loss: 1.895234]\n",
      "epoch:21 step:20221 [D loss: 0.708364, acc: 56.25%] [G loss: 1.867278]\n",
      "epoch:21 step:20222 [D loss: 0.681908, acc: 56.25%] [G loss: 1.750264]\n",
      "epoch:21 step:20223 [D loss: 0.647696, acc: 60.94%] [G loss: 1.776340]\n",
      "epoch:21 step:20224 [D loss: 0.618902, acc: 64.84%] [G loss: 1.993816]\n",
      "epoch:21 step:20225 [D loss: 0.614184, acc: 66.41%] [G loss: 1.917722]\n",
      "epoch:21 step:20226 [D loss: 0.630199, acc: 64.84%] [G loss: 2.087818]\n",
      "epoch:21 step:20227 [D loss: 0.642629, acc: 60.16%] [G loss: 1.839354]\n",
      "epoch:21 step:20228 [D loss: 0.636397, acc: 64.06%] [G loss: 1.912102]\n",
      "epoch:21 step:20229 [D loss: 0.626671, acc: 64.84%] [G loss: 1.973532]\n",
      "epoch:21 step:20230 [D loss: 0.657396, acc: 66.41%] [G loss: 1.939888]\n",
      "epoch:21 step:20231 [D loss: 0.603991, acc: 71.09%] [G loss: 2.099230]\n",
      "epoch:21 step:20232 [D loss: 0.684645, acc: 55.47%] [G loss: 1.915704]\n",
      "epoch:21 step:20233 [D loss: 0.653021, acc: 62.50%] [G loss: 1.947040]\n",
      "epoch:21 step:20234 [D loss: 0.656315, acc: 58.59%] [G loss: 2.078115]\n",
      "epoch:21 step:20235 [D loss: 0.631181, acc: 69.53%] [G loss: 2.000361]\n",
      "epoch:21 step:20236 [D loss: 0.689510, acc: 57.03%] [G loss: 1.640421]\n",
      "epoch:21 step:20237 [D loss: 0.640533, acc: 60.94%] [G loss: 1.767747]\n",
      "epoch:21 step:20238 [D loss: 0.626368, acc: 62.50%] [G loss: 1.883364]\n",
      "epoch:21 step:20239 [D loss: 0.626297, acc: 65.62%] [G loss: 1.855152]\n",
      "epoch:21 step:20240 [D loss: 0.683317, acc: 58.59%] [G loss: 1.915071]\n",
      "epoch:21 step:20241 [D loss: 0.705609, acc: 57.81%] [G loss: 1.974064]\n",
      "epoch:21 step:20242 [D loss: 0.666552, acc: 57.03%] [G loss: 1.809114]\n",
      "epoch:21 step:20243 [D loss: 0.683509, acc: 57.81%] [G loss: 1.822547]\n",
      "epoch:21 step:20244 [D loss: 0.681819, acc: 54.69%] [G loss: 1.668385]\n",
      "epoch:21 step:20245 [D loss: 0.672727, acc: 58.59%] [G loss: 1.713514]\n",
      "epoch:21 step:20246 [D loss: 0.671152, acc: 60.16%] [G loss: 1.929136]\n",
      "epoch:21 step:20247 [D loss: 0.612877, acc: 68.75%] [G loss: 1.891724]\n",
      "epoch:21 step:20248 [D loss: 0.630815, acc: 63.28%] [G loss: 1.891965]\n",
      "epoch:21 step:20249 [D loss: 0.610802, acc: 68.75%] [G loss: 1.783589]\n",
      "epoch:21 step:20250 [D loss: 0.712785, acc: 53.12%] [G loss: 1.922977]\n",
      "epoch:21 step:20251 [D loss: 0.694946, acc: 52.34%] [G loss: 1.867335]\n",
      "epoch:21 step:20252 [D loss: 0.630793, acc: 67.97%] [G loss: 1.880651]\n",
      "epoch:21 step:20253 [D loss: 0.668889, acc: 58.59%] [G loss: 1.707206]\n",
      "epoch:21 step:20254 [D loss: 0.678702, acc: 54.69%] [G loss: 1.703133]\n",
      "epoch:21 step:20255 [D loss: 0.645025, acc: 57.81%] [G loss: 1.866821]\n",
      "epoch:21 step:20256 [D loss: 0.659969, acc: 60.16%] [G loss: 1.810062]\n",
      "epoch:21 step:20257 [D loss: 0.668220, acc: 59.38%] [G loss: 1.885122]\n",
      "epoch:21 step:20258 [D loss: 0.639493, acc: 66.41%] [G loss: 1.806708]\n",
      "epoch:21 step:20259 [D loss: 0.659309, acc: 61.72%] [G loss: 1.765175]\n",
      "epoch:21 step:20260 [D loss: 0.649555, acc: 59.38%] [G loss: 1.813112]\n",
      "epoch:21 step:20261 [D loss: 0.675507, acc: 58.59%] [G loss: 1.745370]\n",
      "epoch:21 step:20262 [D loss: 0.600578, acc: 70.31%] [G loss: 1.807455]\n",
      "epoch:21 step:20263 [D loss: 0.640836, acc: 57.03%] [G loss: 1.855810]\n",
      "epoch:21 step:20264 [D loss: 0.610025, acc: 66.41%] [G loss: 1.876474]\n",
      "epoch:21 step:20265 [D loss: 0.615017, acc: 70.31%] [G loss: 2.105794]\n",
      "epoch:21 step:20266 [D loss: 0.668740, acc: 62.50%] [G loss: 1.988471]\n",
      "epoch:21 step:20267 [D loss: 0.675143, acc: 58.59%] [G loss: 1.793272]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:21 step:20268 [D loss: 0.637900, acc: 65.62%] [G loss: 1.858285]\n",
      "epoch:21 step:20269 [D loss: 0.637896, acc: 64.84%] [G loss: 1.888856]\n",
      "epoch:21 step:20270 [D loss: 0.644948, acc: 64.84%] [G loss: 1.958294]\n",
      "epoch:21 step:20271 [D loss: 0.680690, acc: 59.38%] [G loss: 1.846515]\n",
      "epoch:21 step:20272 [D loss: 0.610574, acc: 74.22%] [G loss: 1.849430]\n",
      "epoch:21 step:20273 [D loss: 0.642316, acc: 61.72%] [G loss: 1.815387]\n",
      "epoch:21 step:20274 [D loss: 0.636238, acc: 64.84%] [G loss: 1.872876]\n",
      "epoch:21 step:20275 [D loss: 0.609208, acc: 68.75%] [G loss: 1.741427]\n",
      "epoch:21 step:20276 [D loss: 0.627871, acc: 67.97%] [G loss: 1.863127]\n",
      "epoch:21 step:20277 [D loss: 0.629388, acc: 64.06%] [G loss: 1.880964]\n",
      "epoch:21 step:20278 [D loss: 0.619412, acc: 64.06%] [G loss: 1.976447]\n",
      "epoch:21 step:20279 [D loss: 0.636733, acc: 67.97%] [G loss: 1.900365]\n",
      "epoch:21 step:20280 [D loss: 0.602118, acc: 71.09%] [G loss: 1.935372]\n",
      "epoch:21 step:20281 [D loss: 0.675692, acc: 53.12%] [G loss: 1.958492]\n",
      "epoch:21 step:20282 [D loss: 0.723763, acc: 59.38%] [G loss: 1.992865]\n",
      "epoch:21 step:20283 [D loss: 0.698972, acc: 54.69%] [G loss: 1.741061]\n",
      "epoch:21 step:20284 [D loss: 0.607681, acc: 68.75%] [G loss: 1.803947]\n",
      "epoch:21 step:20285 [D loss: 0.693444, acc: 51.56%] [G loss: 1.916748]\n",
      "epoch:21 step:20286 [D loss: 0.622688, acc: 64.06%] [G loss: 1.857768]\n",
      "epoch:21 step:20287 [D loss: 0.657839, acc: 63.28%] [G loss: 1.759622]\n",
      "epoch:21 step:20288 [D loss: 0.630586, acc: 63.28%] [G loss: 1.948756]\n",
      "epoch:21 step:20289 [D loss: 0.625731, acc: 64.84%] [G loss: 1.867571]\n",
      "epoch:21 step:20290 [D loss: 0.621512, acc: 68.75%] [G loss: 1.973729]\n",
      "epoch:21 step:20291 [D loss: 0.680060, acc: 60.16%] [G loss: 1.766863]\n",
      "epoch:21 step:20292 [D loss: 0.705476, acc: 55.47%] [G loss: 1.850308]\n",
      "epoch:21 step:20293 [D loss: 0.607171, acc: 67.97%] [G loss: 1.695853]\n",
      "epoch:21 step:20294 [D loss: 0.660751, acc: 62.50%] [G loss: 1.892842]\n",
      "epoch:21 step:20295 [D loss: 0.654231, acc: 53.12%] [G loss: 1.815749]\n",
      "epoch:21 step:20296 [D loss: 0.654609, acc: 58.59%] [G loss: 1.780289]\n",
      "epoch:21 step:20297 [D loss: 0.626831, acc: 67.97%] [G loss: 1.913905]\n",
      "epoch:21 step:20298 [D loss: 0.660375, acc: 57.81%] [G loss: 1.827762]\n",
      "epoch:21 step:20299 [D loss: 0.649521, acc: 61.72%] [G loss: 1.798938]\n",
      "epoch:21 step:20300 [D loss: 0.654731, acc: 59.38%] [G loss: 1.914974]\n",
      "epoch:21 step:20301 [D loss: 0.602547, acc: 67.97%] [G loss: 2.067915]\n",
      "epoch:21 step:20302 [D loss: 0.695269, acc: 61.72%] [G loss: 1.707408]\n",
      "epoch:21 step:20303 [D loss: 0.656502, acc: 60.94%] [G loss: 1.951355]\n",
      "epoch:21 step:20304 [D loss: 0.651517, acc: 64.84%] [G loss: 1.919198]\n",
      "epoch:21 step:20305 [D loss: 0.662930, acc: 58.59%] [G loss: 1.755624]\n",
      "epoch:21 step:20306 [D loss: 0.614488, acc: 65.62%] [G loss: 1.799746]\n",
      "epoch:21 step:20307 [D loss: 0.638232, acc: 67.97%] [G loss: 1.846209]\n",
      "epoch:21 step:20308 [D loss: 0.682486, acc: 60.94%] [G loss: 1.755078]\n",
      "epoch:21 step:20309 [D loss: 0.625332, acc: 71.09%] [G loss: 1.974677]\n",
      "epoch:21 step:20310 [D loss: 0.631759, acc: 67.19%] [G loss: 1.847861]\n",
      "epoch:21 step:20311 [D loss: 0.635961, acc: 60.94%] [G loss: 1.857000]\n",
      "epoch:21 step:20312 [D loss: 0.707180, acc: 58.59%] [G loss: 1.669878]\n",
      "epoch:21 step:20313 [D loss: 0.634735, acc: 64.06%] [G loss: 1.866235]\n",
      "epoch:21 step:20314 [D loss: 0.638178, acc: 64.84%] [G loss: 1.837102]\n",
      "epoch:21 step:20315 [D loss: 0.618103, acc: 65.62%] [G loss: 1.955274]\n",
      "epoch:21 step:20316 [D loss: 0.633486, acc: 65.62%] [G loss: 1.917627]\n",
      "epoch:21 step:20317 [D loss: 0.618474, acc: 65.62%] [G loss: 1.754745]\n",
      "epoch:21 step:20318 [D loss: 0.645171, acc: 66.41%] [G loss: 1.999398]\n",
      "epoch:21 step:20319 [D loss: 0.640078, acc: 63.28%] [G loss: 1.950096]\n",
      "epoch:21 step:20320 [D loss: 0.631406, acc: 60.16%] [G loss: 1.962557]\n",
      "epoch:21 step:20321 [D loss: 0.622269, acc: 63.28%] [G loss: 2.015767]\n",
      "epoch:21 step:20322 [D loss: 0.647924, acc: 65.62%] [G loss: 2.031505]\n",
      "epoch:21 step:20323 [D loss: 0.624992, acc: 62.50%] [G loss: 1.957686]\n",
      "epoch:21 step:20324 [D loss: 0.604321, acc: 69.53%] [G loss: 2.068973]\n",
      "epoch:21 step:20325 [D loss: 0.580360, acc: 64.84%] [G loss: 2.327340]\n",
      "epoch:21 step:20326 [D loss: 0.584699, acc: 72.66%] [G loss: 1.970894]\n",
      "epoch:21 step:20327 [D loss: 0.624291, acc: 66.41%] [G loss: 2.178886]\n",
      "epoch:21 step:20328 [D loss: 0.668156, acc: 64.84%] [G loss: 1.916703]\n",
      "epoch:21 step:20329 [D loss: 0.660910, acc: 64.84%] [G loss: 1.871048]\n",
      "epoch:21 step:20330 [D loss: 0.607443, acc: 70.31%] [G loss: 1.834699]\n",
      "epoch:21 step:20331 [D loss: 0.677638, acc: 62.50%] [G loss: 1.955886]\n",
      "epoch:21 step:20332 [D loss: 0.647106, acc: 58.59%] [G loss: 1.983833]\n",
      "epoch:21 step:20333 [D loss: 0.680397, acc: 56.25%] [G loss: 1.889855]\n",
      "epoch:21 step:20334 [D loss: 0.682199, acc: 61.72%] [G loss: 1.757152]\n",
      "epoch:21 step:20335 [D loss: 0.709909, acc: 51.56%] [G loss: 1.705733]\n",
      "epoch:21 step:20336 [D loss: 0.666436, acc: 57.81%] [G loss: 1.805800]\n",
      "epoch:21 step:20337 [D loss: 0.619455, acc: 65.62%] [G loss: 1.783817]\n",
      "epoch:21 step:20338 [D loss: 0.670886, acc: 57.81%] [G loss: 1.836138]\n",
      "epoch:21 step:20339 [D loss: 0.600869, acc: 65.62%] [G loss: 1.845252]\n",
      "epoch:21 step:20340 [D loss: 0.671544, acc: 58.59%] [G loss: 1.820521]\n",
      "epoch:21 step:20341 [D loss: 0.670263, acc: 60.94%] [G loss: 1.739680]\n",
      "epoch:21 step:20342 [D loss: 0.671778, acc: 57.03%] [G loss: 1.874535]\n",
      "epoch:21 step:20343 [D loss: 0.648766, acc: 63.28%] [G loss: 1.762888]\n",
      "epoch:21 step:20344 [D loss: 0.666049, acc: 53.12%] [G loss: 1.786393]\n",
      "epoch:21 step:20345 [D loss: 0.634348, acc: 68.75%] [G loss: 1.856490]\n",
      "epoch:21 step:20346 [D loss: 0.664475, acc: 57.81%] [G loss: 1.721014]\n",
      "epoch:21 step:20347 [D loss: 0.674958, acc: 55.47%] [G loss: 1.817720]\n",
      "epoch:21 step:20348 [D loss: 0.660598, acc: 60.16%] [G loss: 1.811119]\n",
      "epoch:21 step:20349 [D loss: 0.632896, acc: 64.84%] [G loss: 1.829613]\n",
      "epoch:21 step:20350 [D loss: 0.677271, acc: 60.16%] [G loss: 1.857634]\n",
      "epoch:21 step:20351 [D loss: 0.657915, acc: 57.81%] [G loss: 1.765708]\n",
      "epoch:21 step:20352 [D loss: 0.701473, acc: 59.38%] [G loss: 1.820658]\n",
      "epoch:21 step:20353 [D loss: 0.656884, acc: 60.16%] [G loss: 1.883277]\n",
      "epoch:21 step:20354 [D loss: 0.622777, acc: 67.19%] [G loss: 1.911270]\n",
      "epoch:21 step:20355 [D loss: 0.649316, acc: 58.59%] [G loss: 1.854191]\n",
      "epoch:21 step:20356 [D loss: 0.628134, acc: 64.84%] [G loss: 1.823496]\n",
      "epoch:21 step:20357 [D loss: 0.664320, acc: 62.50%] [G loss: 1.953973]\n",
      "epoch:21 step:20358 [D loss: 0.636187, acc: 67.97%] [G loss: 1.987020]\n",
      "epoch:21 step:20359 [D loss: 0.627454, acc: 67.19%] [G loss: 1.868220]\n",
      "epoch:21 step:20360 [D loss: 0.658535, acc: 55.47%] [G loss: 1.786824]\n",
      "epoch:21 step:20361 [D loss: 0.667934, acc: 55.47%] [G loss: 1.888813]\n",
      "epoch:21 step:20362 [D loss: 0.646708, acc: 64.06%] [G loss: 1.898452]\n",
      "epoch:21 step:20363 [D loss: 0.607411, acc: 67.19%] [G loss: 2.073963]\n",
      "epoch:21 step:20364 [D loss: 0.632299, acc: 62.50%] [G loss: 1.848992]\n",
      "epoch:21 step:20365 [D loss: 0.676620, acc: 57.03%] [G loss: 1.841769]\n",
      "epoch:21 step:20366 [D loss: 0.605384, acc: 70.31%] [G loss: 1.912312]\n",
      "epoch:21 step:20367 [D loss: 0.624488, acc: 62.50%] [G loss: 1.914367]\n",
      "epoch:21 step:20368 [D loss: 0.612484, acc: 65.62%] [G loss: 1.904017]\n",
      "epoch:21 step:20369 [D loss: 0.673029, acc: 56.25%] [G loss: 1.942302]\n",
      "epoch:21 step:20370 [D loss: 0.576319, acc: 71.88%] [G loss: 1.966574]\n",
      "epoch:21 step:20371 [D loss: 0.581623, acc: 66.41%] [G loss: 2.217989]\n",
      "epoch:21 step:20372 [D loss: 0.619305, acc: 64.06%] [G loss: 2.064174]\n",
      "epoch:21 step:20373 [D loss: 0.679009, acc: 57.81%] [G loss: 1.876015]\n",
      "epoch:21 step:20374 [D loss: 0.659708, acc: 61.72%] [G loss: 1.834621]\n",
      "epoch:21 step:20375 [D loss: 0.727576, acc: 57.03%] [G loss: 1.822358]\n",
      "epoch:21 step:20376 [D loss: 0.644146, acc: 60.16%] [G loss: 1.993859]\n",
      "epoch:21 step:20377 [D loss: 0.669221, acc: 57.03%] [G loss: 1.943623]\n",
      "epoch:21 step:20378 [D loss: 0.610524, acc: 64.06%] [G loss: 1.807562]\n",
      "epoch:21 step:20379 [D loss: 0.648048, acc: 61.72%] [G loss: 1.796864]\n",
      "epoch:21 step:20380 [D loss: 0.678732, acc: 57.03%] [G loss: 1.832323]\n",
      "epoch:21 step:20381 [D loss: 0.751340, acc: 52.34%] [G loss: 1.751790]\n",
      "epoch:21 step:20382 [D loss: 0.655083, acc: 57.81%] [G loss: 1.796531]\n",
      "epoch:21 step:20383 [D loss: 0.620826, acc: 60.16%] [G loss: 1.748067]\n",
      "epoch:21 step:20384 [D loss: 0.632394, acc: 63.28%] [G loss: 1.968389]\n",
      "epoch:21 step:20385 [D loss: 0.602925, acc: 67.19%] [G loss: 2.034681]\n",
      "epoch:21 step:20386 [D loss: 0.630387, acc: 64.06%] [G loss: 1.952798]\n",
      "epoch:21 step:20387 [D loss: 0.655052, acc: 63.28%] [G loss: 1.814138]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:21 step:20388 [D loss: 0.642474, acc: 65.62%] [G loss: 1.938873]\n",
      "epoch:21 step:20389 [D loss: 0.676346, acc: 57.03%] [G loss: 1.863886]\n",
      "epoch:21 step:20390 [D loss: 0.587163, acc: 74.22%] [G loss: 1.869536]\n",
      "epoch:21 step:20391 [D loss: 0.634709, acc: 64.84%] [G loss: 1.986566]\n",
      "epoch:21 step:20392 [D loss: 0.640360, acc: 65.62%] [G loss: 1.897710]\n",
      "epoch:21 step:20393 [D loss: 0.704330, acc: 59.38%] [G loss: 1.768576]\n",
      "epoch:21 step:20394 [D loss: 0.631726, acc: 60.16%] [G loss: 1.860857]\n",
      "epoch:21 step:20395 [D loss: 0.689977, acc: 59.38%] [G loss: 1.825312]\n",
      "epoch:21 step:20396 [D loss: 0.662532, acc: 59.38%] [G loss: 2.013788]\n",
      "epoch:21 step:20397 [D loss: 0.626781, acc: 67.19%] [G loss: 2.082796]\n",
      "epoch:21 step:20398 [D loss: 0.612178, acc: 64.06%] [G loss: 2.086779]\n",
      "epoch:21 step:20399 [D loss: 0.704707, acc: 60.16%] [G loss: 1.797370]\n",
      "epoch:21 step:20400 [D loss: 0.657712, acc: 60.94%] [G loss: 1.855401]\n",
      "##############\n",
      "[2.35523405 1.48849233 6.14193344 4.79678052 3.66136433 5.49597318\n",
      " 4.38345442 4.75582745 4.42886523 3.70822543]\n",
      "##########\n",
      "epoch:21 step:20401 [D loss: 0.626289, acc: 63.28%] [G loss: 1.879049]\n",
      "epoch:21 step:20402 [D loss: 0.690246, acc: 58.59%] [G loss: 1.972995]\n",
      "epoch:21 step:20403 [D loss: 0.624883, acc: 63.28%] [G loss: 1.849586]\n",
      "epoch:21 step:20404 [D loss: 0.653559, acc: 56.25%] [G loss: 1.773583]\n",
      "epoch:21 step:20405 [D loss: 0.646485, acc: 66.41%] [G loss: 1.985743]\n",
      "epoch:21 step:20406 [D loss: 0.719315, acc: 50.00%] [G loss: 1.753322]\n",
      "epoch:21 step:20407 [D loss: 0.651359, acc: 57.81%] [G loss: 1.845896]\n",
      "epoch:21 step:20408 [D loss: 0.646929, acc: 61.72%] [G loss: 1.856914]\n",
      "epoch:21 step:20409 [D loss: 0.646660, acc: 60.94%] [G loss: 1.875924]\n",
      "epoch:21 step:20410 [D loss: 0.661994, acc: 61.72%] [G loss: 1.832360]\n",
      "epoch:21 step:20411 [D loss: 0.687436, acc: 56.25%] [G loss: 1.818619]\n",
      "epoch:21 step:20412 [D loss: 0.680882, acc: 59.38%] [G loss: 1.809577]\n",
      "epoch:21 step:20413 [D loss: 0.620010, acc: 66.41%] [G loss: 1.937891]\n",
      "epoch:21 step:20414 [D loss: 0.631065, acc: 63.28%] [G loss: 1.782305]\n",
      "epoch:21 step:20415 [D loss: 0.611642, acc: 67.19%] [G loss: 1.883010]\n",
      "epoch:21 step:20416 [D loss: 0.692983, acc: 54.69%] [G loss: 1.890419]\n",
      "epoch:21 step:20417 [D loss: 0.640759, acc: 62.50%] [G loss: 1.938563]\n",
      "epoch:21 step:20418 [D loss: 0.707034, acc: 54.69%] [G loss: 1.652922]\n",
      "epoch:21 step:20419 [D loss: 0.702937, acc: 57.03%] [G loss: 1.720463]\n",
      "epoch:21 step:20420 [D loss: 0.618082, acc: 69.53%] [G loss: 1.775416]\n",
      "epoch:21 step:20421 [D loss: 0.666068, acc: 60.94%] [G loss: 1.734296]\n",
      "epoch:21 step:20422 [D loss: 0.671066, acc: 60.16%] [G loss: 1.843524]\n",
      "epoch:21 step:20423 [D loss: 0.633967, acc: 64.84%] [G loss: 1.825118]\n",
      "epoch:21 step:20424 [D loss: 0.617052, acc: 66.41%] [G loss: 1.960744]\n",
      "epoch:21 step:20425 [D loss: 0.655920, acc: 60.94%] [G loss: 1.877717]\n",
      "epoch:21 step:20426 [D loss: 0.651214, acc: 57.03%] [G loss: 1.790295]\n",
      "epoch:21 step:20427 [D loss: 0.648756, acc: 59.38%] [G loss: 1.786870]\n",
      "epoch:21 step:20428 [D loss: 0.681172, acc: 60.16%] [G loss: 1.771331]\n",
      "epoch:21 step:20429 [D loss: 0.697812, acc: 55.47%] [G loss: 1.784853]\n",
      "epoch:21 step:20430 [D loss: 0.662085, acc: 60.16%] [G loss: 1.728458]\n",
      "epoch:21 step:20431 [D loss: 0.647074, acc: 63.28%] [G loss: 1.904757]\n",
      "epoch:21 step:20432 [D loss: 0.632891, acc: 67.19%] [G loss: 1.805010]\n",
      "epoch:21 step:20433 [D loss: 0.589365, acc: 68.75%] [G loss: 1.807859]\n",
      "epoch:21 step:20434 [D loss: 0.635590, acc: 61.72%] [G loss: 1.942556]\n",
      "epoch:21 step:20435 [D loss: 0.646592, acc: 57.81%] [G loss: 1.722632]\n",
      "epoch:21 step:20436 [D loss: 0.666633, acc: 60.94%] [G loss: 1.737060]\n",
      "epoch:21 step:20437 [D loss: 0.711951, acc: 51.56%] [G loss: 1.805248]\n",
      "epoch:21 step:20438 [D loss: 0.651170, acc: 63.28%] [G loss: 1.790143]\n",
      "epoch:21 step:20439 [D loss: 0.658619, acc: 57.81%] [G loss: 1.661182]\n",
      "epoch:21 step:20440 [D loss: 0.659832, acc: 55.47%] [G loss: 1.844726]\n",
      "epoch:21 step:20441 [D loss: 0.647992, acc: 61.72%] [G loss: 1.820022]\n",
      "epoch:21 step:20442 [D loss: 0.694110, acc: 56.25%] [G loss: 1.713715]\n",
      "epoch:21 step:20443 [D loss: 0.684000, acc: 57.81%] [G loss: 1.880584]\n",
      "epoch:21 step:20444 [D loss: 0.644927, acc: 62.50%] [G loss: 1.812784]\n",
      "epoch:21 step:20445 [D loss: 0.641448, acc: 61.72%] [G loss: 1.944301]\n",
      "epoch:21 step:20446 [D loss: 0.614053, acc: 64.84%] [G loss: 1.933177]\n",
      "epoch:21 step:20447 [D loss: 0.594725, acc: 72.66%] [G loss: 1.852395]\n",
      "epoch:21 step:20448 [D loss: 0.642744, acc: 62.50%] [G loss: 1.867141]\n",
      "epoch:21 step:20449 [D loss: 0.662434, acc: 58.59%] [G loss: 1.736025]\n",
      "epoch:21 step:20450 [D loss: 0.604101, acc: 65.62%] [G loss: 1.948069]\n",
      "epoch:21 step:20451 [D loss: 0.606339, acc: 66.41%] [G loss: 2.178808]\n",
      "epoch:21 step:20452 [D loss: 0.672478, acc: 57.81%] [G loss: 2.140833]\n",
      "epoch:21 step:20453 [D loss: 0.679838, acc: 60.94%] [G loss: 1.862962]\n",
      "epoch:21 step:20454 [D loss: 0.626182, acc: 60.94%] [G loss: 1.899458]\n",
      "epoch:21 step:20455 [D loss: 0.666346, acc: 58.59%] [G loss: 1.928665]\n",
      "epoch:21 step:20456 [D loss: 0.690746, acc: 57.81%] [G loss: 1.911202]\n",
      "epoch:21 step:20457 [D loss: 0.614638, acc: 68.75%] [G loss: 1.975921]\n",
      "epoch:21 step:20458 [D loss: 0.599814, acc: 66.41%] [G loss: 1.936821]\n",
      "epoch:21 step:20459 [D loss: 0.590559, acc: 69.53%] [G loss: 2.118770]\n",
      "epoch:21 step:20460 [D loss: 0.669387, acc: 59.38%] [G loss: 1.899585]\n",
      "epoch:21 step:20461 [D loss: 0.715229, acc: 53.12%] [G loss: 1.656715]\n",
      "epoch:21 step:20462 [D loss: 0.599311, acc: 66.41%] [G loss: 1.838656]\n",
      "epoch:21 step:20463 [D loss: 0.585573, acc: 71.09%] [G loss: 2.044007]\n",
      "epoch:21 step:20464 [D loss: 0.673535, acc: 62.50%] [G loss: 1.800101]\n",
      "epoch:21 step:20465 [D loss: 0.674866, acc: 60.94%] [G loss: 1.827168]\n",
      "epoch:21 step:20466 [D loss: 0.607546, acc: 71.09%] [G loss: 2.045530]\n",
      "epoch:21 step:20467 [D loss: 0.596802, acc: 66.41%] [G loss: 2.006222]\n",
      "epoch:21 step:20468 [D loss: 0.636800, acc: 64.84%] [G loss: 2.055570]\n",
      "epoch:21 step:20469 [D loss: 0.583097, acc: 71.09%] [G loss: 2.002605]\n",
      "epoch:21 step:20470 [D loss: 0.631685, acc: 61.72%] [G loss: 1.903259]\n",
      "epoch:21 step:20471 [D loss: 0.673733, acc: 57.81%] [G loss: 1.816950]\n",
      "epoch:21 step:20472 [D loss: 0.657163, acc: 57.81%] [G loss: 1.911706]\n",
      "epoch:21 step:20473 [D loss: 0.699054, acc: 53.12%] [G loss: 1.814072]\n",
      "epoch:21 step:20474 [D loss: 0.706653, acc: 57.81%] [G loss: 1.756194]\n",
      "epoch:21 step:20475 [D loss: 0.676299, acc: 60.16%] [G loss: 1.735867]\n",
      "epoch:21 step:20476 [D loss: 0.644918, acc: 62.50%] [G loss: 1.785528]\n",
      "epoch:21 step:20477 [D loss: 0.704963, acc: 55.47%] [G loss: 1.872764]\n",
      "epoch:21 step:20478 [D loss: 0.653312, acc: 57.03%] [G loss: 1.784759]\n",
      "epoch:21 step:20479 [D loss: 0.690588, acc: 59.38%] [G loss: 1.920944]\n",
      "epoch:21 step:20480 [D loss: 0.682199, acc: 56.25%] [G loss: 1.769692]\n",
      "epoch:21 step:20481 [D loss: 0.678159, acc: 58.59%] [G loss: 1.978492]\n",
      "epoch:21 step:20482 [D loss: 0.629951, acc: 61.72%] [G loss: 1.915056]\n",
      "epoch:21 step:20483 [D loss: 0.665477, acc: 51.56%] [G loss: 1.860047]\n",
      "epoch:21 step:20484 [D loss: 0.630996, acc: 60.94%] [G loss: 1.886908]\n",
      "epoch:21 step:20485 [D loss: 0.625069, acc: 64.84%] [G loss: 1.910492]\n",
      "epoch:21 step:20486 [D loss: 0.635147, acc: 64.84%] [G loss: 1.919516]\n",
      "epoch:21 step:20487 [D loss: 0.678482, acc: 58.59%] [G loss: 1.751182]\n",
      "epoch:21 step:20488 [D loss: 0.668063, acc: 58.59%] [G loss: 1.903470]\n",
      "epoch:21 step:20489 [D loss: 0.664093, acc: 60.16%] [G loss: 1.856825]\n",
      "epoch:21 step:20490 [D loss: 0.676153, acc: 60.94%] [G loss: 1.849360]\n",
      "epoch:21 step:20491 [D loss: 0.640851, acc: 64.06%] [G loss: 1.925721]\n",
      "epoch:21 step:20492 [D loss: 0.590695, acc: 74.22%] [G loss: 2.064470]\n",
      "epoch:21 step:20493 [D loss: 0.612292, acc: 64.06%] [G loss: 2.027548]\n",
      "epoch:21 step:20494 [D loss: 0.631319, acc: 64.84%] [G loss: 1.764767]\n",
      "epoch:21 step:20495 [D loss: 0.724471, acc: 53.12%] [G loss: 1.784581]\n",
      "epoch:21 step:20496 [D loss: 0.636397, acc: 64.06%] [G loss: 1.837240]\n",
      "epoch:21 step:20497 [D loss: 0.700548, acc: 56.25%] [G loss: 1.686093]\n",
      "epoch:21 step:20498 [D loss: 0.672220, acc: 57.03%] [G loss: 1.735869]\n",
      "epoch:21 step:20499 [D loss: 0.638467, acc: 63.28%] [G loss: 1.810068]\n",
      "epoch:21 step:20500 [D loss: 0.598576, acc: 67.19%] [G loss: 1.948894]\n",
      "epoch:21 step:20501 [D loss: 0.658708, acc: 61.72%] [G loss: 1.700108]\n",
      "epoch:21 step:20502 [D loss: 0.656029, acc: 62.50%] [G loss: 1.869121]\n",
      "epoch:21 step:20503 [D loss: 0.645804, acc: 57.81%] [G loss: 1.841242]\n",
      "epoch:21 step:20504 [D loss: 0.700849, acc: 58.59%] [G loss: 1.736023]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:21 step:20505 [D loss: 0.703437, acc: 52.34%] [G loss: 1.691236]\n",
      "epoch:21 step:20506 [D loss: 0.679681, acc: 56.25%] [G loss: 1.683148]\n",
      "epoch:21 step:20507 [D loss: 0.675205, acc: 57.81%] [G loss: 1.725458]\n",
      "epoch:21 step:20508 [D loss: 0.650047, acc: 62.50%] [G loss: 1.774173]\n",
      "epoch:21 step:20509 [D loss: 0.628205, acc: 62.50%] [G loss: 1.899554]\n",
      "epoch:21 step:20510 [D loss: 0.629132, acc: 64.84%] [G loss: 1.827457]\n",
      "epoch:21 step:20511 [D loss: 0.687970, acc: 57.81%] [G loss: 1.870318]\n",
      "epoch:21 step:20512 [D loss: 0.644894, acc: 64.06%] [G loss: 1.765753]\n",
      "epoch:21 step:20513 [D loss: 0.607851, acc: 68.75%] [G loss: 1.861473]\n",
      "epoch:21 step:20514 [D loss: 0.678797, acc: 58.59%] [G loss: 1.850644]\n",
      "epoch:21 step:20515 [D loss: 0.627094, acc: 63.28%] [G loss: 1.786730]\n",
      "epoch:21 step:20516 [D loss: 0.666924, acc: 57.03%] [G loss: 1.789630]\n",
      "epoch:21 step:20517 [D loss: 0.663484, acc: 60.16%] [G loss: 1.879729]\n",
      "epoch:21 step:20518 [D loss: 0.626799, acc: 60.94%] [G loss: 1.945781]\n",
      "epoch:21 step:20519 [D loss: 0.565389, acc: 72.66%] [G loss: 1.935148]\n",
      "epoch:21 step:20520 [D loss: 0.635811, acc: 62.50%] [G loss: 1.796594]\n",
      "epoch:21 step:20521 [D loss: 0.657540, acc: 60.16%] [G loss: 1.910820]\n",
      "epoch:21 step:20522 [D loss: 0.654079, acc: 60.16%] [G loss: 1.980492]\n",
      "epoch:21 step:20523 [D loss: 0.658579, acc: 64.84%] [G loss: 1.756949]\n",
      "epoch:21 step:20524 [D loss: 0.602961, acc: 66.41%] [G loss: 1.880582]\n",
      "epoch:21 step:20525 [D loss: 0.624971, acc: 59.38%] [G loss: 1.917841]\n",
      "epoch:21 step:20526 [D loss: 0.633429, acc: 60.94%] [G loss: 1.940738]\n",
      "epoch:21 step:20527 [D loss: 0.692265, acc: 55.47%] [G loss: 1.755922]\n",
      "epoch:21 step:20528 [D loss: 0.653379, acc: 61.72%] [G loss: 1.842999]\n",
      "epoch:21 step:20529 [D loss: 0.655882, acc: 62.50%] [G loss: 1.898237]\n",
      "epoch:21 step:20530 [D loss: 0.627164, acc: 64.84%] [G loss: 1.918634]\n",
      "epoch:21 step:20531 [D loss: 0.624254, acc: 62.50%] [G loss: 1.943136]\n",
      "epoch:21 step:20532 [D loss: 0.657952, acc: 58.59%] [G loss: 1.848607]\n",
      "epoch:21 step:20533 [D loss: 0.687470, acc: 57.81%] [G loss: 1.820828]\n",
      "epoch:21 step:20534 [D loss: 0.650412, acc: 64.84%] [G loss: 1.865796]\n",
      "epoch:21 step:20535 [D loss: 0.720742, acc: 53.12%] [G loss: 1.745954]\n",
      "epoch:21 step:20536 [D loss: 0.665141, acc: 57.03%] [G loss: 1.841365]\n",
      "epoch:21 step:20537 [D loss: 0.698691, acc: 59.38%] [G loss: 1.807508]\n",
      "epoch:21 step:20538 [D loss: 0.634627, acc: 64.06%] [G loss: 1.823312]\n",
      "epoch:21 step:20539 [D loss: 0.693039, acc: 56.25%] [G loss: 1.804217]\n",
      "epoch:21 step:20540 [D loss: 0.688941, acc: 55.47%] [G loss: 1.812346]\n",
      "epoch:21 step:20541 [D loss: 0.640875, acc: 62.50%] [G loss: 1.815743]\n",
      "epoch:21 step:20542 [D loss: 0.643082, acc: 67.19%] [G loss: 1.841298]\n",
      "epoch:21 step:20543 [D loss: 0.634504, acc: 63.28%] [G loss: 1.876099]\n",
      "epoch:21 step:20544 [D loss: 0.682699, acc: 52.34%] [G loss: 1.777219]\n",
      "epoch:21 step:20545 [D loss: 0.647817, acc: 60.94%] [G loss: 1.814956]\n",
      "epoch:21 step:20546 [D loss: 0.689874, acc: 49.22%] [G loss: 1.848624]\n",
      "epoch:21 step:20547 [D loss: 0.707784, acc: 51.56%] [G loss: 1.797228]\n",
      "epoch:21 step:20548 [D loss: 0.639199, acc: 60.16%] [G loss: 1.862187]\n",
      "epoch:21 step:20549 [D loss: 0.640410, acc: 63.28%] [G loss: 1.812069]\n",
      "epoch:21 step:20550 [D loss: 0.619623, acc: 65.62%] [G loss: 1.714868]\n",
      "epoch:21 step:20551 [D loss: 0.657666, acc: 60.16%] [G loss: 1.842594]\n",
      "epoch:21 step:20552 [D loss: 0.648705, acc: 63.28%] [G loss: 1.933787]\n",
      "epoch:21 step:20553 [D loss: 0.623833, acc: 64.06%] [G loss: 1.840079]\n",
      "epoch:21 step:20554 [D loss: 0.693865, acc: 57.03%] [G loss: 1.746566]\n",
      "epoch:21 step:20555 [D loss: 0.569149, acc: 72.66%] [G loss: 1.952784]\n",
      "epoch:21 step:20556 [D loss: 0.658439, acc: 60.16%] [G loss: 1.899685]\n",
      "epoch:21 step:20557 [D loss: 0.696672, acc: 57.03%] [G loss: 1.831799]\n",
      "epoch:21 step:20558 [D loss: 0.712084, acc: 56.25%] [G loss: 1.778586]\n",
      "epoch:21 step:20559 [D loss: 0.626450, acc: 65.62%] [G loss: 1.958331]\n",
      "epoch:21 step:20560 [D loss: 0.670523, acc: 57.81%] [G loss: 1.884978]\n",
      "epoch:21 step:20561 [D loss: 0.604988, acc: 66.41%] [G loss: 1.972607]\n",
      "epoch:21 step:20562 [D loss: 0.686986, acc: 60.16%] [G loss: 1.758429]\n",
      "epoch:21 step:20563 [D loss: 0.667382, acc: 61.72%] [G loss: 1.917267]\n",
      "epoch:21 step:20564 [D loss: 0.638907, acc: 64.06%] [G loss: 1.777352]\n",
      "epoch:21 step:20565 [D loss: 0.664477, acc: 61.72%] [G loss: 1.903351]\n",
      "epoch:21 step:20566 [D loss: 0.650750, acc: 60.94%] [G loss: 1.913203]\n",
      "epoch:21 step:20567 [D loss: 0.596027, acc: 67.19%] [G loss: 1.976736]\n",
      "epoch:21 step:20568 [D loss: 0.712867, acc: 63.28%] [G loss: 1.817465]\n",
      "epoch:21 step:20569 [D loss: 0.641435, acc: 67.97%] [G loss: 1.988270]\n",
      "epoch:21 step:20570 [D loss: 0.679806, acc: 61.72%] [G loss: 1.867445]\n",
      "epoch:21 step:20571 [D loss: 0.623088, acc: 61.72%] [G loss: 1.950150]\n",
      "epoch:21 step:20572 [D loss: 0.634647, acc: 64.06%] [G loss: 1.677265]\n",
      "epoch:21 step:20573 [D loss: 0.662975, acc: 57.81%] [G loss: 1.765862]\n",
      "epoch:21 step:20574 [D loss: 0.637147, acc: 66.41%] [G loss: 1.862361]\n",
      "epoch:21 step:20575 [D loss: 0.678302, acc: 56.25%] [G loss: 1.990849]\n",
      "epoch:21 step:20576 [D loss: 0.614903, acc: 64.84%] [G loss: 1.907998]\n",
      "epoch:21 step:20577 [D loss: 0.631864, acc: 62.50%] [G loss: 1.937393]\n",
      "epoch:21 step:20578 [D loss: 0.653006, acc: 65.62%] [G loss: 2.012161]\n",
      "epoch:21 step:20579 [D loss: 0.627461, acc: 60.94%] [G loss: 1.961188]\n",
      "epoch:21 step:20580 [D loss: 0.624982, acc: 65.62%] [G loss: 1.986778]\n",
      "epoch:21 step:20581 [D loss: 0.644686, acc: 58.59%] [G loss: 2.019492]\n",
      "epoch:21 step:20582 [D loss: 0.631537, acc: 64.06%] [G loss: 1.967224]\n",
      "epoch:21 step:20583 [D loss: 0.609160, acc: 67.19%] [G loss: 2.134658]\n",
      "epoch:21 step:20584 [D loss: 0.630445, acc: 64.06%] [G loss: 2.115366]\n",
      "epoch:21 step:20585 [D loss: 0.667312, acc: 60.94%] [G loss: 1.949165]\n",
      "epoch:21 step:20586 [D loss: 0.612823, acc: 65.62%] [G loss: 1.952025]\n",
      "epoch:21 step:20587 [D loss: 0.649301, acc: 64.06%] [G loss: 1.828496]\n",
      "epoch:21 step:20588 [D loss: 0.577178, acc: 68.75%] [G loss: 1.839624]\n",
      "epoch:21 step:20589 [D loss: 0.622634, acc: 65.62%] [G loss: 1.985935]\n",
      "epoch:21 step:20590 [D loss: 0.652300, acc: 64.84%] [G loss: 2.017437]\n",
      "epoch:21 step:20591 [D loss: 0.694704, acc: 54.69%] [G loss: 1.904460]\n",
      "epoch:21 step:20592 [D loss: 0.655281, acc: 59.38%] [G loss: 1.856308]\n",
      "epoch:21 step:20593 [D loss: 0.626181, acc: 61.72%] [G loss: 2.086226]\n",
      "epoch:21 step:20594 [D loss: 0.628909, acc: 67.97%] [G loss: 1.911475]\n",
      "epoch:21 step:20595 [D loss: 0.601622, acc: 67.19%] [G loss: 2.135251]\n",
      "epoch:21 step:20596 [D loss: 0.622465, acc: 68.75%] [G loss: 2.208599]\n",
      "epoch:21 step:20597 [D loss: 0.751027, acc: 52.34%] [G loss: 1.947140]\n",
      "epoch:21 step:20598 [D loss: 0.619171, acc: 67.97%] [G loss: 2.100363]\n",
      "epoch:21 step:20599 [D loss: 0.593147, acc: 65.62%] [G loss: 1.945994]\n",
      "epoch:21 step:20600 [D loss: 0.624511, acc: 66.41%] [G loss: 2.067708]\n",
      "##############\n",
      "[2.48112201 1.48921842 6.47323844 4.73738035 3.5480145  5.6552597\n",
      " 4.43663973 4.69957664 4.66026089 3.63373326]\n",
      "##########\n",
      "epoch:21 step:20601 [D loss: 0.641825, acc: 61.72%] [G loss: 2.149937]\n",
      "epoch:21 step:20602 [D loss: 0.656392, acc: 61.72%] [G loss: 1.986439]\n",
      "epoch:21 step:20603 [D loss: 0.663923, acc: 60.94%] [G loss: 2.111145]\n",
      "epoch:21 step:20604 [D loss: 0.595563, acc: 67.97%] [G loss: 2.125414]\n",
      "epoch:21 step:20605 [D loss: 0.754276, acc: 55.47%] [G loss: 1.776638]\n",
      "epoch:21 step:20606 [D loss: 0.687637, acc: 52.34%] [G loss: 1.890153]\n",
      "epoch:21 step:20607 [D loss: 0.647309, acc: 62.50%] [G loss: 2.077546]\n",
      "epoch:21 step:20608 [D loss: 0.663463, acc: 60.94%] [G loss: 2.031519]\n",
      "epoch:21 step:20609 [D loss: 0.661135, acc: 62.50%] [G loss: 1.854313]\n",
      "epoch:21 step:20610 [D loss: 0.657350, acc: 63.28%] [G loss: 1.929803]\n",
      "epoch:21 step:20611 [D loss: 0.643382, acc: 60.94%] [G loss: 1.991326]\n",
      "epoch:21 step:20612 [D loss: 0.676628, acc: 64.06%] [G loss: 2.074919]\n",
      "epoch:21 step:20613 [D loss: 0.585261, acc: 70.31%] [G loss: 2.090614]\n",
      "epoch:21 step:20614 [D loss: 0.549444, acc: 78.12%] [G loss: 2.327598]\n",
      "epoch:22 step:20615 [D loss: 0.656810, acc: 60.94%] [G loss: 1.862772]\n",
      "epoch:22 step:20616 [D loss: 0.609482, acc: 64.06%] [G loss: 1.881327]\n",
      "epoch:22 step:20617 [D loss: 0.661878, acc: 58.59%] [G loss: 1.912165]\n",
      "epoch:22 step:20618 [D loss: 0.636534, acc: 61.72%] [G loss: 1.929104]\n",
      "epoch:22 step:20619 [D loss: 0.647713, acc: 66.41%] [G loss: 1.953598]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:22 step:20620 [D loss: 0.596015, acc: 67.97%] [G loss: 2.074013]\n",
      "epoch:22 step:20621 [D loss: 0.614972, acc: 66.41%] [G loss: 2.111810]\n",
      "epoch:22 step:20622 [D loss: 0.614324, acc: 67.97%] [G loss: 1.968089]\n",
      "epoch:22 step:20623 [D loss: 0.602172, acc: 67.97%] [G loss: 1.892598]\n",
      "epoch:22 step:20624 [D loss: 0.636073, acc: 65.62%] [G loss: 2.184926]\n",
      "epoch:22 step:20625 [D loss: 0.643048, acc: 60.94%] [G loss: 1.982055]\n",
      "epoch:22 step:20626 [D loss: 0.636463, acc: 59.38%] [G loss: 1.964576]\n",
      "epoch:22 step:20627 [D loss: 0.631364, acc: 67.19%] [G loss: 2.011822]\n",
      "epoch:22 step:20628 [D loss: 0.624781, acc: 65.62%] [G loss: 1.898380]\n",
      "epoch:22 step:20629 [D loss: 0.547992, acc: 77.34%] [G loss: 2.065616]\n",
      "epoch:22 step:20630 [D loss: 0.607636, acc: 67.97%] [G loss: 2.030294]\n",
      "epoch:22 step:20631 [D loss: 0.701884, acc: 53.91%] [G loss: 2.011627]\n",
      "epoch:22 step:20632 [D loss: 0.654794, acc: 63.28%] [G loss: 1.944548]\n",
      "epoch:22 step:20633 [D loss: 0.655509, acc: 60.94%] [G loss: 1.896542]\n",
      "epoch:22 step:20634 [D loss: 0.712014, acc: 53.91%] [G loss: 1.689340]\n",
      "epoch:22 step:20635 [D loss: 0.675945, acc: 61.72%] [G loss: 1.853335]\n",
      "epoch:22 step:20636 [D loss: 0.670244, acc: 60.16%] [G loss: 1.906178]\n",
      "epoch:22 step:20637 [D loss: 0.664321, acc: 65.62%] [G loss: 2.021406]\n",
      "epoch:22 step:20638 [D loss: 0.606915, acc: 71.09%] [G loss: 1.990740]\n",
      "epoch:22 step:20639 [D loss: 0.598009, acc: 67.19%] [G loss: 2.088737]\n",
      "epoch:22 step:20640 [D loss: 0.618437, acc: 67.19%] [G loss: 1.809586]\n",
      "epoch:22 step:20641 [D loss: 0.621088, acc: 64.06%] [G loss: 1.861062]\n",
      "epoch:22 step:20642 [D loss: 0.700900, acc: 58.59%] [G loss: 1.836660]\n",
      "epoch:22 step:20643 [D loss: 0.603927, acc: 64.84%] [G loss: 1.810452]\n",
      "epoch:22 step:20644 [D loss: 0.673108, acc: 55.47%] [G loss: 1.887659]\n",
      "epoch:22 step:20645 [D loss: 0.681169, acc: 59.38%] [G loss: 1.769436]\n",
      "epoch:22 step:20646 [D loss: 0.650941, acc: 62.50%] [G loss: 1.896177]\n",
      "epoch:22 step:20647 [D loss: 0.690001, acc: 50.78%] [G loss: 1.783970]\n",
      "epoch:22 step:20648 [D loss: 0.652587, acc: 66.41%] [G loss: 1.897940]\n",
      "epoch:22 step:20649 [D loss: 0.675730, acc: 62.50%] [G loss: 1.862970]\n",
      "epoch:22 step:20650 [D loss: 0.652185, acc: 62.50%] [G loss: 2.071502]\n",
      "epoch:22 step:20651 [D loss: 0.595900, acc: 71.88%] [G loss: 2.007649]\n",
      "epoch:22 step:20652 [D loss: 0.589086, acc: 71.09%] [G loss: 2.020797]\n",
      "epoch:22 step:20653 [D loss: 0.634143, acc: 63.28%] [G loss: 1.950482]\n",
      "epoch:22 step:20654 [D loss: 0.588430, acc: 66.41%] [G loss: 2.096110]\n",
      "epoch:22 step:20655 [D loss: 0.669296, acc: 62.50%] [G loss: 1.812858]\n",
      "epoch:22 step:20656 [D loss: 0.637764, acc: 63.28%] [G loss: 1.957257]\n",
      "epoch:22 step:20657 [D loss: 0.634707, acc: 60.16%] [G loss: 1.829916]\n",
      "epoch:22 step:20658 [D loss: 0.705452, acc: 59.38%] [G loss: 1.710790]\n",
      "epoch:22 step:20659 [D loss: 0.651630, acc: 63.28%] [G loss: 1.861173]\n",
      "epoch:22 step:20660 [D loss: 0.660945, acc: 64.06%] [G loss: 1.967820]\n",
      "epoch:22 step:20661 [D loss: 0.590428, acc: 70.31%] [G loss: 1.833140]\n",
      "epoch:22 step:20662 [D loss: 0.616035, acc: 67.19%] [G loss: 1.884530]\n",
      "epoch:22 step:20663 [D loss: 0.653329, acc: 62.50%] [G loss: 1.861685]\n",
      "epoch:22 step:20664 [D loss: 0.602580, acc: 69.53%] [G loss: 1.954031]\n",
      "epoch:22 step:20665 [D loss: 0.641429, acc: 63.28%] [G loss: 1.977683]\n",
      "epoch:22 step:20666 [D loss: 0.668713, acc: 60.94%] [G loss: 1.958492]\n",
      "epoch:22 step:20667 [D loss: 0.598387, acc: 67.19%] [G loss: 1.860042]\n",
      "epoch:22 step:20668 [D loss: 0.591461, acc: 70.31%] [G loss: 1.964631]\n",
      "epoch:22 step:20669 [D loss: 0.637453, acc: 64.06%] [G loss: 2.111293]\n",
      "epoch:22 step:20670 [D loss: 0.631958, acc: 64.84%] [G loss: 2.002877]\n",
      "epoch:22 step:20671 [D loss: 0.689162, acc: 58.59%] [G loss: 2.020600]\n",
      "epoch:22 step:20672 [D loss: 0.622131, acc: 62.50%] [G loss: 1.928538]\n",
      "epoch:22 step:20673 [D loss: 0.670476, acc: 64.06%] [G loss: 1.915097]\n",
      "epoch:22 step:20674 [D loss: 0.657426, acc: 57.81%] [G loss: 1.865122]\n",
      "epoch:22 step:20675 [D loss: 0.662232, acc: 59.38%] [G loss: 1.877608]\n",
      "epoch:22 step:20676 [D loss: 0.667099, acc: 59.38%] [G loss: 1.876379]\n",
      "epoch:22 step:20677 [D loss: 0.653928, acc: 63.28%] [G loss: 1.900062]\n",
      "epoch:22 step:20678 [D loss: 0.591670, acc: 62.50%] [G loss: 1.863171]\n",
      "epoch:22 step:20679 [D loss: 0.659228, acc: 62.50%] [G loss: 1.924484]\n",
      "epoch:22 step:20680 [D loss: 0.597067, acc: 68.75%] [G loss: 1.938176]\n",
      "epoch:22 step:20681 [D loss: 0.667097, acc: 64.84%] [G loss: 1.883153]\n",
      "epoch:22 step:20682 [D loss: 0.662460, acc: 59.38%] [G loss: 1.725924]\n",
      "epoch:22 step:20683 [D loss: 0.665398, acc: 60.16%] [G loss: 1.938454]\n",
      "epoch:22 step:20684 [D loss: 0.609154, acc: 65.62%] [G loss: 1.883375]\n",
      "epoch:22 step:20685 [D loss: 0.673739, acc: 59.38%] [G loss: 1.659692]\n",
      "epoch:22 step:20686 [D loss: 0.649866, acc: 62.50%] [G loss: 1.873680]\n",
      "epoch:22 step:20687 [D loss: 0.672723, acc: 58.59%] [G loss: 1.896841]\n",
      "epoch:22 step:20688 [D loss: 0.701793, acc: 51.56%] [G loss: 1.867250]\n",
      "epoch:22 step:20689 [D loss: 0.613558, acc: 63.28%] [G loss: 2.040522]\n",
      "epoch:22 step:20690 [D loss: 0.593762, acc: 73.44%] [G loss: 1.997890]\n",
      "epoch:22 step:20691 [D loss: 0.601339, acc: 66.41%] [G loss: 1.997880]\n",
      "epoch:22 step:20692 [D loss: 0.677418, acc: 56.25%] [G loss: 1.667455]\n",
      "epoch:22 step:20693 [D loss: 0.676513, acc: 56.25%] [G loss: 1.866849]\n",
      "epoch:22 step:20694 [D loss: 0.713198, acc: 55.47%] [G loss: 1.827199]\n",
      "epoch:22 step:20695 [D loss: 0.707485, acc: 55.47%] [G loss: 1.632085]\n",
      "epoch:22 step:20696 [D loss: 0.611100, acc: 67.97%] [G loss: 1.824506]\n",
      "epoch:22 step:20697 [D loss: 0.635991, acc: 63.28%] [G loss: 1.777194]\n",
      "epoch:22 step:20698 [D loss: 0.640009, acc: 63.28%] [G loss: 1.928084]\n",
      "epoch:22 step:20699 [D loss: 0.703427, acc: 51.56%] [G loss: 1.834022]\n",
      "epoch:22 step:20700 [D loss: 0.667082, acc: 58.59%] [G loss: 1.680482]\n",
      "epoch:22 step:20701 [D loss: 0.627466, acc: 61.72%] [G loss: 1.791243]\n",
      "epoch:22 step:20702 [D loss: 0.667626, acc: 59.38%] [G loss: 1.910187]\n",
      "epoch:22 step:20703 [D loss: 0.637410, acc: 61.72%] [G loss: 1.845176]\n",
      "epoch:22 step:20704 [D loss: 0.684619, acc: 60.16%] [G loss: 1.928088]\n",
      "epoch:22 step:20705 [D loss: 0.656500, acc: 61.72%] [G loss: 1.774145]\n",
      "epoch:22 step:20706 [D loss: 0.646428, acc: 62.50%] [G loss: 1.832594]\n",
      "epoch:22 step:20707 [D loss: 0.650695, acc: 60.16%] [G loss: 1.864739]\n",
      "epoch:22 step:20708 [D loss: 0.667917, acc: 58.59%] [G loss: 1.913343]\n",
      "epoch:22 step:20709 [D loss: 0.655655, acc: 64.84%] [G loss: 1.802803]\n",
      "epoch:22 step:20710 [D loss: 0.659083, acc: 63.28%] [G loss: 1.830917]\n",
      "epoch:22 step:20711 [D loss: 0.593998, acc: 75.00%] [G loss: 1.852977]\n",
      "epoch:22 step:20712 [D loss: 0.686889, acc: 57.81%] [G loss: 1.769609]\n",
      "epoch:22 step:20713 [D loss: 0.659449, acc: 64.84%] [G loss: 1.785378]\n",
      "epoch:22 step:20714 [D loss: 0.630030, acc: 63.28%] [G loss: 1.885102]\n",
      "epoch:22 step:20715 [D loss: 0.667132, acc: 63.28%] [G loss: 1.913784]\n",
      "epoch:22 step:20716 [D loss: 0.631730, acc: 65.62%] [G loss: 1.834090]\n",
      "epoch:22 step:20717 [D loss: 0.624650, acc: 66.41%] [G loss: 1.920854]\n",
      "epoch:22 step:20718 [D loss: 0.634348, acc: 57.81%] [G loss: 1.829358]\n",
      "epoch:22 step:20719 [D loss: 0.670189, acc: 61.72%] [G loss: 1.876148]\n",
      "epoch:22 step:20720 [D loss: 0.611475, acc: 67.97%] [G loss: 2.040497]\n",
      "epoch:22 step:20721 [D loss: 0.626409, acc: 66.41%] [G loss: 2.167561]\n",
      "epoch:22 step:20722 [D loss: 0.645548, acc: 65.62%] [G loss: 1.806815]\n",
      "epoch:22 step:20723 [D loss: 0.686662, acc: 59.38%] [G loss: 1.758750]\n",
      "epoch:22 step:20724 [D loss: 0.671559, acc: 56.25%] [G loss: 1.841742]\n",
      "epoch:22 step:20725 [D loss: 0.629480, acc: 63.28%] [G loss: 1.899019]\n",
      "epoch:22 step:20726 [D loss: 0.637809, acc: 63.28%] [G loss: 1.877086]\n",
      "epoch:22 step:20727 [D loss: 0.644224, acc: 66.41%] [G loss: 1.897141]\n",
      "epoch:22 step:20728 [D loss: 0.572200, acc: 69.53%] [G loss: 2.044388]\n",
      "epoch:22 step:20729 [D loss: 0.587241, acc: 69.53%] [G loss: 2.252843]\n",
      "epoch:22 step:20730 [D loss: 0.551371, acc: 72.66%] [G loss: 2.201113]\n",
      "epoch:22 step:20731 [D loss: 0.611620, acc: 68.75%] [G loss: 2.056863]\n",
      "epoch:22 step:20732 [D loss: 0.628585, acc: 63.28%] [G loss: 2.065090]\n",
      "epoch:22 step:20733 [D loss: 0.582738, acc: 65.62%] [G loss: 2.273757]\n",
      "epoch:22 step:20734 [D loss: 0.619253, acc: 65.62%] [G loss: 2.094691]\n",
      "epoch:22 step:20735 [D loss: 0.659247, acc: 59.38%] [G loss: 1.997432]\n",
      "epoch:22 step:20736 [D loss: 0.593901, acc: 69.53%] [G loss: 2.142699]\n",
      "epoch:22 step:20737 [D loss: 0.618230, acc: 67.19%] [G loss: 1.936585]\n",
      "epoch:22 step:20738 [D loss: 0.733218, acc: 50.78%] [G loss: 1.884027]\n",
      "epoch:22 step:20739 [D loss: 0.689152, acc: 51.56%] [G loss: 1.851538]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:22 step:20740 [D loss: 0.602594, acc: 67.19%] [G loss: 1.884894]\n",
      "epoch:22 step:20741 [D loss: 0.642264, acc: 61.72%] [G loss: 1.778470]\n",
      "epoch:22 step:20742 [D loss: 0.667302, acc: 59.38%] [G loss: 1.919942]\n",
      "epoch:22 step:20743 [D loss: 0.719412, acc: 50.78%] [G loss: 1.852402]\n",
      "epoch:22 step:20744 [D loss: 0.677974, acc: 59.38%] [G loss: 1.930350]\n",
      "epoch:22 step:20745 [D loss: 0.655724, acc: 60.16%] [G loss: 1.877093]\n",
      "epoch:22 step:20746 [D loss: 0.619767, acc: 65.62%] [G loss: 1.726685]\n",
      "epoch:22 step:20747 [D loss: 0.686881, acc: 55.47%] [G loss: 1.704877]\n",
      "epoch:22 step:20748 [D loss: 0.622697, acc: 66.41%] [G loss: 1.917943]\n",
      "epoch:22 step:20749 [D loss: 0.673049, acc: 63.28%] [G loss: 1.891273]\n",
      "epoch:22 step:20750 [D loss: 0.654183, acc: 64.84%] [G loss: 1.859642]\n",
      "epoch:22 step:20751 [D loss: 0.672436, acc: 57.03%] [G loss: 1.799967]\n",
      "epoch:22 step:20752 [D loss: 0.592142, acc: 69.53%] [G loss: 1.950758]\n",
      "epoch:22 step:20753 [D loss: 0.690741, acc: 51.56%] [G loss: 1.828895]\n",
      "epoch:22 step:20754 [D loss: 0.676532, acc: 57.03%] [G loss: 1.819233]\n",
      "epoch:22 step:20755 [D loss: 0.689195, acc: 60.94%] [G loss: 1.731270]\n",
      "epoch:22 step:20756 [D loss: 0.689208, acc: 51.56%] [G loss: 1.778579]\n",
      "epoch:22 step:20757 [D loss: 0.684174, acc: 50.00%] [G loss: 1.873990]\n",
      "epoch:22 step:20758 [D loss: 0.634412, acc: 63.28%] [G loss: 1.949814]\n",
      "epoch:22 step:20759 [D loss: 0.661749, acc: 59.38%] [G loss: 1.765922]\n",
      "epoch:22 step:20760 [D loss: 0.668933, acc: 66.41%] [G loss: 1.818966]\n",
      "epoch:22 step:20761 [D loss: 0.661072, acc: 62.50%] [G loss: 1.667899]\n",
      "epoch:22 step:20762 [D loss: 0.685966, acc: 57.03%] [G loss: 1.763124]\n",
      "epoch:22 step:20763 [D loss: 0.619174, acc: 64.84%] [G loss: 1.781510]\n",
      "epoch:22 step:20764 [D loss: 0.639173, acc: 65.62%] [G loss: 1.904348]\n",
      "epoch:22 step:20765 [D loss: 0.635385, acc: 67.19%] [G loss: 1.970483]\n",
      "epoch:22 step:20766 [D loss: 0.646045, acc: 64.06%] [G loss: 1.971502]\n",
      "epoch:22 step:20767 [D loss: 0.678485, acc: 57.03%] [G loss: 1.874093]\n",
      "epoch:22 step:20768 [D loss: 0.671163, acc: 60.16%] [G loss: 2.007603]\n",
      "epoch:22 step:20769 [D loss: 0.633275, acc: 62.50%] [G loss: 1.826806]\n",
      "epoch:22 step:20770 [D loss: 0.618125, acc: 64.84%] [G loss: 2.049319]\n",
      "epoch:22 step:20771 [D loss: 0.599503, acc: 67.97%] [G loss: 1.895899]\n",
      "epoch:22 step:20772 [D loss: 0.674679, acc: 55.47%] [G loss: 1.809118]\n",
      "epoch:22 step:20773 [D loss: 0.688008, acc: 55.47%] [G loss: 1.892175]\n",
      "epoch:22 step:20774 [D loss: 0.687224, acc: 60.16%] [G loss: 1.709875]\n",
      "epoch:22 step:20775 [D loss: 0.681651, acc: 53.12%] [G loss: 1.809088]\n",
      "epoch:22 step:20776 [D loss: 0.661340, acc: 60.94%] [G loss: 1.834078]\n",
      "epoch:22 step:20777 [D loss: 0.688811, acc: 51.56%] [G loss: 1.864239]\n",
      "epoch:22 step:20778 [D loss: 0.633088, acc: 63.28%] [G loss: 1.834751]\n",
      "epoch:22 step:20779 [D loss: 0.643129, acc: 62.50%] [G loss: 1.759457]\n",
      "epoch:22 step:20780 [D loss: 0.617486, acc: 67.19%] [G loss: 1.825384]\n",
      "epoch:22 step:20781 [D loss: 0.661615, acc: 57.81%] [G loss: 1.890965]\n",
      "epoch:22 step:20782 [D loss: 0.624038, acc: 67.97%] [G loss: 1.844539]\n",
      "epoch:22 step:20783 [D loss: 0.656042, acc: 60.94%] [G loss: 1.856572]\n",
      "epoch:22 step:20784 [D loss: 0.631045, acc: 62.50%] [G loss: 1.846588]\n",
      "epoch:22 step:20785 [D loss: 0.623110, acc: 64.06%] [G loss: 1.939492]\n",
      "epoch:22 step:20786 [D loss: 0.683921, acc: 53.12%] [G loss: 1.848404]\n",
      "epoch:22 step:20787 [D loss: 0.663637, acc: 59.38%] [G loss: 1.803498]\n",
      "epoch:22 step:20788 [D loss: 0.623925, acc: 67.97%] [G loss: 1.751464]\n",
      "epoch:22 step:20789 [D loss: 0.683878, acc: 54.69%] [G loss: 1.815176]\n",
      "epoch:22 step:20790 [D loss: 0.662877, acc: 64.06%] [G loss: 1.807287]\n",
      "epoch:22 step:20791 [D loss: 0.677853, acc: 60.94%] [G loss: 1.765316]\n",
      "epoch:22 step:20792 [D loss: 0.646035, acc: 60.16%] [G loss: 1.831277]\n",
      "epoch:22 step:20793 [D loss: 0.650423, acc: 60.16%] [G loss: 1.746401]\n",
      "epoch:22 step:20794 [D loss: 0.620138, acc: 64.06%] [G loss: 1.879742]\n",
      "epoch:22 step:20795 [D loss: 0.669385, acc: 57.03%] [G loss: 1.882677]\n",
      "epoch:22 step:20796 [D loss: 0.722851, acc: 53.91%] [G loss: 1.879035]\n",
      "epoch:22 step:20797 [D loss: 0.688345, acc: 57.03%] [G loss: 1.876907]\n",
      "epoch:22 step:20798 [D loss: 0.642670, acc: 66.41%] [G loss: 1.930614]\n",
      "epoch:22 step:20799 [D loss: 0.686328, acc: 53.12%] [G loss: 1.772322]\n",
      "epoch:22 step:20800 [D loss: 0.612913, acc: 69.53%] [G loss: 1.894223]\n",
      "##############\n",
      "[2.54030357 1.41405085 6.26314274 4.63838515 3.61324481 5.64069799\n",
      " 4.30004083 4.69546251 4.50813652 3.55935012]\n",
      "##########\n",
      "epoch:22 step:20801 [D loss: 0.624581, acc: 63.28%] [G loss: 1.859153]\n",
      "epoch:22 step:20802 [D loss: 0.690093, acc: 58.59%] [G loss: 1.744185]\n",
      "epoch:22 step:20803 [D loss: 0.682814, acc: 58.59%] [G loss: 1.781551]\n",
      "epoch:22 step:20804 [D loss: 0.683003, acc: 54.69%] [G loss: 1.844220]\n",
      "epoch:22 step:20805 [D loss: 0.633740, acc: 60.94%] [G loss: 1.959214]\n",
      "epoch:22 step:20806 [D loss: 0.645049, acc: 60.94%] [G loss: 1.892179]\n",
      "epoch:22 step:20807 [D loss: 0.663202, acc: 60.16%] [G loss: 1.844643]\n",
      "epoch:22 step:20808 [D loss: 0.626045, acc: 60.94%] [G loss: 2.004848]\n",
      "epoch:22 step:20809 [D loss: 0.623468, acc: 60.94%] [G loss: 1.716445]\n",
      "epoch:22 step:20810 [D loss: 0.655196, acc: 57.81%] [G loss: 1.852180]\n",
      "epoch:22 step:20811 [D loss: 0.627008, acc: 67.97%] [G loss: 1.937291]\n",
      "epoch:22 step:20812 [D loss: 0.610247, acc: 64.06%] [G loss: 1.993969]\n",
      "epoch:22 step:20813 [D loss: 0.674521, acc: 59.38%] [G loss: 1.828140]\n",
      "epoch:22 step:20814 [D loss: 0.684582, acc: 60.16%] [G loss: 1.721570]\n",
      "epoch:22 step:20815 [D loss: 0.645647, acc: 64.06%] [G loss: 1.917986]\n",
      "epoch:22 step:20816 [D loss: 0.689529, acc: 58.59%] [G loss: 1.855067]\n",
      "epoch:22 step:20817 [D loss: 0.622920, acc: 66.41%] [G loss: 1.988825]\n",
      "epoch:22 step:20818 [D loss: 0.653965, acc: 54.69%] [G loss: 1.838598]\n",
      "epoch:22 step:20819 [D loss: 0.658415, acc: 60.94%] [G loss: 1.888821]\n",
      "epoch:22 step:20820 [D loss: 0.625272, acc: 66.41%] [G loss: 2.070831]\n",
      "epoch:22 step:20821 [D loss: 0.640733, acc: 59.38%] [G loss: 2.181293]\n",
      "epoch:22 step:20822 [D loss: 0.567778, acc: 70.31%] [G loss: 2.259219]\n",
      "epoch:22 step:20823 [D loss: 0.654043, acc: 63.28%] [G loss: 2.022438]\n",
      "epoch:22 step:20824 [D loss: 0.678594, acc: 56.25%] [G loss: 1.864594]\n",
      "epoch:22 step:20825 [D loss: 0.740000, acc: 51.56%] [G loss: 1.775085]\n",
      "epoch:22 step:20826 [D loss: 0.661319, acc: 61.72%] [G loss: 1.799379]\n",
      "epoch:22 step:20827 [D loss: 0.659859, acc: 61.72%] [G loss: 1.788542]\n",
      "epoch:22 step:20828 [D loss: 0.677804, acc: 53.91%] [G loss: 1.700204]\n",
      "epoch:22 step:20829 [D loss: 0.699907, acc: 53.91%] [G loss: 1.722796]\n",
      "epoch:22 step:20830 [D loss: 0.655569, acc: 62.50%] [G loss: 1.899194]\n",
      "epoch:22 step:20831 [D loss: 0.618265, acc: 70.31%] [G loss: 1.753840]\n",
      "epoch:22 step:20832 [D loss: 0.601223, acc: 67.19%] [G loss: 2.008610]\n",
      "epoch:22 step:20833 [D loss: 0.572501, acc: 71.88%] [G loss: 2.252098]\n",
      "epoch:22 step:20834 [D loss: 0.714359, acc: 55.47%] [G loss: 1.755392]\n",
      "epoch:22 step:20835 [D loss: 0.647487, acc: 61.72%] [G loss: 1.775287]\n",
      "epoch:22 step:20836 [D loss: 0.612673, acc: 65.62%] [G loss: 1.886967]\n",
      "epoch:22 step:20837 [D loss: 0.644294, acc: 62.50%] [G loss: 1.940660]\n",
      "epoch:22 step:20838 [D loss: 0.675275, acc: 55.47%] [G loss: 1.830961]\n",
      "epoch:22 step:20839 [D loss: 0.669534, acc: 58.59%] [G loss: 1.916754]\n",
      "epoch:22 step:20840 [D loss: 0.604227, acc: 69.53%] [G loss: 1.886176]\n",
      "epoch:22 step:20841 [D loss: 0.617570, acc: 65.62%] [G loss: 1.915212]\n",
      "epoch:22 step:20842 [D loss: 0.721619, acc: 53.91%] [G loss: 1.776031]\n",
      "epoch:22 step:20843 [D loss: 0.602666, acc: 71.09%] [G loss: 2.053766]\n",
      "epoch:22 step:20844 [D loss: 0.566786, acc: 70.31%] [G loss: 1.996655]\n",
      "epoch:22 step:20845 [D loss: 0.614472, acc: 69.53%] [G loss: 2.416317]\n",
      "epoch:22 step:20846 [D loss: 0.563731, acc: 74.22%] [G loss: 2.241694]\n",
      "epoch:22 step:20847 [D loss: 0.637966, acc: 61.72%] [G loss: 1.828540]\n",
      "epoch:22 step:20848 [D loss: 0.674563, acc: 55.47%] [G loss: 1.921858]\n",
      "epoch:22 step:20849 [D loss: 0.649653, acc: 62.50%] [G loss: 1.906579]\n",
      "epoch:22 step:20850 [D loss: 0.647185, acc: 60.94%] [G loss: 1.822122]\n",
      "epoch:22 step:20851 [D loss: 0.668222, acc: 59.38%] [G loss: 1.892946]\n",
      "epoch:22 step:20852 [D loss: 0.639336, acc: 60.16%] [G loss: 2.000415]\n",
      "epoch:22 step:20853 [D loss: 0.664195, acc: 60.16%] [G loss: 1.880670]\n",
      "epoch:22 step:20854 [D loss: 0.668127, acc: 57.03%] [G loss: 1.751812]\n",
      "epoch:22 step:20855 [D loss: 0.640727, acc: 63.28%] [G loss: 1.828641]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:22 step:20856 [D loss: 0.643261, acc: 65.62%] [G loss: 1.908783]\n",
      "epoch:22 step:20857 [D loss: 0.630099, acc: 63.28%] [G loss: 1.928539]\n",
      "epoch:22 step:20858 [D loss: 0.651156, acc: 61.72%] [G loss: 1.730512]\n",
      "epoch:22 step:20859 [D loss: 0.622851, acc: 66.41%] [G loss: 1.901876]\n",
      "epoch:22 step:20860 [D loss: 0.692796, acc: 60.16%] [G loss: 1.871206]\n",
      "epoch:22 step:20861 [D loss: 0.612955, acc: 63.28%] [G loss: 1.916470]\n",
      "epoch:22 step:20862 [D loss: 0.623211, acc: 63.28%] [G loss: 1.908183]\n",
      "epoch:22 step:20863 [D loss: 0.679987, acc: 57.03%] [G loss: 1.745014]\n",
      "epoch:22 step:20864 [D loss: 0.681598, acc: 57.03%] [G loss: 1.800759]\n",
      "epoch:22 step:20865 [D loss: 0.664578, acc: 60.16%] [G loss: 1.832234]\n",
      "epoch:22 step:20866 [D loss: 0.674718, acc: 54.69%] [G loss: 1.803823]\n",
      "epoch:22 step:20867 [D loss: 0.690360, acc: 52.34%] [G loss: 1.868366]\n",
      "epoch:22 step:20868 [D loss: 0.631171, acc: 68.75%] [G loss: 1.814710]\n",
      "epoch:22 step:20869 [D loss: 0.604436, acc: 67.97%] [G loss: 1.826042]\n",
      "epoch:22 step:20870 [D loss: 0.623662, acc: 64.84%] [G loss: 1.919884]\n",
      "epoch:22 step:20871 [D loss: 0.707487, acc: 56.25%] [G loss: 1.774539]\n",
      "epoch:22 step:20872 [D loss: 0.640601, acc: 66.41%] [G loss: 1.752234]\n",
      "epoch:22 step:20873 [D loss: 0.640449, acc: 60.94%] [G loss: 1.941355]\n",
      "epoch:22 step:20874 [D loss: 0.635948, acc: 64.06%] [G loss: 1.880272]\n",
      "epoch:22 step:20875 [D loss: 0.611043, acc: 68.75%] [G loss: 2.017462]\n",
      "epoch:22 step:20876 [D loss: 0.576131, acc: 69.53%] [G loss: 2.228484]\n",
      "epoch:22 step:20877 [D loss: 0.633486, acc: 61.72%] [G loss: 2.093841]\n",
      "epoch:22 step:20878 [D loss: 0.567010, acc: 71.88%] [G loss: 2.095211]\n",
      "epoch:22 step:20879 [D loss: 0.613823, acc: 68.75%] [G loss: 1.933640]\n",
      "epoch:22 step:20880 [D loss: 0.663757, acc: 61.72%] [G loss: 1.986066]\n",
      "epoch:22 step:20881 [D loss: 0.673404, acc: 60.16%] [G loss: 1.898257]\n",
      "epoch:22 step:20882 [D loss: 0.678617, acc: 61.72%] [G loss: 1.902105]\n",
      "epoch:22 step:20883 [D loss: 0.637236, acc: 60.94%] [G loss: 1.950795]\n",
      "epoch:22 step:20884 [D loss: 0.628423, acc: 65.62%] [G loss: 1.865285]\n",
      "epoch:22 step:20885 [D loss: 0.611631, acc: 67.97%] [G loss: 2.014216]\n",
      "epoch:22 step:20886 [D loss: 0.628044, acc: 67.19%] [G loss: 2.009874]\n",
      "epoch:22 step:20887 [D loss: 0.615265, acc: 67.19%] [G loss: 1.826697]\n",
      "epoch:22 step:20888 [D loss: 0.606105, acc: 66.41%] [G loss: 2.138751]\n",
      "epoch:22 step:20889 [D loss: 0.588683, acc: 67.19%] [G loss: 2.043793]\n",
      "epoch:22 step:20890 [D loss: 0.589136, acc: 67.97%] [G loss: 2.208082]\n",
      "epoch:22 step:20891 [D loss: 0.724447, acc: 54.69%] [G loss: 1.954953]\n",
      "epoch:22 step:20892 [D loss: 0.621708, acc: 68.75%] [G loss: 1.897099]\n",
      "epoch:22 step:20893 [D loss: 0.659279, acc: 61.72%] [G loss: 1.908353]\n",
      "epoch:22 step:20894 [D loss: 0.586233, acc: 70.31%] [G loss: 1.962954]\n",
      "epoch:22 step:20895 [D loss: 0.681599, acc: 54.69%] [G loss: 1.845232]\n",
      "epoch:22 step:20896 [D loss: 0.626272, acc: 71.09%] [G loss: 1.880452]\n",
      "epoch:22 step:20897 [D loss: 0.587812, acc: 74.22%] [G loss: 2.048481]\n",
      "epoch:22 step:20898 [D loss: 0.663480, acc: 56.25%] [G loss: 1.889425]\n",
      "epoch:22 step:20899 [D loss: 0.678604, acc: 57.81%] [G loss: 1.764722]\n",
      "epoch:22 step:20900 [D loss: 0.600292, acc: 69.53%] [G loss: 1.931481]\n",
      "epoch:22 step:20901 [D loss: 0.617141, acc: 57.81%] [G loss: 1.924940]\n",
      "epoch:22 step:20902 [D loss: 0.663404, acc: 60.16%] [G loss: 1.872895]\n",
      "epoch:22 step:20903 [D loss: 0.600296, acc: 74.22%] [G loss: 1.921802]\n",
      "epoch:22 step:20904 [D loss: 0.636721, acc: 66.41%] [G loss: 1.962700]\n",
      "epoch:22 step:20905 [D loss: 0.642848, acc: 64.84%] [G loss: 1.876543]\n",
      "epoch:22 step:20906 [D loss: 0.607579, acc: 64.84%] [G loss: 1.842945]\n",
      "epoch:22 step:20907 [D loss: 0.647401, acc: 66.41%] [G loss: 1.970826]\n",
      "epoch:22 step:20908 [D loss: 0.695458, acc: 60.16%] [G loss: 1.845963]\n",
      "epoch:22 step:20909 [D loss: 0.712376, acc: 51.56%] [G loss: 1.866412]\n",
      "epoch:22 step:20910 [D loss: 0.625492, acc: 65.62%] [G loss: 2.012438]\n",
      "epoch:22 step:20911 [D loss: 0.646824, acc: 58.59%] [G loss: 1.790636]\n",
      "epoch:22 step:20912 [D loss: 0.614659, acc: 66.41%] [G loss: 2.075068]\n",
      "epoch:22 step:20913 [D loss: 0.599423, acc: 67.97%] [G loss: 1.896159]\n",
      "epoch:22 step:20914 [D loss: 0.631546, acc: 64.84%] [G loss: 2.072888]\n",
      "epoch:22 step:20915 [D loss: 0.763791, acc: 51.56%] [G loss: 1.915686]\n",
      "epoch:22 step:20916 [D loss: 0.646232, acc: 58.59%] [G loss: 1.787456]\n",
      "epoch:22 step:20917 [D loss: 0.690330, acc: 60.16%] [G loss: 1.762427]\n",
      "epoch:22 step:20918 [D loss: 0.685102, acc: 53.12%] [G loss: 1.841820]\n",
      "epoch:22 step:20919 [D loss: 0.651163, acc: 66.41%] [G loss: 1.940410]\n",
      "epoch:22 step:20920 [D loss: 0.682683, acc: 58.59%] [G loss: 1.792376]\n",
      "epoch:22 step:20921 [D loss: 0.690661, acc: 53.12%] [G loss: 1.789863]\n",
      "epoch:22 step:20922 [D loss: 0.643271, acc: 65.62%] [G loss: 1.771266]\n",
      "epoch:22 step:20923 [D loss: 0.590751, acc: 72.66%] [G loss: 1.888105]\n",
      "epoch:22 step:20924 [D loss: 0.701177, acc: 54.69%] [G loss: 1.866928]\n",
      "epoch:22 step:20925 [D loss: 0.635081, acc: 65.62%] [G loss: 1.842550]\n",
      "epoch:22 step:20926 [D loss: 0.593690, acc: 67.97%] [G loss: 2.206986]\n",
      "epoch:22 step:20927 [D loss: 0.602462, acc: 69.53%] [G loss: 2.096056]\n",
      "epoch:22 step:20928 [D loss: 0.623084, acc: 65.62%] [G loss: 2.211350]\n",
      "epoch:22 step:20929 [D loss: 0.599335, acc: 69.53%] [G loss: 1.979079]\n",
      "epoch:22 step:20930 [D loss: 0.697844, acc: 53.12%] [G loss: 1.720825]\n",
      "epoch:22 step:20931 [D loss: 0.659153, acc: 61.72%] [G loss: 1.963919]\n",
      "epoch:22 step:20932 [D loss: 0.665327, acc: 56.25%] [G loss: 1.924992]\n",
      "epoch:22 step:20933 [D loss: 0.666668, acc: 60.94%] [G loss: 1.761460]\n",
      "epoch:22 step:20934 [D loss: 0.661222, acc: 63.28%] [G loss: 1.912711]\n",
      "epoch:22 step:20935 [D loss: 0.651912, acc: 60.94%] [G loss: 1.999973]\n",
      "epoch:22 step:20936 [D loss: 0.639102, acc: 64.06%] [G loss: 1.813357]\n",
      "epoch:22 step:20937 [D loss: 0.676279, acc: 60.94%] [G loss: 1.685606]\n",
      "epoch:22 step:20938 [D loss: 0.618165, acc: 64.06%] [G loss: 1.912567]\n",
      "epoch:22 step:20939 [D loss: 0.640958, acc: 63.28%] [G loss: 1.829802]\n",
      "epoch:22 step:20940 [D loss: 0.612685, acc: 67.97%] [G loss: 1.759802]\n",
      "epoch:22 step:20941 [D loss: 0.667315, acc: 60.94%] [G loss: 1.859282]\n",
      "epoch:22 step:20942 [D loss: 0.636886, acc: 62.50%] [G loss: 1.770616]\n",
      "epoch:22 step:20943 [D loss: 0.703111, acc: 60.16%] [G loss: 2.004685]\n",
      "epoch:22 step:20944 [D loss: 0.610356, acc: 60.94%] [G loss: 1.981522]\n",
      "epoch:22 step:20945 [D loss: 0.645168, acc: 60.94%] [G loss: 1.861127]\n",
      "epoch:22 step:20946 [D loss: 0.697305, acc: 53.91%] [G loss: 1.905046]\n",
      "epoch:22 step:20947 [D loss: 0.661104, acc: 53.91%] [G loss: 1.839789]\n",
      "epoch:22 step:20948 [D loss: 0.610347, acc: 64.06%] [G loss: 1.953006]\n",
      "epoch:22 step:20949 [D loss: 0.601165, acc: 66.41%] [G loss: 1.990350]\n",
      "epoch:22 step:20950 [D loss: 0.688595, acc: 60.16%] [G loss: 1.999292]\n",
      "epoch:22 step:20951 [D loss: 0.689801, acc: 54.69%] [G loss: 2.006771]\n",
      "epoch:22 step:20952 [D loss: 0.592897, acc: 71.88%] [G loss: 1.998537]\n",
      "epoch:22 step:20953 [D loss: 0.654962, acc: 60.16%] [G loss: 1.881303]\n",
      "epoch:22 step:20954 [D loss: 0.608402, acc: 67.97%] [G loss: 1.996697]\n",
      "epoch:22 step:20955 [D loss: 0.686886, acc: 59.38%] [G loss: 1.809679]\n",
      "epoch:22 step:20956 [D loss: 0.654807, acc: 57.81%] [G loss: 1.829319]\n",
      "epoch:22 step:20957 [D loss: 0.671174, acc: 56.25%] [G loss: 1.857398]\n",
      "epoch:22 step:20958 [D loss: 0.669719, acc: 58.59%] [G loss: 1.921192]\n",
      "epoch:22 step:20959 [D loss: 0.619237, acc: 62.50%] [G loss: 2.212000]\n",
      "epoch:22 step:20960 [D loss: 0.540460, acc: 75.00%] [G loss: 2.172579]\n",
      "epoch:22 step:20961 [D loss: 0.592696, acc: 69.53%] [G loss: 2.244379]\n",
      "epoch:22 step:20962 [D loss: 0.712932, acc: 56.25%] [G loss: 1.867674]\n",
      "epoch:22 step:20963 [D loss: 0.756881, acc: 46.88%] [G loss: 1.709448]\n",
      "epoch:22 step:20964 [D loss: 0.661336, acc: 59.38%] [G loss: 1.936156]\n",
      "epoch:22 step:20965 [D loss: 0.730571, acc: 52.34%] [G loss: 1.788899]\n",
      "epoch:22 step:20966 [D loss: 0.682175, acc: 57.03%] [G loss: 1.861218]\n",
      "epoch:22 step:20967 [D loss: 0.681064, acc: 62.50%] [G loss: 1.807621]\n",
      "epoch:22 step:20968 [D loss: 0.619122, acc: 66.41%] [G loss: 1.949940]\n",
      "epoch:22 step:20969 [D loss: 0.717280, acc: 55.47%] [G loss: 1.722725]\n",
      "epoch:22 step:20970 [D loss: 0.676573, acc: 60.94%] [G loss: 1.800550]\n",
      "epoch:22 step:20971 [D loss: 0.633187, acc: 63.28%] [G loss: 1.814915]\n",
      "epoch:22 step:20972 [D loss: 0.613967, acc: 61.72%] [G loss: 1.942744]\n",
      "epoch:22 step:20973 [D loss: 0.645033, acc: 65.62%] [G loss: 1.965557]\n",
      "epoch:22 step:20974 [D loss: 0.629805, acc: 64.84%] [G loss: 1.825262]\n",
      "epoch:22 step:20975 [D loss: 0.687977, acc: 64.84%] [G loss: 1.826853]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:22 step:20976 [D loss: 0.678866, acc: 52.34%] [G loss: 1.901931]\n",
      "epoch:22 step:20977 [D loss: 0.668042, acc: 53.91%] [G loss: 1.855326]\n",
      "epoch:22 step:20978 [D loss: 0.647048, acc: 64.06%] [G loss: 1.786918]\n",
      "epoch:22 step:20979 [D loss: 0.661197, acc: 57.03%] [G loss: 1.744842]\n",
      "epoch:22 step:20980 [D loss: 0.668121, acc: 60.16%] [G loss: 1.792047]\n",
      "epoch:22 step:20981 [D loss: 0.689197, acc: 56.25%] [G loss: 1.864346]\n",
      "epoch:22 step:20982 [D loss: 0.623103, acc: 67.19%] [G loss: 1.730255]\n",
      "epoch:22 step:20983 [D loss: 0.651149, acc: 60.94%] [G loss: 1.768290]\n",
      "epoch:22 step:20984 [D loss: 0.623893, acc: 65.62%] [G loss: 1.938254]\n",
      "epoch:22 step:20985 [D loss: 0.566938, acc: 70.31%] [G loss: 2.026652]\n",
      "epoch:22 step:20986 [D loss: 0.596643, acc: 66.41%] [G loss: 2.037352]\n",
      "epoch:22 step:20987 [D loss: 0.678531, acc: 62.50%] [G loss: 1.800413]\n",
      "epoch:22 step:20988 [D loss: 0.617550, acc: 67.19%] [G loss: 2.110353]\n",
      "epoch:22 step:20989 [D loss: 0.694918, acc: 54.69%] [G loss: 1.762614]\n",
      "epoch:22 step:20990 [D loss: 0.675957, acc: 55.47%] [G loss: 1.867115]\n",
      "epoch:22 step:20991 [D loss: 0.720463, acc: 50.78%] [G loss: 1.711236]\n",
      "epoch:22 step:20992 [D loss: 0.645190, acc: 64.06%] [G loss: 1.822274]\n",
      "epoch:22 step:20993 [D loss: 0.690744, acc: 58.59%] [G loss: 1.956658]\n",
      "epoch:22 step:20994 [D loss: 0.618737, acc: 71.09%] [G loss: 1.885253]\n",
      "epoch:22 step:20995 [D loss: 0.620271, acc: 67.19%] [G loss: 1.995512]\n",
      "epoch:22 step:20996 [D loss: 0.642575, acc: 61.72%] [G loss: 1.766012]\n",
      "epoch:22 step:20997 [D loss: 0.595900, acc: 70.31%] [G loss: 1.926792]\n",
      "epoch:22 step:20998 [D loss: 0.590855, acc: 65.62%] [G loss: 1.884946]\n",
      "epoch:22 step:20999 [D loss: 0.604952, acc: 64.84%] [G loss: 1.849853]\n",
      "epoch:22 step:21000 [D loss: 0.652402, acc: 65.62%] [G loss: 1.805888]\n",
      "##############\n",
      "[2.46965334 1.43314932 6.22658463 4.73864512 3.70160066 5.633932\n",
      " 4.41147808 4.57489003 4.51075174 3.8901666 ]\n",
      "##########\n",
      "epoch:22 step:21001 [D loss: 0.683485, acc: 55.47%] [G loss: 1.774271]\n",
      "epoch:22 step:21002 [D loss: 0.632841, acc: 64.84%] [G loss: 1.900765]\n",
      "epoch:22 step:21003 [D loss: 0.686859, acc: 63.28%] [G loss: 1.831589]\n",
      "epoch:22 step:21004 [D loss: 0.636649, acc: 62.50%] [G loss: 2.064633]\n",
      "epoch:22 step:21005 [D loss: 0.681469, acc: 59.38%] [G loss: 1.871318]\n",
      "epoch:22 step:21006 [D loss: 0.664716, acc: 60.94%] [G loss: 1.828833]\n",
      "epoch:22 step:21007 [D loss: 0.650798, acc: 61.72%] [G loss: 1.921869]\n",
      "epoch:22 step:21008 [D loss: 0.685952, acc: 60.16%] [G loss: 1.851085]\n",
      "epoch:22 step:21009 [D loss: 0.613148, acc: 64.84%] [G loss: 1.886935]\n",
      "epoch:22 step:21010 [D loss: 0.665212, acc: 57.81%] [G loss: 1.818648]\n",
      "epoch:22 step:21011 [D loss: 0.666354, acc: 57.03%] [G loss: 1.739777]\n",
      "epoch:22 step:21012 [D loss: 0.604555, acc: 70.31%] [G loss: 1.930000]\n",
      "epoch:22 step:21013 [D loss: 0.649338, acc: 58.59%] [G loss: 1.834592]\n",
      "epoch:22 step:21014 [D loss: 0.710181, acc: 55.47%] [G loss: 1.837471]\n",
      "epoch:22 step:21015 [D loss: 0.661033, acc: 59.38%] [G loss: 1.819407]\n",
      "epoch:22 step:21016 [D loss: 0.661912, acc: 55.47%] [G loss: 1.850910]\n",
      "epoch:22 step:21017 [D loss: 0.675880, acc: 53.91%] [G loss: 1.832073]\n",
      "epoch:22 step:21018 [D loss: 0.700235, acc: 59.38%] [G loss: 1.852505]\n",
      "epoch:22 step:21019 [D loss: 0.614914, acc: 68.75%] [G loss: 2.022487]\n",
      "epoch:22 step:21020 [D loss: 0.622602, acc: 67.97%] [G loss: 2.134950]\n",
      "epoch:22 step:21021 [D loss: 0.594577, acc: 69.53%] [G loss: 1.811109]\n",
      "epoch:22 step:21022 [D loss: 0.669819, acc: 59.38%] [G loss: 1.816602]\n",
      "epoch:22 step:21023 [D loss: 0.655619, acc: 60.94%] [G loss: 1.688467]\n",
      "epoch:22 step:21024 [D loss: 0.620446, acc: 65.62%] [G loss: 1.887587]\n",
      "epoch:22 step:21025 [D loss: 0.659172, acc: 57.81%] [G loss: 1.862704]\n",
      "epoch:22 step:21026 [D loss: 0.567591, acc: 75.00%] [G loss: 2.049217]\n",
      "epoch:22 step:21027 [D loss: 0.653020, acc: 61.72%] [G loss: 1.918485]\n",
      "epoch:22 step:21028 [D loss: 0.639852, acc: 62.50%] [G loss: 2.089488]\n",
      "epoch:22 step:21029 [D loss: 0.658843, acc: 67.19%] [G loss: 1.952517]\n",
      "epoch:22 step:21030 [D loss: 0.647466, acc: 64.06%] [G loss: 2.027855]\n",
      "epoch:22 step:21031 [D loss: 0.654431, acc: 64.06%] [G loss: 2.004059]\n",
      "epoch:22 step:21032 [D loss: 0.625707, acc: 62.50%] [G loss: 1.804346]\n",
      "epoch:22 step:21033 [D loss: 0.660184, acc: 61.72%] [G loss: 1.883208]\n",
      "epoch:22 step:21034 [D loss: 0.677629, acc: 53.12%] [G loss: 1.897494]\n",
      "epoch:22 step:21035 [D loss: 0.672174, acc: 60.16%] [G loss: 1.902980]\n",
      "epoch:22 step:21036 [D loss: 0.654247, acc: 60.16%] [G loss: 1.839041]\n",
      "epoch:22 step:21037 [D loss: 0.623343, acc: 68.75%] [G loss: 1.875270]\n",
      "epoch:22 step:21038 [D loss: 0.649190, acc: 57.81%] [G loss: 1.909879]\n",
      "epoch:22 step:21039 [D loss: 0.673071, acc: 59.38%] [G loss: 1.937611]\n",
      "epoch:22 step:21040 [D loss: 0.661409, acc: 60.16%] [G loss: 1.963941]\n",
      "epoch:22 step:21041 [D loss: 0.587564, acc: 75.78%] [G loss: 2.052090]\n",
      "epoch:22 step:21042 [D loss: 0.575324, acc: 67.19%] [G loss: 2.063022]\n",
      "epoch:22 step:21043 [D loss: 0.611012, acc: 63.28%] [G loss: 2.108409]\n",
      "epoch:22 step:21044 [D loss: 0.585885, acc: 68.75%] [G loss: 2.283941]\n",
      "epoch:22 step:21045 [D loss: 0.640114, acc: 62.50%] [G loss: 1.925339]\n",
      "epoch:22 step:21046 [D loss: 0.667637, acc: 57.03%] [G loss: 1.974553]\n",
      "epoch:22 step:21047 [D loss: 0.609959, acc: 65.62%] [G loss: 1.814618]\n",
      "epoch:22 step:21048 [D loss: 0.597128, acc: 73.44%] [G loss: 2.056968]\n",
      "epoch:22 step:21049 [D loss: 0.623040, acc: 66.41%] [G loss: 1.913385]\n",
      "epoch:22 step:21050 [D loss: 0.603293, acc: 68.75%] [G loss: 2.023333]\n",
      "epoch:22 step:21051 [D loss: 0.722860, acc: 55.47%] [G loss: 1.765282]\n",
      "epoch:22 step:21052 [D loss: 0.689520, acc: 55.47%] [G loss: 1.804665]\n",
      "epoch:22 step:21053 [D loss: 0.655955, acc: 63.28%] [G loss: 1.912261]\n",
      "epoch:22 step:21054 [D loss: 0.693119, acc: 58.59%] [G loss: 1.758803]\n",
      "epoch:22 step:21055 [D loss: 0.650875, acc: 59.38%] [G loss: 1.920598]\n",
      "epoch:22 step:21056 [D loss: 0.653054, acc: 63.28%] [G loss: 1.875927]\n",
      "epoch:22 step:21057 [D loss: 0.710920, acc: 63.28%] [G loss: 1.850457]\n",
      "epoch:22 step:21058 [D loss: 0.681852, acc: 53.12%] [G loss: 1.825016]\n",
      "epoch:22 step:21059 [D loss: 0.659030, acc: 61.72%] [G loss: 1.761669]\n",
      "epoch:22 step:21060 [D loss: 0.676721, acc: 62.50%] [G loss: 1.796404]\n",
      "epoch:22 step:21061 [D loss: 0.622833, acc: 64.84%] [G loss: 1.881511]\n",
      "epoch:22 step:21062 [D loss: 0.668778, acc: 60.16%] [G loss: 1.804351]\n",
      "epoch:22 step:21063 [D loss: 0.603352, acc: 71.09%] [G loss: 1.902798]\n",
      "epoch:22 step:21064 [D loss: 0.609414, acc: 67.19%] [G loss: 1.906214]\n",
      "epoch:22 step:21065 [D loss: 0.624990, acc: 64.06%] [G loss: 1.898268]\n",
      "epoch:22 step:21066 [D loss: 0.598261, acc: 68.75%] [G loss: 1.875080]\n",
      "epoch:22 step:21067 [D loss: 0.626446, acc: 66.41%] [G loss: 1.993104]\n",
      "epoch:22 step:21068 [D loss: 0.635123, acc: 62.50%] [G loss: 1.838088]\n",
      "epoch:22 step:21069 [D loss: 0.634281, acc: 60.94%] [G loss: 1.986088]\n",
      "epoch:22 step:21070 [D loss: 0.586491, acc: 75.00%] [G loss: 2.009071]\n",
      "epoch:22 step:21071 [D loss: 0.609680, acc: 66.41%] [G loss: 2.142907]\n",
      "epoch:22 step:21072 [D loss: 0.654470, acc: 59.38%] [G loss: 1.766802]\n",
      "epoch:22 step:21073 [D loss: 0.701706, acc: 54.69%] [G loss: 1.816660]\n",
      "epoch:22 step:21074 [D loss: 0.719638, acc: 51.56%] [G loss: 1.764196]\n",
      "epoch:22 step:21075 [D loss: 0.679208, acc: 57.03%] [G loss: 1.797521]\n",
      "epoch:22 step:21076 [D loss: 0.647146, acc: 64.84%] [G loss: 1.724737]\n",
      "epoch:22 step:21077 [D loss: 0.648916, acc: 63.28%] [G loss: 1.999782]\n",
      "epoch:22 step:21078 [D loss: 0.631900, acc: 63.28%] [G loss: 1.701797]\n",
      "epoch:22 step:21079 [D loss: 0.695914, acc: 55.47%] [G loss: 1.908927]\n",
      "epoch:22 step:21080 [D loss: 0.697910, acc: 61.72%] [G loss: 1.892352]\n",
      "epoch:22 step:21081 [D loss: 0.670913, acc: 62.50%] [G loss: 1.885470]\n",
      "epoch:22 step:21082 [D loss: 0.643614, acc: 58.59%] [G loss: 1.963325]\n",
      "epoch:22 step:21083 [D loss: 0.631828, acc: 60.16%] [G loss: 1.968917]\n",
      "epoch:22 step:21084 [D loss: 0.655286, acc: 62.50%] [G loss: 1.942935]\n",
      "epoch:22 step:21085 [D loss: 0.635333, acc: 58.59%] [G loss: 2.058770]\n",
      "epoch:22 step:21086 [D loss: 0.624145, acc: 64.84%] [G loss: 2.037786]\n",
      "epoch:22 step:21087 [D loss: 0.715968, acc: 48.44%] [G loss: 1.686002]\n",
      "epoch:22 step:21088 [D loss: 0.684500, acc: 57.03%] [G loss: 1.893032]\n",
      "epoch:22 step:21089 [D loss: 0.693602, acc: 59.38%] [G loss: 1.894973]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:22 step:21090 [D loss: 0.673434, acc: 57.81%] [G loss: 1.897788]\n",
      "epoch:22 step:21091 [D loss: 0.670096, acc: 60.16%] [G loss: 1.852127]\n",
      "epoch:22 step:21092 [D loss: 0.669524, acc: 57.03%] [G loss: 1.729568]\n",
      "epoch:22 step:21093 [D loss: 0.647523, acc: 60.16%] [G loss: 2.005953]\n",
      "epoch:22 step:21094 [D loss: 0.630440, acc: 67.19%] [G loss: 2.069999]\n",
      "epoch:22 step:21095 [D loss: 0.604264, acc: 66.41%] [G loss: 2.180425]\n",
      "epoch:22 step:21096 [D loss: 0.663578, acc: 61.72%] [G loss: 1.808394]\n",
      "epoch:22 step:21097 [D loss: 0.659873, acc: 57.03%] [G loss: 1.827086]\n",
      "epoch:22 step:21098 [D loss: 0.638723, acc: 64.06%] [G loss: 1.835873]\n",
      "epoch:22 step:21099 [D loss: 0.702714, acc: 53.91%] [G loss: 1.872497]\n",
      "epoch:22 step:21100 [D loss: 0.681862, acc: 57.03%] [G loss: 1.834716]\n",
      "epoch:22 step:21101 [D loss: 0.658277, acc: 60.94%] [G loss: 1.908263]\n",
      "epoch:22 step:21102 [D loss: 0.620618, acc: 67.19%] [G loss: 1.967110]\n",
      "epoch:22 step:21103 [D loss: 0.680360, acc: 53.91%] [G loss: 1.822915]\n",
      "epoch:22 step:21104 [D loss: 0.657312, acc: 63.28%] [G loss: 1.870315]\n",
      "epoch:22 step:21105 [D loss: 0.667246, acc: 64.06%] [G loss: 1.950227]\n",
      "epoch:22 step:21106 [D loss: 0.628212, acc: 67.19%] [G loss: 1.694839]\n",
      "epoch:22 step:21107 [D loss: 0.683529, acc: 56.25%] [G loss: 1.815891]\n",
      "epoch:22 step:21108 [D loss: 0.643253, acc: 64.84%] [G loss: 2.029684]\n",
      "epoch:22 step:21109 [D loss: 0.637879, acc: 67.19%] [G loss: 1.901664]\n",
      "epoch:22 step:21110 [D loss: 0.655165, acc: 64.06%] [G loss: 1.812824]\n",
      "epoch:22 step:21111 [D loss: 0.624700, acc: 66.41%] [G loss: 2.011871]\n",
      "epoch:22 step:21112 [D loss: 0.661784, acc: 57.03%] [G loss: 1.869921]\n",
      "epoch:22 step:21113 [D loss: 0.620811, acc: 67.97%] [G loss: 1.938559]\n",
      "epoch:22 step:21114 [D loss: 0.709035, acc: 56.25%] [G loss: 1.779812]\n",
      "epoch:22 step:21115 [D loss: 0.689361, acc: 55.47%] [G loss: 1.710658]\n",
      "epoch:22 step:21116 [D loss: 0.671676, acc: 57.03%] [G loss: 1.726744]\n",
      "epoch:22 step:21117 [D loss: 0.678116, acc: 58.59%] [G loss: 1.777702]\n",
      "epoch:22 step:21118 [D loss: 0.646248, acc: 63.28%] [G loss: 1.904030]\n",
      "epoch:22 step:21119 [D loss: 0.642876, acc: 62.50%] [G loss: 1.896647]\n",
      "epoch:22 step:21120 [D loss: 0.683767, acc: 55.47%] [G loss: 1.762128]\n",
      "epoch:22 step:21121 [D loss: 0.692421, acc: 60.94%] [G loss: 1.757436]\n",
      "epoch:22 step:21122 [D loss: 0.610815, acc: 67.97%] [G loss: 1.937487]\n",
      "epoch:22 step:21123 [D loss: 0.631646, acc: 62.50%] [G loss: 1.863412]\n",
      "epoch:22 step:21124 [D loss: 0.670504, acc: 60.16%] [G loss: 1.821351]\n",
      "epoch:22 step:21125 [D loss: 0.690940, acc: 51.56%] [G loss: 1.756163]\n",
      "epoch:22 step:21126 [D loss: 0.680940, acc: 56.25%] [G loss: 1.751358]\n",
      "epoch:22 step:21127 [D loss: 0.640946, acc: 66.41%] [G loss: 1.749011]\n",
      "epoch:22 step:21128 [D loss: 0.633694, acc: 60.16%] [G loss: 1.772997]\n",
      "epoch:22 step:21129 [D loss: 0.639661, acc: 60.16%] [G loss: 1.810911]\n",
      "epoch:22 step:21130 [D loss: 0.617186, acc: 69.53%] [G loss: 1.949744]\n",
      "epoch:22 step:21131 [D loss: 0.639273, acc: 59.38%] [G loss: 1.978743]\n",
      "epoch:22 step:21132 [D loss: 0.681661, acc: 61.72%] [G loss: 1.718003]\n",
      "epoch:22 step:21133 [D loss: 0.661328, acc: 56.25%] [G loss: 1.873481]\n",
      "epoch:22 step:21134 [D loss: 0.586368, acc: 69.53%] [G loss: 1.929667]\n",
      "epoch:22 step:21135 [D loss: 0.589351, acc: 70.31%] [G loss: 1.936571]\n",
      "epoch:22 step:21136 [D loss: 0.649806, acc: 58.59%] [G loss: 2.061131]\n",
      "epoch:22 step:21137 [D loss: 0.577166, acc: 74.22%] [G loss: 2.045096]\n",
      "epoch:22 step:21138 [D loss: 0.633152, acc: 65.62%] [G loss: 1.832936]\n",
      "epoch:22 step:21139 [D loss: 0.661135, acc: 62.50%] [G loss: 1.870615]\n",
      "epoch:22 step:21140 [D loss: 0.638499, acc: 62.50%] [G loss: 1.772150]\n",
      "epoch:22 step:21141 [D loss: 0.658901, acc: 57.81%] [G loss: 1.786427]\n",
      "epoch:22 step:21142 [D loss: 0.752194, acc: 50.00%] [G loss: 1.673475]\n",
      "epoch:22 step:21143 [D loss: 0.680699, acc: 55.47%] [G loss: 1.668552]\n",
      "epoch:22 step:21144 [D loss: 0.662848, acc: 57.81%] [G loss: 1.845291]\n",
      "epoch:22 step:21145 [D loss: 0.648380, acc: 61.72%] [G loss: 1.857347]\n",
      "epoch:22 step:21146 [D loss: 0.600583, acc: 67.97%] [G loss: 2.082973]\n",
      "epoch:22 step:21147 [D loss: 0.628671, acc: 64.84%] [G loss: 1.922777]\n",
      "epoch:22 step:21148 [D loss: 0.605286, acc: 62.50%] [G loss: 2.006038]\n",
      "epoch:22 step:21149 [D loss: 0.637452, acc: 67.97%] [G loss: 2.019418]\n",
      "epoch:22 step:21150 [D loss: 0.603233, acc: 65.62%] [G loss: 2.022581]\n",
      "epoch:22 step:21151 [D loss: 0.654162, acc: 61.72%] [G loss: 1.725549]\n",
      "epoch:22 step:21152 [D loss: 0.671356, acc: 57.03%] [G loss: 1.711806]\n",
      "epoch:22 step:21153 [D loss: 0.654971, acc: 59.38%] [G loss: 1.920789]\n",
      "epoch:22 step:21154 [D loss: 0.677536, acc: 54.69%] [G loss: 1.877101]\n",
      "epoch:22 step:21155 [D loss: 0.615786, acc: 67.97%] [G loss: 1.964665]\n",
      "epoch:22 step:21156 [D loss: 0.663149, acc: 60.16%] [G loss: 1.838488]\n",
      "epoch:22 step:21157 [D loss: 0.699062, acc: 55.47%] [G loss: 1.873172]\n",
      "epoch:22 step:21158 [D loss: 0.618478, acc: 66.41%] [G loss: 1.789583]\n",
      "epoch:22 step:21159 [D loss: 0.624494, acc: 67.19%] [G loss: 1.839881]\n",
      "epoch:22 step:21160 [D loss: 0.646138, acc: 58.59%] [G loss: 1.940985]\n",
      "epoch:22 step:21161 [D loss: 0.604595, acc: 67.19%] [G loss: 1.886259]\n",
      "epoch:22 step:21162 [D loss: 0.622197, acc: 60.94%] [G loss: 1.994520]\n",
      "epoch:22 step:21163 [D loss: 0.620132, acc: 66.41%] [G loss: 1.910634]\n",
      "epoch:22 step:21164 [D loss: 0.643067, acc: 64.06%] [G loss: 2.001682]\n",
      "epoch:22 step:21165 [D loss: 0.599142, acc: 66.41%] [G loss: 2.138249]\n",
      "epoch:22 step:21166 [D loss: 0.621413, acc: 62.50%] [G loss: 2.137670]\n",
      "epoch:22 step:21167 [D loss: 0.660846, acc: 58.59%] [G loss: 1.877065]\n",
      "epoch:22 step:21168 [D loss: 0.620257, acc: 65.62%] [G loss: 1.926826]\n",
      "epoch:22 step:21169 [D loss: 0.637628, acc: 67.19%] [G loss: 1.932257]\n",
      "epoch:22 step:21170 [D loss: 0.604871, acc: 68.75%] [G loss: 1.860583]\n",
      "epoch:22 step:21171 [D loss: 0.603098, acc: 64.84%] [G loss: 2.052537]\n",
      "epoch:22 step:21172 [D loss: 0.618192, acc: 64.06%] [G loss: 2.022604]\n",
      "epoch:22 step:21173 [D loss: 0.702255, acc: 56.25%] [G loss: 1.781698]\n",
      "epoch:22 step:21174 [D loss: 0.689606, acc: 55.47%] [G loss: 1.829462]\n",
      "epoch:22 step:21175 [D loss: 0.651877, acc: 60.94%] [G loss: 1.779060]\n",
      "epoch:22 step:21176 [D loss: 0.626204, acc: 71.88%] [G loss: 2.095609]\n",
      "epoch:22 step:21177 [D loss: 0.682402, acc: 59.38%] [G loss: 1.890541]\n",
      "epoch:22 step:21178 [D loss: 0.626874, acc: 64.84%] [G loss: 2.018877]\n",
      "epoch:22 step:21179 [D loss: 0.699036, acc: 54.69%] [G loss: 1.847575]\n",
      "epoch:22 step:21180 [D loss: 0.719780, acc: 53.12%] [G loss: 1.658535]\n",
      "epoch:22 step:21181 [D loss: 0.654508, acc: 58.59%] [G loss: 1.830346]\n",
      "epoch:22 step:21182 [D loss: 0.659307, acc: 64.84%] [G loss: 1.832090]\n",
      "epoch:22 step:21183 [D loss: 0.631435, acc: 59.38%] [G loss: 1.850634]\n",
      "epoch:22 step:21184 [D loss: 0.652769, acc: 57.03%] [G loss: 1.784172]\n",
      "epoch:22 step:21185 [D loss: 0.624855, acc: 67.19%] [G loss: 1.951488]\n",
      "epoch:22 step:21186 [D loss: 0.642628, acc: 64.84%] [G loss: 1.850385]\n",
      "epoch:22 step:21187 [D loss: 0.699847, acc: 56.25%] [G loss: 1.870613]\n",
      "epoch:22 step:21188 [D loss: 0.621017, acc: 70.31%] [G loss: 1.835646]\n",
      "epoch:22 step:21189 [D loss: 0.610628, acc: 62.50%] [G loss: 1.924083]\n",
      "epoch:22 step:21190 [D loss: 0.704155, acc: 56.25%] [G loss: 1.844800]\n",
      "epoch:22 step:21191 [D loss: 0.652520, acc: 62.50%] [G loss: 1.826115]\n",
      "epoch:22 step:21192 [D loss: 0.672153, acc: 57.03%] [G loss: 1.865909]\n",
      "epoch:22 step:21193 [D loss: 0.655175, acc: 60.94%] [G loss: 1.774171]\n",
      "epoch:22 step:21194 [D loss: 0.710280, acc: 51.56%] [G loss: 1.787513]\n",
      "epoch:22 step:21195 [D loss: 0.655684, acc: 63.28%] [G loss: 1.815232]\n",
      "epoch:22 step:21196 [D loss: 0.633744, acc: 61.72%] [G loss: 1.884211]\n",
      "epoch:22 step:21197 [D loss: 0.674685, acc: 64.84%] [G loss: 1.788387]\n",
      "epoch:22 step:21198 [D loss: 0.642460, acc: 62.50%] [G loss: 1.805536]\n",
      "epoch:22 step:21199 [D loss: 0.676535, acc: 56.25%] [G loss: 1.907603]\n",
      "epoch:22 step:21200 [D loss: 0.677614, acc: 60.94%] [G loss: 1.819981]\n",
      "##############\n",
      "[2.37342015 1.42257546 6.15160374 4.67516295 3.50303261 5.66362146\n",
      " 4.1375772  4.64206766 4.58580399 3.82268628]\n",
      "##########\n",
      "epoch:22 step:21201 [D loss: 0.663697, acc: 62.50%] [G loss: 1.834027]\n",
      "epoch:22 step:21202 [D loss: 0.642883, acc: 57.81%] [G loss: 2.014228]\n",
      "epoch:22 step:21203 [D loss: 0.655015, acc: 63.28%] [G loss: 2.025462]\n",
      "epoch:22 step:21204 [D loss: 0.673812, acc: 57.81%] [G loss: 1.773113]\n",
      "epoch:22 step:21205 [D loss: 0.597587, acc: 71.09%] [G loss: 1.960024]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:22 step:21206 [D loss: 0.629109, acc: 67.19%] [G loss: 1.983994]\n",
      "epoch:22 step:21207 [D loss: 0.633121, acc: 62.50%] [G loss: 2.049812]\n",
      "epoch:22 step:21208 [D loss: 0.683193, acc: 53.12%] [G loss: 1.898468]\n",
      "epoch:22 step:21209 [D loss: 0.641039, acc: 70.31%] [G loss: 1.871115]\n",
      "epoch:22 step:21210 [D loss: 0.674300, acc: 57.03%] [G loss: 1.783757]\n",
      "epoch:22 step:21211 [D loss: 0.650251, acc: 57.03%] [G loss: 1.805795]\n",
      "epoch:22 step:21212 [D loss: 0.647493, acc: 64.84%] [G loss: 1.950199]\n",
      "epoch:22 step:21213 [D loss: 0.665120, acc: 59.38%] [G loss: 1.878987]\n",
      "epoch:22 step:21214 [D loss: 0.648008, acc: 59.38%] [G loss: 1.877115]\n",
      "epoch:22 step:21215 [D loss: 0.731143, acc: 57.03%] [G loss: 1.807695]\n",
      "epoch:22 step:21216 [D loss: 0.664799, acc: 56.25%] [G loss: 1.788597]\n",
      "epoch:22 step:21217 [D loss: 0.626280, acc: 64.06%] [G loss: 1.910349]\n",
      "epoch:22 step:21218 [D loss: 0.611046, acc: 63.28%] [G loss: 1.814885]\n",
      "epoch:22 step:21219 [D loss: 0.601077, acc: 64.84%] [G loss: 2.013057]\n",
      "epoch:22 step:21220 [D loss: 0.667039, acc: 62.50%] [G loss: 1.965212]\n",
      "epoch:22 step:21221 [D loss: 0.618736, acc: 64.84%] [G loss: 1.837815]\n",
      "epoch:22 step:21222 [D loss: 0.637607, acc: 59.38%] [G loss: 1.783888]\n",
      "epoch:22 step:21223 [D loss: 0.656084, acc: 63.28%] [G loss: 2.073181]\n",
      "epoch:22 step:21224 [D loss: 0.651406, acc: 60.94%] [G loss: 1.893836]\n",
      "epoch:22 step:21225 [D loss: 0.637182, acc: 61.72%] [G loss: 1.819892]\n",
      "epoch:22 step:21226 [D loss: 0.677641, acc: 57.81%] [G loss: 1.824719]\n",
      "epoch:22 step:21227 [D loss: 0.630318, acc: 64.06%] [G loss: 1.814191]\n",
      "epoch:22 step:21228 [D loss: 0.684749, acc: 54.69%] [G loss: 1.814041]\n",
      "epoch:22 step:21229 [D loss: 0.669025, acc: 59.38%] [G loss: 1.778331]\n",
      "epoch:22 step:21230 [D loss: 0.650675, acc: 56.25%] [G loss: 1.678999]\n",
      "epoch:22 step:21231 [D loss: 0.662334, acc: 59.38%] [G loss: 1.790825]\n",
      "epoch:22 step:21232 [D loss: 0.665887, acc: 58.59%] [G loss: 1.762334]\n",
      "epoch:22 step:21233 [D loss: 0.662591, acc: 61.72%] [G loss: 1.851651]\n",
      "epoch:22 step:21234 [D loss: 0.672752, acc: 65.62%] [G loss: 1.905196]\n",
      "epoch:22 step:21235 [D loss: 0.629648, acc: 66.41%] [G loss: 1.860602]\n",
      "epoch:22 step:21236 [D loss: 0.627454, acc: 60.94%] [G loss: 1.941387]\n",
      "epoch:22 step:21237 [D loss: 0.650156, acc: 60.16%] [G loss: 1.765701]\n",
      "epoch:22 step:21238 [D loss: 0.637233, acc: 67.19%] [G loss: 2.046003]\n",
      "epoch:22 step:21239 [D loss: 0.703208, acc: 55.47%] [G loss: 1.703135]\n",
      "epoch:22 step:21240 [D loss: 0.623002, acc: 64.06%] [G loss: 1.897271]\n",
      "epoch:22 step:21241 [D loss: 0.625112, acc: 63.28%] [G loss: 1.961050]\n",
      "epoch:22 step:21242 [D loss: 0.702794, acc: 50.78%] [G loss: 1.711709]\n",
      "epoch:22 step:21243 [D loss: 0.621696, acc: 64.06%] [G loss: 1.903037]\n",
      "epoch:22 step:21244 [D loss: 0.619776, acc: 68.75%] [G loss: 1.925630]\n",
      "epoch:22 step:21245 [D loss: 0.654015, acc: 61.72%] [G loss: 1.948447]\n",
      "epoch:22 step:21246 [D loss: 0.643706, acc: 59.38%] [G loss: 1.871905]\n",
      "epoch:22 step:21247 [D loss: 0.635126, acc: 60.94%] [G loss: 1.867902]\n",
      "epoch:22 step:21248 [D loss: 0.629069, acc: 62.50%] [G loss: 1.994996]\n",
      "epoch:22 step:21249 [D loss: 0.625084, acc: 65.62%] [G loss: 1.810059]\n",
      "epoch:22 step:21250 [D loss: 0.639323, acc: 60.16%] [G loss: 1.945193]\n",
      "epoch:22 step:21251 [D loss: 0.614016, acc: 67.19%] [G loss: 1.982271]\n",
      "epoch:22 step:21252 [D loss: 0.631633, acc: 65.62%] [G loss: 2.034684]\n",
      "epoch:22 step:21253 [D loss: 0.670754, acc: 51.56%] [G loss: 1.952130]\n",
      "epoch:22 step:21254 [D loss: 0.706182, acc: 48.44%] [G loss: 1.908285]\n",
      "epoch:22 step:21255 [D loss: 0.655322, acc: 61.72%] [G loss: 1.831660]\n",
      "epoch:22 step:21256 [D loss: 0.670344, acc: 58.59%] [G loss: 1.952521]\n",
      "epoch:22 step:21257 [D loss: 0.650392, acc: 59.38%] [G loss: 1.815232]\n",
      "epoch:22 step:21258 [D loss: 0.620940, acc: 63.28%] [G loss: 1.908093]\n",
      "epoch:22 step:21259 [D loss: 0.625247, acc: 67.19%] [G loss: 1.848476]\n",
      "epoch:22 step:21260 [D loss: 0.627969, acc: 60.16%] [G loss: 2.051687]\n",
      "epoch:22 step:21261 [D loss: 0.577760, acc: 66.41%] [G loss: 2.113297]\n",
      "epoch:22 step:21262 [D loss: 0.614078, acc: 64.06%] [G loss: 2.240997]\n",
      "epoch:22 step:21263 [D loss: 0.598475, acc: 65.62%] [G loss: 2.120944]\n",
      "epoch:22 step:21264 [D loss: 0.624815, acc: 64.06%] [G loss: 1.949681]\n",
      "epoch:22 step:21265 [D loss: 0.688700, acc: 61.72%] [G loss: 1.966622]\n",
      "epoch:22 step:21266 [D loss: 0.688297, acc: 56.25%] [G loss: 1.936564]\n",
      "epoch:22 step:21267 [D loss: 0.631748, acc: 64.06%] [G loss: 1.916739]\n",
      "epoch:22 step:21268 [D loss: 0.655694, acc: 63.28%] [G loss: 1.984301]\n",
      "epoch:22 step:21269 [D loss: 0.669980, acc: 59.38%] [G loss: 1.935262]\n",
      "epoch:22 step:21270 [D loss: 0.671080, acc: 64.06%] [G loss: 1.649376]\n",
      "epoch:22 step:21271 [D loss: 0.750731, acc: 52.34%] [G loss: 1.731251]\n",
      "epoch:22 step:21272 [D loss: 0.699592, acc: 53.12%] [G loss: 1.725959]\n",
      "epoch:22 step:21273 [D loss: 0.664893, acc: 60.16%] [G loss: 1.677169]\n",
      "epoch:22 step:21274 [D loss: 0.666087, acc: 59.38%] [G loss: 1.743330]\n",
      "epoch:22 step:21275 [D loss: 0.657113, acc: 60.16%] [G loss: 1.839373]\n",
      "epoch:22 step:21276 [D loss: 0.626258, acc: 63.28%] [G loss: 1.889216]\n",
      "epoch:22 step:21277 [D loss: 0.647746, acc: 61.72%] [G loss: 1.754240]\n",
      "epoch:22 step:21278 [D loss: 0.640632, acc: 61.72%] [G loss: 1.933906]\n",
      "epoch:22 step:21279 [D loss: 0.697073, acc: 53.91%] [G loss: 1.601318]\n",
      "epoch:22 step:21280 [D loss: 0.649557, acc: 64.06%] [G loss: 1.903699]\n",
      "epoch:22 step:21281 [D loss: 0.646896, acc: 63.28%] [G loss: 1.805148]\n",
      "epoch:22 step:21282 [D loss: 0.631492, acc: 64.06%] [G loss: 1.869858]\n",
      "epoch:22 step:21283 [D loss: 0.636112, acc: 65.62%] [G loss: 1.738914]\n",
      "epoch:22 step:21284 [D loss: 0.638718, acc: 55.47%] [G loss: 1.818513]\n",
      "epoch:22 step:21285 [D loss: 0.655893, acc: 60.94%] [G loss: 1.777037]\n",
      "epoch:22 step:21286 [D loss: 0.588680, acc: 63.28%] [G loss: 1.896233]\n",
      "epoch:22 step:21287 [D loss: 0.653422, acc: 56.25%] [G loss: 1.715230]\n",
      "epoch:22 step:21288 [D loss: 0.667888, acc: 67.19%] [G loss: 1.903223]\n",
      "epoch:22 step:21289 [D loss: 0.625971, acc: 60.94%] [G loss: 1.822547]\n",
      "epoch:22 step:21290 [D loss: 0.656760, acc: 58.59%] [G loss: 1.832198]\n",
      "epoch:22 step:21291 [D loss: 0.627964, acc: 63.28%] [G loss: 2.042296]\n",
      "epoch:22 step:21292 [D loss: 0.625794, acc: 67.19%] [G loss: 1.852262]\n",
      "epoch:22 step:21293 [D loss: 0.641459, acc: 64.06%] [G loss: 1.825194]\n",
      "epoch:22 step:21294 [D loss: 0.600173, acc: 68.75%] [G loss: 1.941702]\n",
      "epoch:22 step:21295 [D loss: 0.624063, acc: 60.16%] [G loss: 2.140849]\n",
      "epoch:22 step:21296 [D loss: 0.602666, acc: 63.28%] [G loss: 1.857476]\n",
      "epoch:22 step:21297 [D loss: 0.677295, acc: 61.72%] [G loss: 1.905221]\n",
      "epoch:22 step:21298 [D loss: 0.678879, acc: 55.47%] [G loss: 1.904539]\n",
      "epoch:22 step:21299 [D loss: 0.605271, acc: 67.97%] [G loss: 1.872359]\n",
      "epoch:22 step:21300 [D loss: 0.641290, acc: 60.16%] [G loss: 1.805177]\n",
      "epoch:22 step:21301 [D loss: 0.604034, acc: 68.75%] [G loss: 1.931774]\n",
      "epoch:22 step:21302 [D loss: 0.562613, acc: 67.19%] [G loss: 2.007475]\n",
      "epoch:22 step:21303 [D loss: 0.655313, acc: 61.72%] [G loss: 1.975142]\n",
      "epoch:22 step:21304 [D loss: 0.684129, acc: 60.16%] [G loss: 1.975624]\n",
      "epoch:22 step:21305 [D loss: 0.697673, acc: 58.59%] [G loss: 1.906167]\n",
      "epoch:22 step:21306 [D loss: 0.611127, acc: 71.09%] [G loss: 2.093514]\n",
      "epoch:22 step:21307 [D loss: 0.639007, acc: 57.81%] [G loss: 1.982850]\n",
      "epoch:22 step:21308 [D loss: 0.575639, acc: 71.09%] [G loss: 1.977813]\n",
      "epoch:22 step:21309 [D loss: 0.633460, acc: 61.72%] [G loss: 2.010644]\n",
      "epoch:22 step:21310 [D loss: 0.644603, acc: 61.72%] [G loss: 1.832369]\n",
      "epoch:22 step:21311 [D loss: 0.651795, acc: 62.50%] [G loss: 1.850132]\n",
      "epoch:22 step:21312 [D loss: 0.679406, acc: 55.47%] [G loss: 1.884479]\n",
      "epoch:22 step:21313 [D loss: 0.607822, acc: 69.53%] [G loss: 1.866087]\n",
      "epoch:22 step:21314 [D loss: 0.633292, acc: 65.62%] [G loss: 1.789748]\n",
      "epoch:22 step:21315 [D loss: 0.677810, acc: 58.59%] [G loss: 1.951321]\n",
      "epoch:22 step:21316 [D loss: 0.632943, acc: 65.62%] [G loss: 1.849787]\n",
      "epoch:22 step:21317 [D loss: 0.679205, acc: 57.81%] [G loss: 1.880259]\n",
      "epoch:22 step:21318 [D loss: 0.664360, acc: 53.12%] [G loss: 1.919321]\n",
      "epoch:22 step:21319 [D loss: 0.641840, acc: 65.62%] [G loss: 1.879305]\n",
      "epoch:22 step:21320 [D loss: 0.690408, acc: 57.03%] [G loss: 1.939407]\n",
      "epoch:22 step:21321 [D loss: 0.621661, acc: 65.62%] [G loss: 1.924299]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:22 step:21322 [D loss: 0.645154, acc: 60.16%] [G loss: 1.940955]\n",
      "epoch:22 step:21323 [D loss: 0.614752, acc: 70.31%] [G loss: 1.992711]\n",
      "epoch:22 step:21324 [D loss: 0.671927, acc: 58.59%] [G loss: 1.766650]\n",
      "epoch:22 step:21325 [D loss: 0.686237, acc: 53.91%] [G loss: 1.979025]\n",
      "epoch:22 step:21326 [D loss: 0.619260, acc: 67.97%] [G loss: 1.877377]\n",
      "epoch:22 step:21327 [D loss: 0.658476, acc: 60.94%] [G loss: 1.817025]\n",
      "epoch:22 step:21328 [D loss: 0.627140, acc: 65.62%] [G loss: 1.787952]\n",
      "epoch:22 step:21329 [D loss: 0.651884, acc: 59.38%] [G loss: 1.858633]\n",
      "epoch:22 step:21330 [D loss: 0.752189, acc: 46.09%] [G loss: 1.744160]\n",
      "epoch:22 step:21331 [D loss: 0.636878, acc: 64.06%] [G loss: 1.904208]\n",
      "epoch:22 step:21332 [D loss: 0.643197, acc: 64.06%] [G loss: 1.659083]\n",
      "epoch:22 step:21333 [D loss: 0.658919, acc: 61.72%] [G loss: 2.027809]\n",
      "epoch:22 step:21334 [D loss: 0.631465, acc: 61.72%] [G loss: 1.914581]\n",
      "epoch:22 step:21335 [D loss: 0.603435, acc: 65.62%] [G loss: 2.061524]\n",
      "epoch:22 step:21336 [D loss: 0.664484, acc: 57.81%] [G loss: 1.886824]\n",
      "epoch:22 step:21337 [D loss: 0.667215, acc: 62.50%] [G loss: 1.958011]\n",
      "epoch:22 step:21338 [D loss: 0.628236, acc: 64.84%] [G loss: 1.747364]\n",
      "epoch:22 step:21339 [D loss: 0.609467, acc: 68.75%] [G loss: 1.857590]\n",
      "epoch:22 step:21340 [D loss: 0.625217, acc: 69.53%] [G loss: 1.992856]\n",
      "epoch:22 step:21341 [D loss: 0.715337, acc: 52.34%] [G loss: 1.846244]\n",
      "epoch:22 step:21342 [D loss: 0.665646, acc: 61.72%] [G loss: 1.988580]\n",
      "epoch:22 step:21343 [D loss: 0.628959, acc: 66.41%] [G loss: 1.859003]\n",
      "epoch:22 step:21344 [D loss: 0.621788, acc: 61.72%] [G loss: 1.879655]\n",
      "epoch:22 step:21345 [D loss: 0.711949, acc: 50.78%] [G loss: 1.855334]\n",
      "epoch:22 step:21346 [D loss: 0.617374, acc: 65.62%] [G loss: 1.788329]\n",
      "epoch:22 step:21347 [D loss: 0.648778, acc: 64.06%] [G loss: 1.876513]\n",
      "epoch:22 step:21348 [D loss: 0.679560, acc: 57.03%] [G loss: 1.698804]\n",
      "epoch:22 step:21349 [D loss: 0.641494, acc: 68.75%] [G loss: 1.899738]\n",
      "epoch:22 step:21350 [D loss: 0.643352, acc: 54.69%] [G loss: 1.842806]\n",
      "epoch:22 step:21351 [D loss: 0.647041, acc: 60.16%] [G loss: 1.908493]\n",
      "epoch:22 step:21352 [D loss: 0.691680, acc: 56.25%] [G loss: 1.852190]\n",
      "epoch:22 step:21353 [D loss: 0.690726, acc: 53.12%] [G loss: 1.761645]\n",
      "epoch:22 step:21354 [D loss: 0.616837, acc: 67.19%] [G loss: 1.910250]\n",
      "epoch:22 step:21355 [D loss: 0.626029, acc: 64.06%] [G loss: 1.842896]\n",
      "epoch:22 step:21356 [D loss: 0.689653, acc: 60.94%] [G loss: 1.822080]\n",
      "epoch:22 step:21357 [D loss: 0.687008, acc: 59.38%] [G loss: 1.906435]\n",
      "epoch:22 step:21358 [D loss: 0.632507, acc: 65.62%] [G loss: 1.859037]\n",
      "epoch:22 step:21359 [D loss: 0.652378, acc: 58.59%] [G loss: 1.903727]\n",
      "epoch:22 step:21360 [D loss: 0.626779, acc: 67.19%] [G loss: 1.775822]\n",
      "epoch:22 step:21361 [D loss: 0.602457, acc: 63.28%] [G loss: 1.886682]\n",
      "epoch:22 step:21362 [D loss: 0.625149, acc: 61.72%] [G loss: 1.906388]\n",
      "epoch:22 step:21363 [D loss: 0.681356, acc: 56.25%] [G loss: 1.770907]\n",
      "epoch:22 step:21364 [D loss: 0.675690, acc: 58.59%] [G loss: 1.845358]\n",
      "epoch:22 step:21365 [D loss: 0.646957, acc: 64.84%] [G loss: 1.908456]\n",
      "epoch:22 step:21366 [D loss: 0.677960, acc: 58.59%] [G loss: 1.801811]\n",
      "epoch:22 step:21367 [D loss: 0.617833, acc: 61.72%] [G loss: 1.918436]\n",
      "epoch:22 step:21368 [D loss: 0.600021, acc: 67.19%] [G loss: 1.961334]\n",
      "epoch:22 step:21369 [D loss: 0.659368, acc: 63.28%] [G loss: 1.907151]\n",
      "epoch:22 step:21370 [D loss: 0.599256, acc: 67.19%] [G loss: 1.837359]\n",
      "epoch:22 step:21371 [D loss: 0.624681, acc: 65.62%] [G loss: 2.063991]\n",
      "epoch:22 step:21372 [D loss: 0.643969, acc: 65.62%] [G loss: 1.856735]\n",
      "epoch:22 step:21373 [D loss: 0.687398, acc: 60.94%] [G loss: 1.784483]\n",
      "epoch:22 step:21374 [D loss: 0.658612, acc: 60.16%] [G loss: 1.874911]\n",
      "epoch:22 step:21375 [D loss: 0.692720, acc: 57.03%] [G loss: 1.727424]\n",
      "epoch:22 step:21376 [D loss: 0.618804, acc: 60.94%] [G loss: 1.944763]\n",
      "epoch:22 step:21377 [D loss: 0.664034, acc: 58.59%] [G loss: 1.871861]\n",
      "epoch:22 step:21378 [D loss: 0.646468, acc: 62.50%] [G loss: 1.807765]\n",
      "epoch:22 step:21379 [D loss: 0.704527, acc: 55.47%] [G loss: 1.743488]\n",
      "epoch:22 step:21380 [D loss: 0.664549, acc: 61.72%] [G loss: 1.835288]\n",
      "epoch:22 step:21381 [D loss: 0.684474, acc: 59.38%] [G loss: 1.770368]\n",
      "epoch:22 step:21382 [D loss: 0.691887, acc: 60.16%] [G loss: 1.780747]\n",
      "epoch:22 step:21383 [D loss: 0.631811, acc: 64.06%] [G loss: 1.833836]\n",
      "epoch:22 step:21384 [D loss: 0.674542, acc: 58.59%] [G loss: 1.739290]\n",
      "epoch:22 step:21385 [D loss: 0.640427, acc: 60.16%] [G loss: 1.744603]\n",
      "epoch:22 step:21386 [D loss: 0.636373, acc: 65.62%] [G loss: 1.993714]\n",
      "epoch:22 step:21387 [D loss: 0.663047, acc: 60.16%] [G loss: 1.831117]\n",
      "epoch:22 step:21388 [D loss: 0.638100, acc: 61.72%] [G loss: 1.882004]\n",
      "epoch:22 step:21389 [D loss: 0.626154, acc: 64.84%] [G loss: 1.954244]\n",
      "epoch:22 step:21390 [D loss: 0.620075, acc: 61.72%] [G loss: 1.865028]\n",
      "epoch:22 step:21391 [D loss: 0.668012, acc: 63.28%] [G loss: 2.020352]\n",
      "epoch:22 step:21392 [D loss: 0.671234, acc: 58.59%] [G loss: 1.887309]\n",
      "epoch:22 step:21393 [D loss: 0.624861, acc: 62.50%] [G loss: 1.749345]\n",
      "epoch:22 step:21394 [D loss: 0.645555, acc: 64.06%] [G loss: 2.123106]\n",
      "epoch:22 step:21395 [D loss: 0.608822, acc: 69.53%] [G loss: 2.085927]\n",
      "epoch:22 step:21396 [D loss: 0.616899, acc: 68.75%] [G loss: 2.184488]\n",
      "epoch:22 step:21397 [D loss: 0.598028, acc: 68.75%] [G loss: 1.997288]\n",
      "epoch:22 step:21398 [D loss: 0.698833, acc: 57.03%] [G loss: 1.730684]\n",
      "epoch:22 step:21399 [D loss: 0.644477, acc: 61.72%] [G loss: 1.896646]\n",
      "epoch:22 step:21400 [D loss: 0.614580, acc: 63.28%] [G loss: 2.177592]\n",
      "##############\n",
      "[2.33267466 1.62545189 6.06557398 4.91378988 3.57155965 5.66372556\n",
      " 4.37601439 4.81935397 4.52727907 3.92255296]\n",
      "##########\n",
      "epoch:22 step:21401 [D loss: 0.701319, acc: 54.69%] [G loss: 1.906887]\n",
      "epoch:22 step:21402 [D loss: 0.647107, acc: 59.38%] [G loss: 1.821862]\n",
      "epoch:22 step:21403 [D loss: 0.633293, acc: 66.41%] [G loss: 1.851310]\n",
      "epoch:22 step:21404 [D loss: 0.616613, acc: 67.19%] [G loss: 2.042510]\n",
      "epoch:22 step:21405 [D loss: 0.618747, acc: 65.62%] [G loss: 1.980336]\n",
      "epoch:22 step:21406 [D loss: 0.589639, acc: 70.31%] [G loss: 2.017570]\n",
      "epoch:22 step:21407 [D loss: 0.663581, acc: 60.16%] [G loss: 1.965262]\n",
      "epoch:22 step:21408 [D loss: 0.729179, acc: 49.22%] [G loss: 1.866918]\n",
      "epoch:22 step:21409 [D loss: 0.702547, acc: 55.47%] [G loss: 1.744050]\n",
      "epoch:22 step:21410 [D loss: 0.655134, acc: 62.50%] [G loss: 1.829592]\n",
      "epoch:22 step:21411 [D loss: 0.688978, acc: 56.25%] [G loss: 1.713708]\n",
      "epoch:22 step:21412 [D loss: 0.722614, acc: 50.00%] [G loss: 1.843491]\n",
      "epoch:22 step:21413 [D loss: 0.644046, acc: 60.94%] [G loss: 1.814548]\n",
      "epoch:22 step:21414 [D loss: 0.726275, acc: 53.91%] [G loss: 1.665913]\n",
      "epoch:22 step:21415 [D loss: 0.649706, acc: 63.28%] [G loss: 1.811411]\n",
      "epoch:22 step:21416 [D loss: 0.634498, acc: 64.06%] [G loss: 1.681176]\n",
      "epoch:22 step:21417 [D loss: 0.654305, acc: 64.84%] [G loss: 1.689216]\n",
      "epoch:22 step:21418 [D loss: 0.655784, acc: 64.06%] [G loss: 1.804756]\n",
      "epoch:22 step:21419 [D loss: 0.626437, acc: 67.97%] [G loss: 1.947603]\n",
      "epoch:22 step:21420 [D loss: 0.603669, acc: 68.75%] [G loss: 1.857028]\n",
      "epoch:22 step:21421 [D loss: 0.674563, acc: 61.72%] [G loss: 1.806305]\n",
      "epoch:22 step:21422 [D loss: 0.666473, acc: 57.81%] [G loss: 1.883754]\n",
      "epoch:22 step:21423 [D loss: 0.612698, acc: 68.75%] [G loss: 1.884245]\n",
      "epoch:22 step:21424 [D loss: 0.666085, acc: 58.59%] [G loss: 1.855963]\n",
      "epoch:22 step:21425 [D loss: 0.600044, acc: 70.31%] [G loss: 1.888817]\n",
      "epoch:22 step:21426 [D loss: 0.679538, acc: 57.03%] [G loss: 1.736956]\n",
      "epoch:22 step:21427 [D loss: 0.623609, acc: 64.06%] [G loss: 1.973149]\n",
      "epoch:22 step:21428 [D loss: 0.628202, acc: 62.50%] [G loss: 1.893458]\n",
      "epoch:22 step:21429 [D loss: 0.600002, acc: 67.97%] [G loss: 2.196492]\n",
      "epoch:22 step:21430 [D loss: 0.599271, acc: 65.62%] [G loss: 1.988401]\n",
      "epoch:22 step:21431 [D loss: 0.710904, acc: 55.47%] [G loss: 1.770019]\n",
      "epoch:22 step:21432 [D loss: 0.699404, acc: 58.59%] [G loss: 1.666712]\n",
      "epoch:22 step:21433 [D loss: 0.683392, acc: 59.38%] [G loss: 1.810961]\n",
      "epoch:22 step:21434 [D loss: 0.660802, acc: 60.16%] [G loss: 1.632753]\n",
      "epoch:22 step:21435 [D loss: 0.664712, acc: 62.50%] [G loss: 1.669976]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:22 step:21436 [D loss: 0.649788, acc: 65.62%] [G loss: 1.836294]\n",
      "epoch:22 step:21437 [D loss: 0.635290, acc: 64.06%] [G loss: 1.817744]\n",
      "epoch:22 step:21438 [D loss: 0.648432, acc: 63.28%] [G loss: 1.835554]\n",
      "epoch:22 step:21439 [D loss: 0.610485, acc: 67.19%] [G loss: 1.962929]\n",
      "epoch:22 step:21440 [D loss: 0.644589, acc: 60.16%] [G loss: 1.858934]\n",
      "epoch:22 step:21441 [D loss: 0.721045, acc: 58.59%] [G loss: 1.790921]\n",
      "epoch:22 step:21442 [D loss: 0.668495, acc: 58.59%] [G loss: 1.778017]\n",
      "epoch:22 step:21443 [D loss: 0.661625, acc: 63.28%] [G loss: 1.740698]\n",
      "epoch:22 step:21444 [D loss: 0.693308, acc: 60.16%] [G loss: 1.619957]\n",
      "epoch:22 step:21445 [D loss: 0.630131, acc: 57.81%] [G loss: 1.927881]\n",
      "epoch:22 step:21446 [D loss: 0.608866, acc: 69.53%] [G loss: 1.770529]\n",
      "epoch:22 step:21447 [D loss: 0.583612, acc: 72.66%] [G loss: 1.860041]\n",
      "epoch:22 step:21448 [D loss: 0.640864, acc: 68.75%] [G loss: 1.824695]\n",
      "epoch:22 step:21449 [D loss: 0.686688, acc: 56.25%] [G loss: 1.913443]\n",
      "epoch:22 step:21450 [D loss: 0.660597, acc: 63.28%] [G loss: 1.832672]\n",
      "epoch:22 step:21451 [D loss: 0.604699, acc: 71.88%] [G loss: 2.029167]\n",
      "epoch:22 step:21452 [D loss: 0.675840, acc: 59.38%] [G loss: 1.822216]\n",
      "epoch:22 step:21453 [D loss: 0.625917, acc: 66.41%] [G loss: 1.790679]\n",
      "epoch:22 step:21454 [D loss: 0.630504, acc: 62.50%] [G loss: 1.750164]\n",
      "epoch:22 step:21455 [D loss: 0.634469, acc: 65.62%] [G loss: 1.841580]\n",
      "epoch:22 step:21456 [D loss: 0.613529, acc: 67.97%] [G loss: 1.931344]\n",
      "epoch:22 step:21457 [D loss: 0.638891, acc: 58.59%] [G loss: 1.989720]\n",
      "epoch:22 step:21458 [D loss: 0.659089, acc: 61.72%] [G loss: 1.812621]\n",
      "epoch:22 step:21459 [D loss: 0.615109, acc: 61.72%] [G loss: 1.963287]\n",
      "epoch:22 step:21460 [D loss: 0.657724, acc: 60.16%] [G loss: 1.723169]\n",
      "epoch:22 step:21461 [D loss: 0.601602, acc: 66.41%] [G loss: 1.928139]\n",
      "epoch:22 step:21462 [D loss: 0.652293, acc: 62.50%] [G loss: 1.954847]\n",
      "epoch:22 step:21463 [D loss: 0.645606, acc: 58.59%] [G loss: 1.996154]\n",
      "epoch:22 step:21464 [D loss: 0.639961, acc: 61.72%] [G loss: 1.806906]\n",
      "epoch:22 step:21465 [D loss: 0.658963, acc: 57.03%] [G loss: 1.812791]\n",
      "epoch:22 step:21466 [D loss: 0.638860, acc: 60.16%] [G loss: 1.793005]\n",
      "epoch:22 step:21467 [D loss: 0.614573, acc: 70.31%] [G loss: 1.812751]\n",
      "epoch:22 step:21468 [D loss: 0.697630, acc: 55.47%] [G loss: 1.898652]\n",
      "epoch:22 step:21469 [D loss: 0.621044, acc: 62.50%] [G loss: 1.750617]\n",
      "epoch:22 step:21470 [D loss: 0.641966, acc: 60.94%] [G loss: 1.832169]\n",
      "epoch:22 step:21471 [D loss: 0.679659, acc: 64.06%] [G loss: 1.935117]\n",
      "epoch:22 step:21472 [D loss: 0.745500, acc: 51.56%] [G loss: 1.800233]\n",
      "epoch:22 step:21473 [D loss: 0.710207, acc: 54.69%] [G loss: 1.772836]\n",
      "epoch:22 step:21474 [D loss: 0.613384, acc: 65.62%] [G loss: 1.887111]\n",
      "epoch:22 step:21475 [D loss: 0.618126, acc: 69.53%] [G loss: 1.780354]\n",
      "epoch:22 step:21476 [D loss: 0.662958, acc: 60.16%] [G loss: 1.707458]\n",
      "epoch:22 step:21477 [D loss: 0.640301, acc: 67.19%] [G loss: 1.815046]\n",
      "epoch:22 step:21478 [D loss: 0.638241, acc: 61.72%] [G loss: 1.907837]\n",
      "epoch:22 step:21479 [D loss: 0.623241, acc: 64.84%] [G loss: 1.884904]\n",
      "epoch:22 step:21480 [D loss: 0.649527, acc: 59.38%] [G loss: 1.883556]\n",
      "epoch:22 step:21481 [D loss: 0.681177, acc: 53.91%] [G loss: 1.835628]\n",
      "epoch:22 step:21482 [D loss: 0.629100, acc: 64.84%] [G loss: 1.882554]\n",
      "epoch:22 step:21483 [D loss: 0.681380, acc: 55.47%] [G loss: 1.659085]\n",
      "epoch:22 step:21484 [D loss: 0.647462, acc: 57.81%] [G loss: 1.847776]\n",
      "epoch:22 step:21485 [D loss: 0.647214, acc: 60.94%] [G loss: 1.793315]\n",
      "epoch:22 step:21486 [D loss: 0.623296, acc: 66.41%] [G loss: 1.852617]\n",
      "epoch:22 step:21487 [D loss: 0.657319, acc: 59.38%] [G loss: 1.694066]\n",
      "epoch:22 step:21488 [D loss: 0.652648, acc: 60.16%] [G loss: 1.849852]\n",
      "epoch:22 step:21489 [D loss: 0.633682, acc: 68.75%] [G loss: 2.017055]\n",
      "epoch:22 step:21490 [D loss: 0.693277, acc: 60.16%] [G loss: 1.801528]\n",
      "epoch:22 step:21491 [D loss: 0.672552, acc: 64.06%] [G loss: 1.789819]\n",
      "epoch:22 step:21492 [D loss: 0.668864, acc: 61.72%] [G loss: 1.846364]\n",
      "epoch:22 step:21493 [D loss: 0.668631, acc: 62.50%] [G loss: 1.769677]\n",
      "epoch:22 step:21494 [D loss: 0.663702, acc: 59.38%] [G loss: 1.866911]\n",
      "epoch:22 step:21495 [D loss: 0.684708, acc: 53.91%] [G loss: 1.856319]\n",
      "epoch:22 step:21496 [D loss: 0.603819, acc: 68.75%] [G loss: 1.794814]\n",
      "epoch:22 step:21497 [D loss: 0.622178, acc: 67.97%] [G loss: 1.907255]\n",
      "epoch:22 step:21498 [D loss: 0.632295, acc: 67.97%] [G loss: 1.953672]\n",
      "epoch:22 step:21499 [D loss: 0.628292, acc: 67.19%] [G loss: 1.802783]\n",
      "epoch:22 step:21500 [D loss: 0.567267, acc: 69.53%] [G loss: 2.055664]\n",
      "epoch:22 step:21501 [D loss: 0.652194, acc: 57.81%] [G loss: 1.954639]\n",
      "epoch:22 step:21502 [D loss: 0.650950, acc: 62.50%] [G loss: 1.891608]\n",
      "epoch:22 step:21503 [D loss: 0.605602, acc: 67.97%] [G loss: 1.928827]\n",
      "epoch:22 step:21504 [D loss: 0.599973, acc: 67.97%] [G loss: 2.033015]\n",
      "epoch:22 step:21505 [D loss: 0.629768, acc: 64.06%] [G loss: 1.942474]\n",
      "epoch:22 step:21506 [D loss: 0.679040, acc: 54.69%] [G loss: 1.890899]\n",
      "epoch:22 step:21507 [D loss: 0.657803, acc: 65.62%] [G loss: 1.988116]\n",
      "epoch:22 step:21508 [D loss: 0.581900, acc: 66.41%] [G loss: 1.848140]\n",
      "epoch:22 step:21509 [D loss: 0.639379, acc: 65.62%] [G loss: 1.955022]\n",
      "epoch:22 step:21510 [D loss: 0.644562, acc: 60.94%] [G loss: 1.894420]\n",
      "epoch:22 step:21511 [D loss: 0.657330, acc: 59.38%] [G loss: 1.931557]\n",
      "epoch:22 step:21512 [D loss: 0.661887, acc: 60.94%] [G loss: 1.817318]\n",
      "epoch:22 step:21513 [D loss: 0.627862, acc: 70.31%] [G loss: 2.054294]\n",
      "epoch:22 step:21514 [D loss: 0.619035, acc: 67.97%] [G loss: 2.085153]\n",
      "epoch:22 step:21515 [D loss: 0.666592, acc: 59.38%] [G loss: 1.933352]\n",
      "epoch:22 step:21516 [D loss: 0.687349, acc: 58.59%] [G loss: 1.800241]\n",
      "epoch:22 step:21517 [D loss: 0.636755, acc: 65.62%] [G loss: 1.904874]\n",
      "epoch:22 step:21518 [D loss: 0.601783, acc: 64.84%] [G loss: 1.871409]\n",
      "epoch:22 step:21519 [D loss: 0.663144, acc: 62.50%] [G loss: 1.995292]\n",
      "epoch:22 step:21520 [D loss: 0.631131, acc: 61.72%] [G loss: 2.145603]\n",
      "epoch:22 step:21521 [D loss: 0.650749, acc: 66.41%] [G loss: 1.785036]\n",
      "epoch:22 step:21522 [D loss: 0.625465, acc: 71.09%] [G loss: 2.023966]\n",
      "epoch:22 step:21523 [D loss: 0.647769, acc: 60.94%] [G loss: 2.131798]\n",
      "epoch:22 step:21524 [D loss: 0.626180, acc: 59.38%] [G loss: 1.963826]\n",
      "epoch:22 step:21525 [D loss: 0.650639, acc: 57.81%] [G loss: 1.961500]\n",
      "epoch:22 step:21526 [D loss: 0.646980, acc: 64.84%] [G loss: 2.172503]\n",
      "epoch:22 step:21527 [D loss: 0.675987, acc: 58.59%] [G loss: 1.860222]\n",
      "epoch:22 step:21528 [D loss: 0.693132, acc: 46.09%] [G loss: 1.661725]\n",
      "epoch:22 step:21529 [D loss: 0.641474, acc: 64.06%] [G loss: 1.842841]\n",
      "epoch:22 step:21530 [D loss: 0.697644, acc: 61.72%] [G loss: 2.064876]\n",
      "epoch:22 step:21531 [D loss: 0.665250, acc: 58.59%] [G loss: 2.048687]\n",
      "epoch:22 step:21532 [D loss: 0.598861, acc: 71.09%] [G loss: 2.084073]\n",
      "epoch:22 step:21533 [D loss: 0.603786, acc: 67.97%] [G loss: 2.174789]\n",
      "epoch:22 step:21534 [D loss: 0.754297, acc: 46.09%] [G loss: 1.767608]\n",
      "epoch:22 step:21535 [D loss: 0.657276, acc: 61.72%] [G loss: 1.835230]\n",
      "epoch:22 step:21536 [D loss: 0.634907, acc: 62.50%] [G loss: 1.795285]\n",
      "epoch:22 step:21537 [D loss: 0.608243, acc: 69.53%] [G loss: 2.081125]\n",
      "epoch:22 step:21538 [D loss: 0.558995, acc: 73.44%] [G loss: 2.077722]\n",
      "epoch:22 step:21539 [D loss: 0.651576, acc: 67.19%] [G loss: 2.145258]\n",
      "epoch:22 step:21540 [D loss: 0.651171, acc: 57.81%] [G loss: 2.083272]\n",
      "epoch:22 step:21541 [D loss: 0.632582, acc: 66.41%] [G loss: 1.954557]\n",
      "epoch:22 step:21542 [D loss: 0.806024, acc: 49.22%] [G loss: 1.806497]\n",
      "epoch:22 step:21543 [D loss: 0.772316, acc: 40.62%] [G loss: 1.837536]\n",
      "epoch:22 step:21544 [D loss: 0.616541, acc: 67.97%] [G loss: 1.936293]\n",
      "epoch:22 step:21545 [D loss: 0.625025, acc: 65.62%] [G loss: 2.073872]\n",
      "epoch:22 step:21546 [D loss: 0.624500, acc: 61.72%] [G loss: 1.841216]\n",
      "epoch:22 step:21547 [D loss: 0.630756, acc: 64.84%] [G loss: 1.827419]\n",
      "epoch:22 step:21548 [D loss: 0.658486, acc: 60.94%] [G loss: 1.881224]\n",
      "epoch:22 step:21549 [D loss: 0.651410, acc: 61.72%] [G loss: 1.888404]\n",
      "epoch:22 step:21550 [D loss: 0.552124, acc: 76.56%] [G loss: 2.141094]\n",
      "epoch:22 step:21551 [D loss: 0.633700, acc: 64.06%] [G loss: 2.600178]\n",
      "epoch:23 step:21552 [D loss: 0.693505, acc: 58.59%] [G loss: 1.909420]\n",
      "epoch:23 step:21553 [D loss: 0.639282, acc: 70.31%] [G loss: 1.814945]\n",
      "epoch:23 step:21554 [D loss: 0.666737, acc: 57.03%] [G loss: 1.974023]\n",
      "epoch:23 step:21555 [D loss: 0.643428, acc: 60.94%] [G loss: 2.012832]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:23 step:21556 [D loss: 0.668850, acc: 63.28%] [G loss: 1.898540]\n",
      "epoch:23 step:21557 [D loss: 0.646975, acc: 62.50%] [G loss: 2.042044]\n",
      "epoch:23 step:21558 [D loss: 0.584534, acc: 73.44%] [G loss: 2.018351]\n",
      "epoch:23 step:21559 [D loss: 0.633117, acc: 64.06%] [G loss: 1.962272]\n",
      "epoch:23 step:21560 [D loss: 0.643031, acc: 67.97%] [G loss: 1.972971]\n",
      "epoch:23 step:21561 [D loss: 0.619019, acc: 71.09%] [G loss: 1.951571]\n",
      "epoch:23 step:21562 [D loss: 0.627757, acc: 64.84%] [G loss: 1.951504]\n",
      "epoch:23 step:21563 [D loss: 0.646752, acc: 60.16%] [G loss: 1.826204]\n",
      "epoch:23 step:21564 [D loss: 0.643916, acc: 64.84%] [G loss: 1.917720]\n",
      "epoch:23 step:21565 [D loss: 0.659337, acc: 61.72%] [G loss: 1.875465]\n",
      "epoch:23 step:21566 [D loss: 0.593723, acc: 67.19%] [G loss: 1.995000]\n",
      "epoch:23 step:21567 [D loss: 0.583215, acc: 67.97%] [G loss: 1.916660]\n",
      "epoch:23 step:21568 [D loss: 0.646587, acc: 64.84%] [G loss: 1.854969]\n",
      "epoch:23 step:21569 [D loss: 0.654108, acc: 60.94%] [G loss: 1.947324]\n",
      "epoch:23 step:21570 [D loss: 0.672000, acc: 64.06%] [G loss: 1.928456]\n",
      "epoch:23 step:21571 [D loss: 0.704112, acc: 58.59%] [G loss: 1.710263]\n",
      "epoch:23 step:21572 [D loss: 0.687293, acc: 53.12%] [G loss: 1.909564]\n",
      "epoch:23 step:21573 [D loss: 0.637895, acc: 62.50%] [G loss: 1.911885]\n",
      "epoch:23 step:21574 [D loss: 0.608856, acc: 63.28%] [G loss: 1.986960]\n",
      "epoch:23 step:21575 [D loss: 0.604054, acc: 68.75%] [G loss: 1.986230]\n",
      "epoch:23 step:21576 [D loss: 0.564216, acc: 75.00%] [G loss: 2.005004]\n",
      "epoch:23 step:21577 [D loss: 0.654038, acc: 56.25%] [G loss: 1.815851]\n",
      "epoch:23 step:21578 [D loss: 0.681193, acc: 59.38%] [G loss: 1.758878]\n",
      "epoch:23 step:21579 [D loss: 0.673208, acc: 53.91%] [G loss: 1.808789]\n",
      "epoch:23 step:21580 [D loss: 0.628791, acc: 69.53%] [G loss: 2.028785]\n",
      "epoch:23 step:21581 [D loss: 0.658097, acc: 60.94%] [G loss: 1.890252]\n",
      "epoch:23 step:21582 [D loss: 0.627409, acc: 65.62%] [G loss: 1.945570]\n",
      "epoch:23 step:21583 [D loss: 0.641394, acc: 64.06%] [G loss: 1.852270]\n",
      "epoch:23 step:21584 [D loss: 0.665924, acc: 65.62%] [G loss: 1.889656]\n",
      "epoch:23 step:21585 [D loss: 0.635348, acc: 66.41%] [G loss: 1.743683]\n",
      "epoch:23 step:21586 [D loss: 0.649993, acc: 57.81%] [G loss: 1.995791]\n",
      "epoch:23 step:21587 [D loss: 0.635200, acc: 67.19%] [G loss: 1.933631]\n",
      "epoch:23 step:21588 [D loss: 0.662797, acc: 61.72%] [G loss: 2.010494]\n",
      "epoch:23 step:21589 [D loss: 0.647404, acc: 64.84%] [G loss: 2.019444]\n",
      "epoch:23 step:21590 [D loss: 0.681425, acc: 57.81%] [G loss: 1.894887]\n",
      "epoch:23 step:21591 [D loss: 0.554883, acc: 70.31%] [G loss: 2.162803]\n",
      "epoch:23 step:21592 [D loss: 0.672377, acc: 55.47%] [G loss: 1.829409]\n",
      "epoch:23 step:21593 [D loss: 0.596438, acc: 68.75%] [G loss: 1.918862]\n",
      "epoch:23 step:21594 [D loss: 0.590149, acc: 71.09%] [G loss: 1.910474]\n",
      "epoch:23 step:21595 [D loss: 0.642444, acc: 64.84%] [G loss: 1.937719]\n",
      "epoch:23 step:21596 [D loss: 0.665536, acc: 61.72%] [G loss: 1.900231]\n",
      "epoch:23 step:21597 [D loss: 0.696446, acc: 53.91%] [G loss: 1.849445]\n",
      "epoch:23 step:21598 [D loss: 0.641984, acc: 60.94%] [G loss: 1.933939]\n",
      "epoch:23 step:21599 [D loss: 0.644624, acc: 64.06%] [G loss: 2.046927]\n",
      "epoch:23 step:21600 [D loss: 0.624489, acc: 71.09%] [G loss: 1.905578]\n",
      "##############\n",
      "[2.17767746 1.53408032 6.25959752 4.67798978 3.59532294 5.45715028\n",
      " 4.19879437 4.64709222 4.49322803 3.6942375 ]\n",
      "##########\n",
      "epoch:23 step:21601 [D loss: 0.685470, acc: 60.16%] [G loss: 1.959922]\n",
      "epoch:23 step:21602 [D loss: 0.687782, acc: 57.81%] [G loss: 1.858347]\n",
      "epoch:23 step:21603 [D loss: 0.637256, acc: 64.06%] [G loss: 1.879603]\n",
      "epoch:23 step:21604 [D loss: 0.644845, acc: 64.06%] [G loss: 1.780020]\n",
      "epoch:23 step:21605 [D loss: 0.621168, acc: 66.41%] [G loss: 1.903559]\n",
      "epoch:23 step:21606 [D loss: 0.669627, acc: 53.12%] [G loss: 2.087236]\n",
      "epoch:23 step:21607 [D loss: 0.633691, acc: 64.06%] [G loss: 1.969245]\n",
      "epoch:23 step:21608 [D loss: 0.656306, acc: 60.94%] [G loss: 1.894944]\n",
      "epoch:23 step:21609 [D loss: 0.660850, acc: 61.72%] [G loss: 1.740785]\n",
      "epoch:23 step:21610 [D loss: 0.748506, acc: 53.91%] [G loss: 1.807173]\n",
      "epoch:23 step:21611 [D loss: 0.666109, acc: 59.38%] [G loss: 1.783292]\n",
      "epoch:23 step:21612 [D loss: 0.647789, acc: 66.41%] [G loss: 1.775543]\n",
      "epoch:23 step:21613 [D loss: 0.633091, acc: 70.31%] [G loss: 1.818413]\n",
      "epoch:23 step:21614 [D loss: 0.689275, acc: 57.03%] [G loss: 1.733141]\n",
      "epoch:23 step:21615 [D loss: 0.632568, acc: 60.16%] [G loss: 1.890959]\n",
      "epoch:23 step:21616 [D loss: 0.656877, acc: 59.38%] [G loss: 1.901032]\n",
      "epoch:23 step:21617 [D loss: 0.652246, acc: 64.06%] [G loss: 1.757530]\n",
      "epoch:23 step:21618 [D loss: 0.651087, acc: 57.81%] [G loss: 1.870220]\n",
      "epoch:23 step:21619 [D loss: 0.650551, acc: 59.38%] [G loss: 1.819132]\n",
      "epoch:23 step:21620 [D loss: 0.566520, acc: 71.88%] [G loss: 1.942769]\n",
      "epoch:23 step:21621 [D loss: 0.625859, acc: 59.38%] [G loss: 1.832981]\n",
      "epoch:23 step:21622 [D loss: 0.655669, acc: 60.94%] [G loss: 1.828674]\n",
      "epoch:23 step:21623 [D loss: 0.671666, acc: 60.94%] [G loss: 1.922424]\n",
      "epoch:23 step:21624 [D loss: 0.630824, acc: 62.50%] [G loss: 1.822411]\n",
      "epoch:23 step:21625 [D loss: 0.612562, acc: 68.75%] [G loss: 1.927979]\n",
      "epoch:23 step:21626 [D loss: 0.644891, acc: 60.94%] [G loss: 1.847913]\n",
      "epoch:23 step:21627 [D loss: 0.598822, acc: 70.31%] [G loss: 2.015173]\n",
      "epoch:23 step:21628 [D loss: 0.650263, acc: 63.28%] [G loss: 1.918792]\n",
      "epoch:23 step:21629 [D loss: 0.659849, acc: 58.59%] [G loss: 1.877942]\n",
      "epoch:23 step:21630 [D loss: 0.656762, acc: 60.16%] [G loss: 1.872297]\n",
      "epoch:23 step:21631 [D loss: 0.660722, acc: 61.72%] [G loss: 1.801457]\n",
      "epoch:23 step:21632 [D loss: 0.723183, acc: 53.91%] [G loss: 1.662469]\n",
      "epoch:23 step:21633 [D loss: 0.668242, acc: 59.38%] [G loss: 1.774409]\n",
      "epoch:23 step:21634 [D loss: 0.629784, acc: 61.72%] [G loss: 1.784619]\n",
      "epoch:23 step:21635 [D loss: 0.650075, acc: 64.06%] [G loss: 1.812459]\n",
      "epoch:23 step:21636 [D loss: 0.622697, acc: 66.41%] [G loss: 1.827727]\n",
      "epoch:23 step:21637 [D loss: 0.635090, acc: 62.50%] [G loss: 1.823591]\n",
      "epoch:23 step:21638 [D loss: 0.626445, acc: 65.62%] [G loss: 1.805653]\n",
      "epoch:23 step:21639 [D loss: 0.643743, acc: 65.62%] [G loss: 1.973444]\n",
      "epoch:23 step:21640 [D loss: 0.590801, acc: 71.09%] [G loss: 2.045998]\n",
      "epoch:23 step:21641 [D loss: 0.652408, acc: 60.16%] [G loss: 1.892063]\n",
      "epoch:23 step:21642 [D loss: 0.664403, acc: 63.28%] [G loss: 1.814732]\n",
      "epoch:23 step:21643 [D loss: 0.665295, acc: 66.41%] [G loss: 1.897884]\n",
      "epoch:23 step:21644 [D loss: 0.644158, acc: 57.81%] [G loss: 1.981327]\n",
      "epoch:23 step:21645 [D loss: 0.637865, acc: 63.28%] [G loss: 1.844573]\n",
      "epoch:23 step:21646 [D loss: 0.655498, acc: 58.59%] [G loss: 1.890881]\n",
      "epoch:23 step:21647 [D loss: 0.631212, acc: 67.19%] [G loss: 1.890724]\n",
      "epoch:23 step:21648 [D loss: 0.640708, acc: 65.62%] [G loss: 1.878455]\n",
      "epoch:23 step:21649 [D loss: 0.686167, acc: 57.81%] [G loss: 1.827850]\n",
      "epoch:23 step:21650 [D loss: 0.654417, acc: 60.94%] [G loss: 1.881384]\n",
      "epoch:23 step:21651 [D loss: 0.648160, acc: 60.16%] [G loss: 1.901655]\n",
      "epoch:23 step:21652 [D loss: 0.609520, acc: 67.97%] [G loss: 1.898031]\n",
      "epoch:23 step:21653 [D loss: 0.703370, acc: 60.16%] [G loss: 1.800722]\n",
      "epoch:23 step:21654 [D loss: 0.642357, acc: 62.50%] [G loss: 1.923775]\n",
      "epoch:23 step:21655 [D loss: 0.645354, acc: 60.94%] [G loss: 1.869437]\n",
      "epoch:23 step:21656 [D loss: 0.724406, acc: 54.69%] [G loss: 1.838650]\n",
      "epoch:23 step:21657 [D loss: 0.630172, acc: 65.62%] [G loss: 1.965437]\n",
      "epoch:23 step:21658 [D loss: 0.623099, acc: 69.53%] [G loss: 2.050030]\n",
      "epoch:23 step:21659 [D loss: 0.673032, acc: 61.72%] [G loss: 1.813759]\n",
      "epoch:23 step:21660 [D loss: 0.687837, acc: 54.69%] [G loss: 1.727369]\n",
      "epoch:23 step:21661 [D loss: 0.699772, acc: 59.38%] [G loss: 1.845184]\n",
      "epoch:23 step:21662 [D loss: 0.635671, acc: 62.50%] [G loss: 1.989014]\n",
      "epoch:23 step:21663 [D loss: 0.597500, acc: 69.53%] [G loss: 1.918459]\n",
      "epoch:23 step:21664 [D loss: 0.612640, acc: 67.97%] [G loss: 2.018262]\n",
      "epoch:23 step:21665 [D loss: 0.625326, acc: 64.84%] [G loss: 2.050349]\n",
      "epoch:23 step:21666 [D loss: 0.556307, acc: 70.31%] [G loss: 2.239449]\n",
      "epoch:23 step:21667 [D loss: 0.601059, acc: 67.19%] [G loss: 1.949033]\n",
      "epoch:23 step:21668 [D loss: 0.653990, acc: 62.50%] [G loss: 2.054658]\n",
      "epoch:23 step:21669 [D loss: 0.661499, acc: 64.06%] [G loss: 2.076806]\n",
      "epoch:23 step:21670 [D loss: 0.584236, acc: 69.53%] [G loss: 2.214917]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:23 step:21671 [D loss: 0.704945, acc: 60.16%] [G loss: 2.046860]\n",
      "epoch:23 step:21672 [D loss: 0.655070, acc: 63.28%] [G loss: 1.972686]\n",
      "epoch:23 step:21673 [D loss: 0.613881, acc: 68.75%] [G loss: 2.051033]\n",
      "epoch:23 step:21674 [D loss: 0.650083, acc: 61.72%] [G loss: 2.029610]\n",
      "epoch:23 step:21675 [D loss: 0.670903, acc: 57.81%] [G loss: 1.839726]\n",
      "epoch:23 step:21676 [D loss: 0.656938, acc: 53.91%] [G loss: 1.751707]\n",
      "epoch:23 step:21677 [D loss: 0.630564, acc: 69.53%] [G loss: 1.900942]\n",
      "epoch:23 step:21678 [D loss: 0.622706, acc: 67.97%] [G loss: 1.885874]\n",
      "epoch:23 step:21679 [D loss: 0.666754, acc: 59.38%] [G loss: 1.941198]\n",
      "epoch:23 step:21680 [D loss: 0.688680, acc: 55.47%] [G loss: 1.744609]\n",
      "epoch:23 step:21681 [D loss: 0.608312, acc: 67.19%] [G loss: 2.040373]\n",
      "epoch:23 step:21682 [D loss: 0.602635, acc: 71.88%] [G loss: 2.032007]\n",
      "epoch:23 step:21683 [D loss: 0.666683, acc: 60.16%] [G loss: 1.823477]\n",
      "epoch:23 step:21684 [D loss: 0.701687, acc: 52.34%] [G loss: 1.805840]\n",
      "epoch:23 step:21685 [D loss: 0.649346, acc: 66.41%] [G loss: 1.821152]\n",
      "epoch:23 step:21686 [D loss: 0.620420, acc: 62.50%] [G loss: 1.926929]\n",
      "epoch:23 step:21687 [D loss: 0.636862, acc: 63.28%] [G loss: 1.798744]\n",
      "epoch:23 step:21688 [D loss: 0.625451, acc: 59.38%] [G loss: 1.691964]\n",
      "epoch:23 step:21689 [D loss: 0.621199, acc: 61.72%] [G loss: 1.781223]\n",
      "epoch:23 step:21690 [D loss: 0.695175, acc: 57.81%] [G loss: 1.903229]\n",
      "epoch:23 step:21691 [D loss: 0.660564, acc: 60.16%] [G loss: 1.824312]\n",
      "epoch:23 step:21692 [D loss: 0.661412, acc: 61.72%] [G loss: 1.732326]\n",
      "epoch:23 step:21693 [D loss: 0.651897, acc: 60.94%] [G loss: 1.841766]\n",
      "epoch:23 step:21694 [D loss: 0.675320, acc: 53.91%] [G loss: 1.831218]\n",
      "epoch:23 step:21695 [D loss: 0.619655, acc: 67.19%] [G loss: 1.967752]\n",
      "epoch:23 step:21696 [D loss: 0.639345, acc: 59.38%] [G loss: 1.912193]\n",
      "epoch:23 step:21697 [D loss: 0.622046, acc: 66.41%] [G loss: 2.109394]\n",
      "epoch:23 step:21698 [D loss: 0.613968, acc: 67.19%] [G loss: 1.807103]\n",
      "epoch:23 step:21699 [D loss: 0.667291, acc: 62.50%] [G loss: 1.673226]\n",
      "epoch:23 step:21700 [D loss: 0.664026, acc: 64.84%] [G loss: 1.876142]\n",
      "epoch:23 step:21701 [D loss: 0.614989, acc: 66.41%] [G loss: 1.899440]\n",
      "epoch:23 step:21702 [D loss: 0.639029, acc: 60.94%] [G loss: 1.991653]\n",
      "epoch:23 step:21703 [D loss: 0.693365, acc: 57.03%] [G loss: 1.926973]\n",
      "epoch:23 step:21704 [D loss: 0.661504, acc: 59.38%] [G loss: 1.887628]\n",
      "epoch:23 step:21705 [D loss: 0.641473, acc: 60.94%] [G loss: 2.008176]\n",
      "epoch:23 step:21706 [D loss: 0.655961, acc: 56.25%] [G loss: 1.810266]\n",
      "epoch:23 step:21707 [D loss: 0.652123, acc: 61.72%] [G loss: 1.877761]\n",
      "epoch:23 step:21708 [D loss: 0.640446, acc: 64.06%] [G loss: 1.801119]\n",
      "epoch:23 step:21709 [D loss: 0.634462, acc: 64.06%] [G loss: 1.779401]\n",
      "epoch:23 step:21710 [D loss: 0.680767, acc: 57.03%] [G loss: 1.886102]\n",
      "epoch:23 step:21711 [D loss: 0.681883, acc: 53.91%] [G loss: 1.662332]\n",
      "epoch:23 step:21712 [D loss: 0.654120, acc: 65.62%] [G loss: 1.809325]\n",
      "epoch:23 step:21713 [D loss: 0.642995, acc: 67.19%] [G loss: 1.855997]\n",
      "epoch:23 step:21714 [D loss: 0.612912, acc: 68.75%] [G loss: 1.823194]\n",
      "epoch:23 step:21715 [D loss: 0.606600, acc: 67.97%] [G loss: 1.877704]\n",
      "epoch:23 step:21716 [D loss: 0.629794, acc: 65.62%] [G loss: 1.932147]\n",
      "epoch:23 step:21717 [D loss: 0.625100, acc: 64.84%] [G loss: 1.965847]\n",
      "epoch:23 step:21718 [D loss: 0.620147, acc: 65.62%] [G loss: 1.878977]\n",
      "epoch:23 step:21719 [D loss: 0.631073, acc: 68.75%] [G loss: 1.859856]\n",
      "epoch:23 step:21720 [D loss: 0.672794, acc: 57.03%] [G loss: 1.863272]\n",
      "epoch:23 step:21721 [D loss: 0.660003, acc: 57.03%] [G loss: 1.841494]\n",
      "epoch:23 step:21722 [D loss: 0.675297, acc: 64.06%] [G loss: 1.767796]\n",
      "epoch:23 step:21723 [D loss: 0.606406, acc: 65.62%] [G loss: 1.824713]\n",
      "epoch:23 step:21724 [D loss: 0.669328, acc: 65.62%] [G loss: 1.717385]\n",
      "epoch:23 step:21725 [D loss: 0.650542, acc: 63.28%] [G loss: 1.887219]\n",
      "epoch:23 step:21726 [D loss: 0.660426, acc: 59.38%] [G loss: 1.744667]\n",
      "epoch:23 step:21727 [D loss: 0.663650, acc: 60.16%] [G loss: 1.793990]\n",
      "epoch:23 step:21728 [D loss: 0.649165, acc: 57.03%] [G loss: 1.806539]\n",
      "epoch:23 step:21729 [D loss: 0.651531, acc: 59.38%] [G loss: 1.825580]\n",
      "epoch:23 step:21730 [D loss: 0.673082, acc: 58.59%] [G loss: 1.864037]\n",
      "epoch:23 step:21731 [D loss: 0.651031, acc: 60.16%] [G loss: 1.823179]\n",
      "epoch:23 step:21732 [D loss: 0.662763, acc: 61.72%] [G loss: 1.814176]\n",
      "epoch:23 step:21733 [D loss: 0.696233, acc: 55.47%] [G loss: 1.779707]\n",
      "epoch:23 step:21734 [D loss: 0.621325, acc: 65.62%] [G loss: 1.895672]\n",
      "epoch:23 step:21735 [D loss: 0.606569, acc: 67.97%] [G loss: 1.811206]\n",
      "epoch:23 step:21736 [D loss: 0.626150, acc: 62.50%] [G loss: 1.895227]\n",
      "epoch:23 step:21737 [D loss: 0.682647, acc: 59.38%] [G loss: 1.904688]\n",
      "epoch:23 step:21738 [D loss: 0.676430, acc: 62.50%] [G loss: 2.011615]\n",
      "epoch:23 step:21739 [D loss: 0.682566, acc: 58.59%] [G loss: 2.041943]\n",
      "epoch:23 step:21740 [D loss: 0.655647, acc: 60.16%] [G loss: 1.831690]\n",
      "epoch:23 step:21741 [D loss: 0.615840, acc: 62.50%] [G loss: 1.870394]\n",
      "epoch:23 step:21742 [D loss: 0.632927, acc: 65.62%] [G loss: 1.938362]\n",
      "epoch:23 step:21743 [D loss: 0.623811, acc: 67.19%] [G loss: 1.917711]\n",
      "epoch:23 step:21744 [D loss: 0.648926, acc: 64.84%] [G loss: 1.854744]\n",
      "epoch:23 step:21745 [D loss: 0.630267, acc: 63.28%] [G loss: 2.079696]\n",
      "epoch:23 step:21746 [D loss: 0.636408, acc: 65.62%] [G loss: 1.892404]\n",
      "epoch:23 step:21747 [D loss: 0.659464, acc: 60.16%] [G loss: 1.793519]\n",
      "epoch:23 step:21748 [D loss: 0.678911, acc: 55.47%] [G loss: 1.837525]\n",
      "epoch:23 step:21749 [D loss: 0.644049, acc: 64.06%] [G loss: 1.878680]\n",
      "epoch:23 step:21750 [D loss: 0.632071, acc: 61.72%] [G loss: 1.884587]\n",
      "epoch:23 step:21751 [D loss: 0.697584, acc: 56.25%] [G loss: 1.846537]\n",
      "epoch:23 step:21752 [D loss: 0.594334, acc: 68.75%] [G loss: 1.980358]\n",
      "epoch:23 step:21753 [D loss: 0.678675, acc: 56.25%] [G loss: 1.804154]\n",
      "epoch:23 step:21754 [D loss: 0.614666, acc: 62.50%] [G loss: 1.897423]\n",
      "epoch:23 step:21755 [D loss: 0.713542, acc: 57.03%] [G loss: 1.883331]\n",
      "epoch:23 step:21756 [D loss: 0.642689, acc: 64.06%] [G loss: 1.920994]\n",
      "epoch:23 step:21757 [D loss: 0.606957, acc: 65.62%] [G loss: 2.013320]\n",
      "epoch:23 step:21758 [D loss: 0.621340, acc: 59.38%] [G loss: 2.096778]\n",
      "epoch:23 step:21759 [D loss: 0.607773, acc: 69.53%] [G loss: 1.965049]\n",
      "epoch:23 step:21760 [D loss: 0.577277, acc: 70.31%] [G loss: 2.194751]\n",
      "epoch:23 step:21761 [D loss: 0.720430, acc: 57.03%] [G loss: 1.923278]\n",
      "epoch:23 step:21762 [D loss: 0.686522, acc: 57.81%] [G loss: 1.794166]\n",
      "epoch:23 step:21763 [D loss: 0.662989, acc: 54.69%] [G loss: 1.770495]\n",
      "epoch:23 step:21764 [D loss: 0.644388, acc: 65.62%] [G loss: 1.795226]\n",
      "epoch:23 step:21765 [D loss: 0.673062, acc: 57.03%] [G loss: 1.801424]\n",
      "epoch:23 step:21766 [D loss: 0.651620, acc: 65.62%] [G loss: 1.822948]\n",
      "epoch:23 step:21767 [D loss: 0.596514, acc: 67.19%] [G loss: 1.958439]\n",
      "epoch:23 step:21768 [D loss: 0.674665, acc: 56.25%] [G loss: 1.940326]\n",
      "epoch:23 step:21769 [D loss: 0.572630, acc: 76.56%] [G loss: 2.119059]\n",
      "epoch:23 step:21770 [D loss: 0.666373, acc: 59.38%] [G loss: 2.086651]\n",
      "epoch:23 step:21771 [D loss: 0.684347, acc: 55.47%] [G loss: 1.825428]\n",
      "epoch:23 step:21772 [D loss: 0.671660, acc: 64.84%] [G loss: 1.955389]\n",
      "epoch:23 step:21773 [D loss: 0.614154, acc: 66.41%] [G loss: 1.961401]\n",
      "epoch:23 step:21774 [D loss: 0.651955, acc: 60.16%] [G loss: 2.040323]\n",
      "epoch:23 step:21775 [D loss: 0.654965, acc: 63.28%] [G loss: 1.829051]\n",
      "epoch:23 step:21776 [D loss: 0.693754, acc: 56.25%] [G loss: 1.900729]\n",
      "epoch:23 step:21777 [D loss: 0.652390, acc: 60.94%] [G loss: 1.872459]\n",
      "epoch:23 step:21778 [D loss: 0.646176, acc: 64.06%] [G loss: 1.816118]\n",
      "epoch:23 step:21779 [D loss: 0.675477, acc: 56.25%] [G loss: 1.849241]\n",
      "epoch:23 step:21780 [D loss: 0.593527, acc: 67.97%] [G loss: 2.219574]\n",
      "epoch:23 step:21781 [D loss: 0.606118, acc: 64.84%] [G loss: 2.048033]\n",
      "epoch:23 step:21782 [D loss: 0.593730, acc: 67.97%] [G loss: 2.303181]\n",
      "epoch:23 step:21783 [D loss: 0.602403, acc: 66.41%] [G loss: 2.260667]\n",
      "epoch:23 step:21784 [D loss: 0.669020, acc: 57.81%] [G loss: 1.824324]\n",
      "epoch:23 step:21785 [D loss: 0.642332, acc: 61.72%] [G loss: 1.852883]\n",
      "epoch:23 step:21786 [D loss: 0.706336, acc: 52.34%] [G loss: 1.971483]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:23 step:21787 [D loss: 0.597627, acc: 67.19%] [G loss: 1.927095]\n",
      "epoch:23 step:21788 [D loss: 0.689647, acc: 57.81%] [G loss: 1.816129]\n",
      "epoch:23 step:21789 [D loss: 0.621077, acc: 65.62%] [G loss: 1.967229]\n",
      "epoch:23 step:21790 [D loss: 0.675457, acc: 54.69%] [G loss: 1.890094]\n",
      "epoch:23 step:21791 [D loss: 0.630191, acc: 61.72%] [G loss: 1.838113]\n",
      "epoch:23 step:21792 [D loss: 0.613144, acc: 67.97%] [G loss: 1.900469]\n",
      "epoch:23 step:21793 [D loss: 0.619141, acc: 68.75%] [G loss: 1.962093]\n",
      "epoch:23 step:21794 [D loss: 0.614803, acc: 63.28%] [G loss: 1.890264]\n",
      "epoch:23 step:21795 [D loss: 0.687438, acc: 58.59%] [G loss: 1.957368]\n",
      "epoch:23 step:21796 [D loss: 0.651866, acc: 57.03%] [G loss: 1.857173]\n",
      "epoch:23 step:21797 [D loss: 0.637705, acc: 65.62%] [G loss: 1.898957]\n",
      "epoch:23 step:21798 [D loss: 0.686038, acc: 58.59%] [G loss: 1.889028]\n",
      "epoch:23 step:21799 [D loss: 0.642967, acc: 61.72%] [G loss: 2.036153]\n",
      "epoch:23 step:21800 [D loss: 0.698299, acc: 54.69%] [G loss: 1.797117]\n",
      "##############\n",
      "[2.4101476  1.37910051 6.1140226  4.6730784  3.63895602 5.40408754\n",
      " 4.25266888 4.64582205 4.69741625 3.58655458]\n",
      "##########\n",
      "epoch:23 step:21801 [D loss: 0.716949, acc: 53.91%] [G loss: 1.744608]\n",
      "epoch:23 step:21802 [D loss: 0.674130, acc: 57.03%] [G loss: 1.779860]\n",
      "epoch:23 step:21803 [D loss: 0.645716, acc: 60.16%] [G loss: 1.782589]\n",
      "epoch:23 step:21804 [D loss: 0.610271, acc: 65.62%] [G loss: 1.842005]\n",
      "epoch:23 step:21805 [D loss: 0.629692, acc: 64.06%] [G loss: 1.834018]\n",
      "epoch:23 step:21806 [D loss: 0.655484, acc: 65.62%] [G loss: 1.843588]\n",
      "epoch:23 step:21807 [D loss: 0.626280, acc: 67.19%] [G loss: 1.965669]\n",
      "epoch:23 step:21808 [D loss: 0.673949, acc: 55.47%] [G loss: 1.758101]\n",
      "epoch:23 step:21809 [D loss: 0.676841, acc: 56.25%] [G loss: 1.892787]\n",
      "epoch:23 step:21810 [D loss: 0.714861, acc: 54.69%] [G loss: 1.972859]\n",
      "epoch:23 step:21811 [D loss: 0.644334, acc: 62.50%] [G loss: 1.801978]\n",
      "epoch:23 step:21812 [D loss: 0.624397, acc: 66.41%] [G loss: 1.895655]\n",
      "epoch:23 step:21813 [D loss: 0.648216, acc: 57.81%] [G loss: 1.959926]\n",
      "epoch:23 step:21814 [D loss: 0.645276, acc: 61.72%] [G loss: 2.026797]\n",
      "epoch:23 step:21815 [D loss: 0.641615, acc: 62.50%] [G loss: 1.963191]\n",
      "epoch:23 step:21816 [D loss: 0.617496, acc: 68.75%] [G loss: 1.895375]\n",
      "epoch:23 step:21817 [D loss: 0.714458, acc: 53.91%] [G loss: 1.858514]\n",
      "epoch:23 step:21818 [D loss: 0.623781, acc: 67.97%] [G loss: 1.779855]\n",
      "epoch:23 step:21819 [D loss: 0.667039, acc: 60.94%] [G loss: 1.718569]\n",
      "epoch:23 step:21820 [D loss: 0.619515, acc: 67.19%] [G loss: 1.981987]\n",
      "epoch:23 step:21821 [D loss: 0.670646, acc: 58.59%] [G loss: 1.954888]\n",
      "epoch:23 step:21822 [D loss: 0.602187, acc: 67.97%] [G loss: 1.898807]\n",
      "epoch:23 step:21823 [D loss: 0.641403, acc: 57.81%] [G loss: 1.982128]\n",
      "epoch:23 step:21824 [D loss: 0.621994, acc: 66.41%] [G loss: 1.752231]\n",
      "epoch:23 step:21825 [D loss: 0.643210, acc: 66.41%] [G loss: 1.935905]\n",
      "epoch:23 step:21826 [D loss: 0.604659, acc: 66.41%] [G loss: 1.904098]\n",
      "epoch:23 step:21827 [D loss: 0.611121, acc: 65.62%] [G loss: 2.085946]\n",
      "epoch:23 step:21828 [D loss: 0.644616, acc: 60.94%] [G loss: 1.971536]\n",
      "epoch:23 step:21829 [D loss: 0.615814, acc: 67.97%] [G loss: 1.858078]\n",
      "epoch:23 step:21830 [D loss: 0.692633, acc: 59.38%] [G loss: 1.878331]\n",
      "epoch:23 step:21831 [D loss: 0.608951, acc: 68.75%] [G loss: 2.029772]\n",
      "epoch:23 step:21832 [D loss: 0.679559, acc: 62.50%] [G loss: 1.820816]\n",
      "epoch:23 step:21833 [D loss: 0.635792, acc: 63.28%] [G loss: 1.974158]\n",
      "epoch:23 step:21834 [D loss: 0.646862, acc: 58.59%] [G loss: 1.908155]\n",
      "epoch:23 step:21835 [D loss: 0.628014, acc: 64.84%] [G loss: 1.816608]\n",
      "epoch:23 step:21836 [D loss: 0.663892, acc: 56.25%] [G loss: 1.849580]\n",
      "epoch:23 step:21837 [D loss: 0.657686, acc: 61.72%] [G loss: 1.859888]\n",
      "epoch:23 step:21838 [D loss: 0.649225, acc: 57.03%] [G loss: 1.765484]\n",
      "epoch:23 step:21839 [D loss: 0.710376, acc: 53.12%] [G loss: 1.800674]\n",
      "epoch:23 step:21840 [D loss: 0.633193, acc: 64.06%] [G loss: 1.855664]\n",
      "epoch:23 step:21841 [D loss: 0.654848, acc: 67.19%] [G loss: 1.748543]\n",
      "epoch:23 step:21842 [D loss: 0.633069, acc: 64.84%] [G loss: 1.860063]\n",
      "epoch:23 step:21843 [D loss: 0.665161, acc: 60.16%] [G loss: 1.976465]\n",
      "epoch:23 step:21844 [D loss: 0.599154, acc: 68.75%] [G loss: 1.834706]\n",
      "epoch:23 step:21845 [D loss: 0.627254, acc: 64.84%] [G loss: 1.892069]\n",
      "epoch:23 step:21846 [D loss: 0.648042, acc: 60.94%] [G loss: 1.931733]\n",
      "epoch:23 step:21847 [D loss: 0.667541, acc: 54.69%] [G loss: 1.933356]\n",
      "epoch:23 step:21848 [D loss: 0.652039, acc: 62.50%] [G loss: 1.851588]\n",
      "epoch:23 step:21849 [D loss: 0.618314, acc: 65.62%] [G loss: 2.026608]\n",
      "epoch:23 step:21850 [D loss: 0.624941, acc: 60.94%] [G loss: 1.890014]\n",
      "epoch:23 step:21851 [D loss: 0.575071, acc: 67.97%] [G loss: 1.966350]\n",
      "epoch:23 step:21852 [D loss: 0.648796, acc: 61.72%] [G loss: 1.947943]\n",
      "epoch:23 step:21853 [D loss: 0.683052, acc: 59.38%] [G loss: 1.980230]\n",
      "epoch:23 step:21854 [D loss: 0.617979, acc: 62.50%] [G loss: 2.050596]\n",
      "epoch:23 step:21855 [D loss: 0.626747, acc: 64.84%] [G loss: 1.777985]\n",
      "epoch:23 step:21856 [D loss: 0.630705, acc: 64.06%] [G loss: 1.982528]\n",
      "epoch:23 step:21857 [D loss: 0.651585, acc: 63.28%] [G loss: 1.912438]\n",
      "epoch:23 step:21858 [D loss: 0.679512, acc: 64.84%] [G loss: 1.895833]\n",
      "epoch:23 step:21859 [D loss: 0.667221, acc: 60.94%] [G loss: 1.890621]\n",
      "epoch:23 step:21860 [D loss: 0.618540, acc: 67.19%] [G loss: 1.949302]\n",
      "epoch:23 step:21861 [D loss: 0.631157, acc: 60.16%] [G loss: 1.809863]\n",
      "epoch:23 step:21862 [D loss: 0.610514, acc: 66.41%] [G loss: 1.938381]\n",
      "epoch:23 step:21863 [D loss: 0.564466, acc: 70.31%] [G loss: 2.184895]\n",
      "epoch:23 step:21864 [D loss: 0.574564, acc: 69.53%] [G loss: 2.250463]\n",
      "epoch:23 step:21865 [D loss: 0.551440, acc: 76.56%] [G loss: 2.040220]\n",
      "epoch:23 step:21866 [D loss: 0.588484, acc: 66.41%] [G loss: 2.277027]\n",
      "epoch:23 step:21867 [D loss: 0.645120, acc: 65.62%] [G loss: 1.696808]\n",
      "epoch:23 step:21868 [D loss: 0.684981, acc: 59.38%] [G loss: 1.942715]\n",
      "epoch:23 step:21869 [D loss: 0.622141, acc: 62.50%] [G loss: 1.994268]\n",
      "epoch:23 step:21870 [D loss: 0.646698, acc: 64.06%] [G loss: 1.839412]\n",
      "epoch:23 step:21871 [D loss: 0.685338, acc: 57.03%] [G loss: 1.849638]\n",
      "epoch:23 step:21872 [D loss: 0.615866, acc: 65.62%] [G loss: 2.013420]\n",
      "epoch:23 step:21873 [D loss: 0.667058, acc: 60.94%] [G loss: 2.020518]\n",
      "epoch:23 step:21874 [D loss: 0.729986, acc: 50.78%] [G loss: 1.851960]\n",
      "epoch:23 step:21875 [D loss: 0.668116, acc: 61.72%] [G loss: 1.721169]\n",
      "epoch:23 step:21876 [D loss: 0.662023, acc: 60.94%] [G loss: 1.871163]\n",
      "epoch:23 step:21877 [D loss: 0.648497, acc: 62.50%] [G loss: 1.896546]\n",
      "epoch:23 step:21878 [D loss: 0.684949, acc: 62.50%] [G loss: 1.911170]\n",
      "epoch:23 step:21879 [D loss: 0.660366, acc: 64.84%] [G loss: 1.839650]\n",
      "epoch:23 step:21880 [D loss: 0.638631, acc: 63.28%] [G loss: 1.915670]\n",
      "epoch:23 step:21881 [D loss: 0.643209, acc: 63.28%] [G loss: 1.877606]\n",
      "epoch:23 step:21882 [D loss: 0.610913, acc: 64.84%] [G loss: 1.845003]\n",
      "epoch:23 step:21883 [D loss: 0.638073, acc: 64.06%] [G loss: 1.918895]\n",
      "epoch:23 step:21884 [D loss: 0.681936, acc: 59.38%] [G loss: 1.978303]\n",
      "epoch:23 step:21885 [D loss: 0.675222, acc: 57.81%] [G loss: 1.987315]\n",
      "epoch:23 step:21886 [D loss: 0.623897, acc: 66.41%] [G loss: 1.914783]\n",
      "epoch:23 step:21887 [D loss: 0.654934, acc: 60.94%] [G loss: 1.969248]\n",
      "epoch:23 step:21888 [D loss: 0.632929, acc: 62.50%] [G loss: 2.048241]\n",
      "epoch:23 step:21889 [D loss: 0.659845, acc: 61.72%] [G loss: 1.823363]\n",
      "epoch:23 step:21890 [D loss: 0.670102, acc: 60.94%] [G loss: 1.953462]\n",
      "epoch:23 step:21891 [D loss: 0.609400, acc: 64.06%] [G loss: 1.937959]\n",
      "epoch:23 step:21892 [D loss: 0.661631, acc: 59.38%] [G loss: 1.797310]\n",
      "epoch:23 step:21893 [D loss: 0.702611, acc: 53.12%] [G loss: 1.838444]\n",
      "epoch:23 step:21894 [D loss: 0.684783, acc: 58.59%] [G loss: 1.844824]\n",
      "epoch:23 step:21895 [D loss: 0.661458, acc: 62.50%] [G loss: 1.882998]\n",
      "epoch:23 step:21896 [D loss: 0.609686, acc: 61.72%] [G loss: 2.018204]\n",
      "epoch:23 step:21897 [D loss: 0.591783, acc: 72.66%] [G loss: 2.161640]\n",
      "epoch:23 step:21898 [D loss: 0.537394, acc: 75.78%] [G loss: 2.273122]\n",
      "epoch:23 step:21899 [D loss: 0.666339, acc: 60.16%] [G loss: 1.776976]\n",
      "epoch:23 step:21900 [D loss: 0.695978, acc: 58.59%] [G loss: 1.734131]\n",
      "epoch:23 step:21901 [D loss: 0.633748, acc: 64.06%] [G loss: 1.964207]\n",
      "epoch:23 step:21902 [D loss: 0.649133, acc: 65.62%] [G loss: 1.879810]\n",
      "epoch:23 step:21903 [D loss: 0.691892, acc: 56.25%] [G loss: 1.807932]\n",
      "epoch:23 step:21904 [D loss: 0.677677, acc: 60.16%] [G loss: 1.820543]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:23 step:21905 [D loss: 0.613150, acc: 68.75%] [G loss: 2.057034]\n",
      "epoch:23 step:21906 [D loss: 0.631880, acc: 62.50%] [G loss: 1.926452]\n",
      "epoch:23 step:21907 [D loss: 0.676138, acc: 57.03%] [G loss: 1.947570]\n",
      "epoch:23 step:21908 [D loss: 0.601940, acc: 68.75%] [G loss: 1.928186]\n",
      "epoch:23 step:21909 [D loss: 0.676701, acc: 57.81%] [G loss: 2.131693]\n",
      "epoch:23 step:21910 [D loss: 0.615155, acc: 68.75%] [G loss: 2.067860]\n",
      "epoch:23 step:21911 [D loss: 0.602165, acc: 64.06%] [G loss: 2.012459]\n",
      "epoch:23 step:21912 [D loss: 0.637225, acc: 62.50%] [G loss: 1.791615]\n",
      "epoch:23 step:21913 [D loss: 0.636093, acc: 60.16%] [G loss: 1.734705]\n",
      "epoch:23 step:21914 [D loss: 0.692568, acc: 60.94%] [G loss: 1.868736]\n",
      "epoch:23 step:21915 [D loss: 0.661740, acc: 59.38%] [G loss: 1.997682]\n",
      "epoch:23 step:21916 [D loss: 0.657463, acc: 61.72%] [G loss: 1.811670]\n",
      "epoch:23 step:21917 [D loss: 0.633317, acc: 64.84%] [G loss: 1.747901]\n",
      "epoch:23 step:21918 [D loss: 0.625415, acc: 62.50%] [G loss: 1.944024]\n",
      "epoch:23 step:21919 [D loss: 0.650148, acc: 60.94%] [G loss: 1.844415]\n",
      "epoch:23 step:21920 [D loss: 0.656199, acc: 65.62%] [G loss: 2.060028]\n",
      "epoch:23 step:21921 [D loss: 0.622784, acc: 64.84%] [G loss: 1.962847]\n",
      "epoch:23 step:21922 [D loss: 0.618340, acc: 66.41%] [G loss: 2.046129]\n",
      "epoch:23 step:21923 [D loss: 0.622186, acc: 63.28%] [G loss: 1.868221]\n",
      "epoch:23 step:21924 [D loss: 0.645336, acc: 60.94%] [G loss: 1.806993]\n",
      "epoch:23 step:21925 [D loss: 0.632748, acc: 63.28%] [G loss: 2.057011]\n",
      "epoch:23 step:21926 [D loss: 0.676385, acc: 58.59%] [G loss: 1.854212]\n",
      "epoch:23 step:21927 [D loss: 0.670779, acc: 54.69%] [G loss: 1.836516]\n",
      "epoch:23 step:21928 [D loss: 0.678739, acc: 59.38%] [G loss: 1.706791]\n",
      "epoch:23 step:21929 [D loss: 0.697745, acc: 63.28%] [G loss: 1.797799]\n",
      "epoch:23 step:21930 [D loss: 0.593621, acc: 70.31%] [G loss: 1.775773]\n",
      "epoch:23 step:21931 [D loss: 0.610141, acc: 64.06%] [G loss: 1.914254]\n",
      "epoch:23 step:21932 [D loss: 0.616049, acc: 71.09%] [G loss: 2.038738]\n",
      "epoch:23 step:21933 [D loss: 0.641353, acc: 65.62%] [G loss: 1.873622]\n",
      "epoch:23 step:21934 [D loss: 0.615138, acc: 65.62%] [G loss: 1.895020]\n",
      "epoch:23 step:21935 [D loss: 0.596305, acc: 71.09%] [G loss: 1.906943]\n",
      "epoch:23 step:21936 [D loss: 0.587529, acc: 68.75%] [G loss: 1.958026]\n",
      "epoch:23 step:21937 [D loss: 0.654946, acc: 64.84%] [G loss: 1.794318]\n",
      "epoch:23 step:21938 [D loss: 0.709069, acc: 49.22%] [G loss: 1.842626]\n",
      "epoch:23 step:21939 [D loss: 0.653056, acc: 60.16%] [G loss: 1.815852]\n",
      "epoch:23 step:21940 [D loss: 0.619271, acc: 66.41%] [G loss: 1.794107]\n",
      "epoch:23 step:21941 [D loss: 0.644743, acc: 66.41%] [G loss: 1.910937]\n",
      "epoch:23 step:21942 [D loss: 0.653737, acc: 63.28%] [G loss: 1.891417]\n",
      "epoch:23 step:21943 [D loss: 0.606617, acc: 67.19%] [G loss: 1.941161]\n",
      "epoch:23 step:21944 [D loss: 0.649932, acc: 63.28%] [G loss: 1.780920]\n",
      "epoch:23 step:21945 [D loss: 0.702859, acc: 57.03%] [G loss: 1.832088]\n",
      "epoch:23 step:21946 [D loss: 0.631210, acc: 63.28%] [G loss: 1.975014]\n",
      "epoch:23 step:21947 [D loss: 0.654529, acc: 67.19%] [G loss: 1.823271]\n",
      "epoch:23 step:21948 [D loss: 0.690139, acc: 57.03%] [G loss: 1.742006]\n",
      "epoch:23 step:21949 [D loss: 0.636112, acc: 63.28%] [G loss: 1.944038]\n",
      "epoch:23 step:21950 [D loss: 0.655801, acc: 62.50%] [G loss: 1.869869]\n",
      "epoch:23 step:21951 [D loss: 0.705147, acc: 52.34%] [G loss: 1.882811]\n",
      "epoch:23 step:21952 [D loss: 0.634439, acc: 60.16%] [G loss: 1.841818]\n",
      "epoch:23 step:21953 [D loss: 0.640229, acc: 62.50%] [G loss: 1.770427]\n",
      "epoch:23 step:21954 [D loss: 0.615194, acc: 66.41%] [G loss: 1.970508]\n",
      "epoch:23 step:21955 [D loss: 0.584926, acc: 67.97%] [G loss: 1.998707]\n",
      "epoch:23 step:21956 [D loss: 0.601352, acc: 64.84%] [G loss: 2.028433]\n",
      "epoch:23 step:21957 [D loss: 0.633728, acc: 63.28%] [G loss: 2.124584]\n",
      "epoch:23 step:21958 [D loss: 0.706439, acc: 56.25%] [G loss: 2.056314]\n",
      "epoch:23 step:21959 [D loss: 0.716724, acc: 53.12%] [G loss: 1.778003]\n",
      "epoch:23 step:21960 [D loss: 0.598428, acc: 68.75%] [G loss: 2.025542]\n",
      "epoch:23 step:21961 [D loss: 0.646424, acc: 62.50%] [G loss: 1.829143]\n",
      "epoch:23 step:21962 [D loss: 0.679318, acc: 67.97%] [G loss: 1.919034]\n",
      "epoch:23 step:21963 [D loss: 0.594695, acc: 64.06%] [G loss: 1.911860]\n",
      "epoch:23 step:21964 [D loss: 0.652510, acc: 58.59%] [G loss: 1.998912]\n",
      "epoch:23 step:21965 [D loss: 0.596591, acc: 70.31%] [G loss: 2.046154]\n",
      "epoch:23 step:21966 [D loss: 0.646288, acc: 64.84%] [G loss: 2.014746]\n",
      "epoch:23 step:21967 [D loss: 0.550201, acc: 75.78%] [G loss: 2.122845]\n",
      "epoch:23 step:21968 [D loss: 0.650392, acc: 61.72%] [G loss: 2.011482]\n",
      "epoch:23 step:21969 [D loss: 0.652608, acc: 58.59%] [G loss: 1.857391]\n",
      "epoch:23 step:21970 [D loss: 0.623518, acc: 62.50%] [G loss: 1.960755]\n",
      "epoch:23 step:21971 [D loss: 0.655641, acc: 60.16%] [G loss: 1.851513]\n",
      "epoch:23 step:21972 [D loss: 0.668676, acc: 60.94%] [G loss: 1.839570]\n",
      "epoch:23 step:21973 [D loss: 0.603926, acc: 67.19%] [G loss: 1.857523]\n",
      "epoch:23 step:21974 [D loss: 0.695496, acc: 57.03%] [G loss: 1.786959]\n",
      "epoch:23 step:21975 [D loss: 0.614351, acc: 65.62%] [G loss: 1.889393]\n",
      "epoch:23 step:21976 [D loss: 0.659829, acc: 57.03%] [G loss: 1.995846]\n",
      "epoch:23 step:21977 [D loss: 0.643218, acc: 60.94%] [G loss: 1.978014]\n",
      "epoch:23 step:21978 [D loss: 0.564492, acc: 71.09%] [G loss: 2.147655]\n",
      "epoch:23 step:21979 [D loss: 0.614984, acc: 63.28%] [G loss: 1.956616]\n",
      "epoch:23 step:21980 [D loss: 0.609991, acc: 64.06%] [G loss: 2.022936]\n",
      "epoch:23 step:21981 [D loss: 0.599541, acc: 64.84%] [G loss: 2.170590]\n",
      "epoch:23 step:21982 [D loss: 0.664213, acc: 60.16%] [G loss: 2.063567]\n",
      "epoch:23 step:21983 [D loss: 0.701286, acc: 60.16%] [G loss: 1.781086]\n",
      "epoch:23 step:21984 [D loss: 0.722203, acc: 53.12%] [G loss: 1.881071]\n",
      "epoch:23 step:21985 [D loss: 0.597589, acc: 68.75%] [G loss: 2.024952]\n",
      "epoch:23 step:21986 [D loss: 0.591383, acc: 69.53%] [G loss: 1.863033]\n",
      "epoch:23 step:21987 [D loss: 0.630612, acc: 64.06%] [G loss: 1.890109]\n",
      "epoch:23 step:21988 [D loss: 0.713684, acc: 55.47%] [G loss: 1.720022]\n",
      "epoch:23 step:21989 [D loss: 0.649512, acc: 60.94%] [G loss: 1.849212]\n",
      "epoch:23 step:21990 [D loss: 0.667254, acc: 55.47%] [G loss: 1.780352]\n",
      "epoch:23 step:21991 [D loss: 0.645053, acc: 63.28%] [G loss: 1.923664]\n",
      "epoch:23 step:21992 [D loss: 0.665017, acc: 60.94%] [G loss: 1.918504]\n",
      "epoch:23 step:21993 [D loss: 0.689392, acc: 60.16%] [G loss: 1.802253]\n",
      "epoch:23 step:21994 [D loss: 0.646390, acc: 64.84%] [G loss: 1.851851]\n",
      "epoch:23 step:21995 [D loss: 0.649050, acc: 64.06%] [G loss: 1.823611]\n",
      "epoch:23 step:21996 [D loss: 0.649705, acc: 63.28%] [G loss: 1.749834]\n",
      "epoch:23 step:21997 [D loss: 0.707577, acc: 53.91%] [G loss: 1.726024]\n",
      "epoch:23 step:21998 [D loss: 0.635008, acc: 62.50%] [G loss: 1.876197]\n",
      "epoch:23 step:21999 [D loss: 0.740502, acc: 53.91%] [G loss: 1.750868]\n",
      "epoch:23 step:22000 [D loss: 0.643866, acc: 62.50%] [G loss: 1.813011]\n",
      "##############\n",
      "[2.40157037 1.55268166 6.4352309  4.80671606 3.60976451 5.58883987\n",
      " 4.39380217 4.7701582  4.5961828  3.66513525]\n",
      "##########\n",
      "epoch:23 step:22001 [D loss: 0.648419, acc: 60.16%] [G loss: 1.831508]\n",
      "epoch:23 step:22002 [D loss: 0.611881, acc: 67.97%] [G loss: 1.839193]\n",
      "epoch:23 step:22003 [D loss: 0.634428, acc: 66.41%] [G loss: 1.864568]\n",
      "epoch:23 step:22004 [D loss: 0.634257, acc: 64.06%] [G loss: 1.997736]\n",
      "epoch:23 step:22005 [D loss: 0.636725, acc: 62.50%] [G loss: 1.822569]\n",
      "epoch:23 step:22006 [D loss: 0.626448, acc: 63.28%] [G loss: 1.918239]\n",
      "epoch:23 step:22007 [D loss: 0.628599, acc: 62.50%] [G loss: 1.914732]\n",
      "epoch:23 step:22008 [D loss: 0.580344, acc: 67.19%] [G loss: 2.040596]\n",
      "epoch:23 step:22009 [D loss: 0.673028, acc: 59.38%] [G loss: 1.739699]\n",
      "epoch:23 step:22010 [D loss: 0.720156, acc: 52.34%] [G loss: 1.770413]\n",
      "epoch:23 step:22011 [D loss: 0.699421, acc: 50.78%] [G loss: 1.665241]\n",
      "epoch:23 step:22012 [D loss: 0.723462, acc: 50.78%] [G loss: 1.780883]\n",
      "epoch:23 step:22013 [D loss: 0.606049, acc: 74.22%] [G loss: 1.809098]\n",
      "epoch:23 step:22014 [D loss: 0.622527, acc: 64.84%] [G loss: 2.017610]\n",
      "epoch:23 step:22015 [D loss: 0.639567, acc: 65.62%] [G loss: 1.786473]\n",
      "epoch:23 step:22016 [D loss: 0.664197, acc: 59.38%] [G loss: 1.810821]\n",
      "epoch:23 step:22017 [D loss: 0.658724, acc: 60.94%] [G loss: 1.886445]\n",
      "epoch:23 step:22018 [D loss: 0.641800, acc: 61.72%] [G loss: 1.855478]\n",
      "epoch:23 step:22019 [D loss: 0.655403, acc: 57.03%] [G loss: 1.852962]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:23 step:22020 [D loss: 0.596365, acc: 65.62%] [G loss: 2.038897]\n",
      "epoch:23 step:22021 [D loss: 0.637228, acc: 64.06%] [G loss: 1.918571]\n",
      "epoch:23 step:22022 [D loss: 0.573203, acc: 71.88%] [G loss: 2.250758]\n",
      "epoch:23 step:22023 [D loss: 0.672566, acc: 61.72%] [G loss: 2.037580]\n",
      "epoch:23 step:22024 [D loss: 0.683022, acc: 60.16%] [G loss: 1.751002]\n",
      "epoch:23 step:22025 [D loss: 0.700277, acc: 57.03%] [G loss: 1.993580]\n",
      "epoch:23 step:22026 [D loss: 0.668034, acc: 57.81%] [G loss: 1.921267]\n",
      "epoch:23 step:22027 [D loss: 0.635772, acc: 62.50%] [G loss: 1.847157]\n",
      "epoch:23 step:22028 [D loss: 0.653996, acc: 55.47%] [G loss: 1.766403]\n",
      "epoch:23 step:22029 [D loss: 0.627987, acc: 65.62%] [G loss: 1.859726]\n",
      "epoch:23 step:22030 [D loss: 0.688099, acc: 57.81%] [G loss: 1.985194]\n",
      "epoch:23 step:22031 [D loss: 0.666977, acc: 56.25%] [G loss: 1.809422]\n",
      "epoch:23 step:22032 [D loss: 0.639577, acc: 64.06%] [G loss: 1.993121]\n",
      "epoch:23 step:22033 [D loss: 0.695522, acc: 57.03%] [G loss: 1.758729]\n",
      "epoch:23 step:22034 [D loss: 0.713991, acc: 51.56%] [G loss: 1.853701]\n",
      "epoch:23 step:22035 [D loss: 0.636933, acc: 66.41%] [G loss: 1.964678]\n",
      "epoch:23 step:22036 [D loss: 0.676383, acc: 60.16%] [G loss: 1.915107]\n",
      "epoch:23 step:22037 [D loss: 0.631719, acc: 62.50%] [G loss: 1.937504]\n",
      "epoch:23 step:22038 [D loss: 0.656260, acc: 61.72%] [G loss: 1.905270]\n",
      "epoch:23 step:22039 [D loss: 0.667199, acc: 62.50%] [G loss: 1.913491]\n",
      "epoch:23 step:22040 [D loss: 0.611801, acc: 64.06%] [G loss: 1.823976]\n",
      "epoch:23 step:22041 [D loss: 0.634472, acc: 67.19%] [G loss: 1.889227]\n",
      "epoch:23 step:22042 [D loss: 0.632362, acc: 66.41%] [G loss: 1.905084]\n",
      "epoch:23 step:22043 [D loss: 0.659687, acc: 65.62%] [G loss: 1.893931]\n",
      "epoch:23 step:22044 [D loss: 0.623007, acc: 59.38%] [G loss: 1.902927]\n",
      "epoch:23 step:22045 [D loss: 0.607273, acc: 65.62%] [G loss: 2.059576]\n",
      "epoch:23 step:22046 [D loss: 0.617668, acc: 68.75%] [G loss: 2.045248]\n",
      "epoch:23 step:22047 [D loss: 0.624697, acc: 63.28%] [G loss: 1.942489]\n",
      "epoch:23 step:22048 [D loss: 0.657431, acc: 57.81%] [G loss: 2.036843]\n",
      "epoch:23 step:22049 [D loss: 0.619999, acc: 67.19%] [G loss: 2.129730]\n",
      "epoch:23 step:22050 [D loss: 0.685437, acc: 57.81%] [G loss: 1.969659]\n",
      "epoch:23 step:22051 [D loss: 0.687906, acc: 53.12%] [G loss: 1.802307]\n",
      "epoch:23 step:22052 [D loss: 0.735424, acc: 50.00%] [G loss: 1.833461]\n",
      "epoch:23 step:22053 [D loss: 0.666400, acc: 62.50%] [G loss: 1.768141]\n",
      "epoch:23 step:22054 [D loss: 0.671941, acc: 61.72%] [G loss: 1.897361]\n",
      "epoch:23 step:22055 [D loss: 0.635667, acc: 66.41%] [G loss: 1.996738]\n",
      "epoch:23 step:22056 [D loss: 0.644148, acc: 62.50%] [G loss: 1.947495]\n",
      "epoch:23 step:22057 [D loss: 0.704630, acc: 52.34%] [G loss: 1.833473]\n",
      "epoch:23 step:22058 [D loss: 0.644417, acc: 64.06%] [G loss: 1.988026]\n",
      "epoch:23 step:22059 [D loss: 0.615971, acc: 65.62%] [G loss: 2.098383]\n",
      "epoch:23 step:22060 [D loss: 0.621471, acc: 63.28%] [G loss: 1.875789]\n",
      "epoch:23 step:22061 [D loss: 0.621456, acc: 64.06%] [G loss: 1.988203]\n",
      "epoch:23 step:22062 [D loss: 0.687634, acc: 54.69%] [G loss: 1.816395]\n",
      "epoch:23 step:22063 [D loss: 0.654513, acc: 62.50%] [G loss: 1.919645]\n",
      "epoch:23 step:22064 [D loss: 0.643336, acc: 62.50%] [G loss: 1.801744]\n",
      "epoch:23 step:22065 [D loss: 0.676505, acc: 59.38%] [G loss: 1.925360]\n",
      "epoch:23 step:22066 [D loss: 0.675208, acc: 57.03%] [G loss: 1.940100]\n",
      "epoch:23 step:22067 [D loss: 0.584547, acc: 70.31%] [G loss: 1.918053]\n",
      "epoch:23 step:22068 [D loss: 0.643086, acc: 58.59%] [G loss: 1.946497]\n",
      "epoch:23 step:22069 [D loss: 0.720562, acc: 46.88%] [G loss: 1.711639]\n",
      "epoch:23 step:22070 [D loss: 0.620175, acc: 61.72%] [G loss: 2.022853]\n",
      "epoch:23 step:22071 [D loss: 0.635146, acc: 60.94%] [G loss: 1.884131]\n",
      "epoch:23 step:22072 [D loss: 0.625235, acc: 64.84%] [G loss: 1.842978]\n",
      "epoch:23 step:22073 [D loss: 0.606481, acc: 66.41%] [G loss: 1.806640]\n",
      "epoch:23 step:22074 [D loss: 0.614839, acc: 67.19%] [G loss: 1.874720]\n",
      "epoch:23 step:22075 [D loss: 0.661101, acc: 69.53%] [G loss: 1.814186]\n",
      "epoch:23 step:22076 [D loss: 0.609886, acc: 71.09%] [G loss: 1.814277]\n",
      "epoch:23 step:22077 [D loss: 0.617631, acc: 67.97%] [G loss: 1.833138]\n",
      "epoch:23 step:22078 [D loss: 0.698971, acc: 54.69%] [G loss: 1.819949]\n",
      "epoch:23 step:22079 [D loss: 0.709402, acc: 50.78%] [G loss: 1.790128]\n",
      "epoch:23 step:22080 [D loss: 0.704458, acc: 52.34%] [G loss: 1.688950]\n",
      "epoch:23 step:22081 [D loss: 0.667755, acc: 62.50%] [G loss: 1.707918]\n",
      "epoch:23 step:22082 [D loss: 0.662976, acc: 60.94%] [G loss: 1.787363]\n",
      "epoch:23 step:22083 [D loss: 0.601003, acc: 67.19%] [G loss: 1.965538]\n",
      "epoch:23 step:22084 [D loss: 0.632194, acc: 65.62%] [G loss: 2.006526]\n",
      "epoch:23 step:22085 [D loss: 0.609052, acc: 67.19%] [G loss: 2.037503]\n",
      "epoch:23 step:22086 [D loss: 0.632502, acc: 65.62%] [G loss: 1.917849]\n",
      "epoch:23 step:22087 [D loss: 0.652533, acc: 62.50%] [G loss: 1.835305]\n",
      "epoch:23 step:22088 [D loss: 0.678374, acc: 61.72%] [G loss: 1.848500]\n",
      "epoch:23 step:22089 [D loss: 0.695733, acc: 60.94%] [G loss: 1.789825]\n",
      "epoch:23 step:22090 [D loss: 0.664103, acc: 60.16%] [G loss: 1.849291]\n",
      "epoch:23 step:22091 [D loss: 0.682840, acc: 60.16%] [G loss: 1.773767]\n",
      "epoch:23 step:22092 [D loss: 0.648132, acc: 61.72%] [G loss: 1.821709]\n",
      "epoch:23 step:22093 [D loss: 0.692271, acc: 50.78%] [G loss: 1.939338]\n",
      "epoch:23 step:22094 [D loss: 0.643094, acc: 58.59%] [G loss: 1.968759]\n",
      "epoch:23 step:22095 [D loss: 0.631409, acc: 64.84%] [G loss: 1.862211]\n",
      "epoch:23 step:22096 [D loss: 0.637282, acc: 60.94%] [G loss: 1.907211]\n",
      "epoch:23 step:22097 [D loss: 0.638701, acc: 66.41%] [G loss: 1.782762]\n",
      "epoch:23 step:22098 [D loss: 0.727390, acc: 56.25%] [G loss: 1.832525]\n",
      "epoch:23 step:22099 [D loss: 0.633909, acc: 65.62%] [G loss: 1.922274]\n",
      "epoch:23 step:22100 [D loss: 0.573322, acc: 71.88%] [G loss: 1.963198]\n",
      "epoch:23 step:22101 [D loss: 0.627865, acc: 66.41%] [G loss: 1.866999]\n",
      "epoch:23 step:22102 [D loss: 0.632307, acc: 62.50%] [G loss: 2.070443]\n",
      "epoch:23 step:22103 [D loss: 0.663639, acc: 61.72%] [G loss: 2.063213]\n",
      "epoch:23 step:22104 [D loss: 0.648958, acc: 64.84%] [G loss: 1.953438]\n",
      "epoch:23 step:22105 [D loss: 0.582178, acc: 67.97%] [G loss: 2.050631]\n",
      "epoch:23 step:22106 [D loss: 0.685285, acc: 54.69%] [G loss: 1.930599]\n",
      "epoch:23 step:22107 [D loss: 0.628190, acc: 64.06%] [G loss: 1.932542]\n",
      "epoch:23 step:22108 [D loss: 0.607517, acc: 68.75%] [G loss: 1.979629]\n",
      "epoch:23 step:22109 [D loss: 0.661923, acc: 60.94%] [G loss: 2.062071]\n",
      "epoch:23 step:22110 [D loss: 0.666735, acc: 59.38%] [G loss: 1.689944]\n",
      "epoch:23 step:22111 [D loss: 0.667564, acc: 60.94%] [G loss: 1.838315]\n",
      "epoch:23 step:22112 [D loss: 0.676962, acc: 59.38%] [G loss: 1.905604]\n",
      "epoch:23 step:22113 [D loss: 0.638059, acc: 64.06%] [G loss: 1.759288]\n",
      "epoch:23 step:22114 [D loss: 0.660399, acc: 57.03%] [G loss: 1.819397]\n",
      "epoch:23 step:22115 [D loss: 0.604946, acc: 67.97%] [G loss: 2.018926]\n",
      "epoch:23 step:22116 [D loss: 0.675520, acc: 58.59%] [G loss: 1.854057]\n",
      "epoch:23 step:22117 [D loss: 0.752064, acc: 51.56%] [G loss: 1.734692]\n",
      "epoch:23 step:22118 [D loss: 0.688457, acc: 53.12%] [G loss: 1.832497]\n",
      "epoch:23 step:22119 [D loss: 0.712445, acc: 52.34%] [G loss: 1.775563]\n",
      "epoch:23 step:22120 [D loss: 0.693259, acc: 55.47%] [G loss: 1.835024]\n",
      "epoch:23 step:22121 [D loss: 0.652101, acc: 60.16%] [G loss: 1.799094]\n",
      "epoch:23 step:22122 [D loss: 0.658543, acc: 64.06%] [G loss: 1.876699]\n",
      "epoch:23 step:22123 [D loss: 0.669761, acc: 59.38%] [G loss: 1.712177]\n",
      "epoch:23 step:22124 [D loss: 0.640748, acc: 58.59%] [G loss: 1.825494]\n",
      "epoch:23 step:22125 [D loss: 0.633885, acc: 60.94%] [G loss: 1.781282]\n",
      "epoch:23 step:22126 [D loss: 0.591654, acc: 67.97%] [G loss: 1.772674]\n",
      "epoch:23 step:22127 [D loss: 0.677549, acc: 57.03%] [G loss: 1.816682]\n",
      "epoch:23 step:22128 [D loss: 0.661009, acc: 60.94%] [G loss: 1.757006]\n",
      "epoch:23 step:22129 [D loss: 0.639376, acc: 63.28%] [G loss: 1.756662]\n",
      "epoch:23 step:22130 [D loss: 0.694702, acc: 57.81%] [G loss: 1.737751]\n",
      "epoch:23 step:22131 [D loss: 0.634794, acc: 66.41%] [G loss: 1.771240]\n",
      "epoch:23 step:22132 [D loss: 0.671530, acc: 59.38%] [G loss: 1.792502]\n",
      "epoch:23 step:22133 [D loss: 0.600364, acc: 70.31%] [G loss: 1.866633]\n",
      "epoch:23 step:22134 [D loss: 0.636372, acc: 62.50%] [G loss: 1.915781]\n",
      "epoch:23 step:22135 [D loss: 0.708648, acc: 56.25%] [G loss: 1.868155]\n",
      "epoch:23 step:22136 [D loss: 0.625660, acc: 68.75%] [G loss: 1.968321]\n",
      "epoch:23 step:22137 [D loss: 0.664733, acc: 54.69%] [G loss: 1.757480]\n",
      "epoch:23 step:22138 [D loss: 0.610879, acc: 69.53%] [G loss: 1.979105]\n",
      "epoch:23 step:22139 [D loss: 0.613338, acc: 67.19%] [G loss: 2.051059]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:23 step:22140 [D loss: 0.625171, acc: 66.41%] [G loss: 2.049739]\n",
      "epoch:23 step:22141 [D loss: 0.693806, acc: 57.03%] [G loss: 1.752219]\n",
      "epoch:23 step:22142 [D loss: 0.615506, acc: 66.41%] [G loss: 1.807479]\n",
      "epoch:23 step:22143 [D loss: 0.648024, acc: 63.28%] [G loss: 1.832396]\n",
      "epoch:23 step:22144 [D loss: 0.649026, acc: 60.94%] [G loss: 1.942788]\n",
      "epoch:23 step:22145 [D loss: 0.612206, acc: 67.19%] [G loss: 1.922706]\n",
      "epoch:23 step:22146 [D loss: 0.626669, acc: 64.84%] [G loss: 1.867050]\n",
      "epoch:23 step:22147 [D loss: 0.650867, acc: 59.38%] [G loss: 1.913000]\n",
      "epoch:23 step:22148 [D loss: 0.683218, acc: 57.81%] [G loss: 1.746161]\n",
      "epoch:23 step:22149 [D loss: 0.637534, acc: 64.06%] [G loss: 1.889100]\n",
      "epoch:23 step:22150 [D loss: 0.675632, acc: 56.25%] [G loss: 2.005908]\n",
      "epoch:23 step:22151 [D loss: 0.658651, acc: 61.72%] [G loss: 1.950585]\n",
      "epoch:23 step:22152 [D loss: 0.638531, acc: 63.28%] [G loss: 1.849823]\n",
      "epoch:23 step:22153 [D loss: 0.676364, acc: 53.12%] [G loss: 1.928706]\n",
      "epoch:23 step:22154 [D loss: 0.619161, acc: 64.84%] [G loss: 1.991980]\n",
      "epoch:23 step:22155 [D loss: 0.711854, acc: 57.03%] [G loss: 1.893663]\n",
      "epoch:23 step:22156 [D loss: 0.560814, acc: 72.66%] [G loss: 2.000494]\n",
      "epoch:23 step:22157 [D loss: 0.657244, acc: 57.03%] [G loss: 1.719158]\n",
      "epoch:23 step:22158 [D loss: 0.645573, acc: 61.72%] [G loss: 1.872594]\n",
      "epoch:23 step:22159 [D loss: 0.640383, acc: 68.75%] [G loss: 1.809807]\n",
      "epoch:23 step:22160 [D loss: 0.631365, acc: 57.81%] [G loss: 2.011133]\n",
      "epoch:23 step:22161 [D loss: 0.671813, acc: 57.81%] [G loss: 1.881207]\n",
      "epoch:23 step:22162 [D loss: 0.667752, acc: 57.03%] [G loss: 1.819963]\n",
      "epoch:23 step:22163 [D loss: 0.697249, acc: 58.59%] [G loss: 1.794197]\n",
      "epoch:23 step:22164 [D loss: 0.642644, acc: 62.50%] [G loss: 1.823235]\n",
      "epoch:23 step:22165 [D loss: 0.724346, acc: 53.91%] [G loss: 1.689572]\n",
      "epoch:23 step:22166 [D loss: 0.704999, acc: 53.12%] [G loss: 1.684364]\n",
      "epoch:23 step:22167 [D loss: 0.693795, acc: 61.72%] [G loss: 1.665571]\n",
      "epoch:23 step:22168 [D loss: 0.621309, acc: 64.06%] [G loss: 1.770700]\n",
      "epoch:23 step:22169 [D loss: 0.631187, acc: 60.94%] [G loss: 1.881424]\n",
      "epoch:23 step:22170 [D loss: 0.700978, acc: 53.91%] [G loss: 1.794166]\n",
      "epoch:23 step:22171 [D loss: 0.664684, acc: 58.59%] [G loss: 1.978604]\n",
      "epoch:23 step:22172 [D loss: 0.611387, acc: 61.72%] [G loss: 1.725983]\n",
      "epoch:23 step:22173 [D loss: 0.664000, acc: 55.47%] [G loss: 1.885340]\n",
      "epoch:23 step:22174 [D loss: 0.599071, acc: 70.31%] [G loss: 1.817901]\n",
      "epoch:23 step:22175 [D loss: 0.598036, acc: 67.97%] [G loss: 2.161252]\n",
      "epoch:23 step:22176 [D loss: 0.644630, acc: 64.06%] [G loss: 1.837045]\n",
      "epoch:23 step:22177 [D loss: 0.617513, acc: 66.41%] [G loss: 1.867389]\n",
      "epoch:23 step:22178 [D loss: 0.621169, acc: 68.75%] [G loss: 1.840839]\n",
      "epoch:23 step:22179 [D loss: 0.665009, acc: 57.81%] [G loss: 1.839739]\n",
      "epoch:23 step:22180 [D loss: 0.609201, acc: 61.72%] [G loss: 1.933391]\n",
      "epoch:23 step:22181 [D loss: 0.588890, acc: 75.00%] [G loss: 1.926313]\n",
      "epoch:23 step:22182 [D loss: 0.635256, acc: 67.97%] [G loss: 1.860850]\n",
      "epoch:23 step:22183 [D loss: 0.618636, acc: 61.72%] [G loss: 1.895796]\n",
      "epoch:23 step:22184 [D loss: 0.630736, acc: 61.72%] [G loss: 1.870052]\n",
      "epoch:23 step:22185 [D loss: 0.653434, acc: 56.25%] [G loss: 1.959943]\n",
      "epoch:23 step:22186 [D loss: 0.614652, acc: 63.28%] [G loss: 2.039394]\n",
      "epoch:23 step:22187 [D loss: 0.694099, acc: 51.56%] [G loss: 1.905631]\n",
      "epoch:23 step:22188 [D loss: 0.634580, acc: 67.97%] [G loss: 1.972680]\n",
      "epoch:23 step:22189 [D loss: 0.577997, acc: 70.31%] [G loss: 2.044289]\n",
      "epoch:23 step:22190 [D loss: 0.654279, acc: 65.62%] [G loss: 1.997495]\n",
      "epoch:23 step:22191 [D loss: 0.672840, acc: 57.81%] [G loss: 1.859123]\n",
      "epoch:23 step:22192 [D loss: 0.654178, acc: 63.28%] [G loss: 1.952508]\n",
      "epoch:23 step:22193 [D loss: 0.616691, acc: 61.72%] [G loss: 2.065864]\n",
      "epoch:23 step:22194 [D loss: 0.662867, acc: 65.62%] [G loss: 1.923847]\n",
      "epoch:23 step:22195 [D loss: 0.580491, acc: 67.97%] [G loss: 2.005050]\n",
      "epoch:23 step:22196 [D loss: 0.700568, acc: 53.91%] [G loss: 2.011791]\n",
      "epoch:23 step:22197 [D loss: 0.610882, acc: 63.28%] [G loss: 2.017638]\n",
      "epoch:23 step:22198 [D loss: 0.636314, acc: 66.41%] [G loss: 2.012659]\n",
      "epoch:23 step:22199 [D loss: 0.566783, acc: 70.31%] [G loss: 2.213431]\n",
      "epoch:23 step:22200 [D loss: 0.623356, acc: 62.50%] [G loss: 2.174725]\n",
      "##############\n",
      "[2.53043978 1.3047408  6.09858636 4.81474737 3.49546152 5.64258817\n",
      " 4.37339125 4.72338932 4.56984215 3.56216361]\n",
      "##########\n",
      "epoch:23 step:22201 [D loss: 0.615477, acc: 65.62%] [G loss: 1.999400]\n",
      "epoch:23 step:22202 [D loss: 0.634459, acc: 61.72%] [G loss: 1.910443]\n",
      "epoch:23 step:22203 [D loss: 0.632547, acc: 59.38%] [G loss: 1.959535]\n",
      "epoch:23 step:22204 [D loss: 0.622540, acc: 67.19%] [G loss: 2.193285]\n",
      "epoch:23 step:22205 [D loss: 0.557852, acc: 75.78%] [G loss: 2.061777]\n",
      "epoch:23 step:22206 [D loss: 0.721128, acc: 54.69%] [G loss: 1.880402]\n",
      "epoch:23 step:22207 [D loss: 0.692353, acc: 57.03%] [G loss: 1.789652]\n",
      "epoch:23 step:22208 [D loss: 0.656002, acc: 61.72%] [G loss: 1.791972]\n",
      "epoch:23 step:22209 [D loss: 0.720817, acc: 55.47%] [G loss: 1.734626]\n",
      "epoch:23 step:22210 [D loss: 0.637145, acc: 64.84%] [G loss: 1.922387]\n",
      "epoch:23 step:22211 [D loss: 0.619541, acc: 67.97%] [G loss: 2.002467]\n",
      "epoch:23 step:22212 [D loss: 0.659119, acc: 60.16%] [G loss: 1.890569]\n",
      "epoch:23 step:22213 [D loss: 0.668847, acc: 61.72%] [G loss: 1.801943]\n",
      "epoch:23 step:22214 [D loss: 0.614444, acc: 67.19%] [G loss: 1.942109]\n",
      "epoch:23 step:22215 [D loss: 0.699592, acc: 59.38%] [G loss: 1.802530]\n",
      "epoch:23 step:22216 [D loss: 0.675645, acc: 64.06%] [G loss: 1.792869]\n",
      "epoch:23 step:22217 [D loss: 0.668989, acc: 58.59%] [G loss: 1.749635]\n",
      "epoch:23 step:22218 [D loss: 0.654958, acc: 56.25%] [G loss: 1.754247]\n",
      "epoch:23 step:22219 [D loss: 0.655101, acc: 63.28%] [G loss: 1.879672]\n",
      "epoch:23 step:22220 [D loss: 0.709426, acc: 50.78%] [G loss: 1.854401]\n",
      "epoch:23 step:22221 [D loss: 0.653767, acc: 63.28%] [G loss: 1.818240]\n",
      "epoch:23 step:22222 [D loss: 0.651835, acc: 58.59%] [G loss: 1.767647]\n",
      "epoch:23 step:22223 [D loss: 0.690409, acc: 56.25%] [G loss: 1.679164]\n",
      "epoch:23 step:22224 [D loss: 0.708742, acc: 56.25%] [G loss: 1.762915]\n",
      "epoch:23 step:22225 [D loss: 0.638353, acc: 62.50%] [G loss: 1.779373]\n",
      "epoch:23 step:22226 [D loss: 0.700292, acc: 53.91%] [G loss: 1.725150]\n",
      "epoch:23 step:22227 [D loss: 0.663468, acc: 58.59%] [G loss: 1.786128]\n",
      "epoch:23 step:22228 [D loss: 0.648342, acc: 62.50%] [G loss: 1.786785]\n",
      "epoch:23 step:22229 [D loss: 0.665143, acc: 61.72%] [G loss: 1.794750]\n",
      "epoch:23 step:22230 [D loss: 0.642988, acc: 61.72%] [G loss: 1.814237]\n",
      "epoch:23 step:22231 [D loss: 0.646438, acc: 62.50%] [G loss: 1.895594]\n",
      "epoch:23 step:22232 [D loss: 0.628958, acc: 71.88%] [G loss: 1.859720]\n",
      "epoch:23 step:22233 [D loss: 0.641177, acc: 67.19%] [G loss: 1.887930]\n",
      "epoch:23 step:22234 [D loss: 0.650909, acc: 62.50%] [G loss: 1.868730]\n",
      "epoch:23 step:22235 [D loss: 0.675490, acc: 60.16%] [G loss: 1.807806]\n",
      "epoch:23 step:22236 [D loss: 0.658932, acc: 62.50%] [G loss: 1.853639]\n",
      "epoch:23 step:22237 [D loss: 0.658785, acc: 65.62%] [G loss: 1.786700]\n",
      "epoch:23 step:22238 [D loss: 0.618490, acc: 67.19%] [G loss: 1.869829]\n",
      "epoch:23 step:22239 [D loss: 0.618674, acc: 66.41%] [G loss: 1.938048]\n",
      "epoch:23 step:22240 [D loss: 0.663066, acc: 60.16%] [G loss: 1.900846]\n",
      "epoch:23 step:22241 [D loss: 0.592338, acc: 67.97%] [G loss: 2.033432]\n",
      "epoch:23 step:22242 [D loss: 0.577031, acc: 67.97%] [G loss: 1.944581]\n",
      "epoch:23 step:22243 [D loss: 0.614422, acc: 67.97%] [G loss: 2.124671]\n",
      "epoch:23 step:22244 [D loss: 0.617071, acc: 63.28%] [G loss: 2.134350]\n",
      "epoch:23 step:22245 [D loss: 0.560260, acc: 75.78%] [G loss: 2.218362]\n",
      "epoch:23 step:22246 [D loss: 0.667412, acc: 60.94%] [G loss: 1.974119]\n",
      "epoch:23 step:22247 [D loss: 0.699617, acc: 56.25%] [G loss: 1.897826]\n",
      "epoch:23 step:22248 [D loss: 0.651626, acc: 64.06%] [G loss: 1.881147]\n",
      "epoch:23 step:22249 [D loss: 0.663596, acc: 64.06%] [G loss: 1.726875]\n",
      "epoch:23 step:22250 [D loss: 0.595641, acc: 65.62%] [G loss: 1.953068]\n",
      "epoch:23 step:22251 [D loss: 0.614772, acc: 68.75%] [G loss: 1.912572]\n",
      "epoch:23 step:22252 [D loss: 0.609585, acc: 70.31%] [G loss: 1.949775]\n",
      "epoch:23 step:22253 [D loss: 0.655902, acc: 64.84%] [G loss: 1.879399]\n",
      "epoch:23 step:22254 [D loss: 0.722068, acc: 53.12%] [G loss: 1.797864]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:23 step:22255 [D loss: 0.646101, acc: 56.25%] [G loss: 1.843476]\n",
      "epoch:23 step:22256 [D loss: 0.685991, acc: 56.25%] [G loss: 1.958136]\n",
      "epoch:23 step:22257 [D loss: 0.623566, acc: 64.84%] [G loss: 1.793932]\n",
      "epoch:23 step:22258 [D loss: 0.634702, acc: 62.50%] [G loss: 2.196550]\n",
      "epoch:23 step:22259 [D loss: 0.633930, acc: 61.72%] [G loss: 2.006839]\n",
      "epoch:23 step:22260 [D loss: 0.618499, acc: 67.97%] [G loss: 2.058234]\n",
      "epoch:23 step:22261 [D loss: 0.660819, acc: 64.84%] [G loss: 1.858757]\n",
      "epoch:23 step:22262 [D loss: 0.629618, acc: 64.84%] [G loss: 2.081973]\n",
      "epoch:23 step:22263 [D loss: 0.616771, acc: 64.06%] [G loss: 1.944091]\n",
      "epoch:23 step:22264 [D loss: 0.693993, acc: 54.69%] [G loss: 1.933407]\n",
      "epoch:23 step:22265 [D loss: 0.661466, acc: 57.03%] [G loss: 1.780990]\n",
      "epoch:23 step:22266 [D loss: 0.630956, acc: 59.38%] [G loss: 1.820178]\n",
      "epoch:23 step:22267 [D loss: 0.689196, acc: 59.38%] [G loss: 1.788337]\n",
      "epoch:23 step:22268 [D loss: 0.620816, acc: 71.09%] [G loss: 1.788104]\n",
      "epoch:23 step:22269 [D loss: 0.658270, acc: 63.28%] [G loss: 1.770302]\n",
      "epoch:23 step:22270 [D loss: 0.658147, acc: 64.84%] [G loss: 1.964122]\n",
      "epoch:23 step:22271 [D loss: 0.662856, acc: 57.81%] [G loss: 1.916061]\n",
      "epoch:23 step:22272 [D loss: 0.605466, acc: 68.75%] [G loss: 1.947677]\n",
      "epoch:23 step:22273 [D loss: 0.693215, acc: 57.03%] [G loss: 1.935431]\n",
      "epoch:23 step:22274 [D loss: 0.635589, acc: 64.84%] [G loss: 1.886759]\n",
      "epoch:23 step:22275 [D loss: 0.632732, acc: 64.84%] [G loss: 2.002484]\n",
      "epoch:23 step:22276 [D loss: 0.661482, acc: 61.72%] [G loss: 1.887215]\n",
      "epoch:23 step:22277 [D loss: 0.662639, acc: 59.38%] [G loss: 1.952501]\n",
      "epoch:23 step:22278 [D loss: 0.705191, acc: 60.16%] [G loss: 1.911112]\n",
      "epoch:23 step:22279 [D loss: 0.611344, acc: 72.66%] [G loss: 1.889683]\n",
      "epoch:23 step:22280 [D loss: 0.635484, acc: 60.16%] [G loss: 1.895154]\n",
      "epoch:23 step:22281 [D loss: 0.647162, acc: 61.72%] [G loss: 1.947999]\n",
      "epoch:23 step:22282 [D loss: 0.708472, acc: 50.78%] [G loss: 1.805992]\n",
      "epoch:23 step:22283 [D loss: 0.637534, acc: 60.94%] [G loss: 1.930313]\n",
      "epoch:23 step:22284 [D loss: 0.657766, acc: 65.62%] [G loss: 1.867118]\n",
      "epoch:23 step:22285 [D loss: 0.601185, acc: 69.53%] [G loss: 1.818268]\n",
      "epoch:23 step:22286 [D loss: 0.659870, acc: 53.91%] [G loss: 1.877068]\n",
      "epoch:23 step:22287 [D loss: 0.667933, acc: 62.50%] [G loss: 1.835461]\n",
      "epoch:23 step:22288 [D loss: 0.678280, acc: 60.94%] [G loss: 1.824077]\n",
      "epoch:23 step:22289 [D loss: 0.637341, acc: 64.84%] [G loss: 1.866852]\n",
      "epoch:23 step:22290 [D loss: 0.677377, acc: 58.59%] [G loss: 1.763936]\n",
      "epoch:23 step:22291 [D loss: 0.626623, acc: 66.41%] [G loss: 1.832844]\n",
      "epoch:23 step:22292 [D loss: 0.667579, acc: 52.34%] [G loss: 1.777735]\n",
      "epoch:23 step:22293 [D loss: 0.670932, acc: 62.50%] [G loss: 1.947244]\n",
      "epoch:23 step:22294 [D loss: 0.606554, acc: 68.75%] [G loss: 1.897038]\n",
      "epoch:23 step:22295 [D loss: 0.631034, acc: 61.72%] [G loss: 1.982482]\n",
      "epoch:23 step:22296 [D loss: 0.610374, acc: 67.97%] [G loss: 1.940737]\n",
      "epoch:23 step:22297 [D loss: 0.670492, acc: 58.59%] [G loss: 1.960244]\n",
      "epoch:23 step:22298 [D loss: 0.620244, acc: 70.31%] [G loss: 2.109369]\n",
      "epoch:23 step:22299 [D loss: 0.617280, acc: 67.97%] [G loss: 1.966492]\n",
      "epoch:23 step:22300 [D loss: 0.669964, acc: 58.59%] [G loss: 1.974463]\n",
      "epoch:23 step:22301 [D loss: 0.619778, acc: 63.28%] [G loss: 1.955789]\n",
      "epoch:23 step:22302 [D loss: 0.699162, acc: 61.72%] [G loss: 1.800776]\n",
      "epoch:23 step:22303 [D loss: 0.652265, acc: 64.06%] [G loss: 1.750094]\n",
      "epoch:23 step:22304 [D loss: 0.674986, acc: 64.06%] [G loss: 1.804717]\n",
      "epoch:23 step:22305 [D loss: 0.594279, acc: 69.53%] [G loss: 1.891675]\n",
      "epoch:23 step:22306 [D loss: 0.603785, acc: 68.75%] [G loss: 1.874610]\n",
      "epoch:23 step:22307 [D loss: 0.645508, acc: 65.62%] [G loss: 1.888839]\n",
      "epoch:23 step:22308 [D loss: 0.620981, acc: 67.19%] [G loss: 2.006374]\n",
      "epoch:23 step:22309 [D loss: 0.684286, acc: 64.84%] [G loss: 1.892927]\n",
      "epoch:23 step:22310 [D loss: 0.593976, acc: 67.19%] [G loss: 1.922266]\n",
      "epoch:23 step:22311 [D loss: 0.682353, acc: 60.94%] [G loss: 1.819324]\n",
      "epoch:23 step:22312 [D loss: 0.630999, acc: 62.50%] [G loss: 1.930635]\n",
      "epoch:23 step:22313 [D loss: 0.684658, acc: 57.03%] [G loss: 1.768224]\n",
      "epoch:23 step:22314 [D loss: 0.630260, acc: 66.41%] [G loss: 1.762330]\n",
      "epoch:23 step:22315 [D loss: 0.666379, acc: 57.03%] [G loss: 1.783186]\n",
      "epoch:23 step:22316 [D loss: 0.672257, acc: 59.38%] [G loss: 1.800347]\n",
      "epoch:23 step:22317 [D loss: 0.691232, acc: 60.16%] [G loss: 1.669620]\n",
      "epoch:23 step:22318 [D loss: 0.637564, acc: 71.88%] [G loss: 1.915338]\n",
      "epoch:23 step:22319 [D loss: 0.654872, acc: 60.94%] [G loss: 1.978353]\n",
      "epoch:23 step:22320 [D loss: 0.651343, acc: 63.28%] [G loss: 1.993718]\n",
      "epoch:23 step:22321 [D loss: 0.639357, acc: 67.97%] [G loss: 1.939169]\n",
      "epoch:23 step:22322 [D loss: 0.601166, acc: 66.41%] [G loss: 1.887211]\n",
      "epoch:23 step:22323 [D loss: 0.591116, acc: 69.53%] [G loss: 1.893109]\n",
      "epoch:23 step:22324 [D loss: 0.613769, acc: 64.06%] [G loss: 1.856890]\n",
      "epoch:23 step:22325 [D loss: 0.599130, acc: 69.53%] [G loss: 2.148846]\n",
      "epoch:23 step:22326 [D loss: 0.600706, acc: 67.97%] [G loss: 2.117321]\n",
      "epoch:23 step:22327 [D loss: 0.623372, acc: 60.94%] [G loss: 1.971219]\n",
      "epoch:23 step:22328 [D loss: 0.601637, acc: 69.53%] [G loss: 2.004131]\n",
      "epoch:23 step:22329 [D loss: 0.641503, acc: 59.38%] [G loss: 2.010590]\n",
      "epoch:23 step:22330 [D loss: 0.658422, acc: 63.28%] [G loss: 1.940827]\n",
      "epoch:23 step:22331 [D loss: 0.656095, acc: 63.28%] [G loss: 2.052248]\n",
      "epoch:23 step:22332 [D loss: 0.620524, acc: 60.16%] [G loss: 2.108524]\n",
      "epoch:23 step:22333 [D loss: 0.563692, acc: 72.66%] [G loss: 1.963802]\n",
      "epoch:23 step:22334 [D loss: 0.671864, acc: 57.81%] [G loss: 2.058055]\n",
      "epoch:23 step:22335 [D loss: 0.679526, acc: 60.94%] [G loss: 1.734959]\n",
      "epoch:23 step:22336 [D loss: 0.663015, acc: 59.38%] [G loss: 1.838321]\n",
      "epoch:23 step:22337 [D loss: 0.577386, acc: 69.53%] [G loss: 2.031536]\n",
      "epoch:23 step:22338 [D loss: 0.688121, acc: 60.16%] [G loss: 1.950453]\n",
      "epoch:23 step:22339 [D loss: 0.695009, acc: 56.25%] [G loss: 1.832222]\n",
      "epoch:23 step:22340 [D loss: 0.706046, acc: 53.12%] [G loss: 2.011954]\n",
      "epoch:23 step:22341 [D loss: 0.629477, acc: 68.75%] [G loss: 2.015321]\n",
      "epoch:23 step:22342 [D loss: 0.625027, acc: 65.62%] [G loss: 2.035494]\n",
      "epoch:23 step:22343 [D loss: 0.597360, acc: 64.06%] [G loss: 2.053349]\n",
      "epoch:23 step:22344 [D loss: 0.635211, acc: 61.72%] [G loss: 2.055649]\n",
      "epoch:23 step:22345 [D loss: 0.686397, acc: 59.38%] [G loss: 1.889654]\n",
      "epoch:23 step:22346 [D loss: 0.730579, acc: 51.56%] [G loss: 1.686062]\n",
      "epoch:23 step:22347 [D loss: 0.658718, acc: 62.50%] [G loss: 1.804785]\n",
      "epoch:23 step:22348 [D loss: 0.689182, acc: 57.81%] [G loss: 1.709986]\n",
      "epoch:23 step:22349 [D loss: 0.686682, acc: 57.03%] [G loss: 1.761127]\n",
      "epoch:23 step:22350 [D loss: 0.652611, acc: 55.47%] [G loss: 1.775146]\n",
      "epoch:23 step:22351 [D loss: 0.656397, acc: 62.50%] [G loss: 1.806956]\n",
      "epoch:23 step:22352 [D loss: 0.722296, acc: 53.91%] [G loss: 1.693853]\n",
      "epoch:23 step:22353 [D loss: 0.706056, acc: 53.91%] [G loss: 1.835480]\n",
      "epoch:23 step:22354 [D loss: 0.608473, acc: 67.97%] [G loss: 1.924639]\n",
      "epoch:23 step:22355 [D loss: 0.610027, acc: 66.41%] [G loss: 1.906281]\n",
      "epoch:23 step:22356 [D loss: 0.652039, acc: 61.72%] [G loss: 1.737802]\n",
      "epoch:23 step:22357 [D loss: 0.683802, acc: 58.59%] [G loss: 1.835882]\n",
      "epoch:23 step:22358 [D loss: 0.637564, acc: 64.84%] [G loss: 1.900101]\n",
      "epoch:23 step:22359 [D loss: 0.655587, acc: 57.81%] [G loss: 1.809928]\n",
      "epoch:23 step:22360 [D loss: 0.591507, acc: 70.31%] [G loss: 1.955835]\n",
      "epoch:23 step:22361 [D loss: 0.612864, acc: 67.97%] [G loss: 1.940394]\n",
      "epoch:23 step:22362 [D loss: 0.691947, acc: 56.25%] [G loss: 1.802849]\n",
      "epoch:23 step:22363 [D loss: 0.625383, acc: 63.28%] [G loss: 1.755865]\n",
      "epoch:23 step:22364 [D loss: 0.661243, acc: 60.16%] [G loss: 1.806941]\n",
      "epoch:23 step:22365 [D loss: 0.670277, acc: 62.50%] [G loss: 1.893034]\n",
      "epoch:23 step:22366 [D loss: 0.610191, acc: 63.28%] [G loss: 2.044980]\n",
      "epoch:23 step:22367 [D loss: 0.606592, acc: 65.62%] [G loss: 1.845402]\n",
      "epoch:23 step:22368 [D loss: 0.673153, acc: 57.03%] [G loss: 1.830761]\n",
      "epoch:23 step:22369 [D loss: 0.696768, acc: 55.47%] [G loss: 1.845245]\n",
      "epoch:23 step:22370 [D loss: 0.651765, acc: 62.50%] [G loss: 1.762837]\n",
      "epoch:23 step:22371 [D loss: 0.707677, acc: 55.47%] [G loss: 1.785417]\n",
      "epoch:23 step:22372 [D loss: 0.652088, acc: 58.59%] [G loss: 1.687495]\n",
      "epoch:23 step:22373 [D loss: 0.645539, acc: 58.59%] [G loss: 1.970867]\n",
      "epoch:23 step:22374 [D loss: 0.644953, acc: 61.72%] [G loss: 1.968425]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:23 step:22375 [D loss: 0.617901, acc: 69.53%] [G loss: 1.792612]\n",
      "epoch:23 step:22376 [D loss: 0.670684, acc: 53.91%] [G loss: 2.089416]\n",
      "epoch:23 step:22377 [D loss: 0.640035, acc: 61.72%] [G loss: 1.857937]\n",
      "epoch:23 step:22378 [D loss: 0.659848, acc: 60.16%] [G loss: 1.810040]\n",
      "epoch:23 step:22379 [D loss: 0.670806, acc: 56.25%] [G loss: 1.814642]\n",
      "epoch:23 step:22380 [D loss: 0.640569, acc: 60.16%] [G loss: 1.773135]\n",
      "epoch:23 step:22381 [D loss: 0.656039, acc: 61.72%] [G loss: 1.789205]\n",
      "epoch:23 step:22382 [D loss: 0.595700, acc: 71.09%] [G loss: 1.875102]\n",
      "epoch:23 step:22383 [D loss: 0.623529, acc: 67.19%] [G loss: 1.875600]\n",
      "epoch:23 step:22384 [D loss: 0.635018, acc: 61.72%] [G loss: 1.861418]\n",
      "epoch:23 step:22385 [D loss: 0.689994, acc: 57.03%] [G loss: 1.857882]\n",
      "epoch:23 step:22386 [D loss: 0.608052, acc: 68.75%] [G loss: 1.909455]\n",
      "epoch:23 step:22387 [D loss: 0.626573, acc: 67.19%] [G loss: 1.751978]\n",
      "epoch:23 step:22388 [D loss: 0.594019, acc: 64.06%] [G loss: 2.043206]\n",
      "epoch:23 step:22389 [D loss: 0.619676, acc: 62.50%] [G loss: 1.895606]\n",
      "epoch:23 step:22390 [D loss: 0.641919, acc: 63.28%] [G loss: 2.071068]\n",
      "epoch:23 step:22391 [D loss: 0.645021, acc: 59.38%] [G loss: 1.852460]\n",
      "epoch:23 step:22392 [D loss: 0.630008, acc: 68.75%] [G loss: 1.953370]\n",
      "epoch:23 step:22393 [D loss: 0.605805, acc: 63.28%] [G loss: 1.940329]\n",
      "epoch:23 step:22394 [D loss: 0.702562, acc: 58.59%] [G loss: 1.909072]\n",
      "epoch:23 step:22395 [D loss: 0.642662, acc: 61.72%] [G loss: 2.142224]\n",
      "epoch:23 step:22396 [D loss: 0.631069, acc: 65.62%] [G loss: 2.130642]\n",
      "epoch:23 step:22397 [D loss: 0.672454, acc: 59.38%] [G loss: 1.887425]\n",
      "epoch:23 step:22398 [D loss: 0.644109, acc: 63.28%] [G loss: 1.770860]\n",
      "epoch:23 step:22399 [D loss: 0.642159, acc: 64.06%] [G loss: 1.874290]\n",
      "epoch:23 step:22400 [D loss: 0.645085, acc: 55.47%] [G loss: 1.926938]\n",
      "##############\n",
      "[2.50814902 1.47089803 6.44234584 4.86667288 3.49155317 5.67739392\n",
      " 4.47346772 4.69888859 4.51841786 3.79738662]\n",
      "##########\n",
      "epoch:23 step:22401 [D loss: 0.703151, acc: 55.47%] [G loss: 1.697981]\n",
      "epoch:23 step:22402 [D loss: 0.698306, acc: 53.12%] [G loss: 1.796688]\n",
      "epoch:23 step:22403 [D loss: 0.682611, acc: 54.69%] [G loss: 1.855127]\n",
      "epoch:23 step:22404 [D loss: 0.643285, acc: 60.16%] [G loss: 1.863254]\n",
      "epoch:23 step:22405 [D loss: 0.649233, acc: 63.28%] [G loss: 1.838612]\n",
      "epoch:23 step:22406 [D loss: 0.683930, acc: 59.38%] [G loss: 1.955510]\n",
      "epoch:23 step:22407 [D loss: 0.679586, acc: 53.91%] [G loss: 1.753013]\n",
      "epoch:23 step:22408 [D loss: 0.615804, acc: 67.19%] [G loss: 1.924500]\n",
      "epoch:23 step:22409 [D loss: 0.739962, acc: 50.78%] [G loss: 1.784527]\n",
      "epoch:23 step:22410 [D loss: 0.679363, acc: 57.81%] [G loss: 1.870521]\n",
      "epoch:23 step:22411 [D loss: 0.673867, acc: 60.16%] [G loss: 1.939034]\n",
      "epoch:23 step:22412 [D loss: 0.667879, acc: 55.47%] [G loss: 1.764955]\n",
      "epoch:23 step:22413 [D loss: 0.672927, acc: 59.38%] [G loss: 1.760294]\n",
      "epoch:23 step:22414 [D loss: 0.639811, acc: 67.97%] [G loss: 1.793397]\n",
      "epoch:23 step:22415 [D loss: 0.624689, acc: 62.50%] [G loss: 1.890064]\n",
      "epoch:23 step:22416 [D loss: 0.648667, acc: 60.94%] [G loss: 1.822816]\n",
      "epoch:23 step:22417 [D loss: 0.643839, acc: 60.16%] [G loss: 1.754931]\n",
      "epoch:23 step:22418 [D loss: 0.685018, acc: 55.47%] [G loss: 1.814824]\n",
      "epoch:23 step:22419 [D loss: 0.589379, acc: 71.88%] [G loss: 1.811498]\n",
      "epoch:23 step:22420 [D loss: 0.647855, acc: 62.50%] [G loss: 1.835045]\n",
      "epoch:23 step:22421 [D loss: 0.678467, acc: 60.16%] [G loss: 1.854316]\n",
      "epoch:23 step:22422 [D loss: 0.677020, acc: 62.50%] [G loss: 1.847207]\n",
      "epoch:23 step:22423 [D loss: 0.627201, acc: 62.50%] [G loss: 1.870137]\n",
      "epoch:23 step:22424 [D loss: 0.675067, acc: 57.81%] [G loss: 1.661170]\n",
      "epoch:23 step:22425 [D loss: 0.680127, acc: 58.59%] [G loss: 1.784581]\n",
      "epoch:23 step:22426 [D loss: 0.647121, acc: 60.16%] [G loss: 1.950457]\n",
      "epoch:23 step:22427 [D loss: 0.688924, acc: 56.25%] [G loss: 1.887064]\n",
      "epoch:23 step:22428 [D loss: 0.628408, acc: 69.53%] [G loss: 1.789207]\n",
      "epoch:23 step:22429 [D loss: 0.636493, acc: 63.28%] [G loss: 1.955662]\n",
      "epoch:23 step:22430 [D loss: 0.634608, acc: 62.50%] [G loss: 1.819928]\n",
      "epoch:23 step:22431 [D loss: 0.654230, acc: 58.59%] [G loss: 1.825641]\n",
      "epoch:23 step:22432 [D loss: 0.669664, acc: 57.81%] [G loss: 1.860435]\n",
      "epoch:23 step:22433 [D loss: 0.657804, acc: 61.72%] [G loss: 1.718381]\n",
      "epoch:23 step:22434 [D loss: 0.650906, acc: 64.84%] [G loss: 1.823608]\n",
      "epoch:23 step:22435 [D loss: 0.590212, acc: 69.53%] [G loss: 1.894294]\n",
      "epoch:23 step:22436 [D loss: 0.618589, acc: 64.84%] [G loss: 1.776771]\n",
      "epoch:23 step:22437 [D loss: 0.595530, acc: 74.22%] [G loss: 1.918786]\n",
      "epoch:23 step:22438 [D loss: 0.673122, acc: 57.03%] [G loss: 1.733799]\n",
      "epoch:23 step:22439 [D loss: 0.697013, acc: 50.78%] [G loss: 1.805928]\n",
      "epoch:23 step:22440 [D loss: 0.606846, acc: 65.62%] [G loss: 1.856408]\n",
      "epoch:23 step:22441 [D loss: 0.649684, acc: 66.41%] [G loss: 1.958350]\n",
      "epoch:23 step:22442 [D loss: 0.688276, acc: 59.38%] [G loss: 1.727757]\n",
      "epoch:23 step:22443 [D loss: 0.660822, acc: 61.72%] [G loss: 1.962126]\n",
      "epoch:23 step:22444 [D loss: 0.652847, acc: 60.94%] [G loss: 1.960294]\n",
      "epoch:23 step:22445 [D loss: 0.607051, acc: 63.28%] [G loss: 2.081900]\n",
      "epoch:23 step:22446 [D loss: 0.641699, acc: 62.50%] [G loss: 1.697029]\n",
      "epoch:23 step:22447 [D loss: 0.715778, acc: 58.59%] [G loss: 1.798746]\n",
      "epoch:23 step:22448 [D loss: 0.666137, acc: 61.72%] [G loss: 1.805715]\n",
      "epoch:23 step:22449 [D loss: 0.618179, acc: 64.84%] [G loss: 1.864198]\n",
      "epoch:23 step:22450 [D loss: 0.568930, acc: 68.75%] [G loss: 1.916098]\n",
      "epoch:23 step:22451 [D loss: 0.697662, acc: 53.91%] [G loss: 1.967866]\n",
      "epoch:23 step:22452 [D loss: 0.652582, acc: 64.06%] [G loss: 2.007912]\n",
      "epoch:23 step:22453 [D loss: 0.697873, acc: 56.25%] [G loss: 1.974722]\n",
      "epoch:23 step:22454 [D loss: 0.618217, acc: 67.19%] [G loss: 1.862130]\n",
      "epoch:23 step:22455 [D loss: 0.625497, acc: 68.75%] [G loss: 1.927672]\n",
      "epoch:23 step:22456 [D loss: 0.655836, acc: 63.28%] [G loss: 2.008060]\n",
      "epoch:23 step:22457 [D loss: 0.593309, acc: 64.84%] [G loss: 2.145834]\n",
      "epoch:23 step:22458 [D loss: 0.612994, acc: 62.50%] [G loss: 1.959818]\n",
      "epoch:23 step:22459 [D loss: 0.664416, acc: 60.94%] [G loss: 1.963061]\n",
      "epoch:23 step:22460 [D loss: 0.615944, acc: 65.62%] [G loss: 2.160778]\n",
      "epoch:23 step:22461 [D loss: 0.615456, acc: 64.84%] [G loss: 1.941566]\n",
      "epoch:23 step:22462 [D loss: 0.632483, acc: 59.38%] [G loss: 1.924387]\n",
      "epoch:23 step:22463 [D loss: 0.615178, acc: 66.41%] [G loss: 2.019216]\n",
      "epoch:23 step:22464 [D loss: 0.634632, acc: 60.94%] [G loss: 1.829497]\n",
      "epoch:23 step:22465 [D loss: 0.620690, acc: 63.28%] [G loss: 1.914280]\n",
      "epoch:23 step:22466 [D loss: 0.669775, acc: 54.69%] [G loss: 2.049385]\n",
      "epoch:23 step:22467 [D loss: 0.609003, acc: 65.62%] [G loss: 2.027437]\n",
      "epoch:23 step:22468 [D loss: 0.623843, acc: 71.88%] [G loss: 1.909389]\n",
      "epoch:23 step:22469 [D loss: 0.564403, acc: 71.88%] [G loss: 2.106046]\n",
      "epoch:23 step:22470 [D loss: 0.639293, acc: 67.19%] [G loss: 2.265709]\n",
      "epoch:23 step:22471 [D loss: 0.718164, acc: 53.91%] [G loss: 1.832214]\n",
      "epoch:23 step:22472 [D loss: 0.627002, acc: 60.94%] [G loss: 2.058001]\n",
      "epoch:23 step:22473 [D loss: 0.665693, acc: 59.38%] [G loss: 1.893071]\n",
      "epoch:23 step:22474 [D loss: 0.580198, acc: 72.66%] [G loss: 2.248618]\n",
      "epoch:23 step:22475 [D loss: 0.630199, acc: 66.41%] [G loss: 2.227586]\n",
      "epoch:23 step:22476 [D loss: 0.545649, acc: 77.34%] [G loss: 2.208381]\n",
      "epoch:23 step:22477 [D loss: 0.640524, acc: 60.94%] [G loss: 2.115203]\n",
      "epoch:23 step:22478 [D loss: 0.637685, acc: 70.31%] [G loss: 2.196046]\n",
      "epoch:23 step:22479 [D loss: 0.752543, acc: 53.91%] [G loss: 1.746397]\n",
      "epoch:23 step:22480 [D loss: 0.731678, acc: 47.66%] [G loss: 1.888633]\n",
      "epoch:23 step:22481 [D loss: 0.615940, acc: 63.28%] [G loss: 2.119370]\n",
      "epoch:23 step:22482 [D loss: 0.611632, acc: 68.75%] [G loss: 2.066177]\n",
      "epoch:23 step:22483 [D loss: 0.634896, acc: 61.72%] [G loss: 1.811837]\n",
      "epoch:23 step:22484 [D loss: 0.638090, acc: 64.84%] [G loss: 1.964485]\n",
      "epoch:23 step:22485 [D loss: 0.672008, acc: 60.94%] [G loss: 1.965709]\n",
      "epoch:23 step:22486 [D loss: 0.645128, acc: 64.84%] [G loss: 1.872591]\n",
      "epoch:23 step:22487 [D loss: 0.537774, acc: 75.78%] [G loss: 2.072066]\n",
      "epoch:23 step:22488 [D loss: 0.597094, acc: 70.31%] [G loss: 2.509574]\n",
      "epoch:24 step:22489 [D loss: 0.709052, acc: 57.03%] [G loss: 1.826150]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:24 step:22490 [D loss: 0.706604, acc: 53.91%] [G loss: 1.874973]\n",
      "epoch:24 step:22491 [D loss: 0.646035, acc: 64.06%] [G loss: 1.974070]\n",
      "epoch:24 step:22492 [D loss: 0.655038, acc: 64.84%] [G loss: 1.866633]\n",
      "epoch:24 step:22493 [D loss: 0.660028, acc: 57.81%] [G loss: 1.800943]\n",
      "epoch:24 step:22494 [D loss: 0.599211, acc: 66.41%] [G loss: 1.883441]\n",
      "epoch:24 step:22495 [D loss: 0.669589, acc: 57.81%] [G loss: 1.943856]\n",
      "epoch:24 step:22496 [D loss: 0.627595, acc: 59.38%] [G loss: 2.044419]\n",
      "epoch:24 step:22497 [D loss: 0.634453, acc: 65.62%] [G loss: 2.063193]\n",
      "epoch:24 step:22498 [D loss: 0.635188, acc: 57.81%] [G loss: 1.992095]\n",
      "epoch:24 step:22499 [D loss: 0.634570, acc: 67.19%] [G loss: 2.018041]\n",
      "epoch:24 step:22500 [D loss: 0.636825, acc: 67.19%] [G loss: 1.877527]\n",
      "epoch:24 step:22501 [D loss: 0.606157, acc: 70.31%] [G loss: 1.887307]\n",
      "epoch:24 step:22502 [D loss: 0.598832, acc: 66.41%] [G loss: 1.914319]\n",
      "epoch:24 step:22503 [D loss: 0.572662, acc: 76.56%] [G loss: 2.057729]\n",
      "epoch:24 step:22504 [D loss: 0.579527, acc: 67.97%] [G loss: 2.220181]\n",
      "epoch:24 step:22505 [D loss: 0.637658, acc: 68.75%] [G loss: 1.927129]\n",
      "epoch:24 step:22506 [D loss: 0.682842, acc: 56.25%] [G loss: 1.918995]\n",
      "epoch:24 step:22507 [D loss: 0.614595, acc: 63.28%] [G loss: 1.960532]\n",
      "epoch:24 step:22508 [D loss: 0.718451, acc: 55.47%] [G loss: 1.727539]\n",
      "epoch:24 step:22509 [D loss: 0.670487, acc: 63.28%] [G loss: 1.703933]\n",
      "epoch:24 step:22510 [D loss: 0.684991, acc: 57.03%] [G loss: 1.801474]\n",
      "epoch:24 step:22511 [D loss: 0.650787, acc: 61.72%] [G loss: 1.988445]\n",
      "epoch:24 step:22512 [D loss: 0.628594, acc: 64.84%] [G loss: 2.015581]\n",
      "epoch:24 step:22513 [D loss: 0.616936, acc: 65.62%] [G loss: 1.921629]\n",
      "epoch:24 step:22514 [D loss: 0.685549, acc: 57.81%] [G loss: 1.792238]\n",
      "epoch:24 step:22515 [D loss: 0.686830, acc: 56.25%] [G loss: 1.806416]\n",
      "epoch:24 step:22516 [D loss: 0.662485, acc: 60.94%] [G loss: 1.918723]\n",
      "epoch:24 step:22517 [D loss: 0.592719, acc: 67.97%] [G loss: 1.819083]\n",
      "epoch:24 step:22518 [D loss: 0.634637, acc: 62.50%] [G loss: 1.927759]\n",
      "epoch:24 step:22519 [D loss: 0.716851, acc: 51.56%] [G loss: 1.656232]\n",
      "epoch:24 step:22520 [D loss: 0.682802, acc: 53.12%] [G loss: 1.816388]\n",
      "epoch:24 step:22521 [D loss: 0.629804, acc: 61.72%] [G loss: 1.876118]\n",
      "epoch:24 step:22522 [D loss: 0.641325, acc: 65.62%] [G loss: 1.914333]\n",
      "epoch:24 step:22523 [D loss: 0.653077, acc: 63.28%] [G loss: 1.741833]\n",
      "epoch:24 step:22524 [D loss: 0.613824, acc: 65.62%] [G loss: 1.972672]\n",
      "epoch:24 step:22525 [D loss: 0.631081, acc: 62.50%] [G loss: 2.084969]\n",
      "epoch:24 step:22526 [D loss: 0.664925, acc: 57.03%] [G loss: 2.017132]\n",
      "epoch:24 step:22527 [D loss: 0.626896, acc: 61.72%] [G loss: 1.971658]\n",
      "epoch:24 step:22528 [D loss: 0.586277, acc: 66.41%] [G loss: 2.156096]\n",
      "epoch:24 step:22529 [D loss: 0.665617, acc: 55.47%] [G loss: 1.845333]\n",
      "epoch:24 step:22530 [D loss: 0.647841, acc: 63.28%] [G loss: 1.914660]\n",
      "epoch:24 step:22531 [D loss: 0.635958, acc: 64.84%] [G loss: 1.960459]\n",
      "epoch:24 step:22532 [D loss: 0.646781, acc: 63.28%] [G loss: 1.915271]\n",
      "epoch:24 step:22533 [D loss: 0.609152, acc: 67.19%] [G loss: 1.800253]\n",
      "epoch:24 step:22534 [D loss: 0.642770, acc: 62.50%] [G loss: 1.878742]\n",
      "epoch:24 step:22535 [D loss: 0.652130, acc: 64.06%] [G loss: 2.019805]\n",
      "epoch:24 step:22536 [D loss: 0.618852, acc: 63.28%] [G loss: 2.037248]\n",
      "epoch:24 step:22537 [D loss: 0.599331, acc: 67.97%] [G loss: 1.924582]\n",
      "epoch:24 step:22538 [D loss: 0.640980, acc: 61.72%] [G loss: 1.910127]\n",
      "epoch:24 step:22539 [D loss: 0.662165, acc: 56.25%] [G loss: 1.879632]\n",
      "epoch:24 step:22540 [D loss: 0.679300, acc: 60.94%] [G loss: 1.975218]\n",
      "epoch:24 step:22541 [D loss: 0.675547, acc: 60.16%] [G loss: 1.942988]\n",
      "epoch:24 step:22542 [D loss: 0.597093, acc: 72.66%] [G loss: 1.946287]\n",
      "epoch:24 step:22543 [D loss: 0.719009, acc: 57.81%] [G loss: 1.901559]\n",
      "epoch:24 step:22544 [D loss: 0.598181, acc: 67.97%] [G loss: 1.938337]\n",
      "epoch:24 step:22545 [D loss: 0.658312, acc: 68.75%] [G loss: 1.806360]\n",
      "epoch:24 step:22546 [D loss: 0.646298, acc: 59.38%] [G loss: 1.886296]\n",
      "epoch:24 step:22547 [D loss: 0.669585, acc: 55.47%] [G loss: 1.963653]\n",
      "epoch:24 step:22548 [D loss: 0.660583, acc: 57.81%] [G loss: 1.839197]\n",
      "epoch:24 step:22549 [D loss: 0.622035, acc: 63.28%] [G loss: 1.903588]\n",
      "epoch:24 step:22550 [D loss: 0.642393, acc: 64.06%] [G loss: 1.823570]\n",
      "epoch:24 step:22551 [D loss: 0.645390, acc: 60.16%] [G loss: 1.932644]\n",
      "epoch:24 step:22552 [D loss: 0.697864, acc: 51.56%] [G loss: 2.040556]\n",
      "epoch:24 step:22553 [D loss: 0.656389, acc: 64.06%] [G loss: 1.947316]\n",
      "epoch:24 step:22554 [D loss: 0.632880, acc: 66.41%] [G loss: 1.865836]\n",
      "epoch:24 step:22555 [D loss: 0.674913, acc: 53.91%] [G loss: 1.855331]\n",
      "epoch:24 step:22556 [D loss: 0.664216, acc: 62.50%] [G loss: 1.899814]\n",
      "epoch:24 step:22557 [D loss: 0.632997, acc: 64.06%] [G loss: 1.969118]\n",
      "epoch:24 step:22558 [D loss: 0.629185, acc: 64.06%] [G loss: 1.855531]\n",
      "epoch:24 step:22559 [D loss: 0.671047, acc: 60.94%] [G loss: 1.742289]\n",
      "epoch:24 step:22560 [D loss: 0.617271, acc: 67.19%] [G loss: 1.757627]\n",
      "epoch:24 step:22561 [D loss: 0.651467, acc: 57.03%] [G loss: 1.819589]\n",
      "epoch:24 step:22562 [D loss: 0.656210, acc: 60.16%] [G loss: 1.921355]\n",
      "epoch:24 step:22563 [D loss: 0.623724, acc: 64.84%] [G loss: 2.066143]\n",
      "epoch:24 step:22564 [D loss: 0.595792, acc: 66.41%] [G loss: 2.002605]\n",
      "epoch:24 step:22565 [D loss: 0.589608, acc: 70.31%] [G loss: 2.046604]\n",
      "epoch:24 step:22566 [D loss: 0.712547, acc: 53.91%] [G loss: 1.877663]\n",
      "epoch:24 step:22567 [D loss: 0.634062, acc: 64.84%] [G loss: 1.839233]\n",
      "epoch:24 step:22568 [D loss: 0.654559, acc: 58.59%] [G loss: 1.814318]\n",
      "epoch:24 step:22569 [D loss: 0.711528, acc: 55.47%] [G loss: 1.701602]\n",
      "epoch:24 step:22570 [D loss: 0.653871, acc: 61.72%] [G loss: 1.844495]\n",
      "epoch:24 step:22571 [D loss: 0.646557, acc: 59.38%] [G loss: 1.827040]\n",
      "epoch:24 step:22572 [D loss: 0.682058, acc: 54.69%] [G loss: 1.896194]\n",
      "epoch:24 step:22573 [D loss: 0.676284, acc: 57.03%] [G loss: 1.856855]\n",
      "epoch:24 step:22574 [D loss: 0.669666, acc: 63.28%] [G loss: 1.760405]\n",
      "epoch:24 step:22575 [D loss: 0.623557, acc: 64.84%] [G loss: 1.905222]\n",
      "epoch:24 step:22576 [D loss: 0.631398, acc: 67.97%] [G loss: 1.852188]\n",
      "epoch:24 step:22577 [D loss: 0.610085, acc: 71.09%] [G loss: 1.938359]\n",
      "epoch:24 step:22578 [D loss: 0.646703, acc: 64.06%] [G loss: 1.898701]\n",
      "epoch:24 step:22579 [D loss: 0.626019, acc: 64.84%] [G loss: 1.916970]\n",
      "epoch:24 step:22580 [D loss: 0.649614, acc: 60.94%] [G loss: 1.809543]\n",
      "epoch:24 step:22581 [D loss: 0.668174, acc: 59.38%] [G loss: 1.994575]\n",
      "epoch:24 step:22582 [D loss: 0.672987, acc: 57.81%] [G loss: 1.840550]\n",
      "epoch:24 step:22583 [D loss: 0.662503, acc: 60.16%] [G loss: 1.882676]\n",
      "epoch:24 step:22584 [D loss: 0.651365, acc: 60.16%] [G loss: 1.892073]\n",
      "epoch:24 step:22585 [D loss: 0.640742, acc: 63.28%] [G loss: 1.791251]\n",
      "epoch:24 step:22586 [D loss: 0.717469, acc: 48.44%] [G loss: 1.780146]\n",
      "epoch:24 step:22587 [D loss: 0.656317, acc: 57.03%] [G loss: 1.728793]\n",
      "epoch:24 step:22588 [D loss: 0.610087, acc: 71.09%] [G loss: 1.979735]\n",
      "epoch:24 step:22589 [D loss: 0.622862, acc: 64.06%] [G loss: 1.894857]\n",
      "epoch:24 step:22590 [D loss: 0.652685, acc: 60.16%] [G loss: 1.787956]\n",
      "epoch:24 step:22591 [D loss: 0.645747, acc: 62.50%] [G loss: 1.854328]\n",
      "epoch:24 step:22592 [D loss: 0.650273, acc: 61.72%] [G loss: 1.820411]\n",
      "epoch:24 step:22593 [D loss: 0.671277, acc: 60.16%] [G loss: 1.892339]\n",
      "epoch:24 step:22594 [D loss: 0.640012, acc: 64.84%] [G loss: 1.930369]\n",
      "epoch:24 step:22595 [D loss: 0.620043, acc: 59.38%] [G loss: 2.211669]\n",
      "epoch:24 step:22596 [D loss: 0.689533, acc: 56.25%] [G loss: 1.856503]\n",
      "epoch:24 step:22597 [D loss: 0.673360, acc: 57.03%] [G loss: 1.784421]\n",
      "epoch:24 step:22598 [D loss: 0.631103, acc: 62.50%] [G loss: 1.828192]\n",
      "epoch:24 step:22599 [D loss: 0.650558, acc: 56.25%] [G loss: 2.002612]\n",
      "epoch:24 step:22600 [D loss: 0.583657, acc: 68.75%] [G loss: 1.965895]\n",
      "##############\n",
      "[2.46089502 1.54321378 6.25621084 4.7077689  3.45652682 5.64962468\n",
      " 4.363028   4.58705005 4.49597144 3.64397947]\n",
      "##########\n",
      "epoch:24 step:22601 [D loss: 0.629475, acc: 64.84%] [G loss: 1.948343]\n",
      "epoch:24 step:22602 [D loss: 0.599404, acc: 60.94%] [G loss: 2.023141]\n",
      "epoch:24 step:22603 [D loss: 0.590993, acc: 68.75%] [G loss: 2.093135]\n",
      "epoch:24 step:22604 [D loss: 0.617258, acc: 62.50%] [G loss: 2.215607]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:24 step:22605 [D loss: 0.626750, acc: 66.41%] [G loss: 2.185826]\n",
      "epoch:24 step:22606 [D loss: 0.657099, acc: 57.03%] [G loss: 2.131696]\n",
      "epoch:24 step:22607 [D loss: 0.614859, acc: 69.53%] [G loss: 2.214105]\n",
      "epoch:24 step:22608 [D loss: 0.650011, acc: 64.06%] [G loss: 1.877897]\n",
      "epoch:24 step:22609 [D loss: 0.640024, acc: 67.97%] [G loss: 1.911616]\n",
      "epoch:24 step:22610 [D loss: 0.597838, acc: 67.97%] [G loss: 2.250547]\n",
      "epoch:24 step:22611 [D loss: 0.676002, acc: 60.94%] [G loss: 1.846145]\n",
      "epoch:24 step:22612 [D loss: 0.640405, acc: 56.25%] [G loss: 1.823065]\n",
      "epoch:24 step:22613 [D loss: 0.722623, acc: 47.66%] [G loss: 1.868962]\n",
      "epoch:24 step:22614 [D loss: 0.612340, acc: 64.06%] [G loss: 1.834710]\n",
      "epoch:24 step:22615 [D loss: 0.683340, acc: 57.03%] [G loss: 1.814425]\n",
      "epoch:24 step:22616 [D loss: 0.642300, acc: 58.59%] [G loss: 1.853543]\n",
      "epoch:24 step:22617 [D loss: 0.646020, acc: 62.50%] [G loss: 1.768932]\n",
      "epoch:24 step:22618 [D loss: 0.646073, acc: 61.72%] [G loss: 1.923923]\n",
      "epoch:24 step:22619 [D loss: 0.638019, acc: 63.28%] [G loss: 1.897485]\n",
      "epoch:24 step:22620 [D loss: 0.630863, acc: 63.28%] [G loss: 1.836346]\n",
      "epoch:24 step:22621 [D loss: 0.717556, acc: 50.78%] [G loss: 1.817889]\n",
      "epoch:24 step:22622 [D loss: 0.654358, acc: 59.38%] [G loss: 1.665953]\n",
      "epoch:24 step:22623 [D loss: 0.645107, acc: 63.28%] [G loss: 1.814159]\n",
      "epoch:24 step:22624 [D loss: 0.671006, acc: 67.19%] [G loss: 1.893621]\n",
      "epoch:24 step:22625 [D loss: 0.646983, acc: 57.03%] [G loss: 1.886502]\n",
      "epoch:24 step:22626 [D loss: 0.665957, acc: 58.59%] [G loss: 1.799300]\n",
      "epoch:24 step:22627 [D loss: 0.659817, acc: 59.38%] [G loss: 1.838566]\n",
      "epoch:24 step:22628 [D loss: 0.683048, acc: 57.03%] [G loss: 1.776503]\n",
      "epoch:24 step:22629 [D loss: 0.643445, acc: 64.06%] [G loss: 1.898456]\n",
      "epoch:24 step:22630 [D loss: 0.667466, acc: 58.59%] [G loss: 1.805363]\n",
      "epoch:24 step:22631 [D loss: 0.613169, acc: 67.19%] [G loss: 1.825632]\n",
      "epoch:24 step:22632 [D loss: 0.649861, acc: 64.84%] [G loss: 1.867656]\n",
      "epoch:24 step:22633 [D loss: 0.677713, acc: 57.03%] [G loss: 1.874439]\n",
      "epoch:24 step:22634 [D loss: 0.590139, acc: 64.06%] [G loss: 2.011790]\n",
      "epoch:24 step:22635 [D loss: 0.636145, acc: 64.84%] [G loss: 1.952077]\n",
      "epoch:24 step:22636 [D loss: 0.626494, acc: 61.72%] [G loss: 1.726357]\n",
      "epoch:24 step:22637 [D loss: 0.648605, acc: 54.69%] [G loss: 1.920442]\n",
      "epoch:24 step:22638 [D loss: 0.694762, acc: 53.12%] [G loss: 1.923733]\n",
      "epoch:24 step:22639 [D loss: 0.618160, acc: 64.84%] [G loss: 1.912170]\n",
      "epoch:24 step:22640 [D loss: 0.639977, acc: 67.97%] [G loss: 1.993400]\n",
      "epoch:24 step:22641 [D loss: 0.688655, acc: 57.03%] [G loss: 1.744858]\n",
      "epoch:24 step:22642 [D loss: 0.680842, acc: 55.47%] [G loss: 1.977567]\n",
      "epoch:24 step:22643 [D loss: 0.692678, acc: 59.38%] [G loss: 1.854838]\n",
      "epoch:24 step:22644 [D loss: 0.622084, acc: 64.06%] [G loss: 1.994034]\n",
      "epoch:24 step:22645 [D loss: 0.681335, acc: 60.94%] [G loss: 1.773714]\n",
      "epoch:24 step:22646 [D loss: 0.706774, acc: 51.56%] [G loss: 1.791258]\n",
      "epoch:24 step:22647 [D loss: 0.648031, acc: 66.41%] [G loss: 1.661105]\n",
      "epoch:24 step:22648 [D loss: 0.673658, acc: 59.38%] [G loss: 1.739751]\n",
      "epoch:24 step:22649 [D loss: 0.632786, acc: 65.62%] [G loss: 1.745478]\n",
      "epoch:24 step:22650 [D loss: 0.611901, acc: 69.53%] [G loss: 1.780971]\n",
      "epoch:24 step:22651 [D loss: 0.644160, acc: 57.81%] [G loss: 1.833966]\n",
      "epoch:24 step:22652 [D loss: 0.662576, acc: 55.47%] [G loss: 1.872475]\n",
      "epoch:24 step:22653 [D loss: 0.674863, acc: 57.03%] [G loss: 1.686178]\n",
      "epoch:24 step:22654 [D loss: 0.670304, acc: 60.94%] [G loss: 1.918490]\n",
      "epoch:24 step:22655 [D loss: 0.630090, acc: 62.50%] [G loss: 1.861344]\n",
      "epoch:24 step:22656 [D loss: 0.625814, acc: 61.72%] [G loss: 1.836292]\n",
      "epoch:24 step:22657 [D loss: 0.612492, acc: 65.62%] [G loss: 1.835929]\n",
      "epoch:24 step:22658 [D loss: 0.660206, acc: 57.81%] [G loss: 1.718321]\n",
      "epoch:24 step:22659 [D loss: 0.613009, acc: 68.75%] [G loss: 2.097683]\n",
      "epoch:24 step:22660 [D loss: 0.664574, acc: 64.06%] [G loss: 1.918066]\n",
      "epoch:24 step:22661 [D loss: 0.632645, acc: 65.62%] [G loss: 1.957056]\n",
      "epoch:24 step:22662 [D loss: 0.663469, acc: 59.38%] [G loss: 1.795333]\n",
      "epoch:24 step:22663 [D loss: 0.624876, acc: 63.28%] [G loss: 1.840701]\n",
      "epoch:24 step:22664 [D loss: 0.633391, acc: 64.84%] [G loss: 1.758974]\n",
      "epoch:24 step:22665 [D loss: 0.611112, acc: 62.50%] [G loss: 1.848138]\n",
      "epoch:24 step:22666 [D loss: 0.645524, acc: 72.66%] [G loss: 1.850229]\n",
      "epoch:24 step:22667 [D loss: 0.641062, acc: 60.94%] [G loss: 1.804303]\n",
      "epoch:24 step:22668 [D loss: 0.684069, acc: 60.94%] [G loss: 1.849012]\n",
      "epoch:24 step:22669 [D loss: 0.651530, acc: 58.59%] [G loss: 1.721418]\n",
      "epoch:24 step:22670 [D loss: 0.668227, acc: 67.19%] [G loss: 1.841125]\n",
      "epoch:24 step:22671 [D loss: 0.677252, acc: 63.28%] [G loss: 1.808994]\n",
      "epoch:24 step:22672 [D loss: 0.700008, acc: 57.03%] [G loss: 1.867856]\n",
      "epoch:24 step:22673 [D loss: 0.611937, acc: 68.75%] [G loss: 1.950249]\n",
      "epoch:24 step:22674 [D loss: 0.671069, acc: 58.59%] [G loss: 1.753867]\n",
      "epoch:24 step:22675 [D loss: 0.712907, acc: 54.69%] [G loss: 1.806152]\n",
      "epoch:24 step:22676 [D loss: 0.658145, acc: 57.81%] [G loss: 1.850814]\n",
      "epoch:24 step:22677 [D loss: 0.644172, acc: 61.72%] [G loss: 1.848240]\n",
      "epoch:24 step:22678 [D loss: 0.662490, acc: 60.94%] [G loss: 1.942585]\n",
      "epoch:24 step:22679 [D loss: 0.616727, acc: 69.53%] [G loss: 1.992228]\n",
      "epoch:24 step:22680 [D loss: 0.630822, acc: 65.62%] [G loss: 1.996409]\n",
      "epoch:24 step:22681 [D loss: 0.672388, acc: 57.03%] [G loss: 1.885213]\n",
      "epoch:24 step:22682 [D loss: 0.636870, acc: 66.41%] [G loss: 1.890587]\n",
      "epoch:24 step:22683 [D loss: 0.644095, acc: 61.72%] [G loss: 1.798110]\n",
      "epoch:24 step:22684 [D loss: 0.678362, acc: 57.81%] [G loss: 2.019062]\n",
      "epoch:24 step:22685 [D loss: 0.604633, acc: 66.41%] [G loss: 1.965546]\n",
      "epoch:24 step:22686 [D loss: 0.646043, acc: 62.50%] [G loss: 1.903111]\n",
      "epoch:24 step:22687 [D loss: 0.665424, acc: 58.59%] [G loss: 1.903484]\n",
      "epoch:24 step:22688 [D loss: 0.716746, acc: 52.34%] [G loss: 1.679260]\n",
      "epoch:24 step:22689 [D loss: 0.638253, acc: 60.94%] [G loss: 1.935199]\n",
      "epoch:24 step:22690 [D loss: 0.604256, acc: 66.41%] [G loss: 1.871065]\n",
      "epoch:24 step:22691 [D loss: 0.711637, acc: 53.91%] [G loss: 1.900603]\n",
      "epoch:24 step:22692 [D loss: 0.641858, acc: 65.62%] [G loss: 1.795858]\n",
      "epoch:24 step:22693 [D loss: 0.614586, acc: 66.41%] [G loss: 1.701290]\n",
      "epoch:24 step:22694 [D loss: 0.626252, acc: 66.41%] [G loss: 1.906908]\n",
      "epoch:24 step:22695 [D loss: 0.627272, acc: 58.59%] [G loss: 2.083754]\n",
      "epoch:24 step:22696 [D loss: 0.555662, acc: 71.88%] [G loss: 2.212951]\n",
      "epoch:24 step:22697 [D loss: 0.627795, acc: 65.62%] [G loss: 2.126526]\n",
      "epoch:24 step:22698 [D loss: 0.663865, acc: 61.72%] [G loss: 1.836619]\n",
      "epoch:24 step:22699 [D loss: 0.697118, acc: 61.72%] [G loss: 1.844187]\n",
      "epoch:24 step:22700 [D loss: 0.707140, acc: 58.59%] [G loss: 1.682962]\n",
      "epoch:24 step:22701 [D loss: 0.601782, acc: 67.97%] [G loss: 1.844961]\n",
      "epoch:24 step:22702 [D loss: 0.682278, acc: 57.03%] [G loss: 1.756931]\n",
      "epoch:24 step:22703 [D loss: 0.724003, acc: 50.78%] [G loss: 1.786406]\n",
      "epoch:24 step:22704 [D loss: 0.653834, acc: 58.59%] [G loss: 1.836751]\n",
      "epoch:24 step:22705 [D loss: 0.598471, acc: 65.62%] [G loss: 1.917483]\n",
      "epoch:24 step:22706 [D loss: 0.618078, acc: 60.94%] [G loss: 1.989252]\n",
      "epoch:24 step:22707 [D loss: 0.600606, acc: 67.19%] [G loss: 2.183547]\n",
      "epoch:24 step:22708 [D loss: 0.675873, acc: 60.16%] [G loss: 1.798119]\n",
      "epoch:24 step:22709 [D loss: 0.623738, acc: 61.72%] [G loss: 1.881287]\n",
      "epoch:24 step:22710 [D loss: 0.642752, acc: 64.06%] [G loss: 1.941287]\n",
      "epoch:24 step:22711 [D loss: 0.637103, acc: 62.50%] [G loss: 1.867832]\n",
      "epoch:24 step:22712 [D loss: 0.602803, acc: 68.75%] [G loss: 1.809879]\n",
      "epoch:24 step:22713 [D loss: 0.617514, acc: 66.41%] [G loss: 1.883907]\n",
      "epoch:24 step:22714 [D loss: 0.604521, acc: 66.41%] [G loss: 1.915529]\n",
      "epoch:24 step:22715 [D loss: 0.653062, acc: 60.94%] [G loss: 1.941889]\n",
      "epoch:24 step:22716 [D loss: 0.688090, acc: 58.59%] [G loss: 1.799568]\n",
      "epoch:24 step:22717 [D loss: 0.569909, acc: 73.44%] [G loss: 2.097111]\n",
      "epoch:24 step:22718 [D loss: 0.600596, acc: 67.97%] [G loss: 2.173483]\n",
      "epoch:24 step:22719 [D loss: 0.552155, acc: 71.09%] [G loss: 2.307983]\n",
      "epoch:24 step:22720 [D loss: 0.609651, acc: 65.62%] [G loss: 2.351718]\n",
      "epoch:24 step:22721 [D loss: 0.672954, acc: 60.16%] [G loss: 1.912923]\n",
      "epoch:24 step:22722 [D loss: 0.652551, acc: 67.19%] [G loss: 1.946012]\n",
      "epoch:24 step:22723 [D loss: 0.678131, acc: 61.72%] [G loss: 1.969582]\n",
      "epoch:24 step:22724 [D loss: 0.635646, acc: 59.38%] [G loss: 1.865090]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:24 step:22725 [D loss: 0.627714, acc: 65.62%] [G loss: 1.942841]\n",
      "epoch:24 step:22726 [D loss: 0.643123, acc: 67.97%] [G loss: 1.813700]\n",
      "epoch:24 step:22727 [D loss: 0.687953, acc: 57.81%] [G loss: 1.852697]\n",
      "epoch:24 step:22728 [D loss: 0.608531, acc: 67.19%] [G loss: 1.973085]\n",
      "epoch:24 step:22729 [D loss: 0.647606, acc: 67.19%] [G loss: 2.047508]\n",
      "epoch:24 step:22730 [D loss: 0.643242, acc: 62.50%] [G loss: 2.058547]\n",
      "epoch:24 step:22731 [D loss: 0.639671, acc: 66.41%] [G loss: 1.932701]\n",
      "epoch:24 step:22732 [D loss: 0.661693, acc: 60.16%] [G loss: 1.889603]\n",
      "epoch:24 step:22733 [D loss: 0.614400, acc: 70.31%] [G loss: 1.964364]\n",
      "epoch:24 step:22734 [D loss: 0.655279, acc: 64.84%] [G loss: 1.970207]\n",
      "epoch:24 step:22735 [D loss: 0.663384, acc: 59.38%] [G loss: 1.947520]\n",
      "epoch:24 step:22736 [D loss: 0.612821, acc: 64.84%] [G loss: 1.897912]\n",
      "epoch:24 step:22737 [D loss: 0.700929, acc: 57.81%] [G loss: 1.897581]\n",
      "epoch:24 step:22738 [D loss: 0.689153, acc: 57.81%] [G loss: 1.691645]\n",
      "epoch:24 step:22739 [D loss: 0.670591, acc: 53.91%] [G loss: 1.669722]\n",
      "epoch:24 step:22740 [D loss: 0.682266, acc: 59.38%] [G loss: 1.727714]\n",
      "epoch:24 step:22741 [D loss: 0.656167, acc: 60.94%] [G loss: 1.768786]\n",
      "epoch:24 step:22742 [D loss: 0.622662, acc: 60.16%] [G loss: 1.827694]\n",
      "epoch:24 step:22743 [D loss: 0.623643, acc: 60.94%] [G loss: 1.835268]\n",
      "epoch:24 step:22744 [D loss: 0.626434, acc: 64.84%] [G loss: 1.770259]\n",
      "epoch:24 step:22745 [D loss: 0.660118, acc: 56.25%] [G loss: 1.857331]\n",
      "epoch:24 step:22746 [D loss: 0.639141, acc: 65.62%] [G loss: 1.885341]\n",
      "epoch:24 step:22747 [D loss: 0.666409, acc: 60.16%] [G loss: 1.860674]\n",
      "epoch:24 step:22748 [D loss: 0.677024, acc: 60.16%] [G loss: 1.907099]\n",
      "epoch:24 step:22749 [D loss: 0.570101, acc: 73.44%] [G loss: 2.049828]\n",
      "epoch:24 step:22750 [D loss: 0.596480, acc: 65.62%] [G loss: 1.955386]\n",
      "epoch:24 step:22751 [D loss: 0.638294, acc: 57.03%] [G loss: 2.116230]\n",
      "epoch:24 step:22752 [D loss: 0.645058, acc: 64.84%] [G loss: 2.195636]\n",
      "epoch:24 step:22753 [D loss: 0.659672, acc: 59.38%] [G loss: 1.835471]\n",
      "epoch:24 step:22754 [D loss: 0.613604, acc: 64.84%] [G loss: 1.896671]\n",
      "epoch:24 step:22755 [D loss: 0.654598, acc: 59.38%] [G loss: 1.908296]\n",
      "epoch:24 step:22756 [D loss: 0.663751, acc: 53.12%] [G loss: 1.797251]\n",
      "epoch:24 step:22757 [D loss: 0.621333, acc: 64.84%] [G loss: 1.967581]\n",
      "epoch:24 step:22758 [D loss: 0.633933, acc: 64.84%] [G loss: 1.863246]\n",
      "epoch:24 step:22759 [D loss: 0.639832, acc: 60.94%] [G loss: 1.950999]\n",
      "epoch:24 step:22760 [D loss: 0.645512, acc: 61.72%] [G loss: 1.917742]\n",
      "epoch:24 step:22761 [D loss: 0.642784, acc: 64.84%] [G loss: 1.970523]\n",
      "epoch:24 step:22762 [D loss: 0.591818, acc: 66.41%] [G loss: 2.080507]\n",
      "epoch:24 step:22763 [D loss: 0.651276, acc: 61.72%] [G loss: 2.204216]\n",
      "epoch:24 step:22764 [D loss: 0.596707, acc: 70.31%] [G loss: 2.224490]\n",
      "epoch:24 step:22765 [D loss: 0.647333, acc: 64.06%] [G loss: 1.946528]\n",
      "epoch:24 step:22766 [D loss: 0.639262, acc: 67.97%] [G loss: 1.830277]\n",
      "epoch:24 step:22767 [D loss: 0.638276, acc: 60.16%] [G loss: 1.934453]\n",
      "epoch:24 step:22768 [D loss: 0.670992, acc: 63.28%] [G loss: 1.964546]\n",
      "epoch:24 step:22769 [D loss: 0.695801, acc: 58.59%] [G loss: 1.941108]\n",
      "epoch:24 step:22770 [D loss: 0.693580, acc: 61.72%] [G loss: 1.804687]\n",
      "epoch:24 step:22771 [D loss: 0.642165, acc: 64.84%] [G loss: 1.844451]\n",
      "epoch:24 step:22772 [D loss: 0.606914, acc: 64.84%] [G loss: 1.828927]\n",
      "epoch:24 step:22773 [D loss: 0.717170, acc: 54.69%] [G loss: 1.811557]\n",
      "epoch:24 step:22774 [D loss: 0.669416, acc: 66.41%] [G loss: 1.901825]\n",
      "epoch:24 step:22775 [D loss: 0.678480, acc: 57.81%] [G loss: 1.815041]\n",
      "epoch:24 step:22776 [D loss: 0.687585, acc: 57.03%] [G loss: 1.887204]\n",
      "epoch:24 step:22777 [D loss: 0.661023, acc: 58.59%] [G loss: 1.816877]\n",
      "epoch:24 step:22778 [D loss: 0.681713, acc: 61.72%] [G loss: 1.849636]\n",
      "epoch:24 step:22779 [D loss: 0.665202, acc: 63.28%] [G loss: 1.748773]\n",
      "epoch:24 step:22780 [D loss: 0.635768, acc: 56.25%] [G loss: 1.941411]\n",
      "epoch:24 step:22781 [D loss: 0.635177, acc: 61.72%] [G loss: 1.999534]\n",
      "epoch:24 step:22782 [D loss: 0.636818, acc: 65.62%] [G loss: 1.943099]\n",
      "epoch:24 step:22783 [D loss: 0.660063, acc: 59.38%] [G loss: 1.802962]\n",
      "epoch:24 step:22784 [D loss: 0.614237, acc: 67.97%] [G loss: 1.882336]\n",
      "epoch:24 step:22785 [D loss: 0.719461, acc: 54.69%] [G loss: 1.860898]\n",
      "epoch:24 step:22786 [D loss: 0.663387, acc: 60.16%] [G loss: 1.819263]\n",
      "epoch:24 step:22787 [D loss: 0.631078, acc: 64.84%] [G loss: 1.818094]\n",
      "epoch:24 step:22788 [D loss: 0.653561, acc: 60.16%] [G loss: 1.943357]\n",
      "epoch:24 step:22789 [D loss: 0.662067, acc: 57.81%] [G loss: 1.769858]\n",
      "epoch:24 step:22790 [D loss: 0.630352, acc: 66.41%] [G loss: 1.876096]\n",
      "epoch:24 step:22791 [D loss: 0.652395, acc: 60.94%] [G loss: 1.893945]\n",
      "epoch:24 step:22792 [D loss: 0.617009, acc: 64.06%] [G loss: 1.727711]\n",
      "epoch:24 step:22793 [D loss: 0.620964, acc: 62.50%] [G loss: 1.840967]\n",
      "epoch:24 step:22794 [D loss: 0.639432, acc: 64.84%] [G loss: 1.798766]\n",
      "epoch:24 step:22795 [D loss: 0.625777, acc: 65.62%] [G loss: 1.925679]\n",
      "epoch:24 step:22796 [D loss: 0.671127, acc: 60.94%] [G loss: 1.764279]\n",
      "epoch:24 step:22797 [D loss: 0.624554, acc: 64.06%] [G loss: 1.757449]\n",
      "epoch:24 step:22798 [D loss: 0.678648, acc: 57.81%] [G loss: 1.792198]\n",
      "epoch:24 step:22799 [D loss: 0.656054, acc: 63.28%] [G loss: 1.920782]\n",
      "epoch:24 step:22800 [D loss: 0.582957, acc: 71.09%] [G loss: 1.993473]\n",
      "##############\n",
      "[2.461062   1.32632842 6.19360131 4.77703159 3.46172866 5.45811803\n",
      " 4.29150988 4.63905271 4.44029554 3.65612932]\n",
      "##########\n",
      "epoch:24 step:22801 [D loss: 0.601897, acc: 67.19%] [G loss: 2.040149]\n",
      "epoch:24 step:22802 [D loss: 0.585510, acc: 71.88%] [G loss: 2.254916]\n",
      "epoch:24 step:22803 [D loss: 0.576688, acc: 72.66%] [G loss: 2.337915]\n",
      "epoch:24 step:22804 [D loss: 0.705343, acc: 57.03%] [G loss: 1.751698]\n",
      "epoch:24 step:22805 [D loss: 0.711334, acc: 50.00%] [G loss: 1.898664]\n",
      "epoch:24 step:22806 [D loss: 0.629703, acc: 63.28%] [G loss: 2.015415]\n",
      "epoch:24 step:22807 [D loss: 0.664679, acc: 60.94%] [G loss: 1.813357]\n",
      "epoch:24 step:22808 [D loss: 0.670918, acc: 62.50%] [G loss: 1.810213]\n",
      "epoch:24 step:22809 [D loss: 0.667737, acc: 55.47%] [G loss: 1.936030]\n",
      "epoch:24 step:22810 [D loss: 0.646060, acc: 67.19%] [G loss: 1.776533]\n",
      "epoch:24 step:22811 [D loss: 0.684508, acc: 63.28%] [G loss: 1.812764]\n",
      "epoch:24 step:22812 [D loss: 0.661721, acc: 62.50%] [G loss: 1.807049]\n",
      "epoch:24 step:22813 [D loss: 0.677417, acc: 58.59%] [G loss: 1.774745]\n",
      "epoch:24 step:22814 [D loss: 0.627674, acc: 65.62%] [G loss: 1.712544]\n",
      "epoch:24 step:22815 [D loss: 0.655435, acc: 62.50%] [G loss: 1.912197]\n",
      "epoch:24 step:22816 [D loss: 0.651634, acc: 60.94%] [G loss: 1.797328]\n",
      "epoch:24 step:22817 [D loss: 0.618622, acc: 60.94%] [G loss: 1.821426]\n",
      "epoch:24 step:22818 [D loss: 0.600672, acc: 71.88%] [G loss: 2.065142]\n",
      "epoch:24 step:22819 [D loss: 0.595276, acc: 67.19%] [G loss: 2.051829]\n",
      "epoch:24 step:22820 [D loss: 0.695706, acc: 54.69%] [G loss: 1.938540]\n",
      "epoch:24 step:22821 [D loss: 0.609102, acc: 67.19%] [G loss: 2.007772]\n",
      "epoch:24 step:22822 [D loss: 0.699316, acc: 57.03%] [G loss: 2.000818]\n",
      "epoch:24 step:22823 [D loss: 0.687439, acc: 63.28%] [G loss: 2.044641]\n",
      "epoch:24 step:22824 [D loss: 0.640267, acc: 67.19%] [G loss: 1.930758]\n",
      "epoch:24 step:22825 [D loss: 0.659759, acc: 57.03%] [G loss: 2.054130]\n",
      "epoch:24 step:22826 [D loss: 0.666827, acc: 60.94%] [G loss: 1.966338]\n",
      "epoch:24 step:22827 [D loss: 0.642557, acc: 62.50%] [G loss: 1.871355]\n",
      "epoch:24 step:22828 [D loss: 0.608034, acc: 71.88%] [G loss: 2.071329]\n",
      "epoch:24 step:22829 [D loss: 0.660571, acc: 60.16%] [G loss: 1.720346]\n",
      "epoch:24 step:22830 [D loss: 0.668529, acc: 56.25%] [G loss: 1.724763]\n",
      "epoch:24 step:22831 [D loss: 0.681591, acc: 59.38%] [G loss: 1.677223]\n",
      "epoch:24 step:22832 [D loss: 0.677929, acc: 55.47%] [G loss: 1.766036]\n",
      "epoch:24 step:22833 [D loss: 0.630549, acc: 65.62%] [G loss: 2.056193]\n",
      "epoch:24 step:22834 [D loss: 0.614828, acc: 68.75%] [G loss: 2.070653]\n",
      "epoch:24 step:22835 [D loss: 0.538073, acc: 71.88%] [G loss: 2.270787]\n",
      "epoch:24 step:22836 [D loss: 0.661602, acc: 59.38%] [G loss: 1.895754]\n",
      "epoch:24 step:22837 [D loss: 0.725254, acc: 46.88%] [G loss: 1.737822]\n",
      "epoch:24 step:22838 [D loss: 0.642550, acc: 57.81%] [G loss: 1.896851]\n",
      "epoch:24 step:22839 [D loss: 0.629838, acc: 65.62%] [G loss: 1.872895]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:24 step:22840 [D loss: 0.657488, acc: 57.03%] [G loss: 1.972038]\n",
      "epoch:24 step:22841 [D loss: 0.680691, acc: 61.72%] [G loss: 1.899936]\n",
      "epoch:24 step:22842 [D loss: 0.616124, acc: 64.06%] [G loss: 1.965135]\n",
      "epoch:24 step:22843 [D loss: 0.662329, acc: 63.28%] [G loss: 1.799624]\n",
      "epoch:24 step:22844 [D loss: 0.650508, acc: 60.94%] [G loss: 1.857015]\n",
      "epoch:24 step:22845 [D loss: 0.604806, acc: 71.09%] [G loss: 1.891361]\n",
      "epoch:24 step:22846 [D loss: 0.587455, acc: 69.53%] [G loss: 1.907644]\n",
      "epoch:24 step:22847 [D loss: 0.623970, acc: 66.41%] [G loss: 2.053446]\n",
      "epoch:24 step:22848 [D loss: 0.656182, acc: 60.94%] [G loss: 1.879444]\n",
      "epoch:24 step:22849 [D loss: 0.617608, acc: 66.41%] [G loss: 1.756098]\n",
      "epoch:24 step:22850 [D loss: 0.651220, acc: 60.16%] [G loss: 1.932326]\n",
      "epoch:24 step:22851 [D loss: 0.688312, acc: 54.69%] [G loss: 1.947156]\n",
      "epoch:24 step:22852 [D loss: 0.605515, acc: 70.31%] [G loss: 1.922378]\n",
      "epoch:24 step:22853 [D loss: 0.655207, acc: 63.28%] [G loss: 1.872516]\n",
      "epoch:24 step:22854 [D loss: 0.641589, acc: 63.28%] [G loss: 1.786747]\n",
      "epoch:24 step:22855 [D loss: 0.673705, acc: 61.72%] [G loss: 1.995093]\n",
      "epoch:24 step:22856 [D loss: 0.656544, acc: 60.16%] [G loss: 1.831094]\n",
      "epoch:24 step:22857 [D loss: 0.703690, acc: 53.91%] [G loss: 1.940490]\n",
      "epoch:24 step:22858 [D loss: 0.641778, acc: 60.16%] [G loss: 2.031210]\n",
      "epoch:24 step:22859 [D loss: 0.614053, acc: 69.53%] [G loss: 1.963976]\n",
      "epoch:24 step:22860 [D loss: 0.639080, acc: 62.50%] [G loss: 1.949340]\n",
      "epoch:24 step:22861 [D loss: 0.728350, acc: 53.12%] [G loss: 1.788414]\n",
      "epoch:24 step:22862 [D loss: 0.633263, acc: 67.19%] [G loss: 1.897623]\n",
      "epoch:24 step:22863 [D loss: 0.661280, acc: 63.28%] [G loss: 1.908362]\n",
      "epoch:24 step:22864 [D loss: 0.636649, acc: 60.16%] [G loss: 1.796206]\n",
      "epoch:24 step:22865 [D loss: 0.685800, acc: 55.47%] [G loss: 1.789957]\n",
      "epoch:24 step:22866 [D loss: 0.675345, acc: 62.50%] [G loss: 1.709176]\n",
      "epoch:24 step:22867 [D loss: 0.616946, acc: 65.62%] [G loss: 1.948140]\n",
      "epoch:24 step:22868 [D loss: 0.581024, acc: 68.75%] [G loss: 1.924623]\n",
      "epoch:24 step:22869 [D loss: 0.643878, acc: 64.84%] [G loss: 1.982260]\n",
      "epoch:24 step:22870 [D loss: 0.631219, acc: 67.19%] [G loss: 1.902121]\n",
      "epoch:24 step:22871 [D loss: 0.685529, acc: 58.59%] [G loss: 1.768812]\n",
      "epoch:24 step:22872 [D loss: 0.579360, acc: 64.06%] [G loss: 2.034200]\n",
      "epoch:24 step:22873 [D loss: 0.655516, acc: 65.62%] [G loss: 2.092569]\n",
      "epoch:24 step:22874 [D loss: 0.724455, acc: 46.88%] [G loss: 1.837066]\n",
      "epoch:24 step:22875 [D loss: 0.655933, acc: 62.50%] [G loss: 1.783894]\n",
      "epoch:24 step:22876 [D loss: 0.686296, acc: 57.81%] [G loss: 1.802359]\n",
      "epoch:24 step:22877 [D loss: 0.682604, acc: 53.12%] [G loss: 1.795568]\n",
      "epoch:24 step:22878 [D loss: 0.655889, acc: 63.28%] [G loss: 1.778573]\n",
      "epoch:24 step:22879 [D loss: 0.677261, acc: 57.03%] [G loss: 1.708160]\n",
      "epoch:24 step:22880 [D loss: 0.708138, acc: 56.25%] [G loss: 1.867962]\n",
      "epoch:24 step:22881 [D loss: 0.631666, acc: 61.72%] [G loss: 1.844368]\n",
      "epoch:24 step:22882 [D loss: 0.655894, acc: 59.38%] [G loss: 1.801349]\n",
      "epoch:24 step:22883 [D loss: 0.644975, acc: 64.06%] [G loss: 1.855678]\n",
      "epoch:24 step:22884 [D loss: 0.656703, acc: 61.72%] [G loss: 1.819414]\n",
      "epoch:24 step:22885 [D loss: 0.639103, acc: 57.81%] [G loss: 1.754310]\n",
      "epoch:24 step:22886 [D loss: 0.651847, acc: 57.81%] [G loss: 2.043219]\n",
      "epoch:24 step:22887 [D loss: 0.637790, acc: 65.62%] [G loss: 1.959788]\n",
      "epoch:24 step:22888 [D loss: 0.727382, acc: 46.88%] [G loss: 1.760195]\n",
      "epoch:24 step:22889 [D loss: 0.608149, acc: 69.53%] [G loss: 1.927629]\n",
      "epoch:24 step:22890 [D loss: 0.620489, acc: 65.62%] [G loss: 2.031813]\n",
      "epoch:24 step:22891 [D loss: 0.691149, acc: 57.81%] [G loss: 1.876501]\n",
      "epoch:24 step:22892 [D loss: 0.684888, acc: 57.03%] [G loss: 2.088875]\n",
      "epoch:24 step:22893 [D loss: 0.587970, acc: 66.41%] [G loss: 1.928351]\n",
      "epoch:24 step:22894 [D loss: 0.617923, acc: 64.06%] [G loss: 2.278805]\n",
      "epoch:24 step:22895 [D loss: 0.716209, acc: 61.72%] [G loss: 1.877490]\n",
      "epoch:24 step:22896 [D loss: 0.650508, acc: 64.06%] [G loss: 1.832256]\n",
      "epoch:24 step:22897 [D loss: 0.683832, acc: 60.94%] [G loss: 1.838875]\n",
      "epoch:24 step:22898 [D loss: 0.669266, acc: 58.59%] [G loss: 1.901783]\n",
      "epoch:24 step:22899 [D loss: 0.653424, acc: 59.38%] [G loss: 1.829337]\n",
      "epoch:24 step:22900 [D loss: 0.669140, acc: 57.03%] [G loss: 1.827638]\n",
      "epoch:24 step:22901 [D loss: 0.712792, acc: 53.12%] [G loss: 1.825624]\n",
      "epoch:24 step:22902 [D loss: 0.631815, acc: 66.41%] [G loss: 1.743602]\n",
      "epoch:24 step:22903 [D loss: 0.676731, acc: 59.38%] [G loss: 2.019037]\n",
      "epoch:24 step:22904 [D loss: 0.684911, acc: 57.81%] [G loss: 2.065643]\n",
      "epoch:24 step:22905 [D loss: 0.674650, acc: 60.16%] [G loss: 1.848825]\n",
      "epoch:24 step:22906 [D loss: 0.688027, acc: 57.81%] [G loss: 1.864760]\n",
      "epoch:24 step:22907 [D loss: 0.637095, acc: 62.50%] [G loss: 1.823021]\n",
      "epoch:24 step:22908 [D loss: 0.642443, acc: 60.94%] [G loss: 1.809167]\n",
      "epoch:24 step:22909 [D loss: 0.667935, acc: 61.72%] [G loss: 1.884824]\n",
      "epoch:24 step:22910 [D loss: 0.683328, acc: 54.69%] [G loss: 1.904070]\n",
      "epoch:24 step:22911 [D loss: 0.594450, acc: 68.75%] [G loss: 1.803618]\n",
      "epoch:24 step:22912 [D loss: 0.668912, acc: 59.38%] [G loss: 1.776450]\n",
      "epoch:24 step:22913 [D loss: 0.640337, acc: 64.06%] [G loss: 1.912308]\n",
      "epoch:24 step:22914 [D loss: 0.662204, acc: 56.25%] [G loss: 1.916099]\n",
      "epoch:24 step:22915 [D loss: 0.651562, acc: 63.28%] [G loss: 1.962855]\n",
      "epoch:24 step:22916 [D loss: 0.555623, acc: 75.78%] [G loss: 2.117167]\n",
      "epoch:24 step:22917 [D loss: 0.623568, acc: 64.06%] [G loss: 1.965716]\n",
      "epoch:24 step:22918 [D loss: 0.609715, acc: 68.75%] [G loss: 1.988410]\n",
      "epoch:24 step:22919 [D loss: 0.595117, acc: 71.09%] [G loss: 1.854096]\n",
      "epoch:24 step:22920 [D loss: 0.651443, acc: 64.06%] [G loss: 1.890456]\n",
      "epoch:24 step:22921 [D loss: 0.663518, acc: 58.59%] [G loss: 1.830707]\n",
      "epoch:24 step:22922 [D loss: 0.606573, acc: 64.06%] [G loss: 2.009643]\n",
      "epoch:24 step:22923 [D loss: 0.618202, acc: 67.97%] [G loss: 2.104632]\n",
      "epoch:24 step:22924 [D loss: 0.601909, acc: 67.19%] [G loss: 1.938055]\n",
      "epoch:24 step:22925 [D loss: 0.655345, acc: 56.25%] [G loss: 1.725947]\n",
      "epoch:24 step:22926 [D loss: 0.699704, acc: 52.34%] [G loss: 1.852605]\n",
      "epoch:24 step:22927 [D loss: 0.688111, acc: 53.12%] [G loss: 1.777694]\n",
      "epoch:24 step:22928 [D loss: 0.634134, acc: 63.28%] [G loss: 1.828084]\n",
      "epoch:24 step:22929 [D loss: 0.727376, acc: 49.22%] [G loss: 1.798989]\n",
      "epoch:24 step:22930 [D loss: 0.679736, acc: 63.28%] [G loss: 1.781154]\n",
      "epoch:24 step:22931 [D loss: 0.663961, acc: 61.72%] [G loss: 1.901796]\n",
      "epoch:24 step:22932 [D loss: 0.689612, acc: 56.25%] [G loss: 1.721594]\n",
      "epoch:24 step:22933 [D loss: 0.620651, acc: 65.62%] [G loss: 1.781893]\n",
      "epoch:24 step:22934 [D loss: 0.667338, acc: 62.50%] [G loss: 1.748717]\n",
      "epoch:24 step:22935 [D loss: 0.640427, acc: 64.84%] [G loss: 1.777256]\n",
      "epoch:24 step:22936 [D loss: 0.672068, acc: 60.94%] [G loss: 1.767503]\n",
      "epoch:24 step:22937 [D loss: 0.695243, acc: 53.91%] [G loss: 1.733587]\n",
      "epoch:24 step:22938 [D loss: 0.670544, acc: 54.69%] [G loss: 1.820888]\n",
      "epoch:24 step:22939 [D loss: 0.629599, acc: 60.94%] [G loss: 1.936191]\n",
      "epoch:24 step:22940 [D loss: 0.652841, acc: 63.28%] [G loss: 1.689046]\n",
      "epoch:24 step:22941 [D loss: 0.663439, acc: 60.94%] [G loss: 1.765884]\n",
      "epoch:24 step:22942 [D loss: 0.624440, acc: 64.06%] [G loss: 1.889819]\n",
      "epoch:24 step:22943 [D loss: 0.638447, acc: 68.75%] [G loss: 1.824314]\n",
      "epoch:24 step:22944 [D loss: 0.595615, acc: 67.97%] [G loss: 1.875859]\n",
      "epoch:24 step:22945 [D loss: 0.543491, acc: 76.56%] [G loss: 2.066402]\n",
      "epoch:24 step:22946 [D loss: 0.671068, acc: 57.03%] [G loss: 1.821597]\n",
      "epoch:24 step:22947 [D loss: 0.613889, acc: 67.19%] [G loss: 1.858836]\n",
      "epoch:24 step:22948 [D loss: 0.619101, acc: 68.75%] [G loss: 1.799267]\n",
      "epoch:24 step:22949 [D loss: 0.643428, acc: 60.94%] [G loss: 1.928203]\n",
      "epoch:24 step:22950 [D loss: 0.618234, acc: 68.75%] [G loss: 1.862905]\n",
      "epoch:24 step:22951 [D loss: 0.625973, acc: 67.19%] [G loss: 1.836472]\n",
      "epoch:24 step:22952 [D loss: 0.692360, acc: 59.38%] [G loss: 1.813700]\n",
      "epoch:24 step:22953 [D loss: 0.716105, acc: 54.69%] [G loss: 1.916633]\n",
      "epoch:24 step:22954 [D loss: 0.683493, acc: 59.38%] [G loss: 1.954279]\n",
      "epoch:24 step:22955 [D loss: 0.693730, acc: 60.16%] [G loss: 1.803119]\n",
      "epoch:24 step:22956 [D loss: 0.607711, acc: 63.28%] [G loss: 2.122654]\n",
      "epoch:24 step:22957 [D loss: 0.612894, acc: 67.19%] [G loss: 2.175050]\n",
      "epoch:24 step:22958 [D loss: 0.574950, acc: 69.53%] [G loss: 2.066164]\n",
      "epoch:24 step:22959 [D loss: 0.554506, acc: 69.53%] [G loss: 2.211793]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:24 step:22960 [D loss: 0.692120, acc: 57.03%] [G loss: 1.887285]\n",
      "epoch:24 step:22961 [D loss: 0.682118, acc: 62.50%] [G loss: 1.864680]\n",
      "epoch:24 step:22962 [D loss: 0.646969, acc: 63.28%] [G loss: 1.796832]\n",
      "epoch:24 step:22963 [D loss: 0.652103, acc: 61.72%] [G loss: 1.981217]\n",
      "epoch:24 step:22964 [D loss: 0.652453, acc: 60.16%] [G loss: 1.881542]\n",
      "epoch:24 step:22965 [D loss: 0.671404, acc: 57.81%] [G loss: 1.698720]\n",
      "epoch:24 step:22966 [D loss: 0.673957, acc: 60.16%] [G loss: 1.845444]\n",
      "epoch:24 step:22967 [D loss: 0.603190, acc: 67.97%] [G loss: 2.042488]\n",
      "epoch:24 step:22968 [D loss: 0.633439, acc: 63.28%] [G loss: 2.214102]\n",
      "epoch:24 step:22969 [D loss: 0.575785, acc: 75.00%] [G loss: 2.209227]\n",
      "epoch:24 step:22970 [D loss: 0.691716, acc: 57.81%] [G loss: 1.805336]\n",
      "epoch:24 step:22971 [D loss: 0.675619, acc: 62.50%] [G loss: 1.800385]\n",
      "epoch:24 step:22972 [D loss: 0.606225, acc: 67.19%] [G loss: 1.952499]\n",
      "epoch:24 step:22973 [D loss: 0.656507, acc: 54.69%] [G loss: 1.843900]\n",
      "epoch:24 step:22974 [D loss: 0.664682, acc: 57.81%] [G loss: 1.856352]\n",
      "epoch:24 step:22975 [D loss: 0.658032, acc: 58.59%] [G loss: 1.853609]\n",
      "epoch:24 step:22976 [D loss: 0.576919, acc: 70.31%] [G loss: 2.101542]\n",
      "epoch:24 step:22977 [D loss: 0.697318, acc: 56.25%] [G loss: 1.856673]\n",
      "epoch:24 step:22978 [D loss: 0.643916, acc: 68.75%] [G loss: 1.880329]\n",
      "epoch:24 step:22979 [D loss: 0.631954, acc: 60.94%] [G loss: 1.956920]\n",
      "epoch:24 step:22980 [D loss: 0.643330, acc: 63.28%] [G loss: 1.846913]\n",
      "epoch:24 step:22981 [D loss: 0.612820, acc: 64.84%] [G loss: 2.068932]\n",
      "epoch:24 step:22982 [D loss: 0.649526, acc: 60.16%] [G loss: 2.017456]\n",
      "epoch:24 step:22983 [D loss: 0.639217, acc: 64.84%] [G loss: 1.982991]\n",
      "epoch:24 step:22984 [D loss: 0.652159, acc: 66.41%] [G loss: 1.852238]\n",
      "epoch:24 step:22985 [D loss: 0.630358, acc: 65.62%] [G loss: 1.990110]\n",
      "epoch:24 step:22986 [D loss: 0.595496, acc: 69.53%] [G loss: 2.056773]\n",
      "epoch:24 step:22987 [D loss: 0.598367, acc: 71.09%] [G loss: 2.005080]\n",
      "epoch:24 step:22988 [D loss: 0.702506, acc: 57.81%] [G loss: 1.791566]\n",
      "epoch:24 step:22989 [D loss: 0.713844, acc: 55.47%] [G loss: 1.832480]\n",
      "epoch:24 step:22990 [D loss: 0.706795, acc: 53.12%] [G loss: 1.767355]\n",
      "epoch:24 step:22991 [D loss: 0.735727, acc: 53.91%] [G loss: 1.722736]\n",
      "epoch:24 step:22992 [D loss: 0.611767, acc: 67.19%] [G loss: 1.974286]\n",
      "epoch:24 step:22993 [D loss: 0.615057, acc: 67.19%] [G loss: 1.834732]\n",
      "epoch:24 step:22994 [D loss: 0.688886, acc: 56.25%] [G loss: 1.813665]\n",
      "epoch:24 step:22995 [D loss: 0.670063, acc: 56.25%] [G loss: 1.895369]\n",
      "epoch:24 step:22996 [D loss: 0.636128, acc: 64.06%] [G loss: 2.087988]\n",
      "epoch:24 step:22997 [D loss: 0.628269, acc: 63.28%] [G loss: 1.945369]\n",
      "epoch:24 step:22998 [D loss: 0.682991, acc: 61.72%] [G loss: 1.764782]\n",
      "epoch:24 step:22999 [D loss: 0.740205, acc: 50.00%] [G loss: 1.784807]\n",
      "epoch:24 step:23000 [D loss: 0.675824, acc: 55.47%] [G loss: 1.773098]\n",
      "##############\n",
      "[2.62258933 1.53418848 6.35653089 4.91352074 3.61951576 5.71755427\n",
      " 4.60858224 4.49322732 4.63035046 3.59196596]\n",
      "##########\n",
      "epoch:24 step:23001 [D loss: 0.651776, acc: 58.59%] [G loss: 1.770110]\n",
      "epoch:24 step:23002 [D loss: 0.632927, acc: 63.28%] [G loss: 1.780432]\n",
      "epoch:24 step:23003 [D loss: 0.593865, acc: 71.09%] [G loss: 1.893216]\n",
      "epoch:24 step:23004 [D loss: 0.613936, acc: 66.41%] [G loss: 2.013444]\n",
      "epoch:24 step:23005 [D loss: 0.662406, acc: 59.38%] [G loss: 1.920660]\n",
      "epoch:24 step:23006 [D loss: 0.663675, acc: 61.72%] [G loss: 1.749197]\n",
      "epoch:24 step:23007 [D loss: 0.642706, acc: 64.06%] [G loss: 1.944496]\n",
      "epoch:24 step:23008 [D loss: 0.625656, acc: 66.41%] [G loss: 2.020450]\n",
      "epoch:24 step:23009 [D loss: 0.598035, acc: 61.72%] [G loss: 1.887663]\n",
      "epoch:24 step:23010 [D loss: 0.627730, acc: 62.50%] [G loss: 1.880644]\n",
      "epoch:24 step:23011 [D loss: 0.607653, acc: 69.53%] [G loss: 2.088193]\n",
      "epoch:24 step:23012 [D loss: 0.676605, acc: 57.81%] [G loss: 1.951057]\n",
      "epoch:24 step:23013 [D loss: 0.617311, acc: 65.62%] [G loss: 1.975113]\n",
      "epoch:24 step:23014 [D loss: 0.628903, acc: 64.84%] [G loss: 1.969339]\n",
      "epoch:24 step:23015 [D loss: 0.616449, acc: 66.41%] [G loss: 2.006775]\n",
      "epoch:24 step:23016 [D loss: 0.746044, acc: 49.22%] [G loss: 1.713047]\n",
      "epoch:24 step:23017 [D loss: 0.705338, acc: 53.12%] [G loss: 1.723731]\n",
      "epoch:24 step:23018 [D loss: 0.696718, acc: 58.59%] [G loss: 1.756144]\n",
      "epoch:24 step:23019 [D loss: 0.668598, acc: 67.19%] [G loss: 1.768876]\n",
      "epoch:24 step:23020 [D loss: 0.653635, acc: 62.50%] [G loss: 1.872939]\n",
      "epoch:24 step:23021 [D loss: 0.612282, acc: 67.97%] [G loss: 1.923843]\n",
      "epoch:24 step:23022 [D loss: 0.617768, acc: 67.97%] [G loss: 1.982605]\n",
      "epoch:24 step:23023 [D loss: 0.650393, acc: 60.94%] [G loss: 1.859306]\n",
      "epoch:24 step:23024 [D loss: 0.572956, acc: 67.97%] [G loss: 2.173077]\n",
      "epoch:24 step:23025 [D loss: 0.648817, acc: 64.06%] [G loss: 1.803708]\n",
      "epoch:24 step:23026 [D loss: 0.629472, acc: 67.97%] [G loss: 1.820262]\n",
      "epoch:24 step:23027 [D loss: 0.683720, acc: 56.25%] [G loss: 1.793350]\n",
      "epoch:24 step:23028 [D loss: 0.661907, acc: 60.94%] [G loss: 1.715206]\n",
      "epoch:24 step:23029 [D loss: 0.679618, acc: 56.25%] [G loss: 1.839236]\n",
      "epoch:24 step:23030 [D loss: 0.702683, acc: 56.25%] [G loss: 1.855474]\n",
      "epoch:24 step:23031 [D loss: 0.693093, acc: 63.28%] [G loss: 1.928326]\n",
      "epoch:24 step:23032 [D loss: 0.665944, acc: 57.81%] [G loss: 1.941381]\n",
      "epoch:24 step:23033 [D loss: 0.624573, acc: 62.50%] [G loss: 1.905023]\n",
      "epoch:24 step:23034 [D loss: 0.642957, acc: 64.06%] [G loss: 1.825686]\n",
      "epoch:24 step:23035 [D loss: 0.651078, acc: 67.19%] [G loss: 1.912505]\n",
      "epoch:24 step:23036 [D loss: 0.626584, acc: 64.84%] [G loss: 1.950148]\n",
      "epoch:24 step:23037 [D loss: 0.605002, acc: 65.62%] [G loss: 2.006742]\n",
      "epoch:24 step:23038 [D loss: 0.638049, acc: 67.97%] [G loss: 1.992540]\n",
      "epoch:24 step:23039 [D loss: 0.644241, acc: 66.41%] [G loss: 1.931731]\n",
      "epoch:24 step:23040 [D loss: 0.588320, acc: 68.75%] [G loss: 1.950452]\n",
      "epoch:24 step:23041 [D loss: 0.706237, acc: 61.72%] [G loss: 1.788020]\n",
      "epoch:24 step:23042 [D loss: 0.608326, acc: 63.28%] [G loss: 2.160460]\n",
      "epoch:24 step:23043 [D loss: 0.646696, acc: 57.03%] [G loss: 1.996489]\n",
      "epoch:24 step:23044 [D loss: 0.600545, acc: 67.97%] [G loss: 2.014880]\n",
      "epoch:24 step:23045 [D loss: 0.585950, acc: 72.66%] [G loss: 1.929323]\n",
      "epoch:24 step:23046 [D loss: 0.656481, acc: 61.72%] [G loss: 1.956442]\n",
      "epoch:24 step:23047 [D loss: 0.724804, acc: 51.56%] [G loss: 1.787184]\n",
      "epoch:24 step:23048 [D loss: 0.689469, acc: 58.59%] [G loss: 1.730914]\n",
      "epoch:24 step:23049 [D loss: 0.674273, acc: 60.94%] [G loss: 1.849257]\n",
      "epoch:24 step:23050 [D loss: 0.653126, acc: 62.50%] [G loss: 1.809706]\n",
      "epoch:24 step:23051 [D loss: 0.655692, acc: 64.84%] [G loss: 1.885414]\n",
      "epoch:24 step:23052 [D loss: 0.626040, acc: 63.28%] [G loss: 1.948403]\n",
      "epoch:24 step:23053 [D loss: 0.692861, acc: 53.12%] [G loss: 1.780988]\n",
      "epoch:24 step:23054 [D loss: 0.672782, acc: 60.94%] [G loss: 1.694492]\n",
      "epoch:24 step:23055 [D loss: 0.687639, acc: 57.03%] [G loss: 1.695919]\n",
      "epoch:24 step:23056 [D loss: 0.636365, acc: 65.62%] [G loss: 1.778586]\n",
      "epoch:24 step:23057 [D loss: 0.735369, acc: 52.34%] [G loss: 1.755436]\n",
      "epoch:24 step:23058 [D loss: 0.653460, acc: 64.84%] [G loss: 1.716378]\n",
      "epoch:24 step:23059 [D loss: 0.632669, acc: 63.28%] [G loss: 1.831234]\n",
      "epoch:24 step:23060 [D loss: 0.690198, acc: 57.03%] [G loss: 1.706582]\n",
      "epoch:24 step:23061 [D loss: 0.652804, acc: 63.28%] [G loss: 1.691327]\n",
      "epoch:24 step:23062 [D loss: 0.704241, acc: 50.78%] [G loss: 1.786923]\n",
      "epoch:24 step:23063 [D loss: 0.637525, acc: 63.28%] [G loss: 1.752970]\n",
      "epoch:24 step:23064 [D loss: 0.655730, acc: 63.28%] [G loss: 1.715166]\n",
      "epoch:24 step:23065 [D loss: 0.635377, acc: 66.41%] [G loss: 1.717182]\n",
      "epoch:24 step:23066 [D loss: 0.661219, acc: 59.38%] [G loss: 1.741040]\n",
      "epoch:24 step:23067 [D loss: 0.654326, acc: 58.59%] [G loss: 1.666001]\n",
      "epoch:24 step:23068 [D loss: 0.631872, acc: 66.41%] [G loss: 1.718075]\n",
      "epoch:24 step:23069 [D loss: 0.633693, acc: 65.62%] [G loss: 1.731939]\n",
      "epoch:24 step:23070 [D loss: 0.624263, acc: 65.62%] [G loss: 1.874212]\n",
      "epoch:24 step:23071 [D loss: 0.684831, acc: 57.81%] [G loss: 1.728877]\n",
      "epoch:24 step:23072 [D loss: 0.670281, acc: 57.81%] [G loss: 1.749271]\n",
      "epoch:24 step:23073 [D loss: 0.613856, acc: 64.06%] [G loss: 1.777352]\n",
      "epoch:24 step:23074 [D loss: 0.670468, acc: 54.69%] [G loss: 1.789492]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:24 step:23075 [D loss: 0.684534, acc: 58.59%] [G loss: 1.881518]\n",
      "epoch:24 step:23076 [D loss: 0.628661, acc: 64.06%] [G loss: 1.896157]\n",
      "epoch:24 step:23077 [D loss: 0.579466, acc: 76.56%] [G loss: 1.790886]\n",
      "epoch:24 step:23078 [D loss: 0.670225, acc: 57.81%] [G loss: 1.800121]\n",
      "epoch:24 step:23079 [D loss: 0.693940, acc: 61.72%] [G loss: 1.973029]\n",
      "epoch:24 step:23080 [D loss: 0.644804, acc: 65.62%] [G loss: 1.818227]\n",
      "epoch:24 step:23081 [D loss: 0.650628, acc: 64.84%] [G loss: 1.831086]\n",
      "epoch:24 step:23082 [D loss: 0.683166, acc: 63.28%] [G loss: 1.781609]\n",
      "epoch:24 step:23083 [D loss: 0.583716, acc: 70.31%] [G loss: 1.995880]\n",
      "epoch:24 step:23084 [D loss: 0.664589, acc: 57.81%] [G loss: 1.781276]\n",
      "epoch:24 step:23085 [D loss: 0.699670, acc: 56.25%] [G loss: 1.795179]\n",
      "epoch:24 step:23086 [D loss: 0.635927, acc: 63.28%] [G loss: 1.866800]\n",
      "epoch:24 step:23087 [D loss: 0.703627, acc: 59.38%] [G loss: 1.905186]\n",
      "epoch:24 step:23088 [D loss: 0.705941, acc: 53.12%] [G loss: 1.771066]\n",
      "epoch:24 step:23089 [D loss: 0.637445, acc: 64.84%] [G loss: 1.903943]\n",
      "epoch:24 step:23090 [D loss: 0.615040, acc: 63.28%] [G loss: 1.786952]\n",
      "epoch:24 step:23091 [D loss: 0.670633, acc: 57.81%] [G loss: 1.784992]\n",
      "epoch:24 step:23092 [D loss: 0.694663, acc: 56.25%] [G loss: 1.827415]\n",
      "epoch:24 step:23093 [D loss: 0.619045, acc: 62.50%] [G loss: 1.958638]\n",
      "epoch:24 step:23094 [D loss: 0.660989, acc: 61.72%] [G loss: 1.785758]\n",
      "epoch:24 step:23095 [D loss: 0.679805, acc: 60.16%] [G loss: 1.907050]\n",
      "epoch:24 step:23096 [D loss: 0.629633, acc: 64.84%] [G loss: 1.828317]\n",
      "epoch:24 step:23097 [D loss: 0.615959, acc: 64.84%] [G loss: 1.987617]\n",
      "epoch:24 step:23098 [D loss: 0.655822, acc: 62.50%] [G loss: 1.784057]\n",
      "epoch:24 step:23099 [D loss: 0.647236, acc: 64.84%] [G loss: 1.838534]\n",
      "epoch:24 step:23100 [D loss: 0.707600, acc: 53.91%] [G loss: 1.820088]\n",
      "epoch:24 step:23101 [D loss: 0.665364, acc: 62.50%] [G loss: 1.730953]\n",
      "epoch:24 step:23102 [D loss: 0.712477, acc: 53.12%] [G loss: 1.741835]\n",
      "epoch:24 step:23103 [D loss: 0.655347, acc: 64.06%] [G loss: 1.747988]\n",
      "epoch:24 step:23104 [D loss: 0.683466, acc: 56.25%] [G loss: 1.824292]\n",
      "epoch:24 step:23105 [D loss: 0.651235, acc: 63.28%] [G loss: 1.707596]\n",
      "epoch:24 step:23106 [D loss: 0.643503, acc: 66.41%] [G loss: 1.774612]\n",
      "epoch:24 step:23107 [D loss: 0.682445, acc: 57.81%] [G loss: 1.754803]\n",
      "epoch:24 step:23108 [D loss: 0.644231, acc: 59.38%] [G loss: 1.825090]\n",
      "epoch:24 step:23109 [D loss: 0.662718, acc: 58.59%] [G loss: 1.759363]\n",
      "epoch:24 step:23110 [D loss: 0.661758, acc: 55.47%] [G loss: 1.961246]\n",
      "epoch:24 step:23111 [D loss: 0.634473, acc: 67.19%] [G loss: 1.871616]\n",
      "epoch:24 step:23112 [D loss: 0.657476, acc: 60.16%] [G loss: 1.948902]\n",
      "epoch:24 step:23113 [D loss: 0.653260, acc: 58.59%] [G loss: 1.728766]\n",
      "epoch:24 step:23114 [D loss: 0.648874, acc: 63.28%] [G loss: 1.864614]\n",
      "epoch:24 step:23115 [D loss: 0.688220, acc: 59.38%] [G loss: 1.837436]\n",
      "epoch:24 step:23116 [D loss: 0.668926, acc: 59.38%] [G loss: 1.672605]\n",
      "epoch:24 step:23117 [D loss: 0.611725, acc: 64.84%] [G loss: 1.796947]\n",
      "epoch:24 step:23118 [D loss: 0.645291, acc: 59.38%] [G loss: 1.935709]\n",
      "epoch:24 step:23119 [D loss: 0.633346, acc: 66.41%] [G loss: 1.872423]\n",
      "epoch:24 step:23120 [D loss: 0.647618, acc: 62.50%] [G loss: 1.950347]\n",
      "epoch:24 step:23121 [D loss: 0.617425, acc: 64.84%] [G loss: 1.966629]\n",
      "epoch:24 step:23122 [D loss: 0.599132, acc: 66.41%] [G loss: 1.868674]\n",
      "epoch:24 step:23123 [D loss: 0.606122, acc: 62.50%] [G loss: 1.902127]\n",
      "epoch:24 step:23124 [D loss: 0.581711, acc: 71.88%] [G loss: 1.782019]\n",
      "epoch:24 step:23125 [D loss: 0.681752, acc: 64.06%] [G loss: 1.930270]\n",
      "epoch:24 step:23126 [D loss: 0.620050, acc: 67.97%] [G loss: 1.976602]\n",
      "epoch:24 step:23127 [D loss: 0.662918, acc: 60.94%] [G loss: 1.927459]\n",
      "epoch:24 step:23128 [D loss: 0.693166, acc: 57.81%] [G loss: 1.857115]\n",
      "epoch:24 step:23129 [D loss: 0.645227, acc: 63.28%] [G loss: 1.827451]\n",
      "epoch:24 step:23130 [D loss: 0.560002, acc: 75.00%] [G loss: 1.894611]\n",
      "epoch:24 step:23131 [D loss: 0.586537, acc: 68.75%] [G loss: 1.978098]\n",
      "epoch:24 step:23132 [D loss: 0.650393, acc: 61.72%] [G loss: 1.969014]\n",
      "epoch:24 step:23133 [D loss: 0.672010, acc: 62.50%] [G loss: 1.973176]\n",
      "epoch:24 step:23134 [D loss: 0.616742, acc: 64.84%] [G loss: 1.944730]\n",
      "epoch:24 step:23135 [D loss: 0.603489, acc: 66.41%] [G loss: 2.154178]\n",
      "epoch:24 step:23136 [D loss: 0.562090, acc: 74.22%] [G loss: 2.231676]\n",
      "epoch:24 step:23137 [D loss: 0.554780, acc: 71.09%] [G loss: 2.134131]\n",
      "epoch:24 step:23138 [D loss: 0.612207, acc: 67.97%] [G loss: 2.054227]\n",
      "epoch:24 step:23139 [D loss: 0.616983, acc: 59.38%] [G loss: 1.917743]\n",
      "epoch:24 step:23140 [D loss: 0.672957, acc: 56.25%] [G loss: 2.074853]\n",
      "epoch:24 step:23141 [D loss: 0.672120, acc: 55.47%] [G loss: 1.988777]\n",
      "epoch:24 step:23142 [D loss: 0.605992, acc: 67.19%] [G loss: 1.956205]\n",
      "epoch:24 step:23143 [D loss: 0.697245, acc: 56.25%] [G loss: 1.986795]\n",
      "epoch:24 step:23144 [D loss: 0.727809, acc: 57.03%] [G loss: 1.832965]\n",
      "epoch:24 step:23145 [D loss: 0.690393, acc: 60.16%] [G loss: 1.713307]\n",
      "epoch:24 step:23146 [D loss: 0.690626, acc: 55.47%] [G loss: 1.732811]\n",
      "epoch:24 step:23147 [D loss: 0.659063, acc: 63.28%] [G loss: 1.743625]\n",
      "epoch:24 step:23148 [D loss: 0.668082, acc: 60.16%] [G loss: 1.841194]\n",
      "epoch:24 step:23149 [D loss: 0.684317, acc: 57.81%] [G loss: 1.797749]\n",
      "epoch:24 step:23150 [D loss: 0.656469, acc: 59.38%] [G loss: 1.784894]\n",
      "epoch:24 step:23151 [D loss: 0.653950, acc: 60.94%] [G loss: 1.834794]\n",
      "epoch:24 step:23152 [D loss: 0.653947, acc: 62.50%] [G loss: 1.831841]\n",
      "epoch:24 step:23153 [D loss: 0.681094, acc: 61.72%] [G loss: 1.865618]\n",
      "epoch:24 step:23154 [D loss: 0.656223, acc: 57.03%] [G loss: 1.747489]\n",
      "epoch:24 step:23155 [D loss: 0.659923, acc: 60.94%] [G loss: 1.641151]\n",
      "epoch:24 step:23156 [D loss: 0.648674, acc: 65.62%] [G loss: 1.700897]\n",
      "epoch:24 step:23157 [D loss: 0.646312, acc: 62.50%] [G loss: 1.670380]\n",
      "epoch:24 step:23158 [D loss: 0.658009, acc: 64.06%] [G loss: 1.845144]\n",
      "epoch:24 step:23159 [D loss: 0.621197, acc: 67.97%] [G loss: 1.775817]\n",
      "epoch:24 step:23160 [D loss: 0.592236, acc: 66.41%] [G loss: 1.869284]\n",
      "epoch:24 step:23161 [D loss: 0.685567, acc: 60.94%] [G loss: 1.842184]\n",
      "epoch:24 step:23162 [D loss: 0.619703, acc: 63.28%] [G loss: 1.867077]\n",
      "epoch:24 step:23163 [D loss: 0.680946, acc: 55.47%] [G loss: 1.711519]\n",
      "epoch:24 step:23164 [D loss: 0.703954, acc: 53.91%] [G loss: 1.739679]\n",
      "epoch:24 step:23165 [D loss: 0.607339, acc: 70.31%] [G loss: 1.844034]\n",
      "epoch:24 step:23166 [D loss: 0.667102, acc: 60.16%] [G loss: 1.837937]\n",
      "epoch:24 step:23167 [D loss: 0.663561, acc: 57.03%] [G loss: 1.825761]\n",
      "epoch:24 step:23168 [D loss: 0.657571, acc: 64.06%] [G loss: 1.886181]\n",
      "epoch:24 step:23169 [D loss: 0.644796, acc: 66.41%] [G loss: 2.027103]\n",
      "epoch:24 step:23170 [D loss: 0.676634, acc: 60.16%] [G loss: 1.840739]\n",
      "epoch:24 step:23171 [D loss: 0.650661, acc: 61.72%] [G loss: 1.644333]\n",
      "epoch:24 step:23172 [D loss: 0.651225, acc: 60.16%] [G loss: 1.728113]\n",
      "epoch:24 step:23173 [D loss: 0.689882, acc: 62.50%] [G loss: 1.811431]\n",
      "epoch:24 step:23174 [D loss: 0.626647, acc: 67.97%] [G loss: 1.872611]\n",
      "epoch:24 step:23175 [D loss: 0.670010, acc: 59.38%] [G loss: 1.792464]\n",
      "epoch:24 step:23176 [D loss: 0.622996, acc: 64.06%] [G loss: 1.872595]\n",
      "epoch:24 step:23177 [D loss: 0.683679, acc: 58.59%] [G loss: 1.781652]\n",
      "epoch:24 step:23178 [D loss: 0.636746, acc: 67.19%] [G loss: 2.025115]\n",
      "epoch:24 step:23179 [D loss: 0.631035, acc: 64.84%] [G loss: 1.887071]\n",
      "epoch:24 step:23180 [D loss: 0.632612, acc: 62.50%] [G loss: 1.881153]\n",
      "epoch:24 step:23181 [D loss: 0.648619, acc: 65.62%] [G loss: 2.008240]\n",
      "epoch:24 step:23182 [D loss: 0.638391, acc: 68.75%] [G loss: 2.064280]\n",
      "epoch:24 step:23183 [D loss: 0.646217, acc: 63.28%] [G loss: 2.080394]\n",
      "epoch:24 step:23184 [D loss: 0.676710, acc: 62.50%] [G loss: 1.718303]\n",
      "epoch:24 step:23185 [D loss: 0.652370, acc: 62.50%] [G loss: 1.877095]\n",
      "epoch:24 step:23186 [D loss: 0.683240, acc: 53.91%] [G loss: 1.900547]\n",
      "epoch:24 step:23187 [D loss: 0.597372, acc: 70.31%] [G loss: 1.899358]\n",
      "epoch:24 step:23188 [D loss: 0.653363, acc: 61.72%] [G loss: 1.853288]\n",
      "epoch:24 step:23189 [D loss: 0.594330, acc: 71.09%] [G loss: 1.957312]\n",
      "epoch:24 step:23190 [D loss: 0.690352, acc: 60.16%] [G loss: 1.784749]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:24 step:23191 [D loss: 0.623060, acc: 62.50%] [G loss: 1.813104]\n",
      "epoch:24 step:23192 [D loss: 0.659556, acc: 58.59%] [G loss: 1.644017]\n",
      "epoch:24 step:23193 [D loss: 0.647561, acc: 61.72%] [G loss: 1.839726]\n",
      "epoch:24 step:23194 [D loss: 0.609087, acc: 71.09%] [G loss: 1.920394]\n",
      "epoch:24 step:23195 [D loss: 0.639006, acc: 63.28%] [G loss: 1.947057]\n",
      "epoch:24 step:23196 [D loss: 0.633376, acc: 64.06%] [G loss: 2.157079]\n",
      "epoch:24 step:23197 [D loss: 0.640429, acc: 65.62%] [G loss: 1.987165]\n",
      "epoch:24 step:23198 [D loss: 0.665371, acc: 60.94%] [G loss: 1.908916]\n",
      "epoch:24 step:23199 [D loss: 0.678668, acc: 60.94%] [G loss: 1.994097]\n",
      "epoch:24 step:23200 [D loss: 0.651013, acc: 60.94%] [G loss: 1.908700]\n",
      "##############\n",
      "[2.30216756 1.36312736 6.36674422 4.75051357 3.58988329 5.62790106\n",
      " 4.42164671 4.84896171 4.3776853  3.62672421]\n",
      "##########\n",
      "epoch:24 step:23201 [D loss: 0.642862, acc: 64.84%] [G loss: 1.881651]\n",
      "epoch:24 step:23202 [D loss: 0.622161, acc: 64.84%] [G loss: 1.817486]\n",
      "epoch:24 step:23203 [D loss: 0.652208, acc: 62.50%] [G loss: 1.888972]\n",
      "epoch:24 step:23204 [D loss: 0.704513, acc: 53.12%] [G loss: 1.799907]\n",
      "epoch:24 step:23205 [D loss: 0.619031, acc: 60.94%] [G loss: 1.885002]\n",
      "epoch:24 step:23206 [D loss: 0.670334, acc: 62.50%] [G loss: 1.866890]\n",
      "epoch:24 step:23207 [D loss: 0.592778, acc: 71.88%] [G loss: 2.107785]\n",
      "epoch:24 step:23208 [D loss: 0.676090, acc: 57.03%] [G loss: 2.066138]\n",
      "epoch:24 step:23209 [D loss: 0.632066, acc: 64.06%] [G loss: 1.914311]\n",
      "epoch:24 step:23210 [D loss: 0.693851, acc: 54.69%] [G loss: 1.784340]\n",
      "epoch:24 step:23211 [D loss: 0.648096, acc: 62.50%] [G loss: 1.826334]\n",
      "epoch:24 step:23212 [D loss: 0.658462, acc: 63.28%] [G loss: 1.829283]\n",
      "epoch:24 step:23213 [D loss: 0.635953, acc: 61.72%] [G loss: 1.882451]\n",
      "epoch:24 step:23214 [D loss: 0.617050, acc: 71.88%] [G loss: 1.940539]\n",
      "epoch:24 step:23215 [D loss: 0.616106, acc: 68.75%] [G loss: 1.882300]\n",
      "epoch:24 step:23216 [D loss: 0.635893, acc: 66.41%] [G loss: 1.854043]\n",
      "epoch:24 step:23217 [D loss: 0.694050, acc: 57.03%] [G loss: 1.821418]\n",
      "epoch:24 step:23218 [D loss: 0.678122, acc: 56.25%] [G loss: 1.873923]\n",
      "epoch:24 step:23219 [D loss: 0.670028, acc: 55.47%] [G loss: 1.704108]\n",
      "epoch:24 step:23220 [D loss: 0.655101, acc: 57.03%] [G loss: 1.747095]\n",
      "epoch:24 step:23221 [D loss: 0.655615, acc: 65.62%] [G loss: 1.803939]\n",
      "epoch:24 step:23222 [D loss: 0.684062, acc: 53.91%] [G loss: 1.841541]\n",
      "epoch:24 step:23223 [D loss: 0.666840, acc: 61.72%] [G loss: 1.769152]\n",
      "epoch:24 step:23224 [D loss: 0.676495, acc: 57.81%] [G loss: 1.842792]\n",
      "epoch:24 step:23225 [D loss: 0.718763, acc: 50.00%] [G loss: 1.790582]\n",
      "epoch:24 step:23226 [D loss: 0.622712, acc: 62.50%] [G loss: 1.744377]\n",
      "epoch:24 step:23227 [D loss: 0.653115, acc: 60.16%] [G loss: 1.786510]\n",
      "epoch:24 step:23228 [D loss: 0.641384, acc: 65.62%] [G loss: 1.715851]\n",
      "epoch:24 step:23229 [D loss: 0.657564, acc: 55.47%] [G loss: 1.735153]\n",
      "epoch:24 step:23230 [D loss: 0.659696, acc: 63.28%] [G loss: 1.844261]\n",
      "epoch:24 step:23231 [D loss: 0.646852, acc: 64.84%] [G loss: 1.801277]\n",
      "epoch:24 step:23232 [D loss: 0.686218, acc: 56.25%] [G loss: 1.806302]\n",
      "epoch:24 step:23233 [D loss: 0.658392, acc: 61.72%] [G loss: 1.828790]\n",
      "epoch:24 step:23234 [D loss: 0.662643, acc: 64.06%] [G loss: 1.940803]\n",
      "epoch:24 step:23235 [D loss: 0.633723, acc: 65.62%] [G loss: 1.765058]\n",
      "epoch:24 step:23236 [D loss: 0.639137, acc: 60.16%] [G loss: 1.637792]\n",
      "epoch:24 step:23237 [D loss: 0.626689, acc: 65.62%] [G loss: 1.744838]\n",
      "epoch:24 step:23238 [D loss: 0.659743, acc: 63.28%] [G loss: 1.794208]\n",
      "epoch:24 step:23239 [D loss: 0.690765, acc: 55.47%] [G loss: 1.830772]\n",
      "epoch:24 step:23240 [D loss: 0.647654, acc: 67.19%] [G loss: 1.746886]\n",
      "epoch:24 step:23241 [D loss: 0.586744, acc: 71.88%] [G loss: 1.709710]\n",
      "epoch:24 step:23242 [D loss: 0.661762, acc: 62.50%] [G loss: 1.922123]\n",
      "epoch:24 step:23243 [D loss: 0.651549, acc: 63.28%] [G loss: 1.912908]\n",
      "epoch:24 step:23244 [D loss: 0.661354, acc: 57.03%] [G loss: 1.905572]\n",
      "epoch:24 step:23245 [D loss: 0.631254, acc: 64.84%] [G loss: 1.834675]\n",
      "epoch:24 step:23246 [D loss: 0.695295, acc: 54.69%] [G loss: 1.781524]\n",
      "epoch:24 step:23247 [D loss: 0.701736, acc: 56.25%] [G loss: 1.668717]\n",
      "epoch:24 step:23248 [D loss: 0.663667, acc: 61.72%] [G loss: 1.655323]\n",
      "epoch:24 step:23249 [D loss: 0.637730, acc: 59.38%] [G loss: 1.713659]\n",
      "epoch:24 step:23250 [D loss: 0.640571, acc: 62.50%] [G loss: 1.795413]\n",
      "epoch:24 step:23251 [D loss: 0.658631, acc: 62.50%] [G loss: 1.754873]\n",
      "epoch:24 step:23252 [D loss: 0.705323, acc: 53.12%] [G loss: 1.784890]\n",
      "epoch:24 step:23253 [D loss: 0.677619, acc: 60.94%] [G loss: 1.831359]\n",
      "epoch:24 step:23254 [D loss: 0.654969, acc: 61.72%] [G loss: 1.798159]\n",
      "epoch:24 step:23255 [D loss: 0.636301, acc: 65.62%] [G loss: 1.882210]\n",
      "epoch:24 step:23256 [D loss: 0.666984, acc: 61.72%] [G loss: 1.819980]\n",
      "epoch:24 step:23257 [D loss: 0.663406, acc: 58.59%] [G loss: 1.965724]\n",
      "epoch:24 step:23258 [D loss: 0.626994, acc: 63.28%] [G loss: 1.807962]\n",
      "epoch:24 step:23259 [D loss: 0.638719, acc: 61.72%] [G loss: 1.838462]\n",
      "epoch:24 step:23260 [D loss: 0.661309, acc: 62.50%] [G loss: 1.846350]\n",
      "epoch:24 step:23261 [D loss: 0.686823, acc: 55.47%] [G loss: 1.805301]\n",
      "epoch:24 step:23262 [D loss: 0.611303, acc: 67.19%] [G loss: 1.871642]\n",
      "epoch:24 step:23263 [D loss: 0.597067, acc: 70.31%] [G loss: 2.071335]\n",
      "epoch:24 step:23264 [D loss: 0.654380, acc: 58.59%] [G loss: 1.882398]\n",
      "epoch:24 step:23265 [D loss: 0.689252, acc: 55.47%] [G loss: 1.798349]\n",
      "epoch:24 step:23266 [D loss: 0.635154, acc: 60.94%] [G loss: 1.938135]\n",
      "epoch:24 step:23267 [D loss: 0.714586, acc: 50.78%] [G loss: 1.978806]\n",
      "epoch:24 step:23268 [D loss: 0.640957, acc: 65.62%] [G loss: 1.893661]\n",
      "epoch:24 step:23269 [D loss: 0.598055, acc: 65.62%] [G loss: 2.038258]\n",
      "epoch:24 step:23270 [D loss: 0.579739, acc: 70.31%] [G loss: 2.165111]\n",
      "epoch:24 step:23271 [D loss: 0.614146, acc: 67.97%] [G loss: 1.969220]\n",
      "epoch:24 step:23272 [D loss: 0.682161, acc: 60.16%] [G loss: 1.898107]\n",
      "epoch:24 step:23273 [D loss: 0.623082, acc: 66.41%] [G loss: 1.962608]\n",
      "epoch:24 step:23274 [D loss: 0.643660, acc: 57.81%] [G loss: 2.095444]\n",
      "epoch:24 step:23275 [D loss: 0.661760, acc: 57.03%] [G loss: 1.892839]\n",
      "epoch:24 step:23276 [D loss: 0.699693, acc: 57.81%] [G loss: 1.771248]\n",
      "epoch:24 step:23277 [D loss: 0.642303, acc: 66.41%] [G loss: 1.990075]\n",
      "epoch:24 step:23278 [D loss: 0.630100, acc: 66.41%] [G loss: 1.960180]\n",
      "epoch:24 step:23279 [D loss: 0.609716, acc: 69.53%] [G loss: 1.942407]\n",
      "epoch:24 step:23280 [D loss: 0.585665, acc: 71.09%] [G loss: 2.088912]\n",
      "epoch:24 step:23281 [D loss: 0.661954, acc: 63.28%] [G loss: 1.905476]\n",
      "epoch:24 step:23282 [D loss: 0.715911, acc: 51.56%] [G loss: 1.754212]\n",
      "epoch:24 step:23283 [D loss: 0.726514, acc: 50.78%] [G loss: 1.836170]\n",
      "epoch:24 step:23284 [D loss: 0.675526, acc: 58.59%] [G loss: 1.759272]\n",
      "epoch:24 step:23285 [D loss: 0.628806, acc: 67.97%] [G loss: 1.746199]\n",
      "epoch:24 step:23286 [D loss: 0.683322, acc: 55.47%] [G loss: 1.895006]\n",
      "epoch:24 step:23287 [D loss: 0.663223, acc: 59.38%] [G loss: 1.801865]\n",
      "epoch:24 step:23288 [D loss: 0.764066, acc: 50.78%] [G loss: 1.837893]\n",
      "epoch:24 step:23289 [D loss: 0.693897, acc: 50.78%] [G loss: 1.847893]\n",
      "epoch:24 step:23290 [D loss: 0.661780, acc: 62.50%] [G loss: 1.639923]\n",
      "epoch:24 step:23291 [D loss: 0.602998, acc: 64.84%] [G loss: 1.715299]\n",
      "epoch:24 step:23292 [D loss: 0.642269, acc: 63.28%] [G loss: 1.953496]\n",
      "epoch:24 step:23293 [D loss: 0.672351, acc: 61.72%] [G loss: 1.906406]\n",
      "epoch:24 step:23294 [D loss: 0.686418, acc: 61.72%] [G loss: 1.828725]\n",
      "epoch:24 step:23295 [D loss: 0.618229, acc: 66.41%] [G loss: 1.759356]\n",
      "epoch:24 step:23296 [D loss: 0.604634, acc: 67.97%] [G loss: 1.764584]\n",
      "epoch:24 step:23297 [D loss: 0.633543, acc: 63.28%] [G loss: 1.881869]\n",
      "epoch:24 step:23298 [D loss: 0.644907, acc: 55.47%] [G loss: 1.818639]\n",
      "epoch:24 step:23299 [D loss: 0.702213, acc: 60.16%] [G loss: 1.845546]\n",
      "epoch:24 step:23300 [D loss: 0.693520, acc: 57.81%] [G loss: 1.719484]\n",
      "epoch:24 step:23301 [D loss: 0.695789, acc: 53.12%] [G loss: 1.873992]\n",
      "epoch:24 step:23302 [D loss: 0.664356, acc: 57.81%] [G loss: 1.747658]\n",
      "epoch:24 step:23303 [D loss: 0.626578, acc: 60.16%] [G loss: 1.930859]\n",
      "epoch:24 step:23304 [D loss: 0.600791, acc: 73.44%] [G loss: 1.880657]\n",
      "epoch:24 step:23305 [D loss: 0.623176, acc: 63.28%] [G loss: 1.802905]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:24 step:23306 [D loss: 0.710483, acc: 59.38%] [G loss: 1.879824]\n",
      "epoch:24 step:23307 [D loss: 0.661238, acc: 60.94%] [G loss: 1.816195]\n",
      "epoch:24 step:23308 [D loss: 0.683283, acc: 53.12%] [G loss: 1.724359]\n",
      "epoch:24 step:23309 [D loss: 0.674546, acc: 59.38%] [G loss: 1.796023]\n",
      "epoch:24 step:23310 [D loss: 0.624053, acc: 57.81%] [G loss: 1.807174]\n",
      "epoch:24 step:23311 [D loss: 0.582372, acc: 71.88%] [G loss: 2.045162]\n",
      "epoch:24 step:23312 [D loss: 0.649027, acc: 63.28%] [G loss: 1.770869]\n",
      "epoch:24 step:23313 [D loss: 0.580945, acc: 71.09%] [G loss: 1.967092]\n",
      "epoch:24 step:23314 [D loss: 0.696214, acc: 56.25%] [G loss: 1.806665]\n",
      "epoch:24 step:23315 [D loss: 0.686814, acc: 52.34%] [G loss: 1.803492]\n",
      "epoch:24 step:23316 [D loss: 0.731908, acc: 51.56%] [G loss: 1.731215]\n",
      "epoch:24 step:23317 [D loss: 0.675628, acc: 54.69%] [G loss: 1.801128]\n",
      "epoch:24 step:23318 [D loss: 0.643136, acc: 63.28%] [G loss: 1.707245]\n",
      "epoch:24 step:23319 [D loss: 0.641423, acc: 64.06%] [G loss: 1.912384]\n",
      "epoch:24 step:23320 [D loss: 0.665688, acc: 61.72%] [G loss: 1.870597]\n",
      "epoch:24 step:23321 [D loss: 0.576278, acc: 70.31%] [G loss: 2.024453]\n",
      "epoch:24 step:23322 [D loss: 0.705081, acc: 54.69%] [G loss: 1.831201]\n",
      "epoch:24 step:23323 [D loss: 0.611234, acc: 71.09%] [G loss: 1.882097]\n",
      "epoch:24 step:23324 [D loss: 0.595657, acc: 66.41%] [G loss: 1.947745]\n",
      "epoch:24 step:23325 [D loss: 0.613927, acc: 70.31%] [G loss: 1.974882]\n",
      "epoch:24 step:23326 [D loss: 0.595419, acc: 69.53%] [G loss: 1.956995]\n",
      "epoch:24 step:23327 [D loss: 0.669196, acc: 62.50%] [G loss: 1.866921]\n",
      "epoch:24 step:23328 [D loss: 0.620722, acc: 67.97%] [G loss: 1.841244]\n",
      "epoch:24 step:23329 [D loss: 0.612416, acc: 71.88%] [G loss: 1.955329]\n",
      "epoch:24 step:23330 [D loss: 0.644331, acc: 63.28%] [G loss: 1.877229]\n",
      "epoch:24 step:23331 [D loss: 0.684713, acc: 59.38%] [G loss: 1.903045]\n",
      "epoch:24 step:23332 [D loss: 0.660766, acc: 57.81%] [G loss: 1.896075]\n",
      "epoch:24 step:23333 [D loss: 0.591870, acc: 70.31%] [G loss: 2.007666]\n",
      "epoch:24 step:23334 [D loss: 0.644060, acc: 61.72%] [G loss: 1.943122]\n",
      "epoch:24 step:23335 [D loss: 0.613240, acc: 65.62%] [G loss: 1.831690]\n",
      "epoch:24 step:23336 [D loss: 0.655709, acc: 64.06%] [G loss: 1.927202]\n",
      "epoch:24 step:23337 [D loss: 0.641925, acc: 60.16%] [G loss: 2.171575]\n",
      "epoch:24 step:23338 [D loss: 0.637935, acc: 59.38%] [G loss: 1.759840]\n",
      "epoch:24 step:23339 [D loss: 0.645118, acc: 63.28%] [G loss: 1.931722]\n",
      "epoch:24 step:23340 [D loss: 0.599697, acc: 69.53%] [G loss: 1.968720]\n",
      "epoch:24 step:23341 [D loss: 0.667261, acc: 60.94%] [G loss: 1.817418]\n",
      "epoch:24 step:23342 [D loss: 0.617836, acc: 67.97%] [G loss: 1.895184]\n",
      "epoch:24 step:23343 [D loss: 0.647340, acc: 64.06%] [G loss: 1.700986]\n",
      "epoch:24 step:23344 [D loss: 0.696296, acc: 56.25%] [G loss: 1.758358]\n",
      "epoch:24 step:23345 [D loss: 0.631692, acc: 64.84%] [G loss: 1.770636]\n",
      "epoch:24 step:23346 [D loss: 0.689911, acc: 56.25%] [G loss: 1.981830]\n",
      "epoch:24 step:23347 [D loss: 0.705518, acc: 53.91%] [G loss: 1.790486]\n",
      "epoch:24 step:23348 [D loss: 0.631387, acc: 61.72%] [G loss: 1.727169]\n",
      "epoch:24 step:23349 [D loss: 0.653381, acc: 64.06%] [G loss: 1.945657]\n",
      "epoch:24 step:23350 [D loss: 0.683553, acc: 58.59%] [G loss: 1.834060]\n",
      "epoch:24 step:23351 [D loss: 0.620342, acc: 60.16%] [G loss: 1.766687]\n",
      "epoch:24 step:23352 [D loss: 0.624695, acc: 64.06%] [G loss: 1.813138]\n",
      "epoch:24 step:23353 [D loss: 0.705310, acc: 47.66%] [G loss: 1.757406]\n",
      "epoch:24 step:23354 [D loss: 0.631831, acc: 62.50%] [G loss: 1.868147]\n",
      "epoch:24 step:23355 [D loss: 0.590283, acc: 68.75%] [G loss: 1.883821]\n",
      "epoch:24 step:23356 [D loss: 0.611959, acc: 67.97%] [G loss: 1.727293]\n",
      "epoch:24 step:23357 [D loss: 0.658169, acc: 60.94%] [G loss: 1.779825]\n",
      "epoch:24 step:23358 [D loss: 0.679772, acc: 56.25%] [G loss: 1.857962]\n",
      "epoch:24 step:23359 [D loss: 0.645151, acc: 60.94%] [G loss: 1.831467]\n",
      "epoch:24 step:23360 [D loss: 0.663708, acc: 58.59%] [G loss: 1.779736]\n",
      "epoch:24 step:23361 [D loss: 0.624045, acc: 68.75%] [G loss: 1.681796]\n",
      "epoch:24 step:23362 [D loss: 0.639321, acc: 66.41%] [G loss: 1.873565]\n",
      "epoch:24 step:23363 [D loss: 0.633877, acc: 64.84%] [G loss: 2.025257]\n",
      "epoch:24 step:23364 [D loss: 0.642966, acc: 61.72%] [G loss: 1.921095]\n",
      "epoch:24 step:23365 [D loss: 0.665628, acc: 58.59%] [G loss: 1.840298]\n",
      "epoch:24 step:23366 [D loss: 0.685228, acc: 57.03%] [G loss: 1.841810]\n",
      "epoch:24 step:23367 [D loss: 0.647974, acc: 63.28%] [G loss: 1.821888]\n",
      "epoch:24 step:23368 [D loss: 0.655759, acc: 61.72%] [G loss: 1.867556]\n",
      "epoch:24 step:23369 [D loss: 0.723877, acc: 53.12%] [G loss: 1.839124]\n",
      "epoch:24 step:23370 [D loss: 0.626058, acc: 63.28%] [G loss: 1.806916]\n",
      "epoch:24 step:23371 [D loss: 0.666639, acc: 56.25%] [G loss: 1.831515]\n",
      "epoch:24 step:23372 [D loss: 0.581596, acc: 70.31%] [G loss: 1.935279]\n",
      "epoch:24 step:23373 [D loss: 0.626806, acc: 67.97%] [G loss: 1.780218]\n",
      "epoch:24 step:23374 [D loss: 0.624904, acc: 68.75%] [G loss: 1.938123]\n",
      "epoch:24 step:23375 [D loss: 0.625189, acc: 65.62%] [G loss: 1.831162]\n",
      "epoch:24 step:23376 [D loss: 0.609198, acc: 67.97%] [G loss: 1.859543]\n",
      "epoch:24 step:23377 [D loss: 0.671277, acc: 63.28%] [G loss: 1.890974]\n",
      "epoch:24 step:23378 [D loss: 0.586291, acc: 66.41%] [G loss: 2.030290]\n",
      "epoch:24 step:23379 [D loss: 0.655015, acc: 66.41%] [G loss: 1.821581]\n",
      "epoch:24 step:23380 [D loss: 0.663919, acc: 64.06%] [G loss: 1.957164]\n",
      "epoch:24 step:23381 [D loss: 0.739707, acc: 50.00%] [G loss: 1.806448]\n",
      "epoch:24 step:23382 [D loss: 0.617886, acc: 67.97%] [G loss: 2.093445]\n",
      "epoch:24 step:23383 [D loss: 0.681687, acc: 58.59%] [G loss: 1.859198]\n",
      "epoch:24 step:23384 [D loss: 0.707166, acc: 54.69%] [G loss: 1.837870]\n",
      "epoch:24 step:23385 [D loss: 0.643023, acc: 64.06%] [G loss: 1.876695]\n",
      "epoch:24 step:23386 [D loss: 0.653730, acc: 60.16%] [G loss: 1.840769]\n",
      "epoch:24 step:23387 [D loss: 0.626602, acc: 67.19%] [G loss: 1.964353]\n",
      "epoch:24 step:23388 [D loss: 0.655588, acc: 65.62%] [G loss: 1.953277]\n",
      "epoch:24 step:23389 [D loss: 0.608301, acc: 69.53%] [G loss: 1.982839]\n",
      "epoch:24 step:23390 [D loss: 0.658467, acc: 63.28%] [G loss: 1.989581]\n",
      "epoch:24 step:23391 [D loss: 0.688190, acc: 57.81%] [G loss: 1.907415]\n",
      "epoch:24 step:23392 [D loss: 0.611097, acc: 67.19%] [G loss: 2.068170]\n",
      "epoch:24 step:23393 [D loss: 0.608275, acc: 65.62%] [G loss: 2.070484]\n",
      "epoch:24 step:23394 [D loss: 0.675633, acc: 61.72%] [G loss: 1.984464]\n",
      "epoch:24 step:23395 [D loss: 0.631153, acc: 66.41%] [G loss: 1.817138]\n",
      "epoch:24 step:23396 [D loss: 0.630584, acc: 66.41%] [G loss: 1.928933]\n",
      "epoch:24 step:23397 [D loss: 0.619291, acc: 69.53%] [G loss: 2.183617]\n",
      "epoch:24 step:23398 [D loss: 0.672184, acc: 57.81%] [G loss: 2.018357]\n",
      "epoch:24 step:23399 [D loss: 0.635032, acc: 60.16%] [G loss: 1.988771]\n",
      "epoch:24 step:23400 [D loss: 0.643397, acc: 66.41%] [G loss: 1.857988]\n",
      "##############\n",
      "[2.53879965 1.58419853 6.2154131  4.7592796  3.44524574 5.38413616\n",
      " 4.3345019  4.67294545 4.56507585 3.68941664]\n",
      "##########\n",
      "epoch:24 step:23401 [D loss: 0.671107, acc: 60.94%] [G loss: 1.851738]\n",
      "epoch:24 step:23402 [D loss: 0.713070, acc: 53.12%] [G loss: 1.834207]\n",
      "epoch:24 step:23403 [D loss: 0.703739, acc: 52.34%] [G loss: 1.980896]\n",
      "epoch:24 step:23404 [D loss: 0.646682, acc: 64.06%] [G loss: 1.874148]\n",
      "epoch:24 step:23405 [D loss: 0.681111, acc: 57.03%] [G loss: 1.887275]\n",
      "epoch:24 step:23406 [D loss: 0.633045, acc: 68.75%] [G loss: 2.127713]\n",
      "epoch:24 step:23407 [D loss: 0.574039, acc: 69.53%] [G loss: 2.230392]\n",
      "epoch:24 step:23408 [D loss: 0.719975, acc: 50.78%] [G loss: 1.759713]\n",
      "epoch:24 step:23409 [D loss: 0.670977, acc: 59.38%] [G loss: 2.002399]\n",
      "epoch:24 step:23410 [D loss: 0.613453, acc: 67.97%] [G loss: 1.879192]\n",
      "epoch:24 step:23411 [D loss: 0.617088, acc: 67.19%] [G loss: 2.043029]\n",
      "epoch:24 step:23412 [D loss: 0.591486, acc: 72.66%] [G loss: 2.110493]\n",
      "epoch:24 step:23413 [D loss: 0.611486, acc: 68.75%] [G loss: 1.985319]\n",
      "epoch:24 step:23414 [D loss: 0.669275, acc: 61.72%] [G loss: 2.031199]\n",
      "epoch:24 step:23415 [D loss: 0.685374, acc: 62.50%] [G loss: 2.034899]\n",
      "epoch:24 step:23416 [D loss: 0.718106, acc: 52.34%] [G loss: 1.809787]\n",
      "epoch:24 step:23417 [D loss: 0.730984, acc: 48.44%] [G loss: 1.900361]\n",
      "epoch:24 step:23418 [D loss: 0.600642, acc: 67.19%] [G loss: 2.024654]\n",
      "epoch:24 step:23419 [D loss: 0.606263, acc: 70.31%] [G loss: 1.997688]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:24 step:23420 [D loss: 0.601466, acc: 67.97%] [G loss: 1.841934]\n",
      "epoch:24 step:23421 [D loss: 0.645210, acc: 60.16%] [G loss: 1.855258]\n",
      "epoch:24 step:23422 [D loss: 0.643354, acc: 64.06%] [G loss: 1.903739]\n",
      "epoch:24 step:23423 [D loss: 0.569730, acc: 73.44%] [G loss: 1.932912]\n",
      "epoch:24 step:23424 [D loss: 0.558093, acc: 75.00%] [G loss: 2.330511]\n",
      "epoch:24 step:23425 [D loss: 0.585765, acc: 71.09%] [G loss: 2.513345]\n",
      "epoch:25 step:23426 [D loss: 0.675905, acc: 58.59%] [G loss: 1.860262]\n",
      "epoch:25 step:23427 [D loss: 0.597699, acc: 65.62%] [G loss: 1.916697]\n",
      "epoch:25 step:23428 [D loss: 0.659900, acc: 60.94%] [G loss: 1.917364]\n",
      "epoch:25 step:23429 [D loss: 0.633227, acc: 67.19%] [G loss: 1.869892]\n",
      "epoch:25 step:23430 [D loss: 0.625690, acc: 64.06%] [G loss: 1.845689]\n",
      "epoch:25 step:23431 [D loss: 0.627618, acc: 60.16%] [G loss: 1.929768]\n",
      "epoch:25 step:23432 [D loss: 0.615564, acc: 67.97%] [G loss: 1.892956]\n",
      "epoch:25 step:23433 [D loss: 0.663077, acc: 63.28%] [G loss: 1.974947]\n",
      "epoch:25 step:23434 [D loss: 0.650043, acc: 62.50%] [G loss: 2.240793]\n",
      "epoch:25 step:23435 [D loss: 0.647166, acc: 65.62%] [G loss: 2.125644]\n",
      "epoch:25 step:23436 [D loss: 0.603690, acc: 66.41%] [G loss: 1.972435]\n",
      "epoch:25 step:23437 [D loss: 0.650498, acc: 62.50%] [G loss: 1.845485]\n",
      "epoch:25 step:23438 [D loss: 0.676444, acc: 62.50%] [G loss: 1.831041]\n",
      "epoch:25 step:23439 [D loss: 0.656031, acc: 63.28%] [G loss: 2.072467]\n",
      "epoch:25 step:23440 [D loss: 0.608763, acc: 64.84%] [G loss: 1.984902]\n",
      "epoch:25 step:23441 [D loss: 0.632017, acc: 66.41%] [G loss: 2.079464]\n",
      "epoch:25 step:23442 [D loss: 0.704606, acc: 57.81%] [G loss: 1.824896]\n",
      "epoch:25 step:23443 [D loss: 0.641190, acc: 63.28%] [G loss: 1.940816]\n",
      "epoch:25 step:23444 [D loss: 0.674677, acc: 57.81%] [G loss: 1.853781]\n",
      "epoch:25 step:23445 [D loss: 0.662740, acc: 54.69%] [G loss: 1.727169]\n",
      "epoch:25 step:23446 [D loss: 0.641378, acc: 59.38%] [G loss: 1.783689]\n",
      "epoch:25 step:23447 [D loss: 0.635462, acc: 65.62%] [G loss: 1.830081]\n",
      "epoch:25 step:23448 [D loss: 0.632706, acc: 63.28%] [G loss: 1.984429]\n",
      "epoch:25 step:23449 [D loss: 0.684716, acc: 57.03%] [G loss: 1.964073]\n",
      "epoch:25 step:23450 [D loss: 0.622306, acc: 61.72%] [G loss: 2.103219]\n",
      "epoch:25 step:23451 [D loss: 0.650791, acc: 59.38%] [G loss: 1.800917]\n",
      "epoch:25 step:23452 [D loss: 0.693067, acc: 55.47%] [G loss: 1.822259]\n",
      "epoch:25 step:23453 [D loss: 0.641623, acc: 64.84%] [G loss: 1.837337]\n",
      "epoch:25 step:23454 [D loss: 0.636340, acc: 64.84%] [G loss: 1.868799]\n",
      "epoch:25 step:23455 [D loss: 0.718997, acc: 59.38%] [G loss: 1.814797]\n",
      "epoch:25 step:23456 [D loss: 0.659358, acc: 57.03%] [G loss: 1.714649]\n",
      "epoch:25 step:23457 [D loss: 0.659915, acc: 60.94%] [G loss: 1.782623]\n",
      "epoch:25 step:23458 [D loss: 0.643463, acc: 59.38%] [G loss: 1.827156]\n",
      "epoch:25 step:23459 [D loss: 0.668948, acc: 59.38%] [G loss: 1.849117]\n",
      "epoch:25 step:23460 [D loss: 0.689472, acc: 56.25%] [G loss: 1.763882]\n",
      "epoch:25 step:23461 [D loss: 0.621654, acc: 64.06%] [G loss: 1.809237]\n",
      "epoch:25 step:23462 [D loss: 0.706789, acc: 51.56%] [G loss: 1.810716]\n",
      "epoch:25 step:23463 [D loss: 0.659756, acc: 59.38%] [G loss: 1.916929]\n",
      "epoch:25 step:23464 [D loss: 0.679974, acc: 60.16%] [G loss: 1.964973]\n",
      "epoch:25 step:23465 [D loss: 0.617616, acc: 64.84%] [G loss: 1.943131]\n",
      "epoch:25 step:23466 [D loss: 0.673636, acc: 62.50%] [G loss: 1.698950]\n",
      "epoch:25 step:23467 [D loss: 0.660878, acc: 61.72%] [G loss: 1.952641]\n",
      "epoch:25 step:23468 [D loss: 0.621466, acc: 67.19%] [G loss: 1.917725]\n",
      "epoch:25 step:23469 [D loss: 0.661058, acc: 62.50%] [G loss: 1.893255]\n",
      "epoch:25 step:23470 [D loss: 0.657959, acc: 62.50%] [G loss: 1.837308]\n",
      "epoch:25 step:23471 [D loss: 0.638003, acc: 63.28%] [G loss: 1.918021]\n",
      "epoch:25 step:23472 [D loss: 0.647598, acc: 64.06%] [G loss: 1.911387]\n",
      "epoch:25 step:23473 [D loss: 0.611589, acc: 66.41%] [G loss: 1.877085]\n",
      "epoch:25 step:23474 [D loss: 0.633024, acc: 65.62%] [G loss: 1.984465]\n",
      "epoch:25 step:23475 [D loss: 0.615073, acc: 66.41%] [G loss: 1.953262]\n",
      "epoch:25 step:23476 [D loss: 0.660942, acc: 64.06%] [G loss: 1.861878]\n",
      "epoch:25 step:23477 [D loss: 0.698146, acc: 57.03%] [G loss: 1.948428]\n",
      "epoch:25 step:23478 [D loss: 0.691045, acc: 57.03%] [G loss: 1.795585]\n",
      "epoch:25 step:23479 [D loss: 0.634956, acc: 64.06%] [G loss: 1.893700]\n",
      "epoch:25 step:23480 [D loss: 0.622976, acc: 66.41%] [G loss: 1.983042]\n",
      "epoch:25 step:23481 [D loss: 0.640832, acc: 64.84%] [G loss: 1.903983]\n",
      "epoch:25 step:23482 [D loss: 0.652719, acc: 60.16%] [G loss: 1.750171]\n",
      "epoch:25 step:23483 [D loss: 0.629767, acc: 61.72%] [G loss: 1.861805]\n",
      "epoch:25 step:23484 [D loss: 0.685570, acc: 58.59%] [G loss: 1.873190]\n",
      "epoch:25 step:23485 [D loss: 0.670951, acc: 61.72%] [G loss: 1.771469]\n",
      "epoch:25 step:23486 [D loss: 0.644020, acc: 60.94%] [G loss: 1.864915]\n",
      "epoch:25 step:23487 [D loss: 0.696520, acc: 58.59%] [G loss: 1.824686]\n",
      "epoch:25 step:23488 [D loss: 0.675051, acc: 64.06%] [G loss: 1.818672]\n",
      "epoch:25 step:23489 [D loss: 0.640034, acc: 67.19%] [G loss: 1.843230]\n",
      "epoch:25 step:23490 [D loss: 0.593868, acc: 67.19%] [G loss: 1.875326]\n",
      "epoch:25 step:23491 [D loss: 0.589489, acc: 72.66%] [G loss: 1.793764]\n",
      "epoch:25 step:23492 [D loss: 0.678721, acc: 56.25%] [G loss: 1.918888]\n",
      "epoch:25 step:23493 [D loss: 0.669792, acc: 58.59%] [G loss: 1.770484]\n",
      "epoch:25 step:23494 [D loss: 0.598898, acc: 73.44%] [G loss: 1.994343]\n",
      "epoch:25 step:23495 [D loss: 0.639524, acc: 60.94%] [G loss: 1.835346]\n",
      "epoch:25 step:23496 [D loss: 0.647233, acc: 60.94%] [G loss: 1.705122]\n",
      "epoch:25 step:23497 [D loss: 0.672410, acc: 64.84%] [G loss: 1.718917]\n",
      "epoch:25 step:23498 [D loss: 0.655148, acc: 59.38%] [G loss: 1.753301]\n",
      "epoch:25 step:23499 [D loss: 0.667112, acc: 57.81%] [G loss: 1.765542]\n",
      "epoch:25 step:23500 [D loss: 0.622594, acc: 65.62%] [G loss: 1.943031]\n",
      "epoch:25 step:23501 [D loss: 0.630531, acc: 67.19%] [G loss: 1.978243]\n",
      "epoch:25 step:23502 [D loss: 0.681694, acc: 57.81%] [G loss: 1.844926]\n",
      "epoch:25 step:23503 [D loss: 0.631919, acc: 61.72%] [G loss: 1.735819]\n",
      "epoch:25 step:23504 [D loss: 0.662698, acc: 60.94%] [G loss: 1.814607]\n",
      "epoch:25 step:23505 [D loss: 0.664182, acc: 62.50%] [G loss: 1.764934]\n",
      "epoch:25 step:23506 [D loss: 0.662018, acc: 60.94%] [G loss: 1.671879]\n",
      "epoch:25 step:23507 [D loss: 0.678060, acc: 53.12%] [G loss: 1.772356]\n",
      "epoch:25 step:23508 [D loss: 0.646469, acc: 62.50%] [G loss: 1.735831]\n",
      "epoch:25 step:23509 [D loss: 0.628783, acc: 67.19%] [G loss: 1.762226]\n",
      "epoch:25 step:23510 [D loss: 0.636147, acc: 64.06%] [G loss: 1.798926]\n",
      "epoch:25 step:23511 [D loss: 0.693363, acc: 54.69%] [G loss: 1.756820]\n",
      "epoch:25 step:23512 [D loss: 0.682612, acc: 56.25%] [G loss: 1.747126]\n",
      "epoch:25 step:23513 [D loss: 0.617418, acc: 68.75%] [G loss: 1.798340]\n",
      "epoch:25 step:23514 [D loss: 0.696744, acc: 53.12%] [G loss: 1.863853]\n",
      "epoch:25 step:23515 [D loss: 0.659413, acc: 58.59%] [G loss: 1.736292]\n",
      "epoch:25 step:23516 [D loss: 0.637308, acc: 67.97%] [G loss: 1.839839]\n",
      "epoch:25 step:23517 [D loss: 0.640158, acc: 63.28%] [G loss: 1.864501]\n",
      "epoch:25 step:23518 [D loss: 0.606970, acc: 67.19%] [G loss: 1.944237]\n",
      "epoch:25 step:23519 [D loss: 0.655707, acc: 64.84%] [G loss: 1.807825]\n",
      "epoch:25 step:23520 [D loss: 0.664204, acc: 58.59%] [G loss: 1.618076]\n",
      "epoch:25 step:23521 [D loss: 0.633696, acc: 63.28%] [G loss: 1.892118]\n",
      "epoch:25 step:23522 [D loss: 0.643956, acc: 60.16%] [G loss: 1.885875]\n",
      "epoch:25 step:23523 [D loss: 0.712852, acc: 56.25%] [G loss: 1.748095]\n",
      "epoch:25 step:23524 [D loss: 0.662003, acc: 61.72%] [G loss: 1.949237]\n",
      "epoch:25 step:23525 [D loss: 0.646633, acc: 61.72%] [G loss: 1.794065]\n",
      "epoch:25 step:23526 [D loss: 0.652281, acc: 65.62%] [G loss: 1.831280]\n",
      "epoch:25 step:23527 [D loss: 0.638002, acc: 61.72%] [G loss: 1.783441]\n",
      "epoch:25 step:23528 [D loss: 0.663242, acc: 60.16%] [G loss: 1.876924]\n",
      "epoch:25 step:23529 [D loss: 0.621548, acc: 64.06%] [G loss: 1.897939]\n",
      "epoch:25 step:23530 [D loss: 0.699514, acc: 54.69%] [G loss: 1.763700]\n",
      "epoch:25 step:23531 [D loss: 0.608103, acc: 64.06%] [G loss: 1.932159]\n",
      "epoch:25 step:23532 [D loss: 0.637277, acc: 64.06%] [G loss: 1.924320]\n",
      "epoch:25 step:23533 [D loss: 0.704181, acc: 52.34%] [G loss: 1.814466]\n",
      "epoch:25 step:23534 [D loss: 0.688602, acc: 53.91%] [G loss: 1.629968]\n",
      "epoch:25 step:23535 [D loss: 0.648801, acc: 65.62%] [G loss: 1.701396]\n",
      "epoch:25 step:23536 [D loss: 0.599955, acc: 68.75%] [G loss: 1.950722]\n",
      "epoch:25 step:23537 [D loss: 0.642036, acc: 62.50%] [G loss: 1.920927]\n",
      "epoch:25 step:23538 [D loss: 0.685390, acc: 57.81%] [G loss: 1.826439]\n",
      "epoch:25 step:23539 [D loss: 0.608459, acc: 65.62%] [G loss: 2.046174]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:25 step:23540 [D loss: 0.606944, acc: 65.62%] [G loss: 2.076262]\n",
      "epoch:25 step:23541 [D loss: 0.575880, acc: 76.56%] [G loss: 2.240660]\n",
      "epoch:25 step:23542 [D loss: 0.605910, acc: 60.94%] [G loss: 2.094155]\n",
      "epoch:25 step:23543 [D loss: 0.616195, acc: 68.75%] [G loss: 2.019027]\n",
      "epoch:25 step:23544 [D loss: 0.639132, acc: 59.38%] [G loss: 2.250786]\n",
      "epoch:25 step:23545 [D loss: 0.635761, acc: 63.28%] [G loss: 2.123985]\n",
      "epoch:25 step:23546 [D loss: 0.665920, acc: 60.94%] [G loss: 2.019415]\n",
      "epoch:25 step:23547 [D loss: 0.645734, acc: 61.72%] [G loss: 2.084723]\n",
      "epoch:25 step:23548 [D loss: 0.659442, acc: 57.03%] [G loss: 1.873133]\n",
      "epoch:25 step:23549 [D loss: 0.681720, acc: 59.38%] [G loss: 1.867593]\n",
      "epoch:25 step:23550 [D loss: 0.705805, acc: 56.25%] [G loss: 1.759769]\n",
      "epoch:25 step:23551 [D loss: 0.662359, acc: 60.94%] [G loss: 1.903754]\n",
      "epoch:25 step:23552 [D loss: 0.681321, acc: 61.72%] [G loss: 1.819452]\n",
      "epoch:25 step:23553 [D loss: 0.631685, acc: 64.84%] [G loss: 1.916159]\n",
      "epoch:25 step:23554 [D loss: 0.698108, acc: 54.69%] [G loss: 1.801971]\n",
      "epoch:25 step:23555 [D loss: 0.646131, acc: 65.62%] [G loss: 1.892689]\n",
      "epoch:25 step:23556 [D loss: 0.677238, acc: 64.06%] [G loss: 1.817299]\n",
      "epoch:25 step:23557 [D loss: 0.658422, acc: 61.72%] [G loss: 1.885878]\n",
      "epoch:25 step:23558 [D loss: 0.703224, acc: 56.25%] [G loss: 1.792113]\n",
      "epoch:25 step:23559 [D loss: 0.670669, acc: 57.81%] [G loss: 1.766280]\n",
      "epoch:25 step:23560 [D loss: 0.636206, acc: 64.84%] [G loss: 1.795653]\n",
      "epoch:25 step:23561 [D loss: 0.674848, acc: 57.03%] [G loss: 1.791123]\n",
      "epoch:25 step:23562 [D loss: 0.700605, acc: 61.72%] [G loss: 1.801803]\n",
      "epoch:25 step:23563 [D loss: 0.677692, acc: 55.47%] [G loss: 1.776212]\n",
      "epoch:25 step:23564 [D loss: 0.660360, acc: 64.06%] [G loss: 1.855244]\n",
      "epoch:25 step:23565 [D loss: 0.670704, acc: 63.28%] [G loss: 1.661286]\n",
      "epoch:25 step:23566 [D loss: 0.661483, acc: 57.03%] [G loss: 1.780948]\n",
      "epoch:25 step:23567 [D loss: 0.702617, acc: 53.91%] [G loss: 1.872629]\n",
      "epoch:25 step:23568 [D loss: 0.715071, acc: 58.59%] [G loss: 1.785849]\n",
      "epoch:25 step:23569 [D loss: 0.673295, acc: 60.16%] [G loss: 1.756446]\n",
      "epoch:25 step:23570 [D loss: 0.655840, acc: 62.50%] [G loss: 1.827111]\n",
      "epoch:25 step:23571 [D loss: 0.608985, acc: 70.31%] [G loss: 1.866870]\n",
      "epoch:25 step:23572 [D loss: 0.633471, acc: 63.28%] [G loss: 1.869113]\n",
      "epoch:25 step:23573 [D loss: 0.667859, acc: 58.59%] [G loss: 1.783600]\n",
      "epoch:25 step:23574 [D loss: 0.639828, acc: 64.84%] [G loss: 1.814331]\n",
      "epoch:25 step:23575 [D loss: 0.649853, acc: 64.84%] [G loss: 1.745997]\n",
      "epoch:25 step:23576 [D loss: 0.637262, acc: 61.72%] [G loss: 1.773502]\n",
      "epoch:25 step:23577 [D loss: 0.637423, acc: 60.94%] [G loss: 1.816209]\n",
      "epoch:25 step:23578 [D loss: 0.640153, acc: 62.50%] [G loss: 1.898466]\n",
      "epoch:25 step:23579 [D loss: 0.614418, acc: 64.06%] [G loss: 1.904758]\n",
      "epoch:25 step:23580 [D loss: 0.680215, acc: 58.59%] [G loss: 1.879316]\n",
      "epoch:25 step:23581 [D loss: 0.615997, acc: 63.28%] [G loss: 1.936652]\n",
      "epoch:25 step:23582 [D loss: 0.697742, acc: 57.03%] [G loss: 1.881655]\n",
      "epoch:25 step:23583 [D loss: 0.651756, acc: 62.50%] [G loss: 1.870949]\n",
      "epoch:25 step:23584 [D loss: 0.672327, acc: 58.59%] [G loss: 1.835023]\n",
      "epoch:25 step:23585 [D loss: 0.722382, acc: 48.44%] [G loss: 1.698625]\n",
      "epoch:25 step:23586 [D loss: 0.631464, acc: 61.72%] [G loss: 1.785266]\n",
      "epoch:25 step:23587 [D loss: 0.659859, acc: 57.03%] [G loss: 1.875205]\n",
      "epoch:25 step:23588 [D loss: 0.625703, acc: 58.59%] [G loss: 1.855442]\n",
      "epoch:25 step:23589 [D loss: 0.608151, acc: 67.19%] [G loss: 1.711091]\n",
      "epoch:25 step:23590 [D loss: 0.663492, acc: 60.94%] [G loss: 1.812546]\n",
      "epoch:25 step:23591 [D loss: 0.655488, acc: 60.16%] [G loss: 1.892578]\n",
      "epoch:25 step:23592 [D loss: 0.643381, acc: 64.06%] [G loss: 1.926169]\n",
      "epoch:25 step:23593 [D loss: 0.642610, acc: 67.19%] [G loss: 1.779572]\n",
      "epoch:25 step:23594 [D loss: 0.621062, acc: 61.72%] [G loss: 1.904907]\n",
      "epoch:25 step:23595 [D loss: 0.642696, acc: 61.72%] [G loss: 1.826441]\n",
      "epoch:25 step:23596 [D loss: 0.669624, acc: 56.25%] [G loss: 1.763138]\n",
      "epoch:25 step:23597 [D loss: 0.684047, acc: 53.91%] [G loss: 1.897956]\n",
      "epoch:25 step:23598 [D loss: 0.645038, acc: 64.06%] [G loss: 1.894551]\n",
      "epoch:25 step:23599 [D loss: 0.643505, acc: 68.75%] [G loss: 1.880328]\n",
      "epoch:25 step:23600 [D loss: 0.673504, acc: 57.81%] [G loss: 1.775998]\n",
      "##############\n",
      "[2.31385001 1.25693625 6.30718958 4.52749991 3.57046112 5.4356553\n",
      " 4.36080602 4.69909064 4.42529088 3.67147272]\n",
      "##########\n",
      "epoch:25 step:23601 [D loss: 0.639386, acc: 60.94%] [G loss: 1.729112]\n",
      "epoch:25 step:23602 [D loss: 0.622411, acc: 68.75%] [G loss: 1.799610]\n",
      "epoch:25 step:23603 [D loss: 0.653232, acc: 60.94%] [G loss: 1.882481]\n",
      "epoch:25 step:23604 [D loss: 0.659960, acc: 60.94%] [G loss: 1.735426]\n",
      "epoch:25 step:23605 [D loss: 0.632317, acc: 66.41%] [G loss: 1.910338]\n",
      "epoch:25 step:23606 [D loss: 0.688817, acc: 55.47%] [G loss: 1.778384]\n",
      "epoch:25 step:23607 [D loss: 0.677514, acc: 57.03%] [G loss: 1.803813]\n",
      "epoch:25 step:23608 [D loss: 0.633986, acc: 66.41%] [G loss: 1.725387]\n",
      "epoch:25 step:23609 [D loss: 0.629313, acc: 67.19%] [G loss: 1.957626]\n",
      "epoch:25 step:23610 [D loss: 0.654977, acc: 64.06%] [G loss: 1.890856]\n",
      "epoch:25 step:23611 [D loss: 0.667977, acc: 60.16%] [G loss: 1.671771]\n",
      "epoch:25 step:23612 [D loss: 0.737831, acc: 57.81%] [G loss: 1.913815]\n",
      "epoch:25 step:23613 [D loss: 0.660738, acc: 60.16%] [G loss: 1.914654]\n",
      "epoch:25 step:23614 [D loss: 0.642510, acc: 64.06%] [G loss: 1.726082]\n",
      "epoch:25 step:23615 [D loss: 0.635358, acc: 64.84%] [G loss: 1.891092]\n",
      "epoch:25 step:23616 [D loss: 0.666767, acc: 53.12%] [G loss: 1.812762]\n",
      "epoch:25 step:23617 [D loss: 0.595029, acc: 64.06%] [G loss: 1.989764]\n",
      "epoch:25 step:23618 [D loss: 0.679862, acc: 58.59%] [G loss: 1.969288]\n",
      "epoch:25 step:23619 [D loss: 0.626922, acc: 66.41%] [G loss: 1.996949]\n",
      "epoch:25 step:23620 [D loss: 0.613316, acc: 70.31%] [G loss: 1.889143]\n",
      "epoch:25 step:23621 [D loss: 0.661901, acc: 58.59%] [G loss: 1.811403]\n",
      "epoch:25 step:23622 [D loss: 0.643358, acc: 64.06%] [G loss: 1.853656]\n",
      "epoch:25 step:23623 [D loss: 0.620822, acc: 65.62%] [G loss: 1.901567]\n",
      "epoch:25 step:23624 [D loss: 0.686148, acc: 55.47%] [G loss: 1.851157]\n",
      "epoch:25 step:23625 [D loss: 0.680093, acc: 57.81%] [G loss: 1.758492]\n",
      "epoch:25 step:23626 [D loss: 0.599463, acc: 68.75%] [G loss: 1.850734]\n",
      "epoch:25 step:23627 [D loss: 0.643440, acc: 58.59%] [G loss: 1.784495]\n",
      "epoch:25 step:23628 [D loss: 0.685790, acc: 62.50%] [G loss: 1.892721]\n",
      "epoch:25 step:23629 [D loss: 0.642723, acc: 61.72%] [G loss: 1.923018]\n",
      "epoch:25 step:23630 [D loss: 0.695752, acc: 58.59%] [G loss: 1.824700]\n",
      "epoch:25 step:23631 [D loss: 0.601419, acc: 69.53%] [G loss: 2.065694]\n",
      "epoch:25 step:23632 [D loss: 0.633605, acc: 63.28%] [G loss: 2.031749]\n",
      "epoch:25 step:23633 [D loss: 0.571711, acc: 72.66%] [G loss: 2.052109]\n",
      "epoch:25 step:23634 [D loss: 0.571100, acc: 71.09%] [G loss: 2.164963]\n",
      "epoch:25 step:23635 [D loss: 0.674599, acc: 58.59%] [G loss: 1.885266]\n",
      "epoch:25 step:23636 [D loss: 0.688741, acc: 60.16%] [G loss: 1.698143]\n",
      "epoch:25 step:23637 [D loss: 0.651782, acc: 58.59%] [G loss: 1.714546]\n",
      "epoch:25 step:23638 [D loss: 0.685369, acc: 58.59%] [G loss: 1.767597]\n",
      "epoch:25 step:23639 [D loss: 0.696504, acc: 57.03%] [G loss: 1.891800]\n",
      "epoch:25 step:23640 [D loss: 0.651099, acc: 59.38%] [G loss: 1.884355]\n",
      "epoch:25 step:23641 [D loss: 0.649240, acc: 64.06%] [G loss: 1.852609]\n",
      "epoch:25 step:23642 [D loss: 0.631295, acc: 64.06%] [G loss: 1.893379]\n",
      "epoch:25 step:23643 [D loss: 0.623362, acc: 71.09%] [G loss: 2.050683]\n",
      "epoch:25 step:23644 [D loss: 0.626934, acc: 62.50%] [G loss: 2.190140]\n",
      "epoch:25 step:23645 [D loss: 0.686857, acc: 50.00%] [G loss: 1.863215]\n",
      "epoch:25 step:23646 [D loss: 0.674319, acc: 57.81%] [G loss: 1.986740]\n",
      "epoch:25 step:23647 [D loss: 0.642916, acc: 65.62%] [G loss: 1.845354]\n",
      "epoch:25 step:23648 [D loss: 0.652771, acc: 62.50%] [G loss: 1.887476]\n",
      "epoch:25 step:23649 [D loss: 0.634997, acc: 61.72%] [G loss: 1.839743]\n",
      "epoch:25 step:23650 [D loss: 0.633014, acc: 64.84%] [G loss: 1.889366]\n",
      "epoch:25 step:23651 [D loss: 0.645673, acc: 58.59%] [G loss: 1.748556]\n",
      "epoch:25 step:23652 [D loss: 0.645085, acc: 60.16%] [G loss: 1.883726]\n",
      "epoch:25 step:23653 [D loss: 0.714624, acc: 50.78%] [G loss: 1.801643]\n",
      "epoch:25 step:23654 [D loss: 0.624078, acc: 60.16%] [G loss: 2.034147]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:25 step:23655 [D loss: 0.594870, acc: 71.09%] [G loss: 2.154694]\n",
      "epoch:25 step:23656 [D loss: 0.595100, acc: 73.44%] [G loss: 2.026815]\n",
      "epoch:25 step:23657 [D loss: 0.586246, acc: 72.66%] [G loss: 2.200876]\n",
      "epoch:25 step:23658 [D loss: 0.641954, acc: 63.28%] [G loss: 2.011105]\n",
      "epoch:25 step:23659 [D loss: 0.673551, acc: 54.69%] [G loss: 1.798503]\n",
      "epoch:25 step:23660 [D loss: 0.627583, acc: 67.97%] [G loss: 1.813185]\n",
      "epoch:25 step:23661 [D loss: 0.616253, acc: 67.19%] [G loss: 1.866951]\n",
      "epoch:25 step:23662 [D loss: 0.645510, acc: 63.28%] [G loss: 2.004984]\n",
      "epoch:25 step:23663 [D loss: 0.629893, acc: 69.53%] [G loss: 1.782305]\n",
      "epoch:25 step:23664 [D loss: 0.647639, acc: 60.94%] [G loss: 1.858663]\n",
      "epoch:25 step:23665 [D loss: 0.609344, acc: 69.53%] [G loss: 1.979291]\n",
      "epoch:25 step:23666 [D loss: 0.588071, acc: 71.88%] [G loss: 2.122269]\n",
      "epoch:25 step:23667 [D loss: 0.625293, acc: 65.62%] [G loss: 1.912996]\n",
      "epoch:25 step:23668 [D loss: 0.640305, acc: 63.28%] [G loss: 1.803467]\n",
      "epoch:25 step:23669 [D loss: 0.629035, acc: 64.84%] [G loss: 1.879977]\n",
      "epoch:25 step:23670 [D loss: 0.660423, acc: 61.72%] [G loss: 2.154227]\n",
      "epoch:25 step:23671 [D loss: 0.608326, acc: 66.41%] [G loss: 2.030759]\n",
      "epoch:25 step:23672 [D loss: 0.646463, acc: 62.50%] [G loss: 1.867802]\n",
      "epoch:25 step:23673 [D loss: 0.674784, acc: 59.38%] [G loss: 2.025387]\n",
      "epoch:25 step:23674 [D loss: 0.696697, acc: 53.91%] [G loss: 1.780068]\n",
      "epoch:25 step:23675 [D loss: 0.743453, acc: 46.88%] [G loss: 1.751199]\n",
      "epoch:25 step:23676 [D loss: 0.650999, acc: 60.94%] [G loss: 1.667410]\n",
      "epoch:25 step:23677 [D loss: 0.689676, acc: 58.59%] [G loss: 1.822351]\n",
      "epoch:25 step:23678 [D loss: 0.676331, acc: 59.38%] [G loss: 1.912547]\n",
      "epoch:25 step:23679 [D loss: 0.638711, acc: 64.84%] [G loss: 1.760905]\n",
      "epoch:25 step:23680 [D loss: 0.644528, acc: 67.97%] [G loss: 1.828310]\n",
      "epoch:25 step:23681 [D loss: 0.638395, acc: 63.28%] [G loss: 1.778756]\n",
      "epoch:25 step:23682 [D loss: 0.698215, acc: 57.03%] [G loss: 1.790645]\n",
      "epoch:25 step:23683 [D loss: 0.717623, acc: 52.34%] [G loss: 1.799550]\n",
      "epoch:25 step:23684 [D loss: 0.649582, acc: 65.62%] [G loss: 1.855913]\n",
      "epoch:25 step:23685 [D loss: 0.673738, acc: 57.03%] [G loss: 1.949014]\n",
      "epoch:25 step:23686 [D loss: 0.659551, acc: 63.28%] [G loss: 1.767671]\n",
      "epoch:25 step:23687 [D loss: 0.619844, acc: 67.19%] [G loss: 2.060866]\n",
      "epoch:25 step:23688 [D loss: 0.655083, acc: 60.94%] [G loss: 1.900314]\n",
      "epoch:25 step:23689 [D loss: 0.657867, acc: 56.25%] [G loss: 1.954814]\n",
      "epoch:25 step:23690 [D loss: 0.698852, acc: 59.38%] [G loss: 1.874475]\n",
      "epoch:25 step:23691 [D loss: 0.679814, acc: 59.38%] [G loss: 1.785521]\n",
      "epoch:25 step:23692 [D loss: 0.647563, acc: 63.28%] [G loss: 1.866662]\n",
      "epoch:25 step:23693 [D loss: 0.673223, acc: 60.94%] [G loss: 1.823394]\n",
      "epoch:25 step:23694 [D loss: 0.668077, acc: 53.91%] [G loss: 1.938208]\n",
      "epoch:25 step:23695 [D loss: 0.616789, acc: 64.06%] [G loss: 1.733871]\n",
      "epoch:25 step:23696 [D loss: 0.619670, acc: 68.75%] [G loss: 1.745368]\n",
      "epoch:25 step:23697 [D loss: 0.633489, acc: 60.94%] [G loss: 1.909736]\n",
      "epoch:25 step:23698 [D loss: 0.674779, acc: 59.38%] [G loss: 1.723652]\n",
      "epoch:25 step:23699 [D loss: 0.565296, acc: 69.53%] [G loss: 2.087646]\n",
      "epoch:25 step:23700 [D loss: 0.642455, acc: 64.06%] [G loss: 2.005704]\n",
      "epoch:25 step:23701 [D loss: 0.604398, acc: 65.62%] [G loss: 1.908438]\n",
      "epoch:25 step:23702 [D loss: 0.650696, acc: 60.16%] [G loss: 1.926329]\n",
      "epoch:25 step:23703 [D loss: 0.602954, acc: 68.75%] [G loss: 1.886461]\n",
      "epoch:25 step:23704 [D loss: 0.637243, acc: 60.16%] [G loss: 1.810788]\n",
      "epoch:25 step:23705 [D loss: 0.634024, acc: 67.97%] [G loss: 1.813042]\n",
      "epoch:25 step:23706 [D loss: 0.653273, acc: 62.50%] [G loss: 1.863416]\n",
      "epoch:25 step:23707 [D loss: 0.634779, acc: 63.28%] [G loss: 1.784570]\n",
      "epoch:25 step:23708 [D loss: 0.654248, acc: 60.94%] [G loss: 1.925792]\n",
      "epoch:25 step:23709 [D loss: 0.685922, acc: 64.06%] [G loss: 1.735319]\n",
      "epoch:25 step:23710 [D loss: 0.617607, acc: 65.62%] [G loss: 1.804625]\n",
      "epoch:25 step:23711 [D loss: 0.632805, acc: 64.06%] [G loss: 1.897690]\n",
      "epoch:25 step:23712 [D loss: 0.629146, acc: 63.28%] [G loss: 1.860842]\n",
      "epoch:25 step:23713 [D loss: 0.663545, acc: 62.50%] [G loss: 1.784584]\n",
      "epoch:25 step:23714 [D loss: 0.631769, acc: 63.28%] [G loss: 2.003008]\n",
      "epoch:25 step:23715 [D loss: 0.670041, acc: 64.84%] [G loss: 1.775767]\n",
      "epoch:25 step:23716 [D loss: 0.634197, acc: 66.41%] [G loss: 1.845986]\n",
      "epoch:25 step:23717 [D loss: 0.635198, acc: 61.72%] [G loss: 1.899799]\n",
      "epoch:25 step:23718 [D loss: 0.601298, acc: 64.84%] [G loss: 1.887103]\n",
      "epoch:25 step:23719 [D loss: 0.660047, acc: 61.72%] [G loss: 1.821708]\n",
      "epoch:25 step:23720 [D loss: 0.653725, acc: 65.62%] [G loss: 1.847780]\n",
      "epoch:25 step:23721 [D loss: 0.645428, acc: 63.28%] [G loss: 1.842511]\n",
      "epoch:25 step:23722 [D loss: 0.705100, acc: 55.47%] [G loss: 1.829733]\n",
      "epoch:25 step:23723 [D loss: 0.590236, acc: 71.09%] [G loss: 2.029737]\n",
      "epoch:25 step:23724 [D loss: 0.616706, acc: 64.06%] [G loss: 2.197643]\n",
      "epoch:25 step:23725 [D loss: 0.618749, acc: 66.41%] [G loss: 1.964443]\n",
      "epoch:25 step:23726 [D loss: 0.678842, acc: 57.81%] [G loss: 1.757564]\n",
      "epoch:25 step:23727 [D loss: 0.690839, acc: 59.38%] [G loss: 1.913434]\n",
      "epoch:25 step:23728 [D loss: 0.700549, acc: 56.25%] [G loss: 1.782387]\n",
      "epoch:25 step:23729 [D loss: 0.609141, acc: 65.62%] [G loss: 1.930325]\n",
      "epoch:25 step:23730 [D loss: 0.678912, acc: 64.06%] [G loss: 1.909314]\n",
      "epoch:25 step:23731 [D loss: 0.672081, acc: 60.94%] [G loss: 1.745284]\n",
      "epoch:25 step:23732 [D loss: 0.595501, acc: 64.84%] [G loss: 1.860045]\n",
      "epoch:25 step:23733 [D loss: 0.628212, acc: 66.41%] [G loss: 1.887835]\n",
      "epoch:25 step:23734 [D loss: 0.629657, acc: 64.84%] [G loss: 1.863257]\n",
      "epoch:25 step:23735 [D loss: 0.688014, acc: 57.81%] [G loss: 1.892143]\n",
      "epoch:25 step:23736 [D loss: 0.638804, acc: 62.50%] [G loss: 1.776099]\n",
      "epoch:25 step:23737 [D loss: 0.618519, acc: 67.19%] [G loss: 2.207972]\n",
      "epoch:25 step:23738 [D loss: 0.598875, acc: 65.62%] [G loss: 2.220849]\n",
      "epoch:25 step:23739 [D loss: 0.580887, acc: 67.97%] [G loss: 2.340642]\n",
      "epoch:25 step:23740 [D loss: 0.602862, acc: 64.84%] [G loss: 2.228648]\n",
      "epoch:25 step:23741 [D loss: 0.736335, acc: 52.34%] [G loss: 1.737167]\n",
      "epoch:25 step:23742 [D loss: 0.655610, acc: 63.28%] [G loss: 1.897193]\n",
      "epoch:25 step:23743 [D loss: 0.603179, acc: 68.75%] [G loss: 1.893361]\n",
      "epoch:25 step:23744 [D loss: 0.671062, acc: 58.59%] [G loss: 1.740964]\n",
      "epoch:25 step:23745 [D loss: 0.627636, acc: 64.84%] [G loss: 1.872737]\n",
      "epoch:25 step:23746 [D loss: 0.647915, acc: 60.16%] [G loss: 1.919381]\n",
      "epoch:25 step:23747 [D loss: 0.630371, acc: 65.62%] [G loss: 1.923429]\n",
      "epoch:25 step:23748 [D loss: 0.628889, acc: 63.28%] [G loss: 1.811480]\n",
      "epoch:25 step:23749 [D loss: 0.655551, acc: 62.50%] [G loss: 1.844840]\n",
      "epoch:25 step:23750 [D loss: 0.668075, acc: 58.59%] [G loss: 1.849071]\n",
      "epoch:25 step:23751 [D loss: 0.632037, acc: 57.81%] [G loss: 1.799846]\n",
      "epoch:25 step:23752 [D loss: 0.611799, acc: 65.62%] [G loss: 1.895380]\n",
      "epoch:25 step:23753 [D loss: 0.639663, acc: 61.72%] [G loss: 1.890881]\n",
      "epoch:25 step:23754 [D loss: 0.628385, acc: 62.50%] [G loss: 2.010051]\n",
      "epoch:25 step:23755 [D loss: 0.635953, acc: 69.53%] [G loss: 2.003054]\n",
      "epoch:25 step:23756 [D loss: 0.631824, acc: 60.94%] [G loss: 1.924286]\n",
      "epoch:25 step:23757 [D loss: 0.614280, acc: 71.09%] [G loss: 1.952990]\n",
      "epoch:25 step:23758 [D loss: 0.635009, acc: 60.16%] [G loss: 1.915439]\n",
      "epoch:25 step:23759 [D loss: 0.694341, acc: 57.81%] [G loss: 2.105294]\n",
      "epoch:25 step:23760 [D loss: 0.665826, acc: 62.50%] [G loss: 1.906738]\n",
      "epoch:25 step:23761 [D loss: 0.637045, acc: 63.28%] [G loss: 1.915178]\n",
      "epoch:25 step:23762 [D loss: 0.639342, acc: 64.06%] [G loss: 1.919248]\n",
      "epoch:25 step:23763 [D loss: 0.634054, acc: 67.19%] [G loss: 1.949687]\n",
      "epoch:25 step:23764 [D loss: 0.565044, acc: 77.34%] [G loss: 2.056826]\n",
      "epoch:25 step:23765 [D loss: 0.620387, acc: 60.16%] [G loss: 1.975249]\n",
      "epoch:25 step:23766 [D loss: 0.719414, acc: 53.12%] [G loss: 1.698030]\n",
      "epoch:25 step:23767 [D loss: 0.694874, acc: 55.47%] [G loss: 1.727671]\n",
      "epoch:25 step:23768 [D loss: 0.691676, acc: 55.47%] [G loss: 1.834130]\n",
      "epoch:25 step:23769 [D loss: 0.665350, acc: 57.03%] [G loss: 1.806917]\n",
      "epoch:25 step:23770 [D loss: 0.594030, acc: 67.97%] [G loss: 2.142478]\n",
      "epoch:25 step:23771 [D loss: 0.675628, acc: 57.81%] [G loss: 2.035721]\n",
      "epoch:25 step:23772 [D loss: 0.594863, acc: 61.72%] [G loss: 2.225226]\n",
      "epoch:25 step:23773 [D loss: 0.661392, acc: 57.03%] [G loss: 1.970714]\n",
      "epoch:25 step:23774 [D loss: 0.685969, acc: 62.50%] [G loss: 1.745597]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:25 step:23775 [D loss: 0.631127, acc: 62.50%] [G loss: 1.833426]\n",
      "epoch:25 step:23776 [D loss: 0.691818, acc: 61.72%] [G loss: 1.812810]\n",
      "epoch:25 step:23777 [D loss: 0.683035, acc: 55.47%] [G loss: 1.683327]\n",
      "epoch:25 step:23778 [D loss: 0.615362, acc: 68.75%] [G loss: 1.839251]\n",
      "epoch:25 step:23779 [D loss: 0.586629, acc: 67.97%] [G loss: 1.884460]\n",
      "epoch:25 step:23780 [D loss: 0.655242, acc: 60.16%] [G loss: 1.836596]\n",
      "epoch:25 step:23781 [D loss: 0.693783, acc: 60.16%] [G loss: 1.755054]\n",
      "epoch:25 step:23782 [D loss: 0.617213, acc: 64.06%] [G loss: 1.828523]\n",
      "epoch:25 step:23783 [D loss: 0.650909, acc: 60.16%] [G loss: 1.828862]\n",
      "epoch:25 step:23784 [D loss: 0.605648, acc: 71.88%] [G loss: 1.896587]\n",
      "epoch:25 step:23785 [D loss: 0.689710, acc: 60.16%] [G loss: 1.899389]\n",
      "epoch:25 step:23786 [D loss: 0.620205, acc: 67.19%] [G loss: 1.816193]\n",
      "epoch:25 step:23787 [D loss: 0.677738, acc: 52.34%] [G loss: 1.732854]\n",
      "epoch:25 step:23788 [D loss: 0.648133, acc: 63.28%] [G loss: 1.899910]\n",
      "epoch:25 step:23789 [D loss: 0.631765, acc: 64.84%] [G loss: 1.802672]\n",
      "epoch:25 step:23790 [D loss: 0.663960, acc: 60.94%] [G loss: 1.831538]\n",
      "epoch:25 step:23791 [D loss: 0.675423, acc: 57.81%] [G loss: 1.971575]\n",
      "epoch:25 step:23792 [D loss: 0.618071, acc: 67.97%] [G loss: 2.037309]\n",
      "epoch:25 step:23793 [D loss: 0.704834, acc: 64.06%] [G loss: 1.805266]\n",
      "epoch:25 step:23794 [D loss: 0.659445, acc: 68.75%] [G loss: 1.774805]\n",
      "epoch:25 step:23795 [D loss: 0.627563, acc: 64.06%] [G loss: 1.845292]\n",
      "epoch:25 step:23796 [D loss: 0.626030, acc: 64.06%] [G loss: 1.863253]\n",
      "epoch:25 step:23797 [D loss: 0.705187, acc: 57.81%] [G loss: 1.845415]\n",
      "epoch:25 step:23798 [D loss: 0.713875, acc: 53.12%] [G loss: 1.720373]\n",
      "epoch:25 step:23799 [D loss: 0.652994, acc: 63.28%] [G loss: 1.801282]\n",
      "epoch:25 step:23800 [D loss: 0.610220, acc: 67.19%] [G loss: 1.731672]\n",
      "##############\n",
      "[2.56928871 1.50690752 6.16303337 4.65991446 3.52850182 5.44019827\n",
      " 4.35098244 4.65964267 4.3812597  3.58417485]\n",
      "##########\n",
      "epoch:25 step:23801 [D loss: 0.718017, acc: 53.12%] [G loss: 1.880169]\n",
      "epoch:25 step:23802 [D loss: 0.670865, acc: 63.28%] [G loss: 1.778818]\n",
      "epoch:25 step:23803 [D loss: 0.687629, acc: 56.25%] [G loss: 1.825681]\n",
      "epoch:25 step:23804 [D loss: 0.644008, acc: 61.72%] [G loss: 2.004965]\n",
      "epoch:25 step:23805 [D loss: 0.652148, acc: 67.19%] [G loss: 1.949139]\n",
      "epoch:25 step:23806 [D loss: 0.681656, acc: 57.81%] [G loss: 1.890922]\n",
      "epoch:25 step:23807 [D loss: 0.641720, acc: 59.38%] [G loss: 1.879987]\n",
      "epoch:25 step:23808 [D loss: 0.672413, acc: 60.94%] [G loss: 1.869258]\n",
      "epoch:25 step:23809 [D loss: 0.630059, acc: 64.84%] [G loss: 1.875678]\n",
      "epoch:25 step:23810 [D loss: 0.657743, acc: 64.84%] [G loss: 1.765683]\n",
      "epoch:25 step:23811 [D loss: 0.701955, acc: 50.78%] [G loss: 1.734458]\n",
      "epoch:25 step:23812 [D loss: 0.655411, acc: 57.03%] [G loss: 1.778756]\n",
      "epoch:25 step:23813 [D loss: 0.668825, acc: 60.16%] [G loss: 1.792118]\n",
      "epoch:25 step:23814 [D loss: 0.641546, acc: 65.62%] [G loss: 1.835629]\n",
      "epoch:25 step:23815 [D loss: 0.643284, acc: 64.84%] [G loss: 1.858217]\n",
      "epoch:25 step:23816 [D loss: 0.693534, acc: 56.25%] [G loss: 1.638267]\n",
      "epoch:25 step:23817 [D loss: 0.645367, acc: 67.19%] [G loss: 1.740858]\n",
      "epoch:25 step:23818 [D loss: 0.638913, acc: 64.06%] [G loss: 1.755365]\n",
      "epoch:25 step:23819 [D loss: 0.641043, acc: 59.38%] [G loss: 1.738012]\n",
      "epoch:25 step:23820 [D loss: 0.654174, acc: 58.59%] [G loss: 2.000270]\n",
      "epoch:25 step:23821 [D loss: 0.707536, acc: 51.56%] [G loss: 1.760666]\n",
      "epoch:25 step:23822 [D loss: 0.681222, acc: 55.47%] [G loss: 1.747747]\n",
      "epoch:25 step:23823 [D loss: 0.657650, acc: 60.94%] [G loss: 1.770442]\n",
      "epoch:25 step:23824 [D loss: 0.690099, acc: 60.94%] [G loss: 1.733062]\n",
      "epoch:25 step:23825 [D loss: 0.666836, acc: 64.84%] [G loss: 1.781573]\n",
      "epoch:25 step:23826 [D loss: 0.655620, acc: 63.28%] [G loss: 1.862196]\n",
      "epoch:25 step:23827 [D loss: 0.677935, acc: 56.25%] [G loss: 1.897036]\n",
      "epoch:25 step:23828 [D loss: 0.702418, acc: 53.12%] [G loss: 1.755086]\n",
      "epoch:25 step:23829 [D loss: 0.644596, acc: 62.50%] [G loss: 1.918536]\n",
      "epoch:25 step:23830 [D loss: 0.661148, acc: 61.72%] [G loss: 1.962925]\n",
      "epoch:25 step:23831 [D loss: 0.628479, acc: 64.06%] [G loss: 1.990535]\n",
      "epoch:25 step:23832 [D loss: 0.614787, acc: 67.19%] [G loss: 1.768852]\n",
      "epoch:25 step:23833 [D loss: 0.676676, acc: 60.16%] [G loss: 1.922117]\n",
      "epoch:25 step:23834 [D loss: 0.630030, acc: 60.94%] [G loss: 1.757711]\n",
      "epoch:25 step:23835 [D loss: 0.664329, acc: 59.38%] [G loss: 1.891368]\n",
      "epoch:25 step:23836 [D loss: 0.651113, acc: 61.72%] [G loss: 1.836144]\n",
      "epoch:25 step:23837 [D loss: 0.596017, acc: 64.84%] [G loss: 1.815477]\n",
      "epoch:25 step:23838 [D loss: 0.666024, acc: 59.38%] [G loss: 1.949446]\n",
      "epoch:25 step:23839 [D loss: 0.606378, acc: 67.97%] [G loss: 2.071145]\n",
      "epoch:25 step:23840 [D loss: 0.590558, acc: 69.53%] [G loss: 2.005594]\n",
      "epoch:25 step:23841 [D loss: 0.619394, acc: 67.97%] [G loss: 2.034887]\n",
      "epoch:25 step:23842 [D loss: 0.681138, acc: 60.94%] [G loss: 1.857194]\n",
      "epoch:25 step:23843 [D loss: 0.647360, acc: 51.56%] [G loss: 1.826651]\n",
      "epoch:25 step:23844 [D loss: 0.671920, acc: 59.38%] [G loss: 1.883512]\n",
      "epoch:25 step:23845 [D loss: 0.627272, acc: 65.62%] [G loss: 1.832344]\n",
      "epoch:25 step:23846 [D loss: 0.704198, acc: 60.16%] [G loss: 1.886618]\n",
      "epoch:25 step:23847 [D loss: 0.675394, acc: 60.94%] [G loss: 1.829038]\n",
      "epoch:25 step:23848 [D loss: 0.606393, acc: 64.84%] [G loss: 1.838277]\n",
      "epoch:25 step:23849 [D loss: 0.724511, acc: 53.91%] [G loss: 1.802014]\n",
      "epoch:25 step:23850 [D loss: 0.633281, acc: 62.50%] [G loss: 1.892143]\n",
      "epoch:25 step:23851 [D loss: 0.668551, acc: 59.38%] [G loss: 1.819371]\n",
      "epoch:25 step:23852 [D loss: 0.598225, acc: 65.62%] [G loss: 1.903407]\n",
      "epoch:25 step:23853 [D loss: 0.630542, acc: 62.50%] [G loss: 2.054090]\n",
      "epoch:25 step:23854 [D loss: 0.625803, acc: 61.72%] [G loss: 2.056299]\n",
      "epoch:25 step:23855 [D loss: 0.574363, acc: 70.31%] [G loss: 2.257470]\n",
      "epoch:25 step:23856 [D loss: 0.572929, acc: 71.09%] [G loss: 1.972936]\n",
      "epoch:25 step:23857 [D loss: 0.685617, acc: 53.91%] [G loss: 1.850762]\n",
      "epoch:25 step:23858 [D loss: 0.673410, acc: 56.25%] [G loss: 1.732747]\n",
      "epoch:25 step:23859 [D loss: 0.638513, acc: 57.03%] [G loss: 1.973267]\n",
      "epoch:25 step:23860 [D loss: 0.590154, acc: 67.97%] [G loss: 2.010025]\n",
      "epoch:25 step:23861 [D loss: 0.649173, acc: 63.28%] [G loss: 1.880584]\n",
      "epoch:25 step:23862 [D loss: 0.743992, acc: 50.00%] [G loss: 1.702467]\n",
      "epoch:25 step:23863 [D loss: 0.673889, acc: 53.91%] [G loss: 1.713367]\n",
      "epoch:25 step:23864 [D loss: 0.654916, acc: 64.84%] [G loss: 1.805699]\n",
      "epoch:25 step:23865 [D loss: 0.691475, acc: 53.12%] [G loss: 1.714536]\n",
      "epoch:25 step:23866 [D loss: 0.625305, acc: 67.19%] [G loss: 1.743356]\n",
      "epoch:25 step:23867 [D loss: 0.723139, acc: 54.69%] [G loss: 1.719390]\n",
      "epoch:25 step:23868 [D loss: 0.716244, acc: 53.91%] [G loss: 1.753059]\n",
      "epoch:25 step:23869 [D loss: 0.661477, acc: 60.94%] [G loss: 1.774547]\n",
      "epoch:25 step:23870 [D loss: 0.617024, acc: 64.84%] [G loss: 1.756908]\n",
      "epoch:25 step:23871 [D loss: 0.707709, acc: 56.25%] [G loss: 1.733959]\n",
      "epoch:25 step:23872 [D loss: 0.687014, acc: 55.47%] [G loss: 1.731372]\n",
      "epoch:25 step:23873 [D loss: 0.663731, acc: 55.47%] [G loss: 1.755133]\n",
      "epoch:25 step:23874 [D loss: 0.707975, acc: 57.81%] [G loss: 1.700503]\n",
      "epoch:25 step:23875 [D loss: 0.674968, acc: 57.81%] [G loss: 1.833630]\n",
      "epoch:25 step:23876 [D loss: 0.632588, acc: 62.50%] [G loss: 1.714758]\n",
      "epoch:25 step:23877 [D loss: 0.609304, acc: 67.97%] [G loss: 1.759799]\n",
      "epoch:25 step:23878 [D loss: 0.574691, acc: 71.88%] [G loss: 1.859769]\n",
      "epoch:25 step:23879 [D loss: 0.630130, acc: 67.97%] [G loss: 1.770235]\n",
      "epoch:25 step:23880 [D loss: 0.661148, acc: 60.16%] [G loss: 1.894746]\n",
      "epoch:25 step:23881 [D loss: 0.628389, acc: 67.97%] [G loss: 1.915317]\n",
      "epoch:25 step:23882 [D loss: 0.635910, acc: 67.97%] [G loss: 1.955009]\n",
      "epoch:25 step:23883 [D loss: 0.630035, acc: 67.97%] [G loss: 1.758675]\n",
      "epoch:25 step:23884 [D loss: 0.654712, acc: 64.06%] [G loss: 1.741938]\n",
      "epoch:25 step:23885 [D loss: 0.700610, acc: 54.69%] [G loss: 1.742042]\n",
      "epoch:25 step:23886 [D loss: 0.627136, acc: 64.06%] [G loss: 1.853854]\n",
      "epoch:25 step:23887 [D loss: 0.630438, acc: 66.41%] [G loss: 1.688581]\n",
      "epoch:25 step:23888 [D loss: 0.672095, acc: 58.59%] [G loss: 1.850978]\n",
      "epoch:25 step:23889 [D loss: 0.663685, acc: 57.81%] [G loss: 1.846042]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:25 step:23890 [D loss: 0.613585, acc: 69.53%] [G loss: 1.866134]\n",
      "epoch:25 step:23891 [D loss: 0.660139, acc: 60.16%] [G loss: 1.835761]\n",
      "epoch:25 step:23892 [D loss: 0.661604, acc: 57.81%] [G loss: 1.811165]\n",
      "epoch:25 step:23893 [D loss: 0.691147, acc: 53.91%] [G loss: 1.856209]\n",
      "epoch:25 step:23894 [D loss: 0.606300, acc: 65.62%] [G loss: 2.055217]\n",
      "epoch:25 step:23895 [D loss: 0.651541, acc: 60.94%] [G loss: 2.034978]\n",
      "epoch:25 step:23896 [D loss: 0.597779, acc: 66.41%] [G loss: 2.217532]\n",
      "epoch:25 step:23897 [D loss: 0.609929, acc: 63.28%] [G loss: 2.076020]\n",
      "epoch:25 step:23898 [D loss: 0.635600, acc: 61.72%] [G loss: 1.829751]\n",
      "epoch:25 step:23899 [D loss: 0.662743, acc: 64.06%] [G loss: 1.817286]\n",
      "epoch:25 step:23900 [D loss: 0.665097, acc: 59.38%] [G loss: 1.820614]\n",
      "epoch:25 step:23901 [D loss: 0.644836, acc: 58.59%] [G loss: 1.988957]\n",
      "epoch:25 step:23902 [D loss: 0.752451, acc: 53.12%] [G loss: 1.729917]\n",
      "epoch:25 step:23903 [D loss: 0.671296, acc: 60.94%] [G loss: 1.870498]\n",
      "epoch:25 step:23904 [D loss: 0.589824, acc: 71.88%] [G loss: 2.000823]\n",
      "epoch:25 step:23905 [D loss: 0.625076, acc: 66.41%] [G loss: 2.057817]\n",
      "epoch:25 step:23906 [D loss: 0.574330, acc: 72.66%] [G loss: 2.031815]\n",
      "epoch:25 step:23907 [D loss: 0.685935, acc: 59.38%] [G loss: 1.821652]\n",
      "epoch:25 step:23908 [D loss: 0.670732, acc: 59.38%] [G loss: 1.777757]\n",
      "epoch:25 step:23909 [D loss: 0.685064, acc: 57.81%] [G loss: 1.906413]\n",
      "epoch:25 step:23910 [D loss: 0.623909, acc: 68.75%] [G loss: 1.834513]\n",
      "epoch:25 step:23911 [D loss: 0.650649, acc: 59.38%] [G loss: 1.821291]\n",
      "epoch:25 step:23912 [D loss: 0.649109, acc: 64.06%] [G loss: 1.885293]\n",
      "epoch:25 step:23913 [D loss: 0.615984, acc: 64.84%] [G loss: 2.064084]\n",
      "epoch:25 step:23914 [D loss: 0.664889, acc: 64.06%] [G loss: 1.880287]\n",
      "epoch:25 step:23915 [D loss: 0.602427, acc: 67.97%] [G loss: 1.858699]\n",
      "epoch:25 step:23916 [D loss: 0.621764, acc: 61.72%] [G loss: 1.896482]\n",
      "epoch:25 step:23917 [D loss: 0.624001, acc: 63.28%] [G loss: 1.792087]\n",
      "epoch:25 step:23918 [D loss: 0.635307, acc: 64.06%] [G loss: 1.849037]\n",
      "epoch:25 step:23919 [D loss: 0.653173, acc: 64.06%] [G loss: 1.983533]\n",
      "epoch:25 step:23920 [D loss: 0.568603, acc: 70.31%] [G loss: 2.013343]\n",
      "epoch:25 step:23921 [D loss: 0.637988, acc: 65.62%] [G loss: 1.852654]\n",
      "epoch:25 step:23922 [D loss: 0.656067, acc: 57.81%] [G loss: 1.960761]\n",
      "epoch:25 step:23923 [D loss: 0.593261, acc: 65.62%] [G loss: 2.306699]\n",
      "epoch:25 step:23924 [D loss: 0.554872, acc: 71.88%] [G loss: 2.273506]\n",
      "epoch:25 step:23925 [D loss: 0.670785, acc: 61.72%] [G loss: 1.905867]\n",
      "epoch:25 step:23926 [D loss: 0.734927, acc: 57.03%] [G loss: 1.712703]\n",
      "epoch:25 step:23927 [D loss: 0.738693, acc: 52.34%] [G loss: 1.851136]\n",
      "epoch:25 step:23928 [D loss: 0.653047, acc: 64.06%] [G loss: 1.812002]\n",
      "epoch:25 step:23929 [D loss: 0.632457, acc: 61.72%] [G loss: 1.866132]\n",
      "epoch:25 step:23930 [D loss: 0.661781, acc: 60.94%] [G loss: 1.853213]\n",
      "epoch:25 step:23931 [D loss: 0.668859, acc: 58.59%] [G loss: 1.801477]\n",
      "epoch:25 step:23932 [D loss: 0.608941, acc: 67.97%] [G loss: 1.983431]\n",
      "epoch:25 step:23933 [D loss: 0.661043, acc: 66.41%] [G loss: 1.978160]\n",
      "epoch:25 step:23934 [D loss: 0.634774, acc: 65.62%] [G loss: 1.879385]\n",
      "epoch:25 step:23935 [D loss: 0.743324, acc: 50.78%] [G loss: 1.851031]\n",
      "epoch:25 step:23936 [D loss: 0.648366, acc: 65.62%] [G loss: 1.821713]\n",
      "epoch:25 step:23937 [D loss: 0.635163, acc: 62.50%] [G loss: 2.029915]\n",
      "epoch:25 step:23938 [D loss: 0.646544, acc: 64.06%] [G loss: 1.968814]\n",
      "epoch:25 step:23939 [D loss: 0.646130, acc: 57.81%] [G loss: 1.790315]\n",
      "epoch:25 step:23940 [D loss: 0.594825, acc: 67.97%] [G loss: 1.846586]\n",
      "epoch:25 step:23941 [D loss: 0.556736, acc: 72.66%] [G loss: 2.171446]\n",
      "epoch:25 step:23942 [D loss: 0.652155, acc: 62.50%] [G loss: 1.956421]\n",
      "epoch:25 step:23943 [D loss: 0.632468, acc: 68.75%] [G loss: 1.823314]\n",
      "epoch:25 step:23944 [D loss: 0.639981, acc: 66.41%] [G loss: 1.915286]\n",
      "epoch:25 step:23945 [D loss: 0.589541, acc: 71.09%] [G loss: 1.994992]\n",
      "epoch:25 step:23946 [D loss: 0.651359, acc: 58.59%] [G loss: 1.981541]\n",
      "epoch:25 step:23947 [D loss: 0.600762, acc: 67.97%] [G loss: 1.994920]\n",
      "epoch:25 step:23948 [D loss: 0.593467, acc: 72.66%] [G loss: 2.175461]\n",
      "epoch:25 step:23949 [D loss: 0.666663, acc: 64.06%] [G loss: 1.896146]\n",
      "epoch:25 step:23950 [D loss: 0.673118, acc: 63.28%] [G loss: 1.949091]\n",
      "epoch:25 step:23951 [D loss: 0.650640, acc: 64.06%] [G loss: 1.736076]\n",
      "epoch:25 step:23952 [D loss: 0.679762, acc: 58.59%] [G loss: 1.848568]\n",
      "epoch:25 step:23953 [D loss: 0.686299, acc: 59.38%] [G loss: 1.646215]\n",
      "epoch:25 step:23954 [D loss: 0.664428, acc: 62.50%] [G loss: 1.701419]\n",
      "epoch:25 step:23955 [D loss: 0.666887, acc: 60.94%] [G loss: 1.754221]\n",
      "epoch:25 step:23956 [D loss: 0.691806, acc: 53.91%] [G loss: 1.799706]\n",
      "epoch:25 step:23957 [D loss: 0.619449, acc: 62.50%] [G loss: 1.966010]\n",
      "epoch:25 step:23958 [D loss: 0.625467, acc: 64.84%] [G loss: 1.834287]\n",
      "epoch:25 step:23959 [D loss: 0.602922, acc: 71.09%] [G loss: 1.999530]\n",
      "epoch:25 step:23960 [D loss: 0.654875, acc: 62.50%] [G loss: 1.805315]\n",
      "epoch:25 step:23961 [D loss: 0.599511, acc: 70.31%] [G loss: 1.933553]\n",
      "epoch:25 step:23962 [D loss: 0.678362, acc: 62.50%] [G loss: 1.803539]\n",
      "epoch:25 step:23963 [D loss: 0.698362, acc: 57.81%] [G loss: 1.696532]\n",
      "epoch:25 step:23964 [D loss: 0.649229, acc: 61.72%] [G loss: 1.875115]\n",
      "epoch:25 step:23965 [D loss: 0.675120, acc: 61.72%] [G loss: 1.764963]\n",
      "epoch:25 step:23966 [D loss: 0.699713, acc: 50.00%] [G loss: 1.738567]\n",
      "epoch:25 step:23967 [D loss: 0.647305, acc: 60.94%] [G loss: 1.824491]\n",
      "epoch:25 step:23968 [D loss: 0.664271, acc: 51.56%] [G loss: 1.871538]\n",
      "epoch:25 step:23969 [D loss: 0.629688, acc: 63.28%] [G loss: 1.871884]\n",
      "epoch:25 step:23970 [D loss: 0.659144, acc: 58.59%] [G loss: 1.937230]\n",
      "epoch:25 step:23971 [D loss: 0.628439, acc: 60.94%] [G loss: 1.921259]\n",
      "epoch:25 step:23972 [D loss: 0.633050, acc: 63.28%] [G loss: 1.742918]\n",
      "epoch:25 step:23973 [D loss: 0.616806, acc: 62.50%] [G loss: 1.955130]\n",
      "epoch:25 step:23974 [D loss: 0.612030, acc: 69.53%] [G loss: 2.115828]\n",
      "epoch:25 step:23975 [D loss: 0.635333, acc: 62.50%] [G loss: 1.883935]\n",
      "epoch:25 step:23976 [D loss: 0.586239, acc: 70.31%] [G loss: 2.041412]\n",
      "epoch:25 step:23977 [D loss: 0.620976, acc: 71.09%] [G loss: 2.060655]\n",
      "epoch:25 step:23978 [D loss: 0.628137, acc: 68.75%] [G loss: 1.918440]\n",
      "epoch:25 step:23979 [D loss: 0.633590, acc: 67.97%] [G loss: 2.139633]\n",
      "epoch:25 step:23980 [D loss: 0.595884, acc: 66.41%] [G loss: 1.983698]\n",
      "epoch:25 step:23981 [D loss: 0.587119, acc: 69.53%] [G loss: 2.076278]\n",
      "epoch:25 step:23982 [D loss: 0.591802, acc: 68.75%] [G loss: 2.022415]\n",
      "epoch:25 step:23983 [D loss: 0.614473, acc: 69.53%] [G loss: 2.254874]\n",
      "epoch:25 step:23984 [D loss: 0.706673, acc: 50.00%] [G loss: 1.820094]\n",
      "epoch:25 step:23985 [D loss: 0.692436, acc: 56.25%] [G loss: 1.754999]\n",
      "epoch:25 step:23986 [D loss: 0.660247, acc: 64.84%] [G loss: 1.966599]\n",
      "epoch:25 step:23987 [D loss: 0.656308, acc: 63.28%] [G loss: 1.977809]\n",
      "epoch:25 step:23988 [D loss: 0.620610, acc: 66.41%] [G loss: 2.083899]\n",
      "epoch:25 step:23989 [D loss: 0.586530, acc: 70.31%] [G loss: 2.114969]\n",
      "epoch:25 step:23990 [D loss: 0.710219, acc: 53.91%] [G loss: 1.869816]\n",
      "epoch:25 step:23991 [D loss: 0.705359, acc: 53.12%] [G loss: 1.702483]\n",
      "epoch:25 step:23992 [D loss: 0.689398, acc: 54.69%] [G loss: 1.949110]\n",
      "epoch:25 step:23993 [D loss: 0.700014, acc: 54.69%] [G loss: 1.809972]\n",
      "epoch:25 step:23994 [D loss: 0.613000, acc: 68.75%] [G loss: 1.861337]\n",
      "epoch:25 step:23995 [D loss: 0.632273, acc: 67.19%] [G loss: 1.943561]\n",
      "epoch:25 step:23996 [D loss: 0.613146, acc: 60.94%] [G loss: 1.941297]\n",
      "epoch:25 step:23997 [D loss: 0.722948, acc: 53.12%] [G loss: 1.875659]\n",
      "epoch:25 step:23998 [D loss: 0.693837, acc: 57.81%] [G loss: 1.791765]\n",
      "epoch:25 step:23999 [D loss: 0.575690, acc: 64.84%] [G loss: 1.995659]\n",
      "epoch:25 step:24000 [D loss: 0.663778, acc: 60.16%] [G loss: 1.822275]\n",
      "##############\n",
      "[2.57099747 1.49642871 6.23557285 4.62087461 3.55623002 5.60702371\n",
      " 4.36354172 4.73832045 4.42955621 3.64979215]\n",
      "##########\n",
      "epoch:25 step:24001 [D loss: 0.696058, acc: 53.91%] [G loss: 1.849715]\n",
      "epoch:25 step:24002 [D loss: 0.651730, acc: 64.06%] [G loss: 1.764398]\n",
      "epoch:25 step:24003 [D loss: 0.606801, acc: 67.19%] [G loss: 1.873120]\n",
      "epoch:25 step:24004 [D loss: 0.654676, acc: 59.38%] [G loss: 1.853458]\n",
      "epoch:25 step:24005 [D loss: 0.628786, acc: 64.06%] [G loss: 1.951079]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:25 step:24006 [D loss: 0.662510, acc: 66.41%] [G loss: 1.877360]\n",
      "epoch:25 step:24007 [D loss: 0.620455, acc: 64.84%] [G loss: 1.927575]\n",
      "epoch:25 step:24008 [D loss: 0.678081, acc: 57.03%] [G loss: 1.858680]\n",
      "epoch:25 step:24009 [D loss: 0.657533, acc: 61.72%] [G loss: 1.859508]\n",
      "epoch:25 step:24010 [D loss: 0.651747, acc: 61.72%] [G loss: 1.868992]\n",
      "epoch:25 step:24011 [D loss: 0.681077, acc: 58.59%] [G loss: 1.847909]\n",
      "epoch:25 step:24012 [D loss: 0.656731, acc: 62.50%] [G loss: 1.768557]\n",
      "epoch:25 step:24013 [D loss: 0.607388, acc: 66.41%] [G loss: 2.096334]\n",
      "epoch:25 step:24014 [D loss: 0.623744, acc: 68.75%] [G loss: 1.973849]\n",
      "epoch:25 step:24015 [D loss: 0.655562, acc: 59.38%] [G loss: 1.806943]\n",
      "epoch:25 step:24016 [D loss: 0.643137, acc: 61.72%] [G loss: 1.964537]\n",
      "epoch:25 step:24017 [D loss: 0.630914, acc: 64.84%] [G loss: 1.824777]\n",
      "epoch:25 step:24018 [D loss: 0.610068, acc: 67.97%] [G loss: 1.996943]\n",
      "epoch:25 step:24019 [D loss: 0.622114, acc: 66.41%] [G loss: 1.851440]\n",
      "epoch:25 step:24020 [D loss: 0.676316, acc: 60.94%] [G loss: 1.812149]\n",
      "epoch:25 step:24021 [D loss: 0.674121, acc: 63.28%] [G loss: 1.772286]\n",
      "epoch:25 step:24022 [D loss: 0.624163, acc: 63.28%] [G loss: 1.796631]\n",
      "epoch:25 step:24023 [D loss: 0.637952, acc: 62.50%] [G loss: 1.908987]\n",
      "epoch:25 step:24024 [D loss: 0.667054, acc: 59.38%] [G loss: 1.854879]\n",
      "epoch:25 step:24025 [D loss: 0.648575, acc: 61.72%] [G loss: 1.783094]\n",
      "epoch:25 step:24026 [D loss: 0.688370, acc: 59.38%] [G loss: 1.914679]\n",
      "epoch:25 step:24027 [D loss: 0.678447, acc: 58.59%] [G loss: 1.904118]\n",
      "epoch:25 step:24028 [D loss: 0.630745, acc: 64.84%] [G loss: 2.049190]\n",
      "epoch:25 step:24029 [D loss: 0.628393, acc: 67.19%] [G loss: 1.996910]\n",
      "epoch:25 step:24030 [D loss: 0.635738, acc: 63.28%] [G loss: 2.015513]\n",
      "epoch:25 step:24031 [D loss: 0.650763, acc: 60.94%] [G loss: 1.899655]\n",
      "epoch:25 step:24032 [D loss: 0.653687, acc: 55.47%] [G loss: 1.848651]\n",
      "epoch:25 step:24033 [D loss: 0.693630, acc: 56.25%] [G loss: 1.854227]\n",
      "epoch:25 step:24034 [D loss: 0.669696, acc: 57.81%] [G loss: 1.788670]\n",
      "epoch:25 step:24035 [D loss: 0.702279, acc: 51.56%] [G loss: 1.834034]\n",
      "epoch:25 step:24036 [D loss: 0.667675, acc: 56.25%] [G loss: 1.861780]\n",
      "epoch:25 step:24037 [D loss: 0.614496, acc: 62.50%] [G loss: 1.803268]\n",
      "epoch:25 step:24038 [D loss: 0.649387, acc: 56.25%] [G loss: 2.025786]\n",
      "epoch:25 step:24039 [D loss: 0.709409, acc: 51.56%] [G loss: 1.645348]\n",
      "epoch:25 step:24040 [D loss: 0.709441, acc: 53.12%] [G loss: 1.808625]\n",
      "epoch:25 step:24041 [D loss: 0.724971, acc: 51.56%] [G loss: 1.675344]\n",
      "epoch:25 step:24042 [D loss: 0.650709, acc: 57.03%] [G loss: 1.907570]\n",
      "epoch:25 step:24043 [D loss: 0.603796, acc: 71.09%] [G loss: 1.765302]\n",
      "epoch:25 step:24044 [D loss: 0.627950, acc: 66.41%] [G loss: 1.936038]\n",
      "epoch:25 step:24045 [D loss: 0.646306, acc: 64.06%] [G loss: 1.903692]\n",
      "epoch:25 step:24046 [D loss: 0.672108, acc: 62.50%] [G loss: 1.847831]\n",
      "epoch:25 step:24047 [D loss: 0.611101, acc: 65.62%] [G loss: 1.886819]\n",
      "epoch:25 step:24048 [D loss: 0.687116, acc: 57.03%] [G loss: 1.869353]\n",
      "epoch:25 step:24049 [D loss: 0.620866, acc: 66.41%] [G loss: 2.059096]\n",
      "epoch:25 step:24050 [D loss: 0.655287, acc: 62.50%] [G loss: 1.818898]\n",
      "epoch:25 step:24051 [D loss: 0.695781, acc: 57.81%] [G loss: 1.808122]\n",
      "epoch:25 step:24052 [D loss: 0.681257, acc: 58.59%] [G loss: 1.837849]\n",
      "epoch:25 step:24053 [D loss: 0.683201, acc: 58.59%] [G loss: 1.735935]\n",
      "epoch:25 step:24054 [D loss: 0.634321, acc: 64.84%] [G loss: 1.835062]\n",
      "epoch:25 step:24055 [D loss: 0.625852, acc: 63.28%] [G loss: 1.858347]\n",
      "epoch:25 step:24056 [D loss: 0.640329, acc: 67.19%] [G loss: 1.988238]\n",
      "epoch:25 step:24057 [D loss: 0.681360, acc: 62.50%] [G loss: 1.924121]\n",
      "epoch:25 step:24058 [D loss: 0.664697, acc: 60.16%] [G loss: 1.883033]\n",
      "epoch:25 step:24059 [D loss: 0.630298, acc: 66.41%] [G loss: 1.932336]\n",
      "epoch:25 step:24060 [D loss: 0.670310, acc: 62.50%] [G loss: 1.878429]\n",
      "epoch:25 step:24061 [D loss: 0.670460, acc: 64.06%] [G loss: 1.785445]\n",
      "epoch:25 step:24062 [D loss: 0.614485, acc: 67.97%] [G loss: 1.864621]\n",
      "epoch:25 step:24063 [D loss: 0.630770, acc: 67.19%] [G loss: 1.838021]\n",
      "epoch:25 step:24064 [D loss: 0.631940, acc: 64.84%] [G loss: 1.935951]\n",
      "epoch:25 step:24065 [D loss: 0.642073, acc: 63.28%] [G loss: 1.956437]\n",
      "epoch:25 step:24066 [D loss: 0.610585, acc: 69.53%] [G loss: 1.848226]\n",
      "epoch:25 step:24067 [D loss: 0.620544, acc: 64.06%] [G loss: 1.932211]\n",
      "epoch:25 step:24068 [D loss: 0.602096, acc: 70.31%] [G loss: 2.011028]\n",
      "epoch:25 step:24069 [D loss: 0.658652, acc: 60.94%] [G loss: 2.092935]\n",
      "epoch:25 step:24070 [D loss: 0.660600, acc: 61.72%] [G loss: 2.005315]\n",
      "epoch:25 step:24071 [D loss: 0.629387, acc: 63.28%] [G loss: 2.025757]\n",
      "epoch:25 step:24072 [D loss: 0.618336, acc: 64.84%] [G loss: 2.062245]\n",
      "epoch:25 step:24073 [D loss: 0.552145, acc: 72.66%] [G loss: 2.415528]\n",
      "epoch:25 step:24074 [D loss: 0.675499, acc: 71.09%] [G loss: 2.203579]\n",
      "epoch:25 step:24075 [D loss: 0.635960, acc: 62.50%] [G loss: 2.146260]\n",
      "epoch:25 step:24076 [D loss: 0.610900, acc: 66.41%] [G loss: 1.928867]\n",
      "epoch:25 step:24077 [D loss: 0.606628, acc: 67.97%] [G loss: 1.771551]\n",
      "epoch:25 step:24078 [D loss: 0.625264, acc: 60.94%] [G loss: 2.009530]\n",
      "epoch:25 step:24079 [D loss: 0.621619, acc: 68.75%] [G loss: 2.064399]\n",
      "epoch:25 step:24080 [D loss: 0.652577, acc: 64.06%] [G loss: 1.913518]\n",
      "epoch:25 step:24081 [D loss: 0.721430, acc: 57.03%] [G loss: 1.856691]\n",
      "epoch:25 step:24082 [D loss: 0.729141, acc: 52.34%] [G loss: 1.612739]\n",
      "epoch:25 step:24083 [D loss: 0.688431, acc: 56.25%] [G loss: 1.688310]\n",
      "epoch:25 step:24084 [D loss: 0.651261, acc: 59.38%] [G loss: 1.813828]\n",
      "epoch:25 step:24085 [D loss: 0.631460, acc: 67.19%] [G loss: 1.881318]\n",
      "epoch:25 step:24086 [D loss: 0.660477, acc: 59.38%] [G loss: 1.859896]\n",
      "epoch:25 step:24087 [D loss: 0.605529, acc: 70.31%] [G loss: 1.937457]\n",
      "epoch:25 step:24088 [D loss: 0.646672, acc: 60.94%] [G loss: 1.850699]\n",
      "epoch:25 step:24089 [D loss: 0.603181, acc: 64.84%] [G loss: 1.976229]\n",
      "epoch:25 step:24090 [D loss: 0.639540, acc: 63.28%] [G loss: 1.771603]\n",
      "epoch:25 step:24091 [D loss: 0.668531, acc: 64.06%] [G loss: 1.853207]\n",
      "epoch:25 step:24092 [D loss: 0.706835, acc: 49.22%] [G loss: 1.670456]\n",
      "epoch:25 step:24093 [D loss: 0.616897, acc: 66.41%] [G loss: 1.840261]\n",
      "epoch:25 step:24094 [D loss: 0.628663, acc: 62.50%] [G loss: 1.824809]\n",
      "epoch:25 step:24095 [D loss: 0.624555, acc: 65.62%] [G loss: 1.781486]\n",
      "epoch:25 step:24096 [D loss: 0.684350, acc: 56.25%] [G loss: 1.835523]\n",
      "epoch:25 step:24097 [D loss: 0.605313, acc: 65.62%] [G loss: 1.880047]\n",
      "epoch:25 step:24098 [D loss: 0.648990, acc: 62.50%] [G loss: 1.821958]\n",
      "epoch:25 step:24099 [D loss: 0.634085, acc: 63.28%] [G loss: 1.891998]\n",
      "epoch:25 step:24100 [D loss: 0.665427, acc: 61.72%] [G loss: 1.802269]\n",
      "epoch:25 step:24101 [D loss: 0.643530, acc: 64.84%] [G loss: 1.981215]\n",
      "epoch:25 step:24102 [D loss: 0.671850, acc: 64.06%] [G loss: 1.967730]\n",
      "epoch:25 step:24103 [D loss: 0.628119, acc: 65.62%] [G loss: 1.905585]\n",
      "epoch:25 step:24104 [D loss: 0.672757, acc: 61.72%] [G loss: 1.886648]\n",
      "epoch:25 step:24105 [D loss: 0.596717, acc: 71.09%] [G loss: 1.922296]\n",
      "epoch:25 step:24106 [D loss: 0.578308, acc: 72.66%] [G loss: 2.018397]\n",
      "epoch:25 step:24107 [D loss: 0.655130, acc: 62.50%] [G loss: 1.904776]\n",
      "epoch:25 step:24108 [D loss: 0.684575, acc: 60.94%] [G loss: 1.764501]\n",
      "epoch:25 step:24109 [D loss: 0.654936, acc: 63.28%] [G loss: 1.814274]\n",
      "epoch:25 step:24110 [D loss: 0.618053, acc: 66.41%] [G loss: 1.877880]\n",
      "epoch:25 step:24111 [D loss: 0.615008, acc: 70.31%] [G loss: 2.015253]\n",
      "epoch:25 step:24112 [D loss: 0.657104, acc: 60.94%] [G loss: 2.016978]\n",
      "epoch:25 step:24113 [D loss: 0.573188, acc: 70.31%] [G loss: 2.007215]\n",
      "epoch:25 step:24114 [D loss: 0.662460, acc: 60.16%] [G loss: 2.120483]\n",
      "epoch:25 step:24115 [D loss: 0.604297, acc: 65.62%] [G loss: 1.963861]\n",
      "epoch:25 step:24116 [D loss: 0.619474, acc: 63.28%] [G loss: 2.014033]\n",
      "epoch:25 step:24117 [D loss: 0.668087, acc: 55.47%] [G loss: 1.898787]\n",
      "epoch:25 step:24118 [D loss: 0.608316, acc: 65.62%] [G loss: 2.150161]\n",
      "epoch:25 step:24119 [D loss: 0.671203, acc: 62.50%] [G loss: 2.155676]\n",
      "epoch:25 step:24120 [D loss: 0.613457, acc: 66.41%] [G loss: 2.077527]\n",
      "epoch:25 step:24121 [D loss: 0.704356, acc: 53.12%] [G loss: 1.867203]\n",
      "epoch:25 step:24122 [D loss: 0.650954, acc: 66.41%] [G loss: 1.872912]\n",
      "epoch:25 step:24123 [D loss: 0.647671, acc: 60.16%] [G loss: 1.813834]\n",
      "epoch:25 step:24124 [D loss: 0.600983, acc: 68.75%] [G loss: 1.962292]\n",
      "epoch:25 step:24125 [D loss: 0.633746, acc: 64.84%] [G loss: 1.875530]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:25 step:24126 [D loss: 0.593183, acc: 69.53%] [G loss: 1.942446]\n",
      "epoch:25 step:24127 [D loss: 0.663851, acc: 65.62%] [G loss: 1.839977]\n",
      "epoch:25 step:24128 [D loss: 0.679236, acc: 61.72%] [G loss: 1.834286]\n",
      "epoch:25 step:24129 [D loss: 0.658028, acc: 57.03%] [G loss: 1.785539]\n",
      "epoch:25 step:24130 [D loss: 0.718222, acc: 57.81%] [G loss: 1.761151]\n",
      "epoch:25 step:24131 [D loss: 0.621056, acc: 67.19%] [G loss: 1.902467]\n",
      "epoch:25 step:24132 [D loss: 0.624174, acc: 64.06%] [G loss: 1.965585]\n",
      "epoch:25 step:24133 [D loss: 0.671913, acc: 61.72%] [G loss: 1.993355]\n",
      "epoch:25 step:24134 [D loss: 0.665638, acc: 64.06%] [G loss: 1.747878]\n",
      "epoch:25 step:24135 [D loss: 0.669979, acc: 60.16%] [G loss: 1.963005]\n",
      "epoch:25 step:24136 [D loss: 0.649022, acc: 64.84%] [G loss: 2.010798]\n",
      "epoch:25 step:24137 [D loss: 0.668135, acc: 58.59%] [G loss: 1.930016]\n",
      "epoch:25 step:24138 [D loss: 0.724826, acc: 53.91%] [G loss: 1.764455]\n",
      "epoch:25 step:24139 [D loss: 0.672860, acc: 61.72%] [G loss: 1.952178]\n",
      "epoch:25 step:24140 [D loss: 0.670517, acc: 58.59%] [G loss: 1.924782]\n",
      "epoch:25 step:24141 [D loss: 0.716283, acc: 50.78%] [G loss: 1.716963]\n",
      "epoch:25 step:24142 [D loss: 0.698029, acc: 57.81%] [G loss: 1.883355]\n",
      "epoch:25 step:24143 [D loss: 0.686910, acc: 60.94%] [G loss: 1.852119]\n",
      "epoch:25 step:24144 [D loss: 0.588463, acc: 71.09%] [G loss: 2.014903]\n",
      "epoch:25 step:24145 [D loss: 0.652423, acc: 60.94%] [G loss: 2.078673]\n",
      "epoch:25 step:24146 [D loss: 0.642261, acc: 62.50%] [G loss: 1.850499]\n",
      "epoch:25 step:24147 [D loss: 0.679896, acc: 61.72%] [G loss: 1.831368]\n",
      "epoch:25 step:24148 [D loss: 0.690032, acc: 52.34%] [G loss: 1.718206]\n",
      "epoch:25 step:24149 [D loss: 0.653647, acc: 60.94%] [G loss: 1.846295]\n",
      "epoch:25 step:24150 [D loss: 0.654019, acc: 60.16%] [G loss: 1.920748]\n",
      "epoch:25 step:24151 [D loss: 0.614596, acc: 69.53%] [G loss: 1.796818]\n",
      "epoch:25 step:24152 [D loss: 0.679112, acc: 61.72%] [G loss: 1.757862]\n",
      "epoch:25 step:24153 [D loss: 0.636216, acc: 65.62%] [G loss: 1.751335]\n",
      "epoch:25 step:24154 [D loss: 0.676830, acc: 57.81%] [G loss: 1.753367]\n",
      "epoch:25 step:24155 [D loss: 0.665207, acc: 59.38%] [G loss: 1.702268]\n",
      "epoch:25 step:24156 [D loss: 0.648993, acc: 59.38%] [G loss: 1.693745]\n",
      "epoch:25 step:24157 [D loss: 0.648510, acc: 61.72%] [G loss: 1.785317]\n",
      "epoch:25 step:24158 [D loss: 0.633048, acc: 60.16%] [G loss: 1.700693]\n",
      "epoch:25 step:24159 [D loss: 0.677383, acc: 60.16%] [G loss: 1.923808]\n",
      "epoch:25 step:24160 [D loss: 0.639170, acc: 61.72%] [G loss: 1.915037]\n",
      "epoch:25 step:24161 [D loss: 0.644142, acc: 64.06%] [G loss: 1.864668]\n",
      "epoch:25 step:24162 [D loss: 0.686390, acc: 57.03%] [G loss: 1.728058]\n",
      "epoch:25 step:24163 [D loss: 0.618269, acc: 67.97%] [G loss: 1.793644]\n",
      "epoch:25 step:24164 [D loss: 0.671768, acc: 62.50%] [G loss: 1.880695]\n",
      "epoch:25 step:24165 [D loss: 0.657346, acc: 59.38%] [G loss: 1.767697]\n",
      "epoch:25 step:24166 [D loss: 0.638965, acc: 61.72%] [G loss: 1.695621]\n",
      "epoch:25 step:24167 [D loss: 0.690273, acc: 54.69%] [G loss: 1.792699]\n",
      "epoch:25 step:24168 [D loss: 0.643104, acc: 58.59%] [G loss: 1.831897]\n",
      "epoch:25 step:24169 [D loss: 0.680383, acc: 56.25%] [G loss: 1.806146]\n",
      "epoch:25 step:24170 [D loss: 0.637643, acc: 62.50%] [G loss: 1.859873]\n",
      "epoch:25 step:24171 [D loss: 0.639575, acc: 64.84%] [G loss: 1.862249]\n",
      "epoch:25 step:24172 [D loss: 0.615344, acc: 69.53%] [G loss: 1.861082]\n",
      "epoch:25 step:24173 [D loss: 0.632527, acc: 63.28%] [G loss: 1.891670]\n",
      "epoch:25 step:24174 [D loss: 0.676955, acc: 53.12%] [G loss: 1.809489]\n",
      "epoch:25 step:24175 [D loss: 0.622395, acc: 67.19%] [G loss: 1.763224]\n",
      "epoch:25 step:24176 [D loss: 0.639321, acc: 61.72%] [G loss: 1.756139]\n",
      "epoch:25 step:24177 [D loss: 0.703050, acc: 53.91%] [G loss: 1.743817]\n",
      "epoch:25 step:24178 [D loss: 0.646539, acc: 61.72%] [G loss: 1.837916]\n",
      "epoch:25 step:24179 [D loss: 0.667638, acc: 58.59%] [G loss: 1.702779]\n",
      "epoch:25 step:24180 [D loss: 0.603459, acc: 67.97%] [G loss: 1.841061]\n",
      "epoch:25 step:24181 [D loss: 0.684126, acc: 58.59%] [G loss: 1.722157]\n",
      "epoch:25 step:24182 [D loss: 0.666207, acc: 60.94%] [G loss: 1.694402]\n",
      "epoch:25 step:24183 [D loss: 0.735087, acc: 52.34%] [G loss: 1.782831]\n",
      "epoch:25 step:24184 [D loss: 0.693872, acc: 57.03%] [G loss: 1.657136]\n",
      "epoch:25 step:24185 [D loss: 0.602679, acc: 67.97%] [G loss: 1.814864]\n",
      "epoch:25 step:24186 [D loss: 0.634417, acc: 64.06%] [G loss: 1.740234]\n",
      "epoch:25 step:24187 [D loss: 0.626929, acc: 63.28%] [G loss: 1.704012]\n",
      "epoch:25 step:24188 [D loss: 0.670859, acc: 60.16%] [G loss: 1.821488]\n",
      "epoch:25 step:24189 [D loss: 0.624917, acc: 64.06%] [G loss: 1.838018]\n",
      "epoch:25 step:24190 [D loss: 0.653094, acc: 60.94%] [G loss: 1.778761]\n",
      "epoch:25 step:24191 [D loss: 0.651454, acc: 63.28%] [G loss: 1.940612]\n",
      "epoch:25 step:24192 [D loss: 0.657837, acc: 57.81%] [G loss: 1.745032]\n",
      "epoch:25 step:24193 [D loss: 0.633011, acc: 64.84%] [G loss: 1.901351]\n",
      "epoch:25 step:24194 [D loss: 0.651586, acc: 55.47%] [G loss: 1.732169]\n",
      "epoch:25 step:24195 [D loss: 0.685702, acc: 57.03%] [G loss: 1.771875]\n",
      "epoch:25 step:24196 [D loss: 0.710751, acc: 57.03%] [G loss: 1.834195]\n",
      "epoch:25 step:24197 [D loss: 0.660431, acc: 60.16%] [G loss: 1.764241]\n",
      "epoch:25 step:24198 [D loss: 0.615568, acc: 67.97%] [G loss: 1.906828]\n",
      "epoch:25 step:24199 [D loss: 0.649813, acc: 66.41%] [G loss: 2.054708]\n",
      "epoch:25 step:24200 [D loss: 0.596392, acc: 69.53%] [G loss: 2.085856]\n",
      "##############\n",
      "[2.35446263 1.43138836 6.33521751 4.66551392 3.44584864 5.21540342\n",
      " 4.39304584 4.65183229 4.47520775 3.59367434]\n",
      "##########\n",
      "epoch:25 step:24201 [D loss: 0.655460, acc: 61.72%] [G loss: 1.979050]\n",
      "epoch:25 step:24202 [D loss: 0.721326, acc: 55.47%] [G loss: 1.900771]\n",
      "epoch:25 step:24203 [D loss: 0.611582, acc: 64.84%] [G loss: 1.823768]\n",
      "epoch:25 step:24204 [D loss: 0.670805, acc: 57.81%] [G loss: 1.850056]\n",
      "epoch:25 step:24205 [D loss: 0.630179, acc: 65.62%] [G loss: 1.917022]\n",
      "epoch:25 step:24206 [D loss: 0.614166, acc: 67.19%] [G loss: 2.010975]\n",
      "epoch:25 step:24207 [D loss: 0.597110, acc: 66.41%] [G loss: 1.874738]\n",
      "epoch:25 step:24208 [D loss: 0.694372, acc: 53.91%] [G loss: 1.925386]\n",
      "epoch:25 step:24209 [D loss: 0.693959, acc: 54.69%] [G loss: 1.718141]\n",
      "epoch:25 step:24210 [D loss: 0.661144, acc: 57.81%] [G loss: 1.869374]\n",
      "epoch:25 step:24211 [D loss: 0.591999, acc: 71.88%] [G loss: 1.837233]\n",
      "epoch:25 step:24212 [D loss: 0.670272, acc: 59.38%] [G loss: 1.819381]\n",
      "epoch:25 step:24213 [D loss: 0.734900, acc: 50.00%] [G loss: 1.751827]\n",
      "epoch:25 step:24214 [D loss: 0.705186, acc: 54.69%] [G loss: 1.860210]\n",
      "epoch:25 step:24215 [D loss: 0.657236, acc: 59.38%] [G loss: 1.847401]\n",
      "epoch:25 step:24216 [D loss: 0.638013, acc: 64.06%] [G loss: 1.808035]\n",
      "epoch:25 step:24217 [D loss: 0.630042, acc: 64.06%] [G loss: 1.933015]\n",
      "epoch:25 step:24218 [D loss: 0.627416, acc: 60.16%] [G loss: 1.921080]\n",
      "epoch:25 step:24219 [D loss: 0.753521, acc: 45.31%] [G loss: 1.562248]\n",
      "epoch:25 step:24220 [D loss: 0.660132, acc: 57.81%] [G loss: 1.718171]\n",
      "epoch:25 step:24221 [D loss: 0.683265, acc: 50.00%] [G loss: 1.781443]\n",
      "epoch:25 step:24222 [D loss: 0.709705, acc: 53.91%] [G loss: 1.692080]\n",
      "epoch:25 step:24223 [D loss: 0.677099, acc: 58.59%] [G loss: 1.815728]\n",
      "epoch:25 step:24224 [D loss: 0.671350, acc: 53.91%] [G loss: 1.740624]\n",
      "epoch:25 step:24225 [D loss: 0.680764, acc: 58.59%] [G loss: 1.641370]\n",
      "epoch:25 step:24226 [D loss: 0.679408, acc: 57.03%] [G loss: 1.777860]\n",
      "epoch:25 step:24227 [D loss: 0.654915, acc: 61.72%] [G loss: 1.820376]\n",
      "epoch:25 step:24228 [D loss: 0.659839, acc: 63.28%] [G loss: 1.783806]\n",
      "epoch:25 step:24229 [D loss: 0.664257, acc: 63.28%] [G loss: 1.801624]\n",
      "epoch:25 step:24230 [D loss: 0.605023, acc: 63.28%] [G loss: 1.853186]\n",
      "epoch:25 step:24231 [D loss: 0.675908, acc: 58.59%] [G loss: 1.761911]\n",
      "epoch:25 step:24232 [D loss: 0.638542, acc: 63.28%] [G loss: 1.812684]\n",
      "epoch:25 step:24233 [D loss: 0.674229, acc: 55.47%] [G loss: 1.798550]\n",
      "epoch:25 step:24234 [D loss: 0.662385, acc: 60.16%] [G loss: 1.827237]\n",
      "epoch:25 step:24235 [D loss: 0.670639, acc: 60.16%] [G loss: 1.833486]\n",
      "epoch:25 step:24236 [D loss: 0.644611, acc: 59.38%] [G loss: 1.905553]\n",
      "epoch:25 step:24237 [D loss: 0.614620, acc: 67.19%] [G loss: 1.663601]\n",
      "epoch:25 step:24238 [D loss: 0.639765, acc: 64.06%] [G loss: 1.867207]\n",
      "epoch:25 step:24239 [D loss: 0.676478, acc: 60.16%] [G loss: 1.781585]\n",
      "epoch:25 step:24240 [D loss: 0.619273, acc: 64.06%] [G loss: 1.971978]\n",
      "epoch:25 step:24241 [D loss: 0.654145, acc: 59.38%] [G loss: 1.957412]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:25 step:24242 [D loss: 0.666145, acc: 60.94%] [G loss: 1.756370]\n",
      "epoch:25 step:24243 [D loss: 0.661726, acc: 59.38%] [G loss: 1.785396]\n",
      "epoch:25 step:24244 [D loss: 0.635419, acc: 67.97%] [G loss: 1.904109]\n",
      "epoch:25 step:24245 [D loss: 0.705051, acc: 54.69%] [G loss: 1.636103]\n",
      "epoch:25 step:24246 [D loss: 0.709197, acc: 50.00%] [G loss: 1.733696]\n",
      "epoch:25 step:24247 [D loss: 0.621830, acc: 64.84%] [G loss: 1.833916]\n",
      "epoch:25 step:24248 [D loss: 0.641019, acc: 64.84%] [G loss: 1.849031]\n",
      "epoch:25 step:24249 [D loss: 0.680127, acc: 57.81%] [G loss: 1.851601]\n",
      "epoch:25 step:24250 [D loss: 0.613756, acc: 67.97%] [G loss: 2.035130]\n",
      "epoch:25 step:24251 [D loss: 0.654112, acc: 60.16%] [G loss: 1.760270]\n",
      "epoch:25 step:24252 [D loss: 0.660143, acc: 57.03%] [G loss: 1.860786]\n",
      "epoch:25 step:24253 [D loss: 0.642312, acc: 57.81%] [G loss: 1.709810]\n",
      "epoch:25 step:24254 [D loss: 0.680456, acc: 60.16%] [G loss: 1.729956]\n",
      "epoch:25 step:24255 [D loss: 0.620208, acc: 67.97%] [G loss: 1.798621]\n",
      "epoch:25 step:24256 [D loss: 0.700916, acc: 57.03%] [G loss: 1.804247]\n",
      "epoch:25 step:24257 [D loss: 0.616127, acc: 67.97%] [G loss: 1.801004]\n",
      "epoch:25 step:24258 [D loss: 0.631875, acc: 67.19%] [G loss: 1.849541]\n",
      "epoch:25 step:24259 [D loss: 0.661031, acc: 64.84%] [G loss: 1.812288]\n",
      "epoch:25 step:24260 [D loss: 0.605957, acc: 67.97%] [G loss: 1.853669]\n",
      "epoch:25 step:24261 [D loss: 0.668029, acc: 62.50%] [G loss: 1.748160]\n",
      "epoch:25 step:24262 [D loss: 0.651161, acc: 58.59%] [G loss: 1.892753]\n",
      "epoch:25 step:24263 [D loss: 0.641225, acc: 64.84%] [G loss: 1.875976]\n",
      "epoch:25 step:24264 [D loss: 0.649506, acc: 65.62%] [G loss: 1.841775]\n",
      "epoch:25 step:24265 [D loss: 0.625643, acc: 62.50%] [G loss: 1.883565]\n",
      "epoch:25 step:24266 [D loss: 0.643539, acc: 56.25%] [G loss: 1.837069]\n",
      "epoch:25 step:24267 [D loss: 0.615756, acc: 61.72%] [G loss: 1.939342]\n",
      "epoch:25 step:24268 [D loss: 0.679584, acc: 57.03%] [G loss: 1.735791]\n",
      "epoch:25 step:24269 [D loss: 0.674758, acc: 64.06%] [G loss: 1.959739]\n",
      "epoch:25 step:24270 [D loss: 0.631976, acc: 70.31%] [G loss: 1.821962]\n",
      "epoch:25 step:24271 [D loss: 0.593179, acc: 67.97%] [G loss: 1.783033]\n",
      "epoch:25 step:24272 [D loss: 0.643772, acc: 65.62%] [G loss: 2.004642]\n",
      "epoch:25 step:24273 [D loss: 0.635570, acc: 64.06%] [G loss: 1.967814]\n",
      "epoch:25 step:24274 [D loss: 0.599479, acc: 67.19%] [G loss: 2.018210]\n",
      "epoch:25 step:24275 [D loss: 0.663319, acc: 60.94%] [G loss: 1.785997]\n",
      "epoch:25 step:24276 [D loss: 0.666058, acc: 62.50%] [G loss: 1.824587]\n",
      "epoch:25 step:24277 [D loss: 0.696800, acc: 59.38%] [G loss: 1.873029]\n",
      "epoch:25 step:24278 [D loss: 0.669923, acc: 58.59%] [G loss: 1.877917]\n",
      "epoch:25 step:24279 [D loss: 0.603117, acc: 64.84%] [G loss: 1.837366]\n",
      "epoch:25 step:24280 [D loss: 0.642038, acc: 63.28%] [G loss: 1.780475]\n",
      "epoch:25 step:24281 [D loss: 0.674095, acc: 60.16%] [G loss: 1.720120]\n",
      "epoch:25 step:24282 [D loss: 0.627967, acc: 64.06%] [G loss: 1.765902]\n",
      "epoch:25 step:24283 [D loss: 0.712981, acc: 48.44%] [G loss: 1.855865]\n",
      "epoch:25 step:24284 [D loss: 0.654162, acc: 62.50%] [G loss: 1.795638]\n",
      "epoch:25 step:24285 [D loss: 0.625965, acc: 65.62%] [G loss: 1.803384]\n",
      "epoch:25 step:24286 [D loss: 0.686985, acc: 57.81%] [G loss: 1.751909]\n",
      "epoch:25 step:24287 [D loss: 0.679711, acc: 57.03%] [G loss: 1.815709]\n",
      "epoch:25 step:24288 [D loss: 0.653834, acc: 60.16%] [G loss: 1.834291]\n",
      "epoch:25 step:24289 [D loss: 0.622904, acc: 64.06%] [G loss: 1.805225]\n",
      "epoch:25 step:24290 [D loss: 0.639642, acc: 63.28%] [G loss: 1.799601]\n",
      "epoch:25 step:24291 [D loss: 0.622956, acc: 67.97%] [G loss: 1.812479]\n",
      "epoch:25 step:24292 [D loss: 0.604461, acc: 70.31%] [G loss: 1.805934]\n",
      "epoch:25 step:24293 [D loss: 0.632602, acc: 60.94%] [G loss: 1.806962]\n",
      "epoch:25 step:24294 [D loss: 0.656819, acc: 55.47%] [G loss: 1.738291]\n",
      "epoch:25 step:24295 [D loss: 0.675856, acc: 58.59%] [G loss: 1.796572]\n",
      "epoch:25 step:24296 [D loss: 0.660626, acc: 57.03%] [G loss: 1.737276]\n",
      "epoch:25 step:24297 [D loss: 0.603649, acc: 66.41%] [G loss: 1.812888]\n",
      "epoch:25 step:24298 [D loss: 0.601928, acc: 67.19%] [G loss: 1.856576]\n",
      "epoch:25 step:24299 [D loss: 0.673446, acc: 55.47%] [G loss: 1.754779]\n",
      "epoch:25 step:24300 [D loss: 0.640100, acc: 60.16%] [G loss: 1.916366]\n",
      "epoch:25 step:24301 [D loss: 0.647315, acc: 65.62%] [G loss: 1.896811]\n",
      "epoch:25 step:24302 [D loss: 0.678489, acc: 57.81%] [G loss: 1.899249]\n",
      "epoch:25 step:24303 [D loss: 0.662152, acc: 58.59%] [G loss: 1.898537]\n",
      "epoch:25 step:24304 [D loss: 0.676023, acc: 57.81%] [G loss: 1.872326]\n",
      "epoch:25 step:24305 [D loss: 0.665921, acc: 52.34%] [G loss: 1.837576]\n",
      "epoch:25 step:24306 [D loss: 0.647675, acc: 57.81%] [G loss: 1.795191]\n",
      "epoch:25 step:24307 [D loss: 0.621809, acc: 65.62%] [G loss: 1.800474]\n",
      "epoch:25 step:24308 [D loss: 0.652496, acc: 60.16%] [G loss: 1.837969]\n",
      "epoch:25 step:24309 [D loss: 0.641582, acc: 61.72%] [G loss: 2.031109]\n",
      "epoch:25 step:24310 [D loss: 0.621976, acc: 60.16%] [G loss: 1.896432]\n",
      "epoch:25 step:24311 [D loss: 0.623140, acc: 62.50%] [G loss: 1.942605]\n",
      "epoch:25 step:24312 [D loss: 0.641209, acc: 57.81%] [G loss: 1.958360]\n",
      "epoch:25 step:24313 [D loss: 0.634386, acc: 68.75%] [G loss: 1.878553]\n",
      "epoch:25 step:24314 [D loss: 0.629081, acc: 67.97%] [G loss: 1.891901]\n",
      "epoch:25 step:24315 [D loss: 0.607566, acc: 69.53%] [G loss: 1.969381]\n",
      "epoch:25 step:24316 [D loss: 0.685069, acc: 51.56%] [G loss: 1.785588]\n",
      "epoch:25 step:24317 [D loss: 0.669871, acc: 59.38%] [G loss: 1.839570]\n",
      "epoch:25 step:24318 [D loss: 0.650707, acc: 62.50%] [G loss: 1.841106]\n",
      "epoch:25 step:24319 [D loss: 0.658704, acc: 65.62%] [G loss: 1.862600]\n",
      "epoch:25 step:24320 [D loss: 0.656411, acc: 64.84%] [G loss: 1.910322]\n",
      "epoch:25 step:24321 [D loss: 0.695829, acc: 53.91%] [G loss: 1.788909]\n",
      "epoch:25 step:24322 [D loss: 0.677003, acc: 57.03%] [G loss: 1.763324]\n",
      "epoch:25 step:24323 [D loss: 0.688253, acc: 58.59%] [G loss: 1.722318]\n",
      "epoch:25 step:24324 [D loss: 0.642686, acc: 59.38%] [G loss: 1.837324]\n",
      "epoch:25 step:24325 [D loss: 0.598609, acc: 68.75%] [G loss: 1.938616]\n",
      "epoch:25 step:24326 [D loss: 0.652266, acc: 61.72%] [G loss: 1.822058]\n",
      "epoch:25 step:24327 [D loss: 0.659940, acc: 60.94%] [G loss: 1.849569]\n",
      "epoch:25 step:24328 [D loss: 0.668312, acc: 59.38%] [G loss: 1.845070]\n",
      "epoch:25 step:24329 [D loss: 0.622531, acc: 65.62%] [G loss: 1.849224]\n",
      "epoch:25 step:24330 [D loss: 0.678054, acc: 60.16%] [G loss: 1.945118]\n",
      "epoch:25 step:24331 [D loss: 0.640530, acc: 67.19%] [G loss: 1.849859]\n",
      "epoch:25 step:24332 [D loss: 0.636889, acc: 64.06%] [G loss: 1.936039]\n",
      "epoch:25 step:24333 [D loss: 0.606368, acc: 67.19%] [G loss: 2.037227]\n",
      "epoch:25 step:24334 [D loss: 0.615828, acc: 69.53%] [G loss: 1.938825]\n",
      "epoch:25 step:24335 [D loss: 0.646877, acc: 63.28%] [G loss: 1.817470]\n",
      "epoch:25 step:24336 [D loss: 0.654150, acc: 61.72%] [G loss: 1.953369]\n",
      "epoch:25 step:24337 [D loss: 0.635209, acc: 65.62%] [G loss: 2.049508]\n",
      "epoch:25 step:24338 [D loss: 0.727929, acc: 50.78%] [G loss: 1.927402]\n",
      "epoch:25 step:24339 [D loss: 0.672167, acc: 60.16%] [G loss: 1.859978]\n",
      "epoch:25 step:24340 [D loss: 0.675272, acc: 58.59%] [G loss: 1.825243]\n",
      "epoch:25 step:24341 [D loss: 0.650601, acc: 57.81%] [G loss: 1.980111]\n",
      "epoch:25 step:24342 [D loss: 0.652223, acc: 62.50%] [G loss: 1.724997]\n",
      "epoch:25 step:24343 [D loss: 0.647528, acc: 60.16%] [G loss: 2.105893]\n",
      "epoch:25 step:24344 [D loss: 0.615978, acc: 58.59%] [G loss: 2.050567]\n",
      "epoch:25 step:24345 [D loss: 0.755326, acc: 58.59%] [G loss: 1.903857]\n",
      "epoch:25 step:24346 [D loss: 0.640097, acc: 61.72%] [G loss: 1.830385]\n",
      "epoch:25 step:24347 [D loss: 0.655656, acc: 63.28%] [G loss: 1.777574]\n",
      "epoch:25 step:24348 [D loss: 0.580587, acc: 75.78%] [G loss: 1.946702]\n",
      "epoch:25 step:24349 [D loss: 0.606541, acc: 70.31%] [G loss: 2.095801]\n",
      "epoch:25 step:24350 [D loss: 0.640073, acc: 62.50%] [G loss: 1.908654]\n",
      "epoch:25 step:24351 [D loss: 0.624155, acc: 64.84%] [G loss: 1.974838]\n",
      "epoch:25 step:24352 [D loss: 0.606774, acc: 68.75%] [G loss: 2.000814]\n",
      "epoch:25 step:24353 [D loss: 0.763507, acc: 50.00%] [G loss: 1.842131]\n",
      "epoch:25 step:24354 [D loss: 0.706215, acc: 53.91%] [G loss: 1.878441]\n",
      "epoch:25 step:24355 [D loss: 0.636839, acc: 66.41%] [G loss: 1.965170]\n",
      "epoch:25 step:24356 [D loss: 0.625382, acc: 64.84%] [G loss: 1.879033]\n",
      "epoch:25 step:24357 [D loss: 0.682038, acc: 59.38%] [G loss: 1.929876]\n",
      "epoch:25 step:24358 [D loss: 0.681887, acc: 60.94%] [G loss: 1.890347]\n",
      "epoch:25 step:24359 [D loss: 0.683263, acc: 62.50%] [G loss: 1.884325]\n",
      "epoch:25 step:24360 [D loss: 0.608509, acc: 72.66%] [G loss: 1.899527]\n",
      "epoch:25 step:24361 [D loss: 0.608632, acc: 67.97%] [G loss: 2.026677]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:25 step:24362 [D loss: 0.613723, acc: 67.19%] [G loss: 2.233672]\n",
      "epoch:26 step:24363 [D loss: 0.644459, acc: 61.72%] [G loss: 1.891514]\n",
      "epoch:26 step:24364 [D loss: 0.650801, acc: 58.59%] [G loss: 1.828535]\n",
      "epoch:26 step:24365 [D loss: 0.630494, acc: 63.28%] [G loss: 1.860087]\n",
      "epoch:26 step:24366 [D loss: 0.687710, acc: 59.38%] [G loss: 1.908061]\n",
      "epoch:26 step:24367 [D loss: 0.627929, acc: 61.72%] [G loss: 1.856967]\n",
      "epoch:26 step:24368 [D loss: 0.610372, acc: 65.62%] [G loss: 1.934005]\n",
      "epoch:26 step:24369 [D loss: 0.595787, acc: 68.75%] [G loss: 1.905036]\n",
      "epoch:26 step:24370 [D loss: 0.621310, acc: 60.94%] [G loss: 2.027494]\n",
      "epoch:26 step:24371 [D loss: 0.674579, acc: 60.94%] [G loss: 1.992796]\n",
      "epoch:26 step:24372 [D loss: 0.625186, acc: 63.28%] [G loss: 2.017334]\n",
      "epoch:26 step:24373 [D loss: 0.640895, acc: 62.50%] [G loss: 1.894109]\n",
      "epoch:26 step:24374 [D loss: 0.613225, acc: 63.28%] [G loss: 1.985947]\n",
      "epoch:26 step:24375 [D loss: 0.628851, acc: 65.62%] [G loss: 1.939860]\n",
      "epoch:26 step:24376 [D loss: 0.722237, acc: 57.03%] [G loss: 1.867587]\n",
      "epoch:26 step:24377 [D loss: 0.580091, acc: 68.75%] [G loss: 2.135452]\n",
      "epoch:26 step:24378 [D loss: 0.641707, acc: 63.28%] [G loss: 1.953026]\n",
      "epoch:26 step:24379 [D loss: 0.668220, acc: 60.94%] [G loss: 1.882648]\n",
      "epoch:26 step:24380 [D loss: 0.629065, acc: 69.53%] [G loss: 1.995425]\n",
      "epoch:26 step:24381 [D loss: 0.631030, acc: 63.28%] [G loss: 1.784711]\n",
      "epoch:26 step:24382 [D loss: 0.720258, acc: 50.78%] [G loss: 1.707102]\n",
      "epoch:26 step:24383 [D loss: 0.668917, acc: 61.72%] [G loss: 1.778044]\n",
      "epoch:26 step:24384 [D loss: 0.690375, acc: 51.56%] [G loss: 1.668403]\n",
      "epoch:26 step:24385 [D loss: 0.625968, acc: 67.97%] [G loss: 1.914157]\n",
      "epoch:26 step:24386 [D loss: 0.621293, acc: 67.19%] [G loss: 1.909300]\n",
      "epoch:26 step:24387 [D loss: 0.636867, acc: 60.16%] [G loss: 1.992587]\n",
      "epoch:26 step:24388 [D loss: 0.706206, acc: 57.81%] [G loss: 1.935658]\n",
      "epoch:26 step:24389 [D loss: 0.681225, acc: 61.72%] [G loss: 1.797813]\n",
      "epoch:26 step:24390 [D loss: 0.668623, acc: 57.03%] [G loss: 1.785463]\n",
      "epoch:26 step:24391 [D loss: 0.637784, acc: 61.72%] [G loss: 1.841430]\n",
      "epoch:26 step:24392 [D loss: 0.686520, acc: 57.03%] [G loss: 1.752258]\n",
      "epoch:26 step:24393 [D loss: 0.716732, acc: 51.56%] [G loss: 1.713236]\n",
      "epoch:26 step:24394 [D loss: 0.637268, acc: 61.72%] [G loss: 1.761957]\n",
      "epoch:26 step:24395 [D loss: 0.637211, acc: 64.84%] [G loss: 1.763197]\n",
      "epoch:26 step:24396 [D loss: 0.607743, acc: 65.62%] [G loss: 1.876608]\n",
      "epoch:26 step:24397 [D loss: 0.658325, acc: 59.38%] [G loss: 1.719097]\n",
      "epoch:26 step:24398 [D loss: 0.607895, acc: 70.31%] [G loss: 2.047097]\n",
      "epoch:26 step:24399 [D loss: 0.603438, acc: 71.88%] [G loss: 1.970887]\n",
      "epoch:26 step:24400 [D loss: 0.648858, acc: 60.16%] [G loss: 1.834762]\n",
      "##############\n",
      "[2.2930933  1.62969127 6.38260703 4.74601644 3.53731459 5.77134957\n",
      " 4.49174539 4.73288507 4.51399773 3.84738727]\n",
      "##########\n",
      "epoch:26 step:24401 [D loss: 0.680568, acc: 61.72%] [G loss: 1.887483]\n",
      "epoch:26 step:24402 [D loss: 0.611893, acc: 69.53%] [G loss: 2.118442]\n",
      "epoch:26 step:24403 [D loss: 0.663109, acc: 57.03%] [G loss: 1.743040]\n",
      "epoch:26 step:24404 [D loss: 0.617753, acc: 62.50%] [G loss: 2.091288]\n",
      "epoch:26 step:24405 [D loss: 0.603985, acc: 69.53%] [G loss: 1.888008]\n",
      "epoch:26 step:24406 [D loss: 0.674606, acc: 57.81%] [G loss: 1.852484]\n",
      "epoch:26 step:24407 [D loss: 0.616708, acc: 67.19%] [G loss: 1.919509]\n",
      "epoch:26 step:24408 [D loss: 0.688199, acc: 61.72%] [G loss: 1.881662]\n",
      "epoch:26 step:24409 [D loss: 0.626724, acc: 64.84%] [G loss: 1.790959]\n",
      "epoch:26 step:24410 [D loss: 0.658071, acc: 66.41%] [G loss: 1.985486]\n",
      "epoch:26 step:24411 [D loss: 0.590120, acc: 67.97%] [G loss: 1.974966]\n",
      "epoch:26 step:24412 [D loss: 0.650361, acc: 60.94%] [G loss: 1.936397]\n",
      "epoch:26 step:24413 [D loss: 0.648571, acc: 64.06%] [G loss: 1.864320]\n",
      "epoch:26 step:24414 [D loss: 0.651956, acc: 60.16%] [G loss: 1.949240]\n",
      "epoch:26 step:24415 [D loss: 0.630938, acc: 60.94%] [G loss: 1.914669]\n",
      "epoch:26 step:24416 [D loss: 0.605781, acc: 67.19%] [G loss: 1.998304]\n",
      "epoch:26 step:24417 [D loss: 0.640096, acc: 62.50%] [G loss: 1.976670]\n",
      "epoch:26 step:24418 [D loss: 0.648314, acc: 61.72%] [G loss: 2.039454]\n",
      "epoch:26 step:24419 [D loss: 0.654277, acc: 60.94%] [G loss: 1.901252]\n",
      "epoch:26 step:24420 [D loss: 0.652690, acc: 60.94%] [G loss: 1.810999]\n",
      "epoch:26 step:24421 [D loss: 0.648242, acc: 64.84%] [G loss: 1.827946]\n",
      "epoch:26 step:24422 [D loss: 0.626340, acc: 67.19%] [G loss: 1.899686]\n",
      "epoch:26 step:24423 [D loss: 0.646026, acc: 63.28%] [G loss: 1.855320]\n",
      "epoch:26 step:24424 [D loss: 0.703122, acc: 59.38%] [G loss: 1.855331]\n",
      "epoch:26 step:24425 [D loss: 0.638923, acc: 61.72%] [G loss: 1.865010]\n",
      "epoch:26 step:24426 [D loss: 0.609331, acc: 64.84%] [G loss: 1.795232]\n",
      "epoch:26 step:24427 [D loss: 0.689939, acc: 57.03%] [G loss: 1.880147]\n",
      "epoch:26 step:24428 [D loss: 0.650476, acc: 62.50%] [G loss: 1.865633]\n",
      "epoch:26 step:24429 [D loss: 0.603199, acc: 66.41%] [G loss: 1.777674]\n",
      "epoch:26 step:24430 [D loss: 0.609419, acc: 68.75%] [G loss: 1.972034]\n",
      "epoch:26 step:24431 [D loss: 0.641462, acc: 66.41%] [G loss: 2.136797]\n",
      "epoch:26 step:24432 [D loss: 0.570551, acc: 72.66%] [G loss: 1.997496]\n",
      "epoch:26 step:24433 [D loss: 0.714549, acc: 56.25%] [G loss: 1.790093]\n",
      "epoch:26 step:24434 [D loss: 0.677162, acc: 60.16%] [G loss: 1.756443]\n",
      "epoch:26 step:24435 [D loss: 0.635445, acc: 60.94%] [G loss: 1.807563]\n",
      "epoch:26 step:24436 [D loss: 0.601282, acc: 65.62%] [G loss: 1.895030]\n",
      "epoch:26 step:24437 [D loss: 0.640592, acc: 61.72%] [G loss: 2.013379]\n",
      "epoch:26 step:24438 [D loss: 0.618795, acc: 69.53%] [G loss: 1.936922]\n",
      "epoch:26 step:24439 [D loss: 0.589733, acc: 67.97%] [G loss: 2.076933]\n",
      "epoch:26 step:24440 [D loss: 0.672552, acc: 54.69%] [G loss: 1.742241]\n",
      "epoch:26 step:24441 [D loss: 0.692975, acc: 60.16%] [G loss: 1.786567]\n",
      "epoch:26 step:24442 [D loss: 0.716508, acc: 53.91%] [G loss: 1.813153]\n",
      "epoch:26 step:24443 [D loss: 0.673884, acc: 57.03%] [G loss: 1.788735]\n",
      "epoch:26 step:24444 [D loss: 0.615879, acc: 64.06%] [G loss: 1.863091]\n",
      "epoch:26 step:24445 [D loss: 0.623827, acc: 65.62%] [G loss: 1.842617]\n",
      "epoch:26 step:24446 [D loss: 0.733716, acc: 47.66%] [G loss: 1.807683]\n",
      "epoch:26 step:24447 [D loss: 0.646009, acc: 64.06%] [G loss: 1.866802]\n",
      "epoch:26 step:24448 [D loss: 0.664649, acc: 57.81%] [G loss: 1.697093]\n",
      "epoch:26 step:24449 [D loss: 0.629454, acc: 64.84%] [G loss: 1.851364]\n",
      "epoch:26 step:24450 [D loss: 0.624547, acc: 63.28%] [G loss: 1.845980]\n",
      "epoch:26 step:24451 [D loss: 0.587228, acc: 71.09%] [G loss: 1.980094]\n",
      "epoch:26 step:24452 [D loss: 0.687674, acc: 52.34%] [G loss: 1.783461]\n",
      "epoch:26 step:24453 [D loss: 0.651916, acc: 66.41%] [G loss: 1.791306]\n",
      "epoch:26 step:24454 [D loss: 0.655698, acc: 64.06%] [G loss: 1.844012]\n",
      "epoch:26 step:24455 [D loss: 0.591667, acc: 67.19%] [G loss: 2.035790]\n",
      "epoch:26 step:24456 [D loss: 0.693969, acc: 53.12%] [G loss: 1.916070]\n",
      "epoch:26 step:24457 [D loss: 0.682525, acc: 62.50%] [G loss: 1.763589]\n",
      "epoch:26 step:24458 [D loss: 0.703179, acc: 56.25%] [G loss: 1.789814]\n",
      "epoch:26 step:24459 [D loss: 0.646269, acc: 58.59%] [G loss: 1.762659]\n",
      "epoch:26 step:24460 [D loss: 0.653182, acc: 60.94%] [G loss: 1.766044]\n",
      "epoch:26 step:24461 [D loss: 0.613390, acc: 64.06%] [G loss: 1.806358]\n",
      "epoch:26 step:24462 [D loss: 0.667061, acc: 61.72%] [G loss: 1.875216]\n",
      "epoch:26 step:24463 [D loss: 0.685351, acc: 57.81%] [G loss: 1.804974]\n",
      "epoch:26 step:24464 [D loss: 0.654776, acc: 60.16%] [G loss: 1.821217]\n",
      "epoch:26 step:24465 [D loss: 0.657277, acc: 60.94%] [G loss: 1.831688]\n",
      "epoch:26 step:24466 [D loss: 0.645083, acc: 64.84%] [G loss: 1.835647]\n",
      "epoch:26 step:24467 [D loss: 0.662170, acc: 62.50%] [G loss: 1.744848]\n",
      "epoch:26 step:24468 [D loss: 0.617315, acc: 67.97%] [G loss: 1.992725]\n",
      "epoch:26 step:24469 [D loss: 0.592683, acc: 74.22%] [G loss: 2.134436]\n",
      "epoch:26 step:24470 [D loss: 0.660113, acc: 60.16%] [G loss: 1.764820]\n",
      "epoch:26 step:24471 [D loss: 0.711125, acc: 55.47%] [G loss: 1.768361]\n",
      "epoch:26 step:24472 [D loss: 0.650124, acc: 59.38%] [G loss: 1.920487]\n",
      "epoch:26 step:24473 [D loss: 0.612681, acc: 67.97%] [G loss: 1.913444]\n",
      "epoch:26 step:24474 [D loss: 0.616235, acc: 67.97%] [G loss: 1.937986]\n",
      "epoch:26 step:24475 [D loss: 0.587137, acc: 68.75%] [G loss: 2.107485]\n",
      "epoch:26 step:24476 [D loss: 0.618237, acc: 67.19%] [G loss: 2.126897]\n",
      "epoch:26 step:24477 [D loss: 0.625283, acc: 63.28%] [G loss: 2.227083]\n",
      "epoch:26 step:24478 [D loss: 0.620475, acc: 67.19%] [G loss: 2.231696]\n",
      "epoch:26 step:24479 [D loss: 0.631124, acc: 60.94%] [G loss: 2.245969]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:26 step:24480 [D loss: 0.619121, acc: 67.19%] [G loss: 2.131570]\n",
      "epoch:26 step:24481 [D loss: 0.604763, acc: 69.53%] [G loss: 2.348387]\n",
      "epoch:26 step:24482 [D loss: 0.695827, acc: 59.38%] [G loss: 2.068496]\n",
      "epoch:26 step:24483 [D loss: 0.642116, acc: 60.16%] [G loss: 2.000437]\n",
      "epoch:26 step:24484 [D loss: 0.638208, acc: 63.28%] [G loss: 2.062538]\n",
      "epoch:26 step:24485 [D loss: 0.618161, acc: 67.19%] [G loss: 1.888676]\n",
      "epoch:26 step:24486 [D loss: 0.691229, acc: 54.69%] [G loss: 1.936688]\n",
      "epoch:26 step:24487 [D loss: 0.711081, acc: 52.34%] [G loss: 1.698310]\n",
      "epoch:26 step:24488 [D loss: 0.651835, acc: 65.62%] [G loss: 2.040427]\n",
      "epoch:26 step:24489 [D loss: 0.635681, acc: 64.06%] [G loss: 1.733519]\n",
      "epoch:26 step:24490 [D loss: 0.658242, acc: 61.72%] [G loss: 1.846129]\n",
      "epoch:26 step:24491 [D loss: 0.645532, acc: 61.72%] [G loss: 1.871001]\n",
      "epoch:26 step:24492 [D loss: 0.650342, acc: 57.81%] [G loss: 1.972309]\n",
      "epoch:26 step:24493 [D loss: 0.650747, acc: 63.28%] [G loss: 1.901850]\n",
      "epoch:26 step:24494 [D loss: 0.666067, acc: 62.50%] [G loss: 1.732815]\n",
      "epoch:26 step:24495 [D loss: 0.678358, acc: 58.59%] [G loss: 1.795254]\n",
      "epoch:26 step:24496 [D loss: 0.712485, acc: 48.44%] [G loss: 1.784806]\n",
      "epoch:26 step:24497 [D loss: 0.674437, acc: 61.72%] [G loss: 1.831765]\n",
      "epoch:26 step:24498 [D loss: 0.630638, acc: 62.50%] [G loss: 1.819981]\n",
      "epoch:26 step:24499 [D loss: 0.672379, acc: 57.81%] [G loss: 1.847403]\n",
      "epoch:26 step:24500 [D loss: 0.588544, acc: 68.75%] [G loss: 1.921843]\n",
      "epoch:26 step:24501 [D loss: 0.698776, acc: 54.69%] [G loss: 1.951622]\n",
      "epoch:26 step:24502 [D loss: 0.628712, acc: 67.19%] [G loss: 1.757303]\n",
      "epoch:26 step:24503 [D loss: 0.626944, acc: 63.28%] [G loss: 1.875680]\n",
      "epoch:26 step:24504 [D loss: 0.649345, acc: 59.38%] [G loss: 1.931204]\n",
      "epoch:26 step:24505 [D loss: 0.611499, acc: 67.97%] [G loss: 1.799853]\n",
      "epoch:26 step:24506 [D loss: 0.624174, acc: 65.62%] [G loss: 1.995204]\n",
      "epoch:26 step:24507 [D loss: 0.652485, acc: 63.28%] [G loss: 1.914538]\n",
      "epoch:26 step:24508 [D loss: 0.596268, acc: 60.94%] [G loss: 1.883971]\n",
      "epoch:26 step:24509 [D loss: 0.714323, acc: 57.81%] [G loss: 1.800364]\n",
      "epoch:26 step:24510 [D loss: 0.672621, acc: 54.69%] [G loss: 1.886795]\n",
      "epoch:26 step:24511 [D loss: 0.665675, acc: 59.38%] [G loss: 1.826577]\n",
      "epoch:26 step:24512 [D loss: 0.643144, acc: 61.72%] [G loss: 1.768515]\n",
      "epoch:26 step:24513 [D loss: 0.631080, acc: 63.28%] [G loss: 1.939191]\n",
      "epoch:26 step:24514 [D loss: 0.647706, acc: 60.94%] [G loss: 1.854735]\n",
      "epoch:26 step:24515 [D loss: 0.686022, acc: 58.59%] [G loss: 1.891601]\n",
      "epoch:26 step:24516 [D loss: 0.632186, acc: 62.50%] [G loss: 1.950577]\n",
      "epoch:26 step:24517 [D loss: 0.589064, acc: 71.88%] [G loss: 1.812915]\n",
      "epoch:26 step:24518 [D loss: 0.649950, acc: 60.16%] [G loss: 1.919935]\n",
      "epoch:26 step:24519 [D loss: 0.628490, acc: 67.97%] [G loss: 1.867512]\n",
      "epoch:26 step:24520 [D loss: 0.708613, acc: 56.25%] [G loss: 1.852366]\n",
      "epoch:26 step:24521 [D loss: 0.605242, acc: 62.50%] [G loss: 1.852549]\n",
      "epoch:26 step:24522 [D loss: 0.677438, acc: 54.69%] [G loss: 1.689180]\n",
      "epoch:26 step:24523 [D loss: 0.649643, acc: 58.59%] [G loss: 1.885573]\n",
      "epoch:26 step:24524 [D loss: 0.595879, acc: 65.62%] [G loss: 1.886469]\n",
      "epoch:26 step:24525 [D loss: 0.660397, acc: 53.91%] [G loss: 1.856943]\n",
      "epoch:26 step:24526 [D loss: 0.624473, acc: 62.50%] [G loss: 1.787022]\n",
      "epoch:26 step:24527 [D loss: 0.640377, acc: 60.94%] [G loss: 1.813711]\n",
      "epoch:26 step:24528 [D loss: 0.658707, acc: 57.81%] [G loss: 1.896453]\n",
      "epoch:26 step:24529 [D loss: 0.639681, acc: 64.84%] [G loss: 2.017626]\n",
      "epoch:26 step:24530 [D loss: 0.619862, acc: 64.84%] [G loss: 1.882125]\n",
      "epoch:26 step:24531 [D loss: 0.643482, acc: 67.19%] [G loss: 1.836552]\n",
      "epoch:26 step:24532 [D loss: 0.625700, acc: 66.41%] [G loss: 1.816604]\n",
      "epoch:26 step:24533 [D loss: 0.666303, acc: 57.81%] [G loss: 1.893009]\n",
      "epoch:26 step:24534 [D loss: 0.668118, acc: 60.16%] [G loss: 1.835196]\n",
      "epoch:26 step:24535 [D loss: 0.642865, acc: 66.41%] [G loss: 1.749332]\n",
      "epoch:26 step:24536 [D loss: 0.664983, acc: 60.94%] [G loss: 1.681823]\n",
      "epoch:26 step:24537 [D loss: 0.684653, acc: 57.81%] [G loss: 1.687338]\n",
      "epoch:26 step:24538 [D loss: 0.725409, acc: 47.66%] [G loss: 1.753244]\n",
      "epoch:26 step:24539 [D loss: 0.693379, acc: 57.03%] [G loss: 1.925206]\n",
      "epoch:26 step:24540 [D loss: 0.634264, acc: 60.16%] [G loss: 1.843199]\n",
      "epoch:26 step:24541 [D loss: 0.615252, acc: 70.31%] [G loss: 1.781309]\n",
      "epoch:26 step:24542 [D loss: 0.658359, acc: 60.94%] [G loss: 1.836139]\n",
      "epoch:26 step:24543 [D loss: 0.640578, acc: 58.59%] [G loss: 1.773838]\n",
      "epoch:26 step:24544 [D loss: 0.728349, acc: 53.12%] [G loss: 1.788933]\n",
      "epoch:26 step:24545 [D loss: 0.661995, acc: 59.38%] [G loss: 1.789683]\n",
      "epoch:26 step:24546 [D loss: 0.651388, acc: 64.06%] [G loss: 1.881437]\n",
      "epoch:26 step:24547 [D loss: 0.655810, acc: 64.84%] [G loss: 1.819040]\n",
      "epoch:26 step:24548 [D loss: 0.664303, acc: 62.50%] [G loss: 1.891129]\n",
      "epoch:26 step:24549 [D loss: 0.651048, acc: 56.25%] [G loss: 1.780820]\n",
      "epoch:26 step:24550 [D loss: 0.605564, acc: 70.31%] [G loss: 1.828660]\n",
      "epoch:26 step:24551 [D loss: 0.645311, acc: 62.50%] [G loss: 1.780114]\n",
      "epoch:26 step:24552 [D loss: 0.617726, acc: 64.84%] [G loss: 1.951656]\n",
      "epoch:26 step:24553 [D loss: 0.644612, acc: 61.72%] [G loss: 1.895629]\n",
      "epoch:26 step:24554 [D loss: 0.585142, acc: 69.53%] [G loss: 1.946511]\n",
      "epoch:26 step:24555 [D loss: 0.683627, acc: 61.72%] [G loss: 1.960304]\n",
      "epoch:26 step:24556 [D loss: 0.603410, acc: 68.75%] [G loss: 1.930853]\n",
      "epoch:26 step:24557 [D loss: 0.661695, acc: 59.38%] [G loss: 1.857344]\n",
      "epoch:26 step:24558 [D loss: 0.627736, acc: 65.62%] [G loss: 1.824592]\n",
      "epoch:26 step:24559 [D loss: 0.623501, acc: 63.28%] [G loss: 1.964774]\n",
      "epoch:26 step:24560 [D loss: 0.697558, acc: 62.50%] [G loss: 1.874871]\n",
      "epoch:26 step:24561 [D loss: 0.664860, acc: 62.50%] [G loss: 1.802080]\n",
      "epoch:26 step:24562 [D loss: 0.684765, acc: 56.25%] [G loss: 1.654836]\n",
      "epoch:26 step:24563 [D loss: 0.648732, acc: 57.03%] [G loss: 1.863913]\n",
      "epoch:26 step:24564 [D loss: 0.647920, acc: 61.72%] [G loss: 1.704528]\n",
      "epoch:26 step:24565 [D loss: 0.657313, acc: 57.81%] [G loss: 1.811749]\n",
      "epoch:26 step:24566 [D loss: 0.657251, acc: 57.03%] [G loss: 1.912928]\n",
      "epoch:26 step:24567 [D loss: 0.625021, acc: 63.28%] [G loss: 1.867301]\n",
      "epoch:26 step:24568 [D loss: 0.611710, acc: 70.31%] [G loss: 1.974039]\n",
      "epoch:26 step:24569 [D loss: 0.633001, acc: 67.19%] [G loss: 1.964998]\n",
      "epoch:26 step:24570 [D loss: 0.571675, acc: 71.09%] [G loss: 2.195766]\n",
      "epoch:26 step:24571 [D loss: 0.607377, acc: 70.31%] [G loss: 2.139541]\n",
      "epoch:26 step:24572 [D loss: 0.670906, acc: 60.16%] [G loss: 1.779978]\n",
      "epoch:26 step:24573 [D loss: 0.664418, acc: 60.16%] [G loss: 1.749579]\n",
      "epoch:26 step:24574 [D loss: 0.729719, acc: 54.69%] [G loss: 1.694847]\n",
      "epoch:26 step:24575 [D loss: 0.600065, acc: 68.75%] [G loss: 1.859941]\n",
      "epoch:26 step:24576 [D loss: 0.712651, acc: 55.47%] [G loss: 1.760758]\n",
      "epoch:26 step:24577 [D loss: 0.688070, acc: 58.59%] [G loss: 1.716148]\n",
      "epoch:26 step:24578 [D loss: 0.677713, acc: 59.38%] [G loss: 1.702351]\n",
      "epoch:26 step:24579 [D loss: 0.712927, acc: 54.69%] [G loss: 1.819072]\n",
      "epoch:26 step:24580 [D loss: 0.637176, acc: 60.94%] [G loss: 2.057596]\n",
      "epoch:26 step:24581 [D loss: 0.606721, acc: 66.41%] [G loss: 2.043100]\n",
      "epoch:26 step:24582 [D loss: 0.751937, acc: 51.56%] [G loss: 1.739175]\n",
      "epoch:26 step:24583 [D loss: 0.613634, acc: 62.50%] [G loss: 1.870320]\n",
      "epoch:26 step:24584 [D loss: 0.655760, acc: 60.94%] [G loss: 1.835513]\n",
      "epoch:26 step:24585 [D loss: 0.659603, acc: 59.38%] [G loss: 1.838531]\n",
      "epoch:26 step:24586 [D loss: 0.651865, acc: 57.81%] [G loss: 1.840158]\n",
      "epoch:26 step:24587 [D loss: 0.671180, acc: 54.69%] [G loss: 1.863735]\n",
      "epoch:26 step:24588 [D loss: 0.704826, acc: 57.81%] [G loss: 1.876721]\n",
      "epoch:26 step:24589 [D loss: 0.700378, acc: 50.78%] [G loss: 1.787502]\n",
      "epoch:26 step:24590 [D loss: 0.667067, acc: 60.16%] [G loss: 1.739492]\n",
      "epoch:26 step:24591 [D loss: 0.615816, acc: 62.50%] [G loss: 2.043025]\n",
      "epoch:26 step:24592 [D loss: 0.622237, acc: 63.28%] [G loss: 1.955201]\n",
      "epoch:26 step:24593 [D loss: 0.604071, acc: 66.41%] [G loss: 2.047049]\n",
      "epoch:26 step:24594 [D loss: 0.581167, acc: 67.19%] [G loss: 2.221800]\n",
      "epoch:26 step:24595 [D loss: 0.618685, acc: 67.97%] [G loss: 1.890150]\n",
      "epoch:26 step:24596 [D loss: 0.651085, acc: 62.50%] [G loss: 1.708345]\n",
      "epoch:26 step:24597 [D loss: 0.696805, acc: 50.78%] [G loss: 1.815694]\n",
      "epoch:26 step:24598 [D loss: 0.684267, acc: 56.25%] [G loss: 1.821874]\n",
      "epoch:26 step:24599 [D loss: 0.595909, acc: 67.19%] [G loss: 1.834878]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:26 step:24600 [D loss: 0.654857, acc: 58.59%] [G loss: 1.852064]\n",
      "##############\n",
      "[2.3473262  1.31479538 6.06570892 4.68238259 3.70257233 5.58783042\n",
      " 4.40266634 4.55561236 4.50833432 3.47835371]\n",
      "##########\n",
      "epoch:26 step:24601 [D loss: 0.636813, acc: 61.72%] [G loss: 1.843571]\n",
      "epoch:26 step:24602 [D loss: 0.673022, acc: 62.50%] [G loss: 1.820037]\n",
      "epoch:26 step:24603 [D loss: 0.606837, acc: 67.97%] [G loss: 1.926587]\n",
      "epoch:26 step:24604 [D loss: 0.622139, acc: 60.16%] [G loss: 1.944494]\n",
      "epoch:26 step:24605 [D loss: 0.631470, acc: 66.41%] [G loss: 1.919126]\n",
      "epoch:26 step:24606 [D loss: 0.679914, acc: 60.16%] [G loss: 1.985531]\n",
      "epoch:26 step:24607 [D loss: 0.650157, acc: 61.72%] [G loss: 1.889936]\n",
      "epoch:26 step:24608 [D loss: 0.651984, acc: 53.12%] [G loss: 1.851616]\n",
      "epoch:26 step:24609 [D loss: 0.682029, acc: 58.59%] [G loss: 1.961183]\n",
      "epoch:26 step:24610 [D loss: 0.613214, acc: 65.62%] [G loss: 1.928694]\n",
      "epoch:26 step:24611 [D loss: 0.694581, acc: 57.81%] [G loss: 1.868892]\n",
      "epoch:26 step:24612 [D loss: 0.764719, acc: 46.09%] [G loss: 1.838028]\n",
      "epoch:26 step:24613 [D loss: 0.675417, acc: 55.47%] [G loss: 1.750179]\n",
      "epoch:26 step:24614 [D loss: 0.709531, acc: 52.34%] [G loss: 1.706667]\n",
      "epoch:26 step:24615 [D loss: 0.593971, acc: 67.19%] [G loss: 1.738658]\n",
      "epoch:26 step:24616 [D loss: 0.634437, acc: 65.62%] [G loss: 1.838635]\n",
      "epoch:26 step:24617 [D loss: 0.638401, acc: 64.84%] [G loss: 1.906635]\n",
      "epoch:26 step:24618 [D loss: 0.633362, acc: 63.28%] [G loss: 1.757686]\n",
      "epoch:26 step:24619 [D loss: 0.653084, acc: 61.72%] [G loss: 1.682039]\n",
      "epoch:26 step:24620 [D loss: 0.656150, acc: 57.81%] [G loss: 1.759898]\n",
      "epoch:26 step:24621 [D loss: 0.582775, acc: 72.66%] [G loss: 1.934283]\n",
      "epoch:26 step:24622 [D loss: 0.639789, acc: 66.41%] [G loss: 1.847718]\n",
      "epoch:26 step:24623 [D loss: 0.679293, acc: 56.25%] [G loss: 1.938314]\n",
      "epoch:26 step:24624 [D loss: 0.641920, acc: 63.28%] [G loss: 1.972412]\n",
      "epoch:26 step:24625 [D loss: 0.640348, acc: 62.50%] [G loss: 2.071955]\n",
      "epoch:26 step:24626 [D loss: 0.624171, acc: 67.19%] [G loss: 2.067894]\n",
      "epoch:26 step:24627 [D loss: 0.631428, acc: 67.19%] [G loss: 1.978921]\n",
      "epoch:26 step:24628 [D loss: 0.612278, acc: 65.62%] [G loss: 2.011872]\n",
      "epoch:26 step:24629 [D loss: 0.672800, acc: 60.16%] [G loss: 1.887087]\n",
      "epoch:26 step:24630 [D loss: 0.683826, acc: 57.03%] [G loss: 1.883327]\n",
      "epoch:26 step:24631 [D loss: 0.633541, acc: 62.50%] [G loss: 2.156426]\n",
      "epoch:26 step:24632 [D loss: 0.655665, acc: 59.38%] [G loss: 1.886223]\n",
      "epoch:26 step:24633 [D loss: 0.641374, acc: 60.16%] [G loss: 1.970758]\n",
      "epoch:26 step:24634 [D loss: 0.671066, acc: 59.38%] [G loss: 1.954717]\n",
      "epoch:26 step:24635 [D loss: 0.618139, acc: 64.84%] [G loss: 1.864437]\n",
      "epoch:26 step:24636 [D loss: 0.630482, acc: 67.19%] [G loss: 2.017115]\n",
      "epoch:26 step:24637 [D loss: 0.614890, acc: 64.06%] [G loss: 1.992068]\n",
      "epoch:26 step:24638 [D loss: 0.572587, acc: 69.53%] [G loss: 2.173651]\n",
      "epoch:26 step:24639 [D loss: 0.676828, acc: 60.94%] [G loss: 1.926239]\n",
      "epoch:26 step:24640 [D loss: 0.639693, acc: 64.84%] [G loss: 1.866036]\n",
      "epoch:26 step:24641 [D loss: 0.705998, acc: 51.56%] [G loss: 1.809582]\n",
      "epoch:26 step:24642 [D loss: 0.660874, acc: 55.47%] [G loss: 1.945154]\n",
      "epoch:26 step:24643 [D loss: 0.662345, acc: 62.50%] [G loss: 1.852271]\n",
      "epoch:26 step:24644 [D loss: 0.636891, acc: 66.41%] [G loss: 1.813537]\n",
      "epoch:26 step:24645 [D loss: 0.682018, acc: 57.03%] [G loss: 1.887032]\n",
      "epoch:26 step:24646 [D loss: 0.661946, acc: 59.38%] [G loss: 1.784656]\n",
      "epoch:26 step:24647 [D loss: 0.636219, acc: 61.72%] [G loss: 1.864851]\n",
      "epoch:26 step:24648 [D loss: 0.612323, acc: 75.78%] [G loss: 1.930766]\n",
      "epoch:26 step:24649 [D loss: 0.627754, acc: 64.84%] [G loss: 2.000529]\n",
      "epoch:26 step:24650 [D loss: 0.663973, acc: 60.94%] [G loss: 1.954444]\n",
      "epoch:26 step:24651 [D loss: 0.646643, acc: 65.62%] [G loss: 1.952341]\n",
      "epoch:26 step:24652 [D loss: 0.629727, acc: 62.50%] [G loss: 1.861467]\n",
      "epoch:26 step:24653 [D loss: 0.675748, acc: 59.38%] [G loss: 1.894565]\n",
      "epoch:26 step:24654 [D loss: 0.587208, acc: 69.53%] [G loss: 1.794893]\n",
      "epoch:26 step:24655 [D loss: 0.694406, acc: 50.78%] [G loss: 1.957653]\n",
      "epoch:26 step:24656 [D loss: 0.651742, acc: 62.50%] [G loss: 1.973335]\n",
      "epoch:26 step:24657 [D loss: 0.629893, acc: 64.06%] [G loss: 1.839825]\n",
      "epoch:26 step:24658 [D loss: 0.590271, acc: 71.88%] [G loss: 2.010201]\n",
      "epoch:26 step:24659 [D loss: 0.657957, acc: 64.06%] [G loss: 1.788809]\n",
      "epoch:26 step:24660 [D loss: 0.642011, acc: 58.59%] [G loss: 1.911498]\n",
      "epoch:26 step:24661 [D loss: 0.608735, acc: 60.16%] [G loss: 2.030208]\n",
      "epoch:26 step:24662 [D loss: 0.631648, acc: 67.19%] [G loss: 2.002495]\n",
      "epoch:26 step:24663 [D loss: 0.724274, acc: 53.91%] [G loss: 1.807291]\n",
      "epoch:26 step:24664 [D loss: 0.635468, acc: 62.50%] [G loss: 1.881923]\n",
      "epoch:26 step:24665 [D loss: 0.682345, acc: 61.72%] [G loss: 1.912129]\n",
      "epoch:26 step:24666 [D loss: 0.649849, acc: 64.06%] [G loss: 1.741432]\n",
      "epoch:26 step:24667 [D loss: 0.627324, acc: 64.84%] [G loss: 1.802630]\n",
      "epoch:26 step:24668 [D loss: 0.675845, acc: 58.59%] [G loss: 1.830348]\n",
      "epoch:26 step:24669 [D loss: 0.630125, acc: 65.62%] [G loss: 1.716862]\n",
      "epoch:26 step:24670 [D loss: 0.629625, acc: 64.06%] [G loss: 1.796201]\n",
      "epoch:26 step:24671 [D loss: 0.604204, acc: 65.62%] [G loss: 1.870881]\n",
      "epoch:26 step:24672 [D loss: 0.664574, acc: 62.50%] [G loss: 1.877936]\n",
      "epoch:26 step:24673 [D loss: 0.625153, acc: 63.28%] [G loss: 1.880465]\n",
      "epoch:26 step:24674 [D loss: 0.628983, acc: 61.72%] [G loss: 2.188262]\n",
      "epoch:26 step:24675 [D loss: 0.623483, acc: 60.94%] [G loss: 1.956685]\n",
      "epoch:26 step:24676 [D loss: 0.613883, acc: 62.50%] [G loss: 2.225627]\n",
      "epoch:26 step:24677 [D loss: 0.607609, acc: 67.19%] [G loss: 2.065581]\n",
      "epoch:26 step:24678 [D loss: 0.690445, acc: 60.94%] [G loss: 1.733948]\n",
      "epoch:26 step:24679 [D loss: 0.686990, acc: 55.47%] [G loss: 1.855859]\n",
      "epoch:26 step:24680 [D loss: 0.617459, acc: 64.84%] [G loss: 1.863930]\n",
      "epoch:26 step:24681 [D loss: 0.655744, acc: 57.81%] [G loss: 1.759044]\n",
      "epoch:26 step:24682 [D loss: 0.671059, acc: 57.81%] [G loss: 1.832889]\n",
      "epoch:26 step:24683 [D loss: 0.604846, acc: 66.41%] [G loss: 1.892344]\n",
      "epoch:26 step:24684 [D loss: 0.692920, acc: 59.38%] [G loss: 1.835848]\n",
      "epoch:26 step:24685 [D loss: 0.647328, acc: 60.16%] [G loss: 1.813053]\n",
      "epoch:26 step:24686 [D loss: 0.615900, acc: 65.62%] [G loss: 1.697717]\n",
      "epoch:26 step:24687 [D loss: 0.663450, acc: 64.84%] [G loss: 1.915322]\n",
      "epoch:26 step:24688 [D loss: 0.615336, acc: 64.06%] [G loss: 1.864954]\n",
      "epoch:26 step:24689 [D loss: 0.651068, acc: 58.59%] [G loss: 1.880997]\n",
      "epoch:26 step:24690 [D loss: 0.614741, acc: 69.53%] [G loss: 1.892929]\n",
      "epoch:26 step:24691 [D loss: 0.647797, acc: 64.84%] [G loss: 1.903593]\n",
      "epoch:26 step:24692 [D loss: 0.648256, acc: 62.50%] [G loss: 2.044291]\n",
      "epoch:26 step:24693 [D loss: 0.643344, acc: 62.50%] [G loss: 1.969079]\n",
      "epoch:26 step:24694 [D loss: 0.612673, acc: 67.97%] [G loss: 1.837775]\n",
      "epoch:26 step:24695 [D loss: 0.688845, acc: 55.47%] [G loss: 1.732118]\n",
      "epoch:26 step:24696 [D loss: 0.625488, acc: 68.75%] [G loss: 1.945785]\n",
      "epoch:26 step:24697 [D loss: 0.653966, acc: 60.94%] [G loss: 2.043901]\n",
      "epoch:26 step:24698 [D loss: 0.659432, acc: 58.59%] [G loss: 1.953801]\n",
      "epoch:26 step:24699 [D loss: 0.633236, acc: 66.41%] [G loss: 1.974151]\n",
      "epoch:26 step:24700 [D loss: 0.610360, acc: 69.53%] [G loss: 1.834690]\n",
      "epoch:26 step:24701 [D loss: 0.637811, acc: 64.06%] [G loss: 1.933631]\n",
      "epoch:26 step:24702 [D loss: 0.616606, acc: 64.84%] [G loss: 2.038597]\n",
      "epoch:26 step:24703 [D loss: 0.697638, acc: 59.38%] [G loss: 1.691973]\n",
      "epoch:26 step:24704 [D loss: 0.754206, acc: 48.44%] [G loss: 1.729797]\n",
      "epoch:26 step:24705 [D loss: 0.705536, acc: 56.25%] [G loss: 1.890605]\n",
      "epoch:26 step:24706 [D loss: 0.681272, acc: 50.00%] [G loss: 1.832633]\n",
      "epoch:26 step:24707 [D loss: 0.649703, acc: 60.16%] [G loss: 2.248677]\n",
      "epoch:26 step:24708 [D loss: 0.628504, acc: 63.28%] [G loss: 2.185853]\n",
      "epoch:26 step:24709 [D loss: 0.596609, acc: 65.62%] [G loss: 2.397892]\n",
      "epoch:26 step:24710 [D loss: 0.657774, acc: 60.94%] [G loss: 1.912773]\n",
      "epoch:26 step:24711 [D loss: 0.691650, acc: 54.69%] [G loss: 1.818252]\n",
      "epoch:26 step:24712 [D loss: 0.628895, acc: 63.28%] [G loss: 1.992813]\n",
      "epoch:26 step:24713 [D loss: 0.649997, acc: 59.38%] [G loss: 1.682222]\n",
      "epoch:26 step:24714 [D loss: 0.636449, acc: 64.06%] [G loss: 1.864419]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:26 step:24715 [D loss: 0.681326, acc: 60.16%] [G loss: 1.820035]\n",
      "epoch:26 step:24716 [D loss: 0.631820, acc: 70.31%] [G loss: 1.764692]\n",
      "epoch:26 step:24717 [D loss: 0.679104, acc: 56.25%] [G loss: 1.679724]\n",
      "epoch:26 step:24718 [D loss: 0.679017, acc: 57.81%] [G loss: 1.688086]\n",
      "epoch:26 step:24719 [D loss: 0.628413, acc: 67.19%] [G loss: 1.962341]\n",
      "epoch:26 step:24720 [D loss: 0.611385, acc: 68.75%] [G loss: 1.910682]\n",
      "epoch:26 step:24721 [D loss: 0.650285, acc: 64.84%] [G loss: 1.914170]\n",
      "epoch:26 step:24722 [D loss: 0.600577, acc: 65.62%] [G loss: 1.955276]\n",
      "epoch:26 step:24723 [D loss: 0.673177, acc: 56.25%] [G loss: 1.840356]\n",
      "epoch:26 step:24724 [D loss: 0.693754, acc: 53.12%] [G loss: 1.779690]\n",
      "epoch:26 step:24725 [D loss: 0.649152, acc: 64.06%] [G loss: 1.846844]\n",
      "epoch:26 step:24726 [D loss: 0.660595, acc: 57.81%] [G loss: 1.833703]\n",
      "epoch:26 step:24727 [D loss: 0.608497, acc: 65.62%] [G loss: 1.901757]\n",
      "epoch:26 step:24728 [D loss: 0.632033, acc: 66.41%] [G loss: 1.879414]\n",
      "epoch:26 step:24729 [D loss: 0.622842, acc: 64.06%] [G loss: 1.837143]\n",
      "epoch:26 step:24730 [D loss: 0.658348, acc: 61.72%] [G loss: 1.856923]\n",
      "epoch:26 step:24731 [D loss: 0.650872, acc: 62.50%] [G loss: 1.894804]\n",
      "epoch:26 step:24732 [D loss: 0.614913, acc: 68.75%] [G loss: 1.970694]\n",
      "epoch:26 step:24733 [D loss: 0.666237, acc: 61.72%] [G loss: 2.102198]\n",
      "epoch:26 step:24734 [D loss: 0.575937, acc: 71.88%] [G loss: 1.953706]\n",
      "epoch:26 step:24735 [D loss: 0.692299, acc: 56.25%] [G loss: 1.779881]\n",
      "epoch:26 step:24736 [D loss: 0.620541, acc: 66.41%] [G loss: 1.980913]\n",
      "epoch:26 step:24737 [D loss: 0.640883, acc: 61.72%] [G loss: 1.759274]\n",
      "epoch:26 step:24738 [D loss: 0.688236, acc: 57.81%] [G loss: 1.911263]\n",
      "epoch:26 step:24739 [D loss: 0.677298, acc: 56.25%] [G loss: 1.742961]\n",
      "epoch:26 step:24740 [D loss: 0.693419, acc: 51.56%] [G loss: 1.875196]\n",
      "epoch:26 step:24741 [D loss: 0.614796, acc: 71.09%] [G loss: 1.881731]\n",
      "epoch:26 step:24742 [D loss: 0.631046, acc: 60.16%] [G loss: 1.952841]\n",
      "epoch:26 step:24743 [D loss: 0.622481, acc: 64.84%] [G loss: 1.953455]\n",
      "epoch:26 step:24744 [D loss: 0.697773, acc: 61.72%] [G loss: 1.904598]\n",
      "epoch:26 step:24745 [D loss: 0.640513, acc: 61.72%] [G loss: 1.969781]\n",
      "epoch:26 step:24746 [D loss: 0.666535, acc: 64.06%] [G loss: 2.061776]\n",
      "epoch:26 step:24747 [D loss: 0.692522, acc: 57.81%] [G loss: 2.002296]\n",
      "epoch:26 step:24748 [D loss: 0.647300, acc: 59.38%] [G loss: 1.818127]\n",
      "epoch:26 step:24749 [D loss: 0.671969, acc: 58.59%] [G loss: 1.754394]\n",
      "epoch:26 step:24750 [D loss: 0.642318, acc: 64.06%] [G loss: 1.729852]\n",
      "epoch:26 step:24751 [D loss: 0.679278, acc: 57.81%] [G loss: 1.743563]\n",
      "epoch:26 step:24752 [D loss: 0.643495, acc: 60.16%] [G loss: 1.826077]\n",
      "epoch:26 step:24753 [D loss: 0.666538, acc: 57.81%] [G loss: 1.869640]\n",
      "epoch:26 step:24754 [D loss: 0.640657, acc: 64.06%] [G loss: 1.855518]\n",
      "epoch:26 step:24755 [D loss: 0.685184, acc: 55.47%] [G loss: 1.906644]\n",
      "epoch:26 step:24756 [D loss: 0.678499, acc: 53.12%] [G loss: 1.758533]\n",
      "epoch:26 step:24757 [D loss: 0.649079, acc: 68.75%] [G loss: 2.016345]\n",
      "epoch:26 step:24758 [D loss: 0.672240, acc: 64.06%] [G loss: 1.709084]\n",
      "epoch:26 step:24759 [D loss: 0.713956, acc: 52.34%] [G loss: 1.708193]\n",
      "epoch:26 step:24760 [D loss: 0.664442, acc: 58.59%] [G loss: 1.804077]\n",
      "epoch:26 step:24761 [D loss: 0.680336, acc: 56.25%] [G loss: 1.731553]\n",
      "epoch:26 step:24762 [D loss: 0.694995, acc: 58.59%] [G loss: 1.775620]\n",
      "epoch:26 step:24763 [D loss: 0.656275, acc: 62.50%] [G loss: 1.831003]\n",
      "epoch:26 step:24764 [D loss: 0.652385, acc: 60.16%] [G loss: 1.966122]\n",
      "epoch:26 step:24765 [D loss: 0.636338, acc: 65.62%] [G loss: 1.792848]\n",
      "epoch:26 step:24766 [D loss: 0.572444, acc: 78.12%] [G loss: 1.903599]\n",
      "epoch:26 step:24767 [D loss: 0.596005, acc: 73.44%] [G loss: 1.993010]\n",
      "epoch:26 step:24768 [D loss: 0.602035, acc: 67.97%] [G loss: 1.884743]\n",
      "epoch:26 step:24769 [D loss: 0.646067, acc: 60.16%] [G loss: 1.851995]\n",
      "epoch:26 step:24770 [D loss: 0.658479, acc: 60.94%] [G loss: 1.765893]\n",
      "epoch:26 step:24771 [D loss: 0.697845, acc: 57.03%] [G loss: 1.806339]\n",
      "epoch:26 step:24772 [D loss: 0.681322, acc: 57.81%] [G loss: 1.758470]\n",
      "epoch:26 step:24773 [D loss: 0.631566, acc: 65.62%] [G loss: 1.832272]\n",
      "epoch:26 step:24774 [D loss: 0.651053, acc: 55.47%] [G loss: 1.913889]\n",
      "epoch:26 step:24775 [D loss: 0.736243, acc: 60.94%] [G loss: 1.879453]\n",
      "epoch:26 step:24776 [D loss: 0.642354, acc: 67.19%] [G loss: 1.883579]\n",
      "epoch:26 step:24777 [D loss: 0.580996, acc: 72.66%] [G loss: 1.996069]\n",
      "epoch:26 step:24778 [D loss: 0.635811, acc: 62.50%] [G loss: 2.008201]\n",
      "epoch:26 step:24779 [D loss: 0.648352, acc: 61.72%] [G loss: 1.975058]\n",
      "epoch:26 step:24780 [D loss: 0.648276, acc: 62.50%] [G loss: 1.912313]\n",
      "epoch:26 step:24781 [D loss: 0.666977, acc: 59.38%] [G loss: 1.935639]\n",
      "epoch:26 step:24782 [D loss: 0.656641, acc: 66.41%] [G loss: 1.952506]\n",
      "epoch:26 step:24783 [D loss: 0.678276, acc: 64.06%] [G loss: 1.883826]\n",
      "epoch:26 step:24784 [D loss: 0.636867, acc: 67.19%] [G loss: 1.875478]\n",
      "epoch:26 step:24785 [D loss: 0.637195, acc: 60.16%] [G loss: 1.728299]\n",
      "epoch:26 step:24786 [D loss: 0.656791, acc: 59.38%] [G loss: 1.745826]\n",
      "epoch:26 step:24787 [D loss: 0.655304, acc: 61.72%] [G loss: 1.917541]\n",
      "epoch:26 step:24788 [D loss: 0.640978, acc: 61.72%] [G loss: 1.858161]\n",
      "epoch:26 step:24789 [D loss: 0.608503, acc: 70.31%] [G loss: 2.025235]\n",
      "epoch:26 step:24790 [D loss: 0.571309, acc: 74.22%] [G loss: 2.185613]\n",
      "epoch:26 step:24791 [D loss: 0.638061, acc: 56.25%] [G loss: 2.116256]\n",
      "epoch:26 step:24792 [D loss: 0.620604, acc: 68.75%] [G loss: 2.074274]\n",
      "epoch:26 step:24793 [D loss: 0.614890, acc: 64.06%] [G loss: 1.948088]\n",
      "epoch:26 step:24794 [D loss: 0.751745, acc: 52.34%] [G loss: 1.853374]\n",
      "epoch:26 step:24795 [D loss: 0.680745, acc: 56.25%] [G loss: 1.769004]\n",
      "epoch:26 step:24796 [D loss: 0.640325, acc: 63.28%] [G loss: 1.983204]\n",
      "epoch:26 step:24797 [D loss: 0.636486, acc: 65.62%] [G loss: 1.931395]\n",
      "epoch:26 step:24798 [D loss: 0.621205, acc: 63.28%] [G loss: 1.968263]\n",
      "epoch:26 step:24799 [D loss: 0.725161, acc: 49.22%] [G loss: 1.818442]\n",
      "epoch:26 step:24800 [D loss: 0.706544, acc: 55.47%] [G loss: 1.853806]\n",
      "##############\n",
      "[2.34128745 1.64456825 6.36170969 4.75187408 3.47536797 5.57452392\n",
      " 4.22736554 4.52167248 4.59672423 3.64671319]\n",
      "##########\n",
      "epoch:26 step:24801 [D loss: 0.712315, acc: 52.34%] [G loss: 1.718528]\n",
      "epoch:26 step:24802 [D loss: 0.659343, acc: 61.72%] [G loss: 1.784638]\n",
      "epoch:26 step:24803 [D loss: 0.620415, acc: 64.06%] [G loss: 1.811792]\n",
      "epoch:26 step:24804 [D loss: 0.638800, acc: 67.97%] [G loss: 1.771178]\n",
      "epoch:26 step:24805 [D loss: 0.654662, acc: 62.50%] [G loss: 1.737014]\n",
      "epoch:26 step:24806 [D loss: 0.627884, acc: 60.94%] [G loss: 1.857190]\n",
      "epoch:26 step:24807 [D loss: 0.660341, acc: 59.38%] [G loss: 1.851054]\n",
      "epoch:26 step:24808 [D loss: 0.618630, acc: 63.28%] [G loss: 1.919852]\n",
      "epoch:26 step:24809 [D loss: 0.662123, acc: 60.94%] [G loss: 1.753736]\n",
      "epoch:26 step:24810 [D loss: 0.647475, acc: 60.16%] [G loss: 1.799401]\n",
      "epoch:26 step:24811 [D loss: 0.656364, acc: 63.28%] [G loss: 1.812307]\n",
      "epoch:26 step:24812 [D loss: 0.619709, acc: 64.84%] [G loss: 1.810431]\n",
      "epoch:26 step:24813 [D loss: 0.600188, acc: 73.44%] [G loss: 1.906156]\n",
      "epoch:26 step:24814 [D loss: 0.622375, acc: 71.88%] [G loss: 1.774171]\n",
      "epoch:26 step:24815 [D loss: 0.597534, acc: 72.66%] [G loss: 1.857305]\n",
      "epoch:26 step:24816 [D loss: 0.627415, acc: 65.62%] [G loss: 1.916618]\n",
      "epoch:26 step:24817 [D loss: 0.659573, acc: 60.16%] [G loss: 1.899360]\n",
      "epoch:26 step:24818 [D loss: 0.656793, acc: 58.59%] [G loss: 1.939995]\n",
      "epoch:26 step:24819 [D loss: 0.613314, acc: 64.06%] [G loss: 2.102233]\n",
      "epoch:26 step:24820 [D loss: 0.667832, acc: 57.81%] [G loss: 1.755027]\n",
      "epoch:26 step:24821 [D loss: 0.649690, acc: 64.84%] [G loss: 1.798230]\n",
      "epoch:26 step:24822 [D loss: 0.649839, acc: 59.38%] [G loss: 1.765280]\n",
      "epoch:26 step:24823 [D loss: 0.686751, acc: 53.91%] [G loss: 1.751710]\n",
      "epoch:26 step:24824 [D loss: 0.669220, acc: 62.50%] [G loss: 1.787033]\n",
      "epoch:26 step:24825 [D loss: 0.623459, acc: 63.28%] [G loss: 1.925514]\n",
      "epoch:26 step:24826 [D loss: 0.681093, acc: 62.50%] [G loss: 1.778486]\n",
      "epoch:26 step:24827 [D loss: 0.682806, acc: 56.25%] [G loss: 1.824238]\n",
      "epoch:26 step:24828 [D loss: 0.657797, acc: 64.06%] [G loss: 1.817889]\n",
      "epoch:26 step:24829 [D loss: 0.671728, acc: 58.59%] [G loss: 1.822560]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:26 step:24830 [D loss: 0.673892, acc: 64.06%] [G loss: 2.129732]\n",
      "epoch:26 step:24831 [D loss: 0.589218, acc: 64.06%] [G loss: 1.926450]\n",
      "epoch:26 step:24832 [D loss: 0.638735, acc: 65.62%] [G loss: 2.156097]\n",
      "epoch:26 step:24833 [D loss: 0.583261, acc: 65.62%] [G loss: 2.089170]\n",
      "epoch:26 step:24834 [D loss: 0.631685, acc: 58.59%] [G loss: 2.118681]\n",
      "epoch:26 step:24835 [D loss: 0.715964, acc: 54.69%] [G loss: 1.838649]\n",
      "epoch:26 step:24836 [D loss: 0.667310, acc: 57.81%] [G loss: 1.860171]\n",
      "epoch:26 step:24837 [D loss: 0.734295, acc: 53.91%] [G loss: 1.858178]\n",
      "epoch:26 step:24838 [D loss: 0.625518, acc: 64.84%] [G loss: 2.047832]\n",
      "epoch:26 step:24839 [D loss: 0.656327, acc: 60.94%] [G loss: 1.865565]\n",
      "epoch:26 step:24840 [D loss: 0.688154, acc: 53.12%] [G loss: 1.782031]\n",
      "epoch:26 step:24841 [D loss: 0.651493, acc: 60.16%] [G loss: 2.005301]\n",
      "epoch:26 step:24842 [D loss: 0.676410, acc: 60.16%] [G loss: 1.863720]\n",
      "epoch:26 step:24843 [D loss: 0.653676, acc: 60.16%] [G loss: 1.914402]\n",
      "epoch:26 step:24844 [D loss: 0.669518, acc: 57.03%] [G loss: 1.788982]\n",
      "epoch:26 step:24845 [D loss: 0.654980, acc: 64.06%] [G loss: 1.746153]\n",
      "epoch:26 step:24846 [D loss: 0.668713, acc: 58.59%] [G loss: 1.923677]\n",
      "epoch:26 step:24847 [D loss: 0.662906, acc: 57.81%] [G loss: 1.855073]\n",
      "epoch:26 step:24848 [D loss: 0.648241, acc: 62.50%] [G loss: 1.837231]\n",
      "epoch:26 step:24849 [D loss: 0.686428, acc: 56.25%] [G loss: 1.869151]\n",
      "epoch:26 step:24850 [D loss: 0.606416, acc: 62.50%] [G loss: 1.928916]\n",
      "epoch:26 step:24851 [D loss: 0.635648, acc: 63.28%] [G loss: 1.925802]\n",
      "epoch:26 step:24852 [D loss: 0.645549, acc: 64.06%] [G loss: 1.924622]\n",
      "epoch:26 step:24853 [D loss: 0.646634, acc: 65.62%] [G loss: 1.921407]\n",
      "epoch:26 step:24854 [D loss: 0.663523, acc: 60.94%] [G loss: 1.756180]\n",
      "epoch:26 step:24855 [D loss: 0.701879, acc: 57.81%] [G loss: 1.701344]\n",
      "epoch:26 step:24856 [D loss: 0.600080, acc: 67.97%] [G loss: 1.922125]\n",
      "epoch:26 step:24857 [D loss: 0.617437, acc: 61.72%] [G loss: 2.012930]\n",
      "epoch:26 step:24858 [D loss: 0.609140, acc: 66.41%] [G loss: 1.896599]\n",
      "epoch:26 step:24859 [D loss: 0.597427, acc: 65.62%] [G loss: 1.982128]\n",
      "epoch:26 step:24860 [D loss: 0.630961, acc: 63.28%] [G loss: 2.029463]\n",
      "epoch:26 step:24861 [D loss: 0.635039, acc: 65.62%] [G loss: 2.017352]\n",
      "epoch:26 step:24862 [D loss: 0.700566, acc: 57.81%] [G loss: 1.636227]\n",
      "epoch:26 step:24863 [D loss: 0.709313, acc: 49.22%] [G loss: 1.679044]\n",
      "epoch:26 step:24864 [D loss: 0.722347, acc: 53.91%] [G loss: 1.837085]\n",
      "epoch:26 step:24865 [D loss: 0.677325, acc: 59.38%] [G loss: 1.764353]\n",
      "epoch:26 step:24866 [D loss: 0.663113, acc: 63.28%] [G loss: 2.106803]\n",
      "epoch:26 step:24867 [D loss: 0.658420, acc: 58.59%] [G loss: 1.910401]\n",
      "epoch:26 step:24868 [D loss: 0.660547, acc: 62.50%] [G loss: 1.737200]\n",
      "epoch:26 step:24869 [D loss: 0.661751, acc: 61.72%] [G loss: 1.808974]\n",
      "epoch:26 step:24870 [D loss: 0.651702, acc: 60.16%] [G loss: 1.891041]\n",
      "epoch:26 step:24871 [D loss: 0.611475, acc: 69.53%] [G loss: 1.868437]\n",
      "epoch:26 step:24872 [D loss: 0.668823, acc: 60.94%] [G loss: 1.797048]\n",
      "epoch:26 step:24873 [D loss: 0.675373, acc: 56.25%] [G loss: 1.874511]\n",
      "epoch:26 step:24874 [D loss: 0.630697, acc: 60.94%] [G loss: 1.825216]\n",
      "epoch:26 step:24875 [D loss: 0.642532, acc: 57.81%] [G loss: 1.887092]\n",
      "epoch:26 step:24876 [D loss: 0.647672, acc: 61.72%] [G loss: 1.756710]\n",
      "epoch:26 step:24877 [D loss: 0.619590, acc: 71.09%] [G loss: 1.790528]\n",
      "epoch:26 step:24878 [D loss: 0.651586, acc: 62.50%] [G loss: 1.799630]\n",
      "epoch:26 step:24879 [D loss: 0.651551, acc: 62.50%] [G loss: 1.797982]\n",
      "epoch:26 step:24880 [D loss: 0.666205, acc: 58.59%] [G loss: 1.756240]\n",
      "epoch:26 step:24881 [D loss: 0.647425, acc: 61.72%] [G loss: 1.824830]\n",
      "epoch:26 step:24882 [D loss: 0.639640, acc: 61.72%] [G loss: 1.781668]\n",
      "epoch:26 step:24883 [D loss: 0.641928, acc: 64.84%] [G loss: 1.864847]\n",
      "epoch:26 step:24884 [D loss: 0.642329, acc: 62.50%] [G loss: 1.953248]\n",
      "epoch:26 step:24885 [D loss: 0.636963, acc: 61.72%] [G loss: 2.012053]\n",
      "epoch:26 step:24886 [D loss: 0.624176, acc: 72.66%] [G loss: 1.813334]\n",
      "epoch:26 step:24887 [D loss: 0.675055, acc: 59.38%] [G loss: 1.757419]\n",
      "epoch:26 step:24888 [D loss: 0.676210, acc: 64.06%] [G loss: 1.785132]\n",
      "epoch:26 step:24889 [D loss: 0.658280, acc: 63.28%] [G loss: 1.774484]\n",
      "epoch:26 step:24890 [D loss: 0.706999, acc: 49.22%] [G loss: 1.703804]\n",
      "epoch:26 step:24891 [D loss: 0.683165, acc: 57.81%] [G loss: 1.719612]\n",
      "epoch:26 step:24892 [D loss: 0.672136, acc: 57.81%] [G loss: 1.771154]\n",
      "epoch:26 step:24893 [D loss: 0.627077, acc: 66.41%] [G loss: 1.723764]\n",
      "epoch:26 step:24894 [D loss: 0.649379, acc: 57.81%] [G loss: 1.880388]\n",
      "epoch:26 step:24895 [D loss: 0.652356, acc: 59.38%] [G loss: 1.713904]\n",
      "epoch:26 step:24896 [D loss: 0.575356, acc: 70.31%] [G loss: 1.973365]\n",
      "epoch:26 step:24897 [D loss: 0.594792, acc: 66.41%] [G loss: 1.855717]\n",
      "epoch:26 step:24898 [D loss: 0.620421, acc: 65.62%] [G loss: 2.115768]\n",
      "epoch:26 step:24899 [D loss: 0.637601, acc: 61.72%] [G loss: 1.888649]\n",
      "epoch:26 step:24900 [D loss: 0.660925, acc: 57.81%] [G loss: 1.797237]\n",
      "epoch:26 step:24901 [D loss: 0.628949, acc: 69.53%] [G loss: 1.876922]\n",
      "epoch:26 step:24902 [D loss: 0.639800, acc: 59.38%] [G loss: 1.826035]\n",
      "epoch:26 step:24903 [D loss: 0.579777, acc: 71.09%] [G loss: 1.935385]\n",
      "epoch:26 step:24904 [D loss: 0.670317, acc: 57.03%] [G loss: 1.867408]\n",
      "epoch:26 step:24905 [D loss: 0.659170, acc: 58.59%] [G loss: 1.955606]\n",
      "epoch:26 step:24906 [D loss: 0.703741, acc: 52.34%] [G loss: 1.891545]\n",
      "epoch:26 step:24907 [D loss: 0.654294, acc: 58.59%] [G loss: 1.877520]\n",
      "epoch:26 step:24908 [D loss: 0.670319, acc: 59.38%] [G loss: 1.776150]\n",
      "epoch:26 step:24909 [D loss: 0.623993, acc: 64.06%] [G loss: 1.872855]\n",
      "epoch:26 step:24910 [D loss: 0.643595, acc: 60.94%] [G loss: 1.910699]\n",
      "epoch:26 step:24911 [D loss: 0.614803, acc: 66.41%] [G loss: 1.970033]\n",
      "epoch:26 step:24912 [D loss: 0.615925, acc: 64.84%] [G loss: 1.953623]\n",
      "epoch:26 step:24913 [D loss: 0.657969, acc: 65.62%] [G loss: 2.010022]\n",
      "epoch:26 step:24914 [D loss: 0.638391, acc: 65.62%] [G loss: 2.000138]\n",
      "epoch:26 step:24915 [D loss: 0.646021, acc: 60.94%] [G loss: 1.915677]\n",
      "epoch:26 step:24916 [D loss: 0.607619, acc: 67.97%] [G loss: 2.042850]\n",
      "epoch:26 step:24917 [D loss: 0.663666, acc: 62.50%] [G loss: 1.924910]\n",
      "epoch:26 step:24918 [D loss: 0.643864, acc: 64.06%] [G loss: 1.979039]\n",
      "epoch:26 step:24919 [D loss: 0.641737, acc: 64.84%] [G loss: 1.859031]\n",
      "epoch:26 step:24920 [D loss: 0.624292, acc: 67.19%] [G loss: 2.019756]\n",
      "epoch:26 step:24921 [D loss: 0.713117, acc: 51.56%] [G loss: 1.803400]\n",
      "epoch:26 step:24922 [D loss: 0.693280, acc: 57.03%] [G loss: 1.712943]\n",
      "epoch:26 step:24923 [D loss: 0.622054, acc: 67.19%] [G loss: 1.833688]\n",
      "epoch:26 step:24924 [D loss: 0.671759, acc: 55.47%] [G loss: 1.923709]\n",
      "epoch:26 step:24925 [D loss: 0.595015, acc: 69.53%] [G loss: 2.159288]\n",
      "epoch:26 step:24926 [D loss: 0.593865, acc: 76.56%] [G loss: 2.075572]\n",
      "epoch:26 step:24927 [D loss: 0.664682, acc: 57.81%] [G loss: 1.966529]\n",
      "epoch:26 step:24928 [D loss: 0.744868, acc: 53.12%] [G loss: 1.711541]\n",
      "epoch:26 step:24929 [D loss: 0.714984, acc: 52.34%] [G loss: 1.798663]\n",
      "epoch:26 step:24930 [D loss: 0.695822, acc: 53.12%] [G loss: 1.763833]\n",
      "epoch:26 step:24931 [D loss: 0.659509, acc: 60.16%] [G loss: 1.709490]\n",
      "epoch:26 step:24932 [D loss: 0.676834, acc: 60.16%] [G loss: 1.743273]\n",
      "epoch:26 step:24933 [D loss: 0.634623, acc: 66.41%] [G loss: 1.748597]\n",
      "epoch:26 step:24934 [D loss: 0.662623, acc: 55.47%] [G loss: 1.800596]\n",
      "epoch:26 step:24935 [D loss: 0.645244, acc: 61.72%] [G loss: 1.763105]\n",
      "epoch:26 step:24936 [D loss: 0.630359, acc: 68.75%] [G loss: 1.891720]\n",
      "epoch:26 step:24937 [D loss: 0.665196, acc: 60.94%] [G loss: 1.723659]\n",
      "epoch:26 step:24938 [D loss: 0.655470, acc: 56.25%] [G loss: 1.805922]\n",
      "epoch:26 step:24939 [D loss: 0.653134, acc: 60.94%] [G loss: 1.782621]\n",
      "epoch:26 step:24940 [D loss: 0.647572, acc: 57.81%] [G loss: 1.698773]\n",
      "epoch:26 step:24941 [D loss: 0.656742, acc: 57.03%] [G loss: 1.756232]\n",
      "epoch:26 step:24942 [D loss: 0.685760, acc: 58.59%] [G loss: 1.839932]\n",
      "epoch:26 step:24943 [D loss: 0.661104, acc: 60.94%] [G loss: 1.781497]\n",
      "epoch:26 step:24944 [D loss: 0.645507, acc: 65.62%] [G loss: 1.766997]\n",
      "epoch:26 step:24945 [D loss: 0.688892, acc: 55.47%] [G loss: 1.742939]\n",
      "epoch:26 step:24946 [D loss: 0.662282, acc: 60.16%] [G loss: 1.661274]\n",
      "epoch:26 step:24947 [D loss: 0.644840, acc: 63.28%] [G loss: 1.773806]\n",
      "epoch:26 step:24948 [D loss: 0.668014, acc: 55.47%] [G loss: 1.835806]\n",
      "epoch:26 step:24949 [D loss: 0.687814, acc: 53.12%] [G loss: 1.718250]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:26 step:24950 [D loss: 0.678477, acc: 60.94%] [G loss: 1.958443]\n",
      "epoch:26 step:24951 [D loss: 0.641514, acc: 63.28%] [G loss: 1.809092]\n",
      "epoch:26 step:24952 [D loss: 0.683165, acc: 54.69%] [G loss: 1.775060]\n",
      "epoch:26 step:24953 [D loss: 0.642393, acc: 58.59%] [G loss: 1.826023]\n",
      "epoch:26 step:24954 [D loss: 0.628266, acc: 66.41%] [G loss: 1.872359]\n",
      "epoch:26 step:24955 [D loss: 0.675551, acc: 57.81%] [G loss: 1.781132]\n",
      "epoch:26 step:24956 [D loss: 0.667831, acc: 59.38%] [G loss: 1.797414]\n",
      "epoch:26 step:24957 [D loss: 0.635808, acc: 63.28%] [G loss: 1.836280]\n",
      "epoch:26 step:24958 [D loss: 0.675952, acc: 54.69%] [G loss: 1.766991]\n",
      "epoch:26 step:24959 [D loss: 0.652529, acc: 64.06%] [G loss: 1.749133]\n",
      "epoch:26 step:24960 [D loss: 0.643525, acc: 61.72%] [G loss: 1.782354]\n",
      "epoch:26 step:24961 [D loss: 0.672898, acc: 53.91%] [G loss: 1.932174]\n",
      "epoch:26 step:24962 [D loss: 0.634888, acc: 68.75%] [G loss: 1.918361]\n",
      "epoch:26 step:24963 [D loss: 0.632290, acc: 69.53%] [G loss: 1.799890]\n",
      "epoch:26 step:24964 [D loss: 0.642943, acc: 62.50%] [G loss: 1.865626]\n",
      "epoch:26 step:24965 [D loss: 0.635524, acc: 67.19%] [G loss: 1.904017]\n",
      "epoch:26 step:24966 [D loss: 0.637491, acc: 59.38%] [G loss: 1.857296]\n",
      "epoch:26 step:24967 [D loss: 0.599530, acc: 70.31%] [G loss: 1.913947]\n",
      "epoch:26 step:24968 [D loss: 0.684729, acc: 57.81%] [G loss: 1.807646]\n",
      "epoch:26 step:24969 [D loss: 0.637776, acc: 65.62%] [G loss: 1.781609]\n",
      "epoch:26 step:24970 [D loss: 0.631369, acc: 69.53%] [G loss: 1.819205]\n",
      "epoch:26 step:24971 [D loss: 0.661129, acc: 57.03%] [G loss: 1.932714]\n",
      "epoch:26 step:24972 [D loss: 0.678919, acc: 51.56%] [G loss: 1.869717]\n",
      "epoch:26 step:24973 [D loss: 0.631810, acc: 61.72%] [G loss: 1.838457]\n",
      "epoch:26 step:24974 [D loss: 0.653945, acc: 58.59%] [G loss: 1.929473]\n",
      "epoch:26 step:24975 [D loss: 0.627915, acc: 58.59%] [G loss: 1.828143]\n",
      "epoch:26 step:24976 [D loss: 0.688208, acc: 55.47%] [G loss: 1.645511]\n",
      "epoch:26 step:24977 [D loss: 0.668969, acc: 57.03%] [G loss: 1.817293]\n",
      "epoch:26 step:24978 [D loss: 0.684819, acc: 51.56%] [G loss: 1.896884]\n",
      "epoch:26 step:24979 [D loss: 0.662054, acc: 55.47%] [G loss: 1.791555]\n",
      "epoch:26 step:24980 [D loss: 0.627145, acc: 64.84%] [G loss: 1.893380]\n",
      "epoch:26 step:24981 [D loss: 0.643730, acc: 62.50%] [G loss: 1.825978]\n",
      "epoch:26 step:24982 [D loss: 0.630985, acc: 61.72%] [G loss: 1.756379]\n",
      "epoch:26 step:24983 [D loss: 0.636523, acc: 67.97%] [G loss: 1.804137]\n",
      "epoch:26 step:24984 [D loss: 0.653710, acc: 57.81%] [G loss: 1.913088]\n",
      "epoch:26 step:24985 [D loss: 0.658561, acc: 58.59%] [G loss: 1.805940]\n",
      "epoch:26 step:24986 [D loss: 0.568045, acc: 68.75%] [G loss: 1.941574]\n",
      "epoch:26 step:24987 [D loss: 0.657480, acc: 64.84%] [G loss: 1.781043]\n",
      "epoch:26 step:24988 [D loss: 0.670986, acc: 56.25%] [G loss: 1.818513]\n",
      "epoch:26 step:24989 [D loss: 0.679353, acc: 53.91%] [G loss: 1.898600]\n",
      "epoch:26 step:24990 [D loss: 0.652080, acc: 64.06%] [G loss: 1.878819]\n",
      "epoch:26 step:24991 [D loss: 0.656194, acc: 60.94%] [G loss: 1.799803]\n",
      "epoch:26 step:24992 [D loss: 0.628217, acc: 61.72%] [G loss: 1.862278]\n",
      "epoch:26 step:24993 [D loss: 0.626812, acc: 63.28%] [G loss: 1.897050]\n",
      "epoch:26 step:24994 [D loss: 0.611386, acc: 64.84%] [G loss: 1.905639]\n",
      "epoch:26 step:24995 [D loss: 0.661307, acc: 59.38%] [G loss: 1.870392]\n",
      "epoch:26 step:24996 [D loss: 0.639974, acc: 62.50%] [G loss: 1.903689]\n",
      "epoch:26 step:24997 [D loss: 0.654319, acc: 57.81%] [G loss: 1.895080]\n",
      "epoch:26 step:24998 [D loss: 0.673779, acc: 57.81%] [G loss: 1.908693]\n",
      "epoch:26 step:24999 [D loss: 0.661978, acc: 62.50%] [G loss: 2.030357]\n",
      "epoch:26 step:25000 [D loss: 0.624589, acc: 63.28%] [G loss: 1.955419]\n",
      "##############\n",
      "[2.59818755 1.52271289 6.22725827 4.8959021  3.54705173 5.500539\n",
      " 4.42357528 4.6754193  4.72102712 3.81892647]\n",
      "##########\n",
      "epoch:26 step:25001 [D loss: 0.655288, acc: 60.16%] [G loss: 1.816616]\n",
      "epoch:26 step:25002 [D loss: 0.676227, acc: 57.03%] [G loss: 1.874487]\n",
      "epoch:26 step:25003 [D loss: 0.599584, acc: 65.62%] [G loss: 2.075369]\n",
      "epoch:26 step:25004 [D loss: 0.627545, acc: 64.84%] [G loss: 1.891575]\n",
      "epoch:26 step:25005 [D loss: 0.586891, acc: 69.53%] [G loss: 2.004107]\n",
      "epoch:26 step:25006 [D loss: 0.599291, acc: 66.41%] [G loss: 1.893541]\n",
      "epoch:26 step:25007 [D loss: 0.680801, acc: 59.38%] [G loss: 1.916446]\n",
      "epoch:26 step:25008 [D loss: 0.672485, acc: 61.72%] [G loss: 2.002738]\n",
      "epoch:26 step:25009 [D loss: 0.625867, acc: 64.84%] [G loss: 2.087318]\n",
      "epoch:26 step:25010 [D loss: 0.602247, acc: 70.31%] [G loss: 2.192995]\n",
      "epoch:26 step:25011 [D loss: 0.626980, acc: 66.41%] [G loss: 1.943872]\n",
      "epoch:26 step:25012 [D loss: 0.642050, acc: 67.19%] [G loss: 1.958549]\n",
      "epoch:26 step:25013 [D loss: 0.620948, acc: 63.28%] [G loss: 1.810961]\n",
      "epoch:26 step:25014 [D loss: 0.649851, acc: 62.50%] [G loss: 1.930400]\n",
      "epoch:26 step:25015 [D loss: 0.668202, acc: 65.62%] [G loss: 1.872710]\n",
      "epoch:26 step:25016 [D loss: 0.599796, acc: 64.84%] [G loss: 2.149199]\n",
      "epoch:26 step:25017 [D loss: 0.695644, acc: 60.16%] [G loss: 1.901917]\n",
      "epoch:26 step:25018 [D loss: 0.722836, acc: 51.56%] [G loss: 1.779540]\n",
      "epoch:26 step:25019 [D loss: 0.679913, acc: 61.72%] [G loss: 1.716216]\n",
      "epoch:26 step:25020 [D loss: 0.718367, acc: 55.47%] [G loss: 1.752934]\n",
      "epoch:26 step:25021 [D loss: 0.651263, acc: 61.72%] [G loss: 1.765206]\n",
      "epoch:26 step:25022 [D loss: 0.662337, acc: 57.81%] [G loss: 1.829214]\n",
      "epoch:26 step:25023 [D loss: 0.624542, acc: 69.53%] [G loss: 1.844203]\n",
      "epoch:26 step:25024 [D loss: 0.634370, acc: 63.28%] [G loss: 1.893893]\n",
      "epoch:26 step:25025 [D loss: 0.644450, acc: 64.84%] [G loss: 1.777856]\n",
      "epoch:26 step:25026 [D loss: 0.673474, acc: 63.28%] [G loss: 1.827819]\n",
      "epoch:26 step:25027 [D loss: 0.632585, acc: 65.62%] [G loss: 1.862516]\n",
      "epoch:26 step:25028 [D loss: 0.681508, acc: 61.72%] [G loss: 1.764518]\n",
      "epoch:26 step:25029 [D loss: 0.658419, acc: 62.50%] [G loss: 1.739207]\n",
      "epoch:26 step:25030 [D loss: 0.662979, acc: 57.03%] [G loss: 1.788062]\n",
      "epoch:26 step:25031 [D loss: 0.684470, acc: 57.03%] [G loss: 1.762482]\n",
      "epoch:26 step:25032 [D loss: 0.661812, acc: 62.50%] [G loss: 1.839745]\n",
      "epoch:26 step:25033 [D loss: 0.634451, acc: 67.19%] [G loss: 1.842382]\n",
      "epoch:26 step:25034 [D loss: 0.607102, acc: 68.75%] [G loss: 1.943458]\n",
      "epoch:26 step:25035 [D loss: 0.631424, acc: 65.62%] [G loss: 1.863641]\n",
      "epoch:26 step:25036 [D loss: 0.604589, acc: 64.06%] [G loss: 1.839925]\n",
      "epoch:26 step:25037 [D loss: 0.679154, acc: 53.91%] [G loss: 1.850965]\n",
      "epoch:26 step:25038 [D loss: 0.643764, acc: 61.72%] [G loss: 1.772453]\n",
      "epoch:26 step:25039 [D loss: 0.648337, acc: 65.62%] [G loss: 1.977127]\n",
      "epoch:26 step:25040 [D loss: 0.647000, acc: 63.28%] [G loss: 1.704237]\n",
      "epoch:26 step:25041 [D loss: 0.607057, acc: 70.31%] [G loss: 1.870924]\n",
      "epoch:26 step:25042 [D loss: 0.623379, acc: 66.41%] [G loss: 1.870617]\n",
      "epoch:26 step:25043 [D loss: 0.668467, acc: 61.72%] [G loss: 2.005727]\n",
      "epoch:26 step:25044 [D loss: 0.642112, acc: 60.94%] [G loss: 1.824627]\n",
      "epoch:26 step:25045 [D loss: 0.648267, acc: 65.62%] [G loss: 1.814835]\n",
      "epoch:26 step:25046 [D loss: 0.662689, acc: 59.38%] [G loss: 1.739215]\n",
      "epoch:26 step:25047 [D loss: 0.611343, acc: 71.88%] [G loss: 1.845013]\n",
      "epoch:26 step:25048 [D loss: 0.662914, acc: 60.94%] [G loss: 2.031445]\n",
      "epoch:26 step:25049 [D loss: 0.668601, acc: 55.47%] [G loss: 1.828980]\n",
      "epoch:26 step:25050 [D loss: 0.634462, acc: 64.84%] [G loss: 1.948349]\n",
      "epoch:26 step:25051 [D loss: 0.646501, acc: 59.38%] [G loss: 1.899015]\n",
      "epoch:26 step:25052 [D loss: 0.634595, acc: 65.62%] [G loss: 1.791216]\n",
      "epoch:26 step:25053 [D loss: 0.584736, acc: 71.09%] [G loss: 2.001838]\n",
      "epoch:26 step:25054 [D loss: 0.626489, acc: 62.50%] [G loss: 2.064397]\n",
      "epoch:26 step:25055 [D loss: 0.621323, acc: 61.72%] [G loss: 2.123492]\n",
      "epoch:26 step:25056 [D loss: 0.600008, acc: 71.88%] [G loss: 2.243048]\n",
      "epoch:26 step:25057 [D loss: 0.673808, acc: 58.59%] [G loss: 1.943153]\n",
      "epoch:26 step:25058 [D loss: 0.659748, acc: 59.38%] [G loss: 1.828741]\n",
      "epoch:26 step:25059 [D loss: 0.629766, acc: 60.16%] [G loss: 1.766470]\n",
      "epoch:26 step:25060 [D loss: 0.692533, acc: 56.25%] [G loss: 1.914464]\n",
      "epoch:26 step:25061 [D loss: 0.674863, acc: 60.16%] [G loss: 1.953138]\n",
      "epoch:26 step:25062 [D loss: 0.638253, acc: 64.84%] [G loss: 1.906755]\n",
      "epoch:26 step:25063 [D loss: 0.672969, acc: 57.81%] [G loss: 1.864411]\n",
      "epoch:26 step:25064 [D loss: 0.643252, acc: 57.81%] [G loss: 1.838977]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:26 step:25065 [D loss: 0.663189, acc: 61.72%] [G loss: 1.721153]\n",
      "epoch:26 step:25066 [D loss: 0.665019, acc: 58.59%] [G loss: 1.808652]\n",
      "epoch:26 step:25067 [D loss: 0.691778, acc: 57.81%] [G loss: 1.861722]\n",
      "epoch:26 step:25068 [D loss: 0.608437, acc: 70.31%] [G loss: 1.850766]\n",
      "epoch:26 step:25069 [D loss: 0.608173, acc: 67.97%] [G loss: 2.013101]\n",
      "epoch:26 step:25070 [D loss: 0.635433, acc: 64.84%] [G loss: 1.933197]\n",
      "epoch:26 step:25071 [D loss: 0.614107, acc: 68.75%] [G loss: 1.961690]\n",
      "epoch:26 step:25072 [D loss: 0.641921, acc: 54.69%] [G loss: 1.742906]\n",
      "epoch:26 step:25073 [D loss: 0.627285, acc: 67.19%] [G loss: 2.068455]\n",
      "epoch:26 step:25074 [D loss: 0.648519, acc: 63.28%] [G loss: 1.887850]\n",
      "epoch:26 step:25075 [D loss: 0.702701, acc: 57.03%] [G loss: 1.910433]\n",
      "epoch:26 step:25076 [D loss: 0.596993, acc: 67.19%] [G loss: 1.918580]\n",
      "epoch:26 step:25077 [D loss: 0.621289, acc: 69.53%] [G loss: 2.030578]\n",
      "epoch:26 step:25078 [D loss: 0.659415, acc: 54.69%] [G loss: 1.760187]\n",
      "epoch:26 step:25079 [D loss: 0.684764, acc: 64.06%] [G loss: 1.913415]\n",
      "epoch:26 step:25080 [D loss: 0.646549, acc: 64.06%] [G loss: 1.897659]\n",
      "epoch:26 step:25081 [D loss: 0.655476, acc: 55.47%] [G loss: 2.018021]\n",
      "epoch:26 step:25082 [D loss: 0.602739, acc: 67.19%] [G loss: 2.060174]\n",
      "epoch:26 step:25083 [D loss: 0.608582, acc: 69.53%] [G loss: 1.883646]\n",
      "epoch:26 step:25084 [D loss: 0.673169, acc: 61.72%] [G loss: 1.921094]\n",
      "epoch:26 step:25085 [D loss: 0.689477, acc: 55.47%] [G loss: 1.916089]\n",
      "epoch:26 step:25086 [D loss: 0.611027, acc: 66.41%] [G loss: 2.022176]\n",
      "epoch:26 step:25087 [D loss: 0.613389, acc: 62.50%] [G loss: 1.986832]\n",
      "epoch:26 step:25088 [D loss: 0.702899, acc: 57.81%] [G loss: 1.948382]\n",
      "epoch:26 step:25089 [D loss: 0.656015, acc: 61.72%] [G loss: 1.853164]\n",
      "epoch:26 step:25090 [D loss: 0.580436, acc: 70.31%] [G loss: 2.059117]\n",
      "epoch:26 step:25091 [D loss: 0.631079, acc: 61.72%] [G loss: 1.962004]\n",
      "epoch:26 step:25092 [D loss: 0.698467, acc: 53.91%] [G loss: 1.853540]\n",
      "epoch:26 step:25093 [D loss: 0.634159, acc: 62.50%] [G loss: 1.802953]\n",
      "epoch:26 step:25094 [D loss: 0.617301, acc: 67.19%] [G loss: 1.886559]\n",
      "epoch:26 step:25095 [D loss: 0.587240, acc: 67.97%] [G loss: 2.045746]\n",
      "epoch:26 step:25096 [D loss: 0.716541, acc: 52.34%] [G loss: 1.859278]\n",
      "epoch:26 step:25097 [D loss: 0.640331, acc: 62.50%] [G loss: 1.921179]\n",
      "epoch:26 step:25098 [D loss: 0.597508, acc: 66.41%] [G loss: 1.946845]\n",
      "epoch:26 step:25099 [D loss: 0.647274, acc: 60.94%] [G loss: 1.891688]\n",
      "epoch:26 step:25100 [D loss: 0.633442, acc: 64.06%] [G loss: 1.888941]\n",
      "epoch:26 step:25101 [D loss: 0.742452, acc: 53.12%] [G loss: 1.769585]\n",
      "epoch:26 step:25102 [D loss: 0.636533, acc: 67.97%] [G loss: 1.877382]\n",
      "epoch:26 step:25103 [D loss: 0.674433, acc: 61.72%] [G loss: 1.868830]\n",
      "epoch:26 step:25104 [D loss: 0.717677, acc: 52.34%] [G loss: 1.880632]\n",
      "epoch:26 step:25105 [D loss: 0.630424, acc: 61.72%] [G loss: 1.954314]\n",
      "epoch:26 step:25106 [D loss: 0.640569, acc: 64.84%] [G loss: 2.008634]\n",
      "epoch:26 step:25107 [D loss: 0.626464, acc: 64.06%] [G loss: 1.865838]\n",
      "epoch:26 step:25108 [D loss: 0.635006, acc: 62.50%] [G loss: 1.965348]\n",
      "epoch:26 step:25109 [D loss: 0.618703, acc: 67.19%] [G loss: 1.935900]\n",
      "epoch:26 step:25110 [D loss: 0.646133, acc: 64.84%] [G loss: 1.818883]\n",
      "epoch:26 step:25111 [D loss: 0.632347, acc: 63.28%] [G loss: 1.784950]\n",
      "epoch:26 step:25112 [D loss: 0.712050, acc: 58.59%] [G loss: 1.929641]\n",
      "epoch:26 step:25113 [D loss: 0.646448, acc: 61.72%] [G loss: 1.692299]\n",
      "epoch:26 step:25114 [D loss: 0.683920, acc: 60.16%] [G loss: 1.705652]\n",
      "epoch:26 step:25115 [D loss: 0.623440, acc: 64.06%] [G loss: 1.856961]\n",
      "epoch:26 step:25116 [D loss: 0.648329, acc: 61.72%] [G loss: 1.989489]\n",
      "epoch:26 step:25117 [D loss: 0.655234, acc: 61.72%] [G loss: 1.741424]\n",
      "epoch:26 step:25118 [D loss: 0.640132, acc: 59.38%] [G loss: 1.880399]\n",
      "epoch:26 step:25119 [D loss: 0.649831, acc: 58.59%] [G loss: 1.865181]\n",
      "epoch:26 step:25120 [D loss: 0.646310, acc: 65.62%] [G loss: 1.821378]\n",
      "epoch:26 step:25121 [D loss: 0.625186, acc: 64.06%] [G loss: 1.706438]\n",
      "epoch:26 step:25122 [D loss: 0.685028, acc: 56.25%] [G loss: 1.812311]\n",
      "epoch:26 step:25123 [D loss: 0.644853, acc: 59.38%] [G loss: 1.799107]\n",
      "epoch:26 step:25124 [D loss: 0.645114, acc: 60.16%] [G loss: 1.887531]\n",
      "epoch:26 step:25125 [D loss: 0.679784, acc: 61.72%] [G loss: 1.736495]\n",
      "epoch:26 step:25126 [D loss: 0.644000, acc: 65.62%] [G loss: 1.853095]\n",
      "epoch:26 step:25127 [D loss: 0.712616, acc: 52.34%] [G loss: 1.769178]\n",
      "epoch:26 step:25128 [D loss: 0.641596, acc: 64.84%] [G loss: 1.848245]\n",
      "epoch:26 step:25129 [D loss: 0.636308, acc: 58.59%] [G loss: 1.866802]\n",
      "epoch:26 step:25130 [D loss: 0.648725, acc: 60.16%] [G loss: 1.849342]\n",
      "epoch:26 step:25131 [D loss: 0.670496, acc: 56.25%] [G loss: 1.900640]\n",
      "epoch:26 step:25132 [D loss: 0.612953, acc: 60.94%] [G loss: 1.925601]\n",
      "epoch:26 step:25133 [D loss: 0.646867, acc: 58.59%] [G loss: 1.767538]\n",
      "epoch:26 step:25134 [D loss: 0.658309, acc: 60.94%] [G loss: 1.861276]\n",
      "epoch:26 step:25135 [D loss: 0.632010, acc: 60.16%] [G loss: 1.932854]\n",
      "epoch:26 step:25136 [D loss: 0.617401, acc: 67.19%] [G loss: 2.299050]\n",
      "epoch:26 step:25137 [D loss: 0.647493, acc: 67.97%] [G loss: 2.176011]\n",
      "epoch:26 step:25138 [D loss: 0.626652, acc: 63.28%] [G loss: 1.979520]\n",
      "epoch:26 step:25139 [D loss: 0.633318, acc: 67.19%] [G loss: 1.959995]\n",
      "epoch:26 step:25140 [D loss: 0.631260, acc: 64.06%] [G loss: 2.055876]\n",
      "epoch:26 step:25141 [D loss: 0.672159, acc: 58.59%] [G loss: 1.863899]\n",
      "epoch:26 step:25142 [D loss: 0.649455, acc: 64.06%] [G loss: 1.982998]\n",
      "epoch:26 step:25143 [D loss: 0.613580, acc: 66.41%] [G loss: 2.177324]\n",
      "epoch:26 step:25144 [D loss: 0.579440, acc: 67.97%] [G loss: 2.191878]\n",
      "epoch:26 step:25145 [D loss: 0.618506, acc: 66.41%] [G loss: 1.912431]\n",
      "epoch:26 step:25146 [D loss: 0.657066, acc: 53.91%] [G loss: 1.794633]\n",
      "epoch:26 step:25147 [D loss: 0.656843, acc: 58.59%] [G loss: 1.835783]\n",
      "epoch:26 step:25148 [D loss: 0.604256, acc: 70.31%] [G loss: 2.063523]\n",
      "epoch:26 step:25149 [D loss: 0.695526, acc: 60.16%] [G loss: 1.750243]\n",
      "epoch:26 step:25150 [D loss: 0.628864, acc: 64.06%] [G loss: 1.914144]\n",
      "epoch:26 step:25151 [D loss: 0.673073, acc: 58.59%] [G loss: 1.879349]\n",
      "epoch:26 step:25152 [D loss: 0.641421, acc: 64.06%] [G loss: 2.039145]\n",
      "epoch:26 step:25153 [D loss: 0.598486, acc: 67.97%] [G loss: 2.073816]\n",
      "epoch:26 step:25154 [D loss: 0.586970, acc: 67.97%] [G loss: 2.084474]\n",
      "epoch:26 step:25155 [D loss: 0.614323, acc: 68.75%] [G loss: 2.033204]\n",
      "epoch:26 step:25156 [D loss: 0.724280, acc: 50.78%] [G loss: 1.776817]\n",
      "epoch:26 step:25157 [D loss: 0.666848, acc: 58.59%] [G loss: 1.829664]\n",
      "epoch:26 step:25158 [D loss: 0.634859, acc: 62.50%] [G loss: 1.976076]\n",
      "epoch:26 step:25159 [D loss: 0.647752, acc: 63.28%] [G loss: 1.840666]\n",
      "epoch:26 step:25160 [D loss: 0.694816, acc: 57.03%] [G loss: 1.766483]\n",
      "epoch:26 step:25161 [D loss: 0.651915, acc: 64.84%] [G loss: 1.767213]\n",
      "epoch:26 step:25162 [D loss: 0.696491, acc: 56.25%] [G loss: 1.823464]\n",
      "epoch:26 step:25163 [D loss: 0.644436, acc: 61.72%] [G loss: 1.816831]\n",
      "epoch:26 step:25164 [D loss: 0.634240, acc: 60.94%] [G loss: 1.855299]\n",
      "epoch:26 step:25165 [D loss: 0.642225, acc: 60.94%] [G loss: 1.867946]\n",
      "epoch:26 step:25166 [D loss: 0.634738, acc: 64.06%] [G loss: 1.996250]\n",
      "epoch:26 step:25167 [D loss: 0.685684, acc: 57.81%] [G loss: 1.793274]\n",
      "epoch:26 step:25168 [D loss: 0.607346, acc: 66.41%] [G loss: 1.921517]\n",
      "epoch:26 step:25169 [D loss: 0.605758, acc: 67.19%] [G loss: 1.861051]\n",
      "epoch:26 step:25170 [D loss: 0.649552, acc: 60.94%] [G loss: 1.883704]\n",
      "epoch:26 step:25171 [D loss: 0.611675, acc: 67.19%] [G loss: 2.011375]\n",
      "epoch:26 step:25172 [D loss: 0.664435, acc: 55.47%] [G loss: 1.691066]\n",
      "epoch:26 step:25173 [D loss: 0.678619, acc: 53.91%] [G loss: 1.799286]\n",
      "epoch:26 step:25174 [D loss: 0.669478, acc: 61.72%] [G loss: 1.754754]\n",
      "epoch:26 step:25175 [D loss: 0.628547, acc: 60.94%] [G loss: 1.960971]\n",
      "epoch:26 step:25176 [D loss: 0.602983, acc: 70.31%] [G loss: 1.844762]\n",
      "epoch:26 step:25177 [D loss: 0.621366, acc: 67.19%] [G loss: 2.203268]\n",
      "epoch:26 step:25178 [D loss: 0.655994, acc: 60.16%] [G loss: 2.056264]\n",
      "epoch:26 step:25179 [D loss: 0.651802, acc: 64.06%] [G loss: 1.924695]\n",
      "epoch:26 step:25180 [D loss: 0.670316, acc: 60.16%] [G loss: 1.809659]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:26 step:25181 [D loss: 0.695156, acc: 57.81%] [G loss: 1.890638]\n",
      "epoch:26 step:25182 [D loss: 0.700901, acc: 52.34%] [G loss: 1.730573]\n",
      "epoch:26 step:25183 [D loss: 0.711076, acc: 60.16%] [G loss: 1.678486]\n",
      "epoch:26 step:25184 [D loss: 0.658618, acc: 62.50%] [G loss: 1.959451]\n",
      "epoch:26 step:25185 [D loss: 0.597284, acc: 71.09%] [G loss: 1.963618]\n",
      "epoch:26 step:25186 [D loss: 0.671228, acc: 60.16%] [G loss: 1.692605]\n",
      "epoch:26 step:25187 [D loss: 0.623571, acc: 63.28%] [G loss: 1.939842]\n",
      "epoch:26 step:25188 [D loss: 0.650885, acc: 59.38%] [G loss: 1.853184]\n",
      "epoch:26 step:25189 [D loss: 0.725133, acc: 50.00%] [G loss: 1.836323]\n",
      "epoch:26 step:25190 [D loss: 0.648685, acc: 61.72%] [G loss: 1.728489]\n",
      "epoch:26 step:25191 [D loss: 0.657010, acc: 60.16%] [G loss: 1.672160]\n",
      "epoch:26 step:25192 [D loss: 0.655813, acc: 59.38%] [G loss: 1.736884]\n",
      "epoch:26 step:25193 [D loss: 0.645944, acc: 64.06%] [G loss: 1.818569]\n",
      "epoch:26 step:25194 [D loss: 0.664039, acc: 60.94%] [G loss: 1.872754]\n",
      "epoch:26 step:25195 [D loss: 0.662753, acc: 59.38%] [G loss: 1.841785]\n",
      "epoch:26 step:25196 [D loss: 0.618611, acc: 64.84%] [G loss: 1.929932]\n",
      "epoch:26 step:25197 [D loss: 0.585626, acc: 75.00%] [G loss: 1.867091]\n",
      "epoch:26 step:25198 [D loss: 0.643161, acc: 65.62%] [G loss: 1.855881]\n",
      "epoch:26 step:25199 [D loss: 0.599671, acc: 67.97%] [G loss: 1.872855]\n",
      "epoch:26 step:25200 [D loss: 0.646069, acc: 68.75%] [G loss: 1.953685]\n",
      "##############\n",
      "[2.3965111  1.62429208 6.14820989 4.66005175 3.48080328 5.61505894\n",
      " 4.33760951 4.54913352 4.44157357 3.37923951]\n",
      "##########\n",
      "epoch:26 step:25201 [D loss: 0.664182, acc: 58.59%] [G loss: 1.909527]\n",
      "epoch:26 step:25202 [D loss: 0.633046, acc: 64.84%] [G loss: 1.984853]\n",
      "epoch:26 step:25203 [D loss: 0.647280, acc: 63.28%] [G loss: 1.993326]\n",
      "epoch:26 step:25204 [D loss: 0.628076, acc: 68.75%] [G loss: 1.940057]\n",
      "epoch:26 step:25205 [D loss: 0.613933, acc: 63.28%] [G loss: 1.864736]\n",
      "epoch:26 step:25206 [D loss: 0.637927, acc: 63.28%] [G loss: 1.860195]\n",
      "epoch:26 step:25207 [D loss: 0.578501, acc: 67.19%] [G loss: 1.931233]\n",
      "epoch:26 step:25208 [D loss: 0.661690, acc: 57.81%] [G loss: 1.879103]\n",
      "epoch:26 step:25209 [D loss: 0.643212, acc: 65.62%] [G loss: 2.011020]\n",
      "epoch:26 step:25210 [D loss: 0.676257, acc: 57.81%] [G loss: 1.986964]\n",
      "epoch:26 step:25211 [D loss: 0.638447, acc: 64.06%] [G loss: 1.979938]\n",
      "epoch:26 step:25212 [D loss: 0.669034, acc: 57.81%] [G loss: 1.894447]\n",
      "epoch:26 step:25213 [D loss: 0.675171, acc: 58.59%] [G loss: 1.857514]\n",
      "epoch:26 step:25214 [D loss: 0.631340, acc: 64.06%] [G loss: 1.912931]\n",
      "epoch:26 step:25215 [D loss: 0.702216, acc: 51.56%] [G loss: 1.914691]\n",
      "epoch:26 step:25216 [D loss: 0.615367, acc: 63.28%] [G loss: 1.752739]\n",
      "epoch:26 step:25217 [D loss: 0.627999, acc: 64.06%] [G loss: 1.912436]\n",
      "epoch:26 step:25218 [D loss: 0.652486, acc: 60.16%] [G loss: 1.946073]\n",
      "epoch:26 step:25219 [D loss: 0.651812, acc: 64.84%] [G loss: 1.821333]\n",
      "epoch:26 step:25220 [D loss: 0.657689, acc: 60.94%] [G loss: 1.904700]\n",
      "epoch:26 step:25221 [D loss: 0.718380, acc: 54.69%] [G loss: 1.743476]\n",
      "epoch:26 step:25222 [D loss: 0.672555, acc: 56.25%] [G loss: 1.955742]\n",
      "epoch:26 step:25223 [D loss: 0.693441, acc: 56.25%] [G loss: 1.762776]\n",
      "epoch:26 step:25224 [D loss: 0.660729, acc: 60.16%] [G loss: 1.927002]\n",
      "epoch:26 step:25225 [D loss: 0.660013, acc: 64.84%] [G loss: 1.828615]\n",
      "epoch:26 step:25226 [D loss: 0.658187, acc: 57.03%] [G loss: 1.858570]\n",
      "epoch:26 step:25227 [D loss: 0.702528, acc: 56.25%] [G loss: 1.753936]\n",
      "epoch:26 step:25228 [D loss: 0.644871, acc: 61.72%] [G loss: 1.915667]\n",
      "epoch:26 step:25229 [D loss: 0.659863, acc: 57.81%] [G loss: 1.773576]\n",
      "epoch:26 step:25230 [D loss: 0.659709, acc: 64.84%] [G loss: 1.701334]\n",
      "epoch:26 step:25231 [D loss: 0.701345, acc: 55.47%] [G loss: 1.886415]\n",
      "epoch:26 step:25232 [D loss: 0.707896, acc: 58.59%] [G loss: 1.773236]\n",
      "epoch:26 step:25233 [D loss: 0.650099, acc: 64.06%] [G loss: 1.753046]\n",
      "epoch:26 step:25234 [D loss: 0.661661, acc: 57.03%] [G loss: 1.772678]\n",
      "epoch:26 step:25235 [D loss: 0.677965, acc: 61.72%] [G loss: 1.793713]\n",
      "epoch:26 step:25236 [D loss: 0.687876, acc: 62.50%] [G loss: 1.679055]\n",
      "epoch:26 step:25237 [D loss: 0.648551, acc: 62.50%] [G loss: 1.725257]\n",
      "epoch:26 step:25238 [D loss: 0.633211, acc: 68.75%] [G loss: 1.867805]\n",
      "epoch:26 step:25239 [D loss: 0.640160, acc: 67.19%] [G loss: 1.797736]\n",
      "epoch:26 step:25240 [D loss: 0.671538, acc: 58.59%] [G loss: 1.890444]\n",
      "epoch:26 step:25241 [D loss: 0.658353, acc: 64.84%] [G loss: 1.854250]\n",
      "epoch:26 step:25242 [D loss: 0.637613, acc: 65.62%] [G loss: 1.837344]\n",
      "epoch:26 step:25243 [D loss: 0.640446, acc: 66.41%] [G loss: 1.967559]\n",
      "epoch:26 step:25244 [D loss: 0.672856, acc: 56.25%] [G loss: 1.813214]\n",
      "epoch:26 step:25245 [D loss: 0.592212, acc: 68.75%] [G loss: 1.862590]\n",
      "epoch:26 step:25246 [D loss: 0.619117, acc: 67.97%] [G loss: 1.919223]\n",
      "epoch:26 step:25247 [D loss: 0.648290, acc: 60.94%] [G loss: 1.850419]\n",
      "epoch:26 step:25248 [D loss: 0.608889, acc: 67.19%] [G loss: 1.999248]\n",
      "epoch:26 step:25249 [D loss: 0.677185, acc: 57.81%] [G loss: 1.815467]\n",
      "epoch:26 step:25250 [D loss: 0.702517, acc: 52.34%] [G loss: 1.931965]\n",
      "epoch:26 step:25251 [D loss: 0.695954, acc: 64.06%] [G loss: 1.864858]\n",
      "epoch:26 step:25252 [D loss: 0.643150, acc: 58.59%] [G loss: 1.862307]\n",
      "epoch:26 step:25253 [D loss: 0.647849, acc: 59.38%] [G loss: 1.837577]\n",
      "epoch:26 step:25254 [D loss: 0.641185, acc: 61.72%] [G loss: 1.841086]\n",
      "epoch:26 step:25255 [D loss: 0.616164, acc: 71.09%] [G loss: 2.020267]\n",
      "epoch:26 step:25256 [D loss: 0.655507, acc: 61.72%] [G loss: 2.021617]\n",
      "epoch:26 step:25257 [D loss: 0.674717, acc: 57.03%] [G loss: 1.779213]\n",
      "epoch:26 step:25258 [D loss: 0.638625, acc: 60.16%] [G loss: 1.832517]\n",
      "epoch:26 step:25259 [D loss: 0.627407, acc: 64.06%] [G loss: 1.974629]\n",
      "epoch:26 step:25260 [D loss: 0.622016, acc: 64.84%] [G loss: 1.843457]\n",
      "epoch:26 step:25261 [D loss: 0.595466, acc: 69.53%] [G loss: 2.004497]\n",
      "epoch:26 step:25262 [D loss: 0.693732, acc: 55.47%] [G loss: 1.965840]\n",
      "epoch:26 step:25263 [D loss: 0.630151, acc: 67.19%] [G loss: 1.884865]\n",
      "epoch:26 step:25264 [D loss: 0.667615, acc: 60.94%] [G loss: 1.890770]\n",
      "epoch:26 step:25265 [D loss: 0.697572, acc: 57.81%] [G loss: 1.841203]\n",
      "epoch:26 step:25266 [D loss: 0.632470, acc: 63.28%] [G loss: 1.982995]\n",
      "epoch:26 step:25267 [D loss: 0.636241, acc: 63.28%] [G loss: 1.987488]\n",
      "epoch:26 step:25268 [D loss: 0.652732, acc: 64.06%] [G loss: 1.984805]\n",
      "epoch:26 step:25269 [D loss: 0.643806, acc: 64.84%] [G loss: 1.975008]\n",
      "epoch:26 step:25270 [D loss: 0.657476, acc: 63.28%] [G loss: 1.877388]\n",
      "epoch:26 step:25271 [D loss: 0.637033, acc: 62.50%] [G loss: 1.953776]\n",
      "epoch:26 step:25272 [D loss: 0.649552, acc: 61.72%] [G loss: 1.904716]\n",
      "epoch:26 step:25273 [D loss: 0.602738, acc: 68.75%] [G loss: 1.956559]\n",
      "epoch:26 step:25274 [D loss: 0.628450, acc: 60.16%] [G loss: 2.137213]\n",
      "epoch:26 step:25275 [D loss: 0.730029, acc: 53.12%] [G loss: 1.719624]\n",
      "epoch:26 step:25276 [D loss: 0.666739, acc: 60.16%] [G loss: 1.804039]\n",
      "epoch:26 step:25277 [D loss: 0.671008, acc: 61.72%] [G loss: 1.997264]\n",
      "epoch:26 step:25278 [D loss: 0.654337, acc: 66.41%] [G loss: 1.922552]\n",
      "epoch:26 step:25279 [D loss: 0.615567, acc: 66.41%] [G loss: 1.866548]\n",
      "epoch:26 step:25280 [D loss: 0.607991, acc: 67.97%] [G loss: 1.949667]\n",
      "epoch:26 step:25281 [D loss: 0.584899, acc: 70.31%] [G loss: 2.048613]\n",
      "epoch:26 step:25282 [D loss: 0.712595, acc: 51.56%] [G loss: 1.763782]\n",
      "epoch:26 step:25283 [D loss: 0.687315, acc: 55.47%] [G loss: 1.929491]\n",
      "epoch:26 step:25284 [D loss: 0.672486, acc: 57.81%] [G loss: 1.818428]\n",
      "epoch:26 step:25285 [D loss: 0.561481, acc: 72.66%] [G loss: 1.957770]\n",
      "epoch:26 step:25286 [D loss: 0.594083, acc: 69.53%] [G loss: 2.148941]\n",
      "epoch:26 step:25287 [D loss: 0.635106, acc: 62.50%] [G loss: 2.040998]\n",
      "epoch:26 step:25288 [D loss: 0.584373, acc: 69.53%] [G loss: 2.122553]\n",
      "epoch:26 step:25289 [D loss: 0.679668, acc: 62.50%] [G loss: 1.956090]\n",
      "epoch:26 step:25290 [D loss: 0.739913, acc: 49.22%] [G loss: 1.701851]\n",
      "epoch:26 step:25291 [D loss: 0.716410, acc: 47.66%] [G loss: 1.825875]\n",
      "epoch:26 step:25292 [D loss: 0.594816, acc: 69.53%] [G loss: 1.981224]\n",
      "epoch:26 step:25293 [D loss: 0.653762, acc: 61.72%] [G loss: 1.922293]\n",
      "epoch:26 step:25294 [D loss: 0.647394, acc: 62.50%] [G loss: 1.793493]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:26 step:25295 [D loss: 0.651120, acc: 65.62%] [G loss: 1.779072]\n",
      "epoch:26 step:25296 [D loss: 0.669888, acc: 67.19%] [G loss: 1.838865]\n",
      "epoch:26 step:25297 [D loss: 0.633158, acc: 63.28%] [G loss: 1.964542]\n",
      "epoch:26 step:25298 [D loss: 0.637440, acc: 62.50%] [G loss: 1.983063]\n",
      "epoch:26 step:25299 [D loss: 0.622283, acc: 64.06%] [G loss: 2.417238]\n",
      "epoch:27 step:25300 [D loss: 0.649602, acc: 69.53%] [G loss: 1.769901]\n",
      "epoch:27 step:25301 [D loss: 0.635440, acc: 67.19%] [G loss: 1.955604]\n",
      "epoch:27 step:25302 [D loss: 0.645839, acc: 63.28%] [G loss: 1.810452]\n",
      "epoch:27 step:25303 [D loss: 0.645256, acc: 66.41%] [G loss: 1.908727]\n",
      "epoch:27 step:25304 [D loss: 0.567034, acc: 75.00%] [G loss: 1.875046]\n",
      "epoch:27 step:25305 [D loss: 0.690252, acc: 57.81%] [G loss: 2.093411]\n",
      "epoch:27 step:25306 [D loss: 0.584161, acc: 71.88%] [G loss: 1.963309]\n",
      "epoch:27 step:25307 [D loss: 0.659075, acc: 60.94%] [G loss: 1.986626]\n",
      "epoch:27 step:25308 [D loss: 0.622285, acc: 59.38%] [G loss: 1.933277]\n",
      "epoch:27 step:25309 [D loss: 0.617472, acc: 67.97%] [G loss: 2.122885]\n",
      "epoch:27 step:25310 [D loss: 0.647590, acc: 64.06%] [G loss: 1.930033]\n",
      "epoch:27 step:25311 [D loss: 0.622344, acc: 65.62%] [G loss: 1.941989]\n",
      "epoch:27 step:25312 [D loss: 0.651867, acc: 64.06%] [G loss: 1.843404]\n",
      "epoch:27 step:25313 [D loss: 0.633665, acc: 64.84%] [G loss: 2.031359]\n",
      "epoch:27 step:25314 [D loss: 0.616275, acc: 65.62%] [G loss: 2.108092]\n",
      "epoch:27 step:25315 [D loss: 0.576146, acc: 70.31%] [G loss: 2.022207]\n",
      "epoch:27 step:25316 [D loss: 0.636331, acc: 64.84%] [G loss: 2.073049]\n",
      "epoch:27 step:25317 [D loss: 0.636822, acc: 66.41%] [G loss: 1.956516]\n",
      "epoch:27 step:25318 [D loss: 0.630925, acc: 64.06%] [G loss: 1.969705]\n",
      "epoch:27 step:25319 [D loss: 0.744604, acc: 52.34%] [G loss: 1.703973]\n",
      "epoch:27 step:25320 [D loss: 0.678635, acc: 57.81%] [G loss: 1.810542]\n",
      "epoch:27 step:25321 [D loss: 0.698971, acc: 55.47%] [G loss: 1.836225]\n",
      "epoch:27 step:25322 [D loss: 0.654977, acc: 64.84%] [G loss: 2.083039]\n",
      "epoch:27 step:25323 [D loss: 0.634758, acc: 59.38%] [G loss: 1.982247]\n",
      "epoch:27 step:25324 [D loss: 0.628580, acc: 64.06%] [G loss: 2.159247]\n",
      "epoch:27 step:25325 [D loss: 0.691687, acc: 57.81%] [G loss: 1.804814]\n",
      "epoch:27 step:25326 [D loss: 0.647469, acc: 56.25%] [G loss: 1.835648]\n",
      "epoch:27 step:25327 [D loss: 0.615990, acc: 64.84%] [G loss: 1.912519]\n",
      "epoch:27 step:25328 [D loss: 0.605076, acc: 64.06%] [G loss: 1.908110]\n",
      "epoch:27 step:25329 [D loss: 0.618439, acc: 65.62%] [G loss: 1.837989]\n",
      "epoch:27 step:25330 [D loss: 0.693375, acc: 55.47%] [G loss: 1.745002]\n",
      "epoch:27 step:25331 [D loss: 0.674256, acc: 59.38%] [G loss: 1.805486]\n",
      "epoch:27 step:25332 [D loss: 0.709507, acc: 59.38%] [G loss: 1.773560]\n",
      "epoch:27 step:25333 [D loss: 0.679528, acc: 57.81%] [G loss: 1.810853]\n",
      "epoch:27 step:25334 [D loss: 0.647920, acc: 63.28%] [G loss: 1.897171]\n",
      "epoch:27 step:25335 [D loss: 0.645134, acc: 60.16%] [G loss: 1.883431]\n",
      "epoch:27 step:25336 [D loss: 0.623527, acc: 67.19%] [G loss: 1.940760]\n",
      "epoch:27 step:25337 [D loss: 0.640868, acc: 62.50%] [G loss: 1.975774]\n",
      "epoch:27 step:25338 [D loss: 0.614585, acc: 64.84%] [G loss: 1.778237]\n",
      "epoch:27 step:25339 [D loss: 0.603626, acc: 69.53%] [G loss: 2.127860]\n",
      "epoch:27 step:25340 [D loss: 0.669796, acc: 57.03%] [G loss: 1.734710]\n",
      "epoch:27 step:25341 [D loss: 0.634708, acc: 64.84%] [G loss: 2.020047]\n",
      "epoch:27 step:25342 [D loss: 0.644937, acc: 64.06%] [G loss: 1.842932]\n",
      "epoch:27 step:25343 [D loss: 0.647603, acc: 60.16%] [G loss: 1.867348]\n",
      "epoch:27 step:25344 [D loss: 0.644532, acc: 64.06%] [G loss: 1.962970]\n",
      "epoch:27 step:25345 [D loss: 0.630581, acc: 67.97%] [G loss: 1.865276]\n",
      "epoch:27 step:25346 [D loss: 0.687614, acc: 56.25%] [G loss: 1.814143]\n",
      "epoch:27 step:25347 [D loss: 0.607823, acc: 63.28%] [G loss: 1.909707]\n",
      "epoch:27 step:25348 [D loss: 0.599463, acc: 71.09%] [G loss: 2.061530]\n",
      "epoch:27 step:25349 [D loss: 0.626748, acc: 65.62%] [G loss: 1.875798]\n",
      "epoch:27 step:25350 [D loss: 0.663271, acc: 59.38%] [G loss: 1.941485]\n",
      "epoch:27 step:25351 [D loss: 0.699392, acc: 57.03%] [G loss: 1.712543]\n",
      "epoch:27 step:25352 [D loss: 0.605634, acc: 62.50%] [G loss: 1.932257]\n",
      "epoch:27 step:25353 [D loss: 0.707498, acc: 59.38%] [G loss: 1.899061]\n",
      "epoch:27 step:25354 [D loss: 0.620002, acc: 63.28%] [G loss: 1.933182]\n",
      "epoch:27 step:25355 [D loss: 0.615459, acc: 70.31%] [G loss: 2.020984]\n",
      "epoch:27 step:25356 [D loss: 0.613045, acc: 68.75%] [G loss: 1.897619]\n",
      "epoch:27 step:25357 [D loss: 0.620861, acc: 66.41%] [G loss: 1.789996]\n",
      "epoch:27 step:25358 [D loss: 0.661892, acc: 59.38%] [G loss: 1.817474]\n",
      "epoch:27 step:25359 [D loss: 0.648268, acc: 60.16%] [G loss: 1.792694]\n",
      "epoch:27 step:25360 [D loss: 0.616573, acc: 63.28%] [G loss: 1.878530]\n",
      "epoch:27 step:25361 [D loss: 0.676018, acc: 64.06%] [G loss: 1.921750]\n",
      "epoch:27 step:25362 [D loss: 0.625119, acc: 64.06%] [G loss: 1.907354]\n",
      "epoch:27 step:25363 [D loss: 0.652806, acc: 62.50%] [G loss: 1.851872]\n",
      "epoch:27 step:25364 [D loss: 0.690978, acc: 60.16%] [G loss: 1.786472]\n",
      "epoch:27 step:25365 [D loss: 0.612168, acc: 67.97%] [G loss: 1.761386]\n",
      "epoch:27 step:25366 [D loss: 0.675771, acc: 57.81%] [G loss: 1.949980]\n",
      "epoch:27 step:25367 [D loss: 0.676566, acc: 59.38%] [G loss: 1.859110]\n",
      "epoch:27 step:25368 [D loss: 0.615952, acc: 70.31%] [G loss: 1.827991]\n",
      "epoch:27 step:25369 [D loss: 0.633932, acc: 64.84%] [G loss: 1.845143]\n",
      "epoch:27 step:25370 [D loss: 0.667422, acc: 62.50%] [G loss: 1.874999]\n",
      "epoch:27 step:25371 [D loss: 0.628049, acc: 62.50%] [G loss: 1.801185]\n",
      "epoch:27 step:25372 [D loss: 0.637500, acc: 60.16%] [G loss: 1.920877]\n",
      "epoch:27 step:25373 [D loss: 0.613936, acc: 65.62%] [G loss: 1.895190]\n",
      "epoch:27 step:25374 [D loss: 0.626140, acc: 64.06%] [G loss: 2.003286]\n",
      "epoch:27 step:25375 [D loss: 0.634178, acc: 65.62%] [G loss: 1.891271]\n",
      "epoch:27 step:25376 [D loss: 0.602201, acc: 69.53%] [G loss: 1.813953]\n",
      "epoch:27 step:25377 [D loss: 0.619531, acc: 68.75%] [G loss: 1.816626]\n",
      "epoch:27 step:25378 [D loss: 0.662744, acc: 62.50%] [G loss: 1.888836]\n",
      "epoch:27 step:25379 [D loss: 0.706547, acc: 53.91%] [G loss: 1.802269]\n",
      "epoch:27 step:25380 [D loss: 0.677678, acc: 56.25%] [G loss: 1.864426]\n",
      "epoch:27 step:25381 [D loss: 0.657502, acc: 58.59%] [G loss: 1.807214]\n",
      "epoch:27 step:25382 [D loss: 0.672537, acc: 60.94%] [G loss: 1.852356]\n",
      "epoch:27 step:25383 [D loss: 0.668068, acc: 58.59%] [G loss: 1.832366]\n",
      "epoch:27 step:25384 [D loss: 0.649499, acc: 59.38%] [G loss: 1.817680]\n",
      "epoch:27 step:25385 [D loss: 0.670759, acc: 63.28%] [G loss: 1.715025]\n",
      "epoch:27 step:25386 [D loss: 0.626146, acc: 65.62%] [G loss: 1.827830]\n",
      "epoch:27 step:25387 [D loss: 0.601702, acc: 67.97%] [G loss: 1.961731]\n",
      "epoch:27 step:25388 [D loss: 0.733936, acc: 52.34%] [G loss: 2.000655]\n",
      "epoch:27 step:25389 [D loss: 0.629466, acc: 67.19%] [G loss: 1.773645]\n",
      "epoch:27 step:25390 [D loss: 0.600021, acc: 65.62%] [G loss: 1.903146]\n",
      "epoch:27 step:25391 [D loss: 0.619200, acc: 64.06%] [G loss: 2.022138]\n",
      "epoch:27 step:25392 [D loss: 0.623409, acc: 67.19%] [G loss: 2.051528]\n",
      "epoch:27 step:25393 [D loss: 0.645090, acc: 66.41%] [G loss: 1.944879]\n",
      "epoch:27 step:25394 [D loss: 0.705582, acc: 50.78%] [G loss: 1.788263]\n",
      "epoch:27 step:25395 [D loss: 0.632040, acc: 64.06%] [G loss: 1.849563]\n",
      "epoch:27 step:25396 [D loss: 0.622783, acc: 66.41%] [G loss: 1.884781]\n",
      "epoch:27 step:25397 [D loss: 0.680109, acc: 58.59%] [G loss: 1.731308]\n",
      "epoch:27 step:25398 [D loss: 0.667651, acc: 57.81%] [G loss: 1.818655]\n",
      "epoch:27 step:25399 [D loss: 0.655806, acc: 60.94%] [G loss: 1.789994]\n",
      "epoch:27 step:25400 [D loss: 0.696536, acc: 60.94%] [G loss: 1.921621]\n",
      "##############\n",
      "[2.39863251 1.5575044  6.14028317 4.70987798 3.4214808  5.53645675\n",
      " 4.25366599 4.72393741 4.35818146 3.55073861]\n",
      "##########\n",
      "epoch:27 step:25401 [D loss: 0.698507, acc: 56.25%] [G loss: 1.887777]\n",
      "epoch:27 step:25402 [D loss: 0.661495, acc: 59.38%] [G loss: 1.802583]\n",
      "epoch:27 step:25403 [D loss: 0.653504, acc: 61.72%] [G loss: 1.832762]\n",
      "epoch:27 step:25404 [D loss: 0.683294, acc: 58.59%] [G loss: 1.794855]\n",
      "epoch:27 step:25405 [D loss: 0.614379, acc: 68.75%] [G loss: 1.993975]\n",
      "epoch:27 step:25406 [D loss: 0.617989, acc: 61.72%] [G loss: 1.940618]\n",
      "epoch:27 step:25407 [D loss: 0.649809, acc: 63.28%] [G loss: 1.847484]\n",
      "epoch:27 step:25408 [D loss: 0.681946, acc: 55.47%] [G loss: 1.663082]\n",
      "epoch:27 step:25409 [D loss: 0.652957, acc: 65.62%] [G loss: 1.803576]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:27 step:25410 [D loss: 0.636591, acc: 66.41%] [G loss: 1.817639]\n",
      "epoch:27 step:25411 [D loss: 0.575225, acc: 73.44%] [G loss: 2.042905]\n",
      "epoch:27 step:25412 [D loss: 0.667435, acc: 60.94%] [G loss: 1.977052]\n",
      "epoch:27 step:25413 [D loss: 0.578912, acc: 70.31%] [G loss: 2.027142]\n",
      "epoch:27 step:25414 [D loss: 0.578810, acc: 68.75%] [G loss: 2.306386]\n",
      "epoch:27 step:25415 [D loss: 0.612689, acc: 67.19%] [G loss: 2.252353]\n",
      "epoch:27 step:25416 [D loss: 0.626100, acc: 61.72%] [G loss: 2.150850]\n",
      "epoch:27 step:25417 [D loss: 0.596617, acc: 70.31%] [G loss: 2.033857]\n",
      "epoch:27 step:25418 [D loss: 0.624237, acc: 66.41%] [G loss: 2.196728]\n",
      "epoch:27 step:25419 [D loss: 0.651571, acc: 60.16%] [G loss: 2.108283]\n",
      "epoch:27 step:25420 [D loss: 0.667435, acc: 66.41%] [G loss: 1.933224]\n",
      "epoch:27 step:25421 [D loss: 0.596292, acc: 67.19%] [G loss: 2.232838]\n",
      "epoch:27 step:25422 [D loss: 0.715976, acc: 55.47%] [G loss: 1.810904]\n",
      "epoch:27 step:25423 [D loss: 0.781576, acc: 47.66%] [G loss: 1.874744]\n",
      "epoch:27 step:25424 [D loss: 0.666021, acc: 58.59%] [G loss: 1.581063]\n",
      "epoch:27 step:25425 [D loss: 0.654113, acc: 60.16%] [G loss: 1.906092]\n",
      "epoch:27 step:25426 [D loss: 0.640503, acc: 65.62%] [G loss: 1.864964]\n",
      "epoch:27 step:25427 [D loss: 0.658597, acc: 57.81%] [G loss: 1.895707]\n",
      "epoch:27 step:25428 [D loss: 0.653049, acc: 61.72%] [G loss: 1.841961]\n",
      "epoch:27 step:25429 [D loss: 0.589235, acc: 69.53%] [G loss: 1.957876]\n",
      "epoch:27 step:25430 [D loss: 0.625755, acc: 62.50%] [G loss: 1.919547]\n",
      "epoch:27 step:25431 [D loss: 0.655354, acc: 56.25%] [G loss: 1.864859]\n",
      "epoch:27 step:25432 [D loss: 0.659110, acc: 64.06%] [G loss: 1.880809]\n",
      "epoch:27 step:25433 [D loss: 0.656833, acc: 61.72%] [G loss: 1.898401]\n",
      "epoch:27 step:25434 [D loss: 0.642237, acc: 64.84%] [G loss: 1.883737]\n",
      "epoch:27 step:25435 [D loss: 0.645211, acc: 64.06%] [G loss: 1.669066]\n",
      "epoch:27 step:25436 [D loss: 0.613834, acc: 67.19%] [G loss: 1.820926]\n",
      "epoch:27 step:25437 [D loss: 0.632228, acc: 64.06%] [G loss: 2.003582]\n",
      "epoch:27 step:25438 [D loss: 0.637025, acc: 53.91%] [G loss: 1.842256]\n",
      "epoch:27 step:25439 [D loss: 0.657768, acc: 63.28%] [G loss: 1.780435]\n",
      "epoch:27 step:25440 [D loss: 0.634617, acc: 59.38%] [G loss: 1.771052]\n",
      "epoch:27 step:25441 [D loss: 0.623741, acc: 64.06%] [G loss: 1.948344]\n",
      "epoch:27 step:25442 [D loss: 0.680845, acc: 56.25%] [G loss: 1.843019]\n",
      "epoch:27 step:25443 [D loss: 0.655747, acc: 59.38%] [G loss: 1.847964]\n",
      "epoch:27 step:25444 [D loss: 0.656155, acc: 66.41%] [G loss: 1.929853]\n",
      "epoch:27 step:25445 [D loss: 0.588189, acc: 67.19%] [G loss: 1.983570]\n",
      "epoch:27 step:25446 [D loss: 0.696379, acc: 60.16%] [G loss: 1.857080]\n",
      "epoch:27 step:25447 [D loss: 0.656676, acc: 67.19%] [G loss: 1.848663]\n",
      "epoch:27 step:25448 [D loss: 0.597850, acc: 68.75%] [G loss: 1.806906]\n",
      "epoch:27 step:25449 [D loss: 0.663954, acc: 63.28%] [G loss: 1.814173]\n",
      "epoch:27 step:25450 [D loss: 0.637159, acc: 67.19%] [G loss: 1.847600]\n",
      "epoch:27 step:25451 [D loss: 0.677944, acc: 60.94%] [G loss: 1.873971]\n",
      "epoch:27 step:25452 [D loss: 0.640597, acc: 59.38%] [G loss: 1.859780]\n",
      "epoch:27 step:25453 [D loss: 0.657350, acc: 58.59%] [G loss: 1.924741]\n",
      "epoch:27 step:25454 [D loss: 0.662003, acc: 60.94%] [G loss: 1.840487]\n",
      "epoch:27 step:25455 [D loss: 0.605898, acc: 69.53%] [G loss: 1.985443]\n",
      "epoch:27 step:25456 [D loss: 0.706056, acc: 57.81%] [G loss: 1.722043]\n",
      "epoch:27 step:25457 [D loss: 0.651038, acc: 58.59%] [G loss: 1.810087]\n",
      "epoch:27 step:25458 [D loss: 0.622721, acc: 66.41%] [G loss: 1.794701]\n",
      "epoch:27 step:25459 [D loss: 0.679121, acc: 60.94%] [G loss: 1.763752]\n",
      "epoch:27 step:25460 [D loss: 0.696892, acc: 56.25%] [G loss: 1.794933]\n",
      "epoch:27 step:25461 [D loss: 0.664457, acc: 60.16%] [G loss: 1.835885]\n",
      "epoch:27 step:25462 [D loss: 0.644330, acc: 65.62%] [G loss: 1.917956]\n",
      "epoch:27 step:25463 [D loss: 0.639370, acc: 61.72%] [G loss: 1.831141]\n",
      "epoch:27 step:25464 [D loss: 0.639862, acc: 63.28%] [G loss: 1.869815]\n",
      "epoch:27 step:25465 [D loss: 0.616873, acc: 66.41%] [G loss: 1.845561]\n",
      "epoch:27 step:25466 [D loss: 0.654245, acc: 64.06%] [G loss: 1.909033]\n",
      "epoch:27 step:25467 [D loss: 0.622242, acc: 69.53%] [G loss: 1.896330]\n",
      "epoch:27 step:25468 [D loss: 0.628560, acc: 66.41%] [G loss: 1.885768]\n",
      "epoch:27 step:25469 [D loss: 0.702181, acc: 58.59%] [G loss: 1.774208]\n",
      "epoch:27 step:25470 [D loss: 0.637524, acc: 63.28%] [G loss: 1.866067]\n",
      "epoch:27 step:25471 [D loss: 0.682997, acc: 57.03%] [G loss: 1.792055]\n",
      "epoch:27 step:25472 [D loss: 0.625015, acc: 66.41%] [G loss: 1.890759]\n",
      "epoch:27 step:25473 [D loss: 0.621726, acc: 60.94%] [G loss: 1.861712]\n",
      "epoch:27 step:25474 [D loss: 0.693564, acc: 55.47%] [G loss: 1.717215]\n",
      "epoch:27 step:25475 [D loss: 0.692100, acc: 57.03%] [G loss: 1.722215]\n",
      "epoch:27 step:25476 [D loss: 0.637612, acc: 63.28%] [G loss: 1.812308]\n",
      "epoch:27 step:25477 [D loss: 0.666832, acc: 57.81%] [G loss: 1.879653]\n",
      "epoch:27 step:25478 [D loss: 0.682911, acc: 58.59%] [G loss: 1.708729]\n",
      "epoch:27 step:25479 [D loss: 0.657863, acc: 60.16%] [G loss: 1.847177]\n",
      "epoch:27 step:25480 [D loss: 0.669504, acc: 63.28%] [G loss: 1.739425]\n",
      "epoch:27 step:25481 [D loss: 0.637100, acc: 62.50%] [G loss: 1.865735]\n",
      "epoch:27 step:25482 [D loss: 0.686586, acc: 60.16%] [G loss: 1.934734]\n",
      "epoch:27 step:25483 [D loss: 0.643117, acc: 62.50%] [G loss: 1.805782]\n",
      "epoch:27 step:25484 [D loss: 0.645756, acc: 71.88%] [G loss: 1.665827]\n",
      "epoch:27 step:25485 [D loss: 0.671946, acc: 57.81%] [G loss: 1.877445]\n",
      "epoch:27 step:25486 [D loss: 0.693957, acc: 60.94%] [G loss: 1.863653]\n",
      "epoch:27 step:25487 [D loss: 0.616461, acc: 60.94%] [G loss: 1.807363]\n",
      "epoch:27 step:25488 [D loss: 0.669243, acc: 60.94%] [G loss: 1.847699]\n",
      "epoch:27 step:25489 [D loss: 0.666442, acc: 63.28%] [G loss: 1.831137]\n",
      "epoch:27 step:25490 [D loss: 0.636497, acc: 60.16%] [G loss: 1.898453]\n",
      "epoch:27 step:25491 [D loss: 0.634004, acc: 65.62%] [G loss: 1.853071]\n",
      "epoch:27 step:25492 [D loss: 0.640656, acc: 64.84%] [G loss: 1.870326]\n",
      "epoch:27 step:25493 [D loss: 0.703475, acc: 57.81%] [G loss: 1.910244]\n",
      "epoch:27 step:25494 [D loss: 0.634641, acc: 64.06%] [G loss: 1.852043]\n",
      "epoch:27 step:25495 [D loss: 0.670543, acc: 57.03%] [G loss: 1.731566]\n",
      "epoch:27 step:25496 [D loss: 0.619717, acc: 63.28%] [G loss: 1.994072]\n",
      "epoch:27 step:25497 [D loss: 0.596174, acc: 70.31%] [G loss: 1.897196]\n",
      "epoch:27 step:25498 [D loss: 0.624819, acc: 64.06%] [G loss: 2.004670]\n",
      "epoch:27 step:25499 [D loss: 0.695656, acc: 57.03%] [G loss: 1.674363]\n",
      "epoch:27 step:25500 [D loss: 0.690930, acc: 56.25%] [G loss: 1.875234]\n",
      "epoch:27 step:25501 [D loss: 0.649678, acc: 64.84%] [G loss: 1.953704]\n",
      "epoch:27 step:25502 [D loss: 0.683573, acc: 58.59%] [G loss: 1.785976]\n",
      "epoch:27 step:25503 [D loss: 0.638023, acc: 66.41%] [G loss: 1.686383]\n",
      "epoch:27 step:25504 [D loss: 0.652088, acc: 60.94%] [G loss: 1.784638]\n",
      "epoch:27 step:25505 [D loss: 0.582293, acc: 67.19%] [G loss: 2.066750]\n",
      "epoch:27 step:25506 [D loss: 0.638569, acc: 61.72%] [G loss: 1.977718]\n",
      "epoch:27 step:25507 [D loss: 0.577770, acc: 72.66%] [G loss: 2.091505]\n",
      "epoch:27 step:25508 [D loss: 0.587417, acc: 64.84%] [G loss: 2.074734]\n",
      "epoch:27 step:25509 [D loss: 0.710988, acc: 53.12%] [G loss: 1.724712]\n",
      "epoch:27 step:25510 [D loss: 0.722032, acc: 53.12%] [G loss: 1.753343]\n",
      "epoch:27 step:25511 [D loss: 0.700958, acc: 56.25%] [G loss: 1.747989]\n",
      "epoch:27 step:25512 [D loss: 0.641209, acc: 64.06%] [G loss: 1.821308]\n",
      "epoch:27 step:25513 [D loss: 0.678262, acc: 61.72%] [G loss: 1.693723]\n",
      "epoch:27 step:25514 [D loss: 0.656651, acc: 58.59%] [G loss: 1.858413]\n",
      "epoch:27 step:25515 [D loss: 0.658080, acc: 53.91%] [G loss: 1.898363]\n",
      "epoch:27 step:25516 [D loss: 0.651998, acc: 60.16%] [G loss: 1.776573]\n",
      "epoch:27 step:25517 [D loss: 0.588695, acc: 69.53%] [G loss: 1.894777]\n",
      "epoch:27 step:25518 [D loss: 0.581112, acc: 64.84%] [G loss: 2.084037]\n",
      "epoch:27 step:25519 [D loss: 0.660364, acc: 57.81%] [G loss: 1.854739]\n",
      "epoch:27 step:25520 [D loss: 0.610750, acc: 62.50%] [G loss: 2.108803]\n",
      "epoch:27 step:25521 [D loss: 0.664799, acc: 60.94%] [G loss: 1.882341]\n",
      "epoch:27 step:25522 [D loss: 0.621659, acc: 64.84%] [G loss: 1.923038]\n",
      "epoch:27 step:25523 [D loss: 0.664993, acc: 60.16%] [G loss: 1.921911]\n",
      "epoch:27 step:25524 [D loss: 0.672080, acc: 60.16%] [G loss: 1.902936]\n",
      "epoch:27 step:25525 [D loss: 0.655981, acc: 60.16%] [G loss: 1.754768]\n",
      "epoch:27 step:25526 [D loss: 0.660850, acc: 61.72%] [G loss: 1.867161]\n",
      "epoch:27 step:25527 [D loss: 0.630228, acc: 61.72%] [G loss: 1.793536]\n",
      "epoch:27 step:25528 [D loss: 0.623090, acc: 60.94%] [G loss: 2.073088]\n",
      "epoch:27 step:25529 [D loss: 0.621854, acc: 61.72%] [G loss: 2.156340]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:27 step:25530 [D loss: 0.552857, acc: 71.09%] [G loss: 2.260947]\n",
      "epoch:27 step:25531 [D loss: 0.630652, acc: 67.19%] [G loss: 2.272629]\n",
      "epoch:27 step:25532 [D loss: 0.677692, acc: 57.03%] [G loss: 1.841056]\n",
      "epoch:27 step:25533 [D loss: 0.612479, acc: 67.97%] [G loss: 1.850685]\n",
      "epoch:27 step:25534 [D loss: 0.604807, acc: 65.62%] [G loss: 1.912440]\n",
      "epoch:27 step:25535 [D loss: 0.626257, acc: 64.84%] [G loss: 2.032833]\n",
      "epoch:27 step:25536 [D loss: 0.651410, acc: 63.28%] [G loss: 1.944376]\n",
      "epoch:27 step:25537 [D loss: 0.644912, acc: 61.72%] [G loss: 1.915685]\n",
      "epoch:27 step:25538 [D loss: 0.682964, acc: 56.25%] [G loss: 1.890154]\n",
      "epoch:27 step:25539 [D loss: 0.671960, acc: 60.16%] [G loss: 1.875959]\n",
      "epoch:27 step:25540 [D loss: 0.640425, acc: 60.94%] [G loss: 1.959419]\n",
      "epoch:27 step:25541 [D loss: 0.665147, acc: 52.34%] [G loss: 1.854796]\n",
      "epoch:27 step:25542 [D loss: 0.600843, acc: 74.22%] [G loss: 1.945512]\n",
      "epoch:27 step:25543 [D loss: 0.628336, acc: 60.94%] [G loss: 1.904040]\n",
      "epoch:27 step:25544 [D loss: 0.645518, acc: 61.72%] [G loss: 1.847966]\n",
      "epoch:27 step:25545 [D loss: 0.624878, acc: 62.50%] [G loss: 1.970725]\n",
      "epoch:27 step:25546 [D loss: 0.686321, acc: 58.59%] [G loss: 1.835171]\n",
      "epoch:27 step:25547 [D loss: 0.577244, acc: 70.31%] [G loss: 2.002561]\n",
      "epoch:27 step:25548 [D loss: 0.675766, acc: 56.25%] [G loss: 1.709973]\n",
      "epoch:27 step:25549 [D loss: 0.694775, acc: 55.47%] [G loss: 1.829337]\n",
      "epoch:27 step:25550 [D loss: 0.669445, acc: 57.81%] [G loss: 1.733998]\n",
      "epoch:27 step:25551 [D loss: 0.661170, acc: 58.59%] [G loss: 1.795220]\n",
      "epoch:27 step:25552 [D loss: 0.645455, acc: 57.03%] [G loss: 1.839551]\n",
      "epoch:27 step:25553 [D loss: 0.643180, acc: 60.16%] [G loss: 1.891912]\n",
      "epoch:27 step:25554 [D loss: 0.662054, acc: 60.94%] [G loss: 1.790295]\n",
      "epoch:27 step:25555 [D loss: 0.714017, acc: 55.47%] [G loss: 1.816222]\n",
      "epoch:27 step:25556 [D loss: 0.640355, acc: 66.41%] [G loss: 1.831848]\n",
      "epoch:27 step:25557 [D loss: 0.640114, acc: 61.72%] [G loss: 1.773880]\n",
      "epoch:27 step:25558 [D loss: 0.676544, acc: 60.94%] [G loss: 1.833335]\n",
      "epoch:27 step:25559 [D loss: 0.611717, acc: 69.53%] [G loss: 1.986100]\n",
      "epoch:27 step:25560 [D loss: 0.640749, acc: 61.72%] [G loss: 1.912499]\n",
      "epoch:27 step:25561 [D loss: 0.660235, acc: 60.16%] [G loss: 1.990416]\n",
      "epoch:27 step:25562 [D loss: 0.632662, acc: 63.28%] [G loss: 2.013384]\n",
      "epoch:27 step:25563 [D loss: 0.645466, acc: 62.50%] [G loss: 2.038908]\n",
      "epoch:27 step:25564 [D loss: 0.647075, acc: 58.59%] [G loss: 1.817704]\n",
      "epoch:27 step:25565 [D loss: 0.667257, acc: 57.81%] [G loss: 1.779209]\n",
      "epoch:27 step:25566 [D loss: 0.689145, acc: 59.38%] [G loss: 1.902713]\n",
      "epoch:27 step:25567 [D loss: 0.642771, acc: 64.84%] [G loss: 1.802718]\n",
      "epoch:27 step:25568 [D loss: 0.639363, acc: 64.06%] [G loss: 1.922133]\n",
      "epoch:27 step:25569 [D loss: 0.618179, acc: 64.06%] [G loss: 1.834196]\n",
      "epoch:27 step:25570 [D loss: 0.615575, acc: 66.41%] [G loss: 1.961175]\n",
      "epoch:27 step:25571 [D loss: 0.652484, acc: 63.28%] [G loss: 1.932294]\n",
      "epoch:27 step:25572 [D loss: 0.652015, acc: 60.94%] [G loss: 1.776313]\n",
      "epoch:27 step:25573 [D loss: 0.655015, acc: 62.50%] [G loss: 1.957756]\n",
      "epoch:27 step:25574 [D loss: 0.612925, acc: 60.94%] [G loss: 1.890827]\n",
      "epoch:27 step:25575 [D loss: 0.651835, acc: 64.06%] [G loss: 2.132480]\n",
      "epoch:27 step:25576 [D loss: 0.676964, acc: 53.12%] [G loss: 1.920744]\n",
      "epoch:27 step:25577 [D loss: 0.622417, acc: 67.19%] [G loss: 1.845917]\n",
      "epoch:27 step:25578 [D loss: 0.694327, acc: 54.69%] [G loss: 1.776594]\n",
      "epoch:27 step:25579 [D loss: 0.616819, acc: 66.41%] [G loss: 1.923805]\n",
      "epoch:27 step:25580 [D loss: 0.727139, acc: 46.88%] [G loss: 1.789498]\n",
      "epoch:27 step:25581 [D loss: 0.643296, acc: 60.16%] [G loss: 1.655578]\n",
      "epoch:27 step:25582 [D loss: 0.661229, acc: 56.25%] [G loss: 1.818861]\n",
      "epoch:27 step:25583 [D loss: 0.636574, acc: 60.16%] [G loss: 1.761933]\n",
      "epoch:27 step:25584 [D loss: 0.671536, acc: 60.16%] [G loss: 1.785021]\n",
      "epoch:27 step:25585 [D loss: 0.658041, acc: 63.28%] [G loss: 1.830734]\n",
      "epoch:27 step:25586 [D loss: 0.651143, acc: 64.84%] [G loss: 1.812814]\n",
      "epoch:27 step:25587 [D loss: 0.683107, acc: 62.50%] [G loss: 1.864132]\n",
      "epoch:27 step:25588 [D loss: 0.642918, acc: 60.16%] [G loss: 1.883894]\n",
      "epoch:27 step:25589 [D loss: 0.632462, acc: 66.41%] [G loss: 1.785204]\n",
      "epoch:27 step:25590 [D loss: 0.650425, acc: 64.06%] [G loss: 1.777439]\n",
      "epoch:27 step:25591 [D loss: 0.623717, acc: 66.41%] [G loss: 1.788485]\n",
      "epoch:27 step:25592 [D loss: 0.663445, acc: 60.94%] [G loss: 1.756317]\n",
      "epoch:27 step:25593 [D loss: 0.629540, acc: 63.28%] [G loss: 1.772412]\n",
      "epoch:27 step:25594 [D loss: 0.658671, acc: 57.81%] [G loss: 1.794512]\n",
      "epoch:27 step:25595 [D loss: 0.607574, acc: 64.06%] [G loss: 1.864968]\n",
      "epoch:27 step:25596 [D loss: 0.613774, acc: 67.19%] [G loss: 1.847033]\n",
      "epoch:27 step:25597 [D loss: 0.661965, acc: 57.03%] [G loss: 1.915471]\n",
      "epoch:27 step:25598 [D loss: 0.640638, acc: 59.38%] [G loss: 1.868953]\n",
      "epoch:27 step:25599 [D loss: 0.697750, acc: 57.03%] [G loss: 1.774208]\n",
      "epoch:27 step:25600 [D loss: 0.687289, acc: 57.03%] [G loss: 1.854366]\n",
      "##############\n",
      "[2.52248317 1.44560833 6.24074459 4.69763343 3.6004404  5.47611895\n",
      " 4.27437618 4.92777135 4.55813234 3.79186792]\n",
      "##########\n",
      "epoch:27 step:25601 [D loss: 0.651882, acc: 62.50%] [G loss: 1.819554]\n",
      "epoch:27 step:25602 [D loss: 0.656721, acc: 64.84%] [G loss: 1.808224]\n",
      "epoch:27 step:25603 [D loss: 0.700509, acc: 53.91%] [G loss: 1.834853]\n",
      "epoch:27 step:25604 [D loss: 0.660243, acc: 57.03%] [G loss: 1.776099]\n",
      "epoch:27 step:25605 [D loss: 0.627370, acc: 67.97%] [G loss: 1.649658]\n",
      "epoch:27 step:25606 [D loss: 0.598181, acc: 71.09%] [G loss: 1.888959]\n",
      "epoch:27 step:25607 [D loss: 0.668474, acc: 61.72%] [G loss: 1.801193]\n",
      "epoch:27 step:25608 [D loss: 0.629120, acc: 62.50%] [G loss: 1.815708]\n",
      "epoch:27 step:25609 [D loss: 0.663855, acc: 60.16%] [G loss: 1.863819]\n",
      "epoch:27 step:25610 [D loss: 0.653805, acc: 62.50%] [G loss: 1.852879]\n",
      "epoch:27 step:25611 [D loss: 0.602165, acc: 68.75%] [G loss: 2.132873]\n",
      "epoch:27 step:25612 [D loss: 0.595704, acc: 70.31%] [G loss: 2.148099]\n",
      "epoch:27 step:25613 [D loss: 0.566142, acc: 73.44%] [G loss: 2.136009]\n",
      "epoch:27 step:25614 [D loss: 0.594738, acc: 70.31%] [G loss: 2.187407]\n",
      "epoch:27 step:25615 [D loss: 0.681086, acc: 60.16%] [G loss: 1.802873]\n",
      "epoch:27 step:25616 [D loss: 0.667031, acc: 63.28%] [G loss: 1.810712]\n",
      "epoch:27 step:25617 [D loss: 0.656517, acc: 57.81%] [G loss: 1.954923]\n",
      "epoch:27 step:25618 [D loss: 0.664760, acc: 60.16%] [G loss: 1.769349]\n",
      "epoch:27 step:25619 [D loss: 0.682981, acc: 61.72%] [G loss: 1.814373]\n",
      "epoch:27 step:25620 [D loss: 0.618046, acc: 66.41%] [G loss: 1.958067]\n",
      "epoch:27 step:25621 [D loss: 0.645972, acc: 55.47%] [G loss: 1.767740]\n",
      "epoch:27 step:25622 [D loss: 0.640358, acc: 62.50%] [G loss: 1.827311]\n",
      "epoch:27 step:25623 [D loss: 0.676426, acc: 59.38%] [G loss: 1.746700]\n",
      "epoch:27 step:25624 [D loss: 0.679153, acc: 55.47%] [G loss: 1.854008]\n",
      "epoch:27 step:25625 [D loss: 0.605236, acc: 66.41%] [G loss: 1.856984]\n",
      "epoch:27 step:25626 [D loss: 0.666496, acc: 61.72%] [G loss: 1.724774]\n",
      "epoch:27 step:25627 [D loss: 0.669068, acc: 64.06%] [G loss: 1.983240]\n",
      "epoch:27 step:25628 [D loss: 0.585067, acc: 68.75%] [G loss: 1.891329]\n",
      "epoch:27 step:25629 [D loss: 0.636035, acc: 61.72%] [G loss: 2.023270]\n",
      "epoch:27 step:25630 [D loss: 0.592875, acc: 66.41%] [G loss: 1.973040]\n",
      "epoch:27 step:25631 [D loss: 0.628454, acc: 67.19%] [G loss: 1.912694]\n",
      "epoch:27 step:25632 [D loss: 0.605252, acc: 70.31%] [G loss: 2.090183]\n",
      "epoch:27 step:25633 [D loss: 0.677744, acc: 54.69%] [G loss: 2.036009]\n",
      "epoch:27 step:25634 [D loss: 0.680677, acc: 55.47%] [G loss: 1.879603]\n",
      "epoch:27 step:25635 [D loss: 0.611834, acc: 67.97%] [G loss: 1.934825]\n",
      "epoch:27 step:25636 [D loss: 0.613979, acc: 63.28%] [G loss: 1.954077]\n",
      "epoch:27 step:25637 [D loss: 0.655752, acc: 59.38%] [G loss: 1.856947]\n",
      "epoch:27 step:25638 [D loss: 0.646157, acc: 60.16%] [G loss: 1.980061]\n",
      "epoch:27 step:25639 [D loss: 0.624751, acc: 60.94%] [G loss: 2.127638]\n",
      "epoch:27 step:25640 [D loss: 0.722019, acc: 51.56%] [G loss: 1.843889]\n",
      "epoch:27 step:25641 [D loss: 0.734958, acc: 47.66%] [G loss: 1.648822]\n",
      "epoch:27 step:25642 [D loss: 0.642476, acc: 64.84%] [G loss: 1.827596]\n",
      "epoch:27 step:25643 [D loss: 0.737028, acc: 54.69%] [G loss: 1.807724]\n",
      "epoch:27 step:25644 [D loss: 0.549190, acc: 74.22%] [G loss: 2.122132]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:27 step:25645 [D loss: 0.596873, acc: 64.06%] [G loss: 2.084124]\n",
      "epoch:27 step:25646 [D loss: 0.610723, acc: 67.97%] [G loss: 2.223881]\n",
      "epoch:27 step:25647 [D loss: 0.710046, acc: 56.25%] [G loss: 1.878981]\n",
      "epoch:27 step:25648 [D loss: 0.731602, acc: 51.56%] [G loss: 1.645308]\n",
      "epoch:27 step:25649 [D loss: 0.675272, acc: 58.59%] [G loss: 1.848648]\n",
      "epoch:27 step:25650 [D loss: 0.659350, acc: 60.16%] [G loss: 1.771927]\n",
      "epoch:27 step:25651 [D loss: 0.677495, acc: 56.25%] [G loss: 1.709531]\n",
      "epoch:27 step:25652 [D loss: 0.671816, acc: 59.38%] [G loss: 1.818976]\n",
      "epoch:27 step:25653 [D loss: 0.646917, acc: 59.38%] [G loss: 1.988889]\n",
      "epoch:27 step:25654 [D loss: 0.715710, acc: 51.56%] [G loss: 1.710202]\n",
      "epoch:27 step:25655 [D loss: 0.667159, acc: 60.94%] [G loss: 1.770145]\n",
      "epoch:27 step:25656 [D loss: 0.724265, acc: 53.12%] [G loss: 1.741393]\n",
      "epoch:27 step:25657 [D loss: 0.624120, acc: 62.50%] [G loss: 1.924762]\n",
      "epoch:27 step:25658 [D loss: 0.624435, acc: 63.28%] [G loss: 1.853851]\n",
      "epoch:27 step:25659 [D loss: 0.626801, acc: 63.28%] [G loss: 2.000277]\n",
      "epoch:27 step:25660 [D loss: 0.673982, acc: 63.28%] [G loss: 1.830197]\n",
      "epoch:27 step:25661 [D loss: 0.677497, acc: 64.84%] [G loss: 1.696816]\n",
      "epoch:27 step:25662 [D loss: 0.685663, acc: 62.50%] [G loss: 1.799292]\n",
      "epoch:27 step:25663 [D loss: 0.663603, acc: 53.91%] [G loss: 1.848083]\n",
      "epoch:27 step:25664 [D loss: 0.645384, acc: 60.16%] [G loss: 1.970720]\n",
      "epoch:27 step:25665 [D loss: 0.679302, acc: 59.38%] [G loss: 1.819238]\n",
      "epoch:27 step:25666 [D loss: 0.648238, acc: 60.94%] [G loss: 1.938601]\n",
      "epoch:27 step:25667 [D loss: 0.679202, acc: 60.16%] [G loss: 1.719471]\n",
      "epoch:27 step:25668 [D loss: 0.668983, acc: 60.16%] [G loss: 1.856114]\n",
      "epoch:27 step:25669 [D loss: 0.598702, acc: 67.19%] [G loss: 1.981833]\n",
      "epoch:27 step:25670 [D loss: 0.691383, acc: 59.38%] [G loss: 1.976121]\n",
      "epoch:27 step:25671 [D loss: 0.621072, acc: 67.97%] [G loss: 1.791725]\n",
      "epoch:27 step:25672 [D loss: 0.708959, acc: 53.91%] [G loss: 1.838856]\n",
      "epoch:27 step:25673 [D loss: 0.602597, acc: 67.19%] [G loss: 1.946917]\n",
      "epoch:27 step:25674 [D loss: 0.651683, acc: 63.28%] [G loss: 1.800028]\n",
      "epoch:27 step:25675 [D loss: 0.719852, acc: 55.47%] [G loss: 1.814670]\n",
      "epoch:27 step:25676 [D loss: 0.654223, acc: 65.62%] [G loss: 1.744832]\n",
      "epoch:27 step:25677 [D loss: 0.637950, acc: 58.59%] [G loss: 1.868541]\n",
      "epoch:27 step:25678 [D loss: 0.602834, acc: 64.06%] [G loss: 1.846157]\n",
      "epoch:27 step:25679 [D loss: 0.630013, acc: 63.28%] [G loss: 1.920543]\n",
      "epoch:27 step:25680 [D loss: 0.609191, acc: 64.06%] [G loss: 1.964790]\n",
      "epoch:27 step:25681 [D loss: 0.646285, acc: 65.62%] [G loss: 1.791087]\n",
      "epoch:27 step:25682 [D loss: 0.667990, acc: 63.28%] [G loss: 1.937817]\n",
      "epoch:27 step:25683 [D loss: 0.602593, acc: 64.84%] [G loss: 1.968926]\n",
      "epoch:27 step:25684 [D loss: 0.611247, acc: 64.84%] [G loss: 1.997424]\n",
      "epoch:27 step:25685 [D loss: 0.674439, acc: 60.94%] [G loss: 1.781555]\n",
      "epoch:27 step:25686 [D loss: 0.641640, acc: 62.50%] [G loss: 1.767195]\n",
      "epoch:27 step:25687 [D loss: 0.619537, acc: 71.88%] [G loss: 1.894374]\n",
      "epoch:27 step:25688 [D loss: 0.654260, acc: 56.25%] [G loss: 1.892123]\n",
      "epoch:27 step:25689 [D loss: 0.618246, acc: 67.97%] [G loss: 1.820922]\n",
      "epoch:27 step:25690 [D loss: 0.638045, acc: 65.62%] [G loss: 1.922024]\n",
      "epoch:27 step:25691 [D loss: 0.636219, acc: 69.53%] [G loss: 1.779378]\n",
      "epoch:27 step:25692 [D loss: 0.693580, acc: 57.03%] [G loss: 1.838334]\n",
      "epoch:27 step:25693 [D loss: 0.685320, acc: 60.16%] [G loss: 1.827240]\n",
      "epoch:27 step:25694 [D loss: 0.650681, acc: 60.16%] [G loss: 1.857684]\n",
      "epoch:27 step:25695 [D loss: 0.684550, acc: 59.38%] [G loss: 1.684966]\n",
      "epoch:27 step:25696 [D loss: 0.654535, acc: 60.16%] [G loss: 1.781504]\n",
      "epoch:27 step:25697 [D loss: 0.671374, acc: 60.94%] [G loss: 1.940694]\n",
      "epoch:27 step:25698 [D loss: 0.632983, acc: 64.06%] [G loss: 1.795887]\n",
      "epoch:27 step:25699 [D loss: 0.692889, acc: 52.34%] [G loss: 2.030005]\n",
      "epoch:27 step:25700 [D loss: 0.663786, acc: 63.28%] [G loss: 1.837563]\n",
      "epoch:27 step:25701 [D loss: 0.663408, acc: 63.28%] [G loss: 1.928871]\n",
      "epoch:27 step:25702 [D loss: 0.645713, acc: 64.06%] [G loss: 1.971432]\n",
      "epoch:27 step:25703 [D loss: 0.606161, acc: 68.75%] [G loss: 1.787344]\n",
      "epoch:27 step:25704 [D loss: 0.631849, acc: 66.41%] [G loss: 1.969842]\n",
      "epoch:27 step:25705 [D loss: 0.604462, acc: 64.06%] [G loss: 2.116618]\n",
      "epoch:27 step:25706 [D loss: 0.649376, acc: 64.06%] [G loss: 2.008933]\n",
      "epoch:27 step:25707 [D loss: 0.701842, acc: 54.69%] [G loss: 1.916398]\n",
      "epoch:27 step:25708 [D loss: 0.621922, acc: 63.28%] [G loss: 1.901438]\n",
      "epoch:27 step:25709 [D loss: 0.649048, acc: 61.72%] [G loss: 1.858678]\n",
      "epoch:27 step:25710 [D loss: 0.689972, acc: 58.59%] [G loss: 1.880102]\n",
      "epoch:27 step:25711 [D loss: 0.633365, acc: 65.62%] [G loss: 1.795034]\n",
      "epoch:27 step:25712 [D loss: 0.645468, acc: 67.19%] [G loss: 1.922488]\n",
      "epoch:27 step:25713 [D loss: 0.601093, acc: 67.97%] [G loss: 2.042052]\n",
      "epoch:27 step:25714 [D loss: 0.639279, acc: 61.72%] [G loss: 1.945704]\n",
      "epoch:27 step:25715 [D loss: 0.641295, acc: 61.72%] [G loss: 2.093503]\n",
      "epoch:27 step:25716 [D loss: 0.633349, acc: 62.50%] [G loss: 1.905321]\n",
      "epoch:27 step:25717 [D loss: 0.640822, acc: 64.06%] [G loss: 1.865457]\n",
      "epoch:27 step:25718 [D loss: 0.684830, acc: 61.72%] [G loss: 1.836740]\n",
      "epoch:27 step:25719 [D loss: 0.657029, acc: 64.84%] [G loss: 1.897955]\n",
      "epoch:27 step:25720 [D loss: 0.650476, acc: 63.28%] [G loss: 1.856140]\n",
      "epoch:27 step:25721 [D loss: 0.632174, acc: 65.62%] [G loss: 1.770479]\n",
      "epoch:27 step:25722 [D loss: 0.696904, acc: 57.03%] [G loss: 1.850276]\n",
      "epoch:27 step:25723 [D loss: 0.714160, acc: 52.34%] [G loss: 1.793976]\n",
      "epoch:27 step:25724 [D loss: 0.671422, acc: 59.38%] [G loss: 1.887198]\n",
      "epoch:27 step:25725 [D loss: 0.604248, acc: 63.28%] [G loss: 1.833568]\n",
      "epoch:27 step:25726 [D loss: 0.620074, acc: 65.62%] [G loss: 1.927876]\n",
      "epoch:27 step:25727 [D loss: 0.618896, acc: 71.09%] [G loss: 1.945269]\n",
      "epoch:27 step:25728 [D loss: 0.624726, acc: 66.41%] [G loss: 2.159916]\n",
      "epoch:27 step:25729 [D loss: 0.580206, acc: 66.41%] [G loss: 2.216389]\n",
      "epoch:27 step:25730 [D loss: 0.663384, acc: 53.91%] [G loss: 2.009431]\n",
      "epoch:27 step:25731 [D loss: 0.674409, acc: 58.59%] [G loss: 1.785869]\n",
      "epoch:27 step:25732 [D loss: 0.618877, acc: 68.75%] [G loss: 1.906254]\n",
      "epoch:27 step:25733 [D loss: 0.686296, acc: 61.72%] [G loss: 1.874668]\n",
      "epoch:27 step:25734 [D loss: 0.635353, acc: 64.84%] [G loss: 1.894430]\n",
      "epoch:27 step:25735 [D loss: 0.580333, acc: 70.31%] [G loss: 1.921212]\n",
      "epoch:27 step:25736 [D loss: 0.721770, acc: 53.91%] [G loss: 1.767146]\n",
      "epoch:27 step:25737 [D loss: 0.711362, acc: 54.69%] [G loss: 1.785109]\n",
      "epoch:27 step:25738 [D loss: 0.680480, acc: 56.25%] [G loss: 1.760257]\n",
      "epoch:27 step:25739 [D loss: 0.709129, acc: 51.56%] [G loss: 1.815151]\n",
      "epoch:27 step:25740 [D loss: 0.669089, acc: 57.81%] [G loss: 1.904650]\n",
      "epoch:27 step:25741 [D loss: 0.701490, acc: 52.34%] [G loss: 1.725785]\n",
      "epoch:27 step:25742 [D loss: 0.686000, acc: 57.81%] [G loss: 1.691550]\n",
      "epoch:27 step:25743 [D loss: 0.704003, acc: 53.91%] [G loss: 1.741685]\n",
      "epoch:27 step:25744 [D loss: 0.665746, acc: 62.50%] [G loss: 1.758862]\n",
      "epoch:27 step:25745 [D loss: 0.658337, acc: 62.50%] [G loss: 1.697217]\n",
      "epoch:27 step:25746 [D loss: 0.648699, acc: 56.25%] [G loss: 1.759964]\n",
      "epoch:27 step:25747 [D loss: 0.689758, acc: 58.59%] [G loss: 1.715075]\n",
      "epoch:27 step:25748 [D loss: 0.662153, acc: 57.81%] [G loss: 1.845107]\n",
      "epoch:27 step:25749 [D loss: 0.614796, acc: 64.06%] [G loss: 1.795680]\n",
      "epoch:27 step:25750 [D loss: 0.632825, acc: 62.50%] [G loss: 1.862018]\n",
      "epoch:27 step:25751 [D loss: 0.685351, acc: 55.47%] [G loss: 1.797689]\n",
      "epoch:27 step:25752 [D loss: 0.638849, acc: 64.06%] [G loss: 1.843619]\n",
      "epoch:27 step:25753 [D loss: 0.716661, acc: 52.34%] [G loss: 1.755559]\n",
      "epoch:27 step:25754 [D loss: 0.619685, acc: 66.41%] [G loss: 1.766134]\n",
      "epoch:27 step:25755 [D loss: 0.569231, acc: 74.22%] [G loss: 2.135463]\n",
      "epoch:27 step:25756 [D loss: 0.612746, acc: 66.41%] [G loss: 2.022089]\n",
      "epoch:27 step:25757 [D loss: 0.726190, acc: 53.91%] [G loss: 1.718610]\n",
      "epoch:27 step:25758 [D loss: 0.667388, acc: 60.16%] [G loss: 1.740445]\n",
      "epoch:27 step:25759 [D loss: 0.695145, acc: 55.47%] [G loss: 1.709467]\n",
      "epoch:27 step:25760 [D loss: 0.684284, acc: 60.16%] [G loss: 1.794486]\n",
      "epoch:27 step:25761 [D loss: 0.731576, acc: 53.91%] [G loss: 1.729116]\n",
      "epoch:27 step:25762 [D loss: 0.660252, acc: 60.94%] [G loss: 1.796052]\n",
      "epoch:27 step:25763 [D loss: 0.645177, acc: 64.06%] [G loss: 1.859260]\n",
      "epoch:27 step:25764 [D loss: 0.646513, acc: 60.16%] [G loss: 1.888329]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:27 step:25765 [D loss: 0.640897, acc: 64.84%] [G loss: 1.772800]\n",
      "epoch:27 step:25766 [D loss: 0.641895, acc: 63.28%] [G loss: 1.952648]\n",
      "epoch:27 step:25767 [D loss: 0.640004, acc: 64.84%] [G loss: 2.079403]\n",
      "epoch:27 step:25768 [D loss: 0.636307, acc: 64.06%] [G loss: 1.993203]\n",
      "epoch:27 step:25769 [D loss: 0.663656, acc: 60.16%] [G loss: 1.939130]\n",
      "epoch:27 step:25770 [D loss: 0.570714, acc: 71.88%] [G loss: 2.094053]\n",
      "epoch:27 step:25771 [D loss: 0.642480, acc: 61.72%] [G loss: 1.922403]\n",
      "epoch:27 step:25772 [D loss: 0.686628, acc: 63.28%] [G loss: 1.762334]\n",
      "epoch:27 step:25773 [D loss: 0.659972, acc: 60.16%] [G loss: 1.935257]\n",
      "epoch:27 step:25774 [D loss: 0.688169, acc: 52.34%] [G loss: 1.785334]\n",
      "epoch:27 step:25775 [D loss: 0.694261, acc: 54.69%] [G loss: 2.013806]\n",
      "epoch:27 step:25776 [D loss: 0.682598, acc: 57.03%] [G loss: 1.772037]\n",
      "epoch:27 step:25777 [D loss: 0.659738, acc: 60.94%] [G loss: 1.850334]\n",
      "epoch:27 step:25778 [D loss: 0.635268, acc: 66.41%] [G loss: 2.057406]\n",
      "epoch:27 step:25779 [D loss: 0.634973, acc: 65.62%] [G loss: 2.006721]\n",
      "epoch:27 step:25780 [D loss: 0.644385, acc: 67.19%] [G loss: 2.011431]\n",
      "epoch:27 step:25781 [D loss: 0.667124, acc: 59.38%] [G loss: 1.738232]\n",
      "epoch:27 step:25782 [D loss: 0.692133, acc: 55.47%] [G loss: 1.740755]\n",
      "epoch:27 step:25783 [D loss: 0.620421, acc: 64.06%] [G loss: 1.857917]\n",
      "epoch:27 step:25784 [D loss: 0.662024, acc: 59.38%] [G loss: 1.798557]\n",
      "epoch:27 step:25785 [D loss: 0.608876, acc: 71.88%] [G loss: 1.845725]\n",
      "epoch:27 step:25786 [D loss: 0.641500, acc: 64.06%] [G loss: 1.828866]\n",
      "epoch:27 step:25787 [D loss: 0.573845, acc: 68.75%] [G loss: 1.940506]\n",
      "epoch:27 step:25788 [D loss: 0.649311, acc: 63.28%] [G loss: 1.941331]\n",
      "epoch:27 step:25789 [D loss: 0.669013, acc: 59.38%] [G loss: 1.835203]\n",
      "epoch:27 step:25790 [D loss: 0.634914, acc: 60.16%] [G loss: 1.843837]\n",
      "epoch:27 step:25791 [D loss: 0.667829, acc: 59.38%] [G loss: 1.785039]\n",
      "epoch:27 step:25792 [D loss: 0.637558, acc: 67.19%] [G loss: 1.819910]\n",
      "epoch:27 step:25793 [D loss: 0.617901, acc: 66.41%] [G loss: 1.925705]\n",
      "epoch:27 step:25794 [D loss: 0.599361, acc: 71.09%] [G loss: 2.102476]\n",
      "epoch:27 step:25795 [D loss: 0.657104, acc: 57.81%] [G loss: 2.065016]\n",
      "epoch:27 step:25796 [D loss: 0.607715, acc: 65.62%] [G loss: 2.086296]\n",
      "epoch:27 step:25797 [D loss: 0.643033, acc: 57.03%] [G loss: 2.016238]\n",
      "epoch:27 step:25798 [D loss: 0.542234, acc: 71.09%] [G loss: 2.090480]\n",
      "epoch:27 step:25799 [D loss: 0.682317, acc: 58.59%] [G loss: 1.809983]\n",
      "epoch:27 step:25800 [D loss: 0.728653, acc: 45.31%] [G loss: 1.665124]\n",
      "##############\n",
      "[2.36017857 1.71186623 6.10961693 4.71928909 3.61913063 5.44600755\n",
      " 4.45853431 4.58733251 4.65467335 3.91388789]\n",
      "##########\n",
      "epoch:27 step:25801 [D loss: 0.684443, acc: 57.03%] [G loss: 1.771783]\n",
      "epoch:27 step:25802 [D loss: 0.646771, acc: 66.41%] [G loss: 1.695464]\n",
      "epoch:27 step:25803 [D loss: 0.632343, acc: 64.84%] [G loss: 1.970998]\n",
      "epoch:27 step:25804 [D loss: 0.641729, acc: 58.59%] [G loss: 1.884152]\n",
      "epoch:27 step:25805 [D loss: 0.718112, acc: 53.91%] [G loss: 1.698404]\n",
      "epoch:27 step:25806 [D loss: 0.672636, acc: 64.84%] [G loss: 1.720275]\n",
      "epoch:27 step:25807 [D loss: 0.624693, acc: 61.72%] [G loss: 2.071558]\n",
      "epoch:27 step:25808 [D loss: 0.633401, acc: 66.41%] [G loss: 1.810816]\n",
      "epoch:27 step:25809 [D loss: 0.693990, acc: 60.16%] [G loss: 1.770461]\n",
      "epoch:27 step:25810 [D loss: 0.649581, acc: 60.94%] [G loss: 1.777361]\n",
      "epoch:27 step:25811 [D loss: 0.672303, acc: 62.50%] [G loss: 1.867101]\n",
      "epoch:27 step:25812 [D loss: 0.647036, acc: 62.50%] [G loss: 1.739653]\n",
      "epoch:27 step:25813 [D loss: 0.691197, acc: 54.69%] [G loss: 1.856205]\n",
      "epoch:27 step:25814 [D loss: 0.606145, acc: 69.53%] [G loss: 2.009662]\n",
      "epoch:27 step:25815 [D loss: 0.635390, acc: 60.94%] [G loss: 1.925012]\n",
      "epoch:27 step:25816 [D loss: 0.630863, acc: 66.41%] [G loss: 1.850285]\n",
      "epoch:27 step:25817 [D loss: 0.732539, acc: 55.47%] [G loss: 1.822535]\n",
      "epoch:27 step:25818 [D loss: 0.662428, acc: 57.03%] [G loss: 2.018061]\n",
      "epoch:27 step:25819 [D loss: 0.639183, acc: 64.06%] [G loss: 1.708232]\n",
      "epoch:27 step:25820 [D loss: 0.658226, acc: 63.28%] [G loss: 1.874403]\n",
      "epoch:27 step:25821 [D loss: 0.636437, acc: 65.62%] [G loss: 1.924744]\n",
      "epoch:27 step:25822 [D loss: 0.652906, acc: 64.84%] [G loss: 1.874167]\n",
      "epoch:27 step:25823 [D loss: 0.728006, acc: 51.56%] [G loss: 1.810370]\n",
      "epoch:27 step:25824 [D loss: 0.679006, acc: 60.16%] [G loss: 1.808142]\n",
      "epoch:27 step:25825 [D loss: 0.657594, acc: 65.62%] [G loss: 1.805684]\n",
      "epoch:27 step:25826 [D loss: 0.679708, acc: 55.47%] [G loss: 1.708792]\n",
      "epoch:27 step:25827 [D loss: 0.711081, acc: 54.69%] [G loss: 1.575819]\n",
      "epoch:27 step:25828 [D loss: 0.655322, acc: 59.38%] [G loss: 1.740843]\n",
      "epoch:27 step:25829 [D loss: 0.665154, acc: 58.59%] [G loss: 1.638572]\n",
      "epoch:27 step:25830 [D loss: 0.714520, acc: 53.91%] [G loss: 1.667241]\n",
      "epoch:27 step:25831 [D loss: 0.622656, acc: 67.19%] [G loss: 1.786485]\n",
      "epoch:27 step:25832 [D loss: 0.656005, acc: 62.50%] [G loss: 1.747187]\n",
      "epoch:27 step:25833 [D loss: 0.620315, acc: 63.28%] [G loss: 1.986538]\n",
      "epoch:27 step:25834 [D loss: 0.638487, acc: 62.50%] [G loss: 1.963189]\n",
      "epoch:27 step:25835 [D loss: 0.655965, acc: 60.94%] [G loss: 1.823855]\n",
      "epoch:27 step:25836 [D loss: 0.654338, acc: 63.28%] [G loss: 1.774359]\n",
      "epoch:27 step:25837 [D loss: 0.728050, acc: 54.69%] [G loss: 1.659372]\n",
      "epoch:27 step:25838 [D loss: 0.656803, acc: 61.72%] [G loss: 1.875111]\n",
      "epoch:27 step:25839 [D loss: 0.667049, acc: 60.16%] [G loss: 1.715877]\n",
      "epoch:27 step:25840 [D loss: 0.641757, acc: 64.06%] [G loss: 1.908013]\n",
      "epoch:27 step:25841 [D loss: 0.645341, acc: 61.72%] [G loss: 1.740141]\n",
      "epoch:27 step:25842 [D loss: 0.638246, acc: 61.72%] [G loss: 1.800601]\n",
      "epoch:27 step:25843 [D loss: 0.643063, acc: 67.19%] [G loss: 1.871323]\n",
      "epoch:27 step:25844 [D loss: 0.646990, acc: 61.72%] [G loss: 1.920192]\n",
      "epoch:27 step:25845 [D loss: 0.683836, acc: 55.47%] [G loss: 1.717504]\n",
      "epoch:27 step:25846 [D loss: 0.649036, acc: 65.62%] [G loss: 1.829239]\n",
      "epoch:27 step:25847 [D loss: 0.684915, acc: 54.69%] [G loss: 1.740791]\n",
      "epoch:27 step:25848 [D loss: 0.605613, acc: 68.75%] [G loss: 1.890177]\n",
      "epoch:27 step:25849 [D loss: 0.584006, acc: 67.97%] [G loss: 2.087989]\n",
      "epoch:27 step:25850 [D loss: 0.644720, acc: 64.06%] [G loss: 1.943028]\n",
      "epoch:27 step:25851 [D loss: 0.627604, acc: 54.69%] [G loss: 1.931468]\n",
      "epoch:27 step:25852 [D loss: 0.671136, acc: 58.59%] [G loss: 1.765561]\n",
      "epoch:27 step:25853 [D loss: 0.592316, acc: 67.97%] [G loss: 1.954382]\n",
      "epoch:27 step:25854 [D loss: 0.657872, acc: 62.50%] [G loss: 1.915885]\n",
      "epoch:27 step:25855 [D loss: 0.644925, acc: 64.06%] [G loss: 1.970839]\n",
      "epoch:27 step:25856 [D loss: 0.659034, acc: 60.16%] [G loss: 1.866617]\n",
      "epoch:27 step:25857 [D loss: 0.584940, acc: 68.75%] [G loss: 1.904068]\n",
      "epoch:27 step:25858 [D loss: 0.711718, acc: 53.12%] [G loss: 1.716502]\n",
      "epoch:27 step:25859 [D loss: 0.695055, acc: 59.38%] [G loss: 1.845291]\n",
      "epoch:27 step:25860 [D loss: 0.630123, acc: 62.50%] [G loss: 1.916204]\n",
      "epoch:27 step:25861 [D loss: 0.644104, acc: 63.28%] [G loss: 1.834862]\n",
      "epoch:27 step:25862 [D loss: 0.657909, acc: 67.19%] [G loss: 1.864828]\n",
      "epoch:27 step:25863 [D loss: 0.551001, acc: 75.00%] [G loss: 2.156177]\n",
      "epoch:27 step:25864 [D loss: 0.637756, acc: 64.84%] [G loss: 1.829880]\n",
      "epoch:27 step:25865 [D loss: 0.761906, acc: 50.00%] [G loss: 1.714565]\n",
      "epoch:27 step:25866 [D loss: 0.661328, acc: 54.69%] [G loss: 1.691687]\n",
      "epoch:27 step:25867 [D loss: 0.640905, acc: 64.06%] [G loss: 1.719649]\n",
      "epoch:27 step:25868 [D loss: 0.701725, acc: 53.12%] [G loss: 1.924048]\n",
      "epoch:27 step:25869 [D loss: 0.666799, acc: 60.94%] [G loss: 1.855385]\n",
      "epoch:27 step:25870 [D loss: 0.647266, acc: 60.94%] [G loss: 1.908289]\n",
      "epoch:27 step:25871 [D loss: 0.674083, acc: 57.03%] [G loss: 1.728973]\n",
      "epoch:27 step:25872 [D loss: 0.673303, acc: 57.03%] [G loss: 1.769108]\n",
      "epoch:27 step:25873 [D loss: 0.631346, acc: 69.53%] [G loss: 1.920227]\n",
      "epoch:27 step:25874 [D loss: 0.688039, acc: 54.69%] [G loss: 1.724506]\n",
      "epoch:27 step:25875 [D loss: 0.638321, acc: 60.16%] [G loss: 1.694189]\n",
      "epoch:27 step:25876 [D loss: 0.703885, acc: 55.47%] [G loss: 1.762341]\n",
      "epoch:27 step:25877 [D loss: 0.618473, acc: 62.50%] [G loss: 1.802543]\n",
      "epoch:27 step:25878 [D loss: 0.715656, acc: 50.00%] [G loss: 1.725091]\n",
      "epoch:27 step:25879 [D loss: 0.652911, acc: 60.94%] [G loss: 1.782419]\n",
      "epoch:27 step:25880 [D loss: 0.655509, acc: 62.50%] [G loss: 1.725474]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:27 step:25881 [D loss: 0.620347, acc: 64.84%] [G loss: 1.846441]\n",
      "epoch:27 step:25882 [D loss: 0.655339, acc: 59.38%] [G loss: 1.733047]\n",
      "epoch:27 step:25883 [D loss: 0.670571, acc: 60.16%] [G loss: 1.705167]\n",
      "epoch:27 step:25884 [D loss: 0.671824, acc: 54.69%] [G loss: 1.817523]\n",
      "epoch:27 step:25885 [D loss: 0.671469, acc: 61.72%] [G loss: 1.685194]\n",
      "epoch:27 step:25886 [D loss: 0.714543, acc: 50.00%] [G loss: 1.846063]\n",
      "epoch:27 step:25887 [D loss: 0.664418, acc: 53.91%] [G loss: 1.921331]\n",
      "epoch:27 step:25888 [D loss: 0.624757, acc: 64.84%] [G loss: 1.933409]\n",
      "epoch:27 step:25889 [D loss: 0.666948, acc: 61.72%] [G loss: 1.790456]\n",
      "epoch:27 step:25890 [D loss: 0.608636, acc: 67.19%] [G loss: 1.847423]\n",
      "epoch:27 step:25891 [D loss: 0.618832, acc: 64.84%] [G loss: 2.036204]\n",
      "epoch:27 step:25892 [D loss: 0.606850, acc: 67.97%] [G loss: 1.885677]\n",
      "epoch:27 step:25893 [D loss: 0.659576, acc: 56.25%] [G loss: 1.826213]\n",
      "epoch:27 step:25894 [D loss: 0.704723, acc: 57.03%] [G loss: 1.769927]\n",
      "epoch:27 step:25895 [D loss: 0.640137, acc: 63.28%] [G loss: 1.763832]\n",
      "epoch:27 step:25896 [D loss: 0.666119, acc: 60.94%] [G loss: 1.696861]\n",
      "epoch:27 step:25897 [D loss: 0.615585, acc: 67.97%] [G loss: 1.788849]\n",
      "epoch:27 step:25898 [D loss: 0.670306, acc: 56.25%] [G loss: 1.778823]\n",
      "epoch:27 step:25899 [D loss: 0.681784, acc: 57.03%] [G loss: 1.784212]\n",
      "epoch:27 step:25900 [D loss: 0.701551, acc: 58.59%] [G loss: 1.923777]\n",
      "epoch:27 step:25901 [D loss: 0.678214, acc: 53.12%] [G loss: 1.775390]\n",
      "epoch:27 step:25902 [D loss: 0.636385, acc: 67.19%] [G loss: 1.850413]\n",
      "epoch:27 step:25903 [D loss: 0.662863, acc: 62.50%] [G loss: 2.004561]\n",
      "epoch:27 step:25904 [D loss: 0.601101, acc: 66.41%] [G loss: 1.784236]\n",
      "epoch:27 step:25905 [D loss: 0.619461, acc: 67.19%] [G loss: 1.722653]\n",
      "epoch:27 step:25906 [D loss: 0.621667, acc: 72.66%] [G loss: 1.899007]\n",
      "epoch:27 step:25907 [D loss: 0.647800, acc: 62.50%] [G loss: 1.997470]\n",
      "epoch:27 step:25908 [D loss: 0.625714, acc: 68.75%] [G loss: 1.781414]\n",
      "epoch:27 step:25909 [D loss: 0.693137, acc: 57.03%] [G loss: 1.807487]\n",
      "epoch:27 step:25910 [D loss: 0.676757, acc: 53.91%] [G loss: 1.762248]\n",
      "epoch:27 step:25911 [D loss: 0.632818, acc: 63.28%] [G loss: 1.735801]\n",
      "epoch:27 step:25912 [D loss: 0.651443, acc: 57.81%] [G loss: 1.843349]\n",
      "epoch:27 step:25913 [D loss: 0.703332, acc: 57.81%] [G loss: 1.780807]\n",
      "epoch:27 step:25914 [D loss: 0.708435, acc: 53.91%] [G loss: 1.698236]\n",
      "epoch:27 step:25915 [D loss: 0.668963, acc: 55.47%] [G loss: 1.785269]\n",
      "epoch:27 step:25916 [D loss: 0.644648, acc: 60.94%] [G loss: 1.950471]\n",
      "epoch:27 step:25917 [D loss: 0.619514, acc: 65.62%] [G loss: 1.844066]\n",
      "epoch:27 step:25918 [D loss: 0.662834, acc: 57.03%] [G loss: 1.808813]\n",
      "epoch:27 step:25919 [D loss: 0.644531, acc: 60.94%] [G loss: 1.915833]\n",
      "epoch:27 step:25920 [D loss: 0.662919, acc: 58.59%] [G loss: 1.770809]\n",
      "epoch:27 step:25921 [D loss: 0.642345, acc: 64.06%] [G loss: 1.889158]\n",
      "epoch:27 step:25922 [D loss: 0.632503, acc: 60.94%] [G loss: 1.926892]\n",
      "epoch:27 step:25923 [D loss: 0.605068, acc: 71.88%] [G loss: 1.970111]\n",
      "epoch:27 step:25924 [D loss: 0.665202, acc: 60.16%] [G loss: 1.821716]\n",
      "epoch:27 step:25925 [D loss: 0.651152, acc: 58.59%] [G loss: 1.829390]\n",
      "epoch:27 step:25926 [D loss: 0.606057, acc: 65.62%] [G loss: 1.752974]\n",
      "epoch:27 step:25927 [D loss: 0.683515, acc: 55.47%] [G loss: 1.753857]\n",
      "epoch:27 step:25928 [D loss: 0.560496, acc: 71.88%] [G loss: 2.056099]\n",
      "epoch:27 step:25929 [D loss: 0.685009, acc: 53.12%] [G loss: 1.882406]\n",
      "epoch:27 step:25930 [D loss: 0.584217, acc: 69.53%] [G loss: 1.942183]\n",
      "epoch:27 step:25931 [D loss: 0.673777, acc: 59.38%] [G loss: 2.026888]\n",
      "epoch:27 step:25932 [D loss: 0.646863, acc: 60.94%] [G loss: 1.896430]\n",
      "epoch:27 step:25933 [D loss: 0.601594, acc: 66.41%] [G loss: 1.911584]\n",
      "epoch:27 step:25934 [D loss: 0.622657, acc: 67.97%] [G loss: 2.028682]\n",
      "epoch:27 step:25935 [D loss: 0.667652, acc: 57.81%] [G loss: 2.023265]\n",
      "epoch:27 step:25936 [D loss: 0.596157, acc: 74.22%] [G loss: 2.088314]\n",
      "epoch:27 step:25937 [D loss: 0.636321, acc: 64.84%] [G loss: 1.889848]\n",
      "epoch:27 step:25938 [D loss: 0.679344, acc: 61.72%] [G loss: 1.804282]\n",
      "epoch:27 step:25939 [D loss: 0.696200, acc: 56.25%] [G loss: 1.939585]\n",
      "epoch:27 step:25940 [D loss: 0.644702, acc: 59.38%] [G loss: 1.893216]\n",
      "epoch:27 step:25941 [D loss: 0.666171, acc: 61.72%] [G loss: 1.874343]\n",
      "epoch:27 step:25942 [D loss: 0.642162, acc: 62.50%] [G loss: 2.007347]\n",
      "epoch:27 step:25943 [D loss: 0.690021, acc: 60.16%] [G loss: 1.952728]\n",
      "epoch:27 step:25944 [D loss: 0.709382, acc: 52.34%] [G loss: 1.959964]\n",
      "epoch:27 step:25945 [D loss: 0.642548, acc: 64.84%] [G loss: 1.896957]\n",
      "epoch:27 step:25946 [D loss: 0.669026, acc: 58.59%] [G loss: 1.944427]\n",
      "epoch:27 step:25947 [D loss: 0.608392, acc: 64.84%] [G loss: 2.069711]\n",
      "epoch:27 step:25948 [D loss: 0.654414, acc: 62.50%] [G loss: 1.931502]\n",
      "epoch:27 step:25949 [D loss: 0.639820, acc: 64.06%] [G loss: 2.006653]\n",
      "epoch:27 step:25950 [D loss: 0.642119, acc: 60.16%] [G loss: 1.804158]\n",
      "epoch:27 step:25951 [D loss: 0.629239, acc: 68.75%] [G loss: 1.976682]\n",
      "epoch:27 step:25952 [D loss: 0.654188, acc: 60.94%] [G loss: 1.959365]\n",
      "epoch:27 step:25953 [D loss: 0.633180, acc: 64.84%] [G loss: 1.971288]\n",
      "epoch:27 step:25954 [D loss: 0.654946, acc: 55.47%] [G loss: 1.960055]\n",
      "epoch:27 step:25955 [D loss: 0.624168, acc: 64.84%] [G loss: 1.827333]\n",
      "epoch:27 step:25956 [D loss: 0.688821, acc: 60.94%] [G loss: 1.813291]\n",
      "epoch:27 step:25957 [D loss: 0.708350, acc: 50.00%] [G loss: 1.697478]\n",
      "epoch:27 step:25958 [D loss: 0.663483, acc: 57.81%] [G loss: 1.780356]\n",
      "epoch:27 step:25959 [D loss: 0.674945, acc: 62.50%] [G loss: 1.802321]\n",
      "epoch:27 step:25960 [D loss: 0.648172, acc: 63.28%] [G loss: 1.761913]\n",
      "epoch:27 step:25961 [D loss: 0.630282, acc: 62.50%] [G loss: 1.868362]\n",
      "epoch:27 step:25962 [D loss: 0.679104, acc: 52.34%] [G loss: 1.861826]\n",
      "epoch:27 step:25963 [D loss: 0.683133, acc: 58.59%] [G loss: 1.722591]\n",
      "epoch:27 step:25964 [D loss: 0.694394, acc: 54.69%] [G loss: 1.769596]\n",
      "epoch:27 step:25965 [D loss: 0.667111, acc: 61.72%] [G loss: 1.748364]\n",
      "epoch:27 step:25966 [D loss: 0.664418, acc: 55.47%] [G loss: 1.628912]\n",
      "epoch:27 step:25967 [D loss: 0.657532, acc: 59.38%] [G loss: 1.782781]\n",
      "epoch:27 step:25968 [D loss: 0.633068, acc: 64.84%] [G loss: 1.781692]\n",
      "epoch:27 step:25969 [D loss: 0.631352, acc: 62.50%] [G loss: 1.913982]\n",
      "epoch:27 step:25970 [D loss: 0.686422, acc: 53.91%] [G loss: 1.793497]\n",
      "epoch:27 step:25971 [D loss: 0.656990, acc: 60.94%] [G loss: 1.741684]\n",
      "epoch:27 step:25972 [D loss: 0.691906, acc: 54.69%] [G loss: 1.787764]\n",
      "epoch:27 step:25973 [D loss: 0.635872, acc: 64.06%] [G loss: 1.843281]\n",
      "epoch:27 step:25974 [D loss: 0.691932, acc: 58.59%] [G loss: 1.774048]\n",
      "epoch:27 step:25975 [D loss: 0.648283, acc: 60.16%] [G loss: 1.773758]\n",
      "epoch:27 step:25976 [D loss: 0.579520, acc: 71.09%] [G loss: 1.931766]\n",
      "epoch:27 step:25977 [D loss: 0.649711, acc: 64.84%] [G loss: 2.080881]\n",
      "epoch:27 step:25978 [D loss: 0.627974, acc: 63.28%] [G loss: 1.878640]\n",
      "epoch:27 step:25979 [D loss: 0.587394, acc: 69.53%] [G loss: 1.845980]\n",
      "epoch:27 step:25980 [D loss: 0.637229, acc: 60.94%] [G loss: 2.012109]\n",
      "epoch:27 step:25981 [D loss: 0.670504, acc: 60.16%] [G loss: 1.827448]\n",
      "epoch:27 step:25982 [D loss: 0.681980, acc: 57.81%] [G loss: 1.846481]\n",
      "epoch:27 step:25983 [D loss: 0.651989, acc: 60.94%] [G loss: 1.778388]\n",
      "epoch:27 step:25984 [D loss: 0.663033, acc: 55.47%] [G loss: 1.846689]\n",
      "epoch:27 step:25985 [D loss: 0.638713, acc: 60.16%] [G loss: 1.962547]\n",
      "epoch:27 step:25986 [D loss: 0.651223, acc: 61.72%] [G loss: 1.853108]\n",
      "epoch:27 step:25987 [D loss: 0.644215, acc: 62.50%] [G loss: 1.940326]\n",
      "epoch:27 step:25988 [D loss: 0.636545, acc: 61.72%] [G loss: 1.880335]\n",
      "epoch:27 step:25989 [D loss: 0.623309, acc: 62.50%] [G loss: 1.919935]\n",
      "epoch:27 step:25990 [D loss: 0.612671, acc: 64.84%] [G loss: 1.905459]\n",
      "epoch:27 step:25991 [D loss: 0.657302, acc: 60.94%] [G loss: 2.041383]\n",
      "epoch:27 step:25992 [D loss: 0.653608, acc: 64.06%] [G loss: 2.061224]\n",
      "epoch:27 step:25993 [D loss: 0.574273, acc: 67.97%] [G loss: 1.954821]\n",
      "epoch:27 step:25994 [D loss: 0.608676, acc: 67.19%] [G loss: 2.040492]\n",
      "epoch:27 step:25995 [D loss: 0.657406, acc: 57.81%] [G loss: 1.723032]\n",
      "epoch:27 step:25996 [D loss: 0.668607, acc: 59.38%] [G loss: 1.864569]\n",
      "epoch:27 step:25997 [D loss: 0.691132, acc: 57.81%] [G loss: 1.800781]\n",
      "epoch:27 step:25998 [D loss: 0.655980, acc: 61.72%] [G loss: 1.850652]\n",
      "epoch:27 step:25999 [D loss: 0.699980, acc: 59.38%] [G loss: 1.872073]\n",
      "epoch:27 step:26000 [D loss: 0.680172, acc: 60.16%] [G loss: 1.807193]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##############\n",
      "[2.53747652 1.58788168 6.27282654 4.89109664 3.61853372 5.465404\n",
      " 4.43943481 4.68796486 4.61567012 3.7074818 ]\n",
      "##########\n",
      "epoch:27 step:26001 [D loss: 0.621678, acc: 63.28%] [G loss: 1.885654]\n",
      "epoch:27 step:26002 [D loss: 0.620192, acc: 60.16%] [G loss: 1.778438]\n",
      "epoch:27 step:26003 [D loss: 0.645990, acc: 60.16%] [G loss: 1.811963]\n",
      "epoch:27 step:26004 [D loss: 0.662888, acc: 54.69%] [G loss: 1.786895]\n",
      "epoch:27 step:26005 [D loss: 0.611704, acc: 65.62%] [G loss: 1.967287]\n",
      "epoch:27 step:26006 [D loss: 0.586388, acc: 71.88%] [G loss: 1.909636]\n",
      "epoch:27 step:26007 [D loss: 0.654580, acc: 64.06%] [G loss: 1.952984]\n",
      "epoch:27 step:26008 [D loss: 0.656082, acc: 60.94%] [G loss: 1.874615]\n",
      "epoch:27 step:26009 [D loss: 0.685744, acc: 53.91%] [G loss: 1.796335]\n",
      "epoch:27 step:26010 [D loss: 0.627726, acc: 66.41%] [G loss: 1.862662]\n",
      "epoch:27 step:26011 [D loss: 0.602078, acc: 67.97%] [G loss: 1.921405]\n",
      "epoch:27 step:26012 [D loss: 0.629002, acc: 68.75%] [G loss: 1.857322]\n",
      "epoch:27 step:26013 [D loss: 0.641740, acc: 61.72%] [G loss: 1.921047]\n",
      "epoch:27 step:26014 [D loss: 0.651168, acc: 56.25%] [G loss: 1.972310]\n",
      "epoch:27 step:26015 [D loss: 0.672636, acc: 57.03%] [G loss: 1.777564]\n",
      "epoch:27 step:26016 [D loss: 0.618277, acc: 65.62%] [G loss: 1.937328]\n",
      "epoch:27 step:26017 [D loss: 0.677446, acc: 62.50%] [G loss: 1.913544]\n",
      "epoch:27 step:26018 [D loss: 0.574399, acc: 71.09%] [G loss: 1.938663]\n",
      "epoch:27 step:26019 [D loss: 0.615360, acc: 60.94%] [G loss: 2.070914]\n",
      "epoch:27 step:26020 [D loss: 0.654114, acc: 56.25%] [G loss: 1.988646]\n",
      "epoch:27 step:26021 [D loss: 0.676647, acc: 57.03%] [G loss: 1.873038]\n",
      "epoch:27 step:26022 [D loss: 0.642202, acc: 60.94%] [G loss: 1.779350]\n",
      "epoch:27 step:26023 [D loss: 0.646112, acc: 59.38%] [G loss: 1.817319]\n",
      "epoch:27 step:26024 [D loss: 0.644404, acc: 63.28%] [G loss: 1.912004]\n",
      "epoch:27 step:26025 [D loss: 0.672016, acc: 55.47%] [G loss: 1.925017]\n",
      "epoch:27 step:26026 [D loss: 0.673581, acc: 59.38%] [G loss: 1.769807]\n",
      "epoch:27 step:26027 [D loss: 0.637346, acc: 60.94%] [G loss: 1.886227]\n",
      "epoch:27 step:26028 [D loss: 0.642258, acc: 67.19%] [G loss: 1.791940]\n",
      "epoch:27 step:26029 [D loss: 0.631969, acc: 64.06%] [G loss: 1.798620]\n",
      "epoch:27 step:26030 [D loss: 0.704354, acc: 59.38%] [G loss: 1.810905]\n",
      "epoch:27 step:26031 [D loss: 0.707870, acc: 56.25%] [G loss: 1.837064]\n",
      "epoch:27 step:26032 [D loss: 0.636658, acc: 61.72%] [G loss: 1.784551]\n",
      "epoch:27 step:26033 [D loss: 0.675030, acc: 64.84%] [G loss: 1.782673]\n",
      "epoch:27 step:26034 [D loss: 0.633640, acc: 64.06%] [G loss: 1.700024]\n",
      "epoch:27 step:26035 [D loss: 0.653728, acc: 67.19%] [G loss: 1.900981]\n",
      "epoch:27 step:26036 [D loss: 0.631138, acc: 64.84%] [G loss: 1.826448]\n",
      "epoch:27 step:26037 [D loss: 0.617926, acc: 63.28%] [G loss: 1.815686]\n",
      "epoch:27 step:26038 [D loss: 0.647407, acc: 59.38%] [G loss: 1.907341]\n",
      "epoch:27 step:26039 [D loss: 0.643544, acc: 64.84%] [G loss: 1.941646]\n",
      "epoch:27 step:26040 [D loss: 0.703869, acc: 51.56%] [G loss: 1.877666]\n",
      "epoch:27 step:26041 [D loss: 0.691083, acc: 56.25%] [G loss: 1.920328]\n",
      "epoch:27 step:26042 [D loss: 0.649512, acc: 64.84%] [G loss: 1.784028]\n",
      "epoch:27 step:26043 [D loss: 0.719755, acc: 51.56%] [G loss: 1.767096]\n",
      "epoch:27 step:26044 [D loss: 0.644675, acc: 61.72%] [G loss: 1.883720]\n",
      "epoch:27 step:26045 [D loss: 0.630423, acc: 64.06%] [G loss: 1.871829]\n",
      "epoch:27 step:26046 [D loss: 0.632122, acc: 65.62%] [G loss: 1.936623]\n",
      "epoch:27 step:26047 [D loss: 0.631058, acc: 61.72%] [G loss: 1.864191]\n",
      "epoch:27 step:26048 [D loss: 0.665421, acc: 58.59%] [G loss: 1.889648]\n",
      "epoch:27 step:26049 [D loss: 0.625515, acc: 64.06%] [G loss: 1.980288]\n",
      "epoch:27 step:26050 [D loss: 0.682443, acc: 58.59%] [G loss: 1.824868]\n",
      "epoch:27 step:26051 [D loss: 0.658521, acc: 59.38%] [G loss: 1.751420]\n",
      "epoch:27 step:26052 [D loss: 0.600080, acc: 65.62%] [G loss: 1.879869]\n",
      "epoch:27 step:26053 [D loss: 0.641764, acc: 57.81%] [G loss: 1.851205]\n",
      "epoch:27 step:26054 [D loss: 0.610366, acc: 66.41%] [G loss: 1.934217]\n",
      "epoch:27 step:26055 [D loss: 0.629736, acc: 63.28%] [G loss: 1.773571]\n",
      "epoch:27 step:26056 [D loss: 0.638018, acc: 56.25%] [G loss: 1.802498]\n",
      "epoch:27 step:26057 [D loss: 0.652027, acc: 64.06%] [G loss: 1.779814]\n",
      "epoch:27 step:26058 [D loss: 0.655904, acc: 64.84%] [G loss: 1.868957]\n",
      "epoch:27 step:26059 [D loss: 0.659491, acc: 57.03%] [G loss: 1.741953]\n",
      "epoch:27 step:26060 [D loss: 0.645854, acc: 60.16%] [G loss: 1.810900]\n",
      "epoch:27 step:26061 [D loss: 0.642747, acc: 59.38%] [G loss: 1.717826]\n",
      "epoch:27 step:26062 [D loss: 0.655780, acc: 60.16%] [G loss: 1.810214]\n",
      "epoch:27 step:26063 [D loss: 0.637979, acc: 63.28%] [G loss: 1.795182]\n",
      "epoch:27 step:26064 [D loss: 0.694157, acc: 57.81%] [G loss: 1.807455]\n",
      "epoch:27 step:26065 [D loss: 0.624981, acc: 67.19%] [G loss: 1.892444]\n",
      "epoch:27 step:26066 [D loss: 0.635348, acc: 67.97%] [G loss: 1.883922]\n",
      "epoch:27 step:26067 [D loss: 0.701923, acc: 54.69%] [G loss: 1.807553]\n",
      "epoch:27 step:26068 [D loss: 0.622374, acc: 65.62%] [G loss: 1.958434]\n",
      "epoch:27 step:26069 [D loss: 0.690389, acc: 59.38%] [G loss: 1.878420]\n",
      "epoch:27 step:26070 [D loss: 0.611576, acc: 65.62%] [G loss: 1.907465]\n",
      "epoch:27 step:26071 [D loss: 0.629286, acc: 61.72%] [G loss: 1.905241]\n",
      "epoch:27 step:26072 [D loss: 0.613199, acc: 67.97%] [G loss: 1.807129]\n",
      "epoch:27 step:26073 [D loss: 0.620008, acc: 63.28%] [G loss: 2.051841]\n",
      "epoch:27 step:26074 [D loss: 0.586982, acc: 66.41%] [G loss: 2.196015]\n",
      "epoch:27 step:26075 [D loss: 0.668991, acc: 62.50%] [G loss: 1.922452]\n",
      "epoch:27 step:26076 [D loss: 0.631359, acc: 65.62%] [G loss: 1.825740]\n",
      "epoch:27 step:26077 [D loss: 0.666791, acc: 57.03%] [G loss: 1.786356]\n",
      "epoch:27 step:26078 [D loss: 0.644413, acc: 62.50%] [G loss: 1.825224]\n",
      "epoch:27 step:26079 [D loss: 0.650662, acc: 60.94%] [G loss: 1.965397]\n",
      "epoch:27 step:26080 [D loss: 0.635549, acc: 61.72%] [G loss: 2.026741]\n",
      "epoch:27 step:26081 [D loss: 0.567526, acc: 71.88%] [G loss: 2.198246]\n",
      "epoch:27 step:26082 [D loss: 0.659245, acc: 67.19%] [G loss: 1.866009]\n",
      "epoch:27 step:26083 [D loss: 0.752210, acc: 50.78%] [G loss: 1.692272]\n",
      "epoch:27 step:26084 [D loss: 0.670608, acc: 62.50%] [G loss: 1.834356]\n",
      "epoch:27 step:26085 [D loss: 0.617589, acc: 61.72%] [G loss: 1.978152]\n",
      "epoch:27 step:26086 [D loss: 0.653339, acc: 60.16%] [G loss: 1.843040]\n",
      "epoch:27 step:26087 [D loss: 0.687071, acc: 55.47%] [G loss: 1.731107]\n",
      "epoch:27 step:26088 [D loss: 0.642248, acc: 65.62%] [G loss: 1.941932]\n",
      "epoch:27 step:26089 [D loss: 0.602524, acc: 65.62%] [G loss: 1.915203]\n",
      "epoch:27 step:26090 [D loss: 0.654542, acc: 64.06%] [G loss: 1.914160]\n",
      "epoch:27 step:26091 [D loss: 0.599563, acc: 64.06%] [G loss: 1.873111]\n",
      "epoch:27 step:26092 [D loss: 0.632688, acc: 60.16%] [G loss: 1.952465]\n",
      "epoch:27 step:26093 [D loss: 0.733982, acc: 54.69%] [G loss: 1.745013]\n",
      "epoch:27 step:26094 [D loss: 0.661232, acc: 60.16%] [G loss: 1.845149]\n",
      "epoch:27 step:26095 [D loss: 0.675434, acc: 58.59%] [G loss: 1.809893]\n",
      "epoch:27 step:26096 [D loss: 0.654018, acc: 58.59%] [G loss: 1.830268]\n",
      "epoch:27 step:26097 [D loss: 0.644846, acc: 60.94%] [G loss: 1.998079]\n",
      "epoch:27 step:26098 [D loss: 0.631694, acc: 64.84%] [G loss: 1.859129]\n",
      "epoch:27 step:26099 [D loss: 0.715037, acc: 49.22%] [G loss: 1.773993]\n",
      "epoch:27 step:26100 [D loss: 0.696463, acc: 58.59%] [G loss: 1.745178]\n",
      "epoch:27 step:26101 [D loss: 0.635082, acc: 65.62%] [G loss: 1.848314]\n",
      "epoch:27 step:26102 [D loss: 0.705327, acc: 53.91%] [G loss: 1.819280]\n",
      "epoch:27 step:26103 [D loss: 0.631239, acc: 64.06%] [G loss: 1.878143]\n",
      "epoch:27 step:26104 [D loss: 0.697691, acc: 61.72%] [G loss: 1.820331]\n",
      "epoch:27 step:26105 [D loss: 0.609456, acc: 67.19%] [G loss: 1.859478]\n",
      "epoch:27 step:26106 [D loss: 0.623068, acc: 69.53%] [G loss: 1.913304]\n",
      "epoch:27 step:26107 [D loss: 0.640811, acc: 63.28%] [G loss: 1.934421]\n",
      "epoch:27 step:26108 [D loss: 0.630157, acc: 62.50%] [G loss: 1.873157]\n",
      "epoch:27 step:26109 [D loss: 0.651952, acc: 59.38%] [G loss: 1.905067]\n",
      "epoch:27 step:26110 [D loss: 0.641261, acc: 59.38%] [G loss: 1.941356]\n",
      "epoch:27 step:26111 [D loss: 0.663865, acc: 58.59%] [G loss: 1.809106]\n",
      "epoch:27 step:26112 [D loss: 0.626110, acc: 65.62%] [G loss: 1.873836]\n",
      "epoch:27 step:26113 [D loss: 0.660717, acc: 57.03%] [G loss: 1.809595]\n",
      "epoch:27 step:26114 [D loss: 0.636516, acc: 60.94%] [G loss: 2.006981]\n",
      "epoch:27 step:26115 [D loss: 0.733177, acc: 50.78%] [G loss: 1.934683]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:27 step:26116 [D loss: 0.652921, acc: 60.16%] [G loss: 1.859905]\n",
      "epoch:27 step:26117 [D loss: 0.670961, acc: 61.72%] [G loss: 1.785274]\n",
      "epoch:27 step:26118 [D loss: 0.673196, acc: 62.50%] [G loss: 1.876463]\n",
      "epoch:27 step:26119 [D loss: 0.701808, acc: 56.25%] [G loss: 1.685434]\n",
      "epoch:27 step:26120 [D loss: 0.673946, acc: 56.25%] [G loss: 1.881073]\n",
      "epoch:27 step:26121 [D loss: 0.623978, acc: 58.59%] [G loss: 1.923392]\n",
      "epoch:27 step:26122 [D loss: 0.673330, acc: 54.69%] [G loss: 1.793838]\n",
      "epoch:27 step:26123 [D loss: 0.693121, acc: 55.47%] [G loss: 1.919802]\n",
      "epoch:27 step:26124 [D loss: 0.638240, acc: 60.16%] [G loss: 1.978071]\n",
      "epoch:27 step:26125 [D loss: 0.609786, acc: 64.84%] [G loss: 1.788903]\n",
      "epoch:27 step:26126 [D loss: 0.654148, acc: 58.59%] [G loss: 1.762607]\n",
      "epoch:27 step:26127 [D loss: 0.699944, acc: 53.91%] [G loss: 1.642109]\n",
      "epoch:27 step:26128 [D loss: 0.641904, acc: 63.28%] [G loss: 1.681428]\n",
      "epoch:27 step:26129 [D loss: 0.632057, acc: 59.38%] [G loss: 1.741194]\n",
      "epoch:27 step:26130 [D loss: 0.620169, acc: 63.28%] [G loss: 1.907655]\n",
      "epoch:27 step:26131 [D loss: 0.662347, acc: 58.59%] [G loss: 1.760753]\n",
      "epoch:27 step:26132 [D loss: 0.657077, acc: 57.81%] [G loss: 1.842869]\n",
      "epoch:27 step:26133 [D loss: 0.654700, acc: 60.94%] [G loss: 1.796022]\n",
      "epoch:27 step:26134 [D loss: 0.714272, acc: 50.00%] [G loss: 1.699708]\n",
      "epoch:27 step:26135 [D loss: 0.679271, acc: 61.72%] [G loss: 1.742767]\n",
      "epoch:27 step:26136 [D loss: 0.597641, acc: 65.62%] [G loss: 1.872699]\n",
      "epoch:27 step:26137 [D loss: 0.668390, acc: 57.03%] [G loss: 1.746817]\n",
      "epoch:27 step:26138 [D loss: 0.661424, acc: 61.72%] [G loss: 1.805536]\n",
      "epoch:27 step:26139 [D loss: 0.613602, acc: 68.75%] [G loss: 1.818752]\n",
      "epoch:27 step:26140 [D loss: 0.629580, acc: 63.28%] [G loss: 1.961594]\n",
      "epoch:27 step:26141 [D loss: 0.613114, acc: 65.62%] [G loss: 1.968567]\n",
      "epoch:27 step:26142 [D loss: 0.673160, acc: 57.81%] [G loss: 1.847093]\n",
      "epoch:27 step:26143 [D loss: 0.655448, acc: 64.06%] [G loss: 1.771680]\n",
      "epoch:27 step:26144 [D loss: 0.627333, acc: 63.28%] [G loss: 1.884521]\n",
      "epoch:27 step:26145 [D loss: 0.624961, acc: 68.75%] [G loss: 1.766700]\n",
      "epoch:27 step:26146 [D loss: 0.683683, acc: 59.38%] [G loss: 1.807414]\n",
      "epoch:27 step:26147 [D loss: 0.634161, acc: 69.53%] [G loss: 1.920933]\n",
      "epoch:27 step:26148 [D loss: 0.605438, acc: 63.28%] [G loss: 1.925380]\n",
      "epoch:27 step:26149 [D loss: 0.684545, acc: 57.03%] [G loss: 1.843506]\n",
      "epoch:27 step:26150 [D loss: 0.690526, acc: 50.78%] [G loss: 1.712756]\n",
      "epoch:27 step:26151 [D loss: 0.608143, acc: 69.53%] [G loss: 1.800537]\n",
      "epoch:27 step:26152 [D loss: 0.619995, acc: 64.84%] [G loss: 1.863449]\n",
      "epoch:27 step:26153 [D loss: 0.623409, acc: 67.19%] [G loss: 1.807215]\n",
      "epoch:27 step:26154 [D loss: 0.648665, acc: 57.81%] [G loss: 1.777402]\n",
      "epoch:27 step:26155 [D loss: 0.672352, acc: 57.81%] [G loss: 1.763731]\n",
      "epoch:27 step:26156 [D loss: 0.667808, acc: 55.47%] [G loss: 1.944860]\n",
      "epoch:27 step:26157 [D loss: 0.722919, acc: 53.91%] [G loss: 1.792487]\n",
      "epoch:27 step:26158 [D loss: 0.682786, acc: 59.38%] [G loss: 1.678579]\n",
      "epoch:27 step:26159 [D loss: 0.607509, acc: 70.31%] [G loss: 1.828589]\n",
      "epoch:27 step:26160 [D loss: 0.651738, acc: 60.16%] [G loss: 1.815816]\n",
      "epoch:27 step:26161 [D loss: 0.690273, acc: 57.81%] [G loss: 1.689075]\n",
      "epoch:27 step:26162 [D loss: 0.685702, acc: 57.03%] [G loss: 1.839731]\n",
      "epoch:27 step:26163 [D loss: 0.660083, acc: 56.25%] [G loss: 1.765924]\n",
      "epoch:27 step:26164 [D loss: 0.675036, acc: 58.59%] [G loss: 1.735830]\n",
      "epoch:27 step:26165 [D loss: 0.629638, acc: 62.50%] [G loss: 1.801213]\n",
      "epoch:27 step:26166 [D loss: 0.678903, acc: 61.72%] [G loss: 1.663707]\n",
      "epoch:27 step:26167 [D loss: 0.626952, acc: 64.84%] [G loss: 1.804961]\n",
      "epoch:27 step:26168 [D loss: 0.679118, acc: 59.38%] [G loss: 1.842789]\n",
      "epoch:27 step:26169 [D loss: 0.684822, acc: 54.69%] [G loss: 1.837850]\n",
      "epoch:27 step:26170 [D loss: 0.688744, acc: 58.59%] [G loss: 1.787275]\n",
      "epoch:27 step:26171 [D loss: 0.647082, acc: 63.28%] [G loss: 1.814299]\n",
      "epoch:27 step:26172 [D loss: 0.651223, acc: 60.94%] [G loss: 1.721417]\n",
      "epoch:27 step:26173 [D loss: 0.611580, acc: 64.06%] [G loss: 1.857833]\n",
      "epoch:27 step:26174 [D loss: 0.607068, acc: 67.97%] [G loss: 1.913162]\n",
      "epoch:27 step:26175 [D loss: 0.694189, acc: 55.47%] [G loss: 1.924993]\n",
      "epoch:27 step:26176 [D loss: 0.665720, acc: 60.94%] [G loss: 1.799939]\n",
      "epoch:27 step:26177 [D loss: 0.656174, acc: 63.28%] [G loss: 1.854836]\n",
      "epoch:27 step:26178 [D loss: 0.676030, acc: 56.25%] [G loss: 1.832329]\n",
      "epoch:27 step:26179 [D loss: 0.623557, acc: 62.50%] [G loss: 1.769013]\n",
      "epoch:27 step:26180 [D loss: 0.687585, acc: 51.56%] [G loss: 1.765958]\n",
      "epoch:27 step:26181 [D loss: 0.646966, acc: 58.59%] [G loss: 1.824501]\n",
      "epoch:27 step:26182 [D loss: 0.615565, acc: 67.19%] [G loss: 1.883086]\n",
      "epoch:27 step:26183 [D loss: 0.644042, acc: 65.62%] [G loss: 1.951109]\n",
      "epoch:27 step:26184 [D loss: 0.630388, acc: 64.06%] [G loss: 1.895160]\n",
      "epoch:27 step:26185 [D loss: 0.609062, acc: 66.41%] [G loss: 1.859402]\n",
      "epoch:27 step:26186 [D loss: 0.704476, acc: 57.81%] [G loss: 1.860304]\n",
      "epoch:27 step:26187 [D loss: 0.638330, acc: 64.06%] [G loss: 1.890676]\n",
      "epoch:27 step:26188 [D loss: 0.675596, acc: 58.59%] [G loss: 1.945719]\n",
      "epoch:27 step:26189 [D loss: 0.635835, acc: 62.50%] [G loss: 1.985129]\n",
      "epoch:27 step:26190 [D loss: 0.616223, acc: 64.06%] [G loss: 1.838366]\n",
      "epoch:27 step:26191 [D loss: 0.681270, acc: 54.69%] [G loss: 1.914990]\n",
      "epoch:27 step:26192 [D loss: 0.697908, acc: 57.03%] [G loss: 1.865739]\n",
      "epoch:27 step:26193 [D loss: 0.631483, acc: 61.72%] [G loss: 1.870036]\n",
      "epoch:27 step:26194 [D loss: 0.671206, acc: 57.81%] [G loss: 1.841985]\n",
      "epoch:27 step:26195 [D loss: 0.672663, acc: 60.16%] [G loss: 1.895605]\n",
      "epoch:27 step:26196 [D loss: 0.650570, acc: 63.28%] [G loss: 1.866901]\n",
      "epoch:27 step:26197 [D loss: 0.616629, acc: 64.06%] [G loss: 1.884348]\n",
      "epoch:27 step:26198 [D loss: 0.614041, acc: 65.62%] [G loss: 1.917283]\n",
      "epoch:27 step:26199 [D loss: 0.630388, acc: 62.50%] [G loss: 1.853439]\n",
      "epoch:27 step:26200 [D loss: 0.679000, acc: 53.91%] [G loss: 1.929718]\n",
      "##############\n",
      "[2.59408317 1.55281451 6.27527487 4.90311039 3.56831317 5.58003255\n",
      " 4.19105709 4.5715089  4.4548605  3.56831263]\n",
      "##########\n",
      "epoch:27 step:26201 [D loss: 0.620013, acc: 64.84%] [G loss: 1.789926]\n",
      "epoch:27 step:26202 [D loss: 0.652756, acc: 63.28%] [G loss: 1.983733]\n",
      "epoch:27 step:26203 [D loss: 0.684127, acc: 57.03%] [G loss: 1.993880]\n",
      "epoch:27 step:26204 [D loss: 0.695172, acc: 54.69%] [G loss: 1.949551]\n",
      "epoch:27 step:26205 [D loss: 0.659496, acc: 58.59%] [G loss: 1.945598]\n",
      "epoch:27 step:26206 [D loss: 0.623233, acc: 64.84%] [G loss: 2.002581]\n",
      "epoch:27 step:26207 [D loss: 0.659273, acc: 60.94%] [G loss: 1.878553]\n",
      "epoch:27 step:26208 [D loss: 0.576969, acc: 71.09%] [G loss: 1.930881]\n",
      "epoch:27 step:26209 [D loss: 0.619386, acc: 69.53%] [G loss: 1.833966]\n",
      "epoch:27 step:26210 [D loss: 0.641482, acc: 58.59%] [G loss: 2.012632]\n",
      "epoch:27 step:26211 [D loss: 0.640901, acc: 63.28%] [G loss: 2.236256]\n",
      "epoch:27 step:26212 [D loss: 0.691918, acc: 54.69%] [G loss: 1.736756]\n",
      "epoch:27 step:26213 [D loss: 0.654302, acc: 58.59%] [G loss: 1.872016]\n",
      "epoch:27 step:26214 [D loss: 0.649349, acc: 67.19%] [G loss: 1.812289]\n",
      "epoch:27 step:26215 [D loss: 0.615534, acc: 67.19%] [G loss: 1.870225]\n",
      "epoch:27 step:26216 [D loss: 0.627500, acc: 67.97%] [G loss: 1.874523]\n",
      "epoch:27 step:26217 [D loss: 0.623143, acc: 65.62%] [G loss: 2.199076]\n",
      "epoch:27 step:26218 [D loss: 0.620633, acc: 63.28%] [G loss: 2.285377]\n",
      "epoch:27 step:26219 [D loss: 0.695621, acc: 57.81%] [G loss: 1.779142]\n",
      "epoch:27 step:26220 [D loss: 0.674428, acc: 61.72%] [G loss: 1.852434]\n",
      "epoch:27 step:26221 [D loss: 0.629733, acc: 63.28%] [G loss: 1.984854]\n",
      "epoch:27 step:26222 [D loss: 0.578176, acc: 67.97%] [G loss: 2.011243]\n",
      "epoch:27 step:26223 [D loss: 0.526718, acc: 80.47%] [G loss: 2.140833]\n",
      "epoch:27 step:26224 [D loss: 0.569078, acc: 70.31%] [G loss: 2.215061]\n",
      "epoch:27 step:26225 [D loss: 0.623814, acc: 67.19%] [G loss: 2.227996]\n",
      "epoch:27 step:26226 [D loss: 0.665733, acc: 58.59%] [G loss: 2.160385]\n",
      "epoch:27 step:26227 [D loss: 0.719732, acc: 57.81%] [G loss: 1.835679]\n",
      "epoch:27 step:26228 [D loss: 0.675824, acc: 57.03%] [G loss: 1.865349]\n",
      "epoch:27 step:26229 [D loss: 0.634954, acc: 67.97%] [G loss: 1.964036]\n",
      "epoch:27 step:26230 [D loss: 0.663495, acc: 58.59%] [G loss: 2.063473]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:27 step:26231 [D loss: 0.636053, acc: 68.75%] [G loss: 1.892782]\n",
      "epoch:27 step:26232 [D loss: 0.674163, acc: 63.28%] [G loss: 1.865153]\n",
      "epoch:27 step:26233 [D loss: 0.655346, acc: 61.72%] [G loss: 2.048216]\n",
      "epoch:27 step:26234 [D loss: 0.628157, acc: 64.06%] [G loss: 1.935643]\n",
      "epoch:27 step:26235 [D loss: 0.614741, acc: 67.19%] [G loss: 2.166077]\n",
      "epoch:27 step:26236 [D loss: 0.617798, acc: 64.06%] [G loss: 2.237791]\n",
      "epoch:28 step:26237 [D loss: 0.686209, acc: 60.16%] [G loss: 1.854958]\n",
      "epoch:28 step:26238 [D loss: 0.643234, acc: 60.94%] [G loss: 1.901815]\n",
      "epoch:28 step:26239 [D loss: 0.648348, acc: 59.38%] [G loss: 1.740700]\n",
      "epoch:28 step:26240 [D loss: 0.620978, acc: 67.97%] [G loss: 1.893896]\n",
      "epoch:28 step:26241 [D loss: 0.662895, acc: 64.84%] [G loss: 1.849540]\n",
      "epoch:28 step:26242 [D loss: 0.660355, acc: 64.06%] [G loss: 1.831645]\n",
      "epoch:28 step:26243 [D loss: 0.635236, acc: 66.41%] [G loss: 1.990953]\n",
      "epoch:28 step:26244 [D loss: 0.646015, acc: 61.72%] [G loss: 1.944868]\n",
      "epoch:28 step:26245 [D loss: 0.592380, acc: 68.75%] [G loss: 2.074144]\n",
      "epoch:28 step:26246 [D loss: 0.673356, acc: 59.38%] [G loss: 1.978673]\n",
      "epoch:28 step:26247 [D loss: 0.614067, acc: 64.84%] [G loss: 1.938881]\n",
      "epoch:28 step:26248 [D loss: 0.621778, acc: 67.19%] [G loss: 1.907301]\n",
      "epoch:28 step:26249 [D loss: 0.671026, acc: 64.06%] [G loss: 1.907500]\n",
      "epoch:28 step:26250 [D loss: 0.637845, acc: 63.28%] [G loss: 1.927261]\n",
      "epoch:28 step:26251 [D loss: 0.584712, acc: 67.97%] [G loss: 2.153141]\n",
      "epoch:28 step:26252 [D loss: 0.606606, acc: 66.41%] [G loss: 2.045645]\n",
      "epoch:28 step:26253 [D loss: 0.660604, acc: 60.16%] [G loss: 1.891754]\n",
      "epoch:28 step:26254 [D loss: 0.670639, acc: 57.81%] [G loss: 2.075128]\n",
      "epoch:28 step:26255 [D loss: 0.627497, acc: 62.50%] [G loss: 1.987454]\n",
      "epoch:28 step:26256 [D loss: 0.730294, acc: 52.34%] [G loss: 1.699637]\n",
      "epoch:28 step:26257 [D loss: 0.705661, acc: 55.47%] [G loss: 1.814281]\n",
      "epoch:28 step:26258 [D loss: 0.644823, acc: 68.75%] [G loss: 1.778167]\n",
      "epoch:28 step:26259 [D loss: 0.716502, acc: 62.50%] [G loss: 1.866657]\n",
      "epoch:28 step:26260 [D loss: 0.604323, acc: 64.06%] [G loss: 1.956862]\n",
      "epoch:28 step:26261 [D loss: 0.593571, acc: 67.19%] [G loss: 2.161967]\n",
      "epoch:28 step:26262 [D loss: 0.657413, acc: 64.84%] [G loss: 1.765182]\n",
      "epoch:28 step:26263 [D loss: 0.672975, acc: 59.38%] [G loss: 1.892792]\n",
      "epoch:28 step:26264 [D loss: 0.599022, acc: 67.97%] [G loss: 1.926858]\n",
      "epoch:28 step:26265 [D loss: 0.612233, acc: 67.19%] [G loss: 1.851206]\n",
      "epoch:28 step:26266 [D loss: 0.637968, acc: 68.75%] [G loss: 2.124596]\n",
      "epoch:28 step:26267 [D loss: 0.685843, acc: 60.16%] [G loss: 1.650378]\n",
      "epoch:28 step:26268 [D loss: 0.711533, acc: 50.78%] [G loss: 1.685330]\n",
      "epoch:28 step:26269 [D loss: 0.648942, acc: 64.06%] [G loss: 1.981369]\n",
      "epoch:28 step:26270 [D loss: 0.675011, acc: 53.12%] [G loss: 1.803611]\n",
      "epoch:28 step:26271 [D loss: 0.618951, acc: 66.41%] [G loss: 1.902219]\n",
      "epoch:28 step:26272 [D loss: 0.620305, acc: 67.97%] [G loss: 1.976558]\n",
      "epoch:28 step:26273 [D loss: 0.655064, acc: 61.72%] [G loss: 2.000162]\n",
      "epoch:28 step:26274 [D loss: 0.625298, acc: 66.41%] [G loss: 1.803531]\n",
      "epoch:28 step:26275 [D loss: 0.605687, acc: 71.88%] [G loss: 1.894230]\n",
      "epoch:28 step:26276 [D loss: 0.643880, acc: 59.38%] [G loss: 1.948578]\n",
      "epoch:28 step:26277 [D loss: 0.698594, acc: 57.03%] [G loss: 1.877072]\n",
      "epoch:28 step:26278 [D loss: 0.616307, acc: 57.81%] [G loss: 1.978339]\n",
      "epoch:28 step:26279 [D loss: 0.629263, acc: 67.19%] [G loss: 1.843352]\n",
      "epoch:28 step:26280 [D loss: 0.650224, acc: 65.62%] [G loss: 1.724085]\n",
      "epoch:28 step:26281 [D loss: 0.642666, acc: 58.59%] [G loss: 1.912803]\n",
      "epoch:28 step:26282 [D loss: 0.699183, acc: 56.25%] [G loss: 1.802810]\n",
      "epoch:28 step:26283 [D loss: 0.624050, acc: 64.06%] [G loss: 2.022188]\n",
      "epoch:28 step:26284 [D loss: 0.607372, acc: 66.41%] [G loss: 1.998975]\n",
      "epoch:28 step:26285 [D loss: 0.622325, acc: 65.62%] [G loss: 1.931144]\n",
      "epoch:28 step:26286 [D loss: 0.602310, acc: 67.19%] [G loss: 1.820462]\n",
      "epoch:28 step:26287 [D loss: 0.637692, acc: 59.38%] [G loss: 1.806193]\n",
      "epoch:28 step:26288 [D loss: 0.644988, acc: 56.25%] [G loss: 1.922335]\n",
      "epoch:28 step:26289 [D loss: 0.639519, acc: 59.38%] [G loss: 1.879182]\n",
      "epoch:28 step:26290 [D loss: 0.607159, acc: 65.62%] [G loss: 2.061314]\n",
      "epoch:28 step:26291 [D loss: 0.704620, acc: 53.91%] [G loss: 1.903744]\n",
      "epoch:28 step:26292 [D loss: 0.607214, acc: 65.62%] [G loss: 1.964998]\n",
      "epoch:28 step:26293 [D loss: 0.720218, acc: 56.25%] [G loss: 1.940782]\n",
      "epoch:28 step:26294 [D loss: 0.636396, acc: 64.06%] [G loss: 1.979531]\n",
      "epoch:28 step:26295 [D loss: 0.623540, acc: 60.94%] [G loss: 1.865781]\n",
      "epoch:28 step:26296 [D loss: 0.670347, acc: 55.47%] [G loss: 1.852510]\n",
      "epoch:28 step:26297 [D loss: 0.631116, acc: 69.53%] [G loss: 1.827872]\n",
      "epoch:28 step:26298 [D loss: 0.680237, acc: 56.25%] [G loss: 1.787237]\n",
      "epoch:28 step:26299 [D loss: 0.683927, acc: 58.59%] [G loss: 1.805772]\n",
      "epoch:28 step:26300 [D loss: 0.659243, acc: 64.06%] [G loss: 1.965995]\n",
      "epoch:28 step:26301 [D loss: 0.661525, acc: 60.94%] [G loss: 1.965551]\n",
      "epoch:28 step:26302 [D loss: 0.617040, acc: 71.09%] [G loss: 1.916123]\n",
      "epoch:28 step:26303 [D loss: 0.641441, acc: 64.84%] [G loss: 1.887862]\n",
      "epoch:28 step:26304 [D loss: 0.703390, acc: 55.47%] [G loss: 1.812216]\n",
      "epoch:28 step:26305 [D loss: 0.632436, acc: 67.97%] [G loss: 1.995352]\n",
      "epoch:28 step:26306 [D loss: 0.613636, acc: 64.84%] [G loss: 1.965519]\n",
      "epoch:28 step:26307 [D loss: 0.664619, acc: 56.25%] [G loss: 1.696336]\n",
      "epoch:28 step:26308 [D loss: 0.718473, acc: 51.56%] [G loss: 1.749950]\n",
      "epoch:28 step:26309 [D loss: 0.702150, acc: 55.47%] [G loss: 1.794386]\n",
      "epoch:28 step:26310 [D loss: 0.666402, acc: 55.47%] [G loss: 1.740739]\n",
      "epoch:28 step:26311 [D loss: 0.627294, acc: 70.31%] [G loss: 1.939427]\n",
      "epoch:28 step:26312 [D loss: 0.646287, acc: 62.50%] [G loss: 1.795044]\n",
      "epoch:28 step:26313 [D loss: 0.566065, acc: 73.44%] [G loss: 1.917084]\n",
      "epoch:28 step:26314 [D loss: 0.646319, acc: 61.72%] [G loss: 1.742734]\n",
      "epoch:28 step:26315 [D loss: 0.669906, acc: 60.16%] [G loss: 1.795836]\n",
      "epoch:28 step:26316 [D loss: 0.663361, acc: 60.94%] [G loss: 1.786316]\n",
      "epoch:28 step:26317 [D loss: 0.657736, acc: 59.38%] [G loss: 1.634539]\n",
      "epoch:28 step:26318 [D loss: 0.676020, acc: 56.25%] [G loss: 1.747822]\n",
      "epoch:28 step:26319 [D loss: 0.662408, acc: 57.03%] [G loss: 1.835987]\n",
      "epoch:28 step:26320 [D loss: 0.615268, acc: 64.84%] [G loss: 1.865803]\n",
      "epoch:28 step:26321 [D loss: 0.663376, acc: 60.16%] [G loss: 1.677542]\n",
      "epoch:28 step:26322 [D loss: 0.711877, acc: 55.47%] [G loss: 1.769506]\n",
      "epoch:28 step:26323 [D loss: 0.674830, acc: 61.72%] [G loss: 1.835981]\n",
      "epoch:28 step:26324 [D loss: 0.681764, acc: 57.81%] [G loss: 1.851298]\n",
      "epoch:28 step:26325 [D loss: 0.613217, acc: 65.62%] [G loss: 1.893484]\n",
      "epoch:28 step:26326 [D loss: 0.668726, acc: 53.91%] [G loss: 1.869482]\n",
      "epoch:28 step:26327 [D loss: 0.624342, acc: 61.72%] [G loss: 1.820037]\n",
      "epoch:28 step:26328 [D loss: 0.659943, acc: 63.28%] [G loss: 1.831366]\n",
      "epoch:28 step:26329 [D loss: 0.630373, acc: 64.06%] [G loss: 1.905321]\n",
      "epoch:28 step:26330 [D loss: 0.635411, acc: 64.06%] [G loss: 1.805549]\n",
      "epoch:28 step:26331 [D loss: 0.634339, acc: 65.62%] [G loss: 1.768238]\n",
      "epoch:28 step:26332 [D loss: 0.653952, acc: 60.16%] [G loss: 1.892777]\n",
      "epoch:28 step:26333 [D loss: 0.676612, acc: 53.12%] [G loss: 1.905396]\n",
      "epoch:28 step:26334 [D loss: 0.672153, acc: 62.50%] [G loss: 1.709634]\n",
      "epoch:28 step:26335 [D loss: 0.630611, acc: 62.50%] [G loss: 1.714729]\n",
      "epoch:28 step:26336 [D loss: 0.648805, acc: 62.50%] [G loss: 1.911086]\n",
      "epoch:28 step:26337 [D loss: 0.654113, acc: 64.06%] [G loss: 1.831539]\n",
      "epoch:28 step:26338 [D loss: 0.719999, acc: 50.78%] [G loss: 1.815734]\n",
      "epoch:28 step:26339 [D loss: 0.623468, acc: 68.75%] [G loss: 1.763188]\n",
      "epoch:28 step:26340 [D loss: 0.697312, acc: 52.34%] [G loss: 1.773839]\n",
      "epoch:28 step:26341 [D loss: 0.643865, acc: 64.06%] [G loss: 1.762112]\n",
      "epoch:28 step:26342 [D loss: 0.609491, acc: 67.97%] [G loss: 2.068744]\n",
      "epoch:28 step:26343 [D loss: 0.649219, acc: 63.28%] [G loss: 1.977895]\n",
      "epoch:28 step:26344 [D loss: 0.683883, acc: 57.03%] [G loss: 1.704806]\n",
      "epoch:28 step:26345 [D loss: 0.673733, acc: 56.25%] [G loss: 1.733774]\n",
      "epoch:28 step:26346 [D loss: 0.713151, acc: 51.56%] [G loss: 1.758017]\n",
      "epoch:28 step:26347 [D loss: 0.615108, acc: 64.84%] [G loss: 1.940866]\n",
      "epoch:28 step:26348 [D loss: 0.630607, acc: 67.19%] [G loss: 1.978361]\n",
      "epoch:28 step:26349 [D loss: 0.585729, acc: 69.53%] [G loss: 2.020924]\n",
      "epoch:28 step:26350 [D loss: 0.582495, acc: 68.75%] [G loss: 2.106473]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:28 step:26351 [D loss: 0.597681, acc: 70.31%] [G loss: 2.146823]\n",
      "epoch:28 step:26352 [D loss: 0.586650, acc: 69.53%] [G loss: 2.022523]\n",
      "epoch:28 step:26353 [D loss: 0.607174, acc: 64.06%] [G loss: 2.056788]\n",
      "epoch:28 step:26354 [D loss: 0.604406, acc: 68.75%] [G loss: 2.036220]\n",
      "epoch:28 step:26355 [D loss: 0.576725, acc: 67.19%] [G loss: 2.180540]\n",
      "epoch:28 step:26356 [D loss: 0.732244, acc: 57.03%] [G loss: 2.060803]\n",
      "epoch:28 step:26357 [D loss: 0.689516, acc: 56.25%] [G loss: 1.856028]\n",
      "epoch:28 step:26358 [D loss: 0.609652, acc: 64.84%] [G loss: 2.073083]\n",
      "epoch:28 step:26359 [D loss: 0.643881, acc: 63.28%] [G loss: 1.882860]\n",
      "epoch:28 step:26360 [D loss: 0.718605, acc: 57.03%] [G loss: 1.889755]\n",
      "epoch:28 step:26361 [D loss: 0.695740, acc: 57.03%] [G loss: 1.637976]\n",
      "epoch:28 step:26362 [D loss: 0.597841, acc: 70.31%] [G loss: 1.876830]\n",
      "epoch:28 step:26363 [D loss: 0.625152, acc: 64.84%] [G loss: 1.838544]\n",
      "epoch:28 step:26364 [D loss: 0.651473, acc: 64.84%] [G loss: 1.783698]\n",
      "epoch:28 step:26365 [D loss: 0.659189, acc: 64.06%] [G loss: 1.773935]\n",
      "epoch:28 step:26366 [D loss: 0.705450, acc: 57.03%] [G loss: 1.838620]\n",
      "epoch:28 step:26367 [D loss: 0.624192, acc: 63.28%] [G loss: 1.860220]\n",
      "epoch:28 step:26368 [D loss: 0.661967, acc: 56.25%] [G loss: 1.941184]\n",
      "epoch:28 step:26369 [D loss: 0.700575, acc: 53.12%] [G loss: 1.787921]\n",
      "epoch:28 step:26370 [D loss: 0.657357, acc: 63.28%] [G loss: 1.719739]\n",
      "epoch:28 step:26371 [D loss: 0.624649, acc: 65.62%] [G loss: 1.798034]\n",
      "epoch:28 step:26372 [D loss: 0.662994, acc: 53.91%] [G loss: 1.884849]\n",
      "epoch:28 step:26373 [D loss: 0.653138, acc: 63.28%] [G loss: 1.877506]\n",
      "epoch:28 step:26374 [D loss: 0.638468, acc: 65.62%] [G loss: 1.818745]\n",
      "epoch:28 step:26375 [D loss: 0.627256, acc: 64.84%] [G loss: 1.752807]\n",
      "epoch:28 step:26376 [D loss: 0.700357, acc: 57.03%] [G loss: 1.752415]\n",
      "epoch:28 step:26377 [D loss: 0.690718, acc: 53.12%] [G loss: 1.882983]\n",
      "epoch:28 step:26378 [D loss: 0.642960, acc: 66.41%] [G loss: 1.773083]\n",
      "epoch:28 step:26379 [D loss: 0.661450, acc: 60.94%] [G loss: 1.837419]\n",
      "epoch:28 step:26380 [D loss: 0.623349, acc: 63.28%] [G loss: 1.944041]\n",
      "epoch:28 step:26381 [D loss: 0.597707, acc: 68.75%] [G loss: 1.992882]\n",
      "epoch:28 step:26382 [D loss: 0.605523, acc: 68.75%] [G loss: 1.989114]\n",
      "epoch:28 step:26383 [D loss: 0.658151, acc: 63.28%] [G loss: 1.848140]\n",
      "epoch:28 step:26384 [D loss: 0.631276, acc: 57.81%] [G loss: 2.001408]\n",
      "epoch:28 step:26385 [D loss: 0.684134, acc: 60.94%] [G loss: 1.887966]\n",
      "epoch:28 step:26386 [D loss: 0.691277, acc: 57.03%] [G loss: 1.849252]\n",
      "epoch:28 step:26387 [D loss: 0.681189, acc: 67.19%] [G loss: 1.909291]\n",
      "epoch:28 step:26388 [D loss: 0.630958, acc: 60.16%] [G loss: 1.938259]\n",
      "epoch:28 step:26389 [D loss: 0.670450, acc: 59.38%] [G loss: 1.804335]\n",
      "epoch:28 step:26390 [D loss: 0.638884, acc: 62.50%] [G loss: 1.951928]\n",
      "epoch:28 step:26391 [D loss: 0.648117, acc: 63.28%] [G loss: 1.807725]\n",
      "epoch:28 step:26392 [D loss: 0.643823, acc: 65.62%] [G loss: 1.908086]\n",
      "epoch:28 step:26393 [D loss: 0.694128, acc: 57.03%] [G loss: 1.781047]\n",
      "epoch:28 step:26394 [D loss: 0.658289, acc: 64.84%] [G loss: 1.703748]\n",
      "epoch:28 step:26395 [D loss: 0.636204, acc: 60.94%] [G loss: 1.968037]\n",
      "epoch:28 step:26396 [D loss: 0.663061, acc: 57.81%] [G loss: 1.681630]\n",
      "epoch:28 step:26397 [D loss: 0.692892, acc: 57.81%] [G loss: 1.782328]\n",
      "epoch:28 step:26398 [D loss: 0.682436, acc: 59.38%] [G loss: 1.774521]\n",
      "epoch:28 step:26399 [D loss: 0.646728, acc: 59.38%] [G loss: 1.776011]\n",
      "epoch:28 step:26400 [D loss: 0.680196, acc: 56.25%] [G loss: 1.807414]\n",
      "##############\n",
      "[2.38627058 1.40684604 6.55642503 4.84257485 3.46893644 5.55179898\n",
      " 4.42269249 4.59791449 4.53933165 3.54371764]\n",
      "##########\n",
      "epoch:28 step:26401 [D loss: 0.665972, acc: 62.50%] [G loss: 1.733955]\n",
      "epoch:28 step:26402 [D loss: 0.621385, acc: 67.19%] [G loss: 1.806184]\n",
      "epoch:28 step:26403 [D loss: 0.670906, acc: 63.28%] [G loss: 1.724621]\n",
      "epoch:28 step:26404 [D loss: 0.611753, acc: 63.28%] [G loss: 1.845892]\n",
      "epoch:28 step:26405 [D loss: 0.674147, acc: 62.50%] [G loss: 1.822042]\n",
      "epoch:28 step:26406 [D loss: 0.653257, acc: 60.16%] [G loss: 1.735174]\n",
      "epoch:28 step:26407 [D loss: 0.625401, acc: 59.38%] [G loss: 1.897443]\n",
      "epoch:28 step:26408 [D loss: 0.678378, acc: 57.03%] [G loss: 1.874616]\n",
      "epoch:28 step:26409 [D loss: 0.662559, acc: 64.06%] [G loss: 1.784564]\n",
      "epoch:28 step:26410 [D loss: 0.638810, acc: 64.06%] [G loss: 1.723730]\n",
      "epoch:28 step:26411 [D loss: 0.664773, acc: 54.69%] [G loss: 1.724567]\n",
      "epoch:28 step:26412 [D loss: 0.742512, acc: 50.78%] [G loss: 1.837827]\n",
      "epoch:28 step:26413 [D loss: 0.648209, acc: 64.84%] [G loss: 1.802803]\n",
      "epoch:28 step:26414 [D loss: 0.683372, acc: 56.25%] [G loss: 1.751137]\n",
      "epoch:28 step:26415 [D loss: 0.655338, acc: 60.16%] [G loss: 1.741525]\n",
      "epoch:28 step:26416 [D loss: 0.642437, acc: 60.16%] [G loss: 1.704674]\n",
      "epoch:28 step:26417 [D loss: 0.644362, acc: 57.81%] [G loss: 1.766299]\n",
      "epoch:28 step:26418 [D loss: 0.632285, acc: 64.84%] [G loss: 1.818980]\n",
      "epoch:28 step:26419 [D loss: 0.658682, acc: 56.25%] [G loss: 1.829322]\n",
      "epoch:28 step:26420 [D loss: 0.623187, acc: 65.62%] [G loss: 1.898524]\n",
      "epoch:28 step:26421 [D loss: 0.638225, acc: 65.62%] [G loss: 1.936155]\n",
      "epoch:28 step:26422 [D loss: 0.644717, acc: 64.06%] [G loss: 1.923264]\n",
      "epoch:28 step:26423 [D loss: 0.618220, acc: 67.19%] [G loss: 1.927525]\n",
      "epoch:28 step:26424 [D loss: 0.627190, acc: 64.06%] [G loss: 1.874273]\n",
      "epoch:28 step:26425 [D loss: 0.660435, acc: 60.94%] [G loss: 1.830960]\n",
      "epoch:28 step:26426 [D loss: 0.593000, acc: 67.19%] [G loss: 1.860016]\n",
      "epoch:28 step:26427 [D loss: 0.615533, acc: 67.19%] [G loss: 1.960709]\n",
      "epoch:28 step:26428 [D loss: 0.666892, acc: 61.72%] [G loss: 1.863428]\n",
      "epoch:28 step:26429 [D loss: 0.639778, acc: 64.84%] [G loss: 1.996835]\n",
      "epoch:28 step:26430 [D loss: 0.616079, acc: 65.62%] [G loss: 2.090313]\n",
      "epoch:28 step:26431 [D loss: 0.632426, acc: 65.62%] [G loss: 2.013415]\n",
      "epoch:28 step:26432 [D loss: 0.645748, acc: 66.41%] [G loss: 1.833661]\n",
      "epoch:28 step:26433 [D loss: 0.666427, acc: 67.97%] [G loss: 1.889521]\n",
      "epoch:28 step:26434 [D loss: 0.671453, acc: 58.59%] [G loss: 1.990756]\n",
      "epoch:28 step:26435 [D loss: 0.656950, acc: 63.28%] [G loss: 1.986357]\n",
      "epoch:28 step:26436 [D loss: 0.688508, acc: 56.25%] [G loss: 1.641960]\n",
      "epoch:28 step:26437 [D loss: 0.644883, acc: 66.41%] [G loss: 1.831559]\n",
      "epoch:28 step:26438 [D loss: 0.655133, acc: 61.72%] [G loss: 1.827340]\n",
      "epoch:28 step:26439 [D loss: 0.691457, acc: 60.94%] [G loss: 1.760632]\n",
      "epoch:28 step:26440 [D loss: 0.662461, acc: 57.81%] [G loss: 1.809249]\n",
      "epoch:28 step:26441 [D loss: 0.659073, acc: 64.06%] [G loss: 1.864408]\n",
      "epoch:28 step:26442 [D loss: 0.632663, acc: 63.28%] [G loss: 2.067829]\n",
      "epoch:28 step:26443 [D loss: 0.636457, acc: 65.62%] [G loss: 1.905291]\n",
      "epoch:28 step:26444 [D loss: 0.614956, acc: 61.72%] [G loss: 2.170209]\n",
      "epoch:28 step:26445 [D loss: 0.621731, acc: 64.06%] [G loss: 2.215407]\n",
      "epoch:28 step:26446 [D loss: 0.671218, acc: 56.25%] [G loss: 1.850686]\n",
      "epoch:28 step:26447 [D loss: 0.653635, acc: 58.59%] [G loss: 1.832221]\n",
      "epoch:28 step:26448 [D loss: 0.726456, acc: 57.03%] [G loss: 1.725999]\n",
      "epoch:28 step:26449 [D loss: 0.644626, acc: 60.16%] [G loss: 1.850425]\n",
      "epoch:28 step:26450 [D loss: 0.686467, acc: 58.59%] [G loss: 1.795064]\n",
      "epoch:28 step:26451 [D loss: 0.679910, acc: 60.16%] [G loss: 1.729631]\n",
      "epoch:28 step:26452 [D loss: 0.616709, acc: 69.53%] [G loss: 1.837853]\n",
      "epoch:28 step:26453 [D loss: 0.626555, acc: 65.62%] [G loss: 1.954944]\n",
      "epoch:28 step:26454 [D loss: 0.636011, acc: 69.53%] [G loss: 1.983224]\n",
      "epoch:28 step:26455 [D loss: 0.593850, acc: 71.09%] [G loss: 1.957284]\n",
      "epoch:28 step:26456 [D loss: 0.753259, acc: 46.09%] [G loss: 1.771541]\n",
      "epoch:28 step:26457 [D loss: 0.659575, acc: 56.25%] [G loss: 1.879714]\n",
      "epoch:28 step:26458 [D loss: 0.690076, acc: 51.56%] [G loss: 1.831087]\n",
      "epoch:28 step:26459 [D loss: 0.681239, acc: 56.25%] [G loss: 1.758942]\n",
      "epoch:28 step:26460 [D loss: 0.615271, acc: 67.97%] [G loss: 1.854712]\n",
      "epoch:28 step:26461 [D loss: 0.655475, acc: 58.59%] [G loss: 1.876420]\n",
      "epoch:28 step:26462 [D loss: 0.627838, acc: 65.62%] [G loss: 1.782995]\n",
      "epoch:28 step:26463 [D loss: 0.600515, acc: 70.31%] [G loss: 1.861175]\n",
      "epoch:28 step:26464 [D loss: 0.625972, acc: 62.50%] [G loss: 1.803130]\n",
      "epoch:28 step:26465 [D loss: 0.600555, acc: 67.97%] [G loss: 2.010572]\n",
      "epoch:28 step:26466 [D loss: 0.572677, acc: 71.88%] [G loss: 2.017142]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:28 step:26467 [D loss: 0.648987, acc: 61.72%] [G loss: 2.422014]\n",
      "epoch:28 step:26468 [D loss: 0.567377, acc: 73.44%] [G loss: 2.238580]\n",
      "epoch:28 step:26469 [D loss: 0.640783, acc: 58.59%] [G loss: 1.955647]\n",
      "epoch:28 step:26470 [D loss: 0.670503, acc: 60.94%] [G loss: 1.950912]\n",
      "epoch:28 step:26471 [D loss: 0.673507, acc: 57.81%] [G loss: 1.853695]\n",
      "epoch:28 step:26472 [D loss: 0.639685, acc: 63.28%] [G loss: 1.865659]\n",
      "epoch:28 step:26473 [D loss: 0.578604, acc: 69.53%] [G loss: 1.970432]\n",
      "epoch:28 step:26474 [D loss: 0.640681, acc: 62.50%] [G loss: 1.889746]\n",
      "epoch:28 step:26475 [D loss: 0.640272, acc: 63.28%] [G loss: 1.910324]\n",
      "epoch:28 step:26476 [D loss: 0.654695, acc: 64.84%] [G loss: 1.844028]\n",
      "epoch:28 step:26477 [D loss: 0.678649, acc: 60.94%] [G loss: 1.978694]\n",
      "epoch:28 step:26478 [D loss: 0.667401, acc: 62.50%] [G loss: 1.971451]\n",
      "epoch:28 step:26479 [D loss: 0.662376, acc: 53.91%] [G loss: 1.939164]\n",
      "epoch:28 step:26480 [D loss: 0.717521, acc: 57.03%] [G loss: 1.935571]\n",
      "epoch:28 step:26481 [D loss: 0.664222, acc: 57.81%] [G loss: 1.880453]\n",
      "epoch:28 step:26482 [D loss: 0.607764, acc: 68.75%] [G loss: 1.946154]\n",
      "epoch:28 step:26483 [D loss: 0.619761, acc: 67.19%] [G loss: 1.825278]\n",
      "epoch:28 step:26484 [D loss: 0.672286, acc: 60.16%] [G loss: 1.946134]\n",
      "epoch:28 step:26485 [D loss: 0.635563, acc: 65.62%] [G loss: 1.871505]\n",
      "epoch:28 step:26486 [D loss: 0.687928, acc: 53.12%] [G loss: 1.687940]\n",
      "epoch:28 step:26487 [D loss: 0.691112, acc: 56.25%] [G loss: 1.712550]\n",
      "epoch:28 step:26488 [D loss: 0.625514, acc: 64.84%] [G loss: 1.778021]\n",
      "epoch:28 step:26489 [D loss: 0.639841, acc: 64.06%] [G loss: 1.907019]\n",
      "epoch:28 step:26490 [D loss: 0.659889, acc: 57.81%] [G loss: 1.817785]\n",
      "epoch:28 step:26491 [D loss: 0.669394, acc: 61.72%] [G loss: 1.740782]\n",
      "epoch:28 step:26492 [D loss: 0.655980, acc: 58.59%] [G loss: 1.780127]\n",
      "epoch:28 step:26493 [D loss: 0.688161, acc: 50.78%] [G loss: 1.750980]\n",
      "epoch:28 step:26494 [D loss: 0.664511, acc: 60.94%] [G loss: 1.778510]\n",
      "epoch:28 step:26495 [D loss: 0.628641, acc: 63.28%] [G loss: 1.754570]\n",
      "epoch:28 step:26496 [D loss: 0.668746, acc: 66.41%] [G loss: 1.921205]\n",
      "epoch:28 step:26497 [D loss: 0.674644, acc: 63.28%] [G loss: 1.894576]\n",
      "epoch:28 step:26498 [D loss: 0.609502, acc: 62.50%] [G loss: 1.892978]\n",
      "epoch:28 step:26499 [D loss: 0.647033, acc: 64.84%] [G loss: 1.931140]\n",
      "epoch:28 step:26500 [D loss: 0.648851, acc: 63.28%] [G loss: 2.000740]\n",
      "epoch:28 step:26501 [D loss: 0.691850, acc: 57.81%] [G loss: 1.664462]\n",
      "epoch:28 step:26502 [D loss: 0.660919, acc: 60.94%] [G loss: 1.850891]\n",
      "epoch:28 step:26503 [D loss: 0.622269, acc: 65.62%] [G loss: 1.843398]\n",
      "epoch:28 step:26504 [D loss: 0.668203, acc: 57.03%] [G loss: 1.816609]\n",
      "epoch:28 step:26505 [D loss: 0.618374, acc: 62.50%] [G loss: 1.929703]\n",
      "epoch:28 step:26506 [D loss: 0.651977, acc: 63.28%] [G loss: 1.869762]\n",
      "epoch:28 step:26507 [D loss: 0.650654, acc: 61.72%] [G loss: 1.769194]\n",
      "epoch:28 step:26508 [D loss: 0.601381, acc: 67.97%] [G loss: 1.888799]\n",
      "epoch:28 step:26509 [D loss: 0.660173, acc: 64.84%] [G loss: 1.852888]\n",
      "epoch:28 step:26510 [D loss: 0.611773, acc: 67.19%] [G loss: 1.878003]\n",
      "epoch:28 step:26511 [D loss: 0.658048, acc: 60.94%] [G loss: 1.963941]\n",
      "epoch:28 step:26512 [D loss: 0.562936, acc: 71.09%] [G loss: 2.135449]\n",
      "epoch:28 step:26513 [D loss: 0.665489, acc: 57.03%] [G loss: 1.904742]\n",
      "epoch:28 step:26514 [D loss: 0.617775, acc: 66.41%] [G loss: 1.941265]\n",
      "epoch:28 step:26515 [D loss: 0.661512, acc: 58.59%] [G loss: 1.830686]\n",
      "epoch:28 step:26516 [D loss: 0.575830, acc: 67.19%] [G loss: 2.072615]\n",
      "epoch:28 step:26517 [D loss: 0.658745, acc: 64.06%] [G loss: 1.846695]\n",
      "epoch:28 step:26518 [D loss: 0.665597, acc: 59.38%] [G loss: 1.704483]\n",
      "epoch:28 step:26519 [D loss: 0.596779, acc: 72.66%] [G loss: 1.942483]\n",
      "epoch:28 step:26520 [D loss: 0.727633, acc: 55.47%] [G loss: 1.804560]\n",
      "epoch:28 step:26521 [D loss: 0.672294, acc: 59.38%] [G loss: 1.764807]\n",
      "epoch:28 step:26522 [D loss: 0.589506, acc: 66.41%] [G loss: 2.002943]\n",
      "epoch:28 step:26523 [D loss: 0.657386, acc: 66.41%] [G loss: 1.797386]\n",
      "epoch:28 step:26524 [D loss: 0.686340, acc: 59.38%] [G loss: 1.837311]\n",
      "epoch:28 step:26525 [D loss: 0.692127, acc: 57.03%] [G loss: 1.806942]\n",
      "epoch:28 step:26526 [D loss: 0.648149, acc: 63.28%] [G loss: 1.880867]\n",
      "epoch:28 step:26527 [D loss: 0.617350, acc: 64.84%] [G loss: 1.771898]\n",
      "epoch:28 step:26528 [D loss: 0.640149, acc: 62.50%] [G loss: 1.921411]\n",
      "epoch:28 step:26529 [D loss: 0.611821, acc: 64.06%] [G loss: 1.778125]\n",
      "epoch:28 step:26530 [D loss: 0.618763, acc: 66.41%] [G loss: 1.729070]\n",
      "epoch:28 step:26531 [D loss: 0.659995, acc: 62.50%] [G loss: 1.825399]\n",
      "epoch:28 step:26532 [D loss: 0.608239, acc: 66.41%] [G loss: 2.010521]\n",
      "epoch:28 step:26533 [D loss: 0.645182, acc: 60.94%] [G loss: 1.870949]\n",
      "epoch:28 step:26534 [D loss: 0.680303, acc: 57.81%] [G loss: 1.911312]\n",
      "epoch:28 step:26535 [D loss: 0.653240, acc: 60.16%] [G loss: 2.042179]\n",
      "epoch:28 step:26536 [D loss: 0.690381, acc: 58.59%] [G loss: 1.954796]\n",
      "epoch:28 step:26537 [D loss: 0.703371, acc: 52.34%] [G loss: 1.776163]\n",
      "epoch:28 step:26538 [D loss: 0.666312, acc: 60.16%] [G loss: 1.833274]\n",
      "epoch:28 step:26539 [D loss: 0.686051, acc: 54.69%] [G loss: 1.689203]\n",
      "epoch:28 step:26540 [D loss: 0.664481, acc: 60.16%] [G loss: 1.901974]\n",
      "epoch:28 step:26541 [D loss: 0.655280, acc: 66.41%] [G loss: 1.833769]\n",
      "epoch:28 step:26542 [D loss: 0.673126, acc: 54.69%] [G loss: 1.723893]\n",
      "epoch:28 step:26543 [D loss: 0.676271, acc: 58.59%] [G loss: 1.829084]\n",
      "epoch:28 step:26544 [D loss: 0.631372, acc: 64.06%] [G loss: 1.765751]\n",
      "epoch:28 step:26545 [D loss: 0.638771, acc: 60.94%] [G loss: 1.816192]\n",
      "epoch:28 step:26546 [D loss: 0.653864, acc: 60.16%] [G loss: 1.763112]\n",
      "epoch:28 step:26547 [D loss: 0.614562, acc: 64.06%] [G loss: 1.883213]\n",
      "epoch:28 step:26548 [D loss: 0.660539, acc: 66.41%] [G loss: 2.053604]\n",
      "epoch:28 step:26549 [D loss: 0.565195, acc: 72.66%] [G loss: 2.010292]\n",
      "epoch:28 step:26550 [D loss: 0.575532, acc: 67.97%] [G loss: 2.214790]\n",
      "epoch:28 step:26551 [D loss: 0.573377, acc: 67.97%] [G loss: 2.191228]\n",
      "epoch:28 step:26552 [D loss: 0.663472, acc: 57.81%] [G loss: 1.848192]\n",
      "epoch:28 step:26553 [D loss: 0.664075, acc: 62.50%] [G loss: 1.770978]\n",
      "epoch:28 step:26554 [D loss: 0.627878, acc: 62.50%] [G loss: 1.884880]\n",
      "epoch:28 step:26555 [D loss: 0.676890, acc: 57.81%] [G loss: 1.702085]\n",
      "epoch:28 step:26556 [D loss: 0.658064, acc: 57.03%] [G loss: 1.802488]\n",
      "epoch:28 step:26557 [D loss: 0.670858, acc: 61.72%] [G loss: 1.828120]\n",
      "epoch:28 step:26558 [D loss: 0.641356, acc: 66.41%] [G loss: 1.791036]\n",
      "epoch:28 step:26559 [D loss: 0.677183, acc: 59.38%] [G loss: 1.863424]\n",
      "epoch:28 step:26560 [D loss: 0.663188, acc: 57.81%] [G loss: 1.790811]\n",
      "epoch:28 step:26561 [D loss: 0.631236, acc: 61.72%] [G loss: 1.860621]\n",
      "epoch:28 step:26562 [D loss: 0.619724, acc: 69.53%] [G loss: 1.817831]\n",
      "epoch:28 step:26563 [D loss: 0.640581, acc: 61.72%] [G loss: 1.825215]\n",
      "epoch:28 step:26564 [D loss: 0.640226, acc: 65.62%] [G loss: 1.862202]\n",
      "epoch:28 step:26565 [D loss: 0.663196, acc: 60.16%] [G loss: 1.837088]\n",
      "epoch:28 step:26566 [D loss: 0.642192, acc: 60.94%] [G loss: 1.937411]\n",
      "epoch:28 step:26567 [D loss: 0.671717, acc: 58.59%] [G loss: 2.016777]\n",
      "epoch:28 step:26568 [D loss: 0.595240, acc: 68.75%] [G loss: 1.997058]\n",
      "epoch:28 step:26569 [D loss: 0.629833, acc: 60.94%] [G loss: 1.963354]\n",
      "epoch:28 step:26570 [D loss: 0.651158, acc: 59.38%] [G loss: 2.031945]\n",
      "epoch:28 step:26571 [D loss: 0.652557, acc: 62.50%] [G loss: 1.948370]\n",
      "epoch:28 step:26572 [D loss: 0.637680, acc: 64.06%] [G loss: 1.942710]\n",
      "epoch:28 step:26573 [D loss: 0.670913, acc: 60.16%] [G loss: 1.897993]\n",
      "epoch:28 step:26574 [D loss: 0.634849, acc: 65.62%] [G loss: 1.974669]\n",
      "epoch:28 step:26575 [D loss: 0.614770, acc: 66.41%] [G loss: 2.065514]\n",
      "epoch:28 step:26576 [D loss: 0.650310, acc: 57.81%] [G loss: 1.935344]\n",
      "epoch:28 step:26577 [D loss: 0.704753, acc: 57.03%] [G loss: 1.791819]\n",
      "epoch:28 step:26578 [D loss: 0.693401, acc: 55.47%] [G loss: 1.823212]\n",
      "epoch:28 step:26579 [D loss: 0.706167, acc: 55.47%] [G loss: 1.868568]\n",
      "epoch:28 step:26580 [D loss: 0.645350, acc: 64.06%] [G loss: 1.928500]\n",
      "epoch:28 step:26581 [D loss: 0.641785, acc: 64.84%] [G loss: 2.041969]\n",
      "epoch:28 step:26582 [D loss: 0.637129, acc: 64.84%] [G loss: 1.996662]\n",
      "epoch:28 step:26583 [D loss: 0.544016, acc: 73.44%] [G loss: 2.141929]\n",
      "epoch:28 step:26584 [D loss: 0.729369, acc: 50.78%] [G loss: 1.880210]\n",
      "epoch:28 step:26585 [D loss: 0.705370, acc: 49.22%] [G loss: 1.640748]\n",
      "epoch:28 step:26586 [D loss: 0.661360, acc: 64.06%] [G loss: 1.864429]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:28 step:26587 [D loss: 0.696953, acc: 53.12%] [G loss: 1.746294]\n",
      "epoch:28 step:26588 [D loss: 0.656521, acc: 62.50%] [G loss: 1.861621]\n",
      "epoch:28 step:26589 [D loss: 0.676875, acc: 53.91%] [G loss: 1.797482]\n",
      "epoch:28 step:26590 [D loss: 0.663261, acc: 60.94%] [G loss: 1.943296]\n",
      "epoch:28 step:26591 [D loss: 0.694516, acc: 61.72%] [G loss: 1.781146]\n",
      "epoch:28 step:26592 [D loss: 0.698394, acc: 56.25%] [G loss: 1.756153]\n",
      "epoch:28 step:26593 [D loss: 0.690859, acc: 56.25%] [G loss: 1.817608]\n",
      "epoch:28 step:26594 [D loss: 0.621664, acc: 67.19%] [G loss: 1.819354]\n",
      "epoch:28 step:26595 [D loss: 0.657169, acc: 58.59%] [G loss: 1.899611]\n",
      "epoch:28 step:26596 [D loss: 0.600911, acc: 67.19%] [G loss: 1.942218]\n",
      "epoch:28 step:26597 [D loss: 0.651910, acc: 59.38%] [G loss: 1.863683]\n",
      "epoch:28 step:26598 [D loss: 0.639989, acc: 64.06%] [G loss: 1.761661]\n",
      "epoch:28 step:26599 [D loss: 0.639079, acc: 61.72%] [G loss: 1.740378]\n",
      "epoch:28 step:26600 [D loss: 0.681920, acc: 60.94%] [G loss: 1.798783]\n",
      "##############\n",
      "[2.53372846 1.50559899 6.28680684 4.80382487 3.50897998 5.40533848\n",
      " 4.36700514 4.58191948 4.55251049 3.6026216 ]\n",
      "##########\n",
      "epoch:28 step:26601 [D loss: 0.635489, acc: 67.97%] [G loss: 1.792161]\n",
      "epoch:28 step:26602 [D loss: 0.653800, acc: 62.50%] [G loss: 1.837387]\n",
      "epoch:28 step:26603 [D loss: 0.629484, acc: 67.97%] [G loss: 1.898430]\n",
      "epoch:28 step:26604 [D loss: 0.633739, acc: 66.41%] [G loss: 1.744089]\n",
      "epoch:28 step:26605 [D loss: 0.618782, acc: 67.97%] [G loss: 1.880039]\n",
      "epoch:28 step:26606 [D loss: 0.611271, acc: 63.28%] [G loss: 1.882667]\n",
      "epoch:28 step:26607 [D loss: 0.658551, acc: 61.72%] [G loss: 1.920090]\n",
      "epoch:28 step:26608 [D loss: 0.671446, acc: 66.41%] [G loss: 1.869650]\n",
      "epoch:28 step:26609 [D loss: 0.652312, acc: 59.38%] [G loss: 1.713857]\n",
      "epoch:28 step:26610 [D loss: 0.649768, acc: 59.38%] [G loss: 1.938345]\n",
      "epoch:28 step:26611 [D loss: 0.651543, acc: 60.94%] [G loss: 1.803206]\n",
      "epoch:28 step:26612 [D loss: 0.661721, acc: 60.94%] [G loss: 1.934767]\n",
      "epoch:28 step:26613 [D loss: 0.707605, acc: 60.94%] [G loss: 1.750899]\n",
      "epoch:28 step:26614 [D loss: 0.663059, acc: 61.72%] [G loss: 1.747823]\n",
      "epoch:28 step:26615 [D loss: 0.647974, acc: 59.38%] [G loss: 1.937224]\n",
      "epoch:28 step:26616 [D loss: 0.672290, acc: 57.03%] [G loss: 1.864072]\n",
      "epoch:28 step:26617 [D loss: 0.658379, acc: 60.94%] [G loss: 2.019972]\n",
      "epoch:28 step:26618 [D loss: 0.688759, acc: 55.47%] [G loss: 1.849516]\n",
      "epoch:28 step:26619 [D loss: 0.653398, acc: 62.50%] [G loss: 1.940945]\n",
      "epoch:28 step:26620 [D loss: 0.616165, acc: 68.75%] [G loss: 1.782813]\n",
      "epoch:28 step:26621 [D loss: 0.604305, acc: 65.62%] [G loss: 2.104267]\n",
      "epoch:28 step:26622 [D loss: 0.652035, acc: 61.72%] [G loss: 1.852827]\n",
      "epoch:28 step:26623 [D loss: 0.627618, acc: 64.06%] [G loss: 1.850349]\n",
      "epoch:28 step:26624 [D loss: 0.641333, acc: 61.72%] [G loss: 1.913564]\n",
      "epoch:28 step:26625 [D loss: 0.662960, acc: 62.50%] [G loss: 1.913058]\n",
      "epoch:28 step:26626 [D loss: 0.576539, acc: 75.78%] [G loss: 1.938389]\n",
      "epoch:28 step:26627 [D loss: 0.671742, acc: 60.94%] [G loss: 1.847213]\n",
      "epoch:28 step:26628 [D loss: 0.633039, acc: 66.41%] [G loss: 1.824609]\n",
      "epoch:28 step:26629 [D loss: 0.645590, acc: 66.41%] [G loss: 1.801489]\n",
      "epoch:28 step:26630 [D loss: 0.654390, acc: 56.25%] [G loss: 1.755199]\n",
      "epoch:28 step:26631 [D loss: 0.641708, acc: 64.06%] [G loss: 1.889033]\n",
      "epoch:28 step:26632 [D loss: 0.669853, acc: 60.94%] [G loss: 1.690889]\n",
      "epoch:28 step:26633 [D loss: 0.641252, acc: 60.94%] [G loss: 1.790478]\n",
      "epoch:28 step:26634 [D loss: 0.676843, acc: 58.59%] [G loss: 2.086373]\n",
      "epoch:28 step:26635 [D loss: 0.687695, acc: 58.59%] [G loss: 1.834515]\n",
      "epoch:28 step:26636 [D loss: 0.667338, acc: 57.81%] [G loss: 1.920228]\n",
      "epoch:28 step:26637 [D loss: 0.613275, acc: 71.09%] [G loss: 1.921955]\n",
      "epoch:28 step:26638 [D loss: 0.661334, acc: 60.16%] [G loss: 1.803413]\n",
      "epoch:28 step:26639 [D loss: 0.659078, acc: 60.16%] [G loss: 1.905356]\n",
      "epoch:28 step:26640 [D loss: 0.638037, acc: 65.62%] [G loss: 2.001255]\n",
      "epoch:28 step:26641 [D loss: 0.653967, acc: 58.59%] [G loss: 1.907671]\n",
      "epoch:28 step:26642 [D loss: 0.551039, acc: 72.66%] [G loss: 2.022004]\n",
      "epoch:28 step:26643 [D loss: 0.639911, acc: 58.59%] [G loss: 1.997663]\n",
      "epoch:28 step:26644 [D loss: 0.683116, acc: 57.81%] [G loss: 1.751853]\n",
      "epoch:28 step:26645 [D loss: 0.617296, acc: 68.75%] [G loss: 1.875801]\n",
      "epoch:28 step:26646 [D loss: 0.709114, acc: 55.47%] [G loss: 1.777252]\n",
      "epoch:28 step:26647 [D loss: 0.639497, acc: 63.28%] [G loss: 1.833491]\n",
      "epoch:28 step:26648 [D loss: 0.634223, acc: 65.62%] [G loss: 1.889210]\n",
      "epoch:28 step:26649 [D loss: 0.654252, acc: 64.84%] [G loss: 1.887367]\n",
      "epoch:28 step:26650 [D loss: 0.629876, acc: 66.41%] [G loss: 1.952828]\n",
      "epoch:28 step:26651 [D loss: 0.593133, acc: 71.88%] [G loss: 1.854368]\n",
      "epoch:28 step:26652 [D loss: 0.661328, acc: 58.59%] [G loss: 2.026660]\n",
      "epoch:28 step:26653 [D loss: 0.643180, acc: 67.97%] [G loss: 1.976308]\n",
      "epoch:28 step:26654 [D loss: 0.646583, acc: 61.72%] [G loss: 1.839917]\n",
      "epoch:28 step:26655 [D loss: 0.661441, acc: 63.28%] [G loss: 1.768567]\n",
      "epoch:28 step:26656 [D loss: 0.617552, acc: 65.62%] [G loss: 1.952866]\n",
      "epoch:28 step:26657 [D loss: 0.687673, acc: 55.47%] [G loss: 1.873461]\n",
      "epoch:28 step:26658 [D loss: 0.690716, acc: 57.81%] [G loss: 1.811146]\n",
      "epoch:28 step:26659 [D loss: 0.698175, acc: 53.91%] [G loss: 1.784607]\n",
      "epoch:28 step:26660 [D loss: 0.617921, acc: 64.84%] [G loss: 1.907935]\n",
      "epoch:28 step:26661 [D loss: 0.656327, acc: 66.41%] [G loss: 1.888569]\n",
      "epoch:28 step:26662 [D loss: 0.630458, acc: 63.28%] [G loss: 1.940526]\n",
      "epoch:28 step:26663 [D loss: 0.606896, acc: 66.41%] [G loss: 2.042225]\n",
      "epoch:28 step:26664 [D loss: 0.674394, acc: 61.72%] [G loss: 2.202362]\n",
      "epoch:28 step:26665 [D loss: 0.664735, acc: 67.97%] [G loss: 2.065915]\n",
      "epoch:28 step:26666 [D loss: 0.594444, acc: 69.53%] [G loss: 2.200422]\n",
      "epoch:28 step:26667 [D loss: 0.661826, acc: 60.94%] [G loss: 1.932309]\n",
      "epoch:28 step:26668 [D loss: 0.654148, acc: 63.28%] [G loss: 1.841807]\n",
      "epoch:28 step:26669 [D loss: 0.648894, acc: 59.38%] [G loss: 1.863115]\n",
      "epoch:28 step:26670 [D loss: 0.614926, acc: 67.97%] [G loss: 1.985858]\n",
      "epoch:28 step:26671 [D loss: 0.671757, acc: 60.94%] [G loss: 1.883500]\n",
      "epoch:28 step:26672 [D loss: 0.623136, acc: 65.62%] [G loss: 1.965066]\n",
      "epoch:28 step:26673 [D loss: 0.749939, acc: 53.91%] [G loss: 1.757704]\n",
      "epoch:28 step:26674 [D loss: 0.658058, acc: 60.94%] [G loss: 1.749307]\n",
      "epoch:28 step:26675 [D loss: 0.643089, acc: 68.75%] [G loss: 1.696523]\n",
      "epoch:28 step:26676 [D loss: 0.703576, acc: 53.12%] [G loss: 1.747081]\n",
      "epoch:28 step:26677 [D loss: 0.662344, acc: 57.03%] [G loss: 1.861386]\n",
      "epoch:28 step:26678 [D loss: 0.719446, acc: 55.47%] [G loss: 1.763578]\n",
      "epoch:28 step:26679 [D loss: 0.635620, acc: 65.62%] [G loss: 1.711805]\n",
      "epoch:28 step:26680 [D loss: 0.640148, acc: 59.38%] [G loss: 1.741694]\n",
      "epoch:28 step:26681 [D loss: 0.679750, acc: 60.94%] [G loss: 1.773978]\n",
      "epoch:28 step:26682 [D loss: 0.695873, acc: 54.69%] [G loss: 1.793073]\n",
      "epoch:28 step:26683 [D loss: 0.661219, acc: 58.59%] [G loss: 1.769901]\n",
      "epoch:28 step:26684 [D loss: 0.696989, acc: 53.91%] [G loss: 1.860138]\n",
      "epoch:28 step:26685 [D loss: 0.696189, acc: 56.25%] [G loss: 1.824379]\n",
      "epoch:28 step:26686 [D loss: 0.651469, acc: 61.72%] [G loss: 1.933134]\n",
      "epoch:28 step:26687 [D loss: 0.627748, acc: 64.06%] [G loss: 1.909219]\n",
      "epoch:28 step:26688 [D loss: 0.592031, acc: 67.19%] [G loss: 1.840649]\n",
      "epoch:28 step:26689 [D loss: 0.615638, acc: 67.19%] [G loss: 2.029705]\n",
      "epoch:28 step:26690 [D loss: 0.648059, acc: 57.81%] [G loss: 1.924112]\n",
      "epoch:28 step:26691 [D loss: 0.667019, acc: 60.94%] [G loss: 1.797035]\n",
      "epoch:28 step:26692 [D loss: 0.655582, acc: 66.41%] [G loss: 1.992641]\n",
      "epoch:28 step:26693 [D loss: 0.594234, acc: 70.31%] [G loss: 1.806053]\n",
      "epoch:28 step:26694 [D loss: 0.657989, acc: 54.69%] [G loss: 1.779537]\n",
      "epoch:28 step:26695 [D loss: 0.664468, acc: 60.94%] [G loss: 1.811092]\n",
      "epoch:28 step:26696 [D loss: 0.699179, acc: 57.03%] [G loss: 1.860863]\n",
      "epoch:28 step:26697 [D loss: 0.655973, acc: 60.94%] [G loss: 1.864067]\n",
      "epoch:28 step:26698 [D loss: 0.680313, acc: 57.81%] [G loss: 1.931737]\n",
      "epoch:28 step:26699 [D loss: 0.616642, acc: 65.62%] [G loss: 1.703112]\n",
      "epoch:28 step:26700 [D loss: 0.684059, acc: 56.25%] [G loss: 1.705421]\n",
      "epoch:28 step:26701 [D loss: 0.664562, acc: 58.59%] [G loss: 1.847854]\n",
      "epoch:28 step:26702 [D loss: 0.661045, acc: 60.16%] [G loss: 1.830553]\n",
      "epoch:28 step:26703 [D loss: 0.607938, acc: 67.19%] [G loss: 1.860638]\n",
      "epoch:28 step:26704 [D loss: 0.644886, acc: 63.28%] [G loss: 1.903209]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:28 step:26705 [D loss: 0.594883, acc: 72.66%] [G loss: 2.007412]\n",
      "epoch:28 step:26706 [D loss: 0.605709, acc: 66.41%] [G loss: 2.117498]\n",
      "epoch:28 step:26707 [D loss: 0.613162, acc: 65.62%] [G loss: 2.204724]\n",
      "epoch:28 step:26708 [D loss: 0.567479, acc: 68.75%] [G loss: 2.205384]\n",
      "epoch:28 step:26709 [D loss: 0.736788, acc: 54.69%] [G loss: 1.656600]\n",
      "epoch:28 step:26710 [D loss: 0.671974, acc: 58.59%] [G loss: 1.927807]\n",
      "epoch:28 step:26711 [D loss: 0.667480, acc: 57.81%] [G loss: 1.869639]\n",
      "epoch:28 step:26712 [D loss: 0.698372, acc: 56.25%] [G loss: 1.775172]\n",
      "epoch:28 step:26713 [D loss: 0.669637, acc: 57.81%] [G loss: 1.762946]\n",
      "epoch:28 step:26714 [D loss: 0.641808, acc: 62.50%] [G loss: 1.832305]\n",
      "epoch:28 step:26715 [D loss: 0.660254, acc: 61.72%] [G loss: 1.972836]\n",
      "epoch:28 step:26716 [D loss: 0.614555, acc: 66.41%] [G loss: 2.038923]\n",
      "epoch:28 step:26717 [D loss: 0.639243, acc: 65.62%] [G loss: 2.096837]\n",
      "epoch:28 step:26718 [D loss: 0.673189, acc: 60.16%] [G loss: 1.717162]\n",
      "epoch:28 step:26719 [D loss: 0.658016, acc: 63.28%] [G loss: 1.832080]\n",
      "epoch:28 step:26720 [D loss: 0.658702, acc: 59.38%] [G loss: 1.818362]\n",
      "epoch:28 step:26721 [D loss: 0.683285, acc: 59.38%] [G loss: 1.793527]\n",
      "epoch:28 step:26722 [D loss: 0.666885, acc: 64.06%] [G loss: 1.799551]\n",
      "epoch:28 step:26723 [D loss: 0.638007, acc: 57.81%] [G loss: 1.857588]\n",
      "epoch:28 step:26724 [D loss: 0.581518, acc: 70.31%] [G loss: 1.967693]\n",
      "epoch:28 step:26725 [D loss: 0.655348, acc: 62.50%] [G loss: 1.776727]\n",
      "epoch:28 step:26726 [D loss: 0.624309, acc: 67.19%] [G loss: 1.893241]\n",
      "epoch:28 step:26727 [D loss: 0.645495, acc: 67.97%] [G loss: 1.799349]\n",
      "epoch:28 step:26728 [D loss: 0.706832, acc: 51.56%] [G loss: 1.913878]\n",
      "epoch:28 step:26729 [D loss: 0.621894, acc: 60.16%] [G loss: 1.810065]\n",
      "epoch:28 step:26730 [D loss: 0.584994, acc: 72.66%] [G loss: 1.932023]\n",
      "epoch:28 step:26731 [D loss: 0.657042, acc: 60.94%] [G loss: 1.983592]\n",
      "epoch:28 step:26732 [D loss: 0.647072, acc: 63.28%] [G loss: 2.040952]\n",
      "epoch:28 step:26733 [D loss: 0.614324, acc: 64.06%] [G loss: 1.962424]\n",
      "epoch:28 step:26734 [D loss: 0.601774, acc: 64.84%] [G loss: 2.133518]\n",
      "epoch:28 step:26735 [D loss: 0.547868, acc: 76.56%] [G loss: 2.101045]\n",
      "epoch:28 step:26736 [D loss: 0.725911, acc: 60.16%] [G loss: 1.837450]\n",
      "epoch:28 step:26737 [D loss: 0.741547, acc: 57.81%] [G loss: 1.815746]\n",
      "epoch:28 step:26738 [D loss: 0.690097, acc: 53.12%] [G loss: 1.753808]\n",
      "epoch:28 step:26739 [D loss: 0.657839, acc: 61.72%] [G loss: 1.753969]\n",
      "epoch:28 step:26740 [D loss: 0.672267, acc: 54.69%] [G loss: 1.946818]\n",
      "epoch:28 step:26741 [D loss: 0.632380, acc: 64.84%] [G loss: 1.891849]\n",
      "epoch:28 step:26742 [D loss: 0.666778, acc: 60.16%] [G loss: 1.753323]\n",
      "epoch:28 step:26743 [D loss: 0.659347, acc: 60.16%] [G loss: 1.840430]\n",
      "epoch:28 step:26744 [D loss: 0.730619, acc: 57.03%] [G loss: 2.005252]\n",
      "epoch:28 step:26745 [D loss: 0.676029, acc: 57.81%] [G loss: 1.933379]\n",
      "epoch:28 step:26746 [D loss: 0.687613, acc: 59.38%] [G loss: 1.778911]\n",
      "epoch:28 step:26747 [D loss: 0.723752, acc: 56.25%] [G loss: 1.690590]\n",
      "epoch:28 step:26748 [D loss: 0.637431, acc: 63.28%] [G loss: 1.863078]\n",
      "epoch:28 step:26749 [D loss: 0.656878, acc: 63.28%] [G loss: 1.840296]\n",
      "epoch:28 step:26750 [D loss: 0.625582, acc: 67.19%] [G loss: 1.820923]\n",
      "epoch:28 step:26751 [D loss: 0.623671, acc: 62.50%] [G loss: 1.996687]\n",
      "epoch:28 step:26752 [D loss: 0.606136, acc: 64.84%] [G loss: 2.057745]\n",
      "epoch:28 step:26753 [D loss: 0.635913, acc: 64.06%] [G loss: 1.800630]\n",
      "epoch:28 step:26754 [D loss: 0.659581, acc: 65.62%] [G loss: 1.715958]\n",
      "epoch:28 step:26755 [D loss: 0.596723, acc: 70.31%] [G loss: 1.860567]\n",
      "epoch:28 step:26756 [D loss: 0.639172, acc: 57.81%] [G loss: 1.825392]\n",
      "epoch:28 step:26757 [D loss: 0.616839, acc: 68.75%] [G loss: 1.836013]\n",
      "epoch:28 step:26758 [D loss: 0.668515, acc: 59.38%] [G loss: 1.942845]\n",
      "epoch:28 step:26759 [D loss: 0.607606, acc: 71.09%] [G loss: 2.163995]\n",
      "epoch:28 step:26760 [D loss: 0.695728, acc: 57.03%] [G loss: 1.879298]\n",
      "epoch:28 step:26761 [D loss: 0.666631, acc: 60.16%] [G loss: 1.762685]\n",
      "epoch:28 step:26762 [D loss: 0.683494, acc: 57.81%] [G loss: 1.801153]\n",
      "epoch:28 step:26763 [D loss: 0.656902, acc: 62.50%] [G loss: 1.796995]\n",
      "epoch:28 step:26764 [D loss: 0.698891, acc: 58.59%] [G loss: 1.758866]\n",
      "epoch:28 step:26765 [D loss: 0.750836, acc: 52.34%] [G loss: 1.719085]\n",
      "epoch:28 step:26766 [D loss: 0.640945, acc: 64.84%] [G loss: 1.730913]\n",
      "epoch:28 step:26767 [D loss: 0.669712, acc: 51.56%] [G loss: 1.807362]\n",
      "epoch:28 step:26768 [D loss: 0.564450, acc: 73.44%] [G loss: 1.837009]\n",
      "epoch:28 step:26769 [D loss: 0.644664, acc: 64.06%] [G loss: 1.824166]\n",
      "epoch:28 step:26770 [D loss: 0.632870, acc: 60.94%] [G loss: 1.980240]\n",
      "epoch:28 step:26771 [D loss: 0.662740, acc: 56.25%] [G loss: 1.844492]\n",
      "epoch:28 step:26772 [D loss: 0.595795, acc: 67.97%] [G loss: 1.886678]\n",
      "epoch:28 step:26773 [D loss: 0.634514, acc: 59.38%] [G loss: 1.881084]\n",
      "epoch:28 step:26774 [D loss: 0.707292, acc: 54.69%] [G loss: 1.811587]\n",
      "epoch:28 step:26775 [D loss: 0.579124, acc: 78.12%] [G loss: 1.950487]\n",
      "epoch:28 step:26776 [D loss: 0.611984, acc: 66.41%] [G loss: 1.901984]\n",
      "epoch:28 step:26777 [D loss: 0.703224, acc: 54.69%] [G loss: 1.789310]\n",
      "epoch:28 step:26778 [D loss: 0.701069, acc: 52.34%] [G loss: 1.923117]\n",
      "epoch:28 step:26779 [D loss: 0.646166, acc: 61.72%] [G loss: 2.020880]\n",
      "epoch:28 step:26780 [D loss: 0.626460, acc: 62.50%] [G loss: 1.845274]\n",
      "epoch:28 step:26781 [D loss: 0.629218, acc: 60.94%] [G loss: 1.844293]\n",
      "epoch:28 step:26782 [D loss: 0.629911, acc: 63.28%] [G loss: 1.896003]\n",
      "epoch:28 step:26783 [D loss: 0.655196, acc: 60.94%] [G loss: 1.894974]\n",
      "epoch:28 step:26784 [D loss: 0.682465, acc: 56.25%] [G loss: 1.778965]\n",
      "epoch:28 step:26785 [D loss: 0.620828, acc: 69.53%] [G loss: 2.060040]\n",
      "epoch:28 step:26786 [D loss: 0.605871, acc: 69.53%] [G loss: 2.102180]\n",
      "epoch:28 step:26787 [D loss: 0.623717, acc: 67.19%] [G loss: 2.049419]\n",
      "epoch:28 step:26788 [D loss: 0.592835, acc: 62.50%] [G loss: 1.984026]\n",
      "epoch:28 step:26789 [D loss: 0.676669, acc: 60.94%] [G loss: 1.940076]\n",
      "epoch:28 step:26790 [D loss: 0.642918, acc: 63.28%] [G loss: 2.053519]\n",
      "epoch:28 step:26791 [D loss: 0.657593, acc: 64.84%] [G loss: 2.032615]\n",
      "epoch:28 step:26792 [D loss: 0.636649, acc: 60.94%] [G loss: 2.089074]\n",
      "epoch:28 step:26793 [D loss: 0.654620, acc: 62.50%] [G loss: 1.905615]\n",
      "epoch:28 step:26794 [D loss: 0.630587, acc: 61.72%] [G loss: 1.954096]\n",
      "epoch:28 step:26795 [D loss: 0.657751, acc: 60.94%] [G loss: 1.754338]\n",
      "epoch:28 step:26796 [D loss: 0.679078, acc: 59.38%] [G loss: 1.859819]\n",
      "epoch:28 step:26797 [D loss: 0.632577, acc: 61.72%] [G loss: 1.869274]\n",
      "epoch:28 step:26798 [D loss: 0.635571, acc: 64.06%] [G loss: 1.751911]\n",
      "epoch:28 step:26799 [D loss: 0.634420, acc: 64.84%] [G loss: 2.002638]\n",
      "epoch:28 step:26800 [D loss: 0.588054, acc: 72.66%] [G loss: 2.091548]\n",
      "##############\n",
      "[2.42039782 1.6358401  6.22709029 4.60100951 3.46138809 5.5651236\n",
      " 4.39689132 4.59374955 4.47542328 3.81949434]\n",
      "##########\n",
      "epoch:28 step:26801 [D loss: 0.687413, acc: 55.47%] [G loss: 1.806079]\n",
      "epoch:28 step:26802 [D loss: 0.686752, acc: 60.94%] [G loss: 1.845128]\n",
      "epoch:28 step:26803 [D loss: 0.628376, acc: 64.84%] [G loss: 1.828504]\n",
      "epoch:28 step:26804 [D loss: 0.662699, acc: 57.81%] [G loss: 1.753544]\n",
      "epoch:28 step:26805 [D loss: 0.661263, acc: 56.25%] [G loss: 1.803978]\n",
      "epoch:28 step:26806 [D loss: 0.656894, acc: 61.72%] [G loss: 1.733579]\n",
      "epoch:28 step:26807 [D loss: 0.653762, acc: 60.16%] [G loss: 1.869045]\n",
      "epoch:28 step:26808 [D loss: 0.666218, acc: 59.38%] [G loss: 1.837348]\n",
      "epoch:28 step:26809 [D loss: 0.643562, acc: 64.06%] [G loss: 1.744043]\n",
      "epoch:28 step:26810 [D loss: 0.578788, acc: 71.88%] [G loss: 1.796852]\n",
      "epoch:28 step:26811 [D loss: 0.656773, acc: 60.94%] [G loss: 1.862584]\n",
      "epoch:28 step:26812 [D loss: 0.655934, acc: 58.59%] [G loss: 1.725569]\n",
      "epoch:28 step:26813 [D loss: 0.670921, acc: 57.81%] [G loss: 1.873797]\n",
      "epoch:28 step:26814 [D loss: 0.654219, acc: 56.25%] [G loss: 1.882846]\n",
      "epoch:28 step:26815 [D loss: 0.681462, acc: 53.91%] [G loss: 1.754153]\n",
      "epoch:28 step:26816 [D loss: 0.657356, acc: 60.16%] [G loss: 1.625304]\n",
      "epoch:28 step:26817 [D loss: 0.674610, acc: 57.81%] [G loss: 1.914446]\n",
      "epoch:28 step:26818 [D loss: 0.622137, acc: 66.41%] [G loss: 1.936515]\n",
      "epoch:28 step:26819 [D loss: 0.705755, acc: 62.50%] [G loss: 1.804815]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:28 step:26820 [D loss: 0.655100, acc: 61.72%] [G loss: 1.874379]\n",
      "epoch:28 step:26821 [D loss: 0.655428, acc: 63.28%] [G loss: 1.878861]\n",
      "epoch:28 step:26822 [D loss: 0.668567, acc: 61.72%] [G loss: 1.791172]\n",
      "epoch:28 step:26823 [D loss: 0.641366, acc: 55.47%] [G loss: 2.027811]\n",
      "epoch:28 step:26824 [D loss: 0.614488, acc: 65.62%] [G loss: 2.002741]\n",
      "epoch:28 step:26825 [D loss: 0.691095, acc: 60.16%] [G loss: 1.972716]\n",
      "epoch:28 step:26826 [D loss: 0.658481, acc: 60.16%] [G loss: 1.886743]\n",
      "epoch:28 step:26827 [D loss: 0.598420, acc: 66.41%] [G loss: 1.887664]\n",
      "epoch:28 step:26828 [D loss: 0.649410, acc: 63.28%] [G loss: 1.841603]\n",
      "epoch:28 step:26829 [D loss: 0.639514, acc: 64.84%] [G loss: 1.948054]\n",
      "epoch:28 step:26830 [D loss: 0.636646, acc: 68.75%] [G loss: 1.773548]\n",
      "epoch:28 step:26831 [D loss: 0.716772, acc: 55.47%] [G loss: 1.876477]\n",
      "epoch:28 step:26832 [D loss: 0.635079, acc: 63.28%] [G loss: 1.824434]\n",
      "epoch:28 step:26833 [D loss: 0.618449, acc: 65.62%] [G loss: 1.879346]\n",
      "epoch:28 step:26834 [D loss: 0.639478, acc: 64.06%] [G loss: 1.878568]\n",
      "epoch:28 step:26835 [D loss: 0.643538, acc: 61.72%] [G loss: 2.014886]\n",
      "epoch:28 step:26836 [D loss: 0.653555, acc: 62.50%] [G loss: 1.871327]\n",
      "epoch:28 step:26837 [D loss: 0.623105, acc: 63.28%] [G loss: 1.870352]\n",
      "epoch:28 step:26838 [D loss: 0.608624, acc: 64.06%] [G loss: 1.827017]\n",
      "epoch:28 step:26839 [D loss: 0.613435, acc: 68.75%] [G loss: 1.896430]\n",
      "epoch:28 step:26840 [D loss: 0.631490, acc: 65.62%] [G loss: 1.876040]\n",
      "epoch:28 step:26841 [D loss: 0.617735, acc: 64.84%] [G loss: 1.849586]\n",
      "epoch:28 step:26842 [D loss: 0.679048, acc: 57.81%] [G loss: 1.846447]\n",
      "epoch:28 step:26843 [D loss: 0.676256, acc: 57.81%] [G loss: 2.038596]\n",
      "epoch:28 step:26844 [D loss: 0.660489, acc: 55.47%] [G loss: 1.958745]\n",
      "epoch:28 step:26845 [D loss: 0.655091, acc: 59.38%] [G loss: 1.886801]\n",
      "epoch:28 step:26846 [D loss: 0.705963, acc: 53.91%] [G loss: 1.842469]\n",
      "epoch:28 step:26847 [D loss: 0.669079, acc: 60.16%] [G loss: 1.752735]\n",
      "epoch:28 step:26848 [D loss: 0.687532, acc: 52.34%] [G loss: 1.754299]\n",
      "epoch:28 step:26849 [D loss: 0.681189, acc: 60.94%] [G loss: 1.846661]\n",
      "epoch:28 step:26850 [D loss: 0.710721, acc: 55.47%] [G loss: 1.647711]\n",
      "epoch:28 step:26851 [D loss: 0.701693, acc: 54.69%] [G loss: 1.761401]\n",
      "epoch:28 step:26852 [D loss: 0.687619, acc: 55.47%] [G loss: 1.933302]\n",
      "epoch:28 step:26853 [D loss: 0.662080, acc: 64.06%] [G loss: 1.989233]\n",
      "epoch:28 step:26854 [D loss: 0.598965, acc: 70.31%] [G loss: 1.782187]\n",
      "epoch:28 step:26855 [D loss: 0.625677, acc: 60.16%] [G loss: 1.720501]\n",
      "epoch:28 step:26856 [D loss: 0.619131, acc: 69.53%] [G loss: 1.871664]\n",
      "epoch:28 step:26857 [D loss: 0.660281, acc: 61.72%] [G loss: 1.906243]\n",
      "epoch:28 step:26858 [D loss: 0.605664, acc: 66.41%] [G loss: 1.959031]\n",
      "epoch:28 step:26859 [D loss: 0.644932, acc: 66.41%] [G loss: 1.978768]\n",
      "epoch:28 step:26860 [D loss: 0.604285, acc: 69.53%] [G loss: 2.089019]\n",
      "epoch:28 step:26861 [D loss: 0.671536, acc: 56.25%] [G loss: 1.755348]\n",
      "epoch:28 step:26862 [D loss: 0.653856, acc: 65.62%] [G loss: 1.851898]\n",
      "epoch:28 step:26863 [D loss: 0.645453, acc: 62.50%] [G loss: 1.940826]\n",
      "epoch:28 step:26864 [D loss: 0.667378, acc: 61.72%] [G loss: 1.879612]\n",
      "epoch:28 step:26865 [D loss: 0.605454, acc: 67.19%] [G loss: 1.962649]\n",
      "epoch:28 step:26866 [D loss: 0.665093, acc: 56.25%] [G loss: 2.032202]\n",
      "epoch:28 step:26867 [D loss: 0.639028, acc: 63.28%] [G loss: 1.913938]\n",
      "epoch:28 step:26868 [D loss: 0.578844, acc: 68.75%] [G loss: 1.956436]\n",
      "epoch:28 step:26869 [D loss: 0.638333, acc: 63.28%] [G loss: 1.884507]\n",
      "epoch:28 step:26870 [D loss: 0.645459, acc: 64.06%] [G loss: 1.912280]\n",
      "epoch:28 step:26871 [D loss: 0.646091, acc: 58.59%] [G loss: 2.011554]\n",
      "epoch:28 step:26872 [D loss: 0.643132, acc: 64.84%] [G loss: 1.853044]\n",
      "epoch:28 step:26873 [D loss: 0.648059, acc: 63.28%] [G loss: 2.075247]\n",
      "epoch:28 step:26874 [D loss: 0.565721, acc: 70.31%] [G loss: 2.177863]\n",
      "epoch:28 step:26875 [D loss: 0.716359, acc: 53.12%] [G loss: 1.962282]\n",
      "epoch:28 step:26876 [D loss: 0.641694, acc: 66.41%] [G loss: 1.900561]\n",
      "epoch:28 step:26877 [D loss: 0.642691, acc: 65.62%] [G loss: 1.990230]\n",
      "epoch:28 step:26878 [D loss: 0.633080, acc: 61.72%] [G loss: 1.920130]\n",
      "epoch:28 step:26879 [D loss: 0.662664, acc: 61.72%] [G loss: 1.837917]\n",
      "epoch:28 step:26880 [D loss: 0.656958, acc: 60.16%] [G loss: 2.043725]\n",
      "epoch:28 step:26881 [D loss: 0.669083, acc: 60.16%] [G loss: 2.006757]\n",
      "epoch:28 step:26882 [D loss: 0.630679, acc: 64.84%] [G loss: 1.918341]\n",
      "epoch:28 step:26883 [D loss: 0.551958, acc: 73.44%] [G loss: 2.077580]\n",
      "epoch:28 step:26884 [D loss: 0.566467, acc: 69.53%] [G loss: 2.350442]\n",
      "epoch:28 step:26885 [D loss: 0.578463, acc: 73.44%] [G loss: 2.096595]\n",
      "epoch:28 step:26886 [D loss: 0.621389, acc: 67.97%] [G loss: 2.051522]\n",
      "epoch:28 step:26887 [D loss: 0.620250, acc: 65.62%] [G loss: 1.965378]\n",
      "epoch:28 step:26888 [D loss: 0.700261, acc: 58.59%] [G loss: 1.931603]\n",
      "epoch:28 step:26889 [D loss: 0.706166, acc: 60.16%] [G loss: 1.959837]\n",
      "epoch:28 step:26890 [D loss: 0.652984, acc: 61.72%] [G loss: 1.882826]\n",
      "epoch:28 step:26891 [D loss: 0.660967, acc: 57.03%] [G loss: 1.856212]\n",
      "epoch:28 step:26892 [D loss: 0.642703, acc: 60.16%] [G loss: 1.759774]\n",
      "epoch:28 step:26893 [D loss: 0.744651, acc: 47.66%] [G loss: 1.840857]\n",
      "epoch:28 step:26894 [D loss: 0.652550, acc: 60.16%] [G loss: 1.912487]\n",
      "epoch:28 step:26895 [D loss: 0.616534, acc: 66.41%] [G loss: 1.850405]\n",
      "epoch:28 step:26896 [D loss: 0.629735, acc: 61.72%] [G loss: 1.814973]\n",
      "epoch:28 step:26897 [D loss: 0.632458, acc: 66.41%] [G loss: 1.924788]\n",
      "epoch:28 step:26898 [D loss: 0.645139, acc: 63.28%] [G loss: 2.005554]\n",
      "epoch:28 step:26899 [D loss: 0.613484, acc: 67.97%] [G loss: 1.957489]\n",
      "epoch:28 step:26900 [D loss: 0.642020, acc: 63.28%] [G loss: 1.832557]\n",
      "epoch:28 step:26901 [D loss: 0.627774, acc: 67.97%] [G loss: 1.754445]\n",
      "epoch:28 step:26902 [D loss: 0.677062, acc: 59.38%] [G loss: 1.881353]\n",
      "epoch:28 step:26903 [D loss: 0.642262, acc: 61.72%] [G loss: 1.778923]\n",
      "epoch:28 step:26904 [D loss: 0.636285, acc: 63.28%] [G loss: 1.941375]\n",
      "epoch:28 step:26905 [D loss: 0.653379, acc: 61.72%] [G loss: 1.899619]\n",
      "epoch:28 step:26906 [D loss: 0.649083, acc: 61.72%] [G loss: 1.702907]\n",
      "epoch:28 step:26907 [D loss: 0.678015, acc: 53.12%] [G loss: 1.835411]\n",
      "epoch:28 step:26908 [D loss: 0.708741, acc: 51.56%] [G loss: 1.834619]\n",
      "epoch:28 step:26909 [D loss: 0.658323, acc: 62.50%] [G loss: 1.865189]\n",
      "epoch:28 step:26910 [D loss: 0.648259, acc: 63.28%] [G loss: 1.833935]\n",
      "epoch:28 step:26911 [D loss: 0.655054, acc: 64.06%] [G loss: 1.774103]\n",
      "epoch:28 step:26912 [D loss: 0.647470, acc: 63.28%] [G loss: 1.722809]\n",
      "epoch:28 step:26913 [D loss: 0.649501, acc: 63.28%] [G loss: 1.807836]\n",
      "epoch:28 step:26914 [D loss: 0.652518, acc: 59.38%] [G loss: 1.884495]\n",
      "epoch:28 step:26915 [D loss: 0.623436, acc: 70.31%] [G loss: 1.797410]\n",
      "epoch:28 step:26916 [D loss: 0.642536, acc: 58.59%] [G loss: 1.795312]\n",
      "epoch:28 step:26917 [D loss: 0.600328, acc: 67.97%] [G loss: 1.985588]\n",
      "epoch:28 step:26918 [D loss: 0.659930, acc: 61.72%] [G loss: 1.810856]\n",
      "epoch:28 step:26919 [D loss: 0.660554, acc: 68.75%] [G loss: 1.750548]\n",
      "epoch:28 step:26920 [D loss: 0.639781, acc: 61.72%] [G loss: 1.828529]\n",
      "epoch:28 step:26921 [D loss: 0.629983, acc: 61.72%] [G loss: 1.883865]\n",
      "epoch:28 step:26922 [D loss: 0.639710, acc: 68.75%] [G loss: 1.881680]\n",
      "epoch:28 step:26923 [D loss: 0.671492, acc: 61.72%] [G loss: 1.921836]\n",
      "epoch:28 step:26924 [D loss: 0.684999, acc: 59.38%] [G loss: 1.976927]\n",
      "epoch:28 step:26925 [D loss: 0.640020, acc: 60.16%] [G loss: 1.934654]\n",
      "epoch:28 step:26926 [D loss: 0.646878, acc: 68.75%] [G loss: 2.020072]\n",
      "epoch:28 step:26927 [D loss: 0.644449, acc: 61.72%] [G loss: 2.009825]\n",
      "epoch:28 step:26928 [D loss: 0.659872, acc: 58.59%] [G loss: 1.964624]\n",
      "epoch:28 step:26929 [D loss: 0.566526, acc: 71.09%] [G loss: 1.937225]\n",
      "epoch:28 step:26930 [D loss: 0.590981, acc: 66.41%] [G loss: 2.004218]\n",
      "epoch:28 step:26931 [D loss: 0.607399, acc: 64.84%] [G loss: 2.122326]\n",
      "epoch:28 step:26932 [D loss: 0.693935, acc: 55.47%] [G loss: 1.831654]\n",
      "epoch:28 step:26933 [D loss: 0.704822, acc: 60.16%] [G loss: 1.866254]\n",
      "epoch:28 step:26934 [D loss: 0.668383, acc: 61.72%] [G loss: 1.819589]\n",
      "epoch:28 step:26935 [D loss: 0.602728, acc: 67.19%] [G loss: 1.932531]\n",
      "epoch:28 step:26936 [D loss: 0.651429, acc: 60.16%] [G loss: 1.999111]\n",
      "epoch:28 step:26937 [D loss: 0.662353, acc: 57.81%] [G loss: 1.965584]\n",
      "epoch:28 step:26938 [D loss: 0.715954, acc: 53.12%] [G loss: 1.839827]\n",
      "epoch:28 step:26939 [D loss: 0.665192, acc: 64.06%] [G loss: 1.727586]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:28 step:26940 [D loss: 0.647699, acc: 60.16%] [G loss: 1.752213]\n",
      "epoch:28 step:26941 [D loss: 0.650068, acc: 58.59%] [G loss: 1.839906]\n",
      "epoch:28 step:26942 [D loss: 0.646362, acc: 63.28%] [G loss: 1.970861]\n",
      "epoch:28 step:26943 [D loss: 0.639793, acc: 58.59%] [G loss: 2.024910]\n",
      "epoch:28 step:26944 [D loss: 0.580624, acc: 70.31%] [G loss: 2.212985]\n",
      "epoch:28 step:26945 [D loss: 0.636500, acc: 64.84%] [G loss: 1.972686]\n",
      "epoch:28 step:26946 [D loss: 0.708298, acc: 57.03%] [G loss: 1.926391]\n",
      "epoch:28 step:26947 [D loss: 0.643130, acc: 63.28%] [G loss: 1.979074]\n",
      "epoch:28 step:26948 [D loss: 0.625408, acc: 65.62%] [G loss: 1.791044]\n",
      "epoch:28 step:26949 [D loss: 0.620891, acc: 65.62%] [G loss: 1.910688]\n",
      "epoch:28 step:26950 [D loss: 0.634974, acc: 67.97%] [G loss: 1.947634]\n",
      "epoch:28 step:26951 [D loss: 0.710083, acc: 53.12%] [G loss: 1.921945]\n",
      "epoch:28 step:26952 [D loss: 0.658718, acc: 60.16%] [G loss: 1.747626]\n",
      "epoch:28 step:26953 [D loss: 0.667066, acc: 57.03%] [G loss: 1.785279]\n",
      "epoch:28 step:26954 [D loss: 0.694882, acc: 59.38%] [G loss: 1.878716]\n",
      "epoch:28 step:26955 [D loss: 0.623806, acc: 60.16%] [G loss: 2.055849]\n",
      "epoch:28 step:26956 [D loss: 0.590524, acc: 70.31%] [G loss: 1.920294]\n",
      "epoch:28 step:26957 [D loss: 0.567434, acc: 71.09%] [G loss: 1.950654]\n",
      "epoch:28 step:26958 [D loss: 0.646447, acc: 64.06%] [G loss: 1.964986]\n",
      "epoch:28 step:26959 [D loss: 0.654979, acc: 58.59%] [G loss: 1.868565]\n",
      "epoch:28 step:26960 [D loss: 0.597996, acc: 63.28%] [G loss: 1.886605]\n",
      "epoch:28 step:26961 [D loss: 0.629031, acc: 64.06%] [G loss: 1.983641]\n",
      "epoch:28 step:26962 [D loss: 0.640001, acc: 65.62%] [G loss: 1.997692]\n",
      "epoch:28 step:26963 [D loss: 0.662926, acc: 58.59%] [G loss: 1.977435]\n",
      "epoch:28 step:26964 [D loss: 0.641052, acc: 64.06%] [G loss: 1.906386]\n",
      "epoch:28 step:26965 [D loss: 0.679337, acc: 57.81%] [G loss: 1.871839]\n",
      "epoch:28 step:26966 [D loss: 0.630633, acc: 62.50%] [G loss: 1.920137]\n",
      "epoch:28 step:26967 [D loss: 0.635153, acc: 57.81%] [G loss: 1.777219]\n",
      "epoch:28 step:26968 [D loss: 0.661186, acc: 62.50%] [G loss: 1.918451]\n",
      "epoch:28 step:26969 [D loss: 0.588938, acc: 69.53%] [G loss: 1.967623]\n",
      "epoch:28 step:26970 [D loss: 0.650082, acc: 64.06%] [G loss: 1.848527]\n",
      "epoch:28 step:26971 [D loss: 0.655524, acc: 60.94%] [G loss: 1.827646]\n",
      "epoch:28 step:26972 [D loss: 0.620471, acc: 63.28%] [G loss: 1.946630]\n",
      "epoch:28 step:26973 [D loss: 0.664205, acc: 60.94%] [G loss: 1.879809]\n",
      "epoch:28 step:26974 [D loss: 0.621907, acc: 63.28%] [G loss: 1.914923]\n",
      "epoch:28 step:26975 [D loss: 0.652265, acc: 64.06%] [G loss: 1.809545]\n",
      "epoch:28 step:26976 [D loss: 0.624282, acc: 65.62%] [G loss: 1.954879]\n",
      "epoch:28 step:26977 [D loss: 0.706424, acc: 56.25%] [G loss: 1.722680]\n",
      "epoch:28 step:26978 [D loss: 0.703354, acc: 55.47%] [G loss: 1.840013]\n",
      "epoch:28 step:26979 [D loss: 0.626683, acc: 62.50%] [G loss: 1.766521]\n",
      "epoch:28 step:26980 [D loss: 0.675704, acc: 55.47%] [G loss: 1.834957]\n",
      "epoch:28 step:26981 [D loss: 0.651209, acc: 63.28%] [G loss: 1.936658]\n",
      "epoch:28 step:26982 [D loss: 0.582256, acc: 71.88%] [G loss: 1.871735]\n",
      "epoch:28 step:26983 [D loss: 0.589438, acc: 75.78%] [G loss: 2.028294]\n",
      "epoch:28 step:26984 [D loss: 0.659716, acc: 64.06%] [G loss: 1.740498]\n",
      "epoch:28 step:26985 [D loss: 0.634987, acc: 58.59%] [G loss: 1.905049]\n",
      "epoch:28 step:26986 [D loss: 0.664370, acc: 61.72%] [G loss: 1.949224]\n",
      "epoch:28 step:26987 [D loss: 0.651802, acc: 62.50%] [G loss: 1.885826]\n",
      "epoch:28 step:26988 [D loss: 0.715259, acc: 56.25%] [G loss: 1.796624]\n",
      "epoch:28 step:26989 [D loss: 0.683454, acc: 57.81%] [G loss: 1.859107]\n",
      "epoch:28 step:26990 [D loss: 0.639581, acc: 67.19%] [G loss: 1.937051]\n",
      "epoch:28 step:26991 [D loss: 0.624833, acc: 72.66%] [G loss: 1.838084]\n",
      "epoch:28 step:26992 [D loss: 0.630077, acc: 65.62%] [G loss: 1.726758]\n",
      "epoch:28 step:26993 [D loss: 0.628961, acc: 62.50%] [G loss: 1.832654]\n",
      "epoch:28 step:26994 [D loss: 0.639757, acc: 63.28%] [G loss: 1.878301]\n",
      "epoch:28 step:26995 [D loss: 0.661107, acc: 64.06%] [G loss: 1.678257]\n",
      "epoch:28 step:26996 [D loss: 0.629471, acc: 60.94%] [G loss: 1.848796]\n",
      "epoch:28 step:26997 [D loss: 0.640362, acc: 63.28%] [G loss: 1.839043]\n",
      "epoch:28 step:26998 [D loss: 0.676306, acc: 59.38%] [G loss: 1.918189]\n",
      "epoch:28 step:26999 [D loss: 0.647458, acc: 64.06%] [G loss: 1.749771]\n",
      "epoch:28 step:27000 [D loss: 0.664472, acc: 57.03%] [G loss: 1.798016]\n",
      "##############\n",
      "[2.57288788 1.70797289 6.1692034  4.76964157 3.52704251 5.50277908\n",
      " 4.31710856 4.57666463 4.41535083 3.90626986]\n",
      "##########\n",
      "epoch:28 step:27001 [D loss: 0.708185, acc: 53.12%] [G loss: 1.692012]\n",
      "epoch:28 step:27002 [D loss: 0.654282, acc: 58.59%] [G loss: 1.795679]\n",
      "epoch:28 step:27003 [D loss: 0.661237, acc: 60.94%] [G loss: 1.998603]\n",
      "epoch:28 step:27004 [D loss: 0.622752, acc: 65.62%] [G loss: 1.775701]\n",
      "epoch:28 step:27005 [D loss: 0.655301, acc: 62.50%] [G loss: 2.021091]\n",
      "epoch:28 step:27006 [D loss: 0.692034, acc: 53.91%] [G loss: 1.967816]\n",
      "epoch:28 step:27007 [D loss: 0.645485, acc: 65.62%] [G loss: 1.931407]\n",
      "epoch:28 step:27008 [D loss: 0.626386, acc: 62.50%] [G loss: 1.972446]\n",
      "epoch:28 step:27009 [D loss: 0.689655, acc: 60.94%] [G loss: 1.801445]\n",
      "epoch:28 step:27010 [D loss: 0.601738, acc: 68.75%] [G loss: 2.229248]\n",
      "epoch:28 step:27011 [D loss: 0.635435, acc: 67.19%] [G loss: 2.239019]\n",
      "epoch:28 step:27012 [D loss: 0.635212, acc: 63.28%] [G loss: 1.950248]\n",
      "epoch:28 step:27013 [D loss: 0.632400, acc: 60.94%] [G loss: 1.979958]\n",
      "epoch:28 step:27014 [D loss: 0.650896, acc: 64.06%] [G loss: 1.923849]\n",
      "epoch:28 step:27015 [D loss: 0.653589, acc: 60.94%] [G loss: 1.781430]\n",
      "epoch:28 step:27016 [D loss: 0.688120, acc: 55.47%] [G loss: 1.873353]\n",
      "epoch:28 step:27017 [D loss: 0.586319, acc: 68.75%] [G loss: 2.157275]\n",
      "epoch:28 step:27018 [D loss: 0.634651, acc: 63.28%] [G loss: 2.145623]\n",
      "epoch:28 step:27019 [D loss: 0.700513, acc: 53.91%] [G loss: 1.861974]\n",
      "epoch:28 step:27020 [D loss: 0.709741, acc: 53.12%] [G loss: 1.715310]\n",
      "epoch:28 step:27021 [D loss: 0.686714, acc: 57.81%] [G loss: 1.770428]\n",
      "epoch:28 step:27022 [D loss: 0.637684, acc: 64.06%] [G loss: 1.901111]\n",
      "epoch:28 step:27023 [D loss: 0.658095, acc: 63.28%] [G loss: 1.811543]\n",
      "epoch:28 step:27024 [D loss: 0.697524, acc: 51.56%] [G loss: 1.760179]\n",
      "epoch:28 step:27025 [D loss: 0.602686, acc: 71.09%] [G loss: 1.838450]\n",
      "epoch:28 step:27026 [D loss: 0.604196, acc: 64.84%] [G loss: 1.921410]\n",
      "epoch:28 step:27027 [D loss: 0.632391, acc: 65.62%] [G loss: 1.824495]\n",
      "epoch:28 step:27028 [D loss: 0.598808, acc: 71.09%] [G loss: 1.947625]\n",
      "epoch:28 step:27029 [D loss: 0.655400, acc: 56.25%] [G loss: 1.882230]\n",
      "epoch:28 step:27030 [D loss: 0.675019, acc: 57.81%] [G loss: 1.705232]\n",
      "epoch:28 step:27031 [D loss: 0.658751, acc: 63.28%] [G loss: 1.843406]\n",
      "epoch:28 step:27032 [D loss: 0.625259, acc: 57.03%] [G loss: 1.940779]\n",
      "epoch:28 step:27033 [D loss: 0.682509, acc: 56.25%] [G loss: 1.762056]\n",
      "epoch:28 step:27034 [D loss: 0.662925, acc: 64.84%] [G loss: 1.856424]\n",
      "epoch:28 step:27035 [D loss: 0.645682, acc: 63.28%] [G loss: 1.725299]\n",
      "epoch:28 step:27036 [D loss: 0.708865, acc: 53.12%] [G loss: 1.695609]\n",
      "epoch:28 step:27037 [D loss: 0.688156, acc: 56.25%] [G loss: 1.736945]\n",
      "epoch:28 step:27038 [D loss: 0.679603, acc: 52.34%] [G loss: 1.834205]\n",
      "epoch:28 step:27039 [D loss: 0.635652, acc: 66.41%] [G loss: 1.917947]\n",
      "epoch:28 step:27040 [D loss: 0.623280, acc: 70.31%] [G loss: 1.896563]\n",
      "epoch:28 step:27041 [D loss: 0.670628, acc: 50.78%] [G loss: 1.921761]\n",
      "epoch:28 step:27042 [D loss: 0.634180, acc: 60.94%] [G loss: 1.887336]\n",
      "epoch:28 step:27043 [D loss: 0.630032, acc: 62.50%] [G loss: 1.799766]\n",
      "epoch:28 step:27044 [D loss: 0.668041, acc: 62.50%] [G loss: 1.895664]\n",
      "epoch:28 step:27045 [D loss: 0.685516, acc: 57.81%] [G loss: 1.984993]\n",
      "epoch:28 step:27046 [D loss: 0.692422, acc: 53.12%] [G loss: 1.882499]\n",
      "epoch:28 step:27047 [D loss: 0.592035, acc: 73.44%] [G loss: 1.816652]\n",
      "epoch:28 step:27048 [D loss: 0.690245, acc: 53.91%] [G loss: 1.761814]\n",
      "epoch:28 step:27049 [D loss: 0.656197, acc: 56.25%] [G loss: 1.871974]\n",
      "epoch:28 step:27050 [D loss: 0.663809, acc: 56.25%] [G loss: 1.904036]\n",
      "epoch:28 step:27051 [D loss: 0.603199, acc: 65.62%] [G loss: 2.059953]\n",
      "epoch:28 step:27052 [D loss: 0.623354, acc: 65.62%] [G loss: 1.963069]\n",
      "epoch:28 step:27053 [D loss: 0.629091, acc: 60.16%] [G loss: 1.854376]\n",
      "epoch:28 step:27054 [D loss: 0.696371, acc: 53.91%] [G loss: 1.726609]\n",
      "epoch:28 step:27055 [D loss: 0.613440, acc: 64.84%] [G loss: 1.821339]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:28 step:27056 [D loss: 0.723078, acc: 51.56%] [G loss: 1.679379]\n",
      "epoch:28 step:27057 [D loss: 0.676389, acc: 54.69%] [G loss: 1.806241]\n",
      "epoch:28 step:27058 [D loss: 0.632810, acc: 66.41%] [G loss: 1.861694]\n",
      "epoch:28 step:27059 [D loss: 0.590243, acc: 67.19%] [G loss: 1.886563]\n",
      "epoch:28 step:27060 [D loss: 0.642814, acc: 62.50%] [G loss: 1.884880]\n",
      "epoch:28 step:27061 [D loss: 0.564252, acc: 73.44%] [G loss: 1.812319]\n",
      "epoch:28 step:27062 [D loss: 0.674881, acc: 58.59%] [G loss: 1.750110]\n",
      "epoch:28 step:27063 [D loss: 0.650008, acc: 61.72%] [G loss: 1.657816]\n",
      "epoch:28 step:27064 [D loss: 0.680559, acc: 63.28%] [G loss: 1.677396]\n",
      "epoch:28 step:27065 [D loss: 0.632994, acc: 64.06%] [G loss: 1.730204]\n",
      "epoch:28 step:27066 [D loss: 0.639603, acc: 58.59%] [G loss: 1.834827]\n",
      "epoch:28 step:27067 [D loss: 0.703361, acc: 53.12%] [G loss: 1.956764]\n",
      "epoch:28 step:27068 [D loss: 0.625530, acc: 65.62%] [G loss: 1.881293]\n",
      "epoch:28 step:27069 [D loss: 0.640328, acc: 61.72%] [G loss: 1.988986]\n",
      "epoch:28 step:27070 [D loss: 0.650890, acc: 60.94%] [G loss: 1.874683]\n",
      "epoch:28 step:27071 [D loss: 0.601984, acc: 65.62%] [G loss: 1.876580]\n",
      "epoch:28 step:27072 [D loss: 0.666772, acc: 65.62%] [G loss: 1.784401]\n",
      "epoch:28 step:27073 [D loss: 0.634105, acc: 61.72%] [G loss: 1.866616]\n",
      "epoch:28 step:27074 [D loss: 0.609116, acc: 71.09%] [G loss: 1.874128]\n",
      "epoch:28 step:27075 [D loss: 0.649101, acc: 61.72%] [G loss: 1.869141]\n",
      "epoch:28 step:27076 [D loss: 0.660110, acc: 60.16%] [G loss: 1.894096]\n",
      "epoch:28 step:27077 [D loss: 0.631461, acc: 64.84%] [G loss: 1.833113]\n",
      "epoch:28 step:27078 [D loss: 0.652066, acc: 59.38%] [G loss: 2.021086]\n",
      "epoch:28 step:27079 [D loss: 0.640486, acc: 60.94%] [G loss: 1.927777]\n",
      "epoch:28 step:27080 [D loss: 0.679861, acc: 53.91%] [G loss: 1.917092]\n",
      "epoch:28 step:27081 [D loss: 0.619070, acc: 65.62%] [G loss: 2.004145]\n",
      "epoch:28 step:27082 [D loss: 0.704656, acc: 57.03%] [G loss: 1.717677]\n",
      "epoch:28 step:27083 [D loss: 0.615094, acc: 64.06%] [G loss: 1.719778]\n",
      "epoch:28 step:27084 [D loss: 0.676404, acc: 55.47%] [G loss: 1.889063]\n",
      "epoch:28 step:27085 [D loss: 0.618862, acc: 68.75%] [G loss: 1.903575]\n",
      "epoch:28 step:27086 [D loss: 0.647678, acc: 61.72%] [G loss: 1.693634]\n",
      "epoch:28 step:27087 [D loss: 0.684766, acc: 57.81%] [G loss: 1.906802]\n",
      "epoch:28 step:27088 [D loss: 0.688950, acc: 58.59%] [G loss: 1.797846]\n",
      "epoch:28 step:27089 [D loss: 0.659279, acc: 64.06%] [G loss: 1.925864]\n",
      "epoch:28 step:27090 [D loss: 0.682292, acc: 58.59%] [G loss: 1.822013]\n",
      "epoch:28 step:27091 [D loss: 0.657758, acc: 59.38%] [G loss: 1.793680]\n",
      "epoch:28 step:27092 [D loss: 0.682374, acc: 61.72%] [G loss: 1.652301]\n",
      "epoch:28 step:27093 [D loss: 0.625599, acc: 67.19%] [G loss: 1.852365]\n",
      "epoch:28 step:27094 [D loss: 0.685834, acc: 54.69%] [G loss: 1.780316]\n",
      "epoch:28 step:27095 [D loss: 0.643990, acc: 60.16%] [G loss: 1.889701]\n",
      "epoch:28 step:27096 [D loss: 0.612387, acc: 68.75%] [G loss: 1.898015]\n",
      "epoch:28 step:27097 [D loss: 0.656578, acc: 63.28%] [G loss: 1.752908]\n",
      "epoch:28 step:27098 [D loss: 0.730146, acc: 51.56%] [G loss: 1.789756]\n",
      "epoch:28 step:27099 [D loss: 0.678855, acc: 51.56%] [G loss: 1.808755]\n",
      "epoch:28 step:27100 [D loss: 0.629367, acc: 64.84%] [G loss: 1.788788]\n",
      "epoch:28 step:27101 [D loss: 0.668628, acc: 55.47%] [G loss: 1.729188]\n",
      "epoch:28 step:27102 [D loss: 0.649623, acc: 60.94%] [G loss: 1.854564]\n",
      "epoch:28 step:27103 [D loss: 0.695310, acc: 55.47%] [G loss: 1.735705]\n",
      "epoch:28 step:27104 [D loss: 0.640792, acc: 64.06%] [G loss: 1.953861]\n",
      "epoch:28 step:27105 [D loss: 0.636930, acc: 63.28%] [G loss: 1.880960]\n",
      "epoch:28 step:27106 [D loss: 0.647748, acc: 60.94%] [G loss: 1.923786]\n",
      "epoch:28 step:27107 [D loss: 0.705433, acc: 57.03%] [G loss: 1.697839]\n",
      "epoch:28 step:27108 [D loss: 0.639983, acc: 61.72%] [G loss: 1.766846]\n",
      "epoch:28 step:27109 [D loss: 0.691334, acc: 59.38%] [G loss: 1.737823]\n",
      "epoch:28 step:27110 [D loss: 0.647184, acc: 63.28%] [G loss: 1.735152]\n",
      "epoch:28 step:27111 [D loss: 0.587319, acc: 68.75%] [G loss: 2.024787]\n",
      "epoch:28 step:27112 [D loss: 0.661667, acc: 58.59%] [G loss: 1.948670]\n",
      "epoch:28 step:27113 [D loss: 0.700581, acc: 55.47%] [G loss: 1.973377]\n",
      "epoch:28 step:27114 [D loss: 0.726892, acc: 49.22%] [G loss: 1.931943]\n",
      "epoch:28 step:27115 [D loss: 0.609764, acc: 65.62%] [G loss: 1.898384]\n",
      "epoch:28 step:27116 [D loss: 0.652037, acc: 62.50%] [G loss: 1.820665]\n",
      "epoch:28 step:27117 [D loss: 0.642544, acc: 63.28%] [G loss: 1.722764]\n",
      "epoch:28 step:27118 [D loss: 0.658064, acc: 61.72%] [G loss: 1.953336]\n",
      "epoch:28 step:27119 [D loss: 0.660892, acc: 63.28%] [G loss: 1.815038]\n",
      "epoch:28 step:27120 [D loss: 0.635448, acc: 60.94%] [G loss: 1.984295]\n",
      "epoch:28 step:27121 [D loss: 0.663740, acc: 59.38%] [G loss: 1.797868]\n",
      "epoch:28 step:27122 [D loss: 0.647072, acc: 59.38%] [G loss: 1.923362]\n",
      "epoch:28 step:27123 [D loss: 0.649266, acc: 63.28%] [G loss: 1.900148]\n",
      "epoch:28 step:27124 [D loss: 0.568882, acc: 74.22%] [G loss: 1.949409]\n",
      "epoch:28 step:27125 [D loss: 0.609302, acc: 68.75%] [G loss: 1.855716]\n",
      "epoch:28 step:27126 [D loss: 0.622458, acc: 64.84%] [G loss: 1.963897]\n",
      "epoch:28 step:27127 [D loss: 0.644175, acc: 62.50%] [G loss: 1.815474]\n",
      "epoch:28 step:27128 [D loss: 0.656274, acc: 57.03%] [G loss: 1.950977]\n",
      "epoch:28 step:27129 [D loss: 0.692855, acc: 52.34%] [G loss: 1.882391]\n",
      "epoch:28 step:27130 [D loss: 0.616022, acc: 66.41%] [G loss: 1.862659]\n",
      "epoch:28 step:27131 [D loss: 0.625876, acc: 64.84%] [G loss: 1.905604]\n",
      "epoch:28 step:27132 [D loss: 0.704928, acc: 51.56%] [G loss: 1.841414]\n",
      "epoch:28 step:27133 [D loss: 0.649736, acc: 62.50%] [G loss: 1.974748]\n",
      "epoch:28 step:27134 [D loss: 0.645468, acc: 67.97%] [G loss: 2.056045]\n",
      "epoch:28 step:27135 [D loss: 0.669531, acc: 58.59%] [G loss: 1.983127]\n",
      "epoch:28 step:27136 [D loss: 0.618489, acc: 64.84%] [G loss: 1.891549]\n",
      "epoch:28 step:27137 [D loss: 0.623689, acc: 65.62%] [G loss: 1.949035]\n",
      "epoch:28 step:27138 [D loss: 0.613052, acc: 66.41%] [G loss: 1.854772]\n",
      "epoch:28 step:27139 [D loss: 0.629058, acc: 65.62%] [G loss: 2.025236]\n",
      "epoch:28 step:27140 [D loss: 0.619386, acc: 64.84%] [G loss: 1.928423]\n",
      "epoch:28 step:27141 [D loss: 0.647187, acc: 64.84%] [G loss: 1.988887]\n",
      "epoch:28 step:27142 [D loss: 0.651722, acc: 57.81%] [G loss: 1.964368]\n",
      "epoch:28 step:27143 [D loss: 0.647364, acc: 67.19%] [G loss: 1.967262]\n",
      "epoch:28 step:27144 [D loss: 0.647737, acc: 67.97%] [G loss: 1.996701]\n",
      "epoch:28 step:27145 [D loss: 0.637817, acc: 63.28%] [G loss: 1.964360]\n",
      "epoch:28 step:27146 [D loss: 0.630910, acc: 60.16%] [G loss: 2.000428]\n",
      "epoch:28 step:27147 [D loss: 0.662876, acc: 59.38%] [G loss: 1.816081]\n",
      "epoch:28 step:27148 [D loss: 0.685868, acc: 57.03%] [G loss: 2.050676]\n",
      "epoch:28 step:27149 [D loss: 0.703299, acc: 52.34%] [G loss: 1.838130]\n",
      "epoch:28 step:27150 [D loss: 0.697643, acc: 60.94%] [G loss: 1.852167]\n",
      "epoch:28 step:27151 [D loss: 0.670768, acc: 57.81%] [G loss: 1.821747]\n",
      "epoch:28 step:27152 [D loss: 0.610328, acc: 67.19%] [G loss: 1.837466]\n",
      "epoch:28 step:27153 [D loss: 0.641975, acc: 62.50%] [G loss: 1.898968]\n",
      "epoch:28 step:27154 [D loss: 0.609439, acc: 71.09%] [G loss: 2.218864]\n",
      "epoch:28 step:27155 [D loss: 0.575474, acc: 71.09%] [G loss: 2.216731]\n",
      "epoch:28 step:27156 [D loss: 0.703780, acc: 52.34%] [G loss: 1.849766]\n",
      "epoch:28 step:27157 [D loss: 0.638133, acc: 61.72%] [G loss: 1.968990]\n",
      "epoch:28 step:27158 [D loss: 0.612310, acc: 62.50%] [G loss: 1.903224]\n",
      "epoch:28 step:27159 [D loss: 0.617794, acc: 62.50%] [G loss: 2.008523]\n",
      "epoch:28 step:27160 [D loss: 0.564582, acc: 78.12%] [G loss: 2.094310]\n",
      "epoch:28 step:27161 [D loss: 0.635578, acc: 63.28%] [G loss: 2.117999]\n",
      "epoch:28 step:27162 [D loss: 0.673096, acc: 55.47%] [G loss: 1.884930]\n",
      "epoch:28 step:27163 [D loss: 0.679642, acc: 53.12%] [G loss: 1.818460]\n",
      "epoch:28 step:27164 [D loss: 0.754434, acc: 50.78%] [G loss: 1.843838]\n",
      "epoch:28 step:27165 [D loss: 0.700642, acc: 48.44%] [G loss: 1.879877]\n",
      "epoch:28 step:27166 [D loss: 0.594047, acc: 66.41%] [G loss: 2.087126]\n",
      "epoch:28 step:27167 [D loss: 0.615557, acc: 67.19%] [G loss: 1.881474]\n",
      "epoch:28 step:27168 [D loss: 0.698880, acc: 57.03%] [G loss: 1.903442]\n",
      "epoch:28 step:27169 [D loss: 0.671519, acc: 59.38%] [G loss: 1.897005]\n",
      "epoch:28 step:27170 [D loss: 0.635484, acc: 67.19%] [G loss: 1.873553]\n",
      "epoch:28 step:27171 [D loss: 0.666923, acc: 56.25%] [G loss: 1.886350]\n",
      "epoch:28 step:27172 [D loss: 0.575996, acc: 71.88%] [G loss: 1.937363]\n",
      "epoch:28 step:27173 [D loss: 0.567719, acc: 66.41%] [G loss: 2.452285]\n",
      "epoch:29 step:27174 [D loss: 0.689000, acc: 56.25%] [G loss: 1.917579]\n",
      "epoch:29 step:27175 [D loss: 0.660553, acc: 57.81%] [G loss: 1.915132]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:29 step:27176 [D loss: 0.647573, acc: 58.59%] [G loss: 2.108672]\n",
      "epoch:29 step:27177 [D loss: 0.643445, acc: 61.72%] [G loss: 1.824190]\n",
      "epoch:29 step:27178 [D loss: 0.648903, acc: 64.84%] [G loss: 1.821518]\n",
      "epoch:29 step:27179 [D loss: 0.607508, acc: 68.75%] [G loss: 1.891358]\n",
      "epoch:29 step:27180 [D loss: 0.692652, acc: 53.91%] [G loss: 1.998369]\n",
      "epoch:29 step:27181 [D loss: 0.643529, acc: 64.06%] [G loss: 1.968768]\n",
      "epoch:29 step:27182 [D loss: 0.634664, acc: 60.16%] [G loss: 1.942558]\n",
      "epoch:29 step:27183 [D loss: 0.598789, acc: 70.31%] [G loss: 1.968579]\n",
      "epoch:29 step:27184 [D loss: 0.597921, acc: 68.75%] [G loss: 1.817466]\n",
      "epoch:29 step:27185 [D loss: 0.603878, acc: 65.62%] [G loss: 1.982214]\n",
      "epoch:29 step:27186 [D loss: 0.639867, acc: 58.59%] [G loss: 1.908554]\n",
      "epoch:29 step:27187 [D loss: 0.616227, acc: 64.06%] [G loss: 1.842566]\n",
      "epoch:29 step:27188 [D loss: 0.608531, acc: 67.19%] [G loss: 2.116425]\n",
      "epoch:29 step:27189 [D loss: 0.607659, acc: 63.28%] [G loss: 2.078482]\n",
      "epoch:29 step:27190 [D loss: 0.703634, acc: 54.69%] [G loss: 1.867122]\n",
      "epoch:29 step:27191 [D loss: 0.704587, acc: 53.12%] [G loss: 1.938910]\n",
      "epoch:29 step:27192 [D loss: 0.670385, acc: 61.72%] [G loss: 1.914341]\n",
      "epoch:29 step:27193 [D loss: 0.711075, acc: 51.56%] [G loss: 1.659901]\n",
      "epoch:29 step:27194 [D loss: 0.688104, acc: 55.47%] [G loss: 1.927719]\n",
      "epoch:29 step:27195 [D loss: 0.653150, acc: 58.59%] [G loss: 1.824874]\n",
      "epoch:29 step:27196 [D loss: 0.652446, acc: 64.84%] [G loss: 1.849797]\n",
      "epoch:29 step:27197 [D loss: 0.635127, acc: 66.41%] [G loss: 1.926946]\n",
      "epoch:29 step:27198 [D loss: 0.625649, acc: 60.16%] [G loss: 1.920102]\n",
      "epoch:29 step:27199 [D loss: 0.636841, acc: 63.28%] [G loss: 1.896819]\n",
      "epoch:29 step:27200 [D loss: 0.648262, acc: 64.06%] [G loss: 1.826267]\n",
      "##############\n",
      "[2.42733242 1.72393183 6.18937214 4.63794315 3.47141501 5.55259473\n",
      " 4.47400436 4.66649821 4.40532208 3.69967638]\n",
      "##########\n",
      "epoch:29 step:27201 [D loss: 0.618178, acc: 67.19%] [G loss: 1.914920]\n",
      "epoch:29 step:27202 [D loss: 0.629776, acc: 64.84%] [G loss: 1.763880]\n",
      "epoch:29 step:27203 [D loss: 0.627680, acc: 60.94%] [G loss: 2.012778]\n",
      "epoch:29 step:27204 [D loss: 0.736710, acc: 53.91%] [G loss: 1.668781]\n",
      "epoch:29 step:27205 [D loss: 0.650822, acc: 60.94%] [G loss: 1.767239]\n",
      "epoch:29 step:27206 [D loss: 0.631349, acc: 60.94%] [G loss: 1.918606]\n",
      "epoch:29 step:27207 [D loss: 0.639979, acc: 67.97%] [G loss: 1.807738]\n",
      "epoch:29 step:27208 [D loss: 0.706771, acc: 52.34%] [G loss: 1.799018]\n",
      "epoch:29 step:27209 [D loss: 0.649241, acc: 60.16%] [G loss: 1.769441]\n",
      "epoch:29 step:27210 [D loss: 0.610500, acc: 65.62%] [G loss: 1.976009]\n",
      "epoch:29 step:27211 [D loss: 0.678149, acc: 55.47%] [G loss: 1.802934]\n",
      "epoch:29 step:27212 [D loss: 0.675093, acc: 64.06%] [G loss: 1.859996]\n",
      "epoch:29 step:27213 [D loss: 0.648419, acc: 59.38%] [G loss: 1.929550]\n",
      "epoch:29 step:27214 [D loss: 0.633214, acc: 64.06%] [G loss: 1.788211]\n",
      "epoch:29 step:27215 [D loss: 0.620562, acc: 64.84%] [G loss: 1.871659]\n",
      "epoch:29 step:27216 [D loss: 0.631546, acc: 68.75%] [G loss: 1.989593]\n",
      "epoch:29 step:27217 [D loss: 0.620711, acc: 65.62%] [G loss: 1.784874]\n",
      "epoch:29 step:27218 [D loss: 0.673300, acc: 55.47%] [G loss: 1.862126]\n",
      "epoch:29 step:27219 [D loss: 0.615053, acc: 63.28%] [G loss: 1.848430]\n",
      "epoch:29 step:27220 [D loss: 0.654031, acc: 60.16%] [G loss: 1.874345]\n",
      "epoch:29 step:27221 [D loss: 0.686013, acc: 59.38%] [G loss: 1.813860]\n",
      "epoch:29 step:27222 [D loss: 0.586551, acc: 67.97%] [G loss: 2.009401]\n",
      "epoch:29 step:27223 [D loss: 0.602438, acc: 71.09%] [G loss: 1.911188]\n",
      "epoch:29 step:27224 [D loss: 0.685825, acc: 57.03%] [G loss: 1.828130]\n",
      "epoch:29 step:27225 [D loss: 0.691686, acc: 57.81%] [G loss: 1.813447]\n",
      "epoch:29 step:27226 [D loss: 0.646149, acc: 64.84%] [G loss: 1.971081]\n",
      "epoch:29 step:27227 [D loss: 0.581013, acc: 73.44%] [G loss: 2.026072]\n",
      "epoch:29 step:27228 [D loss: 0.598391, acc: 71.88%] [G loss: 1.976055]\n",
      "epoch:29 step:27229 [D loss: 0.598417, acc: 63.28%] [G loss: 1.957285]\n",
      "epoch:29 step:27230 [D loss: 0.681042, acc: 60.94%] [G loss: 1.793207]\n",
      "epoch:29 step:27231 [D loss: 0.665017, acc: 58.59%] [G loss: 1.723860]\n",
      "epoch:29 step:27232 [D loss: 0.579980, acc: 71.88%] [G loss: 1.939199]\n",
      "epoch:29 step:27233 [D loss: 0.656258, acc: 63.28%] [G loss: 1.883011]\n",
      "epoch:29 step:27234 [D loss: 0.670275, acc: 57.81%] [G loss: 1.801147]\n",
      "epoch:29 step:27235 [D loss: 0.680116, acc: 62.50%] [G loss: 1.841215]\n",
      "epoch:29 step:27236 [D loss: 0.629792, acc: 62.50%] [G loss: 1.826628]\n",
      "epoch:29 step:27237 [D loss: 0.661120, acc: 58.59%] [G loss: 1.900559]\n",
      "epoch:29 step:27238 [D loss: 0.669131, acc: 63.28%] [G loss: 1.947021]\n",
      "epoch:29 step:27239 [D loss: 0.628303, acc: 64.06%] [G loss: 1.923610]\n",
      "epoch:29 step:27240 [D loss: 0.635059, acc: 62.50%] [G loss: 1.887782]\n",
      "epoch:29 step:27241 [D loss: 0.631519, acc: 64.84%] [G loss: 1.830051]\n",
      "epoch:29 step:27242 [D loss: 0.605797, acc: 68.75%] [G loss: 1.895637]\n",
      "epoch:29 step:27243 [D loss: 0.591080, acc: 71.09%] [G loss: 1.944066]\n",
      "epoch:29 step:27244 [D loss: 0.697227, acc: 51.56%] [G loss: 1.803407]\n",
      "epoch:29 step:27245 [D loss: 0.663853, acc: 60.16%] [G loss: 1.883325]\n",
      "epoch:29 step:27246 [D loss: 0.665194, acc: 60.16%] [G loss: 1.863623]\n",
      "epoch:29 step:27247 [D loss: 0.640299, acc: 60.16%] [G loss: 1.925721]\n",
      "epoch:29 step:27248 [D loss: 0.647430, acc: 61.72%] [G loss: 2.075369]\n",
      "epoch:29 step:27249 [D loss: 0.564246, acc: 73.44%] [G loss: 2.176840]\n",
      "epoch:29 step:27250 [D loss: 0.627368, acc: 64.84%] [G loss: 2.156935]\n",
      "epoch:29 step:27251 [D loss: 0.678538, acc: 60.16%] [G loss: 1.832975]\n",
      "epoch:29 step:27252 [D loss: 0.693318, acc: 53.12%] [G loss: 1.683418]\n",
      "epoch:29 step:27253 [D loss: 0.636732, acc: 64.06%] [G loss: 1.836897]\n",
      "epoch:29 step:27254 [D loss: 0.729355, acc: 50.00%] [G loss: 1.765864]\n",
      "epoch:29 step:27255 [D loss: 0.645834, acc: 67.19%] [G loss: 1.758148]\n",
      "epoch:29 step:27256 [D loss: 0.657066, acc: 57.03%] [G loss: 1.791005]\n",
      "epoch:29 step:27257 [D loss: 0.654240, acc: 60.16%] [G loss: 1.814976]\n",
      "epoch:29 step:27258 [D loss: 0.638458, acc: 62.50%] [G loss: 1.813420]\n",
      "epoch:29 step:27259 [D loss: 0.655079, acc: 57.81%] [G loss: 1.795126]\n",
      "epoch:29 step:27260 [D loss: 0.646248, acc: 63.28%] [G loss: 1.876898]\n",
      "epoch:29 step:27261 [D loss: 0.658916, acc: 60.94%] [G loss: 1.884128]\n",
      "epoch:29 step:27262 [D loss: 0.644405, acc: 64.84%] [G loss: 1.930850]\n",
      "epoch:29 step:27263 [D loss: 0.663728, acc: 59.38%] [G loss: 1.793097]\n",
      "epoch:29 step:27264 [D loss: 0.674410, acc: 62.50%] [G loss: 1.822123]\n",
      "epoch:29 step:27265 [D loss: 0.628894, acc: 65.62%] [G loss: 1.894153]\n",
      "epoch:29 step:27266 [D loss: 0.619247, acc: 64.06%] [G loss: 2.063513]\n",
      "epoch:29 step:27267 [D loss: 0.632772, acc: 66.41%] [G loss: 1.942049]\n",
      "epoch:29 step:27268 [D loss: 0.693779, acc: 51.56%] [G loss: 1.764488]\n",
      "epoch:29 step:27269 [D loss: 0.620621, acc: 68.75%] [G loss: 1.994590]\n",
      "epoch:29 step:27270 [D loss: 0.639002, acc: 63.28%] [G loss: 1.793063]\n",
      "epoch:29 step:27271 [D loss: 0.694646, acc: 60.16%] [G loss: 1.729020]\n",
      "epoch:29 step:27272 [D loss: 0.633338, acc: 61.72%] [G loss: 1.952600]\n",
      "epoch:29 step:27273 [D loss: 0.643837, acc: 64.06%] [G loss: 1.932915]\n",
      "epoch:29 step:27274 [D loss: 0.606806, acc: 67.19%] [G loss: 2.003296]\n",
      "epoch:29 step:27275 [D loss: 0.632486, acc: 60.16%] [G loss: 2.012926]\n",
      "epoch:29 step:27276 [D loss: 0.638651, acc: 67.19%] [G loss: 1.900174]\n",
      "epoch:29 step:27277 [D loss: 0.690120, acc: 55.47%] [G loss: 1.932527]\n",
      "epoch:29 step:27278 [D loss: 0.717565, acc: 51.56%] [G loss: 1.768500]\n",
      "epoch:29 step:27279 [D loss: 0.574647, acc: 70.31%] [G loss: 2.018370]\n",
      "epoch:29 step:27280 [D loss: 0.605751, acc: 64.84%] [G loss: 2.067864]\n",
      "epoch:29 step:27281 [D loss: 0.719322, acc: 47.66%] [G loss: 1.788023]\n",
      "epoch:29 step:27282 [D loss: 0.739093, acc: 55.47%] [G loss: 1.704883]\n",
      "epoch:29 step:27283 [D loss: 0.634608, acc: 63.28%] [G loss: 1.819199]\n",
      "epoch:29 step:27284 [D loss: 0.609706, acc: 70.31%] [G loss: 1.998463]\n",
      "epoch:29 step:27285 [D loss: 0.623670, acc: 62.50%] [G loss: 2.002285]\n",
      "epoch:29 step:27286 [D loss: 0.611805, acc: 68.75%] [G loss: 2.092712]\n",
      "epoch:29 step:27287 [D loss: 0.611702, acc: 69.53%] [G loss: 1.992587]\n",
      "epoch:29 step:27288 [D loss: 0.605708, acc: 65.62%] [G loss: 2.216311]\n",
      "epoch:29 step:27289 [D loss: 0.637831, acc: 61.72%] [G loss: 2.142496]\n",
      "epoch:29 step:27290 [D loss: 0.669885, acc: 55.47%] [G loss: 2.204038]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:29 step:27291 [D loss: 0.630752, acc: 62.50%] [G loss: 2.041331]\n",
      "epoch:29 step:27292 [D loss: 0.606281, acc: 65.62%] [G loss: 2.199816]\n",
      "epoch:29 step:27293 [D loss: 0.601387, acc: 68.75%] [G loss: 1.949855]\n",
      "epoch:29 step:27294 [D loss: 0.692992, acc: 53.91%] [G loss: 1.989904]\n",
      "epoch:29 step:27295 [D loss: 0.622410, acc: 67.97%] [G loss: 2.039048]\n",
      "epoch:29 step:27296 [D loss: 0.719174, acc: 56.25%] [G loss: 1.836141]\n",
      "epoch:29 step:27297 [D loss: 0.700798, acc: 60.94%] [G loss: 1.932888]\n",
      "epoch:29 step:27298 [D loss: 0.696395, acc: 60.94%] [G loss: 1.770561]\n",
      "epoch:29 step:27299 [D loss: 0.674288, acc: 58.59%] [G loss: 1.802237]\n",
      "epoch:29 step:27300 [D loss: 0.643433, acc: 59.38%] [G loss: 1.833776]\n",
      "epoch:29 step:27301 [D loss: 0.624805, acc: 64.84%] [G loss: 1.864392]\n",
      "epoch:29 step:27302 [D loss: 0.687087, acc: 53.91%] [G loss: 1.769150]\n",
      "epoch:29 step:27303 [D loss: 0.676840, acc: 55.47%] [G loss: 1.816898]\n",
      "epoch:29 step:27304 [D loss: 0.625340, acc: 67.97%] [G loss: 2.029469]\n",
      "epoch:29 step:27305 [D loss: 0.642827, acc: 59.38%] [G loss: 1.829005]\n",
      "epoch:29 step:27306 [D loss: 0.694572, acc: 57.81%] [G loss: 1.766364]\n",
      "epoch:29 step:27307 [D loss: 0.633742, acc: 61.72%] [G loss: 1.927078]\n",
      "epoch:29 step:27308 [D loss: 0.653885, acc: 62.50%] [G loss: 1.785222]\n",
      "epoch:29 step:27309 [D loss: 0.657740, acc: 63.28%] [G loss: 1.775442]\n",
      "epoch:29 step:27310 [D loss: 0.683487, acc: 60.16%] [G loss: 1.793627]\n",
      "epoch:29 step:27311 [D loss: 0.698121, acc: 59.38%] [G loss: 1.759119]\n",
      "epoch:29 step:27312 [D loss: 0.661587, acc: 58.59%] [G loss: 1.947306]\n",
      "epoch:29 step:27313 [D loss: 0.665075, acc: 66.41%] [G loss: 1.620822]\n",
      "epoch:29 step:27314 [D loss: 0.694241, acc: 58.59%] [G loss: 1.791721]\n",
      "epoch:29 step:27315 [D loss: 0.699268, acc: 58.59%] [G loss: 1.730233]\n",
      "epoch:29 step:27316 [D loss: 0.654054, acc: 57.03%] [G loss: 1.789701]\n",
      "epoch:29 step:27317 [D loss: 0.633754, acc: 64.84%] [G loss: 1.844394]\n",
      "epoch:29 step:27318 [D loss: 0.633910, acc: 64.84%] [G loss: 1.886445]\n",
      "epoch:29 step:27319 [D loss: 0.646903, acc: 63.28%] [G loss: 1.931976]\n",
      "epoch:29 step:27320 [D loss: 0.620476, acc: 64.84%] [G loss: 1.851643]\n",
      "epoch:29 step:27321 [D loss: 0.695032, acc: 55.47%] [G loss: 1.793205]\n",
      "epoch:29 step:27322 [D loss: 0.656676, acc: 56.25%] [G loss: 1.842930]\n",
      "epoch:29 step:27323 [D loss: 0.609524, acc: 64.06%] [G loss: 1.844092]\n",
      "epoch:29 step:27324 [D loss: 0.632264, acc: 68.75%] [G loss: 2.044543]\n",
      "epoch:29 step:27325 [D loss: 0.662298, acc: 61.72%] [G loss: 1.824797]\n",
      "epoch:29 step:27326 [D loss: 0.667182, acc: 58.59%] [G loss: 1.838315]\n",
      "epoch:29 step:27327 [D loss: 0.684228, acc: 57.03%] [G loss: 1.802781]\n",
      "epoch:29 step:27328 [D loss: 0.648412, acc: 67.19%] [G loss: 1.741395]\n",
      "epoch:29 step:27329 [D loss: 0.613503, acc: 64.06%] [G loss: 1.921746]\n",
      "epoch:29 step:27330 [D loss: 0.634802, acc: 61.72%] [G loss: 1.875573]\n",
      "epoch:29 step:27331 [D loss: 0.655483, acc: 62.50%] [G loss: 1.797674]\n",
      "epoch:29 step:27332 [D loss: 0.692656, acc: 60.16%] [G loss: 1.971453]\n",
      "epoch:29 step:27333 [D loss: 0.669672, acc: 60.16%] [G loss: 1.817197]\n",
      "epoch:29 step:27334 [D loss: 0.707302, acc: 53.91%] [G loss: 1.764793]\n",
      "epoch:29 step:27335 [D loss: 0.611076, acc: 67.19%] [G loss: 1.838331]\n",
      "epoch:29 step:27336 [D loss: 0.677832, acc: 60.16%] [G loss: 1.808752]\n",
      "epoch:29 step:27337 [D loss: 0.667779, acc: 57.81%] [G loss: 1.687416]\n",
      "epoch:29 step:27338 [D loss: 0.666945, acc: 57.03%] [G loss: 1.898336]\n",
      "epoch:29 step:27339 [D loss: 0.621878, acc: 71.09%] [G loss: 1.841106]\n",
      "epoch:29 step:27340 [D loss: 0.623130, acc: 62.50%] [G loss: 1.920055]\n",
      "epoch:29 step:27341 [D loss: 0.613504, acc: 64.84%] [G loss: 1.908696]\n",
      "epoch:29 step:27342 [D loss: 0.641814, acc: 62.50%] [G loss: 1.932212]\n",
      "epoch:29 step:27343 [D loss: 0.593254, acc: 66.41%] [G loss: 1.852945]\n",
      "epoch:29 step:27344 [D loss: 0.667078, acc: 60.16%] [G loss: 2.026741]\n",
      "epoch:29 step:27345 [D loss: 0.603958, acc: 68.75%] [G loss: 1.792398]\n",
      "epoch:29 step:27346 [D loss: 0.623553, acc: 61.72%] [G loss: 1.777291]\n",
      "epoch:29 step:27347 [D loss: 0.670298, acc: 57.81%] [G loss: 1.728807]\n",
      "epoch:29 step:27348 [D loss: 0.672497, acc: 58.59%] [G loss: 1.692636]\n",
      "epoch:29 step:27349 [D loss: 0.651099, acc: 61.72%] [G loss: 1.759796]\n",
      "epoch:29 step:27350 [D loss: 0.671613, acc: 57.03%] [G loss: 1.779260]\n",
      "epoch:29 step:27351 [D loss: 0.657468, acc: 63.28%] [G loss: 1.814005]\n",
      "epoch:29 step:27352 [D loss: 0.642221, acc: 63.28%] [G loss: 1.996973]\n",
      "epoch:29 step:27353 [D loss: 0.704883, acc: 55.47%] [G loss: 1.792539]\n",
      "epoch:29 step:27354 [D loss: 0.671806, acc: 58.59%] [G loss: 1.862917]\n",
      "epoch:29 step:27355 [D loss: 0.632458, acc: 64.06%] [G loss: 1.784560]\n",
      "epoch:29 step:27356 [D loss: 0.654015, acc: 59.38%] [G loss: 1.924162]\n",
      "epoch:29 step:27357 [D loss: 0.658519, acc: 60.16%] [G loss: 1.796649]\n",
      "epoch:29 step:27358 [D loss: 0.663280, acc: 65.62%] [G loss: 1.831942]\n",
      "epoch:29 step:27359 [D loss: 0.650292, acc: 57.81%] [G loss: 1.838545]\n",
      "epoch:29 step:27360 [D loss: 0.659139, acc: 61.72%] [G loss: 1.919219]\n",
      "epoch:29 step:27361 [D loss: 0.645103, acc: 62.50%] [G loss: 1.824124]\n",
      "epoch:29 step:27362 [D loss: 0.653473, acc: 60.16%] [G loss: 1.886048]\n",
      "epoch:29 step:27363 [D loss: 0.624516, acc: 67.97%] [G loss: 1.876665]\n",
      "epoch:29 step:27364 [D loss: 0.676953, acc: 56.25%] [G loss: 1.930521]\n",
      "epoch:29 step:27365 [D loss: 0.671892, acc: 64.84%] [G loss: 1.890336]\n",
      "epoch:29 step:27366 [D loss: 0.658250, acc: 59.38%] [G loss: 1.863720]\n",
      "epoch:29 step:27367 [D loss: 0.668359, acc: 58.59%] [G loss: 1.850174]\n",
      "epoch:29 step:27368 [D loss: 0.657848, acc: 54.69%] [G loss: 1.722301]\n",
      "epoch:29 step:27369 [D loss: 0.712010, acc: 54.69%] [G loss: 1.808285]\n",
      "epoch:29 step:27370 [D loss: 0.622952, acc: 64.84%] [G loss: 1.805768]\n",
      "epoch:29 step:27371 [D loss: 0.665547, acc: 63.28%] [G loss: 1.802342]\n",
      "epoch:29 step:27372 [D loss: 0.621503, acc: 63.28%] [G loss: 1.958380]\n",
      "epoch:29 step:27373 [D loss: 0.703298, acc: 53.12%] [G loss: 1.650811]\n",
      "epoch:29 step:27374 [D loss: 0.636856, acc: 67.97%] [G loss: 2.093236]\n",
      "epoch:29 step:27375 [D loss: 0.644209, acc: 64.84%] [G loss: 1.703020]\n",
      "epoch:29 step:27376 [D loss: 0.636099, acc: 67.97%] [G loss: 1.821468]\n",
      "epoch:29 step:27377 [D loss: 0.626642, acc: 62.50%] [G loss: 1.859639]\n",
      "epoch:29 step:27378 [D loss: 0.632817, acc: 64.06%] [G loss: 1.820125]\n",
      "epoch:29 step:27379 [D loss: 0.614453, acc: 72.66%] [G loss: 2.069182]\n",
      "epoch:29 step:27380 [D loss: 0.626538, acc: 63.28%] [G loss: 2.028458]\n",
      "epoch:29 step:27381 [D loss: 0.594570, acc: 71.88%] [G loss: 2.175475]\n",
      "epoch:29 step:27382 [D loss: 0.585162, acc: 70.31%] [G loss: 2.116486]\n",
      "epoch:29 step:27383 [D loss: 0.738000, acc: 53.12%] [G loss: 1.798762]\n",
      "epoch:29 step:27384 [D loss: 0.707671, acc: 52.34%] [G loss: 1.701761]\n",
      "epoch:29 step:27385 [D loss: 0.660210, acc: 60.94%] [G loss: 1.671266]\n",
      "epoch:29 step:27386 [D loss: 0.698576, acc: 53.91%] [G loss: 1.809682]\n",
      "epoch:29 step:27387 [D loss: 0.722124, acc: 55.47%] [G loss: 1.755563]\n",
      "epoch:29 step:27388 [D loss: 0.689615, acc: 55.47%] [G loss: 1.785088]\n",
      "epoch:29 step:27389 [D loss: 0.618882, acc: 64.84%] [G loss: 1.952853]\n",
      "epoch:29 step:27390 [D loss: 0.637142, acc: 64.06%] [G loss: 1.870063]\n",
      "epoch:29 step:27391 [D loss: 0.593365, acc: 69.53%] [G loss: 2.039794]\n",
      "epoch:29 step:27392 [D loss: 0.629682, acc: 61.72%] [G loss: 2.030798]\n",
      "epoch:29 step:27393 [D loss: 0.669084, acc: 62.50%] [G loss: 1.897749]\n",
      "epoch:29 step:27394 [D loss: 0.638402, acc: 64.06%] [G loss: 1.904081]\n",
      "epoch:29 step:27395 [D loss: 0.653373, acc: 59.38%] [G loss: 2.039802]\n",
      "epoch:29 step:27396 [D loss: 0.652327, acc: 62.50%] [G loss: 1.818013]\n",
      "epoch:29 step:27397 [D loss: 0.681317, acc: 57.03%] [G loss: 1.898028]\n",
      "epoch:29 step:27398 [D loss: 0.594378, acc: 68.75%] [G loss: 1.925286]\n",
      "epoch:29 step:27399 [D loss: 0.637101, acc: 63.28%] [G loss: 1.836923]\n",
      "epoch:29 step:27400 [D loss: 0.615353, acc: 70.31%] [G loss: 1.829875]\n",
      "##############\n",
      "[2.39812944 1.47636271 6.1963721  4.7963659  3.44604793 5.58649148\n",
      " 4.25054637 4.79021536 4.41934112 3.70997059]\n",
      "##########\n",
      "epoch:29 step:27401 [D loss: 0.654550, acc: 61.72%] [G loss: 1.762820]\n",
      "epoch:29 step:27402 [D loss: 0.623420, acc: 70.31%] [G loss: 2.074426]\n",
      "epoch:29 step:27403 [D loss: 0.615183, acc: 67.97%] [G loss: 2.174678]\n",
      "epoch:29 step:27404 [D loss: 0.573377, acc: 71.88%] [G loss: 2.335907]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:29 step:27405 [D loss: 0.528301, acc: 75.78%] [G loss: 2.351316]\n",
      "epoch:29 step:27406 [D loss: 0.636195, acc: 61.72%] [G loss: 1.916516]\n",
      "epoch:29 step:27407 [D loss: 0.685192, acc: 60.16%] [G loss: 1.889875]\n",
      "epoch:29 step:27408 [D loss: 0.633660, acc: 64.84%] [G loss: 1.883082]\n",
      "epoch:29 step:27409 [D loss: 0.674699, acc: 60.94%] [G loss: 1.788414]\n",
      "epoch:29 step:27410 [D loss: 0.572408, acc: 69.53%] [G loss: 1.986282]\n",
      "epoch:29 step:27411 [D loss: 0.630270, acc: 67.19%] [G loss: 1.929913]\n",
      "epoch:29 step:27412 [D loss: 0.658979, acc: 60.16%] [G loss: 1.849274]\n",
      "epoch:29 step:27413 [D loss: 0.675765, acc: 60.16%] [G loss: 1.942505]\n",
      "epoch:29 step:27414 [D loss: 0.599439, acc: 67.97%] [G loss: 2.095269]\n",
      "epoch:29 step:27415 [D loss: 0.655788, acc: 61.72%] [G loss: 2.025098]\n",
      "epoch:29 step:27416 [D loss: 0.657473, acc: 62.50%] [G loss: 1.759160]\n",
      "epoch:29 step:27417 [D loss: 0.650097, acc: 60.94%] [G loss: 2.019880]\n",
      "epoch:29 step:27418 [D loss: 0.629945, acc: 67.19%] [G loss: 1.942454]\n",
      "epoch:29 step:27419 [D loss: 0.660991, acc: 62.50%] [G loss: 2.047611]\n",
      "epoch:29 step:27420 [D loss: 0.719731, acc: 53.12%] [G loss: 1.729483]\n",
      "epoch:29 step:27421 [D loss: 0.620412, acc: 66.41%] [G loss: 2.034785]\n",
      "epoch:29 step:27422 [D loss: 0.667325, acc: 59.38%] [G loss: 1.794269]\n",
      "epoch:29 step:27423 [D loss: 0.681116, acc: 57.81%] [G loss: 1.779189]\n",
      "epoch:29 step:27424 [D loss: 0.676170, acc: 57.03%] [G loss: 1.885041]\n",
      "epoch:29 step:27425 [D loss: 0.678207, acc: 58.59%] [G loss: 1.848071]\n",
      "epoch:29 step:27426 [D loss: 0.639176, acc: 59.38%] [G loss: 1.866766]\n",
      "epoch:29 step:27427 [D loss: 0.650129, acc: 62.50%] [G loss: 1.800532]\n",
      "epoch:29 step:27428 [D loss: 0.680762, acc: 56.25%] [G loss: 1.856849]\n",
      "epoch:29 step:27429 [D loss: 0.643757, acc: 63.28%] [G loss: 1.881547]\n",
      "epoch:29 step:27430 [D loss: 0.625868, acc: 66.41%] [G loss: 1.811980]\n",
      "epoch:29 step:27431 [D loss: 0.655527, acc: 59.38%] [G loss: 1.846971]\n",
      "epoch:29 step:27432 [D loss: 0.659959, acc: 59.38%] [G loss: 1.785847]\n",
      "epoch:29 step:27433 [D loss: 0.636187, acc: 61.72%] [G loss: 1.951429]\n",
      "epoch:29 step:27434 [D loss: 0.667836, acc: 61.72%] [G loss: 1.911212]\n",
      "epoch:29 step:27435 [D loss: 0.602172, acc: 68.75%] [G loss: 1.961661]\n",
      "epoch:29 step:27436 [D loss: 0.640541, acc: 64.84%] [G loss: 1.782845]\n",
      "epoch:29 step:27437 [D loss: 0.631679, acc: 67.19%] [G loss: 2.009971]\n",
      "epoch:29 step:27438 [D loss: 0.663565, acc: 61.72%] [G loss: 1.866927]\n",
      "epoch:29 step:27439 [D loss: 0.638159, acc: 65.62%] [G loss: 1.952999]\n",
      "epoch:29 step:27440 [D loss: 0.640679, acc: 60.94%] [G loss: 1.844338]\n",
      "epoch:29 step:27441 [D loss: 0.712742, acc: 57.03%] [G loss: 1.654805]\n",
      "epoch:29 step:27442 [D loss: 0.631821, acc: 59.38%] [G loss: 1.787677]\n",
      "epoch:29 step:27443 [D loss: 0.670820, acc: 60.94%] [G loss: 1.852387]\n",
      "epoch:29 step:27444 [D loss: 0.631861, acc: 62.50%] [G loss: 1.739914]\n",
      "epoch:29 step:27445 [D loss: 0.644676, acc: 64.84%] [G loss: 1.994302]\n",
      "epoch:29 step:27446 [D loss: 0.688368, acc: 57.81%] [G loss: 1.857760]\n",
      "epoch:29 step:27447 [D loss: 0.616426, acc: 64.84%] [G loss: 2.057853]\n",
      "epoch:29 step:27448 [D loss: 0.592466, acc: 66.41%] [G loss: 1.969929]\n",
      "epoch:29 step:27449 [D loss: 0.598910, acc: 64.84%] [G loss: 2.012580]\n",
      "epoch:29 step:27450 [D loss: 0.665745, acc: 61.72%] [G loss: 1.979263]\n",
      "epoch:29 step:27451 [D loss: 0.619985, acc: 64.06%] [G loss: 1.859915]\n",
      "epoch:29 step:27452 [D loss: 0.665350, acc: 56.25%] [G loss: 1.941399]\n",
      "epoch:29 step:27453 [D loss: 0.688416, acc: 57.81%] [G loss: 2.013423]\n",
      "epoch:29 step:27454 [D loss: 0.656815, acc: 62.50%] [G loss: 1.788824]\n",
      "epoch:29 step:27455 [D loss: 0.618751, acc: 68.75%] [G loss: 1.764857]\n",
      "epoch:29 step:27456 [D loss: 0.628061, acc: 67.19%] [G loss: 1.893275]\n",
      "epoch:29 step:27457 [D loss: 0.673448, acc: 58.59%] [G loss: 1.787554]\n",
      "epoch:29 step:27458 [D loss: 0.643952, acc: 57.81%] [G loss: 1.808021]\n",
      "epoch:29 step:27459 [D loss: 0.593043, acc: 67.97%] [G loss: 1.973055]\n",
      "epoch:29 step:27460 [D loss: 0.676599, acc: 54.69%] [G loss: 1.804394]\n",
      "epoch:29 step:27461 [D loss: 0.641640, acc: 61.72%] [G loss: 1.832184]\n",
      "epoch:29 step:27462 [D loss: 0.605279, acc: 68.75%] [G loss: 1.756956]\n",
      "epoch:29 step:27463 [D loss: 0.685816, acc: 54.69%] [G loss: 1.804185]\n",
      "epoch:29 step:27464 [D loss: 0.691434, acc: 60.94%] [G loss: 1.912767]\n",
      "epoch:29 step:27465 [D loss: 0.669650, acc: 57.81%] [G loss: 1.788623]\n",
      "epoch:29 step:27466 [D loss: 0.656634, acc: 60.16%] [G loss: 1.889429]\n",
      "epoch:29 step:27467 [D loss: 0.638166, acc: 65.62%] [G loss: 1.795083]\n",
      "epoch:29 step:27468 [D loss: 0.618846, acc: 64.06%] [G loss: 1.781152]\n",
      "epoch:29 step:27469 [D loss: 0.642839, acc: 60.16%] [G loss: 2.022800]\n",
      "epoch:29 step:27470 [D loss: 0.636826, acc: 55.47%] [G loss: 1.877599]\n",
      "epoch:29 step:27471 [D loss: 0.658530, acc: 64.06%] [G loss: 1.907066]\n",
      "epoch:29 step:27472 [D loss: 0.625178, acc: 62.50%] [G loss: 1.880187]\n",
      "epoch:29 step:27473 [D loss: 0.658135, acc: 60.16%] [G loss: 1.840744]\n",
      "epoch:29 step:27474 [D loss: 0.644802, acc: 62.50%] [G loss: 1.766523]\n",
      "epoch:29 step:27475 [D loss: 0.615147, acc: 69.53%] [G loss: 1.853153]\n",
      "epoch:29 step:27476 [D loss: 0.627884, acc: 67.97%] [G loss: 1.904882]\n",
      "epoch:29 step:27477 [D loss: 0.627547, acc: 64.84%] [G loss: 1.868358]\n",
      "epoch:29 step:27478 [D loss: 0.599457, acc: 67.19%] [G loss: 1.859224]\n",
      "epoch:29 step:27479 [D loss: 0.689661, acc: 58.59%] [G loss: 1.865036]\n",
      "epoch:29 step:27480 [D loss: 0.604201, acc: 72.66%] [G loss: 1.889962]\n",
      "epoch:29 step:27481 [D loss: 0.698317, acc: 58.59%] [G loss: 1.770039]\n",
      "epoch:29 step:27482 [D loss: 0.656150, acc: 63.28%] [G loss: 1.809350]\n",
      "epoch:29 step:27483 [D loss: 0.638437, acc: 62.50%] [G loss: 1.855847]\n",
      "epoch:29 step:27484 [D loss: 0.642943, acc: 63.28%] [G loss: 1.837749]\n",
      "epoch:29 step:27485 [D loss: 0.644561, acc: 57.81%] [G loss: 2.109026]\n",
      "epoch:29 step:27486 [D loss: 0.579731, acc: 71.09%] [G loss: 2.171850]\n",
      "epoch:29 step:27487 [D loss: 0.580301, acc: 67.97%] [G loss: 2.178005]\n",
      "epoch:29 step:27488 [D loss: 0.568408, acc: 74.22%] [G loss: 2.184460]\n",
      "epoch:29 step:27489 [D loss: 0.712568, acc: 58.59%] [G loss: 1.863908]\n",
      "epoch:29 step:27490 [D loss: 0.669713, acc: 60.94%] [G loss: 1.816130]\n",
      "epoch:29 step:27491 [D loss: 0.702863, acc: 58.59%] [G loss: 1.922842]\n",
      "epoch:29 step:27492 [D loss: 0.700845, acc: 56.25%] [G loss: 1.888718]\n",
      "epoch:29 step:27493 [D loss: 0.670724, acc: 53.91%] [G loss: 1.732463]\n",
      "epoch:29 step:27494 [D loss: 0.633871, acc: 60.16%] [G loss: 2.079005]\n",
      "epoch:29 step:27495 [D loss: 0.626161, acc: 68.75%] [G loss: 1.920766]\n",
      "epoch:29 step:27496 [D loss: 0.707196, acc: 51.56%] [G loss: 1.761789]\n",
      "epoch:29 step:27497 [D loss: 0.639846, acc: 60.16%] [G loss: 1.821472]\n",
      "epoch:29 step:27498 [D loss: 0.680295, acc: 53.12%] [G loss: 1.794756]\n",
      "epoch:29 step:27499 [D loss: 0.650550, acc: 60.94%] [G loss: 1.835836]\n",
      "epoch:29 step:27500 [D loss: 0.633258, acc: 65.62%] [G loss: 1.912237]\n",
      "epoch:29 step:27501 [D loss: 0.652648, acc: 60.16%] [G loss: 1.931875]\n",
      "epoch:29 step:27502 [D loss: 0.629176, acc: 67.19%] [G loss: 1.785494]\n",
      "epoch:29 step:27503 [D loss: 0.630153, acc: 67.19%] [G loss: 1.896164]\n",
      "epoch:29 step:27504 [D loss: 0.644491, acc: 62.50%] [G loss: 1.972637]\n",
      "epoch:29 step:27505 [D loss: 0.632625, acc: 62.50%] [G loss: 1.854367]\n",
      "epoch:29 step:27506 [D loss: 0.632887, acc: 65.62%] [G loss: 1.826924]\n",
      "epoch:29 step:27507 [D loss: 0.632318, acc: 64.84%] [G loss: 1.862981]\n",
      "epoch:29 step:27508 [D loss: 0.665740, acc: 60.16%] [G loss: 1.928057]\n",
      "epoch:29 step:27509 [D loss: 0.696200, acc: 53.91%] [G loss: 1.993284]\n",
      "epoch:29 step:27510 [D loss: 0.653859, acc: 62.50%] [G loss: 1.920364]\n",
      "epoch:29 step:27511 [D loss: 0.631744, acc: 67.19%] [G loss: 1.839470]\n",
      "epoch:29 step:27512 [D loss: 0.703426, acc: 52.34%] [G loss: 1.965769]\n",
      "epoch:29 step:27513 [D loss: 0.615615, acc: 67.19%] [G loss: 1.804679]\n",
      "epoch:29 step:27514 [D loss: 0.691151, acc: 55.47%] [G loss: 1.697629]\n",
      "epoch:29 step:27515 [D loss: 0.682657, acc: 51.56%] [G loss: 1.817506]\n",
      "epoch:29 step:27516 [D loss: 0.618458, acc: 66.41%] [G loss: 2.035946]\n",
      "epoch:29 step:27517 [D loss: 0.689806, acc: 60.16%] [G loss: 1.734079]\n",
      "epoch:29 step:27518 [D loss: 0.615239, acc: 65.62%] [G loss: 2.105405]\n",
      "epoch:29 step:27519 [D loss: 0.592466, acc: 67.19%] [G loss: 2.233832]\n",
      "epoch:29 step:27520 [D loss: 0.583855, acc: 70.31%] [G loss: 2.046012]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:29 step:27521 [D loss: 0.678180, acc: 60.16%] [G loss: 1.833302]\n",
      "epoch:29 step:27522 [D loss: 0.727990, acc: 52.34%] [G loss: 1.659573]\n",
      "epoch:29 step:27523 [D loss: 0.621639, acc: 67.19%] [G loss: 1.804607]\n",
      "epoch:29 step:27524 [D loss: 0.669431, acc: 60.94%] [G loss: 1.756661]\n",
      "epoch:29 step:27525 [D loss: 0.657983, acc: 61.72%] [G loss: 1.818851]\n",
      "epoch:29 step:27526 [D loss: 0.691032, acc: 59.38%] [G loss: 1.944952]\n",
      "epoch:29 step:27527 [D loss: 0.647831, acc: 63.28%] [G loss: 1.961866]\n",
      "epoch:29 step:27528 [D loss: 0.709497, acc: 52.34%] [G loss: 1.733724]\n",
      "epoch:29 step:27529 [D loss: 0.648578, acc: 59.38%] [G loss: 1.858245]\n",
      "epoch:29 step:27530 [D loss: 0.586252, acc: 68.75%] [G loss: 2.002983]\n",
      "epoch:29 step:27531 [D loss: 0.676108, acc: 60.16%] [G loss: 1.890610]\n",
      "epoch:29 step:27532 [D loss: 0.608360, acc: 66.41%] [G loss: 1.883437]\n",
      "epoch:29 step:27533 [D loss: 0.596494, acc: 68.75%] [G loss: 1.995439]\n",
      "epoch:29 step:27534 [D loss: 0.660572, acc: 61.72%] [G loss: 1.901796]\n",
      "epoch:29 step:27535 [D loss: 0.662004, acc: 60.16%] [G loss: 1.942560]\n",
      "epoch:29 step:27536 [D loss: 0.636805, acc: 62.50%] [G loss: 1.943336]\n",
      "epoch:29 step:27537 [D loss: 0.626413, acc: 60.94%] [G loss: 1.896759]\n",
      "epoch:29 step:27538 [D loss: 0.624738, acc: 67.19%] [G loss: 1.876271]\n",
      "epoch:29 step:27539 [D loss: 0.643757, acc: 65.62%] [G loss: 1.856632]\n",
      "epoch:29 step:27540 [D loss: 0.631282, acc: 64.06%] [G loss: 1.831455]\n",
      "epoch:29 step:27541 [D loss: 0.643402, acc: 67.97%] [G loss: 1.861279]\n",
      "epoch:29 step:27542 [D loss: 0.668360, acc: 53.91%] [G loss: 1.982145]\n",
      "epoch:29 step:27543 [D loss: 0.679092, acc: 59.38%] [G loss: 1.901199]\n",
      "epoch:29 step:27544 [D loss: 0.611231, acc: 71.88%] [G loss: 2.000730]\n",
      "epoch:29 step:27545 [D loss: 0.634031, acc: 64.06%] [G loss: 1.850285]\n",
      "epoch:29 step:27546 [D loss: 0.662859, acc: 60.16%] [G loss: 1.779777]\n",
      "epoch:29 step:27547 [D loss: 0.661254, acc: 60.94%] [G loss: 1.904457]\n",
      "epoch:29 step:27548 [D loss: 0.649903, acc: 64.84%] [G loss: 1.769050]\n",
      "epoch:29 step:27549 [D loss: 0.692777, acc: 58.59%] [G loss: 1.875283]\n",
      "epoch:29 step:27550 [D loss: 0.691481, acc: 50.78%] [G loss: 1.721437]\n",
      "epoch:29 step:27551 [D loss: 0.656633, acc: 62.50%] [G loss: 1.791862]\n",
      "epoch:29 step:27552 [D loss: 0.643278, acc: 63.28%] [G loss: 1.869792]\n",
      "epoch:29 step:27553 [D loss: 0.661403, acc: 60.94%] [G loss: 1.769067]\n",
      "epoch:29 step:27554 [D loss: 0.630654, acc: 63.28%] [G loss: 1.946902]\n",
      "epoch:29 step:27555 [D loss: 0.687037, acc: 57.81%] [G loss: 1.849247]\n",
      "epoch:29 step:27556 [D loss: 0.690573, acc: 60.16%] [G loss: 1.795914]\n",
      "epoch:29 step:27557 [D loss: 0.606128, acc: 71.88%] [G loss: 1.897709]\n",
      "epoch:29 step:27558 [D loss: 0.622280, acc: 60.16%] [G loss: 1.903766]\n",
      "epoch:29 step:27559 [D loss: 0.652059, acc: 57.03%] [G loss: 1.783029]\n",
      "epoch:29 step:27560 [D loss: 0.633712, acc: 63.28%] [G loss: 1.729889]\n",
      "epoch:29 step:27561 [D loss: 0.633932, acc: 59.38%] [G loss: 1.883690]\n",
      "epoch:29 step:27562 [D loss: 0.648831, acc: 64.84%] [G loss: 1.832325]\n",
      "epoch:29 step:27563 [D loss: 0.657682, acc: 63.28%] [G loss: 2.045646]\n",
      "epoch:29 step:27564 [D loss: 0.631581, acc: 65.62%] [G loss: 1.838212]\n",
      "epoch:29 step:27565 [D loss: 0.615976, acc: 67.19%] [G loss: 2.011755]\n",
      "epoch:29 step:27566 [D loss: 0.628686, acc: 61.72%] [G loss: 1.899501]\n",
      "epoch:29 step:27567 [D loss: 0.644024, acc: 64.84%] [G loss: 1.908467]\n",
      "epoch:29 step:27568 [D loss: 0.614328, acc: 66.41%] [G loss: 1.908257]\n",
      "epoch:29 step:27569 [D loss: 0.665558, acc: 61.72%] [G loss: 1.738609]\n",
      "epoch:29 step:27570 [D loss: 0.693853, acc: 55.47%] [G loss: 1.697957]\n",
      "epoch:29 step:27571 [D loss: 0.688444, acc: 58.59%] [G loss: 2.032845]\n",
      "epoch:29 step:27572 [D loss: 0.622233, acc: 68.75%] [G loss: 1.991759]\n",
      "epoch:29 step:27573 [D loss: 0.661787, acc: 62.50%] [G loss: 1.851214]\n",
      "epoch:29 step:27574 [D loss: 0.689896, acc: 60.94%] [G loss: 1.739197]\n",
      "epoch:29 step:27575 [D loss: 0.697429, acc: 55.47%] [G loss: 1.922008]\n",
      "epoch:29 step:27576 [D loss: 0.656505, acc: 58.59%] [G loss: 1.969088]\n",
      "epoch:29 step:27577 [D loss: 0.690273, acc: 62.50%] [G loss: 1.945685]\n",
      "epoch:29 step:27578 [D loss: 0.601317, acc: 72.66%] [G loss: 2.090584]\n",
      "epoch:29 step:27579 [D loss: 0.664923, acc: 63.28%] [G loss: 1.982540]\n",
      "epoch:29 step:27580 [D loss: 0.701636, acc: 59.38%] [G loss: 1.915989]\n",
      "epoch:29 step:27581 [D loss: 0.629591, acc: 64.84%] [G loss: 1.761358]\n",
      "epoch:29 step:27582 [D loss: 0.662798, acc: 58.59%] [G loss: 1.919550]\n",
      "epoch:29 step:27583 [D loss: 0.680440, acc: 57.03%] [G loss: 1.927388]\n",
      "epoch:29 step:27584 [D loss: 0.626275, acc: 66.41%] [G loss: 1.841766]\n",
      "epoch:29 step:27585 [D loss: 0.714366, acc: 57.03%] [G loss: 1.879426]\n",
      "epoch:29 step:27586 [D loss: 0.625591, acc: 60.94%] [G loss: 1.828193]\n",
      "epoch:29 step:27587 [D loss: 0.645806, acc: 61.72%] [G loss: 2.057540]\n",
      "epoch:29 step:27588 [D loss: 0.605329, acc: 60.16%] [G loss: 1.954627]\n",
      "epoch:29 step:27589 [D loss: 0.629516, acc: 62.50%] [G loss: 2.059165]\n",
      "epoch:29 step:27590 [D loss: 0.599371, acc: 65.62%] [G loss: 2.134813]\n",
      "epoch:29 step:27591 [D loss: 0.687158, acc: 53.91%] [G loss: 1.936295]\n",
      "epoch:29 step:27592 [D loss: 0.697233, acc: 58.59%] [G loss: 2.056209]\n",
      "epoch:29 step:27593 [D loss: 0.643411, acc: 64.06%] [G loss: 1.885902]\n",
      "epoch:29 step:27594 [D loss: 0.646942, acc: 57.81%] [G loss: 1.888064]\n",
      "epoch:29 step:27595 [D loss: 0.642254, acc: 60.16%] [G loss: 1.841588]\n",
      "epoch:29 step:27596 [D loss: 0.630988, acc: 64.06%] [G loss: 1.794944]\n",
      "epoch:29 step:27597 [D loss: 0.685207, acc: 61.72%] [G loss: 1.891071]\n",
      "epoch:29 step:27598 [D loss: 0.673643, acc: 56.25%] [G loss: 2.000163]\n",
      "epoch:29 step:27599 [D loss: 0.630221, acc: 64.06%] [G loss: 1.933375]\n",
      "epoch:29 step:27600 [D loss: 0.604178, acc: 67.97%] [G loss: 2.060114]\n",
      "##############\n",
      "[2.42300288 1.6283182  6.26136336 4.68340313 3.5530805  5.62053816\n",
      " 4.40850618 4.67131718 4.59665587 3.67781243]\n",
      "##########\n",
      "epoch:29 step:27601 [D loss: 0.573918, acc: 72.66%] [G loss: 2.132128]\n",
      "epoch:29 step:27602 [D loss: 0.634628, acc: 66.41%] [G loss: 2.245863]\n",
      "epoch:29 step:27603 [D loss: 0.612600, acc: 60.94%] [G loss: 2.286255]\n",
      "epoch:29 step:27604 [D loss: 0.593419, acc: 67.19%] [G loss: 1.960916]\n",
      "epoch:29 step:27605 [D loss: 0.697600, acc: 54.69%] [G loss: 1.711217]\n",
      "epoch:29 step:27606 [D loss: 0.660118, acc: 64.06%] [G loss: 1.975404]\n",
      "epoch:29 step:27607 [D loss: 0.651841, acc: 58.59%] [G loss: 1.822751]\n",
      "epoch:29 step:27608 [D loss: 0.618641, acc: 65.62%] [G loss: 1.755342]\n",
      "epoch:29 step:27609 [D loss: 0.689865, acc: 57.81%] [G loss: 1.886780]\n",
      "epoch:29 step:27610 [D loss: 0.716277, acc: 49.22%] [G loss: 1.728826]\n",
      "epoch:29 step:27611 [D loss: 0.681422, acc: 62.50%] [G loss: 1.770450]\n",
      "epoch:29 step:27612 [D loss: 0.659539, acc: 57.03%] [G loss: 1.833328]\n",
      "epoch:29 step:27613 [D loss: 0.693042, acc: 56.25%] [G loss: 1.708059]\n",
      "epoch:29 step:27614 [D loss: 0.682707, acc: 52.34%] [G loss: 1.825417]\n",
      "epoch:29 step:27615 [D loss: 0.678943, acc: 60.16%] [G loss: 1.697240]\n",
      "epoch:29 step:27616 [D loss: 0.731911, acc: 54.69%] [G loss: 1.754973]\n",
      "epoch:29 step:27617 [D loss: 0.622497, acc: 66.41%] [G loss: 1.734353]\n",
      "epoch:29 step:27618 [D loss: 0.652354, acc: 59.38%] [G loss: 1.784408]\n",
      "epoch:29 step:27619 [D loss: 0.639674, acc: 65.62%] [G loss: 1.889670]\n",
      "epoch:29 step:27620 [D loss: 0.615937, acc: 64.84%] [G loss: 1.904692]\n",
      "epoch:29 step:27621 [D loss: 0.655425, acc: 59.38%] [G loss: 1.856389]\n",
      "epoch:29 step:27622 [D loss: 0.640788, acc: 59.38%] [G loss: 1.838757]\n",
      "epoch:29 step:27623 [D loss: 0.630713, acc: 61.72%] [G loss: 1.711548]\n",
      "epoch:29 step:27624 [D loss: 0.667918, acc: 64.84%] [G loss: 1.844822]\n",
      "epoch:29 step:27625 [D loss: 0.603459, acc: 70.31%] [G loss: 1.893545]\n",
      "epoch:29 step:27626 [D loss: 0.659688, acc: 64.06%] [G loss: 1.959241]\n",
      "epoch:29 step:27627 [D loss: 0.649075, acc: 64.84%] [G loss: 1.841760]\n",
      "epoch:29 step:27628 [D loss: 0.666374, acc: 61.72%] [G loss: 1.865802]\n",
      "epoch:29 step:27629 [D loss: 0.606351, acc: 67.19%] [G loss: 1.949941]\n",
      "epoch:29 step:27630 [D loss: 0.609308, acc: 63.28%] [G loss: 1.949456]\n",
      "epoch:29 step:27631 [D loss: 0.681303, acc: 60.16%] [G loss: 1.805428]\n",
      "epoch:29 step:27632 [D loss: 0.681271, acc: 57.03%] [G loss: 1.775929]\n",
      "epoch:29 step:27633 [D loss: 0.672122, acc: 53.91%] [G loss: 1.722850]\n",
      "epoch:29 step:27634 [D loss: 0.685778, acc: 55.47%] [G loss: 1.709386]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:29 step:27635 [D loss: 0.648529, acc: 64.84%] [G loss: 1.874983]\n",
      "epoch:29 step:27636 [D loss: 0.625994, acc: 62.50%] [G loss: 1.755139]\n",
      "epoch:29 step:27637 [D loss: 0.629152, acc: 66.41%] [G loss: 1.836180]\n",
      "epoch:29 step:27638 [D loss: 0.639355, acc: 64.84%] [G loss: 1.957454]\n",
      "epoch:29 step:27639 [D loss: 0.699125, acc: 53.12%] [G loss: 1.915511]\n",
      "epoch:29 step:27640 [D loss: 0.612779, acc: 70.31%] [G loss: 1.880834]\n",
      "epoch:29 step:27641 [D loss: 0.633059, acc: 67.19%] [G loss: 2.142483]\n",
      "epoch:29 step:27642 [D loss: 0.623544, acc: 67.97%] [G loss: 2.093732]\n",
      "epoch:29 step:27643 [D loss: 0.615153, acc: 69.53%] [G loss: 2.120329]\n",
      "epoch:29 step:27644 [D loss: 0.567804, acc: 75.00%] [G loss: 2.229023]\n",
      "epoch:29 step:27645 [D loss: 0.618628, acc: 62.50%] [G loss: 2.116916]\n",
      "epoch:29 step:27646 [D loss: 0.650621, acc: 64.06%] [G loss: 1.786906]\n",
      "epoch:29 step:27647 [D loss: 0.677111, acc: 54.69%] [G loss: 1.837225]\n",
      "epoch:29 step:27648 [D loss: 0.701613, acc: 55.47%] [G loss: 1.886350]\n",
      "epoch:29 step:27649 [D loss: 0.735933, acc: 55.47%] [G loss: 1.875367]\n",
      "epoch:29 step:27650 [D loss: 0.705480, acc: 54.69%] [G loss: 1.671477]\n",
      "epoch:29 step:27651 [D loss: 0.635579, acc: 65.62%] [G loss: 1.845937]\n",
      "epoch:29 step:27652 [D loss: 0.669336, acc: 61.72%] [G loss: 1.931438]\n",
      "epoch:29 step:27653 [D loss: 0.623533, acc: 67.19%] [G loss: 1.983270]\n",
      "epoch:29 step:27654 [D loss: 0.613228, acc: 71.09%] [G loss: 2.125242]\n",
      "epoch:29 step:27655 [D loss: 0.636559, acc: 64.06%] [G loss: 1.840908]\n",
      "epoch:29 step:27656 [D loss: 0.646908, acc: 59.38%] [G loss: 1.923335]\n",
      "epoch:29 step:27657 [D loss: 0.636336, acc: 64.84%] [G loss: 1.967568]\n",
      "epoch:29 step:27658 [D loss: 0.657369, acc: 64.06%] [G loss: 1.747453]\n",
      "epoch:29 step:27659 [D loss: 0.676571, acc: 60.16%] [G loss: 1.842687]\n",
      "epoch:29 step:27660 [D loss: 0.632605, acc: 65.62%] [G loss: 1.874201]\n",
      "epoch:29 step:27661 [D loss: 0.608711, acc: 64.84%] [G loss: 1.932114]\n",
      "epoch:29 step:27662 [D loss: 0.692542, acc: 60.94%] [G loss: 1.742175]\n",
      "epoch:29 step:27663 [D loss: 0.599994, acc: 71.09%] [G loss: 1.935568]\n",
      "epoch:29 step:27664 [D loss: 0.634818, acc: 67.19%] [G loss: 2.009668]\n",
      "epoch:29 step:27665 [D loss: 0.634885, acc: 64.06%] [G loss: 1.829459]\n",
      "epoch:29 step:27666 [D loss: 0.629883, acc: 66.41%] [G loss: 1.877053]\n",
      "epoch:29 step:27667 [D loss: 0.636656, acc: 60.94%] [G loss: 2.056154]\n",
      "epoch:29 step:27668 [D loss: 0.607508, acc: 66.41%] [G loss: 1.964659]\n",
      "epoch:29 step:27669 [D loss: 0.642936, acc: 64.84%] [G loss: 1.942642]\n",
      "epoch:29 step:27670 [D loss: 0.651215, acc: 66.41%] [G loss: 1.918716]\n",
      "epoch:29 step:27671 [D loss: 0.622433, acc: 64.84%] [G loss: 1.921300]\n",
      "epoch:29 step:27672 [D loss: 0.654622, acc: 68.75%] [G loss: 2.019368]\n",
      "epoch:29 step:27673 [D loss: 0.667040, acc: 60.16%] [G loss: 1.816077]\n",
      "epoch:29 step:27674 [D loss: 0.700657, acc: 55.47%] [G loss: 1.738130]\n",
      "epoch:29 step:27675 [D loss: 0.658714, acc: 55.47%] [G loss: 1.758864]\n",
      "epoch:29 step:27676 [D loss: 0.708824, acc: 56.25%] [G loss: 1.876548]\n",
      "epoch:29 step:27677 [D loss: 0.684636, acc: 58.59%] [G loss: 1.935483]\n",
      "epoch:29 step:27678 [D loss: 0.599597, acc: 67.19%] [G loss: 1.992072]\n",
      "epoch:29 step:27679 [D loss: 0.618134, acc: 68.75%] [G loss: 1.691044]\n",
      "epoch:29 step:27680 [D loss: 0.657446, acc: 60.16%] [G loss: 1.765917]\n",
      "epoch:29 step:27681 [D loss: 0.588023, acc: 72.66%] [G loss: 1.936235]\n",
      "epoch:29 step:27682 [D loss: 0.623826, acc: 62.50%] [G loss: 1.816232]\n",
      "epoch:29 step:27683 [D loss: 0.647379, acc: 61.72%] [G loss: 1.830639]\n",
      "epoch:29 step:27684 [D loss: 0.635105, acc: 61.72%] [G loss: 1.804407]\n",
      "epoch:29 step:27685 [D loss: 0.630600, acc: 67.97%] [G loss: 1.916627]\n",
      "epoch:29 step:27686 [D loss: 0.667826, acc: 57.81%] [G loss: 1.908399]\n",
      "epoch:29 step:27687 [D loss: 0.693414, acc: 60.16%] [G loss: 1.900384]\n",
      "epoch:29 step:27688 [D loss: 0.628172, acc: 66.41%] [G loss: 1.900531]\n",
      "epoch:29 step:27689 [D loss: 0.637705, acc: 60.16%] [G loss: 1.932664]\n",
      "epoch:29 step:27690 [D loss: 0.611139, acc: 68.75%] [G loss: 1.916600]\n",
      "epoch:29 step:27691 [D loss: 0.646129, acc: 60.16%] [G loss: 1.788356]\n",
      "epoch:29 step:27692 [D loss: 0.641594, acc: 58.59%] [G loss: 1.788769]\n",
      "epoch:29 step:27693 [D loss: 0.610588, acc: 64.06%] [G loss: 1.889188]\n",
      "epoch:29 step:27694 [D loss: 0.646203, acc: 64.84%] [G loss: 1.885687]\n",
      "epoch:29 step:27695 [D loss: 0.640309, acc: 60.94%] [G loss: 1.965446]\n",
      "epoch:29 step:27696 [D loss: 0.623907, acc: 62.50%] [G loss: 1.962410]\n",
      "epoch:29 step:27697 [D loss: 0.676902, acc: 57.81%] [G loss: 1.871623]\n",
      "epoch:29 step:27698 [D loss: 0.694925, acc: 57.81%] [G loss: 1.793680]\n",
      "epoch:29 step:27699 [D loss: 0.650996, acc: 63.28%] [G loss: 1.834100]\n",
      "epoch:29 step:27700 [D loss: 0.641908, acc: 59.38%] [G loss: 1.880180]\n",
      "epoch:29 step:27701 [D loss: 0.749454, acc: 45.31%] [G loss: 1.777426]\n",
      "epoch:29 step:27702 [D loss: 0.661671, acc: 59.38%] [G loss: 1.651106]\n",
      "epoch:29 step:27703 [D loss: 0.743607, acc: 47.66%] [G loss: 1.766361]\n",
      "epoch:29 step:27704 [D loss: 0.731232, acc: 51.56%] [G loss: 1.778319]\n",
      "epoch:29 step:27705 [D loss: 0.599850, acc: 64.84%] [G loss: 1.941798]\n",
      "epoch:29 step:27706 [D loss: 0.615736, acc: 64.84%] [G loss: 1.803253]\n",
      "epoch:29 step:27707 [D loss: 0.622180, acc: 64.84%] [G loss: 2.019372]\n",
      "epoch:29 step:27708 [D loss: 0.657914, acc: 58.59%] [G loss: 1.727760]\n",
      "epoch:29 step:27709 [D loss: 0.643974, acc: 67.97%] [G loss: 1.829179]\n",
      "epoch:29 step:27710 [D loss: 0.686888, acc: 53.91%] [G loss: 1.761260]\n",
      "epoch:29 step:27711 [D loss: 0.667171, acc: 57.03%] [G loss: 1.694088]\n",
      "epoch:29 step:27712 [D loss: 0.616143, acc: 62.50%] [G loss: 1.817483]\n",
      "epoch:29 step:27713 [D loss: 0.674907, acc: 57.81%] [G loss: 1.825791]\n",
      "epoch:29 step:27714 [D loss: 0.635769, acc: 63.28%] [G loss: 1.761491]\n",
      "epoch:29 step:27715 [D loss: 0.667125, acc: 60.16%] [G loss: 1.784235]\n",
      "epoch:29 step:27716 [D loss: 0.633569, acc: 61.72%] [G loss: 1.872186]\n",
      "epoch:29 step:27717 [D loss: 0.678187, acc: 55.47%] [G loss: 1.856624]\n",
      "epoch:29 step:27718 [D loss: 0.611221, acc: 66.41%] [G loss: 1.902816]\n",
      "epoch:29 step:27719 [D loss: 0.633875, acc: 64.06%] [G loss: 1.946321]\n",
      "epoch:29 step:27720 [D loss: 0.693237, acc: 57.81%] [G loss: 1.885056]\n",
      "epoch:29 step:27721 [D loss: 0.611630, acc: 68.75%] [G loss: 1.946030]\n",
      "epoch:29 step:27722 [D loss: 0.674641, acc: 62.50%] [G loss: 1.977404]\n",
      "epoch:29 step:27723 [D loss: 0.622573, acc: 67.97%] [G loss: 1.999143]\n",
      "epoch:29 step:27724 [D loss: 0.610595, acc: 67.19%] [G loss: 2.032490]\n",
      "epoch:29 step:27725 [D loss: 0.649546, acc: 57.81%] [G loss: 1.928471]\n",
      "epoch:29 step:27726 [D loss: 0.632789, acc: 61.72%] [G loss: 1.965531]\n",
      "epoch:29 step:27727 [D loss: 0.627386, acc: 67.19%] [G loss: 1.989426]\n",
      "epoch:29 step:27728 [D loss: 0.639827, acc: 61.72%] [G loss: 1.938436]\n",
      "epoch:29 step:27729 [D loss: 0.628081, acc: 67.19%] [G loss: 1.895626]\n",
      "epoch:29 step:27730 [D loss: 0.652771, acc: 64.06%] [G loss: 1.980477]\n",
      "epoch:29 step:27731 [D loss: 0.573778, acc: 69.53%] [G loss: 1.975485]\n",
      "epoch:29 step:27732 [D loss: 0.666980, acc: 60.16%] [G loss: 1.860866]\n",
      "epoch:29 step:27733 [D loss: 0.736501, acc: 53.12%] [G loss: 1.834175]\n",
      "epoch:29 step:27734 [D loss: 0.654327, acc: 64.06%] [G loss: 1.746359]\n",
      "epoch:29 step:27735 [D loss: 0.666742, acc: 58.59%] [G loss: 1.975001]\n",
      "epoch:29 step:27736 [D loss: 0.620572, acc: 64.06%] [G loss: 1.920215]\n",
      "epoch:29 step:27737 [D loss: 0.669185, acc: 57.81%] [G loss: 2.038246]\n",
      "epoch:29 step:27738 [D loss: 0.662890, acc: 63.28%] [G loss: 1.957084]\n",
      "epoch:29 step:27739 [D loss: 0.730694, acc: 54.69%] [G loss: 1.670086]\n",
      "epoch:29 step:27740 [D loss: 0.659027, acc: 57.81%] [G loss: 1.817961]\n",
      "epoch:29 step:27741 [D loss: 0.672368, acc: 57.81%] [G loss: 1.767046]\n",
      "epoch:29 step:27742 [D loss: 0.667997, acc: 57.81%] [G loss: 1.840807]\n",
      "epoch:29 step:27743 [D loss: 0.642466, acc: 64.84%] [G loss: 1.815625]\n",
      "epoch:29 step:27744 [D loss: 0.631047, acc: 63.28%] [G loss: 1.934561]\n",
      "epoch:29 step:27745 [D loss: 0.645430, acc: 67.19%] [G loss: 1.740001]\n",
      "epoch:29 step:27746 [D loss: 0.657084, acc: 59.38%] [G loss: 1.778950]\n",
      "epoch:29 step:27747 [D loss: 0.624842, acc: 65.62%] [G loss: 1.907428]\n",
      "epoch:29 step:27748 [D loss: 0.639422, acc: 65.62%] [G loss: 1.798983]\n",
      "epoch:29 step:27749 [D loss: 0.706783, acc: 57.03%] [G loss: 1.717582]\n",
      "epoch:29 step:27750 [D loss: 0.688875, acc: 51.56%] [G loss: 1.700745]\n",
      "epoch:29 step:27751 [D loss: 0.691316, acc: 61.72%] [G loss: 1.729115]\n",
      "epoch:29 step:27752 [D loss: 0.650903, acc: 62.50%] [G loss: 1.828695]\n",
      "epoch:29 step:27753 [D loss: 0.633650, acc: 67.19%] [G loss: 1.780128]\n",
      "epoch:29 step:27754 [D loss: 0.647922, acc: 56.25%] [G loss: 1.821650]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:29 step:27755 [D loss: 0.618767, acc: 70.31%] [G loss: 1.819257]\n",
      "epoch:29 step:27756 [D loss: 0.671513, acc: 59.38%] [G loss: 1.744869]\n",
      "epoch:29 step:27757 [D loss: 0.662041, acc: 54.69%] [G loss: 1.712209]\n",
      "epoch:29 step:27758 [D loss: 0.684771, acc: 62.50%] [G loss: 1.864754]\n",
      "epoch:29 step:27759 [D loss: 0.706421, acc: 56.25%] [G loss: 1.632470]\n",
      "epoch:29 step:27760 [D loss: 0.658163, acc: 60.94%] [G loss: 1.774436]\n",
      "epoch:29 step:27761 [D loss: 0.699265, acc: 58.59%] [G loss: 1.887412]\n",
      "epoch:29 step:27762 [D loss: 0.643842, acc: 63.28%] [G loss: 1.791494]\n",
      "epoch:29 step:27763 [D loss: 0.615624, acc: 67.97%] [G loss: 1.902063]\n",
      "epoch:29 step:27764 [D loss: 0.601863, acc: 64.84%] [G loss: 1.755997]\n",
      "epoch:29 step:27765 [D loss: 0.673723, acc: 64.06%] [G loss: 1.816717]\n",
      "epoch:29 step:27766 [D loss: 0.607386, acc: 65.62%] [G loss: 1.775560]\n",
      "epoch:29 step:27767 [D loss: 0.678368, acc: 58.59%] [G loss: 1.725131]\n",
      "epoch:29 step:27768 [D loss: 0.649103, acc: 62.50%] [G loss: 1.853278]\n",
      "epoch:29 step:27769 [D loss: 0.683978, acc: 59.38%] [G loss: 1.767689]\n",
      "epoch:29 step:27770 [D loss: 0.652142, acc: 60.94%] [G loss: 1.740248]\n",
      "epoch:29 step:27771 [D loss: 0.703145, acc: 53.12%] [G loss: 1.768366]\n",
      "epoch:29 step:27772 [D loss: 0.584914, acc: 68.75%] [G loss: 1.932687]\n",
      "epoch:29 step:27773 [D loss: 0.617185, acc: 67.19%] [G loss: 1.796192]\n",
      "epoch:29 step:27774 [D loss: 0.673776, acc: 64.06%] [G loss: 1.788372]\n",
      "epoch:29 step:27775 [D loss: 0.610112, acc: 64.06%] [G loss: 1.799459]\n",
      "epoch:29 step:27776 [D loss: 0.629301, acc: 64.84%] [G loss: 2.037900]\n",
      "epoch:29 step:27777 [D loss: 0.642398, acc: 64.84%] [G loss: 1.977381]\n",
      "epoch:29 step:27778 [D loss: 0.605379, acc: 69.53%] [G loss: 2.025122]\n",
      "epoch:29 step:27779 [D loss: 0.604307, acc: 64.06%] [G loss: 1.958937]\n",
      "epoch:29 step:27780 [D loss: 0.608482, acc: 66.41%] [G loss: 1.935247]\n",
      "epoch:29 step:27781 [D loss: 0.715005, acc: 55.47%] [G loss: 1.927630]\n",
      "epoch:29 step:27782 [D loss: 0.610726, acc: 71.09%] [G loss: 1.891024]\n",
      "epoch:29 step:27783 [D loss: 0.630428, acc: 67.97%] [G loss: 1.930702]\n",
      "epoch:29 step:27784 [D loss: 0.638715, acc: 65.62%] [G loss: 1.819246]\n",
      "epoch:29 step:27785 [D loss: 0.654022, acc: 63.28%] [G loss: 1.837712]\n",
      "epoch:29 step:27786 [D loss: 0.635155, acc: 66.41%] [G loss: 1.923252]\n",
      "epoch:29 step:27787 [D loss: 0.689232, acc: 56.25%] [G loss: 1.704728]\n",
      "epoch:29 step:27788 [D loss: 0.685387, acc: 57.81%] [G loss: 1.732691]\n",
      "epoch:29 step:27789 [D loss: 0.651630, acc: 60.16%] [G loss: 2.008893]\n",
      "epoch:29 step:27790 [D loss: 0.707674, acc: 54.69%] [G loss: 1.774488]\n",
      "epoch:29 step:27791 [D loss: 0.628229, acc: 63.28%] [G loss: 1.881037]\n",
      "epoch:29 step:27792 [D loss: 0.647356, acc: 58.59%] [G loss: 1.749992]\n",
      "epoch:29 step:27793 [D loss: 0.636843, acc: 63.28%] [G loss: 1.869690]\n",
      "epoch:29 step:27794 [D loss: 0.678949, acc: 55.47%] [G loss: 1.802553]\n",
      "epoch:29 step:27795 [D loss: 0.640595, acc: 65.62%] [G loss: 1.984341]\n",
      "epoch:29 step:27796 [D loss: 0.637812, acc: 64.84%] [G loss: 1.946555]\n",
      "epoch:29 step:27797 [D loss: 0.621221, acc: 66.41%] [G loss: 1.967589]\n",
      "epoch:29 step:27798 [D loss: 0.641608, acc: 58.59%] [G loss: 1.779374]\n",
      "epoch:29 step:27799 [D loss: 0.663459, acc: 58.59%] [G loss: 1.899686]\n",
      "epoch:29 step:27800 [D loss: 0.638583, acc: 60.94%] [G loss: 1.883690]\n",
      "##############\n",
      "[2.54700757 1.60358855 6.17768906 4.68872372 3.66131156 5.58643543\n",
      " 4.49204814 4.70543708 4.51235497 3.78407245]\n",
      "##########\n",
      "epoch:29 step:27801 [D loss: 0.674366, acc: 58.59%] [G loss: 1.760234]\n",
      "epoch:29 step:27802 [D loss: 0.633181, acc: 63.28%] [G loss: 1.953597]\n",
      "epoch:29 step:27803 [D loss: 0.571979, acc: 71.09%] [G loss: 1.986796]\n",
      "epoch:29 step:27804 [D loss: 0.593644, acc: 72.66%] [G loss: 1.854090]\n",
      "epoch:29 step:27805 [D loss: 0.650466, acc: 62.50%] [G loss: 1.941812]\n",
      "epoch:29 step:27806 [D loss: 0.608383, acc: 62.50%] [G loss: 2.115969]\n",
      "epoch:29 step:27807 [D loss: 0.603358, acc: 63.28%] [G loss: 1.839656]\n",
      "epoch:29 step:27808 [D loss: 0.592712, acc: 67.97%] [G loss: 1.950142]\n",
      "epoch:29 step:27809 [D loss: 0.629698, acc: 62.50%] [G loss: 1.858248]\n",
      "epoch:29 step:27810 [D loss: 0.654223, acc: 65.62%] [G loss: 1.810736]\n",
      "epoch:29 step:27811 [D loss: 0.626762, acc: 64.84%] [G loss: 1.985993]\n",
      "epoch:29 step:27812 [D loss: 0.668133, acc: 57.81%] [G loss: 2.007539]\n",
      "epoch:29 step:27813 [D loss: 0.709040, acc: 57.03%] [G loss: 1.954887]\n",
      "epoch:29 step:27814 [D loss: 0.704288, acc: 57.81%] [G loss: 1.958830]\n",
      "epoch:29 step:27815 [D loss: 0.605409, acc: 62.50%] [G loss: 2.213227]\n",
      "epoch:29 step:27816 [D loss: 0.648275, acc: 61.72%] [G loss: 2.048722]\n",
      "epoch:29 step:27817 [D loss: 0.630916, acc: 60.16%] [G loss: 2.088974]\n",
      "epoch:29 step:27818 [D loss: 0.641029, acc: 60.16%] [G loss: 1.991355]\n",
      "epoch:29 step:27819 [D loss: 0.630790, acc: 64.06%] [G loss: 2.157437]\n",
      "epoch:29 step:27820 [D loss: 0.606691, acc: 69.53%] [G loss: 2.037475]\n",
      "epoch:29 step:27821 [D loss: 0.607315, acc: 66.41%] [G loss: 2.350644]\n",
      "epoch:29 step:27822 [D loss: 0.678625, acc: 67.97%] [G loss: 2.021634]\n",
      "epoch:29 step:27823 [D loss: 0.644492, acc: 62.50%] [G loss: 1.901709]\n",
      "epoch:29 step:27824 [D loss: 0.626552, acc: 63.28%] [G loss: 1.982599]\n",
      "epoch:29 step:27825 [D loss: 0.631239, acc: 60.16%] [G loss: 2.001333]\n",
      "epoch:29 step:27826 [D loss: 0.659694, acc: 60.16%] [G loss: 1.886122]\n",
      "epoch:29 step:27827 [D loss: 0.638713, acc: 68.75%] [G loss: 1.748041]\n",
      "epoch:29 step:27828 [D loss: 0.663359, acc: 57.81%] [G loss: 1.816581]\n",
      "epoch:29 step:27829 [D loss: 0.717146, acc: 56.25%] [G loss: 1.825392]\n",
      "epoch:29 step:27830 [D loss: 0.735921, acc: 52.34%] [G loss: 1.669279]\n",
      "epoch:29 step:27831 [D loss: 0.670463, acc: 61.72%] [G loss: 1.790532]\n",
      "epoch:29 step:27832 [D loss: 0.654002, acc: 60.94%] [G loss: 1.806339]\n",
      "epoch:29 step:27833 [D loss: 0.669665, acc: 59.38%] [G loss: 1.788945]\n",
      "epoch:29 step:27834 [D loss: 0.622550, acc: 67.19%] [G loss: 1.777541]\n",
      "epoch:29 step:27835 [D loss: 0.672157, acc: 57.81%] [G loss: 1.996810]\n",
      "epoch:29 step:27836 [D loss: 0.664609, acc: 59.38%] [G loss: 1.827368]\n",
      "epoch:29 step:27837 [D loss: 0.667134, acc: 58.59%] [G loss: 1.840665]\n",
      "epoch:29 step:27838 [D loss: 0.656719, acc: 60.94%] [G loss: 1.713690]\n",
      "epoch:29 step:27839 [D loss: 0.677734, acc: 55.47%] [G loss: 1.663990]\n",
      "epoch:29 step:27840 [D loss: 0.738689, acc: 49.22%] [G loss: 1.693806]\n",
      "epoch:29 step:27841 [D loss: 0.624696, acc: 64.84%] [G loss: 1.834422]\n",
      "epoch:29 step:27842 [D loss: 0.659280, acc: 56.25%] [G loss: 1.765583]\n",
      "epoch:29 step:27843 [D loss: 0.657748, acc: 61.72%] [G loss: 1.823120]\n",
      "epoch:29 step:27844 [D loss: 0.655999, acc: 59.38%] [G loss: 1.612199]\n",
      "epoch:29 step:27845 [D loss: 0.675602, acc: 58.59%] [G loss: 1.837458]\n",
      "epoch:29 step:27846 [D loss: 0.650705, acc: 60.16%] [G loss: 1.711782]\n",
      "epoch:29 step:27847 [D loss: 0.640315, acc: 60.16%] [G loss: 1.795883]\n",
      "epoch:29 step:27848 [D loss: 0.719944, acc: 52.34%] [G loss: 1.734875]\n",
      "epoch:29 step:27849 [D loss: 0.638789, acc: 69.53%] [G loss: 1.744641]\n",
      "epoch:29 step:27850 [D loss: 0.611509, acc: 67.97%] [G loss: 1.861851]\n",
      "epoch:29 step:27851 [D loss: 0.670077, acc: 56.25%] [G loss: 1.915120]\n",
      "epoch:29 step:27852 [D loss: 0.621933, acc: 66.41%] [G loss: 1.777889]\n",
      "epoch:29 step:27853 [D loss: 0.615548, acc: 63.28%] [G loss: 1.847618]\n",
      "epoch:29 step:27854 [D loss: 0.584139, acc: 73.44%] [G loss: 1.915377]\n",
      "epoch:29 step:27855 [D loss: 0.680400, acc: 60.94%] [G loss: 1.783822]\n",
      "epoch:29 step:27856 [D loss: 0.666953, acc: 61.72%] [G loss: 1.956252]\n",
      "epoch:29 step:27857 [D loss: 0.686225, acc: 53.12%] [G loss: 1.846691]\n",
      "epoch:29 step:27858 [D loss: 0.664556, acc: 60.16%] [G loss: 1.939991]\n",
      "epoch:29 step:27859 [D loss: 0.615754, acc: 64.84%] [G loss: 1.978190]\n",
      "epoch:29 step:27860 [D loss: 0.646727, acc: 61.72%] [G loss: 1.876476]\n",
      "epoch:29 step:27861 [D loss: 0.665453, acc: 60.16%] [G loss: 1.803186]\n",
      "epoch:29 step:27862 [D loss: 0.691990, acc: 57.03%] [G loss: 2.009073]\n",
      "epoch:29 step:27863 [D loss: 0.607873, acc: 67.19%] [G loss: 1.862944]\n",
      "epoch:29 step:27864 [D loss: 0.678379, acc: 58.59%] [G loss: 1.946397]\n",
      "epoch:29 step:27865 [D loss: 0.632032, acc: 65.62%] [G loss: 1.863639]\n",
      "epoch:29 step:27866 [D loss: 0.627517, acc: 64.84%] [G loss: 2.130544]\n",
      "epoch:29 step:27867 [D loss: 0.618613, acc: 71.09%] [G loss: 1.967411]\n",
      "epoch:29 step:27868 [D loss: 0.599545, acc: 67.19%] [G loss: 1.866113]\n",
      "epoch:29 step:27869 [D loss: 0.731801, acc: 49.22%] [G loss: 1.910538]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:29 step:27870 [D loss: 0.613452, acc: 60.94%] [G loss: 1.882453]\n",
      "epoch:29 step:27871 [D loss: 0.672931, acc: 60.16%] [G loss: 1.793445]\n",
      "epoch:29 step:27872 [D loss: 0.634717, acc: 63.28%] [G loss: 1.868276]\n",
      "epoch:29 step:27873 [D loss: 0.670552, acc: 56.25%] [G loss: 1.829442]\n",
      "epoch:29 step:27874 [D loss: 0.631478, acc: 66.41%] [G loss: 1.937310]\n",
      "epoch:29 step:27875 [D loss: 0.602139, acc: 65.62%] [G loss: 1.873680]\n",
      "epoch:29 step:27876 [D loss: 0.710085, acc: 57.03%] [G loss: 1.809974]\n",
      "epoch:29 step:27877 [D loss: 0.653200, acc: 59.38%] [G loss: 1.775785]\n",
      "epoch:29 step:27878 [D loss: 0.644987, acc: 58.59%] [G loss: 1.810421]\n",
      "epoch:29 step:27879 [D loss: 0.649844, acc: 62.50%] [G loss: 1.980637]\n",
      "epoch:29 step:27880 [D loss: 0.603068, acc: 68.75%] [G loss: 2.034883]\n",
      "epoch:29 step:27881 [D loss: 0.626201, acc: 63.28%] [G loss: 1.834467]\n",
      "epoch:29 step:27882 [D loss: 0.621639, acc: 68.75%] [G loss: 1.911426]\n",
      "epoch:29 step:27883 [D loss: 0.640539, acc: 64.84%] [G loss: 1.866541]\n",
      "epoch:29 step:27884 [D loss: 0.635930, acc: 61.72%] [G loss: 1.932740]\n",
      "epoch:29 step:27885 [D loss: 0.633196, acc: 63.28%] [G loss: 2.006196]\n",
      "epoch:29 step:27886 [D loss: 0.660933, acc: 57.81%] [G loss: 1.927150]\n",
      "epoch:29 step:27887 [D loss: 0.641137, acc: 61.72%] [G loss: 1.803435]\n",
      "epoch:29 step:27888 [D loss: 0.603262, acc: 63.28%] [G loss: 2.004210]\n",
      "epoch:29 step:27889 [D loss: 0.721636, acc: 55.47%] [G loss: 1.888548]\n",
      "epoch:29 step:27890 [D loss: 0.635825, acc: 63.28%] [G loss: 1.882749]\n",
      "epoch:29 step:27891 [D loss: 0.644658, acc: 61.72%] [G loss: 1.873337]\n",
      "epoch:29 step:27892 [D loss: 0.631531, acc: 65.62%] [G loss: 2.122971]\n",
      "epoch:29 step:27893 [D loss: 0.623790, acc: 63.28%] [G loss: 2.108650]\n",
      "epoch:29 step:27894 [D loss: 0.665210, acc: 63.28%] [G loss: 2.045679]\n",
      "epoch:29 step:27895 [D loss: 0.652488, acc: 61.72%] [G loss: 1.887774]\n",
      "epoch:29 step:27896 [D loss: 0.695546, acc: 55.47%] [G loss: 1.724093]\n",
      "epoch:29 step:27897 [D loss: 0.633455, acc: 67.97%] [G loss: 2.009271]\n",
      "epoch:29 step:27898 [D loss: 0.618161, acc: 67.97%] [G loss: 1.994924]\n",
      "epoch:29 step:27899 [D loss: 0.658768, acc: 63.28%] [G loss: 2.003170]\n",
      "epoch:29 step:27900 [D loss: 0.653517, acc: 61.72%] [G loss: 1.955174]\n",
      "epoch:29 step:27901 [D loss: 0.632308, acc: 65.62%] [G loss: 1.976886]\n",
      "epoch:29 step:27902 [D loss: 0.621958, acc: 67.97%] [G loss: 1.936241]\n",
      "epoch:29 step:27903 [D loss: 0.672632, acc: 58.59%] [G loss: 1.841406]\n",
      "epoch:29 step:27904 [D loss: 0.647438, acc: 59.38%] [G loss: 1.810447]\n",
      "epoch:29 step:27905 [D loss: 0.679662, acc: 55.47%] [G loss: 1.757691]\n",
      "epoch:29 step:27906 [D loss: 0.646728, acc: 65.62%] [G loss: 1.841861]\n",
      "epoch:29 step:27907 [D loss: 0.646711, acc: 60.94%] [G loss: 1.855693]\n",
      "epoch:29 step:27908 [D loss: 0.674918, acc: 58.59%] [G loss: 1.785059]\n",
      "epoch:29 step:27909 [D loss: 0.593085, acc: 67.19%] [G loss: 1.993350]\n",
      "epoch:29 step:27910 [D loss: 0.616369, acc: 70.31%] [G loss: 1.791499]\n",
      "epoch:29 step:27911 [D loss: 0.667219, acc: 59.38%] [G loss: 1.841297]\n",
      "epoch:29 step:27912 [D loss: 0.672042, acc: 60.94%] [G loss: 1.782981]\n",
      "epoch:29 step:27913 [D loss: 0.606642, acc: 64.06%] [G loss: 1.886729]\n",
      "epoch:29 step:27914 [D loss: 0.660191, acc: 64.06%] [G loss: 1.876378]\n",
      "epoch:29 step:27915 [D loss: 0.650164, acc: 61.72%] [G loss: 1.831562]\n",
      "epoch:29 step:27916 [D loss: 0.601346, acc: 65.62%] [G loss: 1.942092]\n",
      "epoch:29 step:27917 [D loss: 0.660834, acc: 57.81%] [G loss: 1.729878]\n",
      "epoch:29 step:27918 [D loss: 0.609724, acc: 65.62%] [G loss: 1.818505]\n",
      "epoch:29 step:27919 [D loss: 0.673656, acc: 57.81%] [G loss: 1.948760]\n",
      "epoch:29 step:27920 [D loss: 0.607110, acc: 73.44%] [G loss: 1.945844]\n",
      "epoch:29 step:27921 [D loss: 0.629570, acc: 64.06%] [G loss: 1.887708]\n",
      "epoch:29 step:27922 [D loss: 0.665651, acc: 60.16%] [G loss: 1.822115]\n",
      "epoch:29 step:27923 [D loss: 0.651478, acc: 65.62%] [G loss: 1.922355]\n",
      "epoch:29 step:27924 [D loss: 0.673524, acc: 58.59%] [G loss: 1.903186]\n",
      "epoch:29 step:27925 [D loss: 0.712144, acc: 53.12%] [G loss: 1.709487]\n",
      "epoch:29 step:27926 [D loss: 0.612033, acc: 67.97%] [G loss: 1.991170]\n",
      "epoch:29 step:27927 [D loss: 0.666166, acc: 62.50%] [G loss: 1.938992]\n",
      "epoch:29 step:27928 [D loss: 0.644690, acc: 63.28%] [G loss: 1.968334]\n",
      "epoch:29 step:27929 [D loss: 0.698201, acc: 54.69%] [G loss: 1.857826]\n",
      "epoch:29 step:27930 [D loss: 0.693596, acc: 58.59%] [G loss: 1.830283]\n",
      "epoch:29 step:27931 [D loss: 0.614253, acc: 72.66%] [G loss: 1.911188]\n",
      "epoch:29 step:27932 [D loss: 0.667491, acc: 59.38%] [G loss: 1.718605]\n",
      "epoch:29 step:27933 [D loss: 0.631892, acc: 60.94%] [G loss: 1.815410]\n",
      "epoch:29 step:27934 [D loss: 0.629041, acc: 61.72%] [G loss: 1.967242]\n",
      "epoch:29 step:27935 [D loss: 0.689971, acc: 51.56%] [G loss: 1.769177]\n",
      "epoch:29 step:27936 [D loss: 0.696078, acc: 54.69%] [G loss: 1.843719]\n",
      "epoch:29 step:27937 [D loss: 0.683189, acc: 58.59%] [G loss: 1.807583]\n",
      "epoch:29 step:27938 [D loss: 0.705240, acc: 54.69%] [G loss: 1.872760]\n",
      "epoch:29 step:27939 [D loss: 0.711630, acc: 50.78%] [G loss: 1.778466]\n",
      "epoch:29 step:27940 [D loss: 0.660822, acc: 60.16%] [G loss: 1.802899]\n",
      "epoch:29 step:27941 [D loss: 0.632917, acc: 62.50%] [G loss: 1.947271]\n",
      "epoch:29 step:27942 [D loss: 0.640924, acc: 66.41%] [G loss: 1.841869]\n",
      "epoch:29 step:27943 [D loss: 0.625491, acc: 66.41%] [G loss: 1.911970]\n",
      "epoch:29 step:27944 [D loss: 0.674375, acc: 58.59%] [G loss: 1.791777]\n",
      "epoch:29 step:27945 [D loss: 0.631799, acc: 63.28%] [G loss: 1.866946]\n",
      "epoch:29 step:27946 [D loss: 0.674071, acc: 60.94%] [G loss: 1.759081]\n",
      "epoch:29 step:27947 [D loss: 0.591856, acc: 68.75%] [G loss: 2.088521]\n",
      "epoch:29 step:27948 [D loss: 0.615070, acc: 71.09%] [G loss: 2.114201]\n",
      "epoch:29 step:27949 [D loss: 0.638406, acc: 63.28%] [G loss: 1.998419]\n",
      "epoch:29 step:27950 [D loss: 0.648303, acc: 61.72%] [G loss: 1.965479]\n",
      "epoch:29 step:27951 [D loss: 0.610600, acc: 67.19%] [G loss: 1.965879]\n",
      "epoch:29 step:27952 [D loss: 0.625141, acc: 64.84%] [G loss: 1.876702]\n",
      "epoch:29 step:27953 [D loss: 0.647464, acc: 60.16%] [G loss: 1.893572]\n",
      "epoch:29 step:27954 [D loss: 0.660202, acc: 62.50%] [G loss: 1.982486]\n",
      "epoch:29 step:27955 [D loss: 0.653869, acc: 65.62%] [G loss: 1.983784]\n",
      "epoch:29 step:27956 [D loss: 0.639652, acc: 64.84%] [G loss: 1.838688]\n",
      "epoch:29 step:27957 [D loss: 0.691188, acc: 60.16%] [G loss: 1.713554]\n",
      "epoch:29 step:27958 [D loss: 0.674660, acc: 61.72%] [G loss: 1.872142]\n",
      "epoch:29 step:27959 [D loss: 0.610207, acc: 64.84%] [G loss: 1.871738]\n",
      "epoch:29 step:27960 [D loss: 0.667631, acc: 64.84%] [G loss: 1.870593]\n",
      "epoch:29 step:27961 [D loss: 0.650042, acc: 64.84%] [G loss: 1.890128]\n",
      "epoch:29 step:27962 [D loss: 0.641765, acc: 60.16%] [G loss: 2.055625]\n",
      "epoch:29 step:27963 [D loss: 0.632637, acc: 65.62%] [G loss: 1.866991]\n",
      "epoch:29 step:27964 [D loss: 0.685984, acc: 60.94%] [G loss: 1.909061]\n",
      "epoch:29 step:27965 [D loss: 0.629371, acc: 65.62%] [G loss: 1.874435]\n",
      "epoch:29 step:27966 [D loss: 0.659706, acc: 57.03%] [G loss: 1.839394]\n",
      "epoch:29 step:27967 [D loss: 0.650265, acc: 59.38%] [G loss: 1.782098]\n",
      "epoch:29 step:27968 [D loss: 0.727963, acc: 45.31%] [G loss: 1.769834]\n",
      "epoch:29 step:27969 [D loss: 0.669321, acc: 54.69%] [G loss: 1.696356]\n",
      "epoch:29 step:27970 [D loss: 0.646507, acc: 66.41%] [G loss: 1.920213]\n",
      "epoch:29 step:27971 [D loss: 0.643849, acc: 61.72%] [G loss: 1.894221]\n",
      "epoch:29 step:27972 [D loss: 0.667029, acc: 59.38%] [G loss: 1.790215]\n",
      "epoch:29 step:27973 [D loss: 0.716096, acc: 58.59%] [G loss: 1.706486]\n",
      "epoch:29 step:27974 [D loss: 0.655262, acc: 60.94%] [G loss: 1.765351]\n",
      "epoch:29 step:27975 [D loss: 0.623597, acc: 65.62%] [G loss: 1.850828]\n",
      "epoch:29 step:27976 [D loss: 0.641045, acc: 67.19%] [G loss: 1.747219]\n",
      "epoch:29 step:27977 [D loss: 0.640024, acc: 64.06%] [G loss: 1.948886]\n",
      "epoch:29 step:27978 [D loss: 0.662056, acc: 54.69%] [G loss: 1.896267]\n",
      "epoch:29 step:27979 [D loss: 0.626663, acc: 66.41%] [G loss: 1.838925]\n",
      "epoch:29 step:27980 [D loss: 0.642843, acc: 65.62%] [G loss: 1.930312]\n",
      "epoch:29 step:27981 [D loss: 0.657748, acc: 60.94%] [G loss: 1.927523]\n",
      "epoch:29 step:27982 [D loss: 0.671819, acc: 61.72%] [G loss: 1.948969]\n",
      "epoch:29 step:27983 [D loss: 0.652163, acc: 56.25%] [G loss: 1.791193]\n",
      "epoch:29 step:27984 [D loss: 0.616544, acc: 63.28%] [G loss: 1.846332]\n",
      "epoch:29 step:27985 [D loss: 0.610898, acc: 67.19%] [G loss: 1.950452]\n",
      "epoch:29 step:27986 [D loss: 0.677262, acc: 62.50%] [G loss: 2.014472]\n",
      "epoch:29 step:27987 [D loss: 0.611089, acc: 65.62%] [G loss: 1.950800]\n",
      "epoch:29 step:27988 [D loss: 0.570546, acc: 68.75%] [G loss: 2.183881]\n",
      "epoch:29 step:27989 [D loss: 0.627784, acc: 66.41%] [G loss: 2.046380]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:29 step:27990 [D loss: 0.684691, acc: 56.25%] [G loss: 1.817436]\n",
      "epoch:29 step:27991 [D loss: 0.676021, acc: 57.03%] [G loss: 1.799149]\n",
      "epoch:29 step:27992 [D loss: 0.634606, acc: 64.06%] [G loss: 1.855769]\n",
      "epoch:29 step:27993 [D loss: 0.691691, acc: 56.25%] [G loss: 1.675019]\n",
      "epoch:29 step:27994 [D loss: 0.681356, acc: 56.25%] [G loss: 1.709130]\n",
      "epoch:29 step:27995 [D loss: 0.650272, acc: 60.94%] [G loss: 1.853742]\n",
      "epoch:29 step:27996 [D loss: 0.615344, acc: 67.97%] [G loss: 1.881715]\n",
      "epoch:29 step:27997 [D loss: 0.686544, acc: 59.38%] [G loss: 1.809553]\n",
      "epoch:29 step:27998 [D loss: 0.643269, acc: 60.94%] [G loss: 1.998341]\n",
      "epoch:29 step:27999 [D loss: 0.687668, acc: 54.69%] [G loss: 1.793290]\n",
      "epoch:29 step:28000 [D loss: 0.638199, acc: 62.50%] [G loss: 1.781586]\n",
      "##############\n",
      "[2.33132682 1.34286611 6.14987486 4.79874249 3.5009655  5.35673421\n",
      " 4.33827509 4.6816606  4.49754165 3.71467231]\n",
      "##########\n",
      "epoch:29 step:28001 [D loss: 0.733953, acc: 50.78%] [G loss: 1.884341]\n",
      "epoch:29 step:28002 [D loss: 0.679234, acc: 59.38%] [G loss: 1.701444]\n",
      "epoch:29 step:28003 [D loss: 0.692665, acc: 59.38%] [G loss: 1.781544]\n",
      "epoch:29 step:28004 [D loss: 0.598011, acc: 69.53%] [G loss: 1.901751]\n",
      "epoch:29 step:28005 [D loss: 0.609269, acc: 69.53%] [G loss: 1.883782]\n",
      "epoch:29 step:28006 [D loss: 0.563276, acc: 71.88%] [G loss: 1.884850]\n",
      "epoch:29 step:28007 [D loss: 0.658588, acc: 54.69%] [G loss: 1.831014]\n",
      "epoch:29 step:28008 [D loss: 0.610808, acc: 64.06%] [G loss: 1.853186]\n",
      "epoch:29 step:28009 [D loss: 0.689288, acc: 60.94%] [G loss: 1.853881]\n",
      "epoch:29 step:28010 [D loss: 0.692160, acc: 59.38%] [G loss: 1.975924]\n",
      "epoch:29 step:28011 [D loss: 0.626514, acc: 66.41%] [G loss: 1.912801]\n",
      "epoch:29 step:28012 [D loss: 0.600557, acc: 66.41%] [G loss: 1.870835]\n",
      "epoch:29 step:28013 [D loss: 0.629737, acc: 67.19%] [G loss: 1.837483]\n",
      "epoch:29 step:28014 [D loss: 0.656886, acc: 60.16%] [G loss: 1.875014]\n",
      "epoch:29 step:28015 [D loss: 0.620705, acc: 68.75%] [G loss: 1.910752]\n",
      "epoch:29 step:28016 [D loss: 0.640100, acc: 64.06%] [G loss: 2.007499]\n",
      "epoch:29 step:28017 [D loss: 0.670136, acc: 60.16%] [G loss: 1.922551]\n",
      "epoch:29 step:28018 [D loss: 0.641765, acc: 61.72%] [G loss: 2.009514]\n",
      "epoch:29 step:28019 [D loss: 0.661912, acc: 56.25%] [G loss: 1.797991]\n",
      "epoch:29 step:28020 [D loss: 0.620765, acc: 66.41%] [G loss: 1.995655]\n",
      "epoch:29 step:28021 [D loss: 0.602442, acc: 72.66%] [G loss: 1.810685]\n",
      "epoch:29 step:28022 [D loss: 0.680572, acc: 59.38%] [G loss: 1.974356]\n",
      "epoch:29 step:28023 [D loss: 0.641931, acc: 61.72%] [G loss: 1.878036]\n",
      "epoch:29 step:28024 [D loss: 0.652450, acc: 60.16%] [G loss: 1.800198]\n",
      "epoch:29 step:28025 [D loss: 0.667716, acc: 66.41%] [G loss: 1.872326]\n",
      "epoch:29 step:28026 [D loss: 0.655400, acc: 59.38%] [G loss: 1.991071]\n",
      "epoch:29 step:28027 [D loss: 0.634109, acc: 62.50%] [G loss: 1.904505]\n",
      "epoch:29 step:28028 [D loss: 0.663036, acc: 56.25%] [G loss: 1.886635]\n",
      "epoch:29 step:28029 [D loss: 0.642741, acc: 63.28%] [G loss: 1.815270]\n",
      "epoch:29 step:28030 [D loss: 0.684012, acc: 59.38%] [G loss: 1.786714]\n",
      "epoch:29 step:28031 [D loss: 0.747600, acc: 46.88%] [G loss: 1.790922]\n",
      "epoch:29 step:28032 [D loss: 0.713619, acc: 51.56%] [G loss: 1.837705]\n",
      "epoch:29 step:28033 [D loss: 0.664899, acc: 56.25%] [G loss: 1.893871]\n",
      "epoch:29 step:28034 [D loss: 0.660508, acc: 60.94%] [G loss: 1.700905]\n",
      "epoch:29 step:28035 [D loss: 0.635601, acc: 66.41%] [G loss: 1.739209]\n",
      "epoch:29 step:28036 [D loss: 0.679484, acc: 54.69%] [G loss: 1.868546]\n",
      "epoch:29 step:28037 [D loss: 0.678958, acc: 54.69%] [G loss: 1.676701]\n",
      "epoch:29 step:28038 [D loss: 0.662070, acc: 61.72%] [G loss: 1.765441]\n",
      "epoch:29 step:28039 [D loss: 0.630511, acc: 62.50%] [G loss: 1.848857]\n",
      "epoch:29 step:28040 [D loss: 0.673175, acc: 54.69%] [G loss: 1.842551]\n",
      "epoch:29 step:28041 [D loss: 0.645550, acc: 64.06%] [G loss: 1.856304]\n",
      "epoch:29 step:28042 [D loss: 0.652504, acc: 58.59%] [G loss: 1.759288]\n",
      "epoch:29 step:28043 [D loss: 0.648315, acc: 61.72%] [G loss: 1.727929]\n",
      "epoch:29 step:28044 [D loss: 0.644095, acc: 64.06%] [G loss: 1.697152]\n",
      "epoch:29 step:28045 [D loss: 0.661226, acc: 59.38%] [G loss: 1.702976]\n",
      "epoch:29 step:28046 [D loss: 0.726936, acc: 49.22%] [G loss: 1.783401]\n",
      "epoch:29 step:28047 [D loss: 0.676316, acc: 58.59%] [G loss: 1.682401]\n",
      "epoch:29 step:28048 [D loss: 0.617207, acc: 67.19%] [G loss: 1.799550]\n",
      "epoch:29 step:28049 [D loss: 0.667995, acc: 67.97%] [G loss: 1.839314]\n",
      "epoch:29 step:28050 [D loss: 0.623492, acc: 62.50%] [G loss: 1.906179]\n",
      "epoch:29 step:28051 [D loss: 0.686811, acc: 62.50%] [G loss: 1.882325]\n",
      "epoch:29 step:28052 [D loss: 0.624099, acc: 65.62%] [G loss: 1.789965]\n",
      "epoch:29 step:28053 [D loss: 0.660218, acc: 60.16%] [G loss: 1.769349]\n",
      "epoch:29 step:28054 [D loss: 0.650559, acc: 60.16%] [G loss: 1.825166]\n",
      "epoch:29 step:28055 [D loss: 0.689868, acc: 60.94%] [G loss: 1.890899]\n",
      "epoch:29 step:28056 [D loss: 0.694936, acc: 53.91%] [G loss: 1.866029]\n",
      "epoch:29 step:28057 [D loss: 0.647949, acc: 60.94%] [G loss: 1.860446]\n",
      "epoch:29 step:28058 [D loss: 0.666038, acc: 64.06%] [G loss: 1.804465]\n",
      "epoch:29 step:28059 [D loss: 0.621702, acc: 64.84%] [G loss: 1.901655]\n",
      "epoch:29 step:28060 [D loss: 0.634742, acc: 59.38%] [G loss: 1.965448]\n",
      "epoch:29 step:28061 [D loss: 0.633841, acc: 64.06%] [G loss: 1.903031]\n",
      "epoch:29 step:28062 [D loss: 0.643264, acc: 67.19%] [G loss: 1.746721]\n",
      "epoch:29 step:28063 [D loss: 0.648645, acc: 63.28%] [G loss: 1.805675]\n",
      "epoch:29 step:28064 [D loss: 0.675541, acc: 60.94%] [G loss: 1.799447]\n",
      "epoch:29 step:28065 [D loss: 0.699155, acc: 56.25%] [G loss: 1.758590]\n",
      "epoch:29 step:28066 [D loss: 0.680760, acc: 54.69%] [G loss: 1.953957]\n",
      "epoch:29 step:28067 [D loss: 0.643217, acc: 64.06%] [G loss: 1.861493]\n",
      "epoch:29 step:28068 [D loss: 0.629002, acc: 63.28%] [G loss: 1.696812]\n",
      "epoch:29 step:28069 [D loss: 0.688517, acc: 61.72%] [G loss: 1.725238]\n",
      "epoch:29 step:28070 [D loss: 0.665275, acc: 61.72%] [G loss: 1.774441]\n",
      "epoch:29 step:28071 [D loss: 0.637593, acc: 63.28%] [G loss: 1.926503]\n",
      "epoch:29 step:28072 [D loss: 0.623423, acc: 63.28%] [G loss: 1.952635]\n",
      "epoch:29 step:28073 [D loss: 0.636191, acc: 67.97%] [G loss: 1.954148]\n",
      "epoch:29 step:28074 [D loss: 0.650953, acc: 60.94%] [G loss: 1.867979]\n",
      "epoch:29 step:28075 [D loss: 0.597273, acc: 70.31%] [G loss: 1.884156]\n",
      "epoch:29 step:28076 [D loss: 0.616226, acc: 67.19%] [G loss: 1.879889]\n",
      "epoch:29 step:28077 [D loss: 0.641482, acc: 64.84%] [G loss: 2.027293]\n",
      "epoch:29 step:28078 [D loss: 0.660333, acc: 62.50%] [G loss: 2.031475]\n",
      "epoch:29 step:28079 [D loss: 0.630700, acc: 63.28%] [G loss: 1.881963]\n",
      "epoch:29 step:28080 [D loss: 0.643636, acc: 63.28%] [G loss: 1.944637]\n",
      "epoch:29 step:28081 [D loss: 0.615684, acc: 71.88%] [G loss: 1.871716]\n",
      "epoch:29 step:28082 [D loss: 0.624877, acc: 68.75%] [G loss: 1.940138]\n",
      "epoch:29 step:28083 [D loss: 0.671539, acc: 56.25%] [G loss: 1.854982]\n",
      "epoch:29 step:28084 [D loss: 0.692838, acc: 53.91%] [G loss: 1.913951]\n",
      "epoch:29 step:28085 [D loss: 0.674288, acc: 58.59%] [G loss: 1.982278]\n",
      "epoch:29 step:28086 [D loss: 0.662938, acc: 57.03%] [G loss: 1.812706]\n",
      "epoch:29 step:28087 [D loss: 0.681283, acc: 59.38%] [G loss: 1.702902]\n",
      "epoch:29 step:28088 [D loss: 0.688215, acc: 57.03%] [G loss: 1.823251]\n",
      "epoch:29 step:28089 [D loss: 0.654891, acc: 60.16%] [G loss: 1.897446]\n",
      "epoch:29 step:28090 [D loss: 0.679049, acc: 58.59%] [G loss: 1.813162]\n",
      "epoch:29 step:28091 [D loss: 0.615553, acc: 65.62%] [G loss: 1.831573]\n",
      "epoch:29 step:28092 [D loss: 0.608413, acc: 72.66%] [G loss: 2.107772]\n",
      "epoch:29 step:28093 [D loss: 0.678481, acc: 54.69%] [G loss: 1.793730]\n",
      "epoch:29 step:28094 [D loss: 0.635555, acc: 64.06%] [G loss: 1.926317]\n",
      "epoch:29 step:28095 [D loss: 0.649448, acc: 63.28%] [G loss: 1.864802]\n",
      "epoch:29 step:28096 [D loss: 0.581883, acc: 69.53%] [G loss: 2.137712]\n",
      "epoch:29 step:28097 [D loss: 0.591563, acc: 68.75%] [G loss: 2.031142]\n",
      "epoch:29 step:28098 [D loss: 0.627310, acc: 67.19%] [G loss: 2.071449]\n",
      "epoch:29 step:28099 [D loss: 0.594063, acc: 68.75%] [G loss: 1.945769]\n",
      "epoch:29 step:28100 [D loss: 0.678839, acc: 60.94%] [G loss: 1.957634]\n",
      "epoch:29 step:28101 [D loss: 0.813354, acc: 48.44%] [G loss: 1.783518]\n",
      "epoch:29 step:28102 [D loss: 0.757096, acc: 46.09%] [G loss: 1.821068]\n",
      "epoch:29 step:28103 [D loss: 0.589923, acc: 69.53%] [G loss: 1.872063]\n",
      "epoch:29 step:28104 [D loss: 0.642344, acc: 64.84%] [G loss: 1.988458]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:29 step:28105 [D loss: 0.633867, acc: 62.50%] [G loss: 1.950405]\n",
      "epoch:29 step:28106 [D loss: 0.648518, acc: 62.50%] [G loss: 1.786213]\n",
      "epoch:29 step:28107 [D loss: 0.651119, acc: 58.59%] [G loss: 1.955636]\n",
      "epoch:29 step:28108 [D loss: 0.590074, acc: 65.62%] [G loss: 1.808781]\n",
      "epoch:29 step:28109 [D loss: 0.581717, acc: 70.31%] [G loss: 1.975477]\n",
      "epoch:29 step:28110 [D loss: 0.637209, acc: 62.50%] [G loss: 2.395953]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "if not os.path.isdir('saved_models_mnist_{}'.format('bigan')):\n",
    "    os.mkdir('saved_models_mnist_{}'.format('bigan'))\n",
    "f = open('saved_models_mnist_{}/log_collapse1.txt'.format('bigan'), mode='w')\n",
    "import torch.utils.data as Data\n",
    "\n",
    "from keras.datasets import mnist,fashion_mnist\n",
    "from keras.layers import Input, Dense, Reshape, Flatten, Dropout, multiply, GaussianNoise\n",
    "from keras.layers import BatchNormalization, Activation, Embedding, ZeroPadding2D\n",
    "from keras.layers import MaxPooling2D, concatenate\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.layers.convolutional import UpSampling2D, Conv2D\n",
    "from keras.models import Sequential, Model\n",
    "from keras.optimizers import Adam\n",
    "from keras import losses\n",
    "from keras.utils import to_categorical\n",
    "import keras.backend as K\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "class BIGAN():\n",
    "    def __init__(self):\n",
    "        self.img_rows = 28\n",
    "        self.img_cols = 28\n",
    "        self.channels = 1\n",
    "        self.img_shape = (self.img_rows, self.img_cols, self.channels)\n",
    "        self.latent_dim = 100\n",
    "        self.x = []\n",
    "        self.y = np.zeros((31, 1), dtype=np.int)\n",
    "        self.y = list(self.y)\n",
    "        for i in range(31):\n",
    "            self.y[i] = []\n",
    "\n",
    "        optimizer = Adam(0.0002, 0.5)\n",
    "\n",
    "        # Build and compile the discriminator\n",
    "        self.discriminator = self.build_discriminator()\n",
    "        self.discriminator.compile(loss=['binary_crossentropy'],\n",
    "            optimizer=optimizer,\n",
    "            metrics=['accuracy'])\n",
    "\n",
    "        # Build the generator\n",
    "        self.generator = self.build_generator()\n",
    "\n",
    "        # Build the encoder\n",
    "        self.encoder = self.build_encoder()\n",
    "\n",
    "        # The part of the bigan that trains the discriminator and encoder\n",
    "        self.discriminator.trainable = False\n",
    "\n",
    "        # Generate image from sampled noise\n",
    "        z = Input(shape=(self.latent_dim, ))\n",
    "        img_ = self.generator(z)\n",
    "\n",
    "        # Encode image\n",
    "        img = Input(shape=self.img_shape)\n",
    "        z_ = self.encoder(img)\n",
    "\n",
    "        # Latent -> img is fake, and img -> latent is valid\n",
    "        fake = self.discriminator([z, img_])\n",
    "        valid = self.discriminator([z_, img])\n",
    "\n",
    "        # Set up and compile the combined model\n",
    "        # Trains generator to fool the discriminator\n",
    "        self.bigan_generator = Model([z, img], [fake, valid])\n",
    "        self.bigan_generator.compile(loss=['binary_crossentropy', 'binary_crossentropy'],\n",
    "            optimizer=optimizer)\n",
    "\n",
    "\n",
    "    def build_encoder(self):\n",
    "        model = Sequential()\n",
    "\n",
    "        model.add(Flatten(input_shape=self.img_shape))\n",
    "        model.add(Dense(512))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(Dense(512))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(Dense(self.latent_dim))\n",
    "\n",
    "        model.summary()\n",
    "\n",
    "        img = Input(shape=self.img_shape)\n",
    "        z = model(img)\n",
    "\n",
    "        return Model(img, z)\n",
    "\n",
    "    def build_generator(self):\n",
    "        model = Sequential()\n",
    "\n",
    "        model.add(Dense(512, input_dim=self.latent_dim))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(Dense(512))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(Dense(np.prod(self.img_shape), activation='tanh'))\n",
    "        model.add(Reshape(self.img_shape))\n",
    "\n",
    "        model.summary()\n",
    "\n",
    "        z = Input(shape=(self.latent_dim,))\n",
    "        gen_img = model(z)\n",
    "\n",
    "        return Model(z, gen_img)\n",
    "\n",
    "    def build_discriminator(self):\n",
    "\n",
    "        z = Input(shape=(self.latent_dim, ))\n",
    "        img = Input(shape=self.img_shape)\n",
    "        d_in = concatenate([z, Flatten()(img)])\n",
    "\n",
    "        model = Dense(1024)(d_in)\n",
    "        model = LeakyReLU(alpha=0.2)(model)\n",
    "        model = Dropout(0.5)(model)\n",
    "        model = Dense(1024)(model)\n",
    "        model = LeakyReLU(alpha=0.2)(model)\n",
    "        model = Dropout(0.5)(model)\n",
    "        model = Dense(1024)(model)\n",
    "        model = LeakyReLU(alpha=0.2)(model)\n",
    "        model = Dropout(0.5)(model)\n",
    "        validity = Dense(1, activation=\"sigmoid\")(model)\n",
    "\n",
    "        return Model([z, img], validity)\n",
    "\n",
    "    def train(self, epochs, batch_size=128, sample_interval=50):\n",
    "\n",
    "        # Load the dataset\n",
    "        (X_train, _), (X_test,y_test) = mnist.load_data()\n",
    "\n",
    "        # Rescale -1 to 1\n",
    "        X_train = (X_train.astype(np.float32) - 127.5) / 127.5\n",
    "        X_train = np.expand_dims(X_train, axis=3)\n",
    "        X_test = (X_test.astype(np.float32) - 127.5) / 127.5\n",
    "        # X_test = X_test / 127.5 - 1.\n",
    "        X_test = np.expand_dims(X_test, axis=3)\n",
    "\n",
    "        # Adversarial ground truths\n",
    "        valid = np.ones((batch_size, 1))\n",
    "        fake = np.zeros((batch_size, 1))\n",
    "\n",
    "        nb_batches = int(X_train.shape[0] / batch_size)\n",
    "        global_step = 0\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "\n",
    "            for index in range(nb_batches):\n",
    "                global_step += 1\n",
    "                image_batch = X_train[index * batch_size:(index + 1) * batch_size]\n",
    "                z = np.random.normal(size=(batch_size, self.latent_dim))\n",
    "                imgs_ = self.generator.predict(z)\n",
    "\n",
    "                # Select a random batch of images and encode\n",
    "                # idx = np.random.randint(0, X_train.shape[0], batch_size)\n",
    "                # imgs = X_train[idx]\n",
    "                z_ = self.encoder.predict(image_batch)\n",
    "\n",
    "                # Train the discriminator (img -> z is valid, z -> img is fake)\n",
    "                d_loss_real = self.discriminator.train_on_batch([z_, image_batch], valid)\n",
    "                d_loss_fake = self.discriminator.train_on_batch([z, imgs_], fake)\n",
    "                d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "\n",
    "                # ---------------------\n",
    "                #  Train Generator\n",
    "                # ---------------------\n",
    "\n",
    "                # Train the generator (z -> img is valid and img -> z is is invalid)\n",
    "                g_loss = self.bigan_generator.train_on_batch([z, image_batch], [valid, fake])\n",
    "\n",
    "                # Plot the progress\n",
    "                print(\"epoch:%d step:%d [D loss: %f, acc: %.2f%%] [G loss: %f]\" % (epoch,global_step, d_loss[0],\n",
    "                                                                                   100 * d_loss[1], g_loss[0]))\n",
    "\n",
    "                sampleSize = 5000\n",
    "                # If at save interval => save generated image samples\n",
    "                if global_step % sample_interval == 0:\n",
    "                    s = self.sample_images(global_step, X_test,y_test, sampleSize)\n",
    "    def sample_images(self, epoch,x_test,y_test,sample_num):\n",
    "        r, c = sample_num//10, 10\n",
    "        noise = np.random.normal(0, 1, (r * c, 100))\n",
    "#         sampled_labels = np.array([num for _ in range(r) for num in range(c)])\n",
    "        gen_imgs = self.generator.predict([noise])\n",
    "        # Rescale images 0 - 1\n",
    "        gen_imgs = 0.5 * gen_imgs + 0.5\n",
    "        labels = np.squeeze(y_test[:sample_num])\n",
    "        labels = np.squeeze(labels)\n",
    "        dis_real = cal_distance_image_real(x_test[:sample_num], labels)\n",
    "        dis_fake = cal_distance_image_fake(gen_imgs)\n",
    "        dis_cha = dis_real - dis_fake\n",
    "        print('##############')\n",
    "        # print(dis_real)\n",
    "        # print(dis_fake)\n",
    "        print(dis_cha)\n",
    "        print('##########')\n",
    "        f.writelines('\\n')\n",
    "        f.writelines('epoch:' + str(epoch))\n",
    "        f.writelines('\\n')\n",
    "        f.writelines('紧度')\n",
    "        f.writelines('\\n')\n",
    "        f.writelines(' %.8f ' % (i) for i in dis_cha)\n",
    "        f.writelines('\\n')\n",
    "if __name__ == '__main__':\n",
    "    bigan = BIGAN()\n",
    "    bigan.train(epochs=30, batch_size=64, sample_interval=200)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pppppppp [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
